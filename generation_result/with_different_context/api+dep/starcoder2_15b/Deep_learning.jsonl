{"instance_id": "1f9f4d85e825", "generation_result": [{"code": "def load_model_from_config(self, half_attention)\n\t\n    if half_attention:\n        model_path = self.model_path.replace(\".safetensors\", \".safetensors.h14\")\n        if os.path.exists(model_path):\n            self.model_path = model_path\n\n    if self.model_cache_path is not None and os.path.exists(self.model_cache_path):\n        try:\n            model = torch.load(self.model_cache_path, map_location=self.device)\n        except Exception:\n            model = None\n    else:\n        model = None\n\n    if model is None:\n        model = load_file(self.model_path, device=self.device)\n        if self.model_cache_path is not None:\n            torch.save(model, self.model_cache_path)\n\n    if self.half_precision:\n        model.half()\n\n    if self.memory_format:\n        model.to(memory_format=torch.channels_last)\n\n    model.eval()\n\n    return {\"model\": model}\n", "CodeBleu_score": 31.774010179314182}, {"code": "def load_model_from_config(self, half_attention)\n\t\n    # Check if a cached model is available and valid\n    if self.cache_dir and self.cache_dir.exists():\n        cached_model_path = self.cache_dir / f\"{self.model_name}.safetensors\"\n        if cached_model_path.exists():\n            try:\n                model = torch.jit.load(cached_model_path)\n                if model.device!= self.device:\n                    model = model.to(self.device)\n                return model\n            except Exception as e:\n                print(f\"Cached model is invalid. Error: {e}\")\n\n    # Load the model from the specified path\n    if self.model_path.suffix == \".safetensors\":\n        model = load_file(self.model_path)\n        model = torch.jit.load(model)\n    else:\n        model = torch.jit.load(self.model_path)\n\n    # Configure the model\n    model.config = OmegaConf.load(self.config_path)\n    model.config.attention_config.use_memory_efficient_attention = half_attention\n    model.config.model.params.memory_efficient_attention_enable_mem_efficient_attention = half_attention\n    model.config.model.params.memory_efficient_attention_use_fp16_acc = half_attention\n    model.config.model.params.memory_efficient_attention_use_fp16_acc_kv = half_attention\n    model.config.model.params.memory_efficient_attention_use_fp16_acc_v = half_attention\n    model.config.model.params.memory_efficient_attention_use_fp16_acc_q = half_attention\n    model.config.model.params.memory_efficient_attention_use_fp16_acc_qk = half_attention\n    model.config.model.params.memory_efficient_attention_use_fp16_acc_qk_v = half_attention\n    model.config.model.params.memory_efficient_attention_use_fp16_acc_qkv = half_attention\n    model.config.model.params.memory_efficient_attention_use_fp16_acc_q_k = half_attention\n    model.config.model.params.memory_efficient_attention_use_fp16_acc_q_v", "CodeBleu_score": 31.108532563532815}, {"code": "def load_model_from_config(self, half_attention)\n\t\n    model = None\n    if self.cache_dir is not None:\n        cache_path = os.path.join(self.cache_dir, f\"{self.model_name}.pt\")\n        if os.path.exists(cache_path):\n            try:\n                model = torch.load(cache_path)\n            except Exception:\n                pass\n    if model is None:\n        model = self.load_model_from_path(self.model_path, half_attention)\n    model.eval()\n    model = model.to(self.device)\n    if self.optimize_memory:\n        model.set_memory_format(torch.channels_last)\n    self.model = model\n    if self.cache_dir is not None:\n        torch.save(model, cache_path)\n    return {\"model\": model}\n\n", "CodeBleu_score": 28.391976896321886}]}
{"instance_id": "8dc955007aa2", "generation_result": [{"code": "def convsample_ddim(model, cond, steps, shape, eta=1.0, callback=None, normals_sequence=None,\n                    mask=None, x0=None, quantize_x0=False, temperature=1., score_corrector=None,\n                    corrector_kwargs=None, x_t=None\n                    )\n\t\n    sampler = DDIMSampler(model)\n    batch_size = shape[0]\n    print(\"eta:\", eta, \"steps:\", steps)\n    samples, values = sampler.sample(\n        steps, batch_size, shape, cond, eta=eta, callback=callback,\n        normals_sequence=normals_sequence, mask=mask, x0=x0, quantize_x0=quantize_x0,\n        temperature=temperature, score_corrector=score_corrector, corrector_kwargs=corrector_kwargs,\n        x_t=x_t\n    )\n    return samples, values\n", "CodeBleu_score": 42.967877769201756}, {"code": "def convsample_ddim(model, cond, steps, shape, eta=1.0, callback=None, normals_sequence=None,\n                    mask=None, x0=None, quantize_x0=False, temperature=1., score_corrector=None,\n                    corrector_kwargs=None, x_t=None\n                    )\n\t\n    # Initialize the sampler with the provided model\n    sampler = DDIMSampler(model)\n    # Extract the batch size from the shape parameter\n    batch_size = shape[0]\n    # Print the eta and steps values\n    print(\"eta:\", eta, \"steps:\", steps)\n    # Call the sample method of the DDIMSampler instance, passing several parameters\n    samples, values = sampler.sample(\n        steps=steps,\n        batch_size=batch_size,\n        shape=shape,\n        cond=cond,\n        eta=eta,\n        callback=callback,\n        normals_sequence=normals_sequence,\n        mask=mask,\n        x0=x0,\n        quantize_x0=quantize_x0,\n        temperature=temperature,\n        score_corrector=score_corrector,\n        corrector_kwargs=corrector_kwargs,\n        x_t=x_t,\n    )\n    # Return the generated samples and intermediate values\n    return samples, values\n", "CodeBleu_score": 49.69524042062503}, {"code": "def convsample_ddim(model, cond, steps, shape, eta=1.0, callback=None, normals_sequence=None,\n                    mask=None, x0=None, quantize_x0=False, temperature=1., score_corrector=None,\n                    corrector_kwargs=None, x_t=None\n                    )\n\t\n    sampler = DDIMSampler(model)\n    batch_size = shape[0]\n    print('eta', eta,'steps', steps)\n    return sampler.sample(steps, batch_size, shape, cond, eta, callback=callback, normals_sequence=normals_sequence,\n                          mask=mask, x0=x0, quantize_x0=quantize_x0, temperature=temperature,\n                          score_corrector=score_corrector, corrector_kwargs=corrector_kwargs, x_t=x_t)\n\n", "CodeBleu_score": 35.417082445938206}]}
{"instance_id": "9bdcd9081480", "generation_result": [{"code": "def ema_scope(self, context=None)\n\t\n    if self.parameters.ema_scope:\n        if self.parameters.ema_scope_verbose:\n            print(\"Switching to EMA weights\")\n        self.parameters.ema_scope_context = context\n        self.parameters.ema_scope_context.ema_scope_switch()\n    yield\n    if self.parameters.ema_scope:\n        if self.parameters.ema_scope_verbose:\n            print(\"Restoring original weights\")\n        self.parameters.ema_scope_context.ema_scope_restore()\n        self.parameters.ema_scope_context = None\n", "CodeBleu_score": 42.448417163426846}, {"code": "def ema_scope(self, context=None)\n\t\n    if context is not None:\n        print(f\"Switching to EMA weights for {self.name}\")\n    self.set_weights(self.get_ema_weights())\n    if context is not None:\n        print(f\"Restored original weights for {self.name}\")\n", "CodeBleu_score": 29.562689670900344}, {"code": "def ema_scope(self, context=None)\n\t\n    if self.ema_enabled:\n        self.parameters.model.load_state_dict(self.parameters.ema_model.state_dict())\n        if context is not None:\n            print('EMA weights are enabled. Switching to EMA weights.')\n    else:\n        if context is not None:\n            print('EMA weights are disabled.')\n    return self.parameters.model\n", "CodeBleu_score": 36.33084767937057}]}
{"instance_id": "14ab009e1c5c", "generation_result": [{"code": "def init_from_ckpt(self, path, ignore_keys=None)\n\t\n    sd = torch.load(path, map_location='cpu')\n    if ignore_keys is not None:\n        for k in list(sd.keys()):\n            if k.startswith(ignore_keys):\n                del sd[k]\n    self.load_state_dict(sd)\n    print(f'loaded {path}')\n    print(f'missing keys: {set(self.state_dict().keys()) - set(sd.keys())}')\n    print(f'unexpected keys: {set(sd.keys()) - set(self.state_dict().keys())}')\n", "CodeBleu_score": 28.51632601314826}, {"code": "def init_from_ckpt(self, path, ignore_keys=None)\n\t\n    sd = torch.load(path, map_location=lambda storage, loc: storage)\n    if ignore_keys is not None:\n        for k in list(sd.keys()):\n            if k.startswith(ignore_keys):\n                del sd[k]\n    self.load_state_dict(sd)\n    print(\"Missing keys:\", set(self.state_dict().keys()) - set(sd.keys()))\n    print(\"Unexpected keys:\", set(sd.keys()) - set(self.state_dict().keys()))\n", "CodeBleu_score": 25.76464975321201}, {"code": "def init_from_ckpt(self, path, ignore_keys=None)\n\t\n    if ignore_keys is None:\n        ignore_keys = []\n    sd = torch.load(path, map_location=lambda storage, loc: storage)\n    sd = {k: v for k, v in sd.items() if not any(k.startswith(ik) for ik in ignore_keys)}\n    missing, unexpected = self.load_state_dict(sd, strict=False)\n    if missing:\n        print(f\"Missing keys: {missing}\")\n    if unexpected:\n        print(f\"Unexpected keys: {unexpected}\")\n", "CodeBleu_score": 43.25068557022153}]}
{"instance_id": "754176e27b81", "generation_result": [{"code": "def forward(self, input, return_pred_indices=False)\n\t\n    quant, emb_loss, info = self.encode(input)\n    pred = self.decode(quant)\n    diff = torch.sum((input - pred) ** 2) / input.numel()\n    if return_pred_indices:\n        return pred, diff, info[\"pred_indices\"]\n    else:\n        return pred, diff\n\n", "CodeBleu_score": 28.48291278256046}, {"code": "def forward(self, input, return_pred_indices=False)\n\t\n    x = self.encode(input)\n    x = self.decode(x)\n    if return_pred_indices:\n        _, pred_indices = self.quantize.get_codebook_indices(x)\n        return x, pred_indices\n    return x\n", "CodeBleu_score": 32.13737483971697}, {"code": "def forward(self, input, return_pred_indices=False)\n\t\n    quant, emb_loss, info = self.encode(input)\n    pred = self.decode(quant)\n    diff = (pred - input).pow(2).sum() / input.numel()\n    if return_pred_indices:\n        return pred, diff, emb_loss, info\n    else:\n        return pred, diff, emb_loss\n", "CodeBleu_score": 25.59149051446316}]}
{"instance_id": "443df6fdaf0d", "generation_result": [{"code": "def get_input(self, batch, k)\n\t\n    x = batch[k]\n    if len(x.shape) == 3:\n        x = x.unsqueeze(0)\n    x = x.permute(0, 3, 1, 2)\n    if self.batch_resize:\n        x = F.interpolate(x, size=(self.batch_resize_range[0] + (self.batch_resize_range[1] - self.batch_resize_range[0]) * self.step / self.num_steps, self.batch_resize_range[0] + (self.batch_resize_range[1] - self.batch_resize_range[0]) * self.step / self.num_steps), mode='bicubic')\n    x = x.float().detach()\n    return x\n", "CodeBleu_score": 40.391771568779085}, {"code": "def get_input(self, batch, k)\n\t\n    x = batch[k]\n    if self.input_size is not None:\n        x = F.interpolate(x, size=self.input_size, mode='bicubic', align_corners=False)\n    x = x.permute(0, 3, 1, 2)\n    x = x.to(torch.float32).contiguous()\n    x = x.detach()\n    return x\n\n", "CodeBleu_score": 23.253183707362062}, {"code": "def get_input(self, batch, k)\n\t\n    x = batch[k]\n    if x.dim() == 3:\n        x = x.unsqueeze(0)\n    x = x.permute(0, 3, 1, 2).float()\n    if self.batch_resizing:\n        x = F.interpolate(x, scale_factor=self.batch_resizing_range[self.step % len(self.batch_resizing_range)], mode=\"bicubic\")\n    x = x.contiguous().detach()\n    return x\n", "CodeBleu_score": 33.08239353807049}]}
{"instance_id": "953a2632fc15", "generation_result": [{"code": "def training_step(self, batch, batch_idx, optimizer_idx)\n\t    # get input\n    x = self.get_input(batch, 0)\n    # get prediction\n    y = self.forward(x)\n    # compute loss\n    loss = self.loss(y, x)\n    # log metrics\n    self.log_dict(\n        {\n            \"train_loss\": loss,\n            \"lr\": self.optimizers()[0].param_groups[0][\"lr\"],\n            \"step\": self.global_step,\n        },\n        on_step=True,\n        on_epoch=True,\n        prog_bar=True,\n    )\n    return loss\n", "CodeBleu_score": 15.616824297221129}, {"code": "def training_step(self, batch, batch_idx, optimizer_idx)\n\t    # get input\n    x = self.get_input(batch, \"x\")\n    x_pred = self.get_input(batch, \"x_pred\")\n    # get predictions\n    if optimizer_idx == 0:\n        # generator\n        pred = self.forward(x)\n        # compute losses\n        loss = self.loss(pred, x_pred)\n    elif optimizer_idx == 1:\n        # discriminator\n        pred = self.forward(x)\n        # compute losses\n        loss = self.loss(pred, x_pred)\n    # log metrics\n    self.log_dict(\n        {\n            \"loss\": loss,\n            \"lr\": self.optimizers()[optimizer_idx].param_groups[0][\"lr\"],\n        },\n        on_step=True,\n        on_epoch=False,\n        prog_bar=True,\n        logger=True,\n    )\n    return loss", "CodeBleu_score": 30.217620575430825}, {"code": "def training_step(self, batch, batch_idx, optimizer_idx)\n\t    if optimizer_idx == 0:\n        x = self.get_input(batch, 0)\n        if self.model_type == \"autoencoder\":\n            x_hat = self(x)\n            loss = self.loss(x_hat, x)\n        elif self.model_type == \"discriminator\":\n            x_hat = self(x)\n            loss = self.loss(x_hat, torch.ones(x_hat.shape[0], device=x_hat.device))\n        self.log_dict(\n            {\n                \"train_loss\": loss,\n                \"train_psnr\": self.psnr(x_hat, x),\n                \"train_ssim\": self.ssim(x_hat, x),\n            },\n            on_step=True,\n            on_epoch=True,\n            prog_bar=True,\n            logger=True,\n        )\n        return loss\n    elif optimizer_idx == 1:\n        x = self.get_input(batch, 1)\n        x_hat = self(x)\n        loss = self.loss(x_hat, torch.zeros(x_hat.shape[0], device=x_hat.device))\n        self.log_dict(\n            {\"train_loss_disc\": loss}, on_step=True, on_epoch=True, prog_bar=True, logger=True\n        )\n        return loss", "CodeBleu_score": 39.93158187208485}]}
{"instance_id": "b449b5e8b77f", "generation_result": [{"code": "def log_images(self, batch, only_inputs=False, plot_ema=False, **kwargs)\n\t    if self.image_key is None:\n        return\n    if not self.global_step % self.log_interval:\n        self.log_image_interval = self.log_interval\n    else:\n        self.log_image_interval = 1\n    if not self.global_step % self.log_image_interval:\n        if self.image_key not in batch:\n            return\n        if self.image_key == \"segmentation\":\n            x = self.get_input(batch, self.image_key)\n            x = self.to_rgb(x)\n        else:\n            x = batch[self.image_key]\n        if len(x.shape) == 3:\n            x = x[..., None]\n        x = x.permute(0, 3, 1, 2).to(memory_format=torch.contiguous_format).float()\n        if self.batch_resize_range is not None:\n            lower_size = self.batch_resize_range[0]\n            upper_size = self.batch_resize_range[1]\n            if self.global_step <= 4:\n                # do the first few batches with max size to avoid later oom\n                new_resize = upper_size\n            else:\n                new_resize = np.random.choice(np.arange(lower_size, upper_size+16, 16))\n            if new_resize!= x.shape[2]:\n                x = F.interpolate(x, size=new_resize, mode=\"bicubic\")\n            x = x.detach()\n        if self.image_key == \"segmentation\":\n            x = self.to_rgb(x)\n        if self.ema_scope is not None:\n            with self.ema_scope:\n                x_rec = self.model(x, **kwargs)\n        else:\n            x_rec = self.model(x, **kwargs)\n        if self.image_key == \"segmentation\":\n            x_rec = self.to_rgb(x_rec)\n        x = x.cpu()\n        x_rec = x_rec.cpu()\n        if not only_inputs:\n            self.logger.experiment.add_images(f\"{self.image_key}_reconstruction\", x_rec, self.global_step, dataformats=\"NCHW\")\n        self.logger.experiment.add_images(", "CodeBleu_score": 33.386080500676016}, {"code": "def log_images(self, batch, only_inputs=False, plot_ema=False, **kwargs)\n\t    if not self.log_images:\n        return\n    if self.image_key is None:\n        return\n    if self.image_key not in batch:\n        return\n    if self.batch_resize_range is not None:\n        batch = batch.detach()\n    if self.global_step % self.log_images_every_n_steps == 0:\n        if self.global_step == 0:\n            self.logger.log_text(f\"Logging images at step {self.global_step}...\", color=\"blue\")\n        if self.image_key == \"segmentation\":\n            if not only_inputs:\n                self.logger.log_segmentation_images(batch, self.image_key, **kwargs)\n            else:\n                self.logger.log_segmentation_images(batch, self.image_key, only_inputs=True, **kwargs)\n        else:\n            if not only_inputs:\n                self.logger.log_images(batch, self.image_key, **kwargs)\n            else:\n                self.logger.log_images(batch, self.image_key, only_inputs=True, **kwargs)\n        if plot_ema:\n            if self.ema_scope is not None:\n                with self.ema_scope:\n                    if self.image_key == \"segmentation\":\n                        if not only_inputs:\n                            self.logger.log_segmentation_images(batch, self.image_key, only_ema=True, **kwargs)\n                        else:\n                            self.logger.log_segmentation_images(batch, self.image_key, only_inputs=True, only_ema=True, **kwargs)\n                    else:\n                        if not only_inputs:\n                            self.logger.log_images(batch, self.image_key, only_ema=True, **kwargs)\n                        else:\n                            self.logger.log_images(batch, self.image_key, only_inputs=True, only_ema=True, **kwargs)\n\n", "CodeBleu_score": 19.004594868803284}, {"code": "def log_images(self, batch, only_inputs=False, plot_ema=False, **kwargs)\n\t    if not hasattr(self, \"image_key\"):\n        self.image_key = \"image\"\n    if not hasattr(self, \"image_key_ema\"):\n        self.image_key_ema = f\"{self.image_key}_ema\"\n    if self.image_key not in batch:\n        return\n    if self.image_key_ema not in batch:\n        batch[self.image_key_ema] = batch[self.image_key]\n    if self.image_key == \"segmentation\":\n        batch[self.image_key] = self.to_rgb(batch[self.image_key])\n        batch[self.image_key_ema] = self.to_rgb(batch[self.image_key_ema])\n    if only_inputs:\n        batch = {self.image_key: batch[self.image_key]}\n    if plot_ema:\n        batch[self.image_key_ema] = batch[self.image_key_ema].detach()\n    self.log_dict(batch, **kwargs)\n\n", "CodeBleu_score": 24.163456993905335}]}
{"instance_id": "540d089e3481", "generation_result": [{"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss == 'l1':\n        loss = torch.abs(pred - target)\n        if mean:\n            return loss.mean()\n        else:\n            return loss\n    elif self.loss == 'l2':\n        loss = torch.pow(pred - target, 2)\n        if mean:\n            return loss.mean()\n        else:\n            return loss\n    else:\n        raise NotImplementedError\n", "CodeBleu_score": 39.46249004308592}, {"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss == 'l1':\n        loss = F.l1_loss(pred, target, reduction='none')\n        if mean:\n            loss = loss.mean()\n    elif self.loss == 'l2':\n        loss = F.mse_loss(pred, target, reduction='none')\n        if mean:\n            loss = loss.mean()\n    else:\n        raise NotImplementedError\n    return loss\n\n", "CodeBleu_score": 49.866180403462565}, {"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss_type == 'l1':\n        loss = abs(pred - target)\n        if mean:\n            return loss.mean()\n        return loss\n    elif self.loss_type == 'l2':\n        loss = (pred - target) ** 2\n        if mean:\n            return loss.mean()\n        return loss\n    else:\n        raise NotImplementedError", "CodeBleu_score": 37.51413242962213}]}
{"instance_id": "63a08283666f", "generation_result": [{"code": "def forward(self, x, c, *args, **kwargs)\n\t\n    # If no timesteps are provided, sample a random timestep for every example in the batch\n    if \"t\" not in kwargs:\n        t = extract_into_tensor(self.timesteps, x.shape[0], device=x.device)\n    else:\n        t = kwargs[\"t\"]\n    # If conditioning is required by the model, apply learned conditioning\n    if self.has_learned_conditioning:\n        c = self.get_learned_conditioning(c)\n    # Process the inputs based on the model's conditioning requirements\n    if self.has_cross_attention:\n        x = self.process_inputs(x, c, t)\n    else:\n        x = self.process_inputs(x, t)\n    # Compute and return the loss for the given inputs and conditioning\n    loss, loss_dict = self.p_losses(x, t)\n    return loss, loss_dict\n\n", "CodeBleu_score": 36.31678409811138}, {"code": "def forward(self, x, c, *args, **kwargs)\n\t\n    if self.training:\n        t = self.timesteps.sample()\n    else:\n        t = self.timesteps.get_indices(0, 1)\n\n    if self.learned_conditioning:\n        c = self.get_learned_conditioning(c)\n\n    if self.conditioning_in_forward:\n        x = self.model(x, t, c)\n    else:\n        x = self.model(x, t)\n\n    if self.training:\n        loss, loss_dict = self.p_losses(x, t, c)\n    else:\n        loss, loss_dict = None, None\n\n    return x, loss, loss_dict\n", "CodeBleu_score": 36.422052516733295}, {"code": "def forward(self, x, c, *args, **kwargs)\n\t\n    c = default(c, lambda: torch.zeros(x.shape[0], self.cond_stage_model.cond_dim, device=x.device))\n    t = self.get_time_steps(x.shape[0])\n    loss, loss_dict = self.p_losses(x, t, c=c)\n    return loss, loss_dict\n", "CodeBleu_score": 16.757384154484125}]}
{"instance_id": "4627e1e8ed62", "generation_result": [{"code": "def get_codebook_entry(self, indices, shape)\n\t\n    indices = self.unmap_to_all(indices)\n    indices = indices.reshape(shape[0], -1)\n    indices = indices.view(-1)\n    z_q = self.embedding(indices)\n    z_q = z_q.reshape(shape)\n    z_q = z_q.permute(0, 2, 1, 3, 4).contiguous()\n    return z_q\n\n\n", "CodeBleu_score": 29.8370893535973}, {"code": "def get_codebook_entry(self, indices, shape)\n\t\n    if self.unmap_to_all:\n        indices = self.unmap_to_all(indices)\n    z_q = self.embedding(indices).view(shape)\n    if self.permute_dim is not None:\n        z_q = z_q.permute(self.permute_dim)\n    return z_q.contiguous()\n\n\n", "CodeBleu_score": 37.801915333383626}, {"code": "def get_codebook_entry(self, indices, shape)\n\t\n    indices = indices.reshape(-1)\n    if self.unmap_to_all:\n        indices = self.unmap_to_all(indices)\n    z_q = self.embedding(indices)\n    if self.reshape_to_input:\n        z_q = z_q.view(shape)\n    if self.permute_to_input:\n        z_q = z_q.permute(self.permute_to_input)\n    if self.contiguous:\n        z_q = z_q.contiguous()\n    return z_q\n\n", "CodeBleu_score": 44.572883881349114}]}
{"instance_id": "c55d014827ef", "generation_result": [{"code": "def factorization(dimension: int, factor:int=-1) -> tuple[int, int]\n\t\n    if factor > 0 and dimension % factor == 0:\n        return (factor, dimension // factor)\n    if factor < 0:\n        factor = -factor\n    for m in range(1, factor+1):\n        if dimension % m == 0:\n            n = dimension // m\n            if m <= n:\n                return (m, n)\n    return (1, dimension)\n\n", "CodeBleu_score": 30.854805868556355}, {"code": "def factorization(dimension: int, factor:int=-1) -> tuple[int, int]\n\t\n    if factor > 0:\n        if dimension % factor == 0:\n            return (factor, dimension // factor)\n        else:\n            raise ValueError(\"The factor does not divide the dimension.\")\n    else:\n        factor = 1\n        for i in range(2, int(math.sqrt(dimension)) + 1):\n            if dimension % i == 0:\n                factor = i\n                break\n        return (factor, dimension // factor)\n", "CodeBleu_score": 28.94684213088575}, {"code": "def factorization(dimension: int, factor:int=-1) -> tuple[int, int]\n\t\n    if factor > 0:\n        if dimension % factor == 0:\n            return (factor, dimension // factor)\n        else:\n            return (1, dimension)\n    else:\n        m = 1\n        n = dimension\n        for i in range(1, int(math.sqrt(dimension)) + 1):\n            if dimension % i == 0:\n                m = i\n                n = dimension // i\n                if m > n:\n                    m, n = n, m\n        return (m, n)\n\n\n", "CodeBleu_score": 41.303142060609034}]}
{"instance_id": "5d613ec26036", "generation_result": [{"code": "def apply_weight_decompose(self, updown, orig_weight)\n\t\n    # Move tensors to the same device and data type\n    updown = updown.to(orig_weight.device, orig_weight.dtype)\n    orig_weight = orig_weight.to(updown.device, updown.dtype)\n\n    # Combine and normalize the weights\n    combined_weight = (updown + orig_weight).norm(dim=1, keepdim=True)\n\n    # Scale and adjust the weights\n    scale = self.scale_factor.to(combined_weight.device, combined_weight.dtype)\n    adjusted_weight = scale * combined_weight - orig_weight\n\n    return adjusted_weight\n", "CodeBleu_score": 14.177904071111639}, {"code": "def apply_weight_decompose(self, updown, orig_weight)\n\t\n    if updown == 1:\n        weight = orig_weight.to(self.weight.device) + self.weight.to(orig_weight.device)\n        weight = self.norm(weight)\n        weight = weight * self.scale1\n    else:\n        weight = orig_weight.to(self.weight.device) - self.weight.to(orig_weight.device)\n        weight = self.norm(weight)\n        weight = weight * self.scale2\n    return weight\n", "CodeBleu_score": 19.75075478490789}, {"code": "def apply_weight_decompose(self, updown, orig_weight)\n\t\n    # get the device and dtype of the original weight\n    device = orig_weight.device\n    dtype = orig_weight.dtype\n\n    # move the tensors to the same device and convert to the same dtype\n    updown = updown.to(device, dtype)\n    orig_weight = orig_weight.to(device, dtype)\n\n    # combine the input weight with the original weight\n    combined_weight = updown + orig_weight\n\n    # normalize the combined weight\n    combined_weight = F.normalize(combined_weight, p=2, dim=1)\n\n    # scale the combined weight\n    scale = self.scale_factor\n    scaled_weight = combined_weight * scale\n\n    # calculate the final adjusted weight\n    adjusted_weight = scaled_weight - orig_weight\n\n    return adjusted_weight\n", "CodeBleu_score": 15.859550443259995}]}
{"instance_id": "c73d0d1cd3e5", "generation_result": [{"code": "def finalize_updown(self, updown, orig_weight, output_shape, ex_bias=None)\n\t # Apply bias\n    if ex_bias is not None:\n        updown = updown + ex_bias\n\n    # Adjust shapes\n    if updown.size!= orig_weight.size:\n        updown = updown.reshape(output_shape)\n\n    # Apply multiplier\n    if ex_bias is not None:\n        ex_bias = ex_bias * self.multiplier\n\n    # Apply weight decomposition\n    if self.apply_weight_decompose:\n        updown = self.apply_weight_decompose(updown, orig_weight)\n\n    return updown, ex_bias\n", "CodeBleu_score": 36.433065012498766}, {"code": "def finalize_updown(self, updown, orig_weight, output_shape, ex_bias=None)\n\t if ex_bias is not None:\n        updown = updown + ex_bias\n    updown = updown.reshape(output_shape)\n    if updown.numel() == orig_weight.numel():\n        updown = updown.reshape(orig_weight.shape)\n    if ex_bias is not None:\n        ex_bias = self.multiplier * ex_bias\n    if self.scale is not None:\n        updown = self.apply_weight_decompose(updown, orig_weight)\n    return updown, ex_bias\n", "CodeBleu_score": 49.43516490228105}, {"code": "def finalize_updown(self, updown, orig_weight, output_shape, ex_bias=None)\n\t updown = updown + ex_bias\n    updown = updown.reshape(output_shape)\n    if updown.numel() == orig_weight.numel():\n        updown = updown.reshape(orig_weight.shape)\n    if ex_bias is not None:\n        ex_bias = ex_bias * self.multiplier\n    if self.scale is not None:\n        updown = self.apply_weight_decompose(updown, orig_weight)\n    return updown, ex_bias\n", "CodeBleu_score": 46.21513744772067}]}
{"instance_id": "c6a41665c62c", "generation_result": [{"code": "def calc_updown(self, orig_weight)\n\t\n    if self.bias is not None:\n        ex_bias = self.bias.to(orig_weight.device, dtype=orig_weight.dtype)\n    else:\n        ex_bias = None\n    updown = self.calc_updown_core(orig_weight)\n    return self.finalize_updown(updown, orig_weight, orig_weight.shape, ex_bias)\n\n", "CodeBleu_score": 49.84867807447056}, {"code": "def calc_updown(self, orig_weight)\n\t\n        updown = self.current_weight.to(orig_weight.device, dtype=orig_weight.dtype)\n        if self.bias is not None:\n            ex_bias = self.bias.to(orig_weight.device, dtype=orig_weight.dtype)\n        else:\n            ex_bias = None\n        output_shape = self.current_weight.shape\n        updown, ex_bias = self.finalize_updown(updown, orig_weight, output_shape, ex_bias)\n        return updown, ex_bias\n\n", "CodeBleu_score": 50.31115406688114}, {"code": "def calc_updown(self, orig_weight)\n\t\n        updown = self.calc_updown_base(orig_weight)\n        return self.finalize_updown(updown, orig_weight, orig_weight.shape)\n", "CodeBleu_score": 12.465438720876435}]}
{"instance_id": "1bb9e5d94af8", "generation_result": [{"code": "def purge_networks_from_memory()\n\t\n    global networks_in_memory, network_count_limit\n    if len(networks_in_memory) > network_count_limit:\n        networks_in_memory.popitem()\n        gc.collect()\n\n", "CodeBleu_score": 26.84971261485928}, {"code": "def purge_networks_from_memory()\n\t\n    if len(networks_in_memory) > network_memory_limit:\n        for i in range(len(networks_in_memory) - network_memory_limit):\n            networks_in_memory.pop(0)\n            gc.collect()\n", "CodeBleu_score": 36.71319703833508}, {"code": "def purge_networks_from_memory()\n\t\n    global networks_in_memory\n    if len(networks_in_memory) > max_networks_in_memory:\n        for key in list(networks_in_memory.keys()):\n            networks_in_memory.pop(key)\n            gc.collect()\n            if len(networks_in_memory) < max_networks_in_memory:\n                break\n\n", "CodeBleu_score": 42.18374279181}]}
{"instance_id": "42b48ebf4d07", "generation_result": [{"code": "def network_restore_weights_from_backup(self: Union[torch.nn.Conv2d, torch.nn.Linear, torch.nn.GroupNorm, torch.nn.LayerNorm, torch.nn.MultiheadAttention]):\n\t\n    if hasattr(self, \"weight_backup\") and hasattr(self, \"bias_backup\"):\n        if isinstance(self, torch.nn.MultiheadAttention):\n            self.in_proj_weight = self.weight_backup\n            self.in_proj_bias = self.bias_backup\n        else:\n            self.weight = self.weight_backup\n            self.bias = self.bias_backup\n    else:\n        print(\"No backup found for layer: {}\".format(self))\n\n", "CodeBleu_score": 27.198831467876435}, {"code": "def network_restore_weights_from_backup(self: Union[torch.nn.Conv2d, torch.nn.Linear, torch.nn.GroupNorm, torch.nn.LayerNorm, torch.nn.MultiheadAttention]):\n\t\n    if hasattr(self, 'weight_backup') and hasattr(self, 'bias_backup'):\n        if isinstance(self, torch.nn.MultiheadAttention):\n            if hasattr(self, 'in_proj_weight_backup') and hasattr(self, 'in_proj_bias_backup'):\n                self.in_proj_weight = copy_(self.in_proj_weight_backup)\n                self.in_proj_bias = copy_(self.in_proj_bias_backup)\n            else:\n                print(\"Warning: Backup weights and biases for MultiheadAttention projections not found.\")\n        self.weight = copy_(self.weight_backup)\n        if hasattr(self, 'bias_backup'):\n            self.bias = copy_(self.bias_backup)\n        else:\n            self.bias = None\n    else:\n        print(\"Warning: Backup weights and biases not found.\")\n", "CodeBleu_score": 40.57801273663666}, {"code": "def network_restore_weights_from_backup(self: Union[torch.nn.Conv2d, torch.nn.Linear, torch.nn.GroupNorm, torch.nn.LayerNorm, torch.nn.MultiheadAttention]):\n\t\n    if hasattr(self, 'weight_backup') and hasattr(self, 'bias_backup'):\n        if isinstance(self, torch.nn.MultiheadAttention):\n            self.in_proj_weight.data = copy_(self.weight_backup['in_proj_weight'])\n            self.in_proj_bias.data = copy_(self.bias_backup['in_proj_bias'])\n            self.out_proj.weight.data = copy_(self.weight_backup['out_proj_weight'])\n            self.out_proj.bias.data = copy_(self.bias_backup['out_proj_bias'])\n        else:\n            self.weight.data = copy_(self.weight_backup)\n            if self.bias is not None:\n                self.bias.data = copy_(self.bias_backup)\n    else:\n        print(f\"No backup weights and biases found for layer {self.__class__.__name__}.\")", "CodeBleu_score": 38.62749280224666}]}
{"instance_id": "c8fcd357131a", "generation_result": [{"code": "def network_forward(org_module, input, original_forward)\n\t\n    if not hasattr(org_module, \"networks\"):\n        return original_forward(org_module, input)\n\n    input = input.to(org_module.networks[0].device)\n\n    org_module.network_restore_weights_from_backup()\n    org_module.network_reset_cached_weight()\n\n    output = original_forward(org_module, input)\n\n    for network in org_module.networks:\n        if network.network_current_names == org_module.network_current_names:\n            output = network(output)\n\n    return output\n", "CodeBleu_score": 41.80324837809961}, {"code": "def network_forward(org_module, input, original_forward)\n\t\n    if not hasattr(org_module, \"network_current_names\"):\n        org_module.network_current_names = ()\n    if not hasattr(org_module, \"network_weights_backup\"):\n        org_module.network_weights_backup = None\n    if not hasattr(org_module, \"network_bias_backup\"):\n        org_module.network_bias_backup = None\n    if not hasattr(org_module, \"network_reset_cached_weight\"):\n        org_module.network_reset_cached_weight = network_reset_cached_weight\n    if not hasattr(org_module, \"network_restore_weights_from_backup\"):\n        org_module.network_restore_weights_from_backup = network_restore_weights_from_backup\n    if not hasattr(org_module, \"network_cast_to_device\"):\n        org_module.network_cast_to_device = network_cast_to_device\n    if not hasattr(org_module, \"network_apply_network\"):\n        org_module.network_apply_network = network_apply_network\n    if not hasattr(org_module, \"network_apply_networks\"):\n        org_module.network_apply_networks = network_apply_networks\n    if not hasattr(org_module, \"network_apply_networks_to_output\"):\n        org_module.network_apply_networks_to_output = network_apply_networks_to_output\n    if not hasattr(org_module, \"network_apply_networks_to_output_with_names\"):\n        org_module.network_apply_networks_to_output_with_names = network_apply_networks_to_output_with_names\n    if not hasattr(org_module, \"network_apply_networks_to_output_with_names_and_args\"):\n        org_module.network_apply_networks_to_output_with_names_and_args = network_apply_networks_to_output_with_names_and_args\n    if not hasattr(org_module, \"network_apply_networks_to_output_with_names_and_args_and_kwargs\"):\n        org_module.network_apply_networks_to_output_with_names_and_args_and_kwargs = network_apply_networks_to_output_with_names_and_args", "CodeBleu_score": 15.1470660548746}, {"code": "def network_forward(org_module, input, original_forward)\n\t\n    if not hasattr(org_module, \"network_weights_backup\"):\n        return original_forward(org_module, input)\n\n    # Cast to appropriate device\n    input = input.to(org_module.network_weights_backup.device)\n\n    # Restore weights\n    org_module.network_restore_weights_from_backup()\n    org_module.network_reset_cached_weight()\n\n    # Forward\n    output = original_forward(org_module, input)\n\n    # Apply loaded networks\n    for network in org_module.networks:\n        if network.name in org_module.network_current_names:\n            output = network.forward(output)\n\n    return output\n", "CodeBleu_score": 42.116362725166034}]}
{"instance_id": "1070211f5c11", "generation_result": [{"code": "def trainables(self)\n\t\n    return [p for p in self.parameters() if p.requires_grad]\n\n\n", "CodeBleu_score": 6.452447410697191}, {"code": "def trainables(self)\n\t\n    trainables = []\n    for layer in self.model.children():\n        if isinstance(layer, torch.nn.Linear) or isinstance(layer, torch.nn.LayerNorm):\n            trainables.extend([p for p in layer.parameters() if p.requires_grad])\n    return trainables\n\n\n\n\n", "CodeBleu_score": 36.077433644852235}, {"code": "def trainables(self)\n\t\n    trainables = []\n    for layer in self.layers:\n        if isinstance(layer, torch.nn.Linear):\n            trainables.append(layer.weight)\n            trainables.append(layer.bias)\n        elif isinstance(layer, torch.nn.LayerNorm):\n            trainables.append(layer.weight)\n            trainables.append(layer.bias)\n    return trainables\n\n\n\n\n", "CodeBleu_score": 35.29591016900989}]}
{"instance_id": "0c3ba62c96a7", "generation_result": [{"code": "def load_hypernetworks(names, multipliers=None)\n\t\n    already_loaded = shared.hypernetworks\n    already_loaded.clear()\n    multipliers = multipliers or [1] * len(names)\n\n    for name, multiplier in zip(names, multipliers):\n        hypernetwork = load_hypernetwork(name)\n\n        if hypernetwork is not None:\n            hypernetwork.set_multiplier(multiplier)\n            already_loaded.append(hypernetwork)\n\n", "CodeBleu_score": 24.26323046933527}, {"code": "def load_hypernetworks(names, multipliers=None)\n\t\n    already_loaded = shared.hypernetworks.get\n    hypernetwork = load_hypernetwork\n    clear\n    enumerate\n\n    load_hypernetworks: def load_hypernetworks(names, multipliers=None):\n        already_loaded = shared.hypernetworks.get\n        hypernetwork = load_hypernetwork\n        clear\n        enumerate\n\n        already_loaded = shared.hypernetworks.get\n        for name in names:\n            hypernetwork = already_loaded.get(name, None)\n\n            if hypernetwork is None:\n                hypernetwork = load_hypernetwork(name)\n\n                if hypernetwork is None:\n                    continue\n\n                if multipliers is not None:\n                    hypernetwork.set_multiplier(multipliers[name])\n\n                shared.hypernetworks[name] = hypernetwork\n\n        return shared.hypernetworks.values()\n    hypernetwork.set_multiplier: def set_multiplier(self, multiplier):\n        for layers in self.layers.values():\n            for layer in layers:\n                layer.multiplier = multiplier\n\n        return self\n\n", "CodeBleu_score": 46.87750838382654}, {"code": "def load_hypernetworks(names, multipliers=None)\n\t\n    already_loaded = shared.hypernetworks.get\n    hypernetwork = load_hypernetwork\n    clear = shared.hypernetworks.clear\n    enumerate = enumerate\n\n    clear()\n\n    for name, multiplier in zip(names, multipliers):\n        loaded = already_loaded.get(name, None)\n\n        if loaded is None:\n            loaded = hypernetwork(name)\n\n            if loaded is None:\n                continue\n\n        if multiplier is not None:\n            loaded.set_multiplier(multiplier)\n\n        shared.hypernetworks[name] = loaded\n\n", "CodeBleu_score": 34.55861828654548}]}
{"instance_id": "bea3772ed532", "generation_result": [{"code": "\ndef apply_hypernetworks(hypernetworks, context, layer=None)\n\t\n    context_k, context_v = context, context\n\n    for hypernetwork in hypernetworks:\n        context_k, context_v = apply_single_hypernetwork(hypernetwork, context_k, context_v, layer)\n\n    return context_k, context_v\n\n\n", "CodeBleu_score": 69.4402416707376}, {"code": "\ndef apply_hypernetworks(hypernetworks, context, layer=None)\n\t\n    context_k = context\n    context_v = context\n    for hypernetwork in hypernetworks:\n        context_k, context_v = apply_single_hypernetwork(hypernetwork, context_k, context_v, layer)\n    return context_k, context_v\n\n", "CodeBleu_score": 77.04946886953897}, {"code": "\ndef apply_hypernetworks(hypernetworks, context, layer=None)\n\t\n    context_k, context_v = context, context\n\n    for hypernetwork in hypernetworks:\n        context_k, context_v = apply_single_hypernetwork(hypernetwork, context_k, context_v, layer)\n\n    return context_k, context_v", "CodeBleu_score": 69.4402416707376}]}
{"instance_id": "922efa3e092f", "generation_result": [{"code": "def attention_CrossAttention_forward(self, x, context=None, mask=None, **kwargs)\n\t\n    if context is None:\n        context = x\n    query = self.to_q(x)\n    key = self.to_k(context)\n    value = self.to_v(context)\n    if self.hypernetworks is not None:\n        key, value = apply_hypernetworks(self.hypernetworks, context, layer=self.layer)\n    query = self.split_heads(query)\n    key = self.split_heads(key)\n    value = self.split_heads(value)\n    a = torch.matmul(query, key.transpose(-1, -2))\n    a = a / math.sqrt(query.size(-1))\n    if mask is not None:\n        a = a.masked_fill(mask[:, None, None, :], -1e10)\n    a = self.softmax(a)\n    a = torch.matmul(a, value)\n    a = self.merge_heads(a)\n    a = self.to_out(a)\n    return a\n\n", "CodeBleu_score": 35.99425153923168}, {"code": "def attention_CrossAttention_forward(self, x, context=None, mask=None, **kwargs)\n\t\n    self.to_q(x)\n    self.to_k(context)\n    self.to_v(context)\n    self.to_out(x)\n    self.to_k\n    self.to_v\n    self.to_out\n    self.to_q\n    self.to_k\n    self.to_v\n    self.to_out\n    self.to_q\n    self.to_k\n    self.to_v\n    self.to_out\n    self.to_k\n    self.to_v\n    self.to_out\n    self.to_q\n    self.to_k\n    self.to_v\n    self.to_out\n    self.to_k\n    self.to_v\n    self.to_out\n    self.to_q\n    self.to_k\n    self.to_v\n    self.to_out\n    self.to_k\n    self.to_v\n    self.to_out\n    self.to_q\n    self.to_k\n    self.to_v\n    self.to_out\n    self.to_k\n    self.to_v\n    self.to_out\n    self.to_q\n    self.to_k\n    self.to_v\n    self.to_out\n    self.to_k\n    self.to_v\n    self.to_out\n    self.to_q\n    self.to_k\n    self.to_v\n    self.to_out\n    self.to_k\n    self.to_v\n    self.to_out\n    self.to_q\n    self.to_k\n    self.to_v\n    self.to_out\n    self.to_k\n    self.to_v\n    self.to_out\n    self.to_q\n    self.to_k\n    self.to_v\n    self.to_out\n    self.to_k\n    self.to_v\n    self.to_out\n    self.to_q\n    self.to_k\n    self.to_v\n    self.to_out\n    self.to_k\n    self.to_v\n    self.to_out\n    self.to_q\n    self.to_k\n    self.to_v\n    self.to_out\n    self.to_k\n    self.to_v\n    self.to_out\n    self.to_q\n    self.to_k\n    self", "CodeBleu_score": 7.862728707684818}, {"code": "def attention_CrossAttention_forward(self, x, context=None, mask=None, **kwargs)\n\t\n    if context is None:\n        context = x\n    if self.qkv_same_dim:\n        q, k, v = self.to_qkv(x).chunk(3, dim=-1)\n    else:\n        q, k, v = self.to_q(x), self.to_k(context), self.to_v(context)\n    q = q * self.scale\n    if self.relative_position_embedding is not None:\n        q = q + self.relative_position_embedding(x)\n    q = self.q_proj(q)\n    k = self.k_proj(k)\n    v = self.v_proj(v)\n    q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=self.num_heads), (q, k, v))\n    if self.hypernetworks is not None:\n        k, v = apply_hypernetworks(self.hypernetworks, context, layer=self.layer)\n    sim = einsum('b h i d, b h j d -> b h i j', q, k)\n    if mask is not None:\n        mask = rearrange(mask, 'b j -> b () j')\n        sim.masked_fill_(~mask, -torch.finfo(sim.dtype).max)\n        del mask\n    attn = sim.softmax(dim=-1)\n    attn = self.attn_drop(attn)\n    x = einsum('b h i j, b h j d -> b h i d', attn, v)\n    x = rearrange(x, 'b h n d -> b n (h d)')\n    x = self.proj(x)\n    x = self.proj_drop(x)\n    return x\n\n", "CodeBleu_score": 53.00193385798737}]}
{"instance_id": "8b7455a03a0e", "generation_result": [{"code": "def save_hypernetwork(hypernetwork, checkpoint, hypernetwork_name, filename)\n\t    hypernetwork.backup_attributes()\n    hypernetwork.name = hypernetwork_name\n    hypernetwork.sd_checkpoint = checkpoint\n    hypernetwork.sd_checkpoint_name = filename\n    try:\n        hypernetwork.save(filename)\n    except Exception as e:\n        hypernetwork.restore_attributes()\n        raise e\n    hypernetwork.restore_attributes()\n", "CodeBleu_score": 13.544633979140047}, {"code": "def save_hypernetwork(hypernetwork, checkpoint, hypernetwork_name, filename)\n\t    try:\n        hypernetwork.save(filename)\n    except Exception as e:\n        hypernetwork.name = hypernetwork_name\n        hypernetwork.checkpoint = checkpoint\n        raise e\n\n", "CodeBleu_score": 18.934850520283675}, {"code": "def save_hypernetwork(hypernetwork, checkpoint, hypernetwork_name, filename)\n\t    # TODO: add a check to make sure the hypernetwork is not already in a checkpoint\n    original_hypernetwork_name = hypernetwork.name\n    original_hypernetwork_checkpoint = hypernetwork.checkpoint\n    original_hypernetwork_checkpoint_name = hypernetwork.checkpoint_name\n    original_hypernetwork_sd_checkpoint = hypernetwork.sd_checkpoint\n    original_hypernetwork_sd_checkpoint_name = hypernetwork.sd_checkpoint_name\n    original_hypernetwork_optimizer_name = hypernetwork.optimizer_name\n    original_hypernetwork_optimizer_state_dict = hypernetwork.optimizer_state_dict\n    original_hypernetwork_step = hypernetwork.step\n\n    hypernetwork.name = hypernetwork_name\n    hypernetwork.checkpoint = checkpoint\n    hypernetwork.checkpoint_name = filename\n    hypernetwork.sd_checkpoint = None\n    hypernetwork.sd_checkpoint_name = None\n    hypernetwork.optimizer_name = None\n    hypernetwork.optimizer_state_dict = None\n    hypernetwork.step = 0\n\n    try:\n        hypernetwork.save(filename)\n    except Exception as e:\n        hypernetwork.name = original_hypernetwork_name\n        hypernetwork.checkpoint = original_hypernetwork_checkpoint\n        hypernetwork.checkpoint_name = original_hypernetwork_checkpoint_name\n        hypernetwork.sd_checkpoint = original_hypernetwork_sd_checkpoint\n        hypernetwork.sd_checkpoint_name = original_hypernetwork_sd_checkpoint_name\n        hypernetwork.optimizer_name = original_hypernetwork_optimizer_name\n        hypernetwork.optimizer_state_dict = original_hypernetwork_optimizer_state_dict\n        hypernetwork.step = original_hypernetwork_step\n        raise e\n\n", "CodeBleu_score": 33.59217969115095}]}
{"instance_id": "3175f835140f", "generation_result": [{"code": "def ema_scope(self, context=None)\n\t\n    if context is None:\n        context = {}\n    if self.ema_decay > 0.0:\n        context[\"ema_backup\"] = self.get_parameters()\n        self.copy_to(self.ema_model)\n        if self.ema_logging:\n            self.log(\"Switching to EMA weights for evaluation\")\n    return context\n\n", "CodeBleu_score": 28.68581197600102}, {"code": "def ema_scope(self, context=None)\n\t\n    if not self.ema_on:\n        return context\n    if context is None:\n        context = {}\n    if self.ema_on:\n        if self.ema_on and self.ema_on.lower() == 'true':\n            self.logger.info('EMA is enabled. Storing current model parameters and copying EMA weights to the model.')\n            context['ema_scope'] = True\n            context['ema_params'] = self.get_parameters()\n            self.load_parameters(self.ema_params)\n        else:\n            self.logger.info('EMA is disabled. Skipping.')\n    return context\n\n", "CodeBleu_score": 39.17636775554752}, {"code": "def ema_scope(self, context=None)\n\t\n    if not self.ema_on:\n        return context\n    if context is None:\n        context = {}\n    if self.ema_on and self.ema_decay > 0:\n        if self.ema_on_start:\n            self.store_model_parameters()\n        if self.ema_on_start_print:\n            self.print_model_parameters()\n        self.copy_to(self.ema_decay)\n        if self.ema_on_start_print:\n            self.print_model_parameters()\n    return context\n\n", "CodeBleu_score": 38.372420135110595}]}
{"instance_id": "a7b5d5cd519a", "generation_result": [{"code": "def p_mean_variance(self, x, t, clip_denoised: bool)\n\t\n    model_output = self.model(x, t)\n    if self.predict_start_from_noise:\n        model_output = self.predict_start_from_noise(x, t, model_output)\n    if clip_denoised:\n        model_output = model_output.clamp(-1, 1)\n    model_mean, posterior_log_variance = model_output.chunk(2, dim=-1)\n    posterior_variance = th.exp(posterior_log_variance)\n    return model_mean, posterior_variance, posterior_log_variance\n", "CodeBleu_score": 42.015333907073305}, {"code": "def p_mean_variance(self, x, t, clip_denoised: bool)\n\t\n    model_output = self.model(x, t)\n    if self.predict_start_from_noise:\n        # this is the case where we predict x0 from noise\n        # we can use the closed form expression for the mean here\n        # (this is the 'denoised' sample)\n        x_recon = self.predict_start_from_noise(x, t, model_output)\n    else:\n        # this is the case where we predict x_t from x_t-1\n        # in this case we need to sample from the model output\n        noise = torch.randn_like(x)\n        x_recon = self.predict_start_from_noise(x, t, noise)\n\n    if clip_denoised:\n        x_recon.clamp_(-1, 1)\n\n    model_mean, posterior_variance, posterior_log_variance = self.q_posterior(x_recon, x, t)\n    return model_mean, posterior_variance, posterior_log_variance\n", "CodeBleu_score": 45.4354750990769}, {"code": "def p_mean_variance(self, x, t, clip_denoised: bool)\n\t\n    model_output = self.model(x, t)\n    if self.predict_start_from_noise:\n        # If predicting from noise, we need to sample from the base noise distribution\n        # Note that this is different from the paper where the base noise is always sampled from a Normal(0, I)\n        # We found that sampling from a Normal(0, I) for every timestep can lead to instability\n        if t == 0:\n            raise ValueError(\"predict_start_from_noise cannot be used at t=0\")\n        x_recon = self.predict_start_from_noise(x, t, model_output)\n    else:\n        # Otherwise, we predict x_recon directly from the model output\n        x_recon = model_output\n    if clip_denoised:\n        x_recon.clamp_(-1, 1)\n    model_mean, posterior_variance, posterior_log_variance = self.q_posterior(x_start=x_recon, x_t=x, t=t)\n    return model_mean, posterior_variance, posterior_log_variance\n\n", "CodeBleu_score": 55.234671405549186}]}
{"instance_id": "a8652fa78473", "generation_result": [{"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss_type == 'l1':\n        loss = abs(pred - target)\n        if mean:\n            return loss.mean()\n        else:\n            return loss\n    elif self.loss_type == 'l2':\n        loss = mse_loss(pred, target, reduction='none')\n        if mean:\n            return loss.mean()\n        else:\n            return loss\n    else:\n        raise NotImplementedError('Only l1 and l2 losses are supported.')\n", "CodeBleu_score": 46.83095782340342}, {"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss_type == 'l1':\n        loss = abs(pred - target)\n        if mean:\n            return loss.mean()\n        else:\n            return loss\n    elif self.loss_type == 'l2':\n        loss = mse_loss(pred, target, reduction='none')\n        if mean:\n            return loss.mean()\n        else:\n            return loss\n    else:\n        raise NotImplementedError\n", "CodeBleu_score": 44.789673870452795}, {"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss == 'l1':\n        if mean:\n            return abs(pred - target).mean()\n        else:\n            return abs(pred - target)\n    elif self.loss == 'l2':\n        if mean:\n            return mse_loss(pred, target, reduction='mean')\n        else:\n            return mse_loss(pred, target, reduction='none')\n    else:\n        raise NotImplementedError\n", "CodeBleu_score": 33.98662384827147}]}
{"instance_id": "d68e4f061349", "generation_result": [{"code": "def _prior_bpd(self, x_start)\n\t    batch_size = x_start.shape[0]\n    t = torch.full((batch_size,), self.num_timesteps - 1, dtype=x_start.dtype, device=x_start.device)\n    mean, variance, log_variance = self.q_mean_variance(x_start, t)\n    kl = normal_kl(mean, variance, log_variance)\n    return kl.mean() / np.log(2)\n", "CodeBleu_score": 31.691158349008177}, {"code": "def _prior_bpd(self, x_start)\n\t    batch_size = x_start.shape[0]\n    t = torch.full((batch_size,), self.num_timesteps - 1, device=x_start.device, dtype=x_start.dtype)\n    mean, _, log_variance = self.q_mean_variance(x_start, t)\n    kl = normal_kl(mean, log_variance)\n    kl = kl.sum(dim=-1, keepdim=True)\n    kl = kl / math.log(2)\n    kl = kl / np.prod(x_start.shape[1:])\n    return kl.mean()\n\n", "CodeBleu_score": 31.889681229229684}, {"code": "def _prior_bpd(self, x_start)\n\t    batch_size = x_start.shape[0]\n    t = torch.full((batch_size,), self.num_timesteps - 1, device=self.device, dtype=torch.long)\n    mean, _, log_variance = self.q_mean_variance(x_start, t)\n    return torch.mean(normal_kl(mean, log_variance)) / np.log(2)\n\n", "CodeBleu_score": 30.061636933535716}]}
{"instance_id": "c695d93fdf2d", "generation_result": [{"code": "def make_cache(subsection: str) -> diskcache.Cache\n\t\n    return diskcache.Cache(\n        os.path.join(cache_dir, subsection),\n        size_limit=4 * 1024 ** 3,\n        disk_min_file_size=256 * 1024,\n    )\n\n", "CodeBleu_score": 66.24734394639475}, {"code": "def make_cache(subsection: str) -> diskcache.Cache\n\t\n    return diskcache.Cache(\n        directory=os.path.join(CACHE_DIR, subsection),\n        size_limit=4 * 1024 * 1024 * 1024,\n        disk_min_file_size=256 * 1024,\n    )\n\n", "CodeBleu_score": 57.71203416090174}, {"code": "def make_cache(subsection: str) -> diskcache.Cache\n\t\n    return diskcache.Cache(path=os.path.join(cache_dir, subsection), size_limit=4 * 1024 ** 3, disk_min_file_size=256 * 1024)\n\n", "CodeBleu_score": 52.85549654471169}]}
{"instance_id": "ed4c543f0d38", "generation_result": [{"code": "def cache(subsection)\n\t\n    cache_obj = caches.get(subsection)\n    if cache_obj is None:\n        with cache_lock:\n            if not os.path.isdir(cache_dir):\n                os.makedirs(cache_dir)\n            if not isfile(cache_filename):\n                convert_old_cached_data()\n            cache_obj = caches.get(subsection)\n            if cache_obj is None:\n                cache_obj = make_cache(subsection)\n                caches[subsection] = cache_obj\n    return cache_obj", "CodeBleu_score": 64.84610751288604}, {"code": "def cache(subsection)\n\t\n    global caches\n\n    cache_obj = caches.get(subsection)\n    if cache_obj is None:\n        with cache_lock:\n            if not os.path.isdir(cache_dir):\n                os.makedirs(cache_dir)\n            if not isfile(cache_filename):\n                convert_old_cached_data()\n\n            cache_obj = caches.get(subsection)\n            if cache_obj is None:\n                cache_obj = make_cache(subsection)\n                caches[subsection] = cache_obj\n\n    return cache_obj\n\n", "CodeBleu_score": 60.489198250694834}, {"code": "def cache(subsection)\n\t\n    cache_obj = caches.get(subsection)\n    if cache_obj is None:\n        with cache_lock:\n            if not os.path.exists(cache_dir):\n                os.makedirs(cache_dir)\n            if not isfile(cache_filename):\n                convert_old_cached_data()\n            cache_obj = caches.get(subsection)\n            if cache_obj is None:\n                cache_obj = make_cache(subsection)\n                caches[subsection] = cache_obj\n    return cache_obj\n", "CodeBleu_score": 65.03435690001642}]}
{"instance_id": "ccf5ea2d9fc1", "generation_result": [{"code": "def cached_data_for_file(subsection, title, filename, func)\n\t\n    cache_obj = cache(subsection)\n    mtime = getmtime(filename)\n    entry = cache_obj.get(title)\n    if entry and mtime < entry.get('mtime'):\n        entry = None\n    if not entry:\n        data = func()\n        cache_obj[title] = {\n           'mtime': mtime,\n            'data': data,\n        }\n        dump_cache()\n    else:\n        data = entry.get('data')\n    return data\n", "CodeBleu_score": 34.00503707774468}, {"code": "def cached_data_for_file(subsection, title, filename, func)\n\t\n\n    cache_obj = cache(subsection)\n\n    mtime = getmtime(filename)\n    cached_data = cache_obj.get(title)\n    if cached_data:\n        if cached_data.get('mtime') < mtime:\n            cache_obj.delete(title)\n            cached_data = None\n\n    if not cached_data:\n        cached_data = func()\n        cache_obj.set(title, {'mtime': mtime, 'data': cached_data})\n        dump_cache()\n\n    return cached_data\n", "CodeBleu_score": 30.339043511711754}, {"code": "def cached_data_for_file(subsection, title, filename, func)\n\t\n\n    cache_obj = cache(subsection)\n    mtime = getmtime(filename)\n    entry = cache_obj.get(title)\n    if not entry or mtime > entry.get('mtime'):\n        cache_obj.invalidate(title)\n        data = func()\n        cache_obj.set(title, data, mtime=mtime)\n        dump_cache()\n    else:\n        data = entry.get('value')\n\n    return data\n\n", "CodeBleu_score": 27.80750164564228}]}
{"instance_id": "e01a2d4f0ac2", "generation_result": [{"code": "def report(message: str, *, exc_info: bool = False) -> None\n\t\n    record_exception()\n    for line in message.splitlines():\n        print(f\"*** {line}\", file=sys.stderr)\n    if exc_info:\n        print(traceback.format_exc(), file=sys.stderr)\n        print(\"---\", file=sys.stderr)\n\n", "CodeBleu_score": 55.51980207626164}, {"code": "def report(message: str, *, exc_info: bool = False) -> None\n\t\n    record_exception()\n    for line in message.splitlines():\n        print(f\"*** {line}\", file=sys.stderr)\n    if exc_info:\n        traceback.print_exc(file=sys.stderr)\n        print(\"---\", file=sys.stderr)\n\n", "CodeBleu_score": 52.94417170818152}, {"code": "def report(message: str, *, exc_info: bool = False) -> None\n\t\n    record_exception()\n\n    print(message, file=sys.stderr)\n\n    if exc_info:\n        traceback.print_exc(file=sys.stderr)\n        print(\"---\", file=sys.stderr)\n\n", "CodeBleu_score": 39.704579696171}]}
{"instance_id": "125fb9d2320b", "generation_result": [{"code": "def extract_original_prompts(style: PromptStyle, prompt, negative_prompt)\n\t\n\n    if style.prompt == \"\" and style.negative_prompt == \"\":\n        return False, prompt, negative_prompt\n\n    if style.prompt:\n        prompt_found, prompt = extract_style_text_from_prompt(style.prompt, prompt)\n        if not prompt_found:\n            return False, prompt, negative_prompt\n\n    if style.negative_prompt:\n        negative_prompt_found, negative_prompt = extract_style_text_from_prompt(style.negative_prompt, negative_prompt)\n        if not negative_prompt_found:\n            return False, prompt, negative_prompt\n\n    return True, prompt, negative_prompt\n\n", "CodeBleu_score": 71.49169674776054}, {"code": "def extract_original_prompts(style: PromptStyle, prompt, negative_prompt)\n\t\n\n    if style.prompt is None and style.negative_prompt is None:\n        return False, prompt, negative_prompt\n\n    prompt_matched, prompt = extract_style_text_from_prompt(style.prompt, prompt)\n    negative_prompt_matched, negative_prompt = extract_style_text_from_prompt(style.negative_prompt, negative_prompt)\n\n    if not prompt_matched or not negative_prompt_matched:\n        return False, prompt, negative_prompt\n\n    return True, prompt, negative_prompt\n\n", "CodeBleu_score": 60.12124182779812}, {"code": "def extract_original_prompts(style: PromptStyle, prompt, negative_prompt)\n\t\n\n    if style.prompt == \"\" and style.negative_prompt == \"\":\n        return False, prompt, negative_prompt\n\n    positive_extracted, prompt = extract_style_text_from_prompt(style.prompt, prompt)\n    negative_extracted, negative_prompt = extract_style_text_from_prompt(style.negative_prompt, negative_prompt)\n\n    if not positive_extracted or not negative_extracted:\n        return False, prompt, negative_prompt\n\n    return True, prompt, negative_prompt\n", "CodeBleu_score": 61.0350975021905}]}
{"instance_id": "0c5f729fbce2", "generation_result": [{"code": "def random_partial(self, count, n_frames)\n\t\n    if self.utterances is None:\n        self._load_utterances()\n\n    utterances = self.utterance_cycler.sample(count)\n\n    a = [(u,) + u.random_partial(n_frames) for u in utterances]\n\n    return a\n", "CodeBleu_score": 94.20475781398312}, {"code": "def random_partial(self, count, n_frames)\n\t\n    if self.utterances is None:\n        self._load_utterances()\n\n    utterances = self.utterance_cycler.sample(count)\n\n    a = [(u,) + u.random_partial(n_frames) for u in utterances]\n\n    return a\n\n", "CodeBleu_score": 94.20475781398312}, {"code": "def random_partial(self, count, n_frames)\n\t\n    if self.utterances is None:\n        self._load_utterances()\n\n    utterances = self.utterance_cycler.sample(count)\n\n    a = [(u,) + u.random_partial(n_frames) for u in utterances]\n\n    return a\n", "CodeBleu_score": 94.20475781398312}]}
{"instance_id": "31e8b27b8f94", "generation_result": [{"code": "def normalize_volume(wav, target_dBFS, increase_only=False, decrease_only=False)\n\t\n    if increase_only and decrease_only:\n        raise ValueError(\"increase_only and decrease_only cannot both be True.\")\n\n    dBFS = get_dBFS(wav)\n    change_in_dBFS = target_dBFS - dBFS\n    if (change_in_dBFS < 0 and increase_only) or (change_in_dBFS > 0 and decrease_only):\n        return wav\n    return wav * (10 ** (change_in_dBFS / 20))\n\n", "CodeBleu_score": 56.897186901186934}, {"code": "def normalize_volume(wav, target_dBFS, increase_only=False, decrease_only=False)\n\t\n    if increase_only and decrease_only:\n        raise ValueError(\"increase_only and decrease_only are mutually exclusive\")\n\n    dBFS = audioop.rms(wav, 2)\n    if dBFS == 0:\n        return wav\n\n    change_in_dBFS = target_dBFS - dBFS\n\n    if (change_in_dBFS < 0 and increase_only) or (change_in_dBFS > 0 and decrease_only):\n        return wav\n\n    return audioop.mul(wav, 2, int(np.power(10, change_in_dBFS / 20)))\n\n", "CodeBleu_score": 47.02887105676623}, {"code": "def normalize_volume(wav, target_dBFS, increase_only=False, decrease_only=False)\n\t\n    if increase_only and decrease_only:\n        raise ValueError(\"`increase_only` and `decrease_only` cannot be set at the same time.\")\n\n    db_change = target_dBFS - rms_db(wav, target_dBFS)\n    if db_change < 0 and increase_only or db_change > 0 and decrease_only:\n        return wav\n    else:\n        return wav * (10 ** (db_change / 20))\n\n", "CodeBleu_score": 45.82378413186123}]}
{"instance_id": "2353831e9033", "generation_result": [{"code": "def load_model(weights_fpath: Path, device=None)\n\t\n    global _model\n    if device is None:\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    _model = SpeakerEncoder(device)\n    _model.load_state_dict(torch.load(weights_fpath, map_location=device))\n    _model.eval()\n    print(f\"Loaded model from {weights_fpath} and set to evaluation mode.\")\n\n", "CodeBleu_score": 39.968929281413466}, {"code": "def load_model(weights_fpath: Path, device=None)\n\t\n    global _model\n    if device is None:\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    _model = SpeakerEncoder(weights_fpath.parent / \"speaker_encoder.pth\", device=device)\n    _model.load_state_dict(torch.load(weights_fpath, map_location=device))\n    _model.eval()\n    print(f\"Loaded model from {weights_fpath} on {device}\")\n\n", "CodeBleu_score": 43.83682965106187}, {"code": "def load_model(weights_fpath: Path, device=None)\n\t\n    global _model\n    if device is None:\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    _model = SpeakerEncoder(device=device)\n    _model.load_state_dict(torch.load(weights_fpath, map_location=device))\n    _model.eval()\n    print(\"Loaded model from {}\".format(weights_fpath))\n    print(\"Trained up to step {}\".format(get_step_from_fname(weights_fpath)))\n\n", "CodeBleu_score": 45.17803015634341}]}
{"instance_id": "3231ad480199", "generation_result": [{"code": "def embed_frames_batch(frames_batch)\n\t\n    if _model is None:\n        raise Exception('Model not loaded')\n    frames_batch_torch = torch.from_numpy(frames_batch).to(_device)\n    embeddings = _model.forward(frames_batch_torch).detach().cpu().numpy()\n    return embeddings\n\n", "CodeBleu_score": 71.92511284220785}, {"code": "def embed_frames_batch(frames_batch)\n\t\n    if not _model:\n        raise Exception(\"Model not loaded\")\n\n    # Convert the input frames_batch from a NumPy array to a PyTorch tensor\n    frames_batch = torch.from_numpy(frames_batch)\n    # Transfer the tensor to the appropriate device\n    frames_batch = frames_batch.to(_device)\n    # Perform a forward pass through the model to compute the embeddings\n    embeds = _model.forward(frames_batch)\n    # Detach the result from the computation graph\n    embeds = embeds.detach()\n    # Move the result to the CPU\n    embeds = embeds.cpu()\n    # Convert the result back to a NumPy array\n    embeds = embeds.numpy()\n\n    return embeds\n\n", "CodeBleu_score": 49.008214201430135}, {"code": "def embed_frames_batch(frames_batch)\n\t\n    if _model is None:\n        raise Exception(\"Model not loaded\")\n\n    frames_batch = torch.from_numpy(frames_batch).to(_device)\n    embeds = _model.forward(frames_batch).detach().cpu().numpy()\n\n    return embeds\n\n", "CodeBleu_score": 69.54748106382162}]}
{"instance_id": "117a41716c8d", "generation_result": [{"code": "def compute_partial_slices(n_samples, partial_utterance_n_frames=partials_n_frames,\n                           min_pad_coverage=0.75, overlap=0.5)\n\t\n    assert 0 <= overlap < 1\n    assert 0 < min_pad_coverage <= 1\n    frame_step = int(sampling_rate * (1 - overlap))\n    n_frames = int(np.ceil(n_samples / frame_step))\n    wav_slices = []\n    mel_slices = []\n    for i in range(n_frames):\n        wav_slice = slice(i * frame_step, (i + 1) * frame_step + partial_utterance_n_frames)\n        mel_slice = slice(i * frame_step, (i + 1) * frame_step + partial_utterance_n_frames)\n        wav_slices.append(wav_slice)\n        mel_slices.append(mel_slice)\n    wav_slices = np.array(wav_slices)\n    mel_slices = np.array(mel_slices)\n    last_slice_coverage = wav_slices[-1].stop / n_samples\n    if last_slice_coverage < min_pad_coverage and len(wav_slices) > 1:\n        wav_slices = wav_slices[:-1]\n        mel_slices = mel_slices[:-1]\n    return wav_slices, mel_slices\n\n", "CodeBleu_score": 42.76047278422313}, {"code": "def compute_partial_slices(n_samples, partial_utterance_n_frames=partials_n_frames,\n                           min_pad_coverage=0.75, overlap=0.5)\n\t\n    assert 0 < overlap < 1, 'Overlap must be between 0 and 1.'\n    assert 0 < min_pad_coverage < 1, 'Minimum padding coverage must be between 0 and 1.'\n    # Compute the number of frames and the frame step\n    n_frames = int(np.ceil(n_samples / partial_utterance_n_frames))\n    frame_step = int(np.round(partial_utterance_n_frames * (1 - overlap)))\n    # Compute wav and mel slices\n    wav_slices = []\n    mel_slices = []\n    for i in range(n_frames):\n        wav_slices.append(slice(i * frame_step, i * frame_step + partial_utterance_n_frames))\n        mel_slices.append(slice(i * frame_step, i * frame_step + partial_utterance_n_frames))\n    # Evaluate the need for extra padding\n    last_wav_slice = wav_slices[-1]\n    last_mel_slice = mel_slices[-1]\n    if last_wav_slice.stop < n_samples and len(wav_slices) > 1:\n        wav_coverage = last_wav_slice.stop / n_samples\n        mel_coverage = last_mel_slice.stop / n_samples\n        if wav_coverage < min_pad_coverage or mel_coverage < min_pad_coverage:\n            wav_slices = wav_slices[:-1]\n            mel_slices = mel_slices[:-1]\n    return wav_slices, mel_slices\n\n", "CodeBleu_score": 39.35149967920207}, {"code": "def compute_partial_slices(n_samples, partial_utterance_n_frames=partials_n_frames,\n                           min_pad_coverage=0.75, overlap=0.5)\n\t\n    assert 0 < overlap < 1\n    assert 0 < min_pad_coverage < 1\n    frame_step = int(sr * (1 - overlap))\n    n_frames = int(np.ceil(n_samples / frame_step))\n    wav_slices = []\n    mel_slices = []\n    for i in range(n_frames):\n        start = i * frame_step\n        end = start + partial_utterance_n_frames\n        wav_slices.append(slice(start, end))\n        mel_slices.append(slice(np.round(start / sr), np.round(end / sr)))\n    wav_coverage = wav_slices[-1].stop / n_samples\n    mel_coverage = mel_slices[-1].stop / (n_samples / sr)\n    if wav_coverage < min_pad_coverage and len(wav_slices) > 1:\n        wav_slices = wav_slices[:-1]\n        mel_slices = mel_slices[:-1]\n    return wav_slices, mel_slices\n", "CodeBleu_score": 37.62815029034421}]}
{"instance_id": "34fe5c864f1c", "generation_result": [{"code": "def plot_embedding_as_heatmap(embed, ax=None, title=\"\", shape=None, color_range=(0, 0.30))\n\t\n    if ax is None:\n        ax = plt.gca()\n    if shape is None:\n        shape = (int(np.sqrt(len(embed))), int(np.sqrt(len(embed))))\n    embed = embed.reshape(shape)\n    ax.imshow(embed, cmap=cm.get_cmap(\"viridis\"), vmin=color_range[0], vmax=color_range[1])\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.set_title(title)\n    sm = cm.ScalarMappable(cmap=cm.get_cmap(\"viridis\"), norm=plt.Normalize(vmin=color_range[0], vmax=color_range[1]))\n    sm.set_array([])\n    plt.colorbar(sm, ax=ax)", "CodeBleu_score": 59.58513176774698}, {"code": "def plot_embedding_as_heatmap(embed, ax=None, title=\"\", shape=None, color_range=(0, 0.30))\n\t\n    if ax is None:\n        ax = plt.gca()\n    if shape is None:\n        height = int(np.sqrt(len(embed)))\n        embed = embed.reshape((height, height))\n    ax.imshow(embed, cmap='Greys', vmin=color_range[0], vmax=color_range[1])\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.set_title(title)\n    plt.colorbar(ax.imshow(embed, cmap='Greys', vmin=color_range[0], vmax=color_range[1]), ax=ax)\n", "CodeBleu_score": 51.32943386604101}, {"code": "def plot_embedding_as_heatmap(embed, ax=None, title=\"\", shape=None, color_range=(0, 0.30))\n\t\n    if ax is None:\n        ax = plt.gca()\n    if shape is None:\n        height = int(np.sqrt(len(embed)))\n        embed = embed.reshape(height, height)\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.set_title(title)\n    sm = plt.cm.ScalarMappable(cmap=plt.cm.get_cmap('viridis', 100))\n    sm.set_array([])\n    sm.set_clim(color_range[0], color_range[1])\n    ax.imshow(embed, cmap=plt.cm.get_cmap('viridis', 100))\n    ax.set_aspect('equal')\n    cbar = plt.colorbar(sm, fraction=0.046, pad=0.04)\n    cbar.ax.set_ylabel('Activation', rotation=270, labelpad=20)\n    cbar.ax.yaxis.set_ticks_position('left')\n    return ax\n", "CodeBleu_score": 57.80139715433095}]}
{"instance_id": "92e93d50b62b", "generation_result": [{"code": "def similarity_matrix(self, embeds)\n\t\n    num_spks, num_utts = embeds.shape[:2]\n    centroids_incl = embeds.mean(dim=1, keepdim=True)\n    centroids_incl = F.normalize(centroids_incl, p=2, dim=2)\n    centroids_excl = embeds.clone()\n    centroids_excl = centroids_excl.sum(dim=1, keepdim=True) - embeds\n    centroids_excl = F.normalize(centroids_excl, p=2, dim=2)\n    sim_mat = torch.zeros((num_spks, num_utts, num_utts), device=embeds.device)\n    mask = torch.eye(num_utts, device=embeds.device).bool()\n    for i in range(num_spks):\n        sim_mat[i] = torch.matmul(embeds[i].unsqueeze(1), centroids_incl[i].transpose(1, 2))\n        sim_mat[i].masked_fill_(mask, 0)\n        sim_mat[i] = torch.matmul(embeds[i].unsqueeze(1), centroids_excl[i].transpose(1, 2))\n        sim_mat[i].masked_fill_(mask, 0)\n    sim_mat = self.scale * sim_mat + self.bias\n    return sim_mat\n", "CodeBleu_score": 34.09228936783229}, {"code": "def similarity_matrix(self, embeds)\n\t\n    num_spks = embeds.shape[0]\n    num_utts = embeds.shape[1]\n    centroids_incl = torch.zeros((num_spks, self.embed_dim), device=embeds.device)\n    centroids_excl = torch.zeros((num_spks, self.embed_dim), device=embeds.device)\n    for i in range(num_spks):\n        centroids_incl[i] = torch.mean(embeds[i], dim=0, keepdim=True)\n        centroids_incl[i] = torch.nn.functional.normalize(centroids_incl[i], p=2, dim=1)\n        centroids_excl[i] = torch.mean(embeds[i, :num_utts - 1], dim=0, keepdim=True)\n        centroids_excl[i] = torch.nn.functional.normalize(centroids_excl[i], p=2, dim=1)\n    sim_mat = torch.zeros((num_spks * num_utts, num_spks * num_utts), device=embeds.device)\n    mask = torch.eye(num_spks * num_utts, device=embeds.device)\n    for i in range(num_spks):\n        sim_mat[i * num_utts:(i + 1) * num_utts, i * num_utts:(i + 1) * num_utts] = torch.matmul(embeds[i], embeds[i].t())\n        sim_mat[i * num_utts:(i + 1) * num_utts, i * num_utts:(i + 1) * num_utts] = sim_mat[i * num_utts:(i + 1) * num_utts, i * num_utts:(i + 1) * num_utts] - mask[i * num_utts:(i + 1) * num_utts, i * num_utts:(i + 1) * num_utts]\n        sim_mat[i * num_utts:(i + 1) * num_utts, i * num_utts + 1:(i + 1)", "CodeBleu_score": 30.202044701742924}, {"code": "def similarity_matrix(self, embeds)\n\t\n    n_speakers = embeds.shape[0]\n    n_utters = embeds.shape[1]\n    centroids_incl = torch.zeros(n_speakers, embeds.shape[2], device=embeds.device)\n    centroids_excl = torch.zeros(n_speakers, embeds.shape[2], device=embeds.device)\n    for i in range(n_speakers):\n        centroids_incl[i] = torch.mean(embeds[i], dim=0, keepdim=True)\n        centroids_incl[i] /= torch.norm(centroids_incl[i], dim=1, keepdim=True)\n        centroids_excl[i] = torch.mean(torch.cat([embeds[i, :i], embeds[i, i+1:]], dim=0), dim=0, keepdim=True)\n        centroids_excl[i] /= torch.norm(centroids_excl[i], dim=1, keepdim=True)\n    sim_mat = torch.zeros(n_speakers, n_utters, n_utters, device=embeds.device)\n    mask = torch.eye(n_utters, device=embeds.device)\n    for i in range(n_speakers):\n        sim_mat[i] = torch.mm(embeds[i], centroids_incl[i].t())\n        sim_mat[i] -= torch.mm(embeds[i], centroids_excl[i].t())\n        sim_mat[i] *= self.scale\n        sim_mat[i] += self.bias\n        sim_mat[i] *= (1 - mask)\n    return sim_mat\n\n", "CodeBleu_score": 35.924912533062766}]}
{"instance_id": "a6eba54108a1", "generation_result": [{"code": "def loss(self, embeds)\n\t\n    speakers_per_batch, utterances_per_speaker = embeds.shape[:2]\n    sim_matrix = self.similarity_matrix(embeds)\n    sim_matrix = sim_matrix.reshape(speakers_per_batch * utterances_per_speaker, -1)\n    labels = np.repeat(np.arange(speakers_per_batch), utterances_per_speaker)\n    loss = self.loss_fn(sim_matrix, torch.from_numpy(labels).to(self.loss_device))\n    with torch.no_grad():\n        preds = sim_matrix.argmax(dim=1)\n        fpr, tpr, thresholds = roc_curve(labels, preds)\n        eer = fpr[np.nanargmin(np.abs(fpr - (1 - tpr)))]\n    return loss, eer\n\n", "CodeBleu_score": 42.063852748190335}, {"code": "def loss(self, embeds)\n\t\n        # Get number of speakers and utterances per speaker\n        speakers_per_batch, utterances_per_speaker = embeds.shape[:2]\n        # Compute similarity matrix\n        sim_matrix = self.similarity_matrix(embeds)\n        # Reshape the similarity matrix to a vector\n        sim_vector = sim_matrix.reshape(speakers_per_batch * utterances_per_speaker,\n                                        speakers_per_batch)\n        # Generate ground truth labels\n        labels = np.zeros((speakers_per_batch * utterances_per_speaker, speakers_per_batch))\n        labels[np.arange(speakers_per_batch * utterances_per_speaker),\n               np.repeat(np.arange(speakers_per_batch), utterances_per_speaker)] = 1\n        # Compute loss\n        loss = self.loss_fn(sim_vector, torch.from_numpy(labels).to(self.loss_device))\n        # Compute EER\n        with torch.no_grad():\n            # Compute predictions\n            preds = sim_vector.cpu().numpy()\n            preds = preds.flatten()\n            # Compute true labels\n            labels = labels.flatten()\n            # Compute ROC curve\n            fpr, tpr, thresholds = roc_curve(labels, preds)\n            # Find threshold where FPR equals FNR\n            fnr = 1 - tpr\n            eer = fpr[np.nanargmin(np.absolute((fnr - fpr)))]\n        return loss, eer\n\n", "CodeBleu_score": 46.06795265995419}, {"code": "def loss(self, embeds)\n\t\n        # Get the number of speakers and utterances per speaker\n        speakers_per_batch, utterances_per_speaker = embeds.shape[:2]\n\n        # Compute the similarity matrix\n        sim_matrix = self.similarity_matrix(embeds)\n        sim_matrix = sim_matrix.reshape(speakers_per_batch, utterances_per_speaker,\n                                        speakers_per_batch, utterances_per_speaker)\n        sim_matrix = sim_matrix.permute(0, 2, 1, 3)\n\n        # Generate the ground truth labels\n        labels = torch.zeros(speakers_per_batch, speakers_per_batch,\n                             utterances_per_speaker, utterances_per_speaker)\n        for i in range(speakers_per_batch):\n            labels[i, i, :, :] = 1\n\n        # Compute the loss\n        loss = self.loss_fn(sim_matrix, labels)\n\n        # Compute the EER without backpropagation\n        with torch.no_grad():\n            preds = sim_matrix.flatten(end_dim=1)\n            labels = labels.flatten(end_dim=1)\n            fpr, tpr, thresholds = roc_curve(labels.cpu().numpy(), preds.cpu().numpy())\n            eer = fpr[np.nanargmin(np.abs(fpr - (1 - tpr)))]\n\n        return loss, eer\n", "CodeBleu_score": 46.831075899111205}]}
{"instance_id": "ac7b74b9d1b1", "generation_result": [{"code": "def log_params(self)\n\t\n    if self.log_parameters:\n        # Get parameters from config file\n        with open(self.config_path, 'r') as f:\n            config = yaml.load(f, Loader=yaml.FullLoader)\n        # Get model parameters\n        model_params = config['model_params']\n        model_params_str = \"\"\n        for param in model_params:\n            model_params_str += param + \": \" + str(model_params[param]) + \"", "CodeBleu_score": 12.675333820267179}, {"code": "def log_params(self)\n\t\n    if not self.log_params:\n        return\n    from. import vis\n    params = ''\n    if self.config:\n        params += 'Configuration Parameters", "CodeBleu_score": 5.272734009635116}, {"code": "def log_params(self)\n\t\n    if self.log_parameters:\n        # get the parameters from the model and data\n        model_params = self.model.get_params()\n        data_params = self.data.get_params()\n\n        # create a formatted string of model parameters\n        model_params_str = \"\"\n        for param_name, param_value in model_params.items():\n            model_params_str += f\"{param_name}: {param_value}", "CodeBleu_score": 9.909183523902385}]}
{"instance_id": "4726b7864386", "generation_result": [{"code": "def draw_projections(self, embeds, utterances_per_speaker, step, out_fpath=None, max_speakers=10)\n\t\n    if self.viz_speakers is None:\n        self.viz_speakers = list(embeds.keys())[:max_speakers]\n    embeds = {speaker: embeds[speaker] for speaker in self.viz_speakers}\n    colors = [self.viz_colors[speaker] for speaker in self.viz_speakers]\n    embeds = np.concatenate([embeds[speaker] for speaker in self.viz_speakers])\n    speakers = np.concatenate([np.repeat(speaker, utterances_per_speaker) for speaker in self.viz_speakers])\n    reducer = umap.UMAP(n_components=2)\n    embeds = reducer.fit_transform(embeds)\n    plt.clf()\n    plt.scatter(embeds[:, 0], embeds[:, 1], s=1, c=speakers, cmap=matplotlib.colors.ListedColormap(colors))\n    plt.set_aspect('equal')\n    plt.gca().set_axis_off()\n    plt.title('Step {}'.format(step))\n    if out_fpath is not None:\n        plt.savefig(out_fpath)\n    if self.viz_show:\n        plt.show()\n\n", "CodeBleu_score": 47.05598830411564}, {"code": "def draw_projections(self, embeds, utterances_per_speaker, step, out_fpath=None, max_speakers=10)\n\t\n    # Limit the number of speakers to a maximum of 10\n    if len(embeds) > max_speakers:\n        embeds = embeds[:max_speakers]\n        utterances_per_speaker = utterances_per_speaker[:max_speakers]\n\n    # Flatten the embeddings and assign colors based on speaker identity\n    colors = np.repeat(np.arange(len(embeds)), utterances_per_speaker)\n    embeds = np.concatenate(embeds, axis=0)\n\n    # Reduce the dimensionality of the embeddings using UMAP\n    reducer = umap.UMAP(random_state=42)\n    embeds = reducer.fit_transform(embeds)\n\n    # Plot the embeddings and display or save to a file if specified\n    plt.clf()\n    plt.scatter(embeds[:, 0], embeds[:, 1], c=colors, cmap='tab10')\n    plt.set_aspect('equal')\n    plt.title(f'Step {step}')\n    if out_fpath is not None:\n        plt.savefig(out_fpath)\n    else:\n        plt.show()", "CodeBleu_score": 42.01404728097932}, {"code": "def draw_projections(self, embeds, utterances_per_speaker, step, out_fpath=None, max_speakers=10)\n\t\n    if self.viz_opts.viz_type == \"umap\":\n        if self.viz_opts.viz_every_n_steps is not None and step % self.viz_opts.viz_every_n_steps!= 0:\n            return\n        if self.viz_opts.viz_max_speakers is not None and self.viz_opts.viz_max_speakers < utterances_per_speaker.max():\n            utterances_per_speaker = utterances_per_speaker[:self.viz_opts.viz_max_speakers]\n            embeds = embeds[:self.viz_opts.viz_max_speakers]\n        embeds = embeds.reshape(-1, self.model_opts.embedding_dim)\n        utterances_per_speaker = np.repeat(utterances_per_speaker, self.model_opts.embedding_dim)\n        reducer = umap.UMAP(random_state=42)\n        reducer.fit(embeds)\n        embeds = reducer.transform(embeds)\n        plt.clf()\n        plt.scatter(\n            embeds[:, 0],\n            embeds[:, 1],\n            c=np.arange(len(utterances_per_speaker)),\n            cmap=\"Spectral\",\n            s=10,\n            alpha=0.5,\n        )\n        plt.gca().set_aspect(\"equal\", \"datalim\")\n        plt.title(f\"Step {step}\")\n        if out_fpath is not None:\n            plt.savefig(out_fpath)\n        else:\n            plt.show()\n\n", "CodeBleu_score": 39.509735713927526}]}
{"instance_id": "274fa50985c0", "generation_result": [{"code": "def add_speaker_embedding(self, x, speaker_embedding)\n\t\n    # Get the dimensions of the input tensor as human-readable names\n    B, T, C = x.size()\n    # Determine the appropriate dimension index for the speaker embedding\n    speaker_embedding_dim = 1 if self.speaker_embedding_dim == \"batch\" else 2\n    # Repeat the speaker embedding to match the length of the input text\n    speaker_embedding = speaker_embedding.repeat(T)\n    # Reshape and transpose the speaker embedding\n    speaker_embedding = speaker_embedding.reshape(B, T, -1)\n    speaker_embedding = speaker_embedding.transpose(1, speaker_embedding_dim)\n    # Concatenate the speaker embedding with the input tensor along the last dimension\n    x = torch.cat((x, speaker_embedding), dim=-1)\n    return x\n", "CodeBleu_score": 30.54396264705162}, {"code": "def add_speaker_embedding(self, x, speaker_embedding)\n\t\n    input_shape = x.size()\n    speaker_embedding_dim = speaker_embedding.dim()\n    if speaker_embedding_dim == 1:\n        speaker_embedding = speaker_embedding.unsqueeze(0)\n    if speaker_embedding_dim == 2:\n        speaker_embedding = speaker_embedding.unsqueeze(1)\n    if speaker_embedding.size(0)!= input_shape[0]:\n        speaker_embedding = speaker_embedding.repeat(input_shape[0], 1)\n    speaker_embedding = speaker_embedding.reshape(input_shape[0], input_shape[1], -1)\n    speaker_embedding = speaker_embedding.transpose(1, 2)\n    x = torch.cat((x, speaker_embedding), dim=-1)\n    return x\n", "CodeBleu_score": 35.890097754582776}, {"code": "def add_speaker_embedding(self, x, speaker_embedding)\n\t\n    # Save the dimensions of the input tensor as human-readable names\n    b, t, e = x.size()\n\n    # Determine the appropriate dimension index for the speaker embedding\n    speaker_embedding_dim = 1 if self.speaker_embedding_dimension == 'batch' else 2\n\n    # Repeat the speaker embedding to match the length of the input text\n    speaker_embedding = speaker_embedding.repeat(t, 1)\n\n    # Reshape and transpose the speaker embedding to match the input tensor\n    speaker_embedding = speaker_embedding.view(b, t, -1)\n    speaker_embedding = speaker_embedding.transpose(speaker_embedding_dim, 2)\n\n    # Concatenate the speaker embedding with the input tensor along the last dimension\n    x = torch.cat((x, speaker_embedding), dim=-1)\n\n    return x\n", "CodeBleu_score": 31.61440408491482}]}
{"instance_id": "e6742ea0fe5e", "generation_result": [{"code": "def init_attention(self, encoder_seq_proj)\n\t\n    device = self.parameters.device\n    batch_size, seq_len = encoder_seq_proj.size(0), encoder_seq_proj.size(1)\n    self.attention = torch.zeros(batch_size, seq_len, device=device)\n    self.cum_attention = torch.zeros(batch_size, seq_len, device=device)\n", "CodeBleu_score": 22.016915250299306}, {"code": "def init_attention(self, encoder_seq_proj)\n\t\n    batch_size, seq_len = encoder_seq_proj.size(0), encoder_seq_proj.size(1)\n    self.attention = torch.zeros(batch_size, seq_len, device=self.parameters.device)\n    self.cumulative_attention = torch.zeros(batch_size, seq_len, device=self.parameters.device)\n\n", "CodeBleu_score": 26.509549402462618}, {"code": "def init_attention(self, encoder_seq_proj)\n\t\n    # Determine device from model parameters\n    device = self.parameters.device\n    # Create and zero cumulative and attention tensors\n    self.cumulative_attention = torch.zeros(encoder_seq_proj.size(), device=device)\n    self.attention = torch.zeros(encoder_seq_proj.size(), device=device)\n", "CodeBleu_score": 22.50297491146399}]}
{"instance_id": "4e1fc1ed0ed7", "generation_result": [{"code": "def load(self, path, optimizer=None)\n\t\n    # Use device of model params as location for loaded state\n    device = next(self.parameters()).device\n    checkpoint = torch.load(str(path), map_location=device)\n    self.load_state_dict(checkpoint[\"model_state\"])\n\n    if \"optimizer_state\" in checkpoint and optimizer is not None:\n        optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n\n", "CodeBleu_score": 67.98839425729693}, {"code": "def load(self, path, optimizer=None)\n\t\n    # Use device of model params as location for loaded state\n    device = next(self.parameters()).device\n    checkpoint = torch.load(str(path), map_location=device)\n    self.load_state_dict(checkpoint[\"model_state\"])\n\n    if \"optimizer_state\" in checkpoint and optimizer is not None:\n        optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n\n", "CodeBleu_score": 67.98839425729693}, {"code": "def load(self, path, optimizer=None)\n\t\n    # Use device of model params as location for loaded state\n    device = next(self.parameters()).device\n    checkpoint = torch.load(str(path), map_location=device)\n    self.load_state_dict(checkpoint[\"model_state\"])\n\n    if \"optimizer_state\" in checkpoint and optimizer is not None:\n        optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n\n", "CodeBleu_score": 67.98839425729693}]}
{"instance_id": "178e6aa16c0f", "generation_result": [{"code": "def save(self, path, optimizer=None)\n\t\n    if optimizer is not None:\n        torch.save({\n            \"model_state\": self.state_dict(),\n            \"optimizer_state\": optimizer.state_dict(),\n        }, str(path))\n    else:\n        torch.save({\n            \"model_state\": self.state_dict(),\n        }, str(path))\n", "CodeBleu_score": 93.4714524643972}, {"code": "def save(self, path, optimizer=None)\n\t\n    if optimizer is not None:\n        torch.save({\n            \"model_state\": self.state_dict(),\n            \"optimizer_state\": optimizer.state_dict(),\n        }, str(path))\n    else:\n        torch.save({\n            \"model_state\": self.state_dict(),\n        }, str(path))\n", "CodeBleu_score": 93.4714524643972}, {"code": "def save(self, path, optimizer=None)\n\t\n    if optimizer is not None:\n        torch.save({\n            \"model_state\": self.state_dict(),\n            \"optimizer_state\": optimizer.state_dict(),\n        }, str(path))\n    else:\n        torch.save({\n            \"model_state\": self.state_dict(),\n        }, str(path))\n\n", "CodeBleu_score": 93.4714524643972}]}
{"instance_id": "6528fd80e87b", "generation_result": [{"code": "def english_cleaners(text)\n\t\n    text = convert_to_ascii(text)\n    text = lowercase(text)\n    text = expand_numbers(text)\n    text = expand_abbreviations(text)\n    text = collapse_whitespace(text)\n    return text\n\n", "CodeBleu_score": 55.88611500274403}, {"code": "def english_cleaners(text)\n\t\n    text = convert_to_ascii(text)\n    text = lowercase(text)\n    text = expand_numbers(text)\n    text = expand_abbreviations(text)\n    text = collapse_whitespace(text)\n    return text\n\n", "CodeBleu_score": 55.88611500274403}, {"code": "def english_cleaners(text)\n\t\n    text = convert_to_ascii(text)\n    text = lowercase(text)\n    text = expand_numbers(text)\n    text = expand_abbreviations(text)\n    text = collapse_whitespace(text)\n    return text\n\n", "CodeBleu_score": 55.88611500274403}]}
{"instance_id": "8a4fe08e0985", "generation_result": [{"code": "def text_to_sequence(text, cleaner_names)\n\t\n    sequence = []\n    # Check for curly braces and treat their contents as ARPAbet:\n    while len(text):\n        m = _curly_re.match(text)\n        if not m:\n            sequence += _symbols_to_sequence(_clean_text(text, cleaner_names))\n            break\n        sequence += _arpabet_to_sequence(m.group(1))\n        text = m.group(2)\n    sequence += [_symbol_to_id[\"~\"]]  # end of sequence.\n    return sequence\n\n", "CodeBleu_score": 61.66580112819824}, {"code": "def text_to_sequence(text, cleaner_names)\n\t\n    sequence = []\n    # Check for curly braces and treat their contents as ARPAbet:\n    while len(text):\n        m = _curly_re.match(text)\n        # If no curly braces (or no content within them),\n        if not m:\n            sequence += _symbols_to_sequence(_clean_text(text, cleaner_names))\n            break\n        # If no content within curly braces, move past the braces\n        if m.group(0) == \"{}\":\n            sequence += _symbols_to_sequence([\"{\"])\n            text = text[1:]\n            continue\n        # If content within curly braces, convert to ARPAbet and append\n        sequence += _arpabet_to_sequence(m.group(0)[1:-1])\n        text = text[m.end(0):]\n    # Append EOS token\n    sequence.append(_symbol_to_id[\"~\"])\n    return sequence", "CodeBleu_score": 59.64048675030492}, {"code": "def text_to_sequence(text, cleaner_names)\n\t\n    sequence = []\n    # Check for curly braces and treat their contents as ARPAbet:\n    while len(text):\n        m = _curly_re.match(text)\n        # No curly braces (normal case)\n        if not m:\n            sequence += _symbols_to_sequence(_clean_text(text, cleaner_names))\n            break\n        # Curly braces with ARPAbet text inside\n        sequence += _arpabet_to_sequence(m.group(1))\n        # Everything else outside curly braces\n        sequence += _symbols_to_sequence(_clean_text(m.group(2), cleaner_names))\n        text = m.group(3)\n    # Append EOS token\n    sequence.append(_symbol_to_id[\"~\"])\n    return sequence\n\n", "CodeBleu_score": 74.33833771918643}]}
{"instance_id": "e1c25a37af1f", "generation_result": [{"code": "def sequence_to_text(sequence)\n\t\n    result = ''\n    for symbol_id in sequence:\n        if symbol_id == 0:\n            result +=''\n        else:\n            result += dictionary[symbol_id]\n    result = result.replace('{','{').replace('}', '} ')\n    return result.strip()\n\n", "CodeBleu_score": 36.332107615232644}, {"code": "def sequence_to_text(sequence)\n\t\n    result = \"\"\n    for i in range(len(sequence)):\n        if sequence[i] == 0:\n            break\n        result += char_map_inv[sequence[i]]\n    return result.replace(\"}\", \" }\")\n\n", "CodeBleu_score": 23.755651362665166}, {"code": "def sequence_to_text(sequence)\n\t\n    result = ''\n    for element in sequence:\n        if element == 0:\n            result +=''\n        elif element == 1:\n            result += '{'\n        elif element == 2:\n            result += '}'\n        else:\n            result += int_to_char[element]\n    result = result.replace('{}','')\n    return result\n\n", "CodeBleu_score": 28.061907423799937}]}
{"instance_id": "7807cad4b05d", "generation_result": [{"code": "def embed_utterance(fpaths, encoder_model_fpath)\n\t\n    if not encoder.is_loaded():\n        encoder.load_model(encoder_model_fpath)\n\n    # Compute the speaker embedding of the utterance\n    wav_fpath, embed_fpath = fpaths\n    wav = np.load(wav_fpath)\n    wav = encoder.preprocess_wav(wav)\n    embed = encoder.embed_utterance(wav)\n    np.save(embed_fpath, embed, allow_pickle=False)\n\n", "CodeBleu_score": 95.3261531719067}, {"code": "def embed_utterance(fpaths, encoder_model_fpath)\n\t\n    if not encoder.is_loaded():\n        encoder.load_model(encoder_model_fpath)\n\n    # Compute the speaker embedding of the utterance\n    wav_fpath, embed_fpath = fpaths\n    wav = np.load(wav_fpath)\n    wav = encoder.preprocess_wav(wav)\n    embed = encoder.embed_utterance(wav)\n    np.save(embed_fpath, embed, allow_pickle=False)\n\n", "CodeBleu_score": 95.3261531719067}, {"code": "def embed_utterance(fpaths, encoder_model_fpath)\n\t\n    if not encoder.is_loaded():\n        encoder.load_model(encoder_model_fpath)\n\n    # Compute the speaker embedding of the utterance\n    wav_fpath, embed_fpath = fpaths\n    wav = np.load(wav_fpath)\n    wav = encoder.preprocess_wav(wav)\n    embed = encoder.embed_utterance(wav)\n    np.save(embed_fpath, embed, allow_pickle=False)\n\n", "CodeBleu_score": 95.3261531719067}]}
{"instance_id": "9f860273a325", "generation_result": [{"code": "def create_embeddings(synthesizer_root: Path, encoder_model_fpath: Path, n_processes: int)\n\t\n    synthesizer_root = Path(synthesizer_root)\n    encoder_model_fpath = Path(encoder_model_fpath)\n    wav_dir = synthesizer_root.joinpath(\"wavs\")\n    metadata_fpath = synthesizer_root.joinpath(\"metadata.csv\")\n    embed_dir = synthesizer_root.joinpath(\"embeds\")\n    if not wav_dir.exists():\n        raise FileNotFoundError(\"Wav directory does not exist.\")\n    if not metadata_fpath.exists():\n        raise FileNotFoundError(\"Metadata file does not exist.\")\n    if not encoder_model_fpath.exists():\n        raise FileNotFoundError(\"Encoder model file does not exist.\")\n    if not embed_dir.exists():\n        embed_dir.mkdir()\n    with metadata_fpath.open(\"r\") as f:\n        lines = f.readlines()\n    wav_paths = [wav_dir.joinpath(line.split(\"|\")[0]) for line in lines]\n    embed_paths = [embed_dir.joinpath(line.split(\"|\")[0] + \".npy\") for line in lines]\n    pool = Pool(n_processes)\n    for _ in tqdm(\n        pool.imap(\n            partial(create_embedding, encoder_model_fpath=encoder_model_fpath),\n            zip(wav_paths, embed_paths),\n        ),\n        total=len(lines),\n    ):\n        pass\n\n", "CodeBleu_score": 44.46848160504497}, {"code": "def create_embeddings(synthesizer_root: Path, encoder_model_fpath: Path, n_processes: int)\n\t\n\n    # Define paths to directories and files\n    wav_dir = synthesizer_root.joinpath(\"wavs\")\n    metadata_fpath = synthesizer_root.joinpath(\"metadata.csv\")\n    embed_dir = synthesizer_root.joinpath(\"embeds\")\n\n    # Verify the existence of directories and files\n    if not wav_dir.exists():\n        raise FileNotFoundError(f\"Directory {wav_dir} not found.\")\n    if not metadata_fpath.exists():\n        raise FileNotFoundError(f\"File {metadata_fpath} not found.\")\n    if not embed_dir.exists():\n        embed_dir.mkdir()\n\n    # Read metadata and prepare file paths for audio and embeddings\n    with metadata_fpath.open() as f:\n        lines = f.readlines()\n    wav_fpaths = [wav_dir.joinpath(line.split(\"|\")[0]) for line in lines]\n    embed_fpaths = [embed_dir.joinpath(line.split(\"|\")[0].replace(\".wav\", \".npy\")) for line in lines]\n\n    # Create a multiprocessing pool to process the audio files in parallel\n    pool = Pool(processes=n_processes)\n\n    # Define a function to create embeddings for a single audio file\n    def create_embedding(wav_fpath, embed_fpath, encoder_model_fpath):\n        # Load the audio file\n        wav, _ = librosa.load(wav_fpath, sr=16000)\n        # Extract the embedding using the encoder model\n        embed = extract_embedding(encoder_model_fpath, wav)\n        # Save the embedding to a file\n        np.save(embed_fpath, embed)\n\n    # Create embeddings for all audio files using the multiprocessing pool\n    with tqdm(total=len(wav_fpaths)) as pbar:\n        for _ in pool.imap_unordered(partial(create_embedding, encoder_model_fpath=encoder_model_fpath), zip(wav_fpaths, embed_fpaths)):\n            pbar.update()\n\n    # Close the multiprocessing pool\n    pool.close()\n    pool.join()\n", "CodeBleu_score": 44.09428985202057}, {"code": "def create_embeddings(synthesizer_root: Path, encoder_model_fpath: Path, n_processes: int)\n\t\n    wav_dir = synthesizer_root.joinpath(\"wavs\")\n    embed_dir = synthesizer_root.joinpath(\"embeds\")\n    metadata_fpath = synthesizer_root.joinpath(\"metadata.csv\")\n\n    if not wav_dir.exists():\n        print(f\"Error: Directory '{wav_dir}' not found\")\n        return\n    if not metadata_fpath.exists():\n        print(f\"Error: File '{metadata_fpath}' not found\")\n        return\n    if not encoder_model_fpath.exists():\n        print(f\"Error: File '{encoder_model_fpath}' not found\")\n        return\n\n    if not embed_dir.exists():\n        embed_dir.mkdir()\n\n    metadata_file = metadata_fpath.open()\n    metadata_file.readline()\n    metadata = [line.split(\"|\") for line in metadata_file]\n    metadata_file.close()\n\n    wav_paths = [wav_dir.joinpath(f\"{row[0]}.wav\") for row in metadata]\n    embed_paths = [embed_dir.joinpath(f\"{row[0]}.npy\") for row in metadata]\n\n    encoder = AudioEncoder(encoder_model_fpath)\n    encoder.load_model()\n\n    def create_embedding(wav_path: Path, embed_path: Path):\n        wav = encoder.preprocess_wav(wav_path)\n        embed = encoder.embed_utterance(wav)\n        np.save(embed_path, embed)\n\n    with Pool(n_processes) as pool:\n        for _ in tqdm(pool.imap_unordered(partial(create_embedding, embed_path=embed_path), wav_paths), total=len(wav_paths)):\n            pass", "CodeBleu_score": 47.5686729576341}]}
{"instance_id": "9eb0abda14a9", "generation_result": [{"code": "def eval_model(attention, mel_prediction, target_spectrogram, input_seq, step,\n               plot_dir, mel_output_dir, wav_dir, sample_num, loss, hparams)\n\t\n    # Save attention plot\n    save_attention(attention, plot_dir.joinpath(f\"step_{step}_sample_{sample_num}_attention\"))\n\n    # Save predicted mel spectrogram\n    plot_spectrogram(mel_prediction.data.cpu().numpy()[0], plot_dir.joinpath(f\"step_{step}_sample_{sample_num}_predicted_mel\"), title=f\"Predicted Mel-Spectrogram", "CodeBleu_score": 6.675280512136291}, {"code": "def eval_model(attention, mel_prediction, target_spectrogram, input_seq, step,\n               plot_dir, mel_output_dir, wav_dir, sample_num, loss, hparams)\n\t\n    print(\"Step: {}\".format(step))\n    print(\"Sample: {}\".format(sample_num))\n    print(\"Loss: {}\".format(loss))\n    print(\"Input: {}\".format(sequence_to_text(input_seq)))\n\n    # save attention plot\n    attn = attention.cpu().detach().numpy()\n    save_attention(attn, plot_dir.joinpath(\"attn_step_{}_sample_{}\".format(step, sample_num)))\n\n    # save predicted mel spectrogram\n    pred_mel = mel_prediction.cpu().detach().numpy()\n    pred_mel = np.reshape(pred_mel, pred_mel.shape[1:])\n    plot_spectrogram(pred_mel, plot_dir.joinpath(\"pred_step_{}_sample_{}\".format(step, sample_num)),\n                     title=\"{} | {} | Step: {} | Loss: {:.3f}\".format(\n                         time_string(), \"Sample {}\".format(sample_num), step, loss),\n                     max_len=hparams.max_seq_len_mel_gt)\n\n    # save mel spectrogram\n    mel = target_spectrogram.cpu().detach().numpy()\n    mel = np.reshape(mel, mel.shape[1:])\n    plot_spectrogram(mel, plot_dir.joinpath(\"mel_step_{}_sample_{}\".format(step, sample_num)),\n                     title=\"{} | {} | Step: {} | Loss: {:.3f}\".format(\n                         time_string(), \"Sample {}\".format(sample_num), step, loss),\n                     max_len=hparams.max_seq_len_mel_gt)\n\n    # save predicted audio\n    predicted_signal = audio.inv_mel_spectrogram(pred_mel.T)\n    save_wav(predicted_signal, wav_dir.joinpath(\"pred_step_{}_sample_{}\".format(step, sample_num)), sr=hparams.sample_rate)\n\n    # save mel spectrogram output\n    np.save(mel_output_dir.joinpath(\"mel_step_{}_sample_{}\".format(step, sample_num)), pred_mel.T)\n", "CodeBleu_score": 43.82914053972701}, {"code": "def eval_model(attention, mel_prediction, target_spectrogram, input_seq, step,\n               plot_dir, mel_output_dir, wav_dir, sample_num, loss, hparams)\n\t\n    # Save attention plot\n    save_attention(attention, plot_dir.joinpath(f\"step_{step}_sample_{sample_num}_attention.png\"))\n\n    # Save predicted mel spectrogram\n    save_mel_spectrogram(mel_prediction, mel_output_dir.joinpath(f\"step_{step}_sample_{sample_num}_predicted_mel.png\"))\n\n    # Reconstruct waveform from predicted mel spectrogram\n    reconstructed_wav = audio.inv_mel_spectrogram(mel_prediction.T, hparams)\n    audio.save_wav(reconstructed_wav, wav_dir.joinpath(f\"step_{step}_sample_{sample_num}_predicted_waveform.wav\"))\n\n    # Plot predicted vs. target mel spectrogram\n    plot_spectrogram(mel_prediction, plot_dir.joinpath(f\"step_{step}_sample_{sample_num}_predicted_mel.png\"),\n                     title=f\"Predicted vs. Target Mel-Spectrogram", "CodeBleu_score": 19.689495750592528}]}
{"instance_id": "6bad4f500286", "generation_result": [{"code": "def num_params(self)\n\t\n    num_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n    print(f\"The model has {num_params:,} trainable parameters\")\n", "CodeBleu_score": 13.440998962811184}, {"code": "def num_params(self)\n\t\n    return sum(p.numel() for p in self.parameters() if p.requires_grad) / 1e6\n", "CodeBleu_score": 5.637300874658717}, {"code": "def num_params(self)\n\t\n    num_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n    print(f\"The model has {num_params/1e6:.3f}M trainable parameters\")\n    return num_params\n", "CodeBleu_score": 15.831166306422729}]}
{"instance_id": "78b876054bd4", "generation_result": [{"code": "def pad_tensor(self, x, pad, side='both')\n\t\n    if side == 'both':\n        return torch.cat([torch.zeros(x.size(0), x.size(1), pad).cuda() if x.is_cuda else torch.zeros(x.size(0), x.size(1), pad), x, torch.zeros(x.size(0), x.size(1), pad).cuda() if x.is_cuda else torch.zeros(x.size(0), x.size(1), pad)], dim=2)\n    elif side == 'before':\n        return torch.cat([torch.zeros(x.size(0), x.size(1), pad).cuda() if x.is_cuda else torch.zeros(x.size(0), x.size(1), pad), x], dim=2)\n    elif side == 'after':\n        return torch.cat([x, torch.zeros(x.size(0), x.size(1), pad).cuda() if x.is_cuda else torch.zeros(x.size(0), x.size(1), pad)], dim=2)\n", "CodeBleu_score": 22.71817138308519}, {"code": "def pad_tensor(self, x, pad, side='both')\n\t\n    if side == 'before':\n        padded = torch.zeros(x.size(0), pad, x.size(2))\n        if x.is_cuda:\n            padded = padded.cuda()\n        padded = torch.cat([padded, x], dim=1)\n    elif side == 'after':\n        padded = torch.zeros(x.size(0), pad, x.size(2))\n        if x.is_cuda:\n            padded = padded.cuda()\n        padded = torch.cat([x, padded], dim=1)\n    elif side == 'both':\n        padded = torch.zeros(x.size(0), pad, x.size(2))\n        if x.is_cuda:\n            padded = padded.cuda()\n        padded = torch.cat([padded, x, padded], dim=1)\n    else:\n        raise ValueError('Side must be one of \\'before\\', \\'after\\', or \\'both\\'')\n    return padded\n", "CodeBleu_score": 32.96146532295371}, {"code": "def pad_tensor(self, x, pad, side='both')\n\t\n    if side == 'before':\n        return torch.cat([x, torch.zeros(x.size(0), pad, x.size(2)).type(x.type())], dim=1)\n    elif side == 'after':\n        return torch.cat([torch.zeros(x.size(0), pad, x.size(2)).type(x.type()), x], dim=1)\n    elif side == 'both':\n        return torch.cat([torch.zeros(x.size(0), pad, x.size(2)).type(x.type()), x, torch.zeros(x.size(0), pad, x.size(2)).type(x.type())], dim=1)\n    else:\n        raise ValueError('Side must be either \"before\", \"after\", or \"both\".')", "CodeBleu_score": 24.259260967453866}]}
{"instance_id": "a7ed1715a3df", "generation_result": [{"code": "def save_attention(attn, path)\n\t    import matplotlib.pyplot as plt\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    cax = ax.matshow(attn, cmap='bone')\n    fig.colorbar(cax)\n    plt.savefig(path+'.png')\n    plt.close\n", "CodeBleu_score": 40.07758645753644}, {"code": "def save_attention(attn, path)\n\t    import matplotlib.pyplot as plt\n    fig = plt.figure(figsize=(10, 10))\n    plt.imshow(attn, cmap='gray')\n    plt.savefig(path + '.png')\n    plt.close(fig)\n", "CodeBleu_score": 49.26511547816859}, {"code": "def save_attention(attn, path)\n\t    import matplotlib.pyplot as plt\n    fig = plt.figure(figsize=(10, 10))\n    plt.imshow(attn, cmap='viridis')\n    plt.colorbar()\n    plt.tight_layout()\n    plt.savefig(path + '.png')\n    plt.close(fig)\n", "CodeBleu_score": 50.977306253400954}]}
{"instance_id": "19375c9f44a2", "generation_result": [{"code": "def plot(array)\n\t\n    import matplotlib.pyplot as plt\n\n    fig = plt.figure(figsize=(30, 5))\n    ax = fig.add_subplot(111)\n    ax.xaxis.label.set_color('grey')\n    ax.yaxis.label.set_color('grey')\n    ax.xaxis.label.set_fontsize(23)\n    ax.yaxis.label.set_fontsize(23)\n    ax.tick_params(axis='x', colors='grey', labelsize=23)\n    ax.tick_params(axis='y', colors='grey', labelsize=23)\n    plt.plot(array)\n\n", "CodeBleu_score": 77.73415655580472}, {"code": "def plot(array)\n\t\n    import matplotlib.pyplot as plt\n\n    fig = plt.figure(figsize=(30, 5))\n    ax = fig.add_subplot(111)\n    ax.xaxis.label.set_color('grey')\n    ax.yaxis.label.set_color('grey')\n    ax.xaxis.label.set_fontsize(23)\n    ax.yaxis.label.set_fontsize(23)\n    ax.tick_params(axis='x', colors='grey', labelsize=23)\n    ax.tick_params(axis='y', colors='grey', labelsize=23)\n    plt.plot(array)\n\n", "CodeBleu_score": 77.73415655580472}, {"code": "def plot(array)\n\t\n    import matplotlib.pyplot as plt\n\n    fig = plt.figure(figsize=(30, 5))\n    ax = fig.add_subplot(111)\n    ax.xaxis.label.set_color('grey')\n    ax.yaxis.label.set_color('grey')\n    ax.xaxis.label.set_fontsize(23)\n    ax.yaxis.label.set_fontsize(23)\n    ax.tick_params(axis='x', colors='grey', labelsize=23)\n    ax.tick_params(axis='y', colors='grey', labelsize=23)\n    plt.plot(array)\n\n", "CodeBleu_score": 77.73415655580472}]}
{"instance_id": "ff94c425920c", "generation_result": [{"code": "def write_to_csv(image_name, prediction, confidence)\n\t\n    with open('predictions.csv', 'a') as csv_file:\n        fieldnames = ['Image Name', 'Prediction', 'Confidence']\n        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n        if csv_file.tell() == 0:\n            writer.writeheader()\n        writer.writerow({'Image Name': image_name, 'Prediction': prediction, 'Confidence': confidence})\n\n", "CodeBleu_score": 45.24055127348744}, {"code": "def write_to_csv(image_name, prediction, confidence)\n\t\n    # Create a dictionary with the image name, prediction, and confidence\n    data = {\n        'image_name': image_name,\n        'prediction': prediction,\n        'confidence': confidence\n    }\n    # Open the CSV file in append mode\n    with open('predictions.csv', 'a') as csv_file:\n        # Create a DictWriter object with the fieldnames\n        writer = csv.DictWriter(csv_file, fieldnames=data.keys())\n        # Write the header if the file is new\n        if csv_file.tell() == 0:\n            writer.writeheader()\n        # Write the data as a new row\n        writer.writerow(data)\n", "CodeBleu_score": 49.353997264426674}, {"code": "def write_to_csv(image_name, prediction, confidence)\n\t\n    with open(CSV_FILE, \"a\") as csv_file:\n        writer = csv.DictWriter(csv_file, fieldnames=[\"image_name\", \"prediction\", \"confidence\"])\n        if not csv_file.tell():\n            writer.writeheader()\n        writer.writerow({\"image_name\": image_name, \"prediction\": prediction, \"confidence\": confidence})\n\n", "CodeBleu_score": 47.69056413840308}]}
{"instance_id": "436161602009", "generation_result": [{"code": "def export_torchscript(model, im, file, optimize, prefix=colorstr(\"TorchScript:\"))\n\t\n    LOGGER.info(f\"{prefix} starting export with torch {torch.__version__}...\")\n    ts = torch.jit.trace(model, im)\n    LOGGER.info(f\"{prefix} tracing completed. Saved as {file}\")\n    if optimize:\n        LOGGER.info(f\"{prefix} optimizing for mobile inference...\")\n        ts = optimize_for_mobile(ts)\n        LOGGER.info(f\"{prefix} optimization completed. Saved as {file}\")\n    ts.save(file)\n    return file, None\n\n", "CodeBleu_score": 33.96994113736396}, {"code": "def export_torchscript(model, im, file, optimize, prefix=colorstr(\"TorchScript:\"))\n\t\n    LOGGER.info(f\"{prefix} Exporting model to TorchScript format...\")\n    ts = torch.jit.trace(model, im)\n    if optimize:\n        ts = optimize_for_mobile(ts)\n        file = file.with_suffix(\".ptl\")\n        LOGGER.info(f\"{prefix} Exporting optimized model to {file}...\")\n    else:\n        file = file.with_suffix(\".pt\")\n        LOGGER.info(f\"{prefix} Exporting standard TorchScript model to {file}...\")\n    ts.save(file)\n    LOGGER.info(f\"{prefix} Exported model to {file}.\")\n    return file, None\n\n", "CodeBleu_score": 36.18447509607653}, {"code": "def export_torchscript(model, im, file, optimize, prefix=colorstr(\"TorchScript:\"))\n\t\n    LOGGER.info(f\"{prefix} Exporting to TorchScript format...\")\n    if not isinstance(im, torch.Tensor):\n        im = torch.from_numpy(im)\n    if im.ndimension() == 3:\n        im = im.unsqueeze(0)\n    im = im.to(model.device)\n    if optimize:\n        LOGGER.info(f\"{prefix} Optimizing model for mobile use...\")\n        model.model.fuse()\n        model.model.eval()\n        model.model = torch.jit.trace(model.model, im)\n        model.model.save(file)\n        model.model = model.model.to(\"cpu\")\n        model.model.eval()\n        model.model = torch.jit.script(model.model)\n        model.model.save(file)\n        model.model = model.model.to(model.device)\n        model.model.eval()\n    else:\n        LOGGER.info(f\"{prefix} Saving standard TorchScript model...\")\n        model.model = torch.jit.trace(model.model, im)\n        model.model.save(file)\n        model.model = model.model.to(model.device)\n        model.model.eval()\n    LOGGER.info(f\"{prefix} Done. Saved model to {file}.\")\n    return file, None\n\n", "CodeBleu_score": 36.254482733316024}]}
{"instance_id": "78335487ac1c", "generation_result": [{"code": "def export_coreml(model, im, file, int8, half, nms, prefix=colorstr(\"CoreML:\")):\n\t    check_requirements([\"coremltools\"])\n    LOGGER.info(f\"{prefix} Exporting {file}...\")\n    if isinstance(model, torch.nn.parallel.DistributedDataParallel):\n        model = model.module  # unwrap\n    file = file.with_suffix(\".mlmodel\")  # update file suffix\n    model.eval()  # set to inference mode\n    # Trace model\n    LOGGER.info(f\"{prefix} Converting model to Torch Script...\")\n    im = torch.zeros(1, 3, *im.shape[-2:]).to(next(model.parameters()).device)\n    with torch.no_grad():\n        traced_model = torch.jit.trace(model, im)\n    # Convert to CoreML\n    LOGGER.info(f\"{prefix} Converting to CoreML...\")\n    coreml_model = ct.convert(traced_model, inputs=[ct.ImageType(name=\"input_1\", shape=im.shape)])\n    # Quantize\n    if int8:\n        LOGGER.info(f\"{prefix} Quantizing model...\")\n        coreml_model = ct.quantization.quantize_weights(coreml_model, nbits=8)\n    # Wrap for NMS\n    if nms:\n        LOGGER.info(f\"{prefix} Wrapping model for NMS support...\")\n        coreml_model = iOSModel(coreml_model)\n    # Save\n    coreml_model.save(file)\n    LOGGER.info(f\"{prefix} Saved model to {file}\")\n    return file, coreml_model\n\n", "CodeBleu_score": 44.70307240287062}, {"code": "def export_coreml(model, im, file, int8, half, nms, prefix=colorstr(\"CoreML:\")):\n\t    check_requirements(\"coremltools\")\n    LOGGER.info(f\"{prefix} Exporting {file}... \")\n    if isinstance(model, torch.nn.Module):\n        model = torch.jit.script(model)\n    elif not isinstance(model, torch.jit.ScriptModule):\n        LOGGER.info(f\"{prefix} Converting model to ScriptModule...\")\n        model = torch.jit.script(model.eval())\n    if isinstance(model, torch.jit.ScriptModule):\n        LOGGER.info(f\"{prefix} Converting ScriptModule to TorchScript...\")\n        model = torch.jit.script(model)\n    if isinstance(model, torch.jit.ScriptModule):\n        LOGGER.info(f\"{prefix} TorchScripting model...\")\n        model = torch.jit.script(model)\n    if isinstance(model, torch.jit.ScriptModule):\n        LOGGER.info(f\"{prefix} Tracing model...\")\n        model = torch.jit.trace(model, im)\n    if isinstance(model, torch.jit.ScriptModule):\n        LOGGER.info(f\"{prefix} Converting to CoreML...\")\n        model = ct.convert(model)\n    if isinstance(model, ct.models.MLModel):\n        if int8:\n            LOGGER.info(f\"{prefix} Applying int8 quantization...\")\n            model = ct.quantize_weights(model, nbits=8)\n        if half:\n            LOGGER.info(f\"{prefix} Applying fp16 quantization...\")\n            model = ct.quantize_weights(model, nbits=16)\n        if nms:\n            LOGGER.info(f\"{prefix} Wrapping model with iOSModel for NMS support...\")\n            model = ct.models.iOSModel(model)\n        file = file.with_suffix(\".mlmodel\")\n        LOGGER.info(f\"{prefix} Saving {file}...\")\n        model.save(file)\n        return file, model\n    else:\n        LOGGER.info(f\"{prefix} Export failed, check logs for errors.\")\n        return None, None\n\n", "CodeBleu_score": 33.55839014570814}, {"code": "def export_coreml(model, im, file, int8, half, nms, prefix=colorstr(\"CoreML:\")):\n\t    check_requirements(\"coremltools\")\n    LOGGER.info(f\"{prefix} Exporting model to CoreML format...\")\n    if isinstance(model, iOSModel):\n        model = model.model\n    if isinstance(model, nn.DataParallel):\n        model = model.module\n    model.eval()\n    if isinstance(im, str):\n        im = Image.open(im)\n    im = im.convert(\"RGB\")\n    im = np.array(im)\n    im = im.transpose(2, 0, 1)\n    im = np.expand_dims(im, 0)\n    im = torch.from_numpy(im)\n    im = im.to(device)\n    if half:\n        im = im.half()\n    im = im.float() / 255.0\n    file = Path(file)\n    file = file.with_suffix(\".mlmodel\")\n    if nms:\n        model = iOSModel(model, nms)\n    trace = torch.jit.trace(model, im)\n    mlmodel = ct.convert(trace, inputs=[ct.ImageType(name=\"input\", shape=im.shape)])\n    if int8:\n        mlmodel = ct.quantize_weights(mlmodel, nbits=8)\n    mlmodel.save(file)\n    LOGGER.info(f\"{prefix} Model exported to {file}\")\n    return file, mlmodel\n\n", "CodeBleu_score": 47.02922601956221}]}
{"instance_id": "80233cf71580", "generation_result": [{"code": "def generate_individual(input_ranges, individual_length)\n\t\n    individual = []\n    for i in range(individual_length):\n        individual.append(random.uniform(input_ranges[i][0], input_ranges[i][1]))\n    return individual\n", "CodeBleu_score": 47.630972577458174}, {"code": "def generate_individual(input_ranges, individual_length)\n\t\n    individual = []\n    for i in range(individual_length):\n        individual.append(random.uniform(input_ranges[i][0], input_ranges[i][1]))\n    return individual\n\n", "CodeBleu_score": 47.630972577458174}, {"code": "def generate_individual(input_ranges, individual_length)\n\t\n    individual = []\n    for i in range(individual_length):\n        individual.append(random.uniform(input_ranges[0], input_ranges[1]))\n    return individual\n", "CodeBleu_score": 45.11328150959641}]}
{"instance_id": "5ea979cb4a8e", "generation_result": [{"code": "def marginal_log_mean_coeff(self, t)\n\t\n    if self.schedule_type == \"discrete\":\n        return torch.log(self.interpolate_fn(t, self.discrete_time_steps, self.alphas_cumprod))\n    elif self.schedule_type == \"linear\":\n        alpha_t = 1 - (1 - self.alpha_start) * t / self.t_max\n        log_alpha_t = torch.log(alpha_t)\n        return log_alpha_t\n    elif self.schedule_type == \"cosine\":\n        log_alpha_t = (\n            self.log_alpha_fn(t)\n            - self.log_alpha_fn(self.t_max)\n            + torch.log(self.alpha_start)\n            + torch.log(self.alpha_end)\n        )\n        return log_alpha_t\n    else:\n        raise NotImplementedError(\n            f\"schedule_type {self.schedule_type} is not supported. Please choose from ['discrete', 'linear', 'cosine']\"\n        )\n\n", "CodeBleu_score": 38.816115533537506}, {"code": "def marginal_log_mean_coeff(self, t)\n\t\n    if self.schedule == \"discrete\":\n        return torch.log(interpolate_fn(t, self.time_steps, self.alphas_cumprod))\n    elif self.schedule == \"linear\":\n        return torch.log(\n            1e-20\n            + torch.exp(self.log_alpha_0)\n            + (self.log_alpha_t - self.log_alpha_0) * t / self.t_max\n        )\n    elif self.schedule == \"cosine\":\n        return torch.log(\n            1e-20\n            + torch.exp(self.log_alpha_0)\n            + (self.log_alpha_t - self.log_alpha_0)\n            * (torch.cos((t / self.t_max) * math.pi) + 1)\n            / 2\n        )\n    else:\n        raise NotImplementedError(f\"Unknown schedule type: {self.schedule}\")\n\n", "CodeBleu_score": 33.487860027600455}, {"code": "def marginal_log_mean_coeff(self, t)\n\t\n    if self.schedule == \"discrete\":\n        return interpolate_fn(t, self.discrete_time_steps, self.log_alpha_t)\n    elif self.schedule == \"linear\":\n        return (\n            2 * (1 - t) * self.log_alpha_t[0]\n            + (t * (t - 1) * self.log_alpha_t[0] - t * (t - 1) * self.log_alpha_t[1])\n        )\n    elif self.schedule == \"cosine\":\n        return (\n            self.log_alpha_t[0]\n            + 0.5 * (self.log_alpha_t[1] - self.log_alpha_t[0])\n            * (1 + torch.cos(math.pi * t))\n        )\n\n", "CodeBleu_score": 32.165820446508}]}
{"instance_id": "7da4fae5a278", "generation_result": [{"code": "def inverse_lambda(self, lamb)\n\t\n    if self.schedule == \"linear\":\n        t = (1 - lamb / self.max_lambda) * self.T\n        return t\n    elif self.schedule == \"discrete\":\n        return interpolate_fn(lamb, self.log_alpha_values, self.t_values)\n    else:\n        log_alpha = self.log_alpha_fn(lamb)\n        return torch.arccos(torch.exp(log_alpha)) / math.pi * self.T\n\n", "CodeBleu_score": 15.224097562674254}, {"code": "def inverse_lambda(self, lamb)\n\t\n    if self.schedule == \"linear\":\n        # linear schedule\n        beta = self.get_beta(lamb)\n        log_alpha = -torch.log(1.0 - beta)\n        log_alpha_0 = torch.log(self.alpha_0)\n        log_alpha_T = torch.log(self.alpha_T)\n        log_alpha_hat = log_alpha_0 - log_alpha\n        log_alpha_hat_T = log_alpha_0 - log_alpha_T\n        t = log_alpha_hat / log_alpha_hat_T\n        t = t.clamp(0.0, 1.0)\n        return t\n    elif self.schedule == \"discrete\":\n        # discrete schedule\n        log_alpha = self.get_log_alpha(lamb)\n        log_alpha_0 = torch.log(self.alpha_0)\n        log_alpha_T = torch.log(self.alpha_T)\n        log_alpha_hat = log_alpha_0 - log_alpha\n        log_alpha_hat_T = log_alpha_0 - log_alpha_T\n        t = log_alpha_hat / log_alpha_hat_T\n        t = t.clamp(0.0, 1.0)\n        return t\n    else:\n        # other schedules\n        log_alpha = self.get_log_alpha(lamb)\n        t = torch.arccos(1.0 - 2.0 * log_alpha) / (2.0 * math.pi)\n        t = t.clamp(0.0, 1.0)\n        return t\n\n", "CodeBleu_score": 27.70647089057807}, {"code": "def inverse_lambda(self, lamb)\n\t\n    if self.schedule == \"linear\":\n        beta_t = self.betas[lamb]\n        tmp = (1.0 - beta_t) / (1.0 - self.betas)\n        return torch.sum(tmp, dim=0) - 1.0\n    elif self.schedule == \"discrete\":\n        log_alpha = self.log_alphas[lamb]\n        log_alpha_0 = self.log_alphas[0]\n        return interpolate_fn(log_alpha, self.log_alphas, self.times)\n    else:\n        alpha = self.alphas[lamb]\n        alpha_0 = self.alphas[0]\n        return torch.acos((alpha / alpha_0).sqrt()) / math.pi * self.T\n\n", "CodeBleu_score": 20.793485818350344}]}
{"instance_id": "a519c853a2b2", "generation_result": [{"code": "def noise_pred_fn(x, t_continuous, cond=None)\n\tif model.model_type == 'unet':\n        x = model(x, t_continuous.expand(x.shape[0], -1))\n    elif model.model_type =='resunet':\n        x = model(x, t_continuous.expand(x.shape[0], -1), cond)\n    else:\n        raise NotImplementedError('model type not supported')\n    return noise_schedule.pred_fn(x)", "CodeBleu_score": 15.937328684630359}, {"code": "def noise_pred_fn(x, t_continuous, cond=None)\n\tif model.type == 'ddpm':\n        t_continuous = t_continuous.expand(x.shape[0], -1)\n        x_noisy = model(x, t_continuous)\n        noise = x - x_noisy\n        noise = noise * noise_schedule.marginal_std(t_continuous)\n        noise = noise + noise_schedule.marginal_mean(t_continuous)\n        return noise\n    elif model.type == 'ddim':\n        t_continuous = t_continuous.expand(x.shape[0], -1)\n        x_noisy = model(x, t_continuous)\n        noise = x - x_noisy\n        noise = noise * noise_schedule.marginal_std(t_continuous)\n        noise = noise + noise_schedule.marginal_mean(t_continuous)\n        return noise\n    elif model.type == 'ddim-discrete':\n        t = get_model_input_time(t_continuous)\n        x_noisy = model(x, t)\n        noise = x - x_noisy\n        noise = noise * noise_schedule.marginal_std(t_continuous)\n        noise = noise + noise_schedule.marginal_mean(t_continuous)\n        return noise\n    elif model.type == 'ddim-discrete-no-noise':\n        t = get_model_input_time(t_continuous)\n        x_noisy = model(x, t)\n        noise = x - x_noisy\n        noise = noise * noise_schedule.marginal_std(t_continuous)\n        noise = noise + noise_schedule.marginal_mean(t_continuous)\n        return noise\n    elif model.type == 'ddim-discrete-no-noise-no-mean':\n        t = get_model_input_time(t_continuous)\n        x_noisy = model(x, t)\n        noise = x - x_noisy\n        noise = noise * noise_schedule.marginal_std(t_continuous)\n        noise = noise + noise_schedule.marginal_mean(t_continuous)\n        return noise\n    elif model.type == 'ddim-discrete-no-noise-no-mean-no-std':\n        t = get_model_input_time(t_continuous)\n        x_noisy = model(x, t)\n        noise = x", "CodeBleu_score": 30.656077517096602}, {"code": "def noise_pred_fn(x, t_continuous, cond=None)\n\tmodel_input_time = get_model_input_time(t_continuous)\n    x = model(x, model_input_time, cond)\n    if model.model_type == \"unet\":\n        x = noise_schedule.marginal_std(t_continuous.expand(x.shape[0], -1)) * x\n    elif model.model_type == \"resunet\":\n        x = noise_schedule.marginal_std(t_continuous.expand(x.shape[0], -1)) * x + noise_schedule.marginal_mean(t_continuous.expand(x.shape[0], -1))\n    return x\n", "CodeBleu_score": 18.65672659522057}]}
{"instance_id": "96a9b7d54823", "generation_result": [{"code": "def dynamic_thresholding_fn(self, x0, t=None)\n\t    x0_abs = torch.abs(x0)\n    x0_abs_flatten = x0_abs.reshape(x0.size(0), -1)\n    if t is None:\n        t = torch.quantile(x0_abs_flatten, self.quantile)\n    scaling_factor = torch.clamp(t / self.quantile, min=self.min_val)\n    x0_clamped = torch.clamp(x0_abs, min=0, max=scaling_factor)\n    return x0_clamped / scaling_factor * self.scale\n", "CodeBleu_score": 27.141111960216808}, {"code": "def dynamic_thresholding_fn(self, x0, t=None)\n\t    x0_abs = torch.abs(x0)\n    x0_abs_reshape = x0_abs.reshape(x0_abs.size(0), -1)\n    x0_abs_reshape_quantile = torch.quantile(x0_abs_reshape, self.quantile)\n    x0_abs_reshape_quantile = expand_dims(x0_abs_reshape_quantile, x0.dim())\n    scale = torch.maximum(x0_abs_reshape_quantile / self.threshold, torch.ones_like(x0_abs_reshape_quantile))\n    scale = torch.clamp(scale, max=self.max_scale)\n    x0_scaled = x0 / scale\n    return x0_scaled * scale\n", "CodeBleu_score": 19.511628278458893}, {"code": "def dynamic_thresholding_fn(self, x0, t=None)\n\t    x0 = x0.reshape((-1,))\n    x0_abs = torch.abs(x0)\n    x0_abs_q = torch.quantile(x0_abs, t)\n    x0_abs_q = torch.maximum(x0_abs_q, torch.ones_like(x0_abs_q))\n    x0_abs_q = torch.clamp(x0_abs_q, max=255)\n    x0_abs_q = expand_dims(x0_abs_q, x0.dim())\n    x0_scaled = x0 / x0_abs_q\n    return x0_scaled\n\n", "CodeBleu_score": 17.943150056109385}]}
{"instance_id": "59c6ec6d7a6a", "generation_result": [{"code": "def data_prediction_fn(self, x, t)\n\t  # Calculate the threshold value\n    threshold = self.dynamic_thresholding_ratio * self.max_threshold\n    # Clamp the input tensor values within the threshold\n    clamped_x = torch.clamp(x, min=0, max=threshold)\n    # Normalize the clamped values\n    normalized_x = clamped_x / threshold\n    return normalized_x\n", "CodeBleu_score": 10.57996238368182}, {"code": "def data_prediction_fn(self, x, t)\n\t  # Calculate the threshold value based on the dynamic thresholding ratio and maximum threshold value\n    threshold = self.dynamic_thresholding_ratio * self.dynamic_thresholding_max\n    # Clamp the input tensor values within this threshold\n    clamped_x = torch.clamp(x, min=0, max=threshold)\n    # Normalize the clamped values\n    normalized_x = clamped_x / threshold\n    return normalized_x\n", "CodeBleu_score": 10.758052323916598}, {"code": "def data_prediction_fn(self, x, t)\n\t  threshold = self.threshold * self.max_threshold\n    return torch.clamp(x, -threshold, threshold) / threshold\n\n", "CodeBleu_score": 7.666945955110107}]}
{"instance_id": "65e3618468a2", "generation_result": [{"code": "def predict_eps_sigma(model, input, sigma_in, **kwargs)\n\t\n    sigma = sigma_in.view(input.shape[0], 1, 1, 1)\n    scaled_input = input * sigma\n    prediction = model(scaled_input)\n    adjusted_prediction = prediction + scaled_input\n    return adjusted_prediction\n\n", "CodeBleu_score": 20.890312108045244}, {"code": "def predict_eps_sigma(model, input, sigma_in, **kwargs)\n\t\n    sigma = sigma_in.view(input.shape[0], -1, 1, 1)\n    input = input.view(input.shape[0], -1, 1, 1)\n    input = (input - model.mu) / model.sigma\n    prediction = model(input, **kwargs)\n    prediction = prediction + sigma\n    return prediction\n\n", "CodeBleu_score": 35.35010211800254}, {"code": "def predict_eps_sigma(model, input, sigma_in, **kwargs)\n\t\n    sigma_in = sigma_in.view(-1, 1)\n    input = input.view(-1, 1)\n    input = input.repeat(1, 100)\n    sigma_in = sigma_in.repeat(1, 100)\n    sigma_in = sigma_in.view(-1, 1)\n    input = input.view(-1, 1)\n    input = input.repeat(1, 100)\n    sigma_in = sigma_in.view(-1, 1)\n    input = input.view(-1, 1)\n    input = input.repeat(1, 100)\n    sigma_in = sigma_in.view(-1, 1)\n    input = input.view(-1, 1)\n    input = input.repeat(1, 100)\n    sigma_in = sigma_in.view(-1, 1)\n    input = input.view(-1, 1)\n    input = input.repeat(1, 100)\n    sigma_in = sigma_in.view(-1, 1)\n    input = input.view(-1, 1)\n    input = input.repeat(1, 100)\n    sigma_in = sigma_in.view(-1, 1)\n    input = input.view(-1, 1)\n    input = input.repeat(1, 100)\n    sigma_in = sigma_in.view(-1, 1)\n    input = input.view(-1, 1)\n    input = input.repeat(1, 100)\n    sigma_in = sigma_in.view(-1, 1)\n    input = input.view(-1, 1)\n    input = input.repeat(1, 100)\n    sigma_in = sigma_in.view(-1, 1)\n    input = input.view(-1, 1)\n    input = input.repeat(1, 100)\n    sigma_in = sigma_in.view(-1, 1)\n    input = input.view(-1, 1)\n    input = input.repeat(1, 100)\n    sigma_in = sigma_in.view(-1, 1)\n    input = input.view(-1, 1)\n    input = input", "CodeBleu_score": 17.532961483037084}]}
{"instance_id": "b35846dfbc66", "generation_result": [{"code": "def cal_intergrand(beta_0, beta_1, taus)\n\t\n    with torch.inference_mode():\n        alpha = t2alpha_fn(beta_0, beta_1, taus)\n        log_alpha = torch.log(alpha)\n    with torch.enable_grad():\n        taus.requires_grad_()\n        log_alpha.backward()\n        grad = taus.grad\n    return alpha * grad\n\n", "CodeBleu_score": 35.30745799947791}, {"code": "def cal_intergrand(beta_0, beta_1, taus)\n\t\n    with torch.inference_mode():\n        alpha = t2alpha_fn(beta_0, beta_1, taus)\n        log_alpha = alpha.log()\n\n    with torch.enable_grad():\n        taus = taus.clone()\n        taus.requires_grad_(True)\n        log_alpha = log_alpha.clone()\n        log_alpha.backward(torch.ones_like(log_alpha))\n\n    return log_alpha.sum() * taus.grad\n\n", "CodeBleu_score": 45.391500124400366}, {"code": "def cal_intergrand(beta_0, beta_1, taus)\n\t\n    # Compute alpha and its logarithm\n    alpha = t2alpha_fn(beta_0, beta_1, taus)\n    log_alpha = torch.log(alpha)\n\n    # Enable gradient tracking for taus\n    taus.requires_grad_(True)\n\n    # Compute the gradient of log_alpha with respect to taus\n    with torch.inference_mode():\n        log_alpha.sum().backward()\n\n    # Compute the integrand\n    integrand = log_alpha * alpha.grad\n\n    # Disable gradient tracking for taus\n    taus.requires_grad_(False)\n\n    return integrand\n", "CodeBleu_score": 33.35233772856151}]}
{"instance_id": "5f3667566a10", "generation_result": [{"code": "def sample_euler_ancestral(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None)\n\t\n    if extra_args is None:\n        extra_args = model.get_default_extra_args(x)\n    if noise_sampler is None:\n        noise_sampler = default_noise_sampler(x)\n    for i, (sigma_from, sigma_to) in enumerate(zip(sigmas[:-1], sigmas[1:])):\n        denoised = model(x, sigma_from, extra_args=extra_args)\n        sigma_down, sigma_up = get_ancestral_step(sigma_from, sigma_to, eta=eta)\n        if callback is not None:\n            callback(x, denoised, sigma_down, sigma_up, i)\n        x = x - sigma_down ** 2 * to_d(x, sigma_down, denoised)\n        if sigma_up:\n            x = x + sigma_up * noise_sampler(sigma_up, sigma_down)\n    if s_noise:\n        x = x + s_noise * noise_sampler(s_noise, 0.)\n    return x\n", "CodeBleu_score": 37.498315203258336}, {"code": "def sample_euler_ancestral(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None)\n\t\n    if extra_args is None:\n        extra_args = model.get_default_extra_args()\n    if noise_sampler is None:\n        noise_sampler = default_noise_sampler\n    if disable is None:\n        disable = []\n    for i, (sigma_from, sigma_to) in enumerate(zip(sigmas[:-1], sigmas[1:])):\n        denoised = model(x, sigma_from, sigma_to, extra_args=extra_args, disable=disable)\n        sigma_down, sigma_up = get_ancestral_step(sigma_from, sigma_to, eta=eta)\n        x = x - to_d(x, sigma_down, denoised) * (sigma_from - sigma_down)\n        if callback is not None:\n            callback(i, x, sigma_from, sigma_to, denoised)\n        if sigma_up > 0:\n            x = x + noise_sampler(sigma_up, sigma_down) * s_noise\n    return x + noise_sampler(sigmas[-1], 0) * s_noise\n", "CodeBleu_score": 37.115139651855195}, {"code": "def sample_euler_ancestral(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None)\n\t\n    if extra_args is None:\n        extra_args = {}\n    if noise_sampler is None:\n        noise_sampler = default_noise_sampler\n    if callback is None:\n        callback = lambda *args, **kwargs: None\n    if disable is None:\n        disable = []\n    if not isinstance(sigmas, list):\n        sigmas = [sigmas]\n    sigmas = list(sigmas)\n    sigmas.sort(reverse=True)\n    for i, sigma in enumerate(sigmas):\n        if i == 0:\n            sigma_next = sigmas[i + 1] if i + 1 < len(sigmas) else 0\n            sigma_down, sigma_up = get_ancestral_step(sigma, sigma_next, eta)\n            x_down = x + sigma_up * noise_sampler(x)\n            x = model(x_down, sigma, **extra_args)\n        else:\n            sigma_prev = sigmas[i - 1]\n            sigma_next = sigmas[i + 1] if i + 1 < len(sigmas) else 0\n            sigma_down, sigma_up = get_ancestral_step(sigma, sigma_next, eta)\n            sigma_up_prev, _ = get_ancestral_step(sigma_prev, sigma)\n            x_down = x + sigma_up * noise_sampler(x)\n            x_up = x_down + sigma_up_prev * noise_sampler(x_down)\n            dx = to_d(x_up, sigma_prev, model(x_down, sigma, **extra_args))\n            x = x_down + dx * (sigma_up ** 2 - sigma_up_prev ** 2)\n        if 'x' not in disable:\n            callback(x=x, sigma=sigma, i=i)\n        if sigma > 0:\n            x = x + s_noise * sigma * noise_sampler(x)\n    return x\n", "CodeBleu_score": 35.14543423292951}]}
{"instance_id": "6261ded88df2", "generation_result": [{"code": "def linear_multistep_coeff(order, t, i, j)\n\t\n    if order > i:\n        raise ValueError('The order cannot be greater than the step index.')\n    def product_term(t):\n        \"\"\"The function calculates the product term for the coefficient, excluding the j-th term.\"\"\"\n        product = 1\n        for k in range(order):\n            if k!= j:\n                product *= (t - t[k])\n        return product\n    return integrate.quad(product_term, t[i], t[i+1])[0]\n\n", "CodeBleu_score": 33.20506380338444}, {"code": "def linear_multistep_coeff(order, t, i, j)\n\t\n    if order > i:\n        raise ValueError(\"The order cannot be higher than the step index.\")\n    def product_term(x):\n        \"\"\"The function computes the product term for the coefficient, excluding the j-th term.\"\"\"\n        result = 1\n        for k in range(order):\n            if k!= j:\n                result *= (x - t[i-k])\n        return result\n    return integrate.quad(product_term, t[i], t[i+1])[0]\n\n\n\n\n", "CodeBleu_score": 39.26478633385324}, {"code": "def linear_multistep_coeff(order, t, i, j)\n\t\n    if order > i:\n        raise ValueError('The order is too high for the step index.')\n    def product_term(k):\n        \"\"\"The function calculates the product term for the coefficient, excluding the j-th term.\"\"\"\n        if k == j:\n            return 1\n        else:\n            return (t[i] - t[k]) / (t[j] - t[k])\n    return integrate.quad(product_term, t[i], t[i+1])[0]\n\n\n", "CodeBleu_score": 26.754289896848952}]}
{"instance_id": "4ee025e33ab1", "generation_result": [{"code": "def sample_lcm(model, x, sigmas, extra_args=None, callback=None, disable=None, noise_sampler=None)\n\t\n    if extra_args is None:\n        extra_args = {}\n    if noise_sampler is None:\n        noise_sampler = default_noise_sampler(x)\n    if callback is None:\n        callback = lambda x: None\n    if disable is None:\n        disable = []\n    for i, sigma in enumerate(sigmas):\n        sigma_next = sigmas[i + 1] if i < len(sigmas) - 1 else 0\n        with torch.no_grad():\n            x_out = model(x, sigma, **extra_args)\n        if \"pred\" not in disable:\n            callback(x_out)\n        if sigma_next > 0:\n            x_noise = noise_sampler(sigma, sigma_next)\n            x = x_out + noise_scaling(x_noise, sigma, sigma_next)\n        else:\n            x = x_out\n    return x\n\n", "CodeBleu_score": 37.67681279208265}, {"code": "def sample_lcm(model, x, sigmas, extra_args=None, callback=None, disable=None, noise_sampler=None)\n\t\n    if extra_args is None:\n        extra_args = model.get_extra_args()\n    if noise_sampler is None:\n        noise_sampler = model.get_noise_sampler()\n    if disable is None:\n        disable = model.get_disable()\n    if callback is None:\n        callback = model.get_callback()\n    x_prev = x.clone()\n    for i, (sigma, sigma_next) in enumerate(zip(sigmas[:-1], sigmas[1:])):\n        x_prev = x.clone()\n        x = model(x, sigma, sigma_next, extra_args=extra_args, disable=disable, noise_sampler=noise_sampler)\n        if callback is not None:\n            callback(i, x, x_prev, sigma, sigma_next)\n        if sigma_next > 0:\n            x = x_prev + noise_scaling(x - x_prev, sigma_next)\n    return x\n\n", "CodeBleu_score": 35.666390274803796}, {"code": "def sample_lcm(model, x, sigmas, extra_args=None, callback=None, disable=None, noise_sampler=None)\n\t\n    if extra_args is None:\n        extra_args = {}\n    if noise_sampler is None:\n        noise_sampler = default_noise_sampler(x)\n    if disable is None:\n        disable = []\n    if callback is None:\n        callback = lambda x, sigma, sigma_next: None\n    for i, (sigma, sigma_next) in enumerate(zip(sigmas[:-1], sigmas[1:])):\n        with torch.no_grad():\n            x = model(x, sigma, **extra_args)\n        if sigma_next > 0:\n            noise_scaling = (sigma_next / sigma) ** 0.5\n            x = x + noise_sampler(sigma, sigma_next) * noise_scaling\n        callback(x, sigma, sigma_next)\n    return x\n\n", "CodeBleu_score": 34.16195246802056}]}
{"instance_id": "cb51f91da606", "generation_result": [{"code": "def append_dims(x, target_dims)\n\t    if target_dims < len(x.shape):\n        raise ValueError(f\"target_dims ({target_dims}) is less than the current dimensions ({len(x.shape)})\")\n    return x.expand(*x.shape, *((1,) * (target_dims - len(x.shape)))).detach().clone() if x.device.type == \"mps\" else x.expand(*x.shape, *((1,) * (target_dims - len(x.shape))))\n", "CodeBleu_score": 29.75477889092947}, {"code": "def append_dims(x, target_dims)\n\t    if x.dim() >= target_dims:\n        raise ValueError(f'The current dimensions of the tensor ({x.dim()}) are greater than or equal to the target dimensions ({target_dims}).')\n    return x.expand(*x.shape, *((1,) * (target_dims - x.dim()))).detach().clone() if x.device.type =='mps' else x.expand(*x.shape, *((1,) * (target_dims - x.dim())))", "CodeBleu_score": 28.948719028375958}, {"code": "def append_dims(x, target_dims)\n\t    if len(x.shape) >= target_dims:\n        raise ValueError(f\"The target dimensions {target_dims} are fewer than the current dimensions {len(x.shape)}.\")\n    else:\n        num_dims_to_append = target_dims - len(x.shape)\n        if x.device.type == \"mps\":\n            return x.expand(x.shape + (1,) * num_dims_to_append).detach().clone()\n        else:\n            return x.expand(x.shape + (1,) * num_dims_to_append)\n", "CodeBleu_score": 29.829945470025198}]}
{"instance_id": "9fac874a3bc7", "generation_result": [{"code": "def download_file(path, url, digest=None)\n\t\n    if not os.path.exists(os.path.dirname(path)):\n        os.makedirs(os.path.dirname(path))\n    if not os.path.exists(path):\n        print(\"Downloading {} to {}\".format(url, path))\n        with open(path, \"wb\") as f:\n            f.write(urlopen(url).read())\n    if digest is not None:\n        with open(path, \"rb\") as f:\n            file_hash = hashlib.sha256(f.read()).hexdigest()\n        if file_hash!= digest:\n            raise Exception(\"File hash mismatch for {}: expected {}, got {}\".format(path, digest, file_hash))\n    return path\n\n", "CodeBleu_score": 49.880556873560586}, {"code": "def download_file(path, url, digest=None)\n\t\n    if not path.parent.exists():\n        path.parent.mkdir(parents=True)\n    if not path.exists():\n        with urlopen(url) as response, path.open(\"wb\") as out_file:\n            shutil.copyfileobj(response, out_file)\n    if digest is not None and hashlib.sha256(path.read_bytes()).hexdigest()!= digest:\n        raise OSError(f\"File {path} has an incorrect hash, please remove it and try again\")\n    return path\n", "CodeBleu_score": 38.15746956596207}, {"code": "def download_file(path, url, digest=None)\n\t\n    if not Path(path).parent.exists():\n        Path(path).parent.mkdir(parents=True)\n    if not Path(path).exists():\n        print(f\"Downloading {url} to {path}\")\n        with urlopen(url) as response, open(path, 'wb') as out_file:\n            shutil.copyfileobj(response, out_file)\n    if digest is not None:\n        with open(path, 'rb') as f:\n            file_hash = hashlib.sha256(f.read()).hexdigest()\n        if file_hash!= digest:\n            raise OSError(f\"File {path} has incorrect hash. Expected {digest}, got {file_hash}\")\n    return path\n", "CodeBleu_score": 55.55037638575844}]}
{"instance_id": "f23165e897ce", "generation_result": [{"code": "def train_mode(model, mode=True)\n\t\n    previous_modes = {}\n    for name, module in model.named_modules():\n        previous_modes[name] = module.training\n    for name, module in model.named_modules():\n        module.train(mode)\n    yield\n    for name, module in model.named_modules():\n        module.train(previous_modes[name])\n\n", "CodeBleu_score": 30.67382276577807}, {"code": "def train_mode(model, mode=True)\n\t\n    if not isinstance(model, nn.Module):\n        raise TypeError('model must be an instance of nn.Module')\n    if not isinstance(mode, bool):\n        raise TypeError('mode must be a boolean')\n    original_modes = {}\n    for name, module in model.named_modules():\n        if isinstance(module, nn.RNNBase) or isinstance(module, nn.LSTM):\n            original_modes[name] = module.training\n    model.train(mode)\n    try:\n        yield\n    finally:\n        for name, module in model.named_modules():\n            if isinstance(module, nn.RNNBase) or isinstance(module, nn.LSTM):\n                module.train(original_modes[name])\n\n", "CodeBleu_score": 28.762259213753538}, {"code": "def train_mode(model, mode=True)\n\t\n    if not isinstance(model, nn.Module):\n        raise TypeError('model must be an instance of nn.Module')\n    if not isinstance(mode, bool):\n        raise TypeError('mode must be a boolean')\n    training_modes = {}\n    for name, module in model.named_modules():\n        if isinstance(module, nn.Module):\n            training_modes[name] = module.training\n    try:\n        model.train(mode)\n        yield\n    finally:\n        for name, module in model.named_modules():\n            if isinstance(module, nn.Module):\n                module.train(training_modes[name])\n", "CodeBleu_score": 29.506090096152786}]}
{"instance_id": "c6d59c6cb8e1", "generation_result": [{"code": "def ema_update(model, averaged_model, decay)\n\t\n    for param_name, param in model.named_parameters():\n        if param_name in averaged_model.state_dict():\n            param_avg = averaged_model.state_dict()[param_name]\n            param_avg.data.copy_(param_avg.data * decay + param.data * (1 - decay))\n        else:\n            print(f\"Warning: parameter {param_name} of model was not in averaged_model\")\n\n    for buffer_name, buffer in model.named_buffers():\n        if buffer_name in averaged_model.state_dict():\n            buffer_avg = averaged_model.state_dict()[buffer_name]\n            buffer_avg.data.copy_(buffer)\n        else:\n            print(f\"Warning: buffer {buffer_name} of model was not in averaged_model\")\n\n", "CodeBleu_score": 35.82868305418936}, {"code": "def ema_update(model, averaged_model, decay)\n\t\n    for param, averaged_param in zip(model.parameters(), averaged_model.parameters()):\n        averaged_param.data.copy_(averaged_param.data * decay + param.data * (1 - decay))\n\n    for buffer, averaged_buffer in zip(model.buffers(), averaged_model.buffers()):\n        averaged_buffer.data.copy_(averaged_buffer.data * decay + buffer.data * (1 - decay))\n\n", "CodeBleu_score": 30.294446786843775}, {"code": "def ema_update(model, averaged_model, decay)\n\t\n    for name, param in model.named_parameters():\n        if param.requires_grad:\n            averaged_param = averaged_model.state_dict()[name]\n            param_copy = torch.empty_like(averaged_param)\n            torch.copy_(param, param_copy)\n            averaged_param.mul_(decay).add_(param_copy, alpha=1 - decay)\n    for name, buffer in model.named_buffers():\n        if buffer.requires_grad:\n            averaged_buffer = averaged_model.state_dict()[name]\n            buffer_copy = torch.empty_like(averaged_buffer)\n            torch.copy_(buffer, buffer_copy)\n            averaged_buffer.mul_(decay).add_(buffer_copy, alpha=1 - decay)\n", "CodeBleu_score": 42.557745856293614}]}
{"instance_id": "600c39a30798", "generation_result": [{"code": "def rand_log_logistic(shape, loc=0., scale=1., min_value=0., max_value=float('inf'), device='cpu', dtype=torch.float32)\n\t\n    min_value = torch.as_tensor(min_value, device=device, dtype=dtype)\n    max_value = torch.as_tensor(max_value, device=device, dtype=dtype)\n    u = torch.rand(shape, device=device, dtype=dtype)\n    u = (u * (max_value.logit() - min_value.logit()) + min_value.logit()).logit()\n    return (u * scale + loc).exp()\n\n", "CodeBleu_score": 36.59111003522625}, {"code": "def rand_log_logistic(shape, loc=0., scale=1., min_value=0., max_value=float('inf'), device='cpu', dtype=torch.float32)\n\t\n    min_value = torch.as_tensor(min_value, device=device, dtype=dtype)\n    max_value = torch.as_tensor(max_value, device=device, dtype=dtype)\n    u = torch.rand(shape, device=device, dtype=dtype)\n    u = min_value.logit(u)\n    u = u.mul(scale).add(loc)\n    u = u.exp()\n    u = u.clamp(min=min_value, max=max_value)\n    return u\n", "CodeBleu_score": 33.58658102994655}, {"code": "def rand_log_logistic(shape, loc=0., scale=1., min_value=0., max_value=float('inf'), device='cpu', dtype=torch.float32)\n\t\n    min_value, max_value = torch.as_tensor(min_value, device=device, dtype=dtype), torch.as_tensor(max_value, device=device, dtype=dtype)\n    min_cdf, max_cdf = torch.sigmoid(min_value.log() - loc / scale), torch.sigmoid(max_value.log() - loc / scale)\n    u = (max_cdf - min_cdf) * torch.rand(shape, device=device, dtype=dtype) + min_cdf\n    return (u.logit() * scale + loc).exp()\n\n", "CodeBleu_score": 36.10400472460327}]}
{"instance_id": "75e2def2467f", "generation_result": [{"code": "def get_activation(activation: Literal[\"elu\", \"snake\", \"none\"], antialias=False, channels=None) -> nn.Module\n\t\n    if activation == \"elu\":\n        return ELU(alpha=1.0, inplace=True)\n    elif activation == \"snake\":\n        return SnakeBeta(channels)\n    elif activation == \"none\":\n        return Identity()\n    else:\n        raise ValueError(f\"Invalid activation: {activation}\")\n", "CodeBleu_score": 41.58981336115525}, {"code": "def get_activation(activation: Literal[\"elu\", \"snake\", \"none\"], antialias=False, channels=None) -> nn.Module\n\t\n    if activation == \"elu\":\n        return ELU()\n    elif activation == \"snake\":\n        return SnakeBeta(channels=channels)\n    elif activation == \"none\":\n        return Identity()\n    else:\n        raise ValueError(f\"Invalid activation type: {activation}\")\n\n", "CodeBleu_score": 39.04137653269673}, {"code": "def get_activation(activation: Literal[\"elu\", \"snake\", \"none\"], antialias=False, channels=None) -> nn.Module\n\t\n    if activation == \"elu\":\n        return ELU(alpha=1.0)\n    if activation == \"snake\":\n        return SnakeBeta(channels)\n    if activation == \"none\":\n        return Identity()\n    raise ValueError(f\"Unknown activation: {activation}\")\n\n", "CodeBleu_score": 36.77321729533651}]}
{"instance_id": "ef1714c79e51", "generation_result": [{"code": "def apply_rotary_pos_emb(t, freqs, scale = 1)\n\t    t_rotated = t.to(freqs.dtype)\n    t_unrotated = t_rotated[:, :, :t_rotated.shape[-1] // 2]\n    t_rotated = t_rotated[:, :, t_rotated.shape[-1] // 2:]\n    freqs = freqs.to(t.dtype)\n    freqs = freqs.view(*((1,) * (t.ndim - 2)), -1, 1)\n    t_rotated = t_rotated * freqs * scale\n    t_rotated = rotate_half(t_rotated)\n    return torch.cat((t_unrotated, t_rotated), dim = -1)\n\n", "CodeBleu_score": 20.367031857774563}, {"code": "def apply_rotary_pos_emb(t, freqs, scale = 1)\n\t    t = t.to(freqs)\n    t_rotated = rotate_half(t)\n    t_unrotated = t[..., : t.shape[-1] // 2]\n    t_rotated = t_rotated[..., t.shape[-1] // 2 :]\n    freqs = freqs * scale\n    freqs = rearrange(freqs, '... d ->... d 1')\n    t_rotated = (t_rotated * freqs.cos()) + (freqs.sin() * t_unrotated)\n    return torch.cat((t_rotated, t_unrotated), dim = -1)\n", "CodeBleu_score": 22.135115592719178}, {"code": "def apply_rotary_pos_emb(t, freqs, scale = 1)\n\t    t = t.to(freqs)\n    t_rotated = rotate_half(t)\n    t_unrotated = t[..., :1]\n    t_rotated = torch.cat((t_rotated, t_unrotated), dim = -1)\n    freqs = freqs.to(t)\n    freqs = freqs * scale\n    freqs = rearrange(freqs, '... d ->... d 1')\n    sin, cos = freqs.sin(), freqs.cos()\n    t_rotated = torch.stack((t_rotated[..., 0] * cos, t_rotated[..., 1] * sin), dim = -1)\n    t_rotated = rearrange(t_rotated, '... j d ->... (j d)')\n    return torch.cat((t_rotated, t_unrotated), dim = -1)\n", "CodeBleu_score": 32.71138898644829}]}
{"instance_id": "a7a5f178c6f6", "generation_result": [{"code": "def idx2vq(self, idx, dim=-1)\n\t\n    return self.codebook.index_select(dim, idx).movedim(dim, -1)\n\n", "CodeBleu_score": 20.724301952363884}, {"code": "def idx2vq(self, idx, dim=-1)\n\t\n    if dim == -1:\n        return self.codebook[idx]\n    else:\n        return self.codebook[idx].movedim(-1, dim)\n", "CodeBleu_score": 31.227505813308344}, {"code": "def idx2vq(self, idx, dim=-1)\n\t\n    return self.codebook.index_select(dim, idx).movedim(self.codebook.dim() - 1, dim)\n\n", "CodeBleu_score": 23.66722297799102}]}
{"instance_id": "4104c041fb20", "generation_result": [{"code": "def encode(self, x, quantize=False)\n\t   z_e_x = x.contiguous().view(-1, x.size(-1)) if len(x.shape) > 2 else x\n    z_q_x, indices = self.vq(z_e_x, self.codebook.weight.detach())\n    vq_loss, commit_loss = None, None\n    if self.ema_loss and self.training:\n        self._updateEMA(z_e_x.detach(), indices.detach())\n    # pick the graded embeddings after updating the codebook in order to have a more accurate commitment loss\n    z_q_x_grd = torch.index_select(self.codebook.weight, dim=0, index=indices)\n    if quantize:\n        vq_loss = (z_q_x_grd - z_e_x.detach()).pow(2).mean()\n        commit_loss = (z_e_x - z_q_x_grd.detach()).pow(2).mean()\n        return z_q_x.view(x.shape), (vq_loss, commit_loss), indices.view(x.shape[:-1])\n    else:\n        return z_e_x.view(x.shape)\n\n", "CodeBleu_score": 26.304424210430227}, {"code": "def encode(self, x, quantize=False)\n\t   z_e_x = x.contiguous().view(-1, x.size(-1)) if len(x.shape) > 2 else x\n    z_q_x, indices = self.vq(z_e_x, self.codebook.weight.detach())\n    vq_loss, commit_loss = None, None\n    if self.ema_loss and self.training:\n        self._updateEMA(z_e_x.detach(), indices.detach())\n    # pick the graded embeddings after updating the codebook in order to have a more accurate commitment loss\n    z_q_x_grd = torch.index_select(self.codebook.weight, dim=0, index=indices)\n    if quantize:\n        vq_loss = (z_q_x_grd - z_e_x.detach()).pow(2).mean()\n        commit_loss = (z_e_x - z_q_x_grd.detach()).pow(2).mean()\n        return z_q_x.view(x.shape), (vq_loss, commit_loss), indices.view(x.shape[:-1])\n    return z_e_x.view(x.shape)\n\n", "CodeBleu_score": 26.25474977037765}, {"code": "def encode(self, x, quantize=False)\n\t   z_e_x = x.contiguous().view(-1, x.size(-1)) if len(x.shape) > 2 else x\n    z_q_x, indices = self.vq(z_e_x, self.codebook.weight.detach())\n    vq_loss, commit_loss = None, None\n    if self.ema_loss and self.training:\n        self._updateEMA(z_e_x.detach(), indices.detach())\n    # pick the graded embeddings after updating the codebook in order to have a more accurate commitment loss\n    z_q_x_grd = torch.index_select(self.codebook.weight, dim=0, index=indices)\n    if quantize:\n        vq_loss = (z_q_x_grd - z_e_x.detach()).pow(2).mean()\n        commit_loss = (z_e_x - z_q_x_grd.detach()).pow(2).mean()\n        return z_q_x.view(x.shape), (vq_loss, commit_loss), indices.view(x.shape[:-1])\n    else:\n        return z_q_x.view(x.shape)\n\n", "CodeBleu_score": 26.304424210430227}]}
{"instance_id": "403c52ba2d3b", "generation_result": [{"code": "def ema_scope(self, context=None)\n\t\n    if self.ema_decay > 0:\n        self.store(self.parameters())\n        self.copy_to(self.ema_model)\n        if context is not None:\n            logpy.info(context)\n    try:\n        yield\n    finally:\n        if self.ema_decay > 0:\n            self.restore(self.parameters())\n            if context is not None:\n                logpy.info(context)\n", "CodeBleu_score": 59.08935787809364}, {"code": "def ema_scope(self, context=None)\n\t\n    if self.use_ema:\n        self.store(self.parameters())\n        self.copy_to(self)\n        if context is not None:\n            logpy.info(f\"EMA: {context} - Applying EMA weights.\")\n    yield\n    if self.use_ema:\n        self.restore(self.parameters())\n        if context is not None:\n            logpy.info(f\"EMA: {context} - Restoring original weights.\")", "CodeBleu_score": 57.077348486516165}, {"code": "def ema_scope(self, context=None)\n\t\n    if self.use_ema:\n        self.store(self.parameters())\n        self.copy_to(self.shadow)\n        if context is not None:\n            context.log(f\"EMA enabled. Switched to EMA weights.\")\n    yield\n    if self.use_ema:\n        self.restore(self.parameters())\n        if context is not None:\n            context.log(f\"EMA disabled. Switched to original weights.\")\n", "CodeBleu_score": 56.68084812668176}]}
{"instance_id": "94b7467f98e4", "generation_result": [{"code": "def post_attention(self, attn, x, gate_msa, shift_mlp, scale_mlp, gate_mlp)\n\t\n    x = x + attn * gate_msa.unsqueeze(2)\n    x = self.norm2(x)\n    x = modulate(x, shift_mlp, scale_mlp)\n    x = self.mlp(x)\n    x = x * gate_mlp.unsqueeze(2)\n    return x\n", "CodeBleu_score": 47.75443886407091}, {"code": "def post_attention(self, attn, x, gate_msa, shift_mlp, scale_mlp, gate_mlp)\n\t\n    x = x + attn * gate_msa.unsqueeze(2)\n    x = self.norm2(x)\n    x = modulate(x, shift_mlp(x), scale_mlp(x))\n    x = self.mlp(x) * gate_mlp.unsqueeze(2)\n    return x\n", "CodeBleu_score": 48.5889425909312}, {"code": "def post_attention(self, attn, x, gate_msa, shift_mlp, scale_mlp, gate_mlp)\n\t\n    x = x + attn * gate_msa\n    x = self.norm1(x)\n    x = modulate(x, shift_mlp, scale_mlp)\n    x = self.norm2(x)\n    x = self.mlp(x) * gate_mlp\n    return x\n", "CodeBleu_score": 41.01633606840146}]}
{"instance_id": "d9fdc05542a2", "generation_result": [{"code": "def _block_mixing(context, x, context_block, x_block, c)\n\t\n    qkv_context, qkv_x = context_block.pre_attention(context), x_block.pre_attention(x)\n    qkv = torch.cat((qkv_context, qkv_x), dim=2)\n    qkv = optimized_attention(qkv, c)\n    qkv_context, qkv_x = torch.split(qkv, [qkv_context.shape[2], qkv_x.shape[2]], dim=2)\n    context = context_block.post_attention(qkv_context) if not context_block.pre_only else qkv_context\n    x = x_block.post_attention(qkv_x) if not x_block.pre_only else qkv_x\n    return context, x\n\n", "CodeBleu_score": 19.93731260564422}, {"code": "def _block_mixing(context, x, context_block, x_block, c)\n\t\n    qkv_context = context_block.pre_attention(context)\n    qkv_x = x_block.pre_attention(x)\n    qkv = torch.cat([qkv_context, qkv_x], dim=1)\n    qkv = attention.optimized_attention(qkv[0], qkv[1], qkv[2], c)\n    context, x = split_qkv(qkv, qkv_context[0].shape[-1], qkv_x[0].shape[-1])\n    context = context_block.post_attention(context)\n    x = x_block.post_attention(x)\n    return context, x\n\n", "CodeBleu_score": 14.20629184372198}, {"code": "def _block_mixing(context, x, context_block, x_block, c)\n\t\n    qkv = context_block.pre_attention(context)\n    qkv = torch.cat((qkv, x_block.pre_attention(x)), dim=0)\n    qkv = optimized_attention(qkv, c)\n    context, x = split_qkv(qkv, context.shape[1], x.shape[1])\n    if context_block.pre_only:\n        context = context_block.post_attention(context)\n    else:\n        x = x_block.post_attention(x)\n    return context, x\n\n", "CodeBleu_score": 18.25721644847715}]}
{"instance_id": "b219bce30160", "generation_result": [{"code": "def get_timestep_embedding(timesteps, embedding_dim)\n\t\n    if timesteps.dim()!= 1:\n        raise ValueError(f\"Timesteps must be a 1D tensor, but got {timesteps.shape}\")\n    timesteps = timesteps.float()\n    num_timesteps = timesteps.shape[0]\n    half_dim = embedding_dim // 2\n    emb = torch.cat(\n        [\n            torch.sin(timesteps[:, None] * torch.exp(torch.arange(0, half_dim, device=timesteps.device) * -(math.log(10000.0) / half_dim))),\n            torch.cos(timesteps[:, None] * torch.exp(torch.arange(0, half_dim, device=timesteps.device) * -(math.log(10000.0) / half_dim))),\n        ],\n        dim=-1,\n    )\n    if embedding_dim % 2 == 1:\n        emb = F.pad(emb, (0, 1), value=0.0)\n    return emb\n\n", "CodeBleu_score": 45.87128593373868}, {"code": "def get_timestep_embedding(timesteps, embedding_dim)\n\t\n    timesteps = timesteps.float()\n    assert timesteps.dim() == 1, 'Timesteps input must be one-dimensional'\n    half_dim = embedding_dim // 2\n    emb = math.log(10000) / (half_dim - 1)\n    emb = torch.exp(torch.arange(half_dim, dtype=torch.float) * -emb)\n    emb = torch.arange(len(timesteps), dtype=torch.float).unsqueeze(1) * emb.unsqueeze(0)\n    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1).to(timesteps.device)\n    if embedding_dim % 2 == 1:\n        emb = F.pad(emb, (0, 1), 'constant', 0)\n    return emb", "CodeBleu_score": 66.30157739929159}, {"code": "def get_timestep_embedding(timesteps, embedding_dim)\n\t\n    assert len(timesteps.shape) == 1, \"Timesteps must be one-dimensional\"\n    half_dim = embedding_dim // 2\n    emb = math.log(10000) / (half_dim - 1)\n    emb = torch.exp(torch.arange(half_dim, dtype=torch.float) * -emb)\n    emb = torch.arange(timesteps.shape[0], dtype=torch.float).unsqueeze(1) * emb.unsqueeze(0)\n    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1).to(timesteps.device)\n    if embedding_dim % 2 == 1:\n        emb = F.pad(emb, (0, 1), \"constant\", value=0)\n    return emb\n\n", "CodeBleu_score": 68.4216900178773}]}
{"instance_id": "6ae8453c86be", "generation_result": [{"code": "def pytorch_attention(q, k, v)\n\t\n    try:\n        scale = (int(q.shape[-1])**(-0.5))\n        q = q.transpose(0, 1)\n        k = k.transpose(0, 1)\n        v = v.transpose(0, 1)\n\n        q = q.contiguous().view(q.shape[0] * q.shape[1], q.shape[2])\n        k = k.contiguous().view(k.shape[0] * k.shape[1], k.shape[2])\n        v = v.contiguous().view(v.shape[0] * v.shape[1], v.shape[2])\n\n        out = torch.nn.functional.scaled_dot_product_attention(q, k, v, None, scale, None, False, False, False)\n        out = out.view(q.shape[0], q.shape[1], out.shape[-1])\n        out = out.transpose(0, 1)\n        return out\n    except model_management.OOM_EXCEPTION:\n        return slice_attention(q, k, v)\n", "CodeBleu_score": 35.1565052381907}, {"code": "def pytorch_attention(q, k, v)\n\t\n    try:\n        r1 = torch.zeros_like(k, device=q.device)\n        scale = (int(q.shape[-1])**(-0.5))\n\n        q = q.transpose(1,2)\n        k = k.transpose(1,2)\n        v = v.transpose(1,2)\n\n        q = q.reshape(q.shape[0], q.shape[1], -1)\n        k = k.reshape(k.shape[0], k.shape[1], -1)\n        v = v.reshape(v.shape[0], v.shape[1], -1)\n\n        r1 = torch.bmm(q, k) * scale\n\n        r2 = torch.nn.functional.softmax(r1, dim=2).permute(0,2,1)\n\n        r1 = torch.bmm(v, r2)\n\n        r1 = r1.reshape(r1.shape[0], r1.shape[2], -1)\n        r1 = r1.transpose(1,2)\n\n        return r1\n    except model_management.OOM_EXCEPTION:\n        return slice_attention(q, k, v)\n", "CodeBleu_score": 30.150755507184137}, {"code": "def pytorch_attention(q, k, v)\n\t\n    try:\n        scale = (int(q.shape[-1])**(-0.5))\n        r1 = torch.nn.functional.scaled_dot_product_attention(q.transpose(0,1), k.transpose(0,1), v.transpose(0,1), None, scale, False)\n        return r1.transpose(0,1)\n    except model_management.OOM_EXCEPTION:\n        logging.warning(\"out of memory error, falling back to slice attention\")\n        return slice_attention(q, k, v)", "CodeBleu_score": 25.12916481591954}]}
{"instance_id": "fe4433c987ad", "generation_result": [{"code": "def apply_control(h, control, name)\n\t\n    try:\n        if control and name in control:\n            h.add(control.pop(name))\n    except:\n        logging.warning('Failed to add control %s to %s.' % (name, h.name))\n    return h\n", "CodeBleu_score": 24.51144359044021}, {"code": "def apply_control(h, control, name)\n\t\n    if control and name in control:\n        control_value = control.pop(name)\n        if control_value:\n            try:\n                h[name] = control_value\n            except:\n                logging.warning(\"Could not add control value {} to header {}.\".format(control_value, h))\n    return h\n", "CodeBleu_score": 40.72902574070095}, {"code": "def apply_control(h, control, name)\n\t\n    if control and name in control:\n        try:\n            h.add_control(control.pop(name))\n        except ValueError as e:\n            logging.warning(\"Failed to add control %s to %s: %s\", name, h.name, e)\n    return h\n\n", "CodeBleu_score": 30.309641659635677}]}
{"instance_id": "56da4547c101", "generation_result": [{"code": "def get_codebook_entry(self, indices, shape)\n\t\n    one_hot = F.one_hot(indices, self.num_embeddings).float()\n    quantized = torch.matmul(one_hot, self.embedding.weight)\n    if shape is not None:\n        quantized = quantized.view(shape)\n        quantized = quantized.permute(0, 2, 1)\n    return quantized\n", "CodeBleu_score": 33.50091042028913}, {"code": "def get_codebook_entry(self, indices, shape)\n\t\n    # Create a one-hot encoding of the indices\n    one_hot = torch.zeros(indices.size(0), self.num_embeddings, device=indices.device)\n    one_hot.scatter_(1, indices.unsqueeze(1), 1)\n    # Multiply the one-hot encoding with the embedding weights to get the quantized vectors\n    quantized = torch.matmul(one_hot, self.embedding.weight)\n    # If a shape is provided, reshape and permute the quantized vectors to match the original input shape\n    if shape is not None:\n        quantized = quantized.view(shape)\n        quantized = quantized.permute(0, 2, 1)\n    return quantized\n", "CodeBleu_score": 41.96857638827455}, {"code": "def get_codebook_entry(self, indices, shape)\n\t\n    indices = indices.unsqueeze(-1)\n    one_hot = torch.zeros(indices.size(0), self.codebook_size, device=indices.device)\n    one_hot.scatter_(1, indices, 1)\n    quantized = torch.matmul(one_hot, self.codebook_embeddings.transpose(0, 1))\n    if shape is not None:\n        quantized = quantized.view(shape)\n        quantized = quantized.permute(0, 3, 1, 2).contiguous()\n    return quantized\n", "CodeBleu_score": 29.868380031869172}]}
{"instance_id": "548f8a1fb37d", "generation_result": [{"code": "def make_noise(self)\n\t\n    self.noises = [torch.randn(1, 1, 4, 4, device=self.device)]\n    for i in range(3, self.log_size + 1):\n        for _ in range(2):\n            self.noises.append(torch.randn(1, 1, 2 ** i, 2 ** i, device=self.device))\n    return self.noises\n", "CodeBleu_score": 45.279708111818216}, {"code": "def make_noise(self)\n\t\n    noise = torch.randn(1, 1, 4, 4, device=self.device)\n    noises = [noise]\n    for i in range(3, self.log_size + 1):\n        for _ in range(2):\n            noises.append(F.interpolate(noise, (2 ** i, 2 ** i), mode='nearest'))\n    return noises\n", "CodeBleu_score": 46.27015415138984}, {"code": "def make_noise(self)\n\t\n    noise = []\n    for i in range(self.log_size):\n        noise.append(torch.randn(4, 4, 4, device=self.device))\n        for _ in range(2):\n            noise.append(torch.randn(4 * 2 ** i, 4 * 2 ** i, 4 * 2 ** i, device=self.device))\n    return noise\n", "CodeBleu_score": 33.48472988058964}]}
{"instance_id": "42fd805563c6", "generation_result": [{"code": "def mean_latent(self, num_latent)\n\t\n    # Create random latent inputs\n    latents = torch.randn(num_latent, self.latent_size, device=self.device)\n    # Process the latent inputs through the style MLP\n    style = self.style_mlp(latents)\n    # Compute the mean of the processed latents along the first dimension\n    mean_latent = torch.mean(style, dim=0, keepdim=True)\n    return mean_latent\n", "CodeBleu_score": 30.677734731440026}, {"code": "def mean_latent(self, num_latent)\n\t\n    latent_in = torch.randn(num_latent, self.latent_size, device=self.device)\n    latent_out = self.style_mlp(latent_in)\n    mean_latent = latent_out.mean(0)\n    return mean_latent\n\n", "CodeBleu_score": 30.801938733005763}, {"code": "def mean_latent(self, num_latent)\n\t\n    latent_in = torch.randn(num_latent, self.latent_size, device=self.device)\n    latent_out = self.style_mlp(latent_in)\n    mean_latent = latent_out.mean(0)\n    return mean_latent\n\n", "CodeBleu_score": 30.801938733005763}]}
{"instance_id": "2f485fd924af", "generation_result": [{"code": "def color_jitter(img, shift)\n\t\n    # shift is a tuple of 3 values, one for each channel\n    # shift is a tuple of 3 values, one for each channel\n    # shift is a tuple of 3 values, one for each channel\n    # shift is a tuple of 3 values, one for each channel\n    # shift is a tuple of 3 values, one for each channel\n    # shift is a tuple of 3 values, one for each channel\n    # shift is a tuple of 3 values, one for each channel\n    # shift is a tuple of 3 values, one for each channel\n    # shift is a tuple of 3 values, one for each channel\n    # shift is a tuple of 3 values, one for each channel\n    # shift is a tuple of 3 values, one for each channel\n    # shift is a tuple of 3 values, one for each channel\n    # shift is a tuple of 3 values, one for each channel\n    # shift is a tuple of 3 values, one for each channel\n    # shift is a tuple of 3 values, one for each channel\n    # shift is a tuple of 3 values, one for each channel\n    # shift is a tuple of 3 values, one for each channel\n    # shift is a tuple of 3 values, one for each channel\n    # shift is a tuple of 3 values, one for each channel\n    # shift is a tuple of 3 values, one for each channel\n    # shift is a tuple of 3 values, one for each channel\n    # shift is a tuple of 3 values, one for each channel\n    # shift is a tuple of 3 values, one for each channel\n    # shift is a tuple of 3 values, one for each channel\n    # shift is a tuple of 3 values, one for each channel\n    # shift is a tuple of 3 values, one for each channel\n    # shift is a tuple of 3 values, one for each channel\n    # shift is a tuple of 3 values, one for each channel\n    # shift is a tuple of 3 values, one for each channel\n    # shift is a tuple of 3 values, one for each channel\n    # shift is a tuple of 3 values, one for each channel\n    # shift is a tuple of 3 values, one for each channel\n    # shift is a tuple of 3 values, one for each channel\n    # shift is a tuple of 3 values, one for each channel\n    #", "CodeBleu_score": 2.393018018018018}, {"code": "def color_jitter(img, shift)\n\t\n    # Randomly shift the RGB values of the image within the specified range\n    jitter = np.random.uniform(-shift, shift, (3,))\n    # Add the jitter values to the image\n    img = img + jitter\n    # Clip the values to ensure they are between 0 and 1\n    img = np.clip(img, 0, 1)\n    return img\n\n", "CodeBleu_score": 55.52996967577801}, {"code": "def color_jitter(img, shift)\n\t\n    # shift = np.random.randint(-10, 10, 3)\n    # print(shift)\n    # print(img.shape)\n    img = img + shift\n    img = np.clip(img, 0, 1)\n    return img\n\n", "CodeBleu_score": 38.56808990855899}]}
{"instance_id": "c87ccf1fbf48", "generation_result": [{"code": "def feed_data(self, data)\n\t\n    if self.device == \"cpu\":\n        self.lq = data[0].to(self.device)\n    else:\n        with torch.cuda.device(self.device):\n            self.lq = data[0].to(self.device)\n    if len(data) > 1:\n        if self.device == \"cpu\":\n            self.gt = data[1].to(self.device)\n        else:\n            with torch.cuda.device(self.device):\n                self.gt = data[1].to(self.device)\n    if len(data) > 2:\n        if self.device == \"cpu\":\n            self.leye = data[2].to(self.device)\n            self.reye = data[3].to(self.device)\n            self.mouth = data[4].to(self.device)\n        else:\n            with torch.cuda.device(self.device):\n                self.leye = data[2].to(self.device)\n                self.reye = data[3].to(self.device)\n                self.mouth = data[4].to(self.device)\n\n", "CodeBleu_score": 27.188030242336957}, {"code": "def feed_data(self, data)\n\t\n    self.lq = data['lq'].to(self.device)\n    if 'gt' in data:\n        self.gt = data['gt'].to(self.device)\n    else:\n        self.gt = None\n    if 'leye' in data:\n        self.leye = data['leye'].to(self.device)\n    else:\n        self.leye = None\n    if'reye' in data:\n        self.reye = data['reye'].to(self.device)\n    else:\n        self.reye = None\n    if'mouth' in data:\n        self.mouth = data['mouth'].to(self.device)\n    else:\n        self.mouth = None\n    return\n\n\n\n\n", "CodeBleu_score": 48.83498760775365}, {"code": "def feed_data(self, data)\n\t\n    # If the data is a dictionary, assign the 'lq' data to self.lq and, if present, assign the 'gt' data to self.gt.\n    if isinstance(data, dict):\n        self.lq = data['lq'].to(self.device)\n        if 'gt' in data.keys():\n            self.gt = data['gt'].to(self.device)\n    # If the data is a tensor, assign the data to self.lq.\n    elif isinstance(data, torch.Tensor):\n        self.lq = data.to(self.device)\n    # If the data is a list, assign the first element to self.lq and, if present, assign the second element to self.gt.\n    elif isinstance(data, list):\n        self.lq = data[0].to(self.device)\n        if len(data) > 1:\n            self.gt = data[1].to(self.device)\n    # If the data is a tuple, assign the first element to self.lq and, if present, assign the second element to self.gt.\n    elif isinstance(data, tuple):\n        self.lq = data[0].to(self.device)\n        if len(data) > 1:\n            self.gt = data[1].to(self.device)\n    # If the data is a numpy array, convert it to a tensor and assign it to self.lq.\n    elif isinstance(data, np.ndarray):\n        self.lq = torch.from_numpy(data).to(self.device)\n    # If the data is a PIL image, convert it to a tensor and assign it to self.lq.\n    elif isinstance(data, Image.Image):\n        self.lq = transforms.ToTensor()(data).to(self.device)\n    # If the data is a string, assume it is a path to an image file, load the image, and assign it to self.lq.\n    elif isinstance(data, str):\n        self.lq = transforms.ToTensor()(Image.open(data).convert('RGB')).to(self.device)\n    # If the data is a number, assume it is a grayscale pixel value, convert it to a tensor, and assign it to self.lq.\n    elif isinstance(data, (int, float)):\n        self.lq = torch.Tensor([data]).to(self.device)\n    # If", "CodeBleu_score": 38.27808913153146}]}
{"instance_id": "8f8c7cb13037", "generation_result": [{"code": "def construct_img_pyramid(self)\n\t\n    img_pyramid = []\n    img_pyramid.insert(0, self.img_gt)\n    for i in range(self.num_scales - 1):\n        img_pyramid.insert(0, F.interpolate(img_pyramid[0], scale_factor=0.5, mode='bilinear', align_corners=True))\n    return img_pyramid\n", "CodeBleu_score": 30.07275471275202}, {"code": "def construct_img_pyramid(self)\n\t\n    # Construct image pyramid\n    img_pyramid = [self.img_gt]\n    for i in range(self.pyramid_levels - 1):\n        img_pyramid.insert(0, F.interpolate(img_pyramid[0], scale_factor=0.5, mode='bilinear', align_corners=False))\n    return img_pyramid\n\n", "CodeBleu_score": 25.883154088667425}, {"code": "def construct_img_pyramid(self)\n\t\n    # Construct the pyramid\n    pyramid = []\n    # Start with the ground truth image\n    img_pyramid_gt = self.img_gt\n    # Iteratively downscale the image\n    for i in range(self.pyramid_depth):\n        # Downscale the image\n        img_pyramid_gt = F.interpolate(img_pyramid_gt, scale_factor=0.5, mode='bilinear', align_corners=False)\n        # Insert the image at the beginning of the pyramid list\n        pyramid.insert(0, img_pyramid_gt)\n    # Return the constructed image pyramid\n    return pyramid", "CodeBleu_score": 42.331900609915394}]}
{"instance_id": "51247274d40c", "generation_result": [{"code": "def save(self, epoch, current_iter)\n\t\n    # Save the generator and discriminator networks.\n    self.save_network(self.netG, 'G', epoch, current_iter)\n    self.save_network(self.ema_netG, 'EMA_G', epoch, current_iter)\n    self.save_network(self.netD, 'D', epoch, current_iter)\n    if self.facial_discriminator:\n        self.save_network(self.netD_eye, 'D_eye', epoch, current_iter)\n        self.save_network(self.netD_mouth, 'D_mouth', epoch, current_iter)\n    # Save the training state.\n    self.save_training_state(epoch, current_iter)\n", "CodeBleu_score": 52.83280762498327}, {"code": "def save(self, epoch, current_iter)\n\t\n    self.save_network(self.netG, 'G', epoch, current_iter)\n    self.save_network(self.ema, 'EMA', epoch, current_iter)\n    self.save_network(self.netD, 'D', epoch, current_iter)\n    if self.facial_discriminators:\n        self.save_network(self.netD_left, 'D_left', epoch, current_iter)\n        self.save_network(self.netD_right, 'D_right', epoch, current_iter)\n        self.save_network(self.netD_mouth, 'D_mouth', epoch, current_iter)\n    self.save_training_state(epoch, current_iter)\n", "CodeBleu_score": 54.67039272386442}, {"code": "def save(self, epoch, current_iter)\n\t\n    # save the generator network\n    torch.save(self.G.state_dict(), os.path.join(self.save_dir, 'G_{}.pt'.format(current_iter)))\n    # save the generator network (EMA)\n    torch.save(self.G_ema.state_dict(), os.path.join(self.save_dir, 'G_ema_{}.pt'.format(current_iter)))\n    # save the discriminator network\n    torch.save(self.D.state_dict(), os.path.join(self.save_dir, 'D_{}.pt'.format(current_iter)))\n    # save the discriminator networks for the left eye, right eye, and mouth (if used)\n    if self.facial_discriminators:\n        torch.save(self.D_left.state_dict(), os.path.join(self.save_dir, 'D_left_{}.pt'.format(current_iter)))\n        torch.save(self.D_right.state_dict(), os.path.join(self.save_dir, 'D_right_{}.pt'.format(current_iter)))\n        torch.save(self.D_mouth.state_dict(), os.path.join(self.save_dir, 'D_mouth_{}.pt'.format(current_iter)))\n    # save the training state\n    torch.save(self.training_state, os.path.join(self.save_dir, 'training_state_{}.pt'.format(current_iter)))\n", "CodeBleu_score": 37.2246494393959}]}
{"instance_id": "afa0dc8128c1", "generation_result": [{"code": "def clean_folder(folder)\n\t\n    for item in os.listdir(folder):\n        path = os.path.join(folder, item)\n        try:\n            if os.path.isfile(path) or os.path.islink(path):\n                os.unlink(path)\n            elif os.path.isdir(path):\n                shutil.rmtree(path)\n        except Exception as e:\n            print('Failed to delete %s. Reason: %s' % (path, e))\n\n", "CodeBleu_score": 52.11541830902594}, {"code": "def clean_folder(folder)\n\t\n    for item in os.listdir(folder):\n        path = os.path.join(folder, item)\n        try:\n            if os.path.isfile(path) or os.path.islink(path):\n                os.unlink(path)\n            elif os.path.isdir(path):\n                shutil.rmtree(path)\n        except Exception as e:\n            print('Failed to delete %s. Reason: %s' % (path, e))\n\n", "CodeBleu_score": 52.11541830902594}, {"code": "def clean_folder(folder)\n\t\n    for item in os.listdir(folder):\n        item_path = os.path.join(folder, item)\n        try:\n            if os.path.isfile(item_path) or os.path.islink(item_path):\n                os.unlink(item_path)\n            elif os.path.isdir(item_path):\n                shutil.rmtree(item_path)\n        except Exception as e:\n            print('Failed to delete %s. Reason: %s' % (item, e))\n", "CodeBleu_score": 52.66095781824942}]}
{"instance_id": "53588b481bdd", "generation_result": [{"code": "def next_batch(self, sample_size=1)\n\t\n    if sample_size > len(self.experiences):\n        sample_size = len(self.experiences)\n    experiences = random.sample(self.experiences, sample_size)\n    self.experiences = [e for e in self.experiences if e not in experiences]\n    return experiences\n", "CodeBleu_score": 63.53212060666847}, {"code": "def next_batch(self, sample_size=1)\n\t\n    if sample_size > len(self.memory):\n        sample_size = len(self.memory)\n    batch = random.sample(self.memory, sample_size)\n    return batch\n", "CodeBleu_score": 44.09666993059672}, {"code": "def next_batch(self, sample_size=1)\n\t\n    if sample_size > len(self.experiences):\n        sample_size = len(self.experiences)\n    if sample_size <= 0:\n        return None\n    batch = random.sample(self.experiences, sample_size)\n    self.experiences = [e for e in self.experiences if e not in batch]\n    return batch\n", "CodeBleu_score": 63.55699913074865}]}
{"instance_id": "6bfa6a4e621c", "generation_result": [{"code": "def find_estimated_top_configs(self)\n\t\n    # Flatten all configurations\n    flattened_configs = []\n    for config in self.all_configs:\n        flattened_config = []\n        for key, value in config.items():\n            if isinstance(value, list):\n                flattened_config.extend(value)\n            else:\n                flattened_config.append(value)\n        flattened_configs.append(flattened_config)\n    # Convert to numpy array\n    flattened_configs = np.array(flattened_configs)\n    # Predict cost for all configurations\n    cost_estimates = self.cost_model.predict(flattened_configs)\n    # Sort configurations based on cost estimates\n    sorted_indices = np.argsort(cost_estimates)\n    if self.metric == \"min\":\n        sorted_indices = sorted_indices[::-1]\n    return sorted_indices\n", "CodeBleu_score": 39.92408878275307}, {"code": "def find_estimated_top_configs(self)\n\t\n    # Flatten all_configs\n    flattened_all_configs = []\n    for config in self.all_configs:\n        flattened_config = []\n        for key, value in config.items():\n            if isinstance(value, list):\n                flattened_config.extend(value)\n            else:\n                flattened_config.append(value)\n        flattened_all_configs.append(flattened_config)\n    # Convert flattened_all_configs to a numpy array\n    all_configs_array = np.array(flattened_all_configs)\n    # Predict cost for all configurations\n    all_configs_cost = self.cost_model.predict(all_configs_array)\n    # Sort configurations based on the specified metric\n    if self.metric == \"min\":\n        top_config_indices = np.argsort(all_configs_cost)[:self.top_configs]\n    elif self.metric == \"max\":\n        top_config_indices = np.argsort(all_configs_cost)[-self.top_configs:]\n    else:\n        raise ValueError(\"Invalid metric specified.\")\n    return top_config_indices\n\n", "CodeBleu_score": 36.827069195161464}, {"code": "def find_estimated_top_configs(self)\n\t\n    all_configs = self.all_configs\n    all_configs_flat = []\n    for config in all_configs:\n        config_flat = []\n        for key, val in config.items():\n            if isinstance(val, list):\n                config_flat.extend(val)\n            else:\n                config_flat.append(val)\n        all_configs_flat.append(config_flat)\n    all_configs_flat = np.array(all_configs_flat)\n    estimates = self.cost_model.predict(all_configs_flat)\n    if self.metric == \"min\":\n        top_configs_indices = np.argsort(estimates)[: self.top_configs]\n    else:\n        top_configs_indices = np.argsort(estimates)[::-1][: self.top_configs]\n    return top_configs_indices\n\n", "CodeBleu_score": 39.65991753599597}]}
{"instance_id": "d4cf48413a57", "generation_result": [{"code": "def flatten(d, parent_key='', sep='_')\n\t\n    items = []\n    for k, v in d.items():\n        new_key = parent_key + sep + k if parent_key else k\n        if isinstance(v, collections.abc.MutableMapping):\n            items.extend(flatten(v, new_key, sep=sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)\n\n", "CodeBleu_score": 82.56863782179195}, {"code": "def flatten(d, parent_key='', sep='_')\n\t\n    items = []\n    for k, v in d.items():\n        new_key = parent_key + sep + k if parent_key else k\n        if isinstance(v, collections.abc.MutableMapping):\n            items.extend(flatten(v, new_key, sep=sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)\n\n", "CodeBleu_score": 82.56863782179195}, {"code": "def flatten(d, parent_key='', sep='_')\n\t\n    items = []\n    for k, v in d.items():\n        new_key = parent_key + sep + k if parent_key else k\n        if isinstance(v, collections.abc.MutableMapping):\n            items.extend(flatten(v, new_key, sep=sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)\n\n", "CodeBleu_score": 82.56863782179195}]}
{"instance_id": "c2a514249c25", "generation_result": [{"code": "def run_after_tuning(self)\n\t\n    if self.optimal_cmd is not None:\n        logger.info(\"Running after tuning: %s\", self.optimal_cmd)\n        result = subprocess.Popen(self.optimal_cmd)\n        result.wait()\n        logger.info(\"Finished running after tuning.\")\n    else:\n        logger.info(\"No optimal configuration found. Skipping after tuning.\")", "CodeBleu_score": 58.178876617535295}, {"code": "def run_after_tuning(self)\n\t\n    if self.optimal_cmd:\n        logger.info(\"Running command after tuning: %s\" % self.optimal_cmd)\n        result = subprocess.Popen(self.optimal_cmd, shell=True)\n        result.wait()\n        logger.info(\"Command after tuning completed successfully.\")\n    else:\n        logger.info(\"No optimal command found, autotuning did not find an optimal configuration.\")", "CodeBleu_score": 51.04009466105119}, {"code": "def run_after_tuning(self)\n\t\n    if self.optimal_cmd:\n        logger.info(\"Running the optimal command: %s\", self.optimal_cmd)\n        result = subprocess.Popen(self.optimal_cmd)\n        result.wait()\n        logger.info(\"Finished running the optimal command.\")\n    else:\n        logger.info(\"Autotuning did not find an optimal configuration.\")\n\n", "CodeBleu_score": 59.31470774603718}]}
{"instance_id": "61802de52df0", "generation_result": [{"code": "def run_job(self, exp: dict, reservations)\n\t\n    if self.master_port is None:\n        self.master_port = self.get_master_port()\n    exp[\"master_port\"] = self.master_port\n    exp[\"result_dir\"] = self.get_result_dir()\n    exp[\"user_args\"] = self.update_args(exp[\"user_args\"], exp[\"arg_map\"])\n    if \"reservations\" in exp:\n        exp[\"reservations\"] = reservations\n    t = threading.Thread(target=self.run_job_thread, args=(exp,))\n    t.start()\n    self.running_experiments[exp[\"name\"]] = {\"timestamp\": time.time(), \"thread\": t}\n\n", "CodeBleu_score": 24.69912607466552}, {"code": "def run_job(self, exp: dict, reservations)\n\t\n    # Configure the experiment's master port and result directory\n    exp['master_port'] = self.master_port\n    exp['result_dir'] = os.path.join(self.result_dir, str(time.time()))\n    # Update user arguments based on argument mappings\n    if exp['arg_mappings']:\n        for arg_mapping in exp['arg_mappings']:\n            if arg_mapping['type'] == 'user_args':\n                user_args = exp['user_args']\n                user_args.insert(arg_mapping['index'], arg_mapping['value'])\n            elif arg_mapping['type'] =='reservations':\n                if arg_mapping['key'] in reservations:\n                    exp[arg_mapping['key']] = reservations[arg_mapping['key']]\n                else:\n                    raise Exception(f\"Reservation key '{arg_mapping['key']}' not found in reservations.\")\n            else:\n                raise Exception(f\"Invalid argument mapping type '{arg_mapping['type']}'.\")\n    # Start the experiment in a new thread\n    thread = threading.Thread(target=self.run_experiment, args=(exp,))\n    thread.start()\n    # Store the running experiment details with a timestamp\n    self.running_experiments[str(time.time())] = exp\n\n", "CodeBleu_score": 25.83269422315968}, {"code": "def run_job(self, exp: dict, reservations)\n\t\n    if not self.running_experiments:\n        self.running_experiments = []\n\n    exp_id = exp['id']\n    exp_script = exp['script']\n    exp_args = exp['args']\n    exp_args_map = exp['args_map']\n    exp_result_dir = exp['result_dir']\n    exp_master_port = exp['master_port']\n\n    # Set the experiment's master port and result directory\n    exp['master_port'] = exp_master_port\n    exp['result_dir'] = exp_result_dir\n\n    # Update user arguments based on argument mappings\n    user_args = exp_args.copy()\n    for arg_map in exp_args_map:\n        arg_name = arg_map['arg_name']\n        arg_value = arg_map['arg_value']\n        if arg_name in user_args:\n            user_args[user_args.index(arg_name)] = arg_value\n\n    # Create a new thread to run the experiment\n    t = threading.Thread(target=self.run_exp_job, args=(exp_script, user_args, exp_result_dir, exp_master_port, exp_id))\n    t.daemon = True\n    t.start()\n\n    # Add the running experiment details to the running_experiments list\n    running_exp = {\n        'exp_id': exp_id,\n       'script': exp_script,\n        'args': exp_args,\n        'args_map': exp_args_map,\n       'result_dir': exp_result_dir,\n       'master_port': exp_master_port,\n        'thread': t,\n        'timestamp': time.time()\n    }\n    self.running_experiments.append(running_exp)\n\n    # Update the reservations with the running experiment details\n    for reservation in reservations:\n        if reservation['exp_id'] == exp_id:\n            reservation['running_exp'] = running_exp\n            break\n\n", "CodeBleu_score": 32.25120472632752}]}
{"instance_id": "4884e2e9665c", "generation_result": [{"code": "def save_exp_results_to_database(self, message, ranks=None, path=None):\n\t\n    if self.dist.is_initialized:\n        if ranks is None:\n            ranks = self.dist.get_rank()\n        if self.logger.debug:\n            if ranks == 0:\n                self.logger.debug(message)\n                if path is None:\n                    path = self.log_path\n                if not os.path.exists(path):\n                    os.makedirs(path)\n                with open(os.path.join(path, self.log_file), 'a') as outfile:\n                    message = {'rank': ranks,'message': message}\n                    json.dump(message, outfile)\n                    outfile.write('", "CodeBleu_score": 29.144358606209366}, {"code": "def save_exp_results_to_database(self, message, ranks=None, path=None):\n\t\n    if not self.is_initialized:\n        return\n    if ranks is None:\n        ranks = self.get_rank()\n    if ranks is None:\n        return\n    if path is None:\n        path = self.log_dir\n    if path is None:\n        return\n    if not os.path.isdir(path):\n        os.makedirs(path)\n    if self.is_master:\n        message = json.dumps(message)\n        with open(os.path.join(path, 'log.json'), 'a') as outfile:\n            outfile.write(message + '", "CodeBleu_score": 28.238912488608868}, {"code": "def save_exp_results_to_database(self, message, ranks=None, path=None):\n\t\n    if dist.is_initialized():\n        if self.rank == 0:\n            logging_permitted = True\n        else:\n            logging_permitted = False\n    else:\n        logging_permitted = True\n    if logging_permitted:\n        if ranks is None:\n            ranks = [self.rank]\n        if path is None:\n            path = self.log_dir + '/results.json'\n        if self.rank in ranks:\n            message['rank'] = self.rank\n            with open(path, 'a') as outfile:\n                json.dump(message, outfile)\n                outfile.write('", "CodeBleu_score": 35.72796446671943}]}
{"instance_id": "0429344c3c4d", "generation_result": [{"code": "def _build_tp_other_layer_map(self, layer_index: int)\n\t\n    if layer_index >= self.num_layers:\n        raise ValueError(f\"layer_index {layer_index} is out of range [0, {self.num_layers})\")\n    if self.num_layers == 1:\n        return None\n    layer_prefix = self.get_layer_prefix(layer_index)\n    layer_files = self.get_files_with_prefix(layer_prefix)\n    layer_files_partitioned = self.partition_data(layer_files, self.tp_degree)\n    return {i: layer_files_partitioned[i] for i in range(self.tp_degree)}\n", "CodeBleu_score": 36.27230357843765}, {"code": "def _build_tp_other_layer_map(self, layer_index: int)\n\t\n    if not self.has_layer_files(layer_index):\n        return {}\n    if not self.verify_layer_index(layer_index):\n        return {}\n    layer_prefix = self.get_layer_prefix(layer_index)\n    layer_files = self.get_files_with_prefix(layer_prefix)\n    layer_files_partitioned = self.partition_data(layer_files, self.tp_degree)\n    layer_map = {}\n    for partition_index, partition_files in enumerate(layer_files_partitioned):\n        layer_map[partition_index] = partition_files\n    return layer_map\n", "CodeBleu_score": 38.44075933434192}, {"code": "def _build_tp_other_layer_map(self, layer_index: int)\n\t\n    # Check if there are any layer files\n    if not self.layer_files:\n        raise ValueError(\"No layer files found.\")\n    # Verify the layer index\n    if layer_index < 0 or layer_index >= self.num_layers:\n        raise ValueError(f\"Invalid layer index: {layer_index}\")\n    # Retrieve files with a specific prefix for the given layer\n    layer_files = self._get_files_with_prefix(f\"layer_{layer_index}_\")\n    # Partition these files based on tp_degree\n    partitioned_files = self._partition_data(layer_files, self.tp_degree)\n    # Create and return a dictionary mapping partition indices to their corresponding file lists\n    return {\n        partition_index: file_list\n        for partition_index, file_list in enumerate(partitioned_files)\n    }\n\n", "CodeBleu_score": 36.5209672852296}]}
{"instance_id": "1b65ecb9be93", "generation_result": [{"code": "def system_info()\n\t\n    return {'operating_system': platform.system(),\n            'architecture': platform.architecture()[0],\n           'version': platform.version(),\n            'processor': platform.processor(),\n            'python_version': platform.python_version()}\n\n", "CodeBleu_score": 65.63484099637488}, {"code": "def system_info()\n\t\n    return {\n        \"Operating System\": platform.system(),\n        \"Architecture\": platform.architecture()[0],\n        \"Version\": platform.version(),\n        \"Processor\": platform.processor(),\n        \"Python Version\": platform.python_version(),\n    }\n\n", "CodeBleu_score": 68.41276349226236}, {"code": "def system_info()\n\t\n    import platform\n    import sys\n    sys_info = {\n        'operating_system': platform.system(),\n        'architecture': platform.architecture()[0],\n       'version': platform.version(),\n        'processor': platform.processor(),\n        'python_version': platform.python_version()\n    }\n    return sys_info\n", "CodeBleu_score": 37.42590064228688}]}
{"instance_id": "969bcc5cf3b5", "generation_result": [{"code": "def cuda_info()\n\t\n    device_count = torch.cuda.device_count()\n    device_names = [torch.cuda.get_device_name(i) for i in range(device_count)]\n    is_available = torch.cuda.is_available()\n    cuda_version = torch.version.cuda\n    return device_names, is_available, cuda_version\n", "CodeBleu_score": 40.86867838287014}, {"code": "def cuda_info()\n\t\n    cuda_info_dict = {}\n    cuda_info_dict['device_name'] = []\n    cuda_info_dict['device_count'] = []\n    cuda_info_dict['is_available'] = []\n    cuda_info_dict['range'] = []\n    cuda_info_dict['cuda_version'] = []\n    cuda_info_dict['cudnn_version'] = []\n    cuda_info_dict['cudart_version'] = []\n    cuda_info_dict['nvcc_version'] = []\n    cuda_info_dict['driver_version'] = []\n    cuda_info_dict['cuda_toolkit_version'] = []\n    cuda_info_dict['cuda_samples_version'] = []\n    cuda_info_dict['cuda_samples_path'] = []\n    cuda_info_dict['cuda_samples_path_set'] = []\n    cuda_info_dict['cuda_toolkit_path'] = []\n    cuda_info_dict['cuda_toolkit_path_set'] = []\n    cuda_info_dict['cuda_toolkit_root_dir'] = []\n    cuda_info_dict['cuda_toolkit_root_dir_set'] = []\n    cuda_info_dict['cuda_path'] = []\n    cuda_info_dict['cuda_path_set'] = []\n    cuda_info_dict['cudnn_path'] = []\n    cuda_info_dict['cudnn_path_set'] = []\n    cuda_info_dict['cudart_path'] = []\n    cuda_info_dict['cudart_path_set'] = []\n    cuda_info_dict['nvcc_path'] = []\n    cuda_info_dict['nvcc_path_set'] = []\n    cuda_info_dict['driver_path'] = []\n    cuda_info_dict['driver_path_set'] = []\n    cuda_info_dict['cuda_samples_path'] = []\n    cuda_info_dict['cuda_samples_path_set'] = []\n    cuda_info_dict['cuda_toolkit_path'] = []\n    cuda_info_dict['cuda_toolkit_path_set'] = []\n    cuda_info_dict['cuda_toolkit_root_dir'] = []\n    cuda_info_dict['cuda_toolkit_root_dir_set'] = []\n    cuda_info_dict['", "CodeBleu_score": 6.614999829415402}, {"code": "def cuda_info()\n\t\n    cuda_info = {}\n    cuda_info[\"device_count\"] = torch.cuda.device_count()\n    cuda_info[\"device_name\"] = [torch.cuda.get_device_name(i) for i in range(torch.cuda.device_count())]\n    cuda_info[\"is_available\"] = torch.cuda.is_available()\n    cuda_info[\"range\"] = torch.cuda.current_device()\n    return cuda_info\n\n", "CodeBleu_score": 50.55062424477761}]}
{"instance_id": "a2ba908a4c70", "generation_result": [{"code": "def compute_encoder_accuracy(dataset_items, encoder_manager)\n\t\n    class_acc_dict = {}\n    class_name_key = encoder_manager.config[\"class_name_key\"]\n    class_name_mapping = encoder_manager.config.get(\"class_name_mapping\", None)\n    for item in tqdm(dataset_items):\n        audio_file = item[\"audio_file\"]\n        embedding = encoder_manager.compute_embedding_from_clip(audio_file)\n        if class_name_key in item:\n            true_class_name = item[class_name_key]\n            if class_name_mapping is not None:\n                true_class_name = class_name_mapping[true_class_name]\n            predicted_class_name = encoder_manager.predict(embedding)\n            if predicted_class_name not in class_acc_dict:\n                class_acc_dict[predicted_class_name] = []\n            class_acc_dict[predicted_class_name].append(true_class_name == predicted_class_name)\n    for class_name, acc_list in class_acc_dict.items():\n        print(f\"Accuracy for class {class_name}: {sum(acc_list) / len(acc_list)}\")\n    print(f\"Average accuracy: {sum([sum(acc_list) / len(acc_list) for acc_list in class_acc_dict.values()]) / len(class_acc_dict)}\")", "CodeBleu_score": 32.4805883534681}, {"code": "def compute_encoder_accuracy(dataset_items, encoder_manager)\n\t\n    class_name_key = encoder_manager.config.get(\"class_name_key\")\n    class_id_to_name = encoder_manager.config.get(\"class_id_to_name\")\n    if class_name_key is None:\n        print(\"No class name key found in the encoder manager's configuration.\")\n        return\n    class_acc_dict = defaultdict(int)\n    class_count_dict = defaultdict(int)\n    for item in tqdm(dataset_items):\n        audio_file = item[\"audio_file\"]\n        embedding = encoder_manager.compute_embedding_from_clip(audio_file)\n        if embedding is None:\n            continue\n        class_name = item[class_name_key]\n        if class_id_to_name is not None:\n            class_id = item[\"class_id\"]\n            class_name = class_id_to_name[class_id]\n        pred_class_name = encoder_manager.predict_class_from_embedding(embedding)\n        class_acc_dict[class_name] += int(pred_class_name == class_name)\n        class_count_dict[class_name] += 1\n    for class_name, class_acc in class_acc_dict.items():\n        print(f\"{class_name}: {class_acc / class_count_dict[class_name]:.2f}\")\n    avg_acc = sum(class_acc_dict.values()) / sum(class_count_dict.values())\n    print(f\"Average Accuracy: {avg_acc:.2f}\")", "CodeBleu_score": 33.71455569473248}, {"code": "def compute_encoder_accuracy(dataset_items, encoder_manager)\n\t\n    class_name_key = encoder_manager.config[\"class_name_key\"]\n    class_name_to_id = encoder_manager.config[\"class_name_to_id\"]\n    class_acc_dict = {\n        class_name: 0 for class_name in class_name_to_id.keys()\n    }\n    for item in tqdm(dataset_items):\n        audio_file = item[\"audio_file\"]\n        embedding = encoder_manager.compute_embedding_from_clip(audio_file)\n        if \"class_name\" in item:\n            class_name = item[\"class_name\"]\n            if class_name in class_name_to_id:\n                class_id = class_name_to_id[class_name]\n                prediction = encoder_manager.inference(embedding, class_id)\n                if prediction == class_name:\n                    class_acc_dict[class_name] += 1\n    for class_name, acc in class_acc_dict.items():\n        print(f\"{class_name}: {acc / len(dataset_items)}\")\n    print(\n        f\"Average accuracy: {sum(class_acc_dict.values()) / len(dataset_items) / len(class_name_to_id)}\"\n    )\n\n", "CodeBleu_score": 26.55222993014974}]}
{"instance_id": "e01ec69ad58a", "generation_result": [{"code": "def adjust_path_and_remove_silence(audio_path)\n\t   # adjust the output path\n    out_path = audio_path.replace(\n        os.path.dirname(audio_path),\n        os.path.join(os.path.dirname(audio_path), \"output\"),\n    )\n\n    # if the file already exists and force is not specified\n    if os.path.exists(out_path) and not force:\n        print(f\"> {out_path} already exists\")\n        return out_path, False\n\n    # create the necessary directory structure\n    Path(os.path.dirname(out_path)).mkdir(parents=True, exist_ok=True)\n\n    # remove silence from the audio\n    out_path, is_speech = remove_silence(\n        model_and_utils,\n        audio_path,\n        out_path,\n        vad_sample_rate=8000,\n        trim_just_beginning_and_end=True,\n        use_cuda=use_cuda,\n    )\n\n    return out_path, is_speech\n\n\n", "CodeBleu_score": 45.45392695470143}, {"code": "def adjust_path_and_remove_silence(audio_path)\n\t   # replace the input directory path with the output directory path\n    out_path = audio_path.replace(input_dir, output_dir)\n    # if the file already exists and force is not specified, return the output path and a False flag\n    if os.path.isfile(out_path) and not force:\n        return out_path, False\n    # otherwise, create the necessary directory structure\n    pathlib.Path(os.path.dirname(out_path)).mkdir(parents=True, exist_ok=True)\n    # remove silence from the audio using specified parameters\n    out_path, is_speech = remove_silence(\n        model_and_utils, audio_path, out_path, vad_sample_rate=8000, trim_just_beginning_and_end=True, use_cuda=False\n    )\n    # return the output path and a flag indicating if speech was detected\n    return out_path, is_speech\n", "CodeBleu_score": 41.48466059204119}, {"code": "def adjust_path_and_remove_silence(audio_path)\n\t   # adjust the output path\n    out_path = audio_path.replace(input_dir, output_dir)\n\n    # if file already exists and force is not specified\n    if os.path.exists(out_path) and not force:\n        print(f\"> The file {out_path} already exists\")\n        return out_path, False\n\n    # create the necessary directory structure\n    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n\n    # remove silence from the audio\n    out_path, is_speech = remove_silence(\n        model_and_utils, audio_path, out_path, vad_sample_rate=8000, trim_just_beginning_and_end=True, use_cuda=False\n    )\n\n    return out_path, is_speech\n\n", "CodeBleu_score": 41.749079576964334}]}
{"instance_id": "7e0bdb0feff8", "generation_result": [{"code": "def resample_files(input_dir, output_sr, output_dir=None, file_ext=\"wav\", n_jobs=10)\n\t\n    if output_dir is None:\n        output_dir = input_dir\n    else:\n        copytree(input_dir, output_dir)\n    print(f\"Copying {input_dir} to {output_dir}\")\n    file_list = []\n    for root, dirs, files in os.walk(output_dir):\n        for file in files:\n            if file.endswith(file_ext):\n                file_list.append(os.path.join(root, file))\n    print(f\"Found {len(file_list)} files\")\n    with Pool(n_jobs) as p:\n        with tqdm(total=len(file_list)) as pbar:\n            for _ in p.imap_unordered(resample_file, zip(file_list, repeat(output_sr))):\n                pbar.update()\n    print(\"Done!\")\n\n", "CodeBleu_score": 40.28748914000807}, {"code": "def resample_files(input_dir, output_sr, output_dir=None, file_ext=\"wav\", n_jobs=10)\n\t\n    if output_dir is None:\n        output_dir = input_dir\n    else:\n        copytree(input_dir, output_dir)\n    input_files = glob.glob(f\"{output_dir}/**/*.{file_ext}\", recursive=True)\n    print(f\"Found {len(input_files)} files\")\n    p = Pool(n_jobs)\n    with tqdm(total=len(input_files)) as pbar:\n        for i, output_file in enumerate(p.imap_unordered(partial(resample_file, output_sr=output_sr), input_files)):\n            pbar.update()\n    print(\"Done\")\n", "CodeBleu_score": 40.54594961542327}, {"code": "def resample_files(input_dir, output_sr, output_dir=None, file_ext=\"wav\", n_jobs=10)\n\t\n    if output_dir is None:\n        output_dir = input_dir\n    copytree(input_dir, output_dir)\n    files = glob.glob(f\"{output_dir}/**/*.{file_ext}\", recursive=True)\n    with Pool(n_jobs) as p:\n        for i, _ in tqdm(enumerate(p.imap_unordered(partial(resample_file, output_sr=output_sr), files)), total=len(files)):\n            pass\n    print(f\"Resampled {len(files)} files to {output_sr} Hz.\")\n\n", "CodeBleu_score": 34.017919701082874}]}
{"instance_id": "a1351834ca57", "generation_result": [{"code": "def evaluation(model, criterion, data_loader, global_step)\n\t\n    model.eval()\n    eval_loss = 0\n    eval_steps = 0\n    for batch_idx, (inputs, labels) in enumerate(data_loader):\n        inputs = inputs.view(inputs.size(0), -1)\n        labels = labels.view(labels.size(0), -1)\n        inputs = inputs.cuda()\n        labels = labels.cuda()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        eval_loss += loss.item()\n        eval_steps += 1\n        if batch_idx == 0:\n            num_classes_in_batch = labels.shape[0]\n            embeddings = outputs.cpu().detach().numpy()\n        else:\n            embeddings = numpy.concatenate((embeddings, outputs.cpu().detach().numpy()))\n    eval_loss /= eval_steps\n    dashboard_logger.eval_stats(global_step, eval_loss)\n    fig = plot_embeddings(embeddings, num_classes_in_batch)\n    dashboard_logger.eval_figures(global_step, fig)\n    return eval_loss\n\n", "CodeBleu_score": 36.7619155030342}, {"code": "def evaluation(model, criterion, data_loader, global_step)\n\t\n    model.eval()\n    eval_loss = 0\n    for batch_idx, (inputs, labels) in enumerate(data_loader):\n        if batch_idx == 0:\n            num_classes_in_batch = len(np.unique(labels))\n            num_utter_per_class = inputs.shape[0] // num_classes_in_batch\n            inputs = inputs.view(num_classes_in_batch, num_utter_per_class, -1)\n            labels = labels.view(num_classes_in_batch, num_utter_per_class)\n        inputs, labels = inputs.cuda(), labels.cuda()\n        outputs = model(inputs)\n        outputs = outputs.view(-1, outputs.shape[-1])\n        labels = labels.view(-1)\n        loss = criterion(outputs, labels)\n        eval_loss += loss.item()\n        if batch_idx == 0:\n            embeddings = outputs.detach().cpu().numpy()\n            fig = plot_embeddings(embeddings, num_classes_in_batch)\n            dashboard_logger.eval_figures(fig, global_step)\n    eval_loss /= len(data_loader)\n    dashboard_logger.eval_stats(eval_loss, global_step)\n    return eval_loss\n\n", "CodeBleu_score": 40.288482134828065}, {"code": "def evaluation(model, criterion, data_loader, global_step)\n\t\n    model.eval()\n    eval_loss = 0\n    for batch_idx, (inputs, labels) in enumerate(data_loader):\n        inputs = inputs.view(inputs.shape[0], -1)\n        labels = labels.view(labels.shape[0], -1)\n        if torch.cuda.is_available():\n            inputs = inputs.cuda()\n            labels = labels.cuda()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        eval_loss += loss.item()\n        if batch_idx == 0:\n            num_classes_in_batch = labels.shape[0]\n            embeddings = outputs.detach().cpu().numpy()\n            fig = plot_embeddings(embeddings, num_classes_in_batch)\n            dashboard_logger.eval_figures(fig, global_step)\n    eval_loss /= len(data_loader)\n    dashboard_logger.eval_stats(eval_loss, global_step)\n    return eval_loss\n\n", "CodeBleu_score": 34.52476810753624}]}
{"instance_id": "565f8ed09916", "generation_result": [{"code": "def register_config(model_name: str) -> Coqpit\n\t\n    if model_name == \"xtts\":\n        try:\n            module = importlib.import_module(\"coqpit.xtts\")\n            class_name = \"XttsConfig\"\n            return getattr(module, class_name)\n        except ModuleNotFoundError:\n            raise ModuleNotFoundError(\n                f\"Could not find configuration class for model {model_name}\"\n            )\n    else:\n        for path in CONFIG_PATHS:\n            try:\n                module_path = path + \".\" + model_name\n                module = importlib.import_module(module_path)\n                class_name = to_camel(model_name) + \"Config\"\n                return getattr(module, class_name)\n            except ModuleNotFoundError:\n                continue\n        raise ModuleNotFoundError(\n            f\"Could not find configuration class for model {model_name}\"\n        )\n\n", "CodeBleu_score": 31.791361793500183}, {"code": "def register_config(model_name: str) -> Coqpit\n\t\n    if model_name == \"xtts\":\n        config_class = find_module(\"xtts.configs\", \"xtts_config\")\n    else:\n        config_class = find_module(\"coqpit.configs\", model_name)\n    return config_class\n\n", "CodeBleu_score": 17.162146478976425}, {"code": "def register_config(model_name: str) -> Coqpit\n\t\n    if model_name == \"xtts\":\n        from coqpit.xtts import XttsConfig\n        return XttsConfig\n    else:\n        config_class_name = to_camel(model_name) + \"Config\"\n        for path in CONFIG_PATHS:\n            try:\n                return find_module(path, config_class_name)\n            except ModuleNotFoundError:\n                continue\n        raise ModuleNotFoundError(f\"Configuration class {config_class_name} not found.\")\n\n", "CodeBleu_score": 32.40355941012385}]}
{"instance_id": "0ea7862506a7", "generation_result": [{"code": "def load_config(config_path: str) -> Coqpit\n\t\n    config_dict = read_json_with_comments(config_path)\n    model_name = _process_model_name(config_dict)\n    config_class = register_config(model_name)\n    config = config_class()\n    config.from_dict(config_dict)\n    return config", "CodeBleu_score": 11.267861974901514}, {"code": "def load_config(config_path: str) -> Coqpit\n\t\n    config_dict = read_json_with_comments(config_path)\n    model_name = _process_model_name(config_dict)\n    config_class = register_config(model_name)\n    config = config_class()\n    config.from_dict(config_dict)\n    return config", "CodeBleu_score": 11.267861974901514}, {"code": "def load_config(config_path: str) -> Coqpit\n\t\n    config_dict = {}\n    config_class = None\n    if config_path.endswith(\".yaml\") or config_path.endswith(\".yml\"):\n        with fsspec.open(config_path, \"r\", encoding=\"utf-8\") as f:\n            config_dict = yaml.safe_load(f)\n    elif config_path.endswith(\".json\"):\n        config_dict = read_json_with_comments(config_path)\n    else:\n        raise ValueError(f\" [!] {config_path} has an unknown extension. Please use.json or.yaml\")\n    model_name = _process_model_name(config_dict)\n    config_class = register_config(model_name)\n    config = config_class()\n    config.from_dict(config_dict)\n    return config", "CodeBleu_score": 50.08950675572782}]}
{"instance_id": "e0db68b63143", "generation_result": [{"code": "def list_files(basePath, validExts=None, contains=None)\n\t\n    for (dirpath, dirnames, filenames) in os.walk(basePath):\n        for filename in filenames:\n            if contains is not None and filename.find(contains) == -1:\n                continue\n            ext = filename[filename.rfind(\".\"):].lower()\n            if validExts is None or ext.endswith(validExts):\n                # construct the full path to the file and yield it\n                fullPath = os.path.join(dirpath, filename).replace(\"\\\\\", \"/\")\n                yield fullPath\n", "CodeBleu_score": 69.37992267918827}, {"code": "def list_files(basePath, validExts=None, contains=None)\n\t\n    # loop over the directory structure\n    for (rootDir, dirNames, filenames) in os.walk(basePath):\n        # loop over the filenames in the current directory\n        for filename in filenames:\n            # if the contains string is not none and the filename does not contain the supplied string,\n            # then ignore the file\n            if contains is not None and filename.find(contains) == -1:\n                continue\n\n            # determine the file extension of the current file\n            ext = filename[filename.rfind(\".\"):].lower()\n\n            # check to see if the file is an image and should be processed\n            if validExts is None or ext.endswith(validExts):\n                # construct the path to the image and yield it\n                imagePath = os.path.join(rootDir, filename).replace(\" \", \"\\\\ \")\n                yield imagePath", "CodeBleu_score": 72.29871358620767}, {"code": "def list_files(basePath, validExts=None, contains=None)\n\t\n    for (dirpath, dirnames, filenames) in os.walk(basePath):\n        for filename in filenames:\n            if contains is not None and filename.find(contains) == -1:\n                continue\n            ext = filename[filename.rfind(\".\"):].lower()\n            if validExts is None or ext.endswith(validExts):\n                # construct the full path to the file and yield it\n                fullPath = os.path.join(dirpath, filename).replace(\"\\\\\", \"/\")\n                yield fullPath\n", "CodeBleu_score": 69.37992267918827}]}
{"instance_id": "b836fcaf0a27", "generation_result": [{"code": "def load_model(xtts_checkpoint, xtts_config, xtts_vocab)\n\t\n    # clear the GPU cache\n    clear_gpu_cache()\n\n    # check if the required paths are provided\n    if xtts_checkpoint is None:\n        return \"ERROR: No checkpoint path provided.\"\n    if xtts_config is None:\n        return \"ERROR: No config path provided.\"\n    if xtts_vocab is None:\n        return \"ERROR: No vocab path provided.\"\n\n    # load the configuration from the provided path\n    config = XttsConfig.load_json(xtts_config)\n\n    # initialize the XTTS model\n    xtts = XTTS.init_from_config(config)\n\n    # load the model checkpoint and vocabulary\n    xtts.load_checkpoint(config, checkpoint_path=xtts_checkpoint, vocab_path=xtts_vocab)\n\n    # move the model to the GPU if available\n    if torch.cuda.is_available():\n        xtts.cuda()\n\n    # print messages indicating the loading progress\n    print(\"Loading XTTS model...\")\n    print(\"Checkpoint path: %s\" % xtts_checkpoint)\n    print(\"Config path: %s\" % xtts_config)\n    print(\"Vocab path: %s\" % xtts_vocab)\n\n    # return a success message once the model is loaded\n    return \"XTTS model loaded successfully.\"\n\n", "CodeBleu_score": 50.733414527791695}, {"code": "def load_model(xtts_checkpoint, xtts_config, xtts_vocab)\n\t\n    # clear the GPU cache\n    clear_gpu_cache()\n\n    # check if the required paths are provided\n    if not xtts_checkpoint or not xtts_config or not xtts_vocab:\n        return \"Error: Missing one or more required paths.\"\n\n    # load the configuration from the provided path\n    config = XttsConfig.load_json(xtts_config)\n\n    # initialize the XTTS model\n    XTTS_MODEL = Xtts(config)\n\n    # load the model checkpoint and vocabulary\n    XTTS_MODEL.load_checkpoint(config, checkpoint_path=xtts_checkpoint, vocab_path=xtts_vocab)\n\n    # move the model to the GPU if available\n    if torch.cuda.is_available():\n        XTTS_MODEL.cuda()\n\n    print(\"XTTS model loaded successfully!\")\n    return XTTS_MODEL\n\n", "CodeBleu_score": 44.29889116421117}, {"code": "def load_model(xtts_checkpoint, xtts_config, xtts_vocab)\n\t\n    # clear the GPU cache\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n\n    # check if the required paths are provided\n    if not xtts_checkpoint or not xtts_config or not xtts_vocab:\n        return \"Error: Please provide the paths to the XTTS checkpoint, config, and vocab.\"\n\n    # load the configuration from the provided path\n    config = XttsConfig.load_json(xtts_config)\n\n    # initialize the XTTS model\n    xtts = Xtts(config)\n\n    # load the model checkpoint and vocabulary\n    xtts.load_checkpoint(config, checkpoint_path=xtts_checkpoint, vocab_path=xtts_vocab)\n\n    # move the model to the GPU if available\n    if torch.cuda.is_available():\n        xtts.cuda()\n\n    # print a message indicating the loading progress\n    print(\"Loading XTTS model...\")\n\n    # return a success message once the model is loaded\n    return \"XTTS model loaded successfully.\"\n\n", "CodeBleu_score": 61.31909001092127}]}
{"instance_id": "c5d63aa0df26", "generation_result": [{"code": "def compute_embedding(self, x, num_frames=250, num_eval=10, return_mean=True, l2_norm=True)\n\t\n    hop_length = self.hop_length\n    if self.torch:\n        hop_length = self.hop_length // self.sr\n    num_frames = min(num_frames, x.shape[0] // hop_length)\n    offset = np.linspace(0, (x.shape[0] - num_frames * hop_length), num_eval, dtype=np.int)\n    embedding_list = []\n    for o in offset:\n        frames = x[o : o + num_frames * hop_length : hop_length]\n        frames_batch = []\n        for i in range(frames.shape[0] // self.window_size):\n            frames_batch.append(frames[i * self.window_size : (i + 1) * self.window_size])\n        frames_batch = torch.stack(frames_batch)\n        if self.torch:\n            frames_batch = torch.tensor(frames_batch).float().to(self.device)\n        else:\n            frames_batch = torch.tensor(frames_batch).float()\n        embedding = self.model(frames_batch)\n        embedding_list.append(embedding)\n    embedding = torch.stack(embedding_list)\n    if return_mean:\n        embedding = torch.mean(embedding, dim=0)\n        if l2_norm:\n            embedding = torch.nn.functional.normalize(embedding, p=2, dim=0)\n    return embedding\n\n", "CodeBleu_score": 34.85175255907685}, {"code": "def compute_embedding(self, x, num_frames=250, num_eval=10, return_mean=True, l2_norm=True)\n\t\n    # Adjust number of frames based on hop length\n    if self.hop_length is not None:\n        num_frames = int(np.ceil(num_frames * self.hop_length / self.sr))\n    # Ensure number of frames does not exceed input length\n    num_frames = min(num_frames, x.shape[0])\n    # Calculate offset positions for evaluation\n    offsets = np.linspace(0, x.shape[0] - num_frames, num_eval, dtype=int)\n    # Extract frames at these offsets\n    frames = [x[offset:offset + num_frames] for offset in offsets]\n    # Concatenate frames into a batch\n    frames_batch = torch.stack(frames)\n    # Perform inference to obtain embeddings\n    embeddings = self.inference(frames_batch)\n    # Compute the mean of the embeddings\n    if return_mean:\n        embeddings = torch.mean(embeddings, dim=0)\n    # Optionally normalize the embeddings with L2 norm\n    if l2_norm:\n        embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=0)\n    return embeddings\n\n", "CodeBleu_score": 43.59512698947964}, {"code": "def compute_embedding(self, x, num_frames=250, num_eval=10, return_mean=True, l2_norm=True)\n\t\n    if self.torch_spec:\n        hop_length = self.hop_length\n        num_frames = int(np.ceil(num_frames * hop_length / self.sample_rate))\n    num_frames = min(num_frames, x.shape[1])\n    offsets = np.linspace(0, x.shape[1] - num_frames, num_eval, dtype=int)\n    embeddings = []\n    for offset in offsets:\n        frames = x[:, offset : offset + num_frames]\n        frames_batch = []\n        for i in range(0, frames.shape[1], self.hop_length):\n            frame = frames[:, i : i + self.hop_length]\n            if frame.shape[1] < self.hop_length:\n                frame = torch.nn.functional.pad(frame, (0, self.hop_length - frame.shape[1]))\n            frames_batch.append(frame)\n        frames_batch = torch.stack(frames_batch)\n        if self.torch_spec:\n            frames_batch = frames_batch.unsqueeze(0).unsqueeze(0)\n        embedding = self.inference(frames_batch)\n        embeddings.append(embedding)\n    embeddings = torch.stack(embeddings)\n    if return_mean:\n        embeddings = torch.mean(embeddings, dim=0)\n        if l2_norm:\n            embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=0)\n    return embeddings\n\n", "CodeBleu_score": 40.84671054263573}]}
{"instance_id": "06f5b0335b26", "generation_result": [{"code": "def create_layer(self, block, planes, blocks, stride=1)\n\t\n    downsample = None\n    if stride!= 1 or self.inplanes!= planes * block.expansion:\n        downsample = nn.Sequential(\n            conv1x1(self.inplanes, planes * block.expansion, stride),\n            nn.BatchNorm2d(planes * block.expansion),\n        )\n\n    layers = []\n    layers.append(block(self.inplanes, planes, stride, downsample))\n    self.inplanes = planes * block.expansion\n    for _ in range(1, blocks):\n        layers.append(block(self.inplanes, planes))\n\n    return nn.Sequential(*layers)\n", "CodeBleu_score": 72.43508513589204}, {"code": "def create_layer(self, block, planes, blocks, stride=1)\n\t\n    downsample = None\n    if stride!= 1 or self.inplanes!= planes * block.expansion:\n        downsample = nn.Sequential(\n            nn.Conv2d(self.inplanes, planes * block.expansion,\n                      kernel_size=1, stride=stride, bias=False),\n            nn.BatchNorm2d(planes * block.expansion),\n        )\n\n    layers = []\n    layers.append(block(self.inplanes, planes, stride, downsample))\n    self.inplanes = planes * block.expansion\n    for i in range(1, blocks):\n        layers.append(block(self.inplanes, planes))\n\n    return nn.Sequential(*layers)\n", "CodeBleu_score": 85.54080921158837}, {"code": "def create_layer(self, block, planes, blocks, stride=1)\n\t\n    downsample = None\n    if stride!= 1 or self.inplanes!= planes * block.expansion:\n        downsample = nn.Sequential(\n            nn.Conv2d(self.inplanes, planes * block.expansion,\n                      kernel_size=1, stride=stride, bias=False),\n            nn.BatchNorm2d(planes * block.expansion),\n        )\n\n    layers = []\n    layers.append(block(self.inplanes, planes, stride, downsample))\n    self.inplanes = planes * block.expansion\n    for i in range(1, blocks):\n        layers.append(block(self.inplanes, planes))\n\n    return nn.Sequential(*layers)\n", "CodeBleu_score": 85.54080921158837}]}
{"instance_id": "9ac60597b633", "generation_result": [{"code": "def plot_embeddings(embeddings, num_classes_in_batch)\n\t\n    # calculate the number of utterances per class\n    num_classes = embeddings.shape[0] // num_classes_in_batch\n    # limit the number of classes to 10 if necessary\n    if num_classes > 10:\n        num_classes = 10\n    # calculate the number of utterances per class\n    num_per_class = embeddings.shape[0] // num_classes\n    # create a figure and axis\n    fig, ax = plt.subplots()\n    # create a list of colors for the classes\n    colors = [\"red\", \"green\", \"blue\", \"orange\", \"purple\", \"brown\", \"pink\", \"gray\", \"olive\", \"cyan\"]\n    # loop over the classes\n    for i in range(num_classes):\n        # select the embeddings for the current class\n        class_embeddings = embeddings[i * num_per_class:(i + 1) * num_per_class]\n        # use UMAP to reduce the dimensionality of the embeddings\n        umap_embeddings = umap.UMAP(n_neighbors=5, min_dist=0.3, metric=\"cosine\").fit_transform(class_embeddings)\n        # plot the embeddings as a scatter plot\n        ax.scatter(umap_embeddings[:, 0], umap_embeddings[:, 1], c=colors[i], label=f\"Class {i}\", s=10)\n    # add a legend to the plot\n    ax.legend()\n    # set the aspect ratio of the plot to be equal\n    ax.set_aspect(\"equal\")\n    # set the title of the plot\n    plt.title(\"UMAP projection\")\n    # save the plot as a PNG file\n    plt.savefig(\"umap.png\")\n    # return the figure\n    return fig\n", "CodeBleu_score": 43.276496852181424}, {"code": "def plot_embeddings(embeddings, num_classes_in_batch)\n\t\n    # Calculate the number of utterances per class\n    num_utterances_per_class = [\n        np.sum(embeddings.labels == i) for i in range(num_classes_in_batch)\n    ]\n    # Limit the number of classes to 10 if necessary\n    if len(num_utterances_per_class) > 10:\n        num_utterances_per_class = num_utterances_per_class[:10]\n    # Transform the embeddings using UMAP\n    reducer = umap.UMAP()\n    embeddings_transformed = reducer.fit_transform(embeddings.embeddings)\n    # Create a scatter plot with colors representing different classes\n    fig, ax = plt.subplots(figsize=(10, 10))\n    for i in range(len(num_utterances_per_class)):\n        ax.scatter(\n            embeddings_transformed[\n                np.where(embeddings.labels == i), 0\n            ],  # x-coordinates\n            embeddings_transformed[\n                np.where(embeddings.labels == i), 1\n            ],  # y-coordinates\n            label=f\"Class {i}\",\n        )\n    # Set the title, adjust for equal aspect ratio, save as \"umap\", and return the figure\n    plt.title(\"UMAP projection\")\n    ax.set_aspect(\"equal\")\n    plt.savefig(\"umap.png\")\n    return fig", "CodeBleu_score": 35.41519388391393}, {"code": "def plot_embeddings(embeddings, num_classes_in_batch)\n\t\n    # Limit the number of classes to 10\n    if num_classes_in_batch > 10:\n        num_classes_in_batch = 10\n    # Calculate the number of utterances per class\n    num_utterances_per_class = int(len(embeddings) / num_classes_in_batch)\n    # Create a color palette for the classes\n    colors = plt.cm.rainbow(np.linspace(0, 1, num_classes_in_batch))\n    # Create a subplot\n    fig, ax = plt.subplots()\n    # Loop through the classes\n    for i in range(num_classes_in_batch):\n        # Get the embeddings for the current class\n        class_embeddings = embeddings[i * num_utterances_per_class:(i + 1) * num_utterances_per_class]\n        # Use UMAP to reduce the dimensionality of the embeddings\n        umap_embeddings = umap.UMAP(n_components=2).fit_transform(class_embeddings)\n        # Plot the embeddings with a scatter plot\n        ax.scatter(umap_embeddings[:, 0], umap_embeddings[:, 1], c=colors[i], label=f\"Class {i}\")\n    # Set the title of the plot\n    plt.title(\"UMAP projection\")\n    # Adjust the aspect ratio of the plot\n    ax.set_aspect(\"equal\", \"box\")\n    # Save the plot as a PNG file\n    plt.savefig(\"umap.png\")\n    # Return the figure\n    return fig", "CodeBleu_score": 44.43437704790956}]}
{"instance_id": "19c2b4af256d", "generation_result": [{"code": "def calc_cosine_sim(self, dvecs, centroids)\n\t\n    cos_sim_matrix = []\n    for spkr, dvec in enumerate(dvecs):\n        for utt in range(len(dvec)):\n            new_centroids = self.calc_new_centroids(dvecs, centroids, spkr, utt)\n            cs_row = []\n            for centroid in new_centroids:\n                cs_row.append(\n                    torch.clamp(\n                        torch.mm(\n                            dvec[utt].unsqueeze(0), centroid.unsqueeze(1)\n                        )\n                        / (\n                            torch.norm(dvec[utt])\n                            * torch.norm(centroid)\n                            + self.epsilon\n                        ),\n                        min=0,\n                    )\n                )\n            cos_sim_matrix.append(torch.stack(cs_row))\n    return torch.stack(cos_sim_matrix)\n\n", "CodeBleu_score": 46.83136260866855}, {"code": "def calc_cosine_sim(self, dvecs, centroids)\n\t\n    cos_sim_matrix = []\n    for spkr in range(dvecs.shape[0]):\n        for utt in range(dvecs.shape[1]):\n            new_centroids = self.calc_new_centroids(dvecs, centroids, spkr, utt)\n            cs_row = []\n            for i in range(new_centroids.shape[0]):\n                cs_row.append(\n                    torch.clamp(\n                        torch.mm(dvecs[spkr, utt].unsqueeze(0), new_centroids[i].transpose(0, 1))\n                        / (\n                            torch.norm(dvecs[spkr, utt])\n                            * torch.norm(new_centroids[i])\n                            + self.eps\n                        ),\n                        min=0,\n                    )\n                )\n            cos_sim_matrix.append(torch.cat(cs_row))\n    return torch.stack(cos_sim_matrix)\n\n", "CodeBleu_score": 44.80738687709031}, {"code": "def calc_cosine_sim(self, dvecs, centroids)\n\t\n    cos_sim_matrix = []\n    for spkr in range(dvecs.shape[0]):\n        for utt in range(dvecs.shape[1]):\n            new_centroids = self.calc_new_centroids(dvecs, centroids, spkr, utt)\n            cs_row = []\n            for i in range(dvecs.shape[0]):\n                dvec = dvecs[i, utt]\n                cs = torch.mm(dvec.unsqueeze(0), new_centroids[i].unsqueeze(1)) / (\n                    torch.norm(dvec) * torch.norm(new_centroids[i])\n                )\n                cs = torch.clamp(cs, min=0.000001)\n                cs_row.append(cs)\n            cos_sim_matrix.append(torch.cat(cs_row))\n    return torch.stack(cos_sim_matrix)\n\n", "CodeBleu_score": 40.303061676405285}]}
{"instance_id": "c525b411a1e7", "generation_result": [{"code": " def embed_loss_softmax(self, dvecs, cos_sim_matrix)\n\t\n    # Initialize empty list to store losses\n    losses = []\n\n    # Iterate over the embeddings\n    for i, dvec in enumerate(dvecs):\n        # Calculate the negative log softmax of the cosine similarity matrix for the current embedding\n        loss = F.nll_loss(F.log_softmax(cos_sim_matrix[i], dim=0), torch.LongTensor([i]))\n        # Append the loss to the list\n        losses.append(loss)\n\n    # Stack the losses into a tensor\n    losses = torch.stack(losses)\n\n    # Return the final stacked tensor of losses\n    return losses\n", "CodeBleu_score": 32.024185978465624}, {"code": " def embed_loss_softmax(self, dvecs, cos_sim_matrix)\n\t\n    losses = []\n    for i, dvec in enumerate(dvecs):\n        cos_sim_matrix_i = cos_sim_matrix[i]\n        cos_sim_matrix_i[i] = -1\n        # print(cos_sim_matrix_i)\n        log_softmax = F.log_softmax(cos_sim_matrix_i, dim=0)\n        # print(log_softmax)\n        # print(dvec)\n        loss_i = -log_softmax[dvec]\n        # print(loss_i)\n        losses.append(loss_i)\n    return torch.stack(losses)\n", "CodeBleu_score": 31.708800816133}, {"code": " def embed_loss_softmax(self, dvecs, cos_sim_matrix)\n\t\n    # Initialize the loss tensor\n    loss = torch.zeros(1, device=self.device)\n    # Iterate over the embeddings\n    for i in range(len(dvecs)):\n        # Compute the negative log softmax of the cosine similarity matrix for the current embedding\n        loss = loss - F.log_softmax(cos_sim_matrix[i], dim=0)[i]\n    # Stack the losses for each row into a tensor\n    loss = torch.stack(loss)\n    # Return the final stacked tensor of losses\n    return loss\n", "CodeBleu_score": 30.364035898719372}]}
{"instance_id": "25ac3945da2e", "generation_result": [{"code": "def embed_loss_contrast(self, dvecs, cos_sim_matrix)\n\t\n    loss_row = []\n    for i in range(len(dvecs)):\n        loss_col = []\n        for j in range(len(dvecs)):\n            if i == j:\n                continue\n            loss_col.append(torch.sigmoid(cos_sim_matrix[i][j]))\n        loss_row.append(torch.stack(loss_col))\n    return torch.stack(loss_row)\n", "CodeBleu_score": 29.037023312518258}, {"code": "def embed_loss_contrast(self, dvecs, cos_sim_matrix)\n\t\n    L_row = []\n    for i, dvec in enumerate(dvecs):\n        cos_sim_row = cos_sim_matrix[i]\n        cos_sim_row_max, _ = torch.max(cos_sim_row, dim=0)\n        cos_sim_row_max = cos_sim_row_max.unsqueeze(0)\n        cos_sim_row_max[i] = -1\n        sigmoid_row = torch.sigmoid(cos_sim_row_max)\n        loss_row = -torch.log(sigmoid_row)\n        L_row.append(loss_row)\n    L = torch.stack(L_row)\n    return L", "CodeBleu_score": 31.809679797017772}, {"code": "def embed_loss_contrast(self, dvecs, cos_sim_matrix)\n\t\n    loss_row = []\n    for i, dvec in enumerate(dvecs):\n        cos_sim_row = cos_sim_matrix[i]\n        max_cos_sim, _ = torch.max(cos_sim_row, dim=0)\n        cos_sim_row[i] = max_cos_sim\n        loss_row.append(torch.log(1 + torch.sum(torch.exp(cos_sim_row))))\n    return torch.stack(loss_row)\n", "CodeBleu_score": 22.53602816442273}]}
{"instance_id": "878d14ffabd2", "generation_result": [{"code": "def forward(self, x, _label=None)\n\t\n    # Ensure at least two utterances per speaker\n    assert x.size(1) >= 2, \"Number of utterances per speaker must be at least two\"\n    # Compute centroids by averaging embeddings\n    centroids = self.calc_centroids(x)\n    # Calculate cosine similarity matrix between embeddings and centroids\n    cos_sim_matrix = self.calc_cosine_sim(x, centroids)\n    # Scale and shift similarity matrix\n    scaled_cos_sim_matrix = (\n        self.scale_factor * cos_sim_matrix + self.shift_factor\n    )  # (N,M,N)\n    # Calculate embedding loss\n    embed_loss = torch.mean(\n        torch.clamp(\n            1 - scaled_cos_sim_matrix,\n            min=0.0,\n        )\n    )\n    return embed_loss\n\n", "CodeBleu_score": 43.96672905331977}, {"code": "def forward(self, x, _label=None)\n\t\n    if x.size(1) < 2:\n        raise ValueError(\"Number of utterances per speaker must be at least two.\")\n    centroids = self.calc_centroids(x)\n    cos_sim_matrix = self.calc_cosine_sim(x, centroids)\n    loss = self.embed_loss(cos_sim_matrix)\n    return loss.mean()\n", "CodeBleu_score": 45.902606621274664}, {"code": "def forward(self, x, _label=None)\n\t\n    # x: (N, M, D)\n    # _label: (N,)\n    # N: number of speakers\n    # M: number of utterances per speaker\n    # D: d-vector dimension\n    # Check that dimensions match\n    assert x.size(0) == _label.size(0), \"The batch size for x and label must match\"\n    assert (\n        x.size(1) >= 2\n    ), \"GE2E loss is calculated using at least 2 utterances per speaker\"\n    # Calculate centroids for each speaker\n    centroids = self.calc_centroids(x, _label)\n    # Calculate cosine similarity matrix\n    cos_sim_matrix = self.calc_cosine_sim(x, centroids)\n    # Calculate embedding loss\n    embed_loss = self.calc_embed_loss(cos_sim_matrix, _label)\n    # Return mean loss\n    return self.embed_loss(embed_loss)\n\n", "CodeBleu_score": 41.38604321345777}]}
{"instance_id": "86a364c72caa", "generation_result": [{"code": "def get_voices(extra_voice_dirs: List[str] = [])\n\toices = {}\n    for voice_dir in [VOICE_DIR] + extra_voice_dirs:\n        for subdir, dirs, files in os.walk(voice_dir):\n            if subdir == voice_dir:\n                continue\n            voice_name = subdir.split(os.sep)[-1]\n            voice_files = []\n            for file in files:\n                if file.endswith(\".npz\"):\n                    voice_files.append(os.path.join(subdir, file))\n                elif file.endswith(\".wav\"):\n                    voice_files.append(os.path.join(subdir, file))\n                elif file.endswith(\".mp3\"):\n                    voice_files.append(os.path.join(subdir, file))\n            if len(voice_files) == 0:\n                for file in files:\n                    if file.endswith(\".wav\"):\n                        voice_files.append(os.path.join(subdir, file))\n                    elif file.endswith(\".mp3\"):\n                        voice_files.append(os.path.join(subdir, file))\n            voices[voice_name] = voice_files\n    return voices\n\n", "CodeBleu_score": 26.913843233913504}, {"code": "def get_voices(extra_voice_dirs: List[str] = [])\n\toice_dirs = [\n        *extra_voice_dirs,\n        *glob.glob(os.path.join(os.path.dirname(os.path.abspath(__file__)), \"voices\", \"*\")),\n    ]\n    voices = {}\n    for voice_dir in voice_dirs:\n        if not os.path.isdir(voice_dir):\n            continue\n        for root, _, files in os.walk(voice_dir):\n            if len(files) == 0:\n                continue\n            npz_files = [f for f in files if f.endswith(\".npz\")]\n            if len(npz_files) == 0:\n                wav_files = [f for f in files if f.endswith(\".wav\")]\n                mp3_files = [f for f in files if f.endswith(\".mp3\")]\n                voices[root] = [\n                    os.path.join(root, f) for f in wav_files + mp3_files if os.path.isfile(os.path.join(root, f))\n                ]\n            else:\n                voices[root] = [os.path.join(root, f) for f in npz_files]\n    return voices\n\n", "CodeBleu_score": 29.482852023523453}, {"code": "def get_voices(extra_voice_dirs: List[str] = [])\n\toices = {}\n    for voice_dir in extra_voice_dirs:\n        for root, dirs, files in os.walk(voice_dir):\n            if len(files) == 0:\n                continue\n            if len(list(filter(lambda x: x.endswith(\".npz\"), files))) == 0:\n                voices[root] = list(filter(lambda x: x.endswith(\".wav\") or x.endswith(\".mp3\"), [os.path.join(root, f) for f in files]))\n            else:\n                voices[root] = list(filter(lambda x: x.endswith(\".npz\"), [os.path.join(root, f) for f in files]))\n    return voices\n\n", "CodeBleu_score": 29.32302904154242}]}
{"instance_id": "02f20c8917e6", "generation_result": [{"code": "def load_voice(model, voice: str, extra_voice_dirs: List[str] = [])\n\t  if voice == \"random\":\n        return None, None, None\n\n    voices = get_voices(extra_voice_dirs)\n    paths = voices[voice]\n\n    # bark only uses a single sample for cloning\n    if len(paths) > 1:\n        raise ValueError(f\"Voice {voice} has multiple paths: {paths}\")\n\n    try:\n        path = voices[voice]\n    except KeyError as e:\n        raise KeyError(f\"Voice {voice} not found in {extra_voice_dirs}\") from e\n\n    if len(paths) == 1 and paths[0].endswith(\".npz\"):\n        return load_npz(path[0])\n\n    audio_path = paths[0]\n    # replace the file extension with.npz\n    output_path = os.path.splitext(audio_path)[0] + \".npz\"\n    generate_voice(audio=audio_path, model=model, output_path=output_path)\n    return load_voice(model, voice, extra_voice_dirs)\n\n", "CodeBleu_score": 96.09328500721692}, {"code": "def load_voice(model, voice: str, extra_voice_dirs: List[str] = [])\n\t  if voice == \"random\":\n        return None, None, None\n\n    voices = get_voices(extra_voice_dirs)\n    paths = voices[voice]\n\n    # bark only uses a single sample for cloning\n    if len(paths) > 1:\n        raise ValueError(f\"Voice {voice} has multiple paths: {paths}\")\n\n    try:\n        path = voices[voice]\n    except KeyError as e:\n        raise KeyError(f\"Voice {voice} not found in {extra_voice_dirs}\") from e\n\n    if len(paths) == 1 and paths[0].endswith(\".npz\"):\n        return load_npz(path[0])\n\n    audio_path = paths[0]\n    # replace the file extension with.npz\n    output_path = os.path.splitext(audio_path)[0] + \".npz\"\n    generate_voice(audio=audio_path, model=model, output_path=output_path)\n    return load_voice(model, voice, extra_voice_dirs)\n\n", "CodeBleu_score": 96.09328500721692}, {"code": "def load_voice(model, voice: str, extra_voice_dirs: List[str] = [])\n\t  if voice == \"random\":\n        return None, None, None\n\n    voices = get_voices(extra_voice_dirs)\n    paths = voices[voice]\n\n    # bark only uses a single sample for cloning\n    if len(paths) > 1:\n        raise ValueError(f\"Voice {voice} has multiple paths: {paths}\")\n\n    try:\n        path = voices[voice]\n    except KeyError as e:\n        raise KeyError(f\"Voice {voice} not found in {extra_voice_dirs}\") from e\n\n    if len(paths) == 1 and paths[0].endswith(\".npz\"):\n        return load_npz(path[0])\n\n    audio_path = paths[0]\n    # replace the file extension with.npz\n    output_path = os.path.splitext(audio_path)[0] + \".npz\"\n    generate_voice(audio=audio_path, model=model, output_path=output_path)\n    return load_voice(model, voice, extra_voice_dirs)\n\n", "CodeBleu_score": 96.09328500721692}]}
{"instance_id": "9458e981389f", "generation_result": [{"code": "def _relative_shift(self, pos_score: torch.Tensor) -> torch.Tensor\n\t\n    batch_size, num_heads, seq_length, _ = pos_score.size()\n    pos_score = pos_score.view(batch_size, num_heads, seq_length, -1)\n    zero_tensor = torch.zeros(pos_score.size(), device=pos_score.device)\n    pos_score = torch.cat([zero_tensor, pos_score], dim=-1)\n    pos_score = pos_score.view(batch_size, num_heads, seq_length + 1, -1)\n    pos_score = pos_score[:, :, :-1, :]\n    pos_score = pos_score.view(batch_size, num_heads, seq_length, -1)\n    return pos_score\n\n", "CodeBleu_score": 36.85656195388531}, {"code": "def _relative_shift(self, pos_score: torch.Tensor) -> torch.Tensor\n\t\n    batch_size, num_heads, length, _ = pos_score.size()\n    pos_score = pos_score.view(batch_size, num_heads, length, length)\n    pos_score = torch.cat([torch.zeros(batch_size, num_heads, 1, length).type_as(pos_score), pos_score], dim=-1)\n    pos_score = pos_score.view(batch_size, num_heads, length, 2 * length - 1)\n    pos_score = pos_score[:, :, :, length - 1:]\n    return pos_score.view(batch_size, num_heads, length, length)\n", "CodeBleu_score": 33.20610056696878}, {"code": "def _relative_shift(self, pos_score: torch.Tensor) -> torch.Tensor\n\t\n    batch_size, num_heads, length, _, max_relative_position = pos_score.size()\n    pos_score_shape = (batch_size, num_heads, length, length + max_relative_position - 1)\n    pos_score = pos_score.view(*pos_score_shape)\n    zero_pad = torch.zeros((batch_size, num_heads, length, 1), device=pos_score.device, dtype=pos_score.dtype)\n    padded_pos_score = torch.cat((pos_score, zero_pad), dim=-1)\n    padded_pos_score = padded_pos_score.view_as(pos_score)\n    return padded_pos_score[..., max_relative_position - 1:]\n", "CodeBleu_score": 33.03917900083262}]}
{"instance_id": "c046d4f00e80", "generation_result": [{"code": "def run_padded_sequence(self, context, lens)\n\t\n    context_embedded = []\n    for i in range(context.size(0)):\n        context_embedded.append(self.embed(context[i, :lens[i]]))\n    context_embedded = torch.nn.utils.rnn.pad_sequence(context_embedded, batch_first=True)\n    context_embedded = self.dropout(context_embedded)\n    for conv in self.convs:\n        context_embedded = conv(context_embedded)\n        context_embedded = torch.nn.functional.relu(context_embedded)\n        context_embedded = self.dropout(context_embedded)\n    return context_embedded\n", "CodeBleu_score": 34.74371409625101}, {"code": "def run_padded_sequence(self, context, lens)\n\t\n    context_embedded = []\n    for i in range(context.size(1)):\n        context_embedded.append(self.context_embedding(context[:, i, :]))\n    context_embedded = torch.stack(context_embedded, dim=1)\n    context_embedded = self.dropout(context_embedded)\n    context_embedded = torch.transpose(context_embedded, 1, 2)\n    for conv in self.convs:\n        context_embedded = conv(context_embedded)\n        context_embedded = F.relu(context_embedded)\n        context_embedded = self.dropout(context_embedded)\n    context_embedded = torch.transpose(context_embedded, 1, 2)\n    context_embedded = torch.stack([context_embedded[i, :lens[i], :] for i in range(context_embedded.size(0))], dim=0)\n    context_embedded = pad_sequence(context_embedded, batch_first=True, padding_value=0)\n    return context_embedded\n", "CodeBleu_score": 31.955296730775174}, {"code": "def run_padded_sequence(self, context, lens)\n\t\n    context_embedded = []\n    for i in range(context.size(0)):\n        context_embedded.append(self.embedding(context[i, :lens[i]]))\n    context_embedded = torch.stack(context_embedded, 0)\n    context_embedded = self.dropout(context_embedded)\n    context_embedded = context_embedded.permute(1, 2, 0)\n    for conv in self.convs:\n        context_embedded = F.relu(conv(context_embedded))\n    context_embedded = context_embedded.permute(2, 0, 1)\n    context_embedded = pad_sequence(context_embedded, batch_first=True)\n    return context_embedded\n\n", "CodeBleu_score": 36.40404477258897}]}
{"instance_id": "db2644f3c681", "generation_result": [{"code": "def run_unsorted_inputs(self, fn, context, lens)\n\t\n    lens_sorted, indices = torch.sort(lens, descending=True)\n    context_sorted = context.index_select(0, indices)\n    context_packed = pack_padded_sequence(context_sorted, lens_sorted.long().cpu().numpy(), batch_first=True)\n    context_out_packed = fn(context_packed)\n    context_out_sorted = pad_packed_sequence(context_out_packed, batch_first=True)[0]\n    context_out = context_out_sorted.index_select(0, indices.sort(0, descending=True)[1])\n    return context_out\n\n", "CodeBleu_score": 17.52838474275553}, {"code": "def run_unsorted_inputs(self, fn, context, lens)\n\t\n    lens_sorted, indices = torch.sort(lens, descending=True)\n    context_sorted = context.index_select(0, indices)\n    context_packed = pack_padded_sequence(context_sorted, lens_sorted.long().cpu(), batch_first=True)\n    context_packed_out = fn(context_packed)\n    context_out, _ = pad_packed_sequence(context_packed_out, batch_first=True)\n    _, indices_inv = torch.sort(indices)\n    context_out = context_out.index_select(0, indices_inv)\n    return context_out\n", "CodeBleu_score": 18.184583296158852}, {"code": "def run_unsorted_inputs(self, fn, context, lens)\n\t\n    # Sort the inputs by length\n    lens_sorted, indices = torch.sort(lens, descending=True)\n    context_sorted = context.index_select(0, indices)\n    # Pack the context sequences\n    packed = pack_padded_sequence(context_sorted, lens_sorted, batch_first=True)\n    # Apply the function to the packed sequences\n    packed_out = fn(packed)\n    # Unpack the sequences\n    unpacked, _ = pad_packed_sequence(packed_out, batch_first=True)\n    # Map the results back to the original indices\n    _, inv_indices = torch.sort(indices)\n    unpacked_sorted = unpacked.index_select(0, inv_indices)\n    return unpacked_sorted\n", "CodeBleu_score": 15.634328665070088}]}
{"instance_id": "ced082bffc63", "generation_result": [{"code": "def remove_weight_norm(self)\n\t\n    self.kernel_predictor.remove_weight_norm()\n    parametrize.remove_parametrizations(self.convt_pre[1], \"weight\")\n    for block in self.conv_blocks:\n        parametrize.remove_parametrizations(block[1], \"weight\")\n", "CodeBleu_score": 70.15353357610886}, {"code": "def remove_weight_norm(self)\n\t\n    self.kernel_predictor.remove_weight_norm()\n    parametrize.remove_parametrizations(self.convt_pre[1], \"weight\")\n    for block in self.conv_blocks:\n        parametrize.remove_parametrizations(block[1], \"weight\")\n\n", "CodeBleu_score": 70.15353357610886}, {"code": "def remove_weight_norm(self)\n\t\n    self.kernel_predictor.remove_weight_norm()\n    parametrize.remove_parametrizations(self.convt_pre[1], \"weight\")\n    for block in self.conv_blocks:\n        parametrize.remove_parametrizations(block[1], \"weight\")\n", "CodeBleu_score": 70.15353357610886}]}
{"instance_id": "b5a39fd97061", "generation_result": [{"code": "def map_mx_to_torch_model(mx_name)\n\t   mx_name = mx_name.lower()\n    if mx_name.startswith('se_'):\n        mx_name = 'gluon_se_' + mx_name[3:]\n    elif mx_name.startswith('senet_'):\n        mx_name = 'gluon_senet_' + mx_name[6:]\n    elif mx_name == 'inceptionv3':\n        mx_name = 'gluon_inceptionv3'\n    return 'gluon_' + mx_name\n\n", "CodeBleu_score": 34.049017624475525}, {"code": "def map_mx_to_torch_model(mx_name)\n\t   mx_name = mx_name.lower()\n    if mx_name.startswith('se_'):\n        mx_name = mx_name.replace('se_', 'gluon_se_')\n    elif mx_name.startswith('senet_'):\n        mx_name = mx_name.replace('senet_', 'gluon_se_')\n    elif mx_name.startswith('inceptionv3'):\n        mx_name = mx_name.replace('inceptionv3', 'gluon_inception_v3')\n    mx_name = 'gluon_' + mx_name\n    return mx_name\n\n", "CodeBleu_score": 48.640671564482105}, {"code": "def map_mx_to_torch_model(mx_name)\n\t   mx_name = mx_name.lower()\n    if mx_name.startswith('se_'):\n        mx_name = mx_name.replace('se_', 'gluon_se_')\n    elif mx_name.startswith('senet_'):\n        mx_name = mx_name.replace('senet_', 'gluon_se_')\n    elif mx_name.startswith('inceptionv3'):\n        mx_name = mx_name.replace('inceptionv3', 'gluon_inceptionv3')\n    mx_name = 'gluon_' + mx_name\n    return mx_name\n\n", "CodeBleu_score": 48.52221083684156}]}
{"instance_id": "7eee5efdbf67", "generation_result": [{"code": "def _num_samples_per_worker(self)\n\t\n    if self.training:\n        if self.num_replicas > 1:\n            num_samples_per_worker = math.ceil(self.num_samples / self.num_replicas)\n        else:\n            num_samples_per_worker = self.num_samples\n    else:\n        num_samples_per_worker = self.num_samples\n    if self.batch_size is not None:\n        num_samples_per_worker = math.ceil(num_samples_per_worker / self.batch_size) * self.batch_size\n    return int(num_samples_per_worker)", "CodeBleu_score": 46.60723375362435}, {"code": "def _num_samples_per_worker(self)\n\t\n    if self.num_replicas is not None:\n        num_workers = self.num_replicas\n    else:\n        num_workers = self.num_workers\n    if self.num_workers is None:\n        num_workers = 1\n    if self.batch_size is not None:\n        num_samples = math.ceil(self.num_samples * self.num_repeats / num_workers)\n        num_samples = math.ceil(num_samples / self.batch_size) * self.batch_size\n    else:\n        num_samples = math.ceil(self.num_samples * self.num_repeats / num_workers)\n    return int(num_samples)\n", "CodeBleu_score": 44.59706120857874}, {"code": "def _num_samples_per_worker(self)\n\t\n    if self.num_workers is None:\n        return None\n    if self.num_replicas is not None:\n        num_samples = self.num_replicas * self.num_samples\n    else:\n        num_samples = self.num_samples\n    if self.num_workers > 1:\n        num_samples = int(math.ceil(num_samples / self.num_workers))\n    if self.batch_size is not None:\n        num_samples = int(math.ceil(num_samples / self.batch_size)) * self.batch_size\n    return num_samples\n\n", "CodeBleu_score": 40.65141151262905}]}
{"instance_id": "a698d65ae190", "generation_result": [{"code": "def find_images_and_targets(\n        folder: str,\n        types: Optional[Union[List, Tuple, Set]] = None,\n        class_to_idx: Optional[Dict] = None,\n        leaf_name_only: bool = True,\n        sort: bool = True\n)\n\t\n    if types is None:\n        types = ['.jpg', '.jpeg', '.png', '.ppm', '.bmp', '.pgm', '.tif']\n    elif isinstance(types, str):\n        types = [types]\n    elif not isinstance(types, (list, tuple, set)):\n        raise TypeError('Argument `types` should be a string or a list/tuple/set of strings, not {}.'.format(type(types)))\n\n    if not isinstance(folder, str):\n        raise TypeError('Argument `folder` should be a string, not {}.'.format(type(folder)))\n    if not os.path.isdir(folder):\n        raise NotADirectoryError('Argument `folder` should be a folder, not a file.')\n\n    if class_to_idx is None:\n        class_to_idx = {}\n    elif not isinstance(class_to_idx, dict):\n        raise TypeError('Argument `class_to_idx` should be a dictionary, not {}.'.format(type(class_to_idx)))\n\n    filenames = []\n    labels = []\n    for root, _, files in os.walk(folder):\n        for filename in files:\n            _, ext = os.path.splitext(filename)\n            if ext.lower() not in types:\n                continue\n            if leaf_name_only:\n                rel_path = os.path.relpath(root, folder)\n                if rel_path.startswith(os.path.sep):\n                    rel_path = rel_path[len(os.path.sep):]\n                label = os.path.basename(rel_path.replace(os.path.sep, '_').lower())\n            else:\n                label = os.path.join(root, filename).lower()\n            if label not in class_to_idx:\n                class_to_idx[label] = len(class_to_idx)\n            labels.append(class_to_idx[label])\n            filenames.append(os.path.join(root, filename))\n    if sort:\n        filenames, labels = zip(*sorted(zip(filenames, labels), key=lambda p: natural_key(p[0])))\n    return filenames, labels, class_to_idx\n\n", "CodeBleu_score": 42.473023879187636}, {"code": "def find_images_and_targets(\n        folder: str,\n        types: Optional[Union[List, Tuple, Set]] = None,\n        class_to_idx: Optional[Dict] = None,\n        leaf_name_only: bool = True,\n        sort: bool = True\n)\n\t\n    if types is None:\n        types = ['.jpg', '.jpeg', '.png', '.ppm', '.bmp', '.pgm', '.tif']\n    if not isinstance(types, (list, tuple, set)):\n        raise ValueError('Argument \"types\" should be a list or tuple or set, not {}'.format(type(types)))\n    if class_to_idx is None:\n        class_to_idx = {}\n    if not isinstance(class_to_idx, dict):\n        raise ValueError('Argument \"class_to_idx\" should be a dict, not {}'.format(type(class_to_idx)))\n    rel_paths_and_classes = []\n    for dirpath, _, fnames in sorted(os.walk(folder)):\n        for fname in sorted(fnames):\n            if is_image_file(fname, types):\n                path = os.path.join(dirpath, fname)\n                if leaf_name_only:\n                    rel_path = os.path.relpath(path, folder)\n                    parts = rel_path.replace(os.path.sep, '/').split('/')\n                    cls = parts[-2] if len(parts) > 1 else parts[0]\n                else:\n                    rel_path = os.path.relpath(path, folder)\n                    cls = rel_path.replace(os.path.sep, '/').rsplit('/', 1)[0]\n                if cls not in class_to_idx:\n                    new_class_id = len(class_to_idx)\n                    class_to_idx[cls] = new_class_id\n                index = class_to_idx[cls]\n                item = (path, index)\n                rel_paths_and_classes.append(item)\n    if sort:\n        rel_paths_and_classes = sorted(rel_paths_and_classes, key=lambda x: natural_key(x[0]))\n    return rel_paths_and_classes, class_to_idx\n\n", "CodeBleu_score": 37.847908956158356}, {"code": "def find_images_and_targets(\n        folder: str,\n        types: Optional[Union[List, Tuple, Set]] = None,\n        class_to_idx: Optional[Dict] = None,\n        leaf_name_only: bool = True,\n        sort: bool = True\n)\n\t\n    if types is None:\n        types = ['.jpg', '.jpeg', '.png', '.ppm', '.bmp', '.pgm', '.tif']\n    elif isinstance(types, str):\n        types = [types]\n    elif not isinstance(types, (list, tuple, set)):\n        raise TypeError('Argument `types` should be a string or a list/tuple/set of strings, not {}'.format(type(types)))\n\n    if not isinstance(leaf_name_only, bool):\n        raise TypeError('Argument `leaf_name_only` should be a boolean, not {}'.format(type(leaf_name_only)))\n\n    if not isinstance(sort, bool):\n        raise TypeError('Argument `sort` should be a boolean, not {}'.format(type(sort)))\n\n    if not os.path.isdir(folder):\n        raise NotADirectoryError('`folder` argument is not a valid directory')\n\n    images = []\n    labels = []\n\n    for root, _, files in os.walk(folder):\n        for fname in files:\n            is_valid = False\n            for ext in types:\n                if fname.lower().endswith(ext):\n                    is_valid = True\n                    break\n            if not is_valid:\n                continue\n            path = os.path.join(root, fname)\n            if leaf_name_only:\n                indir = ''\n            else:\n                indir = root + os.sep\n            if class_to_idx is not None:\n                label = class_to_idx[indir]\n            else:\n                label = indir\n                if class_to_idx is None:\n                    class_to_idx = {}\n            images.append(path)\n            labels.append(label)\n\n    if class_to_idx is None:\n        class_to_idx = {k: v for v, k in enumerate(sorted(set(labels), key=natural_key))}\n    zipped = sorted(zip(images, labels), key=lambda p: natural_key(p[0]))\n\n    return zipped, class_to_idx\n\n", "CodeBleu_score": 38.9201330441871}]}
{"instance_id": "e7fe43bdecdc", "generation_result": [{"code": "def pytorch_worker_seed()\n\t\n    worker_info = get_worker_info()\n    if worker_info is not None:\n        # favour the seed already created for pytorch dataloader workers if it exists\n        return worker_info.seed\n    # fallback to wds rank based seed\n    return wds.utils.pytorch_worker_seed()\n\n", "CodeBleu_score": 66.80150511570375}, {"code": "def pytorch_worker_seed()\n\t\n    worker_info = get_worker_info()\n    if worker_info is not None:\n        # favour the seed already created for pytorch dataloader workers if it exists\n        return worker_info.seed\n    # fallback to wds rank based seed\n    return wds.utils.pytorch_worker_seed()\n\n", "CodeBleu_score": 66.80150511570375}, {"code": "def pytorch_worker_seed()\n\t\n    worker_info = get_worker_info()\n    if worker_info is not None:\n        # favour the seed already created for pytorch dataloader workers if it exists\n        return worker_info.seed\n    # fallback to wds rank based seed\n    return wds.utils.pytorch_worker_seed()\n\n", "CodeBleu_score": 66.80150511570375}]}
{"instance_id": "0ffc7e7ded25", "generation_result": [{"code": "def solarize_add(img, add, thresh=128, **__)\n\t\n    if img.mode in (\"L\", \"RGB\"):\n        lut = []\n        for i in range(256):\n            if i < thresh:\n                lut.append(i + add)\n            else:\n                lut.append(i)\n        return img.point(lut)\n    return img\n\n", "CodeBleu_score": 51.013637426595906}, {"code": "def solarize_add(img, add, thresh=128, **__)\n\t\n    if img.mode not in (\"L\", \"RGB\"):\n        return img\n    lut = []\n    for i in range(256):\n        if i < thresh:\n            lut.append(min(i + add, 255))\n        else:\n            lut.append(i)\n    return img.point(lut)\n", "CodeBleu_score": 49.39223053797332}, {"code": "def solarize_add(img, add, thresh=128, **__)\n\t\n    if img.mode == \"L\":\n        lut = []\n        for i in range(256):\n            if i < thresh:\n                lut.append(i + add)\n            else:\n                lut.append(i)\n        return img.point(lut)\n    elif img.mode == \"RGB\":\n        lut = []\n        for i in range(256):\n            if i < thresh:\n                lut.append(i + add)\n            else:\n                lut.append(i)\n        return img.convert(\"RGB\").point(lut)\n    else:\n        return img\n", "CodeBleu_score": 51.80643663051773}]}
{"instance_id": "5a0d29577a21", "generation_result": [{"code": "def _get_weighted_transforms(transforms: Dict)\n\t\n    transform_names = list(transforms.keys())\n    transform_probabilities = list(transforms.values())\n    transform_probabilities = np.array(transform_probabilities)\n    transform_probabilities = transform_probabilities / np.sum(transform_probabilities)\n    return transform_names, transform_probabilities\n\n", "CodeBleu_score": 17.31199721575088}, {"code": "def _get_weighted_transforms(transforms: Dict)\n\t\n    return zip(*[(k, v / sum(transforms.values())) for k, v in transforms.items()])\n\n", "CodeBleu_score": 20.7334385264274}, {"code": "def _get_weighted_transforms(transforms: Dict)\n\t\n    transform_names = list(transforms.keys())\n    transform_probs = list(transforms.values())\n    transform_probs = [float(p) / sum(transform_probs) for p in transform_probs]\n    return transform_names, transform_probs\n\n", "CodeBleu_score": 20.36075162399639}]}
{"instance_id": "84bc5658c135", "generation_result": [{"code": "def _apply_basic(self, img, mixing_weights, m)\n\t\n    mixed = np.zeros(img.shape, dtype=img.dtype)\n    for w in mixing_weights:\n        seq = self._get_sequence(m)\n        for f in seq:\n            img = f(img)\n        mixed = mixed + w * img\n    mixed = np.clip(mixed, 0, 255).astype(img.dtype)\n    mixed = Image.fromarray(mixed)\n    return Image.blend(img, mixed, m)\n\n", "CodeBleu_score": 27.96926240256652}, {"code": "def _apply_basic(self, img, mixing_weights, m)\n\t\n    mixed = np.zeros(img.shape, dtype=np.float32)\n    for w in mixing_weights:\n        op = self._get_ops(m)\n        mixed += self._apply_ops(img, op, w)\n    mixed = np.clip(mixed, 0, 1)\n    mixed = self._to_image(mixed)\n    mixed = Image.blend(img, mixed, m)\n    return mixed\n", "CodeBleu_score": 22.081800145276038}, {"code": "def _apply_basic(self, img, mixing_weights, m)\n\t\n    mixed = np.zeros(img.shape, dtype=img.dtype)\n    for weight in mixing_weights:\n        m.reset()\n        img_aug = self._apply_sequence(img, m)\n        img_aug = np.clip(weight * img_aug, 0, 255).astype(img.dtype)\n        mixed = np.clip(mixed + img_aug, 0, 255).astype(img.dtype)\n    mixed = Image.fromarray(mixed)\n    mixed = Image.blend(img, mixed, m.params['blend'])\n    return mixed\n", "CodeBleu_score": 28.533514060212546}]}
{"instance_id": "84b5b752ff04", "generation_result": [{"code": "def adapt_to_chs(x, n)\n\t\n    if not isinstance(x, (tuple, list)):\n        x = (x,) * n\n    elif len(x)!= n:\n        _logger.warning(\n            \"Length of x does not match n. Mean of x is used to adapt x to n.\"\n        )\n        x = (np.mean(x),) * n\n    assert len(x) == n\n    return x\n\n", "CodeBleu_score": 41.698107007538695}, {"code": "def adapt_to_chs(x, n)\n\t\n    if not isinstance(x, (tuple, list)):\n        x = repeat(x, n)\n    elif len(x)!= n:\n        _logger.warning(\"Length of x is not equal to n. Replacing x with mean of x.\")\n        x = repeat(np.mean(x), n)\n    assert len(x) == n\n    return x\n\n", "CodeBleu_score": 37.10637361254453}, {"code": "def adapt_to_chs(x, n)\n\t\n    if not isinstance(x, (tuple, list)):\n        x = (x,) * n\n    elif len(x)!= n:\n        _logger.warning(\n            f\"The length of x ({len(x)}) does not match the required number of channels (n={n}).", "CodeBleu_score": 28.032943486254812}]}
{"instance_id": "54754b6ba394", "generation_result": [{"code": "def _worker_init(worker_id, worker_seeding='all')\n\t\n    worker_info = torch.utils.data.get_worker_info()\n    if worker_info is None:\n        return\n    worker_id = worker_info.id\n    if worker_seeding == 'all':\n        seed = worker_info.seed\n        torch.manual_seed(seed)\n        random.seed(seed)\n        np.random.seed(seed)\n    elif callable(worker_seeding):\n        worker_seeding(worker_id)\n    else:\n        raise TypeError('worker_seeding must be a callable or \"all\"')\n", "CodeBleu_score": 43.486712877933506}, {"code": "def _worker_init(worker_id, worker_seeding='all')\n\t\n    worker_info = torch.utils.data.get_worker_info()\n    if worker_info is not None:\n        worker_id = worker_info.id\n        worker_seeding = worker_info.seed\n    if worker_id!= 0 and worker_seeding is not None:\n        if callable(worker_seeding):\n            worker_seeding(worker_id)\n        elif isinstance(worker_seeding, str):\n            if worker_seeding == 'all':\n                torch.manual_seed(worker_info.seed)\n                random.seed(worker_info.seed)\n                np.random.seed(worker_info.seed)\n        else:\n            raise ValueError(f\"worker_seeding must be a callable or a string, but got {type(worker_seeding)}\")\n\n", "CodeBleu_score": 38.187523101326505}, {"code": "def _worker_init(worker_id, worker_seeding='all')\n\t\n    worker_info = torch.utils.data.get_worker_info()\n    if worker_info is None:\n        raise ValueError(\"torch.utils.data.DataLoader with num_workers > 0 \"\n                         \"and multi-process data loading is not supported \"\n                         \"when using custom collate_fn. Please set \"\n                         \"collate_fn=None.\")\n    if worker_id < 0:\n        raise ValueError(\"worker_id should be non-negative.\")\n    if worker_id >= worker_info.num_workers:\n        raise ValueError(\"worker_id should be less than the number of workers.\")\n    if isinstance(worker_seeding, str):\n        if worker_seeding == 'all':\n            seed = worker_info.seed\n        else:\n            raise ValueError(f\"worker_seeding should be either 'all' or a callable, but got {worker_seeding}.\")\n    else:\n        seed = worker_seeding(worker_id, worker_info.num_workers)\n    torch.manual_seed(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n\n", "CodeBleu_score": 41.33833261213567}]}
{"instance_id": "81dfdce940db", "generation_result": [{"code": "def _decode_and_center_crop(image_bytes, image_size, resize_method)\n\t\n    shape = tf.image.extract_jpeg_shape(image_bytes)\n    image_height = shape[0]\n    image_width = shape[1]\n    crop_size = tf.cast(tf.minimum(image_height, image_width), dtype=tf.int32)\n    offset_height = ((image_height - crop_size) + 1) // 2\n    offset_width = ((image_width - crop_size) + 1) // 2\n    image_bytes = tf.image.decode_and_crop_jpeg(\n        image_bytes,\n        [offset_height, offset_width, crop_size, crop_size],\n        channels=3,\n    )\n    image = tf.image.resize(\n        image_bytes,\n        [image_size, image_size],\n        method=resize_method,\n        preserve_aspect_ratio=False,\n    )\n    return image\n\n", "CodeBleu_score": 38.10544413642996}, {"code": "def _decode_and_center_crop(image_bytes, image_size, resize_method)\n\t\n    image_shape = tf.image.extract_jpeg_shape(image_bytes)\n    image_height = tf.cast(image_shape[0], dtype=tf.float32)\n    image_width = tf.cast(image_shape[1], dtype=tf.float32)\n    padded_center_crop_size = tf.cast(\n        ((image_size / (2.0 + _CROP_PADDING)) * tf.cast(\n            tf.minimum(image_height, image_width), dtype=tf.float32)),\n        dtype=tf.int32)\n    offset_height = ((image_height - padded_center_crop_size) + 1) // 2\n    offset_width = ((image_width - padded_center_crop_size) + 1) // 2\n    crop_window = tf.stack([offset_height, offset_width,\n                            padded_center_crop_size, padded_center_crop_size])\n    image = tf.image.decode_and_crop_jpeg(image_bytes, crop_window, channels=3)\n    image = tf.image.resize(image, [image_size, image_size], method=resize_method)\n    return image\n\n", "CodeBleu_score": 52.20284295500876}, {"code": "def _decode_and_center_crop(image_bytes, image_size, resize_method)\n\t\n    # Decode the image to a uint8 tensor.\n    image = tf.image.decode_jpeg(image_bytes, channels=3)\n    # Get the image shape and calculate the padded center crop size.\n    image_shape = tf.shape(image)\n    image_height = image_shape[0]\n    image_width = image_shape[1]\n    padded_center_crop_size = tf.cast(\n        ((image_size / (image_size + CROP_PADDING)) *\n         tf.cast(tf.minimum(image_height, image_width), tf.float32)),\n        tf.int32)\n    # Determine the offset for the crop window.\n    offset_height = ((image_height - padded_center_crop_size) + 1) // 2\n    offset_width = ((image_width - padded_center_crop_size) + 1) // 2\n    # Crop the image.\n    image = tf.image.crop_to_bounding_box(\n        image, offset_height, offset_width, padded_center_crop_size,\n        padded_center_crop_size)\n    # Resize the image.\n    image = tf.image.resize(\n        image, [image_size, image_size], method=resize_method)\n    return image\n\n", "CodeBleu_score": 50.40671716294833}]}
{"instance_id": "cf16a6d37111", "generation_result": [{"code": "def preprocess_for_train(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    image = _decode_and_random_crop(image_bytes, image_size, interpolation)\n    image = _flip(image)\n    image = tf.reshape(image, [image_size, image_size, 3])\n    image = tf.cast(image, tf.bfloat16 if use_bfloat16 else tf.float32)\n    return image\n\n", "CodeBleu_score": 34.55452050848393}, {"code": "def preprocess_for_train(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    image = _decode_and_random_crop(image_bytes, image_size, interpolation)\n    image = _flip(image)\n    image = tf.reshape(image, [image_size, image_size, 3])\n    image = tf.image.convert_image_dtype(image, dtype=tf.bfloat16 if use_bfloat16 else tf.float32)\n    return image\n\n", "CodeBleu_score": 42.898446301658076}, {"code": "def preprocess_for_train(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    image = _decode_and_random_crop(image_bytes, image_size, interpolation)\n    image = _flip(image)\n    image = tf.reshape(image, [image_size, image_size, 3])\n    image = tf.cast(image, dtype=tf.bfloat16 if use_bfloat16 else tf.float32)\n    return image\n\n", "CodeBleu_score": 40.92077124969757}]}
{"instance_id": "78b2a3bd2142", "generation_result": [{"code": "def preprocess_for_eval(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    image = _decode_and_center_crop(image_bytes, image_size, resize_method=interpolation)\n    image = tf.reshape(image, [image_size, image_size, 3])\n    if use_bfloat16:\n        image = tf.cast(image, tf.bfloat16)\n    else:\n        image = tf.cast(image, tf.float32)\n    return image\n\n", "CodeBleu_score": 38.502519510392695}, {"code": "def preprocess_for_eval(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    if interpolation == 'bicubic':\n        resize_method = tf.image.ResizeMethod.BICUBIC\n    elif interpolation == 'bilinear':\n        resize_method = tf.image.ResizeMethod.BILINEAR\n    elif interpolation == 'nearest':\n        resize_method = tf.image.ResizeMethod.NEAREST_NEIGHBOR\n    else:\n        raise ValueError('Invalid interpolation method: {}'.format(interpolation))\n\n    image = _decode_and_center_crop(image_bytes, image_size, resize_method)\n    image = tf.reshape(image, [image_size, image_size, 3])\n    if use_bfloat16:\n        image = tf.cast(image, tf.bfloat16)\n    else:\n        image = tf.cast(image, tf.float32)\n    return image\n\n", "CodeBleu_score": 56.26848212370427}, {"code": "def preprocess_for_eval(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    image = _decode_and_center_crop(image_bytes, image_size,\n                                    resize_method=interpolation)\n    image = tf.reshape(image, [image_size, image_size, 3])\n    image = tf.cast(image, dtype=tf.bfloat16 if use_bfloat16 else tf.float32)\n    return image\n\n", "CodeBleu_score": 42.52341967545225}]}
{"instance_id": "8bd4364a9a0e", "generation_result": [{"code": "def evaluate(self, model, eval_dataloader=None, output_file=\"merged\")\n\t\n    if eval_dataloader is None:\n        eval_dataloader = self.eval_dataloader\n    if self.compute_metrics is None:\n        raise ValueError(\"To be able to use the `evaluate()` method, you need to define a `compute_metrics` function in your Trainer subclass.\")\n    eval_dataloader = self.get_eval_dataloader(eval_dataloader)\n    eval_output_dir = self.args.output_dir\n    if self.is_world_master():\n        os.makedirs(eval_output_dir, exist_ok=True)\n    model.eval()\n    if self.args.past_index >= 0:\n        self._past = None\n    eval_loss = 0.0\n    eval_examples = 0\n    preds = None\n    out_label_ids = None\n    self.control = self.callback_handler.on_eval_begin(self.args, self.state, self.control)\n    for step, inputs in enumerate(eval_dataloader):\n        if self.args.past_index >= 0:\n            self._past = None\n        inputs = self._prepare_inputs(inputs)\n        if self.use_tpu:\n            with torch.xla.amp.autocast():\n                outputs = model(**inputs)\n        else:\n            outputs = model(**inputs)\n        logits = outputs[0]\n        if self.args.past_index >= 0:\n            self._past = outputs[self.args.past_index - 1]\n        eval_loss += outputs[self.args.eval_loss_index].mean().item()\n        if self.args.past_index >= 0:\n            self._past = None\n        eval_examples += self.args.eval_batch_size\n        if self.args.label_smoothing_factor > 0:\n            # TODO(thom): Remove and de-duplicate with training loss computation\n            labels = inputs[\"labels\"]\n            # Logits and labels are of shape (batch_size, sequence_length, config.vocab_size)\n            # Apply softmax on the last axis (sequence_length) to get probabilities\n            # Rearrange to have them of shape (batch_size, sequence_length, num_labels)\n            # Smooth the probabilities\n            if self.config.problem_type == \"multi_label_classification\":\n                loss_fct = BCEWithLogitsLoss()\n                # Only keep", "CodeBleu_score": 42.833592115738426}, {"code": "def evaluate(self, model, eval_dataloader=None, output_file=\"merged\")\n\t\n    if eval_dataloader is None:\n        eval_dataloader = self.eval_dataloader\n    if output_file is None:\n        output_file = \"merged\"\n    if self.compute_metrics is None:\n        raise ValueError(\"compute_metrics is not defined\")\n    if self.compute_metrics is not None:\n        self.compute_metrics = self.compute_metrics\n    if self.compute_metrics is None:\n        raise ValueError(\"compute_metrics is not defined\")\n    if self.compute_metrics is not None:\n        self.compute_metrics = self.compute_metrics\n    if self.compute_metrics is None:\n        raise ValueError(\"compute_metrics is not defined\")\n    if self.compute_metrics is not None:\n        self.compute_metrics = self.compute_metrics\n    if self.compute_metrics is None:\n        raise ValueError(\"compute_metrics is not defined\")\n    if self.compute_metrics is not None:\n        self.compute_metrics = self.compute_metrics\n    if self.compute_metrics is None:\n        raise ValueError(\"compute_metrics is not defined\")\n    if self.compute_metrics is not None:\n        self.compute_metrics = self.compute_metrics\n    if self.compute_metrics is None:\n        raise ValueError(\"compute_metrics is not defined\")\n    if self.compute_metrics is not None:\n        self.compute_metrics = self.compute_metrics\n    if self.compute_metrics is None:\n        raise ValueError(\"compute_metrics is not defined\")\n    if self.compute_metrics is not None:\n        self.compute_metrics = self.compute_metrics\n    if self.compute_metrics is None:\n        raise ValueError(\"compute_metrics is not defined\")\n    if self.compute_metrics is not None:\n        self.compute_metrics = self.compute_metrics\n    if self.compute_metrics is None:\n        raise ValueError(\"compute_metrics is not defined\")\n    if self.compute_metrics is not None:\n        self.compute_metrics = self.compute_metrics\n    if self.compute_metrics is None:\n        raise ValueError(\"compute_metrics is not defined\")\n    if self.compute_metrics is not None:\n        self.compute_metrics = self.compute_metrics\n    if self.compute_metrics is None:\n        raise ValueError(\"compute_metrics is not defined\")\n    if self.compute_metrics is not None:\n        self.compute_metrics", "CodeBleu_score": 32.40734675588348}, {"code": "def evaluate(self, model, eval_dataloader=None, output_file=\"merged\")\n\t\n    if eval_dataloader is None:\n        eval_dataloader = self.eval_dataloader\n    output_file = f\"{output_file}.json\"\n    self.logger.info(f\"***** Running evaluation *****\")\n    self.logger.info(f\"  Num examples = {self.num_eval_examples}\")\n    self.logger.info(f\"  Batch size = {self.eval_batch_size}\")\n    self.logger.info(f\"  Num steps = {self.num_eval_steps}\")\n    all_results = []\n    start_time = time.time()\n    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n        model.eval()\n        batch = tuple(t.to(self.device) for t in batch)\n        with torch.no_grad():\n            inputs = {\n                \"input_ids\": batch[0],\n                \"attention_mask\": batch[1],\n                \"labels\": batch[3],\n            }\n            if self.model_type!= \"distilbert\":\n                inputs[\"token_type_ids\"] = (\n                    batch[2] if self.model_type in [\"bert\", \"xlnet\"] else None\n                )  # XLM, DistilBERT, RoBERTa, and XLM-RoBERTa don't use segment_ids\n            outputs = model(**inputs)\n            eval_loss, logits = outputs[:2]\n            eval_loss = eval_loss.mean().item()\n        nb_eval_steps += 1\n        if len(all_results) % 1000 == 0:\n            self.logger.info(\"Processing example: %d\" % (len(all_results)))\n        if self.output_mode == \"classification\":\n            predictions = logits.detach().cpu().numpy()\n        elif self.output_mode == \"regression\":\n            predictions = logits.detach().cpu().numpy()\n        if self.num_labels == 1:\n            all_results.append(predictions[0])\n        else:\n            all_results.append(predictions)\n    evalTime = format_time(time.time() - start_time)\n    self.logger.info(f\"Evaluation completed in {evalTime}\")\n    if self.output_mode == \"classification\":\n        preds = np.argmax(all_results, axis=1)\n    elif self.output_mode == \"regression\":\n        preds = np.", "CodeBleu_score": 42.27102593731096}]}
{"instance_id": "22d682c958c0", "generation_result": [{"code": " def print_computed_metrics(self, metrics)\n\t\n    print(\"Rank-1: {:.4f}, Rank-5: {:.4f}, mIoU: {:.4f}\".format(\n        metrics[\"rank1\"], metrics[\"rank5\"], metrics[\"mean_iou\"]))\n\n", "CodeBleu_score": 22.612830137467192}, {"code": " def print_computed_metrics(self, metrics)\n\t\n    print(f\"Rank@1: {metrics['rank1']:.4f}\")\n    print(f\"Rank@5: {metrics['rank5']:.4f}\")\n    print(f\"mIoU: {metrics['mIoU']:.4f}\")\n", "CodeBleu_score": 18.86316895617349}, {"code": " def print_computed_metrics(self, metrics)\n\t\n    print(\"Rank-1: {:.4f}\".format(metrics['rank-1']))\n    print(\"Rank-5: {:.4f}\".format(metrics['rank-5']))\n    print(\"mIoU: {:.4f}\".format(metrics['mean-iou']))\n", "CodeBleu_score": 23.916634807974447}]}
{"instance_id": "39c1a50b7e7d", "generation_result": [{"code": "def _eval_predictions(self, segments, data)\n\t\n    average_iou = []\n    average_ranks = []\n    for segment, gt in zip(segments, data):\n        ious = []\n        ranks = []\n        for pred in segment:\n            ious.append(self._iou(pred, gt))\n            ranks.append(self._rank(pred, gt))\n        average_iou.append(np.mean(np.sort(ious)[-5:]))\n        average_ranks.append(np.mean(np.sort(ranks)[:5]))\n    return np.mean(average_ranks), np.mean(average_ranks), np.mean(average_iou)\n\n", "CodeBleu_score": 32.56385069716051}, {"code": "def _eval_predictions(self, segments, data)\n\t\n    average_ranks = []\n    average_iou = []\n    for segment, gt in zip(segments, data):\n        iou = []\n        ranks = []\n        for pred in segment:\n            iou.append(self._iou(pred, gt))\n            ranks.append(self._rank(pred, gt))\n        average_ranks.append(np.mean(np.sort(ranks)[:5]))\n        average_iou.append(np.mean(np.sort(iou)[-5:]))\n    return np.mean(average_ranks == 1), np.mean(average_ranks <= 5), np.mean(average_iou)\n", "CodeBleu_score": 35.261284975519274}, {"code": "def _eval_predictions(self, segments, data)\n\t\n    average_iou = []\n    average_ranks = []\n    for segment, gt in zip(segments, data):\n        iou_list = []\n        rank_list = []\n        for pred in segment:\n            iou_list.append(self._iou(pred, gt))\n            rank_list.append(self._rank(pred, gt))\n        average_iou.append(np.mean(np.sort(iou_list)[-3:]))\n        average_ranks.append(np.mean(np.sort(rank_list)[-3:]))\n    return np.mean(np.array(average_ranks) == 1), np.mean(np.array(average_ranks) == 5), np.mean(average_iou)\n\n", "CodeBleu_score": 33.5031591086615}]}
{"instance_id": "8ccf84f8d034", "generation_result": [{"code": "def predict_loop(self, model, eval_dataloader, output_file=None)\n\t\n    scores = []\n    model.eval()\n    model.to_ctx(ctx=0)\n    for i, batch in enumerate(eval_dataloader):\n        batch = self.to_ctx(batch, ctx=0)\n        with torch.no_grad():\n            outputs = model(batch)\n        outputs.update(batch)\n        self.finalize(outputs)\n        if output_file is not None:\n            self.save(output_file, outputs)\n        scores.append(outputs)\n    return scores\n\n", "CodeBleu_score": 42.53783041364042}, {"code": "def predict_loop(self, model, eval_dataloader, output_file=None)\n\t\n    scores = []\n    model.eval()\n    model.to_ctx(0)\n    with torch.no_grad():\n        for data in eval_dataloader:\n            data = self.to_ctx(data, ctx=0)\n            outputs = model(data)\n            outputs.update(data)\n            self.finalize(outputs)\n            if output_file is not None:\n                self.save(outputs, output_file)\n            scores.append(outputs)\n    return scores\n", "CodeBleu_score": 48.476836254450404}, {"code": "def predict_loop(self, model, eval_dataloader, output_file=None)\n\t\n    scores = []\n    model.eval()\n    model = model.to(0)\n    for batch in eval_dataloader:\n        batch = self.to_ctx(batch, ctx=0)\n        with torch.no_grad():\n            outputs = model(batch)\n        outputs.update(batch)\n        scores.append(outputs)\n    self.finalize(output_file)\n    return scores\n\n", "CodeBleu_score": 50.11825360081872}]}
{"instance_id": "e68529ad727d", "generation_result": [{"code": "def _aggregate_scores(self, scores)\n\t\n    assert len(scores) == 2\n    scores = [np.concatenate(score, axis=0) for score in scores]\n    self.full_scores = None\n    return np.dot(scores[0], scores[1].T)\n", "CodeBleu_score": 36.60835326764767}, {"code": "def _aggregate_scores(self, scores)\n\t\n    assert len(scores) == 2\n    scores = [np.concatenate(score, axis=0) for score in scores]\n    self.full_scores = None\n    return np.dot(scores[0], scores[1].T)\n", "CodeBleu_score": 36.60835326764767}, {"code": "def _aggregate_scores(self, scores)\n\t\n    assert len(scores) == 2, 'There should be exactly two sets of scores'\n    scores = [np.concatenate(s, axis=0) for s in scores]\n    self.full_scores = None\n    return np.dot(scores[0], scores[1].T)\n", "CodeBleu_score": 34.74290735204178}]}
{"instance_id": "e4e1861fc357", "generation_result": [{"code": "def finalize(self, Y_pred, Y_true, output_file=None)\n\t\n\n    # Concatenate predicted and true labels\n    Y_pred = torch.cat(Y_pred, dim=0).detach().cpu().numpy()\n    Y_true = torch.cat(Y_true, dim=0).detach().cpu().numpy()\n\n    # Check for prediction errors\n    errors = np.where(Y_pred!= Y_true)[0]\n    if len(errors) > 0:\n        print('Prediction errors:')\n        print(Y_pred[errors])\n        print(Y_true[errors])\n        print(errors)\n        print()\n\n    # Save predictions and true labels\n    if output_file is not None:\n        with open(output_file, 'wb') as f:\n            pickle.dump({'Y_pred': Y_pred, 'Y_true': Y_true}, f)\n\n    # Return predictions and true labels\n    return {'Y_pred': Y_pred, 'Y_true': Y_true}\n", "CodeBleu_score": 38.52073704650211}, {"code": "def finalize(self, Y_pred, Y_true, output_file=None)\n\t\n    if not self.fitted:\n        raise ValueError(\"The model is not fitted yet. Please fit the model before finalizing the predictions.\")\n    if Y_pred is None:\n        raise ValueError(\"The predictions are not available. Please make sure you have specified the predictions.\")\n    if Y_true is None:\n        raise ValueError(\"The true labels are not available. Please make sure you have specified the true labels.\")\n    if len(Y_pred)!= len(Y_true):\n        raise ValueError(\"The length of the predicted labels is not equal to the length of the true labels.\")\n    Y_pred = np.array(Y_pred)\n    Y_true = np.array(Y_true)\n    if len(Y_pred.shape) == 1:\n        Y_pred = Y_pred.reshape(-1, 1)\n    if len(Y_true.shape) == 1:\n        Y_true = Y_true.reshape(-1, 1)\n    if Y_pred.shape!= Y_true.shape:\n        raise ValueError(\"The shape of the predicted labels is not equal to the shape of the true labels.\")\n    if output_file is not None:\n        with open(output_file, 'wb') as f:\n            pickle.dump({'Y_pred': Y_pred, 'Y_true': Y_true}, f)\n    return {'Y_pred': Y_pred, 'Y_true': Y_true}\n\n", "CodeBleu_score": 35.57151810646752}, {"code": "def finalize(self, Y_pred, Y_true, output_file=None)\n\t\n\n    # check for errors\n    error_index = []\n    for i in range(len(Y_pred)):\n        if len(Y_pred[i])!= len(Y_true[i]):\n            error_index.append(i)\n    if len(error_index) > 0:\n        print('ERROR: The following samples have different number of predicted and true labels:')\n        for i in error_index:\n            print(i, Y_pred[i], Y_true[i])\n        print('The samples will be removed from the predictions and true labels.')\n        for i in sorted(error_index, reverse=True):\n            del Y_pred[i]\n            del Y_true[i]\n\n    # concatenate and convert to numpy array\n    Y_pred = torch.cat(Y_pred, dim=0).cpu().numpy()\n    Y_true = torch.cat(Y_true, dim=0).cpu().numpy()\n\n    # save predictions and true labels\n    if output_file is not None:\n        with open(output_file, 'wb') as f:\n            pickle.dump({'Y_pred': Y_pred, 'Y_true': Y_true}, f)\n\n    return {'Y_pred': Y_pred, 'Y_true': Y_true}\n", "CodeBleu_score": 43.514467710507574}]}
{"instance_id": "7df9ab9a4920", "generation_result": [{"code": "def reduce_metrics(logging_outputs) -> None\n\t\n    loss_sum = sum(log.get('loss', 0) for log in logging_outputs)\n    sample_size = sum(log.get('sample_size', 0) for log in logging_outputs)\n    metrics.log_scalar('loss', loss_sum / sample_size, sample_size, round=3)\n\n", "CodeBleu_score": 52.7910982560264}, {"code": "def reduce_metrics(logging_outputs) -> None\n\t\n    loss_sum = sum(log.get(\"loss\", 0) for log in logging_outputs)\n    sample_size = sum(log.get(\"sample_size\", 0) for log in logging_outputs)\n    metrics.log_scalar(\"loss\", loss_sum / sample_size, sample_size, round=3)\n", "CodeBleu_score": 62.429646714521894}, {"code": "def reduce_metrics(logging_outputs) -> None\n\t\n    loss_sum = sum(log.get(\"loss\", 0) for log in logging_outputs)\n    sample_size = sum(log.get(\"sample_size\", 0) for log in logging_outputs)\n    metrics.log_scalar(\"loss\", loss_sum / sample_size, sample_size, round=3)\n", "CodeBleu_score": 62.429646714521894}]}
{"instance_id": "7f9c326a96f7", "generation_result": [{"code": "def _mm_on_the_fly(\n        self,\n        cmasks,\n        vmasks,\n        attention_mask\n    )\n\t\n    if attention_mask is None:\n        attention_mask = self._mm_attention_mask(cmasks, vmasks)\n    token_type_ids = torch.cat(\n        [torch.zeros(cmasks.size(0), cmasks.size(1)),\n         torch.ones(vmasks.size(0), vmasks.size(1)),\n         torch.zeros(cmasks.size(0), cmasks.size(1))], dim=1)\n    return attention_mask, token_type_ids\n\n", "CodeBleu_score": 50.63391939741143}, {"code": "def _mm_on_the_fly(\n        self,\n        cmasks,\n        vmasks,\n        attention_mask\n    )\n\t\n    # attention mask\n    if attention_mask is None:\n        attention_mask = self._mm_attention_mask(cmasks, vmasks)\n    # token type ids\n    token_type_ids = torch.cat(\n        [torch.zeros_like(cmasks), torch.ones_like(vmasks), torch.zeros_like(cmasks)],\n        dim=1,\n    )\n    return attention_mask, token_type_ids\n", "CodeBleu_score": 39.07152135008074}, {"code": "def _mm_on_the_fly(\n        self,\n        cmasks,\n        vmasks,\n        attention_mask\n    )\n\t\n    if attention_mask is None:\n        mm_mask = self._mm_attention_mask(cmasks, vmasks)\n    else:\n        mm_mask = attention_mask\n    token_type_ids = torch.cat([torch.zeros_like(cmasks), torch.ones_like(vmasks)], dim=1)\n    return mm_mask, token_type_ids\n", "CodeBleu_score": 28.741083690979107}]}
{"instance_id": "fa99d2a148a5", "generation_result": [{"code": "def prepare_inputs_for_generation(\n        self,\n        input_ids,\n        input_video_embeds,\n        attention_mask=None,\n        token_type_ids=None,\n        **model_kwargs\n    )\n\t\n    # Adjust the sequence length to match the combined length of input_ids and input_video_embeds\n    combined_sequence_length = input_ids.size(1) + input_video_embeds.size(1)\n    if attention_mask is not None:\n        attention_mask = torch.cat(\n            [attention_mask, torch.ones(attention_mask.size(0), input_video_embeds.size(1)).to(attention_mask.device)], dim=1\n        )\n    if token_type_ids is not None:\n        token_type_ids = torch.cat(\n            [token_type_ids, torch.zeros(token_type_ids.size(0), input_video_embeds.size(1)).to(token_type_ids.device)], dim=1\n        )\n    # Pad input_ids and input_video_embeds to match the combined sequence length\n    input_ids = torch.cat([input_ids, torch.zeros(input_ids.size(0), input_video_embeds.size(1)).to(input_ids.device)], dim=1)\n    input_video_embeds = torch.cat([input_video_embeds, torch.zeros(input_video_embeds.size(0), input_video_embeds.size(1)).to(input_video_embeds.device)], dim=1)\n    # Return a dictionary containing the updated input_ids, input_video_embeds, attention_mask, and token_type_ids\n    return {\n        \"input_ids\": input_ids,\n        \"input_video_embeds\": input_video_embeds,\n        \"attention_mask\": attention_mask,\n        \"token_type_ids\": token_type_ids,\n    }\n\n", "CodeBleu_score": 49.37563507287109}, {"code": "def prepare_inputs_for_generation(\n        self,\n        input_ids,\n        input_video_embeds,\n        attention_mask=None,\n        token_type_ids=None,\n        **model_kwargs\n    )\n\t\n    # Adjust the sequence length to match the combined length of input_ids and input_video_embeds\n    sequence_length = input_ids.size(1) + input_video_embeds.size(1)\n    input_ids = input_ids.expand(input_video_embeds.size(0), sequence_length)\n    input_video_embeds = input_video_embeds.expand(input_ids.size(0), sequence_length, input_video_embeds.size(2))\n\n    # Modify the attention_mask and token_type_ids to fit the new sequence length\n    if attention_mask is not None:\n        attention_mask = attention_mask.expand(input_video_embeds.size(0), sequence_length)\n    if token_type_ids is not None:\n        token_type_ids = token_type_ids.expand(input_video_embeds.size(0), sequence_length)\n\n    return {\n        \"input_ids\": input_ids,\n        \"input_video_embeds\": input_video_embeds,\n        \"attention_mask\": attention_mask,\n        \"token_type_ids\": token_type_ids,\n        **model_kwargs,\n    }\n", "CodeBleu_score": 47.48651697806429}, {"code": "def prepare_inputs_for_generation(\n        self,\n        input_ids,\n        input_video_embeds,\n        attention_mask=None,\n        token_type_ids=None,\n        **model_kwargs\n    )\n\t\n    # If there are no video embeddings, return the original inputs\n    if input_video_embeds is None:\n        return {\n            \"input_ids\": input_ids,\n            \"attention_mask\": attention_mask,\n            \"token_type_ids\": token_type_ids,\n        }\n    # Adjust the sequence length to match the combined length of input_ids and input_video_embeds\n    combined_sequence_length = input_ids.size(1) + input_video_embeds.size(1)\n    # Create a new attention_mask with the combined sequence length\n    new_attention_mask = torch.ones(input_ids.size(0), combined_sequence_length, dtype=torch.long, device=input_ids.device)\n    # Copy the original attention_mask into the new attention_mask\n    new_attention_mask[:, : input_ids.size(1)] = attention_mask\n    # Create a new token_type_ids with the combined sequence length\n    new_token_type_ids = torch.zeros(input_ids.size(0), combined_sequence_length, dtype=torch.long, device=input_ids.device)\n    # Copy the original token_type_ids into the new token_type_ids\n    new_token_type_ids[:, : input_ids.size(1)] = token_type_ids\n    # Concatenate the input_ids and input_video_embeds along the sequence dimension\n    new_input_ids = torch.cat([input_ids, input_video_embeds], dim=1)\n    # Return a dictionary containing the updated input_ids, input_video_embeds, attention_mask, and token_type_ids\n    return {\n        \"input_ids\": new_input_ids,\n        \"attention_mask\": new_attention_mask,\n        \"token_type_ids\": new_token_type_ids,\n    }\n", "CodeBleu_score": 39.319434967327794}]}
{"instance_id": "b1b0d25fe620", "generation_result": [{"code": "def _init_pipeline(self, cfg: ConfigType) -> Compose\n\t    if'meta_keys' in cfg:\n        cfg['meta_keys'].remove('img_id')\n    load_img_idx = self._get_transform_idx(cfg, 'LoadImageFromFile')\n    if load_img_idx == -1:\n        raise ValueError(\"LoadImageFromFile is not found in the test pipeline.\")\n    cfg[load_img_idx]['type'] ='mmdet.InferencerLoader'\n    return Compose(cfg)\n", "CodeBleu_score": 34.60473165129525}, {"code": "def _init_pipeline(self, cfg: ConfigType) -> Compose\n\t    if'meta_keys' in cfg:\n        cfg['meta_keys'].remove('img_id')\n    load_image_idx = self._get_transform_idx(cfg, 'LoadImageFromFile')\n    if load_image_idx == -1:\n        raise ValueError(\n            'LoadImageFromFile transform not found in the pipeline')\n    cfg[load_image_idx]['type'] ='mmdet.InferencerLoader'\n    return Compose(cfg)\n", "CodeBleu_score": 34.59499250745814}, {"code": "def _init_pipeline(self, cfg: ConfigType) -> Compose\n\t    if 'img_id' in cfg.get('meta_keys', []):\n        cfg['meta_keys'].remove('img_id')\n    load_img_idx = self._get_transform_idx(cfg, 'LoadImageFromFile')\n    if load_img_idx == -1:\n        raise ValueError(\"LoadImageFromFile is not found in the pipeline.\")\n    cfg['transforms'][load_img_idx]['type'] ='mmdet.InferencerLoader'\n    return Compose(cfg['transforms'])\n", "CodeBleu_score": 32.68338167833009}]}
{"instance_id": "7967962856eb", "generation_result": [{"code": "def _inputs_to_list(self, inputs: InputsType) -> list\n\t\n    if isinstance(inputs, str):\n        if isdir(inputs):\n            inputs = list_dir_or_file(inputs, suffix=\".jpg\")\n        else:\n            inputs = [inputs]\n    elif not isinstance(inputs, (list, tuple)):\n        inputs = [inputs]\n    return inputs\n\n", "CodeBleu_score": 38.74069969516586}, {"code": "def _inputs_to_list(self, inputs: InputsType) -> list\n\t\n    if isinstance(inputs, str):\n        if isdir(inputs):\n            inputs = list_dir_or_file(inputs, exts=self.exts)\n        else:\n            inputs = [inputs]\n    elif not isinstance(inputs, (list, tuple)):\n        inputs = [inputs]\n    return inputs\n\n", "CodeBleu_score": 38.651383523569535}, {"code": "def _inputs_to_list(self, inputs: InputsType) -> list\n\t\n    if isinstance(inputs, str):\n        if isdir(inputs):\n            inputs = list_dir_or_file(inputs, exts=self.exts)\n        else:\n            raise ValueError(f\"inputs must be a directory or a list of file paths, but got {inputs}.\")\n    elif not isinstance(inputs, (list, tuple)):\n        inputs = [inputs]\n    return inputs\n\n", "CodeBleu_score": 40.72145259507022}]}
{"instance_id": "74f727f06fa0", "generation_result": [{"code": "def _get_chunk_data(self, inputs: Iterable, chunk_size: int)\n\t\n    chunk_data = []\n    for input in inputs:\n        if isinstance(input, dict) and ('img' in input or 'img_path' in input):\n            chunk_data.append(self.pipeline(input))\n        else:\n            chunk_data.append(input)\n        if len(chunk_data) == chunk_size:\n            yield chunk_data\n            chunk_data = []\n    if len(chunk_data) > 0:\n        yield chunk_data\n", "CodeBleu_score": 27.615924402936397}, {"code": "def _get_chunk_data(self, inputs: Iterable, chunk_size: int)\n\t\n    chunk_data = []\n    for input_data in inputs:\n        if isinstance(input_data, dict) and ('img' in input_data or 'img_path' in input_data):\n            input_data = self.pipeline(input_data)\n        chunk_data.append(input_data)\n        if len(chunk_data) == chunk_size:\n            yield chunk_data\n            chunk_data = []\n    if chunk_data:\n        yield chunk_data\n\n", "CodeBleu_score": 28.53583050491985}, {"code": "def _get_chunk_data(self, inputs: Iterable, chunk_size: int)\n\t\n    chunk_data = []\n    for i, input_data in enumerate(inputs):\n        if isinstance(input_data, dict):\n            if 'img' in input_data:\n                input_data = self.pipeline(input_data)\n            elif 'img_path' in input_data:\n                input_data = self.pipeline(input_data)\n        chunk_data.append(input_data)\n        if (i + 1) % chunk_size == 0:\n            yield chunk_data\n            chunk_data = []\n    if chunk_data:\n        yield chunk_data", "CodeBleu_score": 36.27242405787184}]}
{"instance_id": "a43ab51afa59", "generation_result": [{"code": "def postprocess(\n        self,\n        preds: PredType,\n        visualization: Optional[List[np.ndarray]] = None,\n        return_datasamples: bool = False,\n        print_result: bool = False,\n        no_save_pred: bool = False,\n        pred_out_dir: str = '',\n        **kwargs,\n    ) -> Dict\n\t    if pred_out_dir!= '' and return_datasamples:\n        warnings.warn('Saving datasamples is not supported.')\n\n    results = []\n    if visualization is not None:\n        for vis in visualization:\n            results.append(vis)\n\n    if return_datasamples:\n        for pred in preds:\n            results.append(self.pred2dict(pred, pred_out_dir))\n    else:\n        for pred in preds:\n            results.append(self.pred2dict(pred))\n\n    if print_result:\n        print(results)\n\n    return results\n\n", "CodeBleu_score": 46.41392780092367}, {"code": "def postprocess(\n        self,\n        preds: PredType,\n        visualization: Optional[List[np.ndarray]] = None,\n        return_datasamples: bool = False,\n        print_result: bool = False,\n        no_save_pred: bool = False,\n        pred_out_dir: str = '',\n        **kwargs,\n    ) -> Dict\n\t    if pred_out_dir == '':\n        pred_out_dir = self.work_dir\n    if return_datasamples and pred_out_dir!= '':\n        warnings.warn('Saving datasamples is not supported. '\n                      'Set pred_out_dir to empty if you want to save datasamples.')\n\n    results = []\n    for data_sample, pred, vis in zip(self.data_samples, preds, visualization):\n        result = self.pred2dict(data_sample, pred, pred_out_dir, **kwargs)\n        if not return_datasamples:\n            results.append(result)\n        if vis is not None:\n            result['visualization'] = vis\n        if print_result:\n            print(result)\n    if return_datasamples:\n        results = self.data_samples\n    return results\n", "CodeBleu_score": 52.81430547026987}, {"code": "def postprocess(\n        self,\n        preds: PredType,\n        visualization: Optional[List[np.ndarray]] = None,\n        return_datasamples: bool = False,\n        print_result: bool = False,\n        no_save_pred: bool = False,\n        pred_out_dir: str = '',\n        **kwargs,\n    ) -> Dict\n\t    if not no_save_pred:\n        if pred_out_dir == '':\n            pred_out_dir = osp.join(self.work_dir, 'pred_results')\n            mmengine.mkdir_or_exist(pred_out_dir)\n        else:\n            mmengine.mkdir_or_exist(pred_out_dir)\n\n    if return_datasamples:\n        if pred_out_dir!= '':\n            warnings.warn('Saving datasamples is not supported. '\n                          'Please set `pred_out_dir` to an empty string.')\n        results = preds\n    else:\n        results = []\n        for data_sample in preds:\n            result = self.pred2dict(data_sample, pred_out_dir)\n            results.append(result)\n\n    if print_result:\n        self.print_results(results)\n\n    return results\n", "CodeBleu_score": 50.83492730278611}]}
{"instance_id": "a3c2561f199c", "generation_result": [{"code": "def build_test_pipeline(cfg: ConfigType) -> ConfigType\n\t\n    # Copy the first transform in the dataset pipeline\n    first_transform = copy.deepcopy(cfg[\"dataset\"][\"pipeline\"][0])\n    # Modify the copied transform to include only the 'Resize' transform\n    first_transform[\"transforms\"] = [transform for transform in first_transform[\"transforms\"] if transform[\"type\"] == \"Resize\"]\n    # Copy the last transform in the pipeline\n    last_transform = copy.deepcopy(cfg[\"dataset\"][\"pipeline\"][-1])\n    # Combine the modified first and copied last transforms into a new test pipeline\n    test_pipeline = {\"type\": \"Compose\", \"transforms\": [first_transform, last_transform]}\n    # Return the constructed test pipeline\n    return test_pipeline\n\n", "CodeBleu_score": 29.82861965167633}, {"code": "def build_test_pipeline(cfg: ConfigType) -> ConfigType\n\t\n    cfg = copy.deepcopy(cfg)\n    cfg[\"pipeline\"] = [cfg[\"pipeline\"][0]]\n    cfg[\"pipeline\"][0][\"transforms\"] = [\n        transform for transform in cfg[\"pipeline\"][0][\"transforms\"] if transform[\"type\"] == \"Resize\"\n    ]\n    cfg[\"pipeline\"].append(cfg[\"pipeline\"][-1])\n    cfg[\"pipeline\"][-1][\"transforms\"] = [\n        transform for transform in cfg[\"pipeline\"][-1][\"transforms\"] if transform[\"type\"]!= \"Resize\"\n    ]\n    cfg[\"pipeline\"] = Compose(cfg[\"pipeline\"])\n    return cfg\n\n", "CodeBleu_score": 28.867814887332266}, {"code": "def build_test_pipeline(cfg: ConfigType) -> ConfigType\n\t\n    cfg = copy.deepcopy(cfg)\n    cfg.dataset.transforms[0] = copy.deepcopy(cfg.dataset.transforms[0])\n    cfg.dataset.transforms[0].transforms = [Resize(cfg.dataset.transforms[0].size)]\n    cfg.dataset.transforms.append(Compose(cfg.dataset.transforms[0].transforms))\n    return cfg\n\n", "CodeBleu_score": 20.38601651856854}]}
{"instance_id": "48a358ab5a38", "generation_result": [{"code": "def inference_mot(model: nn.Module, img: np.ndarray, frame_id: int,\n                  video_len: int) -> SampleList\n\t\n    # prepare data\n    data = dict(\n        img=img,\n        frame_id=frame_id,\n        ori_shape=img.shape[:2],\n        img_id=0,\n        video_len=video_len)\n    data = model.test_pipeline(data)\n\n    # prepare model\n    if next(model.parameters()).device!= torch.device('cpu'):\n        model = model.cpu()\n        model.eval()\n        for m in model.modules():\n            if isinstance(m, RoIPool):\n                raise NotImplementedError('RoIPool is not supported on GPU.')\n\n    # forward the model\n    with torch.no_grad():\n        result = model.test_step(model.data_preprocessor(data, True))\n    return result\n\n", "CodeBleu_score": 39.74626910976367}, {"code": "def inference_mot(model: nn.Module, img: np.ndarray, frame_id: int,\n                  video_len: int) -> SampleList\n\t\n    # prepare data\n    data = dict(\n        img=img,\n        frame_id=frame_id,\n        ori_shape=img.shape[:2],\n        img_id=frame_id,\n        video_len=video_len)\n    cfg = model.cfg\n    test_pipeline = build_test_pipeline(cfg)\n    data = test_pipeline(data)\n    data = default_collate([data])\n    if next(model.parameters()).is_cuda:\n        # avoid unsupported ops in onnx\n        model = model.cpu()\n    with torch.no_grad():\n        result = model.test_step(data)\n    return result\n\n", "CodeBleu_score": 37.3081390687784}, {"code": "def inference_mot(model: nn.Module, img: np.ndarray, frame_id: int,\n                  video_len: int) -> SampleList\n\t\n    # prepare data\n    data = dict(\n        img=img,\n        frame_id=frame_id,\n        ori_shape=img.shape[:2],\n        img_id=0,\n        video_len=video_len)\n    data = model.test_pipeline(data)\n    data = collate([data], samples_per_gpu=1)\n    if next(model.parameters()).is_cuda:\n        # avoid unsupported ops in RoIPool when tracking on CPU device\n        model = _convert_model_from_roi_pool(model)\n    with torch.no_grad():\n        result = model.test_step(data)\n    return result\n\n", "CodeBleu_score": 32.308420858298895}]}
{"instance_id": "6c4ea2ce1787", "generation_result": [{"code": "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\n\t\n    while True:\n        total_length = len(tokens_a) + len(tokens_b)\n        if total_length <= max_num_tokens:\n            break\n\n        trunc_tokens = tokens_a if len(tokens_a) > len(tokens_b) else tokens_b\n        assert len(trunc_tokens) >= 1\n\n        if rng.random() < 0.5:\n            del trunc_tokens[0]\n        else:\n            trunc_tokens.pop()\n\n", "CodeBleu_score": 77.75746428493808}, {"code": "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\n\t\n    while True:\n        total_length = len(tokens_a) + len(tokens_b)\n        if total_length <= max_num_tokens:\n            break\n\n        trunc_tokens = tokens_a if len(tokens_a) > len(tokens_b) else tokens_b\n        assert len(trunc_tokens) >= 1\n\n        # We want to sometimes truncate from the front and sometimes from the\n        # back to add more randomness and avoid biases.\n        if rng.random() < 0.5:\n            del trunc_tokens[0]\n        else:\n            trunc_tokens.pop()\n\n", "CodeBleu_score": 77.75746428493808}, {"code": "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\n\t\n    while True:\n        total_length = len(tokens_a) + len(tokens_b)\n        if total_length <= max_num_tokens:\n            break\n\n        trunc_tokens = tokens_a if len(tokens_a) > len(tokens_b) else tokens_b\n        assert len(trunc_tokens) >= 1\n\n        # We want to sometimes truncate from the front and sometimes from the\n        # back to add more randomness and avoid biases.\n        if rng.random() < 0.5:\n            del trunc_tokens[0]\n        else:\n            trunc_tokens.pop()\n", "CodeBleu_score": 77.75746428493808}]}
{"instance_id": "007181b6729d", "generation_result": [{"code": "def get_activation(activation_string)\n\t\n    if isinstance(activation_string, str):\n        activation_string = activation_string.lower()\n        if activation_string == \"\":\n            return None\n        elif activation_string == \"relu\":\n            return tf.nn.relu\n        elif activation_string == \"sigmoid\":\n            return tf.nn.sigmoid\n        elif activation_string == \"tanh\":\n            return tf.nn.tanh\n        else:\n            raise ValueError(\"Unsupported activation function: {}\".format(activation_string))\n    else:\n        return activation_string\n\n", "CodeBleu_score": 54.47684646285255}, {"code": "def get_activation(activation_string)\n\t\n    if not isinstance(activation_string, str):\n        return activation_string\n    if activation_string.lower() =='relu':\n        return tf.nn.relu\n    elif activation_string.lower() == 'elu':\n        return tf.nn.elu\n    elif activation_string.lower() == 'leaky_relu':\n        return tf.nn.leaky_relu\n    elif activation_string.lower() =='selu':\n        return tf.nn.selu\n    elif activation_string.lower() == 'tanh':\n        return tf.nn.tanh\n    elif activation_string.lower() =='sigmoid':\n        return tf.nn.sigmoid\n    elif activation_string.lower() =='softmax':\n        return tf.nn.softmax\n    elif activation_string.lower() =='softplus':\n        return tf.nn.softplus\n    elif activation_string.lower() =='softsign':\n        return tf.nn.softsign\n    elif activation_string.lower() == 'None':\n        return None\n    else:\n        raise ValueError('Unknown activation function:'+ activation_string)\n\n", "CodeBleu_score": 28.383184027658526}, {"code": "def get_activation(activation_string)\n\t\n    if not isinstance(activation_string, str):\n        return activation_string\n    if activation_string.lower() =='relu':\n        return tf.nn.relu\n    if activation_string.lower() == 'elu':\n        return tf.nn.elu\n    if activation_string.lower() == 'leaky_relu':\n        return tf.nn.leaky_relu\n    if activation_string.lower() =='selu':\n        return tf.nn.selu\n    if activation_string.lower() =='softplus':\n        return tf.nn.softplus\n    if activation_string.lower() =='softsign':\n        return tf.nn.softsign\n    if activation_string.lower() =='sigmoid':\n        return tf.nn.sigmoid\n    if activation_string.lower() == 'tanh':\n        return tf.nn.tanh\n    if activation_string.lower() == 'none':\n        return None\n    raise ValueError('Unsupported activation function: {}'.format(activation_string))\n\n", "CodeBleu_score": 33.110726842840506}]}
{"instance_id": "d5d6e0315899", "generation_result": [{"code": "def dropout(input_tensor, dropout_prob)\n\t\n    if dropout_prob is None or dropout_prob == 0.0:\n        return input_tensor\n    output = tf.nn.dropout(input_tensor, 1.0 - dropout_prob)\n    return output\n\n", "CodeBleu_score": 87.51451468041077}, {"code": "def dropout(input_tensor, dropout_prob)\n\t\n    if dropout_prob is None or dropout_prob == 0.0:\n        return input_tensor\n\n    output = tf.nn.dropout(input_tensor, 1.0 - dropout_prob)\n    return output\n\n", "CodeBleu_score": 87.51451468041077}, {"code": "def dropout(input_tensor, dropout_prob)\n\t\n    if dropout_prob is None or dropout_prob == 0.0:\n        return input_tensor\n    output = tf.nn.dropout(input_tensor, 1.0 - dropout_prob)\n    return output", "CodeBleu_score": 87.51451468041077}]}
{"instance_id": "828917b9ac53", "generation_result": [{"code": "def create_attention_mask_from_input_mask(from_tensor, to_mask)\n\t\n    from_shape = get_shape_list(from_tensor, expected_rank=[2, 3])\n    batch_size = from_shape[0]\n    from_seq_length = from_shape[1]\n\n    to_shape = get_shape_list(to_mask, expected_rank=2)\n    to_seq_length = to_shape[1]\n\n    to_mask = tf.cast(\n        tf.reshape(to_mask, [batch_size, 1, to_seq_length]), tf.float32)\n\n    # We don't assume that `from_tensor` is a mask (although it could be). We\n    # don't actually care if we attend *from* padding tokens (only *to* padding)\n    # tokens so we create a tensor of all ones.\n    #\n    # `broadcast_ones` = [batch_size, from_seq_length, 1]\n    broadcast_ones = tf.ones(\n        shape=[batch_size, from_seq_length, 1], dtype=tf.float32)\n\n    # Here we broadcast along two dimensions to create the mask.\n    mask = broadcast_ones * to_mask\n\n    return mask\n", "CodeBleu_score": 64.09901013046853}, {"code": "def create_attention_mask_from_input_mask(from_tensor, to_mask)\n\t\n    from_shape = get_shape_list(from_tensor, expected_rank=[2, 3])\n    batch_size = from_shape[0]\n    from_seq_length = from_shape[1]\n\n    to_shape = get_shape_list(to_mask, expected_rank=2)\n    to_seq_length = to_shape[1]\n\n    to_mask = tf.cast(\n        tf.reshape(to_mask, [batch_size, 1, to_seq_length]), tf.float32)\n\n    # We don't assume that `from_tensor` is a mask (although it could be). We\n    # don't actually care if we attend *from* padding tokens (only *to* padding)\n    # tokens so we create a tensor of all ones.\n    #\n    # `broadcast_ones` = [batch_size, from_seq_length, 1]\n    broadcast_ones = tf.ones(\n        shape=[batch_size, from_seq_length, 1], dtype=tf.float32)\n\n    # Here we broadcast along two dimensions to create the mask.\n    mask = broadcast_ones * to_mask\n\n    return mask\n", "CodeBleu_score": 64.09901013046853}, {"code": "def create_attention_mask_from_input_mask(from_tensor, to_mask)\n\t\n    from_shape = get_shape_list(from_tensor, expected_rank=[2, 3])\n    batch_size = from_shape[0]\n    from_seq_length = from_shape[1]\n\n    to_shape = get_shape_list(to_mask, expected_rank=2)\n    to_seq_length = to_shape[1]\n\n    to_mask = tf.cast(\n        tf.reshape(to_mask, [batch_size, 1, to_seq_length]), tf.float32)\n\n    broadcast_ones = tf.ones(\n        shape=[batch_size, from_seq_length, 1], dtype=tf.float32)\n\n    mask = broadcast_ones * to_mask\n\n    return mask\n\n", "CodeBleu_score": 64.09901013046853}]}
{"instance_id": "d7798e41a723", "generation_result": [{"code": "def transpose_for_scores(input_tensor, batch_size, num_attention_heads,\n                           seq_length, width)\n\t\n    output_tensor = tf.reshape(\n        input_tensor, [batch_size, seq_length, num_attention_heads, width])\n\n    output_tensor = tf.transpose(output_tensor, [0, 2, 1, 3])\n    return output_tensor\n", "CodeBleu_score": 59.99845781533536}, {"code": "def transpose_for_scores(input_tensor, batch_size, num_attention_heads,\n                           seq_length, width)\n\t\n    output_tensor = tf.reshape(input_tensor, [batch_size, seq_length, num_attention_heads, width])\n    output_tensor = tf.transpose(output_tensor, [0, 2, 1, 3])\n    return output_tensor\n", "CodeBleu_score": 54.22065594588168}, {"code": "def transpose_for_scores(input_tensor, batch_size, num_attention_heads,\n                           seq_length, width)\n\t\n    output_tensor = tf.reshape(input_tensor, [batch_size, seq_length, num_attention_heads, width])\n    output_tensor = tf.transpose(output_tensor, [0, 2, 1, 3])\n    return output_tensor\n\n", "CodeBleu_score": 54.22065594588168}]}
{"instance_id": "711d466ace09", "generation_result": [{"code": "def reshape_to_matrix(input_tensor)\n\t\n    if input_tensor.shape.ndims == 2:\n        return input_tensor\n\n    width = input_tensor.shape.with_rank_at_least(2)[-1]\n    output_tensor = tf.reshape(input_tensor, [-1, width])\n    return output_tensor\n\n", "CodeBleu_score": 49.29583549594652}, {"code": "def reshape_to_matrix(input_tensor)\n\t\n    if input_tensor.shape.ndims == 2:\n        return input_tensor\n\n    input_tensor_shape = get_shape_list(input_tensor)\n\n    # The total size of the input tensor is width * height.\n    # The second dimension of the input tensor.\n    width = input_tensor_shape[-1]\n\n    # The first dimension of the input tensor.\n    # The last dimension is the width we want to reshape to.\n    output_tensor = tf.reshape(input_tensor, [-1, width])\n\n    return output_tensor\n\n", "CodeBleu_score": 48.58395606729262}, {"code": "def reshape_to_matrix(input_tensor)\n\t\n    if input_tensor.shape.ndims == 2:\n        return input_tensor\n    else:\n        return tf.reshape(input_tensor, [-1, input_tensor.shape[-1]])\n", "CodeBleu_score": 16.99760026860185}]}
{"instance_id": "7adcd08f531e", "generation_result": [{"code": "def get_train_examples(self, data_dir)\n\t\n    return self._create_examples(\n        self._read_tsv(os.path.join(data_dir, \"train.tsv\")), \"train\")\n\n", "CodeBleu_score": 6.324149162613836}, {"code": "def get_train_examples(self, data_dir)\n\t\n    return self._create_examples(\n        self._read_tsv(os.path.join(data_dir, \"train.tsv\")), \"train\")\n\n", "CodeBleu_score": 6.324149162613836}, {"code": "def get_train_examples(self, data_dir)\n\t\n    return self._create_examples(\n        self._read_tsv(os.path.join(data_dir, \"train.tsv\")), \"train\")\n\n", "CodeBleu_score": 6.324149162613836}]}
{"instance_id": "26a0bb96dab9", "generation_result": [{"code": "def metric_fn(per_example_loss, label_ids, logits, is_real_example)\n\t\n\n    predictions = tf.argmax(logits, axis=-1, output_type=tf.int32)\n    accuracy = tf.metrics.accuracy(\n        labels=label_ids, predictions=predictions, weights=is_real_example)\n    loss = tf.metrics.mean(values=per_example_loss, weights=is_real_example)\n    return {\n        \"eval_accuracy\": accuracy,\n        \"eval_loss\": loss,\n    }\n\n", "CodeBleu_score": 65.47238279114057}, {"code": "def metric_fn(per_example_loss, label_ids, logits, is_real_example)\n\t\n    predictions = tf.argmax(logits, axis=-1, output_type=tf.int32)\n    accuracy = tf.metrics.accuracy(\n        labels=label_ids, predictions=predictions, weights=is_real_example)\n    loss = tf.metrics.mean(values=per_example_loss, weights=is_real_example)\n    return {\n        \"eval_accuracy\": accuracy,\n        \"eval_loss\": loss,\n    }\n\n", "CodeBleu_score": 65.47238279114057}, {"code": "def metric_fn(per_example_loss, label_ids, logits, is_real_example)\n\t\n    predictions = tf.argmax(logits, axis=-1, output_type=tf.int32)\n    accuracy = tf.metrics.accuracy(\n        labels=label_ids, predictions=predictions, weights=is_real_example)\n    loss = tf.metrics.mean(values=per_example_loss, weights=is_real_example)\n    return {\n        \"eval_accuracy\": accuracy,\n        \"eval_loss\": loss,\n    }\n", "CodeBleu_score": 65.47238279114057}]}
{"instance_id": "6154e4d28509", "generation_result": [{"code": "def convert_examples_to_features(examples, label_list, max_seq_length,\n                                 tokenizer)\n\t  features = []\n    for (ex_index, example) in enumerate(examples):\n        if ex_index % 10000 == 0:\n            tf.logging.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n        feature = convert_single_example(ex_index, example, label_list,\n                                         max_seq_length, tokenizer)\n        features.append(feature)\n    return features\n\n", "CodeBleu_score": 82.28316162220912}, {"code": "def convert_examples_to_features(examples, label_list, max_seq_length,\n                                 tokenizer)\n\t  features = []\n    for (ex_index, example) in enumerate(examples):\n        if ex_index % 10000 == 0:\n            tf.logging.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n\n        feature = convert_single_example(ex_index, example, label_list,\n                                         max_seq_length, tokenizer)\n\n        features.append(feature)\n    return features\n", "CodeBleu_score": 82.28316162220912}, {"code": "def convert_examples_to_features(examples, label_list, max_seq_length,\n                                 tokenizer)\n\t  features = []\n    for (ex_index, example) in enumerate(examples):\n        if ex_index % 10000 == 0:\n            tf.logging.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n        feature = convert_single_example(ex_index, example, label_list,\n                                         max_seq_length, tokenizer)\n        features.append(feature)\n    return features\n\n", "CodeBleu_score": 82.28316162220912}]}
{"instance_id": "e09c0db71ebb", "generation_result": [{"code": "def get_next_sentence_output(bert_config, input_tensor, labels)\n\t\n    with tf.variable_scope(\"cls/seq_relationship\"):\n        output_weights = tf.get_variable(\n            \"output_weights\",\n            shape=[2, bert_config.hidden_size],\n            initializer=modeling.create_initializer(bert_config.initializer_range))\n        output_bias = tf.get_variable(\n            \"output_bias\", shape=[2], initializer=tf.zeros_initializer())\n\n        logits = tf.matmul(input_tensor, output_weights, transpose_b=True)\n        logits = tf.nn.bias_add(logits, output_bias)\n        log_probs = tf.nn.log_softmax(logits, axis=-1)\n\n        labels = tf.reshape(labels, [-1])\n        one_hot_labels = tf.one_hot(labels, depth=2, dtype=tf.float32)\n\n        per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n        loss = tf.reduce_mean(per_example_loss)\n        return (loss, per_example_loss, log_probs)\n\n", "CodeBleu_score": 86.94900271179796}, {"code": "def get_next_sentence_output(bert_config, input_tensor, labels)\n\t\n    with tf.variable_scope(\"cls/seq_relationship\"):\n        output_weights = tf.get_variable(\n            \"output_weights\",\n            shape=[2, bert_config.hidden_size],\n            initializer=modeling.create_initializer(bert_config.initializer_range))\n        output_bias = tf.get_variable(\n            \"output_bias\", shape=[2], initializer=tf.zeros_initializer())\n\n        logits = tf.matmul(input_tensor, output_weights, transpose_b=True)\n        logits = tf.nn.bias_add(logits, output_bias)\n        log_probs = tf.nn.log_softmax(logits, axis=-1)\n        labels = tf.reshape(labels, [-1])\n        one_hot_labels = tf.one_hot(labels, depth=2, dtype=tf.float32)\n        per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n        loss = tf.reduce_mean(per_example_loss)\n        return (loss, per_example_loss, log_probs)\n\n", "CodeBleu_score": 86.94900271179796}, {"code": "def get_next_sentence_output(bert_config, input_tensor, labels)\n\t\n    with tf.variable_scope(\"cls/seq_relationship\"):\n        output_weights = tf.get_variable(\n            \"output_weights\",\n            shape=[2, bert_config.hidden_size],\n            initializer=modeling.create_initializer(bert_config.initializer_range))\n        output_bias = tf.get_variable(\n            \"output_bias\", shape=[2], initializer=tf.zeros_initializer())\n\n        logits = tf.matmul(input_tensor, output_weights, transpose_b=True)\n        logits = tf.nn.bias_add(logits, output_bias)\n        log_probs = tf.nn.log_softmax(logits, axis=-1)\n        labels = tf.reshape(labels, [-1])\n        one_hot_labels = tf.one_hot(labels, depth=2, dtype=tf.float32)\n        per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n        loss = tf.reduce_mean(per_example_loss)\n        return (loss, per_example_loss, log_probs)\n\n", "CodeBleu_score": 86.94900271179796}]}
{"instance_id": "1f66b020f485", "generation_result": [{"code": "def gather_indexes(sequence_tensor, positions)\n\t\n    # sequence_tensor: [batch_size, seq_length, hidden_size]\n    # positions: [batch_size, num_positions]\n    # output: [batch_size, num_positions, hidden_size]\n    sequence_shape = modeling.get_shape_list(sequence_tensor, expected_rank=3)\n    batch_size = sequence_shape[0]\n    seq_length = sequence_shape[1]\n    width = sequence_shape[2]\n\n    flat_offsets = tf.reshape(tf.range(0, batch_size, dtype=tf.int32) * seq_length, [-1, 1])\n    flat_positions = tf.reshape(positions + flat_offsets, [-1])\n    flat_sequence_tensor = tf.reshape(sequence_tensor, [batch_size * seq_length, width])\n    output_tensor = tf.gather(flat_sequence_tensor, flat_positions)\n    return output_tensor\n", "CodeBleu_score": 55.18468812590758}, {"code": "def gather_indexes(sequence_tensor, positions)\n\t\n    # sequence_tensor: [batch_size, seq_length, dim]\n    # positions: [batch_size, n_positions]\n    # output: [batch_size, n_positions, dim]\n    sequence_flat = tf.reshape(sequence_tensor, [-1, tf.shape(sequence_tensor)[-1]])\n    flat_offsets = tf.reshape(tf.range(tf.shape(sequence_tensor)[0]) * tf.shape(sequence_tensor)[1], [-1, 1])\n    flat_positions = flat_offsets + tf.reshape(positions, [-1])\n    output_flat = tf.gather(sequence_flat, flat_positions)\n    return tf.reshape(output_flat, tf.shape(positions) + [tf.shape(sequence_tensor)[-1]])\n\n", "CodeBleu_score": 23.12641542085893}, {"code": "def gather_indexes(sequence_tensor, positions)\n\t\n    # sequence_tensor: [batch_size, seq_length, dim]\n    # positions: [batch_size, n_positions]\n    # output: [batch_size, n_positions, dim]\n    # sequence_tensor: [batch_size, seq_length, dim]\n    # positions: [batch_size, n_positions]\n    # output: [batch_size, n_positions, dim]\n    seq_length = modeling.get_shape_list(sequence_tensor)[1]\n    batch_size = modeling.get_shape_list(sequence_tensor)[0]\n    width = modeling.get_shape_list(sequence_tensor)[2]\n\n    flat_offsets = tf.reshape(\n        tf.range(0, batch_size, dtype=tf.int32) * seq_length, [-1, 1])\n    flat_positions = tf.reshape(positions + flat_offsets, [-1])\n    flat_sequence_tensor = tf.reshape(sequence_tensor,\n                                      [batch_size * seq_length, width])\n    output_tensor = tf.gather(flat_sequence_tensor, flat_positions)\n    return output_tensor\n\n", "CodeBleu_score": 49.00243576558256}]}
{"instance_id": "956bffee9a33", "generation_result": [{"code": "def _decode_record(record, name_to_features)\n\t\n    example = tf.parse_single_example(record, name_to_features)\n    for name in list(example.keys()):\n        t = example[name]\n        if t.dtype == tf.int64:\n            t = tf.to_int32(t)\n        example[name] = t\n    return example\n\n", "CodeBleu_score": 71.32095143473128}, {"code": "def _decode_record(record, name_to_features)\n\t\n    example = tf.parse_single_example(record, name_to_features)\n    for name in list(example.keys()):\n        t = example[name]\n        if t.dtype == tf.int64:\n            t = tf.to_int32(t)\n        example[name] = t\n    return example\n", "CodeBleu_score": 71.32095143473128}, {"code": "def _decode_record(record, name_to_features)\n\t\n    example = tf.parse_single_example(record, name_to_features)\n    for name in list(example.keys()):\n        t = example[name]\n        if t.dtype == tf.int64:\n            t = tf.to_int32(t)\n        example[name] = t\n    return example", "CodeBleu_score": 71.32095143473128}]}
{"instance_id": "6b97f354e641", "generation_result": [{"code": "def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer,\n                         orig_answer_text)\n\t\n    tok_answer_text = tokenizer.tokenize(orig_answer_text)\n    for new_start in range(input_start, input_end + 1):\n        for new_end in range(input_end, new_start - 1, -1):\n            text_span = doc_tokens[new_start:(new_end + 1)]\n            if text_span == tok_answer_text:\n                return (new_start, new_end)\n\n    return (input_start, input_end)\n\n", "CodeBleu_score": 59.8710992763108}, {"code": "def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer,\n                         orig_answer_text)\n\t\n    tok_answer_text = tokenizer.tokenize(orig_answer_text)\n    for new_start in range(input_start, input_end + 1):\n        for new_end in range(input_end, new_start - 1, -1):\n            text_span = \" \".join(doc_tokens[new_start:(new_end + 1)])\n            if text_span == \"\".join(tok_answer_text):\n                return (new_start, new_end)\n\n    return (input_start, input_end)\n\n", "CodeBleu_score": 69.40717313257213}, {"code": "def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer,\n                         orig_answer_text)\n\t\n    tok_answer_text = tokenizer.tokenize(orig_answer_text)\n    for new_start in range(input_start, input_end + 1):\n        for new_end in range(input_end, new_start - 1, -1):\n            text_span = \" \".join(doc_tokens[new_start:(new_end + 1)])\n            if text_span == \"\".join(tok_answer_text):\n                return (new_start, new_end)\n\n    return (input_start, input_end)\n\n", "CodeBleu_score": 69.40717313257213}]}
{"instance_id": "aba334b92b34", "generation_result": [{"code": "def _check_is_max_context(doc_spans, cur_span_index, position)\n\t\n    best_score = None\n    best_span_index = None\n    for (span_index, doc_span) in enumerate(doc_spans):\n        end = doc_span.start + doc_span.length - 1\n        if position < doc_span.start:\n            continue\n        if position > end:\n            continue\n        num_left_context = position - doc_span.start\n        num_right_context = end - position\n        score = min(num_left_context, num_right_context) + 0.01 * doc_span.length\n        if best_score is None or score > best_score:\n            best_score = score\n            best_span_index = span_index\n    return cur_span_index == best_span_index\n", "CodeBleu_score": 78.23889503357583}, {"code": "def _check_is_max_context(doc_spans, cur_span_index, position)\n\t\n    best_score = None\n    best_span_index = None\n    for (span_index, doc_span) in enumerate(doc_spans):\n        end = doc_span.start + doc_span.length - 1\n        if position < doc_span.start:\n            continue\n        if position > end:\n            continue\n        num_left_context = position - doc_span.start\n        num_right_context = end - position\n        score = min(num_left_context, num_right_context) + 0.01 * doc_span.length\n        if best_score is None or score > best_score:\n            best_score = score\n            best_span_index = span_index\n    return cur_span_index == best_span_index\n", "CodeBleu_score": 78.23889503357583}, {"code": "def _check_is_max_context(doc_spans, cur_span_index, position)\n\t\n    best_score = None\n    best_span_index = None\n    for (span_index, doc_span) in enumerate(doc_spans):\n        end = doc_span.start + doc_span.length - 1\n        if position < doc_span.start:\n            continue\n        if position > end:\n            continue\n        num_left_context = position - doc_span.start\n        num_right_context = end - position\n        score = min(num_left_context, num_right_context) + 0.01 * doc_span.length\n        if best_score is None or score > best_score:\n            best_score = score\n            best_span_index = span_index\n    return cur_span_index == best_span_index\n\n", "CodeBleu_score": 78.23889503357583}]}
{"instance_id": "cf92ad509f6a", "generation_result": [{"code": "def load_vocab(vocab_file)\n\t\n    vocab = collections.OrderedDict()\n    index = 0\n    with open(vocab_file, \"r\", encoding=\"utf-8\") as reader:\n        while True:\n            token = convert_to_unicode(reader.readline())\n            if not token:\n                break\n            token = token.strip()\n            vocab[token] = index\n            index += 1\n    return vocab\n\n", "CodeBleu_score": 67.2956448618734}, {"code": "def load_vocab(vocab_file)\n\t\n    vocab = collections.OrderedDict()\n    index = 0\n    with open(vocab_file, \"r\", encoding=\"utf-8\") as reader:\n        while True:\n            token = convert_to_unicode(reader.readline())\n            if not token:\n                break\n            token = token.strip()\n            vocab[token] = index\n            index += 1\n    return vocab\n\n", "CodeBleu_score": 67.2956448618734}, {"code": "def load_vocab(vocab_file)\n\t\n    vocab = collections.OrderedDict()\n    index = 0\n    with open(vocab_file, \"r\", encoding=\"utf-8\") as reader:\n        while True:\n            token = convert_to_unicode(reader.readline())\n            if not token:\n                break\n            token = token.strip()\n            vocab[token] = index\n            index += 1\n    return vocab\n\n", "CodeBleu_score": 67.2956448618734}]}
{"instance_id": "6c865803ac12", "generation_result": [{"code": "def convert_by_vocab(vocab, items)\n\t\n    output = []\n    for item in items:\n        output.append(vocab[item])\n    return output\n\n", "CodeBleu_score": 79.63749907165396}, {"code": "def convert_by_vocab(vocab, items)\n\t\n    output = []\n    for item in items:\n        output.append(vocab[item])\n    return output\n\n", "CodeBleu_score": 79.63749907165396}, {"code": "def convert_by_vocab(vocab, items)\n\t\n    output = []\n    for item in items:\n        output.append(vocab[item])\n    return output\n\n", "CodeBleu_score": 79.63749907165396}]}
{"instance_id": "e93b9200dc4a", "generation_result": [{"code": "def tokenize(self, text)\n\t\n    split_tokens = []\n    for token in self.basic_tokenizer.tokenize(text):\n        for sub_token in self.wordpiece_tokenizer.tokenize(token):\n            split_tokens.append(sub_token)\n\n    return split_tokens\n", "CodeBleu_score": 88.10762355418231}, {"code": "def tokenize(self, text)\n\t\n    split_tokens = []\n    for token in self.basic_tokenizer.tokenize(text, never_split=self.all_special_tokens):\n        for sub_token in self.wordpiece_tokenizer.tokenize(token):\n            split_tokens.append(sub_token)\n\n    return split_tokens\n", "CodeBleu_score": 62.56859574727436}, {"code": "def tokenize(self, text)\n\t\n    split_tokens = []\n    for token in self.basic_tokenizer.tokenize(text):\n        for sub_token in self.wordpiece_tokenizer.tokenize(token):\n            split_tokens.append(sub_token)\n    return split_tokens\n\n", "CodeBleu_score": 88.10762355418231}]}
{"instance_id": "2ff6f6db53c9", "generation_result": [{"code": "def safe_load(path, offset, duration, sample_rate, dtype)\n\t\n    try:\n        audio, sr = librosa.load(path, sr=sample_rate, offset=offset, duration=duration)\n        audio = audio.astype(dtype)\n        return audio, False\n    except Exception as e:\n        logger.exception(f\"Failed to load audio from {path} with offset {offset} and duration {duration} with sample rate {sample_rate} and dtype {dtype}.\")\n        return -1.0, True\n\n", "CodeBleu_score": 33.2713499346427}, {"code": "def safe_load(path, offset, duration, sample_rate, dtype)\n\t\n    try:\n        logger.info(f\"Loading {path} with offset {offset} and duration {duration} and sample rate {sample_rate} and dtype {dtype}\")\n        y, sr = librosa.load(path, offset=offset, duration=duration, sr=sample_rate, dtype=dtype)\n        return y, False\n    except Exception as e:\n        logger.exception(f\"Error loading {path} with offset {offset} and duration {duration} and sample rate {sample_rate} and dtype {dtype}\")\n        return -1.0, True\n", "CodeBleu_score": 35.61787771513297}, {"code": "def safe_load(path, offset, duration, sample_rate, dtype)\n\t\n    try:\n        logger.info(f\"Loading audio data from {path} with offset {offset} and duration {duration}.\")\n        audio, sr = librosa.load(path, sr=sample_rate, offset=offset, duration=duration)\n        if dtype == np.float32:\n            audio = audio.astype(np.float32)\n        elif dtype == np.float64:\n            audio = audio.astype(np.float64)\n        else:\n            raise ValueError(f\"Invalid dtype: {dtype}.\")\n        logger.info(\"Audio data loaded successfully.\")\n        return audio, False\n    except Exception as e:\n        logger.exception(f\"Error loading audio data from {path}: {e}\")\n        return -1.0, True\n\n", "CodeBleu_score": 41.02147931826335}]}
{"instance_id": "12fbe1f03d8f", "generation_result": [{"code": "def to_n_channels(waveform: tf.Tensor, n_channels: int) -> tf.Tensor\n\t\n    if n_channels == 1:\n        return tf.expand_dims(waveform, axis=-1)\n    elif n_channels > 1:\n        if waveform.shape[-1] == n_channels:\n            return waveform\n        elif waveform.shape[-1] > n_channels:\n            return waveform[..., :n_channels]\n        else:\n            return tf.tile(tf.expand_dims(waveform, axis=-1), [1, 1, n_channels])\n    else:\n        raise ValueError(f\"n_channels must be 1 or greater, but got {n_channels}.\")\n\n", "CodeBleu_score": 40.808066833350885}, {"code": "def to_n_channels(waveform: tf.Tensor, n_channels: int) -> tf.Tensor\n\t\n    waveform = tf.expand_dims(waveform, axis=-1)\n    waveform = tf.tile(waveform, [1, 1, n_channels])\n    waveform = waveform[..., :n_channels]\n    return waveform\n", "CodeBleu_score": 45.127650880150505}, {"code": "def to_n_channels(waveform: tf.Tensor, n_channels: int) -> tf.Tensor\n\t\n    n_channels_in = tf.shape(waveform)[-1]\n    if n_channels_in >= n_channels:\n        return waveform[..., :n_channels]\n    else:\n        waveform_tiled = tf.tile(waveform, [1, 1, n_channels // n_channels_in])\n        return waveform_tiled[..., :n_channels]\n", "CodeBleu_score": 48.08212554491351}]}
{"instance_id": "5e215f029b2e", "generation_result": [{"code": "def spectrogram_to_db_uint(\n    spectrogram: tf.Tensor, db_range: float = 100.0, **kwargs\n) -> tf.Tensor\n\t\n    db_spectrogram = gain_to_db(spectrogram, **kwargs)\n    max_db = tf.reduce_max(db_spectrogram)\n    clipped_db_spectrogram = tf.clip_by_value(db_spectrogram, max_db - db_range, max_db)\n    uint_spectrogram = from_float32_to_uint8(clipped_db_spectrogram)\n    return uint_spectrogram\n\n", "CodeBleu_score": 47.81910214765941}, {"code": "def spectrogram_to_db_uint(\n    spectrogram: tf.Tensor, db_range: float = 100.0, **kwargs\n) -> tf.Tensor\n\t\n    spectrogram_db = gain_to_db(spectrogram, **kwargs)\n    max_db = tf.reduce_max(spectrogram_db)\n    clipped_db = tf.clip_by_value(spectrogram_db, max_db - db_range, max_db)\n    return from_float32_to_uint8(clipped_db)\n\n", "CodeBleu_score": 40.57132596614642}, {"code": "def spectrogram_to_db_uint(\n    spectrogram: tf.Tensor, db_range: float = 100.0, **kwargs\n) -> tf.Tensor\n\t\n    # Convert from gain to decibel\n    db_spectrogram = gain_to_db(spectrogram, **kwargs)\n    # Find the maximum dB value\n    max_db = tf.reduce_max(db_spectrogram)\n    # Clip the dB spectrogram to a specified range below the maximum\n    clipped_db_spectrogram = tf.clip_by_value(db_spectrogram, max_db - db_range, max_db)\n    # Convert from float32 to uint8\n    uint_spectrogram = from_float32_to_uint8(clipped_db_spectrogram)\n    return uint_spectrogram\n\n", "CodeBleu_score": 47.81910214765941}]}
{"instance_id": "b65c0968dae7", "generation_result": [{"code": "def compute_spectrogram_tf(\n    waveform: tf.Tensor,\n    frame_length: int = 2048,\n    frame_step: int = 512,\n    spec_exponent: float = 1.0,\n    window_exponent: float = 1.0,\n) -> tf.Tensor\n\t\n    # Compute the Hann window function raised to the window_exponent\n    hann_window = tf.signal.hann_window(\n        frame_length, periodic=True, dtype=tf.dtypes.float32\n    )\n    hann_window = tf.pow(hann_window, window_exponent)\n    # Compute the Short-Time Fourier Transform (STFT)\n    stft = tf.signal.stft(\n        waveform,\n        frame_length=frame_length,\n        frame_step=frame_step,\n        fft_length=frame_length,\n        window_fn=lambda x: x * hann_window,\n        pad_end=False,\n    )\n    # Transpose the STFT tensor\n    stft = tf.transpose(stft, [0, 2, 1])\n    # Compute the absolute value of the STFT tensor raised to the spec_exponent\n    spectrogram = tf.pow(tf.abs(stft), spec_exponent)\n    return spectrogram", "CodeBleu_score": 41.55505196093666}, {"code": "def compute_spectrogram_tf(\n    waveform: tf.Tensor,\n    frame_length: int = 2048,\n    frame_step: int = 512,\n    spec_exponent: float = 1.0,\n    window_exponent: float = 1.0,\n) -> tf.Tensor\n\t\n    # Hann window function\n    hann_window = tf.signal.hann_window(frame_length, periodic=True, dtype=tf.dtypes.float32)\n    # Apply the Hann window function raised to a specified exponent\n    hann_window = tf.pow(hann_window, window_exponent)\n    # Short-Time Fourier Transform (STFT)\n    stft = tf.signal.stft(\n        waveform,\n        frame_length=frame_length,\n        frame_step=frame_step,\n        fft_length=frame_length,\n        window_fn=lambda x: hann_window,\n        pad_end=False,\n    )\n    # Transpose the STFT tensor\n    stft = tf.transpose(stft)\n    # Compute the absolute value of the STFT tensor\n    stft = tf.abs(stft)\n    # Raise the absolute value to a specified exponent\n    stft = tf.pow(stft, spec_exponent)\n    return stft\n\n", "CodeBleu_score": 39.4851597340873}, {"code": "def compute_spectrogram_tf(\n    waveform: tf.Tensor,\n    frame_length: int = 2048,\n    frame_step: int = 512,\n    spec_exponent: float = 1.0,\n    window_exponent: float = 1.0,\n) -> tf.Tensor\n\t\n    # Compute the STFT of the input waveform\n    stft = tf.signal.stft(\n        waveform,\n        frame_length=frame_length,\n        frame_step=frame_step,\n        fft_length=frame_length,\n        window_fn=lambda x: tf.signal.hann_window(x, periodic=False) ** window_exponent,\n    )\n    # Transpose the STFT tensor\n    stft = tf.transpose(stft, [0, 2, 1])\n    # Compute the spectrogram as the absolute value of the STFT raised to a specified exponent\n    spectrogram = tf.abs(stft) ** spec_exponent\n    return spectrogram\n\n", "CodeBleu_score": 39.20898578795968}]}
{"instance_id": "a4686140a634", "generation_result": [{"code": "def time_stretch(\n    spectrogram: tf.Tensor,\n    factor: float = 1.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n    time_dim = tf.shape(spectrogram)[1]\n    new_time_dim = tf.cast(time_dim * factor, tf.int32)\n    spectrogram = tf.image.resize(\n        spectrogram, [tf.shape(spectrogram)[0], new_time_dim], method=method\n    )\n    spectrogram = spectrogram[:, :time_dim, :]\n    return spectrogram\n\n", "CodeBleu_score": 37.761730215769475}, {"code": "def time_stretch(\n    spectrogram: tf.Tensor,\n    factor: float = 1.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n    time_dim = tf.shape(spectrogram)[2]\n    new_time_dim = tf.cast(time_dim * factor, tf.int32)\n    spectrogram = tf.expand_dims(spectrogram, 0)\n    spectrogram = tf.image.resize(spectrogram, (1, new_time_dim), method=method)\n    spectrogram = tf.squeeze(spectrogram, 0)\n    return spectrogram\n\n", "CodeBleu_score": 34.22345924102309}, {"code": "def time_stretch(\n    spectrogram: tf.Tensor,\n    factor: float = 1.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n    time_dim = spectrogram.shape[1]\n    new_time_dim = int(time_dim * factor)\n    spectrogram = tf.image.resize(\n        spectrogram,\n        size=(new_time_dim, spectrogram.shape[0]),\n        method=method,\n    )\n    if new_time_dim > time_dim:\n        spectrogram = spectrogram[:, :time_dim, :]\n    else:\n        spectrogram = spectrogram[:, :new_time_dim, :]\n    return spectrogram\n\n", "CodeBleu_score": 32.676017404420726}]}
{"instance_id": "6dd01101d4ae", "generation_result": [{"code": "def pitch_shift(\n    spectrogram: tf.Tensor,\n    semitone_shift: float = 0.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n    # Calculate the resizing factor based on the semitone shift\n    resizing_factor = tf.math.pow(2.0, semitone_shift / 12.0)\n    # Resize the spectrogram along the frequency axis using the resizing factor and the specified method\n    resized_spectrogram = tf.image.resize(\n        spectrogram,\n        size=(tf.shape(spectrogram)[0] * resizing_factor, tf.shape(spectrogram)[1]),\n        method=method,\n    )\n    # Pad the resized spectrogram to maintain the original dimensions\n    padded_spectrogram = tf.pad(\n        resized_spectrogram,\n        paddings=[\n            [0, 0],\n            [0, tf.shape(spectrogram)[0] - tf.shape(resized_spectrogram)[0]],\n        ],\n    )\n    return padded_spectrogram\n\n", "CodeBleu_score": 30.46691323511163}, {"code": "def pitch_shift(\n    spectrogram: tf.Tensor,\n    semitone_shift: float = 0.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n    # Calculate the resizing factor\n    resize_factor = 2.0 ** (semitone_shift / 12.0)\n    # Resize the spectrogram along the frequency axis using the specified method\n    resized_spectrogram = tf.image.resize(\n        spectrogram,\n        size=[tf.shape(spectrogram)[0] * resize_factor, tf.shape(spectrogram)[1]],\n        method=method,\n        preserve_aspect_ratio=False,\n    )\n    # Pad the resized spectrogram to maintain the original dimensions\n    padded_spectrogram = tf.pad(\n        resized_spectrogram,\n        paddings=[[0, 0], [0, tf.shape(spectrogram)[0] - tf.shape(resized_spectrogram)[0]]],\n    )\n    return padded_spectrogram\n\n", "CodeBleu_score": 32.46326703779208}, {"code": "def pitch_shift(\n    spectrogram: tf.Tensor,\n    semitone_shift: float = 0.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n    # Calculate the resizing factor based on the semitone shift\n    resize_factor = 2.0 ** (semitone_shift / 12.0)\n    # Resize the spectrogram along the frequency axis using the specified method\n    resized_spectrogram = tf.image.resize(\n        spectrogram,\n        size=(tf.shape(spectrogram)[0], tf.cast(tf.shape(spectrogram)[1] * resize_factor, tf.int32)),\n        method=method,\n    )\n    # Pad the resized spectrogram to maintain the original dimensions\n    resized_spectrogram = tf.pad(\n        resized_spectrogram,\n        paddings=[[0, 0], [0, tf.shape(spectrogram)[1] - tf.shape(resized_spectrogram)[1]]],\n        mode=\"CONSTANT\",\n    )\n    return resized_spectrogram\n\n", "CodeBleu_score": 33.892267581916556}]}
{"instance_id": "612f0826e56a", "generation_result": [{"code": "def _get_conv_activation_layer(params: Dict) -> Any\n\t\n    if params.get(\"conv_activation\") == \"ReLU\":\n        return ReLU()\n    elif params.get(\"conv_activation\") == \"LeakyReLU\":\n        return LeakyReLU(negative_slope=0.2)\n    elif params.get(\"conv_activation\") == \"ELU\":\n        return ELU()\n    else:\n        raise ValueError(\n            f'The \"conv_activation\" parameter must be one of \"ReLU\", \"LeakyReLU\", or \"ELU\".'\n        )\n\n", "CodeBleu_score": 46.00599543313782}, {"code": "def _get_conv_activation_layer(params: Dict) -> Any\n\t\n    if params.get(\"conv_activation\") == \"ReLU\":\n        return ReLU()\n    elif params.get(\"conv_activation\") == \"LeakyReLU\":\n        return LeakyReLU(negative_slope=0.2)\n    elif params.get(\"conv_activation\") == \"ELU\":\n        return ELU()\n    else:\n        raise ValueError(\n            f\"Invalid value for the 'conv_activation' parameter: {params.get('conv_activation')}. \"\n            \"Please specify a valid value: 'ReLU', 'LeakyReLU', or 'ELU'.\"\n        )\n\n", "CodeBleu_score": 43.90187238928797}, {"code": "def _get_conv_activation_layer(params: Dict) -> Any\n\t\n    conv_activation = params.get(\"conv_activation\", \"ReLU\")\n    if conv_activation == \"ReLU\":\n        return ReLU()\n    elif conv_activation == \"LeakyReLU\":\n        return LeakyReLU(negative_slope=params.get(\"negative_slope\", 0.2))\n    elif conv_activation == \"ELU\":\n        return ELU()\n    else:\n        raise ValueError(f\"Invalid 'conv_activation' parameter: {conv_activation}\")\n\n", "CodeBleu_score": 51.826772078570485}]}
{"instance_id": "2f8b5a2fc0f2", "generation_result": [{"code": "def softmax_unet(\n    input_tensor: tf.Tensor, instruments: Iterable[str], params: Dict = {}\n) -> Dict\n\t\n    logit_mask_list = []\n    for instrument in instruments:\n        logit_mask_list.append(apply_unet(input_tensor, instrument, params))\n    output_tensor = tf.stack(logit_mask_list, axis=-1)\n    output_tensor = Softmax(axis=-1)(output_tensor)\n    output_dict = {}\n    for i, instrument in enumerate(instruments):\n        output_dict[instrument] = Multiply()([output_tensor[..., i], input_tensor])\n    return output_dict\n\n", "CodeBleu_score": 35.28788227801176}, {"code": "def softmax_unet(\n    input_tensor: tf.Tensor, instruments: Iterable[str], params: Dict = {}\n) -> Dict\n\t\n    logit_mask_list = []\n    for i, instrument in enumerate(instruments):\n        logit_mask_list.append(\n            apply_unet(\n                input_tensor,\n                output_name=instrument,\n                params=params,\n                output_mask_logit=True,\n            )\n        )\n    logit_mask_list = tf.stack(logit_mask_list, axis=-1)\n    softmax_mask = Softmax(axis=-1)(logit_mask_list)\n    output = {}\n    for i, instrument in enumerate(instruments):\n        output[instrument] = Multiply(name=instrument)([softmax_mask[..., i], input_tensor])\n    return output\n\n", "CodeBleu_score": 45.34917205831975}, {"code": "def softmax_unet(\n    input_tensor: tf.Tensor, instruments: Iterable[str], params: Dict = {}\n) -> Dict\n\t\n    logit_mask_list = []\n    for instrument in instruments:\n        logit_mask_list.append(\n            apply_unet(input_tensor, output_name=instrument, params=params)\n        )\n    logit_mask_tensor = tf.stack(logit_mask_list, axis=0)\n    mask_tensor = tf.nn.softmax(logit_mask_tensor, axis=0)\n    output = {}\n    for i, instrument in enumerate(instruments):\n        output[instrument] = Multiply(name=instrument)([mask_tensor[i], input_tensor])\n    return output\n\n", "CodeBleu_score": 37.23553705429411}]}
{"instance_id": "400b19ce8520", "generation_result": [{"code": "def compute_file_checksum(path)\n\t\n    sha256 = hashlib.sha256()\n    with open(path, \"rb\") as file:\n        while True:\n            data = file.read(4096)\n            if not data:\n                break\n            sha256.update(data)\n    return sha256.hexdigest()\n\n", "CodeBleu_score": 42.74912515457635}, {"code": "def compute_file_checksum(path)\n\t\n    sha256 = hashlib.sha256()\n    with open(path, \"rb\") as f:\n        while True:\n            data = f.read(65536)\n            if not data:\n                break\n            sha256.update(data)\n    return sha256.hexdigest()\n", "CodeBleu_score": 42.60781395547664}, {"code": "def compute_file_checksum(path)\n\t\n    with open(path, 'rb') as f:\n        sha256 = hashlib.sha256()\n        while True:\n            data = f.read(65536)\n            if not data:\n                break\n            sha256.update(data)\n    return sha256.hexdigest()\n", "CodeBleu_score": 38.93543354687298}]}
{"instance_id": "5f2e9c6e15f1", "generation_result": [{"code": "def checksum(self, name: str) -> str\n\t\n    url = f\"{self.host}/{self.repository}/{self.release}/index.json\"\n    response = httpx.get(url)\n    response.raise_for_status()\n    index = response.json()\n    if name not in index:\n        raise ValueError(f\"Model '{name}' not found in index\")\n    return index[name][\"checksum\"]\n", "CodeBleu_score": 35.88636688920561}, {"code": "def checksum(self, name: str) -> str\n\t\n    url = f\"{self.host}/{self.repository}/{self.release}/index.json\"\n    response = httpx.get(url)\n    response.raise_for_status()\n    index = response.json()\n    if name not in index:\n        raise ValueError(f\"Model {name} not found in index\")\n    return index[name][\"checksum\"]\n", "CodeBleu_score": 35.93558683842498}, {"code": "def checksum(self, name: str) -> str\n\t\n    url = f\"{self.host}/{self.repository}/{self.release_path}/{self.checksum_index}\"\n    response = httpx.get(url)\n    response.raise_for_status()\n    index = response.json()\n    if name not in index:\n        raise ValueError(f\"Model {name} not found in index\")\n    return index[name]", "CodeBleu_score": 36.418427243737774}]}
{"instance_id": "9312c8b6032e", "generation_result": [{"code": "def download(self, name: str, path: str) -> None:\n\t\n    url: str = \"/\".join(\n        (\n            self._host,\n            self._repository,\n            self.RELEASE_PATH,\n            self._release,\n            name,\n        )\n    )\n    logger.info(f\"Downloading {url} to {path}\")\n    with httpx.Client() as client:\n        with tarfile.open(fileobj=client.stream(url), mode=\"r:gz\") as tar:\n            tar.extractall(path)\n    checksum: str = compute_file_checksum(path)\n    if checksum!= self.checksum(name):\n        raise ValueError(f\"Checksum mismatch for {name}\")\n    logger.info(f\"Extracted {name} to {path}\")\n", "CodeBleu_score": 32.5618731833195}, {"code": "def download(self, name: str, path: str) -> None:\n\t\n    url: str = \"/\".join(\n        (self._host, self._repository, self.RELEASE_PATH, self._release, name)\n    )\n    logger.info(f\"Downloading {url}\")\n    with httpx.Client() as client:\n        with client.stream(\"GET\", url) as stream:\n            with tarfile.open(fileobj=stream.iter_raw(), mode=\"r:gz\") as tar:\n                tar.extractall(path)\n\n", "CodeBleu_score": 31.628813011888873}, {"code": "def download(self, name: str, path: str) -> None:\n\t\n    url: str = \"/\".join(\n        (\n            self._host,\n            self._repository,\n            self.RELEASE_PATH,\n            self._release,\n            self.MODELS_PATH,\n            name,\n        )\n    )\n    logger.info(f\"Downloading {name} from {url}\")\n    with httpx.Client() as client:\n        with client.stream(\"GET\", url) as stream:\n            with NamedTemporaryFile() as temp:\n                for chunk in stream.iter_raw():\n                    temp.write(chunk)\n                temp.flush()\n                if compute_file_checksum(temp.name)!= self.checksum(name):\n                    raise IOError(f\"Checksum mismatch for {name}\")\n                tar = tarfile.open(temp.name)\n                tar.extractall(path)\n                os.unlink(temp.name)\n                logger.info(f\"Extracted {name} to {path}\")", "CodeBleu_score": 46.94957654871396}]}
{"instance_id": "ee9493f37d77", "generation_result": [{"code": "def load_configuration(descriptor: str) -> Dict\n\t\n    if descriptor.startswith(\"embedded:\"):\n        name = descriptor[len(\"embedded:\") :]\n        if loader.is_resource(name):\n            with loader.open_text(__name__, name) as f:\n                return json.load(f)\n        else:\n            raise SpleeterError(\n                f\"Embedded configuration '{name}' not found in the resources.\"\n            )\n    else:\n        if not os.path.exists(descriptor):\n            raise SpleeterError(f\"Configuration file '{descriptor}' not found.\")\n        with open(descriptor, \"r\") as f:\n            return json.load(f)\n\n", "CodeBleu_score": 54.97998654787298}, {"code": "def load_configuration(descriptor: str) -> Dict\n\t\n    if descriptor.startswith('embedded:'):\n        name = descriptor[len('embedded:'):]\n        if not loader.is_resource(name):\n            raise SpleeterError(f'Embedded configuration \"{name}\" not found.')\n        with loader.open_text(name) as f:\n            return json.load(f)\n    else:\n        if not os.path.exists(descriptor):\n            raise SpleeterError(f'Configuration file \"{descriptor}\" not found.')\n        with open(descriptor, 'r') as f:\n            return json.load(f)", "CodeBleu_score": 52.60660041098034}, {"code": "def load_configuration(descriptor: str) -> Dict\n\t\n    if descriptor.startswith(\"embedded:\"):\n        name = descriptor.split(\":\")[1]\n        try:\n            with loader.open_text(\"spleeter.resources\", f\"{name}.json\") as f:\n                return json.load(f)\n        except FileNotFoundError:\n            raise SpleeterError(f\"Embedded configuration '{name}' not found.\")\n    else:\n        try:\n            with open(descriptor, \"r\") as f:\n                return json.load(f)\n        except FileNotFoundError:\n            raise SpleeterError(f\"Configuration file '{descriptor}' not found.\")", "CodeBleu_score": 48.42005620249047}]}
{"instance_id": "546134b67fbc", "generation_result": [{"code": "def sync_apply(\n    tensor_dict: Dict[str, tf.Tensor], func: Callable, concat_axis: int = 1\n) -> Dict[str, tf.Tensor]\n\t\n    if concat_axis not in (0, 1):\n        raise NotImplementedError(f\"concat_axis must be either 0 or 1, not {concat_axis}\")\n    if concat_axis == 0:\n        tensor = tf.concat(list(tensor_dict.values()), axis=0)\n        tensor = func(tensor)\n        split = tf.split(tensor, len(tensor_dict))\n        return {k: v for k, v in zip(tensor_dict.keys(), split)}\n    else:\n        tensor = tf.concat(list(tensor_dict.values()), axis=1)\n        tensor = func(tensor)\n        split = tf.split(tensor, tf.shape(tensor)[1], axis=1)\n        return {k: v for k, v in zip(tensor_dict.keys(), split)}", "CodeBleu_score": 44.72430019332443}, {"code": "def sync_apply(\n    tensor_dict: Dict[str, tf.Tensor], func: Callable, concat_axis: int = 1\n) -> Dict[str, tf.Tensor]\n\t\n    if concat_axis not in [0, 1]:\n        raise NotImplementedError(\n            \"The concatenation axis must be either 0 or 1, but got {}.\".format(\n                concat_axis\n            )\n        )\n    # Concatenate tensors\n    tensor_list = tf.concat(tensor_dict.values(), axis=concat_axis)\n    # Apply function\n    tensor_list = func(tensor_list)\n    # Split tensors\n    tensor_dict = dict(\n        zip(tensor_dict.keys(), tf.split(tensor_list, len(tensor_dict), axis=concat_axis))\n    )\n    return tensor_dict\n\n", "CodeBleu_score": 35.97137122680402}, {"code": "def sync_apply(\n    tensor_dict: Dict[str, tf.Tensor], func: Callable, concat_axis: int = 1\n) -> Dict[str, tf.Tensor]\n\t\n    if concat_axis not in [0, 1]:\n        raise NotImplementedError(\n            \"concat_axis should be either 0 or 1, but got {}\".format(concat_axis)\n        )\n    concat_tensor = tf.concat(list(tensor_dict.values()), concat_axis)\n    processed_tensor = func(concat_tensor)\n    if concat_axis == 0:\n        return dict(\n            zip(\n                tensor_dict.keys(),\n                tf.split(processed_tensor, [v.shape[1] for v in tensor_dict.values()])\n            )\n        )\n    else:\n        return dict(\n            zip(\n                tensor_dict.keys(),\n                tf.split(processed_tensor, [v.shape[0] for v in tensor_dict.values()])\n            )\n        )\n", "CodeBleu_score": 41.17450103121259}]}
{"instance_id": "df48e78d46d2", "generation_result": [{"code": "def from_float32_to_uint8(\n    tensor: tf.Tensor,\n    tensor_key: str = \"tensor\",\n    min_key: str = \"min\",\n    max_key: str = \"max\",\n) -> tf.Tensor\n\t\n    min_value = tf.reduce_min(tensor)\n    max_value = tf.reduce_max(tensor)\n    tensor = tf.cast(\n        ((tensor - min_value) / (max_value - min_value)) * 255.0, dtype=tf.uint8\n    )\n    return {\n        tensor_key: tensor,\n        min_key: min_value,\n        max_key: max_value,\n    }\n", "CodeBleu_score": 45.44973615431849}, {"code": "def from_float32_to_uint8(\n    tensor: tf.Tensor,\n    tensor_key: str = \"tensor\",\n    min_key: str = \"min\",\n    max_key: str = \"max\",\n) -> tf.Tensor\n\t\n    tensor = tf.cast(tensor, tf.float32)\n    min_value = tf.reduce_min(tensor)\n    max_value = tf.reduce_max(tensor)\n    tensor = (tensor - min_value) / (max_value - min_value)\n    tensor = tf.clip_by_value(tensor, 0.0, 1.0)\n    tensor = tf.round(tensor * 255.0)\n    tensor = tf.cast(tensor, tf.uint8)\n    return {\n        tensor_key: tensor,\n        min_key: min_value,\n        max_key: max_value,\n    }\n\n", "CodeBleu_score": 44.92158167367497}, {"code": "def from_float32_to_uint8(\n    tensor: tf.Tensor,\n    tensor_key: str = \"tensor\",\n    min_key: str = \"min\",\n    max_key: str = \"max\",\n) -> tf.Tensor\n\t\n    min_value = tf.reduce_min(tensor)\n    max_value = tf.reduce_max(tensor)\n    tensor = tf.cast(\n        tf.clip_by_value(\n            (tensor - min_value) / (max_value - min_value), 0, 1\n        )\n        * 255,\n        tf.uint8,\n    )\n    return {\n        tensor_key: tensor,\n        min_key: min_value,\n        max_key: max_value,\n    }\n", "CodeBleu_score": 44.310218615892765}]}
{"instance_id": "7f479e5de492", "generation_result": [{"code": "def pad_and_partition(tensor: tf.Tensor, segment_len: int) -> tf.Tensor\n\t\n    tensor_len = tf.shape(tensor)[0]\n    pad_len = tf.math.floormod(tensor_len, segment_len)\n    pad_tensor = tf.pad(tensor, [[0, pad_len]])\n    return tf.reshape(pad_tensor, [-1, segment_len, tf.shape(tensor)[1]])\n\n", "CodeBleu_score": 24.179681724433586}, {"code": "def pad_and_partition(tensor: tf.Tensor, segment_len: int) -> tf.Tensor\n\t\n    tensor_len = tf.shape(tensor)[0]\n    padding_len = tf.math.floormod(tensor_len, segment_len)\n    padding = tf.zeros([segment_len - padding_len] + tensor.shape[1:], dtype=tensor.dtype)\n    padded_tensor = tf.concat([tensor, padding], axis=0)\n    segments = tf.reshape(padded_tensor, [-1, segment_len] + tensor.shape[1:])\n    return segments\n", "CodeBleu_score": 29.88040476557573}, {"code": "def pad_and_partition(tensor: tf.Tensor, segment_len: int) -> tf.Tensor\n\t\n    # Calculate the number of segments\n    num_segments = tf.math.ceil(tf.cast(tf.shape(tensor)[0], tf.float32) / segment_len)\n    # Calculate the padding needed to make the tensor length a multiple of the segment length\n    padding_needed = tf.math.ceil(num_segments) * segment_len - tf.shape(tensor)[0]\n    # Pad the tensor with zeros\n    padded_tensor = tf.pad(tensor, [[0, padding_needed]] + [[0, 0]] * (len(tensor.shape) - 1))\n    # Reshape the tensor into segments\n    segments = tf.reshape(padded_tensor, [-1, segment_len] + list(tensor.shape[1:]))\n    return segments\n\n", "CodeBleu_score": 42.757260468487765}]}
{"instance_id": "1789fae7253f", "generation_result": [{"code": "def pad_and_reshape(instr_spec, frame_length, F) -> Any\n\t\n    # Calculate the number of extra rows needed for padding\n    extra_rows = frame_length - instr_spec.shape[0] % frame_length\n    # Create an extension of zeros\n    extension = tf.zeros(\n        shape=(extra_rows, instr_spec.shape[1]), dtype=instr_spec.dtype\n    )\n    # Concatenate the extension to the original spectrogram along the frequency axis\n    instr_spec = tf.concat([instr_spec, extension], axis=0)\n    # Reshape the extended spectrogram by merging the first two dimensions\n    instr_spec = tf.reshape(instr_spec, [-1, frame_length, F])\n    return instr_spec\n\n", "CodeBleu_score": 18.535948984399027}, {"code": "def pad_and_reshape(instr_spec, frame_length, F) -> Any\n\t\n    # Calculate the number of extra rows needed for padding\n    extra_rows = frame_length - tf.shape(instr_spec)[0] % frame_length\n    # Create an extension of zeros\n    extension = tf.zeros(shape=[extra_rows, tf.shape(instr_spec)[1]])\n    # Concatenate the extension to the original spectrogram along the frequency axis\n    instr_spec = tf.concat([instr_spec, extension], axis=0)\n    # Reshape the extended spectrogram by merging the first two dimensions\n    instr_spec = tf.reshape(instr_spec, shape=[-1, F])\n    return instr_spec\n", "CodeBleu_score": 18.84803355957253}, {"code": "def pad_and_reshape(instr_spec, frame_length, F) -> Any\n\t\n    # Calculate the number of extra rows needed for padding\n    num_extra_rows = (frame_length - (instr_spec.shape[0] % frame_length)) % frame_length\n    # Create an extension of zeros\n    extension = tf.zeros((num_extra_rows, instr_spec.shape[1]))\n    # Concatenate the extension to the original spectrogram along the frequency axis\n    padded_instr_spec = tf.concat([instr_spec, extension], axis=0)\n    # Reshape the extended spectrogram by merging the first two dimensions\n    reshaped_instr_spec = tf.reshape(padded_instr_spec, [1, -1, F])\n    return reshaped_instr_spec\n", "CodeBleu_score": 18.35198076754225}]}
{"instance_id": "8a4f4658d97c", "generation_result": [{"code": "def dataset_from_csv(csv_path: str, **kwargs) -> Any\n\t\n    df = pd.read_csv(csv_path)\n    return from_tensor_slices(df)\n\n", "CodeBleu_score": 13.30239607634188}, {"code": "def dataset_from_csv(csv_path: str, **kwargs) -> Any\n\t\n    df = pd.read_csv(csv_path)\n    dataset = tf.data.Dataset.from_tensor_slices(df)\n    return dataset\n\n", "CodeBleu_score": 24.061023281051106}, {"code": "def dataset_from_csv(csv_path: str, **kwargs) -> Any\n\t\n    df = pd.read_csv(csv_path)\n    return tf.data.Dataset.from_tensor_slices(df)\n\n", "CodeBleu_score": 17.232156492595102}]}
{"instance_id": "9a16e81b182f", "generation_result": [{"code": "def check_tensor_shape(tensor_tf: tf.Tensor, target_shape: Any) -> bool\n\t\n    result = True\n    for i, dim in enumerate(target_shape):\n        if dim is not None:\n            result = tf.logical_and(result, tf.equal(tf.shape(tensor_tf)[i], dim))\n    return result\n\n", "CodeBleu_score": 40.96549122846467}, {"code": "def check_tensor_shape(tensor_tf: tf.Tensor, target_shape: Any) -> bool\n\t\n    result = True\n    for dim, length in enumerate(target_shape):\n        if length is not None:\n            result = tf.logical_and(result, tf.equal(tf.shape(tensor_tf)[dim], length))\n    return result\n", "CodeBleu_score": 42.28173603768897}, {"code": "def check_tensor_shape(tensor_tf: tf.Tensor, target_shape: Any) -> bool\n\t\n    result = True\n    for dim, length in enumerate(target_shape):\n        if length is not None:\n            result = tf.logical_and(result, tf.equal(tf.shape(tensor_tf)[dim], length))\n    return result\n\n", "CodeBleu_score": 42.28173603768897}]}
{"instance_id": "49d5d3084b3a", "generation_result": [{"code": "def get_training_dataset(\n    audio_params: Dict, audio_adapter: AudioAdapter, audio_path: str\n) -> Any\n\t# Chunk duration\n    chunk_duration = 11.88\n    # Random seed\n    random_seed = 42\n    builder = DatasetBuilder(\n        audio_params=audio_params,\n        audio_adapter=audio_adapter,\n        chunk_duration=chunk_duration,\n        random_seed=random_seed,\n    )\n    dataset = builder.build(\n        train_csv,\n        batch_size=8,\n        cache_directory=cache_directory,\n        random_data_augmentation=True,\n        random_time_crop=True,\n        n_chunks_per_song=2,\n    )\n    return dataset\n\n", "CodeBleu_score": 32.9768325228888}, {"code": "def get_training_dataset(\n    audio_params: Dict, audio_adapter: AudioAdapter, audio_path: str\n) -> Any\n\tbuilder = DatasetBuilder(audio_params, audio_adapter, chunk_duration=11.88, seed=1)\n    dataset = builder.build(\n        train_csv,\n        batch_size=8,\n        cache_directory=cache_directory,\n        random_time_crop=True,\n        random_data_augmentation=True,\n        n_chunks_per_song=2,\n    )\n    return dataset\n\n", "CodeBleu_score": 22.668625879676522}, {"code": "def get_training_dataset(\n    audio_params: Dict, audio_adapter: AudioAdapter, audio_path: str\n) -> Any\n\t# Initialize DatasetBuilder.\n    builder = DatasetBuilder(\n        audio_params=audio_params,\n        audio_adapter=audio_adapter,\n        chunk_duration=audio_params[\"chunk_duration\"],\n        random_seed=42,\n    )\n    # Build dataset.\n    dataset = builder.build(\n        train_csv,\n        batch_size=batch_size,\n        random_data_augmentation=True,\n        cache_directory=cache_directory,\n        n_chunks_per_song=n_chunks_per_song,\n    )\n    return dataset\n\n", "CodeBleu_score": 31.937933120001983}]}
{"instance_id": "fe8bbbba06e3", "generation_result": [{"code": "def instruments(self) -> Any\n\t\n    if self._instruments is None:\n        self._instruments = []\n        for instrument in self.instrument_list:\n            self._instruments.append(InstrumentDatasetBuilder(instrument, self.instrument_list))\n    for instrument in self._instruments:\n        yield instrument\n\n", "CodeBleu_score": 47.64052925725674}, {"code": "def instruments(self) -> Any\n\t\n    if not self._instrument_builders:\n        for instrument in self._instruments:\n            self._instrument_builders.append(InstrumentDatasetBuilder(instrument))\n    for instrument_builder in self._instrument_builders:\n        yield instrument_builder\n\n", "CodeBleu_score": 40.92387456420741}, {"code": "def instruments(self) -> Any\n\t\n    if self._instruments is None:\n        self._instruments = []\n        for instrument in self._instrument_list:\n            instrument_builder = InstrumentDatasetBuilder(instrument)\n            self._instruments.append(instrument_builder)\n    for instrument in self._instruments:\n        yield instrument\n\n", "CodeBleu_score": 49.18017068580035}]}
{"instance_id": "438b57fd3413", "generation_result": [{"code": "def cache(self, dataset: Any, cache: Optional[str], wait: bool) -> Any\n\t\n    if cache is not None:\n        if wait:\n            while not exists(f\"{cache}.index\"):\n                logger.info(f\"Cache not available, wait {self.WAIT_PERIOD}\")\n                time.sleep(self.WAIT_PERIOD)\n        cache_path = os.path.split(cache)[0]\n        os.makedirs(cache_path, exist_ok=True)\n        return dataset.cache(cache)\n    return dataset\n\n", "CodeBleu_score": 93.65648279369884}, {"code": "def cache(self, dataset: Any, cache: Optional[str], wait: bool) -> Any\n\t\n    if cache is not None:\n        if wait:\n            while not exists(f\"{cache}.index\"):\n                logger.info(f\"Cache not available, wait {self.WAIT_PERIOD}\")\n                time.sleep(self.WAIT_PERIOD)\n        cache_path = os.path.split(cache)[0]\n        os.makedirs(cache_path, exist_ok=True)\n        return dataset.cache(cache)\n    return dataset", "CodeBleu_score": 93.65648279369884}, {"code": "def cache(self, dataset: Any, cache: Optional[str], wait: bool) -> Any\n\t\n    if cache is not None:\n        if wait:\n            while not exists(f\"{cache}.index\"):\n                logger.info(f\"Cache not available, wait {self.WAIT_PERIOD}\")\n                time.sleep(self.WAIT_PERIOD)\n        cache_path = os.path.split(cache)[0]\n        os.makedirs(cache_path, exist_ok=True)\n        return dataset.cache(cache)\n    return dataset", "CodeBleu_score": 93.65648279369884}]}
{"instance_id": "8838a3a09a66", "generation_result": [{"code": "def create_estimator(params: Dict, MWF: bool) -> tf.Tensor\n\t\n    # Load the model directory using a ModelProvider\n    model_provider = ModelProvider.default()\n    model_dir = model_provider.get(params[\"model_name\"])\n\n    # Update the parameters\n    params[\"model_dir\"] = model_dir\n    params[\"batch_size\"] = params[\"batch_size\"]\n    params[\"num_epochs\"] = params[\"num_epochs\"]\n    params[\"num_classes\"] = params[\"num_classes\"]\n    params[\"num_examples\"] = params[\"num_examples\"]\n\n    # Set up the session configuration to limit GPU memory usage\n    config = ConfigProto()\n    config.gpu_options.allow_growth = True\n\n    # Create a RunConfig with this session configuration\n    run_config = RunConfig(session_config=config)\n\n    # Initialize and return a TensorFlow estimator using the specified model function, model directory, parameters, and configuration\n    return Estimator(\n        model_fn=model_fn,\n        model_dir=model_dir,\n        params=params,\n        config=run_config,\n    )\n\n", "CodeBleu_score": 31.73218789344195}, {"code": "def create_estimator(params: Dict, MWF: bool) -> tf.Tensor\n\t\n    model_provider = ModelProvider.default(model_dir=params[\"model_dir\"])\n    model_provider.update_params(params)\n    session_config = tf.ConfigProto()\n    session_config.gpu_options.allow_growth = True\n    run_config = RunConfig(session_config=session_config)\n    return Estimator(\n        model_fn=model_fn,\n        model_dir=params[\"model_dir\"],\n        config=run_config,\n        params=params,\n        warm_start_from=params[\"model_dir\"] if MWF else None,\n    )\n\n", "CodeBleu_score": 33.895571807950084}, {"code": "def create_estimator(params: Dict, MWF: bool) -> tf.Tensor\n\t\n    # Load the model directory\n    model_provider = ModelProvider.default()\n    model_provider.load(params[\"model_dir\"])\n    # Update the parameters\n    params.update(model_provider.params)\n    # Set up the session configuration to limit GPU memory usage\n    session_config = tf.ConfigProto()\n    session_config.gpu_options.allow_growth = True\n    # Create a RunConfig with this session configuration\n    run_config = RunConfig(session_config=session_config)\n    # Initialize and return the TensorFlow estimator\n    return tf.estimator.Estimator(\n        model_fn=model_provider.model_fn,\n        model_dir=params[\"model_dir\"],\n        params=params,\n        config=run_config,\n    )\n\n", "CodeBleu_score": 36.08446845454147}]}
{"instance_id": "779c812bf2cb", "generation_result": [{"code": "def _separate_tensorflow(\n        self, waveform: np.ndarray, audio_descriptor: AudioDescriptor\n    ) -> Dict\n\t\n    waveform = ensure_stereo(waveform)\n    prediction_generator = self._get_prediction_generator()\n    self._data_generator.update_data(\n        {\"waveform\": waveform, \"audio_descriptor\": audio_descriptor}\n    )\n    prediction = next(prediction_generator)\n    prediction.pop(\"audio_id\")\n    return prediction\n", "CodeBleu_score": 55.26513556443673}, {"code": "def _separate_tensorflow(\n        self, waveform: np.ndarray, audio_descriptor: AudioDescriptor\n    ) -> Dict\n\t\n    waveform = ensure_stereo(waveform)\n    self._prediction_generator = self._get_prediction_generator()\n    self.update_data(\n        {\n            \"waveform\": waveform,\n            \"audio_id\": audio_descriptor.audio_id,\n        }\n    )\n    prediction = next(self._prediction_generator)\n    prediction.pop(\"audio_id\")\n    return prediction\n", "CodeBleu_score": 44.74884923519279}, {"code": "def _separate_tensorflow(\n        self, waveform: np.ndarray, audio_descriptor: AudioDescriptor\n    ) -> Dict\n\t\n    waveform = to_stereo(waveform)\n    self.update_data(\n        {\n            \"waveform\": waveform,\n            \"audio_id\": audio_descriptor.audio_id,\n        }\n    )\n    prediction = next(self._get_prediction_generator())\n    prediction.pop(\"audio_id\")\n    return prediction\n", "CodeBleu_score": 40.17730591351044}]}
{"instance_id": "cf580cd2ed93", "generation_result": [{"code": "def compute_backbone_shapes(config, image_shape)\n\t\n    if callable(config.BACKBONE):\n        return config.COMPUTE_BACKBONE_SHAPE(image_shape)\n    else:\n        assert config.BACKBONE in [\"resnet50\", \"resnet101\"]\n        return np.array(\n            [\n                tuple(int(x / 2 ** 4) for x in image_shape),\n                tuple(int(x / 2 ** 5) for x in image_shape),\n                tuple(int(x / 2 ** 6) for x in image_shape),\n                tuple(int(x / 2 ** 7) for x in image_shape),\n            ]\n        )\n\n", "CodeBleu_score": 47.813447651087436}, {"code": "def compute_backbone_shapes(config, image_shape)\n\t\n    if callable(config.BACKBONE):\n        return config.COMPUTE_BACKBONE_SHAPE(image_shape)\n    # Currently supports ResNet only\n    assert config.BACKBONE in [\"resnet50\", \"resnet101\"]\n    return np.array(\n        [\n            [\n                int(math.ceil(image_shape[0] / stride)),\n                int(math.ceil(image_shape[1] / stride)),\n            ]\n            for stride in config.BACKBONE_STRIDES\n        ]\n    )\n\n", "CodeBleu_score": 81.85065464141955}, {"code": "def compute_backbone_shapes(config, image_shape)\n\t\n    if callable(config.BACKBONE):\n        return config.COMPUTE_BACKBONE_SHAPE(image_shape)\n    # Currently supports ResNet only\n    assert config.BACKBONE in [\"resnet50\", \"resnet101\"]\n    return np.array(\n        [\n            tuple(int(math.ceil(dim / stride)) for dim in image_shape)\n            for stride in config.BACKBONE_STRIDES\n        ]\n    )\n\n", "CodeBleu_score": 57.30487428644799}]}
{"instance_id": "ebd61a2e59ca", "generation_result": [{"code": "def clip_boxes_graph(boxes, window)\n\t\n    # Split the boxes into their components\n    x1, y1, x2, y2 = tf.split(boxes, 4, axis=1)\n\n    # Clip the coordinates to lie within the window\n    x1 = tf.maximum(tf.minimum(x1, window[2]), window[0])\n    x2 = tf.maximum(tf.minimum(x2, window[2]), window[0])\n    y1 = tf.maximum(tf.minimum(y1, window[3]), window[1])\n    y2 = tf.maximum(tf.minimum(y2, window[3]), window[1])\n\n    # Rebuild the clipped boxes\n    clipped = tf.concat([x1, y1, x2, y2], axis=1, name=\"clipped_boxes\")\n\n    # Rebuild the shape to match the input\n    clipped.set_shape((clipped.shape[0], 4))\n\n    return clipped\n\n", "CodeBleu_score": 37.22779216539621}, {"code": "def clip_boxes_graph(boxes, window)\n\t\n    # Split into individual coordinates\n    ymin, xmin, ymax, xmax = tf.split(axis=1, num_or_size_splits=4, value=boxes)\n    # Clip\n    ymin = tf.maximum(tf.minimum(ymin, window[0]), 0)\n    xmin = tf.maximum(tf.minimum(xmin, window[1]), 0)\n    ymax = tf.maximum(tf.minimum(ymax, window[2]), 0)\n    xmax = tf.maximum(tf.minimum(xmax, window[3]), 0)\n    clipped = tf.concat(axis=1, values=[ymin, xmin, ymax, xmax])\n    clipped.set_shape((clipped.shape[0], 4))\n    return clipped\n", "CodeBleu_score": 28.01810464360157}, {"code": "def clip_boxes_graph(boxes, window)\n\t\n    # Split the window and box coordinates into individual components\n    wy1, wx1, wy2, wx2 = tf.split(window, 4)\n    by1, bx1, by2, bx2 = tf.split(boxes, 4, axis=1)\n    # Clip the box coordinates to lie within the window boundaries\n    clipped_by1 = tf.maximum(tf.minimum(by1, wy2), wy1)\n    clipped_bx1 = tf.maximum(tf.minimum(bx1, wx2), wx1)\n    clipped_by2 = tf.maximum(tf.minimum(by2, wy2), wy1)\n    clipped_bx2 = tf.maximum(tf.minimum(bx2, wx2), wx1)\n    # Concatenate the clipped coordinates back into a tensor\n    clipped_boxes = tf.concat([clipped_by1, clipped_bx1, clipped_by2, clipped_bx2], axis=1)\n    clipped_boxes.set_shape((clipped_boxes.shape[0], 4))\n    return clipped_boxes\n", "CodeBleu_score": 34.72224337443542}]}
{"instance_id": "8e0411587a70", "generation_result": [{"code": "def rpn_graph(feature_map, anchors_per_location, anchor_stride)\n\t\n    # Anchor Score. [batch, height, width, anchors per location * 2].\n    x = KL.Conv2D(256, (3, 3), padding=\"same\", activation=\"relu\", kernel_initializer=\"normal\", name=\"rpn_conv_shared\")(feature_map)\n    # x = KL.TimeDistributed(KL.Conv2D(256, (3, 3), padding=\"same\", activation=\"relu\", kernel_initializer=\"normal\", name=\"rpn_conv_shared\"))(feature_map)\n    # x = KL.TimeDistributed(KL.Conv2D(256, (3, 3), padding=\"same\", activation=\"relu\", kernel_initializer=\"normal\", name=\"rpn_conv_shared\"))(feature_map)\n    # x = KL.TimeDistributed(KL.Conv2D(256, (3, 3), padding=\"same\", activation=\"relu\", kernel_initializer=\"normal\", name=\"rpn_conv_shared\"))(feature_map)\n    # x = KL.TimeDistributed(KL.Conv2D(256, (3, 3), padding=\"same\", activation=\"relu\", kernel_initializer=\"normal\", name=\"rpn_conv_shared\"))(feature_map)\n    # x = KL.TimeDistributed(KL.Conv2D(256, (3, 3), padding=\"same\", activation=\"relu\", kernel_initializer=\"normal\", name=\"rpn_conv_shared\"))(feature_map)\n    # x = KL.TimeDistributed(KL.Conv2D(256, (3, 3), padding=\"same\", activation=\"relu\", kernel_initializer=\"normal\", name=\"rpn_conv_shared\"))(feature_map)\n    # x = KL.TimeDistributed(KL.Conv2D(256, (3, 3), padding=\"same\", activation=\"relu\", kernel_initializer=\"normal\", name=\"rpn_conv_shared\"))(feature_map)\n    # x = KL.TimeDistributed(KL.Conv2D(256, (3, 3), padding=\"same\", activation=\"relu\", kernel_initializer=\"normal\", name=\"rpn_conv_shared\"))(feature_map)\n    # x = KL.TimeDistributed(KL.Conv2", "CodeBleu_score": 6.87249579248586}, {"code": "def rpn_graph(feature_map, anchors_per_location, anchor_stride)\n\t\n    # Shared convolutional base of the RPN\n    shared = KL.Conv2D(512, (3, 3), padding='same', activation='relu', strides=anchor_stride,\n                       name='rpn_conv_shared')(feature_map)\n\n    # Anchor Score. [batch, height, width, anchors per location * 2].\n    x = KL.Conv2D(2 * anchors_per_location, (1, 1), padding='valid', activation='linear', name='rpn_class_raw')(shared)\n\n    # Reshape to [batch, anchors, 2]\n    rpn_class_logits = KL.Lambda(lambda t: tf.reshape(t, [tf.shape(t)[0], -1, 2]))(x)\n\n    # Softmax on last dimension of BG/FG.\n    rpn_probs = KL.Activation(\"softmax\", name=\"rpn_class_xxx\")(rpn_class_logits)\n\n    # Bounding box refinement. [batch, H, W, anchors per location * depth]\n    # where depth is [x, y, log(w), log(h)]\n    x = KL.Conv2D(anchors_per_location * 4, (1, 1), padding=\"valid\", activation='linear', name='rpn_bbox_pred')(shared)\n\n    # Reshape to [batch, anchors, 4]\n    rpn_bbox = KL.Lambda(lambda t: tf.reshape(t, [tf.shape(t)[0], -1, 4]))(x)\n\n    return [rpn_class_logits, rpn_probs, rpn_bbox]\n", "CodeBleu_score": 79.39176524727398}, {"code": "def rpn_graph(feature_map, anchors_per_location, anchor_stride)\n\t\n    # Shared convolutional base of the RPN\n    shared = KL.Conv2D(512, (3, 3), padding='same', activation='relu', strides=anchor_stride, name='rpn_conv_shared')(feature_map)\n    # Anchor Score. [batch, height, width, anchors per location * 2].\n    x = KL.Conv2D(2 * anchors_per_location, (1, 1), padding='valid', activation='linear', name='rpn_class_raw')(shared)\n    # Reshape to [batch, anchors, 2]\n    rpn_class_logits = KL.Lambda(lambda t: tf.reshape(t, [tf.shape(t)[0], -1, 2]))(x)\n    # Softmax on last dimension of BG/FG.\n    rpn_probs = KL.Activation(\"softmax\", name=\"rpn_class_xxx\")(rpn_class_logits)\n    # Bounding box refinement. [batch, H, W, anchors per location * depth]\n    # where depth is [x, y, log(w), log(h)]\n    x = KL.Conv2D(anchors_per_location * 4, (1, 1), padding=\"valid\", activation='linear', name='rpn_bbox_pred')(shared)\n    # Reshape to [batch, anchors, 4]\n    rpn_bbox = KL.Lambda(lambda t: tf.reshape(t, [tf.shape(t)[0], -1, 4]))(x)\n    return [rpn_class_logits, rpn_probs, rpn_bbox]\n\n", "CodeBleu_score": 77.80515265065107}]}
{"instance_id": "70777c9261d8", "generation_result": [{"code": "def smooth_l1_loss(y_true, y_pred)\n\t\n    diff = K.abs(y_true - y_pred)\n    less_than_one = K.cast(K.less(diff, 1.0), \"float32\")\n    loss = (less_than_one * 0.5 * diff**2) + (1 - less_than_one) * (diff - 0.5)\n    return loss\n\n", "CodeBleu_score": 60.928513203413225}, {"code": "def smooth_l1_loss(y_true, y_pred)\n\t\n    diff = K.abs(y_true - y_pred)\n    less_than_one = K.cast(K.less(diff, 1.0), \"float32\")\n    loss = (less_than_one * 0.5 * diff**2) + (1 - less_than_one) * (diff - 0.5)\n    return loss\n\n", "CodeBleu_score": 60.928513203413225}, {"code": "def smooth_l1_loss(y_true, y_pred)\n\t\n    diff = K.abs(y_true - y_pred)\n    less_than_one = K.cast(K.less(diff, 1.0), \"float32\")\n    loss = (less_than_one * 0.5 * diff**2) + (1 - less_than_one) * (diff - 0.5)\n    return loss\n", "CodeBleu_score": 60.928513203413225}]}
{"instance_id": "0e747b604087", "generation_result": [{"code": "def build_model(x_train, num_classes)\n\t\n    tf.reset_default_graph()\n    input_layer = KL.Input(shape=x_train.shape[1:])\n    conv_layer_1 = KL.Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(input_layer)\n    conv_layer_2 = KL.Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(conv_layer_1)\n    max_pool_layer = KL.MaxPooling2D(pool_size=(2, 2))(conv_layer_2)\n    flatten_layer = KL.Flatten()(max_pool_layer)\n    dense_layer_1 = KL.Dense(units=128, activation='relu')(flatten_layer)\n    output_layer = KL.Dense(units=num_classes, activation='softmax')(dense_layer_1)\n    model = KM.Model(inputs=input_layer, outputs=output_layer)\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model\n", "CodeBleu_score": 28.116538101138506}, {"code": "def build_model(x_train, num_classes)\n\t\n    tf.reset_default_graph()\n    inputs = KL.Input(shape=x_train.shape[1:])\n    x = KL.Conv2D(32, (3, 3), activation='relu')(inputs)\n    x = KL.Conv2D(32, (3, 3), activation='relu')(x)\n    x = KL.MaxPooling2D(pool_size=(2, 2))(x)\n    x = KL.Flatten()(x)\n    x = KL.Dense(128, activation='relu')(x)\n    outputs = KL.Dense(num_classes, activation='softmax')(x)\n    model = KM.Model(inputs=inputs, outputs=outputs)\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model\n\n", "CodeBleu_score": 35.15241568334767}, {"code": "def build_model(x_train, num_classes)\n\t\n    tf.reset_default_graph()\n    input_shape = (x_train.shape[1], x_train.shape[2], x_train.shape[3])\n    inputs = KL.Input(shape=input_shape)\n    x = KL.Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n    x = KL.Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n    x = KL.MaxPooling2D((2, 2), strides=(2, 2))(x)\n    x = KL.Flatten()(x)\n    x = KL.Dense(512, activation='relu')(x)\n    outputs = KL.Dense(num_classes, activation='softmax')(x)\n    model = KM.Model(inputs=inputs, outputs=outputs)\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n    return model\n\n", "CodeBleu_score": 31.143417944790365}]}
{"instance_id": "813e3e2844f4", "generation_result": [{"code": "def extract_bboxes(mask)\n\t\n    boxes = np.zeros([mask.shape[-1], 4], dtype=np.int32)\n    for i in range(mask.shape[-1]):\n        m = mask[:, :, i]\n        # Bounding box.\n        horizontal_indicies = np.where(np.any(m, axis=0))[0]\n        vertical_indicies = np.where(np.any(m, axis=1))[0]\n        if horizontal_indicies.shape[0]:\n            x1, x2 = horizontal_indicies[[0, -1]]\n            y1, y2 = vertical_indicies[[0, -1]]\n            # x2 and y2 should not be part of the box. Increment by 1.\n            x2 += 1\n            y2 += 1\n        else:\n            # No mask for this instance. Might happen due to\n            # resizing or cropping. Set bbox to zeros\n            x1, x2, y1, y2 = 0, 0, 0, 0\n        boxes[i] = np.array([y1, x1, y2, x2])\n    return boxes.astype(np.int32)\n", "CodeBleu_score": 80.34862039701206}, {"code": "def extract_bboxes(mask)\n\t\n    boxes = np.zeros([mask.shape[-1], 4], dtype=np.int32)\n    for i in range(mask.shape[-1]):\n        m = mask[:, :, i]\n        # Bounding box.\n        horizontal_indicies = np.where(np.any(m, axis=0))[0]\n        vertical_indicies = np.where(np.any(m, axis=1))[0]\n        if horizontal_indicies.shape[0]:\n            x1, x2 = horizontal_indicies[[0, -1]]\n            y1, y2 = vertical_indicies[[0, -1]]\n            # x2 and y2 should not be part of the box. Increment by 1.\n            x2 += 1\n            y2 += 1\n        else:\n            # No mask for this instance. Might happen due to\n            # resizing or cropping. Set bbox to zeros\n            x1, x2, y1, y2 = 0, 0, 0, 0\n        boxes[i] = np.array([y1, x1, y2, x2])\n    return boxes.astype(np.int32)\n", "CodeBleu_score": 80.34862039701206}, {"code": "def extract_bboxes(mask)\n\t\n    boxes = np.zeros([mask.shape[-1], 4], dtype=np.int32)\n    for i in range(mask.shape[-1]):\n        m = mask[:, :, i]\n        # Bounding box.\n        horizontal_indicies = np.where(np.any(m, axis=0))[0]\n        vertical_indicies = np.where(np.any(m, axis=1))[0]\n        if horizontal_indicies.shape[0]:\n            x1, x2 = horizontal_indicies[[0, -1]]\n            y1, y2 = vertical_indicies[[0, -1]]\n            # x2 and y2 should not be part of the box. Increment by 1.\n            x2 += 1\n            y2 += 1\n        else:\n            # No mask for this instance. Might happen due to\n            # resizing or cropping. Set bbox to zeros\n            x1, x2, y1, y2 = 0, 0, 0, 0\n        boxes[i] = np.array([y1, x1, y2, x2])\n    return boxes.astype(np.int32)\n\n", "CodeBleu_score": 80.34862039701206}]}
{"instance_id": "7ad5d430d6ed", "generation_result": [{"code": "def compute_iou(box, boxes, box_area, boxes_area)\n\t\n    # Calculate the intersection areas\n    y1 = np.maximum(box[0], boxes[:, 0])\n    y2 = np.minimum(box[2], boxes[:, 2])\n    x1 = np.maximum(box[1], boxes[:, 1])\n    x2 = np.minimum(box[3], boxes[:, 3])\n    intersection = np.maximum(x2 - x1, 0) * np.maximum(y2 - y1, 0)\n    # Calculate the union areas\n    union = box_area + boxes_area[:] - intersection[:]\n    # Compute the IoU values\n    iou = intersection / union\n    return iou\n", "CodeBleu_score": 58.663259083089194}, {"code": "def compute_iou(box, boxes, box_area, boxes_area)\n\t\n    # 1. Calculate intersection areas\n    x1 = np.maximum(box[0], boxes[:, 0])\n    y1 = np.maximum(box[1], boxes[:, 1])\n    x2 = np.minimum(box[2], boxes[:, 2])\n    y2 = np.minimum(box[3], boxes[:, 3])\n    intersection = np.maximum(x2 - x1, 0) * np.maximum(y2 - y1, 0)\n    # 2. Calculate union areas\n    union = box_area + boxes_area[:] - intersection[:]\n    # 3. Compute IoU\n    iou = intersection / union\n    return iou\n\n", "CodeBleu_score": 55.49021365037661}, {"code": "def compute_iou(box, boxes, box_area, boxes_area)\n\t\n    # Calculate the intersection areas\n    y1 = np.maximum(box[0], boxes[:, 0])\n    x1 = np.maximum(box[1], boxes[:, 1])\n    y2 = np.minimum(box[2], boxes[:, 2])\n    x2 = np.minimum(box[3], boxes[:, 3])\n    intersection = np.maximum(x2 - x1, 0) * np.maximum(y2 - y1, 0)\n    # Calculate the union areas\n    union = box_area + boxes_area[:] - intersection[:]\n    # Compute the IoU\n    iou = intersection / union\n    return iou\n", "CodeBleu_score": 55.99607250877621}]}
{"instance_id": "15941b852859", "generation_result": [{"code": "def compute_overlaps(boxes1, boxes2)\n\t\n    # Areas of anchors and GT boxes\n    area1 = (boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1])\n    area2 = (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1])\n\n    # Compute overlaps to generate matrix [boxes1 count, boxes2 count]\n    # Each cell contains the IoU value.\n    overlaps = np.zeros((boxes1.shape[0], boxes2.shape[0]))\n    for i in range(overlaps.shape[1]):\n        box2 = boxes2[i]\n        overlaps[:, i] = compute_iou(box2, boxes1, area2[i], area1)\n    return overlaps\n\n", "CodeBleu_score": 62.2853282032944}, {"code": "def compute_overlaps(boxes1, boxes2)\n\t\n    # Areas of anchors and GT boxes\n    area1 = (boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1])\n    area2 = (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1])\n    # Compute overlaps to generate matrix [boxes1 count, boxes2 count]\n    # Each cell contains the IoU value.\n    overlaps = np.zeros((boxes1.shape[0], boxes2.shape[0]))\n    for i in range(overlaps.shape[1]):\n        box2 = boxes2[i]\n        overlaps[:, i] = compute_iou(box2, boxes1, area2[i], area1)\n    return overlaps\n\n", "CodeBleu_score": 62.2853282032944}, {"code": "def compute_overlaps(boxes1, boxes2)\n\t\n    # Areas of anchors and GT boxes\n    area1 = (boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1])\n    area2 = (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1])\n\n    # Compute overlaps to generate matrix [boxes1 count, boxes2 count]\n    # Each cell contains the IoU value.\n    overlaps = np.zeros((boxes1.shape[0], boxes2.shape[0]))\n    for i in range(overlaps.shape[1]):\n        box2 = boxes2[i]\n        overlaps[:, i] = compute_iou(box2, boxes1, area2[i], area1)\n    return overlaps\n\n", "CodeBleu_score": 62.2853282032944}]}
{"instance_id": "597533722ce3", "generation_result": [{"code": "def load_image(self, image_id):\n\t\n    image_info = self.image_info[image_id]\n    image = imread(image_info['path'])\n    if image.ndim == 2:\n        image = gray2rgb(image)\n    elif image.shape[2] == 4:\n        image = image[:, :, :3]\n    return image\n\n", "CodeBleu_score": 50.02561069555366}, {"code": "def load_image(self, image_id):\n\t\n    image_info = self.image_info[image_id]\n    image = imread(image_info['path'])\n    # If grayscale. Convert to RGB for consistency.\n    if image.ndim!= 3:\n        image = gray2rgb(image)\n    # If has an alpha channel, remove it for consistency\n    if image.shape[-1] == 4:\n        image = image[..., :3]\n    return image\n", "CodeBleu_score": 66.84333098078051}, {"code": "def load_image(self, image_id):\n\t\n    info = self.image_info[image_id]\n    path = info['path']\n    image = read_image(path)\n    if image.ndim == 3 and image.shape[2] == 4:\n        image = image[:, :, :3]\n    if image.ndim == 2:\n        image = gray2rgb(image)\n    return image\n", "CodeBleu_score": 51.315791542913736}]}
{"instance_id": "9b852997ca05", "generation_result": [{"code": "def display_images(images, titles=None, cols=4, cmap=None, norm=None,\n                   interpolation=None)\n\t\n    titles = titles if titles is not None else [\"\"] * len(images)\n    rows = int(math.ceil(len(images) / float(cols)))\n    fig = plt.figure(figsize=(1.8 * cols, 1.4 * rows))\n    fig.subplots_adjust(hspace=0.4, wspace=0.4)\n    for i, (image, title) in enumerate(zip(images, titles)):\n        ax = fig.add_subplot(rows, cols, i + 1, xticks=[], yticks=[])\n        ax.imshow(image, cmap=cmap, norm=norm, interpolation=interpolation)\n        ax.set_title(title, fontsize=10)\n    plt.show()\n\n", "CodeBleu_score": 49.26115665674981}, {"code": "def display_images(images, titles=None, cols=4, cmap=None, norm=None,\n                   interpolation=None)\n\t\n    titles = titles if titles is not None else [\"\"] * len(images)\n    rows = int(math.ceil(len(images) / float(cols)))\n    fig = plt.figure()\n    for i, image in enumerate(images):\n        ax = fig.add_subplot(rows, cols, i + 1)\n        ax.imshow(image, cmap=cmap, norm=norm, interpolation=interpolation)\n        ax.set_title(titles[i])\n    fig.set_tight_layout(True)\n    plt.show()\n", "CodeBleu_score": 38.94362247366766}, {"code": "def display_images(images, titles=None, cols=4, cmap=None, norm=None,\n                   interpolation=None)\n\t\n    titles = titles if titles is not None else [\"\"] * len(images)\n    rows = int(np.ceil(len(images) / float(cols)))\n    fig = plt.figure()\n    for i, image in enumerate(images):\n        plt.subplot(rows, cols, i + 1)\n        plt.imshow(image, cmap=cmap, norm=norm, interpolation=interpolation)\n        plt.axis('off')\n        plt.title(titles[i])\n    fig.set_size_inches(np.array(fig.get_size_inches()) * 2)\n    plt.show()\n", "CodeBleu_score": 41.49233755810249}]}
