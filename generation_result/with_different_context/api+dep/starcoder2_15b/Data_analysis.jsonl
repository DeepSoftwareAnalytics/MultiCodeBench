{"instance_id": "58936b2a621b", "generation_result": [{"code": "def set_model_params(model: LogisticRegression, params: NDArrays) -> LogisticRegression\n\t\n    model.coef_ = params[0]\n    if model.fit_intercept:\n        model.intercept_ = params[1]\n    return model\n\n", "CodeBleu_score": 83.87646776544821}, {"code": "def set_model_params(model: LogisticRegression, params: NDArrays) -> LogisticRegression\n\t\n    model.coef_ = params[0]\n    if model.fit_intercept:\n        model.intercept_ = params[1]\n    return model\n", "CodeBleu_score": 83.87646776544821}, {"code": "def set_model_params(model: LogisticRegression, params: NDArrays) -> LogisticRegression\n\t\n    model.coef_ = params[0]\n    if model.fit_intercept:\n        model.intercept_ = params[1]\n    return model\n", "CodeBleu_score": 83.87646776544821}]}
{"instance_id": "83b142d9d251", "generation_result": [{"code": "def support_query_split(\n    data,\n    label,\n    support_ratio: float,\n) -> Tuple[List, List, List, List]\n\t\n    x_train, x_test, y_train, y_test = train_test_split(\n        data,\n        label,\n        test_size=support_ratio,\n        random_state=42,\n        stratify=label,\n    )\n    return x_train, x_test, y_train, y_test\n\n", "CodeBleu_score": 65.28958649903562}, {"code": "def support_query_split(\n    data,\n    label,\n    support_ratio: float,\n) -> Tuple[List, List, List, List]\n\t\n    x_train, x_test, y_train, y_test = train_test_split(\n        data,\n        label,\n        stratify=label,\n        test_size=support_ratio,\n        random_state=42,\n    )\n    return x_train, x_test, y_train, y_test\n\n", "CodeBleu_score": 67.46349954251389}, {"code": "def support_query_split(\n    data,\n    label,\n    support_ratio: float,\n) -> Tuple[List, List, List, List]\n\t\n    x_train, x_test, y_train, y_test = train_test_split(\n        data, label, test_size=support_ratio, random_state=42, stratify=label\n    )\n    return x_train, x_test, y_train, y_test\n\n", "CodeBleu_score": 68.12968840831522}]}
{"instance_id": "5fd02b540f61", "generation_result": [{"code": "def load_data() -> (\n    Tuple[List[np.ndarray], List[np.ndarray], List[np.ndarray], List[np.ndarray]]\n)\n\t\n    X, y = make_regression(n_features=3, random_state=42)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n    return X_train, y_train, X_test, y_test\n\n", "CodeBleu_score": 46.38918226352721}, {"code": "def load_data() -> (\n    Tuple[List[np.ndarray], List[np.ndarray], List[np.ndarray], List[np.ndarray]]\n)\n\t\n    X, y = make_regression(n_features=3, random_state=42)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n    return X_train, y_train, X_test, y_test\n\n", "CodeBleu_score": 47.27525702263863}, {"code": "def load_data() -> (\n    Tuple[List[np.ndarray], List[np.ndarray], List[np.ndarray], List[np.ndarray]]\n)\n\t\n    X, y = make_regression(n_features=3, random_state=42)\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=42\n    )\n    return X_train, y_train, X_test, y_test\n\n", "CodeBleu_score": 43.90183930658108}]}
{"instance_id": "ad958cb31a57", "generation_result": [{"code": "def ndarrays_to_gmmparam(ndarrays: NDArray) -> GMMParameters\n\t\n    label = ndarrays[0]\n    means = ndarrays[1]\n    weights = ndarrays[2]\n    covariances = ndarrays[3]\n    num_samples = ndarrays[4]\n    return GMMParameters(label, means, weights, covariances, num_samples)\n", "CodeBleu_score": 40.08951228295815}, {"code": "def ndarrays_to_gmmparam(ndarrays: NDArray) -> GMMParameters\n\t\n    return GMMParameters(\n        label=ndarrays[0],\n        means=ndarrays[1],\n        weights=ndarrays[2],\n        covariances=ndarrays[3],\n        num_samples=ndarrays[4],\n    )\n\n", "CodeBleu_score": 79.03672928896962}, {"code": "def ndarrays_to_gmmparam(ndarrays: NDArray) -> GMMParameters\n\t\n    return GMMParameters(ndarrays[0], ndarrays[1], ndarrays[2], ndarrays[3], ndarrays[4])\n", "CodeBleu_score": 28.988763610480394}]}
{"instance_id": "061d5a5b9d56", "generation_result": [{"code": "def datafiles_fusion(data_paths):\n\t\n    X, Y = load_svmlight_file(data_paths[0])\n    X = X.toarray()\n    for i in range(1, len(data_paths)):\n        X_i, Y_i = load_svmlight_file(data_paths[i], n_features=X.shape[1])\n        X = np.concatenate((X, X_i.toarray()), axis=0)\n        Y = np.concatenate((Y, Y_i), axis=0)\n    return X, Y\n", "CodeBleu_score": 43.76730663239975}, {"code": "def datafiles_fusion(data_paths):\n\t\n    X, Y = load_svmlight_file(data_paths[0])\n    X = X.toarray()\n    for i in range(1, len(data_paths)):\n        X_new, Y_new = load_svmlight_file(data_paths[i])\n        X_new = X_new.toarray()\n        X = np.concatenate((X, X_new), axis=0)\n        Y = np.concatenate((Y, Y_new), axis=0)\n    return X, Y\n", "CodeBleu_score": 41.14428044688519}, {"code": "def datafiles_fusion(data_paths):\n\t\n    X, Y = load_svmlight_file(data_paths[0])\n    X = X.toarray()\n    for path in data_paths[1:]:\n        X_new, Y_new = load_svmlight_file(path)\n        if X_new.shape[1]!= X.shape[1]:\n            raise ValueError(\"Number of features in data files must match\")\n        X = np.concatenate((X, X_new.toarray()), axis=0)\n        Y = np.concatenate((Y, Y_new), axis=0)\n    return X, Y\n", "CodeBleu_score": 38.62814754561389}]}
{"instance_id": "977d3bd5d5a3", "generation_result": [{"code": "def eval_learning(y_test, y_pred)\n\t\n    accuracy = accuracy_score(y_test, y_pred)\n    recall = recall_score(y_test, y_pred, average=\"micro\")\n    precision = precision_score(y_test, y_pred, average=\"micro\")\n    f1 = f1_score(y_test, y_pred, average=\"micro\")\n    return accuracy, recall, precision, f1\n", "CodeBleu_score": 40.38465881740944}, {"code": "def eval_learning(y_test, y_pred)\n\t\n    accuracy = accuracy_score(y_test, y_pred)\n    recall = recall_score(y_test, y_pred, average=\"micro\")\n    precision = precision_score(y_test, y_pred, average=\"micro\")\n    f1 = f1_score(y_test, y_pred, average=\"micro\")\n    return accuracy, recall, precision, f1\n\n", "CodeBleu_score": 40.38465881740944}, {"code": "def eval_learning(y_test, y_pred)\n\t\n    accuracy = accuracy_score(y_test, y_pred)\n    recall = recall_score(y_test, y_pred, average=\"micro\")\n    precision = precision_score(y_test, y_pred, average=\"micro\")\n    f1 = f1_score(y_test, y_pred, average=\"micro\")\n    return accuracy, recall, precision, f1\n", "CodeBleu_score": 40.38465881740944}]}
{"instance_id": "f4c838f1f092", "generation_result": [{"code": "def train(model, train_loader, num_epochs=1)\n\t\n    criterion = nn.BCELoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n    model.train()\n    for epoch in range(num_epochs):\n        for X_batch, y_batch in train_loader:\n            optimizer.zero_grad()\n            outputs = model(X_batch)\n            loss = criterion(outputs, y_batch)\n            loss.backward()\n            optimizer.step()\n\n", "CodeBleu_score": 72.89324029568995}, {"code": "def train(model, train_loader, num_epochs=1)\n\t\n    criterion = nn.BCELoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n    model.train()\n    for epoch in range(num_epochs):\n        for X_batch, y_batch in train_loader:\n            optimizer.zero_grad()\n            outputs = model(X_batch)\n            loss = criterion(outputs, y_batch)\n            loss.backward()\n            optimizer.step()\n\n", "CodeBleu_score": 72.89324029568995}, {"code": "def train(model, train_loader, num_epochs=1)\n\t\n    criterion = nn.BCELoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n    model.train()\n    for epoch in range(num_epochs):\n        for X_batch, y_batch in train_loader:\n            optimizer.zero_grad()\n            outputs = model(X_batch)\n            loss = criterion(outputs, y_batch)\n            loss.backward()\n            optimizer.step()\n\n", "CodeBleu_score": 72.89324029568995}]}
{"instance_id": "35068c779c02", "generation_result": [{"code": "def _load_libsvm_dataset(cfg: DictConfig) -> Dataset\n\t\n    if cfg.dataset_type!= \"LIBSVM\":\n        raise RuntimeError(f\"Dataset type {cfg.dataset_type} is not supported\")\n    data, labels = load_svmlight_file(cfg.dataset_path)\n    data = data.toarray().astype(np.float32)\n    labels = labels.astype(np.float32)\n    if cfg.dataset_name == \"MUSHROOMS\":\n        labels[labels!= 1] = 0\n    else:\n        raise RuntimeError(f\"Dataset {cfg.dataset_name} is not supported\")\n    print(f\"Labels: {np.unique(labels, return_counts=True)}\")\n    print(f\"Data shape: {data.shape}\")\n    return data_utils.TensorDataset(torch.Tensor(data), torch.Tensor(labels))\n\n", "CodeBleu_score": 33.579246825958045}, {"code": "def _load_libsvm_dataset(cfg: DictConfig) -> Dataset\n\t\n    assert cfg.type == \"LIBSVM\", f\"Expected LIBSVM dataset, got {cfg.type}\"\n    dataset_path = os.path.join(cfg.root, cfg.name)\n    data, labels = load_svmlight_file(dataset_path)\n    data = data.toarray().astype(np.float32)\n    labels = labels.astype(np.int64)\n    print(f\"Unique labels: {np.unique(labels, return_counts=True)}\")\n    print(f\"Data shape: {data.shape}\")\n    if cfg.name == \"mushrooms\":\n        labels[labels!= 1] = 0\n        labels[labels == 1] = 1\n    else:\n        raise RuntimeError(f\"Unknown dataset name: {cfg.name}\")\n    return data_utils.TensorDataset(torch.Tensor(data), torch.Tensor(labels))\n\n", "CodeBleu_score": 31.803863487587957}, {"code": "def _load_libsvm_dataset(cfg: DictConfig) -> Dataset\n\t\n    if cfg.dataset_type!= \"LIBSVM\":\n        raise RuntimeError(f\"Dataset type {cfg.dataset_type} is not supported.\")\n    dataset_path = os.path.join(cfg.dataset_path, cfg.dataset_name)\n    data, labels = load_svmlight_file(dataset_path)\n    data = data.toarray().astype(np.float32)\n    labels = labels.astype(np.float32)\n    print(\"Labels:\", np.unique(labels, return_counts=True))\n    print(\"Data shape:\", data.shape)\n    if cfg.dataset_name == \"MUSHROOMS\":\n        labels[labels == 1] = 0\n        labels[labels!= 0] = 1\n    else:\n        raise RuntimeError(f\"Dataset {cfg.dataset_name} is not supported.\")\n    return data_utils.TensorDataset(torch.Tensor(data), torch.Tensor(labels))\n\n", "CodeBleu_score": 41.38201879726857}]}
{"instance_id": "d602a96e571e", "generation_result": [{"code": "def _fit_transform_one(transformer, X, y, weight, **fit_params)\n\t    if hasattr(transformer, 'fit_transform'):\n        res = transformer.fit_transform(X, y, **fit_params)\n    else:\n        res = transformer.fit(X, y, **fit_params).transform(X)\n    if weight is None:\n        return res, transformer\n    return res * weight, transformer\n\n", "CodeBleu_score": 91.4137695924232}, {"code": "def _fit_transform_one(transformer, X, y, weight, **fit_params)\n\t    if hasattr(transformer, 'fit_transform'):\n        res = transformer.fit_transform(X, y, **fit_params)\n    else:\n        res = transformer.fit(X, y, **fit_params).transform(X)\n    # multiply result by a weight\n    if weight is None:\n        return res, transformer\n    return res * weight, transformer\n\n", "CodeBleu_score": 91.4137695924232}, {"code": "def _fit_transform_one(transformer, X, y, weight, **fit_params)\n\t    if hasattr(transformer, \"fit_transform\"):\n        res = transformer.fit_transform(X, y, **fit_params)\n    else:\n        res = transformer.fit(X, y, **fit_params).transform(X)\n    # add.copy() if sparse output\n    if weight is None:\n        return res, transformer\n    res *= weight\n    return res, transformer\n\n", "CodeBleu_score": 79.77230164425028}]}
{"instance_id": "fcf88a773fd8", "generation_result": [{"code": "def _transform_selected(X, transform, selected=\"all\", copy=True)\n\t\n    if isinstance(selected, str) and selected == \"all\":\n        return transform(X, copy=copy)\n\n    if isinstance(selected, (int, np.integer)):\n        selected = [selected]\n\n    if isinstance(selected, slice):\n        selected = np.arange(X.shape[1])[selected]\n\n    if not isinstance(selected, (list, np.ndarray)):\n        raise ValueError(\"Invalid value for'selected' parameter. \"\n                         \"Supported values are: 'all', slice, list/array of \"\n                         \"integers, or integer. Got %s.\" % selected)\n\n    if not np.issubdtype(type(selected[0]), np.integer):\n        raise ValueError(\"Invalid value for'selected' parameter. \"\n                         \"Supported values are: 'all', slice, list/array of \"\n                         \"integers, or integer. Got %s.\" % selected)\n\n    if len(selected) == 0:\n        return X\n\n    if isinstance(X, sparse.spmatrix):\n        if sparse.isspmatrix_csc(X):\n            X_sparse_format = \"csc\"\n        else:\n            X_sparse_format = \"csr\"\n\n        X_selected = X[:, selected]\n        X_other = X[:, np.logical_not(np.in1d(np.arange(X.shape[1]), selected))]\n    else:\n        X_selected = X[:, selected]\n        X_other = X[:, np.logical_not(np.in1d(np.arange(X.shape[1]), selected))]\n\n    X_selected_transformed = transform(X_selected, copy=copy)\n\n    if isinstance(X, sparse.spmatrix):\n        X_out = sparse.hstack((X_selected_transformed, X_other),\n                              format=X_sparse_format)\n    else:\n        X_out = np.hstack((X_selected_transformed, X_other))\n\n    return X_out", "CodeBleu_score": 40.67417372290796}, {"code": "def _transform_selected(X, transform, selected=\"all\", copy=True)\n\t\n    # check and ensure that X is a valid array, accepting sparse 'csc' format if specified\n    X = check_array(X, accept_sparse='csc' if sparse.issparse(transform) else None, copy=copy)\n    # if all features are to be transformed, apply the transformation to the entire array\n    if selected == \"all\":\n        return transform.transform(X)\n    # if no features are selected, return X unchanged\n    elif selected is None:\n        return X\n    # if specific features are selected, identify the selected and non-selected features, apply the transformation to the selected features, and then combine the transformed and non-transformed features\n    else:\n        # identify the selected and non-selected features\n        selected = np.asarray(selected)\n        mask = np.in1d(np.arange(X.shape[1]), selected).reshape((-1, 1))\n        # apply the transformation to the selected features\n        X_selected = transform.transform(X[:, selected])\n        # combine the transformed and non-transformed features\n        if sparse.issparse(X_selected):\n            X_selected = sparse.hstack((X_selected, X[:, ~mask]))\n        else:\n            X_selected = np.hstack((X_selected, X[:, ~mask]))\n        return X_selected", "CodeBleu_score": 40.516298224253276}, {"code": "def _transform_selected(X, transform, selected=\"all\", copy=True)\n\t\n    X = check_array(X, accept_sparse='csc' if copy else ['csr', 'csc'])\n\n    if selected == \"all\":\n        return transform(X)\n    else:\n        # If some features are selected, we need to transform the selected\n        # features, and then combine the transformed and non-transformed\n        # features.\n        selected = np.asarray(selected)\n        if len(selected) == 0:\n            return X\n        mask = np.in1d(np.arange(X.shape[1]), selected)\n        mask_not = np.logical_not(mask)\n        X_sel = transform(X[:, mask])\n        X_not_sel = X[:, mask_not]\n        if sparse.issparse(X_sel):\n            return sparse.hstack([X_sel, X_not_sel]).tocsr()\n        else:\n            return np.hstack([X_sel, X_not_sel])\n\n", "CodeBleu_score": 40.57377360586894}]}
{"instance_id": "c9dfb00047d1", "generation_result": [{"code": "def _transform_new(self, X)\n\t\n    X_temp = check_array(X, dtype=None)\n    if not hasattr(X, 'dtype') and np.issubdtype(X_temp.dtype, np.str_):\n        X = check_array(X, dtype=np.object)\n    else:\n        X = X_temp\n\n    _, n_features = X.shape\n    X_int = np.zeros_like(X, dtype=np.int)\n    X_mask = np.ones_like(X, dtype=np.bool)\n\n    for i in range(n_features):\n        Xi = X[:, i]\n        valid_mask = np.in1d(Xi, self.categories_[i])\n\n        if not np.all(valid_mask):\n            if handle_unknown == 'error':\n                diff = np.unique(X[~valid_mask, i])\n                msg = (\"Found unknown categories {0} in column {1}\"\n                       \" during transform\".format(diff, i))\n                raise ValueError(msg)\n            else:\n                # Set the problematic rows to an acceptable value and\n                # continue `The rows are marked `X_mask` and will be\n                # removed later.\n                X_mask[:, i] = valid_mask\n                Xi = Xi.copy()\n                Xi[~valid_mask] = self.categories_[i][0]\n        X_int[:, i] = self._label_encoders_[i].transform(Xi)\n\n    if self.encoding == 'ordinal':\n        return X_int.astype(self.dtype, copy=False)\n\n    mask = X_mask.ravel()\n    n_values = [cats.shape[0] for cats in self.categories_]\n    n_values = np.array([0] + n_values)\n    indices = np.cumsum(n_values)\n\n    column_indices = (X_int + indices[:-1]).ravel()[mask]\n    row_indices = np.repeat(np.arange(n_samples, dtype=np.int32),\n                            n_features)[mask]\n    data = np.ones(n_samples * n_features)[mask]\n\n    out = sparse.csr_matrix((data, (row_indices, column_indices)),\n                            shape=(n_samples, indices[-1]),\n                            dtype=self.dtype)\n    if self.encoding == 'onehot-dense':\n       ", "CodeBleu_score": 53.78311183775802}, {"code": "def _transform_new(self, X)\n\t\n    X_temp = check_array(X, dtype=None)\n    if not hasattr(X, 'dtype') and np.issubdtype(X_temp.dtype, np.str_):\n        X = check_array(X, dtype=np.object)\n    else:\n        X = X_temp\n\n    _, n_features = X.shape\n    X_int = np.zeros_like(X, dtype=np.int)\n    X_mask = np.ones_like(X, dtype=np.bool)\n\n    for i in range(n_features):\n        Xi = X[:, i]\n        valid_mask = np.in1d(Xi, self.categories_[i])\n\n        if not np.all(valid_mask):\n            if handle_unknown == 'error':\n                diff = np.unique(X[~valid_mask, i])\n                msg = (\"Found unknown categories {0} in column {1}\"\n                       \" during transform\".format(diff, i))\n                raise ValueError(msg)\n            else:\n                # Set the problematic rows to an acceptable value and\n                # continue `The rows are marked `X_mask` and will be\n                # removed later.\n                X_mask[:, i] = valid_mask\n                Xi = Xi.copy()\n                Xi[~valid_mask] = self.categories_[i][0]\n        X_int[:, i] = self._label_encoders_[i].transform(Xi)\n\n    n_values = [cats.shape[0] for cats in self.categories_]\n    n_values = np.array([0] + n_values)\n    indices = np.cumsum(n_values)\n\n    column_indices = (X_int + indices[:-1]).ravel()\n    row_indices = np.repeat(np.arange(n_samples, dtype=np.int32),\n                            n_features)\n    data = np.ones(n_samples * n_features)\n\n    out = sparse.csr_matrix((data, (row_indices, column_indices)),\n                            shape=(n_samples, indices[-1]),\n                            dtype=self.dtype).tocsr()\n    if self.encoding == 'onehot-dense':\n        return out.toarray()\n    else:\n        return out\n\n", "CodeBleu_score": 53.29226298677525}, {"code": "def _transform_new(self, X)\n\t\n    X_temp = check_array(X, dtype=None)\n    if not hasattr(X, 'dtype') and np.issubdtype(X_temp.dtype, np.str_):\n        X = check_array(X, dtype=np.object)\n    else:\n        X = X_temp\n\n    _, n_features = X.shape\n    X_int = np.zeros_like(X, dtype=np.int)\n    X_mask = np.ones_like(X, dtype=np.bool)\n\n    for i in range(n_features):\n        Xi = X[:, i]\n        valid_mask = np.in1d(Xi, self.categories_[i])\n\n        if not np.all(valid_mask):\n            if self.handle_unknown == 'error':\n                diff = np.unique(X[~valid_mask, i])\n                msg = (\"Found unknown categories {0} in column {1}\"\n                       \" during transform\".format(diff, i))\n                raise ValueError(msg)\n            else:\n                # Set the problematic rows to an acceptable value and\n                # continue `The rows are marked `X_mask` and will be\n                # removed later.\n                X_mask[:, i] = valid_mask\n                Xi = Xi.copy()\n                Xi[~valid_mask] = self.categories_[i][0]\n        X_int[:, i] = self._label_encoders_[i].transform(Xi)\n\n    if self.encoding == 'ordinal':\n        return X_int.astype(self.dtype, copy=False)\n\n    mask = X_mask.ravel()\n    n_values = [cats.shape[0] for cats in self.categories_]\n    n_values = np.array([0] + n_values)\n    indices = np.cumsum(n_values)\n\n    column_indices = (X_int + indices[:-1]).ravel()[mask]\n    row_indices = np.repeat(np.arange(n_samples, dtype=np.int32),\n                            n_features)[mask]\n    data = np.ones(n_samples * n_features)[mask]\n\n    out = sparse.csr_matrix((data, (row_indices, column_indices)),\n                            shape=(n_samples, indices[-1]),\n                            dtype=self.dtype)\n    if self.encoding == 'onehot-dense", "CodeBleu_score": 53.77168782531211}]}
{"instance_id": "fcde391a8972", "generation_result": [{"code": "def convert_to_sklearn(self)\n\t\n    from scipy.stats import loguniform, uniform\n    if self.log:\n        return loguniform(self.lower_bound, self.upper_bound)\n    else:\n        return uniform(self.lower_bound, self.upper_bound - self.lower_bound)", "CodeBleu_score": 45.79449123378524}, {"code": "def convert_to_sklearn(self)\n\t\n    if self.log:\n        return loguniform(self.lower, self.upper)\n    else:\n        return uniform(self.lower, self.upper - self.lower)\n", "CodeBleu_score": 30.997576913458495}, {"code": "def convert_to_sklearn(self)\n\t\n    from scipy.stats import loguniform, uniform\n    if self.log:\n        return loguniform(self.lower, self.upper)\n    return uniform(self.lower, self.upper)", "CodeBleu_score": 42.441125290935446}]}
{"instance_id": "5d5413adcca6", "generation_result": [{"code": "def _render(self, state: AnalysisState) -> None\n\t\n    additional_kwargs, fig_args, features = self._get_args()\n    if self.headers:\n        self.render_header_if_needed(state)\n    fig, axs = plt.subplots(**fig_args)\n    axs.ravel\n    notna\n    plt.show\n    PartialDependenceDisplay.from_estimator\n    len\n    plt.tight_layout\n    self.render_header_if_needed\n    _SklearnAutoGluonWrapper\n    self._get_args\n\n", "CodeBleu_score": 27.799602991269357}, {"code": "def _render(self, state: AnalysisState) -> None\n\t\n    additional_kwargs, fig_args, features = self._get_args()\n    if self.header:\n        self.render_header_if_needed(state)\n\n    fig, axs = plt.subplots(**fig_args)\n    if self.two_way:\n        notna = state.data[features[0]].notna() & state.data[features[1]].notna()\n        state.data = state.data[notna]\n    pd_line_kw = additional_kwargs.get(\"pd_line_kw\", {})\n    ice_lines_kw = additional_kwargs.get(\"ice_lines_kw\", {})\n    pd_line_kw.update(dict(label=\"PDP\"))\n    ice_lines_kw.update(dict(label=\"ICE\"))\n    additional_kwargs[\"pd_line_kw\"] = pd_line_kw\n    additional_kwargs[\"ice_lines_kw\"] = ice_lines_kw\n    PartialDependenceDisplay.from_estimator(\n        self.model,\n        state.data[features],\n        ax=axs,\n        **additional_kwargs,\n    )\n    plt.tight_layout()\n    plt.show()\n\n", "CodeBleu_score": 46.06782397955712}, {"code": "def _render(self, state: AnalysisState) -> None\n\t\n    additional_kwargs, fig_args, features = self._get_args()\n    if self.header:\n        self.render_header_if_needed(state)\n    fig, axs = plt.subplots(**fig_args)\n    axs.ravel\n    notna = state.data[features].notna()\n    PartialDependenceDisplay.from_estimator(\n        self.model,\n        state.data[features],\n        ax=axs,\n        feature_names=features,\n        **{**additional_kwargs, **self.pd_line_kw},\n    )\n    if self.two_way:\n        PartialDependenceDisplay.from_estimator(\n            self.model,\n            state.data[features],\n            ax=axs[1],\n            feature_names=features,\n            **{**additional_kwargs, **self.ice_lines_kw},\n        )\n        PartialDependenceDisplay.from_estimator(\n            self.model,\n            state.data[features],\n            ax=axs[2],\n            feature_names=features,\n            **{**additional_kwargs, **self.pd_line_kw, **self.ice_lines_kw},\n        )\n    plt.tight_layout()\n    plt.show()\n\n", "CodeBleu_score": 41.67445979963318}]}
{"instance_id": "db8aac29180b", "generation_result": [{"code": "def _preprocess(self, X, **kwargs)\n\t\n    X = super()._preprocess(X, **kwargs)\n    if self._feature_generator is None:\n        self._feature_generator = LabelEncoderFeatureGenerator(verbosity=0)\n        self._feature_generator.fit(X=X)\n    if self._feature_generator.features_in:\n        X = X.copy()\n        X[self._feature_generator.features_in] = self._feature_generator.transform(X=X)\n    X = X.fillna(0).to_numpy(dtype=np.float32)\n    return X\n\n", "CodeBleu_score": 79.5391784405486}, {"code": "def _preprocess(self, X, **kwargs)\n\t\n    X = super()._preprocess(X, **kwargs)\n    if self._feature_generator is None:\n        self._feature_generator = LabelEncoderFeatureGenerator(verbosity=0)\n        self._feature_generator.fit(X=X)\n    if self._feature_generator.features_in:\n        X = X.copy()\n        X[self._feature_generator.features_in] = self._feature_generator.transform(X=X)\n    X = X.fillna(0).to_numpy(dtype=np.float32)\n    return X\n\n", "CodeBleu_score": 79.5391784405486}, {"code": "def _preprocess(self, X, **kwargs)\n\t\n    X = super()._preprocess(X, **kwargs)\n    if self._feature_generator is None:\n        self._feature_generator = LabelEncoderFeatureGenerator(verbosity=0)\n        self._feature_generator.fit(X=X)\n    if self._feature_generator.features_in:\n        X = X.copy()\n        X[self._feature_generator.features_in] = self._feature_generator.transform(X=X)\n    X = X.fillna(0).to_numpy(dtype=np.float32)\n    return X\n", "CodeBleu_score": 79.5391784405486}]}
{"instance_id": "d4eb45e6d013", "generation_result": [{"code": "def _estimate_memory_usage(self, X, **kwargs)\n\t\n    model_params = self._get_model_params()\n    num_trees_per_estimator = self._get_num_trees_per_estimator()\n    min_estimators = max(40, num_trees_per_estimator) if self.search_space is None else 40\n    bytes_per_estimator = (X.memory_usage(deep=True).sum() / X.shape[0]) * 1.5\n    return bytes_per_estimator * min_estimators\n", "CodeBleu_score": 17.617645799871724}, {"code": "def _estimate_memory_usage(self, X, **kwargs)\n\t\n    # Get model parameters\n    model_params = self._get_model_params()\n    # Get the final number of estimators\n    num_estimators = model_params.get(\"n_estimators\")\n    # Set a minimum of 40 estimators if the final number is less than 40 or if a search space is defined\n    if num_estimators is None or num_estimators < 40 or self.search_space is not None:\n        num_estimators = 40\n    # Get the number of trees per estimator\n    num_trees_per_estimator = self._get_num_trees_per_estimator()\n    # Calculate the bytes used per estimator based on the size of X, with an adjustment factor\n    bytes_per_estimator = 2 * X.shape[0] * X.shape[1] * 8 * 1.1\n    # Calculate the expected minimum memory usage\n    min_memory_usage = bytes_per_estimator * num_estimators\n    return min_memory_usage\n\n", "CodeBleu_score": 43.33291873141087}, {"code": "def _estimate_memory_usage(self, X, **kwargs)\n\t\n    model_params = self._get_model_params()\n    num_trees_per_estimator = self._get_num_trees_per_estimator()\n    num_estimators = max(40, len(model_params.get(\"search_space\", {})))\n    bytes_per_estimator = (X.shape[0] * X.shape[1] * 8) / (num_trees_per_estimator * 100)\n    return num_estimators * bytes_per_estimator\n\n", "CodeBleu_score": 18.512450152840355}]}
{"instance_id": "6923cb007f4c", "generation_result": [{"code": "def downscale_vectorizer(vectorizer, ngram_freq, vocab_size)\n\t\n    # Create a Counter object from the n-gram frequency data\n    counter = Counter(ngram_freq)\n    # Identify the top n most common n-grams according to the specified vocabulary size\n    top_n_grams = counter.most_common(vocab_size)\n    # Sort the names of the top n n-grams\n    sorted_top_n_grams = sorted([ngram[0] for ngram in top_n_grams])\n    # Create a new vocabulary dictionary, mapping each n-gram to a unique index\n    new_vocab = {ngram: idx for idx, ngram in enumerate(sorted_top_n_grams)}\n    # Update the vectorizer's vocabulary with the new, downsized vocabulary\n    vectorizer.vocabulary_ = new_vocab\n    return vectorizer\n\n", "CodeBleu_score": 36.00638945412816}, {"code": "def downscale_vectorizer(vectorizer, ngram_freq, vocab_size)\n\t\n    ngram_counter = Counter(ngram_freq)\n    top_ngrams = sorted(ngram_counter.most_common(vocab_size))\n    new_vocab = {ngram: i for i, (ngram, _) in enumerate(top_ngrams)}\n    vectorizer.vocabulary_ = new_vocab\n    return vectorizer\n", "CodeBleu_score": 33.148360458842}, {"code": "def downscale_vectorizer(vectorizer, ngram_freq, vocab_size)\n\t\n    # Create a Counter object from the n-gram frequency data\n    counter = Counter(ngram_freq)\n\n    # Identify the top n most common n-grams according to the specified vocabulary size\n    top_n = counter.most_common(vocab_size)\n\n    # Sort the names of the top n n-grams\n    sorted_names = sorted([name for name, count in top_n])\n\n    # Create a new vocabulary dictionary, mapping each n-gram to a unique index\n    new_vocab = {name: index for index, name in enumerate(sorted_names)}\n\n    # Update the vectorizer's vocabulary with the new, downsized vocabulary\n    vectorizer.vocabulary_ = new_vocab\n\n    return vectorizer\n\n", "CodeBleu_score": 49.21984177425214}]}
{"instance_id": "0e837b2d8088", "generation_result": [{"code": " def _select_continuous(self, df, features)\n\t\n    feature_types = defaultdict(list)\n    skew_threshold = self.params.get(\"skew_threshold\", 0.5)\n    for feature in features:\n        if df[feature].dtype.kind in [\"i\", \"f\"]:\n            skew = df[feature].skew()\n            if np.abs(skew) > skew_threshold:\n                feature_types[\"skewed\"].append(feature)\n            else:\n                feature_types[\"continuous\"].append(feature)\n        else:\n            feature_types[\"categorical\"].append(feature)\n    return feature_types\n", "CodeBleu_score": 47.89369618716551}, {"code": " def _select_continuous(self, df, features)\n\t\n    # Initialize a dictionary to store feature types\n    feature_types = defaultdict(list)\n    # Get the skewness threshold from the parameters\n    skewness_threshold = self.parameters[\"skewness_threshold\"]\n    # For each feature, calculate the skewness and compare it to the threshold\n    for feature in features:\n        skewness = skew(df[feature])\n        if abs(skewness) > skewness_threshold:\n            feature_types[\"skewed\"].append(feature)\n        else:\n            feature_types[\"continuous\"].append(feature)\n    return feature_types\n", "CodeBleu_score": 36.68239191815304}, {"code": " def _select_continuous(self, df, features)\n\t\n    skew_dict = defaultdict(list)\n    skew_threshold = self.parameters[\"skew_threshold\"]\n    for feature in features:\n        skew = df[feature].skew()\n        if skew > skew_threshold:\n            skew_dict[\"skewed\"].append(feature)\n        else:\n            skew_dict[\"continuous\"].append(feature)\n    return skew_dict\n\n\n", "CodeBleu_score": 36.97562081795328}]}
{"instance_id": "bb569c10f975", "generation_result": [{"code": "def _get_weights(dist, weights):\n\t\n    if weights is None or weights == \"uniform\":\n        return None\n    elif weights == \"distance\":\n        # handle infinite distances\n        inf_mask = np.isinf(dist)\n        finite_dist = np.where(inf_mask, np.inf, dist)\n        # compute inverse of the distances\n        weights = 1.0 / finite_dist\n        # handle infinite weights\n        inf_rows = np.isinf(weights).any(1)\n        weights[inf_rows] = inf_mask[inf_rows]\n        return weights\n    elif callable(weights):\n        # compute weights using the callable function\n        return weights(dist)\n    else:\n        raise ValueError(\"weights should be 'uniform', 'distance', or a callable function\")", "CodeBleu_score": 64.02684521545001}, {"code": "def _get_weights(dist, weights):\n\t\n    if weights == \"uniform\":\n        return None\n    elif weights == \"distance\":\n        # if any distances are zero, set them to 1 to avoid division by zero\n        with np.errstate(divide=\"ignore\"):\n            weights = 1.0 / dist\n        inf_mask = np.isinf(weights)\n        weights[inf_mask] = inf_mask.sum()\n        weights /= weights.sum(axis=1)[:, np.newaxis]\n        return weights\n    elif callable(weights):\n        return weights(dist)\n    else:\n        raise ValueError(\"weights not recognized; should be 'uniform', 'distance', or a callable function\")\n", "CodeBleu_score": 66.40355379289831}, {"code": "def _get_weights(dist, weights):\n\t\n    if weights is None or weights == 'uniform':\n        return None\n    elif weights == 'distance':\n        # if any weights are infinity, replace them with max of the other weights\n        inf_mask = np.isinf(dist)\n        dist[inf_mask] = dist[~inf_mask].max() * 10\n        return 1. / dist\n    elif callable(weights):\n        return weights(dist)\n    else:\n        raise ValueError(\"weights not recognized; should be 'uniform', 'distance', or a callable function\")\n", "CodeBleu_score": 55.300707828618926}]}
{"instance_id": "12aaa9c78402", "generation_result": [{"code": "def predict_loo(self)\n\t\n    X = self._fit_X\n    y = self._y\n    if _y.ndim == 1:\n        y = y.reshape((-1, 1))\n    n_samples = X.shape[0]\n    y_pred = np.empty((n_samples, y.shape[1]))\n    for i in range(n_samples):\n        X_train = np.delete(X, i, axis=0)\n        y_train = np.delete(y, i, axis=0)\n        self.fit(X_train, y_train)\n        y_pred[i] = self.predict(X[i].reshape(1, -1))\n    return y_pred.ravel()\n", "CodeBleu_score": 32.82977928473021}, {"code": "def predict_loo(self)\n\t\n    X = self._fit_X\n    y = self._y\n    n_samples = X.shape[0]\n    y_pred = np.empty((n_samples, 1))\n    for i in range(n_samples):\n        X_train = np.delete(X, i, axis=0)\n        y_train = np.delete(y, i, axis=0)\n        X_test = X[i, :].reshape(1, -1)\n        y_pred[i] = self.predict(X_test, X_train, y_train)\n    return y_pred.ravel()\n", "CodeBleu_score": 21.99322948414877}, {"code": "def predict_loo(self)\n\t\n    X = self._fit_X\n    y = self._y\n    k = self.n_neighbors\n    weights = self.weights\n    if weights not in [\"uniform\", \"distance\"]:\n        raise ValueError(\n            \"weights not recognized, must be either 'uniform' or 'distance'\"\n        )\n    # get the indices and distances of the k-nearest neighbors\n    # for each sample in the training set\n    indices, distances = self.kneighbors(X)\n    # initialize the leave-one-out predictions array\n    y_pred = np.empty(len(y))\n    # loop over each sample in the training set\n    for i in range(len(y)):\n        # get the indices and distances of the k-nearest neighbors\n        # excluding the current sample\n        indices_i = indices[i, 1:]\n        distances_i = distances[i, 1:]\n        # get the target values of the k-nearest neighbors\n        y_neighbors = y[indices_i]\n        # get the weights of the k-nearest neighbors\n        if weights == \"uniform\":\n            weights_i = np.ones_like(distances_i)\n        else:\n            weights_i = 1.0 / distances_i\n        # calculate the weighted prediction for the current sample\n        if y.ndim == 1:\n            y_pred[i] = np.average(y_neighbors, weights=weights_i)\n        else:\n            y_pred[i] = np.average(y_neighbors, axis=0, weights=weights_i)\n    # if the target variable was one-dimensional,\n    # flatten the predictions back to a one-dimensional array\n    if y.ndim == 1:\n        y_pred = y_pred.ravel()\n    return y_pred\n", "CodeBleu_score": 33.47678879310102}]}
{"instance_id": "b316687d9e9b", "generation_result": [{"code": "def isotonic(input_data, quantile_list)\n\t\n    quantile_list = np.array(quantile_list).reshape(-1)\n    batch_size = input_data.shape[0]\n    new_output_data = []\n    for i in range(batch_size):\n        ir = IsotonicRegression(out_of_bounds=\"clip\")\n        ir.fit(quantile_list, input_data[i])\n        new_output_data.append(ir.transform(quantile_list))\n    return np.stack(new_output_data)\n", "CodeBleu_score": 47.24107886524598}, {"code": "def isotonic(input_data, quantile_list)\n\t\n    quantile_list = np.array(quantile_list).reshape(-1)\n    batch_size = input_data.shape[0]\n    new_output_data = []\n    for i in range(batch_size):\n        new_output_data.append(IsotonicRegression(quantile_list).fit_transform(input_data[i]))\n    return np.stack(new_output_data)\n", "CodeBleu_score": 50.01649260808596}, {"code": "def isotonic(input_data, quantile_list)\n\t\n    quantile_list = np.array(quantile_list).reshape(-1)\n    batch_size = input_data.shape[0]\n    new_output_data = []\n    for i in range(batch_size):\n        fit_transform = IsotonicRegression(quantile_list).fit_transform(input_data[i])\n        reshape = fit_transform.reshape(1, -1)\n        new_output_data.append(reshape)\n    return np.stack(new_output_data)\n", "CodeBleu_score": 46.9951682036116}]}
{"instance_id": "17566e088b4e", "generation_result": [{"code": "def _train_vectorizer(text_data: list, vectorizer)\n\t\n    vectorizer.fit(text_data)\n    vectorizer.stop_words_ = None\n    return vectorizer, vectorizer.transform(text_data)\n", "CodeBleu_score": 40.6826747095547}, {"code": "def _train_vectorizer(text_data: list, vectorizer)\n\t\n    # Fit the vectorizer to the text data\n    vectorizer.fit(text_data)\n    # Transform the text data into a matrix representation\n    text_matrix = vectorizer.transform(text_data)\n    # Set the stop_words_ attribute to None to reduce the object size significantly\n    vectorizer.stop_words_ = None\n    return vectorizer, text_matrix\n\n", "CodeBleu_score": 42.56732773185301}, {"code": "def _train_vectorizer(text_data: list, vectorizer)\n\t\n    vectorizer.fit(text_data)\n    vectorizer.stop_words_ = None\n    return vectorizer.transform(text_data)\n\n", "CodeBleu_score": 34.67230403994112}]}
{"instance_id": "be23c0afc2d6", "generation_result": [{"code": "def preprocess(df, test_size, shuffle)\n\t\n    df = df.dropna(axis=0, how='any')\n    df = df.drop(columns=['id', 'days_to_death', 'year_of_death'])\n    n_unique = df.nunique()\n    cols_to_drop = [col for col, count in n_unique.items() if count <= 1]\n    df = df.drop(columns=cols_to_drop)\n    X = df.drop(columns=['overall_survival'])\n    y = df['overall_survival']\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, shuffle=shuffle)\n    return X_train, X_test, y_train, y_test", "CodeBleu_score": 29.85669117730113}, {"code": "def preprocess(df, test_size, shuffle)\n\t\n    # Remove rows with missing entries marked by '--'\n    df = df.replace('--', np.nan)\n    df = df.dropna()\n\n    # Drop columns with unique values less than or equal to one or containing 'id' in their name\n    cols_to_drop = [col for col in df.columns if df[col].nunique() <= 1 or 'id' in col.lower()]\n    df = df.drop(columns=cols_to_drop)\n\n    # Drop shortcut columns like 'days_to_death' and 'year_of_death'\n    shortcut_cols = ['days_to_death', 'year_of_death']\n    df = df.drop(columns=shortcut_cols)\n\n    # Split the dataframe into training and testing sets\n    X = df.drop(columns=['cancer_type'])\n    y = df['cancer_type']\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, shuffle=shuffle)\n\n    return X_train, X_test, y_train, y_test\n\n", "CodeBleu_score": 27.769122866416467}, {"code": "def preprocess(df, test_size, shuffle)\n\t\n    # Remove rows with missing entries marked by '--'\n    df = df[~df.isin(['--'])]\n    df = df.dropna()\n\n    # Drop columns with unique values less than or equal to one\n    n_unique = df.nunique()\n    drop_cols = [col for col, count in n_unique.items() if count <= 1]\n    df = df.drop(columns=drop_cols)\n\n    # Drop columns containing 'id' in their name\n    drop_cols = [col for col in df.columns if 'id' in col]\n    df = df.drop(columns=drop_cols)\n\n    # Drop shortcut columns\n    drop_cols = ['days_to_death', 'year_of_death']\n    df = df.drop(columns=drop_cols)\n\n    # Split the dataframe into training and testing sets\n    X_train, X_test = train_test_split(df, test_size=test_size, shuffle=shuffle)\n\n    return X_train, X_test\n\n", "CodeBleu_score": 33.04308247605331}]}
{"instance_id": "6a82997a8c59", "generation_result": [{"code": "def _test_classifier_iterative_fit(classifier, dataset=\"iris\", sparse=False)\n\t\n    X_train, Y_train, X_test, Y_test = get_dataset(dataset=dataset, make_sparse=sparse)\n\n    search_space = classifier.get_hyperparameter_search_space()\n    default_configuration = search_space.get_default_configuration()\n    classifier.initialize(default_configuration, random_state=1)\n\n    # Iterative fitting\n    for i in range(2):\n        classifier.iterative_fit(X_train, Y_train, refit=True)\n\n    for i in range(2):\n        classifier.iterative_fit(X_train, Y_train, refit=False)\n        classifier.iterative_fit(X_train, Y_train, refit=True)\n\n    # Full fitting\n    while not classifier.configuration_fully_fitted:\n        classifier.iterative_fit(X_train, Y_train, refit=True)\n\n    # Predicting\n    predictions = classifier.predict(X_test)\n\n    return predictions, Y_test, classifier\n\n", "CodeBleu_score": 40.34758424236225}, {"code": "def _test_classifier_iterative_fit(classifier, dataset=\"iris\", sparse=False)\n\t\n    X_train, Y_train, X_test, Y_test = get_dataset(dataset=dataset, make_sparse=sparse)\n\n    search_space = classifier.get_hyperparameter_search_space()\n    default_configuration = search_space.get_default_configuration()\n\n    classifier.initialize(default_configuration, random_state=42)\n    classifier.iterative_fit(X_train, Y_train, refit=True, batch_size=10, iterations=2)\n    classifier.iterative_fit(X_train, Y_train, refit=False, batch_size=10, iterations=2)\n    classifier.iterative_fit(X_train, Y_train, refit=False, batch_size=10, iterations=4)\n    classifier.iterative_fit(X_train, Y_train, refit=False, batch_size=10, iterations=8)\n    classifier.iterative_fit(X_train, Y_train, refit=False, batch_size=10, iterations=16)\n    classifier.iterative_fit(X_train, Y_train, refit=False, batch_size=10, iterations=32)\n    classifier.iterative_fit(X_train, Y_train, refit=False, batch_size=10, iterations=64)\n    classifier.iterative_fit(X_train, Y_train, refit=False, batch_size=10, iterations=128)\n    classifier.iterative_fit(X_train, Y_train, refit=False, batch_size=10, iterations=256)\n    classifier.iterative_fit(X_train, Y_train, refit=False, batch_size=10, iterations=512)\n    classifier.iterative_fit(X_train, Y_train, refit=False, batch_size=10, iterations=1024)\n    classifier.iterative_fit(X_train, Y_train, refit=False, batch_size=10, iterations=2048)\n    classifier.iterative_fit(X_train, Y_train, refit=False, batch_size=10, iterations=4096)", "CodeBleu_score": 22.232325635677487}, {"code": "def _test_classifier_iterative_fit(classifier, dataset=\"iris\", sparse=False)\n\t\n    X_train, Y_train, X_test, Y_test = get_dataset(dataset=dataset, make_sparse=sparse)\n    configuration_space = classifier.get_hyperparameter_search_space()\n    default_configuration = configuration_space.get_default_configuration()\n    classifier = classifier.get_initialized_classifier(default_configuration)\n    classifier.random_state = 42\n    classifier.iterative_fit(X_train, Y_train, 2, refit=True)\n    for i in range(2, 100, 2):\n        classifier.iterative_fit(X_train, Y_train, i)\n        if classifier.configuration_fully_fitted:\n            break\n    predictions = classifier.predict(X_test)\n    return predictions, Y_test, classifier\n\n", "CodeBleu_score": 40.74530157239886}]}
{"instance_id": "d0d8450c4c17", "generation_result": [{"code": "def _load_models(self)\n\t\n    if self._ensemble_class is not None:\n        self._models = self._load_ensemble()\n        self._models_cv = None\n        self._logger.debug(\n            \"Ensemble class defined, loading ensemble with seed {}\".format(self._seed)\n        )\n    else:\n        # if no ensemble is defined, we load the best single model\n        # if the user did not specify a resampling strategy\n        # or the resampling strategy is not a cross-validation\n        # strategy, we cannot load the cross-validated models\n        if (\n            self._resampling_strategy is None\n            or not isinstance(self._resampling_strategy, CrossValTypes)\n        ):\n            self._models_cv = None\n        else:\n            self._models_cv = self._load_cv_models_by_identifiers(\n                self._load_best_individual_model().get_selected_model_identifiers()\n            )\n\n        self._models = self._load_best_individual_model()\n        self._logger.debug(\n            \"No ensemble class defined, loading best single model with seed {}\".format(\n                self._seed\n            )\n        )\n\n    self._logger.debug(\n        \"Loaded {} models and {} cross-validated models\".format(\n            len(self._models), len(self._models_cv)\n        )\n    )\n\n", "CodeBleu_score": 39.57357488010338}, {"code": "def _load_models(self)\n\t\n    # This function loads models based on the specified ensemble class and resampling strategy.\n    # If an ensemble class is defined, it loads the ensemble using the backend and a seed.\n    # If the ensemble is not loaded and certain evaluator output conditions and resampling strategies are met, it loads the best individual model instead.\n    # If the ensemble is successfully loaded, it retrieves model identifiers and loads the corresponding models using the backend.\n    # For specific resampling strategies, it also loads cross-validated models; otherwise, it sets the cross-validated models to None.\n    # If no ensemble is loaded, it initializes empty lists for models and cross-validated models.\n    self.models_ = []\n    self.cv_models_ = []\n\n    # Load the ensemble if it exists\n    if self.ensemble_ is not None:\n        self.ensemble_ = self.ensemble_(\n            metrics=self._metrics,\n            task_type=self._task,\n            seed=self._seed,\n            run_history=self.runhistory_,\n            backend=self._backend,\n            random_state=self._seed,\n        )\n        self.identifiers_ = self.ensemble_.get_identifiers_with_weights()\n        self.weights_ = [weight for _, weight in self.identifiers_]\n        self.identifiers_ = [identifier for identifier, _ in self.identifiers_]\n        self.models_ = self._backend.load_models_by_identifiers(\n            self.identifiers_, self.ensemble_.get_selected_configurations()\n        )\n\n        # Load cross-validated models if the resampling strategy is not None\n        if self.resampling_strategy_ is not None:\n            self.cv_models_ = self._backend.load_cv_models_by_identifiers(\n                self.identifiers_,\n                self.ensemble_.get_selected_configurations(),\n                self.resampling_strategy_,\n            )\n        else:\n            self.cv_models_ = None\n    # Load the best individual model if the ensemble is not loaded and certain evaluator output conditions are met\n    elif self.ensemble_ is None and self.output_y_hat_optimization and (\n        self.resampling_strategy_ is None\n        or self.resampling_strategy_.is_cross_validated() is False\n    ):\n        self._logger.warning(\n            \"No valid ensemble was created. Please check the log\"\n            \"file for errors. Default to the best individual estimator:{}\".format(\n                self.", "CodeBleu_score": 44.44950998776275}, {"code": "def _load_models(self)\n\t\n    # This function loads models based on the specified ensemble class and resampling strategy.\n    # If an ensemble class is defined, it loads the ensemble using the backend and a seed.\n    # If the ensemble is not loaded and certain evaluator output conditions and resampling strategies are met, it loads the best individual model instead.\n    # If the ensemble is successfully loaded, it retrieves model identifiers and loads the corresponding models using the backend.\n    # For specific resampling strategies, it also loads cross-validated models; otherwise, it sets the cross-validated models to None.\n    # If no ensemble is loaded, it initializes empty lists for models and cross-validated models.\n\n    # If an ensemble class is defined, it loads the ensemble using the backend and a seed.\n    if self._ensemble_class is not None:\n        # If the ensemble is not loaded and certain evaluator output conditions and resampling strategies are met, it loads the best individual model instead.\n        if not self._ensemble_class.is_fitted and (\n            self.output_directory is not None\n            and self.output_directory!= \"\"\n            and self.output_directory!= \"None\"\n            and self.output_directory!= \"none\"\n            and self.output_directory!= \"null\"\n            and self.output_directory!= \"Null\"\n            and self.output_directory!= \"NULL\"\n            and self.output_directory!= \"Null\"\n            and self.output_directory!= \"nULL\"\n            and self.output_directory!= \"NULl\"\n            and self.output_directory!= \"NuLL\"\n            and self.output_directory!= \"nUll\"\n            and self.output_directory!= \"nUll\"\n            and self.output_directory!= \"nuLL\"\n            and self.output_directory!= \"nUll\"\n            and self.output_directory!= \"nuLl\"\n            and self.output_directory!= \"nUll\"\n            and self.output_directory!= \"nUll\"\n            and self.output_directory!= \"nUll\"\n            and self.output_directory!= \"nUll\"\n            and self.output_directory!= \"nUll\"\n            and self.output_directory!= \"nUll\"\n            and self.output_directory!= \"nUll\"\n            and self.output_directory!= \"nUll\"\n            and self.output_directory!= \"nUll\"\n            and", "CodeBleu_score": 24.40987510253916}]}
{"instance_id": "ef71b1cddb50", "generation_result": [{"code": "def load_task(task_id)\n\t\n    task = openml.tasks.get_task(task_id)\n    X, y = task.get_X_and_y()\n    train_indices, test_indices = task.get_train_test_split_indices()\n    X_train, y_train = X[train_indices], y[train_indices]\n    X_test, y_test = X[test_indices], y[test_indices]\n    dataset = task.get_dataset()\n    categorical_features = dataset.retrieve_class_attribute().categories\n    feature_types = []\n    for feature in dataset.features:\n        if feature.name in categorical_features:\n            feature_types.append(feature.name)\n        else:\n            feature_types.append(feature.name)\n    label_values = np.unique(y_train)\n    label_mapping = {label: index for index, label in enumerate(label_values)}\n    y_train = np.array([label_mapping[label] for label in y_train])\n    y_test = np.array([label_mapping[label] for label in y_test])\n    return X_train, y_train, X_test, y_test, feature_types\n\n", "CodeBleu_score": 40.144296970891105}, {"code": "def load_task(task_id)\n\t\n    task = openml.tasks.get_task(task_id)\n    X, y, categorical_indicator, attribute_names = task.get_X_and_y()\n    train_indices, test_indices = task.get_train_test_split_indices()\n    X_train = X[train_indices]\n    y_train = y[train_indices]\n    X_test = X[test_indices]\n    y_test = y[test_indices]\n    dataset = task.get_dataset()\n    label_names = dataset.retrieve_class_labels()\n    label_dict = {label: i for i, label in enumerate(label_names)}\n    y_train = np.array([label_dict[label] for label in y_train])\n    y_test = np.array([label_dict[label] for label in y_test])\n    return X_train, y_train, X_test, y_test, categorical_indicator\n", "CodeBleu_score": 40.833903655324946}, {"code": "def load_task(task_id)\n\t\n    # Load the task\n    task = openml.tasks.get_task(task_id)\n\n    # Extract features (X) and labels (y)\n    X, y = task.get_X_and_y()\n\n    # Obtain train-test split indices\n    train_indices, test_indices = task.get_train_test_split_indices()\n\n    # Create training and testing sets for both features and labels\n    X_train, X_test = X[train_indices], X[test_indices]\n    y_train, y_test = y[train_indices], y[test_indices]\n\n    # Fetch the dataset associated with the task\n    dataset = task.get_dataset()\n\n    # Map unique label values to integer indices for both training and testing labels\n    label_encoder = LabelEncoder()\n    y_train_encoded = label_encoder.fit_transform(y_train)\n    y_test_encoded = label_encoder.transform(y_test)\n\n    # Determine the categorical or numerical nature of each feature\n    feature_types = []\n    for feature in dataset.features:\n        if feature.data_type == \"nominal\":\n            feature_types.append(\"categorical\")\n        else:\n            feature_types.append(\"numerical\")\n\n    return X_train, y_train_encoded, X_test, y_test_encoded, feature_types\n", "CodeBleu_score": 26.65939286222797}]}
{"instance_id": "90bd6edbcc9e", "generation_result": [{"code": "def convert_if_sparse(\n    y: SUPPORTED_TARGET_TYPES,\n) -> Union[np.ndarray, List, pd.DataFrame, pd.Series]\n\t\n    if isinstance(y, sp.sparse.spmatrix):\n        y = y.toarray()\n        if y.ndim == 1:\n            y = y.flatten()\n    return y\n\n", "CodeBleu_score": 55.9206743264562}, {"code": "def convert_if_sparse(\n    y: SUPPORTED_TARGET_TYPES,\n) -> Union[np.ndarray, List, pd.DataFrame, pd.Series]\n\t\n    if isinstance(y, scipy.sparse.spmatrix):\n        y = y.toarray()\n        if y.ndim == 1:\n            y = y.flatten()\n    return y\n\n", "CodeBleu_score": 55.93959727127174}, {"code": "def convert_if_sparse(\n    y: SUPPORTED_TARGET_TYPES,\n) -> Union[np.ndarray, List, pd.DataFrame, pd.Series]\n\t\n    if isinstance(y, scipy.sparse.spmatrix):\n        y = y.toarray()\n        if y.ndim == 1:\n            y = y.flatten()\n    return y\n\n", "CodeBleu_score": 55.93959727127174}]}
{"instance_id": "5728e3d4f914", "generation_result": [{"code": "def _calculate_weights(self) -> None\n\t\n    # Count the occurrences of each ensemble member index\n    counts = Counter(self.ensemble_indices_)\n    # Initialize a weights array with zeros, having a length equal to the number of input models\n    weights = np.zeros(len(self.ensemble_))\n    # For each ensemble member, calculate the weight as the frequency divided by the ensemble size\n    for i in range(len(self.ensemble_)):\n        weights[i] = counts[i] / len(self.ensemble_)\n    # If the sum of weights is less than 1, normalize the weights to ensure their sum equals 1\n    if np.sum(weights) < 1:\n        weights /= np.sum(weights)\n    # Store the calculated weights in the instance variable self.weights_\n    self.weights_ = weights\n", "CodeBleu_score": 33.780833710056356}, {"code": "def _calculate_weights(self) -> None\n\t\n    self.weights_ = np.zeros(len(self.models_))\n    for i, model in enumerate(self.models_):\n        self.weights_[i] = np.sum(self.ensemble_members_ == i) / len(self.ensemble_members_)\n    if np.sum(self.weights_) < 1:\n        self.weights_ = self.weights_ / np.sum(self.weights_)\n", "CodeBleu_score": 24.229529884782625}, {"code": "def _calculate_weights(self) -> None\n\t\n    # Count the occurrences of each ensemble member index\n    counts = np.bincount(self.ensemble_)\n    # Initialize a weights array with zeros, having a length equal to the number of input models\n    weights = np.zeros(len(self.models_))\n    # For each ensemble member, calculate the weight as the frequency divided by the ensemble size\n    for i, count in enumerate(counts):\n        weights[i] = count / len(self.ensemble_)\n    # If the sum of weights is less than 1, normalize them to ensure their sum equals 1\n    if np.sum(weights) < 1:\n        weights /= np.sum(weights)\n    # Store the calculated weights in the instance variable self.weights_\n    self.weights_ = weights\n", "CodeBleu_score": 31.413026229049514}]}
{"instance_id": "4f9c4799fd8f", "generation_result": [{"code": "def predict_proba(self, X, batch_size=None, n_jobs=1)\n\t\n    # This function predicts class probabilities for input data X, optionally using specified batch size and number of jobs.\n    # It calls the superclass's predict_proba method to obtain the predicted probabilities.\n    # If the target type is not \"multilabel-indicator\", it asserts that the sum of probabilities for each instance is 1.\n    # It also asserts that all probability values lie between 0 and 1.\n    # The function returns the predicted probabilities.\n    if self.target_type_ == \"multilabel-indicator\":\n        return super().predict_proba(X, batch_size=batch_size, n_jobs=n_jobs)\n    else:\n        proba = super().predict_proba(X, batch_size=batch_size, n_jobs=n_jobs)\n        assert np.allclose(np.sum(proba, axis=1), np.ones(len(proba)))\n        assert np.all((proba >= 0) & (proba <= 1))\n        return proba\n\n", "CodeBleu_score": 55.0044292412891}, {"code": "def predict_proba(self, X, batch_size=None, n_jobs=1)\n\t\n    proba = super().predict_proba(X, batch_size=batch_size, n_jobs=n_jobs)\n    if self.target_type_!= \"multilabel-indicator\":\n        assert np.allclose(np.sum(proba, axis=1), 1)\n    assert np.all(proba >= 0) and np.all(proba <= 1)\n    return proba\n\n", "CodeBleu_score": 31.855754302276907}, {"code": "def predict_proba(self, X, batch_size=None, n_jobs=1)\n\t\n    # This function predicts class probabilities for input data X, optionally using specified batch size and number of jobs.\n    # It calls the superclass's predict_proba method to obtain the predicted probabilities.\n    # If the target type is not \"multilabel-indicator\", it asserts that the sum of probabilities for each instance is 1.\n    # It also asserts that all probability values lie between 0 and 1.\n    # The function returns the predicted probabilities.\n    return self.automl_.predict_proba(X, batch_size=batch_size, n_jobs=n_jobs)\n\n", "CodeBleu_score": 9.489018990699531}]}
{"instance_id": "a9a13150aebe", "generation_result": [{"code": "def convert_to_num(Ybin: np.ndarray) -> np.ndarray:\n\t\n    return np.dot(Ybin, range(len(Ybin[0])))\n", "CodeBleu_score": 14.984348431665081}, {"code": "def convert_to_num(Ybin: np.ndarray) -> np.ndarray:\n\t\n    return np.dot(Ybin, range(Ybin.shape[1]))\n\n", "CodeBleu_score": 20.397341916101205}, {"code": "def convert_to_num(Ybin: np.ndarray) -> np.ndarray:\n\t\n    return np.dot(Ybin, range(Ybin.shape[1]))\n", "CodeBleu_score": 20.397341916101205}]}
{"instance_id": "e97d38e62f81", "generation_result": [{"code": "def fit_transformer(self, X, y, fit_params=None)\n\t\n\n    if fit_params is None:\n        fit_params = {}\n\n    if self.config[\"balancing:strategy\"] == \"weighting\":\n        balancing = Balancing(strategy=\"weighting\")\n        _init_params, _fit_params = balancing.get_weights(\n            y,\n            self.config[\"classifier:__choice__\"],\n            self.config[\"feature_preprocessor:__choice__\"],\n            {},\n            {},\n        )\n        _init_params.update(self.init_params)\n        self.set_hyperparameters(\n            feat_type=self.feat_type,\n            configuration=self.config,\n            init_params=_init_params,\n        )\n\n        if _fit_params is not None:\n            fit_params.update(_fit_params)\n\n    X, fit_params = super().fit_transformer(X, y, fit_params=fit_params)\n\n    return X, fit_params\n", "CodeBleu_score": 98.02795049070139}, {"code": "def fit_transformer(self, X, y, fit_params=None)\n\t\n    if fit_params is None:\n        fit_params = {}\n\n    if self.config[\"balancing:strategy\"] == \"weighting\":\n        balancing = Balancing(strategy=\"weighting\")\n        _init_params, _fit_params = balancing.get_weights(\n            y,\n            self.config[\"classifier:__choice__\"],\n            self.config[\"feature_preprocessor:__choice__\"],\n            {},\n            {},\n        )\n        _init_params.update(self.init_params)\n        self.set_hyperparameters(\n            feat_type=self.feat_type,\n            configuration=self.config,\n            init_params=_init_params,\n        )\n\n        if _fit_params is not None:\n            fit_params.update(_fit_params)\n\n    X, fit_params = super().fit_transformer(X, y, fit_params=fit_params)\n\n    return X, fit_params\n", "CodeBleu_score": 98.02795049070139}, {"code": "def fit_transformer(self, X, y, fit_params=None)\n\t\n    if fit_params is None:\n        fit_params = {}\n\n    if self.config[\"balancing:strategy\"] == \"weighting\":\n        balancing = Balancing(strategy=\"weighting\")\n        _init_params, _fit_params = balancing.get_weights(\n            y,\n            self.config[\"classifier:__choice__\"],\n            self.config[\"feature_preprocessor:__choice__\"],\n            {},\n            {},\n        )\n        _init_params.update(self.init_params)\n        self.set_hyperparameters(\n            feat_type=self.feat_type,\n            configuration=self.config,\n            init_params=_init_params,\n        )\n\n        if _fit_params is not None:\n            fit_params.update(_fit_params)\n\n    X, fit_params = super().fit_transformer(X, y, fit_params=fit_params)\n\n    return X, fit_params\n", "CodeBleu_score": 98.02795049070139}]}
{"instance_id": "b1aaad80a9c4", "generation_result": [{"code": "def test_glm_identity_link_func()\n\t\n    # Load the dataset\n    df = pd.read_csv(\"https://raw.githubusercontent.com/scikit-learn/scikit-learn/main/sklearn/datasets/data/diabetes.csv\")\n    # Select the features and target variable\n    X = df.drop(\"target\", axis=1)\n    y = df[\"target\"]\n    # Create a Generalized Linear Model (GLM) with an identity link function\n    model = sm.GLM(y, X, family=sm.families.Tweedie(link=sm.families.links.Power(1)))\n    # Fit the model\n    model_fit = model.fit()\n    # Wrap the model in a custom wrapper\n    wrapped_model = utils.StatsmodelsSklearnLikeWrapper(model_fit)\n    # Create an assembler to convert the model into an expression\n    assembler = assemblers.StatsmodelsModelAssemblerSelector()\n    expr = assembler.assemble(wrapped_model)\n    # Define the expected expression\n    expected_expr = ast.BinNumExpr(\n        ast.NumVal(0.0001),\n        ast.BinNumExpr(\n            ast.NumVal(0.0001),\n            ast.BinNumExpr(\n                ast.NumVal(0.0001),\n                ast.BinNumExpr(\n                    ast.NumVal(0.0001),\n                    ast.BinNumExpr(\n                        ast.NumVal(0.0001),\n                        ast.BinNumExpr(\n                            ast.NumVal(0.0001),\n                            ast.BinNumExpr(\n                                ast.NumVal(0.0001),\n                                ast.BinNumExpr(\n                                    ast.NumVal(0.0001),\n                                    ast.BinNumExpr(\n                                        ast.NumVal(0.0001),\n                                        ast.BinNumExpr(\n                                            ast.NumVal(0.0001),\n                                            ast.BinNumExpr(\n                                                ast.NumVal(0.0001),\n                                                ast.BinNumExpr(\n                                                    ast.NumVal(0.0001),\n                                                    ast.BinNumExpr(\n                                                        ast.NumVal(0.0001),\n                                                        ast.BinNumExpr(\n                                                            ast.NumVal(0.0001),\n                                                            ast.BinNumExpr(\n                                                                ast.NumVal(0.0001),", "CodeBleu_score": 30.79637725833374}, {"code": "def test_glm_identity_link_func()\n\t\n    # This function tests the Generalized Linear Model (GLM) with an identity link function.\n    # It wraps the GLM from the Statsmodels library using a custom wrapper and sets it up with a Tweedie family and Power(1) link.\n    # The model is fitted with a small dataset.\n    # An assembler is used to convert the fitted model into an expression representation.\n    # The test compares the assembled expression to an expected expression using an assertion to ensure they match.\n    # Create a small dataset\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    y = np.array([1, 2, 3])\n    # Wrap the GLM model with a custom wrapper\n    model = utils.StatsmodelsSklearnLikeWrapper(sm.GLM(family=sm.families.Tweedie(link=sm.families.links.Power(1))))\n    # Fit the model with the dataset\n    model.fit(X, y)\n    # Assemble the model into an expression\n    assembler = assemblers.StatsmodelsModelAssemblerSelector()\n    expr = assembler.assemble(model)\n    # Define the expected expression\n    expected_expr = ast.BinNumExpr(\n        left=ast.NumVal(value=1),\n        op=ast.BinNumOpType.ADD,\n        right=ast.BinNumExpr(\n            left=ast.FeatureRef(index=0),\n            op=ast.BinNumOpType.MUL,\n            right=ast.NumVal(value=0.5),\n        ),\n    )\n    # Compare the assembled expression to the expected expression\n    assert utils.cmp_exprs(expr, expected_expr)\n\n", "CodeBleu_score": 37.93818108048661}, {"code": "def test_glm_identity_link_func()\n\t\n    # Create a small dataset\n    X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    y = np.array([10, 20, 30])\n    # Wrap the GLM from the Statsmodels library using a custom wrapper\n    wrapped_model = utils.StatsmodelsSklearnLikeWrapper(sm.GLM, family=sm.families.Tweedie(link=sm.families.links.Power(1)))\n    # Set up the GLM with a Tweedie family and Power(1) link\n    wrapped_model.fit(X, y)\n    # Assemble the model into an expression representation\n    assembled_expr = assemblers.StatsmodelsModelAssemblerSelector(wrapped_model).assemble()\n    # Create an expected expression\n    expected_expr = ast.BinNumExpr(\n        left=ast.BinNumExpr(\n            left=ast.BinNumExpr(\n                left=ast.BinNumExpr(\n                    left=ast.FeatureRef(index=0),\n                    op=ast.BinOpType.MUL,\n                    right=ast.NumVal(value=1.0),\n                ),\n                op=ast.BinOpType.ADD,\n                right=ast.BinNumExpr(\n                    left=ast.BinNumExpr(\n                        left=ast.FeatureRef(index=1),\n                        op=ast.BinOpType.MUL,\n                        right=ast.NumVal(value=2.0),\n                    ),\n                    op=ast.BinOpType.ADD,\n                    right=ast.BinNumExpr(\n                        left=ast.FeatureRef(index=2),\n                        op=ast.BinOpType.MUL,\n                        right=ast.NumVal(value=3.0),\n                    ),\n                ),\n            ),\n            op=ast.BinOpType.MUL,\n            right=ast.NumVal(value=1.0),\n        ),\n        op=ast.BinOpType.ADD,\n        right=ast.NumVal(value=0.0),\n    )\n    # Compare the assembled expression to the expected expression\n    assert assembled_expr == expected_expr\n", "CodeBleu_score": 31.90640900008433}]}
{"instance_id": "bcab63e592d0", "generation_result": [{"code": "def _compute_aggregation(\n        self, col: Variable, agg: Union[str, Callable, Tuple[str, Union[str, Callable]]]\n    ) -> pd.Series\n\t\n    if isinstance(agg, tuple):\n        agg_name, agg_func = agg\n    else:\n        agg_name = agg if isinstance(agg, str) else agg.__name__\n        agg_func = agg\n    return self.agg({col: (col + \"_\" + agg_name, agg_func)})\n", "CodeBleu_score": 57.94937279845664}, {"code": "def _compute_aggregation(\n        self, col: Variable, agg: Union[str, Callable, Tuple[str, Union[str, Callable]]]\n    ) -> pd.Series\n\t\n    if isinstance(agg, tuple):\n        name, agg = agg\n    else:\n        name = agg.__name__ if callable(agg) else agg\n    return self.agg({col: (col, agg)}).rename(columns={col: name})\n\n", "CodeBleu_score": 61.91600452721604}, {"code": "def _compute_aggregation(\n        self, col: Variable, agg: Union[str, Callable, Tuple[str, Union[str, Callable]]]\n    ) -> pd.Series\n\t\n    if isinstance(agg, tuple):\n        name, agg = agg\n    else:\n        name = agg.__name__ if isinstance(agg, Callable) else agg\n    return getattr(self.obj, agg)(col).rename(col + \"_\" + name)\n", "CodeBleu_score": 59.84440825793769}]}
{"instance_id": "114d623dae7b", "generation_result": [{"code": "def _corrfunc(self, x, y, **kws)\n\t\n    r, _ = stats.pearsonr(x, y)\n    ax = plt.gca()\n    ax.annotate(\"r = {:.2f}\".format(r),\n                xy=(.1,.9), xycoords=ax.transAxes,\n                size=min(abs(r) * 100, 20))\n", "CodeBleu_score": 13.605385531225773}, {"code": "def _corrfunc(self, x, y, **kws)\n\t\n    r, _ = stats.pearsonr(x, y)\n    ax = plt.gca()\n    ax.annotate(\"r = {:.2f}\".format(r),\n                xy=(.1,.9), xycoords=ax.transAxes)\n", "CodeBleu_score": 10.900904044378585}, {"code": "def _corrfunc(self, x, y, **kws)\n\t\n    r, _ = stats.pearsonr(x, y)\n    ax = plt.gca()\n    ax.annotate(\"r = {:.2f}\".format(r),\n                xy=(.1,.9), xycoords=ax.transAxes,\n                size=min(abs(r) * 100, 100))\n", "CodeBleu_score": 13.613432106333539}]}
{"instance_id": "4c1ade308953", "generation_result": [{"code": "def _round_digits(src: float, rounddigit: int = None, method='decimal')\n\t    if method == 'decimal':\n        return round(src, rounddigit)\n    elif method =='sig':\n        if rounddigit is None:\n            return src\n        else:\n            with decimal.localcontext() as ctx:\n                ctx.prec = rounddigit\n                return ctx.create_decimal(src).normalize()\n    elif method == 'format':\n        if rounddigit is None:\n            return src\n        else:\n            return float(f\"{src:.{rounddigit}g}\")\n    else:\n        raise ValueError(f\"Invalid rounding method: {method}\")\n\n", "CodeBleu_score": 66.71183737862894}, {"code": "def _round_digits(src: float, rounddigit: int = None, method='decimal')\n\t    if method == 'decimal':\n        return round(src, rounddigit)\n    elif method =='sig':\n        return decimal.Decimal(src).quantize(decimal.Decimal(10) ** -rounddigit)\n    elif method == 'format':\n        return float(f'{src:.{rounddigit}g}')\n    else:\n        raise ValueError(f'Unknown rounding method: {method}')\n\n", "CodeBleu_score": 57.684023908961194}, {"code": "def _round_digits(src: float, rounddigit: int = None, method='decimal')\n\t    if method == 'decimal':\n        return round(src, rounddigit)\n    elif method =='sig':\n        return decimal.Decimal(src).quantize(decimal.Decimal(10) ** -rounddigit)\n    elif method == 'format':\n        return float(f\"{src:.{rounddigit}g}\")\n    else:\n        raise ValueError('Invalid method')\n", "CodeBleu_score": 58.45219487217271}]}
{"instance_id": "a9923e678320", "generation_result": [{"code": "def _round_dict_digits(cls, srcdict: Dict[str, float], rounddigit: int = None, method='decimal')\n\t   dstdict = {}\n    for k, v in srcdict.items():\n        if rounddigit is not None and isinstance(v, float):\n            dstdict[k] = cls._round_digits(v, rounddigit=rounddigit, method=method)\n        else:\n            dstdict[k] = v\n    return dstdict\n\n", "CodeBleu_score": 82.35846047381594}, {"code": "def _round_dict_digits(cls, srcdict: Dict[str, float], rounddigit: int = None, method='decimal')\n\t   dstdict = {}\n    for k, v in srcdict.items():\n        if rounddigit is not None and isinstance(v, float):\n            dstdict[k] = cls._round_digits(v, rounddigit=rounddigit, method=method)\n        else:\n            dstdict[k] = v\n    return dstdict\n\n", "CodeBleu_score": 82.35846047381594}, {"code": "def _round_dict_digits(cls, srcdict: Dict[str, float], rounddigit: int = None, method='decimal')\n\t   dstdict = {}\n    for k, v in srcdict.items():\n        if rounddigit is not None and isinstance(v, float):\n            dstdict[k] = cls._round_digits(v, rounddigit=rounddigit, method=method)\n        else:\n            dstdict[k] = v\n    return dstdict\n", "CodeBleu_score": 82.35846047381594}]}
{"instance_id": "b6106e079dc0", "generation_result": [{"code": "def _scatterplot_ndarray(cls, x, x_name, y, y_name, hue_data, hue_name, ax, scatter_kws, legend_kws)\n\t\n    df = pd.DataFrame({x_name: x, y_name: y})\n    if hue_data is not None:\n        df[\"hue\"] = hue_data\n        hue_name = hue_name or \"hue\"\n    sns.scatterplot(data=df, x=x_name, y=y_name, hue=hue_name, ax=ax, **scatter_kws)\n    ax.legend(**legend_kws)\n\n", "CodeBleu_score": 29.930751838063678}, {"code": "def _scatterplot_ndarray(cls, x, x_name, y, y_name, hue_data, hue_name, ax, scatter_kws, legend_kws)\n\t\n    # Create a DataFrame from the x and y values\n    data = pd.DataFrame({x_name: x, y_name: y})\n\n    # If hue_data is provided, add it to the DataFrame\n    if hue_data is not None:\n        data['hue'] = hue_data\n\n    # Plot the scatter plot using Seaborn\n    sns.scatterplot(data=data, x=x_name, y=y_name, hue='hue', ax=ax, **scatter_kws)\n\n    # Set the title for the legend\n    if 'title' not in legend_kws:\n        legend_kws['title'] = hue_name\n\n    # Plot the legend\n    ax.legend(**legend_kws)\n", "CodeBleu_score": 34.266124148736715}, {"code": "def _scatterplot_ndarray(cls, x, x_name, y, y_name, hue_data, hue_name, ax, scatter_kws, legend_kws)\n\t\n    # Combine x and y values into a DataFrame with specified column names\n    data = pd.DataFrame({x_name: x, y_name: y})\n\n    # Add hue field to DataFrame if hue_data is provided\n    if hue_data is not None:\n        data['hue'] = hue_data\n\n    # Plot scatter plot using Seaborn's scatterplot function\n    sns.scatterplot(data=data, x=x_name, y=y_name, hue='hue', ax=ax, **scatter_kws)\n\n    # Set legend title if no title is specified\n    if 'title' not in legend_kws:\n        legend_kws['title'] = hue_name\n\n    # Set legend title and other keyword arguments\n    ax.legend(**legend_kws)\n\n", "CodeBleu_score": 34.266124148736715}]}
{"instance_id": "257e0aa5dd4d", "generation_result": [{"code": "def fit(self, **fit_args) -> \"TimeSeriesModel\"\n\t\n    as_df = self.node.item\n    end = self._get_transformed_data(as_series=True)\n    if self.node.exogenous:\n        ex = as_df[self.node.exogenous]\n    else:\n        ex = None\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\"ignore\", category=UserWarning)\n        warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n        self.model = self.model.fit(y=end, X=ex, **fit_args)\n    return self\n\n", "CodeBleu_score": 81.66611757152214}, {"code": "def fit(self, **fit_args) -> \"TimeSeriesModel\"\n\t\n    as_df = self.node.item\n    end = self._get_transformed_data(as_series=True)\n    if self.node.exogenous:\n        ex = as_df[self.node.exogenous]\n    else:\n        ex = None\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\"ignore\", category=UserWarning)\n        warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n        self.model = self.model.fit(y=end, X=ex, **fit_args)\n    return self\n\n", "CodeBleu_score": 81.66611757152214}, {"code": "def fit(self, **fit_args) -> \"TimeSeriesModel\"\n\t\n    as_df = self.node.item\n    end = self._get_transformed_data(as_series=True)\n    if self.node.exogenous:\n        ex = as_df[self.node.exogenous]\n    else:\n        ex = None\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\"ignore\", category=UserWarning)\n        warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n        self.model = self.model.fit(y=end, X=ex, **fit_args)\n    return self\n", "CodeBleu_score": 81.66611757152214}]}
{"instance_id": "6446de2223e8", "generation_result": [{"code": "def create_model(self, **kwargs)\n\t    model_type = self.model_type\n    if model_type == \"holt_winters\":\n        transformed_data = self._get_transformed_data(as_series=True)\n        model = ExponentialSmoothing(\n            transformed_data,\n            trend=self.trend,\n            seasonal=self.seasonal,\n            seasonal_periods=self.seasonal_periods,\n        )\n    elif model_type == \"auto_arima\":\n        try:\n            from pmdarima import AutoARIMA\n        except ImportError:\n            logger.error(\"pmdarima is not installed. Please install it.\")\n            return\n        model = AutoARIMA(\n            start_p=self.start_p,\n            start_q=self.start_q,\n            max_p=self.max_p,\n            max_q=self.max_q,\n            start_P=self.start_P,\n            start_Q=self.start_Q,\n            max_P=self.max_P,\n            max_Q=self.max_Q,\n            seasonal=self.seasonal,\n            stepwise=self.stepwise,\n            suppress_warnings=self.suppress_warnings,\n            trace=self.trace,\n            error_action=self.error_action,\n            random_state=self.random_state,\n            n_fits=self.n_fits,\n            out_of_sample_size=self.out_of_sample_size,\n            scoring=self.scoring,\n            scoring_args=self.scoring_args,\n            with_intercept=self.with_intercept,\n            **kwargs,\n        )\n    elif model_type == \"sarimax\":\n        transformed_data = self._get_transformed_data()\n        exog = self.exog\n        if exog:\n            exog = self.exog.transform(self.exog.item[self.exog.key])\n            exog = pandas.DataFrame(exog, index=transformed_data.index)\n            model = SARIMAX(\n                transformed_data,\n                exog=exog,\n                order=(self.p, self.d, self.q),\n                seasonal_order=(self.P, self.D, self.Q, self.s),\n                enforce_stationarity=self.enforce_stationarity,\n                enforce_invertibility=self", "CodeBleu_score": 42.4259557571075}, {"code": "def create_model(self, **kwargs)\n\t    model_type = self.model_type\n    if model_type == \"holt_winters\":\n        transformed_data = self._get_transformed_data(as_series=True)\n        model = ExponentialSmoothing(transformed_data, **kwargs)\n    elif model_type == \"auto_arima\":\n        try:\n            from pmdarima import AutoARIMA\n        except ImportError:\n            logger.error(\"pmdarima is not installed. Please install it to use this model.\")\n            return\n        model = AutoARIMA(**kwargs)\n    elif model_type == \"sarimax\":\n        transformed_data = self._get_transformed_data()\n        exog = self.exogenous_variables\n        if exog:\n            model = SARIMAX(transformed_data, exog=exog, **kwargs)\n        else:\n            model = SARIMAX(transformed_data, **kwargs)\n    else:\n        raise ValueError(f\"Unsupported model type: {model_type}\")\n    return model", "CodeBleu_score": 47.445233381193006}, {"code": "def create_model(self, **kwargs)\n\t    model_type = self.model_type\n    if model_type == \"holt_winters\":\n        transformed_data = self._get_transformed_data(as_series=True)\n        model = ExponentialSmoothing(transformed_data, **kwargs)\n    elif model_type == \"auto_arima\":\n        try:\n            from pmdarima import AutoARIMA\n        except ImportError:\n            logger.error(\n                \"AutoARIMA is not installed. Please install it by running 'pip install pmdarima'.\"\n            )\n            return\n        model = AutoARIMA(**kwargs)\n    elif model_type == \"sarimax\":\n        transformed_data = self._get_transformed_data()\n        exog = self.exog\n        if exog:\n            model = SARIMAX(transformed_data, exog=exog, **kwargs)\n        else:\n            model = SARIMAX(transformed_data, **kwargs)\n    else:\n        raise ValueError(f\"Invalid model type: {model_type}\")\n    return model\n\n", "CodeBleu_score": 49.280291556608525}]}
{"instance_id": "b2f67820e27f", "generation_result": [{"code": "def _to_chx(array)\n\t\n    if isinstance(array, numpy.ndarray):\n        return _from_numpy(array)\n    if cupy is not None and isinstance(array, cupy.ndarray):\n        return _from_cupy(array)\n    return array\n", "CodeBleu_score": 88.40913031500585}, {"code": "def _to_chx(array)\n\t\n    if isinstance(array, numpy.ndarray):\n        return _from_numpy(array)\n    if cupy is not None and isinstance(array, cupy.ndarray):\n        return _from_cupy(array)\n    return array", "CodeBleu_score": 88.40913031500585}, {"code": "def _to_chx(array)\n\t\n    if isinstance(array, numpy.ndarray):\n        return _from_numpy(array)\n    if cupy is not None and isinstance(array, cupy.ndarray):\n        return _from_cupy(array)\n    return array\n", "CodeBleu_score": 88.40913031500585}]}
{"instance_id": "b96f7d9bcc95", "generation_result": [{"code": "def _fix_numpy_split(ys, x, indices_or_sections, axis)\n\t\n    # Check if all arrays in ys have the same number of dimensions as x\n    if not all(len(y.shape) == len(x.shape) for y in ys):\n        # Calculate the expected shapes for the split segments using a temporary split on an empty array with the same shape as x along the specified axis\n        split_sizes = numpy.array_split(numpy.empty(x.shape), indices_or_sections, axis=axis).shape[1:]\n        # For each segment, if the dimensions do not match, reshape the segment to the expected shape based on the calculated split sizes\n        ys = [y.reshape(split_sizes[i]) if len(y.shape)!= len(x.shape) else y for i, y in enumerate(ys)]\n    return ys\n\n", "CodeBleu_score": 37.34744918313571}, {"code": "def _fix_numpy_split(ys, x, indices_or_sections, axis)\n\t\n    import numpy as np\n    if not all(len(y.shape) == len(x.shape) for y in ys):\n        # calculate the expected shapes for the split segments\n        split_sizes = np.array_split(np.empty_like(x), indices_or_sections, axis=axis).shape[1:]\n        # reshape each segment to the expected shape\n        ys = [np.reshape(y, split_sizes[i]) for i, y in enumerate(ys)]\n    return ys\n", "CodeBleu_score": 36.54113124464151}, {"code": "def _fix_numpy_split(ys, x, indices_or_sections, axis)\n\t\n    ys = list(ys)\n    if not all(y.ndim == x.ndim for y in ys):\n        split_sizes = numpy.array_split(numpy.empty_like(x), indices_or_sections, axis=axis).shape\n        for i, y in enumerate(ys):\n            if y.ndim!= x.ndim:\n                ys[i] = y.reshape(split_sizes[i])\n    return ys\n\n", "CodeBleu_score": 39.26516767991508}]}
{"instance_id": "cf39fb123842", "generation_result": [{"code": "def to_device_batch(batch):\n\t\n    if device is None:\n        return batch\n    elif device < 0:\n        return [chainer.dataset.to_device(device, x) for x in batch]\n    else:\n        xp = get_array_module(batch[0])\n        batch = xp.concatenate(batch)\n        sections = xp.cumsum([len(x) for x in batch])[:-1]\n        batch = chainer.dataset.to_device(device, batch)\n        return xp.split(batch, sections)\n", "CodeBleu_score": 57.80415244827015}, {"code": "def to_device_batch(batch):\n\t\n    if device is None:\n        return batch\n    elif device < 0:\n        return [chainer.dataset.to_device(device, x) for x in batch]\n    else:\n        xp = get_array_module(batch[0])\n        with chainer.using_device(device):\n            concat = xp.concatenate(batch, axis=0)\n            sections = numpy.cumsum([len(x) for x in batch[:-1]], dtype=numpy.int32)\n            return xp.split(concat, sections)\n", "CodeBleu_score": 68.86006889080626}, {"code": "def to_device_batch(batch):\n\t\n    if device is None:\n        return batch\n    elif device < 0:\n        return chainer.dataset.to_device(device, batch)\n    else:\n        xp = get_array_module(batch)\n        batch = xp.concatenate(batch, axis=0)\n        sections = numpy.cumsum([len(x) for x in batch])[:-1]\n        batch = chainer.dataset.to_device(device, batch)\n        batch = xp.split(batch, sections)\n        return batch\n", "CodeBleu_score": 57.34704938328987}]}
{"instance_id": "845c744d897c", "generation_result": [{"code": "def make_image(trainer)\n\t\n    xp = trainer.updater.device.xp\n    # Set a random seed\n    seed = trainer.updater.iteration\n    np.random.seed(seed)\n    # Determine the number of images to generate\n    n_images = 100\n    # Generate latent variable z\n    z = xp.asarray(gen.make_hidden(n_images))\n    # Generate images using generator model\n    with chainer.using_config(\"train\", False):\n        x = gen(z)\n    # Move images to CPU\n    x = chainer.backends.cuda.to_cpu(x.data)\n    # Scale images to [0, 255] range\n    x = np.clip(x * 255, 0.0, 255.0)\n    x = x.astype(np.uint8)\n    # Reshape and transpose images to form a grid\n    x = x.reshape((10, 10, 28, 28))\n    x = x.transpose(0, 2, 1, 3)\n    x = x.reshape((280, 280))\n    # Create directory for saving preview image if it doesn't exist\n    preview_dir = \"preview\"\n    if not os.path.exists(preview_dir):\n        os.makedirs(preview_dir)\n    # Save generated image as a PNG file\n    preview_path = os.path.join(preview_dir, f\"{seed:08d}.png\")\n    Image.fromarray(x).save(preview_path)\n\n", "CodeBleu_score": 47.3336070712537}, {"code": "def make_image(trainer)\n\t\n    np.random.seed()\n    n_images = 16\n    z = Variable(gen.make_hidden(n_images))\n    with chainer.using_config('train', False):\n        x = gen(z)\n    x = chainer.backends.cuda.to_cpu(x.data)\n    x = np.asarray(np.clip(x * 255, 0.0, 255.0), dtype=np.uint8)\n    n_rows = int(np.sqrt(n_images))\n    x = x.reshape(n_rows, n_rows, 128, 128, 3)\n    x = x.transpose(0, 3, 1, 4, 2)\n    x = x.reshape(128 * n_rows, 128 * n_rows, 3)\n    preview_dir = 'preview'\n    preview_path = preview_dir + '/image_at_epoch_{:04d}.png'.format(trainer.updater.epoch)\n    if not os.path.exists(preview_dir):\n        os.makedirs(preview_dir)\n    Image.fromarray(x).save(preview_path)\n\n", "CodeBleu_score": 60.473398042888164}, {"code": "def make_image(trainer)\n\t\n    xp = trainer.updater.get_device()\n    seed = trainer.updater.iteration\n    with chainer.using_config('train', False), chainer.using_config('enable_backprop', False):\n        z = xp.asarray(gen.make_hidden(100, seed))\n        x = gen(z)\n    x = chainer.backends.cuda.to_cpu(x.data)\n    x = np.asarray(np.clip(x * 255, 0.0, 255.0), dtype=np.uint8)\n    n_images = x.shape[0]\n    rows = int(np.sqrt(n_images))\n    while n_images % rows!= 0:\n        rows -= 1\n    cols = n_images // rows\n    x = x.reshape((rows, cols, 3, 64, 64))\n    x = x.transpose(0, 3, 1, 4, 2)\n    x = x.reshape((rows * 64, cols * 64, 3))\n    preview_dir = 'preview'\n    if not os.path.exists(preview_dir):\n        os.makedirs(preview_dir)\n    preview_path = os.path.join(preview_dir, f'preview_{seed}.png')\n    Image.fromarray(x).save(preview_path)\n", "CodeBleu_score": 60.65123071757077}]}
{"instance_id": "fc2162d43aee", "generation_result": [{"code": "def _cuda_can_cast(from_dtype: npt.DTypeLike, to_dtype: npt.DTypeLike) -> bool\n\t\n    from_dtype = numpy.dtype(from_dtype)\n    to_dtype = numpy.dtype(to_dtype)\n    if from_dtype == to_dtype:\n        return True\n    if from_dtype.kind == \"f\" and to_dtype.kind == \"i\":\n        return True\n    if from_dtype.kind == \"i\" and to_dtype.kind == \"f\":\n        return False\n    if from_dtype.kind == \"i\" and to_dtype.kind == \"u\":\n        return False\n    if from_dtype.kind == \"u\" and to_dtype.kind == \"i\":\n        return False\n    if from_dtype.kind == \"i\" and to_dtype.kind == \"b\":\n        return False\n    if from_dtype.kind == \"b\" and to_dtype.kind == \"i\":\n        return False\n    if from_dtype.kind == \"u\" and to_dtype.kind == \"b\":\n        return False\n    if from_dtype.kind == \"b\" and to_dtype.kind == \"u\":\n        return False\n    if from_dtype.kind == \"b\" and to_dtype.kind == \"f\":\n        return False\n    if from_dtype.kind == \"f\" and to_dtype.kind == \"b\":\n        return False\n    if from_dtype.kind == \"b\" and to_dtype.kind == \"O\":\n        return False\n    if from_dtype.kind == \"O\" and to_dtype.kind == \"b\":\n        return False\n    if from_dtype.kind == \"O\" and to_dtype.kind == \"i\":\n        return False\n    if from_dtype.kind == \"i\" and to_dtype.kind == \"O\":\n        return False\n    if from_dtype.kind == \"O\" and to_dtype.kind == \"u\":\n        return False\n    if from_dtype.kind == \"u\" and to_dtype.kind == \"O\":\n        return False\n    if from_dtype.kind == \"O\" and to_dtype.kind == \"f\":\n        return False\n    if from_dtype.kind == \"f\" and to_dtype.kind == \"O\":\n        return False\n    if from_dtype.kind == \"O\" and to_dtype.kind == \"S\":\n        return False\n    if from_dtype.kind == \"S\" and to_dtype.kind", "CodeBleu_score": 38.837354800762036}, {"code": "def _cuda_can_cast(from_dtype: npt.DTypeLike, to_dtype: npt.DTypeLike) -> bool\n\t\n    from_typechar = _typechars.find(from_dtype.type.char)\n    to_typechar = _typechars.find(to_dtype.type.char)\n    return from_typechar <= to_typechar\n\n", "CodeBleu_score": 41.83947857549334}, {"code": "def _cuda_can_cast(from_dtype: npt.DTypeLike, to_dtype: npt.DTypeLike) -> bool\n\t\n    from_dtype = numpy.dtype(from_dtype)\n    to_dtype = numpy.dtype(to_dtype)\n    return _typechars.find(from_dtype.char) >= _typechars.find(to_dtype.char)\n\n", "CodeBleu_score": 69.02391844154371}]}
{"instance_id": "5b8e2d5e6e12", "generation_result": [{"code": "def manual_legend(g, labels, colors, **legend_kwargs)\n\t\n    if g is None:\n        g = plt.gca()\n    handles = [patches.Patch(color=c, label=l) for l, c in zip(labels, colors)]\n    return g.legend(handles=handles, **legend_kwargs)", "CodeBleu_score": 38.798794031584016}, {"code": "def manual_legend(g, labels, colors, **legend_kwargs)\n\t\n    if g is None:\n        g = plt.gca()\n    if len(labels)!= len(colors):\n        raise YellowbrickValueError(\"The number of labels and colors must be equal.\")\n    legend_handles = [patches.Patch(color=c, label=l) for l, c in zip(labels, colors)]\n    return g.legend(handles=legend_handles, **legend_kwargs)\n", "CodeBleu_score": 54.50203275393897}, {"code": "def manual_legend(g, labels, colors, **legend_kwargs)\n\t\n    # Get the matplotlib Axes object from the input\n    if isinstance(g, Visualizer):\n        ax = g.ax\n    elif isinstance(g, None):\n        ax = plt.gca()\n    elif isinstance(g, Axes):\n        ax = g\n    else:\n        raise YellowbrickValueError(\"The input must be a Visualizer, None, or an Axes object.\")\n\n    # Check that the lengths of the labels and colors lists match\n    if len(labels)!= len(colors):\n        raise YellowbrickValueError(\"The lengths of the labels and colors lists must match.\")\n\n    # Create legend handles by pairing each label with its corresponding color\n    handles = [patches.Patch(color=color, label=label) for label, color in zip(labels, colors)]\n\n    # Return the legend object created with the specified legend handles and additional keyword arguments\n    return ax.legend(handles=handles, **legend_kwargs)\n", "CodeBleu_score": 50.08959454966087}]}
{"instance_id": "494e7f719eb6", "generation_result": [{"code": "def pcoords_time(X, y, fast=True)\n\t\n    plt.clf()\n    plt.close()\n    if fast:\n        time_start = time.time()\n        pca = ParallelCoordinates(features=features, classes=classes, fast=fast)\n        pca.fit_transform(X, y)\n        time_end = time.time()\n        time_elapsed = time_end - time_start\n    else:\n        time_start = time.time()\n        pca = ParallelCoordinates(features=features, classes=classes, fast=fast)\n        pca.fit_transform(X, y)\n        time_end = time.time()\n        time_elapsed = time_end - time_start\n    return time_elapsed\n\n", "CodeBleu_score": 43.5290157411122}, {"code": "def pcoords_time(X, y, fast=True)\n\t\n    fig, ax = plt.subplots()\n    if fast:\n        viz = ParallelCoordinates(ax=ax, fast=True)\n    else:\n        viz = ParallelCoordinates(ax=ax, fast=False)\n    start = time.time()\n    viz.fit_transform(X, y)\n    plt.cla()\n    plt.close()\n    return time.time() - start\n\n", "CodeBleu_score": 48.810977740570394}, {"code": "def pcoords_time(X, y, fast=True)\n\t\n    # Create a plot axis\n    fig, ax = plt.subplots()\n    # Initialize the ParallelCoordinates visualizer with the specified speed setting\n    if fast:\n        oz = ParallelCoordinates(normalize=\"standard\", sample=0.05)\n    else:\n        oz = ParallelCoordinates(normalize=\"standard\")\n    # Record the start time\n    start = time.time()\n    # Fit and transform the input data X and y with the visualizer\n    oz.fit_transform(X, y)\n    # Calculate the elapsed time\n    elapsed_time = time.time() - start\n    # Clear and close the current plot to free up resources\n    plt.cla()\n    plt.close()\n    # Return the elapsed time for the fit and transform process\n    return elapsed_time\n", "CodeBleu_score": 58.77336386683151}]}
{"instance_id": "db4143bc029f", "generation_result": [{"code": "def draw_instances(self, X, y, **kwargs):\n\t\n    # Get the alpha value for the scatter plot\n    alpha = kwargs.pop(\"alpha\", self.alpha)\n\n    # Get the color for each instance\n    colors = self.get_colors(y)\n\n    # Plot each instance on the axis\n    for xi, yi, ci in zip(X, y, colors):\n        self.ax.scatter(self.range_, xi, c=ci, alpha=alpha, **kwargs)\n\n    return self.ax\n", "CodeBleu_score": 37.044760927780736}, {"code": "def draw_instances(self, X, y, **kwargs):\n\t\n    # Get the alpha value for the scatter plot\n    alpha = kwargs.pop(\"alpha\", getattr(self, \"alpha\", 0.25))\n\n    # Plot the instances\n    for i, (x, yi) in enumerate(zip(X, y)):\n        # Get the color for the instance\n        color = self.get_colors(yi)[0]\n\n        # Plot the instance\n        self.ax.plot(self.sequence_, x, color=color, alpha=alpha, **kwargs)\n\n    # Return the axis\n    return self.ax\n\n", "CodeBleu_score": 45.493811212853295}, {"code": "def draw_instances(self, X, y, **kwargs):\n\t\n    # Get the alpha transparency value from the object's attributes or default to 0.25\n    alpha = kwargs.pop(\"alpha\", self.alpha)\n\n    # For each instance, extract the feature vector and corresponding label,\n    # determine the color based on the label, and plot the feature vector\n    # against a predefined increment sequence on the axis\n    for i, (xi, yi) in enumerate(zip(X, y)):\n        color = self.get_colors(yi)\n        self.ax.plot(\n            self.feature_increments, xi, color=color, alpha=alpha, **kwargs\n        )\n\n    # Return the axis object with the plotted instances\n    return self.ax\n", "CodeBleu_score": 38.43572824982685}]}
{"instance_id": "676433312ffe", "generation_result": [{"code": "def draw_classes(self, X, y, **kwargs):\n\t\n    # Set the alpha value from the object's attributes or default to 0.5\n    alpha = getattr(self, \"alpha\", 0.5)\n\n    # Add a column of ones to X to separate the instances visually\n    X_increments = np.hstack((np.ones((X.shape[0], 1)), X))\n\n    # Append None to the increments list to create breaks between instances\n    increments_separated = np.vstack(\n        (X_increments, np.array([None] * X_increments.shape[1]))\n    )\n\n    # Identify unique class labels in y\n    unique_classes = np.unique(y)\n\n    # Iterate over classes\n    for i, c in enumerate(unique_classes):\n        # Select the instances of X belonging to the current class\n        X_in_class = X_increments[y == c]\n\n        # Plot the instances as a single line plot with the specified color and alpha transparency\n        colors = self.get_colors(y)\n        kwargs[\"color\"] = colors[i]\n        kwargs[\"alpha\"] = alpha\n        kwargs[\"label\"] = c\n        self.ax.plot(X_in_class.flatten(), increments_separated.flatten(), **kwargs)\n\n    return self.ax\n\n", "CodeBleu_score": 41.552128688473104}, {"code": "def draw_classes(self, X, y, **kwargs):\n\t\n    # Get the alpha transparency from the object's attributes or default to 0.5\n    alpha = getattr(self, \"alpha\", 0.5)\n\n    # Add a column of ones to X to separate the instances visually\n    X = np.hstack((X, np.ones((X.shape[0], 1))))\n\n    # Append None to the increments list to create breaks between instances\n    increments_separated = []\n    for increment in self.increments:\n        increments_separated.append(increment)\n        increments_separated.append(None)\n\n    # Identify unique class labels in y\n    classes = np.unique(y)\n\n    # Iterate over the classes\n    for c in classes:\n        # Determine the color\n        color = self.get_colors(c)\n\n        # Select the instances of X belonging to class c\n        X_in_class = X[y == c]\n\n        # Plot the instances of X_in_class as a single line plot with the specified color and alpha transparency\n        self.ax.plot(X_in_class[:, 0], X_in_class[:, 1], color=color, alpha=alpha, **kwargs)\n\n    # Return the axis object with the plotted classes\n    return self.ax\n\n", "CodeBleu_score": 37.70342390333173}, {"code": "def draw_classes(self, X, y, **kwargs):\n\t\n    alpha = kwargs.pop(\"alpha\", self.alpha)\n    increments = []\n    for i, label in enumerate(self.get_classes(y)):\n        color = self.get_colors([label])[0]\n        X_in_class = X[y == label]\n        increments.append(X_in_class.shape[0])\n        self.ax.plot(np.ones(X_in_class.shape[0]) * i, X_in_class, color=color, alpha=alpha, **kwargs)\n    increments_separated = np.hstack((0, np.cumsum(increments)))\n    for i in increments_separated:\n        self.ax.axvline(i, color=\"k\", linestyle=\"--\", linewidth=1)\n\n", "CodeBleu_score": 35.196043728869206}]}
{"instance_id": "9d297b12a673", "generation_result": [{"code": "def feature_analysis(fname=\"feature_analysis.png\")\n\t\n    fig, axes = plt.subplots(ncols=2, figsize=(10, 5))\n\n    X, y = load_occupancy()\n    oz = RadViz(classes=list(y.unique()))\n    oz.fit(X, y)\n    oz.finalize()\n    axes[0].set_title(\"Occupancy\")\n\n    X, y = load_concrete()\n    oz = Rank2D(features=X.columns.tolist())\n    oz.fit(X, y)\n    oz.transform(X)\n    oz.finalize()\n    axes[1].set_title(\"Concrete\")\n\n    fig.tight_layout()\n    fig.savefig(fname)\n\n", "CodeBleu_score": 39.81296900080858}, {"code": "def feature_analysis(fname=\"feature_analysis.png\")\n\t\n    fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 6))\n    occupancy = load_occupancy()\n    oz = RadViz(ax=ax1)\n    oz.fit(occupancy.X, occupancy.y)\n    oz.finalize()\n\n    concrete = load_concrete()\n    oz = Rank2D(ax=ax2)\n    oz.fit(concrete.X, concrete.y)\n    oz.transform(concrete.X)\n    oz.finalize()\n\n    fig.tight_layout()\n    fig.savefig(fname)", "CodeBleu_score": 24.17084059531126}, {"code": "def feature_analysis(fname=\"feature_analysis.png\")\n\t\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n\n    # RadViz\n    X, y = load_occupancy()\n    oz = RadViz(classes=[\"unoccupied\", \"occupied\"])\n    oz.fit(X, y)\n    oz.finalize()\n    ax1.set_title(\"RadViz\")\n    ax1.set_xlabel(\"\")\n    ax1.set_ylabel(\"\")\n    ax1.set_xticks([])\n    ax1.set_yticks([])\n\n    # Rank2D\n    X, y = load_concrete()\n    oz = Rank2D(features=X.columns, algorithm=\"pearson\")\n    oz.fit(X, y)\n    oz.transform(X)\n    oz.finalize()\n    ax2.set_title(\"Rank2D\")\n    ax2.set_xlabel(\"\")\n    ax2.set_ylabel(\"\")\n    ax2.set_xticks([])\n    ax2.set_yticks([])\n\n    plt.tight_layout()\n    plt.savefig(fname)\n", "CodeBleu_score": 35.44755538955331}]}
{"instance_id": "fd1b4b9cd7ef", "generation_result": [{"code": "def regression(fname=\"regression.png\")\n\t\n    fig, axes = plt.subplots(ncols=2, figsize=(18, 6))\n    alphas = np.logspace(-10, 1, 400)\n\n    X, y = load_concrete()\n\n    for ax, (name, Estimator) in zip(\n        axes,\n        [\n            (\"LassoCV\", LassoCV(alphas=alphas, random_state=0)),\n            (\"RidgeCV\", RidgeCV(alphas=alphas)),\n        ],\n    ):\n        oz = PredictionError(Estimator, ax=ax)\n        oz.fit(X, y)\n        oz.score(X, y)\n        oz.finalize()\n\n    plt.tight_layout()\n    return save_fig(fname)\n\n", "CodeBleu_score": 39.630822609776104}, {"code": "def regression(fname=\"regression.png\")\n\t\n    # Create a figure with two subplots of size 18x6 inches\n    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(18, 6))\n\n    # Define a range of alpha values for regularization\n    alphas = np.logspace(-10, 1, 400)\n\n    # Load the concrete dataset and split it into training and testing sets\n    X, y = load_concrete()\n\n    # Plot the prediction error using LassoCV with the specified alphas\n    oz = PredictionError(LassoCV(alphas=alphas, cv=5))\n    oz.fit(X, y)\n    oz.score(X, y)\n    oz.finalize()\n\n    # Plot the residuals using RidgeCV with the same alphas\n    oz = ResidualsPlot(RidgeCV(alphas=alphas, cv=5))\n    oz.fit(X, y)\n    oz.score(X, y)\n    oz.finalize()\n\n    # Adjust the layout and save the figure to a specified path\n    plt.tight_layout()\n    plt.savefig(fname)\n\n    # Return the path\n    return fname\n\n", "CodeBleu_score": 37.76248512216983}, {"code": "def regression(fname=\"regression.png\")\n\t\n    fig, ax = plt.subplots(1, 2, figsize=(18, 6))\n    alphas = np.logspace(-10, 1, 400)\n\n    X, y = load_concrete()\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n\n    oz = PredictionError(LassoCV(alphas=alphas), ax=ax[0])\n    oz.fit(X_train, y_train)\n    oz.score(X_test, y_test)\n    oz.finalize()\n\n    oz = ResidualsPlot(RidgeCV(alphas=alphas), ax=ax[1])\n    oz.fit(X_train, y_train)\n    oz.score(X_test, y_test)\n    oz.finalize()\n\n    plt.tight_layout()\n    path = os.path.join(FIGURES, fname)\n    plt.savefig(path)\n    return path", "CodeBleu_score": 58.34579326963595}]}
{"instance_id": "627126918f88", "generation_result": [{"code": "def classification(fname=\"classification.png\")\n\t   fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n    X_train, X_test, y_train, y_test = load_spam(split=True)\n    nb = MultinomialNB()\n    nb.fit(X_train, y_train)\n    oz = ClassificationReport(nb, ax=axes[0])\n    oz.fit(X_train, y_train)\n    oz.score(X_test, y_test)\n    oz.finalize()\n\n    X, y = load_spam()\n    lr = LogisticRegression()\n    lr.fit(X, y)\n    oz = DiscriminationThreshold(lr, ax=axes[1])\n    oz.fit(X, y)\n    oz.finalize()\n\n    plt.tight_layout()\n    plt.savefig(fname)", "CodeBleu_score": 41.82413408249588}, {"code": "def classification(fname=\"classification.png\")\n\t   fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n\n    X_train, X_test, y_train, y_test = load_spam(split=True)\n    model = MultinomialNB()\n    model.fit(X_train, y_train)\n\n    ClassificationReport(model, classes=model.classes_, ax=ax1).fit(\n        X_train, y_train, X_test, y_test\n    )\n\n    X, y = load_spam(split=False)\n    model = LogisticRegression(solver=\"liblinear\")\n    model.fit(X, y)\n\n    DiscriminationThreshold(model, ax=ax2).fit(X, y)\n\n    plt.tight_layout()\n    plt.savefig(fname)\n\n", "CodeBleu_score": 29.837260256224685}, {"code": "def classification(fname=\"classification.png\")\n\t   fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n    X_train, X_test, y_train, y_test = load_spam(split=True)\n    model = MultinomialNB()\n    model.fit(X_train, y_train)\n    oz = ClassificationReport(model, classes=[\"ham\", \"spam\"], ax=axes[0])\n    oz.fit(X_train, y_train)\n    oz.score(X_test, y_test)\n    oz.finalize()\n\n    X, y = load_spam(split=False)\n    model = LogisticRegression()\n    model.fit(X, y)\n    oz = DiscriminationThreshold(model, ax=axes[1])\n    oz.fit(X, y)\n    oz.finalize()\n\n    plt.tight_layout()\n    plt.savefig(fname)", "CodeBleu_score": 46.70923438662034}]}
{"instance_id": "318d3a674694", "generation_result": [{"code": "def get_color_cycle()\n\t\n    try:\n        return list(matplotlib.rcParams['axes.prop_cycle'])\n    except KeyError:\n        return matplotlib.rcParams['axes.color_cycle']\n\n\n", "CodeBleu_score": 21.377666384883153}, {"code": "def get_color_cycle()\n\t\n    try:\n        from matplotlib import __version__ as mpl_version\n        if mpl_version >= '1.5.0':\n            from matplotlib import rcParams\n            return rcParams['axes.prop_cycle'].by_key()['color']\n    except KeyError:\n        pass\n    from matplotlib import rcParams\n    return rcParams['axes.color_cycle']\n", "CodeBleu_score": 32.29237732016149}, {"code": "def get_color_cycle()\n\t\n    try:\n        import matplotlib as mpl\n        if mpl.__version__ >= \"1.5.0\":\n            return mpl.rcParams['axes.prop_cycle'].by_key()['color']\n        else:\n            return mpl.rcParams['axes.color_cycle']\n    except KeyError:\n        return mpl.rcParams['axes.color_cycle']\n\n\n", "CodeBleu_score": 31.18906643099478}]}
{"instance_id": "f3fac557334c", "generation_result": [{"code": "def plot_knee_normalized(\n        self,\n    )\n\t\n    from matplotlib import pyplot as plt\n    plt.figure(figsize=(8, 8))\n    plt.plot(self.x_normalized, self.y_normalized)\n    plt.plot(self.distance_curve, color=\"red\")\n    plt.ylim(0, max(self.y_normalized) + 0.1)\n    plt.vlines(self.knee_point, 0, max(self.y_normalized), color=\"black\")\n    plt.yticks(\n        np.arange(0, max(self.y_normalized) + 0.1, 0.1),\n        np.arange(0, max(self.y_normalized) + 0.1, 0.1),\n    )\n    plt.xticks(\n        np.arange(0, max(self.x_normalized) + 0.1, 0.1),\n        np.arange(0, max(self.x_normalized) + 0.1, 0.1),\n    )\n    plt.show()", "CodeBleu_score": 57.63577590368796}, {"code": "def plot_knee_normalized(\n        self,\n    )\n\t\n    fig = plt.figure(figsize=(8, 8))\n    plt.plot(self.x_normalized, self.y_normalized)\n    plt.plot(self.x_normalized, self.distance, color=\"red\")\n    plt.ylim(min(self.y_normalized) - 0.1, max(self.y_normalized) + 0.1)\n    plt.xlim(min(self.x_normalized) - 0.1, max(self.x_normalized) + 0.1)\n    plt.vlines(self.knee_point, min(self.y_normalized), max(self.y_normalized), color=\"red\")\n    plt.yticks(np.arange(min(self.y_normalized), max(self.y_normalized) + 0.1, 0.1))\n    plt.xticks(np.arange(min(self.x_normalized), max(self.x_normalized) + 0.1, 0.1))\n    plt.show()", "CodeBleu_score": 41.266194910892814}, {"code": "def plot_knee_normalized(\n        self,\n    )\n\t\n    import matplotlib.pyplot as plt\n    plt.figure(figsize=(8, 8))\n    plt.plot(self.x_normalized, self.y_normalized)\n    plt.plot(self.distance_curve, color='red')\n    plt.vlines(self.knee, 0, 1, color='black')\n    plt.ylim(0, 1)\n    plt.xlim(0, 1)\n    plt.yticks(np.arange(0, 1.1, 0.1))\n    plt.xticks(np.arange(0, 1.1, 0.1))\n    plt.show()\n", "CodeBleu_score": 37.05537974255943}]}
{"instance_id": "b2d48aae4fce", "generation_result": [{"code": "def plot_knee(\n        self,\n    )\n\t\n    # This function plots a curve and its knee point, if it exists.\n    # It uses matplotlib to create a plot with a specified figure size of 8x8 inches.\n    # The function plots the data points defined by self.x and self.y.\n    # It then draws a vertical line at the knee point along the y-axis limits.\n    # The plot is displayed with the curve and the knee point highlighted.\n\n    plt.figure(figsize=(8, 8))\n    plt.plot(self.x, self.y)\n    plt.vlines(self.knee_x, plt.ylim()[0], plt.ylim()[1], color=\"red\")\n    plt.show()", "CodeBleu_score": 58.12253229528511}, {"code": "def plot_knee(\n        self,\n    )\n\t\n    plt.figure(figsize=(8, 8))\n    plt.plot(self.x, self.y)\n    plt.vlines(self.knee, plt.ylim()[0], plt.ylim()[1], linestyles=\"dashed\")\n    plt.show()\n", "CodeBleu_score": 62.59171772404311}, {"code": "def plot_knee(\n        self,\n    )\n\t\n    plt.figure(figsize=(8, 8))\n    plt.plot(self.x, self.y)\n    plt.vlines(self.knee, plt.ylim()[0], plt.ylim()[1])\n    plt.show()\n\n", "CodeBleu_score": 56.4215207944197}]}
{"instance_id": "4450ba2384c9", "generation_result": [{"code": "def plot_pca_interactive(data, n_components=6)\n\t\n    from sklearn.decomposition import PCA\n    from ipywidgets import interact\n    pca = PCA(n_components=n_components)\n    Xproj = pca.fit_transform(data)\n\n    def show_decomp(i=0):\n        plot_image_components(data[i], Xproj[i], pca.mean_, pca.components_)\n\n    interact(show_decomp, i=(0, data.shape[0] - 1));\n", "CodeBleu_score": 86.91920119658214}, {"code": "def plot_pca_interactive(data, n_components=6)\n\t\n    from sklearn.decomposition import PCA\n    from ipywidgets import interact\n    pca = PCA(n_components=n_components)\n    Xproj = pca.fit_transform(data)\n    def show_decomp(i=0):\n        plot_image_components(data[i], Xproj[i], pca.mean_, pca.components_)\n    interact(show_decomp, i=(0, data.shape[0] - 1));\n\n", "CodeBleu_score": 86.91920119658214}, {"code": "def plot_pca_interactive(data, n_components=6)\n\t\n    from sklearn.decomposition import PCA\n    from ipywidgets import interact\n\n    pca = PCA(n_components=n_components)\n    Xproj = pca.fit_transform(data)\n    eigenfaces = pca.components_.reshape((n_components, data.shape[1]))\n\n    def show_decomp(i=0):\n        plot_image_components(data[i], Xproj[i],\n                              pca.mean_, eigenfaces,\n                              imshape=imshape,\n                              n_components=n_components)\n\n    interact(show_decomp,\n             i=(0, data.shape[0] - 1))\n\n", "CodeBleu_score": 72.47663815172075}]}
{"instance_id": "b09a163f3ff1", "generation_result": [{"code": "def plot_iris_knn()\n\t\n    # Load the Iris dataset\n    iris = datasets.load_iris()\n    X = iris.data[:, :2]  # we only take the first two features.\n    y = iris.target\n\n    # Create color maps\n    cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n    cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n\n    # Create a k-Nearest Neighbors classifier with 5 neighbors\n    knn = neighbors.KNeighborsClassifier(n_neighbors=5)\n\n    # Train the classifier on the Iris dataset\n    knn.fit(X, y)\n\n    # Create a mesh grid over the feature space\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n                         np.linspace(y_min, y_max, 100))\n\n    # Predict the class for each point in the mesh grid\n    Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n\n    # Plot the decision boundaries\n    Z = Z.reshape(xx.shape)\n    plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n\n    # Plot the training points\n    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold,\n                edgecolor='k', s=20)\n    plt.xlabel('Sepal length')\n    plt.ylabel('Sepal width')\n    plt.axis('tight')\n    plt.show()\n\n", "CodeBleu_score": 52.03501815114912}, {"code": "def plot_iris_knn()\n\t\n    # Load the Iris dataset\n    iris = datasets.load_iris()\n    # Extract the first two features and the target labels\n    X = iris.data[:, :2]\n    y = iris.target\n    # Create a kNN classifier with 5 neighbors\n    knn = neighbors.KNeighborsClassifier(n_neighbors=5)\n    # Fit the classifier to the data\n    knn.fit(X, y)\n    # Define a mesh grid over the feature space\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1), np.arange(y_min, y_max, 0.1))\n    # Predict the class for each point in the mesh grid\n    Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n    # Reshape the predictions and display as a color plot\n    Z = Z.reshape(xx.shape)\n    pl.figure()\n    pl.pcolormesh(xx, yy, Z, cmap=pl.cm.Pastel1)\n    # Plot the training points\n    pl.scatter(X[:, 0], X[:, 1], c=y, cmap=pl.cm.Set1, edgecolor='k', s=20)\n    pl.xlabel('Sepal length')\n    pl.ylabel('Sepal width')\n    pl.axis('tight')\n    pl.show()", "CodeBleu_score": 62.200306881559044}, {"code": "def plot_iris_knn()\n\t\n    # Load the Iris dataset\n    iris = datasets.load_iris()\n    X = iris.data[:, :2]  # we only take the first two features.\n    y = iris.target\n\n    # Create color maps\n    cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n    cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n\n    # Create a kNN classifier with 5 neighbors\n    knn = neighbors.KNeighborsClassifier(n_neighbors=5)\n\n    # Train the classifier on the Iris dataset\n    knn.fit(X, y)\n\n    # Define a mesh grid over the feature space\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n                         np.linspace(y_min, y_max, 100))\n\n    # Predict the class for each point in the mesh grid\n    Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n\n    # Reshape the predictions and display as a color plot\n    Z = Z.reshape(xx.shape)\n    pl.figure()\n    pl.pcolormesh(xx, yy, Z, cmap=cmap_light)\n\n    # Plot the training points\n    pl.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold, edgecolor='k', s=20)\n    pl.xlabel('Sepal length')\n    pl.ylabel('Sepal width')\n    pl.axis('tight')\n\n    pl.show()\n\n", "CodeBleu_score": 54.88255573276967}]}
{"instance_id": "8eaee3207c09", "generation_result": [{"code": "def plot_sgd_separator()\n\t\n    # we create 50 separable points\n    X, Y = make_blobs(n_samples=50, centers=2, random_state=0, cluster_std=0.60)\n\n    # fit the model\n    clf = SGDClassifier(loss=\"hinge\", alpha=0.01, max_iter=200, fit_intercept=True)\n    clf.fit(X, Y)\n\n    # create a mesh to plot in\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n                         np.linspace(y_min, y_max, 100))\n\n    # Plot the decision boundary. For that, we will assign a color to each\n    # point in the mesh [x_min, x_max]x[y_min, y_max].\n    Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n\n    fig, ax = plt.subplots()\n    ax.contour(xx, yy, Z, levels=[-1, 0, 1], alpha=0.5,\n               linestyles=['--', '-', '--'])\n    ax.scatter(X[:, 0], X[:, 1], c=Y, cmap=plt.cm.Paired)\n\n    ax.axis('tight')\n    plt.show()\n\n", "CodeBleu_score": 41.66862701928895}, {"code": "def plot_sgd_separator()\n\t\n    # generate and plot data\n    X, y = make_blobs(n_samples=50, centers=2, random_state=0, cluster_std=0.60)\n    ax = plt.axes()\n    ax.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm)\n\n    # fit the model\n    clf = SGDClassifier(loss=\"hinge\", alpha=0.01, max_iter=200, fit_intercept=True)\n    clf.fit(X, y)\n\n    # create a mesh to plot in\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n                         np.linspace(y_min, y_max, 100))\n\n    # get the separating hyperplane\n    Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n\n    # put the result into a color plot\n    Z = Z.reshape(xx.shape)\n    ax.contour(xx, yy, Z, colors='k', levels=[-1, 0, 1], alpha=0.5,\n               linestyles=['--', '-', '--'])\n\n    # plot support vectors\n    ax.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=100,\n               linewidth=1, facecolors='none', edgecolors='k')\n    ax.axis('tight')\n    plt.show()\n\n", "CodeBleu_score": 38.10599266438332}, {"code": "def plot_sgd_separator()\n\t\n    # create the dataset\n    X, y = make_blobs(n_samples=50, centers=2, random_state=0, cluster_std=0.60)\n    # fit the classifier\n    clf = SGDClassifier(loss=\"hinge\", alpha=0.01, max_iter=200, fit_intercept=True)\n    clf.fit(X, y)\n    # create a mesh to plot in\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n                         np.linspace(y_min, y_max, 100))\n    # plot the decision function for each datapoint on the grid\n    Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    # plot the line, the points, and the nearest vectors to the plane\n    plt.figure()\n    plt.clf()\n    plt.axes([0, 0, 1, 1])\n    plt.pcolormesh(xx, yy, -Z, cmap=plt.cm.RdBu)\n    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.bwr, edgecolors='k')\n    plt.axis('tight')\n    # plot the decision boundary\n    ax = plt.gca()\n    xlim = ax.get_xlim()\n    ylim = ax.get_ylim()\n    # create grid to evaluate model\n    xx = np.linspace(xlim[0], xlim[1], 30)\n    yy = np.linspace(ylim[0], ylim[1], 30)\n    YY, XX = np.meshgrid(yy, xx)\n    xy = np.vstack([XX.ravel(), YY.ravel()]).T\n    Z = clf.decision_function(xy).reshape(XX.shape)\n    # plot decision boundary and margins\n    ax.contour(XX, YY, Z, colors='k', levels=[-1, 0, 1", "CodeBleu_score": 42.329413704348106}]}
{"instance_id": "8b54f999b4c0", "generation_result": [{"code": "def dump_svmlight_file(self, file)\n\t\n    data = np.array(self.data)\n    X = data[:, 0:2]\n    y = data[:, 2]\n    dump_svmlight_file(X, y, file)\n\n", "CodeBleu_score": 54.50515311965599}, {"code": "def dump_svmlight_file(self, file)\n\t\n    data = np.array(self.data)\n    X = data[:, 0:2]\n    y = data[:, 2]\n    dump_svmlight_file(X, y, file)\n\n", "CodeBleu_score": 54.50515311965599}, {"code": "def dump_svmlight_file(self, file)\n\t\n    data = np.array(self.data)\n    X = data[:, 0:2]\n    y = data[:, 2]\n    dump_svmlight_file(X, y, file)\n\n", "CodeBleu_score": 54.50515311965599}]}
{"instance_id": "2419013adc6e", "generation_result": [{"code": "def fit(self)\n\t   print(\"fit the model\")\n    train = np.array(self.model.data)\n    X = train[:, 0:2]\n    y = train[:, 2]\n\n    C = float(self.complexity.get())\n    gamma = float(self.gamma.get())\n    coef0 = float(self.coef0.get())\n    degree = int(self.degree.get())\n    kernel_map = {0: \"linear\", 1: \"rbf\", 2: \"poly\"}\n    if len(np.unique(y)) == 1:\n        clf = svm.OneClassSVM(kernel=kernel_map[self.kernel.get()],\n                              gamma=gamma, coef0=coef0, degree=degree)\n        clf.fit(X)\n    else:\n        clf = svm.SVC(kernel=kernel_map[self.kernel.get()], C=C,\n                      gamma=gamma, coef0=coef0, degree=degree)\n        clf.fit(X, y)\n    if hasattr(clf,'score'):\n        print(\"Accuracy:\", clf.score(X, y) * 100)\n    X1, X2, Z = self.decision_surface(clf)\n    self.model.clf = clf\n    self.model.set_surface((X1, X2, Z))\n    self.model.surface_type = self.surface_type.get()\n    self.fitted = True\n    self.model.changed(\"surface\")\n", "CodeBleu_score": 97.69395591245392}, {"code": "def fit(self)\n\t   print(\"fit the model\")\n    train = np.array(self.model.data)\n    X = train[:, 0:2]\n    y = train[:, 2]\n\n    C = float(self.complexity.get())\n    gamma = float(self.gamma.get())\n    coef0 = float(self.coef0.get())\n    degree = int(self.degree.get())\n    kernel_map = {0: \"linear\", 1: \"rbf\", 2: \"poly\"}\n    if len(np.unique(y)) == 1:\n        clf = svm.OneClassSVM(kernel=kernel_map[self.kernel.get()],\n                              gamma=gamma, coef0=coef0, degree=degree)\n        clf.fit(X)\n    else:\n        clf = svm.SVC(kernel=kernel_map[self.kernel.get()], C=C,\n                      gamma=gamma, coef0=coef0, degree=degree)\n        clf.fit(X, y)\n    if hasattr(clf,'score'):\n        print(\"Accuracy:\", clf.score(X, y) * 100)\n    X1, X2, Z = self.decision_surface(clf)\n    self.model.clf = clf\n    self.model.set_surface((X1, X2, Z))\n    self.model.surface_type = self.surface_type.get()\n    self.fitted = True\n    self.model.changed(\"surface\")\n", "CodeBleu_score": 97.69395591245392}, {"code": "def fit(self)\n\t   print(\"fit the model\")\n    train = np.array(self.model.data)\n    X = train[:, 0:2]\n    y = train[:, 2]\n\n    C = float(self.complexity.get())\n    gamma = float(self.gamma.get())\n    coef0 = float(self.coef0.get())\n    degree = int(self.degree.get())\n    kernel_map = {0: \"linear\", 1: \"rbf\", 2: \"poly\"}\n    if len(np.unique(y)) == 1:\n        clf = svm.OneClassSVM(kernel=kernel_map[self.kernel.get()],\n                              gamma=gamma, coef0=coef0, degree=degree)\n        clf.fit(X)\n    else:\n        clf = svm.SVC(kernel=kernel_map[self.kernel.get()], C=C,\n                      gamma=gamma, coef0=coef0, degree=degree)\n        clf.fit(X, y)\n    if hasattr(clf,'score'):\n        print(\"Accuracy:\", clf.score(X, y) * 100)\n    X1, X2, Z = self.decision_surface(clf)\n    self.model.clf = clf\n    self.model.set_surface((X1, X2, Z))\n    self.model.surface_type = self.surface_type.get()\n    self.fitted = True\n    self.model.changed(\"surface\")\n\n", "CodeBleu_score": 97.69395591245392}]}
{"instance_id": "e887fd2a8513", "generation_result": [{"code": "def source_decode(sourcecode, verbose=0)\n\t\n    if sourcecode.startswith(\"tpot.\"):\n        import_string = sourcecode.split(\".\")[0]\n        object_string = sourcecode.split(\".\")[1]\n    else:\n        import_string = sourcecode\n        object_string = sourcecode\n    try:\n        imported_object = import_from_string(import_string)\n    except ImportError as e:\n        if verbose > 2:\n            raise e\n        else:\n            print(\"Warning: Could not import %s.\" % import_string)\n        return import_string, object_string, None\n    return import_string, object_string, imported_object\n\n", "CodeBleu_score": 35.80534814472707}, {"code": "def source_decode(sourcecode, verbose=0)\n\t\n    if sourcecode.startswith(\"tpot.\"):\n        module_name = sourcecode.split(\".\")[0]\n        object_name = sourcecode.split(\".\")[1]\n    else:\n        module_name = sourcecode\n        object_name = sourcecode\n    try:\n        module = import_module(module_name)\n    except ImportError as e:\n        if verbose > 2:\n            raise e\n        else:\n            print(f\"Warning: Could not import module '{module_name}'.\")\n        return None, None, None\n    try:\n        obj = getattr(module, object_name)\n    except AttributeError as e:\n        if verbose > 2:\n            raise e\n        else:\n            print(f\"Warning: Could not find object '{object_name}' in module '{module_name}'.\")\n        return None, None, None\n    return module_name, object_name, obj\n", "CodeBleu_score": 42.25870510590039}, {"code": "def source_decode(sourcecode, verbose=0)\n\t\n    if sourcecode.startswith(\"tpot.\"):\n        module_name = sourcecode.split(\".\")[0]\n        object_name = sourcecode.split(\".\")[1]\n    else:\n        module_name = sourcecode\n        object_name = sourcecode\n    try:\n        module = importlib.import_module(module_name)\n        object = getattr(module, object_name)\n    except ImportError as e:\n        if verbose > 2:\n            raise e\n        else:\n            print(\"Warning: Could not import %s. Please check the import string.\" % sourcecode)\n        object = None\n    return module_name, object_name, object\n", "CodeBleu_score": 39.29395971644403}]}
{"instance_id": "5d6f629c3363", "generation_result": [{"code": "def set_sample_weight(pipeline_steps, sample_weight=None)\n\t\n    sample_weight_dict = {}\n    if sample_weight is not None:\n        for name, step in pipeline_steps:\n            if hasattr(step, \"fit\"):\n                args = inspect.getargspec(step.fit)\n                if \"sample_weight\" in args.args:\n                    sample_weight_dict[name + \"_sample_weight\"] = sample_weight\n    if sample_weight_dict:\n        return sample_weight_dict\n    else:\n        return None\n", "CodeBleu_score": 51.534727726927734}, {"code": "def set_sample_weight(pipeline_steps, sample_weight=None)\n\t\n    sample_weight_dict = {}\n    if sample_weight is not None:\n        for step in pipeline_steps:\n            fit_args = inspect.getargspec(step[1].fit).args\n            if \"sample_weight\" in fit_args:\n                sample_weight_dict[step[0] + \"_sample_weight\"] = sample_weight\n    if len(sample_weight_dict) > 0:\n        return sample_weight_dict\n    else:\n        return None\n\n", "CodeBleu_score": 41.815811246683765}, {"code": "def set_sample_weight(pipeline_steps, sample_weight=None)\n\t\n    sample_weights = {}\n    if sample_weight is not None:\n        for step in pipeline_steps:\n            fit_signature = inspect.signature(step[1].fit)\n            if \"sample_weight\" in fit_signature.parameters:\n                sample_weights[step[0] + \"_sample_weight\"] = sample_weight\n    if sample_weights:\n        return sample_weights\n    else:\n        return None\n\n", "CodeBleu_score": 35.48572039988343}]}
{"instance_id": "cae17d519818", "generation_result": [{"code": "def _summary_of_best_pipeline(self, features, target):\n\t\n    if self.fitted_pipeline_ is None:\n        raise RuntimeError(\n            \"No fitted pipeline found. Please call fit() before calling summary().\"\n        )\n    if self.verbosity >= 1:\n        print(\"Best pipeline: {}", "CodeBleu_score": 9.518268206887303}, {"code": "def _summary_of_best_pipeline(self, features, target):\n\t\n\n    # Check if an optimized pipeline exists\n    if not self._pareto_front:\n        raise RuntimeError(\n            \"No optimized pipeline found. Please ensure that TPOT has been fitted before calling this method.\"\n        )\n\n    # Compile and fit the best pipeline\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        self.best_pipeline_.fit(features, target)\n\n    # Print the best pipeline string\n    if self.verbosity >= 1:\n        print(\"", "CodeBleu_score": 17.22906503363388}, {"code": "def _summary_of_best_pipeline(self, features, target):\n\t\n    # This function provides a summary of the best pipeline identified by TPOT optimization.\n    # It first checks if an optimized pipeline exists, raising a RuntimeError with a descriptive message if not.\n    # If an optimized pipeline is available, it compiles and fits the pipeline using the provided features and target, while ignoring warnings.\n    # If verbosity is set to 1 or 2, it prints the best pipeline string, adding extra spacing if verbosity is 2 or higher.\n    # The function also compiles and fits the entire Pareto front of pipelines, storing them as fitted models for convenience.\n    if not self._pareto_front:\n        raise RuntimeError(\n            \"No optimized pipeline found. Please ensure that TPOT has been fitted to the data before calling summary().\"\n        )\n\n    if self.verbosity > 0:\n        self._pbar.write(\"\", file=self.log_file_)\n        self._pbar.write(\n            \"Best pipeline: {}", "CodeBleu_score": 12.698490528064104}]}
{"instance_id": "638683d78167", "generation_result": [{"code": "def score(self, testing_features, testing_target)\n\t    if not self._optimized:\n        raise ValueError(\n            \"Error: Please fit the pipeline first before scoring it on testing data.\"\n        )\n\n    testing_features = self._check_dataset(testing_features, testing_target)\n\n    if self._scoring_function is not None:\n        if isinstance(self._scoring_function, str):\n            scorer = SCORERS.get(self._scoring_function)\n            if scorer is None:\n                raise ValueError(\n                    \"Error: The scoring function provided is not a valid scorer.\"\n                )\n        elif callable(self._scoring_function):\n            scorer = self._scoring_function\n        else:\n            raise ValueError(\n                \"Error: The scoring function provided is not a valid scorer.\"\n            )\n\n        return scorer(self._fitted_pipeline, testing_features, testing_target)\n    else:\n        raise ValueError(\n            \"Error: Please provide a scoring function to evaluate the pipeline.\"\n        )\n\n", "CodeBleu_score": 43.57497993193199}, {"code": "def score(self, testing_features, testing_target)\n\t    if not self.fitted_:\n        raise RuntimeError(\n            \"The pipeline has not been fitted. Please call fit() first.\"\n        )\n    testing_features, testing_target = self._check_dataset(\n        testing_features, testing_target\n    )\n    if isinstance(self.scoring_function, str):\n        if self.scoring_function not in SCORERS:\n            raise ValueError(\n                \"Error: Scoring function {} not recognized. Please choose from the \"\n                \"list of available scorers: {}.\".format(\n                    self.scoring_function, list(SCORERS.keys())\n                )\n            )\n        scorer = SCORERS[self.scoring_function]\n    elif callable(self.scoring_function):\n        scorer = self.scoring_function\n    else:\n        raise ValueError(\n            \"Error: Scoring function {} not recognized. Please choose from the \"\n            \"list of available scorers: {}.\".format(\n                self.scoring_function, list(SCORERS.keys())\n            )\n        )\n    return scorer(self.fitted_pipeline_, testing_features, testing_target)\n\n", "CodeBleu_score": 50.38678165492265}, {"code": "def score(self, testing_features, testing_target)\n\t    if not self._optimized:\n        raise ValueError(\"Error: The pipeline has not been optimized.\")\n\n    testing_features, testing_target = self._check_dataset(\n        testing_features, testing_target\n    )\n\n    if isinstance(self.scoring, str):\n        scorer = SCORERS.get(self.scoring)\n        if scorer is None:\n            raise ValueError(\n                \"Error: The provided scoring function is not supported. Please \"\n                \"choose from the following list: {}\".format(\n                    \", \".join(list(SCORERS.keys()))\n                )\n            )\n    elif callable(self.scoring):\n        scorer = self.scoring\n    else:\n        raise ValueError(\n            \"Error: The provided scoring function is not supported. Please \"\n            \"choose from the following list: {}\".format(\n                \", \".join(list(SCORERS.keys()))\n            )\n        )\n\n    return scorer(self, testing_features, testing_target)", "CodeBleu_score": 43.86765457494214}]}
{"instance_id": "a04382560333", "generation_result": [{"code": " def _impute_values(self, features)\n\t\n    if self.verbosity > 1:\n        self._pbar.write(\n            \"Imputing missing values in training features\", file=self.log_file_\n        )\n    if self._imputer is None:\n        self._imputer = SimpleImputer(strategy=\"median\")\n        self._imputer.fit(features)\n    return self._imputer.transform(features)\n\n", "CodeBleu_score": 68.44232279926008}, {"code": " def _impute_values(self, features)\n\t\n    if self.verbosity > 1:\n        print(\"Imputing missing values using median strategy\")\n    if not hasattr(self, \"_imputer\"):\n        self._imputer = SimpleImputer(strategy=\"median\")\n        self._imputer.fit(features)\n    return self._imputer.transform(features)\n", "CodeBleu_score": 64.42652507048948}, {"code": " def _impute_values(self, features)\n\t\n    if self.verbosity > 1:\n        print(\"Imputing missing values\")\n    if self._imputer is None:\n        self._imputer = SimpleImputer(strategy=\"median\")\n        self._imputer.fit(features)\n    return self._imputer.transform(features)\n\n\n", "CodeBleu_score": 74.57052005194736}]}
{"instance_id": "0043d411c332", "generation_result": [{"code": "def _compile_to_sklearn(self, expr)\n\t\n    # Generate pipeline code string\n    pipeline_code = generate_pipeline_code(expr, self.operators)\n    # Evaluate the pipeline code string within a specific context\n    pipeline = eval(pipeline_code, self.context)\n    # Set the pipeline's memory\n    pipeline.memory = self.memory\n    # Set the random state recursively for each step in the pipeline\n    if self.random_state is not None:\n        set_param_recursive(pipeline, \"random_state\", self.random_state)\n    # Return the configured scikit-learn pipeline\n    return pipeline\n\n", "CodeBleu_score": 22.660892548521673}, {"code": "def _compile_to_sklearn(self, expr)\n\t\n    # Generate the pipeline code string from the expression\n    pipeline_code = generate_pipeline_code(expr, self.operators)\n\n    # Evaluate the pipeline code within a specific context\n    ctx = {}\n    exec(pipeline_code, ctx)\n    pipeline = ctx[\"pipeline\"]\n\n    # Set the pipeline's memory attribute\n    pipeline.memory = self.memory\n\n    # If a random state is specified, recursively set the random_state parameter for each step in the pipeline\n    if self.random_state is not None:\n        set_param_recursive(pipeline, \"random_state\", self.random_state)\n\n    return pipeline", "CodeBleu_score": 36.49612463997302}, {"code": "def _compile_to_sklearn(self, expr)\n\t\n    pipeline_code = generate_pipeline_code(expr)\n    pipeline = eval(pipeline_code, self.context)\n    pipeline.memory = self.memory\n    if self.random_state is not None:\n        set_param_recursive(pipeline, \"random_state\", self.random_state)\n    return pipeline\n\n", "CodeBleu_score": 21.359846475732216}]}
{"instance_id": "ca0ce462aa76", "generation_result": [{"code": "def pick_two_individuals_eligible_for_crossover(population)\n\t\n    for ind1 in population:\n        ind1_prims = set(ind1.primitives)\n        ind1_str = str(ind1)\n        for ind2 in population:\n            ind2_prims = set(ind2.primitives)\n            ind2_str = str(ind2)\n            if ind1_prims.intersection(ind2_prims) and ind1_str!= ind2_str:\n                if (ind1, ind2) in eligible_pairs or (ind2, ind1) in eligible_pairs:\n                    continue\n                eligible_pairs.add((ind1, ind2))\n                eligible_pairs.add((ind2, ind1))\n    if not eligible_pairs:\n        return None, None\n    ind1, ind2 = random.choice(list(eligible_pairs))\n    return ind1, ind2\n", "CodeBleu_score": 28.131562227068173}, {"code": "def pick_two_individuals_eligible_for_crossover(population)\n\t\n    def get_primitive_nodes(individual):\n        \"\"\"The function get_primitive_nodes(individual) takes an individual and returns a set of primitive nodes. It first converts the individual to a string representation and then uses the re module to extract all primitive nodes from the string. It returns a set of primitive nodes.\"\"\"\n        string_representation = str(individual)\n        primitive_nodes = set(re.findall(r\"\\d+\\.0\", string_representation))\n        return primitive_nodes\n    def get_eligible_pairs(population):\n        \"\"\"The function get_eligible_pairs(population) takes a population of individuals and returns a set of pairs of individuals that are eligible for crossover. It first creates sets of primitive nodes for each individual in the population. It then identifies pairs of individuals that share at least one primitive and are not identical. Eligible pairs are considered in both possible orders. The function returns a set of eligible pairs.\"\"\"\n        eligible_pairs = set()\n        for i, individual1 in enumerate(population):\n            primitive_nodes1 = get_primitive_nodes(individual1)\n            for j, individual2 in enumerate(population):\n                if i!= j:\n                    primitive_nodes2 = get_primitive_nodes(individual2)\n                    if len(primitive_nodes1.intersection(primitive_nodes2)) > 0:\n                        eligible_pairs.add((individual1, individual2))\n                        eligible_pairs.add((individual2, individual1))\n        return eligible_pairs\n    eligible_pairs = get_eligible_pairs(population)\n    if len(eligible_pairs) == 0:\n        return None, None\n    else:\n        eligible_pair = random.choice(list(eligible_pairs))\n        return eligible_pair[0], eligible_pair[1]\n", "CodeBleu_score": 26.61099699934826}, {"code": "def pick_two_individuals_eligible_for_crossover(population)\n\t\n    # Create sets of primitive nodes for each individual\n    primitive_sets = [set(individual.primitives) for individual in population]\n    # Convert each individual to a string representation\n    string_representations = [str(individual) for individual in population]\n    # Identify pairs of individuals that share at least one primitive and are not identical\n    eligible_pairs = set()\n    for i, primitive_set in enumerate(primitive_sets):\n        for j, other_primitive_set in enumerate(primitive_sets[i + 1:], i + 1):\n            if primitive_set & other_primitive_set and string_representations[i]!= string_representations[j]:\n                eligible_pairs.add((i, j))\n                eligible_pairs.add((j, i))\n    # If no eligible pairs are found, return None for both individuals\n    if not eligible_pairs:\n        return None, None\n    # Randomly select one of the eligible pairs\n    pair = random.choice(list(eligible_pairs))\n    # Return the corresponding individuals from the population\n    return population[pair[0]], population[pair[1]]\n", "CodeBleu_score": 33.32724300804589}]}
{"instance_id": "4b03c843f506", "generation_result": [{"code": "def generate_export_pipeline_code(pipeline_tree, operators)\n\t    steps = _process_operator(pipeline_tree, operators)\n    num_steps = len(steps)\n\n    if num_steps > 1:\n        formatted_steps = \",", "CodeBleu_score": 12.349309620518845}, {"code": "def generate_export_pipeline_code(pipeline_tree, operators)\n\t    steps = _process_operator(pipeline_tree, operators)\n    num_steps = len(steps)\n    if num_steps > 1:\n        formatted_steps = \",", "CodeBleu_score": 12.349309620518845}, {"code": "def generate_export_pipeline_code(pipeline_tree, operators)\n\t    steps = []\n    for operator in pipeline_tree:\n        steps.extend(_process_operator(operator, operators))\n\n    if len(steps) > 1:\n        code = \"make_pipeline({})\".format(\", \".join(steps))\n        code = _indent(code, 4)\n    else:\n        code = steps[0]\n    return code\n\n", "CodeBleu_score": 30.455361305997585}]}
{"instance_id": "e9aec2ee39a4", "generation_result": [{"code": "def _process_operator(operator, operators, depth=0)\n\t    steps = []\n    op_name = operator[0]\n\n    if op_name == \"CombineDFs\":\n        steps.append(_combine_dfs(operator[1], operator[2], operators))\n    else:\n        input_name, args = operator[1], operator[2:]\n        tpot_op = get_by_name(op_name, operators)\n\n        if input_name!= \"input_matrix\":\n            steps.extend(_process_operator(input_name, operators, depth + 1))\n\n        # If the step is an estimator and is not the last step then we must\n        # add its guess as synthetic feature(s)\n        # classification prediction for both regression and classification\n        # classification probabilities for classification if available\n        if tpot_op.root and depth > 0:\n            steps.append(\n                \"StackingEstimator(estimator={})\".format(tpot_op.export(*args))\n            )\n        else:\n            steps.append(tpot_op.export(*args))\n    return steps\n", "CodeBleu_score": 83.21845852108702}, {"code": "def _process_operator(operator, operators, depth=0)\n\t    steps = []\n    op_name = operator[0]\n\n    if op_name == \"CombineDFs\":\n        steps.append(_combine_dfs(operator[1], operator[2], operators))\n    else:\n        input_name, args = operator[1], operator[2:]\n        tpot_op = get_by_name(op_name, operators)\n\n        if input_name!= \"input_matrix\":\n            steps.extend(_process_operator(input_name, operators, depth + 1))\n\n        # If the step is an estimator and is not the last step then we must\n        # add its guess as synthetic feature(s)\n        # classification prediction for both regression and classification\n        # classification probabilities for classification if available\n        if tpot_op.root and depth > 0:\n            steps.append(\n                \"StackingEstimator(estimator={})\".format(tpot_op.export(*args))\n            )\n        else:\n            steps.append(tpot_op.export(*args))\n    return steps\n", "CodeBleu_score": 83.21845852108702}, {"code": "def _process_operator(operator, operators, depth=0)\n\t    steps = []\n    op_name = operator[0]\n\n    if op_name == \"CombineDFs\":\n        steps.append(_combine_dfs(operator[1], operator[2], operators))\n    else:\n        input_name, args = operator[1], operator[2:]\n        tpot_op = get_by_name(op_name, operators)\n\n        if input_name!= \"input_matrix\":\n            steps.extend(_process_operator(input_name, operators, depth + 1))\n\n        # If the step is an estimator and is not the last step then we must\n        # add its guess as synthetic feature(s)\n        # classification prediction for both regression and classification\n        # classification probabilities for classification if available\n        if tpot_op.root and depth > 0:\n            steps.append(\n                \"StackingEstimator(estimator={})\".format(tpot_op.export(*args))\n            )\n        else:\n            steps.append(tpot_op.export(*args))\n    return steps\n", "CodeBleu_score": 83.21845852108702}]}
{"instance_id": "0513bf969510", "generation_result": [{"code": "def transform(self, X)\n\t\n    if not self.categorical_features:\n        raise ValueError(\"No categorical features found.\")\n    return self.encoder.transform(X[:, self.categorical_features])\n", "CodeBleu_score": 13.188070461101972}, {"code": "def transform(self, X)\n\t\n    # Select categorical features\n    X_selected = self._select_categorical_features(X)\n    # Check if any categorical features are found\n    if X_selected.shape[1] == 0:\n        raise ValueError(\"No categorical features found.\")\n    # Apply one-hot encoding to categorical features\n    ohe = OneHotEncoder(sparse=False)\n    X_transformed = ohe.fit_transform(X_selected)\n    return X_transformed\n", "CodeBleu_score": 27.667012594916862}, {"code": "def transform(self, X)\n\t\n    if self.categorical_features is None:\n        raise ValueError(\"No categorical features found.\")\n    else:\n        ohe = OneHotEncoder(sparse=False)\n        X_selected = X[:, self.categorical_features]\n        return ohe.fit_transform(X_selected)\n", "CodeBleu_score": 31.129905050831475}]}
{"instance_id": "5b1bcf42ca9e", "generation_result": [{"code": "def balanced_accuracy(y_true, y_pred)\n\t\n    classes = np.unique(y_true)\n    accuracies = []\n    for c in classes:\n        y_true_c = y_true == c\n        y_pred_c = y_pred == c\n        tp = np.sum(y_true_c & y_pred_c)\n        tn = np.sum(~y_true_c & ~y_pred_c)\n        fp = np.sum(~y_true_c & y_pred_c)\n        fn = np.sum(y_true_c & ~y_pred_c)\n        if tp + fn == 0:\n            sensitivity = 1\n        else:\n            sensitivity = tp / (tp + fn)\n        if tn + fp == 0:\n            specificity = 1\n        else:\n            specificity = tn / (tn + fp)\n        class_accuracy = (sensitivity + specificity) / 2\n        accuracies.append(class_accuracy)\n    return np.mean(accuracies)\n", "CodeBleu_score": 21.14356669633203}, {"code": "def balanced_accuracy(y_true, y_pred)\n\t\n    classes = np.unique(y_true)\n    accuracies = []\n    for c in classes:\n        if c in y_true and c in y_pred:\n            tp = np.sum((y_true == c) & (y_pred == c))\n            tn = np.sum((y_true!= c) & (y_pred!= c))\n            fp = np.sum((y_true!= c) & (y_pred == c))\n            fn = np.sum((y_true == c) & (y_pred!= c))\n            if tp + fn == 0:\n                sensitivity = 1\n            else:\n                sensitivity = tp / (tp + fn)\n            if tn + fp == 0:\n                specificity = 1\n            else:\n                specificity = tn / (tn + fp)\n            accuracies.append((sensitivity + specificity) / 2)\n    return np.mean(accuracies)", "CodeBleu_score": 26.46174412061463}, {"code": "def balanced_accuracy(y_true, y_pred)\n\t\n    classes = np.unique(y_true)\n    accuracies = []\n    for c in classes:\n        true_positives = np.sum(np.logical_and(y_true == c, y_pred == c))\n        true_negatives = np.sum(np.logical_and(y_true!= c, y_pred!= c))\n        total_positives = np.sum(y_true == c)\n        total_negatives = np.sum(y_true!= c)\n        if total_positives == 0:\n            sensitivity = 1\n        else:\n            sensitivity = true_positives / total_positives\n        if total_negatives == 0:\n            specificity = 1\n        else:\n            specificity = true_negatives / total_negatives\n        class_accuracy = (sensitivity + specificity) / 2\n        accuracies.append(class_accuracy)\n    return np.mean(accuracies)\n", "CodeBleu_score": 22.99427456416059}]}
{"instance_id": "19a9ad39de0a", "generation_result": [{"code": "def transform(self, X, y=None)\n\t\n    X = check_array(X, accept_sparse=True)\n    X = X.toarray() if issparse(X) else X\n    n_samples, n_features = X.shape\n    X_transformed = np.copy(X)\n    X_transformed = np.hstack((X_transformed, np.reshape(np.count_nonzero(X, axis=1), (n_samples, 1))))\n    X_transformed = np.hstack((X_transformed, np.reshape(n_features - np.count_nonzero(X, axis=1), (n_samples, 1))))\n    return X_transformed\n", "CodeBleu_score": 29.75850183558458}, {"code": "def transform(self, X, y=None)\n\t\n\n    # Check input\n    X = check_array(X)\n\n    # Create new features\n    non_zero_count = np.count_nonzero(X, axis=1)\n    zero_count = X.shape[1] - non_zero_count\n\n    # Concatenate new features\n    X = np.concatenate((np.reshape(non_zero_count, (-1, 1)), np.reshape(zero_count, (-1, 1)), X), axis=1)\n\n    return X\n", "CodeBleu_score": 21.118587216163803}, {"code": "def transform(self, X, y=None)\n\t\n    X = check_array(X)\n    X_transformed = np.copy(X)\n    X_transformed = np.hstack((X_transformed, np.count_nonzero(X, axis=1).reshape(-1, 1)))\n    X_transformed = np.hstack((X_transformed, np.count_nonzero(X == 0, axis=1).reshape(-1, 1)))\n    return X_transformed\n\n", "CodeBleu_score": 24.93145909483827}]}
{"instance_id": "5e8db0b5d447", "generation_result": [{"code": "def _init_pretest(self, features, target)\n\t\n    self.n_classes = len(np.unique(target))\n    if self.train_size < self.n_classes:\n        self.train_size = self.n_classes\n    self.pretest_X, self.pretest_y = train_test_split(features, target, train_size=self.train_size, random_state=self.random_state)\n    if len(np.unique(self.pretest_y)) < self.n_classes:\n        for i in np.unique(target):\n            if i not in np.unique(self.pretest_y):\n                self.pretest_y = np.append(self.pretest_y, i)\n                self.pretest_X = np.append(self.pretest_X, _safe_indexing(features, np.where(target == i)[0][0]))\n                self.pretest_X = self.pretest_X.reshape(-1, features.shape[1])\n", "CodeBleu_score": 35.70929756546808}, {"code": "def _init_pretest(self, features, target)\n\t\n    self.n_classes = len(np.unique(target))\n    if self.n_classes < self.training_size:\n        raise ValueError(\n            \"The number of unique target classes must be at least the training size.\"\n        )\n    self.pretest_X, self.pretest_y = train_test_split(\n        features, target, train_size=self.training_size, random_state=self.random_state\n    )\n    if len(np.unique(self.pretest_y)) < self.n_classes:\n        self.pretest_y = _safe_indexing(\n            target,\n            np.concatenate(\n                [\n                    np.where(target == i)[0][0:1]\n                    for i in np.unique(self.pretest_y)\n                ],\n                axis=0,\n            ),\n        )\n\n", "CodeBleu_score": 36.16346912387948}, {"code": "def _init_pretest(self, features, target)\n\t\n    self.n_classes = len(np.unique(target))\n    if self.train_size < self.n_classes:\n        self.train_size = self.n_classes\n    self.pretest_X, self.pretest_y = train_test_split(features, target, train_size=self.train_size, random_state=self.random_state)\n    if not np.array_equal(np.unique(self.pretest_y), np.unique(target)):\n        self.pretest_y = _safe_indexing(target, np.unique(target))\n", "CodeBleu_score": 25.829040575585303}]}
{"instance_id": "5e3d48667396", "generation_result": [{"code": "def _to_numpy_type(dtype)\n\t\n    if dtype == tf.string:\n        return np.dtype(np.str_)\n    elif dtype == tf.float32:\n        return np.dtype(np.float32)\n    elif dtype == tf.float64:\n        return np.dtype(np.float64)\n    elif dtype == tf.int32:\n        return np.dtype(np.int32)\n    elif dtype == tf.int64:\n        return np.dtype(np.int64)\n    elif dtype == tf.bool:\n        return np.dtype(np.bool_)\n    else:\n        return dtype\n\n", "CodeBleu_score": 36.26382250156247}, {"code": "def _to_numpy_type(dtype)\n\t\n    if isinstance(dtype, tf.DType):\n        return dtype.as_numpy_dtype\n    return dtype\n\n", "CodeBleu_score": 49.19748004987954}, {"code": "def _to_numpy_type(dtype)\n\t\n    if dtype is None:\n        return None\n    elif isinstance(dtype, tf.DType):\n        return dtype.as_numpy_dtype\n    elif isinstance(dtype, np.dtype):\n        return dtype\n    else:\n        raise TypeError(\"Unsupported data type: {}\".format(dtype))\n", "CodeBleu_score": 57.16986935338956}]}
{"instance_id": "8f1120fdd463", "generation_result": [{"code": "def _dataset_as_numpy(ds, batch_size=None)\n\t\n    try:\n        if batch_size is None:\n            return ds.batch(batch_size).as_numpy_iterator()\n        else:\n            return ds.batch(batch_size).as_numpy_iterator()\n    except AttributeError:\n        # Fallback to default method\n        return iter(ds)\n", "CodeBleu_score": 11.116982710118245}, {"code": "def _dataset_as_numpy(ds, batch_size=None)\n\t\n    if batch_size is None:\n        try:\n            return ds.batch(batch_size).as_numpy_iterator()\n        except AttributeError:\n            return ds.as_numpy_iterator()\n    else:\n        try:\n            return ds.batch(batch_size).as_numpy_iterator()\n        except AttributeError:\n            return ds.batch(batch_size).make_one_shot_iterator().get_next()\n\n", "CodeBleu_score": 15.847469604752268}, {"code": "def _dataset_as_numpy(ds, batch_size=None)\n\t\n    try:\n        if batch_size:\n            ds = ds.batch(batch_size)\n        return ds.as_numpy_iterator()\n    except AttributeError:\n        # TensorFlow 1.X\n        if batch_size:\n            ds = ds.batch(batch_size)\n        return ds.make_one_shot_iterator().get_next()\n\n", "CodeBleu_score": 14.595031407112536}]}
{"instance_id": "282271955871", "generation_result": [{"code": "def get_prng(seed)\n\t\n    if not np.isscalar(seed):\n        raise TypeError(f\"Seed must be a scalar, got {type(seed)}\")\n    if not np.issubdtype(seed, np.integer):\n        raise TypeError(f\"Seed must be an integer, got {type(seed)}\")\n    if seed < 0:\n        raise ValueError(f\"Seed must be non-negative, got {seed}\")\n    if seed > 2**32 - 1:\n        raise ValueError(f\"Seed must be less than {2**32}, got {seed}\")\n    return jax.random.PRNGKey(seed)\n", "CodeBleu_score": 18.699808246746915}, {"code": "def get_prng(seed)\n\t\n    if isinstance(seed, int):\n        seed = jnp.array(seed, dtype=jnp.uint32)\n    if not jnp.isscalar(seed):\n        raise TypeError(\"Seed must be a scalar.\")\n    return jax.random.PRNGKey(seed)\n\n", "CodeBleu_score": 24.770598904516188}, {"code": "def get_prng(seed)\n\t\n    if not isinstance(seed, int):\n        raise TypeError(\"The seed must be an integer.\")\n    if seed < 0:\n        raise ValueError(\"The seed must be a non-negative integer.\")\n    return jax.random.PRNGKey(seed)\n", "CodeBleu_score": 19.10957244230143}]}
{"instance_id": "42234dccce9a", "generation_result": [{"code": "def convert_Timestamp(cls, s, dtype, pandas_type)\n\t\n    if pdt.is_datetime64_dtype(dtype):\n        return s.dt.tz_localize(cls.tz)\n    elif pdt.is_datetimetz_dtype(dtype):\n        return s.dt.tz_convert(cls.tz)\n    elif pdt.is_numeric_dtype(dtype):\n        return s.astype(dtype='datetime64[ns]').dt.tz_localize(cls.tz)\n    else:\n        return pd.to_datetime(s).dt.tz_localize(cls.tz)\n", "CodeBleu_score": 46.65444144157498}, {"code": "def convert_Timestamp(cls, s, dtype, pandas_type)\n\t\n    if pdt.is_datetime64_dtype(dtype):\n        return s.dt.tz_localize(dtype.tz)\n    elif pdt.is_datetimetz_dtype(dtype):\n        return s.dt.tz_convert(dtype.tz)\n    elif pdt.is_numeric_dtype(dtype):\n        return s.astype(dtype=dtype).dt.tz_localize(dtype.tz)\n    else:\n        return s.astype(dtype=dtype).dt.tz_localize(\"UTC\").dt.tz_convert(dtype.tz)\n", "CodeBleu_score": 46.50821966936401}, {"code": "def convert_Timestamp(cls, s, dtype, pandas_type)\n\t\n    if pdt.is_datetime64_dtype(dtype):\n        s = s.dt.tz_localize(dtype.tz)\n    elif pdt.is_datetimetz_dtype(dtype):\n        s = s.dt.tz_convert(dtype.tz)\n    elif pdt.is_numeric_dtype(dtype):\n        s = pd.to_datetime(s, unit='ns', utc=True).dt.tz_localize(dtype.tz)\n    else:\n        s = pd.to_datetime(s).dt.tz_localize(dtype.tz)\n    return s\n", "CodeBleu_score": 49.3900879442833}]}
{"instance_id": "9cee7388879b", "generation_result": [{"code": "def visit(cls, op: ops.BetweenTime, arg, lower_bound, upper_bound)\n\t\n    if arg.tz is not None:\n        lower_bound = lower_bound.tz_convert(arg.tz)\n        upper_bound = upper_bound.tz_convert(arg.tz)\n    indexer = (lower_bound <= arg) & (arg < upper_bound)\n    return to_dask_array(indexer, dtype=np.bool_)\n\n", "CodeBleu_score": 24.338129126409434}, {"code": "def visit(cls, op: ops.BetweenTime, arg, lower_bound, upper_bound)\n\t\n    if arg.index.tz is not None:\n        lower_bound = lower_bound.tz_convert(\"UTC\")\n        upper_bound = upper_bound.tz_convert(\"UTC\")\n    indexer = (arg >= lower_bound) & (arg <= upper_bound)\n    return da.from_array(indexer.values, chunks=arg.chunks)\n\n", "CodeBleu_score": 25.43537965222327}, {"code": "def visit(cls, op: ops.BetweenTime, arg, lower_bound, upper_bound)\n\t\n    if arg.tz is not None:\n        lower_bound = lower_bound.tz_convert(arg.tz)\n        upper_bound = upper_bound.tz_convert(arg.tz)\n    indexer = (lower_bound <= arg) & (arg < upper_bound)\n    return to_dask_array(indexer, dtype=bool, name=op.name)\n", "CodeBleu_score": 24.37265617588268}]}
{"instance_id": "0ab1cc8ccf71", "generation_result": [{"code": "def asseries(cls, value, like=None)\n\t\n    if isinstance(value, dd.Series):\n        return value\n    if isinstance(value, dd.Scalar):\n        return dd.from_array(value.to_delayed(), like=like)\n    if isinstance(value, pd.Series):\n        return dd.from_pandas(value, npartitions=1)\n    if like is not None:\n        if isinstance(like, tuple):\n            return tuple(asseries(cls, v, like=None) for v in like)\n        if isinstance(like, list):\n            return list(asseries(cls, v, like=None) for v in like)\n        if isinstance(like, dict):\n            return dict((k, asseries(cls, v, like=None)) for k, v in like.items())\n        return dd.from_array(like, like=like)\n    return dd.from_array(value, like=like)\n", "CodeBleu_score": 34.892233528522034}, {"code": "def asseries(cls, value, like=None)\n\t\n    if isinstance(value, dd.Series):\n        return value\n    elif isinstance(value, dd.Scalar):\n        return dd.from_array(value.to_delayed().compute(), like=like)\n    elif isinstance(value, pd.Series):\n        return dd.from_pandas(value, npartitions=1)\n    elif isinstance(value, (tuple, list, dict)):\n        return asseries(cls, like, like=value)\n    else:\n        return dd.from_array(value, like=like)\n", "CodeBleu_score": 28.827981839641925}, {"code": "def asseries(cls, value, like=None)\n\t\n    if isinstance(value, dd.Series):\n        return value\n    elif isinstance(value, dd.Scalar):\n        return dd.from_array(value, chunks=1).to_series()\n    elif isinstance(value, pd.Series):\n        return dd.from_pandas(value, npartitions=1)\n    elif like is not None:\n        if isinstance(like, tuple):\n            return dd.from_array(\n                np.full(like, value, dtype=value.dtype), chunks=like\n            ).to_series()\n        elif isinstance(like, list):\n            return dd.from_array(\n                np.full(len(like), value, dtype=value.dtype), chunks=len(like)\n            ).to_series()\n        elif isinstance(like, dict):\n            return dd.from_array(\n                np.full(len(like), value, dtype=value.dtype), chunks=len(like)\n            ).to_series()\n        else:\n            return dd.from_array(\n                np.full(len(like), value, dtype=value.dtype), chunks=len(like)\n            ).to_series()\n    else:\n        return dd.from_array(\n            np.full(len(value), value, dtype=value.dtype), chunks=len(value)\n        ).to_series()\n\n", "CodeBleu_score": 39.00170757222149}]}
{"instance_id": "926220a06a0c", "generation_result": [{"code": "def do_connect(\n        self,\n        dictionary: MutableMapping[str, dd.DataFrame] | None = None,\n    ) -> None\n\t  if dictionary is None:\n        dictionary = {}\n\n    for k, v in dictionary.items():\n        if not isinstance(v, (dd.DataFrame, pd.DataFrame)):\n            raise TypeError(\n                f\"Expected an instance of 'dask.dataframe.DataFrame' for {k!r},\"\n                f\" got an instance of '{type(v).__name__}' instead.\"\n            )\n    super().do_connect(dictionary)\n\n", "CodeBleu_score": 96.71809435326834}, {"code": "def do_connect(\n        self,\n        dictionary: MutableMapping[str, dd.DataFrame] | None = None,\n    ) -> None\n\t  if dictionary is None:\n        dictionary = {}\n\n    for k, v in dictionary.items():\n        if not isinstance(v, (dd.DataFrame, pd.DataFrame)):\n            raise TypeError(\n                f\"Expected an instance of 'dask.dataframe.DataFrame' for {k!r},\"\n                f\" got an instance of '{type(v).__name__}' instead.\"\n            )\n    super().do_connect(dictionary)\n\n", "CodeBleu_score": 96.71809435326834}, {"code": "def do_connect(\n        self,\n        dictionary: MutableMapping[str, dd.DataFrame] | None = None,\n    ) -> None\n\t  if dictionary is None:\n        dictionary = {}\n\n    for k, v in dictionary.items():\n        if not isinstance(v, (dd.DataFrame, pd.DataFrame)):\n            raise TypeError(\n                f\"Expected an instance of 'dask.dataframe.DataFrame' for {k!r},\"\n                f\" got an instance of '{type(v).__name__}' instead.\"\n            )\n    super().do_connect(dictionary)\n\n", "CodeBleu_score": 96.71809435326834}]}
{"instance_id": "95afc38c9125", "generation_result": [{"code": "def get_estimator(library_name: str, estimator_name: str)\n\t\n    classes = get_module_members(library_name)\n    if estimator_name not in classes:\n        raise ValueError(f\"Estimator {estimator_name} could not be found in module {library_name}\")\n    if len(classes[estimator_name]) > 1:\n        logger.debug(f\"Multiple classes found for estimator {estimator_name}: {classes[estimator_name]}\")\n        logger.warning(f\"Multiple classes found for estimator {estimator_name}, using {classes[estimator_name][0]}\")\n    estimator = classes[estimator_name][0]\n    if not issubclass(estimator, BaseEstimator):\n        logger.info(f\"Estimator {estimator_name} is not a subclass of sklearn's BaseEstimator\")\n    return estimator\n", "CodeBleu_score": 41.34923202551177}, {"code": "def get_estimator(library_name: str, estimator_name: str)\n\t\n    classes = get_module_members(library_name)\n    if estimator_name not in classes:\n        raise ValueError(f\"Estimator {estimator_name} not found in module {library_name}\")\n    if len(classes[estimator_name]) > 1:\n        logger.debug(f\"Multiple classes found for estimator {estimator_name} in module {library_name}: {classes[estimator_name]}. Using the first one.\")\n        logger.warning(f\"Multiple classes found for estimator {estimator_name} in module {library_name}: {classes[estimator_name]}. Using the first one.\")\n    estimator = classes[estimator_name][0]\n    if not issubclass(estimator, BaseEstimator):\n        logger.info(f\"Estimator {estimator_name} is not a subclass of sklearn.BaseEstimator.\")\n    return estimator\n", "CodeBleu_score": 41.74668772569479}, {"code": "def get_estimator(library_name: str, estimator_name: str)\n\t\n    classes = get_module_members(library_name)\n    if estimator_name not in classes:\n        raise ValueError(f\"Estimator {estimator_name} not found in module {library_name}\")\n    if len(classes[estimator_name]) > 1:\n        logger.debug(f\"Multiple classes found for estimator {estimator_name}: {classes[estimator_name]}\")\n        logger.warning(f\"Using first class found for estimator {estimator_name}: {classes[estimator_name][0]}\")\n    estimator_class = classes[estimator_name][0]\n    if not issubclass(estimator_class, BaseEstimator):\n        logger.info(f\"Class {estimator_class} is not a subclass of sklearn.BaseEstimator\")\n    return estimator_class\n", "CodeBleu_score": 38.79720512725678}]}
{"instance_id": "9562a9cc043b", "generation_result": [{"code": "def estimator_to_task(estimator_name: str) -> str\n\t\n    if estimator_name.endswith(\"Regressor\"):\n        return \"Regression\"\n    elif estimator_name.endswith(\"Classifier\"):\n        return \"Classification\"\n    elif estimator_name.endswith(\"Clusterer\"):\n        return \"Clustering\"\n    else:\n        return \"unknown\"\n\n", "CodeBleu_score": 19.306582374234424}, {"code": "def estimator_to_task(estimator_name: str) -> str\n\t\n    estimator_to_task_map = {\n        \"Classifier\": \"classification\",\n        \"Regressor\": \"regression\",\n        \"Transformer\": \"transformation\",\n        \"Clusterer\": \"clustering\",\n    }\n    if estimator_name.endswith(\"Classifier\"):\n        return estimator_to_task_map[\"Classifier\"]\n    elif estimator_name.endswith(\"Regressor\"):\n        return estimator_to_task_map[\"Regressor\"]\n    elif estimator_name.endswith(\"Transformer\"):\n        return estimator_to_task_map[\"Transformer\"]\n    elif estimator_name.endswith(\"Clusterer\"):\n        return estimator_to_task_map[\"Clusterer\"]\n    else:\n        return \"unknown\"\n\n", "CodeBleu_score": 21.7800751050185}, {"code": "def estimator_to_task(estimator_name: str) -> str\n\t\n    if estimator_name.endswith(\"Classifier\"):\n        return \"classification\"\n    elif estimator_name.endswith(\"Regressor\"):\n        return \"regression\"\n    elif estimator_name.endswith(\"Clusterer\"):\n        return \"clustering\"\n    else:\n        return \"unknown\"\n\n", "CodeBleu_score": 19.306582374234424}]}
{"instance_id": "2f50195d043a", "generation_result": [{"code": "def load_data_from_cache(data_cache: str, data_name: str) -> Dict\n\t    assert os.path.isdir(data_cache)\n    data = {}\n    for data_filename in get_filenames_by_prefix(data_cache, data_name):\n        if data_filename.endswith(\".json\"):\n            continue\n        component, extension = data_filename.split(\".\")\n        data[component] = load_data_file(\n            os.path.join(data_cache, data_filename), extension\n        )\n    return data\n\n", "CodeBleu_score": 47.46381819880382}, {"code": "def load_data_from_cache(data_cache: str, data_name: str) -> Dict\n\t    data_cache = os.path.join(data_cache, data_name)\n    if not os.path.isdir(data_cache):\n        raise ValueError(f\"Data cache {data_cache} does not exist\")\n    filenames = get_filenames_by_prefix(data_cache, \"\")\n    data = {}\n    for filename in filenames:\n        if filename.endswith(\".json\"):\n            continue\n        component, extension = filename.split(\".\")\n        data[component] = load_data_file(os.path.join(data_cache, filename), extension)\n    return data\n\n", "CodeBleu_score": 44.215658788604244}, {"code": "def load_data_from_cache(data_cache: str, data_name: str) -> Dict\n\t    assert os.path.isdir(data_cache)\n    prefix_expr = get_expr_by_prefix(data_name)\n    filenames = filter(\n        lambda x: re.search(prefix_expr, x) is not None, os.listdir(data_cache)\n    )\n    data = {}\n    for filename in filenames:\n        component, extension = filename.split(\".\")\n        if extension == \"json\":\n            continue\n        data[component] = load_data_file(\n            os.path.join(data_cache, filename), extension\n        )\n    return data\n\n", "CodeBleu_score": 44.007182621109884}]}
{"instance_id": "e0281206029e", "generation_result": [{"code": "def retrieve(url: str, filename: str) -> None\n\t\n    if isfile(filename):\n        return\n    if url.startswith(\"http\"):\n        response = requests.get(url, stream=True)\n        if response.status_code!= 200:\n            raise AssertionError(f\"Download failed with status code {response.status_code}\")\n        total_size = int(response.headers.get(\"content-length\", 0))\n        block_size = 1024\n        written_size = 0\n        with open(filename, \"wb\") as datafile:\n            for data in response.iter_content(block_size):\n                written_size += len(data)\n                datafile.write(data)\n        if total_size!= written_size:\n            raise AssertionError(\"Download size mismatch\")\n    else:\n        raise AssertionError(f\"URL {url} must start with 'http'\")\n", "CodeBleu_score": 65.78213668469397}, {"code": "def retrieve(url: str, filename: str) -> None\n\t\n    if isfile(filename):\n        return\n    if url.startswith(\"http\"):\n        response = requests.get(url)\n        if response.status_code!= 200:\n            raise AssertionError(f\"Download failed with status code {response.status_code}\")\n        total_size = int(response.headers.get(\"Content-Length\", 0))\n        block_size = 1024\n        with open(filename, \"wb\") as datafile:\n            written_size = 0\n            for data in response.iter_content(block_size):\n                datafile.write(data)\n                written_size += len(data)\n                print(f\"Downloaded {written_size} of {total_size} bytes\")\n            if written_size!= total_size:\n                raise AssertionError(f\"Downloaded size {written_size} does not match expected size {total_size}\")\n    else:\n        raise AssertionError(f\"URL {url} does not start with 'http'\")\n", "CodeBleu_score": 50.27221107971016}, {"code": "def retrieve(url: str, filename: str) -> None\n\t\n    if isfile(filename):\n        return\n    if url.startswith(\"http\"):\n        response = requests.get(url)\n        if response.status_code!= 200:\n            raise AssertionError(f\"Download failed with status code {response.status_code}\")\n        total_size = int(response.headers.get(\"Content-Length\", 0))\n        block_size = 1024\n        written_size = 0\n        with open(filename, \"wb\") as datafile:\n            for chunk in response.iter_content(block_size):\n                datafile.write(chunk)\n                written_size += len(chunk)\n                if total_size > 0:\n                    print(f\"Downloaded {written_size} of {total_size} bytes\")\n        if total_size!= written_size:\n            raise AssertionError(\"Download size does not match expected content length\")\n    else:\n        raise AssertionError(\"Invalid URL\")\n", "CodeBleu_score": 51.70018284451046}]}
{"instance_id": "a69a71cc2fe8", "generation_result": [{"code": "def load_covtype(\n    data_name: str, data_cache: str, raw_data_cache: str, dataset_params: Dict\n) -> Tuple[Dict, Dict]\n\t\n    dataset_params = dataset_params.get(data_name, {})\n    dataset_params.setdefault(\"binary\", False)\n    dataset_params.setdefault(\"split\", \"stratified\")\n    dataset_params.setdefault(\"split_ratio\", 0.2)\n    dataset_params.setdefault(\"random_state\", 0)\n    dataset_params.setdefault(\"shuffle\", True)\n    dataset_params.setdefault(\"stratify\", True)\n\n    # load data\n    data_path = os.path.join(raw_data_cache, \"covtype.data\")\n    if not os.path.exists(data_path):\n        fetch_covtype(data_cache)\n    data = pd.read_csv(data_path, header=None)\n\n    # process target\n    y = data[54].values - 1\n    if dataset_params[\"binary\"]:\n        y = y.astype(bool)\n\n    # process features\n    X = data.iloc[:, :54].values\n\n    # create description\n    description = {\n        \"n_classes\": len(np.unique(y)),\n        \"split\": dataset_params[\"split\"],\n        \"split_ratio\": dataset_params[\"split_ratio\"],\n        \"random_state\": dataset_params[\"random_state\"],\n        \"shuffle\": dataset_params[\"shuffle\"],\n        \"stratify\": dataset_params[\"stratify\"],\n    }\n\n    return X, y, description\n", "CodeBleu_score": 35.56505788749622}, {"code": "def load_covtype(\n    data_name: str, data_cache: str, raw_data_cache: str, dataset_params: Dict\n) -> Tuple[Dict, Dict]\n\t\n\n    # Get data\n    data = fetch_covtype(data_cache=data_cache, raw_data_cache=raw_data_cache)\n    X = data.data\n    y = data.target\n\n    # Adjust labels to be zero-indexed\n    y -= 1\n\n    # Convert to binary if necessary\n    if dataset_params.get(\"binary\", False):\n        y[y!= 0] = 1\n\n    # Create description dictionary\n    description = {\n        \"num_classes\": len(np.unique(y)),\n        \"default_split_params\": {\"test_size\": 0.2, \"random_state\": 42},\n    }\n\n    return X, y, description\n\n", "CodeBleu_score": 43.396427522703945}, {"code": "def load_covtype(\n    data_name: str, data_cache: str, raw_data_cache: str, dataset_params: Dict\n) -> Tuple[Dict, Dict]\n\t\n\n    # Download the dataset from the UCI repository\n    dataset_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/covtype/covtype.data.gz\"\n    dataset_path = os.path.join(data_cache, \"covtype.data\")\n    download_dataset(dataset_url, dataset_path)\n\n    # Load the dataset features and labels\n    dataset_features = load_covtype_features(dataset_path)\n    dataset_labels = load_covtype_labels(dataset_path)\n\n    # Adjust the labels to be zero-indexed\n    dataset_labels -= 1\n\n    # Optionally convert the labels to binary\n    if dataset_params.get(\"binary\"):\n        dataset_labels = (dataset_labels > 2).astype(int)\n\n    # Create a description dictionary\n    description = {\n        \"num_classes\": np.unique(dataset_labels).shape[0],\n        \"split_params\": {\"train_size\": 0.8, \"val_size\": 0.1, \"test_size\": 0.1},\n    }\n\n    # Return the dataset and its description\n    return {\"features\": dataset_features, \"labels\": dataset_labels}, description\n\n", "CodeBleu_score": 39.599816200062534}]}
{"instance_id": "129cbd2529ef", "generation_result": [{"code": "def flatten_list(input_list: List, ensure_type_homogeneity: bool = False) -> List\n\t    output_list = list()\n    # iteration with inner recursion\n    for value in input_list:\n        if isinstance(value, list):\n            inner_flat_list = flatten_list(value)\n            for inner_value in inner_flat_list:\n                output_list.append(inner_value)\n        else:\n            output_list.append(value)\n    if ensure_type_homogeneity:\n        ensure_list_types_homogeneity(output_list)\n    return output_list\n\n", "CodeBleu_score": 82.22123475884622}, {"code": "def flatten_list(input_list: List, ensure_type_homogeneity: bool = False) -> List\n\t    output_list = list()\n    # iteration with inner recursion\n    for value in input_list:\n        if isinstance(value, list):\n            inner_flat_list = flatten_list(value)\n            for inner_value in inner_flat_list:\n                output_list.append(inner_value)\n        else:\n            output_list.append(value)\n    if ensure_type_homogeneity:\n        ensure_list_types_homogeneity(output_list)\n    return output_list", "CodeBleu_score": 82.22123475884622}, {"code": "def flatten_list(input_list: List, ensure_type_homogeneity: bool = False) -> List\n\t    output_list = list()\n    for value in input_list:\n        if isinstance(value, list):\n            inner_flat_list = flatten_list(value)\n            for inner_value in inner_flat_list:\n                output_list.append(inner_value)\n        else:\n            output_list.append(value)\n    if ensure_type_homogeneity:\n        ensure_list_types_homogeneity(output_list)\n    return output_list", "CodeBleu_score": 82.22123475884622}]}
{"instance_id": "2d38c2c60b59", "generation_result": [{"code": "def logit(\n    x: np.ndarray,\n    /,\n    *,\n    eps: Optional[float] = None,\n    complex_mode: Literal[\"split\", \"magnitude\", \"jax\"] = \"jax\",\n    out: Optional[np.ndarray] = None,\n)\n\t\n    if eps is None:\n        eps = 0\n    if complex_mode == \"split\":\n        return np.log(x / (1 - x))\n    elif complex_mode == \"magnitude\":\n        return np.log(np.abs(x) / (1 - np.abs(x)))\n    elif complex_mode == \"jax\":\n        return jax.numpy.log(jax.numpy.abs(x) / (1 - jax.numpy.abs(x)))\n    else:\n        raise ValueError(f\"Invalid value for complex_mode: {complex_mode}\")\n    if out is not None:\n        out[:] = np.log(np.clip(x, eps, 1 - eps))\n    else:\n        return np.log(np.clip(x, eps, 1 - eps))\n\n", "CodeBleu_score": 39.99592838544544}, {"code": "def logit(\n    x: np.ndarray,\n    /,\n    *,\n    eps: Optional[float] = None,\n    complex_mode: Literal[\"split\", \"magnitude\", \"jax\"] = \"jax\",\n    out: Optional[np.ndarray] = None,\n)\n\t\n    if eps is None:\n        eps = 0.0\n    if complex_mode == \"split\":\n        real = np.log(x.real) - np.log(1 - x.real)\n        imag = np.log(x.imag) - np.log(1 - x.imag)\n        return np.array(real, imag, dtype=x.dtype)\n    elif complex_mode == \"magnitude\":\n        return np.log(x) - np.log(1 - x)\n    elif complex_mode == \"jax\":\n        return np.log(x) - np.log1p(-x)\n    else:\n        raise ValueError(f\"Invalid complex_mode: {complex_mode}\")\n    if out is None:\n        return out\n    else:\n        return out\n\n", "CodeBleu_score": 36.29170838776105}, {"code": "def logit(\n    x: np.ndarray,\n    /,\n    *,\n    eps: Optional[float] = None,\n    complex_mode: Literal[\"split\", \"magnitude\", \"jax\"] = \"jax\",\n    out: Optional[np.ndarray] = None,\n)\n\t\n    if eps is None:\n        eps = 0.0\n    if complex_mode == \"split\":\n        real_part = np.log(np.clip(x.real, eps, 1 - eps) / (1 - np.clip(x.real, eps, 1 - eps)))\n        imag_part = np.log(np.clip(x.imag, eps, 1 - eps) / (1 - np.clip(x.imag, eps, 1 - eps)))\n        return np.array(real_part + 1j * imag_part, dtype=x.dtype, copy=False, order=\"K\", subok=True, ndmin=x.ndim)\n    if complex_mode == \"magnitude\":\n        x = np.abs(x)\n        return np.log(np.clip(x, eps, 1 - eps) / (1 - np.clip(x, eps, 1 - eps)))\n    if complex_mode == \"jax\":\n        return jnp.log(jnp.clip(x, eps, 1 - eps) / (1 - jnp.clip(x, eps, 1 - eps)))\n    raise ValueError(f\"Invalid value for complex_mode: {complex_mode}\")\n", "CodeBleu_score": 42.620838882360225}]}
{"instance_id": "205e296f8621", "generation_result": [{"code": "def hardshrink(\n    x: np.ndarray, /, *, lambd: float = 0.5, out: Optional[np.ndarray] = None\n) -> np.ndarray\n\t\n    if out is not None:\n        out[:] = x\n        out[np.abs(x) <= lambd] = 0\n        return out\n    return np.where(np.abs(x) <= lambd, 0, x)\n\n", "CodeBleu_score": 32.741237914757036}, {"code": "def hardshrink(\n    x: np.ndarray, /, *, lambd: float = 0.5, out: Optional[np.ndarray] = None\n) -> np.ndarray\n\t\n    return np.where(np.abs(x) <= lambd, 0, x)\n", "CodeBleu_score": 23.021761690299734}, {"code": "def hardshrink(\n    x: np.ndarray, /, *, lambd: float = 0.5, out: Optional[np.ndarray] = None\n) -> np.ndarray\n\t\n    return np.where(np.abs(x) <= lambd, 0, x, out=out)\n\n", "CodeBleu_score": 23.825718385836865}]}
{"instance_id": "5b133f6ebe2d", "generation_result": [{"code": "def vorbis_window(\n    window_length: np.ndarray,\n    *,\n    dtype: np.dtype = np.float32,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n\t\n    window = np.zeros(window_length, dtype=dtype)\n    for i in range(window_length):\n        window[i] = round(np.sin((i + 0.5) / window_length * np.pi) ** 2, 4)\n    return window\n\n", "CodeBleu_score": 36.55475402604692}, {"code": "def vorbis_window(\n    window_length: np.ndarray,\n    *,\n    dtype: np.dtype = np.float32,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n\t\n    window = np.zeros(window_length, dtype=dtype)\n    for i in range(window_length):\n        window[i] = round(np.sin(np.pi * i / window_length) ** 2)\n    return window\n\n", "CodeBleu_score": 33.872747505263504}, {"code": "def vorbis_window(\n    window_length: np.ndarray,\n    *,\n    dtype: np.dtype = np.float32,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n\t\n    window = np.empty(window_length, dtype=dtype)\n    for i in range(window_length):\n        window[i] = np.sin(np.pi * i / window_length) ** 2\n    if out is None:\n        return window\n    else:\n        out[:] = window\n        return out\n\n", "CodeBleu_score": 35.39977963762419}]}
{"instance_id": "473296f771a9", "generation_result": [{"code": "def polyval(coeffs: np.ndarray, x: np.ndarray) -> np.ndarray\n\t\n    with ivy.PreciseMode(True):\n        promoted_type = ivy.promote_types(ivy.dtype(coeffs[0]), ivy.dtype(x[0]))\n    result = np.polyval(coeffs, x)\n    result = np.asarray(result, np.dtype(promoted_type))\n    return result\n\n", "CodeBleu_score": 94.57815771555306}, {"code": "def polyval(coeffs: np.ndarray, x: np.ndarray) -> np.ndarray\n\t\n    with ivy.PreciseMode(True):\n        promoted_type = ivy.promote_types(ivy.dtype(coeffs[0]), ivy.dtype(x[0]))\n    result = np.polyval(coeffs, x)\n    result = np.asarray(result, np.dtype(promoted_type))\n    return result\n", "CodeBleu_score": 94.57815771555306}, {"code": "def polyval(coeffs: np.ndarray, x: np.ndarray) -> np.ndarray\n\t\n    with ivy.PreciseMode(True):\n        promoted_type = ivy.promote_types(ivy.dtype(coeffs[0]), ivy.dtype(x[0]))\n    result = np.polyval(coeffs, x)\n    result = np.asarray(result, np.dtype(promoted_type))\n    return result\n\n", "CodeBleu_score": 94.57815771555306}]}
{"instance_id": "b8455221cb2a", "generation_result": [{"code": "def amax(\n    x: np.ndarray,\n    /,\n    *,\n    axis: Optional[Union[int, Sequence[int]]] = None,\n    keepdims: bool = False,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n\t\n    axis = tuple(axis) if isinstance(axis, list) else axis\n    ret = np.amax(a=x, axis=axis, out=out, keepdims=keepdims)\n    return np.asarray(ret) if np.isscalar(ret) else ret\n\n", "CodeBleu_score": 78.35765070891931}, {"code": "def amax(\n    x: np.ndarray,\n    /,\n    *,\n    axis: Optional[Union[int, Sequence[int]]] = None,\n    keepdims: bool = False,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n\t\n    axis = tuple(axis) if isinstance(axis, list) else axis\n    ret = np.amax(a=x, axis=axis, out=out, keepdims=keepdims)\n    return np.asarray(ret) if np.isscalar(ret) else ret\n\n", "CodeBleu_score": 78.35765070891931}, {"code": "def amax(\n    x: np.ndarray,\n    /,\n    *,\n    axis: Optional[Union[int, Sequence[int]]] = None,\n    keepdims: bool = False,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n\t\n    axis = tuple(axis) if isinstance(axis, list) else axis\n    ret = np.amax(a=x, axis=axis, out=out, keepdims=keepdims)\n    return np.asarray(ret) if np.isscalar(ret) else ret\n\n", "CodeBleu_score": 78.35765070891931}]}
{"instance_id": "f72489f434a3", "generation_result": [{"code": "def copysign(\n    x1: npt.ArrayLike,\n    x2: npt.ArrayLike,\n    /,\n    *,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n\t\n    x1, x2 = promote_types_of_inputs(x1, x2)\n    if not ivy.is_float_dtype(x1):\n        x1 = x1.astype(ivy.default_float_dtype())\n        x2 = x2.astype(ivy.default_float_dtype())\n    return np.copysign(x1, x2, out=out)", "CodeBleu_score": 66.8587924544778}, {"code": "def copysign(\n    x1: npt.ArrayLike,\n    x2: npt.ArrayLike,\n    /,\n    *,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n\t\n    x1 = ivy.to_numpy(x1)\n    x2 = ivy.to_numpy(x2)\n    if not ivy.is_float_dtype(x1.dtype):\n        x1 = x1.astype(ivy.default_float_dtype())\n    if not ivy.is_float_dtype(x2.dtype):\n        x2 = x2.astype(ivy.default_float_dtype())\n    x1, x2 = promote_types_of_inputs(x1, x2)\n    return np.copysign(x1, x2, out=out)", "CodeBleu_score": 61.85941889531439}, {"code": "def copysign(\n    x1: npt.ArrayLike,\n    x2: npt.ArrayLike,\n    /,\n    *,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n\t\n    x1 = ivy.from_numpy(x1)\n    x2 = ivy.from_numpy(x2)\n    if not ivy.is_float_dtype(x1.dtype):\n        x1 = x1.astype(ivy.default_float_dtype())\n    if not ivy.is_float_dtype(x2.dtype):\n        x2 = x2.astype(ivy.default_float_dtype())\n    x1, x2 = promote_types_of_inputs(x1, x2)\n    return np.copysign(x1, x2, out=out)", "CodeBleu_score": 61.67762184377313}]}
{"instance_id": "55c0d77e4dc2", "generation_result": [{"code": "def count_nonzero(\n    a: np.ndarray,\n    /,\n    *,\n    axis: Optional[Union[int, Tuple[int, ...]]] = None,\n    keepdims: bool = False,\n    dtype: Optional[np.dtype] = None,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n\t    if isinstance(axis, int):\n        axis = (axis,)\n    if out is not None:\n        out = out.astype(dtype)\n    if np.isscalar(np.count_nonzero(a, axis=axis, keepdims=keepdims, dtype=dtype, out=out)):\n        return np.array(np.count_nonzero(a, axis=axis, keepdims=keepdims, dtype=dtype, out=out), dtype=dtype)\n    else:\n        return np.count_nonzero(a, axis=axis, keepdims=keepdims, dtype=dtype, out=out)", "CodeBleu_score": 54.10638531702684}, {"code": "def count_nonzero(\n    a: np.ndarray,\n    /,\n    *,\n    axis: Optional[Union[int, Tuple[int, ...]]] = None,\n    keepdims: bool = False,\n    dtype: Optional[np.dtype] = None,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n\t    if isinstance(a, np.ndarray):\n        if isinstance(axis, int):\n            axis = (axis,)\n        if isinstance(axis, tuple):\n            if len(axis) == 1:\n                if isinstance(axis[0], int):\n                    if axis[0] < 0:\n                        axis = (a.ndim + axis[0],)\n                    if axis[0] < a.ndim:\n                        if keepdims:\n                            if dtype is None:\n                                return np.array(\n                                    np.count_nonzero(a, axis=axis, keepdims=keepdims),\n                                    dtype=dtype,\n                                )\n                            else:\n                                return np.array(\n                                    np.count_nonzero(a, axis=axis, keepdims=keepdims),\n                                    dtype=dtype,\n                                )\n                        else:\n                            if dtype is None:\n                                return np.array(\n                                    np.count_nonzero(a, axis=axis, keepdims=keepdims),\n                                    dtype=dtype,\n                                )\n                            else:\n                                return np.array(\n                                    np.count_nonzero(a, axis=axis, keepdims=keepdims),\n                                    dtype=dtype,\n                                )\n                    else:\n                        raise ValueError(\"axis is out of bounds\")\n                else:\n                    raise TypeError(\"axis must be an integer\")\n            else:\n                raise ValueError(\"axis must be an integer\")\n        else:\n            raise TypeError(\"axis must be an integer\")\n    else:\n        raise TypeError(\"a must be an ndarray\")\n\n", "CodeBleu_score": 44.98948700101381}, {"code": "def count_nonzero(\n    a: np.ndarray,\n    /,\n    *,\n    axis: Optional[Union[int, Tuple[int, ...]]] = None,\n    keepdims: bool = False,\n    dtype: Optional[np.dtype] = None,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n\t    if isinstance(axis, int):\n        axis = (axis,)\n    if isinstance(a, (int, float, complex, str, bool)):\n        a = np.array(a, dtype=dtype)\n    if isinstance(a, np.ndarray):\n        if out is not None:\n            a = np.count_nonzero(a, axis=axis, keepdims=keepdims, dtype=dtype, out=out)\n        else:\n            a = np.count_nonzero(a, axis=axis, keepdims=keepdims, dtype=dtype)\n    if isinstance(a, (int, float, complex, str, bool)):\n        a = np.array(a, dtype=dtype)\n    return a\n", "CodeBleu_score": 51.26917099007845}]}
{"instance_id": "784bf7d24fc2", "generation_result": [{"code": "def gradient(\n    x: np.ndarray,\n    /,\n    *,\n    spacing: Union[int, list, tuple] = 1,\n    axis: Optional[Union[int, list, tuple]] = None,\n    edge_order: int = 1,\n) -> Union[np.ndarray, List[np.ndarray]]\n\t\n    if type(spacing) in (int, float):\n        return np.gradient(x, spacing, axis=axis, edge_order=edge_order)\n    return np.gradient(x, *spacing, axis=axis, edge_order=edge_order)\n", "CodeBleu_score": 96.56449344642135}, {"code": "def gradient(\n    x: np.ndarray,\n    /,\n    *,\n    spacing: Union[int, list, tuple] = 1,\n    axis: Optional[Union[int, list, tuple]] = None,\n    edge_order: int = 1,\n) -> Union[np.ndarray, List[np.ndarray]]\n\t\n    if type(spacing) in (int, float):\n        return np.gradient(x, spacing, axis=axis, edge_order=edge_order)\n    return np.gradient(x, *spacing, axis=axis, edge_order=edge_order)\n", "CodeBleu_score": 96.56449344642135}, {"code": "def gradient(\n    x: np.ndarray,\n    /,\n    *,\n    spacing: Union[int, list, tuple] = 1,\n    axis: Optional[Union[int, list, tuple]] = None,\n    edge_order: int = 1,\n) -> Union[np.ndarray, List[np.ndarray]]\n\t\n    if type(spacing) in (int, float):\n        return np.gradient(x, spacing, axis=axis, edge_order=edge_order)\n    return np.gradient(x, *spacing, axis=axis, edge_order=edge_order)", "CodeBleu_score": 96.56449344642135}]}
{"instance_id": "efa0a51af6bc", "generation_result": [{"code": "def xlogy(\n    x: np.ndarray, y: np.ndarray, /, *, out: Optional[np.ndarray] = None\n) -> np.ndarray\n\t\n    if out is None:\n        out = np.empty_like(x)\n    out.fill(0.0)\n    mask = x!= 0\n    if mask.any():\n        out[mask] = x[mask] * np.log(y[mask])\n    return out\n", "CodeBleu_score": 44.77657563401972}, {"code": "def xlogy(\n    x: np.ndarray, y: np.ndarray, /, *, out: Optional[np.ndarray] = None\n) -> np.ndarray\n\t\n    if np.all(x == 0):\n        return 0.0\n    else:\n        return np.log(y) * x", "CodeBleu_score": 47.22008711524066}, {"code": "def xlogy(\n    x: np.ndarray, y: np.ndarray, /, *, out: Optional[np.ndarray] = None\n) -> np.ndarray\n\t\n    if out is None:\n        out = np.empty_like(x)\n    if np.all(x == 0):\n        return 0.0\n    else:\n        return np.multiply(x, np.log(y), out=out)\n\n", "CodeBleu_score": 53.714666332827065}]}
{"instance_id": "ac1645284260", "generation_result": [{"code": "def _determine_depth_max_pooling(x, kernel, strides, dims, data_format=\"channel_last\")\n\tx_shape = x.shape\n    kernel, strides, depth_pooling = _depth_max_pooling_helper(\n        x_shape, kernel, strides, dims, data_format\n    )\n    if depth_pooling:\n        # Transpose the input tensor to reorder the dimensions.\n        if data_format == \"channel_last\":\n            x = np.transpose(x, (0, 3, 1, 2))\n        else:\n            x = np.transpose(x, (0, 2, 3, 1))\n    return x, kernel, strides, depth_pooling\n\n", "CodeBleu_score": 48.439300131161545}, {"code": "def _determine_depth_max_pooling(x, kernel, strides, dims, data_format=\"channel_last\")\n\t# Determine depth pooling.\n    # We assume that the kernel and the data have the same data_format.\n    x_shape = x.shape\n    kernel, strides, depth_pooling = _depth_max_pooling_helper(\n        x_shape, kernel, strides, dims, data_format\n    )\n    if depth_pooling:\n        # Transpose the input tensor to reorder its dimensions.\n        x = _transpose_for_depth_pooling(x, data_format)\n    return x, kernel, strides, depth_pooling\n\n", "CodeBleu_score": 43.200023085008276}, {"code": "def _determine_depth_max_pooling(x, kernel, strides, dims, data_format=\"channel_last\")\n\tx_shape = K.int_shape(x)\n    if len(x_shape)!= dims + 2:\n        raise ValueError(\n            \"Inputs should have rank \" + str(dims + 2) + \" but has rank \" + str(len(x_shape))\n        )\n    kernel, strides, depth_pooling = _depth_max_pooling_helper(\n        kernel, strides, dims, data_format, x_shape\n    )\n    if depth_pooling:\n        if data_format == \"channel_last\":\n            x = K.permute_dimensions(x, (0, 3, 1, 2))\n        else:\n            x = K.permute_dimensions(x, (0, 2, 3, 1))\n    return x, kernel, strides, depth_pooling\n\n", "CodeBleu_score": 45.16332272071755}]}
{"instance_id": "18ff0a4f7550", "generation_result": [{"code": "def dropout1d(\n    x: np.ndarray,\n    prob: float,\n    /,\n    *,\n    training: bool = True,\n    data_format: str = \"NWC\",\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n\t\n    if training:\n        if data_format == \"NCW\":\n            x = np.transpose(x, (0, 2, 1))\n        shape = x.shape\n        if len(shape) == 3:\n            batch_size, length, channels = shape\n        else:\n            length, channels = shape\n            batch_size = 1\n        mask = binomial(1, 1 - prob, (batch_size, length, channels))\n        x = x * mask * (1 / (1 - prob))\n        if data_format == \"NCW\":\n            x = np.transpose(x, (0, 2, 1))\n    if out is not None:\n        out[:] = x\n    return x\n\n", "CodeBleu_score": 47.51626489291688}, {"code": "def dropout1d(\n    x: np.ndarray,\n    prob: float,\n    /,\n    *,\n    training: bool = True,\n    data_format: str = \"NWC\",\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n\t\n    if training:\n        if data_format == \"NCW\":\n            x = np.transpose(x, (0, 2, 1))\n        shape = x.shape\n        batched = len(shape) > 2\n        if batched:\n            shape = shape[1:]\n        mask = np.random.binomial(1, 1 - prob, shape)\n        x = x * mask / (1 - prob)\n        if data_format == \"NCW\":\n            x = np.transpose(x, (0, 2, 1))\n    return x\n\n", "CodeBleu_score": 53.91719029926551}, {"code": "def dropout1d(\n    x: np.ndarray,\n    prob: float,\n    /,\n    *,\n    training: bool = True,\n    data_format: str = \"NWC\",\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n\t\n    if not isinstance(x, np.ndarray):\n        raise TypeError(\"x must be a NumPy array\")\n    if not isinstance(prob, float):\n        raise TypeError(\"prob must be a float\")\n    if not isinstance(training, bool):\n        raise TypeError(\"training must be a boolean\")\n    if not isinstance(data_format, str):\n        raise TypeError(\"data_format must be a string\")\n    if not isinstance(out, (np.ndarray, type(None))):\n        raise TypeError(\"out must be a NumPy array or None\")\n    if not 0.0 <= prob <= 1.0:\n        raise ValueError(\"prob must be between 0.0 and 1.0\")\n    if data_format not in (\"NWC\", \"NCW\"):\n        raise ValueError(\"data_format must be 'NWC' or 'NCW'\")\n    if out is not None and not isinstance(out, np.ndarray):\n        raise TypeError(\"out must be a NumPy array\")\n    if out is not None and not out.shape == x.shape:\n        raise ValueError(\"out must have the same shape as x\")\n    if training:\n        if data_format == \"NCW\":\n            x = np.transpose(x, (0, 2, 1))\n        n_samples, n_channels, n_width = x.shape\n        mask = np.where(\n            binomial(1, 1 - prob, n_samples * n_channels * n_width).reshape(\n                n_samples, n_channels, n_width\n            ),\n            1 / (1 - prob),\n            0,\n        )\n        x *= mask\n        if data_format == \"NCW\":\n            x = np.transpose(x, (0, 2, 1))\n    if out is not None:\n        out[:] = x\n        return out\n    return x\n\n", "CodeBleu_score": 40.2248846101}]}
{"instance_id": "0ce45eb7cbd9", "generation_result": [{"code": "def top_k(\n    x: np.ndarray,\n    k: int,\n    /,\n    *,\n    axis: int = -1,\n    largest: bool = True,\n    sorted: bool = True,\n    out: Optional[Tuple[np.ndarray, np.ndarray]] = None,\n) -> Tuple[np.ndarray, np.ndarray]\n\t\n    if x.ndim == 0:\n        raise ValueError(\"x must have at least one dimension\")\n    if axis < -x.ndim or axis >= x.ndim:\n        raise ValueError(\"axis is out of bounds\")\n    if k < 0:\n        raise ValueError(\"k must be non-negative\")\n    if k > x.shape[axis]:\n        raise ValueError(\"k cannot be greater than the size of the specified axis\")\n    if out is not None and len(out)!= 2:\n        raise ValueError(\"out must have length 2\")\n    if out is not None and out[0].shape!= out[1].shape:\n        raise ValueError(\"out arrays must have the same shape\")\n    if out is not None and out[0].shape[axis]!= k:\n        raise ValueError(\"out arrays must have the same size along the specified axis\")\n    if out is not None and out[0].ndim!= x.ndim:\n        raise ValueError(\"out arrays must have the same number of dimensions as x\")\n    if out is not None and out[0].dtype!= x.dtype:\n        raise ValueError(\"out arrays must have the same dtype as x\")\n    if out is not None and out[1].dtype!= np.intp:\n        raise ValueError(\"out arrays must have dtype np.intp for indices\")\n    if largest:\n        topk_res = np.partition(x, -k, axis=axis)[:k]\n        ind = np.argpartition(x, -k, axis=axis)[:k]\n    else:\n        topk_res = np.partition(x, k - 1, axis=axis)[k - 1 :]\n        ind = np.argpartition(x, k - 1, axis=axis)[k - 1 :]\n    if sorted:\n        topk_res = np.sort(topk_res, axis=axis)\n        ind = np.take_along_axis(np.arange(x.shape[axis]), ind, axis=axis)\n    if out is None:\n        return topk_res, ind\n    else:\n        out[0][...] = topk_res\n        out[1][...] = ind\n        return out\n\n", "CodeBleu_score": 39.932585450999355}, {"code": "def top_k(\n    x: np.ndarray,\n    k: int,\n    /,\n    *,\n    axis: int = -1,\n    largest: bool = True,\n    sorted: bool = True,\n    out: Optional[Tuple[np.ndarray, np.ndarray]] = None,\n) -> Tuple[np.ndarray, np.ndarray]\n\t\n    if not isinstance(x, np.ndarray):\n        raise TypeError(\"x must be a NumPy array.\")\n    if not isinstance(k, int):\n        raise TypeError(\"k must be an integer.\")\n    if not isinstance(axis, int):\n        raise TypeError(\"axis must be an integer.\")\n    if not isinstance(largest, bool):\n        raise TypeError(\"largest must be a boolean.\")\n    if not isinstance(sorted, bool):\n        raise TypeError(\"sorted must be a boolean.\")\n    if out is not None:\n        if not isinstance(out, tuple):\n            raise TypeError(\"out must be a tuple.\")\n        if len(out)!= 2:\n            raise ValueError(\"out must have length 2.\")\n        if not isinstance(out[0], np.ndarray):\n            raise TypeError(\"out[0] must be a NumPy array.\")\n        if not isinstance(out[1], np.ndarray):\n            raise TypeError(\"out[1] must be a NumPy array.\")\n    if axis < -x.ndim or axis >= x.ndim:\n        raise ValueError(\"axis is out of bounds for the input array.\")\n    if k < 0 or k > x.shape[axis]:\n        raise ValueError(\"k is out of bounds for the input array.\")\n    if out is not None:\n        if out[0].shape!= x.shape:\n            raise ValueError(\"out[0] has the wrong shape.\")\n        if out[1].shape!= x.shape:\n            raise ValueError(\"out[1] has the wrong shape.\")\n    if largest:\n        topk_res = np.sort(x, axis=axis)[..., -k:]\n    else:\n        topk_res = np.sort(x, axis=axis)[..., :k]\n    if sorted:\n        if largest:\n            idx = np.argsort(topk_res, axis=axis)[..., ::-1]\n        else:\n            idx = np.argsort(topk_res, axis=axis)\n    else:\n        idx = np.arange(topk_res.shape[axis])[..., np.newaxis]\n    if out is None:\n        return topk_res, idx\n    else:\n        out[0][...] = topk_res\n        out[1][...] = idx\n        return out\n\n", "CodeBleu_score": 38.94957183029666}, {"code": "def top_k(\n    x: np.ndarray,\n    k: int,\n    /,\n    *,\n    axis: int = -1,\n    largest: bool = True,\n    sorted: bool = True,\n    out: Optional[Tuple[np.ndarray, np.ndarray]] = None,\n) -> Tuple[np.ndarray, np.ndarray]\n\t\n    if out is not None:\n        values, indices = out\n    else:\n        values = np.empty(x.shape, dtype=x.dtype)\n        indices = np.empty(x.shape, dtype=np.int32)\n    if axis < 0:\n        axis += x.ndim\n    if axis == 0:\n        if largest:\n            indices[0] = np.argpartition(x[0], -k)[-k:]\n            values[0] = x[0, indices[0]]\n        else:\n            indices[0] = np.argpartition(x[0], k)[:k]\n            values[0] = x[0, indices[0]]\n        for i in range(1, x.shape[0]):\n            if largest:\n                indices[i] = np.argpartition(x[i], -k)[-k:]\n                values[i] = x[i, indices[i]]\n            else:\n                indices[i] = np.argpartition(x[i], k)[:k]\n                values[i] = x[i, indices[i]]\n    else:\n        if largest:\n            indices[..., 0] = np.argpartition(x[..., 0], -k, axis=axis)[..., -k:]\n            values[..., 0] = x[..., indices[..., 0]]\n        else:\n            indices[..., 0] = np.argpartition(x[..., 0], k, axis=axis)[..., :k]\n            values[..., 0] = x[..., indices[..., 0]]\n        for i in range(1, x.shape[axis]):\n            if largest:\n                indices[..., i] = np.argpartition(x[..., i], -k, axis=axis)[..., -k:]\n                values[..., i] = x[..., indices[..., i]]\n            else:\n                indices[..., i] = np.argpartition(x[..., i], k, axis=axis)[..., :k]\n                values[..., i] = x[..., indices[..., i]]\n    if sorted:\n        if largest:\n            np.sort(values, axis=axis)\n            np.sort(indices, axis=axis)\n        else:\n            np.sort(values, axis=axis)\n            np.sort(indices, axis=axis)\n    return values, indices", "CodeBleu_score": 36.22563050091362}]}
{"instance_id": "1424f9541998", "generation_result": [{"code": "def l1_normalize(\n    x: np.ndarray,\n    /,\n    *,\n    axis: Optional[int] = None,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n\t\n    if axis is None:\n        norm = np.sum(np.abs(x.reshape(-1)))\n        denorm = np.ones_like(x) * norm\n    else:\n        norm = np.sum(np.abs(x), axis=axis)\n        denorm = np.divide(norm, np.abs(x) + 1e-8)\n    return np.divide(x, denorm, out=out)\n\n", "CodeBleu_score": 65.15199851296578}, {"code": "def l1_normalize(\n    x: np.ndarray,\n    /,\n    *,\n    axis: Optional[int] = None,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n\t\n    if axis is None:\n        norm = np.sum(np.abs(x.reshape(-1)))\n        denormalization = np.ones_like(x) * norm\n    else:\n        norm = np.sum(np.abs(x), axis=axis)\n        denormalization = np.divide(norm, np.abs(x) + 1e-12)\n    return np.divide(x, denormalization, out=out)\n", "CodeBleu_score": 65.27246973443015}, {"code": "def l1_normalize(\n    x: np.ndarray,\n    /,\n    *,\n    axis: Optional[int] = None,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n\t\n    if axis is None:\n        norm = np.sum(np.abs(x.reshape(-1)))\n        denormalization = np.full(x.shape, norm)\n    else:\n        norm = np.sum(np.abs(x), axis=axis)\n        denormalization = norm / (np.abs(x) + np.finfo(x.dtype).eps)\n    return np.divide(x, denormalization, out=out)", "CodeBleu_score": 57.86531833751856}]}
{"instance_id": "4c3a386a1f05", "generation_result": [{"code": "def beta(\n    alpha: Union[float, np.ndarray],\n    beta: Union[float, np.ndarray],\n    /,\n    *,\n    shape: Optional[Union[ivy.NativeShape, Sequence[int]]] = None,\n    device: Optional[str] = None,\n    dtype: Optional[np.dtype] = None,\n    seed: Optional[int] = None,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n\t\n    shape = _check_bounds_and_get_shape(alpha, beta, shape).shape\n    if seed is not None:\n        np.random.seed(seed)\n    return np.asarray(np.random.beta(alpha, beta, shape), dtype=dtype)\n\n", "CodeBleu_score": 90.33571882792663}, {"code": "def beta(\n    alpha: Union[float, np.ndarray],\n    beta: Union[float, np.ndarray],\n    /,\n    *,\n    shape: Optional[Union[ivy.NativeShape, Sequence[int]]] = None,\n    device: Optional[str] = None,\n    dtype: Optional[np.dtype] = None,\n    seed: Optional[int] = None,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n\t\n    shape = _check_bounds_and_get_shape(alpha, beta, shape).shape\n    if seed is not None:\n        np.random.seed(seed)\n    return np.asarray(np.random.beta(alpha, beta, shape), dtype=dtype)\n\n", "CodeBleu_score": 90.33571882792663}, {"code": "def beta(\n    alpha: Union[float, np.ndarray],\n    beta: Union[float, np.ndarray],\n    /,\n    *,\n    shape: Optional[Union[ivy.NativeShape, Sequence[int]]] = None,\n    device: Optional[str] = None,\n    dtype: Optional[np.dtype] = None,\n    seed: Optional[int] = None,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n\t\n    shape = _check_bounds_and_get_shape(alpha, beta, shape).shape\n    if seed is not None:\n        np.random.seed(seed)\n    return np.asarray(np.random.beta(alpha, beta, shape), dtype=dtype)\n\n", "CodeBleu_score": 90.33571882792663}]}
{"instance_id": "271200a8b261", "generation_result": [{"code": "def invert_permutation(\n    x: Union[np.ndarray, list, tuple],\n    /,\n) -> np.ndarray\n\t\n    inverse = np.argsort(np.argsort(x))\n    return inverse\n\n", "CodeBleu_score": 24.240990182200417}, {"code": "def invert_permutation(\n    x: Union[np.ndarray, list, tuple],\n    /,\n) -> np.ndarray\n\t\n    inverse = np.argsort(np.argsort(x))\n    return inverse\n", "CodeBleu_score": 24.240990182200417}, {"code": "def invert_permutation(\n    x: Union[np.ndarray, list, tuple],\n    /,\n) -> np.ndarray\n\t\n    inverse = np.argsort(np.argsort(x))\n    return inverse\n\n", "CodeBleu_score": 24.240990182200417}]}
{"instance_id": "040d73b37229", "generation_result": [{"code": "def plot_medical_charts() -> None\n\t\n    # Set style\n    sns.set()\n\n    # Create figure and subplots\n    fig, (ax1, ax2, ax3, ax4) = plt.subplots(4, 1, figsize=(10, 10))\n    fig.subplots_adjust(hspace=0.5)\n\n    # Plot\n    plot_monthly_vomit(ax1, data)\n    plot_days_between_vomit(ax2, data)\n    plot_daycare_days(ax3, data)\n    plot_doctor_visit_monthly(ax4, data)\n\n    # Export figure\n    export_figure(fig, \"medical_charts.png\")\n\n", "CodeBleu_score": 26.041383127697333}, {"code": "def plot_medical_charts() -> None\n\t\n    # Set style\n    sns.set()\n\n    # Create figure with subplots\n    fig, (ax1, ax2, ax3, ax4) = plt.subplots(4, 1)\n\n    # Plot\n    plot_monthly_vomit(ax1, medical_data)\n    plot_days_between_vomit(ax2, medical_data)\n    plot_daycare_days(ax3, medical_data)\n    plot_doctor_visit_monthly(ax4, medical_data)\n\n    # Adjust layout\n    fig.subplots_adjust(hspace=0.5)\n\n    # Export figure\n    export_figure(fig, \"medical_charts.png\")\n\n", "CodeBleu_score": 27.637938671450176}, {"code": "def plot_medical_charts() -> None\n\t\n    # Set style\n    sns.set()\n\n    # Create figure and subplots\n    fig, (ax1, ax2, ax3, ax4) = plt.subplots(4, 1, figsize=(15, 15))\n    plot_monthly_vomit(ax1, data)\n    plot_days_between_vomit(ax2, data)\n    plot_daycare_days(ax3, data)\n    plot_doctor_visit_monthly(ax4, data)\n\n    # Adjust layout\n    fig.subplots_adjust(hspace=0.5)\n\n    # Export figure\n    export_figure(fig, \"medical_charts.png\")", "CodeBleu_score": 26.013429313435683}]}
{"instance_id": "7061f4d0c3fc", "generation_result": [{"code": "def plot_feeding_24h_viz() -> None\n\t\n\n    # Import and extract data\n    data = pd.read_csv(\"data/feeding_data.csv\")\n    data = parse_raw_data(data, [\"Feeding Timestamp\"])\n\n    # Set up plot\n    sns.set_style(\"darkgrid\")\n\n    # Calculate offset from birthday\n    offset = data.loc[data[\"Feeding Type\"] == \"Solid\", \"day_number\"].iloc[-1]\n\n    # Plot data\n    fig_ax = plt.figure().add_subplot()\n    fig_ax.scatter(\n        x=data.loc[data[\"Feeding Type\"] == \"Bottle\", \"timestamp_hour\"],\n        y=data.loc[data[\"Feeding Type\"] == \"Bottle\", \"day_number\"] - offset,\n        c=\"red\",\n        label=\"Bottle\",\n    )\n    fig_ax.scatter(\n        x=data.loc[data[\"Feeding Type\"] == \"Solid\", \"timestamp_hour\"],\n        y=data.loc[data[\"Feeding Type\"] == \"Solid\", \"day_number\"] - offset,\n        c=\"blue\",\n        label=\"Solid\",\n    )\n    plt.legend()\n\n    # Calculate end date\n    end_date = get_end_date(data, first_year_only=False)\n\n    # Format plot\n    plt.xlim(0, 24)\n    plt.ylim(-end_date, 0)\n    plt.xlabel(\"Hour\")\n    plt.ylabel(\"Day\")\n    plt.title(\"24-Hour Feeding Visualization\")\n\n    # Export figure\n    export_figure(fig_ax, \"24-Hour Feeding Visualization\")\n\n", "CodeBleu_score": 27.60661274218042}, {"code": "def plot_feeding_24h_viz() -> None\n\t\n    # Import data\n    data = pd.read_csv(\"data/feeding_data.csv\")\n\n    # Parse data\n    data = parse_raw_data(data, key=[\"Timestamp\"])\n\n    # Set up plot\n    sns.set_style(\"darkgrid\")\n\n    # Compute offset\n    offset = data[data[\"Feeding Type\"] == \"Solid\"][\"day_number\"].iloc[0] - 1\n\n    # Plot\n    fig_ax = plt.figure(figsize=(10, 5))\n    fig_ax.add_subplot(111)\n    plt.scatter(\n        data[data[\"Feeding Type\"] == \"Bottle\"][\"timestamp_hour\"] + offset,\n        data[data[\"Feeding Type\"] == \"Bottle\"][\"day_number\"],\n        color=\"red\",\n        label=\"Bottle\",\n    )\n    plt.scatter(\n        data[data[\"Feeding Type\"] == \"Solid\"][\"timestamp_hour\"] + offset,\n        data[data[\"Feeding Type\"] == \"Solid\"][\"day_number\"],\n        color=\"blue\",\n        label=\"Solid\",\n    )\n    plt.legend()\n\n    # Assign the end date. Either 365 or actual day number.\n    end_date = get_end_date(data, first_year_only=False)\n\n    # Format plot\n    format_24h_week_plot(end_date)\n\n    # Export figure\n    export_figure(fig_ax, \"feeding_24h_viz\")", "CodeBleu_score": 24.052325438688936}, {"code": "def plot_feeding_24h_viz() -> None\n\t\n\n    # Import data\n    data = pd.read_csv(\n        \"data/feeding_data.csv\",\n        parse_dates=[\"Date\"],\n        index_col=\"Date\",\n        infer_datetime_format=True,\n    )\n\n    # Extract bottle and solid feeding data\n    bottle_feeding_data = parse_raw_data(\n        data, key=[\"Bottle Feeding Time\", \"Bottle Feeding Date\"]\n    )\n    solid_feeding_data = parse_raw_data(\n        data, key=[\"Solid Feeding Time\", \"Solid Feeding Date\"]\n    )\n\n    # Set up plot\n    sns.set(style=\"darkgrid\")\n    fig_ax = plt.figure().add_subplot()\n\n    # Calculate offset from birthday\n    offset = (\n        solid_feeding_data[\"Date\"].iloc[-1]\n        - solid_feeding_data[\"Date\"].iloc[0]\n        + pd.Timedelta(days=1)\n    )\n\n    # Plot bottle and solid feeding data\n    fig_ax.scatter(\n        bottle_feeding_data[\"timestamp_hour\"],\n        bottle_feeding_data[\"day_number\"] - offset.days,\n        color=\"red\",\n        label=\"Bottle\",\n    )\n    fig_ax.scatter(\n        solid_feeding_data[\"timestamp_hour\"],\n        solid_feeding_data[\"day_number\"] - offset.days,\n        color=\"blue\",\n        label=\"Solid\",\n    )\n    plt.legend()\n\n    # Set end date\n    end_date = get_end_date(solid_feeding_data, first_year_only=False)\n\n    # Format plot\n    format_24h_week_plot(fig_ax, end_date)\n\n    # Export figure\n    export_figure(fig_ax, \"feeding_24h_viz\")\n\n", "CodeBleu_score": 32.52799205494436}]}
{"instance_id": "6c1ffa63d122", "generation_result": [{"code": "def plot(self, ax, task: ClassificationTask = ClassificationTask.Result, **kwargs):\n\t\n    # Retrieve target counts for the specified task\n    target_counts, target_names = self._get_target_counts(task)\n\n    # Create a bar plot on the provided axis\n    sns.barplot(x=target_names, y=target_counts, ax=ax, **kwargs)\n\n    # Set plot labels and title\n    ax.set_xlabel('Target')\n    ax.set_ylabel('Count')\n    ax.set_title(f'Target Counts for {task.name}')\n\n    # Rotate x-axis labels if necessary\n    if len(target_names) > 5:\n        ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n\n    # Display the plot\n    plt.show()\n", "CodeBleu_score": 37.086314091574735}, {"code": "def plot(self, ax, task: ClassificationTask = ClassificationTask.Result, **kwargs):\n\t\n    target_counts, target_names = self._get_target_counts(task)\n    sns.barplot(x=target_names, y=target_counts, ax=ax, **kwargs)\n    ax.set_xlabel('Target')\n    ax.set_ylabel('Count')\n    ax.set_title(f'{task.name} Counts')\n\n", "CodeBleu_score": 41.6307011798465}, {"code": "def plot(self, ax, task: ClassificationTask = ClassificationTask.Result, **kwargs):\n\t\n    target_counts, target_names = self._get_target_counts(task)\n    sns.barplot(x=target_names, y=target_counts, ax=ax, **kwargs)\n    ax.set_title(f'{task.name} distribution')\n    ax.set_xlabel('Target')\n    ax.set_ylabel('Count')\n", "CodeBleu_score": 41.29950987585723}]}
{"instance_id": "2f81a906bad5", "generation_result": [{"code": "def render_to_rgb(figure)\n\t\n    canvas = plt_backend_agg.FigureCanvasAgg(figure)\n    canvas.draw()\n    data = np.frombuffer(canvas.buffer_rgba(), dtype=np.uint8)\n    width, height = figure.get_size_inches() * figure.dpi\n    data = data.reshape(int(height), int(width), 4)\n    data = data[:, :, :3]\n    if close:\n        plt.close(figure)\n    return data.transpose(2, 0, 1)\n", "CodeBleu_score": 23.48982608500583}, {"code": "def render_to_rgb(figure)\n\t\n    if isinstance(figure, list):\n        return np.stack([render_to_rgb(fig) for fig in figure])\n    canvas = plt_backend_agg.FigureCanvasAgg(figure)\n    canvas.draw()\n    data = np.frombuffer(canvas.buffer_rgba(), dtype=np.uint8)\n    width, height = figure.get_size_inches() * figure.dpi\n    data = data.reshape(int(height), int(width), 4)\n    data = data[:, :, :3]\n    if figure.get_figwidth() < figure.get_figheight():\n        data = np.moveaxis(data, 0, 1)\n    plt.close(figure)\n    return data\n", "CodeBleu_score": 51.4685587439059}, {"code": "def render_to_rgb(figure)\n\t\n    from matplotlib.backends import backend_agg as plt_backend_agg\n    import numpy as np\n    import matplotlib.pyplot as plt\n\n    def _render_to_rgb(fig):\n        canvas = plt_backend_agg.FigureCanvasAgg(fig)\n        canvas.draw()\n        data = np.frombuffer(canvas.buffer_rgba(), dtype=np.uint8)\n        width, height = fig.canvas.get_width_height()\n        data = data.reshape((height, width, 4))\n        return data[:, :, :3]\n\n    if isinstance(figure, list):\n        images = [_render_to_rgb(fig) for fig in figure]\n        return np.stack(images)\n    else:\n        return _render_to_rgb(figure)\n", "CodeBleu_score": 48.58816324339309}]}
{"instance_id": "c3037b01569c", "generation_result": [{"code": "def beholder_pytorch()\n\t\n    # Create a Beholder object with a specified log directory\n    beholder = beholder_lib.Beholder(\"logs/beholder_pytorch\")\n    for i in range(1000):\n        # Generate random tensors and associated names\n        tensor1 = randn(10, 10)\n        tensor2 = randn(10, 10)\n        name1 = \"Tensor 1\"\n        name2 = \"Tensor 2\"\n        # Update the Beholder visualization with the generated tensors\n        beholder.update(tensor_and_name(tensor1, name1), tensor_and_name(tensor2, name2), i)\n        # Sleep briefly and print the current iteration index\n        time.sleep(0.01)\n        print(i)\n", "CodeBleu_score": 23.67502745000694}, {"code": "def beholder_pytorch()\n\t\n    # Create a Beholder object with a specified log directory\n    beholder = beholder_lib.Beholder(logdir='logs/pytorch')\n\n    # Loop 1000 times\n    for i in range(1000):\n        # Generate two lists of random tensors with associated names\n        tensor_list = [torch.randn(10, 10) for _ in range(10)]\n        name_list = [f'tensor_{i}' for i in range(10)]\n\n        # Update the Beholder visualization with the generated tensors and a random frame\n        beholder.update(tensor_list, name_list, frame=np.random.randint(1000))\n\n        # Sleep briefly and print the current iteration index\n        time.sleep(0.01)\n        print(f'Iteration {i}')\n\n", "CodeBleu_score": 27.025072577644693}, {"code": "def beholder_pytorch()\n\t\n    # Define a log directory for Beholder to save its visualizations\n    log_dir = \"beholder_logs/pytorch\"\n\n    # Initialize a Beholder object\n    beholder = beholder_lib.Beholder(log_dir)\n\n    # Loop 1000 times\n    for i in range(1000):\n        # Generate two lists of random tensors with associated names\n        tensors = [\n            tensor_and_name(randn(10, 10), \"random_tensor_1\"),\n            tensor_and_name(randn(10, 10), \"random_tensor_2\"),\n        ]\n\n        # Update the Beholder visualization with the generated tensors and a random frame\n        beholder.update(tensors, frame=randint(100))\n\n        # Sleep briefly\n        time.sleep(0.01)\n\n        # Print the current iteration index\n        print(i)", "CodeBleu_score": 29.00215488953373}]}
{"instance_id": "cd57b5de8a88", "generation_result": [{"code": "def log_pr_data(self, tag, summary, num_thresholds, step=None)\n\t\n    tensor_data = summary.value[0].tensor\n    tensor_shape = tensor_data.tensor_shape\n    tensor_dims = tensor_shape.dim\n    tensor_dims = [dim.size for dim in tensor_dims]\n    tensor_data = tensor_data.float_val\n    tensor_data = np.array(tensor_data)\n    tensor_data = tensor_data.reshape(tensor_dims)\n    thresholds = np.linspace(0, 1, num_thresholds)\n    tp = np.sum(tensor_data[:, :, 0, 0], axis=0)\n    fp = np.sum(tensor_data[:, :, 0, 1], axis=0)\n    tn = np.sum(tensor_data[:, :, 1, 0], axis=0)\n    fn = np.sum(tensor_data[:, :, 1, 1], axis=0)\n    precision = tp / (tp + fp)\n    recall = tp / (tp + fn)\n    pr_data = {\n        \"thresholds\": thresholds.tolist(),\n        \"tp\": tp.tolist(),\n        \"fp\": fp.tolist(),\n        \"tn\": tn.tolist(),\n        \"fn\": fn.tolist(),\n        \"precision\": precision.tolist(),\n        \"recall\": recall.tolist(),\n    }\n    self.log_asset_data(tag, pr_data, step=step)\n\n", "CodeBleu_score": 24.051025094209777}, {"code": "def log_pr_data(self, tag, summary, num_thresholds, step=None)\n\t\n    # Extract tensor data\n    tensor = summary.value[0].tensor\n    # Reshape data based on tensor dimensions\n    data = tensor.float_val if tensor.float_val else tensor.int_val\n    dims = tensor.tensor_shape.dim\n    shape = [d.size for d in dims]\n    data = np.reshape(data, shape)\n    # Compute thresholds\n    thresholds = np.linspace(0, 1, num_thresholds).tolist()\n    # Split data into TP, FP, TN, FN, precision, recall\n    tp = np.vsplit(data, 2)[0]\n    fp = np.vsplit(data, 2)[1]\n    tn = np.vsplit(fp, 2)[0]\n    fn = np.vsplit(fp, 2)[1]\n    precision = tp / (tp + fp)\n    recall = tp / (tp + fn)\n    # Flatten data\n    tp = tp.flatten().tolist()\n    fp = fp.flatten().tolist()\n    tn = tn.flatten().tolist()\n    fn = fn.flatten().tolist()\n    precision = precision.flatten().tolist()\n    recall = recall.flatten().tolist()\n    # Store data in dictionary\n    pr_data = {\n        \"thresholds\": thresholds,\n        \"tp\": tp,\n        \"fp\": fp,\n        \"tn\": tn,\n        \"fn\": fn,\n        \"precision\": precision,\n        \"recall\": recall,\n    }\n    # Log data\n    self.log_asset_data(pr_data, tag, step=step)", "CodeBleu_score": 32.02961000050921}, {"code": "def log_pr_data(self, tag, summary, num_thresholds, step=None)\n\t\n    tensor = summary.value\n    tensor_data = tensor.tensor_content\n    tensor_data = np.frombuffer(tensor_data, dtype=np.float32)\n    tensor_data = tensor_data.reshape(tensor.tensor_shape.dim)\n    thresholds = np.linspace(0, 1, num_thresholds)\n    tp = tensor_data[0]\n    fp = tensor_data[1]\n    tn = tensor_data[2]\n    fn = tensor_data[3]\n    precision = tp / (tp + fp)\n    recall = tp / (tp + fn)\n    pr_data = {\n        \"precision\": precision.flatten().tolist(),\n        \"recall\": recall.flatten().tolist(),\n        \"thresholds\": thresholds.tolist(),\n        \"tag\": tag,\n    }\n    self.log_asset_data(pr_data, step=step)\n", "CodeBleu_score": 31.364803256983148}]}
{"instance_id": "7c4ea9beecc9", "generation_result": [{"code": "def load_mnist(images, labels, num)\n\t\n    f_images = gzip.open(images, 'rb')\n    f_labels = gzip.open(labels, 'rb')\n\n    f_images.read(16)\n    f_labels.read(8)\n\n    data = np.zeros((num, 784))\n    target = np.zeros((num, 1))\n\n    for i in range(num):\n        for j in range(28):\n            for k in range(28):\n                data[i][28*j+k] = ord(f_images.read(1))\n        target[i] = ord(f_labels.read(1))\n\n    f_images.close()\n    f_labels.close()\n\n    return data, target", "CodeBleu_score": 44.77802016417185}, {"code": "def load_mnist(images, labels, num)\n\t\n    # Open the images and labels files\n    f_images = gzip.open(images, 'rb')\n    f_labels = gzip.open(labels, 'rb')\n\n    # Read the header bytes of the images file\n    f_images.read(4)\n    num_images = int.from_bytes(f_images.read(4), 'big')\n    rows = int.from_bytes(f_images.read(4), 'big')\n    cols = int.from_bytes(f_images.read(4), 'big')\n\n    # Read the header bytes of the labels file\n    f_labels.read(4)\n    num_labels = int.from_bytes(f_labels.read(4), 'big')\n\n    # Initialize empty arrays for the data and target labels\n    data = np.zeros((num, rows * cols), dtype=np.uint8)\n    target = np.zeros((num,), dtype=np.uint8)\n\n    # Read the image and label data byte by byte\n    for i in range(num):\n        for j in range(rows * cols):\n            data[i, j] = ord(f_images.read(1))\n        target[i] = ord(f_labels.read(1))\n\n    # Close the files\n    f_images.close()\n    f_labels.close()\n\n    # Return the data and target arrays\n    return data, target\n", "CodeBleu_score": 47.061015939858905}, {"code": "def load_mnist(images, labels, num)\n\t\n    import gzip\n    import numpy as np\n\n    # Open the images and labels files\n    f_images = gzip.open(images, 'rb')\n    f_labels = gzip.open(labels, 'rb')\n\n    # Read the header bytes\n    f_images.read(4)\n    num_images = int.from_bytes(f_images.read(4), 'big')\n    rows = int.from_bytes(f_images.read(4), 'big')\n    cols = int.from_bytes(f_images.read(4), 'big')\n    f_labels.read(4)\n    num_labels = int.from_bytes(f_labels.read(4), 'big')\n\n    # Initialize empty arrays for the data and target labels\n    data = np.zeros((num, rows*cols), dtype=np.uint8)\n    target = np.zeros((num, 1), dtype=np.uint8)\n\n    # Read the image and label data byte by byte\n    for i in range(num):\n        for j in range(rows*cols):\n            data[i][j] = ord(f_images.read(1))\n        target[i] = ord(f_labels.read(1))\n\n    # Close the files\n    f_images.close()\n    f_labels.close()\n\n    # Reshape the data array to 2D\n    data = data.reshape((num, rows, cols))\n\n    # Return the data and target arrays\n    return data, target\n\n", "CodeBleu_score": 40.076535819237705}]}
{"instance_id": "bf4f1c828b25", "generation_result": [{"code": "def compute_stats(self, ldf: LuxDataFrame)\n\t\n    # TODO: Check if the index is unique\n    ldf.unique_values = {}\n    ldf.min_max = {}\n    ldf.cardinality = {}\n    ldf.length = len(ldf)\n    for column in ldf.columns:\n        if is_datetime_dtype(ldf[column]):\n            ldf.unique_values[column.strftime(\"%Y-%m-%d %H:%M:%S\")] = ldf[column].unique()\n            ldf.cardinality[column.strftime(\"%Y-%m-%d %H:%M:%S\")] = len(ldf[column].unique())\n        elif is_numeric_dtype(ldf[column]):\n            ldf.unique_values[column] = ldf[column].unique()\n            ldf.cardinality[column] = len(ldf[column].unique())\n            ldf.min_max[column] = (ldf[column].min(), ldf[column].max())\n        else:\n            ldf.unique_values[column] = ldf[column].unique()\n            ldf.cardinality[column] = len(ldf[column].unique())\n    if not is_integer_dtype(ldf.index):\n        ldf.unique_values[\"__index__\"] = ldf.index.unique()\n        ldf.cardinality[\"__index__\"] = len(ldf.index.unique())\n", "CodeBleu_score": 37.62029032554913}, {"code": "def compute_stats(self, ldf: LuxDataFrame)\n\t\n    self.min = {}\n    self.max = {}\n    self.unique = {}\n    self.cardinality = {}\n    self.data_type = {}\n    self.data_type_lookup = {}\n    self.length = len(ldf)\n    for column in ldf.columns:\n        if isinstance(column, pd.DatetimeIndex):\n            # if the column is a DatetimeIndex, use the string representation as the key\n            key = str(column)\n        else:\n            # otherwise, use the column name as the key\n            key = column\n        if is_datetime_dtype(ldf[column]):\n            # if the column is a datetime column, use the string representation as the key\n            self.data_type[key] = \"temporal\"\n        elif is_numeric_dtype(ldf[column]):\n            # if the column is a numeric column, use the string representation as the key\n            self.data_type[key] = \"quantitative\"\n            self.min[key] = min(ldf[column])\n            self.max[key] = max(ldf[column])\n        else:\n            # otherwise, use the string representation as the key\n            self.data_type[key] = \"nominal\"\n        # compute unique values and cardinality for the column\n        self.unique[key] = unique(ldf[column])\n        self.cardinality[key] = len(self.unique[key])\n        # add the key to the data_type_lookup dictionary\n        self.data_type_lookup[key] = self.data_type[key]\n    # if the index is not of integer type, compute unique values and cardinality for the index as well\n    if not is_integer_dtype(ldf.index):\n        self.data_type[\"_row_index\"] = \"nominal\"\n        self.unique[\"_row_index\"] = unique(ldf.index)\n        self.cardinality[\"_row_index\"] = len(self.unique[\"_row_index\"])\n        self.data_type_lookup[\"_row_index\"] = self.data_type[\"_row_index\"]\n", "CodeBleu_score": 31.848117319239254}, {"code": "def compute_stats(self, ldf: LuxDataFrame)\n\t\n    self.data = {}\n    self.data[\"length\"] = len(ldf)\n    self.data[\"cardinality\"] = {}\n    self.data[\"unique\"] = {}\n    self.data[\"min\"] = {}\n    self.data[\"max\"] = {}\n    for column in ldf.columns:\n        if is_datetime_dtype(ldf[column]):\n            self.data[\"cardinality\"][column] = len(ldf[column].dt.strftime(\"%Y-%m-%d %H:%M:%S\").unique())\n            self.data[\"unique\"][column] = ldf[column].dt.strftime(\"%Y-%m-%d %H:%M:%S\").unique()\n        elif is_numeric_dtype(ldf[column]):\n            self.data[\"cardinality\"][column] = len(ldf[column].unique())\n            self.data[\"unique\"][column] = ldf[column].unique()\n            self.data[\"min\"][column] = ldf[column].min()\n            self.data[\"max\"][column] = ldf[column].max()\n        else:\n            self.data[\"cardinality\"][column] = len(ldf[column].unique())\n            self.data[\"unique\"][column] = ldf[column].unique()\n    if not is_integer_dtype(ldf.index):\n        self.data[\"cardinality\"][\"index\"] = len(ldf.index.unique())\n        self.data[\"unique\"][\"index\"] = ldf.index.unique()\n", "CodeBleu_score": 30.231530450646805}]}
{"instance_id": "3e6fc3d2f47c", "generation_result": [{"code": "def execute_preview(tbl: LuxSQLTable, preview_size=5)\n\t\n    preview_query = \"SELECT * FROM {tbl_name} LIMIT {preview_size}\"\n    query = preview_query.format(tbl_name=tbl.name, preview_size=preview_size)\n    return pandas.read_sql(query, con=tbl.connection)\n\n", "CodeBleu_score": 31.01417501663355}, {"code": "def execute_preview(tbl: LuxSQLTable, preview_size=5)\n\t\n    preview_query = \"SELECT * FROM {} LIMIT {}\"\n    query = preview_query.format(tbl.table_name, preview_size)\n    return pandas.read_sql(query, conn)\n\n", "CodeBleu_score": 21.334598781516323}, {"code": "def execute_preview(tbl: LuxSQLTable, preview_size=5)\n\t\n    preview_query = f\"SELECT * FROM {tbl.name} LIMIT {preview_size}\"\n    return pandas.read_sql(preview_query, con=conn)\n\n", "CodeBleu_score": 19.31235703062984}]}
{"instance_id": "b3dee87be4b8", "generation_result": [{"code": "def execute_sampling(tbl: LuxSQLTable)\n\t\n    SAMPLE_FLAG = lux.config.SAMPLE_FLAG\n    SAMPLE_START = lux.config.SAMPLE_START\n    SAMPLE_CAP = lux.config.SAMPLE_CAP\n    SAMPLE_FRAC = lux.config.SAMPLE_FRAC\n    if SAMPLE_FLAG:\n        # get table size\n        # TODO: this is a hacky way to get the table size, we should have a better way to do this\n        if tbl.table_name == \"table\":\n            tbl.table_name = \"FROM \" + tbl.table_name\n        tbl.sql_query = f\"SELECT COUNT(*) AS table_size {tbl.table_name}\"\n        tbl.df = pd.read_sql(tbl.sql_query, con=tbl.connection)\n        table_size = tbl.df[\"table_size\"][0]\n        # calculate limit\n        limit = SAMPLE_START\n        if table_size < SAMPLE_CAP:\n            limit = table_size\n        else:\n            limit = int(table_size * SAMPLE_FRAC)\n        # sample\n        tbl.sql_query = f\"SELECT * {tbl.table_name} LIMIT {limit}\"\n        tbl.df = pd.read_sql(tbl.sql_query, con=tbl.connection)\n        tbl._sampled = True\n    else:\n        tbl.sql_query = f\"SELECT * {tbl.table_name}\"\n        tbl.df = pd.read_sql(tbl.sql_query, con=tbl.connection)\n        tbl._sampled = False", "CodeBleu_score": 27.357853916284636}, {"code": "def execute_sampling(tbl: LuxSQLTable)\n\t\n    SAMPLE_FLAG = lux.config.SAMPLE_FLAG\n    SAMPLE_START = lux.config.SAMPLE_START\n    SAMPLE_CAP = lux.config.SAMPLE_CAP\n    SAMPLE_FRAC = lux.config.SAMPLE_FRAC\n    if SAMPLE_FLAG:\n        query = f\"SELECT COUNT(*) FROM {tbl.table_name}\"\n        tbl.length = int(pd.read_sql(query, con=tbl.connection).iloc[0][0])\n        limit = min(SAMPLE_CAP, max(SAMPLE_START, int(tbl.length * SAMPLE_FRAC)))\n        query = f\"SELECT * FROM {tbl.table_name} LIMIT {limit}\"\n        tbl._sampled = pd.read_sql(query, con=tbl.connection)\n    else:\n        tbl.length = -1\n        tbl._sampled = None\n\n", "CodeBleu_score": 31.17669657062881}, {"code": "def execute_sampling(tbl: LuxSQLTable)\n\t\n    if lux.config.SAMPLE_FLAG:\n        tbl.length = tbl.executor.execute_query(\n            f\"SELECT COUNT(*) FROM {tbl.table_name}\"\n        ).iloc[0, 0]\n        limit = lux.config.SAMPLE_START\n        if tbl.length > lux.config.SAMPLE_CAP:\n            limit = int(lux.config.SAMPLE_FRAC * tbl.length)\n        tbl._sampled = tbl.executor.execute_query(\n            f\"SELECT * FROM {tbl.table_name} LIMIT {limit}\"\n        )\n\n", "CodeBleu_score": 25.3194775357786}]}
{"instance_id": "485253b1088a", "generation_result": [{"code": "def draw_text(\n    params: Dict[str, dict],\n    background: BaseGeometry\n) -> None\n\t\n    # Override default text settings with provided parameters\n    text_params = override_params(params[\"text\"], params[\"text_params\"])\n\n    # Extract the text, x, and y values from the text_params dictionary\n    text = text_params.pop(\"text\")\n    x = text_params.pop(\"x\")\n    y = text_params.pop(\"y\")\n\n    # Retrieve the background's bounds\n    xmin, xmax = background.bounds[0], background.bounds[2]\n    ymin, ymax = background.bounds[1], background.bounds[3]\n\n    # Interpolate the x and y values to fit within the background's bounds\n    x = np.interp(x, [0, 1], [xmin, xmax])\n    y = np.interp(y, [0, 1], [ymin, ymax])\n\n    # Draw the text at the calculated position with the remaining parameters\n    plt.text(x, y, text, **text_params)\n\n", "CodeBleu_score": 21.286856599111992}, {"code": "def draw_text(\n    params: Dict[str, dict],\n    background: BaseGeometry\n) -> None\n\t\n\n    text = params.pop(\"text\")\n    x = params.pop(\"x\")\n    y = params.pop(\"y\")\n    x = np.interp(x, [0, 1], [background.bounds[0], background.bounds[2]])\n    y = np.interp(y, [0, 1], [background.bounds[1], background.bounds[3]])\n    plt.text(x, y, text, **params)\n\n", "CodeBleu_score": 16.316639943747347}, {"code": "def draw_text(\n    params: Dict[str, dict],\n    background: BaseGeometry\n) -> None\n\t\n    # Override default text settings with provided parameters\n    text_params = override_params(params[\"text\"], params[\"text_params\"])\n\n    # Extract text, x, and y values from text_params\n    text = text_params.pop(\"text\")\n    x = text_params.pop(\"x\")\n    y = text_params.pop(\"y\")\n\n    # Retrieve background's bounds\n    x_min, y_min, x_max, y_max = background.bounds\n\n    # Interpolate x and y values to fit within background's bounds\n    x = np.interp(x, [0, 1], [x_min, x_max])\n    y = np.interp(y, [0, 1], [y_min, y_max])\n\n    # Draw text at calculated position with remaining parameters\n    plt.text(x, y, text, **text_params)", "CodeBleu_score": 18.48612064882218}]}
{"instance_id": "5f70a88b6b72", "generation_result": [{"code": "def _determine_format_string( dates, datetime_format=None )\n\t\n    if datetime_format is not None:\n        return datetime_format\n    else:\n        avg_days = np.mean(np.diff([d.toordinal() for d in dates]))\n        if avg_days < 0.33:\n            if len(set([d.year for d in dates])) > 1:\n                return '%b %d, %H:%M'\n            else:\n                return '%H:%M'\n        else:\n            if len(set([d.year for d in dates])) > 1:\n                return '%Y-%b-%d'\n            else:\n                return '%b %d'\n", "CodeBleu_score": 36.638474642690845}, {"code": "def _determine_format_string( dates, datetime_format=None )\n\t\n    # Calculate the average number of days between consecutive dates in the list\n    avg_days = (dates[-1] - dates[0]).days / len(dates)\n    # If datetime_format is provided, return this format\n    if datetime_format:\n        return datetime_format\n    # If the average days between points is less than 0.33, indicating intraday data\n    if avg_days < 0.33:\n        # Check if the data spans more than one day\n        if dates[-1] - dates[0] > dt.timedelta(days=1):\n            # Set the format string to '%b %d, %H:%M'\n            format_string = '%b %d, %H:%M'\n        else:\n            # Set the format string to '%H:%M'\n            format_string = '%H:%M'\n    # For daily or less frequent data\n    else:\n        # Check if the data spans multiple years\n        if dates[-1].year - dates[0].year > 0:\n            # Set the format string to '%Y-%b-%d'\n            format_string = '%Y-%b-%d'\n        else:\n            # Set the format string to '%b %d'\n            format_string = '%b %d'\n    # Return the determined format string\n    return format_string\n", "CodeBleu_score": 40.32070940966714}, {"code": "def _determine_format_string( dates, datetime_format=None )\n\t\n    if datetime_format is not None:\n        return datetime_format\n    else:\n        avg_days = (mdates.date2num(dates[-1]) - mdates.date2num(dates[0])) / len(dates)\n        if avg_days < 0.33:\n            if dates[-1].year!= dates[0].year:\n                return '%b %d, %H:%M'\n            else:\n                return '%H:%M'\n        else:\n            if dates[-1].year!= dates[0].year:\n                return '%Y-%b-%d'\n            else:\n                return '%b %d'\n", "CodeBleu_score": 45.64870125973154}]}
{"instance_id": "f9589bf4666d", "generation_result": [{"code": "def _mpf_to_rgba(c, alpha=None)\n\t\n    if _is_uint8_rgb_or_rgba(c):\n        c = tuple([v/255.0 for v in c])\n    if alpha is not None:\n        c = c + (alpha,)\n    return mcolors.to_rgba(c)\n\n", "CodeBleu_score": 27.829328740243888}, {"code": "def _mpf_to_rgba(c, alpha=None)\n\t\n    if _is_uint8_rgb_or_rgba(c):\n        c = tuple(v/255.0 for v in c)\n    if alpha is not None:\n        c = c + (alpha,)\n    return mcolors.to_rgba(c)\n\n", "CodeBleu_score": 26.09998899218473}, {"code": "def _mpf_to_rgba(c, alpha=None)\n\t\n    if _is_uint8_rgb_or_rgba(c):\n        if len(c) == 4:\n            c = c[0:3] + (c[3] / 255.0,)\n        else:\n            c = tuple(c / 255.0 for c in c)\n    if alpha is not None:\n        c = c[0:3] + (alpha,)\n    return mcolors.to_rgba(c)\n\n", "CodeBleu_score": 38.13117400311774}]}
{"instance_id": "4202e378ab0b", "generation_result": [{"code": "def subplots(self,*args,**kwargs)\n\t'style' in kwargs or not hasattr(self,'mpfstyle'):\n        style = _check_for_and_apply_style(kwargs)\n        self.mpfstyle = style\n    else:\n        style = _check_for_and_apply_style(dict(style=self.mpfstyle))\n\n    axlist = mplfigure.Figure.subplots(self,*args,**kwargs)\n\n    if isinstance(axlist,mpl_axes.Axes):\n        axlist.mpfstyle = style\n    elif isinstance(axlist,np.ndarray):\n        for ax in axlist.flatten():\n            ax.mpfstyle = style\n    else:\n       raise TypeError('Unexpected type ('+str(type(axlist))+') '+\n                      'returned from \"matplotlib.figure.Figure.subplots()\"')\n    return axlist\n\n", "CodeBleu_score": 90.36974166135758}, {"code": "def subplots(self,*args,**kwargs)\n\t'style' in kwargs or not hasattr(self,'mpfstyle'):\n        style = _check_for_and_apply_style(kwargs)\n        self.mpfstyle = style\n    else:\n        style = _check_for_and_apply_style(dict(style=self.mpfstyle))\n\n    axlist = mplfigure.Figure.subplots(self,*args,**kwargs)\n\n    if isinstance(axlist,mpl_axes.Axes):\n        axlist.mpfstyle = style\n    elif isinstance(axlist,np.ndarray):\n        for ax in axlist.flatten():\n            ax.mpfstyle = style\n    else:\n       raise TypeError('Unexpected type ('+str(type(axlist))+') '+\n                      'returned from \"matplotlib.figure.Figure.subplots()\"')\n    return axlist\n", "CodeBleu_score": 90.36974166135758}, {"code": "def subplots(self,*args,**kwargs)\n\tstyle' in kwargs or not hasattr(self,'mpfstyle'):\n        style = _check_for_and_apply_style(kwargs)\n        self.mpfstyle = style\n    else:\n        style = _check_for_and_apply_style(dict(style=self.mpfstyle))\n\n    axlist = mplfigure.Figure.subplots(self,*args,**kwargs)\n\n    if isinstance(axlist,mpl_axes.Axes):\n        axlist.mpfstyle = style\n    elif isinstance(axlist,np.ndarray):\n        for ax in axlist.flatten():\n            ax.mpfstyle = style\n    else:\n       raise TypeError('Unexpected type ('+str(type(axlist))+') '+\n                      'returned from \"matplotlib.figure.Figure.subplots()\"')\n    return axlist\n", "CodeBleu_score": 87.75159647597353}]}
{"instance_id": "3e2b6dfe4753", "generation_result": [{"code": "def _adjust_figsize(fig,config)\n\t\n    if not fig:\n        return\n    if 'figsize' in config:\n        fig.set_size_inches(config['figsize'])\n    else:\n        figsize = config.get('figure_ratio',(16,9))\n        if isinstance(figsize,str):\n            figsize = eval(figsize)\n        assert isinstance(figsize,tuple)\n        assert len(figsize)==2\n        assert isinstance(figsize[0],(int,float))\n        assert isinstance(figsize[1],(int,float))\n        figsize = np.array(figsize)\n        if figsize[0]<figsize[1]:\n            figsize = figsize[::-1]\n        assert figsize[0]>=figsize[1]\n        assert figsize[0]>=1\n        assert figsize[1]>=1\n        figsize = figsize/figsize[0]\n        figsize = np.array(figsize)\n        figsize = figsize*config.get('figure_ratio_scale',1)\n        figsize = figsize*config.get('figure_ratio_adjust',1)\n        fig.set_size_inches(figsize)\n    return fig\n", "CodeBleu_score": 33.56783158335816}, {"code": "def _adjust_figsize(fig,config)\n\t\n    if fig is None:\n        return\n    if 'figsize' in config:\n        fig.set_size_inches(config['figsize'])\n    else:\n        ratio = config['figure_ratio']\n        if ratio < 0.25:\n            ratio = 0.25\n        elif ratio > 1.0:\n            ratio = 1.0\n        fig.set_size_inches(fig.get_figwidth(), fig.get_figwidth() * ratio)", "CodeBleu_score": 23.48223275668116}, {"code": "def _adjust_figsize(fig,config)\n\t\n    if fig is None:\n        return\n    figsize = config.get(\"figsize\")\n    if figsize is None:\n        figsize = fig.get_size_inches()\n        fig_ratio = figsize[0] / figsize[1]\n        if fig_ratio > 1.618:\n            figsize[0] = figsize[1] * 1.618\n        elif fig_ratio < 1 / 1.618:\n            figsize[1] = figsize[0] / 1.618\n        fig.set_size_inches(figsize)\n    else:\n        fig.set_size_inches(figsize)", "CodeBleu_score": 28.098963741895766}]}
{"instance_id": "986e5804b941", "generation_result": [{"code": "def _plot_mav(ax,config,xdates,prices,apmav=None,apwidth=None)\n\t\n    if apmav is None:\n        if'mav' not in config:\n            return []\n        mav = config['mav']\n    else:\n        mav = apmav\n    if apwidth is not None:\n        width = apwidth\n    else:\n        width = config['mav_width']\n    if isinstance(mav,dict):\n        mavs = mav.get('periods',[])\n        shifts = mav.get('shift',[])\n        if len(shifts) == 1:\n            shifts = shifts*len(mavs)\n        if len(shifts)!= len(mavs):\n            raise ValueError('`mav` dict must have either a single `shift` value, or a list of `shift` values the same length as the list of `periods`.')\n        if len(mavs) > 7:\n            raise ValueError('Cannot plot more than 7 moving averages.')\n        for i,mav in enumerate(mavs):\n            if isinstance(mav,dict):\n                if 'period' not in mav:\n                    raise ValueError('`mav` dict must have a `period` value.')\n                if'shift' not in mav:\n                    mav['shift'] = shifts[i]\n                else:\n                    mav['shift'] = mav['shift'] + shifts[i]\n                mav = mav['period']\n            mav_prices = prices.rolling(mav).mean().shift(shifts[i])\n            ax.plot(xdates,mav_prices,width,color=config['mavcolors'][i])\n    else:\n        if len(mav) > 7:\n            raise ValueError('Cannot plot more than 7 moving averages.')\n        for i,mav in enumerate(mav):\n            mav_prices = prices.rolling(mav).mean()\n            ax.plot(xdates,mav_prices,width,color=config['mavcolors'][i])\n    return mav_prices\n\n", "CodeBleu_score": 39.77588725283257}, {"code": "def _plot_mav(ax,config,xdates,prices,apmav=None,apwidth=None)\n\t\n    if apmav is None:\n        if'mav' in config:\n            apmav = config['mav']\n        else:\n            return None\n    if isinstance(apmav,dict):\n        mav_list = [mav for mav in apmav.values() if mav is not None]\n    else:\n        mav_list = apmav\n    if len(mav_list) == 0:\n        return None\n    if apwidth is None:\n        if'mav_width' in config:\n            apwidth = config['mav_width']\n        else:\n            apwidth = 1.0\n    if isinstance(apwidth,dict):\n        width_list = [width for width in apwidth.values() if width is not None]\n    else:\n        width_list = [apwidth]*len(mav_list)\n    if len(width_list) == 0:\n        width_list = [1.0]*len(mav_list)\n    if len(width_list) < len(mav_list):\n        width_list = width_list + [1.0]*(len(mav_list)-len(width_list))\n    if len(mav_list) > 7:\n        mav_list = mav_list[0:7]\n        width_list = width_list[0:7]\n    mav_prices = []\n    for mav,width in zip(mav_list,width_list):\n        if isinstance(mav,dict):\n            if 'periods' in mav:\n                mav_periods = mav['periods']\n                del mav['periods']\n            else:\n                mav_periods = None\n            if'shift' in mav:\n                mav_shift = mav['shift']\n                del mav['shift']\n            else:\n                mav_shift = None\n        else:\n            mav_periods = mav\n            mav_shift = None\n        if mav_periods is None:\n            continue\n        mav_prices.append(prices.rolling(mav_periods).mean())\n        if mav_shift is not None:\n            mav_prices[-1] = mav_prices[-1].shift(mav_shift)\n        ax.plot(xdates,mav_prices[-1],**config['mav_style'],linewidth=width)\n    return mav_prices\n\n", "CodeBleu_score": 39.25505233619741}, {"code": "def _plot_mav(ax,config,xdates,prices,apmav=None,apwidth=None)\n\t\n    if apmav is None:\n        apmav = config['mav']\n    if apmav is None:\n        return []\n\n    if isinstance(apmav,dict):\n        if 'periods' in apmav:\n            periods = apmav['periods']\n            if isinstance(periods,int):\n                periods = [periods]\n            else:\n                periods = list(periods)\n        else:\n            periods = []\n        if'shift' in apmav:\n            shift = apmav['shift']\n            if isinstance(shift,int):\n                shift = [shift]\n            else:\n                shift = list(shift)\n        else:\n            shift = []\n        if 'linewidth' in apmav:\n            linewidth = apmav['linewidth']\n        else:\n            linewidth = config['linewidth']\n        if 'color' in apmav:\n            color = apmav['color']\n        else:\n            color = config['color']\n    else:\n        periods = apmav\n        shift = [0]*len(periods)\n        linewidth = config['linewidth']\n        color = config['color']\n\n    if apwidth is None:\n        apwidth = config['width']\n\n    if len(periods) == 0:\n        return []\n\n    if len(periods) > 7:\n        periods = periods[:7]\n        shift   = shift[:7]\n\n    mavs = []\n    for i in range(len(periods)):\n        mav = prices.rolling(periods[i]).mean()\n        if shift[i]!= 0:\n            mav = mav.shift(shift[i])\n        mavs.append(mav)\n        ax.plot(xdates,mav,linewidth=linewidth,color=color[i])\n\n    return mavs\n\n", "CodeBleu_score": 41.66819801741032}]}
{"instance_id": "5308e20f3c90", "generation_result": [{"code": "def flops_conv(opnode: ConvolutionForward, inputs, outputs)\n\t\n    kernel_h, kernel_w = inputs[1].shape[2:]\n    input_channels = inputs[0].shape[1]\n    output_size = np.prod(outputs[0].shape)\n    bias_flops = 0\n    if opnode.op.bias is not None:\n        bias_flops = output_size\n    return output_size * input_channels * (kernel_h * kernel_w + bias_flops)\n\n", "CodeBleu_score": 23.744379933051246}, {"code": "def flops_conv(opnode: ConvolutionForward, inputs, outputs)\n\t\n    kernel_h, kernel_w = inputs[1].shape[2:]\n    in_channels = inputs[0].shape[1]\n    out_elements = np.prod(outputs[0].shape)\n    if opnode.op.bias is not None:\n        bias_flops = out_elements\n    else:\n        bias_flops = 0\n    return out_elements * in_channels * (kernel_h * kernel_w + bias_flops)\n\n", "CodeBleu_score": 24.244367643212016}, {"code": "def flops_conv(opnode: ConvolutionForward, inputs, outputs)\n\t\n    # Extract the kernel height and width from the input tensor's shape\n    kernel_h, kernel_w = inputs[1].shape[2:]\n\n    # Determine the number of input channels\n    in_channels = inputs[0].shape[1]\n\n    # Calculate the product of the output tensor's dimensions\n    out_elements = np.prod(outputs[0].shape)\n\n    # Check if the operation includes a bias term\n    bias = opnode.attrs.get(\"bias\")\n    if bias is not None:\n        bias_flops = out_elements\n\n    # Compute the FLOPs\n    flops = out_elements * in_channels * (kernel_h * kernel_w + bias_flops)\n\n    return flops\n\n", "CodeBleu_score": 25.628253281277015}]}
{"instance_id": "70811d3cf578", "generation_result": [{"code": "def mge_dtype_to_ir_type(mge_dtype)\n\t\n    mge_dtype = np.dtype(mge_dtype)\n    assert isinstance(mge_dtype, np.dtype)\n    return _dtype_to_ir_type[mge_dtype]()\n\n", "CodeBleu_score": 20.149569944707952}, {"code": "def mge_dtype_to_ir_type(mge_dtype)\n\t\n    mge_dtype = np.dtype(mge_dtype)\n    assert isinstance(mge_dtype, np.dtype)\n    return _dtype_to_ir_type[mge_dtype]()\n\n", "CodeBleu_score": 20.149569944707952}, {"code": "def mge_dtype_to_ir_type(mge_dtype)\n\t\n    mge_dtype = np.dtype(mge_dtype)\n    assert isinstance(mge_dtype, np.dtype)\n    return _dtype_to_ir_type[mge_dtype.type](mge_dtype)", "CodeBleu_score": 24.32360236309702}]}
{"instance_id": "a108c9ba2c22", "generation_result": [{"code": "def _numpy_array_constant(x: np.ndarray, canonicalize_types) -> Sequence[ir.Value]\n\t\n    x = np.ascontiguousarray(x)\n    if canonicalize_types:\n        x = np.array(x, dtype=canonicalize_dtype(x.dtype))\n    ir_type = mge_dtype_to_ir_type(x.dtype)\n    if ir_type.is_bfloat16:\n        x = x.astype(np.float32)\n    elif ir_type.is_boolean:\n        x = x.astype(np.int32)\n    elements = np.ravel(x).tolist()\n    return [hlo.ConstantOp(elements=elements).result]\n\n", "CodeBleu_score": 37.58369566240605}, {"code": "def _numpy_array_constant(x: np.ndarray, canonicalize_types) -> Sequence[ir.Value]\n\t\n    x = np.ascontiguousarray(x)\n    if canonicalize_types:\n        x = np.array(x, dtype=canonicalize_dtype(x.dtype))\n    ir_type = mge_dtype_to_ir_type(x.dtype)\n    if ir_type.is_bfloat16:\n        x = x.astype(np.float32)\n    if ir_type.is_bool:\n        x = x.astype(np.int32)\n    elements = ir.DenseElementsAttr.get(x)\n    return ir.ConstantOp(elements, result_type=ir_type).result\n\n", "CodeBleu_score": 37.97614473981293}, {"code": "def _numpy_array_constant(x: np.ndarray, canonicalize_types) -> Sequence[ir.Value]\n\t\n    if canonicalize_types:\n        x = np.array(x, dtype=canonicalize_dtype(x.dtype))\n    ir_type = mge_dtype_to_ir_type(x.dtype)\n    if ir_type.is_bool:\n        x = np.array(x, dtype=np.int8)\n    if ir_type.is_bf16:\n        x = np.array(x, dtype=np.float32)\n    x = np.ascontiguousarray(x)\n    elements = ir.DenseElementsAttr.get(x)\n    return hlo.ConstantOp(elements, ir_type).results\n\n", "CodeBleu_score": 40.488049960629695}]}
{"instance_id": "3345fe4d008b", "generation_result": [{"code": "def _ndarray_constant_handler(\n    val: np.ndarray, canonicalize_types\n) -> Sequence[ir.Value]\n\t\n    if not val.flags[\"C_CONTIGUOUS\"]:\n        zero_strides = np.where(val.strides == 0)[0]\n        if len(zero_strides) > 0:\n            non_zero_strides = np.where(val.strides!= 0)[0]\n            collapsed = np.reshape(val, val.shape[zero_strides[0]] * val.shape[zero_strides[1]])\n            if canonicalize_types:\n                collapsed = np.asarray(collapsed, dtype.canonicalize_dtype(collapsed.dtype))\n            return _numpy_array_constant(collapsed, canonicalize_types) + (\n                dense_int_elements(non_zero_strides),\n            )\n    return _numpy_array_constant(val, canonicalize_types)\n\n", "CodeBleu_score": 34.48450275288921}, {"code": "def _ndarray_constant_handler(\n    val: np.ndarray, canonicalize_types\n) -> Sequence[ir.Value]\n\t\n    if np.any(val.strides == 0) and val.size > 0:\n        # Identify the zero-stride and non-zero-stride axes.\n        zero_stride_axes = np.where(val.strides == 0)[0]\n        non_zero_stride_axes = np.where(val.strides!= 0)[0]\n        # Collapse the array along the zero-stride axes.\n        collapsed_val = val.transpose(\n            tuple(non_zero_stride_axes) + tuple(zero_stride_axes)\n        )\n        collapsed_val = collapsed_val.reshape(collapsed_val.shape[:-len(zero_stride_axes)])\n        # Canonicalize the data type if requested.\n        if canonicalize_types:\n            collapsed_val = np.asarray(collapsed_val, dtype.canonicalize_dtype(collapsed_val.dtype))\n        # Create a broadcast operation for the collapsed array, matching the original shape and non-zero-stride axes.\n        collapsed_val = np.broadcast_to(\n            collapsed_val,\n            tuple(val.shape[axis] for axis in non_zero_stride_axes)\n            + tuple(val.shape[axis] for axis in zero_stride_axes),\n        )\n        # Return the result.\n        return _numpy_array_constant(collapsed_val, canonicalize_types)\n    else:\n        return _numpy_array_constant(val, canonicalize_types)\n\n", "CodeBleu_score": 46.36763344836922}, {"code": "def _ndarray_constant_handler(\n    val: np.ndarray, canonicalize_types\n) -> Sequence[ir.Value]\n\t\n    if val.size > 0 and np.any(val.strides == 0):\n        # Collapse the array along the zero-stride axes\n        nonzero_strides = np.where(val.strides!= 0)[0]\n        collapsed_val = np.moveaxis(val, nonzero_strides, np.arange(len(nonzero_strides)))\n        collapsed_val = np.ascontiguousarray(collapsed_val)\n        # Broadcast the collapsed array to match the original shape and non-zero-stride axes\n        collapsed_shape = collapsed_val.shape\n        broadcast_shape = [1] * len(collapsed_shape)\n        broadcast_shape[nonzero_strides] = collapsed_shape\n        broadcast_shape = tuple(broadcast_shape)\n        collapsed_val = np.broadcast_to(collapsed_val, broadcast_shape)\n        # Process the collapsed array using _numpy_array_constant\n        collapsed_val = np.ascontiguousarray(collapsed_val)\n        return _numpy_array_constant(collapsed_val, canonicalize_types)\n    else:\n        return _numpy_array_constant(val, canonicalize_types)", "CodeBleu_score": 32.59054760062152}]}
{"instance_id": "91b298678351", "generation_result": [{"code": "def dense_bool_elements(xs: Sequence[bool]) -> ir.DenseElementsAttr\n\t\n    return ir.DenseElementsAttr.get(\n        np.packbits(np.array(xs, dtype=np.bool_)).view(np.uint8),\n        ir.IntegerType.get_signless(1),\n        shape=[len(xs)],\n    )\n\n", "CodeBleu_score": 28.703971724190534}, {"code": "def dense_bool_elements(xs: Sequence[bool]) -> ir.DenseElementsAttr\n\t\n    if len(xs) == 1:\n        if xs[0]:\n            return ir.DenseElementsAttr.get(ir.IntegerType.get_signless(1), [1])\n        else:\n            return ir.DenseElementsAttr.get(ir.IntegerType.get_signless(1), [0])\n    return ir.DenseElementsAttr.get(ir.IntegerType.get_signless(1), np.packbits(np.array(xs, dtype=np.bool_), bitorder=\"little\"))\n", "CodeBleu_score": 33.96080405791725}, {"code": "def dense_bool_elements(xs: Sequence[bool]) -> ir.DenseElementsAttr\n\t\n    xs = np.array(xs, dtype=np.bool_)\n    if xs.size == 1:\n        xs = np.packbits(np.array([xs[0]], dtype=np.bool_))\n    else:\n        xs = np.packbits(xs, bitorder=\"little\")\n    return ir.DenseElementsAttr.get(xs, ir.IntegerType.get_signless(1))\n", "CodeBleu_score": 45.99706056594079}]}
{"instance_id": "06ebabf1e0ff", "generation_result": [{"code": "def create_tensor_lower(ctx, *args: Union[HLOTensor, Sequence[HLOTensor]])\n\t\n    assert len(args) == 1\n    assert len(ctx.vars_in) == 1\n    assert len(ctx.vars_out) == 1\n    var_in = ctx.vars_in[0]\n    var_out = ctx.vars_out[0]\n    if var_in.data is not None:\n        var_in.data.set_value(var_in.data.to_numpy(var_in.data.dtype))\n        var_out.data.set_value(var_in.data.to_numpy(var_out.data.dtype))\n    else:\n        assert var_out.data is not None\n        assert var_in.shape == var_out.shape\n        data = var_out.data.to_numpy(var_out.data.dtype)\n        var_in.data.set_value(data)\n    return HLOTensor(data=var_in.data, shape=var_in.shape, dtype=var_in.dtype, device=var_in.device)\n\n", "CodeBleu_score": 49.22190449787972}, {"code": "def create_tensor_lower(ctx, *args: Union[HLOTensor, Sequence[HLOTensor]])\n\t\n    assert len(args) == 1, \"Only one tensor can be created at a time\"\n    assert len(ctx.vars_in) == 1, \"Only one tensor can be created at a time\"\n    assert len(ctx.vars_out) == 1, \"Only one tensor can be created at a time\"\n    in_var = ctx.vars_in[0]\n    out_var = ctx.vars_out[0]\n    if in_var.bound_data is not None:\n        out_var.set_value(in_var.bound_data)\n    elif out_var.bound_data is not None:\n        in_var.set_value(out_var.bound_data)\n    else:\n        assert False, \"Only tensor creation from constants is supported\"\n    assert in_var.shape == out_var.shape, \"Input and output shapes must be the same\"\n    return HLOTensor(in_var.shape, in_var.dtype, in_var.bound_data)\n\n", "CodeBleu_score": 40.051003697616814}, {"code": "def create_tensor_lower(ctx, *args: Union[HLOTensor, Sequence[HLOTensor]])\n\t\n    assert len(args) == 1\n    assert len(ctx.vars_in) == 1\n    assert len(ctx.vars_out) == 1\n    in_var = ctx.vars_in[0]\n    out_var = ctx.vars_out[0]\n    if in_var.bound_data:\n        in_var.set_value(in_var.bound_data)\n        out_var.set_value(in_var.bound_data)\n    elif out_var.bound_data:\n        out_var.set_value(out_var.bound_data)\n        in_var.set_value(out_var.bound_data)\n    else:\n        assert False, \"Only tensor creation from constants is supported.\"\n    assert in_var.shape == out_var.shape\n    len_ = in_var.shape[0]\n    dtype = out_var.dtype\n    if dtype == np.float64:\n        data = np.asarray(out_var.bound_data, dtype=np.float64)\n    elif dtype == np.float32:\n        data = np.asarray(out_var.bound_data, dtype=np.float32)\n    elif dtype == np.int64:\n        data = np.asarray(out_var.bound_data, dtype=np.int64)\n    elif dtype == np.int32:\n        data = np.asarray(out_var.bound_data, dtype=np.int32)\n    elif dtype == np.bool_:\n        data = np.asarray(out_var.bound_data, dtype=np.bool_)\n    else:\n        assert False, \"Unsupported data type.\"\n    return HLOTensor(data, dtype, len_)\n", "CodeBleu_score": 42.52468618730848}]}
{"instance_id": "6cd750fa0c8d", "generation_result": [{"code": "def _device_put_scalar(x, device)\n\t\n    # Convert the scalar to a numpy array\n    x_np = _scalar_to_np_array(x)\n    # Transfer the numpy array to the specified device\n    x_dev = _device_put_array(x_np, device)\n    return x_dev\n", "CodeBleu_score": 9.484740681459202}, {"code": "def _device_put_scalar(x, device)\n\t\n    x = _scalar_type_to_dtype(x)\n    return _device_put_numpy(np.asarray(x), device)\n\n", "CodeBleu_score": 9.031104152766384}, {"code": "def _device_put_scalar(x, device)\n\t\n    x = _convert_scalar_type_to_dtype(x, None)\n    return _device_put_numpy(x, device)\n\n", "CodeBleu_score": 11.103426780554779}]}
{"instance_id": "140de7c2b6a8", "generation_result": [{"code": "def _shape_equal(lhs_shape, rhs_shape)\n\t\n    if isinstance(lhs_shape, np.ndarray):\n        lhs_shape = lhs_shape.tolist()\n    if isinstance(rhs_shape, np.ndarray):\n        rhs_shape = rhs_shape.tolist()\n    assert isinstance(lhs_shape, (tuple, list))\n    assert isinstance(rhs_shape, (tuple, list))\n    if len(lhs_shape) == 0 and len(rhs_shape) == 0:\n        return True\n    if len(lhs_shape) > 0 and len(rhs_shape) > 0:\n        if not isinstance(lhs_shape[0], int):\n            raise ValueError(\"The first element of lhs_shape is not an integer.\")\n        if not isinstance(rhs_shape[0], int):\n            raise ValueError(\"The first element of rhs_shape is not an integer.\")\n    if len(lhs_shape)!= len(rhs_shape):\n        return False\n    for i in range(len(lhs_shape)):\n        if lhs_shape[i]!= rhs_shape[i]:\n            return False\n    return True\n\n", "CodeBleu_score": 52.23160533366096}, {"code": "def _shape_equal(lhs_shape, rhs_shape)\n\t\n    if isinstance(lhs_shape, np.ndarray):\n        lhs_shape = lhs_shape.tolist()\n    if isinstance(rhs_shape, np.ndarray):\n        rhs_shape = rhs_shape.tolist()\n    assert isinstance(lhs_shape, tuple) or isinstance(lhs_shape, list)\n    assert isinstance(rhs_shape, tuple) or isinstance(rhs_shape, list)\n    if len(lhs_shape) == 0 and len(rhs_shape) == 0:\n        return True\n    if len(lhs_shape) == 0 or len(rhs_shape) == 0:\n        return False\n    if not isinstance(lhs_shape[0], int) or not isinstance(rhs_shape[0], int):\n        return False\n    if len(lhs_shape)!= len(rhs_shape):\n        return False\n    for l, r in zip(lhs_shape, rhs_shape):\n        if l!= r:\n            return False\n    return True\n", "CodeBleu_score": 53.26884397371764}, {"code": "def _shape_equal(lhs_shape, rhs_shape)\n\t\n    if isinstance(lhs_shape, np.ndarray):\n        lhs_shape = lhs_shape.tolist()\n    if isinstance(rhs_shape, np.ndarray):\n        rhs_shape = rhs_shape.tolist()\n    assert isinstance(lhs_shape, (tuple, list)), f\"lhs_shape is not a tuple or list: {lhs_shape}\"\n    assert isinstance(rhs_shape, (tuple, list)), f\"rhs_shape is not a tuple or list: {rhs_shape}\"\n    if len(lhs_shape) == 0 and len(rhs_shape) == 0:\n        return True\n    if not isinstance(lhs_shape[0], int) or not isinstance(rhs_shape[0], int):\n        return False\n    if len(lhs_shape)!= len(rhs_shape):\n        return False\n    for l, r in zip(lhs_shape, rhs_shape):\n        if l!= r:\n            return False\n    return True\n", "CodeBleu_score": 55.87858236065897}]}
{"instance_id": "55d0c71a259b", "generation_result": [{"code": "def read(\n    training_set_filename,\n    test_set_filename,\n    training_set_metadata_filename,\n    test_set_metadata_filename,\n    dtypes,\n    meta_dtypes,\n)\n\t\n    training_set = pd.read_csv(\n        training_set_filename, dtype=dtypes, index_col=\"id\"\n    )\n    test_set = pd.read_csv(test_set_filename, dtype=dtypes, index_col=\"id\")\n    training_metadata = pd.read_csv(\n        training_set_metadata_filename, dtype=meta_dtypes, index_col=\"id\"\n    )\n    meta_dtypes.pop(\"target\")\n    test_metadata = pd.read_csv(\n        test_set_metadata_filename, dtype=meta_dtypes, index_col=\"id\"\n    )\n    meta_dtypes[\"target\"] = np.float64\n    return training_set, test_set, training_metadata, test_metadata\n\n", "CodeBleu_score": 36.65002714557196}, {"code": "def read(\n    training_set_filename,\n    test_set_filename,\n    training_set_metadata_filename,\n    test_set_metadata_filename,\n    dtypes,\n    meta_dtypes,\n)\n\t\n    # Load training set\n    training_set = pd.read_csv(\n        training_set_filename, dtype=dtypes, index_col=\"sig_id\"\n    )\n    # Load test set\n    test_set = pd.read_csv(test_set_filename, dtype=dtypes, index_col=\"sig_id\")\n    # Load training metadata\n    meta_dtypes_copy = meta_dtypes.copy()\n    meta_dtypes_copy.pop(\"target\")\n    training_metadata = pd.read_csv(\n        training_set_metadata_filename, dtype=meta_dtypes_copy\n    )\n    # Load test metadata\n    test_metadata = pd.read_csv(test_set_metadata_filename, dtype=meta_dtypes)\n    return training_set, test_set, training_metadata, test_metadata\n\n", "CodeBleu_score": 42.991518877206346}, {"code": "def read(\n    training_set_filename,\n    test_set_filename,\n    training_set_metadata_filename,\n    test_set_metadata_filename,\n    dtypes,\n    meta_dtypes,\n)\n\t\n    training_set = pd.read_csv(\n        training_set_filename, dtype=dtypes, index_col=\"sig_id\"\n    )\n    test_set = pd.read_csv(test_set_filename, dtype=dtypes, index_col=\"sig_id\")\n    training_metadata = pd.read_csv(\n        training_set_metadata_filename, dtype=meta_dtypes, index_col=\"sig_id\"\n    )\n    test_metadata = pd.read_csv(\n        test_set_metadata_filename, dtype=meta_dtypes, index_col=\"sig_id\"\n    )\n    meta_dtypes.pop(\"target\")\n    test_metadata = pd.read_csv(\n        test_set_metadata_filename, dtype=meta_dtypes, index_col=\"sig_id\"\n    )\n    return training_set, test_set, training_metadata, test_metadata\n\n", "CodeBleu_score": 32.668679809756895}]}
{"instance_id": "9e5e02cd73e0", "generation_result": [{"code": "def _show_cmap(cmap)\n\t\n    from rcmod import axes_style\n    fig, ax = plt.subplots(figsize=(10, 1))\n    ax.set(xlim=(0, 1), ylim=(0, 1))\n    ax.set_axis_off()\n    ax.pcolormesh(np.linspace(0, 1, 1000).reshape(1, -1), cmap=cmap)\n    axes_style('white')\n    fig.show()\n", "CodeBleu_score": 36.23619940203084}, {"code": "def _show_cmap(cmap)\n\t\n    from rcmod import axes_style\n    fig, ax = plt.subplots(figsize=(10, 0.5))\n    ax.set(xticks=[], yticks=[])\n    ax.pcolormesh(np.linspace(0, 1, 256).reshape(1, -1), cmap=cmap)\n    axes_style(\"white\")\n    plt.show()\n", "CodeBleu_score": 42.342679546596564}, {"code": "def _show_cmap(cmap)\n\t\n    from rcmod import axes_style\n    fig, ax = plt.subplots(figsize=(6, 1))\n    fig.subplots_adjust(bottom=0.5)\n    ax.set_title(cmap.name +'colormap')\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.set_facecolor('white')\n    axes_style('white')\n    bounds = np.linspace(0, 1, 100)\n    norm = colors.BoundaryNorm(bounds, cmap.N)\n    ax.pcolormesh(bounds, cmap=cmap, norm=norm)\n    fig.show()\n", "CodeBleu_score": 44.47476177050043}]}
{"instance_id": "ad0781b5cc38", "generation_result": [{"code": "def transform_to_data_coordinates(obj, xdata, ydata)\n\t\n    if obj.axes is None:\n        return xdata, ydata\n    else:\n        if obj.get_transform()!= obj.axes.transData:\n            transform = composite_transform_factory(obj.get_transform(), obj.axes.transData.inverted())\n            points = np.array([xdata, ydata]).T\n            return transform.transform(points).T\n        else:\n            return xdata, ydata\n", "CodeBleu_score": 65.71538346505143}, {"code": "def transform_to_data_coordinates(obj, xdata, ydata)\n\t\n    if obj.axes is None:\n        return xdata, ydata\n    if obj.get_transform()!= obj.axes.transData:\n        xdata, ydata = zip(*obj.axes.transData.inverted().transform(obj.get_transform().transform(zip(xdata, ydata))))\n    return xdata, ydata\n\n", "CodeBleu_score": 49.10180887779036}, {"code": "def transform_to_data_coordinates(obj, xdata, ydata)\n\t\n    if obj.axes is None:\n        return xdata, ydata\n\n    if obj.get_transform()!= obj.axes.transData:\n        trans = obj.axes.transData + obj.get_transform().inverted()\n    else:\n        trans = None\n\n    if trans is not None:\n        points = np.column_stack((xdata, ydata))\n        points = trans.transform(points)\n        xdata, ydata = points.T\n\n    return xdata, ydata\n\n", "CodeBleu_score": 52.824727109029}]}
{"instance_id": "bf776cadef8d", "generation_result": [{"code": "def kdeplot(self, data=None, data2=None, *args, **kwargs)\n\t\n    data = self._maybe_target_series(data, key='data')\n    if data2 is not None:\n        if not pd.api.types.is_list_like(data2):\n            data2 = self._df[data2]\n    return self._module.kdeplot(data, data2=data2, *args, **kwargs)\n", "CodeBleu_score": 78.95861998202267}, {"code": "def kdeplot(self, data=None, data2=None, *args, **kwargs)\n\t\n    data = self._maybe_target_series(data, key='data')\n\n    if data2 is not None:\n        if not pd.api.types.is_list_like(data2):\n            data2 = self._df[data2]\n    return self._module.kdeplot(data, data2=data2, *args, **kwargs)\n", "CodeBleu_score": 78.95861998202267}, {"code": "def kdeplot(self, data=None, data2=None, *args, **kwargs)\n\t\n    data = self._maybe_target_series(data, key='data')\n    if data2 is not None:\n        if not pd.api.types.is_list_like(data2):\n            data2 = self._df[data2]\n    return self._module.kdeplot(data, data2=data2, *args, **kwargs)\n", "CodeBleu_score": 78.95861998202267}]}
{"instance_id": "fb44964174d9", "generation_result": [{"code": "def plot_most_used_words(df)\n\t\n    df['text_clean'] = df['text_clean'].str.replace('programa', 'programar')\n    df_words = df['text_clean'].str.split(expand=True).stack().value_counts().reset_index()\n    df_words.columns = ['word', 'count']\n    df_words = df_words[df_words['word'].str.isalpha()]\n    df_words = df_words[df_words['word'].str.len() > 1]\n    df_words = df_words[~df_words['word'].isin(stop_words)]\n    df_words = df_words.head(20)\n    plt.figure(figsize=(10, 6))\n    sns.barplot(x='count', y='word', data=df_words)\n    plt.xlabel('Count')\n    plt.ylabel('Word')\n    plt.title('Most Frequent Words')\n    plt.savefig('most_frequent_words.png')\n    plt.close()\n\n", "CodeBleu_score": 33.24528954049572}, {"code": "def plot_most_used_words(df)\n\t\n    df['text'] = df['text'].str.replace('programa', 'programar')\n    df['text'] = df['text'].str.replace('programar', 'programar')\n    df['text'] = df['text'].str.replace('programaci\u00f3n', 'programar')\n    df['text'] = df['text'].str.replace('programar', 'programar')\n    df['text'] = df['text'].str.replace('programar', 'programar')\n    df['text'] = df['text'].str.replace('programar', 'programar')\n    df['text'] = df['text'].str.replace('programar', 'programar')\n    df['text'] = df['text'].str.replace('programar', 'programar')\n    df['text'] = df['text'].str.replace('programar', 'programar')\n    df['text'] = df['text'].str.replace('programar', 'programar')\n    df['text'] = df['text'].str.replace('programar', 'programar')\n    df['text'] = df['text'].str.replace('programar', 'programar')\n    df['text'] = df['text'].str.replace('programar', 'programar')\n    df['text'] = df['text'].str.replace('programar', 'programar')\n    df['text'] = df['text'].str.replace('programar', 'programar')\n    df['text'] = df['text'].str.replace('programar', 'programar')\n    df['text'] = df['text'].str.replace('programar', 'programar')\n    df['text'] = df['text'].str.replace('programar', 'programar')\n    df['text'] = df['text'].str.replace('programar', 'programar')\n    df['text'] = df['text'].str.replace('programar', 'programar')\n    df['text'] = df['text'].str.replace('programar', 'programar')\n    df['text'] = df['text'].str.replace('programar', 'programar')\n    df['text'] = df['text'].str.replace('programar', 'programar')\n    df['text'] = df['text'].str.replace('programar', 'programar')\n    df['text'] = df['", "CodeBleu_score": 11.570986337745147}, {"code": "def plot_most_used_words(df)\n\t\n    df = df.replace(\"programa\", \"programar\")\n    df = df[df[\"token\"].apply(lambda x: x.isalpha() and len(x) > 1 and x not in stop_words)]\n    df = df[\"token\"].value_counts()[:20]\n    plt.figure(figsize=(10, 6))\n    sns.barplot(x=df.values, y=df.index, palette=\"Blues_r\")\n    plt.xlabel(\"Frequ\u00eancia\")\n    plt.ylabel(\"Palavra\")\n    plt.title(\"Palavras mais frequentes\")\n    plt.savefig(\"most_used_words.png\")", "CodeBleu_score": 36.01040040858173}]}
{"instance_id": "13414a7eb4e3", "generation_result": [{"code": "def dask_rolling_wrapper(moving_func, a, window, min_count=None, axis=-1)\n\t\n    import dask.array as da\n    import bottleneck as bn\n    a = da.promote_dtype(a, np.float64)\n    overlap = (window - 1) // 2\n    fill_value = np.nan if np.issubdtype(a.dtype, np.floating) else bn.nanmin(a)\n    ag = da.overlap.overlap(a, depth={axis: overlap}, boundary={axis: fill_value})\n    return da.overlap.trim_internal(\n        da.map_blocks(\n            moving_func,\n            ag,\n            window,\n            min_count=min_count,\n            axis=axis,\n            meta=a._meta,\n        ),\n        trim={axis: overlap},\n    )\n\n", "CodeBleu_score": 40.49213877208269}, {"code": "def dask_rolling_wrapper(moving_func, a, window, min_count=None, axis=-1)\n\t\n    import dask.array as da\n    import bottleneck as bn\n    a = da.promote_dtype(a, bn.dtypes.maybe_promote(a.dtype))\n    depth = min(window, a.shape[axis] // 2)\n    boundary = a.dtype.type(0)\n    ag = da.overlap.overlap(a, depth=depth, boundary=boundary, trim=False)\n    return da.overlap.trim_internal(\n        da.map_blocks(moving_func, ag, window, min_count=min_count, axis=axis),\n        depth=depth,\n    )\n\n", "CodeBleu_score": 36.10081355154379}, {"code": "def dask_rolling_wrapper(moving_func, a, window, min_count=None, axis=-1)\n\t\n    import dask.array as da\n    a = dtypes.maybe_promote(a)\n    overlap = int(window / 2)\n    if axis == 0:\n        fill_value = a[0, 0]\n    elif axis == 1:\n        fill_value = a[0, 0]\n    else:\n        fill_value = a[0, 0, 0]\n    ag = da.overlap.overlap(a, depth={axis: overlap}, boundary={axis: fill_value})\n    return da.overlap.trim_internal(da.map_blocks(moving_func, ag, window, min_count, axis=axis),\n                                    depth={axis: overlap}, trim={axis: 0})\n\n", "CodeBleu_score": 23.94041920065174}]}
{"instance_id": "68ce25d325d4", "generation_result": [{"code": "def least_squares(lhs, rhs, rcond=None, skipna=False)\n\t\n    if skipna:\n        if rhs.ndim == 1:\n            rhs = rhs.reshape(-1, 1)\n        coeffs, residuals, _, _ = lstsq(lhs, rhs, rcond=rcond)\n        return coeffs.reshape(-1), residuals.reshape(-1)\n    else:\n        return lstsq(lhs, rhs, rcond=rcond)\n\n", "CodeBleu_score": 16.34704427014768}, {"code": "def least_squares(lhs, rhs, rcond=None, skipna=False)\n\t\n    # Convert the input arrays to Dask arrays\n    lhs = da.from_array(lhs, chunks=lhs.shape)\n    rhs = da.from_array(rhs, chunks=rhs.shape)\n\n    if skipna:\n        # Reshape the right-hand side array if necessary\n        if rhs.ndim > 1:\n            rhs = rhs.reshape((rhs.shape[0], -1))\n\n        # Apply the polynomial fit along the specified axis\n        coeffs, residuals, _, _ = da.linalg.lstsq(lhs, rhs)\n\n        # Extract coefficients and residuals from the result\n        coeffs = coeffs.reshape(lhs.shape)\n        residuals = residuals.reshape(rhs.shape)\n    else:\n        # Compute the least squares solution using Dask's linear algebra functions\n        coeffs, residuals, _, _ = da.linalg.lstsq(lhs, rhs, rcond=rcond)\n\n    return coeffs, residuals\n", "CodeBleu_score": 33.34188302293979}, {"code": "def least_squares(lhs, rhs, rcond=None, skipna=False)\n\t\n    if skipna:\n        da = dask.array\n        if len(rhs.shape) == 1:\n            rhs = rhs.reshape(-1, 1)\n        if len(lhs.shape) == 1:\n            lhs = lhs.reshape(-1, 1)\n        lhs = da.from_array(lhs)\n        rhs = da.from_array(rhs)\n        coeffs, residuals, _, _ = da.linalg.lstsq(lhs, rhs, rcond=rcond)\n        coeffs = coeffs.reshape(-1)\n        residuals = residuals.reshape(-1)\n    else:\n        coeffs, residuals, _, _ = np.linalg.lstsq(lhs, rhs, rcond=rcond)\n    return coeffs, residuals", "CodeBleu_score": 32.569073947139806}]}
{"instance_id": "c0c2c53b76e0", "generation_result": [{"code": "def from_array(\n        self, data: Any, chunks: T_Chunks | _NormalizedChunks, **kwargs: Any\n    ) -> DaskArray | Any\n\t\n    import dask.array as da\n\n    if isinstance(data, ImplicitToExplicitIndexingAdapter):\n        # lazily loaded backend array classes should use NumPy array operations.\n        kwargs[\"meta\"] = np.ndarray\n\n    return da.from_array(\n        data,\n        chunks,\n        **kwargs,\n    )  # type: ignore[no-untyped-call]\n", "CodeBleu_score": 92.04127144323292}, {"code": "def from_array(\n        self, data: Any, chunks: T_Chunks | _NormalizedChunks, **kwargs: Any\n    ) -> DaskArray | Any\n\t\n    import dask.array as da\n\n    if isinstance(data, ImplicitToExplicitIndexingAdapter):\n        # lazily loaded backend array classes should use NumPy array operations.\n        kwargs[\"meta\"] = np.ndarray\n\n    return da.from_array(\n        data,\n        chunks,\n        **kwargs,\n    )  # type: ignore[no-untyped-call]\n\n", "CodeBleu_score": 92.04127144323292}, {"code": "def from_array(\n        self, data: Any, chunks: T_Chunks | _NormalizedChunks, **kwargs: Any\n    ) -> DaskArray | Any\n\t\n", "CodeBleu_score": 22.332506203473944}]}
{"instance_id": "6559019d09a0", "generation_result": [{"code": "def __dask_postpersist__(\n        self,\n    ) -> tuple[\n        Callable[\n            [Graph, PostPersistCallable[Any], Any, Any],\n            Self,\n        ],\n        tuple[Any, ...],\n    ]\n\t  if is_duck_dask_array(self._data):\n        a: tuple[PostPersistCallable[Any], tuple[Any,...]]\n        a = self._data.__dask_postpersist__()  # type: ignore[no-untyped-call]\n        array_func, array_args = a\n\n        return self._dask_finalize, (array_func,) + array_args\n    else:\n        raise AttributeError(\"Method requires self.data to be a dask array.\")\n", "CodeBleu_score": 90.73114578298566}, {"code": "def __dask_postpersist__(\n        self,\n    ) -> tuple[\n        Callable[\n            [Graph, PostPersistCallable[Any], Any, Any],\n            Self,\n        ],\n        tuple[Any, ...],\n    ]\n\t  if is_duck_dask_array(self._data):\n        a: tuple[PostPersistCallable[Any], tuple[Any,...]]\n        a = self._data.__dask_postpersist__()  # type: ignore[no-untyped-call]\n        array_func, array_args = a\n\n        return self._dask_finalize, (array_func,) + array_args\n    else:\n        raise AttributeError(\"Method requires self.data to be a dask array.\")", "CodeBleu_score": 90.73114578298566}, {"code": "def __dask_postpersist__(\n        self,\n    ) -> tuple[\n        Callable[\n            [Graph, PostPersistCallable[Any], Any, Any],\n            Self,\n        ],\n        tuple[Any, ...],\n    ]\n\t  if is_duck_dask_array(self._data):\n        a: tuple[PostPersistCallable[Any], tuple[Any,...]]\n        a = self._data.__dask_postpersist__()  # type: ignore[no-untyped-call]\n        array_func, array_args = a\n\n        return self._dask_finalize, (array_func,) + array_args\n    else:\n        raise AttributeError(\"Method requires self.data to be a dask array.\")\n", "CodeBleu_score": 90.73114578298566}]}
{"instance_id": "4a2801bb3851", "generation_result": [{"code": "def _dask_or_eager_func(\n    name,\n    eager_module=np,\n    dask_module=\"dask.array\",\n)\n\t\n    def f(*args, **kwargs):\n        if any(is_duck_dask_array(x) for x in args):\n            if isinstance(dask_module, str):\n                dask_module = import_module(dask_module)\n            return getattr(dask_module, name)(*args, **kwargs)\n        else:\n            return getattr(eager_module, name)(*args, **kwargs)\n    return f\n", "CodeBleu_score": 46.82056471183707}, {"code": "def _dask_or_eager_func(\n    name,\n    eager_module=np,\n    dask_module=\"dask.array\",\n)\n\t\n\n    def f(*args, **kwargs):\n        if any(is_duck_dask_array(arg) for arg in args):\n            dask_module = import_module(dask_module)\n            func = getattr(dask_module, name)\n        else:\n            func = getattr(eager_module, name)\n        return func(*args, **kwargs)\n\n    return f\n\n", "CodeBleu_score": 53.638894734201806}, {"code": "def _dask_or_eager_func(\n    name,\n    eager_module=np,\n    dask_module=\"dask.array\",\n)\n\t\n    def f(*args, **kwargs):\n        if any(is_duck_dask_array(x) for x in args):\n            if isinstance(dask_module, str):\n                dask_module = import_module(dask_module)\n            return getattr(dask_module, name)(*args, **kwargs)\n        else:\n            return getattr(eager_module, name)(*args, **kwargs)\n    return f", "CodeBleu_score": 46.82056471183707}]}
{"instance_id": "eef8d746d01e", "generation_result": [{"code": "def lazy_array_equiv(arr1, arr2)\n\t\n    if arr1 is arr2:\n        return True\n\n    arr1 = asarray(arr1)\n    arr2 = asarray(arr2)\n\n    if arr1.shape!= arr2.shape:\n        return False\n\n    if is_duck_dask_array(arr1) and is_duck_dask_array(arr2):\n        return tokenize(arr1) == tokenize(arr2)\n\n    return None", "CodeBleu_score": 68.18529094879572}, {"code": "def lazy_array_equiv(arr1, arr2)\n\t\n    if arr1 is arr2:\n        return True\n\n    arr1 = asarray(arr1)\n    arr2 = asarray(arr2)\n\n    if arr1.shape!= arr2.shape:\n        return False\n\n    if hasattr(arr1, \"dask\") and hasattr(arr2, \"dask\"):\n        if tokenize(arr1) == tokenize(arr2):\n            return True\n\n    return None", "CodeBleu_score": 59.12170678617923}, {"code": "def lazy_array_equiv(arr1, arr2)\n\t\n    if arr1 is arr2:\n        return True\n\n    arr1, arr2 = asarray(arr1), asarray(arr2)\n\n    if arr1.shape!= arr2.shape:\n        return False\n\n    if is_duck_dask_array(arr1) and is_duck_dask_array(arr2):\n        return tokenize(arr1) == tokenize(arr2)\n\n    return None", "CodeBleu_score": 61.06945500603816}]}
{"instance_id": "15e830ff5fcb", "generation_result": [{"code": "def randn(shape, frac_nan=None, chunks=None, seed=0)\n\t\n    if chunks is None:\n        return np.random.randn(*shape)\n    else:\n        if frac_nan is not None:\n            assert frac_nan < 1, \"frac_nan must be < 1\"\n        rs = np.random.RandomState(seed)\n        return da.random.random(shape, chunks=chunks, random_state=rs) * (\n            1 - frac_nan\n        ) + (frac_nan / 2)\n\n", "CodeBleu_score": 37.851557277565675}, {"code": "def randn(shape, frac_nan=None, chunks=None, seed=0)\n\t\n    if chunks is None:\n        rng = np.random.default_rng(seed)\n        arr = rng.standard_normal(shape)\n    else:\n        rng = da.random.RandomState(seed)\n        arr = rng.standard_normal(shape, chunks=chunks)\n\n    if frac_nan is not None:\n        arr = da.where(rng.random(shape, chunks=chunks) < frac_nan, np.nan, arr)\n\n    return arr\n\n", "CodeBleu_score": 59.2070671707146}, {"code": "def randn(shape, frac_nan=None, chunks=None, seed=0)\n\t\n    rng = RandomState(seed)\n    if chunks is None:\n        arr = rng.standard_normal(shape)\n    else:\n        arr = dask.array.random.random(shape, chunks=chunks, random_state=rng)\n    if frac_nan is not None:\n        if frac_nan > 0:\n            nan_mask = rng.choice(\n                [True, False], size=arr.shape, p=[frac_nan, 1 - frac_nan]\n            )\n            arr = da.where(nan_mask, np.nan, arr)\n    return arr\n\n", "CodeBleu_score": 45.90440334202709}]}
{"instance_id": "5b57abbbe38b", "generation_result": [{"code": "def analyst_price_target(self) -> pd.DataFrame\n\t\n    if self._analyst_price_target is None:\n        raise YFNotImplementedError('analyst_price_target')\n    return self._analyst_price_target\n\n\n\n\n", "CodeBleu_score": 90.57611571255117}, {"code": "def analyst_price_target(self) -> pd.DataFrame\n\t\n    if self._analyst_price_target is None:\n        raise YFNotImplementedError('analyst_price_target')\n    return self._analyst_price_target\n\n\n\n\n", "CodeBleu_score": 90.57611571255117}, {"code": "def analyst_price_target(self) -> pd.DataFrame\n\t\n    if self._analyst_price_target is None:\n        raise YFNotImplementedError('analyst_price_target')\n    return self._analyst_price_target\n\n\n\n\n", "CodeBleu_score": 90.57611571255117}]}
{"instance_id": "592e414671e9", "generation_result": [{"code": "def _fetch_time_series(self, name, timescale, proxy=None)\n\t\n    if name not in const.fundamentals_keys:\n        raise ValueError(\n            f\"Invalid financial statement name {name}. \"\n            f\"Valid names are {list(const.fundamentals_keys.keys())}\"\n        )\n\n    if timescale not in const.fundamentals_time_periods:\n        raise ValueError(\n            f\"Invalid period {timescale}. \"\n            f\"Valid periods are {list(const.fundamentals_time_periods.keys())}\"\n        )\n\n    try:\n        return self._create_financials_table(name, timescale, proxy)\n    except Exception as e:\n        utils.get_yf_logger().error(e)\n        return pd.DataFrame()\n\n", "CodeBleu_score": 33.077568561539294}, {"code": "def _fetch_time_series(self, name, timescale, proxy=None)\n\t\n    if name not in const.fundamentals_keys:\n        raise ValueError(\"Invalid name\")\n\n    if timescale not in const.fundamentals_periods:\n        raise ValueError(\"Invalid timescale\")\n\n    try:\n        return self._create_financials_table(name, timescale, proxy)\n    except Exception:\n        utils.get_yf_logger().exception(\"Error fetching %s time series\", name)\n        return pd.DataFrame()\n\n", "CodeBleu_score": 28.32163560648914}, {"code": "def _fetch_time_series(self, name, timescale, proxy=None)\n\t\n    if name not in const.fundamentals_keys:\n        raise ValueError(\n            f\"Invalid financial statement name: {name}. \"\n            f\"Allowed values are: {', '.join(const.fundamentals_keys.keys())}\"\n        )\n\n    if timescale not in const.fundamentals_periods:\n        raise ValueError(\n            f\"Invalid period: {timescale}. \"\n            f\"Allowed values are: {', '.join(const.fundamentals_periods)}\"\n        )\n\n    try:\n        return self._create_financials_table(name, timescale, proxy)\n    except Exception as e:\n        utils.get_yf_logger().error(f\"Failed to get {name} time series: {e}\")\n        return pd.DataFrame()\n", "CodeBleu_score": 36.248577336713076}]}
{"instance_id": "af01d9bae091", "generation_result": [{"code": "def _parse_fund_ownership(self, data)\n\t   # Extract the list of ownership details\n    ownership_details = data[\"ownershipDetails\"]\n    # Iterate through each owner's data\n    for owner in ownership_details:\n        # Parse raw values\n        self._parse_raw_values(owner)\n        # Remove the'maxAge' field\n        if \"maxAge\" in owner:\n            del owner[\"maxAge\"]\n    # Convert the processed data into a pandas DataFrame\n    self._mutualfund = pd.DataFrame(ownership_details)\n    # If the DataFrame is not empty\n    if not self._mutualfund.empty:\n        # Convert the'reportDate' to a datetime format\n        self._mutualfund[\"reportDate\"] = pd.to_datetime(self._mutualfund[\"reportDate\"])\n        # Rename specific columns for clarity\n        self._mutualfund = self._mutualfund.rename(\n            columns={\n                \"reportDate\": \"Report Date\",\n                \"ownerName\": \"\",\n                \"ownerCik\": \"Owner CIK\",\n                \"value\": \"Value\",\n                \"valueChange\": \"Value Change\",\n                \"percentTotal\": \"Percent of Total\",\n                \"percentInstitution\": \"Percent Institution\",\n                \"percentMutualFund\": \"Percent Mutual Fund\",\n                \"percentOther\": \"Percent Other\",\n                \"shares\": \"Shares\",\n                \"sharesChange\": \"Shares Change\",\n            }\n        )\n\n", "CodeBleu_score": 35.96913725687415}, {"code": "def _parse_fund_ownership(self, data)\n\t   data = data['ownershipData']\n    data = data['owners']\n    data = data['owner']\n    self._mutualfund = pd.DataFrame(data)\n    self._mutualfund = self._mutualfund.applymap(self._parse_raw_values)\n    self._mutualfund = self._mutualfund.drop(columns=['maxAge'])\n    if not self._mutualfund.empty:\n        self._mutualfund['reportDate'] = pd.to_datetime(self._mutualfund['reportDate'])\n        self._mutualfund = self._mutualfund.rename(columns={'reportDate': 'Report Date', 'name': 'Name', 'title': 'Title', 'pctOut': 'Ownership %'})\n    self._mutualfund = self._mutualfund.set_index('Report Date')\n    return self._mutualfund\n", "CodeBleu_score": 33.60151708464153}, {"code": "def _parse_fund_ownership(self, data)\n\t   owners = data['ownershipGovernance']['owners']\n    owners_data = []\n    for owner in owners:\n        owner_data = {}\n        for key, value in owner.items():\n            if key =='maxAge':\n                continue\n            owner_data[key] = self._parse_raw_values(value)\n        owners_data.append(owner_data)\n    if owners_data:\n        owners_df = pd.DataFrame(owners_data)\n        owners_df['reportDate'] = pd.to_datetime(owners_df['reportDate'])\n        owners_df = owners_df.rename(\n            columns={'reportDate': 'Report Date', 'ownerName': 'Owner Name', 'ownerType': 'Owner Type',\n                     'ownershipPct': 'Ownership Percentage'})\n        self._mutualfund = owners_df\n\n", "CodeBleu_score": 28.014979409733943}]}
{"instance_id": "2b5c6f1253db", "generation_result": [{"code": "def currency(self)\n\t\n    if self.currency is not None:\n        return self.currency\n    else:\n        self.get_history_metadata()\n        self.currency = self.history_metadata['currency']\n        return self.currency\n\n\n\n\n", "CodeBleu_score": 45.599256853109274}, {"code": "def currency(self)\n\t\n    if self.currency is None:\n        self.get_history_metadata()\n        self.currency = self.metadata['currency']\n    return self.currency\n\n\n\n\n", "CodeBleu_score": 34.04749140244726}, {"code": "def currency(self)\n\t\n    if self.currency == None:\n        self.get_history_metadata()\n        self.currency = self.metadata['currency']\n    return self.currency\n\n\n\n\n", "CodeBleu_score": 33.75608080546895}]}
{"instance_id": "16b80be60e3b", "generation_result": [{"code": "def recommendations(self) -> pd.DataFrame\n\t    if self._recommendations is None:\n        self._fetch(self._data.proxy, ['recommendationTrend'])\n        if self._recommendations is None:\n            self._recommendations = pd.DataFrame()\n        else:\n            try:\n                self._recommendations = pd.DataFrame(self._recommendations['recommendationTrend']['trend'])\n            except Exception as e:\n                raise YFDataException(f\"Error parsing recommendationTrend data: {str(e)}\")\n    return self._recommendations\n\n", "CodeBleu_score": 45.33251288052789}, {"code": "def recommendations(self) -> pd.DataFrame\n\t    if self._recommendations is None:\n        self._fetch(self._data.proxy, ['recommendationTrend'])\n        if self._data.raw_data is None:\n            self._recommendations = pd.DataFrame()\n        else:\n            try:\n                self._recommendations = pd.DataFrame(self._data.raw_data['recommendationTrend']['recommendationTrend'])\n                self._recommendations['period'] = pd.to_datetime(self._recommendations['period'], unit='s')\n            except KeyError:\n                raise YFDataException(\"No recommendation data found for the symbol\")\n    return self._recommendations\n", "CodeBleu_score": 48.66503000766787}, {"code": "def recommendations(self) -> pd.DataFrame\n\t    if self._recommendations is None:\n        self._recommendations = self._fetch(self._data.proxy, ['recommendationTrend'])\n        if self._recommendations is None:\n            self._recommendations = pd.DataFrame()\n        else:\n            try:\n                self._recommendations = pd.DataFrame(self._recommendations['recommendationTrend']['trend'])\n            except KeyError:\n                raise YFDataException(\"Failed to parse recommendations data\")\n    return self._recommendations\n", "CodeBleu_score": 49.682654879727565}]}
{"instance_id": "abaf8c2a3ecc", "generation_result": [{"code": "def plot_elbow_curve(clf, X, title='Elbow Plot', cluster_ranges=None, n_jobs=1,\n                     show_cluster_time=True, ax=None, figsize=None,\n                     title_fontsize=\"large\", text_fontsize=\"medium\")\n\t\n    if not hasattr(clf, 'n_clusters'):\n        raise TypeError('Classifier needs to have a n_clusters attribute')\n    if cluster_ranges is None:\n        cluster_ranges = range(2, 11)\n    scores = Parallel(n_jobs=n_jobs)(delayed(clf.set_params(n_clusters=n_clusters).fit)(X).score(X) for n_clusters in cluster_ranges)\n    if ax is None:\n        fig, ax = plt.subplots(figsize=figsize)\n    ax.plot(cluster_ranges, scores, marker='o')\n    ax.set_xlabel('Number of clusters')\n    ax.set_ylabel('Sum of squared errors')\n    ax.set_title(title, fontsize=title_fontsize)\n    if show_cluster_time:\n        ax2 = ax.twinx()\n        durations = Parallel(n_jobs=n_jobs)(delayed(clf.set_params(n_clusters=n_clusters).fit)(X).score(X) for n_clusters in cluster_ranges)\n        ax2.plot(cluster_ranges, durations, marker='o', color='r')\n        ax2.set_ylabel('Clustering duration (s)', color='r')\n        ax2.tick_params('y', colors='r')\n        ax2.set_ylim(bottom=0)\n    return ax\n", "CodeBleu_score": 48.83589688847796}, {"code": "def plot_elbow_curve(clf, X, title='Elbow Plot', cluster_ranges=None, n_jobs=1,\n                     show_cluster_time=True, ax=None, figsize=None,\n                     title_fontsize=\"large\", text_fontsize=\"medium\")\n\t\n    if not hasattr(clf, 'n_clusters'):\n        raise TypeError(\"The clustering algorithm does not have the attribute 'n_clusters'.\")\n    if cluster_ranges is None:\n        cluster_ranges = range(2, 10)\n    if ax is None:\n        _, ax = plt.subplots(figsize=figsize)\n    sse = []\n    time_taken = []\n    for n_clusters in Parallel(n_jobs=n_jobs)(delayed(clf.set_params)(n_clusters=n_clusters)\n                                             .fit(X).score(X) for n_clusters in cluster_ranges):\n        sse.append(-n_clusters)\n        time_taken.append(clf.n_iter_)\n    ax.plot(cluster_ranges, sse, marker='o')\n    ax.set_title(title, fontsize=title_fontsize)\n    ax.set_xlabel(\"Number of Clusters\", fontsize=text_fontsize)\n    ax.set_ylabel(\"Sum of Squared Errors\", fontsize=text_fontsize)\n    ax.grid(True)\n    ax.tick_params(labelsize=text_fontsize)\n    if show_cluster_time:\n        ax2 = ax.twinx()\n        ax2.plot(cluster_ranges, time_taken, marker='o', color='r')\n        ax2.set_ylabel(\"Clustering Time (seconds)\", color='r', fontsize=text_fontsize)\n        ax2.tick_params('y', colors='r', labelsize=text_fontsize)\n    return ax", "CodeBleu_score": 51.49907902445523}, {"code": "def plot_elbow_curve(clf, X, title='Elbow Plot', cluster_ranges=None, n_jobs=1,\n                     show_cluster_time=True, ax=None, figsize=None,\n                     title_fontsize=\"large\", text_fontsize=\"medium\")\n\t\n    if not hasattr(clf, 'n_clusters'):\n        raise TypeError('The clustering algorithm does not have the attribute \"n_clusters\".')\n\n    if cluster_ranges is None:\n        cluster_ranges = range(2, 11)\n\n    scores = Parallel(n_jobs=n_jobs)(delayed(clf.set_params(n_clusters=n_clusters).fit)(X).score(X)\n                                     for n_clusters in cluster_ranges)\n\n    if ax is None:\n        fig, ax = plt.subplots(figsize=figsize)\n    ax.plot(cluster_ranges, np.absolute(scores), '-o')\n    ax.set_ylabel('Sum of Squared Errors', fontsize=text_fontsize)\n    ax.set_xlabel('Number of Clusters', fontsize=text_fontsize)\n    ax.set_title(title, fontsize=title_fontsize)\n    ax.grid(True)\n    ax.tick_params(labelsize=text_fontsize)\n\n    if show_cluster_time:\n        durations = Parallel(n_jobs=n_jobs)(delayed(clf.set_params(n_clusters=n_clusters).fit)(X)\n                                            for n_clusters in cluster_ranges)\n        durations = [np.round(duration.duration, 2) for duration in durations]\n\n        ax2 = ax.twinx()\n        ax2.plot(cluster_ranges, durations, '-o', color='r')\n        ax2.set_ylabel('Clustering Duration (s)', color='r', fontsize=text_fontsize)\n        ax2.tick_params('y', colors='r', labelsize=text_fontsize)\n\n    return ax\n\n", "CodeBleu_score": 57.56105961302636}]}
{"instance_id": "8a90815c4ee1", "generation_result": [{"code": "def _clone_and_score_clusterer(clf, X, n_clusters)\n\t\n    start_time = time.time()\n    clf = clone(clf)\n    clf.n_clusters = n_clusters\n    clf.fit(X)\n    score = clf.score(X)\n    time_taken = time.time() - start_time\n    return score, time_taken\n\n", "CodeBleu_score": 38.90537012627341}, {"code": "def _clone_and_score_clusterer(clf, X, n_clusters)\n\t\n    t0 = time.time()\n    clf = clone(clf)\n    clf.set_params(n_clusters=n_clusters)\n    clf.fit(X)\n    score = clf.score(X)\n    time_taken = time.time() - t0\n    return score, time_taken\n\n", "CodeBleu_score": 39.77908454419575}, {"code": "def _clone_and_score_clusterer(clf, X, n_clusters)\n\t\n    start_time = time.time()\n    clf = clone(clf)\n    clf.n_clusters = n_clusters\n    clf.fit(X)\n    score = clf.score(X)\n    elapsed_time = time.time() - start_time\n    return score, elapsed_time\n\n", "CodeBleu_score": 38.524267927182585}]}
{"instance_id": "da40bd5dd81a", "generation_result": [{"code": "def clustering_factory(clf)\n\t\n    if not hasattr(clf, 'fit') or not hasattr(clf, 'fit_predict'):\n        raise TypeError('The object is not a valid clusterer instance.')\n    for method in ['plot_silhouette', 'plot_elbow_curve']:\n        if hasattr(clf, method):\n            warnings.warn(f'The method {method} already exists. It will be overridden.')\n        setattr(clf, method, MethodType(locals()[method], clf))\n    return clf\n\n", "CodeBleu_score": 25.16618989021049}, {"code": "def clustering_factory(clf)\n\t\n    if not hasattr(clf, 'fit') or not hasattr(clf, 'fit_predict'):\n        raise TypeError('The object is not a valid clusterer instance.')\n\n    def plot_silhouette(X, **kwargs):\n        \"\"\"The function plot_silhouette takes the data X and optional keyword arguments kwargs as input.", "CodeBleu_score": 9.526338223767038}, {"code": "def clustering_factory(clf)\n\t\n    if not hasattr(clf, 'fit') or not hasattr(clf, 'fit_predict'):\n        raise TypeError('clf must have methods fit and fit_predict')\n\n    def plot_silhouette(X, **kwargs):\n        \"\"\"The function plot_silhouette takes a feature matrix X as an argument.", "CodeBleu_score": 8.588186514960581}]}
{"instance_id": "b8d8ba3c0b1b", "generation_result": [{"code": "def plot_learning_curve(clf, X, y, title='Learning Curve', cv=None,\n                        train_sizes=None, n_jobs=1, scoring=None,\n                        ax=None, figsize=None, title_fontsize=\"large\",\n                        text_fontsize=\"medium\")\n\t\n    if ax is None:\n        fig, ax = plt.subplots(figsize=figsize)\n    else:\n        fig = ax.figure\n    ax.set_title(title, fontsize=title_fontsize)\n    if ylim is not None:\n        ax.set_ylim(*ylim)\n    ax.set_xlabel(\"Training examples\")\n    ax.set_ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        clf, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes, scoring=scoring)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    ax.grid()\n    ax.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                    train_scores_mean + train_scores_std, alpha=0.1,\n                    color=\"r\")\n    ax.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                    test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    ax.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n            label=\"Training score\")\n    ax.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n            label=\"Cross-validation score\")\n    ax.legend(loc=\"best\")\n    return ax\n\n", "CodeBleu_score": 72.82914464654583}, {"code": "def plot_learning_curve(clf, X, y, title='Learning Curve', cv=None,\n                        train_sizes=None, n_jobs=1, scoring=None,\n                        ax=None, figsize=None, title_fontsize=\"large\",\n                        text_fontsize=\"medium\")\n\t\n    if ax is None:\n        fig, ax = plt.subplots(1, 1, figsize=figsize)\n\n    ax.set_title(title, fontsize=title_fontsize)\n    ax.set_xlabel(\"Training examples\")\n    ax.set_ylabel(\"Score\")\n\n    train_sizes, train_scores, test_scores = learning_curve(\n        clf, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes, scoring=scoring)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    ax.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                    train_scores_mean + train_scores_std, alpha=0.1,\n                    color=\"r\")\n    ax.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                    test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    ax.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n            label=\"Training score\")\n    ax.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n            label=\"Cross-validation score\")\n\n    ax.legend(loc=\"best\")\n    ax.grid()\n    ax.tick_params(labelsize=text_fontsize)\n    return ax\n", "CodeBleu_score": 75.40067122695783}, {"code": "def plot_learning_curve(clf, X, y, title='Learning Curve', cv=None,\n                        train_sizes=None, n_jobs=1, scoring=None,\n                        ax=None, figsize=None, title_fontsize=\"large\",\n                        text_fontsize=\"medium\")\n\t\n    if ax is None:\n        fig, ax = plt.subplots(figsize=figsize)\n    ax.set_title(title, fontsize=title_fontsize)\n    if cv is None:\n        cv = ShuffleSplit(n_splits=100, test_size=0.2, random_state=0)\n    if train_sizes is None:\n        train_sizes = np.linspace(.1, 1.0, 5)\n    train_sizes, train_scores, test_scores = learning_curve(clf, X, y, cv=cv, n_jobs=n_jobs,\n                                                            train_sizes=train_sizes, scoring=scoring)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    ax.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                    train_scores_mean + train_scores_std, alpha=0.1,\n                    color=\"r\")\n    ax.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                    test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    ax.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n            label=\"Training score\")\n    ax.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n            label=\"Cross-validation score\")\n    ax.legend(loc=\"best\", fontsize=text_fontsize)\n    ax.grid(True)\n    ax.set_xlabel(\"Training examples\", fontsize=text_fontsize)\n    ax.set_ylabel(\"Score\", fontsize=text_fontsize)\n    return ax\n", "CodeBleu_score": 74.88855390339725}]}
{"instance_id": "e6e76eb35945", "generation_result": [{"code": "def plot_elbow_curve(clf, X, title='Elbow Plot', cluster_ranges=None,\n                     ax=None, figsize=None, title_fontsize=\"large\",\n                     text_fontsize=\"medium\")\n\t\n    if not hasattr(clf, \"n_clusters\"):\n        raise TypeError(\"The classifier must have a 'n_clusters' attribute.\")\n\n    if cluster_ranges is None:\n        cluster_ranges = range(1, 11, 2)\n\n    clfs = []\n    scores = []\n    for i in cluster_ranges:\n        current_clf = clone(clf)\n        setattr(current_clf, \"n_clusters\", i)\n        current_clf.fit(X)\n        score = current_clf.score(X)\n        scores.append(score)\n        clfs.append(current_clf)\n\n    if ax is None:\n        fig, ax = plt.subplots(figsize=figsize)\n\n    ax.plot(cluster_ranges, np.absolute(scores))\n    ax.set_xlabel(\"Number of Clusters\")\n    ax.set_ylabel(\"Score\")\n    ax.set_title(title)\n    ax.grid()\n    ax.tick_params(axis=\"both\", which=\"major\", labelsize=text_fontsize)\n    ax.tick_params(axis=\"both\", which=\"minor\", labelsize=text_fontsize)\n    ax.title.set_fontsize(title_fontsize)\n\n    return ax\n", "CodeBleu_score": 57.530297250495806}, {"code": "def plot_elbow_curve(clf, X, title='Elbow Plot', cluster_ranges=None,\n                     ax=None, figsize=None, title_fontsize=\"large\",\n                     text_fontsize=\"medium\")\n\t\n    if not hasattr(clf, \"n_clusters\"):\n        raise TypeError(f\"The classifier {clf} does not have an n_clusters attribute.\")\n    if cluster_ranges is None:\n        cluster_ranges = range(1, 11, 2)\n    clfs = []\n    scores = []\n    for n_clusters in cluster_ranges:\n        current_clf = clone(clf)\n        setattr(current_clf, \"n_clusters\", n_clusters)\n        current_clf.fit(X)\n        score = current_clf.score(X)\n        clfs.append(current_clf)\n        scores.append(score)\n    if ax is None:\n        _, ax = plt.subplots(1, 1, figsize=figsize)\n    ax.plot(cluster_ranges, np.absolute(scores), marker=\"o\")\n    ax.set_title(title, fontsize=title_fontsize)\n    ax.set_xlabel(\"Number of clusters\", fontsize=text_fontsize)\n    ax.set_ylabel(\"Score\", fontsize=text_fontsize)\n    ax.grid(True)\n    ax.tick_params(axis=\"both\", which=\"major\", labelsize=text_fontsize)\n    return ax", "CodeBleu_score": 62.50361125828058}, {"code": "def plot_elbow_curve(clf, X, title='Elbow Plot', cluster_ranges=None,\n                     ax=None, figsize=None, title_fontsize=\"large\",\n                     text_fontsize=\"medium\")\n\t\n    if not hasattr(clf, \"n_clusters\"):\n        raise TypeError(\n            \"The provided classifier does not have an n_clusters attribute.\")\n    if cluster_ranges is None:\n        cluster_ranges = range(1, 11, 2)\n    clfs = []\n    scores = []\n    for n_clusters in cluster_ranges:\n        setattr(clf, \"n_clusters\", n_clusters)\n        current_clf = clone(clf)\n        current_clf.fit(X)\n        score = current_clf.score(X)\n        scores.append(score)\n        clfs.append(current_clf)\n    if ax is None:\n        _, ax = plt.subplots(1, 1, figsize=figsize)\n    ax.plot(cluster_ranges, np.absolute(scores))\n    ax.set_xlabel(\"Number of clusters\")\n    ax.set_ylabel(\"Score (higher is better)\")\n    ax.set_title(title)\n    ax.grid(True)\n    ax.tick_params(labelsize=text_fontsize)\n    ax.set_title(title, fontsize=title_fontsize)\n    return ax", "CodeBleu_score": 59.84756459607026}]}
{"instance_id": "6f4530c8a20f", "generation_result": [{"code": "def plot_pca_2d_projection(clf, X, y, title='PCA 2-D Projection', ax=None,\n                           figsize=None, cmap='Spectral',\n                           title_fontsize=\"large\", text_fontsize=\"medium\")\n\t\n    if ax is None:\n        fig, ax = plt.subplots(figsize=figsize)\n    else:\n        fig = ax.figure\n    X_pca = clf.transform(X)\n    ax.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap=cmap)\n    ax.set_title(title, fontsize=title_fontsize)\n    ax.set_xlabel('First principal component')\n    ax.set_ylabel('Second principal component')\n    ax.legend(np.unique(y), title='Classes', fontsize=text_fontsize)\n    return ax\n", "CodeBleu_score": 32.7762653154098}, {"code": "def plot_pca_2d_projection(clf, X, y, title='PCA 2-D Projection', ax=None,\n                           figsize=None, cmap='Spectral',\n                           title_fontsize=\"large\", text_fontsize=\"medium\")\n\t\n    if ax is None:\n        fig, ax = plt.subplots(figsize=figsize)\n\n    # Perform PCA transformation\n    X_pca = clf.transform(X)\n\n    # Plot the 2D scatter plot\n    ax.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap=cmap)\n\n    # Set the axis labels\n    ax.set_xlabel(\"PC1\")\n    ax.set_ylabel(\"PC2\")\n\n    # Set the title and font size\n    ax.set_title(title, fontsize=title_fontsize)\n\n    # Set the legend\n    legend_handles = []\n    for label in np.unique(y):\n        legend_handles.append(\n            mpl.lines.Line2D(\n                [], [], color=cmap(label), marker=\"o\", linestyle=\"None\",\n                label=label\n            )\n        )\n    ax.legend(handles=legend_handles, title=\"Labels\", loc=\"best\")\n\n    # Set the tick parameters\n    ax.tick_params(axis=\"both\", which=\"major\", labelsize=text_fontsize)\n\n    return ax\n\n", "CodeBleu_score": 38.867098552708455}, {"code": "def plot_pca_2d_projection(clf, X, y, title='PCA 2-D Projection', ax=None,\n                           figsize=None, cmap='Spectral',\n                           title_fontsize=\"large\", text_fontsize=\"medium\")\n\t\n    # Perform PCA\n    pca = PCA(n_components=2)\n    X_pca = pca.fit_transform(X)\n\n    # Set up the plot\n    if ax is None:\n        fig, ax = plt.subplots(figsize=figsize)\n    else:\n        fig = ax.get_figure()\n\n    # Plot the data\n    ax.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap=cmap)\n\n    # Label the axes\n    ax.set_xlabel(f\"Principal Component 1 ({pca.explained_variance_ratio_[0]:.2%})\", fontsize=text_fontsize)\n    ax.set_ylabel(f\"Principal Component 2 ({pca.explained_variance_ratio_[1]:.2%})\", fontsize=text_fontsize)\n\n    # Add a legend\n    classes = np.unique(y)\n    ax.legend(classes, title=\"Classes\", loc=\"best\", fontsize=text_fontsize)\n\n    # Set the title\n    ax.set_title(title, fontsize=title_fontsize)\n\n    # Return the axis object\n    return ax\n\n", "CodeBleu_score": 40.13674763644049}]}
{"instance_id": "e7c2775d024c", "generation_result": [{"code": "def barplot(ax)\n\t\n    clip1, clip2 = clip_path(ax)\n\n    Y1 = uniform(0.1, 0.8, size=16)\n    Y2 = uniform(0.1, 0.8, size=16)\n\n    ax.bar(np.arange(16), Y1, width=0.9, align=\"edge\", clip_path=clip1,\n           color=\"0.5\", edgecolor=\"white\", linewidth=1)\n    ax.bar(np.arange(16), Y2, width=0.9, align=\"edge\", clip_path=clip2,\n           color=\"0.5\", edgecolor=\"white\", linewidth=1)\n    ax.bar(np.arange(16), Y1, width=0.9, align=\"edge\", clip_path=clip1,\n           color=\"r\", edgecolor=\"white\", linewidth=1)\n    ax.bar(np.arange(16), Y2, width=0.9, align=\"edge\", clip_path=clip2,\n           color=\"r\", edgecolor=\"white\", linewidth=1)\n\n    ax.set_xlim(-1, 16)\n\n    ax.text(0.5, 0.9, \"Bar plot\", transform=ax.transAxes, ha=\"center\",\n            fontsize=24, color=\"black\")\n    ax.text(0.5, 0.8, \"ax.bar()\", transform=ax.transAxes, ha=\"center\",\n            fontsize=16, color=\"blue\", family=\"monospace\")\n\n", "CodeBleu_score": 32.206989465752834}, {"code": "def barplot(ax)\n\t\n    clip1, clip2 = clip_path(ax)\n\n    # Generate data for 16 bars\n    Y1 = uniform(0.1, 0.9, size=16)\n    Y2 = uniform(0.1, 0.9, size=16)\n\n    # Plot 4 sets of bars\n    ax.bar(np.arange(16), Y1, color=\"0.7\", clip_path=clip1)\n    ax.bar(np.arange(16), Y2, color=\"0.7\", clip_path=clip2)\n    ax.bar(np.arange(16), Y1, color=\"0.4\", clip_path=clip1)\n    ax.bar(np.arange(16), Y2, color=\"0.4\", clip_path=clip2)\n\n    # Set x-axis limits\n    ax.set_xlim(-1, 16)\n\n    # Add text labels\n    ax.text(-0.1, 1.15, \"Bar plot\", transform=ax.transAxes,\n            ha=\"right\", va=\"top\", weight=\"bold\", size=\"large\")\n    ax.text(-0.1, 1.1, \"ax.bar()\", transform=ax.transAxes,\n            ha=\"right\", va=\"top\", size=\"medium\")\n\n", "CodeBleu_score": 30.20287603173334}, {"code": "def barplot(ax)\n\t\n    clip1, clip2 = clip_path(ax)\n\n    Y1 = uniform(0.2, 0.8, 16)\n    Y2 = uniform(0.2, 0.8, 16)\n\n    ax.bar(np.arange(16), Y1, 1, clip_path=clip1, color='0.75', edgecolor='0.75')\n    ax.bar(np.arange(16), Y2, 1, clip_path=clip2, color='0.75', edgecolor='0.75')\n\n    ax.bar(np.arange(16), Y1, 1, clip_path=clip1, color='r', edgecolor='r')\n    ax.bar(np.arange(16), Y2, 1, clip_path=clip2, color='r', edgecolor='r')\n\n    ax.set_xlim(-1, 16)\n\n    ax.text(-0.5, 0.5, \"Bar plot\",\n            transform=ax.transAxes,\n            horizontalalignment='center',\n            verticalalignment='center')\n\n    ax.text(0.5, 0.5, \"ax.bar()\",\n            transform=ax.transAxes,\n            horizontalalignment='center',\n            verticalalignment='center',\n            fontdict=dict(family='monospace'))\n", "CodeBleu_score": 35.320522907115674}]}
{"instance_id": "0ec1d146a32b", "generation_result": [{"code": "def split(n_segment)\n\t\n    X0 = np.arange(0, n_segment)\n    X1 = np.arange(1, n_segment + 1)\n    return X0, X1\n\n", "CodeBleu_score": 11.283893063391078}, {"code": "def split(n_segment)\n\t\n    #n_segment = 2\n    #n_segment = 5\n    #n_segment = 10\n    #n_segment = 100\n    #n_segment = 1000\n    #n_segment = 10000\n    #n_segment = 100000\n    #n_segment = 1000000\n    #n_segment = 10000000\n    #n_segment = 100000000\n    #n_segment = 1000000000\n    #n_segment = 10000000000\n    #n_segment = 100000000000\n    #n_segment = 1000000000000\n    #n_segment = 10000000000000\n    #n_segment = 100000000000000\n    #n_segment = 1000000000000000\n    #n_segment = 10000000000000000\n    #n_segment = 100000000000000000\n    #n_segment = 1000000000000000000\n    #n_segment = 10000000000000000000\n    #n_segment = 100000000000000000000\n    #n_segment = 1000000000000000000000\n    #n_segment = 10000000000000000000000\n    #n_segment = 100000000000000000000000\n    #n_segment = 1000000000000000000000000\n    #n_", "CodeBleu_score": 0.7575757575757576}, {"code": "def split(n_segment)\n\t\n    #n_segment=4\n    #width=100\n    #padding=20\n    width=100\n    padding=20\n    X0=np.arange(0,n_segment*width+(n_segment-1)*padding,width+padding)\n    X1=X0+width\n    return X0,X1\n\n", "CodeBleu_score": 31.075233476434565}]}
{"instance_id": "604a3e93ac0d", "generation_result": [{"code": "def pixelated_text(dpi=100)\n\t\n    fig = plt.figure(figsize=(75 / dpi, 75 / dpi), dpi=dpi)\n    canvas = FigureCanvasAgg(fig)\n    ax = fig.gca()\n    ax.text(0.5, 0.5, 'a', va='center', ha='center', fontsize=75)\n    ax.axis('off')\n    canvas.draw()\n    image = np.frombuffer(canvas.tostring_argb(), dtype=np.uint8)\n    image = image.reshape(fig.canvas.get_width_height()[::-1] + (4,))\n    image = np.roll(image, shift=3, axis=2)\n    return image\n", "CodeBleu_score": 52.92147887180831}, {"code": "def pixelated_text(dpi=100)\n\t\n    fig = plt.figure(figsize=(75 / dpi, 75 / dpi), dpi=dpi)\n    canvas = FigureCanvas(fig)\n    ax = fig.gca()\n    ax.text(0.5, 0.5, \"a\", va=\"center\", ha=\"center\", size=75)\n    ax.axis(\"off\")\n    canvas.draw()\n    image = np.frombuffer(canvas.tostring_argb(), dtype=np.uint8)\n    image = image.reshape(fig.canvas.get_width_height()[::-1] + (4,))\n    image = np.roll(image, shift=3, axis=2)\n    return image\n", "CodeBleu_score": 56.21280218200049}, {"code": "def pixelated_text(dpi=100)\n\t\n    fig = plt.figure(figsize=(1, 1), dpi=dpi)\n    ax = fig.gca()\n    ax.text(0.5, 0.5, 'a', va='center', ha='center', fontsize=75)\n    ax.axis('off')\n    canvas = FigureCanvasAgg(fig)\n    canvas.draw()\n    image = np.frombuffer(canvas.tostring_argb(), dtype=np.uint8)\n    image = image.reshape(fig.canvas.get_width_height()[::-1] + (4,))\n    image = np.roll(image, shift=1, axis=2)\n    return image\n\n", "CodeBleu_score": 54.50994487227667}]}
{"instance_id": "2a7fc7d560ea", "generation_result": [{"code": "def curve()\n\t\n    # Set the number of components to a random integer between 2 and 10\n    n = randint(2, 10)\n\n    # Set the range of possible centers and widths\n    centers = np.linspace(0, 10, 100)\n    widths = np.linspace(0.1, 1, 100)\n\n    # Initialize an array of zeros\n    curve = np.zeros(100)\n\n    # Iterate over the number of components\n    for i in range(n):\n        # Randomly select a center, width, and scale\n        center = uniform(0, 10)\n        width = uniform(0.1, 1)\n        scale = uniform(0, 1)\n\n        # Calculate the Gaussian component\n        gaussian = scale * np.exp(-(centers - center)**2 / (2 * width**2))\n\n        # Normalize the Gaussian component to sum to 1\n        gaussian /= gaussian.sum()\n\n        # Add the Gaussian component to the curve\n        curve += gaussian\n\n    # Normalize the curve to sum to 1\n    curve /= curve.sum()\n\n    return curve\n", "CodeBleu_score": 32.66314029789431}, {"code": "def curve()\n\t\n    n = randint(1, 4)\n    centers = np.linspace(0, 1, n)\n    widths = np.abs(normal(0.1, 0.05, n))\n    scales = normal(1, 0.5, n)\n    widths = widths / widths.sum() * 0.5\n    scales = scales / scales.sum() * 2\n    curve = np.zeros(100)\n    for c, w, s in zip(centers, widths, scales):\n        curve += s * np.exp(-(np.linspace(0, 1, 100) - c)**2 / w**2)\n    return curve", "CodeBleu_score": 37.65545937154874}, {"code": "def curve()\n\t\n    # Generate random number of components\n    n_components = randint(1, 5)\n    # Initialize array of zeros\n    curve = np.zeros(1000)\n    # Loop over the number of components\n    for i in range(n_components):\n        # Generate random center, width, and scale\n        center = randint(0, 1000)\n        width = randint(1, 100)\n        scale = randint(1, 10)\n        # Add scaled Gaussian component to the curve\n        curve += scale * normal(center, width, 1000)\n    # Normalize the curve\n    curve /= curve.max()\n    return curve\n", "CodeBleu_score": 20.735236531522034}]}
{"instance_id": "be694335ebfc", "generation_result": [{"code": "def difference(derivative, accuracy=1)\n\t\n    derivative += 1\n    radius = 0.5 * accuracy * derivative\n    points = np.arange(-radius, radius + 1)\n    vander = np.vander(points, increasing=True)\n    coefficients = np.linalg.inv(vander)[derivative - 1]\n    coefficients /= np.math.factorial(derivative - 1)\n    return coefficients, points\n", "CodeBleu_score": 49.45936142753343}, {"code": "def difference(derivative, accuracy=1)\n\t\n    order = derivative + 1\n    radius = accuracy * order\n    points = np.arange(-radius, radius + 1)\n    vander = np.vander(points, increasing=True)\n    inv = np.linalg.inv(vander)\n    return inv[order, :], points\n", "CodeBleu_score": 43.04019038593779}, {"code": "def difference(derivative, accuracy=1)\n\t\n    order = derivative + 1\n    radius = accuracy * order\n    points = np.arange(-radius, radius + 1)\n    vander = np.vander(points)\n    inv = np.linalg.inv(vander)\n    coefficients = inv[order]\n    coefficients /= np.math.factorial(derivative)\n    return coefficients, points\n", "CodeBleu_score": 44.48386070841958}]}
{"instance_id": "fa3fb47f33fa", "generation_result": [{"code": "def interpolate(X, Y, T)\n\t\n    # Calculate the distances between consecutive points\n    distances = np.sqrt(np.diff(X) ** 2 + np.diff(Y) ** 2)\n    # Compute the cumulative distances\n    cumulative_distances = np.cumsum(distances)\n    # Interpolate the X and Y coordinates at the positions specified by T\n    interpolated_X = np.interp(T, cumulative_distances, X)\n    interpolated_Y = np.interp(T, cumulative_distances, Y)\n    # Calculate the total distance covered by the original coordinates\n    total_distance = np.sum(distances)\n    return interpolated_X, interpolated_Y, total_distance\n\n", "CodeBleu_score": 32.576772238759496}, {"code": "def interpolate(X, Y, T)\n\t\n    d = np.diff(X)\n    s = np.cumsum(d)\n    s = np.insert(s, 0, 0)\n    return np.interp(T, s, X), np.interp(T, s, Y), s[-1]\n\n", "CodeBleu_score": 20.950212047678523}, {"code": "def interpolate(X, Y, T)\n\t\n    # Calculate the distances between consecutive points\n    distances = np.sqrt(np.diff(X)**2 + np.diff(Y)**2)\n    # Compute the cumulative distances\n    cumulative_distances = np.cumsum(distances)\n    # Interpolate the X and Y coordinates at the positions specified by T\n    interpolated_X = np.interp(T, cumulative_distances, X)\n    interpolated_Y = np.interp(T, cumulative_distances, Y)\n    # Return the interpolated X and Y coordinates along with the total distance covered by the original coordinates\n    return interpolated_X, interpolated_Y, np.sum(distances)\n\n", "CodeBleu_score": 27.339830672173033}]}
{"instance_id": "870a2dad3c66", "generation_result": [{"code": "def mandelbrot_set(xmin, xmax, ymin, ymax, xn, yn, maxiter, horizon=2.0)\n\t\n    X = np.linspace(xmin, xmax, xn).astype(np.float32)\n    Y = np.linspace(ymin, ymax, yn).astype(np.float32)\n    C = X + Y[:, None] * 1j\n    N = np.zeros(C.shape, dtype=int)\n    Z = np.zeros(C.shape, np.complex64)\n    for n in range(maxiter):\n        I = np.less(abs(Z), horizon)\n        N[I] = n\n        Z[I] = Z[I]**2 + C[I]\n    N[N == maxiter-1] = 0\n    return Z, N\n", "CodeBleu_score": 70.26541649753419}, {"code": "def mandelbrot_set(xmin, xmax, ymin, ymax, xn, yn, maxiter, horizon=2.0)\n\t\n    X = np.linspace(xmin, xmax, xn).astype(np.float32)\n    Y = np.linspace(ymin, ymax, yn).astype(np.float32)\n    C = X + Y[:, None] * 1j\n    N = np.zeros_like(C, dtype=int)\n    Z = np.zeros_like(C)\n    for n in range(maxiter):\n        I = np.less(abs(Z), horizon)\n        N[I] = n\n        Z[I] = Z[I]**2 + C[I]\n    N[N == maxiter-1] = 0\n    return Z, N\n", "CodeBleu_score": 60.99525936332715}, {"code": "def mandelbrot_set(xmin, xmax, ymin, ymax, xn, yn, maxiter, horizon=2.0)\n\t\n    X = np.linspace(xmin, xmax, xn).astype(np.float32)\n    Y = np.linspace(ymin, ymax, yn).astype(np.float32)\n    C = X + Y[:, None] * 1j\n    N = np.zeros_like(C, dtype=int)\n    Z = np.zeros_like(C)\n    for n in range(maxiter):\n        I = np.less(abs(Z), horizon)\n        N[I] = n\n        Z[I] = Z[I]**2 + C[I]\n    N[N == maxiter-1] = 0\n    return Z, N\n\n", "CodeBleu_score": 60.99525936332715}]}
{"instance_id": "e1a908945acf", "generation_result": [{"code": "def smooth2d(A, sigma=3)\n\t\n    window_len = max(int(sigma) * 3 + 1, 3)\n    if window_len % 2 == 0:\n        window_len += 1\n    A1 = np.asarray([smooth1d(x, window_len) for x in A])\n    A1 = np.transpose(A1)\n    A1 = np.asarray([smooth1d(x, window_len) for x in A1])\n    A1 = np.transpose(A1)\n    return A1\n\n", "CodeBleu_score": 40.94367031429632}, {"code": "def smooth2d(A, sigma=3)\n\t\n    window_len = max(int(sigma * 3), 3)\n    if window_len % 2 == 0:\n        window_len += 1\n    A1 = np.asarray([smooth1d(x, window_len) for x in A])\n    A1 = np.transpose(A1)\n    A1 = np.asarray([smooth1d(x, window_len) for x in A1])\n    A1 = np.transpose(A1)\n    return A1\n\n", "CodeBleu_score": 49.82846626877048}, {"code": "def smooth2d(A, sigma=3)\n\t\n    window_len = max(int(sigma) * 3 + 1, 3)\n    if window_len % 2 == 0:\n        window_len += 1\n    A1 = np.transpose(A)\n    A2 = np.transpose(A1)\n    sA = smooth1d(A, window_len)\n    sA1 = np.transpose(sA)\n    sA2 = smooth1d(sA1, window_len)\n    sA3 = np.transpose(sA2)\n    return sA3\n\n", "CodeBleu_score": 28.929550074343403}]}
{"instance_id": "94563c54b483", "generation_result": [{"code": "def process_image(self, padded_src, dpi)\n\t\n    ox, oy = self.ox, self.oy\n    ox_pix, oy_pix = ox * dpi, oy * dpi\n    return np.roll(np.roll(padded_src, int(ox_pix), axis=1), int(oy_pix), axis=0)\n", "CodeBleu_score": 29.610649534618076}, {"code": "def process_image(self, padded_src, dpi)\n\t\n    ox, oy = self.ox, self.oy\n    return np.roll(np.roll(padded_src, int(ox * dpi / 25.4), axis=1), int(oy * dpi / 25.4), axis=0)\n", "CodeBleu_score": 27.684428507905707}, {"code": "def process_image(self, padded_src, dpi)\n\t\n    ox, oy = self.ox, self.oy\n    ox, oy = ox * dpi, oy * dpi\n    return np.roll(np.roll(padded_src, int(ox), axis=1), int(oy), axis=0)\n", "CodeBleu_score": 28.4609923193973}]}
{"instance_id": "d71d232872fe", "generation_result": [{"code": "def gaussian(shape=(25, 25), width=0.5, center=0.0)\n\t\n    shape = np.array(shape)\n    width = np.array(width)\n    center = np.array(center)\n    grid = [slice(-c, c) for c, w in zip(shape, width)]\n    grid = np.mgrid[grid]\n    grid = grid.astype(float)\n    grid = grid / width\n    grid = grid - center\n    grid = np.sum(grid ** 2, 0)\n    grid = np.exp(-grid / 2)\n    return grid\n\n", "CodeBleu_score": 19.836033255056414}, {"code": "def gaussian(shape=(25, 25), width=0.5, center=0.0)\n\t\n    if isinstance(shape, int):\n        shape = (shape, shape)\n    if isinstance(width, float):\n        width = (width, width)\n    if isinstance(center, float):\n        center = (center, center)\n    grid = [np.linspace(-center[i] + (i - shape[i] / 2) * width[i] / shape[i],\n                        -center[i] + (i + 1 - shape[i] / 2) * width[i] / shape[i],\n                        shape[i]) for i in range(len(shape))]\n    grid = np.meshgrid(*grid)\n    grid = np.array(grid)\n    grid = np.moveaxis(grid, 0, -1)\n    grid = grid.reshape(-1, len(shape))\n    grid = np.sum((grid - center) ** 2, axis=1)\n    grid = grid / (2 * width[0] ** 2)\n    return np.exp(-grid)\n\n", "CodeBleu_score": 35.772750703916735}, {"code": "def gaussian(shape=(25, 25), width=0.5, center=0.0)\n\t\n    if np.iterable(shape):\n        shape = tuple(shape)\n    else:\n        shape = (shape, shape)\n    if np.iterable(width):\n        width = tuple(width)\n    else:\n        width = (width, width)\n    if np.iterable(center):\n        center = tuple(center)\n    else:\n        center = (center, center)\n    grid = np.meshgrid(np.linspace(-1, 1, shape[0]), np.linspace(-1, 1, shape[1]))\n    grid = np.sqrt((grid[0] - center[0]) ** 2 + (grid[1] - center[1]) ** 2)\n    return np.exp(-(grid / width[0]) ** 2 - (grid / width[1]) ** 2)\n", "CodeBleu_score": 34.79050115164334}]}
{"instance_id": "7bbd08c987bc", "generation_result": [{"code": "def stimulus(position, size, intensity)\n\t\n    # Convert polar coordinates to Cartesian coordinates\n    x = position[0] * np.cos(position[1])\n    y = position[0] * np.sin(position[1])\n    # Normalize coordinates\n    x_norm = x / (2 * np.pi)\n    y_norm = y / (2 * np.pi)\n    # Create a mesh grid representing the spatial domain, normalized and centered\n    X, Y = np.meshgrid(np.linspace(-1, 1, 512), np.linspace(-1, 1, 512))\n    # Compute the squared Euclidean distance (R) from each point in the grid to the stimulus center\n    R = (X - x_norm) ** 2 + (Y - y_norm) ** 2\n    # Return an exponential decay function of these distances, scaled by the size parameter\n    return intensity * np.exp(-R / size ** 2)", "CodeBleu_score": 21.619775301199084}, {"code": "def stimulus(position, size, intensity)\n\t\n    # Convert polar coordinates to Cartesian coordinates\n    x = position[0] * np.cos(position[1])\n    y = position[0] * np.sin(position[1])\n    # Normalize the coordinates\n    x_norm = x / 100\n    y_norm = y / 100\n    # Create a mesh grid representing the spatial domain, normalized and centered\n    X, Y = np.meshgrid(np.linspace(-1, 1, 100), np.linspace(-1, 1, 100))\n    # Compute the squared Euclidean distance from each point in the grid to the stimulus center\n    R = (X - x_norm) ** 2 + (Y - y_norm) ** 2\n    # Return an exponential decay function of these distances, scaled by the size parameter\n    return intensity * np.exp(-R / size ** 2)\n", "CodeBleu_score": 20.448310402817615}, {"code": "def stimulus(position, size, intensity)\n\t\n    x, y = position\n    x, y = np.deg2rad(x), np.deg2rad(y)\n    x, y = np.cos(x), np.sin(y)\n    x, y = x * np.cos(np.pi / 4), y * np.sin(np.pi / 4)\n    x, y = x / 2, y / 2\n    x, y = np.array([x, y])\n    x, y = np.meshgrid(x, y)\n    R = np.sqrt(x**2 + y**2)\n    R = np.exp(-R / size)\n    R = R * intensity\n    return R\n\n", "CodeBleu_score": 14.857914047505552}]}
{"instance_id": "2b5806238d5d", "generation_result": [{"code": "def plot(ax, text)\n\t\n    ax.set_xlim(0, 1)\n    ax.set_ylim(0, 1)\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.set_xlabel('x-axis', fontsize=14, fontweight='bold')\n    ax.set_ylabel('y-axis', fontsize=14, fontweight='bold')\n    ax.text(0.5, 0.5, text, horizontalalignment='center', verticalalignment='center', fontsize=14, fontfamily='monospace')\n    ax.set_title('Title', fontfamily='monospace', fontweight='bold')\n", "CodeBleu_score": 41.11039647284949}, {"code": "def plot(ax, text)\n\t\n    ax.set_xlim(0, 1)\n    ax.set_ylim(0, 1)\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.text(0.5, 0.5, text,\n            fontsize=12,\n            horizontalalignment='center',\n            verticalalignment='center')\n    ax.set_title(text, fontdict={'family':'monospace', 'weight': 'bold'})\n", "CodeBleu_score": 34.547480845424246}, {"code": "def plot(ax, text)\n\t\n    ax.set_xlim(0, 1)\n    ax.set_ylim(0, 1)\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    ax.text(0.5, 0.5, text, horizontalalignment='center', verticalalignment='center', fontsize=18)\n    ax.set_title('Title', fontdict={'family':'serif', 'weight': 'bold'})\n\n", "CodeBleu_score": 38.18480086514698}]}
{"instance_id": "0b42aac61e21", "generation_result": [{"code": "def predict_cumulative_hazard_function(self, X, **kwargs)\n\t\n    Xt = X\n    for name, transform in self.steps[:-1]:\n        Xt = transform.transform(Xt)\n    return self.steps[-1][-1].predict_cumulative_hazard_function(Xt, **kwargs)\n", "CodeBleu_score": 58.4852522744281}, {"code": "def predict_cumulative_hazard_function(self, X, **kwargs)\n\t\n    Xt = X\n    for name, transform in self._iter(with_final=False):\n        Xt = transform.transform(Xt)\n    return self.steps[-1][-1].predict_cumulative_hazard_function(Xt, **kwargs)\n", "CodeBleu_score": 67.73617564261339}, {"code": "def predict_cumulative_hazard_function(self, X, **kwargs)\n\t\n    Xt = X\n    for name, transform in self.steps[:-1]:\n        Xt = transform.transform(Xt, **kwargs)\n    return self.steps[-1][-1].predict_cumulative_hazard_function(Xt, **kwargs)\n", "CodeBleu_score": 51.27496974230309}]}
{"instance_id": "c8439e4918ad", "generation_result": [{"code": "def predict_survival_function(self, X, **kwargs)\n\t\n    X_ = self.transform(X, **kwargs)\n    return self._final_estimator.predict_survival_function(X_, **kwargs)\n", "CodeBleu_score": 17.257176024910155}, {"code": "def predict_survival_function(self, X, **kwargs)\n\t\n    # Apply transformations to X\n    transformed_X = self.transform(X, **kwargs)\n    # Predict survival function with transformed data and any additional keyword arguments\n    return self._final_estimator.predict_survival_function(transformed_X, **kwargs)\n", "CodeBleu_score": 20.006508641588834}, {"code": "def predict_survival_function(self, X, **kwargs)\n\t\n    if self._final_estimator.__class__.__name__ not in [\"KaplanMeierFitter\", \"NelsonAalenFitter\"]:\n        raise ValueError(\"The final estimator must be a KaplanMeierFitter or NelsonAalenFitter.\")\n    if self._final_estimator.__class__.__name__ == \"KaplanMeierFitter\":\n        self._final_estimator.predict_survival_function = self._final_estimator.predict_cumulative_hazard_function\n    else:\n        self._final_estimator.predict_survival_function = self._final_estimator.predict_survival_function\n    return self._final_estimator.predict_survival_function(self.transform(X), **kwargs)\n", "CodeBleu_score": 23.128990771024256}]}
{"instance_id": "216662ca7db4", "generation_result": [{"code": "def _check_success(self, results)\n\t\n    pass\n\n", "CodeBleu_score": 1.9422863485016646}, {"code": "def _check_success(self, results)\n\t\n    if results['status'] == SolverStatus.ok:\n        if results['solver']['name'] == 'ipopt':\n            if results['solver']['name'] == 'ipopt':\n                if results['solver']['name'] == 'ipopt':\n                    if results['solver']['name'] == 'ipopt':\n                        if results['solver']['name'] == 'ipopt':\n                            if results['solver']['name'] == 'ipopt':\n                                if results['solver']['name'] == 'ipopt':\n                                    if results['solver']['name'] == 'ipopt':\n                                        if results['solver']['name'] == 'ipopt':\n                                            if results['solver']['name'] == 'ipopt':\n                                                if results['solver']['name'] == 'ipopt':\n                                                    if results['solver']['name'] == 'ipopt':\n                                                        if results['solver']['name'] == 'ipopt':\n                                                            if results['solver']['name'] == 'ipopt':\n                                                                if results['solver']['name'] == 'ipopt':\n                                                                    if results['solver']['name'] == 'ipopt':\n                                                                        if results['solver']['name'] == 'ipopt':\n                                                                            if results['solver']['name'] == 'ipopt':\n                                                                                if results['solver']['name'] == 'ipopt':\n                                                                                    if results['solver']['name'] == 'ipopt':\n                                                                                        if results['solver']['name'] == 'ipopt':\n                                                                                            if results['solver']['name'] == 'ipopt':\n                                                                                                if results['solver']['name'] == 'ipopt':\n                                                                                                    if results['solver']['name'] == 'ipopt':\n                                                                                                        if results['solver']['name'] == 'ipopt':\n                                                                                                            if results['solver']['name'] == 'ipopt':\n                                                                                                                if results['solver']['name'] == 'ipopt':\n                                                                                                                    if results['solver']['name'] == 'ipopt':\n                                                                                                                        if results['solver']['name'] == 'ipopt':\n                                                                                                                            if results['solver']['name'] == 'ipopt':\n                                                                                                                                if results['solver']['name'] == 'ipopt':\n                                                                                                                                    if results['solver']['name'] == 'ipopt':\n                                                                                                                                        if results['solver']['name'] == 'ipopt':\n                                                                                                                                            if results['solver']['name'] == 'ipopt':\n                                                                                                                                                if results['solver']['name'] == 'ipopt':\n                                                                                                                                                    if results['solver']['name'] == 'ipopt':\n                                                                                                                                                        if results", "CodeBleu_score": 9.862450896471902}, {"code": "def _check_success(self, results)\n\t\n    pass\n", "CodeBleu_score": 1.9422863485016646}]}
{"instance_id": "1fe2e379fca0", "generation_result": [{"code": "def _decompose(self, P)\n\t\n    evals, evecs = linalg.eigh(P)\n    max_eval = np.max(evals)\n    condition_number = max_eval * np.finfo(float).eps * P.shape[0]\n    not_below_cutoff = np.abs(evals) > condition_number\n    evals = evals[not_below_cutoff]\n    evecs = evecs[:, not_below_cutoff]\n    evals /= np.max(evals)\n    return np.dot(evecs, np.diag(np.sqrt(evals))), max_eval\n", "CodeBleu_score": 25.27433741372473}, {"code": "def _decompose(self, P)\n\t\n    # Compute eigenvalues and eigenvectors of P\n    eigenvalues, eigenvectors = linalg.eigh(P)\n    # Identify largest eigenvalue\n    max_eigenvalue = np.max(eigenvalues)\n    # Compute condition number\n    condition_number = max_eigenvalue * np.finfo(float).eps * P.shape[0]\n    # Filter out eigenvalues below cutoff\n    not_below_cutoff = np.abs(eigenvalues) > condition_number\n    eigenvalues = eigenvalues[not_below_cutoff]\n    eigenvectors = eigenvectors[:, not_below_cutoff]\n    # Normalize eigenvalues\n    eigenvalues /= max_eigenvalue\n    # Return transposed decomposed matrix and largest eigenvalue\n    return eigenvectors.T, max_eigenvalue\n", "CodeBleu_score": 26.941644056699793}, {"code": "def _decompose(self, P)\n\t\n    # Compute the eigenvalues and eigenvectors of P using linalg.eigh\n    w, v = linalg.eigh(P)\n    # Identify the largest eigenvalue\n    max_eigenvalue = np.max(w)\n    # Compute a condition number based on the largest eigenvalue, the shape of P, and machine epsilon\n    condition_number = max_eigenvalue * np.finfo(float).eps * P.shape[0]\n    # Filter out eigenvalues below the negative cutoff\n    not_below_cutoff = np.abs(w) >= condition_number\n    # Normalize the eigenvalues so that the maximum eigenvalue is set to 1\n    w = w[not_below_cutoff] / max_eigenvalue\n    v = v[:, not_below_cutoff]\n    # Return the transposed decomposed matrix and the largest eigenvalue\n    return v.T, max_eigenvalue\n", "CodeBleu_score": 23.181029914039076}]}
{"instance_id": "20c3f5d05d79", "generation_result": [{"code": "def _update_coef(self, coef, D)\n\t\n    self.coef_ = coef[np.flatnonzero(np.abs(coef) > self.tol)] * D[np.flatnonzero(np.abs(coef) > self.tol)]\n    self.coef_ /= linalg.norm(self.coef_, ord=2)\n", "CodeBleu_score": 23.378124804365104}, {"code": "def _update_coef(self, coef, D)\n\t\n    # select coefficients that are greater than a small threshold\n    nonzero = np.flatnonzero(np.abs(coef) > self.tol)\n    # scale coefficients by corresponding elements in another array\n    coef[nonzero] *= D[nonzero]\n    # store the result in the model's coefficient attribute\n    self.coef_ = coef\n", "CodeBleu_score": 19.364330565964437}, {"code": "def _update_coef(self, coef, D)\n\t\n    if self.alpha_ is None:\n        self.alpha_ = np.zeros(self.coef_.shape[0])\n    if self.alpha_scale is None:\n        self.alpha_scale = np.zeros(self.coef_.shape[0])\n    if self.alpha_scale_ is None:\n        self.alpha_scale_ = np.zeros(self.coef_.shape[0])\n    if self.alpha_scale_ is None:\n        self.alpha_scale_ = np.zeros(self.coef_.shape[0])\n    if self.alpha_scale_ is None:\n        self.alpha_scale_ = np.zeros(self.coef_.shape[0])\n    if self.alpha_scale_ is None:\n        self.alpha_scale_ = np.zeros(self.coef_.shape[0])\n    if self.alpha_scale_ is None:\n        self.alpha_scale_ = np.zeros(self.coef_.shape[0])\n    if self.alpha_scale_ is None:\n        self.alpha_scale_ = np.zeros(self.coef_.shape[0])\n    if self.alpha_scale_ is None:\n        self.alpha_scale_ = np.zeros(self.coef_.shape[0])\n    if self.alpha_scale_ is None:\n        self.alpha_scale_ = np.zeros(self.coef_.shape[0])\n    if self.alpha_scale_ is None:\n        self.alpha_scale_ = np.zeros(self.coef_.shape[0])\n    if self.alpha_scale_ is None:\n        self.alpha_scale_ = np.zeros(self.coef_.shape[0])\n    if self.alpha_scale_ is None:\n        self.alpha_scale_ = np.zeros(self.coef_.shape[0])\n    if self.alpha_scale_ is None:\n        self.alpha_scale_ = np.zeros(self.coef_.shape[0])\n    if self.alpha_scale_ is None:\n        self.alpha_scale_ = np.zeros(self.coef_.shape[0])\n    if self.alpha_scale_ is None:\n        self.alpha_scale_ = np.zeros(self.coef_.shape[0])\n    if self.alpha_scale_ is None:\n        self.alpha_scale_ = np.zeros", "CodeBleu_score": 19.245089606818528}]}
{"instance_id": "37c347d00d35", "generation_result": [{"code": "def gradient(self, y_true, raw_prediction, sample_weight=None, **kwargs)\n\t\n    y_true = y_true.astype(np.float64)\n    raw_prediction = raw_prediction.ravel()\n    raw_prediction = np.clip(raw_prediction, -self.max_iter, self.max_iter)\n    # Compute the negative gradient of the Cox PH model using event type and time\n    gradient = np.zeros_like(raw_prediction, dtype=np.float64)\n    unique_times = np.unique(y_true[:, 1])\n    for i, time in enumerate(unique_times):\n        at_risk = (y_true[:, 1] >= time)\n        num_at_risk = at_risk.sum()\n        if num_at_risk == 0:\n            continue\n        # Compute the risk set\n        risk_set = raw_prediction[at_risk]\n        # Compute the risk score\n        risk_score = np.exp(risk_set - raw_prediction[i])\n        # Compute the risk score sum\n        risk_score_sum = risk_score.sum()\n        # Compute the gradient for each risk set\n        gradient[at_risk] -= risk_score / risk_score_sum\n        gradient[i] += (num_at_risk - risk_score) / risk_score_sum\n    # Apply sample weights if provided\n    if sample_weight is not None:\n        gradient *= sample_weight\n    return gradient\n\n", "CodeBleu_score": 45.68947847317557}, {"code": "def gradient(self, y_true, raw_prediction, sample_weight=None, **kwargs)\n\t\n    raw_prediction = raw_prediction.ravel()\n    y_true = y_true.ravel()\n    event_type = y_true[:, 0]\n    time = y_true[:, 1]\n    raw_prediction = raw_prediction.ravel()\n    event_type = event_type.ravel()\n    time = time.ravel()\n    if sample_weight is None:\n        sample_weight = np.ones(len(raw_prediction))\n    sample_weight = sample_weight.ravel()\n    event_mask = event_type == 1\n    time = time[event_mask]\n    raw_prediction = raw_prediction[event_mask]\n    sample_weight = sample_weight[event_mask]\n    numerator = np.exp(raw_prediction)\n    denominator = np.exp(raw_prediction).sum()\n    gradient = (numerator / denominator) - sample_weight\n    return gradient * time\n", "CodeBleu_score": 34.81152493889468}, {"code": "def gradient(self, y_true, raw_prediction, sample_weight=None, **kwargs)\n\t\n    if sample_weight is None:\n        sample_weight = np.ones_like(y_true)\n    else:\n        sample_weight = np.asarray(sample_weight)\n    event_type = y_true[:, 0]\n    time = y_true[:, 1]\n    raw_prediction = raw_prediction.ravel()\n    raw_prediction = np.asarray(raw_prediction)\n    raw_prediction[raw_prediction == 0] = 1e-05\n    raw_prediction[raw_prediction == -np.inf] = 1e-05\n    raw_prediction[raw_prediction == np.inf] = 1e-05\n    raw_prediction = np.asarray(raw_prediction)\n    raw_prediction = np.asarray(raw_prediction)\n    raw_prediction = np.asarray(raw_prediction)\n    raw_prediction = np.asarray(raw_prediction)\n    raw_prediction = np.asarray(raw_prediction)\n    raw_prediction = np.asarray(raw_prediction)\n    raw_prediction = np.asarray(raw_prediction)\n    raw_prediction = np.asarray(raw_prediction)\n    raw_prediction = np.asarray(raw_prediction)\n    raw_prediction = np.asarray(raw_prediction)\n    raw_prediction = np.asarray(raw_prediction)\n    raw_prediction = np.asarray(raw_prediction)\n    raw_prediction = np.asarray(raw_prediction)\n    raw_prediction = np.asarray(raw_prediction)\n    raw_prediction = np.asarray(raw_prediction)\n    raw_prediction = np.asarray(raw_prediction)\n    raw_prediction = np.asarray(raw_prediction)\n    raw_prediction = np.asarray(raw_prediction)\n    raw_prediction = np.asarray(raw_prediction)\n    raw_prediction = np.asarray(raw_prediction)\n    raw_prediction = np.asarray(raw_prediction)\n    raw_prediction = np.asarray(raw_prediction)\n    raw_prediction = np.asarray(raw_prediction)\n    raw_prediction = np.asarray(raw_prediction)\n    raw_prediction = np.asarray(raw_prediction)\n    raw_prediction = np.asarray(raw_prediction)\n    raw_prediction = np.asarray(raw_prediction)\n    raw_prediction = np.asarray(raw_prediction)\n    raw_", "CodeBleu_score": 29.28271221034858}]}
{"instance_id": "6be432d27e52", "generation_result": [{"code": "def _compute_missing_values_in_feature_mask(self, X, estimator_name=None)\n\t\n    if not self._support_missing_values:\n        if not np.isfinite(np.sum(X, dtype=np.float64)):\n            raise ValueError(\"X contains NaN or infinity.\")\n        return None\n    if estimator_name is None:\n        estimator_name = \"estimator\"\n    if not isinstance(X, np.ndarray):\n        raise ValueError(\n            \"X is not a numpy array. Please provide a numpy array to \"\n            f\"{estimator_name}.\"\n        )\n    if not np.isfinite(np.sum(X, dtype=np.float64)):\n        raise ValueError(\n            f\"{estimator_name} does not support missing values, but \"\n            \"the input contains missing values.\"\n        )\n    with np.errstate(invalid=\"ignore\"):\n        mask = np.isnan(X)\n    if not np.all(np.isfinite(X)):\n        raise ValueError(\n            f\"{estimator_name} does not support missing values, but \"\n            \"the input contains missing values.\"\n        )\n    if np.any(mask):\n        return mask\n    return None\n\n", "CodeBleu_score": 33.85464346364822}, {"code": "def _compute_missing_values_in_feature_mask(self, X, estimator_name=None)\n\t\n    if not self._support_missing_values(X):\n        if not np.isfinite(X).all():\n            raise ValueError(\n                \"X contains NaN or infinity. \"\n                \"This estimator does not support missing values.\"\n            )\n        return None\n    # Check if the sum of X is finite.\n    # If not, raise a ValueError for any infinite elements.\n    with np.errstate(over=\"ignore\"):\n        if not np.isfinite(np.sum(X)):\n            raise ValueError(\n                \"X contains NaN or infinity. \"\n                \"This estimator does not support missing values.\"\n            )\n    # If the sum is finite and not NaN, return None, indicating no missing\n    # values.\n    if np.isnan(np.sum(X)):\n        mask = np.isnan(X).any(axis=0)\n        if mask.any():\n            if estimator_name is None:\n                raise ValueError(\n                    \"X contains NaN. \"\n                    \"This estimator does not support missing values.\"\n                )\n            else:\n                raise ValueError(\n                    \"X contains NaN. \"\n                    f\"This estimator does not support missing values \"\n                    f\"when {estimator_name} is used.\"\n                )\n        return mask\n    return None\n", "CodeBleu_score": 34.00157263325758}, {"code": "def _compute_missing_values_in_feature_mask(self, X, estimator_name=None)\n\t\n    if not self._support_missing_values:\n        if not np.isfinite(np.sum(X, dtype=np.float64)):\n            raise ValueError(\n                \"X contains {} element that is neither finite nor \"\n                \"NaN.\".format(estimator_name)\n            )\n        return None\n    if np.isnan(np.sum(X, dtype=np.float64)):\n        return np.isnan(X).any(axis=0)\n", "CodeBleu_score": 24.941984220092817}]}
{"instance_id": "dc5bda895a07", "generation_result": [{"code": "def predict_survival_function(self, X, check_input=True, return_array=False)\n\t   if self.low_memory:\n        check_is_fitted(self, \"tree_\")\n        X = self._validate_X_predict(X, check_input, accept_sparse=\"csr\")\n        pred = self.tree_.predict(X)\n        return pred[..., 1]\n\n    sf = self.predict_survival_function_array(X, check_input=check_input)\n    if return_array:\n        return sf\n    return _array_to_step_function(self.event_times_, sf)\n\n", "CodeBleu_score": 74.62340211309495}, {"code": "def predict_survival_function(self, X, check_input=True, return_array=False)\n\t   if self.low_memory:\n        check_is_fitted(self, \"tree_\")\n        X = self._validate_X_predict(X, check_input, accept_sparse=\"csr\")\n        pred = self.tree_.predict(X)\n        if return_array:\n            return pred[..., 1]\n        else:\n            return self._array_to_step_function(self.event_times_, pred[..., 1])\n\n    chf = self.predict_cumulative_hazard_function(X, check_input, return_array=True)\n    sf = 1 - np.exp(-chf)\n    if return_array:\n        return sf\n    else:\n        return self._array_to_step_function(self.event_times_, sf)\n\n", "CodeBleu_score": 65.49543859144674}, {"code": "def predict_survival_function(self, X, check_input=True, return_array=False)\n\t   if self.low_memory:\n        check_is_fitted(self, \"tree_\")\n        X = self._validate_X_predict(X, check_input, accept_sparse=\"csr\")\n        pred = self.tree_.predict(X)\n        if return_array:\n            return pred[..., 1]\n        return _array_to_step_function(self.event_times_, pred[..., 1])\n    else:\n        chf = self.predict_cumulative_hazard_function(X, check_input, return_array=True)\n        surv = 1 - chf\n        if return_array:\n            return surv\n        return _array_to_step_function(self.event_times_, surv)\n\n", "CodeBleu_score": 67.9265689983325}]}
{"instance_id": "84ae915b5933", "generation_result": [{"code": "def _pre_fit(self, X, y)\n\t\n    X = check_array_survival(X, dtype=np.float64)\n    if X.shape[0] < 2:\n        raise ValueError(\n            \"Survival analysis requires at least 2 samples.\")\n\n    # Separate events and times\n    y = check_array_survival(y)\n    event_indicator = y[:, 0]\n    survival_times = y[:, 1]\n\n    # Center and normalize\n    if self.center:\n        feature_means = np.mean(X, axis=0)\n        X = X - feature_means\n    if self.normalize:\n        feature_scales = np.std(X, axis=0)\n        X = X / feature_scales\n\n    # Sort in descending order based on survival time\n    sorting_index = np.argsort(survival_times)[::-1]\n    X = X[sorting_index]\n    event_indicator = event_indicator[sorting_index]\n    survival_times = survival_times[sorting_index]\n\n    # Convert to Fortran-contiguous arrays\n    X = np.asfortranarray(X)\n    event_indicator = np.asfortranarray(event_indicator)\n    survival_times = np.asfortranarray(survival_times)\n    feature_means = np.asfortranarray(feature_means)\n    feature_scales = np.asfortranarray(feature_scales)\n\n    return X, event_indicator, survival_times, feature_means, feature_scales\n", "CodeBleu_score": 30.219020344574883}, {"code": "def _pre_fit(self, X, y)\n\t\n    X, y = check_array_survival(X, y)\n    n_samples = X.shape[0]\n    if n_samples < self.min_samples:\n        raise ValueError(\n            \"The number of samples {} is below the required minimal \"\n            \"number of samples {}.\".format(n_samples, self.min_samples)\n        )\n    X = X.astype(np.float64)\n    if self.center:\n        X_mean = np.average(X, axis=0)\n        X -= X_mean\n    else:\n        X_mean = np.zeros(X.shape[1])\n    if self.normalize:\n        X = f_normalize(X)\n    # Sort data\n    sort_idx = np.argsort(y[:, 0])[::-1]\n    X = X[sort_idx]\n    y = y[sort_idx]\n    # Convert to Fortran-contiguous arrays\n    X = np.asfortranarray(X)\n    y = np.asfortranarray(y)\n    return X, y[:, 1], y[:, 0], X_mean, self.scale\n", "CodeBleu_score": 38.91367233117965}, {"code": "def _pre_fit(self, X, y)\n\t\n    X = check_array_survival(X, dtype=np.float64)\n    if self.center:\n        self.feature_means_ = np.mean(X, axis=0)\n        X -= self.feature_means_\n    if self.normalize:\n        self.feature_scales_ = np.std(X, axis=0)\n        X /= self.feature_scales_\n    X = np.asfortranarray(X)\n    self.event_ = y[:, 1]\n    self.time_ = y[:, 0]\n    sort_idx = np.argsort(self.time_)[::-1]\n    self.time_ = self.time_[sort_idx]\n    self.event_ = self.event_[sort_idx]\n    X = X[sort_idx, :]\n    return X, self.event_, self.time_\n\n", "CodeBleu_score": 24.48665894159487}]}
{"instance_id": "c70bb013f85f", "generation_result": [{"code": "def _check_alphas(self)\n\t\n    if self.alphas is None:\n        # alphas is None, create new path\n        if self.n_alphas <= 0:\n            raise ValueError(\"n_alphas must be positive integer\")\n        alphas = np.logspace(np.log10(self.alpha_max), np.log10(self.alpha_min), self.n_alphas)\n    else:\n        # alphas is given, check if it is valid\n        alphas = column_or_1d(self.alphas)\n        if not np.isfinite(alphas).all():\n            raise ValueError(\"alphas must be finite\")\n        if (alphas < 0).any():\n            raise ValueError(\"alphas must be non-negative\")\n    return alphas, self.alphas is None\n", "CodeBleu_score": 50.27019960012631}, {"code": "def _check_alphas(self)\n\t\n    if self.alphas is None:\n        self.alphas = np.empty(self.n_alphas)\n        new_path = True\n    else:\n        self.alphas = _check_alphas(self.alphas)\n        new_path = False\n    return self.alphas, new_path\n", "CodeBleu_score": 21.286455026961665}, {"code": "def _check_alphas(self)\n\t\n    if self.alphas is None:\n        alphas = np.linspace(0, 1, self.n_alphas)\n        new_path = True\n    else:\n        alphas = column_or_1d(self.alphas)\n        if len(alphas) < 1:\n            raise ValueError(\"alphas must be a non-empty array\")\n        alphas = check_array(alphas, ensure_2d=False)\n        alphas = check_non_negative(alphas, \"alphas\")\n        alphas = check_finite(alphas, \"alphas\")\n        new_path = False\n    return alphas, new_path\n\n", "CodeBleu_score": 46.98094671607081}]}
{"instance_id": "7b54a0554ba4", "generation_result": [{"code": "def nelson_aalen_estimator(event, time)\n\t\n    event, time = check_consistent_length(event, time)\n    check_y_survival(event, time)\n\n    order = np.argsort(time, kind=\"mergesort\")\n    times, n_events, n_at_risk, _ = _compute_counts(event, time, order)\n    cumulative_hazard = np.cumsum(n_events / n_at_risk)\n\n    return times, cumulative_hazard\n\n", "CodeBleu_score": 40.52037791646334}, {"code": "def nelson_aalen_estimator(event, time)\n\t\n    check_consistent_length(event, time)\n    time = np.asarray(time)\n    event = np.asarray(event)\n    order = np.argsort(time, kind=\"mergesort\")\n    time = time[order]\n    event = event[order]\n    uniq_time, n_events, n_at_risk = _compute_counts(event, time, order)\n    cum_hazard = np.cumsum(n_events / n_at_risk)\n    return uniq_time, cum_hazard\n\n", "CodeBleu_score": 32.43740444681198}, {"code": "def nelson_aalen_estimator(event, time)\n\t\n    check_consistent_length(event, time)\n    check_y_survival(event, time)\n\n    order = np.argsort(time, kind=\"mergesort\")\n    times, n_events, n_at_risk, _ = _compute_counts(event, time, order)\n\n    cumulative_hazard = np.cumsum(n_events / n_at_risk)\n\n    return times, cumulative_hazard\n\n", "CodeBleu_score": 37.61673654592893}]}
{"instance_id": "35af806300cc", "generation_result": [{"code": "def ipc_weights(event, time)\n\tevent, time = check_y_survival(event, time)\n    check_consistent_length(event, time)\n\n    if np.all(event):\n        return np.ones(event.shape[0])\n\n    time_enter, time_exit = time\n    _, prob_survival = kaplan_meier_estimator(event, time_exit, time_enter, reverse=True)\n\n    uniq_times = np.unique(time_exit)\n    mask = np.searchsorted(uniq_times, time_exit)\n    prob_survival = np.take(prob_survival, mask)\n\n    assert np.all(prob_survival > 0)\n\n    weights = np.where(event, 1 / prob_survival, 1)\n    return weights\n\n", "CodeBleu_score": 38.83389507748423}, {"code": "def ipc_weights(event, time)\n\tevent, time = check_y_survival(event, None, time)\n    check_consistent_length(event, time)\n\n    if np.all(event):\n        return np.ones(time.shape[0])\n\n    _, prob_survival = kaplan_meier_estimator(event, time, reverse=True)\n\n    idx = np.searchsorted(time, time[event])\n    surv_prob = np.take(prob_survival, idx)\n\n    assert np.all(surv_prob > 0)\n\n    weights = 1.0 / surv_prob\n\n    return weights\n\n", "CodeBleu_score": 53.63362152698959}, {"code": "def ipc_weights(event, time)\n\tif np.all(event):\n        return np.ones(event.shape[0])\n\n    _, prob_survival = kaplan_meier_estimator(event, time, reverse=True)\n    uniq_times, _ = np.unique(time, return_counts=True)\n    indices = np.searchsorted(uniq_times, time[event])\n    surv_prob = np.take(prob_survival, indices)\n    assert np.all(surv_prob > 0)\n    weights = 1.0 / surv_prob\n    return weights\n\n", "CodeBleu_score": 51.759380712980004}]}
{"instance_id": "72b19985ea11", "generation_result": [{"code": "def get_dfs(\n        self,\n        dfs: Union[\n            pd.DataFrame, BaseConnector, List[Union[pd.DataFrame, BaseConnector]]\n        ],\n    )\n\t\n    if not isinstance(dfs, list):\n        dfs = [dfs]\n\n    connectors = []\n    for df in dfs:\n        if isinstance(df, pd.DataFrame):\n            df_type = type(df)\n            if df_type in self.pandas_connectors:\n                connector = self.pandas_connectors[df_type](df)\n            else:\n                raise ValueError(\n                    f\"Unsupported pandas dataframe type: {df_type}.", "CodeBleu_score": 31.68234459643679}, {"code": "def get_dfs(\n        self,\n        dfs: Union[\n            pd.DataFrame, BaseConnector, List[Union[pd.DataFrame, BaseConnector]]\n        ],\n    )\n\t\n    if not isinstance(dfs, list):\n        dfs = [dfs]\n    connectors = []\n    for df_type in dfs:\n        if isinstance(df_type, pd.DataFrame):\n            connectors.append(PandasConnector(df_type))\n        elif isinstance(df_type, dd.DataFrame):\n            connectors.append(DaskConnector(df_type))\n        elif isinstance(df_type, pl.DataFrame):\n            connectors.append(PolarsConnector(df_type))\n        elif isinstance(df_type, BaseConnector):\n            connectors.append(df_type)\n        else:\n            raise ValueError(\n                \"Invalid input type. Input must be a pandas, modin, or polars dataframe, or a custom connector.\"\n            )\n    return connectors\n", "CodeBleu_score": 41.433515034632066}, {"code": "def get_dfs(\n        self,\n        dfs: Union[\n            pd.DataFrame, BaseConnector, List[Union[pd.DataFrame, BaseConnector]]\n        ],\n    )\n\t\n    if isinstance(dfs, list):\n        connectors = []\n        for df in dfs:\n            connectors.append(self._get_df(df))\n        return connectors\n    else:\n        return [self._get_df(dfs)]\n\n    def _get_df(self, df):\n        \"\"\"This function converts a single data input into a connector object.", "CodeBleu_score": 14.110813943187562}]}
{"instance_id": "cf22fb696e90", "generation_result": [{"code": "def execute(self) -> pd.DataFrame\n\t   if self._cached is not None:\n        return self._cached\n\n    if isinstance(self._cached, pd.DataFrame):\n        return self._cached\n\n    data = self._fetch_data()\n    self._cached = data\n\n    return data\n\n", "CodeBleu_score": 43.53074951653533}, {"code": "def execute(self) -> pd.DataFrame\n\t  if self._cached:\n        return self._cached\n    if isinstance(self._cached, pd.DataFrame):\n        return self._cached\n    self._cached = self._fetch_data()\n    return self._cached\n", "CodeBleu_score": 45.8921819578144}, {"code": "def execute(self) -> pd.DataFrame\n\t  if isinstance(self._cached(include_additional_filters=True), pd.DataFrame):\n        return self._cached(include_additional_filters=True)\n    elif isinstance(self._cached(include_additional_filters=False), pd.DataFrame):\n        return self._cached(include_additional_filters=False)\n    elif isinstance(self._instance, pd.DataFrame):\n        return self._instance\n    else:\n        self._instance = self._fetch_data()\n        return self._instance\n\n", "CodeBleu_score": 59.82774797237398}]}
{"instance_id": "21df5ecea95f", "generation_result": [{"code": "def column_hash(self)\n\t\n    if not isinstance(self._instance, pd.DataFrame):\n        self._instance = self._fetch_data()\n\n    columns_str = \"|\".join(self._instance.columns)\n    formula = self.config.formula or \"\"\n    hash_str = f\"{columns_str}|{formula}\"\n    return hashlib.sha256(hash_str.encode()).hexdigest()\n", "CodeBleu_score": 69.10256009607143}, {"code": "def column_hash(self)\n\t\n    if not isinstance(self.instance, pd.DataFrame):\n        self.instance = self.execute()\n    columns_str = \"|\".join(self.instance.columns)\n    formula_str = self._build_formula()\n    columns_str += formula_str\n    hash_object = hashlib.sha256(columns_str.encode())\n    return hash_object.hexdigest()\n", "CodeBleu_score": 64.69496562955197}, {"code": "def column_hash(self)\n\t\n    if not isinstance(self._instance, pd.DataFrame):\n        self._instance = self._fetch_data()\n    columns_str = \"|\".join(self._instance.columns)\n    formula_str = self.config.formula if self.config.formula else \"\"\n    return hashlib.sha256((columns_str + formula_str).encode()).hexdigest()\n\n", "CodeBleu_score": 68.58765828051945}]}
{"instance_id": "27ee5f415000", "generation_result": [{"code": "def _load_df(self, df: Union[pd.DataFrame, pd.Series, str, list, dict])\n\t\n    if isinstance(df, pd.Series):\n        df = df.to_frame()\n    elif isinstance(df, pd.DataFrame):\n        pass\n    elif isinstance(df, str):\n        df = self.import_from_file(df)\n    elif isinstance(df, list):\n        df = pd.DataFrame(df)\n    elif isinstance(df, dict):\n        df = pd.DataFrame(df)\n    else:\n        raise ValueError(f\"Unsupported input type: {type(df)}\")\n    return df", "CodeBleu_score": 42.307348303595774}, {"code": "def _load_df(self, df: Union[pd.DataFrame, pd.Series, str, list, dict])\n\t\n    if isinstance(df, pd.Series):\n        df = df.to_frame()\n    elif isinstance(df, (list, dict)):\n        df = pd.DataFrame(df)\n    elif isinstance(df, str):\n        df = self.import_from_file(df)\n    else:\n        raise ValueError(f\"Unsupported input type: {type(df)}\")\n    return df\n", "CodeBleu_score": 36.6866100788409}, {"code": "def _load_df(self, df: Union[pd.DataFrame, pd.Series, str, list, dict])\n\t\n    if isinstance(df, pd.Series):\n        df = df.to_frame()\n    elif isinstance(df, str):\n        df = self._import_from_file(df)\n    elif isinstance(df, list) or isinstance(df, dict):\n        df = pd.DataFrame(df)\n    else:\n        raise ValueError(f\"Unsupported input type: {type(df)}\")\n    return df\n", "CodeBleu_score": 35.97116099741448}]}
{"instance_id": "38f48a358606", "generation_result": [{"code": "def sample(self, n: int = 3) -> pd.DataFrame\n\t\n\n    sampled_df = pd.DataFrame()\n    if len(self.df) <= n:\n        sampled_df = self.df.sample(frac=1)\n    else:\n        for col in self.df.columns:\n            col_sample = self._sample_column(col, n)\n            sampled_df[col] = col_sample\n\n    # anonymize the sampled dataframe head\n    sampled_df = Anonymizer.anonymize_dataframe_head(sampled_df)\n\n    return sampled_df\n", "CodeBleu_score": 76.95090533983699}, {"code": "def sample(self, n: int = 3) -> pd.DataFrame\n\t\n    sampled_df = pd.DataFrame()\n    if len(self.df) <= n:\n        sampled_df = self.df.sample(frac=1)\n    else:\n        for col in self.df.columns:\n            col_sample = self._sample_column(col, n)\n            sampled_df[col] = col_sample\n\n    # anonymize the sampled dataframe head\n    sampled_df = Anonymizer.anonymize_dataframe_head(sampled_df)\n\n    return sampled_df\n", "CodeBleu_score": 76.95090533983699}, {"code": "def sample(self, n: int = 3) -> pd.DataFrame\n\t\n    sampled_df = pd.DataFrame()\n    if len(self.df) <= n:\n        sampled_df = self.df.sample(frac=1)\n    else:\n        for col in self.df.columns:\n            col_sample = self._sample_column(col, n)\n            sampled_df[col] = col_sample\n\n    # anonymize the sampled dataframe head\n    sampled_df = Anonymizer.anonymize_dataframe_head(sampled_df)\n\n    return sampled_df\n\n", "CodeBleu_score": 76.95090533983699}]}
{"instance_id": "8bddad2e31ca", "generation_result": [{"code": "def convert_df_to_csv(self, df: pd.DataFrame, extras: dict) -> str\n\t\n    csv_str = \"\"\n    if \"name\" in extras:\n        csv_str += f\"<df name='{extras['name']}'>\"\n    if \"description\" in extras:\n        csv_str += f\"<description>{extras['description']}</description>\"\n    csv_str += f\"<dimensions rows='{df.shape[0]}' columns='{df.shape[1]}'>\"\n    csv_str += df.to_csv(index=False)\n    csv_str += \"</dimensions></df>\"\n    return csv_str\n\n", "CodeBleu_score": 35.37146184115527}, {"code": "def convert_df_to_csv(self, df: pd.DataFrame, extras: dict) -> str\n\t\n    # Construct the opening tag with the DataFrame's name and description\n    tag = f'<DataFrame name=\"{df.name}\" description=\"{extras[\"description\"]}\"'\n\n    # Add details about the DataFrame's dimensions and content in CSV format\n    tag += f' dimensions=\"{df.shape[0]}x{df.shape[1]}\"'\n    tag += f' content=\"{df.to_csv(index=False, header=False)}\"'\n\n    # Add additional details from the extras dictionary\n    for key, value in extras.items():\n        if key not in [\"name\", \"description\"]:\n            tag += f' {key}=\"{value}\"'\n\n    # Close the tag\n    tag += \"/>\"\n\n    return tag\n\n", "CodeBleu_score": 35.815751493121304}, {"code": "def convert_df_to_csv(self, df: pd.DataFrame, extras: dict) -> str\n\t\n    # Create the opening tag with the DataFrame's name and description\n    tag = f\"<DataFrame name='{df.name}' description='{extras['description']}'>\"\n\n    # Add the DataFrame's dimensions and content in CSV format\n    tag += f\"<Dimensions rows='{df.shape[0]}' columns='{df.shape[1]}'/>\"\n    tag += f\"<Content>{df.to_csv(index=False)}</Content>\"\n\n    # Add additional details from the extras dictionary\n    for key, value in extras.items():\n        tag += f\"<{key}>{value}</{key}>\"\n\n    # Close the tag\n    tag += \"</DataFrame>\"\n\n    return tag\n\n", "CodeBleu_score": 33.46263764072032}]}
{"instance_id": "5380bb60fd39", "generation_result": [{"code": "def convert_df_sql_connector_to_str\n\t\n    df_name = df.name\n    df_description = df.get_description()\n    html_str = f\"<table class='dataframe' name='{df_name}'><caption>{df_description}</caption>\"\n    html_str += df.get_head().to_csv()\n    html_str += \"</table>\"\n    return html_str", "CodeBleu_score": 13.434333213518917}, {"code": "def convert_df_sql_connector_to_str\n\t\n    df_name = df.get_name()\n    df_desc = df.get_description()\n    table_tag = f'<table id=\"{df_name}\"><caption>{df_name}: {df_desc}</caption>'\n    table_head = df.head().to_csv(index=False)\n    table_tail = '</table>'\n    html_str = table_tag + table_head + table_tail\n    return html_str\n", "CodeBleu_score": 20.90517863157844}, {"code": "def convert_df_sql_connector_to_str\n\t\n    df = df.get_head()\n    to_csv = df.to_csv(index=False)\n    html_table = f\"<table><caption>{df.name}<br>{df.description}</caption><tbody>{to_csv}</tbody></table>\"\n    return html_table\n\n", "CodeBleu_score": 11.13559863702209}]}
{"instance_id": "0afd995716c5", "generation_result": [{"code": "def convert_df_to_yml(self, df: pd.DataFrame, extras: dict) -> str:\n\t\n    # Convert the DataFrame to JSON format\n    json_data = self.convert_df_to_json(df, extras)\n\n    # Convert the JSON data to YAML format\n    yaml_data = yaml.dump(json_data)\n\n    # If the DataFrame is a direct SQL query, wrap the YAML string in HTML table tags\n    if \"is_direct_sql\" in extras and extras[\"is_direct_sql\"]:\n        yaml_data = f\"<table><tr><td><pre>{yaml_data}</pre></td></tr></table>\"\n\n    return yaml_data\n", "CodeBleu_score": 62.32819789776363}, {"code": "def convert_df_to_yml(self, df: pd.DataFrame, extras: dict) -> str:\n\t\n    # Convert the DataFrame to JSON format\n    json_df = self.convert_df_to_json(df, extras)\n\n    # Convert the JSON to YAML\n    yaml_df = yaml.dump(json_df)\n\n    if \"is_direct_sql\" in extras and extras[\"is_direct_sql\"]:\n        # Wrap the YAML string in HTML table tags\n        wrapped_yaml_df = f\"<table><tr><td><pre>{yaml_df}</pre></td></tr></table>\"\n        return wrapped_yaml_df\n\n    return yaml_df\n\n", "CodeBleu_score": 63.544927685737186}, {"code": "def convert_df_to_yml(self, df: pd.DataFrame, extras: dict) -> str:\n\t\n    # Convert DataFrame to JSON\n    json_data = self.convert_df_to_json(df, extras)\n\n    # Convert JSON to YAML\n    yaml_data = yaml.dump(json_data)\n\n    # Wrap YAML in HTML table tags if \"is_direct_sql\" is True\n    if \"is_direct_sql\" in extras and extras[\"is_direct_sql\"]:\n        yaml_data = f\"<table><tr><td>{yaml_data}</td></tr></table>\"\n\n    return yaml_data\n\n", "CodeBleu_score": 63.03051918979564}]}
{"instance_id": "e2b90ebd4e5d", "generation_result": [{"code": "def validate_value(self, expected_type: str) -> bool\n\t\n    if not expected_type:\n        return True\n    if expected_type == \"number\":\n        return isinstance(self.value, (int, float))\n    if expected_type == \"string\":\n        return isinstance(self.value, str)\n    if expected_type == \"dataframe\":\n        return isinstance(self.value, (pd.DataFrame, pd.Series))\n    if expected_type == \"plot\":\n        return isinstance(self.value, (str, dict))\n    return False\n", "CodeBleu_score": 51.7659198487355}, {"code": "def validate_value(self, expected_type: str) -> bool\n\t\n    if not expected_type:\n        return True\n\n    if expected_type == \"number\":\n        return isinstance(self.value, (int, float))\n\n    if expected_type == \"string\":\n        return isinstance(self.value, str)\n\n    if expected_type == \"dataframe\":\n        return isinstance(self.value, (pd.DataFrame, pd.Series))\n\n    if expected_type == \"plot\":\n        return isinstance(self.value, dict) or re.match(r\"^.+\\.(png|jpg|jpeg|pdf)$\", self.value)\n\n    return False\n", "CodeBleu_score": 54.00425208272753}, {"code": "def validate_value(self, expected_type: str) -> bool\n\t\n    if not expected_type:\n        return True\n\n    if expected_type == \"number\":\n        return isinstance(self.value, (int, float))\n\n    if expected_type == \"string\":\n        return isinstance(self.value, str)\n\n    if expected_type == \"dataframe\":\n        return isinstance(self.value, (pd.DataFrame, pd.Series))\n\n    if expected_type == \"plot\":\n        if isinstance(self.value, dict):\n            return all(\n                [\n                    isinstance(v, str)\n                    and re.match(r\"^.+\\.(png|jpg|jpeg|svg)$\", v)\n                    for v in self.value.values()\n                ]\n            )\n        return isinstance(self.value, str) and re.match(\n            r\"^.+\\.(png|jpg|jpeg|svg)$\", self.value\n        )\n\n    return False\n", "CodeBleu_score": 57.03638125464846}]}
{"instance_id": "d95828fdcbcb", "generation_result": [{"code": " def _distance(self, X, X2=None)\n\t\n    if X2 is None:\n        X2 = X\n    if isinstance(self.distance, str):\n        distance = pairwise_distance(self.distance, X, X2, **self.distance_params)\n    else:\n        distance = self.distance(X, X2, **self.distance_params)\n    return distance\n", "CodeBleu_score": 19.30993434482463}, {"code": " def _distance(self, X, X2=None)\n\t\n    if not X2:\n        if self.distance == \"euclidean\":\n            distance = euclidean\n        elif self.distance == \"dtw\":\n            distance = dtw\n        elif self.distance == \"ddtw\":\n            distance = ddtw\n        elif self.distance == \"wdtw\":\n            distance = wdtw\n        elif self.distance == \"msm\":\n            distance = msm\n        elif self.distance == \"lcss\":\n            distance = lcss\n        elif self.distance == \"erp\":\n            distance = erp\n        elif self.distance == \"edr\":\n            distance = edr\n        elif self.distance == \"softdtw\":\n            distance = softdtw\n        elif self.distance == \"twe\":\n            distance = twe\n        elif self.distance == \"msm\":\n            distance = msm\n        elif self.distance == \"lcss\":\n            distance = lcss\n        elif self.distance == \"erp\":\n            distance = erp\n        elif self.distance == \"edr\":\n            distance = edr\n        elif self.distance == \"softdtw\":\n            distance = softdtw\n        elif self.distance == \"twe\":\n            distance = twe\n        elif self.distance == \"msm\":\n            distance = msm\n        elif self.distance == \"lcss\":\n            distance = lcss\n        elif self.distance == \"erp\":\n            distance = erp\n        elif self.distance == \"edr\":\n            distance = edr\n        elif self.distance == \"softdtw\":\n            distance = softdtw\n        elif self.distance == \"twe\":\n            distance = twe\n        elif self.distance == \"msm\":\n            distance = msm\n        elif self.distance == \"lcss\":\n            distance = lcss\n        elif self.distance == \"erp\":\n            distance = erp\n        elif self.distance == \"edr\":\n            distance = edr\n        elif self.distance == \"softdtw\":\n            distance = softdtw\n        elif self.distance == \"twe\":\n            distance = twe\n        elif self.distance == \"msm\":\n            distance = msm\n        elif self.distance == \"lcss\":\n            distance = lcss\n        elif self.distance == \"erp\":\n            distance = erp\n        elif self.distance == \"edr\":\n            distance = edr\n        elif self.distance == \"softdtw\":\n            distance = sof", "CodeBleu_score": 15.776515098903083}, {"code": " def _distance(self, X, X2=None)\n\t\n    if X2 is None:\n        if isinstance(self.distance, str):\n            distance = pairwise_distance(\n                X, X2, metric=self.distance, **self.distance_params\n            )\n        else:\n            sig = signature(self.distance)\n            if \"X2\" in sig.parameters:\n                distance = self.distance(X, X2, **self.distance_params)\n            else:\n                distance = self.distance(X, **self.distance_params)\n    else:\n        if isinstance(self.distance, str):\n            distance = pairwise_distance(\n                X, X2, metric=self.distance, **self.distance_params\n            )\n        else:\n            distance = self.distance(X, X2, **self.distance_params)\n    return distance\n", "CodeBleu_score": 38.011073022443654}]}
{"instance_id": "497d73b298ee", "generation_result": [{"code": "def _convert_X_to_sklearn(self, X)\n\t\n    if not isinstance(X, pd.DataFrame):\n        X = pd.DataFrame(X)\n\n    if len(X.columns) == 1:\n        X = X.iloc[:, 0]\n\n    if len(X.index.names) == 1:\n        X = X.reset_index()\n\n    X_pivot = X.pivot(index='id', columns='time', values='value')\n    X_pivot = X_pivot.fillna(0)\n    X_numpy = X_pivot.to_numpy()\n\n    if len(X_numpy.shape) == 1:\n        X_numpy = X_numpy.reshape(-1, 1)\n\n    if self.unequal_length:\n        lengths = X_pivot.apply(lambda x: x.to_numpy().nonzero()[0].shape[0], axis=1).to_numpy()\n        lengths = lengths.reshape(-1, 1)\n        X_numpy = np.concatenate((lengths, X_numpy), axis=1)\n\n    return X_numpy\n\n", "CodeBleu_score": 41.98454660074868}, {"code": "def _convert_X_to_sklearn(self, X)\n\t\n    if isinstance(X, pd.DataFrame):\n        if self.unequal_length:\n            X = X.reset_index().pivot(index='index', columns='level_0', values=self.target)\n            X = X.fillna(0).to_numpy()\n            X = np.concatenate((np.expand_dims(X.shape[1] - np.sum(X == 0, axis=1), axis=1), X), axis=1)\n        else:\n            X = X.to_numpy().flatten()\n    elif isinstance(X, pd.Series):\n        if self.unequal_length:\n            X = X.reset_index().pivot(index='index', columns='level_0', values=self.target)\n            X = X.fillna(0).to_numpy()\n            X = np.concatenate((np.expand_dims(X.shape[1] - np.sum(X == 0, axis=1), axis=1), X), axis=1)\n        else:\n            X = X.to_numpy()\n    elif isinstance(X, np.ndarray):\n        if self.unequal_length:\n            X = np.concatenate((np.expand_dims(X.shape[1] - np.sum(X == 0, axis=1), axis=1), X), axis=1)\n        else:\n            X = X.flatten()\n    elif isinstance(X, list):\n        if self.unequal_length:\n            X = np.concatenate((np.expand_dims(np.array([len(x) for x in X]), axis=1), X), axis=1)\n        else:\n            X = np.array(X)\n    else:\n        raise ValueError(\"Invalid data type for X.\")\n    return X\n\n", "CodeBleu_score": 37.29821137296602}, {"code": "def _convert_X_to_sklearn(self, X)\n\t\n    if isinstance(X, pd.DataFrame):\n        X_w_ix = X.reset_index()\n        X_pivot = X_w_ix.pivot(index='index', columns='id', values='value')\n        X_pivot = X_pivot.fillna(0)\n        X_np = X_pivot.to_numpy()\n        X_lengths = X.groupby('id').apply(len).values\n        X_np = np.concatenate((X_lengths.reshape(-1, 1), X_np), axis=1)\n    elif isinstance(X, pd.Series):\n        X_np = X.to_numpy().reshape(-1, 1)\n    else:\n        X_np = X\n    return X_np\n", "CodeBleu_score": 43.67423252609902}]}
{"instance_id": "8798260a6f12", "generation_result": [{"code": "def is_sklearn_estimator(obj)\n\t\n    return isclass(obj) and issubclass(obj, SklearnBaseEstimator) and not issubclass(obj, BaseObject)\n", "CodeBleu_score": 14.940245748602448}, {"code": "def is_sklearn_estimator(obj)\n\t\n    return isclass(obj) and issubclass(obj, SklearnBaseEstimator) and not issubclass(obj, BaseObject)\n", "CodeBleu_score": 14.940245748602448}, {"code": "def is_sklearn_estimator(obj)\n\t\n    return isclass(obj) and issubclass(obj, SklearnBaseEstimator) and not issubclass(obj, BaseObject)\n", "CodeBleu_score": 14.940245748602448}]}
{"instance_id": "2ab2596d8520", "generation_result": [{"code": "def __rmul__(self, other)\n\t\n    if isinstance(other, BaseTransformer):\n        return SklearnRegressorPipeline(\n            regressor=self.regressor,\n            transformers=other * self.transformers,\n        )\n    return NotImplemented\n\n", "CodeBleu_score": 28.206854382004504}, {"code": "def __rmul__(self, other)\n\t\n    if isinstance(other, BaseTransformer):\n        transformer_pipeline = TransformerPipeline(self.transformers + [other])\n        return SklearnRegressorPipeline(transformer_pipeline, self.regressor)\n    else:\n        return NotImplemented\n", "CodeBleu_score": 32.058307476192354}, {"code": "def __rmul__(self, other)\n\t\n    if isinstance(other, BaseTransformer):\n        new_transformers = other.transformers + self.transformers\n        return SklearnRegressorPipeline(regressor=self.regressor, transformers=new_transformers)\n    return NotImplemented\n\n", "CodeBleu_score": 29.56543428326101}]}
{"instance_id": "5c1c6d14ca93", "generation_result": [{"code": "def prep_skl_df(df, copy_df=False)\n\t\n    if copy_df:\n        df = df.copy()\n    df.columns = df.columns.astype(str)\n    if not df.columns.is_unique:\n        raise ValueError(\"Duplicate columns found in DataFrame\")\n    return df\n\n", "CodeBleu_score": 49.15347229942837}, {"code": "def prep_skl_df(df, copy_df=False)\n\t\n    if copy_df:\n        df = df.copy()\n    df.columns = df.columns.astype(str)\n    if not df.columns.is_unique:\n        df.columns = [f\"{col}_{i}\" for i, col in enumerate(df.columns)]\n    return df\n", "CodeBleu_score": 50.32402710839163}, {"code": "def prep_skl_df(df, copy_df=False)\n\t\n    if copy_df:\n        df = df.copy()\n    df.columns = [str(col) for col in df.columns]\n    if df.columns.dtype!= \"object\":\n        print(\"Column names are not strings.\")\n        return df\n    else:\n        print(\"Column names are strings.\")\n        return df\n", "CodeBleu_score": 44.87315540611749}]}
{"instance_id": "d5e567c20361", "generation_result": [{"code": "def predict(self, X)\n\t\n    if not self._is_fitted:\n        raise NotFittedError(\n            f\"This instance of {self.__class__.__name__} has not \"\n            f\"been fitted yet; please call `fit` first.\"\n        )\n\n    # treat case of single class seen in fit\n    if self.n_classes_ == 1:\n        return np.repeat(self.classes_[0], X.shape[0])\n\n    if isinstance(X, np.ndarray) and len(X.shape) == 3 and X.shape[1] == 1:\n        X = np.reshape(X, (X.shape[0], -1))\n    elif isinstance(X, pd.DataFrame) and len(X.shape) == 2:\n        X = X.to_numpy()\n    elif not isinstance(X, np.ndarray) or len(X.shape) > 2:\n        raise ValueError(\n            \"RotationForest is not a time series classifier. \"\n            \"A valid sklearn input such as a 2d numpy array is required.\"\n            \"Sparse input formats are currently not supported.\"\n        )\n    X = self._validate_data(X=X, reset=False)\n\n    # replace missing values with 0 and remove useless attributes\n    X = X[:, self._useful_atts]\n\n    # normalise the data.\n    X = (X - self._min) / self._ptp\n\n    y_probas = Parallel(n_jobs=self._n_jobs)(\n        delayed(self._predict_proba_for_estimator)(\n            X,\n            self.estimators_[i],\n            self._pcas[i],\n            self._groups[i],\n        )\n        for i in range(self._n_estimators)\n    )\n\n    output = np.sum(y_probas, axis=0) / (\n        np.ones(self.n_classes_) * self._n_estimators\n    )\n    rng = check_random_state(self.random_state)\n    return self.classes_.take(\n        np.argmax(output, axis=1),\n        axis=0,\n        out=np.empty(output.shape[0], dtype=np.intp),\n    )\n\n", "CodeBleu_score": 40.8348092615503}, {"code": "def predict(self, X)\n\t\n    if not self._is_fitted:\n        raise NotFittedError(\n            f\"This instance of {self.__class__.__name__} has not \"\n            f\"been fitted yet; please call `fit` first.\"\n        )\n\n    # treat case of single class seen in fit\n    if self.n_classes_ == 1:\n        return np.repeat(self.classes_[0], X.shape[0])\n\n    if isinstance(X, np.ndarray) and len(X.shape) == 3 and X.shape[1] == 1:\n        X = np.reshape(X, (X.shape[0], -1))\n    elif isinstance(X, pd.DataFrame) and len(X.shape) == 2:\n        X = X.to_numpy()\n    elif not isinstance(X, np.ndarray) or len(X.shape) > 2:\n        raise ValueError(\n            \"RotationForest is not a time series classifier. \"\n            \"A valid sklearn input such as a 2d numpy array is required.\"\n            \"Sparse input formats are currently not supported.\"\n        )\n    X = self._validate_data(X=X, reset=False)\n\n    # replace missing values with 0 and remove useless attributes\n    X = X[:, self._useful_atts]\n\n    # normalise the data.\n    X = (X - self._min) / self._ptp\n\n    y_probas = Parallel(n_jobs=self._n_jobs)(\n        delayed(self._predict_proba_for_estimator)(\n            X,\n            self.estimators_[i],\n            self._pcas[i],\n            self._groups[i],\n        )\n        for i in range(self._n_estimators)\n    )\n\n    output = np.sum(y_probas, axis=0) / (\n        np.ones(self.n_classes_) * self._n_estimators\n    )\n    rng = check_random_state(self.random_state)\n    indices = rng.choice(\n        np.flatnonzero(output == output.max(axis=1)[:, np.newaxis]),\n        size=X.shape[0],\n    )\n    return self.classes_[indices]", "CodeBleu_score": 38.181886938594765}, {"code": "def predict(self, X)\n\t\n    if not self._is_fitted:\n        raise NotFittedError(\n            f\"This instance of {self.__class__.__name__} has not \"\n            f\"been fitted yet; please call `fit` first.\"\n        )\n\n    # treat case of single class seen in fit\n    if self.n_classes_ == 1:\n        return np.repeat(self.classes_[0], X.shape[0])\n\n    if isinstance(X, np.ndarray) and len(X.shape) == 3 and X.shape[1] == 1:\n        X = np.reshape(X, (X.shape[0], -1))\n    elif isinstance(X, pd.DataFrame) and len(X.shape) == 2:\n        X = X.to_numpy()\n    elif not isinstance(X, np.ndarray) or len(X.shape) > 2:\n        raise ValueError(\n            \"RotationForest is not a time series classifier. \"\n            \"A valid sklearn input such as a 2d numpy array is required.\"\n            \"Sparse input formats are currently not supported.\"\n        )\n    X = self._validate_data(X=X, reset=False)\n\n    # replace missing values with 0 and remove useless attributes\n    X = X[:, self._useful_atts]\n\n    # normalise the data.\n    X = (X - self._min) / self._ptp\n\n    rng = check_random_state(self.random_state)\n    y_probas = Parallel(n_jobs=self._n_jobs)(\n        delayed(self._predict_proba_for_estimator)(\n            X,\n            self.estimators_[i],\n            self._pcas[i],\n            self._groups[i],\n        )\n        for i in range(self._n_estimators)\n    )\n\n    output = np.sum(y_probas, axis=0) / (\n        np.ones(self.n_classes_) * self._n_estimators\n    )\n    y_pred = self.classes_.take(\n        np.argmax(output, axis=1), axis=0, mode=\"clip\"\n    )\n\n    # if we have a single class, we need to reshape the array\n    if self.n_classes_ == 1:\n        y_pred = np.reshape(y_pred, (y_pred.shape[0], 1))\n\n    # randomly choose", "CodeBleu_score": 37.854378665831845}]}
{"instance_id": "210b60dffd55", "generation_result": [{"code": "def _predict_proba_for_estimator(self, X, clf, pcas, groups)\n\t\n    # transform the data\n    transformed_data = np.concatenate(\n        [\n            pcas[i].transform(X[:, groups[i]])\n            for i in range(len(groups))\n        ],\n        axis=1,\n    )\n\n    # replace missing values with 0\n    transformed_data = np.nan_to_num(transformed_data)\n\n    # predict class probabilities\n    class_probabilities = clf.predict_proba(transformed_data)\n\n    # adjust probabilities if the number of predicted classes does not match the expected number of classes\n    if len(class_probabilities[0])!= self.n_classes_:\n        class_probabilities = np.hstack(\n            (\n                class_probabilities,\n                np.zeros(\n                    (\n                        class_probabilities.shape[0],\n                        self.n_classes_ - class_probabilities.shape[1],\n                    ),\n                ),\n            )\n        )\n\n    return class_probabilities", "CodeBleu_score": 26.974789946174983}, {"code": "def _predict_proba_for_estimator(self, X, clf, pcas, groups)\n\t\n    # transform the data\n    X_transformed = np.concatenate(\n        [pcas[i].transform(X[:, groups[i]]) for i in range(len(groups))], axis=1\n    )\n    # replace missing values with 0\n    X_transformed = np.nan_to_num(X_transformed, nan=0)\n\n    # predict probabilities\n    y_proba = clf.predict_proba(X_transformed)\n\n    # adjust probabilities if the number of classes is not the same as the expected number of classes\n    if y_proba.shape[1]!= self.n_classes_:\n        y_proba = np.concatenate(\n            [\n                y_proba,\n                np.zeros((y_proba.shape[0], self.n_classes_ - y_proba.shape[1])),\n            ],\n            axis=1,\n        )\n\n    return y_proba\n", "CodeBleu_score": 29.318454736567332}, {"code": "def _predict_proba_for_estimator(self, X, clf, pcas, groups)\n\t\n    X = np.concatenate(\n        [pcas[i].transform(X[:, groups[i]]) for i in range(len(groups))], axis=1\n    )\n    X = np.nan_to_num(X)\n    X = np.zeros(X.shape)\n    y_probas = clf.predict_proba(X)\n    if y_probas.shape[1]!= self.n_classes_:\n        y_probas = np.concatenate(\n            [y_probas, np.zeros((y_probas.shape[0], self.n_classes_ - y_probas.shape[1]))],\n            axis=1,\n        )\n    return y_probas\n\n\n", "CodeBleu_score": 28.745481641766723}]}
{"instance_id": "9436ee0860bb", "generation_result": [{"code": "def predict_proba(self, X)\n\t\n    if not self._is_fitted:\n        raise NotFittedError(\n            f\"This instance of {self.__class__.__name__} has not \"\n            f\"been fitted yet; please call `fit` first.\"\n        )\n\n    # treat case of single class seen in fit\n    if self.n_classes_ == 1:\n        return np.repeat([[1]], X.shape[0], axis=0)\n\n    if isinstance(X, np.ndarray) and len(X.shape) == 3 and X.shape[1] == 1:\n        X = np.reshape(X, (X.shape[0], -1))\n    elif not isinstance(X, np.ndarray) or len(X.shape) > 2:\n        raise ValueError(\n            \"ContinuousIntervalTree is not a time series classifier. \"\n            \"A valid sklearn input such as a 2d numpy array is required.\"\n            \"Sparse input formats are currently not supported.\"\n        )\n    X = self._validate_data(X=X, reset=False, force_all_finite=\"allow-nan\")\n\n    dists = np.zeros((X.shape[0], self.n_classes_))\n    for i in range(X.shape[0]):\n        dists[i] = self._root.predict_proba(X[i], self.n_classes_)\n    return dists\n\n", "CodeBleu_score": 88.06183340968327}, {"code": "def predict_proba(self, X)\n\t\n    if not self._is_fitted:\n        raise NotFittedError(\n            f\"This instance of {self.__class__.__name__} has not \"\n            f\"been fitted yet; please call `fit` first.\"\n        )\n\n    # treat case of single class seen in fit\n    if self.n_classes_ == 1:\n        return np.repeat([[1]], X.shape[0], axis=0)\n\n    if isinstance(X, np.ndarray) and len(X.shape) == 3 and X.shape[1] == 1:\n        X = np.reshape(X, (X.shape[0], -1))\n    elif not isinstance(X, np.ndarray) or len(X.shape) > 2:\n        raise ValueError(\n            \"ContinuousIntervalTree is not a time series classifier. \"\n            \"A valid sklearn input such as a 2d numpy array is required.\"\n            \"Sparse input formats are currently not supported.\"\n        )\n    X = self._validate_data(X=X, reset=False, force_all_finite=\"allow-nan\")\n\n    dists = np.zeros((X.shape[0], self.n_classes_))\n    for i in range(X.shape[0]):\n        dists[i] = self._root.predict_proba(X[i], self.n_classes_)\n    return dists\n\n", "CodeBleu_score": 88.06183340968327}, {"code": "def predict_proba(self, X)\n\t\n    if not self._is_fitted:\n        raise NotFittedError(\n            f\"This instance of {self.__class__.__name__} has not \"\n            f\"been fitted yet; please call `fit` first.\"\n        )\n\n    # treat case of single class seen in fit\n    if self.n_classes_ == 1:\n        return np.repeat([[1]], X.shape[0], axis=0)\n\n    if isinstance(X, np.ndarray) and len(X.shape) == 3 and X.shape[1] == 1:\n        X = np.reshape(X, (X.shape[0], -1))\n    elif not isinstance(X, np.ndarray) or len(X.shape) > 2:\n        raise ValueError(\n            \"ContinuousIntervalTree is not a time series classifier. \"\n            \"A valid sklearn input such as a 2d numpy array is required.\"\n            \"Sparse input formats are currently not supported.\"\n        )\n    X = self._validate_data(X=X, reset=False, force_all_finite=\"allow-nan\")\n\n    dists = np.zeros((X.shape[0], self.n_classes_))\n    for i in range(X.shape[0]):\n        dists[i] = self._root.predict_proba(X[i], self.n_classes_)\n    return dists\n\n", "CodeBleu_score": 88.06183340968327}]}
{"instance_id": "d7cb1cb66840", "generation_result": [{"code": "def _convert_X_to_sklearn(self, X)\n\t\n    if self.transformers_ is None:\n        raise TypeError(\"The transformers_ attribute is None. Please fit the transformer before calling the _convert_X_to_sklearn method.\")\n    scitype = self.transformers_.scitype\n    if scitype == \"Primitives\":\n        return convert_to(X, to_type=\"numpy2d\", dtype=None)\n    elif scitype == \"Series\":\n        return convert_to(X, to_type=\"numpy2d\", dtype=None).reshape(-1, 1)\n    else:\n        raise TypeError(f\"Unexpected scitype: {scitype}\")\n", "CodeBleu_score": 29.137236299967057}, {"code": "def _convert_X_to_sklearn(self, X)\n\t\n    if self.transformers_ is None:\n        raise NotFittedError(\n            \"This %s instance is not fitted yet. Call 'fit' with \"\n            \"appropriate arguments before using this method.\" % type(self).__name__\n        )\n    if self.transformers_[-1][-1] == \"drop\":\n        self.transformers_.pop()\n    scitype = self.transformers_[-1][-1]\n    if scitype == \"Primitives\":\n        if isinstance(X, pd.DataFrame):\n            return X.values\n        elif isinstance(X, np.ndarray):\n            return X\n        else:\n            raise TypeError(\n                \"Input data X must be of type pandas.DataFrame or numpy.ndarray.\"\n            )\n    elif scitype == \"Series\":\n        if isinstance(X, pd.DataFrame):\n            return X.values.flatten()\n        elif isinstance(X, np.ndarray):\n            return X.flatten()\n        else:\n            raise TypeError(\n                \"Input data X must be of type pandas.DataFrame or numpy.ndarray.\"\n            )\n    else:\n        raise TypeError(\n            \"Unexpected output type from transformers_. Expected 'Primitives' or 'Series', got %s.\"\n            % scitype\n        )\n\n", "CodeBleu_score": 24.808517210768105}, {"code": "def _convert_X_to_sklearn(self, X)\n\t\n    if self.transformers_ is None:\n        raise TypeError(\"The transformer has not been fitted yet.\")\n    if self.transformers_[0][1].scitype_transform == \"Primitives\":\n        if isinstance(X, pd.DataFrame):\n            return X.values\n        elif isinstance(X, pd.Series):\n            return X.values.reshape(-1, 1)\n        elif isinstance(X, np.ndarray):\n            return X\n        else:\n            raise TypeError(\n                \"X is not a pandas DataFrame, Series, or numpy array.\"\n            )\n    elif self.transformers_[0][1].scitype_transform == \"Series\":\n        if isinstance(X, pd.DataFrame):\n            return X.values.flatten()\n        elif isinstance(X, pd.Series):\n            return X.values\n        elif isinstance(X, np.ndarray):\n            return X.flatten()\n        else:\n            raise TypeError(\n                \"X is not a pandas DataFrame, Series, or numpy array.\"\n            )\n    else:\n        raise TypeError(\n            \"Unexpected output type from transformers_. The output scitype should be either Primitives or Series.\"\n        )\n", "CodeBleu_score": 17.251598659497628}]}
{"instance_id": "497cb75f2b4d", "generation_result": [{"code": "def set_random_state(estimator, random_state=0)\n\t\n    random_state = check_random_state(random_state)\n    params = {}\n    for key in estimator.get_params().keys():\n        if key.endswith('random_state'):\n            params[key] = random_state.randint(np.iinfo('i').max)\n    if params:\n        estimator.set_params(**params)\n", "CodeBleu_score": 43.24689254393331}, {"code": "def set_random_state(estimator, random_state=0)\n\t\n    random_state = check_random_state(random_state)\n    params = {}\n    for param, value in estimator.get_params().items():\n        if param.endswith('random_state'):\n            params[param] = random_state.randint(0, np.iinfo(np.int32).max)\n    if len(params) > 0:\n        estimator.set_params(**params)\n", "CodeBleu_score": 42.45196336443583}, {"code": "def set_random_state(estimator, random_state=0)\n\t\n    # Check random_state parameter\n    random_state = check_random_state(random_state)\n    # Initialize empty dictionary to hold parameters that need updating\n    params = {}\n    # Iterate over the estimator's parameters\n    for key, value in estimator.get_params().items():\n        # Check for parameters related to random state\n        if key.endswith(\"random_state\"):\n            # Assign a random integer value\n            params[key] = random_state.randint(np.iinfo(np.int32).max)\n    # Update the estimator with the new random state values\n    if len(params) > 0:\n        estimator.set_params(**params)\n\n", "CodeBleu_score": 44.76139152526789}]}
{"instance_id": "59ffd1d596a1", "generation_result": [{"code": "def _transform(X, intervals)\n\t\n    # Calculate the number of instances and intervals\n    n_instances, n_features = X.shape\n    n_intervals = len(intervals)\n\n    # Initialize an empty array to store the transformed features\n    X_trans = np.empty((n_instances, n_intervals * 3))\n\n    # Iterate over each interval\n    for i, (start, stop) in enumerate(intervals):\n        # Slice the input matrix\n        X_slice = X[:, start:stop]\n\n        # Calculate the mean, standard deviation, and slope of the slice\n        mean = np.mean(X_slice, axis=1)\n        std = np.std(X_slice, axis=1)\n        slope = _slope(X_slice, axis=1)\n\n        # Store the mean, standard deviation, and slope in the transformed array\n        X_trans[:, 3 * i] = mean\n        X_trans[:, 3 * i + 1] = std\n        X_trans[:, 3 * i + 2] = slope\n\n    return X_trans.T\n\n", "CodeBleu_score": 39.997967882835205}, {"code": "def _transform(X, intervals)\n\t\n    # Calculate number of instances and intervals\n    n_instances, n_features = X.shape\n    n_intervals = len(intervals)\n\n    # Initialize array to store transformed features\n    X_trans = np.empty((n_instances, n_intervals * 3))\n\n    # For each interval, slice the input matrix, compute mean, std, and slope,\n    # and store these values in the transformed array\n    for i, (start, end) in enumerate(intervals):\n        X_trans[:, i * 3] = np.mean(X[:, start:end], axis=1)\n        X_trans[:, i * 3 + 1] = np.std(X[:, start:end], axis=1)\n        X_trans[:, i * 3 + 2] = _slope(X[:, start:end], axis=1)\n\n    return X_trans.T\n\n", "CodeBleu_score": 22.65949192973998}, {"code": "def _transform(X, intervals)\n\t\n    n_instances, n_features = X.shape\n    n_intervals = len(intervals)\n    X_trans = np.empty((n_instances, n_features * n_intervals * 3))\n\n    for i, (start, end) in enumerate(intervals):\n        X_trans[:, i * n_features : (i + 1) * n_features] = X[:, start:end].mean(\n            axis=1\n        )\n        X_trans[:, (i + n_intervals) * n_features : (i + n_intervals + 1) * n_features] = (\n            X[:, start:end].std(axis=1)\n        )\n        X_trans[:, (i + 2 * n_intervals) * n_features : (i + 2 * n_intervals + 1) * n_features] = (\n            _slope(X[:, start:end], axis=1)\n        )\n\n    return X_trans.T\n\n", "CodeBleu_score": 25.097206673864914}]}
{"instance_id": "5a01594eebfb", "generation_result": [{"code": "def plot_and_annotate_facets(\n            self, plot: str, plot_params: dict, configuration: dict,\n            annotation_func: str, *args, annotation_params: dict = None,\n            ax_op_before: List[Union[str, Optional[list],\n                                     Optional[dict]]] = None,\n            ax_op_after: List[Union[str, Optional[list],\n                                    Optional[dict]]] = None,\n            annotate_params: dict = None, **kwargs)\n\t   if annotation_params is None:\n        annotation_params = {}\n    if annotate_params is None:\n        annotate_params = {}\n    if ax_op_before is None:\n        ax_op_before = []\n    if ax_op_after is None:\n        ax_op_after = []\n\n    self.configure(**configuration)\n\n    plot_params = self._get_plot_params(plot_params, plot, **kwargs)\n\n    self.new_plot(plot=plot, **plot_params)\n\n    self.configure(**annotation_params)\n\n    getattr(self, annotation_func)(**annotate_params)\n\n    _apply_ax_operations(self.ax, ax_op_before)\n\n    self._apply_ax_operations(ax_op_after)\n\n    return self._get_output()\n\n", "CodeBleu_score": 52.56891590350315}, {"code": "def plot_and_annotate_facets(\n            self, plot: str, plot_params: dict, configuration: dict,\n            annotation_func: str, *args, annotation_params: dict = None,\n            ax_op_before: List[Union[str, Optional[list],\n                                     Optional[dict]]] = None,\n            ax_op_after: List[Union[str, Optional[list],\n                                    Optional[dict]]] = None,\n            annotate_params: dict = None, **kwargs)\n\t   if annotation_params is None:\n        annotation_params = {}\n    if annotate_params is None:\n        annotate_params = {}\n    if ax_op_before is None:\n        ax_op_before = []\n    if ax_op_after is None:\n        ax_op_after = []\n\n    if not isinstance(ax_op_before, list):\n        ax_op_before = [ax_op_before]\n    if not isinstance(ax_op_after, list):\n        ax_op_after = [ax_op_after]\n\n    if not isinstance(plot_params, dict):\n        plot_params = {}\n\n    if not isinstance(configuration, dict):\n        configuration = {}\n\n    if not isinstance(annotate_params, dict):\n        annotate_params = {}\n\n    if not isinstance(plot_params, dict):\n        plot_params = {}\n\n    if not isinstance(configuration, dict):\n        configuration = {}\n\n    if not isinstance(annotate_params, dict):\n        annotate_params = {}\n\n    if not isinstance(plot_params, dict):\n        plot_params = {}\n\n    if not isinstance(configuration, dict):\n        configuration = {}\n\n    if not isinstance(annotate_params, dict):\n        annotate_params = {}\n\n    if not isinstance(plot_params, dict):\n        plot_params = {}\n\n    if not isinstance(configuration, dict):\n        configuration = {}\n\n    if not isinstance(annotate_params, dict):\n        annotate_params = {}\n\n    if not isinstance(plot_params, dict):\n        plot_params = {}\n\n    if not isinstance(configuration, dict):\n        configuration = {}\n\n    if not isinstance(annotate_params, dict):\n        annotate_params = {}\n\n    if not isinstance(plot_params, dict):\n        plot_params = {}\n\n    if not isinstance(configuration, dict):\n        configuration = {}\n\n    if not isinstance(annotate_params, dict):\n        annotate_params = {}\n\n    if not isinstance(plot_params, dict):\n        plot_params = {}\n\n    if not isinstance(configuration, dict):\n        configuration = {}\n\n    if not isinstance(annotate_params, dict):\n        annotate_params = {}\n\n    if not isinstance(plot_params, dict):\n        plot_params = {}\n\n    if not isinstance(configuration, dict):\n        configuration = {}\n\n    if not isinstance(annotate_params, dict):\n        annotate_params = {}\n\n    if not isinstance(plot_params, dict):\n        plot_params = {}\n\n   ", "CodeBleu_score": 32.41269843507082}, {"code": "def plot_and_annotate_facets(\n            self, plot: str, plot_params: dict, configuration: dict,\n            annotation_func: str, *args, annotation_params: dict = None,\n            ax_op_before: List[Union[str, Optional[list],\n                                     Optional[dict]]] = None,\n            ax_op_after: List[Union[str, Optional[list],\n                                    Optional[dict]]] = None,\n            annotate_params: dict = None, **kwargs)\n\t    if annotation_params is None:\n        annotation_params = {}\n    if annotate_params is None:\n        annotate_params = {}\n\n    if ax_op_before is None:\n        ax_op_before = []\n    if ax_op_after is None:\n        ax_op_after = []\n\n    self.configure(**configuration)\n\n    if self._verbose:\n        self.print_configuration()\n\n    self.new_plot(plot=plot, **plot_params)\n\n    _apply_ax_operations(self.ax, ax_op_before)\n\n    getattr(self, annotation_func)(**annotation_params)\n\n    self.configure(**annotate_params)\n\n    _apply_ax_operations(self.ax, ax_op_after)\n\n    return self._get_output()", "CodeBleu_score": 50.73850902765786}]}
{"instance_id": "b5b17d312c74", "generation_result": [{"code": "def predict_proba(self, X)\n\t\n    check_is_fitted(self)\n    X = check_array(X)\n    if X.shape[1]!= self._feature_count:\n        raise ValueError(\"Input data has wrong number of features.\")\n    scores = self._execute(X)\n    probabilities = self._transformer.transform(scores)\n    return np.vstack((1 - probabilities, probabilities)).T\n", "CodeBleu_score": 25.144373356013748}, {"code": "def predict_proba(self, X)\n\t\n    if not hasattr(self, '_program'):\n        raise NotFittedError(\"The model has not been fitted.\")\n    X = check_array(X)\n    if X.shape[1]!= self.n_features_:\n        raise ValueError(f\"The input data has {X.shape[1]} features, but the model expects {self.n_features_} features.\")\n    scores = self.execute(X)\n    probabilities = np.vstack((1 - scores, scores)).T\n    return probabilities", "CodeBleu_score": 40.65290356921681}, {"code": "def predict_proba(self, X)\n\t\n    check_is_fitted(self)\n    X = check_array(X)\n    if X.shape[1]!= self._n_features:\n        raise ValueError(\"X has %d features per sample; expecting %d\"\n                         % (X.shape[1], self._n_features))\n    scores = self._program.execute(X)\n    probs = np.exp(scores) / (1 + np.exp(scores))\n    return np.hstack((1 - probs, probs))\n", "CodeBleu_score": 20.162398967402968}]}
{"instance_id": "83f71c959df7", "generation_result": [{"code": "def point_mutation(self, random_state)\n\t\n    mutated_program = copy.deepcopy(self)\n    mutation_points = np.where(random_state.uniform(size=len(mutated_program)) < self.mutation_probability)\n    for point in mutation_points[0]:\n        node = mutated_program[point]\n        if isinstance(node, FunctionNode):\n            arity = node.arity\n            valid_functions = [function for function in self.function_set if function.arity == arity]\n            mutated_program[point] = random_state.choice(valid_functions)\n        else:\n            if random_state.uniform() < 0.5:\n                mutated_program[point] = random_state.uniform(self.constant_range[0], self.constant_range[1])\n            else:\n                mutated_program[point] = random_state.randint(self.n_features)\n    return mutated_program, list(mutation_points[0])\n", "CodeBleu_score": 26.238584991653642}, {"code": "def point_mutation(self, random_state)\n\t\n    # Create a copy of the program to be mutated\n    mutated_program = copy.deepcopy(self)\n    # Identify nodes to modify based on a uniform random distribution and a probability threshold\n    nodes_to_mutate = [\n        node for node in mutated_program.expand_tree(return_type=tuple)\n        if random_state.uniform() < self.mutation_probability\n    ]\n    # For each node to be mutated, if it is a function, it finds a valid replacement function with the same arity and updates the node.\n    # If the node is a terminal, it replaces it with either a constant or a variable, ensuring valid replacement according to the defined constant range and number of features.\n    for node in nodes_to_mutate:\n        if isinstance(node.data, gplearn.functions.Function):\n            # Find a valid replacement function with the same arity\n            arity = len(node.children)\n            valid_functions = [\n                function for function in self.function_set\n                if len(function.arity) == arity\n            ]\n            # Update the node with the replacement function\n            replacement_function = random_state.choice(valid_functions)\n            node.data = replacement_function\n        elif isinstance(node.data, gplearn.functions.Terminal):\n            # Replace the terminal with either a constant or a variable, ensuring valid replacement according to the defined constant range and number of features.\n            if random_state.uniform() < 0.5:\n                # Replace with a constant\n                constant = random_state.uniform(self.const_range[0],\n                                               self.const_range[1])\n                node.data = gplearn.functions.make_terminal(constant)\n            else:\n                # Replace with a variable\n                variable_index = random_state.randint(0, self.n_features)\n                node.data = gplearn.functions.make_terminal(\n                    gplearn.functions.make_terminal(variable_index,\n                                                   gplearn.functions.Variable))\n    # Return the mutated program and the list of modified node indices.\n    return mutated_program, [node.idx for node in nodes_def point_mutation(self, random_state):\n    \"\"\"This function performs a point mutation on a program using a given random state.", "CodeBleu_score": 25.487710115337}, {"code": "def point_mutation(self, random_state)\n\t\n    program = copy.deepcopy(self)\n    nodes_to_mutate = []\n    for i, node in enumerate(program.nodes):\n        if random_state.uniform() < self.pset.mutation_prob:\n            nodes_to_mutate.append(i)\n    for node_idx in nodes_to_mutate:\n        node = program.nodes[node_idx]\n        if isinstance(node, gp.Primitive):\n            arity = len(node.args)\n            new_func = random_state.choice(self.pset.get_arity_n_function(arity))\n            program.nodes[node_idx] = new_func\n        elif isinstance(node, gp.Terminal):\n            if random_state.uniform() < self.pset.const_prob:\n                program.nodes[node_idx] = gp.Const(random_state.uniform(self.pset.min_const, self.pset.max_const))\n            else:\n                program.nodes[node_idx] = gp.Var(random_state.randint(0, self.pset.num_features))\n    return program, nodes_to_mutate\n\n", "CodeBleu_score": 24.016761212967346}]}
{"instance_id": "f1853251e671", "generation_result": [{"code": "def bias(close, length=None, mamode=None, offset=None, **kwargs)\n\t\n    # Validate inputs\n    if not isinstance(close, pd.Series):\n        raise TypeError(\"Argument 'close' must be a pandas Series\")\n    if length is None:\n        length = 10\n    if mamode is None:\n        mamode = \"ema\"\n    if offset is None:\n        offset = 0\n    # Calculate moving average\n    if mamode == \"sma\":\n        ma = close.rolling(length).mean()\n    elif mamode == \"ema\":\n        ma = close.ewm(span=length, adjust=False).mean()\n    elif mamode == \"wma\":\n        weights = np.arange(1, length + 1)\n        ma = close.rolling(length).apply(lambda x: np.dot(x, weights) / weights.sum(), raw=True)\n    else:\n        raise ValueError(\"Invalid value for'mamode'\")\n    # Calculate BIAS\n    bias = (close - ma) / ma * 100\n    # Shift and fill\n    if offset!= 0:\n        bias = bias.shift(offset)\n    if kwargs.get(\"fillna\", False):\n        bias = bias.fillna(kwargs.get(\"fillna\"))\n    if kwargs.get(\"fill_method\", None) is not None:\n        bias = bias.fillna(method=kwargs.get(\"fill_method\"))\n    # Name and categorize the series\n    bias.name = f\"BIAS_{length}_{mamode}\"\n    bias.category = \"momentum\"\n    return bias\n", "CodeBleu_score": 40.73509088664245}, {"code": "def bias(close, length=None, mamode=None, offset=None, **kwargs)\n\t\n    # Validate inputs\n    length = int(length) if length and length > 0 else 10\n    mamode = mamode.lower() if isinstance(mamode, str) else \"sma\"\n    offset = int(offset) if offset and offset!= 0 else None\n    # Calculate moving average\n    if mamode == \"ema\":\n        ma = ta.EMA(close, timeperiod=length)\n    elif mamode == \"wma\":\n        ma = ta.WMA(close, timeperiod=length)\n    elif mamode == \"dema\":\n        ma = ta.DEMA(close, timeperiod=length)\n    elif mamode == \"tema\":\n        ma = ta.TEMA(close, timeperiod=length)\n    elif mamode == \"trima\":\n        ma = ta.TRIMA(close, timeperiod=length)\n    elif mamode == \"kama\":\n        ma = ta.KAMA(close, timeperiod=length)\n    elif mamode == \"mama\":\n        ma = ta.MAMA(close, fastlimit=0, slowlimit=0)[0]\n    elif mamode == \"t3\":\n        ma = ta.T3(close, timeperiod=length, vfactor=0)\n    else:\n        ma = ta.SMA(close, timeperiod=length)\n    # Calculate bias\n    bias = (close - ma) / ma\n    # Name and categorize the series\n    bias.name = f\"BIAS_{length}_{mamode.upper()}\"\n    bias.category = \"momentum\"\n    # Offset\n    if offset is not None:\n        bias = bias.shift(offset)\n    # Handle fills\n    if \"fillna\" in kwargs:\n        bias.fillna(kwargs[\"fillna\"], inplace=True)\n    if \"fill_method\" in kwargs:\n        bias.fillna(method=kwargs[\"fill_method\"], inplace=True)\n    # Name and categorize the series after fills\n    bias.name = f\"BIAS_{length}_{mamode.upper()}\"\n    bias.category = \"momentum\"\n    return bias\n", "CodeBleu_score": 52.02269361309171}, {"code": "def bias(close, length=None, mamode=None, offset=None, **kwargs)\n\t\n    # Validate inputs\n    length = int(length) if length and length > 0 else 10\n    mamode = mamode.lower() if isinstance(mamode, str) else \"ema\"\n    offset = int(offset) if offset and offset!= 0 else None\n    # Calculate moving average\n    if mamode == \"ema\":\n        ma = ema(close, length, **kwargs)\n    elif mamode == \"sma\":\n        ma = sma(close, length, **kwargs)\n    elif mamode == \"wma\":\n        ma = wma(close, length, **kwargs)\n    else:\n        raise ValueError(\"Invalid moving average mode.\")\n    # Calculate BIAS\n    bias = (close - ma) / ma\n    # Shift and fill\n    if offset is not None:\n        bias = bias.shift(offset)\n    if kwargs.get(\"fillna\", False):\n        bias = bias.fillna(kwargs.get(\"fill_method\", \"backfill\"))\n    # Name and categorize the series\n    bias.name = f\"BIAS_{length}_{mamode}\"\n    bias.category = \"momentum\"\n    return bias\n\n", "CodeBleu_score": 46.262776151140926}]}
{"instance_id": "77825be37c24", "generation_result": [{"code": "def coins_bar(broker='hb', code='btc', ktype='D', size='2000')\n\t\n    try:\n        if broker == 'hb':\n            if ktype == 'D':\n                ktype = 'day'\n            elif ktype == 'W':\n                ktype = 'week'\n            elif ktype == 'M':\n                ktype ='month'\n            elif ktype == '5':\n                ktype = '5min'\n            elif ktype == '15':\n                ktype = '15min'\n            elif ktype == '30':\n                ktype = '30min'\n            elif ktype == '60':\n                ktype = '60min'\n            elif ktype == '180':\n                ktype = '180min'\n            elif ktype == '1':\n                ktype = '1min'\n            elif ktype == '3':\n                ktype = '3min'\n            elif ktype == '15':\n                ktype = '15min'\n            elif ktype == '30':\n                ktype = '30min'\n            elif ktype == '60':\n                ktype = '60min'\n            elif ktype == '180':\n                ktype = '180min'\n            elif ktype == '1D':\n                ktype = 'day'\n            elif ktype == '1W':\n                ktype = 'week'\n            elif ktype == '1M':\n                ktype ='month'\n            elif ktype == '5M':\n                ktype = '5min'\n            elif ktype == '15M':\n                ktype = '15min'\n            elif ktype == '30M':\n                ktype = '30min'\n            elif ktype == '60M':\n                ktype = '60min'\n            elif ktype == '180M':\n                ktype = '180min'\n            elif ktype == '1H':\n                ktype = '1min'\n            elif ktype == '3H':\n                ktype = '3min'\n            elif ktype == '15H':\n                ktype = '15min'\n            elif ktype == '30H':\n                ktype = '30min'\n            elif ktype == '60H':\n                ktype = '60min'\n            elif ktype == '180H':\n                ktype = '180min'\n            else:\n                ktype", "CodeBleu_score": 13.920879866193614}, {"code": "def coins_bar(broker='hb', code='btc', ktype='D', size='2000')\n\t\n    try:\n        if broker == 'ok':\n            url = 'https://www.okex.com/api/spot/v3/instruments/{}/candles?granularity={}&limit={}'.format(code, ktype, size)\n        elif broker == 'zb':\n            url = 'http://api.zb.cn/data/v1/kline?market={}&type={}&size={}'.format(code, ktype, size)\n        elif broker == 'hb':\n            url = 'https://api.huobi.pro/market/history/kline?period={}&size={}&symbol={}'.format(ktype, size, code)\n        elif broker == 'kr':\n            url = 'https://api.kucoin.com/v1/market/candles?symbol={}&type={}&limit={}'.format(code, ktype, size)\n        elif broker == 'bi':\n            url = 'https://api.binance.com/api/v3/klines?symbol={}&interval={}&limit={}'.format(code, ktype, size)\n        else:\n            print('Error: Broker not supported.')\n            return None\n        print(url)\n        r = requests.get(url)\n        js = r.json()\n        if broker == 'ok':\n            js = js['data']\n        elif broker == 'zb':\n            js = js['data']['data']\n        elif broker == 'kr':\n            js = js['data']\n        elif broker == 'bi':\n            js = js\n        else:\n            print('Error: Broker not supported.')\n            return None\n        df = pd.DataFrame(js)\n        if broker == 'ok':\n            df.columns = ['date', 'open', 'high', 'low', 'close', 'volume']\n            df['date'] = pd.to_datetime(df['date'], unit='ms')\n        elif broker == 'zb':\n            df.columns = ['date', 'open', 'high', 'low', 'close', 'volume']\n            df['date'] = pd.to_datetime(df['date'], unit='ms')\n        elif broker == 'kr':\n            df.columns = ['date', 'open', 'high', 'low', 'close', 'volume']\n            df['date'] = pd.to_datetime(df['date'], unit='ms')\n        elif broker == 'bi':\n            df.columns", "CodeBleu_score": 32.349895960607476}, {"code": "def coins_bar(broker='hb', code='btc', ktype='D', size='2000')\n\t\n    try:\n        if broker == 'hb':\n            if ktype == '1day':\n                ktype = '1min'\n            url = 'https://api.huobi.pro/market/history/kline?period=' + ktype + '&size=' + size + '&symbol=' + code + 'usdt'\n        elif broker == 'zb':\n            if ktype == '1day':\n                ktype = '1min'\n            url = 'http://api.zb.com/data/v1/kline?market=' + code + '_usdt&type=' + ktype + '&size=' + size\n        elif broker == 'okex':\n            if ktype == '1day':\n                ktype = '1min'\n            url = 'https://www.okex.com/api/spot/v3/instruments/' + code + '-usdt/candles?granularity=' + ktype + '&limit=' + size\n        elif broker == 'binance':\n            if ktype == '1day':\n                ktype = '1min'\n            url = 'https://api.binance.com/api/v3/klines?symbol=' + code.upper() + 'USDT&interval=' + ktype + '&limit=' + size\n        elif broker == 'bitfinex':\n            if ktype == '1day':\n                ktype = '1min'\n            url = 'https://api-pub.bitfinex.com/v2/candles/trade:' + ktype + ':t' + code.upper() + 'USD/hist?limit=' + size\n        elif broker == 'bitmex':\n            if ktype == '1day':\n                ktype = '1min'\n            url = 'https://www.bitmex.com/api/v1/trade/bucketed?binSize=' + ktype + '&partial=false&symbol=' + code.upper() + '&count=' + size + '&reverse=true'\n        elif broker == 'kraken':\n            if ktype == '1day':\n                ktype = '1min'\n            url = 'https://api.kraken.com/0/public/OHLC?pair=' + code.upper() + 'USD&interval=' + ktype + '&since=0&count=' + size\n        elif broker == 'bittrex':\n            if ktype == '1", "CodeBleu_score": 21.011135156167676}]}
{"instance_id": "c6552851da8a", "generation_result": [{"code": "def coins_snapshot(broker='hb', code='btc', size='5')\n\t\n    try:\n        js = _get_data(broker, code, size)\n        ask = pd.DataFrame(js['asks'], columns=['price', 'amount'])\n        bid = pd.DataFrame(js['bids'], columns=['price', 'amount'])\n        if broker == 'ok':\n            ask['timestamp'] = int2time(js['date'])\n            bid['timestamp'] = int2time(js['date'])\n        else:\n            ask['timestamp'] = int2time(js['timestamp'])\n            bid['timestamp'] = int2time(js['timestamp'])\n        return pd.Panel({'asks': ask, 'bids': bid})\n    except Exception:\n        print(traceback.print_exc())\n\n", "CodeBleu_score": 35.5875294925852}, {"code": "def coins_snapshot(broker='hb', code='btc', size='5')\n\t\n    try:\n        data = _get_data(broker, code, size)\n        asks = pd.DataFrame(data['asks'])\n        bids = pd.DataFrame(data['bids'])\n        asks['timestamp'] = asks[2].apply(int2time)\n        bids['timestamp'] = bids[2].apply(int2time)\n        return pd.Panel({'asks': asks, 'bids': bids})\n    except Exception:\n        print(traceback.print_exc())\n\n", "CodeBleu_score": 24.048852856122515}, {"code": "def coins_snapshot(broker='hb', code='btc', size='5')\n\t\n    try:\n        js = _get_data(broker, code, size)\n        if broker == 'ok':\n            ask = pd.DataFrame(js['asks'], columns=['price', 'amount', 'timestamp'])\n            ask['timestamp'] = ask['timestamp'].apply(lambda x: int2time(x / 1000))\n            bid = pd.DataFrame(js['bids'], columns=['price', 'amount', 'timestamp'])\n            bid['timestamp'] = bid['timestamp'].apply(lambda x: int2time(x / 1000))\n        elif broker == 'hb':\n            ask = pd.DataFrame(js['asks'], columns=['price', 'amount'])\n            ask['timestamp'] = js['ts']\n            bid = pd.DataFrame(js['bids'], columns=['price', 'amount'])\n            bid['timestamp'] = js['ts']\n        elif broker == 'zb':\n            ask = pd.DataFrame(js['ask'], columns=['price', 'amount'])\n            ask['timestamp'] = js['date']\n            bid = pd.DataFrame(js['bid'], columns=['price', 'amount'])\n            bid['timestamp'] = js['date']\n        data = {'ask': ask, 'bid': bid}\n        return pd.Panel(data)\n    except Exception:\n        print(traceback.print_exc())\n\n", "CodeBleu_score": 44.52901202248557}]}
{"instance_id": "8b5fee723398", "generation_result": [{"code": "def coins_trade(broker='hb', code='btc')\n\t\n    url = 'http://api.huobi.com/staticmarket/ticker_btc_json.js'\n    if broker == 'ok':\n        url = 'https://www.okcoin.com/api/v1/trades.do?symbol=' + code\n    if broker == 'chbtc':\n        url = 'http://api.chbtc.com/data/trades?coin=' + code\n    try:\n        data = _get_data(url)\n        if data is not None:\n            if broker == 'hb':\n                data = pd.DataFrame(data['ticker']['trade'])\n                data = data[['tid', 'date', 'price', 'amount']]\n                data.columns = ['tid', 'datetime', 'price', 'amount']\n                data['datetime'] = data['datetime'].apply(int2time)\n            if broker == 'ok':\n                data = pd.DataFrame(data)\n                data = data[['tid', 'date', 'price', 'amount']]\n                data.columns = ['tid', 'datetime', 'price', 'amount']\n                data['datetime'] = data['datetime'].apply(int2time)\n            if broker == 'chbtc':\n                data = pd.DataFrame(data)\n                data = data[['tid', 'date', 'price', 'amount']]\n                data.columns = ['tid', 'datetime', 'price', 'amount']\n                data['datetime'] = data['datetime'].apply(int2time)\n            data = data[['tid', 'datetime', 'price', 'amount', 'type']]\n            return data\n    except Exception:\n        print(traceback.print_exc())\n        return None\n", "CodeBleu_score": 44.43586835090845}, {"code": "def coins_trade(broker='hb', code='btc')\n\t\n    url = f'http://api.huobi.pro/market/history/trade?symbol={code}usdt&size=1000'\n    data = _get_data(url)\n    if data is not None:\n        if broker == 'hb':\n            df = pd.DataFrame(data['data'])\n            df = df[['id', 'ts', 'price', 'amount']]\n            df['datetime'] = df['ts'].apply(lambda x: int2time(x / 1000))\n            df['type'] = df['amount'].apply(lambda x: 'buy' if x > 0 else'sell')\n            df = df.rename(columns={'id': 'tid', 'amount': 'volume'})\n        elif broker == 'ok':\n            df = pd.DataFrame(data['data'])\n            df = df[['tid', 'date_ms', 'price', 'amount']]\n            df['datetime'] = df['date_ms'].apply(lambda x: int2time(x / 1000))\n            df['type'] = df['amount'].apply(lambda x: 'buy' if x > 0 else'sell')\n            df = df.rename(columns={'amount': 'volume'})\n        elif broker == 'chbtc':\n            df = pd.DataFrame(data['trades'])\n            df = df[['tid', 'date', 'price', 'amount']]\n            df['datetime'] = df['date'].apply(lambda x: int2time(x))\n            df['type'] = df['amount'].apply(lambda x: 'buy' if x > 0 else'sell')\n            df = df.rename(columns={'amount': 'volume'})\n        else:\n            return None\n        df = df[['tid', 'datetime', 'price', 'volume', 'type']]\n        return df\n    else:\n        return None\n\n", "CodeBleu_score": 54.612982955945945}, {"code": "def coins_trade(broker='hb', code='btc')\n\t\n    url = 'http://api.btcgateway.pro/data/v1/kline?symbol=%s_%s&type=1min&size=1000' % (broker, code)\n    data = _get_data(url)\n    if data is not None:\n        if broker == 'hb':\n            df = pd.DataFrame(data['data'])\n            df = df[[0, 1, 2, 3, 4]]\n            df.columns = ['tid', 'datetime', 'price', 'amount', 'type']\n            df['datetime'] = df['datetime'].apply(int2time)\n        elif broker == 'ok':\n            df = pd.DataFrame(data['data']['data'])\n            df = df[[0, 1, 2, 3, 4]]\n            df.columns = ['tid', 'datetime', 'price', 'amount', 'type']\n            df['datetime'] = df['datetime'].apply(int2time)\n        elif broker == 'chbtc':\n            df = pd.DataFrame(data['result'])\n            df = df[[0, 1, 2, 3, 4]]\n            df.columns = ['tid', 'datetime', 'price', 'amount', 'type']\n            df['datetime'] = df['datetime'].apply(int2time)\n        else:\n            df = pd.DataFrame()\n        df.columns = ['tid', 'datetime', 'price', 'amount', 'type']\n    return df\n\n", "CodeBleu_score": 46.75253235096438}]}
{"instance_id": "25e0a134c45b", "generation_result": [{"code": "def _parse_fund_data(url, fund_type='open')\n\t\n    _write_console('\u6b63\u5728\u83b7\u53d6%s\u57fa\u91d1\u6570\u636e\uff0c\u8bf7\u8010\u5fc3\u7b49\u5f85...' % url)\n    try:\n        response = urlopen(url)\n    except Exception as e:\n        print(e)\n        return None\n    text = response.read()\n    text = text.decode('utf-8')\n    text = text.replace('--', '')\n    text = text.replace('null', '\"\"')\n    text = text.replace('\\'', '\\\"')\n    text = text.replace('(', '')\n    text = text.replace(')', '')\n    text = text.replace(';', '')\n    text = text.replace('var', '')\n    text = text.replace('data_sg = ', '')\n    text = text.replace('data_g = ', '')\n    text = text.replace('data_z = ', '')\n    text = text.replace('data_all = ', '')\n    text = text.replace('data_zzc = ', '')\n    text = text.replace('data_sg =', '')\n    text = text.replace('data_g =', '')\n    text = text.replace('data_z =', '')\n    text = text.replace('data_all =', '')\n    text = text.replace('data_zzc =', '')\n    text = text.replace('data_sg=', '')\n    text = text.replace('data_g=', '')\n    text = text.replace('data_z=', '')\n    text = text.replace('data_all=', '')\n    text = text.replace('data_zzc=', '')\n    text = text.replace('data_sg =', '')\n    text = text.replace('data_g =', '')\n    text = text.replace('data_z =', '')\n    text = text.replace('data_all =', '')\n    text = text.replace('data_zzc =', '')\n    text = text.replace('data_sg=', '')\n    text = text.replace('data_g=', '')\n    text = text.replace('data_z=', '')\n    text = text.replace('data_all=', '')\n    text = text.replace('data_zzc=', '')\n    text = text.replace('data_sg =', '')\n    text = text.replace('data_g =', '')\n    text = text.replace('data_z =', '')\n    text = text.replace('data_all =', '')\n    text = text.replace", "CodeBleu_score": 29.4616533108921}, {"code": "def _parse_fund_data(url, fund_type='open')\n\t\n    print('\u6b63\u5728\u83b7\u53d6\u6570\u636e\uff0c\u8bf7\u7a0d\u5019...')\n    try:\n        response = urlopen(url)\n        text = response.read()\n        text = text.decode('utf-8')\n        text = text.replace('--', '')\n        json_data = json.loads(text)\n        if fund_type == 'open':\n            fund_df = pd.DataFrame(json_data['data']['diff'], columns=['fcode', 'fname', 'fund_type', 'date', 'unit_nav', 'acc_nav', 'nav_change', 'nav_change_rate', 'total_nav', 'total_nav_change', 'total_nav_change_rate','seven_days_nav_change_rate','seven_days_nav_change','seven_days_total_nav_change','seven_days_total_nav_change_rate', 'thirty_days_nav_change_rate', 'thirty_days_nav_change', 'thirty_days_total_nav_change', 'thirty_days_total_nav_change_rate','sixty_days_nav_change_rate','sixty_days_nav_change','sixty_days_total_nav_change','sixty_days_total_nav_change_rate', 'year_nav_change_rate', 'year_nav_change', 'year_total_nav_change', 'year_total_nav_change_rate', 'two_years_nav_change_rate', 'two_years_nav_change', 'two_years_total_nav_change', 'two_years_total_nav_change_rate', 'three_years_nav_change_rate', 'three_years_nav_change', 'three_years_total_nav_change', 'three_years_total_nav_change_rate', 'five_years_nav_change_rate', 'five_years_nav_change', 'five_years_total_nav_change', 'five_years_total_nav_change_rate'])\n        else:\n            fund_df = pd.DataFrame(json_data['data']['diff'], columns=['fcode', 'fname', 'fund_type', 'date', 'unit_nav', 'acc_nav', 'nav_change', 'nav_", "CodeBleu_score": 31.42297795376936}, {"code": "def _parse_fund_data(url, fund_type='open')\n\t\n    print(url)\n    try:\n        with urlopen(url) as response:\n            text = response.read()\n            text = text.decode('utf-8')\n            text = text.replace(' ', '')\n            text = text.replace('", "CodeBleu_score": 14.642057904554903}]}
{"instance_id": "401d18e122d8", "generation_result": [{"code": "def get_future_daily(start = None, end = None, market = 'CFFEX')\n\t\n    if start is None:\n        start = today()\n    if end is None:\n        end = today()\n    start = convert_date(start)\n    end = convert_date(end)\n    df_list = []\n    for date in daterange(start, end):\n        if market == 'CFFEX':\n            df = get_future_daily_cffex(date)\n        elif market == 'SHFE':\n            df = get_future_daily_shfe(date)\n        elif market == 'DCE':\n            df = get_future_daily_dce(date)\n        elif market == 'CZCE':\n            df = get_future_daily_czce(date)\n        else:\n            raise ValueError('Invalid market specified.')\n        if not df.empty:\n            df_list.append(df)\n    if len(df_list) > 0:\n        return pd.concat(df_list)\n    else:\n        return pd.DataFrame()\n", "CodeBleu_score": 46.15718032414612}, {"code": "def get_future_daily(start = None, end = None, market = 'CFFEX')\n\t\n    if start is None:\n        start = today()\n    if end is None:\n        end = today()\n    start = convert_date(start)\n    end = convert_date(end)\n    df_list = []\n    for date in pd.date_range(start, end):\n        if market == 'CFFEX':\n            df = get_future_daily_cffex(date)\n        elif market == 'DCE':\n            df = get_future_daily_dce(date)\n        elif market == 'SHFE':\n            df = get_future_daily_shfe(date)\n        elif market == 'CZCE':\n            df = get_future_daily_czce(date)\n        else:\n            raise ValueError(f'Invalid market: {market}')\n        if not df.empty:\n            df_list.append(df)\n    if len(df_list) > 0:\n        return pd.concat(df_list, ignore_index=True)\n    else:\n        return pd.DataFrame()\n", "CodeBleu_score": 44.39523262581732}, {"code": "def get_future_daily(start = None, end = None, market = 'CFFEX')\n\t\n    if start is None:\n        start = today\n    if end is None:\n        end = today\n    start = ct.convert_date(start)\n    end = ct.convert_date(end)\n    df_list = []\n    for date in pd.date_range(start, end):\n        if market == 'CFFEX':\n            df = get_future_daily_cffex(date)\n        elif market == 'DCE':\n            df = get_future_daily_dce(date)\n        elif market == 'SHFE':\n            df = get_future_daily_shfe(date)\n        elif market == 'CZCE':\n            df = get_future_daily_czce(date)\n        else:\n            raise ValueError('Invalid market.')\n        if not df.empty:\n            df_list.append(df)\n    if len(df_list) > 0:\n        return pd.concat(df_list)\n    else:\n        return pd.DataFrame()\n\n", "CodeBleu_score": 50.21453349455838}]}
{"instance_id": "697779825988", "generation_result": [{"code": "def realtime_boxoffice(retry_count=3,pause=0.001)\n\t    for _ in range(retry_count):\n        try:\n            req = Request(url='http://www.cbooo.cn/BoxOffice/getRealTimeData')\n            response = urlopen(req)\n            lines = response.read().decode('utf-8')\n            json_data = json.loads(lines)\n            df = pd.DataFrame(json_data['data2'])\n            df = df.drop(['MovieImg','mId'], axis=1)\n            df['timestamp'] = du.get_now()\n            return df\n        except Exception as e:\n            print(e)\n            time.sleep(pause)\n    return None\n", "CodeBleu_score": 45.71579256251571}, {"code": "def realtime_boxoffice(retry_count=3,pause=0.001)\n\t    for _ in range(retry_count):\n        try:\n            request = Request(realtime_boxoffice_url)\n            response = urlopen(request)\n            lines = response.read()\n            json_data = json.loads(lines.decode('utf-8'))\n            df = pd.DataFrame(json_data['data2'])\n            df = df.drop(['MovieImg','mId'], axis=1)\n            df['timestamp'] = du.get_now()\n            return df\n        except Exception as e:\n            print(e)\n            time.sleep(pause)", "CodeBleu_score": 44.680308081248484}, {"code": "def realtime_boxoffice(retry_count=3,pause=0.001)\n\t    for _ in range(retry_count):\n        try:\n            request = Request(realtime_boxoffice_url)\n            response = urlopen(request)\n            data = json.loads(response.read().decode('utf-8-sig'))\n            df = pd.DataFrame(data['data2'])\n            df.drop(['MovieImg','mId'], axis=1, inplace=True)\n            df['timestamp'] = du.get_now()\n            return df\n        except Exception as e:\n            print(e)\n            time.sleep(pause)\n", "CodeBleu_score": 39.87731949999134}]}
