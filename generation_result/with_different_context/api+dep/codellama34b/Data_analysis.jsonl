{"instance_id": "58936b2a621b", "generation_result": [{"code": "def set_model_params(model: LogisticRegression, params: NDArrays) -> LogisticRegression\n\tmodel.coef_ = params[0]\n    if model.fit_intercept:\n        model.intercept_ = params[1]\n    return model\n\n\n\n\n\n", "CodeBleu_score": 82.78484165299523}, {"code": "def set_model_params(model: LogisticRegression, params: NDArrays) -> LogisticRegression\n\tmodel.coef_ = params[0]\n    if model.fit_intercept:\n        model.intercept_ = params[1]\n    return model\n\n\n\n\n\n", "CodeBleu_score": 82.78484165299523}, {"code": "def set_model_params(model: LogisticRegression, params: NDArrays) -> LogisticRegression\n\tmodel.coef_ = params[0]\n    if model.fit_intercept:\n        model.intercept_ = params[1]\n    return model\n\n", "CodeBleu_score": 82.78484165299523}]}
{"instance_id": "83b142d9d251", "generation_result": [{"code": "def support_query_split(\n    data,\n    label,\n    support_ratio: float,\n) -> Tuple[List, List, List, List]\n\tx_train, x_test, y_train, y_test = train_test_split(\n        data, label, test_size=1 - support_ratio, random_state=42, stratify=label\n    )\n    return x_train, x_test, y_train, y_test\n\n", "CodeBleu_score": 67.90492453542434}, {"code": "def support_query_split(\n    data,\n    label,\n    support_ratio: float,\n) -> Tuple[List, List, List, List]\n\t# Split data and labels into training and testing sets\n    x_train, x_test, y_train, y_test = train_test_split(\n        data, label, test_size=1 - support_ratio, random_state=42, stratify=label\n    )\n    return x_train, x_test, y_train, y_test\n\n", "CodeBleu_score": 68.38675789341384}, {"code": "def support_query_split(\n    data,\n    label,\n    support_ratio: float,\n) -> Tuple[List, List, List, List]\n\tx_train, x_test, y_train, y_test = train_test_split(\n        data, label, test_size=1 - support_ratio, random_state=42, stratify=label\n    )\n    return x_train, x_test, y_train, y_test", "CodeBleu_score": 67.90492453542434}]}
{"instance_id": "5fd02b540f61", "generation_result": [{"code": "def load_data() -> (\n    Tuple[List[np.ndarray], List[np.ndarray], List[np.ndarray], List[np.ndarray]]\n)\n\tX, y = make_regression(\n        n_samples=100, n_features=3, n_informative=2, random_state=123\n    )\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=123\n    )\n    return X_train, y_train, X_test, y_test\n\n", "CodeBleu_score": 37.63634281378486}, {"code": "def load_data() -> (\n    Tuple[List[np.ndarray], List[np.ndarray], List[np.ndarray], List[np.ndarray]]\n)\n\tX, y = make_regression(\n        n_samples=1000, n_features=3, noise=10, random_state=1\n    )\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=1\n    )\n    return X_train, y_train, X_test, y_test\n\n", "CodeBleu_score": 38.146059189331616}, {"code": "def load_data() -> (\n    Tuple[List[np.ndarray], List[np.ndarray], List[np.ndarray], List[np.ndarray]]\n)\n\tX, y = make_regression(n_samples=1000, n_features=3, noise=10, random_state=1)\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.33, random_state=1\n    )\n    return X_train, y_train, X_test, y_test\n\n", "CodeBleu_score": 38.46568805434154}]}
{"instance_id": "ad958cb31a57", "generation_result": [{"code": "def ndarrays_to_gmmparam(ndarrays: NDArray) -> GMMParameters\n\tgmm_param = GMMParameters()\n    gmm_param.label = ndarrays[0]\n    gmm_param.means = ndarrays[1]\n    gmm_param.weights = ndarrays[2]\n    gmm_param.covariances = ndarrays[3]\n    gmm_param.num_samples = ndarrays[4]\n    return gmm_param\n\n\n\n\n", "CodeBleu_score": 44.014097423493624}, {"code": "def ndarrays_to_gmmparam(ndarrays: NDArray) -> GMMParameters\n\tgmm_params = GMMParameters()\n    gmm_params.label = ndarrays[0].tolist()\n    gmm_params.means = ndarrays[1].tolist()\n    gmm_params.weights = ndarrays[2].tolist()\n    gmm_params.covariances = ndarrays[3].tolist()\n    gmm_params.num_samples = ndarrays[4].tolist()\n    return gmm_params\n\n", "CodeBleu_score": 41.60206684058291}, {"code": "def ndarrays_to_gmmparam(ndarrays: NDArray) -> GMMParameters\n\tgmm_params = GMMParameters()\n    gmm_params.label = ndarrays[0]\n    gmm_params.means = ndarrays[1]\n    gmm_params.weights = ndarrays[2]\n    gmm_params.covariances = ndarrays[3]\n    gmm_params.num_samples = ndarrays[4]\n    return gmm_params\n\n", "CodeBleu_score": 43.62839141074365}]}
{"instance_id": "061d5a5b9d56", "generation_result": [{"code": "def datafiles_fusion(data_paths):\n\t# Load the first data file\n    X, Y = load_svmlight_file(data_paths[0])\n    X = X.toarray()\n    # Concatenate the features and labels from each data file\n    for data_path in data_paths[1:]:\n        X_temp, Y_temp = load_svmlight_file(data_path)\n        X_temp = X_temp.toarray()\n        X = np.concatenate((X, X_temp), axis=0)\n        Y = np.concatenate((Y, Y_temp), axis=0)\n    return X, Y", "CodeBleu_score": 36.433035246890434}, {"code": "def datafiles_fusion(data_paths):\n\t# Load the first data file\n    X, Y = load_svmlight_file(data_paths[0])\n    X = X.toarray()\n    # Initialize the feature and label arrays\n    X_fuse = X\n    Y_fuse = Y\n\n    # Load each data file and concatenate the features and labels\n    for i in range(1, len(data_paths)):\n        X, Y = load_svmlight_file(data_paths[i])\n        X = X.toarray()\n        X_fuse = np.concatenate((X_fuse, X), axis=0)\n        Y_fuse = np.concatenate((Y_fuse, Y), axis=0)\n\n    return X_fuse, Y_fuse", "CodeBleu_score": 38.28425875981398}, {"code": "def datafiles_fusion(data_paths):\n\tX, Y = load_svmlight_file(data_paths[0])\n    X = X.toarray()\n    for data_path in data_paths[1:]:\n        X_temp, Y_temp = load_svmlight_file(data_path)\n        X_temp = X_temp.toarray()\n        if X_temp.shape[1] != X.shape[1]:\n            raise ValueError('Number of features in data file {0} ({1}) does not match number of features in data file {2} ({3})'.format(data_path, X_temp.shape[1], data_paths[0], X.shape[1]))\n        X = np.concatenate((X, X_temp), axis=0)\n        Y = np.concatenate((Y, Y_temp), axis=0)\n    return X, Y", "CodeBleu_score": 35.182714018864694}]}
{"instance_id": "977d3bd5d5a3", "generation_result": [{"code": "def eval_learning(y_test, y_pred)\n\t# Calculate accuracy\n    accuracy = accuracy_score(y_test, y_pred)\n\n    # Calculate recall\n    recall = recall_score(y_test, y_pred, average=\"micro\")\n\n    # Calculate precision\n    precision = precision_score(y_test, y_pred, average=\"micro\")\n\n    # Calculate F1 score\n    f1 = f1_score(y_test, y_pred, average=\"micro\")\n\n    return accuracy, recall, precision, f1", "CodeBleu_score": 40.38465881740944}, {"code": "def eval_learning(y_test, y_pred)\n\taccuracy = accuracy_score(y_test, y_pred)\n    recall = recall_score(y_test, y_pred, average='micro')\n    precision = precision_score(y_test, y_pred, average='micro')\n    f1 = f1_score(y_test, y_pred, average='micro')\n    print(f'Accuracy: {accuracy:.3f}", "CodeBleu_score": 28.15410237240718}, {"code": "def eval_learning(y_test, y_pred)\n\taccuracy = accuracy_score(y_test, y_pred)\n    recall = recall_score(y_test, y_pred, average=\"micro\")\n    precision = precision_score(y_test, y_pred, average=\"micro\")\n    f1 = f1_score(y_test, y_pred, average=\"micro\")\n    return accuracy, recall, precision, f1", "CodeBleu_score": 39.95805736557168}]}
{"instance_id": "f4c838f1f092", "generation_result": [{"code": "def train(model, train_loader, num_epochs=1)\n\tcriterion = nn.BCELoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n    model.train()\n    for epoch in range(num_epochs):\n        for X_batch, y_batch in train_loader:\n            optimizer.zero_grad()\n            outputs = model(X_batch)\n            loss = criterion(outputs, y_batch)\n            loss.backward()\n            optimizer.step()", "CodeBleu_score": 72.56317172205301}, {"code": "def train(model, train_loader, num_epochs=1)\n\tcriterion = nn.BCELoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n    model.train()\n    for epoch in range(num_epochs):\n        for X_batch, y_batch in train_loader:\n            optimizer.zero_grad()\n            outputs = model(X_batch)\n            loss = criterion(outputs, y_batch)\n            loss.backward()\n            optimizer.step()", "CodeBleu_score": 72.56317172205301}, {"code": "def train(model, train_loader, num_epochs=1)\n\tcriterion = nn.BCELoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n    model.train()\n    for epoch in range(num_epochs):\n        for X_batch, y_batch in train_loader:\n            optimizer.zero_grad()\n            outputs = model(X_batch)\n            loss = criterion(outputs, y_batch)\n            loss.backward()\n            optimizer.step()", "CodeBleu_score": 72.56317172205301}]}
{"instance_id": "35068c779c02", "generation_result": [{"code": "def _load_libsvm_dataset(cfg: DictConfig) -> Dataset\n\tif cfg.dataset.type != \"libsvm\":\n        raise RuntimeError(f\"Invalid dataset type: {cfg.dataset.type}\")\n    dataset_path = cfg.dataset.path\n    dataset_name = cfg.dataset.name\n    print(f\"Loading {dataset_name} dataset from {dataset_path}\")\n    data, labels = load_svmlight_file(dataset_path)\n    data = data.toarray().astype(np.float32)\n    if dataset_name == \"MUSHROOMS\":\n        labels = labels == 1\n    else:\n        raise RuntimeError(f\"Invalid dataset name: {dataset_name}\")\n    print(f\"Unique labels: {np.unique(labels, return_counts=True)}\")\n    print(f\"Shape of features: {data.shape}\")\n    return data_utils.TensorDataset(\n        torch.Tensor(data), torch.Tensor(labels).type(torch.LongTensor)\n    )", "CodeBleu_score": 38.78706408947639}, {"code": "def _load_libsvm_dataset(cfg: DictConfig) -> Dataset\n\tif cfg.dataset.type != 'LIBSVM':\n        raise RuntimeError(f\"Dataset type {cfg.dataset.type} not supported\")\n\n    dataset_path = cfg.dataset.path\n    dataset_name = cfg.dataset.name\n    print(f\"Loading dataset {dataset_name} from {dataset_path}\")\n\n    data, labels = load_svmlight_file(dataset_path)\n    data = data.toarray().astype(np.float32)\n    labels = labels.astype(np.float32)\n    print(f\"Unique labels: {np.unique(labels, return_counts=True)}\")\n    print(f\"Features shape: {data.shape}\")\n\n    if dataset_name == 'MUSHROOMS':\n        labels = (labels == 1).astype(np.float32)\n    else:\n        raise RuntimeError(f\"Dataset {dataset_name} not supported\")\n\n    return data_utils.TensorDataset(torch.Tensor(data), torch.Tensor(labels))", "CodeBleu_score": 41.81949522611387}, {"code": "def _load_libsvm_dataset(cfg: DictConfig) -> Dataset\n\tif cfg.dataset.type != \"libsvm\":\n        raise RuntimeError(\"Dataset type must be libsvm\")\n\n    dataset_path = cfg.dataset.path\n    dataset_name = cfg.dataset.name\n\n    data, labels = load_svmlight_file(dataset_path)\n    data = data.toarray().astype(np.float32)\n    labels = labels.astype(np.int64)\n\n    print(f\"Unique labels: {np.unique(labels, return_counts=True)}\")\n    print(f\"Shape of features: {data.shape}\")\n\n    if dataset_name == \"MUSHROOMS\":\n        labels = labels - 1\n    else:\n        raise RuntimeError(\"Dataset must be MUSHROOMS\")\n\n    return data_utils.TensorDataset(torch.Tensor(data), torch.Tensor(labels))", "CodeBleu_score": 39.740816745561474}]}
{"instance_id": "d602a96e571e", "generation_result": [{"code": "def _fit_transform_one(transformer, X, y, weight, **fit_params)\n\tres = transformer.fit_transform(X, y, **fit_params)\n    if weight is None:\n        return res, transformer\n    return res * weight, transformer\n\n", "CodeBleu_score": 38.82392474296739}, {"code": "def _fit_transform_one(transformer, X, y, weight, **fit_params)\n\tres = transformer.fit_transform(X, y, **fit_params)\n    if weight is None:\n        return res, transformer\n    return res * weight, transformer\n\n", "CodeBleu_score": 38.82392474296739}, {"code": "def _fit_transform_one(transformer, X, y, weight, **fit_params)\n\tres = transformer.fit_transform(X, y, **fit_params)\n    if weight is None:\n        return res, transformer\n    return res * weight, transformer\n\n", "CodeBleu_score": 38.82392474296739}]}
{"instance_id": "fcf88a773fd8", "generation_result": [{"code": "def _transform_selected(X, transform, selected=\"all\", copy=True)\n\tX = check_array(X, accept_sparse='csc', copy=copy, dtype=FLOAT_DTYPES)\n    n_samples, n_features = X.shape\n\n    if isinstance(selected, six.string_types) and selected == \"all\":\n        return transform(X)\n    elif (not isinstance(selected, collections.Iterable)) or isinstance(\n            selected, six.string_types):\n        raise ValueError(\"Wrong input for parameter `selected`. \"\n                         \"Selected should be a list of integers, \"\n                         \"a string or an array of integers. \"\n                         \"Got %s instead\" % type(selected))\n\n    # The mask for selected features is computed once and for all.\n    # Note: it is different from the one used in _fit_new\n    # because here we only care about the features selected in the\n    # current iteration while _fit_new cares about all the selected\n    # features so far.\n    if selected == \"all\" or np.isscalar(selected):\n        selected_mask = None\n    else:\n        selected_mask = np.zeros(n_features, dtype=np.bool)\n        selected_mask[np.asarray(selected)] = 1\n\n    if np.sum(selected_mask) == 0:\n        # No features selected. We can return X unchanged.\n        return X\n    elif np.sum(selected_mask) == n_features:\n        # All features selected. We can avoid the for loop.\n        return transform(X)\n\n    # If we are here, it means that only a subset of the features is selected.\n    # We apply the transformation to the selected features and combine with\n    # the rest of the features.\n    ind = np.arange(n_features)\n    sel = np.arange(n_features)[selected_mask]\n    other = ind[~selected_mask]\n    n_sel = len(sel)\n    n_other = len(other)\n\n    X_sel = transform(X[:, sel])\n    X_other = X[:, other]\n\n    if sparse.issparse(X_sel) or sparse.iss", "CodeBleu_score": 47.38304715373533}, {"code": "def _transform_selected(X, transform, selected=\"all\", copy=True)\n\tX = check_array(X, accept_sparse='csc', copy=copy)\n    n_samples, n_features = X.shape\n\n    if selected == \"all\":\n        return transform(X)\n    elif not selected:\n        return X\n    elif len(selected) == n_features:\n        return transform(X)\n    elif len(selected) == 0:\n        return X\n\n    if not _is_arraylike(selected):\n        raise ValueError(\"'selected' should be a list or array\")\n\n    # Ensure that the array is two-dimensional\n    if X.ndim == 1:\n        X = X.reshape(1, -1)\n\n    if np.max(selected) > n_features - 1:\n        raise ValueError(\"The elements of 'selected' should be less than \"\n                         \"the number of columns of X.\")\n    if np.min(selected) < 0:\n        raise ValueError(\"The elements of 'selected' should be greater \"\n                         \"than or equal to 0.\")\n\n    if isinstance(selected, (list, tuple)):\n        selected = np.array(selected, dtype=np.int32)\n\n    mask = np.ones(n_features, dtype=bool)\n    mask[selected] = False\n\n    if X.ndim == 1:\n        non_selected = X[mask]\n    else:\n        non_selected = X[:, mask]\n\n    if X.ndim == 1:\n        selected = X[selected]\n    else:\n        selected = X[:, selected]\n\n    selected = transform(selected)\n\n    if sparse.issparse(selected) and sparse.issparse(non_selected):\n        return sparse.hstack((selected, non_selected))\n    elif sparse.issparse(selected) or sparse.issparse(non_selected):\n        return sparse.hstack((selected, non_selected)).toarray()\n    else:\n        return np.hstack((selected, non_selected))\n\n", "CodeBleu_score": 45.11803562301647}, {"code": "def _transform_selected(X, transform, selected=\"all\", copy=True)\n\tX = check_array(X, accept_sparse='csc', copy=copy, dtype=FLOAT_DTYPES)\n    n_samples, n_features = X.shape\n\n    if isinstance(selected, six.string_types) and selected == \"all\":\n        return transform(X)\n    elif (not hasattr(selected, \"__len__\")) or len(selected) == n_features:\n        return transform(X)\n    elif len(selected) == 0:\n        return X\n\n    if (sparse.issparse(X) and not sparse.issparse(transform(X[:, 0]))):\n        raise ValueError(\"cannot use sparse input with a dense transform \"\n                         \"function\")\n\n    indices = np.arange(n_features)\n    sel = np.zeros(n_features, dtype=bool)\n    sel[np.asarray(selected)] = True\n    not_sel = np.logical_not(sel)\n    n_selected = sel.sum()\n\n    if n_selected == 0:\n        return X\n    elif n_selected == n_features:\n        return transform(X)\n\n    X_sel = transform(X[:, sel])\n    X_not_sel = X[:, not_sel]\n\n    if sparse.issparse(X_sel) or sparse.issparse(X_not_sel):\n        return sparse.hstack((X_sel, X_not_sel)).tocsr()\n    else:\n        return np.hstack((X_sel, X_not_sel))\n\n", "CodeBleu_score": 63.03183729212838}]}
{"instance_id": "c9dfb00047d1", "generation_result": [{"code": "def _transform_new(self, X)\n\t# Check the input array X and adjust its data type if necessary\n    X_temp = check_array(X, dtype=None)\n    if not hasattr(X, 'dtype') and np.issubdtype(X_temp.dtype, np.str_):\n        X = check_array(X, dtype=np.object)\n    else:\n        X = X_temp\n\n    # Retrieve the number of samples and features in X\n    n_samples, n_features = X.shape\n\n    # Transform the input array into integer-encoded values and generate a mask for valid entries\n    X_int = np.zeros_like(X, dtype=np.int)\n    X_mask = np.ones_like(X, dtype=np.bool)\n\n    for i in range(n_features):\n        Xi = X[:, i]\n        valid_mask = np.in1d(Xi, self.categories_[i])\n\n        if not np.all(valid_mask):\n            if handle_unknown == 'error':\n                diff = np.unique(X[~valid_mask, i])\n                msg = (\"Found unknown categories {0} in column {1}\"\n                       \" during transform\".format(diff, i))\n                raise ValueError(msg)\n            else:\n                # Set the problematic rows to an acceptable value and\n                # continue `The rows are marked `X_mask` and will be\n                # removed later.\n                X_mask[:, i] = valid_mask\n                Xi = Xi.copy()\n                Xi[~valid_mask] = self.categories_[i][0]\n        X_int[:, i] = self._label_encoders_[i].transform(Xi)\n\n    # Calculate the number of categories for each feature\n    n_values = [len(self.categories_[i]) for i in range(n_features)]\n\n    # Compute feature indices\n    indices = np.cumsum([0] + n_values)\n\n    # Construct indices and indptr arrays for creating a sparse matrix\n    indptr = np.zeros(n_samples + 1, dtype=np.", "CodeBleu_score": 45.17916853825241}, {"code": "def _transform_new(self, X)\n\t# Check the input array X and adjust its data type if necessary\n    X_temp = check_array(X, dtype=None)\n    if not hasattr(X, 'dtype') and np.issubdtype(X_temp.dtype, np.str_):\n        X = check_array(X, dtype=np.object)\n    else:\n        X = X_temp\n\n    # Retrieve the number of samples and features in X\n    n_samples, n_features = X.shape\n\n    # Transform the input array into integer-encoded values and generate a mask for valid entries\n    X_int, X_mask = self._transform(X)\n\n    # Calculate the number of categories for each feature\n    n_values = [len(le.classes_) for le in self._label_encoders_]\n\n    # Compute feature indices\n    indices = np.cumsum([0] + n_values)\n\n    # Construct indices and indptr arrays for creating a sparse matrix\n    indptr = np.zeros(n_samples + 1, dtype=np.int)\n    indptr[1:] = np.cumsum(X_mask.sum(axis=1).ravel())\n\n    # Construct a sparse matrix with binary data\n    data = np.ones(n_samples * np.sum(X_mask))\n    indices = np.repeat(indices, n_samples)\n    indptr = np.repeat(indptr, n_features)\n    indptr[1:] = indptr[:-1] + X_mask.sum(axis=1).ravel()\n    indices = indices.take(np.where(X_mask.ravel())[0])\n    data = data.take(np.where(X_mask.ravel())[0])\n\n    out = sparse.csr_matrix((data, indices, indptr),\n                            shape=(n_samples, np.sum(n_values)),\n                            dtype=self.dtype)\n\n    # Convert the sparse matrix to a dense array if the sparse attribute is set to False\n    if not self.sparse:\n        out = out.toarray()\n\n    # Return the transformed data", "CodeBleu_score": 58.84790158091775}, {"code": "def _transform_new(self, X)\n\t# Check the input array X and adjust its data type if necessary\n    X_temp = check_array(X, dtype=None)\n    if not hasattr(X, 'dtype') and np.issubdtype(X_temp.dtype, np.str_):\n        X = check_array(X, dtype=np.object)\n    else:\n        X = X_temp\n\n    # Retrieve the number of samples and features in X\n    n_samples, n_features = X.shape\n\n    # Transform the input array into integer-encoded values and generate a mask for valid entries\n    X_int, X_mask = self._transform(X)\n\n    # Calculate the number of categories for each feature\n    n_values = []\n    for i in range(n_features):\n        n_values.append(len(self.categories_[i]))\n\n    # Compute feature indices\n    indices = []\n    indptr = []\n    indptr.append(0)\n    for i in range(n_features):\n        indices += list(np.ravel(np.array(range(n_samples)) * n_values[i] + X_int[:, i].ravel()))\n        indptr.append(len(indices))\n\n    # Construct indices and indptr arrays for creating a sparse matrix\n    indices = np.array(indices)\n    indptr = np.array(indptr)\n\n    # The output is a sparse CSR matrix with binary data, which can be converted to a dense array if the sparse attribute is set to False\n    out = sparse.csr_matrix((np.ones(n_samples * n_features), indices, indptr),\n                            shape=(n_samples, n_features * n_values),\n                            dtype=self.dtype)\n\n    # Return the transformed data\n    return out.toarray() if self.sparse else out", "CodeBleu_score": 50.18683200786668}]}
{"instance_id": "fcde391a8972", "generation_result": [{"code": "def convert_to_sklearn(self)\n\tif self.log:\n        from scipy.stats import loguniform\n        return loguniform(self.lower, self.upper)\n    else:\n        from scipy.stats import uniform\n        return uniform(self.lower, self.upper)\n\n", "CodeBleu_score": 38.24116359227293}, {"code": "def convert_to_sklearn(self)\n\tif self.log:\n        from scipy.stats import loguniform\n        return loguniform(self.lower, self.upper)\n    else:\n        from scipy.stats import uniform\n        return uniform(self.lower, self.upper)", "CodeBleu_score": 38.24116359227293}, {"code": "def convert_to_sklearn(self)\n\tfrom scipy.stats import uniform\n    from scipy.stats import loguniform\n    if self.log:\n        return loguniform(self.lower, self.upper)\n    else:\n        return uniform(self.lower, self.upper)\n\n\n\n\n", "CodeBleu_score": 37.133265878361485}]}
{"instance_id": "5d5413adcca6", "generation_result": [{"code": "def _render(self, state: AnalysisState) -> None\n\tadditional_kwargs, fig_args, features = self._get_args()\n    self._render_header_if_needed(state)\n    fig, axs = plt.subplots(**fig_args)\n    kwargs = {**self.kwargs, **additional_kwargs}\n    if self.two_way:\n        data = state.data[state.data[self.features].notna().all(axis=1)]\n    else:\n        data = state.data\n    PartialDependenceDisplay.from_estimator(\n        state.estimator,\n        data,\n        features,\n        **kwargs,\n        ax=axs,\n    )\n    plt.tight_layout()\n    plt.show()\n\n", "CodeBleu_score": 41.64070856999766}, {"code": "def _render(self, state: AnalysisState) -> None\n\tadditional_kwargs, fig_args, features = self._get_args()\n    if self.header:\n        self.render_header_if_needed(state)\n    fig, axs = plt.subplots(**fig_args)\n    kwargs = {**self.kwargs, **additional_kwargs}\n    if self.two_way:\n        data = state.data.loc[state.data[features[0]].notna() & state.data[features[1]].notna(), :]\n    else:\n        data = state.data\n    PartialDependenceDisplay.from_estimator(\n        state.model,\n        data,\n        features,\n        ax=axs,\n        **kwargs,\n    )\n    plt.tight_layout()\n    plt.show()\n\n", "CodeBleu_score": 51.05542029259675}, {"code": "def _render(self, state: AnalysisState) -> None\n\tadditional_kwargs, fig_args, features = self._get_args()\n    if self.header is not None:\n        self.render_header_if_needed()\n\n    fig, axs = plt.subplots(**fig_args)\n    kwargs = {**self.kwargs, **additional_kwargs}\n    if self.two_way:\n        data = state.data.loc[state.data.isna().sum(axis=1) == 0]\n    else:\n        data = state.data\n    PartialDependenceDisplay.from_estimator(\n        state.model,\n        features,\n        data,\n        **kwargs,\n    ).plot(ax=axs)\n    plt.tight_layout()\n    plt.show()", "CodeBleu_score": 47.38374757343323}]}
{"instance_id": "db8aac29180b", "generation_result": [{"code": "def _preprocess(self, X, **kwargs)\n\tX = super()._preprocess(X, **kwargs)\n    if self._feature_generator is None:\n        self._feature_generator = LabelEncoderFeatureGenerator(verbosity=0)\n        self._feature_generator.fit(X=X)\n    if self._feature_generator.features_in:\n        X = X.copy()\n        X[self._feature_generator.features_in] = self._feature_generator.transform(X=X)\n    X = X.fillna(0).to_numpy(dtype=np.float32)\n    return X", "CodeBleu_score": 79.24938401754183}, {"code": "def _preprocess(self, X, **kwargs)\n\tX = super()._preprocess(X, **kwargs)\n    if self._feature_generator is None:\n        self._feature_generator = LabelEncoderFeatureGenerator(verbosity=0)\n        self._feature_generator.fit(X=X)\n    if self._feature_generator.features_in:\n        X = X.copy()\n        X[self._feature_generator.features_in] = self._feature_generator.transform(X=X)\n    X = X.fillna(0).to_numpy(dtype=np.float32)\n    return X", "CodeBleu_score": 79.24938401754183}, {"code": "def _preprocess(self, X, **kwargs)\n\tX = super()._preprocess(X, **kwargs)\n    if self._feature_generator is None:\n        self._feature_generator = LabelEncoderFeatureGenerator(verbosity=0)\n        self._feature_generator.fit(X=X)\n    if self._feature_generator.features_in:\n        X = X.copy()\n        X[self._feature_generator.features_in] = self._feature_generator.transform(X=X)\n    X = X.fillna(0).to_numpy(dtype=np.float32)\n    return X", "CodeBleu_score": 79.24938401754183}]}
{"instance_id": "d4eb45e6d013", "generation_result": [{"code": "def _estimate_memory_usage(self, X, **kwargs)\n\tif self.problem_type == REGRESSION:\n        # TODO: Determine what the memory usage will be for regression\n        return 0\n    # Get model parameters\n    model_params = self._get_model_params()\n    # Determine number of estimators\n    num_estimators = model_params['n_estimators']\n    if self.problem_type in [MULTICLASS, SOFTCLASS]:\n        if self.num_classes is None:\n            num_estimators = 10  # Guess since it wasn't passed in, could also check y for a better value\n        else:\n            num_estimators = self.num_classes\n    # TODO: Determine if this is the best way to determine the number of trees per estimator\n    num_trees_per_estimator = self._get_num_trees_per_estimator()\n    # TODO: Determine if this is the best way to determine the number of estimators\n    if num_estimators < 40 or self.hyperparameter_tune:\n        num_estimators = 40\n    # TODO: Determine if this is the best way to determine the number of estimators\n    if num_estimators > 1000:\n        num_estimators = 1000\n    # TODO: Determine if this is the best way to determine the number of estimators\n    if num_estimators < 40:\n        num_estimators = 40\n    # TODO: Determine if this is the best way to determine the number of estimators\n    if num_estimators > 1000:\n        num_estimators = 1000\n    # TODO: Determine if this is the best way to determine the number of estimators\n    if num_estimators < 40 or self.hyperparameter_tune:\n        num_estimators = 40\n    # TODO: Determine if this is the best way to determine the number of estimators\n    if num_estimators > 1000:\n        num_estimators = 1000", "CodeBleu_score": 34.39527947105811}, {"code": "def _estimate_memory_usage(self, X, **kwargs)\n\t# Retrieve model parameters\n    model_params = self._get_model_params()\n    num_trees_per_estimator = self._get_num_trees_per_estimator()\n    if 'num_trees' in model_params:\n        num_trees = model_params['num_trees']\n    else:\n        num_trees = 100\n    if 'num_estimators' in model_params:\n        num_estimators = model_params['num_estimators']\n    else:\n        num_estimators = 100\n    if 'max_leaves' in model_params:\n        max_leaves = model_params['max_leaves']\n    else:\n        max_leaves = 2 ** 10\n    if 'max_depth' in model_params:\n        max_depth = model_params['max_depth']\n    else:\n        max_depth = 10\n    if 'max_bins' in model_params:\n        max_bins = model_params['max_bins']\n    else:\n        max_bins = 255\n\n    # Determine the final number of estimators\n    if self.problem_type in [MULTICLASS, SOFTCLASS]:\n        if self.num_classes is None:\n            num_estimators = max(num_estimators, num_trees_per_estimator)\n        else:\n            num_estimators = max(num_estimators, self.num_classes)\n    else:\n        num_estimators = max(num_estimators, num_trees_per_estimator)\n    num_estimators = max(num_estimators, 40)\n\n    # Calculate the number of trees per estimator\n    if num_estimators > num_trees:\n        num_trees_per_estimator = num_trees // num_estimators\n    else:\n        num_trees_per_estimator = 1\n\n    # Estimate the bytes used per estimator\n    if isinstance(X, pd.DataFrame):\n        # Calculate the", "CodeBleu_score": 32.71399885794624}, {"code": "def _estimate_memory_usage(self, X, **kwargs)\n\t# TODO: This function currently does not account for memory usage of the model after training, only during training.\n    # TODO: This function currently does not account for memory usage of the model after training, only during training.\n    # TODO: This function currently does not account for memory usage of the model after training, only during training.\n    # TODO: This function currently does not account for memory usage of the model after training, only during training.\n    # TODO: This function currently does not account for memory usage of the model after training, only during training.\n    # TODO: This function currently does not account for memory usage of the model after training, only during training.\n    # TODO: This function currently does not account for memory usage of the model after training, only during training.\n    # TODO: This function currently does not account for memory usage of the model after training, only during training.\n    # TODO: This function currently does not account for memory usage of the model after training, only during training.\n    # TODO: This function currently does not account for memory usage of the model after training, only during training.\n    # TODO: This function currently does not account for memory usage of the model after training, only during training.\n    # TODO: This function currently does not account for memory usage of the model after training, only during training.\n    # TODO: This function currently does not account for memory usage of the model after training, only during training.\n    # TODO: This function currently does not account for memory usage of the model after training, only during training.\n    # TODO: This function currently does not account for memory usage of the model after training, only during training.\n    # TODO: This function currently does not account for memory usage of the model after training, only during training.\n    # TODO: This function currently does not account for memory usage of the model after training, only during training.\n    # TODO: This function currently does not account for memory usage of the model after training, only during training.\n    # TODO: This function currently does not account for memory usage of the model after training, only during training.\n    # TODO: This function currently does not account for memory usage of the model after training, only during training.\n    # TODO: This function currently does not account for memory usage of the model after training, only during training.\n    # TODO: This function currently", "CodeBleu_score": 2.4404761904761907}]}
{"instance_id": "6923cb007f4c", "generation_result": [{"code": "def downscale_vectorizer(vectorizer, ngram_freq, vocab_size)\n\t# Create a Counter object from the n-gram frequency data\n    counter = Counter(ngram_freq)\n    # Identify the top n most common n-grams according to the specified vocabulary size\n    top_n = counter.most_common(vocab_size)\n    # Sort the n-grams by name\n    top_n.sort(key=lambda x: x[0])\n    # Create a new vocabulary dictionary, mapping each n-gram to a unique index\n    new_vocab = {ngram: index for index, (ngram, count) in enumerate(top_n)}\n    # Update the vectorizer's vocabulary\n    vectorizer.vocabulary_ = new_vocab\n    return vectorizer", "CodeBleu_score": 39.484735233672986}, {"code": "def downscale_vectorizer(vectorizer, ngram_freq, vocab_size)\n\t# Create a counter object from the n-gram frequency data\n    counter = Counter(ngram_freq)\n    # Identify the top n most common n-grams according to the specified vocabulary size\n    top_n_ngrams = counter.most_common(vocab_size)\n    # Sort the names of the top n n-grams\n    sorted_ngrams = [ngram[0] for ngram in sorted(top_n_ngrams, key=lambda x: x[1], reverse=True)]\n    # Create a new vocabulary dictionary, mapping each n-gram to a unique index\n    new_vocab = dict(zip(sorted_ngrams, range(len(sorted_ngrams))))\n    # Update the vectorizer's vocabulary with the new, downsized vocabulary\n    vectorizer.vocabulary_ = new_vocab\n    return vectorizer", "CodeBleu_score": 24.371177945976065}, {"code": "def downscale_vectorizer(vectorizer, ngram_freq, vocab_size)\n\t# create a Counter object from n-gram frequency data\n    counter = Counter(ngram_freq)\n    # identify the top n most common n-grams according to the specified vocabulary size\n    top_n = counter.most_common(vocab_size)\n    # sort the n-grams in descending order of frequency\n    sorted_top_n = sorted(top_n, key=lambda x: x[1], reverse=True)\n    # create a list of n-grams from the sorted top n\n    ngram_list = [x[0] for x in sorted_top_n]\n    # create a new vocabulary dictionary, mapping each n-gram to a unique index\n    new_vocab = {ngram: i for i, ngram in enumerate(ngram_list)}\n    # update the vectorizer's vocabulary with the new vocabulary\n    vectorizer.vocabulary_ = new_vocab", "CodeBleu_score": 45.30236622199038}]}
{"instance_id": "0e837b2d8088", "generation_result": [{"code": " def _select_continuous(self, df, features)\n\tskewness_threshold = self.parameters[\"skewness_threshold\"]\n    continuous_features = []\n    skewed_features = []\n    for feature in features:\n        if df[feature].skew() < skewness_threshold:\n            continuous_features.append(feature)\n        else:\n            skewed_features.append(feature)\n    return {\"continuous\": continuous_features, \"skewed\": skewed_features}", "CodeBleu_score": 37.03605684057079}, {"code": " def _select_continuous(self, df, features)\n\tskewness_threshold = self.parameters['skewness_threshold']\n    skewed_features = {}\n    skewed_features['continuous'] = []\n    skewed_features['skewed'] = []\n    for feature in features:\n        if np.abs(df[feature].skew()) > skewness_threshold:\n            skewed_features['skewed'].append(feature)\n        else:\n            skewed_features['continuous'].append(feature)\n    return skewed_features", "CodeBleu_score": 48.488354430156924}, {"code": " def _select_continuous(self, df, features)\n\tskewness_threshold = self.parameters[\"skewness_threshold\"]\n    continuous_features = []\n    skewed_features = []\n    for feature in features:\n        if skew(df[feature]) > skewness_threshold:\n            skewed_features.append(feature)\n        else:\n            continuous_features.append(feature)\n    return {\"continuous\": continuous_features, \"skewed\": skewed_features}", "CodeBleu_score": 37.638594985182195}]}
{"instance_id": "bb569c10f975", "generation_result": [{"code": "def _get_weights(dist, weights):\n\tif weights is None or weights == \"uniform\":\n        return None\n    elif weights == \"distance\":\n        if dist.dtype is np.dtype(object):\n            raise ValueError(\"'distance' weights are not supported for non-numeric data\")\n        elif np.any(dist == 0):\n            raise ValueError(\"'distance' weights are not supported for zero distances\")\n        elif np.any(np.isinf(dist)):\n            weights = np.zeros_like(dist)\n            mask = ~np.isinf(dist)\n            weights[mask] = 1 / dist[mask]\n            return weights\n        else:\n            return 1 / dist\n    elif callable(weights):\n        return weights(dist)\n    else:\n        raise ValueError(\"unrecognized weights argument: should be 'uniform', 'distance', or a callable function\")", "CodeBleu_score": 49.555161465847654}, {"code": "def _get_weights(dist, weights):\n\tif weights is None or weights == 'uniform':\n        return None\n    elif weights == 'distance':\n        w = 1.0 / dist\n        w[np.isinf(w)] = 0\n        return w\n    elif callable(weights):\n        return weights(dist)\n    else:\n        raise ValueError('weights not recognized: should be \"uniform\", \"distance\", or a callable function')", "CodeBleu_score": 38.64053209683429}, {"code": "def _get_weights(dist, weights):\n\tif weights is None or weights == \"uniform\"::\n        return None\n    elif weights == \"distance\"::\n        dist = np.asarray(dist)\n        if np.any(dist == 0.0)::\n            raise ValueError(\"The distance matrix contains zero distances, which are not allowed when weights='distance'.\")\n        elif np.isinf(dist).any()::\n            # Handle infinity values by masking them out and using a separate array for weighting\n            # The masked array will contain the non-infinite values of the distances\n            # The mask itself will be True where the distances are infinite\n            # The mask will be used to scale the weights later\n            mask = np.isinf(dist)\n            dist = np.ma.array(dist, mask=mask)\n            dist.set_fill_value(1.0)\n            weights = 1.0 / dist\n            # Now that the weights have been calculated, mask the infinite distances by setting them to the average of the other distances\n            # This preserves the sum(weights) == 1 property\n            dist.data[mask] = dist.data[~mask].mean()\n        else::\n            weights = 1.0 / dist\n        return weights\n    elif callable(weights)::\n        return weights(dist)\n    else::\n        raise ValueError(\"weights not recognized: should be 'uniform', 'distance', or a callable function\")", "CodeBleu_score": 51.95311990728049}]}
{"instance_id": "12aaa9c78402", "generation_result": [{"code": "def predict_loo(self)\n\t# Compute the distances and indices of the k-nearest neighbors\n    dist, ind = self.kneighbors()\n    # Get the weights for the neighbors\n    weights = self._get_weights(dist)\n    # If the target variable _y is one-dimensional, reshape it to a two-dimensional array\n    if len(_y.shape) == 1:\n        _y = _y.reshape(-1, 1)\n    # Initialize a zero-filled array of the same shape as the target variable _y\n    y_pred = np.empty(_y.shape)\n    # Loop through each row of the target variable _y\n    for i in range(len(_y)):\n        # Loop through each column of the target variable _y\n        for j in range(len(_y[i])):\n            # If the weights parameter is set to 'uniform', the weights are all equal to 1\n            if self.weights == 'uniform':\n                weights_j = np.ones(len(ind[i]))\n            # Otherwise, the weights are obtained using the _get_weights function\n            else:\n                weights_j = weights[i]\n            # Calculate the weighted prediction\n            y_pred[i, j] = np.sum(weights_j * _y[ind[i], j]) / np.sum(weights_j)\n    # If the original target variable _y was one-dimensional, flatten the predictions\n    if len(_y.shape) == 1:\n        y_pred = y_pred.ravel()\n    # Return the leave-one-out predictions\n    return y_pred", "CodeBleu_score": 40.13845350447393}, {"code": "def predict_loo(self)\n\t# Compute distances and indices of k-nearest neighbors\n    distances, indices = self.kneighbors()\n    # Get weights for the neighbors\n    weights = self._get_weights(distances)\n    # Initialize predictions\n    predictions = np.empty(len(self._y))\n    # Loop through each observation\n    for i in range(len(self._y)):\n        # Get the indices of the k-nearest neighbors\n        neighbors = indices[i]\n        # Get the target values of the k-nearest neighbors\n        targets = self._y[neighbors]\n        # If the target variable is one-dimensional, reshape it to a two-dimensional array\n        if len(targets.shape) == 1:\n            targets = targets.reshape(-1, 1)\n        # If the weights are not None, calculate the weighted prediction\n        if weights is not None:\n            predictions[i] = np.sum(np.multiply(targets, weights[i]), axis=0) / np.sum(weights[i])\n        # Otherwise, calculate the unweighted prediction\n        else:\n            predictions[i] = np.mean(targets, axis=0)\n    # If the original target variable _y was one-dimensional, flatten the predictions\n    if len(self._y.reshape(-1, 1).shape) == 2:\n        predictions = predictions.ravel()\n    # Return the predictions\n    return predictions", "CodeBleu_score": 37.35603945225545}, {"code": "def predict_loo(self)\n\t# Get the distances and indices of the k-nearest neighbors\n    distances, indices = self.kneighbors(self._X)\n\n    # Get the weights for the neighbors\n    weights = self._get_weights(distances)\n\n    # If the target variable is one-dimensional, reshape it to a two-dimensional array\n    if len(self._y.shape) == 1:\n        _y = self._y.reshape(-1, 1)\n    else:\n        _y = self._y\n\n    # Make predictions by averaging the neighbor target values, either unweighted or weighted\n    if self.weights == 'uniform':\n        predictions = np.empty(len(self._y))\n        for i in range(len(self._y)):\n            predictions[i] = np.mean(_y[indices[i]])\n    else:\n        predictions = np.empty(len(self._y))\n        for i in range(len(self._y)):\n            predictions[i] = np.sum(_y[indices[i]] * weights[i]) / np.sum(weights[i])\n\n    # If the original target variable was one-dimensional, flatten the predictions\n    if len(self._y.shape) == 1:\n        predictions = predictions.ravel()\n\n    return predictions", "CodeBleu_score": 34.6633714955334}]}
{"instance_id": "b316687d9e9b", "generation_result": [{"code": "def isotonic(input_data, quantile_list)\n\tquantile_list = np.array(quantile_list).reshape(-1)\n    batch_size = input_data.shape[0]\n    new_output_data = []\n    for i in range(batch_size):\n        iso_reg = IsotonicRegression(y_min=quantile_list[0], y_max=quantile_list[-1],\n                                     increasing=True, out_of_bounds='clip')\n        new_output_data.append(iso_reg.fit_transform(input_data[i]))\n    new_output_data = np.stack(new_output_data)\n    return new_output_data", "CodeBleu_score": 43.08326299978408}, {"code": "def isotonic(input_data, quantile_list)\n\tquantile_list = np.array(quantile_list).reshape(1, -1)\n    batch_size = input_data.shape[0]\n    new_output_data = []\n    for i in range(batch_size):\n        iso = IsotonicRegression(increasing='decreasing')\n        iso.fit_transform(quantile_list, input_data[i])\n        new_output_data.append(iso.transform(quantile_list))\n    new_output_data = np.stack(new_output_data)\n    return new_output_data\n\n", "CodeBleu_score": 41.71556394490472}, {"code": "def isotonic(input_data, quantile_list)\n\tquantile_list = np.array(quantile_list).reshape(1, -1)\n    batch_size = input_data.shape[0]\n    new_output_data = []\n    for i in range(batch_size):\n        iso_reg = IsotonicRegression(y_min=0, y_max=1, out_of_bounds='clip')\n        iso_reg.fit(quantile_list, input_data[i])\n        new_output_data.append(iso_reg.transform(quantile_list))\n    return np.stack(new_output_data)", "CodeBleu_score": 41.87675811856658}]}
{"instance_id": "17566e088b4e", "generation_result": [{"code": "def _train_vectorizer(text_data: list, vectorizer)\n\tvectorizer.fit(text_data)\n    vectorizer.set_params(stop_words=None)\n    vectorizer_matrix = vectorizer.transform(text_data)\n    return vectorizer, vectorizer_matrix\n\n\n\n\n", "CodeBleu_score": 30.842720661798477}, {"code": "def _train_vectorizer(text_data: list, vectorizer)\n\tvectorizer.fit(text_data)\n    vectorizer.set_params(stop_words=None)\n    vectorizer.set_params(vocabulary=None)\n    matrix = vectorizer.transform(text_data)\n    return vectorizer, matrix\n\n", "CodeBleu_score": 31.23351720025299}, {"code": "def _train_vectorizer(text_data: list, vectorizer)\n\tvectorizer.fit(text_data)\n    vectorizer.set_params(stop_words=None)\n    vectorizer_matrix = vectorizer.transform(text_data)\n    return vectorizer, vectorizer_matrix\n\n\n\n\n", "CodeBleu_score": 30.842720661798477}]}
{"instance_id": "be23c0afc2d6", "generation_result": [{"code": "def preprocess(df, test_size, shuffle)\n\t# remove rows with missing entries\n    df = df[df.apply(lambda x: x.str.contains(\"--\").sum() == 0)]\n\n    # drop columns with unique values less than or equal to one\n    df = df.loc[:, (df.nunique() > 1)]\n\n    # drop columns with \"id\" in their name\n    df = df.loc[:, ~df.columns.str.contains(\"id\")]\n\n    # drop columns with shortcuts like \"days_to_death\" and \"year_of_death\"\n    df = df.loc[:, ~df.columns.str.contains(\"days_to_death\")]\n    df = df.loc[:, ~df.columns.str.contains(\"year_of_death\")]\n\n    # split into training and testing sets\n    X_train, X_test = train_test_split(df, test_size=test_size, shuffle=shuffle)\n\n    return X_train, X_test", "CodeBleu_score": 25.345051516429397}, {"code": "def preprocess(df, test_size, shuffle)\n\t# Remove rows with missing entries\n    df = df.replace(\"--\", np.nan).dropna()\n    # Drop columns with unique values less than or equal to one or containing \"id\" in their name\n    df = df.drop([col for col in df.columns if df[col].nunique() <= 1 or \"id\" in col], axis=1)\n    # Drop shortcut columns\n    df = df.drop([\"days_to_death\", \"year_of_death\"], axis=1)\n    # Split into training and testing sets\n    train_df, test_df = train_test_split(df, test_size=test_size, shuffle=shuffle)\n    return train_df, test_df\n\n", "CodeBleu_score": 26.68265606261786}, {"code": "def preprocess(df, test_size, shuffle)\n\tdf = df.dropna(axis=0, how='any')\n    df = df.drop(df.columns[df.nunique() <= 1], axis=1)\n    df = df.drop(df.columns[df.columns.str.contains('id')], axis=1)\n    df = df.drop(['days_to_death', 'year_of_death'], axis=1)\n    X = df.drop(['death'], axis=1)\n    y = df['death']\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, shuffle=shuffle)\n    return X_train, X_test, y_train, y_test", "CodeBleu_score": 25.82346649993836}]}
{"instance_id": "6a82997a8c59", "generation_result": [{"code": "def _test_classifier_iterative_fit(classifier, dataset=\"iris\", sparse=False)\n\tX_train, Y_train, X_test, Y_test = get_dataset(dataset, sparse)\n\n    cs = classifier.get_hyperparameter_search_space()\n    default = cs.get_default_configuration()\n    classifier = classifier(random_state=1, **default)\n    classifier.iterative_fit(X_train, Y_train, n_iter=2, refit=True)\n    n_iter = 2\n    while not classifier.configuration_fully_fitted():\n        n_iter *= 2\n        classifier.iterative_fit(X_train, Y_train, n_iter=n_iter, refit=False)\n\n    predictions = classifier.predict(X_test)\n    return predictions, Y_test, classifier\n\n", "CodeBleu_score": 42.57426969995289}, {"code": "def _test_classifier_iterative_fit(classifier, dataset=\"iris\", sparse=False)\n\tX_train, Y_train, X_test, Y_test = get_dataset(\n        dataset=dataset, make_sparse=sparse, add_NaNs=sparse\n    )\n\n    cs = classifier.get_hyperparameter_search_space()\n    default = cs.get_default_configuration()\n\n    classifier = classifier(**default, random_state=1)\n    classifier.iterative_fit(X_train, Y_train, n_iter=2, refit=True)\n    n_iter = 2\n    while not classifier.configuration_fully_fitted():\n        n_iter *= 2\n        classifier.iterative_fit(X_train, Y_train, n_iter=n_iter, refit=False)\n\n    try:\n        Y_pred = classifier.predict(X_test)\n    except NotImplementedError:\n        Y_pred = None\n\n    return Y_pred, Y_test, classifier\n\n", "CodeBleu_score": 41.59021972858742}, {"code": "def _test_classifier_iterative_fit(classifier, dataset=\"iris\", sparse=False)\n\tX_train, Y_train, X_test, Y_test = get_dataset(dataset=dataset, make_sparse=sparse)\n    cs = classifier.get_hyperparameter_search_space()\n    default = cs.get_default_configuration()\n    classifier = classifier(**default)\n    classifier.fit(X_train, Y_train)\n    classifier.iterative_fit(X_train, Y_train, n_iter=2, refit=True)\n    while not classifier.configuration_fully_fitted():\n        n_iter = int(2 * classifier.iterative_fit_n_iter)\n        classifier.iterative_fit(X_train, Y_train, n_iter=n_iter)\n    predictions = classifier.predict(X_test)\n    return predictions, Y_test, classifier\n\n", "CodeBleu_score": 44.230519513366716}]}
{"instance_id": "d0d8450c4c17", "generation_result": [{"code": "def _load_models(self)\n\tif self._ensemble_class is not None:\n        self._ensemble = self._load_ensemble()\n        if self._ensemble is None:\n            self._ensemble = self._load_best_individual_model()\n\n    if self._ensemble is not None:\n        self._identifiers = self._ensemble.get_selected_model_identifiers()\n        self._models = self._load_models_by_identifiers(self._identifiers)\n        self._cv_models = self._load_cv_models_by_identifiers(self._identifiers)\n    else:\n        self._models = []\n        self._cv_models = []\n", "CodeBleu_score": 29.155287402560088}, {"code": "def _load_models(self)\n\t# Load the ensemble\n    self._load_ensemble()\n\n    # Load the best model if no ensemble is loaded\n    if not self.ensemble_:\n        self._load_best_individual_model()\n\n    # Get the model identifiers\n    identifiers = self.get_selected_model_identifiers()\n\n    # Load the models\n    self.models_ = self._load_models_by_identifiers(identifiers)\n\n    # Load the cross-validated models\n    if self.ensemble_ is not None and self.ensemble_.is_valid() and self.ensemble_.needs_cv_folds():\n        self.cv_models_ = self._load_cv_models_by_identifiers(identifiers)\n    else:\n        self.cv_models_ = []\n\n    # Check if all models are valid\n    if not self.is_valid():\n        self._logger.error(\n            \"Ensemble is not valid. Ensemble contains only invalid models. \"\n            \"Please check the log file for more information.\"\n        )\n        raise ValueError(\"Ensemble is not valid.\")\n\n    # Check if all models are loaded\n    if not self.is_all_models_loaded():\n        self._logger.error(\n            \"Ensemble is not valid. Ensemble contains only unloaded models. \"\n            \"Please check the log file for more information.\"\n        )\n        raise ValueError(\"Ensemble is not valid.\")\n\n    # Check if all cross-validated models are loaded\n    if not self.is_all_cv_models_loaded():\n        self._logger.error(\n            \"Ensemble is not valid. Ensemble contains only unloaded cross-validated models. \"\n            \"Please check the log file for more information.\"\n        )\n        raise ValueError(\"Ensemble is not valid.\")\n\n", "CodeBleu_score": 33.524680039591075}, {"code": "def _load_models(self)\n\tif self.ensemble_ is not None:\n        self.ensemble_ = self._load_ensemble(self.ensemble_)\n        self.identifiers_ = self.ensemble_.get_selected_model_identifiers()\n        self.weights_ = self.ensemble_.weights_\n        self.models_ = self._load_models_by_identifiers(self.identifiers_)\n        self.cv_models_ = self._load_cv_models_by_identifiers(self.identifiers_)\n    else:\n        self.ensemble_ = None\n        self.identifiers_ = []\n        self.weights_ = []\n        self.models_ = []\n        self.cv_models_ = []\n\n", "CodeBleu_score": 29.24971787183066}]}
{"instance_id": "ef71b1cddb50", "generation_result": [{"code": "def load_task(task_id)\n\ttask = openml.tasks.get_task(task_id)\n    dataset = task.get_dataset()\n    X, y, categorical_indicator, _ = dataset.get_data(\n        dataset_format=\"array\", target=task.target_name\n    )\n    X_train, X_test, y_train, y_test = task.get_train_test_split_indices(X, y)\n    y_train = np.array([task.class_labels[y] for y in y_train])\n    y_test = np.array([task.class_labels[y] for y in y_test])\n    return (\n        X_train,\n        X_test,\n        y_train,\n        y_test,\n        [cat == \"categorical\" for cat in categorical_indicator],\n    )", "CodeBleu_score": 22.534658148876957}, {"code": "def load_task(task_id)\n\ttask = openml.tasks.get_task(task_id)\n    dataset = task.get_dataset()\n    X, y, categorical_indicator, _ = dataset.get_data(\n        dataset_format=\"array\", target=task.target_name\n    )\n    X_train, X_test, y_train, y_test = task.get_train_test_split_indices(X, y)\n    y_train = np.array([task.target_name_map[y] for y in y_train])\n    y_test = np.array([task.target_name_map[y] for y in y_test])\n    return X_train, y_train, X_test, y_test, categorical_indicator", "CodeBleu_score": 22.24264131386801}, {"code": "def load_task(task_id)\n\ttask = openml.tasks.get_task(task_id)\n    dataset = task.get_dataset()\n    X, y, categorical_indicator, _ = dataset.get_data(\n        dataset_format=\"array\", target=task.target_name\n    )\n    X_train, X_test, y_train, y_test = task.get_train_test_split_indices(X, y)\n    y_train = np.array([np.unique(y).tolist().index(i) for i in y_train])\n    y_test = np.array([np.unique(y).tolist().index(i) for i in y_test])\n    return (\n        X_train,\n        y_train,\n        X_test,\n        y_test,\n        [True if i == \"categorical\" else False for i in categorical_indicator],\n    )", "CodeBleu_score": 26.4254608740017}]}
{"instance_id": "90bd6edbcc9e", "generation_result": [{"code": "def convert_if_sparse(\n    y: SUPPORTED_TARGET_TYPES,\n) -> Union[np.ndarray, List, pd.DataFrame, pd.Series]\n\tif issparse(y):\n        y = y.toarray()\n        if len(y.shape) == 2 and y.shape[1] == 1:\n            y = np.ravel(y)\n    return y", "CodeBleu_score": 51.68170775708071}, {"code": "def convert_if_sparse(\n    y: SUPPORTED_TARGET_TYPES,\n) -> Union[np.ndarray, List, pd.DataFrame, pd.Series]\n\tif issparse(y):\n        y = y.toarray()\n        if y.ndim == 1:\n            y = y.flatten()\n    return y", "CodeBleu_score": 51.028960355030314}, {"code": "def convert_if_sparse(\n    y: SUPPORTED_TARGET_TYPES,\n) -> Union[np.ndarray, List, pd.DataFrame, pd.Series]\n\tif isinstance(y, spmatrix):\n        y = y.toarray()\n        if y.ndim == 1:\n            y = y.flatten()\n        return y\n    else:\n        return y", "CodeBleu_score": 60.71171538802267}]}
{"instance_id": "5728e3d4f914", "generation_result": [{"code": "def _calculate_weights(self) -> None\n\t# Initialize weights array with zeros\n    weights = np.zeros(self.n_models)\n\n    # Count the occurrences of each ensemble member index\n    counter = Counter(self.ensemble_member_indices)\n\n    # Calculate the weight for each ensemble member\n    for ensemble_member_index in range(self.n_models):\n        weights[ensemble_member_index] = counter[ensemble_member_index] / self.n_models\n\n    # If the sum of weights is less than 1, normalize the weights\n    if np.sum(weights) < 1:\n        weights = weights / np.sum(weights)\n\n    # Store the calculated weights in the instance variable self.weights_\n    self.weights_ = weights", "CodeBleu_score": 43.3605895869903}, {"code": "def _calculate_weights(self) -> None\n\toccurrences = Counter(self.ensemble_members)\n    weights = np.zeros(self.num_models)\n    for index, count in occurrences.items():\n        weights[index] = count / self.ensemble_size\n    if np.sum(weights) < 1:\n        weights = weights / np.sum(weights)\n    self.weights_ = weights", "CodeBleu_score": 40.00163895809341}, {"code": "def _calculate_weights(self) -> None\n\toccurrences = Counter(self.ensemble_members)\n    weights = np.zeros(len(self.input_models))\n    for key, value in occurrences.items():\n        weights[key] = value / len(self.ensemble_members)\n    if np.sum(weights) < 1:\n        weights = weights / np.sum(weights)\n    self.weights_ = weights", "CodeBleu_score": 40.93850216713886}]}
{"instance_id": "4f9c4799fd8f", "generation_result": [{"code": "def predict_proba(self, X, batch_size=None, n_jobs=1)\n\t# Call the superclass's predict_proba method to obtain the predicted probabilities\n    probas = super().predict_proba(X, batch_size=batch_size, n_jobs=n_jobs)\n\n    # If the target type is not \"multilabel-indicator\", assert that the sum of probabilities for each instance is 1\n    if self.target_type != MULTILABEL_INDICATOR:\n        assert np.allclose(probas.sum(axis=1), 1), \"Probabilities do not sum to 1\"\n\n    # Assert that all probability values lie between 0 and 1\n    assert (probas >= 0).all() and (probas <= 1).all(), \"Probabilities are not between 0 and 1\"\n\n    # Return the predicted probabilities\n    return probas\n\n", "CodeBleu_score": 40.729789051675006}, {"code": "def predict_proba(self, X, batch_size=None, n_jobs=1)\n\t# Check that the target type is \"multilabel-indicator\"\n    if self.target_type_ != \"multilabel-indicator\":\n        raise NotImplementedError(\n            \"predict_proba is only implemented for the \"\n            \"multilabel-indicator target type\"\n        )\n\n    # Predict the probabilities\n    probas = super().predict_proba(X, batch_size=batch_size, n_jobs=n_jobs)\n\n    # Assert that the sum of probabilities for each instance is 1\n    if not np.allclose(probas.sum(axis=1), np.ones_like(probas.sum(axis=1))):\n        raise AssertionError(\n            \"The sum of the predicted probabilities for each instance is not 1\"\n        )\n\n    # Assert that all probability values lie between 0 and 1\n    if not np.all((probas >= 0) & (probas <= 1)):\n        raise AssertionError(\n            \"Some predicted probabilities are not between 0 and 1\"\n        )\n\n    return probas", "CodeBleu_score": 50.148314555751405}, {"code": "def predict_proba(self, X, batch_size=None, n_jobs=1)\n\tprobas = super().predict_proba(X, batch_size=batch_size, n_jobs=n_jobs)\n    if self.automl_.target_type != \"multilabel-indicator\":\n        assert np.allclose(probas.sum(axis=1), 1)\n    assert (probas >= 0).all() and (probas <= 1).all()\n    return probas", "CodeBleu_score": 35.45635885456486}]}
{"instance_id": "a9a13150aebe", "generation_result": [{"code": "def convert_to_num(Ybin: np.ndarray) -> np.ndarray:\n\tif len(Ybin.shape) == 1:\n        return Ybin\n    else:\n        return np.dot(Ybin, range(Ybin.shape[1]))", "CodeBleu_score": 34.975079710234944}, {"code": "def convert_to_num(Ybin: np.ndarray) -> np.ndarray:\n\tif len(Ybin.shape) == 1:\n        return Ybin\n    else:\n        return np.dot(Ybin, range(Ybin.shape[1]))\n\n", "CodeBleu_score": 34.975079710234944}, {"code": "def convert_to_num(Ybin: np.ndarray) -> np.ndarray:\n\tif Ybin.ndim == 1:\n        return Ybin\n    else:\n        return np.dot(Ybin, range(Ybin.shape[1]))\n\n\n\n\n", "CodeBleu_score": 29.611555676167068}]}
{"instance_id": "e97d38e62f81", "generation_result": [{"code": "def fit_transformer(self, X, y, fit_params=None)\n\tif fit_params is None:\n        fit_params = {}\n\n    if self.config[\"balancing:strategy\"] == \"weighting\":\n        balancing = Balancing(strategy=\"weighting\")\n        _init_params, _fit_params = balancing.get_weights(\n            y,\n            self.config[\"classifier:__choice__\"],\n            self.config[\"feature_preprocessor:__choice__\"],\n            {},\n            {},\n        )\n        _init_params.update(self.init_params)\n        self.set_hyperparameters(\n            feat_type=self.feat_type,\n            configuration=self.config,\n            init_params=_init_params,\n        )\n\n        if _fit_params is not None:\n            fit_params.update(_fit_params)\n\n    X, fit_params = super().fit_transformer(X, y, fit_params=fit_params)\n\n    return X, fit_params\n\n", "CodeBleu_score": 97.87308038655074}, {"code": "def fit_transformer(self, X, y, fit_params=None)\n\tif fit_params is None:\n        fit_params = {}\n\n    if self.config[\"balancing:strategy\"] == \"weighting\":\n        balancing = Balancing(strategy=\"weighting\")\n        _init_params, _fit_params = balancing.get_weights(\n            y,\n            self.config[\"classifier:__choice__\"],\n            self.config[\"feature_preprocessor:__choice__\"],\n            {},\n            {},\n        )\n        _init_params.update(self.init_params)\n        self.set_hyperparameters(\n            feat_type=self.feat_type,\n            configuration=self.config,\n            init_params=_init_params,\n        )\n\n        if _fit_params is not None:\n            fit_params.update(_fit_params)\n\n    X, fit_params = super().fit_transformer(X, y, fit_params=fit_params)\n\n    return X, fit_params\n\n", "CodeBleu_score": 97.87308038655074}, {"code": "def fit_transformer(self, X, y, fit_params=None)\n\tif fit_params is None:\n        fit_params = {}\n\n    if self.config[\"balancing:strategy\"] == \"weighting\":\n        balancing = Balancing(strategy=\"weighting\")\n        _init_params, _fit_params = balancing.get_weights(\n            y,\n            self.config[\"classifier:__choice__\"],\n            self.config[\"feature_preprocessor:__choice__\"],\n            {},\n            {},\n        )\n        _init_params.update(self.init_params)\n        self.set_hyperparameters(\n            feat_type=self.feat_type,\n            configuration=self.config,\n            init_params=_init_params,\n        )\n\n        if _fit_params is not None:\n            fit_params.update(_fit_params)\n\n    X, fit_params = super().fit_transformer(X, y, fit_params=fit_params)\n\n    return X, fit_params\n\n", "CodeBleu_score": 97.87308038655074}]}
{"instance_id": "b1aaad80a9c4", "generation_result": [{"code": "def test_glm_identity_link_func()\n\tdata = pd.DataFrame({'x': [1, 2, 3, 4, 5], 'y': [2, 4, 6, 8, 10]})\n    model = sm.GLM(data['y'], data[['x']], family=sm.families.Tweedie(var_power=1))\n    model = model.fit()\n    assembler = assemblers.StatsmodelsModelAssemblerSelector(model)\n    assembler.assemble()\n    expected_expr = ast.BinNumExpr(\n        ast.FeatureRef(0),\n        ast.NumVal(2.0),\n        ast.BinNumOpType.ADD\n    )\n    utils.assert_equal_dags(assembler.out_model.expr, expected_expr)\n\n", "CodeBleu_score": 36.24293921573455}, {"code": "def test_glm_identity_link_func()\n\t# Import the necessary libraries\n    import numpy as np\n    import pandas as pd\n    from sklearn.datasets import load_boston\n    from sklearn.model_selection import train_test_split\n    from pyspark.ml.feature import VectorAssembler\n    from pyspark.ml import Pipeline\n    from pyspark.ml.regression import GeneralizedLinearRegression\n    from pyspark.sql.types import DoubleType\n    from pyspark.sql import SparkSession\n    from pyspark.sql.functions import col\n    import pyspark.sql.functions as F\n    from pyspark.sql.dataframe import DataFrame\n    from pyspark.ml.linalg import Vectors\n    from pyspark.ml.feature import VectorAssembler\n    from pyspark.ml.regression import GeneralizedLinearRegression\n    import pyspark.sql.functions as F\n    import pyspark.sql.types as T\n    from pyspark.sql.types import DoubleType\n    from pyspark.sql import SparkSession\n    from pyspark.sql.functions import col\n    import pyspark.sql.functions as F\n    from pyspark.sql.dataframe import DataFrame\n    from pyspark.ml.linalg import Vectors\n    from pyspark.ml.feature import VectorAssembler\n    from pyspark.ml.regression import GeneralizedLinearRegression\n    import pyspark.sql.functions as F\n    import pyspark.sql.types as T\n    from pyspark.sql.types import DoubleType\n    from pyspark.sql import SparkSession\n    from pyspark.sql.functions import col\n    import pyspark.sql.functions as F\n    from pyspark.sql.dataframe import DataFrame\n    from pyspark.ml.linalg import Vectors\n    from pyspark.ml.feature import VectorAssembler\n    from pyspark.ml.regression import GeneralizedLinearRegression\n    import pyspark.sql.functions as F\n    import pyspark.sql.types as T\n    from pyspark.sql.types import DoubleType\n    from pyspark.sql import Spark", "CodeBleu_score": 8.353760038148808}, {"code": "def test_glm_identity_link_func()\n\tdata = pd.DataFrame({\n        \"y\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15],\n        \"x1\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15],\n        \"x2\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15],\n        \"x3\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15],\n        \"x4\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15],\n        \"x5\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15],\n        \"x6\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15],\n        \"x7\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15],\n        \"x8\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,", "CodeBleu_score": 4.416108655352118}]}
{"instance_id": "bcab63e592d0", "generation_result": [{"code": "def _compute_aggregation(\n        self, col: Variable, agg: Union[str, Callable, Tuple[str, Union[str, Callable]]]\n    ) -> pd.Series\n\t# Handle named aggregations\n    if isinstance(agg, tuple):\n        agg_name, agg_func = agg\n    else:\n        agg_name = agg\n        agg_func = agg\n\n    # Handle string aggregations\n    if isinstance(agg_func, str):\n        agg_func = getattr(pd.Series, agg_func)\n\n    # Construct a new column name\n    new_col_name = f\"{col.name}_{agg_name}\"\n\n    # Apply the aggregation\n    agg_result = agg_func(col)\n\n    # Rename the aggregated column\n    agg_result.name = new_col_name\n\n    return agg_result", "CodeBleu_score": 48.978314778444734}, {"code": "def _compute_aggregation(\n        self, col: Variable, agg: Union[str, Callable, Tuple[str, Union[str, Callable]]]\n    ) -> pd.Series\n\tif isinstance(agg, str):\n        name = agg\n        func = getattr(col, agg)\n    elif callable(agg):\n        name = agg.__name__\n        func = agg\n    elif isinstance(agg, tuple):\n        name, func = agg\n    else:\n        raise ValueError(f\"Invalid aggregation: {agg}\")\n    return col.groupby(self.group_by).agg(func).rename(f\"{col.name}_{name}\")\n\n", "CodeBleu_score": 48.69435340116003}, {"code": "def _compute_aggregation(\n        self, col: Variable, agg: Union[str, Callable, Tuple[str, Union[str, Callable]]]\n    ) -> pd.Series\n\tif isinstance(agg, str):\n        agg_name = agg\n        agg_func = agg\n    elif callable(agg):\n        agg_name = agg.__name__\n        agg_func = agg\n    elif isinstance(agg, tuple):\n        agg_name = agg[0]\n        agg_func = agg[1]\n        if callable(agg_func):\n            agg_name = agg_func.__name__\n        elif isinstance(agg_func, str):\n            agg_name = agg_func\n        else:\n            raise ValueError(\n                f\"Aggregation function must be a string or callable, not {type(agg_func)}\"\n            )\n    else:\n        raise ValueError(\n            f\"Aggregation must be a string, callable, or tuple, not {type(agg)}\"\n        )\n\n    new_col_name = f\"{col.name}_{agg_name}\"\n    return self.agg({new_col_name: agg_func})[new_col_name]\n\n", "CodeBleu_score": 38.943528715700594}]}
{"instance_id": "114d623dae7b", "generation_result": [{"code": "def _corrfunc(self, x, y, **kws)\n\t# Get the correlation coefficient and adjust the font size\n    r, p = stats.pearsonr(x, y)\n    fontsize = np.ceil(min([max([r, -1]), 1]) * 100) + 10\n    # Get the current axis\n    ax = plt.gca()\n    # Get the hue group index\n    try:\n        hue_index = kws['hue_index']\n    except KeyError:\n        hue_index = 0\n    # Get the hue group count\n    try:\n        hue_count = kws['hue_count']\n    except KeyError:\n        hue_count = 1\n    # Get the x and y positions\n    try:\n        xpos = kws['xpos']\n    except KeyError:\n        xpos = 0.5\n    try:\n        ypos = kws['ypos']\n    except KeyError:\n        ypos = 0.5\n    # Get the hue group position\n    hue_pos = hue_index / (hue_count - 1)\n    # Get the x and y axes limits\n    xlim = ax.get_xlim()\n    ylim = ax.get_ylim()\n    # Get the x and y axis lengths\n    xlen = abs(xlim[1] - xlim[0])\n    ylen = abs(ylim[1] - ylim[0])\n    # Get the x and y axis labels\n    xlabel = ax.get_xlabel()\n    ylabel = ax.get_ylabel()\n    # Get the x and y axis label lengths\n    xlablen = len(xlabel)\n    ylablen = len(ylabel)\n    # Get the x and y axis label positions\n    xlabpos = ax.xaxis.label.get_position()[0]\n    ylabpos = ax.yaxis.label.get_position()[1]\n    # Get the x and y axis label sizes\n    xlabsize = ax.xaxis.label.get_size()\n    ylabsize = ax.yaxis.label.get_size()\n    # Get the x and y", "CodeBleu_score": 33.14663799251573}, {"code": "def _corrfunc(self, x, y, **kws)\n\t# Handle optional hue groupings\n    if 'hue' in kws:\n        hue = kws.pop('hue')\n        # Handle hue groups\n        hue_groups = np.unique(hue)\n        # Compute the correlation coefficient for each hue group\n        hue_corrs = []\n        for hue_group in hue_groups:\n            # Mask out NaNs\n            mask = np.logical_or(np.isnan(x), np.isnan(y))\n            mask = np.logical_or(mask, hue != hue_group)\n            # Compute the correlation coefficient\n            corr = stats.pearsonr(x[~mask], y[~mask])[0]\n            hue_corrs.append(corr)\n        # Compute the average correlation coefficient\n        corr = np.mean(hue_corrs)\n        # Compute the font size based on the number of hue groups\n        size = min(24, 16 + 4 * len(hue_groups))\n    else:\n        # Mask out NaNs\n        mask = np.logical_or(np.isnan(x), np.isnan(y))\n        # Compute the correlation coefficient\n        corr = stats.pearsonr(x[~mask], y[~mask])[0]\n        # Compute the font size based on the number of hue groups\n        size = 16\n    # Compute the adjusted font size\n    size = np.ceil(size * abs(corr))\n    # Get the figure and axes\n    fig = plt.gcf()\n    ax = plt.gca()\n    # Get the axes position\n    position = ax.get_position()\n    # Compute the new axes position\n    new_position = [position.x0, position.y0,\n                    position.width, position.height * 0.9]\n    # Set the new axes position\n    ax.set_position(new_position)\n    # Set the new axis title\n    ax.set_title(f'{kws.get(\"title\", \"\")}", "CodeBleu_score": 34.25007720663267}, {"code": "def _corrfunc(self, x, y, **kws)\n\t# Calculate the correlation coefficient\n    r = stats.pearsonr(x, y)[0]\n    # Format the correlation coefficient\n    r_text = \"r = {:.2f}\".format(r)\n    # Calculate the number of hue groups\n    hue_groups = len(np.unique(kws['hue']))\n    # Calculate the font size\n    font_size = np.ceil(10 * hue_groups / 3)\n    # Get the current axis\n    ax = plt.gca()\n    # Annotate the axis\n    ax.annotate(r_text, xy=(0.5, 0.5), xycoords='axes fraction', fontsize=font_size)", "CodeBleu_score": 15.837910414969315}]}
{"instance_id": "4c1ade308953", "generation_result": [{"code": "def _round_digits(src: float, rounddigit: int = None, method='decimal')\n\tif rounddigit is None:\n        rounddigit = 2\n    if method == 'decimal':\n        return round(src, rounddigit)\n    elif method == 'sig':\n        with decimal.localcontext() as ctx:\n            ctx.create_decimal(10)\n            return ctx.create_decimal(src).quantize(\n                decimal.Decimal(10) ** (-rounddigit))\n    elif method == 'format':\n        return float(format(src, f'.{rounddigit}g'))", "CodeBleu_score": 64.249874755633}, {"code": "def _round_digits(src: float, rounddigit: int = None, method='decimal')\n\tif method == 'decimal':\n        return round(src, rounddigit)\n    elif method == 'sig':\n        with decimal.localcontext() as ctx:\n            ctx.prec = rounddigit\n            return ctx.create_decimal(src)\n    elif method == 'format':\n        return float(format(src, f'.{rounddigit}g'))\n    else:\n        raise ValueError('Method must be one of \"decimal\", \"sig\", or \"format\".')", "CodeBleu_score": 78.38405486192109}, {"code": "def _round_digits(src: float, rounddigit: int = None, method='decimal')\n\tif rounddigit is None:\n        return src\n    if method == 'decimal':\n        return round(src, rounddigit)\n    elif method == 'sig':\n        with decimal.localcontext() as ctx:\n            ctx.prec = rounddigit\n            return ctx.create_decimal(src)\n    elif method == 'format':\n        return float(format(src, f'.{rounddigit}g'))\n    else:\n        raise ValueError(f\"Method must be one of 'decimal', 'sig', or 'format', not {method}\")", "CodeBleu_score": 77.64556211973871}]}
{"instance_id": "a9923e678320", "generation_result": [{"code": "def _round_dict_digits(cls, srcdict: Dict[str, float], rounddigit: int = None, method='decimal')\n\tdstdict = {}\n    for k, v in srcdict.items():\n        if rounddigit is not None and isinstance(v, float):\n            dstdict[k] = _round_digits(v, rounddigit=rounddigit, method=method)\n        else:\n            dstdict[k] = v\n    return dstdict\n\n", "CodeBleu_score": 74.53306748512102}, {"code": "def _round_dict_digits(cls, srcdict: Dict[str, float], rounddigit: int = None, method='decimal')\n\tdstdict = {}\n    for k, v in srcdict.items():\n        if rounddigit is not None and isinstance(v, float):\n            dstdict[k] = _round_digits(v, rounddigit=rounddigit, method=method)\n        else:\n            dstdict[k] = v\n    return dstdict\n\n", "CodeBleu_score": 74.53306748512102}, {"code": "def _round_dict_digits(cls, srcdict: Dict[str, float], rounddigit: int = None, method='decimal')\n\tdstdict = {}\n    for k, v in srcdict.items():\n        if rounddigit is not None and isinstance(v, float):\n            dstdict[k] = _round_digits(v, rounddigit=rounddigit, method=method)\n        else:\n            dstdict[k] = v\n    return dstdict\n\n", "CodeBleu_score": 74.53306748512102}]}
{"instance_id": "b6106e079dc0", "generation_result": [{"code": "def _scatterplot_ndarray(cls, x, x_name, y, y_name, hue_data, hue_name, ax, scatter_kws, legend_kws)\n\tif hue_data is None:\n        data = pd.DataFrame({x_name: x, y_name: y})\n    else:\n        data = pd.DataFrame({x_name: x, y_name: y, hue_name: hue_data})\n    if ax is None:\n        ax = plt.gca()\n    cls.scatterplot(x=x_name, y=y_name, hue=hue_name, data=data, ax=ax, **scatter_kws)\n    if hue_data is not None:\n        if legend_kws is None:\n            legend_kws = {}\n        if \"title\" not in legend_kws:\n            legend_kws[\"title\"] = hue_name\n        ax.legend(**legend_kws)\n\n", "CodeBleu_score": 49.763800612222916}, {"code": "def _scatterplot_ndarray(cls, x, x_name, y, y_name, hue_data, hue_name, ax, scatter_kws, legend_kws)\n\t# Combine x and y into a DataFrame\n    data = pd.DataFrame({x_name: x, y_name: y})\n\n    # If hue_data is provided, add a hue field to the DataFrame\n    if hue_data is not None:\n        data[hue_name] = hue_data\n\n    # If a legend title is not specified, set it to the hue field name\n    if legend_kws is not None and 'title' not in legend_kws.keys():\n        legend_kws['title'] = hue_name\n\n    # Plot the scatter plot\n    cls.scatterplot(data=data, x=x_name, y=y_name, hue=hue_name, ax=ax, **scatter_kws)\n\n    # Add a legend\n    if hue_data is not None:\n        cls.legend(**legend_kws)\n\n", "CodeBleu_score": 41.97394833918279}, {"code": "def _scatterplot_ndarray(cls, x, x_name, y, y_name, hue_data, hue_name, ax, scatter_kws, legend_kws)\n\t# Combine the x and y values into a DataFrame with specified column names\n    df = pd.DataFrame({x_name: x, y_name: y})\n\n    # If hue data is provided, add a hue field to the DataFrame for color-coding the points\n    if hue_data is not None:\n        df[hue_name] = hue_data\n\n    # Plot the scatter plot using Seaborn's scatterplot function, applying any additional keyword arguments for the plot and legend\n    cls.scatterplot(x=x_name, y=y_name, hue=hue_name, data=df, ax=ax, **scatter_kws)\n\n    # If no title is specified for the legend, set the title to the hue field name\n    if legend_kws.get(\"title\") is None and hue_name is not None:\n        legend_kws[\"title\"] = hue_name\n\n    # Add a legend to the plot\n    ax.legend(**legend_kws)\n\n    # Return the plot\n    return ax\n\n", "CodeBleu_score": 35.00915306051963}]}
{"instance_id": "257e0aa5dd4d", "generation_result": [{"code": "def fit(self, **fit_args) -> \"TimeSeriesModel\"\n\tas_df = self.node.item\n    end = self._get_transformed_data(as_series=True)\n    if self.node.exogenous:\n        ex = as_df[self.node.exogenous]\n    else:\n        ex = None\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\"ignore\", category=UserWarning)\n        warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n        self.model = self.model.fit(y=end, X=ex, **fit_args)\n    return self\n\n", "CodeBleu_score": 81.38523406593907}, {"code": "def fit(self, **fit_args) -> \"TimeSeriesModel\"\n\tas_df = self.node.item\n    end = self._get_transformed_data(as_series=True)\n    if self.node.exogenous:\n        ex = as_df[self.node.exogenous]\n    else:\n        ex = None\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\"ignore\", category=UserWarning)\n        warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n        self.model = self.model.fit(y=end, X=ex, **fit_args)\n    return self", "CodeBleu_score": 81.38523406593907}, {"code": "def fit(self, **fit_args) -> \"TimeSeriesModel\"\n\tas_df = self.node.item\n    end = self._get_transformed_data(as_series=True)\n    if self.node.exogenous:\n        ex = as_df[self.node.exogenous]\n    else:\n        ex = None\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\"ignore\", category=UserWarning)\n        warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n        self.model = self.model.fit(y=end, X=ex, **fit_args)\n    return self\n\n", "CodeBleu_score": 81.38523406593907}]}
{"instance_id": "6446de2223e8", "generation_result": [{"code": "def create_model(self, **kwargs)\n\tif self.model_type == \"holt_winters\":\n        transformed_data = self._get_transformed_data(as_series=True)\n        model = ExponentialSmoothing(transformed_data, **kwargs)\n        return model\n    elif self.model_type == \"auto_arima\":\n        try:\n            from pmdarima import AutoARIMA\n        except ModuleNotFoundError:\n            logger.error(\n                \"pmdarima is not installed. Please install pmdarima and try again.\"\n            )\n            raise\n        transformed_data = self._get_transformed_data()\n        model = AutoARIMA(**kwargs)\n        model.fit(transformed_data)\n        return model\n    elif self.model_type == \"sarimax\":\n        transformed_data = self._get_transformed_data()\n        if self.exogenous_variables is not None:\n            exogenous_data = self.exogenous_variables.to_pandas()\n            model = SARIMAX(\n                transformed_data,\n                exogenous=exogenous_data,\n                enforce_invertibility=False,\n                **kwargs,\n            )\n        else:\n            model = SARIMAX(transformed_data, enforce_invertibility=False, **kwargs)\n        return model\n    else:\n        raise ValueError(f\"Model type '{self.model_type}' not recognized.\")\n\n", "CodeBleu_score": 48.258229721956816}, {"code": "def create_model(self, **kwargs)\n\tif self.model_type == \"holt_winters\":\n        transformed_data = self.transform_function.transform(self.data)\n        return ExponentialSmoothing(transformed_data, **kwargs)\n    elif self.model_type == \"auto_arima\":\n        try:\n            from pmdarima import AutoARIMA\n        except ModuleNotFoundError:\n            logger.error(\n                \"pmdarima not installed. Please install it to use AutoARIMA.\"\n            )\n            raise\n        else:\n            return AutoARIMA(**kwargs)\n    elif self.model_type == \"sarimax\":\n        transformed_data = self.transform_function.transform(self.data)\n        exogenous_variables = self.exogenous_variables\n        if exogenous_variables is not None:\n            exogenous_variables = self.transform_function.transform(\n                exogenous_variables\n            )\n        return SARIMAX(transformed_data, exogenous_variables, **kwargs)\n    else:\n        raise ValueError(\n            f\"Model type {self.model_type} is not supported. Please choose from 'holt_winters', 'auto_arima', or 'sarimax'.\"\n        )\n\n", "CodeBleu_score": 38.13879412048619}, {"code": "def create_model(self, **kwargs)\n\tmodel_type = self.model_type\n    if model_type == \"holt_winters\":\n        transformed_data = self.transform_data(as_series=True)\n        model = ExponentialSmoothing(transformed_data)\n        model.fit()\n    elif model_type == \"auto_arima\":\n        try:\n            from pmdarima import AutoARIMA\n        except ModuleNotFoundError:\n            logger.error(\n                \"pmdarima is not installed. Please install it using 'pip install pmdarima'.\"\n            )\n            sys.exit()\n        transformed_data = self.transform_data(as_series=True)\n        model = AutoARIMA(transformed_data, **kwargs)\n        model.fit()\n    elif model_type == \"sarimax\":\n        transformed_data = self.transform_data(as_series=True)\n        exogenous_variables = self.get_exogenous_variables()\n        if exogenous_variables is not None:\n            model = SARIMAX(\n                transformed_data, exogenous=exogenous_variables, **kwargs\n            )\n        else:\n            model = SARIMAX(transformed_data, **kwargs)\n        model.fit()\n    else:\n        raise Exception(f\"{model_type} is not a valid model type.\")\n    return model\n\n", "CodeBleu_score": 42.06371068179919}]}
{"instance_id": "b2f67820e27f", "generation_result": [{"code": "def _to_chx(array)\n\tif isinstance(array, numpy.ndarray):\n        return _from_numpy(array)\n    elif cupy is not None and isinstance(array, cupy.ndarray):\n        return _from_cupy(array)\n    else:\n        return array\n\n", "CodeBleu_score": 89.76276169788974}, {"code": "def _to_chx(array)\n\tif isinstance(array, numpy.ndarray):\n        return _from_numpy(array)\n    elif cupy is not None and isinstance(array, cupy.ndarray):\n        return _from_cupy(array)\n    else:\n        return array\n\n", "CodeBleu_score": 89.76276169788974}, {"code": "def _to_chx(array)\n\tif isinstance(array, numpy.ndarray):\n        return _from_numpy(array)\n    if cupy is not None and isinstance(array, cupy.ndarray):\n        return _from_cupy(array)\n    return array\n\n", "CodeBleu_score": 87.97048848572042}]}
{"instance_id": "b96f7d9bcc95", "generation_result": [{"code": "def _fix_numpy_split(ys, x, indices_or_sections, axis)\n\tif not isinstance(x, np.ndarray):\n        return ys\n    if axis < 0:\n        axis += len(x.shape)\n    if len(x.shape) != len(ys[0].shape):\n        shape = list(x.shape)\n        shape[axis] = 0\n        tmp = np.empty(shape)\n        tmp_split = np.split(tmp, indices_or_sections, axis)\n        expected_sizes = [len(x) for x in tmp_split]\n        for i, y in enumerate(ys):\n            if len(y.shape) != len(x.shape):\n                ys[i] = y.reshape(expected_sizes[i] + [-1] + list(y.shape)[len(y.shape) - axis + 1:])\n    return ys", "CodeBleu_score": 38.79761990726656}, {"code": "def _fix_numpy_split(ys, x, indices_or_sections, axis)\n\tif np.__version__ >= '1.11.0':\n        return np.split(x, indices_or_sections, axis)\n    else:\n        # Calculate the expected shapes of the split segments using a temporary split on an empty array with the same shape as x along the specified axis\n        expected_shapes = np.split(np.empty(x.shape), indices_or_sections, axis)[1:]\n        # For each segment, if the dimensions do not match, reshape the segment to the expected shape\n        for i, (y, expected_shape) in enumerate(zip(ys, expected_shapes)):\n            if y.ndim != expected_shape.ndim:\n                ys[i] = y.reshape(expected_shape.shape)\n        return ys\n\n", "CodeBleu_score": 33.46215998133902}, {"code": "def _fix_numpy_split(ys, x, indices_or_sections, axis)\n\tif len(ys) == 0:\n        return ys\n    if len(ys[0].shape) == len(x.shape):\n        return ys\n    # Calculate the expected shapes of the split segments\n    indices_or_sections = _fix_numpy_split_indices(x, indices_or_sections, axis)\n    shapes = np.empty(len(indices_or_sections) - 1, dtype=np.intp)\n    shapes[:] = np.diff(indices_or_sections)\n    # Reshape the split segments if necessary\n    for i, y in enumerate(ys):\n        if y.shape != tuple(shapes[i]):\n            ys[i] = y.reshape(tuple(shapes[i]))\n    return ys\n\n", "CodeBleu_score": 38.30475752844685}]}
{"instance_id": "cf39fb123842", "generation_result": [{"code": "def to_device_batch(batch):\n\tif device is None:\n        return batch\n    elif device < 0:\n        return [to_device(device, x) for x in batch]\n    else:\n        xp = get_array_module(batch[0])\n        concat = xp.concatenate(batch, axis=0)\n        sections = [len(x) for x in batch]\n        split = xp.cumsum(xp.array(sections[:-1]))\n        return xp.split(to_device(device, concat), split)\n\n", "CodeBleu_score": 55.08337226334588}, {"code": "def to_device_batch(batch):\n\tif device is None:\n        return batch\n    elif device < 0:\n        return [to_device(device, b) for b in batch]\n    else:\n        xp = get_array_module(batch[0])\n        b_concat = xp.concatenate(batch, axis=0)\n        b_dev = to_device(device, b_concat)\n        sections = [len(b) for b in batch]\n        return xp.split(b_dev, np.cumsum(sections)[:-1])", "CodeBleu_score": 46.26333367298864}, {"code": "def to_device_batch(batch):\n\tif device is None:\n        return batch\n    elif device < 0:\n        return [to_device(device, x) for x in batch]\n    else:\n        xp = get_array_module(batch[0])\n        concat = xp.concatenate(batch, axis=0)\n        sections = [len(x) for x in batch]\n        split = xp.cumsum(xp.array(sections[:-1]))\n        return xp.split(to_device(device, concat), split)", "CodeBleu_score": 55.08337226334588}]}
{"instance_id": "845c744d897c", "generation_result": [{"code": "def make_image(trainer)\n\t# Set a random seed\n    seed = 123\n    xp = trainer.updater.get_optimizer('main').target.xp\n    xp.random.seed(seed)\n    # Determine the number of images to generate\n    n_images = 16\n    # Create a latent variable z\n    z = Variable(xp.random.normal(size=(n_images, 100)).astype(xp.float32))\n    # Generate images\n    with chainer.using_config('train', False):\n        x = trainer.updater.get_optimizer('main').target(z)\n    # Move the images to the CPU\n    x = to_cpu(x)\n    # Scale the images to a [0, 255] range\n    x = 127.5 * (x + 1.0)\n    # Convert the images to uint8 type\n    x = x.astype(xp.uint8)\n    # Reshape and transpose the images\n    x = x.reshape((n_images, 3, 64, 64))\n    x = x.transpose(0, 2, 3, 1)\n    # Create a directory for saving the preview image if it doesn't exist\n    preview_dir = os.path.join(trainer.out, 'preview')\n    if not os.path.exists(preview_dir):\n        os.makedirs(preview_dir)\n    # Save the generated image as a PNG file\n    preview_file = os.path.join(preview_dir,\n                                'iteration-{0:07d}.png'.format(trainer.updater.iteration))\n    Image.fromarray(x).save(preview_file, format='PNG')", "CodeBleu_score": 47.74678108765649}, {"code": "def make_image(trainer)\n\t# Set a random seed\n    seed = 123\n    np.random.seed(seed)\n\n    # Determine the number of images to generate\n    n_images = 10\n\n    # Create a latent variable z\n    z = Variable(np.random.randn(n_images, 100).astype(np.float32))\n\n    # Generate images\n    with chainer.using_config('train', False):\n        x = trainer.updater.gen(z)\n\n    # Move the images to the CPU\n    x = x.reshape((n_images, 3, 64, 64)).transpose((0, 2, 3, 1))\n    x = xp.asarray(x)\n\n    # Scale images to [0, 255] range and convert to uint8\n    x = np.clip(x, 0, 1)\n    x = (x * 255).astype(np.uint8)\n\n    # Create a directory for saving the preview image if it doesn't exist\n    preview_dir = 'preview'\n    if not exists(preview_dir):\n        os.makedirs(preview_dir)\n\n    # Save the generated image as a PNG file\n    filename = preview_dir + '/iteration-{}.png'.format(trainer.updater.iteration)\n    Image.fromarray(x).save(filename, format='PNG')", "CodeBleu_score": 43.0966418573145}, {"code": "def make_image(trainer)\n\t# Set a random seed\n    seed(0)\n\n    # Determine the number of images to generate\n    n_images = 4\n\n    # Create a latent variable z\n    z = Variable(xp.asarray(np.random.randn(n_images, 100)))\n\n    # Generate images using a generator model\n    # with the training mode turned off\n    with chainer.using_config('train', False):\n        x = gen(z)\n\n    # Move the generated images to the CPU\n    x = x.array\n\n    # Scale the images to a [0, 255] range and\n    # convert them to uint8 type\n    x = np.clip(x, 0, 1)\n    x = (x * 255).astype(np.uint8)\n\n    # Reshape and transpose the images\n    # to form a grid of images\n    x = x.reshape(n_images, 64, 64)\n    x = x.transpose(0, 2, 1)\n\n    # Create a directory for saving the preview image\n    # if it doesn't exist\n    os.makedirs('preview', exist_ok=True)\n\n    # Save the generated image as a PNG file\n    # named according to the current training iteration\n    Image.fromarray(x).save('preview/{}.png'.format(trainer.updater.iteration))", "CodeBleu_score": 34.04268951763001}]}
{"instance_id": "fc2162d43aee", "generation_result": [{"code": "def _cuda_can_cast(from_dtype: npt.DTypeLike, to_dtype: npt.DTypeLike) -> bool\n\tfrom_dtype = np.dtype(from_dtype)\n    to_dtype = np.dtype(to_dtype)\n    if from_dtype == to_dtype:\n        return True\n    from_type = from_dtype.type\n    to_type = to_dtype.type\n    if from_type == np.bool_:\n        if to_type in (np.bool_, np.uint8, np.int8, np.int32, np.int64):\n            return True\n        return False\n    if from_type == np.uint8:\n        if to_type in (np.uint8, np.int8, np.int32, np.int64):\n            return True\n        return False\n    if from_type == np.int8:\n        if to_type in (np.int8, np.int32, np.int64):\n            return True\n        return False\n    if from_type == np.int32:\n        if to_type in (np.int32, np.int64):\n            return True\n        return False\n    if from_type == np.int64:\n        if to_type in (np.int64,):\n            return True\n        return False\n    if from_type == np.float16:\n        if to_type in (np.float16, np.float32, np.float64):\n            return True\n        return False\n    if from_type == np.float32:\n        if to_type in (np.float32, np.float64):\n            return True\n        return False\n    if from_type == np.float64:\n        if to_type in (np.float64,):\n            return True\n        return False\n    if from_type == np.complex64:\n        if to_type in (np.complex64, np.complex128):\n            return True\n        return False\n    if from_type == np.complex128:\n        if to_type in (np.complex128,):\n            return True\n        return False\n    return False", "CodeBleu_score": 34.841121432742625}, {"code": "def _cuda_can_cast(from_dtype: npt.DTypeLike, to_dtype: npt.DTypeLike) -> bool\n\tfrom_dtype = np.dtype(from_dtype)\n    to_dtype = np.dtype(to_dtype)\n\n    if from_dtype == to_dtype:\n        return True\n\n    from_typechar = _typechars.find(from_dtype.char)\n    to_typechar = _typechars.find(to_dtype.char)\n\n    return from_typechar >= to_typechar", "CodeBleu_score": 49.88450533694467}, {"code": "def _cuda_can_cast(from_dtype: npt.DTypeLike, to_dtype: npt.DTypeLike) -> bool\n\tfrom_dtype = np.dtype(from_dtype)\n    to_dtype = np.dtype(to_dtype)\n\n    if from_dtype.kind == to_dtype.kind:\n        return True\n\n    # The following code is based on the NumPy function can_cast.\n    # See https://github.com/numpy/numpy/blob/v1.20.0/numpy/core/numerictypes.py#L1032-L1045\n    # for the NumPy implementation.\n    #\n    # The basic idea is to compare the type characters of the input data types.\n    # For example, float64 can be cast to float32, but float32 cannot be cast to float64.\n    # This is because the type character 'f' for float32 is at a higher position in the\n    # string _typechars than the type character 'd' for float64.\n    #\n    # The exception is when the input data types are complex types.\n    # In this case, the type character 'F' for complex64 is at a higher position in the\n    # string _typechars than the type character 'D' for complex128.\n    # However, complex64 cannot be cast to complex128.\n    # This is because the type character 'f' for float32 is at a higher position in the\n    # string _typechars than the type character 'F' for complex64.\n    #\n    # Note that the type character 'V' for void is at a lower position in the string\n    # _typechars than the type character 'O' for object.\n    # This is because the type character 'V' for void is at a lower position in the\n    # string _typechars than the type character 'b' for boolean.\n    # However, void can be cast to object.\n    # This is because the type character 'V' for void is at a lower position in the\n    # string _typechars than the type character 'O' for object.\n    #\n    # The following code is based on the NumPy function can_cast.\n    # See https://github.com/numpy/numpy/blob/v", "CodeBleu_score": 37.12409685404299}]}
{"instance_id": "5b8e2d5e6e12", "generation_result": [{"code": "def manual_legend(g, labels, colors, **legend_kwargs)\n\t# Obtain the matplotlib Axes object from the input, which can be a Visualizer, None, or an Axes object.\n    if isinstance(g, Visualizer):\n        ax = g.ax\n    elif isinstance(g, plt.Axes):\n        ax = g\n    elif g is None:\n        ax = plt.gca()\n    else:\n        raise YellowbrickValueError(\n            \"Could not obtain Axes object from input, please pass a Visualizer, None, or an Axes object\"\n        )\n\n    # Check that the lengths of the labels and colors lists match.\n    if len(labels) != len(colors):\n        raise YellowbrickValueError(\n            \"The number of labels and colors must match, but {} labels and {} colors were provided\".format(\n                len(labels), len(colors)\n            )\n        )\n\n    # Create legend handles by pairing each label with its corresponding color using patches.Patch.\n    handles = [\n        patches.Patch(color=color, label=label)\n        for label, color in zip(labels, colors)\n    ]\n\n    # Return the legend object created with the specified legend handles and additional keyword arguments.\n    return ax.legend(handles=handles, **legend_kwargs)", "CodeBleu_score": 53.95932245507262}, {"code": "def manual_legend(g, labels, colors, **legend_kwargs)\n\t# Obtain the matplotlib Axes object from the input\n    if isinstance(g, Visualizer):\n        ax = g.ax\n    elif g is None:\n        ax = plt.gca()\n    else:\n        ax = g\n\n    # Check that the lengths of the labels and colors lists match\n    if len(labels) != len(colors):\n        raise YellowbrickValueError(\n            \"The number of labels and colors must match, but \"\n            \"{} labels were passed with {} colors.\".format(len(labels), len(colors))\n        )\n\n    # Create legend handles by pairing each label with its corresponding color\n    handles = [\n        patches.Patch(color=color, label=label)\n        for label, color in zip(labels, colors)\n    ]\n\n    # Return the legend object created with the specified legend handles and additional keyword arguments\n    return ax.legend(handles=handles, **legend_kwargs)", "CodeBleu_score": 58.02262696786018}, {"code": "def manual_legend(g, labels, colors, **legend_kwargs)\n\t# Obtain the matplotlib Axes object from the input, which can be a Visualizer, None, or an Axes object\n    if isinstance(g, Visualizer):\n        ax = plt.gca()\n    elif g is None:\n        ax = plt.gca()\n    elif isinstance(g, plt.Axes):\n        ax = g\n    else:\n        raise YellowbrickValueError(\n            \"manual_legend must be passed a Visualizer, None, or an Axes object\"\n        )\n\n    # Check that the lengths of the labels and colors lists match\n    if len(labels) != len(colors):\n        raise YellowbrickValueError(\n            \"manual_legend requires the labels and colors lists to have the same length\"\n        )\n\n    # Create legend handles by pairing each label with its corresponding color using patches.Patch\n    legend_handles = [\n        patches.Patch(color=color, label=label) for color, label in zip(colors, labels)\n    ]\n\n    # Return the legend object created with the specified legend handles and additional keyword arguments\n    return ax.legend(handles=legend_handles, **legend_kwargs)", "CodeBleu_score": 55.546670134574626}]}
{"instance_id": "494e7f719eb6", "generation_result": [{"code": "def pcoords_time(X, y, fast=True)\n\tfig, ax = plt.subplots()\n    start = time.time()\n    pcoords = ParallelCoordinates(ax, fast=fast)\n    pcoords.fit_transform(X, y)\n    end = time.time()\n    plt.cla()\n    plt.close()\n    return end - start\n\n", "CodeBleu_score": 23.893855873667572}, {"code": "def pcoords_time(X, y, fast=True)\n\tfig, ax = plt.subplots(1, 1, figsize=(10, 10))\n    pcoords = ParallelCoordinates(ax, fast=fast)\n    start_time = time.time()\n    pcoords.fit_transform(X, y)\n    elapsed_time = time.time() - start_time\n    plt.cla()\n    plt.close()\n    return elapsed_time", "CodeBleu_score": 45.28288151308687}, {"code": "def pcoords_time(X, y, fast=True)\n\tfig, ax = plt.subplots()\n    if fast:\n        oz = ParallelCoordinates(ax, classes=np.unique(y))\n    else:\n        oz = ParallelCoordinates(ax, classes=np.unique(y), alpha=0.05)\n    start = time.time()\n    oz.fit_transform(X, y)\n    end = time.time()\n    plt.cla()\n    plt.close()\n    return end - start\n\n", "CodeBleu_score": 51.097065343163685}]}
{"instance_id": "db4143bc029f", "generation_result": [{"code": "def draw_instances(self, X, y, **kwargs):\n\talpha = kwargs.pop(\"alpha\", self.alpha)\n    ax = kwargs.pop(\"ax\", None)\n    colors = kwargs.pop(\"colors\", None)\n\n    if ax is None:\n        ax = plt.gca()\n\n    if colors is None:\n        colors = self.get_colors(y)\n\n    for i, (x, yi, color) in enumerate(zip(X, y, colors)):\n        ax.plot(self.increment_, x, alpha=alpha, color=color, **kwargs)\n\n    return ax\n\n", "CodeBleu_score": 32.88238006299846}, {"code": "def draw_instances(self, X, y, **kwargs):\n\talpha = kwargs.pop(\"alpha\", self.alpha)\n    ax = kwargs.pop(\"ax\", None)\n    ax = self.ax if ax is None else ax\n    colors = self.get_colors(y)\n    for i, (x, yi, color) in enumerate(zip(X, y, colors)):\n        ax.plot(self.increment_, x, color=color, alpha=alpha, **kwargs)\n    return ax\n\n", "CodeBleu_score": 36.956114075680816}, {"code": "def draw_instances(self, X, y, **kwargs):\n\talpha = kwargs.pop(\"alpha\", self.alpha)\n    ax = kwargs.pop(\"ax\", self.ax)\n    colors = self.get_colors(y)\n    for xi, yi, color in zip(X, y, colors):\n        ax.plot(self.x_axis_, xi, color=color, alpha=alpha, **kwargs)\n    return ax\n\n", "CodeBleu_score": 37.08253066964655}]}
{"instance_id": "676433312ffe", "generation_result": [{"code": "def draw_classes(self, X, y, **kwargs):\n\t# Set the alpha value from the object's attributes or defaults to 0.5\n        alpha = kwargs.pop(\"alpha\", self.alpha)\n\n        # Add a column of ones to X to separate the instances visually\n        # and append None to the increments list to create breaks between instances\n        X = np.hstack([X, np.ones((X.shape[0], 1))])\n        self.increments_separated.append(None)\n\n        # Identify unique class labels in y\n        unique_classes = np.unique(y)\n\n        # Iterate over the unique class labels\n        for unique_class in unique_classes:\n            # Determine the color for the class\n            color = self.get_colors(unique_class)\n\n            # Select the instances of X belonging to that class\n            X_in_class = X[y == unique_class, :]\n\n            # Plot the instances as a single line plot with the specified color and alpha transparency\n            self.ax.plot(\n                X_in_class[:, :-1].flatten(),\n                X_in_class[:, -1].flatten(),\n                color=color,\n                alpha=alpha,\n                **kwargs\n            )\n\n        # Return the axis object with the plotted classes\n        return self.ax\n\n", "CodeBleu_score": 50.399850925061585}, {"code": "def draw_classes(self, X, y, **kwargs):\n\t# Set the alpha transparency value from the object's attributes or defaults to 0.5\n    kwargs.setdefault(\"alpha\", self.alpha)\n\n    # Add a column of ones to X to separate the instances visually\n    # and append None to the increments list to create breaks between instances\n    X_in_class = np.hstack([X, np.ones((X.shape[0], 1))])\n    increments_separated = self.increments.tolist()\n    increments_separated.append(None)\n\n    # Identify unique class labels in y\n    classes = np.unique(y)\n\n    # Iterate over the unique class labels\n    for c in classes:\n        # Determine the color for the current class\n        color = self.get_colors(c)\n\n        # Select the instances of X belonging to the current class\n        X_in_class_c = X_in_class[y == c, :]\n\n        # Plot the instances as a single line plot with the specified color and alpha transparency\n        self.ax.plot(\n            X_in_class_c[:, 0],\n            X_in_class_c[:, 1],\n            color=color,\n            drawstyle=\"steps-post\",\n            **kwargs\n        )\n\n    # Return the axis object with the plotted classes\n    return self.ax\n\n", "CodeBleu_score": 48.756886904947464}, {"code": "def draw_classes(self, X, y, **kwargs):\n\t# Set the alpha transparency value\n    kwargs.setdefault(\"alpha\", self.alpha)\n\n    # Add a column of ones to separate the instances visually\n    X_in_class = np.hstack([X, np.ones((X.shape[0], 1))])\n\n    # Get the unique classes in y\n    classes = np.unique(y)\n\n    # Iterate over the unique classes and plot each one\n    for class_ in classes:\n        # Get the instances of X that belong to this class\n        X_in_class_in_class = X_in_class[y == class_, :]\n\n        # Get the color for this class\n        color = self.get_colors(class_)\n\n        # Plot the instances of X belonging to this class\n        self.ax.plot(X_in_class_in_class[:, :-1], color=color, **kwargs)\n\n    # Return the axis object with the plotted classes\n    return self.ax\n\n", "CodeBleu_score": 32.962295449798326}]}
{"instance_id": "9d297b12a673", "generation_result": [{"code": "def feature_analysis(fname=\"feature_analysis.png\")\n\tfig, axes = plt.subplots(1, 2, figsize=(12, 6))\n\n    oz = RadViz(\n        classes=[\"unoccupied\", \"occupied\"],\n        features=[\"temperature\", \"relative humidity\", \"light\", \"C02\", \"humidity\"],\n    )\n    X, y = load_occupancy()\n    oz.fit(X, y)\n    oz.finalize()\n    oz.poof(outpath=fname, ax=axes[0])\n\n    r2d = Rank2D(\n        classes=[\"weak\", \"medium\", \"strong\"],\n        features=[\"cement\", \"slag\", \"ash\", \"water\", \"splast\", \"coarse\", \"fine\", \"age\"],\n    )\n    X, y = load_concrete()\n    r2d.fit(X, y)\n    r2d.transform(X)\n    r2d.finalize()\n    r2d.poof(outpath=fname, ax=axes[1])\n\n    plt.tight_layout()\n    plt.savefig(fname)\n\n", "CodeBleu_score": 38.981161794902945}, {"code": "def feature_analysis(fname=\"feature_analysis.png\")\n\tfig = plt.figure(figsize=(12, 6))\n    gs = fig.add_gridspec(2, 2)\n    ax1 = fig.add_subplot(gs[0, 0])\n    ax2 = fig.add_subplot(gs[0, 1])\n    ax3 = fig.add_subplot(gs[1, :])\n\n    X, y = load_occupancy()\n    oz = RadViz(X, y, ax=ax1)\n    oz.fit()\n    oz.finalize()\n\n    X, y = load_concrete()\n    oz = Rank2D(X, y, ax=ax2)\n    oz.fit()\n    oz.transform(X)\n    oz.finalize()\n\n    plt.tight_layout()\n    plt.savefig(fname)", "CodeBleu_score": 23.581225333388115}, {"code": "def feature_analysis(fname=\"feature_analysis.png\")\n\tfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n\n    # Perform RadViz analysis on occupancy data\n    X, y = load_occupancy()\n    oz = RadViz(classes=y.unique(), ax=ax1)\n    oz.fit(X, y)\n    oz.finalize()\n\n    # Perform Rank2D analysis on concrete data\n    X, y = load_concrete()\n    r2d = Rank2D(ax=ax2)\n    r2d.fit(X, y)\n    r2d.transform(X)\n    r2d.finalize()\n\n    # Save the figure\n    plt.savefig(fname)\n\n", "CodeBleu_score": 27.512793956566938}]}
{"instance_id": "fd1b4b9cd7ef", "generation_result": [{"code": "def regression(fname=\"regression.png\")\n\tplt.subplots(1, 2, figsize=(18, 6))\n    alphas = np.logspace(-2, 1, 10)\n    X_train, X_test, y_train, y_test = load_concrete(split=True)\n    for alpha in alphas:\n        lasso = LassoCV(alphas=[alpha], cv=5).fit(X_train, y_train)\n        plt.subplot(1, 2, 1)\n        plt.plot(alpha, lasso.score(X_test, y_test), \"o\", label=\"alpha={}\".format(alpha))\n        plt.legend()\n\n        ridge = RidgeCV(alphas=[alpha], cv=5).fit(X_train, y_train)\n        plt.subplot(1, 2, 2)\n        plt.plot(alpha, ridge.score(X_test, y_test), \"o\", label=\"alpha={}\".format(alpha))\n        plt.legend()\n\n    plt.subplot(1, 2, 1)\n    plt.title(\"Prediction Error\")\n    plt.xlabel(\"alpha\")\n    plt.ylabel(\"score\")\n\n    plt.subplot(1, 2, 2)\n    plt.title(\"Residuals\")\n    plt.xlabel(\"alpha\")\n    plt.ylabel(\"score\")\n\n    plt.tight_layout()\n    oz.savefig(fname)\n    return oz.path(fname)\n\n", "CodeBleu_score": 32.634029968783146}, {"code": "def regression(fname=\"regression.png\")\n\talphas = np.logspace(-5, 2, 50)\n    fig, axes = plt.subplots(1, 2, figsize=(18, 6))\n\n    X_train, X_test, y_train, y_test = load_concrete(split=True)\n\n    for ax, model in zip(axes, [LassoCV, RidgeCV]):\n        model = model(alphas=alphas)\n        model.fit(X_train, y_train)\n        preds = model.predict(X_test)\n        oz.plot_prediction_error(y_test, preds, ax=ax)\n\n    oz.plot_residuals(y_test, preds, ax=axes[1])\n\n    oz.finalize(fig)\n    path = os.path.join(FIGURES, fname)\n    plt.savefig(path)\n    return path\n\n", "CodeBleu_score": 44.48705215819474}, {"code": "def regression(fname=\"regression.png\")\n\tplt.subplots(nrows=1, ncols=2, figsize=(18, 6))\n\n    X_train, X_test, y_train, y_test = load_concrete(split=True)\n\n    alphas = np.logspace(-4, 0, 50)\n\n    plt.subplot(1, 2, 1)\n    plt.title(\"Prediction Error\")\n    plt.xlabel(\"alpha\")\n    plt.ylabel(\"MSE\")\n    plt.plot(\n        alphas,\n        [\n            cross_val_score(LassoCV(alphas=alphas, random_state=42), X_train, y_train, cv=5).mean()\n            for alpha in alphas\n        ],\n        label=\"LassoCV\",\n    )\n    plt.plot(\n        alphas,\n        [\n            cross_val_score(RidgeCV(alphas=alphas, random_state=42), X_train, y_train, cv=5).mean()\n            for alpha in alphas\n        ],\n        label=\"RidgeCV\",\n    )\n    plt.legend()\n\n    plt.subplot(1, 2, 2)\n    plt.title(\"Residuals\")\n    plt.xlabel(\"alpha\")\n    plt.ylabel(\"MSE\")\n    plt.plot(\n        alphas,\n        [\n            cross_val_score(LassoCV(alphas=alphas, random_state=42), X_train, y_train, cv=5).mean()\n            for alpha in alphas\n        ],\n        label=\"LassoCV\",\n    )\n    plt.plot(\n        alphas,\n        [\n            cross_val_score(RidgeCV(alphas=alphas, random_state=42), X_train, y_train, cv=5).mean()\n            for alpha in alphas\n        ],\n        label=\"RidgeCV\",\n    )\n    plt.legend()\n\n    plt.tight_layout()\n    return plt.savefig(fname)\n\n", "CodeBleu_score": 22.621138765533704}]}
{"instance_id": "627126918f88", "generation_result": [{"code": "def classification(fname=\"classification.png\")\n\tfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n    # Load spam dataset with a train-test split, fit a MultinomialNB classifier, and display a ClassificationReport\n    X_train, X_test, y_train, y_test = load_spam(split=True)\n    model = MultinomialNB()\n    model.fit(X_train, y_train)\n    oz.classification.ClassificationReport(model, X_test, y_test, ax=axes[0])\n\n    # Load entire spam dataset, fit a LogisticRegression model, and display a DiscriminationThreshold plot\n    X, y = load_spam(split=False)\n    model = LogisticRegression()\n    model.fit(X, y)\n    oz.classification.DiscriminationThreshold(model, X, y, ax=axes[1])\n\n    # Finalize both visualizations, adjust the layout, and save the figure\n    oz.finalize(fig)\n    plt.tight_layout()\n    plt.savefig(fname)", "CodeBleu_score": 39.414130732689344}, {"code": "def classification(fname=\"classification.png\")\n\tfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\n\n    # Load the spam dataset with a train-test split\n    X_train, X_test, y_train, y_test = load_spam(split=True)\n\n    # Fit a MultinomialNB classifier to the training data\n    clf = MultinomialNB()\n    clf.fit(X_train, y_train)\n\n    # Display a ClassificationReport on the left axis\n    oz.classification_report(clf, X_test, y_test, ax=ax1)\n\n    # Load the entire spam dataset\n    X, y = load_spam()\n\n    # Fit a LogisticRegression model to the data\n    clf = LogisticRegression()\n    clf.fit(X, y)\n\n    # Display a DiscriminationThreshold plot on the right axis\n    oz.discrimination_threshold(clf, X, y, ax=ax2)\n\n    # Finalize the visualizations\n    oz.finalize(ax1)\n    oz.finalize(ax2)\n\n    # Adjust the layout and save the figure\n    fig.tight_layout()\n    fig.savefig(fname)", "CodeBleu_score": 23.64633000505809}, {"code": "def classification(fname=\"classification.png\")\n\tfig, axes = plt.subplots(1, 2, figsize=(12, 6))\n\n    # Left Axis\n    X_train, X_test, y_train, y_test = load_spam(split=True)\n    clf = MultinomialNB()\n    oz.fit(clf, X_train, y_train)\n    oz.score(clf, X_test, y_test)\n    oz.classification_report(clf, X_test, y_test)\n    oz.finalize(axes[0])\n\n    # Right Axis\n    X, y = load_spam(split=False)\n    clf = LogisticRegression()\n    oz.fit(clf, X, y)\n    oz.discrimination_threshold(clf, X, y)\n    oz.finalize(axes[1])\n\n    plt.tight_layout()\n    plt.savefig(fname, dpi=300)\n    plt.show()", "CodeBleu_score": 36.53293648726456}]}
{"instance_id": "318d3a674694", "generation_result": [{"code": "def get_color_cycle()\n\timport matplotlib as mpl\n    import matplotlib.pyplot as plt\n    import warnings\n    import numpy as np\n    import sys\n    import os\n    import matplotlib.colors as colors\n\n    if sys.version_info < (3, 0):\n        from itertools import izip_longest\n    else:\n        izip_longest = zip_longest\n\n    # If there is a KeyError, it falls back to the old color cycle\n    try:\n        # Get the current color cycle from the rcParams dictionary\n        current_cycler = plt.rcParams['axes.prop_cycle']\n        # Convert the cycler to a list of colors\n        current_cycler = current_cycler.by_key()['color']\n    except KeyError:\n        # Get the current color cycle from the rcParams dictionary\n        current_cycler = plt.rcParams['axes.color_cycle']\n        # Convert the cycler to a list of colors\n        current_cycler = [current_cycler[i] for i in range(len(current_cycler))]\n\n    # Check if the current color cycle is a list of colors\n    if not isinstance(current_cycler, list):\n        raise TypeError('The current color cycle is not a list of colors.')\n\n    # Check if the current color cycle is empty\n    if not current_cycler:\n        raise ValueError('The current color cycle is empty.')\n\n    # Check if the current color cycle contains only strings\n    if not all(isinstance(c, str) for c in current_cycler):\n        raise TypeError('The current color cycle does not contain only strings.')\n\n    # Check if the current color cycle contains only valid colors\n    for c in current_cycler:\n        try:\n            colors.to_rgba(c)\n        except ValueError:\n            raise ValueError('The current color cycle contains the invalid color: \"{0}\".'.format(c))\n\n    # Check if the current color cycle contains only valid colors\n    for c in current_cycler:\n        try:\n            colors.to_rgba(c)\n        except ValueError:\n            raise ValueError('The current color cycle contains the invalid color: \"{0", "CodeBleu_score": 31.1649641855813}, {"code": "def get_color_cycle()\n\ttry:\n        return plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n    except KeyError:\n        pass\n    return plt.rcParams[\"axes.color_cycle\"]\n\n", "CodeBleu_score": 32.2347288892667}, {"code": "def get_color_cycle()\n\ttry:\n        from matplotlib import rcParams\n        from cycler import Cycler\n        from matplotlib import __version__ as mpl_version\n        if mpl_version >= '1.5.0':\n            try:\n                color_cycle = rcParams['axes.prop_cycle']\n                if not isinstance(color_cycle, Cycler):\n                    raise KeyError\n            except KeyError:\n                raise\n        else:\n            color_cycle = rcParams['axes.color_cycle']\n        return color_cycle\n    except (KeyError, ImportError):\n        return ['b', 'g', 'r', 'c', 'm', 'y', 'k']\n\n", "CodeBleu_score": 31.73864633527962}]}
{"instance_id": "f3fac557334c", "generation_result": [{"code": "def plot_knee_normalized(\n        self,\n    )\n\tplt.figure(figsize=(8, 8))\n    plt.ylim(0, 1)\n    plt.vlines(self.knee, 0, 1, colors='r')\n    plt.yticks(np.arange(0, 1.1, 0.1))\n    plt.xticks(np.arange(min(self.x), max(self.x), 0.1))\n    plt.plot(self.x, self.y, 'b', self.x, self.distance, 'r')\n    plt.show()", "CodeBleu_score": 34.64060887615496}, {"code": "def plot_knee_normalized(\n        self,\n    )\n\tplt.figure(figsize=(8, 8))\n    plt.ylim(0, 1)\n    plt.vlines(self.knee_x, 0, 1, linestyles=\"dashed\")\n    plt.yticks(np.arange(0, 1.1, 0.1))\n    plt.xticks(np.arange(min(self.x), max(self.x) + 1, 0.1))\n    plt.plot(self.x, self.y, label=\"Normalized Curve\")\n    plt.plot(self.x, self.distance, \"r\", label=\"Distance Curve\")\n    plt.legend(loc=\"upper right\")\n    plt.show()", "CodeBleu_score": 36.215679005300395}, {"code": "def plot_knee_normalized(\n        self,\n    )\n\timport matplotlib.pyplot as plt\n    plt.figure(figsize=(8,8))\n    plt.plot(self.normalized_x, self.normalized_y, color='b')\n    plt.plot(self.normalized_x, self.distance_curve, color='r')\n    plt.xlim(min(self.normalized_x), max(self.normalized_x))\n    plt.ylim(min(self.normalized_y), max(self.normalized_y))\n    plt.yticks(np.arange(min(self.normalized_y), max(self.normalized_y), 0.1))\n    plt.xticks(np.arange(min(self.normalized_x), max(self.normalized_x), 0.1))\n    if self.knee_point:\n        plt.vlines(self.knee_point, min(self.normalized_y), max(self.normalized_y), colors='g', linestyles='--')\n    plt.show()", "CodeBleu_score": 37.64366776739221}]}
{"instance_id": "b2d48aae4fce", "generation_result": [{"code": "def plot_knee(\n        self,\n    )\n\t# Plot the data points\n    plt.figure(figsize=(8, 8))\n    plt.plot(self.x, self.y)\n\n    # Draw a vertical line at the knee point\n    plt.vlines(self.knee, plt.ylim()[0], plt.ylim()[1], linestyles='--', colors='r')\n\n    # Display the plot\n    plt.show()", "CodeBleu_score": 61.737834949105384}, {"code": "def plot_knee(\n        self,\n    )\n\tplt.figure(figsize=(8, 8))\n    plt.plot(self.x, self.y, color=\"black\")\n    plt.ylim(0, 1)\n    plt.vlines(self.knee, 0, 1, color=\"red\", linestyles=\"dashed\")\n    plt.show()", "CodeBleu_score": 40.06503777017768}, {"code": "def plot_knee(\n        self,\n    )\n\t# Import the necessary modules\n    import matplotlib.pyplot as plt\n\n    # Create a figure with the specified size\n    plt.figure(figsize=(8, 8))\n\n    # Plot the data points\n    plt.plot(self.x, self.y)\n\n    # Draw a vertical line at the knee point\n    plt.vlines(self.knee_point, plt.ylim()[0], plt.ylim()[1])\n\n    # Display the plot\n    plt.show()", "CodeBleu_score": 59.43169261881056}]}
{"instance_id": "4450ba2384c9", "generation_result": [{"code": "def plot_pca_interactive(data, n_components=6)\n\tfrom sklearn.decomposition import PCA\n    pca = PCA(n_components=n_components)\n    pca.fit(data)\n    data_pca = pca.transform(data)\n    print('Explained variance: {}'.format(pca.explained_variance_ratio_))\n\n    def show_decomp(i):\n        x = data[i]\n        x_pca = data_pca[i]\n        plot_image_components(x, x_pca, components=pca.components_,\n                              imshape=(28, 28), n_components=n_components,\n                              fontsize=14)\n\n    interact(show_decomp, i=(0, data.shape[0] - 1))\n\n", "CodeBleu_score": 58.747789535319605}, {"code": "def plot_pca_interactive(data, n_components=6)\n\tfrom sklearn.decomposition import PCA\n    from IPython.html.widgets import interact\n\n    pca = PCA(n_components=n_components)\n    pca.fit(data)\n    transformed = pca.transform(data)\n    components = pca.components_\n\n    def show_decomp(index):\n        plot_image_components(data[index], transformed[index], components=components,\n                              imshape=data.shape[1:], n_components=n_components)\n\n    interact(show_decomp, index=(0, len(data) - 1))", "CodeBleu_score": 62.556212486866016}, {"code": "def plot_pca_interactive(data, n_components=6)\n\tfrom sklearn.decomposition import PCA\n    pca = PCA(n_components=n_components)\n    pca.fit(data)\n    transformed = pca.transform(data)\n\n    def show_decomp(idx):\n        x = data[idx]\n        coeffs = transformed[idx]\n        show_image_components(x, coeffs, components=pca.components_,\n                              n_components=n_components, fontsize=12)\n\n    interact(show_decomp, idx=(0, len(data) - 1, 1))\n\n\n", "CodeBleu_score": 54.695121567271755}]}
{"instance_id": "b09a163f3ff1", "generation_result": [{"code": "def plot_iris_knn()\n\t# Load the Iris dataset\n    iris = datasets.load_iris()\n    X = iris.data[:, :2]  # we only take the first two features.\n    y = iris.target\n\n    # Create a mesh grid on the feature space\n    h = .02  # step size in the mesh\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n\n    # Create a kNN classifier with 5 neighbors\n    knn = neighbors.KNeighborsClassifier(n_neighbors=5)\n    knn.fit(X, y)\n\n    # Plot the decision boundary\n    Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n\n    # Put the result into a color plot\n    Z = Z.reshape(xx.shape)\n    pl.figure()\n    pl.pcolormesh(xx, yy, Z, cmap=pl.cm.Paired)\n\n    # Plot also the training points\n    pl.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', cmap=pl.cm.Paired)\n    pl.xlabel('Sepal length')\n    pl.ylabel('Sepal width')\n\n    pl.xlim(xx.min(), xx.max())\n    pl.ylim(yy.min(), yy.max())\n    pl.xticks(())\n    pl.yticks(())\n\n    pl.show()\n\n", "CodeBleu_score": 63.037966996596495}, {"code": "def plot_iris_knn()\n\t# Load the Iris dataset from sklearn\n    iris = datasets.load_iris()\n\n    # Extract the first two features and the target labels\n    features = iris.data[:, :2]\n    target = iris.target\n\n    # Create a kNN classifier with 5 neighbors\n    knn = neighbors.KNeighborsClassifier(n_neighbors=5)\n\n    # Train the classifier\n    knn.fit(features, target)\n\n    # Create a mesh grid over the feature space\n    xx, yy = np.meshgrid(np.linspace(0, 8, 100), np.linspace(0, 4, 100))\n\n    # Predict the class for each point in the grid\n    Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n\n    # Reshape the predictions into a 100x100 array\n    Z = Z.reshape(xx.shape)\n\n    # Plot the decision boundary\n    pl.figure()\n    pl.pcolormesh(xx, yy, Z, cmap=pl.cm.Paired)\n\n    # Plot the training points\n    pl.scatter(features[:, 0], features[:, 1], c=target, cmap=pl.cm.Paired)\n\n    # Label the axes and show the plot\n    pl.xlabel(\"Sepal length\")\n    pl.ylabel(\"Sepal width\")\n    pl.show()\n\n", "CodeBleu_score": 32.79773667971148}, {"code": "def plot_iris_knn()\n\t# load the iris dataset\n    data = datasets.load_iris()\n    X, y = data.data[:, :2], data.target\n    # create a mesh grid over the feature space\n    xx, yy = np.meshgrid(np.linspace(X[:, 0].min(), X[:, 0].max(), 100),\n                         np.linspace(X[:, 1].min(), X[:, 1].max(), 100))\n    # create a kNN classifier with 5 neighbors\n    knn = neighbors.KNeighborsClassifier(n_neighbors=5)\n    # train the kNN classifier on the data\n    knn.fit(X, y)\n    # make predictions over the entire feature space\n    Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n    # reshape the predictions to match the mesh grid\n    Z = Z.reshape(xx.shape)\n    # create a figure\n    pl.figure()\n    # create a color plot with the predictions\n    pl.pcolormesh(xx, yy, Z)\n    # plot the training points\n    pl.scatter(X[:, 0], X[:, 1], c=y, edgecolor='k', s=20)\n    # label the axes\n    pl.xlabel('Sepal length')\n    pl.ylabel('Sepal width')\n    # show the plot\n    pl.show()", "CodeBleu_score": 39.34143005544992}]}
{"instance_id": "8eaee3207c09", "generation_result": [{"code": "def plot_sgd_separator()\n\t# Create 50 separable points\n    X, y = make_blobs(n_samples=50, centers=2, random_state=61)\n    # Create a meshgrid to plot in\n    h = .02  # step size in the mesh\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n    # Fit SGD classifier to the data\n    clf = SGDClassifier(loss=\"hinge\", alpha=0.01, max_iter=200,\n                        fit_intercept=True, random_state=42)\n    clf.fit(X, y)\n    # Plot the decision boundary. For that, we will assign a color to each\n    # point in the mesh [x_min, x_max]x[y_min, y_max].\n    Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n    # Put the result into a color plot\n    Z = Z.reshape(xx.shape)\n    plt.contour(xx, yy, Z, levels=[-1, 0, 1], linestyles=[\"--\", \"-\", \"--\"])\n    plt.xlabel(\"$x_1$\")\n    plt.ylabel(\"$x_2$\")\n    plt.xlim(xx.min(), xx.max())\n    plt.ylim(yy.min(), yy.max())\n    plt.title(\"Decision surface of linear SGD\")\n    plt.axis(\"tight\")\n    # Plot also the training points\n    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired,\n                edgecolors=\"k\")", "CodeBleu_score": 38.41105193013009}, {"code": "def plot_sgd_separator()\n\tX, y = make_blobs(n_samples=50, centers=2, random_state=42, cluster_std=2.0)\n    clf = SGDClassifier(loss=\"hinge\", alpha=0.01, max_iter=200, fit_intercept=True)\n    clf.fit(X, y)\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 500),\n                         np.linspace(y_min, y_max, 500))\n    Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    plt.contour(xx, yy, Z, levels=[-1, 0, 1], linestyles=[\"--\", \"-\", \"--\"], colors=[\"red\", \"blue\", \"red\"])\n    plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Paired)\n    plt.xlabel(\"$x_1$\", fontsize=18)\n    plt.ylabel(\"$x_2$\", fontsize=18, rotation=0)\n    plt.axis(\"tight\")\n    plt.show()", "CodeBleu_score": 35.70309957085028}, {"code": "def plot_sgd_separator()\n\t# Create 50 separable points\n    X, y = make_blobs(n_samples=50, centers=2, random_state=6152020, cluster_std=1.5)\n\n    # Create SGD classifier with hinge loss\n    clf = SGDClassifier(loss=\"hinge\", alpha=0.01, max_iter=200, fit_intercept=True)\n\n    # Fit the classifier to the data\n    clf.fit(X, y)\n\n    # Plot the decision boundary\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 500),\n                         np.linspace(y_min, y_max, 500))\n\n    # Make classification predictions over the grid\n    Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n\n    # Plot the decision boundary in the mesh\n    plt.contour(xx, yy, Z, levels=[-1, 0, 1], linestyles=[\"--\", \"-\", \"--\"])\n\n    # Plot the training points\n    plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap=plt.cm.Spectral)\n\n    # Set the axis limits\n    plt.xlim(xx.min(), xx.max())\n    plt.ylim(yy.min(), yy.max())\n\n    # Set the ticks\n    plt.xticks(())\n    plt.yticks(())\n\n    # Show the plot\n    plt.show()\n\n", "CodeBleu_score": 34.72393540044345}]}
{"instance_id": "8b54f999b4c0", "generation_result": [{"code": "def dump_svmlight_file(self, file)\n\tdata = np.array(self.data)\n        X = data[:, 0:2]\n        y = data[:, 2]\n        dump_svmlight_file(X, y, file)\n\n", "CodeBleu_score": 52.25583582055906}, {"code": "def dump_svmlight_file(self, file)\n\tdata = np.array(self.data)\n    X = data[:, 0:2]\n    y = data[:, 2]\n    dump_svmlight_file(X, y, file)\n\n", "CodeBleu_score": 53.37186017015665}, {"code": "def dump_svmlight_file(self, file)\n\tdata = np.array(self.data)\n    X = data[:, 0:2]\n    y = data[:, 2]\n    dump_svmlight_file(X, y, file)\n\n", "CodeBleu_score": 53.37186017015665}]}
{"instance_id": "2419013adc6e", "generation_result": [{"code": "def fit(self)\n\tprint(\"fit the model\")\n    train = np.array(self.model.data)\n    X = train[:, 0:2]\n    y = train[:, 2]\n\n    C = float(self.complexity.get())\n    gamma = float(self.gamma.get())\n    coef0 = float(self.coef0.get())\n    degree = int(self.degree.get())\n    kernel_map = {0: \"linear\", 1: \"rbf\", 2: \"poly\"}\n    if len(np.unique(y)) == 1:\n        clf = svm.OneClassSVM(kernel=kernel_map[self.kernel.get()],\n                              gamma=gamma, coef0=coef0, degree=degree)\n        clf.fit(X)\n    else:\n        clf = svm.SVC(kernel=kernel_map[self.kernel.get()], C=C,\n                      gamma=gamma, coef0=coef0, degree=degree)\n        clf.fit(X, y)\n    if hasattr(clf, 'score'):\n        print(\"Accuracy:\", clf.score(X, y) * 100)\n    X1, X2, Z = self.decision_surface(clf)\n    self.model.clf = clf\n    self.model.set_surface((X1, X2, Z))\n    self.model.surface_type = self.surface_type.get()\n    self.fitted = True\n    self.model.changed(\"surface\")\n\n", "CodeBleu_score": 98.7568182783107}, {"code": "def fit(self)\n\tprint(\"fit the model\")\n    train = np.array(self.model.data)\n    X = train[:, 0:2]\n    y = train[:, 2]\n\n    C = float(self.complexity.get())\n    gamma = float(self.gamma.get())\n    coef0 = float(self.coef0.get())\n    degree = int(self.degree.get())\n    kernel_map = {0: \"linear\", 1: \"rbf\", 2: \"poly\"}\n    if len(np.unique(y)) == 1:\n        clf = svm.OneClassSVM(kernel=kernel_map[self.kernel.get()],\n                              gamma=gamma, coef0=coef0, degree=degree)\n        clf.fit(X)\n    else:\n        clf = svm.SVC(kernel=kernel_map[self.kernel.get()], C=C,\n                      gamma=gamma, coef0=coef0, degree=degree)\n        clf.fit(X, y)\n    if hasattr(clf, 'score'):\n        print(\"Accuracy:\", clf.score(X, y) * 100)\n    X1, X2, Z = self.decision_surface(clf)\n    self.model.clf = clf\n    self.model.set_surface((X1, X2, Z))\n    self.model.surface_type = self.surface_type.get()\n    self.fitted = True\n    self.model.changed(\"surface\")", "CodeBleu_score": 98.7568182783107}, {"code": "def fit(self)\n\tprint(\"fit the model\")\n    train = np.array(self.model.data)\n    X = train[:, 0:2]\n    y = train[:, 2]\n\n    C = float(self.complexity.get())\n    gamma = float(self.gamma.get())\n    coef0 = float(self.coef0.get())\n    degree = int(self.degree.get())\n    kernel_map = {0: \"linear\", 1: \"rbf\", 2: \"poly\"}\n    if len(np.unique(y)) == 1:\n        clf = svm.OneClassSVM(kernel=kernel_map[self.kernel.get()],\n                              gamma=gamma, coef0=coef0, degree=degree)\n        clf.fit(X)\n    else:\n        clf = svm.SVC(kernel=kernel_map[self.kernel.get()], C=C,\n                      gamma=gamma, coef0=coef0, degree=degree)\n        clf.fit(X, y)\n    if hasattr(clf, 'score'):\n        print(\"Accuracy:\", clf.score(X, y) * 100)\n    X1, X2, Z = self.decision_surface(clf)\n    self.model.clf = clf\n    self.model.set_surface((X1, X2, Z))\n    self.model.surface_type = self.surface_type.get()\n    self.fitted = True\n    self.model.changed(\"surface\")\n\n", "CodeBleu_score": 98.7568182783107}]}
{"instance_id": "e887fd2a8513", "generation_result": [{"code": "def source_decode(sourcecode, verbose=0)\n\tif sourcecode.startswith(\"tpot.\"):\n        sourcecode = sourcecode[4:]\n        module, object_ = sourcecode.split(\".\", 1)\n        try:\n            obj = importlib.import_module(module)\n            obj = getattr(obj, object_)\n        except ImportError as e:\n            if verbose > 2:\n                raise e\n            else:\n                print(f\"WARNING: Failed to import {sourcecode} with error {e}\")\n                return None\n    else:\n        module, object_ = sourcecode.rsplit(\".\", 1)\n        try:\n            obj = importlib.import_module(module)\n            obj = getattr(obj, object_)\n        except ImportError as e:\n            if verbose > 2:\n                raise e\n            else:\n                print(f\"WARNING: Failed to import {sourcecode} with error {e}\")\n                return None\n    return module, object_, obj\n\n", "CodeBleu_score": 40.13708177678366}, {"code": "def source_decode(sourcecode, verbose=0)\n\tif verbose > 2:\n        print(sourcecode)\n    if sourcecode.startswith(\"tpot.\"):\n        sourcecode = sourcecode.split(\".\", 1)[1]\n    try:\n        import_str = \"import {0}\".format(sourcecode)\n        exec(import_str)\n        obj_str = sourcecode.split(\".\")[-1]\n        obj = eval(obj_str)\n        return import_str, obj_str, obj\n    except ImportError as e:\n        if verbose > 2:\n            raise e\n        else:\n            print(\"Warning: Could not import {}\".format(sourcecode))\n            return None, None, None\n\n", "CodeBleu_score": 43.73967682143884}, {"code": "def source_decode(sourcecode, verbose=0)\n\timport importlib\n    import tpot\n    import traceback\n    import sys\n    import warnings\n    import re\n    # Split the source code into the module and object components\n    if sourcecode.startswith(\"tpot.\"):\n        mod_str = sourcecode.split(\".\")[1]\n        obj_str = \".\".join(sourcecode.split(\".\")[2:])\n    else:\n        mod_str = sourcecode.split(\".\")[0]\n        obj_str = \".\".join(sourcecode.split(\".\")[1:])\n    # Try to import the object from the module\n    try:\n        obj = importlib.import_module(mod_str)\n        for attr in obj_str.split(\".\"):\n            obj = getattr(obj, attr)\n    except ImportError:\n        if verbose > 2:\n            raise ImportError(traceback.format_exc())\n        else:\n            warnings.warn(\"Could not import object {} from module {}\".format(obj_str, mod_str))\n        obj = None\n    # Return the import string, object string, and the imported object\n    return \"{}.{}\".format(mod_str, obj_str), obj_str, obj\n\n", "CodeBleu_score": 42.7699965769464}]}
{"instance_id": "5d6f629c3363", "generation_result": [{"code": "def set_sample_weight(pipeline_steps, sample_weight=None)\n\tsample_weights = {}\n    if sample_weight is not None:\n        for step in pipeline_steps:\n            if 'sample_weight' in inspect.getargspec(step[1].fit)[0]:\n                sample_weights[step[0] + '__sample_weight'] = sample_weight\n        if sample_weights:\n            return sample_weights\n        else:\n            return None\n    else:\n        return None\n\n", "CodeBleu_score": 36.713742888058576}, {"code": "def set_sample_weight(pipeline_steps, sample_weight=None)\n\tsample_weight_dict = {}\n    if sample_weight is not None:\n        for step in pipeline_steps:\n            step_name = step[0]\n            step_obj = step[1]\n            step_args = inspect.getargspec(step_obj.fit)\n            if \"sample_weight\" in step_args.args:\n                sample_weight_dict[step_name + \"_sample_weight\"] = sample_weight\n        if len(sample_weight_dict) > 0:\n            return sample_weight_dict\n        else:\n            return None\n    else:\n        return None\n\n", "CodeBleu_score": 40.669424933500906}, {"code": "def set_sample_weight(pipeline_steps, sample_weight=None)\n\tsample_weight_dict = {}\n    if sample_weight is not None:\n        for step in pipeline_steps:\n            step_name = step[0]\n            step_obj = step[1]\n            fit_args = inspect.getargspec(step_obj.fit)\n            if \"sample_weight\" in fit_args[0]:\n                sample_weight_dict[step_name + \"_sample_weight\"] = sample_weight\n        if sample_weight_dict:\n            return sample_weight_dict\n        else:\n            return None\n    else:\n        return None\n\n", "CodeBleu_score": 44.842160641415454}]}
{"instance_id": "cae17d519818", "generation_result": [{"code": "def _summary_of_best_pipeline(self, features, target):\n\twe do not mess up indices\n        pretty = dirty_string\n        for (start, end) in reversed(parameter_prefixes):\n            pretty = pretty[: start + 2] + pretty[end:]\n\n        return pretty", "CodeBleu_score": 5.424725282083622}, {"code": "def _summary_of_best_pipeline(self, features, target):\n\twe do not mess up indices\n        pretty = dirty_string\n        for (start, end) in reversed(parameter_prefixes):\n            pretty = pretty[: start + 2] + pretty[end:]\n\n        return pretty", "CodeBleu_score": 5.424725282083622}, {"code": "def _summary_of_best_pipeline(self, features, target):\n\twe do not mess up indices\n        pretty = dirty_string\n        for (start, end) in reversed(parameter_prefixes):\n            pretty = pretty[: start + 2] + pretty[end:]\n\n        return pretty", "CodeBleu_score": 5.424725282083622}]}
{"instance_id": "638683d78167", "generation_result": [{"code": "def score(self, testing_features, testing_target)\n\tif not self.fitted_pipeline:\n        raise NotFittedError(\n            \"This %s instance is not fitted yet. Call 'fit' with appropriate arguments before using this method.\"\n            % type(self).__name__\n        )\n\n    testing_features = testing_features.astype(np.float)\n    testing_target = testing_target.astype(np.float)\n\n    self._check_dataset(testing_features, testing_target)\n\n    if callable(self.scoring):\n        scorer = self.scoring\n    else:\n        scorer = SCORERS[self.scoring]\n\n    return scorer(self.fitted_pipeline, testing_features, testing_target)", "CodeBleu_score": 38.66435424282501}, {"code": "def score(self, testing_features, testing_target)\n\tif not self._optimized:\n        raise RuntimeError(\n            \"Pipeline has not been optimized. Please call the optimize() method \"\n            \"before calling the score() method.\"\n        )\n    testing_features = testing_features.astype(np.float64)\n    testing_target = testing_target.astype(np.float64)\n    self._check_dataset(testing_features, testing_target)\n\n    if isinstance(self.scoring, str):\n        if self.scoring not in SCORERS:\n            raise ValueError(\n                \"Error: The provided scoring function is not supported. Please \"\n                \"use one of the following: {}\".format(SCORERS.keys())\n            )\n        scorer = SCORERS[self.scoring]\n    elif callable(self.scoring):\n        scorer = self.scoring\n    else:\n        raise ValueError(\n            \"Error: The provided scoring function is not supported. Please \"\n            \"use one of the following: {}\".format(SCORERS.keys())\n        )\n\n    return scorer(self._pipeline, testing_features, testing_target)\n\n", "CodeBleu_score": 44.23903270123651}, {"code": "def score(self, testing_features, testing_target)\n\tif not self.fitted_pipeline:\n        raise RuntimeError(\"The pipeline has not been fitted yet. Please fit the pipeline first.\")\n\n    testing_features = testing_features.astype(np.float64)\n    testing_target = testing_target.astype(np.float64)\n    testing_features, testing_target = self._check_dataset(\n        testing_features, testing_target\n    )\n\n    if isinstance(self.scoring, str):\n        scorer = SCORERS[self.scoring]\n    elif callable(self.scoring):\n        scorer = self.scoring\n    else:\n        raise ValueError(\n            \"The provided scoring function is not a valid scoring function.\"\n        )\n\n    return scorer(self.fitted_pipeline, testing_features, testing_target)", "CodeBleu_score": 54.21862762254541}]}
{"instance_id": "a04382560333", "generation_result": [{"code": " def _impute_values(self, features)\n\tif self.verbosity > 1:\n        print(\"Imputing missing values with strategy: median\")\n    if not self.imputer_fitted:\n        self.imputer = SimpleImputer(strategy=\"median\")\n        self.imputer.fit(features)\n        self.imputer_fitted = True\n    features = self.imputer.transform(features)\n    return features\n\n", "CodeBleu_score": 70.71775677953103}, {"code": " def _impute_values(self, features)\n\tif self.verbosity > 1:\n        print(\"Imputing missing values...\")\n    if not self._imputer:\n        self._imputer = SimpleImputer(strategy=\"median\")\n        self._imputer.fit(features)\n    features = self._imputer.transform(features)\n    return features\n\n", "CodeBleu_score": 71.5353391933778}, {"code": " def _impute_values(self, features)\n\tif self.verbosity > 1:\n        print(\"Imputing missing values...\")\n\n    if not self.imputer:\n        self.imputer = SimpleImputer(strategy=\"median\")\n        self.imputer.fit(features)\n\n    features = self.imputer.transform(features)\n    return features", "CodeBleu_score": 70.86878756264687}]}
{"instance_id": "0043d411c332", "generation_result": [{"code": "def _compile_to_sklearn(self, expr)\n\t# generate pipeline code string\n    pipeline_code = self._generate_pipeline_code(expr)\n    # evaluate pipeline code string\n    pipeline = self._eval(pipeline_code)\n    # set memory attribute\n    pipeline.memory = self.memory\n    # set random_state recursively\n    if self.random_state is not None:\n        self._set_param_recursive(pipeline, 'random_state', self.random_state)\n    return pipeline\n\n", "CodeBleu_score": 21.40960044881143}, {"code": "def _compile_to_sklearn(self, expr)\n\tpipeline_code = self.generate_pipeline_code(expr)\n    pipeline_obj = self.eval(pipeline_code)\n    pipeline_obj.memory = self.memory\n    if self.random_state is not None:\n        self.set_param_recursive(pipeline_obj, 'random_state', self.random_state)\n    return pipeline_obj\n\n", "CodeBleu_score": 21.290710941806864}, {"code": "def _compile_to_sklearn(self, expr)\n\t# Generate the pipeline code string from the expression and a predefined set of operators\n    pipeline_code = self._generate_pipeline_code(expr)\n\n    # Evaluate the string within a specific context to obtain the scikit-learn pipeline object\n    pipeline = eval(pipeline_code, self._context)\n\n    # Set the pipeline's memory attribute\n    pipeline.set_params(memory=self._memory)\n\n    # If a random state is specified, recursively set the random_state parameter for each step in the pipeline\n    if self._random_state is not None:\n        self._set_param_recursive(pipeline, 'random_state', self._random_state)\n\n    # Return the configured scikit-learn pipeline\n    return pipeline\n\n", "CodeBleu_score": 26.79481777455683}]}
{"instance_id": "ca0ce462aa76", "generation_result": [{"code": "def pick_two_individuals_eligible_for_crossover(population)\n\t# Create sets of primitive nodes for each individual.\n    primitives = [ind1_prims.primitive_nodes(ind) for ind in population]\n    # Convert each individual to a string representation.\n    str_population = [str(ind) for ind in population]\n    # Identify pairs of individuals that share at least one primitive and are not identical.\n    eligible_pairs = []\n    for i, ind1 in enumerate(population):\n        for j, ind2 in enumerate(population):\n            if i != j and ind1 != ind2 and not ind1_prims.isdisjoint(primitives[i], primitives[j]):\n                eligible_pairs.append((i, j))\n    # If no eligible pairs are found, return None for both individuals.\n    if len(eligible_pairs) == 0:\n        return None, None\n    # Otherwise, randomly select one of the eligible pairs and return the corresponding individuals from the population.\n    else:\n        pair = random.choice(eligible_pairs)\n        return population[pair[0]], population[pair[1]]\n\n", "CodeBleu_score": 27.077584427663048}, {"code": "def pick_two_individuals_eligible_for_crossover(population)\n\tprimitive_sets = [set(ind.primitives()) for ind in population]\n    population_strings = [str(ind) for ind in population]\n    eligible_pairs = []\n    for i in range(len(population)):\n        for j in range(i + 1, len(population)):\n            if primitive_sets[i].isdisjoint(primitive_sets[j]) and population_strings[i] != population_strings[j]:\n                eligible_pairs.append((i, j))\n    if len(eligible_pairs) == 0:\n        return None, None\n    else:\n        pair = eligible_pairs[randint(0, len(eligible_pairs) - 1)]\n        return population[pair[0]], population[pair[1]]\n\n", "CodeBleu_score": 26.79762726534124}, {"code": "def pick_two_individuals_eligible_for_crossover(population)\n\t# Create sets of primitive nodes for each individual\n    primitive_sets = [set(ind1_prims.get_primitive_nodes(ind)) for ind in population]\n    # Convert each individual to a string representation\n    population_strings = [str(ind) for ind in population]\n    # Identify pairs of individuals that share at least one primitive and are not identical\n    pairs = [(i, j) for i in range(len(population)) for j in range(i + 1, len(population)) if (primitive_sets[i].isdisjoint(primitive_sets[j]) == False) and (population_strings[i] != population_strings[j])]\n    # If no eligible pairs are found, return None for both individuals\n    if len(pairs) == 0:\n        return None, None\n    # Otherwise, randomly select one of the eligible pairs and return the corresponding individuals from the population\n    else:\n        pair = pairs[randint(0, len(pairs) - 1)]\n        return population[pair[0]], population[pair[1]]\n\n", "CodeBleu_score": 23.342625888041162}]}
{"instance_id": "4b03c843f506", "generation_result": [{"code": "def generate_export_pipeline_code(pipeline_tree, operators)\n\tsteps = _process_operator(pipeline_tree, operators)\n    len_steps = len(steps)\n    if len_steps > 1:\n        steps_str = \",", "CodeBleu_score": 11.242624801427132}, {"code": "def generate_export_pipeline_code(pipeline_tree, operators)\n\tsteps = _process_operator(pipeline_tree, operators)\n    len_steps = len(steps)\n\n    if len_steps == 1:\n        return steps[0]\n    else:\n        return \"make_pipeline(", "CodeBleu_score": 15.57265028105361}, {"code": "def generate_export_pipeline_code(pipeline_tree, operators)\n\tsteps = []\n    for operator in pipeline_tree:\n        steps.extend(_process_operator(operator, operators))\n\n    len_steps = len(steps)\n    if len_steps > 1:\n        steps_code = format(\n            \"make_pipeline(", "CodeBleu_score": 20.478282522791115}]}
{"instance_id": "e9aec2ee39a4", "generation_result": [{"code": "def _process_operator(operator, operators, depth=0)\n\tsteps = []\n    op_name = operator[0]\n\n    if op_name == \"CombineDFs\":\n        steps.append(_combine_dfs(operator[1], operator[2], operators))\n    else:\n        input_name, args = operator[1], operator[2:]\n        tpot_op = get_by_name(op_name, operators)\n\n        if input_name != \"input_matrix\":\n            steps.extend(_process_operator(input_name, operators, depth + 1))\n\n        # If the step is an estimator and is not the last step then we must\n        # add its guess as synthetic feature(s)\n        # classification prediction for both regression and classification\n        # classification probabilities for classification if available\n        if tpot_op.root and depth > 0:\n            steps.append(\n                \"StackingEstimator(estimator={})\".format(tpot_op.export(*args))\n            )\n        else:\n            steps.append(tpot_op.export(*args))\n    return steps\n\n", "CodeBleu_score": 85.07455460075597}, {"code": "def _process_operator(operator, operators, depth=0)\n\tsteps = []\n    op_name = operator[0]\n\n    if op_name == \"CombineDFs\":\n        steps.append(_combine_dfs(operator[1], operator[2], operators))\n    else:\n        input_name, args = operator[1], operator[2:]\n        tpot_op = get_by_name(op_name, operators)\n\n        if input_name != \"input_matrix\":\n            steps.extend(_process_operator(input_name, operators, depth + 1))\n\n        # If the step is an estimator and is not the last step then we must\n        # add its guess as synthetic feature(s)\n        # classification prediction for both regression and classification\n        # classification probabilities for classification if available\n        if tpot_op.root and depth > 0:\n            steps.append(\n                \"StackingEstimator(estimator={})\".format(tpot_op.export(*args))\n            )\n        else:\n            steps.append(tpot_op.export(*args))\n    return steps\n\n", "CodeBleu_score": 85.07455460075597}, {"code": "def _process_operator(operator, operators, depth=0)\n\tsteps = []\n    op_name = operator[0]\n\n    if op_name == \"CombineDFs\":\n        steps.append(_combine_dfs(operator[1], operator[2], operators))\n    else:\n        input_name, args = operator[1], operator[2:]\n        tpot_op = get_by_name(op_name, operators)\n\n        if input_name != \"input_matrix\":\n            steps.extend(_process_operator(input_name, operators, depth + 1))\n\n        # If the step is an estimator and is not the last step then we must\n        # add its guess as synthetic feature(s)\n        # classification prediction for both regression and classification\n        # classification probabilities for classification if available\n        if tpot_op.root and depth > 0:\n            steps.append(\n                \"StackingEstimator(estimator={})\".format(tpot_op.export(*args))\n            )\n        else:\n            steps.append(tpot_op.export(*args))\n    return steps\n\n", "CodeBleu_score": 85.07455460075597}]}
{"instance_id": "0513bf969510", "generation_result": [{"code": "def transform(self, X)\n\tif not isinstance(X, pd.DataFrame):\n        X = pd.DataFrame(X)\n\n    # find categorical columns\n    categorical_columns = X.select_dtypes(include=['object', 'category']).columns\n\n    if len(categorical_columns) == 0:\n        raise ValueError('No categorical columns found in the input data.')\n\n    # apply one-hot encoding\n    ohe = OneHotEncoder(sparse=False)\n    ohe.fit_transform(_X_selected)\n\n    return ohe", "CodeBleu_score": 39.834687147949914}, {"code": "def transform(self, X)\n\tX = check_array(X, accept_sparse=True)\n    if not hasattr(self, 'categorical_features_'):\n        raise ValueError('This AutoSelectCategoricalFeatures instance is '\n                         'not fitted yet. Call \\'fit\\' with appropriate '\n                         'arguments before using this estimator.')\n    X_selected = X[:, self.categorical_features_]\n    if X_selected.shape[1] == 0:\n        raise ValueError('No categorical features found in the data')\n    return self.one_hot_encoder_.transform(X_selected)", "CodeBleu_score": 29.93594886948224}, {"code": "def transform(self, X)\n\tself.X = X\n    self.X_selected = self._X_selected()\n    if self.X_selected.shape[1] == 0:\n        raise ValueError(\"No categorical features found in the input data.\")\n    else:\n        ohe = OneHotEncoder(sparse=False)\n        return ohe.fit_transform(self.X_selected)", "CodeBleu_score": 24.51545986038177}]}
{"instance_id": "5b1bcf42ca9e", "generation_result": [{"code": "def balanced_accuracy(y_true, y_pred)\n\t# Get all unique classes\n    classes = np.unique(y_true)\n    # Initialize a list to store class accuracies\n    class_accuracies = []\n    for c in classes:\n        # Get the true and predicted labels for the current class\n        y_true_c = y_true[y_true == c]\n        y_pred_c = y_pred[y_true == c]\n        # Calculate sensitivity and specificity\n        sensitivity = np.sum(y_pred_c == c) / np.sum(y_true_c == c)\n        specificity = np.sum(y_pred_c != c) / np.sum(y_true_c != c)\n        # If the true labels consist of only one class, specificity is set to 1\n        if len(classes) == 1:\n            specificity = 1\n        # Calculate the class accuracy\n        class_accuracy = np.mean([sensitivity, specificity])\n        # Store the class accuracy\n        class_accuracies.append(class_accuracy)\n    # Return the mean of the class accuracies\n    return np.mean(class_accuracies)", "CodeBleu_score": 17.993465458159314}, {"code": "def balanced_accuracy(y_true, y_pred)\n\t# Create a list of unique classes\n    classes = np.unique(y_true)\n    # Initialize the class accuracy list\n    class_accuracies = []\n    # Loop through all classes\n    for c in classes:\n        # Identify true positives\n        tp = np.sum((y_true == c) & (y_pred == c))\n        # Identify false positives\n        fp = np.sum((y_true != c) & (y_pred == c))\n        # Identify false negatives\n        fn = np.sum((y_true == c) & (y_pred != c))\n        # Identify true negatives\n        tn = np.sum((y_true != c) & (y_pred != c))\n        # Calculate sensitivity\n        sensitivity = tp / (tp + fn)\n        # Calculate specificity\n        specificity = tn / (tn + fp)\n        # Calculate class accuracy\n        class_accuracy = (sensitivity + specificity) / 2\n        # Append class accuracy to the list\n        class_accuracies.append(class_accuracy)\n    # Return the mean of the class accuracies\n    return np.mean(class_accuracies)", "CodeBleu_score": 21.39868226376531}, {"code": "def balanced_accuracy(y_true, y_pred)\n\t# Find unique classes in true and predicted labels\n    unique_classes = np.unique(np.append(y_true, y_pred))\n    # Initialize list to hold class accuracies\n    class_accuracies = []\n    # Iterate over each class\n    for c in unique_classes:\n        # Identify true and predicted labels for this class\n        class_true = np.where(y_true == c, 1, 0)\n        class_pred = np.where(y_pred == c, 1, 0)\n        # Calculate sensitivity and specificity\n        sensitivity = np.sum(class_true * class_pred) / np.sum(class_true)\n        specificity = np.sum((1 - class_true) * (1 - class_pred)) / np.sum(1 - class_true)\n        # Ensure specificity is defined, even if class is underrepresented in y_true\n        if np.sum(class_true) == 0:\n            specificity = 1\n        # Calculate class accuracy\n        class_accuracy = (sensitivity + specificity) / 2\n        # Append to list of class accuracies\n        class_accuracies.append(class_accuracy)\n    # Return mean of class accuracies\n    return np.mean(class_accuracies)", "CodeBleu_score": 20.885603704569423}]}
{"instance_id": "19a9ad39de0a", "generation_result": [{"code": "def transform(self, X, y=None)\n\tX = check_array(X)\n    non_zero_elements_per_row = np.count_nonzero(X, axis=1)\n    zero_elements_per_row = X.shape[1] - non_zero_elements_per_row\n    non_zero_elements_per_row = non_zero_elements_per_row.reshape(-1, 1)\n    zero_elements_per_row = zero_elements_per_row.reshape(-1, 1)\n    X_transformed = np.hstack((non_zero_elements_per_row, zero_elements_per_row, X))\n    return X_transformed\n\n\n\n\n", "CodeBleu_score": 19.4720418737235}, {"code": "def transform(self, X, y=None)\n\tX = check_array(X)\n    nonzero = np.count_nonzero(X, axis=1)\n    zero = np.reshape(np.count_nonzero(X == 0, axis=1), (-1, 1))\n    X = np.hstack((zero, nonzero))\n    return X\n", "CodeBleu_score": 10.495660018421724}, {"code": "def transform(self, X, y=None)\n\tX = check_array(X)\n    n_nonzero = np.count_nonzero(X, axis=1)\n    n_zero = np.reshape(X.shape[1] - n_nonzero, (-1, 1))\n    X_transformed = np.hstack((n_nonzero, n_zero, X))\n    return X_transformed\n\n\n\n\n\n", "CodeBleu_score": 14.273345586400875}]}
{"instance_id": "5e8db0b5d447", "generation_result": [{"code": "def _init_pretest(self, features, target)\n\tself.n_classes = len(np.unique(target))\n    self.train_size = max(self.n_classes, self.train_size)\n    self.pretest_X, self.pretest_y = train_test_split(features, target, train_size=self.train_size, random_state=self.random_state)\n    if not np.array_equal(np.unique(self.pretest_y), np.arange(self.n_classes)):\n        self.pretest_y = _safe_indexing(self.pretest_y, np.arange(self.n_classes))\n\n", "CodeBleu_score": 22.128500078769402}, {"code": "def _init_pretest(self, features, target)\n\tself.n_classes = len(np.unique(target))\n    if self.train_size < self.n_classes:\n        self.train_size = self.n_classes\n    self.pretest_X, self.pretest_y = train_test_split(features, target, train_size=self.train_size, random_state=self.random_state)\n    if len(np.unique(self.pretest_y)) < self.n_classes:\n        self.pretest_y = _safe_indexing(self.pretest_y, np.unique(target))\n\n", "CodeBleu_score": 25.55114613677244}, {"code": "def _init_pretest(self, features, target)\n\tself.n_classes = len(np.unique(target))\n    self.pretest_size = max(self.n_classes, self.pretest_size)\n    self.pretest_X, self.pretest_y = train_test_split(features, target, train_size=self.pretest_size, random_state=self.random_state)\n    if len(np.unique(self.pretest_y)) < self.n_classes:\n        self.pretest_y = _safe_indexing(self.pretest_y, np.unique(target))\n\n", "CodeBleu_score": 21.339545160188965}]}
{"instance_id": "5e3d48667396", "generation_result": [{"code": "def _to_numpy_type(dtype)\n\tif isinstance(dtype, tf.DType):\n        dtype = dtype.as_numpy_dtype\n    return dtype\n", "CodeBleu_score": 43.572026921522266}, {"code": "def _to_numpy_type(dtype)\n\tif isinstance(dtype, tf.DType):\n        return tf.experimental.numpy.dtypes.as_numpy_dtype(dtype)\n    return dtype\n\n\n\n\n", "CodeBleu_score": 60.88476631370183}, {"code": "def _to_numpy_type(dtype)\n\tif isinstance(dtype, tf.DType):\n        return dtype.as_numpy_dtype\n    else:\n        return dtype\n", "CodeBleu_score": 43.80255556965443}]}
{"instance_id": "8f1120fdd463", "generation_result": [{"code": "def _dataset_as_numpy(ds, batch_size=None)\n\ttry:\n        # If the user has not specified a batch size, we try to get the batch size from the dataset.\n        if batch_size is None:\n            batch_size = ds.batch(1).batch_size\n\n        # We use the TensorFlow dataset's as_numpy_iterator function to iterate over the batches.\n        for batch in ds.batch(batch_size).as_numpy_iterator():\n            yield _to_numpy(batch)\n    except AttributeError:\n        # If the as_numpy_iterator function is unavailable, we fall back to the default method.\n        for batch in ds.batch(batch_size):\n            yield _to_numpy(batch)\n\n", "CodeBleu_score": 18.8285664966695}, {"code": "def _dataset_as_numpy(ds, batch_size=None)\n\ttry:\n        if batch_size is not None:\n            ds = ds.batch(batch_size)\n        return ds.as_numpy_iterator()\n    except AttributeError:\n        return _dataset_as_numpy_fallback(ds, batch_size)\n\n", "CodeBleu_score": 13.327158048277148}, {"code": "def _dataset_as_numpy(ds, batch_size=None)\n\ttry:\n        if batch_size is not None:\n            ds = ds.batch(batch_size)\n        return ds.as_numpy_iterator()\n    except AttributeError:\n        return _dataset_as_numpy_fallback(ds, batch_size)\n\n", "CodeBleu_score": 13.327158048277148}]}
{"instance_id": "282271955871", "generation_result": [{"code": "def get_prng(seed)\n\tif seed is None:\n        seed = np.random.randint(2**32)\n    seed = np.uint32(seed)\n    return jax.random.PRNGKey(np.concatenate([seed, seed]).tobytes())", "CodeBleu_score": 19.59556844985848}, {"code": "def get_prng(seed)\n\tseed = np.asarray(seed)\n    assert seed.ndim == 0, \"Seed must be a scalar\"\n    seed = np.uint32(seed)\n    key = jax.random.PRNGKey(seed)\n    return key\n\n", "CodeBleu_score": 18.970487465436996}, {"code": "def get_prng(seed)\n\tif isinstance(seed, int):\n        seed = np.uint32(seed)\n    elif isinstance(seed, np.ndarray):\n        seed = np.uint32(seed)\n    elif isinstance(seed, list):\n        seed = np.uint32(seed)\n    elif isinstance(seed, tuple):\n        seed = np.uint32(seed)\n    else:\n        raise TypeError(f\"Seed must be an integer, numpy array, list or tuple. Received {type(seed)} instead.\")\n    seed = np.right_shift(seed, 0)\n    seed = np.reshape(seed, (-1,))\n    seed = np.asarray(seed, dtype=np.uint32)\n    seed = np.bitwise_and(seed, np.uint32(4294967295))\n    seed1, seed2 = np.split(seed, 2)\n    seed = np.concatenate((seed1, seed2))\n    return seed\n\n", "CodeBleu_score": 32.55495653656831}]}
{"instance_id": "42234dccce9a", "generation_result": [{"code": "def convert_Timestamp(cls, s, dtype, pandas_type)\n\tif isinstance(dtype, pd.DatetimeTZDtype):\n        return s.dt.tz_convert(dtype.tz)\n    elif pdt.is_datetime64_dtype(s):\n        return s.dt.tz_localize(dtype)\n    elif pdt.is_numeric_dtype(s):\n        return pd.to_datetime(s, unit=dtype).dt.tz_localize(dtype)\n    else:\n        return pd.to_datetime(s, utc=True).dt.tz_localize(dtype)\n\n", "CodeBleu_score": 61.46507425744837}, {"code": "def convert_Timestamp(cls, s, dtype, pandas_type)\n\tif dtype == 'datetime64[ns, UTC]':\n        return s.dt.tz_convert(tz='UTC')\n    elif dtype == 'datetime64[ns, CET]':\n        return s.dt.tz_convert(tz='CET')\n    elif dtype == 'datetime64[ns, EST]':\n        return s.dt.tz_convert(tz='EST')\n    elif dtype == 'datetime64[ns, MST]':\n        return s.dt.tz_convert(tz='MST')\n    elif dtype == 'datetime64[ns, PST]':\n        return s.dt.tz_convert(tz='PST')\n    elif dtype == 'datetime64[ns, EET]':\n        return s.dt.tz_convert(tz='EET')\n    elif dtype == 'datetime64[ns, AEST]':\n        return s.dt.tz_convert(tz='AEST')\n    elif dtype == 'datetime64[ns, AEDT]':\n        return s.dt.tz_convert(tz='AEDT')\n    elif dtype == 'datetime64[ns, ACT]':\n        return s.dt.tz_convert(tz='ACT')\n    elif dtype == 'datetime64[ns, AWST]':\n        return s.dt.tz_convert(tz='AWST')\n    elif dtype == 'datetime64[ns, JST]':\n        return s.dt.tz_convert(tz='JST')\n    elif dtype == 'datetime64[ns, NZST]':\n        return s.dt.tz_convert(tz='NZST')\n    elif dtype == 'datetime64[ns, NZDT]':\n        return s.dt.tz_convert(tz='NZDT')\n    elif dtype == 'datetime64[ns, UTC]':\n        return s.dt.tz_localize(tz='UTC')\n    elif dtype == 'datetime64[ns, CET]':\n        return s.dt.tz_localize(tz='CET')\n    elif dtype == 'datetime64[ns, EST]':\n        return s.dt.tz_localize", "CodeBleu_score": 27.989159445062278}, {"code": "def convert_Timestamp(cls, s, dtype, pandas_type)\n\tif dtype == pd.DatetimeTZDtype:\n        return s.dt.tz_convert(pandas_type.tz)\n    elif pd.api.types.is_datetime64_dtype(s):\n        return s.dt.tz_localize(pandas_type.tz)\n    elif pd.api.types.is_numeric_dtype(s):\n        return pd.to_datetime(s, unit=pandas_type.unit).dt.tz_localize(pandas_type.tz)\n    else:\n        return pd.to_datetime(s, utc=True).dt.tz_convert(pandas_type.tz)\n\n", "CodeBleu_score": 56.878395842396}]}
{"instance_id": "9cee7388879b", "generation_result": [{"code": "def visit(cls, op: ops.BetweenTime, arg, lower_bound, upper_bound)\n\tif not isinstance(arg, dd.DataFrame):\n        raise TypeError(\n            \"Expected a Dask DataFrame, got {}\".format(type(arg).__name__)\n        )\n\n    if not is_datetime_like_dtype(arg[op.arg.name].dtype):\n        raise TypeError(\n            \"Expected a datetime-like column, got {}\".format(\n                arg[op.arg.name].dtype\n            )\n        )\n\n    if not is_datetime_like_dtype(lower_bound.dtype):\n        raise TypeError(\n            \"Expected a datetime-like lower bound, got {}\".format(\n                lower_bound.dtype\n            )\n        )\n\n    if not is_datetime_like_dtype(upper_bound.dtype):\n        raise TypeError(\n            \"Expected a datetime-like upper bound, got {}\".format(\n                upper_bound.dtype\n            )\n        )\n\n    if not isinstance(op.left, ir.TimeScalar):\n        raise TypeError(\n            \"Expected a TimeScalar, got {}\".format(type(op.left).__name__)\n        )\n\n    if not isinstance(op.right, ir.TimeScalar):\n        raise TypeError(\n            \"Expected a TimeScalar, got {}\".format(type(op.right).__name__)\n        )\n\n    if not isinstance(op.arg, ir.ColumnExpr):\n        raise TypeError(\n            \"Expected a ColumnExpr, got {}\".format(type(op.arg).__name__)\n        )\n\n    if not isinstance(op.arg.op(), ops.TableColumn):\n        raise TypeError(\n            \"Expected a TableColumn, got {}\".format(\n                type(op.arg.op()).__name__\n            )\n        )\n\n    if not isinstance(op.arg.op().table, ir.Table):\n        raise TypeError(\n            \"Expected a Table, got {}\".format(\n                type(op.arg.op().table).__name__\n            )\n        )\n\n    if not isinstance(op.arg.op().table.op(), ops.", "CodeBleu_score": 19.148712856200262}, {"code": "def visit(cls, op: ops.BetweenTime, arg, lower_bound, upper_bound)\n\tif arg.dtype.tz is None:\n        raise ValueError(\n            \"The data has no timezone information. \"\n            \"Please use tz_localize to localize the data to a timezone.\"\n        )\n    else:\n        arg = arg.dt.tz_convert(\"UTC\")\n        indexer = (arg.dt.hour >= lower_bound) & (arg.dt.hour <= upper_bound)\n        return da.where(indexer, True, False)\n\n", "CodeBleu_score": 28.981003974189523}, {"code": "def visit(cls, op: ops.BetweenTime, arg, lower_bound, upper_bound)\n\tif not op.timezone:\n        raise ValueError(\n            \"BetweenTime operation requires a timezone. None was specified.\"\n        )\n\n    # Convert the data to UTC if necessary\n    if not arg.dtype.tz:\n        arg = arg.dt.tz_localize(op.timezone)\n\n    # Convert the lower and upper bounds to UTC\n    lower_bound = pd.Timestamp(lower_bound, tz=op.timezone)\n    upper_bound = pd.Timestamp(upper_bound, tz=op.timezone)\n\n    # Create a boolean indexer to identify rows within the specified time range\n    indexer = (arg >= lower_bound) & (arg <= upper_bound)\n\n    # Convert the indexer to a Dask array\n    indexer = da.from_array(indexer.values, chunks=arg.chunksize)\n\n    # Convert the indexer back to a Dask DataFrame\n    return dd.from_array(indexer, columns=arg.columns)\n\n", "CodeBleu_score": 34.93876653760817}]}
{"instance_id": "0ab1cc8ccf71", "generation_result": [{"code": "def asseries(cls, value, like=None)\n\tif isinstance(value, da.Array):\n        return dd.from_dask_array(value)\n    elif isinstance(value, dd.Series):\n        return value\n    elif isinstance(value, dd.DataFrame):\n        raise TypeError(\"asseries(cls, value, like=None) cannot convert a Dask DataFrame into a Dask Series.\")\n    elif isinstance(value, da.Scalar):\n        return dd.from_dask_array(value.reshape(1))\n    elif isinstance(value, pd.Series):\n        return dd.from_pandas(value, npartitions=1)\n    elif isinstance(value, pd.DataFrame):\n        raise TypeError(\"asseries(cls, value, like=None) cannot convert a pandas DataFrame into a Dask Series.\")\n    elif isinstance(value, (tuple, list)):\n        if like is None:\n            raise TypeError(\"asseries(cls, value, like=None) cannot convert a list or tuple into a Dask Series without a 'like' object.\")\n        else:\n            return cls(like).map_partitions(lambda x: pd.Series(value))\n    elif isinstance(value, dict):\n        if like is None:\n            raise TypeError(\"asseries(cls, value, like=None) cannot convert a dict into a Dask Series without a 'like' object.\")\n        else:\n            return cls(like).map_partitions(lambda x: pd.Series(value))\n    else:\n        if like is None:\n            return dd.from_pandas(pd.Series([value]), npartitions=1)\n        else:\n            return cls(like).map_partitions(lambda x: pd.Series([value]))", "CodeBleu_score": 37.95615584586666}, {"code": "def asseries(cls, value, like=None)\n\tif isinstance(value, da.Array):\n        return value.to_delayed().map_partitions(pd.Series).from_delayed()\n    elif isinstance(value, dd.Series):\n        return value\n    elif isinstance(value, dd.Scalar):\n        return value.to_delayed().map_partitions(pd.Series).from_delayed()\n    elif isinstance(value, pd.Series):\n        return dd.from_pandas(value, npartitions=1)\n    elif isinstance(value, (list, tuple, dict)):\n        if like is None:\n            return dd.from_pandas(pd.Series(value), npartitions=1)\n        else:\n            if isinstance(like, dd.Series):\n                return dd.from_pandas(pd.Series(value), like.npartitions)\n            elif isinstance(like, dd.DataFrame):\n                return dd.from_pandas(pd.Series(value), like.npartitions)\n            else:\n                raise ValueError(\"The 'like' argument must be a Dask object.\")\n    else:\n        return dd.from_pandas(pd.Series(value), npartitions=1)\n\n", "CodeBleu_score": 38.99992170627582}, {"code": "def asseries(cls, value, like=None)\n\tif isinstance(value, dd.Series):\n        return value\n    if isinstance(value, dd.Scalar):\n        return da.from_delayed(value.to_delayed(), meta=pd.Series([])).to_dask_array().to_dask_dataframe()\n    if isinstance(value, pd.Series):\n        return dd.from_pandas(value, npartitions=1)\n    if isinstance(like, (tuple, list)):\n        return da.from_delayed(value.to_delayed(), meta=pd.Series([])).to_dask_array().to_dask_dataframe()\n    if isinstance(like, dict):\n        return da.from_delayed(value.to_delayed(), meta=pd.Series([])).to_dask_array().to_dask_dataframe()\n    return dd.from_pandas(pd.Series([value]), npartitions=1)\n\n", "CodeBleu_score": 36.19792159038592}]}
{"instance_id": "926220a06a0c", "generation_result": [{"code": "def do_connect(\n        self,\n        dictionary: MutableMapping[str, dd.DataFrame] | None = None,\n    ) -> None\n\tif dictionary is None:\n        dictionary = {}\n\n    for k, v in dictionary.items():\n        if not isinstance(v, (dd.DataFrame, pd.DataFrame)):\n            raise TypeError(\n                f\"Expected an instance of 'dask.dataframe.DataFrame' for {k!r},\"\n                f\" got an instance of '{type(v).__name__}' instead.\"\n            )\n    super().do_connect(dictionary)\n\n", "CodeBleu_score": 96.84942538636408}, {"code": "def do_connect(\n        self,\n        dictionary: MutableMapping[str, dd.DataFrame] | None = None,\n    ) -> None\n\tif dictionary is None:\n        dictionary = {}\n\n    for k, v in dictionary.items():\n        if not isinstance(v, (dd.DataFrame, pd.DataFrame)):\n            raise TypeError(\n                f\"Expected an instance of 'dask.dataframe.DataFrame' for {k!r},\"\n                f\" got an instance of '{type(v).__name__}' instead.\"\n            )\n    super().do_connect(dictionary)", "CodeBleu_score": 96.84942538636408}, {"code": "def do_connect(\n        self,\n        dictionary: MutableMapping[str, dd.DataFrame] | None = None,\n    ) -> None\n\tif dictionary is None:\n        dictionary = {}\n\n    for k, v in dictionary.items():\n        if not isinstance(v, (dd.DataFrame, pd.DataFrame)):\n            raise TypeError(\n                f\"Expected an instance of 'dask.dataframe.DataFrame' for {k!r},\"\n                f\" got an instance of '{type(v).__name__}' instead.\"\n            )\n    super().do_connect(dictionary)", "CodeBleu_score": 96.84942538636408}]}
{"instance_id": "95afc38c9125", "generation_result": [{"code": "def get_estimator(library_name: str, estimator_name: str)\n\tclasses = get_module_members(library_name)\n    if estimator_name not in classes:\n        raise ValueError(f\"{estimator_name} could not be found in {library_name}\")\n    estimators = [x for x in classes if x.lower() == estimator_name.lower()]\n    if len(estimators) > 1:\n        logger.debug(f\"Multiple estimators found: {estimators}\")\n        logger.warning(f\"Multiple estimators found: {estimators}. Using the first one.\")\n        estimator_name = estimators[0]\n    estimator = classes[estimator_name]\n    if not issubclass(estimator, BaseEstimator):\n        logger.info(f\"{estimator_name} is not a subclass of BaseEstimator\")\n    return estimator\n\n", "CodeBleu_score": 42.192134385578605}, {"code": "def get_estimator(library_name: str, estimator_name: str)\n\tclasses = get_module_members(library_name)\n    if estimator_name not in classes:\n        raise ValueError(f\"The estimator {estimator_name} could not be found in the module {library_name}\")\n    estimator_class = classes[estimator_name]\n    matching_classes = [k for k, v in classes.items() if v == estimator_class]\n    if len(matching_classes) > 1:\n        logger.debug(f\"Multiple classes were found for the estimator {estimator_name} in the module {library_name}: {matching_classes}\")\n        logger.warning(f\"Multiple classes were found for the estimator {estimator_name} in the module {library_name}. The first one will be used.\")\n    if not issubclass(estimator_class, BaseEstimator):\n        logger.info(f\"The class {estimator_class} is not a subclass of sklearn's BaseEstimator\")\n    return estimator_class", "CodeBleu_score": 41.681864821917245}, {"code": "def get_estimator(library_name: str, estimator_name: str)\n\tclasses = get_module_members(library_name)\n    if estimator_name not in classes:\n        raise ValueError(f\"{estimator_name} could not be found in {library_name}.\")\n    else:\n        estimator_classes = [c for c in classes.values() if c.__name__ == estimator_name]\n        if len(estimator_classes) > 1:\n            logger.debug(f\"Multiple classes found for {estimator_name}: {estimator_classes}\")\n            logger.warning(f\"Multiple classes found for {estimator_name}. Using the first one.\")\n        estimator_class = estimator_classes[0]\n        if not issubclass(estimator_class, BaseEstimator):\n            logger.info(f\"{estimator_class} is not a subclass of BaseEstimator.\")\n        return estimator_class", "CodeBleu_score": 40.27796995515748}]}
{"instance_id": "9562a9cc043b", "generation_result": [{"code": "def estimator_to_task(estimator_name: str) -> str\n\twith open(abspath(join(dirname(__file__), \"estimator_to_task_map.json\"))) as f:\n        estimator_to_task_map = json.load(f)\n    for postfix, task in estimator_to_task_map.items():\n        if estimator_name.endswith(postfix):\n            return task\n    return \"unknown\"", "CodeBleu_score": 40.52878027525163}, {"code": "def estimator_to_task(estimator_name: str) -> str\n\twith open(abspath(join(dirname(__file__), \"estimator_to_task_map.json\"))) as f:\n        estimator_to_task_map = json.load(f)\n    for postfix, task in estimator_to_task_map.items():\n        if estimator_name.endswith(postfix):\n            return task\n    return \"unknown\"", "CodeBleu_score": 40.52878027525163}, {"code": "def estimator_to_task(estimator_name: str) -> str\n\twith open(abspath(join(dirname(__file__), \"estimator_to_task_map.json\"))) as json_file:\n        estimator_to_task_map = json.load(json_file)\n\n    for estimator_name_postfix, task in estimator_to_task_map.items():\n        if estimator_name.endswith(estimator_name_postfix):\n            return task\n\n    return \"unknown\"", "CodeBleu_score": 42.26312337610067}]}
{"instance_id": "2f50195d043a", "generation_result": [{"code": "def load_data_from_cache(data_cache: str, data_name: str) -> Dict\n\tassert os.path.isdir(data_cache)\n    filenames = get_filenames_by_prefix(data_cache, data_name)\n    data = {}\n    for filename in filenames:\n        component, extension = filename.split(\".\")\n        if extension == \"json\":\n            continue\n        data_filepath = os.path.join(data_cache, filename)\n        data[component] = load_data_file(data_filepath, extension)\n    return data\n\n", "CodeBleu_score": 43.83560788342551}, {"code": "def load_data_from_cache(data_cache: str, data_name: str) -> Dict\n\tdata = {}\n    filenames = get_filenames_by_prefix(data_cache, data_name)\n    for filename in filenames:\n        component, extension = filename.split(\".\")[0].split(\"_\", 1)\n        if extension == \"json\":\n            continue\n        data_filepath = os.path.join(data_cache, filename)\n        data[component] = load_data_file(data_filepath, extension)\n    return data\n\n", "CodeBleu_score": 41.83797106323951}, {"code": "def load_data_from_cache(data_cache: str, data_name: str) -> Dict\n\tdata_files = get_filenames_by_prefix(data_cache, data_name)\n    data = {}\n    for data_file in data_files:\n        data_filename, data_extension = data_file.split(\".\")\n        if data_extension == \"json\":\n            continue\n        component, extension = data_filename.split(\"_\", 1)\n        data[component] = load_data_file(\n            os.path.join(data_cache, data_file), extension\n        )\n    return data\n\n", "CodeBleu_score": 52.5445782655095}]}
{"instance_id": "e0281206029e", "generation_result": [{"code": "def retrieve(url: str, filename: str) -> None\n\tif os.path.isfile(filename):\n        return\n    if not url.startswith(\"http\"):\n        raise ValueError(\"Invalid URL\")\n    response = requests.get(url)\n    if response.status_code != 200:\n        raise AssertionError(f\"Download failed: {response.status_code}\")\n    total_size = int(response.headers.get('content-length', 0))\n    block_size = 1024\n    with open(filename, \"wb\") as datafile:\n        for data in response.iter_content(block_size):\n            datafile.write(data)\n            datafile.flush()\n        datafile.close()\n    if total_size != 0 and os.path.getsize(filename) != total_size:\n        raise AssertionError(\"Download size mismatch\")\n\n", "CodeBleu_score": 52.97620607017246}, {"code": "def retrieve(url: str, filename: str) -> None\n\tif os.path.isfile(filename):\n        return\n\n    if not url.startswith(\"http\"):\n        raise ValueError(\"URL must start with 'http'\")\n\n    response = requests.get(url)\n    assert response.status_code == 200, f\"Download failed: {response.status_code}\"\n    content_length = int(response.headers.get(\"content-length\"))\n    block_size = 1024\n    written = 0\n    with open(filename, \"wb\") as datafile:\n        for data in response.iter_content(block_size):\n            written += len(data)\n            datafile.write(data)\n\n    assert written == content_length, f\"{written} != {content_length}\"", "CodeBleu_score": 45.55068710932794}, {"code": "def retrieve(url: str, filename: str) -> None\n\tif isfile(filename):\n        return\n    if url.startswith(\"http\"):\n        response = requests.get(url)\n        assert response.status_code == 200, f\"Download failed with status code {response.status_code}\"\n        total_size = int(response.headers.get(\"content-length\", 0))\n        block_size = 1024\n        written_size = 0\n        with open(filename, \"wb\") as datafile:\n            for data in response.iter_content(block_size):\n                written_size += len(data)\n                datafile.write(data)\n        assert written_size == total_size, f\"Downloaded size {written_size} does not match expected size {total_size}\"\n\n", "CodeBleu_score": 50.63104652536145}]}
{"instance_id": "a69a71cc2fe8", "generation_result": [{"code": "def load_covtype(\n    data_name: str, data_cache: str, raw_data_cache: str, dataset_params: Dict\n) -> Tuple[Dict, Dict]\n\t# retrieve the dataset\n    X, y = fetch_covtype(data_cache, raw_data_cache, dataset_params)\n\n    # convert the labels to binary\n    if dataset_params.get(\"binary\"):\n        y = y.astype(np.int32)\n        y[y != 2] = 0\n        y[y == 2] = 1\n\n    # convert the labels to zero-indexed\n    y = y - 1\n\n    # create the description dictionary\n    dataset_params[\"num_classes\"] = len(np.unique(y))\n    dataset_params[\"num_features\"] = X.shape[1]\n    dataset_params[\"split_params\"] = {\n        \"val_split\": 0.2,\n        \"test_split\": 0.2,\n        \"num_splits\": 5,\n    }\n    dataset_params[\"y_mean\"] = None\n    dataset_params[\"y_std\"] = None\n\n    return X, y, dataset_params", "CodeBleu_score": 37.79945645511099}, {"code": "def load_covtype(\n    data_name: str, data_cache: str, raw_data_cache: str, dataset_params: Dict\n) -> Tuple[Dict, Dict]\n\t# Load dataset\n    X, y = fetch_covtype(data_cache, raw_data_cache, dataset_params)\n    # Process labels\n    y = y.astype(int)\n    num_classes = len(np.unique(y))\n    if dataset_params.get(\"binary\", False):\n        y = (y == 2).astype(int)\n        num_classes = 2\n    # Create description\n    description = {\n        \"dataset_name\": data_name,\n        \"num_classes\": num_classes,\n        \"split_params\": {\n            \"val_split\": 0.2,\n            \"test_split\": 0.2,\n            \"num_splits\": 5,\n            \"shuffle\": True,\n            \"stratify\": True,\n        },\n    }\n    return X, y, description", "CodeBleu_score": 41.515939501468765}, {"code": "def load_covtype(\n    data_name: str, data_cache: str, raw_data_cache: str, dataset_params: Dict\n) -> Tuple[Dict, Dict]\n\t# load data\n    X, y = fetch_covtype(\n        data_cache,\n        raw_data_cache,\n        dataset_params.get(\"url\"),\n        dataset_params.get(\"filename\"),\n    )\n    # process labels\n    y = y.astype(np.int64)\n    if dataset_params.get(\"binary_target\"):\n        y = y == 2\n    else:\n        y -= 1\n    # create description\n    dataset_params[\"n_classes\"] = len(np.unique(y))\n    dataset_params[\"n_features\"] = X.shape[1]\n    dataset_params[\"n_samples\"] = X.shape[0]\n    dataset_params[\"split_params\"] = {\n        \"split_method\": \"stratified\",\n        \"split_ratio\": [0.8, 0.1, 0.1],\n        \"split_seed\": 42,\n    }\n    dataset_params[\"y_description\"] = {\n        \"task\": \"classification\",\n        \"classes\": [str(i) for i in range(dataset_params[\"n_classes\"])],\n    }\n    dataset_params[\"X_description\"] = {\n        \"features\": [str(i) for i in range(dataset_params[\"n_features\"])]\n    }\n    # return data and description\n    return X, y, dataset_params", "CodeBleu_score": 35.106945924612035}]}
{"instance_id": "129cbd2529ef", "generation_result": [{"code": "def flatten_list(input_list: List, ensure_type_homogeneity: bool = False) -> List\n\toutput_list = list()\n    # iteration with inner recursion\n    for value in input_list:\n        if isinstance(value, list):\n            inner_flat_list = flatten_list(value)\n            for inner_value in inner_flat_list:\n                output_list.append(inner_value)\n        else:\n            output_list.append(value)\n    if ensure_type_homogeneity:\n        ensure_list_types_homogeneity(output_list)\n    return output_list\n\n", "CodeBleu_score": 81.18553468541374}, {"code": "def flatten_list(input_list: List, ensure_type_homogeneity: bool = False) -> List\n\toutput_list = list()\n    # iteration with inner recursion\n    for value in input_list:\n        if isinstance(value, list):\n            inner_flat_list = flatten_list(value)\n            for inner_value in inner_flat_list:\n                output_list.append(inner_value)\n        else:\n            output_list.append(value)\n    if ensure_type_homogeneity:\n        ensure_list_types_homogeneity(output_list)\n    return output_list\n\n", "CodeBleu_score": 81.18553468541374}, {"code": "def flatten_list(input_list: List, ensure_type_homogeneity: bool = False) -> List\n\toutput_list = list()\n    # iteration with inner recursion\n    for value in input_list:\n        if isinstance(value, list):\n            inner_flat_list = flatten_list(value)\n            for inner_value in inner_flat_list:\n                output_list.append(inner_value)\n        else:\n            output_list.append(value)\n    if ensure_type_homogeneity:\n        ensure_list_types_homogeneity(output_list)\n    return output_list\n\n", "CodeBleu_score": 81.18553468541374}]}
{"instance_id": "2d38c2c60b59", "generation_result": [{"code": "def logit(\n    x: np.ndarray,\n    /,\n    *,\n    eps: Optional[float] = None,\n    complex_mode: Literal[\"split\", \"magnitude\", \"jax\"] = \"jax\",\n    out: Optional[np.ndarray] = None,\n)\n\tif eps is None:\n        x = np.clip(x, 0.0, 1.0)\n    else:\n        x = np.clip(x, eps, 1 - eps)\n    if complex_mode == \"split\":\n        return np.log(x / (1 - x))\n    elif complex_mode == \"magnitude\":\n        return np.log(x / (1 - x))\n    elif complex_mode == \"jax\":\n        return np.log(x / (1 - x))\n    else:\n        raise ValueError(f\"complex_mode must be one of ['split', 'magnitude', 'jax']\")\n\n", "CodeBleu_score": 43.695169590553895}, {"code": "def logit(\n    x: np.ndarray,\n    /,\n    *,\n    eps: Optional[float] = None,\n    complex_mode: Literal[\"split\", \"magnitude\", \"jax\"] = \"jax\",\n    out: Optional[np.ndarray] = None,\n)\n\t...\n\n", "CodeBleu_score": 15.592515592515593}, {"code": "def logit(\n    x: np.ndarray,\n    /,\n    *,\n    eps: Optional[float] = None,\n    complex_mode: Literal[\"split\", \"magnitude\", \"jax\"] = \"jax\",\n    out: Optional[np.ndarray] = None,\n)\n\tif eps is None:\n        eps = 0\n    if complex_mode not in [\"split\", \"magnitude\", \"jax\"]:\n        raise ValueError(\n            \"complex_mode must be one of 'split', 'magnitude' or 'jax'.\"\n        )\n    if out is None:\n        out = np.zeros(x.shape, dtype=x.dtype)\n    if np.isscalar(x):\n        if np.iscomplex(x):\n            if complex_mode == \"split\":\n                out = np.log(x.real + eps) - np.log(1 - x.real + eps)\n            elif complex_mode == \"magnitude\":\n                out = np.log(np.abs(x) + eps) - np.log(1 - np.abs(x) + eps)\n            elif complex_mode == \"jax\":\n                out = np.log(x.real + eps) - np.log(1 - x.real + eps)\n                out = out + 1j * np.angle(x)\n        else:\n            out = np.log(x + eps) - np.log(1 - x + eps)\n    else:\n        if np.iscomplexobj(x):\n            if complex_mode == \"split\":\n                out = np.log(x.real + eps) - np.log(1 - x.real + eps)\n            elif complex_mode == \"magnitude\":\n                out = np.log(np.abs(x) + eps) - np.log(1 - np.abs(x) + eps)\n            elif complex_mode == \"jax\":\n                out = np.log(x.real + eps) - np.log(1 - x.real + eps)\n                out = out + 1j * np.angle(x)\n        else:\n            out = np.log(x + eps) - np.log(1 - x + eps)\n    return out.astype(x.dtype)\n\n", "CodeBleu_score": 37.49380930884848}]}
{"instance_id": "205e296f8621", "generation_result": [{"code": "def hardshrink(\n    x: np.ndarray, /, *, lambd: float = 0.5, out: Optional[np.ndarray] = None\n) -> np.ndarray\n\tif out is None:\n        return np.where(np.abs(x) <= lambd, 0, x)\n    np.where(np.abs(x) <= lambd, 0, x, out=out)\n    return out\n\n", "CodeBleu_score": 38.56552042352573}, {"code": "def hardshrink(\n    x: np.ndarray, /, *, lambd: float = 0.5, out: Optional[np.ndarray] = None\n) -> np.ndarray\n\tif ivy.exists(out):\n        return ivy.inplace_update(out, x, np.where(x < lambd, x, 0))\n    return np.where(x < lambd, x, 0)\n\n", "CodeBleu_score": 45.46391090279786}, {"code": "def hardshrink(\n    x: np.ndarray, /, *, lambd: float = 0.5, out: Optional[np.ndarray] = None\n) -> np.ndarray\n\tif ivy.exists(out):\n        return ivy.inplace_update(out, np.where(x > lambd, x, 0))\n    return np.where(x > lambd, x, 0)\n\n", "CodeBleu_score": 46.73966350104276}]}
{"instance_id": "5b133f6ebe2d", "generation_result": [{"code": "def vorbis_window(\n    window_length: np.ndarray,\n    *,\n    dtype: np.dtype = np.float32,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n\tif out is None:\n        out = np.empty(window_length, dtype=dtype)\n    else:\n        assert out.dtype == dtype\n        assert len(out) == window_length\n\n    for n in range(window_length):\n        out[n] = np.sin(np.pi * np.sin(np.pi * 2.0 * n / window_length) ** 2)\n\n    return out\n\n", "CodeBleu_score": 35.81784493457407}, {"code": "def vorbis_window(\n    window_length: np.ndarray,\n    *,\n    dtype: np.dtype = np.float32,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n\tif out is None:\n        out = np.empty(window_length, dtype=dtype)\n    else:\n        assert out.dtype == dtype\n        assert out.shape[0] == window_length\n    window_length_minus_one = window_length - 1\n    for i in range(window_length):\n        out[i] = round(\n            (np.sin(np.pi * np.sin(np.pi * i / window_length_minus_one) ** 2) ** 2)\n            * (1 - (i / window_length_minus_one) ** 2),\n            5,\n        )\n    return out\n\n", "CodeBleu_score": 36.35305966696984}, {"code": "def vorbis_window(\n    window_length: np.ndarray,\n    *,\n    dtype: np.dtype = np.float32,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n\t# Initialize the output array if not provided\n    if out is None:\n        out = np.empty(window_length, dtype=dtype)\n\n    # Generate the window\n    for i in range(window_length):\n        out[i] = np.sin(np.pi * np.sin(np.pi * (i / window_length)) / 2)\n\n    # Round the window values and return the array\n    return np.round(out)", "CodeBleu_score": 40.66511991562227}]}
{"instance_id": "473296f771a9", "generation_result": [{"code": "def polyval(coeffs: np.ndarray, x: np.ndarray) -> np.ndarray\n\twith ivy.PreciseMode(True):\n        promoted_type = ivy.promote_types(ivy.dtype(coeffs[0]), ivy.dtype(x[0]))\n    result = np.polyval(coeffs, x)\n    result = np.asarray(result, np.dtype(promoted_type))\n    return result\n\n", "CodeBleu_score": 94.04270794741467}, {"code": "def polyval(coeffs: np.ndarray, x: np.ndarray) -> np.ndarray\n\twith ivy.PreciseMode(True):\n        promoted_type = ivy.promote_types(ivy.dtype(coeffs[0]), ivy.dtype(x[0]))\n    result = np.polyval(coeffs, x)\n    result = np.asarray(result, np.dtype(promoted_type))\n    return result", "CodeBleu_score": 94.04270794741467}, {"code": "def polyval(coeffs: np.ndarray, x: np.ndarray) -> np.ndarray\n\twith ivy.PreciseMode(True):\n        promoted_type = ivy.promote_types(ivy.dtype(coeffs[0]), ivy.dtype(x[0]))\n    result = np.polyval(coeffs, x)\n    result = np.asarray(result, np.dtype(promoted_type))\n    return result\n\n", "CodeBleu_score": 94.04270794741467}]}
{"instance_id": "b8455221cb2a", "generation_result": [{"code": "def amax(\n    x: np.ndarray,\n    /,\n    *,\n    axis: Optional[Union[int, Sequence[int]]] = None,\n    keepdims: bool = False,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n\taxis = tuple(axis) if isinstance(axis, list) else axis\n    ret = np.amax(a=x, axis=axis, out=out, keepdims=keepdims)\n    return np.asarray(ret) if np.isscalar(ret) else ret\n\n", "CodeBleu_score": 77.68208199905418}, {"code": "def amax(\n    x: np.ndarray,\n    /,\n    *,\n    axis: Optional[Union[int, Sequence[int]]] = None,\n    keepdims: bool = False,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n\taxis = tuple(axis) if isinstance(axis, list) else axis\n    ret = np.amax(a=x, axis=axis, out=out, keepdims=keepdims)\n    return np.asarray(ret) if np.isscalar(ret) else ret\n\n", "CodeBleu_score": 77.68208199905418}, {"code": "def amax(\n    x: np.ndarray,\n    /,\n    *,\n    axis: Optional[Union[int, Sequence[int]]] = None,\n    keepdims: bool = False,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n\taxis = tuple(axis) if isinstance(axis, list) else axis\n    ret = np.amax(a=x, axis=axis, out=out, keepdims=keepdims)\n    return np.asarray(ret) if np.isscalar(ret) else ret\n\n", "CodeBleu_score": 77.68208199905418}]}
{"instance_id": "f72489f434a3", "generation_result": [{"code": "def copysign(\n    x1: npt.ArrayLike,\n    x2: npt.ArrayLike,\n    /,\n    *,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n\treturn np.copysign(\n        x1, x2, out=out\n    )  # type: ignore\n\n", "CodeBleu_score": 21.199066523163996}, {"code": "def copysign(\n    x1: npt.ArrayLike,\n    x2: npt.ArrayLike,\n    /,\n    *,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n\treturn np.copysign(\n        x1, x2, out=out\n    )  # type: ignore[return-value] # noqa\n\n", "CodeBleu_score": 22.454127032819464}, {"code": "def copysign(\n    x1: npt.ArrayLike,\n    x2: npt.ArrayLike,\n    /,\n    *,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n\treturn np.copysign(\n        x1, x2, out=out\n    )  # type: ignore\n\n", "CodeBleu_score": 21.199066523163996}]}
{"instance_id": "55c0d77e4dc2", "generation_result": [{"code": "def count_nonzero(\n    a: np.ndarray,\n    /,\n    *,\n    axis: Optional[Union[int, Tuple[int, ...]]] = None,\n    keepdims: bool = False,\n    dtype: Optional[np.dtype] = None,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n\tif axis is not None:\n        axis = tuple(axis)\n    if out is None:\n        out = np.count_nonzero(a, axis=axis, keepdims=keepdims, dtype=dtype)\n    else:\n        np.count_nonzero(a, axis=axis, keepdims=keepdims, dtype=dtype, out=out)\n    if np.isscalar(out):\n        if dtype is None:\n            dtype = np.int64\n        out = np.array(out, dtype=dtype)\n    return out\n\n", "CodeBleu_score": 58.4911535595827}, {"code": "def count_nonzero(\n    a: np.ndarray,\n    /,\n    *,\n    axis: Optional[Union[int, Tuple[int, ...]]] = None,\n    keepdims: bool = False,\n    dtype: Optional[np.dtype] = None,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n\ta = np.array(a, copy=False)\n    if axis is None:\n        a = a.ravel()\n        axis = 0\n    elif isinstance(axis, int):\n        axis = (axis,)\n    if dtype is None:\n        dtype = np.intp\n    if out is None:\n        out = np.count_nonzero(a, axis=axis, keepdims=keepdims, dtype=dtype)\n    else:\n        np.count_nonzero(a, axis=axis, keepdims=keepdims, dtype=dtype, out=out)\n    if np.isscalar(out):\n        out = np.array(out, dtype=dtype)\n    return out\n\n", "CodeBleu_score": 52.9448285408799}, {"code": "def count_nonzero(\n    a: np.ndarray,\n    /,\n    *,\n    axis: Optional[Union[int, Tuple[int, ...]]] = None,\n    keepdims: bool = False,\n    dtype: Optional[np.dtype] = None,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n\tif dtype is None:\n        dtype = np.int64\n    if out is not None:\n        if out.dtype != dtype:\n            raise TypeError(\"out must have the same dtype as the output\")\n        if out.shape != ():\n            raise TypeError(\"out must be a scalar\")\n    if not isinstance(a, np.ndarray):\n        raise TypeError(\"a must be an array\")\n    if axis is not None:\n        if not isinstance(axis, tuple):\n            axis = (axis,)\n        if not all(isinstance(x, int) for x in axis):\n            raise TypeError(\"axis must contain integers\")\n        if not all(x >= 0 for x in axis):\n            raise ValueError(\"axis must contain non-negative integers\")\n        if not all(x < a.ndim for x in axis):\n            raise ValueError(\"axis must contain integers smaller than a.ndim\")\n        if len(axis) != len(set(axis)):\n            raise ValueError(\"axis must not contain duplicate integers\")\n    if not isinstance(keepdims, bool):\n        raise TypeError(\"keepdims must be a boolean\")\n    if not isinstance(dtype, np.dtype):\n        raise TypeError(\"dtype must be a dtype\")\n    if out is not None and not isinstance(out, np.ndarray):\n        raise TypeError(\"out must be an array\")\n    if out is not None and out.dtype != dtype:\n        raise TypeError(\"out must have the same dtype as the output\")\n    if out is not None and out.shape != ():\n        raise TypeError(\"out must be a scalar\")\n    if not isinstance(a, np.ndarray):\n        raise TypeError(\"a must be an array\")\n    if axis is not None:\n        if not isinstance(axis, tuple):\n            axis = (axis,)\n        if not all(isinstance(x, int) for x in axis):\n            raise TypeError(\"axis must contain integers\")\n        if not all(x >= 0 for x in axis):\n            raise ValueError(\"axis must contain non-negative integers\")\n        if not all(x < a.ndim for x in axis):\n            raise ValueError(\"axis must contain", "CodeBleu_score": 42.80109134084691}]}
{"instance_id": "784bf7d24fc2", "generation_result": [{"code": "def gradient(\n    x: np.ndarray,\n    /,\n    *,\n    spacing: Union[int, list, tuple] = 1,\n    axis: Optional[Union[int, list, tuple]] = None,\n    edge_order: int = 1,\n) -> Union[np.ndarray, List[np.ndarray]]\n\tif type(spacing) in (int, float):\n        return np.gradient(x, spacing, axis=axis, edge_order=edge_order)\n    return np.gradient(x, *spacing, axis=axis, edge_order=edge_order)\n\n", "CodeBleu_score": 95.90395527731977}, {"code": "def gradient(\n    x: np.ndarray,\n    /,\n    *,\n    spacing: Union[int, list, tuple] = 1,\n    axis: Optional[Union[int, list, tuple]] = None,\n    edge_order: int = 1,\n) -> Union[np.ndarray, List[np.ndarray]]\n\tif type(spacing) in (int, float):\n        return np.gradient(x, spacing, axis=axis, edge_order=edge_order)\n    return np.gradient(x, *spacing, axis=axis, edge_order=edge_order)\n\n", "CodeBleu_score": 95.90395527731977}, {"code": "def gradient(\n    x: np.ndarray,\n    /,\n    *,\n    spacing: Union[int, list, tuple] = 1,\n    axis: Optional[Union[int, list, tuple]] = None,\n    edge_order: int = 1,\n) -> Union[np.ndarray, List[np.ndarray]]\n\tif type(spacing) in (int, float):\n        return np.gradient(x, spacing, axis=axis, edge_order=edge_order)\n    return np.gradient(x, *spacing, axis=axis, edge_order=edge_order)\n\n", "CodeBleu_score": 95.90395527731977}]}
{"instance_id": "efa0a51af6bc", "generation_result": [{"code": "def xlogy(\n    x: np.ndarray, y: np.ndarray, /, *, out: Optional[np.ndarray] = None\n) -> np.ndarray\n\tx, y = promote_types_of_inputs(x, y)\n    if out is None:\n        out = np.empty_like(x)\n    np.multiply(np.log(y), x, out=out)\n    return out\n\n", "CodeBleu_score": 55.836862542724916}, {"code": "def xlogy(\n    x: np.ndarray, y: np.ndarray, /, *, out: Optional[np.ndarray] = None\n) -> np.ndarray\n\tif out is None:\n        out = np.empty_like(x)\n    x, y = np.broadcast_arrays(x, y)\n    if x.size == 0:\n        return out\n    if np.all(x == 0):\n        out.fill(0.0)\n        return out\n    x, y = promote_types_of_inputs(x, y)\n    np.log(y, out=out)\n    out *= x\n    return out\n\n", "CodeBleu_score": 53.59878163438777}, {"code": "def xlogy(\n    x: np.ndarray, y: np.ndarray, /, *, out: Optional[np.ndarray] = None\n) -> np.ndarray\n\tx, y = promote_types_of_inputs(x, y)\n    if out is None:\n        out = np.zeros(x.shape, dtype=x.dtype)\n    else:\n        out = np.asarray(out)\n    if np.all(x == 0):\n        return 0.0\n    else:\n        return np.multiply(x, np.log(y, out=out), out=out)\n\n", "CodeBleu_score": 58.03986233061642}]}
{"instance_id": "ac1645284260", "generation_result": [{"code": "def _determine_depth_max_pooling(x, kernel, strides, dims, data_format=\"channel_last\")\n\t# Determine depth pooling.\n    # We assume that the kernel and the data have the same data_format.\n    depth_pooling = False\n    CHANNEL_LAST = \"channel_last\"\n    channel_idx = -1 if data_format == CHANNEL_LAST else 1\n    if len(kernel) == dims + 2:\n        spatial_kernel = kernel[1:-1] if data_format == CHANNEL_LAST else kernel[2:]\n        if kernel[channel_idx] != 1:\n            depth_pooling = True\n            if any(i != 1 for i in spatial_kernel):\n                raise NotImplementedError(\n                    \"MaxPooling supports exactly one of pooling across\"\n                    \" depth or pooling across width/height.\"\n                )\n            if len(strides) != dims + 2 or strides[channel_idx] != kernel[channel_idx]:\n                raise NotImplementedError(\n                    \"Depthwise max pooling requires the depth window to equal the depth\"\n                    \" stride\"\n                )\n            if x.shape[channel_idx] % kernel[channel_idx] != 0:\n                raise NotImplementedError(\n                    \"Depthwise max pooling requires the depth window to evenly divide\"\n                    \" the input depth\"\n                )\n            kernel = [kernel[channel_idx], *[1] * (dims - 1)]\n            strides = [strides[channel_idx], *[1] * (dims - 1)]\n        else:\n            kernel = spatial_kernel\n            if len(strides) == dims + 2:\n                strides = strides[1:-1] if data_format == CHANNEL_LAST else strides[2:]\n    return kernel, strides, depth_pooling\n\n", "CodeBleu_score": 25.007791313961913}, {"code": "def _determine_depth_max_pooling(x, kernel, strides, dims, data_format=\"channel_last\")\n\tkernel, strides, depth_pooling = _depth_max_pooling_helper(\n        x, kernel, strides, dims, data_format\n    )\n    if depth_pooling:\n        if data_format == \"channel_first\":\n            x = np.transpose(x, [0, 2] + list(range(3, dims + 2)) + [1])\n        else:\n            x = np.transpose(x, [0] + list(range(2, dims + 1)) + [1, dims + 1])\n    return x, kernel, strides, depth_pooling\n\n", "CodeBleu_score": 53.575191768799066}, {"code": "def _determine_depth_max_pooling(x, kernel, strides, dims, data_format=\"channel_last\")\n\t# Determine depth pooling.\n    # We assume that the kernel and the data have the same data_format.\n    depth_pooling = False\n    CHANNEL_LAST = \"channel_last\"\n    channel_idx = -1 if data_format == CHANNEL_LAST else 1\n    if len(kernel) == dims + 2:\n        spatial_kernel = kernel[1:-1] if data_format == CHANNEL_LAST else kernel[2:]\n        if kernel[channel_idx] != 1:\n            depth_pooling = True\n            if any(i != 1 for i in spatial_kernel):\n                raise NotImplementedError(\n                    \"MaxPooling supports exactly one of pooling across\"\n                    \" depth or pooling across width/height.\"\n                )\n            if len(strides) != dims + 2 or strides[channel_idx] != kernel[channel_idx]:\n                raise NotImplementedError(\n                    \"Depthwise max pooling requires the depth window to equal the depth\"\n                    \" stride\"\n                )\n            if x.shape[channel_idx] % kernel[channel_idx] != 0:\n                raise NotImplementedError(\n                    \"Depthwise max pooling requires the depth window to evenly divide\"\n                    \" the input depth\"\n                )\n            kernel = [kernel[channel_idx], *[1] * (dims - 1)]\n            strides = [strides[channel_idx], *[1] * (dims - 1)]\n        else:\n            kernel = spatial_kernel\n            if len(strides) == dims + 2:\n                strides = strides[1:-1] if data_format == CHANNEL_LAST else strides[2:]\n    return kernel, strides, depth_pooling\n\n", "CodeBleu_score": 25.007791313961913}]}
{"instance_id": "18ff0a4f7550", "generation_result": [{"code": "def dropout1d(\n    x: np.ndarray,\n    prob: float,\n    /,\n    *,\n    training: bool = True,\n    data_format: str = \"NWC\",\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n\tif not training:\n        return x\n\n    if data_format not in (\"NWC\", \"NCW\"):\n        raise ValueError(\"data_format must be either \\\"NWC\\\" or \\\"NCW\\\".\")\n\n    if out is None:\n        out = np.empty_like(x)\n\n    if data_format == \"NCW\":\n        x = np.transpose(x, (0, 2, 1))\n\n    if x.ndim == 3:\n        batched = True\n        shape = x.shape\n        x = x.reshape(-1, shape[-1])\n    else:\n        batched = False\n        shape = x.shape\n\n    mask = np.random.binomial(1, 1 - prob, x.shape)\n    out = x * mask / (1 - prob)\n\n    if batched:\n        out = out.reshape(shape)\n\n    if data_format == \"NCW\":\n        out = np.transpose(out, (0, 2, 1))\n\n    return out\n\n", "CodeBleu_score": 51.14608755099119}, {"code": "def dropout1d(\n    x: np.ndarray,\n    prob: float,\n    /,\n    *,\n    training: bool = True,\n    data_format: str = \"NWC\",\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n\tif not training:\n        return x\n    if data_format == \"NCW\":\n        x = np.transpose(x, (0, 2, 1))\n    if out is None:\n        out = np.empty_like(x)\n    if x.ndim == 2:\n        out = binomial(n=1, p=1 - prob, size=x.shape)\n    elif x.ndim == 3:\n        out = binomial(n=1, p=1 - prob, size=x.shape[:2] + (1,))\n    out = x * out / (1 - prob)\n    if data_format == \"NCW\":\n        out = np.transpose(out, (0, 2, 1))\n    return out\n\n", "CodeBleu_score": 47.359680780826665}, {"code": "def dropout1d(\n    x: np.ndarray,\n    prob: float,\n    /,\n    *,\n    training: bool = True,\n    data_format: str = \"NWC\",\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n\tif not training:\n        return x\n\n    if data_format == \"NCW\":\n        x = np.transpose(x, (0, 2, 1))\n\n    if out is None:\n        out = np.empty_like(x)\n\n    mask = np.random.binomial(1, prob, size=x.shape)\n    out = x * (1 / (1 - prob)) * mask\n    out[mask == 0] = 0\n\n    if data_format == \"NCW\":\n        out = np.transpose(out, (0, 2, 1))\n\n    return out\n\n", "CodeBleu_score": 43.64941560057034}]}
{"instance_id": "0ce45eb7cbd9", "generation_result": [{"code": "def top_k(\n    x: np.ndarray,\n    k: int,\n    /,\n    *,\n    axis: int = -1,\n    largest: bool = True,\n    sorted: bool = True,\n    out: Optional[Tuple[np.ndarray, np.ndarray]] = None,\n) -> Tuple[np.ndarray, np.ndarray]\n\tx = np.asarray(x)\n    if axis < 0:\n        axis = x.ndim + axis\n    if axis < 0 or axis >= x.ndim:\n        raise ValueError(f\"invalid axis: {axis}\")\n    if k < 1:\n        raise ValueError(f\"invalid k: {k}\")\n    if k > x.shape[axis]:\n        raise ValueError(f\"invalid k: {k} > {x.shape[axis]}\")\n    if x.size == 0:\n        raise ValueError(\"input array cannot be empty\")\n    if out is not None:\n        if len(out) != 2:\n            raise ValueError(\"out must be a tuple of two arrays\")\n        if out[0].shape != x.shape:\n            raise ValueError(\"out[0] must have the same shape as x\")\n        if out[1].shape != x.shape:\n            raise ValueError(\"out[1] must have the same shape as x\")\n    if largest:\n        idx = np.argpartition(x, -k, axis=axis)\n        idx = idx[..., -k:]\n    else:\n        idx = np.argpartition(x, k - 1, axis=axis)\n        idx = idx[..., :k]\n    idx = np.take_along_axis(idx, np.argsort(x.take_along_axis(idx, axis=axis), axis=axis), axis=axis)\n    if sorted:\n        idx = np.take_along_axis(idx, np.argsort(idx, axis=axis), axis=axis)\n    if out is not None:\n        out[0][...] = x.take_along_axis(idx, axis=axis)\n        out[1][...] = idx\n        return out\n    return x.take_along_axis(idx, axis=axis), idx", "CodeBleu_score": 38.76306466908682}, {"code": "def top_k(\n    x: np.ndarray,\n    k: int,\n    /,\n    *,\n    axis: int = -1,\n    largest: bool = True,\n    sorted: bool = True,\n    out: Optional[Tuple[np.ndarray, np.ndarray]] = None,\n) -> Tuple[np.ndarray, np.ndarray]\n\tif out is None:\n        out = (np.empty_like(x), np.empty(x.shape[axis], dtype=np.intp))\n    if axis < 0:\n        axis += x.ndim\n    if largest:\n        indices = np.argpartition(x, -k, axis=axis)[(-k,)]\n    else:\n        indices = np.argpartition(x, k, axis=axis)[:k]\n    np.take_along_axis(x, indices, axis=axis, out=out[0])\n    out[1].flat = np.take(indices, np.argsort(np.take(x, indices, axis=axis), axis=axis), axis=axis)\n    if sorted:\n        np.sort(out[0], axis=axis)\n    return out\n\n", "CodeBleu_score": 46.92702424152929}, {"code": "def top_k(\n    x: np.ndarray,\n    k: int,\n    /,\n    *,\n    axis: int = -1,\n    largest: bool = True,\n    sorted: bool = True,\n    out: Optional[Tuple[np.ndarray, np.ndarray]] = None,\n) -> Tuple[np.ndarray, np.ndarray]\n\tx = np.asanyarray(x)\n    if axis < 0:\n        axis += x.ndim\n    if axis < 0 or axis >= x.ndim:\n        raise ValueError(f\"axis={axis} out of bounds\")\n    if k < 1:\n        raise ValueError(\"k must be at least 1\")\n    if k > x.shape[axis]:\n        raise ValueError(f\"k={k} exceeds {x.shape[axis]} along axis={axis}\")\n    if x.size == 0:\n        return np.take(x, [], axis=axis), np.array([], dtype=np.intp)\n    if out is not None:\n        if len(out) != 2:\n            raise ValueError(\"out must have length 2\")\n        if out[0] is None:\n            out = out[1:]\n        if len(out) != 1:\n            raise ValueError(\"out must have length 1\")\n        if out[0] is None:\n            out = None\n    if largest:\n        index = np.argpartition(x, -k, axis=axis)[..., -k:]\n        top = np.take_along_axis(x, index, axis=axis)\n        topk_res = np.argpartition(top, k - 1, axis=axis)[..., :k]\n    else:\n        index = np.argpartition(x, k - 1, axis=axis)[..., :k]\n        top = np.take_along_axis(x, index, axis=axis)\n        topk_res = np.argpartition(top, k - 1, axis=axis)[..., :k]\n    if sorted:\n        sort_indices = np.argsort(top, axis=axis)\n        topk_res = np.take_along_axis(topk_res, sort_indices, axis=axis)\n        index = np.take_along_axis(index, sort_indices, axis=axis)\n    if out is not None:\n        out[0][...] = np.take_along_axis(x, topk_res, axis=axis)\n        out[1", "CodeBleu_score": 42.006097700297616}]}
{"instance_id": "1424f9541998", "generation_result": [{"code": "def l1_normalize(\n    x: np.ndarray,\n    /,\n    *,\n    axis: Optional[int] = None,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n\tnorm = np.abs(x).sum(axis=axis, keepdims=True)\n    if out is not None:\n        out[:] = x / (norm + 1e-8)\n    else:\n        out = x / (norm + 1e-8)\n    return out\n\n", "CodeBleu_score": 28.945974252542733}, {"code": "def l1_normalize(\n    x: np.ndarray,\n    /,\n    *,\n    axis: Optional[int] = None,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n\tif axis is None:\n        norm = np.abs(x).reshape(-1).sum()\n        denorm = np.ones_like(x) * norm\n    else:\n        norm = np.abs(x).sum(axis=axis, keepdims=True)\n        denorm = np.divide(norm, np.abs(x) + 1e-8)\n    if out is None:\n        out = np.divide(x, denorm)\n    else:\n        np.divide(x, denorm, out=out)\n    return out", "CodeBleu_score": 62.95322442097952}, {"code": "def l1_normalize(\n    x: np.ndarray,\n    /,\n    *,\n    axis: Optional[int] = None,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n\tif out is None:\n        out = np.empty_like(x)\n    if axis is None:\n        out[:] = x / (np.sum(np.abs(x)) + 1e-12)\n    else:\n        out[:] = x / (np.sum(np.abs(x), axis=axis, keepdims=True) + 1e-12)\n    return out\n\n", "CodeBleu_score": 45.278234725777935}]}
{"instance_id": "4c3a386a1f05", "generation_result": [{"code": "def beta(\n    alpha: Union[float, np.ndarray],\n    beta: Union[float, np.ndarray],\n    /,\n    *,\n    shape: Optional[Union[ivy.NativeShape, Sequence[int]]] = None,\n    device: Optional[str] = None,\n    dtype: Optional[np.dtype] = None,\n    seed: Optional[int] = None,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n\tshape = _check_bounds_and_get_shape(alpha, beta, shape).shape\n    if seed is not None:\n        np.random.seed(seed)\n    return np.asarray(np.random.beta(alpha, beta, shape), dtype=dtype)\n\n", "CodeBleu_score": 89.70998520485102}, {"code": "def beta(\n    alpha: Union[float, np.ndarray],\n    beta: Union[float, np.ndarray],\n    /,\n    *,\n    shape: Optional[Union[ivy.NativeShape, Sequence[int]]] = None,\n    device: Optional[str] = None,\n    dtype: Optional[np.dtype] = None,\n    seed: Optional[int] = None,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n\tshape = _check_bounds_and_get_shape(alpha, beta, shape).shape\n    if seed is not None:\n        np.random.seed(seed)\n    return np.asarray(np.random.beta(alpha, beta, shape), dtype=dtype)\n\n", "CodeBleu_score": 89.70998520485102}, {"code": "def beta(\n    alpha: Union[float, np.ndarray],\n    beta: Union[float, np.ndarray],\n    /,\n    *,\n    shape: Optional[Union[ivy.NativeShape, Sequence[int]]] = None,\n    device: Optional[str] = None,\n    dtype: Optional[np.dtype] = None,\n    seed: Optional[int] = None,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n\tshape = _check_bounds_and_get_shape(alpha, beta, shape).shape\n    if seed is not None:\n        np.random.seed(seed)\n    return np.asarray(np.random.beta(alpha, beta, shape), dtype=dtype)\n\n", "CodeBleu_score": 89.70998520485102}]}
{"instance_id": "271200a8b261", "generation_result": [{"code": "def invert_permutation(\n    x: Union[np.ndarray, list, tuple],\n    /,\n) -> np.ndarray\n\tx = np.array(x)\n    sorted_indices = np.argsort(x)\n    inverse = np.zeros_like(sorted_indices)\n    inverse[sorted_indices] = np.arange(len(x))\n    return inverse.astype(int)\n\n", "CodeBleu_score": 57.487966223994036}, {"code": "def invert_permutation(\n    x: Union[np.ndarray, list, tuple],\n    /,\n) -> np.ndarray\n\tx_sorted = np.argsort(x)\n    inverse = np.zeros_like(x_sorted)\n    inverse[x_sorted] = np.arange(len(x_sorted))\n    return np.argsort(inverse)\n\n", "CodeBleu_score": 43.759632626522574}, {"code": "def invert_permutation(\n    x: Union[np.ndarray, list, tuple],\n    /,\n) -> np.ndarray\n\tif not isinstance(x, (np.ndarray, list, tuple)):\n        raise TypeError(\n            f\"x must be an array-like object, not {type(x).__name__}\"\n        )\n    x = np.array(x)\n    if x.ndim > 1:\n        raise ValueError(\"x must be a 1-dimensional array\")\n    if x.size == 0:\n        return np.array([])\n    sorted_indices = np.argsort(x)\n    inverse = np.zeros_like(sorted_indices)\n    inverse[sorted_indices] = np.arange(len(x))\n    return inverse.astype(int)\n\n", "CodeBleu_score": 63.77902049706976}]}
{"instance_id": "040d73b37229", "generation_result": [{"code": "def plot_medical_charts() -> None\n\t# Set style\n    sns.set(style=\"darkgrid\")\n\n    # Create figure with subplots\n    fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n\n    # Plot data\n    plot_monthly_vomit(axes[0, 0], medical_data)\n    plot_days_between_vomit(axes[0, 1], medical_data)\n    plot_daycare_days(axes[1, 0], medical_data)\n    plot_doctor_visit_monthly(axes[1, 1], medical_data)\n\n    # Adjust layout\n    fig.subplots_adjust(hspace=0.4, wspace=0.4)\n\n    # Export figure\n    export_figure(fig, \"medical_charts.png\")\n\n", "CodeBleu_score": 30.46834049678564}, {"code": "def plot_medical_charts() -> None\n\t# Configure plotting environment\n    sns.set(style=\"whitegrid\")\n    fig, axes = plt.subplots(2, 2, figsize=(12, 12))\n    register_matplotlib_converters()\n\n    # Read data\n    data = pd.read_csv(\"data/medical.csv\")\n\n    # Plot\n    plot_monthly_vomit(axes[0, 0], data)\n    plot_days_between_vomit(axes[0, 1], data)\n    plot_daycare_days(axes[1, 0], data)\n    plot_doctor_visit_monthly(axes[1, 1], data)\n\n    # Adjust layout\n    fig.subplots_adjust(hspace=0.4)\n\n    # Export figure\n    export_figure(fig, \"medical_charts.png\")\n\n", "CodeBleu_score": 31.56595798384776}, {"code": "def plot_medical_charts() -> None\n\t# Set plotting style\n    sns.set(style=\"darkgrid\")\n\n    # Create figure with subplots\n    fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n\n    # Plot data\n    plot_monthly_vomit(axes[0, 0], data)\n    plot_days_between_vomit(axes[0, 1], data)\n    plot_daycare_days(axes[1, 0], data)\n    plot_doctor_visit_monthly(axes[1, 1], data)\n\n    # Adjust layout\n    fig.subplots_adjust(hspace=0.4)\n\n    # Export figure\n    export_figure(fig, \"medical_charts.png\")\n\n", "CodeBleu_score": 28.552719985992276}]}
{"instance_id": "7061f4d0c3fc", "generation_result": [{"code": "def plot_feeding_24h_viz() -> None\n\t# Import and extract data\n    data = parse_raw_data(\n        data=pd.read_csv(\"data/raw/baby_feeding_data.csv\"),\n        key=[\"Date\", \"Bottle Feeding\", \"Solid Feeding\"],\n    )\n\n    # Set up plot\n    plt.figure(figsize=(10, 5))\n    plt.style.use(\"dark_background\")\n    plt.grid(axis=\"both\")\n\n    # Compute offset from birthday\n    offset = get_end_date(data, first_year_only=False)\n\n    # Create scatter plot\n    plt.scatter(\n        x=data[\"timestamp_hour\"],\n        y=data[\"day_number\"] - offset,\n        c=data[\"Bottle Feeding\"],\n        cmap=\"Blues\",\n    )\n    plt.scatter(\n        x=data[\"timestamp_hour\"],\n        y=data[\"day_number\"] - offset,\n        c=data[\"Solid Feeding\"],\n        cmap=\"Reds\",\n    )\n\n    # Add legend\n    plt.legend([\"Bottle Feeding\", \"Solid Feeding\"])\n\n    # Format plot\n    format_24h_week_plot_horizontal(\n        x_label=\"Time of Day (Hour)\",\n        y_label=\"Days Since Birth\",\n        x_ticks=list(range(24)),\n        y_ticks=list(range(0, offset + 1, 7)),\n    )\n\n    # Export figure\n    export_figure(\"figures/feeding_24h_viz.png\")\n\n", "CodeBleu_score": 26.54420429774651}, {"code": "def plot_feeding_24h_viz() -> None\n\t# Import and extract feeding data\n    feeding_data = parse_raw_data(data=data, key=[\"Date\", \"Time\"])\n\n    # Set up the plot\n    fig_ax = plt.subplots(figsize=(15, 10))\n    plt.grid(True, which=\"both\", axis=\"both\", linestyle=\"-\", color=\"lightgrey\")\n\n    # Compute the offset from the birthday\n    offset = feeding_data[\"day_number\"].iloc[-1]\n\n    # Create a scatter plot of feeding times\n    fig_ax.scatter(\n        feeding_data[\"timestamp_hour\"],\n        feeding_data[\"day_number\"] - offset,\n        c=feeding_data[\"Feeding type\"],\n        cmap=plt.get_cmap(\"tab10\"),\n    )\n\n    # Add a legend\n    plt.legend(\n        title=\"Feeding type\",\n        handles=[\n            Patch(color=\"tab:blue\", label=\"Solid\"),\n            Patch(color=\"tab:red\", label=\"Bottle\"),\n        ],\n    )\n\n    # Format the plot\n    end_date = get_end_date(data=feeding_data, first_year_only=False)\n    format_24h_week_plot_horizontal(\n        fig_ax=fig_ax,\n        end_date=end_date,\n        offset=offset,\n        xlabel=\"Time of day\",\n        ylabel=\"Days since birth\",\n    )\n\n    # Export the figure\n    export_figure(fig_ax=fig_ax, filename=\"feeding_24h_viz.png\")\n\n", "CodeBleu_score": 27.263635886152027}, {"code": "def plot_feeding_24h_viz() -> None\n\t# Import and extract feeding data\n    feeding_data = parse_raw_data(data, key=[\"Start\", \"End\"])\n\n    # Set up a plot with a dark grid style\n    fig, ax = plt.subplots(figsize=(10, 6))\n    sns.set(style=\"darkgrid\")\n\n    # Compute the offset from the birthday based on the last solid feeding date\n    offset = feeding_data[feeding_data[\"Type\"] == \"solid\"][\"day_number\"].iloc[-1]\n\n    # Create a scatter plot of feeding times, using red for bottle feedings and blue for solid feedings\n    ax.scatter(\n        feeding_data[feeding_data[\"Type\"] == \"bottle\"][\"timestamp_hour\"],\n        feeding_data[feeding_data[\"Type\"] == \"bottle\"][\"day_number\"] - offset,\n        c=\"red\",\n        label=\"bottle\",\n    )\n    ax.scatter(\n        feeding_data[feeding_data[\"Type\"] == \"solid\"][\"timestamp_hour\"],\n        feeding_data[feeding_data[\"Type\"] == \"solid\"][\"day_number\"] - offset,\n        c=\"blue\",\n        label=\"solid\",\n    )\n\n    # Add a legend to distinguish between the two\n    ax.legend()\n\n    # Set the end date for the plot\n    end_date = get_end_date(feeding_data, first_year_only=True)\n\n    # Format the plot to display feeding data over a week in a 24-hour format\n    ax.set_xlim(0, 24)\n    ax.set_ylim(-1, end_date)\n    ax.set_xlabel(\"Hour of the Day\")\n    ax.set_ylabel(\"Day Number\")\n\n    # Export the figure\n    export_figure(fig, \"feeding_24h_viz\")\n\n", "CodeBleu_score": 23.524851421740394}]}
{"instance_id": "6c1ffa63d122", "generation_result": [{"code": "def plot(self, ax, task: ClassificationTask = ClassificationTask.Result, **kwargs):\n\t\"\"\"\n    Parameters\n    ----------\n    ax : Axes\n        Axes to plot the bar plot on.\n    task : ClassificationTask\n        Classification task to plot target counts for.\n    **kwargs :\n        Keyword arguments passed to seaborn.barplot.\n\n    Returns\n    -------\n    None\n    \"\"\"\n    target_counts, target_names = self._get_target_counts(task)\n    sns.barplot(x=target_names, y=target_counts, ax=ax, **kwargs)", "CodeBleu_score": 45.80788839017864}, {"code": "def plot(self, ax, task: ClassificationTask = ClassificationTask.Result, **kwargs):\n\t\"\"\"\n    Parameters\n    ----------\n    ax : Axes\n        The axis on which the plot will be drawn.\n    task : ClassificationTask, default=ClassificationTask.Result\n        The classification task for which the target counts will be plotted.\n    **kwargs : dict\n        Additional keyword arguments to be passed to the barplot function from seaborn.\n\n    Returns\n    -------\n    None\n        The function returns nothing.\n    \"\"\"\n    target_counts, target_names = self._get_target_counts(task)\n    sns.barplot(target_names, target_counts, ax=ax, **kwargs)\n\n", "CodeBleu_score": 44.62205944273477}, {"code": "def plot(self, ax, task: ClassificationTask = ClassificationTask.Result, **kwargs):\n\tif task not in self._target_counts:\n        self._get_target_counts(task)\n\n    counts, target_names = self._target_counts[task]\n    sns.barplot(target_names, counts, ax=ax, **kwargs)", "CodeBleu_score": 44.94540051841069}]}
{"instance_id": "2f81a906bad5", "generation_result": [{"code": "def render_to_rgb(figure)\n\t# Create a canvas from the figure\n    canvas = plt_backend_agg.FigureCanvasAgg(figure)\n\n    # Draw the figure on the canvas\n    canvas.draw()\n\n    # Get the width and height of the figure\n    width, height = get_width_height(figure)\n\n    # Get the RGBA buffer data from the canvas\n    buffer = canvas.buffer_rgba()\n\n    # Convert the RGBA data to an NumPy array\n    image = np.frombuffer(buffer, dtype='uint8')\n\n    # Reshape the image to have the correct dimensions\n    image = image.reshape(height, width, 4)\n\n    # Close the figure\n    plt.close(figure)\n\n    # Convert the image to RGB format and move the color channels to the first dimension\n    image = image[:, :, :3]\n    image = np.moveaxis(image, 2, 0)\n\n    # Return the image\n    return image", "CodeBleu_score": 16.528015940381284}, {"code": "def render_to_rgb(figure)\n\tif isinstance(figure, list):\n        rgb_images = []\n        for fig in figure:\n            canvas = plt_backend_agg.FigureCanvasAgg(fig)\n            canvas.draw()\n            width, height = get_width_height(fig)\n            rgba_buffer = canvas.buffer_rgba()\n            rgb = np.frombuffer(rgba_buffer, dtype=np.uint8).reshape(height, width, 4)[:, :, :3]\n            rgb_images.append(rgb)\n            plt.close(fig)\n        rgb_images = np.stack(rgb_images)\n        return rgb_images\n    else:\n        canvas = plt_backend_agg.FigureCanvasAgg(figure)\n        canvas.draw()\n        width, height = get_width_height(figure)\n        rgba_buffer = canvas.buffer_rgba()\n        rgb = np.frombuffer(rgba_buffer, dtype=np.uint8).reshape(height, width, 4)[:, :, :3]\n        plt.close(figure)\n        return rgb\n\n", "CodeBleu_score": 35.948000449143485}, {"code": "def render_to_rgb(figure)\n\tif isinstance(figure, list):\n        rgb_images = []\n        for fig in figure:\n            canvas = plt_backend_agg.FigureCanvasAgg(fig)\n            canvas.draw()\n            width, height = get_width_height(fig)\n            rgba_buffer = canvas.buffer_rgba()\n            image = np.frombuffer(rgba_buffer, np.uint8)\n            image = image.reshape(height, width, 4)\n            image = image[..., :3]\n            rgb_images.append(image)\n        rgb_images = np.stack(rgb_images, axis=0)\n        return rgb_images\n    else:\n        canvas = plt_backend_agg.FigureCanvasAgg(figure)\n        canvas.draw()\n        width, height = get_width_height(figure)\n        rgba_buffer = canvas.buffer_rgba()\n        image = np.frombuffer(rgba_buffer, np.uint8)\n        image = image.reshape(height, width, 4)\n        image = image[..., :3]\n        plt.close(figure)\n        return image", "CodeBleu_score": 35.45175887126862}]}
{"instance_id": "c3037b01569c", "generation_result": [{"code": "def beholder_pytorch()\n\timport time\n    import torch\n    import beholder_lib\n\n    # instantiate Beholder\n    beholder = beholder_lib.Beholder(log_dir='./beholder_logs/pytorch_example')\n\n    # run loop\n    for i in range(1000):\n        # generate random tensors\n        tensor_1 = torch.randn(10, 10, 10)\n        tensor_2 = torch.randn(10, 10, 10)\n        tensor_3 = torch.randn(10, 10, 10)\n        tensor_4 = torch.randn(10, 10, 10)\n        tensor_5 = torch.randn(10, 10, 10)\n        tensor_6 = torch.randn(10, 10, 10)\n        tensor_7 = torch.randn(10, 10, 10)\n        tensor_8 = torch.randn(10, 10, 10)\n        tensor_9 = torch.randn(10, 10, 10)\n        tensor_10 = torch.randn(10, 10, 10)\n\n        # generate random names\n        name_1 = 'tensor_1'\n        name_2 = 'tensor_2'\n        name_3 = 'tensor_3'\n        name_4 = 'tensor_4'\n        name_5 = 'tensor_5'\n        name_6 = 'tensor_6'\n        name_7 = 'tensor_7'\n        name_8 = 'tensor_8'\n        name_9 = 'tensor_9'\n        name_10 = 'tensor_10'\n\n        # generate random frame\n        frame = str(time.time())\n\n        # update Beholder\n        beholder.update(\n            [\n                tensor_and_name(tensor_1, name_1),\n                tensor_and_name(tensor_2, name_2),\n                tensor_and_name(tensor_3, name_3),", "CodeBleu_score": 22.907991243233063}, {"code": "def beholder_pytorch()\n\timport torch\n    import time\n    import beholder_lib\n    import random\n    import string\n    import os\n    import numpy as np\n    from torch.autograd import Variable\n    from beholder_lib import Beholder\n    import matplotlib.pyplot as plt\n    import torch.nn.functional as F\n    import torch.optim as optim\n    import torchvision\n    import torchvision.transforms as transforms\n\n    # Create the log directory\n    log_dir = os.path.join('beholder_logs', 'beholder_pytorch')\n    if not os.path.exists(log_dir):\n        os.makedirs(log_dir)\n\n    # Create a Beholder object\n    beholder = Beholder(log_dir=log_dir)\n\n    # Create a loop\n    for i in range(1000):\n\n        # Generate a random tensor\n        tensor_1 = torch.randn(10, 10)\n        tensor_2 = torch.randn(10, 10)\n\n        # Generate a random name\n        name_1 = ''.join(random.choice(string.ascii_uppercase + string.digits) for _ in range(6))\n        name_2 = ''.join(random.choice(string.ascii_uppercase + string.digits) for _ in range(6))\n\n        # Update the Beholder object\n        beholder.update({tensor_and_name(tensor_1, name_1), tensor_and_name(tensor_2, name_2)}, frame=i)\n\n        # Sleep briefly\n        time.sleep(0.01)\n\n        # Print the iteration index\n        print(i)\n\n\n\n\n\n", "CodeBleu_score": 29.682063904131734}, {"code": "def beholder_pytorch()\n\tfor i in range(1000):\n        t1 = torch.randn(3, 10, 10)\n        t2 = torch.randn(3, 10, 10)\n        t3 = torch.randn(3, 10, 10)\n        t4 = torch.randn(3, 10, 10)\n        beholder = Beholder(log_dir='logs/pytorch')\n        beholder.update([t1, t2], ['t1', 't2'], frame=randint(0, 100))\n        time.sleep(0.01)\n        print(i)\n", "CodeBleu_score": 23.93520480107223}]}
{"instance_id": "cd57b5de8a88", "generation_result": [{"code": "def log_pr_data(self, tag, summary, num_thresholds, step=None)\n\tpr_data = {}\n    pr_data['tag'] = tag\n    pr_data['thresholds'] = np.linspace(0, 1, num_thresholds)\n    tensor_data = np.fromiter(summary.value[0].tensor.float_val, dtype=np.float32)\n    tensor_data = np.vsplit(tensor_data, 4)\n    for i, tensor in enumerate(tensor_data):\n        if i == 0:\n            pr_data['TP'] = tensor.flatten().tolist()\n        elif i == 1:\n            pr_data['FP'] = tensor.flatten().tolist()\n        elif i == 2:\n            pr_data['TN'] = tensor.flatten().tolist()\n        elif i == 3:\n            pr_data['FN'] = tensor.flatten().tolist()\n    pr_data['precision'] = np.divide(pr_data['TP'], (np.add(pr_data['TP'], pr_data['FP'])))\n    pr_data['recall'] = np.divide(pr_data['TP'], (np.add(pr_data['TP'], pr_data['FN'])))\n    self.log_asset_data(pr_data, step=step)", "CodeBleu_score": 34.627867484715374}, {"code": "def log_pr_data(self, tag, summary, num_thresholds, step=None)\n\ttensor = summary.tensor\n    tensor_data = tensor.float_val\n    tensor_shape = tensor.tensor_shape.dim\n    tensor_dims = len(tensor_shape)\n    if tensor_dims == 2:\n        tensor_data = np.fromiter(tensor_data, dtype=float).reshape(tensor_shape[0].size, tensor_shape[1].size)\n    elif tensor_dims == 3:\n        tensor_data = np.fromiter(tensor_data, dtype=float).reshape(tensor_shape[0].size, tensor_shape[1].size, tensor_shape[2].size)\n    elif tensor_dims == 4:\n        tensor_data = np.fromiter(tensor_data, dtype=float).reshape(tensor_shape[0].size, tensor_shape[1].size, tensor_shape[2].size, tensor_shape[3].size)\n    else:\n        raise ValueError(\"The shape of the tensor must have 2, 3, or 4 dimensions.\")\n\n    thresholds = np.linspace(0, 1, num_thresholds)\n    precision = list(tensor_data[:, 0])\n    recall = list(tensor_data[:, 1])\n    TP = list(tensor_data[:, 2])\n    FP = list(tensor_data[:, 3])\n    TN = list(tensor_data[:, 4])\n    FN = list(tensor_data[:, 5])\n\n    pr_data = {\n        \"thresholds\": thresholds.tolist(),\n        \"precision\": precision,\n        \"recall\": recall,\n        \"TP\": TP,\n        \"FP\": FP,\n        \"TN\": TN,\n        \"FN\": FN\n    }\n    self.log_asset_data(tag, pr_data, step=step)", "CodeBleu_score": 31.228347550237736}, {"code": "def log_pr_data(self, tag, summary, num_thresholds, step=None)\n\ttensor_data = summary.tensor_data\n    tensor_shape = summary.tensor_shape\n    tensor_data = np.fromiter(tensor_data, dtype=np.float32)\n    tensor_data = np.vsplit(tensor_data, tensor_shape.dim[0].size)\n    tensor_data = np.hsplit(tensor_data[0], tensor_shape.dim[1].size)\n    thresholds = [i / num_thresholds for i in range(num_thresholds + 1)]\n    tp = tensor_data[0].flatten().tolist()\n    fp = tensor_data[1].flatten().tolist()\n    tn = tensor_data[2].flatten().tolist()\n    fn = tensor_data[3].flatten().tolist()\n    precision = tensor_data[4].flatten().tolist()\n    recall = tensor_data[5].flatten().tolist()\n    pr_data = {\n        \"thresholds\": thresholds,\n        \"tp\": tp,\n        \"fp\": fp,\n        \"tn\": tn,\n        \"fn\": fn,\n        \"precision\": precision,\n        \"recall\": recall,\n    }\n    self.log_asset_data(tag, pr_data, step=step)\n\n", "CodeBleu_score": 29.03069605158081}]}
{"instance_id": "7c4ea9beecc9", "generation_result": [{"code": "def load_mnist(images, labels, num)\n\tdata = np.zeros((num, 784), dtype=np.float32)\n    target = np.zeros((num, 10), dtype=np.float32)\n    with gzip.open(images, 'rb') as f_images, gzip.open(labels, 'rb') as f_labels:\n        f_images.read(16)\n        f_labels.read(8)\n        for i in range(num):\n            data[i] = np.array(list(f_images.read(784))).reshape((784,))\n            target[i, int(ord(f_labels.read(1)))] = 1.0\n    return data, target", "CodeBleu_score": 44.56692548909699}, {"code": "def load_mnist(images, labels, num)\n\tdata = np.zeros((num, 28, 28), dtype=np.float32)\n    target = np.zeros((num,), dtype=np.int32)\n\n    with gzip.open(images, 'rb') as f_images:\n        f_images.read(16)\n        for i in range(num):\n            for row in range(28):\n                for col in range(28):\n                    data[i][row][col] = ord(f_images.read(1)) / 255.\n\n    with gzip.open(labels, 'rb') as f_labels:\n        f_labels.read(8)\n        for i in range(num):\n            target[i] = ord(f_labels.read(1))\n\n    return data, target\n\n", "CodeBleu_score": 48.28538916401081}, {"code": "def load_mnist(images, labels, num)\n\tdata = np.zeros((num, 784))\n    target = np.zeros((num, 10))\n    with gzip.open(images) as f_images, gzip.open(labels) as f_labels:\n        f_images.read(16)\n        f_labels.read(8)\n        for i in range(num):\n            for j in range(784):\n                data[i, j] = ord(f_images.read(1))\n            target[i, int(ord(f_labels.read(1)))] = 1\n    return data, target", "CodeBleu_score": 42.483697645997836}]}
{"instance_id": "bf4f1c828b25", "generation_result": [{"code": "def compute_stats(self, ldf: LuxDataFrame)\n\tself.unique_values = {}\n    self.min_max = {}\n    self.cardinality = {}\n    self.length = len(ldf)\n    self.data_type = {}\n    self.data_type_ratio = {}\n    self.data_model_type = {}\n    self.data_model_type_ratio = {}\n    self.data_type_ratio[\"quantitative\"] = 0\n    self.data_type_ratio[\"temporal\"] = 0\n    self.data_type_ratio[\"nominal\"] = 0\n    self.data_type_ratio[\"geographical\"] = 0\n    self.data_model_type_ratio[\"measure\"] = 0\n    self.data_model_type_ratio[\"dimension\"] = 0\n    self.data_model_type_ratio[\"unknown\"] = 0\n    self.has_time = False\n    self.has_geo = False\n    self.has_id = False\n    self.has_measure = False\n    self.has_dimension = False\n    self.has_unknown = False\n    self.data_type_ratio[\"quantitative\"] = 0\n    self.data_type_ratio[\"temporal\"] = 0\n    self.data_type_ratio[\"nominal\"] = 0\n    self.data_type_ratio[\"geographical\"] = 0\n    self.data_model_type_ratio[\"measure\"] = 0\n    self.data_model_type_ratio[\"dimension\"] = 0\n    self.data_model_type_ratio[\"unknown\"] = 0\n    self.has_time = False\n    self.has_geo = False\n    self.has_id = False\n    self.has_measure = False\n    self.has_dimension = False\n    self.has_unknown = False\n    for attr in ldf.columns:\n        if ldf[attr].dtype == \"datetime64[ns]\":\n            self.unique_values[attr] = ldf[attr].dt.strftime(\"%Y/%m/%d %H:%M:%S\").unique()\n            self.min", "CodeBleu_score": 22.94658666811607}, {"code": "def compute_stats(self, ldf: LuxDataFrame)\n\tself.unique_values = {}\n    self.cardinality = {}\n    self.min_max = {}\n    self.length = len(ldf)\n    for attr in ldf.columns:\n        if ldf[attr].dtype == \"datetime64[ns]\":\n            self.unique_values[attr] = ldf[attr].apply(lambda x: x.strftime(\"%Y-%m-%d %H:%M:%S\")).unique()\n        else:\n            self.unique_values[attr] = ldf[attr].unique()\n        self.cardinality[attr] = len(self.unique_values[attr])\n        if ldf[attr].dtype == \"float64\" or ldf[attr].dtype == \"int64\":\n            self.min_max[attr] = (ldf[attr].min(), ldf[attr].max())\n    if not ldf.index.dtype == \"int64\":\n        self.unique_values[ldf.index.name] = ldf.index.unique()\n        self.cardinality[ldf.index.name] = len(self.unique_values[ldf.index.name])\n\n", "CodeBleu_score": 35.33497839209459}, {"code": "def compute_stats(self, ldf: LuxDataFrame)\n\tself.unique_values = {}\n    self.cardinality = {}\n    self.min_max = {}\n    self.length = len(ldf)\n    for attribute in ldf.columns:\n        if (\n            ldf[attribute].dtype == \"datetime64[ns]\"\n            or ldf[attribute].dtype == \"datetime64[ns, UTC]\"\n        ):\n            self.unique_values[attribute] = ldf[attribute].dt.strftime(\"%Y-%m-%d %H:%M:%S\").unique()\n            self.cardinality[attribute] = len(self.unique_values[attribute])\n        else:\n            self.unique_values[attribute] = ldf[attribute].unique()\n            self.cardinality[attribute] = len(self.unique_values[attribute])\n        if ldf[attribute].dtype != \"object\" and ldf[attribute].dtype != \"bool\":\n            self.min_max[attribute] = (ldf[attribute].min(), ldf[attribute].max())\n    if not isinstance(ldf.index, pd.RangeIndex):\n        self.unique_values[\"index\"] = ldf.index.unique()\n        self.cardinality[\"index\"] = len(self.unique_values[\"index\"])\n        if ldf.index.dtype != \"object\" and ldf.index.dtype != \"bool\":\n            self.min_max[\"index\"] = (ldf.index.min(), ldf.index.max())\n", "CodeBleu_score": 41.380826758311514}]}
{"instance_id": "3e6fc3d2f47c", "generation_result": [{"code": "def execute_preview(tbl: LuxSQLTable, preview_size=5)\n\t# TODO: add in the preview_size parameter\n    # TODO: add in the preview_query template\n    preview_query = \"SELECT * FROM {table_name} LIMIT {preview_size}\"\n    preview_query = preview_query.format(table_name=tbl.name, preview_size=preview_size)\n    df = pd.read_sql(preview_query, tbl.engine)\n    return df\n\n", "CodeBleu_score": 30.186137010152454}, {"code": "def execute_preview(tbl: LuxSQLTable, preview_size=5)\n\tpreview_query = tbl.preview_query\n    preview_query.format(tbl.name, preview_size)\n    df = pd.read_sql(preview_query, tbl.engine)\n    return df\n\n", "CodeBleu_score": 22.38372868779493}, {"code": "def execute_preview(tbl: LuxSQLTable, preview_size=5)\n\tquery = tbl.preview_query.format(tbl.table_name, preview_size)\n    df = pd.read_sql(query, tbl.connection)\n    return df", "CodeBleu_score": 19.716767528966248}]}
{"instance_id": "b3dee87be4b8", "generation_result": [{"code": "def execute_sampling(tbl: LuxSQLTable)\n\timport pandas as pd\n    import lux\n    import numpy as np\n    import warnings\n    from lux.utils import utils\n    from lux.utils.utils import get_filter_specs\n    from lux.utils.date_utils import compute_date_granularity\n    from lux.utils.date_utils import compute_time_granularity\n    from lux.utils.date_utils import compute_weekday\n    from lux.utils.utils import check_import_lux_widget\n    from lux.utils.utils import check_if_id_like\n\n    if lux.config.sampling:\n        # Check if sampling is enabled\n        if lux.config.sampling_start:\n            # Check if sampling start is set\n            if lux.config.sampling_cap:\n                # Check if sampling cap is set\n                if lux.config.sampling_fraction:\n                    # Check if sampling fraction is set\n                    if lux.config.sampling_flag:\n                        # Check if sampling flag is set\n                        # Retrieve sampling parameters\n                        sampling_flag = lux.config.sampling_flag\n                        sampling_start = lux.config.sampling_start\n                        sampling_cap = lux.config.sampling_cap\n                        sampling_fraction = lux.config.sampling_fraction\n\n                        # Check if sampling flag is present in the table\n                        if sampling_flag in tbl.columns:\n                            # Check if sampling flag is present in the table\n                            # Retrieve the length of the table\n                            tbl_len_query = f\"SELECT COUNT(*) FROM {tbl.name}\"\n                            tbl_len = pd.read_sql(tbl_len_query, tbl.context.connection)\n                            tbl_len = tbl_len.values[0][0]\n\n                            # Calculate the sample limit\n                            sample_limit = int(tbl_len * sampling_fraction)\n\n                            # Check if sample limit is greater than sampling cap\n                            if sample_limit > sampling_cap:\n                                # If sample limit is greater than sampling cap, set the limit to the cap\n                                sample_limit = sampling_cap\n\n                            # Check if sample", "CodeBleu_score": 25.12046683870311}, {"code": "def execute_sampling(tbl: LuxSQLTable)\n\t# Get sampling configuration parameters\n    SAMPLE_FLAG = lux.config.sampling\n    SAMPLE_START = lux.config.sampling_start\n    SAMPLE_CAP = lux.config.sampling_cap\n    SAMPLE_FRAC = lux.config.sampling_fraction\n    # Get table length\n    table_length = len(tbl)\n    # If table length is less than the cap, then sample the whole table\n    if table_length <= SAMPLE_CAP:\n        sample_limit = table_length\n    # If table length is greater than the cap, then sample the table based on the fraction\n    else:\n        sample_limit = int(SAMPLE_FRAC * table_length)\n    # If table length is greater than the start, then sample the table\n    if table_length >= SAMPLE_START:\n        # If table length is less than the cap, then sample the whole table\n        if table_length <= SAMPLE_CAP:\n            sample_limit = table_length\n        # If table length is greater than the cap, then sample the table based on the fraction\n        else:\n            sample_limit = int(SAMPLE_FRAC * table_length)\n        # Format sample query\n        sample_query = f\"SELECT * FROM {tbl.table_name} LIMIT {sample_limit}\"\n        # Execute sample query\n        tbl._sampled = pd.read_sql(sample_query, tbl.engine)\n    # If table length is less than the start, then do not sample the table\n    else:\n        tbl._sampled = tbl\n    return tbl\n\n", "CodeBleu_score": 27.337932587464554}, {"code": "def execute_sampling(tbl: LuxSQLTable)\n\t# Get sampling configuration parameters\n    sample_flag = lux.config.sampling\n    sample_start = lux.config.sampling_start\n    sample_cap = lux.config.sampling_cap\n    sample_frac = lux.config.sampling_frac\n\n    # If sampling is disabled, return\n    if sample_flag is False:\n        return\n\n    # If sampling is enabled, execute sample query\n    else:\n        # Get table length\n        table_length = tbl.query([\"count(*) as table_length\"]).values[0][0]\n        # Calculate limit\n        limit = min(sample_cap, int(table_length * sample_frac))\n        # Format sample query\n        sample_query = f\"SELECT * FROM {tbl.name} LIMIT {limit}\"\n        # Execute sample query\n        tbl._sampled = pd.read_sql(sample_query, tbl.con)", "CodeBleu_score": 27.071780515552884}]}
{"instance_id": "485253b1088a", "generation_result": [{"code": "def draw_text(\n    params: Dict[str, dict],\n    background: BaseGeometry\n) -> None\n\t# Override default parameters with provided parameters\n    text_params = override_params(\n        default_dict=DEFAULT_TEXT_PARAMS,\n        new_dict=params\n    )\n\n    # Extract parameters\n    text = text_params.pop(\"text\")\n    x = text_params.pop(\"x\")\n    y = text_params.pop(\"y\")\n\n    # Get background bounds\n    x_bounds = background.bounds[0]\n    y_bounds = background.bounds[1]\n\n    # Interpolate x and y values\n    x = np.interp(x, [0, 1], x_bounds)\n    y = np.interp(y, [0, 1], y_bounds)\n\n    # Draw text\n    plt.text(x, y, text, **text_params)\n\n", "CodeBleu_score": 17.779440439816955}, {"code": "def draw_text(\n    params: Dict[str, dict],\n    background: BaseGeometry\n) -> None\n\ttext_params = override_params(\n        {\n            \"text\": \"\",\n            \"x\": 0,\n            \"y\": 0,\n            \"ha\": \"center\",\n            \"va\": \"center\",\n            \"transform\": None,\n            \"color\": \"black\",\n            \"fontsize\": 14,\n            \"fontweight\": \"normal\",\n            \"fontstyle\": \"normal\",\n            \"fontfamily\": \"sans-serif\",\n            \"alpha\": 1,\n            \"bbox\": None,\n            \"zorder\": 2,\n        },\n        params,\n    )\n\n    x = np.interp(\n        text_params[\"x\"],\n        [background.bounds[0], background.bounds[2]],\n        [0, 1],\n    )\n\n    y = np.interp(\n        text_params[\"y\"],\n        [background.bounds[1], background.bounds[3]],\n        [0, 1],\n    )\n\n    plt.text(\n        x,\n        y,\n        text_params[\"text\"],\n        ha=text_params[\"ha\"],\n        va=text_params[\"va\"],\n        transform=text_params[\"transform\"],\n        color=text_params[\"color\"],\n        fontsize=text_params[\"fontsize\"],\n        fontweight=text_params[\"fontweight\"],\n        fontstyle=text_params[\"fontstyle\"],\n        fontfamily=text_params[\"fontfamily\"],\n        alpha=text_params[\"alpha\"],\n        bbox=text_params[\"bbox\"],\n        zorder=text_params[\"zorder\"],\n    )\n\n", "CodeBleu_score": 20.747749182526093}, {"code": "def draw_text(\n    params: Dict[str, dict],\n    background: BaseGeometry\n) -> None\n\ttext_params = override_params(\n        default_dict=dict(\n            text=\"\",\n            x=0,\n            y=0,\n            ha=\"center\",\n            va=\"center\",\n            color=\"black\",\n            size=10,\n            weight=\"normal\",\n            alpha=1,\n            rotation=0,\n            bbox=None,\n            zorder=0,\n            backgroundcolor=\"white\",\n        ),\n        new_dict=params,\n    )\n\n    x, y = background.bounds\n\n    x = np.interp(text_params[\"x\"], [0, 1], [x[0], x[1]])\n    y = np.interp(text_params[\"y\"], [0, 1], [y[0], y[1]])\n\n    plt.text(\n        x,\n        y,\n        text_params[\"text\"],\n        ha=text_params[\"ha\"],\n        va=text_params[\"va\"],\n        color=text_params[\"color\"],\n        size=text_params[\"size\"],\n        weight=text_params[\"weight\"],\n        alpha=text_params[\"alpha\"],\n        rotation=text_params[\"rotation\"],\n        bbox=text_params[\"bbox\"],\n        zorder=text_params[\"zorder\"],\n        backgroundcolor=text_params[\"backgroundcolor\"],\n    )\n\n", "CodeBleu_score": 26.165250162955722}]}
{"instance_id": "5f70a88b6b72", "generation_result": [{"code": "def _determine_format_string( dates, datetime_format=None )\n\timport numpy as np\n    import matplotlib.dates as mdates\n\n    if datetime_format is None:\n        days_between_dates = np.diff(mdates.date2num(dates))\n        mean_days_between_dates = np.mean(days_between_dates)\n        if mean_days_between_dates < 0.33:\n            if max(dates) - min(dates) > np.timedelta64(1, 'D'):\n                datetime_format = '%b %d, %H:%M'\n            else:\n                datetime_format = '%H:%M'\n        else:\n            if max(dates) - min(dates) > np.timedelta64(365, 'D'):\n                datetime_format = '%Y-%b-%d'\n            else:\n                datetime_format = '%b %d'\n\n    return datetime_format\n\n\n\n\n", "CodeBleu_score": 42.8738394940682}, {"code": "def _determine_format_string( dates, datetime_format=None )\n\tif datetime_format is not None:\n        return datetime_format\n    days_between_dates = np.diff(dates)\n    avg_days = np.mean(days_between_dates)\n    if avg_days < 0.33:\n        if days_between_dates.max() > 1:\n            return '%b %d, %H:%M'\n        else:\n            return '%H:%M'\n    else:\n        if (dates.max() - dates.min()) > 365:\n            return '%Y-%b-%d'\n        else:\n            return '%b %d'", "CodeBleu_score": 32.03919981286596}, {"code": "def _determine_format_string( dates, datetime_format=None )\n\tif datetime_format is not None:\n        return datetime_format\n    if len( dates ) < 2:\n        raise ValueError( 'Date list must contain at least two dates' )\n    # Calculate the average number of days between consecutive dates in the list\n    avg_days = np.mean( [ dates[ i ] - dates[ i - 1 ] for i in range( 1, len( dates ) ) ] )\n    if avg_days < 0.33:\n        # Intraday data\n        if ( dates[ -1 ] - dates[ 0 ] ).days > 1:\n            # More than one day of data\n            return '%b %d, %H:%M'\n        else:\n            # Less than one day of data\n            return '%H:%M'\n    else:\n        # Daily or less frequent data\n        if ( dates[ -1 ] - dates[ 0 ] ).days > 365:\n            # More than one year of data\n            return '%Y-%b-%d'\n        else:\n            # Less than one year of data\n            return '%b %d'\n\n\n\n\n", "CodeBleu_score": 38.55232559672284}]}
{"instance_id": "f9589bf4666d", "generation_result": [{"code": "def _mpf_to_rgba(c, alpha=None)\n\tif c is None:\n        return None\n    if not _is_uint8_rgb_or_rgba(c):\n        c = mcolors.to_rgba(c, alpha)\n    else:\n        c = [v / 255.0 for v in c[0:3]]\n        if len(c) == 3:\n            if alpha is not None:\n                c.append(alpha)\n        else:\n            c[3] = alpha\n    return c\n\n", "CodeBleu_score": 28.31389997544369}, {"code": "def _mpf_to_rgba(c, alpha=None)\n\tif _is_uint8_rgb_or_rgba(c):\n        if c[0] > 1 or c[1] > 1 or c[2] > 1:\n            c = tuple([v / 255.0 for v in c])\n        if alpha is not None:\n            c = c + (alpha,)\n        return c\n    else:\n        return mcolors.to_rgba(c, alpha)", "CodeBleu_score": 25.80815908365447}, {"code": "def _mpf_to_rgba(c, alpha=None)\n\tif _is_uint8_rgb_or_rgba(c):\n        c = tuple(v/255 for v in c)\n    rgba = mcolors.to_rgba(c)\n    if alpha is not None:\n        rgba = rgba[0:3] + (alpha,)\n    return rgba\n\n", "CodeBleu_score": 30.99577687482237}]}
{"instance_id": "4202e378ab0b", "generation_result": [{"code": "def subplots(self,*args,**kwargs)\n\tif 'style' in kwargs or not hasattr(self,'mpfstyle'):\n        style = _check_for_and_apply_style(kwargs)\n        self.mpfstyle = style\n    else:\n        style = _check_for_and_apply_style(dict(style=self.mpfstyle))\n\n    axlist = mplfigure.Figure.subplots(self,*args,**kwargs)\n\n    if isinstance(axlist,mpl_axes.Axes):\n        axlist.mpfstyle = style\n    elif isinstance(axlist,np.ndarray):\n        for ax in axlist.flatten():\n            ax.mpfstyle = style\n    else:\n       raise TypeError('Unexpected type ('+str(type(axlist))+') '+\n                       'returned from \"matplotlib.figure.Figure.subplots()\"')\n    return axlist", "CodeBleu_score": 91.3032669441917}, {"code": "def subplots(self,*args,**kwargs)\n\tif 'style' in kwargs or not hasattr(self,'mpfstyle'):\n        style = _check_for_and_apply_style(kwargs)\n        self.mpfstyle = style\n    else:\n        style = _check_for_and_apply_style(dict(style=self.mpfstyle))\n\n    axlist = mplfigure.Figure.subplots(self,*args,**kwargs)\n\n    if isinstance(axlist,mpl_axes.Axes):\n        axlist.mpfstyle = style\n    elif isinstance(axlist,np.ndarray):\n        for ax in axlist.flatten():\n            ax.mpfstyle = style\n    else:\n       raise TypeError('Unexpected type ('+str(type(axlist))+') '+\n                       'returned from \"matplotlib.figure.Figure.subplots()\"')\n    return axlist", "CodeBleu_score": 91.3032669441917}, {"code": "def subplots(self,*args,**kwargs)\n\tif 'style' in kwargs or not hasattr(self,'mpfstyle'):\n        style = _check_for_and_apply_style(kwargs)\n        self.mpfstyle = style\n    else:\n        style = _check_for_and_apply_style(dict(style=self.mpfstyle))\n\n    axlist = mplfigure.Figure.subplots(self,*args,**kwargs)\n\n    if isinstance(axlist,mpl_axes.Axes):\n        axlist.mpfstyle = style\n    elif isinstance(axlist,np.ndarray):\n        for ax in axlist.flatten():\n            ax.mpfstyle = style\n    else:\n       raise TypeError('Unexpected type ('+str(type(axlist))+') '+\n                       'returned from \"matplotlib.figure.Figure.subplots()\"')\n    return axlist", "CodeBleu_score": 91.3032669441917}]}
{"instance_id": "3e2b6dfe4753", "generation_result": [{"code": "def _adjust_figsize(fig,config)\n\tif fig is None:\n        return\n    if 'figsize' in config:\n        fig.set_size_inches(config['figsize'])\n    else:\n        fig_ratio = config['fig_ratio']\n        fig_width = config['fig_width']\n        fig_height = config['fig_height']\n        if fig_ratio > 1.0:\n            fig_width = fig_width * fig_ratio\n        elif fig_ratio < 1.0:\n            fig_height = fig_height * fig_ratio\n        fig.set_size_inches(fig_width, fig_height)\n\n\n\n\n\n", "CodeBleu_score": 25.366096769160617}, {"code": "def _adjust_figsize(fig,config)\n\tif fig is None:\n        return\n    if 'figsize' in config:\n        fig.set_size_inches(config['figsize'])\n    else:\n        ratio = config['ratio']\n        fig_width = fig.get_figwidth()\n        fig_height = fig.get_figheight()\n        fig_ratio = fig_height/fig_width\n        if fig_ratio > ratio:\n            fig_height = fig_width*ratio\n        elif fig_ratio < 1/ratio:\n            fig_width = fig_height/ratio\n        fig.set_size_inches(fig_width,fig_height)", "CodeBleu_score": 29.06101852101628}, {"code": "def _adjust_figsize(fig,config)\n\tif fig is None:\n        return\n    if 'figsize' in config:\n        fig.set_size_inches(config['figsize'])\n    else:\n        ratio = config['ratio']\n        if ratio > 1:\n            fig.set_size_inches(config['figsize'] * ratio)\n        elif ratio < 1:\n            fig.set_size_inches(config['figsize'] / ratio)\n        else:\n            fig.set_size_inches(config['figsize'])\n\n\n\n\n\n", "CodeBleu_score": 22.536748228273296}]}
{"instance_id": "986e5804b941", "generation_result": [{"code": "def _plot_mav(ax,config,xdates,prices,apmav=None,apwidth=None)\n\tto me that there may be some or significant overlap\n    # TODO:  between what the following functions actually do:\n    # TODO:  At the very least, all four of them appear to communicate \n    # TODO:  to matplotlib that the xaxis should be treated as dates:\n    # TODO:   ->  'ax.autoscale_view()'\n    # TODO:   ->  'ax.xaxis_dates()'\n    # TODO:   ->  'plt.autofmt_xdates()'\n    # TODO:   ->  'fig.autofmt_xdate()'\n    # TODO: ================================================================\n    \n\n    #if config['autofmt_xdate']:\n        #print('CALLING fig.autofmt_xdate()')\n        #fig.autofmt_xdate()\n\n    axA1.autoscale_view()  # Is this really necessary??\n                           # It appears to me, based on experience coding types 'ohlc' and 'candle'\n                           # for `addplot`, that this IS necessary when the only thing done to the\n                           # the axes is .add_collection().  (However, if ax.plot() .scatter() or\n                           # .bar() was called, then possibly this is not necessary; not entirely\n                           # sure, but it definitely was necessary to get 'ohlc' and 'candle' \n                           # working in `addplot`).\n\n    axA1.set_ylabel(config['ylabel'])\n\n    if config['volume']:\n        if external_axes_mode:\n            volumeAxes.tick_params(axis='x',rotation=xrotation)\n            volumeAxes.xaxis.set_major_formatter(formatter)\n\n        vscale = 'linear'\n        ysd = config['volume_yscale']\n        if isinstance(ysd,dict):\n            yscale = ysd['yscale']\n            del      ysd['yscale']\n            volumeAxes.set_yscale(yscale,**ysd)\n            vscale = yscale\n        elif isinstance(ysd,str):\n            volumeAxes.set_yscale(ysd)\n            vscale = ysd\n        offset = ''\n        if vscale == 'linear':\n            vxp = config['volume_exponent']\n            if vxp == 'legacy':\n                volumeAxes.figure.canvas.draw()  # This is needed to calculate offset\n                offset = volumeAxes.yaxis.get_major_formatter().get_offset()\n                if len(offset) > 0:\n                    offset = (' x '+offset)\n            elif isinstance(vxp,int) and vxp > 0:\n                volumeAxes.ticklabel_format(useOffset=False,scilimits=(vxp,vxp),axis='y')\n                offset = '  $10^{'+str(vxp)+'}$'\n            elif isinstance(vxp,int) and vxp == 0:\n                volumeAxes.ticklabel_format(useOffset=False,style='plain',axis='y')\n                offset = ''\n            else:\n                offset = ''\n                scilims = plt.rcParams['axes.formatter.limits']\n                if scilims[0] < scilims[1]:\n                    for power in (5,4,3,2,1):\n                        xp = scilims[1]*power\n                        if vymax >= 10.**xp:\n                            volumeAxes.ticklabel_format(useOffset=False,scilimits=(xp,xp),axis='y')\n                            offset = '  $10^{'+str(xp)+'}$'\n                            break\n                elif scilims[0] == scilims[1] and scilims[1] != 0:\n                    volumeAxes.ticklabel_format(useOffset=False,scilimits=scilims,axis='y')\n                    offset = ' $10^'+str(scilims[1])+'$'\n            volumeAxes.yaxis.offsetText.set_visible(False)\n\n        if config['ylabel_lower'] is None:\n            vol_label = 'Volume'+offset\n        else:\n            if len(offset) > 0:\n                offset = '", "CodeBleu_score": 30.007100920182584}, {"code": "def _plot_mav(ax,config,xdates,prices,apmav=None,apwidth=None)\n\tto me that there may be some or significant overlap\n    # TODO:  between what the following functions actually do:\n    # TODO:  At the very least, all four of them appear to communicate \n    # TODO:  to matplotlib that the xaxis should be treated as dates:\n    # TODO:   ->  'ax.autoscale_view()'\n    # TODO:   ->  'ax.xaxis_dates()'\n    # TODO:   ->  'plt.autofmt_xdates()'\n    # TODO:   ->  'fig.autofmt_xdate()'\n    # TODO: ================================================================\n    \n\n    #if config['autofmt_xdate']:\n        #print('CALLING fig.autofmt_xdate()')\n        #fig.autofmt_xdate()\n\n    axA1.autoscale_view()  # Is this really necessary??\n                           # It appears to me, based on experience coding types 'ohlc' and 'candle'\n                           # for `addplot`, that this IS necessary when the only thing done to the\n                           # the axes is .add_collection().  (However, if ax.plot() .scatter() or\n                           # .bar() was called, then possibly this is not necessary; not entirely\n                           # sure, but it definitely was necessary to get 'ohlc' and 'candle' \n                           # working in `addplot`).\n\n    axA1.set_ylabel(config['ylabel'])\n\n    if config['volume']:\n        if external_axes_mode:\n            volumeAxes.tick_params(axis='x',rotation=xrotation)\n            volumeAxes.xaxis.set_major_formatter(formatter)\n\n        vscale = 'linear'\n        ysd = config['volume_yscale']\n        if isinstance(ysd,dict):\n            yscale = ysd['yscale']\n            del      ysd['yscale']\n            volumeAxes.set_yscale(yscale,**ysd)\n            vscale = yscale\n        elif isinstance(ysd,str):\n            volumeAxes.set_yscale(ysd)\n            vscale = ysd\n        offset = ''\n        if vscale == 'linear':\n            vxp = config['volume_exponent']\n            if vxp == 'legacy':\n                volumeAxes.figure.canvas.draw()  # This is needed to calculate offset\n                offset = volumeAxes.yaxis.get_major_formatter().get_offset()\n                if len(offset) > 0:\n                    offset = (' x '+offset)\n            elif isinstance(vxp,int) and vxp > 0:\n                volumeAxes.ticklabel_format(useOffset=False,scilimits=(vxp,vxp),axis='y')\n                offset = '  $10^{'+str(vxp)+'}$'\n            elif isinstance(vxp,int) and vxp == 0:\n                volumeAxes.ticklabel_format(useOffset=False,style='plain',axis='y')\n                offset = ''\n            else:\n                offset = ''\n                scilims = plt.rcParams['axes.formatter.limits']\n                if scilims[0] < scilims[1]:\n                    for power in (5,4,3,2,1):\n                        xp = scilims[1]*power\n                        if vymax >= 10.**xp:\n                            volumeAxes.ticklabel_format(useOffset=False,scilimits=(xp,xp),axis='y')\n                            offset = '  $10^{'+str(xp)+'}$'\n                            break\n                elif scilims[0] == scilims[1] and scilims[1] != 0:\n                    volumeAxes.ticklabel_format(useOffset=False,scilimits=scilims,axis='y')\n                    offset = ' $10^'+str(scilims[1])+'$'\n            volumeAxes.yaxis.offsetText.set_visible(False)\n\n        if config['ylabel_lower'] is None:\n            vol_label = 'Volume'+offset\n        else:\n            if len(offset) > 0:\n                offset = '", "CodeBleu_score": 30.007100920182584}, {"code": "def _plot_mav(ax,config,xdates,prices,apmav=None,apwidth=None)\n\tto me that there may be some or significant overlap\n    # TODO:  between what the following functions actually do:\n    # TODO:  At the very least, all four of them appear to communicate \n    # TODO:  to matplotlib that the xaxis should be treated as dates:\n    # TODO:   ->  'ax.autoscale_view()'\n    # TODO:   ->  'ax.xaxis_dates()'\n    # TODO:   ->  'plt.autofmt_xdates()'\n    # TODO:   ->  'fig.autofmt_xdate()'\n    # TODO: ================================================================\n    \n\n    #if config['autofmt_xdate']:\n        #print('CALLING fig.autofmt_xdate()')\n        #fig.autofmt_xdate()\n\n    axA1.autoscale_view()  # Is this really necessary??\n                           # It appears to me, based on experience coding types 'ohlc' and 'candle'\n                           # for `addplot`, that this IS necessary when the only thing done to the\n                           # the axes is .add_collection().  (However, if ax.plot() .scatter() or\n                           # .bar() was called, then possibly this is not necessary; not entirely\n                           # sure, but it definitely was necessary to get 'ohlc' and 'candle' \n                           # working in `addplot`).\n\n    axA1.set_ylabel(config['ylabel'])\n\n    if config['volume']:\n        if external_axes_mode:\n            volumeAxes.tick_params(axis='x',rotation=xrotation)\n            volumeAxes.xaxis.set_major_formatter(formatter)\n\n        vscale = 'linear'\n        ysd = config['volume_yscale']\n        if isinstance(ysd,dict):\n            yscale = ysd['yscale']\n            del      ysd['yscale']\n            volumeAxes.set_yscale(yscale,**ysd)\n            vscale = yscale\n        elif isinstance(ysd,str):\n            volumeAxes.set_yscale(ysd)\n            vscale = ysd\n        offset = ''\n        if vscale == 'linear':\n            vxp = config['volume_exponent']\n            if vxp == 'legacy':\n                volumeAxes.figure.canvas.draw()  # This is needed to calculate offset\n                offset = volumeAxes.yaxis.get_major_formatter().get_offset()\n                if len(offset) > 0:\n                    offset = (' x '+offset)\n            elif isinstance(vxp,int) and vxp > 0:\n                volumeAxes.ticklabel_format(useOffset=False,scilimits=(vxp,vxp),axis='y')\n                offset = '  $10^{'+str(vxp)+'}$'\n            elif isinstance(vxp,int) and vxp == 0:\n                volumeAxes.ticklabel_format(useOffset=False,style='plain',axis='y')\n                offset = ''\n            else:\n                offset = ''\n                scilims = plt.rcParams['axes.formatter.limits']\n                if scilims[0] < scilims[1]:\n                    for power in (5,4,3,2,1):\n                        xp = scilims[1]*power\n                        if vymax >= 10.**xp:\n                            volumeAxes.ticklabel_format(useOffset=False,scilimits=(xp,xp),axis='y')\n                            offset = '  $10^{'+str(xp)+'}$'\n                            break\n                elif scilims[0] == scilims[1] and scilims[1] != 0:\n                    volumeAxes.ticklabel_format(useOffset=False,scilimits=scilims,axis='y')\n                    offset = ' $10^'+str(scilims[1])+'$'\n            volumeAxes.yaxis.offsetText.set_visible(False)\n\n        if config['ylabel_lower'] is None:\n            vol_label = 'Volume'+offset\n        else:\n            if len(offset) > 0:\n                offset = '", "CodeBleu_score": 30.007100920182584}]}
{"instance_id": "5308e20f3c90", "generation_result": [{"code": "def flops_conv(opnode: ConvolutionForward, inputs, outputs)\n\tkh, kw = opnode.kernel_shape\n    n_in = inputs[0].shape[1]\n    n_out = np.prod(outputs[0].shape)\n    n_flops = n_out * (n_in * kh * kw + 1 if opnode.has_bias else n_in * kh * kw)\n    return n_flops\n\n", "CodeBleu_score": 15.332650989426696}, {"code": "def flops_conv(opnode: ConvolutionForward, inputs, outputs)\n\t# Get the kernel size from the input tensor's shape\n    kernel_size = inputs[0].shape[2:4]\n    # Get the number of input channels\n    num_channels = inputs[0].shape[1]\n    # Get the total number of output elements\n    num_output_elements = np.prod(outputs[0].shape)\n    # Compute the FLOPs\n    flops = num_output_elements * num_channels * np.prod(kernel_size)\n    # Check if the convolution operation includes a bias term\n    if len(inputs) == 3:\n        # Include the bias term in the FLOPs\n        flops += num_output_elements\n    return flops\n\n", "CodeBleu_score": 18.76007668285849}, {"code": "def flops_conv(opnode: ConvolutionForward, inputs, outputs)\n\tkernel_height = opnode.kernel_shape[0]\n    kernel_width = opnode.kernel_shape[1]\n    input_channels = inputs[0].shape[1]\n    output_elements = np.prod(outputs[0].shape)\n    flops = output_elements * input_channels * kernel_height * kernel_width\n    if opnode.with_bias:\n        flops += output_elements\n    return flops\n", "CodeBleu_score": 17.68994578809923}]}
{"instance_id": "70811d3cf578", "generation_result": [{"code": "def mge_dtype_to_ir_type(mge_dtype)\n\tmge_dtype = np.dtype(mge_dtype)\n    assert isinstance(mge_dtype, np.dtype)\n    return _dtype_to_ir_type[mge_dtype](mge_dtype)", "CodeBleu_score": 20.773363693926143}, {"code": "def mge_dtype_to_ir_type(mge_dtype)\n\tmge_dtype = np.dtype(mge_dtype)\n    assert isinstance(mge_dtype, np.dtype)\n    return _dtype_to_ir_type[mge_dtype](mge_dtype)", "CodeBleu_score": 20.773363693926143}, {"code": "def mge_dtype_to_ir_type(mge_dtype)\n\tmge_dtype = np.dtype(mge_dtype)\n    assert isinstance(mge_dtype, np.dtype)\n    return _dtype_to_ir_type[mge_dtype](mge_dtype)\n\n", "CodeBleu_score": 20.773363693926143}]}
{"instance_id": "a108c9ba2c22", "generation_result": [{"code": "def _numpy_array_constant(x: np.ndarray, canonicalize_types) -> Sequence[ir.Value]\n\tif canonicalize_types:\n        x = np.ascontiguousarray(x.view(dtype.canonicalize_dtype(x.dtype)))\n    else:\n        x = np.ascontiguousarray(x)\n\n    if x.dtype == np.bool_:\n        x = x.astype(np.uint8)\n    elif x.dtype == np.bfloat16:\n        x = x.astype(np.float32)\n\n    ir_type = mge_dtype_to_ir_type(x.dtype)\n    ir_attr = ir.DenseElementsAttr.get(x, type=ir_type)\n    return [\n        hlo.ConstantOp(ir_attr).result,\n    ]\n\n", "CodeBleu_score": 45.59547558298875}, {"code": "def _numpy_array_constant(x: np.ndarray, canonicalize_types) -> Sequence[ir.Value]\n\tif canonicalize_types:\n        x = np.ascontiguousarray(x.view(np.dtype(x.dtype.name)))\n    else:\n        x = np.ascontiguousarray(x)\n    if x.dtype == np.bool_:\n        x = x.astype(np.uint8)\n    elif x.dtype == np.bfloat16:\n        x = x.astype(np.float32)\n    ir_type = mge_dtype_to_ir_type(x.dtype)\n    dense_attr = DenseElementsAttr.get(x, type=ir_type)\n    return [hlo.ConstantOp(ir_type, dense_attr).result]\n\n", "CodeBleu_score": 44.29118317594649}, {"code": "def _numpy_array_constant(x: np.ndarray, canonicalize_types) -> Sequence[ir.Value]\n\tif canonicalize_types:\n        x = np.ascontiguousarray(x.view(x.dtype.canonicalize_dtype))\n    else:\n        x = np.ascontiguousarray(x)\n    ir_type = mge_dtype_to_ir_type(x.dtype)\n    if ir_type == ir.BF16Type.get():\n        x = x.astype(np.float32)\n        ir_type = ir.F32Type.get()\n    if ir_type == ir.BoolType.get():\n        x = x.astype(np.uint8)\n        ir_type = ir.IntegerType.get_signless(8)\n    return hlo.ConstantOp(ir_type, x).results\n\n", "CodeBleu_score": 40.16035423458014}]}
{"instance_id": "3345fe4d008b", "generation_result": [{"code": "def _ndarray_constant_handler(\n    val: np.ndarray, canonicalize_types\n) -> Sequence[ir.Value]\n\tif np.any(np.equal(val.strides, 0)) and val.size > 0:\n        zero_stride_axes, non_zero_stride_axes = np.where(\n            np.equal(val.strides, 0)\n        ), np.where(np.not_equal(val.strides, 0))\n        collapsed_val = val.reshape(\n            tuple(val.shape[i] for i in zero_stride_axes)\n            + tuple(val.shape[i] for i in non_zero_stride_axes)\n        )\n        collapsed_val = np.broadcast_to(\n            collapsed_val, tuple(val.shape[i] for i in range(val.ndim))\n        )\n        return hlo.BroadcastInDimOp(\n            _numpy_array_constant(collapsed_val, canonicalize_types)[0],\n            dense_int_elements(val.shape),\n        ).result\n    return _numpy_array_constant(val, canonicalize_types)\n\n", "CodeBleu_score": 38.76403397817172}, {"code": "def _ndarray_constant_handler(\n    val: np.ndarray, canonicalize_types\n) -> Sequence[ir.Value]\n\tif np.any(np.equal(val.strides, 0)) and val.size > 0:\n        zero_stride_axes = np.where(np.equal(val.strides, 0))[0]\n        nonzero_stride_axes = np.where(np.not_equal(val.strides, 0))[0]\n        collapsed_val = np.reshape(val, np.delete(val.shape, zero_stride_axes))\n        collapsed_val = np.broadcast_to(\n            collapsed_val, np.take(val.shape, nonzero_stride_axes)\n        )\n        return _numpy_array_constant(collapsed_val, canonicalize_types)\n    else:\n        return _numpy_array_constant(val, canonicalize_types)\n\n", "CodeBleu_score": 32.749775404935335}, {"code": "def _ndarray_constant_handler(\n    val: np.ndarray, canonicalize_types\n) -> Sequence[ir.Value]\n\tif not val.shape:\n        return _numpy_array_constant(val, canonicalize_types)\n    zero_stride_axes, nonzero_stride_axes = np.where(val.strides == 0)[0], np.where(\n        val.strides != 0\n    )[0]\n    if zero_stride_axes.size:\n        collapsed_val = val[tuple(slice(None) for _ in zero_stride_axes)]\n        collapsed_val = np.broadcast_to(\n            collapsed_val,\n            tuple(val.shape[axis] for axis in zero_stride_axes)\n            + tuple(val.shape[axis] for axis in nonzero_stride_axes),\n        )\n        return _numpy_array_constant(collapsed_val, canonicalize_types)\n    return _numpy_array_constant(val, canonicalize_types)\n\n", "CodeBleu_score": 31.692530866758307}]}
{"instance_id": "91b298678351", "generation_result": [{"code": "def dense_bool_elements(xs: Sequence[bool]) -> ir.DenseElementsAttr\n\tbits = np.packbits(xs, bitorder='little')\n    if len(xs) == 1:\n        bits = np.array([0 if xs[0] is False else 255], dtype=np.uint8)\n    return ir.DenseElementsAttr.get_bool(bits, shape=[len(xs)])\n\n", "CodeBleu_score": 42.53509598747732}, {"code": "def dense_bool_elements(xs: Sequence[bool]) -> ir.DenseElementsAttr\n\tif len(xs) == 1:\n        xs = [xs[0] for _ in range(8)]\n    else:\n        xs = [xs[i % len(xs)] for i in range(8)]\n    bits = np.packbits(xs)\n    attr = ir.DenseElementsAttr.get(\n        ir.IntegerType.get_signless(1), np.array([bits], dtype=np.uint8)\n    )\n    return attr\n\n", "CodeBleu_score": 47.89953079076018}, {"code": "def dense_bool_elements(xs: Sequence[bool]) -> ir.DenseElementsAttr\n\treturn ir.DenseElementsAttr(\n        np.packbits(xs, bitorder=\"little\", axis=0),\n        ir.IntegerType.get_signless(1),\n        ir.ArrayAttr.get([ir.IntegerAttr.get(ir.IntegerType.get_signless(64), len(xs))]),\n    )\n\n", "CodeBleu_score": 29.523193487802445}]}
{"instance_id": "06ebabf1e0ff", "generation_result": [{"code": "def create_tensor_lower(ctx, *args: Union[HLOTensor, Sequence[HLOTensor]])\n\tassert len(args) == 1\n    assert len(ctx.vars_in) == 1\n    assert len(ctx.vars_out) == 1\n    input_var = ctx.vars_in[0]\n    output_var = ctx.vars_out[0]\n    if input_var.has_bound_data():\n        input_var.set_value(input_var.get_bound_data())\n        output_var.set_value(input_var.get_bound_data())\n    elif output_var.has_bound_data():\n        output_var.set_value(output_var.get_bound_data())\n        input_var.set_value(output_var.get_bound_data())\n    else:\n        raise AssertionError(\"Only tensor creation from constants is supported\")\n    assert input_var.shape == output_var.shape\n    data = np.asarray(output_var.get_bound_data(), dtype=output_var.dtype)\n    return HLOTensor(data)\n\n", "CodeBleu_score": 45.136868526911336}, {"code": "def create_tensor_lower(ctx, *args: Union[HLOTensor, Sequence[HLOTensor]])\n\tassert len(args) == 1\n    assert len(ctx.vars_in) == 1\n    assert len(ctx.vars_out) == 1\n    in_var = ctx.vars_in[0]\n    out_var = ctx.vars_out[0]\n    if in_var.has_bound_data():\n        out_var.set_value(in_var.get_value())\n    elif out_var.has_bound_data():\n        in_var.set_value(out_var.get_value())\n    else:\n        raise AssertionError(\"Only tensor creation from constants is supported.\")\n    assert in_var.shape == out_var.shape\n    return HLOTensor(np.asarray(out_var.get_value(), dtype=out_var.dtype))\n\n", "CodeBleu_score": 34.83345849282359}, {"code": "def create_tensor_lower(ctx, *args: Union[HLOTensor, Sequence[HLOTensor]])\n\tif len(args) != 1:\n        raise ValueError(\n            f\"{ctx.op_name} takes one argument, but {len(args)} were given.\"\n        )\n    if len(ctx.vars_in) != 1 or len(ctx.vars_out) != 1:\n        raise ValueError(\n            f\"{ctx.op_name} takes one input and one output, but {len(ctx.vars_in)} inputs and {len(ctx.vars_out)} outputs were given.\"\n        )\n    in_var = ctx.vars_in[0]\n    out_var = ctx.vars_out[0]\n    if in_var.has_bound_data():\n        data = in_var.get_bound_data()\n        out_var.set_bound_data(data)\n    elif out_var.has_bound_data():\n        data = out_var.get_bound_data()\n        in_var.set_bound_data(data)\n    else:\n        raise AssertionError(\n            f\"{ctx.op_name} only supports tensor creation from constants.\"\n        )\n    assert in_var.shape == out_var.shape, (\n        f\"{ctx.op_name} input and output must have the same shape.\"\n    )\n    data = np.asarray(out_var.get_bound_data(), dtype=out_var.dtype)\n    return HLOTensor(data)\n\n", "CodeBleu_score": 40.473160179990344}]}
{"instance_id": "6cd750fa0c8d", "generation_result": [{"code": "def _device_put_scalar(x, device)\n\tx = _scalar_type_to_dtype(x)\n    return _device_put_array(x, device)", "CodeBleu_score": 7.748187644292362}, {"code": "def _device_put_scalar(x, device)\n\tif not isinstance(x, (np.ndarray, numbers.Number)):\n        raise TypeError(\"{} is not a number or a numpy array\".format(x))\n    if isinstance(x, np.ndarray) and x.shape != ():\n        raise ValueError(\"{} is an array with shape {}\".format(x, x.shape))\n    x = _scalar_type_to_dtype(x)\n    return _device_put_array(x, device)\n\n", "CodeBleu_score": 33.14271549515827}, {"code": "def _device_put_scalar(x, device)\n\tif not isinstance(x, (np.ndarray, np.generic)):\n        x = np.asarray(x)\n        if x.shape == ():\n            x = np.asarray(x, dtype=_scalar_type_to_dtype[type(x)])\n    return device_put(x, device)", "CodeBleu_score": 37.7593137183419}]}
{"instance_id": "140de7c2b6a8", "generation_result": [{"code": "def _shape_equal(lhs_shape, rhs_shape)\n\tif isinstance(lhs_shape, np.ndarray):\n        lhs_shape = lhs_shape.tolist()\n    if isinstance(rhs_shape, np.ndarray):\n        rhs_shape = rhs_shape.tolist()\n    assert isinstance(lhs_shape, (tuple, list)) and isinstance(rhs_shape, (tuple, list)), \"The shapes are not lists or tuples.\"\n    if len(lhs_shape) == 0 and len(rhs_shape) == 0:\n        return True\n    assert isinstance(lhs_shape[0], int) and isinstance(rhs_shape[0], int), \"The shapes are not integers.\"\n    if len(lhs_shape) != len(rhs_shape):\n        return False\n    for lhs_dim, rhs_dim in zip(lhs_shape, rhs_shape):\n        if lhs_dim != rhs_dim:\n            return False\n    return True", "CodeBleu_score": 52.208324668082575}, {"code": "def _shape_equal(lhs_shape, rhs_shape)\n\t# Convert to lists if the shapes are NumPy arrays\n    if isinstance(lhs_shape, np.ndarray):\n        lhs_shape = lhs_shape.tolist()\n    if isinstance(rhs_shape, np.ndarray):\n        rhs_shape = rhs_shape.tolist()\n    # Assert that both shapes are either tuples or lists\n    assert isinstance(lhs_shape, (tuple, list)), \"lhs_shape is not a tuple or list\"\n    assert isinstance(rhs_shape, (tuple, list)), \"rhs_shape is not a tuple or list\"\n    # If both shapes are empty, return True\n    if len(lhs_shape) == 0 and len(rhs_shape) == 0:\n        return True\n    # Verify that the first elements of the shapes are integers\n    assert isinstance(lhs_shape[0], int), \"lhs_shape is not an integer\"\n    assert isinstance(rhs_shape[0], int), \"rhs_shape is not an integer\"\n    # If the shapes have different lengths, return False\n    if len(lhs_shape) != len(rhs_shape):\n        return False\n    # Iterate through the elements of both shapes\n    for lhs_dim, rhs_dim in zip(lhs_shape, rhs_shape):\n        # If any corresponding elements are not equal, return False\n        if lhs_dim != rhs_dim:\n            return False\n    # If all checks pass, return True\n    return True", "CodeBleu_score": 53.10679348601647}, {"code": "def _shape_equal(lhs_shape, rhs_shape)\n\tif isinstance(lhs_shape, np.ndarray):\n        lhs_shape = lhs_shape.tolent\n    if isinstance(rhs_shape, np.ndarray):\n        rhs_shape = rhs_shape.tolist()\n    assert isinstance(lhs_shape, tuple) or isinstance(lhs_shape, list), \"lhs_shape must be a tuple or a list.\"\n    assert isinstance(rhs_shape, tuple) or isinstance(rhs_shape, list), \"rhs_shape must be a tuple or a list.\"\n    if len(lhs_shape) == 0 and len(rhs_shape) == 0:\n        return True\n    if len(lhs_shape) != len(rhs_shape):\n        return False\n    if len(lhs_shape) != 0 and not isinstance(lhs_shape[0], int):\n        return False\n    if len(rhs_shape) != 0 and not isinstance(rhs_shape[0], int):\n        return False\n    for lhs_dim, rhs_dim in zip(lhs_shape, rhs_shape):\n        if lhs_dim != rhs_dim:\n            return False\n    return True", "CodeBleu_score": 53.65188220567403}]}
{"instance_id": "55d0c71a259b", "generation_result": [{"code": "def read(\n    training_set_filename,\n    test_set_filename,\n    training_set_metadata_filename,\n    test_set_metadata_filename,\n    dtypes,\n    meta_dtypes,\n)\n\ttraining_set = pd.read_csv(training_set_filename, dtype=dtypes)\n    test_set = pd.read_csv(test_set_filename, dtype=dtypes)\n    training_set_metadata = pd.read_csv(\n        training_set_metadata_filename, dtype=meta_dtypes\n    )\n    meta_dtypes.pop(\"target\")\n    test_set_metadata = pd.read_csv(test_set_metadata_filename, dtype=meta_dtypes)\n    meta_dtypes[\"target\"] = \"category\"\n    return training_set, test_set, training_set_metadata, test_set_metadata\n\n", "CodeBleu_score": 45.935730854413954}, {"code": "def read(\n    training_set_filename,\n    test_set_filename,\n    training_set_metadata_filename,\n    test_set_metadata_filename,\n    dtypes,\n    meta_dtypes,\n)\n\ttraining_set = pd.read_csv(\n        training_set_filename, dtype=dtypes, na_values=[\"na\", \"?\", \"\"]\n    )\n    test_set = pd.read_csv(test_set_filename, dtype=dtypes, na_values=[\"na\", \"?\", \"\"])\n\n    meta_dtypes.pop(\"target\")\n    training_set_metadata = pd.read_csv(\n        training_set_metadata_filename, dtype=meta_dtypes, na_values=[\"na\", \"?\", \"\"]\n    )\n    test_set_metadata = pd.read_csv(\n        test_set_metadata_filename, dtype=meta_dtypes, na_values=[\"na\", \"?\", \"\"]\n    )\n    meta_dtypes[\"target\"] = \"category\"\n\n    return (training_set, test_set, training_set_metadata, test_set_metadata)\n\n", "CodeBleu_score": 36.14681048847215}, {"code": "def read(\n    training_set_filename,\n    test_set_filename,\n    training_set_metadata_filename,\n    test_set_metadata_filename,\n    dtypes,\n    meta_dtypes,\n)\n\ttraining_set = pd.read_csv(\n        training_set_filename, dtype=dtypes, low_memory=False\n    )\n    test_set = pd.read_csv(test_set_filename, dtype=dtypes, low_memory=False)\n    test_set_metadata = pd.read_csv(\n        test_set_metadata_filename, dtype=meta_dtypes, low_memory=False\n    )\n    training_set_metadata = pd.read_csv(\n        training_set_metadata_filename, dtype=meta_dtypes, low_memory=False\n    )\n    return (training_set, test_set, training_set_metadata, test_set_metadata)\n\n", "CodeBleu_score": 27.052305544052995}]}
{"instance_id": "9e5e02cd73e0", "generation_result": [{"code": "def _show_cmap(cmap)\n\tfrom rcmod import axes_style\n    axes_style(\"white\")\n    fig, ax = plt.subplots(figsize=(10, 1))\n    ax.set(xticks=[], yticks=[])\n    x = np.linspace(0, 1, 100)\n    y = np.linspace(0, 1, 100)\n    X, Y = np.meshgrid(x, y)\n    ax.pcolormesh(X, Y, np.outer(x, y), cmap=cmap)\n    plt.show()\n\n", "CodeBleu_score": 56.6749111936643}, {"code": "def _show_cmap(cmap)\n\tfrom rcmod import axes_style\n    axes_style(\"white\")\n    fig, ax = plt.subplots(figsize=(10, 2))\n    ax.set(xticks=[], yticks=[])\n    x = np.linspace(0, 1, 256)\n    x = np.vstack((x, x))\n    ax.pcolormesh(x, cmap=cmap)\n    plt.show()\n\n", "CodeBleu_score": 62.49595196597264}, {"code": "def _show_cmap(cmap)\n\tfrom rcmod import axes_style\n    axes_style(\"white\")\n    fig, ax = plt.subplots(figsize=(6, 0.3))\n    ax.set(xticks=[], yticks=[])\n    x = np.linspace(0, 1, 256)\n    x = np.c_[x, x]\n    ax.pcolormesh(x, cmap=cmap)\n\n", "CodeBleu_score": 62.06671805819061}]}
{"instance_id": "ad0781b5cc38", "generation_result": [{"code": "def transform_to_data_coordinates(obj, xdata, ydata)\n\t# Check if object is an axis.\n    if isinstance(obj, Axes):\n        # Check if object's transform differs from the data transform.\n        if obj.transData != obj.transData.transform:\n            # Get the inverse of the data transform.\n            trans_inv = obj.transData.inverted()\n            # Combine the object's transform with the inverse data transform.\n            trans_inv += obj.transData\n            # Apply the composite transform to the points.\n            xdata, ydata = trans_inv.transform(np.array([xdata, ydata]).T).T\n        # Return the transformed points.\n        return xdata, ydata\n    # Check if object is a patch.\n    elif isinstance(obj, Patch):\n        # Get the axes of the patch.\n        ax = obj.axes\n        # Check if object's transform differs from the data transform.\n        if ax.transData != obj.get_transform().transform:\n            # Get the inverse of the data transform.\n            trans_inv = ax.transData.inverted()\n            # Combine the object's transform with the inverse data transform.\n            trans_inv += obj.get_transform()\n            # Apply the composite transform to the points.\n            xdata, ydata = trans_inv.transform(np.array([xdata, ydata]).T).T\n        # Return the transformed points.\n        return xdata, ydata\n    # Check if object is a text.\n    elif isinstance(obj, Text):\n        # Get the axes of the text.\n        ax = obj.axes\n        # Check if object's transform differs from the data transform.\n        if ax.transData != obj.get_transform().transform:\n            # Get the inverse of the data transform.\n            trans_inv = ax.transData.inverted()\n            # Combine the object's transform with the inverse data transform.\n            trans_inv += obj.get_transform()\n            # Apply the composite transform to the points.\n            xdata, ydata = trans_inv.transform(np.array([xdata, ydata]).T).T\n        # Return the transformed points.", "CodeBleu_score": 44.06125023442811}, {"code": "def transform_to_data_coordinates(obj, xdata, ydata)\n\t# If the axes are not defined, the transform is the identity transform.\n    if obj.axes is None:\n        return xdata, ydata\n\n    # If the transform is the same as the data transform, no transformation is needed.\n    if obj.get_transform() == obj.axes.transData:\n        return xdata, ydata\n\n    # Otherwise, combine the object's transform with the inverse data transform.\n    else:\n        trans = obj.get_transform() - obj.axes.transData\n        return trans.transform(np.array([xdata, ydata]).T).T", "CodeBleu_score": 42.123644841297065}, {"code": "def transform_to_data_coordinates(obj, xdata, ydata)\n\tax = obj.axes\n    if ax is None:\n        return xdata, ydata\n    transform = ax.transData\n    return transform.transform(obj.get_transform().inverted().transform((xdata, ydata)))\n", "CodeBleu_score": 27.730634252824014}]}
{"instance_id": "bf776cadef8d", "generation_result": [{"code": "def kdeplot(self, data=None, data2=None, *args, **kwargs)\n\tdata = self._maybe_target_series(data, key='data')\n\n    if data2 is not None:\n        if not pd.api.types.is_list_like(data2):\n            data2 = self._df[data2]\n    return self._module.kdeplot(data, data2=data2, *args, **kwargs)", "CodeBleu_score": 78.44847260741699}, {"code": "def kdeplot(self, data=None, data2=None, *args, **kwargs)\n\tdata = self._maybe_target_series(data, key='data')\n\n    if data2 is not None:\n        if not pd.api.types.is_list_like(data2):\n            data2 = self._df[data2]\n    return self._module.kdeplot(data, data2=data2, *args, **kwargs)\n\n", "CodeBleu_score": 78.44847260741699}, {"code": "def kdeplot(self, data=None, data2=None, *args, **kwargs)\n\tdata = self._maybe_target_series(data, key='data')\n\n    if data2 is not None:\n        if not pd.api.types.is_list_like(data2):\n            data2 = self._df[data2]\n    return self._module.kdeplot(data, data2=data2, *args, **kwargs)", "CodeBleu_score": 78.44847260741699}]}
{"instance_id": "fb44964174d9", "generation_result": [{"code": "def plot_most_used_words(df)\n\tdf.loc[df.tokens == \"programa\", \"tokens\"] = \"programar\"\n    df = df[df.tokens.str.isalpha()]\n    df = df[df.tokens.str.len() > 1]\n    df = df[df.tokens.isin(stopwords.words(\"portuguese\")) == False]\n    df = df.tokens.value_counts()[:20]\n    plt.figure(figsize=(10, 5))\n    sns.barplot(x=df.index, y=df.values, color=\"blue\")\n    plt.xlabel(\"Word\")\n    plt.ylabel(\"Frequency\")\n    plt.title(\"Most Frequent Words\")\n    plt.savefig(\"most_frequent_words.png\")", "CodeBleu_score": 29.584167421633516}, {"code": "def plot_most_used_words(df)\n\tdf['token'] = df['token'].str.replace('programa', 'programar')\n    df = df[df['token'].str.isalpha() & (df['token'].str.len() > 1) & ~df['token'].isin(stopwords.words('portuguese'))]\n    df = df['token'].value_counts().nlargest(20).reset_index()\n    plt.figure(figsize=(12, 5))\n    sns.barplot(x='token', y='index', data=df)\n    plt.xlabel('Words')\n    plt.ylabel('Count')\n    plt.title('Most frequent words')\n    plt.savefig('most_used_words.png')", "CodeBleu_score": 37.327101061405955}, {"code": "def plot_most_used_words(df)\n\tdf.loc[df[\"token\"] == \"programa\", \"token\"] = \"programar\"\n    df_words = df[df[\"is_alpha\"] & (df[\"token\"].str.len() > 1) & ~df[\"is_stop\"]]\n    df_words[\"token\"].value_counts().head(20).plot(kind=\"bar\")\n    plt.xlabel(\"Word\")\n    plt.ylabel(\"Count\")\n    plt.title(\"Most frequent words\")\n    plt.savefig(\"most_used_words.png\")\n\n", "CodeBleu_score": 26.333642751862996}]}
{"instance_id": "13414a7eb4e3", "generation_result": [{"code": "def dask_rolling_wrapper(moving_func, a, window, min_count=None, axis=-1)\n\t# Import Dask array as da and promote the data type of the input array a to handle edge cases, setting the fill value accordingly\n    da = import_da()\n    a = da.from_array(a, chunks=a.chunks)\n    dtype = da.dtypes.maybe_promote(a.dtype)\n    if min_count is None:\n        fill_value = (a.mean() * window).astype(dtype)\n    else:\n        fill_value = a.mean().astype(dtype)\n\n    # Calculate the depth of overlap for the specified axis\n    depth = (window - 1) // 2\n\n    # Set the boundary fill values\n    boundary = da.full((a.ndim, 2), fill_value)\n    boundary[axis, 0] = a.mean(axis=axis).compute()\n\n    # Create an overlapped array ag using da.overlap.overlap\n    ag = da.overlap.overlap(a, depth=depth, boundary=boundary, trim=False)\n\n    # Apply the moving_func to the overlapped array using da.map_blocks with the specified window size and min_count\n    agg = da.map_blocks(moving_func, ag, window, axis=axis, min_count=min_count, dtype=dtype)\n\n    # Trim the overlapped regions using da.overlap.trim_internal and return the resulting array\n    return da.overlap.trim_internal(agg)\n\n", "CodeBleu_score": 41.41512854959202}, {"code": "def dask_rolling_wrapper(moving_func, a, window, min_count=None, axis=-1)\n\t# Import Dask array as da and promote the data type of the input array a to handle edge cases\n    da = pyimport(\"dask.array\")\n    dtype = da.dtypes.maybe_promote(a.dtype)\n    a = a.astype(dtype)\n\n    # Set the fill value according to the data type\n    if dtype == \"float64\":\n        fill_value = np.nan\n    else:\n        fill_value = da.dtypes.get_fill_value(dtype)\n\n    # Calculate the depth of overlap for the specified axis\n    depth = (window - 1) // 2\n\n    # Set the boundary fill values\n    boundary = np.empty(len(a.chunks[axis]), dtype=object)\n    boundary[:] = fill_value\n    left_boundary = boundary[:depth][::-1]\n    right_boundary = boundary[depth:]\n\n    # Create an overlapped array ag using da.overlap.overlap\n    ag = da.overlap.overlap(a, depth=depth, boundary=boundary, axis=axis)\n\n    # Apply the moving_func to the overlapped array using da.map_blocks with the specified window size and min_count\n    agg = da.map_blocks(moving_func, ag, window, axis=axis, min_count=min_count)\n\n    # Trim the overlapped regions using da.overlap.trim_internal and return the resulting array\n    return da.overlap.trim_internal(agg, depth, left_boundary, right_boundary, axis=axis)\n\n", "CodeBleu_score": 42.28050148476346}, {"code": "def dask_rolling_wrapper(moving_func, a, window, min_count=None, axis=-1)\n\t# Promote the data type of the input array a to handle edge cases, setting the fill value accordingly.\n    a = da.asarray(a)\n    dtype = da.utils.dtypes.maybe_promote(a.dtype)\n    fill_value = da.utils.dtypes.get_fill_value(dtype)\n\n    # Calculate the depth of overlap for the specified axis.\n    depth = (window - 1) // 2\n\n    # Set the boundary fill values.\n    boundary = da.ones((depth,) * a.ndim, dtype=a.dtype) * fill_value\n\n    # Create an overlapped array ag using da.overlap.overlap.\n    ag = da.overlap.overlap(a, depth=depth, boundary=boundary, axis=axis)\n\n    # Apply the moving_func to the overlapped array using da.map_blocks with the specified window size and min_count.\n    r = da.map_blocks(moving_func, ag, window, min_count=min_count, axis=axis)\n\n    # Trim the overlapped regions using da.overlap.trim_internal and return the resulting array.\n    return da.overlap.trim_internal(r, depth)", "CodeBleu_score": 35.98880656522421}]}
{"instance_id": "68ce25d325d4", "generation_result": [{"code": "def least_squares(lhs, rhs, rcond=None, skipna=False)\n\tlhs = da.from_array(lhs, chunks=lhs.chunks)\n    rhs = da.from_array(rhs, chunks=rhs.chunks)\n    if skipna:\n        lhs = da.apply_along_axis(lambda x: np.polyfit(x, np.arange(len(x)), 1), axis=0, arr=lhs)\n        rhs = rhs.reshape((rhs.shape[0], rhs.shape[1] * rhs.shape[2]))\n        coeffs, residuals, _, _ = lhs\n        coeffs = coeffs.reshape((coeffs.shape[0], coeffs.shape[1], 1))\n        residuals = residuals.reshape((residuals.shape[0], residuals.shape[1], 1))\n        return coeffs, residuals\n    else:\n        coeffs, residuals, _, _ = da.linalg.lstsq(lhs, rhs, rcond=rcond)\n        return coeffs, residuals", "CodeBleu_score": 41.447361207626166}, {"code": "def least_squares(lhs, rhs, rcond=None, skipna=False)\n\tif skipna:\n        lhs = da.from_array(lhs, chunks=lhs.chunks)\n        rhs = da.from_array(rhs, chunks=rhs.chunks)\n        coeffs = da.apply_along_axis(lambda x: np.polyfit(x, rhs, deg=x.shape[0] - 1), 0, lhs)\n        coeffs = coeffs.reshape((lhs.shape[1], rhs.shape[1], -1))\n        residuals = da.apply_alongs_axis(lambda x: np.polyval(x, lhs), 0, coeffs)\n        residuals = residuals.reshape(rhs.shape)\n        if rhs.shape[1] == 1:\n            residuals = residuals.reshape(rhs.shape[0])\n    else:\n        lhs = da.from_array(lhs, chunks=lhs.chunks)\n        rhs = da.from_array(rhs, chunks=rhs.chunks)\n        if rhs.shape[1] == 1:\n            rhs = rhs.reshape(rhs.shape[0])\n        coeffs, residuals, rank, s = da.linalg.lstsq(lhs, rhs, rcond)\n    return coeffs, residuals\n\n", "CodeBleu_score": 43.850587044610755}, {"code": "def least_squares(lhs, rhs, rcond=None, skipna=False)\n\tda = import_required('dask.array',\n                         'least_squares requires the dask.array package", "CodeBleu_score": 1.5073874225821011}]}
{"instance_id": "c0c2c53b76e0", "generation_result": [{"code": "def from_array(\n        self, data: Any, chunks: T_Chunks | _NormalizedChunks, **kwargs: Any\n    ) -> DaskArray | Any\n\timport dask.array as da\n\n    if isinstance(data, ImplicitToExplicitIndexingAdapter):\n        # lazily loaded backend array classes should use NumPy array operations.\n        kwargs[\"meta\"] = np.ndarray\n\n    return da.from_array(\n        data,\n        chunks,\n        **kwargs,\n    )  # type: ignore[no-untyped-call]", "CodeBleu_score": 92.3542706764267}, {"code": "def from_array(\n        self, data: Any, chunks: T_Chunks | _NormalizedChunks, **kwargs: Any\n    ) -> DaskArray | Any\n\timport dask.array as da\n\n    if isinstance(data, ImplicitToExplicitIndexingAdapter):\n        # lazily loaded backend array classes should use NumPy array operations.\n        kwargs[\"meta\"] = np.ndarray\n\n    return da.from_array(\n        data,\n        chunks,\n        **kwargs,\n    )  # type: ignore[no-untyped-call]", "CodeBleu_score": 92.3542706764267}, {"code": "def from_array(\n        self, data: Any, chunks: T_Chunks | _NormalizedChunks, **kwargs: Any\n    ) -> DaskArray | Any\n\timport dask.array as da\n\n    if isinstance(data, ImplicitToExplicitIndexingAdapter):\n        # lazily loaded backend array classes should use NumPy array operations.\n        kwargs[\"meta\"] = np.ndarray\n\n    return da.from_array(\n        data,\n        chunks,\n        **kwargs,\n    )  # type: ignore[no-untyped-call]\n\n", "CodeBleu_score": 92.3542706764267}]}
{"instance_id": "6559019d09a0", "generation_result": [{"code": "def __dask_postpersist__(\n        self,\n    ) -> tuple[\n        Callable[\n            [Graph, PostPersistCallable[Any], Any, Any],\n            Self,\n        ],\n        tuple[Any, ...],\n    ]\n\tif is_duck_dask_array(self._data):\n        a: tuple[PostPersistCallable[Any], tuple[Any, ...]]\n        a = self._data.__dask_postpersist__()  # type: ignore[no-untyped-call]\n        array_func, array_args = a\n\n        return self._dask_finalize, (array_func,) + array_args\n    else:\n        raise AttributeError(\"Method requires self.data to be a dask array.\")", "CodeBleu_score": 94.10588162387555}, {"code": "def __dask_postpersist__(\n        self,\n    ) -> tuple[\n        Callable[\n            [Graph, PostPersistCallable[Any], Any, Any],\n            Self,\n        ],\n        tuple[Any, ...],\n    ]\n\tif is_duck_dask_array(self._data):\n        a: tuple[PostPersistCallable[Any], tuple[Any, ...]]\n        a = self._data.__dask_postpersist__()  # type: ignore[no-untyped-call]\n        array_func, array_args = a\n\n        return self._dask_finalize, (array_func,) + array_args\n    else:\n        raise AttributeError(\"Method requires self.data to be a dask array.\")", "CodeBleu_score": 94.10588162387555}, {"code": "def __dask_postpersist__(\n        self,\n    ) -> tuple[\n        Callable[\n            [Graph, PostPersistCallable[Any], Any, Any],\n            Self,\n        ],\n        tuple[Any, ...],\n    ]\n\t", "CodeBleu_score": 21.982758620689655}]}
{"instance_id": "4a2801bb3851", "generation_result": [{"code": "def _dask_or_eager_func(\n    name,\n    eager_module=np,\n    dask_module=\"dask.array\",\n)\n\tdef f(*args, **kwargs):\n        if any(is_duck_dask_array(a) for a in args):\n            module = import_module(dask_module)\n            func = getattr(module, name)\n        else:\n            module = eager_module\n            func = getattr(module, name)\n        return func(*args, **kwargs)\n    return f", "CodeBleu_score": 55.27418874593768}, {"code": "def _dask_or_eager_func(\n    name,\n    eager_module=np,\n    dask_module=\"dask.array\",\n)\n\tdef f(*args, **kwargs):\n        if any(is_duck_dask_array(a) for a in args):\n            module = import_module(dask_module)\n            func = getattr(module, name)\n        else:\n            module = eager_module\n            func = getattr(module, name)\n        return func(*args, **kwargs)\n\n    return f", "CodeBleu_score": 55.27418874593768}, {"code": "def _dask_or_eager_func(\n    name,\n    eager_module=np,\n    dask_module=\"dask.array\",\n)\n\tdef f(*args, **kwargs):\n        if any(is_duck_dask_array(arg) for arg in args):\n            module = import_module(dask_module)\n            func = getattr(module, name)\n        else:\n            func = getattr(eager_module, name)\n        return func(*args, **kwargs)\n    return f", "CodeBleu_score": 52.5614040826712}]}
{"instance_id": "eef8d746d01e", "generation_result": [{"code": "def lazy_array_equiv(arr1, arr2)\n\tif arr1 is arr2:\n        return True\n\n    arr1 = asarray(arr1)\n    arr2 = asarray(arr2)\n\n    if arr1.shape != arr2.shape:\n        return False\n\n    if is_duck_dask_array(arr1) and is_duck_dask_array(arr2):\n        return tokenize(arr1) == tokenize(arr2)\n\n    return None\n\n", "CodeBleu_score": 70.36826912476108}, {"code": "def lazy_array_equiv(arr1, arr2)\n\tif arr1 is arr2:\n        return True\n\n    arr1 = asarray(arr1)\n    arr2 = asarray(arr2)\n\n    if arr1.shape != arr2.shape:\n        return False\n\n    if is_duck_dask_array(arr1):\n        arr1 = arr1.compute()\n    if is_duck_dask_array(arr2):\n        arr2 = arr2.compute()\n\n    if is_duck_dask_array(arr1) and is_duck_dask_array(arr2):\n        return tokenize(arr1) == tokenize(arr2)\n\n", "CodeBleu_score": 72.19700817961072}, {"code": "def lazy_array_equiv(arr1, arr2)\n\tif arr1 is arr2:\n        return True\n\n    arr1 = asarray(arr1)\n    arr2 = asarray(arr2)\n\n    if arr1.shape != arr2.shape:\n        return False\n\n    if hasattr(arr1, \"__dask_tokenize__\"):\n        return tokenize(arr1) == tokenize(arr2)\n\n", "CodeBleu_score": 53.18221784645223}]}
{"instance_id": "15e830ff5fcb", "generation_result": [{"code": "def randn(shape, frac_nan=None, chunks=None, seed=0)\n\tif chunks is None:\n        return np.random.RandomState(seed).standard_normal(shape)\n    else:\n        return da.random.RandomState(seed).standard_normal(shape, chunks=chunks)\n\n", "CodeBleu_score": 19.041554797332587}, {"code": "def randn(shape, frac_nan=None, chunks=None, seed=0)\n\tif chunks is None:\n        rng = np.random.RandomState(seed)\n        x = rng.standard_normal(shape)\n        if frac_nan is not None:\n            x[rng.choice(range(x.size), int(x.size * frac_nan), replace=False)] = np.nan\n        return x\n    else:\n        return da.random.standard_normal(shape, chunks=chunks)\n\n", "CodeBleu_score": 46.290998219432765}, {"code": "def randn(shape, frac_nan=None, chunks=None, seed=0)\n\tif chunks is not None:\n        import dask.array as da\n        rng = da.random.RandomState(seed)\n        x = rng.standard_normal(shape, chunks=chunks)\n    else:\n        import numpy as np\n        rng = np.random.RandomState(seed)\n        x = rng.standard_normal(shape)\n    if frac_nan is not None:\n        inds = rng.choice(list(range(x.size)), int(x.size * frac_nan), replace=False)\n        x.flat[inds] = np.nan\n    return x\n\n", "CodeBleu_score": 76.08373160097173}]}
{"instance_id": "5b57abbbe38b", "generation_result": [{"code": "def analyst_price_target(self) -> pd.DataFrame\n\tif self._analyst_price_target is None:\n        raise YFNotImplementedError(\"analyst_price_target\")\n    else:\n        return self._analyst_price_target\n\n", "CodeBleu_score": 52.81873886621548}, {"code": "def analyst_price_target(self) -> pd.DataFrame\n\tif self._analyst_price_target is None:\n        raise YFNotImplementedError(\"analyst_price_target\")\n    return self._analyst_price_target\n\n", "CodeBleu_score": 80.21012753152378}, {"code": "def analyst_price_target(self) -> pd.DataFrame\n\tif self._analyst_price_target is None:\n        raise YFNotImplementedError('analyst_price_target')\n    return self._analyst_price_target\n\n\n\n\n\n", "CodeBleu_score": 89.72656559975366}]}
{"instance_id": "592e414671e9", "generation_result": [{"code": "def _fetch_time_series(self, name, timescale, proxy=None)\n\tif name not in const.fundamentals_keys:\n        raise ValueError(\n            f\"Invalid name provided. Name must be one of {const.fundamentals_keys.keys()}.\"\n        )\n\n    if timescale not in const.fundamentals_timescales:\n        raise ValueError(\n            f\"Invalid timescale provided. Timescale must be one of {const.fundamentals_timescales}.\"\n        )\n\n    try:\n        return self._create_financials_table(name, timescale, proxy)\n    except Exception as e:\n        self._logger.error(e)\n        return pd.DataFrame()\n\n", "CodeBleu_score": 32.223407039130144}, {"code": "def _fetch_time_series(self, name, timescale, proxy=None)\n\tif name not in const.fundamentals_keys:\n        raise ValueError(\n            f\"The name {name} is not supported. Supported names are {const.fundamentals_keys}\"\n        )\n\n    if timescale not in const.fundamentals_timescales:\n        raise ValueError(\n            f\"The timescale {timescale} is not supported. Supported timescales are {const.fundamentals_timescales}\"\n        )\n\n    try:\n        return self._create_financials_table(name, timescale, proxy)\n    except Exception as e:\n        logger.error(e)\n        return pd.DataFrame()\n\n\n", "CodeBleu_score": 32.77128001251072}, {"code": "def _fetch_time_series(self, name, timescale, proxy=None)\n\tif name not in const.fundamentals_keys:\n        raise ValueError(\n            \"Invalid financial statement. \"\n            \"Valid statements are: \"\n            f\"{', '.join(const.fundamentals_keys)}\"\n        )\n\n    if timescale not in const.fundamentals_timescales:\n        raise ValueError(\n            \"Invalid timescale. \"\n            \"Valid timescales are: \"\n            f\"{', '.join(const.fundamentals_timescales)}\"\n        )\n\n    try:\n        return self._create_financials_table(name, timescale, proxy)\n    except Exception as e:\n        self._logger.error(f\"Failed to fetch financial data. Error: {e}\")\n        return pd.DataFrame()\n\n", "CodeBleu_score": 37.841838944455475}]}
{"instance_id": "af01d9bae091", "generation_result": [{"code": "def _parse_fund_ownership(self, data)\n\tself._mutualfund = pd.DataFrame()\n    if data:\n        owners = data.get('owners')\n        if owners:\n            for owner in owners:\n                owner.pop('maxAge')\n                owner['reportDate'] = pd.to_datetime(owner['reportDate'])\n            self._mutualfund = pd.DataFrame(owners)\n        if not self._mutualfund.empty:\n            self._mutualfund.rename(columns={'reportDate': 'Date'}, inplace=True)\n            self._mutualfund.rename(columns={'reportedHolding': 'Holding'}, inplace=True)\n            self._mutualfund.rename(columns={'reportedTitle': 'Title'}, inplace=True)\n            self._mutualfund.rename(columns={'value': 'Value'}, inplace=True)", "CodeBleu_score": 32.76947109180065}, {"code": "def _parse_fund_ownership(self, data)\n\tself._mutualfund = pd.DataFrame()\n    if data.get('fundOwnershipList') is not None:\n        self._mutualfund = pd.DataFrame(data['fundOwnershipList'])\n        self._mutualfund.drop(columns=['maxAge'], inplace=True)\n        for owner in self._mutualfund.items:\n            self._parse_raw_values(owner)\n        if not self._mutualfund.empty:\n            self._mutualfund['reportDate'] = pd.to_datetime(self._mutualfund['reportDate'])\n            self._mutualfund.rename(columns={'reportDate': 'Report Date', 'organization': 'Owner', 'positionDirect': 'Direct Held', 'positionIndirect': 'Indirect Held', 'positionSummary': 'Total Held', 'positionDirectDate': 'Direct Held Date', 'positionIndirectDate': 'Indirect Held Date', 'positionSummaryDate': 'Total Held Date'}, inplace=True)", "CodeBleu_score": 29.840459112884798}, {"code": "def _parse_fund_ownership(self, data)\n\tself._mutualfund = pd.DataFrame()\n    if 'fundOwnershipList' in data:\n        for owner in data['fundOwnershipList']:\n            if 'reportDate' in owner:\n                owner['reportDate'] = self._parse_raw_values(owner['reportDate'], 'datetime')\n            else:\n                owner['reportDate'] = None\n            if 'maxAge' in owner:\n                del owner['maxAge']\n            self._mutualfund = self._mutualfund.append(owner, ignore_index=True)\n        if not self._mutualfund.empty:\n            self._mutualfund.rename(columns={'reportDate': 'Date', 'reportedHoldings': 'Holdings'}, inplace=True)\n            self._mutualfund['Date'] = pd.to_datetime(self._mutualfund['Date'])", "CodeBleu_score": 35.17737032742313}]}
{"instance_id": "2b5c6f1253db", "generation_result": [{"code": "def currency(self)\n\t# If the currency attribute is already set, return it.\n    if self.currency:\n        return self.currency\n\n    # If not, fetch the metadata for the ticker's history.\n    metadata = self.get_history_metadata()\n\n    # Extract the currency information from the metadata.\n    currency = metadata[\"currency\"]\n\n    # Assign the currency information to the currency attribute.\n    self.currency = currency\n\n    # Return the currency.\n    return currency\n\n\n\n\n", "CodeBleu_score": 40.42976505119675}, {"code": "def currency(self)\n\tif self.currency:\n        return self.currency\n    else:\n        metadata = self.get_history_metadata()\n        self.currency = metadata[\"currency\"]\n        return self.currency", "CodeBleu_score": 43.81915176465073}, {"code": "def currency(self)\n\tif self.currency:\n        return self.currency\n    else:\n        metadata = get_history_metadata(self.ticker)\n        self.currency = metadata[\"currency\"]\n        return self.currency\n\n\n\n\n", "CodeBleu_score": 44.50664367484041}]}
{"instance_id": "16b80be60e3b", "generation_result": [{"code": "def recommendations(self) -> pd.DataFrame\n\tif self._recommendations is None:\n        self._recommendations = pd.DataFrame()\n        result = self._fetch(self._proxy, [\"recommendationTrend\"])\n        if result is None:\n            return self._recommendations\n        try:\n            self._recommendations = pd.DataFrame(result[\"recommendationTrend\"][\"trend\"])\n        except KeyError as e:\n            raise YFDataException(f\"Failed to parse recommendation trend data: {e}\")\n    return self._recommendations\n\n", "CodeBleu_score": 50.85167741464274}, {"code": "def recommendations(self) -> pd.DataFrame\n\tif self._recommendations is None:\n        self._fetch_recommendations()\n    if self._recommendations is None:\n        self._recommendations = pd.DataFrame()\n    return self._recommendations\n\n", "CodeBleu_score": 20.47252956680539}, {"code": "def recommendations(self) -> pd.DataFrame\n\tif self._recommendations is None:\n        self._recommendations = self._fetch(self._proxy, [\"recommendationTrend\"])\n        if self._recommendations is None:\n            self._recommendations = pd.DataFrame()\n        else:\n            try:\n                self._recommendations = pd.DataFrame(self._recommendations[\"quoteSummary\"][\"result\"][0][\"recommendationTrend\"][\"trend\"])\n            except (KeyError, TypeError):\n                raise YFDataException(\"Failed to parse recommendations data\")\n    return self._recommendations\n\n", "CodeBleu_score": 51.571560952977194}]}
{"instance_id": "abaf8c2a3ecc", "generation_result": [{"code": "def plot_elbow_curve(clf, X, title='Elbow Plot', cluster_ranges=None, n_jobs=1,\n                     show_cluster_time=True, ax=None, figsize=None,\n                     title_fontsize=\"large\", text_fontsize=\"medium\")\n\tif not hasattr(clf, 'n_clusters'):\n        raise TypeError(f'{clf} does not have the attribute n_clusters.')\n    if cluster_ranges is None:\n        cluster_ranges = range(1, 11)\n    if ax is None:\n        fig, ax = plt.subplots(figsize=figsize)\n    ax2 = ax.twinx()\n    ax.set_title(title, fontsize=title_fontsize)\n    ax.set_xlabel('Number of Clusters', fontsize=text_fontsize)\n    ax.set_ylabel('Sum of Squared Errors', fontsize=text_fontsize)\n    ax2.set_ylabel('Clustering Time (s)', fontsize=text_fontsize)\n    ax.grid()\n    ax.tick_params(axis='both', which='major', labelsize=text_fontsize)\n    ax2.tick_params(axis='both', which='major', labelsize=text_fontsize)\n    results = Parallel(n_jobs=n_jobs)(delayed(score_clusterer)(\n        clf, X, n_clusters=n_clusters) for n_clusters in cluster_ranges)\n    sse = [result['sse'] for result in results]\n    if show_cluster_time:\n        clustering_time = [result['clustering_time']\n                           for result in results]\n        ax2.plot(cluster_ranges, clustering_time, '--', color='black')\n    ax.plot(cluster_ranges, sse, '-o', color='black')\n    return ax", "CodeBleu_score": 53.900607504461505}, {"code": "def plot_elbow_curve(clf, X, title='Elbow Plot', cluster_ranges=None, n_jobs=1,\n                     show_cluster_time=True, ax=None, figsize=None,\n                     title_fontsize=\"large\", text_fontsize=\"medium\")\n\t# Check if the classifier has the attribute 'n_clusters'\n    if not hasattr(clf, 'n_clusters'):\n        raise TypeError(\"The classifier must have the attribute 'n_clusters'.\")\n    # If no cluster range is specified, use the default\n    if cluster_ranges is None:\n        cluster_ranges = range(1, 11)\n    # Check if the classifier has the attribute 'n_clusters'\n    if not hasattr(clf, 'n_clusters'):\n        raise TypeError(\"The classifier must have the attribute 'n_clusters'.\")\n    # If no cluster range is specified, use the default\n    if cluster_ranges is None:\n        cluster_ranges = range(1, 11)\n    # Create a Parallel object with the specified number of jobs\n    parallel = Parallel(n_jobs=n_jobs)\n    # Create a delayed function that returns the score and duration\n    delayed_score_duration = delayed(_score_duration)\n    # Create a list of delayed functions that return the score and duration\n    delayed_score_durations = [delayed_score_duration(clf, X, n_clusters) for n_clusters in cluster_ranges]\n    # Calculate the scores and durations in parallel\n    scores_durations = parallel(delayed_score_durations)\n    # Sort the scores and durations by cluster number\n    scores_durations.sort(key=lambda x: x[0])\n    # Unpack the scores and durations\n    scores, durations = zip(*scores_durations)\n    # Create a figure with the specified size\n    fig = plt.figure(figsize=figsize)\n    # If no axes object is specified, create one and set the title\n    if ax is None:\n        ax = fig.add_subplot(1, 1, 1)\n        ax.set_title(title, fontsize=title_fontsize)\n    # Plot the sum of squared errors against the number of clusters\n    ax.plot(cluster_ranges, scores, 'bx-')\n    # If specified, plot", "CodeBleu_score": 43.764394933481995}, {"code": "def plot_elbow_curve(clf, X, title='Elbow Plot', cluster_ranges=None, n_jobs=1,\n                     show_cluster_time=True, ax=None, figsize=None,\n                     title_fontsize=\"large\", text_fontsize=\"medium\")\n\t# Check if the classifier has the n_clusters attribute\n    if not hasattr(clf, 'n_clusters'):\n        raise TypeError(\n            \"The classifier must have the n_clusters attribute\")\n\n    # Set the cluster range if not specified\n    if cluster_ranges is None:\n        cluster_ranges = range(1, 11)\n\n    # Create a list to store the SSEs\n    sse_list = []\n\n    # Create a list to store the clustering durations\n    time_list = []\n\n    # Create a list to store the cluster labels\n    labels_list = []\n\n    # Create a list to store the silhouette scores\n    silhouette_list = []\n\n    # Create a list to store the calinski-harabasz scores\n    calinski_harabasz_list = []\n\n    # Create a list to store the davies-bouldin scores\n    davies_bouldin_list = []\n\n    # Create a list to store the adjusted rand scores\n    adjusted_rand_list = []\n\n    # Create a list to store the adjusted mutual info scores\n    adjusted_mutual_info_list = []\n\n    # Create a list to store the homogeneity scores\n    homogeneity_list = []\n\n    # Create a list to store the completeness scores\n    completeness_list = []\n\n    # Create a list to store the v-measure scores\n    v_measure_list = []\n\n    # Create a list to store the fowlkes-mallows scores\n    fowlkes_mallows_list = []\n\n    # Create a list to store the adjusted fowlkes-mallows scores\n    adjusted_fowlkes_mallows_list = []\n\n    # Create a list to store the normalized mutual info scores\n    normalized_mutual_info_list = []\n\n    # Create a list to store the adjusted normalized mutual info scores\n    adjusted_normalized_mutual_info_list = []\n\n    # Create a list to store the accuracy scores\n    accuracy_list = []", "CodeBleu_score": 18.7868017599819}]}
{"instance_id": "8a90815c4ee1", "generation_result": [{"code": "def _clone_and_score_clusterer(clf, X, n_clusters)\n\tstart = time.time()\n    clf = clone(clf)\n    setattr(clf, 'n_clusters', n_clusters)\n    clf.fit(X)\n    end = time.time()\n    return clf.score(X), end - start", "CodeBleu_score": 48.383451828655545}, {"code": "def _clone_and_score_clusterer(clf, X, n_clusters)\n\tstart = time.time()\n    clf_clone = clone(clf)\n    setattr(clf_clone, 'n_clusters', n_clusters)\n    clf_clone.fit(X)\n    end = time.time()\n    return clf_clone.score(X), end - start\n\n", "CodeBleu_score": 33.798635038746994}, {"code": "def _clone_and_score_clusterer(clf, X, n_clusters)\n\tstart_time = time.time()\n    clf_clone = clone(clf)\n    setattr(clf_clone, 'n_clusters', n_clusters)\n    clf_clone.fit(X)\n    return clf_clone.score(X), time.time() - start_time", "CodeBleu_score": 31.88434485662734}]}
{"instance_id": "da40bd5dd81a", "generation_result": [{"code": "def clustering_factory(clf)\n\t# Check if the clustering object has the required methods\n    required_methods = ['fit', 'fit_predict']\n    for method in required_methods:\n        if not hasattr(clf, method):\n            raise TypeError(\"The object does not have the required method: %s\" % method)\n\n    # Define additional methods\n    def plot_silhouette(self, X, y=None, **kwargs):\n        \"\"\"The plot_silhouette method takes the input data X and the labels y as arguments.", "CodeBleu_score": 17.478666410585483}, {"code": "def clustering_factory(clf)\n\t# Check if the clustering object has the required methods\n    if not hasattr(clf, \"fit\"):\n        raise TypeError(\"The object does not have the method 'fit'\")\n    if not hasattr(clf, \"fit_predict\"):\n        raise TypeError(\"The object does not have the method 'fit_predict'\")\n\n    # Define additional methods\n    def plot_silhouette(self, X, labels, metric=\"euclidean\", **kwargs):\n        \"\"\"The function plot_silhouette takes the clustering object, the data X and the labels as arguments.", "CodeBleu_score": 14.425418365111643}, {"code": "def clustering_factory(clf)\n\trequired_methods = ['fit', 'fit_predict']\n    for method in required_methods:\n        if not hasattr(clf, method):\n            raise TypeError(\"The clustering object should have a method called \" + method + \".\")\n\n    additional_methods = {'plot_silhouette': plot_silhouette,\n                          'plot_elbow_curve': plot_elbow_curve}\n\n    for name, method in additional_methods.items():\n        if hasattr(clf, name):\n            warnings.warn(\"The method \" + name + \" is being overridden.\")\n        setattr(clf, name, types.MethodType(method, clf))\n\n    return clf\n\n\n\n\n\n", "CodeBleu_score": 39.84343965319418}]}
{"instance_id": "b8d8ba3c0b1b", "generation_result": [{"code": "def plot_learning_curve(clf, X, y, title='Learning Curve', cv=None,\n                        train_sizes=None, n_jobs=1, scoring=None,\n                        ax=None, figsize=None, title_fontsize=\"large\",\n                        text_fontsize=\"medium\")\n\tif ax is None:\n        _, ax = plt.subplots(1, 1, figsize=figsize)\n\n    if train_sizes is None:\n        train_sizes = np.linspace(0.1, 1.0, 5)\n\n    train_sizes, train_scores, test_scores = learning_curve(\n        clf, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes, scoring=scoring)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n\n    ax.grid()\n    ax.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                    train_scores_mean + train_scores_std, alpha=0.1,\n                    color=\"r\")\n    ax.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                    test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    ax.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n            label=\"Training score\")\n    ax.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n            label=\"Cross-validation score\")\n    ax.legend(loc=\"best\")\n    ax.set_title(title, fontsize=title_fontsize)\n    ax.set_xlabel(\"Training examples\", fontsize=text_fontsize)\n    ax.set_ylabel(\"Accuracy\", fontsize=text_fontsize)\n    ax.tick_params(labelsize=text_fontsize)\n    return ax", "CodeBleu_score": 85.25052633350785}, {"code": "def plot_learning_curve(clf, X, y, title='Learning Curve', cv=None,\n                        train_sizes=None, n_jobs=1, scoring=None,\n                        ax=None, figsize=None, title_fontsize=\"large\",\n                        text_fontsize=\"medium\")\n\tif ax is None:\n        fig, ax = plt.subplots(figsize=figsize)\n\n    ax.set_title(title, fontsize=title_fontsize)\n    ax.set_xlabel(\"Training examples\", fontsize=text_fontsize)\n    ax.set_ylabel(\"Score\", fontsize=text_fontsize)\n\n    train_sizes, train_scores, test_scores = learning_curve(clf, X, y, cv=cv,\n                                                            train_sizes=train_sizes,\n                                                            n_jobs=n_jobs,\n                                                            scoring=scoring)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n\n    ax.grid()\n\n    ax.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                    train_scores_mean + train_scores_std, alpha=0.1,\n                    color=\"r\")\n    ax.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                    test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    ax.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n            label=\"Training score\")\n    ax.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n            label=\"Cross-validation score\")\n\n    ax.legend(loc=\"best\")\n    return ax", "CodeBleu_score": 70.71060577628872}, {"code": "def plot_learning_curve(clf, X, y, title='Learning Curve', cv=None,\n                        train_sizes=None, n_jobs=1, scoring=None,\n                        ax=None, figsize=None, title_fontsize=\"large\",\n                        text_fontsize=\"medium\")\n\tif ax is None:\n        fig, ax = plt.subplots(1, 1, figsize=figsize)\n\n    # Calculate learning curve points\n    train_sizes, train_scores, test_scores = learning_curve(clf, X, y, cv=cv, n_jobs=n_jobs, scoring=scoring, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n\n    # Plot learning curve\n    ax.grid()\n    ax.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                    train_scores_mean + train_scores_std, alpha=0.1,\n                    color=\"r\")\n    ax.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                    test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    ax.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n            label=\"Training score\")\n    ax.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n            label=\"Cross-validation score\")\n    ax.legend(loc=\"best\")\n    ax.set_title(title, fontsize=title_fontsize)\n    ax.set_xlabel(\"Training examples\", fontsize=text_fontsize)\n    ax.set_ylabel(\"Score\", fontsize=text_fontsize)\n    ax.tick_params(labelsize=text_fontsize)\n\n    return ax", "CodeBleu_score": 72.57646315971272}]}
{"instance_id": "e6e76eb35945", "generation_result": [{"code": "def plot_elbow_curve(clf, X, title='Elbow Plot', cluster_ranges=None,\n                     ax=None, figsize=None, title_fontsize=\"large\",\n                     text_fontsize=\"medium\")\n\tif not hasattr(clf, \"n_clusters\"):\n        raise TypeError(\"The classifier must have an n_clusters attribute.\")\n    if cluster_ranges is None:\n        cluster_ranges = range(1, 12, 2)\n    clfs = []\n    for n_clusters in cluster_ranges:\n        clf_copy = clone(clf)\n        setattr(clf_copy, \"n_clusters\", n_clusters)\n        clfs.append(clf_copy)\n    scores = []\n    for clf in clfs:\n        clf.fit(X)\n        scores.append(np.absolute(clf.score(X)))\n    if ax is None:\n        fig, ax = plt.subplots(figsize=figsize)\n    ax.set_xlabel(\"Number of Clusters\", fontsize=text_fontsize)\n    ax.set_ylabel(\"Score\", fontsize=text_fontsize)\n    ax.set_title(title, fontsize=title_fontsize)\n    ax.grid(True)\n    ax.plot(cluster_ranges, scores)\n    ax.tick_params(axis='both', labelsize=text_fontsize)\n    return ax", "CodeBleu_score": 59.029782922973325}, {"code": "def plot_elbow_curve(clf, X, title='Elbow Plot', cluster_ranges=None,\n                     ax=None, figsize=None, title_fontsize=\"large\",\n                     text_fontsize=\"medium\")\n\tif not hasattr(clf, 'n_clusters'):\n        raise TypeError(\"The classifier must have an n_clusters attribute.\")\n    if cluster_ranges is None:\n        cluster_ranges = range(1, 12, 2)\n    clfs = [clone(clf).set_params(n_clusters=n) for n in cluster_ranges]\n    scores = [clf.fit(X).score(X) for clf in clfs]\n    if ax is None:\n        fig, ax = plt.subplots(figsize=figsize)\n    ax.plot(cluster_ranges, np.absolute(scores), 'bx-')\n    ax.grid(True)\n    ax.set_xlabel('Number of clusters', fontsize=text_fontsize)\n    ax.set_ylabel('Score', fontsize=text_fontsize)\n    ax.set_title(title, fontsize=title_fontsize)\n    ax.tick_params(labelsize=text_fontsize)\n    return ax", "CodeBleu_score": 56.62655868035531}, {"code": "def plot_elbow_curve(clf, X, title='Elbow Plot', cluster_ranges=None,\n                     ax=None, figsize=None, title_fontsize=\"large\",\n                     text_fontsize=\"medium\")\n\tif not hasattr(clf, 'n_clusters'):\n        raise TypeError(\"The classifier must have an attribute n_clusters.\")\n    if cluster_ranges is None:\n        cluster_ranges = range(1, 12, 2)\n    clfs = []\n    for n_clusters in cluster_ranges:\n        clf_copy = clone(clf)\n        setattr(clf_copy, 'n_clusters', n_clusters)\n        clfs.append(clf_copy)\n    scores = [clf.fit(X).score(X) for clf in clfs]\n    if ax is None:\n        fig, ax = plt.subplots(figsize=figsize)\n    ax.set_xlabel(\"Number of Clusters\", fontsize=text_fontsize)\n    ax.set_ylabel(\"Score\", fontsize=text_fontsize)\n    ax.plot(cluster_ranges, np.absolute(scores), 'b*-')\n    ax.grid(True)\n    ax.tick_params(labelsize=text_fontsize)\n    ax.set_title(title, fontsize=title_fontsize)\n    return ax", "CodeBleu_score": 59.61314104970792}]}
{"instance_id": "6f4530c8a20f", "generation_result": [{"code": "def plot_pca_2d_projection(clf, X, y, title='PCA 2-D Projection', ax=None,\n                           figsize=None, cmap='Spectral',\n                           title_fontsize=\"large\", text_fontsize=\"medium\")\n\tif ax is None:\n        _, ax = plt.subplots(figsize=figsize)\n\n    X_transformed = clf.transform(X)\n    labels = np.unique(y)\n    colors = get_cmap(cmap).colors\n    colors = colors[:len(labels)]\n    for label, color in zip(labels, colors):\n        ax.scatter(X_transformed[y == label, 0],\n                   X_transformed[y == label, 1],\n                   label=label,\n                   color=color)\n\n    ax.set_title(title, fontsize=title_fontsize)\n    ax.set_xlabel('PC1', fontsize=text_fontsize)\n    ax.set_ylabel('PC2', fontsize=text_fontsize)\n    ax.tick_params(axis='both', which='major', labelsize=text_fontsize)\n    ax.legend(fontsize=text_fontsize)\n    return ax", "CodeBleu_score": 51.11180444689105}, {"code": "def plot_pca_2d_projection(clf, X, y, title='PCA 2-D Projection', ax=None,\n                           figsize=None, cmap='Spectral',\n                           title_fontsize=\"large\", text_fontsize=\"medium\")\n\tif ax is None:\n        _, ax = plt.subplots(figsize=figsize)\n\n    X_pca = clf.transform(X)\n    colors = get_cmap(cmap)(np.linspace(0, 1, len(np.unique(y))))\n    for i, lbl in enumerate(np.unique(y)):\n        ax.scatter(X_pca[y == lbl, 0], X_pca[y == lbl, 1],\n                   color=colors[i], label=lbl)\n\n    ax.set_title(title, fontsize=title_fontsize)\n    ax.set_xlabel('PC 1', fontsize=text_fontsize)\n    ax.set_ylabel('PC 2', fontsize=text_fontsize)\n    ax.tick_params(axis='both', labelsize=text_fontsize)\n    ax.legend(fontsize=text_fontsize)\n\n    return ax", "CodeBleu_score": 45.76009978588649}, {"code": "def plot_pca_2d_projection(clf, X, y, title='PCA 2-D Projection', ax=None,\n                           figsize=None, cmap='Spectral',\n                           title_fontsize=\"large\", text_fontsize=\"medium\")\n\tif ax is None:\n        fig, ax = plt.subplots(figsize=figsize)\n\n    X_transformed = clf.transform(X)\n    x1 = X_transformed[:, 0]\n    x2 = X_transformed[:, 1]\n    labels = np.unique(y)\n    colors = get_cmap(cmap).colors\n    for label, color in zip(labels, colors):\n        ax.scatter(x=x1[y == label], y=x2[y == label],\n                   c=color, label=label)\n\n    ax.set_xlabel('PC 1', fontsize=text_fontsize)\n    ax.set_ylabel('PC 2', fontsize=text_fontsize)\n    ax.legend(fontsize=text_fontsize)\n    ax.set_title(title, fontsize=title_fontsize)\n    return ax\n\n", "CodeBleu_score": 48.64332003753715}]}
{"instance_id": "e7c2775d024c", "generation_result": [{"code": "def barplot(ax)\n\tclip1, clip2 = clip_path(ax)\n    Y1 = np.random.uniform(0.0, 1.0, 16)\n    Y2 = np.random.uniform(0.0, 1.0, 16)\n    X = np.arange(16)\n    ax.bar(X, Y1, color='0.75', width=1, align='center', clip_path=clip1)\n    ax.bar(X, Y2, color='0.50', width=1, align='center', clip_path=clip2)\n    ax.bar(X, Y1, color='r', width=1, align='center', clip_path=clip1)\n    ax.bar(X, Y2, color='r', width=1, align='center', clip_path=clip2)\n    ax.set_xlim(-1, 16)\n    ax.text(0, -0.05, 'Bar plot', color='black', transform=ax.transAxes,\n            ha='left', va='top')\n    ax.text(0, -0.05, 'ax.bar()', color='blue', transform=ax.transAxes,\n            ha='left', va='top', family='monospace')\n\n", "CodeBleu_score": 36.51858441406244}, {"code": "def barplot(ax)\n\tclip1, clip2 = clip_path(ax)\n\n    Y1 = np.random.uniform(0.0, 1.0, 16)\n    Y2 = np.random.uniform(0.0, 1.0, 16)\n\n    ax.bar(np.arange(16) - 0.25, Y1, width=0.5, color='0.75',\n           edgecolor='black', linewidth=1, clip_path=clip1)\n    ax.bar(np.arange(16) + 0.25, Y2, width=0.5, color='0.50',\n           edgecolor='black', linewidth=1, clip_path=clip2)\n\n    ax.bar(np.arange(16) - 0.25, Y1, width=0.5, color='red',\n           edgecolor='black', linewidth=1, clip_path=clip1)\n    ax.bar(np.arange(16) + 0.25, Y2, width=0.5, color='red',\n           edgecolor='black', linewidth=1, clip_path=clip2)\n\n    ax.set_xlim(-1, 16)\n\n    ax.text(0.0, -0.05, 'Bar plot', ha='left', va='top',\n            transform=ax.transAxes)\n    ax.text(0.0, -0.10, r'$\\mathtt{ax.bar()}$', ha='left', va='top',\n            color='blue', transform=ax.transAxes)", "CodeBleu_score": 39.14916873682235}, {"code": "def barplot(ax)\n\tclip1, clip2 = clip_path(ax)\n\n    Y1 = np.random.uniform(0, 1, 16)\n    Y2 = np.random.uniform(0, 1, 16)\n\n    ax.bar(np.arange(16) - 0.25, Y1, 0.5,\n           color=\"0.75\", edgecolor=\"black\", linewidth=1,\n           clip_path=clip1)\n    ax.bar(np.arange(16) + 0.25, Y2, 0.5,\n           color=\"0.50\", edgecolor=\"black\", linewidth=1,\n           clip_path=clip2)\n\n    ax.bar(np.arange(16) - 0.25, Y1, 0.5,\n           color=\"red\", edgecolor=\"black\", linewidth=1,\n           clip_path=clip1)\n    ax.bar(np.arange(16) + 0.25, Y2, 0.5,\n           color=\"red\", edgecolor=\"black\", linewidth=1,\n           clip_path=clip2)\n\n    ax.set_xlim(-1, 16)\n    ax.text(0.5, -0.1, \"Bar plot\",\n            horizontalalignment='center',\n            transform=ax.transAxes)\n    ax.text(0.5, -0.2, r\"$\\mathtt{ax.bar()}$\",\n            horizontalalignment='center',\n            transform=ax.transAxes)\n\n", "CodeBleu_score": 38.20416266878352}]}
{"instance_id": "0ec1d146a32b", "generation_result": [{"code": "def split(n_segment)\n\twidth = 2 * np.pi / n_segment\n    padding = width / 2\n    X0 = np.arange(0, 2 * np.pi, width)\n    X1 = X0 + width\n    return X0, X1", "CodeBleu_score": 28.395972121824652}, {"code": "def split(n_segment)\n\t# The function split calculates the starting and ending positions of segments within a fixed width. It takes the number of segments as input and computes the width of each segment and the padding between segments. The function then determines the starting (X0) and ending (X1) positions of each segment and returns them as two separate arrays.\n\n    # The function split calculates the starting and ending positions of segments within a fixed width. It takes the number of segments as input and computes the width of each segment and the padding between segments. The function then determines the starting (X0) and ending (X1) positions of each segment and returns them as two separate arrays.\n\n    # The function split calculates the starting and ending positions of segments within a fixed width. It takes the number of segments as input and computes the width of each segment and the padding between segments. The function then determines the starting (X0) and ending (X1) positions of each segment and returns them as two separate arrays.\n\n    # The function split calculates the starting and ending positions of segments within a fixed width. It takes the number of segments as input and computes the width of each segment and the padding between segments. The function then determines the starting (X0) and ending (X1) positions of each segment and returns them as two separate arrays.\n\n    # The function split calculates the starting and ending positions of segments within a fixed width. It takes the number of segments as input and computes the width of each segment and the padding between segments. The function then determines the starting (X0) and ending (X1) positions of each segment and returns them as two separate arrays.\n\n    # The function split calculates the starting and ending positions of segments within a fixed width. It takes the number of segments as input and computes the width of each segment and the padding between segments. The function then determines the starting (X0) and ending (X1) positions of each segment and returns them as two separate arrays.\n\n    # The function split calculates the starting and ending positions of segments within a fixed width. It takes the number of segments as input and computes the width of each segment and the padding between segments. The function then determines the starting (X0) and ending (X1) positions of each segment and returns them as two separate arrays.\n\n    # The function split calcul", "CodeBleu_score": 0.7575757575757576}, {"code": "def split(n_segment)\n\twidth = 1 / n_segment\n    padding = width / 2\n    x0 = np.arange(0, 1, width)\n    x1 = np.arange(width, 1 + width, width)\n    return x0, x1\n\n", "CodeBleu_score": 19.923949550446775}]}
{"instance_id": "604a3e93ac0d", "generation_result": [{"code": "def pixelated_text(dpi=100)\n\t# Create a figure with a canvas and a resolution of 100 dpi\n    fig = Figure(figsize=(1, 1), dpi=dpi)\n    canvas = FigureCanvasAgg(fig)\n\n    # Get the figure's axes\n    ax = fig.gca()\n\n    # Remove the axis\n    ax.axis('off')\n\n    # Place the character 'a' at the center of the figure\n    ax.text(0.5, 0.5, 'a', fontsize=75)\n\n    # Draw the canvas\n    canvas.draw()\n\n    # Convert the canvas to a NumPy array\n    image = np.frombuffer(canvas.tostring_argb(), dtype='uint8')\n\n    # Reshape the array to the dimensions specified by dpi\n    image = image.reshape((dpi, dpi, 4))\n\n    # Adjust the color channels by rolling the array along the last axis\n    image = np.roll(image, 3, axis=2)\n\n    # Return the resulting image array\n    return image\n\n", "CodeBleu_score": 60.02464595007131}, {"code": "def pixelated_text(dpi=100)\n\tfig, ax = plt.subplots()\n    ax.axis('off')\n    ax.text(0.5, 0.5, 'a', fontsize=75)\n    canvas = FigureCanvasAgg(fig)\n    canvas.draw()\n    image = np.frombuffer(canvas.tostring_argb(), dtype=np.uint8)\n    image = image.reshape((int(fig.bbox.bounds[3]), int(fig.bbox.bounds[2]), 4))\n    image = np.roll(image, 3, axis=2)\n    return image", "CodeBleu_score": 56.959826304408914}, {"code": "def pixelated_text(dpi=100)\n\tfig = plt.figure(figsize=(1, 1), dpi=dpi)\n    canvas = FigureCanvasAgg(fig)\n    ax = fig.gca()\n    ax.axis('off')\n    ax.text(0.5, 0.5, 'a', fontsize=75, color='black')\n    canvas.draw()\n    image = np.frombuffer(canvas.tostring_argb(), dtype='uint8').reshape(dpi, dpi, 4)\n    image = image[:, :, 1:]\n    image = np.roll(image, 1, axis=2)\n    return image", "CodeBleu_score": 56.731274651322906}]}
{"instance_id": "2a7fc7d560ea", "generation_result": [{"code": "def curve()\n\t# number of components\n    n = np.random.randint(1, 10)\n    # centers\n    centers = np.random.uniform(0, 1, n)\n    # widths\n    widths = np.random.normal(0.1, 0.05, n)\n    # scales\n    scales = np.random.normal(0.5, 0.25, n)\n    # normalize widths and scales\n    widths /= widths.sum()\n    scales /= scales.sum()\n    # generate curve\n    curve = np.zeros(100)\n    for center, width, scale in zip(centers, widths, scales):\n        curve += scale * np.exp(-0.5 * ((np.linspace(0, 1, 100) - center) / width) ** 2)\n    return curve", "CodeBleu_score": 48.64234337478267}, {"code": "def curve()\n\tnum_components = randint(1, 4)\n    centers = np.random.normal(0, 1, num_components)\n    widths = np.random.normal(0, 1, num_components)\n    scales = np.random.normal(0, 1, num_components)\n    widths = widths / widths.sum()\n    scales = scales / scales.sum()\n    x = np.linspace(-4, 4, 80)\n    curve = np.zeros(len(x))\n    for center, width, scale in zip(centers, widths, scales):\n        curve += scale * np.exp(-((x - center) / width) ** 2)\n    return curve", "CodeBleu_score": 43.29558825854688}, {"code": "def curve()\n\t# Generate the number of components\n    n_components = randint(1, 4)\n    # Generate the centers of the components\n    centers = np.linspace(0, 1, n_components)\n    # Generate the widths of the components\n    widths = np.random.normal(0.1, 0.01, n_components)\n    # Normalize the widths to ensure they sum to 1\n    widths = widths / widths.sum()\n    # Generate the scales of the components\n    scales = np.random.uniform(0.5, 1, n_components)\n    # Normalize the scales to ensure they sum to 1\n    scales = scales / scales.sum()\n    # Initialize an array of zeros\n    curve = np.zeros(len(x))\n    # Iteratively add scaled Gaussian components to the array\n    for center, width, scale in zip(centers, widths, scales):\n        curve += scale * np.exp(-((x - center) / width) ** 2)\n    return curve\n\n", "CodeBleu_score": 46.762065460347436}]}
{"instance_id": "be694335ebfc", "generation_result": [{"code": "def difference(derivative, accuracy=1)\n\tderivative += 1\n    radius = accuracy + derivative\n    x = np.linspace(-radius, radius, 2 * radius + 1)\n    v = np.vander(x, derivative)\n    d = np.linalg.inv(v).T[derivative]\n    return d / factorial(derivative - 1), x\n\n", "CodeBleu_score": 53.4305438996244}, {"code": "def difference(derivative, accuracy=1)\n\t# Increment the derivative order and calculate the radius\n    derivative += 1\n    radius = (derivative + accuracy) // 2\n\n    # Define a range of points centered around zero\n    points = np.arange(-radius, radius + 1)\n\n    # Calculate the inverse of the Vandermonde matrix for these points\n    inv_vander = np.vander(points, derivative + 1, True)\n\n    # Return the coefficients scaled by the factorial of one less than the derivative order\n    return inv_vander[derivative], points\n\n", "CodeBleu_score": 43.36857851016668}, {"code": "def difference(derivative, accuracy=1)\n\tderivative += 1\n    radius = accuracy * derivative\n    points = np.arange(-radius, radius + 1)\n    vander = np.vander(points, derivative)\n    inv = np.linalg.inv(vander)\n    return inv[derivative - 1] / factorial(derivative - 1), points\n\n", "CodeBleu_score": 52.698653649280914}]}
{"instance_id": "fa3fb47f33fa", "generation_result": [{"code": "def interpolate(X, Y, T)\n\t# Calculate the distances between consecutive points\n    dist = np.sqrt(np.diff(X)**2 + np.diff(Y)**2)\n    # Calculate the cumulative distances\n    cum_dist = np.cumsum(np.insert(dist, 0, 0))\n    # Interpolate X and Y coordinates at the positions specified by T\n    X_interp = np.interp(T, cum_dist, X)\n    Y_interp = np.interp(T, cum_dist, Y)\n    # Return the interpolated X and Y coordinates along with the total distance covered by the original coordinates\n    return X_interp, Y_interp, cum_dist[-1]\n\n", "CodeBleu_score": 30.449890029125616}, {"code": "def interpolate(X, Y, T)\n\t# Calculate the distances between consecutive points\n    D = np.diff(X)\n\n    # Calculate the cumulative distances\n    C = np.cumsum(D)\n\n    # Calculate the total distance covered by the original coordinates\n    total_dist = C[-1]\n\n    # Use the cumulative distances to interpolate the X and Y coordinates at the positions specified by T\n    X_T = np.interp(T, C, X)\n    Y_T = np.interp(T, C, Y)\n\n    # Return the interpolated X and Y coordinates along with the total distance covered by the original coordinates\n    return X_T, Y_T, total_dist", "CodeBleu_score": 20.88463516067907}, {"code": "def interpolate(X, Y, T)\n\t# Calculate the distances between consecutive points\n    dist = np.diff(X)\n\n    # Calculate the cumulative distances\n    cum_dist = np.cumsum(dist)\n\n    # Interpolate the X and Y coordinates at the positions specified by T\n    X_interp = np.interp(T, cum_dist, X)\n    Y_interp = np.interp(T, cum_dist, Y)\n\n    # Calculate the total distance covered by the original coordinates\n    total_dist = np.sum(dist)\n\n    # Return the interpolated X and Y coordinates along with the total distance\n    return X_interp, Y_interp, total_dist", "CodeBleu_score": 18.711176214017087}]}
{"instance_id": "870a2dad3c66", "generation_result": [{"code": "def mandelbrot_set(xmin, xmax, ymin, ymax, xn, yn, maxiter, horizon=2.0)\n\tC = np.linspace(xmin, xmax, xn) + 1j*np.linspace(ymin, ymax, yn)\n    N = np.zeros(C.shape, dtype=int)\n    Z = np.zeros(C.shape, dtype=complex)\n    for n in range(maxiter):\n        I = np.less(N, maxiter)\n        Z[I] = Z[I]**2 + C[I]\n        N[I] += 1\n        I = np.greater(abs(Z), horizon)\n        N[I] = maxiter\n    return Z, N", "CodeBleu_score": 51.86235566327536}, {"code": "def mandelbrot_set(xmin, xmax, ymin, ymax, xn, yn, maxiter, horizon=2.0)\n\t# Create a grid of complex numbers C from the specified ranges\n    x = np.linspace(xmin, xmax, xn, dtype=np.float32)\n    y = np.linspace(ymin, ymax, yn, dtype=np.float32)\n    C = x + 1j*y[:, None]\n    # Initialize iteration count array N and complex array Z\n    N = np.zeros(C.shape, dtype=np.int32)\n    Z = np.zeros(C.shape, dtype=np.complex64)\n    # Iteratively compute the Mandelbrot sequence, updating N with the iteration count where the magnitude of Z exceeds the horizon\n    for n in range(maxiter):\n        Z = Z**2 + C\n        N[np.less(abs(Z), horizon)] = n\n    # Points that remain bounded after maxiter iterations are set to 0 in N\n    N[N == maxiter-1] = 0\n    return Z, N\n\n", "CodeBleu_score": 47.95106132574291}, {"code": "def mandelbrot_set(xmin, xmax, ymin, ymax, xn, yn, maxiter, horizon=2.0)\n\t# Create a grid of complex numbers C from the specified ranges\n    x = np.linspace(xmin, xmax, xn, dtype=np.float32)\n    y = np.linspace(ymin, ymax, yn, dtype=np.float32)\n    C = x[:, None] + 1j*y[None, :]\n    # Initialize iteration count array N and complex array Z\n    N = np.zeros(C.shape, dtype=np.uint8)\n    Z = np.zeros(C.shape, np.complex64)\n    # Iteratively compute the Mandelbrot set\n    for n in range(maxiter):\n        # Compute new Z\n        Z = Z**2 + C\n        # Check for divergence\n        idx = np.less(abs(Z), horizon)\n        # Update iteration count\n        N[idx] = n\n        # All points have diverged\n        if np.all(idx == False):\n            break\n    return Z, N", "CodeBleu_score": 50.30851903812745}]}
{"instance_id": "e1a908945acf", "generation_result": [{"code": "def smooth2d(A, sigma=3)\n\twindow_len = int(max(1, 2 * int(sigma * 3) + 1))\n    window_len = int(window_len / 2) * 2 + 1\n    A = np.asarray(A)\n    out1 = np.array(\n        [smooth1d(A[i, :], window_len) for i in range(A.shape[0])]\n    )\n    out2 = np.array(\n        [smooth1d(out1[:, i], window_len) for i in range(out1.shape[1])]\n    )\n    out2 = out2.T\n    return out2", "CodeBleu_score": 42.44248127047639}, {"code": "def smooth2d(A, sigma=3)\n\twindow_len = max(\n        int(sigma * 4), 3\n    )  # ensure window is odd, at least 3\n    window_len = (window_len // 2) * 2 + 1  # ensure window is odd\n    A = smooth1d(A, window_len)\n    A = np.transpose(smooth1d(np.transpose(A), window_len))\n    return A", "CodeBleu_score": 33.055661690473876}, {"code": "def smooth2d(A, sigma=3)\n\twindow_len = int(max(2, 2 * np.ceil(sigma) + 1))\n    A_smooth = np.zeros(np.shape(A))\n    for i in range(np.shape(A)[0]):\n        A_smooth[i, :] = smooth1d(A[i, :], window_len)\n    A_smooth = np.transpose(A_smooth)\n    for i in range(np.shape(A)[1]):\n        A_smooth[:, i] = smooth1d(A_smooth[:, i], window_len)\n    A_smooth = np.transpose(A_smooth)\n    return A_smooth", "CodeBleu_score": 24.790330696387723}]}
{"instance_id": "94563c54b483", "generation_result": [{"code": "def process_image(self, padded_src, dpi)\n\tsrc_w, src_h = padded_src.shape[:2]\n    pixels_per_inch = dpi / 25.4\n    ox_pixels = int(self.ox * pixels_per_inch)\n    oy_pixels = int(self.oy * pixels_per_inch)\n    # shift image\n    padded_src = np.roll(padded_src, ox_pixels, axis=1)\n    padded_src = np.roll(padded_src, oy_pixels, axis=0)\n    return padded_src", "CodeBleu_score": 33.59965958357037}, {"code": "def process_image(self, padded_src, dpi)\n\t# Get the offsets from the instance's attributes\n    ox, oy = self.ox, self.oy\n    # Convert the offsets to pixels\n    ox_pixels = int(ox * dpi)\n    oy_pixels = int(oy * dpi)\n    # Shift the image horizontally and vertically\n    shifted_image = np.roll(padded_src, ox_pixels, axis=1)\n    shifted_image = np.roll(shifted_image, oy_pixels, axis=0)\n    # Return the processed image\n    return shifted_image\n\n\n\n\n", "CodeBleu_score": 22.260780672115803}, {"code": "def process_image(self, padded_src, dpi)\n\toffset_x_px = self.offset_x * dpi\n    offset_y_px = self.offset_y * dpi\n    return np.roll(padded_src, (offset_y_px, offset_x_px), axis=(0, 1))", "CodeBleu_score": 19.707031082790685}]}
{"instance_id": "d71d232872fe", "generation_result": [{"code": "def gaussian(shape=(25, 25), width=0.5, center=0.0)\n\tshape = tuple(shape)\n    width = tuple([width] * len(shape))\n    center = tuple([center] * len(shape))\n    grid = np.meshgrid(*[np.arange(i) for i in shape], indexing='ij')\n    grid = np.stack(grid, axis=-1)\n\n    grid = (grid - center) / width\n    grid = np.square(grid).sum(axis=-1)\n    return np.exp(-0.5 * grid)", "CodeBleu_score": 19.942604870519965}, {"code": "def gaussian(shape=(25, 25), width=0.5, center=0.0)\n\tshape = tuple(shape)\n    width = tuple(width)\n    center = tuple(center)\n    grid = np.meshgrid(*[np.linspace(-1, 1, size) for size in shape], indexing='ij')\n    grid = np.array(grid).reshape(len(shape), -1).T\n    dist = np.sum((grid - center)**2 / width**2, axis=1)\n    return np.exp(-0.5 * dist)\n\n\n\n\n", "CodeBleu_score": 16.821023461978942}, {"code": "def gaussian(shape=(25, 25), width=0.5, center=0.0)\n\tif isinstance(shape, int):\n        shape = (shape, shape)\n    if isinstance(width, float):\n        width = (width, width)\n    if isinstance(center, float):\n        center = (center, center)\n    grid = np.meshgrid(*[np.linspace(0, s - 1, s) for s in shape], indexing='ij')\n    grid = np.stack(grid, axis=-1)\n    grid = np.reshape(grid, shape + (len(shape),))\n    grid = grid - np.array(center)\n    grid = np.square(grid / np.array(width))\n    grid = np.sum(grid, axis=-1)\n    grid = np.exp(-0.5 * grid)\n    return grid\n\n", "CodeBleu_score": 33.99797726868362}]}
{"instance_id": "7bbd08c987bc", "generation_result": [{"code": "def stimulus(position, size, intensity)\n\tx, y = position\n    x = x - np.mean(x)\n    y = y - np.mean(y)\n    x, y = np.meshgrid(x, y)\n    R = np.sqrt(x**2 + y**2)\n    return np.exp(-R**2 / size**2) * intensity\n\n", "CodeBleu_score": 17.990741912053508}, {"code": "def stimulus(position, size, intensity)\n\t# convert polar coordinates to Cartesian coordinates\n    x = position[0] * np.cos(position[1])\n    y = position[0] * np.sin(position[1])\n    # normalize and center coordinates\n    x = (x + 1) / 2\n    y = (y + 1) / 2\n    # create mesh grid representing the spatial domain\n    X, Y = np.meshgrid(np.linspace(0, 1, 100), np.linspace(0, 1, 100))\n    # normalize and center coordinates\n    X = (X + 1) / 2\n    Y = (Y + 1) / 2\n    # compute squared Euclidean distance (R) from each point in the grid to the stimulus center\n    R = np.sqrt((X - x) ** 2 + (Y - y) ** 2)\n    # return exponential decay function of these distances, scaled by the size parameter\n    return np.exp(-(R / size) ** 2) * intensity\n\n", "CodeBleu_score": 26.355556448592417}, {"code": "def stimulus(position, size, intensity)\n\tx, y = position\n    r = np.sqrt((x - cartesian[:, :, 0]) ** 2 + (y - cartesian[:, :, 1]) ** 2)\n    return np.exp(-r ** 2 / (2 * size ** 2)) * intensity", "CodeBleu_score": 11.4106466877558}]}
{"instance_id": "2b5806238d5d", "generation_result": [{"code": "def plot(ax, text)\n\tax.set_xlim(0, 1)\n    ax.set_ylim(0, 1)\n    ax.set_xticks([0, 1])\n    ax.set_yticks([0, 1])\n    ax.set_xlabel(\"x\")\n    ax.set_ylabel(\"y\")\n    ax.text(0.5, 0.5, text, ha=\"center\", va=\"center\", fontsize=24, fontweight=\"bold\")\n    ax.set_title(\"Title\", fontfamily=\"serif\", fontweight=\"bold\")\n\n", "CodeBleu_score": 43.552138213671576}, {"code": "def plot(ax, text)\n\tax.set_xlim(0, 1)\n    ax.set_ylim(0, 1)\n    ax.set_xlabel(\"x\")\n    ax.set_ylabel(\"y\")\n    ax.set_xticks([0, 0.5, 1])\n    ax.set_yticks([0, 0.5, 1])\n    ax.text(0.5, 0.5, text, fontsize=16, fontweight=\"bold\", ha=\"center\", va=\"center\")\n    ax.set_title(\"Title\", fontsize=16, fontweight=\"bold\", fontfamily=\"serif\")", "CodeBleu_score": 47.22896703702389}, {"code": "def plot(ax, text)\n\tax.set_xlim(0, 1)\n    ax.set_ylim(0, 1)\n    ax.set_xticks([0, 1])\n    ax.set_yticks([0, 1])\n    ax.set_xlabel(\"x\")\n    ax.set_ylabel(\"y\")\n    ax.text(0.5, 0.5, text, horizontalalignment=\"center\", fontsize=40, fontweight=\"bold\")\n    ax.set_title(\"Title\", fontsize=24, fontweight=\"bold\", fontfamily=\"serif\")", "CodeBleu_score": 46.04254935029745}]}
{"instance_id": "0b42aac61e21", "generation_result": [{"code": "def predict_cumulative_hazard_function(self, X, **kwargs)\n\tXt = X\n    for _, name, transform in self._iter(with_final=False):\n        Xt = transform.transform(Xt)\n    return self.steps[-1][-1].predict_cumulative_hazard_function(Xt, **kwargs)\n\n", "CodeBleu_score": 73.6476496113998}, {"code": "def predict_cumulative_hazard_function(self, X, **kwargs)\n\tXt = X\n    for t in self._iter:\n        if t is self.steps[-1][1]:\n            return t.predict_cumulative_hazard_function(Xt, **kwargs)\n        Xt = t.transform(Xt)\n\n", "CodeBleu_score": 38.29421920021171}, {"code": "def predict_cumulative_hazard_function(self, X, **kwargs)\n\tXt = X\n    for _, name, transform in self._iter(with_final=False):\n        Xt = transform.transform(Xt)\n    return self.steps[-1][-1].predict_cumulative_hazard_function(Xt, **kwargs)\n\n\n\n\n", "CodeBleu_score": 73.6476496113998}]}
{"instance_id": "c8439e4918ad", "generation_result": [{"code": "def predict_survival_function(self, X, **kwargs)\n\tXt = X\n    for name, transform in self.steps[:-1]:\n        if transform is None:\n            continue\n        Xt = transform.transform(Xt)\n    return self.steps[-1][-1].predict_survival_function(Xt, **kwargs)", "CodeBleu_score": 55.15153204189319}, {"code": "def predict_survival_function(self, X, **kwargs)\n\tif self._iter is None:\n        self._iter = iter(self.steps)\n    Xt = X\n    for _, name, transform in self._iter:\n        if transform is None:\n            continue\n        Xt = transform.transform(Xt, **kwargs)\n    _, _, final_estimator = next(self._iter)\n    return final_estimator.predict_survival_function(Xt, **kwargs)", "CodeBleu_score": 40.75168442202266}, {"code": "def predict_survival_function(self, X, **kwargs)\n\tXt = self.transform(X)\n    return self._final_estimator.predict_survival_function(Xt, **kwargs)\n\n", "CodeBleu_score": 16.53717312506595}]}
{"instance_id": "216662ca7db4", "generation_result": [{"code": "def _check_success(self, results)\n\tif results.get('success'):\n        return\n    elif results.get('status') == 'maxiter':\n        warnings.warn('Maximum iterations reached. Try increasing maxiter.', ConvergenceWarning)\n    elif results.get('status') == 'primal_infeasible':\n        raise RuntimeError('Primal infeasible.')\n    elif results.get('status') == 'dual_infeasible':\n        raise RuntimeError('Dual infeasible.')\n    else:\n        raise RuntimeError('Solver terminated with an unexpected status: {}.'.format(results.get('status')))\n\n", "CodeBleu_score": 25.587351972705967}, {"code": "def _check_success(self, results)\n\tif results.get(\"solver\") is None:\n        raise RuntimeError(\"Solver did not provide an exit flag.\")\n    elif results.get(\"solver\").get(\"termination_condition\") == \"optimal\":\n        return\n    elif results.get(\"solver\").get(\"termination_condition\") == \"max_iterations\":\n        warnings.warn(\n            \"Maximum iterations reached. Check model formulation for numerical difficulties.\",\n            category=ConvergenceWarning,\n        )\n        return\n    elif results.get(\"solver\").get(\"termination_condition\") == \"primal_infeasible\":\n        raise RuntimeError(\"Problem is primal infeasible.\")\n    elif results.get(\"solver\").get(\"termination_condition\") == \"dual_infeasible\":\n        raise RuntimeError(\"Problem is dual infeasible.\")\n    else:\n        raise RuntimeError(\n            \"Solver terminated with unknown status: {}\".format(\n                results.get(\"solver\").get(\"termination_condition\")\n            )\n        )\n\n", "CodeBleu_score": 28.5814519210712}, {"code": "def _check_success(self, results)\n\tif results.get('solver_status') == 'ok':\n        pass\n    elif results.get('solver_status') == 'warning':\n        if results.get('message') == 'Maximum iterations reached':\n            warnings.warn('Maximum iterations reached.')\n        else:\n            raise RuntimeError(results.get('message'))\n    elif results.get('solver_status') == 'error':\n        if results.get('message') == 'Primal infeasible':\n            raise RuntimeError('Primal infeasible.')\n        elif results.get('message') == 'Dual infeasible':\n            raise RuntimeError('Dual infeasible.')\n        else:\n            raise RuntimeError(results.get('message'))\n    else:\n        raise RuntimeError('Unknown exit flag.')", "CodeBleu_score": 29.148419700259176}]}
{"instance_id": "1fe2e379fca0", "generation_result": [{"code": "def _decompose(self, P)\n\tevals, evecs = linalg.eigh(P)\n    evals = np.abs(evals)\n    evals_max = np.max(evals)\n    evals_min = np.min(evals)\n    evals_cond = evals_max / evals_min\n    evals_cutoff = -evals_max * np.finfo(float).eps\n    evals_below_cutoff = evals < evals_cutoff\n    evals_not_below_cutoff = evals > evals_cutoff\n    evals_norm = evals / evals_max\n    evals_norm[evals_below_cutoff] = 0\n    evecs = evecs[:, evals_not_below_cutoff]\n    evals = evals[evals_not_below_cutoff]\n    return evecs.T, evals_norm\n\n", "CodeBleu_score": 26.821797358010883}, {"code": "def _decompose(self, P)\n\t# get eigenvalues and eigenvectors\n    eigenvals, eigenvecs = np.linalg.eigh(P)\n    # get condition number\n    cond = np.max(eigenvals) / np.min(eigenvals)\n    # get cutoff\n    cutoff = np.abs(np.finfo(float).eps * cond)\n    # identify largest eigenvalue\n    max_eigenval = np.max(eigenvals)\n    # filter out eigenvalues below cutoff\n    not_below_cutoff = eigenvals > -cutoff\n    eigenvals = eigenvals[not_below_cutoff]\n    eigenvecs = eigenvecs[:, not_below_cutoff]\n    # normalize eigenvals\n    eigenvals = eigenvals / max_eigenval\n    # return transposed decomposed matrix and largest eigenvalue\n    return eigenvecs.T, max_eigenval\n\n", "CodeBleu_score": 23.78011508310727}, {"code": "def _decompose(self, P)\n\t# Compute eigenvalues and eigenvectors\n    w, v = linalg.eigh(P)\n    # Compute condition number\n    cond = np.abs(w[0] / w[-1])\n    # Check if all eigenvalues are above the negative cutoff\n    if not np.all(w > -self.cutoff):\n        raise ValueError(\"All eigenvalues must be above the negative cutoff.\")\n    # Filter out eigenvalues below the positive cutoff\n    w = w[w > self.cutoff]\n    v = v[:, w > self.cutoff]\n    # Normalize eigenvalues\n    w = w / np.max(w)\n    # Return decomposed matrix and largest eigenvalue\n    return v.T, np.max(w)\n\n", "CodeBleu_score": 20.38867783671407}]}
{"instance_id": "20c3f5d05d79", "generation_result": [{"code": "def _update_coef(self, coef, D)\n\t# select the coefficients that are greater than the threshold\n    coef_ = coef[np.flatnonzero(D > self.threshold)]\n    # scale the selected coefficients by the corresponding elements in D\n    coef_ *= D[np.flatnonzero(D > self.threshold)]\n    # update the model's coefficients\n    self.coef_ = coef_\n\n\n\n\n", "CodeBleu_score": 22.620326405615177}, {"code": "def _update_coef(self, coef, D)\n\t# Select elements of coef that are greater than a small threshold\n    coef_selected = coef[np.flatnonzero(coef)]\n    # Scale the selected coefficients by corresponding elements in D\n    coef_scaled = coef_selected * D[np.flatnonzero(coef)]\n    # Store the scaled coefficients in the model's coefficients attribute\n    self.coef_ = coef_scaled", "CodeBleu_score": 21.45858345628991}, {"code": "def _update_coef(self, coef, D)\n\tself.coef_ = coef[D > self.threshold] * D[D > self.threshold]", "CodeBleu_score": 19.617225017697965}]}
{"instance_id": "37c347d00d35", "generation_result": [{"code": "def gradient(self, y_true, raw_prediction, sample_weight=None, **kwargs)\n\t# Calculate the negative gradient\n    gradient = -coxph_negative_gradient(y_true, raw_prediction)\n\n    # Apply sample weights to the gradient, if provided\n    if sample_weight is not None:\n        sample_weight = sample_weight.reshape(gradient.shape[0], 1)\n        gradient *= sample_weight\n\n    # Return the weighted or unweighted gradient\n    return gradient\n\n", "CodeBleu_score": 46.47235803189951}, {"code": "def gradient(self, y_true, raw_prediction, sample_weight=None, **kwargs)\n\t# Calculate the negative gradient\n    gradient = self.coxph_negative_gradient(y_true, raw_prediction)\n    # Apply sample weights\n    if sample_weight is not None:\n        sample_weight = self.astype(sample_weight, dtype=\"float64\", copy=False)\n        gradient *= sample_weight\n        gradient /= sample_weight.mean()\n    return gradient\n\n", "CodeBleu_score": 34.243075869473365}, {"code": "def gradient(self, y_true, raw_prediction, sample_weight=None, **kwargs)\n\tif sample_weight is None:\n        sample_weight = np.ones(y_true.shape[0])\n    sample_weight = sample_weight.reshape(-1, 1)\n    event_matrix = y_true.reshape(-1, 1)\n    gradient_matrix = self.negative_gradient(raw_prediction)\n    gradient_matrix = gradient_matrix * event_matrix\n    return np.dot(sample_weight, gradient_matrix)\n\n", "CodeBleu_score": 36.62521067049623}]}
{"instance_id": "6be432d27e52", "generation_result": [{"code": "def _compute_missing_values_in_feature_mask(self, X, estimator_name=None)\n\tif not self._support_missing_values(X):\n        assert_all_finite(X, allow_nan=True)\n        return None\n\n    with np.errstate(over=\"ignore\"):\n        feature_mask = np.isnan(np.sum(X, axis=0))\n\n    if np.any(feature_mask):\n        warnings.warn(\n            \"Missing values in features, this might lead to unexpected \"\n            \"results and can be avoided by setting \"\n            \"missing_values='error' in the estimator. \"\n            \"This warning is only raised when \"\n            \"missing_values is not explicitly set. \"\n            \"Estimator: {0}\".format(estimator_name),\n            UserWarning,\n        )\n\n    return feature_mask", "CodeBleu_score": 40.11745159156752}, {"code": "def _compute_missing_values_in_feature_mask(self, X, estimator_name=None)\n\tif self._support_missing_values(X):\n        if not np.isfinite(X).all():\n            msg = (\"Estimator %s cannot accept %s as it contains NaN, \"\n                   \"infinity or a value too large for %s.\"\n                   % (estimator_name, X, X.dtype))\n            raise ValueError(msg)\n        feature_mask = np.isnan(X).sum(axis=0)\n        feature_mask = np.array(feature_mask, dtype=bool)\n        if feature_mask.any():\n            return feature_mask\n    return None\n\n\n\n\n", "CodeBleu_score": 37.27534403947105}, {"code": "def _compute_missing_values_in_feature_mask(self, X, estimator_name=None)\n\tif not self._support_missing_values(X):\n        # X does not support missing values\n        _assert_all_finite(X, allow_nan=True, estimator_name=estimator_name)\n        return None\n\n    with np.errstate(over=\"ignore\"):\n        feature_mask = np.sum(X, axis=0)\n\n    if np.isfinite(feature_mask).all():\n        # No missing values\n        return None\n\n    if not np.isfinite(feature_mask).any():\n        # Only missing values\n        return np.ones(X.shape[1], dtype=np.bool)\n\n    # Missing values\n    return ~np.isfinite(feature_mask)\n\n", "CodeBleu_score": 37.08482830962317}]}
{"instance_id": "dc5bda895a07", "generation_result": [{"code": "def predict_survival_function(self, X, check_input=True, return_array=False)\n\tself._validate_X_predict(X, check_input)\n    if self.low_memory:\n        check_is_fitted(self, \"tree_\")\n        pred = self.tree_.predict(X)\n        if return_array:\n            return pred[:, 1]\n        return _array_to_step_function(self.times_, pred[:, 1])\n    return self.tree_.predict(X, return_array=return_array)[:, 1]\n\n", "CodeBleu_score": 45.87535190614979}, {"code": "def predict_survival_function(self, X, check_input=True, return_array=False)\n\tcheck_is_fitted(self, \"tree_\")\n    X = self._validate_X_predict(X, check_input, accept_sparse=\"csr\")\n    pred = self.tree_.predict(X)\n    if return_array:\n        return pred[..., 1]\n    return _array_to_step_function(self.times_, pred[..., 1])\n\n", "CodeBleu_score": 62.668035584497225}, {"code": "def predict_survival_function(self, X, check_input=True, return_array=False)\n\tself._validate_X_predict(X, check_input)\n\n    if self.low_memory:\n        check_is_fitted(self, \"tree_\")\n        X = self._validate_data(X, accept_sparse=\"csr\", reset=False)\n        pred = self.tree_.predict(X)\n        if return_array:\n            return pred[:, 1]\n        return _array_to_step_function(self.times_, pred[:, 1])\n\n    chf = self.predict_cumulative_hazard_function(X, check_input, return_array)\n    sf = np.exp(-chf)\n    if return_array:\n        return sf\n    return _array_to_step_function(self.times_, sf)\n\n", "CodeBleu_score": 56.29484029409646}]}
{"instance_id": "84ae915b5933", "generation_result": [{"code": "def _pre_fit(self, X, y)\n\tX, y = self._validate_data(X, y, multi_output=True)\n    self.y = y\n    self.event_observed = y[:, 0]\n    self.time = y[:, 1]\n    self.n_samples, self.n_features = X.shape\n    self.classes_ = np.unique(self.event_observed)\n    self.n_classes_ = len(self.classes_)\n    if self.n_samples < 2:\n        raise ValueError(\n            \"n_samples=%d. Cannot calculate variance.\" % self.n_samples)\n    if self.n_features < 1:\n        raise ValueError(\n            \"n_features=%d. Cannot calculate variance.\" % self.n_features)\n    if self.n_features == 1:\n        warnings.warn(\"n_features=1. The model will be a simple\"\n                      \" constant function.\")\n    if self.n_classes_ == 1:\n        raise ValueError(\"The model will always predict class %s.\" %\n                         str(self.classes_[0]))\n    if self.n_classes_ > 2:\n        raise ValueError(\"Multiclass option not supported for survival analysis.\")\n    if self.classes_[0] != 0:\n        raise ValueError(\"First class must be 0.\")\n    if self.classes_[1] != 1:\n        raise ValueError(\"Second class must be 1.\")\n    if self.n_features == 1:\n        self.feature_means_ = np.zeros(1)\n        self.feature_scales_ = np.ones(1)\n    else:\n        self.feature_means_ = np.average(X, axis=0, weights=self.event_observed)\n        self.feature_scales_ = np.ones(self.n_features)\n        if self.normalize:\n            self.feature_scales_ = np.sqrt(np.average((X - self.feature_means_) ** 2, axis=0, weights=self.event_observed))\n    if self.normalize:\n        X", "CodeBleu_score": 23.596000444899122}, {"code": "def _pre_fit(self, X, y)\n\tX, y = self._validate_data(X, y, multi_output=True)\n    if self.normalize:\n        X = f_normalize(X, axis=0, copy=True)\n    return X, y\n\n", "CodeBleu_score": 4.799142933678841}, {"code": "def _pre_fit(self, X, y)\n\tX, y = self._validate_data(X, y, multi_output=True)\n\n    if np.any(y[:, 0] < 0):\n        raise ValueError(\"Survival times must be positive.\")\n\n    if np.any(y[:, 1] != 0) and np.any(y[:, 1] != 1):\n        raise ValueError(\"Event indicators must be 0 or 1.\")\n\n    if self.fit_intercept:\n        X = np.c_[X, np.ones(X.shape[0])]\n\n    if self.normalize:\n        X = f_normalize(X, axis=0)\n\n    # Sort the data in descending order of survival time.\n    order = np.argsort(-y[:, 0])\n    X = X[order]\n    y = y[order]\n\n    X = np.asfortranarray(X)\n    y = np.asfortranarray(y)\n\n    return X, y\n\n\n\n\n", "CodeBleu_score": 30.631208811298254}]}
{"instance_id": "c70bb013f85f", "generation_result": [{"code": "def _check_alphas(self)\n\tif self.alphas is None:\n        self.alphas = np.empty(self.n_alphas)\n        self.new_path = True\n    else:\n        self.alphas = np.asarray(self.alphas)\n        if self.alphas.ndim > 1:\n            raise ValueError(\"alphas must be a 1D array\")\n        if len(self.alphas) != self.n_alphas:\n            raise ValueError(\"alphas must be of length n_alphas\")\n        if np.any(~np.isfinite(self.alphas)):\n            raise ValueError(\"alphas must be finite\")\n        if np.any(self.alphas < 0):\n            raise ValueError(\"alphas must be non-negative\")\n        self.new_path = False\n    return self.alphas, self.new_path\n\n", "CodeBleu_score": 31.33777293944699}, {"code": "def _check_alphas(self)\n\tif self.alphas is None:\n        new_path = True\n        self.alphas = np.empty(self.n_alphas)\n    else:\n        new_path = False\n        self.alphas = check_array(self.alphas, ensure_2d=False,\n                                  ensure_min_samples=1,\n                                  ensure_min_features=1,\n                                  dtype=np.float64)\n        if self.alphas.ndim > 1:\n            raise ValueError(\"alphas must be 1D\")\n        if self.alphas.shape[0] != self.n_alphas:\n            raise ValueError(\"alphas must have length n_alphas\")\n        if not np.all(np.isfinite(self.alphas)):\n            raise ValueError(\"alphas must be finite\")\n        if not np.all(self.alphas >= 0):\n            raise ValueError(\"alphas must be non-negative\")\n    return self.alphas, new_path", "CodeBleu_score": 35.64226615397321}, {"code": "def _check_alphas(self)\n\tif self.alphas is None:\n        self.alphas = np.empty(self.n_alphas)\n        self.alphas_path = True\n    else:\n        self.alphas = check_array(self.alphas, ensure_2d=False,\n                                  dtype=None, order=None)\n        self.alphas_path = False\n        if self.alphas.ndim > 1:\n            raise ValueError(\"alphas must be 1D\")\n        if len(self.alphas) == 0:\n            raise ValueError(\"alphas must contain at least one value\")\n        if np.any(~np.isfinite(self.alphas)):\n            raise ValueError(\"alphas must contain only finite values\")\n        if np.any(self.alphas < 0):\n            raise ValueError(\"alphas must contain only non-negative values\")\n    return self.alphas, self.alphas_path", "CodeBleu_score": 36.18766784015246}]}
{"instance_id": "7b54a0554ba4", "generation_result": [{"code": "def nelson_aalen_estimator(event, time)\n\tevent, time = check_y_survival(event, time)\n    times, n_events, n_at_risk, _ = _compute_counts(event, time)\n\n    # calculate the Nelson-Aalen cumulative hazard\n    hazard = n_events / n_at_risk\n    cumhaz = np.cumsum(hazard)\n\n    return times, cumhaz\n\n", "CodeBleu_score": 38.63921930845938}, {"code": "def nelson_aalen_estimator(event, time)\n\tcheck_y_survival(event, time)\n    check_consistent_length(event, time)\n    times, n_events, n_at_risk, n_censored = _compute_counts(event, time)\n    hazard = n_events / n_at_risk\n    cum_hazard = np.cumsum(hazard)\n    return times, cum_hazard\n\n", "CodeBleu_score": 35.1123498080805}, {"code": "def nelson_aalen_estimator(event, time)\n\tevent, time = check_y_survival(event, time)\n    check_consistent_length(event, time)\n\n    times, n_events, n_at_risk, _ = _compute_counts(event, time)\n\n    hazard = n_events / n_at_risk\n    cumulative_hazard = np.cumsum(hazard)\n\n    return times, cumulative_hazard\n\n", "CodeBleu_score": 43.750084809280736}]}
{"instance_id": "35af806300cc", "generation_result": [{"code": "def ipc_weights(event, time)\n\t# Check if all events have occurred\n    if np.all(event):\n        return np.ones(len(event))\n\n    # Calculate the Kaplan-Meier estimator for the event and time data in reverse\n    time_rev = np.flip(time)\n    event_rev = np.flip(event)\n    time_km, prob_survival, _ = kaplan_meier_estimator(event_rev, time_rev, reverse=True)\n\n    # Find the index positions of event times within the unique time values\n    # and retrieve the corresponding survival probabilities\n    # Ensure all retrieved probabilities are positive\n    time_km_rev = np.flip(time_km)\n    prob_survival_rev = np.flip(prob_survival)\n    survival_prob = np.interp(time, time_km_rev, prob_survival_rev)\n    survival_prob = np.where(survival_prob > 0, survival_prob, 1)\n\n    # Create a weights array, assigning inverse probabilities to events\n    weights = 1 / survival_prob\n\n    return weights\n\n", "CodeBleu_score": 32.14710303790851}, {"code": "def ipc_weights(event, time)\n\tif np.all(event):\n        return np.ones(event.shape[0])\n    else:\n        time_rev = time[::-1]\n        event_rev = event[::-1]\n        uniq_times, prob_survival, _ = kaplan_meier_estimator(\n            event_rev, time_rev, reverse=True\n        )\n        uniq_times = uniq_times[::-1]\n        prob_survival = prob_survival[::-1]\n        idx = np.searchsorted(uniq_times, time)\n        weights = 1 / prob_survival[idx]\n        weights[weights <= 0] = 1\n        return weights\n\n", "CodeBleu_score": 37.10548767336009}, {"code": "def ipc_weights(event, time)\n\tif np.all(event):\n        return np.ones(event.shape[0])\n    else:\n        time_rev = time[::-1]\n        event_rev = event[::-1]\n        uniq_times, prob_survival = kaplan_meier_estimator(\n            event_rev, time_rev, reverse=True\n        )\n        index_pos = np.searchsorted(uniq_times, time)\n        weights = 1 / prob_survival[index_pos]\n        weights[weights < 0] = 0\n        return weights\n\n", "CodeBleu_score": 40.42379050062056}]}
{"instance_id": "72b19985ea11", "generation_result": [{"code": "def get_dfs(\n        self,\n        dfs: Union[\n            pd.DataFrame, BaseConnector, List[Union[pd.DataFrame, BaseConnector]]\n        ],\n    )\n\tif not isinstance(dfs, list):\n        dfs = [dfs]\n\n    for df in dfs:\n        if isinstance(df, pd.DataFrame):\n            dfs.append(PandasConnector(df))\n        elif isinstance(df, BaseConnector):\n            dfs.append(df)\n        elif isinstance(df, list):\n            dfs.append(get_dfs(df))\n        else:\n            raise ValueError(\n                f\"{df} is not a valid input. Please provide a pandas, modin, polars, or custom dataframe.\"\n            )\n\n    return dfs\n\n", "CodeBleu_score": 27.258512873648556}, {"code": "def get_dfs(\n        self,\n        dfs: Union[\n            pd.DataFrame, BaseConnector, List[Union[pd.DataFrame, BaseConnector]]\n        ],\n    )\n\tif not isinstance(dfs, list):\n        dfs = [dfs]\n\n    connectors = []\n    for df in dfs:\n        if isinstance(df, pd.DataFrame):\n            df = PandasConnector(df)\n        elif isinstance(df, BaseConnector):\n            pass\n        elif isinstance(df, PolarsDataFrame):\n            df = PolarsConnector(df)\n        else:\n            raise ValueError(\n                f\"Input type {type(df)} is not supported. Please provide a pandas, polars, or modin dataframe, or a custom connector.\"\n            )\n        connectors.append(df)\n    return connectors", "CodeBleu_score": 33.43675171449306}, {"code": "def get_dfs(\n        self,\n        dfs: Union[\n            pd.DataFrame, BaseConnector, List[Union[pd.DataFrame, BaseConnector]]\n        ],\n    )\n\tif not isinstance(dfs, list):\n        dfs = [dfs]\n    connectors = []\n    for df in dfs:\n        if isinstance(df, pd.DataFrame):\n            connectors.append(PandasConnector(df))\n        elif isinstance(df, PolarsDataFrame):\n            connectors.append(PolarsConnector(df))\n        elif isinstance(df, BaseConnector):\n            connectors.append(df)\n        else:\n            raise ValueError(\n                \"The provided data type is not supported. Please provide a pandas, polars, or modin dataframe, or a custom connector.\"\n            )\n    return connectors", "CodeBleu_score": 37.43038783668005}]}
{"instance_id": "cf22fb696e90", "generation_result": [{"code": "def execute(self) -> pd.DataFrame\n\tif isinstance(self._data, pd.DataFrame):\n            return self._data\n\n        cached = self._cached()\n        if cached is not None:\n            return pd.read_parquet(cached)\n\n        data = self._fetch_data()\n        self._data = data\n\n        return data\n\n", "CodeBleu_score": 47.62385555529437}, {"code": "def execute(self) -> pd.DataFrame\n\tif self._cached(include_additional_filters=True):\n        return pd.read_parquet(self._cached(include_additional_filters=True))\n    elif self._cached(include_additional_filters=False):\n        return pd.read_parquet(self._cached(include_additional_filters=False))\n    else:\n        self._data = self._fetch_data()\n        return self._data\n\n", "CodeBleu_score": 50.84841795895518}, {"code": "def execute(self) -> pd.DataFrame\n\tif self._cached():\n        return pd.read_parquet(self._cached())\n\n    if isinstance(self._data, pd.DataFrame):\n        return self._data\n\n    self._data = self._fetch_data()\n    return self._data", "CodeBleu_score": 53.41130007150559}]}
{"instance_id": "21df5ecea95f", "generation_result": [{"code": "def column_hash(self)\n\tif not isinstance(self._instance, pd.DataFrame):\n        self.execute()\n    columns_str = \"|\".join(self._instance.columns)\n    columns_str = f\"{columns_str}{self._formula}\"\n    return hashlib.sha256(columns_str.encode()).hexdigest()\n\n", "CodeBleu_score": 56.77008137833426}, {"code": "def column_hash(self)\n\tif not isinstance(self._instance, pd.DataFrame):\n        self._instance = self._fetch_data()\n\n    columns_str = \"|\".join(self._instance.columns)\n    columns_str = columns_str.encode(\"utf-8\")\n    formula_str = self._build_formula()\n    formula_str = formula_str.encode(\"utf-8\")\n\n    return hashlib.sha256(columns_str + formula_str).hexdigest()", "CodeBleu_score": 70.45513438544317}, {"code": "def column_hash(self)\n\tif not isinstance(self._instance, pd.DataFrame):\n        self.execute()\n\n    columns_str = \"|\".join(self._instance.columns)\n    columns_str = f\"{columns_str}|{self._formula}\"\n    return hashlib.sha256(columns_str.encode()).hexdigest()\n\n", "CodeBleu_score": 56.78006129107479}]}
{"instance_id": "27ee5f415000", "generation_result": [{"code": "def _load_df(self, df: Union[pd.DataFrame, pd.Series, str, list, dict])\n\tif isinstance(df, pd.DataFrame):\n        self.df = df\n    elif isinstance(df, pd.Series):\n        self.df = df.to_frame()\n    elif isinstance(df, str):\n        self.df = FileImporter.import_from_file(df)\n    elif isinstance(df, list) or isinstance(df, dict):\n        self.df = pd.DataFrame(df)\n    else:\n        raise ValueError(\n            \"Input type is not supported. Please provide a pandas DataFrame, Series, list, dictionary, or file path.\"\n        )\n\n", "CodeBleu_score": 51.104360653048595}, {"code": "def _load_df(self, df: Union[pd.DataFrame, pd.Series, str, list, dict])\n\tif isinstance(df, pd.DataFrame):\n        self.df = df\n    elif isinstance(df, pd.Series):\n        self.df = df.to_frame()\n    elif isinstance(df, str):\n        self.df = FileImporter.import_from_file(df)\n    elif isinstance(df, list) or isinstance(df, dict):\n        self.df = pd.DataFrame(df)\n    else:\n        raise ValueError(\n            \"The input is not of a supported type. Please provide a pandas DataFrame, a pandas Series, a list, a dictionary, or a string representing a file path.\"\n        )", "CodeBleu_score": 51.98560766047233}, {"code": "def _load_df(self, df: Union[pd.DataFrame, pd.Series, str, list, dict])\n\tif isinstance(df, pd.DataFrame):\n        self._df = df\n    elif isinstance(df, pd.Series):\n        self._df = df.to_frame()\n    elif isinstance(df, list):\n        self._df = pd.DataFrame(df)\n    elif isinstance(df, dict):\n        self._df = pd.DataFrame(df)\n    elif isinstance(df, str):\n        self._df = FileImporter.import_from_file(df)\n    else:\n        raise ValueError(\"The input type is not supported.\")", "CodeBleu_score": 50.837920419336015}]}
{"instance_id": "38f48a358606", "generation_result": [{"code": "def sample(self, n: int = 3) -> pd.DataFrame\n\tsampled_df = pd.DataFrame()\n    if len(self.df) <= n:\n        sampled_df = self.df.sample(frac=1)\n    else:\n        for col in self.df.columns:\n            col_sample = self._sample_column(col, n)\n            sampled_df[col] = col_sample\n\n    # anonymize the sampled dataframe head\n    sampled_df = Anonymizer.anonymize_dataframe_head(sampled_df)\n\n    return sampled_df\n\n", "CodeBleu_score": 77.0886131503128}, {"code": "def sample(self, n: int = 3) -> pd.DataFrame\n\tsampled_df = pd.DataFrame()\n    if len(self.df) <= n:\n        sampled_df = self.df.sample(frac=1)\n    else:\n        for col in self.df.columns:\n            col_sample = self._sample_column(col, n)\n            sampled_df[col] = col_sample\n\n    # anonymize the sampled dataframe head\n    sampled_df = Anonymizer.anonymize_dataframe_head(sampled_df)\n\n    return sampled_df\n\n", "CodeBleu_score": 77.0886131503128}, {"code": "def sample(self, n: int = 3) -> pd.DataFrame\n\tsampled_df = pd.DataFrame()\n    if len(self.df) <= n:\n        sampled_df = self.df.sample(frac=1)\n    else:\n        for col in self.df.columns:\n            col_sample = self._sample_column(col, n)\n            sampled_df[col] = col_sample\n\n    # anonymize the sampled dataframe head\n    sampled_df = Anonymizer.anonymize_dataframe_head(sampled_df)\n\n    return sampled_df\n\n", "CodeBleu_score": 77.0886131503128}]}
{"instance_id": "8bddad2e31ca", "generation_result": [{"code": "def convert_df_to_csv(self, df: pd.DataFrame, extras: dict) -> str\n\t# TODO: add optional name and description attributes\n    # TODO: add optional details about the DataFrame's dimensions and content in CSV format\n    # TODO: add optional details about the DataFrame's data types\n    # TODO: add optional details about the DataFrame's missing values\n    # TODO: add optional details about the DataFrame's memory usage\n    # TODO: add optional details about the DataFrame's index\n    # TODO: add optional details about the DataFrame's columns\n    # TODO: add optional details about the DataFrame's dtypes\n    # TODO: add optional details about the DataFrame's shape\n    # TODO: add optional details about the DataFrame's size\n    # TODO: add optional details about the DataFrame's values\n    # TODO: add optional details about the DataFrame's columns' names\n    # TODO: add optional details about the DataFrame's columns' data types\n    # TODO: add optional details about the DataFrame's columns' values\n    # TODO: add optional details about the DataFrame's columns' missing values\n    # TODO: add optional details about the DataFrame's columns' memory usage\n    # TODO: add optional details about the DataFrame's columns' unique values\n    # TODO: add optional details about the DataFrame's columns' unique values' counts\n    # TODO: add optional details about the DataFrame's columns' unique values' frequencies\n    # TODO: add optional details about the DataFrame's columns' unique values' proportions\n    # TODO: add optional details about the DataFrame's columns' unique values' cumulative frequencies\n    # TODO: add optional details about the DataFrame's columns' unique values' cumulative proportions\n    # TODO: add optional details about the DataFrame's columns' unique values' quartiles\n    # TODO: add optional details about the DataFrame's columns' unique values' quantiles\n    # TODO: add optional details about the DataFrame's columns' unique values' means\n    # TODO: add optional details about the DataFrame's columns' unique values' medians\n    # TODO: add optional details about the DataFrame's columns' unique values' modes\n    # TODO: add optional details about the DataFrame's columns' unique values' standard deviations\n    # TODO:", "CodeBleu_score": 12.885375494071145}, {"code": "def convert_df_to_csv(self, df: pd.DataFrame, extras: dict) -> str\n\t# check if the DataFrame has a name\n    if df.name:\n        # if so, add it to the opening tag\n        name = 'name=\"{}\"'.format(df.name)\n    else:\n        # if not, add a placeholder\n        name = ''\n    # check if the DataFrame has a description\n    if df.description:\n        # if so, add it to the opening tag\n        description = 'description=\"{}\"'.format(df.description)\n    else:\n        # if not, add a placeholder\n        description = ''\n    # check if the DataFrame has any extra attributes\n    if extras:\n        # if so, add them to the opening tag\n        extra = ' '.join('{}=\"{}\"'.format(key, val) for key, val in extras.items())\n    else:\n        # if not, add a placeholder\n        extra = ''\n    # construct the opening tag\n    opening_tag = '<DataFrame {} {} {}>", "CodeBleu_score": 36.11018087667673}, {"code": "def convert_df_to_csv(self, df: pd.DataFrame, extras: dict) -> str\n\tcsv_string = \"<\" + df.name\n    if df.description:\n        csv_string += \" description=\\\"\" + df.description + \"\\\"\"\n    csv_string += \">\"\n    csv_string += \"", "CodeBleu_score": 27.238040862213197}]}
{"instance_id": "5380bb60fd39", "generation_result": [{"code": "def convert_df_sql_connector_to_str\n\tif self.description:\n        description = f\"<p>{self.description}</p>\"\n    else:\n        description = \"\"\n    table_string = f\"\"\"\n    <table name=\"{self.name}\">\n    {description}\n    {self.df.to_csv(index=False)}\n    </table>\n    \"\"\"\n    return table_string", "CodeBleu_score": 22.560217333945918}, {"code": "def convert_df_sql_connector_to_str\n\tdf_str = '<table id=\"{}\"'.format(df.name)\n    if df.description:\n        df_str += ' description=\"{}\"'.format(df.description)\n    df_str += '>", "CodeBleu_score": 20.063694252430373}, {"code": "def convert_df_sql_connector_to_str\n\ttable_name = df.metadata.get('table_name', 'table')\n    table_description = df.metadata.get('description', '')\n    table_description = f'<p>{table_description}</p>' if table_description else ''\n    table_head = df.head().to_csv(index=False)\n    table_head = table_head.replace('", "CodeBleu_score": 38.138891074478465}]}
{"instance_id": "0afd995716c5", "generation_result": [{"code": "def convert_df_to_yml(self, df: pd.DataFrame, extras: dict) -> str:\n\t# Convert the DataFrame to JSON\n    json = self.convert_df_to_json(df, extras)\n\n    # Convert the JSON to YAML\n    yaml = yaml.dump(json)\n\n    # If the DataFrame was generated from a direct SQL query, wrap the YAML in HTML table tags\n    if \"is_direct_sql\" in extras and extras[\"is_direct_sql\"]:\n        return f\"<table><tr><td><pre>{yaml}</pre></td></tr></table>\"\n\n    # Return the YAML string\n    return yaml\n\n", "CodeBleu_score": 65.08263088100001}, {"code": "def convert_df_to_yml(self, df: pd.DataFrame, extras: dict) -> str:\n\t# Convert the DataFrame to JSON format\n    df_json = self.convert_df_to_json(df, extras)\n\n    # Convert the JSON to YAML format\n    df_yml = yaml.dump(df_json, sort_keys=False)\n\n    # If the extras dictionary contains the key \"is_direct_sql\" with a value of True, wrap the YAML string in HTML table tags\n    if \"is_direct_sql\" in extras and extras[\"is_direct_sql\"]:\n        return f\"<table><tbody><tr><td><pre>{df_yml}</pre></td></tr></tbody></table>\"\n\n    return df_yml\n\n", "CodeBleu_score": 74.3068379136799}, {"code": "def convert_df_to_yml(self, df: pd.DataFrame, extras: dict) -> str:\n\tjson_str = self.convert_df_to_json(df, extras)\n    yaml_str = yaml.dump(json_str, sort_keys=False)\n\n    if \"is_direct_sql\" in extras and extras[\"is_direct_sql\"]:\n        return f\"<table><tr><td><pre>{yaml_str}</pre></td></tr></table>\"\n\n    return yaml_str\n\n", "CodeBleu_score": 76.04624308199325}]}
{"instance_id": "e2b90ebd4e5d", "generation_result": [{"code": "def validate_value(self, expected_type: str) -> bool\n\tif expected_type == \"\":\n        return True\n    elif expected_type == \"number\":\n        return isinstance(self, (int, float))\n    elif expected_type == \"string\":\n        return isinstance(self, str)\n    elif expected_type == \"dataframe\":\n        return isinstance(self, (pd.DataFrame, pd.Series))\n    elif expected_type == \"plot\":\n        if isinstance(self, str):\n            return re.match(r\"^.+\\.(?:png|jpg|jpeg|pdf)$\", self) is not None\n        elif isinstance(self, dict):\n            return True\n        else:\n            return False\n    else:\n        raise ValueError(\n            f\"{expected_type} is not a valid expected type. Please use one of the following: number, string, dataframe, plot\"\n        )", "CodeBleu_score": 72.27100454040163}, {"code": "def validate_value(self, expected_type: str) -> bool\n\tif expected_type == \"\":\n        return True\n    elif expected_type == \"number\":\n        if isinstance(self.value, int) or isinstance(self.value, float):\n            return True\n        else:\n            return False\n    elif expected_type == \"string\":\n        if isinstance(self.value, str):\n            return True\n        else:\n            return False\n    elif expected_type == \"dataframe\":\n        if isinstance(self.value, pd.DataFrame) or isinstance(self.value, pd.Series):\n            return True\n        else:\n            return False\n    elif expected_type == \"plot\":\n        if isinstance(self.value, str):\n            if re.match(r\"^.*\\.png$\", self.value) or re.match(r\"^.*\\.pdf$\", self.value):\n                return True\n            else:\n                return False\n        elif isinstance(self.value, dict):\n            if \"data\" in self.value.keys() and \"layout\" in self.value.keys():\n                return True\n            else:\n                return False\n        else:\n            return False\n    else:\n        raise ValueError(f\"{expected_type} is not a valid expected_type. Please use one of the following: 'number', 'string', 'dataframe', 'plot' or ''\")", "CodeBleu_score": 46.28188936733951}, {"code": "def validate_value(self, expected_type: str) -> bool\n\tif expected_type == \"\":\n        return True\n    elif expected_type == \"number\":\n        if isinstance(self.value, int) or isinstance(self.value, float):\n            return True\n        else:\n            return False\n    elif expected_type == \"string\":\n        if isinstance(self.value, str):\n            return True\n        else:\n            return False\n    elif expected_type == \"dataframe\":\n        if isinstance(self.value, pd.DataFrame) or isinstance(self.value, pd.Series):\n            return True\n        else:\n            return False\n    elif expected_type == \"plot\":\n        if isinstance(self.value, str):\n            if re.match(r\"^[a-zA-Z0-9_-]+\\.(png|svg|jpg)$\", self.value):\n                return True\n            else:\n                return False\n        elif isinstance(self.value, dict):\n            return True\n        else:\n            return False\n    else:\n        raise ValueError(\"Invalid expected_type: \" + expected_type)", "CodeBleu_score": 48.80084662540221}]}
{"instance_id": "d95828fdcbcb", "generation_result": [{"code": " def _distance(self, X, X2=None)\n\tif self.params is None:\n        self.params = {}\n\n    if isinstance(self.metric, str):\n        return pairwise_distance(X, X2, metric=self.metric, **self.params)\n    else:\n        return self.metric(X, X2, **self.params)", "CodeBleu_score": 10.692671568173209}, {"code": " def _distance(self, X, X2=None)\n\tif isinstance(self.distance, str):\n        return pairwise_distance(X, X2, metric=self.distance, **self.distance_params)\n    else:\n        if X2 is None:\n            sig = signature(self.distance)\n            if \"X2\" not in sig.parameters:\n                X2 = X\n        return self.distance(X, X2, **self.distance_params)", "CodeBleu_score": 21.241436853484892}, {"code": " def _distance(self, X, X2=None)\n\tif isinstance(self.distance, str):\n        return pairwise_distance(X, X2, metric=self.distance, **self.params)\n    else:\n        return self.distance(X, X2, **self.params)", "CodeBleu_score": 8.152990789666935}]}
{"instance_id": "497d73b298ee", "generation_result": [{"code": "def _convert_X_to_sklearn(self, X)\n\tif isinstance(X, pd.DataFrame):\n        if len(X.columns) == 1:\n            X_pivot = X.pivot(columns=X.columns[0])\n        else:\n            X_pivot = X\n        X_w_ix = X_pivot.reset_index()\n        X_w_ix.columns = X_w_ix.columns.astype(str)\n        X_w_ix.fillna(0, inplace=True)\n        X_w_ix = X_w_ix.to_numpy()\n        X_w_ix = np.concatenate([np.array([X_w_ix.shape[1]] * X_w_ix.shape[0]).reshape(-1, 1), X_w_ix], axis=1)\n    else:\n        X_w_ix = X.values.flatten()\n    return X_w_ix", "CodeBleu_score": 42.27033630260219}, {"code": "def _convert_X_to_sklearn(self, X)\n\tif isinstance(X, pd.DataFrame):\n        X_w_ix = X.reset_index()\n        X_pivot = X_w_ix.pivot(index=X_w_ix.columns[0], columns='variable', values='value')\n        X_pivot.fillna(0, inplace=True)\n        X_pivot = X_pivot.to_numpy()\n        X_lengths = np.array([len(X_pivot[i]) for i in range(len(X_pivot))])\n        X_lengths = np.concatenate([X_lengths.reshape(-1, 1), X_pivot], axis=1)\n    elif isinstance(X, np.ndarray):\n        X_lengths = np.array([len(X[i]) for i in range(len(X))])\n        X_lengths = np.concatenate([X_lengths.reshape(-1, 1), X], axis=1)\n    else:\n        X_lengths = X\n    return X_lengths", "CodeBleu_score": 40.84714577138936}, {"code": "def _convert_X_to_sklearn(self, X)\n\tif self.equal_length:\n        if isinstance(X, pd.DataFrame):\n            X = X.to_numpy().flatten()\n        elif isinstance(X, pd.Series):\n            X = X.to_numpy()\n        elif isinstance(X, np.ndarray):\n            X = X.flatten()\n        elif isinstance(X, list):\n            X = np.array(X).flatten()\n        else:\n            raise TypeError(f\"X must be a pandas DataFrame, Series, numpy array, or list, not {type(X)}\")\n    else:\n        X_w_ix = X.reset_index()\n        X_pivot = X_w_ix.pivot(index=\"index\", columns=\"variable\", values=\"value\")\n        X_pivot = X_pivot.fillna(0)\n        X = np.concatenate((np.array([X_pivot.index.size]).reshape(-1, 1), X_pivot.to_numpy()), axis=1)\n    return X", "CodeBleu_score": 40.82798598783365}]}
{"instance_id": "8798260a6f12", "generation_result": [{"code": "def is_sklearn_estimator(obj)\n\treturn isclass(obj) and issubclass(obj, sklearn.base.BaseEstimator) and not issubclass(obj, BaseObject)\n\n", "CodeBleu_score": 12.60472491309036}, {"code": "def is_sklearn_estimator(obj)\n\treturn isclass(obj) and issubclass(obj, SklearnBaseEstimator) and not issubclass(obj, BaseObject)\n\n", "CodeBleu_score": 14.398781154631099}, {"code": "def is_sklearn_estimator(obj)\n\t# check if obj is a class\n    if not isinstance(obj, type):\n        raise TypeError(f\"The object {obj} is not a class.\")\n    # check if obj is a subclass of SklearnBaseEstimator\n    if not issubclass(obj, SklearnBaseEstimator):\n        raise TypeError(f\"The object {obj} is not a subclass of SklearnBaseEstimator.\")\n    # check if obj is a subclass of BaseObject\n    if issubclass(obj, BaseObject):\n        raise TypeError(f\"The object {obj} is a subclass of BaseObject, indicating it is a sktime estimator.\")\n    return True\n\n", "CodeBleu_score": 20.524031251271712}]}
{"instance_id": "2ab2596d8520", "generation_result": [{"code": "def __rmul__(self, other)\n\tif isinstance(other, BaseTransformer):\n        return SklearnRegressorPipeline(self.regressor,\n                                        TransformerPipeline(self.transformers, other))\n    return NotImplemented", "CodeBleu_score": 23.62556197156499}, {"code": "def __rmul__(self, other)\n\tif isinstance(other, BaseTransformer):\n        new_transformers = self.transformers\n        new_transformers.append(other)\n        new_pipeline = TransformerPipeline(new_transformers)\n        return SklearnRegressorPipeline(self.regressor, new_pipeline)\n    else:\n        return NotImplemented", "CodeBleu_score": 33.45464851721367}, {"code": "def __rmul__(self, other)\n\tif isinstance(other, BaseTransformer):\n        return SklearnRegressorPipeline(self.regressor, TransformerPipeline(other, self.transformer))\n    else:\n        return NotImplemented\n\n", "CodeBleu_score": 19.046794690084294}]}
{"instance_id": "5c1c6d14ca93", "generation_result": [{"code": "def prep_skl_df(df, copy_df=False)\n\tif copy_df:\n        df = df.copy()\n    if not all([isinstance(col, str) for col in df.columns]):\n        df.columns = df.columns.astype(str)\n    return df", "CodeBleu_score": 48.40941368039326}, {"code": "def prep_skl_df(df, copy_df=False)\n\tif copy_df:\n        df = df.copy()\n    df.columns = [str(col) for col in df.columns]\n    return df", "CodeBleu_score": 39.92114389169378}, {"code": "def prep_skl_df(df, copy_df=False)\n\tif copy_df:\n        df = df.copy()\n    df.columns = df.columns.astype(str)\n    if not df.columns.astype(str).equals(df.columns):\n        df.columns = df.columns.astype(str)\n    return df\n\n", "CodeBleu_score": 50.88111022756681}]}
{"instance_id": "d5e567c20361", "generation_result": [{"code": "def predict(self, X)\n\t# initialize random state\n    rng = check_random_state(self.random_state)\n\n    # get predicted probabilities\n    probs = self.predict_proba(X)\n\n    # select the indices of the maximum probability values\n    # for each sample\n    indices = probs.argmax(axis=1)\n\n    # randomly choose one of the indices\n    # for each sample\n    indices = [rng.choice(np.flatnonzero(probs[i] == probs[i].max())) for i in range(len(probs))]\n\n    # map the indices to the corresponding class labels\n    return self.classes_[indices]\n\n\n", "CodeBleu_score": 35.9692649334588}, {"code": "def predict(self, X)\n\t# Initialize a random state using the object's random_state attribute.\n    rng = check_random_state(self.random_state)\n    # Obtain the predicted probabilities.\n    y_probas = self.predict_proba(X)\n    # Select the indices of the maximum probability values.\n    y_predict = rng.choice(np.flatnonzero(y_probas.max(axis=1)), size=X.shape[0])\n    # Map the chosen indices to the corresponding class labels.\n    y_predict = self.classes_[y_predict]\n    return y_predict\n\n\n", "CodeBleu_score": 37.93181353765384}, {"code": "def predict(self, X)\n\trng = check_random_state(self.random_state)\n    return np.array(\n        [\n            self.classes_[int(rng.choice(np.flatnonzero(prob == prob.max())))]\n            for prob in self.predict_proba(X)\n        ]\n    )\n\n\n", "CodeBleu_score": 68.08587905202594}]}
{"instance_id": "210b60dffd55", "generation_result": [{"code": "def _predict_proba_for_estimator(self, X, clf, pcas, groups)\n\tX_transformed = np.concatenate(\n        [\n            pca.transform(X[:, group])\n            for pca, group in zip(pcas, groups)\n        ],\n        axis=1,\n    )\n    X_transformed = np.nan_to_num(X_transformed)\n    y_proba = clf.predict_proba(X_transformed)\n    if y_proba.shape[1] != self.n_classes_:\n        y_proba = self._adjust_proba(y_proba)\n    return y_proba\n\n", "CodeBleu_score": 23.97105369560592}, {"code": "def _predict_proba_for_estimator(self, X, clf, pcas, groups)\n\tX_transformed = np.concatenate(\n        [\n            pca.transform(X[:, group])\n            for pca, group in zip(pcas, groups)\n        ],\n        axis=1,\n    )\n    X_transformed = np.nan_to_num(X_transformed)\n\n    y_probas = clf.predict_proba(X_transformed)\n\n    if y_probas.shape[1] != self.n_classes_:\n        # the number of classes predicted by the classifier\n        # does not match the expected number of classes\n        # this can happen when using a base estimator\n        # that does not support multi-class classification\n        # (e.g., using a base estimator that only supports\n        # binary classification with n_classes_ > 2)\n        # we adjust the probabilities to match the expected\n        # class structure\n        y_probas = np.concatenate(\n            [\n                y_probas,\n                np.zeros(\n                    (\n                        y_probas.shape[0],\n                        self.n_classes_ - y_probas.shape[1],\n                    )\n                ),\n            ],\n            axis=1,\n        )\n        y_probas = y_probas.reshape(\n            (\n                y_probas.shape[0],\n                self.n_classes_,\n            )\n        )\n\n    return y_probas", "CodeBleu_score": 24.201188499244275}, {"code": "def _predict_proba_for_estimator(self, X, clf, pcas, groups)\n\t# transform the data using the corresponding PCA\n    X_transformed = np.concatenate(\n        [\n            pca.transform(X[:, group])\n            for pca, group in zip(pcas, groups)\n        ],\n        axis=1,\n    )\n\n    # replace missing values with 0\n    X_transformed = np.nan_to_num(X_transformed)\n\n    # compute class probabilities\n    y_proba = clf.predict_proba(X_transformed)\n\n    # adjust the probabilities to match the expected class structure\n    if y_proba.shape[1] != self.n_classes_:\n        y_proba = np.concatenate(\n            (\n                y_proba,\n                np.zeros((y_proba.shape[0], self.n_classes_ - y_proba.shape[1])),\n            ),\n            axis=1,\n        )\n        y_proba = y_proba[:, self.classes_]\n\n    return y_proba", "CodeBleu_score": 28.31598796271037}]}
{"instance_id": "9436ee0860bb", "generation_result": [{"code": "def predict_proba(self, X)\n\tif not self._is_fitted:\n        raise NotFittedError(\n            f\"This instance of {self.__class__.__name__} has not \"\n            f\"been fitted yet; please call `fit` first.\"\n        )\n\n    # treat case of single class seen in fit\n    if self.n_classes_ == 1:\n        return np.repeat([[1]], X.shape[0], axis=0)\n\n    if isinstance(X, np.ndarray) and len(X.shape) == 3 and X.shape[1] == 1:\n        X = np.reshape(X, (X.shape[0], -1))\n    elif not isinstance(X, np.ndarray) or len(X.shape) > 2:\n        raise ValueError(\n            \"ContinuousIntervalTree is not a time series classifier. \"\n            \"A valid sklearn input such as a 2d numpy array is required.\"\n            \"Sparse input formats are currently not supported.\"\n        )\n    X = self._validate_data(X=X, reset=False, force_all_finite=\"allow-nan\")\n\n    dists = np.zeros((X.shape[0], self.n_classes_))\n    for i in range(X.shape[0]):\n        dists[i] = self._root.predict_proba(X[i], self.n_classes_)\n    return dists", "CodeBleu_score": 89.68560719097412}, {"code": "def predict_proba(self, X)\n\tif not self._is_fitted:\n        raise NotFittedError(\n            f\"This instance of {self.__class__.__name__} has not \"\n            f\"been fitted yet; please call `fit` first.\"\n        )\n\n    # treat case of single class seen in fit\n    if self.n_classes_ == 1:\n        return np.repeat([[1]], X.shape[0], axis=0)\n\n    if isinstance(X, np.ndarray) and len(X.shape) == 3 and X.shape[1] == 1:\n        X = np.reshape(X, (X.shape[0], -1))\n    elif not isinstance(X, np.ndarray) or len(X.shape) > 2:\n        raise ValueError(\n            \"ContinuousIntervalTree is not a time series classifier. \"\n            \"A valid sklearn input such as a 2d numpy array is required.\"\n            \"Sparse input formats are currently not supported.\"\n        )\n    X = self._validate_data(X=X, reset=False, force_all_finite=\"allow-nan\")\n\n    dists = np.zeros((X.shape[0], self.n_classes_))\n    for i in range(X.shape[0]):\n        dists[i] = self._root.predict_proba(X[i], self.n_classes_)\n    return dists", "CodeBleu_score": 89.68560719097412}, {"code": "def predict_proba(self, X)\n\tif not self._is_fitted:\n        raise NotFittedError(\n            f\"This instance of {self.__class__.__name__} has not \"\n            f\"been fitted yet; please call `fit` first.\"\n        )\n\n    # treat case of single class seen in fit\n    if self.n_classes_ == 1:\n        return np.repeat([[1]], X.shape[0], axis=0)\n\n    if isinstance(X, np.ndarray) and len(X.shape) == 3 and X.shape[1] == 1:\n        X = np.reshape(X, (X.shape[0], -1))\n    elif not isinstance(X, np.ndarray) or len(X.shape) > 2:\n        raise ValueError(\n            \"ContinuousIntervalTree is not a time series classifier. \"\n            \"A valid sklearn input such as a 2d numpy array is required.\"\n            \"Sparse input formats are currently not supported.\"\n        )\n    X = self._validate_data(X=X, reset=False, force_all_finite=\"allow-nan\")\n\n    dists = np.zeros((X.shape[0], self.n_classes_))\n    for i in range(X.shape[0]):\n        dists[i] = self._root.predict_proba(X[i], self.n_classes_)\n    return dists", "CodeBleu_score": 89.68560719097412}]}
{"instance_id": "d7cb1cb66840", "generation_result": [{"code": "def _convert_X_to_sklearn(self, X)\n\tif self.transformers_ is None:\n        raise RuntimeError(\"transformers_ attribute is not set. Call the fit method first.\")\n\n    if self.transformers_ == \"Primitives\":\n        X = self._convert_to_Primitives(X)\n    elif self.transformers_ == \"Series\":\n        X = self._convert_to_Series(X)\n    else:\n        raise TypeError(\n            \"transformers_ attribute is neither 'Primitives' nor 'Series'. It is of type %s\"\n            % type(self.transformers_)\n        )\n\n    return X\n\n", "CodeBleu_score": 27.55717511161919}, {"code": "def _convert_X_to_sklearn(self, X)\n\tX_sklearn = self.transformers_.transform(X)\n    scitype = self.transformers_.get_tag(\"scitype:transform-output\")\n    if scitype == \"Primitives\":\n        X_sklearn = X_sklearn.to_numpy().reshape(-1, 1)\n    elif scitype == \"Series\":\n        X_sklearn = X_sklearn.to_numpy()\n    else:\n        raise TypeError(\n            f\"The scitype of the output of {self.name} is {scitype}, which is not supported by scikit-learn.\"\n        )\n    return X_sklearn\n\n", "CodeBleu_score": 33.49809807112129}, {"code": "def _convert_X_to_sklearn(self, X)\n\tX_transformed = self.transform(X)\n    if isinstance(X_transformed, pd.DataFrame):\n        X_transformed = X_transformed.values\n    elif isinstance(X_transformed, pd.Series):\n        X_transformed = X_transformed.to_numpy()\n    elif isinstance(X_transformed, np.ndarray):\n        X_transformed = X_transformed\n    else:\n        raise TypeError(\"The output type of the transformer is not recognized\")\n    return X_transformed\n\n", "CodeBleu_score": 21.509944341126655}]}
{"instance_id": "497cb75f2b4d", "generation_result": [{"code": "def set_random_state(estimator, random_state=0)\n\t# Ensure random_state is a valid random state object\n    random_state = check_random_state(random_state)\n    # Initialize an empty dictionary to hold parameters that need updating\n    params = {}\n    # Iterate over the estimator's parameters, checking for those related to random state\n    for param in estimator.get_params():\n        # If the parameter is related to random state, assign it a random integer value\n        if param.endswith(\"random_state\"):\n            params[param] = random_state.randint(np.iinfo(np.int32).max)\n    # Update the estimator with the new random state values if any were found\n    if params:\n        estimator.set_params(**params)\n\n\n\n\n", "CodeBleu_score": 40.795751008033584}, {"code": "def set_random_state(estimator, random_state=0)\n\t# Check that random_state is a valid random state object\n    random_state = check_random_state(random_state)\n    # Initialize an empty dictionary to hold parameters that need updating\n    params = {}\n    # Iterate over the estimator's parameters, checking for those related to random state\n    for param in estimator.get_params().keys():\n        if param.endswith('random_state'):\n            params[param] = random_state.randint(0, 100000)\n    # Update the estimator with the new random state values if any were found\n    if params:\n        estimator.set_params(**params)\n    return estimator", "CodeBleu_score": 35.71770182770993}, {"code": "def set_random_state(estimator, random_state=0)\n\t# Ensure the random state is a valid random state object\n    random_state = check_random_state(random_state)\n    # Initialize an empty dictionary to hold parameters that need updating\n    params = {}\n    # Iterate over the estimator's parameters, checking for those related to random state\n    for param in estimator.get_params():\n        if param.endswith('random_state'):\n            params[param] = random_state.randint(1000000000)\n    # Update the estimator with the new random state values if any were found\n    if params:\n        estimator.set_params(**params)\n    return estimator\n\n", "CodeBleu_score": 33.320437878143096}]}
{"instance_id": "59ffd1d596a1", "generation_result": [{"code": "def _transform(X, intervals)\n\t# Initialize variables\n    n_instances, n_intervals = intervals.shape\n    n_features = X.shape[1]\n    transformed = np.empty((n_features, n_instances * n_intervals))\n\n    # For each interval, compute the mean, standard deviation, and slope\n    # of the corresponding slice of X\n    for i in range(n_instances):\n        for j in range(n_intervals):\n            idx = np.arange(intervals[i, j, 0], intervals[i, j, 1])\n            transformed[:, i * n_intervals + j] = np.array([\n                np.mean(X[idx, :], axis=0),\n                np.std(X[idx, :], axis=0),\n                _slope(X[idx, :], axis=0)\n            ])\n\n    # Return the transpose of the transformed array\n    return transformed.T", "CodeBleu_score": 29.43213670277915}, {"code": "def _transform(X, intervals)\n\t# Number of instances\n    n = X.shape[0]\n\n    # Number of intervals\n    m = len(intervals)\n\n    # Initialize empty array to store transformed features\n    X_new = np.empty((n, 3 * m))\n\n    # For each interval\n    for i in range(m):\n        # Slice the input matrix\n        x = X[:, intervals[i][0] : intervals[i][1] + 1]\n\n        # Compute mean, standard deviation, and slope\n        x_mean = np.mean(x, axis=1)\n        x_std = np.std(x, axis=1)\n        x_slope = _slope(x, axis=1)\n\n        # Store in transformed array\n        X_new[:, 3 * i] = x_mean\n        X_new[:, 3 * i + 1] = x_std\n        X_new[:, 3 * i + 2] = x_slope\n\n    # Return the transposed transformed array\n    return X_new.T\n\n", "CodeBleu_score": 31.59991271797739}, {"code": "def _transform(X, intervals)\n\tn_instances, n_intervals = X.shape[0], len(intervals)\n    transformed = np.empty((n_intervals, n_instances))\n    for i, interval in enumerate(intervals):\n        transformed[i, :] = np.mean(X[:, interval[0] : interval[1]], axis=1)\n        transformed[i + n_intervals, :] = np.std(\n            X[:, interval[0] : interval[1]], axis=1\n        )\n        transformed[i + 2 * n_intervals, :] = _slope(\n            X[:, interval[0] : interval[1]], axis=1\n        )\n    return transformed.T\n\n", "CodeBleu_score": 28.97537033248982}]}
{"instance_id": "5a01594eebfb", "generation_result": [{"code": "def plot_and_annotate_facets(\n            self, plot: str, plot_params: dict, configuration: dict,\n            annotation_func: str, *args, annotation_params: dict = None,\n            ax_op_before: List[Union[str, Optional[list],\n                                     Optional[dict]]] = None,\n            ax_op_after: List[Union[str, Optional[list],\n                                    Optional[dict]]] = None,\n            annotate_params: dict = None, **kwargs)\n\tif annotation_params is None:\n        annotation_params = {}\n    if annotate_params is None:\n        annotate_params = {}\n    if ax_op_before is None:\n        ax_op_before = []\n    if ax_op_after is None:\n        ax_op_after = []\n\n    plotter = self._get_plotter(\n        engine=plot_params.get(\"engine\", \"seaborn\"),\n        ax=self.ax,\n        pairs=self.pairs,\n        plot=plot,\n        data=self.data,\n        x=self.x,\n        y=self.y,\n        hue=self.hue,\n        order=self.order,\n        hue_order=self.hue_order,\n        **plot_params)\n\n    plotter.plot(*args, **kwargs)\n\n    _apply_ax_operations(self.ax, ax_op_before)\n\n    plotter.configure(**configuration)\n\n    if annotation_func is not None:\n        plotter.annotate(**annotate_params)\n\n    _apply_ax_operations(self.ax, ax_op_after)\n\n    return self.ax, plotter.annotations\n\n", "CodeBleu_score": 48.66222518101088}, {"code": "def plot_and_annotate_facets(\n            self, plot: str, plot_params: dict, configuration: dict,\n            annotation_func: str, *args, annotation_params: dict = None,\n            ax_op_before: List[Union[str, Optional[list],\n                                     Optional[dict]]] = None,\n            ax_op_after: List[Union[str, Optional[list],\n                                    Optional[dict]]] = None,\n            annotate_params: dict = None, **kwargs)\n\tif annotation_params is None:\n        annotation_params = {}\n    if annotate_params is None:\n        annotate_params = {}\n    if ax_op_before is None:\n        ax_op_before = []\n    if ax_op_after is None:\n        ax_op_after = []\n\n    plot_params = self._get_plot_params(plot_params)\n    annotate_params = self._get_annotate_params(annotate_params)\n\n    ax = self._get_ax()\n    self.plot(plot, ax, *args, **plot_params, **kwargs)\n\n    self._apply_ax_operations(ax, ax_op_before)\n\n    if configuration is not None:\n        self.configure(**configuration)\n\n    if annotation_func is not None:\n        self.annotate(**annotation_params)\n\n    self._apply_ax_operations(ax, ax_op_after)\n\n    return self._get_output()\n\n", "CodeBleu_score": 53.54532670472971}, {"code": "def plot_and_annotate_facets(\n            self, plot: str, plot_params: dict, configuration: dict,\n            annotation_func: str, *args, annotation_params: dict = None,\n            ax_op_before: List[Union[str, Optional[list],\n                                     Optional[dict]]] = None,\n            ax_op_after: List[Union[str, Optional[list],\n                                    Optional[dict]]] = None,\n            annotate_params: dict = None, **kwargs)\n\tif annotation_params is None:\n        annotation_params = {}\n\n    if annotate_params is None:\n        annotate_params = {}\n\n    annotate_params.update(configuration)\n    annotate_params[\"verbose\"] = configuration[\"verbose\"]\n\n    if ax_op_before is None:\n        ax_op_before = []\n\n    if ax_op_after is None:\n        ax_op_after = []\n\n    if annotation_func == \"annotate_pairwise\":\n        annotation_params[\"ax_op_before\"] = ax_op_before\n        annotation_params[\"ax_op_after\"] = ax_op_after\n        annotation_params[\"annotate_params\"] = annotate_params\n\n    if plot == \"boxplot\":\n        if \"data\" in kwargs:\n            data = kwargs[\"data\"]\n        else:\n            data = None\n        if \"x\" in kwargs:\n            x = kwargs[\"x\"]\n        else:\n            x = None\n        if \"y\" in kwargs:\n            y = kwargs[\"y\"]\n        else:\n            y = None\n        if \"hue\" in kwargs:\n            hue = kwargs[\"hue\"]\n        else:\n            hue = None\n        if \"order\" in kwargs:\n            order = kwargs[\"order\"]\n        else:\n            order = None\n        if \"hue_order\" in kwargs:\n            hue_order = kwargs[\"hue_order\"]\n        else:\n            hue_order = None\n        if \"pairs\" in kwargs:\n            pairs = kwargs[\"pairs\"]\n        else:\n            pairs = None\n        if \"engine\" in kwargs:\n            engine = kwargs[\"engine\"]\n        else:\n            engine = \"seaborn\"\n\n        plotter = self.new_plot(ax=ax, pairs=pairs, plot=plot, data=data, x=x,\n                                y=y, hue=hue, order=order, hue_order=hue_order,\n                                engine=engine, **plot_params)\n    else:\n        plotter = self.", "CodeBleu_score": 36.74987004603588}]}
{"instance_id": "b5b17d312c74", "generation_result": [{"code": "def predict_proba(self, X)\n\tif not hasattr(self, '_program'):\n        raise NotFittedError(\"This %s instance is not fitted yet. Call 'fit' with appropriate arguments before using this method.\" % self.__class__.__name__)\n\n    X = check_array(X)\n    if X.shape[1] != self.n_features_:\n        raise ValueError(\"X has %d features per sample; expecting %d\"\n                         % (X.shape[1], self.n_features_))\n\n    scores = execute(self._program, X)\n    probs = self._transformer.transform(scores)\n\n    return np.vstack((1 - probs, probs)).T", "CodeBleu_score": 39.942003291746836}, {"code": "def predict_proba(self, X)\n\tif not hasattr(self, '_program'):\n        raise NotFittedError(\"This model has not been fitted yet. Call 'fit' with appropriate arguments before using this method.\")\n    X = check_array(X)\n    if X.shape[1] != self.n_features_in_:\n        raise ValueError(\"Number of features of the model must match the input. Model n_features_in_ is {0} and input n_features is {1}.\".format(self.n_features_in_, X.shape[1]))\n    scores = self.execute(X)\n    probs = self._transformer.transform(scores)\n    return probs", "CodeBleu_score": 46.88708470518465}, {"code": "def predict_proba(self, X)\n\tif not hasattr(self, '_program'):\n        raise NotFittedError(\"The model has not been fitted yet. Call 'fit' with appropriate arguments before using this estimator.\")\n    X = check_array(X)\n    if X.shape[1] != self.n_features_:\n        raise ValueError(\"The number of features in the model does not match the input. Model n_features is %s and input n_features is %s \" % (self.n_features_, X.shape[1]))\n    scores = self._program.execute(X)\n    probs = np.vstack((np.exp(-scores), np.exp(scores))).T\n    return probs", "CodeBleu_score": 52.37698085746481}]}
{"instance_id": "83f71c959df7", "generation_result": [{"code": "def point_mutation(self, random_state)\n\t# Create a copy of the program\n    program = copy.deepcopy(self)\n    # Identify nodes to mutate\n    nodes_to_mutate = np.where(random_state.uniform(size=len(program)) < self.mutation_rate)[0]\n    # Mutate the nodes\n    for node_idx in nodes_to_mutate:\n        # Identify the node to be mutated\n        node = program.nodes[node_idx]\n        # If the node is a function, find a valid replacement function\n        if node.type == \"function\":\n            # Identify the arity of the function\n            arity = len(node.inputs)\n            # Identify a valid replacement function with the same arity\n            valid_functions = [f for f in self.functions if f.arity == arity]\n            replacement_function = random_state.choice(valid_functions)\n            # Update the node with the replacement function\n            node.function = replacement_function\n        # If the node is a terminal, find a valid replacement terminal\n        elif node.type == \"terminal\":\n            # Identify the type of terminal\n            terminal_type = node.value[0]\n            # If the terminal is a constant, find a valid replacement constant\n            if terminal_type == \"constant\":\n                # Identify a valid replacement constant\n                valid_constants = [c for c in self.constants if c >= self.constant_range[0] and c <= self.constant_range[1]]\n                replacement_constant = random_state.choice(valid_constants)\n                # Update the node with the replacement constant\n                node.value = (\"constant\", replacement_constant)\n            # If the terminal is a variable, find a valid replacement variable\n            elif terminal_type == \"variable\":\n                # Identify a valid replacement variable\n                valid_variables = [v for v in self.variables if v < self.num_features]\n                replacement_variable = random_state.choice(valid_variables)\n                # Update the node with the replacement variable\n                node.value = (\"variable\", replacement_variable)\n    # Return the mutated program and the list of modified node indices\n    return program, nodes_to_mutate", "CodeBleu_score": 27.05265570980097}, {"code": "def point_mutation(self, random_state)\n\t# Create a copy of the program\n    mutated_program = copy.deepcopy(self)\n\n    # Identify nodes to mutate\n    mutation_indices = np.where(\n        random_state.uniform(size=self.size) < self.mutation_probability\n    )[0]\n\n    # Mutate each node\n    for i in mutation_indices:\n        # If the node is a function, find a valid replacement function\n        if self.nodes[i][\"type\"] == \"function\":\n            mutated_program.nodes[i][\"function\"] = random_state.choice(\n                self.valid_functions[self.nodes[i][\"arity\"]]\n            )\n        # If the node is a terminal, replace it with a constant or variable\n        elif self.nodes[i][\"type\"] == \"terminal\":\n            if random_state.randint(2) == 0:\n                mutated_program.nodes[i][\"value\"] = random_state.uniform(\n                    self.constant_range[0], self.constant_range[1]\n                )\n            else:\n                mutated_program.nodes[i][\"value\"] = random_state.randint(\n                    self.num_features\n                )\n\n    return mutated_program, mutation_indices\n\n", "CodeBleu_score": 26.409251835097656}, {"code": "def point_mutation(self, random_state)\n\t# Make a copy of the program\n    program = copy.deepcopy(self)\n    # Identify nodes to mutate\n    nodes_to_mutate = np.where(random_state.uniform(size=len(program.nodes)) < self.mutation_rate)[0]\n    # Mutate nodes\n    for node_index in nodes_to_mutate:\n        node = program.nodes[node_index]\n        # If the node is a function, replace it with a valid function\n        if isinstance(node, Function):\n            # Find a valid replacement function\n            replacement_function = self.find_valid_replacement_function(node)\n            # Update the node\n            program.nodes[node_index] = replacement_function\n        # If the node is a terminal, replace it with either a constant or a variable\n        else:\n            # If the node is a constant, replace it with a variable\n            if isinstance(node, Constant):\n                program.nodes[node_index] = Variable(random_state.randint(self.num_features))\n            # If the node is a variable, replace it with a constant\n            else:\n                program.nodes[node_index] = Constant(random_state.uniform(self.constant_range[0], self.constant_range[1]))\n    # Return the mutated program and the list of modified node indices\n    return program, nodes_to_mutate", "CodeBleu_score": 25.559431059715127}]}
{"instance_id": "f1853251e671", "generation_result": [{"code": "def bias(close, length=None, mamode=None, offset=None, **kwargs)\n\tlength = int(length) if length and length > 0 else 14\n    mamode = mamode.lower() if mamode and isinstance(mamode, str) else \"ema\"\n    offset = get_offset(offset)\n\n    # Validate Arguments\n    assert isinstance(close, pd.Series), \"close argument has to be of type pd.Series\"\n    assert isinstance(length, int) and length > 0, \"length argument has to be of type int and greater than 0\"\n    assert isinstance(mamode, str) and mamode in [\"ema\", \"sma\", \"wma\"], \"mamode argument has to be of type str and in ['ema', 'sma', 'wma']\"\n    assert isinstance(offset, int) and offset >= 0, \"offset argument has to be of type int and greater than or equal to 0\"\n\n    # Calculate Result\n    ma = ma(close=close, length=length, mamode=mamode, **kwargs)\n    bias = close.sub(ma, fill_value=0).div(ma, fill_value=0)\n\n    # Offset\n    if offset != 0:\n        bias = bias.shift(offset)\n\n    # Name & Category\n    bias.name = f\"BIAS_{length}_{mamode}\"\n    bias.category = \"momentum\"\n\n    return bias\n\n", "CodeBleu_score": 44.90322078843988}, {"code": "def bias(close, length=None, mamode=None, offset=None, **kwargs)\n\tlength = int(length) if length and length > 0 else 20\n    mamode = mamode if mamode and mamode in ['ema', 'sma', 'wma', 'hma'] else 'sma'\n    close = verify_series(close)\n    sma = ma(close, length=length, mamode=mamode, **kwargs)\n    bias = close.pct_change()\n    bias = bias.shift()\n    bias = bias.fillna(0)\n    bias = bias.multiply(100)\n    bias = bias.subtract(sma)\n    bias = bias.shift(-1)\n    bias = bias.fillna(method='ffill')\n    bias = bias.fillna(method='bfill')\n    bias.name = f\"BIAS_{length}\"\n    bias.category = 'momentum'\n    return bias\n\n", "CodeBleu_score": 29.65506225434758}, {"code": "def bias(close, length=None, mamode=None, offset=None, **kwargs)\n\tlength = int(length) if length and length > 0 else 20\n    mamode = mamode.lower() if mamode and isinstance(mamode, str) else \"ema\"\n    offset = get_offset(offset)\n\n    # Validate Arguments\n    close = verify_series(close)\n\n    # Calculate Result\n    ma = ma(close, length=length, mamode=mamode)\n    bias = close.sub(ma).divide(ma).multiply(100)\n\n    # Offset\n    if offset != 0:\n        bias = bias.shift(offset)\n\n    # Handle fills\n    if \"fillna\" in kwargs:\n        bias.fillna(kwargs[\"fillna\"], inplace=True)\n    if \"fill_method\" in kwargs:\n        bias.fillna(method=kwargs[\"fill_method\"], inplace=True)\n\n    # Name & Categorize it\n    bias.name = f\"BIAS_{length}_{mamode}\"\n    bias.category = \"momentum\"\n\n    return bias\n\n", "CodeBleu_score": 53.081249740593805}]}
{"instance_id": "77825be37c24", "generation_result": [{"code": "def coins_bar(broker='hb', code='btc', ktype='D', size='2000')\n\ttry:\n        # format the URL based on the broker, cryptocurrency code, kline type, and size\n        if broker == 'hb':\n            url = f'https://api.huobi.pro/market/history/kline?period={ktype}&size={size}&symbol={code}'\n        elif broker == 'bx':\n            url = f'https://api.bixin.com/api/v1/market/kline?symbol={code}&type={ktype}&limit={size}'\n        elif broker == 'bfx':\n            url = f'https://api.bitfinex.com/v2/candles/trade:{size}:{ktype}:t{code}/hist'\n        elif broker == 'cb':\n            url = f'https://api.coinbase.com/v2/products/{code}-USD/candles?granularity={size}'\n        elif broker == 'bt':\n            url = f'https://api.bybit.com/public/linear/kline?symbol={code}&interval={ktype}&limit={size}'\n        elif broker == 'bb':\n            url = f'https://api.binance.com/api/v3/klines?symbol={code}&interval={ktype}&limit={size}'\n        elif broker == 'mxc':\n            url = f'https://www.mxc.com/open/api/v1/data/kline?symbol={code}&period={ktype}&size={size}'\n        else:\n            raise Exception(f'Broker {broker} not supported.')\n        # fetch the data and handle any broker-specific data structures\n        if broker == 'hb':\n            data = json.loads(_get_data(url))['data']\n            df = pd.DataFrame(data)\n            df.columns = ['date', 'open', 'high', 'low', 'close', 'volume']\n        elif broker == 'bx':\n            data = json.loads(_get_data(url))['data']\n            df = pd.DataFrame(data)\n            df.columns = ['date', 'open', 'high', 'low', 'close', '", "CodeBleu_score": 22.128100906462457}, {"code": "def coins_bar(broker='hb', code='btc', ktype='D', size='2000')\n\ttry:\n        if broker == 'hb':\n            url = 'http://market.huobi.com/history/kline?symbol={}_{}&period={}&size={}'.format(\n                code, broker, ktype, size)\n            r = requests.get(url)\n            js = r.json()\n            df = pd.DataFrame(js['data'])\n            df = df.rename(columns={'vol': 'volume'})\n            df['date'] = df['id'].apply(lambda x: int2time(x))\n            df['date'] = pd.to_datetime(df['date'])\n            df.set_index('date', inplace=True)\n            return df\n        elif broker == 'ok':\n            url = 'https://www.okex.com/api/v1/kline.do?symbol={}_{}&type={}&size={}'.format(\n                broker, code, ktype, size)\n            r = requests.get(url)\n            js = r.json()\n            df = pd.DataFrame(js['kline'])\n            df = df.rename(columns={'vol': 'volume'})\n            df['date'] = df['date'].apply(lambda x: int2time(x))\n            df['date'] = pd.to_datetime(df['date'])\n            df.set_index('date', inplace=True)\n            return df\n        elif broker == 'btc':\n            url = 'https://api.btc.com/v1/kline?symbol={}_{}&type={}&size={}'.format(\n                broker, code, ktype, size)\n            r = requests.get(url)\n            js = r.json()\n            df = pd.DataFrame(js['data'])\n            df = df.rename(columns={'vol': 'volume'})\n            df['date'] = df['id'].apply(lambda x: int2time(x))\n            df['date'] = pd.to_datetime(df['date'])\n            df.set_index('date', inplace=True)\n            return df\n        else:\n            print('Broker not supported.')\n    except Exception:\n        print", "CodeBleu_score": 35.600024940308735}, {"code": "def coins_bar(broker='hb', code='btc', ktype='D', size='2000')\n\ttry:\n        if broker == 'hb':\n            if ktype == 'D':\n                url = 'https://api.huobi.pro/market/history/kline?period=1day&size=' + size + '&symbol=' + code\n            elif ktype == 'W':\n                url = 'https://api.huobi.pro/market/history/kline?period=1week&size=' + size + '&symbol=' + code\n            elif ktype == 'M':\n                url = 'https://api.huobi.pro/market/history/kline?period=1mon&size=' + size + '&symbol=' + code\n            else:\n                url = 'https://api.huobi.pro/market/history/kline?period=1min&size=' + size + '&symbol=' + code\n            data = requests.get(url)\n            js = json.loads(data.content.decode('utf-8'))\n            df = pd.DataFrame(js['data'])\n            df.columns = ['date', 'open', 'high', 'low', 'close', 'volume', 'amount', 'id']\n            df['date'] = pd.to_datetime(df['date'], unit='ms')\n            return df\n        elif broker == 'bt':\n            if ktype == 'D':\n                url = 'https://api.binance.com/api/v1/klines?symbol=' + code + '&interval=1d&limit=' + size\n            elif ktype == 'W':\n                url = 'https://api.binance.com/api/v1/klines?symbol=' + code + '&interval=1w&limit=' + size\n            elif ktype == 'M':\n                url = 'https://api.binance.com/api/v1/klines?symbol=' + code + '&interval=1M&limit=' + size\n            else:\n                url = 'https://api.binance.com/api/v1/klines?symbol=' + code + '&interval=1m&limit=' + size\n            data = requests.get(url)\n            js = json.loads(data.content.decode('utf-8'))\n            df =", "CodeBleu_score": 28.321378044203694}]}
{"instance_id": "c6552851da8a", "generation_result": [{"code": "def coins_snapshot(broker='hb', code='btc', size='5')\n\t\"\"\"\n    :param broker: The broker to fetch data from. Currently, only 'hb' is supported.\n    :type broker: str\n    :param code: The cryptocurrency to fetch data for. Currently, only 'btc' is supported.\n    :type code: str\n    :param size: The number of data points to fetch. Currently, only '5' is supported.\n    :type size: str\n    :return: A pandas Panel containing the ask and bid data for the specified cryptocurrency.\n    :rtype: pandas.Panel\n    \"\"\"\n    try:\n        if broker == 'hb':\n            if code == 'btc':\n                if size == '5':\n                    url = 'http://api.huobi.com/staticmarket/ticker_btc_json.js'\n                    js = _get_data(url)\n                    ask = pd.DataFrame(js['ticker']['ask'], columns=['ask'])\n                    bid = pd.DataFrame(js['ticker']['bid'], columns=['bid'])\n                    ask['timestamp'] = [int2time(i) for i in js['ticker']['ts']]\n                    bid['timestamp'] = [int2time(i) for i in js['ticker']['ts']]\n                    data = {'ask': ask, 'bid': bid}\n                    return pd.Panel(data)\n    except Exception:\n        print(traceback.print_exc())\n\n", "CodeBleu_score": 38.40415037930791}, {"code": "def coins_snapshot(broker='hb', code='btc', size='5')\n\ttry:\n        data = get_data(broker, code, size)\n        data = data.split('", "CodeBleu_score": 9.250426839771364}, {"code": "def coins_snapshot(broker='hb', code='btc', size='5')\n\ttry:\n        data = _get_data(broker, code, size)\n        if broker == 'hb':\n            ts = int2time(data['ts'])\n            ask = pd.DataFrame(data['tick']['asks'], columns=['ask_price', 'ask_size'])\n            bid = pd.DataFrame(data['tick']['bids'], columns=['bid_price', 'bid_size'])\n        elif broker == 'ok':\n            ts = int2time(data['data']['timestamp'])\n            ask = pd.DataFrame(data['data']['asks'], columns=['ask_price', 'ask_size'])\n            bid = pd.DataFrame(data['data']['bids'], columns=['bid_price', 'bid_size'])\n        ask['ts'] = ts\n        bid['ts'] = ts\n        data = {'ask': ask, 'bid': bid}\n        data = pd.Panel(data)\n        return data\n    except Exception:\n        print(traceback.print_exc())", "CodeBleu_score": 42.465626198398276}]}
{"instance_id": "8b5fee723398", "generation_result": [{"code": "def coins_trade(broker='hb', code='btc')\n\t# \u83b7\u53d6\u6570\u636e\n    data = _get_data(broker, code)\n    if data is None:\n        return None\n    # \u5904\u7406\u6570\u636e\n    if broker == 'hb':\n        df = pd.DataFrame(data['data'], columns=['tid', 'price', 'amount', 'time', 'type'])\n        df['datetime'] = df['time'].apply(int2time)\n        df = df[['tid', 'datetime', 'price', 'amount', 'type']]\n    elif broker == 'ok':\n        df = pd.DataFrame(data['data'], columns=['tid', 'price', 'amount', 'time', 'type'])\n        df['datetime'] = df['time'].apply(int2time)\n        df = df[['tid', 'datetime', 'price', 'amount', 'type']]\n    elif broker == 'chbtc':\n        df = pd.DataFrame(data, columns=['tid', 'price', 'amount', 'time', 'type'])\n        df['datetime'] = df['time'].apply(int2time)\n        df = df[['tid', 'datetime', 'price', 'amount', 'type']]\n    else:\n        raise ValueError('Invalid broker!')\n    return df\n\n", "CodeBleu_score": 45.442473193301744}, {"code": "def coins_trade(broker='hb', code='btc')\n\turl = 'https://api.huobi.pro/market/history/trade?symbol=' + code + 'usdt&size=1000'\n    data = requests.get(url)\n    data = data.json()\n    data = data['data']\n    df = pd.DataFrame(data)\n    df = df[['id', 'ts', 'price', 'amount', 'direction']]\n    df.columns = ['tid', 'datetime', 'price', 'amount', 'type']\n    df['datetime'] = df['datetime'].apply(int2time)\n    df = df[['tid', 'datetime', 'price', 'amount', 'type']]\n    return df\n\n\n\n", "CodeBleu_score": 39.987692528104105}, {"code": "def coins_trade(broker='hb', code='btc')\n\tif broker == 'hb':\n        url = 'http://api.huobi.com/staticmarket/ticker_btc_json.js'\n        if code == 'ltc':\n            url = 'http://api.huobi.com/staticmarket/ticker_ltc_json.js'\n        elif code == 'eth':\n            url = 'http://api.huobi.com/staticmarket/ticker_eth_json.js'\n        try:\n            js = _get_data(url)\n            df = pd.DataFrame(js)\n            df = df.rename(columns={'data': 'tid', 'id': 'datetime', 'price': 'price', 'amount': 'amount', 'type': 'type'})\n            df['datetime'] = df['datetime'].apply(int2time)\n            df = df[['tid', 'datetime', 'price', 'amount', 'type']]\n            return df\n        except Exception:\n            print(traceback.print_exc())\n    elif broker == 'ok':\n        url = 'https://www.okcoin.cn/api/v1/trades.do?symbol=ltc_btc'\n        if code == 'ltc':\n            url = 'https://www.okcoin.cn/api/v1/trades.do?symbol=ltc_btc'\n        elif code == 'eth':\n            url = 'https://www.okcoin.cn/api/v1/trades.do?symbol=eth_btc'\n        try:\n            js = _get_data(url)\n            df = pd.DataFrame(js)\n            df = df.rename(columns={'tid': 'tid', 'date': 'datetime', 'price': 'price', 'amount': 'amount', 'type': 'type'})\n            df['datetime'] = df['datetime'].apply(int2time)\n            df = df[['tid', 'datetime', 'price', 'amount', 'type']]\n            return df\n        except Exception:\n            print(traceback.print_exc())\n    elif broker == 'chbtc':\n        url = 'http://api.chbtc.com/data", "CodeBleu_score": 42.727809879047804}]}
{"instance_id": "25e0a134c45b", "generation_result": [{"code": "def _parse_fund_data(url, fund_type='open')\n\ttry:\n        ct._write_console('Retrieving fund data from ' + url + '.')\n        response = urlopen(Request(url, headers={'User-Agent': 'Mozilla/5.0'}))\n        text = response.read().decode('utf-8')\n        text = text.replace('\"', '\\\"')\n        text = text.replace('False', 'false')\n        text = text.replace('True', 'true')\n        text = text.replace('None', 'null')\n        text = text.replace(': null,', ': None,')\n        text = text.replace(': null}', ': None}')\n        text = text.replace('\\\\\"', '\\\\\\\\\"')\n        text = text.replace(': null,', ': None,')\n        text = text.replace(': null}', ': None}')\n        text = text.replace('\\\\\"', '\\\\\\\\\"')\n        text = text.replace(': null,', ': None,')\n        text = text.replace(': null}', ': None}')\n        text = text.replace('\\\\\"', '\\\\\\\\\"')\n        text = text.replace(': null,', ': None,')\n        text = text.replace(': null}', ': None}')\n        text = text.replace('\\\\\"', '\\\\\\\\\"')\n        text = text.replace(': null,', ': None,')\n        text = text.replace(': null}', ': None}')\n        text = text.replace('\\\\\"', '\\\\\\\\\"')\n        text = text.replace(': null,', ': None,')\n        text = text.replace(': null}', ': None}')\n        text = text.replace('\\\\\"', '\\\\\\\\\"')\n        text = text.replace(': null,', ': None,')\n        text = text.replace(': null}', ': None}')\n        text = text.replace('\\\\\"', '\\\\\\\\\"')\n        text = text.replace(': null,', ': None,')\n        text = text.replace(': null}', ': None}')\n        text = text.replace('", "CodeBleu_score": 24.881975186555906}, {"code": "def _parse_fund_data(url, fund_type='open')\n\ttry:\n        ct._write_console('Requesting ' + url)\n        response = urlopen(Request(url, headers={'User-Agent': 'Mozilla/5.0'}))\n        text = response.read().decode('utf-8')\n        text = text.replace('var fundData = ', '')\n        text = text.replace(';', '')\n        text = text.replace('null', '\"null\"')\n        text = text.replace(':null', ':\"null\"')\n        text = text.replace('{', '{\"')\n        text = text.replace(':', '\":')\n        text = text.replace('}', '\"}')\n        text = text.replace('\"{', '{\"')\n        text = text.replace('}\"', '\"}')\n        text = text.replace('\"\"', '\"')\n        text = text.replace('},\"', '},\"')\n        text = text.replace('},\"', '},\"')\n        text = text.replace('},\"', '},\"')\n        text = text.replace('},\"', '},\"')\n        text = text.replace('},\"', '},\"')\n        text = text.replace('},\"', '},\"')\n        text = text.replace('},\"', '},\"')\n        text = text.replace('},\"', '},\"')\n        text = text.replace('},\"', '},\"')\n        text = text.replace('},\"', '},\"')\n        text = text.replace('},\"', '},\"')\n        text = text.replace('},\"', '},\"')\n        text = text.replace('},\"', '},\"')\n        text = text.replace('},\"', '},\"')\n        text = text.replace('},\"', '},\"')\n        text = text.replace('},\"', '},\"')\n        text = text.replace('},\"', '},\"')\n        text = text.replace('},\"', '},\"')\n        text = text.replace('},\"', '},\"')\n        text = text.replace('},\"', '},\"')\n        text = text.replace('},\"', '},\"')", "CodeBleu_score": 25.55193581818722}, {"code": "def _parse_fund_data(url, fund_type='open')\n\tprint('Retrieving fund data from ' + url)\n    try:\n        response = urlopen(Request(url, headers={'User-Agent': 'Mozilla/5.0'}))\n        text = response.read().decode('utf-8')\n        text = text.replace('\\\\', '')\n        text = text.replace('\"{symbol\"', '\"symbol\"')\n        text = text.replace('\"{name\"', '\"name\"')\n        text = text.replace('\"{last_updated\"', '\"last_updated\"')\n        text = text.replace('\"{nav_date\"', '\"nav_date\"')\n        text = text.replace('\"{nav\"', '\"nav\"')\n        text = text.replace('\"{nav_change\"', '\"nav_change\"')\n        text = text.replace('\"{nav_change_percent\"', '\"nav_change_percent\"')\n        text = text.replace('\"{aum\"', '\"aum\"')\n        text = text.replace('\"{aum_date\"', '\"aum_date\"')\n        text = text.replace('\"{aum_change\"', '\"aum_change\"')\n        text = text.replace('\"{aum_change_percent\"', '\"aum_change_percent\"')\n        text = text.replace('\"{discount_rate\"', '\"discount_rate\"')\n        text = text.replace('\"{discount_rate_date\"', '\"discount_rate_date\"')\n        text = text.replace('\"{discount_rate_change\"', '\"discount_rate_change\"')\n        text = text.replace('\"{discount_rate_change_percent\"', '\"discount_rate_change_percent\"')\n        text = text.replace('\"{morningstar_category\"', '\"morningstar_category\"')\n        text = text.replace('\"{morningstar_rating\"', '\"morningstar_rating\"')\n        text = text.replace('\"{morningstar_risk_rating\"', '\"morningstar_risk_rating\"')\n        text = text.replace('\"{morningstar_return_rating\"', '\"morningstar_return_rating\"", "CodeBleu_score": 28.83189646618034}]}
{"instance_id": "401d18e122d8", "generation_result": [{"code": "def get_future_daily(start = None, end = None, market = 'CFFEX')\n\tif market.upper() == 'CFFEX':\n        func = get_cffex_daily\n    elif market.upper() == 'SHFE':\n        func = get_shfe_daily\n    elif market.upper() == 'DCE':\n        func = get_dce_daily\n    elif market.upper() == 'CZCE':\n        func = get_czce_daily\n    else:\n        print('Invalid market code.')\n        return\n    if start is None:\n        start = datetime.date.today()\n    if end is None:\n        end = datetime.date.today()\n    start = ct.convert_date(start)\n    end = ct.convert_date(end)\n    df_list = []\n    for d in pd.date_range(start, end):\n        df = func(d.strftime('%Y%m%d'))\n        if not df.empty:\n            df_list.append(df)\n    if len(df_list) > 0:\n        return pd.concat(df_list)\n    else:\n        return pd.DataFrame()\n\n", "CodeBleu_score": 57.20574175009852}, {"code": "def get_future_daily(start = None, end = None, market = 'CFFEX')\n\tif start == None:\n        start = today\n    else:\n        start = ct.convert_date(start)\n    if end == None:\n        end = today\n    else:\n        end = ct.convert_date(end)\n\n    if market.upper() == 'CFFEX':\n        func = get_future_daily_CFFEX\n    elif market.upper() == 'SHFE':\n        func = get_future_daily_SHFE\n    elif market.upper() == 'DCE':\n        func = get_future_daily_DCE\n    elif market.upper() == 'CZCE':\n        func = get_future_daily_CZCE\n\n    df_list = []\n    for date in pd.date_range(start, end):\n        df = func(date.strftime('%Y%m%d'))\n        if not df.empty:\n            df_list.append(df)\n\n    if len(df_list) > 0:\n        return pd.concat(df_list)\n    else:\n        return pd.DataFrame()", "CodeBleu_score": 52.73531376360565}, {"code": "def get_future_daily(start = None, end = None, market = 'CFFEX')\n\tif market.upper() == 'CFFEX':\n        func = get_cffex_daily\n    elif market.upper() == 'SHFE':\n        func = get_shfe_daily\n    elif market.upper() == 'DCE':\n        func = get_dce_daily\n    elif market.upper() == 'CZCE':\n        func = get_czce_daily\n    else:\n        raise ValueError('Invalid market.')\n    if start is None:\n        start = datetime.date.today()\n    if end is None:\n        end = datetime.date.today()\n    start = ct.convert_date(start)\n    end = ct.convert_date(end)\n    df_list = []\n    for i in range((end - start).days + 1):\n        day = start + datetime.timedelta(days = i)\n        df = func(day)\n        if not df.empty:\n            df_list.append(df)\n    if len(df_list) > 0:\n        return pd.concat(df_list)\n    else:\n        return pd.DataFrame()", "CodeBleu_score": 57.04697106655057}]}
{"instance_id": "697779825988", "generation_result": [{"code": "def realtime_boxoffice(retry_count=3,pause=0.001)\n\t#\n    for i in range(retry_count):\n        try:\n            request = Request('http://www.kobis.or.kr/kobisopenapi/webservice/rest/boxoffice/searchDailyBoxOfficeList.json?key=24261416756f685367633231342363362666d693335696e6534376a6664363862323533333969656a6266373931366b3835363466313035333032393739656336373631323034363538343037313237323133393731323933333839323934323437333934313237323133393731323933333839323934323437333934313237323133393731323933333839323934323437333934313237323133393731323933333839323934323437333934313237323133393731323933333839323934323437333934313237323133393731323933333839323934323437333934313237323", "CodeBleu_score": 9.618336844246144}, {"code": "def realtime_boxoffice(retry_count=3,pause=0.001)\n\tfor i in range(retry_count):\n        try:\n            response = urlopen(Request('http://www.cgv.co.kr/common/showtimes/xml.movie.xml.php?key=430338241232&areacode=01&dt=20210121'))\n            lines = response.readlines()\n            data = lines[1]\n            data = data.decode('utf-8')\n            data = json.loads(data)\n            df = pd.DataFrame(data['data2'])\n            df.drop(['MovieImg','mId'],axis=1,inplace=True)\n            df['timestamp'] = du.get_now()\n            return df\n        except:\n            time.sleep(pause)\n            continue", "CodeBleu_score": 36.144190781375904}, {"code": "def realtime_boxoffice(retry_count=3,pause=0.001)\n\tfor _ in range(retry_count):\n        try:\n            response = urlopen('http://www.cgv.co.kr/common/showtimes/xml.json?key=43053844153624453', timeout=10)\n            if len(response.read()) > 0:\n                df = pd.DataFrame(json.loads(response.read().decode('utf-8'))['data2'])\n                df = df.drop(['MovieImg', 'mId'], axis=1)\n                df['timestamp'] = du.get_now()\n                return df\n            else:\n                time.sleep(pause)\n        except Exception as e:\n            print(e)\n            time.sleep(pause)\n\n", "CodeBleu_score": 43.19464311270466}]}
