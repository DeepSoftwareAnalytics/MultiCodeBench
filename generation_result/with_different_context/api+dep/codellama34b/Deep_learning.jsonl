{"instance_id": "1f9f4d85e825", "generation_result": [{"code": "def load_model_from_config(self, half_attention)\n\t# Check if a cached model is available and valid\n    cache_path = self.config_path.parent / 'cache'\n    cache_path.mkdir(exist_ok=True)\n    cache_file = cache_path / f'{self.config_path.stem}.pt'\n    if cache_file.exists() and cache_file.stat().st_mtime > self.config_path.stat().st_mtime:\n        print(f'Loading model from cache: {cache_file}')\n        model = torch.load(cache_file)\n    else:\n        # Load model from path\n        print(f'Loading model from path: {self.model_path}')\n        model = instantiate_from_config(self.config)\n        if self.model_path.suffix == '.safetensors':\n            # Load from SafeTensor file\n            print('Loading from SafeTensor file')\n            model.load_state_dict(torch.load(self.model_path))\n        else:\n            # Load from other PyTorch-compatible file\n            print('Loading from other PyTorch-compatible file')\n            model.load_state_dict(torch.load(self.model_path, map_location='cpu'))\n        # Move model to appropriate device\n        model.to(self.device)\n        # Configure model using settings from YAML file\n        model.eval()\n        if half_attention:\n            model.half_attention()\n        if self.memory_format:\n            model.memory_format_optimization(self.memory_format)\n        # Cache the loaded model for future use\n        torch.save(model, cache_file)\n    return model", "CodeBleu_score": 32.203931466036245}, {"code": "def load_model_from_config(self, half_attention)\n\tif self.cache_dir is not None:\n        # Check for cached model\n        model_path = os.path.join(self.cache_dir, self.model_name)\n        if os.path.exists(model_path):\n            # Load model from cache\n            print(f\"Loading model from cache: {model_path}\")\n            model = torch.load(model_path)\n            return model\n    # Load model from path\n    model = instantiate_from_config(self.config)\n    # Load state dict from path\n    model_path = self.model_path\n    if model_path.endswith(\".safetensors\"):\n        # Load state dict from SafeTensor file\n        print(f\"Loading model from SafeTensor file: {model_path}\")\n        model.load_state_dict(torch.load(model_path, map_location=\"cpu\"))\n    else:\n        # Load state dict from other PyTorch-compatible file\n        print(f\"Loading model from file: {model_path}\")\n        model.load_state_dict(torch.load(model_path, map_location=\"cpu\"))\n    # Move model to device\n    model.to(self.device)\n    # Move model to half-precision\n    if half_attention:\n        print(\"Using half-precision attention\")\n        model.half()\n    # Optimize memory format\n    if self.memory_format is not None:\n        print(f\"Optimizing memory format: {self.memory_format}\")\n        model.to(memory_format=self.memory_format)\n    # Ensure model is in evaluation mode\n    model.eval()\n    # Cache model\n    if self.cache_dir is not None:\n        print(f\"Caching model: {model_path}\")\n        torch.save(model, model_path)\n    return model\n\n", "CodeBleu_score": 37.30307320754779}, {"code": "def load_model_from_config(self, half_attention)\n\t# Check if the model is already cached\n    if self.use_cache:\n        cached_model = self.load_cached_model(self.model_path)\n        if cached_model is not None:\n            return cached_model\n\n    # Load the model from a file\n    model = self.load_model_from_file(self.model_path)\n\n    # Configure the model\n    model = self.configure_model(model, self.model_config_path)\n\n    # Move the model to the appropriate device\n    model = self.move_model_to_device(model)\n\n    # Use half-precision attention if specified\n    if half_attention:\n        model = self.use_half_precision_attention(model)\n\n    # Use memory format optimization if specified\n    if self.memory_format_opt:\n        model = self.use_memory_format_optimization(model)\n\n    # Ensure the model is in evaluation mode\n    model = self.use_evaluation_mode(model)\n\n    # Cache the loaded model\n    if self.use_cache:\n        self.cache_model(self.model_path, model)\n\n    # Return the model\n    return model\n\n", "CodeBleu_score": 23.048220430760757}]}
{"instance_id": "8dc955007aa2", "generation_result": [{"code": "def convsample_ddim(model, cond, steps, shape, eta=1.0, callback=None, normals_sequence=None,\n                    mask=None, x0=None, quantize_x0=False, temperature=1., score_corrector=None,\n                    corrector_kwargs=None, x_t=None\n                    )\n\tprint(f'eta: {eta}, steps: {steps}')\n    sampler = DDIMSampler(model, eta=eta)\n    batch_size = shape[0]\n    if normals_sequence is None:\n        normals_sequence = torch.randn(batch_size, *shape[1:], device=cond.device)\n    if mask is None:\n        mask = torch.ones(batch_size, *shape[1:], device=cond.device, dtype=torch.bool)\n    if x0 is None:\n        x0 = torch.zeros(batch_size, *shape[1:], device=cond.device)\n    if corrector_kwargs is None:\n        corrector_kwargs = {}\n    if x_t is None:\n        x_t = torch.zeros_like(x0)\n    samples, intermediates = sampler.sample(\n        steps,\n        shape=shape,\n        conditioning=cond,\n        callback=callback,\n        normals_sequence=normals_sequence,\n        mask=mask,\n        x0=x0,\n        quantize_x0=quantize_x0,\n        temperature=temperature,\n        score_corrector=score_corrector,\n        corrector_kwargs=corrector_kwargs,\n        x_t=x_t,\n    )\n    return samples, intermediates", "CodeBleu_score": 49.33948057639709}, {"code": "def convsample_ddim(model, cond, steps, shape, eta=1.0, callback=None, normals_sequence=None,\n                    mask=None, x0=None, quantize_x0=False, temperature=1., score_corrector=None,\n                    corrector_kwargs=None, x_t=None\n                    )\n\t\"\"\"\n    Args:\n        model (DiscreteDiffusion): The model to use for sampling.\n        cond (Tensor): A batch of conditioning variables.\n        steps (int): The number of diffusion steps to sample.\n        shape (Tuple[int, int, int, int]): The shape of the generated samples.\n        eta (float, optional): The amount of noise to add during sampling. Defaults to 1.0.\n        callback (Callable, optional): A callback to run at each diffusion step. Defaults to None.\n        normals_sequence (Tensor, optional): A batch of noise to use for sampling. Defaults to None.\n        mask (Tensor, optional): A batch of binary masks to apply to the generated samples. Defaults to None.\n        x0 (Tensor, optional): A batch of images to start sampling from. Defaults to None.\n        quantize_x0 (bool, optional): Whether to quantize the x0 values. Defaults to False.\n        temperature (float, optional): The temperature to use for sampling. Defaults to 1.0.\n        score_corrector (Callable, optional): A score corrector function. Defaults to None.\n        corrector_kwargs (Dict, optional): Arguments to pass to the score corrector. Defaults to None.\n        x_t (Tensor, optional): An intermediate image to use for sampling. Defaults to None.\n\n    Returns:\n        Tuple[Tensor, Tensor]: The generated samples and the logits of the samples.\n    \"\"\"\n    sampler = DDIMSampler(model, eta=eta, callback=callback, normals_sequence=normals_sequence,\n                          mask=mask, x0=x0, quantize_x0=quantize_x0, temperature=temperature,\n                          score_corrector=score_corrector, corrector_kwargs=corrector_kwargs, x_t=x_t)\n    batch_size = shape[0]\n    print(f'eta: {eta}, steps: {steps}')\n    samples, intermediates = sampler.sample(steps, batch_size, shape, cond=cond)\n    return samples, intermediates", "CodeBleu_score": 36.86202845704336}, {"code": "def convsample_ddim(model, cond, steps, shape, eta=1.0, callback=None, normals_sequence=None,\n                    mask=None, x0=None, quantize_x0=False, temperature=1., score_corrector=None,\n                    corrector_kwargs=None, x_t=None\n                    )\n\tprint(f'Running DDIM sampling with eta: {eta}, {steps} steps')\n    print(f'Output image shape: {shape}')\n    sampler = DDIMSampler(model, eta=eta)\n    batch_size = shape[0]\n    img = sampler.sample(steps, batch_size, shape, cond, callback=callback, normals_sequence=normals_sequence,\n                         mask=mask, x0=x0, quantize_x0=quantize_x0, temperature=temperature,\n                         score_corrector=score_corrector, corrector_kwargs=corrector_kwargs, x_t=x_t)\n    return img, sampler.intermediates\n\n", "CodeBleu_score": 46.529358032218035}]}
{"instance_id": "9bdcd9081480", "generation_result": [{"code": "def ema_scope(self, context=None)\n\tif not self.ema_weights:\n        return\n\n    if context is not None:\n        print(f'{context}: Using EMA weights.')\n    self.restore_weights()\n    self.store_weights()\n    self.copy_weights(self.ema_weights)\n\n    if context is not None:\n        print(f'{context}: Using EMA weights.')\n\n    def restore_weights():\n        if context is not None:\n            print(f'{context}: Restoring original weights.')\n        self.restore_weights()\n\n    return restore_weights\n\n", "CodeBleu_score": 51.76027093544664}, {"code": "def ema_scope(self, context=None)\n\tif context is None:\n        context = self.name\n    if self.ema_weights is None:\n        return\n    self.restore()\n    if context is not None:\n        print(f\"Switching to EMA weights for {context}\")\n    self.store()\n    self.copy_to(self.ema_weights)\n\n", "CodeBleu_score": 35.5369070243104}, {"code": "def ema_scope(self, context=None)\n\tif self.ema_weights is None:\n        return\n    if context is None:\n        context = self\n    if self.ema_context is None:\n        context.print(\"Using EMA weights.\")\n        self.ema_context = context\n        self.restore_weights()\n        self.store_weights(self.ema_weights)\n    else:\n        context.print(\"Using original weights.\")\n        self.ema_context = None\n        self.restore_weights()", "CodeBleu_score": 35.457596480741294}]}
{"instance_id": "14ab009e1c5c", "generation_result": [{"code": "def init_from_ckpt(self, path, ignore_keys=None)\n\tsd = torch.load(path)\n    if ignore_keys is not None:\n        for k in ignore_keys:\n            if k in sd:\n                del sd[k]\n    self.load_state_dict(sd)\n    missing_keys = []\n    unexpected_keys = []\n    error_msgs = []\n    # copy state_dict so _load_from_state_dict can modify it\n    metadata = getattr(sd, '_metadata', None)\n    state_dict = sd.copy()\n    if metadata is not None:\n        state_dict._metadata = metadata\n\n    def load(module, prefix=''):\n        local_metadata = {} if metadata is None else metadata.get(prefix[:-1], {})\n        module._load_from_state_dict(\n            state_dict, prefix, local_metadata, True, missing_keys, unexpected_keys, error_msgs)\n        for name, child in module._modules.items():\n            if child is not None:\n                load(child, prefix + name + '.')\n\n    load(self)\n    if len(unexpected_keys) > 0:\n        error_msgs.insert(\n            0, 'Unexpected key(s) in state_dict: {}. '.format(\n                ', '.join('\"{}\"'.format(k) for k in unexpected_keys)))\n    if len(missing_keys) > 0:\n        error_msgs.insert(\n            0, 'Missing key(s) in state_dict: {}. '.format(\n                ', '.join('\"{}\"'.format(k) for k in missing_keys)))\n    if len(error_msgs) > 0:\n        raise RuntimeError('Error(s) in loading state_dict for {}:", "CodeBleu_score": 29.58030935179946}, {"code": "def init_from_ckpt(self, path, ignore_keys=None)\n\tsd = torch.load(path, map_location=lambda storage, loc: storage)\n    if ignore_keys is not None:\n        for k in ignore_keys:\n            if k in sd:\n                del sd[k]\n    self.load_state_dict(sd, strict=False)\n    print(f\"Model loaded from {path}.\")\n    print(f\"Missing keys: {sd.keys() - self.state_dict().keys()}\")\n    print(f\"Unexpected keys: {self.state_dict().keys() - sd.keys()}\")", "CodeBleu_score": 23.481527316035564}, {"code": "def init_from_ckpt(self, path, ignore_keys=None)\n\tsd = torch.load(path, map_location='cpu')\n    if ignore_keys is not None:\n        for key in ignore_keys:\n            if key in sd:\n                del sd[key]\n    self.load_state_dict(sd, strict=False)\n    missing, unexpected = self.load_state_dict(sd, strict=False)\n    if len(missing) > 0:\n        print('MISSING KEYS:')\n        print(missing)\n    if len(unexpected) > 0:\n        print('UNEXPECTED KEYS:')\n        print(unexpected)\n\n", "CodeBleu_score": 32.79044783979944}]}
{"instance_id": "754176e27b81", "generation_result": [{"code": "def forward(self, input, return_pred_indices=False)\n\tquant, diff, info = self.encode(input)\n    dec = self.decode(quant)\n    if return_pred_indices:\n        return dec, diff, info\n    return dec, diff\n\n", "CodeBleu_score": 53.2991804414535}, {"code": "def forward(self, input, return_pred_indices=False)\n\tquant, diff, info = self.encode(input)\n    dec = self.decode(quant)\n    if return_pred_indices:\n        return dec, diff, info\n    else:\n        return dec, diff\n\n", "CodeBleu_score": 50.97775916518488}, {"code": "def forward(self, input, return_pred_indices=False)\n\tquant, diff, info = self.encode(input)\n    dec = self.decode(quant)\n    if return_pred_indices:\n        return dec, diff, info\n    else:\n        return dec, diff", "CodeBleu_score": 50.97775916518488}]}
{"instance_id": "443df6fdaf0d", "generation_result": [{"code": "def get_input(self, batch, k)\n\tx = batch[k]\n    if len(x.shape) == 3:\n        x = x[..., None]\n    x = x.permute(0, 3, 1, 2).contiguous().float()\n    if self.opt.resize_factors is not None:\n        x = F.interpolate(x, scale_factor=self.opt.resize_factors[self.training_step], mode='bicubic', align_corners=False)\n    return x.detach()", "CodeBleu_score": 36.407532510208476}, {"code": "def get_input(self, batch, k)\n\tx = batch[k]\n    if len(x.shape) == 3:\n        x = x[..., None]\n    x = x.permute(0, 3, 1, 2).float()\n    if self.resize:\n        size = int(self.size * self.rng(self.step))\n        x = F.interpolate(x, size=size, mode='bicubic', align_corners=False)\n    x = x.detach()\n    return x", "CodeBleu_score": 38.39803710064562}, {"code": "def get_input(self, batch, k)\n\tx = batch[k]\n    if x.ndim == 2:\n        x = x[:, :, None, None]\n    elif x.ndim == 3:\n        x = x.transpose(2, 0, 1)\n    elif x.ndim == 4:\n        x = x.transpose(0, 3, 1, 2)\n    else:\n        raise ValueError(f\"Wrong number of dimensions for key '{k}': {x.ndim}.\")\n    if self.resize_batch:\n        x = F.interpolate(x, size=int(self.batch_size * self.resize_factors[self.step]), mode='bicubic', align_corners=False)\n    x = x.float().contiguous().detach()\n    return x", "CodeBleu_score": 35.594720232881315}]}
{"instance_id": "953a2632fc15", "generation_result": [{"code": "def training_step(self, batch, batch_idx, optimizer_idx)\n\tx = self.get_input(batch, self.image_key)\n    x_hat = self(x)\n\n    if optimizer_idx == 0:\n        loss = self.loss(x_hat, x, optimizer_idx)\n        self.log_dict({f\"{self.loss_names[optimizer_idx]}_loss\": loss}, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n    elif optimizer_idx == 1:\n        loss = self.loss(x_hat, x, optimizer_idx)\n        self.log_dict({f\"{self.loss_names[optimizer_idx]}_loss\": loss}, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n    else:\n        raise NotImplementedError\n    return loss\n\n", "CodeBleu_score": 27.698439149207353}, {"code": "def training_step(self, batch, batch_idx, optimizer_idx)\n\tx, y = batch\n    x_hat = self(x)\n    if optimizer_idx == 0:\n        loss = F.mse_loss(x_hat, x)\n        self.log_dict({\"loss_ae\": loss})\n    else:\n        loss = -F.log_softmax(self.discriminator(x_hat), dim=1).mean(0)\n        self.log_dict({\"loss_disc\": loss})\n    return loss\n\n", "CodeBleu_score": 15.785561006972076}, {"code": "def training_step(self, batch, batch_idx, optimizer_idx)\n\tx, y = self.get_input(batch, \"x\"), self.get_input(batch, \"y\")\n    x_hat, z = self(x)\n    loss_ae = self.loss(x_hat, x, z)\n    loss_disc = self.loss(x_hat, y, z)\n    self.log_dict({\"loss_ae\": loss_ae, \"loss_disc\": loss_disc}, prog_bar=True)\n    return loss_ae if optimizer_idx == 0 else loss_disc", "CodeBleu_score": 17.920187231843272}]}
{"instance_id": "b449b5e8b77f", "generation_result": [{"code": "def log_images(self, batch, only_inputs=False, plot_ema=False, **kwargs)\n\tif not hasattr(self, \"ema_scope\"):\n        self.ema_scope = None\n    if plot_ema:\n        if self.ema_scope is None:\n            self.ema_scope = EMAScope(self)\n        self.ema_scope.load_ema_weights()\n    if self.global_step == 0:\n        self.log(\"inputs\", self.get_input(batch, self.image_key))\n        if not only_inputs:\n            self.log(\"reconstructions\", self.to_rgb(self.decode(self.encode(self.get_input(batch, self.image_key)))))\n        if plot_ema:\n            self.log(\"ema_reconstructions\", self.to_rgb(self.decode(self.encode(self.get_input(batch, self.image_key)))))\n        if self.ema_scope is not None:\n            self.ema_scope.load_params()\n    else:\n        self.log(\"inputs\", self.get_input(batch, self.image_key))\n        if not only_inputs:\n            self.log(\"reconstructions\", self.to_rgb(self.decode(self.encode(self.get_input(batch, self.image_key)))))\n        if plot_ema:\n            self.log(\"ema_reconstructions\", self.to_rgb(self.decode(self.encode(self.get_input(batch, self.image_key)))))\n        if self.ema_scope is not None:\n            self.ema_scope.load_params()\n", "CodeBleu_score": 23.70277810745689}, {"code": "def log_images(self, batch, only_inputs=False, plot_ema=False, **kwargs)\n\tif not self.log_input:\n        return\n    if self.global_step < self.log_start_step:\n        return\n    if self.global_step % self.log_interval != 0:\n        return\n    if self.global_batch % self.log_batch_interval != 0:\n        return\n    if self.global_epoch < self.log_start_epoch:\n        return\n    if self.global_epoch % self.log_epoch_interval != 0:\n        return\n    if self.global_step > self.log_end_step:\n        return\n    if self.global_epoch > self.log_end_epoch:\n        return\n\n    x = self.get_input(batch, self.image_key)\n    if self.global_step == self.log_start_step:\n        self.log_input_shape = x.shape\n    assert x.shape == self.log_input_shape, f\"{x.shape} != {self.log_input_shape}\"\n\n    if self.log_target is not None:\n        y = self.get_input(batch, self.log_target)\n        if self.global_step == self.log_start_step:\n            self.log_target_shape = y.shape\n        assert y.shape == self.log_target_shape, f\"{y.shape} != {self.log_target_shape}\"\n\n    if self.log_model is not None:\n        z = self.get_input(batch, self.log_model)\n        if self.global_step == self.log_start_step:\n            self.log_model_shape = z.shape\n        assert z.shape == self.log_model_shape, f\"{z.shape} != {self.log_model_shape}\"\n\n    if self.global_step == self.log_start_step:\n        self.log_input_channels = x.shape[1]\n        self.log_target_channels = y.shape[1] if self.log_target is not None else None\n        self.log_model_channels = z.shape[1", "CodeBleu_score": 24.885234557526125}, {"code": "def log_images(self, batch, only_inputs=False, plot_ema=False, **kwargs)\n\tif not self.global_step % self.log_interval:\n        self.model.eval()\n        with torch.no_grad():\n            x = self.get_input(batch, self.image_key)\n            if self.image_key == \"segmentation\" and not hasattr(self, \"colorize\"):\n                self.register_buffer(\"colorize\", torch.randn(3, x.shape[1], 1, 1).to(x))\n            if self.global_step == 0:\n                self.logger.experiment.add_image(\n                    f\"{self.image_key}_input\",\n                    make_grid(x, **self.make_grid_kwargs),\n                    self.global_step,\n                )\n            if plot_ema and self.model.uses_ema:\n                x_recon_ema = self.model(x, return_latents=False, ema=True)\n                if self.image_key == \"segmentation\":\n                    x_recon_ema = F.conv2d(\n                        x_recon_ema, weight=self.model.colorize\n                    )\n                    x_recon_ema = 2. * (\n                        x_recon_ema - x_recon_ema.min()\n                    ) / (x_recon_ema.max() - x_recon_ema.min()) - 1.\n                self.logger.experiment.add_image(\n                    f\"{self.image_key}_recon_ema\",\n                    make_grid(x_recon_ema, **self.make_grid_kwargs),\n                    self.global_step,\n                )\n            if not only_inputs:\n                x_recon = self.model(x, return_latents=False, ema=plot_ema)\n                if self.image_key == \"segmentation\":\n                    x_recon = F.conv2d(x_recon, weight=self.model.colorize)\n                    x_recon = 2. * (x_recon - x_recon.min()) / (\n                        x_re", "CodeBleu_score": 27.425361277735306}]}
{"instance_id": "540d089e3481", "generation_result": [{"code": "def get_loss(self, pred, target, mean=True)\n\tif self.loss_type == 'l1':\n        loss = torch.mean(torch.abs(pred - target)) if mean else torch.abs(pred - target)\n    elif self.loss_type == 'l2':\n        loss = torch.mean(torch.pow(pred - target, 2)) if mean else torch.pow(pred - target, 2)\n    else:\n        raise NotImplementedError(\"Unknown loss type\")\n    return loss\n\n\n\n\n", "CodeBleu_score": 43.301702542569195}, {"code": "def get_loss(self, pred, target, mean=True)\n\tif self.loss_type == 'l1':\n        if mean:\n            return torch.mean(torch.abs(pred - target))\n        else:\n            return torch.abs(pred - target)\n    elif self.loss_type == 'l2':\n        if mean:\n            return torch.mean((pred - target) ** 2)\n        else:\n            return (pred - target) ** 2\n    else:\n        raise NotImplementedError('Unknown loss type!')", "CodeBleu_score": 35.364229698012586}, {"code": "def get_loss(self, pred, target, mean=True)\n\tif self.loss_type == 'l1':\n        if mean:\n            return torch.mean(torch.abs(pred - target))\n        else:\n            return torch.abs(pred - target)\n    elif self.loss_type == 'l2':\n        if mean:\n            return torch.mean((pred - target) ** 2)\n        else:\n            return (pred - target) ** 2\n    else:\n        raise NotImplementedError(\"Unknown loss type!\")", "CodeBleu_score": 35.41256281229509}]}
{"instance_id": "63a08283666f", "generation_result": [{"code": "def forward(self, x, c, *args, **kwargs)\n\tif self.training:\n        return self.training_step(x, c, *args, **kwargs)\n    else:\n        return self.validation_step(x, c, *args, **kwargs)\n", "CodeBleu_score": 12.120985159213891}, {"code": "def forward(self, x, c, *args, **kwargs)\n\t# TODO: add support for multiple time steps\n    # TODO: add support for multiple conditioning inputs\n    # TODO: add support for multiple conditioning types\n\n    # get the time step for the current forward pass\n    t = self.get_current_time_step(x)\n\n    # get the learned conditioning for the current time step\n    c = self.get_learned_conditioning(c)\n\n    # get the noise for the current time step\n    noise = self.get_noise(x, t)\n\n    # apply the model's conditioning requirements\n    x = self.apply_conditioning_to_input(x, c, t)\n\n    # compute the loss\n    loss, loss_dict = self.p_losses(x_start=x, t=t, noise=noise)\n\n    return loss, loss_dict\n\n", "CodeBleu_score": 12.860033100707192}, {"code": "def forward(self, x, c, *args, **kwargs)\n\t# get conditioning\n    c = self.get_learned_conditioning(c)\n\n    # get time steps\n    t = self.get_t(x.shape[0])\n\n    # get noise\n    noise = self.get_noise(x, t)\n\n    # compute loss\n    loss, loss_dict = self.p_losses(x_start=x, t=t, noise=noise)\n\n    # add conditioning to loss dict\n    loss_dict.update({'cond': c})\n\n    return loss, loss_dict\n\n", "CodeBleu_score": 10.865091850419953}]}
{"instance_id": "4627e1e8ed62", "generation_result": [{"code": "def get_codebook_entry(self, indices, shape)\n\tif self.remap_to_all:\n        indices = self.unmap_to_all(indices)\n    z_q = self.embedding(indices)\n    if self.permutation is not None:\n        z_q = z_q.view(z_q.shape[0], -1, self.z_channels).permute(0, 2, 1)\n        z_q = z_q.reshape(z_q.shape[0], -1, *shape[1:])\n    else:\n        z_q = z_q.view(z_q.shape[0], *shape[1:], self.z_channels).permute(0, 3, 1, 2)\n    return z_q\n\n\n", "CodeBleu_score": 54.12395618391521}, {"code": "def get_codebook_entry(self, indices, shape)\n\tif self.re_embed:\n        indices = self.unmap_to_all(indices)\n    z_q = self.embedding(indices)\n    z_q = z_q.view(shape)\n    z_q = z_q.permute(0, 3, 1, 2).contiguous()\n    return z_q", "CodeBleu_score": 45.00655406929634}, {"code": "def get_codebook_entry(self, indices, shape)\n\tassert len(shape) == 2\n    assert shape[1] == self.embedding_dim\n    if self.remap is not None:\n        indices = self.remap.new(indices)\n        indices[indices >= self.used] = 0\n    z_q = self.embedding[indices].contiguous().view(shape)\n    if self.permutation is not None:\n        z_q = z_q.permute(self.permutation)\n    return z_q", "CodeBleu_score": 45.78529718929376}]}
{"instance_id": "c55d014827ef", "generation_result": [{"code": "def factorization(dimension: int, factor:int=-1) -> tuple[int, int]\n\tif factor < 0:\n        if dimension % 2 == 0:\n            n, m = 2, dimension // 2\n        else:\n            n, m = 1, dimension\n        while n < m:\n            if m % n == 0:\n                return m // n, n\n            else:\n                n += 1\n        return m, 1\n    else:\n        if factor == 0:\n            return 0, 0\n        if dimension % factor == 0:\n            return dimension // factor, factor\n        else:\n            raise ValueError(f\"Cannot factorize dimension {dimension} with factor {factor}\")", "CodeBleu_score": 36.267896947309346}, {"code": "def factorization(dimension: int, factor:int=-1) -> tuple[int, int]\n\tif factor < 0:\n        factor = -factor\n        m = dimension\n        n = 1\n        while n < m:\n            if m % n == 0 and m // n <= factor:\n                return m // n, n\n            n += 1\n        return m, 1\n    else:\n        if dimension % factor == 0:\n            return factor, dimension // factor\n        else:\n            raise ValueError(f\"Dimension {dimension} is not divisible by factor {factor}.\")", "CodeBleu_score": 33.02625398788008}, {"code": "def factorization(dimension: int, factor:int=-1) -> tuple[int, int]\n\tif factor > 0:\n        if dimension % factor == 0:\n            return factor, dimension // factor\n        else:\n            raise ValueError(f\"The factor {factor} does not divide the dimension {dimension}.\")\n    else:\n        if factor == 0:\n            raise ValueError(\"The factor cannot be 0.\")\n\n        if factor == -1:\n            m = dimension\n            n = 1\n            min_sum = m + n\n            for i in range(1, int(dimension**0.5) + 1):\n                if dimension % i == 0:\n                    if i + dimension // i < min_sum:\n                        m = i\n                        n = dimension // i\n                        min_sum = m + n\n            return m, n\n        else:\n            m = dimension\n            n = 1\n            min_sum = m + n\n            for i in range(1, int(dimension**0.5) + 1):\n                if dimension % i == 0:\n                    if i + dimension // i < min_sum and i <= -factor:\n                        m = i\n                        n = dimension // i\n                        min_sum = m + n\n            return m, n", "CodeBleu_score": 34.28120633182892}]}
{"instance_id": "5d613ec26036", "generation_result": [{"code": "def apply_weight_decompose(self, updown, orig_weight)\n\tif updown == 'up':\n        merged_weight = torch.add(orig_weight, updown)\n        merged_weight = merged_weight.to(device=orig_weight.device, dtype=orig_weight.dtype)\n        merged_weight = merged_weight.transpose(1, 0)\n        merged_weight = merged_weight.reshape(merged_weight.shape[0], -1)\n        merged_weight = F.normalize(merged_weight)\n        merged_weight = merged_weight.reshape(orig_weight.shape[1], orig_weight.shape[0])\n        merged_weight = merged_weight.transpose(1, 0)\n        merged_weight = merged_weight.reshape(orig_weight.shape)\n        adjusted_weight = merged_weight - orig_weight\n    elif updown == 'down':\n        merged_weight = torch.add(orig_weight, updown)\n        merged_weight = merged_weight.to(device=orig_weight.device, dtype=orig_weight.dtype)\n        merged_weight = merged_weight.transpose(1, 0)\n        merged_weight = merged_weight.reshape(merged_weight.shape[0], -1)\n        merged_weight = F.normalize(merged_weight)\n        merged_weight = merged_weight.reshape(orig_weight.shape[1], orig_weight.shape[0])\n        merged_weight = merged_weight.transpose(1, 0)\n        merged_weight = merged_weight.reshape(orig_weight.shape)\n        adjusted_weight = orig_weight - merged_weight\n    else:\n        raise Exception(\"updown must be 'up' or 'down'\")\n    return adjusted_weight", "CodeBleu_score": 40.23251472853092}, {"code": "def apply_weight_decompose(self, updown, orig_weight)\n\tassert updown.device == orig_weight.device, \"The input weight and the original weight must be on the same device.\"\n    assert updown.dtype == orig_weight.dtype, \"The input weight and the original weight must have the same data type.\"\n    merged_weight = updown + orig_weight\n    merged_weight = merged_weight.to(torch.float32)\n    merged_weight = merged_weight.reshape(merged_weight.shape[0], -1)\n    merged_weight = merged_weight.transpose(0, 1)\n    merged_weight = merged_weight.norm(dim=1)\n    merged_weight = merged_weight.reshape(1, -1)\n    merged_weight = merged_weight.transpose(0, 1)\n    merged_weight = merged_weight * self.scale_factor\n    merged_weight = merged_weight.to(torch.float32)\n    adjusted_weight = merged_weight - orig_weight\n    return adjusted_weight", "CodeBleu_score": 37.2334067097997}, {"code": "def apply_weight_decompose(self, updown, orig_weight)\n\t# Ensure all tensors are on the same device\n    if updown.device != orig_weight.device:\n        updown = updown.to(orig_weight.device)\n    # Ensure all tensors are of the same data type\n    if updown.dtype != orig_weight.dtype:\n        updown = updown.type(orig_weight.dtype)\n    # Ensure all tensors are of the same shape\n    if updown.shape != orig_weight.shape:\n        updown = updown.reshape(orig_weight.shape)\n\n    # Decompose the weight\n    weight = updown - orig_weight\n\n    # Normalize the weight\n    weight = weight / torch.max(weight)\n\n    # Scale the weight\n    weight = weight * self.scale_factor\n\n    # Normalize the weight\n    weight = weight / torch.max(weight)\n\n    # Adjust the weight\n    weight = orig_weight - weight\n\n    return weight", "CodeBleu_score": 29.867749695946134}]}
{"instance_id": "c73d0d1cd3e5", "generation_result": [{"code": "def finalize_updown(self, updown, orig_weight, output_shape, ex_bias=None)\n\t# Match the device/dtype\n    orig_weight = orig_weight.to(updown.dtype)\n    updown = updown.to(orig_weight.device)\n    if ex_bias is not None:\n        ex_bias = ex_bias.to(orig_weight.device)\n\n    # Reshape the bias\n    if ex_bias is not None:\n        if len(output_shape) == 4:\n            ex_bias = ex_bias.reshape(output_shape[1], 1, 1)\n        elif len(output_shape) == 2:\n            ex_bias = ex_bias.reshape(output_shape[1], 1)\n        else:\n            raise ValueError(f\"Unsupported output shape: {output_shape}\")\n\n    # Reshape updown\n    if updown.numel() == orig_weight.numel():\n        updown = updown.reshape(orig_weight.shape)\n    else:\n        updown = updown.reshape(output_shape)\n\n    # Apply the multiplier\n    if ex_bias is not None:\n        ex_bias = ex_bias * self.multiplier\n\n    # Apply the weight decomposition\n    updown = self.apply_weight_decompose(updown, orig_weight)\n\n    return updown, ex_bias\n\n", "CodeBleu_score": 54.708140129156966}, {"code": "def finalize_updown(self, updown, orig_weight, output_shape, ex_bias=None)\n\t# Apply bias\n    if ex_bias is not None:\n        ex_bias = ex_bias.to(updown.dtype)\n        updown = updown * self.multiplier(self) + ex_bias\n    else:\n        updown = updown * self.multiplier(self)\n\n    # Reshape updown to match output shape\n    if updown.numel() == orig_weight.numel():\n        updown = updown.reshape(orig_weight.shape)\n    else:\n        updown = updown.reshape(output_shape)\n\n    # Apply weight decomposition\n    if self.calc_scale(self) != 1.0:\n        updown = self.apply_weight_decompose(updown, orig_weight)\n\n    return updown, ex_bias\n\n", "CodeBleu_score": 49.46877933748832}, {"code": "def finalize_updown(self, updown, orig_weight, output_shape, ex_bias=None)\n\tif ex_bias is not None:\n        ex_bias = ex_bias.to(updown.dtype)\n        updown = updown * ex_bias\n\n    if orig_weight.numel() == updown.numel():\n        updown = updown.reshape(orig_weight.shape)\n    else:\n        updown = updown.reshape(output_shape)\n\n    if self.multiplier() != 1.0:\n        updown = self.apply_weight_decompose(updown, orig_weight)\n    return updown, ex_bias", "CodeBleu_score": 43.38339864541722}]}
{"instance_id": "c6a41665c62c", "generation_result": [{"code": "def calc_updown(self, orig_weight)\n\torig_weight = orig_weight.to(self.weight.device, dtype=self.weight.dtype)\n    updown = self.weight - orig_weight\n    updown = self.finalize_updown(updown, orig_weight, self.weight.shape, self.bias)\n    return updown\n\n", "CodeBleu_score": 35.29756314295308}, {"code": "def calc_updown(self, orig_weight)\n\tif self.weight is None:\n        return orig_weight\n\n    if self.bias is not None:\n        self.bias = self.bias.to(orig_weight.device, dtype=orig_weight.dtype)\n\n    if self.weight.size().numel() == orig_weight.size().numel():\n        updown = self.weight\n    else:\n        updown = self.weight.reshape(orig_weight.shape)\n\n    if self.dora_scale is not None:\n        updown = self.apply_weight_decompose(updown, orig_weight)\n\n    updown = updown * self.calc_scale() * self.multiplier()\n\n    return updown", "CodeBleu_score": 43.513022628598044}, {"code": "def calc_updown(self, orig_weight)\n\tif self.weight.size() != orig_weight.size():\n        raise ValueError('The shape of the current weight tensor does not match the shape of the original weight tensor.')\n\n    if self.bias is not None:\n        if self.bias.size() != self.orig_bias.size():\n            raise ValueError('The shape of the current bias tensor does not match the shape of the original bias tensor.')\n\n    orig_weight = orig_weight.to(self.weight.device, dtype=self.weight.dtype)\n    if self.bias is not None:\n        self.orig_bias = self.orig_bias.to(self.weight.device, dtype=self.weight.dtype)\n\n    updown = self.weight - orig_weight\n    ex_bias = None\n\n    if self.bias is not None:\n        ex_bias = self.bias - self.orig_bias\n\n    return self.finalize_updown(updown, orig_weight, self.weight.shape, ex_bias)", "CodeBleu_score": 43.262439418088725}]}
{"instance_id": "1bb9e5d94af8", "generation_result": [{"code": "def purge_networks_from_memory()\n\twhile len(networks_in_memory) > max_networks_in_memory:\n        networks_in_memory.pop(0)\n    gc.collect()", "CodeBleu_score": 16.821372662213868}, {"code": "def purge_networks_from_memory()\n\tif len(networks_in_memory) > MAX_NETWORKS_IN_MEMORY:\n        while len(networks_in_memory) > MAX_NETWORKS_IN_MEMORY:\n            networks_in_memory.pop(0)\n        gc.collect()", "CodeBleu_score": 33.03565716104556}, {"code": "def purge_networks_from_memory()\n\twhile len(networks_in_memory) > networks_in_memory_limit:\n        networks_in_memory.pop(0)\n    gc.collect()\n\n\n\n\n", "CodeBleu_score": 17.45934741179273}]}
{"instance_id": "42b48ebf4d07", "generation_result": [{"code": "def network_restore_weights_from_backup(self: Union[torch.nn.Conv2d, torch.nn.Linear, torch.nn.GroupNorm, torch.nn.LayerNorm, torch.nn.MultiheadAttention]):\n\tif self.backup_weights is not None:\n        self.weight.data.copy_(self.backup_weights)\n    if self.backup_bias is not None:\n        self.bias.data.copy_(self.backup_bias)\n    else:\n        self.bias = None\n    if isinstance(self, torch.nn.MultiheadAttention):\n        if self.backup_in_proj_weights is not None:\n            self.in_proj_weight.data.copy_(self.backup_in_proj_weights)\n        if self.backup_in_proj_biases is not None:\n            self.in_proj_bias.data.copy_(self.backup_in_proj_biases)\n        else:\n            self.in_proj_bias = None", "CodeBleu_score": 34.371797245406555}, {"code": "def network_restore_weights_from_backup(self: Union[torch.nn.Conv2d, torch.nn.Linear, torch.nn.GroupNorm, torch.nn.LayerNorm, torch.nn.MultiheadAttention]):\n\tif hasattr(self, \"weight_backup\") and hasattr(self, \"bias_backup\"):\n        self.weight = self.weight_backup\n        self.bias = self.bias_backup\n    elif hasattr(self, \"weight_backup\"):\n        self.weight = self.weight_backup\n        self.bias = None\n    else:\n        print(\"Warning: No backup weights found for layer:\", self)\n\n", "CodeBleu_score": 22.647039311246537}, {"code": "def network_restore_weights_from_backup(self: Union[torch.nn.Conv2d, torch.nn.Linear, torch.nn.GroupNorm, torch.nn.LayerNorm, torch.nn.MultiheadAttention]):\n\tif self.backup_weights is not None:\n        self.weight = self.backup_weights.clone().detach()\n    if self.backup_bias is not None:\n        self.bias = self.backup_bias.clone().detach()\n    else:\n        self.bias = None\n    if isinstance(self, torch.nn.MultiheadAttention):\n        if self.backup_in_proj_weight is not None:\n            self.in_proj_weight = self.backup_in_proj_weight.clone().detach()\n        if self.backup_in_proj_bias is not None:\n            self.in_proj_bias = self.backup_in_proj_bias.clone().detach()\n        else:\n            self.in_proj_bias = None", "CodeBleu_score": 36.72806005536723}]}
{"instance_id": "c8fcd357131a", "generation_result": [{"code": "def network_forward(org_module, input, original_forward)\n\tif len(self.networks) == 0:\n        return original_forward(input)\n\n    input = input.to(self.device)\n    network_reset_cached_weight(self.sd_module)\n\n    output = original_forward(input)\n\n    for network in self.networks:\n        if network.network_current_names == self.network_current_names:\n            continue\n\n        network.network_current_names = self.network_current_names\n        network_reset_cached_weight(network.sd_module)\n        output = network(output)\n\n    return output\n\n", "CodeBleu_score": 34.79860592351054}, {"code": "def network_forward(org_module, input, original_forward)\n\tif len(org_module.networks) == 0:\n        return original_forward(input)\n\n    device = next(org_module.parameters()).device\n    input = input.to(device)\n    org_module.network_reset_cached_weight()\n    output = original_forward(input)\n\n    for network in org_module.networks:\n        if network.current_name in org_module.network_current_names:\n            continue\n\n        if not network.matches(org_module):\n            continue\n\n        network.apply(output)\n        org_module.network_current_names += (network.current_name,)\n\n    return output\n\n", "CodeBleu_score": 38.734434274554914}, {"code": "def network_forward(org_module, input, original_forward)\n\tif len(devices.loaded_networks) == 0:\n        return original_forward(input)\n\n    input = input.to(devices.current_device)\n    network_restore_weights_from_backup(org_module)\n\n    output = original_forward(input)\n    for network in devices.loaded_networks:\n        if network.layer_name == org_module.network_current_names:\n            output = network.forward(output)\n\n    return output\n\n", "CodeBleu_score": 33.24801606355132}]}
{"instance_id": "1070211f5c11", "generation_result": [{"code": "def trainables(self)\n\ttrainables = []\n    for name, param in self.named_parameters():\n        if 'weight' in name or 'bias' in name:\n            trainables.append(param)\n    return trainables", "CodeBleu_score": 24.972383791562}, {"code": "def trainables(self)\n\ttrainables = []\n    for name, module in self.named_modules():\n        if isinstance(module, torch.nn.Linear) or isinstance(module, torch.nn.LayerNorm):\n            trainables.append(module)\n    return trainables\n\n\n\n\n\n", "CodeBleu_score": 34.60987303524293}, {"code": "def trainables(self)\n\treturn [p for p in self.parameters() if p.requires_grad]\n\n\n\n\n\n", "CodeBleu_score": 6.194787406674239}]}
{"instance_id": "0c3ba62c96a7", "generation_result": [{"code": "def load_hypernetworks(names, multipliers=None)\n\tif multipliers is None:\n        multipliers = [None] * len(names)\n\n    if len(multipliers) != len(names):\n        errors.report(f\"The number of multipliers ({len(multipliers)}) does not match the number of hypernetworks ({len(names)})\", error=ValueError)\n\n    already_loaded = []\n\n    for name, multiplier in zip(names, multipliers):\n        if name in shared.hypernetworks:\n            already_loaded.append(name)\n            continue\n\n        hypernetwork = load_hypernetwork(name)\n\n        if hypernetwork is None:\n            continue\n\n        if multiplier is not None:\n            hypernetwork.set_multiplier(multiplier)\n\n        shared.hypernetworks[name] = hypernetwork\n\n    return already_loaded\n\n", "CodeBleu_score": 36.815974650004804}, {"code": "def load_hypernetworks(names, multipliers=None)\n\tif multipliers is None:\n        multipliers = [None] * len(names)\n\n    already_loaded = shared.loaded_hypernetworks.get()\n    shared.loaded_hypernetworks.clear()\n\n    for name, multiplier in zip(names, multipliers):\n        if name in already_loaded:\n            hypernetwork = already_loaded.get(name)\n        else:\n            hypernetwork = load_hypernetwork(name)\n\n        if hypernetwork is not None:\n            if multiplier is not None:\n                hypernetwork.set_multiplier(multiplier)\n\n            shared.loaded_hypernetworks.append(hypernetwork)\n\n    return shared.loaded_hypernetworks.get()\n\n", "CodeBleu_score": 42.24963989304352}, {"code": "def load_hypernetworks(names, multipliers=None)\n\tif multipliers is None:\n        multipliers = [None] * len(names)\n\n    already_loaded = shared.hypernetworks_loaded\n\n    clear()\n\n    for name, multiplier in zip(names, multipliers):\n        hypernetwork = load_hypernetwork(name)\n\n        if hypernetwork is None:\n            continue\n\n        if multiplier is not None:\n            hypernetwork.set_multiplier(multiplier)\n\n        already_loaded.append(hypernetwork)\n\n", "CodeBleu_score": 32.96307216475448}]}
{"instance_id": "bea3772ed532", "generation_result": [{"code": "\ndef apply_hypernetworks(hypernetworks, context, layer=None)\n\tcontext_k, context_v = context, context\n    for hypernetwork in hypernetworks:\n        context_k, context_v = apply_single_hypernetwork(hypernetwork, context_k, context_v, layer=layer)\n    return context_k, context_v\n\n", "CodeBleu_score": 54.97795636711834}, {"code": "\ndef apply_hypernetworks(hypernetworks, context, layer=None)\n\tcontext_k, context_v = context, context\n    for hypernetwork in hypernetworks:\n        context_k, context_v = apply_single_hypernetwork(hypernetwork, context_k, context_v, layer)\n    return context_k, context_v", "CodeBleu_score": 69.97333483627517}, {"code": "\ndef apply_hypernetworks(hypernetworks, context, layer=None)\n\tcontext_k, context_v = context, context\n\n    for hypernetwork in hypernetworks:\n        context_k, context_v = apply_single_hypernetwork(hypernetwork, context_k, context_v, layer=layer)\n\n    return context_k, context_v", "CodeBleu_score": 54.97795636711834}]}
{"instance_id": "922efa3e092f", "generation_result": [{"code": "def attention_CrossAttention_forward(self, x, context=None, mask=None, **kwargs)\n\t# x: [batch_size, seq_len, dim]\n    # context: [batch_size, seq_len, dim]\n    # mask: [batch_size, seq_len]\n    # self.to_q: Linear(in_features=256, out_features=256, bias=True)\n    # self.to_k: Linear(in_features=256, out_features=256, bias=True)\n    # self.to_v: Linear(in_features=256, out_features=256, bias=True)\n    # self.to_out: Linear(in_features=256, out_features=256, bias=True)\n    # self.softmax: Softmax(dim=-1)\n    # self.masked_fill_: Tensor.masked_fill_\n    # self.finfo: torch.finfo\n    # self.hypernetworks: []\n    # self.num_heads: 4\n    # self.head_dim: 64\n    # self.dropout: Dropout(p=0.1, inplace=False)\n    # self.training: True\n    # self.layer: 0\n    # self.attention_dropout: Dropout(p=0.1, inplace=False)\n    # self.attention_dropout.p: 0.1\n    # self.attention_dropout.training: True\n    # self.attention_dropout.inplace: False\n    # self.attention_dropout.p: 0.1\n    # self.attention_dropout.training: True\n    # self.attention_dropout.inplace: False\n    # self.attention_dropout.p: 0.1\n    # self.attention_dropout.training: True\n    # self.attention_dropout.inplace: False\n    # self.attention_dropout.p: 0.1\n    # self.attention_dropout.training: True\n    # self.attention_dropout.inplace: False\n    # self.attention_dropout.p", "CodeBleu_score": 1.9834764933446987}, {"code": "def attention_CrossAttention_forward(self, x, context=None, mask=None, **kwargs)\n\t# Compute query, key, and value projections\n    query = self.to_q(x)\n    key = self.to_k(x)\n    value = self.to_v(x)\n\n    # Apply hypernetworks\n    if context is not None:\n        context_k, context_v = apply_hypernetworks(self.hypernetworks, context)\n    else:\n        context_k, context_v = None, None\n\n    # Reshape for multi-head attention\n    query = rearrange(query, \"b s (h d) -> b h s d\", h=self.heads)\n    key = rearrange(key, \"b s (h d) -> b h s d\", h=self.heads)\n    value = rearrange(value, \"b s (h d) -> b h s d\", h=self.heads)\n    if context_k is not None:\n        context_k = rearrange(context_k, \"b s (h d) -> b h s d\", h=self.heads)\n    if context_v is not None:\n        context_v = rearrange(context_v, \"b s (h d) -> b h s d\", h=self.heads)\n\n    # Compute scaled dot-product attention\n    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.head_dim)\n\n    # Apply mask\n    if mask is not None:\n        scores = scores.masked_fill_(mask == 0, -1e9)\n\n    # Apply attention dropout\n    scores = self.dropout(scores)\n\n    # Compute attention weights\n    attn = torch.softmax(scores, dim=-1)\n\n    # Apply value projections\n    out = torch.matmul(attn, value)\n\n    # Concatenate heads\n    out = rearrange(out, \"b h s d -> b s (h d)\")\n\n    # Pass through final linear layer\n    out = self.to_out(out)\n\n    return out\n\n", "CodeBleu_score": 41.67064308024654}, {"code": "def attention_CrossAttention_forward(self, x, context=None, mask=None, **kwargs)\n\t# x: (batch_size, seq_len, d_model)\n    # context: (batch_size, seq_len, d_model)\n    # mask: (batch_size, seq_len, seq_len)\n\n    # Project query, key, and value\n    q = self.to_q(x)\n    k = self.to_k(x)\n    v = self.to_v(x)\n\n    # Apply hypernetworks to the context\n    if self.hypernetworks:\n        context_k, context_v = apply_hypernetworks(self.hypernetworks, context, self)\n    else:\n        context_k = context\n        context_v = context\n\n    # Reshape for multi-head attention\n    q = q.view(q.size(0), -1, self.n_heads, self.d_k).transpose(1, 2)\n    k = k.view(k.size(0), -1, self.n_heads, self.d_k).transpose(1, 2)\n    v = v.view(v.size(0), -1, self.n_heads, self.d_v).transpose(1, 2)\n    context_k = context_k.view(context_k.size(0), -1, self.n_heads, self.d_k).transpose(1, 2)\n    context_v = context_v.view(context_v.size(0), -1, self.n_heads, self.d_v).transpose(1, 2)\n\n    # Scaled dot-product attention\n    attn_scores = torch.matmul(q, context_k.transpose(-2, -1)) / math.sqrt(self.d_k)\n\n    # Apply mask\n    if mask is not None:\n        attn_scores = attn_scores.masked_fill_(mask == 0, -1e9)\n\n    # Softmax\n    attn_scores = sim.softmax(attn_scores, dim=-1)\n\n    # Dropout\n    if self", "CodeBleu_score": 34.869648305872545}]}
{"instance_id": "8b7455a03a0e", "generation_result": [{"code": "def save_hypernetwork(hypernetwork, checkpoint, hypernetwork_name, filename)\n\tif hypernetwork.sd_checkpoint_name is not None:\n        raise ValueError(\"Hypernetwork already has a saved checkpoint.\")\n\n    hypernetwork.sd_checkpoint = checkpoint\n    hypernetwork.sd_checkpoint_name = hypernetwork_name\n    hypernetwork.save(filename)\n    hypernetwork.sd_checkpoint = None\n    hypernetwork.sd_checkpoint_name = None\n\n", "CodeBleu_score": 25.607100041297326}, {"code": "def save_hypernetwork(hypernetwork, checkpoint, hypernetwork_name, filename)\n\tif checkpoint is None:\n        return\n\n    # Backup the original attributes\n    original_checkpoint = hypernetwork.sd_checkpoint\n    original_checkpoint_name = hypernetwork.sd_checkpoint_name\n\n    # Update the hypernetwork with the new checkpoint data\n    hypernetwork.sd_checkpoint = checkpoint\n    hypernetwork.sd_checkpoint_name = hypernetwork_name\n\n    try:\n        hypernetwork.save(filename)\n    except Exception as e:\n        # Restore the original attributes\n        hypernetwork.sd_checkpoint = original_checkpoint\n        hypernetwork.sd_checkpoint_name = original_checkpoint_name\n\n        # Re-raise the exception\n        raise e\n\n    # Restore the original attributes\n    hypernetwork.sd_checkpoint = original_checkpoint\n    hypernetwork.sd_checkpoint_name = original_checkpoint_name\n\n", "CodeBleu_score": 40.260639888265736}, {"code": "def save_hypernetwork(hypernetwork, checkpoint, hypernetwork_name, filename)\n\thypernetwork.sd_checkpoint = checkpoint\n    hypernetwork.sd_checkpoint_name = hypernetwork_name\n    hypernetwork.save(filename)\n    hypernetwork.sd_checkpoint = None\n    hypernetwork.sd_checkpoint_name = None\n\n", "CodeBleu_score": 8.60951361695084}]}
{"instance_id": "3175f835140f", "generation_result": [{"code": "def ema_scope(self, context=None)\n\tif context is None:\n        context = self\n\n    if self.ema_enabled:\n        self.ema_store()\n        self.ema_copy_to()\n        if self.ema_logging:\n            print(\"EMA weights copied to model.\")\n\n    try:\n        yield\n    finally:\n        if self.ema_enabled:\n            self.ema_restore()\n            if self.ema_logging:\n                print(\"Original weights restored.\")\n\n\n\n\n", "CodeBleu_score": 35.58190732052533}, {"code": "def ema_scope(self, context=None)\n\tif context is None:\n        context = self.context\n    if not context.ema:\n        return\n    if not context.ema_m:\n        raise ValueError(\"ema_m is not defined\")\n    if context.ema_w is None:\n        context.ema_w = torch.zeros_like(self.parameters())\n    if context.ema_w.device != self.parameters().device:\n        context.ema_w = context.ema_w.to(self.parameters().device)\n    context.ema_w.copy_(self.parameters())\n    self.copy_to(context.ema_w)\n    if context.verbose:\n        print(f\"ema_scope: using EMA weights\")\n    try:\n        yield\n    finally:\n        self.copy_to(context.ema_w)\n        if context.verbose:\n            print(f\"ema_scope: restored original weights\")\n\n\n", "CodeBleu_score": 37.97563820237295}, {"code": "def ema_scope(self, context=None)\n\tif context is None:\n        context = self\n    if context.ema_enabled:\n        context.ema.store()\n        context.ema.copy_to()\n        if context.ema.log:\n            print('EMA weights copied to model')\n\n    try:\n        yield\n    finally:\n        if context.ema_enabled:\n            context.ema.restore()\n            if context.ema.log:\n                print('Model weights restored')", "CodeBleu_score": 40.35027145882829}]}
{"instance_id": "a7b5d5cd519a", "generation_result": [{"code": "def p_mean_variance(self, x, t, clip_denoised: bool)\n\t# model_output = self.model(x, t)\n    # if self.model_output_size == 2:\n    #     model_mean, model_log_variance = model_output[:, :, :3], model_output[:, :, 3]\n    # else:\n    #     model_mean, model_log_variance = model_output[:, :, :128], model_output[:, :, 128:]\n    # model_log_variance = self.log_variance_clipping(model_log_variance)\n    # if self.model_output_size == 2:\n    #     noise = x\n    # else:\n    #     noise = model_mean\n    # if self.predict_x_start_from_noise:\n    #     x_recon = self.predict_start_from_noise(x_start=noise, t=t, noise=noise)\n    # else:\n    #     x_recon = noise\n    # if clip_denoised:\n    #     x_recon.clamp_(-1., 1.)\n    # model_mean, model_variance, model_log_variance = self.q_posterior(x_start=x_recon, x_t=x, t=t)\n    # return model_mean, model_variance, model_log_variance\n\n", "CodeBleu_score": 4.788306451612903}, {"code": "def p_mean_variance(self, x, t, clip_denoised: bool)\n\tx_recon = self.predict_xstart_from_noise(x, t)\n    if clip_denoised:\n        x_recon.clamp_(-1.0, 1.0)\n\n    model_mean, posterior_variance, posterior_log_variance_clipped = self.q_posterior(x_start=x_recon, x_t=x, t=t)\n    return model_mean, posterior_variance, posterior_log_variance_clipped\n\n", "CodeBleu_score": 29.324055429523792}, {"code": "def p_mean_variance(self, x, t, clip_denoised: bool)\n\tassert x.shape[1] == self.ch\n    model_output = self.model(x, t)\n    if self.model_output_size == 2:\n        assert len(model_output) == 2\n        mean, log_variance = model_output\n        posterior_variance = F.softplus(log_variance)\n    else:\n        assert len(model_output) == 3\n        _, _, posterior_variance = model_output\n    if clip_denoised:\n        x_recon = self.predict_start_from_noise(x_start=mean, t=t, noise=torch.randn_like(x))\n        x_recon.clamp_(-1., 1.)\n    else:\n        x_recon = None\n    posterior_log_variance_clipped = posterior_variance.clamp(LOG_COEF_MIN, LOG_COEF_MAX)\n    posterior_mean, posterior_variance, posterior_log_variance_clipped = self.q_posterior(x_start=mean, x_t=x, t=t)\n    return posterior_mean, posterior_variance, posterior_log_variance_clipped, x_recon", "CodeBleu_score": 44.417688748433946}]}
{"instance_id": "a8652fa78473", "generation_result": [{"code": "def get_loss(self, pred, target, mean=True)\n\tif self.loss_type == 'l1':\n        loss = torch.abs(pred - target)\n        if mean:\n            loss = loss.mean()\n    elif self.loss_type == 'l2':\n        loss = torch.nn.functional.mse_loss(pred, target, reduction='none')\n        if mean:\n            loss = loss.mean()\n    else:\n        raise NotImplementedError(f\"Unknown loss type: {self.loss_type}\")\n    return loss", "CodeBleu_score": 61.39995982682682}, {"code": "def get_loss(self, pred, target, mean=True)\n\tif self.loss_type == 'l1':\n        loss = torch.abs(pred - target)\n        if mean:\n            loss = loss.mean()\n    elif self.loss_type == 'l2':\n        loss = torch.nn.functional.mse_loss(pred, target, reduction='mean' if mean else 'none')\n    else:\n        raise NotImplementedError('Unknown loss type: {}'.format(self.loss_type))\n    return loss", "CodeBleu_score": 54.95429426982228}, {"code": "def get_loss(self, pred, target, mean=True)\n\tif self.loss_type == 'l1':\n        loss = torch.abs(pred - target)\n        if mean:\n            loss = loss.mean()\n    elif self.loss_type == 'l2':\n        loss = torch.nn.functional.mse_loss(pred, target, reduction='none')\n        if mean:\n            loss = loss.mean()\n    else:\n        raise NotImplementedError('Unknown loss type: {}'.format(self.loss_type))\n    return loss", "CodeBleu_score": 61.477270288811546}]}
{"instance_id": "d68e4f061349", "generation_result": [{"code": "def _prior_bpd(self, x_start)\n\tbatch_size = x_start.shape[0]\n    t = torch.full((batch_size,), self.num_timesteps, dtype=torch.long, device=x_start.device)\n    mean, _, log_variance = self.q_mean_variance(x_start, t)\n    kl = normal_kl(mean, log_variance)\n    return kl.sum() / (np.log(2.) * mean_flat(x_start).shape[0])\n\n", "CodeBleu_score": 33.68371315596178}, {"code": "def _prior_bpd(self, x_start)\n\t# Get batch size\n    batch_size = x_start.shape[0]\n    # Create t tensor\n    t = torch.ones(batch_size, dtype=torch.long, device=x_start.device) * self.num_timesteps\n    # Get predicted distribution at final timestep\n    mean, _, log_variance = self.q_mean_variance(x_start, t)\n    # Calculate KL divergence between predicted distribution and standard normal distribution\n    kl = normal_kl(mean, log_variance)\n    # Return mean KL divergence normalized by log(2)\n    return kl.sum() / (np.log(2) * mean.nelement())\n\n", "CodeBleu_score": 37.01189632532106}, {"code": "def _prior_bpd(self, x_start)\n\tbpd = torch.zeros(x_start.shape[0])\n    for t in range(self.num_timesteps):\n        mean, _, log_variance = self.q_mean_variance(x_start, t)\n        kl = normal_kl(mean, log_variance)\n        bpd += kl.sum(dim=list(range(1, len(kl.shape))))\n    return bpd / np.log(2.)\n\n", "CodeBleu_score": 32.820362974737016}]}
{"instance_id": "c695d93fdf2d", "generation_result": [{"code": "def make_cache(subsection: str) -> diskcache.Cache\n\treturn diskcache.Cache(\n        directory=os.path.join(CACHE_DIR, subsection),\n        size_limit=4 * 1024 ** 3,\n        min_file_size=256 * 1024,\n    )", "CodeBleu_score": 56.14526131954539}, {"code": "def make_cache(subsection: str) -> diskcache.Cache\n\tcache_dir = pathlib.Path(os.environ[\"HOME\"], \".cache\", \"pytube\")\n    if not cache_dir.exists():\n        cache_dir.mkdir(parents=True)\n    return diskcache.Cache(\n        directory=cache_dir,\n        name=subsection,\n        size_limit=4 * 1024 ** 3,\n        min_file_size=256 * 1024,\n    )\n\n", "CodeBleu_score": 50.23258076872317}, {"code": "def make_cache(subsection: str) -> diskcache.Cache\n\treturn diskcache.Cache(\n        directory=f\"{CACHE_DIR}/{subsection}\",\n        size_limit=4 * 1024 ** 3,\n        min_file_size=256 * 1024,\n    )", "CodeBleu_score": 50.22631293902986}]}
{"instance_id": "ed4c543f0d38", "generation_result": [{"code": "def cache(subsection)\n\tcache_obj = caches.get(subsection)\n    if cache_obj is None:\n        with cache_lock:\n            if not os.path.exists(cache_dir):\n                os.makedirs(cache_dir)\n            if not isfile(cache_filename):\n                convert_old_cached_data()\n\n            cache_obj = caches.get(subsection)\n            if cache_obj is None:\n                cache_obj = make_cache(subsection)\n                caches[subsection] = cache_obj\n\n    return cache_obj\n\n", "CodeBleu_score": 65.09005108102716}, {"code": "def cache(subsection)\n\tcache_obj = caches.get(subsection)\n    if cache_obj is None:\n        with cache_lock:\n            if not os.path.exists(cache_dir):\n                os.makedirs(cache_dir)\n\n            if not isfile(cache_filename):\n                convert_old_cached_data()\n\n            cache_obj = caches.get(subsection)\n            if cache_obj is None:\n                cache_obj = make_cache(subsection)\n                caches[subsection] = cache_obj\n\n    return cache_obj\n\n", "CodeBleu_score": 65.09005108102716}, {"code": "def cache(subsection)\n\tcache_obj = caches.get(subsection)\n    if cache_obj is None:\n        with cache_lock:\n            if not os.path.exists(cache_dir):\n                os.makedirs(cache_dir)\n            if not isfile(cache_filename):\n                convert_old_cached_data()\n\n            cache_obj = caches.get(subsection)\n            if cache_obj is None:\n                cache_obj = make_cache(subsection)\n                caches[subsection] = cache_obj\n\n    return cache_obj\n\n", "CodeBleu_score": 65.09005108102716}]}
{"instance_id": "ccf5ea2d9fc1", "generation_result": [{"code": "def cached_data_for_file(subsection, title, filename, func)\n\tcache_obj = cache(subsection)\n    mtime = os.path.getmtime(filename)\n    entry = cache_obj.get(title)\n    if entry and entry.get(\"mtime\") >= mtime:\n        return entry.get(\"data\")\n    else:\n        data = func()\n        cache_obj.set(title, {\"mtime\": mtime, \"data\": data})\n        dump_cache()\n        return data\n\n", "CodeBleu_score": 24.705072426147705}, {"code": "def cached_data_for_file(subsection, title, filename, func)\n\tcache_obj = cache(subsection)\n    mtime = os.path.getmtime(filename)\n    entry = cache_obj.get(title)\n    if entry:\n        if entry.get('mtime') < mtime:\n            cache_obj.invalidate(title)\n            entry = None\n    if not entry:\n        data = func()\n        entry = {'data': data, 'mtime': mtime}\n        cache_obj.set(title, entry)\n        dump_cache()\n    return entry['data']\n\n", "CodeBleu_score": 38.173677096413364}, {"code": "def cached_data_for_file(subsection, title, filename, func)\n\tcache_obj = cache(subsection)\n    mtime = os.path.getmtime(filename)\n    entry = cache_obj.get(title)\n    if entry and entry.get(\"mtime\") >= mtime:\n        return entry.get(\"data\")\n    else:\n        data = func()\n        cache_obj.set(title, {\"data\": data, \"mtime\": mtime}, expire=expire_time)\n        dump_cache()\n        return data\n\n", "CodeBleu_score": 25.302218472884153}]}
{"instance_id": "e01a2d4f0ac2", "generation_result": [{"code": "def report(message: str, *, exc_info: bool = False) -> None\n\trecord_exception()\n\n    print(\"***\", message, file=sys.stderr, sep=\"", "CodeBleu_score": 19.362018089978257}, {"code": "def report(message: str, *, exc_info: bool = False) -> None\n\trecord_exception()\n    print(\"***\", message, sep=\"", "CodeBleu_score": 13.037112082482485}, {"code": "def report(message: str, *, exc_info: bool = False) -> None\n\trecord_exception()\n    print(\"***\", message, sep=\"", "CodeBleu_score": 13.037112082482485}]}
{"instance_id": "125fb9d2320b", "generation_result": [{"code": "def extract_original_prompts(style: PromptStyle, prompt, negative_prompt)\n\tif style.prompt == '' and style.negative_prompt == '':\n        return False, prompt, negative_prompt\n\n    if style.prompt != '':\n        success, prompt = extract_style_text_from_prompt(style.prompt, prompt)\n        if not success:\n            return False, prompt, negative_prompt\n\n    if style.negative_prompt != '':\n        success, negative_prompt = extract_style_text_from_prompt(style.negative_prompt, negative_prompt)\n        if not success:\n            return False, prompt, negative_prompt\n\n    return True, prompt, negative_prompt\n\n", "CodeBleu_score": 66.69600350213094}, {"code": "def extract_original_prompts(style: PromptStyle, prompt, negative_prompt)\n\tif style.prompt == '' and style.negative_prompt == '':\n        return False, prompt, negative_prompt\n\n    if style.prompt != '':\n        prompt_extracted, prompt = extract_style_text_from_prompt(style.prompt, prompt)\n        if not prompt_extracted:\n            return False, prompt, negative_prompt\n\n    if style.negative_prompt != '':\n        negative_prompt_extracted, negative_prompt = extract_style_text_from_prompt(style.negative_prompt, negative_prompt)\n        if not negative_prompt_extracted:\n            return False, prompt, negative_prompt\n\n    return True, prompt, negative_prompt\n\n", "CodeBleu_score": 67.702433296301}, {"code": "def extract_original_prompts(style: PromptStyle, prompt, negative_prompt)\n\tif style.prompt == '' or style.negative_prompt == '':\n        return False, prompt, negative_prompt\n\n    prompt_extracted, prompt = extract_style_text_from_prompt(style.prompt, prompt)\n    negative_prompt_extracted, negative_prompt = extract_style_text_from_prompt(style.negative_prompt, negative_prompt)\n\n    if prompt_extracted and negative_prompt_extracted:\n        return True, prompt, negative_prompt\n    else:\n        return False, prompt, negative_prompt\n\n", "CodeBleu_score": 56.97794012171571}]}
{"instance_id": "0c5f729fbce2", "generation_result": [{"code": "def random_partial(self, count, n_frames)\n\t", "CodeBleu_score": 4.226190476190476}, {"code": "def random_partial(self, count, n_frames)\n\tif self.utterances is None:\n        self._load_utterances()\n\n    utterances = self.utterance_cycler.sample(count)\n\n    a = [(u,) + u.random_partial(n_frames) for u in utterances]\n\n    return a", "CodeBleu_score": 93.58876149224903}, {"code": "def random_partial(self, count, n_frames)\n\t", "CodeBleu_score": 4.226190476190476}]}
{"instance_id": "31e8b27b8f94", "generation_result": [{"code": "def normalize_volume(wav, target_dBFS, increase_only=False, decrease_only=False)\n\tif increase_only and decrease_only:\n        raise ValueError(\"Both increase_only and decrease_only are set\")\n    dBFS_change = target_dBFS - 10 * np.log10(np.mean(wav ** 2))\n    if (dBFS_change < 0 and increase_only) or (dBFS_change > 0 and decrease_only):\n        return wav\n    return wav * (10 ** (dBFS_change / 20))", "CodeBleu_score": 92.12905740681829}, {"code": "def normalize_volume(wav, target_dBFS, increase_only=False, decrease_only=False)\n\tif increase_only and decrease_only:\n        raise ValueError(\"Both increase_only and decrease_only are set\")\n    change_in_dBFS = target_dBFS - 10 * np.log10(np.mean(wav ** 2))\n    if (change_in_dBFS < 0 and increase_only) or (change_in_dBFS > 0 and decrease_only):\n        return wav\n    return wav * (10 ** (change_in_dBFS / 20))\n\n", "CodeBleu_score": 84.19601267832965}, {"code": "def normalize_volume(wav, target_dBFS, increase_only=False, decrease_only=False)\n\tif increase_only and decrease_only:\n        raise ValueError(\"Both increase_only and decrease_only are set\")\n    dBFS_change = target_dBFS - 10 * np.log10(np.mean(wav ** 2))\n    if (dBFS_change < 0 and increase_only) or (dBFS_change > 0 and decrease_only):\n        return wav\n    return wav * 10 ** (dBFS_change / 20)", "CodeBleu_score": 87.99536200949383}]}
{"instance_id": "2353831e9033", "generation_result": [{"code": "def load_model(weights_fpath: Path, device=None)\n\tdevice = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    _model = SpeakerEncoder(device).to(device)\n    _model.load_state_dict(torch.load(weights_fpath, map_location=device))\n    _model.eval()\n    print(f\"Loaded {weights_fpath} on {device} ({_model.device})\")\n    return _model", "CodeBleu_score": 27.336891462852485}, {"code": "def load_model(weights_fpath: Path, device=None)\n\tdevice = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    _model = SpeakerEncoder(device).to(device)\n    _model.load_state_dict(torch.load(weights_fpath, map_location=device))\n    _model.eval()\n    print(f\"Loaded speaker encoder \\\"{weights_fpath}\\\" (step {_model.step})\")\n    return _model", "CodeBleu_score": 27.94818494990281}, {"code": "def load_model(weights_fpath: Path, device=None)\n\tdevice = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(\"Using device: {}\".format(device))\n\n    _model = SpeakerEncoder(device)\n    checkpoint = torch.load(weights_fpath, map_location=device)\n    _model.load_state_dict(checkpoint[\"model_state\"])\n\n    step = checkpoint[\"step\"]\n    _model.eval()\n    print(\"Loaded step {} from {}\".format(step, weights_fpath))\n    return _model", "CodeBleu_score": 43.017929073845}]}
{"instance_id": "3231ad480199", "generation_result": [{"code": "def embed_frames_batch(frames_batch)\n\t# Check if the model is loaded\n    if not _model:\n        raise Exception(\"You forgot to load the model. Use load_model().\")\n\n    # Convert the input NumPy array to a PyTorch tensor\n    frames_batch_tensor = torch.from_numpy(frames_batch).to(_device)\n\n    # Perform a forward pass through the model to compute the embeddings\n    embeds = _model.forward(frames_batch_tensor)\n\n    # Detach the embeddings from the computation graph\n    embeds = embeds.detach()\n\n    # Move the embeddings to the CPU and convert them back to a NumPy array\n    embeds = embeds.cpu().numpy()\n\n    return embeds", "CodeBleu_score": 54.021337249720204}, {"code": "def embed_frames_batch(frames_batch)\n\tif not _model:\n        raise Exception(\"You forgot to load the model: 'load_model()'.\")\n\n    frames_batch = torch.from_numpy(frames_batch).to(_device)\n    embeds = _model.forward(frames_batch).detach().cpu().numpy()\n\n    return embeds", "CodeBleu_score": 61.87289769230277}, {"code": "def embed_frames_batch(frames_batch)\n\t# Check if model is loaded, raise exception if not\n    if _model is None:\n        raise Exception(\"Model must be loaded before inference.\")\n\n    # Convert frames_batch to PyTorch tensor, move to _device\n    frames_batch = torch.from_numpy(frames_batch).to(_device)\n\n    # Perform a forward pass through the model to compute the embeddings\n    embeds = _model.forward(frames_batch).detach().cpu().numpy()\n\n    return embeds", "CodeBleu_score": 71.40467174963541}]}
{"instance_id": "117a41716c8d", "generation_result": [{"code": "def compute_partial_slices(n_samples, partial_utterance_n_frames=partials_n_frames,\n                           min_pad_coverage=0.75, overlap=0.5)\n\tassert 0.0 < overlap < 1.0, \"Overlap must be between 0 and 1\"\n    assert 0 <= min_pad_coverage <= 1.0, \"Padding Coverage must be between 0 and 1\"\n    assert partial_utterance_n_frames % 2 == 0, \"partial_utterance_n_frames must be divisible by 2\"\n\n    frame_step = int((1.0 - overlap) * partial_utterance_n_frames)\n    n_frames = int(np.ceil(n_samples / hparams.hop_size))\n    n_partials = int(np.ceil((n_frames - partial_utterance_n_frames) / frame_step)) + 1\n\n    wav_slices, mel_slices = [], []\n    for i in range(n_partials):\n        start = i * frame_step\n        end = start + partial_utterance_n_frames\n        if end <= n_frames:\n            wav_slices.append(slice(start * hparams.hop_size, end * hparams.hop_size))\n            mel_slices.append(slice(start, end))\n\n    coverage = wav_slices[-1].stop * 100 / n_samples\n    if coverage < min_pad_coverage and len(mel_slices) > 1:\n        wav_slices = wav_slices[:-1]\n        mel_slices = mel_slices[:-1]\n\n    return wav_slices, mel_slices", "CodeBleu_score": 41.9522096896601}, {"code": "def compute_partial_slices(n_samples, partial_utterance_n_frames=partials_n_frames,\n                           min_pad_coverage=0.75, overlap=0.5)\n\tassert 0.0 <= overlap < 1.0\n    assert 0.0 <= min_pad_coverage <= 1.0\n\n    frame_step = int(sampling_rate * overlap / 1000)\n    frame_length = int(sampling_rate * partial_utterance_n_frames / 1000)\n\n    # Compute how many frames separate two partial utterances\n    offset = (frame_length - frame_step)\n\n    # Compute the number of partial utterances\n    n_partials = int(np.ceil((n_samples - frame_length) / offset)) + 1\n\n    # Compute the wav and mel ranges\n    wav_slices = [(i * offset, (i + 1) * offset + frame_length)\n                  for i in range(n_partials)]\n    mel_slices = [(i * offset, (i + 1) * offset + frame_length)\n                  for i in range(n_partials)]\n\n    # Evaluate whether to remove last frame\n    coverage = (n_samples - wav_slices[-1][0]) / frame_length\n    if coverage < min_pad_coverage and len(mel_slices) > 1:\n        wav_slices = wav_slices[:-1]\n        mel_slices = mel_slices[:-1]\n\n    return wav_slices, mel_slices", "CodeBleu_score": 36.777160214390605}, {"code": "def compute_partial_slices(n_samples, partial_utterance_n_frames=partials_n_frames,\n                           min_pad_coverage=0.75, overlap=0.5)\n\tassert 0.0 <= overlap < 1.0\n    assert 0 <= min_pad_coverage <= 1.0\n    assert partial_utterance_n_frames % 2 == 0\n\n    frame_step = int((1.0 - overlap) * partial_utterance_n_frames)\n    n_frames = int(np.ceil(n_samples / hparams.hop_length))\n    n_partials = int(np.ceil((n_frames - partial_utterance_n_frames) / frame_step)) + 1\n\n    wav_slices = []\n    mel_slices = []\n    for i in range(n_partials):\n        start = i * frame_step\n        end = start + partial_utterance_n_frames\n        if end > n_frames:\n            start = max(0, n_frames - partial_utterance_n_frames)\n            end = n_frames\n        wav_slices.append(slice(start * hparams.hop_length, end * hparams.hop_length))\n        mel_slices.append(slice(start, end))\n\n    coverage = wav_slices[-1].stop * 100.0 / n_samples\n    if coverage < min_pad_coverage and len(mel_slices) > 1:\n        wav_slices = wav_slices[:-1]\n        mel_slices = mel_slices[:-1]\n\n    return wav_slices, mel_slices", "CodeBleu_score": 41.42189724442235}]}
{"instance_id": "34fe5c864f1c", "generation_result": [{"code": "def plot_embedding_as_heatmap(embed, ax=None, title=\"\", shape=None, color_range=(0, 0.30))\n\tif ax is None:\n        ax = plt.gca()\n    if shape is None:\n        shape = int(np.sqrt(embed.shape[0]))\n        embed = embed.reshape((shape, shape))\n    cmap = cm.get_cmap(\"RdBu\")\n    sm = cm.ScalarMappable(cmap=cmap)\n    sm.set_clim(color_range)\n    ax.set_title(title)\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.imshow(embed, cmap=cmap)\n    plt.colorbar(sm, ax=ax)", "CodeBleu_score": 54.72156325008033}, {"code": "def plot_embedding_as_heatmap(embed, ax=None, title=\"\", shape=None, color_range=(0, 0.30))\n\tif ax is None:\n        ax = plt.gca()\n    if shape is None:\n        shape = int(np.sqrt(len(embed)))\n    embed = embed.reshape(shape, shape)\n    cmap = cm.get_cmap('viridis', 256)\n    cmap = cmap.copy()\n    cmap.set_under('white')\n    cmap.set_bad('white')\n    sm = cm.ScalarMappable(cmap=cmap)\n    sm.set_clim(color_range)\n    ax.set_title(title)\n    ax.imshow(embed, cmap=cmap)\n    ax.set_xticks([])\n    ax.set_yticks([])\n    plt.colorbar(sm)", "CodeBleu_score": 58.82044098044683}, {"code": "def plot_embedding_as_heatmap(embed, ax=None, title=\"\", shape=None, color_range=(0, 0.30))\n\tif ax is None:\n        ax = plt.gca()\n    if shape is None:\n        shape = int(np.sqrt(len(embed)))\n    embed = embed.reshape(shape, shape)\n    cmap = cm.get_cmap(\"viridis\", 256)\n    sm = cm.ScalarMappable(cmap=cmap)\n    sm.set_clim(color_range[0], color_range[1])\n    ax.set_title(title)\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.imshow(embed, cmap=cmap)\n    plt.colorbar(sm)", "CodeBleu_score": 56.35485404662418}]}
{"instance_id": "92e93d50b62b", "generation_result": [{"code": "def similarity_matrix(self, embeds)\n\tnum_speakers = embeds.shape[0]\n    num_utterances = embeds.shape[1]\n\n    centroids_incl = torch.zeros(num_speakers, num_utterances, self.embedding_dim, device=self.device)\n    centroids_excl = torch.zeros(num_speakers, self.embedding_dim, device=self.device)\n    for i in range(num_speakers):\n        centroids_incl[i] = torch.mean(embeds[i], dim=0, keepdim=True)\n        centroids_excl[i] = torch.mean(embeds[i], dim=0)\n\n    centroids_incl = F.normalize(centroids_incl, p=2, dim=2)\n    centroids_excl = F.normalize(centroids_excl, p=2, dim=1)\n\n    sim_matrix = torch.zeros(num_speakers, num_speakers, device=self.device)\n    mask = np.eye(num_speakers) == 1\n\n    for i in range(num_speakers):\n        sim_matrix[i] = torch.matmul(centroids_incl[i], centroids_excl.T)\n        sim_matrix[i][mask[i]] = -np.inf\n\n    sim_matrix = sim_matrix * self.scale + self.bias\n\n    return sim_matrix", "CodeBleu_score": 35.1677322791426}, {"code": "def similarity_matrix(self, embeds)\n\tbatch_size = embeds.shape[0]\n    num_utts = embeds.shape[1]\n    centroids_incl = torch.zeros(batch_size, num_utts, self.emb_dim).to(self.device)\n    centroids_excl = torch.zeros(batch_size, num_utts, self.emb_dim).to(self.device)\n    sim_matrix = torch.zeros(batch_size, num_utts, num_utts).to(self.device)\n    mask = np.eye(num_utts)\n    for i in range(batch_size):\n        for j in range(num_utts):\n            centroids_incl[i, j, :] = torch.mean(embeds[i, :, :], dim=0)\n            centroids_excl[i, j, :] = torch.mean(embeds[i, :, :], dim=0)\n        centroids_incl[i, :, :] = centroids_incl[i, :, :] / torch.norm(centroids_incl[i, :, :], dim=1, keepdim=True)\n        centroids_excl[i, :, :] = centroids_excl[i, :, :] / torch.norm(centroids_excl[i, :, :], dim=1, keepdim=True)\n        for j in range(num_utts):\n            for k in range(num_utts):\n                if mask[j][k] == 0:\n                    sim_matrix[i, j, k] = torch.dot(centroids_incl[i, j, :], centroids_excl[i, k, :])\n    sim_matrix = sim_matrix * self.scale + self.bias\n    return sim_matrix\n\n", "CodeBleu_score": 32.52167826675327}, {"code": "def similarity_matrix(self, embeds)\n\tnum_spkrs_per_batch = embeds.shape[0]\n    num_utts_per_spkr = embeds.shape[1]\n    centroids_incl = torch.zeros(num_spkrs_per_batch, num_utts_per_spkr, self.emb_dim).to(self.device)\n    centroids_excl = torch.zeros(num_spkrs_per_batch, self.emb_dim).to(self.device)\n    sim_matrix = torch.zeros(num_spkrs_per_batch, num_spkrs_per_batch).to(self.device)\n    mask = np.eye(num_spkrs_per_batch)\n\n    for spkr_idx in range(num_spkrs_per_batch):\n        centroids_incl[spkr_idx] = torch.mean(embeds[spkr_idx], dim=0)\n        centroids_excl[spkr_idx] = torch.mean(embeds[spkr_idx], dim=0)\n        centroids_incl[spkr_idx] = centroids_incl[spkr_idx] / torch.norm(centroids_incl[spkr_idx])\n        centroids_excl[spkr_idx] = centroids_excl[spkr_idx] / torch.norm(centroids_excl[spkr_idx])\n\n    for spkr_idx in range(num_spkrs_per_batch):\n        for spkr_idx2 in range(num_spkrs_per_batch):\n            if spkr_idx == spkr_idx2:\n                continue\n            sim_matrix[spkr_idx, spkr_idx2] = torch.sum(centroids_incl[spkr_idx] * centroids_excl[spkr_idx2])\n\n    sim_matrix = sim_matrix * mask\n    sim_matrix = sim_matrix * self.scale + self.bias\n\n    return sim_matrix", "CodeBleu_score": 31.316020402371063}]}
{"instance_id": "a6eba54108a1", "generation_result": [{"code": "def loss(self, embeds)\n\tspeakers_per_batch, utterances_per_speaker = embeds.shape[:2]\n    similarity_matrix = self.similarity_matrix(embeds)\n    sim_matrix_reshaped = similarity_matrix.reshape(-1, speakers_per_batch)\n\n    # Ground truth labels\n    labels = torch.arange(speakers_per_batch).repeat(utterances_per_speaker).long().to(self.loss_device)\n\n    # Loss\n    loss = self.loss_fn(sim_matrix_reshaped, labels)\n\n    # Equal Error Rate\n    with torch.no_grad():\n        inv_argmax = lambda i: torch.nonzero(i == torch.max(i))[0]\n        preds = F.softmax(sim_matrix_reshaped, dim=1)\n        preds = list(map(inv_argmax, preds))\n        preds = torch.LongTensor(preds).to(self.loss_device)\n        eer, _, _ = roc_curve(labels.flatten(), preds.flatten())\n        eer = brentq(lambda x: 1. - x - interp1d(eer, preds.flatten(), labels.flatten())(x), 0., 1.)\n    return loss, eer\n\n", "CodeBleu_score": 53.30577311402519}, {"code": "def loss(self, embeds)\n\tspeakers_per_batch, utterances_per_speaker = embeds.shape[:2]\n    sim_matrix = self.similarity_matrix(embeds)\n    sim_matrix = sim_matrix.reshape((speakers_per_batch * utterances_per_speaker,\n                                     speakers_per_batch))\n    labels = np.repeat(np.arange(speakers_per_batch), utterances_per_speaker)\n    labels = torch.LongTensor(labels).to(self.loss_device)\n    loss = self.loss_fn(sim_matrix, labels)\n    eer = self.compute_eer(sim_matrix.detach().cpu().numpy(), labels.detach().cpu().numpy())\n    return loss, eer\n\n", "CodeBleu_score": 41.51278525570624}, {"code": "def loss(self, embeds)\n\t# Compute similarity matrix\n    sim_matrix = self.similarity_matrix(embeds)\n\n    # Reshape similarity matrix to a 1-D vector of size speakers_per_batch * (speakers_per_batch - 1)\n    # Each row of the similarity matrix contains the dot products between an utterance and all other utterances from other speakers\n    sim_matrix_reshaped = sim_matrix.reshape(-1, 1)\n\n    # Compute labels: for each utterance, there should be as many positive labels as there are utterances from other speakers\n    labels = np.repeat(np.arange(embeds.shape[0]), embeds.shape[0] - 1)\n    labels = (labels == np.arange(len(labels))[:, np.newaxis]).astype(np.float32)\n    labels = torch.from_numpy(labels)\n\n    # Compute loss\n    loss = self.loss_fn(sim_matrix_reshaped, labels)\n\n    # Compute EER without backpropagation\n    with torch.no_grad():\n        preds = sim_matrix_reshaped.detach().cpu().numpy()\n        labels = labels.detach().cpu().numpy()\n        labels[labels == 0] = -1\n        labels[labels == 1] = 1\n        labels = labels.flatten()\n\n        # Compute false positive and true positive rates\n        fpr, tpr, thresholds = roc_curve(labels, preds)\n        eer = brentq(lambda x: 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n        eer = np.nan_to_num(eer)\n\n    return loss, eer\n\n", "CodeBleu_score": 46.016855037202106}]}
{"instance_id": "ac7b74b9d1b1", "generation_result": [{"code": "def log_params(self)\n\tif not self.log_parameters:\n        return\n    # import parameters\n    from .. import params\n    # get parameters\n    model_params = params.model_params\n    data_params = params.data_params\n    # create parameters string\n    params_str = ''\n    # iterate through model parameters\n    for param_name in model_params:\n        # get parameter value\n        param_value = getattr(self, param_name)\n        # append to parameters string\n        params_str += f'{param_name}: {param_value}", "CodeBleu_score": 16.085875784794688}, {"code": "def log_params(self)\n\t# imports\n    from ..utils.logging import log_string\n    from ..utils.saving import load_params\n    from ..utils.data import load_data\n\n    # get params\n    params = load_params(self.params)\n    data_params = load_data(self.data_params)\n\n    # log params\n    if self.log_params:\n        # log model params\n        model_params = params['model_params']\n        model_params_string = f\"Model Parameters", "CodeBleu_score": 14.138916173595945}, {"code": "def log_params(self)\n\tif self.log_params:\n        from .. import params\n        from ..utils.display import params_to_str\n        from ..utils.display import display_params\n        from ..utils.display import display_text\n        from ..utils.display import display_notebook\n        from ..utils.display import display_html\n        from ..utils.display import display_markdown\n        from ..utils.display import display_javascript\n        from ..utils.display import display_pdf\n        from ..utils.display import display_png\n        from ..utils.display import display_jpeg\n        from ..utils.display import display_gif\n        from ..utils.display import display_svg\n        from ..utils.display import display_latex\n        from ..utils.display import display_json\n        from ..utils.display import display_csv\n        from ..utils.display import display_xml\n        from ..utils.display import display_yaml\n        from ..utils.display import display_toml\n        from ..utils.display import display_rst\n        from ..utils.display import display_ascii\n        from ..utils.display import display_ansi\n        from ..utils.display import display_ascii_lines\n        from ..utils.display import display_ansi_lines\n        from ..utils.display import display_progress\n        from ..utils.display import display_profile\n        from ..utils.display import display_profile_stats\n        from ..utils.display import display_profile_timeline\n        from ..utils.display import display_table\n        from ..utils.display import display_object\n        from ..utils.display import display_dict\n        from ..utils.display import display_list\n        from ..utils.display import display_set\n        from ..utils.display import display_tuple\n        from ..utils.display import display_exception\n        from ..utils.display import display_traceback\n        from ..utils.display import display_error\n        from ..utils.display import display_warning\n        from ..utils.display import display_info\n        from ..utils.display import display_success\n        from ..utils.display import display_failure\n        from ..utils.display import display_confusion_matrix\n        from ..utils.display import display_image\n        from ..utils.display import display_audio", "CodeBleu_score": 5.532104226397324}]}
{"instance_id": "4726b7864386", "generation_result": [{"code": "def draw_projections(self, embeds, utterances_per_speaker, step, out_fpath=None, max_speakers=10)\n\tif not self.visualize_embeddings:\n        return\n    # Limit the number of speakers\n    speakers = list(embeds.keys())[:max_speakers]\n    # Flatten the embeddings\n    embeds = np.array([embeds[s] for s in speakers])\n    # Assign colors based on speaker identity\n    colors = np.repeat(np.arange(len(speakers)), utterances_per_speaker)\n    # Reduce the dimensionality of the embeddings\n    reducer = umap.UMAP(random_state=1)\n    embeds_2d = reducer.fit_transform(embeds)\n    # Plot the embeddings\n    fig, ax = plt.subplots(figsize=(12, 10))\n    ax.scatter(embeds_2d[:, 0], embeds_2d[:, 1], c=colors, cmap=\"tab20\", alpha=0.5)\n    ax.set_title(f\"Speaker Embeddings (step {step})\")\n    ax.set_aspect(\"equal\", \"datalim\")\n    plt.tight_layout()\n    if out_fpath:\n        plt.savefig(out_fpath)\n    else:\n        plt.draw()\n        plt.pause(0.001)\n    plt.clf()", "CodeBleu_score": 45.79814197758816}, {"code": "def draw_projections(self, embeds, utterances_per_speaker, step, out_fpath=None, max_speakers=10)\n\t# Limit the number of speakers\n    if len(embeds) > max_speakers:\n        embeds = embeds[:max_speakers]\n        utterances_per_speaker = utterances_per_speaker[:max_speakers]\n\n    # Flatten the embeddings\n    embeds = np.array([e.flatten() for e in embeds])\n\n    # Assign colors based on speaker identity\n    colors = np.repeat(np.arange(len(embeds)), utterances_per_speaker)\n\n    # Reduce the dimensionality of the embeddings\n    reducer = umap.UMAP(random_state=1)\n    projections = reducer.fit_transform(embeds)\n\n    # Plot the embeddings\n    fig, ax = plt.subplots()\n    ax.scatter(projections[:, 0], projections[:, 1], c=colors, cmap=\"tab10\")\n\n    # Set the aspect ratio\n    ax.set_aspect(\"equal\")\n\n    # Set the title\n    ax.set_title(f\"Step: {step}\")\n\n    # Display or save the plot\n    if out_fpath is None:\n        plt.show()\n    else:\n        plt.savefig(out_fpath)\n        plt.clf()\n\n    # Update the visualization unless disabled\n    if self.visualization_steps > 0 and step % self.visualization_steps == 0:\n        draw_projections(self, embeds, utterances_per_speaker, step, out_fpath)", "CodeBleu_score": 45.15935431925959}, {"code": "def draw_projections(self, embeds, utterances_per_speaker, step, out_fpath=None, max_speakers=10)\n\tif not self.display_projections:\n        return\n    if len(embeds) == 0:\n        return\n\n    embeds = np.array(embeds)\n    embeds = np.reshape(embeds, (embeds.shape[0], -1))\n    reducer = umap.UMAP()\n    projected = reducer.fit_transform(embeds)\n\n    plt.figure(figsize=(16, 10))\n    plt.gca().set_aspect(\"equal\", \"datalim\")\n    plt.gca().set_xticks([])\n    plt.gca().set_yticks([])\n    plt.title(f\"Step {step}\")\n\n    speakers = list(utterances_per_speaker.keys())\n    colors = plt.get_cmap(\"tab20\")(np.linspace(0, 1, len(speakers)))\n    for i, speaker in enumerate(speakers):\n        color = colors[i]\n        utterances = utterances_per_speaker[speaker]\n        xs = projected[utterances, 0]\n        ys = projected[utterances, 1]\n        plt.scatter(xs, ys, c=[color], label=speaker, alpha=0.5)\n\n    plt.legend(loc=\"upper left\", bbox_to_anchor=(1, 1))\n    if out_fpath is not None:\n        plt.savefig(out_fpath)\n        plt.close()\n    else:\n        plt.pause(0.01)", "CodeBleu_score": 47.5711215623411}]}
{"instance_id": "274fa50985c0", "generation_result": [{"code": "def add_speaker_embedding(self, x, speaker_embedding)\n\t# save the dimensions of the input tensor as human-readable names\n    batch_size, sequence_length, hidden_size = x.size()\n    # determine the appropriate dimension index for the speaker embedding\n    speaker_embedding_dim = 1 if self.speaker_embedding_first else 2\n    # repeat the speaker embedding to match the length of the input text\n    speaker_embedding = speaker_embedding.repeat_interleave(sequence_length, dim=0)\n    # reshape and transpose the speaker embedding\n    speaker_embedding = speaker_embedding.reshape(batch_size, sequence_length, -1).transpose(speaker_embedding_dim, 2)\n    # concatenate the speaker embedding with the input tensor along the last dimension\n    x = torch.cat([x, speaker_embedding], dim=speaker_embedding_dim)\n    # return the result\n    return x", "CodeBleu_score": 33.442443531586754}, {"code": "def add_speaker_embedding(self, x, speaker_embedding)\n\tx_dims = self.get_dims(x)\n    speaker_embedding_dims = self.get_dims(speaker_embedding)\n    speaker_embedding_dims.name = \"speaker_embedding\"\n    speaker_embedding_dims.index = self.get_dim_index(x_dims, speaker_embedding_dims.name)\n    speaker_embedding = speaker_embedding.repeat_interleave(x_dims.text_length, dim=0)\n    speaker_embedding = speaker_embedding.reshape(x_dims.text_length, speaker_embedding_dims.embedding_length)\n    speaker_embedding = speaker_embedding.transpose(0, 1)\n    x = torch.cat((x, speaker_embedding), dim=speaker_embedding_dims.index)\n    return x", "CodeBleu_score": 20.24699562106907}, {"code": "def add_speaker_embedding(self, x, speaker_embedding)\n\t# save dimensions of input tensor as human-readable names\n    batch_size, sequence_length, dimension = x.size()\n    # determine dimension index for speaker embedding\n    if self.speaker_embedding_dimension == \"embedding\":\n        speaker_embedding_dimension_index = 1\n    elif self.speaker_embedding_dimension == \"hidden\":\n        speaker_embedding_dimension_index = 2\n    # repeat speaker embedding to match length of input text\n    speaker_embedding = speaker_embedding.repeat_interleave(sequence_length, dim=0)\n    # reshape and transpose speaker embedding\n    speaker_embedding = speaker_embedding.reshape(batch_size, sequence_length, -1).transpose(1, 2)\n    # concatenate speaker embedding with input tensor along last dimension\n    x = torch.cat([x, speaker_embedding], dim=speaker_embedding_dimension_index)\n    return x", "CodeBleu_score": 34.501887834228526}]}
{"instance_id": "e6742ea0fe5e", "generation_result": [{"code": "def init_attention(self, encoder_seq_proj)\n\tdevice = next(self.parameters()).device\n    batch_size, seq_length, _ = encoder_seq_proj.size()\n    self.cumulative_alpha = torch.zeros(batch_size, seq_length, device=device)\n    self.attention = torch.zeros(batch_size, seq_length, device=device)\n\n", "CodeBleu_score": 24.37340093454993}, {"code": "def init_attention(self, encoder_seq_proj)\n\tself.device = next(self.parameters()).device\n    self.attention = torch.zeros(encoder_seq_proj.size()[0], encoder_seq_proj.size()[1])\n    self.attention = self.attention.to(self.device)", "CodeBleu_score": 17.411065437757507}, {"code": "def init_attention(self, encoder_seq_proj)\n\tself.device = next(self.parameters()).device\n    self.batch_size, self.seq_len, self.hidden_size = encoder_seq_proj.size()\n    self.cumulative_alpha = torch.zeros(self.batch_size, self.seq_len).to(self.device)\n    self.attention_history = torch.zeros(self.batch_size, self.seq_len).to(self.device)\n\n", "CodeBleu_score": 19.133132861277318}]}
{"instance_id": "4e1fc1ed0ed7", "generation_result": [{"code": "def load(self, path, optimizer=None)\n\t# Use device of model params as location for loaded state\n    device = next(self.parameters()).device\n    checkpoint = torch.load(str(path), map_location=device)\n    self.load_state_dict(checkpoint[\"model_state\"])\n\n    if \"optimizer_state\" in checkpoint and optimizer is not None:\n        optimizer.load_state_dict(checkpoint[\"optimizer_state\"])", "CodeBleu_score": 67.98839425729693}, {"code": "def load(self, path, optimizer=None)\n\t# Use device of model params as location for loaded state\n    device = next(self.parameters()).device\n    checkpoint = torch.load(str(path), map_location=device)\n    self.load_state_dict(checkpoint[\"model_state\"])\n\n    if \"optimizer_state\" in checkpoint and optimizer is not None:\n        optimizer.load_state_dict(checkpoint[\"optimizer_state\"])", "CodeBleu_score": 67.98839425729693}, {"code": "def load(self, path, optimizer=None)\n\t# Use device of model params as location for loaded state\n    device = next(self.parameters()).device\n    checkpoint = torch.load(str(path), map_location=device)\n    self.load_state_dict(checkpoint[\"model_state\"])\n\n    if \"optimizer_state\" in checkpoint and optimizer is not None:\n        optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n\n", "CodeBleu_score": 67.98839425729693}]}
{"instance_id": "178e6aa16c0f", "generation_result": [{"code": "def save(self, path, optimizer=None)\n\tif optimizer is not None:\n        torch.save({\n            \"model_state\": self.state_dict(),\n            \"optimizer_state\": optimizer.state_dict(),\n        }, str(path))\n    else:\n        torch.save({\n            \"model_state\": self.state_dict(),\n        }, str(path))\n\n", "CodeBleu_score": 93.03451014294475}, {"code": "def save(self, path, optimizer=None)\n\tif optimizer is not None:\n        torch.save({\n            \"model_state\": self.state_dict(),\n            \"optimizer_state\": optimizer.state_dict(),\n        }, str(path))\n    else:\n        torch.save({\n            \"model_state\": self.state_dict(),\n        }, str(path))", "CodeBleu_score": 93.03451014294475}, {"code": "def save(self, path, optimizer=None)\n\tif optimizer is not None:\n        torch.save({\n            \"model_state\": self.state_dict(),\n            \"optimizer_state\": optimizer.state_dict(),\n        }, str(path))\n    else:\n        torch.save({\n            \"model_state\": self.state_dict(),\n        }, str(path))", "CodeBleu_score": 93.03451014294475}]}
{"instance_id": "6528fd80e87b", "generation_result": [{"code": "def english_cleaners(text)\n\ttext = convert_to_ascii(text)\n    text = lowercase(text)\n    text = expand_numbers(text)\n    text = expand_abbreviations(text)\n    text = collapse_whitespace(text)\n    return text\n\n", "CodeBleu_score": 55.236418047148106}, {"code": "def english_cleaners(text)\n\ttext = convert_to_ascii(text)\n    text = lowercase(text)\n    text = expand_numbers(text)\n    text = expand_abbreviations(text)\n    text = collapse_whitespace(text)\n    return text", "CodeBleu_score": 55.236418047148106}, {"code": "def english_cleaners(text)\n\ttext = convert_to_ascii(text)\n    text = lowercase(text)\n    text = expand_numbers(text)\n    text = expand_abbreviations(text)\n    text = collapse_whitespace(text)\n    return text", "CodeBleu_score": 55.236418047148106}]}
{"instance_id": "8a4fe08e0985", "generation_result": [{"code": "def text_to_sequence(text, cleaner_names)\n\tsequence = []\n    # Check for curly braces and treat their contents as ARPAbet:\n    while len(text):\n        m = _curly_re.match(text)\n        if not m:\n            sequence += _symbols_to_sequence(_clean_text(text, cleaner_names))\n            break\n        sequence += _symbols_to_sequence(_arpabet_to_sequence(m.group(1)))\n        text = m.group(2)\n    # Append EOS token\n    sequence.append(_symbol_to_id[\"~\"])\n    return sequence", "CodeBleu_score": 62.92769887295322}, {"code": "def text_to_sequence(text, cleaner_names)\n\tsequence = []\n    # Check for curly braces and treat their contents as ARPAbet:\n    while len(text):\n        m = _curly_re.match(text)\n        if not m:\n            sequence += _symbols_to_sequence(_clean_text(text, cleaner_names).split(\" \"))\n            break\n        sequence += _symbols_to_sequence(_arpabet_to_sequence(m.group(1)))\n        text = m.group(2)\n    # Append EOS token\n    sequence.append(_symbol_to_id[\"~\"])\n    return sequence", "CodeBleu_score": 59.587192573296}, {"code": "def text_to_sequence(text, cleaner_names)\n\tsequence = []\n    text = _clean_text(text, cleaner_names)\n    while len(text):\n        m = _curly_re.match(text)\n        if not m:\n            sequence += _symbols_to_sequence(_basic_cleaners(text))\n            break\n        sequence += _symbols_to_sequence(_arpabet_to_sequence(m.group(1)))\n        text = m.group(2)\n    return sequence + [len(_symbol_to_id)]", "CodeBleu_score": 52.11324477163431}]}
{"instance_id": "e1c25a37af1f", "generation_result": [{"code": "def sequence_to_text(sequence)\n\tresult = \"\"\n    for symbol_id in sequence:\n        if symbol_id in _arpabet_to_symbol:\n            result += \"{\" + _arpabet_to_symbol[symbol_id] + \"}\"\n        else:\n            result += _symbol_to_arpabet[symbol_id]\n    result = result.replace(\"}{\", \" \")\n    return result", "CodeBleu_score": 40.61425723885163}, {"code": "def sequence_to_text(sequence)\n\tresult = ''\n    for symbol_id in sequence:\n        if symbol_id in _arpabet_to_symbol:\n            symbol = '{%s}' % _arpabet_to_symbol[symbol_id]\n        else:\n            symbol = _symbol_to_id[symbol_id]\n        result += symbol\n    result = result.replace('}{', ' ')\n    return result", "CodeBleu_score": 44.11373164375096}, {"code": "def sequence_to_text(sequence)\n\tresult = \"\"\n    for symbol_id in sequence:\n        if symbol_id in _arpabet_id_to_symbol:\n            symbol = \"{%s}\" % _arpabet_id_to_symbol[symbol_id]\n        else:\n            symbol = _id_to_symbol[symbol_id]\n        result += symbol\n    result = result.replace(\"}{\", \" \")\n    return result\n\n", "CodeBleu_score": 47.83510380799913}]}
{"instance_id": "7807cad4b05d", "generation_result": [{"code": "def embed_utterance(fpaths, encoder_model_fpath)\n\tif not encoder.is_loaded():\n        encoder.load_model(encoder_model_fpath)\n\n    # Compute the speaker embedding of the utterance\n    wav_fpath, embed_fpath = fpaths\n    wav = np.load(wav_fpath)\n    wav = encoder.preprocess_wav(wav)\n    embed = encoder.embed_utterance(wav)\n    np.save(embed_fpath, embed, allow_pickle=False)\n\n", "CodeBleu_score": 94.89398130768922}, {"code": "def embed_utterance(fpaths, encoder_model_fpath)\n\tif not encoder.is_loaded():\n        encoder.load_model(encoder_model_fpath)\n\n    # Compute the speaker embedding of the utterance\n    wav_fpath, embed_fpath = fpaths\n    wav = np.load(wav_fpath)\n    wav = encoder.preprocess_wav(wav)\n    embed = encoder.embed_utterance(wav)\n    np.save(embed_fpath, embed, allow_pickle=False)\n\n", "CodeBleu_score": 94.89398130768922}, {"code": "def embed_utterance(fpaths, encoder_model_fpath)\n\tif not encoder.is_loaded():\n        encoder.load_model(encoder_model_fpath)\n\n    # Compute the speaker embedding of the utterance\n    wav_fpath, embed_fpath = fpaths\n    wav = np.load(wav_fpath)\n    wav = encoder.preprocess_wav(wav)\n    embed = encoder.embed_utterance(wav)\n    np.save(embed_fpath, embed, allow_pickle=False)\n\n", "CodeBleu_score": 94.89398130768922}]}
{"instance_id": "9f860273a325", "generation_result": [{"code": "def create_embeddings(synthesizer_root: Path, encoder_model_fpath: Path, n_processes: int)\n\tembed_dir = synthesizer_root.joinpath(\"embed\")\n    wav_dir = synthesizer_root.joinpath(\"audio\")\n    metadata_fpath = synthesizer_root.joinpath(\"train.txt\")\n    \n    print(\"Preparing the encoder, the model and the audio files...\")\n    encoder.load_model(encoder_model_fpath)\n    \n    assert wav_dir.exists()\n    assert metadata_fpath.exists()\n    assert embed_dir.exists()\n    \n    with metadata_fpath.open(\"r\") as metadata_file:\n        metadata = [line.split(\"|\") for line in metadata_file]\n        frame_shift_ms = int(metadata[0][4])\n        sampling_rate = int(metadata[0][2])\n        hop_length = int(frame_shift_ms / 1000 * sampling_rate)\n    \n    wav_fpaths = list(wav_dir.glob(\"*.wav\"))\n    assert len(wav_fpaths) > 0\n    \n    embed_fpaths = []\n    for wav_fpath in wav_fpaths:\n        embed_fpaths.append(embed_dir.joinpath(wav_fpath.name.replace(\".wav\", \".npy\")))\n    \n    print(\"Creating embeddings...\")\n    if n_processes == 1:\n        [encoder.embed_utterance(wav_fpath, embed_fpath, hop_length) for wav_fpath, embed_fpath in tqdm(zip(wav_fpaths, embed_fpaths))]\n    else:\n        func = partial(encoder.embed_utterance, hop_length=hop_length)\n        job = Pool(n_processes).imap(func, zip(wav_fpaths, embed_fpaths))\n        list(tqdm(job, \"Embedding\", len(wav_fpaths), unit=\"utterances\"))\n    \n    print(\"Created %d embeddings\" % len(embed_fpaths))", "CodeBleu_score": 55.07557483606791}, {"code": "def create_embeddings(synthesizer_root: Path, encoder_model_fpath: Path, n_processes: int)\n\t####################################################################################################################\n    # Check if the synthesizer_root directory exists\n    if not synthesizer_root.exists():\n        raise Exception(\"The synthesizer_root directory does not exist. Please specify a valid path to the synthesizer_root directory and try again.\")\n    ####################################################################################################################\n    # Check if the encoder_model_fpath file exists\n    if not encoder_model_fpath.exists():\n        raise Exception(\"The encoder_model_fpath file does not exist. Please specify a valid path to the encoder_model_fpath file and try again.\")\n    ####################################################################################################################\n    # Check if the metadata file exists\n    metadata_fpath = synthesizer_root.joinpath(\"train.txt\")\n    if not metadata_fpath.exists():\n        raise Exception(\"The metadata file does not exist. Please specify a valid path to the metadata file and try again.\")\n    ####################################################################################################################\n    # Check if the wav directory exists\n    wav_dir = synthesizer_root.joinpath(\"audio\")\n    if not wav_dir.exists():\n        raise Exception(\"The wav directory does not exist. Please specify a valid path to the wav directory and try again.\")\n    ####################################################################################################################\n    # Check if the embed directory exists\n    embed_dir = synthesizer_root.joinpath(\"embeds\")\n    if not embed_dir.exists():\n        embed_dir.mkdir(parents=True, exist_ok=True)\n    ####################################################################################################################\n    # Read metadata file\n    with metadata_fpath.open(\"r\", encoding=\"utf-8\") as metadata_file:\n        metadata = [line.split(\"|\") for line in metadata_file]\n    ####################################################################################################################\n    # Prepare file paths for audio and embeddings\n    file_paths = [(wav_dir.joinpath(m[0]), embed_dir.joinpath(m[2])) for m in metadata]\n    ####################################################################################################################\n    # Create embeddings\n    encoder = Encoder(enc", "CodeBleu_score": 50.71290698813039}, {"code": "def create_embeddings(synthesizer_root: Path, encoder_model_fpath: Path, n_processes: int)\n\twav_dir = synthesizer_root.joinpath(\"audio\")\n    assert wav_dir.exists()\n    metadata_fpath = synthesizer_root.joinpath(\"train.txt\")\n    assert metadata_fpath.exists()\n    embed_dir = synthesizer_root.joinpath(\"embeds\")\n    embed_dir.mkdir(exist_ok=True)\n    print(\"embed_dir\", embed_dir)\n    # Gather the input audio filepath and the target output embed filepath\n    with metadata_fpath.open(\"r\", encoding=\"utf-8\") as metadata_file:\n        metadata = [line.split(\"|\") for line in metadata_file]\n        fpaths = [wav_dir.joinpath(m[0]) for m in metadata]\n        embed_fpaths = [embed_dir.joinpath(m[0].replace(\".wav\", \".npy\")) for m in metadata]\n\n    # TODO: improve on the multiprocessing, it's terrible. Disk I/O is the bottleneck here.\n    # Embed the utterances in separate threads\n    func = partial(embed_utterance, encoder_model_fpath)\n    job = Pool(n_processes).imap(func, tqdm(zip(fpaths, embed_fpaths)))\n    list(job)", "CodeBleu_score": 69.11618810348308}]}
{"instance_id": "9eb0abda14a9", "generation_result": [{"code": "def eval_model(attention, mel_prediction, target_spectrogram, input_seq, step,\n               plot_dir, mel_output_dir, wav_dir, sample_num, loss, hparams)\n\t# save predicted mel spectrogram to disk (debug)\n    mel_filename = \"mel-prediction-step-{}_sample-{}.npy\".format(step, sample_num)\n    np.save(mel_output_dir.joinpath(mel_filename), mel_prediction.T, allow_pickle=False)\n\n    # save griffin lim inverted wav for debug (mel -> wav)\n    wav = audio.inv_mel_spectrogram(mel_prediction.T, hparams)\n    audio.save_wav(wav, wav_dir.joinpath(\"step-{}-wave-from-mel_sample_{}.wav\".format(step, sample_num)), hparams)\n\n    # save real and predicted mel-spectrogram plot to disk (control purposes)\n    spec_filename = \"mel-spectrogram-prediction-step-{}_sample-{}.png\".format(step, sample_num)\n    plot_spectrogram(mel_prediction, plot_dir.joinpath(spec_filename), title=\"{} Step {}, Loss: {:.5f}\".format(hparams.model_type, step, loss), target_spectrogram=target_spectrogram, auto_aspect=True)\n\n    # save attention plot to disk (control purposes)\n    if attention is not None:\n        attn_filename = \"alignment-step-{}_sample-{}.png\".format(step, sample_num)\n        save_attention(attention, plot_dir.joinpath(attn_filename))\n\n    # print the sequence and attention to screen (control purposes)\n    print(\"", "CodeBleu_score": 43.47783610641559}, {"code": "def eval_model(attention, mel_prediction, target_spectrogram, input_seq, step,\n               plot_dir, mel_output_dir, wav_dir, sample_num, loss, hparams)\n\t# save attention plot to disk\n    attention_path = plot_dir.joinpath(\"attention_step_{}_{}.png\".format(step, sample_num))\n    save_attention(attention, attention_path)\n\n    # save predicted mel spectrogram to disk\n    mel_output_path = mel_output_dir.joinpath(\"mel-prediction-step-{}_{}.npy\".format(step, sample_num))\n    np.save(str(mel_output_path), mel_prediction.T, allow_pickle=False)\n\n    # save griffin lim inverted wav for debug (mel -> wav)\n    wav = audio.inv_mel_spectrogram(mel_prediction.T, hparams)\n    audio.save_wav(wav, wav_dir.joinpath(\"step-{}-wave-from-mel.wav\".format(step)))\n\n    # save real and predicted mel-spectrogram plot to disk\n    spec_path = plot_dir.joinpath(\"step-{}-mel-spectrogram.png\".format(step))\n    plot_spectrogram(mel_prediction, spec_path, title=\"{} Step {}, Loss={:.5f}\".format(time_string(), step, loss),\n                     target_spectrogram=target_spectrogram, auto_aspect=True)\n\n    # print the sequence and attention to the screen\n    print(\"", "CodeBleu_score": 48.08679479675402}, {"code": "def eval_model(attention, mel_prediction, target_spectrogram, input_seq, step,\n               plot_dir, mel_output_dir, wav_dir, sample_num, loss, hparams)\n\t# save attention map\n    attention_path = plot_dir.joinpath(\"attention_step{}_sample{}.png\".format(step, sample_num))\n    save_attention(attention, attention_path)\n\n    # save predicted mel spectrogram\n    mel_output_path = mel_output_dir.joinpath(\"mel-prediction-step{}_sample{}.npy\".format(step, sample_num))\n    np.save(str(mel_output_path), mel_prediction.T, allow_pickle=False)\n\n    # save reconstructed wav\n    wav_path = wav_dir.joinpath(\"step-{}-wave-from-mel.wav\".format(step))\n    audio.inv_mel_spectrogram(mel_prediction.T, str(wav_path))\n\n    # save alignment plot\n    title = \"{} - Step {}, Loss: {:.5f}\".format(time_string(), step, loss)\n    plot_path = plot_dir.joinpath(\"step-{}-mel-spectrogram-plot.png\".format(step))\n    plot_spectrogram(mel_prediction, plot_path, title, split_title=True,\n                     target_spectrogram=target_spectrogram, auto_aspect=True)\n\n    # print input sequence\n    print(\"", "CodeBleu_score": 45.77248652899291}]}
{"instance_id": "6bad4f500286", "generation_result": [{"code": "def num_params(self)\n\tparams = filter(lambda p: p.requires_grad, self.parameters())\n    num_params = sum([np.prod(p.size()) for p in params])\n    print(\"The number of trainable parameters in the model is {:.3f} M\".format(num_params / 1e6))", "CodeBleu_score": 35.927523568340106}, {"code": "def num_params(self)\n\tparams = list(filter(lambda p: p.requires_grad, self.parameters()))\n    num_params = sum([np.prod(p.size()) for p in params])\n    print(\"Trainable Parameters: %.3fM\" % (num_params / 1e6))\n\n", "CodeBleu_score": 33.67741337012991}, {"code": "def num_params(self)\n\tparameters = filter(lambda p: p.requires_grad, self.parameters())\n    parameters = sum([np.prod(p.size()) for p in parameters])\n    print(\"Trainable Parameters: %.3fM\" % (parameters / 1e6))", "CodeBleu_score": 46.71448637721295}]}
{"instance_id": "78b876054bd4", "generation_result": [{"code": "def pad_tensor(self, x, pad, side='both')\n\t# determine the new size\n    new_size = x.size(0) + pad * 2\n\n    # create the new tensor\n    if x.is_cuda:\n        x_pad = torch.cuda.FloatTensor(new_size, x.size(1), x.size(2)).fill_(0)\n    else:\n        x_pad = torch.zeros(new_size, x.size(1), x.size(2))\n\n    # place the original tensor into the new tensor\n    if side == 'both':\n        x_pad[pad:x.size(0) + pad, :, :] = x\n    elif side == 'before':\n        x_pad[pad:, :, :] = x\n    elif side == 'after':\n        x_pad[:x.size(0), :, :] = x\n\n    return x_pad", "CodeBleu_score": 35.683623106949796}, {"code": "def pad_tensor(self, x, pad, side='both')\n\tif side not in ['before', 'after', 'both']:\n        raise ValueError(\"side must be one of 'before', 'after', or 'both'\")\n    # Determine the new total size of the tensor after padding\n    new_size = x.size(0) + pad\n    # Create a new tensor filled with zeros on either the CPU or GPU, depending on availability\n    if torch.cuda.is_available():\n        new_tensor = torch.zeros(new_size, x.size(1), x.size(2)).cuda()\n    else:\n        new_tensor = torch.zeros(new_size, x.size(1), x.size(2))\n    # Place the original tensor into this new padded tensor according to the specified side\n    if side == 'before':\n        new_tensor[pad:, :, :] = x\n    elif side == 'after':\n        new_tensor[:-pad, :, :] = x\n    else:\n        new_tensor[:pad, :, :] = x[:pad, :, :]\n        new_tensor[-pad:, :, :] = x[-pad:, :, :]\n    return new_tensor\n\n\n\n\n", "CodeBleu_score": 35.896478552306874}, {"code": "def pad_tensor(self, x, pad, side='both')\n\t# Determine the new size of the tensor after padding\n    new_size = x.size()[0] + pad\n\n    # Create a new tensor filled with zeros on either the CPU or GPU, depending on availability\n    if torch.cuda.is_available():\n        x_pad = torch.zeros(new_size, *x.size()[1:]).cuda()\n    else:\n        x_pad = torch.zeros(new_size, *x.size()[1:])\n\n    # Place the original tensor into the new padded tensor according to the specified side\n    if side == 'both':\n        x_pad[pad//2:pad//2+x.size()[0]] = x\n    elif side == 'before':\n        x_pad[pad:pad+x.size()[0]] = x\n    elif side == 'after':\n        x_pad[:x.size()[0]] = x\n    else:\n        raise ValueError('side must be either \"both\", \"before\", or \"after\"')\n\n    return x_pad", "CodeBleu_score": 33.38397769830239}]}
{"instance_id": "a7ed1715a3df", "generation_result": [{"code": "def save_attention(attn, path)\n\timport matplotlib.pyplot as plt\n    fig = plt.figure(figsize=(10, 10))\n    plt.imshow(attn.T, cmap='viridis', aspect='auto')\n    plt.colorbar()\n    plt.tight_layout()\n    plt.savefig(path + '.png', bbox_inches='tight')\n    plt.close(fig)", "CodeBleu_score": 54.05351434164118}, {"code": "def save_attention(attn, path)\n\tplt.close\n    fig.savefig\n    plt.imshow\n    plt.figure\n\n\n\n\n\n", "CodeBleu_score": 7.222032755823203}, {"code": "def save_attention(attn, path)\n\timport matplotlib.pyplot as plt\n    fig = plt.figure(figsize=(12, 6))\n    plt.imshow(attn.T, cmap='Blues', aspect='auto')\n    plt.xlabel('Head')\n    plt.ylabel('Token')\n    plt.colorbar()\n    plt.tight_layout()\n    fig.savefig(path + '.png', bbox_inches='tight')\n    plt.close(fig)", "CodeBleu_score": 59.809055902904404}]}
{"instance_id": "19375c9f44a2", "generation_result": [{"code": "def plot(array)\n\timport matplotlib.pyplot as plt\n\n    fig = plt.figure(figsize=(30, 5))\n    ax = fig.add_subplot(111)\n    ax.xaxis.label.set_color('grey')\n    ax.yaxis.label.set_color('grey')\n    ax.xaxis.label.set_fontsize(23)\n    ax.yaxis.label.set_fontsize(23)\n    ax.tick_params(axis='x', colors='grey', labelsize=23)\n    ax.tick_params(axis='y', colors='grey', labelsize=23)\n    plt.plot(array)", "CodeBleu_score": 77.42583347278438}, {"code": "def plot(array)\n\timport matplotlib.pyplot as plt\n\n    fig = plt.figure(figsize=(30, 5))\n    ax = fig.add_subplot(111)\n    ax.xaxis.label.set_color('grey')\n    ax.yaxis.label.set_color('grey')\n    ax.xaxis.label.set_fontsize(23)\n    ax.yaxis.label.set_fontsize(23)\n    ax.tick_params(axis='x', colors='grey', labelsize=23)\n    ax.tick_params(axis='y', colors='grey', labelsize=23)\n    plt.plot(array)\n\n", "CodeBleu_score": 77.42583347278438}, {"code": "def plot(array)\n\timport matplotlib.pyplot as plt\n\n    fig = plt.figure(figsize=(30, 5))\n    ax = fig.add_subplot(111)\n    ax.xaxis.label.set_color('grey')\n    ax.yaxis.label.set_color('grey')\n    ax.xaxis.label.set_fontsize(23)\n    ax.yaxis.label.set_fontsize(23)\n    ax.tick_params(axis='x', colors='grey', labelsize=23)\n    ax.tick_params(axis='y', colors='grey', labelsize=23)\n    plt.plot(array)\n\n", "CodeBleu_score": 77.42583347278438}]}
{"instance_id": "ff94c425920c", "generation_result": [{"code": "def write_to_csv(image_name, prediction, confidence)\n\t# Create a dictionary with the data we want to write to the CSV file\n    data = {\n        \"image_name\": image_name,\n        \"prediction\": prediction,\n        \"confidence\": confidence\n    }\n\n    # Open the CSV file in append mode\n    with open(csv_path, 'a') as csv_file:\n        # Create a CSV writer\n        writer = csv.DictWriter(csv_file, fieldnames=data.keys())\n\n        # Write the header if the file is new\n        if csv_file.tell() == 0:\n            writer.writeheader()\n\n        # Write the data as a new row\n        writer.writerow(data)", "CodeBleu_score": 50.99564815655056}, {"code": "def write_to_csv(image_name, prediction, confidence)\n\tdata = {'image': image_name, 'prediction': prediction, 'confidence': confidence}\n    with open('predictions.csv', 'a') as csv_file:\n        fieldnames = ['image', 'prediction', 'confidence']\n        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n        if csv_file.tell() == 0:\n            writer.writeheader()\n        writer.writerow(data)", "CodeBleu_score": 48.02480616188767}, {"code": "def write_to_csv(image_name, prediction, confidence)\n\twith open('predictions.csv', 'a') as csv_file:\n        fieldnames = ['image_name', 'prediction', 'confidence']\n        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n\n        if csv_file.tell() == 0:\n            writer.writeheader()\n\n        writer.writerow({'image_name': image_name, 'prediction': prediction, 'confidence': confidence})", "CodeBleu_score": 44.747511502162865}]}
{"instance_id": "436161602009", "generation_result": [{"code": "def export_torchscript(model, im, file, optimize, prefix=colorstr(\"TorchScript:\"))\n\ttry:\n        LOGGER.info(f\"{prefix} starting export with torch {torch.__version__}...\")\n        f = str(file).lower()\n        if f.endswith(\".pt\"):\n            file = Path(file).with_suffix(\".torchscript.pt\")\n        else:\n            file = Path(file).with_suffix(\".torchscript.pth\")\n        if not isinstance(im, torch.Tensor):  # not a tensor\n            im = torch.zeros([1, 3, *im.shape[-2:]])  # size batch_size, channels, height, width\n\n        jit_model = torch.jit.trace(model, im)\n        extra_files = {\"config.txt\": f\"{model.model.yaml}\"}  # YOLOv5-6.0.0.yaml\n        if optimize:  # script, save for mobile\n            jit_model = optimize_for_mobile(jit_model)\n            extra_files[\"model.yaml\"] = f\"{model.model.yaml}\"  # YOLOv5-6.0.0.yaml\n        jit_model.save(file, _extra_files=extra_files)\n        LOGGER.info(f\"{prefix} export success, saved as {file}\")\n    except Exception as e:\n        LOGGER.info(f\"{prefix} export failure: {e}\")\n        return\n    return file\n\n", "CodeBleu_score": 48.825706833791685}, {"code": "def export_torchscript(model, im, file, optimize, prefix=colorstr(\"TorchScript:\"))\n\ttry:\n        LOGGER.info(f\"{prefix} starting export with torch {torch.__version__}...\")\n        f = str(file).endswith(\".pt\")\n        model.model[-1].export = not f  # set Detect() layer export=True\n        model.eval()\n        model.model = torch.jit.trace(model.model, im, check_trace=False)  # run model once\n        if optimize:\n            from torch.utils.mobile_optimizer import optimize_for_mobile\n\n            model.model = optimize_for_mobile(model.model)  # optimize for mobile\n        model.model.save(str(file))\n        LOGGER.info(f\"{prefix} export success, saved to {file}\")\n    except Exception as e:\n        LOGGER.info(f\"{prefix} export failure: {e}\")\n    finally:\n        model.model.export = False  # reset Detect() layer export\n        model.train()\n    return str(file)\n\n", "CodeBleu_score": 41.7901588355938}, {"code": "def export_torchscript(model, im, file, optimize, prefix=colorstr(\"TorchScript:\"))\n\ttry:\n        LOGGER.info(f\"{prefix} starting export with torch {torch.__version__}...\")\n        f = str(file).lower()\n        if f.endswith(\".pt\"):\n            file = Path(file).with_suffix(\".torchscript.pt\")\n        else:\n            file = Path(file).with_suffix(\".torchscript.pth\")\n        ts = torch.jit.trace(model, im, strict=False)\n        extra_files = {\"config.txt\": \"\", \"names.txt\": \"\"}  # torchscript extra files\n        extra_files.update({\"version.txt\": yolov5_version})  # yolov5 version\n        ts.save(file, _extra_files=extra_files)\n        LOGGER.info(f\"{prefix} export success, saved as {file}\")\n        return file, None\n    except Exception as e:\n        LOGGER.info(f\"{prefix} export failure: {e}\")\n        return None, e\n\n", "CodeBleu_score": 53.61563003992968}]}
{"instance_id": "78335487ac1c", "generation_result": [{"code": "def export_coreml(model, im, file, int8, half, nms, prefix=colorstr(\"CoreML:\")):\n\ttry:\n        import coremltools as ct\n    except ImportError:\n        print(\"ERROR: coremltools not installed, cannot export to CoreML. See %s\" % HELP_URLS[\"coreml\"])\n        return\n    LOGGER.info(f\"{prefix} starting export with torch {torch.__version__}...\")\n    f = file.with_suffix(\".mlmodel\")  # force .mlmodel suffix\n    if int8 or half:\n        LOGGER.info(f\"{prefix} quantizing model...\")\n        model = quantize_weights(model, int8, half)\n    if half:\n        LOGGER.info(f\"{prefix} converting to FP16 half-precision...\")\n        model = model.half()\n\n    # wrap model for NMS support\n    if nms:\n        LOGGER.info(f\"{prefix} wrapping model for NMS support...\")\n        model = iOSModel(model, nms=True)\n\n    # trace and convert model\n    LOGGER.info(f\"{prefix} tracing and converting to CoreML format...\")\n    img = im.unsqueeze(0).type(torch.cuda.HalfTensor if half else torch.cuda.FloatTensor)\n    traced_model = torch.jit.trace(model, img)\n    ct_model = ct.convert(\n        traced_model,\n        inputs=[ct.ImageType(name=\"image\", shape=img.shape, scale=1 / 255.0, bias=[0, 0, 0])],\n    )\n\n    # apply quantization\n    if int8:\n        LOGGER.info(f\"{prefix} applying quantization...\")\n        ct_model = ct_model.quantize(quantization_mode=\"linear\")\n\n    # save and return\n    LOGGER.info(f\"{prefix} saving to {f}...\")\n    ct_model.save(str(f))\n    LOGGER.info(f\"{prefix} saved to {f}.\")\n    return str(f), ct_model\n\n", "CodeBleu_score": 46.19760913518892}, {"code": "def export_coreml(model, im, file, int8, half, nms, prefix=colorstr(\"CoreML:\")):\n\ttry:\n        import coremltools as ct\n    except ImportError:\n        raise ImportError(f'CoreML export requires \"coremltools>=4.1\", install with `pip install coremltools`.')\n    LOGGER.info(f\"{prefix} starting export with torch {torch.__version__}...\")\n    assert im.shape[1] == im.shape[2], \"image must be square\"\n    assert im.max() <= 255, \"image max 255\"\n    assert im.min() >= 0, \"image min 0\"\n    assert im.dtype == np.uint8, \"image must be uint8\"\n\n    im = im.transpose(2, 0, 1)  # (c, h, w)\n    im = np.ascontiguousarray(im)\n    im = torch.from_numpy(im).unsqueeze(0)\n    im = im.to(torch.float32 if not half else torch.float16)\n\n    # wrap model with iOSModel for NMS support\n    if nms:\n        model = iOSModel(model, nms=nms)\n\n    # trace model\n    LOGGER.info(f\"{prefix} tracing {im.shape}...\")\n    traced_model = torch.jit.trace(model, im, strict=False)\n\n    # convert model\n    LOGGER.info(f\"{prefix} converting...\")\n    mlmodel = ct.convert(traced_model, inputs=[ct.ImageType(name=\"image\", shape=im.shape, scale=1 / 255.0, bias=[0, 0, 0])])\n\n    # quantize model\n    if int8:\n        LOGGER.info(f\"{prefix} quantizing...\")\n        mlmodel = quantize_weights(mlmodel, 8)\n\n    # save model\n    LOGGER.info(f\"{prefix} saving {file}...\")\n    mlmodel.save(file)\n    LOGGER.info(f\"{prefix} saved {file}.\")\n    return file, mlmodel", "CodeBleu_score": 47.97223760558846}, {"code": "def export_coreml(model, im, file, int8, half, nms, prefix=colorstr(\"CoreML:\")):\n\ttry:\n        import coremltools as ct\n    except ImportError:\n        raise ImportError(\"CoreML export requires 'coremltools>=4.0' to be installed. Run 'pip install coremltools>=4.0'\")\n    file = Path(file).with_suffix(\".mlmodel\")\n    print(f\"{prefix} starting export with torch {torch.__version__}...\")\n    if half:\n        model.half()\n    if int8:\n        model.eval()\n        model.fuse()\n        model.qconfig = torch.quantization.get_default_qconfig(\"qint8\")\n        torch.quantization.prepare(model, inplace=True)\n        torch.quantization.convert(model, inplace=True)\n    model = model.to(device)\n    model.eval()\n    with torch.no_grad():\n        model = torch.jit.trace(model, im, strict=False)  # trace model\n        model = torch.jit.freeze(model)  # freeze model\n        jit = True  # jit model\n        if nms:\n            model = iOSModel(model, nms)  # wrap model\n            jit = False\n        print(f\"{prefix} exporting with torch {torch.__version__}...\")\n        ct_model = ct.convert(\n            model,\n            inputs=[ct.ImageType(name=\"image\", shape=im.shape, scale=1 / 255.0, bias=[0, 0, 0])],\n            # image input type\n            outputs=[\"classLabels\", \"confidence\", *([f\"bboxCoordinates{i}\" for i in range(model.nu)] if nms else [])],\n            # outputs\n            class_labels=list(range(model.nc)),  # class labels\n            minimum_deployment_target=\"13\",\n            convert_to=\"neuralnetwork\",\n            jit_trace=jit,\n        )\n        if int8:\n            ct_model = ct_model.quantize(input_shapes={\"image\": [1, 3", "CodeBleu_score": 48.613379345840904}]}
{"instance_id": "80233cf71580", "generation_result": [{"code": "def generate_individual(input_ranges, individual_length)\n\tindividual = []\n    for i in range(individual_length):\n        individual.append(random.uniform(input_ranges[i][0], input_ranges[i][1]))\n    return individual", "CodeBleu_score": 46.97407887252343}, {"code": "def generate_individual(input_ranges, individual_length)\n\tindividual = []\n    for i in range(individual_length):\n        individual.append(random.uniform(input_ranges[i][0], input_ranges[i][1]))\n    return individual", "CodeBleu_score": 46.97407887252343}, {"code": "def generate_individual(input_ranges, individual_length)\n\tindividual = []\n    for i in range(individual_length):\n        individual.append(random.uniform(input_ranges[i][0], input_ranges[i][1]))\n    return individual", "CodeBleu_score": 46.97407887252343}]}
{"instance_id": "5ea979cb4a8e", "generation_result": [{"code": "def marginal_log_mean_coeff(self, t)\n\tif self.schedule_type == \"discrete\":\n        return self.interpolate_fn(t, self.t_grid, self.log_alpha_fn(self.t_grid))\n    elif self.schedule_type == \"linear\":\n        return self.log_alpha_fn(t)\n    elif self.schedule_type == \"cosine\":\n        return torch.cos(t * math.pi / 2) * self.log_alpha_fn(1) + torch.sin(t * math.pi / 2) * self.log_alpha_fn(0)\n    else:\n        raise NotImplementedError\n\n", "CodeBleu_score": 31.836470511197803}, {"code": "def marginal_log_mean_coeff(self, t)\n\tif self.schedule == \"discrete\":\n        return self.interpolate_fn(t, self.xp, self.log_alpha_fn(self.yp))\n    elif self.schedule == \"linear\":\n        return self.log_alpha_fn(\n            self.yp\n            + (self.log_alpha_fn(self.yp) - self.log_alpha_fn(self.yp[0]))\n            * (t - self.xp[0])\n            / (self.xp[-1] - self.xp[0])\n        )\n    elif self.schedule == \"cosine\":\n        return self.log_alpha_fn(\n            self.yp[0]\n            + (self.log_alpha_fn(self.yp[-1]) - self.log_alpha_fn(self.yp[0]))\n            * 0.5\n            * (1 + torch.cos(np.pi * t / self.xp[-1]))\n        )\n    else:\n        raise NotImplementedError\n\n", "CodeBleu_score": 31.548039024419673}, {"code": "def marginal_log_mean_coeff(self, t)\n\tif self.schedule_type == 'discrete':\n        return interpolate_fn(t, self.log_alpha_t_positions, self.log_alpha_t_values)\n    elif self.schedule_type == 'linear':\n        return self.log_alpha_t_positions[0] + t * self.log_alpha_t_values[0]\n    elif self.schedule_type == 'cosine':\n        return torch.cos(t * math.pi / 2) * self.log_alpha_t_values[0] + self.log_alpha_t_values[1]\n    else:\n        raise NotImplementedError", "CodeBleu_score": 30.077018731031036}]}
{"instance_id": "7da4fae5a278", "generation_result": [{"code": "def inverse_lambda(self, lamb)\n\tif self.schedule_type == \"linear\":\n        t = self.T * (lamb - self.lamb_min) / (self.lamb_max - self.lamb_min)\n    elif self.schedule_type == \"discrete\":\n        t = interpolate_fn(lamb, self.log_alpha, self.t_grid)\n    else:\n        log_alpha = torch.log(self.alpha_min) + (self.alpha_max - self.alpha_min) * lamb\n        t = self.T * torch.acos(\n            torch.sqrt(\n                torch.flip(\n                    torch.cumsum(\n                        torch.flip(torch.exp(-log_alpha), dims=[1]), dim=1\n                    ),\n                    dims=[1],\n                ),\n            )\n        )\n        t = torch.where(\n            torch.isnan(t),\n            torch.zeros_like(t),\n            torch.where(\n                torch.isinf(t),\n                torch.full_like(t, self.T),\n                torch.where(\n                    torch.eq(lamb, self.lamb_min),\n                    torch.zeros_like(t),\n                    torch.where(torch.eq(lamb, self.lamb_max), torch.full_like(t, self.T), t),\n                ),\n            ),\n        )\n    return t\n\n", "CodeBleu_score": 33.51204107564196}, {"code": "def inverse_lambda(self, lamb)\n\tif self.schedule == \"linear\":\n        # linear schedule\n        t = (lamb - self.beta_0) / (self.beta_1 - self.beta_0)\n    elif self.schedule == \"discrete\":\n        # discrete schedule\n        t = interpolate_fn(lamb, self.log_alpha, torch.arange(0, self.T, device=self.device))\n    else:\n        # other schedules\n        log_alpha = (lamb - self.beta_0) / (self.beta_1 - self.beta_0)\n        t = torch.arccos(log_alpha) / np.pi\n    return t\n\n", "CodeBleu_score": 23.37573580571581}, {"code": "def inverse_lambda(self, lamb)\n\tif self.schedule_type == \"linear\":\n        if self.beta_type == \"constant\":\n            t = lamb / self.beta\n        elif self.beta_type == \"linear\":\n            t = lamb / self.beta_fn(self.T)\n        elif self.beta_type == \"exp\":\n            t = lamb / self.beta_fn(self.T)\n        elif self.beta_type == \"log\":\n            t = lamb / self.beta_fn(self.T)\n        elif self.beta_type == \"sqrt\":\n            t = lamb / self.beta_fn(self.T)\n        else:\n            raise ValueError(f\"Unknown beta type: {self.beta_type}\")\n    elif self.schedule_type == \"discrete\":\n        t = self.interpolate_fn(lamb, self.log_alpha, self.t_fn)\n    elif self.schedule_type == \"continuous\":\n        log_alpha = self.log_alpha_fn(lamb)\n        t = torch.acos(\n            torch.sqrt(\n                (torch.exp(log_alpha) - torch.exp(self.log_alpha_min))\n                / (torch.exp(self.log_alpha_max) - torch.exp(self.log_alpha_min))\n            )\n        )\n        t = t / np.pi * self.T\n    else:\n        raise ValueError(f\"Unknown schedule type: {self.schedule_type}\")\n    return t\n\n", "CodeBleu_score": 28.110383452966314}]}
{"instance_id": "a519c853a2b2", "generation_result": [{"code": "def noise_pred_fn(x, t_continuous, cond=None)\n\tt_continuous = t_continuous.expand(x.dim(0), -1)\n    if cond is not None:\n        x = torch.cat([x, cond], dim=-1)\n    x = model(x, t_continuous)\n    x = x.reshape(x.shape[0], -1)\n    if model_type == 'classic':\n        x = x.tanh()\n    elif model_type == 'alpha':\n        x = x.tanh()\n        x = noise_schedule.marginal_alpha(t_continuous) * x\n    elif model_type == 'alpha_std':\n        x = x.tanh()\n        x = noise_schedule.marginal_alpha(t_continuous) * x\n        x = noise_schedule.marginal_std(t_continuous) * x\n    elif model_type == 'alpha_sq':\n        x = x.tanh()\n        x = noise_schedule.marginal_alpha(t_continuous) * x\n        x = noise_schedule.marginal_std(t_continuous) * x\n        x = x**2\n    elif model_type == 'alpha_sq_std':\n        x = x.tanh()\n        x = noise_schedule.marginal_alpha(t_continuous) * x\n        x = noise_schedule.marginal_std(t_continuous) * x\n        x = x**2\n        x = noise_schedule.marginal_std(t_continuous) * x\n    elif model_type == 'alpha_sq_alpha':\n        x = x.tanh()\n        x = noise_schedule.marginal_alpha(t_continuous) * x\n        x = noise_schedule.marginal_std(t_continuous) * x\n        x = x**2\n        x = noise_schedule.marginal_alpha(t_continuous) * x\n    else:\n        raise ValueError(f'Unknown model type {model_type}')\n    return x", "CodeBleu_score": 43.56694596485785}, {"code": "def noise_pred_fn(x, t_continuous, cond=None)\n\tt_expanded = t_continuous.expand(x.shape[0], -1)\n    if cond is None:\n        z = model(x, t_expanded)\n    else:\n        z = model(x, t_expanded, cond)\n    if isinstance(noise_schedule, GaussianNoise):\n        return z\n    elif isinstance(noise_schedule, HomoscedasticNoise):\n        return z\n    elif isinstance(noise_schedule, HeteroscedasticNoise):\n        return z\n    elif isinstance(noise_schedule, PoissonNoise):\n        return z\n    else:\n        raise NotImplementedError\n\n", "CodeBleu_score": 12.216533519342576}, {"code": "def noise_pred_fn(x, t_continuous, cond=None)\n\tif cond is None:\n        cond = torch.zeros(x.shape[0], 1).to(x)\n    t_expand = t_continuous.expand(x.shape[0], 1)\n    if model.type == 'classification':\n        return model(x, t_expand, cond)\n    elif model.type == 'regression':\n        return model(x, t_expand, cond)\n    elif model.type == 'hybrid':\n        pred_mean, pred_logvar = model(x, t_expand, cond)\n        pred_std = torch.exp(0.5 * pred_logvar)\n        return pred_mean, pred_std\n    else:\n        raise ValueError('unknown model type: ' + model.type)\n\n", "CodeBleu_score": 21.0556355475649}]}
{"instance_id": "96a9b7d54823", "generation_result": [{"code": "def dynamic_thresholding_fn(self, x0, t=None)\n\tif t is None:\n        t = self.threshold\n    # Compute the quantile of absolute values\n    x0_abs = torch.abs(x0)\n    x0_abs_flat = x0_abs.flatten()\n    x0_abs_flat = x0_abs_flat[x0_abs_flat > 0]\n    threshold = torch.quantile(x0_abs_flat, t)\n    # Determine the scaling factor\n    scaling_factor = torch.maximum(threshold, torch.ones_like(x0))\n    # Clamp x0 values\n    x0_clamped = torch.clamp(x0, -scaling_factor, scaling_factor)\n    # Scale x0 values\n    x0_scaled = x0_clamped / scaling_factor\n    return x0_scaled\n\n", "CodeBleu_score": 35.70982688731952}, {"code": "def dynamic_thresholding_fn(self, x0, t=None)\n\tif t is None:\n        t = self.threshold\n    # Compute the quantile of absolute values.\n    x0_abs = torch.abs(x0)\n    x0_abs_sorted = torch.sort(x0_abs, dim=0, descending=True)[0]\n    x0_abs_quantile = x0_abs_sorted[torch.floor(torch.tensor(x0_abs.shape[0] * t)).long()]\n    # Determine the scaling factor.\n    x0_abs_max = torch.maximum(x0_abs_quantile, torch.ones_like(x0_abs_quantile))\n    # Clamp x0 values.\n    x0_abs_clamped = torch.clamp(x0_abs, 0, x0_abs_max)\n    # Scale the tensor.\n    x0_scaled = x0_abs_clamped / (x0_abs_max + 1e-8)\n    return x0_scaled\n\n", "CodeBleu_score": 38.80159694633768}, {"code": "def dynamic_thresholding_fn(self, x0, t=None)\n\tif t is None:\n        t = self.t\n    x0_abs = torch.abs(x0)\n    x0_abs_max = torch.max(x0_abs)\n    if x0_abs_max == 0:\n        return torch.zeros_like(x0)\n    x0_abs_q = torch.quantile(x0_abs, t)\n    x0_abs_q = expand_dims(x0_abs_q, x0.dim())\n    x0_abs_q = x0_abs_q.to(x0.device)\n    x0_abs_q = x0_abs_q.to(x0.dtype)\n    scale = x0_abs_max / x0_abs_q\n    x0 = torch.clamp(x0, -x0_abs_max, x0_abs_max)\n    x0 = x0 / scale\n    return x0\n\n", "CodeBleu_score": 37.078780303036034}]}
{"instance_id": "59c6ec6d7a6a", "generation_result": [{"code": "def data_prediction_fn(self, x, t)\n\tx = self.model(x, t)\n    x_max = torch.max(x)\n    x_min = torch.min(x)\n    x_diff = x_max - x_min\n    x_mean = torch.mean(x)\n    x_std = torch.std(x)\n    x_diff_ratio = x_diff / x_std\n    x_threshold = x_mean + self.threshold_ratio * x_diff_ratio\n    x = torch.clamp(x, x_threshold, x_max)\n    x = (x - x_threshold) / (x_max - x_threshold)\n    return x\n\n", "CodeBleu_score": 19.669665162790512}, {"code": "def data_prediction_fn(self, x, t)\n\treturn self.model(x, t)\n\n", "CodeBleu_score": 1.7699693687413316}, {"code": "def data_prediction_fn(self, x, t)\n\treturn self.model(x, t)\n\n", "CodeBleu_score": 1.7699693687413316}]}
{"instance_id": "65e3618468a2", "generation_result": [{"code": "def predict_eps_sigma(model, input, sigma_in, **kwargs)\n\t# Reshape sigma to match the dimensions of the input\n    sigma_in = sigma_in.view(input.shape)\n    # Scale the input\n    input = input * sigma_in\n    # Predict epsilon\n    eps_pred = model(input, **kwargs)\n    # Adjust the prediction\n    eps_pred = eps_pred + (input - eps_pred) / sigma_in\n    return eps_pred\n\n", "CodeBleu_score": 29.87399679916266}, {"code": "def predict_eps_sigma(model, input, sigma_in, **kwargs)\n\t# reshape sigma to match the dimensions of the input\n    sigma_in = sigma_in.view(input.shape[0], -1)\n\n    # scale the input\n    input = input.view(input.shape[0], -1)\n    input = input / sigma_in\n\n    # compute the prediction by adjusting the model output with the scaled input and sigma\n    prediction = model(input, **kwargs)\n    prediction = prediction * sigma_in\n\n    # return the adjusted prediction\n    return prediction", "CodeBleu_score": 23.34203151567757}, {"code": "def predict_eps_sigma(model, input, sigma_in, **kwargs)\n\tsigma_in = sigma_in.view(input.shape)\n    input_scaled = input * sigma_in\n    output = model(input_scaled, **kwargs)\n    output_scaled = output * sigma_in\n    eps = output_scaled - input_scaled\n    return eps", "CodeBleu_score": 24.47130859897413}]}
{"instance_id": "b35846dfbc66", "generation_result": [{"code": "def cal_intergrand(beta_0, beta_1, taus)\n\ttaus.requires_grad_()\n    with torch.enable_grad():\n        taus.clone().requires_grad_()\n        alpha = t2alpha_fn(beta_0, beta_1, taus)\n        log_alpha = alpha.log()\n        log_alpha.sum().backward()\n        grad_log_alpha = taus.grad\n        return alpha * grad_log_alpha\n\n", "CodeBleu_score": 41.29624855892933}, {"code": "def cal_intergrand(beta_0, beta_1, taus)\n\ttaus.requires_grad_()\n    with torch.enable_grad():\n        alpha = t2alpha_fn(beta_0, beta_1, taus)\n        log_alpha = alpha.log()\n        log_alpha.sum().backward()\n        taus.requires_grad_(False)\n        return log_alpha.detach() * alpha.detach()\n\n", "CodeBleu_score": 37.82110328099698}, {"code": "def cal_intergrand(beta_0, beta_1, taus)\n\twith torch.enable_grad():\n        taus = taus.clone().requires_grad_()\n        alpha = t2alpha_fn(beta_0, beta_1, taus)\n        log_alpha = alpha.log()\n        log_alpha.sum().backward()\n        grad_log_alpha = taus.grad.clone()\n        alpha.detach_()\n        return grad_log_alpha * alpha", "CodeBleu_score": 43.35630294036051}]}
{"instance_id": "5f3667566a10", "generation_result": [{"code": "def sample_euler_ancestral(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None)\n\tif extra_args is None:\n        extra_args = {}\n    if noise_sampler is None:\n        noise_sampler = default_noise_sampler(x)\n    x = x.clone()\n    sigma_down, sigma_up = get_ancestral_step(sigmas[0], sigmas[1], eta)\n    for i, sigma in enumerate(sigmas):\n        if disable and disable(sigma):\n            continue\n        denoised = model(x, sigmas[0], **extra_args)\n        x = x - to_d(x, sigma, denoised) * sigma_down\n        x = x + noise_sampler(sigma_down, sigma_up) * sigma_up\n        x = x.detach()\n        if callback:\n            callback(x, sigma, denoised, i)\n    return x + s_noise * noise_sampler(sigmas[-1], 0.)\n\n", "CodeBleu_score": 34.854446001733194}, {"code": "def sample_euler_ancestral(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None)\n\tif extra_args is None:\n        extra_args = {}\n    if noise_sampler is None:\n        noise_sampler = default_noise_sampler(x)\n    if disable is None:\n        disable = []\n    # Initialize the noise sampler.\n    sigma_noise = noise_sampler(s_noise, 0)\n    # Iterate over the sigmas.\n    for i, sigma in enumerate(sigmas):\n        # Denoise the input.\n        denoised = model(x, sigma, **extra_args)\n        # Calculate the ancestral step.\n        sigma_down, sigma_up = get_ancestral_step(sigma_noise, sigma, eta)\n        # Update the input using the Euler method.\n        x = x + to_d(x, sigma_noise, denoised) * sigma_down\n        # Add noise if required.\n        if sigma_up:\n            x = x + sigma_up * noise_sampler(sigma_up, sigma_down)\n        # Call the callback if required.\n        if callback and (i + 1) % callback == 0:\n            callback(x, sigma, denoised)\n    # Add noise if required.\n    if s_noise:\n        x = x + s_noise * noise_sampler(s_noise, 0)\n    return x\n\n", "CodeBleu_score": 35.21218872849205}, {"code": "def sample_euler_ancestral(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None)\n\tif extra_args is None:\n        extra_args = {}\n    if noise_sampler is None:\n        noise_sampler = default_noise_sampler(x)\n    x = x.detach().clone()\n    x.requires_grad_(True)\n    for i, sigma in enumerate(sigmas):\n        if disable is not None and disable(i):\n            continue\n        sigma_down, sigma_up = get_ancestral_step(sigmas[i - 1], sigma, eta)\n        sigma_next = sigma_down\n        for _ in range(100):\n            if sigma_next < sigma_down:\n                break\n            with torch.no_grad():\n                x_denoised = model(x, sigma_next, **extra_args)\n            x_denoised.backward(torch.ones_like(x_denoised))\n            x_grad = x.grad.detach()\n            x.grad.zero_()\n            x_grad = to_d(x, sigma_next, x_denoised)\n            x_next = x - sigma_next ** 2 * x_grad\n            x_next.clamp_(-1., 1.)\n            sigma_next = (sigma_down ** 2 + sigma_up ** 2 + 2 * sigma_down * sigma_up * x_next - 2 * sigma_down * sigma_up * x) / (sigma_down ** 2 + sigma_up ** 2 - 2 * sigma_down * sigma_up * x_grad)\n        x = x_next\n        if callback is not None:\n            callback(x, i)\n    if s_noise > 0:\n        x = x + s_noise * noise_sampler(sigma, sigma_next)\n    return x\n\n", "CodeBleu_score": 34.13140544110636}]}
{"instance_id": "6261ded88df2", "generation_result": [{"code": "def linear_multistep_coeff(order, t, i, j)\n\tdef product_term(k):\n        return (t[i+k] - t[j]) / (t[i+k] - t[j+k])\n    if i + order < j:\n        raise ValueError('The order is too high for the step index.')\n    elif i + order == j:\n        return 1\n    else:\n        return integrate.quad(product_term, i, j)[0]\n\n", "CodeBleu_score": 25.903177836729196}, {"code": "def linear_multistep_coeff(order, t, i, j)\n\tdef prod(k):\n        \"\"\"This function computes the product term for the coefficient, excluding the j-th term.\"\"\"\n        if k == i:\n            return 1\n        elif k == j:\n            return 0\n        else:\n            return t[k] - t[j]\n    if order == 1:\n        return 1\n    elif order == 2:\n        return (t[i] - t[j]) / (t[i] - t[j+1])\n    elif order == 3:\n        if j == i:\n            raise ValueError(\"The order is too high for the step index.\")\n        elif j == i+1:\n            return (t[i] - t[j+1]) / (t[i] - t[j])\n        else:\n            return (t[i] - t[j]) / ((t[i] - t[j]) * (t[i] - t[j+1]))\n    elif order == 4:\n        if j == i:\n            raise ValueError(\"The order is too high for the step index.\")\n        elif j == i+1:\n            return (t[i] - t[j+1]) / (t[i] - t[j])\n        elif j == i+2:\n            return (t[i] - t[j]) / ((t[i] - t[j]) * (t[i] - t[j+1]))\n        else:\n            return (t[i] - t[j]) / ((t[i] - t[j]) * (t[i] - t[j+1]) * (t[i] - t[j+2]))\n    elif order == 5:\n        if j == i:\n            raise ValueError(\"The order is too high for the step index.\")\n        elif j == i+1:\n            return (t[i] - t[j+1]) / (t[i] - t[j])\n        elif j == i+2:\n            return (t[i] - t[j]) / ((t[i] - t[j]) * (t[i] - t[j+1]))\n        elif j == i+3:", "CodeBleu_score": 16.75878397145989}, {"code": "def linear_multistep_coeff(order, t, i, j)\n\tdef prod(k):\n        return t[k] - t[i]\n    if order == 1:\n        return 1\n    elif order == 2:\n        if j == 0:\n            return 1\n        else:\n            return (t[i+1] - t[i]) / (t[j+1] - t[j])\n    elif order == 3:\n        if j == 0:\n            return 1\n        elif j == 1:\n            return (t[i+1] - t[i]) / (t[j+1] - t[j])\n        else:\n            return ((t[i+1] - t[i]) / (t[j+1] - t[j])) * ((t[i+1] - t[i]) / (t[j+1] - t[j]))\n    elif order == 4:\n        if j == 0:\n            return 1\n        elif j == 1:\n            return (t[i+1] - t[i]) / (t[j+1] - t[j])\n        elif j == 2:\n            return ((t[i+1] - t[i]) / (t[j+1] - t[j])) * ((t[i+1] - t[i]) / (t[j+1] - t[j]))\n        else:\n            return ((t[i+1] - t[i]) / (t[j+1] - t[j])) * ((t[i+1] - t[i]) / (t[j+1] - t[j])) * ((t[i+1] - t[i]) / (t[j+1] - t[j]))\n    elif order == 5:\n        if j == 0:\n            return 1\n        elif j == 1:\n            return (t[i+1] - t[i]) / (t[j+1] - t[j])\n        elif j == 2:\n            return ((t[i+1] - t[i]) / (t[j+1] - t[j])) * ((t[i+1] - t[i]) / (t", "CodeBleu_score": 15.472167576936519}]}
{"instance_id": "4ee025e33ab1", "generation_result": [{"code": "def sample_lcm(model, x, sigmas, extra_args=None, callback=None, disable=None, noise_sampler=None)\n\tif extra_args is None:\n        extra_args = {}\n    if disable is None:\n        disable = []\n    if noise_sampler is None:\n        noise_sampler = default_noise_sampler(x)\n    for i, sigma in enumerate(sigmas):\n        x_next = model(x, sigma, **extra_args)\n        if callback is not None:\n            callback(x_next, i, sigma)\n        if i + 1 < len(sigmas) and sigmas[i + 1] > 0:\n            noise_scaling(x_next, sigma, sigmas[i + 1], noise_sampler)\n        x = x_next\n    return x\n\n", "CodeBleu_score": 34.17308039802164}, {"code": "def sample_lcm(model, x, sigmas, extra_args=None, callback=None, disable=None, noise_sampler=None)\n\tif extra_args is None:\n        extra_args = {}\n    if noise_sampler is None:\n        noise_sampler = default_noise_sampler(x)\n    if disable is None:\n        disable = []\n    for i, sigma in enumerate(sigmas):\n        if callback is not None:\n            callback(i, sigma)\n        if sigma > 0:\n            noise = noise_sampler(sigma, sigmas[i + 1])\n            if 'noise_scaling' in extra_args:\n                noise = noise * extra_args['noise_scaling']\n            x = model(x, noise=noise, **extra_args)\n        else:\n            x = model(x, **extra_args)\n        if 'trange' in extra_args:\n            extra_args['trange'] = extra_args['trange'][1:]\n    return x", "CodeBleu_score": 36.89051206302727}, {"code": "def sample_lcm(model, x, sigmas, extra_args=None, callback=None, disable=None, noise_sampler=None)\n\tif extra_args is None:\n        extra_args = {}\n    if noise_sampler is None:\n        noise_sampler = default_noise_sampler(x)\n    x = x.detach().clone()\n    for i, sigma in enumerate(sigmas):\n        if disable is not None and disable[i]:\n            continue\n        x_next = model(x, sigma, **extra_args)\n        if callback is not None:\n            callback(x_next, sigma, **extra_args)\n        if sigma_next > 0:\n            noise = noise_sampler(sigma, sigma_next)\n            x_next = x_next + noise_scaling(x_next, sigma, sigma_next) * noise\n        x = x_next\n    return x\n\n", "CodeBleu_score": 36.04681799431725}]}
{"instance_id": "cb51f91da606", "generation_result": [{"code": "def append_dims(x, target_dims)\n\tif x.dim() > target_dims:\n        raise ValueError(\"The target dimensions must be greater than or equal to the current dimensions.\")\n    elif x.dim() == target_dims:\n        return x\n    else:\n        x = x.detach() if x.device.type == \"mps\" else x\n        return x.expand(*x.shape, *[1 for _ in range(target_dims - x.dim())])", "CodeBleu_score": 34.43200631296557}, {"code": "def append_dims(x, target_dims)\n\tif x.dim() > target_dims:\n        raise ValueError(\n            \"The number of target dimensions must be greater than or equal to the current number of dimensions.\"\n        )\n    elif x.dim() == target_dims:\n        return x\n    else:\n        return x.expand(\n            *x.shape, *(1 for i in range(target_dims - x.dim()))\n        ).detach() if x.device.type == \"mps\" else x.expand(\n            *x.shape, *(1 for i in range(target_dims - x.dim()))\n        )\n\n", "CodeBleu_score": 31.393837703253837}, {"code": "def append_dims(x, target_dims)\n\tif x.dim() < target_dims:\n        if x.device.type == 'mps':\n            return x.clone().detach().expand(tuple([1 for i in range(target_dims - x.dim())] + list(x.shape)))\n        else:\n            return x.expand(tuple([1 for i in range(target_dims - x.dim())] + list(x.shape)))\n    else:\n        raise ValueError(f\"Cannot append dimensions to a tensor with {x.dim()} dimensions to reach a target of {target_dims} dimensions\")", "CodeBleu_score": 31.90354679647892}]}
{"instance_id": "9fac874a3bc7", "generation_result": [{"code": "def download_file(path, url, digest=None)\n\tif not os.path.exists(path):\n        os.makedirs(path)\n    file_path = os.path.join(path, os.path.basename(url))\n    if not os.path.exists(file_path):\n        with open(file_path, \"wb\") as f:\n            response = urllib.request.urlopen(url)\n            shutil.copyfileobj(response, f)\n    if digest is not None:\n        with open(file_path, \"rb\") as f:\n            if hashlib.sha256(f.read()).hexdigest() != digest:\n                raise OSError(\n                    \"File {} does not match expected SHA-256 digest {}\".format(\n                        file_path, digest\n                    )\n                )\n    return file_path", "CodeBleu_score": 48.39409819459455}, {"code": "def download_file(path, url, digest=None)\n\tif not os.path.exists(path):\n        os.makedirs(path)\n    file_path = os.path.join(path, os.path.basename(url))\n    if os.path.exists(file_path):\n        if digest is not None:\n            with open(file_path, 'rb') as f:\n                sha256 = hashlib.sha256(f.read()).hexdigest()\n            if sha256 != digest:\n                raise OSError('File {} exists but hash does not match.'.format(file_path))\n        return file_path\n    with open(file_path, 'wb') as f:\n        response = urllib.request.urlopen(url)\n        shutil.copyfileobj(response, f)\n    if digest is not None:\n        with open(file_path, 'rb') as f:\n            sha256 = hashlib.sha256(f.read()).hexdigest()\n        if sha256 != digest:\n            raise OSError('File {} hash does not match.'.format(file_path))\n    return file_path\n\n\n\n\n", "CodeBleu_score": 46.852388414274415}, {"code": "def download_file(path, url, digest=None)\n\tif not os.path.exists(path):\n        os.makedirs(path)\n    path = os.path.join(path, os.path.basename(url))\n    if not os.path.exists(path):\n        with open(path, 'wb') as f:\n            f.write(urlopen(url).read())\n    if digest:\n        with open(path, 'rb') as f:\n            if hashlib.sha256(f.read()).hexdigest() != digest:\n                raise OSError('File {} has an SHA-256 digest of {}, expected {}'.format(path, hashlib.sha256(f.read()).hexdigest(), digest))\n    return path", "CodeBleu_score": 44.369162828992856}]}
{"instance_id": "f23165e897ce", "generation_result": [{"code": "def train_mode(model, mode=True)\n\tif mode:\n        return model.train()\n    else:\n        return model.eval()", "CodeBleu_score": 13.369412496655281}, {"code": "def train_mode(model, mode=True)\n\tdef set_train_mode(module, mode):\n        module.training = mode\n        for module in module.children():\n            set_train_mode(module, mode)\n\n    r\"\"\"Sets the module in training mode.\"\"\"\n    if mode:\n        model.train()\n    else:\n        model.eval()\n\n", "CodeBleu_score": 34.79359017488208}, {"code": "def train_mode(model, mode=True)\n\tpass\n", "CodeBleu_score": 3.0330882352941217}]}
{"instance_id": "c6d59c6cb8e1", "generation_result": [{"code": "def ema_update(model, averaged_model, decay)\n\tmodel_params = dict(model.named_parameters())\n    averaged_params = dict(averaged_model.named_parameters())\n    model_buffers = dict(model.named_buffers())\n    averaged_buffers = dict(averaged_model.named_buffers())\n\n    # Check that both models have the same parameters and buffers\n    assert model_params.keys() == averaged_params.keys()\n    assert model_buffers.keys() == averaged_buffers.keys()\n\n    # Update averaged model's parameters\n    for name, param in averaged_params.items():\n        param.data.mul_(decay).add_(1 - decay, model_params[name].data)\n\n    # Copy the current model's buffers to the averaged model\n    for name, buf in averaged_buffers.items():\n        buf.data.copy_(model_buffers[name].data)\n\n", "CodeBleu_score": 61.23297066280243}, {"code": "def ema_update(model, averaged_model, decay)\n\tmodel_params = {k: v for k, v in model.named_parameters()}\n    averaged_params = {k: v for k, v in averaged_model.named_parameters()}\n    model_buffers = {k: v for k, v in model.named_buffers()}\n    averaged_buffers = {k: v for k, v in averaged_model.named_buffers()}\n\n    # check if the models have the same parameters and buffers\n    assert model_params.keys() == averaged_params.keys()\n    assert model_buffers.keys() == averaged_buffers.keys()\n\n    # update the parameters of the averaged model\n    for name, param in averaged_params.items():\n        averaged_params[name].data.mul_(decay).add_(1 - decay, model_params[name].data)\n\n    # copy the buffers of the current model to the averaged model\n    for name, buf in averaged_buffers.items():\n        averaged_buffers[name].data.copy_(model_buffers[name].data)", "CodeBleu_score": 56.23802958331239}, {"code": "def ema_update(model, averaged_model, decay)\n\tmodel_buffers = dict(model.named_buffers())\n    averaged_buffers = dict(averaged_model.named_buffers())\n    model_params = dict(model.named_parameters())\n    averaged_params = dict(averaged_model.named_parameters())\n    for name, averaged_param in averaged_params.items():\n        averaged_param.data.mul_(decay).add_(1 - decay, model_params[name].data)\n    for name, averaged_buffer in averaged_buffers.items():\n        averaged_buffer.data.copy_(model_buffers[name].data)", "CodeBleu_score": 48.187764035230344}]}
{"instance_id": "600c39a30798", "generation_result": [{"code": "def rand_log_logistic(shape, loc=0., scale=1., min_value=0., max_value=float('inf'), device='cpu', dtype=torch.float32)\n\tif not isinstance(shape, torch.Size):\n        shape = torch.Size(shape)\n    min_value = torch.as_tensor(min_value, dtype=dtype, device=device)\n    max_value = torch.as_tensor(max_value, dtype=dtype, device=device)\n    u = torch.rand(shape, dtype=dtype, device=device)\n    u = min_value.log() - max_value.log() * u + torch.rand(shape, dtype=dtype, device=device).log()\n    return torch.as_tensor(loc, dtype=dtype, device=device) + scale * (u.logit() + 1).exp()\n", "CodeBleu_score": 43.0291246485157}, {"code": "def rand_log_logistic(shape, loc=0., scale=1., min_value=0., max_value=float('inf'), device='cpu', dtype=torch.float32)\n\tmin_value = torch.as_tensor(min_value, dtype=dtype, device=device)\n    max_value = torch.as_tensor(max_value, dtype=dtype, device=device)\n    u = torch.rand(shape, dtype=dtype, device=device)\n    return torch.exp(loc + scale * torch.logit(u * (max_value.log() - min_value.log()) + min_value.log()))", "CodeBleu_score": 30.418237144370053}, {"code": "def rand_log_logistic(shape, loc=0., scale=1., min_value=0., max_value=float('inf'), device='cpu', dtype=torch.float32)\n\tmin_value = torch.as_tensor(min_value, device=device, dtype=dtype)\n    max_value = torch.as_tensor(max_value, device=device, dtype=dtype)\n    u = torch.rand(shape, device=device, dtype=dtype)\n    return torch.exp(\n        torch.log(u) + torch.log(scale) + loc\n    ) * (\n        torch.exp(min_value.log() - loc) - torch.exp(max_value.log() - loc)\n    ) + torch.exp(loc)", "CodeBleu_score": 39.078065973120424}]}
{"instance_id": "75e2def2467f", "generation_result": [{"code": "def get_activation(activation: Literal[\"elu\", \"snake\", \"none\"], antialias=False, channels=None) -> nn.Module\n\tif activation == \"elu\":\n        if antialias:\n            return Activation1d(nn.ELU())\n        else:\n            return nn.ELU()\n    elif activation == \"snake\":\n        if antialias:\n            return Activation1d(SnakeBeta(channels))\n        else:\n            return SnakeBeta(channels)\n    elif activation == \"none\":\n        return Identity()\n    else:\n        raise ValueError(f\"Unknown activation type: {activation}\")", "CodeBleu_score": 45.60511359733491}, {"code": "def get_activation(activation: Literal[\"elu\", \"snake\", \"none\"], antialias=False, channels=None) -> nn.Module\n\tif activation == \"elu\":\n        return ELU()\n    elif activation == \"snake\":\n        return SnakeBeta(channels)\n    elif activation == \"none\":\n        return Identity()\n    else:\n        raise ValueError(f\"Unknown activation type {activation}\")\n\n", "CodeBleu_score": 42.54980539975436}, {"code": "def get_activation(activation: Literal[\"elu\", \"snake\", \"none\"], antialias=False, channels=None) -> nn.Module\n\tif activation == \"elu\":\n        if antialias:\n            return Activation1d(nn.ELU())\n        else:\n            return nn.ELU()\n    elif activation == \"snake\":\n        if antialias:\n            return Activation1d(SnakeBeta(channels))\n        else:\n            return SnakeBeta(channels)\n    elif activation == \"none\":\n        if antialias:\n            return Activation1d(nn.Identity())\n        else:\n            return nn.Identity()\n    else:\n        raise ValueError(f\"Activation {activation} not recognized.\")", "CodeBleu_score": 40.05321536627607}]}
{"instance_id": "ef1714c79e51", "generation_result": [{"code": "def apply_rotary_pos_emb(t, freqs, scale = 1)\n\tif t.dtype != torch.float32:\n        t = t.float()\n\n    freqs = freqs.type_as(t)\n\n    if t.dim() == 2:\n        t = t.unsqueeze(0)\n\n    n, h, device = *t.shape[:2], t.device\n    freqs = freqs.to(device)\n\n    t_rotated = t * freqs.cos()\n    t_unrotated = t * freqs.sin()\n\n    t_rotated = rearrange(t_rotated, '... (n d) -> ... n d', n = n)\n    t_unrotated = rearrange(t_unrotated, '... (n d) -> ... n d', n = n)\n\n    t_rotated = rotate_half(t_rotated)\n    t_rotated = t_rotated.reshape(n, h, -1).type_as(t)\n\n    t = t_rotated + t_unrotated\n    return t", "CodeBleu_score": 36.512058184828504}, {"code": "def apply_rotary_pos_emb(t, freqs, scale = 1)\n\tt = t.float()\n    freqs = freqs.float()\n    original_shape, t = t.shape, rearrange(t, '... n d -> ... (n d)')\n    rot_dim = freqs.shape[-1]\n    t_rotated, t_unrotated = t[..., :rot_dim], t[..., rot_dim:]\n    t_rotated = (t_rotated * freqs.cos()) + (rotate_half(t_rotated) * freqs.sin())\n    t = torch.cat((t_rotated, t_unrotated), dim = -1)\n    return rearrange(t, '... (n d) -> ... n d', n = original_shape[-2]) * scale\n\n", "CodeBleu_score": 39.69115486961189}, {"code": "def apply_rotary_pos_emb(t, freqs, scale = 1)\n\tfreqs = freqs.type(t.dtype).to(t.device)\n    t_unrotated = t\n    if t.dim() == 3:\n        t = rearrange(t, 'b n d -> b n () d')\n    if freqs.dim() == 3:\n        freqs = rearrange(freqs, 'n () d -> () n d')\n    t_rotated = (t * freqs.cos()) + (rotate_half(t) * freqs.sin())\n    t = t_unrotated * scale + t_rotated * (1 - scale)\n    return t\n\n", "CodeBleu_score": 30.597488893751578}]}
{"instance_id": "a7a5f178c6f6", "generation_result": [{"code": "def idx2vq(self, idx, dim=-1)\n\tif not isinstance(idx, torch.Tensor):\n        idx = torch.tensor(idx)\n    if idx.device != self.codebook.device:\n        idx = idx.to(self.codebook.device)\n    if idx.dtype != self.codebook.dtype:\n        idx = idx.to(self.codebook.dtype)\n    embeddings = self.codebook[idx]\n    if dim != -1:\n        embeddings = embeddings.movedim(-1, dim)\n    return embeddings\n\n", "CodeBleu_score": 55.41009220236488}, {"code": "def idx2vq(self, idx, dim=-1)\n\tembeddings = self.codebook[idx]\n    if dim != -1:\n        embeddings = embeddings.movedim(-1, dim)\n    return embeddings\n\n", "CodeBleu_score": 34.30773438926686}, {"code": "def idx2vq(self, idx, dim=-1)\n\tif dim == -1:\n        return self.codebook[idx]\n    else:\n        return self.codebook[idx].movedim(-1, dim)", "CodeBleu_score": 30.941835611563878}]}
{"instance_id": "4104c041fb20", "generation_result": [{"code": "def encode(self, x, quantize=False)\n\tfor block in self.blocks:\n        x = block(x)\n    if quantize:\n        z_q_x, (vq_loss, commit_loss), indices = self.forward(x)\n        return z_q_x, x, indices, (vq_loss, commit_loss)\n    else:\n        return x\n\n", "CodeBleu_score": 42.86202984930577}, {"code": "def encode(self, x, quantize=False)\n\tx = self.in_block(x)\n    if self.down_blocks is not None:\n        for block in self.down_blocks:\n            x = block(x)\n    x = self.out_block(x)\n    if self.quantize:\n        x, (vq_loss, commit_loss), indices = self.forward(x, get_losses=True)\n        return x, x.detach(), indices, (vq_loss, commit_loss)\n    return x\n\n", "CodeBleu_score": 29.680241660091784}, {"code": "def encode(self, x, quantize=False)\n\tif self.in_block is not None:\n        x = self.in_block(x)\n    for block in self.blocks:\n        x = block(x)\n    if self.down_blocks is not None:\n        x = self.down_blocks(x)\n    if quantize:\n        z_q_x, (vq_loss, commit_loss), indices = self.forward(x, get_losses=True)\n        return z_q_x, x, indices, (vq_loss, commit_loss)\n    else:\n        return x\n\n", "CodeBleu_score": 51.83332919897939}]}
{"instance_id": "403c52ba2d3b", "generation_result": [{"code": "def ema_scope(self, context=None)\n\tif context is not None:\n        self.logger.info(f\"Switching to EMA parameters in {context}\")\n    self.store(self.parameters())\n    self.copy_to(self.parameters())\n    if context is not None:\n        self.logger.info(f\"Restoring parameters after {context}\")\n    self.restore(self.parameters())\n\n", "CodeBleu_score": 45.41205793712616}, {"code": "def ema_scope(self, context=None)\n\tif self.ema_decay > 0:\n        self.store(self.parameters())\n        self.copy_to(self)\n        if context:\n            context.log(f\"EMA: using EMA parameters\")\n    try:\n        yield\n    finally:\n        if self.ema_decay > 0:\n            self.restore(self.parameters())\n            if context:\n                context.log(f\"EMA: restored original parameters\")", "CodeBleu_score": 46.571578313535426}, {"code": "def ema_scope(self, context=None)\n\tif self.ema is not None:\n        if context is not None:\n            context.log(\"Using EMA weights\")\n        self.ema.store(self.parameters())\n        self.ema.copy_to(self)\n        self.ema_applied = True\n    try:\n        yield\n    finally:\n        if self.ema is not None:\n            self.ema.restore(self.parameters())\n            self.ema_applied = False\n            if context is not None:\n                context.log(\"Restored parameters\")\n\n", "CodeBleu_score": 62.88604886196292}]}
{"instance_id": "94b7467f98e4", "generation_result": [{"code": "def post_attention(self, attn, x, gate_msa, shift_mlp, scale_mlp, gate_mlp)\n\tassert not self.pre_only\n    x = self.proj(x)\n    x = self.proj_drop(x)\n    x = x + self.gate_msa(gate_msa) * attn\n    x = self.norm1(x)\n    x = self.modulate(x, shift_mlp, scale_mlp)\n    x = self.gate_mlp(gate_mlp) * self.mlp(x)\n    x = x + self.drop_path(x)\n    return x\n\n", "CodeBleu_score": 53.88325087962793}, {"code": "def post_attention(self, attn, x, gate_msa, shift_mlp, scale_mlp, gate_mlp)\n\tif self.pre_only:\n        return x\n    x = x + self.attn_drop(attn)\n    x = self.norm1(x)\n    x = self.mlp(x)\n    x = self.proj(x)\n    x = self.proj_drop(x)\n    return x\n\n", "CodeBleu_score": 38.47590116128949}, {"code": "def post_attention(self, attn, x, gate_msa, shift_mlp, scale_mlp, gate_mlp)\n\tx = x + gate_msa * attn\n    x = self.norm1(x)\n    x = x + self.mlp(self.norm2(x)) * gate_mlp\n    return x\n\n", "CodeBleu_score": 34.43237929000266}]}
{"instance_id": "d9fdc05542a2", "generation_result": [{"code": "def _block_mixing(context, x, context_block, x_block, c)\n\t# Pre-attention\n    qkv_context = context_block.pre_attention(context)\n    qkv_x = x_block.pre_attention(x)\n    # Attention\n    qkv = (torch.cat([qkv_context[0], qkv_x[0]], dim=2),\n           torch.cat([qkv_context[1], qkv_x[1]], dim=2),\n           torch.cat([qkv_context[2], qkv_x[2]], dim=2))\n    attn = optimized_attention(qkv, context_block.num_heads)\n    # Post-attention\n    context = context_block.post_attention(attn[:, :, :c])\n    x = x_block.post_attention(attn[:, :, c:])\n    return context, x\n\n", "CodeBleu_score": 17.027089376377916}, {"code": "def _block_mixing(context, x, context_block, x_block, c)\n\tB, L, C = x.shape\n    qkv = context_block.pre_attention(context)\n    x_qkv = x_block.pre_attention(x)\n    qkv = torch.cat((qkv[0], x_qkv[0]), dim=2)\n    k = torch.cat((qkv[1], x_qkv[1]), dim=2)\n    v = torch.cat((qkv[2], x_qkv[2]), dim=2)\n    attn_out = optimized_attention(qkv, context_block.num_heads)\n    context_out, x_out = torch.split(attn_out, [context.shape[1], x.shape[1]], dim=1)\n    context = context_block.post_attention(context_out)\n    x = x_block.post_attention(x_out)\n    return context, x\n\n", "CodeBleu_score": 21.37302136512316}, {"code": "def _block_mixing(context, x, context_block, x_block, c)\n\t# Pre-attention\n    q, k, v = context_block.pre_attention(context)\n    q_x, k_x, v_x = x_block.pre_attention(x)\n    # Attention\n    qkv = (q, k, v)\n    qkv_x = (q_x, k_x, v_x)\n    qkv = torch.cat((qkv, qkv_x), dim=1)\n    attn_out = optimized_attention(qkv, context_block.num_heads)\n    q, k, v, q_x, k_x, v_x = split_qkv(attn_out, context_block.head_dim)\n    # Post-attention\n    context = context_block.post_attention(q)\n    x = x_block.post_attention(q_x)\n    # Update context\n    context = context.reshape(context.shape[0], context.shape[1], -1)\n    context = torch.cat((context, x), dim=1)\n    return context\n\n", "CodeBleu_score": 19.742187521955685}]}
{"instance_id": "b219bce30160", "generation_result": [{"code": "def get_timestep_embedding(timesteps, embedding_dim)\n\tassert len(timesteps.shape) == 1, 'timesteps input must be a one-dimensional tensor'\n    assert embedding_dim % 2 == 0, 'embedding dimension must be even'\n    half_dim = embedding_dim // 2\n    emb = math.log(10000) / (half_dim - 1)\n    emb = torch.exp(torch.arange(half_dim, dtype=torch.float32) * -emb)\n    emb = timesteps.float()[:, None] * emb[None, :]\n    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n    if embedding_dim % 2 == 1:\n        emb = torch.nn.functional.pad(emb, (0, 1))\n    return emb", "CodeBleu_score": 76.20281213010409}, {"code": "def get_timestep_embedding(timesteps, embedding_dim)\n\tassert len(timesteps.shape) == 1, 'timesteps input must be a 1D tensor'\n    assert embedding_dim % 2 == 0, 'embedding dimension must be even'\n    # Create a range of timesteps from 0 to the input length\n    timesteps = torch.arange(timesteps.shape[0], device=timesteps.device)\n    # Calculate the sine and cosine embeddings\n    sinusoid_inp = torch.cat((timesteps.float()[:, None].sin(), timesteps.float()[:, None].cos()), dim=1)\n    # Scale the sine and cosine embeddings by the logarithm of the sequence length\n    sinusoid_inp = sinusoid_inp * math.log(10000) ** (-torch.arange(embedding_dim // 2, device=timesteps.device) / embedding_dim)\n    # Add the scaled embeddings to the original embeddings\n    sinusoid_inp = torch.cat((sinusoid_inp.sin(), sinusoid_inp.cos()), dim=1)\n    # Pad the embeddings if the embedding dimension is odd\n    if embedding_dim % 2 == 1:\n        sinusoid_inp = torch.nn.functional.pad(sinusoid_inp, (0, 1, 0, 0))\n    return sinusoid_inp", "CodeBleu_score": 46.06919720938462}, {"code": "def get_timestep_embedding(timesteps, embedding_dim)\n\tassert len(timesteps.shape) == 1, 'timesteps input must be a 1D tensor'\n    assert timesteps.dtype == torch.int64, 'timesteps input must be of type int64'\n    assert embedding_dim % 2 == 0, 'embedding dimension must be even'\n    half_dim = embedding_dim // 2\n    emb = math.log(10000) / (half_dim - 1)\n    emb = torch.exp(torch.arange(half_dim, dtype=torch.float32) * -emb)\n    emb = timesteps.float()[:, None] * emb[None, :]\n    emb = torch.cat((torch.sin(emb), torch.cos(emb)), dim=1)\n    if embedding_dim % 2 == 1:\n        emb = torch.nn.functional.pad(emb, (0, 1))\n    return emb", "CodeBleu_score": 69.6343059124304}]}
{"instance_id": "6ae8453c86be", "generation_result": [{"code": "def pytorch_attention(q, k, v)\n\tq = q.contiguous().view(q.shape[0], -1, q.shape[-1]).transpose(0, 1)\n    k = k.contiguous().view(k.shape[0], -1, k.shape[-1]).transpose(0, 1)\n    v = v.contiguous().view(v.shape[0], -1, v.shape[-1]).transpose(0, 1)\n\n    try:\n        r1 = torch.nn.functional.multi_head_attention_forward(q, k, v,\n                                                              embed_dim_to_check=v.shape[-1],\n                                                              num_heads=1,\n                                                              in_proj_weight=None,\n                                                              in_proj_bias=None,\n                                                              bias_k=None,\n                                                              bias_v=None,\n                                                              add_zero_attn=False,\n                                                              dropout_p=0,\n                                                              out_proj_weight=None,\n                                                              out_proj_bias=None,\n                                                              training=False,\n                                                              key_padding_mask=None,\n                                                              need_weights=False,\n                                                              attn_mask=None,\n                                                              use_separate_proj_weight=False,\n                                                              q_proj_weight=None,\n                                                              k_proj_weight=None,\n                                                              v_proj_weight=None,\n                                                              static_k=None,\n                                                              static_v=None)[0]\n    except RuntimeError as e:\n        if isinstance(e, model_management.OOM_EXCEPTION):\n            logging.warning(\"out of memory error, switching to slice attention\")\n            r1 = slice_attention(q, k, v)\n        else:\n            raise e\n\n    return r1.transpose(0, 1).contiguous().view(q.shape[1], q.shape[0], -1)", "CodeBleu_score": 30.5771895449068}, {"code": "def pytorch_attention(q, k, v)\n\t# reshape and transpose tensors\n    q = q.contiguous().view(q.shape[0], -1, q.shape[-1]).transpose(0, 1)\n    k = k.contiguous().view(k.shape[0], -1, k.shape[-1]).transpose(0, 1)\n    v = v.contiguous().view(v.shape[0], -1, v.shape[-1]).transpose(0, 1)\n\n    # compute attention\n    try:\n        r = torch.nn.functional.multi_head_attention_forward(q, k, v,\n                                                             embed_dim_to_check=v.shape[-1],\n                                                             num_heads=1,\n                                                             in_proj_weight=None,\n                                                             in_proj_bias=None,\n                                                             bias_k=None,\n                                                             bias_v=None,\n                                                             add_zero_attn=False,\n                                                             dropout_p=0,\n                                                             out_proj_weight=None,\n                                                             out_proj_bias=None,\n                                                             training=True,\n                                                             key_padding_mask=None,\n                                                             need_weights=False,\n                                                             attn_mask=None,\n                                                             use_separate_proj_weight=False,\n                                                             q_proj_weight=None,\n                                                             k_proj_weight=None,\n                                                             v_proj_weight=None,\n                                                             static_k=None,\n                                                             static_v=None)[0]\n    except RuntimeError as e:\n        if \"out of memory\" in str(e):\n            logging.warning(\"out of memory error, switching to slice attention\")\n            r = slice_attention(q, k, v)\n        else:\n            raise e\n\n    # reshape and transpose back\n    r = r.transpose(0, 1).contiguous().view(q.shape[1], q.shape[0", "CodeBleu_score": 30.30502772744975}, {"code": "def pytorch_attention(q, k, v)\n\ttry:\n        return torch.nn.functional.multi_head_attention_forward(q, k, v, model_management.get_free_memory(q.device), 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,", "CodeBleu_score": 10.107852456178538}]}
{"instance_id": "fe4433c987ad", "generation_result": [{"code": "def apply_control(h, control, name)\n\tif control and name:\n        try:\n            h.add(control.pop(name))\n        except (KeyError, AttributeError):\n            logging.warning(f\"Could not add {name} control to {h.name}.\")\n    return h\n\n\n\n", "CodeBleu_score": 21.88864859273455}, {"code": "def apply_control(h, control, name)\n\tif control and name:\n        try:\n            h = h.add(control.pop(name))\n        except:\n            logging.warning(f\"Could not apply {name} control to {h.name}.\")\n    return h\n\n", "CodeBleu_score": 31.003079799655875}, {"code": "def apply_control(h, control, name)\n\tif control and name:\n        try:\n            h = h.add(control.pop(name))\n        except Exception as e:\n            logging.warning(f\"Failed to add {name} to {h.name}.", "CodeBleu_score": 28.334116898734298}]}
{"instance_id": "56da4547c101", "generation_result": [{"code": "def get_codebook_entry(self, indices, shape)\n\tone_hot = F.one_hot(indices, num_classes=self.num_embeddings)\n    z_q = torch.matmul(one_hot.float(), self.embedding.weight)\n    if shape is not None:\n        z_q = z_q.view(shape)\n        z_q = z_q.permute(0, 3, 1, 2).contiguous()\n    return z_q\n", "CodeBleu_score": 53.25441253446886}, {"code": "def get_codebook_entry(self, indices, shape)\n\tone_hot = F.one_hot(indices.view(-1), self.num_embeddings).type(self.weight.dtype)\n    z_q = torch.matmul(one_hot, self.weight.t())\n    z_q = z_q.view(shape)\n    z_q = z_q.permute(0, 3, 1, 2).contiguous()\n    return z_q\n\n", "CodeBleu_score": 26.889609032890654}, {"code": "def get_codebook_entry(self, indices, shape)\n\tone_hot = F.one_hot(indices, num_classes=self.embedding.shape[0])\n    z_q = torch.matmul(one_hot.float(), self.embedding)\n    if shape is not None:\n        z_q = z_q.view(shape)\n        z_q = z_q.permute(0, 3, 1, 2).contiguous()\n    return z_q\n\n", "CodeBleu_score": 54.44666234199052}]}
{"instance_id": "548f8a1fb37d", "generation_result": [{"code": "def make_noise(self)\n\tnoise = torch.randn(4, 4, device=self.device)\n    noise_list = [noise]\n    for i in range(3, self.log_size):\n        noise = torch.randn(2 ** i, 2 ** i, device=self.device)\n        noise_list.append(noise)\n        noise_list.append(noise)\n    return noise_list", "CodeBleu_score": 34.36948739363935}, {"code": "def make_noise(self)\n\tnoise = torch.randn(4, 4).to(self.device)\n    self.noises.append(noise)\n    for size in range(2**3, 2**self.log_size, 2):\n        noise = torch.randn(size, size).to(self.device)\n        self.noises.append(noise)\n        self.noises.append(noise.transpose(0, 1))", "CodeBleu_score": 28.039304998208415}, {"code": "def make_noise(self)\n\tnoise = torch.randn(4, 4, device=self.device)\n    self.noise = [noise]\n    for size in range(4, self.log_size + 1):\n        self.noise.append(torch.randn(2 ** size, 2 ** size, device=self.device))\n        self.noise.append(torch.randn(2 ** size, 2 ** size, device=self.device))\n    return self.noise", "CodeBleu_score": 31.382590247705917}]}
{"instance_id": "42fd805563c6", "generation_result": [{"code": "def mean_latent(self, num_latent)\n\t# Create a random latent vector\n    latent = torch.randn(1, num_latent, 1, 1, device=self.device)\n    # Process the latent vector through the style MLP\n    latent = self.style_mlp(latent)\n    # Compute the mean of the processed latent vector\n    mean_latent = latent.mean(dim=0).detach()\n    # Return the mean latent vector\n    return mean_latent", "CodeBleu_score": 38.33017785219518}, {"code": "def mean_latent(self, num_latent)\n\tlatent = torch.randn(1, num_latent)\n    latent = self.style_mlp(latent)\n    latent = latent.mean(0, keepdim=True)\n    return latent", "CodeBleu_score": 20.190522411886004}, {"code": "def mean_latent(self, num_latent)\n\tlatents = torch.randn(num_latent, self.latent_dim, device=self.device)\n    latents = self.style_mlp(latents)\n    mean_latent = torch.mean(latents, dim=0)\n    return mean_latent\n\n", "CodeBleu_score": 29.374965371503198}]}
{"instance_id": "2f485fd924af", "generation_result": [{"code": "def color_jitter(img, shift)\n\tjitter = np.random.uniform(-shift, shift, 3)\n    img = img + jitter\n    img = np.clip(img, 0, 1)\n    return img\n\n", "CodeBleu_score": 54.67597839821603}, {"code": "def color_jitter(img, shift)\n\tjitter = np.random.randint(-shift, shift, (3,))\n    img = img + jitter\n    img = np.clip(img, 0, 1)\n    return img\n\n", "CodeBleu_score": 52.09226148249378}, {"code": "def color_jitter(img, shift)\n\tshift = np.random.uniform(-shift, shift, 3)\n    img = img + shift\n    img = np.clip(img, 0, 1)\n    return img\n\n\n\n\n\n", "CodeBleu_score": 53.05120057815961}]}
{"instance_id": "c87ccf1fbf48", "generation_result": [{"code": "def feed_data(self, data)\n\tif self.device is not None:\n        if torch.is_tensor(data['lq']):\n            self.lq = data['lq'].to(self.device)\n        else:\n            self.lq = [d.to(self.device) for d in data['lq']]\n        if 'gt' in data.keys():\n            if torch.is_tensor(data['gt']):\n                self.gt = data['gt'].to(self.device)\n            else:\n                self.gt = [d.to(self.device) for d in data['gt']]\n        if 'face_landmarks' in data.keys():\n            self.left_eye = data['face_landmarks'][0].to(self.device)\n            self.right_eye = data['face_landmarks'][1].to(self.device)\n            self.mouth = data['face_landmarks'][2].to(self.device)\n\n    else:\n        self.lq = data['lq']\n        if 'gt' in data.keys():\n            self.gt = data['gt']\n        if 'face_landmarks' in data.keys():\n            self.left_eye = data['face_landmarks'][0]\n            self.right_eye = data['face_landmarks'][1]\n            self.mouth = data['face_landmarks'][2]\n\n\n\n\n\n", "CodeBleu_score": 39.22408337685961}, {"code": "def feed_data(self, data)\n\tself.lq = data['lq'].to(self.device)\n    if 'gt' in data:\n        self.gt = data['gt'].to(self.device)\n    if 'eye_pos' in data:\n        self.left_eye_pos = data['eye_pos'][0].to(self.device)\n        self.right_eye_pos = data['eye_pos'][1].to(self.device)\n    if 'mouth_pos' in data:\n        self.mouth_pos = data['mouth_pos'].to(self.device)", "CodeBleu_score": 55.78598952611947}, {"code": "def feed_data(self, data)\n\tself.lq = data['lq'].to(self.device)\n    if 'gt' in data:\n        self.gt = data['gt'].to(self.device)\n    if 'gt_landmark' in data:\n        self.gt_landmark = data['gt_landmark'].to(self.device)\n        self.left_eye = data['gt_landmark'][:, :2, :].to(self.device)\n        self.right_eye = data['gt_landmark'][:, 2:4, :].to(self.device)\n        self.mouth = data['gt_landmark'][:, 4:, :].to(self.device)\n\n", "CodeBleu_score": 53.368714855523436}]}
{"instance_id": "8f8c7cb13037", "generation_result": [{"code": "def construct_img_pyramid(self)\n\tpyramid_gt = [self.gt]\n    for i in range(self.num_scales):\n        pyramid_gt.insert(0, F.interpolate(pyramid_gt[0], scale_factor=0.5, mode='bilinear', align_corners=False))\n    return pyramid_gt", "CodeBleu_score": 27.18911309878958}, {"code": "def construct_img_pyramid(self)\n\tpyramid_gt = []\n    pyramid_gt.insert(0, self.gt)\n    for i in range(self.num_levels):\n        pyramid_gt.insert(0, F.interpolate(pyramid_gt[0], scale_factor=0.5, mode='bilinear'))\n    return pyramid_gt", "CodeBleu_score": 27.83484535707228}, {"code": "def construct_img_pyramid(self)\n\tpyramid_gt = [self.gt]\n    for i in range(1, self.num_scales):\n        pyramid_gt.insert(0, F.interpolate(pyramid_gt[0], scale_factor=0.5, mode='bilinear', align_corners=False))\n    return pyramid_gt", "CodeBleu_score": 27.44610823251249}]}
{"instance_id": "51247274d40c", "generation_result": [{"code": "def save(self, epoch, current_iter)\n\tif self.save_network:\n        self.save_network_state(epoch, current_iter)\n    if self.save_training_state:\n        self.save_training_state(epoch, current_iter)", "CodeBleu_score": 24.08155535559028}, {"code": "def save(self, epoch, current_iter)\n\tif self.opt.save_network:\n        print('saving the network...')\n        save_filename = 'netG_epoch_%03d_iter_%03d.pth' % (epoch, current_iter)\n        save_path = os.path.join(self.save_dir, save_filename)\n        torch.save(self.netG.state_dict(), save_path)\n        save_filename = 'netG_ema_epoch_%03d_iter_%03d.pth' % (epoch, current_iter)\n        save_path = os.path.join(self.save_dir, save_filename)\n        torch.save(self.netG_ema.state_dict(), save_path)\n        save_filename = 'netD_epoch_%03d_iter_%03d.pth' % (epoch, current_iter)\n        save_path = os.path.join(self.save_dir, save_filename)\n        torch.save(self.netD.state_dict(), save_path)\n        if self.opt.use_facial_discriminator:\n            save_filename = 'netD_left_eye_epoch_%03d_iter_%03d.pth' % (epoch, current_iter)\n            save_path = os.path.join(self.save_dir, save_filename)\n            torch.save(self.netD_left_eye.state_dict(), save_path)\n            save_filename = 'netD_right_eye_epoch_%03d_iter_%03d.pth' % (epoch, current_iter)\n            save_path = os.path.join(self.save_dir, save_filename)\n            torch.save(self.netD_right_eye.state_dict(), save_path)\n            save_filename = 'netD_mouth_epoch_%03d_iter_%03d.pth' % (epoch, current_iter)\n            save_path = os.path.join(self.save_", "CodeBleu_score": 32.6867975027762}, {"code": "def save(self, epoch, current_iter)\n\tself.save_network(self.net_G, 'G', current_iter)\n    self.save_network(self.net_G_ema, 'G_ema', current_iter)\n    self.save_network(self.net_D, 'D', current_iter)\n    if self.opt.use_eye_d:\n        self.save_network(self.net_D_eye, 'D_eye', current_iter)\n    if self.opt.use_mouth_d:\n        self.save_network(self.net_D_mouth, 'D_mouth', current_iter)\n    self.save_training_state(epoch, current_iter)", "CodeBleu_score": 61.86004686150823}]}
{"instance_id": "afa0dc8128c1", "generation_result": [{"code": "def clean_folder(folder)\n\tfor item in os.listdir(folder):\n        item_path = os.path.join(folder, item)\n        if os.path.isfile(item_path):\n            try:\n                os.unlink(item_path)\n            except Exception as e:\n                print(f\"Error deleting {item_path}: {e}\")\n        elif os.path.islink(item_path):\n            try:\n                os.unlink(item_path)\n            except Exception as e:\n                print(f\"Error deleting {item_path}: {e}\")\n        elif os.path.isdir(item_path):\n            try:\n                shutil.rmtree(item_path)\n            except Exception as e:\n                print(f\"Error deleting {item_path}: {e}\")", "CodeBleu_score": 47.560246200137}, {"code": "def clean_folder(folder)\n\tfor filename in os.listdir(folder):\n        file_path = os.path.join(folder, filename)\n        try:\n            if os.path.isfile(file_path) or os.path.islink(file_path):\n                os.unlink(file_path)\n            elif os.path.isdir(file_path):\n                shutil.rmtree(file_path)\n        except Exception as e:\n            print('Failed to delete %s. Reason: %s' % (file_path, e))", "CodeBleu_score": 72.08101009891512}, {"code": "def clean_folder(folder)\n\tfor item in os.listdir(folder):\n        path = os.path.join(folder, item)\n        try:\n            if os.path.isfile(path) or os.path.islink(path):\n                os.unlink(path)\n            elif os.path.isdir(path):\n                shutil.rmtree(path)\n        except Exception as e:\n            print('Failed to delete %s. Reason: %s' % (path, e))", "CodeBleu_score": 52.44670441639686}]}
{"instance_id": "53588b481bdd", "generation_result": [{"code": "def next_batch(self, sample_size=1)\n\tif sample_size > len(self.experiences):\n        sample_size = len(self.experiences)\n    batch = random.sample(self.experiences, sample_size)\n    for experience in batch:\n        self.experiences.remove(experience)\n    return batch", "CodeBleu_score": 49.55684932968932}, {"code": "def next_batch(self, sample_size=1)\n\tif sample_size >= len(self.experiences):\n        sample_size = len(self.experiences)\n    sample_indices = random.sample(range(len(self.experiences)), sample_size)\n    sample = [self.experiences[i] for i in sample_indices]\n    self.experiences = [self.experiences[i] for i in range(len(self.experiences)) if i not in sample_indices]\n    return sample", "CodeBleu_score": 47.39685840081113}, {"code": "def next_batch(self, sample_size=1)\n\tif sample_size > len(self.experiences):\n        sample_size = len(self.experiences)\n\n    batch = random.sample(self.experiences, sample_size)\n    for experience in batch:\n        self.experiences.remove(experience)\n\n    return batch", "CodeBleu_score": 49.55684932968932}]}
{"instance_id": "6bfa6a4e621c", "generation_result": [{"code": "def find_estimated_top_configs(self)\n\tall_configs = self.all_configs\n    cost_model = self.cost_model\n    metric = self.metric\n    if all_configs is None:\n        raise ValueError(\"No configurations have been evaluated yet. Please call 'fit' first.\")\n    if cost_model is None:\n        raise ValueError(\"No cost model has been specified. Please call 'set_cost_model' first.\")\n    if metric is None:\n        raise ValueError(\"No metric has been specified. Please call 'set_metric' first.\")\n    if metric not in [\"time\", \"accuracy\", \"loss\"]:\n        raise ValueError(\"Metric must be one of 'time', 'accuracy', or 'loss'.\")\n    if metric == \"time\" and not self.time_predictor_type:\n        raise ValueError(\"No time predictor has been specified. Please call 'set_time_predictor_type' first.\")\n    if metric == \"accuracy\" and not self.accuracy_predictor_type:\n        raise ValueError(\"No accuracy predictor has been specified. Please call 'set_accuracy_predictor_type' first.\")\n    if metric == \"loss\" and not self.loss_predictor_type:\n        raise ValueError(\"No loss predictor has been specified. Please call 'set_loss_predictor_type' first.\")\n    if metric == \"time\":\n        predictor_type = self.time_predictor_type\n    elif metric == \"accuracy\":\n        predictor_type = self.accuracy_predictor_type\n    else:\n        predictor_type = self.loss_predictor_type\n    if predictor_type == \"forest\":\n        predictor = self.cost_model.predictor\n    else:\n        predictor = self.cost_model.predictor.predictor\n    if metric == \"time\":\n        feature_val = self.cost_model.time_normalization_constants\n    elif metric == \"accuracy\":\n        feature_val = self.cost_model.accuracy_normalization_constants\n    else:\n        feature_val = self.cost_model.loss_normalization_constants\n    len_feature", "CodeBleu_score": 11.684032433457988}, {"code": "def find_estimated_top_configs(self)\n\tall_configs = self.get_all_configs()\n    feature_val = []\n    for config in all_configs:\n        config_flat = flatten(config)\n        feature_val.append(np.array([config_flat[key] for key in self.feature_names]))\n    feature_val = np.array(feature_val)\n    predict = self.cost_model.predict(feature_val)\n    if self.metric == 'time':\n        idx = np.argsort(predict)\n    else:\n        idx = np.argsort(predict)[::-1]\n    return idx\n\n\n\n\n", "CodeBleu_score": 25.558736051757847}, {"code": "def find_estimated_top_configs(self)\n\tflattened_ds_config = self.flatten_configs(self.all_configs)\n\n    # convert to numpy array\n    flattened_ds_config = np.array(list(flattened_ds_config.values()))\n\n    # predict\n    configs = self.model.predict(flattened_ds_config)\n\n    # sort\n    if self.metric == 'accuracy':\n        configs = np.argsort(configs)[::-1]\n    elif self.metric == 'loss':\n        configs = np.argsort(configs)\n\n    # return top configs\n    return configs[:self.top_k]\n\n", "CodeBleu_score": 22.46840851265159}]}
{"instance_id": "d4cf48413a57", "generation_result": [{"code": "def flatten(d, parent_key='', sep='_')\n\titems = []\n    for k, v in d.items():\n        new_key = parent_key + sep + k if parent_key else k\n        if isinstance(v, dict):\n            items.extend(flatten(v, new_key, sep=sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)\n\n", "CodeBleu_score": 73.62692265992665}, {"code": "def flatten(d, parent_key='', sep='_')\n\titems = []\n    for k, v in d.items():\n        new_key = parent_key + sep + k if parent_key else k\n        if isinstance(v, collections.abc.MutableMapping):\n            items.extend(flatten(v, new_key, sep=sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)\n\n", "CodeBleu_score": 82.16984618063623}, {"code": "def flatten(d, parent_key='', sep='_')\n\titems = []\n    for k, v in d.items():\n        new_key = parent_key + sep + k if parent_key else k\n        if isinstance(v, collections.abc.MutableMapping):\n            items.extend(flatten(v, new_key, sep=sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)", "CodeBleu_score": 82.16984618063623}]}
{"instance_id": "c2a514249c25", "generation_result": [{"code": "def run_after_tuning(self)\n\tif self.optimal_cmd:\n        logger.info(f\"Running command {self.optimal_cmd} after tuning\")\n        result = subprocess.Popen(self.optimal_cmd, shell=True)\n        result.wait()\n    else:\n        logger.info(f\"No optimal command found. Autotuning did not find an optimal configuration.\")", "CodeBleu_score": 55.409518919379884}, {"code": "def run_after_tuning(self)\n\tif self.optimal_cmd:\n        logger.info(\"Running the optimal command: %s\" % self.optimal_cmd)\n        result = subprocess.Popen(self.optimal_cmd, shell=True)\n        result.wait()\n        logger.info(\"Finished running the optimal command: %s\" % self.optimal_cmd)\n    else:\n        logger.info(\"No optimal command found\")", "CodeBleu_score": 51.14957820447906}, {"code": "def run_after_tuning(self)\n\tif self.optimal_cmd:\n        logger.info(\"Running the optimal command: %s\" % self.optimal_cmd)\n        result = subprocess.Popen(self.optimal_cmd, shell=True)\n        result.wait()\n        logger.info(\"Finished running the optimal command.\")\n    else:\n        logger.info(\"Autotuning did not find an optimal configuration.\")", "CodeBleu_score": 52.66239783901354}]}
{"instance_id": "61802de52df0", "generation_result": [{"code": "def run_job(self, exp: dict, reservations)\n\tself.set_job_params(exp, reservations)\n    self.update_user_args(exp)\n    self.update_result_dir(exp)\n    self.update_master_port(exp)\n    thread = threading.Thread(\n        target=self.run_script, args=(exp[\"script\"], exp[\"user_args\"]))\n    thread.start()\n    self.running_experiments[str(time.time())] = thread", "CodeBleu_score": 15.461545136774008}, {"code": "def run_job(self, exp: dict, reservations)\n\tself.logger.info(\"Running job {}\".format(exp[\"id\"]))\n    self.logger.info(\"Experiment: {}\".format(exp))\n    self.logger.info(\"Reservations: {}\".format(reservations))\n    # Set experiment parameters\n    exp[\"master_port\"] = self.get_val_by_key(reservations, \"master_port\")\n    exp[\"result_dir\"] = self.get_val_by_key(reservations, \"result_dir\")\n    # Update user arguments\n    user_args = exp[\"user_args\"]\n    for arg_mapping in exp[\"arg_mappings\"]:\n        user_args[user_args.index(arg_mapping[\"user_arg\"])] = arg_mapping[\"value\"]\n    # Create a new thread to run the experiment\n    t = threading.Thread(target=self.run_exp, args=(exp,))\n    # Add the thread to the running experiments list\n    self.running_experiments[time.time()] = t\n    # Start the thread\n    t.start()", "CodeBleu_score": 28.022318033501502}, {"code": "def run_job(self, exp: dict, reservations)\n\texp_id = exp['exp_id']\n    exp_script = exp['exp_script']\n    exp_args = exp['exp_args']\n    exp_reservations = exp['exp_reservations']\n    exp_reservations.extend(reservations)\n    exp_user_args = exp['exp_user_args']\n    exp_user_args.append('--master_port ' + str(self.master_port))\n    exp_user_args.append('--result_dir ' + self.result_dir)\n    for mapping in exp['arg_mappings']:\n        if mapping['key'] in exp_user_args:\n            index = exp_user_args.index(mapping['key'])\n            exp_user_args[index] = mapping['key']\n            exp_user_args.insert(index + 1, get_val_by_key(mapping['value'], exp_reservations))\n        else:\n            exp_user_args.append(mapping['key'])\n            exp_user_args.append(get_val_by_key(mapping['value'], exp_reservations))\n    t = threading.Thread(target=self.run_exp, args=(exp_script, exp_args, exp_user_args))\n    t.start()\n    self.running_experiments[time.time()] = {'exp_id': exp_id, 'exp_reservations': exp_reservations}", "CodeBleu_score": 27.911516791379242}]}
{"instance_id": "4884e2e9665c", "generation_result": [{"code": "def save_exp_results_to_database(self, message, ranks=None, path=None):\n\tif dist.is_initialized() and dist.get_rank() != 0:\n        return\n    elif dist.is_initialized() and dist.get_rank() == 0:\n        if ranks is not None:\n            message['rank'] = ranks\n        if path is not None:\n            with open(path, 'a') as outfile:\n                json.dump(message, outfile)\n                outfile.write('", "CodeBleu_score": 33.29414597396345}, {"code": "def save_exp_results_to_database(self, message, ranks=None, path=None):\n\tif dist.is_initialized() and dist.get_rank() == 0:\n        if ranks is not None:\n            message = {rank: message for rank in ranks}\n        else:\n            message = {dist.get_rank(): message}\n        if path is not None:\n            with open(path, \"a\") as outfile:\n                json.dump(message, outfile)\n                outfile.write(\"", "CodeBleu_score": 33.18936695404335}, {"code": "def save_exp_results_to_database(self, message, ranks=None, path=None):\n\tif dist.is_initialized() and dist.get_rank() == 0:\n        message['rank'] = dist.get_rank()\n        with open(path, 'a') as outfile:\n            json.dump(message, outfile)\n            outfile.write('", "CodeBleu_score": 25.028916255902473}]}
{"instance_id": "0429344c3c4d", "generation_result": [{"code": "def _build_tp_other_layer_map(self, layer_index: int)\n\t# Check if layer exists\n    if layer_index not in self.layers:\n        raise ValueError(f\"Layer with index {layer_index} does not exist.\")\n\n    # Get files with layer prefix\n    layer_files = self.get_files_with_prefix(f\"{self.layer_prefix}{layer_index}\")\n\n    # Partition data\n    partition_map = self.partition_data(layer_files, self.tp_degree)\n\n    return partition_map\n\n", "CodeBleu_score": 29.433360546072183}, {"code": "def _build_tp_other_layer_map(self, layer_index: int)\n\t# Check if there are any layer files\n    if not self.layer_files:\n        raise ValueError(\"No layer files found!\")\n\n    # Verify the layer index\n    if layer_index < 0 or layer_index >= len(self.layer_files):\n        raise ValueError(f\"Layer index out of range! (layer_index = {layer_index}, number of layers = {len(self.layer_files)})\")\n\n    # Get files with the correct prefix\n    layer_files = get_files_with_prefix(self.layer_files[layer_index], self.file_prefix)\n\n    # Partition the files based on tp_degree\n    tp_degree_file_map = partition_data(layer_files, self.tp_degree)\n\n    # Create a dictionary mapping partition indices to their corresponding file lists\n    tp_other_layer_map = {}\n    for partition_index in tp_degree_file_map:\n        tp_other_layer_map[partition_index] = tp_degree_file_map[partition_index]\n\n    return tp_other_layer_map", "CodeBleu_score": 42.08814633933447}, {"code": "def _build_tp_other_layer_map(self, layer_index: int)\n\tif not self._layer_files:\n        raise RuntimeError(\"No layer files found.\")\n    if layer_index < 0 or layer_index >= self.num_layers:\n        raise IndexError(f\"Layer index out of range. Layer index must be between 0 and {self.num_layers - 1}\")\n    tp_degree = self._layer_files[layer_index].tp_degree\n    if tp_degree == 0:\n        return {}\n    tp_other_layer_files = get_files_with_prefix(self._layer_files[layer_index].path, self._layer_files[layer_index].prefix)\n    tp_other_layer_files = partition_data(tp_other_layer_files, tp_degree)\n    tp_other_layer_map = {}\n    for tp_index, files in enumerate(tp_other_layer_files):\n        tp_other_layer_map[tp_index] = files\n    return tp_other_layer_map\n\n", "CodeBleu_score": 36.36027544186698}]}
{"instance_id": "1b65ecb9be93", "generation_result": [{"code": "def system_info()\n\tinfo = {}\n    info['os'] = platform.system()\n    info['arch'] = platform.architecture()[0]\n    info['version'] = platform.version()\n    info['processor'] = platform.processor()\n    info['python_version'] = platform.python_version()\n    return info", "CodeBleu_score": 42.640527915259995}, {"code": "def system_info()\n\tsystem_info = {\n        \"os\": platform.system(),\n        \"architecture\": platform.architecture(),\n        \"version\": platform.version(),\n        \"processor\": platform.processor(),\n        \"python_version\": platform.python_version()\n    }\n    return system_info", "CodeBleu_score": 64.90833291242829}, {"code": "def system_info()\n\tinfo = {'Operating System': platform.system(),\n            'Architecture': platform.architecture(),\n            'Version': platform.version(),\n            'Processor': platform.processor(),\n            'Python Version': platform.python_version()}\n    return info", "CodeBleu_score": 50.482779021379876}]}
{"instance_id": "969bcc5cf3b5", "generation_result": [{"code": "def cuda_info()\n\timport pynvml\n    import numpy as np\n    import torch\n    pynvml.nvmlInit()\n    deviceCount = pynvml.nvmlDeviceGetCount()\n    print(\"Device count:\", deviceCount)\n    device_name = []\n    for i in range(deviceCount):\n        handle = pynvml.nvmlDeviceGetHandleByIndex(i)\n        name = pynvml.nvmlDeviceGetName(handle)\n        device_name.append(name)\n    print(\"Device names:\", device_name)\n    device_availability = []\n    for i in range(deviceCount):\n        handle = pynvml.nvmlDeviceGetHandleByIndex(i)\n        if pynvml.nvmlDeviceGetTemperature(handle, pynvml.NVML_TEMPERATURE_GPU) != -1:\n            device_availability.append(\"Available\")\n        else:\n            device_availability.append(\"Not Available\")\n    print(\"Device availability:\", device_availability)\n    print(\"CUDA version:\", torch.version.cuda)\n    pynvml.nvmlShutdown()\n\n", "CodeBleu_score": 36.675034890466016}, {"code": "def cuda_info()\n\timport pynvml\n    import numpy as np\n    import os\n    import platform\n    import torch\n    from subprocess import check_output\n\n    # CUDA version\n    try:\n        cuda_version = check_output([\"nvcc\", \"--version\"]).decode(\"utf-8\").split(\"release\")[1].split(\",\")[0].strip()\n    except FileNotFoundError:\n        cuda_version = \"Not found\"\n\n    # CUDA device info\n    try:\n        pynvml.nvmlInit()\n        device_count = pynvml.nvmlDeviceGetCount()\n        device_names = []\n        for i in range(device_count):\n            handle = pynvml.nvmlDeviceGetHandleByIndex(i)\n            device_name = pynvml.nvmlDeviceGetName(handle).decode(\"utf-8\")\n            device_names.append(device_name)\n        is_available = True\n    except pynvml.NVMLError_LibraryNotFound:\n        device_names = [\"Not found\"]\n        device_count = 0\n        is_available = False\n\n    # Environment info\n    env_dict = {}\n    env_dict[\"CUDA\"] = cuda_version\n    env_dict[\"CUDA Device Names\"] = device_names\n    env_dict[\"Device Count\"] = device_count\n    env_dict[\"CUDA available\"] = is_available\n    env_dict[\"PyTorch CUDA build\"] = torch.version.cuda\n    env_dict[\"PyTorch version\"] = torch.__version__\n    env_dict[\"Torchvision version\"] = torchvision.__version__\n    env_dict[\"OS\"] = platform.system()\n    env_dict[\"Python version\"] = platform.python_version()\n    env_dict[\"CUDA_VISIBLE_DEVICES\"] = os.environ.get(\"CUDA_VISIBLE_DEVICES\", \"Not found\")\n    env_dict[\"CUDA_HOME\"] = os.environ.get(\"CUDA_HOME\", \"Not found\")\n\n    return env_dict", "CodeBleu_score": 35.67330329568177}, {"code": "def cuda_info()\n\timport pynvml\n    pynvml.nvmlInit()\n    deviceCount = pynvml.nvmlDeviceGetCount()\n    devices = []\n    for i in range(deviceCount):\n        handle = pynvml.nvmlDeviceGetHandleByIndex(i)\n        name = pynvml.nvmlDeviceGetName(handle)\n        devices.append(name)\n    pynvml.nvmlShutdown()\n    return devices\n\n", "CodeBleu_score": 22.289176026694328}]}
{"instance_id": "a2ba908a4c70", "generation_result": [{"code": "def compute_encoder_accuracy(dataset_items, encoder_manager)\n\t# Retrieve the class name key and optional class ID to class name mapping from the encoder manager's configuration\n    class_name_key = encoder_manager.config.class_name_key\n    class_id_to_class_name_mapping = encoder_manager.config.class_id_to_class_name_mapping\n    # Initialize the class accuracy dictionary\n    class_acc_dict = {}\n    # Iterate through the dataset\n    for item in tqdm(dataset_items):\n        # Extract the audio file\n        audio_file = item[\"audio_file\"]\n        # Compute the embedding\n        embedding = encoder_manager.compute_embedding_from_clip(audio_file)\n        # Predict the class label if criteria and mappings are available\n        if class_name_key in item and class_id_to_class_name_mapping:\n            # Retrieve the true class name\n            true_class_name = item[class_name_key]\n            # Retrieve the predicted class label\n            predicted_class_label = encoder_manager.predict_class_label(embedding)\n            # Retrieve the predicted class name\n            predicted_class_name = class_id_to_class_name_mapping[predicted_class_label]\n            # Update the class accuracy dictionary\n            if true_class_name not in class_acc_dict:\n                class_acc_dict[true_class_name] = {\"count\": 0, \"correct\": 0}\n            class_acc_dict[true_class_name][\"count\"] += 1\n            if true_class_name == predicted_class_name:\n                class_acc_dict[true_class_name][\"correct\"] += 1\n    # Calculate and print the accuracy for each class\n    for class_name in class_acc_dict:\n        count = class_acc_dict[class_name][\"count\"]\n        correct = class_acc_dict[class_name][\"correct\"]\n        acc = float(correct) / count\n        print(f\"{class_name} accuracy: {acc * 100:.2f}% ({correct}/{count})\")\n    # Calculate and print", "CodeBleu_score": 30.247540485939044}, {"code": "def compute_encoder_accuracy(dataset_items, encoder_manager)\n\t# Retrieve class name key and optional class ID to class name mapping from the encoder manager's configuration.\n    class_name_key = encoder_manager.config[\"class_name_key\"]\n    class_id_to_class_name = encoder_manager.config[\"class_id_to_class_name\"]\n\n    # Initialize a dictionary to store the accuracy for each class.\n    class_acc_dict = {}\n\n    # Iterate through all items in the dataset.\n    for item in tqdm(dataset_items):\n        # Extract the audio file.\n        audio = item[\"audio\"]\n\n        # Compute the embedding.\n        embedding = encoder_manager.compute_embedding_from_clip(audio)\n\n        # If criteria and mappings are available, predict the class label.\n        if class_name_key and class_id_to_class_name:\n            # Predict the class label.\n            predicted_class_id = encoder_manager.predict_class_from_embedding(embedding)\n            predicted_class_name = class_id_to_class_name[predicted_class_id]\n\n            # If the predicted class name is not in the dictionary, initialize it to zero.\n            if predicted_class_name not in class_acc_dict:\n                class_acc_dict[predicted_class_name] = 0\n\n            # If the true class name is not in the dictionary, initialize it to zero.\n            if item[class_name_key] not in class_acc_dict:\n                class_acc_dict[item[class_name_key]] = 0\n\n            # If the predicted class name is equal to the true class name, increment the accuracy for the class.\n            if predicted_class_name == item[class_name_key]:\n                class_acc_dict[predicted_class_name] += 1\n\n    # Print the accuracy for each class.\n    for class_name, acc in class_acc_dict.items():\n        print(f\"{class_name}: {acc / len(dataset_items)}\")\n\n    # Calculate and print the average accuracy across all classes.\n    acc_list = list(class_", "CodeBleu_score": 29.01084617643881}, {"code": "def compute_encoder_accuracy(dataset_items, encoder_manager)\n\tclass_names = encoder_manager.config.get(\"class_names\")\n    class_id_to_class_name = encoder_manager.config.get(\"class_id_to_class_name\")\n    class_acc_dict = {}\n    for class_name in class_names:\n        class_acc_dict[class_name] = {\"num_correct\": 0, \"num_total\": 0}\n    for item in tqdm(dataset_items, desc=\"Computing encoder accuracy\"):\n        audio_file = item.get(\"audio_file\")\n        if audio_file:\n            embedding = encoder_manager.compute_embedding_from_clip(audio_file)\n            predicted_class_name = encoder_manager.inference(embedding)\n            if predicted_class_name:\n                true_class_name = item.get(\"class_name\")\n                if true_class_name:\n                    class_acc_dict[true_class_name][\"num_total\"] += 1\n                    if predicted_class_name == true_class_name:\n                        class_acc_dict[true_class_name][\"num_correct\"] += 1\n    class_acc_dict = {\n        class_name: {\n            \"accuracy\": float(num_correct) / num_total,\n            \"num_total\": num_total,\n            \"num_correct\": num_correct,\n        }\n        for class_name, num_correct, num_total in class_acc_dict.items()\n    }\n    class_acc_dict[\"average\"] = {\n        \"accuracy\": sum(\n            class_acc_dict[class_name][\"accuracy\"] * class_acc_dict[class_name][\"num_total\"]\n            for class_name in class_names\n        )\n        / sum(class_acc_dict[class_name][\"num_total\"] for class_name in class_names),\n        \"num_total\": sum(class_acc_dict[class_name][\"num_total\"] for class_name in class_names),\n        \"num_correct\": sum(class_acc_dict[class_name", "CodeBleu_score": 33.515670379762824}]}
{"instance_id": "e01ec69ad58a", "generation_result": [{"code": "def adjust_path_and_remove_silence(audio_path)\n\t# adjust the path\n    audio_path = audio_path.replace(args.input_dir, args.output_dir)\n\n    # if force is not specified and the file already exists, skip it\n    if not args.force and pathlib.Path(audio_path).exists():\n        return audio_path, False\n\n    # make the directories if needed\n    mkdir(audio_path)\n\n    # remove silence\n    out_path, is_speech = remove_silence(\n        model_and_utils, audio_path, audio_path, vad_sample_rate=args.vad_sample_rate, use_cuda=args.use_cuda\n    )\n\n    return out_path, is_speech", "CodeBleu_score": 37.636934042768196}, {"code": "def adjust_path_and_remove_silence(audio_path)\n\t# adjust the output path\n    output_path = audio_path.replace(args.input_dir, args.output_dir)\n    mkdir(output_path)\n\n    # remove silence\n    output_path, is_speech = remove_silence(\n        model_and_utils, audio_path, output_path, vad_sample_rate=args.vad_sample_rate, trim_just_beginning_and_end=args.trim_just_beginning_and_end, use_cuda=args.use_cuda\n    )\n\n    return output_path, is_speech", "CodeBleu_score": 30.449876214497067}, {"code": "def adjust_path_and_remove_silence(audio_path)\n\t# adjust the path\n    audio_path = audio_path.replace(args.input_dir, args.output_dir)\n    # if the file exists and force is not specified, return the output path and a False flag\n    if pathlib.Path(audio_path).exists() and not args.force:\n        return audio_path, False\n    # otherwise, create the necessary directory structure\n    pathlib.Path(audio_path).parent.mkdir(parents=True, exist_ok=True)\n    # remove silence from the audio using specified parameters\n    out_path, is_speech = remove_silence(\n        model_and_utils, audio_path, audio_path, use_cuda=args.use_cuda, vad_sample_rate=args.vad_sample_rate\n    )\n    # return the output path and a flag indicating if speech was detected\n    return out_path, is_speech\n\n\n", "CodeBleu_score": 46.289210021274954}]}
{"instance_id": "7e0bdb0feff8", "generation_result": [{"code": "def resample_files(input_dir, output_sr, output_dir=None, file_ext=\"wav\", n_jobs=10)\n\tif output_dir is None:\n        output_dir = input_dir\n    else:\n        copytree(input_dir, output_dir)\n    files = glob.glob(os.path.join(output_dir, \"**\", f\"*.{file_ext}\"), recursive=True)\n    with Pool(n_jobs) as p:\n        with tqdm(total=len(files)) as pbar:\n            for _ in p.imap_unordered(\n                partial(resample_file, output_sr=output_sr), files\n            ):\n                pbar.update()\n    print(f\"Resampled {len(files)} files.\")", "CodeBleu_score": 39.731017795292644}, {"code": "def resample_files(input_dir, output_sr, output_dir=None, file_ext=\"wav\", n_jobs=10)\n\t# If output directory is None, use input directory\n    if output_dir is None:\n        output_dir = input_dir\n\n    # Copy input directory to output directory\n    copytree(input_dir, output_dir)\n\n    # Get all audio files in output directory\n    audio_files = glob.glob(join(output_dir, \"**\", f\"*.{file_ext}\"), recursive=True)\n\n    # Resample audio files in parallel\n    with Pool(n_jobs) as p:\n        pbar = tqdm(total=len(audio_files))\n        p.imap_unordered(\n            func=resample_file,\n            iterable=zip(audio_files, repeat(output_sr)),\n            callback=lambda _: pbar.update(1),\n        )\n    pbar.close()\n\n    print(\"Resampling complete!\")\n\n", "CodeBleu_score": 42.05990250071583}, {"code": "def resample_files(input_dir, output_sr, output_dir=None, file_ext=\"wav\", n_jobs=10)\n\tif output_dir is None:\n        output_dir = input_dir\n    else:\n        copytree(input_dir, output_dir)\n    files = glob.glob(output_dir + \"**/*.\" + file_ext, recursive=True)\n    with Pool(processes=n_jobs) as p:\n        with tqdm(total=len(files)) as pbar:\n            p.imap_unordered(\n                partial(\n                    resample_file,\n                    output_sr=output_sr,\n                    output_dir=output_dir,\n                    file_ext=file_ext,\n                    pbar=pbar,\n                ),\n                files,\n            )\n    print(\"Resampling complete.\")\n\n", "CodeBleu_score": 37.395316575565055}]}
{"instance_id": "a1351834ca57", "generation_result": [{"code": "def evaluation(model, criterion, data_loader, global_step)\n\tmodel.eval()\n    eval_loss = 0\n    num_classes_in_batch = 0\n    embeddings = None\n    labels = None\n    with torch.no_grad():\n        for batch_idx, (inputs, targets) in enumerate(data_loader):\n            inputs = inputs.view(inputs.shape[0], -1)\n            inputs = inputs.cuda()\n            targets = targets.cuda()\n            outputs = model(inputs)\n            outputs = outputs.view(outputs.shape[0], -1)\n            loss = criterion(outputs, targets)\n            eval_loss += loss.item()\n            num_classes_in_batch += len(targets)\n            if embeddings is None:\n                embeddings = outputs.detach().cpu().numpy()\n                labels = targets.detach().cpu().numpy()\n            else:\n                embeddings = np.concatenate((embeddings, outputs.detach().cpu().numpy()))\n                labels = np.concatenate((labels, targets.detach().cpu().numpy()))\n\n    eval_loss /= num_classes_in_batch\n    dashboard_logger.eval_stats(global_step, eval_loss)\n    fig = plot_embeddings(embeddings, num_classes_in_batch)\n    dashboard_logger.eval_figures(global_step, fig)\n    return eval_loss", "CodeBleu_score": 38.33657614266066}, {"code": "def evaluation(model, criterion, data_loader, global_step)\n\tmodel.eval()\n    eval_loss = 0\n    with torch.no_grad():\n        for batch_idx, (inputs, labels) in enumerate(data_loader):\n            inputs = inputs.view(-1, 1, 80, 100).cuda()\n            labels = labels.view(-1).cuda()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            eval_loss += loss.item()\n        eval_loss /= len(data_loader)\n    dashboard_logger.eval_stats(eval_loss, global_step)\n    dashboard_logger.eval_figures(\n        plot_embeddings(outputs.detach().cpu().numpy(), len(data_loader)), global_step\n    )\n    return eval_loss", "CodeBleu_score": 25.87279187420224}, {"code": "def evaluation(model, criterion, data_loader, global_step)\n\tmodel.eval()\n    eval_loss = 0\n    with torch.no_grad():\n        for batch_idx, (inputs, labels) in enumerate(data_loader):\n            inputs, labels = inputs.cuda(), labels.cuda()\n            inputs, labels = inputs.view(\n                inputs.shape[0] * inputs.shape[1], -1\n            ), labels.view(labels.shape[0] * labels.shape[1])\n            outputs = model(inputs)\n            outputs = outputs.view(labels.shape[0], labels.shape[1], -1)\n            outputs = outputs.transpose(1, 2)\n            loss = criterion(outputs, labels)\n            eval_loss += loss.item()\n\n            if batch_idx == len(data_loader) - 1:\n                embeddings = outputs.detach().cpu().numpy()\n                num_classes_in_batch = labels.shape[0]\n                fig = plot_embeddings(embeddings, num_classes_in_batch)\n                dashboard_logger.eval_figures(\n                    {\"umap\": fig}, global_step=global_step\n                )\n\n    eval_loss /= len(data_loader)\n    dashboard_logger.eval_stats(\n        {\"loss\": eval_loss}, global_step=global_step\n    )\n    model.train()\n    return eval_loss", "CodeBleu_score": 41.038335892945256}]}
{"instance_id": "565f8ed09916", "generation_result": [{"code": "def register_config(model_name: str) -> Coqpit\n\tconfig_name = to_camel(model_name) + \"Config\"\n    config_class = None\n    config_paths = [\"configs\", \"xtts.configs\"]\n    for config_path in config_paths:\n        try:\n            config_class = find_module(config_path, config_name)\n        except ModuleNotFoundError:\n            pass\n        else:\n            break\n    if config_class is None:\n        raise ModuleNotFoundError(f\"Could not find the configuration class for {model_name}\")\n    return config_class\n\n", "CodeBleu_score": 39.005700214937626}, {"code": "def register_config(model_name: str) -> Coqpit\n\tconfig_class = None\n    config_class_name = to_camel(model_name) + \"Config\"\n    config_class_path = \"configs\"\n    config_class_paths = [\n        config_class_path + \".xtts\",\n        config_class_path + \".xtts_v2\",\n        config_class_path,\n    ]\n    for config_class_path in config_class_paths:\n        try:\n            config_class = find_module(config_class_path, config_class_name)\n            break\n        except ModuleNotFoundError:\n            continue\n    if config_class is None:\n        raise ModuleNotFoundError(\n            f\"Could not find configuration class {config_class_name} in any of the following paths:", "CodeBleu_score": 36.507935767670205}, {"code": "def register_config(model_name: str) -> Coqpit\n\tconfig_class = None\n    config_class_name = to_camel(model_name) + \"Config\"\n    try:\n        config_class = find_module(\n            \"configs\", config_class_name\n        )  # search in configs\n    except ModuleNotFoundError:\n        pass\n    try:\n        config_class = find_module(\n            \"xtts.configs\", config_class_name\n        )  # search in xtts/configs\n    except ModuleNotFoundError:\n        pass\n    try:\n        config_class = find_module(\n            \"xtts.configs.xtts\", config_class_name\n        )  # search in xtts/configs/xtts\n    except ModuleNotFoundError:\n        pass\n    if config_class is None:\n        raise ModuleNotFoundError(\n            f\"Could not find configuration class for model {model_name}.\"\n        )\n    return config_class\n\n", "CodeBleu_score": 38.588265947756035}]}
{"instance_id": "0ea7862506a7", "generation_result": [{"code": "def load_config(config_path: str) -> Coqpit\n\tconfig_dict = {}\n    if config_path.endswith(\".json\"):\n        with fsspec.open(config_path, \"r\", encoding=\"utf-8\") as f:\n            config_dict = json.load(f)\n    elif config_path.endswith(\".yaml\"):\n        with fsspec.open(config_path, \"r\", encoding=\"utf-8\") as f:\n            config_dict = yaml.safe_load(f)\n    else:\n        raise TypeError(f\" [!] Unknown file extension: {config_path}\")\n    model_name = _process_model_name(config_dict)\n    config_class = register_config(model_name)\n    config_dict.update({\"model\": model_name})\n    config = config_class()\n    config.from_dict(config_dict)\n    return config\n\n", "CodeBleu_score": 55.776589261727906}, {"code": "def load_config(config_path: str) -> Coqpit\n\tconfig_dict = read_json_with_comments(config_path)\n    model_name = _process_model_name(config_dict)\n    config_class = register_config(model_name)\n    config = config_class()\n    config.from_dict(config_dict)\n    return config\n\n", "CodeBleu_score": 11.131405622652286}, {"code": "def load_config(config_path: str) -> Coqpit\n\t# determine file extension\n    _, file_extension = os.path.splitext(config_path)\n    if file_extension == \".json\":\n        with fsspec.open(config_path, \"r\", encoding=\"utf-8\") as f:\n            config_dict = json.load(f)\n    elif file_extension == \".yaml\":\n        with fsspec.open(config_path, \"r\", encoding=\"utf-8\") as f:\n            config_dict = yaml.safe_load(f)\n    else:\n        raise TypeError(f\" [!] Unknown file extension: {file_extension}\")\n    # process model name\n    model_name = _process_model_name(config_dict)\n    # register config\n    config_class = register_config(model_name)\n    # create config\n    config = config_class.from_dict(config_dict)\n    # update config\n    config.update()\n    return config\n\n", "CodeBleu_score": 53.84899603925243}]}
{"instance_id": "e0db68b63143", "generation_result": [{"code": "def list_files(basePath, validExts=None, contains=None)\n\tif validExts is None:\n        validExts = []\n    for root, dirs, files in os.walk(basePath):\n        for filename in files:\n            if contains is not None and filename.find(contains) == -1:\n                continue\n            ext = filename[filename.rfind('.') + 1:].lower()\n            if len(validExts) == 0 or ext.lower() in validExts:\n                yield os.path.join(root, filename)", "CodeBleu_score": 62.67388286892227}, {"code": "def list_files(basePath, validExts=None, contains=None)\n\t# check if the provided path is valid\n    if not os.path.isdir(basePath):\n        raise Exception(\"list_files: The provided path is not a valid directory.\")\n\n    # walk through the directory\n    for root, dirs, files in os.walk(basePath):\n        for filename in files:\n            # check if the file contains the string\n            if contains is not None and filename.find(contains) == -1:\n                continue\n            # check if the file extension is valid\n            if validExts is not None and not filename.lower().endswith(tuple(validExts)):\n                continue\n            # join the two strings in order to form the full filepath.\n            filePath = os.path.join(root, filename)\n            # yield the full file path\n            yield filePath", "CodeBleu_score": 47.48368279831616}, {"code": "def list_files(basePath, validExts=None, contains=None)\n\t# If no valid extensions are provided, set to a null list\n    if not validExts:\n        validExts = []\n\n    # Walk through the base directory\n    for root, dirs, files in os.walk(basePath):\n        # For each file, check if it is a valid file\n        for filename in files:\n            # Check if the file contains a specified string\n            if contains and contains not in filename:\n                continue\n            # Get the file extension\n            ext = filename[filename.rfind('.'):].lower()\n            # Check if the file extension is in the list of valid extensions\n            if ext not in validExts:\n                continue\n            # Construct the full file path\n            filepath = os.path.join(root, filename)\n            # Yield the file path\n            yield filepath", "CodeBleu_score": 50.93854697083386}]}
{"instance_id": "b836fcaf0a27", "generation_result": [{"code": "def load_model(xtts_checkpoint, xtts_config, xtts_vocab)\n\tclear_gpu_cache()\n    if xtts_checkpoint is None or xtts_config is None or xtts_vocab is None:\n        return \"Please provide the paths to the model checkpoint, config, and vocabulary.\"\n    XttsConfig.load_json(xtts_config)\n    Xtts.init_from_config()\n    Xtts_MODEL.load_checkpoint(checkpoint_path=xtts_checkpoint, vocab_path=xtts_vocab)\n    if torch.cuda.is_available():\n        Xtts_MODEL.cuda()\n    return \"XTTS model loaded successfully!\"", "CodeBleu_score": 34.641113663210554}, {"code": "def load_model(xtts_checkpoint, xtts_config, xtts_vocab)\n\tclear_gpu_cache()\n    if xtts_checkpoint is None:\n        return \"XTTS checkpoint path is required.\"\n    if xtts_config is None:\n        return \"XTTS config path is required.\"\n    if xtts_vocab is None:\n        return \"XTTS vocab path is required.\"\n\n    config = XttsConfig.load_json(xtts_config)\n    XTTS_MODEL = XTTS.init_from_config(config)\n    XTTS_MODEL.load_checkpoint(xtts_checkpoint, xtts_vocab)\n\n    if torch.cuda.is_available():\n        XTTS_MODEL.cuda()\n\n    return \"XTTS model loaded successfully.\"\n\n", "CodeBleu_score": 47.31289045642387}, {"code": "def load_model(xtts_checkpoint, xtts_config, xtts_vocab)\n\tclear_gpu_cache()\n    if xtts_checkpoint is None:\n        return \"Please specify the checkpoint path.\"\n    if xtts_config is None:\n        return \"Please specify the config path.\"\n    if xtts_vocab is None:\n        return \"Please specify the vocab path.\"\n\n    config = XttsConfig.load_json(xtts_config)\n    XTTS_MODEL = XTTS(config)\n    XTTS_MODEL.load_checkpoint(xtts_config, xtts_checkpoint, xtts_vocab)\n    if torch.cuda.is_available():\n        XTTS_MODEL.cuda()\n    return \"XTTS model loaded successfully.\"\n\n", "CodeBleu_score": 46.95258291900877}]}
{"instance_id": "c5d63aa0df26", "generation_result": [{"code": "def compute_embedding(self, x, num_frames=250, num_eval=10, return_mean=True, l2_norm=True)\n\tif self.torch_spec:\n        x = torch.as_tensor(x, dtype=torch.float32)\n        x = x.unsqueeze(0)\n        if self.hop_length is not None:\n            num_frames = int(np.ceil((x.shape[-1] - self.filter_length) / self.hop_length))\n            num_frames = min(num_frames, x.shape[-1])\n        if num_frames > x.shape[-1]:\n            num_frames = x.shape[-1]\n        offsets = np.linspace(\n            0, x.shape[-1] - self.filter_length, num_frames, dtype=int\n        )\n        frames_batch = [x[:, offset : offset + self.filter_length] for offset in offsets]\n        frames_batch = torch.stack(frames_batch, dim=0)\n        embeddings = self.inference(frames_batch)\n        if return_mean:\n            embeddings = torch.mean(embeddings, dim=0)\n        if l2_norm:\n            embeddings = torch.nn.functional.normalize(embeddings)\n        embeddings = embeddings.detach().numpy()\n    else:\n        if self.hop_length is not None:\n            num_frames = int(np.ceil((x.shape[-1] - self.filter_length) / self.hop_length))\n            num_frames = min(num_frames, x.shape[-1])\n        if num_frames > x.shape[-1]:\n            num_frames = x.shape[-1]\n        offsets = np.linspace(\n            0, x.shape[-1] - self.filter_length, num_frames, dtype=int\n        )\n        frames_batch = [x[offset : offset + self.filter_length] for offset in offsets]\n        frames_batch = np.stack(frames_batch, axis=0)\n        embeddings = self.inference(frames_batch)\n        if return_mean:\n            embeddings = np.mean(embeddings, axis=", "CodeBleu_score": 35.16197650034225}, {"code": "def compute_embedding(self, x, num_frames=250, num_eval=10, return_mean=True, l2_norm=True)\n\t# adjust number of frames based on hop length\n    num_frames = self.hop_length * num_frames\n    # ensure num_frames does not exceed input length\n    num_frames = min(num_frames, x.shape[1])\n    # compute offset positions for evaluation\n    frame_offsets = np.linspace(0, x.shape[1] - num_frames, num_eval, dtype=int)\n    # extract frames at these offsets, concatenate them into a batch\n    frames_batch = []\n    for offset in frame_offsets:\n        frames_batch.append(x[:, offset : offset + num_frames])\n    frames_batch = torch.cat(frames_batch, dim=0)\n    # perform inference to obtain embeddings\n    embeddings = self.inference(frames_batch)\n    # compute mean of embeddings\n    if return_mean:\n        embeddings = torch.mean(embeddings, dim=0, keepdim=True)\n    # normalize embeddings with L2 norm\n    if l2_norm:\n        embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)\n    return embeddings", "CodeBleu_score": 53.86703752638308}, {"code": "def compute_embedding(self, x, num_frames=250, num_eval=10, return_mean=True, l2_norm=True)\n\tif self.hop_length is not None:\n        num_frames = int(np.ceil(num_frames * self.hop_length / self.win_length))\n        num_frames = min(num_frames, x.shape[1])\n    if num_frames > x.shape[1]:\n        num_frames = x.shape[1]\n    if num_eval > num_frames:\n        num_eval = num_frames\n    offsets = np.linspace(0, x.shape[1] - num_frames, num_eval, dtype=int)\n    frames_batch = []\n    for offset in offsets:\n        frames_batch.append(x[:, offset : offset + num_frames])\n    frames_batch = torch.cat(frames_batch, dim=0)\n    embeddings = self.inference(frames_batch)\n    if return_mean:\n        embeddings = torch.mean(embeddings, dim=0, keepdim=True)\n    if l2_norm:\n        embeddings = torch.nn.functional.normalize(embeddings)\n    return embeddings", "CodeBleu_score": 57.61628509686605}]}
{"instance_id": "06f5b0335b26", "generation_result": [{"code": "def create_layer(self, block, planes, blocks, stride=1)\n\tdownsample = None\n    if stride != 1 or self.inplanes != planes * block.expansion:\n        downsample = nn.Sequential(\n            nn.Conv2d(self.inplanes, planes * block.expansion,\n                      kernel_size=1, stride=stride, bias=False),\n            nn.BatchNorm2d(planes * block.expansion),\n        )\n\n    layers = []\n    layers.append(block(self.inplanes, planes, stride, downsample))\n    self.inplanes = planes * block.expansion\n    for i in range(1, blocks):\n        layers.append(block(self.inplanes, planes))\n\n    return nn.Sequential(*layers)", "CodeBleu_score": 85.2471646907718}, {"code": "def create_layer(self, block, planes, blocks, stride=1)\n\tdownsample = None\n    if stride != 1 or self.inplanes != planes * block.expansion:\n        downsample = nn.Sequential(\n            nn.Conv2d(self.inplanes, planes * block.expansion,\n                      kernel_size=1, stride=stride, bias=False),\n            nn.BatchNorm2d(planes * block.expansion),\n        )\n\n    layers = []\n    layers.append(block(self.inplanes, planes, stride, downsample))\n    self.inplanes = planes * block.expansion\n    for i in range(1, blocks):\n        layers.append(block(self.inplanes, planes))\n\n    return nn.Sequential(*layers)", "CodeBleu_score": 85.2471646907718}, {"code": "def create_layer(self, block, planes, blocks, stride=1)\n\tdownsample = None\n    if stride != 1 or self.inplanes != planes * block.expansion:\n        downsample = nn.Sequential(\n            nn.Conv2d(self.inplanes, planes * block.expansion,\n                      kernel_size=1, stride=stride, bias=False),\n            nn.BatchNorm2d(planes * block.expansion),\n        )\n\n    layers = []\n    layers.append(block(self.inplanes, planes, stride, downsample))\n    self.inplanes = planes * block.expansion\n    for i in range(1, blocks):\n        layers.append(block(self.inplanes, planes))\n\n    return nn.Sequential(*layers)", "CodeBleu_score": 85.2471646907718}]}
{"instance_id": "9ac60597b633", "generation_result": [{"code": "def plot_embeddings(embeddings, num_classes_in_batch)\n\t# Calculate the number of utterances per class\n    num_utterances_per_class = np.sum(num_classes_in_batch, axis=0)\n\n    # Limit the number of classes to 10\n    num_classes = num_utterances_per_class.shape[0]\n    if num_classes > 10:\n        num_utterances_per_class = num_utterances_per_class[:10]\n        embeddings = embeddings[:10]\n\n    # Transform the embeddings with UMAP\n    transformer = umap.UMAP()\n    transformed = transformer.fit_transform(embeddings)\n\n    # Create a scatter plot with colors representing different classes\n    fig, ax = plt.subplots()\n    colors = plt.get_cmap(\"tab20\")(np.arange(num_classes) / num_classes)\n    for i in range(num_classes):\n        ax.scatter(\n            transformed[num_classes_in_batch[:, i] == 1, 0],\n            transformed[num_classes_in_batch[:, i] == 1, 1],\n            label=i,\n            color=colors[i],\n        )\n\n    # Adjust for equal aspect ratio and save the plot\n    ax.set_aspect(\"equal\", \"datalim\")\n    plt.title(\"UMAP projection\")\n    plt.legend()\n    plt.tight_layout()\n    plt.savefig(\"umap.png\")\n    return fig", "CodeBleu_score": 31.59181447977273}, {"code": "def plot_embeddings(embeddings, num_classes_in_batch)\n\tnum_classes = len(num_classes_in_batch)\n    if num_classes > 10:\n        num_classes_in_batch = num_classes_in_batch[:10]\n        embeddings = embeddings[:10]\n        num_classes = len(num_classes_in_batch)\n    num_utterances_per_class = np.repeat(num_classes_in_batch, num_classes_in_batch)\n    reducer = umap.UMAP()\n    embeddings_2d = reducer.fit_transform(embeddings)\n    fig, ax = plt.subplots(figsize=(10, 10))\n    ax.scatter(\n        embeddings_2d[:, 0],\n        embeddings_2d[:, 1],\n        c=num_utterances_per_class,\n        s=50,\n        cmap=\"Spectral\",\n    )\n    plt.title(\"UMAP projection\")\n    plt.gca().set_aspect(\"equal\", \"datalim\")\n    plt.tight_layout()\n    plt.savefig(\"umap.png\")\n    return fig", "CodeBleu_score": 45.526530203024365}, {"code": "def plot_embeddings(embeddings, num_classes_in_batch)\n\t# Calculate the number of utterances per class\n    num_utterances_per_class = np.repeat(num_classes_in_batch, num_classes_in_batch)\n\n    # Limit the number of classes to 10\n    if num_classes_in_batch > 10:\n        num_classes_in_batch = 10\n\n    # Transform the embeddings using UMAP\n    reducer = umap.UMAP(n_components=2, random_state=42)\n    embeddings_2d = reducer.fit_transform(embeddings)\n\n    # Create a scatter plot with colors representing different classes\n    fig, ax = plt.subplots(figsize=(8, 8))\n    ax.scatter(\n        embeddings_2d[:, 0],\n        embeddings_2d[:, 1],\n        c=num_utterances_per_class,\n        cmap=\"tab10\",\n        marker=\".\",\n    )\n\n    # Set the aspect ratio to be equal\n    ax.set_aspect(\"equal\", \"box\")\n\n    # Title the plot\n    ax.set_title(\"UMAP projection\")\n\n    # Save the plot as \"umap\"\n    plt.savefig(\"umap\")\n\n    # Return the figure\n    return fig", "CodeBleu_score": 40.906258903920914}]}
{"instance_id": "19c2b4af256d", "generation_result": [{"code": "def calc_cosine_sim(self, dvecs, centroids)\n\tcs_row = []\n    for spkr in range(self.num_spkr):\n        for utt in range(self.num_utt):\n            new_centroids = self.calc_new_centroids(dvecs, centroids, spkr, utt)\n            dvec = dvecs[spkr, utt].unsqueeze(0)\n            cs = torch.mm(dvec, new_centroids.transpose(0, 1))\n            cs = torch.clamp(cs, min=self.eps)\n            cs_row.append(cs)\n    return torch.stack(cs_row)\n\n", "CodeBleu_score": 30.288082643095976}, {"code": "def calc_cosine_sim(self, dvecs, centroids)\n\tcos_sim_matrix = []\n    for spkr, utts in enumerate(self.train_data):\n        new_centroids = self.calc_new_centroids(dvecs, centroids, spkr, utts)\n        cs_row = []\n        for utt in utts:\n            cs_row.append(torch.clamp(torch.mm(dvecs[spkr, utt].unsqueeze(1), new_centroids.transpose(0, 1)), 0, 1))\n        cos_sim_matrix.append(torch.cat(cs_row))\n    return torch.stack(cos_sim_matrix)\n\n", "CodeBleu_score": 38.3479871107837}, {"code": "def calc_cosine_sim(self, dvecs, centroids)\n\tcos_sim_matrix = []\n    for spkr in range(dvecs.shape[0]):\n        cs_row = []\n        for utt in range(dvecs.shape[1]):\n            new_centroids = self.calc_new_centroids(dvecs, centroids, spkr, utt)\n            dvec = dvecs[spkr, utt]\n            cs = torch.mm(dvec.unsqueeze(0), new_centroids.transpose(0, 1))\n            cs = torch.clamp(cs, min=self.eps)\n            cs_row.append(cs)\n        cos_sim_matrix.append(torch.stack(cs_row))\n    return torch.stack(cos_sim_matrix)", "CodeBleu_score": 40.81886240704352}]}
{"instance_id": "c525b411a1e7", "generation_result": [{"code": " def embed_loss_softmax(self, dvecs, cos_sim_matrix)\n\tlosses = []\n    for i in range(dvecs.shape[0]):\n        loss = F.log_softmax(cos_sim_matrix[i], dim=0)\n        losses.append(loss)\n    losses = torch.stack(losses)\n    return losses", "CodeBleu_score": 35.70301132238757}, {"code": " def embed_loss_softmax(self, dvecs, cos_sim_matrix)\n\tlosses = []\n    for dvec in dvecs:\n        #print(dvec.shape)\n        #print(cos_sim_matrix.shape)\n        #print(F.log_softmax(cos_sim_matrix, dim=1).shape)\n        #print(F.log_softmax(cos_sim_matrix, dim=1)[dvec.item()].shape)\n        #print(F.log_softmax(cos_sim_matrix, dim=1)[dvec.item()][dvec.item()].shape)\n        #print(F.log_softmax(cos_sim_matrix, dim=1)[dvec.item()][dvec.item()].item())\n        losses.append(F.log_softmax(cos_sim_matrix, dim=1)[dvec.item()][dvec.item()].item())\n    return torch.stack(losses)", "CodeBleu_score": 20.029686211597184}, {"code": " def embed_loss_softmax(self, dvecs, cos_sim_matrix)\n\tlosses = []\n    for i, dvec in enumerate(dvecs):\n        # The cosine similarity matrix is a matrix of dot products of the embeddings.\n        # The cosine similarity of a row with itself is 1, and the cosine similarity of a row with another row is the dot product of the embeddings.\n        # The cosine similarity matrix is a matrix of dot products of the embeddings.\n        # The cosine similarity of a row with itself is 1, and the cosine similarity of a row with another row is the dot product of the embeddings.\n        cos_sim_matrix[i, i] = 0\n        # The softmax of the cosine similarity matrix is a matrix of probabilities of the embeddings.\n        # The softmax of the cosine similarity matrix is a matrix of probabilities of the embeddings.\n        probs = F.softmax(cos_sim_matrix[i], dim=0)\n        # The negative log softmax of the cosine similarity matrix is a matrix of log probabilities of the embeddings.\n        # The negative log softmax of the cosine similarity matrix is a matrix of log probabilities of the embeddings.\n        log_probs = F.log_softmax(cos_sim_matrix[i], dim=0)\n        # The loss for the current embedding is the negative log probability of the embedding itself.\n        # The loss for the current embedding is the negative log probability of the embedding itself.\n        loss = -log_probs[i]\n        # The loss for the current embedding is the negative log probability of the embedding itself.\n        # The loss for the current embedding is the negative log probability of the embedding itself.\n        loss = -log_probs[i]\n        # The loss for the current embedding is the negative log probability of the embedding itself.\n        # The loss for the current embedding is the negative log probability of the embedding itself.\n        loss = -log_probs[i]\n        # The loss for the current embedding is the negative log probability of the embedding itself.\n        # The loss for the current embedding is the negative log probability of the embedding itself.\n        loss = -log_probs[i]\n        # The loss for the current embedding is the negative log probability", "CodeBleu_score": 19.94420857699374}]}
{"instance_id": "25ac3945da2e", "generation_result": [{"code": "def embed_loss_contrast(self, dvecs, cos_sim_matrix)\n\tL_row = []\n    for i in range(dvecs.shape[0]):\n        cos_sim_matrix_i = cos_sim_matrix[i, :]\n        cos_sim_matrix_i[i] = -1\n        L_row.append(torch.sigmoid(cos_sim_matrix_i))\n    L_row = torch.stack(L_row)\n    return L_row", "CodeBleu_score": 24.94896551714166}, {"code": "def embed_loss_contrast(self, dvecs, cos_sim_matrix)\n\tL_row = []\n    for i in range(len(dvecs)):\n        cos_sim_matrix_i = cos_sim_matrix[i]\n        cos_sim_matrix_i[i] = 0\n        # Compute the sigmoid of the cosine similarity matrix for the current embedding\n        cos_sim_matrix_i_sigmoid = torch.sigmoid(cos_sim_matrix_i)\n        # Exclude the current centroid\n        cos_sim_matrix_i_sigmoid[i] = 0\n        # Compute the contrast loss\n        L_row.append(torch.max(cos_sim_matrix_i_sigmoid))\n    # Stack the losses for each row\n    L_row = torch.stack(L_row)\n    # Return the stacked tensor of losses\n    return L_row", "CodeBleu_score": 29.401991902957032}, {"code": "def embed_loss_contrast(self, dvecs, cos_sim_matrix)\n\tL_row = []\n    for dvec in dvecs:\n        # Compute the sigmoid of the cosine similarity matrix\n        cos_sim_matrix_sigmoid = torch.sigmoid(cos_sim_matrix)\n        # Exclude the current centroid from the sigmoid matrix\n        cos_sim_matrix_sigmoid[range(cos_sim_matrix_sigmoid.shape[0]), range(cos_sim_matrix_sigmoid.shape[1])] = 0\n        # Compute the contrast loss for the current embedding\n        L_row.append(torch.max(torch.stack([torch.zeros(1, device=self.device), 1 - cos_sim_matrix_sigmoid[i, :] + cos_sim_matrix_sigmoid[i, i]])) for i in range(dvecs.shape[0]))\n    # Stack the losses for each row\n    L = torch.stack(L_row)\n    # Return the final stacked tensor of losses\n    return L", "CodeBleu_score": 29.433690984564727}]}
{"instance_id": "878d14ffabd2", "generation_result": [{"code": "def forward(self, x, _label=None)\n\tassert x.size(1) >= 2, \"GE2E loss requires at least two utterances per speaker.\"\n    x = x.float()\n    dvecs = self.embed_loss(x)\n    centroids = self.calc_centroids(dvecs)\n    cos_sim_matrix = self.calc_cosine_sim(dvecs, centroids)\n    loss = self.calc_loss(cos_sim_matrix)\n    return loss.mean()\n\n", "CodeBleu_score": 43.300484090273514}, {"code": "def forward(self, x, _label=None)\n\tif x.size(1) < 2:\n        raise ValueError(\"Expected at least 2 utterances per speaker, found \" + str(x.size(1)))\n    dvecs = x.transpose(0, 1)\n    centroids = self.calc_centroids(dvecs)\n    cos_sim_matrix = self.calc_cosine_sim(dvecs, centroids)\n    cos_sim_matrix = self.scale_and_shift_sim_matrix(cos_sim_matrix)\n    embed_loss = self.calc_embed_loss(cos_sim_matrix)\n    return torch.mean(embed_loss)", "CodeBleu_score": 38.17687686164331}, {"code": "def forward(self, x, _label=None)\n\tif x.size(0) < 2:\n        raise ValueError(\"GE2E loss requires at least two speakers, got {}\".format(x.size(0)))\n    if x.size(1) < 2:\n        raise ValueError(\n            \"GE2E loss requires at least two utterances per speaker, got {}\".format(x.size(1))\n        )\n    dvecs = x.transpose(0, 1)\n    centroids = self.calc_centroids(dvecs)\n    cos_sim_matrix = self.calc_cosine_sim(dvecs, centroids)\n    sim_matrix = self.scale_and_shift(cos_sim_matrix)\n    embed_loss = self.calc_embed_loss(sim_matrix)\n    return torch.mean(embed_loss)", "CodeBleu_score": 36.473846729973104}]}
{"instance_id": "86a364c72caa", "generation_result": [{"code": "def get_voices(extra_voice_dirs: List[str] = [])\n\tvoice_dirs = [\n        \"voices\",\n        \"voices/en\",\n        \"voices/en/en_us\",\n        \"voices/en/en_uk\",\n        \"voices/en/en_uk/en_uk_female\",\n        \"voices/en/en_uk/en_uk_male\",\n        \"voices/en/en_us/en_us_female\",\n        \"voices/en/en_us/en_us_male\",\n    ]\n\n    voice_dirs.extend(extra_voice_dirs)\n\n    voices = {}\n\n    for voice_dir in voice_dirs:\n        for subdir, dirs, files in os.walk(voice_dir):\n            voice_files = []\n            for file in files:\n                if file.endswith(\".npz\"):\n                    voice_files.append(os.path.join(subdir, file))\n                elif file.endswith(\".wav\") or file.endswith(\".mp3\"):\n                    voice_files.append(os.path.join(subdir, file))\n            if voice_files:\n                voices[subdir] = voice_files\n\n    return voices\n\n", "CodeBleu_score": 24.37918567783907}, {"code": "def get_voices(extra_voice_dirs: List[str] = [])\n\tvoices = {}\n    for voice_dir in extra_voice_dirs:\n        for root, dirs, files in os.walk(voice_dir):\n            voice_files = []\n            for file in files:\n                if file.endswith(\".npz\"):\n                    voice_files.append(os.path.join(root, file))\n                elif file.endswith(\".wav\") or file.endswith(\".mp3\"):\n                    voice_files.append(os.path.join(root, file))\n            if len(voice_files) > 0:\n                voices[root] = voice_files\n    return voices\n\n", "CodeBleu_score": 30.237114806931686}, {"code": "def get_voices(extra_voice_dirs: List[str] = [])\n\tvoices = {}\n    for voice_dir in extra_voice_dirs:\n        for root, dirs, files in os.walk(voice_dir):\n            if not dirs:\n                continue\n            for subdir in dirs:\n                subdir_path = os.path.join(root, subdir)\n                voice_files = []\n                for voice_file in os.listdir(subdir_path):\n                    if voice_file.endswith(\".npz\"):\n                        voice_files.append(os.path.join(subdir_path, voice_file))\n                if not voice_files:\n                    for voice_file in os.listdir(subdir_path):\n                        if voice_file.endswith(\".wav\") or voice_file.endswith(\".mp3\"):\n                            voice_files.append(os.path.join(subdir_path, voice_file))\n                if voice_files:\n                    voices[subdir] = voice_files\n    return voices", "CodeBleu_score": 27.98396655777165}]}
{"instance_id": "02f20c8917e6", "generation_result": [{"code": "def load_voice(model, voice: str, extra_voice_dirs: List[str] = [])\n\tif voice == \"random\":\n        return None, None, None\n\n    voices = get_voices(extra_voice_dirs)\n    paths = voices[voice]\n\n    # bark only uses a single sample for cloning\n    if len(paths) > 1:\n        raise ValueError(f\"Voice {voice} has multiple paths: {paths}\")\n\n    try:\n        path = voices[voice]\n    except KeyError as e:\n        raise KeyError(f\"Voice {voice} not found in {extra_voice_dirs}\") from e\n\n    if len(paths) == 1 and paths[0].endswith(\".npz\"):\n        return load_npz(path[0])\n\n    audio_path = paths[0]\n    # replace the file extension with .npz\n    output_path = os.path.splitext(audio_path)[0] + \".npz\"\n    generate_voice(audio=audio_path, model=model, output_path=output_path)\n    return load_voice(model, voice, extra_voice_dirs)", "CodeBleu_score": 96.01808764220647}, {"code": "def load_voice(model, voice: str, extra_voice_dirs: List[str] = [])\n\tif voice == \"random\":\n        return None, None, None\n\n    voices = get_voices(extra_voice_dirs)\n    paths = voices[voice]\n\n    # bark only uses a single sample for cloning\n    if len(paths) > 1:\n        raise ValueError(f\"Voice {voice} has multiple paths: {paths}\")\n\n    try:\n        path = voices[voice]\n    except KeyError as e:\n        raise KeyError(f\"Voice {voice} not found in {extra_voice_dirs}\") from e\n\n    if len(paths) == 1 and paths[0].endswith(\".npz\"):\n        return load_npz(path[0])\n\n    audio_path = paths[0]\n    # replace the file extension with .npz\n    output_path = os.path.splitext(audio_path)[0] + \".npz\"\n    generate_voice(audio=audio_path, model=model, output_path=output_path)\n    return load_voice(model, voice, extra_voice_dirs)\n\n", "CodeBleu_score": 96.01808764220647}, {"code": "def load_voice(model, voice: str, extra_voice_dirs: List[str] = [])\n\tif voice == \"random\":\n        return None, None, None\n\n    voices = get_voices(extra_voice_dirs)\n    paths = voices[voice]\n\n    # bark only uses a single sample for cloning\n    if len(paths) > 1:\n        raise ValueError(f\"Voice {voice} has multiple paths: {paths}\")\n\n    try:\n        path = voices[voice]\n    except KeyError as e:\n        raise KeyError(f\"Voice {voice} not found in {extra_voice_dirs}\") from e\n\n    if len(paths) == 1 and paths[0].endswith(\".npz\"):\n        return load_npz(path[0])\n\n    audio_path = paths[0]\n    # replace the file extension with .npz\n    output_path = os.path.splitext(audio_path)[0] + \".npz\"\n    generate_voice(audio=audio_path, model=model, output_path=output_path)\n    return load_voice(model, voice, extra_voice_dirs)\n\n", "CodeBleu_score": 96.01808764220647}]}
{"instance_id": "9458e981389f", "generation_result": [{"code": "def _relative_shift(self, pos_score: torch.Tensor) -> torch.Tensor\n\tbatch_size, num_heads, seq_length, _ = pos_score.size()\n    zero_tensor = torch.zeros((batch_size, num_heads, seq_length, 1), device=pos_score.device, dtype=pos_score.dtype)\n    concat_tensor = torch.cat((zero_tensor, pos_score), dim=-1)\n    reshaped_tensor = concat_tensor.view(batch_size, num_heads, 2 * seq_length)\n    # Extract the second half of the tensor\n    relative_shifted_tensor = reshaped_tensor[:, :, seq_length:]\n    return relative_shifted_tensor.view(batch_size, num_heads, seq_length, seq_length)\n\n", "CodeBleu_score": 30.424636642043453}, {"code": "def _relative_shift(self, pos_score: torch.Tensor) -> torch.Tensor\n\tpos_score_dims = pos_score.size()\n    pos_score_zero = torch.zeros(\n        pos_score_dims[0], pos_score_dims[1], pos_score_dims[2], 1\n    )\n    pos_score_shifted = torch.cat(\n        (pos_score_zero, pos_score), dim=3\n    ).view(\n        pos_score_dims[0], pos_score_dims[1], pos_score_dims[2] * 2\n    )[:, :, 1:]\n    return pos_score_shifted\n\n", "CodeBleu_score": 22.634383393622716}, {"code": "def _relative_shift(self, pos_score: torch.Tensor) -> torch.Tensor\n\tbatch_size, num_heads, seq_len, _ = pos_score.size()\n    zeros_shape = (batch_size, num_heads, seq_len, 1)\n    relative_shift = torch.zeros(zeros_shape, dtype=pos_score.dtype, device=pos_score.device)\n    relative_shift = torch.cat([relative_shift, pos_score], dim=-1)\n    relative_shift = relative_shift.view(batch_size, num_heads, 2 * seq_len, -1)\n    relative_shift = relative_shift[:, :, seq_len:, :]\n    return relative_shift", "CodeBleu_score": 30.02749056300727}]}
{"instance_id": "c046d4f00e80", "generation_result": [{"code": "def run_padded_sequence(self, context, lens)\n\t# Extract the context up to the specified length\n    context = context[:, :max(lens)]\n    # Pad the context to create a consistent batch size\n    context = pad_sequence(context, batch_first=True)\n    # Apply convolutional layers\n    for conv in self.convs:\n        context = self.dropout(F.relu(conv(context)))\n    # Return the padded context\n    return context\n\n\n\n\n", "CodeBleu_score": 23.598767198970354}, {"code": "def run_padded_sequence(self, context, lens)\n\t# Extract the context up to the specified length\n    context = context[:, :self.max_len]\n\n    # Pad the context to a consistent batch size\n    context_embedded = pad_sequence(context, batch_first=True)\n\n    # Apply the convolutional layers\n    for conv in self.convs:\n        context_embedded = F.relu(conv(context_embedded))\n\n    # Apply dropout\n    context_embedded = self.dropout(context_embedded)\n\n    # Transpose the context\n    context_embedded = context_embedded.transpose(1, 2)\n\n    # Return the context\n    return context_embedded", "CodeBleu_score": 28.21512934149542}, {"code": "def run_padded_sequence(self, context, lens)\n\t# Extract the context up to the specified length\n    context_embedded = self.context_embedded(context)\n    context_embedded = context_embedded.transpose(0, 1)\n    context_embedded = pad_sequence(context_embedded.clone(), batch_first=True, padding_value=0)\n    context_embedded = self.dropout(context_embedded)\n    # Apply the convolutional layers\n    for i in range(self.num_layers):\n        context_embedded = self.conv[i](context_embedded)\n        context_embedded = F.relu(context_embedded)\n        context_embedded = self.dropout(context_embedded)\n    # Pad the sequence to create a consistent batch size\n    context_embedded = pad_sequence(context_embedded.clone(), batch_first=True, padding_value=0)\n    return context_embedded", "CodeBleu_score": 25.634530812542994}]}
{"instance_id": "db2644f3c681", "generation_result": [{"code": "def run_unsorted_inputs(self, fn, context, lens)\n\tlens_sorted, indices = torch.sort(lens, descending=True)\n    context_sorted = context[indices]\n    context_packed = pack_padded_sequence(context_sorted, lens_sorted, batch_first=True)\n    context_packed_out = fn(context_packed)\n    context_unpacked, _ = pad_packed_sequence(context_packed_out, batch_first=True)\n    _, indices_unsort = torch.sort(indices, descending=False)\n    return context_unpacked[indices_unsort]", "CodeBleu_score": 16.270122505310148}, {"code": "def run_unsorted_inputs(self, fn, context, lens)\n\t# Sort lengths and indices\n    lens_sorted, indices = torch.sort(lens, descending=True)\n    context_sorted = context.index_select(0, indices)\n    # Pack the sequences\n    packed_context = torch.nn.utils.rnn.pack_padded_sequence(context_sorted, lens_sorted.cpu().numpy())\n    # Apply the function\n    packed_output = fn(packed_context)\n    # Unpack the sequences\n    output_sorted, _ = torch.nn.utils.rnn.pad_packed_sequence(packed_output)\n    # Unsort the results\n    output = output_sorted.index_select(0, torch.argsort(indices))\n    return output", "CodeBleu_score": 15.294999581089613}, {"code": "def run_unsorted_inputs(self, fn, context, lens)\n\tlens_sorted, indices = lens.sort(descending=True)\n    context_sorted = fn(context[indices])\n    context_unsorted = context_sorted[indices]\n    return context_unsorted", "CodeBleu_score": 4.539625224355893}]}
{"instance_id": "ced082bffc63", "generation_result": [{"code": "def remove_weight_norm(self)\n\tself.kernel_predictor.remove_weight_norm()\n    parametrize.remove_parametrizations(self.convt_pre[1], \"weight\")\n    for block in self.conv_blocks:\n        parametrize.remove_parametrizations(block[1], \"weight\")", "CodeBleu_score": 69.598046653168}, {"code": "def remove_weight_norm(self)\n\tself.kernel_predictor.remove_weight_norm()\n    parametrize.remove_parametrizations(self.convt_pre[1], \"weight\")\n    for block in self.conv_blocks:\n        parametrize.remove_parametrizations(block[1], \"weight\")", "CodeBleu_score": 69.598046653168}, {"code": "def remove_weight_norm(self)\n\tself.kernel_predictor.remove_weight_norm()\n    parametrize.remove_parametrizations(self.convt_pre[1], \"weight\")\n    for block in self.conv_blocks:\n        parametrize.remove_parametrizations(block[1], \"weight\")\n\n\n", "CodeBleu_score": 69.598046653168}]}
{"instance_id": "b5a39fd97061", "generation_result": [{"code": "def map_mx_to_torch_model(mx_name)\n\tmx_name = mx_name.lower()\n    mx_name = mx_name.replace('se_', 'se-')\n    mx_name = mx_name.replace('senet_', 'se-')\n    mx_name = mx_name.replace('inceptionv3', 'inception-v3')\n    torch_name = 'gluon_' + mx_name\n    return torch_name\n\n", "CodeBleu_score": 37.34547029374266}, {"code": "def map_mx_to_torch_model(mx_name)\n\ttorch_name = mx_name.lower()\n    torch_name = torch_name.replace('se_', 'se-')\n    torch_name = torch_name.replace('senet_', 'senet-')\n    torch_name = torch_name.replace('inceptionv3', 'inception_v3')\n    torch_name = 'gluon_' + torch_name\n    return torch_name\n\n", "CodeBleu_score": 49.083888793627246}, {"code": "def map_mx_to_torch_model(mx_name)\n\tmx_name = mx_name.lower()\n    if mx_name.startswith('se_'):\n        mx_name = mx_name.replace('se_', 'se-')\n    elif mx_name.startswith('senet_'):\n        mx_name = mx_name.replace('senet_', 'se-')\n    elif mx_name.startswith('inceptionv3'):\n        mx_name = mx_name.replace('inceptionv3', 'inception-v3')\n    return 'gluon_' + mx_name\n\n\n\n\n", "CodeBleu_score": 46.51963894231933}]}
{"instance_id": "7eee5efdbf67", "generation_result": [{"code": "def _num_samples_per_worker(self)\n\tnum_workers = self.num_workers\n    num_repeats = self.num_repeats\n    num_samples = self.num_samples\n    batch_size = self.batch_size\n    if num_repeats > 1 or self.training:\n        num_samples = math.ceil(num_samples / num_workers)\n    if batch_size > 0:\n        num_samples = num_samples - (num_samples % batch_size)\n    return int(num_samples)\n\n", "CodeBleu_score": 30.202205206061826}, {"code": "def _num_samples_per_worker(self)\n\t# The number of samples to be processed by each worker\n    num_samples_per_worker = self.repeats * self.num_samples\n\n    # If training or multiple replicas are used, round up the number of samples\n    if self.training or self.num_replicas > 1:\n        num_samples_per_worker = math.ceil(num_samples_per_worker)\n\n    # If a batch size is specified during training, adjust the number of samples to be a multiple of the batch size\n    if self.training and self.batch_size is not None:\n        num_samples_per_worker = (num_samples_per_worker // self.batch_size) * self.batch_size\n\n    return int(num_samples_per_worker)", "CodeBleu_score": 45.37778561638549}, {"code": "def _num_samples_per_worker(self)\n\tnum_workers = self.num_workers\n    if self.repeats > 1:\n        num_workers = num_workers * self.repeats\n\n    num_samples = math.ceil(self.num_samples / num_workers)\n\n    if self.batch_size > 0:\n        num_samples = int(math.ceil(num_samples / self.batch_size) * self.batch_size)\n\n    return num_samples", "CodeBleu_score": 34.83755422458755}]}
{"instance_id": "a698d65ae190", "generation_result": [{"code": "def find_images_and_targets(\n        folder: str,\n        types: Optional[Union[List, Tuple, Set]] = None,\n        class_to_idx: Optional[Dict] = None,\n        leaf_name_only: bool = True,\n        sort: bool = True\n)\n\tif types is None:\n        types = IMG_EXTENSIONS\n    labels = []\n    filenames = []\n    # find images and labels\n    for root, subdirs, files in os.walk(folder, topdown=True):\n        rel_path = os.path.relpath(root, folder) if (root != folder) else ''\n        label = os.path.basename(rel_path) if leaf_name_only else rel_path.replace(os.path.sep, '_')\n        for f in files:\n            base, ext = os.path.splitext(f)\n            if ext.lower() in types:\n                filenames.append(os.path.join(root, f))\n                labels.append(label)\n    # build class index\n    if class_to_idx is None:\n        unique_labels = set(labels)\n        sorted_labels = list(sorted(unique_labels, key=natural_key))\n        class_to_idx = {c: idx for idx, c in enumerate(sorted_labels)}\n    # join filenames with labels\n    samples = [(f, class_to_idx[l]) for f, l in zip(filenames, labels) if l in class_to_idx]\n    if sort:\n        samples.sort(key=lambda x: natural_key(x[0]))\n    return samples, class_to_idx\n\n", "CodeBleu_score": 71.384446122287}, {"code": "def find_images_and_targets(\n        folder: str,\n        types: Optional[Union[List, Tuple, Set]] = None,\n        class_to_idx: Optional[Dict] = None,\n        leaf_name_only: bool = True,\n        sort: bool = True\n)\n\tif types is None:\n        types = IMG_EXTENSIONS\n    labels = []\n    filenames = []\n    # walk the folder structure\n    for root, subdirs, files in os.walk(folder, topdown=False):\n        rel_path = os.path.relpath(root, folder) if (root != folder) else ''\n        label = os.path.basename(rel_path) if leaf_name_only else rel_path.replace(os.path.sep, '_')\n        for f in files:\n            base, ext = os.path.splitext(f)\n            if ext.lower() in types:\n                filenames.append(os.path.join(root, f))\n                labels.append(label)\n    if class_to_idx is None:\n        # build class index\n        unique_labels = set(labels)\n        sorted_labels = list(sorted(unique_labels, key=natural_key))\n        class_to_idx = {c: idx for idx, c in enumerate(sorted_labels)}\n    images_and_targets = [(f, class_to_idx[l]) for f, l in zip(filenames, labels) if l in class_to_idx]\n    if sort:\n        images_and_targets = sorted(images_and_targets, key=lambda k: natural_key(k[0]))\n    return images_and_targets, class_to_idx\n\n", "CodeBleu_score": 78.35557006801447}, {"code": "def find_images_and_targets(\n        folder: str,\n        types: Optional[Union[List, Tuple, Set]] = None,\n        class_to_idx: Optional[Dict] = None,\n        leaf_name_only: bool = True,\n        sort: bool = True\n)\n\tif types is None:\n        types = IMG_EXTENSIONS\n    labels = []\n    filenames = []\n    # print(f\"Searching for images in '{folder}'\")\n    for root, subdirs, files in os.walk(folder):\n        if leaf_name_only:\n            rel_path = os.path.relpath(root, folder)\n            # print(f\"rel_path: {rel_path}\")\n            if rel_path == '.':\n                rel_path = ''\n            label = os.path.basename(rel_path)\n            # print(f\"label: {label}\")\n        else:\n            label = os.path.relpath(root, folder)\n            # print(f\"label: {label}\")\n        if label not in labels:\n            labels.append(label)\n        for f in files:\n            base, ext = os.path.splitext(f)\n            if ext.lower() in types:\n                filenames.append(os.path.join(root, f))\n    if class_to_idx is None:\n        # print(f\"labels: {labels}\")\n        # print(f\"filenames: {filenames}\")\n        if sort:\n            labels.sort(key=natural_key)\n        # print(f\"labels: {labels}\")\n        # print(f\"filenames: {filenames}\")\n        class_to_idx = {labels[i]: i for i in range(len(labels))}\n    # print(f\"labels: {labels}\")\n    # print(f\"filenames: {filenames}\")\n    # print(f\"class_to_idx: {class_to_idx}\")\n    # print(f\"len(labels): {len(labels)}\")\n    # print(f\"len(filenames): {len(filenames)}\")\n    # print(f\"len(class_to_idx): {len(class_to_idx)}\")\n    targets = [class_to_idx[os.path.relpath(os.path.dirname(f), folder)] for f in filenames]\n    # print(f\"targets: {target", "CodeBleu_score": 45.15486979046504}]}
{"instance_id": "e7fe43bdecdc", "generation_result": [{"code": "def pytorch_worker_seed()\n\tworker_info = get_worker_info()\n    if worker_info is not None:\n        # favour the seed already created for pytorch dataloader workers if it exists\n        return worker_info.seed\n    # fallback to wds rank based seed\n    return wds.utils.pytorch_worker_seed()", "CodeBleu_score": 65.96393825034185}, {"code": "def pytorch_worker_seed()\n\tworker_info = get_worker_info()\n    if worker_info is not None:\n        # favour the seed already created for pytorch dataloader workers if it exists\n        return worker_info.seed\n    # fallback to wds rank based seed\n    return wds.utils.pytorch_worker_seed()", "CodeBleu_score": 65.96393825034185}, {"code": "def pytorch_worker_seed()\n\tpass\n\n", "CodeBleu_score": 1.3888888890370064}]}
{"instance_id": "0ffc7e7ded25", "generation_result": [{"code": "def solarize_add(img, add, thresh=128, **__)\n\tlut = []\n    for i in range(256):\n        if i < thresh:\n            lut.append(min(255, i + add))\n        else:\n            lut.append(i)\n    if img.mode in (\"L\", \"RGB\"):\n        if img.mode == \"RGB\" and len(img.split()) == 4:\n            raise ValueError(\"cannot apply solarize to RGBA image\")\n        return img.point(lut)\n    else:\n        return img\n\n", "CodeBleu_score": 66.22267261330227}, {"code": "def solarize_add(img, add, thresh=128, **__)\n\tlut = []\n    for i in range(256):\n        if i < thresh:\n            lut.append(min(255, i + add))\n        else:\n            lut.append(i)\n    if img.mode in (\"L\", \"RGB\"):\n        if img.mode == \"RGB\" and isinstance(add, int):\n            add = (add, add, add)\n        return img.point(lut)\n    else:\n        return img\n\n", "CodeBleu_score": 70.76331300380112}, {"code": "def solarize_add(img, add, thresh=128, **__)\n\tlut = []\n    for i in range(256):\n        if i < thresh:\n            lut.append(min(255, i + add))\n        else:\n            lut.append(i)\n    if img.mode in (\"L\", \"RGB\"):\n        if img.mode == \"RGB\" and len(lut) == 256:\n            lut = lut + lut + lut\n        return img.point(lut)\n    else:\n        return img\n\n", "CodeBleu_score": 84.36617471305652}]}
{"instance_id": "5a0d29577a21", "generation_result": [{"code": "def _get_weighted_transforms(transforms: Dict)\n\ttransforms_names = list(transforms.keys())\n    transforms_probs = np.array([transforms[name] for name in transforms_names])\n    transforms_probs = transforms_probs / np.sum(transforms_probs)\n    return transforms_names, transforms_probs\n\n", "CodeBleu_score": 18.334437807639762}, {"code": "def _get_weighted_transforms(transforms: Dict)\n\ttransform_names = list(transforms.keys())\n    transform_probs = np.array(list(transforms.values()))\n    transform_probs = transform_probs / np.sum(transform_probs)\n    return transform_names, transform_probs\n\n", "CodeBleu_score": 24.05570880901949}, {"code": "def _get_weighted_transforms(transforms: Dict)\n\ttransforms_names = list(transforms.keys())\n    transforms_probs = np.array(list(transforms.values()))\n    transforms_probs = transforms_probs / np.sum(transforms_probs)\n    return transforms_names, transforms_probs\n\n", "CodeBleu_score": 23.754008624901473}]}
{"instance_id": "84bc5658c135", "generation_result": [{"code": "def _apply_basic(self, img, mixing_weights, m)\n\t# Initialize an empty array for the mixed image\n    mixed = np.zeros(img.size, img.mode)\n\n    # Iterate through the mixing weights\n    for weight in mixing_weights:\n        # Apply a random sequence of operations to the image\n        # and accumulate the results\n        mixed += weight * self._apply_ops(img, m)\n\n    # Clip the mixed image to valid pixel values\n    mixed = np.clip(mixed, 0, 255).astype(np.uint8)\n\n    # Convert the mixed image back to an image\n    mixed = Image.fromarray(mixed)\n\n    # Blend the original image and the mixed image\n    # using the specified blending factor\n    return Image.blend(img, mixed, m)", "CodeBleu_score": 22.805392425246655}, {"code": "def _apply_basic(self, img, mixing_weights, m)\n\tmixed = np.zeros(img.shape, dtype=np.uint8)\n    for w in mixing_weights:\n        img = self.apply_to_image(img, w)\n        mixed = np.clip(mixed + w * np.asarray(img), 0, 255).astype(np.uint8)\n    return Image.blend(img, Image.fromarray(mixed), m)", "CodeBleu_score": 20.493022620539563}, {"code": "def _apply_basic(self, img, mixing_weights, m)\n\tmixed = np.zeros(img.shape)\n    for w in mixing_weights:\n        op = random.choice(self.operations)\n        img = op(img, m)\n        mixed += w * img\n    mixed = np.clip(mixed, 0, 255).astype(np.uint8)\n    mixed = Image.fromarray(mixed)\n    return Image.blend(img, mixed, m)\n\n", "CodeBleu_score": 24.968864680788197}]}
{"instance_id": "84b5b752ff04", "generation_result": [{"code": "def adapt_to_chs(x, n)\n\tif not isinstance(x, (tuple, list)):\n        _logger.warning(f\"x is not a tuple or list. Repeating x {n} times.\")\n        x = tuple(repeat(x, n))\n    elif len(x) != n:\n        _logger.warning(f\"x is a tuple or list of length {len(x)}, but n is {n}. Calculating the mean of x and repeating it {n} times.\")\n        x = tuple(repeat(np.mean(x), n))\n    else:\n        assert len(x) == n, f\"x is a tuple or list of length {len(x)}, but n is {n}.\"\n    return x", "CodeBleu_score": 50.11044739246009}, {"code": "def adapt_to_chs(x, n)\n\tif not isinstance(x, (tuple, list)):\n        _logger.warning(f\"The input x is not a tuple or list, so it is repeated {n} times to form a tuple.\")\n        x = repeat(x, n)\n    elif len(x) != n:\n        _logger.warning(f\"The input x is a tuple or list of length {len(x)}, so it is averaged and repeated {n} times to form a tuple.\")\n        x = repeat(np.mean(x), n)\n    elif len(x) == n:\n        assert len(x) == n, f\"The input x is a tuple or list of length {len(x)}, but it must be of length {n}.\"\n    else:\n        raise ValueError(\"The input x must be a tuple or list.\")\n    return x", "CodeBleu_score": 41.43760684788578}, {"code": "def adapt_to_chs(x, n)\n\tif not isinstance(x, (tuple, list)):\n        x = [x] * n\n    else:\n        if len(x) != n:\n            _logger.warning(\n                f\"The length of x ({len(x)}) does not match the number of channels ({n}). The mean of x will be repeated {n} times.\"\n            )\n            x = [np.mean(x)] * n\n        else:\n            assert len(x) == n, f\"The length of x ({len(x)}) does not match the number of channels ({n}).\"\n    return tuple(x)\n\n", "CodeBleu_score": 37.54430052811882}]}
{"instance_id": "54754b6ba394", "generation_result": [{"code": "def _worker_init(worker_id, worker_seeding='all')\n\tworker_info = get_worker_info()\n    if worker_info is None or worker_info.id != worker_id:\n        raise RuntimeError(\"Can't retrieve worker information.\")\n    if callable(worker_seeding):\n        worker_seeding(worker_info)\n    elif worker_seeding == 'all':\n        seed = worker_info.seed % (2 ** 32)\n        random.seed(seed)\n        torch.manual_seed(seed)\n        np.random.seed(seed)\n    elif worker_seeding == 'numpy':\n        seed = worker_info.seed % (2 ** 32)\n        np.random.seed(seed)\n    else:\n        raise ValueError(\"worker_seeding should be 'all', 'numpy', or a callable.\")\n\n", "CodeBleu_score": 45.1092524699332}, {"code": "def _worker_init(worker_id, worker_seeding='all')\n\tworker_info = torch.utils.data.get_worker_info()\n    if worker_info is not None:\n        if worker_info.id != worker_id:\n            raise RuntimeError(\n                'worker_init: worker_info.id != worker_id')\n        if isinstance(worker_seeding, str):\n            if worker_seeding == 'all':\n                seed = worker_info.seed % (2**32)\n                random.seed(seed)\n                torch.manual_seed(seed)\n                np.random.seed(seed)\n            elif worker_seeding == 'numpy':\n                seed = worker_info.seed % (2**32)\n                np.random.seed(seed)\n            else:\n                raise ValueError(\n                    f'worker_init: unknown worker_seeding={worker_seeding}')\n        elif callable(worker_seeding):\n            worker_seeding(worker_info.seed)\n        else:\n            raise ValueError(\n                f'worker_init: unknown worker_seeding={worker_seeding}')\n    else:\n        raise RuntimeError(\n            'worker_init: worker_info is None')", "CodeBleu_score": 42.227718583469006}, {"code": "def _worker_init(worker_id, worker_seeding='all')\n\tworker_info = torch.utils.data.get_worker_info()\n    if worker_info is None:\n        return\n    if worker_info.id != worker_id:\n        raise RuntimeError('Expected worker_id to be {}, but got {}'.format(\n            worker_id, worker_info.id))\n    if isinstance(worker_seeding, str):\n        seed = worker_info.seed % (2**32 - 1)\n        random.seed(seed)\n        torch.manual_seed(seed)\n        if worker_seeding == 'all':\n            np.random.seed(seed)\n    elif callable(worker_seeding):\n        worker_seeding(worker_info.seed % (2**32 - 1))\n    else:\n        raise ValueError('worker_seeding has to be either a callable or \"all\"')", "CodeBleu_score": 51.45161645855859}]}
{"instance_id": "81dfdce940db", "generation_result": [{"code": "def _decode_and_center_crop(image_bytes, image_size, resize_method)\n\tshape = extract_jpeg_shape(image_bytes)\n    image_height = shape[0]\n    image_width = shape[1]\n    padded_center_crop_size = _padded_center_crop(image_height, image_width, image_size, _CROP_PADDING)\n    offset_height = ((image_height - padded_center_crop_size[0]) + 1) // 2\n    offset_width = ((image_width - padded_center_crop_size[1]) + 1) // 2\n    crop_window = tf.stack([offset_height, offset_width,\n                            padded_center_crop_size[0], padded_center_crop_size[1]])\n    image = tf.image.decode_and_crop_jpeg(image_bytes, crop_window, channels=3)\n    image = tf.image.resize(image, image_size, method=resize_method)\n    return image", "CodeBleu_score": 37.04216346344592}, {"code": "def _decode_and_center_crop(image_bytes, image_size, resize_method)\n\tshape = extract_jpeg_shape(image_bytes)\n    image_height = shape[0]\n    image_width = shape[1]\n    padded_center_crop_size = _padded_center_crop(image_height, image_width, image_size, _CROP_PADDING)\n    offset_height = ((image_height - padded_center_crop_size[0]) + 1) // 2\n    offset_width = ((image_width - padded_center_crop_size[1]) + 1) // 2\n    crop_window = tf.stack([offset_height, offset_width,\n                            padded_center_crop_size[0], padded_center_crop_size[1]])\n    image = tf.image.decode_and_crop_jpeg(image_bytes, crop_window, channels=3)\n    image = tf.image.resize([image], [image_size, image_size],\n                            method=resize_method)[0]\n    return image\n\n", "CodeBleu_score": 40.68285993517458}, {"code": "def _decode_and_center_crop(image_bytes, image_size, resize_method)\n\t# extract image shape\n    shape = extract_jpeg_shape(image_bytes)\n    # calculate padded center crop size\n    padded_center_crop_size = _calculate_padded_center_crop_size(image_size, shape)\n    # determine offset for crop window\n    offset = _determine_offset(image_size, padded_center_crop_size, shape)\n    # decode and crop image\n    image = tf.image.decode_and_crop_jpeg(image_bytes, offset, padded_center_crop_size)\n    # resize image\n    image = resize(image, image_size, resize_method)\n    return image\n\n", "CodeBleu_score": 11.655925867023742}]}
{"instance_id": "cf16a6d37111", "generation_result": [{"code": "def preprocess_for_train(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\timage = _decode_and_random_crop(image_bytes, image_size, interpolation)\n    image = _flip(image)\n    image = convert_image_dtype(image, use_bfloat16)\n    image = tf.reshape(image, [image_size, image_size, 3])\n    return image\n\n", "CodeBleu_score": 27.84609433280479}, {"code": "def preprocess_for_train(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\timage = _decode_and_random_crop(image_bytes, image_size, interpolation)\n    image = _flip(image)\n    image = convert_image_dtype(image, use_bfloat16)\n    return image\n\n", "CodeBleu_score": 18.665097925086506}, {"code": "def preprocess_for_train(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\timage_size = utils.parse_image_size(image_size)\n    image = _decode_and_random_crop(image_bytes, image_size, interpolation)\n    image = _flip(image)\n    image = tf.reshape(image, [image_size, image_size, 3])\n    image = utils.convert_image_dtype(image, use_bfloat16)\n    return image\n\n", "CodeBleu_score": 32.30530755612415}]}
{"instance_id": "78b2a3bd2142", "generation_result": [{"code": "def preprocess_for_eval(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\tif use_bfloat16:\n        dtype = tf.bfloat16\n    else:\n        dtype = tf.float32\n\n    if interpolation == 'bicubic':\n        resize_method = tf.image.ResizeMethod.BICUBIC\n    elif interpolation == 'bilinear':\n        resize_method = tf.image.ResizeMethod.BILINEAR\n    elif interpolation == 'nearest':\n        resize_method = tf.image.ResizeMethod.NEAREST_NEIGHBOR\n    else:\n        raise ValueError('Invalid interpolation method: {}'.format(interpolation))\n\n    image = _decode_and_center_crop(image_bytes, image_size, resize_method)\n    image = tf.reshape(image, [image_size, image_size, 3])\n    image = convert_image_dtype(image, dtype)\n\n    return image\n\n", "CodeBleu_score": 57.840726500810824}, {"code": "def preprocess_for_eval(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\tif use_bfloat16:\n        dtype = tf.bfloat16\n    else:\n        dtype = tf.float32\n\n    if interpolation == 'bicubic':\n        resize_method = tf.image.ResizeMethod.BICUBIC\n    elif interpolation == 'bilinear':\n        resize_method = tf.image.ResizeMethod.BILINEAR\n    else:\n        raise ValueError('Invalid interpolation method: {}'.format(interpolation))\n\n    image = _decode_and_center_crop(image_bytes, image_size, resize_method)\n    image = tf.reshape(image, [image_size, image_size, 3])\n    image = tf.image.convert_image_dtype(image, dtype)\n    return image\n\n", "CodeBleu_score": 60.12352442304484}, {"code": "def preprocess_for_eval(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\tif interpolation == 'bicubic':\n        resize_method = tf.image.ResizeMethod.BICUBIC\n    elif interpolation == 'bilinear':\n        resize_method = tf.image.ResizeMethod.BILINEAR\n    elif interpolation == 'nearest':\n        resize_method = tf.image.ResizeMethod.NEAREST_NEIGHBOR\n    else:\n        raise ValueError('Invalid interpolation method: {}'.format(interpolation))\n\n    image = _decode_and_center_crop(image_bytes, image_size, resize_method)\n    image = tf.reshape(image, [image_size, image_size, 3])\n\n    image = convert_image_dtype(image, use_bfloat16)\n\n    return image\n\n", "CodeBleu_score": 54.825799488803476}]}
{"instance_id": "8bd4364a9a0e", "generation_result": [{"code": "def evaluate(self, model, eval_dataloader=None, output_file=\"merged\")\n\tif eval_dataloader is None:\n        eval_dataloader = self.eval_dataloader\n    self.predict(model, eval_dataloader, output_file)\n    return self.compute_metrics(eval_dataloader)", "CodeBleu_score": 46.45265199314374}, {"code": "def evaluate(self, model, eval_dataloader=None, output_file=\"merged\")\n\tif eval_dataloader is None:\n        eval_dataloader = self.get_eval_dataloader()\n\n    output_file = os.path.join(self.args.output_dir, output_file)\n    self.predict(model, eval_dataloader, output_file)\n\n    metrics = self.compute_metrics(eval_dataloader)\n\n    return metrics", "CodeBleu_score": 54.711339932474125}, {"code": "def evaluate(self, model, eval_dataloader=None, output_file=\"merged\")\n\tif eval_dataloader is None:\n        eval_dataloader = self.eval_dataloader\n\n    # Predict\n    self.predict(model, eval_dataloader, output_file)\n\n    # Compute metrics\n    metrics = self.compute_metrics(output_file)\n\n    return metrics", "CodeBleu_score": 53.358750452130934}]}
{"instance_id": "22d682c958c0", "generation_result": [{"code": " def print_computed_metrics(self, metrics)\n\tprint(f\"Rank1: {metrics['Rank1']:.4f}\")\n    print(f\"Rank5: {metrics['Rank5']:.4f}\")\n    print(f\"mIoU: {metrics['mIoU']:.4f}\")", "CodeBleu_score": 17.810665500005612}, {"code": " def print_computed_metrics(self, metrics)\n\tprint(\"Rank1: {0:.4f}\".format(metrics[\"Rank1\"]))\n    print(\"Rank5: {0:.4f}\".format(metrics[\"Rank5\"]))\n    print(\"mIoU: {0:.4f}\".format(metrics[\"MeanIoU\"]))", "CodeBleu_score": 23.627747276954583}, {"code": " def print_computed_metrics(self, metrics)\n\tprint(\"rank1: {:.4f}\".format(metrics[\"rank1\"]))\n    print(\"rank5: {:.4f}\".format(metrics[\"rank5\"]))\n    print(\"miou: {:.4f}\".format(metrics[\"miou\"]))", "CodeBleu_score": 25.59831617213768}]}
{"instance_id": "39c1a50b7e7d", "generation_result": [{"code": "def _eval_predictions(self, segments, data)\n\t# Calculate IoU and rank for each segment-data pair\n    iou = []\n    rank = []\n    for seg, gt in zip(segments, data):\n        iou.append(self._iou(seg, gt))\n        rank.append(self._rank(seg, gt))\n    # Average the top 10 values for each\n    average_iou = np.mean(np.sort(iou)[-10:])\n    average_ranks = np.mean(np.sort(rank)[:10])\n    # Calculate the percentage of segments with rank 1 and rank 5\n    rank1 = np.sum(np.array(rank) == 1) / len(rank)\n    rank5 = np.sum(np.array(rank) <= 5) / len(rank)\n    return rank1, rank5, average_iou\n\n", "CodeBleu_score": 32.738763363053536}, {"code": "def _eval_predictions(self, segments, data)\n\taverage_iou = []\n    average_ranks = []\n    for seg, gt in zip(segments, data):\n        ious = [_iou(pred, gt) for pred in seg]\n        ranks = [_rank(pred, gt) for pred in seg]\n        average_iou.append(np.sum(np.sort(ious)[-3:])/3)\n        average_ranks.append(np.mean(np.sort(ranks)[:3]))\n    return np.mean(average_ranks), np.mean(average_iou)", "CodeBleu_score": 25.58276280078059}, {"code": "def _eval_predictions(self, segments, data)\n\tassert len(segments) == len(data)\n    average_iou = []\n    average_ranks = []\n    for segment, gt in zip(segments, data):\n        iou = [self._iou(pred, gt) for pred in segment]\n        average_iou.append(np.mean(np.sort(iou)[-3:]))\n        rank = [self._rank(segment, gt) for gt in data]\n        average_ranks.append(np.mean(np.sort(rank)[-3:]))\n    return np.mean(average_ranks), np.mean(average_iou)", "CodeBleu_score": 26.893484843137994}]}
{"instance_id": "8ccf84f8d034", "generation_result": [{"code": "def predict_loop(self, model, eval_dataloader, output_file=None)\n\tself.eval\n    self.to_ctx\n    self.finalize\n\n    scores = []\n    model.eval()\n    model.to(self.ctx[0])\n    for batch in tqdm(eval_dataloader, desc=\"Predicting\"):\n        batch = self.to_ctx(batch)\n        with torch.no_grad():\n            outputs = model(**batch)\n        outputs = self.postprocess_outputs(outputs)\n        outputs = self.update_outputs(outputs, batch)\n        scores.append(outputs)\n    return self.finalize(scores, output_file)\n\n", "CodeBleu_score": 42.918417701095365}, {"code": "def predict_loop(self, model, eval_dataloader, output_file=None)\n\tscores = []\n    model.eval()\n    with torch.no_grad():\n        for batch in eval_dataloader:\n            batch = self.to_ctx(batch)\n            outputs = model(**batch)\n            outputs = self.postprocess(outputs)\n            scores.append(outputs)\n    scores = self.finalize(scores, output_file)\n    return scores", "CodeBleu_score": 42.25043727886939}, {"code": "def predict_loop(self, model, eval_dataloader, output_file=None)\n\t# Initialize the scores\n    scores = []\n\n    # Set the model to evaluation mode\n    model.eval()\n\n    # Transfer the model to GPU 0\n    model.to(0)\n\n    # Iterate over the dataloader\n    for batch in tqdm(eval_dataloader, desc=\"Predicting\"):\n\n        # Transfer data to GPU 0\n        batch = self.to_ctx(batch, 0)\n\n        # Make predictions without computing gradients\n        with torch.no_grad():\n            outputs = model(**batch)\n\n        # Update the scores\n        scores.append(outputs)\n\n        # Process the outputs\n        outputs = self.process_outputs(outputs)\n\n    # Finalize and return the results\n    results = self.finalize(outputs, output_file)\n    return results", "CodeBleu_score": 45.425911309913765}]}
{"instance_id": "e68529ad727d", "generation_result": [{"code": "def _aggregate_scores(self, scores)\n\tassert len(scores) == 2\n    self.full_scores = np.concatenate(scores, axis=0)\n    return np.matmul(self.full_scores[:, 0], self.full_scores[:, 1].T)", "CodeBleu_score": 32.40441150447836}, {"code": "def _aggregate_scores(self, scores)\n\tassert len(scores) == 2, \"The scores must be a list of two sets of scores.\"\n    self.full_scores = np.concatenate(scores, axis=0)\n    return np.matmul(self.full_scores[:, 0], self.full_scores[:, 1].T)", "CodeBleu_score": 31.1866680474304}, {"code": "def _aggregate_scores(self, scores)\n\tassert len(scores) == 2\n    self.full_scores = None\n    scores = np.concatenate(scores, axis=0)\n    return np.matmul(scores[:, 0], scores[:, 1].T)", "CodeBleu_score": 33.05571694673641}]}
{"instance_id": "e4e1861fc357", "generation_result": [{"code": "def finalize(self, Y_pred, Y_true, output_file=None)\n\t# Concatenate predictions and true labels\n    Y_pred = torch.cat(Y_pred, dim=0).numpy()\n    Y_true = torch.cat(Y_true, dim=0).numpy()\n\n    # Check for prediction errors\n    if np.any(Y_pred != Y_true):\n        print(\"Prediction errors:\")\n        for i, (y_pred, y_true) in enumerate(zip(Y_pred, Y_true)):\n            if y_pred != y_true:\n                print(f\"Predicted label: {y_pred}, true label: {y_true}\")\n                print(f\"Sample {i}: {self.data.samples[i]}\")\n\n    # Save predictions and true labels\n    if output_file is not None:\n        with open(output_file, \"wb\") as f:\n            pickle.dump({\"Y_pred\": Y_pred, \"Y_true\": Y_true}, f)\n\n    return {\"Y_pred\": Y_pred, \"Y_true\": Y_true}", "CodeBleu_score": 43.68880339398998}, {"code": "def finalize(self, Y_pred, Y_true, output_file=None)\n\tY_pred = np.concatenate(Y_pred)\n    Y_true = np.concatenate(Y_true)\n    if output_file is not None:\n        pickle.dump((Y_pred, Y_true), open(output_file, \"wb\"))\n    if self.verbose:\n        print(\"Predicted labels: \", Y_pred[:10])\n        print(\"True labels: \", Y_true[:10])\n        print(\"Prediction error: \", np.mean(Y_pred != Y_true))\n    return {\"Y_pred\": Y_pred, \"Y_true\": Y_true}", "CodeBleu_score": 25.869651257013704}, {"code": "def finalize(self, Y_pred, Y_true, output_file=None)\n\tY_pred = torch.cat(Y_pred, dim=0).numpy()\n    Y_true = torch.cat(Y_true, dim=0).numpy()\n\n    if output_file is not None:\n        with open(output_file, 'wb') as f:\n            pickle.dump([Y_pred, Y_true], f)\n\n    print(\"Predicted labels shape:\", Y_pred.shape)\n    print(\"True labels shape:\", Y_true.shape)\n\n    if Y_pred.shape != Y_true.shape:\n        print(\"Prediction Error: Predicted labels and true labels have different shapes!\")\n        print(\"Predicted labels shape:\", Y_pred.shape)\n        print(\"True labels shape:\", Y_true.shape)\n        print(\"Predicted labels:\", Y_pred[:10])\n        print(\"True labels:\", Y_true[:10])\n\n    return {\"Y_pred\": Y_pred, \"Y_true\": Y_true}", "CodeBleu_score": 38.67590194410984}]}
{"instance_id": "7df9ab9a4920", "generation_result": [{"code": "def reduce_metrics(logging_outputs) -> None\n\t\"\"\"\n    The function reduces metrics by calculating the average loss from logging outputs.\n\n    It sums the loss values and sample sizes from the logs, then logs the average loss as a scalar with three decimal places.\n    \"\"\"\n    \"\"\"\n    The function reduces metrics by calculating the average loss from logging outputs.\n\n    It sums the loss values and sample sizes from the logs, then logs the average loss as a scalar with three decimal places.\n    \"\"\"\n    \"\"\"\n    The function reduces metrics by calculating the average loss from logging outputs.\n\n    It sums the loss values and sample sizes from the logs, then logs the average loss as a scalar with three decimal places.\n    \"\"\"\n    \"\"\"\n    The function reduces metrics by calculating the average loss from logging outputs.\n\n    It sums the loss values and sample sizes from the logs, then logs the average loss as a scalar with three decimal places.\n    \"\"\"\n    \"\"\"\n    The function reduces metrics by calculating the average loss from logging outputs.\n\n    It sums the loss values and sample sizes from the logs, then logs the average loss as a scalar with three decimal places.\n    \"\"\"\n    \"\"\"\n    The function reduces metrics by calculating the average loss from logging outputs.\n\n    It sums the loss values and sample sizes from the logs, then logs the average loss as a scalar with three decimal places.\n    \"\"\"\n    \"\"\"\n    The function reduces metrics by calculating the average loss from logging outputs.\n\n    It sums the loss values and sample sizes from the logs, then logs the average loss as a scalar with three decimal places.\n    \"\"\"\n    \"\"\"\n    The function reduces metrics by calculating the average loss from logging outputs.\n\n    It sums the loss values and sample sizes from the logs, then logs the average loss as a scalar with three decimal places.\n    \"\"\"\n    \"\"\"\n    The function reduces metrics by calculating the average loss from logging outputs.\n\n    It sums the loss values and sample sizes from the logs, then logs the average loss as a scalar with three decimal places.\n    \"\"\"\n    \"\"\"\n    The function reduces metrics by calculating the average loss from logging outputs.\n\n    It sums the loss values and sample sizes from the logs, then logs the average loss as a scalar with three decimal places.\n    \"\"\"\n    \"\"\"\n    The function reduces metrics by calculating the average loss from logging outputs.\n\n    It", "CodeBleu_score": 4.488632537209555}, {"code": "def reduce_metrics(logging_outputs) -> None\n\t\"\"\"\n    Args:\n        logging_outputs (List[Dict[str, Any]]): A list of dictionaries containing the following keys:\n            - 'loss' (FloatTensor): a tensor containing the loss value\n            - 'sample_size' (FloatTensor): a tensor containing the sample size\n    \"\"\"\n    \"\"\"\n    \"\"\"\n    loss_sum = sum(log.get(\"loss\", 0) for log in logging_outputs)\n    sample_size = sum(log.get(\"sample_size\", 0) for log in logging_outputs)\n    metrics.log_scalar(\n        \"loss\", loss_sum / sample_size, sample_size, round=3\n    )\n    \"\"\"\n    \"\"\"", "CodeBleu_score": 45.47872522641023}, {"code": "def reduce_metrics(logging_outputs) -> None\n\t\"\"\"\n    The function reduces metrics by calculating the average loss from logging outputs.\n    It sums the loss values and sample sizes from the logs, then logs the average loss as a scalar with three decimal places.\n    \"\"\"\n    \"\"\"\n    The function reduces metrics by calculating the average loss from logging outputs.\n    It sums the loss values and sample sizes from the logs, then logs the average loss as a scalar with three decimal places.\n    \"\"\"\n    \"\"\"\n    The function reduces metrics by calculating the average loss from logging outputs.\n    It sums the loss values and sample sizes from the logs, then logs the average loss as a scalar with three decimal places.\n    \"\"\"\n    \"\"\"\n    The function reduces metrics by calculating the average loss from logging outputs.\n    It sums the loss values and sample sizes from the logs, then logs the average loss as a scalar with three decimal places.\n    \"\"\"\n    \"\"\"\n    The function reduces metrics by calculating the average loss from logging outputs.\n    It sums the loss values and sample sizes from the logs, then logs the average loss as a scalar with three decimal places.\n    \"\"\"\n    \"\"\"\n    The function reduces metrics by calculating the average loss from logging outputs.\n    It sums the loss values and sample sizes from the logs, then logs the average loss as a scalar with three decimal places.\n    \"\"\"\n    \"\"\"\n    The function reduces metrics by calculating the average loss from logging outputs.\n    It sums the loss values and sample sizes from the logs, then logs the average loss as a scalar with three decimal places.\n    \"\"\"\n    \"\"\"\n    The function reduces metrics by calculating the average loss from logging outputs.\n    It sums the loss values and sample sizes from the logs, then logs the average loss as a scalar with three decimal places.\n    \"\"\"\n    \"\"\"\n    The function reduces metrics by calculating the average loss from logging outputs.\n    It sums the loss values and sample sizes from the logs, then logs the average loss as a scalar with three decimal places.\n    \"\"\"\n    \"\"\"\n    The function reduces metrics by calculating the average loss from logging outputs.\n    It sums the loss values and sample sizes from the logs, then logs the average loss as a scalar with three decimal places.\n    \"\"\"\n    \"\"\"\n    The function reduces metrics by calculating the average loss from logging outputs.\n    It sums the loss values and sample sizes from the logs,", "CodeBleu_score": 4.488632537209555}]}
{"instance_id": "7f9c326a96f7", "generation_result": [{"code": "def _mm_on_the_fly(\n        self,\n        cmasks,\n        vmasks,\n        attention_mask\n    )\n\tif attention_mask is None:\n        attention_mask = self._mm_attention_mask(cmasks, vmasks)\n    token_type_ids = torch.cat(\n        [\n            torch.zeros(\n                cmasks.size(0),\n                cmasks.size(1),\n                dtype=torch.long,\n                device=cmasks.device,\n            ),\n            torch.ones(\n                vmasks.size(0),\n                vmasks.size(1),\n                dtype=torch.long,\n                device=vmasks.device,\n            ),\n        ],\n        dim=1,\n    )\n    return attention_mask, token_type_ids\n\n", "CodeBleu_score": 70.22292921070239}, {"code": "def _mm_on_the_fly(\n        self,\n        cmasks,\n        vmasks,\n        attention_mask\n    )\n\tif attention_mask is None:\n        attention_mask = torch.cat([cmasks[:, :1], vmasks, cmasks[:, 1:]], dim=1)\n    token_type_ids = torch.cat([torch.zeros(cmasks.size(0), 1), torch.ones(cmasks.size(0), vmasks.size(1)), torch.zeros(cmasks.size(0), cmasks.size(1))], dim=1)\n    return attention_mask, token_type_ids\n\n", "CodeBleu_score": 40.68854357641627}, {"code": "def _mm_on_the_fly(\n        self,\n        cmasks,\n        vmasks,\n        attention_mask\n    )\n\tif attention_mask is None:\n        attention_mask = self._mm_attention_mask(cmasks, vmasks)\n    token_type_ids = torch.cat([torch.zeros(cmasks.size(0), 1), torch.ones(cmasks.size(0), 1)], dim=1)\n    token_type_ids = token_type_ids[:, None, :].repeat(1, attention_mask.size(-1), 1)\n    return attention_mask, token_type_ids", "CodeBleu_score": 48.447670828742794}]}
{"instance_id": "fa99d2a148a5", "generation_result": [{"code": "def prepare_inputs_for_generation(\n        self,\n        input_ids,\n        input_video_embeds,\n        attention_mask=None,\n        token_type_ids=None,\n        **model_kwargs\n    )\n\tif input_ids is not None and input_video_embeds is not None:\n            input_ids = input_ids[:, -1:]\n            input_video_embeds = input_video_embeds[:, -1:]\n            attention_mask = attention_mask[:, -1:]\n            token_type_ids = token_type_ids[:, -1:]\n\n        return {\n            \"input_ids\": input_ids,\n            \"input_video_embeds\": input_video_embeds,\n            \"attention_mask\": attention_mask,\n            \"token_type_ids\": token_type_ids,\n        }", "CodeBleu_score": 39.402272855221625}, {"code": "def prepare_inputs_for_generation(\n        self,\n        input_ids,\n        input_video_embeds,\n        attention_mask=None,\n        token_type_ids=None,\n        **model_kwargs\n    )\n\t# cut decoder_input_ids if past is used\n        if input_ids is not None:\n            input_ids = input_ids[:, -1].unsqueeze(-1)\n        if input_video_embeds is not None:\n            input_video_embeds = input_video_embeds[:, -1].unsqueeze(-1)\n        # update sequence length to match input_ids and input_video_embeds\n        sequence_length = input_ids.shape[1] + input_video_embeds.shape[1]\n\n        # update attention mask\n        if attention_mask is not None:\n            attention_mask = attention_mask[:, -1].unsqueeze(-1)\n            attention_mask = self._merge_attention_mask(attention_mask, sequence_length)\n        # update token type ids\n        if token_type_ids is not None:\n            token_type_ids = token_type_ids[:, -1].unsqueeze(-1)\n\n        # merge inputs\n        input_ids = self._merge_inputs(input_ids, input_video_embeds)\n\n        return {\n            \"input_ids\": input_ids,\n            \"attention_mask\": attention_mask,\n            \"token_type_ids\": token_type_ids,\n        }", "CodeBleu_score": 46.62073347509287}, {"code": "def prepare_inputs_for_generation(\n        self,\n        input_ids,\n        input_video_embeds,\n        attention_mask=None,\n        token_type_ids=None,\n        **model_kwargs\n    )\n\t# 1. cut to max_seq_len\n        if attention_mask is not None:\n            max_seq_len = attention_mask.sum(dim=1).max().item()\n            input_ids = input_ids[:, :max_seq_len]\n            input_video_embeds = input_video_embeds[:, :max_seq_len]\n            attention_mask = attention_mask[:, :max_seq_len]\n            token_type_ids = token_type_ids[:, :max_seq_len]\n\n        # 2. adjust the sequence length to match the combined length of input_ids and input_video_embeds\n        seq_len = input_ids.size(1) + input_video_embeds.size(1)\n        input_ids = torch.cat([input_ids, torch.zeros(input_ids.size(0), seq_len - input_ids.size(1), dtype=torch.long, device=input_ids.device)], dim=1)\n        input_video_embeds = torch.cat([input_video_embeds, torch.zeros(input_video_embeds.size(0), seq_len - input_video_embeds.size(1), dtype=torch.float, device=input_video_embeds.device)], dim=1)\n        attention_mask = torch.cat([attention_mask, torch.zeros(attention_mask.size(0), seq_len - attention_mask.size(1), dtype=torch.long, device=attention_mask.device)], dim=1)\n        token_type_ids = torch.cat([token_type_ids, torch.zeros(token_type_ids.size(0), seq_len - token_type_ids.size(1), dtype=torch.long, device=token_type_ids.device)], dim=1)\n\n        # 3. adjust attention_mask and token_type_ids to fit the new sequence length\n        attention_mask = attention_mask[:, :seq_len]\n        token_type_ids = token_type_ids[:, :seq_len]", "CodeBleu_score": 38.751997598705195}]}
{"instance_id": "b1b0d25fe620", "generation_result": [{"code": "def _init_pipeline(self, cfg: ConfigType) -> Compose\n\tif 'meta_keys' in cfg:\n        cfg['meta_keys'] = tuple(k for k in cfg['meta_keys'] if k != 'img_id')\n\n    load_idx = self._get_transform_idx(cfg, 'LoadImageFromFile')\n    if load_idx == -1:\n        raise ValueError('The pipeline must contain LoadImageFromFile transform.')\n\n    cfg[load_idx]['type'] = 'mmdet.InferencerLoader'\n    return Compose(cfg)\n\n", "CodeBleu_score": 34.06818766981518}, {"code": "def _init_pipeline(self, cfg: ConfigType) -> Compose\n\tif 'meta_keys' in cfg:\n            if 'img_id' in cfg['meta_keys']:\n                cfg['meta_keys'].remove('img_id')\n\n        inferencer_loader_idx = self._get_transform_idx(cfg, 'LoadImageFromFile')\n        if inferencer_loader_idx == -1:\n            raise ValueError(\n                'Could not find the LoadImageFromFile transform in the pipeline.')\n\n        cfg[inferencer_loader_idx]['type'] = 'mmdet.InferencerLoader'\n        return Compose(cfg)\n\n", "CodeBleu_score": 31.95765708062058}, {"code": "def _init_pipeline(self, cfg: ConfigType) -> Compose\n\tmeta_keys = cfg.get('meta_keys', ['img_id'])\n    if 'img_id' in meta_keys:\n        meta_keys.remove('img_id')\n    transforms = []\n    for i, transform in enumerate(cfg['transforms']):\n        if transform['type'] == 'LoadImageFromFile':\n            transform['type'] = 'mmdet.InferencerLoader'\n        transforms.append(transform)\n    return Compose(transforms)", "CodeBleu_score": 22.108559326546267}]}
{"instance_id": "7967962856eb", "generation_result": [{"code": "def _inputs_to_list(self, inputs: InputsType) -> list\n\tif isinstance(inputs, str):\n        if isdir(inputs):\n            inputs = list_dir_or_file(inputs)\n        else:\n            inputs = [inputs]\n    elif not isinstance(inputs, (list, tuple)):\n        inputs = [inputs]\n    return inputs", "CodeBleu_score": 36.96059066575894}, {"code": "def _inputs_to_list(self, inputs: InputsType) -> list\n\tif isinstance(inputs, str):\n        if isdir(inputs):\n            inputs = list_dir_or_file(inputs)\n    if not isinstance(inputs, (list, tuple)):\n        inputs = [inputs]\n    return inputs\n\n", "CodeBleu_score": 35.09831618851747}, {"code": "def _inputs_to_list(self, inputs: InputsType) -> list\n\tif isinstance(inputs, str):\n        if isdir(inputs):\n            inputs = list_dir_or_file(inputs)\n        else:\n            inputs = [inputs]\n    elif not isinstance(inputs, (list, tuple)):\n        inputs = [inputs]\n    return inputs\n\n\n", "CodeBleu_score": 36.96059066575894}]}
{"instance_id": "74f727f06fa0", "generation_result": [{"code": "def _get_chunk_data(self, inputs: Iterable, chunk_size: int)\n\tchunk_data = []\n    for input in inputs:\n        if isinstance(input, dict) and (\"img\" in input or \"img_path\" in input):\n            input = self.pipeline(copy.deepcopy(input))\n        chunk_data.append(input)\n        if len(chunk_data) == chunk_size:\n            yield chunk_data\n            chunk_data = []\n    if chunk_data:\n        yield chunk_data", "CodeBleu_score": 30.893765693270343}, {"code": "def _get_chunk_data(self, inputs: Iterable, chunk_size: int)\n\tchunk_data = []\n    for input in inputs:\n        if isinstance(input, dict) and (\"img\" in input or \"img_path\" in input):\n            input = self.pipeline(input)\n        chunk_data.append(input)\n        if len(chunk_data) == chunk_size:\n            yield chunk_data\n            chunk_data = []\n    if len(chunk_data) > 0:\n        yield chunk_data\n\n", "CodeBleu_score": 26.414917464439313}, {"code": "def _get_chunk_data(self, inputs: Iterable, chunk_size: int)\n\tchunk_data = []\n    for input in inputs:\n        if isinstance(input, dict):\n            if 'img' in input:\n                input = input['img']\n            elif 'img_path' in input:\n                input = input['img_path']\n        chunk_data.append(input)\n        if len(chunk_data) == chunk_size:\n            yield self.pipeline(chunk_data)\n            chunk_data = []\n    if chunk_data:\n        yield self.pipeline(chunk_data)\n", "CodeBleu_score": 32.17740115562066}]}
{"instance_id": "a43ab51afa59", "generation_result": [{"code": "def postprocess(\n        self,\n        preds: PredType,\n        visualization: Optional[List[np.ndarray]] = None,\n        return_datasamples: bool = False,\n        print_result: bool = False,\n        no_save_pred: bool = False,\n        pred_out_dir: str = '',\n        **kwargs,\n    ) -> Dict\n\tif not no_save_pred:\n        if not osp.isdir(pred_out_dir):\n            os.makedirs(pred_out_dir)\n        if not osp.isdir(osp.join(pred_out_dir, 'preds')):\n            os.makedirs(osp.join(pred_out_dir, 'preds'))\n\n    results = []\n    if return_datasamples:\n        warnings.warn('Saving datasamples is not supported.')\n    for data_sample in preds:\n        result = self.pred2dict(data_sample, pred_out_dir)\n        if return_datasamples:\n            data_sample.add_result(result)\n            results.append(data_sample)\n        else:\n            results.append(result)\n\n    if print_result:\n        print(results)\n\n    return results\n\n", "CodeBleu_score": 49.910757339316525}, {"code": "def postprocess(\n        self,\n        preds: PredType,\n        visualization: Optional[List[np.ndarray]] = None,\n        return_datasamples: bool = False,\n        print_result: bool = False,\n        no_save_pred: bool = False,\n        pred_out_dir: str = '',\n        **kwargs,\n    ) -> Dict\n\tif not no_save_pred and pred_out_dir != '':\n        if return_datasamples:\n            warnings.warn(\n                'Saving datasamples is not supported. '\n                'Please set return_datasamples to False.')\n        else:\n            self.num_predicted_imgs = 0\n            mmcv.mkdir_or_exist(osp.join(pred_out_dir, 'preds'))\n\n    if return_datasamples:\n        if isinstance(preds, tuple):\n            preds = preds[0]\n        results = []\n        for data_sample in preds:\n            results.append(data_sample)\n    else:\n        results = []\n        for data_sample in preds:\n            results.append(self.pred2dict(data_sample, pred_out_dir))\n\n    if visualization is not None:\n        results.append(visualization)\n\n    if print_result:\n        print(results)\n\n    return results\n\n", "CodeBleu_score": 47.271354451365745}, {"code": "def postprocess(\n        self,\n        preds: PredType,\n        visualization: Optional[List[np.ndarray]] = None,\n        return_datasamples: bool = False,\n        print_result: bool = False,\n        no_save_pred: bool = False,\n        pred_out_dir: str = '',\n        **kwargs,\n    ) -> Dict\n\t# Initialize the output directory if predictions are not to be saved.\n        if not no_save_pred and pred_out_dir == '':\n            pred_out_dir = osp.join(self.work_dir, 'preds')\n            mmcv.mkdir_or_exist(pred_out_dir)\n\n        # Prepare a result dictionary.\n        results = {}\n        if 'img_path' in preds:\n            img_path = osp.basename(preds['img_path'])\n            img_path = osp.splitext(img_path)[0]\n            out_img_path = osp.join(pred_out_dir, 'preds',\n                                    img_path + '_panoptic_seg.png')\n            out_json_path = osp.join(pred_out_dir, 'preds', img_path + '.json')\n        else:\n            out_img_path = osp.join(\n                pred_out_dir, 'preds',\n                f'{self.num_predicted_imgs}_panoptic_seg.png')\n            out_json_path = osp.join(pred_out_dir, 'preds',\n                                     f'{self.num_predicted_imgs}.json')\n            self.num_predicted_imgs += 1\n\n        # Convert predictions to dictionaries and append them to results.\n        if not return_datasamples:\n            for key in preds:\n                if key == 'pred_instances':\n                    masks = preds[key].get('masks')\n                    pred_instances = preds[key].numpy()\n                    result = {\n                        'labels': pred_instances.labels.tolist(),\n                        'scores': pred_instances.scores.tolist()\n                    }\n                    if 'bboxes' in pred_instances:\n                        result['bboxes'] = pred_instances.bboxes.tolist()\n                    if masks is not None:\n                        if 'bboxes' not in pred_instances or pred_instances.bboxes.sum(", "CodeBleu_score": 35.086862891997164}]}
{"instance_id": "a3c2561f199c", "generation_result": [{"code": "def build_test_pipeline(cfg: ConfigType) -> ConfigType\n\t# Get the first transform in the dataset pipeline\n    transform = cfg.dataset.pipeline[0].copy()\n    # Modify the first transform to include only the 'Resize' transform\n    transform['transforms'] = [transform['transforms'][0]]\n    # Get the last transform in the dataset pipeline\n    transform_last = cfg.dataset.pipeline[-1].copy()\n    # Combine the modified first and last transforms into a new test pipeline\n    cfg.dataset.pipeline = [transform, transform_last]\n    cfg.dataset.pipeline = Compose(cfg.dataset.pipeline)\n    return cfg", "CodeBleu_score": 28.53567492979552}, {"code": "def build_test_pipeline(cfg: ConfigType) -> ConfigType\n\tdataset_pipeline = cfg.dataset.pipeline\n    test_pipeline = [\n        dataset_pipeline[0].copy(\n            {\"transforms\": [\"Resize\"]}\n        ),\n        dataset_pipeline[-1].copy(),\n    ]\n    return Compose(test_pipeline)", "CodeBleu_score": 17.040760258389334}, {"code": "def build_test_pipeline(cfg: ConfigType) -> ConfigType\n\t# Copy the first transform in the dataset pipeline\n    transforms_copy = copy.deepcopy(cfg.dataset.pipeline[0])\n\n    # Modify the copied transform to include only the 'Resize' transform\n    transforms_copy['transforms'] = [transforms_copy['transforms'][0]]\n\n    # Copy the last transform in the dataset pipeline\n    transforms_last = copy.deepcopy(cfg.dataset.pipeline[-1])\n\n    # Combine the modified first and copied last transforms into a new test pipeline using Compose\n    cfg.dataset.pipeline = [transforms_copy, transforms_last]\n    cfg.dataset.pipeline = Compose(cfg.dataset.pipeline)\n\n    return cfg", "CodeBleu_score": 28.933115699757828}]}
{"instance_id": "48a358ab5a38", "generation_result": [{"code": "def inference_mot(model: nn.Module, img: np.ndarray, frame_id: int,\n                  video_len: int) -> SampleList\n\t# prepare data\n    data = dict(\n        img=img,\n        frame_id=frame_id,\n        original_shape=img.shape,\n        img_id=frame_id,\n        video_len=video_len)\n    data = SampleList([data])\n\n    # build test pipeline\n    test_pipeline = build_test_pipeline(model.cfg)\n    data = test_pipeline(data)\n\n    # no grad\n    if not model.device.type == 'cpu':\n        torch.no_grad()\n\n    # forward\n    model.eval()\n    data = model(data)\n\n    return data\n\n", "CodeBleu_score": 30.449503602062382}, {"code": "def inference_mot(model: nn.Module, img: np.ndarray, frame_id: int,\n                  video_len: int) -> SampleList\n\t# prepare data\n    data = dict(\n        img=img,\n        frame_id=frame_id,\n        original_shape=img.shape,\n        img_id=frame_id,\n        video_len=video_len)\n\n    # build the data pipeline\n    test_pipeline = model.cfg.test_pipeline\n    test_pipeline = Compose(test_pipeline)\n    data = test_pipeline(data)\n    data = data.to('cpu')\n\n    # forward the model\n    with torch.no_grad():\n        model_output = model(data)\n\n    return model_output\n\n", "CodeBleu_score": 25.931460910133765}, {"code": "def inference_mot(model: nn.Module, img: np.ndarray, frame_id: int,\n                  video_len: int) -> SampleList\n\t# prepare data\n    data = dict(\n        img=img,\n        frame_id=frame_id,\n        original_shape=img.shape,\n        img_id=frame_id,\n        video_len=video_len)\n\n    # build test pipeline\n    test_pipeline = build_test_pipeline(model.cfg)\n\n    # prepare data\n    data = test_pipeline(data)\n\n    # forward the model\n    with torch.no_grad():\n        model_input = model.get_model_input(data)\n        model_output = model(**model_input)\n        model_output = model.get_model_output(model_output)\n\n    return model_output\n\n", "CodeBleu_score": 28.51980748249709}]}
{"instance_id": "6c4ea2ce1787", "generation_result": [{"code": "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\n\twhile True:\n        total_length = len(tokens_a) + len(tokens_b)\n        if total_length <= max_num_tokens:\n            break\n\n        trunc_tokens = tokens_a if len(tokens_a) > len(tokens_b) else tokens_b\n        assert len(trunc_tokens) >= 1\n\n        # We want to sometimes truncate from the front and sometimes from the\n        # back to add more randomness and avoid biases.\n        if rng.random() < 0.5:\n            del trunc_tokens[0]\n        else:\n            trunc_tokens.pop()", "CodeBleu_score": 77.08479493351732}, {"code": "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\n\twhile True:\n        total_length = len(tokens_a) + len(tokens_b)\n        if total_length <= max_num_tokens:\n            break\n\n        trunc_tokens = tokens_a if len(tokens_a) > len(tokens_b) else tokens_b\n        assert len(trunc_tokens) >= 1\n\n        # We want to sometimes truncate from the front and sometimes from the\n        # back to add more randomness and avoid biases.\n        if rng.random() < 0.5:\n            del trunc_tokens[0]\n        else:\n            trunc_tokens.pop()", "CodeBleu_score": 77.08479493351732}, {"code": "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\n\twhile True:\n        total_length = len(tokens_a) + len(tokens_b)\n        if total_length <= max_num_tokens:\n            break\n        trunc_tokens = tokens_a if len(tokens_a) > len(tokens_b) else tokens_b\n        assert len(trunc_tokens) >= 1\n        # We want to sometimes truncate from the front and sometimes from the\n        # back to add more randomness and avoid biases.\n        if rng.random() < 0.5:\n            del trunc_tokens[0]\n        else:\n            trunc_tokens.pop()", "CodeBleu_score": 77.08479493351732}]}
{"instance_id": "007181b6729d", "generation_result": [{"code": "def get_activation(activation_string)\n\tif not isinstance(activation_string, str):\n        return activation_string\n\n    if not activation_string:\n        return None\n\n    activation_string = activation_string.lower()\n    if activation_string == \"relu\":\n        return tf.nn.relu\n    elif activation_string == \"gelu\":\n        return gelu\n    elif activation_string == \"tanh\":\n        return tf.tanh\n    else:\n        raise ValueError(\"Unsupported activation: %s\" % activation_string)", "CodeBleu_score": 76.08833442208918}, {"code": "def get_activation(activation_string)\n\tif not isinstance(activation_string, str):\n        return activation_string\n    if not activation_string:\n        return None\n    activation_string = activation_string.lower()\n    if activation_string == \"relu\":\n        return tf.nn.relu\n    elif activation_string == \"gelu\":\n        return gelu\n    elif activation_string == \"tanh\":\n        return tf.tanh\n    else:\n        raise ValueError(\"Unsupported activation: %s\" % (activation_string))\n\n", "CodeBleu_score": 63.98498758390539}, {"code": "def get_activation(activation_string)\n\tif not isinstance(activation_string, str):\n        return activation_string\n\n    activation_string = activation_string.lower()\n    if not activation_string:\n        return None\n\n    # Threshold\n    elif activation_string == 'threshold':\n        return tf.keras.activations.threshold\n\n    # Linear\n    elif activation_string == 'linear':\n        return tf.keras.activations.linear\n\n    # Relu\n    elif activation_string == 'relu':\n        return tf.keras.activations.relu\n\n    # Selu\n    elif activation_string == 'selu':\n        return tf.keras.activations.selu\n\n    # Elu\n    elif activation_string == 'elu':\n        return tf.keras.activations.elu\n\n    # Softplus\n    elif activation_string == 'softplus':\n        return tf.keras.activations.softplus\n\n    # Softsign\n    elif activation_string == 'softsign':\n        return tf.keras.activations.softsign\n\n    # Sigmoid\n    elif activation_string == 'sigmoid':\n        return tf.keras.activations.sigmoid\n\n    # Tanh\n    elif activation_string == 'tanh':\n        return tf.keras.activations.tanh\n\n    # Hard sigmoid\n    elif activation_string == 'hard_sigmoid':\n        return tf.keras.activations.hard_sigmoid\n\n    # Exponential\n    elif activation_string == 'exponential':\n        return tf.keras.activations.exponential\n\n    # Softmax\n    elif activation_string == 'softmax':\n        return tf.keras.activations.softmax\n\n    # Log softmax\n    elif activation_string == 'log_softmax':\n        return tf.keras.activations.log_softmax\n\n    # Swish\n    elif activation_string == 'swish':\n        return tf.keras.activations.swish\n\n    else:\n        raise ValueError('Unsupported activation: {}'.format(activation_string))", "CodeBleu_score": 49.28339452367151}]}
{"instance_id": "d5d6e0315899", "generation_result": [{"code": "def dropout(input_tensor, dropout_prob)\n\tif dropout_prob is None or dropout_prob == 0.0:\n        return input_tensor\n\n    output = tf.nn.dropout(input_tensor, 1.0 - dropout_prob)\n    return output", "CodeBleu_score": 87.68572576748926}, {"code": "def dropout(input_tensor, dropout_prob)\n\tif dropout_prob is None or dropout_prob == 0.0:\n        return input_tensor\n\n    output = tf.nn.dropout(input_tensor, 1.0 - dropout_prob)\n    return output", "CodeBleu_score": 87.68572576748926}, {"code": "def dropout(input_tensor, dropout_prob)\n\tif dropout_prob is None or dropout_prob == 0.0:\n        return input_tensor\n    output = tf.nn.dropout(input_tensor, 1.0 - dropout_prob)\n    return output\n\n", "CodeBleu_score": 87.68572576748926}]}
{"instance_id": "828917b9ac53", "generation_result": [{"code": "def create_attention_mask_from_input_mask(from_tensor, to_mask)\n\tfrom_shape = get_shape_list(from_tensor, expected_rank=[2, 3])\n    batch_size = from_shape[0]\n    from_seq_length = from_shape[1]\n\n    to_shape = get_shape_list(to_mask, expected_rank=2)\n    to_seq_length = to_shape[1]\n\n    to_mask = tf.cast(tf.reshape(to_mask, [batch_size, 1, to_seq_length]), tf.float32)\n\n    # We don't assume that `from_tensor` is a mask (although it could be). We\n    # don't actually care if we attend *from* padding tokens (only *to* padding)\n    # tokens so we create a tensor of all ones.\n    #\n    # `broadcast_ones` = [batch_size, from_seq_length, 1]\n    broadcast_ones = tf.ones(shape=[batch_size, from_seq_length, 1], dtype=tf.float32)\n\n    # Here we broadcast along two dimensions to create the mask.\n    mask = broadcast_ones * to_mask\n\n    return mask\n\n", "CodeBleu_score": 60.05374192462243}, {"code": "def create_attention_mask_from_input_mask(from_tensor, to_mask)\n\tfrom_shape = get_shape_list(from_tensor, expected_rank=[2, 3])\n    batch_size = from_shape[0]\n    from_seq_length = from_shape[1]\n\n    to_shape = get_shape_list(to_mask, expected_rank=2)\n    to_seq_length = to_shape[1]\n\n    to_mask = tf.cast(tf.reshape(to_mask, [batch_size, 1, to_seq_length]), tf.float32)\n\n    # We don't assume that `from_tensor` is a mask (although it could be). We\n    # don't actually care if we attend *from* padding tokens (only *to* padding)\n    # tokens so we create a tensor of all ones.\n    #\n    # `broadcast_ones` = [batch_size, from_seq_length, 1]\n    broadcast_ones = tf.ones(shape=[batch_size, from_seq_length, 1], dtype=tf.float32)\n\n    # Here we broadcast along two dimensions to create the mask.\n    mask = broadcast_ones * to_mask\n\n    return mask\n\n", "CodeBleu_score": 60.05374192462243}, {"code": "def create_attention_mask_from_input_mask(from_tensor, to_mask)\n\tfrom_shape = get_shape_list(from_tensor, expected_rank=[2, 3])\n    batch_size = from_shape[0]\n    from_seq_length = from_shape[1]\n\n    to_shape = get_shape_list(to_mask, expected_rank=2)\n    to_seq_length = to_shape[1]\n\n    to_mask = tf.cast(\n        tf.reshape(to_mask, (batch_size, 1, to_seq_length)),\n        dtype=from_tensor.dtype)\n\n    # We don't assume that `from_tensor` is a mask (although it could be). We\n    # don't actually care if we attend *from* padding tokens (only *to* padding)\n    # tokens so we create a tensor of all ones.\n    #\n    # `broadcast_ones` = [batch_size, from_seq_length, 1]\n    broadcast_ones = tf.ones(\n        shape=[batch_size, from_seq_length, 1], dtype=from_tensor.dtype)\n\n    # Here we broadcast along two dimensions to create the mask.\n    mask = broadcast_ones * to_mask\n\n    return mask\n\n", "CodeBleu_score": 52.63591857783214}]}
{"instance_id": "d7798e41a723", "generation_result": [{"code": "def transpose_for_scores(input_tensor, batch_size, num_attention_heads,\n                           seq_length, width)\n\tinput_tensor = tf.reshape(\n        input_tensor, [batch_size, seq_length, num_attention_heads, width])\n\n    return tf.transpose(input_tensor, [0, 2, 1, 3])\n\n", "CodeBleu_score": 49.526466148446104}, {"code": "def transpose_for_scores(input_tensor, batch_size, num_attention_heads,\n                           seq_length, width)\n\tinput_tensor = tf.reshape(\n        input_tensor, [batch_size, seq_length, num_attention_heads, width])\n\n    return tf.transpose(input_tensor, [0, 2, 1, 3])\n\n", "CodeBleu_score": 49.526466148446104}, {"code": "def transpose_for_scores(input_tensor, batch_size, num_attention_heads,\n                           seq_length, width)\n\toutput_tensor = tf.reshape(\n        input_tensor, [batch_size, seq_length, num_attention_heads, width])\n\n    output_tensor = tf.transpose(output_tensor, [0, 2, 1, 3])\n    return output_tensor", "CodeBleu_score": 60.207631454348444}]}
{"instance_id": "711d466ace09", "generation_result": [{"code": "def reshape_to_matrix(input_tensor)\n\tif len(input_tensor.shape) < 2:\n        raise ValueError(\"The input tensor must have a rank of 2 or higher.\")\n    if len(input_tensor.shape) == 2:\n        return input_tensor\n    input_width = input_tensor.shape[-1]\n    return tf.reshape(input_tensor, [-1, input_width])", "CodeBleu_score": 41.217490700661884}, {"code": "def reshape_to_matrix(input_tensor)\n\tif len(input_tensor.shape) < 2:\n        raise ValueError('The input tensor must have a rank of 2 or higher.')\n    if len(input_tensor.shape) == 2:\n        return input_tensor\n    input_shape = input_tensor.shape.as_list()\n    input_width = input_shape[-1]\n    output_tensor = tf.reshape(input_tensor, [-1, input_width])\n    return output_tensor", "CodeBleu_score": 53.52528315583316}, {"code": "def reshape_to_matrix(input_tensor)\n\tif len(input_tensor.shape) < 2:\n        raise ValueError(f\"The input tensor must be of rank 2 or higher, but it is of rank {len(input_tensor.shape)}.\")\n    if len(input_tensor.shape) == 2:\n        return input_tensor\n    else:\n        return tf.reshape(input_tensor, [-1, input_tensor.shape[-1]])", "CodeBleu_score": 31.43482362297685}]}
{"instance_id": "7adcd08f531e", "generation_result": [{"code": "def get_train_examples(self, data_dir)\n\ttrain_file = os.path.join(data_dir, \"train.tsv\")\n    train_examples = self._read_tsv(train_file)\n\n    return train_examples", "CodeBleu_score": 8.217422361422898}, {"code": "def get_train_examples(self, data_dir)\n\ttrain_data = []\n    with open(os.path.join(data_dir, \"train.tsv\"), \"r\", encoding=\"utf-8\") as f:\n        reader = csv.reader(f, delimiter=\"\\t\", quotechar=None)\n        header = next(reader)\n        for line in reader:\n            text_a = tokenization.convert_to_unicode(line[3])\n            text_b = tokenization.convert_to_unicode(line[4])\n            label = tokenization.convert_to_unicode(line[0])\n            if label == \"contradictory\":\n                label = \"contradiction\"\n            train_data.append(\n                InputExample(guid=None, text_a=text_a, text_b=text_b, label=label))\n    return train_data", "CodeBleu_score": 40.606964986839245}, {"code": "def get_train_examples(self, data_dir)\n\ttrain_file = os.path.join(data_dir, \"train.tsv\")\n    train_examples = self._read_tsv(train_file)\n    return train_examples\n\n", "CodeBleu_score": 8.217422361422898}]}
{"instance_id": "26a0bb96dab9", "generation_result": [{"code": "def metric_fn(per_example_loss, label_ids, logits, is_real_example)\n\tpredictions = tf.argmax(logits, axis=-1, output_type=tf.int32)\n    accuracy = tf.metrics.accuracy(\n        labels=label_ids, predictions=predictions, weights=is_real_example)\n    loss = tf.metrics.mean(values=per_example_loss, weights=is_real_example)\n    return {\n        'eval_accuracy': accuracy,\n        'eval_loss': loss,\n    }", "CodeBleu_score": 60.06019775499314}, {"code": "def metric_fn(per_example_loss, label_ids, logits, is_real_example)\n\tpredictions = tf.argmax(input=logits, axis=-1, output_type=tf.int32)\n    accuracy = tf.metrics.accuracy(\n        labels=label_ids, predictions=predictions, weights=is_real_example)\n    loss = tf.metrics.mean(values=per_example_loss, weights=is_real_example)\n    return {\n        \"eval_accuracy\": accuracy,\n        \"eval_loss\": loss,\n    }", "CodeBleu_score": 62.35452659374221}, {"code": "def metric_fn(per_example_loss, label_ids, logits, is_real_example)\n\tpredictions = tf.argmax(input=logits, axis=-1, output_type=tf.int32)\n    accuracy = tf.metrics.accuracy(\n        labels=label_ids, predictions=predictions, weights=is_real_example)\n    loss = tf.metrics.mean(values=per_example_loss, weights=is_real_example)\n    return {\n        \"eval_accuracy\": accuracy,\n        \"eval_loss\": loss,\n    }", "CodeBleu_score": 62.35452659374221}]}
{"instance_id": "6154e4d28509", "generation_result": [{"code": "def convert_examples_to_features(examples, label_list, max_seq_length,\n                                 tokenizer)\n\tfeatures = []\n    for (ex_index, example) in enumerate(examples):\n        if ex_index % 10000 == 0:\n            tf.logging.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n        feature = convert_single_example(ex_index, example, label_list,\n                                         max_seq_length, tokenizer)\n        features.append(feature)\n    return features\n\n", "CodeBleu_score": 82.90526147266776}, {"code": "def convert_examples_to_features(examples, label_list, max_seq_length,\n                                 tokenizer)\n\tfeatures = []\n  for (ex_index, example) in enumerate(examples):\n    if ex_index % 10000 == 0:\n      tf.logging.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n\n    feature = convert_single_example(ex_index, example, label_list,\n                                     max_seq_length, tokenizer)\n\n    features.append(feature)\n  return features\n\n", "CodeBleu_score": 84.04025358705603}, {"code": "def convert_examples_to_features(examples, label_list, max_seq_length,\n                                 tokenizer)\n\tfeatures = []\n    for (ex_index, example) in enumerate(examples):\n        if ex_index % 10000 == 0:\n            tf.logging.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n        feature = convert_single_example(ex_index, example, label_list,\n                                         max_seq_length, tokenizer)\n        features.append(feature)\n    return features\n\n", "CodeBleu_score": 82.90526147266776}]}
{"instance_id": "e09c0db71ebb", "generation_result": [{"code": "def get_next_sentence_output(bert_config, input_tensor, labels)\n\twith tf.variable_scope(\"cls/seq_relationship\"):\n        output_weights = tf.get_variable(\n            \"output_weights\",\n            shape=[2, bert_config.hidden_size],\n            initializer=modeling.create_initializer(bert_config.initializer_range))\n        output_bias = tf.get_variable(\n            \"output_bias\", shape=[2], initializer=tf.zeros_initializer())\n\n        logits = tf.matmul(input_tensor, output_weights, transpose_b=True)\n        logits = tf.nn.bias_add(logits, output_bias)\n        log_probs = tf.nn.log_softmax(logits, axis=-1)\n        labels = tf.reshape(labels, [-1])\n        one_hot_labels = tf.one_hot(labels, depth=2, dtype=tf.float32)\n        per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n        loss = tf.reduce_mean(per_example_loss)\n        return (loss, per_example_loss, log_probs)", "CodeBleu_score": 86.98012808751268}, {"code": "def get_next_sentence_output(bert_config, input_tensor, labels)\n\twith tf.variable_scope(\"cls/seq_relationship\"):\n        output_weights = tf.get_variable(\n            \"output_weights\",\n            shape=[2, bert_config.hidden_size],\n            initializer=modeling.create_initializer(bert_config.initializer_range))\n        output_bias = tf.get_variable(\n            \"output_bias\", shape=[2], initializer=tf.zeros_initializer())\n\n        logits = tf.matmul(input_tensor, output_weights, transpose_b=True)\n        logits = tf.nn.bias_add(logits, output_bias)\n        log_probs = tf.nn.log_softmax(logits, axis=-1)\n        labels = tf.reshape(labels, [-1])\n        one_hot_labels = tf.one_hot(labels, depth=2, dtype=tf.float32)\n        per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n        loss = tf.reduce_mean(per_example_loss)\n        return (loss, per_example_loss, log_probs)", "CodeBleu_score": 86.98012808751268}, {"code": "def get_next_sentence_output(bert_config, input_tensor, labels)\n\t# Input tensor is of size [batch_size, hidden_size]\n    with tf.variable_scope(\"cls/seq_relationship\"):\n        output_weights = tf.get_variable(\n            \"output_weights\",\n            shape=[2, bert_config.hidden_size],\n            initializer=modeling.create_initializer(bert_config.initializer_range))\n        output_bias = tf.get_variable(\n            \"output_bias\", shape=[2], initializer=tf.zeros_initializer())\n\n        logits = tf.matmul(input_tensor, output_weights, transpose_b=True)\n        logits = tf.nn.bias_add(logits, output_bias)\n        log_probs = tf.nn.log_softmax(logits, axis=-1)\n\n        labels = tf.reshape(labels, [-1])\n        one_hot_labels = tf.one_hot(labels, depth=2, dtype=tf.float32)\n\n        per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n        loss = tf.reduce_mean(per_example_loss)\n        return (loss, per_example_loss, log_probs)", "CodeBleu_score": 86.94900271179796}]}
{"instance_id": "1f66b020f485", "generation_result": [{"code": "def gather_indexes(sequence_tensor, positions)\n\tsequence_shape = get_shape_list(sequence_tensor, expected_rank=3)\n    batch_size = sequence_shape[0]\n    seq_length = sequence_shape[1]\n    width = sequence_shape[2]\n\n    flat_offsets = tf.reshape(\n        tf.range(0, batch_size, dtype=tf.int32) * seq_length, [-1, 1])\n    flat_positions = tf.reshape(positions + flat_offsets, [-1])\n    flat_sequence_tensor = tf.reshape(sequence_tensor,\n                                      [batch_size * seq_length, width])\n    output_tensor = tf.gather(flat_sequence_tensor, flat_positions)\n    return output_tensor\n\n", "CodeBleu_score": 56.45491338727393}, {"code": "def gather_indexes(sequence_tensor, positions)\n\tsequence_shape = get_shape_list(sequence_tensor, expected_rank=3)\n    batch_size = sequence_shape[0]\n    seq_length = sequence_shape[1]\n    width = sequence_shape[2]\n\n    flat_offsets = tf.reshape(\n        tf.range(0, batch_size, dtype=tf.int32) * seq_length, [-1, 1])\n    flat_positions = tf.reshape(positions + flat_offsets, [-1])\n    flat_sequence_tensor = tf.reshape(sequence_tensor,\n                                      [batch_size * seq_length, width])\n    output_tensor = tf.gather(flat_sequence_tensor, flat_positions)\n    return output_tensor\n\n", "CodeBleu_score": 56.45491338727393}, {"code": "def gather_indexes(sequence_tensor, positions)\n\tsequence_shape = get_shape_list(sequence_tensor, expected_rank=3)\n    batch_size = sequence_shape[0]\n    seq_length = sequence_shape[1]\n    width = sequence_shape[2]\n\n    flat_offsets = tf.reshape(\n        tf.range(0, batch_size, dtype=tf.int32) * seq_length, [-1, 1])\n    flat_positions = tf.reshape(positions + flat_offsets, [-1])\n    flat_sequence_tensor = tf.reshape(sequence_tensor,\n                                      [batch_size * seq_length, width])\n    output_tensor = tf.gather(flat_sequence_tensor, flat_positions)\n    return output_tensor\n\n", "CodeBleu_score": 56.45491338727393}]}
{"instance_id": "956bffee9a33", "generation_result": [{"code": "def _decode_record(record, name_to_features)\n\texample = tf.parse_single_example(record, name_to_features)\n\n    # tf.Example only supports tf.int64, but the TPU only supports tf.int32.\n    # So cast all int64 to int32.\n    for name in list(example.keys()):\n        t = example[name]\n        if t.dtype == tf.int64:\n            t = tf.to_int32(t)\n        example[name] = t\n\n    return example", "CodeBleu_score": 72.6131236812332}, {"code": "def _decode_record(record, name_to_features)\n\texample = tf.parse_single_example(record, name_to_features)\n    for name in list(example.keys()):\n        t = example[name]\n        if t.dtype == tf.int64:\n            t = tf.to_int32(t)\n        example[name] = t\n    return example", "CodeBleu_score": 72.6131236812332}, {"code": "def _decode_record(record, name_to_features)\n\texample = tf.parse_single_example(record, name_to_features)\n\n    # tf.Example only supports tf.int64, but the TPU only supports tf.int32.\n    # So cast all int64 to int32.\n    for name in list(example.keys()):\n        t = example[name]\n        if t.dtype == tf.int64:\n            t = tf.to_int32(t)\n        example[name] = t\n\n    return example", "CodeBleu_score": 72.6131236812332}]}
{"instance_id": "6b97f354e641", "generation_result": [{"code": "def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer,\n                         orig_answer_text)\n\ttok_answer_text = \" \".join(tokenizer.tokenize(orig_answer_text))\n\n    for new_start in range(input_start, input_end + 1):\n        for new_end in range(input_end, new_start - 1, -1):\n            text_span = \" \".join(doc_tokens[new_start:(new_end + 1)])\n            if text_span == tok_answer_text:\n                return new_start, new_end\n\n    return input_start, input_end", "CodeBleu_score": 69.18002743421648}, {"code": "def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer,\n                         orig_answer_text)\n\ttok_answer_text = \" \".join(tokenizer.tokenize(orig_answer_text))\n\n    for new_start in range(input_start, input_end + 1):\n        for new_end in range(input_end, new_start - 1, -1):\n            text_span = \" \".join(doc_tokens[new_start:(new_end + 1)])\n            if text_span == tok_answer_text:\n                return (new_start, new_end)\n\n    return (input_start, input_end)\n\n", "CodeBleu_score": 79.18073177732636}, {"code": "def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer,\n                         orig_answer_text)\n\ttok_answer_text = \" \".join(tokenizer.tokenize(orig_answer_text))\n\n    for new_start in range(input_start, input_end + 1):\n        for new_end in range(input_end, new_start - 1, -1):\n            text_span = \" \".join(doc_tokens[new_start:(new_end + 1)])\n            if text_span == tok_answer_text:\n                return (new_start, new_end)\n\n    return (input_start, input_end)", "CodeBleu_score": 79.18073177732636}]}
{"instance_id": "aba334b92b34", "generation_result": [{"code": "def _check_is_max_context(doc_spans, cur_span_index, position)\n\tbest_score = None\n    best_span_index = None\n    for (span_index, doc_span) in enumerate(doc_spans):\n        end = doc_span.start + doc_span.length - 1\n        if position < doc_span.start:\n            continue\n        if position > end:\n            continue\n        num_left_context = position - doc_span.start\n        num_right_context = end - position\n        score = min(num_left_context, num_right_context) + 0.01 * doc_span.length\n        if best_score is None or score > best_score:\n            best_score = score\n            best_span_index = span_index\n\n    return cur_span_index == best_span_index\n\n", "CodeBleu_score": 79.59396180782194}, {"code": "def _check_is_max_context(doc_spans, cur_span_index, position)\n\tbest_score = None\n    best_span_index = None\n    for (span_index, doc_span) in enumerate(doc_spans):\n        end = doc_span.start + doc_span.length - 1\n        if position < doc_span.start:\n            continue\n        if position > end:\n            continue\n        num_left_context = position - doc_span.start\n        num_right_context = end - position\n        score = min(num_left_context, num_right_context) + 0.01 * doc_span.length\n        if best_score is None or score > best_score:\n            best_score = score\n            best_span_index = span_index\n\n    return cur_span_index == best_span_index\n\n", "CodeBleu_score": 79.59396180782194}, {"code": "def _check_is_max_context(doc_spans, cur_span_index, position)\n\tbest_score = None\n    best_span_index = None\n    for (span_index, doc_span) in enumerate(doc_spans):\n        end = doc_span.start + doc_span.length - 1\n        if position < doc_span.start:\n            continue\n        if position > end:\n            continue\n        num_left_context = position - doc_span.start\n        num_right_context = end - position\n        score = min(num_left_context, num_right_context) + 0.01 * doc_span.length\n        if best_score is None or score > best_score:\n            best_score = score\n            best_span_index = span_index\n\n    return cur_span_index == best_span_index", "CodeBleu_score": 79.59396180782194}]}
{"instance_id": "cf92ad509f6a", "generation_result": [{"code": "def load_vocab(vocab_file)\n\tvocab = collections.OrderedDict()\n    index = 0\n    with open(vocab_file, \"r\", encoding=\"utf-8\") as reader:\n        while True:\n            token = convert_to_unicode(reader.readline())\n            if not token:\n                break\n            token = token.strip()\n            vocab[token] = index\n            index += 1\n    return vocab\n\n", "CodeBleu_score": 67.52201655712655}, {"code": "def load_vocab(vocab_file)\n\tvocab = collections.OrderedDict()\n    index = 0\n    with open(vocab_file, \"r\", encoding=\"utf-8\") as reader:\n        while True:\n            token = convert_to_unicode(reader.readline())\n            if not token:\n                break\n            token = token.strip()\n            vocab[token] = index\n            index += 1\n    return vocab\n\n", "CodeBleu_score": 67.52201655712655}, {"code": "def load_vocab(vocab_file)\n\tvocab = collections.OrderedDict()\n    index = 0\n    with tf.gfile.GFile(vocab_file, \"r\") as reader:\n        while True:\n            token = convert_to_unicode(reader.readline())\n            if not token:\n                break\n            token = token.strip()\n            vocab[token] = index\n            index += 1\n    return vocab\n\n", "CodeBleu_score": 75.66753987805544}]}
{"instance_id": "6c865803ac12", "generation_result": [{"code": "def convert_by_vocab(vocab, items)\n\toutput = []\n    for item in items:\n        output.append(vocab[item])\n    return output\n\n", "CodeBleu_score": 79.88651544154007}, {"code": "def convert_by_vocab(vocab, items)\n\toutput = []\n    for item in items:\n        output.append(vocab[item])\n    return output\n\n", "CodeBleu_score": 79.88651544154007}, {"code": "def convert_by_vocab(vocab, items)\n\toutput = []\n    for item in items:\n        output.append(vocab[item])\n    return output\n\n", "CodeBleu_score": 79.88651544154007}]}
{"instance_id": "e93b9200dc4a", "generation_result": [{"code": "def tokenize(self, text)\n\ttokens = self.basic_tokenizer.tokenize(text)\n    split_tokens = []\n    for token in tokens:\n        for sub_token in self.wordpiece_tokenizer.tokenize(token):\n            split_tokens.append(sub_token)\n\n    return split_tokens", "CodeBleu_score": 56.99338599992889}, {"code": "def tokenize(self, text)\n\ttokens = self.basic_tokenizer.tokenize(text)\n    split_tokens = []\n    for token in tokens:\n        for sub_token in self.wordpiece_tokenizer.tokenize(token):\n            split_tokens.append(sub_token)\n\n    return split_tokens", "CodeBleu_score": 56.99338599992889}, {"code": "def tokenize(self, text)\n\ttokens = self.basic_tokenizer.tokenize(text)\n    split_tokens = []\n    for token in tokens:\n        for sub_token in self.wordpiece_tokenizer.tokenize(token):\n            split_tokens.append(sub_token)\n    return split_tokens", "CodeBleu_score": 56.99338599992889}]}
{"instance_id": "2ff6f6db53c9", "generation_result": [{"code": "def safe_load(path, offset, duration, sample_rate, dtype)\n\ttry:\n        logger.info(f\"Loading {path} at offset {offset} with duration {duration} at sample rate {sample_rate} with dtype {dtype}...\")\n        data = self.load(path, offset, duration, sample_rate, dtype)\n        logger.info(f\"Successfully loaded {path} at offset {offset} with duration {duration} at sample rate {sample_rate} with dtype {dtype}.\")\n        return data, False\n    except Exception as e:\n        logger.exception(f\"Failed to load {path} at offset {offset} with duration {duration} at sample rate {sample_rate} with dtype {dtype}.\")\n        return -1.0, True", "CodeBleu_score": 30.6988313313808}, {"code": "def safe_load(path, offset, duration, sample_rate, dtype)\n\ttry:\n        logger.info(f'Loading {path}')\n        data = self.load(path, offset, duration, sample_rate, dtype)\n        return data, False\n    except Exception as e:\n        logger.exception(f'Error loading {path}')\n        return -1.0, True", "CodeBleu_score": 24.74595799298666}, {"code": "def safe_load(path, offset, duration, sample_rate, dtype)\n\ttry:\n        logger.info(f\"Loading {path} with offset {offset}, duration {duration}, sample_rate {sample_rate}, and dtype {dtype}\")\n        data = self.load(path, offset=offset, duration=duration, sr=sample_rate, dtype=dtype)\n        logger.info(f\"Loaded {path} with offset {offset}, duration {duration}, sample_rate {sample_rate}, and dtype {dtype}\")\n        return data, False\n    except Exception as e:\n        logger.exception(f\"Error loading {path} with offset {offset}, duration {duration}, sample_rate {sample_rate}, and dtype {dtype}\")\n        return -1.0, True", "CodeBleu_score": 36.2790039531071}]}
{"instance_id": "12fbe1f03d8f", "generation_result": [{"code": "def to_n_channels(waveform: tf.Tensor, n_channels: int) -> tf.Tensor\n\tn_channels = int(n_channels)\n    if n_channels <= 0:\n        raise ValueError(\"n_channels must be greater than 0.\")\n    if n_channels == 1:\n        return tf.squeeze(waveform, axis=-1)\n    waveform = tf.convert_to_tensor(waveform)\n    waveform_shape = tf.shape(waveform)\n    n_waveform_channels = waveform_shape[-1]\n    if n_waveform_channels == n_channels:\n        return waveform\n    elif n_waveform_channels < n_channels:\n        return tf.tile(waveform, [1] * (len(waveform_shape) - 1) + [n_channels // n_waveform_channels])[:, :n_channels]\n    else:\n        return waveform[:, :n_channels]", "CodeBleu_score": 45.12406572907934}, {"code": "def to_n_channels(waveform: tf.Tensor, n_channels: int) -> tf.Tensor\n\twaveform = tf.convert_to_tensor(waveform)\n    waveform_shape = tf.shape(waveform)\n    waveform_channels = waveform_shape[-1]\n    if waveform_channels == n_channels:\n        return waveform\n    elif waveform_channels > n_channels:\n        return waveform[..., :n_channels]\n    else:\n        return waveform[..., :n_channels]\n\n", "CodeBleu_score": 43.871285036312976}, {"code": "def to_n_channels(waveform: tf.Tensor, n_channels: int) -> tf.Tensor\n\tn_channels = int(n_channels)\n    if n_channels < 1:\n        raise ValueError(\"n_channels must be a positive integer.\")\n    if len(waveform.shape) != 2:\n        raise ValueError(\"waveform must be a 2-dimensional tensor.\")\n    if waveform.shape[1] == n_channels:\n        return waveform\n    elif waveform.shape[1] > n_channels:\n        return waveform[:, :n_channels]\n    else:\n        return waveform[:, :n_channels][:, :n_channels]", "CodeBleu_score": 39.78854937328802}]}
{"instance_id": "5e215f029b2e", "generation_result": [{"code": "def spectrogram_to_db_uint(\n    spectrogram: tf.Tensor, db_range: float = 100.0, **kwargs\n) -> tf.Tensor\n\treturn from_float32_to_uint8(\n        tf.clip_by_value(\n            gain_to_db(spectrogram, **kwargs),\n            -db_range,\n            tf.reduce_max(gain_to_db(spectrogram, **kwargs)),\n        )\n    )\n\n", "CodeBleu_score": 36.29878783331524}, {"code": "def spectrogram_to_db_uint(\n    spectrogram: tf.Tensor, db_range: float = 100.0, **kwargs\n) -> tf.Tensor\n\tspectrogram = gain_to_db(spectrogram, **kwargs)\n    spectrogram = tf.clip_by_value(spectrogram, -db_range, 0.0)\n    spectrogram = from_float32_to_uint8(spectrogram, db_range)\n    return spectrogram\n\n", "CodeBleu_score": 36.63194875504484}, {"code": "def spectrogram_to_db_uint(\n    spectrogram: tf.Tensor, db_range: float = 100.0, **kwargs\n) -> tf.Tensor\n\tdb_spectrogram = gain_to_db(spectrogram, **kwargs)\n    db_spectrogram = tf.maximum(db_spectrogram, -db_range)\n    db_spectrogram = tf.minimum(db_spectrogram, db_range)\n    db_spectrogram = db_spectrogram + db_range\n    db_spectrogram = tf.cast(db_spectrogram, tf.uint8)\n    return db_spectrogram\n\n", "CodeBleu_score": 42.30598518911084}]}
{"instance_id": "b65c0968dae7", "generation_result": [{"code": "def compute_spectrogram_tf(\n    waveform: tf.Tensor,\n    frame_length: int = 2048,\n    frame_step: int = 512,\n    spec_exponent: float = 1.0,\n    window_exponent: float = 1.0,\n) -> tf.Tensor\n\twindow_length = frame_length\n    waveform_length = tf.shape(waveform)[0]\n    window = tf.signal.hann_window(window_length, periodic=True, dtype=waveform.dtype)\n    window = tf.pow(window, window_exponent)\n    stfts = tf.signal.stft(\n        waveform,\n        frame_length=frame_length,\n        frame_step=frame_step,\n        fft_length=frame_length,\n        window_fn=lambda frame_step, frame_length: window,\n        pad_end=False,\n    )\n    spectrogram = tf.abs(stfts)\n    spectrogram = tf.pow(spectrogram, spec_exponent)\n    spectrogram = tf.transpose(spectrogram)\n    return spectrogram\n\n", "CodeBleu_score": 39.806382462182114}, {"code": "def compute_spectrogram_tf(\n    waveform: tf.Tensor,\n    frame_length: int = 2048,\n    frame_step: int = 512,\n    spec_exponent: float = 1.0,\n    window_exponent: float = 1.0,\n) -> tf.Tensor\n\twindow = tf.signal.hann_window(\n        frame_length, periodic=True, dtype=tf.float32\n    ) ** window_exponent\n    stft = tf.signal.stft(\n        waveform,\n        frame_length=frame_length,\n        frame_step=frame_step,\n        fft_length=frame_length,\n        window_fn=lambda frame_step, frame_length: window,\n    )\n    spectrogram = tf.abs(stft) ** spec_exponent\n    return tf.transpose(spectrogram, perm=[1, 0, 2])", "CodeBleu_score": 41.98001997920575}, {"code": "def compute_spectrogram_tf(\n    waveform: tf.Tensor,\n    frame_length: int = 2048,\n    frame_step: int = 512,\n    spec_exponent: float = 1.0,\n    window_exponent: float = 1.0,\n) -> tf.Tensor\n\twaveform = tf.cast(waveform, tf.float32)\n    window_length = frame_length\n    window = tf.signal.hann_window(window_length, periodic=True)\n    window = tf.pow(window, window_exponent)\n    stfts = tf.signal.stft(\n        signals=waveform,\n        frame_length=frame_length,\n        frame_step=frame_step,\n        fft_length=frame_length,\n        window_fn=tf.signal.hann_window,\n        pad_end=True,\n    )\n    spectrograms = tf.abs(stfts)\n    spectrograms = tf.pow(spectrograms, spec_exponent)\n    spectrograms = tf.transpose(spectrograms)\n    return spectrograms\n\n", "CodeBleu_score": 34.824610962012336}]}
{"instance_id": "a4686140a634", "generation_result": [{"code": "def time_stretch(\n    spectrogram: tf.Tensor,\n    factor: float = 1.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t# calculate new time dimension\n    new_time = tf.cast(tf.shape(spectrogram)[1] * factor, tf.int32)\n    # resize spectrogram\n    stretched = tf.image.resize(spectrogram, [tf.shape(spectrogram)[0], new_time], method=method)\n    # adjust to original dimensions\n    return resize_image_with_crop_or_pad(stretched, tf.shape(spectrogram)[0], tf.shape(spectrogram)[1])\n\n", "CodeBleu_score": 37.17874592390144}, {"code": "def time_stretch(\n    spectrogram: tf.Tensor,\n    factor: float = 1.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t# Calculate new time dimension\n    new_time_dim = tf.cast(tf.shape(spectrogram)[1] * factor, tf.int32)\n    # Resize spectrogram\n    spectrogram_resized = tf.image.resize(\n        spectrogram, [tf.shape(spectrogram)[0], new_time_dim], method=method\n    )\n    # Crop or pad the resized spectrogram\n    spectrogram_stretched = resize_image_with_crop_or_pad(\n        spectrogram_resized, tf.shape(spectrogram)[0], tf.shape(spectrogram)[1]\n    )\n    return spectrogram_stretched\n\n", "CodeBleu_score": 37.898460081056164}, {"code": "def time_stretch(\n    spectrogram: tf.Tensor,\n    factor: float = 1.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\tif factor == 1.0:\n        return spectrogram\n    if factor < 1.0:\n        raise ValueError(\"Factor must be greater than 1.\")\n    # Calculate new time dimension.\n    new_time_dim = tf.cast(tf.shape(spectrogram)[1] * factor, tf.int32)\n    # Resize spectrogram.\n    resized_spectrogram = resize_image_with_crop_or_pad(\n        spectrogram, new_time_dim, method=method\n    )\n    # Adjust to original dimensions.\n    if new_time_dim > tf.shape(spectrogram)[1]:\n        # Crop.\n        return resized_spectrogram[:, : tf.shape(spectrogram)[1], :]\n    else:\n        # Pad.\n        return tf.pad(\n            resized_spectrogram,\n            [[0, 0], [0, tf.shape(spectrogram)[1] - new_time_dim], [0, 0]],\n            mode=\"CONSTANT\",\n        )", "CodeBleu_score": 35.89113482912696}]}
{"instance_id": "6dd01101d4ae", "generation_result": [{"code": "def pitch_shift(\n    spectrogram: tf.Tensor,\n    semitone_shift: float = 0.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\tif semitone_shift == 0.0:\n        return spectrogram\n    if method not in (\n        tf.image.ResizeMethod.BILINEAR,\n        tf.image.ResizeMethod.NEAREST_NEIGHBOR,\n    ):\n        raise ValueError(\n            f\"Invalid method: {method}.", "CodeBleu_score": 18.439176523617856}, {"code": "def pitch_shift(\n    spectrogram: tf.Tensor,\n    semitone_shift: float = 0.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\tif semitone_shift == 0.0:\n        return spectrogram\n    n_semitones = tf.cast(semitone_shift, tf.float32)\n    n_pitches = tf.shape(spectrogram)[1]\n    n_pitches_shifted = tf.cast(n_pitches * (2 ** (n_semitones / 12)), tf.int32)\n    resizing_factor = tf.cast(n_pitches_shifted / n_pitches, tf.float32)\n    spectrogram_shifted = tf.image.resize(\n        spectrogram, [n_pitches, n_pitches_shifted], method=method\n    )\n    spectrogram_shifted = tf.pad(\n        spectrogram_shifted,\n        [[0, 0], [0, n_pitches - n_pitches_shifted]],\n        constant_values=0,\n    )\n    return spectrogram_shifted\n\n", "CodeBleu_score": 41.25017895897217}, {"code": "def pitch_shift(\n    spectrogram: tf.Tensor,\n    semitone_shift: float = 0.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\tif semitone_shift == 0.0:\n        return spectrogram\n\n    factor = 2 ** (semitone_shift / 12)\n    spectrogram_shape = tf.shape(spectrogram)\n    new_size = tf.cast(\n        tf.cast(spectrogram_shape[1], tf.float32) * factor, tf.int32\n    )\n    spectrogram = tf.image.resize(\n        spectrogram, [spectrogram_shape[0], new_size], method=method\n    )\n    spectrogram = tf.pad(\n        spectrogram,\n        [\n            [0, 0],\n            [\n                0,\n                tf.maximum(\n                    0,\n                    tf.cast(\n                        spectrogram_shape[1] - tf.cast(new_size, tf.int32),\n                        tf.int32,\n                    ),\n                ),\n            ],\n        ],\n    )\n\n    return spectrogram", "CodeBleu_score": 41.0520911544968}]}
{"instance_id": "612f0826e56a", "generation_result": [{"code": "def _get_conv_activation_layer(params: Dict) -> Any\n\tconv_activation = params.get(\"conv_activation\", \"ReLU\")\n    if conv_activation == \"ReLU\":\n        return ReLU()\n    elif conv_activation == \"ELU\":\n        return ELU()\n    elif conv_activation == \"LeakyReLU\":\n        return LeakyReLU(0.2)\n    else:\n        raise ValueError(f\"Invalid conv_activation parameter: {conv_activation}\")", "CodeBleu_score": 53.89446919225256}, {"code": "def _get_conv_activation_layer(params: Dict) -> Any\n\tif params.get(\"conv_activation\") is not None:\n        if params[\"conv_activation\"] == \"ReLU\":\n            return nn.ReLU()\n        elif params[\"conv_activation\"] == \"ELU\":\n            return nn.ELU()\n        elif params[\"conv_activation\"] == \"LeakyReLU\":\n            return nn.LeakyReLU(negative_slope=0.2)\n        else:\n            raise ValueError(\n                f\"Invalid activation function: {params['conv_activation']}.", "CodeBleu_score": 33.50326525278474}, {"code": "def _get_conv_activation_layer(params: Dict) -> Any\n\tif params.get(\"conv_activation\", \"relu\") == \"relu\":\n        return ReLU()\n    elif params.get(\"conv_activation\", \"relu\") == \"elu\":\n        return ELU()\n    elif params.get(\"conv_activation\", \"relu\") == \"leaky_relu\":\n        return LeakyReLU()\n    else:\n        return ReLU()\n\n", "CodeBleu_score": 39.71608036002034}]}
{"instance_id": "2f8b5a2fc0f2", "generation_result": [{"code": "def softmax_unet(\n    input_tensor: tf.Tensor, instruments: Iterable[str], params: Dict = {}\n) -> Dict\n\tlogit_mask_list = []\n    for instrument in instruments:\n        logit_mask_list.append(\n            apply_unet(\n                input_tensor,\n                output_name=f\"{instrument}_mask\",\n                params=params,\n                output_mask_logit=True,\n            )\n        )\n    output_masks = tf.stack(logit_mask_list, axis=-1)\n    softmax_masks = tf.nn.softmax(output_masks, axis=-1)\n    output_dict = {}\n    for i, instrument in enumerate(instruments):\n        output_dict[instrument] = Multiply(\n            name=f\"{instrument}_spectrogram\"\n        )([softmax_masks[..., i], input_tensor])\n    return output_dict", "CodeBleu_score": 49.12752652564589}, {"code": "def softmax_unet(\n    input_tensor: tf.Tensor, instruments: Iterable[str], params: Dict = {}\n) -> Dict\n\tlogit_mask_list = []\n    for instrument in instruments:\n        logit_mask_list.append(\n            apply_unet(\n                input_tensor,\n                f\"{instrument}_logit_mask\",\n                params=params,\n                output_mask_logit=True,\n            )\n        )\n    softmax_mask = tf.stack(logit_mask_list, axis=params.get(\"softmax_axis\", -1))\n    softmax_mask = tf.nn.softmax(softmax_mask)\n    output_dict = {}\n    for instrument, logit_mask in zip(instruments, logit_mask_list):\n        output_dict[instrument] = tf.multiply(\n            input_tensor, tf.expand_dims(softmax_mask[:, :, :, :, :, instrument], axis=-1)\n        )\n    return output_dict\n\n", "CodeBleu_score": 40.03946075170764}, {"code": "def softmax_unet(\n    input_tensor: tf.Tensor, instruments: Iterable[str], params: Dict = {}\n) -> Dict\n\tlogging.info(\"Apply softmax unet\")\n    logit_mask_list = []\n    for instrument in instruments:\n        logit_mask_list.append(\n            apply_unet(\n                input_tensor,\n                output_name=f\"{instrument}_logit_mask\",\n                params=params,\n                output_mask_logit=True,\n            )\n        )\n    softmax_mask = tf.stack(logit_mask_list, axis=1)\n    softmax_mask = tf.math.softmax(softmax_mask, axis=1)\n    output_dict = {}\n    for i, instrument in enumerate(instruments):\n        output_dict[instrument] = Multiply(name=f\"{instrument}_mask\")(\n            [softmax_mask[:, i], input_tensor]\n        )\n    return output_dict\n\n", "CodeBleu_score": 46.912561597734175}]}
{"instance_id": "400b19ce8520", "generation_result": [{"code": "def compute_file_checksum(path)\n\twith open(path, 'rb') as file:\n        sha256 = hashlib.sha256()\n        while True:\n            data = file.read(65536)\n            if not data:\n                break\n            sha256.update(data)\n        return sha256.hexdigest()", "CodeBleu_score": 41.29091605231858}, {"code": "def compute_file_checksum(path)\n\tsha256 = hashlib.sha256()\n    with open(path, 'rb') as f:\n        for chunk in iter(lambda: f.read(4096), b''):\n            sha256.update(chunk)\n    return sha256.hexdigest()", "CodeBleu_score": 56.22658680978611}, {"code": "def compute_file_checksum(path)\n\tsha256 = hashlib.sha256()\n    with open(path, 'rb') as f:\n        for chunk in iter(lambda: f.read(4096), b''):\n            sha256.update(chunk)\n    return sha256.hexdigest()\n\n", "CodeBleu_score": 56.22658680978611}]}
{"instance_id": "5f2e9c6e15f1", "generation_result": [{"code": "def checksum(self, name: str) -> str\n\turl = f\"{self.host}/{self.repository}/{self.release}/{self.checksum_index}\"\n    response = httpx.get(url)\n    response.raise_for_status()\n    checksum_index = response.json()\n    if name not in checksum_index:\n        raise ValueError(f\"Model {name} not found in checksum index.\")\n    return checksum_index[name]", "CodeBleu_score": 34.806523926816865}, {"code": "def checksum(self, name: str) -> str\n\turl = f\"{self.host}/{self.repository}/{self.release}/{self.checksum_index}\"\n    response = httpx.get(url)\n    response.raise_for_status()\n    checksums = response.json()\n    if name in checksums:\n        return checksums[name]\n    else:\n        raise ValueError(f\"Model name {name} not found in checksum index.\")\n\n", "CodeBleu_score": 32.75890812248278}, {"code": "def checksum(self, name: str) -> str\n\turl = self.host + self.repository + self.release_path + self.checksum_index\n    response = httpx.get(url)\n    response.raise_for_status()\n    checksum_index = response.json()\n    if name in checksum_index:\n        return checksum_index[name]\n    else:\n        raise ValueError(f\"Model {name} not found in checksum index.\")\n\n\n\n\n", "CodeBleu_score": 32.50483830255691}]}
{"instance_id": "9312c8b6032e", "generation_result": [{"code": "def download(self, name: str, path: str) -> None:\n\turl: str = \"/\".join(\n        (\n            self._host,\n            self._repository,\n            self.RELEASE_PATH,\n            self._release,\n            self.ARCHIVE_PATH,\n            name,\n        )\n    )\n    logger.info(f\"Downloading model {name} from {url}\")\n    with httpx.stream(method=\"GET\", url=url) as response:\n        response.raise_for_status()\n        with NamedTemporaryFile() as stream:\n            for chunk in response.iter_raw():\n                stream.write(chunk)\n            stream.flush()\n            logger.info(f\"Model {name} downloaded\")\n            self.validate_checksum(name, stream.name)\n            self.extract(name, path)\n            logger.info(f\"Model {name} extracted\")\n\n", "CodeBleu_score": 41.62099778122778}, {"code": "def download(self, name: str, path: str) -> None:\n\turl: str = \"/\".join(\n        (\n            self._host,\n            self._repository,\n            self.RELEASE_PATH,\n            self._release,\n            self.MODEL_PATH,\n            name,\n        )\n    )\n    logger.info(f\"Downloading model {name} from {url}\")\n    response: httpx.Response = httpx.stream(\n        \"GET\", url, follow_redirects=True, timeout=None\n    )\n    response.raise_for_status()\n    with NamedTemporaryFile() as stream:\n        for chunk in response.iter_raw():\n            stream.write(chunk)\n        stream.flush()\n        checksum: str = compute_file_checksum(stream.name)\n        if checksum != self.checksum(name):\n            raise IOError(f\"Checksum mismatch for model {name}\")\n        logger.info(f\"Extracting model {name} to {path}\")\n        with tarfile.open(stream.name) as tar:\n            tar.extractall(path)\n        logger.info(f\"Extraction of model {name} completed\")\n\n", "CodeBleu_score": 50.349883993531655}, {"code": "def download(self, name: str, path: str) -> None:\n\turl: str = \"/\".join((self._host, self._repository, self.RELEASE_PATH, self._release, name))\n    logger.info(f\"Downloading model {name} from {url}\")\n    with NamedTemporaryFile() as stream:\n        response: httpx.Response = self._client.stream(\"GET\", url)\n        response.raise_for_status()\n        for chunk in response.iter_raw():\n            stream.write(chunk)\n        stream.flush()\n        checksum: str = compute_file_checksum(stream.name)\n        if checksum != self.checksum(name):\n            raise IOError(\"Checksum validation failed\")\n        logger.info(f\"Extracting model {name} to {path}\")\n        with tarfile.open(stream.name) as tar:\n            tar.extractall(path)", "CodeBleu_score": 47.133776735835106}]}
{"instance_id": "ee9493f37d77", "generation_result": [{"code": "def load_configuration(descriptor: str) -> Dict\n\tif descriptor.startswith(\"embedded://\"):\n        name = descriptor[len(\"embedded://\"):]\n        if not loader.is_resource(name):\n            raise SpleeterError(f\"Embedded configuration '{name}' not found\")\n        with loader.open_text(name) as f:\n            return json.load(f)\n    else:\n        with open(descriptor) as f:\n            return json.load(f)", "CodeBleu_score": 41.57803767005276}, {"code": "def load_configuration(descriptor: str) -> Dict\n\tif descriptor.startswith(\"embedded://\"):\n        name = descriptor.replace(\"embedded://\", \"\")\n        if not loader.is_resource(name):\n            raise SpleeterError(f\"Embedded configuration '{name}' not found\")\n        with loader.open_text(name) as file:\n            return json.load(file)\n    elif loader.exists(descriptor):\n        with open(descriptor) as file:\n            return json.load(file)\n    else:\n        raise SpleeterError(f\"Configuration file '{descriptor}' not found\")", "CodeBleu_score": 56.16490270925861}, {"code": "def load_configuration(descriptor: str) -> Dict\n\tif descriptor.startswith(\"embedded://\"):\n        name = descriptor[12:]\n        if loader.is_resource(name):\n            with loader.open_text(name) as f:\n                return json.load(f)\n        else:\n            raise SpleeterError(f\"Embedded configuration '{name}' not found\")\n    else:\n        if os.path.isfile(descriptor):\n            with open(descriptor) as f:\n                return json.load(f)\n        else:\n            raise SpleeterError(f\"Configuration file '{descriptor}' not found\")\n\n", "CodeBleu_score": 40.40730528641987}]}
{"instance_id": "546134b67fbc", "generation_result": [{"code": "def sync_apply(\n    tensor_dict: Dict[str, tf.Tensor], func: Callable, concat_axis: int = 1\n) -> Dict[str, tf.Tensor]\n\tif concat_axis not in [0, 1]:\n        raise NotImplementedError(\n            f\"The concatenation axis must be either 0 or 1, got {concat_axis} instead.\"\n        )\n    tensor_dict_values = list(tensor_dict.values())\n    tensor_dict_keys = list(tensor_dict.keys())\n    concatenated_tensor = tf.concat(tensor_dict_values, axis=concat_axis)\n    processed_tensor = func(concatenated_tensor)\n    if concat_axis == 0:\n        split_tensor = tf.split(processed_tensor, len(tensor_dict_values), axis=0)\n    else:\n        split_tensor = tf.split(processed_tensor, len(tensor_dict_values), axis=1)\n    processed_tensor_dict = {\n        key: value for key, value in zip(tensor_dict_keys, split_tensor)\n    }\n    return processed_tensor_dict\n\n", "CodeBleu_score": 46.66375618618758}, {"code": "def sync_apply(\n    tensor_dict: Dict[str, tf.Tensor], func: Callable, concat_axis: int = 1\n) -> Dict[str, tf.Tensor]\n\tif concat_axis not in [0, 1]:\n        raise NotImplementedError(\n            \"Concatenation axis must be either 0 or 1. Received: {}\".format(concat_axis)\n        )\n\n    if concat_axis == 0:\n        tensor_dict_values = tf.stack(list(tensor_dict.values()), axis=0)\n    elif concat_axis == 1:\n        tensor_dict_values = tf.stack(list(tensor_dict.values()), axis=1)\n\n    tensor_dict_values = func(tensor_dict_values)\n\n    if concat_axis == 0:\n        tensor_dict_values = tf.unstack(tensor_dict_values, axis=0)\n    elif concat_axis == 1:\n        tensor_dict_values = tf.unstack(tensor_dict_values, axis=1)\n\n    tensor_dict_keys = list(tensor_dict.keys())\n    tensor_dict = {\n        k: v for k, v in zip(tensor_dict_keys, tensor_dict_values)\n    }\n\n    return tensor_dict\n\n\n\n\n", "CodeBleu_score": 45.350631980325645}, {"code": "def sync_apply(\n    tensor_dict: Dict[str, tf.Tensor], func: Callable, concat_axis: int = 1\n) -> Dict[str, tf.Tensor]\n\tif concat_axis not in [0, 1]:\n        raise NotImplementedError(\n            f\"Concatenation axis {concat_axis} is not implemented. \"\n            f\"Please use 0 or 1.\"\n        )\n    tensor_list = list(tensor_dict.values())\n    tensor_concat = tf.concat(tensor_list, axis=concat_axis)\n    tensor_processed = func(tensor_concat)\n    if concat_axis == 0:\n        tensor_list_processed = tf.split(tensor_processed, len(tensor_list), axis=0)\n    elif concat_axis == 1:\n        tensor_list_processed = tf.split(tensor_processed, len(tensor_list), axis=1)\n    tensor_dict_processed = {\n        key: tensor_list_processed[i] for i, key in enumerate(tensor_dict.keys())\n    }\n    return tensor_dict_processed\n\n", "CodeBleu_score": 48.526030608286455}]}
{"instance_id": "df48e78d46d2", "generation_result": [{"code": "def from_float32_to_uint8(\n    tensor: tf.Tensor,\n    tensor_key: str = \"tensor\",\n    min_key: str = \"min\",\n    max_key: str = \"max\",\n) -> tf.Tensor\n\ttensor_min = tf.reduce_min(tensor)\n    tensor_max = tf.reduce_max(tensor)\n    tensor_scaled = tf.cast(\n        tf.round((tensor - tensor_min) / (tensor_max - tensor_min) * 255),\n        tf.uint8,\n    )\n    return {tensor_key: tensor_scaled, min_key: tensor_min, max_key: tensor_max}\n\n", "CodeBleu_score": 53.2777732360766}, {"code": "def from_float32_to_uint8(\n    tensor: tf.Tensor,\n    tensor_key: str = \"tensor\",\n    min_key: str = \"min\",\n    max_key: str = \"max\",\n) -> tf.Tensor\n\tmin_value = tf.reduce_min(tensor)\n    max_value = tf.reduce_max(tensor)\n    tensor = tf.cast(tensor, dtype=tf.float32)\n    tensor = (tensor - min_value) / (max_value - min_value)\n    tensor = tf.cast(tensor * 255, dtype=tf.uint8)\n    return {tensor_key: tensor, min_key: min_value, max_key: max_value}\n\n", "CodeBleu_score": 50.50569344490696}, {"code": "def from_float32_to_uint8(\n    tensor: tf.Tensor,\n    tensor_key: str = \"tensor\",\n    min_key: str = \"min\",\n    max_key: str = \"max\",\n) -> tf.Tensor\n\ttensor_min = tf.reduce_min(tensor)\n    tensor_max = tf.reduce_max(tensor)\n    tensor_scaled = tf.cast(\n        (tensor - tensor_min) / (tensor_max - tensor_min) * 255, tf.uint8\n    )\n    return {tensor_key: tensor_scaled, min_key: tensor_min, max_key: tensor_max}\n\n", "CodeBleu_score": 54.11955322211966}]}
{"instance_id": "7f479e5de492", "generation_result": [{"code": "def pad_and_partition(tensor: tf.Tensor, segment_len: int) -> tf.Tensor\n\tif segment_len <= 0:\n        raise ValueError(\"segment_len must be greater than zero.\")\n    if tensor.shape[0] % segment_len != 0:\n        pad_len = segment_len - tensor.shape[0] % segment_len\n        tensor = tf.pad(tensor, [[0, pad_len], [0, 0]], constant_values=0)\n    return tf.reshape(tensor, [-1, segment_len] + list(tensor.shape[1:]))\n\n", "CodeBleu_score": 31.734055190210135}, {"code": "def pad_and_partition(tensor: tf.Tensor, segment_len: int) -> tf.Tensor\n\ttensor_len = tf.shape(tensor)[0]\n    pad_len = tf.cast(tf.math.floormod(tensor_len, segment_len), tf.int32)\n    paddings = [[0, pad_len]] + [[0, 0]] * (len(tensor.shape) - 1)\n    padded = tf.pad(tensor, paddings, \"CONSTANT\")\n    partitioned = tf.reshape(padded, [-1, segment_len] + list(tensor.shape[1:]))\n    return partitioned\n\n", "CodeBleu_score": 35.084043955485924}, {"code": "def pad_and_partition(tensor: tf.Tensor, segment_len: int) -> tf.Tensor\n\ttensor_len = tf.shape(tensor)[0]\n    pad_len = tf.math.floormod(segment_len - tf.math.floormod(tensor_len, segment_len), segment_len)\n    paddings = [[0, pad_len], [0, 0]]\n    tensor = tf.pad(tensor, paddings)\n    tensor = tf.reshape(tensor, [tf.math.floordiv(tf.shape(tensor)[0], segment_len), segment_len, tf.shape(tensor)[1]])\n    return tensor\n", "CodeBleu_score": 35.24357601108363}]}
{"instance_id": "1789fae7253f", "generation_result": [{"code": "def pad_and_reshape(instr_spec, frame_length, F) -> Any\n\t# Calculate the number of extra rows needed for padding\n    num_rows = frame_length - tf.shape(instr_spec)[0]\n    # Create an extension of zeros\n    extension = tf.zeros([num_rows, F], dtype=tf.float32)\n    # Concatenate the extension to the original spectrogram along the frequency axis\n    instr_spec = tf.concat([instr_spec, extension], axis=0)\n    # Reshape the extended spectrogram by merging the first two dimensions\n    instr_spec = tf.reshape(instr_spec, [1, frame_length * F])\n    return instr_spec\n\n\n\n\n\n", "CodeBleu_score": 16.719745781859245}, {"code": "def pad_and_reshape(instr_spec, frame_length, F) -> Any\n\t# Calculate the number of extra rows needed for padding\n    extra_rows = frame_length - tf.shape(instr_spec)[0] % frame_length\n    # Create an extension of zeros\n    extension = tf.zeros([extra_rows, F], dtype=tf.float32)\n    # Concatenate the extension to the original spectrogram along the frequency axis\n    instr_spec = tf.concat([instr_spec, extension], 0)\n    # Reshape the extended spectrogram by merging the first two dimensions\n    instr_spec = tf.reshape(instr_spec, [-1, F])\n    return instr_spec\n\n", "CodeBleu_score": 15.968830938007109}, {"code": "def pad_and_reshape(instr_spec, frame_length, F) -> Any\n\tn_rows = tf.shape(instr_spec)[0]\n    n_cols = tf.shape(instr_spec)[1]\n    n_frames = tf.shape(instr_spec)[2]\n    n_rows_needed = tf.cast(tf.math.ceil(n_rows / F), tf.int32) * F\n    n_rows_needed = tf.cast(n_rows_needed, tf.int32)\n    n_rows_needed = tf.cast(n_rows_needed, tf.int32)\n    n_rows_needed = tf.cast(n_rows_needed, tf.int32)\n    n_rows_needed = tf.cast(n_rows_needed, tf.int32)\n    n_rows_needed = tf.cast(n_rows_needed, tf.int32)\n    n_rows_needed = tf.cast(n_rows_needed, tf.int32)\n    n_rows_needed = tf.cast(n_rows_needed, tf.int32)\n    n_rows_needed = tf.cast(n_rows_needed, tf.int32)\n    n_rows_needed = tf.cast(n_rows_needed, tf.int32)\n    n_rows_needed = tf.cast(n_rows_needed, tf.int32)\n    n_rows_needed = tf.cast(n_rows_needed, tf.int32)\n    n_rows_needed = tf.cast(n_rows_needed, tf.int32)\n    n_rows_needed = tf.cast(n_rows_needed, tf.int32)\n    n_rows_needed = tf.cast(n_rows_needed, tf.int32)\n    n_rows_needed = tf.cast(n_rows_needed, tf.int32)\n    n_rows_needed = tf.cast(n_rows_needed, tf.int32)\n    n_rows_needed = tf.cast", "CodeBleu_score": 11.656472292057954}]}
{"instance_id": "8a4f4658d97c", "generation_result": [{"code": "def dataset_from_csv(csv_path: str, **kwargs) -> Any\n\tdf = pd.read_csv(csv_path, **kwargs)\n    return tf.data.Dataset.from_tensor_slices(dict(df))", "CodeBleu_score": 26.170329590788334}, {"code": "def dataset_from_csv(csv_path: str, **kwargs) -> Any\n\tdf = pd.read_csv(csv_path, **kwargs)\n    return tf.data.Dataset.from_tensor_slices(dict(df))\n\n", "CodeBleu_score": 26.170329590788334}, {"code": "def dataset_from_csv(csv_path: str, **kwargs) -> Any\n\tdf = pd.read_csv(csv_path)\n    return tf.data.Dataset.from_tensor_slices(dict(df))\n\n", "CodeBleu_score": 16.93934100836908}]}
{"instance_id": "9a16e81b182f", "generation_result": [{"code": "def check_tensor_shape(tensor_tf: tf.Tensor, target_shape: Any) -> bool\n\tresult = True\n    for i, target_length in enumerate(target_shape):\n        if target_length is not None:\n            result = result and (tf.shape(tensor_tf)[i] == target_length)\n    return result", "CodeBleu_score": 44.35472288457063}, {"code": "def check_tensor_shape(tensor_tf: tf.Tensor, target_shape: Any) -> bool\n\tresult = True\n    for i, target_dim in enumerate(target_shape):\n        result = result and (tf.shape(tensor_tf)[i] == target_dim)\n    return result", "CodeBleu_score": 31.520235888313657}, {"code": "def check_tensor_shape(tensor_tf: tf.Tensor, target_shape: Any) -> bool\n\tresult = True\n    for i, dim_len in enumerate(target_shape):\n        result = result and (tf.shape(tensor_tf)[i] == dim_len)\n    return result\n\n", "CodeBleu_score": 30.548767596657424}]}
{"instance_id": "49d5d3084b3a", "generation_result": [{"code": "def get_training_dataset(\n    audio_params: Dict, audio_adapter: AudioAdapter, audio_path: str\n) -> Any\n\tdataset_builder = DatasetBuilder(\n        audio_params, audio_adapter, random_seed=1234\n    )\n    dataset = dataset_builder.build(\n        train_csv,\n        batch_size=batch_size,\n        shuffle=True,\n        convert_to_uint=True,\n        random_data_augmentation=True,\n        random_time_crop=True,\n        infinite_generator=True,\n        cache_directory=cache_directory,\n        wait_for_cache=True,\n        num_parallel_calls=4,\n        n_chunks_per_song=n_chunks_per_song,\n    )\n    return dataset\n\n", "CodeBleu_score": 34.465937149951536}, {"code": "def get_training_dataset(\n    audio_params: Dict, audio_adapter: AudioAdapter, audio_path: str\n) -> Any\n\t# Initialize dataset builder.\n    builder = DatasetBuilder(\n        audio_params,\n        audio_adapter,\n        chunk_duration=11.88,\n        random_seed=42,\n    )\n    # Build dataset.\n    dataset = builder.build(\n        os.path.join(audio_path, \"train.csv\"),\n        batch_size=8,\n        cache_directory=\"cache/train\",\n        wait_for_cache=False,\n        n_chunks_per_song=2,\n        random_data_augmentation=True,\n        random_time_crop=True,\n        num_parallel_calls=4,\n    )\n    return dataset\n\n", "CodeBleu_score": 41.38492074694348}, {"code": "def get_training_dataset(\n    audio_params: Dict, audio_adapter: AudioAdapter, audio_path: str\n) -> Any\n\tbuilder = DatasetBuilder(\n        audio_params,\n        audio_adapter,\n        chunk_duration=11.88,\n        random_seed=42,\n    )\n    return builder.build(\n        csv_path=os.path.join(audio_path, \"train.csv\"),\n        batch_size=8,\n        convert_to_uint=True,\n        random_data_augmentation=True,\n        random_time_crop=True,\n        infinite_generator=True,\n        cache_directory=os.path.join(audio_path, \"cache\"),\n        wait_for_cache=True,\n        num_parallel_calls=4,\n        n_chunks_per_song=2,\n    )\n\n", "CodeBleu_score": 44.44458611916849}]}
{"instance_id": "fe8bbbba06e3", "generation_result": [{"code": "def instruments(self) -> Any\n\tif not hasattr(self, '_instrument_builders'):\n        self._instrument_builders = []\n        for instrument in self._instruments:\n            self._instrument_builders.append(InstrumentDatasetBuilder(instrument, self._dataset_name, self._data_home))\n\n    for instrument_builder in self._instrument_builders:\n        yield instrument_builder", "CodeBleu_score": 52.02014236825413}, {"code": "def instruments(self) -> Any\n\tif not self._instrument_builders:\n        for instrument in self.instruments:\n            self._instrument_builders.append(InstrumentDatasetBuilder(instrument, self.dataset_name))\n    for builder in self._instrument_builders:\n        yield builder", "CodeBleu_score": 44.01254467243408}, {"code": "def instruments(self) -> Any\n\tif not self._instrument_builders:\n        self._instrument_builders = []\n        for instrument in self.instruments:\n            self._instrument_builders.append(InstrumentDatasetBuilder(instrument, self._dataset))\n    for instrument_builder in self._instrument_builders:\n        yield instrument_builder", "CodeBleu_score": 62.47536175892287}]}
{"instance_id": "438b57fd3413", "generation_result": [{"code": "def cache(self, dataset: Any, cache: Optional[str], wait: bool) -> Any\n\tif cache is not None:\n        if wait:\n            while not exists(f\"{cache}.index\"):\n                logger.info(f\"Cache not available, wait {self.WAIT_PERIOD}\")\n                time.sleep(self.WAIT_PERIOD)\n        cache_path = os.path.split(cache)[0]\n        os.makedirs(cache_path, exist_ok=True)\n        return dataset.cache(cache)\n    return dataset\n\n", "CodeBleu_score": 93.3254942392973}, {"code": "def cache(self, dataset: Any, cache: Optional[str], wait: bool) -> Any\n\tif cache is not None:\n        if wait:\n            while not exists(f\"{cache}.index\"):\n                logger.info(f\"Cache not available, wait {self.WAIT_PERIOD}\")\n                time.sleep(self.WAIT_PERIOD)\n        cache_path = os.path.split(cache)[0]\n        os.makedirs(cache_path, exist_ok=True)\n        return dataset.cache(cache)\n    return dataset\n\n", "CodeBleu_score": 93.3254942392973}, {"code": "def cache(self, dataset: Any, cache: Optional[str], wait: bool) -> Any\n\tif cache is not None:\n        if wait:\n            while not exists(f\"{cache}.index\"):\n                logger.info(f\"Cache not available, wait {self.WAIT_PERIOD}\")\n                time.sleep(self.WAIT_PERIOD)\n        cache_path = os.path.split(cache)[0]\n        os.makedirs(cache_path, exist_ok=True)\n        return dataset.cache(cache)\n    return dataset\n\n", "CodeBleu_score": 93.3254942392973}]}
{"instance_id": "8838a3a09a66", "generation_result": [{"code": "def create_estimator(params: Dict, MWF: bool) -> tf.Tensor\n\tmodel_provider = ModelProvider.default(params[\"model_dir\"], MWF)\n    model_fn = model_provider.get_model_fn(params)\n    params.update(model_provider.get_params())\n    session_config = tf.ConfigProto()\n    session_config.gpu_options.allow_growth = True\n    run_config = RunConfig(session_config=session_config)\n    return tf.estimator.Estimator(model_fn=model_fn, params=params, config=run_config)", "CodeBleu_score": 33.92037121899206}, {"code": "def create_estimator(params: Dict, MWF: bool) -> tf.Tensor\n\tmodel_dir = ModelProvider.default(params[\"model_name\"], params[\"model_version\"])\n    if MWF:\n        model_dir = ModelProvider.default(params[\"model_name\"], params[\"model_version\"], params[\"model_weight_file\"])\n    params.update(model_dir)\n    config = ConfigProto()\n    config.gpu_options.allow_growth = True\n    config.gpu_options.per_process_gpu_memory_fraction = 0.7\n    run_config = RunConfig(session_config=config)\n    estimator = tf.estimator.Estimator(model_fn=model_fn, model_dir=model_dir, params=params, config=run_config)\n    return estimator\n\n", "CodeBleu_score": 46.47393427890331}, {"code": "def create_estimator(params: Dict, MWF: bool) -> tf.Tensor\n\tmodel_dir = ModelProvider.default(params)\n    params.update(model_dir)\n    session_config = tf.ConfigProto()\n    session_config.gpu_options.allow_growth = True\n    run_config = RunConfig(session_config=session_config)\n    return tf.estimator.Estimator(\n        model_fn=ModelProvider.default(params, MWF),\n        model_dir=model_dir,\n        params=params,\n        config=run_config,\n    )", "CodeBleu_score": 29.720119722862403}]}
{"instance_id": "779c812bf2cb", "generation_result": [{"code": "def _separate_tensorflow(\n        self, waveform: np.ndarray, audio_descriptor: AudioDescriptor\n    ) -> Dict\n\tif waveform.shape[1] == 1:\n            waveform = to_stereo(waveform)\n        prediction_generator = self._get_prediction_generator()\n        update_data(self, {\"waveform\": waveform, \"audio_id\": audio_descriptor.id})\n        prediction = next(prediction_generator)\n        prediction.pop(\"audio_id\")\n        return prediction\n\n", "CodeBleu_score": 66.54670282941443}, {"code": "def _separate_tensorflow(\n        self, waveform: np.ndarray, audio_descriptor: AudioDescriptor\n    ) -> Dict\n\twaveform = to_stereo(waveform)\n    self._update_data(waveform, audio_descriptor)\n    prediction = next(self._get_prediction_generator())\n    prediction.pop(\"audio_id\")\n    return prediction", "CodeBleu_score": 33.08431769558485}, {"code": "def _separate_tensorflow(\n        self, waveform: np.ndarray, audio_descriptor: AudioDescriptor\n    ) -> Dict\n\twaveform = to_stereo(waveform)\n        prediction_generator = self._get_prediction_generator()\n        self.update_data(data={\"waveform\": waveform, \"audio_id\": audio_descriptor.id})\n        prediction = next(prediction_generator)\n        prediction.pop(\"audio_id\")\n        return prediction\n\n", "CodeBleu_score": 52.02028312736837}]}
{"instance_id": "cf580cd2ed93", "generation_result": [{"code": "def compute_backbone_shapes(config, image_shape)\n\tif callable(config.BACKBONE):\n        return config.COMPUTE_BACKBONE_SHAPE(image_shape)\n    else:\n        assert config.BACKBONE in [\"resnet50\", \"resnet101\"]\n        return np.array(\n            [[int(math.ceil(image_shape[0] / stride)),\n              int(math.ceil(image_shape[1] / stride))]\n             for stride in config.BACKBONE_STRIDES])", "CodeBleu_score": 89.49750468368438}, {"code": "def compute_backbone_shapes(config, image_shape)\n\tif callable(config.BACKBONE):\n        return config.COMPUTE_BACKBONE_SHAPE(image_shape)\n    else:\n        # Currently supports ResNet only.\n        assert config.BACKBONE in [\"resnet50\", \"resnet101\"]\n        return np.array(\n            [[int(math.ceil(image_shape[0] / stride)),\n              int(math.ceil(image_shape[1] / stride))]\n             for stride in config.BACKBONE_STRIDES])", "CodeBleu_score": 89.49750468368438}, {"code": "def compute_backbone_shapes(config, image_shape)\n\tif callable(config.BACKBONE):\n        return config.COMPUTE_BACKBONE_SHAPE(image_shape)\n    else:\n        assert config.BACKBONE in [\"resnet50\", \"resnet101\"]\n        return np.array(\n            [[int(math.ceil(image_shape[0] / stride)),\n              int(math.ceil(image_shape[1] / stride))]\n             for stride in config.BACKBONE_STRIDES])", "CodeBleu_score": 89.49750468368438}]}
{"instance_id": "ebd61a2e59ca", "generation_result": [{"code": "def clip_boxes_graph(boxes, window)\n\t# Split the window and the box coordinates into individual components\n    wy1, wx1, wy2, wx2 = tf.split(window, 4)\n    y1, x1, y2, x2 = tf.split(boxes, 4, axis=1)\n    # Clip the y's and x's\n    y1 = tf.maximum(tf.minimum(y1, wy2), wy1)\n    x1 = tf.maximum(tf.minimum(x1, wx2), wx1)\n    y2 = tf.maximum(tf.minimum(y2, wy2), wy1)\n    x2 = tf.maximum(tf.minimum(x2, wx2), wx1)\n    # Concatenate the clipped coordinates back together\n    clipped = tf.concat([y1, x1, y2, x2], axis=1)\n    # Set the shape of the clipped coordinates\n    clipped.set_shape(boxes.get_shape())\n    return clipped\n\n", "CodeBleu_score": 48.9629816485163}, {"code": "def clip_boxes_graph(boxes, window)\n\twy1, wy2 = window[0:2]\n    wx1, wx2 = window[2:4]\n    y1, x1, y2, x2 = tf.split(boxes, 4, axis=1)\n    y1 = tf.maximum(tf.minimum(y1, wy2), wy1)\n    y2 = tf.maximum(tf.minimum(y2, wy2), wy1)\n    x1 = tf.maximum(tf.minimum(x1, wx2), wx1)\n    x2 = tf.maximum(tf.minimum(x2, wx2), wx1)\n    clipped = tf.concat([y1, x1, y2, x2], axis=1)\n    clipped.set_shape(boxes.shape)\n    return clipped", "CodeBleu_score": 42.587964120142274}, {"code": "def clip_boxes_graph(boxes, window)\n\t# Split the window and the boxes coordinates into individual components\n    wy1, wx1, wy2, wx2 = tf.split(window, 4)\n    y1, x1, y2, x2 = tf.split(boxes, 4, axis=1)\n    # Clip the y's\n    y1 = tf.maximum(tf.minimum(y1, wy2), wy1)\n    y2 = tf.maximum(tf.minimum(y2, wy2), wy1)\n    # Clip the x's\n    x1 = tf.maximum(tf.minimum(x1, wx2), wx1)\n    x2 = tf.maximum(tf.minimum(x2, wx2), wx1)\n    # Concatenate the clipped coordinates back into a tensor\n    clipped = tf.concat([y1, x1, y2, x2], axis=1)\n    # Return the clipped coordinates with a fixed shape\n    clipped.set_shape((clipped.shape[0], 4))\n    return clipped", "CodeBleu_score": 54.13252232486975}]}
{"instance_id": "8e0411587a70", "generation_result": [{"code": "def rpn_graph(feature_map, anchors_per_location, anchor_stride)\n\twith tf.variable_scope(\"rpn_conv1\"):\n        x = KL.Conv2D(512, (3, 3), padding=\"same\", activation=\"relu\", kernel_initializer=\"he_normal\", kernel_regularizer=KL.regularizers.l2(1e-4))(feature_map)\n\n    with tf.variable_scope(\"rpn_class_x\"):\n        rpn_class_x = KL.Conv2D(2 * anchors_per_location, (1, 1), padding=\"valid\", activation=\"linear\", kernel_initializer=\"he_normal\", kernel_regularizer=KL.regularizers.l2(1e-4))(x)\n        rpn_class_x = KL.Lambda(lambda t: tf.reshape(t, (-1, 2)))(rpn_class_x)\n        rpn_class_logits = KL.Activation(\"linear\")(rpn_class_x)\n        rpn_probs = KL.Activation(\"softmax\", name=\"rpn_probs\")(rpn_class_logits)\n\n    with tf.variable_scope(\"rpn_bbox_x\"):\n        rpn_bbox_x = KL.Conv2D(4 * anchors_per_location, (1, 1), padding=\"valid\", activation=\"linear\", kernel_initializer=\"he_normal\", kernel_regularizer=KL.regularizers.l2(1e-4))(x)\n        rpn_bbox = KL.Lambda(lambda t: tf.reshape(t, (-1, 4)))(rpn_bbox_x)\n\n    return [rpn_class_logits, rpn_probs, rpn_bbox]", "CodeBleu_score": 51.85510813416111}, {"code": "def rpn_graph(feature_map, anchors_per_location, anchor_stride)\n\t# Shared convolutional layer\n    shared = KL.Conv2D(512, (3, 3), padding='same', activation='relu', kernel_initializer='normal', name='rpn_conv_shared')(feature_map)\n\n    # Class scores\n    x = KL.Conv2D(2 * anchors_per_location, (1, 1), activation='linear', kernel_initializer='zero', name='rpn_out_class')(shared)\n    # Reshape to [batch, anchors, 2]\n    x_class = KL.Lambda(lambda t: tf.reshape(t, [tf.shape(t)[0], -1, 2]))(x)\n    # Softmax on last dimension of BG/FG\n    x_class = KL.Activation(lambda t: tf.nn.softmax(t, axis=-1), name='rpn_out_class_softmax')(x_class)\n\n    # Bounding box refinements\n    x = KL.Conv2D(4 * anchors_per_location, (1, 1), activation='linear', kernel_initializer='zero', name='rpn_out_regress')(shared)\n    # Reshape to [batch, anchors, 4]\n    x_regress = KL.Lambda(lambda t: tf.reshape(t, [tf.shape(t)[0], -1, 4]))(x)\n\n    return [x_class, x_regress, anchors_per_location, anchor_stride]\n\n", "CodeBleu_score": 60.64001240857677}, {"code": "def rpn_graph(feature_map, anchors_per_location, anchor_stride)\n\tshared = KL.Conv2D(512, (3, 3), padding=\"same\", activation=\"relu\", kernel_initializer=\"he_normal\", kernel_regularizer=KR.l2(l2_reg), name=\"rpn_conv_shared\")(feature_map)\n\n    x = KL.Conv2D(2 * anchors_per_location, (1, 1), padding=\"valid\", activation=\"linear\", kernel_initializer=\"he_normal\", kernel_regularizer=KR.l2(l2_reg), name=\"rpn_class_raw\")(shared)\n    x = KL.Lambda(lambda t: tf.reshape(t, (-1, 2)), name=\"rpn_class_logits\")(x)\n    x = KL.Activation(\"softmax\", name=\"rpn_class\")(x)\n\n    x = KL.Conv2D(4 * anchors_per_location, (1, 1), padding=\"valid\", activation=\"linear\", kernel_initializer=\"he_normal\", kernel_regularizer=KR.l2(l2_reg), name=\"rpn_bbox_pred\")(shared)\n    x = KL.Lambda(lambda t: tf.reshape(t, (-1, 4)), name=\"rpn_bbox\")(x)\n\n    return [x[0], x[1], x[2]]", "CodeBleu_score": 48.51220978213537}]}
{"instance_id": "70777c9261d8", "generation_result": [{"code": "def smooth_l1_loss(y_true, y_pred)\n\tabs_diff = K.abs(y_true - y_pred)\n    less_than_one = K.less(abs_diff, 1)\n    squared_diff = 0.5 * K.square(abs_diff)\n    linear_diff = abs_diff - 0.5\n    loss = less_than_one * squared_diff + (1 - less_than_one) * linear_diff\n    return K.mean(loss)", "CodeBleu_score": 36.39661532360794}, {"code": "def smooth_l1_loss(y_true, y_pred)\n\tdiff = K.abs(y_true - y_pred)\n    less_than_one = K.cast(K.less(diff, 1.0), \"float32\")\n    loss = (less_than_one * 0.5 * diff**2) + (1 - less_than_one) * (diff - 0.5)\n    return loss\n\n", "CodeBleu_score": 60.28930735456834}, {"code": "def smooth_l1_loss(y_true, y_pred)\n\tdiff = K.abs(y_true - y_pred)\n    less_than_one = K.cast(K.less(diff, 1.0), \"float32\")\n    loss = (less_than_one * 0.5 * diff**2) + (1 - less_than_one) * (diff - 0.5)\n    return loss\n\n", "CodeBleu_score": 60.28930735456834}]}
{"instance_id": "0e747b604087", "generation_result": [{"code": "def build_model(x_train, num_classes)\n\t# Reset the TensorFlow graph\n    tf.reset_default_graph()\n\n    # Define the input layer\n    input_layer = KL.Input(shape=x_train.shape[1:])\n\n    # Define the first convolutional layer\n    conv1 = KL.Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(input_layer)\n\n    # Define the second convolutional layer\n    conv2 = KL.Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(conv1)\n\n    # Define the max-pooling layer\n    pool = KL.MaxPooling2D(pool_size=(2, 2))(conv2)\n\n    # Flatten the output of the previous layer\n    flat = KL.Flatten()(pool)\n\n    # Define the first dense layer\n    dense1 = KL.Dense(units=128, activation='relu')(flat)\n\n    # Define the output layer\n    output = KL.Dense(units=num_classes, activation='softmax')(dense1)\n\n    # Define the model\n    model = KM.Model(inputs=input_layer, outputs=output)\n\n    # Compile the model\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n    # Return the model\n    return model", "CodeBleu_score": 29.86763503324524}, {"code": "def build_model(x_train, num_classes)\n\t# Reset the TensorFlow graph\n    tf.reset_default_graph()\n\n    # Define the input layer\n    x = KL.Input(shape = x_train.shape[1:])\n\n    # Define the convolutional layers\n    conv1 = KL.Conv2D(32, (3, 3), activation = 'relu', padding = 'same')(x)\n    conv2 = KL.Conv2D(32, (3, 3), activation = 'relu', padding = 'same')(conv1)\n\n    # Define the max-pooling layer\n    pool = KL.MaxPooling2D(pool_size = (2, 2))(conv2)\n\n    # Flatten the output of the convolutional layers\n    flat = KL.Flatten()(pool)\n\n    # Define the dense layers\n    dense1 = KL.Dense(128, activation = 'relu')(flat)\n    dense2 = KL.Dense(num_classes, activation = 'softmax')(dense1)\n\n    # Create the model\n    model = KM.Model(inputs = x, outputs = dense2)\n\n    # Compile the model\n    model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n\n    return model", "CodeBleu_score": 31.71611489208017}, {"code": "def build_model(x_train, num_classes)\n\t# Reset the graph\n    tf.reset_default_graph()\n\n    # Define the input layer\n    X_input = KL.Input(shape=x_train.shape[1:])\n\n    # Define the first convolutional layer\n    X = KL.Conv2D(32, (3, 3), activation='relu')(X_input)\n    # Define the first max-pooling layer\n    X = KL.MaxPooling2D(pool_size=(2, 2))(X)\n\n    # Define the second convolutional layer\n    X = KL.Conv2D(32, (3, 3), activation='relu')(X)\n    # Define the second max-pooling layer\n    X = KL.MaxPooling2D(pool_size=(2, 2))(X)\n\n    # Flatten the output of the second convolutional layer\n    X = KL.Flatten()(X)\n\n    # Define the first dense layer\n    X = KL.Dense(128, activation='relu')(X)\n    # Define the output layer\n    X = KL.Dense(num_classes, activation='softmax')(X)\n\n    # Create the model\n    model = KM.Model(inputs=X_input, outputs=X, name='Dog_Breed_Classifier')\n\n    return model\n\n", "CodeBleu_score": 30.8416252476327}]}
{"instance_id": "813e3e2844f4", "generation_result": [{"code": "def extract_bboxes(mask)\n\tboxes = np.zeros([mask.shape[-1], 4], dtype=np.int32)\n    for i in range(mask.shape[-1]):\n        m = mask[:, :, i]\n        # Bounding box.\n        horizontal_indices = np.where(np.any(m, axis=0))[0]\n        vertical_indices = np.where(np.any(m, axis=1))[0]\n        if horizontal_indices.shape[0]:\n            x1, x2 = horizontal_indices[[0, -1]]\n            y1, y2 = vertical_indices[[0, -1]]\n            # x2 and y2 should not be part of the box. Increment by 1.\n            x2 += 1\n            y2 += 1\n        else:\n            # No mask for this instance. Might happen due to\n            # resizing or cropping. Set bbox to zeros\n            x1, x2, y1, y2 = 0, 0, 0, 0\n        boxes[i] = np.array([y1, x1, y2, x2], dtype=np.int32)\n    return boxes.astype(np.int32)", "CodeBleu_score": 72.22585230250299}, {"code": "def extract_bboxes(mask)\n\t# initialize array to store bboxes\n    bboxes = np.zeros((mask.shape[-1], 4), dtype=np.int32)\n    # loop through mask layers\n    for i in range(mask.shape[-1]):\n        # determine horizontal and vertical indices where mask is present\n        horizontal_indices = np.where(np.any(mask[:, :, i], axis=0))[0]\n        vertical_indices = np.where(np.any(mask[:, :, i], axis=1))[0]\n        # set bounding box or set it to zeros\n        if horizontal_indices.shape[0]:\n            x1, x2 = horizontal_indices[[0, -1]]\n            y1, y2 = vertical_indices[[0, -1]]\n            # x2 and y2 should not be part of the box. Increment by 1.\n            x2 += 1\n            y2 += 1\n            bboxes[i] = x1, y1, x2, y2\n    return bboxes", "CodeBleu_score": 35.99444911494821}, {"code": "def extract_bboxes(mask)\n\t# initialize bbox array\n    bbox_arr = np.zeros((mask.shape[-1], 4), dtype=np.int32)\n    # iterate through all mask layers\n    for i in range(mask.shape[-1]):\n        # calculate indices where mask is present\n        horizontal_indices = np.where(np.any(mask[:, :, i], axis=0))[0]\n        vertical_indices = np.where(np.any(mask[:, :, i], axis=1))[0]\n        # check if mask is present\n        if horizontal_indices.shape[0]:\n            # store bounding box\n            bbox_arr[i, 0] = horizontal_indices[0]\n            bbox_arr[i, 2] = horizontal_indices[-1]\n            bbox_arr[i, 1] = vertical_indices[0]\n            bbox_arr[i, 3] = vertical_indices[-1]\n    return bbox_arr", "CodeBleu_score": 28.340473418615215}]}
{"instance_id": "7ad5d430d6ed", "generation_result": [{"code": "def compute_iou(box, boxes, box_area, boxes_area)\n\t# Calculate intersection areas\n    y1 = np.maximum(box[0], boxes[:, 0])\n    y2 = np.minimum(box[2], boxes[:, 2])\n    x1 = np.maximum(box[1], boxes[:, 1])\n    x2 = np.minimum(box[3], boxes[:, 3])\n    intersection = np.maximum(y2 - y1, 0) * np.maximum(x2 - x1, 0)\n    # Calculate union areas\n    union = box_area + boxes_area[:] - intersection[:]\n    # Compute IoU\n    iou = intersection / union\n    return iou\n\n", "CodeBleu_score": 56.12768883353916}, {"code": "def compute_iou(box, boxes, box_area, boxes_area)\n\t# Calculate intersection areas\n    y1 = np.maximum(box[0], boxes[:, 0])\n    y2 = np.minimum(box[2], boxes[:, 2])\n    x1 = np.maximum(box[1], boxes[:, 1])\n    x2 = np.minimum(box[3], boxes[:, 3])\n    intersection = np.maximum(y2 - y1, 0) * np.maximum(x2 - x1, 0)\n    union = box_area + boxes_area[:] - intersection[:]\n    iou = intersection / union\n    return iou\n\n", "CodeBleu_score": 56.12768883353916}, {"code": "def compute_iou(box, boxes, box_area, boxes_area)\n\t# Calculate the intersection areas\n    # print(boxes.shape)\n    # print(box.shape)\n    # print(boxes_area.shape)\n    # print(box_area.shape)\n    # print(boxes.shape)\n    # print(boxes_area.shape)\n    # print(box.shape)\n    # print(box_area.shape)\n    # print(boxes.shape)\n    # print(boxes_area.shape)\n    # print(box.shape)\n    # print(box_area.shape)\n    # print(boxes.shape)\n    # print(boxes_area.shape)\n    # print(box.shape)\n    # print(box_area.shape)\n    # print(boxes.shape)\n    # print(boxes_area.shape)\n    # print(box.shape)\n    # print(box_area.shape)\n    # print(boxes.shape)\n    # print(boxes_area.shape)\n    # print(box.shape)\n    # print(box_area.shape)\n    # print(boxes.shape)\n    # print(boxes_area.shape)\n    # print(box.shape)\n    # print(box_area.shape)\n    # print(boxes.shape)\n    # print(boxes_area.shape)\n    # print(box.shape)\n    # print(box_area.shape)\n    # print(boxes.shape)\n    # print(boxes_area.shape)\n    # print(box.shape)\n    # print(box_area.shape)\n    # print(boxes.shape)\n    # print(boxes_area.shape)\n    # print(box.shape)\n    # print(box_area.shape)\n    # print(boxes.shape)\n    # print(boxes_area.shape)\n    # print(box.shape)\n    # print(box_area.shape)\n    # print(boxes.shape)\n    # print(boxes_area.shape)\n    # print(box.shape)\n    # print(box_area.shape)", "CodeBleu_score": 1.8523002421307506}]}
{"instance_id": "15941b852859", "generation_result": [{"code": "def compute_overlaps(boxes1, boxes2)\n\t# if there are no boxes, return an empty list\n    if boxes1.shape[0] == 0 or boxes2.shape[0] == 0:\n        return np.zeros((boxes1.shape[0], boxes2.shape[0]))\n    # if the boxes are the same, return 1.0\n    if boxes1 is boxes2:\n        return np.ones((boxes1.shape[0], boxes2.shape[0]))\n    # compute the area of each box in both sets of boxes\n    area1 = (boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1])\n    area2 = (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1])\n    # compute the intersection of each box in the first set with each box in the second set\n    # this gives an intersection matrix of shape [boxes1, boxes2]\n    # where each element is the area of the intersection of the two boxes\n    intersections = np.zeros((boxes1.shape[0], boxes2.shape[0]))\n    for i in range(boxes1.shape[0]):\n        for j in range(boxes2.shape[0]):\n            intersections[i, j] = compute_iou(boxes1[i], boxes2[j], area1[i], area2[j])\n    return intersections\n\n", "CodeBleu_score": 60.33575945186649}, {"code": "def compute_overlaps(boxes1, boxes2)\n\t# if there are no boxes, return an empty array\n    if boxes1.shape[0] == 0 or boxes2.shape[0] == 0:\n        return np.zeros((boxes1.shape[0], boxes2.shape[0]))\n\n    # if the boxes are empty, return an empty array\n    if len(boxes1) == 0 or len(boxes2) == 0:\n        return []\n\n    # compute area of each box in both sets of boxes\n    area1 = (boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1])\n    area2 = (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1])\n\n    # compute overlaps\n    # each box in boxes1 has to be compared with every box in boxes2\n    overlaps = np.zeros((boxes1.shape[0], boxes2.shape[0]))\n    for i in range(boxes1.shape[0]):\n        # get the box in boxes1\n        box1 = boxes1[i]\n\n        # get the area of the box in boxes1\n        area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])\n\n        # iterate through all boxes in boxes2\n        for j in range(boxes2.shape[0]):\n            # get the box in boxes2\n            box2 = boxes2[j]\n\n            # get the area of the box in boxes2\n            area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n\n            # find the intersection of the two boxes\n            x1 = max(box1[0], box2[0])\n            x2 = min(box1[2], box2[2])\n            y1 = max(box1[1], box2[1])\n            y2 = min(box1[3], box2[3])\n            intersection = max(x2 - x1, 0) * max(y2", "CodeBleu_score": 57.24537407460216}, {"code": "def compute_overlaps(boxes1, boxes2)\n\t# if there are no boxes, return an empty array\n    if boxes1.shape[0] == 0 or boxes2.shape[0] == 0:\n        return np.zeros((boxes1.shape[0], boxes2.shape[0]))\n    # if the boxes are the same, return an array of ones\n    if boxes1 is boxes2:\n        return np.ones((boxes1.shape[0], boxes2.shape[0]))\n    # compute the areas of boxes in both sets\n    areas1 = (boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1])\n    areas2 = (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1])\n    # compute the bounding box of the union\n    y1 = np.maximum(boxes1[:, 0], boxes2[:, 0])\n    y2 = np.minimum(boxes1[:, 2], boxes2[:, 2])\n    x1 = np.maximum(boxes1[:, 1], boxes2[:, 1])\n    x2 = np.minimum(boxes1[:, 3], boxes2[:, 3])\n    # compute the area of intersection\n    intersection = np.maximum(y2 - y1, 0) * np.maximum(x2 - x1, 0)\n    # compute the area of union\n    union = areas1 + areas2 - intersection\n    # compute the intersection over union\n    iou = intersection / union\n    # return the intersection over union matrix\n    return iou\n\n", "CodeBleu_score": 51.30686033575953}]}
{"instance_id": "597533722ce3", "generation_result": [{"code": "def load_image(self, image_id):\n\timage = Image.open(self.image_info[image_id][\"path\"])\n    image = image.convert(\"RGB\")\n    image = np.array(image)\n    if image.shape[2] == 4:\n        image = image[:, :, :3]\n    return image", "CodeBleu_score": 52.282787941416146}, {"code": "def load_image(self, image_id):\n\timage_info = self.image_info[image_id]\n    image = skimage.io.imread(image_info['path'])\n    if image.ndim != 3:\n        image = skimage.color.gray2rgb(image)\n    if image.shape[-1] == 4:\n        image = image[..., :3]\n    return image", "CodeBleu_score": 82.86067373690851}, {"code": "def load_image(self, image_id):\n\tinfo = self.image_info[image_id]\n    image = skimage.io.imread(info['path'])\n    image = gray2rgb(image) if len(image.shape) == 2 else image\n    if len(image.shape) == 4:\n        image = image[:,:,:3]\n    return image", "CodeBleu_score": 51.37913608595345}]}
{"instance_id": "9b852997ca05", "generation_result": [{"code": "def display_images(images, titles=None, cols=4, cmap=None, norm=None,\n                   interpolation=None)\n\ttitles = titles if titles is not None else ['Image (%d)' % i for i in range(1, len(images) + 1)]\n    rows = len(images) // cols + 1\n    plt.figure(figsize=(14, 14 * rows // cols))\n    i = 1\n    for image, title in zip(images, titles):\n        plt.subplot(rows, cols, i)\n        plt.title(title, fontsize=9)\n        plt.axis('off')\n        plt.imshow(image.astype(np.uint8), cmap=cmap, norm=norm, interpolation=interpolation)\n        i += 1\n    plt.show()", "CodeBleu_score": 73.70842751045569}, {"code": "def display_images(images, titles=None, cols=4, cmap=None, norm=None,\n                   interpolation=None)\n\ttitles = titles if titles is not None else [\"Image (%d)\" % i for i in range(1, len(images) + 1)]\n    rows = len(images) // cols + 1\n    plt.figure(figsize=(14, 14 * rows // cols))\n    i = 1\n    for image, title in zip(images, titles):\n        plt.subplot(rows, cols, i)\n        plt.title(title, fontsize=9)\n        plt.axis('off')\n        plt.imshow(image.astype(np.uint8), cmap=cmap,\n                   norm=norm, interpolation=interpolation)\n        i += 1\n    plt.show()", "CodeBleu_score": 73.89989212419813}, {"code": "def display_images(images, titles=None, cols=4, cmap=None, norm=None,\n                   interpolation=None)\n\ttitles = titles if titles is not None else ['Image (%d)' % i for i in range(1, len(images) + 1)]\n    rows = len(images) // cols + 1\n    plt.figure(figsize=(14, 14 * rows // cols))\n    i = 1\n    for image, title in zip(images, titles):\n        plt.subplot(rows, cols, i)\n        plt.title(title, fontsize=9)\n        plt.axis('off')\n        plt.imshow(image.astype(np.uint8), cmap=cmap,\n                   norm=norm, interpolation=interpolation)\n        i += 1\n    plt.show()", "CodeBleu_score": 73.83419300849245}]}
