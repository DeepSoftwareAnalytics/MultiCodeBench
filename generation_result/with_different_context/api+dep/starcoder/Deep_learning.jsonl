{"instance_id": "1f9f4d85e825", "generation_result": [{"code": "def load_model_from_config(self, half_attention)\n\t\n    print(self.config.model)\n    if self.config.cache_path is not None:\n        cached_model_path = self.get_cached_model_path()\n        if os.path.isfile(cached_model_path):\n            print(\"Loading cached model from %s\" % cached_model_path)\n            model = torch.load(cached_model_path)\n            if model.config.half_precision_attention == half_attention:\n                return model\n    print(\"Loading model from %s\" % self.config.model)\n    model = instantiate_from_config(self.config.model)\n    if self.config.cache_path is not None:\n        print(\"Saving cached model to %s\" % cached_model_path)\n        torch.save(model, cached_model_path)\n    model.eval()\n    if half_attention:\n        model.half()\n    if self.config.device != \"cpu\":\n        model.to(self.config.device)\n    if self.config.memory_format == \"channels_last\":\n        model = model.to(memory_format=torch.channels_last)\n    return model\n", "CodeBleu_score": 31.2662575296579}, {"code": "def load_model_from_config(self, half_attention)\n\t\n    if self.model_path is None:\n        raise ValueError(\"Model path not specified.\")\n    if not os.path.exists(self.model_path):\n        raise FileNotFoundError(f\"Model path {self.model_path} does not exist.\")\n    if not os.path.isfile(self.model_path):\n        raise ValueError(f\"Model path {self.model_path} is not a file.\")\n    if self.cache_path is not None:\n        if not os.path.exists(self.cache_path):\n            os.makedirs(self.cache_path)\n        if not os.path.isdir(self.cache_path):\n            raise ValueError(f\"Cache path {self.cache_path} is not a directory.\")\n    model_filename = os.path.basename(self.model_path)\n    cache_filename = os.path.join(self.cache_path, model_filename)\n    if self.cache_path is not None and os.path.exists(cache_filename):\n        if not os.path.isfile(cache_filename):\n            raise ValueError(f\"Cache path {self.cache_path} is not a file.\")\n        if os.path.getmtime(cache_filename) < os.path.getmtime(self.model_path):\n            print(\n                f\"Cached model {model_filename} is out of date and will be overwritten.\"\n            )\n        else:\n            print(f\"Loading cached model {model_filename}.\")\n            return torch.load(cache_filename)\n    print(f\"Loading model from {self.model_path}.\")\n    model_config = OmegaConf.load(self.model_path)\n    model = instantiate_from_config(model_config.model)\n    if self.device == \"cpu\":\n        model.cpu()\n    else:\n        model.cuda(self.device)\n    model.eval()\n    if half_attention:\n        model.half()\n    if self.memory_format == \"channels_last\":\n        model.to(memory_format=torch.channels_last)\n    model.load_state_dict(torch.load(self.model_path))\n    if self.cache_path is not None:\n        print(f\"Saving cached model {model_filename}.\")\n        torch.save(model, cache_filename)\n    return model\n\n", "CodeBleu_score": 34.349064726952165}, {"code": "def load_model_from_config(self, half_attention)\n\t\n    cache_path = self.get_cache_path()\n    if cache_path.is_file() and self.config[\"use_cache\"]:\n        print(\"Loading model from cache.\")\n        model = torch.load(cache_path)\n    else:\n        print(\"Loading model from file.\")\n        model = instantiate_from_config(self.config[\"model\"])\n        if self.config[\"fp16\"] and not half_attention:\n            model.half()\n        model.to(self.device)\n        model.eval()\n        if self.config[\"use_cache\"]:\n            print(\"Saving model to cache.\")\n            torch.save(model, cache_path)\n    if self.config[\"memory_format\"] == \"channels_last\":\n        model.to(memory_format=torch.channels_last)\n    return model\n\n", "CodeBleu_score": 23.793821024863497}]}
{"instance_id": "8dc955007aa2", "generation_result": [{"code": "def convsample_ddim(model, cond, steps, shape, eta=1.0, callback=None, normals_sequence=None,\n                    mask=None, x0=None, quantize_x0=False, temperature=1., score_corrector=None,\n                    corrector_kwargs=None, x_t=None\n                    )\n\t\n    batch_size = shape[0]\n    print(\"eta: {}\".format(eta))\n    print(\"steps: {}\".format(steps))\n    ddim = DDIMSampler(model, batch_size)\n    samples = ddim.sample(steps, shape, cond, eta=eta, callback=callback, normals_sequence=normals_sequence,\n                          mask=mask, x0=x0, quantize_x0=quantize_x0, temperature=temperature,\n                          score_corrector=score_corrector, corrector_kwargs=corrector_kwargs, x_t=x_t)\n    return samples, ddim\n\n", "CodeBleu_score": 44.64006072445553}, {"code": "def convsample_ddim(model, cond, steps, shape, eta=1.0, callback=None, normals_sequence=None,\n                    mask=None, x0=None, quantize_x0=False, temperature=1., score_corrector=None,\n                    corrector_kwargs=None, x_t=None\n                    )\n\t\n    batch_size, channels, height, width = shape\n    print(\"batch_size = {}\".format(batch_size))\n    print(\"eta = {}\".format(eta))\n    print(\"steps = {}\".format(steps))\n    ddim = DDIMSampler(model, batch_size, shape, cond, eta=eta, score_corrector=score_corrector,\n                       corrector_kwargs=corrector_kwargs)\n    samples = ddim.sample(steps, callback=callback, normals_sequence=normals_sequence, mask=mask, x0=x0,\n                          quantize_x0=quantize_x0, temperature=temperature, x_t=x_t)\n    return samples, ddim.intermediates\n", "CodeBleu_score": 42.900700580322834}, {"code": "def convsample_ddim(model, cond, steps, shape, eta=1.0, callback=None, normals_sequence=None,\n                    mask=None, x0=None, quantize_x0=False, temperature=1., score_corrector=None,\n                    corrector_kwargs=None, x_t=None\n                    )\n\t\n\n    batch_size, channels, height, width = shape\n    ddim = DDIMSampler(model, cond, eta=eta, score_corrector=score_corrector, corrector_kwargs=corrector_kwargs)\n    print(\"eta: \", eta)\n    print(\"steps: \", steps)\n    samples = ddim.sample(steps, batch_size, shape, cond, mask=mask, x0=x0, quantize_x0=quantize_x0, temperature=temperature, x_t=x_t)\n    return samples, ddim\n", "CodeBleu_score": 21.57027487873645}]}
{"instance_id": "9bdcd9081480", "generation_result": [{"code": "def ema_scope(self, context=None)\n\t\n    if context is None:\n        context = self.context\n    if self.ema_enabled:\n        if context:\n            print(f'EMA enabled. Switching weights to EMA weights.')\n        self.parameters = self.ema_parameters\n        self.ema_parameters = None\n        self.ema_enabled = False\n    else:\n        if context:\n            print(f'EMA disabled. Switching weights to non-EMA weights.')\n        self.ema_parameters = self.parameters\n        self.parameters = None\n        self.ema_enabled = True\n    return self.ema_enabled\n", "CodeBleu_score": 35.33925880716626}, {"code": "def ema_scope(self, context=None)\n\t\n    if self.parameters.ema_enabled:\n        if context is not None:\n            context.print(\"Using EMA weights\")\n        with self.ema_scope():\n            yield\n    else:\n        yield\n", "CodeBleu_score": 21.629845084154514}, {"code": "def ema_scope(self, context=None)\n\t\n    if context is None:\n        context = self.context\n    if self.ema_enabled:\n        if context is not None:\n            context.print(\"Switching to EMA weights.\")\n        self.ema_restore()\n        self.ema_enabled = False\n    else:\n        if context is not None:\n            context.print(\"Restoring original weights.\")\n        self.ema_store()\n        self.ema_enabled = True\n", "CodeBleu_score": 42.562140805237014}]}
{"instance_id": "14ab009e1c5c", "generation_result": [{"code": "def init_from_ckpt(self, path, ignore_keys=None)\n\t\n    if not os.path.isfile(path):\n        raise FileNotFoundError(\"Checkpoint '{}' not found\".format(path))\n    checkpoint = torch.load(path, map_location=\"cpu\")\n    state_dict = checkpoint[\"state_dict\"]\n    if ignore_keys is not None:\n        for k in ignore_keys:\n            del state_dict[k]\n    self.load_state_dict(state_dict, strict=False)\n    missing_keys, unexpected_keys = self.load_state_dict(state_dict, strict=False)\n    if len(missing_keys) > 0:\n        logger.info(\"Weights of {} not initialized from pretrained model: {}\".format(self.__class__.__name__, missing_keys))\n    if len(unexpected_keys) > 0:\n        logger.info(\"Weights from pretrained model not used in {}: {}\".format(self.__class__.__name__, unexpected_keys))\n    logger.info(\"Loaded {} from checkpoint '{}'\".format(self.__class__.__name__, path))\n    return checkpoint\n", "CodeBleu_score": 34.10778048874206}, {"code": "def init_from_ckpt(self, path, ignore_keys=None)\n\t\n    if not os.path.isfile(path):\n        raise ValueError(\"Checkpoint {} not found\".format(path))\n    state_dict = torch.load(path, map_location=lambda storage, loc: storage)\n    if ignore_keys is not None:\n        for k in ignore_keys:\n            del state_dict[k]\n    self.load_state_dict(state_dict, strict=False)\n    missing_keys, unexpected_keys = self.load_state_dict(state_dict, strict=False)\n    if len(missing_keys) > 0:\n        print(\"Missing Keys: {}\".format(missing_keys))\n    if len(unexpected_keys) > 0:\n        print(\"Unexpected Keys: {}\".format(unexpected_keys))\n", "CodeBleu_score": 37.02121264073659}, {"code": "def init_from_ckpt(self, path, ignore_keys=None)\n\t\n    if not os.path.exists(path):\n        raise ValueError(\"Checkpoint '{}' does not exist\".format(path))\n    checkpoint = torch.load(path)\n    print(\"Loading model from checkpoint '{}'\".format(path))\n    # get state_dict from checkpoint\n    state_dict = checkpoint['state_dict']\n    # remove keys from checkpoint that correspond to ignore_keys\n    if ignore_keys is not None:\n        for k in ignore_keys:\n            if k in state_dict.keys():\n                del state_dict[k]\n    # load state_dict\n    self.load_state_dict(state_dict, strict=False)\n    # load optimizer state from checkpoint only when optimizer type is not changed.\n    if checkpoint['config']['optimizer']['type'] != self.config['optimizer']['type']:\n        print('Warning: Optimizer type given in config file is different from that of checkpoint. Optimizer parameters not being resumed.')\n    else:\n        self.optimizer.load_state_dict(checkpoint['optimizer'])\n    # load amp state from checkpoint only when amp_opt_level is not changed.\n    if checkpoint['config']['amp_opt_level'] != self.config['amp_opt_level']:\n        print('Warning: AMP optimization level given in config file is different from that of checkpoint. AMP state not being resumed.')\n    else:\n        amp.load_state_dict(checkpoint['amp'])\n    # return model after loading checkpoint.\n    return self\n", "CodeBleu_score": 25.107776437896124}]}
{"instance_id": "754176e27b81", "generation_result": [{"code": "def forward(self, input, return_pred_indices=False)\n\t\n    quant, diff, info = self.encode(input)\n    dec = self.decode(quant)\n    if return_pred_indices:\n        _, pred_indices = torch.max(dec, dim=1)\n        return dec, diff, pred_indices, info\n    return dec, diff, info\n\n", "CodeBleu_score": 44.057464497013555}, {"code": "def forward(self, input, return_pred_indices=False)\n\t\n    quant, diff, info = self.encode(input)\n    dec = self.decode(quant)\n    if return_pred_indices:\n        return dec, diff, info, self.quantize.get_prediction_indices()\n    else:\n        return dec, diff, info\n\n", "CodeBleu_score": 44.3129047585114}, {"code": "def forward(self, input, return_pred_indices=False)\n\t\n    quant, diff, info = self.encode(input)\n    dec = self.decode(quant)\n    if return_pred_indices:\n        return dec, diff, info, quant\n    else:\n        return dec, diff, info\n\n", "CodeBleu_score": 45.08419697173648}]}
{"instance_id": "443df6fdaf0d", "generation_result": [{"code": "def get_input(self, batch, k)\n\t\n    x = batch[k]\n    if len(x.shape) == 3:\n        x = x.unsqueeze(0)\n    if self.resize_enabled:\n        x = F.interpolate(x, size=self.resize_range, mode='bicubic')\n    x = x.permute(0, 3, 1, 2)\n    x = x.detach()\n    return x\n", "CodeBleu_score": 28.081936654982563}, {"code": "def get_input(self, batch, k)\n\t\n    # Get the input from the batch\n    x = batch[k]\n    # If the input is 2D, add a channel dimension\n    if len(x.shape) == 2:\n        x = x[None, :, :]\n    # If the input is 3D, permute the axes to (B, C, H, W)\n    elif len(x.shape) == 3:\n        x = x.permute(2, 0, 1)\n    # If batch resizing is enabled, resize the input\n    if self.resize_batches:\n        x = F.interpolate(x, size=(self.resize_range, self.resize_range), mode='bicubic')\n    # Convert the input to a contiguous float tensor and detach it from the computation graph\n    x = x.detach().to(torch.float32)\n    return x\n\n", "CodeBleu_score": 31.096729152996605}, {"code": "def get_input(self, batch, k)\n\t\n    x = batch[k]\n    if len(x.shape) == 3:\n        x = x.unsqueeze(0)\n    if self.resize:\n        x = F.interpolate(x, size=self.resize_range[1], mode='bicubic')\n    x = x.permute(0, 3, 1, 2)\n    x = x.detach()\n    return x\n\n\n", "CodeBleu_score": 29.802046557351357}]}
{"instance_id": "953a2632fc15", "generation_result": [{"code": "def training_step(self, batch, batch_idx, optimizer_idx)\n\t\n    # get data\n    x = self.get_input(batch, \"img\")\n    x_rec = self.forward(x)\n    loss, log_dict_input = self.loss(x, x_rec, optimizer_idx, global_step=self.global_step)\n    self.log_dict(log_dict_input, on_step=True, on_epoch=False, prog_bar=False, logger=True)\n    return loss\n", "CodeBleu_score": 18.214610703033763}, {"code": "def training_step(self, batch, batch_idx, optimizer_idx)\n\t\n    x = self.get_input(batch, \"x\")\n    y = self.get_input(batch, \"y\")\n    y_hat = self(x)\n    if optimizer_idx == 0:\n        ae_loss = self.loss(y_hat, y)\n        self.log_dict(\n            {\"train_ae_loss\": ae_loss, \"train_loss\": ae_loss}, on_step=True, on_epoch=True, prog_bar=True\n        )\n        return ae_loss\n    elif optimizer_idx == 1:\n        disc_loss = self.loss(y_hat, y)\n        self.log_dict(\n            {\"train_disc_loss\": disc_loss, \"train_loss\": disc_loss}, on_step=True, on_epoch=True, prog_bar=True\n        )\n        return disc_loss\n\n", "CodeBleu_score": 31.195147361667285}, {"code": "def training_step(self, batch, batch_idx, optimizer_idx)\n\t\n    x = self.get_input(batch, \"img\")\n    if self.global_step <= 4:\n        # do the first few batches with max size to avoid later oom\n        new_resize = self.batch_resize_range[1]\n    else:\n        new_resize = np.random.choice(np.arange(self.batch_resize_range[0], self.batch_resize_range[1]+16, 16))\n    if new_resize != x.shape[2]:\n        x = F.interpolate(x, size=new_resize, mode=\"bicubic\")\n    x = x.detach()\n    y = self.forward(x)\n    y_hat = self.forward(y)\n    y_hat_detach = y_hat.detach()\n    if optimizer_idx == 0:\n        loss = self.loss(y_hat, x)\n        self.log_dict(\n            {\"train_loss\": loss}, on_epoch=True, sync_dist=True, prog_bar=True\n        )\n        return loss\n    elif optimizer_idx == 1:\n        loss = self.loss(y_hat_detach, y)\n        self.log_dict(\n            {\"train_loss\": loss}, on_epoch=True, sync_dist=True, prog_bar=True\n        )\n        return loss\n    else:\n        raise ValueError(f\"Invalid optimizer index: {optimizer_idx}\")\n", "CodeBleu_score": 40.9764203022538}]}
{"instance_id": "b449b5e8b77f", "generation_result": [{"code": "def log_images(self, batch, only_inputs=False, plot_ema=False, **kwargs)\n\t\n    self.ema_scope.to(self.device)\n    if not only_inputs:\n        self.ema_scope.eval()\n    self.ema_scope.requires_grad_(False)\n    with torch.no_grad():\n        # get inputs\n        x = self.get_input(batch, \"image\")\n        x_rec = self.forward(x, **kwargs)\n        if x_rec.shape[1] == 1:\n            x_rec = x_rec.repeat(1, 3, 1, 1)\n        if self.image_key == \"segmentation\":\n            x_rec = self.to_rgb(x_rec)\n            x = self.to_rgb(x)\n        if not only_inputs:\n            x_rec_ema = self.ema_scope.ema(x_rec)\n            if x_rec_ema.shape[1] == 1:\n                x_rec_ema = x_rec_ema.repeat(1, 3, 1, 1)\n            if self.image_key == \"segmentation\":\n                x_rec_ema = self.to_rgb(x_rec_ema)\n        else:\n            x_rec_ema = None\n    self.ema_scope.requires_grad_(True)\n    self.ema_scope.train()\n    # log images\n    if self.global_step == 0:\n        self.logger.experiment.add_images(\"inputs\", x, self.global_step)\n    if not only_inputs:\n        self.logger.experiment.add_images(\"reconstructions\", x_rec, self.global_step)\n        if plot_ema:\n            self.logger.experiment.add_images(\"ema_reconstructions\", x_rec_ema, self.global_step)\n", "CodeBleu_score": 35.39538094018234}, {"code": "def log_images(self, batch, only_inputs=False, plot_ema=False, **kwargs)\n\t\n    if self.global_step % self.log_interval != 0:\n        return\n    if self.global_step == 0:\n        self.logger.info(f\"Logging images to {self.log_dir}\")\n    if self.ema_scope is not None and plot_ema:\n        # use EMA weights for logging\n        with torch.no_grad():\n            ema_model = copy.deepcopy(self.model)\n            ema_model.set_ema_weights(self.ema_scope)\n            ema_model.eval()\n            ema_model = ema_model.to(self.device)\n            self.log_images(batch, ema_model, only_inputs=only_inputs, plot_ema=False, **kwargs)\n        return\n    if only_inputs:\n        self.log_inputs(batch, **kwargs)\n        return\n    # log inputs\n    self.log_inputs(batch, **kwargs)\n    # log reconstructions\n    self.log_reconstructions(batch, **kwargs)\n\n", "CodeBleu_score": 25.946982801741537}, {"code": "def log_images(self, batch, only_inputs=False, plot_ema=False, **kwargs)\n\t\n    if not hasattr(self, \"ema_scope\"):\n        self.ema_scope = None\n    if self.ema_scope is None:\n        self.ema_scope = \"ema\"\n    if not hasattr(self, \"batch_resize_range\"):\n        self.batch_resize_range = None\n    if self.batch_resize_range is None:\n        self.batch_resize_range = [256, 512]\n    if not hasattr(self, \"image_key\"):\n        self.image_key = \"img\"\n    if not hasattr(self, \"input_key\"):\n        self.input_key = \"img\"\n    if not hasattr(self, \"log_keys\"):\n        self.log_keys = [\"img\"]\n    if not hasattr(self, \"log_prefix\"):\n        self.log_prefix = \"\"\n    if not hasattr(self, \"log_max_outputs\"):\n        self.log_max_outputs = 4\n    if not hasattr(self, \"log_every_n_steps\"):\n        self.log_every_n_steps = 100\n    if not hasattr(self, \"log_ema_weights\"):\n        self.log_ema_weights = True\n    if not hasattr(self, \"log_to_tb\"):\n        self.log_to_tb = True\n    if not hasattr(self, \"log_to_wandb\"):\n        self.log_to_wandb = False\n    if not hasattr(self, \"log_to_console\"):\n        self.log_to_console = True\n    if not hasattr(self, \"log_to_file\"):\n        self.log_to_file = True\n    if not hasattr(self, \"log_file_path\"):\n        self.log_file_path = \"logs/log.txt\"\n    if not hasattr(self, \"log_file_mode\"):\n        self.log_file_mode = \"a\"\n    if not hasattr(self, \"log_file_every_n_steps\"):\n        self.log_file_every_n_steps = 100\n    if not hasattr(self, \"log_file_max_bytes\"):\n        self.log_file_max_bytes = 1e8\n    if not hasattr(self, \"log_file_backup_count\"):\n        self.log_file_backup_count = 10\n   ", "CodeBleu_score": 16.426678795428668}]}
{"instance_id": "540d089e3481", "generation_result": [{"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss_type == 'l1':\n        loss = torch.abs(pred - target)\n        if mean:\n            loss = torch.mean(loss)\n    elif self.loss_type == 'l2':\n        loss = torch.pow(pred - target, 2)\n        if mean:\n            loss = torch.mean(loss)\n        else:\n            loss = torch.sum(loss)\n    else:\n        raise NotImplementedError\n    return loss\n", "CodeBleu_score": 48.99610825666263}, {"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss_type == 'l1':\n        loss = torch.abs(pred - target)\n        if mean:\n            loss = loss.mean()\n    elif self.loss_type == 'l2':\n        loss = (pred - target)**2\n        if mean:\n            loss = loss.mean()\n        else:\n            loss = loss.sum()\n    else:\n        raise NotImplementedError\n    return loss\n", "CodeBleu_score": 51.60710457552793}, {"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss_type == \"l1\":\n        if mean:\n            return torch.mean(torch.abs(pred - target))\n        else:\n            return torch.sum(torch.abs(pred - target))\n    elif self.loss_type == \"l2\":\n        if mean:\n            return torch.mean((pred - target) ** 2)\n        else:\n            return torch.sum((pred - target) ** 2)\n    else:\n        raise NotImplementedError(\n            \"The loss type specified is not implemented. Please choose either 'l1' or 'l2'.\"\n        )\n", "CodeBleu_score": 30.737614916408145}]}
{"instance_id": "63a08283666f", "generation_result": [{"code": "def forward(self, x, c, *args, **kwargs)\n\t\n    time_steps = self.get_time_steps(x, c, *args, **kwargs)\n    x = self.get_learned_conditioning(c)\n    x = self.q_sample(x, time_steps)\n    model_out = self.model(x, time_steps, *args, **kwargs)\n    return model_out\n", "CodeBleu_score": 10.661812332414419}, {"code": "def forward(self, x, c, *args, **kwargs)\n\t\n    if self.training and self.training_steps is not None:\n        t = torch.randint(0, self.training_steps, (x.shape[0],), device=x.device)\n    else:\n        t = torch.randint(0, self.num_timesteps, (x.shape[0],), device=x.device)\n\n    if self.learned_conditioning:\n        c = self.get_learned_conditioning(c)\n\n    if self.conditioning_method == 'concat':\n        x = torch.cat([x, c.expand(x.shape[0], -1, -1, -1)], dim=1)\n    elif self.conditioning_method == 'noise':\n        x = x + c.expand(x.shape[0], -1, -1, -1)\n    else:\n        raise NotImplementedError(f'Conditioning method {self.conditioning_method} not yet supported')\n\n    return self.p_losses(x, t)\n", "CodeBleu_score": 45.47402660428689}, {"code": "def forward(self, x, c, *args, **kwargs)\n\t\n    b, *_, device = *x.shape, x.device\n    t = torch.randint(0, self.num_timesteps, (b,), device=device).long()\n    if self.cond_stage_model is not None:\n        c = self.get_learned_conditioning(c)\n    x = self.q_sample(x, t)\n    x = self.model(x, c, t, *args, **kwargs)\n    return x\n", "CodeBleu_score": 40.98204382296662}]}
{"instance_id": "4627e1e8ed62", "generation_result": [{"code": "def get_codebook_entry(self, indices, shape)\n\t\n    assert len(shape) == 2\n    assert len(indices.shape) == 2\n    assert indices.shape[1] == shape[0]\n    z_q = self.embedding(indices.contiguous().view(-1)).view(indices.shape[0], -1, shape[1])\n    if self.map_to_all:\n        z_q = self.unmap_to_all(z_q)\n    return z_q.view(indices.shape[0], -1, shape[1])\n", "CodeBleu_score": 43.70066346695793}, {"code": "def get_codebook_entry(self, indices, shape)\n\t\n    assert indices.shape[0] == shape[0]\n    indices = self.unmap_to_all(indices)\n    z_q = self.embedding(indices.to(torch.long))\n    z_q = z_q.view(shape)\n    z_q = z_q.permute(0, 3, 1, 2)\n    return z_q.contiguous()\n", "CodeBleu_score": 24.968004463688732}, {"code": "def get_codebook_entry(self, indices, shape)\n\t\n    if self.map_to_all:\n        indices = self.map_to_all(indices)\n    z_q = self.embedding(indices)\n    if self.unmap_to_all:\n        z_q = self.unmap_to_all(z_q)\n    z_q = z_q.view(shape)\n    return z_q\n", "CodeBleu_score": 33.16182318583954}]}
{"instance_id": "c55d014827ef", "generation_result": [{"code": "def factorization(dimension: int, factor:int=-1) -> tuple[int, int]\n\t\n    if factor < 0:\n        factor = abs(factor)\n        while factor > 1:\n            if dimension % factor == 0:\n                return factorization(dimension, factor)\n            factor -= 1\n        return factorization(dimension, factor)\n    elif factor > 1:\n        if dimension % factor == 0:\n            return factorization(dimension, factor)\n        else:\n            return factorization(dimension, factor-1)\n    else:\n        return dimension, 1\n\n", "CodeBleu_score": 29.56552153734625}, {"code": "def factorization(dimension: int, factor:int=-1) -> tuple[int, int]\n\t\n    if dimension < 1:\n        raise ValueError(\"Dimension must be a positive integer.\")\n    if factor < -1:\n        raise ValueError(\"Factor must be a non-negative integer or -1.\")\n    if factor > dimension:\n        raise ValueError(\"Factor must be less than or equal to dimension.\")\n    if factor == -1:\n        factor = dimension\n    if factor > 0 and dimension % factor == 0:\n        return factor, dimension // factor\n    if factor > 0:\n        for i in range(2, factor):\n            if factor % i == 0 and dimension % i == 0:\n                return i, dimension // i\n    if factor > 0:\n        for i in range(2, int(math.sqrt(dimension)) + 1):\n            if factor % i == 0 and dimension % i == 0:\n                return i, dimension // i\n    if factor > 0:\n        for i in range(2, factor):\n            if factor % i == 0 and dimension % i == 0:\n                return i, dimension // i\n    else:\n        for i in range(2, int(math.sqrt(dimension)) + 1):\n            if factor % i == 0 and dimension % i == 0:\n                return i, dimension // i\n        return dimension, 1\n", "CodeBleu_score": 31.39073789541523}, {"code": "def factorization(dimension: int, factor:int=-1) -> tuple[int, int]\n\t\n    if factor > 0 and dimension % factor == 0:\n        return factor, dimension // factor\n    elif factor < 0:\n        if dimension == 1:\n            return 1, 1\n        for f in range(2, abs(factor)+1):\n            if dimension % f == 0:\n                return f, dimension // f\n    else:\n        return 1, dimension\n\n", "CodeBleu_score": 25.04425574311138}]}
{"instance_id": "5d613ec26036", "generation_result": [{"code": "def apply_weight_decompose(self, updown, orig_weight)\n\t\n    # Ensure that all tensors are on the same device and have the same data type\n    updown = updown.to(orig_weight.device)\n    orig_weight = orig_weight.to(orig_weight.device)\n    orig_weight = orig_weight.to(updown.dtype)\n    # Transpose the weight to be of shape (out_channels, in_channels, kernel_size[0], kernel_size[1])\n    updown = updown.transpose(0, 1)\n    # Reshape the weight to be of shape (out_channels, in_channels * kernel_size[0] * kernel_size[1])\n    updown = updown.reshape(updown.shape[0], -1)\n    # Normalize the weight\n    updown = F.normalize(updown, dim=1, p=2)\n    # Scale the weight\n    updown = updown * self.scale\n    # Calculate the adjusted weight\n    adjusted_weight = updown - orig_weight\n    # Return the adjusted weight\n    return adjusted_weight\n", "CodeBleu_score": 17.900420011156793}, {"code": "def apply_weight_decompose(self, updown, orig_weight)\n\t\n    # Ensure that the input weight and original weight are on the same device and have the same data type\n    updown = updown.to(orig_weight.device)\n    orig_weight = orig_weight.to(orig_weight.device)\n    # Ensure that the input weight and original weight have the same dimensions\n    if updown.ndim != orig_weight.ndim:\n        raise ValueError(\"The input weight and original weight must have the same number of dimensions.\")\n    # Ensure that the input weight and original weight have the same shape\n    if updown.shape != orig_weight.shape:\n        raise ValueError(\"The input weight and original weight must have the same shape.\")\n    # Ensure that the input weight and original weight have the same number of elements\n    if updown.numel() != orig_weight.numel():\n        raise ValueError(\"The input weight and original weight must have the same number of elements.\")\n    # Ensure that the input weight and original weight have the same data type\n    if updown.dtype != orig_weight.dtype:\n        raise ValueError(\"The input weight and original weight must have the same data type.\")\n    # Ensure that the input weight and original weight are not sparse\n    if updown.is_sparse:\n        raise ValueError(\"The input weight and original weight must not be sparse.\")\n    # Ensure that the input weight and original weight are not quantized\n    if updown.is_quantized:\n        raise ValueError(\"The input weight and original weight must not be quantized.\")\n    # Ensure that the input weight and original weight are not sparse\n    if orig_weight.is_sparse:\n        raise ValueError(\"The input weight and original weight must not be sparse.\")\n    # Ensure that the input weight and original weight are not quantized\n    if orig_weight.is_quantized:\n        raise ValueError(\"The input weight and original weight must not be quantized.\")\n    # Ensure that the input weight and original weight are not sparse\n    if updown.requires_grad:\n        raise ValueError(\"The input weight and original weight must not require gradients.\")\n    # Ensure that the input weight and original weight are not sparse\n    if orig_weight.requires_grad:\n        raise ValueError(\"The input weight and original weight must not require gradients.\")\n    # Ensure that the input weight and original weight are not sparse\n    if updown.is_cuda:\n        raise ValueError(\"The input weight and original weight must not be on the GPU.\")\n    # Ensure that", "CodeBleu_score": 13.976462823161828}, {"code": "def apply_weight_decompose(self, updown, orig_weight)\n\t\n    # Make sure all tensors are on the same device and have the same data type\n    updown.to(self.weight.device)\n    orig_weight.to(self.weight.device)\n    orig_weight.to(self.weight.dtype)\n    # Transpose the original weight to match the shape of the updown tensor\n    orig_weight = orig_weight.transpose(0, 1)\n    # Reshape the original weight to match the shape of the updown tensor\n    orig_weight = orig_weight.reshape(updown.shape)\n    # Merge the updown and original weight tensors\n    merged_weight = updown + orig_weight\n    # Normalize the merged weight tensor\n    norm = torch.norm(merged_weight, p=2, dim=1).unsqueeze(1)\n    # Scale the merged weight tensor\n    merged_scale = merged_weight / norm\n    # Calculate the final adjusted weight tensor\n    adjusted_weight = merged_scale - orig_weight\n    # Return the final adjusted weight tensor\n    return adjusted_weight\n", "CodeBleu_score": 28.03298453691806}]}
{"instance_id": "c73d0d1cd3e5", "generation_result": [{"code": "def finalize_updown(self, updown, orig_weight, output_shape, ex_bias=None)\n\t\n    # Adjust the updown tensor with the bias and reshape it to the output shape\n    updown = updown.to(orig_weight.device)\n    updown = updown + ex_bias\n    updown = updown.reshape(output_shape)\n\n    # If the original and updown weights have the same number of elements, reshape updown to match the original weight's shape\n    if orig_weight.numel() == updown.numel():\n        updown = updown.reshape(orig_weight.shape)\n\n    # Apply the additional bias\n    if ex_bias is not None:\n        ex_bias = ex_bias * self.multiplier\n\n    # Apply the weight decomposition\n    if self.apply_weight_decompose:\n        updown = self.apply_weight_decompose(updown, orig_weight)\n\n    return updown, ex_bias\n\n", "CodeBleu_score": 47.57692163378234}, {"code": "def finalize_updown(self, updown, orig_weight, output_shape, ex_bias=None)\n\t\n    # Match the device/dtype\n    updown = updown.to(orig_weight.device)\n    orig_weight = orig_weight.to(updown.dtype)\n\n    # Adjust the updown tensor with the bias and reshape it\n    if ex_bias is not None:\n        updown = updown + ex_bias\n\n    if updown.ndim > 2:\n        updown = updown.reshape(updown.shape[0], -1)\n\n    # If the original and updown weights have the same number of elements, reshape updown to match the original weight's shape\n    if updown.shape[1] == orig_weight.numel():\n        updown = updown.reshape(updown.shape[0], *orig_weight.shape)\n\n    # Apply a multiplier to the bias\n    if self.multiplier is not None:\n        updown = self.multiplier(updown)\n\n    # Apply a weight decomposition to the updown tensor\n    if self.apply_weight_decompose is not None:\n        updown = self.apply_weight_decompose(updown, orig_weight)\n\n    return updown\n\n", "CodeBleu_score": 53.78726969830817}, {"code": "def finalize_updown(self, updown, orig_weight, output_shape, ex_bias=None)\n\t\n    # Match the device/dtype\n    orig_weight = orig_weight.to(updown.dtype)\n    updown = updown.to(orig_weight.device)\n\n    # Apply the bias\n    if ex_bias is not None:\n        ex_bias = ex_bias.to(updown.dtype)\n        updown = updown + ex_bias\n\n    # Reshape updown to match the original weight's shape\n    if updown.numel() == orig_weight.numel():\n        updown = updown.reshape(orig_weight.shape)\n\n    # Reshape updown to match the output shape\n    updown = updown.reshape(output_shape)\n\n    # Apply a multiplier to the bias\n    if self.multiplier is not None:\n        updown = self.multiplier(updown)\n\n    # Apply a weight decomposition to updown\n    if self.apply_weight_decompose is not None:\n        updown = self.apply_weight_decompose(updown, orig_weight)\n\n    return updown\n", "CodeBleu_score": 56.3255847828755}]}
{"instance_id": "c6a41665c62c", "generation_result": [{"code": "def calc_updown(self, orig_weight)\n\t\n    if self.updown is None:\n        return None\n\n    if self.updown.device != orig_weight.device:\n        self.updown = self.updown.to(orig_weight.device)\n\n    if self.bias is not None:\n        self.bias = self.bias.to(orig_weight.device)\n\n    return self.finalize_updown(self.updown, orig_weight, orig_weight.shape)\n", "CodeBleu_score": 40.33319083974381}, {"code": "def calc_updown(self, orig_weight)\n\t\n    if orig_weight.device != self.weight.device:\n        self.weight = self.weight.to(orig_weight.device)\n        if self.bias is not None:\n            self.bias = self.bias.to(orig_weight.device)\n\n    if self.bias is not None:\n        output_shape = (self.weight.shape[0], self.weight.shape[1], 1, 1)\n    else:\n        output_shape = (self.weight.shape[0], self.weight.shape[1], 1, 1)\n\n    return self.finalize_updown(self.weight, orig_weight, output_shape)\n", "CodeBleu_score": 42.841887647861846}, {"code": "def calc_updown(self, orig_weight)\n\t\n    orig_weight = orig_weight.to(self.weight.device)\n    updown = self.weight.detach().clone()\n    ex_bias = self.bias.detach().clone() if self.bias is not None else None\n\n    if self.dora_scale is not None:\n        updown = self.apply_weight_decompose(updown, orig_weight)\n\n    return self.finalize_updown(updown, orig_weight, self.weight.shape, ex_bias)\n\n", "CodeBleu_score": 37.65909228176857}]}
{"instance_id": "1bb9e5d94af8", "generation_result": [{"code": "def purge_networks_from_memory()\n\t\n    while len(networks_in_memory) > NETWORK_MEMORY_LIMIT:\n        networks_in_memory.pop()\n        gc.collect()\n", "CodeBleu_score": 16.390286455670605}, {"code": "def purge_networks_from_memory()\n\t\n    global networks_in_memory\n    global max_networks_in_memory\n    global gc_threshold\n    if len(networks_in_memory) > gc_threshold:\n        while len(networks_in_memory) > max_networks_in_memory:\n            networks_in_memory.pop()\n        gc.collect()\n", "CodeBleu_score": 34.58348215502179}, {"code": "def purge_networks_from_memory()\n\t\n    while len(networks_in_memory) > NETWORKS_LIMIT:\n        networks_in_memory.pop()\n        gc.collect()\n", "CodeBleu_score": 16.001032731274407}]}
{"instance_id": "42b48ebf4d07", "generation_result": [{"code": "def network_restore_weights_from_backup(self: Union[torch.nn.Conv2d, torch.nn.Linear, torch.nn.GroupNorm, torch.nn.LayerNorm, torch.nn.MultiheadAttention]):\n\t\n    # Check if backup exists\n    if os.path.exists(self.weights_backup_path) and os.path.exists(self.bias_backup_path):\n        # Load backup\n        weights_backup = torch.load(self.weights_backup_path)\n        bias_backup = torch.load(self.bias_backup_path)\n        # Check if layer is MultiheadAttention\n        if isinstance(self, torch.nn.MultiheadAttention):\n            # Restore projection weights and biases\n            self.in_proj_weight.data = weights_backup[0]\n            self.in_proj_bias.data = weights_backup[1]\n            self.out_proj.weight.data = weights_backup[2]\n            self.out_proj.bias.data = bias_backup\n        else:\n            # Restore weights and biases\n            self.weight.data = weights_backup\n            self.bias.data = bias_backup\n    else:\n        # Set bias to None\n        self.bias = None\n\n", "CodeBleu_score": 40.86940234907848}, {"code": "def network_restore_weights_from_backup(self: Union[torch.nn.Conv2d, torch.nn.Linear, torch.nn.GroupNorm, torch.nn.LayerNorm, torch.nn.MultiheadAttention]):\n\t\n    # Get the layer name\n    layer_name = self.__class__.__name__\n    # Get the backup weights and biases\n    backup_weights = self.backup_weights\n    backup_biases = self.backup_biases\n    # Check if the layer has a backup\n    if backup_weights is not None:\n        # Check if the layer is a MultiheadAttention layer\n        if layer_name == \"MultiheadAttention\":\n            # Get the projection weights and biases\n            projection_weights = self.in_proj_weight\n            projection_biases = self.in_proj_bias\n            # Copy the backup weights and biases to the projection weights and biases\n            projection_weights.data = backup_weights.data\n            projection_biases.data = backup_biases.data\n        else:\n            # Copy the backup weights to the layer weights\n            self.weight.data = backup_weights.data\n    # Check if the layer has a bias backup\n    if backup_biases is not None:\n        # Set the layer bias to the backup bias\n        self.bias.data = backup_biases.data\n    else:\n        # Set the layer bias to None\n        self.bias = None\n\n", "CodeBleu_score": 34.073955374477336}, {"code": "def network_restore_weights_from_backup(self: Union[torch.nn.Conv2d, torch.nn.Linear, torch.nn.GroupNorm, torch.nn.LayerNorm, torch.nn.MultiheadAttention]):\n\t\n    if hasattr(self, 'weight_backup') and self.weight_backup is not None:\n        self.weight.data = self.weight_backup.data.clone()\n    if hasattr(self, 'bias_backup') and self.bias_backup is not None:\n        self.bias.data = self.bias_backup.data.clone()\n    if isinstance(self, torch.nn.MultiheadAttention):\n        if hasattr(self, 'in_proj_weight_backup') and self.in_proj_weight_backup is not None:\n            self.in_proj_weight.data = self.in_proj_weight_backup.data.clone()\n        if hasattr(self, 'in_proj_bias_backup') and self.in_proj_bias_backup is not None:\n            self.in_proj_bias.data = self.in_proj_bias_backup.data.clone()\n        if hasattr(self, 'out_proj_weight_backup') and self.out_proj_weight_backup is not None:\n            self.out_proj_weight.data = self.out_proj_weight_backup.data.clone()\n        if hasattr(self, 'out_proj_bias_backup') and self.out_proj_bias_backup is not None:\n            self.out_proj_bias.data = self.out_proj_bias_backup.data.clone()\n", "CodeBleu_score": 36.26309104959813}]}
{"instance_id": "c8fcd357131a", "generation_result": [{"code": "def network_forward(org_module, input, original_forward)\n\t\n    if not hasattr(org_module, \"loaded_networks\"):\n        return original_forward(org_module, input)\n\n    # Cast input to the correct device\n    input = input.to(org_module.loaded_networks[0].weight.device)\n\n    # Restore weights from backup\n    org_module.network_restore_weights_from_backup()\n\n    # Perform the original forward pass\n    output = original_forward(org_module, input)\n\n    # Apply loaded networks\n    for loaded_network in org_module.loaded_networks:\n        if loaded_network.network_current_names == org_module.network_current_names:\n            output = loaded_network(output)\n\n    # Reset cached weights\n    org_module.network_reset_cached_weight()\n\n    return output\n", "CodeBleu_score": 44.68550011114487}, {"code": "def network_forward(org_module, input, original_forward)\n\t\n    if network_loaded():\n        network_restore_weights_from_backup(org_module)\n        network_reset_cached_weight(org_module)\n        org_module.to(input.device)\n        output = original_forward(org_module, input)\n        for net in network_loaded():\n            if net.layer_name == org_module.layer_name:\n                output = net.forward(output, input)\n    else:\n        output = original_forward(org_module, input)\n    return output\n", "CodeBleu_score": 37.8054726594641}, {"code": "def network_forward(org_module, input, original_forward)\n\t\n    if not hasattr(org_module, \"forward_networks\"):\n        return original_forward(org_module, input)\n\n    # Cast the input to the appropriate device\n    if isinstance(input, (list, tuple)):\n        input = [i.to(org_module.weight.device) for i in input]\n    else:\n        input = input.to(org_module.weight.device)\n\n    # Restore the original weights and reset the cached weights\n    network_restore_weights_from_backup(org_module)\n    network_reset_cached_weight(org_module)\n\n    # Call the original forward method\n    output = original_forward(org_module, input)\n\n    # Apply each loaded network\n    for name, network in org_module.forward_networks.items():\n        if name in org_module.network_current_names:\n            # Apply the network\n            if isinstance(input, (list, tuple)):\n                output = network(*input)\n            else:\n                output = network(input)\n\n    # Return the output\n    return output\n", "CodeBleu_score": 43.60722048079104}]}
{"instance_id": "1070211f5c11", "generation_result": [{"code": "def trainables(self)\n\t\n    return [p for p in self.parameters() if p.requires_grad]\n\n", "CodeBleu_score": 6.452447410697191}, {"code": "def trainables(self)\n\t\n    trainables = []\n    for layer in self.modules():\n        if isinstance(layer, (torch.nn.Linear, torch.nn.LayerNorm)):\n            trainables.extend(layer.parameters())\n    return trainables\n\n", "CodeBleu_score": 35.06674339876085}, {"code": "def trainables(self)\n\t\n    trainable_params = []\n    for name, param in self.named_parameters():\n        if param.requires_grad:\n            trainable_params.append(param)\n    return trainable_params\n\n", "CodeBleu_score": 22.79166744561824}]}
{"instance_id": "0c3ba62c96a7", "generation_result": [{"code": "def load_hypernetworks(names, multipliers=None)\n\t\n    if multipliers is None:\n        multipliers = [1.0] * len(names)\n\n    already_loaded = shared.hypernetworks.get(names[0], None)\n    if already_loaded is not None:\n        return\n\n    shared.hypernetworks.clear()\n    for name, multiplier in zip(names, multipliers):\n        hypernetwork = load_hypernetwork(name)\n        if hypernetwork is not None:\n            hypernetwork.set_multiplier(multiplier)\n            shared.hypernetworks.append(hypernetwork)\n\n", "CodeBleu_score": 36.387903996526184}, {"code": "def load_hypernetworks(names, multipliers=None)\n\t\n    if multipliers is None:\n        multipliers = [1] * len(names)\n\n    if len(multipliers) != len(names):\n        raise ValueError(\n            \"The number of hypernetwork names and multipliers must be the same.\"\n        )\n\n    already_loaded = shared.hypernetworks.get()\n    if already_loaded is None:\n        already_loaded = []\n        shared.hypernetworks.set(already_loaded)\n\n    clear()\n\n    for name, multiplier in zip(names, multipliers):\n        hypernetwork = load_hypernetwork(name)\n        if hypernetwork is not None:\n            hypernetwork.set_multiplier(multiplier)\n            already_loaded.append(hypernetwork)\n\n", "CodeBleu_score": 41.10066791090047}, {"code": "def load_hypernetworks(names, multipliers=None)\n\t\n    if multipliers is None:\n        multipliers = [1.0] * len(names)\n\n    if len(names) != len(multipliers):\n        raise ValueError(\"names and multipliers must have the same length\")\n\n    already_loaded = shared.hypernetworks\n\n    if len(already_loaded) == 0:\n        clear()\n\n    for name, multiplier in zip(names, multipliers):\n        if name in already_loaded:\n            hypernetwork = already_loaded[name]\n        else:\n            hypernetwork = load_hypernetwork(name)\n\n        if hypernetwork is not None:\n            hypernetwork.set_multiplier(multiplier)\n            already_loaded[name] = hypernetwork\n\n    return already_loaded\n\n", "CodeBleu_score": 36.62023429526256}]}
{"instance_id": "bea3772ed532", "generation_result": [{"code": "\ndef apply_hypernetworks(hypernetworks, context, layer=None)\n\t\n    context_k, context_v = context, context\n\n    for hypernetwork in hypernetworks:\n        context_k, context_v = apply_single_hypernetwork(hypernetwork, context_k, context_v, layer=layer)\n\n    return context_k, context_v\n\n", "CodeBleu_score": 55.435688315169486}, {"code": "\ndef apply_hypernetworks(hypernetworks, context, layer=None)\n\t\n\n    context_k, context_v = context, context\n    for hypernetwork in hypernetworks:\n        context_k, context_v = apply_single_hypernetwork(hypernetwork, context_k, context_v, layer=layer)\n    return context_k, context_v\n\n", "CodeBleu_score": 55.435688315169486}, {"code": "\ndef apply_hypernetworks(hypernetworks, context, layer=None)\n\t\n    context_k, context_v = context, context\n\n    for context_k, context_v in zip(context_k, context_v):\n        for hypernetwork in hypernetworks:\n            context_k, context_v = apply_single_hypernetwork(hypernetwork, context_k, context_v, layer)\n\n    return context_k, context_v\n\n\n", "CodeBleu_score": 62.86986170443332}]}
{"instance_id": "922efa3e092f", "generation_result": [{"code": "def attention_CrossAttention_forward(self, x, context=None, mask=None, **kwargs)\n\t_Parameters_:", "CodeBleu_score": 1.9834764933446987}, {"code": "def attention_CrossAttention_forward(self, x, context=None, mask=None, **kwargs)\n\t_parameters_: * **x** - Input tensor of shape `(batch_size, input_length, input_size)`. * **context** - Input tensor of shape `(batch_size, context_length, context_size)` or `(batch_size, context_length, context_size, heads)`. * **mask** - Binary mask of shape `(batch_size, context_length)` or `(batch_size, heads, context_length, context_length)`. * **kwargs** - Additional keyword arguments.", "CodeBleu_score": 7.28012845119946}, {"code": "def attention_CrossAttention_forward(self, x, context=None, mask=None, **kwargs)\n\t_CrossAttention_forward(Tensor query, Tensor key, Tensor value, Tensor context, Tensor mask, List[Tensor] weight_list, List[Tensor] bias_list, int num_heads, bool use_separate_proj_weight, bool use_separate_proj_bias, bool add_zero_attn, float dropout_p, bool is_cross_attention, bool is_decoding, bool use_softmax, bool is_encoder_decoder, int max_seq_len, int embedding_dim, int num_heads, int num_clusters, int num_clusters_heads, int num_clusters_per_head, int cluster_size, float eps, int layer_idx, bool use_cache, bool is_self_attention, bool use_separate_cls_weight, bool use_separate_cls_bias, bool use_separate_out_proj_weight, bool use_separate_out_proj_bias, bool use_separate_q_proj_weight, bool use_separate_q_proj_bias, bool use_separate_kv_proj_weight, bool use_separate_kv_proj_bias, bool use_separate_out_proj_weight_kv, bool use_separate_out_proj_bias_kv, bool use_separate_q_proj_weight_kv, bool use_separate_q_proj_bias_kv, bool use_separate_kv_proj_weight_kv, bool use_separate_kv_proj_bias_kv, bool use_separate_out_proj_weight_q, bool use_separate_out_proj_bias_q, bool use_separate_q_proj_weight_q, bool use_separate_q_proj_bias_q, bool use_separate_kv_proj_weight_q, bool use_separate_kv_proj_bias_q, bool use_separate_out_proj_weight_kv_q, bool use_separate_out_proj_bias_kv_q, bool use_separate_q_proj_weight_kv_q, bool use_separate_q_proj_bias_kv_q, bool use_separate_kv_proj_weight_kv_q, bool use_separate_kv_proj_bias_kv_q, bool use_separate_out_proj_weight_kv_kv, bool use_separate_out_", "CodeBleu_score": 13.653944370986002}]}
{"instance_id": "8b7455a03a0e", "generation_result": [{"code": "def save_hypernetwork(hypernetwork, checkpoint, hypernetwork_name, filename)\n\t\n    original_name = hypernetwork.name\n    original_checkpoint = hypernetwork.sd_checkpoint\n    original_checkpoint_name = hypernetwork.sd_checkpoint_name\n    hypernetwork.name = hypernetwork_name\n    hypernetwork.sd_checkpoint = checkpoint\n    hypernetwork.sd_checkpoint_name = checkpoint.name\n    try:\n        hypernetwork.save(filename)\n    except Exception as e:\n        hypernetwork.name = original_name\n        hypernetwork.sd_checkpoint = original_checkpoint\n        hypernetwork.sd_checkpoint_name = original_checkpoint_name\n        raise e\n\n", "CodeBleu_score": 38.04279173725107}, {"code": "def save_hypernetwork(hypernetwork, checkpoint, hypernetwork_name, filename)\n\t\n    try:\n        hypernetwork.checkpoint = checkpoint\n        hypernetwork.name = hypernetwork_name\n        hypernetwork.save(filename)\n    except Exception as e:\n        hypernetwork.checkpoint = checkpoint\n        hypernetwork.name = hypernetwork_name\n        raise e\n    finally:\n        hypernetwork.checkpoint = None\n        hypernetwork.name = None\n", "CodeBleu_score": 29.417870274648088}, {"code": "def save_hypernetwork(hypernetwork, checkpoint, hypernetwork_name, filename)\n\t\n    try:\n        # Save the original attributes\n        original_sd_checkpoint = hypernetwork.sd_checkpoint\n        original_sd_checkpoint_name = hypernetwork.sd_checkpoint_name\n        original_step = hypernetwork.step\n        original_name = hypernetwork.name\n\n        # Update the hypernetwork with the new checkpoint data\n        hypernetwork.sd_checkpoint = checkpoint\n        hypernetwork.sd_checkpoint_name = hypernetwork_name\n        hypernetwork.step = 0\n        hypernetwork.name = hypernetwork_name\n\n        # Save the hypernetwork\n        hypernetwork.save(filename)\n    except Exception as e:\n        # Restore the original attributes\n        hypernetwork.sd_checkpoint = original_sd_checkpoint\n        hypernetwork.sd_checkpoint_name = original_sd_checkpoint_name\n        hypernetwork.step = original_step\n        hypernetwork.name = original_name\n\n        # Re-raise the exception\n        raise e\n\n", "CodeBleu_score": 48.22438155553421}]}
{"instance_id": "3175f835140f", "generation_result": [{"code": "def ema_scope(self, context=None)\n\t\n    if context is None:\n        context = get_current_context()\n    if context.ema_enabled:\n        if context.ema_scope_count == 0:\n            # Save the current model parameters\n            context.ema_original_weights = self.parameters()\n            # Copy EMA weights to the model\n            self.copy_to(context.ema_model)\n            # Log the action\n            if context.ema_logging:\n                context.ema_log.append(('ema_scope', self))\n        context.ema_scope_count += 1\n    yield\n    if context.ema_enabled:\n        context.ema_scope_count -= 1\n        if context.ema_scope_count == 0:\n            # Restore the original model parameters\n            self.copy_to(context.ema_original_weights)\n            # Log the restoration\n            if context.ema_logging:\n                context.ema_log.append(('ema_scope', self))\n", "CodeBleu_score": 32.14697222691374}, {"code": "def ema_scope(self, context=None)\n\t\n    if self.ema_enabled:\n        if context is None:\n            context = self.context\n        if self.ema_scope_count == 0:\n            if self.ema_log_enabled:\n                self.ema_log.append(\"ema_scope: EMA scope enabled\")\n            if self.ema_restore_enabled:\n                self.ema_restore_weights = self.get_weights()\n                self.set_weights(self.ema_weights)\n        self.ema_scope_count += 1\n    yield\n    if self.ema_enabled:\n        self.ema_scope_count -= 1\n        if self.ema_scope_count == 0:\n            if self.ema_log_enabled:\n                self.ema_log.append(\"ema_scope: EMA scope disabled\")\n            if self.ema_restore_enabled:\n                self.set_weights(self.ema_restore_weights)\n                self.ema_restore_weights = None\n", "CodeBleu_score": 36.97010209297694}, {"code": "def ema_scope(self, context=None)\n\t\n    if context is None:\n        context = self.context\n    if context.ema_enabled:\n        if context.ema_logging:\n            self.log_ema_scope(context)\n        self.copy_to(context.ema_model)\n    yield\n    if context.ema_enabled:\n        self.copy_to(context.ema_model)\n        if context.ema_logging:\n            self.log_ema_scope(context)\n", "CodeBleu_score": 35.91725552863194}]}
{"instance_id": "a7b5d5cd519a", "generation_result": [{"code": "def p_mean_variance(self, x, t, clip_denoised: bool)\n\t\n    x_recon = self.predict_start_from_noise(x, t, self.denoise_fn(x, t))\n    if clip_denoised:\n        x_recon.clamp_(-1.0, 1.0)\n    model_mean, posterior_variance, posterior_log_variance = self.q_posterior(x_start=x_recon, x_t=x, t=t)\n    return model_mean, posterior_variance, posterior_log_variance\n", "CodeBleu_score": 35.378814416497164}, {"code": "def p_mean_variance(self, x, t, clip_denoised: bool)\n\t\n    x_recon = self.predict_start_from_noise(x, t, self.model(x, t))\n    if clip_denoised:\n        x_recon = x_recon.clamp_(-1.0, 1.0)\n    model_mean, posterior_variance, posterior_log_variance = self.q_posterior(x_start=x, x_t=x_recon, t=t)\n    return model_mean, posterior_variance, posterior_log_variance\n\n", "CodeBleu_score": 33.67298513623285}, {"code": "def p_mean_variance(self, x, t, clip_denoised: bool)\n\t\n    x_recon = self.predict_start_from_noise(x, t, self.denoise_fn(x, t))\n    if clip_denoised:\n        x_recon.clamp_(-1.0, 1.0)\n    model_mean, posterior_variance, posterior_log_variance = self.q_posterior(x_start=x_recon, x_t=x, t=t)\n    return model_mean, posterior_variance, posterior_log_variance\n", "CodeBleu_score": 35.378814416497164}]}
{"instance_id": "a8652fa78473", "generation_result": [{"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss_type == 'l1':\n        if mean:\n            return torch.mean(torch.abs(pred - target))\n        else:\n            return torch.abs(pred - target)\n    elif self.loss_type == 'l2':\n        if mean:\n            return torch.mean((pred - target)**2)\n        else:\n            return (pred - target)**2\n    else:\n        raise NotImplementedError(\"Unknown loss type\")\n", "CodeBleu_score": 35.55193690726113}, {"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss_type == 'l1':\n        loss = torch.abs(pred - target)\n        if mean:\n            loss = torch.mean(loss)\n    elif self.loss_type == 'l2':\n        loss = torch.pow(pred - target, 2)\n        if not mean:\n            loss = torch.mean(loss)\n    else:\n        raise NotImplementedError(f'Loss type {self.loss_type} is not implemented.')\n    return loss\n", "CodeBleu_score": 45.9615996712491}, {"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss_type == 'l1':\n        loss = torch.abs(pred - target)\n        if mean:\n            loss = torch.mean(loss)\n    elif self.loss_type == 'l2':\n        loss = F.mse_loss(pred, target, reduction='mean' if mean else 'none')\n    else:\n        raise NotImplementedError\n    return loss\n", "CodeBleu_score": 43.926721172068774}]}
{"instance_id": "d68e4f061349", "generation_result": [{"code": "def _prior_bpd(self, x_start)\n\t\n    batch_size = x_start.shape[0]\n    t = torch.full((batch_size,), self.num_timesteps, device=x_start.device, dtype=torch.long)\n    mean, _, log_variance = self.q_mean_variance(x_start=x_start, t=t)\n    return normal_kl(mean, log_variance) / np.log(2.0)\n\n", "CodeBleu_score": 29.402704866946607}, {"code": "def _prior_bpd(self, x_start)\n\t\n    batch_size = x_start.shape[0]\n    t = torch.full((batch_size,), self.num_timesteps - 1, device=x_start.device, dtype=torch.long)\n    mean, _, log_variance = self.q_mean_variance(x_start, t)\n    kl = normal_kl(mean, log_variance, torch.zeros_like(mean), torch.zeros_like(log_variance))\n    return kl.sum(dim=1) / (math.log(2) * x_start.shape[1])\n", "CodeBleu_score": 32.668244577000905}, {"code": "def _prior_bpd(self, x_start)\n\t\n    # get the batch size\n    batch_size = x_start.shape[0]\n\n    # create a tensor t representing the final timestep for each batch element\n    t = torch.zeros(batch_size, device=self.betas.device, dtype=torch.long)\n\n    # compute the mean and log variance of the predicted distribution at this timestep\n    mean, variance, log_variance = self.q_mean_variance(x_start, t)\n\n    # calculate the KL divergence between this distribution and a standard normal distribution\n    kl = normal_kl(mean, log_variance, variance)\n\n    # return the mean KL divergence normalized by the logarithm of 2\n    return kl.mean() / math.log(2.0)\n", "CodeBleu_score": 30.816077701747602}]}
{"instance_id": "c695d93fdf2d", "generation_result": [{"code": "def make_cache(subsection: str) -> diskcache.Cache\n\t\n    cache_dir = os.path.join(os.environ[\"HOME\"], \".cache\", \"wiki_cache\")\n    if not os.path.exists(cache_dir):\n        os.makedirs(cache_dir)\n    return diskcache.Cache(\n        os.path.join(cache_dir, subsection), size_limit=4 * 1024 ** 3, disk_min_file_size=256\n    )\n", "CodeBleu_score": 59.946731366577396}, {"code": "def make_cache(subsection: str) -> diskcache.Cache\n\t\n    cache_dir = os.path.join(CACHE_DIR, subsection)\n    cache = diskcache.Cache(cache_dir, size_limit=4 * 1024 ** 3, disk=diskcache.JSONDisk, disk_min_file_size=256)\n    return cache\n\n", "CodeBleu_score": 49.8265789325742}, {"code": "def make_cache(subsection: str) -> diskcache.Cache\n\t\n    return diskcache.Cache(\n        f\"{CACHE_DIRECTORY}/{subsection}\",\n        size_limit=4 * 1024 ** 3,\n        disk=diskcache.JSONDisk,\n        disk_min_file_size=256,\n    )\n\n", "CodeBleu_score": 50.537240551853536}]}
{"instance_id": "ed4c543f0d38", "generation_result": [{"code": "def cache(subsection)\n\t\n    with cache_lock:\n        if not os.path.exists(cache_dir):\n            os.makedirs(cache_dir)\n\n        if not os.path.isfile(cache_filename):\n            convert_old_cached_data()\n\n        if subsection not in caches:\n            caches[subsection] = make_cache(subsection)\n\n        return caches[subsection]\n", "CodeBleu_score": 41.4027647040064}, {"code": "def cache(subsection)\n\t\n    global caches, cache_lock\n\n    cache_lock.acquire()\n\n    if subsection not in caches:\n        if not os.path.isdir(cache_dir):\n            os.makedirs(cache_dir)\n        if not os.path.isfile(cache_filename):\n            convert_old_cached_data()\n\n    cache_lock.release()\n\n    cache_obj = caches.get(subsection)\n    if cache_obj is None:\n        cache_obj = make_cache(subsection)\n        caches[subsection] = cache_obj\n\n    return cache_obj\n\n", "CodeBleu_score": 57.89670016266382}, {"code": "def cache(subsection)\n\t\n    global caches, cache_lock\n\n    with cache_lock:\n        if subsection in caches:\n            return caches[subsection]\n\n        if not os.path.exists(cache_dir):\n            os.makedirs(cache_dir)\n\n        if os.path.isfile(cache_filename):\n            with open(cache_filename, \"r\", encoding=\"utf8\") as file:\n                data = json.load(file)\n        else:\n            convert_old_cached_data()\n            if os.path.isfile(cache_filename):\n                with open(cache_filename, \"r\", encoding=\"utf8\") as file:\n                    data = json.load(file)\n            else:\n                data = {}\n\n        if subsection in data:\n            cache_obj = make_cache(subsection)\n            caches[subsection] = cache_obj\n            return cache_obj\n\n        cache_obj = make_cache(subsection)\n        caches[subsection] = cache_obj\n        data[subsection] = {}\n\n        with open(cache_filename, \"w\", encoding=\"utf8\") as file:\n            json.dump(data, file, indent=4, sort_keys=True, ensure_ascii=False)\n\n        return cache_obj", "CodeBleu_score": 49.8843514933762}]}
{"instance_id": "ccf5ea2d9fc1", "generation_result": [{"code": "def cached_data_for_file(subsection, title, filename, func)\n\t\n\n    cache_obj = cache(subsection)\n    entry = cache_obj.get(title)\n    if entry:\n        mtime = os.path.getmtime(filename)\n        if mtime > entry.get(\"mtime\"):\n            cache_obj.delete(title)\n            entry = None\n\n    if not entry:\n        data = func()\n        mtime = os.path.getmtime(filename)\n        entry = {\"data\": data, \"mtime\": mtime}\n        cache_obj.set(title, entry)\n        dump_cache()\n\n    return entry.get(\"data\")\n\n", "CodeBleu_score": 44.775459444764685}, {"code": "def cached_data_for_file(subsection, title, filename, func)\n\t\n\n    cache_obj = cache(subsection)\n    entry = cache_obj.get(title)\n    if entry:\n        mtime = getmtime(filename)\n        if mtime > entry.get('mtime'):\n            cache_obj.delete(title)\n            entry = None\n\n    if not entry:\n        data = func()\n        mtime = getmtime(filename)\n        entry = {'mtime': mtime, 'data': data}\n        cache_obj.set(title, entry)\n        dump_cache()\n\n    return entry.get('data')\n", "CodeBleu_score": 40.66000248296}, {"code": "def cached_data_for_file(subsection, title, filename, func)\n\t\n\n    cache_obj = cache(subsection)\n    entry = cache_obj.get(title)\n    mtime = getmtime(filename)\n    if entry and entry.get(\"mtime\") >= mtime:\n        cache_obj.delete(title)\n        entry = None\n    if not entry:\n        data = func()\n        entry = {\"data\": data, \"mtime\": mtime}\n        cache_obj.set(title, entry)\n        dump_cache()\n    return entry[\"data\"]\n", "CodeBleu_score": 35.738870526336974}]}
{"instance_id": "e01a2d4f0ac2", "generation_result": [{"code": "def report(message: str, *, exc_info: bool = False) -> None\n\t\n    record_exception()\n    for line in message.splitlines():\n        print(\"***\", line, file=sys.stderr)\n    if exc_info:\n        print(\"---\", file=sys.stderr)\n        print(textwrap.indent(traceback.format_exc(), \"    \"), file=sys.stderr)\n\n", "CodeBleu_score": 74.20523584662352}, {"code": "def report(message: str, *, exc_info: bool = False) -> None\n\t\n    record_exception()\n    for line in message.splitlines():\n        print(\"*** \" + line, file=sys.stderr)\n    if exc_info:\n        print(textwrap.indent(traceback.format_exc(), \"    \"), file=sys.stderr)\n        print(\"---\", file=sys.stderr)\n\n", "CodeBleu_score": 73.6174005969053}, {"code": "def report(message: str, *, exc_info: bool = False) -> None\n\t\n    record_exception()\n    print(\"***\", message, file=sys.stderr)\n    if exc_info:\n        print(\"---\", file=sys.stderr)\n        print(textwrap.indent(traceback.format_exc(), \"    \"), file=sys.stderr)\n\n", "CodeBleu_score": 55.76258774579736}]}
{"instance_id": "125fb9d2320b", "generation_result": [{"code": "def extract_original_prompts(style: PromptStyle, prompt, negative_prompt)\n\t\n\n    if style.prompt == \"\" and style.negative_prompt == \"\":\n        return False, prompt, negative_prompt\n\n    if style.prompt == \"\":\n        return False, prompt, negative_prompt\n\n    if style.negative_prompt == \"\":\n        return False, prompt, negative_prompt\n\n    extracted_prompt, prompt = extract_style_text_from_prompt(style.prompt, prompt)\n\n    if not extracted_prompt:\n        return False, prompt, negative_prompt\n\n    extracted_negative_prompt, negative_prompt = extract_style_text_from_prompt(style.negative_prompt, negative_prompt)\n\n    if not extracted_negative_prompt:\n        return False, prompt, negative_prompt\n\n    return True, prompt, negative_prompt", "CodeBleu_score": 66.45963498715179}, {"code": "def extract_original_prompts(style: PromptStyle, prompt, negative_prompt)\n\t\n\n    if style.prompt == \"\" and style.negative_prompt == \"\":\n        return False, prompt, negative_prompt\n\n    if style.prompt != \"\":\n        prompt_extracted, prompt = extract_style_text_from_prompt(style.prompt, prompt)\n    else:\n        prompt_extracted = False\n\n    if style.negative_prompt != \"\":\n        negative_prompt_extracted, negative_prompt = extract_style_text_from_prompt(style.negative_prompt, negative_prompt)\n    else:\n        negative_prompt_extracted = False\n\n    if prompt_extracted and negative_prompt_extracted:\n        return True, prompt, negative_prompt\n    else:\n        return False, prompt, negative_prompt", "CodeBleu_score": 56.5271739215513}, {"code": "def extract_original_prompts(style: PromptStyle, prompt, negative_prompt)\n\t\n\n    if style.positive_prompt == \"\" and style.negative_prompt == \"\":\n        return False, prompt, negative_prompt\n\n    if style.positive_prompt != \"\":\n        is_positive_prompt_extracted, prompt = extract_style_text_from_prompt(style.positive_prompt, prompt)\n\n        if not is_positive_prompt_extracted:\n            return False, prompt, negative_prompt\n\n    if style.negative_prompt != \"\":\n        is_negative_prompt_extracted, negative_prompt = extract_style_text_from_prompt(style.negative_prompt, negative_prompt)\n\n        if not is_negative_prompt_extracted:\n            return False, prompt, negative_prompt\n\n    return True, prompt, negative_prompt", "CodeBleu_score": 63.66156845967777}]}
{"instance_id": "0c5f729fbce2", "generation_result": [{"code": "def random_partial(self, count, n_frames)\n\t\n    if self.utterances is None:\n        self._load_utterances()\n\n    utterances = self.utterance_cycler.sample(count)\n\n    a = [(u,) + u.random_partial(n_frames) for u in utterances]\n\n    return a\n", "CodeBleu_score": 94.20475781398312}, {"code": "def random_partial(self, count, n_frames)\n\t\n    if self.utterances is None:\n        self._load_utterances()\n\n    utterances = self.utterance_cycler.sample(count)\n\n    a = [(u,) + u.random_partial(n_frames) for u in utterances]\n\n    return a\n\n", "CodeBleu_score": 94.20475781398312}, {"code": "def random_partial(self, count, n_frames)\n\t\n    if self.utterances is None:\n        self._load_utterances()\n\n    utterances = self.utterance_cycler.sample(count)\n\n    a = [(u,) + u.random_partial(n_frames) for u in utterances]\n\n    return a\n", "CodeBleu_score": 94.20475781398312}]}
{"instance_id": "31e8b27b8f94", "generation_result": [{"code": "def normalize_volume(wav, target_dBFS, increase_only=False, decrease_only=False)\n\t\n    if increase_only and decrease_only:\n        raise ValueError(\"Both increase only and decrease only are set\")\n    rms = np.sqrt(np.mean((wav * 32768) ** 2))\n    change_in_dBFS = target_dBFS - 20 * np.log10(rms)\n    if (change_in_dBFS < 0 and increase_only) or (change_in_dBFS > 0 and decrease_only):\n        return wav\n    return wav * (10 ** (change_in_dBFS / 20))\n\n", "CodeBleu_score": 69.42607875542353}, {"code": "def normalize_volume(wav, target_dBFS, increase_only=False, decrease_only=False)\n\t\n    if increase_only and decrease_only:\n        raise ValueError(\"Both increase only and decrease only are set\")\n    RMS = np.sqrt(np.mean(wav ** 2))\n    change_in_dBFS = target_dBFS - 10 * np.log10(RMS)\n    if (change_in_dBFS < 0 and increase_only) or (change_in_dBFS > 0 and decrease_only):\n        return wav\n    return wav * (10 ** (change_in_dBFS / 20))\n", "CodeBleu_score": 75.34841795640982}, {"code": "def normalize_volume(wav, target_dBFS, increase_only=False, decrease_only=False)\n\t\n    if increase_only and decrease_only:\n        raise ValueError(\"Both increase only and decrease only are set\")\n    dBFS_change = target_dBFS - 10 * np.log10(np.mean(wav ** 2))\n    if (dBFS_change < 0 and increase_only) or (dBFS_change > 0 and decrease_only):\n        return wav\n    return wav * (10 ** (dBFS_change / 20))\n\n", "CodeBleu_score": 96.28790819755086}]}
{"instance_id": "2353831e9033", "generation_result": [{"code": "def load_model(weights_fpath: Path, device=None)\n\t\n\n    if device is None:\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # Initialize the model and load the weights\n    _model = SpeakerEncoder()\n    _model.load_state_dict(torch.load(weights_fpath, map_location=device))\n    _model.eval()\n\n    print(f\"Loaded model: {weights_fpath}", "CodeBleu_score": 35.03661965151883}, {"code": "def load_model(weights_fpath: Path, device=None)\n\t\n    global _model\n    if device is None:\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    checkpoint = torch.load(weights_fpath, map_location=device)\n    _model = SpeakerEncoder(device, checkpoint[\"model_kwargs\"])\n    _model.load_state_dict(checkpoint[\"model\"])\n    _model.eval()\n    print(\"Loaded model:\", weights_fpath)\n    print(\"Model step:\", checkpoint[\"step\"])\n    return _model\n", "CodeBleu_score": 52.79202025400054}, {"code": "def load_model(weights_fpath: Path, device=None)\n\t\n    global _model\n    if device is None:\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    checkpoint = torch.load(weights_fpath, map_location=device)\n    _model = SpeakerEncoder(**checkpoint[\"model_kwargs\"])\n    _model.load_state_dict(checkpoint[\"state_dict\"])\n    _model.eval()\n    print(f\"Loaded model {weights_fpath} trained to step {checkpoint['step']} on device {device}.\")\n    return _model\n", "CodeBleu_score": 54.0811024052736}]}
{"instance_id": "3231ad480199", "generation_result": [{"code": "def embed_frames_batch(frames_batch)\n\t\n    if _model is None:\n        raise Exception('Model not loaded!')\n    frames_batch = torch.from_numpy(frames_batch).to(_device)\n    with torch.no_grad():\n        embeds = _model.forward(frames_batch).detach().cpu().numpy()\n    return embeds\n", "CodeBleu_score": 68.42581326693666}, {"code": "def embed_frames_batch(frames_batch)\n\t\n    if _model is None:\n        raise Exception(\"Model not loaded!\")\n    frames_batch = torch.from_numpy(frames_batch).to(_device)\n    with torch.no_grad():\n        embeds = _model.forward(frames_batch).detach().cpu().numpy()\n    return embeds\n\n", "CodeBleu_score": 70.24827081320355}, {"code": "def embed_frames_batch(frames_batch)\n\t\n    if _model is None:\n        raise Exception('Model not loaded!')\n\n    frames_batch = torch.from_numpy(frames_batch).to(_device)\n    embeddings = _model.forward(frames_batch).detach().cpu().numpy()\n    return embeddings\n\n", "CodeBleu_score": 67.96104617456447}]}
{"instance_id": "117a41716c8d", "generation_result": [{"code": "def compute_partial_slices(n_samples, partial_utterance_n_frames=partials_n_frames,\n                           min_pad_coverage=0.75, overlap=0.5)\n\t\n    assert 0.0 < overlap < 1.0, \"Overlap must be between 0.0 and 1.0!\"\n    assert 0.0 < min_pad_coverage < 1.0, \"Minimum pad coverage must be between 0.0 and 1.0!\"\n    frame_step = int(np.ceil(n_samples * (1.0 - overlap)))\n    assert frame_step > 0, \"Overlap must be lower than 1.0 for the given sampling rate!\"\n    n_frames = int(np.ceil(n_samples / frame_step))\n    assert n_frames > 0, \"The audio file has less than 1 frame!\"\n    assert partial_utterance_n_frames > 0, \"Partial utterance number of frames must be greater than 0!\"\n    assert partial_utterance_n_frames <= n_frames, \"Partial utterance number of frames must be less than or equal to the number of frames!\"\n    assert 0.0 < min_pad_coverage <= 1.0, \"Minimum pad coverage must be between 0.0 and 1.0!\"\n    n_pad = int(np.ceil(partial_utterance_n_frames * (1.0 - min_pad_coverage)))\n    assert n_pad >= 0, \"Invalid value for minimum pad coverage!\"\n    wav_slices = []\n    mel_slices = []\n    for i in range(0, n_frames - partial_utterance_n_frames + 1, frame_step):\n        mel_slices.append((i, i + partial_utterance_n_frames))\n        wav_slices.append((i * frame_step, i * frame_step + partial_utterance_n_frames * frame_step))\n    last_wav_slice = wav_slices[-1]\n    last_mel_slice = mel_slices[-1]\n    last_wav_slice_coverage = (last_wav_slice[1] - last_wav_slice[0]) / float(n_samples)\n    last_mel_slice_coverage = (last_mel_slice[1] - last_mel_slice[0]) / float(partial_utterance_n_frames)\n    if last_wav_slice_coverage < min_pad_coverage and len(wav_slices) > 1:", "CodeBleu_score": 30.057610650468842}, {"code": "def compute_partial_slices(n_samples, partial_utterance_n_frames=partials_n_frames,\n                           min_pad_coverage=0.75, overlap=0.5)\n\t\n    # Compute number of frames, ensure it is not smaller than partial utterance size\n    n_frames = int(np.ceil(float(n_samples) / sampling_rate * frames_per_second))\n    assert n_frames >= partial_utterance_n_frames, \"Audio file is too short.\"\n\n    # Compute frame step\n    frame_step = int(\n        np.ceil(\n            float(n_frames - partial_utterance_n_frames) * (1. - overlap)))\n\n    # Compute number of slices\n    n_slices = int(np.ceil(float(n_frames - partial_utterance_n_frames) / frame_step))\n\n    # Compute mel and wav slices\n    wav_slices = []\n    mel_slices = []\n    for i in range(n_slices):\n        mel_slices.append(slice(i * frame_step, i * frame_step + partial_utterance_n_frames))\n        wav_slices.append(slice(int(i * frame_step * sampling_rate / frames_per_second),\n                                int((i * frame_step + partial_utterance_n_frames) * sampling_rate / frames_per_second)))\n\n    # Evaluate coverage for last slice, remove last slice if coverage is too low\n    coverage = (mel_slices[-1].stop - mel_slices[-1].start) / float(n_frames)\n    if coverage < min_pad_coverage and len(mel_slices) > 1:\n        mel_slices = mel_slices[:-1]\n        wav_slices = wav_slices[:-1]\n\n    return wav_slices, mel_slices\n", "CodeBleu_score": 42.06457065817843}, {"code": "def compute_partial_slices(n_samples, partial_utterance_n_frames=partials_n_frames,\n                           min_pad_coverage=0.75, overlap=0.5)\n\t\n    # Compute the number of frames per partial utterance, based on the sampling rate\n    frames_per_partial = int(np.ceil(partial_utterance_n_frames / (1 - overlap)))\n\n    # To guarantee some minimum amount of padding, we need to compute the number of frames\n    # that the last partial utterance should cover.\n    last_partial_frames = int(np.ceil((n_samples - frames_per_partial * (partials_n_utterances - 1)) / frames_per_partial))\n\n    # Make sure that the last partial utterance has a minimum coverage of min_pad_coverage\n    assert last_partial_frames >= int(min_pad_coverage * partial_utterance_n_frames)\n\n    # Compute the number of frames that should be covered by at least one partial utterance,\n    # based on the minimum coverage\n    min_frames = int(np.ceil(min_pad_coverage * partial_utterance_n_frames))\n\n    # Compute the number of frames that are covered by all the partial utterances,\n    # based on the minimum coverage and number of utterances\n    partials_coverage = frames_per_partial * partials_n_utterances\n\n    # Compute the number of frames of padding that should be added,\n    # based on how much is not covered by partials_coverage\n    pad_size = (min_frames - partials_coverage)\n\n    # Make sure that the padding is not more than what is available in the audio\n    assert pad_size <= n_samples\n\n    # Compute the frame step, which is the number of frames that will be skipped\n    # when processing partial utterances\n    frame_step = int(np.ceil((1 - overlap) * frames_per_partial))\n\n    # Initialize wav_slices and mel_slices\n    wav_slices = []\n    mel_slices = []\n\n    # Add slices until the coverage is reached\n    for i in range(partials_n_utterances):\n        # Compute mel and wav slices\n        mel_start = int(i * frames_per_partial)\n        mel_end = mel_start + frames_per_partial\n        mel_slices.append(slice(mel_start, mel_end))\n\n        wav_start = int(i * frame_step)\n        wav_end = wav_start + frames_per_partial\n        wav_slices.append(slice(wav_start, wav", "CodeBleu_score": 24.52695588024741}]}
{"instance_id": "34fe5c864f1c", "generation_result": [{"code": "def plot_embedding_as_heatmap(embed, ax=None, title=\"\", shape=None, color_range=(0, 0.30))\n\t\n    if ax is None:\n        ax = plt.gca()\n    if shape is None:\n        height = int(np.sqrt(len(embed)))\n        shape = (height, height)\n    embed = embed.reshape(shape)\n    cmap = cm.get_cmap(\"Reds\")\n    norm = Normalize(vmin=color_range[0], vmax=color_range[1])\n    sm = cm.ScalarMappable(cmap=cmap, norm=norm)\n    sm.set_array([])\n    ax.imshow(embed, cmap=cmap, norm=norm)\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.set_title(title)\n    plt.colorbar(sm, fraction=0.046, pad=0.04)\n    plt.show()\n\n", "CodeBleu_score": 67.25393307076732}, {"code": "def plot_embedding_as_heatmap(embed, ax=None, title=\"\", shape=None, color_range=(0, 0.30))\n\t\n    if ax is None:\n        ax = plt.gca()\n    if shape is None:\n        height = int(np.sqrt(len(embed)))\n        width = int(np.ceil(len(embed) / height))\n        shape = (height, width)\n    embed = embed.reshape(shape)\n    ax.imshow(embed, cmap=cm.get_cmap(\"Blues\"), vmin=color_range[0], vmax=color_range[1])\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.set_title(title)\n    plt.colorbar(cm.ScalarMappable(cmap=cm.get_cmap(\"Blues\")), ax=ax)\n", "CodeBleu_score": 57.1183090788352}, {"code": "def plot_embedding_as_heatmap(embed, ax=None, title=\"\", shape=None, color_range=(0, 0.30))\n\t\n    if ax is None:\n        ax = plt.gca()\n    if shape is None:\n        height = int(np.sqrt(len(embed)))\n        shape = (height, height)\n    embed = embed.reshape(shape)\n    cmap = cm.get_cmap(\"plasma\")\n    sm = cm.ScalarMappable(cmap=cmap, norm=plt.Normalize(vmin=color_range[0], vmax=color_range[1]))\n    sm.set_array([])\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.set_title(title)\n    ax.imshow(embed, cmap=cmap)\n    plt.colorbar(sm)\n", "CodeBleu_score": 67.27664775952972}]}
{"instance_id": "92e93d50b62b", "generation_result": [{"code": "def similarity_matrix(self, embeds)\n\t\n    # Get number of speakers and utterances per speaker\n    num_speakers, num_utts = embeds.shape[:2]\n    # Initialize similarity matrix and mask matrix\n    sim_mat = torch.zeros((num_speakers, num_speakers), device=embeds.device)\n    mask_mat = torch.ones((num_speakers, num_speakers), device=embeds.device)\n    # Loop over speakers\n    for i in range(num_speakers):\n        # Get centroids for speaker i\n        centroids_incl = torch.mean(embeds[i], dim=0)\n        centroids_incl = centroids_incl / torch.norm(centroids_incl)\n        centroids_excl = torch.mean(embeds[i][:, 1:], dim=0)\n        centroids_excl = centroids_excl / torch.norm(centroids_excl)\n        # Calculate similarity scores\n        sim_scores_incl = torch.matmul(embeds[i], centroids_incl)\n        sim_scores_excl = torch.matmul(embeds[i], centroids_excl)\n        # Update similarity matrix\n        sim_mat[i] = sim_scores_incl\n        # Update mask matrix\n        mask_mat[i, i] = 0.0\n    # Scale and bias similarity matrix\n    sim_mat = self.scale * sim_mat + self.bias\n    # Mask similarity matrix\n    sim_mat = sim_mat * mask_mat\n    # Return similarity matrix\n    return sim_mat\n", "CodeBleu_score": 28.009953369699268}, {"code": "def similarity_matrix(self, embeds)\n\t\n    batch_size, num_speakers, num_utterances, _ = embeds.shape\n    centroids_incl = torch.zeros(batch_size, num_speakers, num_utterances, self.embedding_size, device=self.device)\n    centroids_excl = torch.zeros(batch_size, num_speakers, self.embedding_size, device=self.device)\n    mask = torch.ones(batch_size, num_speakers, num_speakers, device=self.device)\n    for batch in range(batch_size):\n        for speaker in range(num_speakers):\n            for utterance in range(num_utterances):\n                centroids_incl[batch, speaker, utterance] = F.normalize(embeds[batch, speaker, utterance], p=2, dim=0)\n            centroids_excl[batch, speaker] = F.normalize(torch.mean(embeds[batch, speaker], dim=0), p=2, dim=0)\n        for speaker1 in range(num_speakers):\n            for speaker2 in range(num_speakers):\n                if speaker1 != speaker2:\n                    mask[batch, speaker1, speaker2] = 0\n    sim_mat = torch.zeros(batch_size, num_speakers, num_speakers, device=self.device)\n    for batch in range(batch_size):\n        for speaker1 in range(num_speakers):\n            for speaker2 in range(num_speakers):\n                if speaker1 != speaker2:\n                    sim_mat[batch, speaker1, speaker2] = torch.sum(torch.mul(centroids_incl[batch, speaker1], centroids_excl[batch, speaker2]))\n    sim_mat = torch.mul(sim_mat, mask)\n    sim_mat = torch.mul(sim_mat, 1.0 / torch.norm(sim_mat, p=2, dim=2, keepdim=True))\n    sim_mat = torch.mul(sim_mat, self.scale)\n    sim_mat = torch.add(sim_mat, self.bias)\n    return sim_mat\n\n", "CodeBleu_score": 25.858321485864284}, {"code": "def similarity_matrix(self, embeds)\n\t\n    batch_size, num_speakers, num_utterances, embedding_size = embeds.shape\n    centroids_incl = torch.zeros((batch_size, num_speakers, embedding_size), device=embeds.device)\n    centroids_excl = torch.zeros((batch_size, num_speakers, embedding_size), device=embeds.device)\n    mask = torch.ones((batch_size, num_speakers, num_speakers), device=embeds.device)\n    for speaker in range(num_speakers):\n        centroids_incl[:, speaker] = torch.mean(embeds[:, speaker], dim=1)\n        centroids_excl[:, speaker] = torch.mean(embeds[:, speaker, 1:], dim=1)\n        centroids_incl[:, speaker] = torch.nn.functional.normalize(centroids_incl[:, speaker], p=2, dim=0)\n        centroids_excl[:, speaker] = torch.nn.functional.normalize(centroids_excl[:, speaker], p=2, dim=0)\n        mask[:, speaker, speaker] = 0\n    similarity_matrix = torch.zeros((batch_size, num_speakers, num_speakers), device=embeds.device)\n    for speaker in range(num_speakers):\n        similarity_matrix[:, speaker] = torch.sum(embeds[:, speaker] * centroids_incl[:, speaker], dim=1) - torch.sum(embeds[:, speaker] * centroids_excl[:, speaker], dim=1)\n    similarity_matrix = similarity_matrix / torch.norm(centroids_incl, dim=2) + torch.norm(centroids_excl, dim=2)\n    return similarity_matrix * mask\n", "CodeBleu_score": 28.692427967727774}]}
{"instance_id": "a6eba54108a1", "generation_result": [{"code": "def loss(self, embeds)\n\t\n    speakers_per_batch, utterances_per_speaker = embeds.shape[:2]\n    sim_matrix = self.similarity_matrix(embeds)\n    sim_matrix = sim_matrix.reshape(-1, speakers_per_batch)\n    labels = np.repeat(np.arange(speakers_per_batch), utterances_per_speaker)\n    loss = self.loss_fn(sim_matrix, torch.from_numpy(labels).to(self.loss_device))\n    with torch.no_grad():\n        preds = sim_matrix.detach().cpu().numpy()\n        fpr, tpr, thresholds = roc_curve(labels, preds)\n        eer = brentq(lambda x : 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n    return loss, eer\n", "CodeBleu_score": 51.101384936184466}, {"code": "def loss(self, embeds)\n\t\n    speakers_per_batch, utterances_per_speaker = embeds.shape[:2]\n\n    # Compute the similarity matrix\n    sim_matrix = self.similarity_matrix(embeds)\n    sim_matrix = sim_matrix.reshape(speakers_per_batch * utterances_per_speaker, -1)\n\n    # Compute the loss\n    pos_sim = torch.exp(sim_matrix)\n    neg_sim = torch.exp(-sim_matrix)\n    loss_partial = torch.log(pos_sim.sum(dim=1)) + torch.log(neg_sim.sum(dim=1))\n    loss_val = -loss_partial.mean()\n\n    # Compute the EER\n    with torch.no_grad():\n        # Create the labels for the EER\n        labels = torch.arange(speakers_per_batch * utterances_per_speaker).to(self.loss_device)\n        labels = torch.repeat_interleave(labels, utterances_per_speaker)\n        labels = torch.repeat_interleave(labels, speakers_per_batch)\n        labels = (labels.detach().cpu().numpy() % speakers_per_batch) == np.arange(speakers_per_batch)\n        labels = torch.from_numpy(labels).to(self.loss_device)\n\n        # Compute the EER\n        fpr, tpr, thresholds = roc_curve(labels.cpu(), sim_matrix.detach().cpu())\n        eer = brentq(lambda x: 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n\n    return loss_val, eer\n", "CodeBleu_score": 55.494274898043514}, {"code": "def loss(self, embeds)\n\t\n    speakers_per_batch, utterances_per_speaker = embeds.shape[:2]\n    sim_matrix = self.similarity_matrix(embeds)\n    sim_matrix = sim_matrix.reshape(speakers_per_batch * utterances_per_speaker,\n                                    speakers_per_batch)\n    sim_matrix = sim_matrix.detach().cpu().numpy()\n    labels = np.repeat(np.arange(speakers_per_batch), utterances_per_speaker)\n    loss_value = self.loss_fn(sim_matrix, labels).item()\n    fpr, tpr, thresholds = roc_curve(labels, sim_matrix.flatten())\n    eer = brentq(lambda x : 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n    return loss_value, eer\n", "CodeBleu_score": 48.97193178856556}]}
{"instance_id": "ac7b74b9d1b1", "generation_result": [{"code": "def log_params(self)\n\t\n    if not self.disable_params:\n        # Import parameter configurations\n        if self.params_config_path is not None:\n            self.params_config = pd.read_csv(self.params_config_path)\n\n        # Create a string of model and data parameters\n        params = []\n        for name, value in self.get_params().items():\n            # Add parameter name and value to the list\n            if self.params_config_path is not None:\n                # If the parameter name is in the parameter configuration file, use the value from the configuration file\n                if name in self.params_config.name.values:\n                    index = self.params_config.loc[self.params_config.name == name].index[0]\n                    value = self.params_config.loc[index, 'value']\n                    params.append(f'{name} = {value}')\n                # If the parameter name is not in the parameter configuration file, use the default value\n                else:\n                    params.append(f'{name} = {value}')\n            # If the parameter configuration file is not provided, use the default value\n            else:\n                params.append(f'{name} = {value}')\n        # Display the parameters\n        self.display(params, title='Parameters')\n", "CodeBleu_score": 23.098849639361013}, {"code": "def log_params(self)\n\t\n    if self.verbose == True:\n        try:\n            import param\n            import panel as pn\n            from panel.template.base import BasicTemplate\n\n            # Create a template\n            template = BasicTemplate(title=\"Parameters\")\n\n            # Create a parameters pane\n            params = pn.pane.Str(\"<h2>Model Parameters</h2>\")\n            params.append(pn.Param(self.model, show_name=False, show_labels=False))\n\n            # Create a data pane\n            data = pn.pane.Str(\"<h2>Data Parameters</h2>\")\n            data.append(pn.Param(self.data, show_name=False, show_labels=False))\n\n            # Combine the parameters and data panes into a column\n            params_and_data = pn.Column(params, data)\n\n            # Render the template\n            template.main.append(params_and_data)\n            template.servable()\n\n        except ImportError:\n            print(\"Please install the param and panel libraries to use this feature.\")\n            pass\n\n", "CodeBleu_score": 24.03188539493334}, {"code": "def log_params(self)\n\t\n    if self.log_params:\n        if self.model_params:\n            model_params = self.model_params\n        else:\n            model_params = self.model.get_params()\n        if self.data_params:\n            data_params = self.data_params\n        else:\n            data_params = self.data.get_params()\n        param_string = \"\"\n        for key, value in model_params.items():\n            param_string += f\"{key} = {value}", "CodeBleu_score": 14.117179787818248}]}
{"instance_id": "4726b7864386", "generation_result": [{"code": "def draw_projections(self, embeds, utterances_per_speaker, step, out_fpath=None, max_speakers=10)\n\t\n    if self.args.no_plot:\n        return\n    if out_fpath is not None and os.path.isfile(out_fpath):\n        return\n    # Limit the number of speakers.\n    if embeds.shape[0] > max_speakers:\n        embeds = embeds[:max_speakers]\n        utterances_per_speaker = utterances_per_speaker[:max_speakers]\n    # Flatten the embeddings.\n    embeds = embeds.reshape(embeds.shape[0], -1)\n    # Assign colors based on speaker identity.\n    colors = [self.get_color(i) for i in range(embeds.shape[0])]\n    # Project the embeddings using UMAP.\n    reducer = umap.UMAP(n_neighbors=min(50, embeds.shape[0] - 1), min_dist=0.0, metric='cosine')\n    embeds = reducer.fit_transform(embeds)\n    # Plot the embeddings.\n    plt.scatter(embeds[:, 0], embeds[:, 1], c=colors, s=10)\n    plt.gca().set_aspect('equal', 'datalim')\n    plt.title(f'Step {step} - Speakers ({embeds.shape[0]})')\n    plt.tight_layout()\n    if out_fpath is not None:\n        plt.savefig(out_fpath)\n    else:\n        plt.show()\n    plt.clf()\n", "CodeBleu_score": 48.597471213614}, {"code": "def draw_projections(self, embeds, utterances_per_speaker, step, out_fpath=None, max_speakers=10)\n\t\n    if out_fpath is not None:\n        self.out_fpath = out_fpath\n    if max_speakers is not None:\n        self.max_speakers = max_speakers\n    if self.max_speakers is not None:\n        embeds = embeds[:self.max_speakers]\n        utterances_per_speaker = utterances_per_speaker[:self.max_speakers]\n    embeds = embeds.reshape(len(embeds), -1)\n    reducer = umap.UMAP(random_state=self.seed)\n    embeds_2d = reducer.fit_transform(embeds)\n    plt.scatter(embeds_2d[:, 0], embeds_2d[:, 1], c=np.arange(len(embeds_2d)), cmap='Spectral')\n    plt.title(f'UMAP projections (step {step})')\n    if self.out_fpath is not None:\n        plt.savefig(self.out_fpath)\n    else:\n        plt.show()\n    plt.clf()\n\n", "CodeBleu_score": 40.6012406581037}, {"code": "def draw_projections(self, embeds, utterances_per_speaker, step, out_fpath=None, max_speakers=10)\n\t\n    if out_fpath is None:\n        plt.ion()\n    else:\n        plt.ioff()\n    plt.clf()\n    plt.title('Step: {}'.format(step))\n    if embeds.shape[0] > max_speakers:\n        embeds = embeds[:max_speakers]\n    embeds = embeds.reshape(-1, embeds.shape[-1])\n    embeds = umap.UMAP().fit_transform(embeds)\n    colors = np.repeat(np.arange(embeds.shape[0]), utterances_per_speaker)\n    plt.scatter(embeds[:, 0], embeds[:, 1], c=colors, cmap='Spectral', s=0.5)\n    if out_fpath is None:\n        plt.pause(0.001)\n    else:\n        plt.savefig(out_fpath)\n        plt.close()\n", "CodeBleu_score": 36.872901535827765}]}
{"instance_id": "274fa50985c0", "generation_result": [{"code": "def add_speaker_embedding(self, x, speaker_embedding)\n\t\n    # Save the dimensions of the input tensor\n    batch_size, seq_length, hidden_size = x.size()\n    # Determine the appropriate dimension index for the speaker embedding\n    if self.speaker_embedding_dim == -1:\n        speaker_embedding_dim = hidden_size\n    else:\n        speaker_embedding_dim = self.speaker_embedding_dim\n    # Repeat the speaker embedding to match the length of the input text\n    speaker_embedding = speaker_embedding.repeat_interleave(seq_length, dim=0)\n    # Reshape and transpose the speaker embedding\n    speaker_embedding = speaker_embedding.reshape(batch_size, seq_length, speaker_embedding_dim)\n    speaker_embedding = speaker_embedding.transpose(1, 2)\n    # Concatenate the input tensor and the speaker embedding\n    x = torch.cat((x, speaker_embedding), dim=2)\n    return x\n\n", "CodeBleu_score": 37.136422979185205}, {"code": "def add_speaker_embedding(self, x, speaker_embedding)\n\t\n    original_size = x.size()\n    speaker_embedding_size = speaker_embedding.size()\n    #print(original_size)\n    #print(speaker_embedding_size)\n    #print(speaker_embedding_size[0])\n    #print(speaker_embedding_size[1])\n    #print(speaker_embedding_size[2])\n    #print(speaker_embedding_size[3])\n    #print(speaker_embedding_size[4])\n    #print(speaker_embedding_size[5])\n    #print(speaker_embedding_size[6])\n    #print(speaker_embedding_size[7])\n    #print(speaker_embedding_size[8])\n    #print(speaker_embedding_size[9])\n    #print(speaker_embedding_size[10])\n    #print(speaker_embedding_size[11])\n    #print(speaker_embedding_size[12])\n    #print(speaker_embedding_size[13])\n    #print(speaker_embedding_size[14])\n    #print(speaker_embedding_size[15])\n    #print(speaker_embedding_size[16])\n    #print(speaker_embedding_size[17])\n    #print(speaker_embedding_size[18])\n    #print(speaker_embedding_size[19])\n    #print(speaker_embedding_size[20])\n    #print(speaker_embedding_size[21])\n    #print(speaker_embedding_size[22])\n    #print(speaker_embedding_size[23])\n    #print(speaker_embedding_size[24])\n    #print(speaker_embedding_size[25])\n    #print(speaker_embedding_size[26])\n    #print(speaker_embedding_size[27])\n    #print(speaker_embedding_size[28])\n    #print(speaker_embedding_size[29])\n    #print(speaker_embedding_size[30])\n    #print(speaker_embedding_size[31])\n    #print(speaker_embedding_size[32])\n    #print(speaker_embedding_size[33])\n    #print(speaker_embedding_size[34])\n    #print(speaker_embedding_size[35])\n    #print(speaker_embedding_size[36])\n   ", "CodeBleu_score": 2.811278695278329}, {"code": "def add_speaker_embedding(self, x, speaker_embedding)\n\t\n    x_dims = x.dim()\n    speaker_embedding_dim = speaker_embedding.dim()\n    speaker_embedding_dim_index = x_dims - speaker_embedding_dim\n    # repeat_interleave repeats the tensor along the given dimension\n    # x.repeat_interleave(speaker_embedding.size()[0], dim=speaker_embedding_dim_index)\n    # x.size()[0]\n    # speaker_embedding.size()[0]\n    # torch.cat concatenates the given sequence of seq tensors in the given dimension\n    # e.reshape\n    # e.transpose\n    # speaker_embedding.dim()\n    return torch.cat([x, speaker_embedding], dim=speaker_embedding_dim_index)\n\n", "CodeBleu_score": 15.194521134044066}]}
{"instance_id": "e6742ea0fe5e", "generation_result": [{"code": "def init_attention(self, encoder_seq_proj)\n\t\n    self.device = next(self.parameters()).device\n    self.encoder_seq_proj_shape = encoder_seq_proj.size()\n    self.cumulative_weights = torch.zeros(self.encoder_seq_proj_shape[0], self.encoder_seq_proj_shape[1], device=self.device)\n    self.attention_weights = torch.zeros(self.encoder_seq_proj_shape[0], self.encoder_seq_proj_shape[1], device=self.device)\n", "CodeBleu_score": 17.98803081845569}, {"code": "def init_attention(self, encoder_seq_proj)\n\t\n    device = self.parameters[0].device\n    batch_size, seq_len, _ = encoder_seq_proj.size()\n    self.cumulative_attention = torch.zeros(batch_size, seq_len, device=device)\n    self.attention = torch.zeros(batch_size, seq_len, device=device)\n", "CodeBleu_score": 23.70048814088178}, {"code": "def init_attention(self, encoder_seq_proj)\n\t\n    self.device = next(self.parameters()).device\n    self.batch_size = encoder_seq_proj.size(0)\n    self.seq_len = encoder_seq_proj.size(1)\n    self.cumulative_attention = torch.zeros(self.batch_size, self.seq_len).to(self.device)\n    self.attention = torch.zeros(self.batch_size, self.seq_len).to(self.device)\n", "CodeBleu_score": 19.51210449001881}]}
{"instance_id": "4e1fc1ed0ed7", "generation_result": [{"code": "def load(self, path, optimizer=None)\n\t\n    # Use device of model params as location for loaded state\n    device = next(self.parameters()).device\n    checkpoint = torch.load(str(path), map_location=device)\n    self.load_state_dict(checkpoint[\"model_state\"])\n\n    if \"optimizer_state\" in checkpoint and optimizer is not None:\n        optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n", "CodeBleu_score": 67.98839425729693}, {"code": "def load(self, path, optimizer=None)\n\t\n    # Use device of model params as location for loaded state\n    device = next(self.parameters()).device\n    checkpoint = torch.load(str(path), map_location=device)\n    self.load_state_dict(checkpoint[\"model_state\"])\n\n    if \"optimizer_state\" in checkpoint and optimizer is not None:\n        optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n\n", "CodeBleu_score": 67.98839425729693}, {"code": "def load(self, path, optimizer=None)\n\t\n    # Use device of model params as location for loaded state\n    device = next(self.parameters()).device\n    checkpoint = torch.load(str(path), map_location=device)\n    self.load_state_dict(checkpoint[\"model_state\"])\n\n    if \"optimizer_state\" in checkpoint and optimizer is not None:\n        optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n\n", "CodeBleu_score": 67.98839425729693}]}
{"instance_id": "178e6aa16c0f", "generation_result": [{"code": "def save(self, path, optimizer=None)\n\t\n    if optimizer is not None:\n        torch.save({\n            \"model_state\": self.state_dict(),\n            \"optimizer_state\": optimizer.state_dict(),\n        }, str(path))\n    else:\n        torch.save({\n            \"model_state\": self.state_dict(),\n        }, str(path))\n\n", "CodeBleu_score": 93.4714524643972}, {"code": "def save(self, path, optimizer=None)\n\t\n    if optimizer is not None:\n        torch.save({\n            \"model_state\": self.state_dict(),\n            \"optimizer_state\": optimizer.state_dict(),\n        }, str(path))\n    else:\n        torch.save({\n            \"model_state\": self.state_dict(),\n        }, str(path))\n\n", "CodeBleu_score": 93.4714524643972}, {"code": "def save(self, path, optimizer=None)\n\t\n    if optimizer is not None:\n        torch.save({\n            \"model_state\": self.state_dict(),\n            \"optimizer_state\": optimizer.state_dict(),\n        }, str(path))\n    else:\n        torch.save({\n            \"model_state\": self.state_dict(),\n        }, str(path))\n", "CodeBleu_score": 93.4714524643972}]}
{"instance_id": "6528fd80e87b", "generation_result": [{"code": "def english_cleaners(text)\n\t\n    text = convert_to_ascii(text)\n    text = lowercase(text)\n    text = expand_numbers(text)\n    text = expand_abbreviations(text)\n    text = collapse_whitespace(text)\n    return text\n", "CodeBleu_score": 55.88611500274403}, {"code": "def english_cleaners(text)\n\t\n    text = convert_to_ascii(text)\n    text = lowercase(text)\n    text = expand_numbers(text)\n    text = expand_abbreviations(text)\n    text = collapse_whitespace(text)\n    return text\n", "CodeBleu_score": 55.88611500274403}, {"code": "def english_cleaners(text)\n\t\n    text = convert_to_ascii(text)\n    text = lowercase(text)\n    text = expand_numbers(text)\n    text = expand_abbreviations(text)\n    text = collapse_whitespace(text)\n    return text\n", "CodeBleu_score": 55.88611500274403}]}
{"instance_id": "8a4fe08e0985", "generation_result": [{"code": "def text_to_sequence(text, cleaner_names)\n\t\n    sequence = []\n    # Check for curly braces and treat their contents as ARPAbet:\n    while len(text):\n        m = _curly_re.match(text)\n        if not m:\n            sequence += _symbols_to_sequence(_clean_text(text, cleaner_names))\n            break\n        sequence += _symbols_to_sequence(_clean_text(m.group(1), cleaner_names))\n        sequence += _arpabet_to_sequence(m.group(2))\n        text = m.group(3)\n    # Append EOS token\n    sequence.append(_symbol_to_id[\"~\"])\n    return sequence\n\n", "CodeBleu_score": 82.3821513994484}, {"code": "def text_to_sequence(text, cleaner_names)\n\t\n    sequence = []\n    # Check for curly braces and treat their contents as ARPAbet:\n    while len(text):\n        m = _curly_re.match(text)\n        if not m:\n            sequence += _symbols_to_sequence(_clean_text(text, cleaner_names))\n            break\n        sequence += _symbols_to_sequence(_clean_text(m.group(1), cleaner_names))\n        sequence += _arpabet_to_sequence(m.group(2))\n        text = m.group(3)\n    # Append EOS token\n    sequence.append(_symbol_to_id[\"~\"])\n    return sequence\n", "CodeBleu_score": 82.3821513994484}, {"code": "def text_to_sequence(text, cleaner_names)\n\t\n    sequence = []\n    # Check for curly braces and treat their contents as ARPAbet:\n    while len(text):\n        m = _curly_re.match(text)\n        if not m:\n            sequence += _symbols_to_sequence(_clean_text(text, cleaner_names))\n            break\n        sequence += _symbols_to_sequence(_clean_text(m.group(1), cleaner_names))\n        sequence += _arpabet_to_sequence(m.group(2))\n        text = m.group(3)\n    # Append EOS token\n    sequence.append(_symbol_to_id[\"~\"])\n    return sequence\n\n", "CodeBleu_score": 82.3821513994484}]}
{"instance_id": "e1c25a37af1f", "generation_result": [{"code": "def sequence_to_text(sequence)\n\t\n    result = ''\n    for symbol_id in sequence:\n        if symbol_id in _id_to_symbol:\n            symbol = _id_to_symbol[symbol_id]\n            # Enclose ARPAbet back in curly braces:\n            if len(symbol) > 1 and symbol[0] == '@':\n                symbol = '{%s}' % symbol[1:]\n            # Replace curly braces after the first with a space, so that\n            # \"hello {world}\" becomes \"hello {world}\" and not \"hello { world}\"\n            if len(result) > 0 and result[-1] == '{':\n                symbol = ' ' + symbol\n            result += symbol\n    return result.replace('}{', ' ')\n\n", "CodeBleu_score": 51.20061202808481}, {"code": "def sequence_to_text(sequence)\n\t\n    result = ''\n    for symbol_id in sequence:\n        if symbol_id in _id_to_symbol:\n            s = _id_to_symbol[symbol_id]\n            # Enclose ARPAbet back in curly braces:\n            if len(s) > 1 and s[0] == '@':\n                s = '{%s}' % s[1:]\n            result += s\n    return result.replace('}{', ' ')\n", "CodeBleu_score": 70.3688557962299}, {"code": "def sequence_to_text(sequence)\n\t\n    result = ''\n    for symbol_id in sequence:\n        if symbol_id in _id_to_symbol:\n            symbol = _id_to_symbol[symbol_id]\n            # Enclose ARPAbet back in curly braces:\n            if len(symbol) > 1 and symbol[0] == '@':\n                symbol = '{%s}' % symbol[1:]\n            # Concatenate the symbols:\n            result += symbol\n    return result.replace('}{', ' ')\n", "CodeBleu_score": 58.50143137937177}]}
{"instance_id": "7807cad4b05d", "generation_result": [{"code": "def embed_utterance(fpaths, encoder_model_fpath)\n\t\n    if not encoder.is_loaded():\n        encoder.load_model(encoder_model_fpath)\n\n    # Compute the speaker embedding of the utterance\n    wav_fpath, embed_fpath = fpaths\n    wav = np.load(wav_fpath)\n    wav = encoder.preprocess_wav(wav)\n    embed = encoder.embed_utterance(wav)\n    np.save(embed_fpath, embed, allow_pickle=False)\n\n", "CodeBleu_score": 95.3261531719067}, {"code": "def embed_utterance(fpaths, encoder_model_fpath)\n\t\n    if not encoder.is_loaded():\n        encoder.load_model(encoder_model_fpath)\n\n    # Compute the speaker embedding of the utterance\n    wav_fpath, embed_fpath = fpaths\n    wav = np.load(wav_fpath)\n    wav = encoder.preprocess_wav(wav)\n    embed = encoder.embed_utterance(wav)\n    np.save(embed_fpath, embed, allow_pickle=False)\n\n", "CodeBleu_score": 95.3261531719067}, {"code": "def embed_utterance(fpaths, encoder_model_fpath)\n\t\n    if not encoder.is_loaded():\n        encoder.load_model(encoder_model_fpath)\n\n    # Compute the speaker embedding of the utterance\n    wav_fpath, embed_fpath = fpaths\n    wav = np.load(wav_fpath)\n    wav = encoder.preprocess_wav(wav)\n    embed = encoder.embed_utterance(wav)\n    np.save(embed_fpath, embed, allow_pickle=False)\n\n", "CodeBleu_score": 95.3261531719067}]}
{"instance_id": "9f860273a325", "generation_result": [{"code": "def create_embeddings(synthesizer_root: Path, encoder_model_fpath: Path, n_processes: int)\n\t\n    # Create output directories\n    embed_dir = synthesizer_root.joinpath(\"embeds\")\n    wav_dir = synthesizer_root.joinpath(\"audio\")\n    embed_dir.mkdir(exist_ok=True)\n\n    # Load encoder\n    encoder = Encoder(encoder_model_fpath)\n\n    # Gather input files\n    metadata_fpath = synthesizer_root.joinpath(\"train.txt\")\n    with metadata_fpath.open(\"r\", encoding=\"utf-8\") as metadata_file:\n        metadata = [line.split(\"|\") for line in metadata_file]\n\n    # Gather the input wave filepath and the target output embed filepath\n    wav_fpaths = [wav_dir.joinpath(m[0] + \".wav\") for m in metadata]\n    embed_fpaths = [embed_dir.joinpath(m[0] + \".npy\") for m in metadata]\n\n    # Create a pool of workers to process the audio\n    # This will speed up the embedding creation process by a lot\n    pool = Pool(n_processes)\n    func = partial(_process_utterance, encoder=encoder)\n    list(\n        tqdm(\n            pool.imap(func, zip(wav_fpaths, embed_fpaths)),\n            total=len(wav_fpaths),\n            desc=\"Creating embeddings\",\n        )\n    )\n\n", "CodeBleu_score": 58.322148556322}, {"code": "def create_embeddings(synthesizer_root: Path, encoder_model_fpath: Path, n_processes: int)\n\t\n    # Verify that the encoder model file exists\n    if not encoder_model_fpath.exists():\n        raise RuntimeError(\"Encoder model file does not exist {}\".format(encoder_model_fpath))\n\n    # Make sure that synthesizer_root is a Path object\n    synthesizer_root = Path(synthesizer_root)\n\n    # Gather the encoder model file's directory\n    encoder_model_dir = encoder_model_fpath.parent\n\n    # Gather the metadata file's path\n    metadata_fpath = synthesizer_root.joinpath(\"train.txt\")\n\n    # Gather the input audio directory\n    wav_dir = synthesizer_root.joinpath(\"audio\")\n\n    # Gather the output embedding directory\n    embed_dir = synthesizer_root.joinpath(\"embeds\")\n\n    # Create the output directory if it doesn't exist\n    if not embed_dir.exists():\n        os.makedirs(embed_dir)\n\n    # Read the metadata file\n    with metadata_fpath.open(\"r\", encoding=\"utf-8\") as metadata_file:\n        metadata = [line.split(\"|\") for line in metadata_file]\n\n    # Create a list of the wav files\n    wav_fpaths = [wav_dir.joinpath(m[0] + \".wav\") for m in metadata]\n\n    # Create a list of the embedding files\n    embed_fpaths = [embed_dir.joinpath(m[0] + \".npy\") for m in metadata]\n\n    # Create a list of the utterance texts\n    texts = [m[2] for m in metadata]\n\n    # Create a list of the speaker names\n    n_frames = [int(m[3]) for m in metadata]\n\n    # Create a list of the mel spectrogram files\n    mel_fpaths = [embed_dir.joinpath(m[0] + \".npy\") for m in metadata]\n\n    # Create a list of the linear spectrogram files\n    linear_fpaths = [embed_dir.joinpath(m[0] + \".npy\") for m in metadata]\n\n    # Create a list of the stop token files\n    stop_fpaths = [embed_dir.joinpath(m[0] + \".npy\") for m in metadata]\n\n    # Create a list of the wav lengths\n    wav_lens = [int(m[4]) for m in metadata]\n\n    # Create a list of the", "CodeBleu_score": 50.054166192176}, {"code": "def create_embeddings(synthesizer_root: Path, encoder_model_fpath: Path, n_processes: int)\n\t\n    # Verify that the encoder model file exists.\n    if not encoder_model_fpath.exists():\n        raise RuntimeError(\"Path {} does not exist\".format(str(encoder_model_fpath)))\n\n    # Create paths to metadata and embedding output directories.\n    metadata_fpath = synthesizer_root.joinpath(\"train.txt\")\n    embed_dir = synthesizer_root.joinpath(\"embeds\")\n    wav_dir = synthesizer_root.joinpath(\"audio\")\n\n    # Create the metadata directory if it doesn't exist.\n    if not metadata_fpath.exists():\n        raise RuntimeError(\"Metadata file {} does not exist\".format(str(metadata_fpath)))\n\n    # Create the embedding directory if it doesn't exist.\n    if not embed_dir.exists():\n        os.makedirs(embed_dir)\n\n    # Get the list of input wav files.\n    with metadata_fpath.open(\"r\", encoding=\"utf-8\") as metadata_file:\n        metadata = [line.split(\"|\") for line in metadata_file]\n\n    # Get the input wav file paths.\n    wav_fpaths = [wav_dir.joinpath(x[0] + \".wav\") for x in metadata]\n\n    # Create a partial function for processing audio files.\n    partial_create_embeddings = partial(create_embedding, encoder_model_fpath=encoder_model_fpath)\n\n    # Create a pool of worker processes.\n    pool = Pool(processes=n_processes)\n\n    # Process the files.\n    for i, _ in tqdm(enumerate(pool.imap_unordered(partial_create_embeddings, wav_fpaths), 1), total=len(wav_fpaths)):\n        pass\n\n    # Close the pool.\n    pool.close()\n    pool.join()\n\n", "CodeBleu_score": 51.66321229485489}]}
{"instance_id": "9eb0abda14a9", "generation_result": [{"code": "def eval_model(attention, mel_prediction, target_spectrogram, input_seq, step,\n               plot_dir, mel_output_dir, wav_dir, sample_num, loss, hparams)\n\t\n    # Save attention plot\n    save_attention(attention, plot_dir.joinpath(f\"attention_{step}\"))\n\n    # Save predicted mel spectrogram to disk\n    plot_spectrogram(\n        mel_prediction,\n        plot_dir.joinpath(f\"mel_prediction_{step}.png\"),\n        title=f\"Step {step}: Predicted Spectrogram\",\n        split_title=True,\n        target_spectrogram=target_spectrogram,\n        max_len=target_spectrogram.size // hparams.outputs_per_step,\n    )\n\n    # Save real and predicted mel-spectrogram plot to disk\n    plot_spectrogram(\n        mel_prediction,\n        plot_dir.joinpath(f\"mel_prediction_vs_target_{step}.png\"),\n        title=f\"Step {step}: Predicted vs Target Mel-Spectrogram\",\n        split_title=True,\n        target_spectrogram=target_spectrogram,\n        auto_aspect=True,\n    )\n\n    # Save real and predicted mel-spectrogram plot to disk\n    plot_spectrogram(\n        target_spectrogram,\n        plot_dir.joinpath(f\"mel_target_{step}.png\"),\n        title=f\"Step {step}: Target Mel-Spectrogram\",\n        split_title=True,\n        auto_aspect=True,\n    )\n\n    # Save real and predicted mel-spectrogram plot to disk\n    plot_spectrogram(\n        mel_prediction,\n        plot_dir.joinpath(f\"mel_prediction_target_diff_{step}.png\"),\n        title=f\"Step {step}: Predicted vs Target Mel-Spectrogram\",\n        split_title=True,\n        target_spectrogram=target_spectrogram,\n        auto_aspect=True,\n    )\n\n    # Save predicted wav to disk\n    audio.save_wav(\n        wav_dir.joinpath(f\"wav_{step}.wav\"),\n        audio.inv_mel_spectrogram(mel_prediction.T, hparams),\n        hparams.sample_rate,\n    )\n\n    # Save real and predicted mel-spectrogram plot to disk\n    plot_spectrogram(\n        mel_prediction,\n        plot_dir.joinpath(f\"mel_prediction_{step}.png\"),\n        title=f\"Step {step}: Predicted Spectrogram\",\n        split_title=True,\n        target_spectrogram=target_spectrogram,\n        max_len", "CodeBleu_score": 23.489611606932947}, {"code": "def eval_model(attention, mel_prediction, target_spectrogram, input_seq, step,\n               plot_dir, mel_output_dir, wav_dir, sample_num, loss, hparams)\n\t\n    # Save attention plot\n    save_attention(attention, plot_dir.joinpath(f\"{step}.png\"))\n\n    # Save predicted mel spectrogram to disk (for comparison)\n    plot_spectrogram(\n        mel_prediction.data.cpu().numpy()[0],\n        mel_output_dir.joinpath(f\"{step}.png\"),\n        title=f\"predicted mel spectrogram at step {step}\",\n        split_title=True,\n        target_spectrogram=target_spectrogram.data.cpu().numpy()[0],\n        max_len=target_spectrogram.size(1),\n        auto_aspect=True\n    )\n\n    # Save real and predicted mel-spectrogram plot to disk\n    plot_spectrogram(\n        mel_prediction.data.cpu().numpy()[0],\n        plot_dir.joinpath(f\"{step}_mel_target_predicted.png\"),\n        title=f\"step {step} mel-spectrogram prediction vs target\",\n        split_title=True,\n        target_spectrogram=target_spectrogram.data.cpu().numpy()[0],\n        max_len=target_spectrogram.size(1),\n        auto_aspect=True\n    )\n\n    # Save real and predicted mel-spectrogram plot to disk\n    plot_spectrogram(\n        mel_prediction.data.cpu().numpy()[0],\n        plot_dir.joinpath(f\"{step}_mel_predicted.png\"),\n        title=f\"step {step} predicted mel-spectrogram\",\n        split_title=True,\n        target_spectrogram=None,\n        max_len=target_spectrogram.size(1),\n        auto_aspect=True\n    )\n\n    # Save real and predicted mel-spectrogram plot to disk\n    plot_spectrogram(\n        target_spectrogram.data.cpu().numpy()[0],\n        plot_dir.joinpath(f\"{step}_mel_target.png\"),\n        title=f\"step {step} target mel-spectrogram\",\n        split_title=True,\n        target_spectrogram=None,\n        max_len=target_spectrogram.size(1),\n        auto_aspect=True\n    )\n\n    # Save predicted audio (linear spectrogram) to disk\n    wav_prediction = audio.inv_mel_spectrogram(mel_prediction.data.cpu().numpy()[0].T, hparams)\n    audio.save_wav(wav_prediction, wav", "CodeBleu_score": 29.861391804823224}, {"code": "def eval_model(attention, mel_prediction, target_spectrogram, input_seq, step,\n               plot_dir, mel_output_dir, wav_dir, sample_num, loss, hparams)\n\t\n    # Save attention plot\n    save_attention(attention[0].data.cpu().numpy(),\n                   path=plot_dir.joinpath(f\"{step}.png\"))\n\n    # Save predicted mel spectrogram to disk (plot and save to disk)\n    plot_spectrogram(\n        mel_prediction[0].data.cpu().numpy(),\n        path=plot_dir.joinpath(f\"{step}_predicted_mel_spectrogram.png\"),\n        title=f\"Predicted Mel-Spectrogram at step {step}\",\n        split_title=True,\n        target_spectrogram=target_spectrogram[0].data.cpu().numpy(),\n        auto_aspect=True)\n\n    # Save predicted wav to disk\n    wav = inv_mel_spectrogram(mel_prediction.data.cpu().numpy()[0], hparams)\n    audio.save_wav(wav, wav_dir.joinpath(f\"{step}_predicted.wav\"), sr=hparams.sample_rate)\n\n    # Save target mel spectrogram to disk (plot and save to disk)\n    plot_spectrogram(\n        target_spectrogram[0].data.cpu().numpy(),\n        path=plot_dir.joinpath(f\"{step}_target_mel_spectrogram.png\"),\n        title=f\"Target Mel-Spectrogram at step {step}\",\n        split_title=True,\n        auto_aspect=True)\n\n    # Save target wav to disk\n    wav = inv_mel_spectrogram(target_spectrogram.data.cpu().numpy()[0], hparams)\n    audio.save_wav(wav, wav_dir.joinpath(f\"{step}_target.wav\"), sr=hparams.sample_rate)\n\n    # Save plot of predicted vs target mel-spectrograms\n    plot_spectrogram(\n        pred_spectrogram=mel_prediction[0].data.cpu().numpy(),\n        target_spectrogram=target_spectrogram[0].data.cpu().numpy(),\n        path=plot_dir.joinpath(f\"{step}_predicted_vs_target_mel_spectrograms.png\"),\n        title=f\"Predicted vs Target Mel-Spectrograms at step {step}\",\n        split_title=True,\n        auto_aspect=True)\n\n    # Save plot of predicted vs target waveforms\n    plot_waveform(\n        pred_waveform=wav,\n        target_waveform=audio.load", "CodeBleu_score": 28.607862953532827}]}
{"instance_id": "6bad4f500286", "generation_result": [{"code": "def num_params(self)\n\t\n    params = filter(lambda p: p.requires_grad, self.parameters())\n    num = sum(map(lambda p: p.numel(), params))\n    print(f'The model has {num / 10 ** 6:.3f} million parameters.')\n", "CodeBleu_score": 26.77740292372318}, {"code": "def num_params(self)\n\t\n    params = filter(lambda p: p.requires_grad, self.parameters())\n    num = sum(map(lambda p: np.prod(p.size()), params))\n    print(num / 1e6)\n", "CodeBleu_score": 25.70847871555475}, {"code": "def num_params(self)\n\t\n    p = filter(lambda p: p.requires_grad, self.parameters())\n    num_params = sum(np.prod(p.size()) for p in p)\n    print('Trainable parameters: %.3fM' % (num_params / 1e6))\n\n", "CodeBleu_score": 33.55851944254826}]}
{"instance_id": "78b876054bd4", "generation_result": [{"code": "def pad_tensor(self, x, pad, side='both')\n\t\n    if side == 'both':\n        pad_left = pad_right = pad\n    elif side == 'before':\n        pad_left = pad_right = 0\n        pad = pad - x.size(0)\n    elif side == 'after':\n        pad_left = 0\n        pad_right = pad\n        pad = 0\n    else:\n        raise ValueError(\"The side argument must be either 'before', 'after', or 'both'.\")\n    new_size = list(x.size())\n    new_size[0] = pad\n    new_tensor = torch.zeros(new_size)\n    if x.is_cuda:\n        new_tensor = new_tensor.cuda()\n    if side == 'both':\n        new_tensor[:x.size(0)] = x\n        return new_tensor\n    elif side == 'before':\n        new_tensor[pad_left:] = x\n        return new_tensor\n    elif side == 'after':\n        new_tensor[:x.size(0)] = x\n        return new_tensor\n", "CodeBleu_score": 25.897453549079536}, {"code": "def pad_tensor(self, x, pad, side='both')\n\t\n    if side == 'both':\n        x = F.pad(x, (pad, pad), 'constant', 0)\n        return torch.cat((x, x), dim=2)\n    elif side == 'before':\n        x = F.pad(x, (pad, 0), 'constant', 0)\n        return torch.cat((x, x[:, :, -pad:]), dim=2)\n    elif side == 'after':\n        x = F.pad(x, (0, pad), 'constant', 0)\n        return torch.cat((x[:, :, :pad], x), dim=2)\n\n", "CodeBleu_score": 29.3244402947753}, {"code": "def pad_tensor(self, x, pad, side='both')\n\t\n    if side == 'both':\n        pad = (pad, pad)\n    elif side == 'before':\n        pad = (pad, 0)\n    elif side == 'after':\n        pad = (0, pad)\n    else:\n        raise ValueError(\"side must be one of 'before', 'after', or 'both'\")\n    new_size = (x.size(0) + pad[0], x.size(1) + pad[1])\n    padded = torch.zeros(new_size)\n    if x.is_cuda:\n        padded = padded.cuda()\n    padded[:, :x.size(1)] = x\n    return padded\n", "CodeBleu_score": 34.42127201978286}]}
{"instance_id": "a7ed1715a3df", "generation_result": [{"code": "def save_attention(attn, path)\n\t\n    import matplotlib.pyplot as plt\n    fig = plt.figure(figsize=(10, 10))\n    ax = fig.add_subplot(1, 1, 1)\n    ax.matshow(attn, cmap='viridis')\n\n    fontdict = {'fontsize': 14}\n\n    ax.set_xticklabels([''] + ['<sos>'] + [t.lower() for t in sentence] + ['<eos>'], fontdict=fontdict, rotation=90)\n    ax.set_yticklabels([''] + [t.lower() for t in translation] + ['<eos>'], fontdict=fontdict)\n\n    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n\n    plt.show()\n    fig.savefig(path, bbox_inches='tight')\n    plt.close()\n", "CodeBleu_score": 45.374262786485716}, {"code": "def save_attention(attn, path)\n\t\n    import matplotlib.pyplot as plt\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    cax = ax.matshow(attn, cmap='bone')\n    fig.colorbar(cax)\n    ax.set_xticklabels([''] + ['<sos>'] + [''] + ['<eos>'] + [''], rotation=90)\n    ax.set_yticklabels([''] + ['<sos>'] + [''] + ['<eos>'] + [''])\n    plt.show()\n    fig.savefig(path + '.png')\n    plt.close()\n    return\n", "CodeBleu_score": 36.14120647814364}, {"code": "def save_attention(attn, path)\n\t\n    import matplotlib.pyplot as plt\n    fig = plt.figure(figsize=(12, 12))\n    ax = fig.add_subplot(1, 1, 1)\n    ax.matshow(attn, cmap='viridis')\n    fontdict = {'fontsize': 14}\n    ax.set_xticklabels([''] + ['<sos>'] + [t.lower() for t in attn_tokens] + ['<eos>'], fontdict=fontdict, rotation=90)\n    ax.set_yticklabels([''] + attn_tokens + ['<eos>'], fontdict=fontdict)\n    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n    ax.set_xlabel('Heads', fontdict=fontdict)\n    ax.set_ylabel('Queries', fontdict=fontdict)\n    plt.savefig(path)\n    plt.close()\n    return fig\n\n", "CodeBleu_score": 45.88505585776967}]}
{"instance_id": "19375c9f44a2", "generation_result": [{"code": "def plot(array)\n\t\n    import matplotlib.pyplot as plt\n\n    fig = plt.figure(figsize=(30, 5))\n    ax = fig.add_subplot(111)\n    ax.xaxis.label.set_color('grey')\n    ax.yaxis.label.set_color('grey')\n    ax.xaxis.label.set_fontsize(23)\n    ax.yaxis.label.set_fontsize(23)\n    ax.tick_params(axis='x', colors='grey', labelsize=23)\n    ax.tick_params(axis='y', colors='grey', labelsize=23)\n    plt.plot(array)\n\n", "CodeBleu_score": 77.73415655580472}, {"code": "def plot(array)\n\t\n    import matplotlib.pyplot as plt\n\n    fig = plt.figure(figsize=(30, 5))\n    ax = fig.add_subplot(111)\n    ax.xaxis.label.set_color('grey')\n    ax.yaxis.label.set_color('grey')\n    ax.xaxis.label.set_fontsize(23)\n    ax.yaxis.label.set_fontsize(23)\n    ax.tick_params(axis='x', colors='grey', labelsize=23)\n    ax.tick_params(axis='y', colors='grey', labelsize=23)\n    plt.plot(array)\n\n", "CodeBleu_score": 77.73415655580472}, {"code": "def plot(array)\n\t\n    import matplotlib.pyplot as plt\n\n    fig = plt.figure(figsize=(30, 5))\n    ax = fig.add_subplot(111)\n    ax.xaxis.label.set_color('grey')\n    ax.yaxis.label.set_color('grey')\n    ax.xaxis.label.set_fontsize(23)\n    ax.yaxis.label.set_fontsize(23)\n    ax.tick_params(axis='x', colors='grey', labelsize=23)\n    ax.tick_params(axis='y', colors='grey', labelsize=23)\n    plt.plot(array)\n\n", "CodeBleu_score": 77.73415655580472}]}
{"instance_id": "ff94c425920c", "generation_result": [{"code": "def write_to_csv(image_name, prediction, confidence)\n\t\n    data = {\n        \"image\": image_name,\n        \"prediction\": prediction,\n        \"confidence\": confidence,\n    }\n    csv_path = Path(\"predictions.csv\")\n    if csv_path.is_file():\n        with open(\"predictions.csv\", \"a\") as csv_file:\n            writer = csv.DictWriter(csv_file, fieldnames=data.keys())\n            if csv_file.tell() == 0:\n                writer.writeheader()\n            writer.writerow(data)\n    else:\n        with open(\"predictions.csv\", \"w\") as csv_file:\n            writer = csv.DictWriter(csv_file, fieldnames=data.keys())\n            writer.writeheader()\n            writer.writerow(data)\n\n", "CodeBleu_score": 48.99992391925512}, {"code": "def write_to_csv(image_name, prediction, confidence)\n\t\n    data = {'image': image_name, 'prediction': prediction, 'confidence': confidence}\n    csv_path = Path('predictions.csv')\n    if not csv_path.is_file():\n        csv_file = open('predictions.csv', 'w', newline='')\n        fieldnames = ['image', 'prediction', 'confidence']\n        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n        writer.writeheader()\n    else:\n        csv_file = open('predictions.csv', 'a', newline='')\n        writer = csv.writer(csv_file)\n    writer.writerow(data.values())\n    csv_file.close()\n", "CodeBleu_score": 44.54140484405629}, {"code": "def write_to_csv(image_name, prediction, confidence)\n\t\n    data = {\n        \"image\": image_name,\n        \"prediction\": prediction,\n        \"confidence\": confidence\n    }\n    csv_path = Path(\"predictions.csv\")\n    if csv_path.is_file():\n        with open(csv_path, \"a\") as csv_file:\n            writer = csv.DictWriter(csv_file, fieldnames=data.keys())\n            if csv_file.tell() == 0:\n                writer.writeheader()\n            writer.writerow(data)\n    else:\n        with open(csv_path, \"w\") as csv_file:\n            writer = csv.DictWriter(csv_file, fieldnames=data.keys())\n            writer.writeheader()\n            writer.writerow(data)\n", "CodeBleu_score": 45.18062414697025}]}
{"instance_id": "436161602009", "generation_result": [{"code": "def export_torchscript(model, im, file, optimize, prefix=colorstr(\"TorchScript:\"))\n\t\n    start_time = time.time()\n    LOGGER.info(\"%s Exporting model... \", prefix)\n    f = file.with_suffix(\".torchscript.pt\")\n    ts = torch.jit.trace(model, im)\n    if optimize:\n        ts = ts.optimize_for_mobile()\n    ts.save(f)\n    LOGGER.info(\n        \"%s Exported PyTorch model in %.2f seconds.\",\n        prefix,\n        (time.time() - start_time),\n    )\n    return f\n\n", "CodeBleu_score": 29.79210127996348}, {"code": "def export_torchscript(model, im, file, optimize, prefix=colorstr(\"TorchScript:\"))\n\t\n    LOGGER.info(f\"{prefix} starting export with torch {torch.__version__}...\")\n    f = file.with_suffix(\".torchscript.pt\")\n    ts = torch.jit.trace(model, im, strict=False)\n    if optimize:\n        LOGGER.info(f\"{prefix} optimizing for mobile...\")\n        ts = torch.jit.optimize_for_mobile(ts)\n    LOGGER.info(f\"{prefix} saving {f}...\")\n    ts.save(f)\n    LOGGER.info(f\"{prefix} done.\")\n    return f, None\n\n", "CodeBleu_score": 44.66453105007611}, {"code": "def export_torchscript(model, im, file, optimize, prefix=colorstr(\"TorchScript:\"))\n\t\n    LOGGER.info(f\"{prefix} starting export with torch {torch.__version__}...\")\n    f = file.with_suffix(\".torchscript.pt\")\n    LOGGER.info(f\"{prefix} tracing model...\")\n    ts = torch.jit.trace(model, im, strict=False)\n    if optimize:\n        LOGGER.info(f\"{prefix} optimizing model...\")\n        ts = optimize_for_mobile(ts)\n    LOGGER.info(f\"{prefix} saving model...\")\n    ts.save(f)\n    LOGGER.info(f\"{prefix} saved model as {f}\")\n    return f, None\n\n", "CodeBleu_score": 46.38253153392519}]}
{"instance_id": "78335487ac1c", "generation_result": [{"code": "def export_coreml(model, im, file, int8, half, nms, prefix=colorstr(\"CoreML:\")):\n\t\n    check_requirements(exclude=[\"coremltools\"])\n    LOGGER.info(f\"{prefix} starting export with torch {torch.__version__}...\")\n    suffix = \".mlmodel\"\n    if file.suffix != suffix:\n        file = file.with_suffix(suffix)\n    if int8:\n        LOGGER.info(f\"{prefix} starting quantization...\")\n        model = quantize_weights(model)\n        model = quantize_model(model)\n    if half:\n        LOGGER.info(f\"{prefix} converting to half precision...\")\n        model = model.half()\n    if nms:\n        LOGGER.info(f\"{prefix} wrapping with custom NMS...\")\n        model = iOSModel(model)\n    LOGGER.info(f\"{prefix} tracing the model...\")\n    traced_model = torch.jit.trace(model, im)\n    LOGGER.info(f\"{prefix} converting the model...\")\n    ct_model = ct.convert(traced_model, inputs=[ct.ImageType(name=\"images\", shape=im.shape)])\n    LOGGER.info(f\"{prefix} saving the model...\")\n    ct_model.save(file)\n    return file, ct_model\n\n", "CodeBleu_score": 37.837003966097505}, {"code": "def export_coreml(model, im, file, int8, half, nms, prefix=colorstr(\"CoreML:\")):\n\t\n    check_requirements(exclude=[\"torch2trt\"])\n    LOGGER.info(f\"{prefix} starting export with torch {torch.__version__}...\")\n    if half:\n        model.half()\n    if int8:\n        model.eval().cpu()\n        model = torch.quantization.quantize_dynamic(model, {torch.nn.Conv2d}, dtype=torch.qint8)\n    else:\n        model.eval().cpu()\n    if nms:\n        model = iOSModel(model)\n    traced_model = torch.jit.trace(model, im)\n    mlmodel = ct.convert(traced_model, inputs=[ct.ImageType(name=\"images\", shape=im.shape)])\n    file = Path(file).with_suffix(\".mlmodel\")\n    mlmodel.save(file)\n    LOGGER.info(f\"{prefix} export success, saved as {file}\")\n    return file, mlmodel\n\n", "CodeBleu_score": 37.240633695612445}, {"code": "def export_coreml(model, im, file, int8, half, nms, prefix=colorstr(\"CoreML:\")):\n\t\n    check_requirements([\"coremltools\"])\n    LOGGER.info(f\"{prefix} starting export with torch {torch.__version__}...\")\n    suffix = \".mlmodel\"\n    file = Path(file)\n    if not file.suffix == suffix:\n        file = file.with_suffix(suffix)\n    if nms:\n        model = iOSModel(model)\n    traced_model = torch.jit.trace(model, im)\n    if int8:\n        LOGGER.info(f\"{prefix} starting quantization...\")\n        try:\n            quantized_model = torch.quantization.quantize_dynamic(\n                traced_model, {torch.nn.Conv2d, torch.nn.Linear}, dtype=torch.qint8\n            )\n            if half:\n                LOGGER.info(f\"{prefix} converting to half precision...\")\n                quantized_model = quantized_model.half()\n        except AttributeError:\n            LOGGER.error(\n                f\"{prefix} quantization is only available in PyTorch 1.8 or newer\"\n            )\n            sys.exit(1)\n    else:\n        quantized_model = traced_model\n    try:\n        coreml_model = ct.convert(quantized_model, inputs=[ct.ImageType(name=\"image\", shape=im.shape)])\n    except RuntimeError as e:\n        LOGGER.error(f\"{prefix} {e}\")\n        sys.exit(1)\n    LOGGER.info(f\"{prefix} saving {file}...\")\n    coreml_model.save(file)\n    LOGGER.info(f\"{prefix} export complete.\")\n    return file, coreml_model\n\n", "CodeBleu_score": 42.620027420864595}]}
{"instance_id": "80233cf71580", "generation_result": [{"code": "def generate_individual(input_ranges, individual_length)\n\t\n    individual = []\n    for i in range(individual_length):\n        individual.append(random.uniform(input_ranges[i][0], input_ranges[i][1]))\n    return individual\n", "CodeBleu_score": 47.630972577458174}, {"code": "def generate_individual(input_ranges, individual_length)\n\t\n    individual = []\n    for i in range(individual_length):\n        individual.append(random.uniform(input_ranges[i][0], input_ranges[i][1]))\n    return individual\n", "CodeBleu_score": 47.630972577458174}, {"code": "def generate_individual(input_ranges, individual_length)\n\t\n    individual = []\n    for i in range(individual_length):\n        individual.append(random.uniform(input_ranges[i][0], input_ranges[i][1]))\n    return individual\n", "CodeBleu_score": 47.630972577458174}]}
{"instance_id": "5ea979cb4a8e", "generation_result": [{"code": "def marginal_log_mean_coeff(self, t)\n\t\n    if self.schedule_type == \"discrete\":\n        return self.log_alpha_fn(t)\n    elif self.schedule_type == \"linear\":\n        return self.log_alpha_fn(t) + 0.5 * (self.log_alpha_fn(t + 1) - self.log_alpha_fn(t - 1))\n    elif self.schedule_type == \"cosine\":\n        return self.log_alpha_fn(t) + 0.5 * (self.log_alpha_fn(t + 1) - self.log_alpha_fn(t - 1))\n\n", "CodeBleu_score": 29.274176742915774}, {"code": "def marginal_log_mean_coeff(self, t)\n\t\n    if self.schedule_type == \"discrete\":\n        # Use a quadratic expression for the discrete schedule\n        return self.log_alpha_fn(t)\n    elif self.schedule_type == \"linear\":\n        # Use a linear expression for the linear schedule\n        return self.log_alpha_fn(t)\n    elif self.schedule_type == \"cosine\":\n        # Apply a cosine-based formula for the cosine schedule\n        return self.log_alpha_fn(t)\n    else:\n        raise ValueError(\"Invalid schedule type.\")\n\n", "CodeBleu_score": 22.704829441051956}, {"code": "def marginal_log_mean_coeff(self, t)\n\t\n    if self.schedule_type == \"discrete\":\n        return self.log_alpha_fn(t)\n    elif self.schedule_type == \"linear\":\n        return self.log_alpha_fn(t) + 0.5 * math.log(2)\n    elif self.schedule_type == \"cosine\":\n        return self.log_alpha_fn(t) + math.log(0.5)\n    else:\n        raise ValueError(\n            \"schedule_type must be one of 'discrete', 'linear', or 'cosine'.\"\n        )\n\n", "CodeBleu_score": 28.279276048832607}]}
{"instance_id": "7da4fae5a278", "generation_result": [{"code": "def inverse_lambda(self, lamb)\n\t\n    if self.schedule == \"linear\":\n        t = (torch.log(lamb) - self.log_beta_min) / self.log_beta_diff\n        t = torch.clamp(t, 0.0, 1.0)\n        t = t * self.T\n    elif self.schedule == \"discrete\":\n        xp = torch.arange(self.T, device=lamb.device).unsqueeze(0)\n        yp = self.log_alpha\n        t = interpolate_fn(lamb, xp, yp)\n    else:\n        t = torch.acos(torch.exp(lamb)) / math.pi * self.T\n    return t\n\n", "CodeBleu_score": 23.32587946082831}, {"code": "def inverse_lambda(self, lamb)\n\t\n    if self.schedule == \"linear\":\n        beta = self.beta\n        t = torch.log(2.0) / (lamb + torch.log(beta))\n        return t\n    elif self.schedule == \"discrete\":\n        log_alpha = self.log_alpha\n        xp = self.xp\n        yp = self.yp\n        t = interpolate_fn(lamb, xp, yp)\n        return t\n    elif self.schedule == \"exp\":\n        log_alpha = self.log_alpha\n        t = torch.log(2.0) / (lamb + log_alpha)\n        return t\n    elif self.schedule == \"log\":\n        log_alpha = self.log_alpha\n        t = torch.log(2.0) / (lamb + log_alpha)\n        return t\n    elif self.schedule == \"log_inv\":\n        log_alpha = self.log_alpha\n        t = torch.log(2.0) / (lamb + log_alpha)\n        return t\n    elif self.schedule == \"log_inv_sqrt\":\n        log_alpha = self.log_alpha\n        t = torch.log(2.0) / (lamb + log_alpha)\n        return t\n    elif self.schedule == \"log_inv_sqrt_2\":\n        log_alpha = self.log_alpha\n        t = torch.log(2.0) / (lamb + log_alpha)\n        return t\n    elif self.schedule == \"log_inv_sqrt_3\":\n        log_alpha = self.log_alpha\n        t = torch.log(2.0) / (lamb + log_alpha)\n        return t\n    elif self.schedule == \"log_inv_sqrt_4\":\n        log_alpha = self.log_alpha\n        t = torch.log(2.0) / (lamb + log_alpha)\n        return t\n    elif self.schedule == \"log_inv_sqrt_5\":\n        log_alpha = self.log_alpha\n        t = torch.log(2.0) / (lamb + log_alpha)\n        return t\n    elif self.schedule == \"log_inv_sqrt_6\":\n        log_alpha = self.log_alpha\n        t = torch.log(2.0) / (lamb + log_alpha)\n        return t\n    elif self.schedule == \"log_inv_sqrt_7\":\n        log_alpha", "CodeBleu_score": 20.53703080408576}, {"code": "def inverse_lambda(self, lamb)\n\t\n    if self.schedule_type == \"linear\":\n        t = torch.sum(\n            self.beta_values * torch.clamp(lamb, min=0.0, max=1.0), dim=1, keepdim=True\n        )\n    elif self.schedule_type == \"discrete\":\n        xp = self.log_alpha\n        yp = torch.arange(xp.shape[1], device=xp.device).float()\n        t = interpolate_fn(lamb, xp, yp)\n    else:\n        t = torch.acos(torch.clamp(lamb, min=0.0, max=1.0)) / math.pi * self.T\n    return t\n\n", "CodeBleu_score": 22.380480873181487}]}
{"instance_id": "a519c853a2b2", "generation_result": [{"code": "def noise_pred_fn(x, t_continuous, cond=None)\n\t\n    t_continuous = expand_dims(t_continuous, x.dim())\n    x = torch.cat([x, t_continuous], dim=-1)\n    if model_type == 'linear':\n        return noise_schedule.marginal_alpha(t_continuous) * model(x, cond=cond)\n    elif model_type == 'quadratic':\n        return noise_schedule.marginal_alpha(t_continuous) * model(x, cond=cond) ** 2\n    elif model_type == 'cubic':\n        return noise_schedule.marginal_alpha(t_continuous) * model(x, cond=cond) ** 3\n    elif model_type == 'exp':\n        return noise_schedule.marginal_alpha(t_continuous) * torch.exp(model(x, cond=cond))\n    elif model_type == 'sin':\n        return noise_schedule.marginal_alpha(t_continuous) * torch.sin(model(x, cond=cond))\n    elif model_type == 'cos':\n        return noise_schedule.marginal_alpha(t_continuous) * torch.cos(model(x, cond=cond))\n    else:\n        raise NotImplementedError", "CodeBleu_score": 25.26266535188027}, {"code": "def noise_pred_fn(x, t_continuous, cond=None)\n\t\n    if cond is None:\n        cond = {}\n    if model_type == 'deterministic':\n        return noise_schedule.marginal_std(t_continuous)\n    elif model_type == 'gaussian':\n        return noise_schedule.marginal_std(t_continuous) * torch.exp(model(x, t_continuous, cond))\n    elif model_type == 'gaussian_discrete':\n        return noise_schedule.marginal_std(t_continuous) * torch.exp(model(x, t_continuous, cond))\n    elif model_type == 'poisson':\n        return noise_schedule.marginal_std(t_continuous) * torch.exp(model(x, t_continuous, cond))\n    elif model_type == 'poisson_discrete':\n        return noise_schedule.marginal_std(t_continuous) * torch.exp(model(x, t_continuous, cond))\n    elif model_type == 'gamma':\n        return noise_schedule.marginal_std(t_continuous) * torch.exp(model(x, t_continuous, cond))\n    elif model_type == 'gamma_discrete':\n        return noise_schedule.marginal_std(t_continuous) * torch.exp(model(x, t_continuous, cond))\n    elif model_type == 'gamma_gamma':\n        return noise_schedule.marginal_std(t_continuous) * torch.exp(model(x, t_continuous, cond))\n    elif model_type == 'gamma_gamma_discrete':\n        return noise_schedule.marginal_std(t_continuous) * torch.exp(model(x, t_continuous, cond))\n    elif model_type == 'gamma_gamma_gamma':\n        return noise_schedule.marginal_std(t_continuous) * torch.exp(model(x, t_continuous, cond))\n    elif model_type == 'gamma_gamma_gamma_discrete':\n        return noise_schedule.marginal_std(t_continuous) * torch.exp(model(x, t_continuous, cond))\n    elif model_type == 'gamma_gamma_gamma_gamma':\n        return noise_schedule.marginal_std(t_continuous) * torch.exp(model(x, t_continuous, cond))\n    elif model_type == 'gamma_gamma_gamma_gamma_discrete':\n       ", "CodeBleu_score": 18.6476316915979}, {"code": "def noise_pred_fn(x, t_continuous, cond=None)\n\t\n    if model_type == \"linear\":\n        return noise_pred_fn_linear(x, t_continuous, cond)\n    elif model_type == \"mlp\":\n        return noise_pred_fn_mlp(x, t_continuous, cond)\n    else:\n        raise ValueError(\"Unknown model type: {}\".format(model_type))", "CodeBleu_score": 7.697332374696031}]}
{"instance_id": "96a9b7d54823", "generation_result": [{"code": "def dynamic_thresholding_fn(self, x0, t=None)\n\t\n    if t is None:\n        t = self.t\n    x0 = x0.reshape(x0.shape[0], -1)\n    x0_abs = torch.abs(x0)\n    q = torch.quantile(x0_abs, t, dim=1)\n    q = torch.unsqueeze(q, dim=1)\n    q = torch.unsqueeze(q, dim=1)\n    q = torch.unsqueeze(q, dim=1)\n    q = torch.unsqueeze(q, dim=1)\n    q = torch.unsqueeze(q, dim=1)\n    q = torch.unsqueeze(q, dim=1)\n    q = torch.unsqueeze(q, dim=1)\n    q = torch.unsqueeze(q, dim=1)\n    q = torch.unsqueeze(q, dim=1)\n    q = torch.unsqueeze(q, dim=1)\n    q = torch.unsqueeze(q, dim=1)\n    q = torch.unsqueeze(q, dim=1)\n    q = torch.unsqueeze(q, dim=1)\n    q = torch.unsqueeze(q, dim=1)\n    q = torch.unsqueeze(q, dim=1)\n    q = torch.unsqueeze(q, dim=1)\n    q = torch.unsqueeze(q, dim=1)\n    q = torch.unsqueeze(q, dim=1)\n    q = torch.unsqueeze(q, dim=1)\n    q = torch.unsqueeze(q, dim=1)\n    q = torch.unsqueeze(q, dim=1)\n    q = torch.unsqueeze(q, dim=1)\n    q = torch.unsqueeze(q, dim=1)\n    q = torch.unsqueeze(q, dim=1)\n    q = torch.unsqueeze(q, dim=1)\n    q = torch.unsqueeze(q, dim=1)\n    q = torch.unsqueeze(q, dim=1)\n    q = torch.unsqueeze(q, dim=1)\n    q = torch.unsqueeze(q, dim=1)\n    q = torch.unsqueeze(q, dim=1)\n    q = torch.unsqueeze(q, dim=1)\n    q = torch.unsqueeze(q, dim=1)\n    q = torch.unsqueeze(q, dim=1)\n    q = torch.unsqueeze(q, dim=1)\n    q = torch.unsqueeze(", "CodeBleu_score": 24.275293546476178}, {"code": "def dynamic_thresholding_fn(self, x0, t=None)\n\t\n    if t is None:\n        t = self.threshold\n    x0 = x0.abs()\n    x0 = torch.quantile(x0, t)\n    x0 = x0.reshape(x0.shape + (1,) * (x0.dim() - 1))\n    x0 = torch.maximum(x0, self.eps)\n    x0 = x0.reshape(x0.shape[:-1])\n    x0 = x0.clamp(min=self.eps)\n    return x0 / x0.max() * self.max_value\n\n", "CodeBleu_score": 37.930851902817366}, {"code": "def dynamic_thresholding_fn(self, x0, t=None)\n\t\n    if t is None:\n        t = self.threshold\n    x0 = x0.reshape(x0.shape[0], -1)\n    quantile = torch.quantile(torch.abs(x0), t, dim=1)\n    scale = torch.max(torch.ones_like(x0), quantile / self.threshold)\n    return torch.clamp(x0 / scale, -1, 1) * self.scale\n", "CodeBleu_score": 38.046337280245524}]}
{"instance_id": "59c6ec6d7a6a", "generation_result": [{"code": "def data_prediction_fn(self, x, t)\n\t\n    threshold = self.dynamic_threshold(x, t)\n    x_clamped = torch.clamp(x, -threshold, threshold)\n    x_clamped = (x_clamped + threshold) / (2. * threshold)\n    return x_clamped\n", "CodeBleu_score": 4.002077921145364}, {"code": "def data_prediction_fn(self, x, t)\n\t\n    x0 = x.clone()\n    x0 = torch.clamp(x0, min=0.)\n    x0 = x0 / (self.max_threshold * self.dynamic_threshold_ratio)\n    return x0\n", "CodeBleu_score": 7.626237443079849}, {"code": "def data_prediction_fn(self, x, t)\n\t\n    x = self.dynamic_thresholding(x, t)\n    return x", "CodeBleu_score": 2.155395009855169}]}
{"instance_id": "65e3618468a2", "generation_result": [{"code": "def predict_eps_sigma(model, input, sigma_in, **kwargs)\n\t\n    sigma_in = sigma_in.view(sigma_in.shape[0], sigma_in.shape[1], 1, 1)\n    input = input * sigma_in\n    return model(input, **kwargs) * sigma_in\n", "CodeBleu_score": 26.45921598552703}, {"code": "def predict_eps_sigma(model, input, sigma_in, **kwargs)\n\t\n    #reshape sigma to match the dimensions of the input\n    sigma_in = sigma_in.view(sigma_in.size(0),1,1,1)\n    #scale the input\n    input = input * sigma_in\n    #compute the prediction\n    pred = model(input, **kwargs)\n    #adjust the prediction\n    pred = pred / sigma_in\n    return pred\n", "CodeBleu_score": 27.594354145036696}, {"code": "def predict_eps_sigma(model, input, sigma_in, **kwargs)\n\t\n    # Reshape sigma to match the dimensions of the input\n    sigma_in = sigma_in.view(sigma_in.shape[0], *([1] * (len(input.shape) - 1)))\n    # Scale the input by sigma\n    input = input * sigma_in\n    # Compute the prediction\n    pred = model(input, **kwargs)\n    # Adjust the prediction by sigma\n    pred = pred / sigma_in\n    return pred\n", "CodeBleu_score": 32.19221921176071}]}
{"instance_id": "b35846dfbc66", "generation_result": [{"code": "def cal_intergrand(beta_0, beta_1, taus)\n\t\n    log_alpha = t2alpha_fn(beta_0, beta_1, taus)\n    log_alpha.sum().backward()\n    return taus.grad.clone()\n\n", "CodeBleu_score": 6.914780966768691}, {"code": "def cal_intergrand(beta_0, beta_1, taus)\n\t\n    with torch.inference_mode():\n        log_alpha = t2alpha_fn(beta_0, beta_1, taus)\n        log_alpha.sum().backward()\n        grad_log_alpha = taus.grad.clone()\n        taus.grad.zero_()\n        return grad_log_alpha * torch.exp(log_alpha)\n\n", "CodeBleu_score": 30.255796364385468}, {"code": "def cal_intergrand(beta_0, beta_1, taus)\n\t\n    with torch.inference_mode():\n        alpha = t2alpha_fn(beta_0, beta_1, taus)\n        log_alpha = torch.log(alpha)\n        log_alpha.sum().backward()\n        grad_log_alpha = taus.grad\n        return grad_log_alpha * alpha\n", "CodeBleu_score": 32.625862903707045}]}
{"instance_id": "5f3667566a10", "generation_result": [{"code": "def sample_euler_ancestral(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None)\n\t\n    if extra_args is None:\n        extra_args = tuple()\n    if noise_sampler is None:\n        noise_sampler = default_noise_sampler\n    if disable is None:\n        disable = []\n    x = x.detach().requires_grad_(True)\n    if callback is not None:\n        callback(x, *extra_args)\n    for i in trange(len(sigmas)):\n        sigma_down, sigma_up = get_ancestral_step(sigmas[i], sigmas[i - 1] if i > 0 else 0., eta=eta)\n        denoised = model(x, sigma_down, *extra_args)\n        d = to_d(x, sigma_up, denoised)\n        x = x.detach() + s_noise * sigma_up * noise_sampler(x)\n        x = x.detach() + d\n        if callback is not None:\n            callback(x, *extra_args)\n    if 'noise' not in disable:\n        x = x.detach() + s_noise * noise_sampler(x)\n    return x\n\n", "CodeBleu_score": 42.26202963666345}, {"code": "def sample_euler_ancestral(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None)\n\t\n    if extra_args is None:\n        extra_args = []\n    if noise_sampler is None:\n        noise_sampler = default_noise_sampler\n    if disable is None:\n        disable = []\n    x = x.detach()\n    x.requires_grad = True\n    x_out = []\n    for i in trange(len(sigmas), desc=\"Ancestral sampling\", disable=disable):\n        sigma_from = sigmas[i - 1] if i > 0 else 0.\n        sigma_to = sigmas[i]\n        sigma_down, sigma_up = get_ancestral_step(sigma_from, sigma_to, eta)\n        if sigma_up == 0.:\n            x_out.append(x)\n            continue\n        denoised = model(x, sigma_down, *extra_args)\n        x_grad = to_d(x, sigma_down, denoised)\n        x = x + s_noise * sigma_up * noise_sampler(x)\n        x = x + x_grad * (sigma_to / sigma_from) ** 0.5\n        if callback is not None:\n            callback(x, i, sigmas, extra_args)\n        x_out.append(x)\n    x_out = torch.stack(x_out)\n    if s_noise != 0.:\n        x_out = x_out + s_noise * noise_sampler(x)\n    return x_out\n\n", "CodeBleu_score": 38.77272744744267}, {"code": "def sample_euler_ancestral(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None)\n\t\n    if extra_args is None:\n        extra_args = {}\n    if noise_sampler is None:\n        noise_sampler = default_noise_sampler\n    if disable is None:\n        disable = []\n    x = x.to(model.device)\n    x_shape = x.shape\n    x = x.reshape(x.shape[0], -1)\n    sigmas = sigmas.to(model.device)\n    sigmas = sigmas.reshape(sigmas.shape[0], -1)\n    if callback is not None:\n        callback(x, **extra_args)\n    for i in trange(len(sigmas), desc='Ancestral sampling', disable=disable):\n        sigma_from = sigmas[i - 1] if i > 0 else 0.\n        sigma_to = sigmas[i]\n        sigma_down, sigma_up = get_ancestral_step(sigma_from, sigma_to, eta)\n        denoised = model(x, sigma_down, **extra_args)\n        x = x + to_d(x, sigma_up, denoised) * s_noise\n        if callback is not None:\n            callback(x, **extra_args)\n        if i < len(sigmas) - 1:\n            noise = noise_sampler(x)\n            x = x + noise * sigma_up\n    x = x.reshape(*x_shape)\n    if s_noise > 0.:\n        x = x + torch.randn_like(x) * s_noise\n    return x\n\n", "CodeBleu_score": 43.68257795092652}]}
{"instance_id": "6261ded88df2", "generation_result": [{"code": "def linear_multistep_coeff(order, t, i, j)\n\t\n    if order > j:\n        raise ValueError(\"Order is too high for step index.\")\n    else:\n        def product_term(t_):\n            return (t_ - t[i]) / (t[i+j] - t[i])\n        return integrate.quad(product_term, t[i], t[i+1])[0]\n\n", "CodeBleu_score": 25.241580312721073}, {"code": "def linear_multistep_coeff(order, t, i, j)\n\t\n    if order == 1:\n        return 1.0\n    elif order == 2:\n        if i == j:\n            return 0.5\n        elif i == j+1:\n            return 0.5\n        else:\n            raise ValueError('The order is too high for the step index.')\n    elif order == 3:\n        if i == j:\n            return 1.0/6.0\n        elif i == j+1:\n            return 2.0/3.0\n        elif i == j+2:\n            return 1.0/6.0\n        else:\n            raise ValueError('The order is too high for the step index.')\n    elif order == 4:\n        if i == j:\n            return 0.5\n        elif i == j+1:\n            return 0.5\n        elif i == j+2:\n            return 0.5\n        elif i == j+3:\n            return 0.5\n        else:\n            raise ValueError('The order is too high for the step index.')\n    elif order == 5:\n        if i == j:\n            return 1.0/12.0\n        elif i == j+1:\n            return 1.0/3.0\n        elif i == j+2:\n            return 1.0/3.0\n        elif i == j+3:\n            return 1.0/3.0\n        elif i == j+4:\n            return 1.0/12.0\n        else:\n            raise ValueError('The order is too high for the step index.')\n    elif order == 6:\n        if i == j:\n            return 1.0/60.0\n        elif i == j+1:\n            return 1.0/12.0\n        elif i == j+2:\n            return 2.0/3.0\n        elif i == j+3:\n            return 2.0/3.0\n        elif i == j+4:\n            return 2.0/3.0\n        elif i == j+5:\n            return 1.0/12.0\n        else:\n            raise ValueError('The order is too high for the step index.')\n    elif order == 7:\n        if i == j:\n            return 1.0/280.0\n        elif i == j+1:\n            return 1.0/20.0\n        elif i ==", "CodeBleu_score": 12.39134588530216}, {"code": "def linear_multistep_coeff(order, t, i, j)\n\t\n    if order >= i+1:\n        raise ValueError(f'The order {order} is too high for the step index {i}.')\n    def nested_function(t):\n        return (t - t_range[i])/(t_range[i+1] - t_range[i])**(order - i)\n    return integrate.quad(nested_function, t_range[i], t_range[i+1])[0]\n\n", "CodeBleu_score": 24.648990172237053}]}
{"instance_id": "4ee025e33ab1", "generation_result": [{"code": "def sample_lcm(model, x, sigmas, extra_args=None, callback=None, disable=None, noise_sampler=None)\n\t\n    if extra_args is None:\n        extra_args = []\n    if noise_sampler is None:\n        noise_sampler = default_noise_sampler(x)\n    if disable is None:\n        disable = []\n    if len(disable) > 0:\n        for d in disable:\n            if d not in ['model', 'callback', 'noise_sampler']:\n                raise ValueError('Unknown disable option: {}'.format(d))\n    if 'model' not in disable:\n        model(x, *extra_args)\n    if 'callback' not in disable:\n        callback(x, *extra_args)\n    for i in trange(len(sigmas)):\n        if sigmas[i] > 0:\n            noise = noise_sampler(sigmas[i], sigmas[i + 1])\n            x = x + sigmas[i] * noise\n            if 'model' not in disable:\n                model(x, *extra_args)\n            if 'callback' not in disable:\n                callback(x, *extra_args)\n        else:\n            x = x + sigmas[i] * noise_sampler(sigmas[i], sigmas[i + 1])\n            if 'model' not in disable:\n                model(x, *extra_args)\n            if 'callback' not in disable:\n                callback(x, *extra_args)\n    return x\n\n", "CodeBleu_score": 31.68426894259793}, {"code": "def sample_lcm(model, x, sigmas, extra_args=None, callback=None, disable=None, noise_sampler=None)\n\t\n    if extra_args is None:\n        extra_args = {}\n    if noise_sampler is None:\n        noise_sampler = default_noise_sampler(x)\n    if disable is None:\n        disable = []\n    if len(disable) > 0:\n        disable = set(disable)\n    for k, v in extra_args.items():\n        if k not in disable:\n            x = x.new_zeros(x.shape[0], v.shape[1])\n            x[:, k] = v\n    for i in trange(len(sigmas), disable=disable.intersection({\"tqdm\", \"tqdm_notebook\"})):\n        sigma = sigmas[i]\n        x_next = model(x, sigma=sigma, **extra_args)\n        if callback is not None:\n            callback(x, x_next, sigma)\n        if i + 1 < len(sigmas):\n            sigma_next = sigmas[i + 1]\n            if sigma_next > 0:\n                x = x + noise_scaling(sigma, sigma_next) * noise_sampler(x, sigma, sigma_next)\n            else:\n                x = x_next\n        else:\n            x = x_next\n    return x\n\n", "CodeBleu_score": 41.24903434286319}, {"code": "def sample_lcm(model, x, sigmas, extra_args=None, callback=None, disable=None, noise_sampler=None)\n\t\n    if extra_args is None:\n        extra_args = {}\n    if noise_sampler is None:\n        noise_sampler = default_noise_sampler(x)\n    if disable is None:\n        disable = []\n    if len(disable) > 0:\n        model.eval()\n    else:\n        model.train()\n    x_out = x.clone()\n    for i, sigma in enumerate(sigmas):\n        if i == len(sigmas) - 1:\n            break\n        x_in = x_out.clone()\n        x_out = model(x_in, sigma=sigma, **extra_args)\n        x_out = x_out.detach()\n        if sigma > 0 and \"noise_scaling\" not in disable:\n            x_out = x_out + noise_sampler(x_in, sigma) * noise_scaling(sigma)\n        if callback is not None:\n            callback(x_in, x_out, sigma)\n    return x_out\n\n", "CodeBleu_score": 34.93726002464507}]}
{"instance_id": "cb51f91da606", "generation_result": [{"code": "def append_dims(x, target_dims)\n\t\n    if x.dim() == target_dims:\n        return x\n    if x.dim() > target_dims:\n        raise ValueError(f\"The number of dimensions of the tensor ({x.dim()}) is larger than the target number of dimensions ({target_dims}).\")\n    n_dims_to_append = target_dims - x.dim()\n    for _ in range(n_dims_to_append):\n        x = x.unsqueeze(-1)\n    return x.clone().detach() if x.device.type == \"mps\" else x\n", "CodeBleu_score": 36.30469072960626}, {"code": "def append_dims(x, target_dims)\n\t\n    if x.dim() == target_dims:\n        return x.detach() if x.is_mps() else x\n    if x.dim() > target_dims:\n        raise ValueError(f\"The tensor has too many dimensions ({x.dim()}). The maximum number of dimensions is {target_dims}.\")\n    return x.new_empty(x.shape + (1,) * (target_dims - x.dim())).expand_as(x)\n", "CodeBleu_score": 22.677263388942656}, {"code": "def append_dims(x, target_dims)\n\t\n    current_dims = len(x.shape)\n    if target_dims < current_dims:\n        raise ValueError(f\"The target dimensions {target_dims} must be greater than the current dimensions {current_dims}.\")\n    num_dims_to_append = target_dims - current_dims\n    if num_dims_to_append > 0:\n        return x.expand(x.shape + (1,)*num_dims_to_append).detach() if x.device.type == 'mps' else x.expand(x.shape + (1,)*num_dims_to_append)\n    else:\n        return x.detach() if x.device.type == 'mps' else x\n\n", "CodeBleu_score": 30.46316286468631}]}
{"instance_id": "9fac874a3bc7", "generation_result": [{"code": "def download_file(path, url, digest=None)\n\t\n    if not os.path.exists(path):\n        os.makedirs(path)\n    if not os.path.exists(os.path.join(path, os.path.basename(url))):\n        with urlopen(url) as response:\n            with open(os.path.join(path, os.path.basename(url)), \"wb\") as out_file:\n                shutil.copyfileobj(response, out_file)\n    if digest is not None:\n        if not os.path.exists(os.path.join(path, os.path.basename(url))):\n            raise OSError(f\"File {os.path.join(path, os.path.basename(url))} does not exist.\")\n        with open(os.path.join(path, os.path.basename(url)), \"rb\") as file:\n            data = file.read()\n        if hashlib.sha256(data).hexdigest() != digest:\n            raise OSError(f\"Hash of downloaded file {os.path.join(path, os.path.basename(url))} does not match provided digest.\")\n    return os.path.join(path, os.path.basename(url))\n\n", "CodeBleu_score": 40.99534648271337}, {"code": "def download_file(path, url, digest=None)\n\t\n    if not os.path.exists(path):\n        os.makedirs(path)\n    if not os.path.exists(path):\n        raise OSError(\"The target directory does not exist.\")\n    if not os.path.isdir(path):\n        raise OSError(\"The target path is not a directory.\")\n    if not url.startswith(\"http\"):\n        raise ValueError(\"The URL does not start with http.\")\n    filename = os.path.basename(url)\n    filepath = os.path.join(path, filename)\n    if not os.path.exists(filepath):\n        with urlopen(url) as response:\n            with open(filepath, \"wb\") as file:\n                shutil.copyfileobj(response, file)\n    if digest is not None:\n        with open(filepath, \"rb\") as file:\n            data = file.read()\n        hash = hashlib.sha256(data).hexdigest()\n        if hash != digest:\n            raise OSError(\"The file's hash does not match the provided digest.\")\n    return filepath\n\n", "CodeBleu_score": 44.292840069106944}, {"code": "def download_file(path, url, digest=None)\n\t\n    if not path.exists():\n        path.mkdir(parents=True)\n    file_path = path / url.split(\"/\")[-1]\n    if not file_path.exists():\n        with urlopen(url) as response:\n            with file_path.open(\"wb\") as file:\n                shutil.copyfileobj(response, file)\n    if digest is not None:\n        with file_path.open(\"rb\") as file:\n            file_hash = hashlib.sha256()\n            while True:\n                chunk = file.read(8192)\n                if not chunk:\n                    break\n                file_hash.update(chunk)\n        if file_hash.hexdigest() != digest:\n            raise OSError(f\"Hash mismatch for {url}\")\n    return file_path\n", "CodeBleu_score": 42.19813540029104}]}
{"instance_id": "f23165e897ce", "generation_result": [{"code": "def train_mode(model, mode=True)\n\t\n    if mode:\n        modes = [m.training for m in model.modules()]\n        for m in model.modules():\n            m.train()\n    else:\n        modes = [m.training for m in model.modules()]\n        for m in model.modules():\n            m.eval()\n    yield\n    for m, mode in zip(model.modules(), modes):\n        m.train(mode)\n", "CodeBleu_score": 36.63822152897859}, {"code": "def train_mode(model, mode=True)\n\t\n    if mode:\n        modes = []\n        for module in model.modules():\n            modes.append(module.training)\n            module.train(mode)\n        return modes\n    else:\n        for module, mode in zip(model.modules(), modes):\n            module.train(mode)\n\n\n", "CodeBleu_score": 32.87164758494181}, {"code": "def train_mode(model, mode=True)\n\t\n    if not isinstance(model, nn.Module):\n        raise TypeError(\"The model should be a nn.Module\")\n    if not isinstance(mode, bool):\n        raise TypeError(\"The mode should be a boolean\")\n    training_modes = []\n    for module in model.modules():\n        training_modes.append(module.training)\n        module.train(mode)\n    yield\n    for module, training_mode in zip(model.modules(), training_modes):\n        module.train(training_mode)\n", "CodeBleu_score": 31.143908356364765}]}
{"instance_id": "c6d59c6cb8e1", "generation_result": [{"code": "def ema_update(model, averaged_model, decay)\n\t\n    for model_param, averaged_param in zip(model.parameters(), averaged_model.parameters()):\n        if averaged_param.dtype != torch.float32:\n            raise ValueError(f\"averaged_param must be of type float32, but got {averaged_param.dtype}.\")\n        if model_param.dtype != averaged_param.dtype:\n            raise ValueError(f\"model_param and averaged_param must have the same type, but got {model_param.dtype} and {averaged_param.dtype}.\")\n        if averaged_param.device != model_param.device:\n            raise ValueError(f\"averaged_param and model_param must be on the same device, but got {averaged_param.device} and {model_param.device}.\")\n\n    for model_buffer, averaged_buffer in zip(model.buffers(), averaged_model.buffers()):\n        if averaged_buffer.dtype != torch.float32:\n            raise ValueError(f\"averaged_buffer must be of type float32, but got {averaged_buffer.dtype}.\")\n        if model_buffer.dtype != averaged_buffer.dtype:\n            raise ValueError(f\"model_buffer and averaged_buffer must have the same type, but got {model_buffer.dtype} and {averaged_buffer.dtype}.\")\n        if averaged_buffer.device != model_buffer.device:\n            raise ValueError(f\"averaged_buffer and model_buffer must be on the same device, but got {averaged_buffer.device} and {model_buffer.device}.\")\n\n    for model_param, averaged_param in zip(model.parameters(), averaged_model.parameters()):\n        averaged_param.mul_(decay).add_(model_param, alpha=1 - decay)\n\n    for model_buffer, averaged_buffer in zip(model.buffers(), averaged_model.buffers()):\n        averaged_buffer.copy_(model_buffer)\n", "CodeBleu_score": 32.04197790772746}, {"code": "def ema_update(model, averaged_model, decay)\n\t\n    # dictionary\n    model_buffers = dict(model.named_buffers())\n    model_params = dict(model.named_parameters())\n    averaged_buffers = dict(averaged_model.named_buffers())\n    averaged_params = dict(averaged_model.named_parameters())\n\n    # update parameters and buffers\n    for name in averaged_params:\n        if name in model_params:\n            # get parameter\n            param = model_params[name]\n            # get averaged parameter\n            averaged_param = averaged_params[name]\n            # update averaged parameter\n            averaged_param.data.mul_(decay).add_(param.data, alpha=1 - decay)\n        else:\n            print(f\"Not found: {name}\")\n\n    for name in averaged_buffers:\n        if name in model_buffers:\n            # get buffer\n            buf = model_buffers[name]\n            # get averaged buffer\n            averaged_buf = averaged_buffers[name]\n            # update averaged buffer\n            averaged_buf.data.copy_(buf.data)\n        else:\n            print(f\"Not found: {name}\")\n", "CodeBleu_score": 33.82584278578148}, {"code": "def ema_update(model, averaged_model, decay)\n\t\n    # dict.items() returns a list of tuples (key, value)\n    for current_params, averaged_params in zip(model.named_parameters(), averaged_model.named_parameters()):\n        name, current_param = current_params\n        # the above is the same as:\n        # for name, current_param in model.named_parameters():\n        #     pass\n        # name = name\n        # current_param = current_param\n        # averaged_param = averaged_params\n        # the above is the same as:\n        # for name, (current_param, averaged_param) in zip(model.named_parameters(), averaged_model.named_parameters()):\n        #     pass\n        # name = name\n        # current_param = current_param\n        # averaged_param = averaged_param\n        # the above is the same as:\n        # for name, current_param, averaged_param in zip(model.named_parameters(), averaged_model.named_parameters()):\n        #     pass\n        # name = name\n        # current_param = current_param\n        # averaged_param = averaged_param\n        # the above is the same as:\n        # for name, (current_param, averaged_param) in zip(model.named_parameters(), averaged_model.named_parameters()):\n        #     pass\n        # name = name\n        # current_param = current_param\n        # averaged_param = averaged_param\n        # the above is the same as:\n        # for name, (current_param, averaged_param) in zip(model.named_parameters(), averaged_model.named_parameters()):\n        #     pass\n        # name = name\n        # current_param = current_param\n        # averaged_param = averaged_param\n        # the above is the same as:\n        # for name, (current_param, averaged_param) in zip(model.named_parameters(), averaged_model.named_parameters()):\n        #     pass\n        # name = name\n        # current_param = current_param\n        # averaged_param = averaged_param\n        # the above is the same as:\n        # for name, (current_param, averaged_param) in zip(model.named_parameters(), averaged_model.named_parameters()):\n        #     pass\n        # name = name\n       ", "CodeBleu_score": 5.575851280310132}]}
{"instance_id": "600c39a30798", "generation_result": [{"code": "def rand_log_logistic(shape, loc=0., scale=1., min_value=0., max_value=float('inf'), device='cpu', dtype=torch.float32)\n\t\n    min_value = torch.as_tensor(min_value, device=device, dtype=dtype)\n    max_value = torch.as_tensor(max_value, device=device, dtype=dtype)\n    u = torch.rand(shape, device=device, dtype=dtype)\n    u = min_value.log() + (max_value.log() - min_value.log()) * u\n    return u.logit().mul_(scale).add_(loc)\n", "CodeBleu_score": 33.458144245036024}, {"code": "def rand_log_logistic(shape, loc=0., scale=1., min_value=0., max_value=float('inf'), device='cpu', dtype=torch.float32)\n\t\n    min_value = torch.as_tensor(min_value, device=device, dtype=dtype)\n    max_value = torch.as_tensor(max_value, device=device, dtype=dtype)\n    u = torch.rand(shape, device=device, dtype=dtype)\n    min_cdf = torch.log(1 + torch.exp(min_value))\n    max_cdf = torch.log(1 + torch.exp(max_value))\n    cdf_range = max_cdf - min_cdf\n    cdf = min_cdf + u * cdf_range\n    u = torch.log(torch.exp(cdf) - 1)\n    u = (u - min_value) / cdf_range\n    return u.to(dtype)\n", "CodeBleu_score": 42.08484254347287}, {"code": "def rand_log_logistic(shape, loc=0., scale=1., min_value=0., max_value=float('inf'), device='cpu', dtype=torch.float32)\n\t\n    u = torch.rand(shape, device=device, dtype=dtype)\n    u = u.clamp(min=min_value, max=max_value)\n    u = torch.as_tensor(u, device=device, dtype=dtype)\n    u = u.log() - torch.log(1. - u)\n    u = u.mul(scale)\n    u = u.add(loc)\n    return u\n", "CodeBleu_score": 24.270524747088228}]}
{"instance_id": "75e2def2467f", "generation_result": [{"code": "def get_activation(activation: Literal[\"elu\", \"snake\", \"none\"], antialias=False, channels=None) -> nn.Module\n\t\n    if activation == \"elu\":\n        if antialias:\n            return nn.Sequential(\n                nn.ELU(),\n                Activation1d(antialias=antialias),\n            )\n        else:\n            return nn.ELU()\n    elif activation == \"snake\":\n        if antialias:\n            return nn.Sequential(\n                SnakeBeta(channels),\n                Activation1d(antialias=antialias),\n            )\n        else:\n            return SnakeBeta(channels)\n    elif activation == \"none\":\n        return Identity()\n    else:\n        raise ValueError(f\"Activation type {activation} not recognized.\")\n", "CodeBleu_score": 39.62241963554998}, {"code": "def get_activation(activation: Literal[\"elu\", \"snake\", \"none\"], antialias=False, channels=None) -> nn.Module\n\t\n    if activation == \"elu\":\n        return ELU(antialias)\n    elif activation == \"snake\":\n        return SnakeBeta(channels)\n    elif activation == \"none\":\n        return Identity()\n    else:\n        raise ValueError(f\"Activation type {activation} is not recognized.\")\n", "CodeBleu_score": 40.209591266871016}, {"code": "def get_activation(activation: Literal[\"elu\", \"snake\", \"none\"], antialias=False, channels=None) -> nn.Module\n\t\n    if activation == \"elu\":\n        if antialias:\n            return nn.Sequential(\n                nn.Upsample(scale_factor=2, mode=\"nearest\"),\n                nn.ReflectionPad2d(1),\n                nn.ELU(alpha=1.0, inplace=True),\n            )\n        else:\n            return nn.ELU(alpha=1.0, inplace=True)\n    elif activation == \"snake\":\n        return SnakeBeta(channels)\n    elif activation == \"none\":\n        return Identity()\n    else:\n        raise ValueError(f\"Activation type {activation} not recognized\")\n", "CodeBleu_score": 38.04906830149327}]}
{"instance_id": "ef1714c79e51", "generation_result": [{"code": "def apply_rotary_pos_emb(t, freqs, scale = 1)\n\t\n    if t.dtype != torch.float32:\n        t = t.to(dtype = torch.float32)\n    if t.ndim < 3:\n        raise ValueError('Input tensor must have at least 3 dimensions.')\n    if freqs.dtype != torch.float32:\n        freqs = freqs.to(dtype = torch.float32)\n    if freqs.ndim != 1:\n        raise ValueError('Frequency tensor must be a 1D tensor.')\n    if t.shape[-2] != freqs.shape[0]:\n        raise ValueError('The last two dimensions of the input tensor must match the frequency tensor.')\n    t_unrotated, t = t[..., :1], t[..., 1:]\n    t_unrotated = t_unrotated.to(dtype = torch.float32)\n    freqs = freqs.to(dtype = torch.float32)\n    freqs = rearrange(freqs, 'j -> () j ()')\n    freqs = scale * freqs\n    sin_emb = torch.sin(freqs * t)\n    cos_emb = torch.cos(freqs * t)\n    t = torch.cat((sin_emb, cos_emb, t), dim = -1)\n    return t_unrotated, t\n\n", "CodeBleu_score": 32.6066696010249}, {"code": "def apply_rotary_pos_emb(t, freqs, scale = 1)\n\t\n    t_unrotated = t\n    if t.dtype != torch.float32:\n        t = t.to(torch.float32)\n    if len(t.shape) < 3:\n        t = rearrange(t, '... -> () ...')\n    if len(freqs.shape) == 1:\n        freqs = rearrange(freqs, 'd -> () d')\n    freqs = rearrange(freqs, '() (j d) -> () j d', j = 2)\n    freqs = freqs * scale\n    t = rearrange(t, '... (j d) -> ... j d', j = 2)\n    freqs = freqs.to(t.device)\n    t1, t2 = t.unbind(dim = -2)\n    freqs1, freqs2 = freqs.unbind(dim = -2)\n    sin = torch.sin(t1 * freqs1 + t2 * freqs2)\n    cos = torch.cos(t1 * freqs1 + t2 * freqs2)\n    t = torch.stack((sin, cos), dim = -2)\n    t = rearrange(t, '... j d -> ... (j d)')\n    return torch.cat((t_unrotated, t), dim = -1)\n\n", "CodeBleu_score": 35.45737123101351}, {"code": "def apply_rotary_pos_emb(t, freqs, scale = 1)\n\t\n    t_unrotated = t\n    if t.dtype != torch.float32:\n        t = t.to(torch.float32)\n    if len(t.shape) == 2:\n        t = rearrange(t, 'n d -> n 1 d')\n    if len(freqs.shape) == 1:\n        freqs = rearrange(freqs, 'j -> 1 j')\n    freqs = freqs * scale\n    freqs = rearrange(freqs, '... j -> ... 1 j')\n    t = torch.cat((t, freqs), dim = -1)\n    t = apply_rotary_emb(t)\n    t_unrotated = t_unrotated[:, :, 0]\n    return t_unrotated, t\n", "CodeBleu_score": 32.20300583729332}]}
{"instance_id": "a7a5f178c6f6", "generation_result": [{"code": "def idx2vq(self, idx, dim=-1)\n\t\n    return self.codebook(idx).movedim(1, dim)\n", "CodeBleu_score": 18.88410605975309}, {"code": "def idx2vq(self, idx, dim=-1)\n\t\n    return self.codebook(idx).movedim(1, dim)\n\n", "CodeBleu_score": 18.88410605975309}, {"code": "def idx2vq(self, idx, dim=-1)\n\t\n    return self.codebook(idx, dim=dim)\n", "CodeBleu_score": 16.59312750823775}]}
{"instance_id": "4104c041fb20", "generation_result": [{"code": "def encode(self, x, quantize=False)\n\t\n    z = x\n    for block in self.blocks:\n        z = block(z)\n    z = self.down_blocks(z)\n    z = self.in_block(z)\n    if quantize:\n        z, (vq_loss, commit_loss), indices = self.vq(z, self.codebook.weight.detach())\n        if self.ema_loss and self.training:\n            self._updateEMA(z.detach(), indices.detach())\n        z_q = torch.index_select(self.codebook.weight, dim=0, index=indices)\n        return z, z_q, indices, (vq_loss, commit_loss)\n    return z\n", "CodeBleu_score": 36.675139507613714}, {"code": "def encode(self, x, quantize=False)\n\t\n    x = self.in_block(x)\n    for down_block in self.down_blocks:\n        x = down_block(x)\n    x = self.bottleneck(x)\n    for up_block in self.up_blocks:\n        x = up_block(x)\n    x = self.out_block(x)\n    if quantize:\n        z_e_x, (vq_loss, commit_loss), indices = self.forward(x, get_losses=True)\n        return z_e_x, z_e_x, indices, vq_loss, commit_loss\n    return x\n\n", "CodeBleu_score": 28.954826079092776}, {"code": "def encode(self, x, quantize=False)\n\t\n    x = self.conv_in(x)\n    x = self.in_block(x)\n    for down_block in self.down_blocks:\n        x = down_block(x)\n    x = self.conv_out(x)\n    if quantize:\n        z_q_x, (vq_loss, commit_loss), indices = self.vq(x, self.codebook.weight.detach())\n        if self.ema_loss and self.training:\n            self._updateEMA(x.detach(), indices.detach())\n        # pick the graded embeddings after updating the codebook in order to have a more accurate commitment loss\n        z_q_x_grd = torch.index_select(self.codebook.weight, dim=0, index=indices)\n        vq_loss = (z_q_x_grd - x.detach()).pow(2).mean()\n        commit_loss = (x - z_q_x_grd.detach()).pow(2).mean()\n        return z_q_x, z_q_x_grd, indices, vq_loss, commit_loss\n    else:\n        return x\n\n", "CodeBleu_score": 26.8964542998457}]}
{"instance_id": "403c52ba2d3b", "generation_result": [{"code": "def ema_scope(self, context=None)\n\t\n    if self.parameters is None:\n        return\n    if context is not None:\n        context.store(self.parameters)\n    self.copy_to(self.ema_model)\n    if context is not None:\n        context.restore(self.parameters)\n\n", "CodeBleu_score": 33.8579569405453}, {"code": "def ema_scope(self, context=None)\n\t\n    if context is None:\n        context = self.ema_context\n    if context is None:\n        yield\n    else:\n        with context:\n            yield\n", "CodeBleu_score": 19.376877182033123}, {"code": "def ema_scope(self, context=None)\n\t\n    def ema_scope_context_manager():\n        self.store(self.parameters())\n        self.copy_to(self)\n        yield\n        self.restore(self.parameters())\n        if context is not None:\n            context.log(f\"EMA weights restored.\")\n    return contextlib.contextmanager(ema_scope_context_manager)()\n", "CodeBleu_score": 34.75385352330878}]}
{"instance_id": "94b7467f98e4", "generation_result": [{"code": "def post_attention(self, attn, x, gate_msa, shift_mlp, scale_mlp, gate_mlp)\n\t\n    if self.pre_only:\n        return x\n    assert attn is not None\n    x = x + attn * gate_msa\n    x = self.norm1(x)\n    x = self.modulate(x, shift_mlp, scale_mlp)\n    x = self.mlp(x)\n    x = self.post_attention(x)\n    return x * gate_mlp\n", "CodeBleu_score": 45.38065454058785}, {"code": "def post_attention(self, attn, x, gate_msa, shift_mlp, scale_mlp, gate_mlp)\n\t\n    x = x + gate_msa.unsqueeze(1) * attn\n    x = self.norm2(x)\n    x = self.modulate(x, shift_mlp, scale_mlp)\n    x = self.post_attention(x)\n    x = x * gate_mlp.unsqueeze(1)\n    return x\n", "CodeBleu_score": 53.71105961916438}, {"code": "def post_attention(self, attn, x, gate_msa, shift_mlp, scale_mlp, gate_mlp)\n\t\n    # add attention output\n    x = x + gate_msa * attn\n    # normalize and modulate\n    x = self.norm1(x)\n    x = self.modulate(x, shift_mlp, scale_mlp)\n    # apply MLP\n    x = self.mlp(x)\n    x = self.norm2(x)\n    x = self.modulate(x, shift_mlp, scale_mlp)\n    # apply gate\n    x = x * gate_mlp\n    return x\n", "CodeBleu_score": 44.37419847056488}]}
{"instance_id": "d9fdc05542a2", "generation_result": [{"code": "def _block_mixing(context, x, context_block, x_block, c)\n\t\n    if c:\n        q, k, v = context_block.pre_attention(context)\n        x_q, x_k, x_v = x_block.pre_attention(x)\n    else:\n        x_q, x_k, x_v = x_block.pre_attention(x)\n        q, k, v = context_block.pre_attention(context)\n    x_q, x_k, x_v = x_q.transpose(1, 2), x_k.transpose(1, 2), x_v.transpose(1, 2)\n    q, k, v = q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2)\n    q, k, v = optimized_attention(q, k, v, num_heads=context_block.num_heads)\n    q, k, v = q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2)\n    x_q, x_k, x_v = x_q.transpose(1, 2), x_k.transpose(1, 2), x_v.transpose(1, 2)\n    if c:\n        x_q = x_q + q\n        x_k = x_k + k\n        x_v = x_v + v\n        x_q, x_k, x_v = x_q.transpose(1, 2), x_k.transpose(1, 2), x_v.transpose(1, 2)\n        x_q = x_q.reshape(x_q.shape[0], x_q.shape[1], -1)\n        x_k = x_k.reshape(x_k.shape[0], x_k.shape[1], -1)\n        x_v = x_v.reshape(x_v.shape[0], x_v.shape[1], -1)\n        x_q = x_block.post_attention(x_q)\n        x_k = x_block.post_attention(x_k)\n        x_v = x_block.post_attention(x_v)\n        x_q = x_q.reshape(x_q.", "CodeBleu_score": 28.827808629491635}, {"code": "def _block_mixing(context, x, context_block, x_block, c)\n\t\n    q, k, v = context_block.pre_attention(context)\n    qx, kx, vx = x_block.pre_attention(x)\n    qkv = torch.cat((q, qx, k, kx, v, vx), dim=2)\n    att = optimized_attention(qkv, c.num_heads)\n    att = att.reshape(att.shape[0], -1, c.head_dim)\n    att = att[:, :context.shape[1], :]\n    att = att[:, :x.shape[1], :]\n    att = att.reshape(att.shape[0], -1, c.num_heads * c.head_dim)\n    att = context_block.post_attention(att)\n    att = x_block.post_attention(att)\n    return att\n\n", "CodeBleu_score": 18.95414976676347}, {"code": "def _block_mixing(context, x, context_block, x_block, c)\n\t\n    # Pre-attention\n    q, k, v = context_block.pre_attention(context)\n    xq, xk, xv = x_block.pre_attention(x)\n    # Attention\n    xq = xq.reshape(xq.shape[0], xq.shape[1], -1)\n    xk = xk.reshape(xk.shape[0], xk.shape[1], -1)\n    xv = xv.reshape(xv.shape[0], xv.shape[1], -1)\n    x_block.attention_cache = (xq, xk, xv)\n    q = torch.cat((q, xq), dim=2)\n    k = torch.cat((k, xk), dim=2)\n    v = torch.cat((v, xv), dim=2)\n    x_block.attention_cache = (q, k, v)\n    q, k, v = optimized_attention(q, k, v, c)\n    x_block.attention_cache = (q, k, v)\n    q, xq = q[:, :, :q.shape[2] - xq.shape[2]], q[:, :, q.shape[2] - xq.shape[2]:]\n    k, xk = k[:, :, :k.shape[2] - xk.shape[2]], k[:, :, k.shape[2] - xk.shape[2]:]\n    v, xv = v[:, :, :v.shape[2] - xv.shape[2]], v[:, :, v.shape[2] - xv.shape[2]:]\n    x_block.attention_cache = (q, k, v)\n    q = q.reshape(q.shape[0], q.shape[1], c, -1).permute(0, 2, 1, 3)\n    k = k.reshape(k.shape[0], k.shape[1], c, -1).permute(0, 2, 1, 3)\n    v = v.reshape(v.shape[0], v.shape[1], c, -1).permute(0, 2, 1, 3)\n    x_block.attention_cache = (q, k, v)\n    #", "CodeBleu_score": 19.15124473285963}]}
{"instance_id": "b219bce30160", "generation_result": [{"code": "def get_timestep_embedding(timesteps, embedding_dim)\n\t\n    assert len(timesteps.shape) == 1  # and timesteps.dtype == tf.int32\n    half_dim = embedding_dim // 2\n    emb = math.log(10000) / (half_dim - 1)\n    emb = torch.exp(torch.arange(half_dim, dtype=torch.float32) * -emb)\n    emb = torch.arange(timesteps.shape[0], dtype=torch.float32).unsqueeze(1) * emb.unsqueeze(0)\n    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n    if embedding_dim % 2 == 1:  # zero pad\n        emb = torch.nn.functional.pad(emb, (0,1,0,0))\n    assert emb.shape == (timesteps.shape[0], embedding_dim)\n    return emb\n\n", "CodeBleu_score": 75.6208905312479}, {"code": "def get_timestep_embedding(timesteps, embedding_dim)\n\t\n    assert len(timesteps.shape) == 1  # and timesteps.dtype == tf.int32\n    half_dim = embedding_dim // 2\n    emb = math.log(10000) / (half_dim - 1)\n    emb = torch.exp(torch.arange(half_dim, dtype=torch.float32) * -emb)\n    emb = torch.arange(timesteps.shape[1], dtype=torch.float32).unsqueeze(1) * emb.unsqueeze(0)\n    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n    if embedding_dim % 2 == 1:  # zero pad\n        emb = torch.nn.functional.pad(emb, (0,1,0,0))\n    assert emb.shape == (timesteps.shape[1], embedding_dim)\n    return emb\n\n", "CodeBleu_score": 75.6208905312479}, {"code": "def get_timestep_embedding(timesteps, embedding_dim)\n\t\n    assert len(timesteps.shape) == 1  # and timesteps.dtype == tf.int32\n    half_dim = embedding_dim // 2\n    emb = math.log(10000) / (half_dim - 1)\n    emb = torch.exp(torch.arange(half_dim, dtype=torch.float32) * -emb)\n    emb = torch.arange(timesteps.shape[0], dtype=torch.float32).unsqueeze(1) * emb.unsqueeze(0)\n    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n    if embedding_dim % 2 == 1:  # zero pad\n        emb = torch.nn.functional.pad(emb, (0,1,0,0))\n    assert emb.shape == (timesteps.shape[0], embedding_dim)\n    return emb\n", "CodeBleu_score": 75.6208905312479}]}
{"instance_id": "6ae8453c86be", "generation_result": [{"code": "def pytorch_attention(q, k, v)\n\t\n    try:\n        q = q.reshape(q.shape[0], q.shape[1], -1)\n        k = k.reshape(k.shape[0], k.shape[1], -1)\n        v = v.reshape(v.shape[0], v.shape[1], -1)\n        scale = (int(q.shape[-1])**(-0.5))\n\n        r1 = torch.bmm(q, k.transpose(1,2)) * scale\n        s2 = torch.nn.functional.softmax(r1, dim=2)\n        r2 = torch.bmm(s2, v)\n        return r2.reshape(r2.shape[0], r2.shape[1], *v.shape[2:])\n    except model_management.OOM_EXCEPTION as e:\n        model_management.soft_empty_cache(True)\n        logging.warning(\"out of memory error, switching to slice attention\")\n        return slice_attention(q, k, v)\n\n", "CodeBleu_score": 36.047251822602654}, {"code": "def pytorch_attention(q, k, v)\n\t\n    try:\n        q = q.transpose(1,2)\n        k = k.transpose(1,2)\n        v = v.transpose(1,2)\n        q = q.reshape(q.shape[0], -1, q.shape[2])\n        k = k.reshape(k.shape[0], -1, k.shape[2])\n        v = v.reshape(v.shape[0], -1, v.shape[2])\n        out = torch.bmm(q, k.transpose(1,2))\n        scale = (int(q.shape[-1])**(-0.5))\n        out = out * scale\n        out = torch.nn.functional.softmax(out, dim=2)\n        out = torch.bmm(out, v)\n        out = out.reshape(q.shape[0], q.shape[1], -1)\n        out = out.transpose(1,2)\n        return out\n    except model_management.OOM_EXCEPTION as e:\n        model_management.soft_empty_cache(True)\n        logging.warning(\"out of memory error, switching to slice attention\")\n        return slice_attention(q, k, v)\n\n", "CodeBleu_score": 36.17205894298239}, {"code": "def pytorch_attention(q, k, v)\n\t\n    try:\n        q = q.transpose(1, 2)\n        k = k.transpose(1, 2)\n        v = v.transpose(1, 2)\n        q = q.reshape(q.shape[0], q.shape[1], -1)\n        k = k.reshape(k.shape[0], k.shape[1], -1)\n        v = v.reshape(v.shape[0], v.shape[1], -1)\n        out = torch.nn.functional.softmax(torch.bmm(q, k.transpose(1, 2)) / math.sqrt(k.shape[-1]), dim=2)\n        out = torch.bmm(out, v)\n        out = out.transpose(1, 2)\n        out = out.reshape(out.shape[0], -1, out.shape[2])\n        return out\n    except model_management.OOM_EXCEPTION as e:\n        logging.warning(\"out of memory error, switching to slice attention\")\n        return slice_attention(q, k, v)\n\n", "CodeBleu_score": 39.34119418710124}]}
{"instance_id": "fe4433c987ad", "generation_result": [{"code": "def apply_control(h, control, name)\n\t\n    if control in control_list and name in control_list[control]:\n        try:\n            h += control_list[control].pop(name)\n        except:\n            logging.warning('Unable to add control to h: {}'.format(h))\n    return h\n", "CodeBleu_score": 29.45063080700835}, {"code": "def apply_control(h, control, name)\n\t\n    if control in control_list and control_list[control]:\n        try:\n            h = h + control_list[control]\n        except:\n            logging.warning(\"Unable to add control \" + control + \" to \" + name)\n    return h\n\n", "CodeBleu_score": 26.08987340615454}, {"code": "def apply_control(h, control, name)\n\t\n    if control in control_adjustments and name in control_adjustments[control]:\n        try:\n            h = h + control_adjustments[control][name]\n            control_adjustments[control].pop(name)\n        except:\n            logging.warning(\"Could not apply control adjustment for %s to %s\", control, name)\n    return h\n\n", "CodeBleu_score": 30.5537948821217}]}
{"instance_id": "56da4547c101", "generation_result": [{"code": "def get_codebook_entry(self, indices, shape)\n\t\n    indices = indices.view(-1, 1)\n    shape = [indices.shape[0]] + shape\n    indices = indices.expand(*shape)\n    one_hot = torch.zeros(shape, device=self.embedding.weight.device, dtype=self.embedding.weight.dtype)\n    one_hot.scatter_(1, indices, 1.0)\n    quantized = torch.matmul(one_hot, self.embedding.weight)\n    if len(shape) == 2:\n        quantized = quantized.permute(1, 0)\n    return quantized\n\n", "CodeBleu_score": 40.805166637336875}, {"code": "def get_codebook_entry(self, indices, shape)\n\t\n    z_q = torch.zeros(shape, device=self.device)\n    z_q = z_q.view(-1, self.num_embeddings)\n    z_q.scatter_(1, indices.view(-1, 1), 1)\n    z_q = z_q.view(shape)\n    z_q = z_q.permute(0, 3, 1, 2)\n    return z_q\n", "CodeBleu_score": 36.6420377369757}, {"code": "def get_codebook_entry(self, indices, shape)\n\t\n    indices = indices.unsqueeze(-1).float()\n    encodings = torch.zeros(indices.shape[0], self.n_embed, device=indices.device)\n    encodings.scatter_(-1, indices, 1)\n    quantized = torch.matmul(encodings, self.embed.weight).view(shape)\n    return quantized\n", "CodeBleu_score": 18.2373588784183}]}
{"instance_id": "548f8a1fb37d", "generation_result": [{"code": "def make_noise(self)\n\t\n    if self.log_size > 3:\n        size = 2 ** 3\n        while size < 2 ** self.log_size:\n            self.noises.append(torch.randn(1, 1, size, size, device=self.device))\n            self.noises.append(torch.randn(1, 1, size, size, device=self.device))\n            size *= 2\n    else:\n        self.noises = [torch.randn(1, 1, 4, 4, device=self.device)]\n    return self.noises\n\n", "CodeBleu_score": 26.833486532407}, {"code": "def make_noise(self)\n\t\n    size = 4\n    noise_list = []\n    while size < 2 ** self.log_size:\n        torch.randn(size, size, device=self.device)\n        noise_list.append(torch.randn(size, size, device=self.device))\n        noise_list.append(torch.randn(size, size, device=self.device))\n        size *= 2\n    return noise_list\n", "CodeBleu_score": 20.812265761209616}, {"code": "def make_noise(self)\n\t\n    size_list = [2**i for i in range(3, self.log_size + 1)]\n    noise_list = [torch.randn(1, 1, size, size, device=self.device) for size in size_list]\n    return noise_list\n", "CodeBleu_score": 26.369161158574016}]}
{"instance_id": "42fd805563c6", "generation_result": [{"code": "def mean_latent(self, num_latent)\n\t\n    latent_in = torch.randn(num_latent, self.style_dim, device=self.device)\n    latent_out = self.style_mlp(latent_in)\n    latent_out = latent_out.mean(dim=0, keepdim=True)\n    return latent_out\n", "CodeBleu_score": 33.29359343155579}, {"code": "def mean_latent(self, num_latent)\n\t\n    latent_in = torch.randn(num_latent, self.style_dim).to(self.device)\n    latent_out = self.style_mlp(latent_in)\n    latent_out = latent_out.mean(dim=0, keepdim=True)\n    return latent_out\n", "CodeBleu_score": 25.85371755105382}, {"code": "def mean_latent(self, num_latent)\n\t\n    latent_in = torch.randn(num_latent, self.style_dim, device=self.device)\n    latent_out = self.style_mlp(latent_in)\n    latent_out = latent_out.mean(dim=0, keepdim=True)\n    return latent_out\n\n", "CodeBleu_score": 33.29359343155579}]}
{"instance_id": "2f485fd924af", "generation_result": [{"code": "def color_jitter(img, shift)\n\t\n    # Convert the image to HSV color space\n    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n\n    # Split the image channels\n    h, s, v = cv2.split(hsv)\n\n    # Generate a random jitter value for each channel\n    dh = np.random.uniform(-shift, shift)\n    ds = np.random.uniform(-shift, shift)\n    dv = np.random.uniform(-shift, shift)\n\n    # Add the jitter value to the image channels\n    h = np.clip(h + dh, 0, 255)\n    s = np.clip(s + ds, 0, 255)\n    v = np.clip(v + dv, 0, 255)\n\n    # Merge the image channels\n    hsv = cv2.merge((h, s, v))\n\n    # Convert the image back to BGR color space\n    img = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n\n    return img\n\n", "CodeBleu_score": 21.66163269964712}, {"code": "def color_jitter(img, shift)\n\t\n    # Split the image into its RGB components\n    r, g, b = img.split()\n    # Generate random jitter values for each RGB channel\n    r_jit = np.random.uniform(-shift, shift)\n    g_jit = np.random.uniform(-shift, shift)\n    b_jit = np.random.uniform(-shift, shift)\n    # Add the jitter values to the RGB components\n    r = np.clip(r + r_jit, 0, 1)\n    g = np.clip(g + g_jit, 0, 1)\n    b = np.clip(b + b_jit, 0, 1)\n    # Recombine the image with the modified RGB components\n    img = Image.merge(\"RGB\", (r, g, b))\n    return img\n\n", "CodeBleu_score": 21.69659697846723}, {"code": "def color_jitter(img, shift)\n\t\n    # Convert the image to RGB\n    img = img.astype(\"float32\")\n    # Randomly shift the values of each channel by a random amount\n    img[:, :, 0] += np.random.uniform(-shift, shift)\n    img[:, :, 1] += np.random.uniform(-shift, shift)\n    img[:, :, 2] += np.random.uniform(-shift, shift)\n    # Clip the pixel values to ensure they remain between 0 and 1\n    img = np.clip(img, 0, 1)\n    # Return the jittered image\n    return img\n", "CodeBleu_score": 44.999425461584195}]}
{"instance_id": "c87ccf1fbf48", "generation_result": [{"code": "def feed_data(self, data)\n\t\n    if self.opt['is_train']:\n        self.lq = data['lq'].to(self.device)\n        if 'gt' in data:\n            self.gt = data['gt'].to(self.device)\n        if 'facial_landmarks' in data:\n            self.facial_landmarks = data['facial_landmarks'].to(self.device)\n    else:\n        self.lq = data['lq'].to(self.device)\n        if 'gt' in data and self.opt['val_save_gt']:\n            self.gt = data['gt'].to(self.device)\n        if 'facial_landmarks' in data:\n            self.facial_landmarks = data['facial_landmarks'].to(self.device)\n        if 'path' in data:\n            self.path = data['path']\n\n", "CodeBleu_score": 53.133163364413384}, {"code": "def feed_data(self, data)\n\t\n    if self.opt['data_type'] == 'img':\n        self.lq = data['lq'].to(self.device)\n        if 'gt' in data:\n            self.gt = data['gt'].to(self.device)\n    elif self.opt['data_type'] == 'lmdb':\n        self.var_L = data['lq'].to(self.device)\n        if 'gt' in data:\n            self.var_H = data['gt'].to(self.device)\n    else:\n        self.lq = data['lq'].to(self.device)\n        if 'gt' in data:\n            self.gt = data['gt'].to(self.device)\n        if 'key' in data:\n            self.key = data['key']\n        if 'LQ_path' in data:\n            self.LQ_path = data['LQ_path']\n        if 'GT_path' in data:\n            self.GT_path = data['GT_path']\n        if 'LQ_ori_path' in data:\n            self.LQ_ori_path = data['LQ_ori_path']\n        if 'GT_ori_path' in data:\n            self.GT_ori_path = data['GT_ori_path']\n        if 'LQ_patch_path' in data:\n            self.LQ_patch_path = data['LQ_patch_path']\n        if 'GT_patch_path' in data:\n            self.GT_patch_path = data['GT_patch_path']\n        if 'LQ_patch_loc' in data:\n            self.LQ_patch_loc = data['LQ_patch_loc']\n        if 'GT_patch_loc' in data:\n            self.GT_patch_loc = data['GT_patch_loc']\n        if 'LQ_patch_num' in data:\n            self.LQ_patch_num = data['LQ_patch_num']\n        if 'GT_patch_num' in data:\n            self.GT_patch_num = data['GT_patch_num']\n        if 'LQ_patch_size' in data:\n            self.LQ_patch_size = data['LQ_patch_size']\n        if 'GT_patch_size' in data:\n            self.GT_patch_size", "CodeBleu_score": 48.108586080117334}, {"code": "def feed_data(self, data)\n\t\n    if self.opt['data_type'] == 'img':\n        self.lq = data['lq'].to(self.device)\n        if 'gt' in data:\n            self.gt = data['gt'].to(self.device)\n    elif self.opt['data_type'] == 'lmdb':\n        self.var_L = data['lq'].to(self.device)\n        if 'gt' in data:\n            self.var_H = data['gt'].to(self.device)\n    else:\n        raise ValueError(\n            f'Data type {self.opt[\"data_type\"]} is not supported. Supported: img | lmdb')\n\n    if 'facial_mask' in data:\n        self.facial_mask = data['facial_mask'].to(self.device)\n    if 'facial_mask_gt' in data:\n        self.facial_mask_gt = data['facial_mask_gt'].to(self.device)\n    if 'facial_mask_lq' in data:\n        self.facial_mask_lq = data['facial_mask_lq'].to(self.device)\n\n    if 'eye_mask' in data:\n        self.eye_mask = data['eye_mask'].to(self.device)\n    if 'eye_mask_gt' in data:\n        self.eye_mask_gt = data['eye_mask_gt'].to(self.device)\n    if 'eye_mask_lq' in data:\n        self.eye_mask_lq = data['eye_mask_lq'].to(self.device)\n\n    if 'mouth_mask' in data:\n        self.mouth_mask = data['mouth_mask'].to(self.device)\n    if 'mouth_mask_gt' in data:\n        self.mouth_mask_gt = data['mouth_mask_gt'].to(self.device)\n    if 'mouth_mask_lq' in data:\n        self.mouth_mask_lq = data['mouth_mask_lq'].to(self.device)\n\n    if 'facial_mask_gt' in data:\n        self.facial_mask_gt = data['facial_mask_gt'].to(self.device)\n    if 'facial_mask_lq' in data:\n        self.facial", "CodeBleu_score": 43.84243577172753}]}
{"instance_id": "8f8c7cb13037", "generation_result": [{"code": "def construct_img_pyramid(self)\n\t\n    # img_pyramid = [self.img_gt]\n    img_pyramid = []\n    # print(self.img_gt.shape)\n    # print(self.img_gt.shape[0]//2)\n    for i in range(self.num_scales):\n        img_pyramid.insert(0, F.interpolate(self.img_gt, scale_factor=1.0/(2**(i+1)), mode='bilinear', align_corners=False))\n        # print(img_pyramid[i].shape)\n        # print(img_pyramid[i].shape[0]//2)\n    return img_pyramid\n", "CodeBleu_score": 26.9996958854701}, {"code": "def construct_img_pyramid(self)\n\t\n    img_pyramid = []\n    img_pyramid.append(self.img_gt)\n    for i in range(self.num_scales):\n        img_pyramid.insert(0, F.interpolate(img_pyramid[0], scale_factor=1/2, mode='bilinear', align_corners=False))\n    return img_pyramid\n", "CodeBleu_score": 25.588764120845266}, {"code": "def construct_img_pyramid(self)\n\t\n    self.img_pyramid = [self.img_gt]\n    for i in range(self.num_scales - 1):\n        self.img_pyramid.insert(0, F.interpolate(self.img_pyramid[0], scale_factor=0.5, mode='bilinear', align_corners=True))\n    return self.img_pyramid\n\n", "CodeBleu_score": 25.475665801285725}]}
{"instance_id": "51247274d40c", "generation_result": [{"code": "def save(self, epoch, current_iter)\n\t\n    # save the state of the model\n    torch.save(self.generator.state_dict(), os.path.join(self.save_dir, 'generator_latest.pth'))\n    torch.save(self.generator_ema.state_dict(), os.path.join(self.save_dir, 'generator_EMA_latest.pth'))\n    torch.save(self.discriminator.state_dict(), os.path.join(self.save_dir, 'discriminator_latest.pth'))\n    if self.use_facial_disc:\n        torch.save(self.facial_disc_left_eye.state_dict(), os.path.join(self.save_dir, 'facial_disc_left_eye_latest.pth'))\n        torch.save(self.facial_disc_right_eye.state_dict(), os.path.join(self.save_dir, 'facial_disc_right_eye_latest.pth'))\n        torch.save(self.facial_disc_mouth.state_dict(), os.path.join(self.save_dir, 'facial_disc_mouth_latest.pth'))\n    # save the overall training state\n    torch.save({\n        'epoch': epoch,\n        'current_iter': current_iter,\n        'generator_state_dict': self.generator.state_dict(),\n        'generator_ema_state_dict': self.generator_ema.state_dict(),\n        'discriminator_state_dict': self.discriminator.state_dict(),\n        'facial_disc_left_eye_state_dict': self.facial_disc_left_eye.state_dict() if self.use_facial_disc else None,\n        'facial_disc_right_eye_state_dict': self.facial_disc_right_eye.state_dict() if self.use_facial_disc else None,\n        'facial_disc_mouth_state_dict': self.facial_disc_mouth.state_dict() if self.use_facial_disc else None,\n        'optimizer_g_state_dict': self.optimizer_g.state_dict(),\n        'optimizer_d_state_dict': self.optimizer_d.state_dict(),\n        'optimizer_f_left_", "CodeBleu_score": 32.374466623227825}, {"code": "def save(self, epoch, current_iter)\n\t\n    # Save generator network.\n    self.save_network(self.netG, \"G\", epoch, current_iter)\n    # Save EMA generator network.\n    self.save_network(self.netG_EMA, \"GEMA\", epoch, current_iter)\n    # Save discriminator network.\n    self.save_network(self.netD, \"D\", epoch, current_iter)\n    # Save discriminator networks for facial parts.\n    if self.opt.use_facial_disc:\n        self.save_network(self.netD_left, \"Dleft\", epoch, current_iter)\n        self.save_network(self.netD_right, \"Dright\", epoch, current_iter)\n        self.save_network(self.netD_mouth, \"Dmouth\", epoch, current_iter)\n    # Save training state.\n    self.save_training_state(epoch, current_iter)\n", "CodeBleu_score": 54.35202516895327}, {"code": "def save(self, epoch, current_iter)\n\t\n    if self.save_network:\n        self.save_network_weights(epoch, current_iter)\n    if self.save_training_state:\n        self.save_training_state_weights(epoch, current_iter)\n", "CodeBleu_score": 24.567988212325258}]}
{"instance_id": "afa0dc8128c1", "generation_result": [{"code": "def clean_folder(folder)\n\t\n    try:\n        for item in os.listdir(folder):\n            item_path = os.path.join(folder, item)\n            if os.path.isfile(item_path):\n                os.unlink(item_path)\n            elif os.path.isdir(item_path):\n                shutil.rmtree(item_path)\n    except Exception as e:\n        print(\"Error: %s : %s\" % (item_path, e.strerror))\n", "CodeBleu_score": 48.38551673390683}, {"code": "def clean_folder(folder)\n\t\n    if os.path.exists(folder):\n        for item in os.listdir(folder):\n            path = os.path.join(folder, item)\n            if os.path.isfile(path):\n                try:\n                    os.unlink(path)\n                except Exception as error:\n                    print(f'Failed to delete {path}. Reason: {error}')\n            elif os.path.isdir(path):\n                try:\n                    shutil.rmtree(path)\n                except Exception as error:\n                    print(f'Failed to delete {path}. Reason: {error}')\n    else:\n        print(f'The folder {folder} does not exist.')\n\n", "CodeBleu_score": 49.2051341661276}, {"code": "def clean_folder(folder)\n\t\n    for item in os.listdir(folder):\n        item_path = os.path.join(folder, item)\n        try:\n            if os.path.isfile(item_path):\n                os.unlink(item_path)\n            elif os.path.isdir(item_path):\n                shutil.rmtree(item_path)\n        except Exception as error:\n            print('Failed to delete %s. Reason: %s' % (item_path, error))\n\n", "CodeBleu_score": 44.052409505211735}]}
{"instance_id": "53588b481bdd", "generation_result": [{"code": "def next_batch(self, sample_size=1)\n\t\n    batch = []\n    if sample_size > len(self.memory):\n        sample_size = len(self.memory)\n    batch = random.sample(self.memory, sample_size)\n    for experience in batch:\n        self.memory.remove(experience)\n    return batch\n", "CodeBleu_score": 51.09007999739647}, {"code": "def next_batch(self, sample_size=1)\n\t\n    # If the sample size exceeds the available experiences, it is adjusted to the maximum available.\n    if sample_size > len(self.memory):\n        sample_size = len(self.memory)\n\n    # Randomly selects the specified number of samples from the memory.\n    batch = random.sample(self.memory, sample_size)\n\n    # Removes the samples from the memory.\n    for experience in batch:\n        self.memory.remove(experience)\n\n    return batch\n\n", "CodeBleu_score": 49.02659354588814}, {"code": "def next_batch(self, sample_size=1)\n\t\n    if self.current_index + sample_size >= len(self.buffer):\n        # If the number of desired samples exceeds the available experiences,\n        # then the sample size is reduced to the number of available experiences.\n        sample_size = len(self.buffer) - self.current_index\n\n    # Randomly select the specified number of samples from the list of experiences.\n    batch = random.sample(self.buffer[self.current_index:self.current_index + sample_size], sample_size)\n\n    # Increment the current index by the sample size.\n    self.current_index += sample_size\n\n    return batch\n", "CodeBleu_score": 39.92361123016639}]}
{"instance_id": "6bfa6a4e621c", "generation_result": [{"code": "def find_estimated_top_configs(self)\n\t\n    all_configs = [config for config in self.all_configs]\n    all_configs_flattened = [flatten(config) for config in all_configs]\n    all_configs_flattened_ds = [flattened_ds_config(config) for config in all_configs]\n    all_configs_flattened_ds_np = np.array(all_configs_flattened_ds)\n    all_configs_flattened_ds_np_norm = (all_configs_flattened_ds_np - np.min(all_configs_flattened_ds_np, axis=0)) / (np.max(all_configs_flattened_ds_np, axis=0) - np.min(all_configs_flattened_ds_np, axis=0))\n    all_configs_flattened_ds_np_norm_scaled = all_configs_flattened_ds_np_norm * (self.cost_model.max_configs - self.cost_model.min_configs) + self.cost_model.min_configs\n    all_configs_flattened_ds_np_norm_scaled_cost = self.cost_model.predict(all_configs_flattened_ds_np_norm_scaled)\n    all_configs_flattened_ds_np_norm_scaled_cost_sorted_indices = np.argsort(all_configs_flattened_ds_np_norm_scaled_cost)\n    return all_configs_flattened_ds_np_norm_scaled_cost_sorted_indices[:self.top_k]\n", "CodeBleu_score": 20.04226825475704}, {"code": "def find_estimated_top_configs(self)\n\t\n    flattened_ds_config = flatten(self.all_configs)\n    feature_val = [float(flattened_ds_config[key]) for key in flattened_ds_config.keys()]\n    feature_val = np.array(feature_val).reshape(1, -1)\n    predict = self.cost_model.predict(feature_val)\n    predict = np.argsort(predict)\n    return predict\n\n", "CodeBleu_score": 14.108695267378835}, {"code": "def find_estimated_top_configs(self)\n\t\n    # flatten all configurations\n    flattened_ds_config = {}\n    for config in self.all_configs:\n        flattened_ds_config.update(config)\n    # collect numerical values\n    feature_val = []\n    for key, value in flattened_ds_config.items():\n        if isinstance(value, int) or isinstance(value, float):\n            feature_val.append(value)\n    # convert to numpy array\n    feature_val = np.array(feature_val)\n    # predict values\n    predicted_val = self.cost_model.predict(feature_val.reshape(1, -1))\n    # sort values\n    if self.metric == \"accuracy\":\n        sorted_index = np.argsort(-predicted_val)\n    elif self.metric == \"latency\":\n        sorted_index = np.argsort(predicted_val)\n    else:\n        raise NotImplementedError(\"The metric specified is not implemented\")\n    # return indices of top configurations\n    return sorted_index[:self.top_k]\n", "CodeBleu_score": 38.30758735581}]}
{"instance_id": "d4cf48413a57", "generation_result": [{"code": "def flatten(d, parent_key='', sep='_')\n\t\n    items = []\n    for k, v in d.items():\n        new_key = parent_key + sep + k if parent_key else k\n        if isinstance(v, collections.abc.MutableMapping):\n            items.extend(flatten(v, new_key, sep=sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)\n\n", "CodeBleu_score": 83.2096634628176}, {"code": "def flatten(d, parent_key='', sep='_')\n\t\n    items = []\n    for k, v in d.items():\n        new_key = parent_key + sep + k if parent_key else k\n        if isinstance(v, collections.abc.MutableMapping):\n            items.extend(flatten(v, new_key, sep=sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)\n", "CodeBleu_score": 83.2096634628176}, {"code": "def flatten(d, parent_key='', sep='_')\n\t\n    items = []\n    for k, v in d.items():\n        new_key = parent_key + sep + k if parent_key else k\n        if isinstance(v, collections.abc.MutableMapping):\n            items.extend(flatten(v, new_key, sep=sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)\n\n", "CodeBleu_score": 83.2096634628176}]}
{"instance_id": "c2a514249c25", "generation_result": [{"code": "def run_after_tuning(self)\n\t\n    if self.optimal_cmd is not None:\n        logger.info(\"Running the command after autotuning.\")\n        result = subprocess.Popen(self.optimal_cmd, shell=True)\n        logger.info(\"Command running in the background. Use the following command to check the status of the command:", "CodeBleu_score": 47.0316906863522}, {"code": "def run_after_tuning(self)\n\t\n    if self.optimal_cmd:\n        logger.info(\n            \"The best configuration is found. Run the command below to reproduce this result.\")\n        logger.info(\" \".join(self.optimal_cmd))\n        result = subprocess.Popen(self.optimal_cmd)\n        result.wait()\n    else:\n        logger.info(\n            \"No optimal configuration is found. Please tune the program and run this script again.\")\n", "CodeBleu_score": 55.578856020260346}, {"code": "def run_after_tuning(self)\n\t\n    if self.optimal_cmd is not None:\n        logger.info('Running command after autotuning...')\n        result = subprocess.Popen(self.optimal_cmd, shell=True)\n        result.wait()\n        logger.info('Command after autotuning completed.')\n    else:\n        logger.info('Autotuning did not find an optimal configuration.')\n", "CodeBleu_score": 50.766810748701594}]}
{"instance_id": "61802de52df0", "generation_result": [{"code": "def run_job(self, exp: dict, reservations)\n\t\n    # Set the master port and result directory\n    exp[\"master_port\"] = self.master_port\n    exp[\"result_dir\"] = self.result_dir\n\n    # Update user arguments based on argument mappings\n    user_args = exp[\"user_args\"]\n    for i, arg in enumerate(user_args):\n        if arg in self.arg_mappings:\n            user_args[i] = self.arg_mappings[arg]\n\n    # Run the experiment in a new thread\n    t = threading.Thread(target=self.run_experiment, args=(exp, reservations))\n    t.start()\n\n    # Store the running experiment details with a timestamp\n    self.running_experiments[t] = {\"exp\": exp, \"start_time\": time.time()}\n\n", "CodeBleu_score": 22.36320889317091}, {"code": "def run_job(self, exp: dict, reservations)\n\t\n    # Set the experiment's master port and result directory\n    exp['master_port'] = self.master_port\n    exp['result_dir'] = self.result_dir\n\n    # Update user arguments based on argument mappings\n    user_args = exp['user_args']\n    for arg in user_args:\n        if arg['type'] == 'map':\n            arg['val'] = self.get_val_by_key(arg['val'])\n\n    # Create a new thread to run the experiment\n    t = threading.Thread(target=self.run_job_thread, args=(exp, reservations))\n    t.start()\n\n    # Store the experiment details with a timestamp\n    self.running_experiments.append({\n        'time': time.time(),\n        'exp': exp,\n        'thread': t,\n        'reservations': reservations\n    })\n\n", "CodeBleu_score": 25.061109928019054}, {"code": "def run_job(self, exp: dict, reservations)\n\t\n    exp_id = exp['id']\n    exp_name = exp['name']\n    exp_script = exp['script']\n    exp_args = exp['arguments']\n    exp_result_dir = exp['result_dir']\n    exp_master_port = exp['master_port']\n    exp_user_args = exp['user_args']\n    exp_user_args_mapping = exp['user_args_mapping']\n    exp_user_args_map_type = exp['user_args_map_type']\n    exp_user_args_map_dir = exp['user_args_map_dir']\n    exp_user_args_map_file = exp['user_args_map_file']\n    exp_user_args_map_key = exp['user_args_map_key']\n    exp_user_args_map_val = exp['user_args_map_val']\n    exp_user_args_map_val_type = exp['user_args_map_val_type']\n    exp_user_args_map_val_dir = exp['user_args_map_val_dir']\n    exp_user_args_map_val_file = exp['user_args_map_val_file']\n    exp_user_args_map_val_key = exp['user_args_map_val_key']\n    exp_user_args_map_val_val = exp['user_args_map_val_val']\n    exp_user_args_map_val_val_type = exp['user_args_map_val_val_type']\n    exp_user_args_map_val_val_dir = exp['user_args_map_val_val_dir']\n    exp_user_args_map_val_val_file = exp['user_args_map_val_val_file']\n    exp_user_args_map_val_val_key = exp['user_args_map_val_val_key']\n    exp_user_args_map_val_val_val = exp['user_args_map_val_val_val']\n    exp_user_args_map_val_val_val_type = exp['user_args_map_val_val_val_type']\n    exp_user_args_map_val_val", "CodeBleu_score": 10.015075796953012}]}
{"instance_id": "4884e2e9665c", "generation_result": [{"code": "def save_exp_results_to_database(self, message, ranks=None, path=None):\n\t\n    if dist.is_initialized():\n        if ranks is None:\n            ranks = dist.get_rank()\n        if ranks == 0:\n            message['rank'] = ranks\n            if path is None:\n                path = self.log_path\n            with open(path, 'a') as outfile:\n                json.dump(message, outfile)\n                outfile.write('", "CodeBleu_score": 33.41050656160689}, {"code": "def save_exp_results_to_database(self, message, ranks=None, path=None):\n\t\n    if not dist.is_initialized() or dist.get_rank() == 0:\n        message[\"rank\"] = dist.get_rank() if dist.is_initialized() else 0\n        if ranks is None or message[\"rank\"] in ranks:\n            if path is None:\n                path = self.log_file_path\n            with open(path, \"a\") as outfile:\n                json.dump(message, outfile)\n                outfile.write(\"", "CodeBleu_score": 36.76776233479894}, {"code": "def save_exp_results_to_database(self, message, ranks=None, path=None):\n\t\n    if not dist.is_initialized() or (dist.is_initialized() and dist.get_rank() == 0):\n        if ranks is not None and dist.get_rank() in ranks:\n            message['rank'] = dist.get_rank()\n        with open(path, 'a') as outfile:\n            json.dump(message, outfile)\n            outfile.write('", "CodeBleu_score": 32.874079451752955}]}
{"instance_id": "0429344c3c4d", "generation_result": [{"code": "def _build_tp_other_layer_map(self, layer_index: int)\n\t\n    if not self.layer_files:\n        raise ValueError(\"No layer files found.\")\n    if layer_index >= len(self.layer_files):\n        raise ValueError(\"Layer index out of range.\")\n    files_with_prefix = get_files_with_prefix(self.layer_files[layer_index], self.tp_degree)\n    partition_data = partition_data(files_with_prefix, self.tp_degree)\n    return {i: partition_data[i] for i in range(self.tp_degree)}\n\n", "CodeBleu_score": 40.09961987601507}, {"code": "def _build_tp_other_layer_map(self, layer_index: int)\n\t\n    if self.tp_degree == 1:\n        return {0: self.get_files_with_prefix(f\"layer_{layer_index}\")}\n    else:\n        files = self.get_files_with_prefix(f\"layer_{layer_index}\")\n        if len(files) == 0:\n            raise ValueError(f\"No layer files found for layer index {layer_index}.\")\n        elif len(files) % self.tp_degree != 0:\n            raise ValueError(\n                f\"Layer files for layer index {layer_index} do not have the same number of partitions as the specified tp_degree.\"\n            )\n        else:\n            partition_data = partition_data(files, self.tp_degree)\n            return {i: partition for i, partition in enumerate(partition_data)}\n\n", "CodeBleu_score": 34.2863912266709}, {"code": "def _build_tp_other_layer_map(self, layer_index: int)\n\t\n    if not self.is_tp_layer(layer_index):\n        raise ValueError(\"Layer index is not a TP layer.\")\n    layer_files = self.get_files_with_prefix(f\"layer_{layer_index}_\")\n    if not layer_files:\n        raise ValueError(\"Layer files not found.\")\n    return partition_data(layer_files, self.tp_degree)\n\n", "CodeBleu_score": 26.5486384785198}]}
{"instance_id": "1b65ecb9be93", "generation_result": [{"code": "def system_info()\n\t\n    info = {}\n    info['os'] = platform.system()\n    info['architecture'] = platform.architecture()\n    info['version'] = platform.version()\n    info['processor'] = platform.processor()\n    info['python_version'] = platform.python_version()\n    return info\n", "CodeBleu_score": 43.20433534648268}, {"code": "def system_info()\n\t\n    info = {}\n    info[\"Operating System\"] = platform.system()\n    info[\"Architecture\"] = platform.architecture()[0]\n    info[\"Version\"] = platform.version()\n    info[\"Processor\"] = platform.processor()\n    info[\"Python Version\"] = platform.python_version()\n    return info\n\n", "CodeBleu_score": 42.439738038620455}, {"code": "def system_info()\n\t\n    system_info = {\n        \"Operating System\": platform.system(),\n        \"Architecture\": platform.architecture(),\n        \"Version\": platform.version(),\n        \"Processor\": platform.processor(),\n        \"Python Version\": platform.python_version()\n    }\n    return system_info\n", "CodeBleu_score": 51.39546124963006}]}
{"instance_id": "969bcc5cf3b5", "generation_result": [{"code": "def cuda_info()\n\t\n    print(\"CUDA Version: \" + str(torch.version.cuda))\n    print(\"CUDA is available: \" + str(torch.cuda.is_available()))\n    print(\"Number of GPUs: \" + str(torch.cuda.device_count()))\n    print(\"Device name: \" + torch.cuda.get_device_name(0))\n    print(\"Device name: \" + torch.cuda.get_device_name(1))\n    print(\"Device name: \" + torch.cuda.get_device_name(2))\n    print(\"Device name: \" + torch.cuda.get_device_name(3))\n    print(\"Device name: \" + torch.cuda.get_device_name(4))\n    print(\"Device name: \" + torch.cuda.get_device_name(5))\n    print(\"Device name: \" + torch.cuda.get_device_name(6))\n    print(\"Device name: \" + torch.cuda.get_device_name(7))\n    print(\"Device name: \" + torch.cuda.get_device_name(8))\n    print(\"Device name: \" + torch.cuda.get_device_name(9))\n    print(\"Device name: \" + torch.cuda.get_device_name(10))\n    print(\"Device name: \" + torch.cuda.get_device_name(11))\n    print(\"Device name: \" + torch.cuda.get_device_name(12))\n    print(\"Device name: \" + torch.cuda.get_device_name(13))\n    print(\"Device name: \" + torch.cuda.get_device_name(14))\n    print(\"Device name: \" + torch.cuda.get_device_name(15))\n    print(\"Device name: \" + torch.cuda.get_device_name(16))\n    print(\"Device name: \" + torch.cuda.get_device_name(17))\n    print(\"Device name: \" + torch.cuda.get_device_name(18))\n    print(\"Device name: \" + torch.cuda.get_device_name(19))\n    print(\"Device name: \" + torch.cuda.get_device_name(20))\n    print(\"Device name: \" + torch.cuda.get_device_name(21))\n    print(\"Device name", "CodeBleu_score": 30.73990341915308}, {"code": "def cuda_info()\n\t\n    print(\"CUDA Version: \" + str(torch.version.cuda))\n    print(\"CUDA is available: \" + str(torch.cuda.is_available()))\n    print(\"Number of CUDA devices: \" + str(torch.cuda.device_count()))\n    print(\"Current CUDA device: \" + str(torch.cuda.current_device()))\n    print(\"CUDA device name: \" + str(torch.cuda.get_device_name(0)))\n    print(\"CUDA device name: \" + str(torch.cuda.get_device_name(1)))\n    print(\"CUDA device name: \" + str(torch.cuda.get_device_name(2)))\n    print(\"CUDA device name: \" + str(torch.cuda.get_device_name(3)))\n    print(\"CUDA device name: \" + str(torch.cuda.get_device_name(4)))\n    print(\"CUDA device name: \" + str(torch.cuda.get_device_name(5)))\n    print(\"CUDA device name: \" + str(torch.cuda.get_device_name(6)))\n    print(\"CUDA device name: \" + str(torch.cuda.get_device_name(7)))\n    print(\"CUDA device name: \" + str(torch.cuda.get_device_name(8)))\n    print(\"CUDA device name: \" + str(torch.cuda.get_device_name(9)))\n    print(\"CUDA device name: \" + str(torch.cuda.get_device_name(10)))\n    print(\"CUDA device name: \" + str(torch.cuda.get_device_name(11)))\n    print(\"CUDA device name: \" + str(torch.cuda.get_device_name(12)))\n    print(\"CUDA device name: \" + str(torch.cuda.get_device_name(13)))\n    print(\"CUDA device name: \" + str(torch.cuda.get_device_name(14)))\n    print(\"CUDA device name: \" + str(torch.cuda.get_device_name(15)))\n    print(\"CUDA device name: \" + str(torch.cuda.get_device_name(16)))\n    print(\"CUDA device name: \" + str(torch.cuda.get_device_name(17)))\n    print(\"CUDA device name: \" + str(torch.cuda.get", "CodeBleu_score": 28.96374538899198}, {"code": "def cuda_info()\n\t\n    print(torch.cuda.get_device_name(0))\n    print(torch.cuda.is_available())\n    print(torch.version.cuda)\n", "CodeBleu_score": 17.566564463467284}]}
{"instance_id": "a2ba908a4c70", "generation_result": [{"code": "def compute_encoder_accuracy(dataset_items, encoder_manager)\n\t\n    class_acc_dict = {}\n    for class_name, class_id in encoder_manager.config[\"class_id_to_name\"].items():\n        class_acc_dict[class_name] = []\n    for dataset_item in tqdm(dataset_items):\n        audio_file = dataset_item[\"audio_file\"]\n        embedding = encoder_manager.compute_embedding_from_clip(audio_file)\n        if \"class_id\" in dataset_item:\n            class_id = dataset_item[\"class_id\"]\n            class_name = encoder_manager.config[\"class_id_to_name\"][class_id]\n            if class_name in class_acc_dict:\n                class_acc_dict[class_name].append(1)\n            else:\n                class_acc_dict[class_name] = [1]\n        else:\n            class_id = None\n            class_name = None\n            for class_name_key, class_name_value in encoder_manager.config[\"class_id_to_name\"].items():\n                if class_name_key in dataset_item[\"class_name\"]:\n                    class_id = encoder_manager.config[\"name_to_id\"][class_name_value]\n                    if class_name_value in class_acc_dict:\n                        class_acc_dict[class_name_value].append(1)\n                    else:\n                        class_acc_dict[class_name_value] = [1]\n                    break\n            if class_id is None:\n                raise RuntimeError(\"No class ID or class name in dataset item\")\n        if class_name is not None:\n            if class_id is not None:\n                prediction = encoder_manager.predict(embedding.unsqueeze(0).cuda(), class_id)\n                if prediction == class_id:\n                    class_acc_dict[class_name].append(1)\n                else:\n                    class_acc_dict[class_name].append(0)\n            else:\n                raise RuntimeError(\"No class ID in dataset item\")\n    for class_name, class_acc in class_acc_dict.items():\n        print(f\"Class: {class_name}, Accuracy: {sum(class_acc) / len(class_acc)}\")\n    print(f\"Average accuracy: {sum([sum(v) / len(v) for v in class_acc_dict", "CodeBleu_score": 33.8670970223408}, {"code": "def compute_encoder_accuracy(dataset_items, encoder_manager)\n\t\n    class_acc_dict = {}\n    for item in tqdm(dataset_items):\n        audio_file = item[\"audio_path\"]\n        true_class_name = item[\"class_name\"]\n        if encoder_manager.class_id_to_class_name_mapping is not None:\n            true_class_id = encoder_manager.class_name_to_class_id_mapping[true_class_name]\n        else:\n            true_class_id = None\n        embedding = encoder_manager.compute_embedding_from_clip(audio_file)\n        predicted_class_name = encoder_manager.predict_label(embedding, true_class_id)\n        if true_class_name not in class_acc_dict:\n            class_acc_dict[true_class_name] = []\n        class_acc_dict[true_class_name].append(true_class_name == predicted_class_name)\n    for class_name, accs in class_acc_dict.items():\n        print(f\"Class {class_name} accuracy: {sum(accs) / len(accs)}\")\n    print(f\"Average accuracy: {sum([sum(accs) / len(accs) for accs in class_acc_dict.values()]) / len(class_acc_dict)}\")\n", "CodeBleu_score": 28.453457664610998}, {"code": "def compute_encoder_accuracy(dataset_items, encoder_manager)\n\t\n    class_acc_dict = {}\n    for class_id, class_name in encoder_manager.config['class_id_to_name'].items():\n        class_acc_dict[class_name] = []\n    for item in tqdm(dataset_items):\n        audio_path = item['audio_path']\n        embedding = encoder_manager.compute_embedding_from_clip(audio_path)\n        if 'class_id' in item:\n            true_class_id = item['class_id']\n            true_class_name = encoder_manager.config['class_id_to_name'][true_class_id]\n            if 'class_id_to_name' in encoder_manager.config:\n                predicted_class_name = encoder_manager.config['class_id_to_name'][encoder_manager.predict(embedding)]\n            else:\n                predicted_class_name = encoder_manager.predict(embedding)\n            if predicted_class_name == true_class_name:\n                class_acc_dict[true_class_name].append(1)\n            else:\n                class_acc_dict[true_class_name].append(0)\n        else:\n            if 'class_id_to_name' in encoder_manager.config:\n                predicted_class_name = encoder_manager.config['class_id_to_name'][encoder_manager.predict(embedding)]\n            else:\n                predicted_class_name = encoder_manager.predict(embedding)\n            class_acc_dict[predicted_class_name].append(1)\n    for class_name, acc_list in class_acc_dict.items():\n        accuracy = sum(acc_list) / len(acc_list)\n        print(f'Accuracy for class {class_name}: {accuracy:.4f}')\n    avg_accuracy = sum([sum(acc_list) / len(acc_list) for acc_list in class_acc_dict.values()]) / len(class_acc_dict)\n    print(f'Average accuracy: {avg_accuracy:.4f}')\n", "CodeBleu_score": 36.29875527038014}]}
{"instance_id": "e01ec69ad58a", "generation_result": [{"code": "def adjust_path_and_remove_silence(audio_path)\n\t\n    # get the output path\n    out_path = audio_path.replace(\n        args.input_dir, args.output_dir\n    )  # replace the input directory with the output directory\n\n    # if the output path already exists and force is not specified\n    if exists(out_path) and not args.force:\n        print(\n            f\"> \u2757 The file {out_path} already exists, skipping it. Use --force to overwrite.\"\n        )\n        return out_path, False\n\n    # create the necessary directory structure\n    out_path_dir = dirname(out_path)\n    if not exists(out_path_dir):\n        mkdir(out_path_dir)\n\n    # remove silence from the audio file\n    out_path, is_speech = remove_silence(\n        model_and_utils, audio_path, out_path, vad_sample_rate=args.vad_sample_rate, trim_just_beginning_and_end=args.trim_just_beginning_and_end, use_cuda=args.use_cuda\n    )\n\n    return out_path, is_speech\n\n", "CodeBleu_score": 41.040735445229146}, {"code": "def adjust_path_and_remove_silence(audio_path)\n\t\n    # get the output path\n    out_path = audio_path.replace(input_dir, output_dir)\n    out_path = out_path.replace(\".wav\", \".pt\")\n\n    # if the file already exists and force is not specified, return the output path and a False flag\n    if out_path.exists() and not force:\n        print(f\"> \u2757 {out_path} already exists. Skipping.\")\n        return out_path, False\n\n    # if the output directory does not exist, create it\n    out_path.parent.mkdir(parents=True, exist_ok=True)\n\n    # remove silence from the audio\n    out_path, is_speech = remove_silence(\n        model_and_utils, audio_path, out_path, vad_sample_rate=8000, trim_just_beginning_and_end=True, use_cuda=False\n    )\n\n    # return the output path and the is_speech flag\n    return out_path, is_speech\n\n", "CodeBleu_score": 41.878408200642305}, {"code": "def adjust_path_and_remove_silence(audio_path)\n\t\n    # get the output path\n    out_path = audio_path.replace(args.input_dir, args.output_dir)\n    out_dir = os.path.dirname(out_path)\n    # if the output path already exists and force is not specified, return the output path and False\n    if os.path.exists(out_path) and not args.force:\n        print(f\"> \u2757 {out_path} already exists, skipping\")\n        return out_path, False\n    # otherwise, create the necessary directory structure, remove silence, and save the audio\n    else:\n        print(f\"> Removing silence from {audio_path}\")\n        os.makedirs(out_dir, exist_ok=True)\n        model_and_utils = get_model_and_utils(args.model_name, args.model_path)\n        out_path, is_speech = remove_silence(\n            model_and_utils, audio_path, out_path, vad_sample_rate=args.vad_sample_rate, use_cuda=args.use_cuda\n        )\n        return out_path, is_speech\n\n\n", "CodeBleu_score": 43.22738656508287}]}
{"instance_id": "7e0bdb0feff8", "generation_result": [{"code": "def resample_files(input_dir, output_sr, output_dir=None, file_ext=\"wav\", n_jobs=10)\n\t\n    if output_dir is not None:\n        copytree(input_dir, output_dir)\n    else:\n        output_dir = input_dir\n    files = glob.glob(os.path.join(output_dir, \"**\", \"*.\" + file_ext), recursive=True)\n    print(f\"Resampling {len(files)} files in {input_dir} to {output_sr} Hz...\")\n    with Pool(n_jobs) as p:\n        with tqdm(total=len(files)) as pbar:\n            for i, _ in enumerate(p.imap_unordered(resample_file, files)):\n                pbar.update()\n    print(\"Done!\")\n\n", "CodeBleu_score": 46.95100372826884}, {"code": "def resample_files(input_dir, output_sr, output_dir=None, file_ext=\"wav\", n_jobs=10)\n\t\n    if output_dir:\n        copytree(input_dir, output_dir)\n    audio_files = glob.glob(os.path.join(input_dir, \"**\", \"*.\" + file_ext), recursive=True)\n    with Pool(n_jobs) as p:\n        for i, _ in tqdm(enumerate(p.imap_unordered(partial(resample_file, output_sr=output_sr), audio_files))):\n            pass\n    print(\"Resampling complete.\")\n", "CodeBleu_score": 36.37958707984751}, {"code": "def resample_files(input_dir, output_sr, output_dir=None, file_ext=\"wav\", n_jobs=10)\n\t\n    if output_dir is None:\n        output_dir = input_dir + \"_resampled\"\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    else:\n        print(\"Output directory already exists.\")\n        return\n    print(\"Resampling files in \" + input_dir + \" to \" + str(output_sr) + \" Hz.\")\n    print(\"Copying \" + input_dir + \" to \" + output_dir + \".\")\n    copytree(input_dir, output_dir)\n    print(\"Resampling files in \" + output_dir + \" to \" + str(output_sr) + \" Hz.\")\n    audio_files = glob.glob(os.path.join(output_dir, \"**\", \"*.\" + file_ext), recursive=True)\n    with Pool(n_jobs) as p:\n        with tqdm(total=len(audio_files)) as pbar:\n            for i, _ in enumerate(p.imap_unordered(resample_file, audio_files)):\n                pbar.update()\n    print(\"Resampling complete.\")\n\n", "CodeBleu_score": 52.08550091876201}]}
{"instance_id": "a1351834ca57", "generation_result": [{"code": "def evaluation(model, criterion, data_loader, global_step)\n\t\n    model.eval()\n    total_loss = 0\n    total_correct = 0\n    total_count = 0\n    for i, (inputs, labels) in enumerate(data_loader):\n        inputs = inputs.view(inputs.shape[0], -1)\n        labels = labels.view(-1)\n        inputs = inputs.cuda()\n        labels = labels.cuda()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        total_loss += loss.item()\n        total_correct += torch.sum(torch.argmax(outputs, dim=1) == labels)\n        total_count += len(labels)\n    avg_loss = total_loss / len(data_loader)\n    avg_acc = total_correct.item() / total_count\n    dashboard_logger.eval_stats(global_step, avg_loss, avg_acc)\n    embeddings = model.get_embedding(inputs)\n    fig = plot_embeddings(embeddings, len(data_loader.dataset))\n    dashboard_logger.eval_figures(global_step, fig)\n    return avg_loss\n\n", "CodeBleu_score": 33.82847144832613}, {"code": "def evaluation(model, criterion, data_loader, global_step)\n\t\n    model.eval()\n    dashboard_logger.eval_stats = defaultdict(list)\n    dashboard_logger.eval_figures = []\n    total_loss = 0\n    total_batches = len(data_loader)\n    with torch.no_grad():\n        for batch_idx, (inputs, labels) in enumerate(data_loader):\n            inputs = inputs.view(inputs.shape[0] * inputs.shape[1], inputs.shape[2], inputs.shape[3])\n            labels = labels.view(labels.shape[0] * labels.shape[1])\n            inputs, labels = inputs.cuda(), labels.cuda()\n            outputs = model(inputs)\n            outputs = outputs.detach()\n            loss = criterion(outputs, labels)\n            total_loss += loss.item()\n            dashboard_logger.eval_stats[\"loss\"].append(loss.item())\n            if batch_idx == total_batches - 1:\n                dashboard_logger.eval_figures.append(\n                    plot_embeddings(model.get_embeddings(inputs).cpu(), num_classes_in_batch=10)\n                )\n            dashboard_logger.log_eval_stats(global_step)\n    return total_loss / total_batches\n\n", "CodeBleu_score": 29.018913230815734}, {"code": "def evaluation(model, criterion, data_loader, global_step)\n\t\n    model.eval()\n    total_loss = 0\n    num_batches = 0\n    num_utter_per_class = data_loader.dataset.num_utter_per_class\n    num_classes_in_batch = data_loader.dataset.num_classes_in_batch\n    embeddings = []\n    labels = []\n    with torch.no_grad():\n        for inputs, labels in data_loader:\n            inputs = inputs.view(\n                num_classes_in_batch * num_utter_per_class,\n                data_loader.dataset.num_features_per_utter,\n            )\n            labels = labels.view(num_classes_in_batch * num_utter_per_class)\n            inputs = inputs.cuda()\n            labels = labels.cuda()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            total_loss += loss.item()\n            num_batches += 1\n            embeddings.append(outputs.detach().cpu())\n            labels.append(labels.detach().cpu())\n    total_loss /= num_batches\n    embeddings = torch.cat(embeddings)\n    labels = torch.cat(labels)\n    fig = plot_embeddings(embeddings, num_classes_in_batch)\n    dashboard_logger.eval_stats(global_step, total_loss)\n    dashboard_logger.eval_figures(global_step, fig)\n    return total_loss\n\n", "CodeBleu_score": 38.45992877791238}]}
{"instance_id": "565f8ed09916", "generation_result": [{"code": "def register_config(model_name: str) -> Coqpit\n\t\n    model_name = model_name.lower()\n    if model_name == \"xtts\":\n        try:\n            from .configs.xtts_config import XTTSConfig\n            return XTTSConfig\n        except ModuleNotFoundError:\n            raise ModuleNotFoundError(\"The configuration class for the model \\\"{}\\\" was not found.\".format(model_name))\n    else:\n        try:\n            from .configs.default_config import DefaultConfig\n            return DefaultConfig\n        except ModuleNotFoundError:\n            raise ModuleNotFoundError(\"The configuration class for the model \\\"{}\\\" was not found.\".format(model_name))\n\n", "CodeBleu_score": 28.806804966955657}, {"code": "def register_config(model_name: str) -> Coqpit\n\t\n    model_name = model_name.lower()\n    config_name = to_camel(model_name)\n    if model_name == \"xtts\":\n        config_name += \"Config\"\n    for path in config_paths:\n        try:\n            config = find_module(path, config_name)\n            return config\n        except ModuleNotFoundError:\n            continue\n    raise ModuleNotFoundError(\n        f\"Could not find the configuration class {config_name} in the following paths: {config_paths}\"\n    )\n\n", "CodeBleu_score": 33.44246755662984}, {"code": "def register_config(model_name: str) -> Coqpit\n\t\n    model_name = model_name.lower()\n    config_class = None\n    if model_name == \"xtts\":\n        try:\n            config_class = find_module(\"coqpit.configs\", model_name + \"_config\")\n        except ModuleNotFoundError:\n            pass\n    if config_class is None:\n        try:\n            config_class = find_module(\"coqpit.configs\", model_name)\n        except ModuleNotFoundError:\n            raise ModuleNotFoundError(\n                f\"Could not find a configuration class for {model_name}.\"\n            )\n    return config_class\n\n", "CodeBleu_score": 41.86095149631739}]}
{"instance_id": "0ea7862506a7", "generation_result": [{"code": "def load_config(config_path: str) -> Coqpit\n\t\n    ext = splitext(config_path)[-1]\n    if ext in [\".yaml\", \".yml\"]:\n        with fsspec.open(config_path, \"r\", encoding=\"utf-8\") as f:\n            config_dict = yaml.safe_load(f)\n    elif ext in [\".json\"]:\n        config_dict = read_json_with_comments(config_path)\n    else:\n        raise TypeError(f\" [!] Unknown file extension: {ext}\")\n    model_name = _process_model_name(config_dict)\n    config_class = register_config(model_name)\n    config = config_class.from_dict(config_dict)\n    return config\n", "CodeBleu_score": 47.664687046636814}, {"code": "def load_config(config_path: str) -> Coqpit\n\t\n    with fsspec.open(config_path, \"r\", encoding=\"utf-8\") as f:\n        input_str = f.read()\n    # handle comments but not urls with //\n    input_str = re.sub(r\"(\\\"(?:[^\\\"\\\\]|\\\\.)*\\\")|(/\\*(?:.|[\\", "CodeBleu_score": 12.55834276678433}, {"code": "def load_config(config_path: str) -> Coqpit\n\t\n    # determine file extension\n    ext = splitext(config_path)[-1]\n    # read config\n    if ext == \".yaml\":\n        with fsspec.open(config_path, \"r\", encoding=\"utf-8\") as f:\n            config_dict = yaml.safe_load(f)\n    elif ext == \".json\":\n        config_dict = read_json_with_comments(config_path)\n    else:\n        raise TypeError(f\" [!] Unknown file extension: {ext}\")\n    # get model name\n    model_name = _process_model_name(config_dict)\n    # register config\n    config_class = register_config(model_name)\n    # create config instance\n    config = config_class()\n    # update config\n    config.from_dict(config_dict)\n    return config\n", "CodeBleu_score": 51.40406144034008}]}
{"instance_id": "e0db68b63143", "generation_result": [{"code": "def list_files(basePath, validExts=None, contains=None)\n\t\n    # Setup valid extensions\n    if validExts is None:\n        validExts = []\n\n    # Walk through directory structure\n    for (rootDir, dirNames, filenames) in os.walk(basePath):\n        # Loop through files\n        for filename in filenames:\n            # Determine the file extension\n            ext = filename[filename.rfind('.'):]\n\n            # Check for extension\n            if len(validExts) > 0 and ext.endswith(tuple(validExts)):\n                # Check for string in filename\n                if contains is None or contains in filename:\n                    # Construct the full filepath\n                    filePath = os.path.join(rootDir, filename)\n                    # Yield (full filepath, filename, file extension)\n                    yield (filePath, filename, ext)\n\n", "CodeBleu_score": 54.13199573185224}, {"code": "def list_files(basePath, validExts=None, contains=None)\n\t\n    # setup the list of valid extensions\n    if validExts is None:\n        validExts = ['.jpg', '.jpeg', '.png', '.bmp', '.tiff']\n\n    # loop over the directory structure\n    for (rootDir, dirNames, filenames) in os.walk(basePath):\n        # loop over the filenames in the current directory\n        for filename in filenames:\n            # if the contains string is not none and the filename does not contain\n            # the supplied string, then ignore the file\n            if contains is not None and filename.find(contains) == -1:\n                continue\n\n            # determine the file extension of the current file\n            ext = filename[filename.rfind('.'):]\n\n            # check to see if the file is an image and should be processed\n            if ext.lower() not in validExts:\n                continue\n\n            # construct the path to the image and yield it\n            imagePath = os.path.join(rootDir, filename)\n            yield imagePath\n\n", "CodeBleu_score": 65.92231178637876}, {"code": "def list_files(basePath, validExts=None, contains=None)\n\t\n    # loop over the directory structure\n    for (rootDir, dirNames, filenames) in os.walk(basePath):\n        # loop over the filenames in the current directory\n        for filename in filenames:\n            # if the contains string is not none and the filename does not contain\n            # the supplied string, then ignore the file\n            if contains is not None and filename.find(contains) == -1:\n                continue\n\n            # determine the file extension of the current file\n            ext = filename[filename.rfind(\".\"):].lower()\n\n            # check to see if the file is an image and should be processed\n            if validExts is None or ext.endswith(validExts):\n                # construct the path to the image and yield it\n                imagePath = os.path.join(rootDir, filename).replace(\"\\\\\", \"/\")\n                yield imagePath\n\n", "CodeBleu_score": 73.74255077788857}]}
{"instance_id": "b836fcaf0a27", "generation_result": [{"code": "def load_model(xtts_checkpoint, xtts_config, xtts_vocab)\n\t\n    clear_gpu_cache()\n    if xtts_checkpoint is None or xtts_config is None or xtts_vocab is None:\n        raise ValueError(\"Please provide all 3 paths: XTTS checkpoint, XTTS config, and XTTS vocab.\")\n    else:\n        print(\"Loading XTTS model...\")\n        config = XttsConfig.load_json(xtts_config)\n        xtts = Xtts.init_from_config(config)\n        xtts.load_checkpoint(xtts_config, checkpoint_path=xtts_checkpoint, vocab_path=xtts_vocab)\n        if torch.cuda.is_available():\n            xtts.cuda()\n        print(\"XTTS model loaded successfully.\")\n        return xtts\n", "CodeBleu_score": 50.90792269453181}, {"code": "def load_model(xtts_checkpoint, xtts_config, xtts_vocab)\n\t\n    clear_gpu_cache()\n    if not os.path.exists(xtts_checkpoint):\n        raise ValueError(f\"The provided XTTS checkpoint path ({xtts_checkpoint}) does not exist.\")\n    if not os.path.exists(xtts_config):\n        raise ValueError(f\"The provided XTTS config path ({xtts_config}) does not exist.\")\n    if not os.path.exists(xtts_vocab):\n        raise ValueError(f\"The provided XTTS vocab path ({xtts_vocab}) does not exist.\")\n\n    print(\"Loading XTTS model...\")\n    config = XttsConfig.load_json(xtts_config)\n    xtts_model = Xtts.init_from_config(config)\n    xtts_model.load_checkpoint(config, checkpoint_path=xtts_checkpoint, vocab_path=xtts_vocab)\n    if torch.cuda.is_available():\n        xtts_model.cuda()\n    print(\"XTTS model loaded successfully.\")\n    return xtts_model\n", "CodeBleu_score": 49.15892824645943}, {"code": "def load_model(xtts_checkpoint, xtts_config, xtts_vocab)\n\t\n\n    clear_gpu_cache()\n    if not os.path.exists(xtts_checkpoint):\n        print(\n            f\"Error: The XTTS checkpoint path {xtts_checkpoint} does not exist. Please provide a valid path to the checkpoint file.\"\n        )\n        return\n    if not os.path.exists(xtts_config):\n        print(\n            f\"Error: The XTTS config path {xtts_config} does not exist. Please provide a valid path to the configuration file.\"\n        )\n        return\n    if not os.path.exists(xtts_vocab):\n        print(\n            f\"Error: The XTTS vocab path {xtts_vocab} does not exist. Please provide a valid path to the vocabulary file.\"\n        )\n        return\n\n    # load the XTTS model\n    xtts_config = XttsConfig.load_json(xtts_config)\n    xtts = Xtts.init_from_config(xtts_config)\n    xtts.load_checkpoint(xtts_config, checkpoint_path=xtts_checkpoint, vocab_path=xtts_vocab)\n\n    # move model to GPU\n    if torch.cuda.is_available():\n        xtts.cuda()\n\n    print(\"XTTS model loaded successfully.\")\n    return xtts\n", "CodeBleu_score": 47.27048691941532}]}
{"instance_id": "c5d63aa0df26", "generation_result": [{"code": "def compute_embedding(self, x, num_frames=250, num_eval=10, return_mean=True, l2_norm=True)\n\t\n    if self.torch_spec:\n        num_frames = int(np.ceil(num_frames / self.hop_length))\n        if num_frames > x.shape[1]:\n            num_frames = x.shape[1]\n        offset_range = np.linspace(0, x.shape[1] - num_frames, num_eval)\n        offset_range = np.ceil(offset_range).astype(int)\n        frames_batch = []\n        for offset in offset_range:\n            frames_batch.append(x[:, offset:offset + num_frames])\n        frames_batch = torch.cat(frames_batch, dim=0)\n    else:\n        num_frames = int(np.ceil(num_frames * self.hop_length))\n        if num_frames > x.shape[0]:\n            num_frames = x.shape[0]\n        offset_range = np.linspace(0, x.shape[0] - num_frames, num_eval)\n        offset_range = np.ceil(offset_range).astype(int)\n        frames_batch = []\n        for offset in offset_range:\n            frames_batch.append(x[offset:offset + num_frames])\n        frames_batch = torch.cat(frames_batch, dim=0)\n    if return_mean:\n        return self.inference(frames_batch).mean(dim=0) / np.sqrt(frames_batch.shape[0]) if l2_norm else self.inference(frames_batch).mean(dim=0)\n    else:\n        return self.inference(frames_batch) / np.sqrt(frames_batch.shape[0]) if l2_norm else self.inference(frames_batch)\n\n", "CodeBleu_score": 39.8622349235054}, {"code": "def compute_embedding(self, x, num_frames=250, num_eval=10, return_mean=True, l2_norm=True)\n\t\n    if self.torch_spec:\n        num_frames = int(np.ceil(num_frames / self.hop_length))\n    num_frames = min(num_frames, x.shape[1])\n    eval_offset = np.linspace(0, x.shape[1] - num_frames, num_eval).astype(int)\n    frames_batch = []\n    for offset in eval_offset:\n        frames_batch.append(x[:, offset:offset + num_frames])\n    frames_batch = torch.cat(frames_batch, 0)\n    if return_mean:\n        embeddings = self.inference(frames_batch)\n        if l2_norm:\n            embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)\n        return torch.mean(embeddings, dim=0).detach().numpy()\n    else:\n        return self.inference(frames_batch).detach().numpy()\n\n", "CodeBleu_score": 47.40283801454467}, {"code": "def compute_embedding(self, x, num_frames=250, num_eval=10, return_mean=True, l2_norm=True)\n\t\n    if self.torch_spec:\n        num_frames = int(np.ceil(num_frames / self.hop_length))\n    if num_frames > x.shape[1]:\n        num_frames = x.shape[1]\n    offset_eval = np.linspace(0, x.shape[1] - num_frames, num_eval)\n    frames_batch = []\n    for offset in offset_eval:\n        if self.torch_spec:\n            frames_batch.append(torch.tensor(x[:, int(offset):int(offset + num_frames)]).unsqueeze(0))\n        else:\n            frames_batch.append(x[:, int(offset):int(offset + num_frames)])\n    frames_batch = torch.cat(frames_batch, dim=0)\n    if return_mean:\n        return self.mean(self.inference(frames_batch), dim=0)\n    else:\n        return self.inference(frames_batch)\n", "CodeBleu_score": 43.97532232903392}]}
{"instance_id": "06f5b0335b26", "generation_result": [{"code": "def create_layer(self, block, planes, blocks, stride=1)\n\t\n    downsample = None\n    if stride != 1 or self.inplanes != planes * block.expansion:\n        downsample = nn.Sequential(\n            nn.Conv2d(self.inplanes, planes * block.expansion, kernel_size=1, stride=stride, bias=False),\n            nn.BatchNorm2d(planes * block.expansion),\n        )\n    layers = []\n    layers.append(block(self.inplanes, planes, stride, downsample))\n    self.inplanes = planes * block.expansion\n    for i in range(1, blocks):\n        layers.append(block(self.inplanes, planes))\n    return nn.Sequential(*layers)\n", "CodeBleu_score": 86.47011482327022}, {"code": "def create_layer(self, block, planes, blocks, stride=1)\n\t\n    downsample = None\n    if stride != 1 or self.inplanes != planes * block.expansion:\n        downsample = nn.Sequential(\n            nn.Conv2d(self.inplanes, planes * block.expansion,\n                      kernel_size=1, stride=stride, bias=False),\n            nn.BatchNorm2d(planes * block.expansion),\n        )\n\n    layers = []\n    layers.append(block(self.inplanes, planes, stride, downsample))\n    self.inplanes = planes * block.expansion\n    for i in range(1, blocks):\n        layers.append(block(self.inplanes, planes))\n\n    return nn.Sequential(*layers)\n", "CodeBleu_score": 85.47050566601277}, {"code": "def create_layer(self, block, planes, blocks, stride=1)\n\t\n    downsample = None\n    if stride != 1 or self.inplanes != planes * block.expansion:\n        downsample = nn.Sequential(\n            nn.Conv2d(self.inplanes, planes * block.expansion, kernel_size=1, stride=stride, bias=False),\n            nn.BatchNorm2d(planes * block.expansion),\n        )\n\n    layers = []\n    layers.append(block(self.inplanes, planes, stride, downsample))\n    self.inplanes = planes * block.expansion\n    for i in range(1, blocks):\n        layers.append(block(self.inplanes, planes))\n\n    return nn.Sequential(*layers)\n", "CodeBleu_score": 86.47011482327022}]}
{"instance_id": "9ac60597b633", "generation_result": [{"code": "def plot_embeddings(embeddings, num_classes_in_batch)\n\t\n    num_classes = min(num_classes_in_batch, 10)\n    num_utterances_per_class = int(embeddings.shape[0] / num_classes)\n    embeddings = embeddings[:num_utterances_per_class * num_classes]\n    embeddings = embeddings.reshape(num_classes, num_utterances_per_class, -1)\n    embeddings = np.mean(embeddings, axis=1)\n    reducer = umap.UMAP()\n    embeddings = reducer.fit_transform(embeddings)\n    fig, ax = plt.subplots(figsize=(10, 10))\n    ax.scatter(embeddings[:, 0], embeddings[:, 1], s=0.1)\n    ax.set_title(\"UMAP projection\")\n    set_aspect(ax)\n    plt.tight_layout()\n    plt.savefig(\"umap\")\n    return fig\n", "CodeBleu_score": 32.27779810937366}, {"code": "def plot_embeddings(embeddings, num_classes_in_batch)\n\t\n    num_classes = min(num_classes_in_batch, 10)\n    fig, ax = plt.subplots()\n    plt.title('UMAP projection')\n    plt.gca().set_aspect('equal', 'datalim')\n    plt.axis('off')\n    for i in range(num_classes):\n        plt.scatter(embeddings[i, :, 0], embeddings[i, :, 1], s=0.1)\n    plt.savefig('umap.png')\n    return fig\n", "CodeBleu_score": 19.686898690812892}, {"code": "def plot_embeddings(embeddings, num_classes_in_batch)\n\t\n    num_classes = min(num_classes_in_batch, 10)\n    num_utterances = np.sum(num_classes_in_batch)\n    num_utterances_per_class = num_utterances // num_classes\n    if num_utterances % num_classes != 0:\n        num_utterances_per_class += 1\n    num_utterances_per_class = min(num_utterances_per_class, 10000)\n    num_utterances_per_class = max(num_utterances_per_class, 1000)\n    num_utterances_per_class = num_utterances // num_classes\n    if num_utterances % num_classes != 0:\n        num_utterances_per_class += 1\n    num_utterances_per_class = min(num_utterances_per_class, 10000)\n    num_utterances_per_class = max(num_utterances_per_class, 1000)\n    num_utterances_per_class = num_utterances // num_classes\n    if num_utterances % num_classes != 0:\n        num_utterances_per_class += 1\n    num_utterances_per_class = min(num_utterances_per_class, 10000)\n    num_utterances_per_class = max(num_utterances_per_class, 1000)\n    num_utterances_per_class = num_utterances // num_classes\n    if num_utterances % num_classes != 0:\n        num_utterances_per_class += 1\n    num_utterances_per_class = min(num_utterances_per_class, 10000)\n    num_utterances_per_class = max(num_utterances_per_class, 1000)\n    num_utterances_per_class = num_utterances // num_classes\n    if num_utterances % num_classes != 0:\n        num_utterances_per_class += 1\n    num_utterances_per_class = min(num_utterances_per_class, 10000)\n    num_utterances_per", "CodeBleu_score": 8.889198854913019}]}
{"instance_id": "19c2b4af256d", "generation_result": [{"code": "def calc_cosine_sim(self, dvecs, centroids)\n\t\n    cos_sim_matrix = []\n    for spkr in range(self.num_spkrs):\n        cs_row = []\n        for utt in range(self.num_utts[spkr]):\n            new_centroids = self.calc_new_centroids(dvecs, centroids, spkr, utt)\n            cs_row.append(\n                torch.clamp(\n                    torch.mm(\n                        dvecs[spkr, utt].unsqueeze(0).transpose(0, 1),\n                        new_centroids.transpose(0, 1),\n                    )\n                    / (\n                        torch.norm(dvecs[spkr, utt])\n                        * torch.norm(new_centroids, dim=1)\n                    ),\n                    min=0.0,\n                )\n            )\n        cos_sim_matrix.append(torch.cat(cs_row))\n    return torch.stack(cos_sim_matrix)\n", "CodeBleu_score": 54.25351823173199}, {"code": "def calc_cosine_sim(self, dvecs, centroids)\n\t\n    cos_sim_matrix = []\n    for spkr in range(dvecs.shape[0]):\n        cs_row = []\n        for utt in range(dvecs.shape[1]):\n            new_centroids = self.calc_new_centroids(dvecs, centroids, spkr, utt)\n            cs_row.append(\n                torch.clamp(\n                    torch.mm(\n                        dvecs[spkr, utt].unsqueeze(0).transpose(0, 1),\n                        new_centroids.transpose(0, 1),\n                    )\n                    / (\n                        torch.norm(dvecs[spkr, utt])\n                        * torch.norm(new_centroids, 2, 1)\n                    ),\n                    1e-6,\n                )\n            )\n        cos_sim_matrix.append(torch.cat(cs_row))\n    return torch.stack(cos_sim_matrix)\n\n", "CodeBleu_score": 50.980137545572646}, {"code": "def calc_cosine_sim(self, dvecs, centroids)\n\t\n    cos_sim_matrix = []\n    for spkr in range(len(dvecs)):\n        cs_row = []\n        for utt in range(len(dvecs[spkr])):\n            new_centroids = self.calc_new_centroids(dvecs, centroids, spkr, utt)\n            cs = torch.mm(dvecs[spkr, utt].unsqueeze(0), new_centroids.transpose(0, 1))\n            cs = torch.clamp(cs, min=-1, max=1)\n            cs_row.append(cs)\n        cos_sim_matrix.append(torch.cat(cs_row))\n    return torch.stack(cos_sim_matrix)\n\n", "CodeBleu_score": 39.18819653613547}]}
{"instance_id": "c525b411a1e7", "generation_result": [{"code": " def embed_loss_softmax(self, dvecs, cos_sim_matrix)\n\t\n    losses = []\n    for i in range(len(dvecs)):\n        dvec = dvecs[i]\n        cos_sim_matrix_row = cos_sim_matrix[i]\n        log_softmax_row = F.log_softmax(cos_sim_matrix_row, dim=1)\n        loss = -log_softmax_row[torch.arange(len(cos_sim_matrix_row)), dvec.argmax()]\n        losses.append(loss)\n    return torch.stack(losses)\n", "CodeBleu_score": 35.746086117170215}, {"code": " def embed_loss_softmax(self, dvecs, cos_sim_matrix)\n\t\n    losses = []\n    for dvec in dvecs:\n        loss = F.log_softmax(cos_sim_matrix, dim=1)\n        losses.append(loss)\n    return torch.stack(losses)\n\n", "CodeBleu_score": 22.7508955787615}, {"code": " def embed_loss_softmax(self, dvecs, cos_sim_matrix)\n\t\n    losses = []\n    for i in range(dvecs.shape[0]):\n        L = -F.log_softmax(cos_sim_matrix[i], dim=0)\n        losses.append(L)\n    losses = torch.stack(losses)\n    return losses\n", "CodeBleu_score": 34.36887815600801}]}
{"instance_id": "25ac3945da2e", "generation_result": [{"code": "def embed_loss_contrast(self, dvecs, cos_sim_matrix)\n\t\n    loss = []\n    for dvec in dvecs:\n        loss_row = []\n        for centroid in self.centroids:\n            # Compute the sigmoid of the cosine similarity matrix\n            sigmoid_cos_sim = torch.sigmoid(cos_sim_matrix(dvec, centroid))\n            # Exclude the current centroid\n            mask = torch.ones(len(self.centroids))\n            mask[self.centroids.index(centroid)] = 0\n            sigmoid_cos_sim = sigmoid_cos_sim * mask\n            # Compute the contrast loss\n            loss_row.append(torch.max(torch.zeros(1), sigmoid_cos_sim).sum())\n        loss.append(torch.stack(loss_row))\n    return torch.stack(loss)\n", "CodeBleu_score": 32.48533330707223}, {"code": "def embed_loss_contrast(self, dvecs, cos_sim_matrix)\n\t\n    L_row = []\n    for i in range(dvecs.shape[0]):\n        cos_sim_matrix_row = cos_sim_matrix[i]\n        cos_sim_matrix_row_no_centroid = cos_sim_matrix_row.copy()\n        cos_sim_matrix_row_no_centroid[i] = -10000\n        L_row.append(torch.sigmoid(cos_sim_matrix_row_no_centroid))\n    L = torch.stack(L_row)\n    return L\n", "CodeBleu_score": 27.928453412542588}, {"code": "def embed_loss_contrast(self, dvecs, cos_sim_matrix)\n\t\n    # Calculate the sigmoid of the cosine similarity matrix for each embedding.\n    # The resulting tensor is of shape (batch_size, num_centroids).\n    sigmoid_cos_sim_matrix = torch.sigmoid(cos_sim_matrix)\n\n    # Create an empty list to store the loss for each row.\n    L_row = []\n\n    # Iterate over the embeddings.\n    for i in range(dvecs.shape[0]):\n        # Get the current embedding.\n        dvec = dvecs[i]\n\n        # Create an empty list to store the loss for each centroid.\n        L = []\n\n        # Iterate over the centroids.\n        for j in range(self.num_centroids):\n            # Get the current centroid.\n            centroid = self.centroids[j]\n\n            # Get the cosine similarity between the current embedding and the current centroid.\n            cos_sim = torch.cosine_similarity(dvec, centroid, dim=0)\n\n            # Get the sigmoid of the cosine similarity.\n            sigmoid_cos_sim = sigmoid_cos_sim_matrix[i, j]\n\n            # Calculate the contrast loss.\n            contrast_loss = 1 - sigmoid_cos_sim * cos_sim\n\n            # Append the contrast loss to the list of losses for the current embedding.\n            L.append(contrast_loss)\n\n        # Append the list of losses for the current embedding to the list of losses for each row.\n        L_row.append(L)\n\n    # Stack the losses for each row into a tensor.\n    L_row = torch.stack(L_row)\n\n    # Stack the losses for each row into a tensor.\n    L_row = torch.stack(L_row)\n\n    # Return the final stacked tensor of losses.\n    return L_row\n", "CodeBleu_score": 32.961778849731}]}
{"instance_id": "878d14ffabd2", "generation_result": [{"code": "def forward(self, x, _label=None)\n\t\n    # x.size = (num_speakers, num_utts_per_speaker, dvec_feats)\n    assert len(x.size()) == 3\n    assert x.size(0) > 1\n    assert x.size(1) > 1\n    assert x.size(2) == self.dvec_feats\n    # compute centroids by averaging the embeddings\n    centroids = torch.mean(x, dim=1)\n    # compute the cosine similarity matrix\n    cos_sim_matrix = self.calc_cosine_sim(x, centroids)\n    # scale and shift the cosine similarity matrix\n    scaled_sim_matrix = self.scale * (cos_sim_matrix - self.shift)\n    # compute the embedding loss\n    embed_loss = self.embed_loss(scaled_sim_matrix)\n    # average the embedding loss\n    loss = embed_loss.mean()\n    return loss\n\n", "CodeBleu_score": 47.03691360932357}, {"code": "def forward(self, x, _label=None)\n\t\n    if x.size()[0] < 2:\n        raise ValueError(\"GE2E loss requires at least 2 speakers\")\n    if x.size()[1] < 2:\n        raise ValueError(\"GE2E loss requires at least 2 utterances per speaker\")\n    # calculate the centroids\n    centroids = torch.mean(x, dim=1)\n    # calculate the cosine similarity matrix\n    cos_sim_matrix = self.calc_cosine_sim(x, centroids)\n    # scale and shift the cosine similarity matrix\n    scaled_matrix = cos_sim_matrix * self.scale\n    shifted_matrix = scaled_matrix + self.shift\n    # calculate the embedding loss\n    embed_loss = self.embed_loss(shifted_matrix)\n    # average the embedding loss\n    loss = torch.mean(embed_loss)\n    return loss\n", "CodeBleu_score": 45.530795698998105}, {"code": "def forward(self, x, _label=None)\n\t\n    # ensure the input has at least two utterances per speaker\n    if x.size(1) < 2:\n        return torch.tensor(0.0).to(x.device)\n    # calculate the centroids\n    centroids = self.centroids(x)\n    # calculate the cosine similarity matrix\n    cos_sim_matrix = self.calc_cosine_sim(x, centroids)\n    # scale and shift the cosine similarity matrix\n    scaled_cos_sim_matrix = self.scale * (cos_sim_matrix - self.shift)\n    # calculate the embedding loss\n    embed_loss = self.embed_loss(scaled_cos_sim_matrix)\n    # average the embedding loss\n    embed_loss = embed_loss.mean()\n    # return the mean loss\n    return embed_loss\n\n", "CodeBleu_score": 46.36347950760972}]}
{"instance_id": "86a364c72caa", "generation_result": [{"code": "def get_voices(extra_voice_dirs: List[str] = [])\n\t\n    # Get all files from specified directories\n    voice_files = []\n    for dir in [os.getcwd()] + extra_voice_dirs:\n        voice_files += [\n            os.path.join(dir, f)\n            for f in os.listdir(dir)\n            if os.path.isfile(os.path.join(dir, f))\n        ]\n\n    # Create a dictionary of voice files\n    voices = {}\n    for file in voice_files:\n        # Get voice name\n        voice_name = os.path.basename(os.path.dirname(file))\n        # If voice name not in dictionary, create a new list\n        if voice_name not in voices:\n            voices[voice_name] = []\n        # Add file to list\n        voices[voice_name].append(file)\n\n    # Return dictionary of voice files\n    return voices\n\n", "CodeBleu_score": 29.696380763625196}, {"code": "def get_voices(extra_voice_dirs: List[str] = [])\n\t\n    voices = {}\n    for voice_dir in extra_voice_dirs:\n        for root, dirs, files in os.walk(voice_dir):\n            if len(files) > 0:\n                voice_name = os.path.basename(root)\n                if voice_name not in voices:\n                    voices[voice_name] = []\n                for file in files:\n                    if file.endswith(\".npz\"):\n                        voices[voice_name].append(os.path.join(root, file))\n                    elif file.endswith(\".wav\") or file.endswith(\".mp3\"):\n                        voices[voice_name].append(os.path.join(root, file))\n    return voices\n", "CodeBleu_score": 29.589588967639237}, {"code": "def get_voices(extra_voice_dirs: List[str] = [])\n\t\n    voice_dict = {}\n    for dir_name in [os.getcwd()] + extra_voice_dirs:\n        for subdir, _, file_list in os.walk(dir_name):\n            if not file_list:\n                continue\n            if any(f.endswith(\".npz\") for f in file_list):\n                voice_dict[subdir] = [\n                    os.path.join(subdir, f) for f in file_list if f.endswith(\".npz\")\n                ]\n            else:\n                voice_dict[subdir] = [\n                    os.path.join(subdir, f) for f in file_list if f.endswith(\".wav\") or f.endswith(\".mp3\")\n                ]\n    return voice_dict\n\n", "CodeBleu_score": 28.050329282691578}]}
{"instance_id": "02f20c8917e6", "generation_result": [{"code": "def load_voice(model, voice: str, extra_voice_dirs: List[str] = [])\n\t\n    if voice == \"random\":\n        return None, None, None\n\n    voices = get_voices(extra_voice_dirs)\n    paths = voices[voice]\n\n    # bark only uses a single sample for cloning\n    if len(paths) > 1:\n        raise ValueError(f\"Voice {voice} has multiple paths: {paths}\")\n\n    try:\n        path = voices[voice]\n    except KeyError as e:\n        raise KeyError(f\"Voice {voice} not found in {extra_voice_dirs}\") from e\n\n    if len(paths) == 1 and paths[0].endswith(\".npz\"):\n        return load_npz(path[0])\n\n    audio_path = paths[0]\n    # replace the file extension with .npz\n    output_path = os.path.splitext(audio_path)[0] + \".npz\"\n    generate_voice(audio=audio_path, model=model, output_path=output_path)\n    return load_voice(model, voice, extra_voice_dirs)\n\n", "CodeBleu_score": 96.1921767598096}, {"code": "def load_voice(model, voice: str, extra_voice_dirs: List[str] = [])\n\t\n    if voice == \"random\":\n        return None, None, None\n\n    voices = get_voices(extra_voice_dirs)\n    paths = voices[voice]\n\n    # bark only uses a single sample for cloning\n    if len(paths) > 1:\n        raise ValueError(f\"Voice {voice} has multiple paths: {paths}\")\n\n    try:\n        path = voices[voice]\n    except KeyError as e:\n        raise KeyError(f\"Voice {voice} not found in {extra_voice_dirs}\") from e\n\n    if len(paths) == 1 and paths[0].endswith(\".npz\"):\n        return load_npz(path[0])\n\n    audio_path = paths[0]\n    # replace the file extension with .npz\n    output_path = os.path.splitext(audio_path)[0] + \".npz\"\n    generate_voice(audio=audio_path, model=model, output_path=output_path)\n    return load_voice(model, voice, extra_voice_dirs)\n", "CodeBleu_score": 96.1921767598096}, {"code": "def load_voice(model, voice: str, extra_voice_dirs: List[str] = [])\n\t\n    if voice == \"random\":\n        return None, None, None\n\n    voices = get_voices(extra_voice_dirs)\n    paths = voices[voice]\n\n    # bark only uses a single sample for cloning\n    if len(paths) > 1:\n        raise ValueError(f\"Voice {voice} has multiple paths: {paths}\")\n\n    try:\n        path = voices[voice]\n    except KeyError as e:\n        raise KeyError(f\"Voice {voice} not found in {extra_voice_dirs}\") from e\n\n    if len(paths) == 1 and paths[0].endswith(\".npz\"):\n        return load_npz(path[0])\n\n    audio_path = paths[0]\n    # replace the file extension with .npz\n    output_path = os.path.splitext(audio_path)[0] + \".npz\"\n    generate_voice(audio=audio_path, model=model, output_path=output_path)\n    return load_voice(model, voice, extra_voice_dirs)\n\n", "CodeBleu_score": 96.1921767598096}]}
{"instance_id": "9458e981389f", "generation_result": [{"code": "def _relative_shift(self, pos_score: torch.Tensor) -> torch.Tensor\n\t\n    batch_size, num_heads, seq_length1, seq_length2 = pos_score.size()\n    zero_pad = torch.zeros((batch_size, num_heads, seq_length1, 1), device=pos_score.device, dtype=pos_score.dtype)\n    padded_pos_score = torch.cat([zero_pad, pos_score], dim=-1)\n    padded_pos_score = padded_pos_score.view(batch_size, num_heads, seq_length2 + 1, seq_length1)\n    pos_score = padded_pos_score[:, :, 1:].view_as(pos_score)\n    return pos_score\n", "CodeBleu_score": 58.619028452407605}, {"code": "def _relative_shift(self, pos_score: torch.Tensor) -> torch.Tensor\n\t\n    batch_size, seq_len, _ = pos_score.size()\n    zero_pad = torch.zeros((batch_size, seq_len, 1), device=pos_score.device, dtype=pos_score.dtype)\n    padded_pos_score = torch.cat((zero_pad, pos_score), dim=-1)\n    padded_pos_score = padded_pos_score.view(batch_size, seq_len + 1, 2 * seq_len + 1)\n    pos_score = padded_pos_score[:, :, 1:].view_as(pos_score)\n    return pos_score\n", "CodeBleu_score": 41.41118525853942}, {"code": "def _relative_shift(self, pos_score: torch.Tensor) -> torch.Tensor\n\t\n    batch_size, num_heads, seq_length1, seq_length2 = pos_score.size()\n    zero_pad = torch.zeros((batch_size, num_heads, seq_length1, 1), device=pos_score.device, dtype=pos_score.dtype)\n    padded_pos_score = torch.cat([zero_pad, pos_score], dim=-1)\n    padded_pos_score = padded_pos_score.view(batch_size, num_heads, seq_length2 + 1, seq_length1)\n    pos_score = padded_pos_score[:, :, 1:].view_as(pos_score)\n    return pos_score\n\n", "CodeBleu_score": 58.619028452407605}]}
{"instance_id": "c046d4f00e80", "generation_result": [{"code": "def run_padded_sequence(self, context, lens)\n\t\n    context_embedded = self.embedding(context)\n    context_embedded = context_embedded.transpose(1, 2)\n    context_embedded = context_embedded.unsqueeze(1)\n    conv_out = []\n    for i in range(self.size):\n        conv_out.append(F.relu(self.conv[i](context_embedded)))\n    conv_out = torch.cat(conv_out, dim=1)\n    conv_out = conv_out.squeeze(1)\n    conv_out = conv_out.transpose(0, 1)\n    conv_out = conv_out.transpose(1, 2)\n    conv_out = conv_out.transpose(0, 1)\n    conv_out = torch.nn.utils.rnn.pad_sequence(conv_out, batch_first=True)\n    conv_out = self.dropout(conv_out)\n    return conv_out\n\n", "CodeBleu_score": 22.119317323565557}, {"code": "def run_padded_sequence(self, context, lens)\n\t\n    # Extract the context up to the maximum length\n    context_embedded = [self.embedding(word) for word in context]\n    context_embedded = [word.view(1, len(word), -1) for word in context_embedded]\n    context_embedded = torch.cat(context_embedded, 0)\n    context_embedded = context_embedded.transpose(0, 1)\n    context_embedded = context_embedded.transpose(1, 2)\n    context_embedded = context_embedded.transpose(0, 1)\n    context_embedded = context_embedded.transpose(1, 2)\n    context_embedded = context_embedded.transpose(0, 1)\n    context_embedded = context_embedded.transpose(1, 2)\n    context_embedded = context_embedded.transpose(0, 1)\n    context_embedded = context_embedded.transpose(1, 2)\n    context_embedded = context_embedded.transpose(0, 1)\n    context_embedded = context_embedded.transpose(1, 2)\n    context_embedded = context_embedded.transpose(0, 1)\n    context_embedded = context_embedded.transpose(1, 2)\n    context_embedded = context_embedded.transpose(0, 1)\n    context_embedded = context_embedded.transpose(1, 2)\n    context_embedded = context_embedded.transpose(0, 1)\n    context_embedded = context_embedded.transpose(1, 2)\n    context_embedded = context_embedded.transpose(0, 1)\n    context_embedded = context_embedded.transpose(1, 2)\n    context_embedded = context_embedded.transpose(0, 1)\n    context_embedded = context_embedded.transpose(1, 2)\n    context_embedded = context_embedded.transpose(0, 1)\n    context_embedded = context_embedded.transpose(1, 2)\n    context_embedded = context_embedded.transpose(0, 1)\n    context_embedded = context_embedded.transpose(1, 2)\n    context_embedded = context_embedded.transpose(0, 1)\n    context_embedded = context_embedded.transpose(1, 2)\n    context_embedded = context_embedded.transpose(0, 1)\n    context_embedded = context_embedded.transpose(", "CodeBleu_score": 11.308491594460294}, {"code": "def run_padded_sequence(self, context, lens)\n\t\n    context_embedded = []\n    for i in range(len(context)):\n        context_embedded.append(self.embedding(context[i]))\n    context_embedded = torch.stack(context_embedded)\n    context_embedded = context_embedded.permute(1, 0, 2)\n    context_embedded = F.relu(self.conv(context_embedded))\n    context_embedded = context_embedded.permute(1, 0, 2)\n    context_embedded = context_embedded.split(lens)\n    context_embedded = pad_sequence(context_embedded, batch_first=True)\n    context_embedded = self.dropout(context_embedded)\n    return context_embedded\n", "CodeBleu_score": 30.929183509641568}]}
{"instance_id": "db2644f3c681", "generation_result": [{"code": "def run_unsorted_inputs(self, fn, context, lens)\n\t\n    lens_sorted, indices_sorted = torch.sort(lens, descending=True)\n    lens_sorted = lens_sorted.long()\n    indices_sorted = indices_sorted.long()\n    context_sorted = context.index_select(0, indices_sorted)\n    context_packed = pack_padded_sequence(context_sorted, lens_sorted, batch_first=True, enforce_sorted=False)\n    context_packed = context_packed.to(device=self.device)\n    output_packed, _ = fn(context_packed)\n    output_sorted, _ = pad_packed_sequence(output_packed, batch_first=True)\n    output = output_sorted.index_select(0, indices_sorted)\n    return output\n", "CodeBleu_score": 22.0709607306837}, {"code": "def run_unsorted_inputs(self, fn, context, lens)\n\t\n    sorted_lens, sorted_indices = torch.sort(lens, descending=True)\n    sorted_context = context[sorted_indices]\n    packed_context = pack_padded_sequence(sorted_context, sorted_lens, batch_first=True)\n    packed_outputs, _ = fn(packed_context)\n    unpacked_outputs, _ = pad_packed_sequence(packed_outputs, batch_first=True)\n    outputs = unpacked_outputs[sorted_indices]\n    return outputs\n", "CodeBleu_score": 13.762050590568405}, {"code": "def run_unsorted_inputs(self, fn, context, lens)\n\t\n    lens_sorted, indices = torch.sort(lens, descending=True)\n    context_sorted = context.index_select(0, indices)\n    context_packed = pack_padded_sequence(context_sorted, lens_sorted, batch_first=True)\n    context_unpacked, _ = pad_packed_sequence(fn(context_packed), batch_first=True)\n    context_unpacked = context_unpacked.index_select(0, indices)\n    return context_unpacked\n", "CodeBleu_score": 14.703440796662301}]}
{"instance_id": "ced082bffc63", "generation_result": [{"code": "def remove_weight_norm(self)\n\t\n    self.kernel_predictor.remove_weight_norm()\n    parametrize.remove_parametrizations(self.convt_pre[1], \"weight\")\n    for block in self.conv_blocks:\n        parametrize.remove_parametrizations(block[1], \"weight\")\n", "CodeBleu_score": 70.15353357610886}, {"code": "def remove_weight_norm(self)\n\t\n    self.kernel_predictor.remove_weight_norm()\n    parametrize.remove_parametrizations(self.convt_pre[1], \"weight\")\n    for block in self.conv_blocks:\n        parametrize.remove_parametrizations(block[1], \"weight\")\n\n", "CodeBleu_score": 70.15353357610886}, {"code": "def remove_weight_norm(self)\n\t\n    self.kernel_predictor.remove_weight_norm()\n    parametrize.remove_parametrizations(self.convt_pre[1], \"weight\")\n    for block in self.conv_blocks:\n        parametrize.remove_parametrizations(block[1], \"weight\")\n", "CodeBleu_score": 70.15353357610886}]}
{"instance_id": "b5a39fd97061", "generation_result": [{"code": "def map_mx_to_torch_model(mx_name)\n\t\n    # Convert the name to lowercase\n    name = mx_name.lower()\n    # Replace 'se_' and 'senet_' prefixes with 'gluon_'\n    name = name.replace('se_', 'gluon_se_')\n    name = name.replace('senet_', 'gluon_senet_')\n    # Replace 'inceptionv3' with 'gluon_inceptionv3'\n    name = name.replace('inceptionv3', 'gluon_inceptionv3')\n    # Prefix the name with 'gluon_'\n    name = 'gluon_' + name\n    return name\n", "CodeBleu_score": 34.547002035136}, {"code": "def map_mx_to_torch_model(mx_name)\n\t\n    torch_name = mx_name.lower()\n    torch_name = torch_name.replace('se_', 'senet_')\n    torch_name = torch_name.replace('inceptionv3', 'inception_v3')\n    torch_name = 'gluon_' + torch_name\n    return torch_name\n", "CodeBleu_score": 41.5823364510508}, {"code": "def map_mx_to_torch_model(mx_name)\n\t\n    torch_name = mx_name.lower()\n    torch_name = torch_name.replace('se_', 'senet_')\n    torch_name = torch_name.replace('inceptionv3', 'inception_v3')\n    torch_name = 'gluon_' + torch_name\n    return torch_name\n", "CodeBleu_score": 41.5823364510508}]}
{"instance_id": "7eee5efdbf67", "generation_result": [{"code": "def _num_samples_per_worker(self)\n\t\n    if self.training or self.num_replicas > 1:\n        # Compute the number of samples per worker\n        num_samples_per_worker = self.num_samples // self.num_replicas\n        # Adjust the number of samples per worker to be a multiple of the batch size\n        num_samples_per_worker = math.ceil(num_samples_per_worker / self.batch_size) * self.batch_size\n        # Make sure the number of samples per worker is at least 1\n        num_samples_per_worker = max(num_samples_per_worker, 1)\n        return num_samples_per_worker\n    else:\n        return self.num_samples\n\n", "CodeBleu_score": 37.78577427017925}, {"code": "def _num_samples_per_worker(self)\n\t\n    num_samples = self.num_samples\n    if self.num_replicas > 1:\n        num_samples = math.ceil(num_samples / self.num_replicas)\n    if self.batch_size is not None:\n        num_samples = math.ceil(num_samples / self.batch_size) * self.batch_size\n    return num_samples\n", "CodeBleu_score": 37.474448514062075}, {"code": "def _num_samples_per_worker(self)\n\t\n    if self.batch_size is not None:\n        assert self.batch_size > 0, \"batch_size should be a positive integer value.\"\n        num_samples_per_worker = math.ceil(\n            self.num_samples / self.num_replicas / self.batch_size\n        )\n    else:\n        num_samples_per_worker = math.ceil(\n            self.num_samples / self.num_replicas\n        )\n    return num_samples_per_worker\n\n", "CodeBleu_score": 31.56889747587664}]}
{"instance_id": "a698d65ae190", "generation_result": [{"code": "def find_images_and_targets(\n        folder: str,\n        types: Optional[Union[List, Tuple, Set]] = None,\n        class_to_idx: Optional[Dict] = None,\n        leaf_name_only: bool = True,\n        sort: bool = True\n)\n\t\n    if types is None:\n        types = get_img_extensions()\n    if class_to_idx is None:\n        class_to_idx = {}\n    labels = []\n    filenames = []\n    for root, subdirs, files in os.walk(folder):\n        for filename in files:\n            name, ext = os.path.splitext(filename)\n            if ext.lower() in types:\n                rel_path = os.path.relpath(root, folder)\n                if leaf_name_only:\n                    rel_path = os.path.basename(rel_path)\n                f = os.path.join(rel_path, filename)\n                filenames.append(f)\n                target = rel_path\n                if target not in class_to_idx:\n                    class_to_idx[target] = len(class_to_idx)\n                labels.append(class_to_idx[target])\n    if sort:\n        filenames = sorted(filenames, key=natural_key)\n        labels = sorted(labels)\n    return filenames, labels, class_to_idx\n\n", "CodeBleu_score": 43.33706900144806}, {"code": "def find_images_and_targets(\n        folder: str,\n        types: Optional[Union[List, Tuple, Set]] = None,\n        class_to_idx: Optional[Dict] = None,\n        leaf_name_only: bool = True,\n        sort: bool = True\n)\n\t\n    if types is None:\n        types = get_img_extensions()\n    if class_to_idx is None:\n        class_to_idx = find_classes(folder)\n    filenames = []\n    labels = []\n    for root, _, fnames in sorted(os.walk(folder)):\n        for fname in sorted(fnames):\n            if fname.lower().endswith(types):\n                path = os.path.join(root, fname) if leaf_name_only else root\n                label = class_to_idx[os.path.relpath(path, folder).split(os.sep)[0]]\n                filenames.append(path)\n                labels.append(label)\n    if sort:\n        filenames, labels = list(zip(*sorted(zip(filenames, labels), key=lambda x: natural_key(x[0]))))\n    return filenames, labels, class_to_idx\n\n", "CodeBleu_score": 37.26520391342515}, {"code": "def find_images_and_targets(\n        folder: str,\n        types: Optional[Union[List, Tuple, Set]] = None,\n        class_to_idx: Optional[Dict] = None,\n        leaf_name_only: bool = True,\n        sort: bool = True\n)\n\t\n    if types is None:\n        types = get_img_extensions()\n    if class_to_idx is None:\n        classes = [d.name for d in os.scandir(folder) if d.is_dir()]\n        classes.sort(key=natural_key)\n        class_to_idx = {classes[i]: i for i in range(len(classes))}\n    else:\n        classes = [k for k in class_to_idx.keys()]\n    images = []\n    dir = Path(folder)\n    for target in sorted(classes):\n        d = dir / target\n        if not d.is_dir():\n            continue\n        for root, _, fnames in sorted(os.walk(d, followlinks=True)):\n            for fname in sorted(fnames):\n                if fname.lower().endswith(types):\n                    path = os.path.join(root, fname) if leaf_name_only else root\n                    item = path, class_to_idx[target]\n                    images.append(item)\n    if sort:\n        images.sort(key=lambda x: natural_key(os.path.split(x[0])[0]))\n    return images, classes, class_to_idx\n\n", "CodeBleu_score": 38.18844639847105}]}
{"instance_id": "e7fe43bdecdc", "generation_result": [{"code": "def pytorch_worker_seed()\n\t\n    worker_info = get_worker_info()\n    if worker_info is not None:\n        # favour the seed already created for pytorch dataloader workers if it exists\n        return worker_info.seed\n    # fallback to wds rank based seed\n    return wds.utils.pytorch_worker_seed()\n\n", "CodeBleu_score": 66.80150511570375}, {"code": "def pytorch_worker_seed()\n\t\n    worker_info = get_worker_info()\n    if worker_info is not None:\n        # favour the seed already created for pytorch dataloader workers if it exists\n        return worker_info.seed\n    # fallback to wds rank based seed\n    return wds.utils.pytorch_worker_seed()\n", "CodeBleu_score": 66.80150511570375}, {"code": "def pytorch_worker_seed()\n\t\n    worker_info = get_worker_info()\n    if worker_info is not None:\n        # favour the seed already created for pytorch dataloader workers if it exists\n        return worker_info.seed\n    # fallback to wds rank based seed\n    return wds.utils.pytorch_worker_seed()\n", "CodeBleu_score": 66.80150511570375}]}
{"instance_id": "0ffc7e7ded25", "generation_result": [{"code": "def solarize_add(img, add, thresh=128, **__)\n\t\n    if not isinstance(img, Image.Image):\n        raise TypeError('img must be an instance of Image')\n    if not isinstance(add, int):\n        raise TypeError('add must be an instance of int')\n    if not isinstance(thresh, int):\n        raise TypeError('thresh must be an instance of int')\n    if img.mode not in ('L', 'RGB'):\n        return img\n    lut = []\n    for i in range(256):\n        if i < thresh:\n            lut.append(min(255, i + add))\n        else:\n            lut.append(i)\n    if img.mode == 'L':\n        return img.point(lut)\n    if img.mode == 'RGB':\n        return img.point(lut, 'RGB')\n\n", "CodeBleu_score": 53.96013031712727}, {"code": "def solarize_add(img, add, thresh=128, **__)\n\t\n    if img.mode == 'L':\n        lut = []\n        for i in range(256):\n            if i < thresh:\n                lut.append(min(255, i + add))\n            else:\n                lut.append(i)\n        if img.mode == 'I;16':\n            lut = array(lut, dtype=uint16)\n        else:\n            lut = array(lut, dtype=uint8)\n        return img.point(lut)\n    elif img.mode == 'RGB':\n        return img.point(lambda i: i + add if i < thresh else i)\n    else:\n        return img\n\n", "CodeBleu_score": 59.37361235664671}, {"code": "def solarize_add(img, add, thresh=128, **__)\n\t\n    if not isinstance(img, Image.Image):\n        raise TypeError(\"img should be PIL Image. Got {}\".format(type(img)))\n\n    if not isinstance(add, (int, float)):\n        raise TypeError(\"add should be a single integer or float value. Got {}\".format(type(add)))\n\n    if not isinstance(thresh, int):\n        raise TypeError(\"thresh should be a single integer value. Got {}\".format(type(thresh)))\n\n    if img.mode == \"P\":\n        img = img.convert(\"RGBA\")\n\n    lut = []\n    for i in range(256):\n        if i < thresh:\n            lut.append(min(255, i + add))\n        else:\n            lut.append(i)\n\n    if img.mode in (\"L\", \"RGB\"):\n        if img.mode == \"RGB\" and len(lut) == 256:\n            lut = lut + lut + lut\n        return img.point(lut)\n\n    return img\n\n", "CodeBleu_score": 69.27375984785449}]}
{"instance_id": "5a0d29577a21", "generation_result": [{"code": "def _get_weighted_transforms(transforms: Dict)\n\t\n    transform_names = list(transforms.keys())\n    probabilities = list(transforms.values())\n    probabilities = np.array(probabilities)\n    probabilities = probabilities / np.sum(probabilities)\n    return transform_names, probabilities\n", "CodeBleu_score": 21.131683407893085}, {"code": "def _get_weighted_transforms(transforms: Dict)\n\t\n    transform_names = list(transforms.keys())\n    transform_probs = list(transforms.values())\n    transform_probs = np.array(transform_probs)\n    transform_probs = transform_probs / np.sum(transform_probs)\n    return transform_names, transform_probs\n\n", "CodeBleu_score": 20.3734651568263}, {"code": "def _get_weighted_transforms(transforms: Dict)\n\t\n    transform_names = list(transforms.keys())\n    probabilities = [transforms[key] for key in transform_names]\n    probabilities = np.array(probabilities)\n    probabilities = probabilities / np.sum(probabilities)\n    return transform_names, probabilities\n\n", "CodeBleu_score": 20.11798714890338}]}
{"instance_id": "84bc5658c135", "generation_result": [{"code": "def _apply_basic(self, img, mixing_weights, m)\n\t\n    mixed = np.zeros(img.shape, dtype=np.float32)\n    for i in range(m):\n        img_aug = img.copy()\n        depth = len(self.augmentations)\n        for _ in range(np.random.randint(1, depth)):\n            op = np.random.choice(self.augmentations)\n            img_aug = op(img_aug)\n        # blend the image with the original image using a random weight\n        alpha = np.random.uniform(0, 1)\n        mixed += alpha * img_aug\n        mixed = np.clip(mixed, 0, 255).astype(np.uint8)\n    return mixed\n", "CodeBleu_score": 33.688939671705086}, {"code": "def _apply_basic(self, img, mixing_weights, m)\n\t\n    mixed = np.zeros(img.shape, dtype=np.uint8)\n    for i in range(m):\n        seq = np.random.choice(self.augmentations, p=mixing_weights)\n        for op in seq:\n            img = op(img)\n        mixed += img\n    return Image.fromarray(np.clip(mixed.astype(np.uint8), 0, 255))\n", "CodeBleu_score": 23.162791192800864}, {"code": "def _apply_basic(self, img, mixing_weights, m)\n\t\n    mixed = np.zeros_like(img)\n    for i in range(m):\n        seq = np.random.choice(self.augmentations, p=mixing_weights)\n        for op in seq:\n            img = op(img)\n        mixed += img\n    mixed = np.clip(mixed, 0, 255).astype(np.uint8)\n    return Image.blend(Image.fromarray(mixed), Image.fromarray(img), 1 - m)\n", "CodeBleu_score": 26.428313155774724}]}
{"instance_id": "84b5b752ff04", "generation_result": [{"code": "def adapt_to_chs(x, n)\n\t\n    if not isinstance(x, (tuple, list)):\n        x = tuple([x] * n)\n    else:\n        if len(x) != n:\n            _logger.warning(\"The number of channels in x is not equal to n. The mean of x is being used.\")\n            x = tuple([np.mean(x)] * n)\n        else:\n            assert len(x) == n\n    return x\n\n", "CodeBleu_score": 39.40910845467341}, {"code": "def adapt_to_chs(x, n)\n\t\n    if isinstance(x, (tuple, list)):\n        if len(x) == n:\n            assert len(x) == n\n        elif len(x) == 1:\n            x = list(x) * n\n        else:\n            _logger.warning('The length of the tuple or list is not equal to the required number of channels. The mean of the tuple or list will be used instead.')\n            x = [np.mean(x)] * n\n    else:\n        x = [x] * n\n    return x\n\n", "CodeBleu_score": 37.84228476771233}, {"code": "def adapt_to_chs(x, n)\n\t\n    if isinstance(x, tuple) or isinstance(x, list):\n        if len(x) == n:\n            assert len(x) == n\n        elif len(x) == 1:\n            x = x * n\n        else:\n            _logger.warning('The input tuple or list has length %d, but %d is required. The mean of the input will be used instead.' % (len(x), n))\n            x = [np.mean(x)] * n\n    else:\n        x = (x,) * n\n    return x\n\n", "CodeBleu_score": 38.30037729052508}]}
{"instance_id": "54754b6ba394", "generation_result": [{"code": "def _worker_init(worker_id, worker_seeding='all')\n\t\n    worker_info = get_worker_info()\n    if worker_info is None:\n        raise RuntimeError('DataLoader worker (pid(s) {}) exited unexpectedly'.format(\n            worker_id))\n    # Check that the worker_id matches the worker_info.id.\n    if worker_id != worker_info.id:\n        raise RuntimeError('The worker_id: {} does not match the worker_info.id: {}. Workers are not properly configured.'.format(\n            worker_id, worker_info.id))\n    # Set the seed for reproducibility.\n    if worker_seeding is not None:\n        if isinstance(worker_seeding, str):\n            if worker_seeding == 'all':\n                worker_seed = worker_info.seed % (2**32 - 1)\n                random.seed(worker_seed)\n                torch.manual_seed(worker_seed)\n                worker_seeding = True\n            else:\n                raise ValueError('Unknown worker_seeding: {}'.format(worker_seeding))\n        if worker_seeding:\n            worker_seed = worker_info.seed % (2**32 - 1)\n            random.seed(worker_seed)\n            torch.manual_seed(worker_seed)\n            worker_seeding(worker_seed)\n", "CodeBleu_score": 43.781480959289574}, {"code": "def _worker_init(worker_id, worker_seeding='all')\n\t\n    worker_info = get_worker_info()\n    if worker_info is None:\n        raise RuntimeError('DataLoader worker (pid(s) {}) exited unexpectedly'.format(\n            ', '.join(str(p) for p in worker_id)))\n    if worker_seeding == 'all':\n        worker_seed = worker_info.seed % (2 ** 32)\n        seed(worker_seed)\n        torch.manual_seed(worker_seed)\n        np.random.seed(worker_seed)\n    elif worker_seeding == 'numpy':\n        np.random.seed(worker_info.seed % (2 ** 32))\n    elif callable(worker_seeding):\n        worker_seeding(worker_info.seed % (2 ** 32))\n    else:\n        raise ValueError('Unknown worker_seeding type: {}'.format(worker_seeding))\n\n", "CodeBleu_score": 41.584938200850644}, {"code": "def _worker_init(worker_id, worker_seeding='all')\n\t\n    worker_info = get_worker_info()\n    if worker_info is None:\n        return\n    if worker_seeding == 'all':\n        seed = worker_info.seed % (2 ** 32 - 1)\n    elif worker_seeding is not None:\n        seed = worker_seeding(worker_info.seed)\n    else:\n        seed = worker_info.seed\n    random.seed(seed)\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n", "CodeBleu_score": 50.00292012629516}]}
{"instance_id": "81dfdce940db", "generation_result": [{"code": "def _decode_and_center_crop(image_bytes, image_size, resize_method)\n\t\n    shape = extract_jpeg_shape(image_bytes)\n    image_height = shape[0]\n    image_width = shape[1]\n    padded_center_crop_size = tf.cast(\n        ((image_size / (image_size + CROP_PADDING)) *\n         tf.cast(tf.minimum(image_height, image_width), tf.float32)), tf.int32)\n\n    offset_height = ((image_height - padded_center_crop_size) + 1) // 2\n    offset_width = ((image_width - padded_center_crop_size) + 1) // 2\n    crop_window = tf.stack([offset_height, offset_width, padded_center_crop_size, padded_center_crop_size])\n    image = decode_and_crop_jpeg(image_bytes, crop_window, channels=3)\n    image = tf.image.resize([image], [image_size, image_size], method=resize_method)[0]\n    return image\n\n", "CodeBleu_score": 58.01157599291374}, {"code": "def _decode_and_center_crop(image_bytes, image_size, resize_method)\n\t\n    shape = extract_jpeg_shape(image_bytes)\n    image_height = shape[0]\n    image_width = shape[1]\n    padded_center_crop_size = tf.cast(\n        ((image_size / (image_size + CROP_PADDING)) *\n         tf.cast(tf.minimum(image_height, image_width), tf.float32)),\n        tf.int32)\n\n    offset_height = ((image_height - padded_center_crop_size) + 1) // 2\n    offset_width = ((image_width - padded_center_crop_size) + 1) // 2\n    crop_window = tf.stack([offset_height, offset_width, padded_center_crop_size, padded_center_crop_size])\n    image = tf.image.decode_and_crop_jpeg(image_bytes, crop_window, channels=3)\n    image = tf.image.resize([image], [image_size, image_size], method=resize_method)[0]\n    return image\n\n", "CodeBleu_score": 60.19204154036265}, {"code": "def _decode_and_center_crop(image_bytes, image_size, resize_method)\n\t\n    shape = extract_jpeg_shape(image_bytes)\n    image_height = shape[0]\n    image_width = shape[1]\n    padded_center_crop_size = tf.cast(\n        ((image_size / (image_size + CROP_PADDING)) *\n         tf.cast(tf.minimum(image_height, image_width), tf.float32)), tf.int32)\n\n    offset_height = ((image_height - padded_center_crop_size) + 1) // 2\n    offset_width = ((image_width - padded_center_crop_size) + 1) // 2\n    crop_window = tf.stack([offset_height, offset_width, padded_center_crop_size, padded_center_crop_size])\n    image = decode_and_crop_jpeg(image_bytes, crop_window, channels=3)\n    image = tf.image.resize([image], [image_size, image_size], method=resize_method)[0]\n    return image\n\n", "CodeBleu_score": 58.01157599291374}]}
{"instance_id": "cf16a6d37111", "generation_result": [{"code": "def preprocess_for_train(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    image = _decode_and_random_crop(image_bytes, image_size, interpolation)\n    image = _flip(image)\n    image = tf.reshape(image, [image_size, image_size, 3])\n    if use_bfloat16:\n        image = tf.image.convert_image_dtype(image, dtype=tf.bfloat16)\n    else:\n        image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n    return image\n\n", "CodeBleu_score": 45.62482957011018}, {"code": "def preprocess_for_train(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    image = _decode_and_random_crop(image_bytes, image_size, interpolation)\n    image = _flip(image)\n    image = tf.reshape(image, [image_size, image_size, 3])\n    if use_bfloat16:\n        image = tf.image.convert_image_dtype(image, dtype=tf.bfloat16)\n    return image\n\n", "CodeBleu_score": 36.82995626753441}, {"code": "def preprocess_for_train(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    image = _decode_and_random_crop(image_bytes, image_size, interpolation)\n    image = _flip(image)\n    image = tf.reshape(image, [image_size, image_size, 3])\n    image = convert_image_dtype(image, dtype=tf.bfloat16 if use_bfloat16 else tf.float32)\n    return image\n", "CodeBleu_score": 42.3991536424366}]}
{"instance_id": "78b2a3bd2142", "generation_result": [{"code": "def preprocess_for_eval(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    if interpolation == 'bicubic':\n        resize_method = tf.image.ResizeMethod.BICUBIC\n    elif interpolation == 'nearest':\n        resize_method = tf.image.ResizeMethod.NEAREST_NEIGHBOR\n    else:\n        raise ValueError('Unknown interpolation method: %s' % interpolation)\n\n    image = _decode_and_center_crop(image_bytes, image_size, resize_method)\n    image = tf.reshape(image, [image_size, image_size, 3])\n\n    if use_bfloat16:\n        image = tf.image.convert_image_dtype(image, dtype=tf.bfloat16)\n    else:\n        image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n\n    return image\n\n", "CodeBleu_score": 56.88497801418067}, {"code": "def preprocess_for_eval(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    if interpolation == 'bicubic':\n        resize_method = tf.image.ResizeMethod.BICUBIC\n    elif interpolation == 'area':\n        resize_method = tf.image.ResizeMethod.AREA\n    else:\n        resize_method = tf.image.ResizeMethod.BILINEAR\n\n    image = _decode_and_center_crop(image_bytes, image_size, resize_method)\n    image = tf.reshape(image, [image_size, image_size, 3])\n    image = convert_image_dtype(image, tf.bfloat16 if use_bfloat16 else tf.float32)\n    return image\n\n", "CodeBleu_score": 65.64802204626655}, {"code": "def preprocess_for_eval(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    image = _decode_and_center_crop(image_bytes, image_size, interpolation)\n    image = tf.reshape(image, [image_size, image_size, 3])\n    if use_bfloat16:\n        image = tf.image.convert_image_dtype(image, dtype=tf.bfloat16)\n    else:\n        image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n    return image\n\n", "CodeBleu_score": 44.65539541163259}]}
{"instance_id": "8bd4364a9a0e", "generation_result": [{"code": "def evaluate(self, model, eval_dataloader=None, output_file=\"merged\")\n\t\n    # If no dataloader is specified, use the default one from the model\n    if eval_dataloader is None:\n        eval_dataloader = self.eval_dataloader\n\n    # Prediction loop\n    self.prediction_loop(\n        dataloader=eval_dataloader,\n        description=\"Evaluation\",\n        prediction_loss_only=True,\n        ignore_keys=self.ignore_keys,\n        metric_key_prefix=\"eval\",\n    )\n\n    # Copy the file to the output directory\n    self.copy_eval_file_to_output(output_file)\n\n    # Run evaluation\n    eval_metrics = self.compute_metrics(eval_dataloader)\n\n    # Prefix all keys with eval_\n    for key in list(eval_metrics.keys()):\n        if not key.startswith(\"eval_\"):\n            eval_metrics[f\"eval_{key}\"] = eval_metrics.pop(key)\n\n    return eval_metrics\n", "CodeBleu_score": 46.50517240791839}, {"code": "def evaluate(self, model, eval_dataloader=None, output_file=\"merged\")\n\t\n    if eval_dataloader is None:\n        eval_dataloader = self.eval_dataloader\n    if self.args.past_index >= 0:\n        self._past = None\n\n    # Temporarily disable metric computation, we will do it in the loop here.\n    compute_metrics = self.compute_metrics\n    self.compute_metrics = None\n    eval_loop = self.prediction_loop if self.args.use_legacy_prediction_loop else self.evaluation_loop\n    try:\n        output = eval_loop(model, eval_dataloader, description=\"Evaluation\")\n    finally:\n        self.compute_metrics = compute_metrics\n\n    if self.args.past_index and hasattr(self, \"_past\"):\n        # Clean the state at the end of the evaluation loop\n        delattr(self, \"_past\")\n\n    # Compute metrics on the evaluation data.\n    eval_metrics = self.compute_metrics(eval_dataloader)\n    metrics = {}\n    metrics.update(eval_metrics)\n\n    # Prefix all keys with eval_\n    for key in list(metrics.keys()):\n        if not key.startswith(\"eval_\"):\n            metrics[f\"eval_{key}\"] = metrics.pop(key)\n\n    self.log(metrics)\n\n    return PredictionOutput(predictions=output.predictions, label_ids=output.label_ids, metrics=metrics)\n", "CodeBleu_score": 37.675761221957224}, {"code": "def evaluate(self, model, eval_dataloader=None, output_file=\"merged\")\n\t\n    if eval_dataloader is None:\n        eval_dataloader = self.eval_dataloader\n    model.eval()\n    self.compute_metrics = self.compute_metrics_fn(eval_dataloader.dataset)\n    eval_loop = self.prediction_loop if self.args.use_legacy_prediction_loop else self.evaluation_loop\n    eval_loop_results = eval_loop(\n        model,\n        eval_dataloader,\n        description=\"Evaluation\",\n        # No point gathering the predictions if there are no metrics, otherwise we defer to\n        # self.args.prediction_loss_only\n        prediction_loss_only=True if self.compute_metrics is None else None,\n        ignore_keys=self.args.ignore_keys,\n        metric_key_prefix=self.args.metric_key_prefix,\n    )\n    # The eval loop can potentially be done in two different ways:\n    # 1. Using the built-in prediction_loop (self.prediction_loop) in which case eval_loop_results\n    #    is a PredictionOutput object and we just need to get its predictions.\n    # 2. Using the legacy evaluation_loop (self.evaluation_loop) in which case eval_loop_results is\n    #    a dictionary and we need to get its \"predictions\" key.\n    if isinstance(eval_loop_results, dict):\n        eval_preds = eval_loop_results[\"predictions\"]\n    else:\n        eval_preds = eval_loop_results.predictions\n    # Finally, we call the compute metrics function we set up earlier to get the metrics.\n    metrics = self.compute_metrics(eval_preds)\n    if output_file is not None:\n        self.write_predictions(eval_preds, output_file)\n    return PredictionOutput(predictions=eval_preds, label_ids=eval_loop_results.label_ids, metrics=metrics)\n", "CodeBleu_score": 47.31456934431295}]}
{"instance_id": "22d682c958c0", "generation_result": [{"code": " def print_computed_metrics(self, metrics)\n\t\n    rank1 = metrics['rank1']\n    rank5 = metrics['rank5']\n    miou = metrics['miou']\n    print('Rank1: {:.4f}\\tRank5: {:.4f}\\tMIoU: {:.4f}'.format(rank1, rank5, miou))\n    return\n", "CodeBleu_score": 26.09045648321691}, {"code": " def print_computed_metrics(self, metrics)\n\t\n    rank1 = metrics['rank1']\n    rank5 = metrics['rank5']\n    miou = metrics['miou']\n    print('Rank@1: {0:.4f}", "CodeBleu_score": 16.77495796998422}, {"code": " def print_computed_metrics(self, metrics)\n\t\n    rank1 = metrics['rank1']\n    rank5 = metrics['rank5']\n    miou = metrics['miou']\n    print('Rank1: {0:.4f}, Rank5: {1:.4f}, mIoU: {2:.4f}'.format(rank1, rank5, miou))\n\n", "CodeBleu_score": 26.96493345854188}]}
{"instance_id": "39c1a50b7e7d", "generation_result": [{"code": "def _eval_predictions(self, segments, data)\n\t\n    average_iou = []\n    average_ranks = []\n    for segment, gt in zip(segments, data):\n        iou = []\n        ranks = []\n        for pred in segment:\n            iou.append(self._iou(pred, gt))\n            ranks.append(self._rank(pred, gt))\n        average_iou.append(np.mean(iou))\n        average_ranks.append(np.mean(ranks))\n    rank1 = np.sum([1 for r in average_ranks if r == 1]) / len(average_ranks)\n    rank5 = np.sum([1 for r in average_ranks if r <= 5]) / len(average_ranks)\n    mean_iou = np.mean(average_iou)\n    return rank1, rank5, mean_iou\n", "CodeBleu_score": 40.13934711274959}, {"code": "def _eval_predictions(self, segments, data)\n\t\n    average_ranks = []\n    average_iou = []\n    for segment, data in zip(segments, data):\n        segment_ranks = []\n        segment_iou = []\n        for pred in segment:\n            rank = self._rank(pred, data)\n            iou = self._iou(pred, data)\n            segment_ranks.append(rank)\n            segment_iou.append(iou)\n        average_ranks.append(np.mean(segment_ranks))\n        average_iou.append(np.mean(segment_iou))\n    rank1 = np.sum(np.array(average_ranks) == 1)/len(average_ranks)\n    rank5 = np.sum(np.array(average_ranks) <= 5)/len(average_ranks)\n    mean_iou = np.mean(average_iou)\n    return rank1, rank5, mean_iou\n", "CodeBleu_score": 45.7477775947257}, {"code": "def _eval_predictions(self, segments, data)\n\t\n    average_ranks = []\n    average_iou = []\n    for segment, data in zip(segments, data):\n        rank_iou = []\n        for pred in segment:\n            rank_iou.append((self._rank(pred, data), self._iou(pred, data)))\n        rank_iou.sort(key=lambda x: x[0])\n        rank_iou = rank_iou[:5]\n        rank_iou = [x[1] for x in rank_iou]\n        average_iou.append(np.mean(rank_iou))\n        average_ranks.append(np.sum([self._rank(x, data) for x in segment])/len(segment))\n    return np.mean(average_ranks), np.mean(average_iou)\n\n", "CodeBleu_score": 35.0397320798806}]}
{"instance_id": "8ccf84f8d034", "generation_result": [{"code": "def predict_loop(self, model, eval_dataloader, output_file=None)\n\t\n    self.scores = []\n    model.eval()\n    model.to(self.ctx)\n    torch.no_grad()\n    for batch in eval_dataloader:\n        batch = self.to_ctx(batch, self.ctx)\n        outputs = model(**batch)\n        self.update(batch, outputs)\n    self.finalize(output_file)\n    return self.scores\n", "CodeBleu_score": 29.000086639422527}, {"code": "def predict_loop(self, model, eval_dataloader, output_file=None)\n\t\n    self.scores = []\n    model.eval()\n    model.to(self.ctx)\n    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n        batch = self.to_ctx(batch, self.ctx)\n        with torch.no_grad():\n            outputs = model(**batch)\n        outputs.update(batch)\n        self.update_scores(outputs)\n    self.finalize(output_file)\n    return self.scores\n\n", "CodeBleu_score": 44.145424400007606}, {"code": "def predict_loop(self, model, eval_dataloader, output_file=None)\n\t\n    self.scores = []\n    model.eval()\n    model.to(self.ctx)\n    for batch in tqdm(eval_dataloader, desc=\"Predicting\"):\n        batch = self.to_ctx(batch, self.ctx)\n        with torch.no_grad():\n            outputs = model(**batch)\n        outputs.update(batch)\n        self.finalize(outputs)\n        self.scores.append(outputs)\n    return self.scores\n", "CodeBleu_score": 44.5339433156272}]}
{"instance_id": "e68529ad727d", "generation_result": [{"code": "def _aggregate_scores(self, scores)\n\t\n    assert len(scores) == 2, \"There should be exactly two sets of scores\"\n    scores = np.concatenate(scores, axis=0)\n    self.full_scores = scores\n    return np.matmul(scores[:, :self.hidden_size], scores[:, self.hidden_size:])\n\n", "CodeBleu_score": 32.35952393723286}, {"code": "def _aggregate_scores(self, scores)\n\t\n    assert len(scores) == 2, \"scores should contain two sets of scores\"\n    assert hasattr(self, \"full_scores\"), \"self.full_scores not found. Are you sure you have run self.get_scores()?\"\n    self.full_scores = None\n    scores = [scores[0].detach().cpu().numpy(), scores[1].detach().cpu().numpy()]\n    scores = [np.squeeze(score) for score in scores]\n    scores = [np.reshape(score, (score.shape[0], -1)) for score in scores]\n    scores = [np.transpose(score) for score in scores]\n    scores = [np.reshape(score, (score.shape[0], -1)) for score in scores]\n    scores = [np.transpose(score) for score in scores]\n    scores = [np.reshape(score, (score.shape[0], -1)) for score in scores]\n    scores = [np.transpose(score) for score in scores]\n    scores = [np.reshape(score, (score.shape[0], -1)) for score in scores]\n    scores = [np.transpose(score) for score in scores]\n    scores = [np.reshape(score, (score.shape[0], -1)) for score in scores]\n    scores = [np.transpose(score) for score in scores]\n    scores = [np.reshape(score, (score.shape[0], -1)) for score in scores]\n    scores = [np.transpose(score) for score in scores]\n    scores = [np.reshape(score, (score.shape[0], -1)) for score in scores]\n    scores = [np.transpose(score) for score in scores]\n    scores = [np.reshape(score, (score.shape[0], -1)) for score in scores]\n    scores = [np.transpose(score) for score in scores]\n    scores = [np.reshape(score, (score.shape[0], -1)) for score in scores]\n    scores = [np.transpose(score) for score in scores]\n    scores = [np.reshape(score, (score.shape[0], -1)) for score in scores]\n    scores = [np.transpose(score) for score in scores]\n    scores = [np.reshape(score, (score.", "CodeBleu_score": 25.23406747701945}, {"code": "def _aggregate_scores(self, scores)\n\t\n    assert len(scores) == 2, \"scores must contain exactly two sets of scores\"\n    text_scores, video_scores = scores\n    text_scores = torch.cat(text_scores, dim=0)\n    video_scores = torch.cat(video_scores, dim=0)\n    self.full_scores = None\n    return torch.matmul(text_scores, video_scores.T)\n", "CodeBleu_score": 33.56695484270112}]}
{"instance_id": "e4e1861fc357", "generation_result": [{"code": "def finalize(self, Y_pred, Y_true, output_file=None)\n\t\n    Y_pred = np.concatenate(Y_pred)\n    Y_true = np.concatenate(Y_true)\n    print \"Finalizing predictions...\"\n    if output_file is not None:\n        print \"Saving predictions to\", output_file\n        with open(output_file, \"w\") as f:\n            pickle.dump({\"Y_pred\": Y_pred, \"Y_true\": Y_true}, f)\n    print \"Number of samples:\", len(Y_true)\n    print \"Number of errors:\", np.sum(Y_pred != Y_true)\n    print \"Number of correct predictions:\", np.sum(Y_pred == Y_true)\n    print \"Accuracy:\", np.sum(Y_pred == Y_true) * 100.0 / len(Y_true)\n    print \"Error rate:\", np.sum(Y_pred != Y_true) * 100.0 / len(Y_true)\n    print \"Done.\"\n    return {\"Y_pred\": Y_pred, \"Y_true\": Y_true}\n", "CodeBleu_score": 35.954073907874175}, {"code": "def finalize(self, Y_pred, Y_true, output_file=None)\n\t\n    Y_pred = np.concatenate(Y_pred)\n    Y_true = np.concatenate(Y_true)\n    print(\"Finalizing predictions...\")\n    self.check_predictions(Y_pred, Y_true)\n    if output_file is not None:\n        print(\"Saving predictions...\")\n        self.save_predictions(Y_pred, Y_true, output_file)\n    return {\"Y_pred\": Y_pred, \"Y_true\": Y_true}\n", "CodeBleu_score": 24.891201597322276}, {"code": "def finalize(self, Y_pred, Y_true, output_file=None)\n\t\n    Y_pred = np.concatenate(Y_pred, axis=0)\n    Y_true = np.concatenate(Y_true, axis=0)\n    if output_file is not None:\n        with open(output_file, 'wb') as f:\n            pickle.dump({'Y_pred': Y_pred, 'Y_true': Y_true}, f)\n    print('Predictions finalized.')\n    return {'Y_pred': Y_pred, 'Y_true': Y_true}\n", "CodeBleu_score": 23.87655061058391}]}
{"instance_id": "7df9ab9a4920", "generation_result": [{"code": "def reduce_metrics(logging_outputs) -> None\n\t\n    loss_sum = sum(log.get('loss', 0) for log in logging_outputs)\n    sample_size = sum(log.get('sample_size', 0) for log in logging_outputs)\n    metrics.log_scalar('loss', loss_sum / sample_size, sample_size, round=3)\n", "CodeBleu_score": 52.7910982560264}, {"code": "def reduce_metrics(logging_outputs) -> None\n\t\n    loss_sum = sum(log.get(\"loss\", 0) for log in logging_outputs)\n    ntokens = sum(log.get(\"ntokens\", 0) for log in logging_outputs)\n    sample_size = sum(log.get(\"sample_size\", 0) for log in logging_outputs)\n\n    metrics.log_scalar(\n        \"loss\", loss_sum / sample_size / math.log(2), sample_size, round=3\n    )\n    if sample_size != ntokens:\n        metrics.log_scalar(\n            \"nll_loss\", loss_sum / ntokens / math.log(2), ntokens, round=3\n        )\n        metrics.log_derived(\n            \"ppl\", lambda meters: utils.get_perplexity(meters[\"nll_loss\"].avg)\n        )\n    else:\n        metrics.log_derived(\n            \"ppl\", lambda meters: utils.get_perplexity(meters[\"loss\"].avg)\n        )\n\n    total = utils.item(sum(log.get(\"total\", 0) for log in logging_outputs))\n    if total > 0:\n        metrics.log_scalar(\"total\", total)\n        n_correct = utils.item(\n            sum(log.get(\"n_correct\", 0) for log in logging_outputs)\n        )\n        metrics.log_scalar(\"n_correct\", n_correct)\n        metrics.log_derived(\n            \"accuracy\",\n            lambda meters: round(\n                meters[\"n_correct\"].sum * 100.0 / meters[\"total\"].sum, 3\n            )\n            if meters[\"total\"].sum > 0\n            else float(\"nan\"),\n        )\n\n    if sample_size != ntokens:\n        metrics.log_derived(\n            \"total_tokens\",\n            lambda meters: round(meters[\"total\"].sum, 3)\n            if meters[\"total\"].sum > 0\n            else float(\"nan\"),\n        )\n        metrics.log_derived(\n            \"total_sequences\",\n            lambda meters: round(meters[\"total\"].count, 3)\n            if meters[\"total\"].count > 0\n            else float(\"nan\"),\n        )\n\n", "CodeBleu_score": 45.763493501973265}, {"code": "def reduce_metrics(logging_outputs) -> None\n\t\n    loss_sum = sum(log.get('loss', 0) for log in logging_outputs)\n    sample_size = sum(log.get('sample_size', 0) for log in logging_outputs)\n\n    metrics.log_scalar('loss', loss_sum / sample_size, sample_size, round=3)\n\n", "CodeBleu_score": 52.7910982560264}]}
{"instance_id": "7f9c326a96f7", "generation_result": [{"code": "def _mm_on_the_fly(\n        self,\n        cmasks,\n        vmasks,\n        attention_mask\n    )\n\t\n    if attention_mask is None:\n        attention_mask = self._mm_attention_mask(cmasks, vmasks)\n    # Construct token type IDs by concatenating zeros for visual tokens and ones for text tokens.\n    token_type_ids = torch.cat([torch.zeros_like(cmasks), torch.ones_like(vmasks)], dim=1)\n    return attention_mask, token_type_ids\n\n", "CodeBleu_score": 33.276029921609336}, {"code": "def _mm_on_the_fly(\n        self,\n        cmasks,\n        vmasks,\n        attention_mask\n    )\n\t\n    if attention_mask is None:\n        attention_mask = self._mm_attention_mask(cmasks, vmasks)\n    token_type_ids = torch.cat([cmasks, vmasks], dim=1)\n    return attention_mask, token_type_ids\n\n", "CodeBleu_score": 28.779517304926504}, {"code": "def _mm_on_the_fly(\n        self,\n        cmasks,\n        vmasks,\n        attention_mask\n    )\n\t\n    if attention_mask is None:\n        attention_mask = self._mm_attention_mask(cmasks, vmasks)\n    attention_mask = attention_mask.to(dtype=torch.float)\n    attention_mask = (1.0 - attention_mask) * -10000.0\n    token_type_ids = torch.cat([torch.zeros_like(cmasks), torch.ones_like(vmasks)], dim=1)\n    return attention_mask, token_type_ids\n\n", "CodeBleu_score": 41.00472209626625}]}
{"instance_id": "fa99d2a148a5", "generation_result": [{"code": "def prepare_inputs_for_generation(\n        self,\n        input_ids,\n        input_video_embeds,\n        attention_mask=None,\n        token_type_ids=None,\n        **model_kwargs\n    )\n\t\n    # input_ids: (batch_size, sequence_length)\n    # input_video_embeds: (batch_size, sequence_length, video_embed_dim)\n    # attention_mask: (batch_size, sequence_length)\n    # token_type_ids: (batch_size, sequence_length)\n    # print(input_ids.size())\n    # print(input_video_embeds.size())\n    # print(attention_mask.size())\n    # print(token_type_ids.size())\n    batch_size, sequence_length = input_ids.size()\n    # print(batch_size, sequence_length)\n    video_embed_dim = input_video_embeds.size(-1)\n    # print(video_embed_dim)\n    # print(input_video_embeds.size())\n    # print(input_ids.size())\n    # print(attention_mask.size())\n    # print(token_type_ids.size())\n    # print(input_video_embeds.size())\n    # print(input_ids.size())\n    # print(attention_mask.size())\n    # print(token_type_ids.size())\n    # print(input_video_embeds.size())\n    # print(input_ids.size())\n    # print(attention_mask.size())\n    # print(token_type_ids.size())\n    # print(input_video_embeds.size())\n    # print(input_ids.size())\n    # print(attention_mask.size())\n    # print(token_type_ids.size())\n    # print(input_video_embeds.size())\n    # print(input_ids.size())\n    # print(attention_mask.size())\n    # print(token_type_ids.size())\n    # print(input_video_embeds.size())\n    # print(input_ids.size())\n    # print(attention_mask.size())\n    # print(token_type_ids.size())\n    # print(input_video_embeds.size())\n    # print(input_ids.size())\n    # print(attention_mask.size())\n    # print(token_type_ids.size())\n    # print(input_video_embeds.size())\n    # print(input_ids.size())\n    # print", "CodeBleu_score": 6.20937912994198}, {"code": "def prepare_inputs_for_generation(\n        self,\n        input_ids,\n        input_video_embeds,\n        attention_mask=None,\n        token_type_ids=None,\n        **model_kwargs\n    )\n\t\n    input_ids, input_video_embeds, attention_mask, token_type_ids = self._prepare_encoder_decoder_kwargs_for_generation(\n        input_ids,\n        input_video_embeds,\n        attention_mask=attention_mask,\n        token_type_ids=token_type_ids,\n        **model_kwargs,\n    )\n\n    # cut decoder_input_ids if past is used\n    if self.config.use_cache:\n        input_ids = input_ids[:, -1:]\n        input_video_embeds = input_video_embeds[:, -1:]\n        attention_mask = attention_mask[:, -1:]\n        token_type_ids = token_type_ids[:, -1:]\n\n    return {\n        \"input_ids\": input_ids,\n        \"input_video_embeds\": input_video_embeds,\n        \"attention_mask\": attention_mask,\n        \"token_type_ids\": token_type_ids,\n    }\n\n", "CodeBleu_score": 43.465912339739276}, {"code": "def prepare_inputs_for_generation(\n        self,\n        input_ids,\n        input_video_embeds,\n        attention_mask=None,\n        token_type_ids=None,\n        **model_kwargs\n    )\n\t\n    # input_ids and input_video_embeds should be already padded\n    input_ids_len = input_ids.shape[1]\n    input_video_embeds_len = input_video_embeds.shape[1]\n    assert input_ids_len == input_video_embeds_len\n\n    # get the combined length of input_ids and input_video_embeds\n    combined_len = input_ids_len + input_video_embeds_len\n\n    # modify attention_mask and token_type_ids\n    if attention_mask is not None:\n        attention_mask = attention_mask[:, None, None, :]\n        attention_mask = attention_mask.expand(\n            attention_mask.size(0), 1, combined_len, attention_mask.size(3)\n        )\n        attention_mask = attention_mask[:, :, :combined_len, :]\n    if token_type_ids is not None:\n        token_type_ids = token_type_ids[:, None, None, :]\n        token_type_ids = token_type_ids.expand(\n            token_type_ids.size(0), 1, combined_len\n        )\n        token_type_ids = token_type_ids[:, :, :combined_len]\n\n    # adjust sequence length\n    if self.config.is_encoder_decoder:\n        max_encoder_length = (\n            self.config.max_position_embeddings\n            if self.config.max_position_embeddings is not None\n            else combined_len\n        )\n        if input_ids.shape[1] > max_encoder_length:\n            input_ids = input_ids[:, :max_encoder_length]\n            if attention_mask is not None:\n                attention_mask = attention_mask[:, :, :max_encoder_length, :]\n            if token_type_ids is not None:\n                token_type_ids = token_type_ids[:, :, :max_encoder_length]\n\n        max_decoder_length = (\n            self.config.max_position_embeddings\n            if self.config.max_position_embeddings is not None\n            else combined_len\n        )\n        if (\n            input_video_embeds.shape[1] > max_decoder_length\n            and self.config.is_decoder\n            and not self.config.add_cross_attention\n        ):\n            input_video_embeds", "CodeBleu_score": 40.55803411890172}]}
{"instance_id": "b1b0d25fe620", "generation_result": [{"code": "def _init_pipeline(self, cfg: ConfigType) -> Compose\n\t\n    test_pipeline = cfg.data.test.pipeline\n    for transform in test_pipeline:\n        if transform['type'] == 'LoadImageFromFile':\n            load_image_from_file_transform_idx = self._get_transform_idx(\n                test_pipeline, 'LoadImageFromFile')\n            break\n    else:\n        raise ValueError('The test pipeline must contain a \\'LoadImageFromFile\\' transform')\n    test_pipeline[load_image_from_file_transform_idx]['type'] = 'mmdet.InferencerLoader'\n    return Compose(test_pipeline)\n\n", "CodeBleu_score": 30.890835193529064}, {"code": "def _init_pipeline(self, cfg: ConfigType) -> Compose\n\t\n    pipeline_cfg = cfg['test_pipeline']\n    # Remove the `img_id` key if present in `meta_keys`.\n    if 'meta_keys' in pipeline_cfg:\n        if 'img_id' in pipeline_cfg['meta_keys']:\n            pipeline_cfg['meta_keys'].remove('img_id')\n    # Find the index of the `LoadImageFromFile` transform.\n    load_image_from_file_idx = self._get_transform_idx(pipeline_cfg, 'LoadImageFromFile')\n    if load_image_from_file_idx == -1:\n        raise ValueError('The index of the `LoadImageFromFile` transform in the test pipeline is not found.')\n    # Update the transform type to `mmdet.InferencerLoader` and return the modified pipeline configuration.\n    pipeline_cfg[load_image_from_file_idx]['type'] = 'mmdet.InferencerLoader'\n    return Compose(pipeline_cfg)\n\n", "CodeBleu_score": 42.13393559962695}, {"code": "def _init_pipeline(self, cfg: ConfigType) -> Compose\n\t\n    if cfg is None:\n        cfg = []\n    if isinstance(cfg, list):\n        cfg = {'pipeline': cfg}\n\n    cfg = ConfigDict(cfg)\n    cfg.pipeline = [transform for transform in cfg.pipeline if transform['type'] != 'LoadImageFromFile' or 'img_id' not in transform['meta_keys']]\n    load_image_from_file_idx = self._get_transform_idx(cfg.pipeline, 'LoadImageFromFile')\n    if load_image_from_file_idx == -1:\n        raise ValueError('LoadImageFromFile is not in the pipeline.')\n    cfg.pipeline[load_image_from_file_idx]['type'] = 'mmdet.InferencerLoader'\n    return Compose(cfg.pipeline)\n\n", "CodeBleu_score": 42.650070110665936}]}
{"instance_id": "7967962856eb", "generation_result": [{"code": "def _inputs_to_list(self, inputs: InputsType) -> list\n\t\n    if isinstance(inputs, str):\n        if isdir(inputs):\n            inputs = list_dir_or_file(inputs)\n        else:\n            inputs = [inputs]\n    elif not isinstance(inputs, (list, tuple)):\n        inputs = [inputs]\n    return inputs\n\n", "CodeBleu_score": 37.27664845802001}, {"code": "def _inputs_to_list(self, inputs: InputsType) -> list\n\t\n    if isinstance(inputs, str):\n        if isdir(inputs):\n            inputs = list_dir_or_file(inputs)\n        else:\n            raise FileNotFoundError(f\"{inputs} is not a valid directory or file.\")\n    elif not isinstance(inputs, (list, tuple)):\n        inputs = [inputs]\n    return inputs\n\n", "CodeBleu_score": 38.11508361998871}, {"code": "def _inputs_to_list(self, inputs: InputsType) -> list\n\t\n    if isinstance(inputs, str):\n        inputs = list_dir_or_file(inputs)\n    elif not isinstance(inputs, (list, tuple)):\n        inputs = [inputs]\n    return inputs\n", "CodeBleu_score": 29.798240086807446}]}
{"instance_id": "74f727f06fa0", "generation_result": [{"code": "def _get_chunk_data(self, inputs: Iterable, chunk_size: int)\n\t\n    for chunk in _chunk_iterable(inputs, chunk_size):\n        chunk_data = []\n        for data in chunk:\n            if isinstance(data, dict) and ('img' in data or 'img_path' in data):\n                chunk_data.append(self.pipeline(data))\n            else:\n                chunk_data.append(data)\n        yield chunk_data", "CodeBleu_score": 22.803685759680317}, {"code": "def _get_chunk_data(self, inputs: Iterable, chunk_size: int)\n\t\n    for input in inputs:\n        if isinstance(input, dict):\n            if 'img' in input:\n                yield self.pipeline(input['img'])\n            elif 'img_path' in input:\n                img = read_image(input['img_path'])\n                yield self.pipeline(img)\n        else:\n            yield self.pipeline(input)\n", "CodeBleu_score": 20.4363469798542}, {"code": "def _get_chunk_data(self, inputs: Iterable, chunk_size: int)\n\t\n    for i, data in enumerate(inputs):\n        if i % chunk_size == 0 and i > 0:\n            yield self.pipeline(chunk_data)\n            chunk_data = []\n        if isinstance(data, dict):\n            if \"img\" in data:\n                chunk_data.append(data[\"img\"])\n            elif \"img_path\" in data:\n                chunk_data.append(data[\"img_path\"])\n        else:\n            chunk_data.append(data)\n    yield self.pipeline(chunk_data)\n\n", "CodeBleu_score": 24.329726692094564}]}
{"instance_id": "a43ab51afa59", "generation_result": [{"code": "def postprocess(\n        self,\n        preds: PredType,\n        visualization: Optional[List[np.ndarray]] = None,\n        return_datasamples: bool = False,\n        print_result: bool = False,\n        no_save_pred: bool = False,\n        pred_out_dir: str = '',\n        **kwargs,\n    ) -> Dict\n\t\n    if no_save_pred and pred_out_dir:\n        warnings.warn(\n            'Saving predictions is not supported when no_save_pred is True.'\n        )\n\n    if not no_save_pred:\n        mmcv.mkdir_or_exist(osp.join(pred_out_dir, 'preds'))\n\n    if return_datasamples and pred_out_dir:\n        warnings.warn(\n            'Saving datasamples is not supported when return_datasamples is True.'\n        )\n\n    if visualization is None:\n        visualization = []\n\n    results = []\n    if isinstance(preds, dict):\n        if 'pred_panoptic_seg' in preds:\n            preds = [preds]\n        else:\n            preds = list(preds.values())\n    for i, pred in enumerate(preds):\n        if isinstance(pred, torch.Tensor):\n            pred = pred.cpu().numpy()\n        if isinstance(pred, np.ndarray):\n            if pred.ndim == 3:\n                pred = [pred]\n            else:\n                assert pred.ndim == 4, \\\n                    f'The dimension of prediction must be 3 or 4, but got {pred.ndim}.'\n        else:\n            raise TypeError(\n                f'Invalid type of prediction: {type(pred)}. '\n                'Supported types are: Tensor, np.ndarray, list[np.ndarray].')\n        for _pred in pred:\n            if _pred.shape[0] == 0:\n                continue\n            if _pred.shape[0] == 1:\n                _pred = _pred[0]\n            if return_datasamples:\n                datasample = self.pred2datasample(_pred)\n                if pred_out_dir:\n                    self.pred2dict(datasample, pred_out_dir)\n                else:\n                    results.append(datasample)\n            else:\n                result = self.pred2dict(\n                    DetDataSample(pred_instances=_pred), pred_out_dir)\n                result['visualization'] = visualization[i]\n                results.append(result)\n        if print_result:\n            self.print_result(results)\n    return results\n\n", "CodeBleu_score": 36.66499738193758}, {"code": "def postprocess(\n        self,\n        preds: PredType,\n        visualization: Optional[List[np.ndarray]] = None,\n        return_datasamples: bool = False,\n        print_result: bool = False,\n        no_save_pred: bool = False,\n        pred_out_dir: str = '',\n        **kwargs,\n    ) -> Dict\n\t\n    if no_save_pred and pred_out_dir != '':\n        warnings.warn(\n            'Saving predictions is not supported when no_save_pred is True.')\n    if return_datasamples and pred_out_dir != '':\n        warnings.warn(\n            'Saving datasamples is not supported when return_datasamples is True.'\n        )\n    if visualization is None:\n        visualization = []\n    results = []\n    if isinstance(preds, dict):\n        if 'instances' in preds:\n            preds = [preds]\n    for data_sample, pred in zip(self.data_samples, preds):\n        if return_datasamples:\n            results.append(data_sample)\n        if not no_save_pred:\n            result = self.pred2dict(data_sample, pred_out_dir)\n            results.append(result)\n        if visualization:\n            result = self.pred2vis(data_sample, pred, visualization)\n            results.append(result)\n    if print_result:\n        self.print_results(results)\n    return results\n\n", "CodeBleu_score": 45.46510596638983}, {"code": "def postprocess(\n        self,\n        preds: PredType,\n        visualization: Optional[List[np.ndarray]] = None,\n        return_datasamples: bool = False,\n        print_result: bool = False,\n        no_save_pred: bool = False,\n        pred_out_dir: str = '',\n        **kwargs,\n    ) -> Dict\n\t\n    if not no_save_pred and pred_out_dir == '':\n        raise ValueError('pred_out_dir must be specified to save predictions.')\n\n    if not return_datasamples and pred_out_dir != '':\n        warnings.warn(\n            'Saving datasamples is not supported. Predictions will be saved instead.'\n        )\n\n    if not hasattr(self, 'results'):\n        self.results = []\n\n    if not hasattr(self, 'num_predicted_imgs'):\n        self.num_predicted_imgs = 0\n\n    if visualization is None:\n        visualization = []\n\n    if isinstance(preds, dict):\n        preds = [preds]\n\n    if isinstance(visualization, np.ndarray):\n        visualization = [visualization]\n\n    if isinstance(preds, list):\n        for i, pred in enumerate(preds):\n            if 'pred_panoptic_seg' in pred:\n                pred['pred_panoptic_seg'] = torch.from_numpy(\n                    pred['pred_panoptic_seg'])\n            if 'pred_instances' in pred:\n                pred['pred_instances'] = Instances.from_dict(pred['pred_instances'])\n            if 'pred_keypoints' in pred:\n                pred['pred_keypoints'] = Keypoints.from_dict(pred['pred_keypoints'])\n            if 'pred_global_masks' in pred:\n                pred['pred_global_masks'] = torch.from_numpy(\n                    pred['pred_global_masks'])\n            if 'pred_local_masks' in pred:\n                pred['pred_local_masks'] = torch.from_numpy(\n                    pred['pred_local_masks'])\n            if 'pred_local_keypoints' in pred:\n                pred['pred_local_keypoints'] = torch.from_numpy(\n                    pred['pred_local_keypoints'])\n            if 'pred_local_keypoints_scores' in pred:\n                pred['pred_local_keypoints_scores'] = torch.from_numpy(\n                    pred['pred_local_keypoints_scores'])\n            if 'pred_global_keypoints' in pred:\n                pred['pred_global_keypoints'] = torch.from_numpy(\n                    pred['pred_global_keypoints'])\n            if 'pred_global_keypoints_scores' in pred:\n                pred['pred_global_keypoints_scores'] = torch.from_numpy", "CodeBleu_score": 33.13065167389417}]}
{"instance_id": "a3c2561f199c", "generation_result": [{"code": "def build_test_pipeline(cfg: ConfigType) -> ConfigType\n\t\n    test_pipeline = cfg['dataset']['test']['pipeline']\n    test_pipeline[0] = copy.deepcopy(test_pipeline[0])\n    test_pipeline[0]['transforms'][0]['type'] = 'Resize'\n    test_pipeline[0]['transforms'][0]['size'] = cfg['dataset']['test']['size']\n    test_pipeline[0]['transforms'][0]['interpolation'] = cfg['dataset']['test']['interpolation']\n    test_pipeline[0]['transforms'][0]['keep_ratio'] = cfg['dataset']['test']['keep_ratio']\n    test_pipeline[-1] = copy.deepcopy(test_pipeline[-1])\n    test_pipeline[-1]['transforms'] = test_pipeline[0]['transforms'] + test_pipeline[-1]['transforms']\n    test_pipeline[-1]['transforms'] = test_pipeline[-1]['transforms'][:-1]\n    test_pipeline[-1]['transforms'][-1]['type'] = 'Collect'\n    test_pipeline[-1]['transforms'][-1]['keys'] = ['img', 'gt_semantic_seg']\n    test_pipeline[-1]['transforms'][-1]['meta_keys'] = ['filename', 'ori_filename', 'ori_shape', 'img_shape', 'pad_shape', 'scale_factor', 'flip', 'flip_direction', 'img_norm_cfg']\n    cfg['dataset']['test']['pipeline'] = Compose(test_pipeline)\n    return cfg\n", "CodeBleu_score": 17.24162218957595}, {"code": "def build_test_pipeline(cfg: ConfigType) -> ConfigType\n\t\n    test_pipeline = copy.deepcopy(cfg.dataset.pipeline[0])\n    test_pipeline.transforms[0] = Resize(cfg.dataset.input_size)\n    test_pipeline.transforms.pop()\n    test_pipeline.transforms.append(Resize(cfg.dataset.input_size))\n    test_pipeline = Compose(test_pipeline.transforms)\n    return test_pipeline\n", "CodeBleu_score": 22.20145840047668}, {"code": "def build_test_pipeline(cfg: ConfigType) -> ConfigType\n\t\n    # Copy the first transform in the dataset pipeline.\n    test_pipeline = deepcopy(cfg.dataset.pipeline[0])\n    # Modify the copied transform to include only the 'Resize' transform.\n    test_pipeline.transforms = [\n        transform for transform in test_pipeline.transforms if transform.type == \"Resize\"\n    ]\n    # Copy the last transform in the pipeline.\n    test_pipeline.transforms.extend(deepcopy(cfg.dataset.pipeline[-1].transforms))\n    # Combine the modified first and copied last transforms into a new test pipeline using Compose.\n    test_pipeline = Compose(test_pipeline.transforms)\n    return test_pipeline\n", "CodeBleu_score": 29.315669455365633}]}
{"instance_id": "48a358ab5a38", "generation_result": [{"code": "def inference_mot(model: nn.Module, img: np.ndarray, frame_id: int,\n                  video_len: int) -> SampleList\n\t\n    if isinstance(model, torch.nn.parallel.DistributedDataParallel):\n        model = model.module\n    cfg = model.cfg\n    device = next(model.parameters()).device\n    if device == torch.device('cpu'):\n        assert not any(\n            [isinstance(module, RoIPool) for module in model.modules()]\n        ), 'CPU inference with RoIPool is not supported'\n    data = dict(img=img, frame_id=frame_id, video_len=video_len)\n    data = test_pipeline(data)\n    data = collate([data], samples_per_gpu=1)\n    if next(model.parameters()).is_cuda:\n        # scatter to specified GPU\n        data = scatter(data, [device])[0]\n    else:\n        for m in model.modules():\n            assert not isinstance(\n                m, RoIPool\n            ), 'CPU inference with RoIPool is not supported'\n    with torch.no_grad():\n        result = model(return_loss=False, rescale=True, **data)\n    return result\n\n", "CodeBleu_score": 58.58811621623643}, {"code": "def inference_mot(model: nn.Module, img: np.ndarray, frame_id: int,\n                  video_len: int) -> SampleList\n\t\n    if isinstance(model, nn.parallel.DistributedDataParallel):\n        model = model.module\n\n    if not isinstance(img, np.ndarray):\n        raise TypeError(f'Input type is not a numpy array. Got {type(img)}')\n    # build the input data\n    data = {'img': img, 'frame_id': frame_id, 'original_shape': img.shape,\n            'img_id': 0, 'video_len': video_len}\n    data = test_pipeline(data)\n    data = collate([data], samples_per_gpu=1)\n    if next(model.parameters()).is_cuda:\n        # scatter to specified GPU\n        data = scatter(data, [device])[0]\n    else:\n        for m in model.modules():\n            assert not isinstance(\n                m, RoIPool\n            ), 'CPU inference with RoIPool is not supported currently.'\n\n    # forward the model\n    with torch.no_grad():\n        result = model(return_loss=False, rescale=True, **data)\n    return result\n", "CodeBleu_score": 59.88908832199817}, {"code": "def inference_mot(model: nn.Module, img: np.ndarray, frame_id: int,\n                  video_len: int) -> SampleList\n\t\n    # prepare data\n    data = dict(img=img, frame_id=frame_id, video_len=video_len)\n    # build test_pipeline\n    test_pipeline = build_test_pipeline(model.cfg)\n    # process data\n    data = test_pipeline(data)\n    # forward\n    if next(model.parameters()).is_cuda:\n        with torch.no_grad():\n            results = model(return_loss=False, rescale=True, **data)\n    else:\n        with torch.no_grad():\n            for module in model.modules():\n                if not isinstance(module, (RoIPool, RoIAlign)):\n                    if isinstance(module, (nn.BatchNorm1d, nn.BatchNorm2d)):\n                        module.eval()\n                    else:\n                        module.requires_grad_(False)\n            results = model(return_loss=False, rescale=True, **data)\n    return results\n\n", "CodeBleu_score": 34.17716420147603}]}
{"instance_id": "6c4ea2ce1787", "generation_result": [{"code": "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\n\t\n    while True:\n        total_length = len(tokens_a) + len(tokens_b)\n        if total_length <= max_num_tokens:\n            break\n\n        trunc_tokens = tokens_a if len(tokens_a) > len(tokens_b) else tokens_b\n        assert len(trunc_tokens) >= 1\n\n        # We want to sometimes truncate from the front and sometimes from the\n        # back to add more randomness and avoid biases.\n        if rng.random() < 0.5:\n            del trunc_tokens[0]\n        else:\n            trunc_tokens.pop()\n", "CodeBleu_score": 79.96334663787925}, {"code": "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\n\t\n    while True:\n        total_length = len(tokens_a) + len(tokens_b)\n        if total_length <= max_num_tokens:\n            break\n        if len(tokens_a) > len(tokens_b):\n            trunc_tokens = tokens_a\n            assert len(trunc_tokens) >= 1\n            # We want to sometimes truncate from the front and sometimes from the\n            # back to add more randomness and avoid biases.\n            if rng.random() < 0.5:\n                del trunc_tokens[0]\n            else:\n                trunc_tokens.pop()\n        else:\n            trunc_tokens = tokens_b\n            assert len(trunc_tokens) >= 1\n            if rng.random() < 0.5:\n                trunc_tokens.pop()\n            else:\n                del trunc_tokens[0]\n    return tokens_a, tokens_b\n\n", "CodeBleu_score": 61.885685584884065}, {"code": "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\n\t\n    while True:\n        total_length = len(tokens_a) + len(tokens_b)\n        if total_length <= max_num_tokens:\n            break\n\n        trunc_tokens = tokens_a if len(tokens_a) > len(tokens_b) else tokens_b\n        assert len(trunc_tokens) >= 1\n\n        # We want to sometimes truncate from the front and sometimes from the\n        # back to add more randomness and avoid biases.\n        if rng.random() < 0.5:\n            del trunc_tokens[0]\n        else:\n            trunc_tokens.pop()\n\n", "CodeBleu_score": 79.96334663787925}]}
{"instance_id": "007181b6729d", "generation_result": [{"code": "def get_activation(activation_string)\n\t\n    if not isinstance(activation_string, str):\n        return activation_string\n    if not activation_string:\n        return None\n    act = activation_string.lower()\n    if act == 'linear':\n        return linear\n    elif act == 'relu':\n        return relu\n    elif act == 'sigmoid':\n        return sigmoid\n    elif act == 'tanh':\n        return tanh\n    elif act == 'softmax':\n        return softmax\n    elif act == 'elu':\n        return elu\n    elif act == 'selu':\n        return selu\n    elif act == 'softplus':\n        return softplus\n    elif act == 'softsign':\n        return softsign\n    elif act == 'hard_sigmoid':\n        return hard_sigmoid\n    elif act == 'exponential':\n        return exponential\n    elif act == 'linear_activation':\n        return linear_activation\n    elif act == 'get_custom_objects().get(activation)':\n        return get_custom_objects().get(activation)\n    else:\n        raise ValueError('Unsupported activation: ' + activation_string)\n", "CodeBleu_score": 61.69416784904166}, {"code": "def get_activation(activation_string)\n\t\n    # First, we check if the input is a string\n    if not isinstance(activation_string, str):\n        return activation_string\n    # If we receive an empty string, we return None\n    if not activation_string:\n        return None\n    # Otherwise, we check to see if the activation function is supported.\n    activation_string = activation_string.lower()\n    if activation_string == \"linear\":\n        return linear\n    elif activation_string == \"relu\":\n        return relu\n    elif activation_string == \"sigmoid\":\n        return sigmoid\n    elif activation_string == \"tanh\":\n        return tanh\n    elif activation_string == \"softmax\":\n        return softmax\n    elif activation_string == \"leaky_relu\":\n        return leaky_relu\n    elif activation_string == \"elu\":\n        return elu\n    elif activation_string == \"selu\":\n        return selu\n    elif activation_string == \"softplus\":\n        return softplus\n    elif activation_string == \"softsign\":\n        return softsign\n    elif activation_string == \"hard_sigmoid\":\n        return hard_sigmoid\n    elif activation_string == \"exponential\":\n        return exponential\n    elif activation_string == \"linear\":\n        return linear\n    elif activation_string == \"swish\":\n        return swish\n    else:\n        raise ValueError(\"Activation function not supported: \" + activation_string)\n", "CodeBleu_score": 56.98724670765702}, {"code": "def get_activation(activation_string)\n\t\n    if not isinstance(activation_string, str):\n        return activation_string\n    if not activation_string:\n        return None\n    act = activation_string.lower()\n    if act == 'linear':\n        return linear\n    elif act == 'relu':\n        return relu\n    elif act == 'sigmoid':\n        return sigmoid\n    elif act == 'tanh':\n        return tanh\n    elif act == 'softmax':\n        return softmax\n    elif act == 'softplus':\n        return softplus\n    elif act == 'softsign':\n        return softsign\n    elif act == 'selu':\n        return selu\n    elif act == 'elu':\n        return elu\n    elif act == 'exponential':\n        return exponential\n    elif act == 'hard_sigmoid':\n        return hard_sigmoid\n    elif act == 'linear_relu':\n        return linear_relu\n    elif act == 'swish':\n        return swish\n    elif act == 'mish':\n        return mish\n    elif act == 'hard_swish':\n        return hard_swish\n    else:\n        raise ValueError('Unsupported activation: %s' % act)\n", "CodeBleu_score": 61.74798614315122}]}
{"instance_id": "d5d6e0315899", "generation_result": [{"code": "def dropout(input_tensor, dropout_prob)\n\t\n    if dropout_prob is None or dropout_prob == 0.0:\n        return input_tensor\n    else:\n        return tf.nn.dropout(input_tensor, 1.0 - dropout_prob)\n\n", "CodeBleu_score": 60.84090845772291}, {"code": "def dropout(input_tensor, dropout_prob)\n\t\n    return tf.keras.layers.Dropout(dropout_prob)(input_tensor)\n\n", "CodeBleu_score": 8.897311733252689}, {"code": "def dropout(input_tensor, dropout_prob)\n\t\n    if dropout_prob is None or dropout_prob == 0.0:\n        return input_tensor\n\n    output = tf.nn.dropout(input_tensor, 1.0 - dropout_prob)\n    return output\n\n", "CodeBleu_score": 87.51451468041077}]}
{"instance_id": "828917b9ac53", "generation_result": [{"code": "def create_attention_mask_from_input_mask(from_tensor, to_mask)\n\t\n    from_shape = get_shape_list(from_tensor, expected_rank=[2, 3])\n    batch_size = from_shape[0]\n    from_seq_length = from_shape[1]\n\n    to_shape = get_shape_list(to_mask, expected_rank=2)\n    to_seq_length = to_shape[1]\n\n    to_mask = tf.cast(\n        tf.reshape(to_mask, [batch_size, 1, to_seq_length]), tf.float32)\n\n    # We don't assume that `from_tensor` is a mask (although it could be). We\n    # don't actually care if we attend *from* padding tokens (only *to* padding)\n    # tokens so we create a tensor of all ones.\n    #\n    # `broadcast_ones` = [batch_size, from_seq_length, 1]\n    broadcast_ones = tf.ones(\n        shape=[batch_size, from_seq_length, 1], dtype=tf.float32)\n\n    # Here we broadcast along two dimensions to create the mask.\n    mask = broadcast_ones * to_mask\n\n    return mask", "CodeBleu_score": 64.09901013046853}, {"code": "def create_attention_mask_from_input_mask(from_tensor, to_mask)\n\t\n    from_shape = get_shape_list(from_tensor, expected_rank=[2, 3])\n    batch_size = from_shape[0]\n    from_seq_length = from_shape[1]\n\n    to_shape = get_shape_list(to_mask, expected_rank=2)\n    to_seq_length = to_shape[1]\n\n    to_mask = tf.cast(\n        tf.reshape(to_mask, [batch_size, 1, to_seq_length]), tf.float32)\n\n    # We don't assume that `from_tensor` is a mask (although it could be). We\n    # don't actually care if we attend *from* padding tokens (only *to* padding)\n    # tokens so we create a tensor of all ones.\n    #\n    # `broadcast_ones` = [batch_size, from_seq_length, 1]\n    broadcast_ones = tf.ones(\n        shape=[batch_size, from_seq_length, 1], dtype=tf.float32)\n\n    # Here we broadcast along two dimensions to create the mask.\n    mask = broadcast_ones * to_mask\n\n    return mask\n\n", "CodeBleu_score": 64.09901013046853}, {"code": "def create_attention_mask_from_input_mask(from_tensor, to_mask)\n\t\n    from_shape = get_shape_list(from_tensor, expected_rank=[2, 3])\n    batch_size = from_shape[0]\n    from_seq_length = from_shape[1]\n\n    to_shape = get_shape_list(to_mask, expected_rank=2)\n    to_seq_length = to_shape[1]\n\n    to_mask = tf.cast(\n        tf.reshape(to_mask, [batch_size, 1, to_seq_length]), tf.float32)\n\n    # We don't assume that `from_tensor` is a mask (although it could be). We\n    # don't actually care if we attend *from* padding tokens (only *to* padding)\n    # tokens so we create a tensor of all ones.\n    #\n    # `broadcast_ones` = [batch_size, from_seq_length, 1]\n    broadcast_ones = tf.ones(\n        shape=[batch_size, from_seq_length, 1], dtype=tf.float32)\n\n    # Here we broadcast along two dimensions to create the mask.\n    mask = broadcast_ones * to_mask\n\n    return mask\n\n", "CodeBleu_score": 64.09901013046853}]}
{"instance_id": "d7798e41a723", "generation_result": [{"code": "def transpose_for_scores(input_tensor, batch_size, num_attention_heads,\n                           seq_length, width)\n\t.\n\n    Args:\n      input_tensor: A tensor with shape [batch_size, seq_length, num_attention_heads, width]\n      batch_size: The batch size.\n      num_attention_heads: The number of attention heads.\n      seq_length: The sequence length.\n      width: The width of each attention head.\n\n    Returns:\n      A tensor with shape [batch_size, num_attention_heads, seq_length, width]\n    \"\"\"\n    output_tensor = tf.reshape(input_tensor, [batch_size, seq_length, num_attention_heads, width])\n\n    output_tensor = tf.transpose(output_tensor, [0, 2, 1, 3])\n    return output_tensor\n\n", "CodeBleu_score": 25.440215513630044}, {"code": "def transpose_for_scores(input_tensor, batch_size, num_attention_heads,\n                           seq_length, width)\n\t.\n    Args:\n        input_tensor: A 4D tensor with shape [batch_size, seq_length, num_attention_heads, width].\n        batch_size: The batch size.\n        num_attention_heads: The number of attention heads.\n        seq_length: The sequence length.\n        width: The width of each attention head.\n    Returns:\n        A 4D tensor with shape [batch_size, num_attention_heads, seq_length, width].\n    \"\"\"\n    output_tensor = tf.reshape(input_tensor, [batch_size, seq_length, num_attention_heads, width])\n\n    output_tensor = tf.transpose(output_tensor, [0, 2, 1, 3])\n    return output_tensor\n\n", "CodeBleu_score": 33.02549877083323}, {"code": "def transpose_for_scores(input_tensor, batch_size, num_attention_heads,\n                           seq_length, width)\n\t.\n\n    Args:\n        input_tensor: A float32 tensor with shape [batch_size, seq_length, num_attention_heads, width].\n        batch_size: A Python integer.\n        num_attention_heads: A Python integer.\n        seq_length: A Python integer.\n        width: A Python integer.\n\n    Returns:\n        A float32 tensor with shape [batch_size, num_attention_heads, seq_length, width].\n    \"\"\"\n    output_tensor = tf.reshape(input_tensor, [batch_size, seq_length, num_attention_heads, width])\n\n    output_tensor = tf.transpose(output_tensor, [0, 2, 1, 3])\n    return output_tensor\n\n", "CodeBleu_score": 30.93841128914153}]}
{"instance_id": "711d466ace09", "generation_result": [{"code": "def reshape_to_matrix(input_tensor)\n\t\n    ndims = input_tensor.shape.ndims\n    if ndims < 2:\n        raise ValueError(\"Input tensor must have at least rank 2. Shape = %s\" %\n                         (input_tensor.shape))\n    if ndims == 2:\n        return input_tensor\n\n    width = input_tensor.shape[-1]\n    output_tensor = tf.reshape(input_tensor, [-1, width])\n    return output_tensor\n\n", "CodeBleu_score": 83.6520561341743}, {"code": "def reshape_to_matrix(input_tensor)\n\t\n    ndims = input_tensor.shape.ndims\n    if ndims < 2:\n        raise ValueError(\"Input tensor must have at least rank 2. Shape = %s\" %\n                         (input_tensor.shape))\n    if ndims == 2:\n        return input_tensor\n\n    width = input_tensor.shape[-1]\n    output_tensor = tf.reshape(input_tensor, [-1, width])\n    return output_tensor\n", "CodeBleu_score": 83.6520561341743}, {"code": "def reshape_to_matrix(input_tensor)\n\t\n    ndims = input_tensor.shape.ndims\n    if ndims < 2:\n        raise ValueError(\"Input tensor must have at least rank 2. Shape = %s\" %\n                         (input_tensor.shape))\n    if ndims == 2:\n        return input_tensor\n\n    width = input_tensor.shape[-1]\n    output_tensor = tf.reshape(input_tensor, [-1, width])\n    return output_tensor\n\n", "CodeBleu_score": 83.6520561341743}]}
{"instance_id": "7adcd08f531e", "generation_result": [{"code": "def get_train_examples(self, data_dir)\n\t\n    examples = []\n    file_path = os.path.join(data_dir, \"train.tsv\")\n    with tf.gfile.Open(file_path, \"r\") as f:\n        reader = csv.reader(f, delimiter=\"\\t\")\n        for i, line in enumerate(reader):\n            if i == 0:\n                continue\n            guid = \"%s-%s\" % (\"train\", i)\n            text_a = tokenization.convert_to_unicode(line[3])\n            text_b = tokenization.convert_to_unicode(line[4])\n            label = tokenization.convert_to_unicode(line[5])\n            if label == \"contradictory\":\n                label = \"contradiction\"\n            examples.append(InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n    return examples", "CodeBleu_score": 48.259374571564315}, {"code": "def get_train_examples(self, data_dir)\n\t\n    train_file = os.path.join(data_dir, \"train.tsv\")\n    return self._create_examples(self._read_tsv(train_file), \"train\")", "CodeBleu_score": 6.209409432138726}, {"code": "def get_train_examples(self, data_dir)\n\t\n    examples = []\n    with open(os.path.join(data_dir, \"train.tsv\"), \"r\") as f:\n        reader = csv.reader(f, delimiter=\"\\t\")\n        for i, line in enumerate(reader):\n            if i == 0:\n                continue\n            guid = \"%s-%s\" % (\"train\", i)\n            text_a = tokenization.convert_to_unicode(line[3])\n            text_b = tokenization.convert_to_unicode(line[4])\n            label = tokenization.convert_to_unicode(line[5])\n            if label == \"contradictory\":\n                label = \"contradiction\"\n            examples.append(\n                InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label)\n            )\n    return examples", "CodeBleu_score": 47.78192566991177}]}
{"instance_id": "26a0bb96dab9", "generation_result": [{"code": "def metric_fn(per_example_loss, label_ids, logits, is_real_example)\n\t\n\n    logits = tf.reshape(logits, [-1, logits.shape[-1]])\n    predictions = tf.argmax(logits, axis=-1, output_type=tf.int32)\n    accuracy = tf.metrics.accuracy(\n        labels=label_ids, predictions=predictions, weights=is_real_example)\n    loss = tf.metrics.mean(values=per_example_loss, weights=is_real_example)\n    return {\n        \"eval_accuracy\": accuracy,\n        \"eval_loss\": loss,\n    }\n", "CodeBleu_score": 62.080369841566}, {"code": "def metric_fn(per_example_loss, label_ids, logits, is_real_example)\n\t\n    logits = tf.argmax(logits, axis=-1, output_type=tf.int32)\n    is_correct = tf.equal(tf.cast(logits, tf.int32), tf.cast(label_ids, tf.int32))\n    accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n    loss = tf.reduce_mean(per_example_loss)\n    return {\n        \"eval_accuracy\": accuracy,\n        \"eval_loss\": loss,\n    }\n\n", "CodeBleu_score": 42.73563148548383}, {"code": "def metric_fn(per_example_loss, label_ids, logits, is_real_example)\n\t\n    logits = tf.argmax(logits, axis=-1, output_type=tf.int32)\n    accuracy = tf.metrics.accuracy(\n        labels=label_ids, predictions=logits, weights=is_real_example)\n    loss = tf.metrics.mean(values=per_example_loss, weights=is_real_example)\n    return {\n        \"eval_accuracy\": accuracy,\n        \"eval_loss\": loss,\n    }\n", "CodeBleu_score": 60.419395660960504}]}
{"instance_id": "6154e4d28509", "generation_result": [{"code": "def convert_examples_to_features(examples, label_list, max_seq_length,\n                                 tokenizer)\n\t\n    features = []\n    for (ex_index, example) in enumerate(examples):\n        if ex_index % 10000 == 0:\n            tf.logging.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n\n        feature = convert_single_example(ex_index, example, label_list,\n                                         max_seq_length, tokenizer)\n        features.append(feature)\n    return features\n", "CodeBleu_score": 83.41734189253587}, {"code": "def convert_examples_to_features(examples, label_list, max_seq_length,\n                                 tokenizer)\n\t\n\n    features = []\n    for (ex_index, example) in enumerate(examples):\n        if ex_index % 10000 == 0:\n            tf.logging.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n\n        feature = convert_single_example(ex_index, example, label_list,\n                                         max_seq_length, tokenizer)\n\n        features.append(feature)\n    return features\n\n", "CodeBleu_score": 83.41734189253587}, {"code": "def convert_examples_to_features(examples, label_list, max_seq_length,\n                                 tokenizer)\n\t\n    features = []\n    for (ex_index, example) in enumerate(examples):\n        if ex_index % 10000 == 0:\n            tf.logging.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n\n        feature = convert_single_example(ex_index, example, label_list,\n                                         max_seq_length, tokenizer)\n\n        features.append(feature)\n    return features\n\n", "CodeBleu_score": 83.41734189253587}]}
{"instance_id": "e09c0db71ebb", "generation_result": [{"code": "def get_next_sentence_output(bert_config, input_tensor, labels)\n\t\n    # Simple binary classification. Note that 0 is \"next sentence\" and 1 is\n    # \"random sentence\". This weight matrix is not used after pre-training.\n    with tf.variable_scope(\"cls/seq_relationship\"):\n        output_weights = tf.get_variable(\n            \"output_weights\",\n            shape=[2, bert_config.hidden_size],\n            initializer=modeling.create_initializer(bert_config.initializer_range))\n        output_bias = tf.get_variable(\n            \"output_bias\", shape=[2], initializer=tf.zeros_initializer())\n\n        logits = tf.matmul(input_tensor, output_weights, transpose_b=True)\n        logits = tf.nn.bias_add(logits, output_bias)\n        log_probs = tf.nn.log_softmax(logits, axis=-1)\n\n        labels = tf.reshape(labels, [-1])\n        one_hot_labels = tf.one_hot(labels, depth=2, dtype=tf.float32)\n        per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n        loss = tf.reduce_mean(per_example_loss)\n        return (loss, per_example_loss, log_probs)\n\n", "CodeBleu_score": 86.94900271179796}, {"code": "def get_next_sentence_output(bert_config, input_tensor, labels)\n\t\n    # Simple binary classification. Note that 0 is \"next sentence\" and 1 is \"random sentence\". This weight matrix is not used after pre-training.\n    with tf.variable_scope(\"cls/seq_relationship\"):\n        output_weights = tf.get_variable(\n            \"output_weights\",\n            shape=[2, bert_config.hidden_size],\n            initializer=modeling.create_initializer(bert_config.initializer_range))\n        output_bias = tf.get_variable(\n            \"output_bias\", shape=[2], initializer=tf.zeros_initializer())\n\n        logits = tf.matmul(input_tensor, output_weights, transpose_b=True)\n        logits = tf.nn.bias_add(logits, output_bias)\n        log_probs = tf.nn.log_softmax(logits, axis=-1)\n\n        labels = tf.reshape(labels, [-1])\n        one_hot_labels = tf.one_hot(labels, depth=2, dtype=tf.float32)\n        per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n        loss = tf.reduce_mean(per_example_loss)\n        return (loss, per_example_loss, log_probs)\n\n", "CodeBleu_score": 86.94900271179796}, {"code": "def get_next_sentence_output(bert_config, input_tensor, labels)\n\t\n    # Simple binary classification. Note that 0 is \"next sentence\" and 1 is\n    # \"random sentence\". This weight matrix is not used after pre-training.\n    with tf.variable_scope(\"cls/seq_relationship\"):\n        output_weights = tf.get_variable(\n            \"output_weights\",\n            shape=[2, bert_config.hidden_size],\n            initializer=modeling.create_initializer(bert_config.initializer_range))\n        output_bias = tf.get_variable(\n            \"output_bias\", shape=[2], initializer=tf.zeros_initializer())\n\n        logits = tf.matmul(input_tensor, output_weights, transpose_b=True)\n        logits = tf.nn.bias_add(logits, output_bias)\n        log_probs = tf.nn.log_softmax(logits, axis=-1)\n\n        labels = tf.reshape(labels, [-1])\n        one_hot_labels = tf.one_hot(labels, depth=2, dtype=tf.float32)\n        per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n        loss = tf.reduce_mean(per_example_loss)\n        return (loss, per_example_loss, log_probs)\n\n", "CodeBleu_score": 86.94900271179796}]}
{"instance_id": "1f66b020f485", "generation_result": [{"code": "def gather_indexes(sequence_tensor, positions)\n\t\n    sequence_shape = modeling.get_shape_list(sequence_tensor, expected_rank=3)\n    batch_size = sequence_shape[0]\n    seq_length = sequence_shape[1]\n    width = sequence_shape[2]\n\n    flat_offsets = tf.reshape(tf.range(0, batch_size, dtype=tf.int32) * seq_length, [-1, 1])\n    flat_positions = tf.reshape(positions + flat_offsets, [-1])\n    flat_sequence_tensor = tf.reshape(sequence_tensor, [batch_size * seq_length, width])\n    output_tensor = tf.gather(flat_sequence_tensor, flat_positions)\n    return output_tensor\n\n", "CodeBleu_score": 55.18468812590758}, {"code": "def gather_indexes(sequence_tensor, positions)\n\t\n    sequence_shape = modeling.get_shape_list(sequence_tensor, expected_rank=3)\n    batch_size = sequence_shape[0]\n    seq_length = sequence_shape[1]\n    width = sequence_shape[2]\n\n    flat_offsets = tf.reshape(tf.range(0, batch_size, dtype=tf.int32) * seq_length, [-1, 1])\n    flat_positions = tf.reshape(positions + flat_offsets, [-1])\n    flat_sequence_tensor = tf.reshape(sequence_tensor, [batch_size * seq_length, width])\n    output_tensor = tf.gather(flat_sequence_tensor, flat_positions)\n    return output_tensor\n\n", "CodeBleu_score": 55.18468812590758}, {"code": "def gather_indexes(sequence_tensor, positions)\n\t\n    sequence_shape = modeling.get_shape_list(sequence_tensor, expected_rank=3)\n    batch_size = sequence_shape[0]\n    seq_length = sequence_shape[1]\n    width = sequence_shape[2]\n\n    flat_offsets = tf.reshape(tf.range(0, batch_size, dtype=tf.int32) * seq_length, [-1, 1])\n    flat_positions = tf.reshape(positions + flat_offsets, [-1])\n    flat_sequence_tensor = tf.reshape(sequence_tensor, [batch_size * seq_length, width])\n    output_tensor = tf.gather(flat_sequence_tensor, flat_positions)\n    return output_tensor\n\n", "CodeBleu_score": 55.18468812590758}]}
{"instance_id": "956bffee9a33", "generation_result": [{"code": "def _decode_record(record, name_to_features)\n\t\n    example = tf.parse_single_example(record, name_to_features)\n    for name in list(example.keys()):\n        t = example[name]\n        if t.dtype == tf.int64:\n            t = tf.to_int32(t)\n        example[name] = t\n    return example\n\n", "CodeBleu_score": 70.13047524425508}, {"code": "def _decode_record(record, name_to_features)\n\t\n    example = tf.parse_single_example(record, name_to_features)\n    for name in list(example.keys()):\n        t = example[name]\n        if t.dtype == tf.int64:\n            t = tf.to_int32(t)\n        example[name] = t\n    return example\n", "CodeBleu_score": 70.13047524425508}, {"code": "def _decode_record(record, name_to_features)\n\t\n    example = tf.parse_single_example(record, name_to_features)\n\n    # tf.Example only supports tf.int64, but the TPU only supports tf.int32. So cast all int64 to int32.\n    for name in list(example.keys()):\n        t = example[name]\n        if t.dtype == tf.int64:\n            t = tf.to_int32(t)\n        example[name] = t\n\n    return example\n\n", "CodeBleu_score": 70.13047524425508}]}
{"instance_id": "6b97f354e641", "generation_result": [{"code": "def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer,\n                         orig_answer_text)\n\t\n    tok_answer_text = \" \".join(tokenizer.tokenize(orig_answer_text))\n\n    for new_start in range(input_start, input_end + 1):\n        for new_end in range(input_end, new_start - 1, -1):\n            text_span = \" \".join(doc_tokens[new_start:(new_end + 1)])\n            if text_span == tok_answer_text:\n                return (new_start, new_end)\n\n    return (input_start, input_end)\n\n", "CodeBleu_score": 78.7094830345058}, {"code": "def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer,\n                         orig_answer_text)\n\t\n    tok_answer_text = \" \".join(tokenizer.tokenize(orig_answer_text))\n\n    for new_start in range(input_start, input_end + 1):\n        for new_end in range(input_end, new_start - 1, -1):\n            text_span = \" \".join(doc_tokens[new_start:(new_end + 1)])\n            if text_span == tok_answer_text:\n                return (new_start, new_end)\n\n    return (input_start, input_end)\n\n", "CodeBleu_score": 78.7094830345058}, {"code": "def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer,\n                         orig_answer_text)\n\t\n    tok_answer_text = \" \".join(tokenizer.tokenize(orig_answer_text))\n\n    for new_start in range(input_start, input_end + 1):\n        for new_end in range(input_end, new_start - 1, -1):\n            text_span = \" \".join(doc_tokens[new_start:(new_end + 1)])\n            if text_span == tok_answer_text:\n                return (new_start, new_end)\n\n    return (input_start, input_end)\n\n", "CodeBleu_score": 78.7094830345058}]}
{"instance_id": "aba334b92b34", "generation_result": [{"code": "def _check_is_max_context(doc_spans, cur_span_index, position)\n\t\n    best_score = None\n    best_span_index = None\n    for (span_index, doc_span) in enumerate(doc_spans):\n        end = doc_span.start + doc_span.length - 1\n        if position < doc_span.start:\n            continue\n        if position > end:\n            continue\n        num_left_context = position - doc_span.start\n        num_right_context = end - position\n        score = min(num_left_context, num_right_context) + 0.01 * doc_span.length\n        if best_score is None or score > best_score:\n            best_score = score\n            best_span_index = span_index\n\n    return cur_span_index == best_span_index\n", "CodeBleu_score": 78.6774915248039}, {"code": "def _check_is_max_context(doc_spans, cur_span_index, position)\n\t\n    best_score = None\n    best_span_index = None\n    for (span_index, doc_span) in enumerate(doc_spans):\n        end = doc_span.start + doc_span.length - 1\n        if position < doc_span.start:\n            continue\n        if position > end:\n            continue\n        num_left_context = position - doc_span.start\n        num_right_context = end - position\n        score = min(num_left_context, num_right_context) + 0.01 * doc_span.length\n        if best_score is None or score > best_score:\n            best_score = score\n            best_span_index = span_index\n\n    return cur_span_index == best_span_index\n\n", "CodeBleu_score": 78.6774915248039}, {"code": "def _check_is_max_context(doc_spans, cur_span_index, position)\n\t\n    best_score = None\n    best_span_index = None\n    for (span_index, doc_span) in enumerate(doc_spans):\n        end = doc_span.start + doc_span.length - 1\n        if position < doc_span.start:\n            continue\n        if position > end:\n            continue\n        num_left_context = position - doc_span.start\n        num_right_context = end - position\n        score = min(num_left_context, num_right_context) + 0.01 * doc_span.length\n        if best_score is None or score > best_score:\n            best_score = score\n            best_span_index = span_index\n\n    return cur_span_index == best_span_index\n\n", "CodeBleu_score": 78.6774915248039}]}
{"instance_id": "cf92ad509f6a", "generation_result": [{"code": "def load_vocab(vocab_file)\n\t\n    vocab = collections.OrderedDict()\n    index = 0\n    with open(vocab_file, \"r\", encoding=\"utf-8\") as reader:\n        while True:\n            token = convert_to_unicode(reader.readline())\n            if not token:\n                break\n            token = token.strip()\n            vocab[token] = index\n            index += 1\n    return vocab\n\n", "CodeBleu_score": 65.86707343330198}, {"code": "def load_vocab(vocab_file)\n\t\n    vocab = collections.OrderedDict()\n    index = 0\n    with tf.gfile.GFile(vocab_file, \"r\") as reader:\n        while True:\n            token = convert_to_unicode(reader.readline())\n            if not token:\n                break\n            token = token.strip()\n            vocab[token] = index\n            index += 1\n    return vocab\n\n", "CodeBleu_score": 74.17973775756064}, {"code": "def load_vocab(vocab_file)\n\t\n    vocab = collections.OrderedDict()\n    index = 0\n    with tf.gfile.GFile(vocab_file, \"r\") as reader:\n        while True:\n            token = convert_to_unicode(reader.readline())\n            if not token:\n                break\n            token = token.strip()\n            vocab[token] = index\n            index += 1\n    return vocab\n\n", "CodeBleu_score": 74.17973775756064}]}
{"instance_id": "6c865803ac12", "generation_result": [{"code": "def convert_by_vocab(vocab, items)\n\t\n    output = []\n    for item in items:\n        output.append(vocab[item])\n    return output\n\n", "CodeBleu_score": 79.63749907165396}, {"code": "def convert_by_vocab(vocab, items)\n\t\n    output = []\n    for item in items:\n        output.append(vocab[item])\n    return output\n\n", "CodeBleu_score": 79.63749907165396}, {"code": "def convert_by_vocab(vocab, items)\n\t\n    output = []\n    for item in items:\n        output.append(vocab[item])\n    return output\n\n", "CodeBleu_score": 79.63749907165396}]}
{"instance_id": "e93b9200dc4a", "generation_result": [{"code": "def tokenize(self, text)\n\t\n    text = self.basic_tokenizer.tokenize(text)\n    text = self.wordpiece_tokenizer.tokenize(text)\n    return text\n", "CodeBleu_score": 16.376586011334997}, {"code": "def tokenize(self, text)\n\t\n    text = convert_to_unicode(text)\n    text = self.basic_tokenizer.tokenize(text)\n    text = self.wordpiece_tokenizer.tokenize(text)\n    return text\n", "CodeBleu_score": 18.778391728140804}, {"code": "def tokenize(self, text)\n\t\n    split_tokens = []\n    for token in self.basic_tokenizer.tokenize(text):\n        for sub_token in self.wordpiece_tokenizer.tokenize(token):\n            split_tokens.append(sub_token)\n\n    return split_tokens\n", "CodeBleu_score": 88.10762355418231}]}
{"instance_id": "2ff6f6db53c9", "generation_result": [{"code": "def safe_load(path, offset, duration, sample_rate, dtype)\n\t\n    try:\n        logger.info(f\"Loading audio data from {path}...\")\n        data, _ = librosa.load(path, offset=offset, duration=duration, sr=sample_rate, dtype=dtype)\n        logger.info(\"Audio data loaded successfully.\")\n        return data, False\n    except Exception as e:\n        logger.exception(f\"An error occurred while loading audio data from {path}.\")\n        return np.array([-1.0]), True\n\n", "CodeBleu_score": 42.094297063217525}, {"code": "def safe_load(path, offset, duration, sample_rate, dtype)\n\t\n    try:\n        logger.info(\"Loading audio data from file: {}\".format(path))\n        audio_data = self.load(path, offset=offset, duration=duration, sample_rate=sample_rate, dtype=dtype)\n        return audio_data, False\n    except Exception as e:\n        logger.exception(e)\n        return -1.0, True\n", "CodeBleu_score": 33.07132759040429}, {"code": "def safe_load(path, offset, duration, sample_rate, dtype)\n\t\n    try:\n        logger.info(\"Loading audio data from %s\", path)\n        audio_data, _ = librosa.load(path, offset=offset, duration=duration, sr=sample_rate, dtype=dtype)\n        logger.info(\"Successfully loaded audio data from %s\", path)\n        return audio_data, False\n    except Exception as e:\n        logger.exception(\"An exception occurred while loading audio data from %s\", path)\n        return -1.0, True\n\n", "CodeBleu_score": 38.76963721819771}]}
{"instance_id": "12fbe1f03d8f", "generation_result": [{"code": "def to_n_channels(waveform: tf.Tensor, n_channels: int) -> tf.Tensor\n\t\n    n_channels_waveform = tf.shape(waveform)[-1]\n    if n_channels_waveform == n_channels:\n        return waveform\n    elif n_channels_waveform > n_channels:\n        return waveform[..., :n_channels]\n    else:\n        return tf.tile(waveform, [1, 1, n_channels])\n", "CodeBleu_score": 43.714358820228696}, {"code": "def to_n_channels(waveform: tf.Tensor, n_channels: int) -> tf.Tensor\n\t\n    if n_channels == 1:\n        return tf.squeeze(waveform, axis=-1)\n    elif n_channels == 2:\n        return tf.tile(waveform, [1, 1, 2])\n    elif n_channels == 4:\n        return tf.tile(waveform, [1, 1, 4])\n    else:\n        if tf.shape(waveform)[-1] >= n_channels:\n            return waveform[..., :n_channels]\n        else:\n            return tf.tile(waveform, [1, 1, n_channels])\n\n", "CodeBleu_score": 46.480844723425655}, {"code": "def to_n_channels(waveform: tf.Tensor, n_channels: int) -> tf.Tensor\n\t\n    if tf.shape(waveform)[-1] >= n_channels:\n        return waveform[..., :n_channels]\n    else:\n        return tf.tile(waveform, [1, 1, n_channels])\n\n", "CodeBleu_score": 40.264104792671354}]}
{"instance_id": "5e215f029b2e", "generation_result": [{"code": "def spectrogram_to_db_uint(\n    spectrogram: tf.Tensor, db_range: float = 100.0, **kwargs\n) -> tf.Tensor\n\t\n    db_spectrogram = gain_to_db(spectrogram, **kwargs)\n    db_spectrogram = tf.clip_by_value(db_spectrogram, -db_range, 0)\n    db_spectrogram = from_float32_to_uint8(db_spectrogram)\n    return db_spectrogram\n\n", "CodeBleu_score": 38.52114951126531}, {"code": "def spectrogram_to_db_uint(\n    spectrogram: tf.Tensor, db_range: float = 100.0, **kwargs\n) -> tf.Tensor\n\t\n    spectrogram_db = gain_to_db(spectrogram, **kwargs)\n    db_range_db = tf.constant(db_range, dtype=tf.float32)\n    max_db = tf.reduce_max(spectrogram_db)\n    min_db = max_db - db_range_db\n    spectrogram_db = tf.clip_by_value(spectrogram_db, min_db, max_db)\n    return from_float32_to_uint8(spectrogram_db)\n\n", "CodeBleu_score": 44.58392865125646}, {"code": "def spectrogram_to_db_uint(\n    spectrogram: tf.Tensor, db_range: float = 100.0, **kwargs\n) -> tf.Tensor\n\t\n    spectrogram_db = gain_to_db(spectrogram, **kwargs)\n    max_db = tf.reduce_max(spectrogram_db)\n    return tf.cast(\n        tf.clip_by_value(spectrogram_db, max_db - db_range, max_db), tf.uint8\n    )\n\n", "CodeBleu_score": 38.53637874508331}]}
{"instance_id": "b65c0968dae7", "generation_result": [{"code": "def compute_spectrogram_tf(\n    waveform: tf.Tensor,\n    frame_length: int = 2048,\n    frame_step: int = 512,\n    spec_exponent: float = 1.0,\n    window_exponent: float = 1.0,\n) -> tf.Tensor\n\t\n    window_fn = tf.signal.hann_window\n    window_fn = functools.partial(window_fn, exponent=window_exponent)\n    stft_fn = functools.partial(\n        tf.signal.stft, frame_length=frame_length, frame_step=frame_step, window_fn=window_fn\n    )\n    spectrogram = tf.abs(stft_fn(waveform)) ** spec_exponent\n    spectrogram = tf.transpose(spectrogram, perm=(1, 0))\n    return spectrogram\n", "CodeBleu_score": 34.978243817454285}, {"code": "def compute_spectrogram_tf(\n    waveform: tf.Tensor,\n    frame_length: int = 2048,\n    frame_step: int = 512,\n    spec_exponent: float = 1.0,\n    window_exponent: float = 1.0,\n) -> tf.Tensor\n\t\n    # Apply Hann window function\n    window_fn = hann_window(frame_length, exponent=window_exponent)\n    windowed_waveform = waveform * window_fn\n    # Compute Short-Time Fourier Transform\n    stft_fn = stft(frame_length, frame_step)\n    stft_waveform = stft_fn(windowed_waveform)\n    # Transpose and apply absolute value\n    stft_waveform = tf.transpose(stft_waveform, perm=[1, 2, 0])\n    stft_waveform = tf.abs(stft_waveform)\n    # Apply exponent\n    stft_waveform = tf.pow(stft_waveform, spec_exponent)\n    return stft_waveform\n\n", "CodeBleu_score": 36.11724955531502}, {"code": "def compute_spectrogram_tf(\n    waveform: tf.Tensor,\n    frame_length: int = 2048,\n    frame_step: int = 512,\n    spec_exponent: float = 1.0,\n    window_exponent: float = 1.0,\n) -> tf.Tensor\n\t\n    # Compute the Short-Time Fourier Transform of the waveform\n    stft_matrix = tf.signal.stft(\n        waveform, frame_length=frame_length, frame_step=frame_step, window_fn=hann_window\n    )\n    # Compute the magnitude of the STFT\n    spectrogram = tf.abs(stft_matrix)\n    # Apply the logarithm to the spectrogram\n    spectrogram = tf.math.log(spectrogram)\n    # Transpose the spectrogram\n    spectrogram = tf.transpose(spectrogram)\n    # Raise the spectrogram to the specified exponent\n    spectrogram = spectrogram ** spec_exponent\n    # Return the spectrogram\n    return spectrogram\n\n", "CodeBleu_score": 32.17330020870403}]}
{"instance_id": "a4686140a634", "generation_result": [{"code": "def time_stretch(\n    spectrogram: tf.Tensor,\n    factor: float = 1.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n    # Calculate the new time dimension.\n    new_time = tf.cast(tf.shape(spectrogram)[1], tf.float32) * factor\n    # Resize the spectrogram.\n    resized_spectrogram = resize_images(\n        spectrogram,\n        size=[tf.cast(new_time, tf.int32), tf.shape(spectrogram)[2]],\n        method=method,\n    )\n    # Crop or pad the spectrogram.\n    cropped_spectrogram = resize_image_with_crop_or_pad(\n        resized_spectrogram,\n        target_height=tf.shape(spectrogram)[0],\n        target_width=tf.shape(spectrogram)[1],\n    )\n    return cropped_spectrogram\n\n", "CodeBleu_score": 36.61311892655085}, {"code": "def time_stretch(\n    spectrogram: tf.Tensor,\n    factor: float = 1.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n    original_shape = tf.shape(spectrogram)\n    time_dim = tf.cast(original_shape[1], tf.float32)\n    new_time_dim = tf.cast(time_dim * factor, tf.int32)\n    resized_spectrogram = resize_images(\n        images=spectrogram,\n        size=(original_shape[0], new_time_dim),\n        method=method,\n    )\n    resized_spectrogram = resize_image_with_crop_or_pad(\n        image=resized_spectrogram,\n        target_height=original_shape[0],\n        target_width=original_shape[1],\n    )\n    return resized_spectrogram\n\n", "CodeBleu_score": 32.23748316962135}, {"code": "def time_stretch(\n    spectrogram: tf.Tensor,\n    factor: float = 1.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n    shape = tf.shape(spectrogram)\n    new_time_dim = tf.cast(shape[1] * factor, tf.int32)\n    resized = tf.image.resize(\n        spectrogram,\n        size=[shape[0], new_time_dim],\n        method=method,\n    )\n    return resize_image_with_crop_or_pad(resized, shape[0], shape[1])\n\n", "CodeBleu_score": 34.69932203439169}]}
{"instance_id": "6dd01101d4ae", "generation_result": [{"code": "def pitch_shift(\n    spectrogram: tf.Tensor,\n    semitone_shift: float = 0.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n    # Calculating the resizing factor\n    factor = 2.0 ** (semitone_shift / 12.0)\n    # Resizing the spectrogram\n    resized_spectrogram = resize_images(\n        spectrogram,\n        size=[\n            int(spectrogram.shape[0] * factor),\n            spectrogram.shape[1],\n        ],\n        method=method,\n    )\n    # Padding the resized spectrogram to maintain the original dimensions\n    padded_spectrogram = tf.pad(\n        resized_spectrogram,\n        paddings=[\n            [\n                (spectrogram.shape[0] - resized_spectrogram.shape[0]) // 2,\n                (spectrogram.shape[0] - resized_spectrogram.shape[0]) // 2,\n            ],\n            [0, 0],\n        ],\n    )\n    return padded_spectrogram\n\n", "CodeBleu_score": 25.722098637395362}, {"code": "def pitch_shift(\n    spectrogram: tf.Tensor,\n    semitone_shift: float = 0.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n    if semitone_shift == 0.0:\n        return spectrogram\n\n    num_rows = tf.shape(spectrogram)[0]\n    num_cols = tf.shape(spectrogram)[1]\n\n    # Calculate the resizing factor\n    semitone_shift_factor = tf.math.exp(\n        tf.cast(semitone_shift, tf.float32) * (tf.math.log(2.0) / 12.0)\n    )\n    new_num_cols = tf.cast(tf.math.ceil(num_cols * semitone_shift_factor), tf.int32)\n\n    # Resize the spectrogram\n    resized_spectrogram = resize_images(\n        spectrogram, [num_rows, new_num_cols], method=method\n    )\n\n    # Pad the resized spectrogram to maintain the original dimensions\n    pad_size = tf.math.maximum(0, new_num_cols - num_cols)\n    resized_spectrogram = tf.pad(\n        resized_spectrogram, [[0, 0], [0, pad_size], [0, 0]], \"CONSTANT\"\n    )\n\n    return resized_spectrogram\n\n", "CodeBleu_score": 45.03446888810306}, {"code": "def pitch_shift(\n    spectrogram: tf.Tensor,\n    semitone_shift: float = 0.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n\n    # Calculating the resizing factor\n    factor = 2.0 ** (semitone_shift / 12.0)\n\n    # Resizing the spectrogram along the frequency axis\n    resized_spectrogram = resize_images(\n        images=spectrogram,\n        size=[int(spectrogram.shape[0] * factor), spectrogram.shape[1]],\n        method=method,\n    )\n\n    # Padding the resized spectrogram to maintain the original dimensions\n    padded_spectrogram = tf.pad(\n        tensor=resized_spectrogram,\n        paddings=[[0, spectrogram.shape[0] - resized_spectrogram.shape[0]], [0, 0]],\n    )\n\n    return padded_spectrogram\n\n", "CodeBleu_score": 29.476468052048975}]}
{"instance_id": "612f0826e56a", "generation_result": [{"code": "def _get_conv_activation_layer(params: Dict) -> Any\n\t\n    if params.get(\"conv_activation\") is None:\n        return ReLU()\n    elif params.get(\"conv_activation\") == \"ReLU\":\n        return ReLU()\n    elif params.get(\"conv_activation\") == \"ELU\":\n        return ELU()\n    elif params.get(\"conv_activation\") == \"LeakyReLU\":\n        return LeakyReLU(alpha=0.2)\n    else:\n        raise ValueError(\n            \"The specified convolutional activation function is not supported. Please choose one of the following: ReLU, ELU, LeakyReLU\"\n        )\n\n", "CodeBleu_score": 45.35577372892645}, {"code": "def _get_conv_activation_layer(params: Dict) -> Any\n\t\n    if params.get(\"conv_activation\", \"ReLU\") == \"ReLU\":\n        return ReLU()\n    elif params.get(\"conv_activation\", \"ReLU\") == \"LeakyReLU\":\n        return LeakyReLU(alpha=0.2)\n    elif params.get(\"conv_activation\", \"ReLU\") == \"ELU\":\n        return ELU()\n    else:\n        raise ValueError(\n            f\"The provided conv_activation parameter: {params.get('conv_activation', 'ReLU')} is not supported. Use ReLU, LeakyReLU, or ELU.\"\n        )\n\n", "CodeBleu_score": 42.91439526266985}, {"code": "def _get_conv_activation_layer(params: Dict) -> Any\n\t\n    if params.get(\"conv_activation\") == \"ReLU\":\n        return nn.ReLU()\n    elif params.get(\"conv_activation\") == \"ELU\":\n        return nn.ELU()\n    elif params.get(\"conv_activation\") == \"LeakyReLU\":\n        return nn.LeakyReLU(negative_slope=0.2)\n    else:\n        raise ValueError(\"The specified activation function is not supported.\")\n\n", "CodeBleu_score": 35.94270728772223}]}
{"instance_id": "2f8b5a2fc0f2", "generation_result": [{"code": "def softmax_unet(\n    input_tensor: tf.Tensor, instruments: Iterable[str], params: Dict = {}\n) -> Dict\n\t\n    output_mask_logit = params.get(\"output_mask_logit\", False)\n    output_name = params.get(\"output_name\", \"output\")\n    logit_mask_list = []\n    for instrument in instruments:\n        logit_mask_list.append(\n            apply_unet(\n                input_tensor,\n                output_name=f\"{output_name}_{instrument}\",\n                params=params,\n                output_mask_logit=output_mask_logit,\n            )\n        )\n    logit_mask = tf.stack(logit_mask_list, axis=-1)\n    output_dict = {}\n    for instrument, logit_mask_instrument in zip(instruments, logit_mask_list):\n        output_dict[instrument] = Multiply(name=instrument)([logit_mask_instrument, input_tensor])\n    return output_dict\n\n", "CodeBleu_score": 38.446922135741765}, {"code": "def softmax_unet(\n    input_tensor: tf.Tensor, instruments: Iterable[str], params: Dict = {}\n) -> Dict\n\t\n    output_dict = {}\n    for instrument in instruments:\n        output_dict[instrument] = apply_unet(\n            input_tensor, output_name=instrument, params=params\n        )\n    return output_dict\n\n", "CodeBleu_score": 18.26797141800781}, {"code": "def softmax_unet(\n    input_tensor: tf.Tensor, instruments: Iterable[str], params: Dict = {}\n) -> Dict\n\t\n    logit_mask_list = []\n    for i, instrument in enumerate(instruments):\n        logit_mask_list.append(\n            apply_unet(\n                input_tensor,\n                output_name=f\"output_{i}\",\n                params=params,\n                output_mask_logit=True,\n            )\n        )\n    logit_mask = tf.stack(logit_mask_list, axis=-1)\n    output_dict = {}\n    for i, instrument in enumerate(instruments):\n        output_dict[instrument] = Multiply(name=instrument)([logit_mask[:, :, :, i], input_tensor])\n    return output_dict\n\n", "CodeBleu_score": 43.728603967040556}]}
{"instance_id": "400b19ce8520", "generation_result": [{"code": "def compute_file_checksum(path)\n\t\n    # Initialize a SHA-256 hash object\n    sha256_hash = hashlib.sha256()\n\n    # Read and update hash in chunks of 4K\n    with open(path, 'rb') as f:\n        for byte_block in iter(lambda: f.read(4096), b\"\"):\n            sha256_hash.update(byte_block)\n\n    # Return hexadecimal representation of the hash\n    return sha256_hash.hexdigest()\n", "CodeBleu_score": 47.44240092901125}, {"code": "def compute_file_checksum(path)\n\t\n    # Initialize a SHA-256 hash object\n    sha256_hash = hashlib.sha256()\n\n    # Read and update hash in chunks of 4K\n    with open(path, 'rb') as f:\n        for byte_block in iter(lambda: f.read(4096), b\"\"):\n            sha256_hash.update(byte_block)\n\n    # Return the hexadecimal representation of the hash\n    return sha256_hash.hexdigest()\n", "CodeBleu_score": 47.44240092901125}, {"code": "def compute_file_checksum(path)\n\t\n    sha256 = hashlib.sha256()\n    with open(path, 'rb') as f:\n        while True:\n            chunk = f.read(1024)\n            if not chunk:\n                break\n            sha256.update(chunk)\n    return sha256.hexdigest()\n", "CodeBleu_score": 41.81003406338342}]}
{"instance_id": "5f2e9c6e15f1", "generation_result": [{"code": "def checksum(self, name: str) -> str\n\t\n    url = f\"{self.host}/{self.repository}/{self.release}/{name}/checksum.txt\"\n    response = httpx.get(url)\n    response.raise_for_status()\n    try:\n        return response.json()[name]\n    except KeyError:\n        raise ValueError(f\"Model name {name} not found in checksum index.\")\n\n", "CodeBleu_score": 27.842054191066378}, {"code": "def checksum(self, name: str) -> str\n\t\n    url = f\"{self.host}/{self.repository}/{self.release}/index.json\"\n    response = httpx.get(url)\n    response.raise_for_status()\n    index = response.json()\n    if name not in index:\n        raise ValueError(f\"No model named {name} found in index.\")\n    return index[name][\"checksum\"]\n", "CodeBleu_score": 37.096075329222955}, {"code": "def checksum(self, name: str) -> str\n\t\n    url = f\"{self.host}/{self.repository}/{self.release_path}/{self.checksum_index}\"\n    response = httpx.get(url)\n    response.raise_for_status()\n    index = response.json()\n    if name not in index:\n        raise ValueError(f\"No checksum for model {name} found in index\")\n    return index[name]\n", "CodeBleu_score": 40.58931822489463}]}
{"instance_id": "9312c8b6032e", "generation_result": [{"code": "def download(self, name: str, path: str) -> None:\n\t\n    url: str = \"/\".join((self._host, self._repository, self.RELEASE_PATH, self._release, name))\n    logger.info(f\"Downloading {url}\")\n    with client.stream(\"GET\", url) as response:\n        response.raise_for_status()\n        with NamedTemporaryFile() as stream:\n            for chunk in response.iter_raw():\n                stream.write(chunk)\n            stream.flush()\n            stream.seek(0)\n            checksum: str = compute_file_checksum(stream.name)\n            if checksum != self.checksum(name):\n                raise IOError(f\"Invalid checksum for {name}\")\n            logger.info(f\"Extracting {url} to {path}\")\n            with tarfile.open(stream.name) as tar:\n                tar.extractall(path)\n            os.unlink(stream.name)\n", "CodeBleu_score": 51.68933606696896}, {"code": "def download(self, name: str, path: str) -> None:\n\t\n    logger.info(f\"Downloading model {name}\")\n    url: str = \"/\".join(\n        (\n            self._host,\n            self._repository,\n            self.RELEASE_PATH,\n            self._release,\n            self.MODELS_PATH,\n            name,\n        )\n    )\n    response: httpx.Response = httpx.stream(\"GET\", url)\n    response.raise_for_status()\n    with NamedTemporaryFile() as tmp:\n        for chunk in response.iter_raw():\n            tmp.write(chunk)\n        tmp.flush()\n        checksum: str = compute_file_checksum(tmp.name)\n        if checksum != self.checksum(name):\n            raise IOError(\n                f\"Invalid checksum for model {name} (expected {self.checksum(name)}, got {checksum})\"\n            )\n        logger.info(f\"Extracting model {name}\")\n        with tarfile.open(tmp.name, \"r:gz\") as tar:\n            tar.extractall(path)\n        os.unlink(tmp.name)\n", "CodeBleu_score": 47.80208233555631}, {"code": "def download(self, name: str, path: str) -> None:\n\t\n    url: str = \"/\".join((self._host, self._repository, self.RELEASE_PATH, self._release, name))\n    logger.info(f\"Downloading {name} from {url}\")\n    with httpx.stream(\"GET\", url) as response:\n        response.raise_for_status()\n        with NamedTemporaryFile() as temp_file:\n            for chunk in response.iter_raw():\n                temp_file.write(chunk)\n            temp_file.flush()\n            temp_file.seek(0)\n            checksum: str = compute_file_checksum(temp_file.name)\n            if checksum != self.checksum(name):\n                raise IOError(f\"Invalid checksum for {name}\")\n            tar = tarfile.open(temp_file.name, \"r:gz\")\n            tar.extractall(path)\n            tar.close()\n            os.unlink(temp_file.name)\n    logger.info(f\"Successfully downloaded {name} to {path}\")\n", "CodeBleu_score": 51.00022118596416}]}
{"instance_id": "ee9493f37d77", "generation_result": [{"code": "def load_configuration(descriptor: str) -> Dict\n\t\n    if descriptor.startswith(\"resource:\"):\n        # Embedded configuration\n        configuration_name = descriptor.split(\"resource:\")[1]\n        configuration_json = loader.get_resource_as_text(configuration_name)\n        if configuration_json is None:\n            raise SpleeterError(\n                \"Configuration '{0}' not found.\".format(configuration_name))\n        return json.loads(configuration_json)\n    else:\n        # File configuration\n        if not os.path.exists(descriptor):\n            raise SpleeterError(\n                \"Configuration file '{0}' not found.\".format(descriptor))\n        with open(descriptor, \"r\") as file:\n            return json.load(file)\n", "CodeBleu_score": 43.47702691480889}, {"code": "def load_configuration(descriptor: str) -> Dict\n\t\n    if descriptor.startswith('embedded:'):\n        return json.loads(loader.get_resource(descriptor[9:]).read_bytes())\n    elif loader.exists(descriptor):\n        with open(descriptor, 'r') as file:\n            return json.load(file)\n    else:\n        raise SpleeterError(\n            'Configuration file not found: {}'.format(descriptor))\n\n", "CodeBleu_score": 28.723696105138036}, {"code": "def load_configuration(descriptor: str) -> Dict\n\t\n    if descriptor.startswith(\"spleeter:\"):\n        configuration_name = descriptor.split(\":\")[1]\n        configuration_path = f\"spleeter:configs/{configuration_name}.json\"\n        if loader.is_resource(configuration_path):\n            return json.load(loader.open_text(configuration_path))\n        raise SpleeterError(\n            f\"Configuration '{configuration_name}' not found in the embedded resources.\"\n        )\n    if os.path.exists(descriptor):\n        with open(descriptor, \"r\") as configuration_file:\n            return json.load(configuration_file)\n    raise SpleeterError(f\"Configuration file '{descriptor}' not found.\")\n\n", "CodeBleu_score": 50.34915921328891}]}
{"instance_id": "546134b67fbc", "generation_result": [{"code": "def sync_apply(\n    tensor_dict: Dict[str, tf.Tensor], func: Callable, concat_axis: int = 1\n) -> Dict[str, tf.Tensor]\n\t\n    if concat_axis not in [0, 1]:\n        raise ValueError(\"The concatenation axis must be either 0 or 1.\")\n    concat_tensors = tf.concat(list(tensor_dict.values()), axis=concat_axis)\n    processed_tensor = func(concat_tensors)\n    if concat_axis == 0:\n        processed_tensors = tf.split(processed_tensor, len(tensor_dict))\n    else:\n        processed_tensors = tf.split(processed_tensor, len(tensor_dict.values()), axis=0)\n    return dict(zip(tensor_dict.keys(), processed_tensors))\n", "CodeBleu_score": 39.755175846974645}, {"code": "def sync_apply(\n    tensor_dict: Dict[str, tf.Tensor], func: Callable, concat_axis: int = 1\n) -> Dict[str, tf.Tensor]\n\t\n    if concat_axis not in [0, 1]:\n        raise ValueError(\n            \"The concatenation axis must be either 0 or 1, but was {}.\".format(\n                concat_axis\n            )\n        )\n    keys = list(tensor_dict.keys())\n    values = list(tensor_dict.values())\n    concat_tensor = tf.concat(values, concat_axis)\n    processed_tensor = func(concat_tensor)\n    if concat_axis == 0:\n        processed_tensor = tf.split(processed_tensor, len(values))\n    else:\n        processed_tensor = tf.split(processed_tensor, len(values), axis=1)\n    processed_tensor = dict(zip(keys, processed_tensor))\n    return processed_tensor\n\n", "CodeBleu_score": 46.7570574824485}, {"code": "def sync_apply(\n    tensor_dict: Dict[str, tf.Tensor], func: Callable, concat_axis: int = 1\n) -> Dict[str, tf.Tensor]\n\t\n    if concat_axis == 0:\n        raise NotImplementedError\n    elif concat_axis == 1:\n        concat_tensor = tf.concat(list(tensor_dict.values()), axis=concat_axis)\n        processed_tensor = func(concat_tensor)\n        split_tensors = tf.split(processed_tensor, len(tensor_dict), axis=concat_axis)\n        return dict(zip(tensor_dict.keys(), split_tensors))\n    else:\n        raise ValueError(\n            f\"The concatenation axis must be either 0 or 1, but {concat_axis} was given.\"\n        )\n", "CodeBleu_score": 37.83995667852048}]}
{"instance_id": "df48e78d46d2", "generation_result": [{"code": "def from_float32_to_uint8(\n    tensor: tf.Tensor,\n    tensor_key: str = \"tensor\",\n    min_key: str = \"min\",\n    max_key: str = \"max\",\n) -> tf.Tensor\n\t\n    assert tensor.dtype == tf.float32, f\"The tensor {tensor_key} has the wrong dtype: {tensor.dtype}.\"\n    min_value = tf.reduce_min(tensor)\n    max_value = tf.reduce_max(tensor)\n    assert min_value <= max_value, f\"The tensor {tensor_key} has min_value > max_value: {min_value} > {max_value}.\"\n    tensor = (tensor - min_value) / (max_value - min_value)\n    tensor = tf.cast(tensor * 255, tf.uint8)\n    return {\n        tensor_key: tensor,\n        min_key: min_value,\n        max_key: max_value,\n    }\n\n", "CodeBleu_score": 48.52104605771882}, {"code": "def from_float32_to_uint8(\n    tensor: tf.Tensor,\n    tensor_key: str = \"tensor\",\n    min_key: str = \"min\",\n    max_key: str = \"max\",\n) -> tf.Tensor\n\t\n    # Compute the minimum and maximum values of the tensor\n    min_value = tf.reduce_min(tensor)\n    max_value = tf.reduce_max(tensor)\n    # Scale the tensor values to the uint8 range\n    tensor_uint8 = tf.cast(\n        tf.math.round((tensor - min_value) * 255 / (max_value - min_value)), tf.uint8\n    )\n    # Return a dictionary containing the scaled uint8 tensor, the original minimum value, and the original maximum value\n    return {tensor_key: tensor_uint8, min_key: min_value, max_key: max_value}\n\n", "CodeBleu_score": 40.56289967253325}, {"code": "def from_float32_to_uint8(\n    tensor: tf.Tensor,\n    tensor_key: str = \"tensor\",\n    min_key: str = \"min\",\n    max_key: str = \"max\",\n) -> tf.Tensor\n\t\n    # Get the minimum and maximum values of the tensor\n    tensor_min = tf.reduce_min(tensor)\n    tensor_max = tf.reduce_max(tensor)\n    # Scale the tensor values to the uint8 range (0-255)\n    tensor_scaled = tf.cast(\n        tf.divide(tf.subtract(tensor, tensor_min), tf.subtract(tensor_max, tensor_min)),\n        tf.uint8,\n    )\n    # Create a dictionary containing the scaled tensor, the original minimum value, and the original maximum value\n    tensor_dict = {\n        tensor_key: tensor_scaled,\n        min_key: tensor_min,\n        max_key: tensor_max,\n    }\n    # Return the dictionary\n    return tensor_dict\n\n", "CodeBleu_score": 51.857376292439426}]}
{"instance_id": "7f479e5de492", "generation_result": [{"code": "def pad_and_partition(tensor: tf.Tensor, segment_len: int) -> tf.Tensor\n\t\n    pad_len = tf.math.mod(tf.shape(tensor)[0], segment_len)\n    pad_len = tf.cond(tf.math.equal(pad_len, 0), lambda: 0, lambda: segment_len - pad_len)\n    padded = tf.pad(tensor, [[0, pad_len], [0, 0]])\n    return tf.reshape(padded, [-1, segment_len, padded.shape[1]])\n\n", "CodeBleu_score": 30.493012547458083}, {"code": "def pad_and_partition(tensor: tf.Tensor, segment_len: int) -> tf.Tensor\n\t\n    pad_len = segment_len - (tf.shape(tensor)[0] % segment_len)\n    padded = tf.pad(tensor, [[0, pad_len], [0, 0]])\n    return tf.reshape(padded, [-1, segment_len, tf.shape(tensor)[1]])\n\n", "CodeBleu_score": 24.220515497328257}, {"code": "def pad_and_partition(tensor: tf.Tensor, segment_len: int) -> tf.Tensor\n\t\n    # Calculate the number of padding elements needed to make the tensor length a multiple of the segment length\n    pad_len = tf.math.floormod(tf.shape(tensor)[0], segment_len)\n    # Pad the tensor\n    tensor = tf.pad(tensor, [[0, pad_len], [0, 0]])\n    # Reshape the tensor into segments of the specified length\n    return tf.reshape(tensor, [-1, segment_len])\n", "CodeBleu_score": 20.150622765621954}]}
{"instance_id": "1789fae7253f", "generation_result": [{"code": "def pad_and_reshape(instr_spec, frame_length, F) -> Any\n\t\n    # Pad spectrogram\n    pad_rows = frame_length - tf.shape(instr_spec)[0] % frame_length\n    pad_cols = frame_length - tf.shape(instr_spec)[1] % frame_length\n    padded = tf.pad(instr_spec, [[0, pad_rows], [0, pad_cols], [0, 0]])\n    # Reshape spectrogram\n    reshaped = tf.reshape(padded, [-1, frame_length, F])\n    return reshaped\n\n", "CodeBleu_score": 14.578329421075301}, {"code": "def pad_and_reshape(instr_spec, frame_length, F) -> Any\n\t\n    # Calculate the number of rows to pad\n    pad_size = frame_length - (tf.shape(instr_spec)[1] % frame_length)\n    # Pad the input spectrogram with zeros\n    instr_spec = tf.pad(instr_spec, tf.constant([[0, 0], [0, pad_size], [0, 0]]))\n    # Reshape the input spectrogram\n    instr_spec = tf.reshape(instr_spec, [-1, frame_length, F])\n    return instr_spec\n", "CodeBleu_score": 12.741586743266971}, {"code": "def pad_and_reshape(instr_spec, frame_length, F) -> Any\n\t\n    extra_rows = frame_length - (tf.shape(instr_spec)[1] % frame_length)\n    if extra_rows != 0:\n        pad = tf.zeros(shape=(tf.shape(instr_spec)[0], extra_rows, tf.shape(instr_spec)[2]))\n        instr_spec_padded = tf.concat([instr_spec, pad], axis=1)\n    else:\n        instr_spec_padded = instr_spec\n    instr_spec_reshaped = tf.reshape(instr_spec_padded, [tf.shape(instr_spec_padded)[0], -1, F])\n    return instr_spec_reshaped\n", "CodeBleu_score": 33.48975562641214}]}
{"instance_id": "8a4f4658d97c", "generation_result": [{"code": "def dataset_from_csv(csv_path: str, **kwargs) -> Any\n\t\n    df = pd.read_csv(csv_path)\n    return from_tensor_slices(df.values, **kwargs)\n\n", "CodeBleu_score": 21.217631595288058}, {"code": "def dataset_from_csv(csv_path: str, **kwargs) -> Any\n\t\n    df = pd.read_csv(csv_path, **kwargs)\n    df = df.to_dict('list')\n    return from_tensor_slices(df)\n\n", "CodeBleu_score": 27.859445658593256}, {"code": "def dataset_from_csv(csv_path: str, **kwargs) -> Any\n\t\n    return from_tensor_slices(pd.read_csv(csv_path, **kwargs).values)\n\n\n\n\n", "CodeBleu_score": 20.009547615835718}]}
{"instance_id": "9a16e81b182f", "generation_result": [{"code": "def check_tensor_shape(tensor_tf: tf.Tensor, target_shape: Any) -> bool\n\t\n    result = True\n    for i, dim in enumerate(target_shape):\n        if dim is not None:\n            result = tf.logical_and(result, tf.equal(tf.shape(tensor_tf)[i], dim))\n    return result\n\n", "CodeBleu_score": 41.45568730689605}, {"code": "def check_tensor_shape(tensor_tf: tf.Tensor, target_shape: Any) -> bool\n\t\n    result = True\n    for i, length in enumerate(target_shape):\n        if length is not None:\n            result = result & (tf.shape(tensor_tf)[i] == length)\n    return result\n", "CodeBleu_score": 35.77260641511835}, {"code": "def check_tensor_shape(tensor_tf: tf.Tensor, target_shape: Any) -> bool\n\t\n    result = True\n    for i, dim in enumerate(target_shape):\n        if dim is not None:\n            result = tf.logical_and(result, tf.equal(tf.shape(tensor_tf)[i], dim))\n    return result\n", "CodeBleu_score": 41.45568730689605}]}
{"instance_id": "49d5d3084b3a", "generation_result": [{"code": "def get_training_dataset(\n    audio_params: Dict, audio_adapter: AudioAdapter, audio_path: str\n) -> Any\n\t\n    builder = DatasetBuilder(\n        audio_params=audio_params,\n        audio_adapter=audio_adapter,\n        chunk_duration=audio_params[\"chunk_duration\"],\n        random_seed=audio_params[\"random_seed\"],\n    )\n    dataset = builder.build(\n        csv_path=audio_path,\n        batch_size=audio_params[\"batch_size\"],\n        shuffle=audio_params[\"shuffle\"],\n        convert_to_uint=audio_params[\"convert_to_uint\"],\n        random_data_augmentation=audio_params[\"random_data_augmentation\"],\n        random_time_crop=audio_params[\"random_time_crop\"],\n        infinite_generator=audio_params[\"infinite_generator\"],\n        cache_directory=audio_params[\"cache_directory\"],\n        wait_for_cache=audio_params[\"wait_for_cache\"],\n        num_parallel_calls=audio_params[\"num_parallel_calls\"],\n        n_chunks_per_song=audio_params[\"n_chunks_per_song\"],\n    )\n    return dataset\n", "CodeBleu_score": 30.529289598188182}, {"code": "def get_training_dataset(\n    audio_params: Dict, audio_adapter: AudioAdapter, audio_path: str\n) -> Any\n\t\n    builder = DatasetBuilder(\n        audio_params=audio_params,\n        audio_adapter=audio_adapter,\n        chunk_duration=audio_params[\"chunk_duration\"],\n        random_seed=audio_params[\"random_seed\"],\n    )\n    dataset = builder.build(\n        csv_path=audio_path,\n        batch_size=audio_params[\"batch_size\"],\n        shuffle=audio_params[\"shuffle\"],\n        convert_to_uint=audio_params[\"convert_to_uint\"],\n        random_data_augmentation=audio_params[\"random_data_augmentation\"],\n        random_time_crop=audio_params[\"random_time_crop\"],\n        infinite_generator=audio_params[\"infinite_generator\"],\n        cache_directory=audio_params[\"cache_directory\"],\n        wait_for_cache=audio_params[\"wait_for_cache\"],\n        num_parallel_calls=audio_params[\"num_parallel_calls\"],\n        n_chunks_per_song=audio_params[\"n_chunks_per_song\"],\n    )\n    return dataset\n\n", "CodeBleu_score": 30.529289598188182}, {"code": "def get_training_dataset(\n    audio_params: Dict, audio_adapter: AudioAdapter, audio_path: str\n) -> Any\n\t\n    builder = DatasetBuilder(\n        chunk_duration=audio_params[\"chunk_duration\"],\n        random_seed=audio_params[\"random_seed\"],\n    )\n    dataset = builder.build(\n        audio_path,\n        batch_size=audio_params[\"batch_size\"],\n        shuffle=audio_params[\"shuffle\"],\n        convert_to_uint=audio_params[\"convert_to_uint\"],\n        random_data_augmentation=audio_params[\"random_data_augmentation\"],\n        random_time_crop=audio_params[\"random_time_crop\"],\n        infinite_generator=audio_params[\"infinite_generator\"],\n        cache_directory=audio_params[\"cache_directory\"],\n        wait_for_cache=audio_params[\"wait_for_cache\"],\n        num_parallel_calls=audio_params[\"num_parallel_calls\"],\n        n_chunks_per_song=audio_params[\"n_chunks_per_song\"],\n    )\n    return dataset\n\n", "CodeBleu_score": 32.606841330465635}]}
{"instance_id": "fe8bbbba06e3", "generation_result": [{"code": "def instruments(self) -> Any\n\t\n    if self.instrument_builders is None:\n        self.instrument_builders = []\n        for instrument in self.instruments:\n            self.instrument_builders.append(InstrumentDatasetBuilder(instrument, self))\n    for builder in self.instrument_builders:\n        yield builder\n\n", "CodeBleu_score": 58.632255320055}, {"code": "def instruments(self) -> Any\n\t\n    if self._instrument_builders is None:\n        self._instrument_builders = [InstrumentDatasetBuilder(self, instrument) for instrument in self.instruments]\n    for instrument_builder in self._instrument_builders:\n        yield instrument_builder\n\n", "CodeBleu_score": 44.48395223477321}, {"code": "def instruments(self) -> Any\n\t\n    if not self.instrument_builders:\n        self.instrument_builders = [InstrumentDatasetBuilder(instrument) for instrument in self.instruments]\n    for instrument_builder in self.instrument_builders:\n        yield instrument_builder\n", "CodeBleu_score": 32.2973833908857}]}
{"instance_id": "438b57fd3413", "generation_result": [{"code": "def cache(self, dataset: Any, cache: Optional[str], wait: bool) -> Any\n\t\n    if cache is not None:\n        if wait:\n            while not exists(f\"{cache}.index\"):\n                logger.info(f\"Cache not available, wait {self.WAIT_PERIOD}\")\n                time.sleep(self.WAIT_PERIOD)\n        cache_path = os.path.split(cache)[0]\n        os.makedirs(cache_path, exist_ok=True)\n        return dataset.cache(cache)\n    return dataset\n", "CodeBleu_score": 93.65648279369884}, {"code": "def cache(self, dataset: Any, cache: Optional[str], wait: bool) -> Any\n\t\n    if cache is not None:\n        if wait:\n            while not exists(f\"{cache}.index\"):\n                logger.info(f\"Cache not available, wait {self.WAIT_PERIOD}\")\n                time.sleep(self.WAIT_PERIOD)\n        cache_path = os.path.split(cache)[0]\n        os.makedirs(cache_path, exist_ok=True)\n        return dataset.cache(cache)\n    return dataset\n", "CodeBleu_score": 93.65648279369884}, {"code": "def cache(self, dataset: Any, cache: Optional[str], wait: bool) -> Any\n\t\n    if cache is not None:\n        if wait:\n            while not exists(f\"{cache}.index\"):\n                logger.info(f\"Cache not available, wait {self.WAIT_PERIOD}\")\n                time.sleep(self.WAIT_PERIOD)\n        cache_path = os.path.split(cache)[0]\n        os.makedirs(cache_path, exist_ok=True)\n        return dataset.cache(cache)\n    return dataset\n", "CodeBleu_score": 93.65648279369884}]}
{"instance_id": "8838a3a09a66", "generation_result": [{"code": "def create_estimator(params: Dict, MWF: bool) -> tf.Tensor\n\t\n\n    # Load the model directory\n    model_provider = ModelProvider(params[\"model_dir\"])\n\n    # Update the parameters\n    params = model_provider.update_parameters(params)\n\n    # Set up the session configuration\n    session_config = ConfigProto(\n        gpu_options=tf.GPUOptions(\n            allow_growth=True\n        )\n    )\n\n    # Create a RunConfig\n    run_config = RunConfig(\n        session_config=session_config,\n        save_checkpoints_steps=params[\"save_checkpoints_steps\"],\n        save_summary_steps=params[\"save_summary_steps\"],\n        keep_checkpoint_max=params[\"keep_checkpoint_max\"]\n    )\n\n    # Initialize and return a TensorFlow estimator\n    return tf.estimator.Estimator(\n        model_fn=model_provider.get_model_fn(MWF),\n        model_dir=params[\"model_dir\"],\n        params=params,\n        config=run_config\n    )\n", "CodeBleu_score": 28.429438172303534}, {"code": "def create_estimator(params: Dict, MWF: bool) -> tf.Tensor\n\t\n    model_dir = params.get(\"model_dir\", None)\n    model_provider = ModelProvider(model_dir)\n    params = model_provider.update_params(params)\n    session_config = ConfigProto()\n    session_config.gpu_options.allow_growth = True\n    run_config = RunConfig(session_config=session_config)\n    estimator = tf.estimator.Estimator(\n        model_fn=model_provider.model_fn, params=params, config=run_config\n    )\n    return estimator\n\n", "CodeBleu_score": 34.65669133582924}, {"code": "def create_estimator(params: Dict, MWF: bool) -> tf.Tensor\n\t\n    model_provider = ModelProvider(params[\"model_dir\"])\n    params = model_provider.update_params(params)\n    session_config = tf.ConfigProto()\n    session_config.gpu_options.allow_growth = True\n    run_config = tf.estimator.RunConfig(session_config=session_config)\n    estimator = tf.estimator.Estimator(\n        model_fn=model_provider.get(params[\"model_type\"]),\n        model_dir=params[\"model_dir\"],\n        params=params,\n        config=run_config)\n    return estimator\n\n", "CodeBleu_score": 38.640584824384895}]}
{"instance_id": "779c812bf2cb", "generation_result": [{"code": "def _separate_tensorflow(\n        self, waveform: np.ndarray, audio_descriptor: AudioDescriptor\n    ) -> Dict\n\t\n    if waveform.ndim != 2:\n        waveform = to_stereo(waveform)\n    self.update_data({\"waveform\": waveform, \"audio_id\": audio_descriptor.id})\n    prediction = next(self._get_prediction_generator())\n    prediction.pop(\"audio_id\")\n    return prediction\n", "CodeBleu_score": 52.22372341100117}, {"code": "def _separate_tensorflow(\n        self, waveform: np.ndarray, audio_descriptor: AudioDescriptor\n    ) -> Dict\n\t\n    waveform = _ensure_stereo(waveform)\n    self.update_data({\"waveform\": waveform, \"audio_id\": audio_descriptor.id})\n    prediction = next(self._get_prediction_generator())\n    prediction.pop(\"audio_id\")\n    return prediction\n", "CodeBleu_score": 36.43249568814283}, {"code": "def _separate_tensorflow(\n        self, waveform: np.ndarray, audio_descriptor: AudioDescriptor\n    ) -> Dict\n\t\n    waveform = ensure_stereo(waveform)\n    self.update_data((waveform, audio_descriptor.id))\n    prediction = next(self._get_prediction_generator())\n    prediction.pop(\"audio_id\")\n    return prediction\n", "CodeBleu_score": 31.724234481903572}]}
{"instance_id": "cf580cd2ed93", "generation_result": [{"code": "def compute_backbone_shapes(config, image_shape)\n\t\n    if callable(config.BACKBONE):\n        return config.COMPUTE_BACKBONE_SHAPE(image_shape)\n\n    assert config.BACKBONE in [\"resnet50\", \"resnet101\"], \"backbone is not resnet50 or resnet101\"\n    return np.array(\n        [[int(math.ceil(image_shape[0] / stride)),\n            int(math.ceil(image_shape[1] / stride))]\n            for stride in config.BACKBONE_STRIDES])\n\n", "CodeBleu_score": 88.67982951914676}, {"code": "def compute_backbone_shapes(config, image_shape)\n\t\n    if callable(config.BACKBONE):\n        return config.COMPUTE_BACKBONE_SHAPE(image_shape)\n\n    assert config.BACKBONE in [\"resnet50\", \"resnet101\"], \"backbone is not resnet50 or resnet101\"\n    return np.array(\n        [[int(math.ceil(image_shape[0] / stride)), int(math.ceil(image_shape[1] / stride))] for stride in config.BACKBONE_STRIDES]\n    )\n\n", "CodeBleu_score": 87.19476552695762}, {"code": "def compute_backbone_shapes(config, image_shape)\n\t\n    if callable(config.BACKBONE):\n        return config.COMPUTE_BACKBONE_SHAPE(image_shape)\n    assert config.BACKBONE in [\"resnet50\", \"resnet101\"]\n    return np.array(\n        [[int(math.ceil(image_shape[0] / stride)), int(math.ceil(image_shape[1] / stride))] for stride in config.BACKBONE_STRIDES])\n\n", "CodeBleu_score": 93.05088425331716}]}
{"instance_id": "ebd61a2e59ca", "generation_result": [{"code": "def clip_boxes_graph(boxes, window)\n\t\n    # Split\n    height, width = tf.split(window, 2)\n    x_min, y_min, x_max, y_max = tf.split(boxes, 4, axis=1)\n    # Clip\n    x_min_clipped = tf.maximum(tf.minimum(x_min, width), 0)\n    y_min_clipped = tf.maximum(tf.minimum(y_min, height), 0)\n    x_max_clipped = tf.maximum(tf.minimum(x_max, width), 0)\n    y_max_clipped = tf.maximum(tf.minimum(y_max, height), 0)\n    # Concatenate\n    clipped = tf.concat([x_min_clipped, y_min_clipped, x_max_clipped, y_max_clipped], axis=1)\n    clipped.set_shape((clipped.shape[0], 4))\n    return clipped\n", "CodeBleu_score": 27.517537847341856}, {"code": "def clip_boxes_graph(boxes, window)\n\t\n    # Split\n    wy1, wx1, wy2, wx2 = tf.split(window, 4)\n    y1, x1, y2, x2 = tf.split(boxes, 4, axis=1)\n    # Clip\n    y1 = tf.maximum(tf.minimum(y1, wy2), wy1)\n    x1 = tf.maximum(tf.minimum(x1, wx2), wx1)\n    y2 = tf.maximum(tf.minimum(y2, wy2), wy1)\n    x2 = tf.maximum(tf.minimum(x2, wx2), wx1)\n    clipped = tf.concat([y1, x1, y2, x2], axis=1)\n    clipped.set_shape((clipped.shape[0], 4))\n    return clipped\n\n", "CodeBleu_score": 56.01484333396114}, {"code": "def clip_boxes_graph(boxes, window)\n\t\n    # Split\n    wy1, wx1, wy2, wx2 = tf.split(window, 4)\n    y1, x1, y2, x2 = tf.split(boxes, 4, axis=1)\n    # Clip\n    y1 = tf.maximum(tf.minimum(y1, wy2), wy1)\n    x1 = tf.maximum(tf.minimum(x1, wx2), wx1)\n    y2 = tf.maximum(tf.minimum(y2, wy2), wy1)\n    x2 = tf.maximum(tf.minimum(x2, wx2), wx1)\n    clipped = tf.concat([y1, x1, y2, x2], axis=1, name=\"clipped_boxes\")\n    clipped.set_shape((clipped.shape[0], 4))\n    return clipped\n", "CodeBleu_score": 61.634432626045545}]}
{"instance_id": "8e0411587a70", "generation_result": [{"code": "def rpn_graph(feature_map, anchors_per_location, anchor_stride)\n\t\n    # Shared convolutional base of the RPN\n    shared = KL.Conv2D(512, (3, 3), padding='same', activation='relu',\n                       strides=anchor_stride,\n                       name='rpn_conv_shared')(feature_map)\n\n    # Anchor Score. [batch, height, width, anchors per location * 2].\n    x = KL.Conv2D(2 * anchors_per_location, (1, 1), padding='valid',\n                  activation='linear', name='rpn_class_raw')(shared)\n\n    # Reshape to [batch, anchors, 2]\n    rpn_class_logits = KL.Lambda(\n        lambda t: tf.reshape(t, [tf.shape(t)[0], -1, 2]))(x)\n\n    # Softmax on last dimension of BG/FG.\n    rpn_probs = KL.Activation(\n        \"softmax\", name=\"rpn_class_xxx\")(rpn_class_logits)\n\n    # Bounding box refinement. [batch, H, W, anchors per location, depth]\n    # where depth is [x, y, log(w), log(h)]\n    x = KL.Conv2D(anchors_per_location * 4, (1, 1), padding=\"valid\",\n                  activation='linear', name='rpn_bbox_pred')(shared)\n\n    # Reshape to [batch, anchors, 4]\n    rpn_bbox = KL.Lambda(lambda t: tf.reshape(t, [tf.shape(t)[0], -1, 4]))(x)\n\n    return [rpn_class_logits, rpn_probs, rpn_bbox]\n\n", "CodeBleu_score": 85.99714523366548}, {"code": "def rpn_graph(feature_map, anchors_per_location, anchor_stride)\n\t\n    # Shared convolutional base of the RPN\n    shared = KL.Conv2D(512, (3, 3), padding='same', activation='relu',\n                       strides=anchor_stride,\n                       name='rpn_conv_shared')(feature_map)\n\n    # Anchor Score. [batch, height, width, anchors per location * 2].\n    x = KL.Conv2D(2 * anchors_per_location, (1, 1), padding='valid',\n                  activation='linear', name='rpn_class_raw')(shared)\n\n    # Reshape to [batch, anchors, 2]\n    rpn_class_logits = KL.Lambda(\n        lambda t: tf.reshape(t, [tf.shape(t)[0], -1, 2]))(x)\n\n    # Softmax on last dimension of BG/FG.\n    rpn_probs = KL.Activation(\n        \"softmax\", name=\"rpn_class_xxx\")(rpn_class_logits)\n\n    # Bounding box refinement. [batch, H, W, anchors per location * depth]\n    # where depth is [x, y, log(w), log(h)]\n    x = KL.Conv2D(anchors_per_location * 4, (1, 1), padding=\"valid\",\n                  activation='linear', name='rpn_bbox_pred')(shared)\n\n    # Reshape to [batch, anchors, 4]\n    rpn_bbox = KL.Lambda(lambda t: tf.reshape(t, [tf.shape(t)[0], -1, 4]))(x)\n\n    return [rpn_class_logits, rpn_probs, rpn_bbox]\n\n", "CodeBleu_score": 85.99714523366548}, {"code": "def rpn_graph(feature_map, anchors_per_location, anchor_stride)\n\t\n\n    # Shared convolutional base of the RPN\n    shared = KL.Conv2D(512, (3, 3), padding='same', activation='relu',\n                       strides=anchor_stride,\n                       name='rpn_conv_shared')(feature_map)\n\n    # Anchor Score. [batch, height, width, anchors per location * 2].\n    x = KL.Conv2D(2 * anchors_per_location, (1, 1), padding='valid',\n                  activation='linear', name='rpn_class_raw')(shared)\n\n    # Reshape to [batch, anchors, 2]\n    rpn_class_logits = KL.Lambda(\n        lambda t: tf.reshape(t, [tf.shape(t)[0], -1, 2]))(x)\n\n    # Softmax on last dimension of BG/FG.\n    rpn_probs = KL.Activation(\n        \"softmax\", name=\"rpn_class_xxx\")(rpn_class_logits)\n\n    # Bounding box refinement. [batch, H, W, anchors per location * depth]\n    # where depth is [x, y, log(w), log(h)]\n    x = KL.Conv2D(anchors_per_location * 4, (1, 1), padding=\"valid\",\n                  activation='linear', name='rpn_bbox_pred')(shared)\n\n    # Reshape to [batch, anchors, 4]\n    rpn_bbox = KL.Lambda(lambda t: tf.reshape(t, [tf.shape(t)[0], -1, 4]))(x)\n\n    return [rpn_class_logits, rpn_probs, rpn_bbox]\n\n", "CodeBleu_score": 85.99714523366548}]}
{"instance_id": "70777c9261d8", "generation_result": [{"code": "def smooth_l1_loss(y_true, y_pred)\n\t\n    abs_loss = K.abs(y_true - y_pred)\n    sq_loss = 0.5 * (y_true - y_pred)**2\n    l1_loss = tf.where(K.less(abs_loss, 1.0), sq_loss, abs_loss - 0.5)\n    return K.sum(l1_loss, axis=-1)\n", "CodeBleu_score": 34.77161987881745}, {"code": "def smooth_l1_loss(y_true, y_pred)\n\t\n    diff = K.abs(y_true - y_pred)\n    less_than_one = K.cast(K.less(diff, 1.0), \"float32\")\n    loss = (less_than_one * 0.5 * diff**2) + (1 - less_than_one) * (diff - 0.5)\n    return loss\n\n", "CodeBleu_score": 60.928513203413225}, {"code": "def smooth_l1_loss(y_true, y_pred)\n\t\n    abs_loss = K.abs(y_true - y_pred)\n    sq_loss = 0.5 * (y_true - y_pred)**2\n    loss = tf.where(K.less(abs_loss, 1.0), sq_loss, abs_loss - 0.5)\n    return K.sum(loss)\n", "CodeBleu_score": 34.965227675738355}]}
{"instance_id": "0e747b604087", "generation_result": [{"code": "def build_model(x_train, num_classes)\n\t\n\n    # Reset the TensorFlow graph.\n    tf.reset_default_graph()\n\n    # Define the input layer.\n    inputs = KL.Input(shape=x_train.shape[1:])\n\n    # Add the first convolutional layer.\n    x = KL.Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(inputs)\n\n    # Add the second convolutional layer.\n    x = KL.Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(x)\n\n    # Add the max-pooling layer.\n    x = KL.MaxPooling2D(pool_size=(2, 2))(x)\n\n    # Add the flatten layer.\n    x = KL.Flatten()(x)\n\n    # Add the first dense layer.\n    x = KL.Dense(units=128, activation='relu')(x)\n\n    # Add the second dense layer.\n    x = KL.Dense(units=num_classes, activation='softmax')(x)\n\n    # Create the model.\n    model = KM.Model(inputs=inputs, outputs=x)\n\n    # Compile the model.\n    model.compile(loss='categorical_crossentropy',\n                  optimizer='adam',\n                  metrics=['accuracy'])\n\n    return model\n\n", "CodeBleu_score": 33.74760851785084}, {"code": "def build_model(x_train, num_classes)\n\t\n    tf.reset_default_graph()\n    x = KL.Input(shape=x_train.shape[1:])\n    x = KL.Conv2D(filters=32, kernel_size=(3, 3), strides=1, padding='same', activation='relu')(x)\n    x = KL.Conv2D(filters=32, kernel_size=(3, 3), strides=1, padding='same', activation='relu')(x)\n    x = KL.MaxPooling2D(pool_size=(2, 2), strides=2)(x)\n    x = KL.Flatten()(x)\n    x = KL.Dense(units=128, activation='relu')(x)\n    x = KL.Dense(units=num_classes, activation='softmax')(x)\n    model = KM.Model(inputs=x, outputs=x)\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n    return model\n\n", "CodeBleu_score": 34.437466661578334}, {"code": "def build_model(x_train, num_classes)\n\t\n    tf.reset_default_graph()\n    x = KL.Input(shape=(x_train.shape[1], x_train.shape[2], x_train.shape[3]))\n    x = KL.Conv2D(filters=32, kernel_size=(3, 3), strides=1, padding='same', activation='relu')(x)\n    x = KL.Conv2D(filters=32, kernel_size=(3, 3), strides=1, padding='same', activation='relu')(x)\n    x = KL.MaxPooling2D(pool_size=(2, 2), strides=2)(x)\n    x = KL.Conv2D(filters=64, kernel_size=(3, 3), strides=1, padding='same', activation='relu')(x)\n    x = KL.Conv2D(filters=64, kernel_size=(3, 3), strides=1, padding='same', activation='relu')(x)\n    x = KL.MaxPooling2D(pool_size=(2, 2), strides=2)(x)\n    x = KL.Flatten()(x)\n    x = KL.Dense(units=512, activation='relu')(x)\n    x = KL.Dense(units=num_classes, activation='softmax')(x)\n    model = KM.Model(inputs=x, outputs=x)\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model\n", "CodeBleu_score": 32.80168200134002}]}
{"instance_id": "813e3e2844f4", "generation_result": [{"code": "def extract_bboxes(mask)\n\t\n    # Extract horizontal and vertical indices where mask is present\n    horizontal_indicies = np.where(np.any(mask, axis=0))[0]\n    vertical_indicies = np.where(np.any(mask, axis=1))[0]\n    # If no mask is present, return array of zeros\n    if horizontal_indicies.shape[0]==0 or vertical_indicies.shape[0]==0:\n        return np.zeros((0,4))\n    # Extract left, top, right, and bottom pixel coordinates\n    x1, y1 = horizontal_indicies[[0, -1]]\n    x2, y2 = vertical_indicies[[0, -1]]\n    # Calculate the width and height of the bounding box\n    width, height = x2 - x1, y2 - y1\n    # Create output array of shape (1,4) and fill it with zeros\n    bbox = np.zeros((1,4))\n    # Populate output array with coordinates\n    bbox[0] = np.array([x1, y1, x2, y2])\n    return bbox\n", "CodeBleu_score": 32.4317256728593}, {"code": "def extract_bboxes(mask)\n\t\n    boxes = []\n    for i in range(mask.shape[-1]):\n        m = mask[:, :, i]\n        # Bounding box.\n        horizontal_indicies = np.where(np.any(m, axis=0))[0]\n        vertical_indicies = np.where(np.any(m, axis=1))[0]\n        if horizontal_indicies.shape[0]:\n            x1, x2 = horizontal_indicies[[0, -1]]\n            y1, y2 = vertical_indicies[[0, -1]]\n            # x2 and y2 should not be part of the box. Increment by 1.\n            x2 += 1\n            y2 += 1\n        else:\n            # No mask for this layer, therefore set the bbox to zeros\n            x1, x2, y1, y2 = 0, 0, 0, 0\n        boxes.append([x1, x2, y1, y2])\n    return boxes\n", "CodeBleu_score": 67.22627968806758}, {"code": "def extract_bboxes(mask)\n\t\n    boxes = np.zeros([mask.shape[-1], 4], dtype=np.int32)\n    for i in range(mask.shape[-1]):\n        m = mask[:, :, i]\n        # Bounding box.\n        horizontal_indicies = np.where(np.any(m, axis=0))[0]\n        vertical_indicies = np.where(np.any(m, axis=1))[0]\n        if horizontal_indicies.shape[0]:\n            x1, x2 = horizontal_indicies[[0, -1]]\n            y1, y2 = vertical_indicies[[0, -1]]\n            # x2 and y2 should not be part of the box. Increment by 1.\n            x2 += 1\n            y2 += 1\n        else:\n            # No mask for this layer, therefore set the box to zeros\n            x1, x2, y1, y2 = 0, 0, 0, 0\n        boxes[i] = np.array([y1, x1, y2, x2])\n    return boxes.astype(np.int32)\n", "CodeBleu_score": 79.44933982147248}]}
{"instance_id": "7ad5d430d6ed", "generation_result": [{"code": "def compute_iou(box, boxes, box_area, boxes_area)\n\t\n    # Calculate intersection areas\n    y1 = np.maximum(box[0], boxes[:, 0])\n    y2 = np.minimum(box[2], boxes[:, 2])\n    x1 = np.maximum(box[1], boxes[:, 1])\n    x2 = np.minimum(box[3], boxes[:, 3])\n    intersection = np.maximum(x2 - x1, 0) * np.maximum(y2 - y1, 0)\n    union = box_area + boxes_area[:] - intersection[:]\n    iou = intersection / union\n    return iou\n\n", "CodeBleu_score": 58.663259083089194}, {"code": "def compute_iou(box, boxes, box_area, boxes_area)\n\t\n    # Calculate intersection areas\n    y1 = np.maximum(box[0], boxes[:, 0])\n    y2 = np.minimum(box[2], boxes[:, 2])\n    x1 = np.maximum(box[1], boxes[:, 1])\n    x2 = np.minimum(box[3], boxes[:, 3])\n    intersection = np.maximum(x2 - x1, 0) * np.maximum(y2 - y1, 0)\n    union = box_area + boxes_area[:] - intersection[:]\n    iou = intersection / union\n    return iou\n\n", "CodeBleu_score": 58.663259083089194}, {"code": "def compute_iou(box, boxes, box_area, boxes_area)\n\t\n    # Calculate intersection areas\n    y1 = np.maximum(box[0], boxes[:, 0])\n    y2 = np.minimum(box[2], boxes[:, 2])\n    x1 = np.maximum(box[1], boxes[:, 1])\n    x2 = np.minimum(box[3], boxes[:, 3])\n    intersection = np.maximum(x2 - x1, 0) * np.maximum(y2 - y1, 0)\n    union = box_area + boxes_area[:] - intersection[:]\n    iou = intersection / union\n    return iou\n\n", "CodeBleu_score": 58.663259083089194}]}
{"instance_id": "15941b852859", "generation_result": [{"code": "def compute_overlaps(boxes1, boxes2)\n\t\n    # Areas of anchors and GT boxes\n    area1 = (boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1])\n    area2 = (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1])\n\n    # Compute overlaps to generate matrix [boxes1 count, boxes2 count]\n    # Each cell contains the IoU value.\n    overlaps = np.zeros((boxes1.shape[0], boxes2.shape[0]))\n    for i in range(overlaps.shape[1]):\n        box2 = boxes2[i]\n        overlaps[:, i] = compute_iou(box2, boxes1, area2[i], area1)\n    return overlaps\n\n", "CodeBleu_score": 62.2853282032944}, {"code": "def compute_overlaps(boxes1, boxes2)\n\t\n    # Areas of anchors and GT boxes\n    area1 = (boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1])\n    area2 = (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1])\n\n    # Compute overlaps to generate matrix [boxes1 count, boxes2 count]\n    # Each cell contains the IoU value.\n    overlaps = np.zeros((boxes1.shape[0], boxes2.shape[0]))\n    for i in range(overlaps.shape[1]):\n        box2 = boxes2[i]\n        overlaps[:, i] = compute_iou(box2, boxes1, area2[i], area1)\n    return overlaps\n\n", "CodeBleu_score": 62.2853282032944}, {"code": "def compute_overlaps(boxes1, boxes2)\n\t\n    # Areas of anchors and GT boxes\n    area1 = (boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1])\n    area2 = (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1])\n\n    # Compute overlaps to generate matrix [boxes1 count, boxes2 count]\n    # Each cell contains the IoU value.\n    overlaps = np.zeros((boxes1.shape[0], boxes2.shape[0]))\n    for i in range(overlaps.shape[1]):\n        box2 = boxes2[i]\n        overlaps[:, i] = compute_iou(box2, boxes1, area2[i], area1)\n    return overlaps\n\n", "CodeBleu_score": 62.2853282032944}]}
{"instance_id": "597533722ce3", "generation_result": [{"code": "def load_image(self, image_id):\n\t\n    image_info = self.image_info[image_id]\n    image = skimage.io.imread(image_info[\"path\"])\n    image = gray2rgb(image)\n    if image.shape[-1] == 4:\n        image = image[..., :3]\n    return image\n", "CodeBleu_score": 66.90258848052369}, {"code": "def load_image(self, image_id):\n\t\n    # load image\n    image = skimage.io.imread(self.image_info[image_id]['path'])\n    # If grayscale. Convert to RGB for consistency.\n    if image.ndim != 3:\n        image = skimage.color.gray2rgb(image)\n    # If has an alpha channel, remove it for consistency\n    if image.shape[-1] == 4:\n        image = image[..., :3]\n    return image\n", "CodeBleu_score": 100.0}, {"code": "def load_image(self, image_id):\n\t\n    image_info = self.image_info[image_id]\n    if image_info[\"source\"] != \"coco\":\n        return super(self.__class__, self).load_image(image_id)\n    image = skimage.io.imread(self.image_info[image_id][\"path\"])\n    image = gray2rgb(image)\n    if image.shape[-1] == 4:\n        image = image[..., :3]\n    return image\n", "CodeBleu_score": 67.648666011941}]}
{"instance_id": "9b852997ca05", "generation_result": [{"code": "def display_images(images, titles=None, cols=4, cmap=None, norm=None,\n                   interpolation=None)\n\t\n    # Create a figure with sub-plots.\n    num_images = len(images)\n    num_cols = min(num_images, cols)\n    num_rows = int(np.ceil(num_images / float(num_cols)))\n    fig, axes = plt.subplots(num_rows, num_cols, figsize=(num_cols, num_rows))\n    # Adjust vertical spacing if we need to print ensemble and best-net.\n    if (num_rows == 1):\n        fig.subplots_adjust(hspace=0.1)\n    if (num_rows == 2):\n        fig.subplots_adjust(hspace=0.2)\n    if (num_rows == 3):\n        fig.subplots_adjust(hspace=0.3)\n    if (num_rows == 4):\n        fig.subplots_adjust(hspace=0.4)\n    if (num_rows == 5):\n        fig.subplots_adjust(hspace=0.5)\n    if (num_rows == 6):\n        fig.subplots_adjust(hspace=0.6)\n    if (num_rows == 7):\n        fig.subplots_adjust(hspace=0.7)\n    if (num_rows == 8):\n        fig.subplots_adjust(hspace=0.8)\n    if (num_rows == 9):\n        fig.subplots_adjust(hspace=0.9)\n    # Interpolation type.\n    if interpolation is None:\n        interpolation = 'bilinear'\n    # Normalize image.\n    if norm is None:\n        norm = mcolors.Normalize(vmin=0, vmax=255)\n    # Cycle through images and plot them.\n    for i, (ax, img) in enumerate(zip(axes.flat, images)):\n        # Interpolation type.\n        if interpolation is None:\n            interpolation = 'bilinear'\n        # Plot image.\n        ax.imshow(img, cmap=cmap, norm=norm, interpolation=interpolation)\n        # Name of the image.\n        if titles is not None:\n            ax.set_title(titles[i], fontsize=9)\n        # Remove ticks from the plot.\n        ax.set_xticks([])\n        ax.set_yticks([])\n    # Ensure the plot is shown correctly with multiple plots\n    # in a single Notebook cell.\n    plt.show()\n", "CodeBleu_score": 26.793193580111836}, {"code": "def display_images(images, titles=None, cols=4, cmap=None, norm=None,\n                   interpolation=None)\n\t\n    # Set default values for titles and number of columns\n    if titles is None:\n        titles = ['Image (%d)' % i for i in range(1, len(images) + 1)]\n    if cols is None:\n        cols = len(images)\n\n    # Create figure and set title\n    plt.figure()\n    plt.suptitle('Displaying images')\n\n    # Adjust the subplot parameters so that the title and axes are aligned\n    # if necessary\n    rows = len(images) // cols + 1\n    for i in range(len(images)):\n        plt.subplot(rows, cols, i + 1)\n        plt.axis('off')\n        plt.imshow(images[i], cmap=cmap, norm=norm, interpolation=interpolation)\n        plt.title(titles[i])\n    plt.show()\n\n", "CodeBleu_score": 46.192333293303165}, {"code": "def display_images(images, titles=None, cols=4, cmap=None, norm=None,\n                   interpolation=None)\n\t\n    # Set default titles if none are provided\n    if titles is None:\n        titles = ['Image (%d)' % i for i in range(1, len(images) + 1)]\n    # Calculate the number of rows needed\n    rows = len(images) // cols + 1\n    # Create figure with subplots\n    fig = plt.figure()\n    for n, (image, title) in enumerate(zip(images, titles)):\n        # Add subplot\n        ax = fig.add_subplot(rows, cols, n + 1)\n        # Set title\n        ax.set_title(title)\n        # Turn off axis\n        plt.axis('off')\n        # Display image\n        plt.imshow(image, cmap=cmap, norm=norm, interpolation=interpolation)\n    # Show figure\n    plt.show()\n\n", "CodeBleu_score": 48.908076205315055}]}
