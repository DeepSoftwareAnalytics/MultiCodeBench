{"instance_id": "1f9f4d85e825", "generation_result": [{"code": "def load_model_from_config(self, half_attention)\n\t\n    if self.config.model.cache_text_encoder and os.path.exists(self.config.model.cache_text_encoder):\n        print(f\"Loading cached text encoder from {self.config.model.cache_text_encoder}\")\n        text_encoder = safetensors.torch.load_file(self.config.model.cache_text_encoder)\n        if \"model\" in text_encoder:\n            text_encoder = text_encoder[\"model\"]\n        text_encoder = text_encoder.eval().requires_grad_(False)\n    else:\n        text_encoder = instantiate_from_config(self.config.model.text_encoder)\n        if self.config.model.cache_text_encoder:\n            print(f\"Caching text encoder to {self.config.model.cache_text_encoder}\")\n            safetensors.torch.save_file(\n                self.config.model.cache_text_encoder, {\"model\": text_encoder.state_dict()}\n            )\n\n    if self.config.model.cache_unet and os.path.exists(self.config.model.cache_unet):\n        print(f\"Loading cached UNet from {self.config.model.cache_unet}\")\n        unet = safetensors.torch.load_file(self.config.model.cache_unet)\n        if \"model\" in unet:\n            unet = unet[\"model\"]\n        unet = unet.eval().requires_grad_(False)\n    else:\n        unet = instantiate_from_config(self.config.model.unet)\n        if self.config.model.cache_unet:\n            print(f\"Caching UNet to {self.config.model.cache_unet}\")\n            safetensors.torch.save_file(self.config.model.cache_unet, {\"model\": unet.state_dict()})\n\n    if self.config.model.cache_vae and os.path.exists(self.config.model.cache_vae):\n        print(f\"Loading cached VAE from {self.config.model.cache_vae}\")\n        vae = safetensors.torch.load_file(self.config.model.cache_vae)\n        if \"model\" in vae:\n            vae = vae[\"model\"]\n       ", "CodeBleu_score": 32.94622884205845}, {"code": "def load_model_from_config(self, half_attention)\n\t\n    if self.model is not None:\n        return self.model\n\n    if self.cache_dir is not None and os.path.exists(self.cache_dir):\n        if os.path.exists(self.cache_dir + \"/model.safetensors\"):\n            model = safetensors.torch.load_file(self.cache_dir + \"/model.safetensors\")\n            self.model = model\n            return self.model\n\n    model = instantiate_from_config(self.config)\n\n    if self.half_attention:\n        model.half_attention()\n\n    if self.memory_format:\n        model.set_memory_format()\n\n    model = model.to(self.device)\n    model.eval()\n\n    if self.cache_dir is not None:\n        os.makedirs(self.cache_dir, exist_ok=True)\n        safetensors.torch.save_file(model, self.cache_dir + \"/model.safetensors\")\n\n    self.model = model\n\n    return self.model\n\n", "CodeBleu_score": 27.86149916144905}, {"code": "def load_model_from_config(self, half_attention)\n\t\n    if self.model is not None:\n        return self.model\n\n    if self.cache_model and os.path.exists(self.cache_model):\n        model = self.load_cached_model()\n        if model is not None:\n            return model\n\n    model = self.load_model()\n    if self.cache_model:\n        self.save_cached_model(model)\n\n    return model\n\n", "CodeBleu_score": 13.235716288809323}]}
{"instance_id": "8dc955007aa2", "generation_result": [{"code": "def convsample_ddim(model, cond, steps, shape, eta=1.0, callback=None, normals_sequence=None,\n                    mask=None, x0=None, quantize_x0=False, temperature=1., score_corrector=None,\n                    corrector_kwargs=None, x_t=None\n                    )\n\t\n    sampler = DDIMSampler(model)\n    batch_size = shape[0]\n    print(f'eta: {eta} steps: {steps}')\n    samples, values = sampler.sample(\n        S=steps,\n        batch_size=batch_size,\n        shape=shape,\n        cond=cond,\n        eta=eta,\n        callback=callback,\n        normals_sequence=normals_sequence,\n        mask=mask,\n        x0=x0,\n        quantize_x0=quantize_x0,\n        temperature=temperature,\n        score_corrector=score_corrector,\n        corrector_kwargs=corrector_kwargs,\n        x_t=x_t\n    )\n    return samples, values\n\n", "CodeBleu_score": 49.834125884667124}, {"code": "def convsample_ddim(model, cond, steps, shape, eta=1.0, callback=None, normals_sequence=None,\n                    mask=None, x0=None, quantize_x0=False, temperature=1., score_corrector=None,\n                    corrector_kwargs=None, x_t=None\n                    )\n\t\n    sampler = DDIMSampler(model)\n    batch_size = shape[0]\n    print(f'eta: {eta}, steps: {steps}')\n    samples = sampler.sample(\n        batch_size=batch_size,\n        shape=shape,\n        cond=cond,\n        eta=eta,\n        callback=callback,\n        normals_sequence=normals_sequence,\n        mask=mask,\n        x0=x0,\n        quantize_x0=quantize_x0,\n        temperature=temperature,\n        score_corrector=score_corrector,\n        corrector_kwargs=corrector_kwargs,\n        x_t=x_t\n    )\n    return samples\n\n", "CodeBleu_score": 47.042278655593876}, {"code": "def convsample_ddim(model, cond, steps, shape, eta=1.0, callback=None, normals_sequence=None,\n                    mask=None, x0=None, quantize_x0=False, temperature=1., score_corrector=None,\n                    corrector_kwargs=None, x_t=None\n                    )\n\t\n    device = 'cuda'\n    sampler = DDIMSampler(model)\n    batch_size = shape[0]\n    print('sampling eta', eta,'steps', steps)\n    if x0 is None:\n        x_t = torch.randn(shape, device=device)\n    else:\n        x_t = x0\n    if quantize_x0:\n        x_t = x_t.add(torch.randn(x_t.shape).to(x_t) * temperature).clamp(-1, 1)\n    samples, values = sampler.sample(S=steps, batch_size=batch_size, shape=shape, cond=cond,\n                                     eta=eta, callback=callback,\n                                     normals_sequence=normals_sequence,\n                                     mask=mask, x0=x_t,\n                                     quantize_x0=quantize_x0,\n                                     temperature=temperature,\n                                     score_corrector=score_corrector,\n                                     corrector_kwargs=corrector_kwargs,\n                                     x_t=x_t\n                                     )\n    return samples, values\n\n", "CodeBleu_score": 50.48359023408051}]}
{"instance_id": "9bdcd9081480", "generation_result": [{"code": "def ema_scope(self, context=None)\n\t\n    if not self.ema_model:\n        yield\n        return\n    if context is None:\n        context = {}\n    if \"ema\" not in context:\n        context[\"ema\"] = True\n        self.switch_ema_model(True)\n        yield\n        self.switch_ema_model(False)\n    else:\n        yield\n\n", "CodeBleu_score": 27.544770080628904}, {"code": "def ema_scope(self, context=None)\n\t\n    if context is None:\n        context = {}\n    if self.ema_config is None:\n        yield\n    else:\n        if context.get(\"no_ema\", False):\n            yield\n        else:\n            if context.get(\"print_ema\", False):\n                print(\"Switching to EMA weights\")\n            self.model.switch_to_ema()\n            yield\n            if context.get(\"print_ema\", False):\n                print(\"Restoring original weights\")\n            self.model.switch_to_ema(restore=True)\n\n", "CodeBleu_score": 39.151770021904184}, {"code": "def ema_scope(self, context=None)\n\t\n    if context is None:\n        context = {}\n    if self.use_ema:\n        if context.get(\"verbose\", False):\n            print(\"Using EMA weights\")\n        self.model.load_state_dict(self.ema.state_dict())\n    yield\n    if self.use_ema:\n        if context.get(\"verbose\", False):\n            print(\"Restoring weights\")\n        self.model.load_state_dict(self.ema.state_backup)\n\n", "CodeBleu_score": 40.93884187910325}]}
{"instance_id": "14ab009e1c5c", "generation_result": [{"code": "def init_from_ckpt(self, path, ignore_keys=None)\n\t\n    if os.path.isfile(path):\n        print(f\"=> loading checkpoint '{path}'\")\n        if version.parse(torch.__version__) >= version.parse(\"1.12.0\"):\n            checkpoint = torch.load(path, map_location=\"cpu\")\n        else:\n            checkpoint = torch.load(path, map_location=\"cpu\")\n        if ignore_keys is not None:\n            for key in ignore_keys:\n                if key in checkpoint[\"state_dict\"]:\n                    del checkpoint[\"state_dict\"][key]\n        self.load_state_dict(checkpoint[\"state_dict\"])\n        print(f\"=> loaded checkpoint '{path}' (epoch {checkpoint['epoch']})\")\n    else:\n        print(f\"=> no checkpoint found at '{path}'\")\n\n", "CodeBleu_score": 24.485650022576845}, {"code": "def init_from_ckpt(self, path, ignore_keys=None)\n\t\n    if ignore_keys is None:\n        ignore_keys = []\n    sd = torch.load(path, map_location=\"cpu\")\n    sd = {k: v for k, v in sd.items() if k not in ignore_keys}\n    missing_keys, unexpected_keys = self.load_state_dict(sd, strict=False)\n    if len(missing_keys) > 0:\n        print(f\"Missing keys: {missing_keys}\")\n    if len(unexpected_keys) > 0:\n        print(f\"Unexpected keys: {unexpected_keys}\")\n\n", "CodeBleu_score": 37.476514494824855}, {"code": "def init_from_ckpt(self, path, ignore_keys=None)\n\t\n    state_dict = torch.load(path, map_location=\"cpu\")\n    if ignore_keys is not None:\n        for key in ignore_keys:\n            state_dict.pop(key, None)\n    self.load_state_dict(state_dict)\n\n", "CodeBleu_score": 10.561548837947734}]}
{"instance_id": "754176e27b81", "generation_result": [{"code": "def forward(self, input, return_pred_indices=False)\n\t\n    # Encode input\n    z, indices, _, _, _ = self.encoder(input)\n    # Decode output\n    pred = self.decoder(z, indices)\n    # Calculate difference between input and output\n    diff = (input - pred).abs().mean()\n    if return_pred_indices:\n        return pred, diff, indices\n    else:\n        return pred, diff\n\n", "CodeBleu_score": 26.111324891819866}, {"code": "def forward(self, input, return_pred_indices=False)\n\t\n    if self.training:\n        self.eval()\n        with torch.no_grad():\n            output = self.forward(input, return_pred_indices=return_pred_indices)\n        self.train()\n        return output\n    else:\n        if self.use_ema:\n            with torch.no_grad():\n                with self.ema_scope():\n                    output = self.forward(input, return_pred_indices=return_pred_indices)\n        else:\n            output = self.forward(input, return_pred_indices=return_pred_indices)\n        if return_pred_indices:\n            return output\n        else:\n            return output[0]\n\n", "CodeBleu_score": 17.924929630359415}, {"code": "def forward(self, input, return_pred_indices=False)\n\t\n    with torch.no_grad():\n        z, _, indices = self.encode(input)\n        pred = self.decode(z, indices)\n        if return_pred_indices:\n            return pred, indices\n        else:\n            return pred\n\n", "CodeBleu_score": 38.45760876679727}]}
{"instance_id": "443df6fdaf0d", "generation_result": [{"code": "def get_input(self, batch, k)\n\t\n    x = batch[k]\n    if x.ndim == 3:\n        x = x.unsqueeze(0)\n    x = x.permute(0, 3, 1, 2)\n    x = x.to(self.device).contiguous().float()\n    if self.batch_resize:\n        x = F.interpolate(x, scale_factor=self.batch_resize_range[self.batch_resize_step], mode=\"bicubic\")\n    x = x.detach()\n    return x\n\n", "CodeBleu_score": 32.058196002234965}, {"code": "def get_input(self, batch, k)\n\t\n    if self.config.model.input_size is not None:\n        size = self.config.model.input_size\n        if isinstance(size, list):\n            size = size[0]\n        size = size * self.config.model.batch_resize_factor\n        size = int(size * (self.global_step / self.config.model.batch_resize_steps))\n        size = min(size, self.config.model.input_size)\n    else:\n        size = None\n    x = batch[k]\n    if x.dim() == 3:\n        x = x[None]\n    if x.dim() == 4:\n        x = x.permute(0, 3, 1, 2)\n    x = x.to(self.device)\n    if self.config.model.batch_resize_factor!= 1.0 and size is not None:\n        x = F.interpolate(x, size=size, mode=\"bicubic\", align_corners=False)\n    x = x.contiguous().float()\n    x = x.detach()\n    return x\n\n", "CodeBleu_score": 49.41772240505881}, {"code": "def get_input(self, batch, k)\n\t\n    # get input\n    x = batch[k]\n    # check dimensions\n    if x.ndim == 2:\n        x = x[None, None]\n    if x.ndim == 3:\n        x = x[None]\n    assert x.ndim == 4\n    # permute\n    x = x.permute(0, 3, 1, 2)\n    # convert to float\n    x = x.to(torch.float32)\n    # resize\n    if self.config.batch_resize:\n        x = F.interpolate(x, self.config.image_size, mode=\"bicubic\", align_corners=False)\n    # detach\n    x = x.detach()\n    return x\n\n", "CodeBleu_score": 34.40890478195158}]}
{"instance_id": "953a2632fc15", "generation_result": [{"code": "def training_step(self, batch, batch_idx, optimizer_idx)\n\t\n    # get input data\n    if isinstance(batch, dict):\n        data = batch[\"data\"]\n    else:\n        data = batch\n    # get predictions\n    if self.model.training_type == \"denoising\":\n        # denoising\n        if self.model.ddim_steps > 0:\n            # ddim\n            pred = self.model.ddim_step(data)\n        else:\n            # ddpm\n            pred = self.model.ddpm_step(data)\n    elif self.model.training_type == \"autoencoding\":\n        # autoencoding\n        pred = self.model.forward(data)\n    else:\n        raise ValueError(f\"Unknown training type: {self.model.training_type}\")\n    # compute losses\n    if optimizer_idx == 0:\n        # autoencoding loss\n        loss = self.model.loss(data, pred)\n    elif optimizer_idx == 1:\n        # discriminator loss\n        loss = self.model.discriminator_loss(data, pred)\n    else:\n        raise ValueError(f\"Unknown optimizer index: {optimizer_idx}\")\n    # log metrics\n    self.log_dict(\n        {\n            \"loss\": loss,\n            \"lr\": self.trainer.optimizers[optimizer_idx].param_groups[0][\"lr\"],\n        },\n        on_step=True,\n        on_epoch=True,\n        prog_bar=True,\n        logger=True,\n    )\n    return loss\n\n", "CodeBleu_score": 32.3247770769315}, {"code": "def training_step(self, batch, batch_idx, optimizer_idx)\n\t\n    if self.global_step % self.cfg.train.gradient_accumulation_steps == 0:\n        self.global_step += 1\n    if self.global_step % self.cfg.train.gradient_accumulation_steps == 0:\n        self.global_step += 1\n    if self.global_step % self.cfg.train.gradient_accumulation_steps == 0:\n        self.global_step += 1\n    if self.global_step % self.cfg.train.gradient_accumulation_steps == 0:\n        self.global_step += 1\n    if self.global_step % self.cfg.train.gradient_accumulation_steps == 0:\n        self.global_step += 1\n    if self.global_step % self.cfg.train.gradient_accumulation_steps == 0:\n        self.global_step += 1\n    if self.global_step % self.cfg.train.gradient_accumulation_steps == 0:\n        self.global_step += 1\n    if self.global_step % self.cfg.train.gradient_accumulation_steps == 0:\n        self.global_step += 1\n    if self.global_step % self.cfg.train.gradient_accumulation_steps == 0:\n        self.global_step += 1\n    if self.global_step % self.cfg.train.gradient_accumulation_steps == 0:\n        self.global_step += 1\n    if self.global_step % self.cfg.train.gradient_accumulation_steps == 0:\n        self.global_step += 1\n    if self.global_step % self.cfg.train.gradient_accumulation_steps == 0:\n        self.global_step += 1\n    if self.global_step % self.cfg.train.gradient_accumulation_steps == 0:\n        self.global_step += 1\n    if self.global_step % self.cfg.train.gradient_accumulation_steps == 0:\n        self.global_step += 1\n    if self.global_step % self.cfg.train.gradient_accumulation_steps == 0:\n        self.global_step += 1\n    if self.global_step % self.cfg.train.gradient_accum", "CodeBleu_score": 12.082836816920944}, {"code": "def training_step(self, batch, batch_idx, optimizer_idx)\n\t\n    if self.model.training_type == \"autoencoder\":\n        x, y = batch\n        x = x.to(self.device)\n        y = y.to(self.device)\n        if self.model.config.model.target_type == \"pixel\":\n            y = y.permute(0, 3, 1, 2)\n        if self.model.config.model.target_type == \"class\":\n            y = y.argmax(dim=1)\n        y = y.to(self.device)\n        loss = self.model(x, y)\n        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n        return loss\n    elif self.model.training_type == \"discriminator\":\n        x, y = batch\n        x = x.to(self.device)\n        y = y.to(self.device)\n        if self.model.config.model.target_type == \"pixel\":\n            y = y.permute(0, 3, 1, 2)\n        if self.model.config.model.target_type == \"class\":\n            y = y.argmax(dim=1)\n        y = y.to(self.device)\n        if optimizer_idx == 0:\n            loss = self.model.d(x, y)\n            self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n            return loss\n        elif optimizer_idx == 1:\n            loss = self.model.g(x, y)\n            self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n            return loss\n\n", "CodeBleu_score": 37.18406485399805}]}
{"instance_id": "b449b5e8b77f", "generation_result": [{"code": "def log_images(self, batch, only_inputs=False, plot_ema=False, **kwargs)\n\t\n    if plot_ema:\n        self.ema_model.eval()\n        with torch.no_grad():\n            if self.ema_model.ema_exists:\n                ema_batch = self.ema_model.apply_ema(batch)\n                ema_batch = self.ema_model.unnormalize(ema_batch)\n                if self.ema_model.ema_exists:\n                    self.logger.experiment.log(\n                        {\n                            \"ema_images\": self.ema_model.unnormalize(ema_batch)\n                            if self.ema_model.ema_exists\n                            else ema_batch,\n                        }\n                    )\n    if only_inputs:\n        self.logger.experiment.log({\"inputs\": batch})\n    else:\n        self.model.eval()\n        with torch.no_grad():\n            recon = self.model(batch)\n            if isinstance(recon, tuple):\n                recon = recon[0]\n            recon = self.model.unnormalize(recon)\n            batch = self.model.unnormalize(batch)\n            if self.model.config.model.params.get(\"colorize\", False):\n                batch = batch.permute(0, 2, 3, 1)\n                recon = recon.permute(0, 2, 3, 1)\n            self.logger.experiment.log(\n                {\"inputs\": batch, \"reconstructions\": recon}\n            )\n\n", "CodeBleu_score": 30.652020715631284}, {"code": "def log_images(self, batch, only_inputs=False, plot_ema=False, **kwargs)\n\t\n    # process input data\n    if isinstance(batch, dict):\n        x = batch['x']\n    else:\n        x = batch\n\n    # log inputs\n    if not only_inputs:\n        if isinstance(x, dict):\n            x = x['x']\n        x = self.module.uncond_conditioning(x)\n        x = self.module.vae.decode(x).cpu().float()\n        x = self.module.vae.post_quant_conv(x)\n        x = (x + 1) * 0.5\n        x = x.clamp(0, 1)\n        x = x.detach().cpu().numpy()\n        if x.shape[1] == 3:\n            x = np.transpose(x, (0, 2, 3, 1))\n        else:\n            x = np.transpose(x, (0, 2, 3, 1))\n            x = np.stack([colorize(img) for img in x], axis=0)\n        self.logger.experiment.add_image('inputs', x, self.global_step, dataformats='NHWC')\n\n    # log reconstructions\n    if not only_inputs:\n        with torch.no_grad():\n            x_recon = self.module(x)\n        x_recon = self.module.vae.decode(x_recon).cpu().float()\n        x_recon = self.module.vae.post_quant_conv(x_recon)\n        x_recon = (x_recon + 1) * 0.5\n        x_recon = x_recon.clamp(0, 1)\n        x_recon = x_recon.detach().cpu().numpy()\n        if x_recon.shape[1] == 3:\n            x_recon = np.transpose(x_recon, (0, 2, 3, 1))\n        else:\n            x_recon = np.transpose(x_recon, (0, 2, 3, 1))\n            x_recon = np.stack([colorize(img) for img in x_recon], axis=0)\n        self.logger.experiment.add_image('reconstructions', x_recon, self.global_step, dataformats='NHWC')\n\n    # log EMA reconstructions\n    if not only_inputs and plot_ema:", "CodeBleu_score": 32.555923863543754}, {"code": "def log_images(self, batch, only_inputs=False, plot_ema=False, **kwargs)\n\t\n    if self.global_step % self.log_every_n_steps == 0:\n        if self.ema_model is not None and plot_ema:\n            ema_model = self.ema_model\n        else:\n            ema_model = self.model\n        if self.global_step == 0:\n            self.logger.log_hyperparams(self.hparams)\n        if self.ema_model is not None and plot_ema:\n            ema_model = self.ema_model\n        else:\n            ema_model = self.model\n        if self.global_step == 0:\n            self.logger.log_hyperparams(self.hparams)\n        if only_inputs:\n            self.logger.experiment.add_images(\n                f\"inputs/{self.global_step}\",\n                self.log_images_inputs(batch),\n                self.global_step,\n            )\n        else:\n            self.logger.experiment.add_images(\n                f\"inputs/{self.global_step}\",\n                self.log_images_inputs(batch),\n                self.global_step,\n            )\n            self.logger.experiment.add_images(\n                f\"reconstructions/{self.global_step}\",\n                self.log_images_reconstructions(batch, ema_model),\n                self.global_step,\n            )\n            if self.ema_model is not None and plot_ema:\n                self.logger.experiment.add_images(\n                    f\"ema_reconstructions/{self.global_step}\",\n                    self.log_images_reconstructions(batch, self.ema_model),\n                    self.global_step,\n                )\n\n", "CodeBleu_score": 21.19634262288448}]}
{"instance_id": "540d089e3481", "generation_result": [{"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss_type == \"l1\":\n        loss = F.l1_loss(pred, target)\n        if mean:\n            loss = loss.mean()\n    elif self.loss_type == \"l2\":\n        loss = F.mse_loss(pred, target)\n        if mean:\n            loss = loss.mean()\n    else:\n        raise ValueError(f\"Unknown loss type {self.loss_type}\")\n    return loss\n\n", "CodeBleu_score": 47.861466236219954}, {"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss_type == 'l1':\n        loss = torch.abs(pred - target)\n        if mean:\n            loss = loss.mean()\n    elif self.loss_type == 'l2':\n        loss = (pred - target) ** 2\n        if mean:\n            loss = loss.mean()\n    else:\n        raise NotImplementedError(f'Loss type {self.loss_type} not implemented')\n    return loss\n\n", "CodeBleu_score": 48.932712702805745}, {"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss == 'l1':\n        if mean:\n            return F.l1_loss(pred, target, reduction='mean')\n        else:\n            return F.l1_loss(pred, target, reduction='none')\n    elif self.loss == 'l2':\n        if mean:\n            return F.mse_loss(pred, target, reduction='mean')\n        else:\n            return F.mse_loss(pred, target, reduction='none')\n    else:\n        raise ValueError(f'Unknown loss type: {self.loss}')\n\n", "CodeBleu_score": 33.83995434904333}]}
{"instance_id": "63a08283666f", "generation_result": [{"code": "def forward(self, x, c, *args, **kwargs)\n\t\n    if self.timestep_embedding is not None:\n        t = extract_into_tensor(x, self.timestep_embedding)\n        t = repeat(t, 'b -> b c', c=self.in_channels)\n        x = torch.cat((x, t), dim=1)\n    if self.conditioning_embedding is not None:\n        c = self.conditioning_embedding(c)\n        c = repeat(c, 'b -> b c', c=self.in_channels)\n        x = torch.cat((x, c), dim=1)\n    return self.model(x, *args, **kwargs)\n\n", "CodeBleu_score": 35.88684796158341}, {"code": "def forward(self, x, c, *args, **kwargs)\n\t\n    t = extract_into_tensor(default(x, c), self.timesteps)\n    c_enc = None\n    if exists(c):\n        assert exists(self.cond_stage_model)\n        assert exists(self.cond_stage_model.enc), 'conditioning model must have an `enc` attribute'\n        c_enc = self.cond_stage_model.enc(c)\n    if self.has_spatial:\n        assert exists(self.cond_stage_model)\n        assert exists(self.cond_stage_model.spatial_model), 'conditioning model must have a `spatial_model` attribute'\n        c_enc = self.cond_stage_model.spatial_model(c_enc, t)\n    if exists(c_enc):\n        x = self.cond_stage_model.combine_x_and_enc(x, c_enc)\n    return self.model(x, t, *args, **kwargs)\n\n", "CodeBleu_score": 29.513112024937122}, {"code": "def forward(self, x, c, *args, **kwargs)\n\t\n    t = default(kwargs.pop(\"t\"), None)\n    if t is None:\n        t = self.get_timestep(self.num_timesteps)\n    c = default(c, None)\n    if c is not None:\n        if self.conditioning == \"latent\":\n            c = self.encode_first_stage(c)\n        elif self.conditioning == \"class\":\n            c = self.class_cond_embedding(c)\n        elif self.conditioning == \"text\":\n            c = self.text_encoder(c)\n    if self.is_first_stage:\n        x = self.encode_first_stage(x)\n    x = self.forward_diffusion(x, t, c, *args, **kwargs)\n    return x\n\n", "CodeBleu_score": 37.312085921100504}]}
{"instance_id": "4627e1e8ed62", "generation_result": [{"code": "def get_codebook_entry(self, indices, shape)\n\t\n    # Retrieve the quantized latent vectors from the codebook based on the given indices\n    quantized_vectors = self.codebook.quantized_vectors[indices]\n\n    # Reshape the quantized latent vectors to match the specified shape\n    quantized_vectors = rearrange(quantized_vectors, \"b (h w) d -> b d h w\", h=shape[2], w=shape[3])\n\n    # Permute the dimensions of the quantized latent vectors to match the original input dimensions\n    quantized_vectors = quantized_vectors.permute(0, 2, 3, 1)\n\n    return quantized_vectors\n\n", "CodeBleu_score": 14.166067605961485}, {"code": "def get_codebook_entry(self, indices, shape)\n\t\n    indices = indices.view(-1)\n    indices = indices + self.offset\n    if self.remapping is not None:\n        indices = self.remapping[indices]\n    latent = self.codebook[indices]\n    latent = latent.view(shape)\n    latent = latent.permute(0, 3, 1, 2)\n    return latent\n\n", "CodeBleu_score": 40.25962733629497}, {"code": "def get_codebook_entry(self, indices, shape)\n\t\n    indices = indices.view(-1)\n    if self.codebook.indices_map is not None:\n        indices = self.codebook.indices_map[indices]\n    quantized = self.codebook.quantize(indices)\n    quantized = quantized.view(shape)\n    return quantized\n\n", "CodeBleu_score": 33.18983570877592}]}
{"instance_id": "c55d014827ef", "generation_result": [{"code": "def factorization(dimension: int, factor:int=-1) -> tuple[int, int]\n\t\n    if factor > 0 and dimension % factor == 0:\n        return (factor, dimension // factor)\n    for i in range(2, dimension // 2 + 1):\n        if dimension % i == 0:\n            if i > factor:\n                break\n            return (i, dimension // i)\n    return (dimension, 1)\n\n", "CodeBleu_score": 25.1149339703344}, {"code": "def factorization(dimension: int, factor:int=-1) -> tuple[int, int]\n\t\n    if factor > 0 and dimension % factor == 0:\n        return factor, dimension // factor\n    else:\n        m = int(np.sqrt(dimension))\n        while m > 0:\n            if dimension % m == 0:\n                n = dimension // m\n                if n >= m:\n                    return m, n\n            m -= 1\n        return 1, dimension\n\n", "CodeBleu_score": 34.09987485836037}, {"code": "def factorization(dimension: int, factor:int=-1) -> tuple[int, int]\n\t\n    if factor > 0 and dimension % factor == 0:\n        return (factor, dimension // factor)\n    elif factor < 0:\n        factor = -factor\n        for i in range(min(dimension, factor), 0, -1):\n            if dimension % i == 0:\n                return (i, dimension // i)\n    else:\n        for i in range(1, dimension + 1):\n            if dimension % i == 0:\n                return (i, dimension // i)\n\n", "CodeBleu_score": 27.353626685256334}]}
{"instance_id": "5d613ec26036", "generation_result": [{"code": "def apply_weight_decompose(self, updown, orig_weight)\n\t\n    if self.weight is None:\n        self.weight = orig_weight.detach().clone()\n    self.weight.data = self.weight.data.to(device=orig_weight.device, dtype=orig_weight.dtype)\n    self.weight.data = self.weight.data * (1 - updown) + orig_weight.data * updown\n    self.weight.data = self.weight.data / self.weight.data.sum()\n    self.weight.data = self.weight.data * self.scale\n    self.weight.data = self.weight.data - orig_weight.data\n\n", "CodeBleu_score": 21.699454452219182}, {"code": "def apply_weight_decompose(self, updown, orig_weight)\n\t\n    weight = updown.to(orig_weight.device).type(orig_weight.dtype)\n    weight = weight + orig_weight\n    weight = weight / weight.norm()\n    weight = weight * self.scale_factor\n    return weight - orig_weight\n\n", "CodeBleu_score": 11.592422816776834}, {"code": "def apply_weight_decompose(self, updown, orig_weight)\n\t\n    if updown == \"up\":\n        weight = (orig_weight * self.weight_scale) + self.weight_decompose_up_scale * self.weight_decompose_up_weight\n    else:\n        weight = (orig_weight * self.weight_scale) + self.weight_decompose_down_scale * self.weight_decompose_down_weight\n    weight = weight / (torch.sum(weight) + self.eps)\n    weight = weight * self.weight_decompose_scale\n    return weight\n\n", "CodeBleu_score": 17.413762212296717}]}
{"instance_id": "c73d0d1cd3e5", "generation_result": [{"code": "def finalize_updown(self, updown, orig_weight, output_shape, ex_bias=None)\n\t\n    if ex_bias is not None:\n        updown = updown + ex_bias\n    updown = updown.reshape(output_shape)\n    if orig_weight.size == updown.size:\n        updown = updown.reshape(orig_weight.shape)\n    if ex_bias is not None:\n        ex_bias = ex_bias * self.scaling_factor\n    if self.scaling_factor is not None:\n        updown = self.decompose(updown)\n    return updown, ex_bias\n\n", "CodeBleu_score": 42.22279740146625}, {"code": "def finalize_updown(self, updown, orig_weight, output_shape, ex_bias=None)\n\t\n    if ex_bias is not None:\n        updown = updown + ex_bias\n    updown = updown.reshape(output_shape)\n    if updown.numel() == orig_weight.numel():\n        updown = updown.reshape(orig_weight.shape)\n    if ex_bias is not None:\n        ex_bias = ex_bias * self.multiplier\n    if self.scaling_factor is not None:\n        updown = self.decompose(updown)\n    return updown, ex_bias\n\n", "CodeBleu_score": 46.26786024048084}, {"code": "def finalize_updown(self, updown, orig_weight, output_shape, ex_bias=None)\n\t\n    updown = updown + ex_bias\n    updown = updown.reshape(output_shape)\n    if updown.nelement() == orig_weight.nelement():\n        updown = updown.reshape(orig_weight.shape)\n    if ex_bias is not None:\n        ex_bias = ex_bias * self.multiplier\n    if self.scaling_factor is not None:\n        updown = sd_models.WeightDecomposition.apply(updown, self.scaling_factor)\n    return updown, ex_bias\n\n", "CodeBleu_score": 45.370152800063416}]}
{"instance_id": "c6a41665c62c", "generation_result": [{"code": "def calc_updown(self, orig_weight)\n\t\n    # transfer the current weight tensor and optional bias to the device of the original weight tensor\n    if self.bias is not None:\n        bias = self.bias.to(orig_weight.device)\n    else:\n        bias = None\n\n    # calculate the updated weight tensor\n    updated_weight = self.calc_updown_func(orig_weight, self.weight, bias)\n\n    return updated_weight\n\n", "CodeBleu_score": 43.55989589082151}, {"code": "def calc_updown(self, orig_weight)\n\t\n    if self.current_weight is None:\n        return orig_weight\n    if self.current_weight.device!= orig_weight.device:\n        self.current_weight = self.current_weight.to(orig_weight.device)\n        if self.current_bias is not None:\n            self.current_bias = self.current_bias.to(orig_weight.device)\n    return self._calc_updown(orig_weight)\n\n", "CodeBleu_score": 39.44490735633557}, {"code": "def calc_updown(self, orig_weight)\n\t\n    current_weight = self.weight.data\n    if self.bias is not None:\n        current_bias = self.bias.data\n    else:\n        current_bias = None\n    if current_weight.device!= orig_weight.device:\n        current_weight = current_weight.to(orig_weight.device)\n        if current_bias is not None:\n            current_bias = current_bias.to(orig_weight.device)\n    return self._calc_updown(orig_weight, current_weight, current_bias)\n\n", "CodeBleu_score": 41.686118339601485}]}
{"instance_id": "1bb9e5d94af8", "generation_result": [{"code": "def purge_networks_from_memory()\n\t\n    if len(network.networks) > network.network_limit:\n        logger.debug(f\"Purging networks from memory. Current network count: {len(network.networks)}\")\n        network.networks = {}\n        logger.debug(\"Purging complete. Garbage collection starting...\")\n        gc.collect()\n        logger.debug(\"Garbage collection complete.\")\n\n", "CodeBleu_score": 19.212251137274748}, {"code": "def purge_networks_from_memory()\n\t\n    if len(network.networks) > network.network_limit:\n        network.networks = network.networks[-network.network_limit:]\n        gc.collect()\n\n", "CodeBleu_score": 22.362015885180465}, {"code": "def purge_networks_from_memory()\n\t\n    if len(network.networks) > network.network_limit:\n        logger.info(\"Purging networks from memory...\")\n        network.networks = {}\n        gc.collect()\n\n", "CodeBleu_score": 15.48738852496221}]}
{"instance_id": "42b48ebf4d07", "generation_result": [{"code": "def network_restore_weights_from_backup(self: Union[torch.nn.Conv2d, torch.nn.Linear, torch.nn.GroupNorm, torch.nn.LayerNorm, torch.nn.MultiheadAttention]):\n\t\n    if hasattr(self, \"weight_backup\"):\n        self.weight.data.copy_(self.weight_backup)\n    if hasattr(self, \"bias_backup\"):\n        if self.bias is not None:\n            self.bias.data.copy_(self.bias_backup)\n        else:\n            self.bias = None\n\n", "CodeBleu_score": 22.110360966589344}, {"code": "def network_restore_weights_from_backup(self: Union[torch.nn.Conv2d, torch.nn.Linear, torch.nn.GroupNorm, torch.nn.LayerNorm, torch.nn.MultiheadAttention]):\n\t\n    if self.weight is not None and hasattr(self, \"weight_backup\") and self.weight_backup is not None:\n        self.weight.data.copy_(self.weight_backup)\n        self.weight_backup = None\n    if self.bias is not None and hasattr(self, \"bias_backup\") and self.bias_backup is not None:\n        self.bias.data.copy_(self.bias_backup)\n        self.bias_backup = None\n    if isinstance(self, torch.nn.MultiheadAttention):\n        for i in range(self.num_heads):\n            if hasattr(self, f\"in_proj_weight_backup_{i}\") and self.in_proj_weight_backup[i] is not None:\n                self.in_proj_weight[i].data.copy_(self.in_proj_weight_backup[i])\n                self.in_proj_weight_backup[i] = None\n            if hasattr(self, f\"in_proj_bias_backup_{i}\") and self.in_proj_bias_backup[i] is not None:\n                self.in_proj_bias[i].data.copy_(self.in_proj_bias_backup[i])\n                self.in_proj_bias_backup[i] = None\n\n", "CodeBleu_score": 40.67652883267718}, {"code": "def network_restore_weights_from_backup(self: Union[torch.nn.Conv2d, torch.nn.Linear, torch.nn.GroupNorm, torch.nn.LayerNorm, torch.nn.MultiheadAttention]):\n\t\n    if hasattr(self, \"weight_backup\") and hasattr(self, \"bias_backup\"):\n        if self.weight_backup is not None:\n            self.weight.data = self.weight_backup\n        if self.bias_backup is not None:\n            self.bias.data = self.bias_backup\n    elif isinstance(self, torch.nn.MultiheadAttention):\n        if hasattr(self, \"in_proj_weight_backup\"):\n            self.in_proj_weight.data = self.in_proj_weight_backup\n        if hasattr(self, \"in_proj_bias_backup\"):\n            self.in_proj_bias.data = self.in_proj_bias_backup\n\n", "CodeBleu_score": 33.06021390930491}]}
{"instance_id": "c8fcd357131a", "generation_result": [{"code": "def network_forward(org_module, input, original_forward)\n\t\n    if not hasattr(org_module, 'networks'):\n        return original_forward(input)\n    device = input.device\n    input = input.to(devices.device)\n    org_module.load_state_dict(org_module.backup_state)\n    org_module.reset_parameters()\n    output = original_forward(input)\n    for network in org_module.networks:\n        if network.match_layer(org_module):\n            output = network.forward(output)\n    return output.to(device)\n\n", "CodeBleu_score": 31.67011368059508}, {"code": "def network_forward(org_module, input, original_forward)\n\t\n    if not hasattr(org_module, 'networks'):\n        return original_forward(input)\n    input = input.to(devices.get_device())\n    org_module.load_state_dict(org_module.networks.get('backup_state', {}))\n    org_module.networks.pop('backup_state', None)\n    output = original_forward(input)\n    for network in org_module.networks.get('networks', []):\n        if network.get('layer_name') == org_module.networks.get('layer_name'):\n            output = network.get('network')(output)\n    return output\n\n", "CodeBleu_score": 34.914965001445644}, {"code": "def network_forward(org_module, input, original_forward)\n\t\n    if not shared.networks:\n        return original_forward(input)\n    input = input.to(shared.device)\n    with torch.no_grad():\n        org_module.apply(shared.restore_weights)\n        org_module.apply(shared.reset_parameters)\n        output = original_forward(input)\n        for network in shared.networks:\n            if network.match(org_module):\n                output = network.forward(output)\n    return output\n\n", "CodeBleu_score": 32.511313556057864}]}
{"instance_id": "1070211f5c11", "generation_result": [{"code": "def trainables(self)\n\t\n    return [p for p in self.parameters() if p.requires_grad]\n\n", "CodeBleu_score": 6.452447410697191}, {"code": "def trainables(self)\n\t\n    trainables = []\n    for name, param in self.named_parameters():\n        if param.requires_grad:\n            trainables.append(param)\n    return trainables\n\n", "CodeBleu_score": 25.147289601791904}, {"code": "def trainables(self)\n\t\n    trainables = []\n    for name, param in self.named_parameters():\n        if param.requires_grad:\n            trainables.append(param)\n    return trainables\n\n", "CodeBleu_score": 25.147289601791904}]}
{"instance_id": "0c3ba62c96a7", "generation_result": [{"code": "def load_hypernetworks(names, multipliers=None)\n\t\n    loaded_hypernetworks = []\n    for name in names:\n        if name in shared.hypernetworks:\n            loaded_hypernetworks.append(shared.hypernetworks[name])\n        else:\n            loaded_hypernetworks.append(sd_models.load_model(name))\n            if multipliers is not None:\n                loaded_hypernetworks[-1].multiplier = multipliers[names.index(name)]\n            shared.hypernetworks[name] = loaded_hypernetworks[-1]\n    return loaded_hypernetworks\n\n", "CodeBleu_score": 30.831672539944037}, {"code": "def load_hypernetworks(names, multipliers=None)\n\t\n    if not hasattr(shared, \"hypernetworks\"):\n        shared.hypernetworks = []\n    shared.hypernetworks = []\n    for name in names:\n        if name in shared.hypernetworks:\n            hypernetwork = shared.hypernetworks[name]\n        else:\n            hypernetwork = sd_models.get_hypernetwork(name)\n            shared.hypernetworks[name] = hypernetwork\n        if multipliers is not None:\n            hypernetwork.multiplier = multipliers[name]\n        shared.hypernetworks.append(hypernetwork)\n\n", "CodeBleu_score": 28.88429516492793}, {"code": "def load_hypernetworks(names, multipliers=None)\n\t\n    if multipliers is None:\n        multipliers = [1.0] * len(names)\n    shared.hypernetworks.clear()\n    for name, multiplier in zip(names, multipliers):\n        if name in shared.hypernetworks:\n            hypernetwork = shared.hypernetworks[name]\n        else:\n            hypernetwork = sd_models.load_hypernetwork(name)\n            shared.hypernetworks[name] = hypernetwork\n        hypernetwork.multiplier = multiplier\n        shared.hypernetworks.append(hypernetwork)\n\n", "CodeBleu_score": 32.06021079167248}]}
{"instance_id": "bea3772ed532", "generation_result": [{"code": "\ndef apply_hypernetworks(hypernetworks, context, layer=None)\n\t\n    context_k = context\n    context_v = context\n    for hypernetwork in hypernetworks:\n        context_k, context_v = hypernetwork(context_k, context_v, layer)\n    return context_k, context_v\n\n", "CodeBleu_score": 60.36450309814241}, {"code": "\ndef apply_hypernetworks(hypernetworks, context, layer=None)\n\t\n    context_k, context_v = context, context\n    for hypernetwork in hypernetworks:\n        context_k, context_v = hypernetwork(context_k, context_v, layer)\n    return context_k, context_v\n\n", "CodeBleu_score": 52.70516813513003}, {"code": "\ndef apply_hypernetworks(hypernetworks, context, layer=None)\n\t\n    context_k, context_v = context, context\n    for hypernetwork in hypernetworks:\n        context_k, context_v = apply_hypernetwork(hypernetwork, context_k, context_v, layer=layer)\n    return context_k, context_v\n", "CodeBleu_score": 51.02529865145008}]}
{"instance_id": "922efa3e092f", "generation_result": [{"code": "def attention_CrossAttention_forward(self, x, context=None, mask=None, **kwargs)\n\t\n    if context is None:\n        return self.self_attention(x, mask=mask, **kwargs)\n    else:\n        if self.hypernet:\n            context = self.hypernet(context)\n        q, k, v = self.in_proj_q(x), self.in_proj_k(context), self.in_proj_v(context)\n        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=self.num_heads), (q, k, v))\n        sim = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n        if mask is not None:\n            mask = rearrange(mask, 'b i -> b () i ()')\n            sim.masked_fill_(~mask, -torch.finfo(sim.dtype).max)\n        attn = sim.softmax(dim=-1)\n        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        out = self.out_proj(out)\n        return out\n\n", "CodeBleu_score": 56.444032825811405}, {"code": "def attention_CrossAttention_forward(self, x, context=None, mask=None, **kwargs)\n\t\n    if context is None:\n        context = x\n    if self.in_proj_weight is not None:\n        q, k, v = x.matmul(self.in_proj_weight).chunk(3, dim=-1)\n    elif self.q_proj is not None:\n        q = self.q_proj(x)\n        k = self.k_proj(context)\n        v = self.v_proj(context)\n    else:\n        q = self.q_proj(x)\n        k = self.k_proj(x)\n        v = self.v_proj(x)\n    q = q * self.scaling\n    if self.bias_k is not None:\n        assert self.bias_v is not None\n        k = torch.cat([k, self.bias_k.repeat(1, bsz, 1)])\n        v = torch.cat([v, self.bias_v.repeat(1, bsz, 1)])\n        if attn_mask is not None:\n            attn_mask = torch.cat([attn_mask, attn_mask.new_zeros(attn_mask.size(0), 1)], dim=1)\n        if key_padding_mask is not None:\n            key_padding_mask = torch.cat(\n                [key_padding_mask, key_padding_mask.new_zeros(key_padding_mask.size(0), 1)], dim=1\n            )\n    q = q.contiguous().view(tgt_len, bsz * self.num_heads, self.head_dim).transpose(0, 1)\n    if k is not None:\n        k = k.contiguous().view(-1, bsz * self.num_heads, self.head_dim).transpose(0, 1)\n    if v is not None:\n        v = v.contiguous().view(-1, bsz * self.num_heads, self.head_dim).transpose(0, 1)\n    if saved_state is not None:\n        if \"prev_key\" in saved_state:\n            prev_key = saved_state[\"prev_key\"].view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n            if static_k is not None:\n                k = torch.cat((prev_key, static", "CodeBleu_score": 33.39812904277245}, {"code": "def attention_CrossAttention_forward(self, x, context=None, mask=None, **kwargs)\n\t\n    # compute query, key, and value projections from the input and context\n    query, key, value = self.compute_qkv(x, context)\n    # apply hypernetworks to the context\n    if self.hypernetwork is not None:\n        context = self.hypernetwork(context)\n    # reshape the tensors for multi-head attention\n    query = query.reshape(x.shape[0], x.shape[1], self.num_heads, self.head_dim)\n    key = key.reshape(x.shape[0], x.shape[1], self.num_heads, self.head_dim)\n    value = value.reshape(x.shape[0], x.shape[1], self.num_heads, self.head_dim)\n    # calculate scaled dot-product attention\n    attn = einsum(\"b q h d, b c h d -> b q c\", query, key) * self.scale\n    # optionally apply a mask to the attention scores\n    if mask is not None:\n        attn = attn.masked_fill(mask, -torch.finfo(attn.dtype).max)\n    attn = attn.softmax(dim=-1)\n    # compute the final attention output\n    out = einsum(\"b q c, b c h d -> b q h d\", attn, value)\n    # reshape back and pass through a final linear layer\n    out = out.reshape(x.shape[0], x.shape[1], self.out_dim)\n    out = self.proj(out)\n    return out\n\n", "CodeBleu_score": 41.29463182834148}]}
{"instance_id": "8b7455a03a0e", "generation_result": [{"code": "def save_hypernetwork(hypernetwork, checkpoint, hypernetwork_name, filename)\n\t\n    # Backup original attributes\n    original_hypernetwork_name = hypernetwork.name\n    original_hypernetwork_checkpoint = hypernetwork.checkpoint\n\n    # Update attributes\n    hypernetwork.name = hypernetwork_name\n    hypernetwork.checkpoint = checkpoint\n\n    # Attempt to save\n    try:\n        hypernetwork.save(filename)\n    except Exception as e:\n        # Restore original attributes\n        hypernetwork.name = original_hypernetwork_name\n        hypernetwork.checkpoint = original_hypernetwork_checkpoint\n        raise e\n\n    # Restore original attributes\n    hypernetwork.name = original_hypernetwork_name\n    hypernetwork.checkpoint = original_hypernetwork_checkpoint\n\n", "CodeBleu_score": 34.822429491043536}, {"code": "def save_hypernetwork(hypernetwork, checkpoint, hypernetwork_name, filename)\n\t\n    # backup original attributes\n    original_attributes = {}\n    for attribute in ['checkpoint', 'hypernetwork_name']:\n        original_attributes[attribute] = getattr(hypernetwork, attribute)\n    # update attributes\n    setattr(hypernetwork, 'checkpoint', checkpoint)\n    setattr(hypernetwork, 'hypernetwork_name', hypernetwork_name)\n    # save hypernetwork\n    try:\n        hypernetwork.save(filename)\n    except:\n        # restore original attributes\n        for attribute in ['checkpoint', 'hypernetwork_name']:\n            setattr(hypernetwork, attribute, original_attributes[attribute])\n        raise\n\n", "CodeBleu_score": 27.323659753144174}, {"code": "def save_hypernetwork(hypernetwork, checkpoint, hypernetwork_name, filename)\n\t\n    # Save the hypernetwork's original attributes\n    original_attributes = {}\n    for attribute_name in ['checkpoint_path', 'checkpoint_name', 'checkpoint_type', 'checkpoint_url', 'checkpoint_subfolder', 'checkpoint_filename']:\n        original_attributes[attribute_name] = getattr(hypernetwork, attribute_name)\n\n    # Update the hypernetwork's attributes with the new checkpoint information\n    hypernetwork.checkpoint_path = checkpoint['path']\n    hypernetwork.checkpoint_name = checkpoint['name']\n    hypernetwork.checkpoint_type = checkpoint['type']\n    hypernetwork.checkpoint_url = checkpoint['url']\n    hypernetwork.checkpoint_subfolder = checkpoint['subfolder']\n    hypernetwork.checkpoint_filename = checkpoint['filename']\n\n    # Attempt to save the hypernetwork\n    try:\n        hypernetwork.save(filename)\n    except Exception as e:\n        # Restore the hypernetwork's original attributes\n        for attribute_name, original_attribute_value in original_attributes.items():\n            setattr(hypernetwork, attribute_name, original_attribute_value)\n\n        # Re-raise the exception\n        raise e\n", "CodeBleu_score": 31.430290992887766}]}
{"instance_id": "3175f835140f", "generation_result": [{"code": "def ema_scope(self, context=None)\n\t\n    if self.ema_model is not None:\n        self.model.store_orig_params()\n        self.model.load_state_dict(self.ema_model.state_dict())\n        if context is not None:\n            context.log_dict({'ema_scope': True})\n    yield\n    if self.ema_model is not None:\n        self.model.load_state_dict(self.model.orig_params)\n        if context is not None:\n            context.log_dict({'ema_scope': False})\n\n", "CodeBleu_score": 57.463922639106}, {"code": "def ema_scope(self, context=None)\n\t\n    if self.ema_model is None:\n        return\n    if context is None:\n        context = self\n    if not hasattr(context, \"ema_backup\"):\n        context.ema_backup = self.model.state_dict()\n    self.model.load_state_dict(self.ema_model.state_dict())\n    if self.ema_log:\n        self.logger.experiment.log_metric(\"ema_on\", 1)\n\n", "CodeBleu_score": 39.9447663338653}, {"code": "def ema_scope(self, context=None)\n\t\n    if not self.use_ema:\n        yield\n        return\n\n    if self.log_ema:\n        self.logger.experiment.log_text(\"Switching to EMA weights\")\n\n    self.store_original_weights()\n    self.load_ema_weights()\n    yield\n    self.load_original_weights()\n\n    if self.log_ema:\n        self.logger.experiment.log_text(\"Restored original weights\")\n\n", "CodeBleu_score": 36.2432211406927}]}
{"instance_id": "a7b5d5cd519a", "generation_result": [{"code": "def p_mean_variance(self, x, t, clip_denoised: bool)\n\t\n    model_output = self.model(x, t)\n    if self.options.clip_denoised:\n        model_output = model_output.clamp(-1, 1)\n    if self.options.denoised_fn == 'v':\n        v_pred = model_output\n        m_pred = x - v_pred\n    elif self.options.denoised_fn == 'eps':\n        m_pred = model_output\n        v_pred = x - m_pred\n    else:\n        raise NotImplementedError(f'Denoised function {self.options.denoised_fn} not implemented')\n    m_pred = m_pred.detach()\n    v_pred = v_pred.detach()\n    return m_pred, v_pred, v_pred.log()\n\n", "CodeBleu_score": 32.05397422447223}, {"code": "def p_mean_variance(self, x, t, clip_denoised: bool)\n\t\n    model_output = self.model(x, t)\n    if self.config.model.model_channels!= self.config.model.condition_channels:\n        model_output = model_output[:, :self.config.model.condition_channels]\n    if self.config.model.condition_on_x_prev:\n        eps, eps_std = model_output[:, :self.config.model.condition_channels], model_output[:, self.config.model.condition_channels:]\n    else:\n        eps, eps_std = model_output, None\n    if self.config.model.clip_denoised:\n        x_recon = x - self.config.model.denoise_fn(eps, x, t)\n        x_recon = x_recon.clamp(-1, 1)\n    else:\n        x_recon = self.config.model.denoise_fn(eps, x, t)\n    if self.config.model.use_scale_shift_norm:\n        mean, log_variance = x_recon.chunk(2, dim=-1)\n        variance = log_variance.exp()\n    else:\n        mean, variance = x_recon, None\n        log_variance = None\n    return mean, variance, log_variance\n\n", "CodeBleu_score": 32.112490060940566}, {"code": "def p_mean_variance(self, x, t, clip_denoised: bool)\n\t\n    model_output = self.model(x, t)\n    if self.model_channels!= self.channels:\n        model_output = self.conv(model_output)\n    if self.use_scale_shift_norm:\n        model_output = self.norm(model_output)\n    pred_noise = self.pred_noise(model_output)\n    if clip_denoised:\n        pred_x = self.clamp_with_grad(x - pred_noise, -1, 1)\n    else:\n        pred_x = x - pred_noise\n    model_mean, posterior_variance, posterior_log_variance = self.posterior_variance(model_output, t)\n    return model_mean, pred_x, posterior_variance, posterior_log_variance\n\n", "CodeBleu_score": 38.80867050817278}]}
{"instance_id": "a8652fa78473", "generation_result": [{"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss_type == 'l1':\n        loss = torch.abs(pred - target)\n        if mean:\n            loss = loss.mean()\n    elif self.loss_type == 'l2':\n        loss = F.mse_loss(pred, target, reduction='none')\n        if mean:\n            loss = loss.mean()\n    else:\n        raise ValueError(f'Unknown loss type: {self.loss_type}')\n    return loss\n\n", "CodeBleu_score": 55.428274621457675}, {"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss_type == 'l1':\n        loss = F.l1_loss(pred, target, reduction='none')\n        if mean:\n            loss = loss.mean()\n    elif self.loss_type == 'l2':\n        loss = F.mse_loss(pred, target, reduction='none')\n        if not mean:\n            loss = loss.mean()\n    else:\n        raise ValueError(f'loss type {self.loss_type} not supported')\n    return loss\n\n", "CodeBleu_score": 53.98676558236416}, {"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss_type == \"l1\":\n        loss = (pred - target).abs()\n        if mean:\n            loss = loss.mean()\n    elif self.loss_type == \"l2\":\n        loss = F.mse_loss(pred, target, reduction=\"none\")\n        if mean:\n            loss = loss.mean()\n    else:\n        raise ValueError(f\"Unknown loss type {self.loss_type}\")\n    return loss\n\n", "CodeBleu_score": 54.493835007570176}]}
{"instance_id": "d68e4f061349", "generation_result": [{"code": "def _prior_bpd(self, x_start)\n\t\n    b = x_start.shape[0]\n    t = torch.full((b,), self.num_timesteps - 1, device=x_start.device).long()\n    mean, logvar = self.model.predict(x_start, t)\n    kl = normal_kl(mean, logvar)\n    return kl.mean() / math.log(2)\n\n", "CodeBleu_score": 22.801149947285275}, {"code": "def _prior_bpd(self, x_start)\n\t\n    b = x_start.shape[0]\n    t = torch.full((b,), self.num_timesteps - 1, device=x_start.device).long()\n    mean, logvar = self.model.predict(x_start, t)\n    kl = normal_kl(mean, logvar)\n    return (kl / math.log(2)).mean()\n", "CodeBleu_score": 22.82735109099767}, {"code": "def _prior_bpd(self, x_start)\n\t\n    b, *_ = x_start.shape\n    t = torch.full((b,), self.num_timesteps - 1, device=x_start.device).long()\n    mean, logvar = self.predict(x_start, t)\n    kl_div = normal_kl(mean, logvar)\n    return kl_div.mean() / math.log(2)\n\n", "CodeBleu_score": 34.23604748513653}]}
{"instance_id": "c695d93fdf2d", "generation_result": [{"code": "def make_cache(subsection: str) -> diskcache.Cache\n\t\n    return diskcache.Cache(os.path.join(data_path, subsection),\n                           size_limit=4 * 1024 * 1024 * 1024,\n                           disk_min_file_size=256 * 1024)\n\n", "CodeBleu_score": 59.90051961140943}, {"code": "def make_cache(subsection: str) -> diskcache.Cache\n\t\n    cache = diskcache.Cache(data_path / subsection, size_limit=4 * 1024 ** 3, disk_min_file_size=256 * 1024)\n    return cache\n\n", "CodeBleu_score": 47.32423246998787}, {"code": "def make_cache(subsection: str) -> diskcache.Cache\n\t\n    cache = diskcache.Cache(\n        os.path.join(data_path, subsection),\n        size_limit=4 * 1024 * 1024 * 1024,\n        disk_min_file_size=256 * 1024,\n    )\n    return cache\n\n", "CodeBleu_score": 57.65908690433794}]}
{"instance_id": "ed4c543f0d38", "generation_result": [{"code": "def cache(subsection)\n\t\n    global caches\n    global cache_lock\n    if subsection in caches:\n        return caches[subsection]\n    with cache_lock:\n        if not os.path.isdir(data_path / \"cache\"):\n            convert_old_cached_data()\n        if not os.path.isfile(data_path / \"cache\" / f\"{subsection}.db\"):\n            make_cache(subsection)\n        caches[subsection] = diskcache.Cache(data_path / \"cache\" / f\"{subsection}.db\")\n    return caches[subsection]\n\n", "CodeBleu_score": 40.34690477294884}, {"code": "def cache(subsection)\n\t\n    if subsection in caches:\n        return caches[subsection]\n    with cache_lock:\n        if not os.path.exists(cache_dir):\n            os.makedirs(cache_dir)\n        cache_filename = os.path.join(cache_dir, subsection + \".db\")\n        if not os.path.isfile(cache_filename):\n            convert_old_cached_data()\n        if subsection not in caches:\n            caches[subsection] = make_cache(subsection)\n        return caches[subsection]\n\n", "CodeBleu_score": 49.4121792314385}, {"code": "def cache(subsection)\n\t\n    global caches\n    global cache_lock\n    if subsection in caches:\n        return caches[subsection]\n    with cache_lock:\n        if not os.path.exists(cache_path):\n            os.makedirs(cache_path)\n        if not os.path.isfile(cache_path + subsection + \".json\"):\n            convert_old_cached_data(subsection)\n        if subsection not in caches:\n            caches[subsection] = make_cache(subsection)\n        return caches[subsection]\n\n", "CodeBleu_score": 45.58478942138451}]}
{"instance_id": "ccf5ea2d9fc1", "generation_result": [{"code": "def cached_data_for_file(subsection, title, filename, func)\n\t\n    cache = cache(subsection)\n    mtime = os.stat(filename).st_mtime\n    if title in cache and cache[title][\"mtime\"] >= mtime:\n        return cache[title][\"data\"]\n    else:\n        data = func()\n        cache[title] = {\"mtime\": mtime, \"data\": data}\n        dump_cache(subsection, cache)\n        return data\n\n", "CodeBleu_score": 23.550699034957667}, {"code": "def cached_data_for_file(subsection, title, filename, func)\n\t\n    cache = cache(subsection)\n    mtime = os.path.getmtime(filename)\n    if title in cache and cache[title][\"mtime\"] >= mtime:\n        return cache[title][\"data\"]\n    data = func()\n    cache[title] = {\"mtime\": mtime, \"data\": data}\n    dump_cache(cache, subsection)\n    return data\n\n", "CodeBleu_score": 16.483841870572654}, {"code": "def cached_data_for_file(subsection, title, filename, func)\n\t\n    cache = cache(subsection)\n    mtime = os.path.getmtime(filename)\n    if title in cache:\n        if cache[title].get('mtime', 0) >= mtime:\n            return cache[title]['data']\n        else:\n            cache.invalidate(title)\n    data = func()\n    cache[title] = {'mtime': mtime, 'data': data}\n    cache.dump_cache()\n    return data\n\n", "CodeBleu_score": 28.421145739730857}]}
{"instance_id": "e01a2d4f0ac2", "generation_result": [{"code": "def report(message: str, *, exc_info: bool = False) -> None\n\t\n    record_exception()\n    print(f\"*** {message}\", file=sys.stderr)\n    if exc_info:\n        traceback.print_exc(file=sys.stderr)\n        print(\"---\", file=sys.stderr)\n\n", "CodeBleu_score": 40.35401246483239}, {"code": "def report(message: str, *, exc_info: bool = False) -> None\n\t\n    record_exception()\n    print(f\"*** {message}\", file=sys.stderr)\n    if exc_info:\n        traceback.print_exc(file=sys.stderr)\n        print(file=sys.stderr)\n\n", "CodeBleu_score": 35.60080005473617}, {"code": "def report(message: str, *, exc_info: bool = False) -> None\n\t\n    record_exception()\n    print(f\"*** {message}\", file=sys.stderr)\n    if exc_info:\n        traceback.print_exc(file=sys.stderr)\n        print(\"---\", file=sys.stderr)\n\n", "CodeBleu_score": 40.35401246483239}]}
{"instance_id": "125fb9d2320b", "generation_result": [{"code": "def extract_original_prompts(style: PromptStyle, prompt, negative_prompt)\n\t\n    if not style.prompt and not style.negative_prompt:\n        return False, prompt, negative_prompt\n    try:\n        prompt = style.extract_from_prompt(prompt)\n    except errors.PromptStyleError:\n        return False, prompt, negative_prompt\n    try:\n        negative_prompt = style.extract_from_prompt(negative_prompt)\n    except errors.PromptStyleError:\n        return False, prompt, negative_prompt\n    return True, prompt, negative_prompt\n\n", "CodeBleu_score": 56.04465951248363}, {"code": "def extract_original_prompts(style: PromptStyle, prompt, negative_prompt)\n\t\n    if style.prompt is None and style.negative_prompt is None:\n        return False, prompt, negative_prompt\n\n    if style.prompt is not None:\n        try:\n            prompt = style.extract_text(prompt)\n        except errors.StyleExtractionError:\n            return False, prompt, negative_prompt\n\n    if style.negative_prompt is not None:\n        try:\n            negative_prompt = style.extract_text(negative_prompt)\n        except errors.StyleExtractionError:\n            return False, prompt, negative_prompt\n\n    return True, prompt, negative_prompt\n\n", "CodeBleu_score": 53.45772906420186}, {"code": "def extract_original_prompts(style: PromptStyle, prompt, negative_prompt)\n\t\n    if style.prompt == \"\" and style.negative_prompt == \"\":\n        return False, prompt, negative_prompt\n    try:\n        prompt = extract_style_text(style, prompt)\n    except errors.StyleExtractionError:\n        return False, prompt, negative_prompt\n    try:\n        negative_prompt = extract_style_text(style, negative_prompt)\n    except errors.StyleExtractionError:\n        return False, prompt, negative_prompt\n    return True, prompt, negative_prompt\n\n", "CodeBleu_score": 50.38638219855815}]}
{"instance_id": "0c5f729fbce2", "generation_result": [{"code": "def random_partial(self, count, n_frames)\n\t\n    if not self.utterances:\n        self._load_utterances()\n    utterances = self.utterance_cycler.sample(count)\n    partials = [(u, u.random_partial(n_frames)) for u in utterances]\n    return partials\n\n", "CodeBleu_score": 69.53856927584349}, {"code": "def random_partial(self, count, n_frames)\n\t\n    if not self.utterances:\n        self._load_utterances()\n    utterances = self.utterance_cycler.sample(count)\n    return [(utterance, utterance.random_partial(n_frames)) for utterance in utterances]\n\n", "CodeBleu_score": 52.37126547868942}, {"code": "def random_partial(self, count, n_frames)\n\t\n    if not self.utterances:\n        self._load_utterances()\n    utterances = self.utterance_cycler.sample(count)\n    partials = [\n        (u, u.random_partial(n_frames)) for u in utterances\n    ]\n    return partials\n\n", "CodeBleu_score": 61.21674843416171}]}
{"instance_id": "31e8b27b8f94", "generation_result": [{"code": "def normalize_volume(wav, target_dBFS, increase_only=False, decrease_only=False)\n\t\n    if increase_only and decrease_only:\n        raise ValueError(\"increase_only and decrease_only are mutually exclusive\")\n\n    dBFS_change = target_dBFS - librosa.core.amplitude_to_db(np.max(np.abs(wav)))\n\n    if dBFS_change < 0 and increase_only:\n        return wav\n\n    if dBFS_change > 0 and decrease_only:\n        return wav\n\n    return wav * (10 ** (dBFS_change / 20))\n\n", "CodeBleu_score": 63.67019036375623}, {"code": "def normalize_volume(wav, target_dBFS, increase_only=False, decrease_only=False)\n\t\n    if increase_only and decrease_only:\n        raise ValueError(\"increase_only and decrease_only are mutually exclusive\")\n\n    change_in_dBFS = target_dBFS - librosa.core.amplitude_to_db(np.max(np.abs(wav)))\n    if increase_only and change_in_dBFS < 0:\n        return wav\n    if decrease_only and change_in_dBFS > 0:\n        return wav\n\n    return wav * (10 ** (change_in_dBFS / 20))\n\n", "CodeBleu_score": 55.427163911297114}, {"code": "def normalize_volume(wav, target_dBFS, increase_only=False, decrease_only=False)\n\t\n    if increase_only and decrease_only:\n        raise ValueError(\"increase_only and decrease_only cannot both be set to True\")\n\n    dBFS_change = target_dBFS - librosa.core.amplitude_to_db(np.max(np.abs(wav)))\n    if dBFS_change < 0 and increase_only or dBFS_change > 0 and decrease_only:\n        return wav\n\n    return wav * (10 ** (dBFS_change / 20))\n\n", "CodeBleu_score": 63.37558651673817}]}
{"instance_id": "2353831e9033", "generation_result": [{"code": "def load_model(weights_fpath: Path, device=None)\n\t\n    global _model\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() and device is None else device)\n    _model = SpeakerEncoder(encoder_embedding_size, device=device)\n    _model.load_state_dict(torch.load(weights_fpath, map_location=device))\n    _model.eval()\n    print(\"Loaded model from: {}\".format(weights_fpath))\n    print(\"Trained from step: {}\".format(int(weights_fpath.stem.split(\"_\")[-1])))\n\n", "CodeBleu_score": 32.434450874423916}, {"code": "def load_model(weights_fpath: Path, device=None)\n\t\n    global _model\n    if device is None:\n        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    _model = SpeakerEncoder(device=device)\n    _model.load_state_dict(torch.load(weights_fpath, map_location=device))\n    _model.eval()\n    print(\"Loaded model from: {}\".format(weights_fpath))\n    print(\"Trained until step: {}\".format(int(weights_fpath.stem.split(\"_\")[-1])))\n\n", "CodeBleu_score": 43.1745329561775}, {"code": "def load_model(weights_fpath: Path, device=None)\n\t\n    global _model\n\n    if device is None:\n        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n    _model = SpeakerEncoder(device=device)\n    _model.load_state_dict(torch.load(weights_fpath, map_location=device))\n    _model.eval()\n    print(\"Loaded model from {}\".format(weights_fpath))\n    print(\"Trained from step: {}\".format(int(weights_fpath.stem.split(\"_\")[-1])))\n\n", "CodeBleu_score": 43.114666310738926}]}
{"instance_id": "3231ad480199", "generation_result": [{"code": "def embed_frames_batch(frames_batch)\n\t\n    if not _model:\n        raise Exception(\"model not set\")\n    frames_batch = torch.from_numpy(frames_batch).to(_device)\n    embeddings = _model.embed_utterance(frames_batch)\n    embeddings = embeddings.detach().cpu().numpy()\n    return embeddings\n\n", "CodeBleu_score": 54.90098151137804}, {"code": "def embed_frames_batch(frames_batch)\n\t\n    if not _model:\n        raise Exception(\"model not set\")\n    frames_batch = torch.from_numpy(frames_batch).to(_device)\n    embeds = _model.embed_utterance(frames_batch)\n    embeds = embeds.detach().cpu().numpy()\n    return embeds\n\n", "CodeBleu_score": 55.347469388875744}, {"code": "def embed_frames_batch(frames_batch)\n\t\n    if not _model:\n        raise Exception(\"model not set\")\n    # Convert to tensor\n    frames_batch = torch.from_numpy(frames_batch).to(_device)\n    # Forward\n    embeds = _model.embed_utterance(frames_batch)\n    embeds = embeds.detach().cpu().numpy()\n    return embeds\n\n", "CodeBleu_score": 55.347469388875744}]}
{"instance_id": "117a41716c8d", "generation_result": [{"code": "def compute_partial_slices(n_samples, partial_utterance_n_frames=partials_n_frames,\n                           min_pad_coverage=0.75, overlap=0.5)\n\t\n    assert 0 <= overlap <= 1, f'Overlap must be in range [0, 1], got {overlap}'\n    assert 0 <= min_pad_coverage <= 1, f'Min pad coverage must be in range [0, 1], got {min_pad_coverage}'\n\n    n_frames = partial_utterance_n_frames\n    frame_step = int(n_frames * (1 - overlap))\n\n    wav_slices = []\n    mel_slices = []\n    start_frame = 0\n    while start_frame + n_frames <= n_samples:\n        wav_slices.append(start_frame)\n        mel_slices.append(start_frame)\n        start_frame += frame_step\n\n    # Check if we need to pad the last slice\n    if len(wav_slices) > 1:\n        last_slice_coverage = (n_samples - wav_slices[-1]) / n_frames\n        if last_slice_coverage < min_pad_coverage:\n            wav_slices = wav_slices[:-1]\n            mel_slices = mel_slices[:-1]\n\n    return wav_slices, mel_slices\n\n", "CodeBleu_score": 33.615862381367926}, {"code": "def compute_partial_slices(n_samples, partial_utterance_n_frames=partials_n_frames,\n                           min_pad_coverage=0.75, overlap=0.5)\n\t\n    assert 0 < overlap < 1, \"overlap should be in range (0,1)\"\n    assert 0 < min_pad_coverage < 1, \"min_pad_coverage should be in range (0,1)\"\n\n    n_frames = partial_utterance_n_frames\n    frame_step = int(n_frames * (1 - overlap))\n\n    wav_slices = []\n    mel_slices = []\n\n    for i in range(0, n_samples.shape[0] - n_frames + 1, frame_step):\n        wav_slices.append(n_samples[i:i + n_frames])\n        mel_slices.append(audio.melspectrogram(n_samples[i:i + n_frames]).T)\n\n    if len(wav_slices) > 1:\n        coverage = (len(wav_slices) - 1) * frame_step / n_samples.shape[0]\n        if coverage < min_pad_coverage:\n            wav_slices.pop()\n            mel_slices.pop()\n\n    return wav_slices, mel_slices\n\n", "CodeBleu_score": 30.208107275065387}, {"code": "def compute_partial_slices(n_samples, partial_utterance_n_frames=partials_n_frames,\n                           min_pad_coverage=0.75, overlap=0.5)\n\t\n    assert 0 < overlap < 1, 'overlap must be between 0 and 1'\n    assert 0 < min_pad_coverage < 1,'min_pad_coverage must be between 0 and 1'\n    assert partial_utterance_n_frames > 0, 'partial_utterance_n_frames must be positive'\n    n_frames = int(np.ceil(n_samples / sampling_rate * hop_length))\n    frame_step = int(np.ceil(partial_utterance_n_frames * (1 - overlap)))\n    wav_slices = []\n    mel_slices = []\n    for i in range(0, n_frames, frame_step):\n        start = i * hop_length\n        end = start + partial_utterance_n_frames * hop_length\n        wav_slice = wav[start:end].astype(np.float32)\n        wav_slices.append(wav_slice)\n        mel_slice = audio.melspectrogram(wav_slice).astype(np.float32).T\n        mel_slices.append(mel_slice)\n    last_wav_coverage = (len(wav_slices[-1]) / sampling_rate) / (len(wav) / sampling_rate)\n    if last_wav_coverage < min_pad_coverage and len(wav_slices) > 1:\n        wav_slices.pop()\n        mel_slices.pop()\n    return wav_slices, mel_slices\n\n", "CodeBleu_score": 39.28682594618264}]}
{"instance_id": "34fe5c864f1c", "generation_result": [{"code": "def plot_embedding_as_heatmap(embed, ax=None, title=\"\", shape=None, color_range=(0, 0.30))\n\t\n    if ax is None:\n        ax = plt.gca()\n    if shape is None:\n        height = int(np.sqrt(len(embed)))\n        shape = (height, height)\n    else:\n        height = shape[0]\n    embed = embed.reshape(shape)\n    ax.imshow(embed, cmap=cm.get_cmap(\"rainbow\"), vmin=color_range[0], vmax=color_range[1])\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.set_title(title)\n    cbar = plt.colorbar(ax=ax)\n    cbar.set_ticks([color_range[0], color_range[1]])\n    cbar.set_ticklabels([\"Low\", \"High\"])\n\n", "CodeBleu_score": 59.94561563901088}, {"code": "def plot_embedding_as_heatmap(embed, ax=None, title=\"\", shape=None, color_range=(0, 0.30))\n\t\n    if ax is None:\n        ax = plt.gca()\n    if shape is None:\n        height = int(np.sqrt(embed.shape[0]))\n        embed = embed.reshape(height, height)\n    else:\n        embed = embed.reshape(shape)\n    ax.imshow(embed, cmap=cm.jet, vmin=color_range[0], vmax=color_range[1])\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.set_title(title)\n    plt.colorbar()\n\n", "CodeBleu_score": 47.717632468055}, {"code": "def plot_embedding_as_heatmap(embed, ax=None, title=\"\", shape=None, color_range=(0, 0.30))\n\t\n    if ax is None:\n        ax = plt.gca()\n    if shape is None:\n        height = int(np.sqrt(embed.shape[0]))\n        embed = embed.reshape((height, height))\n    ax.imshow(embed, cmap=cm.get_cmap(\"magma\"), vmin=color_range[0], vmax=color_range[1])\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.set_title(title)\n    plt.colorbar(ax=ax)\n\n", "CodeBleu_score": 46.67805873589116}]}
{"instance_id": "92e93d50b62b", "generation_result": [{"code": "def similarity_matrix(self, embeds)\n\t\n    # Compute centroids\n    centroids_incl = embeds.mean(dim=1, keepdim=True)\n    centroids_incl = F.normalize(centroids_incl, p=2, dim=2)\n    centroids_excl = embeds.clone()\n    centroids_excl = centroids_excl.mean(dim=1, keepdim=True)\n    centroids_excl = F.normalize(centroids_excl, p=2, dim=2)\n    # Initialize similarity matrix\n    sim_mat = torch.zeros(embeds.size(0), embeds.size(0), device=embeds.device)\n    mask = torch.eye(embeds.size(0), device=embeds.device)\n    # Compute similarity matrix\n    for i in range(embeds.size(0)):\n        sim_mat[i] = torch.mm(embeds[i].unsqueeze(0), centroids_incl)\n        sim_mat[i] += torch.mm(embeds[i].unsqueeze(0), centroids_excl)\n        sim_mat[i] /= 2\n        sim_mat[i] /= self.scale\n        sim_mat[i] += self.bias\n    # Apply mask to remove self-similarity\n    sim_mat = sim_mat * (1 - mask)\n    return sim_mat\n\n", "CodeBleu_score": 23.39112670301192}, {"code": "def similarity_matrix(self, embeds)\n\t\n    num_speakers = embeds.size(0)\n    num_utterances = embeds.size(1)\n    centroids_incl = embeds.mean(dim=1)\n    centroids_incl = F.normalize(centroids_incl, p=2, dim=1)\n    centroids_excl = torch.zeros(num_speakers, embeds.size(2), device=self.device)\n    for i in range(num_speakers):\n        centroids_excl[i] = F.normalize(embeds[i, :-1].mean(dim=0), p=2, dim=0)\n    sim_matrix = torch.zeros(num_speakers * num_utterances, num_speakers * num_utterances, device=self.device)\n    mask = torch.ones_like(sim_matrix)\n    mask = mask.triu(diagonal=1)\n    for i in range(num_speakers):\n        for j in range(num_utterances):\n            sim_matrix[i * num_utterances + j, i * num_utterances:(i + 1) * num_utterances] = torch.matmul(\n                embeds[i, j].unsqueeze(0), centroids_incl[i:i + 1].t())\n            sim_matrix[i * num_utterances + j, i * num_utterances + j] = 0\n            sim_matrix[i * num_utterances + j, (i + 1) * num_utterances:(i + 2) * num_utterances] = torch.matmul(\n                embeds[i, j].unsqueeze(0), centroids_excl[i:i + 1].t())\n    sim_matrix = self.scale * sim_matrix + self.bias\n    return sim_matrix, mask\n\n", "CodeBleu_score": 31.521190144730433}, {"code": "def similarity_matrix(self, embeds)\n\t\n    n_spks = embeds.shape[0]\n    n_utts = embeds.shape[1]\n    centroids_incl = torch.mean(embeds, dim=1, keepdim=True)\n    centroids_incl = F.normalize(centroids_incl, p=2, dim=2)\n    centroids_excl = torch.mean(embeds, dim=1, keepdim=True)\n    centroids_excl = F.normalize(centroids_excl, p=2, dim=2)\n    sim_matrix = torch.zeros((n_spks, n_utts, n_utts), device=embeds.device)\n    mask = torch.ones_like(sim_matrix) - torch.eye(n_utts, device=embeds.device)\n    for i in range(n_spks):\n        sim_matrix[i] = torch.matmul(embeds[i], centroids_incl[i].transpose(1, 2))\n        sim_matrix[i] += torch.matmul(embeds[i], centroids_excl[i].transpose(1, 2))\n        sim_matrix[i] /= 2\n        sim_matrix[i] *= self.scale\n        sim_matrix[i] += self.bias\n        sim_matrix[i] *= mask\n    return sim_matrix\n\n", "CodeBleu_score": 27.779241867789466}]}
{"instance_id": "a6eba54108a1", "generation_result": [{"code": "def loss(self, embeds)\n\t\n    # Get the number of speakers and utterances per speaker from the shape of the input embeddings\n    num_speakers, utterances_per_speaker, embedding_size = embeds.shape\n\n    # Compute a similarity matrix\n    similarity_matrix = torch.matmul(embeds, embeds.transpose(1, 0))\n\n    # Reshape the similarity matrix\n    similarity_matrix = similarity_matrix.view(num_speakers, utterances_per_speaker, num_speakers, utterances_per_speaker)\n\n    # Generate ground truth labels\n    labels = torch.zeros(num_speakers, utterances_per_speaker, num_speakers, utterances_per_speaker)\n    for i in range(num_speakers):\n        for j in range(utterances_per_speaker):\n            labels[i, j, i, j] = 1\n\n    # Calculate the loss\n    loss = self.loss_fn(similarity_matrix, labels)\n\n    # Calculate the EER without backpropagation\n    with torch.no_grad():\n        similarity_matrix = similarity_matrix.cpu().numpy()\n        labels = labels.cpu().numpy()\n        similarity_matrix = similarity_matrix.reshape(num_speakers * utterances_per_speaker, num_speakers * utterances_per_speaker)\n        labels = labels.reshape(num_speakers * utterances_per_speaker, num_speakers * utterances_per_speaker)\n        fpr, tpr, thresholds = roc_curve(labels.flatten(), similarity_matrix.flatten())\n        eer = brentq(lambda x: 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n\n    return loss, eer\n\n", "CodeBleu_score": 46.53324196818933}, {"code": "def loss(self, embeds)\n\t\n    # Get the number of speakers and utterances per speaker\n    num_speakers = embeds.shape[0]\n    num_utterances = embeds.shape[1]\n\n    # Calculate the similarity matrix\n    similarity_matrix = torch.matmul(embeds, embeds.T)\n\n    # Reshape the similarity matrix\n    similarity_matrix = similarity_matrix.view(num_speakers, num_utterances, num_speakers, num_utterances)\n\n    # Generate ground truth labels\n    labels = torch.eye(num_speakers).repeat(num_utterances, num_utterances).view(num_speakers * num_utterances,\n                                                                                 num_speakers * num_utterances)\n\n    # Calculate the loss\n    loss = self.loss_fn(similarity_matrix, labels)\n\n    # Calculate the EER without backpropagation\n    with torch.no_grad():\n        # Compute the ROC curve\n        fpr, tpr, thresholds = roc_curve(labels.cpu().numpy(), similarity_matrix.cpu().numpy())\n\n        # Find the threshold where the false positive rate equals the false negative rate\n        eer = brentq(lambda x: 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n\n    return loss, eer\n\n", "CodeBleu_score": 40.614298709703974}, {"code": "def loss(self, embeds)\n\t\n    # Number of utterances per speaker\n    n_utts = embeds.shape[0] // self.n_speakers\n\n    # Compute similarity matrix\n    sim_mat = torch.matmul(embeds, embeds.T)\n\n    # Generate ground truth labels\n    labels = torch.arange(self.n_speakers).repeat(n_utts)\n\n    # Calculate loss\n    loss = self.criterion(sim_mat, labels)\n\n    # Calculate EER\n    with torch.no_grad():\n        # Compute similarity matrix and reshape\n        sim_mat = sim_mat.reshape(self.n_speakers, n_utts, self.n_speakers, n_utts)\n        sim_mat = sim_mat.permute(0, 2, 1, 3)\n        sim_mat = sim_mat.reshape(self.n_speakers * self.n_speakers, n_utts * n_utts)\n\n        # Generate ground truth labels\n        labels = torch.arange(self.n_speakers).repeat(self.n_speakers * n_utts)\n\n        # Compute ROC curve\n        fpr, tpr, thresholds = roc_curve(labels.numpy(), sim_mat.numpy())\n\n        # Find EER threshold\n        eer = brentq(lambda x: 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n\n    return loss, eer\n\n", "CodeBleu_score": 41.6116005738704}]}
{"instance_id": "ac7b74b9d1b1", "generation_result": [{"code": "def log_params(self)\n\t\n    if self.log_params:\n        params = \"\"\n        for k, v in self.params.items():\n            params += k + \" : \" + str(v) + \"", "CodeBleu_score": 6.971301402297055}, {"code": "def log_params(self)\n\t\n    if not self.disable_logging:\n        # Import parameter configurations\n        import encoder.configs.model_params as model_params\n        import encoder.configs.data_params as data_params\n\n        # Get model and data parameter names and values\n        model_params_names = [name for name in model_params.__dict__ if not name.startswith('__')]\n        model_params_values = [model_params.__dict__[name] for name in model_params_names]\n        data_params_names = [name for name in data_params.__dict__ if not name.startswith('__')]\n        data_params_values = [data_params.__dict__[name] for name in data_params_names]\n\n        # Format model and data parameters\n        model_params_formatted = [f'{name} = {value}' for name, value in zip(model_params_names, model_params_values)]\n        data_params_formatted = [f'{name} = {value}' for name, value in zip(data_params_names, data_params_values)]\n\n        # Display model and data parameters\n        self.vis.text('", "CodeBleu_score": 28.408957360089587}, {"code": "def log_params(self)\n\t\n    if self.params.log_params:\n        model_params = ''\n        data_params = ''\n        for name, param in self.model.named_parameters():\n            model_params += name + ':'+ str(param.data.cpu().numpy()) + '", "CodeBleu_score": 8.928758551906988}]}
{"instance_id": "4726b7864386", "generation_result": [{"code": "def draw_projections(self, embeds, utterances_per_speaker, step, out_fpath=None, max_speakers=10)\n\t\n    # Limit the number of speakers to visualize\n    if len(embeds) > max_speakers:\n        embeds = embeds[:max_speakers]\n        utterances_per_speaker = utterances_per_speaker[:max_speakers]\n\n    # Flatten the embeddings and assign colors based on speaker identity\n    flat_embeds = np.vstack(embeds)\n    colors = np.repeat(np.arange(len(embeds)), utterances_per_speaker)\n    # Reduce the dimensionality of the embeddings using UMAP\n    start = timer()\n    reducer = umap.UMAP(n_neighbors=10, min_dist=0.01, metric=\"cosine\")\n    embeds_2d = reducer.fit_transform(flat_embeds)\n    print(f\"UMAP projection took {timer() - start:.2f} seconds\")\n    # Plot the embeddings and display or save to a file\n    fig = plt.figure(figsize=(10, 10))\n    plt.scatter(embeds_2d[:, 0], embeds_2d[:, 1], c=colors, cmap=\"tab10\")\n    plt.title(f\"UMAP projection of speaker embeddings (step {step})\")\n    if out_fpath:\n        plt.savefig(out_fpath)\n        plt.close(fig)\n    else:\n        plt.show()\n\n", "CodeBleu_score": 45.58949412308784}, {"code": "def draw_projections(self, embeds, utterances_per_speaker, step, out_fpath=None, max_speakers=10)\n\t\n\n    if self.visdom_env is not None:\n        self.visdom_env = self.visdom_env + f'_{self.model_name}'\n    else:\n        self.visdom_env = self.model_name\n\n    if len(utterances_per_speaker) > max_speakers:\n        utterances_per_speaker = utterances_per_speaker[:max_speakers]\n\n    flattened_embeds = []\n    flattened_speakers = []\n    flattened_utterances = []\n    for i, utterances in enumerate(utterances_per_speaker):\n        flattened_embeds.extend(embeds[i])\n        flattened_speakers.extend([i] * len(utterances))\n        flattened_utterances.extend(utterances)\n\n    umap_embeds = umap.UMAP(n_neighbors=5, min_dist=0.0, metric='cosine').fit_transform(flattened_embeds)\n\n    self.vis.scatter(\n        X=umap_embeds,\n        win=self.visdom_env,\n        opts=dict(\n            title=f'Step {step}',\n            markersize=3,\n            markercolor=flattened_speakers,\n            markercolor_max=len(utterances_per_speaker),\n            markercolor_min=0\n        ),\n        env=self.visdom_env\n    )\n\n    if out_fpath is not None:\n        self.vis.save([self.visdom_env], out_fpath)\n\n", "CodeBleu_score": 32.59212252142931}, {"code": "def draw_projections(self, embeds, utterances_per_speaker, step, out_fpath=None, max_speakers=10)\n\t\n    if self.visdom is None:\n        return\n\n    if out_fpath is None:\n        out_fpath = self.log_dir / 'projections'\n        out_fpath.mkdir(parents=True, exist_ok=True)\n        out_fpath = out_fpath / 'projections.png'\n\n    if self.visdom is not None:\n        self.visdom.win_exists = True\n        self.visdom.win = 'projections'\n\n    if len(embeds) > max_speakers:\n        embeds = embeds[:max_speakers]\n        utterances_per_speaker = utterances_per_speaker[:max_speakers]\n\n    if len(embeds) == 0:\n        return\n\n    embeds = embeds.reshape(len(embeds) * utterances_per_speaker, -1)\n\n    speakers = []\n    for i, utt_per_spk in enumerate(utterances_per_speaker):\n        speakers += [i] * utt_per_spk\n\n    speakers = np.array(speakers)\n\n    # UMAP\n    embeds = umap.UMAP(n_neighbors=10, min_dist=0.01, metric='cosine').fit_transform(embeds)\n\n    # Plot\n    plt.figure(figsize=(10, 10))\n    plt.scatter(embeds[:, 0], embeds[:, 1], c=speakers, cmap='tab20')\n    plt.title('Step: {}'.format(step))\n    plt.savefig(out_fpath)\n    plt.close()\n\n    if self.visdom is not None:\n        self.visdom.image(np.array(Image.open(out_fpath)), win=self.visdom.win, opts={'title': 'Projections'})\n\n", "CodeBleu_score": 41.190435649265275}]}
{"instance_id": "274fa50985c0", "generation_result": [{"code": "def add_speaker_embedding(self, x, speaker_embedding)\n\t\n    x_dim_names = x.dim_names\n    speaker_embedding_dim = x_dim_names.index(\"time\")\n    speaker_embedding = speaker_embedding.repeat(x.shape[speaker_embedding_dim], 1)\n    speaker_embedding = speaker_embedding.reshape(\n        [1] * (len(x_dim_names) - 1) + [speaker_embedding.shape[0], speaker_embedding.shape[1]]\n    )\n    speaker_embedding = speaker_embedding.transpose(0, len(x_dim_names) - 1)\n    x = torch.cat([x, speaker_embedding], dim=len(x_dim_names) - 1)\n    return x\n\n", "CodeBleu_score": 32.305608220130225}, {"code": "def add_speaker_embedding(self, x, speaker_embedding)\n\t\n    # Save the dimensions of the input tensor as human-readable names\n    batch_size, sequence_length, embedding_size = x.shape\n    # Determine the appropriate dimension index for the speaker embedding\n    speaker_embedding_dim = speaker_embedding.shape[-1]\n    # Repeat the speaker embedding to match the length of the input text\n    speaker_embedding = speaker_embedding.repeat(sequence_length, 1)\n    # Reshape and transpose the speaker embedding\n    speaker_embedding = speaker_embedding.view(\n        sequence_length, batch_size, speaker_embedding_dim\n    )\n    speaker_embedding = speaker_embedding.permute(1, 0, 2)\n    # Concatenate the speaker embedding with the input tensor along the last dimension\n    x = torch.cat([x, speaker_embedding], dim=-1)\n    return x\n\n", "CodeBleu_score": 19.49513867262328}, {"code": "def add_speaker_embedding(self, x, speaker_embedding)\n\t\n    # Save the dimensions of the input tensor as human-readable names\n    batch_size, sequence_length, _ = x.shape\n    # Determine the appropriate dimension index for the speaker embedding\n    speaker_embedding_dim = speaker_embedding.shape[1]\n    speaker_embedding_index = x.shape[2]\n    # Repeat the speaker embedding to match the length of the input text\n    speaker_embedding = speaker_embedding.repeat(1, sequence_length, 1)\n    # Reshape and transpose the speaker embedding\n    speaker_embedding = speaker_embedding.view(batch_size, sequence_length, speaker_embedding_dim, 1)\n    speaker_embedding = speaker_embedding.permute(0, 1, 3, 2)\n    # Concatenate the speaker embedding with the input tensor along the last dimension\n    x = torch.cat([x, speaker_embedding], dim=speaker_embedding_index)\n    return x\n\n", "CodeBleu_score": 18.358345148802485}]}
{"instance_id": "e6742ea0fe5e", "generation_result": [{"code": "def init_attention(self, encoder_seq_proj)\n\t\n    self.device = next(self.parameters()).device\n    self.batch_size = encoder_seq_proj.shape[0]\n    self.seq_len = encoder_seq_proj.shape[1]\n    self.cumulative = torch.zeros(self.batch_size, self.seq_len, device=self.device)\n    self.attention = torch.zeros(self.batch_size, self.seq_len, device=self.device)\n\n", "CodeBleu_score": 25.681574849063203}, {"code": "def init_attention(self, encoder_seq_proj)\n\t\n    device = self.encoder_seq_proj.weight.device\n    batch_size, seq_len = encoder_seq_proj.shape[0], encoder_seq_proj.shape[1]\n    self.cumulative_attention = torch.zeros(batch_size, seq_len, device=device)\n    self.attention = torch.zeros(batch_size, seq_len, device=device)\n\n", "CodeBleu_score": 21.165365591665054}, {"code": "def init_attention(self, encoder_seq_proj)\n\t\n    batch_size, seq_len = encoder_seq_proj.size()[:2]\n    device = encoder_seq_proj.device\n    self.cum_attention = torch.zeros((batch_size, seq_len), device=device)\n    self.attention = torch.zeros((batch_size, seq_len), device=device)\n\n", "CodeBleu_score": 31.556302055511843}]}
{"instance_id": "4e1fc1ed0ed7", "generation_result": [{"code": "def load(self, path, optimizer=None)\n\t\n    if not os.path.isfile(path):\n        raise ValueError(f\"Checkpoint not found at {path}\")\n\n    checkpoint = torch.load(path)\n    if \"model_state\" in checkpoint:\n        self.load_state_dict(checkpoint[\"model_state\"])\n    else:\n        self.load_state_dict(checkpoint)\n\n    if optimizer is not None and \"optimizer_state\" in checkpoint:\n        optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n\n    print(f\"Loaded checkpoint from {path}\")\n\n", "CodeBleu_score": 38.524489747767475}, {"code": "def load(self, path, optimizer=None)\n\t\n    path = Path(path)\n    if not path.exists():\n        raise FileNotFoundError(f\"File {path} does not exist.\")\n    if not path.is_file():\n        raise NotADirectoryError(f\"Path {path} is not a file.\")\n    checkpoint = torch.load(path, map_location=self.device)\n    self.load_state_dict(checkpoint[\"model_state_dict\"])\n    if optimizer is not None and \"optimizer_state_dict\" in checkpoint:\n        optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n\n", "CodeBleu_score": 42.740543657773934}, {"code": "def load(self, path, optimizer=None)\n\t\n    path = Path(path)\n    if not path.exists():\n        raise FileNotFoundError(f\"No checkpoint found at path {path}\")\n    if not path.is_file():\n        raise IsADirectoryError(f\"The path {path} is a directory, not a file.\")\n    checkpoint = torch.load(path, map_location=self.device)\n    self.load_state_dict(checkpoint[\"model_state\"])\n    if optimizer is not None and \"optimizer_state\" in checkpoint:\n        optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n\n", "CodeBleu_score": 43.130940470621084}]}
{"instance_id": "178e6aa16c0f", "generation_result": [{"code": "def save(self, path, optimizer=None)\n\t\n    if isinstance(path, Path):\n        path = str(path)\n    if not path.endswith(\".pt\"):\n        path += \".pt\"\n    if optimizer is not None:\n        torch.save(\n            {\"model_state_dict\": self.state_dict(), \"optimizer_state_dict\": optimizer.state_dict()},\n            path,\n        )\n    else:\n        torch.save(self.state_dict(), path)\n\n", "CodeBleu_score": 51.46561128468796}, {"code": "def save(self, path, optimizer=None)\n\t\n    if not isinstance(path, Path):\n        path = Path(path)\n    if not path.parent.exists():\n        os.makedirs(path.parent)\n    if optimizer is not None:\n        torch.save({\n           'model_state_dict': self.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict()\n        }, path)\n    else:\n        torch.save(self.state_dict(), path)\n\n", "CodeBleu_score": 60.5128112202943}, {"code": "def save(self, path, optimizer=None)\n\t\n    if not isinstance(path, Path):\n        path = Path(path)\n    if not path.parent.exists():\n        path.parent.mkdir(parents=True)\n    if optimizer is None:\n        torch.save(self.state_dict(), path)\n    else:\n        torch.save({\n            \"model_state\": self.state_dict(),\n            \"optimizer_state\": optimizer.state_dict()\n        }, path)\n\n", "CodeBleu_score": 61.24793711190733}]}
{"instance_id": "6528fd80e87b", "generation_result": [{"code": "def english_cleaners(text)\n\t\n    text = unidecode(text)\n    text = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", text)\n    text = re.sub(r\"\\'s\", \" \\'s\", text)\n    text = re.sub(r\"\\'ve\", \" \\'ve\", text)\n    text = re.sub(r\"n\\'t\", \" n\\'t\", text)\n    text = re.sub(r\"\\'re\", \" \\'re\", text)\n    text = re.sub(r\"\\'d\", \" \\'d\", text)\n    text = re.sub(r\"\\'ll\", \" \\'ll\", text)\n    text = re.sub(r\",\", \", \", text)\n    text = re.sub(r\"!\", \"! \", text)\n    text = re.sub(r\"\\(\", \" \\( \", text)\n    text = re.sub(r\"\\)\", \" \\) \", text)\n    text = re.sub(r\"\\?\", \" \\? \", text)\n    text = re.sub(r\"\\s{2,}\", \" \", text)\n    text = normalize_numbers(text)\n    return text.strip().lower()\n\n", "CodeBleu_score": 40.934214583190084}, {"code": "def english_cleaners(text)\n\t\n    text = unidecode(text)\n    text = re.sub(r\"[^A-Za-z0-9]+\", \" \", text)\n    text = re.sub(r\"\\s+\", \" \", text)\n    return text.strip()\n\n", "CodeBleu_score": 17.45862639156794}, {"code": "def english_cleaners(text)\n\t\n    # collapse whitespace\n    text = re.sub(r\"\\s+\", \" \", text).strip()\n    # convert to ascii\n    text = unidecode(text)\n    # lowercase\n    text = text.lower()\n    # expand numbers\n    text = re.sub(r\"\\b-\\b\", \"-\", text)\n    text = re.sub(r\"(\\d)-(\\d)\", r\"\\1 - \\2\", text)\n    text = re.sub(r\"(\\d)- (\\d)\", r\"\\1-\\2\", text)\n    text = re.sub(r\"(\\d) -(\\d)\", r\"\\1-\\2\", text)\n    text = re.sub(r\"(\\d) - (\\d)\", r\"\\1-\\2\", text)\n    text = re.sub(r\"(\\d)\\.(\\d)\", r\"\\1.\\2\", text)\n    text = re.sub(r\"(\\d)\\. (\\d)\", r\"\\1.\\2\", text)\n    text = re.sub(r\"(\\d) \\.(\\d)\", r\"\\1.\\2\", text)\n    text = re.sub(r\"(\\d) \\. (\\d)\", r\"\\1.\\2\", text)\n    text = normalize_numbers(text)\n    # expand abbreviations\n    text = re.sub(r\"(\\w)\\.(\\w\\.)+\", r\"\\1\", text)\n    text = re.sub(r\"(\\w)\\. (\\w\\.)+\", r\"\\1\", text)\n    text = re.sub(r\"(\\w) \\.(\\w\\.)+\", r\"\\1\", text)\n    text = re.sub(r\"(\\w) \\. (\\w\\.)+\", r\"\\1\", text)\n    text = re.sub(r\"(\\w)\\.(\\w)\", r\"\\1 \\2\", text)\n    text = re.sub(r\"(\\w)\\. (\\w)\", r\"\\1 \\2\", text)\n    text = re.sub(r\"(\\w) \\.(\\w)\", r\"\\1 \\2\", text)\n    text = re.sub(r\"(\\w) \\. (\\w)\", r\"\\1 \\2\", text)\n    text = re.sub(r\"\\.(\\w)\", r\" \\1\", text)\n    text = re.sub(r\"\\. (\\w)\", r\" \\1\", text)\n    text = re", "CodeBleu_score": 36.84495630494605}]}
{"instance_id": "8a4fe08e0985", "generation_result": [{"code": "def text_to_sequence(text, cleaner_names)\n\t\n    # text = text.lower()\n    sequence = []\n    # Check for curly braces and treat their contents as ARPAbet:\n    while len(text) > 0:\n        m = _curly_re.match(text)\n        if not m:\n            sequence += _symbols_to_sequence(_clean_text(text, cleaner_names))\n            break\n        sequence += _symbols_to_sequence(_clean_text(m.group(1), cleaner_names))\n        sequence += _arpabet_to_sequence(m.group(2))\n        text = m.group(3)\n    # Add end-of-sequence token:\n    sequence.append(_eos)\n    return sequence\n\n", "CodeBleu_score": 72.93672796006862}, {"code": "def text_to_sequence(text, cleaner_names)\n\t\n    # text = text.lower()\n    sequence = []\n\n    # Check for curly braces and treat their contents as ARPAbet phonetic notation if found.\n    if len(text) and text[0] == '{' and text[-1] == '}':\n        sequence += _arpabet_to_sequence(_clean_text(text[1:-1], cleaner_names), cleaner_names)\n    else:\n        # Just regular text.\n        sequence += _text_to_sequence(_clean_text(text, cleaner_names), cleaner_names)\n\n    # Add EOS token\n    sequence += [len(symbols)]\n\n    return sequence\n\n", "CodeBleu_score": 27.844408936784326}, {"code": "def text_to_sequence(text, cleaner_names)\n\t\n    sequence = []\n\n    # Check for curly braces and treat their contents separately\n    text_no_curly = re.sub(r'\\{.*?\\}', '', text)\n    text_no_curly = re.sub(r'\\s', '', text_no_curly)\n    text_no_curly = text_no_curly.strip()\n    sequence += text_to_sequence(text_no_curly, cleaner_names)\n    curly_contents = [m.group(0) for m in re.finditer(r'\\{.*?\\}', text)]\n    for i, curly_content in enumerate(curly_contents):\n        curly_content = curly_content[1:-1]  # Remove curly braces\n        curly_content = re.sub(r'\\s', '', curly_content)\n        if len(curly_content) > 0:\n            sequence += text_to_sequence(curly_content, cleaner_names)\n        sequence += text_to_sequence(' ', cleaner_names)\n    sequence = sequence[:-1] + [len(symbols) - 1]  # Append EOS token\n    return sequence\n\n", "CodeBleu_score": 31.303959088308954}]}
{"instance_id": "e1c25a37af1f", "generation_result": [{"code": "def sequence_to_text(sequence)\n\t\n    result = \"\"\n    for symbol_id in sequence:\n        if symbol_id in symbols:\n            s = symbols[symbol_id]\n            # Enclose ARPAbet symbols in curly braces.\n            if len(s) > 1 and s[0] == \"@\":\n                s = \"{\" + s[1:] + \"}\"\n            result += s\n    # Replace adjacent curly braces with a space.\n    result = re.sub(r\"(\\{[^}]+\\})+\", \" \", result)\n    return result.strip()\n\n", "CodeBleu_score": 56.23307179299465}, {"code": "def sequence_to_text(sequence)\n\t\n    result = ''\n    for symbol_id in sequence:\n        if symbol_id in symbols:\n            s = symbols[symbol_id]\n            if len(s) > 1 and s[0] == '@':\n                s = cleaners.arpabet_to_valid_ascii(s[1:])\n            else:\n                s = cleaners.text_to_valid_ascii(s)\n            if len(s) > 0:\n                result = result +'' + s\n        elif symbol_id == 0:\n            result = result +''\n        else:\n            print('Unknown symbol: {}'.format(symbol_id))\n    return re.sub('\\s+','', result).strip()", "CodeBleu_score": 49.6393655491677}, {"code": "def sequence_to_text(sequence)\n\t\n    result = ''\n    for symbol_id in sequence:\n        if symbol_id in symbols:\n            s = symbols[symbol_id]\n            # Enclose ARPAbet symbols in curly braces.\n            if len(s) > 1 and s[0] == '@':\n                s = '{%s}' % s[1:]\n            result += s\n    # Replace adjacent curly braces with a space.\n    result = re.sub(r'(\\{[^}]+\\})+','', result)\n    return result.strip()\n\n", "CodeBleu_score": 61.248667120785306}]}
{"instance_id": "7807cad4b05d", "generation_result": [{"code": "def embed_utterance(fpaths, encoder_model_fpath)\n\t\n    if not encoder.is_loaded:\n        encoder.load_model(encoder_model_fpath)\n    embeds = []\n    for fpath in fpaths:\n        wav = encoder.preprocess_wav(fpath)\n        embed = encoder.embed_utterance(wav)\n        embeds.append(embed)\n    return embeds\n\n", "CodeBleu_score": 45.55119222620424}, {"code": "def embed_utterance(fpaths, encoder_model_fpath)\n\t\n\n    # Load encoder model\n    encoder_model = encoder.load_model(encoder_model_fpath)\n\n    # Load and preprocess waveform\n    waveform, sample_rate = librosa.load(fpaths[0], sr=None)\n    waveform = encoder.preprocess_wav(waveform)\n\n    # Compute embedding\n    embedding = encoder.embed_utterance(encoder_model, waveform)\n\n    # Save embedding\n    np.save(fpaths[1], embedding)\n\n", "CodeBleu_score": 22.985381276119426}, {"code": "def embed_utterance(fpaths, encoder_model_fpath)\n\t\n    encoder_model = encoder.load_model(encoder_model_fpath)\n    preprocessor = encoder.preprocess_wav\n    embed = encoder.embed_utterance\n    for fpath in tqdm(fpaths):\n        wav = encoder.preprocess_wav(fpath)\n        embeds = embed(encoder_model, wav)\n        np.save(fpath.with_suffix('.npy'), embeds, allow_pickle=False)\n\n", "CodeBleu_score": 36.907004138524925}]}
{"instance_id": "9f860273a325", "generation_result": [{"code": "def create_embeddings(synthesizer_root: Path, encoder_model_fpath: Path, n_processes: int)\n\t\n\n    # Check if the synthesizer root path exists\n    if not synthesizer_root.exists():\n        raise FileNotFoundError(f\"Synthesizer root path {synthesizer_root} not found.\")\n\n    # Check if the encoder model file path exists\n    if not encoder_model_fpath.exists():\n        raise FileNotFoundError(f\"Encoder model file path {encoder_model_fpath} not found.\")\n\n    # Check if the metadata file exists\n    metadata_fpath = synthesizer_root / \"metadata.csv\"\n    if not metadata_fpath.exists():\n        raise FileNotFoundError(f\"Metadata file {metadata_fpath} not found.\")\n\n    # Check if the audio directory exists\n    audio_dir = synthesizer_root / \"audio\"\n    if not audio_dir.exists():\n        raise FileNotFoundError(f\"Audio directory {audio_dir} not found.\")\n\n    # Check if the embeddings directory exists\n    embeddings_dir = synthesizer_root / \"embeddings\"\n    if not embeddings_dir.exists():\n        raise FileNotFoundError(f\"Embeddings directory {embeddings_dir} not found.\")\n\n    # Read the metadata file\n    metadata = pd.read_csv(metadata_fpath)\n\n    # Create a list of audio file paths\n    audio_fpaths = [audio_dir / f\"{row['id']}.wav\" for _, row in metadata.iterrows()]\n\n    # Create a list of embedding file paths\n    embedding_fpaths = [embeddings_dir / f\"{row['id']}.npy\" for _, row in metadata.iterrows()]\n\n    # Create a pool of processes\n    pool = Pool(n_processes)\n\n    # Create a partial function for embedding generation\n    generate_embedding = partial(\n        _generate_embedding, encoder_model_fpath=encoder_model_fpath\n    )\n\n    # Generate embeddings in parallel\n    with tqdm(total=len(audio_fpaths)) as pbar:\n        for _ in pool.imap_unordered(generate_embedding, zip(audio_fpaths, embedding_fpaths)):\n            pbar.update()\n\n", "CodeBleu_score": 33.24538131109268}, {"code": "def create_embeddings(synthesizer_root: Path, encoder_model_fpath: Path, n_processes: int)\n\t\n    # Create the output directory if it doesn't exist\n    output_dir = synthesizer_root / \"embeddings\"\n    output_dir.mkdir(exist_ok=True)\n\n    # Load the encoder model\n    encoder_model = encoder.load_model(encoder_model_fpath)\n\n    # Read the metadata file\n    metadata_fpath = synthesizer_root / \"metadata.csv\"\n    metadata_df = pd.read_csv(metadata_fpath, header=None, names=[\"file_path\", \"text\"])\n\n    # Get the list of audio files\n    audio_files = metadata_df[\"file_path\"].tolist()\n\n    # Get the list of audio file paths\n    audio_filepaths = [synthesizer_root / \"wavs\" / audio_file for audio_file in audio_files]\n\n    # Create a list of tuples containing the audio file path and the output path for the corresponding embedding\n    audio_embedding_pairs = [(audio_filepath, output_dir / f\"{audio_filepath.stem}.npy\") for audio_filepath in audio_filepaths]\n\n    # Create a partial function with the encoder model and output directory\n    partial_func = partial(create_embedding, encoder_model=encoder_model, output_dir=output_dir)\n\n    # Create a multiprocessing pool\n    with Pool(n_processes) as pool:\n        # Create embeddings for the audio files in parallel\n        for _ in tqdm(pool.imap_unordered(partial_func, audio_embedding_pairs), total=len(audio_embedding_pairs)):\n            pass\n\n", "CodeBleu_score": 27.62085748451193}, {"code": "def create_embeddings(synthesizer_root: Path, encoder_model_fpath: Path, n_processes: int)\n\t\n    # Check if the synthesizer root directory exists\n    if not synthesizer_root.exists():\n        raise FileNotFoundError(f\"Synthesizer root directory not found: {synthesizer_root}\")\n\n    # Check if the encoder model file exists\n    if not encoder_model_fpath.exists():\n        raise FileNotFoundError(f\"Encoder model file not found: {encoder_model_fpath}\")\n\n    # Load the encoder model\n    encoder.load_model(encoder_model_fpath)\n\n    # Read the metadata file\n    metadata_fpath = synthesizer_root / \"metadata.csv\"\n    metadata_df = pd.read_csv(metadata_fpath, sep=\"|\")\n\n    # Prepare the file paths for audio and embeddings\n    audio_dir = synthesizer_root / \"wavs\"\n    embeddings_dir = synthesizer_root / \"embeddings\"\n    audio_fpaths = [audio_dir / f\"{row.id}.wav\" for _, row in metadata_df.iterrows()]\n    embeddings_fpaths = [embeddings_dir / f\"{row.id}.npy\" for _, row in metadata_df.iterrows()]\n\n    # Create a multiprocessing pool for parallel processing\n    with Pool(n_processes) as pool:\n        # Define a partial function for processing audio files\n        process_audio_partial = partial(process_audio, encoder_model_fpath=encoder_model_fpath)\n\n        # Process the audio files in parallel, generating embeddings\n        for _ in tqdm(pool.imap_unordered(process_audio_partial, zip(audio_fpaths, embeddings_fpaths)), total=len(audio_fpaths)):\n            pass\n\n", "CodeBleu_score": 35.365617377602945}]}
{"instance_id": "9eb0abda14a9", "generation_result": [{"code": "def eval_model(attention, mel_prediction, target_spectrogram, input_seq, step,\n               plot_dir, mel_output_dir, wav_dir, sample_num, loss, hparams)\n\t\n    # Plot attention plot\n    attention_plot = plot_attention(attention, input_seq, step, plot_dir)\n\n    # Plot predicted spectrogram\n    plot_spectrogram(mel_prediction, step, mel_output_dir, title=\"Predicted mel spectrogram\")\n\n    # Reconstruct waveform\n    wav = audio.inv_mel_spectrogram(mel_prediction.T, hparams)\n    audio.save_wav(wav, wav_dir / f\"{step}.wav\")\n\n    # Plot predicted and target mel spectrograms\n    plot_spectrogram(mel_prediction, target_spectrogram, step, plot_dir,\n                     title=f\"{hparams.model_type} - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - Step {step} - Loss {loss:.4f}\")\n\n    # Print input sequence\n    print(\"Input text:\", sequence_to_text(input_seq))\n\n", "CodeBleu_score": 24.88207109726577}, {"code": "def eval_model(attention, mel_prediction, target_spectrogram, input_seq, step,\n               plot_dir, mel_output_dir, wav_dir, sample_num, loss, hparams)\n\t\n    if attention is not None:\n        plot_attention(attention, input_seq, step, plot_dir)\n\n    plot_spectrogram(mel_prediction, title=f\"Predicted Mel Spectrogram (Step {step})\",\n                     path=mel_output_dir / f\"mel_step_{step}.png\")\n    plot_spectrogram(target_spectrogram, title=f\"Target Mel Spectrogram (Step {step})\",\n                     path=mel_output_dir / f\"target_mel_step_{step}.png\")\n\n    plot_spectrogram(mel_prediction, target_spectrogram,\n                     title=f\"Mel Spectrogram Comparison (Step {step}, Loss {loss:.4f})\",\n                     path=plot_dir / f\"mel_step_{step}.png\")\n\n    plot_attention(attention, input_seq, step, plot_dir)\n\n    reconstruction = audio.inv_spectrogram(mel_prediction.data.cpu())\n    audio.save_wav(reconstruction, wav_dir / f\"step_{step}.wav\")\n\n    print(f\"Step {step}: {sequence_to_text(input_seq)}\")\n\n", "CodeBleu_score": 29.111263499744744}, {"code": "def eval_model(attention, mel_prediction, target_spectrogram, input_seq, step,\n               plot_dir, mel_output_dir, wav_dir, sample_num, loss, hparams)\n\t\n    # Set the model to evaluation mode\n    attention.eval()\n    mel_prediction.eval()\n    # Move the model to the device specified by the hparams\n    attention.to(hparams.device)\n    mel_prediction.to(hparams.device)\n    # Initialize the attention window\n    attention_window = ValueWindow(10)\n    # Move the input sequence to the device specified by the hparams\n    input_seq = input_seq.to(hparams.device)\n    # Get the attention and mel spectrogram predictions for the input sequence\n    attention_output, mel_output = attention(input_seq), mel_prediction(input_seq)\n    # Move the attention and mel spectrogram predictions to the CPU\n    attention_output, mel_output = attention_output.cpu(), mel_output.cpu()\n    # Get the attention weights from the attention output\n    attention_weights = attention_output.squeeze(1).transpose(0, 1)\n    # Add the attention weights to the attention window\n    attention_window.add_value(attention_weights)\n    # Get the average attention weights from the attention window\n    average_attention_weights = attention_window.get_average()\n    # Get the predicted mel spectrogram from the mel output\n    predicted_mel = mel_output.squeeze(1).transpose(0, 1)\n    # Get the target mel spectrogram from the target spectrogram\n    target_mel = target_spectrogram.squeeze(1).transpose(0, 1)\n    # Plot the attention weights\n    plot_attention(average_attention_weights, input_seq, symbols, step, plot_dir)\n    # Plot the predicted mel spectrogram\n    plot_spectrogram(predicted_mel, title=\"Predicted Mel\", step=step,\n                     plot_dir=plot_dir)\n    # Plot the target mel spectrogram\n    plot_spectrogram(target_mel, title=\"Target Mel\", step=step, plot_dir=plot_dir)\n    # Plot the predicted and target mel spectrograms\n    plot_spectrogram(predicted_mel, target_mel, title=\"Predicted vs. Target Mel\",\n                     step=step, plot_dir=plot_dir)\n    # Generate the reconstructed waveform from the predicted mel spectrogram\n    reconstructed_waveform = audio.inv_spectrogram(predicted_mel.data.numpy())\n    # Save the reconstructed waveform to the", "CodeBleu_score": 25.10425427280095}]}
{"instance_id": "6bad4f500286", "generation_result": [{"code": "def num_params(self)\n\t\n    pp = 0\n    for p in list(self.parameters()):\n        nn = 1\n        for s in list(p.size()):\n            nn = nn * s\n        pp += nn\n    print(\"Number of parameters: %.3fM\" % (pp / 1000000))\n\n", "CodeBleu_score": 26.0346814134138}, {"code": "def num_params(self)\n\t\n    total_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n    print(f\"Number of trainable parameters: {total_params / 1e6:.3f}M\")\n\n", "CodeBleu_score": 14.363028395028854}, {"code": "def num_params(self)\n\t\n    params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n    print(f\"The number of trainable parameters is {params:,} or {params/1e6:,.3f}M\")\n\n", "CodeBleu_score": 14.84374588380924}]}
{"instance_id": "78b876054bd4", "generation_result": [{"code": "def pad_tensor(self, x, pad, side='both')\n\t\n    if side == 'before':\n        return torch.cat([torch.full((x.size(0), x.size(1), pad), 0), x], dim=-1)\n    elif side == 'after':\n        return torch.cat([x, torch.full((x.size(0), x.size(1), pad), 0)], dim=-1)\n    elif side == 'both':\n        return torch.cat([torch.full((x.size(0), x.size(1), pad), 0), x, torch.full((x.size(0), x.size(1), pad), 0)], dim=-1)\n\n", "CodeBleu_score": 23.021969460047853}, {"code": "def pad_tensor(self, x, pad, side='both')\n\t\n    if side == 'before':\n        return torch.cat([x, torch.zeros(pad, *x.shape[1:]).type(x.type())], dim=0)\n    elif side == 'after':\n        return torch.cat([torch.zeros(pad, *x.shape[1:]).type(x.type()), x], dim=0)\n    elif side == 'both':\n        return torch.cat([torch.zeros(pad, *x.shape[1:]).type(x.type()), x, torch.zeros(pad, *x.shape[1:]).type(x.type())], dim=0)\n    else:\n        raise ValueError('Side must be either \"before\", \"after\", or \"both\"')\n\n", "CodeBleu_score": 24.933529613164485}, {"code": "def pad_tensor(self, x, pad, side='both')\n\t\n    if side == 'before':\n        return torch.cat((torch.zeros(x.size(0), pad, x.size(2)).to(x.device), x), dim=1)\n    elif side == 'after':\n        return torch.cat((x, torch.zeros(x.size(0), pad, x.size(2)).to(x.device)), dim=1)\n    elif side == 'both':\n        return torch.cat((torch.zeros(x.size(0), pad, x.size(2)).to(x.device), x, torch.zeros(x.size(0), pad, x.size(2)).to(x.device)), dim=1)\n    else:\n        raise ValueError('Side should be either \"before\", \"after\", or \"both\".')\n\n", "CodeBleu_score": 22.83574362338099}]}
{"instance_id": "a7ed1715a3df", "generation_result": [{"code": "def save_attention(attn, path)\n\t    import matplotlib.pyplot as plt\n    fig = plt.figure(figsize=(10, 10))\n    ax = fig.add_subplot(111)\n    cax = ax.matshow(attn, cmap='bone')\n    fig.colorbar(cax)\n    plt.savefig(path + '.png')\n    plt.close(fig)\n\n", "CodeBleu_score": 51.93000263799167}, {"code": "def save_attention(attn, path)\n\t    import matplotlib.pyplot as plt\n    fig = plt.figure(figsize=(10, 10))\n    ax = fig.add_subplot(111)\n    cax = ax.matshow(attn, cmap='bone')\n    fig.colorbar(cax)\n    plt.savefig(path + '.png')\n    plt.close()\n\n", "CodeBleu_score": 50.1766983915584}, {"code": "def save_attention(attn, path)\n\t    import matplotlib.pyplot as plt\n\n    fig = plt.figure(figsize=(10, 10))\n    plt.imshow(attn, interpolation=\"nearest\", cmap=\"gray\")\n    plt.savefig(path + \".png\")\n    plt.close(fig)\n\n", "CodeBleu_score": 53.27564217300651}]}
{"instance_id": "19375c9f44a2", "generation_result": [{"code": "def plot(array)\n\t\n    plt.figure(figsize=(10, 5))\n    ax = plt.subplot(111)\n    ax.spines[\"left\"].set_color(\"red\")\n    ax.spines[\"bottom\"].set_color(\"red\")\n    ax.spines[\"right\"].set_color(None)\n    ax.spines[\"top\"].set_color(None)\n    ax.xaxis.label.set_color(\"red\")\n    ax.tick_params(axis=\"x\", colors=\"red\")\n    ax.yaxis.label.set_color(\"red\")\n    ax.tick_params(axis=\"y\", colors=\"red\")\n    plt.plot(array)\n    plt.show()\n\n", "CodeBleu_score": 37.60787708880886}, {"code": "def plot(array)\n\t\n    fig = plt.figure(figsize=(10, 5))\n    ax = fig.add_subplot(111)\n    ax.set_xlabel(\"x\", color=\"white\", fontsize=14)\n    ax.set_ylabel(\"y\", color=\"white\", fontsize=14)\n    ax.tick_params(axis=\"x\", colors=\"white\", labelsize=12)\n    ax.tick_params(axis=\"y\", colors=\"white\", labelsize=12)\n    ax.plot(array)\n    plt.show()\n\n", "CodeBleu_score": 28.148509947714896}, {"code": "def plot(array)\n\t\n    fig = plt.figure(figsize=(16, 10))\n    ax = fig.add_subplot(111)\n    ax.set_xlabel(\"x\", color=\"white\", fontsize=15)\n    ax.set_ylabel(\"y\", color=\"white\", fontsize=15)\n    ax.tick_params(axis=\"x\", colors=\"white\", labelsize=15)\n    ax.tick_params(axis=\"y\", colors=\"white\", labelsize=15)\n    plt.plot(array)\n    plt.show()\n\n", "CodeBleu_score": 25.8759887096934}]}
{"instance_id": "ff94c425920c", "generation_result": [{"code": "def write_to_csv(image_name, prediction, confidence)\n\t\n    with open(\"predictions.csv\", \"a\") as f:\n        writer = csv.writer(f)\n        if os.stat(\"predictions.csv\").st_size == 0:\n            writer.writerow([\"Image Name\", \"Prediction\", \"Confidence\"])\n        writer.writerow([image_name, prediction, confidence])\n\n", "CodeBleu_score": 35.638790477031776}, {"code": "def write_to_csv(image_name, prediction, confidence)\n\t\n    # Create a dictionary with the image name, prediction, and confidence\n    data = {\"image_name\": image_name, \"prediction\": prediction, \"confidence\": confidence}\n    # Open the CSV file in append mode\n    with open(\"output.csv\", \"a\") as csv_file:\n        # Create a CSV writer object\n        writer = csv.DictWriter(csv_file, fieldnames=data.keys())\n        # Write the header if the file is new\n        if os.path.getsize(\"output.csv\") == 0:\n            writer.writeheader()\n        # Write the data as a new row\n        writer.writerow(data)\n\n", "CodeBleu_score": 53.55194906868117}, {"code": "def write_to_csv(image_name, prediction, confidence)\n\t\n    data = {\"Image\": image_name, \"Prediction\": prediction, \"Confidence\": confidence}\n    with open(\"prediction_results.csv\", \"a\", newline=\"\") as csvfile:\n        fieldnames = [\"Image\", \"Prediction\", \"Confidence\"]\n        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n        if not os.path.isfile(\"prediction_results.csv\"):\n            writer.writeheader()\n        writer.writerow(data)\n\n", "CodeBleu_score": 56.251443286267}]}
{"instance_id": "436161602009", "generation_result": [{"code": "def export_torchscript(model, im, file, optimize, prefix=colorstr(\"TorchScript:\"))\n\t\n    LOGGER.info(f\"{prefix} Exporting model to TorchScript...\")\n    model.model.train()\n    model.model.fuse()\n    model.model.to(im.device).eval()\n    model.model.model[-1].export = True  # set Detect() layer export flag\n    if optimize:\n        LOGGER.info(f\"{prefix} Optimizing model for mobile use...\")\n        model.model = optimize_for_mobile(model.model)\n    traced_model = torch.jit.trace(model.model, im)\n    if optimize:\n        traced_model.save(file)\n    else:\n        traced_model._save_for_lite_interpreter(file)\n    return file, None\n\n", "CodeBleu_score": 34.111460032747445}, {"code": "def export_torchscript(model, im, file, optimize, prefix=colorstr(\"TorchScript:\"))\n\t\n    LOGGER.info(f\"{prefix} Exporting model to TorchScript format\")\n    LOGGER.info(f\"{prefix} Input image shape: {im.shape}\")\n    LOGGER.info(f\"{prefix} Input image size: {file_size(file)}\")\n    with smart_inference_mode(model):\n        if optimize:\n            model = optimize_for_mobile(model)\n        ts = torch.jit.trace(model, im)\n        ts.save(file)\n    LOGGER.info(f\"{prefix} Exported model to {file}\")\n    return file, None\n\n", "CodeBleu_score": 31.32798543623544}, {"code": "def export_torchscript(model, im, file, optimize, prefix=colorstr(\"TorchScript:\"))\n\t\n    LOGGER.info(f\"{prefix}Exporting model to TorchScript format...\")\n    start = time.time()\n    if optimize:\n        LOGGER.info(f\"{prefix}Optimizing model for mobile use...\")\n        model = optimize_for_mobile(model)\n    f = f\"{file}.torchscript\"\n    traced_script_module = torch.jit.trace(model, im)\n    traced_script_module.save(f)\n    LOGGER.info(f\"{prefix}Model exported to {colorstr(f, 'green')} in {time.time() - start:.2f}s\")\n    return f, None\n\n", "CodeBleu_score": 32.91920426348615}]}
{"instance_id": "78335487ac1c", "generation_result": [{"code": "def export_coreml(model, im, file, int8, half, nms, prefix=colorstr(\"CoreML:\")):\n\t    if not torch.backends.mps.is_available():\n        import coremltools\n    else:\n        LOGGER.warning(\n            \"CoreML export is not supported on Apple Silicon. Skipping CoreML export.\"\n        )\n        return None, None\n    LOGGER.info(f\"{prefix} Exporting model to CoreML format\")\n    if isinstance(model, ClassificationModel):\n        model.model.eval()\n        traced_model = torch.jit.trace(model.model, im)\n    elif isinstance(model, DetectionModel):\n        model.model.eval()\n        traced_model = torch.jit.trace(model.model, im)\n    elif isinstance(model, SegmentationModel):\n        model.model.eval()\n        traced_model = torch.jit.trace(model.model, im)\n    else:\n        LOGGER.warning(\n            f\"{prefix} Unsupported model type for CoreML export. Skipping CoreML export.\"\n        )\n        return None, None\n    if nms:\n        LOGGER.info(f\"{prefix} Adding iOSModel for NMS support\")\n        traced_model = coremltools.models.MLModel(traced_model)\n    try:\n        coreml_model = coremltools.converters.convert(\n            traced_model,\n            inputs=[\n                coremltools.TensorType(\n                    shape=im.shape,\n                    name=\"image\",\n                    is_bgr=model.model.names[0] == \"yolo\",\n                )\n            ],\n            minimum_deployment_target=coremltools.target.ios,\n            convert_to=\"mlprogram\",\n        )\n    except Exception as e:\n        LOGGER.warning(f\"{prefix} CoreML conversion failed: {e}\")\n        return None, None\n    if int8:\n        if not coremltools.utils.macos_version() >= (11, 0):\n            LOGGER.warning(\n                f\"{prefix} CoreML INT8 quantization is only supported on macOS 11.0+. Skipping CoreML INT8 quantization.\"\n            )\n        else:\n            LOGGER.info(f\"{prefix} Quantizing model to INT8\")\n            coreml_model = coremltools.utils.quantization.quantize_weights(\n                coreml_model, nbits=8\n            )\n    if half:\n        if not coremltools.utils.macos_version() >= (12, 0):\n            LOGGER", "CodeBleu_score": 36.29636814463034}, {"code": "def export_coreml(model, im, file, int8, half, nms, prefix=colorstr(\"CoreML:\")):\n\t    from coremltools.converters.mil import register_torch_op\n\n    if not COREML_EXPORT_SUPPORTED:\n        LOGGER.error(\n            \"%s CoreML export is not supported on your system and/or Python version. Please see https://github.com/apple/coremltools#system-requirements for more details.\"\n            % prefix\n        )\n        return \"\", None\n    LOGGER.info(\n        \"%s Exporting %s to CoreML format, please wait...\"\n        % (prefix, Path(file).stem)\n    )\n    if isinstance(model, (ClassificationModel, SegmentationModel)):\n        model.model.eval()\n    elif isinstance(model, DetectionModel):\n        model.model.model.eval()\n    else:\n        model.eval()\n    if isinstance(model, (ClassificationModel, SegmentationModel)):\n        model.model.model = model.model.model.half() if half else model.model.model.float()\n    elif isinstance(model, DetectionModel):\n        model.model.model.model = model.model.model.model.half() if half else model.model.model.model.float()\n    else:\n        model.model = model.model.half() if half else model.model.float()\n    if isinstance(model, (ClassificationModel, SegmentationModel)):\n        model.model.forward = model.model.forward.trace(\n            model.model.forward,\n            args=(im,),\n            strict=False,\n        )\n    elif isinstance(model, DetectionModel):\n        model.model.model.forward = model.model.model.forward.trace(\n            model.model.model.forward,\n            args=(im,),\n            strict=False,\n        )\n    else:\n        model.forward = model.forward.trace(model.forward, args=(im,), strict=False)\n    if isinstance(model, (ClassificationModel, SegmentationModel)):\n        model.model.forward = model.model.forward.eval()\n    elif isinstance(model, DetectionModel):\n        model.model.model.forward = model.model.model.forward.eval()\n    else:\n        model.forward = model.forward.eval()\n    if isinstance(model, (ClassificationModel, SegmentationModel)):\n        coreml_model = ct.convert(model.model.model, inputs=[ct.ImageType(name=\"input_1\", shape=im.shape)])\n    elif isinstance(model, DetectionModel", "CodeBleu_score": 32.37411321541385}, {"code": "def export_coreml(model, im, file, int8, half, nms, prefix=colorstr(\"CoreML:\")):\n\t    try:\n        import coremltools\n    except ImportError:\n        LOGGER.error(f\"{prefix} coremltools not found, please install with: pip install coremltools\")\n        return None, None\n    LOGGER.info(f\"{prefix} starting export with torch {torch.__version__} {torch.cuda.get_device_properties(0).name if torch.cuda.is_available() else 'CPU'}\")\n    if isinstance(model, torch.nn.parallel.DistributedDataParallel):\n        model = model.module\n    if isinstance(model, torch.nn.DataParallel):\n        model = model.module\n    if isinstance(model, torch.jit.ScriptModule):\n        model = model.forward\n    if isinstance(model, torch.jit.TopLevelTracedModule):\n        model = model.forward\n    if isinstance(model, torch.jit.RecursiveScriptModule):\n        model = model.forward\n    if isinstance(model, torch.jit.ScriptFunction):\n        model = model.forward\n    if isinstance(model, torch.jit.RecursiveScriptFunction):\n        model = model.forward\n    if isinstance(model, torch.jit.ScriptMethod):\n        model = model.forward\n    if isinstance(model, torch.jit.RecursiveScriptMethod):\n        model = model.forward\n    if isinstance(model, torch.jit.TopLevelTracedModule):\n        model = model.forward\n    if isinstance(model, torch.jit.TopLevelTracedModule):\n        model = model.forward\n    if isinstance(model, torch.jit.TopLevelInlinedCode):\n        model = model.forward\n    if isinstance(model, torch.jit.TopLevelInlinedCode):\n        model = model.forward\n    if isinstance(model, torch.jit.TopLevelInlinedCode):\n        model = model.forward\n    if isinstance(model, torch.jit.TopLevelInlinedCode):\n        model = model.forward\n    if isinstance(model, torch.jit.TopLevelInlinedCode):\n        model = model.forward\n    if isinstance(model, torch.jit.TopLevelInlinedCode):\n        model = model.forward\n    if isinstance(model, torch.jit.TopLevelInlinedCode):\n        model = model.forward\n    if isinstance(model, torch.jit.TopLevelInlinedCode):\n        model = model.forward\n    if isinstance(model, torch.jit.TopLevelInlinedCode):\n        model = model.forward\n    if isinstance(model, torch.jit.TopLevelInlined", "CodeBleu_score": 21.064884291787116}]}
{"instance_id": "80233cf71580", "generation_result": [{"code": "def generate_individual(input_ranges, individual_length)\n\t\n    individual = []\n    for i in range(individual_length):\n        individual.append(random.uniform(input_ranges[i][0], input_ranges[i][1]))\n    return individual\n\n", "CodeBleu_score": 47.630972577458174}, {"code": "def generate_individual(input_ranges, individual_length)\n\t\n    individual = []\n    for i in range(individual_length):\n        individual.append(random.uniform(input_ranges[i][0], input_ranges[i][1]))\n    return individual\n\n", "CodeBleu_score": 47.630972577458174}, {"code": "def generate_individual(input_ranges, individual_length)\n\t\n    individual = []\n    for i in range(individual_length):\n        individual.append(random.uniform(*input_ranges[i]))\n    return individual\n\n", "CodeBleu_score": 45.41230804837097}]}
{"instance_id": "5ea979cb4a8e", "generation_result": [{"code": "def marginal_log_mean_coeff(self, t)\n\t\n    if self.schedule == \"discrete\":\n        if t < self.T:\n            return torch.log(torch.tensor(self.T - t, dtype=torch.float32))\n        else:\n            return torch.log(torch.tensor(1, dtype=torch.float32))\n    elif self.schedule == \"linear\":\n        return torch.log(torch.tensor(1 + (t / self.T) * (self.T - 1), dtype=torch.float32))\n    elif self.schedule == \"cosine\":\n        return torch.log(torch.tensor(math.cos((t / self.T) * math.pi / 2), dtype=torch.float32))\n    else:\n        raise ValueError(\"Invalid schedule type\")\n\n", "CodeBleu_score": 33.53401501680523}, {"code": "def marginal_log_mean_coeff(self, t)\n\t\n    if self.schedule == \"discrete\":\n        if self.t_start is None:\n            t_start = t\n        else:\n            t_start = self.t_start\n        if self.t_end is None:\n            t_end = t\n        else:\n            t_end = self.t_end\n        alpha_t = torch.log(t_end / t_start) - torch.sum(\n            torch.log1p(\n                torch.exp(torch.arange(math.log(t_start), math.log(t_end), math.log(t_end) - math.log(t_start), device=t.device))\n            )\n        )\n        return alpha_t\n    elif self.schedule == \"linear\":\n        if self.t_start is None:\n            t_start = t\n        else:\n            t_start = self.t_start\n        if self.t_end is None:\n            t_end = t\n        else:\n            t_end = self.t_end\n        alpha_t = 2 * torch.log(t) - torch.log(t_end) - torch.log(t_start)\n        return alpha_t\n    elif self.schedule == \"cosine\":\n        if self.t_start is None:\n            t_start = t\n        else:\n            t_start = self.t_start\n        if self.t_end is None:\n            t_end = t\n        else:\n            t_end = self.t_end\n        alpha_t = 0.5 * torch.log(t_end / t_start) - 0.5 * torch.log(t_end / t_start) * torch.cos(\n            math.pi * (torch.log(t) - torch.log(t_start)) / (torch.log(t_end) - torch.log(t_start))\n        )\n        return alpha_t\n\n", "CodeBleu_score": 33.81993657763756}, {"code": "def marginal_log_mean_coeff(self, t)\n\t\n    if self.schedule == \"discrete\":\n        log_alpha_t = torch.log(torch.tensor(t))\n    elif self.schedule == \"linear\":\n        log_alpha_t = torch.log(torch.tensor(t)) - torch.log(torch.tensor(1 - t))\n    elif self.schedule == \"cosine\":\n        log_alpha_t = torch.log(torch.tensor(0.001 + 0.999 * t)) - torch.log(\n            torch.tensor(0.001 + 0.999 * (1 - t))\n        )\n    else:\n        raise NotImplementedError\n    return log_alpha_t\n\n", "CodeBleu_score": 31.813888456406293}]}
{"instance_id": "7da4fae5a278", "generation_result": [{"code": "def inverse_lambda(self, lamb)\n\t\n    if self.schedule == \"linear\":\n        log_alpha = (self.T - self.t) / self.T * (self.log_alpha_0 - self.log_alpha_T) + self.log_alpha_T\n        log_alpha = log_alpha.clamp(min=self.log_alpha_T)\n        t = (log_alpha - lamb).clamp(min=0.0).sqrt() * self.T\n    elif self.schedule == \"discrete\":\n        t = torch.zeros_like(lamb)\n        for i in range(self.T):\n            log_alpha = self.log_alpha_0 - i * (self.log_alpha_0 - self.log_alpha_T) / self.T\n            t = t + (log_alpha - lamb).clamp(min=0.0).sqrt()\n        t = t * self.T / self.T\n    else:\n        log_alpha = self.log_alpha_0 - (self.log_alpha_0 - self.log_alpha_T) * lamb / (lamb + math.exp(self.log_alpha_0) - 1.0)\n        t = (log_alpha - lamb).clamp(min=0.0).sqrt() * self.T\n    return t\n\n", "CodeBleu_score": 31.117174137377752}, {"code": "def inverse_lambda(self, lamb)\n\t\n    if self.schedule == \"linear\":\n        temp = lamb - self.log_alpha_0\n        t = (self.beta_0 - self.beta_T) * temp / (self.beta_0 * (1 - self.beta_T) + self.beta_T * temp)\n    elif self.schedule == \"discrete\":\n        t = torch.searchsorted(self.log_alpha_t, lamb)\n        t = t.float() / (self.num_steps - 1)\n    else:\n        log_alpha = self.log_alpha_0 + (self.log_alpha_t - self.log_alpha_0) * lamb / (self.log_alpha_t - self.log_alpha_0).max()\n        t = torch.acos(log_alpha.sqrt()) / math.pi\n    return t\n\n", "CodeBleu_score": 25.72096279318663}, {"code": "def inverse_lambda(self, lamb)\n\t\n    if self.schedule == \"linear\":\n        # Compute t\n        beta = lamb * (1 - self.beta_end) + self.beta_end\n        log_alpha = (1 - beta) / (self.beta_end * self.num_diffusion_timesteps)\n        log_alpha_t = log_alpha * self.num_diffusion_timesteps\n        alpha_t = torch.exp(log_alpha_t)\n        t = (1 - alpha_t) / (1 - alpha_t.pow(self.beta_end))\n        return t\n    elif self.schedule == \"discrete\":\n        # Compute t\n        log_alpha_t = self.log_alpha_0 - (self.log_alpha_0 - self.log_alpha_T) * lamb\n        alpha_t = torch.exp(log_alpha_t)\n        t = (1 - alpha_t) / (1 - alpha_t.pow(self.beta_end))\n        return t\n    else:\n        # Compute t\n        log_alpha_t = (\n            self.log_alpha_0\n            - (self.log_alpha_0 - self.log_alpha_T)\n            * lamb\n            + self.log_alpha_T\n        )\n        alpha_t = torch.exp(log_alpha_t)\n        t = (1 - alpha_t) / (1 - alpha_t.pow(self.beta_end))\n        return t\n\n", "CodeBleu_score": 29.290814626266474}]}
{"instance_id": "a519c853a2b2", "generation_result": [{"code": "def noise_pred_fn(x, t_continuous, cond=None)\n\t\n    if t_continuous is not None:\n        t_continuous = t_continuous.expand(x.shape[0], -1)\n    if cond is not None:\n        cond = cond.expand(x.shape[0], -1)\n    if isinstance(x, torch.Tensor):\n        if cond is not None:\n            x = torch.cat([x, cond], dim=1)\n        x = model(x, t_continuous)\n        if model.output_type == \"eps\":\n            x = model.cond_eps_to_eps(x, t_continuous)\n        elif model.output_type == \"v_pred\":\n            x = model.cond_v_pred_to_eps(x, t_continuous)\n        elif model.output_type == \"prev_sample\":\n            x = model.cond_prev_sample_to_eps(x, t_continuous)\n        elif model.output_type == \"prev_x\":\n            x = model.cond_prev_x_to_eps(x, t_continuous)\n        elif model.output_type == \"x_0\":\n            x = model.cond_x_0_to_eps(x, t_continuous)\n        elif model.output_type == \"prev_x_and_eps\":\n            x = model.cond_prev_x_and_eps_to_eps(x, t_continuous)\n        elif model.output_type == \"prev_sample_and_eps\":\n            x = model.cond_prev_sample_and_eps_to_eps(x, t_continuous)\n        elif model.output_type == \"x_0_and_eps\":\n            x = model.cond_x_0_and_eps_to_eps(x, t_continuous)\n        elif model.output_type == \"prev_x_and_x_0\":\n            x = model.cond_prev_x_and_x_0_to_eps(x, t_continuous)\n        elif model.output_type == \"prev_x_and_x_0_and_eps\":\n            x = model.cond_prev_x_and_x_0_and_eps_to_eps(x, t_continuous)\n        elif model.output_type == \"prev_sample_and_x_0\":\n            x = model.cond", "CodeBleu_score": 25.439566362856773}, {"code": "def noise_pred_fn(x, t_continuous, cond=None)\n\t\n    if t_continuous is not None:\n        t_continuous = t_continuous.expand(x.shape[0], -1)\n    if cond is not None:\n        cond = cond.expand(x.shape[0], -1)\n    if model_type == \"unet\":\n        model_out = model(x, t_continuous, cond)\n    elif model_type == \"resnet\":\n        model_out = model(x, t_continuous)\n    if model_type == \"unet\":\n        out = model_out * math.sqrt(torch.exp(model_sigma_log_scale * t_continuous)) + model_mu_log_scale * t_continuous\n    elif model_type == \"resnet\":\n        out = model_out * math.sqrt(torch.exp(model_sigma_log_scale * t_continuous))\n    return out\n\n", "CodeBleu_score": 28.39785043915044}, {"code": "def noise_pred_fn(x, t_continuous, cond=None)\n\t\n    if t_continuous is not None:\n        t_continuous = t_continuous.expand(x.shape[0], -1)\n    if cond is not None:\n        cond = cond.expand(x.shape[0], -1, -1)\n    if model_type == \"unet\":\n        model_out = model(x, t_continuous, cond)\n    elif model_type == \"resunet\":\n        model_out = model(x, t_continuous, cond)\n    elif model_type == \"resunet_3d\":\n        model_out = model(x, t_continuous, cond)\n    elif model_type == \"ddpm\":\n        model_out = model(x, t_continuous, cond)\n    elif model_type == \"ddpm_3d\":\n        model_out = model(x, t_continuous, cond)\n    elif model_type == \"ddpm_3d_2\":\n        model_out = model(x, t_continuous, cond)\n    elif model_type == \"ddpm_3d_3\":\n        model_out = model(x, t_continuous, cond)\n    elif model_type == \"ddpm_3d_4\":\n        model_out = model(x, t_continuous, cond)\n    elif model_type == \"ddpm_3d_5\":\n        model_out = model(x, t_continuous, cond)\n    elif model_type == \"ddpm_3d_6\":\n        model_out = model(x, t_continuous, cond)\n    elif model_type == \"ddpm_3d_7\":\n        model_out = model(x, t_continuous, cond)\n    elif model_type == \"ddpm_3d_8\":\n        model_out = model(x, t_continuous, cond)\n    elif model_type == \"ddpm_3d_9\":\n        model_out = model(x, t_continuous, cond)\n    elif model_type == \"ddpm_3d_10\":\n        model_out = model(x, t_continuous, cond)\n    elif model_type == \"ddpm_3d_11\":\n        model_out = model(x, t_continuous, cond)\n    elif model_type == \"ddpm_3d_12\":\n        model_out = model", "CodeBleu_score": 20.221026185739817}]}
{"instance_id": "96a9b7d54823", "generation_result": [{"code": "def dynamic_thresholding_fn(self, x0, t=None)\n\t\n    if t is None:\n        t = self.t\n    x0 = x0.reshape(x0.shape[0], -1)\n    q = torch.quantile(torch.abs(x0), 1 - t)\n    q = q.reshape(1, -1)\n    q = q.repeat(x0.shape[0], 1)\n    q = torch.max(q, torch.ones_like(q) * self.q_min)\n    x0 = torch.where(torch.abs(x0) < q, x0, torch.sign(x0) * q)\n    x0 = x0 / q\n    return x0\n\n", "CodeBleu_score": 40.12250474050154}, {"code": "def dynamic_thresholding_fn(self, x0, t=None)\n\t\n    if t is None:\n        t = self.t\n    x0 = x0.reshape(x0.shape[0], -1)\n    x0_abs = torch.abs(x0)\n    x0_abs_max = torch.quantile(x0_abs, t, dim=1, keepdim=True)\n    x0_abs_max[x0_abs_max < self.min_value] = self.min_value\n    scale = self.scale / x0_abs_max\n    x0 = torch.clamp(x0, -self.scale, self.scale) * scale\n    return x0\n\n", "CodeBleu_score": 38.44365065695885}, {"code": "def dynamic_thresholding_fn(self, x0, t=None)\n\t\n    # reshape x0 to compute the quantile of absolute values\n    x = x0.reshape(x0.shape[0], -1)\n    x = x.abs()\n    x = x.sort(dim=-1, descending=True)[0]\n    # determine the scaling factor\n    if t is None:\n        t = self.threshold\n    scaling_factor = x[math.ceil(t * x.shape[-1]) - 1]\n    scaling_factor = scaling_factor.unsqueeze(dim=-1).unsqueeze(dim=-1)\n    # clamp x0 values based on the scaling factor\n    x0 = x0.clamp(min=-scaling_factor, max=scaling_factor)\n    # scale the tensor\n    x0 = x0 / scaling_factor\n    return x0\n\n", "CodeBleu_score": 22.1504043260981}]}
{"instance_id": "59c6ec6d7a6a", "generation_result": [{"code": "def data_prediction_fn(self, x, t)\n\t\n    x0 = x.clone()\n    x0 = x0.view(x0.size(0), -1)\n    x0 = x0.detach()\n    x0 = x0.cpu().numpy()\n    x0 = x0.astype(np.float32)\n    x0 = torch.from_numpy(x0)\n    x0 = x0.cuda()\n    x0 = x0.view(x0.size(0), -1)\n    x0 = x0.detach()\n    x0 = x0.cpu().numpy()\n    x0 = x0.astype(np.float32)\n    x0 = torch.from_numpy(x0)\n    x0 = x0.cuda()\n    x0 = x0.view(x0.size(0), -1)\n    x0 = x0.detach()\n    x0 = x0.cpu().numpy()\n    x0 = x0.astype(np.float32)\n    x0 = torch.from_numpy(x0)\n    x0 = x0.cuda()\n    x0 = x0.view(x0.size(0), -1)\n    x0 = x0.detach()\n    x0 = x0.cpu().numpy()\n    x0 = x0.astype(np.float32)\n    x0 = torch.from_numpy(x0)\n    x0 = x0.cuda()\n    x0 = x0.view(x0.size(0), -1)\n    x0 = x0.detach()\n    x0 = x0.cpu().numpy()\n    x0 = x0.astype(np.float32)\n    x0 = torch.from_numpy(x0)\n    x0 = x0.cuda()\n    x0 = x0.view(x0.size(0), -1)\n    x0 = x0.detach()\n    x0 = x0.cpu().numpy()\n    x0 = x0.astype(np.float32)\n    x0 = torch.from_numpy(x0)\n    x0 = x0.cuda()\n    x0 = x0.view(x0.size(0), -1)\n    x0 = x0.detach()\n    x0 = x0.cpu().numpy()\n    x0 = x0.astype(", "CodeBleu_score": 9.527840218735948}, {"code": "def data_prediction_fn(self, x, t)\n\t\n    x0 = x.clone()\n    if self.threshold_ratio > 0:\n        # Calculate the threshold value based on the dynamic thresholding ratio and maximum threshold value\n        threshold = self.threshold_ratio * self.max_threshold_value\n        # Clamp the input tensor values within this threshold\n        x0 = torch.clamp(x0, -threshold, threshold)\n    # Normalize the clamped values\n    x0 = x0 / self.max_threshold_value\n    return x0\n\n", "CodeBleu_score": 13.808448419410013}, {"code": "def data_prediction_fn(self, x, t)\n\t\n    # Calculate the threshold value based on the dynamic thresholding ratio and maximum threshold value\n    threshold = self.threshold * self.max_threshold\n    # Clamp the input tensor values within this threshold\n    x = torch.clamp(x, -threshold, threshold)\n    # Normalize the clamped values\n    x = x / threshold\n    return x\n\n", "CodeBleu_score": 9.354062705541264}]}
{"instance_id": "65e3618468a2", "generation_result": [{"code": "def predict_eps_sigma(model, input, sigma_in, **kwargs)\n\t\n    sigma = sigma_in.reshape(input.shape)\n    input = input.reshape(input.shape)\n    sigma = sigma.reshape(input.shape)\n    pred = model(input)\n    pred = pred + sigma * input\n    return pred\n\n", "CodeBleu_score": 21.199400558560875}, {"code": "def predict_eps_sigma(model, input, sigma_in, **kwargs)\n\t\n    sigma_in = sigma_in.reshape(input.shape)\n    scaled_input = input / (sigma_in + 1e-10)\n    pred = model(scaled_input)\n    pred = pred * sigma_in + input\n    return pred\n\n", "CodeBleu_score": 19.83990372620424}, {"code": "def predict_eps_sigma(model, input, sigma_in, **kwargs)\n\t\n    # Reshape sigma to match the dimensions of the input\n    sigma = sigma_in.reshape(input.shape)\n    # Scale the input by the standard deviation\n    scaled_input = input * sigma\n    # Compute the prediction by adjusting the model output with the scaled input and sigma\n    prediction = model(scaled_input) + sigma\n    return prediction\n\n", "CodeBleu_score": 19.707655865936193}]}
{"instance_id": "b35846dfbc66", "generation_result": [{"code": "def cal_intergrand(beta_0, beta_1, taus)\n\t\n    taus = taus.clone().detach().requires_grad_(True)\n    alpha = torch.exp(-beta_0 * taus) * torch.exp(-beta_1 * taus ** 2)\n    log_alpha = torch.log(alpha)\n    log_alpha.backward(torch.ones_like(log_alpha), retain_graph=True)\n    integrand = alpha * taus.grad\n    return integrand\n\n", "CodeBleu_score": 23.376259066152873}, {"code": "def cal_intergrand(beta_0, beta_1, taus)\n\t\n    # clone and set the required gradients for taus\n    taus = taus.clone().requires_grad_()\n    # calculate alpha and its logarithm\n    alpha = torch.exp(torch.sum(taus, dim=1))\n    log_alpha = torch.log(alpha)\n    # perform backpropagation to obtain the gradient of log_alpha with respect to taus\n    log_alpha.backward(torch.ones_like(log_alpha))\n    # calculate the integrand\n    integrand = torch.exp(-beta_0 * alpha - beta_1 * log_alpha) * taus.grad\n    # return the integrand\n    return integrand\n\n", "CodeBleu_score": 20.76093768052622}, {"code": "def cal_intergrand(beta_0, beta_1, taus)\n\t\n    alpha = torch.exp(beta_0 + beta_1 * taus)\n    log_alpha = torch.log(alpha)\n    log_alpha.backward(gradient=torch.ones_like(log_alpha), retain_graph=True)\n    d_log_alpha_d_taus = taus.grad\n    taus.grad.zero_()\n    return alpha * d_log_alpha_d_taus\n\n", "CodeBleu_score": 20.875999591154777}]}
{"instance_id": "5f3667566a10", "generation_result": [{"code": "def sample_euler_ancestral(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None)\n\t\n    if extra_args is None:\n        extra_args = model.init_extra_args(x)\n    if noise_sampler is None:\n        noise_sampler = model.init_noise_sampler(x)\n    if disable is None:\n        disable = {}\n    if callback is not None:\n        callback(x, extra_args)\n    for sigma in sigmas:\n        x = model.denoise(x, extra_args, disable=disable)\n        x = x + eta * (model.ancestral_step(x, extra_args, disable=disable) - x)\n        if callback is not None:\n            callback(x, extra_args)\n    if s_noise > 0:\n        x = x + s_noise * noise_sampler()\n    return x\n\n", "CodeBleu_score": 29.122100712083597}, {"code": "def sample_euler_ancestral(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None)\n\t\n    if extra_args is None:\n        extra_args = model.init_extra_args(x)\n    if noise_sampler is None:\n        noise_sampler = model.init_noise_sampler(x)\n    for i, sigma in enumerate(sigmas):\n        if disable is not None and i in disable:\n            continue\n        x = model.denoise(x, extra_args, sigma)\n        x = x + eta * (model.ancestral_step(x, extra_args, sigma) - x)\n        if callback is not None:\n            callback(x, i)\n    if s_noise > 0:\n        x = x + s_noise * noise_sampler(x)\n    return x\n\n", "CodeBleu_score": 27.56178828413483}, {"code": "def sample_euler_ancestral(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None)\n\t\n    if extra_args is None:\n        extra_args = model.init_extra_args(x)\n    if noise_sampler is None:\n        noise_sampler = model.init_noise_sampler(x)\n    for sigma in tqdm(sigmas, desc='Ancestral sampling', disable=disable):\n        x = model.denoise(x, sigma, extra_args=extra_args, eta=eta, s_noise=s_noise)\n        delta = model.ancestral_step(x, sigma, extra_args=extra_args)\n        x = x + delta\n        if callback is not None:\n            callback(x)\n    x = x + noise_sampler(s_noise)\n    return x\n\n", "CodeBleu_score": 30.70902408365183}]}
{"instance_id": "6261ded88df2", "generation_result": [{"code": "def linear_multistep_coeff(order, t, i, j)\n\t\n    def product_term(k):\n        if k == j:\n            return 1\n        return (t[i+1] - t[k]) / (t[j] - t[k])\n    if order > len(t) - 1:\n        raise ValueError(\"Order is too high for the number of steps.\")\n    return integrate.quad(product_term, t[0], t[i])[0]\n\n", "CodeBleu_score": 25.012053652177098}, {"code": "def linear_multistep_coeff(order, t, i, j)\n\t\n    if order > i + 1:\n        raise ValueError(f\"Order {order} is too high for step {i}\")\n\n    def product_term(k):\n        \"\"\"The function calculates the product term for the linear multistep coefficient, excluding the j-th term.", "CodeBleu_score": 19.37979225247461}, {"code": "def linear_multistep_coeff(order, t, i, j)\n\t\n    if order > i:\n        raise ValueError(\"Order cannot be higher than the step index\")\n\n    def product_term(k):\n        if k == j:\n            return 1\n        else:\n            return (t[i] - t[k]) / (t[j] - t[k])\n\n    return integrate.quad(product_term, t[i], t[i + 1])[0]\n\n", "CodeBleu_score": 26.508412160141003}]}
{"instance_id": "4ee025e33ab1", "generation_result": [{"code": "def sample_lcm(model, x, sigmas, extra_args=None, callback=None, disable=None, noise_sampler=None)\n\t\n    if extra_args is None:\n        extra_args = model.get_extra_args(x)\n    if noise_sampler is None:\n        noise_sampler = model.get_noise_sampler(x)\n    for sigma in tqdm(sigmas, disable=disable):\n        if callback is not None:\n            callback(x)\n        x = model(x, extra_args)\n        if sigma > 0:\n            x = x + sigma * noise_sampler()\n    return x\n\n", "CodeBleu_score": 25.378216243245667}, {"code": "def sample_lcm(model, x, sigmas, extra_args=None, callback=None, disable=None, noise_sampler=None)\n\t\n    if extra_args is None:\n        extra_args = model.get_extra_args(x)\n    if noise_sampler is None:\n        noise_sampler = model.get_noise_sampler(x)\n    for sigma in tqdm(sigmas, desc=\"LCM sampling\", leave=False):\n        x = model(x, sigma, extra_args=extra_args, noise_sampler=noise_sampler)\n        if callback is not None:\n            callback(x)\n        if sigma > 0:\n            x = x + sigma * noise_sampler(x)\n    return x\n\n", "CodeBleu_score": 29.765041318859186}, {"code": "def sample_lcm(model, x, sigmas, extra_args=None, callback=None, disable=None, noise_sampler=None)\n\t\n    if extra_args is None:\n        extra_args = model.get_extra_args()\n    if noise_sampler is None:\n        noise_sampler = model.get_noise_sampler()\n    x_prev = x.clone()\n    for sigma in tqdm(sigmas):\n        if callback is not None:\n            callback(x_prev, sigma)\n        if sigma > 0:\n            x_prev = x_prev + sigma * noise_sampler(x_prev)\n        x_prev = model(x_prev, sigma, *extra_args)\n    return x_prev\n\n", "CodeBleu_score": 26.14450289527916}]}
{"instance_id": "cb51f91da606", "generation_result": [{"code": "def append_dims(x, target_dims)\n\t    if len(x.shape) >= target_dims:\n        raise ValueError(f\"The tensor already has {len(x.shape)} dimensions. It cannot be expanded to {target_dims} dimensions.\")\n    num_dims_to_add = target_dims - len(x.shape)\n    return x.expand(*x.shape, *((1,) * num_dims_to_add)).detach() if x.device.type == \"mps\" else x.expand(*x.shape, *((1,) * num_dims_to_add))\n\n", "CodeBleu_score": 34.422900726820835}, {"code": "def append_dims(x, target_dims)\n\t    n_dims_to_append = target_dims - x.ndim\n    if n_dims_to_append < 0:\n        raise ValueError(\n            f\"The target dimensions must be greater than or equal to the current dimensions, but got target_dims={target_dims} and x.ndim={x.ndim}.\"\n        )\n    if n_dims_to_append == 0:\n        return x\n    return x.expand(*x.shape, *((1,) * n_dims_to_append)).detach().clone()\n\n", "CodeBleu_score": 27.329067196778965}, {"code": "def append_dims(x, target_dims)\n\t    if len(x.shape) > target_dims:\n        raise ValueError(\n            f\"The number of dimensions of the input tensor ({len(x.shape)}) is greater than the target dimensions ({target_dims}).\"\n        )\n    return x.expand(*x.shape, *((1,) * (target_dims - len(x.shape))))\n\n", "CodeBleu_score": 20.733268642416697}]}
{"instance_id": "9fac874a3bc7", "generation_result": [{"code": "def download_file(path, url, digest=None)\n\t\n    if path.is_file():\n        print(f\"Using cached file {path}\")\n        return path\n    path.parent.mkdir(parents=True, exist_ok=True)\n    print(f\"Downloading {path} from {url}\")\n    urllib.request.urlretrieve(url, path)\n    if digest is not None:\n        with open(path, \"rb\") as f:\n            actual_digest = hashlib.sha256(f.read()).hexdigest()\n        if actual_digest!= digest:\n            raise RuntimeError(\n                f\"File {path} has wrong hash. Expected {digest}, got {actual_digest}\"\n            )\n    return path\n\n", "CodeBleu_score": 46.83028397588094}, {"code": "def download_file(path, url, digest=None)\n\t\n    if not Path(path).exists():\n        Path(path).parent.mkdir(parents=True, exist_ok=True)\n        urllib.request.urlretrieve(url, path)\n    if digest is not None:\n        with open(path, \"rb\") as f:\n            file_hash = hashlib.sha256(f.read()).hexdigest()\n        if file_hash!= digest:\n            raise ValueError(\n                f\"File {path} has incorrect hash. Expected {digest}, got {file_hash}.\"\n            )\n    return path\n\n", "CodeBleu_score": 48.7681778226271}, {"code": "def download_file(path, url, digest=None)\n\t\n    if path.is_file():\n        print(f\"{path} already exists\")\n        return path\n    path.parent.mkdir(parents=True, exist_ok=True)\n    print(f\"Downloading {path} from {url}\")\n    urllib.request.urlretrieve(url, path)\n    if digest is not None:\n        print(f\"Verifying {path}\")\n        with path.open(\"rb\") as f:\n            if hashlib.sha256(f.read()).hexdigest()!= digest:\n                raise RuntimeError(f\"{path} has an invalid hash\")\n    return path\n\n", "CodeBleu_score": 35.0516418352288}]}
{"instance_id": "f23165e897ce", "generation_result": [{"code": "def train_mode(model, mode=True)\n\t\n    if not isinstance(model, nn.Module):\n        raise TypeError(\"model must be an instance of nn.Module\")\n    if not isinstance(mode, bool):\n        raise TypeError(\"mode must be a boolean value\")\n    current_modes = {\n        module: module.training for module in model.modules()\n    }  # save current training modes of all modules\n    model.train(mode)  # set model to specified mode\n    try:\n        yield\n    finally:\n        for module, mode in current_modes.items():\n            module.train(mode)  # restore original modes\n\n", "CodeBleu_score": 28.427638991121608}, {"code": "def train_mode(model, mode=True)\n\t\n    if not isinstance(model, nn.Module):\n        raise TypeError(\"model must be a nn.Module\")\n    if not isinstance(mode, bool):\n        raise TypeError(\"mode must be a bool\")\n    original_modes = {}\n    for name, module in model.named_modules():\n        original_modes[name] = module.training\n    try:\n        for name, module in model.named_modules():\n            module.train(mode)\n        yield\n    finally:\n        for name, module in model.named_modules():\n            module.train(original_modes[name])\n\n", "CodeBleu_score": 29.44777280211205}, {"code": "def train_mode(model, mode=True)\n\t\n    if not isinstance(model, nn.Module):\n        raise TypeError(\"model must be an instance of nn.Module\")\n    if not isinstance(mode, bool):\n        raise TypeError(\"mode must be a boolean\")\n    original_modes = {}\n    for name, module in model.named_modules():\n        original_modes[name] = module.training\n    try:\n        model.train(mode)\n        yield\n    finally:\n        for name, module in model.named_modules():\n            module.train(original_modes[name])\n\n", "CodeBleu_score": 29.462480972707805}]}
{"instance_id": "c6d59c6cb8e1", "generation_result": [{"code": "def ema_update(model, averaged_model, decay)\n\t\n    for param, avg_param in zip(model.parameters(), averaged_model.parameters()):\n        avg_param.data.mul_(decay).add_(param.data, alpha=1 - decay)\n    for buffer, avg_buffer in zip(model.buffers(), averaged_model.buffers()):\n        avg_buffer.data.copy_(buffer.data)\n\n", "CodeBleu_score": 27.49313936795724}, {"code": "def ema_update(model, averaged_model, decay)\n\t\n    for current_params, averaged_params in zip(model.parameters(), averaged_model.parameters()):\n        averaged_params.data.mul_(decay).add_(current_params.data, alpha=1 - decay)\n    for current_buffer, averaged_buffer in zip(model.buffers(), averaged_model.buffers()):\n        averaged_buffer.copy_(current_buffer)\n\n", "CodeBleu_score": 30.275472504593264}, {"code": "def ema_update(model, averaged_model, decay)\n\t\n    with torch.no_grad():\n        for current_params, averaged_params in zip(model.parameters(), averaged_model.parameters()):\n            averaged_params.data.mul_(decay).add_(current_params.data, alpha=1 - decay)\n        for current_buffer, averaged_buffer in zip(model.buffers(), averaged_model.buffers()):\n            averaged_buffer.data.mul_(decay).add_(current_buffer.data, alpha=1 - decay)\n\n", "CodeBleu_score": 33.63067930793508}]}
{"instance_id": "600c39a30798", "generation_result": [{"code": "def rand_log_logistic(shape, loc=0., scale=1., min_value=0., max_value=float('inf'), device='cpu', dtype=torch.float32)\n\t\n    min_value = torch.tensor(min_value, device=device, dtype=dtype)\n    max_value = torch.tensor(max_value, device=device, dtype=dtype)\n    min_cdf = torch.sigmoid(torch.log(min_value - loc) / scale)\n    max_cdf = torch.sigmoid(torch.log(max_value - loc) / scale)\n    uniform_samples = torch.rand(shape, device=device, dtype=dtype) * (max_cdf - min_cdf) + min_cdf\n    return torch.exp(torch.log(uniform_samples) * scale + loc)\n\n", "CodeBleu_score": 44.04489952752107}, {"code": "def rand_log_logistic(shape, loc=0., scale=1., min_value=0., max_value=float('inf'), device='cpu', dtype=torch.float32)\n\t\n    min_value = torch.as_tensor(min_value, dtype=dtype, device=device)\n    max_value = torch.as_tensor(max_value, dtype=dtype, device=device)\n    u = torch.rand(shape, dtype=dtype, device=device)\n    u = u * (torch.log(max_value) - torch.log(min_value)) + torch.log(min_value)\n    return (torch.exp(u) - loc) / scale\n\n", "CodeBleu_score": 29.199715706404326}, {"code": "def rand_log_logistic(shape, loc=0., scale=1., min_value=0., max_value=float('inf'), device='cpu', dtype=torch.float32)\n\t\n    min_value = torch.tensor(min_value, device=device, dtype=dtype)\n    max_value = torch.tensor(max_value, device=device, dtype=dtype)\n    min_cdf = torch.sigmoid(torch.log(min_value - loc) / scale)\n    max_cdf = torch.sigmoid(torch.log(max_value - loc) / scale)\n    u = torch.rand(shape, device=device, dtype=dtype) * (max_cdf - min_cdf) + min_cdf\n    return loc + scale * torch.log(u / (1 - u))\n\n", "CodeBleu_score": 42.699876743454034}]}
{"instance_id": "75e2def2467f", "generation_result": [{"code": "def get_activation(activation: Literal[\"elu\", \"snake\", \"none\"], antialias=False, channels=None) -> nn.Module\n\t\n    if activation == \"elu\":\n        return nn.ELU()\n    elif activation == \"snake\":\n        return SnakeBeta(channels)\n    elif activation == \"none\":\n        return nn.Identity()\n    else:\n        raise ValueError(f\"Invalid activation type: {activation}\")\n\n", "CodeBleu_score": 42.608873649955676}, {"code": "def get_activation(activation: Literal[\"elu\", \"snake\", \"none\"], antialias=False, channels=None) -> nn.Module\n\t\n    if activation == \"elu\":\n        return nn.ELU(inplace=True)\n    elif activation == \"snake\":\n        return SnakeBeta(channels)\n    elif activation == \"none\":\n        return nn.Identity()\n    else:\n        raise ValueError(f\"Unknown activation: {activation}\")\n\n    if antialias:\n        return Activation1d(activation)\n\n", "CodeBleu_score": 47.98827453440577}, {"code": "def get_activation(activation: Literal[\"elu\", \"snake\", \"none\"], antialias=False, channels=None) -> nn.Module\n\t\n    if activation == \"elu\":\n        return nn.ELU(inplace=True)\n    elif activation == \"snake\":\n        return SnakeBeta(channels)\n    elif activation == \"none\":\n        return nn.Identity()\n    else:\n        raise ValueError(\"Unknown activation type\")\n\n", "CodeBleu_score": 39.715889141190104}]}
{"instance_id": "ef1714c79e51", "generation_result": [{"code": "def apply_rotary_pos_emb(t, freqs, scale = 1)\n\t\n    t = t.to(freqs.dtype)\n    t = t.reshape(t.shape[0], t.shape[1], -1, 2)\n    freqs = freqs.reshape(1, 1, -1, 2)\n    sin, cos = torch.sin(freqs * scale), torch.cos(freqs * scale)\n    return torch.cat([t[..., 0] + sin, t[..., 1] + cos, t[..., 2], t[..., 3]], dim=-1)\n\n", "CodeBleu_score": 15.433183743664458}, {"code": "def apply_rotary_pos_emb(t, freqs, scale = 1)\n\t\n    if t.dtype == torch.float16:\n        t = t.float()\n    assert t.ndim == 3, \"Only supports 3-dimensional tensors\"\n    assert t.shape[-1] % 2 == 0, \"Only supports even-sized tensors\"\n    assert freqs.shape == (2, t.shape[-1] // 2), \"Frequency tensor must have shape (2, t.shape[-1] // 2)\"\n    t_rot = torch.einsum(\"...d,ld->...l\", t, freqs)\n    t = torch.stack((-t_rot[..., 1], t_rot[..., 0]), dim=-1)\n    return torch.cat((t, t_rot), dim=-1)\n\n", "CodeBleu_score": 26.414706155800467}, {"code": "def apply_rotary_pos_emb(t, freqs, scale = 1)\n\t\n    t = t.to(torch.float32) if t.dtype not in [torch.float32, torch.float16] else t\n    t, freqs = map(lambda t: rearrange(t, '... d ->... d 1'), (t, freqs))\n    freqs_sin, freqs_cos = map(lambda t: scale * t.to(t.dtype), (torch.sin(freqs), torch.cos(freqs)))\n    t_rotated = torch.stack((-freqs_sin, freqs_cos), dim=-1) * t\n    return torch.cat((t, t_rotated), dim=-1)\n\n", "CodeBleu_score": 21.84949820948529}]}
{"instance_id": "a7a5f178c6f6", "generation_result": [{"code": "def idx2vq(self, idx, dim=-1)\n\t\n    return self.vq_codebook.weight.index_select(dim, idx).view_as(idx)\n\n", "CodeBleu_score": 19.33269865504602}, {"code": "def idx2vq(self, idx, dim=-1)\n\t\n    idx = idx.unsqueeze(dim)\n    return self.embedding(idx).squeeze(dim)\n\n", "CodeBleu_score": 20.82034742224402}, {"code": "def idx2vq(self, idx, dim=-1)\n\t\n    return self.codebook(idx).transpose(dim, -1)\n\n", "CodeBleu_score": 16.840795329970103}]}
{"instance_id": "4104c041fb20", "generation_result": [{"code": "def encode(self, x, quantize=False)\n\t\n    if self.use_conv:\n        x = self.conv_1(x)\n        x = self.conv_2(x)\n        x = self.conv_3(x)\n    for i in range(len(self.blocks)):\n        x = self.blocks[i](x)\n    if quantize:\n        x_q, x_q_loss, x_q_indices = self.quantize(x)\n        return x_q, x, x_q_loss, x_q_indices\n    else:\n        return x\n\n", "CodeBleu_score": 42.07650865671233}, {"code": "def encode(self, x, quantize=False)\n\t\n    if self.use_quantization:\n        z, indices, combined_loss = self.quantizer(x)\n        return z, indices, combined_loss\n    else:\n        z = self.encoder(x)\n        return z\n\n", "CodeBleu_score": 31.113484235671656}, {"code": "def encode(self, x, quantize=False)\n\t\n    if quantize:\n        z, indices, loss = self.quantize(self.encode(x))\n        return z, indices, loss\n    else:\n        return self.encode(x)\n\n", "CodeBleu_score": 26.386830429922604}]}
{"instance_id": "403c52ba2d3b", "generation_result": [{"code": "def ema_scope(self, context=None)\n\t\n    if self.ema_config is None:\n        yield\n        return\n    if context is None:\n        context = {}\n    if \"ema_scope\" not in context:\n        context[\"ema_scope\"] = 0\n    if context[\"ema_scope\"] == 0:\n        self.log(f\"Switching to EMA weights\")\n    context[\"ema_scope\"] += 1\n    self.switch_to_ema_weights()\n    yield\n    context[\"ema_scope\"] -= 1\n    if context[\"ema_scope\"] == 0:\n        self.log(f\"Restoring original weights\")\n    self.switch_to_original_weights()\n\n", "CodeBleu_score": 38.10895836617338}, {"code": "def ema_scope(self, context=None)\n\t\n    if not self.ema_decay:\n        yield\n        return\n    if context is None:\n        context = {}\n    self.store_parameters(context)\n    self.load_ema_parameters()\n    if self.ema_log:\n        self.log(f\"Loading EMA weights\", context)\n    yield\n    self.load_parameters(context)\n    if self.ema_log:\n        self.log(f\"Restoring original weights\", context)\n\n", "CodeBleu_score": 41.25443845482758}, {"code": "def ema_scope(self, context=None)\n\t\n    if context is None:\n        context = {}\n    if self.ema_decay > 0.0:\n        if self.ema_weights is None:\n            self.ema_weights = LitEma(self.parameters(), self.ema_decay)\n        self.ema_weights.store(self.parameters())\n        self.load_state_dict(self.ema_weights.state_dict())\n        if \"ema_applied\" not in context:\n            context[\"ema_applied\"] = True\n            self.logger.info(\"EMA weights applied\")\n        else:\n            self.logger.info(\"EMA weights restored\")\n    yield\n    if self.ema_decay > 0.0:\n        self.load_state_dict(self.ema_weights.state_dict())\n        if \"ema_applied\" in context:\n            del context[\"ema_applied\"]\n            self.logger.info(\"EMA weights restored\")\n\n", "CodeBleu_score": 37.34234610578558}]}
{"instance_id": "94b7467f98e4", "generation_result": [{"code": "def post_attention(self, attn, x, gate_msa, shift_mlp, scale_mlp, gate_mlp)\n\t\n    x = x + attn * gate_msa\n    x = self.norm(x)\n    x = self.mlp(x, shift_mlp, scale_mlp) * gate_mlp\n    return x\n\n", "CodeBleu_score": 33.383503597135324}, {"code": "def post_attention(self, attn, x, gate_msa, shift_mlp, scale_mlp, gate_mlp)\n\t\n    x = x + attn * gate_msa\n    x = self.norm(x)\n    x = x + shift_mlp(x) * scale_mlp(x) * gate_mlp(x)\n    return x\n\n", "CodeBleu_score": 34.16230683299353}, {"code": "def post_attention(self, attn, x, gate_msa, shift_mlp, scale_mlp, gate_mlp)\n\t\n    x = x + attn * gate_msa\n    x = self.norm(x)\n    x = x + shift_mlp(x) * scale_mlp(x) * gate_mlp\n    return x\n\n", "CodeBleu_score": 33.39284552246268}]}
{"instance_id": "d9fdc05542a2", "generation_result": [{"code": "def _block_mixing(context, x, context_block, x_block, c)\n\t\n    # Pre-attention block\n    qkv = context_block(context)\n    qkv = rearrange(qkv, \"b (qkv c) h w -> qkv b c h w\", qkv=3)\n    q, k, v = qkv\n\n    # Optimized attention\n    q = rearrange(q, \"b c h w -> b (h w) c\")\n    k = rearrange(k, \"b c h w -> b c (h w)\")\n    v = rearrange(v, \"b c h w -> b (h w) c\")\n    attention = torch.einsum(\"b i d, b d j -> b i j\", q, k) * c\n    attention = attention.softmax(dim=-1)\n    attention = torch.einsum(\"b i j, b j d -> b i d\", attention, v)\n    attention = rearrange(attention, \"b (h w) c -> b c h w\", h=q.shape[1])\n\n    # Post-attention block\n    if context_block.post_attention_block:\n        context = context_block.post_attention_block(context, attention)\n\n    # Pre-attention block\n    qkv = x_block(x)\n    qkv = rearrange(qkv, \"b (qkv c) h w -> qkv b c h w\", qkv=3)\n    q, k, v = qkv\n\n    # Optimized attention\n    q = rearrange(q, \"b c h w -> b (h w) c\")\n    k = rearrange(k, \"b c h w -> b c (h w)\")\n    v = rearrange(v, \"b c h w -> b (h w) c\")\n    attention = torch.einsum(\"b i d, b d j -> b i j\", q, k) * c\n    attention = attention.softmax(dim=-1)\n    attention = torch.einsum(\"b i j, b j d -> b i d\", attention, v)\n    attention = rearrange(attention, \"b (h w) c -> b c h w\", h=q.shape[1])\n\n    # Post-attention block\n    if x_block.post_attention_block:\n        x = x_block.post_attention_block(x, attention)\n\n    return context, x\n\n", "CodeBleu_score": 29.79017150886899}, {"code": "def _block_mixing(context, x, context_block, x_block, c)\n\t\n    context, x = context_block(context, x)\n    q, k, v = context_block.pre_attention(context, x)\n    q = q.chunk(c, dim=-1)\n    k = k.chunk(c, dim=-1)\n    v = v.chunk(c, dim=-1)\n    qkv = torch.cat([q, k, v], dim=-1)\n    qkv = x_block.attention(qkv)\n    q, k, v = qkv.chunk(3, dim=-1)\n    context = context_block.post_attention(q, k, v, context, x)\n    x = x_block.post_attention(q, k, v, context, x)\n    return context, x\n\n", "CodeBleu_score": 14.94778122208604}, {"code": "def _block_mixing(context, x, context_block, x_block, c)\n\t\n    qkv_context = context_block.qkv(context)\n    qkv_x = x_block.qkv(x)\n    qkv = torch.cat([qkv_context, qkv_x], dim=1)\n    qkv = context_block.split_heads(qkv)\n    qkv = context_block.attend(qkv, c)\n    qkv = context_block.merge_heads(qkv)\n    qkv_context, qkv_x = qkv.chunk(2, dim=1)\n    context = context_block.proj(qkv_context)\n    x = x_block.proj(qkv_x)\n    return context, x\n\n", "CodeBleu_score": 13.000936580662154}]}
{"instance_id": "b219bce30160", "generation_result": [{"code": "def get_timestep_embedding(timesteps, embedding_dim)\n\t\n    assert embedding_dim % 2 == 0, \"embedding_dim must be divisible by 2\"\n    half_dim = embedding_dim // 2\n    emb = math.log(10000) / (half_dim - 1)\n    emb = torch.exp(torch.arange(half_dim, dtype=torch.float32) * -emb)\n    emb = timesteps[:, None] * emb[None, :]\n    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n    if embedding_dim % 2 == 1:\n        emb = torch.cat([emb, torch.zeros(emb.shape[0], 1, dtype=torch.float32)], dim=1)\n    return emb\n\n", "CodeBleu_score": 70.26178534600506}, {"code": "def get_timestep_embedding(timesteps, embedding_dim)\n\t\n    assert embedding_dim % 2 == 0, \"embedding_dim must be divisible by 2\"\n    half_dim = embedding_dim // 2\n    emb = math.log(10000) / (half_dim - 1)\n    emb = torch.exp(torch.arange(half_dim, dtype=torch.float32) * -emb)\n    emb = timesteps[:, None] * emb[None, :]\n    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n    if embedding_dim % 2 == 1:\n        emb = torch.nn.functional.pad(emb, (0, 1))\n    return emb\n\n", "CodeBleu_score": 71.98126496377103}, {"code": "def get_timestep_embedding(timesteps, embedding_dim)\n\t\n    timesteps = timesteps.reshape(-1)\n    half_dim = embedding_dim // 2\n    emb = math.log(10000) / (half_dim - 1)\n    emb = torch.exp(torch.arange(half_dim, dtype=timesteps.dtype) * -emb)\n    emb = timesteps.float()[:, None] * emb[None, :]\n    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n    if embedding_dim % 2 == 1:\n        emb = torch.cat([emb, torch.zeros(emb.shape[0], 1, dtype=emb.dtype, device=emb.device)], dim=1)\n    return emb\n\n", "CodeBleu_score": 74.6539948276431}]}
{"instance_id": "6ae8453c86be", "generation_result": [{"code": "def pytorch_attention(q, k, v)\n\t\n    try:\n        return torch.nn.functional.scaled_dot_product_attention(q, k, v, None, dropout_p=0)\n    except RuntimeError as e:\n        if \"out of memory\" in str(e):\n            logging.info(\"OOM error occurred. Switching to slice attention.\")\n            return slice_attention(q, k, v)\n        else:\n            raise e\n\n", "CodeBleu_score": 15.591969710742173}, {"code": "def pytorch_attention(q, k, v)\n\t\n    try:\n        # reshape q, k, v for multihead attention\n        q = q.transpose(1, 2).contiguous().view(q.shape[0], q.shape[2], q.shape[1] * q.shape[3])\n        k = k.transpose(1, 2).contiguous().view(k.shape[0], k.shape[2], k.shape[1] * k.shape[3])\n        v = v.transpose(1, 2).contiguous().view(v.shape[0], v.shape[2], v.shape[1] * v.shape[3])\n\n        # scaled dot-product attention\n        attn_output = torch.bmm(q, k.transpose(1, 2))\n        attn_output = attn_output / math.sqrt(k.shape[1])\n        attn_output = torch.nn.functional.softmax(attn_output, dim=-1)\n        attn_output = torch.bmm(attn_output, v)\n\n        # reshape attn_output back to the original dimensions\n        attn_output = attn_output.view(attn_output.shape[0], attn_output.shape[2], q.shape[1], q.shape[3])\n        attn_output = attn_output.transpose(1, 2).contiguous()\n    except RuntimeError as e:\n        if 'out of memory' in str(e):\n            logging.warning(f\"OOM exception occurred. Switching to slice attention.\")\n            attn_output = slice_attention(q, k, v)\n        else:\n            raise e\n\n    return attn_output\n\n", "CodeBleu_score": 33.015176924057954}, {"code": "def pytorch_attention(q, k, v)\n\t\n    try:\n        return torch.nn.functional.scaled_dot_product_attention(q, k, v, None, dropout_p=0)\n    except RuntimeError as e:\n        if \"out of memory\" in str(e):\n            logging.warning(\"Attention out of memory. Switching to slice attention.\")\n            return slice_attention(q, k, v)\n        else:\n            raise e\n\n", "CodeBleu_score": 16.26014807037634}]}
{"instance_id": "fe4433c987ad", "generation_result": [{"code": "def apply_control(h, control, name)\n\t\n    if exists(control) and exists(name):\n        control = control.pop(name, None)\n        if exists(control):\n            try:\n                h += control\n            except:\n                logging.warning(f\"Failed to apply control {name} to h.\")\n    return h\n\n", "CodeBleu_score": 40.759452883743684}, {"code": "def apply_control(h, control, name)\n\t\n    if exists(control) and name in control:\n        try:\n            h = h + control[name]\n        except Exception as e:\n            logging.warning(f\"Failed to add control {name} to h. Error: {e}\")\n    return h\n\n", "CodeBleu_score": 32.959710621166494}, {"code": "def apply_control(h, control, name)\n\t\n    if exists(control) and exists(name):\n        control = control.pop(name, None)\n        if exists(control):\n            try:\n                h = h + control\n            except:\n                logging.warning(f\"Could not apply control {name}.\")\n    return h\n\n", "CodeBleu_score": 38.53994449499104}]}
{"instance_id": "56da4547c101", "generation_result": [{"code": "def get_codebook_entry(self, indices, shape)\n\t\n    one_hot = F.one_hot(indices, num_classes=self.num_embeddings)\n    quantized = torch.matmul(one_hot, self.weight)\n    if shape is not None:\n        quantized = quantized.permute(0, 2, 1).contiguous().view(shape)\n    return quantized\n\n", "CodeBleu_score": 32.366259210432176}, {"code": "def get_codebook_entry(self, indices, shape)\n\t\n    # Create a one-hot encoding of the indices\n    one_hot = F.one_hot(indices, num_classes=self.num_embeddings).float()\n    # Multiply the one-hot encoding with the embedding weights to get the quantized vectors\n    quantized = F.embedding(\n        indices, self.weight.detach()\n    )  #.detach() is added to avoid backprop through the embedding layer\n    # If a shape is provided, reshape and permute the quantized vectors to match the original input shape\n    if shape is not None:\n        quantized = quantized.view(shape + (self.embedding_dim,))\n        quantized = quantized.permute(0, *(range(-1, -len(shape) - 1, -1)), -1)\n    return quantized\n\n", "CodeBleu_score": 37.31637350656782}, {"code": "def get_codebook_entry(self, indices, shape)\n\t\n    indices = indices.view(-1)\n    if self.codebook_shape is None:\n        codebook_shape = self.codebook_shape\n    else:\n        codebook_shape = self.codebook_shape\n    if shape is not None:\n        codebook_shape = codebook_shape[1:]\n        codebook_shape = codebook_shape[::-1]\n        codebook_shape = shape[1:] + codebook_shape\n        codebook_shape = tuple(codebook_shape)\n    else:\n        codebook_shape = codebook_shape[1:]\n    indices = F.one_hot(indices, num_classes=self.codebook_shape[0]).float()\n    codebook_vectors = self.codebook_vectors.weight\n    codebook_vectors = codebook_vectors.unsqueeze(0)\n    codebook_vectors = codebook_vectors.expand(indices.shape[0], -1, -1)\n    codebook_vectors = codebook_vectors.permute(0, 2, 1)\n    codebook_vectors = torch.matmul(codebook_vectors, indices)\n    codebook_vectors = codebook_vectors.permute(0, 2, 1)\n    codebook_vectors = codebook_vectors.view(-1, *codebook_shape)\n    return codebook_vectors\n\n", "CodeBleu_score": 38.68885877233061}]}
{"instance_id": "548f8a1fb37d", "generation_result": [{"code": "def make_noise(self)\n\t\n    log_size = int(math.log(self.out_size, 2))\n    noises = [torch.randn(1, 1, 4, 4, device=self.device)]\n\n    for size in 2 ** torch.arange(3, log_size + 1):\n        noises.append(torch.randn(1, 1, size, size, device=self.device))\n        noises.append(torch.randn(1, 1, size, size, device=self.device))\n\n    return noises\n\n", "CodeBleu_score": 31.057533696576467}, {"code": "def make_noise(self)\n\t\n    log_size = int(math.log2(self.out_size))\n    noises = [torch.randn(1, 1, 4, 4, device=self.device)]\n\n    for size in 2 ** torch.arange(3, log_size + 1):\n        noises.append(torch.randn(1, 1, size, size, device=self.device))\n        noises.append(torch.randn(1, 1, size, size, device=self.device))\n\n    return noises\n\n", "CodeBleu_score": 31.0816244868469}, {"code": "def make_noise(self)\n\t\n    log_size = int(math.log2(self.img_size))\n    noises = [torch.randn(1, 1, 4, 4, device=self.device)]\n\n    for i in range(3, log_size + 1):\n        for _ in range(2):\n            noises.append(torch.randn(1, 1, 2 ** i, 2 ** i, device=self.device))\n\n    return noises\n\n", "CodeBleu_score": 46.08770207999607}]}
{"instance_id": "42fd805563c6", "generation_result": [{"code": "def mean_latent(self, num_latent)\n\t\n    latent_in = torch.randn(num_latent, self.latent_size, device=self.device)\n    latent = self.style(latent_in).mean(0, keepdim=True)\n    return latent\n\n", "CodeBleu_score": 33.64655066986853}, {"code": "def mean_latent(self, num_latent)\n\t\n    latent_in = torch.randn(num_latent, self.latent_size, device=self.device)\n    latent = self.style(latent_in).mean(0, keepdim=True)\n    return latent\n\n", "CodeBleu_score": 33.64655066986853}, {"code": "def mean_latent(self, num_latent)\n\t\n    latent_in = torch.randn(num_latent, self.latent_size, device=self.device)\n    latent = self.style(latent_in).mean(0, keepdim=True)\n    return latent\n\n", "CodeBleu_score": 33.64655066986853}]}
{"instance_id": "2f485fd924af", "generation_result": [{"code": "def color_jitter(img, shift)\n\t\n    img = adjust_brightness(img, shift)\n    img = adjust_contrast(img, shift)\n    img = adjust_hue(img, shift)\n    img = adjust_saturation(img, shift)\n    return img\n\n", "CodeBleu_score": 15.4279697125584}, {"code": "def color_jitter(img, shift)\n\t\n    brightness = random.uniform(-shift, shift)\n    contrast = random.uniform(-shift, shift)\n    saturation = random.uniform(-shift, shift)\n    hue = random.uniform(-shift, shift)\n    img = adjust_brightness(img, brightness)\n    img = adjust_contrast(img, contrast)\n    img = adjust_saturation(img, saturation)\n    img = adjust_hue(img, hue)\n    return img\n\n", "CodeBleu_score": 28.81693076740637}, {"code": "def color_jitter(img, shift)\n\t\n    brightness = shift[0]\n    contrast = shift[1]\n    saturation = shift[2]\n    hue = shift[3]\n    img = adjust_brightness(img, brightness)\n    img = adjust_contrast(img, contrast)\n    img = adjust_saturation(img, saturation)\n    img = adjust_hue(img, hue)\n    return img\n\n", "CodeBleu_score": 15.010416023204254}]}
{"instance_id": "c87ccf1fbf48", "generation_result": [{"code": "def feed_data(self, data)\n\t\n    self.lq = data['lq'].to(self.device)\n    if 'gt' in data:\n        self.gt = data['gt'].to(self.device)\n    if 'lq_size' in data:\n        self.lq_size = data['lq_size']\n    if 'gt_size' in data:\n        self.gt_size = data['gt_size']\n    if 'lq_path' in data:\n        self.lq_path = data['lq_path']\n    if 'gt_path' in data:\n        self.gt_path = data['gt_path']\n    if 'lq_path_lmark' in data:\n        self.lq_path_lmark = data['lq_path_lmark']\n    if 'gt_path_lmark' in data:\n        self.gt_path_lmark = data['gt_path_lmark']\n    if 'lq_lmark' in data:\n        self.lq_lmark = data['lq_lmark'].to(self.device)\n    if 'gt_lmark' in data:\n        self.gt_lmark = data['gt_lmark'].to(self.device)\n\n", "CodeBleu_score": 49.8204992631675}, {"code": "def feed_data(self, data)\n\t\n    self.lq = data['lq'].to(self.device)  # low-quality image\n    self.lq_path = data['lq_path']  # low-quality image path\n    self.lq_size = data['lq_size']  # low-quality image size\n    if self.opt['phase']!= 'test':\n        self.gt = data['gt'].to(self.device)  # ground-truth image\n        self.gt_path = data['gt_path']  # ground-truth image path\n        self.gt_size = data['gt_size']  # ground-truth image size\n    if self.opt['face_enhancement']['use_facial_landmarks']:\n        self.left_eye = data['left_eye'].to(self.device)\n        self.right_eye = data['right_eye'].to(self.device)\n        self.mouth = data['mouth'].to(self.device)\n\n", "CodeBleu_score": 37.279905689922984}, {"code": "def feed_data(self, data)\n\t\n    self.lq = data['lq'].to(self.device)\n    if 'gt' in data:\n        self.gt = data['gt'].to(self.device)\n    if 'lq_path' in data:\n        self.lq_path = data['lq_path']\n    if 'gt_path' in data:\n        self.gt_path = data['gt_path']\n    if 'lq_size' in data:\n        self.lq_size = data['lq_size']\n    if 'gt_size' in data:\n        self.gt_size = data['gt_size']\n    if 'lq_landmarks' in data:\n        self.lq_landmarks = data['lq_landmarks'].to(self.device)\n    if 'gt_landmarks' in data:\n        self.gt_landmarks = data['gt_landmarks'].to(self.device)\n    if 'lq_landmarks_path' in data:\n        self.lq_landmarks_path = data['lq_landmarks_path']\n    if 'gt_landmarks_path' in data:\n        self.gt_landmarks_path = data['gt_landmarks_path']\n\n", "CodeBleu_score": 49.38005257468394}]}
{"instance_id": "8f8c7cb13037", "generation_result": [{"code": "def construct_img_pyramid(self)\n\t\n    img_pyramid = []\n    img_pyramid.append(self.gt_img)\n    for i in range(self.num_scales):\n        img_pyramid.insert(0, F.interpolate(img_pyramid[0], scale_factor=0.5, mode='bilinear', align_corners=False))\n    return img_pyramid\n\n", "CodeBleu_score": 27.499651548269206}, {"code": "def construct_img_pyramid(self)\n\t\n    img_pyramid = []\n    img_pyramid.append(self.gt_img)\n    for _ in range(self.num_stages):\n        img_pyramid.insert(0, F.interpolate(img_pyramid[0], scale_factor=0.5, mode='bilinear', align_corners=False))\n    return img_pyramid\n\n", "CodeBleu_score": 28.68590461100829}, {"code": "def construct_img_pyramid(self)\n\t\n    img_pyramid = []\n    img_pyramid.append(self.gt)\n    for i in range(self.num_scales - 1):\n        img_pyramid.insert(\n            0,\n            F.interpolate(\n                img_pyramid[0],\n                scale_factor=1 / 2,\n                mode=\"bilinear\",\n                align_corners=False,\n            ),\n        )\n    return img_pyramid\n\n", "CodeBleu_score": 21.257266877181742}]}
{"instance_id": "51247274d40c", "generation_result": [{"code": "def save(self, epoch, current_iter)\n\t\n    if self.opt['dist']:\n        torch.save(self.netG.module.state_dict(),\n                   self.save_dir / f'netG_{epoch}_{current_iter}.pth')\n        torch.save(self.netG_ema.module.state_dict(),\n                   self.save_dir / f'netG_ema_{epoch}_{current_iter}.pth')\n        torch.save(self.netD.module.state_dict(),\n                   self.save_dir / f'netD_{epoch}_{current_iter}.pth')\n        if self.opt['face_discriminator']:\n            torch.save(self.netD_left.module.state_dict(),\n                       self.save_dir / f'netD_left_{epoch}_{current_iter}.pth')\n            torch.save(self.netD_right.module.state_dict(),\n                       self.save_dir / f'netD_right_{epoch}_{current_iter}.pth')\n            torch.save(self.netD_mouth.module.state_dict(),\n                       self.save_dir / f'netD_mouth_{epoch}_{current_iter}.pth')\n    else:\n        torch.save(self.netG.state_dict(),\n                   self.save_dir / f'netG_{epoch}_{current_iter}.pth')\n        torch.save(self.netG_ema.state_dict(),\n                   self.save_dir / f'netG_ema_{epoch}_{current_iter}.pth')\n        torch.save(self.netD.state_dict(),\n                   self.save_dir / f'netD_{epoch}_{current_iter}.pth')\n        if self.opt['face_discriminator']:\n            torch.save(self.netD_left.state_dict(),\n                       self.save_dir / f'netD_left_{epoch}_{current_iter}.pth')\n            torch.save(self.netD_right.state_dict(),\n                       self.save_dir / f'netD_right_{epoch}_{current_iter}.pth')\n            torch.save(self.netD_mouth.state_dict(),\n                       self.save_dir / f'netD_mouth_{epoch}_{current_iter}.pth')\n\n    torch.save(self.state_dict(), self.save_dir /'state.pth')\n\n", "CodeBleu_score": 31.86935928355652}, {"code": "def save(self, epoch, current_iter)\n\t\n    if self.opt['dist']:\n        torch.distributed.barrier()\n\n    # save the overall training state\n    if self.opt['rank'] == 0:\n        save_dir = os.path.join(self.opt['path']['models'], self.opt['name'])\n        if not os.path.exists(save_dir):\n            os.makedirs(save_dir)\n        save_path = os.path.join(save_dir, 'training_state.pt')\n        torch.save(self.net_g.state_dict(), os.path.join(save_dir, 'net_g.pth'))\n        torch.save(self.ema.state_dict(), os.path.join(save_dir, 'ema.pth'))\n        if self.opt['dist']:\n            torch.save(self.net_d.module.state_dict(), os.path.join(save_dir, 'net_d.pth'))\n        else:\n            torch.save(self.net_d.state_dict(), os.path.join(save_dir, 'net_d.pth'))\n        if self.opt['train']['use_face_disc']:\n            torch.save(self.net_d_face_left.state_dict(), os.path.join(save_dir, 'net_d_face_left.pth'))\n            torch.save(self.net_d_face_right.state_dict(), os.path.join(save_dir, 'net_d_face_right.pth'))\n            torch.save(self.net_d_face_mouth.state_dict(), os.path.join(save_dir, 'net_d_face_mouth.pth'))\n        torch.save(self.optim_g.state_dict(), os.path.join(save_dir, 'optim_g.pth'))\n        torch.save(self.optim_d.state_dict(), os.path.join(save_dir, 'optim_d.pth'))\n        torch.save({'epoch': epoch, 'iter': current_iter}, save_path)\n\n", "CodeBleu_score": 33.21289670387456}, {"code": "def save(self, epoch, current_iter)\n\t\n    if self.opt['dist']:\n        torch.distributed.barrier()\n    if self.opt['rank'] == 0:\n        save_dir = os.path.join(self.opt['path']['root'], self.opt['path']['experiments'], self.opt['name'])\n        if not os.path.exists(save_dir):\n            os.makedirs(save_dir)\n        if self.opt['is_train']:\n            if self.opt['save_models']['net_g']:\n                torch.save(self.net_g.state_dict(),\n                           os.path.join(save_dir, 'latest_net_g.pth'))\n                torch.save(self.net_g.state_dict(),\n                           os.path.join(save_dir, 'latest_net_g_ema.pth'))\n            if self.opt['save_models']['net_d']:\n                torch.save(self.net_d.state_dict(),\n                           os.path.join(save_dir, 'latest_net_d.pth'))\n                if self.opt['network_d']['which_model_d'] == 'discriminator_vgg_128':\n                    torch.save(self.net_d_aux.state_dict(),\n                               os.path.join(save_dir, 'latest_net_d_aux.pth'))\n            if self.opt['save_models']['net_g_s']:\n                torch.save(self.net_g_s.state_dict(),\n                           os.path.join(save_dir, 'latest_net_g_s.pth'))\n            if self.opt['save_models']['net_d_s']:\n                torch.save(self.net_d_s.state_dict(),\n                           os.path.join(save_dir, 'latest_net_d_s.pth'))\n            if self.opt['save_models']['net_d_s_aux']:\n                torch.save(self.net_d_s_aux.state_dict(),\n                           os.path.join(save_dir, 'latest_net_d_s_aux.pth'))\n            if self.opt['save_models']['net_d_e']:\n                torch.save(self.net_d_e.state_dict(),\n                           os.path.join(save_dir, 'latest_", "CodeBleu_score": 32.942314034373865}]}
{"instance_id": "afa0dc8128c1", "generation_result": [{"code": "def clean_folder(folder)\n\t\n    for filename in os.listdir(folder):\n        file_path = os.path.join(folder, filename)\n        try:\n            if os.path.isfile(file_path) or os.path.islink(file_path):\n                os.unlink(file_path)\n            elif os.path.isdir(file_path):\n                shutil.rmtree(file_path)\n        except Exception as e:\n            print('Failed to delete %s. Reason: %s' % (file_path, e))\n\n", "CodeBleu_score": 70.93528996364833}, {"code": "def clean_folder(folder)\n\t\n    for filename in os.listdir(folder):\n        file_path = os.path.join(folder, filename)\n        try:\n            if os.path.isfile(file_path) or os.path.islink(file_path):\n                os.unlink(file_path)\n            elif os.path.isdir(file_path):\n                shutil.rmtree(file_path)\n        except Exception as e:\n            print('Failed to delete %s. Reason: %s' % (file_path, e))\n\n", "CodeBleu_score": 70.93528996364833}, {"code": "def clean_folder(folder)\n\t\n    for filename in os.listdir(folder):\n        file_path = os.path.join(folder, filename)\n        try:\n            if os.path.isfile(file_path) or os.path.islink(file_path):\n                os.unlink(file_path)\n            elif os.path.isdir(file_path):\n                shutil.rmtree(file_path)\n        except Exception as e:\n            print('Failed to delete %s. Reason: %s' % (file_path, e))\n\n", "CodeBleu_score": 70.93528996364833}]}
{"instance_id": "53588b481bdd", "generation_result": [{"code": "def next_batch(self, sample_size=1)\n\t\n    if sample_size > len(self.experiences):\n        sample_size = len(self.experiences)\n    batch = random.sample(self.experiences, sample_size)\n    self.experiences = [x for x in self.experiences if x not in batch]\n    return batch\n\n", "CodeBleu_score": 72.3484173821077}, {"code": "def next_batch(self, sample_size=1)\n\t\n    if sample_size > len(self.experiences):\n        sample_size = len(self.experiences)\n    batch = random.sample(self.experiences, sample_size)\n    return batch\n\n", "CodeBleu_score": 45.49355414720005}, {"code": "def next_batch(self, sample_size=1)\n\t\n    if sample_size > len(self.experiences):\n        sample_size = len(self.experiences)\n    batch = random.sample(self.experiences, sample_size)\n    return batch", "CodeBleu_score": 45.49355414720005}]}
{"instance_id": "6bfa6a4e621c", "generation_result": [{"code": "def find_estimated_top_configs(self)\n\t\n    # Flatten all_configs\n    flattened_configs = []\n    for config in self.all_configs:\n        flattened_configs.append(\n            [\n                value\n                for key, value in config.items()\n                if not isinstance(value, dict) and not isinstance(value, list)\n            ]\n        )\n\n    # Convert flattened_configs to numpy array\n    flattened_configs = np.array(flattened_configs)\n\n    # Get numerical values from flattened_configs\n    numerical_values = np.array(\n        [\n            value\n            for value in flattened_configs\n            if isinstance(value, numbers.Number)\n        ]\n    )\n\n    # Predict estimates for numerical values\n    estimates = self.cost_model.predict(numerical_values)\n\n    # Sort estimates based on the specified metric\n    if self.metric == \"min\":\n        sorted_indices = np.argsort(estimates)\n    else:\n        sorted_indices = np.argsort(-estimates)\n\n    # Return the indices of the top configurations\n    return sorted_indices\n\n", "CodeBleu_score": 34.672026845594786}, {"code": "def find_estimated_top_configs(self)\n\t\n    # Flatten all_configs\n    all_configs_flat = []\n    for config in self.all_configs:\n        all_configs_flat.append(flatten_dict(config))\n\n    # Collect numerical values\n    all_configs_flat_num = []\n    for config in all_configs_flat:\n        all_configs_flat_num.append(\n            {k: v for k, v in config.items() if isinstance(v, numbers.Number)}\n        )\n\n    # Convert to numpy array\n    all_configs_flat_num_np = np.array(\n        [list(config.values()) for config in all_configs_flat_num]\n    )\n\n    # Predict estimates\n    estimates = self.cost_model.predict(all_configs_flat_num_np)\n\n    # Sort and return indices\n    if self.metric == \"min\":\n        return np.argsort(estimates)\n    elif self.metric == \"max\":\n        return np.argsort(-estimates)\n    else:\n        raise ValueError(f\"Invalid metric: {self.metric}\")\n\n", "CodeBleu_score": 40.16428954908561}, {"code": "def find_estimated_top_configs(self)\n\t\n    # Flatten all_configs\n    flat_configs = []\n    for config in self.all_configs:\n        flat_configs.append(flatten_dict(config))\n    # Extract numerical values from flat_configs\n    num_configs = []\n    for config in flat_configs:\n        num_configs.append([value for key, value in config.items() if isinstance(value, numbers.Number)])\n    # Convert num_configs to numpy array\n    num_configs = np.array(num_configs)\n    # Predict estimates for the numerical values\n    estimates = self.cost_model.predict(num_configs)\n    # Sort the configurations based on the specified metric\n    if self.metric == \"min\":\n        sorted_indices = np.argsort(estimates)\n    else:\n        sorted_indices = np.argsort(-estimates)\n    # Return the indices of the top configurations\n    return sorted_indices\n\n", "CodeBleu_score": 41.36257336638187}]}
{"instance_id": "d4cf48413a57", "generation_result": [{"code": "def flatten(d, parent_key='', sep='_')\n\t\n    items = []\n    for k, v in d.items():\n        new_key = parent_key + sep + k if parent_key else k\n        if isinstance(v, collections.abc.MutableMapping):\n            items.extend(flatten(v, new_key, sep=sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)\n\n", "CodeBleu_score": 83.2096634628176}, {"code": "def flatten(d, parent_key='', sep='_')\n\t\n    items = []\n    for k, v in d.items():\n        new_key = parent_key + sep + k if parent_key else k\n        if isinstance(v, collections.abc.MutableMapping):\n            items.extend(flatten(v, new_key, sep=sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)\n\n", "CodeBleu_score": 83.2096634628176}, {"code": "def flatten(d, parent_key='', sep='_')\n\t\n    items = []\n    for k, v in d.items():\n        new_key = parent_key + sep + k if parent_key else k\n        if isinstance(v, collections.abc.MutableMapping):\n            items.extend(flatten(v, new_key, sep=sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)\n\n", "CodeBleu_score": 83.2096634628176}]}
{"instance_id": "c2a514249c25", "generation_result": [{"code": "def run_after_tuning(self)\n\t\n    if self.optimal_cmd:\n        # Run the optimal command\n        print(f\"Running optimal command: {self.optimal_cmd}\")\n        process = subprocess.Popen(self.optimal_cmd, shell=True)\n        process.wait()\n        print(\"Tuning completed successfully.\")\n    else:\n        print(\"No optimal command found. Autotuning did not find an optimal configuration.\")\n\n", "CodeBleu_score": 51.16701171775565}, {"code": "def run_after_tuning(self)\n\t\n    if self.optimal_cmd:\n        print(f\"Running optimal command: {self.optimal_cmd}\")\n        p = subprocess.Popen(self.optimal_cmd, shell=True)\n        p.wait()\n        print(\"Finished running optimal command\")\n    else:\n        print(\"No optimal command found. Autotuning did not find an optimal configuration.\")\n\n", "CodeBleu_score": 51.542490984535114}, {"code": "def run_after_tuning(self)\n\t\n    if self.optimal_cmd is not None:\n        print(\"Running optimal command: \", self.optimal_cmd)\n        subprocess.Popen(self.optimal_cmd, shell=True).wait()\n        print(\"Optimal command ran successfully\")\n    else:\n        print(\"Autotuning did not find an optimal configuration\")\n\n", "CodeBleu_score": 30.627613027794574}]}
{"instance_id": "61802de52df0", "generation_result": [{"code": "def run_job(self, exp: dict, reservations)\n\t\n    # Set the master port and result directory for the experiment\n    exp['master_port'] = self.master_port\n    exp['result_dir'] = self.result_dir\n    # Update the user arguments based on the argument mappings\n    user_args = exp['user_args']\n    for arg_map in self.arg_maps:\n        if arg_map['exp_arg'] in user_args:\n            user_args[arg_map['user_arg']] = user_args[arg_map['exp_arg']]\n    # Create a new thread to run the experiment\n    thread = threading.Thread(target=self.run_experiment, args=(exp, reservations))\n    thread.start()\n    # Add the running experiment details to running_experiments\n    running_exp = copy.deepcopy(exp)\n    running_exp['thread'] = thread\n    running_exp['timestamp'] = time.time()\n    self.running_experiments[running_exp['id']] = running_exp\n\n", "CodeBleu_score": 21.34355762739577}, {"code": "def run_job(self, exp: dict, reservations)\n\t\n    # Set the master port and result directory\n    exp['master_port'] = self.master_port\n    exp['result_dir'] = self.result_dir\n    # Update user arguments based on argument mappings\n    exp['user_args'] = self.update_args(exp['user_args'])\n    # Start the experiment in a new thread\n    thread = threading.Thread(target=self.run_job_thread, args=(exp, reservations))\n    thread.start()\n    # Store the running experiment details with a timestamp\n    self.running_experiments[time.time()] = exp\n\n", "CodeBleu_score": 12.929767686849281}, {"code": "def run_job(self, exp: dict, reservations)\n\t\n\n    # Set the master port and result directory for the experiment\n    master_port = self.master_port\n    result_directory = self.result_directory\n\n    # Update user arguments based on argument mappings\n    user_args = copy.deepcopy(exp['user_args'])\n    for mapping in exp['argument_mappings']:\n        if mapping['from'] in user_args:\n            user_args[mapping['to']] = user_args[mapping['from']]\n            del user_args[mapping['from']]\n\n    # Create a new thread to run the experiment\n    thread = threading.Thread(target=self.run_experiment, args=(exp['script'], user_args, master_port, result_directory, reservations))\n    thread.start()\n\n    # Store the running experiment details with a timestamp\n    self.running_experiments[time.time()] = {'exp': exp, 'thread': thread}\n\n", "CodeBleu_score": 24.214883639643375}]}
{"instance_id": "4884e2e9665c", "generation_result": [{"code": "def save_exp_results_to_database(self, message, ranks=None, path=None):\n\t\n    if self.is_initialized() and self.args.log_results_to_db:\n        if ranks is None:\n            ranks = [dist.get_rank()]\n        if path is None:\n            path = self.args.log_dir\n        if not os.path.exists(path):\n            os.makedirs(path)\n        for rank in ranks:\n            message[\"rank\"] = rank\n            with open(os.path.join(path, \"results.json\"), \"a\") as f:\n                json.dump(message, f)\n                f.write(\"", "CodeBleu_score": 30.684388342950736}, {"code": "def save_exp_results_to_database(self, message, ranks=None, path=None):\n\t\n    if self.is_initialized():\n        if ranks is None:\n            ranks = [dist.get_rank()]\n        if dist.get_rank() in ranks:\n            if path is None:\n                path = self.exp_results_path\n            if path is not None:\n                message = copy.deepcopy(message)\n                message['rank'] = dist.get_rank()\n                message['timestamp'] = time.time()\n                message = json.dumps(message)\n                with open(path, 'a') as f:\n                    f.write(message + '", "CodeBleu_score": 35.361365735938136}, {"code": "def save_exp_results_to_database(self, message, ranks=None, path=None):\n\t\n    if not self.is_initialized:\n        return\n    if ranks is None:\n        ranks = [0]\n    if path is None:\n        path = self.results_file\n    if dist.is_initialized():\n        if dist.get_rank() in ranks:\n            message['rank'] = dist.get_rank()\n            with open(path, 'a') as f:\n                f.write(json.dumps(message) + '", "CodeBleu_score": 29.61692260303402}]}
{"instance_id": "0429344c3c4d", "generation_result": [{"code": "def _build_tp_other_layer_map(self, layer_index: int)\n\t\n    if self.layer_files is None:\n        raise ValueError(\"No layer files found\")\n    if layer_index not in self.layer_files:\n        raise ValueError(f\"No layer files found for layer index {layer_index}\")\n\n    layer_files = self.layer_files[layer_index]\n    if not layer_files:\n        raise ValueError(f\"No layer files found for layer index {layer_index}\")\n\n    tp_other_layer_map = {}\n    for layer_file in layer_files:\n        if layer_file.startswith(f\"{self.layer_prefix}{layer_index}_\"):\n            tp_degree = int(re.search(r\"tp_degree_(\\d+)\", layer_file).group(1))\n            if tp_degree not in tp_other_layer_map:\n                tp_other_layer_map[tp_degree] = []\n            tp_other_layer_map[tp_degree].append(layer_file)\n\n    return tp_other_layer_map\n\n", "CodeBleu_score": 28.733440219260213}, {"code": "def _build_tp_other_layer_map(self, layer_index: int)\n\t\n    layer_files = [\n        f for f in os.listdir(self.layer_dir)\n        if f.startswith(f\"{self.layer_prefix}{layer_index}_\")\n    ]\n    if not layer_files:\n        raise ValueError(f\"No layer files found for layer {layer_index}\")\n    if layer_index < 0 or layer_index >= len(layer_files):\n        raise ValueError(f\"Layer index {layer_index} is out of range\")\n    layer_files = sorted(layer_files)\n    tp_other_layer_map = {}\n    for file in layer_files:\n        match = re.search(r\"_\\d+_\", file)\n        if match:\n            tp_degree = int(match.group(0).strip(\"_\"))\n            if tp_degree not in tp_other_layer_map:\n                tp_other_layer_map[tp_degree] = []\n            tp_other_layer_map[tp_degree].append(file)\n    return tp_other_layer_map\n\n", "CodeBleu_score": 27.06528736098023}, {"code": "def _build_tp_other_layer_map(self, layer_index: int)\n\t\n    # Check if there are any layer files\n    if not self.layer_files:\n        raise ValueError(\"No layer files found\")\n    # Verify the layer index\n    if layer_index < 0 or layer_index >= len(self.layer_files):\n        raise ValueError(\n            f\"Invalid layer index {layer_index}. Layer index must be between 0 and {len(self.layer_files) - 1}.\"\n        )\n    # Get files with a specific prefix for the given layer\n    layer_files = self.layer_files[layer_index]\n    # Partition these files based on tp_degree\n    partition_files = {}\n    for file in layer_files:\n        # Get the partition index from the file name\n        partition_index = int(re.search(r\"partition_(\\d+)\", file).group(1))\n        # Add the file to the corresponding partition index\n        if partition_index not in partition_files:\n            partition_files[partition_index] = []\n        partition_files[partition_index].append(file)\n    # Create and return a dictionary mapping partition indices to their corresponding file lists\n    return partition_files\n\n", "CodeBleu_score": 34.55166936981684}]}
{"instance_id": "1b65ecb9be93", "generation_result": [{"code": "def system_info()\n\t\n    info = {\n        \"os\": platform.system(),\n        \"architecture\": platform.machine(),\n        \"version\": platform.release(),\n        \"processor\": platform.processor(),\n        \"python_version\": platform.python_version(),\n    }\n    return info\n\n", "CodeBleu_score": 54.5282483934804}, {"code": "def system_info()\n\t\n    info = {}\n    info[\"os\"] = platform.system()\n    info[\"architecture\"] = platform.architecture()[0]\n    info[\"version\"] = platform.release()\n    info[\"processor\"] = platform.processor()\n    info[\"python_version\"] = sys.version.split(\" \")[0]\n    return info\n\n", "CodeBleu_score": 42.357325580549265}, {"code": "def system_info()\n\t\n    system = platform.system()\n    architecture = platform.architecture()[0]\n    version = platform.version()\n    processor = platform.processor()\n    python_version = platform.python_version()\n    return {\"system\": system, \"architecture\": architecture, \"version\": version, \"processor\": processor, \"python_version\": python_version}\n\n", "CodeBleu_score": 44.11553590347457}]}
{"instance_id": "969bcc5cf3b5", "generation_result": [{"code": "def cuda_info()\n\t\n    if torch.cuda.is_available():\n        gpu_name = torch.cuda.get_device_name(0)\n        cuda_available = \"Yes\"\n        cuda_version = torch.version.cuda\n    else:\n        gpu_name = \"None\"\n        cuda_available = \"No\"\n        cuda_version = \"None\"\n    return gpu_name, cuda_available, cuda_version\n\n", "CodeBleu_score": 41.213843857678945}, {"code": "def cuda_info()\n\t\n    cuda_available = torch.cuda.is_available()\n    cuda_version = torch.version.cuda\n    cuda_devices = [torch.cuda.get_device_name(i) for i in range(torch.cuda.device_count())]\n    return cuda_available, cuda_version, cuda_devices\n\n", "CodeBleu_score": 45.87352074370546}, {"code": "def cuda_info()\n\t\n    if torch.cuda.is_available():\n        print('CUDA is available')\n        print('CUDA device name:'+ torch.cuda.get_device_name(0))\n        print('CUDA version:'+ str(torch.version.cuda))\n    else:\n        print('CUDA is not available')\n\n", "CodeBleu_score": 29.72508916163435}]}
{"instance_id": "a2ba908a4c70", "generation_result": [{"code": "def compute_encoder_accuracy(dataset_items, encoder_manager)\n\t\n    class_name_key = encoder_manager.config[\"class_name_key\"]\n    class_id_to_class_name = encoder_manager.config.get(\"class_id_to_class_name\", None)\n    total_count = 0\n    class_count = {}\n    class_correct = {}\n    for item in tqdm(dataset_items):\n        audio_path = item[\"audio_path\"]\n        audio = torch.from_numpy(item[\"audio\"]).unsqueeze(0).to(encoder_manager.device)\n        embedding = encoder_manager.get_embedding(audio)\n        class_name = item[class_name_key]\n        if class_id_to_class_name is not None:\n            class_id = item[\"class_id\"]\n            class_name = class_id_to_class_name[class_id]\n        predicted_class_name = encoder_manager.predict_class_name(embedding)\n        total_count += 1\n        if class_name not in class_count:\n            class_count[class_name] = 1\n            class_correct[class_name] = 1 if class_name == predicted_class_name else 0\n        else:\n            class_count[class_name] += 1\n            class_correct[class_name] += 1 if class_name == predicted_class_name else 0\n    for class_name in class_count:\n        print(f\"{class_name} accuracy: {class_correct[class_name] / class_count[class_name] * 100:.2f}%\")\n    print(f\"Average accuracy: {sum(class_correct.values()) / sum(class_count.values()) * 100:.2f}%\")\n\n", "CodeBleu_score": 34.514054422810084}, {"code": "def compute_encoder_accuracy(dataset_items, encoder_manager)\n\t\n    # Get class name key and optional class ID to class name mapping from encoder manager's configuration\n    class_name_key = encoder_manager.config.get(\"class_name_key\", None)\n    class_id_to_name_mapping = encoder_manager.config.get(\"class_id_to_name_mapping\", None)\n    # Initialize accuracy counters\n    total_samples = 0\n    correct_samples = 0\n    class_accuracies = {}\n    # Iterate over each item in the dataset\n    for item in tqdm(dataset_items):\n        # Extract audio file\n        audio_file = item[\"audio_file\"]\n        # Compute embedding\n        embedding = encoder_manager.compute_embedding(audio_file)\n        # Predict class label if criteria and mappings are available\n        if class_name_key is not None and class_id_to_name_mapping is not None:\n            predicted_class = encoder_manager.predict_class(embedding)\n            # Record accuracy for each class\n            true_class = item[class_name_key]\n            if true_class not in class_accuracies:\n                class_accuracies[true_class] = {\"total\": 0, \"correct\": 0}\n            class_accuracies[true_class][\"total\"] += 1\n            if predicted_class == true_class:\n                class_accuracies[true_class][\"correct\"] += 1\n        # Update accuracy counters\n        total_samples += 1\n        if predicted_class == true_class:\n            correct_samples += 1\n    # Calculate and print accuracy for each class and average accuracy across all classes\n    print(f\"Accuracy for each class:\")\n    for class_name, accuracy_data in class_accuracies.items():\n        accuracy = accuracy_data[\"correct\"] / accuracy_data[\"total\"]\n        print(f\"Class: {class_name}, Accuracy: {accuracy:.2%}\")\n    average_accuracy = correct_samples / total_samples\n    print(f\"Average accuracy across all classes: {average_accuracy:.2%}\")\n\n", "CodeBleu_score": 34.6436565773149}, {"code": "def compute_encoder_accuracy(dataset_items, encoder_manager)\n\t\n    # Get the class name key and optional class ID to class name mapping from the encoder manager's configuration\n    class_name_key = encoder_manager.config[\"class_name_key\"]\n    class_id_to_name = encoder_manager.config.get(\"class_id_to_name\", None)\n    # Initialize a dictionary to store the accuracy for each class\n    class_accuracies = {}\n    # Initialize a variable to store the total number of samples\n    total_samples = 0\n    # Initialize a variable to store the total number of correct predictions\n    total_correct = 0\n    # Loop over the items in the dataset\n    for item in tqdm(dataset_items):\n        # Extract the audio file from the item\n        audio_file = item[\"audio_file\"]\n        # Compute the embedding for the audio file\n        embedding = encoder_manager.compute_embedding(audio_file)\n        # Predict the class label for the audio file\n        predicted_class_name = encoder_manager.predict_class(embedding)\n        # Get the true class name from the item\n        true_class_name = item[class_name_key]\n        # If class ID to class name mapping is available, convert the true class ID to class name\n        if class_id_to_name is not None:\n            true_class_name = class_id_to_name[true_class_name]\n        # Increment the total number of samples\n        total_samples += 1\n        # If the true class name matches the predicted class name, increment the total number of correct predictions\n        if true_class_name == predicted_class_name:\n            total_correct += 1\n        # If the true class name is not in the class accuracies dictionary, add it with an initial accuracy of 0\n        if true_class_name not in class_accuracies:\n            class_accuracies[true_class_name] = 0\n        # Increment the accuracy for the true class name\n        class_accuracies[true_class_name] += 1\n    # Calculate the average accuracy across all classes\n    avg_accuracy = total_correct / total_samples\n    # Print the accuracy for each class\n    for class_name, accuracy in class_accuracies.items():\n        accuracy = accuracy / total_samples\n        print(f\"Accuracy for class {class_name}: {accuracy:.4f}\")\n    # Print the average accuracy", "CodeBleu_score": 29.48170432273726}]}
{"instance_id": "e01ec69ad58a", "generation_result": [{"code": "def adjust_path_and_remove_silence(audio_path)\n\t\n    input_dir = args.input_dir\n    output_dir = args.output_dir\n    audio_path = str(audio_path)\n    audio_path = audio_path.replace(input_dir, output_dir)\n    audio_path = audio_path.replace(\".wav\", \".mp3\")\n    audio_path = pathlib.Path(audio_path)\n    audio_path.parent.mkdir(parents=True, exist_ok=True)\n    if not args.force and audio_path.is_file():\n        return audio_path, False\n    if args.vad:\n        vad_model, vad_utils = get_vad_model_and_utils()\n        audio, sr = torchaudio.load(str(audio_path))\n        audio = remove_silence(audio, sr, vad_model, vad_utils, args.vad_threshold, args.vad_min_length)\n        if audio.shape[1] == 0:\n            return audio_path, False\n        torchaudio.save(str(audio_path), audio, sr)\n    return audio_path, True\n\n", "CodeBleu_score": 38.359572310691846}, {"code": "def adjust_path_and_remove_silence(audio_path)\n\t\n    audio_path = pathlib.Path(audio_path)\n    output_path = pathlib.Path(audio_path.as_posix().replace(args.input_dir, args.output_dir))\n    output_path.parent.mkdir(parents=True, exist_ok=True)\n    if output_path.exists() and not args.force:\n        return output_path, False\n    audio, rate = torchaudio.load(audio_path)\n    audio = audio.numpy().squeeze()\n    if args.vad_model_path:\n        audio, rate = remove_silence(audio, rate, args.vad_model_path, args.vad_model_type, args.vad_model_device, args.vad_threshold, args.vad_padding)\n    torchaudio.save(output_path, torch.from_numpy(audio), rate)\n    return output_path, True\n\n", "CodeBleu_score": 42.71354973070015}, {"code": "def adjust_path_and_remove_silence(audio_path)\n\t\n    audio_path = pathlib.Path(audio_path)\n    output_path = audio_path.parent.parent.joinpath(f\"{output_dir}\", audio_path.parent.name, audio_path.name)\n    output_path.parent.mkdir(parents=True, exist_ok=True)\n    if not force and output_path.exists():\n        return output_path, False\n    remove_silence(audio_path, output_path, vad_model, vad_utils, min_silence_len, min_speech_len, silence_threshold)\n    return output_path, True\n\n", "CodeBleu_score": 28.048537908245574}]}
{"instance_id": "7e0bdb0feff8", "generation_result": [{"code": "def resample_files(input_dir, output_sr, output_dir=None, file_ext=\"wav\", n_jobs=10)\n\t\n    if output_dir is None:\n        output_dir = input_dir\n    copytree(input_dir, output_dir)\n    file_paths = glob.glob(os.path.join(output_dir, \"**\", f\"*.{file_ext}\"), recursive=True)\n    with Pool(n_jobs) as pool:\n        for _ in tqdm(pool.imap_unordered(resample_file, file_paths), total=len(file_paths)):\n            pass\n    print(\"Resampling complete.\")\n\n", "CodeBleu_score": 36.83882773655201}, {"code": "def resample_files(input_dir, output_sr, output_dir=None, file_ext=\"wav\", n_jobs=10)\n\t\n    if not output_dir:\n        output_dir = input_dir\n\n    copytree(input_dir, output_dir)\n\n    files = glob.glob(f\"{output_dir}/**/*.{file_ext}\", recursive=True)\n\n    with Pool(n_jobs) as p:\n        for _ in tqdm(\n            p.imap_unordered(\n                lambda file: resample_file(file, output_sr), files\n            ),\n            total=len(files),\n        ):\n            pass\n\n    print(f\"Resampling complete. Files resampled to {output_sr} Hz.\")\n\n", "CodeBleu_score": 32.88777288033368}, {"code": "def resample_files(input_dir, output_sr, output_dir=None, file_ext=\"wav\", n_jobs=10)\n\t\n    if output_dir is None:\n        output_dir = input_dir\n    copytree(input_dir, output_dir)\n    file_paths = []\n    for root, dirs, files in os.walk(output_dir):\n        for file in files:\n            if file.endswith(file_ext):\n                file_paths.append(os.path.join(root, file))\n    with Pool(n_jobs) as p:\n        with tqdm(total=len(file_paths)) as pbar:\n            for _ in p.imap_unordered(lambda x: resample_file(x, output_sr), file_paths):\n                pbar.update()\n    print(\"Resampling complete.\")\n\n", "CodeBleu_score": 38.890386586786}]}
{"instance_id": "a1351834ca57", "generation_result": [{"code": "def evaluation(model, criterion, data_loader, global_step)\n\t\n    model.eval()\n    running_loss = 0.0\n    with torch.no_grad():\n        for i, (inputs, labels) in enumerate(data_loader):\n            inputs = inputs.to(model.device)\n            labels = labels.to(model.device)\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            running_loss += loss.item()\n    avg_loss = running_loss / (i + 1)\n    print(\"Evaluation loss: {:.4f}\".format(avg_loss))\n    return avg_loss\n\n", "CodeBleu_score": 18.56814624606117}, {"code": "def evaluation(model, criterion, data_loader, global_step)\n\t\n    model.eval()\n    criterion.eval()\n    total_loss = 0\n    with torch.no_grad():\n        for batch_idx, (inputs, labels) in enumerate(data_loader):\n            inputs = inputs.to(model.device)\n            labels = labels.to(model.device)\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            total_loss += loss.item()\n            if batch_idx == 0:\n                plot_embeddings(outputs, labels, global_step)\n    avg_loss = total_loss / len(data_loader)\n    logging.info(f\"Evaluation loss: {avg_loss:.5f}\")\n    return avg_loss\n\n", "CodeBleu_score": 24.835950029039978}, {"code": "def evaluation(model, criterion, data_loader, global_step)\n\t\n    model.eval()\n    total_loss = 0\n    for batch_idx, (inputs, labels) in enumerate(data_loader):\n        inputs = inputs.to(model.device)\n        labels = labels.to(model.device)\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        total_loss += loss.item()\n    avg_loss = total_loss / (batch_idx + 1)\n    logger.info(f\"Evaluation loss at step {global_step}: {avg_loss:.4f}\")\n    return avg_loss\n\n", "CodeBleu_score": 17.33050540371946}]}
{"instance_id": "565f8ed09916", "generation_result": [{"code": "def register_config(model_name: str) -> Coqpit\n\t\n    if model_name == \"xtts\":\n        config_name = \"XTTSConfig\"\n    else:\n        config_name = model_name.capitalize() + \"Config\"\n    for path in [\n        \"TTS.config.configs\",\n        \"TTS.config.configs.fastpitch\",\n        \"TTS.config.configs.fastspeech2\",\n        \"TTS.config.configs.tacotron2\",\n        \"TTS.config.configs.transformer\",\n        \"TTS.config.configs.vits\",\n        \"TTS.config.configs.whisper\",\n    ]:\n        try:\n            config_class = find_module(path, config_name)\n            return config_class\n        except ModuleNotFoundError:\n            pass\n    raise ModuleNotFoundError(f\"Config class {config_name} not found.\")\n\n", "CodeBleu_score": 41.59829211783145}, {"code": "def register_config(model_name: str) -> Coqpit\n\t\n    if model_name == \"xtts\":\n        config_class = find_module(\n            \"TTS.config.xtts_configs\", f\"{model_name.upper()}Config\"\n        )\n    else:\n        config_class = find_module(\n            \"TTS.config.shared_configs\", f\"{model_name.upper()}Config\"\n        )\n    if config_class is None:\n        raise ModuleNotFoundError(\n            f\"Could not find {model_name.upper()}Config in TTS.config.shared_configs or TTS.config.xtts_configs\"\n        )\n    return config_class\n\n", "CodeBleu_score": 31.018264096313676}, {"code": "def register_config(model_name: str) -> Coqpit\n\t\n    try:\n        config_class = find_module(f\"TTS.config.{model_name}_config\", f\"{model_name.upper()}Config\")\n    except ModuleNotFoundError:\n        config_class = find_module(f\"TTS.config.xtts_config\", f\"{model_name.upper()}Config\")\n    return config_class\n\n", "CodeBleu_score": 19.700742922210722}]}
{"instance_id": "0ea7862506a7", "generation_result": [{"code": "def load_config(config_path: str) -> Coqpit\n\t\n    if config_path.endswith(\".yaml\") or config_path.endswith(\".yml\"):\n        with open(config_path, \"r\") as f:\n            config_dict = yaml.safe_load(f)\n    elif config_path.endswith(\".json\"):\n        with open(config_path, \"r\") as f:\n            config_dict = json.load(f)\n    else:\n        raise ValueError(f\"Unknown config file extension for {config_path}\")\n    model_name = config_dict.get(\"model_name\", None)\n    if model_name is None:\n        raise ValueError(\"No model name specified in config\")\n    config_class = find_module(model_name, \"config\", \"Config\")\n    config = config_class()\n    for key, value in config_dict.items():\n        if key == \"model_name\":\n            continue\n        if key in config.__dict__:\n            if isinstance(value, dict):\n                for sub_key, sub_value in value.items():\n                    if sub_key in config.__dict__[key].__dict__:\n                        setattr(config.__dict__[key], sub_key, sub_value)\n            else:\n                setattr(config, key, value)\n        else:\n            raise ValueError(f\"Unknown config key: {key}\")\n    return config\n\n", "CodeBleu_score": 40.037903213272095}, {"code": "def load_config(config_path: str) -> Coqpit\n\t\n    with open(config_path, \"r\") as f:\n        config_dict = yaml.safe_load(f)\n\n    model_name = config_dict.get(\"model_name\", None)\n    if model_name is None:\n        raise ValueError(f\"The configuration file does not contain the'model_name' key.\")\n\n    model_config_class = find_module(model_name)\n    if model_config_class is None:\n        raise ValueError(f\"Could not find the model configuration class for '{model_name}'.\")\n\n    config = model_config_class()\n    config.update_from_dict(config_dict)\n\n    return config\n\n", "CodeBleu_score": 33.48062753694009}, {"code": "def load_config(config_path: str) -> Coqpit\n\t\n    if config_path.endswith(\".yaml\") or config_path.endswith(\".yml\"):\n        with open(config_path, \"r\") as f:\n            config_dict = yaml.safe_load(f)\n    elif config_path.endswith(\".json\"):\n        with open(config_path, \"r\") as f:\n            config_dict = json.load(f)\n    else:\n        raise ValueError(\n            f\"Unknown file extension for configuration file: {config_path}\"\n        )\n    model_name = config_dict[\"model_name\"]\n    config_class = find_module(model_name, \"config\")\n    config = config_class()\n    config.update(config_dict)\n    return config\n\n", "CodeBleu_score": 39.12519009148292}]}
{"instance_id": "e0db68b63143", "generation_result": [{"code": "def list_files(basePath, validExts=None, contains=None)\n\t\n    for (dirpath, dirnames, filenames) in os.walk(basePath):\n        for filename in filenames:\n            if contains is not None and contains not in filename:\n                continue\n            if validExts is not None and not filename.endswith(validExts):\n                continue\n            # print(filename)\n            yield os.path.join(dirpath, filename)\n\n", "CodeBleu_score": 46.121383758961954}, {"code": "def list_files(basePath, validExts=None, contains=None)\n\t\n    for (dirpath, dirnames, filenames) in os.walk(basePath):\n        for filename in filenames:\n            if contains is not None and contains not in filename:\n                continue\n            if validExts is not None and os.path.splitext(filename)[1].lower() not in validExts:\n                continue\n            yield os.path.join(dirpath, filename)\n\n", "CodeBleu_score": 45.69821036913449}, {"code": "def list_files(basePath, validExts=None, contains=None)\n\t\n    for (dirpath, dirnames, filenames) in os.walk(basePath):\n        for filename in filenames:\n            if contains is not None and contains not in filename:\n                continue\n            if validExts is not None and os.path.splitext(filename)[1].lower() not in validExts:\n                continue\n            yield os.path.join(dirpath, filename)\n\n", "CodeBleu_score": 45.69821036913449}]}
{"instance_id": "b836fcaf0a27", "generation_result": [{"code": "def load_model(xtts_checkpoint, xtts_config, xtts_vocab)\n\t\n    # Clear GPU cache\n    torch.cuda.empty_cache()\n\n    # Check if the required paths are provided\n    if xtts_checkpoint is None or xtts_config is None or xtts_vocab is None:\n        return \"Please provide a checkpoint, config, and vocab path.\"\n\n    # Load the configuration from the provided path\n    xtts_config = XttsConfig.from_json_file(xtts_config)\n\n    # Initialize the XTTS model\n    model = Xtts(xtts_config)\n\n    # Load the model checkpoint and vocabulary\n    model.load_checkpoint(xtts_checkpoint)\n    model.load_vocab(xtts_vocab)\n\n    # Move the model to the GPU if available\n    if torch.cuda.is_available():\n        model.cuda()\n\n    # Print a message indicating the loading progress\n    print(\"Loading model...\")\n\n    # Return a success message once the model is loaded\n    return \"Model loaded successfully!\"\n\n", "CodeBleu_score": 45.83797379154825}, {"code": "def load_model(xtts_checkpoint, xtts_config, xtts_vocab)\n\t\n    torch.cuda.empty_cache()\n    if xtts_checkpoint is None:\n        print(\"Please provide a valid checkpoint path!\")\n        return\n    if xtts_config is None:\n        print(\"Please provide a valid config path!\")\n        return\n    if xtts_vocab is None:\n        print(\"Please provide a valid vocab path!\")\n        return\n    print(\"Loading XTTS model...\")\n    config = XttsConfig.from_json_file(xtts_config)\n    model = Xtts(config)\n    model.load_state_dict(torch.load(xtts_checkpoint, map_location=\"cpu\"))\n    model.load_vocab(xtts_vocab)\n    if torch.cuda.is_available():\n        model.cuda()\n    print(\"XTTS model loaded successfully!\")\n    return model\n\n", "CodeBleu_score": 50.36104794864668}, {"code": "def load_model(xtts_checkpoint, xtts_config, xtts_vocab)\n\t\n    try:\n        torch.cuda.empty_cache()\n        if xtts_checkpoint is None:\n            raise ValueError(\"Please provide a valid checkpoint path.\")\n        if xtts_config is None:\n            raise ValueError(\"Please provide a valid config path.\")\n        if xtts_vocab is None:\n            raise ValueError(\"Please provide a valid vocab path.\")\n        config = XttsConfig.from_json_file(xtts_config)\n        model = Xtts(config)\n        model.load_state_dict(torch.load(xtts_checkpoint, map_location=\"cpu\"))\n        if torch.cuda.is_available():\n            model = model.cuda()\n        print(\"Model loaded successfully.\")\n        return model\n    except Exception as e:\n        print(e)\n        print(\"Error occurred while loading the model.\")\n\n", "CodeBleu_score": 43.49885970073115}]}
{"instance_id": "c5d63aa0df26", "generation_result": [{"code": "def compute_embedding(self, x, num_frames=250, num_eval=10, return_mean=True, l2_norm=True)\n\t\n    if isinstance(x, torch.Tensor):\n        x_len = x.shape[-1]\n    else:\n        x_len = x.shape[0]\n    hop_length = self.hop_length\n    if self.use_torch:\n        hop_length = self.hop_length // self.sample_rate\n    if x_len < num_frames * hop_length:\n        num_frames = x_len // hop_length\n    if num_frames < 1:\n        num_frames = 1\n    offsets = np.linspace(0, x_len - num_frames * hop_length, num_eval, dtype=int)\n    embeddings = []\n    for offset in offsets:\n        if isinstance(x, torch.Tensor):\n            x_batch = x[..., offset : offset + num_frames * hop_length : hop_length]\n        else:\n            x_batch = x[offset : offset + num_frames * hop_length : hop_length]\n        if self.use_torch:\n            x_batch = torch.from_numpy(x_batch).to(self.device)\n            x_batch = x_batch.unsqueeze(0)\n        else:\n            x_batch = np.expand_dims(x_batch, axis=0)\n        embedding = self.forward(x_batch)\n        if self.use_torch:\n            embedding = embedding.squeeze(0).cpu().numpy()\n        embeddings.append(embedding)\n    embeddings = np.stack(embeddings)\n    if return_mean:\n        embeddings = np.mean(embeddings, axis=0)\n        if l2_norm:\n            embeddings = embeddings / np.linalg.norm(embeddings)\n    return embeddings\n\n", "CodeBleu_score": 37.567109061055405}, {"code": "def compute_embedding(self, x, num_frames=250, num_eval=10, return_mean=True, l2_norm=True)\n\t\n    if self.use_torch:\n        num_frames = (\n            self.hop_length * num_frames\n        )  # adjust number of frames based on hop length if using torch\n    if num_frames > x.shape[1]:\n        num_frames = x.shape[1]  # ensure number of frames does not exceed input length\n    offsets = np.linspace(\n        0, x.shape[1] - num_frames, num_eval, dtype=int\n    )  # calculate offset positions for evaluation\n    embeddings = []\n    for offset in offsets:\n        # extract frames at these offsets, concatenate into a batch, and perform inference\n        batch = x[:, offset : offset + num_frames]\n        batch = torch.from_numpy(batch).to(self.device)\n        batch = batch.unsqueeze(0)\n        with torch.no_grad():\n            emb = self.model.forward(batch)\n        embeddings.append(emb.detach().cpu().numpy())\n    embeddings = np.concatenate(embeddings, axis=1)\n    if return_mean:\n        embeddings = np.mean(embeddings, axis=1)  # compute mean of embeddings\n        if l2_norm:\n            embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)\n    return embeddings\n\n", "CodeBleu_score": 40.02779110702715}, {"code": "def compute_embedding(self, x, num_frames=250, num_eval=10, return_mean=True, l2_norm=True)\n\t\n    if self.torch_spec:\n        hop_length = self.hparams.hop_length\n        num_frames = int(num_frames * self.hparams.sampling_rate / hop_length)\n    else:\n        num_frames = int(num_frames)\n    if num_frames * self.hparams.hop_length > x.shape[-1]:\n        num_frames = x.shape[-1] // self.hparams.hop_length\n    num_frames = num_frames - num_frames % num_eval\n    offsets = torch.linspace(0, num_frames - num_eval, num_eval, dtype=torch.long)\n    x = x.unsqueeze(0).expand(num_eval, -1, -1)\n    x = x[:, :, offsets * self.hparams.hop_length:(offsets + num_frames) * self.hparams.hop_length]\n    x = x.contiguous().view(num_eval * num_frames, -1)\n    x = self.preprocess(x)\n    x = x.unsqueeze(1).expand(-1, self.hparams.n_mels, -1)\n    x = self.encoder(x)\n    x = x.transpose(1, 2)\n    x = self.projector(x)\n    x = x.transpose(1, 2)\n    x = self.postprocess(x)\n    x = x.view(num_eval, num_frames, -1)\n    x = x.mean(1)\n    if return_mean:\n        x = x.mean(0)\n    if l2_norm:\n        x = F.normalize(x, p=2, dim=-1)\n    return x\n\n", "CodeBleu_score": 34.40509416015075}]}
{"instance_id": "06f5b0335b26", "generation_result": [{"code": "def create_layer(self, block, planes, blocks, stride=1)\n\t\n    downsample = None\n    if stride!= 1 or self.inplanes!= planes * block.expansion:\n        downsample = nn.Sequential(\n            nn.Conv2d(\n                self.inplanes,\n                planes * block.expansion,\n                kernel_size=1,\n                stride=stride,\n                bias=False,\n            ),\n            nn.BatchNorm2d(planes * block.expansion),\n        )\n\n    layers = []\n    layers.append(block(self.inplanes, planes, stride, downsample))\n    self.inplanes = planes * block.expansion\n    for _ in range(1, blocks):\n        layers.append(block(self.inplanes, planes))\n\n    return nn.Sequential(*layers)\n\n", "CodeBleu_score": 82.34750668917684}, {"code": "def create_layer(self, block, planes, blocks, stride=1)\n\t\n    downsample = None\n    if stride!= 1 or self.inplanes!= planes * block.expansion:\n        downsample = nn.Sequential(\n            nn.Conv2d(\n                self.inplanes, planes * block.expansion, kernel_size=1, stride=stride, bias=False\n            ),\n            nn.BatchNorm2d(planes * block.expansion),\n        )\n\n    layers = []\n    layers.append(block(self.inplanes, planes, stride, downsample))\n    self.inplanes = planes * block.expansion\n    for i in range(1, blocks):\n        layers.append(block(self.inplanes, planes))\n\n    return nn.Sequential(*layers)\n\n", "CodeBleu_score": 81.19621256302787}, {"code": "def create_layer(self, block, planes, blocks, stride=1)\n\t\n    downsample = None\n    if stride!= 1 or self.inplanes!= planes * block.expansion:\n        downsample = nn.Sequential(\n            nn.Conv2d(\n                self.inplanes, planes * block.expansion, kernel_size=1, stride=stride, bias=False\n            ),\n            nn.BatchNorm2d(planes * block.expansion),\n        )\n    layers = []\n    layers.append(block(self.inplanes, planes, stride, downsample))\n    self.inplanes = planes * block.expansion\n    for i in range(1, blocks):\n        layers.append(block(self.inplanes, planes))\n    return nn.Sequential(*layers)\n\n", "CodeBleu_score": 81.19621256302787}]}
{"instance_id": "9ac60597b633", "generation_result": [{"code": "def plot_embeddings(embeddings, num_classes_in_batch)\n\t\n    # Limit the number of classes to 10 if necessary\n    if num_classes_in_batch > 10:\n        num_classes_in_batch = 10\n\n    # Calculate the number of utterances per class\n    num_utterances_per_class = embeddings.shape[0] // num_classes_in_batch\n\n    # Create a list of colors for each class\n    colors = matplotlib.cm.get_cmap(\"tab10\").colors[:num_classes_in_batch]\n\n    # Create a list of labels for each class\n    labels = [f\"Class {i}\" for i in range(num_classes_in_batch)]\n\n    # Create a list of markers for each class\n    markers = [\"o\", \"s\", \"d\", \"v\", \"^\", \"<\", \">\", \"8\", \"p\", \"P\"]\n\n    # Initialize a figure and axis\n    fig, ax = plt.subplots()\n\n    # Iterate over each class and plot the corresponding embeddings\n    for i in range(num_classes_in_batch):\n        start_idx = i * num_utterances_per_class\n        end_idx = (i + 1) * num_utterances_per_class\n        embeddings_class = embeddings[start_idx:end_idx]\n        color = colors[i]\n        label = labels[i]\n        marker = markers[i]\n        ax.scatter(\n            embeddings_class[:, 0],\n            embeddings_class[:, 1],\n            color=color,\n            label=label,\n            marker=marker,\n        )\n\n    # Add a legend\n    ax.legend()\n\n    # Add a title\n    ax.set_title(\"UMAP projection\")\n\n    # Adjust for equal aspect ratio\n    ax.set_aspect(\"equal\")\n\n    # Save the figure as \"umap.png\"\n    fig.savefig(\"umap.png\")\n\n    # Return the figure\n    return fig\n\n", "CodeBleu_score": 44.44936207543742}, {"code": "def plot_embeddings(embeddings, num_classes_in_batch)\n\t\n    # Calculate the number of utterances per class\n    num_utterances_per_class = num_classes_in_batch\n    # If there are more than 10 classes, limit the number of classes to 10\n    if num_classes_in_batch > 10:\n        num_classes_in_batch = 10\n        num_utterances_per_class = int(num_utterances_per_class / 10)\n    # Create a UMAP object\n    reducer = umap.UMAP()\n    # Fit the UMAP object to the embeddings\n    umap_embeddings = reducer.fit_transform(embeddings)\n    # Create a scatter plot of the UMAP embeddings\n    fig, ax = plt.subplots()\n    for i in range(num_classes_in_batch):\n        ax.scatter(\n            umap_embeddings[i * num_utterances_per_class : (i + 1) * num_utterances_per_class, 0],\n            umap_embeddings[i * num_utterances_per_class : (i + 1) * num_utterances_per_class, 1],\n            label=f\"Class {i}\",\n        )\n    # Set the title of the plot\n    ax.set_title(\"UMAP projection\")\n    # Set the aspect ratio of the plot to be equal\n    ax.set_aspect(\"equal\", adjustable=\"box\")\n    # Add a legend to the plot\n    ax.legend()\n    # Save the plot as an image file\n    fig.savefig(\"umap.png\")\n    # Return the figure\n    return fig\n\n", "CodeBleu_score": 39.274067831825924}, {"code": "def plot_embeddings(embeddings, num_classes_in_batch)\n\t\n    # Calculate the number of utterances per class\n    num_utterances = embeddings.shape[0] // num_classes_in_batch\n    # Limit the number of classes to 10 if necessary\n    num_classes = min(num_classes_in_batch, 10)\n    # Create a scatter plot with colors representing different classes\n    fig = plt.figure(figsize=(10, 10))\n    for i in range(num_classes):\n        start_idx = i * num_utterances\n        end_idx = (i + 1) * num_utterances\n        plt.scatter(\n            embeddings[start_idx:end_idx, 0],\n            embeddings[start_idx:end_idx, 1],\n            label=f\"Class {i}\",\n        )\n    plt.title(\"UMAP projection\")\n    plt.gca().set_aspect(\"equal\", \"datalim\")\n    plt.legend()\n    plt.savefig(\"umap.png\")\n    return fig\n\n", "CodeBleu_score": 30.572634638074348}]}
{"instance_id": "19c2b4af256d", "generation_result": [{"code": "def calc_cosine_sim(self, dvecs, centroids)\n\t\n    cos_sim_mat = []\n    for spk in range(len(dvecs)):\n        spk_centroids = centroids[spk]\n        for utt in range(len(dvecs[spk])):\n            utt_dvec = dvecs[spk][utt]\n            spk_centroids_new = torch.cat(\n                (spk_centroids[:utt], spk_centroids[utt + 1 :]), dim=0\n            )\n            cos_sim = torch.nn.functional.cosine_similarity(\n                utt_dvec.unsqueeze(0).repeat(len(spk_centroids_new), 1), spk_centroids_new\n            )\n            cos_sim = torch.clamp(cos_sim, min=0.0)\n            cos_sim_mat.append(cos_sim)\n    cos_sim_mat = torch.stack(cos_sim_mat)\n    return cos_sim_mat\n\n", "CodeBleu_score": 30.80330382097251}, {"code": "def calc_cosine_sim(self, dvecs, centroids)\n\t\n    sim_matrix = torch.zeros((dvecs.shape[0], centroids.shape[0]), dtype=torch.float32, device=self.device)\n    for i, speaker in enumerate(self.speakers):\n        speaker_dvecs = dvecs[i * self.num_utterances:(i + 1) * self.num_utterances]\n        speaker_centroids = centroids[i * self.num_utterances:(i + 1) * self.num_utterances]\n        for j, dvec in enumerate(speaker_dvecs):\n            new_centroids = torch.cat((speaker_centroids[:j], speaker_centroids[j + 1:]))\n            sim_matrix[i * self.num_utterances + j] = F.cosine_similarity(dvec.unsqueeze(0), new_centroids, dim=1).clamp(min=0.00001)\n    return sim_matrix\n\n", "CodeBleu_score": 29.334688822058897}, {"code": "def calc_cosine_sim(self, dvecs, centroids)\n\t\n    cosine_sim = []\n    for i, centroid in enumerate(centroids):\n        new_centroids = torch.cat((centroids[:i], centroids[i + 1 :]), dim=0)\n        new_centroids = new_centroids / torch.norm(new_centroids, dim=1, keepdim=True)\n        sim = torch.mm(dvecs[i], new_centroids.t())\n        sim = torch.clamp(sim, min=1e-10)\n        cosine_sim.append(sim)\n    cosine_sim = torch.cat(cosine_sim, dim=1)\n    return cosine_sim\n\n", "CodeBleu_score": 29.103810518697205}]}
{"instance_id": "c525b411a1e7", "generation_result": [{"code": " def embed_loss_softmax(self, dvecs, cos_sim_matrix)\n\t\n    losses = []\n    for i in range(len(dvecs)):\n        dvec = dvecs[i]\n        dvec = dvec.unsqueeze(0)\n        dvec = dvec.expand_as(cos_sim_matrix)\n        dvec = dvec.reshape(-1, 1)\n        dvec = dvec.expand_as(cos_sim_matrix)\n        dvec = dvec.reshape(-1, 1)\n        dvec = dvec.expand_as(cos_sim_matrix)\n        dvec = dvec.reshape(-1, 1)\n        dvec = dvec.expand_as(cos_sim_matrix)\n        dvec = dvec.reshape(-1, 1)\n        dvec = dvec.expand_as(cos_sim_matrix)\n        dvec = dvec.reshape(-1, 1)\n        dvec = dvec.expand_as(cos_sim_matrix)\n        dvec = dvec.reshape(-1, 1)\n        dvec = dvec.expand_as(cos_sim_matrix)\n        dvec = dvec.reshape(-1, 1)\n        dvec = dvec.expand_as(cos_sim_matrix)\n        dvec = dvec.reshape(-1, 1)\n        dvec = dvec.expand_as(cos_sim_matrix)\n        dvec = dvec.reshape(-1, 1)\n        dvec = dvec.expand_as(cos_sim_matrix)\n        dvec = dvec.reshape(-1, 1)\n        dvec = dvec.expand_as(cos_sim_matrix)\n        dvec = dvec.reshape(-1, 1)\n        dvec = dvec.expand_as(cos_sim_matrix)\n        dvec = dvec.reshape(-1, 1)\n        dvec = dvec.expand_as(cos_sim_matrix)\n        dvec = dvec.reshape(-1, 1)\n        dvec = dvec.expand_as(cos_sim_matrix)\n        dvec = dvec.reshape(-1, 1)\n        dvec = dvec.expand_as(cos_sim_matrix)\n        dvec = dvec.reshape(-1, 1)\n        dvec = dvec.expand_as(cos", "CodeBleu_score": 18.8516752964088}, {"code": " def embed_loss_softmax(self, dvecs, cos_sim_matrix)\n\t\n    loss_fn = nn.CrossEntropyLoss(reduction=\"none\")\n    loss_matrix = torch.zeros(dvecs.shape[0], device=dvecs.device)\n    for i, dvec in enumerate(dvecs):\n        loss_matrix[i] = loss_fn(cos_sim_matrix[i], torch.tensor(i, device=dvec.device))\n    return loss_matrix.mean()\n", "CodeBleu_score": 25.08310189251443}, {"code": " def embed_loss_softmax(self, dvecs, cos_sim_matrix)\n\t\n    losses = []\n    for i in range(len(dvecs)):\n        # Compute the negative log softmax of the cosine similarity matrix for each embedding.\n        loss = -torch.nn.functional.log_softmax(cos_sim_matrix[i], dim=0)\n        losses.append(loss)\n    return torch.stack(losses)\n\n def embed_loss_cosine(self, dvecs, cos_sim_matrix):\n    \"\"\"The function calculates the embedding loss using cosine similarity.", "CodeBleu_score": 33.13777789718382}]}
{"instance_id": "25ac3945da2e", "generation_result": [{"code": "def embed_loss_contrast(self, dvecs, cos_sim_matrix)\n\t\n    loss_row = []\n    for i in range(len(dvecs)):\n        sim_row = cos_sim_matrix[i]\n        sim_row = sim_row.masked_fill(sim_row == 1, -1)\n        loss_row.append(torch.nn.functional.logsigmoid(sim_row).sum())\n    loss_row = torch.stack(loss_row)\n    return loss_row\n\n", "CodeBleu_score": 22.159971024207376}, {"code": "def embed_loss_contrast(self, dvecs, cos_sim_matrix)\n\t\n    loss = []\n    for i in range(len(dvecs)):\n        sim = torch.sigmoid(cos_sim_matrix[i])\n        sim[i] = 0\n        loss.append(torch.mean(sim))\n    loss = torch.stack(loss)\n    return loss\n\n", "CodeBleu_score": 20.603896161287167}, {"code": "def embed_loss_contrast(self, dvecs, cos_sim_matrix)\n\t\n    loss_row = []\n    for i, dvec in enumerate(dvecs):\n        sigmoid_cos_sim = torch.sigmoid(cos_sim_matrix[i])\n        sigmoid_cos_sim[i] = 0\n        loss_row.append(torch.log(1 - sigmoid_cos_sim[i]).sum())\n    return torch.stack(loss_row)\n\n", "CodeBleu_score": 19.977104226909244}]}
{"instance_id": "878d14ffabd2", "generation_result": [{"code": "def forward(self, x, _label=None)\n\t\n    # x.shape = (num_speakers, num_utts_per_speaker, dvec_feats)\n    # Check that input is of the correct shape\n    assert x.dim() == 3, \"Input must be of dimension (num_speakers, num_utts_per_speaker, dvec_feats)\"\n    # Check that input has at least two utterances per speaker\n    assert x.shape[1] > 1, \"Each speaker must have at least two utterances\"\n    # Calculate centroids for each speaker\n    centroids = torch.mean(x, dim=1)\n    # Calculate cosine similarity matrix between embeddings and centroids\n    cos_sim_matrix = F.cosine_similarity(x.view(-1, x.shape[-1]), centroids.view(-1, x.shape[-1]), dim=1)\n    # Reshape cosine similarity matrix to be of shape (num_speakers, num_utts_per_speaker, num_speakers)\n    cos_sim_matrix = cos_sim_matrix.view(x.shape[0], x.shape[1], x.shape[0])\n    # Scale and shift cosine similarity matrix\n    scaled_cos_sim_matrix = self.w * cos_sim_matrix + self.b\n    # Calculate embedding loss\n    embedding_loss = F.cross_entropy(scaled_cos_sim_matrix, torch.arange(x.shape[0]).repeat_interleave(x.shape[1]))\n    # Return mean loss\n    return embedding_loss.mean()\n\n", "CodeBleu_score": 44.52060051336979}, {"code": "def forward(self, x, _label=None)\n\t\n    # Check if there are at least two utterances per speaker\n    if x.shape[1] < 2:\n        raise ValueError(\"Number of utterances per speaker must be at least two\")\n\n    # Compute centroids by averaging the embeddings\n    centroids = x.mean(dim=1)\n\n    # Calculate cosine similarity matrix between the embeddings and centroids\n    cos_sim_matrix = F.cosine_similarity(x.view(-1, x.shape[-1]), centroids.view(-1, x.shape[-1]), dim=1)\n    cos_sim_matrix = cos_sim_matrix.view(x.shape[0], x.shape[1], -1)\n\n    # Scale and shift the similarity matrix\n    scaled_cos_sim_matrix = (cos_sim_matrix - self.cosine_sim_threshold) / (1 - self.cosine_sim_threshold)\n\n    # Calculate the embedding loss\n    embedding_loss = (1 - scaled_cos_sim_matrix) ** 2\n\n    # Calculate the mean loss\n    mean_loss = embedding_loss.mean()\n\n    return mean_loss", "CodeBleu_score": 32.50339884684049}, {"code": "def forward(self, x, _label=None)\n\t\n    # x = x.squeeze()\n    num_speakers, num_utts_per_speaker, dvec_feats = x.shape\n    # print(x.shape)\n    # print(num_speakers, num_utts_per_speaker, dvec_feats)\n    # print(x.shape)\n    # print(x.shape)\n    # print(x.shape)\n    # print(x.shape)\n    # print(x.shape)\n    # print(x.shape)\n    # print(x.shape)\n    # print(x.shape)\n    # print(x.shape)\n    # print(x.shape)\n    # print(x.shape)\n    # print(x.shape)\n    # print(x.shape)\n    # print(x.shape)\n    # print(x.shape)\n    # print(x.shape)\n    # print(x.shape)\n    # print(x.shape)\n    # print(x.shape)\n    # print(x.shape)\n    # print(x.shape)\n    # print(x.shape)\n    # print(x.shape)\n    # print(x.shape)\n    # print(x.shape)\n    # print(x.shape)\n    # print(x.shape)\n    # print(x.shape)\n    # print(x.shape)\n    # print(x.shape)\n    # print(x.shape)\n    # print(x.shape)\n    # print(x.shape)\n    # print(x.shape)\n    # print(x.shape)\n    # print(x.shape)\n    # print(x.shape)\n    # print(x.shape)\n    # print(x.shape)\n    # print(x.shape)\n    # print(x.shape)\n    # print(x.shape)\n    # print(x.shape)\n    # print(x.shape)\n    # print(x.shape)\n    # print(x.shape)\n    # print(x.shape)\n    # print(x.shape)\n    # print(x.shape)\n    # print(x.shape)\n    # print(x.shape)\n    # print(x.shape)\n    # print(x.shape)\n    # print(x.shape)\n    # print(x.shape)\n    # print(x.shape)\n    #", "CodeBleu_score": 5.844820010456416}]}
{"instance_id": "86a364c72caa", "generation_result": [{"code": "def get_voices(extra_voice_dirs: List[str] = [])\n\toices = {}\n    for voice_dir in extra_voice_dirs:\n        for dirpath, _, filenames in os.walk(voice_dir):\n            if len(filenames) == 0:\n                continue\n            if dirpath.endswith(\"npz\"):\n                continue\n            if len(glob(os.path.join(dirpath, \"*.npz\"))) > 0:\n                voices[dirpath] = glob(os.path.join(dirpath, \"*.npz\"))\n            else:\n                voices[dirpath] = glob(os.path.join(dirpath, \"*.wav\"))\n                voices[dirpath] += glob(os.path.join(dirpath, \"*.mp3\"))\n    return voices\n\n", "CodeBleu_score": 28.619433625220896}, {"code": "def get_voices(extra_voice_dirs: List[str] = [])\n\toices = {}\n    for voice_dir in extra_voice_dirs:\n        for dirpath, dirnames, filenames in os.walk(voice_dir):\n            if not dirnames:\n                if \".npz\" in filenames:\n                    voices[dirpath] = [os.path.join(dirpath, f) for f in filenames if f.endswith(\".npz\")]\n                elif \".wav\" in filenames:\n                    voices[dirpath] = [os.path.join(dirpath, f) for f in filenames if f.endswith(\".wav\")]\n                elif \".mp3\" in filenames:\n                    voices[dirpath] = [os.path.join(dirpath, f) for f in filenames if f.endswith(\".mp3\")]\n    return voices\n\n", "CodeBleu_score": 29.789849998181545}, {"code": "def get_voices(extra_voice_dirs: List[str] = [])\n\toices = {}\n    for root, dirs, files in os.walk(\"voices\"):\n        for dir_ in dirs:\n            dir_path = os.path.join(root, dir_)\n            npz_files = glob(f\"{dir_path}/*.npz\")\n            wav_files = glob(f\"{dir_path}/*.wav\")\n            mp3_files = glob(f\"{dir_path}/*.mp3\")\n            if len(npz_files) > 0:\n                voices[dir_] = npz_files\n            elif len(wav_files) > 0:\n                voices[dir_] = wav_files\n            elif len(mp3_files) > 0:\n                voices[dir_] = mp3_files\n    for extra_voice_dir in extra_voice_dirs:\n        for root, dirs, files in os.walk(extra_voice_dir):\n            for dir_ in dirs:\n                dir_path = os.path.join(root, dir_)\n                npz_files = glob(f\"{dir_path}/*.npz\")\n                wav_files = glob(f\"{dir_path}/*.wav\")\n                mp3_files = glob(f\"{dir_path}/*.mp3\")\n                if len(npz_files) > 0:\n                    voices[dir_] = npz_files\n                elif len(wav_files) > 0:\n                    voices[dir_] = wav_files\n                elif len(mp3_files) > 0:\n                    voices[dir_] = mp3_files\n    return voices\n\n", "CodeBleu_score": 26.586001466505582}]}
{"instance_id": "02f20c8917e6", "generation_result": [{"code": "def load_voice(model, voice: str, extra_voice_dirs: List[str] = [])\n\t   if voice == \"random\":\n        return None, None, None, None, None, None\n    voice_dirs = [os.path.join(os.path.dirname(model.config.get_path()), \"voices\")] + extra_voice_dirs\n    voice_paths = get_voices(voice, voice_dirs)\n    if len(voice_paths) > 1:\n        raise ValueError(f\"Multiple voices found for {voice} in {voice_dirs}\")\n    voice_path = voice_paths[0]\n    if not os.path.exists(voice_path):\n        raise ValueError(f\"Voice {voice} not found in {voice_dirs}\")\n    if voice_path.endswith(\".npz\"):\n        voice_data = np.load(voice_path)\n        return (\n            voice_data[\"mel\"],\n            voice_data[\"pitch\"],\n            voice_data[\"energy\"],\n            voice_data[\"duration\"],\n            voice_data[\"pitch_contour\"],\n            voice_data[\"energy_contour\"],\n        )\n    else:\n        audio, sr = librosa.load(voice_path, sr=model.config.sampling_rate)\n        audio = convert_audio(audio, sr, model.config.sampling_rate)\n        with inference_mode(model):\n            with clear_cuda_cache():\n                mel, pitch, energy, duration, pitch_contour, energy_contour = model.inference(\n                    audio, None, None, None, None\n                )\n        np.savez(\n            voice_path.replace(\".wav\", \".npz\"),\n            mel=mel,\n            pitch=pitch,\n            energy=energy,\n            duration=duration,\n            pitch_contour=pitch_contour,\n            energy_contour=energy_contour,\n        )\n        return (\n            mel,\n            pitch,\n            energy,\n            duration,\n            pitch_contour,\n            energy_contour,\n        )\n\n", "CodeBleu_score": 41.52128572450241}, {"code": "def load_voice(model, voice: str, extra_voice_dirs: List[str] = [])\n\t   if voice == \"random\":\n        return None, None, None, None, None, None\n    voice_dirs = [os.path.join(os.path.dirname(__file__), \"voices\", voice)]\n    voice_dirs += extra_voice_dirs\n    voice_paths = get_voices(voice_dirs)\n    if len(voice_paths) > 1:\n        raise ValueError(f\"Multiple voices found for {voice}, please specify one of {voice_paths}\")\n    voice_path = voice_paths[0]\n    if voice_path.endswith(\".npz\"):\n        voice_data = np.load(voice_path)\n        voice_data = {\n            key: value if not key.startswith(\"mel_\") else np.stack(value)\n            for key, value in voice_data.items()\n        }\n        return voice_data\n    else:\n        if not os.path.exists(voice_path):\n            raise ValueError(f\"Voice not found at {voice_path}\")\n        voice_data = load_voice(model, voice_path)\n        np.savez(voice_path + \".npz\", **voice_data)\n        return load_voice(model, voice_path + \".npz\")\n\n", "CodeBleu_score": 47.506607638903645}, {"code": "def load_voice(model, voice: str, extra_voice_dirs: List[str] = [])\n\t   if voice == \"random\":\n        return None\n    voice_paths = get_voices(voice, extra_voice_dirs)\n    if len(voice_paths) > 1:\n        raise ValueError(f\"Multiple voices found for {voice}.\")\n    voice_path = voice_paths[0]\n    if os.path.exists(voice_path):\n        if voice_path.endswith(\".npz\"):\n            return np.load(voice_path)[\"arr_0\"]\n        else:\n            return load_voice(model, voice_path, extra_voice_dirs)\n    else:\n        raise ValueError(f\"Voice {voice} not found.\")\n\n", "CodeBleu_score": 36.234363217025155}]}
{"instance_id": "9458e981389f", "generation_result": [{"code": "def _relative_shift(self, pos_score: torch.Tensor) -> torch.Tensor\n\t\n    batch_size, num_heads, length, dim = pos_score.size()\n    pos_score = pos_score.view(batch_size, num_heads, length, 1, dim)\n    pos_score = torch.cat((torch.zeros_like(pos_score), pos_score), dim=3)\n    pos_score = pos_score.view(batch_size, num_heads, length, length + 1, dim)\n    pos_score = pos_score[:, :, :, 1:, :]\n    return pos_score\n\n", "CodeBleu_score": 29.824860982890698}, {"code": "def _relative_shift(self, pos_score: torch.Tensor) -> torch.Tensor\n\t\n    batch_size, length, channels = pos_score.shape\n    pos_score = pos_score.unsqueeze(3)\n    pos_score = pos_score.reshape(batch_size, length, channels, 1)\n    pos_score = pos_score[:, :, :, 1:]\n    pos_score = pos_score.reshape(batch_size, length, channels)\n    return pos_score\n\n", "CodeBleu_score": 22.239633502735355}, {"code": "def _relative_shift(self, pos_score: torch.Tensor) -> torch.Tensor\n\t\n    batch_size, num_heads, length, dim = pos_score.size()\n    zero_tensor = torch.zeros(batch_size, num_heads, 1, dim, device=pos_score.device)\n    pos_score_pad = torch.cat([zero_tensor, pos_score], dim=-1)\n    pos_score_pad = pos_score_pad.view(batch_size, num_heads, length + 1, dim)\n    pos_score = pos_score_pad[:, :, 1:].view_as(pos_score)\n    return pos_score\n\n", "CodeBleu_score": 37.53571835869489}]}
{"instance_id": "c046d4f00e80", "generation_result": [{"code": "def run_padded_sequence(self, context, lens)\n\t\n    batch_size = context.shape[0]\n    max_len = context.shape[1]\n    context_len = lens.max().item()\n    context_pad = torch.zeros(batch_size, context_len, context.shape[2], device=context.device)\n    for i in range(batch_size):\n        context_pad[i, :lens[i], :] = context[i, :lens[i], :]\n    context_pad = context_pad.unsqueeze(1)\n    for layer in self.layers:\n        context_pad = layer(context_pad)\n    context_pad = context_pad.squeeze(1)\n    return context_pad\n\n", "CodeBleu_score": 24.24144603556585}, {"code": "def run_padded_sequence(self, context, lens)\n\t\n    # Extract the context up to the specified length for each batch element\n    context = context[:, :lens.max()]\n    # Apply the convolutional layers to the extracted context\n    context = self.conv_layers(context)\n    # Apply ReLU activation and dropout to the context\n    context = self.relu(context)\n    context = self.dropout(context)\n    # Pad the context to create a consistent batch size\n    context = torch.nn.functional.pad(context, (0, 0, 0, self.output_size - context.size(2)))\n    # Return the padded sequence\n    return context\n\n", "CodeBleu_score": 22.247700930498937}, {"code": "def run_padded_sequence(self, context, lens)\n\t\n    # Pad the context to the maximum length\n    context = torch.nn.functional.pad(context, (0, 0, 0, self.max_len - context.shape[1]))\n    # Run the padded sequence through the convolutional layers\n    for conv_layer in self.conv_layers:\n        context = conv_layer(context)\n    # Apply the kernel predictor to the context\n    context = self.kernel_predictor(context)\n    # Get the output shape of the kernel predictor\n    output_shape = context.shape[2:]\n    # Reshape the context to match the output shape of the kernel predictor\n    context = context.view(context.shape[0], context.shape[1], -1)\n    # Pad the context to the maximum length\n    context = torch.nn.functional.pad(context, (0, 0, 0, self.max_len - context.shape[1]))\n    # Reshape the context to match the output shape of the kernel predictor\n    context = context.view(-1, *output_shape)\n    # Return the padded sequence\n    return context\n\n", "CodeBleu_score": 29.4764798356305}]}
{"instance_id": "db2644f3c681", "generation_result": [{"code": "def run_unsorted_inputs(self, fn, context, lens)\n\t\n    # Sort the input based on the lengths\n    sorted_lens, indices = lens.sort(dim=0, descending=True)\n    sorted_context = context.index_select(dim=1, index=indices)\n    # Pack the sequences\n    packed_context = torch.nn.utils.rnn.pack_padded_sequence(sorted_context, sorted_lens, batch_first=True)\n    # Run the function on the packed sequences\n    packed_output = fn(packed_context)\n    # Unpack the sequences\n    output, _ = torch.nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n    # Map the output back to the original indices\n    _, reverse_indices = indices.sort(dim=0)\n    output = output.index_select(dim=1, index=reverse_indices)\n    return output\n\n", "CodeBleu_score": 17.110702733620606}, {"code": "def run_unsorted_inputs(self, fn, context, lens)\n\t\n    # sort the inputs based on their lengths\n    sorted_lens, indices = lens.sort(descending=True)\n    sorted_context = context.index_select(0, indices)\n    # pack the sequences\n    packed_context = torch.nn.utils.rnn.pack_padded_sequence(sorted_context, sorted_lens, enforce_sorted=True)\n    # run the function on the packed sequences\n    packed_output = fn(packed_context)\n    # unpack the sequences\n    unpacked_output, _ = torch.nn.utils.rnn.pad_packed_sequence(packed_output)\n    # map the results back to the original indices\n    _, original_indices = indices.sort()\n    output = unpacked_output.index_select(0, original_indices)\n    return output\n\n", "CodeBleu_score": 15.336116111115821}, {"code": "def run_unsorted_inputs(self, fn, context, lens)\n\t\n    # sort lengths\n    lens, indices = lens.sort(0, descending=True)\n    # sort context\n    context = context.index_select(0, indices)\n    # pack\n    context = torch.nn.utils.rnn.pack_padded_sequence(context, lens.cpu(), batch_first=True)\n    # run function\n    context = fn(context)\n    # unpack\n    context = torch.nn.utils.rnn.pad_packed_sequence(context, batch_first=True)[0]\n    # unsort\n    context = context.index_select(0, indices.flip([0]))\n    return context\n\n", "CodeBleu_score": 18.632755188006914}]}
{"instance_id": "ced082bffc63", "generation_result": [{"code": "def remove_weight_norm(self)\n\t\n    self.kernel_predictor.remove_weight_norm()\n    parametrize.remove_parametrizations_(self.convt_pre)\n    for block in self.conv_blocks:\n        parametrize.remove_parametrizations_(block)\n\n", "CodeBleu_score": 44.69943940808976}, {"code": "def remove_weight_norm(self)\n\t\n    if self.kernel_predictor is not None:\n        self.kernel_predictor.remove_weight_norm()\n    remove_weight_norm_from_conv_layer(self.convt_pre)\n    for conv_block in self.conv_blocks:\n        remove_weight_norm_from_conv_layer(conv_block)\n\n", "CodeBleu_score": 38.347908955814376}, {"code": "def remove_weight_norm(self)\n\t\n    if self.kernel_predictor is not None:\n        self.kernel_predictor.remove_weight_norm()\n    if self.convt_pre is not None:\n        self.convt_pre.remove_weight_norm()\n    for conv_block in self.conv_blocks:\n        conv_block.remove_weight_norm()\n\n", "CodeBleu_score": 38.02129882402877}]}
{"instance_id": "b5a39fd97061", "generation_result": [{"code": "def map_mx_to_torch_model(mx_name)\n\t   mx_name = mx_name.lower()\n    if mx_name.startswith(\"se_\"):\n        mx_name = \"gluon_\" + mx_name.replace(\"se_\", \"se\")\n    elif mx_name.startswith(\"senet_\"):\n        mx_name = \"gluon_\" + mx_name.replace(\"senet_\", \"se\")\n    elif mx_name.startswith(\"inceptionv3\"):\n        mx_name = \"gluon_\" + mx_name.replace(\"inceptionv3\", \"inception_v3\")\n    else:\n        mx_name = \"gluon_\" + mx_name\n    return mx_name\n\n", "CodeBleu_score": 44.96894243873017}, {"code": "def map_mx_to_torch_model(mx_name)\n\t   mx_name = mx_name.lower()\n    mx_name = mx_name.replace(\"se_\", \"\")\n    mx_name = mx_name.replace(\"senet_\", \"\")\n    mx_name = mx_name.replace(\"inceptionv3\", \"inception_v3\")\n    return \"gluon_\" + mx_name\n\n", "CodeBleu_score": 29.357865712003218}, {"code": "def map_mx_to_torch_model(mx_name)\n\t   name = mx_name.lower()\n    if name.startswith('se_'):\n        name = name.replace('se_', 'gluon_se_')\n    elif name.startswith('senet_'):\n        name = name.replace('senet_', 'gluon_se_')\n    elif name.startswith('inceptionv3'):\n        name = 'gluon_inception_v3'\n    else:\n        name = 'gluon_' + name\n    return name\n\n", "CodeBleu_score": 45.19851290468796}]}
{"instance_id": "7eee5efdbf67", "generation_result": [{"code": "def _num_samples_per_worker(self)\n\t\n    if self.num_replicas is None:\n        num_replicas = 1\n    else:\n        num_replicas = self.num_replicas\n    if self.num_samples is None:\n        num_samples = len(self.dataset)\n    else:\n        num_samples = self.num_samples\n    if self.batch_size is None:\n        batch_size = 1\n    else:\n        batch_size = self.batch_size\n    if self.batch_size is not None:\n        num_samples = int(math.ceil(num_samples / batch_size)) * batch_size\n    if self.num_workers is None:\n        num_workers = 0\n    else:\n        num_workers = self.num_workers\n    if num_workers == 0:\n        return num_samples\n    if self.drop_last:\n        num_samples = num_samples - num_samples % (num_workers * num_replicas)\n    if num_samples % (num_workers * num_replicas)!= 0:\n        num_samples = num_samples + num_workers * num_replicas - num_samples % (num_workers * num_replicas)\n    return num_samples // num_replicas\n\n", "CodeBleu_score": 31.442110261084228}, {"code": "def _num_samples_per_worker(self)\n\t\n    num_samples = self.num_samples\n    if self.num_repeats is not None:\n        num_samples = num_samples * self.num_repeats\n    if self.num_workers is not None:\n        num_samples = num_samples // self.num_workers\n    if self.batch_size is not None:\n        num_samples = math.ceil(num_samples / self.batch_size) * self.batch_size\n    return num_samples\n\n", "CodeBleu_score": 39.367596863207304}, {"code": "def _num_samples_per_worker(self)\n\t\n    if self.training or self.num_replicas > 1:\n        num_samples = math.ceil(self.num_samples / self.num_replicas)\n    else:\n        num_samples = self.num_samples\n    if self.batch_size is not None:\n        num_samples = num_samples * self.batch_size\n    return int(num_samples)\n\n", "CodeBleu_score": 33.949367583454396}]}
{"instance_id": "a698d65ae190", "generation_result": [{"code": "def find_images_and_targets(\n        folder: str,\n        types: Optional[Union[List, Tuple, Set]] = None,\n        class_to_idx: Optional[Dict] = None,\n        leaf_name_only: bool = True,\n        sort: bool = True\n)\n\t\n\n    # If no types are provided, set the default to include common image file extensions\n    if not types:\n        types = ['.jpg', '.jpeg', '.png', '.ppm', '.bmp', '.pgm', '.tif', '.tiff', '.webp']\n\n    # If no class-to-index mapping is provided, create an empty dictionary\n    if not class_to_idx:\n        class_to_idx = {}\n\n    # Initialize an empty list to store the image-path and class-index pairs\n    images = []\n\n    # Iterate over the files in the folder\n    for f in sorted(os.listdir(folder), key=natural_key):\n        # Skip files that are not in the allowed types\n        if not f.lower().endswith(tuple(types)):\n            continue\n\n        # If leaf_name_only is True, use only the last part of the file path as the class name\n        if leaf_name_only:\n            c = f.split(os.path.sep)[-1].split('.')[0]\n        # Otherwise, use the full file path as the class name\n        else:\n            c = f\n\n        # If the class name is not in the class-to-index mapping, add it\n        if c not in class_to_idx:\n            class_to_idx[c] = len(class_to_idx)\n\n        # Append the image-path and class-index pair to the list\n        images.append((os.path.join(folder, f), class_to_idx[c]))\n\n    # If sorting is enabled, sort the list by the class index\n    if sort:\n        images.sort(key=lambda x: x[1])\n\n    # Return the list of image-path and class-index pairs and the class-to-index mapping\n    return images, class_to_idx\n\n", "CodeBleu_score": 30.95733621257025}, {"code": "def find_images_and_targets(\n        folder: str,\n        types: Optional[Union[List, Tuple, Set]] = None,\n        class_to_idx: Optional[Dict] = None,\n        leaf_name_only: bool = True,\n        sort: bool = True\n)\n\t\n    if types is None:\n        types = ['.jpg', '.jpeg', '.png', '.ppm', '.bmp', '.pgm', '.tif']\n    images: List[str] = []\n    targets: List[int] = []\n    dir = os.path.expanduser(folder)\n    for root, _, fnames in sorted(os.walk(dir, followlinks=True)):\n        for fname in sorted(fnames, key=natural_key):\n            if leaf_name_only:\n                path = os.path.join(root, fname)\n            else:\n                path = os.path.join(root, fname)\n            if any(path.endswith(extension) for extension in types):\n                if class_to_idx is None:\n                    target = path.split('/')[-2]\n                    if target not in class_to_idx:\n                        class_to_idx[target] = len(class_to_idx)\n                else:\n                    target = class_to_idx[path.split('/')[-2]]\n                images.append(path)\n                targets.append(target)\n    if sort:\n        images, targets = zip(*sorted(zip(images, targets)))\n    return images, targets, class_to_idx", "CodeBleu_score": 37.875478603342756}, {"code": "def find_images_and_targets(\n        folder: str,\n        types: Optional[Union[List, Tuple, Set]] = None,\n        class_to_idx: Optional[Dict] = None,\n        leaf_name_only: bool = True,\n        sort: bool = True\n)\n\t\n    if types is None:\n        types = ['.jpg', '.jpeg', '.png', '.ppm', '.bmp', '.pgm', '.tif']\n    images = []\n    targets = []\n    for root, _, files in sorted(os.walk(folder, followlinks=True)):\n        for fname in sorted(files, key=natural_key):\n            if not fname.lower().endswith(types):\n                continue\n            if leaf_name_only:\n                path = os.path.join(root, fname)\n                item = os.path.basename(path)\n            else:\n                path = os.path.join(root, fname)\n                item = path\n            images.append(path)\n            parts = root.split(os.path.sep)\n            if class_to_idx is None:\n                if len(parts) == 1:\n                    target = parts[0]\n                else:\n                    target = os.path.join(*parts[1:])\n            else:\n                target = class_to_idx[os.path.basename(root)]\n            targets.append(target)\n    if class_to_idx is None:\n        class_to_idx = {cls: i for i, cls in enumerate(sorted(set(targets)))}\n    targets = [class_to_idx[target] for target in targets]\n    if sort:\n        images, targets = zip(*sorted(zip(images, targets), key=lambda p: p[0]))\n    return list(images), list(targets), class_to_idx\n\n", "CodeBleu_score": 42.063637279595845}]}
{"instance_id": "e7fe43bdecdc", "generation_result": [{"code": "def pytorch_worker_seed()\n\t\n    worker_info = get_worker_info()\n    if worker_info is not None:\n        seed = worker_info.seed\n    else:\n        seed = os.environ.get(\"RANK\", 0)\n    return seed\n\n", "CodeBleu_score": 52.175710107172144}, {"code": "def pytorch_worker_seed()\n\t\n    worker_info = get_worker_info()\n    if worker_info is not None:\n        return worker_info.seed\n    else:\n        return torch.distributed.get_rank()\n\n", "CodeBleu_score": 53.8430821490384}, {"code": "def pytorch_worker_seed()\n\t\n    worker_info = get_worker_info()\n    if worker_info is not None:\n        return worker_info.seed\n    else:\n        return torch.distributed.get_rank()\n\n", "CodeBleu_score": 53.8430821490384}]}
{"instance_id": "0ffc7e7ded25", "generation_result": [{"code": "def solarize_add(img, add, thresh=128, **__)\n\t\n    if img.mode in (\"L\", \"RGB\"):\n        lut = [min(255, x + add) if x < thresh else x for x in range(256)]\n        return img.point(lut)\n    return img\n\n", "CodeBleu_score": 31.07424693233567}, {"code": "def solarize_add(img, add, thresh=128, **__)\n\t\n    if img.mode in (\"L\", \"RGB\"):\n        lut = [min(255, x + add) if x < thresh else x for x in range(256)]\n        return img.point(lut)\n    return img\n\n", "CodeBleu_score": 31.07424693233567}, {"code": "def solarize_add(img, add, thresh=128, **__)\n\t\n    if img.mode == \"L\":\n        lut = [min(x + add, 255) if x < thresh else x for x in range(256)]\n        return img.point(lut)\n    elif img.mode == \"RGB\":\n        lut = [min(x + add, 255) if x < thresh else x for x in range(256)]\n        return img.convert(\"RGB\", lut=lut)\n    else:\n        return img\n\n", "CodeBleu_score": 39.5511738428194}]}
{"instance_id": "5a0d29577a21", "generation_result": [{"code": "def _get_weighted_transforms(transforms: Dict)\n\t\n    transforms_list = list(transforms.keys())\n    probabilities = list(transforms.values())\n    probabilities = [p / sum(probabilities) for p in probabilities]\n    return transforms_list, probabilities\n\n", "CodeBleu_score": 21.39992132415359}, {"code": "def _get_weighted_transforms(transforms: Dict)\n\t\n    transforms_list = list(transforms.keys())\n    probabilities = list(transforms.values())\n    total = sum(probabilities)\n    probabilities = [p / total for p in probabilities]\n    return transforms_list, probabilities\n\n", "CodeBleu_score": 19.690347644331023}, {"code": "def _get_weighted_transforms(transforms: Dict)\n\t\n    transforms_list = list(transforms.keys())\n    probs = list(transforms.values())\n    probs_sum = sum(probs)\n    probs = [prob / probs_sum for prob in probs]\n    return transforms_list, probs\n\n", "CodeBleu_score": 23.489738604038617}]}
{"instance_id": "84bc5658c135", "generation_result": [{"code": "def _apply_basic(self, img, mixing_weights, m)\n\t\n    mixed_img = np.zeros(img.shape, dtype=img.dtype)\n    for w in mixing_weights:\n        if w == 0:\n            continue\n        # apply a random sequence of operations to the image\n        img_aug = self.apply_aug_list(img, m)\n        # accumulate the results\n        mixed_img = mixed_img + img_aug * w\n    # clip to valid pixel values\n    mixed_img = np.clip(mixed_img, 0, 255)\n    # convert back to an image\n    mixed_img = Image.fromarray(mixed_img.astype(np.uint8))\n    # blend with the original image\n    mixed_img = ImageChops.blend(mixed_img, Image.fromarray(img), self.mixing_factor)\n    return mixed_img\n\n", "CodeBleu_score": 29.231885513816763}, {"code": "def _apply_basic(self, img, mixing_weights, m)\n\t\n    mixed_img = np.zeros(img.shape, dtype=np.float32)\n    for i in range(len(mixing_weights)):\n        weight = mixing_weights[i]\n        seq = random.choice(m)\n        for op in seq:\n            img = op(img)\n        mixed_img += weight * img\n    mixed_img = np.clip(mixed_img, 0, 255)\n    mixed_img = Image.fromarray(mixed_img.astype(np.uint8))\n    return ImageChops.blend(img, mixed_img, self.alpha)\n\n", "CodeBleu_score": 30.380865246493844}, {"code": "def _apply_basic(self, img, mixing_weights, m)\n\t\n    mixed_img = np.zeros(img.shape, dtype=np.float32)\n    for weight in mixing_weights:\n        if weight == 0:\n            continue\n        if weight == 1:\n            mixed_img = img\n            break\n        if m.p == 1.0:\n            mixed_img += self.apply(img, m) * weight\n        else:\n            mixed_img += self.apply(img, m) * weight * m.p\n    mixed_img = np.clip(mixed_img, 0, 255).astype(np.uint8)\n    mixed_img = Image.fromarray(mixed_img)\n    return Image.blend(img, mixed_img, m.alpha)\n\n", "CodeBleu_score": 30.13268476023498}]}
{"instance_id": "84b5b752ff04", "generation_result": [{"code": "def adapt_to_chs(x, n)\n\t\n    if not isinstance(x, (tuple, list)):\n        x = tuple(repeat(x, n))\n    elif len(x)!= n:\n        logging.warning(f\"x has length {len(x)}, but n is {n}.\")\n        x = tuple(repeat(torch.mean(torch.stack(x)), n))\n    assert len(x) == n\n    return x\n\n", "CodeBleu_score": 50.19078831582827}, {"code": "def adapt_to_chs(x, n)\n\t\n    if not isinstance(x, (tuple, list)):\n        x = tuple(repeat(x, n))\n    elif len(x)!= n:\n        x = tuple(repeat(torch.mean(torch.stack(x, dim=0)), n))\n        logging.warning(f\"The length of x ({len(x)}) does not match the required number of channels ({n}). \"\n                        \"The mean of x has been repeated n times.\")\n    assert len(x) == n, f\"The length of x ({len(x)}) does not match the required number of channels ({n}).\"\n    return x\n\n", "CodeBleu_score": 53.372922009220204}, {"code": "def adapt_to_chs(x, n)\n\t\n    if not isinstance(x, (tuple, list)):\n        x = (x,) * n\n    elif len(x)!= n:\n        logging.warning(f\"Length of x ({len(x)}) does not match n ({n}). Repeating mean of x.\")\n        x = (torch.stack(x).mean(),) * n\n    assert len(x) == n, f\"Length of x ({len(x)}) does not match n ({n}).\"\n    return x\n\n", "CodeBleu_score": 46.76857608763317}]}
{"instance_id": "54754b6ba394", "generation_result": [{"code": "def _worker_init(worker_id, worker_seeding='all')\n\t\n    worker_info = torch.utils.data.get_worker_info()\n    if worker_info is None:\n        raise RuntimeError('Worker_init function was called on a non-worker process.')\n    seed = worker_info.seed\n    if worker_seeding == 'all':\n        torch.manual_seed(seed)\n        random.seed(seed)\n        np.random.seed(seed)\n    elif callable(worker_seeding):\n        worker_seeding(seed)\n\n", "CodeBleu_score": 40.36069632114351}, {"code": "def _worker_init(worker_id, worker_seeding='all')\n\t\n    worker_info = torch.utils.data.get_worker_info()\n    if worker_info is None:\n        raise RuntimeError('worker_init_fn cannot be used without the '\n                          'multiprocessing backend')\n    if worker_id >= worker_info.num_workers:\n        raise RuntimeError(f'worker_id {worker_id} is out of range '\n                           f'({worker_info.num_workers})')\n\n    if callable(worker_seeding):\n        worker_seeding(worker_id)\n    elif worker_seeding == 'all':\n        seed = worker_info.seed\n        torch.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n        random.seed(seed)\n        np.random.seed(seed)\n    else:\n        raise ValueError(f'worker_seeding must be a callable or \"all\", got {worker_seeding}')\n\n", "CodeBleu_score": 34.749151482527}, {"code": "def _worker_init(worker_id, worker_seeding='all')\n\t\n    worker_info = torch.utils.data.get_worker_info()\n    if worker_info is None:\n        raise ValueError(\"worker_init_fn cannot be used in a single-process setting\")\n    if worker_seeding == 'all':\n        seed = worker_info.seed\n        torch.manual_seed(seed)\n        np.random.seed(seed)\n    elif callable(worker_seeding):\n        seed = worker_seeding(worker_id)\n        torch.manual_seed(seed)\n        np.random.seed(seed)\n    elif isinstance(worker_seeding, int):\n        np.random.seed(worker_seeding)\n    else:\n        raise ValueError(\"worker_seeding should be a callable, 'all', or an integer\")\n\n", "CodeBleu_score": 47.01827219245017}]}
{"instance_id": "81dfdce940db", "generation_result": [{"code": "def _decode_and_center_crop(image_bytes, image_size, resize_method)\n\t\n    # Decode the image\n    image = tf.io.decode_jpeg(image_bytes, channels=3)\n    # Get the image shape\n    image_shape = tf.shape(image)\n    # Calculate the padded center crop size\n    padded_center_crop_size = tf.cast(\n        (image_size / CROP_PADDING) * tf.cast(tf.reduce_min(image_shape[:2]), tf.float32),\n        tf.int32,\n    )\n    # Calculate the offset for the crop window\n    offset_height = ((image_shape[0] - padded_center_crop_size) + 1) // 2\n    offset_width = ((image_shape[1] - padded_center_crop_size) + 1) // 2\n    # Crop the image\n    image = tf.image.crop_to_bounding_box(\n        image, offset_height, offset_width, padded_center_crop_size, padded_center_crop_size\n    )\n    # Resize the cropped image\n    image = tf.image.resize(image, [image_size, image_size], method=resize_method)\n    return image\n\n", "CodeBleu_score": 39.673681543195514}, {"code": "def _decode_and_center_crop(image_bytes, image_size, resize_method)\n\t\n    image = tf.image.decode_jpeg(image_bytes, channels=3)\n    image_shape = tf.shape(image)\n    height, width = image_shape[0], image_shape[1]\n    image_size_const = tf.constant(image_size, dtype=tf.int32)\n    resize_const = tf.constant(RESIZE_SIDE_MIN, dtype=tf.int32)\n    # Crop the central region of the image with padding\n    # Calculate the padded center crop size and offset\n    image_height = tf.cast(image_size_const / resize_const * resize_const, tf.int32)\n    if height > width:\n        padded_center_crop_size = tf.cast(width * image_height / height, tf.int32)\n    else:\n        padded_center_crop_size = image_height\n    offset_height = ((height - padded_center_crop_size) + 1) // 2\n    offset_width = ((width - padded_center_crop_size) + 1) // 2\n    image = tf.image.crop_to_bounding_box(\n        image, offset_height, offset_width, padded_center_crop_size, padded_center_crop_size\n    )\n    # Resize to the specified size with the specified method\n    if resize_method == \"area\":\n        image = tf.image.resize(image, [image_size_const, image_size_const], method=\"area\")\n    elif resize_method == \"lanczos3\":\n        image = tf.image.resize(image, [image_size_const, image_size_const], method=\"lanczos3\")\n    elif resize_method == \"lanczos5\":\n        image = tf.image.resize(image, [image_size_const, image_size_const], method=\"lanczos5\")\n    elif resize_method == \"bicubic\":\n        image = tf.image.resize(image, [image_size_const, image_size_const], method=\"bicubic\")\n    elif resize_method == \"gaussian\":\n        image = tf.image.resize(image, [image_size_const, image_size_const], method=\"gaussian\")\n    elif resize_method == \"nearest\":\n        image = tf.image.resize(image,", "CodeBleu_score": 38.58141701515796}, {"code": "def _decode_and_center_crop(image_bytes, image_size, resize_method)\n\t\n    image = tf.io.decode_jpeg(image_bytes, channels=3)\n    image_shape = tf.shape(image)\n    height, width = image_shape[0], image_shape[1]\n    padded_center_crop_size = tf.cast(\n        tf.cast(\n            (image_size / CROP_PADDING, image_size / CROP_PADDING),\n            tf.int32,\n        ),\n        tf.float32,\n    )\n    offset_height = (height - padded_center_crop_size[0]) // 2\n    offset_width = (width - padded_center_crop_size[1]) // 2\n    image = tf.image.crop_to_bounding_box(\n        image,\n        offset_height,\n        offset_width,\n        padded_center_crop_size[0],\n        padded_center_crop_size[1],\n    )\n    image = tf.image.resize(image, [image_size, image_size], method=resize_method)\n    return image\n\n", "CodeBleu_score": 27.901437633795755}]}
{"instance_id": "cf16a6d37111", "generation_result": [{"code": "def preprocess_for_train(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    # Decode the image\n    image = tf.io.decode_image(image_bytes, channels=3, dtype=tf.dtypes.uint8)\n    # Randomly crop the image\n    image = tf.image.random_crop(image, size=[image_size, image_size, 3])\n    # Flip the image horizontally\n    image = tf.image.random_flip_left_right(image)\n    # Resize the image\n    image = tf.image.resize(image, [image_size, image_size], method=interpolation)\n    # Convert the image to the specified dtype\n    image = tf.cast(image, dtype=tf.bfloat16 if use_bfloat16 else tf.float32)\n    # Normalize the image\n    image = image / 127.5 - 1.0\n    return image\n\n", "CodeBleu_score": 42.544613403688636}, {"code": "def preprocess_for_train(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    image = tf.io.decode_image(image_bytes, channels=3, dtype=tf.dtypes.uint8, expand_animations=False)\n    image = tf.image.resize(image, [image_size + 10, image_size + 10], method=interpolation)\n    image = tf.image.random_crop(image, [image_size, image_size, 3])\n    image = tf.image.random_flip_left_right(image)\n    image = tf.cast(image, tf.float32)\n    if use_bfloat16:\n        image = tf.cast(image, tf.bfloat16)\n    return image\n\n", "CodeBleu_score": 39.71269864777825}, {"code": "def preprocess_for_train(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    image = decode_image(image_bytes, image_size, interpolation)\n    image = random_crop(image)\n    image = random_flip(image)\n    image = tf.reshape(image, [image_size, image_size, 3])\n    image = tf.cast(image, tf.bfloat16 if use_bfloat16 else tf.float32)\n    return image\n\n", "CodeBleu_score": 33.37125908568391}]}
{"instance_id": "78b2a3bd2142", "generation_result": [{"code": "def preprocess_for_eval(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    if image_size is None:\n        image_size = [IMAGE_SIZE, IMAGE_SIZE]\n    image = tf.image.decode_jpeg(image_bytes, channels=3)\n    image = tf.cast(image, tf.float32)\n    image = tf.image.resize(image, image_size, method=interpolation)\n    image = tf.reshape(image, [*image_size, 3])\n    if use_bfloat16:\n        image = tf.cast(image, tf.bfloat16)\n    return image\n\n", "CodeBleu_score": 44.48203051717684}, {"code": "def preprocess_for_eval(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    if image_size is None:\n        image_size = [256, 256]\n    # Decode the image\n    image = tf.io.decode_image(image_bytes, channels=3)\n    # Resize the image\n    image = tf.image.resize(image, image_size, method=interpolation)\n    # Convert the image data type to bfloat16 or float32\n    if use_bfloat16:\n        image = tf.cast(image, tf.bfloat16)\n    else:\n        image = tf.cast(image, tf.float32)\n    # Normalize the image\n    image = (image / 127.5) - 1\n    return image\n\n", "CodeBleu_score": 41.619771194752936}, {"code": "def preprocess_for_eval(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    image = tf.io.decode_image(image_bytes, channels=3, expand_animations=False)\n    image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n    image = tf.image.resize_with_crop_or_pad(image, image_size, image_size)\n    if interpolation == 'bicubic':\n        image = tf.image.resize(image, (image_size, image_size), method='bicubic')\n    elif interpolation == 'bilinear':\n        image = tf.image.resize(image, (image_size, image_size), method='bilinear')\n    elif interpolation == 'nearest':\n        image = tf.image.resize(image, (image_size, image_size), method='nearest')\n    elif interpolation == 'area':\n        image = tf.image.resize(image, (image_size, image_size), method='area')\n    else:\n        raise ValueError('Invalid interpolation method')\n    if use_bfloat16:\n        image = tf.cast(image, tf.bfloat16)\n    return image\n\n", "CodeBleu_score": 35.89614752550987}]}
{"instance_id": "8bd4364a9a0e", "generation_result": [{"code": "def evaluate(self, model, eval_dataloader=None, output_file=\"merged\")\n\t\n    if eval_dataloader is None:\n        eval_dataloader = self.eval_dataloader\n    model.eval()\n    eval_loss = 0\n    nb_eval_steps = 0\n    preds = None\n    out_label_ids = None\n    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n        batch = tuple(t.to(self.device) for t in batch)\n        with torch.no_grad():\n            inputs = {\n                \"input_ids\": batch[0],\n                \"attention_mask\": batch[1],\n                \"labels\": batch[3],\n            }\n            if self.model_type!= \"distilbert\":\n                inputs[\"token_type_ids\"] = (\n                    batch[2] if self.model_type in [\"bert\", \"xlnet\"] else None\n                )  # XLM, DistilBERT, RoBERTa, and XLM-RoBERTa don't use segment_ids\n            outputs = model(**inputs)\n            tmp_eval_loss, logits = outputs[:2]\n            eval_loss += tmp_eval_loss.mean().item()\n        nb_eval_steps += 1\n        if preds is None:\n            preds = logits.detach().cpu().numpy()\n            out_label_ids = inputs[\"labels\"].detach().cpu().numpy()\n        else:\n            preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n            out_label_ids = np.append(\n                out_label_ids, inputs[\"labels\"].detach().cpu().numpy(), axis=0\n            )\n    eval_loss = eval_loss / nb_eval_steps\n    preds = np.argmax(preds, axis=1)\n    result = compute_metrics(preds, out_label_ids)\n    result[\"eval_loss\"] = eval_loss\n    output_eval_file = os.path.join(self.args.output_dir, output_file)\n    with open(output_eval_file, \"w\") as writer:\n        logger.info(\"***** Eval results *****\")\n        for key in sorted(result.keys()):\n            logger.info(\"  %s = %s\", key, str(result[key]))\n            writer.write(\"%s = %s", "CodeBleu_score": 42.32503691567183}, {"code": "def evaluate(self, model, eval_dataloader=None, output_file=\"merged\")\n\t\n    # If no evaluation dataloader is provided, use the default one\n    if eval_dataloader is None:\n        eval_dataloader = self.eval_dataloader\n\n    # Set the model to evaluation mode\n    model.eval()\n\n    # Initialize the output file\n    output_file = f\"{output_file}.txt\"\n    with open(output_file, \"w\") as f:\n        f.write(\"index\\tprediction", "CodeBleu_score": 41.33587456343377}, {"code": "def evaluate(self, model, eval_dataloader=None, output_file=\"merged\")\n\t\n\n    # Check if the model has been trained\n    if not model.is_trained:\n        raise Exception(\"The model has not been trained yet.\")\n\n    # Check if the model has been evaluated\n    if model.is_evaluated:\n        print(\"The model has already been evaluated.\")\n        return\n\n    # Get the evaluation dataloader\n    if eval_dataloader is None:\n        eval_dataloader = self.eval_dataloader\n\n    # Evaluate the model on the evaluation dataloader\n    print(\"Evaluating the model...\")\n    eval_output_dir = os.path.join(self.output_dir, \"eval_results\")\n    os.makedirs(eval_output_dir, exist_ok=True)\n    output_file = os.path.join(eval_output_dir, output_file)\n    model.evaluate(eval_dataloader, output_file)\n\n    # Compute evaluation metrics\n    print(\"Computing evaluation metrics...\")\n    metrics = model.compute_metrics(output_file)\n\n    # Save the evaluation metrics to the output directory\n    with open(os.path.join(eval_output_dir, \"metrics.txt\"), \"w\") as f:\n        for metric, value in metrics.items():\n            f.write(f\"{metric}: {value}", "CodeBleu_score": 46.8576511848331}]}
{"instance_id": "22d682c958c0", "generation_result": [{"code": " def print_computed_metrics(self, metrics)\n\t\n    print(f\"Rank-1: {metrics['rank-1']:.4f}\")\n    print(f\"Rank-5: {metrics['rank-5']:.4f}\")\n    print(f\"mIoU: {metrics['mean_iou']:.4f}\")\n\n", "CodeBleu_score": 18.470711940931743}, {"code": " def print_computed_metrics(self, metrics)\n\t\n    print(f\"Rank-1: {metrics['rank1']:.4f}\")\n    print(f\"Rank-5: {metrics['rank5']:.4f}\")\n    print(f\"mIoU: {metrics['mIoU']:.4f}\")\n\n", "CodeBleu_score": 18.29980644136352}, {"code": " def print_computed_metrics(self, metrics)\n\t\n    print(f\"Rank@1: {metrics['rank1']:.4f}\")\n    print(f\"Rank@5: {metrics['rank5']:.4f}\")\n    print(f\"mIoU: {metrics['miou']:.4f}\")\n\n\n def save_computed_metrics(self, metrics, file_path):\n    \"\"\"The function saves the computed metrics for rank1, rank5, and miou to a JSON file.", "CodeBleu_score": 25.772292002880103}]}
{"instance_id": "39c1a50b7e7d", "generation_result": [{"code": "def _eval_predictions(self, segments, data)\n\t\n\n    # Initialize the variables for the IoU, rank, and count\n    iou_sum = 0.0\n    rank_sum = 0.0\n    count = 0\n\n    # Iterate over the segments and data\n    for segment, datum in zip(segments, data):\n        # Get the ground truth bounding box\n        gt_bbox = datum[\"bbox\"]\n        # Get the predicted bounding boxes\n        pred_bboxes = segment[\"bboxes\"]\n        # Get the predicted labels\n        pred_labels = segment[\"labels\"]\n        # Get the predicted scores\n        pred_scores = segment[\"scores\"]\n        # Get the top 5 predicted bounding boxes\n        top_5_bboxes = pred_bboxes[:5]\n        # Get the top 5 predicted labels\n        top_5_labels = pred_labels[:5]\n        # Get the top 5 predicted scores\n        top_5_scores = pred_scores[:5]\n        # Initialize the IoU and rank variables\n        iou = 0.0\n        rank = 0.0\n        # Iterate over the top 5 predicted bounding boxes\n        for i, bbox in enumerate(top_5_bboxes):\n            # Calculate the IoU between the ground truth bounding box and the predicted bounding box\n            iou_value = self._get_iou(gt_bbox, bbox)\n            # If the IoU is greater than the current IoU, update the IoU and rank\n            if iou_value > iou:\n                iou = iou_value\n                rank = i + 1\n        # If the ground truth label is in the top 5 predicted labels, update the rank\n        if datum[\"label\"] in top_5_labels:\n            rank = min(rank, 5)\n        # Update the IoU and rank sums\n        iou_sum += iou\n        rank_sum += rank\n        count += 1\n\n    # Calculate the percentage of segments with rank 1 and rank 5\n    rank1 = (rank_sum == 1.0) / count * 100\n    rank5 = (rank_sum <= 5.0) / count * 100\n    # Calculate the mean IoU\n    mean_iou = iou_sum / count\n\n    # Return the rank1, rank5, and mean IoU\n    return rank1, rank5, mean_iou\n\n", "CodeBleu_score": 28.09024709424933}, {"code": "def _eval_predictions(self, segments, data)\n\t\n\n    # Initialize variables\n    rank1 = 0\n    rank5 = 0\n    mean_iou = 0\n\n    # Iterate over segments\n    for segment in segments:\n        # Initialize variables\n        max_iou = 0\n        max_rank = 0\n\n        # Iterate over data\n        for d in data:\n            # Calculate IoU\n            iou = self._calculate_iou(segment, d)\n\n            # Update max IoU and max rank\n            if iou > max_iou:\n                max_iou = iou\n                max_rank = 1\n            elif iou == max_iou:\n                max_rank += 1\n\n        # Update rank1 and rank5\n        if max_rank == 1:\n            rank1 += 1\n        if max_rank <= 5:\n            rank5 += 1\n\n        # Update mean IoU\n        mean_iou += max_iou\n\n    # Calculate averages\n    rank1 /= len(segments)\n    rank5 /= len(segments)\n    mean_iou /= len(segments)\n\n    return rank1, rank5, mean_iou\n\n", "CodeBleu_score": 24.46865636872723}, {"code": "def _eval_predictions(self, segments, data)\n\t\n    # Get the ground truth bounding boxes\n    gt_bboxes = np.array(data[\"bboxes\"])\n    gt_classes = np.array(data[\"classes\"])\n    gt_difficult = np.array(data[\"difficult\"])\n\n    # Convert the segments to bounding boxes\n    seg_bboxes = np.array(\n        [\n            [\n                seg[\"x1\"],\n                seg[\"y1\"],\n                seg[\"x2\"],\n                seg[\"y2\"],\n            ]\n            for seg in segments\n        ]\n    )\n\n    # Calculate the IoU for each segment-ground truth pair\n    ious = iou(seg_bboxes, gt_bboxes)\n\n    # Calculate the rank for each segment\n    ranks = np.argsort(-ious, axis=1)\n\n    # Calculate the rank of each ground truth\n    gt_ranks = np.argsort(-ious, axis=0)\n\n    # Calculate the rank of each segment\n    seg_ranks = np.zeros(ious.shape[0])\n    for i in range(ious.shape[0]):\n        seg_ranks[i] = gt_ranks[ranks[i, 0]]\n\n    # Calculate the number of segments with rank 1 and rank 5\n    rank1 = (seg_ranks == 0).sum()\n    rank5 = (seg_ranks < 5).sum()\n\n    # Calculate the mean IoU for each segment\n    mean_ious = ious[np.arange(ious.shape[0]), ranks[:, 0]]\n\n    # Calculate the mean IoU across all segments\n    mean_iou = mean_ious.mean()\n\n    return rank1, rank5, mean_iou\n\n", "CodeBleu_score": 24.489496898517476}]}
{"instance_id": "8ccf84f8d034", "generation_result": [{"code": "def predict_loop(self, model, eval_dataloader, output_file=None)\n\t\n    scores = []\n    model.eval()\n    model.to(self.device)\n    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n        batch = tuple(t.to(self.device) for t in batch)\n        with torch.no_grad():\n            inputs = {\n                \"input_ids\": batch[0],\n                \"attention_mask\": batch[1],\n                \"labels\": batch[3],\n            }\n            if self.model_type!= \"distilbert\":\n                inputs[\"token_type_ids\"] = (\n                    batch[2] if self.model_type in [\"bert\", \"xlnet\"] else None\n                )\n            outputs = model(**inputs)\n            tmp_eval_loss, logits = outputs[:2]\n            scores.append(logits.detach().cpu().numpy())\n    if output_file is not None:\n        with open(output_file, \"w\") as f:\n            for item in scores:\n                f.write(\"%s", "CodeBleu_score": 29.284970946129164}, {"code": "def predict_loop(self, model, eval_dataloader, output_file=None)\n\t\n    scores = []\n    model.eval()\n    model.to(self.device)\n    for data in tqdm(eval_dataloader):\n        data = {k: v.to(self.device) for k, v in data.items()}\n        with torch.no_grad():\n            outputs = model(**data)\n        scores.append(outputs)\n    scores = self.postprocess(scores)\n    if output_file is not None:\n        with open(output_file, \"w\") as f:\n            json.dump(scores, f)\n    return scores\n\n", "CodeBleu_score": 43.8949606600399}, {"code": "def predict_loop(self, model, eval_dataloader, output_file=None)\n\t\n    scores = []\n    model.eval()\n    model.to(self.device)\n    with torch.no_grad():\n        for batch in tqdm(eval_dataloader):\n            batch = {k: v.to(self.device) for k, v in batch.items()}\n            outputs = model(**batch)\n            scores.append(outputs)\n    scores = torch.cat(scores, dim=0)\n    scores = scores.cpu().numpy()\n    if output_file is not None:\n        self.save_predictions(scores, output_file)\n    return scores\n\n", "CodeBleu_score": 39.55236398752112}]}
{"instance_id": "e68529ad727d", "generation_result": [{"code": "def _aggregate_scores(self, scores)\n\t\n    assert len(scores) == 2, 'There must be exactly two sets of scores.'\n    scores = torch.cat([s.unsqueeze(0) for s in scores], 0)\n    self.full_scores = None\n    return scores.matmul(scores.transpose(1, 2))\n\n", "CodeBleu_score": 31.060611399195214}, {"code": "def _aggregate_scores(self, scores)\n\t\n    assert len(scores) == 2\n    scores = torch.cat(scores, dim=1)\n    self.full_scores = None\n    return torch.einsum('ij,ij->i', scores[:, :self.hidden_size], scores[:, self.hidden_size:])\n\n", "CodeBleu_score": 33.401657001029875}, {"code": "def _aggregate_scores(self, scores)\n\t\n    assert len(scores) == 2\n    scores = torch.cat(scores, dim=1)\n    self.full_scores = None\n    return torch.einsum('ij,ij->i', scores[:, :self.hidden_size], scores[:, self.hidden_size:])\n", "CodeBleu_score": 33.401657001029875}]}
{"instance_id": "e4e1861fc357", "generation_result": [{"code": "def finalize(self, Y_pred, Y_true, output_file=None)\n\t\n\n    # Convert to numpy arrays\n    Y_pred = np.array(Y_pred)\n    Y_true = np.array(Y_true)\n\n    # Check for prediction errors\n    if len(Y_pred)!= len(Y_true):\n        raise ValueError(\"The number of predicted and true labels do not match.\")\n\n    # Print samples of prediction errors\n    if self.verbose:\n        print(\"", "CodeBleu_score": 11.81566278264154}, {"code": "def finalize(self, Y_pred, Y_true, output_file=None)\n\t\n    Y_pred = np.concatenate(Y_pred, axis=0)\n    Y_true = np.concatenate(Y_true, axis=0)\n    if Y_pred.shape[0]!= Y_true.shape[0]:\n        print(\"Prediction error!\")\n        print(\"Predicted labels shape: \", Y_pred.shape)\n        print(\"True labels shape: \", Y_true.shape)\n        print(\"Sample of prediction errors:\")\n        for i in range(Y_pred.shape[0]):\n            if Y_pred[i]!= Y_true[i]:\n                print(\"Predicted: \", Y_pred[i], \" True: \", Y_true[i])\n    if output_file is not None:\n        with open(output_file, \"wb\") as f:\n            pickle.dump({\"Y_pred\": Y_pred, \"Y_true\": Y_true}, f)\n    return {\"Y_pred\": Y_pred, \"Y_true\": Y_true}\n\n", "CodeBleu_score": 38.72939989212014}, {"code": "def finalize(self, Y_pred, Y_true, output_file=None)\n\t\n\n    # concatenate predictions\n    Y_pred = torch.cat(Y_pred, dim=0).cpu().numpy()\n    Y_true = torch.cat(Y_true, dim=0).cpu().numpy()\n\n    # check for prediction errors\n    errors = np.where(Y_pred!= Y_true)[0]\n    if len(errors) > 0:\n        print(f\"Found {len(errors)} prediction errors.\")\n        print(f\"Samples of prediction errors:\")\n        for i in range(min(5, len(errors))):\n            print(f\"True label: {Y_true[errors[i]]}, predicted label: {Y_pred[errors[i]]}\")\n\n    # save predictions and true labels\n    if output_file is not None:\n        with open(output_file, \"wb\") as f:\n            pickle.dump({\"Y_pred\": Y_pred, \"Y_true\": Y_true}, f)\n\n    return {\"Y_pred\": Y_pred, \"Y_true\": Y_true}\n\n", "CodeBleu_score": 43.68847678443824}]}
{"instance_id": "7df9ab9a4920", "generation_result": [{"code": "def reduce_metrics(logging_outputs) -> None\n\t\n    loss_sum = sum(log.get(\"loss\", 0) for log in logging_outputs)\n    sample_size = sum(log.get(\"sample_size\", 0) for log in logging_outputs)\n    metrics.log_scalar(\n        \"loss\", loss_sum / sample_size, sample_size, round=3\n    )  # round to 3 decimal places\n\n", "CodeBleu_score": 54.58085773512743}, {"code": "def reduce_metrics(logging_outputs) -> None\n\t\n    loss_sum = sum(log.get(\"loss\", 0) for log in logging_outputs)\n    sample_size = sum(log.get(\"sample_size\", 0) for log in logging_outputs)\n    metrics.log_scalar(\"loss\", loss_sum / sample_size, sample_size, round=3)\n\n", "CodeBleu_score": 62.429646714521894}, {"code": "def reduce_metrics(logging_outputs) -> None\n\t\n    loss_sum = sum(log.get(\"loss\", 0) for log in logging_outputs)\n    sample_size = sum(log.get(\"sample_size\", 0) for log in logging_outputs)\n\n    metrics.log_scalar(\n        \"loss\", loss_sum / sample_size, sample_size, round=3\n    )\n\n", "CodeBleu_score": 57.00304501759299}]}
{"instance_id": "7f9c326a96f7", "generation_result": [{"code": "def _mm_on_the_fly(\n        self,\n        cmasks,\n        vmasks,\n        attention_mask\n    )\n\t\n    if attention_mask is None:\n        attention_mask = cmasks.unsqueeze(1) * cmasks.unsqueeze(2)\n        attention_mask = attention_mask * vmasks.unsqueeze(1) * vmasks.unsqueeze(2)\n    token_type_ids = torch.cat([torch.zeros_like(cmasks), torch.ones_like(vmasks)], dim=-1)\n    return attention_mask, token_type_ids\n\n", "CodeBleu_score": 34.12897966900494}, {"code": "def _mm_on_the_fly(\n        self,\n        cmasks,\n        vmasks,\n        attention_mask\n    )\n\t\n    if attention_mask is None:\n        attention_mask = torch.cat((cmasks, vmasks), dim=1)\n    token_type_ids = torch.cat((torch.zeros_like(cmasks), torch.ones_like(vmasks)), dim=1)\n    return attention_mask, token_type_ids\n\n", "CodeBleu_score": 25.485742335871564}, {"code": "def _mm_on_the_fly(\n        self,\n        cmasks,\n        vmasks,\n        attention_mask\n    )\n\t\n    if attention_mask is None:\n        attention_mask = torch.cat([cmasks, vmasks], dim=1)\n    token_type_ids = torch.cat([torch.zeros_like(cmasks), torch.ones_like(vmasks)], dim=1)\n    return attention_mask, token_type_ids\n\n", "CodeBleu_score": 25.434930551458244}]}
{"instance_id": "fa99d2a148a5", "generation_result": [{"code": "def prepare_inputs_for_generation(\n        self,\n        input_ids,\n        input_video_embeds,\n        attention_mask=None,\n        token_type_ids=None,\n        **model_kwargs\n    )\n\t\n    # Prepare input_ids and attention_mask\n    if attention_mask is None:\n        attention_mask = input_ids.ne(self.config.pad_token_id)\n    # Prepare token_type_ids\n    if token_type_ids is None:\n        token_type_ids = torch.zeros_like(input_ids)\n\n    # Prepare input_video_embeds\n    if input_video_embeds is None:\n        input_video_embeds = torch.zeros(\n            (input_ids.shape[0], self.config.video_embed_dim),\n            dtype=torch.float,\n            device=input_ids.device,\n        )\n\n    # Adjust sequence length to match combined length of input_ids and input_video_embeds\n    combined_sequence_length = input_ids.shape[-1] + input_video_embeds.shape[-1]\n    if attention_mask.shape[-1] < combined_sequence_length:\n        attention_mask = torch.cat(\n            [\n                attention_mask,\n                torch.ones(\n                    (attention_mask.shape[0], combined_sequence_length - attention_mask.shape[-1]),\n                    dtype=attention_mask.dtype,\n                    device=attention_mask.device,\n                ),\n            ],\n            dim=-1,\n        )\n    if token_type_ids.shape[-1] < combined_sequence_length:\n        token_type_ids = torch.cat(\n            [\n                token_type_ids,\n                torch.zeros(\n                    (token_type_ids.shape[0], combined_sequence_length - token_type_ids.shape[-1]),\n                    dtype=token_type_ids.dtype,\n                    device=token_type_ids.device,\n                ),\n            ],\n            dim=-1,\n        )\n\n    # Prepare input_video_embeds\n    if input_video_embeds.shape[-1] < combined_sequence_length:\n        input_video_embeds = torch.cat(\n            [\n                input_video_embeds,\n                torch.zeros(\n                    (input_video_embeds.shape[0], combined_sequence_length - input_video_embeds.shape[-1]),\n                    dtype=input_video_embeds.dtype,\n                    device=input_video_embeds.device,\n                ),\n            ],\n            dim=-1,\n        )\n\n    return {\n       ", "CodeBleu_score": 23.90306482679113}, {"code": "def prepare_inputs_for_generation(\n        self,\n        input_ids,\n        input_video_embeds,\n        attention_mask=None,\n        token_type_ids=None,\n        **model_kwargs\n    )\n\t\n    if attention_mask is None:\n        attention_mask = input_ids.ne(self.config.pad_token_id)\n    if token_type_ids is None:\n        token_type_ids = torch.zeros_like(input_ids)\n    if input_video_embeds is not None:\n        input_ids = torch.cat((input_ids, input_video_embeds), dim=1)\n        attention_mask = torch.cat((attention_mask, torch.ones_like(input_video_embeds)), dim=1)\n        token_type_ids = torch.cat((token_type_ids, torch.ones_like(input_video_embeds)), dim=1)\n    return {\n        \"input_ids\": input_ids,\n        \"attention_mask\": attention_mask,\n        \"token_type_ids\": token_type_ids,\n        **model_kwargs,\n    }\n\n", "CodeBleu_score": 38.503490968811505}, {"code": "def prepare_inputs_for_generation(\n        self,\n        input_ids,\n        input_video_embeds,\n        attention_mask=None,\n        token_type_ids=None,\n        **model_kwargs\n    )\n\t\n\n    if input_video_embeds is not None:\n        # Adjust the sequence length to match the combined length of input_ids and input_video_embeds\n        sequence_length = input_ids.shape[-1] + input_video_embeds.shape[-1]\n        input_ids = input_ids.expand(-1, sequence_length)\n        input_video_embeds = input_video_embeds.expand(-1, sequence_length)\n\n        # Modify the attention_mask and token_type_ids to fit the new sequence length\n        if attention_mask is not None:\n            attention_mask = attention_mask.expand(-1, sequence_length)\n        if token_type_ids is not None:\n            token_type_ids = token_type_ids.expand(-1, sequence_length)\n\n    return {\n        \"input_ids\": input_ids,\n        \"input_video_embeds\": input_video_embeds,\n        \"attention_mask\": attention_mask,\n        \"token_type_ids\": token_type_ids,\n    }\n\n", "CodeBleu_score": 49.256428469764614}]}
{"instance_id": "b1b0d25fe620", "generation_result": [{"code": "def _init_pipeline(self, cfg: ConfigType) -> Compose\n\t    if 'img_id' in cfg.test_pipeline.meta_keys:\n        cfg.test_pipeline.meta_keys.remove('img_id')\n\n    load_img_idx = None\n    for idx, transform in enumerate(cfg.test_pipeline):\n        if isinstance(transform, dict) and transform['type'] == 'LoadImageFromFile':\n            load_img_idx = idx\n            break\n\n    if load_img_idx is None:\n        raise ValueError('LoadImageFromFile is not found in the test pipeline')\n\n    cfg.test_pipeline[load_img_idx]['type'] ='mmdet.InferencerLoader'\n    return Compose(cfg.test_pipeline)\n\n", "CodeBleu_score": 35.226729335094795}, {"code": "def _init_pipeline(self, cfg: ConfigType) -> Compose\n\t    cfg = copy.deepcopy(cfg)\n    meta_keys = cfg.get('meta_keys', None)\n    if meta_keys is not None and 'img_id' in meta_keys:\n        cfg['meta_keys'].remove('img_id')\n    load_img_idx = None\n    for i, transform in enumerate(cfg['pipeline']):\n        if isinstance(transform, dict) and transform['type'] == 'LoadImageFromFile':\n            load_img_idx = i\n            break\n    if load_img_idx is None:\n        raise ValueError(\n            'LoadImageFromFile is not found in the pipeline, please check the '\n            'pipeline config.')\n    cfg['pipeline'][load_img_idx]['type'] ='mmdet.InferencerLoader'\n    return Compose(cfg['pipeline'])\n\n", "CodeBleu_score": 33.67953294801587}, {"code": "def _init_pipeline(self, cfg: ConfigType) -> Compose\n\t    if 'img_id' in cfg.test_pipeline[0].keys():\n        cfg.test_pipeline[0].pop('img_id')\n    load_img_idx = None\n    for idx, transform in enumerate(cfg.test_pipeline):\n        if transform['type'] == 'LoadImageFromFile':\n            load_img_idx = idx\n            break\n    if load_img_idx is None:\n        raise RuntimeError('LoadImageFromFile is not found in the pipeline')\n    cfg.test_pipeline[load_img_idx]['type'] ='mmdet.InferencerLoader'\n    return Compose(cfg.test_pipeline)\n\n", "CodeBleu_score": 31.66226927933658}]}
{"instance_id": "7967962856eb", "generation_result": [{"code": "def _inputs_to_list(self, inputs: InputsType) -> list\n\t\n    if isinstance(inputs, str) and isdir(inputs):\n        inputs = list_dir_or_file(inputs, suffix='.jpg')\n    if not isinstance(inputs, (list, tuple)):\n        inputs = [inputs]\n    return inputs\n\n", "CodeBleu_score": 36.878044598044696}, {"code": "def _inputs_to_list(self, inputs: InputsType) -> list\n\t\n    if isinstance(inputs, str) and isdir(inputs):\n        inputs = [join_path(inputs, f) for f in list_dir_or_file(inputs)]\n    if not isinstance(inputs, (list, tuple)):\n        inputs = [inputs]\n    return inputs\n\n", "CodeBleu_score": 39.27308472130836}, {"code": "def _inputs_to_list(self, inputs: InputsType) -> list\n\t\n    if isinstance(inputs, str):\n        if isdir(inputs):\n            inputs = list_dir_or_file(inputs, suffix='.jpg')\n        else:\n            inputs = [inputs]\n    if not isinstance(inputs, (list, tuple)):\n        inputs = [inputs]\n    return inputs\n\n", "CodeBleu_score": 40.71555134640562}]}
{"instance_id": "74f727f06fa0", "generation_result": [{"code": "def _get_chunk_data(self, inputs: Iterable, chunk_size: int)\n\t\n    if not isinstance(inputs, (list, tuple)):\n        inputs = [inputs]\n    data = []\n    for input_ in inputs:\n        if isinstance(input_, dict):\n            if 'img' in input_:\n                data.append(self.pipeline(input_))\n            elif 'img_path' in input_:\n                data.append(self.pipeline(dict(img_path=input_['img_path'])))\n        else:\n            data.append(self.pipeline(input_))\n    num_data = len(data)\n    for i in range(0, num_data, chunk_size):\n        yield data[i:i + chunk_size]\n\n", "CodeBleu_score": 36.23913693274229}, {"code": "def _get_chunk_data(self, inputs: Iterable, chunk_size: int)\n\t\n    chunk_data = []\n    for input in inputs:\n        if isinstance(input, dict):\n            if 'img' in input:\n                data = self.pipeline(input)\n            elif 'img_path' in input:\n                data = self.pipeline(dict(img_path=input['img_path']))\n            else:\n                raise ValueError(\n                    'The input must contain \"img\" or \"img_path\" key.')\n        else:\n            data = self.pipeline(dict(img_path=input))\n        chunk_data.append(data)\n        if len(chunk_data) == chunk_size:\n            yield chunk_data\n            chunk_data = []\n    if chunk_data:\n        yield chunk_data\n\n", "CodeBleu_score": 42.610426459691794}, {"code": "def _get_chunk_data(self, inputs: Iterable, chunk_size: int)\n\t\n    data = []\n    for input_ in inputs:\n        if isinstance(input_, dict):\n            if 'img' in input_:\n                data.append(input_)\n            elif 'img_path' in input_:\n                input_['img'] = self.pipeline(input_)['img']\n                data.append(input_)\n        else:\n            data.append(input_)\n        if len(data) == chunk_size:\n            yield data\n            data = []\n    if data:\n        yield data\n\n", "CodeBleu_score": 32.64848057074056}]}
{"instance_id": "a43ab51afa59", "generation_result": [{"code": "def postprocess(\n        self,\n        preds: PredType,\n        visualization: Optional[List[np.ndarray]] = None,\n        return_datasamples: bool = False,\n        print_result: bool = False,\n        no_save_pred: bool = False,\n        pred_out_dir: str = '',\n        **kwargs,\n    ) -> Dict\n\t\n    if visualization is None:\n        visualization = [None] * len(preds)\n    if not no_save_pred and not osp.isdir(pred_out_dir):\n        mmcv.mkdir_or_exist(pred_out_dir)\n    results = []\n    for i in track(range(len(preds)), description='postprocessing'):\n        pred = preds[i]\n        if not return_datasamples:\n            pred = self.convert_pred(pred)\n        if not no_save_pred:\n            self.save_pred(pred, pred_out_dir, i)\n        if print_result:\n            self.print_result(pred)\n        result = dict(pred=pred, visualization=visualization[i])\n        results.append(result)\n    return results\n\n", "CodeBleu_score": 44.85388630927625}, {"code": "def postprocess(\n        self,\n        preds: PredType,\n        visualization: Optional[List[np.ndarray]] = None,\n        return_datasamples: bool = False,\n        print_result: bool = False,\n        no_save_pred: bool = False,\n        pred_out_dir: str = '',\n        **kwargs,\n    ) -> Dict\n\t\n    if not no_save_pred:\n        if pred_out_dir == '':\n            pred_out_dir = self.work_dir\n        else:\n            mmengine.mkdir_or_exist(pred_out_dir)\n    else:\n        pred_out_dir = ''\n\n    if return_datasamples:\n        if pred_out_dir:\n            warnings.warn(\n                'Saving datasamples is not supported. '\n                'The predictions will not be saved.')\n        result = {}\n    else:\n        result = []\n\n    if visualization is None:\n        visualization = [None] * len(preds)\n\n    for pred, vis in zip(preds, visualization):\n        if return_datasamples:\n            pred = self.pack_results(pred, pred_out_dir)\n        else:\n            pred = self.pack_results(pred)\n        if print_result:\n            self.print_results(pred, **kwargs)\n        if return_datasamples:\n            result[pred['img_id']] = pred\n        else:\n            result.append(pred)\n\n    return result\n\n", "CodeBleu_score": 44.19160790178833}, {"code": "def postprocess(\n        self,\n        preds: PredType,\n        visualization: Optional[List[np.ndarray]] = None,\n        return_datasamples: bool = False,\n        print_result: bool = False,\n        no_save_pred: bool = False,\n        pred_out_dir: str = '',\n        **kwargs,\n    ) -> Dict\n\t\n    if pred_out_dir:\n        mmengine.mkdir_or_exist(pred_out_dir)\n    results = dict()\n    if return_datasamples:\n        if not no_save_pred:\n            warnings.warn('Saving datasamples is not supported.')\n        results['datasamples'] = preds\n    else:\n        results['preds'] = self.format_pred(preds)\n    if visualization is not None:\n        results['visualization'] = visualization\n    if print_result:\n        self.print_result(results)\n    return results\n\n", "CodeBleu_score": 40.87836415484012}]}
{"instance_id": "a3c2561f199c", "generation_result": [{"code": "def build_test_pipeline(cfg: ConfigType) -> ConfigType\n\t\n    init_default_scope(cfg.get('default_scope','mmdet'))\n    if isinstance(cfg, Config):\n        cfg = copy.deepcopy(cfg)\n    else:\n        cfg = Config(cfg)\n    cfg.setdefault('type', 'Compose')\n    cfg.setdefault('transforms', [])\n    cfg.transforms = copy.deepcopy(cfg.transforms)\n    if len(cfg.transforms) == 0:\n        raise ValueError('Please specify at least one transform in the '\n                         'pipeline.')\n    cfg.transforms[0] = copy.deepcopy(cfg.transforms[0])\n    cfg.transforms[0].setdefault('type', 'Resize')\n    cfg.transforms[0].setdefault('scale', (1333, 800))\n    cfg.transforms[0].setdefault('keep_ratio', True)\n    cfg.transforms[0].setdefault('test_mode', True)\n    cfg.transforms.append(copy.deepcopy(cfg.transforms[-1]))\n    cfg.transforms[-1].setdefault('type', 'PackDetInputs')\n    return cfg\n\n", "CodeBleu_score": 27.115159803286907}, {"code": "def build_test_pipeline(cfg: ConfigType) -> ConfigType\n\t\n    init_default_scope(cfg.get('default_scope','mmdet'))\n    dataset_cfg = copy.deepcopy(cfg.dataset)\n    if 'pipeline' in dataset_cfg:\n        dataset_cfg.pipeline = dataset_cfg.pipeline[0]\n    else:\n        dataset_cfg.transforms = dataset_cfg.transforms[0]\n    test_pipeline = Compose([dataset_cfg, cfg.test_pipeline[-1]])\n    return test_pipeline\n\n", "CodeBleu_score": 29.173810426446305}, {"code": "def build_test_pipeline(cfg: ConfigType) -> ConfigType\n\t\n    dataset_cfg = cfg.dataset_config\n    test_pipeline = copy.deepcopy(dataset_cfg.test_pipeline)\n    resize_cfg = copy.deepcopy(dataset_cfg.pipeline[0])\n    resize_cfg.type = 'Resize'\n    test_pipeline[0] = resize_cfg\n    test_pipeline.append(copy.deepcopy(dataset_cfg.pipeline[-1]))\n    test_pipeline = Compose(test_pipeline)\n    return test_pipeline\n\n", "CodeBleu_score": 22.51352690872449}]}
{"instance_id": "48a358ab5a38", "generation_result": [{"code": "def inference_mot(model: nn.Module, img: np.ndarray, frame_id: int,\n                  video_len: int) -> SampleList\n\t\n    data = dict(img=img, frame_id=frame_id, img_shape=img.shape,\n                img_id=frame_id, video_len=video_len)\n    cfg = model.cfg\n    test_pipeline = cfg.test_pipeline\n    test_pipeline = Compose(test_pipeline)\n    data = test_pipeline(data)\n    data = default_collate([data])\n    if next(model.parameters()).is_cuda:\n        assert not any(\n            isinstance(m, RoIPool) for m in model.modules()\n        ), 'RoIPool is not supported on GPU.'\n        batch_size = data['img'][0].shape[0]\n        if batch_size > 1:\n            warnings.warn('batch_size will be reset to 1 in non-distributed '\n                          'testing.')\n        data['img'] = [data['img'][0][0].unsqueeze(0)]\n        data['img_metas'] = [data['img_metas'][0][0].unsqueeze(0)]\n        data['proposals'] = [data['proposals'][0][0].unsqueeze(0)]\n        if 'gt_bboxes' in data:\n            data['gt_bboxes'] = [data['gt_bboxes'][0][0].unsqueeze(0)]\n        if 'gt_labels' in data:\n            data['gt_labels'] = [data['gt_labels'][0][0].unsqueeze(0)]\n        if 'gt_masks' in data:\n            data['gt_masks'] = [data['gt_masks'][0][0].unsqueeze(0)]\n    with torch.no_grad():\n        result = model(return_loss=False, rescale=True, **data)\n    return result\n\n", "CodeBleu_score": 38.88643777323314}, {"code": "def inference_mot(model: nn.Module, img: np.ndarray, frame_id: int,\n                  video_len: int) -> SampleList\n\t\n    data = dict(img=img, frame_id=frame_id, ori_shape=img.shape, img_id=frame_id,\n                video_len=video_len)\n    test_pipeline = Compose(model.test_pipeline)\n    data = test_pipeline(data)\n    data = default_collate([data])\n    if next(model.parameters()).is_cuda:\n        model = revert_sync_batchnorm(model)\n    with torch.no_grad():\n        result = model.forward(**data, return_loss=False)\n    return result\n\n", "CodeBleu_score": 34.27385844481699}, {"code": "def inference_mot(model: nn.Module, img: np.ndarray, frame_id: int,\n                  video_len: int) -> SampleList\n\t\n    data = dict(img=img, frame_id=frame_id, ori_shape=img.shape, img_id=0,\n                video_len=video_len)\n    cfg = model.cfg\n    test_pipeline = Compose(cfg.test_pipeline)\n    data = test_pipeline(data)\n    data = default_collate([data])\n    if next(model.parameters()).is_cuda:\n        data = revert_sync_batchnorm(data)\n    with torch.no_grad():\n        result = model.forward(**data, return_loss=False)\n    return result\n\n", "CodeBleu_score": 34.39100146466379}]}
{"instance_id": "6c4ea2ce1787", "generation_result": [{"code": "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\n\t\n    while True:\n        total_length = len(tokens_a) + len(tokens_b)\n        if total_length <= max_num_tokens:\n            break\n\n        truncate_from_a = True if len(tokens_a) > len(tokens_b) else False\n        if truncate_from_a:\n            truncate_from = rng.randint(0, 1)\n            if truncate_from == 0:\n                tokens_a.pop(0)\n            else:\n                tokens_a.pop()\n        else:\n            truncate_from = rng.randint(0, 1)\n            if truncate_from == 0:\n                tokens_b.pop(0)\n            else:\n                tokens_b.pop()\n\n", "CodeBleu_score": 49.893708305248936}, {"code": "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\n\t\n    while True:\n        total_length = len(tokens_a) + len(tokens_b)\n        if total_length <= max_num_tokens:\n            break\n\n        trunc_tokens = tokens_a if len(tokens_a) > len(tokens_b) else tokens_b\n        assert len(trunc_tokens) >= 1\n\n        # We want to sometimes truncate from the front and sometimes from the\n        # back to add more randomness and avoid biases.\n        if rng.random() < 0.5:\n            del trunc_tokens[0]\n        else:\n            trunc_tokens.pop()\n\n", "CodeBleu_score": 79.2280525202322}, {"code": "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\n\t\n\n    while True:\n        total_length = len(tokens_a) + len(tokens_b)\n        if total_length <= max_num_tokens:\n            break\n\n        trunc_tokens = tokens_a if len(tokens_a) > len(tokens_b) else tokens_b\n        assert len(trunc_tokens) >= 1\n\n        # We want to sometimes truncate from the front and sometimes from the\n        # back to add more randomness and avoid biases.\n        if rng.random() < 0.5:\n            del trunc_tokens[0]\n        else:\n            trunc_tokens.pop()\n\n", "CodeBleu_score": 79.2280525202322}]}
{"instance_id": "007181b6729d", "generation_result": [{"code": "def get_activation(activation_string)\n\t\n    if isinstance(activation_string, six.string_types):\n        if not activation_string:\n            return None\n        if activation_string == \"relu\":\n            return tf.nn.relu\n        if activation_string == \"elu\":\n            return tf.nn.elu\n        if activation_string == \"leaky_relu\":\n            return tf.nn.leaky_relu\n        if activation_string == \"selu\":\n            return tf.nn.selu\n        if activation_string == \"softplus\":\n            return tf.nn.softplus\n        if activation_string == \"softsign\":\n            return tf.nn.softsign\n        if activation_string == \"sigmoid\":\n            return tf.nn.sigmoid\n        if activation_string == \"tanh\":\n            return tf.nn.tanh\n        if activation_string == \"swish\":\n            return tf.nn.swish\n        if activation_string == \"gelu\":\n            return tf.nn.gelu\n        if activation_string == \"log_sigmoid\":\n            return tf.nn.log_sigmoid\n        if activation_string == \"log_softmax\":\n            return tf.nn.log_softmax\n        if activation_string == \"softmax\":\n            return tf.nn.softmax\n        if activation_string == \"softsign\":\n            return tf.nn.softsign\n        if activation_string == \"softmax_cross_entropy_with_logits\":\n            return tf.nn.softmax_cross_entropy_with_logits\n        if activation_string == \"sigmoid_cross_entropy_with_logits\":\n            return tf.nn.sigmoid_cross_entropy_with_logits\n        if activation_string == \"tanh_cross_entropy_with_logits\":\n            return tf.nn.tanh_cross_entropy_with_logits\n        if activation_string == \"l2_loss\":\n            return tf.nn.l2_loss\n        if activation_string == \"l2_normalize\":\n            return tf.nn.l2_normalize\n        if activation_string == \"l2_regularizer\":\n            return tf.nn.l2_regularizer\n        if activation_string == \"moments\":\n            return tf.nn.moments\n        if activation_string == \"sufficient_statistics\":\n            return tf.nn.sufficient_statistics\n        if activation_string == \"weighted_cross_entropy_with_logits\":\n            return tf.nn.weighted_cross_entropy_with_logits", "CodeBleu_score": 31.07649613311066}, {"code": "def get_activation(activation_string)\n\t\n    if isinstance(activation_string, six.string_types):\n        if activation_string == \"\":\n            return None\n        if activation_string == \"relu\":\n            return tf.nn.relu\n        if activation_string == \"leaky_relu\":\n            return tf.nn.leaky_relu\n        if activation_string == \"elu\":\n            return tf.nn.elu\n        if activation_string == \"selu\":\n            return tf.nn.selu\n        if activation_string == \"sigmoid\":\n            return tf.nn.sigmoid\n        if activation_string == \"tanh\":\n            return tf.nn.tanh\n        if activation_string == \"softmax\":\n            return tf.nn.softmax\n        if activation_string == \"softplus\":\n            return tf.nn.softplus\n        if activation_string == \"softsign\":\n            return tf.nn.softsign\n        if activation_string == \"swish\":\n            return tf.nn.swish\n        if activation_string == \"log_sigmoid\":\n            return tf.nn.log_sigmoid\n        if activation_string == \"relu6\":\n            return tf.nn.relu6\n        if activation_string == \"crelu\":\n            return tf.nn.crelu\n        if activation_string == \"relu_layer\":\n            return tf.nn.relu_layer\n        if activation_string == \"leaky_relu_layer\":\n            return tf.nn.leaky_relu_layer\n        if activation_string == \"elu_layer\":\n            return tf.nn.elu_layer\n        if activation_string == \"selu_layer\":\n            return tf.nn.selu_layer\n        if activation_string == \"sigmoid_layer\":\n            return tf.nn.sigmoid_layer\n        if activation_string == \"tanh_layer\":\n            return tf.nn.tanh_layer\n        if activation_string == \"softmax_layer\":\n            return tf.nn.softmax_layer\n        if activation_string == \"softplus_layer\":\n            return tf.nn.softplus_layer\n        if activation_string == \"softsign_layer\":\n            return tf.nn.softsign_layer\n        if activation_string == \"swish_layer\":\n            return tf.nn.swish_layer\n        if activation_string == \"log_sigmoid_layer\":\n            return tf.nn.log_sigmoid_layer\n        if activation_string == \"relu6_layer\":", "CodeBleu_score": 23.953469604158055}, {"code": "def get_activation(activation_string)\n\t\n    if isinstance(activation_string, six.string_types):\n        if not activation_string:\n            return None\n        elif activation_string =='relu':\n            return tf.nn.relu\n        elif activation_string == 'elu':\n            return tf.nn.elu\n        elif activation_string == 'leaky_relu':\n            return tf.nn.leaky_relu\n        elif activation_string =='selu':\n            return tf.nn.selu\n        elif activation_string =='softplus':\n            return tf.nn.softplus\n        elif activation_string =='softsign':\n            return tf.nn.softsign\n        elif activation_string =='sigmoid':\n            return tf.nn.sigmoid\n        elif activation_string == 'tanh':\n            return tf.nn.tanh\n        else:\n            raise ValueError('Unsupported activation: %s' % activation_string)\n    elif callable(activation_string):\n        return activation_string\n    else:\n        raise TypeError('Invalid type of activation function. Expected string or callable, got {}.'.format(type(activation_string).__name__))\n\n", "CodeBleu_score": 36.72910905784772}]}
{"instance_id": "d5d6e0315899", "generation_result": [{"code": "def dropout(input_tensor, dropout_prob)\n\t\n    if dropout_prob is None or dropout_prob == 0.0:\n        return input_tensor\n    else:\n        return tf.nn.dropout(input_tensor, dropout_prob)\n\n", "CodeBleu_score": 53.31243274886709}, {"code": "def dropout(input_tensor, dropout_prob)\n\t\n    if dropout_prob is None or dropout_prob == 0.0:\n        return input_tensor\n    output_tensor = tf.nn.dropout(input_tensor, keep_prob=1 - dropout_prob)\n    return output_tensor\n\n", "CodeBleu_score": 65.4886701800924}, {"code": "def dropout(input_tensor, dropout_prob)\n\t\n    if dropout_prob is not None and dropout_prob > 0.0:\n        output_tensor = tf.nn.dropout(input_tensor, keep_prob=1.0 - dropout_prob)\n    else:\n        output_tensor = input_tensor\n\n    return output_tensor\n\n", "CodeBleu_score": 50.07206941604334}]}
{"instance_id": "828917b9ac53", "generation_result": [{"code": "def create_attention_mask_from_input_mask(from_tensor, to_mask)\n\t\n    from_shape = get_shape_list(from_tensor, expected_rank=[2, 3])\n    batch_size = from_shape[0]\n    from_seq_length = from_shape[1]\n\n    to_shape = get_shape_list(to_mask, expected_rank=2)\n    to_seq_length = to_shape[1]\n\n    to_mask = tf.cast(\n        tf.reshape(to_mask, [batch_size, 1, to_seq_length]), tf.float32)\n    broadcast_ones = tf.ones(\n        shape=[batch_size, from_seq_length, 1], dtype=tf.float32)\n    mask = broadcast_ones * to_mask\n\n    return mask\n\n", "CodeBleu_score": 64.09901013046853}, {"code": "def create_attention_mask_from_input_mask(from_tensor, to_mask)\n\t\n    from_shape = get_shape_list(from_tensor, expected_rank=[2, 3])\n    batch_size = from_shape[0]\n    from_seq_length = from_shape[1]\n    to_shape = get_shape_list(to_mask, expected_rank=2)\n    to_seq_length = to_shape[1]\n    to_mask = reshape_to_matrix(to_mask)\n    to_mask = cast_float(to_mask)\n    return tf.reshape(tf.tile(to_mask, [from_seq_length, 1]),\n                      [batch_size, from_seq_length, to_seq_length])\n\n", "CodeBleu_score": 37.843952755427566}, {"code": "def create_attention_mask_from_input_mask(from_tensor, to_mask)\n\t\n    from_shape = get_shape_list(from_tensor, expected_rank=[2, 3])\n    batch_size = from_shape[0]\n    from_seq_length = from_shape[1]\n\n    to_shape = get_shape_list(to_mask, expected_rank=2)\n    to_seq_length = to_shape[1]\n\n    to_mask = reshape_to_matrix(to_mask)\n    # Create attention mask.\n    return create_attention_mask_from_input_mask(batch_size, from_seq_length,\n                                                 to_seq_length, to_mask)\n\n", "CodeBleu_score": 35.5181361371708}]}
{"instance_id": "d7798e41a723", "generation_result": [{"code": "def transpose_for_scores(input_tensor, batch_size, num_attention_heads,\n                           seq_length, width)\n\t\n    output_tensor = tf.reshape(\n        input_tensor, [batch_size, seq_length, num_attention_heads, width])\n\n    output_tensor = tf.transpose(output_tensor, [0, 2, 1, 3])\n    return output_tensor\n", "CodeBleu_score": 59.99845781533536}, {"code": "def transpose_for_scores(input_tensor, batch_size, num_attention_heads,\n                           seq_length, width)\n\t\n    output_tensor = tf.reshape(\n        input_tensor, [batch_size, seq_length, num_attention_heads, width])\n\n    output_tensor = tf.transpose(output_tensor, [0, 2, 1, 3])\n    return output_tensor\n\n", "CodeBleu_score": 59.99845781533536}, {"code": "def transpose_for_scores(input_tensor, batch_size, num_attention_heads,\n                           seq_length, width)\n\t\n    output_tensor = tf.reshape(\n        input_tensor, [batch_size, seq_length, num_attention_heads, width])\n    output_tensor = tf.transpose(output_tensor, [0, 2, 1, 3])\n    return output_tensor\n", "CodeBleu_score": 59.99845781533536}]}
{"instance_id": "711d466ace09", "generation_result": [{"code": "def reshape_to_matrix(input_tensor)\n\t\n    input_shape = input_tensor.shape\n    input_shape_list = input_shape.as_list()\n    if len(input_shape_list) < 2:\n        raise ValueError(\"Input tensor must have at least rank 2. Shape = %s\" % input_shape_list)\n    if len(input_shape_list) == 2:\n        return input_tensor\n    width = input_shape_list[-1]\n    output_tensor = tf.reshape(input_tensor, [-1, width])\n    return output_tensor\n\n", "CodeBleu_score": 61.008918466884666}, {"code": "def reshape_to_matrix(input_tensor)\n\t\n    input_tensor = tf.convert_to_tensor(input_tensor)\n    if input_tensor.shape.ndims is None:\n        raise ValueError(\n            \"input to reshape_to_matrix must be a tensor and not a tensor shape\"\n        )\n    if input_tensor.shape.ndims == 2:\n        return input_tensor\n    if input_tensor.shape.ndims < 2:\n        raise ValueError(\n            \"input to reshape_to_matrix must have at least rank 2, got rank: %d\"\n            % input_tensor.shape.ndims\n        )\n    return tf.reshape(input_tensor, [-1, input_tensor.shape[-1]])\n\n", "CodeBleu_score": 33.61020863710028}, {"code": "def reshape_to_matrix(input_tensor)\n\t\n    input_shape = input_tensor.shape\n    if len(input_shape) == 2:\n        return input_tensor\n    else:\n        return tf.reshape(input_tensor, [-1, input_shape[-1]])\n\n", "CodeBleu_score": 17.932672209660414}]}
{"instance_id": "7adcd08f531e", "generation_result": [{"code": "def get_train_examples(self, data_dir)\n\t\n    examples = []\n    with tf.gfile.Open(data_dir, \"r\") as f:\n        reader = csv.reader(f, delimiter=\"\\t\")\n        for line in reader:\n            if sys.version_info[0] == 2:\n                line = list(unicode(cell, \"utf-8\") for cell in line)\n            guid = line[0]\n            text_a = tokenization.convert_to_unicode(line[5])\n            text_b = tokenization.convert_to_unicode(line[6])\n            if line[3] == \"contradictory\":\n                label = \"contradiction\"\n            else:\n                label = tokenization.convert_to_unicode(line[3])\n            examples.append(\n                InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label)\n            )\n    return examples\n\n", "CodeBleu_score": 46.42703737629948}, {"code": "def get_train_examples(self, data_dir)\n\t\n    # See __call__ for description of tsv_path\n    tsv_path = os.path.join(data_dir, \"train.tsv\")\n    examples = []\n    with tf.gfile.Open(tsv_path, \"r\") as f:\n        reader = csv.reader(f, delimiter=\"\\t\")\n        for line in reader:\n            if sys.version_info[0] == 2:\n                line = list(unicode(cell, \"utf-8\") for cell in line)\n            guid = line[0]\n            text_a = tokenization.convert_to_unicode(line[8])\n            text_b = tokenization.convert_to_unicode(line[9])\n            label = tokenization.convert_to_unicode(line[-1])\n            if label == \"contradictory\":\n                label = \"contradiction\"\n            examples.append(\n                InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label)\n            )\n    return examples\n\n", "CodeBleu_score": 47.856913027908355}, {"code": "def get_train_examples(self, data_dir)\n\t\n    examples = []\n    with tf.gfile.Open(os.path.join(data_dir, \"train.tsv\"), \"r\") as f:\n        reader = csv.reader(f, delimiter=\"\\t\")\n        for line in reader:\n            if sys.version_info[0] == 2:\n                line = list(unicode(cell, \"utf-8\") for cell in line)\n            examples.append(\n                InputExample(\n                    guid=line[0],\n                    text_a=line[3],\n                    text_b=line[4],\n                    label=line[1],\n                )\n            )\n    return examples\n\n", "CodeBleu_score": 23.496070166917683}]}
{"instance_id": "26a0bb96dab9", "generation_result": [{"code": "def metric_fn(per_example_loss, label_ids, logits, is_real_example)\n\t\n    predictions = tf.argmax(logits, axis=-1, output_type=tf.int32)\n    accuracy = tf.metrics.accuracy(\n        labels=label_ids, predictions=predictions, weights=is_real_example)\n    loss = tf.metrics.mean(values=per_example_loss, weights=is_real_example)\n    return {\n        \"eval_accuracy\": accuracy,\n        \"eval_loss\": loss,\n    }\n\n", "CodeBleu_score": 65.47238279114057}, {"code": "def metric_fn(per_example_loss, label_ids, logits, is_real_example)\n\t\n    predictions = tf.argmax(logits, axis=-1, output_type=tf.int32)\n    accuracy = tf.metrics.accuracy(label_ids, predictions, weights=is_real_example)\n    loss = tf.metrics.mean(values=per_example_loss, weights=is_real_example)\n    return {\"eval_accuracy\": accuracy, \"eval_loss\": loss}\n\n", "CodeBleu_score": 50.86492464554971}, {"code": "def metric_fn(per_example_loss, label_ids, logits, is_real_example)\n\t\n\n    predictions = tf.argmax(logits, axis=-1, output_type=tf.int32)\n    loss = tf.losses.sparse_softmax_cross_entropy(\n        labels=label_ids, logits=logits)\n\n    return {\n        \"eval_accuracy\":\n            tf.metrics.accuracy(\n                labels=label_ids, predictions=predictions, weights=is_real_example),\n        \"eval_loss\":\n            tf.metrics.mean(values=loss, weights=is_real_example)\n    }\n\n", "CodeBleu_score": 47.008437132376066}]}
{"instance_id": "6154e4d28509", "generation_result": [{"code": "def convert_examples_to_features(examples, label_list, max_seq_length,\n                                 tokenizer)\n\t\n    label_map = {}\n    for (i, label) in enumerate(label_list):\n        label_map[label] = i\n\n    features = []\n    for (ex_index, example) in enumerate(examples):\n        if ex_index % 10000 == 0:\n            tf.logging.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n\n        feature = convert_single_example(ex_index, example, label_map,\n                                         max_seq_length, tokenizer)\n\n        features.append(feature)\n    return features\n\n", "CodeBleu_score": 71.43894611015895}, {"code": "def convert_examples_to_features(examples, label_list, max_seq_length,\n                                 tokenizer)\n\t\n    label_map = {}\n    for (i, label) in enumerate(label_list):\n        label_map[label] = i\n\n    features = []\n    for (ex_index, example) in enumerate(examples):\n        if ex_index % 10000 == 0:\n            tf.logging.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n\n        feature = convert_single_example(ex_index, example, label_map,\n                                         max_seq_length, tokenizer)\n\n        features.append(feature)\n    return features\n\n", "CodeBleu_score": 71.43894611015895}, {"code": "def convert_examples_to_features(examples, label_list, max_seq_length,\n                                 tokenizer)\n\t\n    label_map = {}\n    for (i, label) in enumerate(label_list):\n        label_map[label] = i\n\n    features = []\n    for (ex_index, example) in enumerate(examples):\n        if ex_index % 10000 == 0:\n            tf.logging.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n\n        feature = convert_single_example(ex_index, example, label_map,\n                                         max_seq_length, tokenizer)\n\n        features.append(feature)\n    return features\n\n", "CodeBleu_score": 71.43894611015895}]}
{"instance_id": "e09c0db71ebb", "generation_result": [{"code": "def get_next_sentence_output(bert_config, input_tensor, labels)\n\t\n    with tf.variable_scope(\"cls/seq_relationship\"):\n        output_weights = tf.get_variable(\n            \"output_weights\",\n            shape=[2, bert_config.hidden_size],\n            initializer=modeling.create_initializer(bert_config.initializer_range))\n        output_bias = tf.get_variable(\n            \"output_bias\", shape=[2], initializer=tf.zeros_initializer())\n\n    logits = tf.matmul(input_tensor, output_weights, transpose_b=True)\n    logits = tf.nn.bias_add(logits, output_bias)\n    log_probs = tf.nn.log_softmax(logits, axis=-1)\n\n    labels = tf.reshape(labels, [-1])\n    one_hot_labels = tf.one_hot(labels, depth=2, dtype=tf.float32)\n\n    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n    loss = tf.reduce_mean(per_example_loss)\n    return (loss, per_example_loss, log_probs)\n\n", "CodeBleu_score": 87.87003311020517}, {"code": "def get_next_sentence_output(bert_config, input_tensor, labels)\n\t\n    with tf.variable_scope(\"cls/seq_relationship\"):\n        output_weights = tf.get_variable(\n            \"output_weights\",\n            shape=[2, bert_config.hidden_size],\n            initializer=modeling.create_initializer(bert_config.initializer_range))\n        output_bias = tf.get_variable(\n            \"output_bias\", shape=[2], initializer=tf.zeros_initializer())\n\n    logits = tf.matmul(input_tensor, output_weights, transpose_b=True)\n    logits = tf.nn.bias_add(logits, output_bias)\n    log_probs = tf.nn.log_softmax(logits, axis=-1)\n    labels = tf.reshape(labels, [-1])\n    one_hot_labels = tf.one_hot(labels, depth=2, dtype=tf.float32)\n    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n    loss = tf.reduce_mean(per_example_loss)\n    return (loss, per_example_loss, log_probs)\n\n", "CodeBleu_score": 87.87003311020517}, {"code": "def get_next_sentence_output(bert_config, input_tensor, labels)\n\t\n    with tf.variable_scope(\"cls/seq_relationship\"):\n        output_weights = tf.get_variable(\n            \"output_weights\",\n            shape=[2, bert_config.hidden_size],\n            initializer=modeling.create_initializer(bert_config.initializer_range))\n        output_bias = tf.get_variable(\n            \"output_bias\", shape=[2], initializer=tf.zeros_initializer())\n\n    logits = tf.matmul(input_tensor, output_weights, transpose_b=True)\n    logits = tf.nn.bias_add(logits, output_bias)\n    log_probs = tf.nn.log_softmax(logits, axis=-1)\n    labels = tf.reshape(labels, [-1])\n    one_hot_labels = tf.one_hot(labels, depth=2, dtype=tf.float32)\n    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n    loss = tf.reduce_mean(per_example_loss)\n    return (loss, per_example_loss, log_probs)\n\n", "CodeBleu_score": 87.87003311020517}]}
{"instance_id": "1f66b020f485", "generation_result": [{"code": "def gather_indexes(sequence_tensor, positions)\n\t\n    sequence_shape = modeling.get_shape_list(sequence_tensor, expected_rank=3)\n    batch_size = sequence_shape[0]\n    seq_length = sequence_shape[1]\n    width = sequence_shape[2]\n    flat_offsets = tf.reshape(tf.range(0, batch_size, dtype=tf.int32) * seq_length, [-1, 1])\n    flat_positions = tf.reshape(positions + flat_offsets, [-1])\n    flat_sequence_tensor = tf.reshape(sequence_tensor, [batch_size * seq_length, width])\n    output_tensor = tf.gather(flat_sequence_tensor, flat_positions)\n    return output_tensor\n\n", "CodeBleu_score": 55.18468812590758}, {"code": "def gather_indexes(sequence_tensor, positions)\n\t\n    sequence_flat = tf.reshape(sequence_tensor, [-1])\n    flat_offsets = tf.reshape(tf.range(0, tf.shape(sequence_tensor)[0]) * tf.shape(sequence_tensor)[1], [-1, 1])\n    flat_positions = flat_offsets + tf.reshape(positions, [-1, 1])\n    flat_sequence_tensor = tf.gather(sequence_flat, flat_positions)\n    return flat_sequence_tensor\n\n", "CodeBleu_score": 21.332799916858757}, {"code": "def gather_indexes(sequence_tensor, positions)\n\t\n    sequence_flat = tf.reshape(sequence_tensor, [-1])\n    flat_offsets = tf.reshape(tf.range(0, tf.shape(sequence_tensor)[0]) * tf.shape(sequence_tensor)[1], [-1, 1])\n    flat_positions = tf.reshape(positions + flat_offsets, [-1])\n    output_tensor = tf.gather(sequence_flat, flat_positions)\n    return output_tensor\n\n", "CodeBleu_score": 24.102585311015652}]}
{"instance_id": "956bffee9a33", "generation_result": [{"code": "def _decode_record(record, name_to_features)\n\t\n    example = tf.parse_single_example(record, name_to_features)\n    for name in list(example.keys()):\n        t = example[name]\n        if t.dtype == tf.int64:\n            t = tf.to_int32(t)\n        example[name] = t\n    return example\n\n", "CodeBleu_score": 71.91618952996937}, {"code": "def _decode_record(record, name_to_features)\n\t\n    example = tf.parse_single_example(record, name_to_features)\n    for name in list(example.keys()):\n        t = example[name]\n        if t.dtype == tf.int64:\n            t = tf.to_int32(t)\n        example[name] = t\n    return example\n\n", "CodeBleu_score": 71.91618952996937}, {"code": "def _decode_record(record, name_to_features)\n\t\n    example = tf.parse_single_example(record, name_to_features)\n    for name in list(example.keys()):\n        t = example[name]\n        if t.dtype == tf.int64:\n            t = tf.to_int32(t)\n        example[name] = t\n    return example\n\n", "CodeBleu_score": 71.91618952996937}]}
{"instance_id": "6b97f354e641", "generation_result": [{"code": "def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer,\n                         orig_answer_text)\n\t\n    tok_answer_text = tokenizer.tokenize(orig_answer_text)\n    for new_start in range(input_start, input_end + 1):\n        for new_end in range(input_end, new_start - 1, -1):\n            text_span = \" \".join(doc_tokens[new_start:(new_end + 1)])\n            if text_span == \"\".join(tok_answer_text):\n                return (new_start, new_end)\n    return (input_start, input_end)\n\n", "CodeBleu_score": 69.00394732612052}, {"code": "def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer,\n                         orig_answer_text)\n\t\n    tok_answer_text = tokenizer.tokenize(orig_answer_text)\n    for new_start in range(input_start, input_end + 1):\n        for new_end in range(input_end, new_start - 1, -1):\n            text_span = \" \".join(doc_tokens[new_start:(new_end + 1)])\n            if text_span == \"\".join(tok_answer_text):\n                return (new_start, new_end)\n    return (input_start, input_end)\n\n", "CodeBleu_score": 69.00394732612052}, {"code": "def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer,\n                         orig_answer_text)\n\t\n    tok_answer_text = tokenizer.tokenize(orig_answer_text)\n    for new_start in range(input_start, input_end + 1):\n        for new_end in range(input_end, new_start - 1, -1):\n            text_span = \" \".join(doc_tokens[new_start:(new_end + 1)])\n            if text_span == \"\".join(tok_answer_text):\n                return (new_start, new_end)\n    return (input_start, input_end)\n\n", "CodeBleu_score": 69.00394732612052}]}
{"instance_id": "aba334b92b34", "generation_result": [{"code": "def _check_is_max_context(doc_spans, cur_span_index, position)\n\t\n    best_score = None\n    best_span_index = None\n    for (span_index, doc_span) in enumerate(doc_spans):\n        end = doc_span.start + doc_span.length - 1\n        if position < doc_span.start:\n            continue\n        if position > end:\n            continue\n        num_left_context = position - doc_span.start\n        num_right_context = end - position\n        score = min(num_left_context, num_right_context) + 0.01 * doc_span.length\n        if best_score is None or score > best_score:\n            best_score = score\n            best_span_index = span_index\n    return cur_span_index == best_span_index\n\n", "CodeBleu_score": 78.6774915248039}, {"code": "def _check_is_max_context(doc_spans, cur_span_index, position)\n\t\n    best_score = None\n    best_span_index = None\n    for (span_index, doc_span) in enumerate(doc_spans):\n        end = doc_span.start + doc_span.length - 1\n        if position < doc_span.start:\n            continue\n        if position > end:\n            continue\n        num_left_context = position - doc_span.start\n        num_right_context = end - position\n        score = min(num_left_context, num_right_context) + 0.01 * doc_span.length\n        if best_score is None or score > best_score:\n            best_score = score\n            best_span_index = span_index\n    return cur_span_index == best_span_index\n\n", "CodeBleu_score": 78.6774915248039}, {"code": "def _check_is_max_context(doc_spans, cur_span_index, position)\n\t\n    best_score = None\n    best_span_index = None\n    for (span_index, doc_span) in enumerate(doc_spans):\n        end = doc_span.start + doc_span.length - 1\n        if position < doc_span.start:\n            continue\n        if position > end:\n            continue\n        num_left_context = position - doc_span.start\n        num_right_context = end - position\n        score = min(num_left_context, num_right_context) + 0.01 * doc_span.length\n        if best_score is None or score > best_score:\n            best_score = score\n            best_span_index = span_index\n    return cur_span_index == best_span_index\n\n", "CodeBleu_score": 78.6774915248039}]}
{"instance_id": "cf92ad509f6a", "generation_result": [{"code": "def load_vocab(vocab_file)\n\t\n    vocab = collections.OrderedDict()\n    index = 0\n    with open(vocab_file, \"r\", encoding=\"utf-8\") as reader:\n        while True:\n            token = reader.readline()\n            if not token:\n                break\n            token = token.rstrip(\"", "CodeBleu_score": 44.423611261959635}, {"code": "def load_vocab(vocab_file)\n\t\n    vocab = collections.OrderedDict()\n    index = 0\n    with open(vocab_file, \"r\", encoding=\"utf-8\") as reader:\n        while True:\n            token = reader.readline()\n            if not token:\n                break\n            token = token.rstrip(\"", "CodeBleu_score": 44.423611261959635}, {"code": "def load_vocab(vocab_file)\n\t\n    vocab = collections.OrderedDict()\n    index = 0\n    with open(vocab_file, \"r\", encoding=\"utf-8\") as reader:\n        while True:\n            token = normalize_text(reader.readline())\n            if not token:\n                break\n            token = token.strip()\n            vocab[token] = index\n            index += 1\n    return vocab\n\n", "CodeBleu_score": 62.89448435610174}]}
{"instance_id": "6c865803ac12", "generation_result": [{"code": "def convert_by_vocab(vocab, items)\n\t\n    output = []\n    for item in items:\n        output.append(vocab[item])\n    return output\n\n", "CodeBleu_score": 79.63749907165396}, {"code": "def convert_by_vocab(vocab, items)\n\t\n    output = []\n    for item in items:\n        output.append(vocab[item])\n    return output\n\n", "CodeBleu_score": 79.63749907165396}, {"code": "def convert_by_vocab(vocab, items)\n\t\n    output = []\n    for item in items:\n        output.append(vocab[item])\n    return output\n\n", "CodeBleu_score": 79.63749907165396}]}
{"instance_id": "e93b9200dc4a", "generation_result": [{"code": "def tokenize(self, text)\n\t\n    split_tokens = []\n    for token in self.basic_tokenizer.tokenize(text):\n        for sub_token in self.wordpiece_tokenizer.tokenize(token):\n            split_tokens.append(sub_token)\n    return split_tokens\n\n", "CodeBleu_score": 88.10762355418231}, {"code": "def tokenize(self, text)\n\t\n    text = convert_to_unicode(text)\n    text = self._clean_text(text)\n    # This was added on November 1st, 2018 for the multilingual and Chinese\n    # models. This is also applied to the English models now, but it doesn't\n    # matter since the English models were not trained on any Chinese data\n    # and generally don't have any Chinese data in them (there are Chinese\n    # characters in the vocabulary because Wikipedia does have some Chinese\n    # words in the English Wikipedia.).\n    text = self._tokenize_chinese_chars(text)\n    orig_tokens = whitespace_tokenize(text)\n    split_tokens = []\n    for token in orig_tokens:\n        split_tokens.extend(self._run_split_on_punc(token))\n    output_tokens = whitespace_tokenize(\" \".join(split_tokens))\n    return output_tokens\n\n", "CodeBleu_score": 30.972225781878667}, {"code": "def tokenize(self, text)\n\t\n    # Split on spaces so that sub-word tokens will not be concatenated\n    # by the tokenizer.\n    tokens = self.basic_tokenizer.tokenize(text)\n    split_tokens = []\n    for token in tokens:\n        split_tokens.extend(self.wordpiece_tokenizer.tokenize(token))\n    return split_tokens\n\n", "CodeBleu_score": 45.690001911733404}]}
{"instance_id": "2ff6f6db53c9", "generation_result": [{"code": "def safe_load(path, offset, duration, sample_rate, dtype)\n\t\n    try:\n        audio_data, _ = load(\n            path,\n            offset=offset,\n            duration=duration,\n            sample_rate=sample_rate,\n            dtype=dtype,\n        )\n        return audio_data, False\n    except Exception as e:\n        print(f\"Failed to load audio data from {path}\")\n        print(f\"Error: {e}\")\n        return -1.0, True\n\n", "CodeBleu_score": 30.431130659878853}, {"code": "def safe_load(path, offset, duration, sample_rate, dtype)\n\t\n    try:\n        audio_data, _ = load_audio(\n            path, offset, duration, sample_rate, dtype\n        )  # Load audio data from the specified file path\n        return audio_data, False  # Return the audio data and a False flag indicating no error\n    except Exception as e:\n        print(\n            f\"Failed to load audio data from {path} with offset {offset} and duration {duration} using sample rate {sample_rate} and dtype {dtype}.\"\n        )\n        print(f\"Error: {e}\")\n        return -1.0, True  # Return a default value of -1.0 with a True flag indicating an error\n\n", "CodeBleu_score": 24.497940116363278}, {"code": "def safe_load(path, offset, duration, sample_rate, dtype)\n\t\n    try:\n        audio, _ = import_module(\"spleeter.audio.adapter\").Adapter.default.load(\n            path, offset, duration, sample_rate, dtype\n        )\n        return audio, False\n    except Exception as e:\n        logging.warning(f\"Unable to load audio from {path}. {e}\")\n        return -1.0, True\n\n", "CodeBleu_score": 29.69136983076726}]}
{"instance_id": "12fbe1f03d8f", "generation_result": [{"code": "def to_n_channels(waveform: tf.Tensor, n_channels: int) -> tf.Tensor\n\t\n    n_channels_waveform = waveform.shape[2]\n    if n_channels_waveform > n_channels:\n        waveform = waveform[:, :, :n_channels]\n    elif n_channels_waveform < n_channels:\n        waveform = tf.tile(waveform, [1, 1, n_channels // n_channels_waveform])\n        waveform = waveform[:, :, :n_channels]\n    return waveform\n\n", "CodeBleu_score": 43.73197893068159}, {"code": "def to_n_channels(waveform: tf.Tensor, n_channels: int) -> tf.Tensor\n\t\n    if waveform.shape[2] >= n_channels:\n        return waveform[:, :, :n_channels]\n    else:\n        return tf.tile(waveform, [1, 1, n_channels // waveform.shape[2]])[:, :, :n_channels]\n\n", "CodeBleu_score": 40.30116288755401}, {"code": "def to_n_channels(waveform: tf.Tensor, n_channels: int) -> tf.Tensor\n\t\n    if n_channels < 1:\n        raise ValueError(f\"n_channels must be greater than or equal to 1, but {n_channels} was given.\")\n    if waveform.shape[-1] == n_channels:\n        return waveform\n    if waveform.shape[-1] > n_channels:\n        return waveform[..., :n_channels]\n    return tf.tile(waveform, [1, 1, n_channels])\n\n", "CodeBleu_score": 42.09707939916595}]}
{"instance_id": "5e215f029b2e", "generation_result": [{"code": "def spectrogram_to_db_uint(\n    spectrogram: tf.Tensor, db_range: float = 100.0, **kwargs\n) -> tf.Tensor\n\t\n    spectrogram_db = gain_to_db(spectrogram, **kwargs)\n    max_db = tf.math.reduce_max(spectrogram_db)\n    spectrogram_db = tf.clip_by_value(spectrogram_db, max_db - db_range, max_db)\n    return tf.cast(spectrogram_db, tf.uint8)\n\n", "CodeBleu_score": 41.962933206993405}, {"code": "def spectrogram_to_db_uint(\n    spectrogram: tf.Tensor, db_range: float = 100.0, **kwargs\n) -> tf.Tensor\n\t\n    # Convert spectrogram to dB\n    db_spectrogram = gain_to_db(spectrogram, **kwargs)\n    # Find the maximum dB value\n    max_db = tf.math.reduce_max(db_spectrogram)\n    # Clip the dB spectrogram to the specified range below the maximum\n    clipped_db_spectrogram = tf.clip_by_value(\n        db_spectrogram, max_db - db_range, max_db\n    )\n    # Convert the clipped dB spectrogram to uint8\n    uint8_spectrogram = tf.cast(\n        (clipped_db_spectrogram - (max_db - db_range))\n        * (255 / db_range),\n        tf.uint8,\n    )\n    return uint8_spectrogram\n\n", "CodeBleu_score": 43.71374004374914}, {"code": "def spectrogram_to_db_uint(\n    spectrogram: tf.Tensor, db_range: float = 100.0, **kwargs\n) -> tf.Tensor\n\t\n    spectrogram_db = gain_to_db(spectrogram, **kwargs)\n    max_db = tf.math.reduce_max(spectrogram_db)\n    spectrogram_db = tf.clip_by_value(spectrogram_db, max_db - db_range, max_db)\n    spectrogram_db_uint = tf.cast(\n        (spectrogram_db - (max_db - db_range)) / db_range * 255, tf.uint8\n    )\n    return spectrogram_db_uint\n\n", "CodeBleu_score": 44.35420562266321}]}
{"instance_id": "b65c0968dae7", "generation_result": [{"code": "def compute_spectrogram_tf(\n    waveform: tf.Tensor,\n    frame_length: int = 2048,\n    frame_step: int = 512,\n    spec_exponent: float = 1.0,\n    window_exponent: float = 1.0,\n) -> tf.Tensor\n\t\n    # Hann window function raised to a specified exponent\n    window = hann_window(frame_length, periodic=True, dtype=tf.dtypes.float32)\n    window = tf.pow(window, window_exponent)\n    # Short-Time Fourier Transform (STFT)\n    spectrogram = stft(\n        waveform,\n        frame_length=frame_length,\n        frame_step=frame_step,\n        fft_length=frame_length,\n        window_fn=lambda x, y: window,\n        pad_end=False,\n    )\n    # Transpose and absolute value raised to a specified exponent\n    spectrogram = tf.abs(tf.transpose(spectrogram))\n    spectrogram = tf.pow(spectrogram, spec_exponent)\n    return spectrogram\n\n", "CodeBleu_score": 41.10387736689709}, {"code": "def compute_spectrogram_tf(\n    waveform: tf.Tensor,\n    frame_length: int = 2048,\n    frame_step: int = 512,\n    spec_exponent: float = 1.0,\n    window_exponent: float = 1.0,\n) -> tf.Tensor\n\t\n    waveform = tf.expand_dims(waveform, axis=0)\n    stft_out = stft(\n        waveform,\n        frame_length=frame_length,\n        frame_step=frame_step,\n        fft_length=frame_length,\n        window_fn=lambda frame_length: tf.pow(\n            hann_window(frame_length, periodic=True), window_exponent\n        ),\n    )\n    spectrogram = tf.abs(stft_out)\n    spectrogram = tf.pow(spectrogram, spec_exponent)\n    spectrogram = tf.transpose(spectrogram, [0, 2, 1])\n    return spectrogram\n\n", "CodeBleu_score": 38.240401719096525}, {"code": "def compute_spectrogram_tf(\n    waveform: tf.Tensor,\n    frame_length: int = 2048,\n    frame_step: int = 512,\n    spec_exponent: float = 1.0,\n    window_exponent: float = 1.0,\n) -> tf.Tensor\n\t\n    # Compute the STFT\n    stft_output = stft(\n        waveform,\n        frame_length=frame_length,\n        frame_step=frame_step,\n        fft_length=frame_length,\n        window_fn=lambda x: hann_window(x, periodic=False) ** window_exponent,\n    )\n    # Transpose the STFT output and take the absolute value\n    spectrogram = tf.abs(tf.transpose(stft_output))\n    # Apply the specified exponent to the spectrogram\n    spectrogram = spectrogram**spec_exponent\n    return spectrogram\n\n", "CodeBleu_score": 37.930890160998274}]}
{"instance_id": "a4686140a634", "generation_result": [{"code": "def time_stretch(\n    spectrogram: tf.Tensor,\n    factor: float = 1.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n    time_dim = tf.shape(spectrogram)[1]\n    new_time_dim = tf.cast(time_dim * factor, tf.int32)\n    spectrogram = tf.image.resize(spectrogram, [new_time_dim, tf.shape(spectrogram)[0]], method)\n    spectrogram = tf.reshape(spectrogram, [new_time_dim, -1, tf.shape(spectrogram)[2]])\n    spectrogram = tf.image.resize(spectrogram, [time_dim, tf.shape(spectrogram)[1]], method)\n    return spectrogram\n\n", "CodeBleu_score": 36.51946420271365}, {"code": "def time_stretch(\n    spectrogram: tf.Tensor,\n    factor: float = 1.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n    time_dim = spectrogram.shape[-1]\n    new_time_dim = tf.cast(tf.math.round(time_dim * factor), tf.int32)\n    resized_spectrogram = tf.image.resize(\n        spectrogram,\n        size=(spectrogram.shape[0], new_time_dim),\n        method=method,\n        antialias=True,\n    )\n    if factor > 1:\n        resized_spectrogram = resized_spectrogram[\n            :, :time_dim\n        ]  # Crop to original time dimension\n    else:\n        resized_spectrogram = tf.pad(\n            resized_spectrogram, [[0, 0], [0, time_dim - new_time_dim]]\n        )  # Pad to original time dimension\n    return resized_spectrogram\n\n", "CodeBleu_score": 33.11690200204288}, {"code": "def time_stretch(\n    spectrogram: tf.Tensor,\n    factor: float = 1.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n    # Calculate the new time dimension by scaling the original time dimension by the factor\n    new_time_dim = tf.cast(tf.shape(spectrogram)[-1] * factor, tf.int32)\n    # Resize the spectrogram using the specified interpolation method\n    resized_spectrogram = tf.image.resize(\n        spectrogram, size=[tf.shape(spectrogram)[0], new_time_dim], method=method\n    )\n    # Adjust the resized spectrogram to the original dimensions by cropping or padding\n    if resized_spectrogram.shape[-1] < spectrogram.shape[-1]:\n        resized_spectrogram = tf.pad(\n            resized_spectrogram,\n            [[0, 0], [0, 0], [0, spectrogram.shape[-1] - resized_spectrogram.shape[-1]]],\n        )\n    elif resized_spectrogram.shape[-1] > spectrogram.shape[-1]:\n        resized_spectrogram = resized_spectrogram[..., : spectrogram.shape[-1]]\n    # Return the resized and adjusted spectrogram\n    return resized_spectrogram\n\n", "CodeBleu_score": 38.39655630054422}]}
{"instance_id": "6dd01101d4ae", "generation_result": [{"code": "def pitch_shift(\n    spectrogram: tf.Tensor,\n    semitone_shift: float = 0.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n    # Calculate the resizing factor\n    resizing_factor = 2.0 ** (semitone_shift / 12.0)\n    # Resize the spectrogram along the frequency axis\n    resized_spectrogram = tf.image.resize(\n        spectrogram,\n        size=(int(spectrogram.shape[1] * resizing_factor), spectrogram.shape[2]),\n        method=method,\n    )\n    # Pad the resized spectrogram to maintain the original dimensions\n    padded_spectrogram = tf.pad(\n        resized_spectrogram,\n        paddings=[[0, 0], [0, spectrogram.shape[1] - resized_spectrogram.shape[1]]],\n    )\n    return padded_spectrogram\n\n", "CodeBleu_score": 29.62728550450881}, {"code": "def pitch_shift(\n    spectrogram: tf.Tensor,\n    semitone_shift: float = 0.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n    # Calculate the resizing factor based on the semitone shift\n    semitone_shift_factor = 2.0 ** (semitone_shift / 12.0)\n    # Resize the spectrogram along the frequency axis using the specified method\n    resized_spectrogram = tf.image.resize(\n        spectrogram,\n        size=(\n            tf.cast(tf.shape(spectrogram)[0] * semitone_shift_factor, tf.int32),\n            tf.shape(spectrogram)[1],\n        ),\n        method=method,\n    )\n    # Pad the resized spectrogram to maintain the original dimensions\n    resized_spectrogram = tf.pad(\n        resized_spectrogram,\n        paddings=[[0, tf.shape(spectrogram)[0] - tf.shape(resized_spectrogram)[0]]],\n        mode=\"CONSTANT\",\n    )\n    return resized_spectrogram\n\n", "CodeBleu_score": 31.58100008708678}, {"code": "def pitch_shift(\n    spectrogram: tf.Tensor,\n    semitone_shift: float = 0.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n    # Calculate the resizing factor\n    semitone_shift = tf.cast(semitone_shift, dtype=tf.float32)\n    resizing_factor = tf.cast(2.0 ** (semitone_shift / 12.0), dtype=tf.float32)\n\n    # Resize the spectrogram along the frequency axis using the specified method\n    resized_spectrogram = tf.image.resize(\n        spectrogram,\n        size=tf.cast(tf.shape(spectrogram)[1] * resizing_factor, dtype=tf.int32),\n        method=method,\n    )\n\n    # Pad the resized spectrogram to maintain the original dimensions\n    pad_width = tf.cast(\n        tf.shape(spectrogram)[1] - tf.shape(resized_spectrogram)[1], dtype=tf.int32\n    )\n    resized_spectrogram = tf.pad(\n        resized_spectrogram, [[0, 0], [0, pad_width], [0, 0]]\n    )\n\n    return resized_spectrogram\n\n", "CodeBleu_score": 32.13177273782918}]}
{"instance_id": "612f0826e56a", "generation_result": [{"code": "def _get_conv_activation_layer(params: Dict) -> Any\n\t\n    if params[\"conv_activation\"] == \"relu\":\n        return ReLU()\n    elif params[\"conv_activation\"] == \"elu\":\n        return ELU()\n    else:\n        return LeakyReLU(alpha=0.2)\n\n", "CodeBleu_score": 37.81568218186228}, {"code": "def _get_conv_activation_layer(params: Dict) -> Any\n\t\n    if params[\"conv_activation\"] == \"ReLU\":\n        return ReLU()\n    elif params[\"conv_activation\"] == \"ELU\":\n        return ELU()\n    else:\n        return LeakyReLU(alpha=0.2)\n\n", "CodeBleu_score": 46.02205608514587}, {"code": "def _get_conv_activation_layer(params: Dict) -> Any\n\t\n    if params[\"conv_activation\"] == \"relu\":\n        return ReLU()\n    elif params[\"conv_activation\"] == \"elu\":\n        return ELU()\n    else:\n        return LeakyReLU(alpha=0.2)\n\n", "CodeBleu_score": 37.81568218186228}]}
{"instance_id": "2f8b5a2fc0f2", "generation_result": [{"code": "def softmax_unet(\n    input_tensor: tf.Tensor, instruments: Iterable[str], params: Dict = {}\n) -> Dict\n\t\n    logit_masks = []\n    for instrument in instruments:\n        logit_mask = unet(input_tensor, params)\n        logit_masks.append(logit_mask)\n    logit_masks = Concatenate(axis=3)(logit_masks)\n    softmax_masks = Softmax(axis=3)(logit_masks)\n    outputs = {}\n    for i, instrument in enumerate(instruments):\n        outputs[instrument] = Multiply()([input_tensor, softmax_masks[:, :, :, i:i + 1]])\n    return outputs\n\n", "CodeBleu_score": 31.12279641833175}, {"code": "def softmax_unet(\n    input_tensor: tf.Tensor, instruments: Iterable[str], params: Dict = {}\n) -> Dict\n\t\n    # Apply U-Net model to input tensor for each instrument\n    logit_masks = []\n    for instrument in instruments:\n        logit_mask = unet_model(input_tensor, instrument, params)\n        logit_masks.append(logit_mask)\n\n    # Combine logit masks using softmax function\n    logit_masks = tf.stack(logit_masks, axis=0)\n    softmax_masks = tf.nn.softmax(logit_masks, axis=0)\n\n    # Create output dictionary of spectrograms for each instrument\n    spectrograms = {}\n    for i, instrument in enumerate(instruments):\n        spectrograms[instrument] = tf.multiply(softmax_masks[i], input_tensor)\n\n    return spectrograms\n\n", "CodeBleu_score": 33.53361613301424}, {"code": "def softmax_unet(\n    input_tensor: tf.Tensor, instruments: Iterable[str], params: Dict = {}\n) -> Dict\n\t\n    logit_masks = []\n    for instrument in instruments:\n        # Create the U-Net model for the instrument\n        unet = create_unet(input_tensor, instrument, params)\n        # Apply the U-Net model to the input tensor\n        logit_mask = unet(input_tensor)\n        # Append the logit mask to the list of logit masks\n        logit_masks.append(logit_mask)\n\n    # Combine the logit masks using a softmax function along the specified axis\n    logit_mask = tf.keras.layers.Concatenate(axis=3)(logit_masks)\n    logit_mask = tf.keras.layers.Softmax(axis=3)(logit_mask)\n\n    # Create an output dictionary where each entry is a spectrogram for an instrument\n    spectrograms = {}\n    for i, instrument in enumerate(instruments):\n        spectrograms[instrument] = tf.keras.layers.Multiply()([input_tensor, logit_mask[:, :, :, i:i + 1]])\n\n    return spectrograms\n\n", "CodeBleu_score": 34.647274213913256}]}
{"instance_id": "400b19ce8520", "generation_result": [{"code": "def compute_file_checksum(path)\n\t\n    with open(path, \"rb\") as f:\n        hash_object = hashlib.sha256()\n        while chunk := f.read(8192):\n            hash_object.update(chunk)\n    return hash_object.hexdigest()\n\n", "CodeBleu_score": 41.11118996577415}, {"code": "def compute_file_checksum(path)\n\t\n    hash_sha256 = hashlib.sha256()\n    with open(path, \"rb\") as f:\n        for chunk in iter(lambda: f.read(4096), b\"\"):\n            hash_sha256.update(chunk)\n    return hash_sha256.hexdigest()\n\n", "CodeBleu_score": 57.44570069380956}, {"code": "def compute_file_checksum(path)\n\t\n    hash_sha256 = hashlib.sha256()\n    with open(path, \"rb\") as f:\n        for chunk in iter(lambda: f.read(4096), b\"\"):\n            hash_sha256.update(chunk)\n    return hash_sha256.hexdigest()\n\n", "CodeBleu_score": 57.44570069380956}]}
{"instance_id": "5f2e9c6e15f1", "generation_result": [{"code": "def checksum(self, name: str) -> str\n\t\n    host = environ.get(\"TORCH_MODEL_ZOO\", \"https://download.pytorch.org\")\n    url = f\"{host}/models/{name}/index.json\"\n    try:\n        response = httpx.get(url)\n        response.raise_for_status()\n    except httpx.HTTPError as e:\n        raise ValueError(f\"Invalid response from {url}: {e}\")\n    index = response.json()\n    if name not in index:\n        raise ValueError(f\"Model {name} not found in {url}\")\n    return index[name][\"sha256\"]\n\n", "CodeBleu_score": 42.85507158912743}, {"code": "def checksum(self, name: str) -> str\n\t\n    url = f\"{self.host}/{self.repository}/releases/{self.release}/checksums.json\"\n    response = httpx.get(url)\n    if response.status_code!= 200:\n        raise ValueError(f\"Failed to get checksums from {url}\")\n    index = response.json()\n    if name not in index:\n        raise ValueError(f\"Model {name} not found in checksum index\")\n    return index[name]\n\n", "CodeBleu_score": 41.91241442243914}, {"code": "def checksum(self, name: str) -> str\n\t\n    if not self.host:\n        raise ValueError(\"host is not set\")\n    if not self.repository:\n        raise ValueError(\"repository is not set\")\n    if not self.release:\n        raise ValueError(\"release is not set\")\n    url = f\"{self.host}/{self.repository}/{self.release}/index.json\"\n    try:\n        response = httpx.get(url)\n        if response.status_code!= 200:\n            raise ValueError(f\"Failed to retrieve checksum index from {url}\")\n    except httpx.HTTPStatusError as e:\n        raise ValueError(f\"Failed to retrieve checksum index from {url}\") from e\n    index: Dict[str, str] = response.json()\n    if name not in index:\n        raise ValueError(f\"Model {name} not found in index\")\n    return index[name]\n\n", "CodeBleu_score": 43.93725880913495}]}
{"instance_id": "9312c8b6032e", "generation_result": [{"code": "def download(self, name: str, path: str) -> None:\n\t\n    url = self.url_map[name]\n    filename = url.rpartition('/')[2]\n    checksum = self.checksum_map[name]\n    if not filename:\n        raise ValueError(f'invalid url: {url}')\n    if not checksum:\n        raise ValueError(f'invalid checksum: {checksum}')\n\n    def _download_and_extract(url: str, filename: str, checksum: str, path: str) -> None:\n        \"\"\"Downloads a file from a given URL, verifies its checksum, and extracts it to a specified path.\n\n        Args:\n            url (str): The URL of the file to download.\n            filename (str): The name of the file to download.\n            checksum (str): The expected checksum of the file.\n            path (str): The path to extract the file to.\n\n        Raises:\n            ValueError: If the checksum of the downloaded file does not match the expected checksum.\n        \"\"\"\n        with NamedTemporaryFile() as temp_file:\n            # Download the file\n            with httpx.stream(\"GET\", url) as response:\n                if response.status_code!= 200:\n                    raise ValueError(f\"Failed to download file {filename} from {url}\")\n                for chunk in response.iter_bytes():\n                    temp_file.write(chunk)\n\n            # Verify the file's integrity\n            temp_file.seek(0)\n            actual_checksum = hashlib.sha256(temp_file.read()).hexdigest()\n            if actual_checksum!= checksum:\n                raise ValueError(f\"Checksum mismatch for file {filename}. Expected {checksum}, got {actual_checksum}.\")\n\n            # Extract the file\n            temp_file.seek(0)\n            with tarfile.open(fileobj=temp_file, mode='r:gz') as tar:\n                tar.extractall(path=path)\n\n    def _download_and_extract_from_cache(url: str, filename: str, checksum: str, path: str) -> None:\n        \"\"\"Downloads a file from a given URL, verifies its checksum, and extracts it to a specified path.\n\n        Args:\n            url (str): The URL of the file to download.\n            filename (str): The name of the file to download.\n            checksum (str): The expected checksum of the file.\n            path (str): The path to extract the file to.\n\n        Raises:\n            ValueError: If the checksum of the downloaded file does", "CodeBleu_score": 40.7053155818823}, {"code": "def download(self, name: str, path: str) -> None:\n\t\n    url = f\"https://download.pytorch.org/models/{name}.tar\"\n    print(f\"Downloading: \\\"{url}\\\" to {path}\")\n    r = httpx.get(url, stream=True)\n    r.raise_for_status()\n    # Calculate the SHA256 checksum of the downloaded file\n    sha256 = hashlib.sha256()\n    with NamedTemporaryFile() as f:\n        for chunk in r.iter_bytes():\n            sha256.update(chunk)\n            f.write(chunk)\n        f.flush()\n        # Extract the archive to the specified path\n        with tarfile.open(f.name, \"r\") as tar:\n            tar.extractall(path)\n    # Check the file's integrity by comparing its checksum with the expected value\n    if sha256.hexdigest()!= self.model_sha256[name]:\n        raise RuntimeError(\"Invalid checksum for downloaded file\")\n    # Delete the temporary file\n    os.remove(f.name)\n    print(f\"Done extracting {name} to {path}\")\n\n", "CodeBleu_score": 30.851244161606182}, {"code": "def download(self, name: str, path: str) -> None:\n\t\n    url = self.url_map[name]\n    with httpx.stream(\"GET\", url) as response:\n        if response.status_code!= 200:\n            raise RuntimeError(f\"Failed to download {name} archive from {url}\")\n        # Create a temporary file for the downloaded archive\n        with NamedTemporaryFile(delete=False) as archive_file:\n            # Stream the archive to the temporary file\n            for chunk in response.iter_bytes():\n                archive_file.write(chunk)\n            archive_file.flush()\n            # Calculate the SHA256 checksum of the downloaded archive\n            checksum = hashlib.sha256(archive_file.read()).hexdigest()\n            archive_file.seek(0)\n            # Check if the checksum matches the expected value\n            if checksum!= self.checksum_map[name]:\n                raise RuntimeError(\n                    f\"Invalid checksum for {name} archive. Expected {self.checksum_map[name]}, got {checksum}.\"\n                )\n            # Extract the archive to the specified path\n            with tarfile.open(fileobj=archive_file, mode=\"r:gz\") as tar:\n                tar.extractall(path=path)\n            # Delete the temporary file\n            os.remove(archive_file.name)\n    self.logger.info(f\"Extracted {name} archive to {path}\")\n\n", "CodeBleu_score": 41.13488886000331}]}
{"instance_id": "ee9493f37d77", "generation_result": [{"code": "def load_configuration(descriptor: str) -> Dict\n\t\n    if descriptor.startswith(\"embedded:\"):\n        name = descriptor.split(\":\")[1]\n        resource = f\"resources/configurations/{name}.json\"\n        if exists(resource):\n            with open(resource, \"r\") as file:\n                return json.load(file)\n        else:\n            raise FileNotFoundError(f\"Embedded configuration '{name}' not found.\")\n    elif exists(descriptor):\n        with open(descriptor, \"r\") as file:\n            return json.load(file)\n    else:\n        raise FileNotFoundError(f\"Configuration file '{descriptor}' not found.\")", "CodeBleu_score": 52.98481196501722}, {"code": "def load_configuration(descriptor: str) -> Dict\n\t\n    if descriptor.startswith(\"embedded:\"):\n        name = descriptor[9:]\n        if name in _embedded_configurations:\n            return json.loads(_embedded_configurations[name])\n        else:\n            raise ValueError(f\"Embedded configuration '{name}' not found\")\n    elif exists(descriptor):\n        with open(descriptor, \"r\") as f:\n            return json.load(f)\n    else:\n        raise ValueError(f\"File '{descriptor}' not found\")\n\n", "CodeBleu_score": 43.06063563812207}, {"code": "def load_configuration(descriptor: str) -> Dict\n\t\n    if descriptor.startswith(\"embedded:\"):\n        name = descriptor.replace(\"embedded:\", \"\")\n        path = f\"configurations/{name}.json\"\n        if exists(path):\n            with open(path, \"r\") as file:\n                return json.load(file)\n        else:\n            raise ValueError(f\"Embedded configuration '{name}' not found.\")\n    elif exists(descriptor):\n        with open(descriptor, \"r\") as file:\n            return json.load(file)\n    else:\n        raise ValueError(f\"Configuration file '{descriptor}' not found.\")", "CodeBleu_score": 53.724205692990154}]}
{"instance_id": "546134b67fbc", "generation_result": [{"code": "def sync_apply(\n    tensor_dict: Dict[str, tf.Tensor], func: Callable, concat_axis: int = 1\n) -> Dict[str, tf.Tensor]\n\t\n    if concat_axis not in (0, 1):\n        raise ValueError(\n            f\"concat_axis must be 0 or 1, not {concat_axis}. \"\n            \"Use concat_axis=0 to concatenate along the batch dimension, \"\n            \"and concat_axis=1 to concatenate along the sequence dimension.\"\n        )\n    # Concatenate tensors along the specified axis\n    concatenated_tensor = tf.concat(list(tensor_dict.values()), axis=concat_axis)\n    # Apply the function to the concatenated tensor\n    processed_tensor = func(concatenated_tensor)\n    # Split the processed tensor back into the original dictionary format\n    split_tensors = tf.split(processed_tensor, len(tensor_dict), axis=concat_axis)\n    processed_tensor_dict = dict(zip(tensor_dict.keys(), split_tensors))\n    return processed_tensor_dict\n\n", "CodeBleu_score": 42.30272738164605}, {"code": "def sync_apply(\n    tensor_dict: Dict[str, tf.Tensor], func: Callable, concat_axis: int = 1\n) -> Dict[str, tf.Tensor]\n\t\n    if concat_axis not in [0, 1]:\n        raise ValueError(\"concat_axis must be either 0 or 1\")\n\n    concat_tensor = tf.concat(list(tensor_dict.values()), axis=concat_axis)\n    processed_tensor = func(concat_tensor)\n    if concat_axis == 0:\n        split_tensor = tf.split(processed_tensor, len(tensor_dict), axis=0)\n    else:\n        split_tensor = tf.split(processed_tensor, len(tensor_dict), axis=1)\n    return {k: v for k, v in zip(tensor_dict.keys(), split_tensor)}\n\n", "CodeBleu_score": 42.2490825834592}, {"code": "def sync_apply(\n    tensor_dict: Dict[str, tf.Tensor], func: Callable, concat_axis: int = 1\n) -> Dict[str, tf.Tensor]\n\t\n    if concat_axis not in [0, 1]:\n        raise ValueError(\"Concat axis must be either 0 or 1\")\n    concat_tensor = tf.concat(list(tensor_dict.values()), axis=concat_axis)\n    concat_tensor = func(concat_tensor)\n    if concat_axis == 0:\n        split_tensors = tf.split(concat_tensor, len(tensor_dict), axis=0)\n    else:\n        split_tensors = tf.split(concat_tensor, len(tensor_dict), axis=1)\n    return dict(zip(tensor_dict.keys(), split_tensors))\n\n", "CodeBleu_score": 38.41769336471588}]}
{"instance_id": "df48e78d46d2", "generation_result": [{"code": "def from_float32_to_uint8(\n    tensor: tf.Tensor,\n    tensor_key: str = \"tensor\",\n    min_key: str = \"min\",\n    max_key: str = \"max\",\n) -> tf.Tensor\n\t\n    min_val = tf.math.reduce_min(tensor)\n    max_val = tf.math.reduce_max(tensor)\n    tensor = tf.cast(tf.clip_by_value(tensor, min_val, max_val), tf.float32)\n    tensor = tf.cast(\n        (tensor - min_val) / (max_val - min_val) * 255.0, tf.uint8\n    )  # type: ignore\n    return {\n        tensor_key: tensor,\n        min_key: min_val,\n        max_key: max_val,\n    }\n\n", "CodeBleu_score": 46.54286234712309}, {"code": "def from_float32_to_uint8(\n    tensor: tf.Tensor,\n    tensor_key: str = \"tensor\",\n    min_key: str = \"min\",\n    max_key: str = \"max\",\n) -> tf.Tensor\n\t\n    tensor_min = tf.math.reduce_min(tensor)\n    tensor_max = tf.math.reduce_max(tensor)\n    tensor_range = tensor_max - tensor_min\n    tensor = (tensor - tensor_min) / tensor_range\n    tensor = tf.math.round(tensor * 255)\n    tensor = tf.cast(tensor, tf.uint8)\n    return {\n        tensor_key: tensor,\n        min_key: tensor_min,\n        max_key: tensor_max,\n    }\n\n", "CodeBleu_score": 59.62176565522379}, {"code": "def from_float32_to_uint8(\n    tensor: tf.Tensor,\n    tensor_key: str = \"tensor\",\n    min_key: str = \"min\",\n    max_key: str = \"max\",\n) -> tf.Tensor\n\t\n    min_value = tf.math.reduce_min(tensor)\n    max_value = tf.math.reduce_max(tensor)\n    normalized_tensor = tf.math.round(\n        ((tensor - min_value) / (max_value - min_value)) * 255.0\n    )\n    return {\n        tensor_key: tf.cast(normalized_tensor, tf.uint8),\n        min_key: min_value,\n        max_key: max_value,\n    }\n\n", "CodeBleu_score": 43.46105485969552}]}
{"instance_id": "7f479e5de492", "generation_result": [{"code": "def pad_and_partition(tensor: tf.Tensor, segment_len: int) -> tf.Tensor\n\t\n    # Calculate the padding needed to make the tensor length a multiple of the segment length\n    padding_needed = segment_len - tf.math.mod(tf.shape(tensor)[0], segment_len)\n    # Pad the tensor with zeros at the end\n    padded_tensor = tf.pad(tensor, [[0, padding_needed], [0, 0]])\n    # Reshape the tensor into segments of the specified length\n    segments = tf.reshape(padded_tensor, [-1, segment_len, tf.shape(tensor)[1]])\n    return segments\n\n", "CodeBleu_score": 28.01480752797149}, {"code": "def pad_and_partition(tensor: tf.Tensor, segment_len: int) -> tf.Tensor\n\t\n    tensor_len = tensor.shape[0]\n    pad_len = segment_len - (tensor_len % segment_len)\n    pad_tensor = tf.concat([tensor, tf.zeros((pad_len,), dtype=tensor.dtype)], axis=0)\n    return tf.reshape(pad_tensor, (-1, segment_len))\n\n", "CodeBleu_score": 18.854800130326005}, {"code": "def pad_and_partition(tensor: tf.Tensor, segment_len: int) -> tf.Tensor\n\t\n    pad_len = segment_len - tf.math.floormod(tf.shape(tensor)[0], segment_len)\n    padded_tensor = tf.pad(tensor, [[0, pad_len]])\n    return tf.reshape(padded_tensor, [-1, segment_len, tf.shape(tensor)[1]])\n\n", "CodeBleu_score": 25.394065643360136}]}
{"instance_id": "1789fae7253f", "generation_result": [{"code": "def pad_and_reshape(instr_spec, frame_length, F) -> Any\n\t\n    # Calculate the number of extra rows needed for padding\n    extra_rows = frame_length - (instr_spec.shape[1] % frame_length)\n    # Create an extension of zeros\n    extension = torch.zeros(instr_spec.shape[0], extra_rows, instr_spec.shape[2], device=instr_spec.device)\n    # Concatenate the extension to the original spectrogram along the frequency axis\n    instr_spec = torch.cat((instr_spec, extension), dim=1)\n    # Reshape the extended spectrogram by merging the first two dimensions\n    instr_spec = instr_spec.reshape(-1, instr_spec.shape[2])\n    return instr_spec\n\n", "CodeBleu_score": 16.499330481580895}, {"code": "def pad_and_reshape(instr_spec, frame_length, F) -> Any\n\t\n    # Calculate the number of extra rows needed for padding\n    num_extra_rows = (frame_length - instr_spec.shape[0] % frame_length) % frame_length\n    # Create an extension of zeros\n    extension = torch.zeros(num_extra_rows, instr_spec.shape[1], device=instr_spec.device)\n    # Concatenate the extension to the original spectrogram along the frequency axis\n    padded_instr_spec = torch.cat((instr_spec, extension), dim=0)\n    # Reshape the extended spectrogram by merging the first two dimensions\n    reshaped_instr_spec = F.reshape(padded_instr_spec, (-1, frame_length, instr_spec.shape[1]))\n    return reshaped_instr_spec\n\n", "CodeBleu_score": 18.37037633663388}, {"code": "def pad_and_reshape(instr_spec, frame_length, F) -> Any\n\t\n    # Get the shape of the input spectrogram\n    shape = instr_spec.shape\n    # Calculate the number of extra rows needed for padding\n    num_extra_rows = frame_length - shape[0] % frame_length\n    # Create an extension of zeros with the required shape\n    extension = torch.zeros((num_extra_rows, shape[1]), device=instr_spec.device)\n    # Concatenate the extension to the original spectrogram along the frequency axis\n    instr_spec = torch.cat([instr_spec, extension], dim=0)\n    # Reshape the extended spectrogram by merging the first two dimensions\n    instr_spec = instr_spec.reshape(-1, instr_spec.shape[2])\n    return instr_spec\n\n", "CodeBleu_score": 18.683475534957914}]}
{"instance_id": "8a4f4658d97c", "generation_result": [{"code": "def dataset_from_csv(csv_path: str, **kwargs) -> Any\n\t\n    df = pd.read_csv(csv_path, **kwargs)\n    return tf.data.Dataset.from_tensor_slices(df.to_dict(orient=\"list\"))\n\n", "CodeBleu_score": 30.305917489063972}, {"code": "def dataset_from_csv(csv_path: str, **kwargs) -> Any\n\t\n    import pandas as pd\n    import tensorflow as tf\n\n    # Read the CSV file into a pandas DataFrame\n    df = pd.read_csv(csv_path)\n\n    # Convert the DataFrame columns into a TensorFlow dataset\n    dataset = tf.data.Dataset.from_tensor_slices(\n        {col: df[col].values for col in df.columns}\n    )\n\n    return dataset\n\n", "CodeBleu_score": 30.69507790614886}, {"code": "def dataset_from_csv(csv_path: str, **kwargs) -> Any\n\t\n    df = pd.read_csv(csv_path)\n    return tf.data.Dataset.from_tensor_slices(dict(df))\n\n", "CodeBleu_score": 17.895501042571066}]}
{"instance_id": "9a16e81b182f", "generation_result": [{"code": "def check_tensor_shape(tensor_tf: tf.Tensor, target_shape: Any) -> bool\n\t\n    result: bool = True\n    for dim, length in enumerate(target_shape):\n        if length is not None:\n            result = result and tf.shape(tensor_tf)[dim] == length\n    return result\n\n", "CodeBleu_score": 36.12852055841959}, {"code": "def check_tensor_shape(tensor_tf: tf.Tensor, target_shape: Any) -> bool\n\t\n    result = True\n    for i, dim in enumerate(target_shape):\n        if dim is not None:\n            result = result and (tensor_tf.shape[i] == dim)\n    return result\n\n", "CodeBleu_score": 31.182997668408653}, {"code": "def check_tensor_shape(tensor_tf: tf.Tensor, target_shape: Any) -> bool\n\t\n    result = True\n    for i, dim in enumerate(target_shape):\n        if dim is not None:\n            result = result and (tensor_tf.shape[i] == dim)\n    return result\n\n", "CodeBleu_score": 31.182997668408653}]}
{"instance_id": "49d5d3084b3a", "generation_result": [{"code": "def get_training_dataset(\n    audio_params: Dict, audio_adapter: AudioAdapter, audio_path: str\n) -> Any\n\t\n    builder = DatasetBuilder(\n        audio_params=audio_params,\n        audio_adapter=audio_adapter,\n        audio_path=audio_path,\n        chunk_duration=audio_params[\"chunk_duration\"],\n        seed=audio_params[\"seed\"],\n    )\n    dataset = builder.build(\n        train_csv=audio_params[\"train_csv\"],\n        cache_dir=audio_params[\"cache_dir\"],\n        batch_size=audio_params[\"batch_size\"],\n        num_chunks_per_song=audio_params[\"num_chunks_per_song\"],\n        augment=audio_params[\"augment\"],\n        cache=audio_params[\"cache\"],\n    )\n    return dataset\n\n", "CodeBleu_score": 34.86614203711331}, {"code": "def get_training_dataset(\n    audio_params: Dict, audio_adapter: AudioAdapter, audio_path: str\n) -> Any\n\t\n    dataset_builder = DatasetBuilder(\n        audio_params, audio_adapter, audio_path, chunk_duration=0.5, random_seed=42\n    )\n    dataset = dataset_builder.build(\n        train_csv=audio_path,\n        cache_dir=None,\n        batch_size=16,\n        chunks_per_song=100,\n        data_augmentation=True,\n        cache_data=True,\n    )\n    return dataset\n\n", "CodeBleu_score": 30.00158260267252}, {"code": "def get_training_dataset(\n    audio_params: Dict, audio_adapter: AudioAdapter, audio_path: str\n) -> Any\n\t\n    builder = DatasetBuilder(\n        audio_adapter=audio_adapter,\n        audio_params=audio_params,\n        chunk_duration=audio_params[\"chunk_duration\"],\n        random_seed=audio_params[\"random_seed\"],\n    )\n    dataset = builder.build(\n        train_csv=audio_path,\n        cache_dir=audio_params[\"cache_dir\"],\n        batch_size=audio_params[\"batch_size\"],\n        num_chunks_per_song=audio_params[\"num_chunks_per_song\"],\n        data_augmentation=audio_params[\"data_augmentation\"],\n        cache=audio_params[\"cache\"],\n    )\n    return dataset\n\n", "CodeBleu_score": 35.06371963893202}]}
{"instance_id": "fe8bbbba06e3", "generation_result": [{"code": "def instruments(self) -> Any\n\t\n    if not self._instruments:\n        self._instruments = [\n            InstrumentDatasetBuilder(instrument_id, self._data_root, self._instrument_dir)\n            for instrument_id in self._instrument_ids\n        ]\n\n    for instrument in self._instruments:\n        yield instrument\n\n", "CodeBleu_score": 35.53978659766752}, {"code": "def instruments(self) -> Any\n\t\n    if not self.instrument_builders:\n        for instrument in self.instruments:\n            self.instrument_builders.append(\n                InstrumentDatasetBuilder(\n                    self.instrument_dataset_builder_config,\n                    instrument,\n                    self.data_root,\n                    self.dataset_type,\n                )\n            )\n    for instrument_builder in self.instrument_builders:\n        yield instrument_builder\n\n", "CodeBleu_score": 38.088386848182196}, {"code": "def instruments(self) -> Any\n\t\n    if not self.instrument_builders:\n        for instrument in self.instruments:\n            self.instrument_builders.append(InstrumentDatasetBuilder(instrument))\n    for instrument_builder in self.instrument_builders:\n        yield instrument_builder\n\n", "CodeBleu_score": 36.933085574906485}]}
{"instance_id": "438b57fd3413", "generation_result": [{"code": "def cache(self, dataset: Any, cache: Optional[str], wait: bool) -> Any\n\t\n    if cache is not None:\n        if wait:\n            while not exists(cache):\n                logging.info(\n                    \"Waiting for cache to be generated at %s\",\n                    cache,\n                )\n                time.sleep(1)\n        if not exists(cache):\n            os.makedirs(cache, exist_ok=True)\n        cache_index = cache + SEPARATOR + \"index\"\n        if exists(cache_index):\n            with open(cache_index, \"r\") as f:\n                index = json.load(f)\n        else:\n            index = {}\n        if dataset not in index:\n            index[dataset] = {}\n        dataset_cache = cache + SEPARATOR + dataset\n        if not exists(dataset_cache):\n            os.makedirs(dataset_cache, exist_ok=True)\n        dataset_cache_index = dataset_cache + SEPARATOR + \"index\"\n        if exists(dataset_cache_index):\n            with open(dataset_cache_index, \"r\") as f:\n                dataset_index = json.load(f)\n        else:\n            dataset_index = {}\n        if self.name not in dataset_index:\n            dataset_index[self.name] = {}\n        if self.version not in dataset_index[self.name]:\n            dataset_index[self.name][self.version] = {}\n        if self.split not in dataset_index[self.name][self.version]:\n            dataset_index[self.name][self.version][self.split] = {}\n        dataset_split_cache = dataset_cache + SEPARATOR + self.split\n        if not exists(dataset_split_cache):\n            os.makedirs(dataset_split_cache, exist_ok=True)\n        dataset_split_cache_index = dataset_split_cache + SEPARATOR + \"index\"\n        if exists(dataset_split_cache_index):\n            with open(dataset_split_cache_index, \"r\") as f:\n                dataset_split_index = json.load(f)\n        else:\n            dataset_split_index = {}\n        if self.subset not in dataset_split_index:\n            dataset_split_index[self.subset] = {}\n        if self.subset_hash not in dataset_split_index[self.subset]:\n            dataset_split_index[self.subset][self.subset_hash] = {}\n        if self", "CodeBleu_score": 46.23723293058914}, {"code": "def cache(self, dataset: Any, cache: Optional[str], wait: bool) -> Any\n\t\n    if cache is not None:\n        cache = os.path.expanduser(cache)\n        if not exists(cache):\n            os.makedirs(cache)\n        if wait:\n            while not exists(os.path.join(cache, \"index.json\")):\n                print(\"Waiting for cache to be created...\")\n                time.sleep(10)\n        return dataset.cache(cache)\n    return dataset\n\n", "CodeBleu_score": 51.78108565704073}, {"code": "def cache(self, dataset: Any, cache: Optional[str], wait: bool) -> Any\n\t\n    if cache is not None:\n        if wait:\n            while not exists(cache + SEPARATOR + \"index\"):\n                print(\"Waiting for cache to be created\")\n                time.sleep(1)\n        if not exists(cache):\n            os.makedirs(cache)\n        self.save(dataset, cache)\n    return dataset\n\n", "CodeBleu_score": 50.18455438088464}]}
{"instance_id": "8838a3a09a66", "generation_result": [{"code": "def create_estimator(params: Dict, MWF: bool) -> tf.Tensor\n\t\n    model_provider = tf.contrib.predictor.from_saved_model(params[\"model_dir\"])\n    params.update(model_provider.get_metadata())\n    session_config = tf.ConfigProto()\n    session_config.gpu_options.allow_growth = True\n    session_config.allow_soft_placement = True\n    config = tf.estimator.RunConfig(session_config=session_config)\n    estimator = tf.estimator.Estimator(\n        model_fn=model_provider.model_fn, model_dir=params[\"model_dir\"], config=config\n    )\n    if MWF:\n        estimator = tf.contrib.estimator.add_model_weight_file_to_estimator(\n            estimator, params[\"model_dir\"]\n        )\n    return estimator\n\n", "CodeBleu_score": 41.42296753725737}, {"code": "def create_estimator(params: Dict, MWF: bool) -> tf.Tensor\n\t\n    model_provider = ModelProvider(model_name=params[\"model_name\"])\n    model_dir = model_provider.get_model_dir(MWF=MWF)\n    params.update(model_provider.get_params())\n    session_config = tf.compat.v1.ConfigProto(\n        gpu_options=tf.compat.v1.GPUOptions(allow_growth=True)\n    )\n    run_config = tf.estimator.RunConfig(session_config=session_config)\n    estimator = tf.estimator.Estimator(\n        model_fn=model_provider.get_model_fn(),\n        model_dir=model_dir,\n        params=params,\n        config=run_config,\n    )\n    return estimator\n\n", "CodeBleu_score": 40.506356286961385}, {"code": "def create_estimator(params: Dict, MWF: bool) -> tf.Tensor\n\t\n    model_provider = ModelProvider()\n    model_dir = model_provider.get_model_dir(params[\"model_name\"])\n    params[\"model_dir\"] = model_dir\n    params[\"model_name\"] = None\n    params[\"model_provider\"] = model_provider\n    params[\"MWF\"] = MWF\n\n    session_config = tf.ConfigProto()\n    session_config.gpu_options.allow_growth = True\n    session_config.allow_soft_placement = True\n    run_config = tf.estimator.RunConfig(session_config=session_config)\n\n    return tf.estimator.Estimator(\n        model_fn=model_fn, model_dir=model_dir, params=params, config=run_config\n    )\n\n", "CodeBleu_score": 41.49064091812365}]}
{"instance_id": "779c812bf2cb", "generation_result": [{"code": "def _separate_tensorflow(\n        self, waveform: np.ndarray, audio_descriptor: AudioDescriptor\n    ) -> Dict\n\t\n    if waveform.ndim == 1:\n        waveform = np.stack([waveform, waveform], axis=1)\n\n    prediction = self.predict(waveform, audio_descriptor)\n    prediction.pop(\"audio_id\", None)\n    return prediction\n\n", "CodeBleu_score": 37.93154507655652}, {"code": "def _separate_tensorflow(\n        self, waveform: np.ndarray, audio_descriptor: AudioDescriptor\n    ) -> Dict\n\t\n    if waveform.shape[1] == 1:\n        waveform = np.stack((waveform, waveform), axis=1)\n\n    data_generator = self.data_generator.get_data_generator(\n        waveform, audio_descriptor\n    )\n\n    prediction = next(self.model.predict(data_generator))\n    prediction[\"audio_id\"] = audio_descriptor.audio_id\n\n    return prediction\n\n", "CodeBleu_score": 48.78632843883377}, {"code": "def _separate_tensorflow(\n        self, waveform: np.ndarray, audio_descriptor: AudioDescriptor\n    ) -> Dict\n\t\n    if waveform.shape[1] == 1:\n        waveform = np.concatenate([waveform, waveform], axis=1)\n    prediction_generator = self.model.separate(waveform)\n    prediction = next(prediction_generator)\n    prediction[\"audio_id\"] = audio_descriptor.audio_id\n    return prediction\n\n", "CodeBleu_score": 49.775719703472674}]}
{"instance_id": "cf580cd2ed93", "generation_result": [{"code": "def compute_backbone_shapes(config, image_shape)\n\t\n    if callable(config.BACKBONE):\n        return config.COMPUTE_BACKBONE_SHAPE(image_shape)\n\n    assert config.BACKBONE in [\"resnet50\", \"resnet101\"], \"Backbone is not one of'resnet50' or'resnet101'\"\n\n    return np.array(\n        [[int(math.ceil(image_shape[0] / stride)), int(math.ceil(image_shape[1] / stride))] for stride in config.BACKBONE_STRIDES]\n    )\n\n", "CodeBleu_score": 86.64382930782119}, {"code": "def compute_backbone_shapes(config, image_shape)\n\t\n    if callable(config.BACKBONE):\n        return config.COMPUTE_BACKBONE_SHAPE(image_shape)\n\n    # Currently supports ResNet only\n    assert config.BACKBONE in [\"resnet50\", \"resnet101\"]\n    return np.array(\n        [\n            [\n                int(math.ceil(image_shape[0] / stride)),\n                int(math.ceil(image_shape[1] / stride)),\n            ]\n            for stride in config.BACKBONE_STRIDES\n        ]\n    )\n\n", "CodeBleu_score": 81.85065464141955}, {"code": "def compute_backbone_shapes(config, image_shape)\n\t\n    if callable(config.BACKBONE):\n        return config.COMPUTE_BACKBONE_SHAPE(image_shape)\n\n    assert config.BACKBONE in [\"resnet50\", \"resnet101\"]\n    return np.array(\n        [\n            [\n                int(math.ceil(image_shape[0] / stride)),\n                int(math.ceil(image_shape[1] / stride)),\n            ]\n            for stride in config.BACKBONE_STRIDES\n        ]\n    )\n\n", "CodeBleu_score": 81.85065464141955}]}
{"instance_id": "ebd61a2e59ca", "generation_result": [{"code": "def clip_boxes_graph(boxes, window)\n\t\n    with tf.name_scope(\"clip_boxes\"):\n        # Split into individual components of box coordinates and window coordinates\n        wy1, wx1, wy2, wx2 = tf.split(window, 4)\n        y1, x1, y2, x2 = tf.split(boxes, 4, axis=1)\n        # Clip\n        y1 = tf.maximum(tf.minimum(y1, wy2), wy1)\n        x1 = tf.maximum(tf.minimum(x1, wx2), wx1)\n        y2 = tf.maximum(tf.minimum(y2, wy2), wy1)\n        x2 = tf.maximum(tf.minimum(x2, wx2), wx1)\n        clipped = tf.concat([y1, x1, y2, x2], axis=1, name=\"clipped_boxes\")\n        clipped.set_shape((clipped.shape[0], 4))\n        return clipped\n\n", "CodeBleu_score": 80.55226704381094}, {"code": "def clip_boxes_graph(boxes, window)\n\t\n    wy1, wx1, wy2, wx2 = tf.split(window, 4)\n    y1, x1, y2, x2 = tf.split(boxes, 4, axis=1)\n    y1 = tf.maximum(tf.minimum(y1, wy2), wy1)\n    x1 = tf.maximum(tf.minimum(x1, wx2), wx1)\n    y2 = tf.maximum(tf.minimum(y2, wy2), wy1)\n    x2 = tf.maximum(tf.minimum(x2, wx2), wx1)\n    clipped = tf.concat([y1, x1, y2, x2], axis=1, name=\"clipped_boxes\")\n    clipped.set_shape((clipped.shape[0], 4))\n    return clipped\n\n", "CodeBleu_score": 61.634432626045545}, {"code": "def clip_boxes_graph(boxes, window)\n\t\n    with tf.name_scope(\"clip_boxes\"):\n        # Split into individual coordinates\n        x1, y1, x2, y2 = tf.split(value=boxes, num_or_size_splits=4, axis=1)\n        # Clip\n        x1 = tf.maximum(tf.minimum(x1, window[2]), window[0])\n        x2 = tf.maximum(tf.minimum(x2, window[3]), window[1])\n        y1 = tf.maximum(tf.minimum(y1, window[2]), window[0])\n        y2 = tf.maximum(tf.minimum(y2, window[3]), window[1])\n        # Rebuild tensor\n        clipped = tf.concat([x1, y1, x2, y2], axis=1, name=\"clipped_boxes\")\n        clipped.set_shape((clipped.shape[0], 4))\n        return clipped\n\n", "CodeBleu_score": 48.58408642599274}]}
{"instance_id": "8e0411587a70", "generation_result": [{"code": "def rpn_graph(feature_map, anchors_per_location, anchor_stride)\n\t\n    # Shared convolutional base of the RPN\n    shared = keras.layers.Conv2D(512, (3, 3), padding='same', activation='relu',\n                                 strides=anchor_stride,\n                                 name='rpn_conv_shared')(feature_map)\n\n    # Anchor Score. [batch, height, width, anchors per location * 2].\n    x = keras.layers.Conv2D(2 * anchors_per_location, (1, 1), padding='valid',\n                            activation='linear', name='rpn_class_raw')(shared)\n\n    # Reshape to [batch, anchors, 2]\n    rpn_class_logits = keras.layers.Lambda(\n        lambda t: tf.reshape(t, [tf.shape(t)[0], -1, 2]))(x)\n\n    # Softmax on last dimension of BG/FG.\n    rpn_probs = keras.layers.Activation(\n        \"softmax\", name=\"rpn_class_xxx\")(rpn_class_logits)\n\n    # Bounding box refinement. [batch, H, W, anchors per location, depth]\n    # where depth is [x, y, log(w), log(h)]\n    x = keras.layers.Conv2D(anchors_per_location * 4, (1, 1), padding=\"valid\",\n                            activation='linear', name='rpn_bbox_pred')(shared)\n\n    # Reshape to [batch, anchors, 4]\n    rpn_bbox = keras.layers.Lambda(lambda t: tf.reshape(t, [tf.shape(t)[0], -1, 4]))(x)\n\n    return [rpn_class_logits, rpn_probs, rpn_bbox]\n\n", "CodeBleu_score": 71.38288462046268}, {"code": "def rpn_graph(feature_map, anchors_per_location, anchor_stride)\n\t\n    # Get the number of anchors per location\n    anchors_per_location = 9\n    # Get the anchor stride\n    anchor_stride = 1\n    # Define a lambda function to generate the anchor boxes\n    def generate_anchors(scales, ratios, shape, feature_stride, anchor_stride):\n        \"\"\"The function generates anchor boxes for object detection.", "CodeBleu_score": 2.406350260094308}, {"code": "def rpn_graph(feature_map, anchors_per_location, anchor_stride)\n\t\n    shared = keras.layers.Conv2D(512, (3, 3), padding=\"same\", activation=\"relu\", strides=anchor_stride, name=\"rpn_conv_shared\")(feature_map)\n    x = keras.layers.Conv2D(2 * anchors_per_location, (1, 1), padding=\"valid\", activation=\"linear\", name=\"rpn_class_raw\")(shared)\n    rpn_class_logits = keras.layers.Lambda(lambda t: tf.reshape(t, [tf.shape(t)[0], -1, 2]))(x)\n    rpn_probs = keras.layers.Activation(\"softmax\", name=\"rpn_class_xxx\")(rpn_class_logits)\n    x = keras.layers.Conv2D(anchors_per_location * 4, (1, 1), padding=\"valid\", activation=\"linear\", name=\"rpn_bbox_pred\")(shared)\n    rpn_bbox = keras.layers.Lambda(lambda t: tf.reshape(t, [tf.shape(t)[0], -1, 4]))(x)\n    return [rpn_class_logits, rpn_probs, rpn_bbox]\n\n", "CodeBleu_score": 59.5706818819254}]}
{"instance_id": "70777c9261d8", "generation_result": [{"code": "def smooth_l1_loss(y_true, y_pred)\n\t\n    x = K.abs(y_true - y_pred)\n    x = tf.where(x < 1.0, 0.5 * x ** 2, x - 0.5)\n    return K.sum(x)\n", "CodeBleu_score": 17.144652017868616}, {"code": "def smooth_l1_loss(y_true, y_pred)\n\t\n    x = K.abs(y_true - y_pred)\n    x = K.switch(x < 1, x ** 2, x)\n    return K.mean(x)\n\n", "CodeBleu_score": 12.391013380975291}, {"code": "def smooth_l1_loss(y_true, y_pred)\n\t\n    x = K.abs(y_true - y_pred)\n    x = K.switch(x < 1, 0.5 * x ** 2, x - 0.5)\n    return K.sum(x)\n\n", "CodeBleu_score": 16.984496638760362}]}
{"instance_id": "0e747b604087", "generation_result": [{"code": "def build_model(x_train, num_classes)\n\t\n    model = Sequential()\n    model.add(\n        Conv2D(32, kernel_size=(3, 3), activation=\"relu\", input_shape=x_train.shape[1:])\n    )\n    model.add(Conv2D(64, (3, 3), activation=\"relu\"))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Flatten())\n    model.add(Dense(128, activation=\"relu\"))\n    model.add(Dense(num_classes, activation=\"softmax\"))\n    model.compile(\n        loss=keras.losses.categorical_crossentropy,\n        optimizer=keras.optimizers.Adadelta(),\n        metrics=[\"accuracy\"],\n    )\n    return model\n\n", "CodeBleu_score": 28.409701198737793}, {"code": "def build_model(x_train, num_classes)\n\t\n    K.clear_session()\n    model = Sequential()\n    model.add(\n        Conv2D(\n            32,\n            kernel_size=(3, 3),\n            activation=\"relu\",\n            input_shape=(x_train.shape[1], x_train.shape[2], x_train.shape[3]),\n        )\n    )\n    model.add(Conv2D(64, (3, 3), activation=\"relu\"))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Flatten())\n    model.add(Dense(128, activation=\"relu\"))\n    model.add(Dense(num_classes, activation=\"softmax\"))\n    model.compile(\n        loss=keras.losses.categorical_crossentropy,\n        optimizer=keras.optimizers.Adam(),\n        metrics=[\"accuracy\"],\n    )\n    return model\n\n", "CodeBleu_score": 28.1284420095811}, {"code": "def build_model(x_train, num_classes)\n\t\n    model = Sequential()\n    model.add(\n        Conv2D(\n            32,\n            kernel_size=(3, 3),\n            activation=\"relu\",\n            input_shape=(x_train.shape[1], x_train.shape[2], x_train.shape[3]),\n        )\n    )\n    model.add(Conv2D(64, (3, 3), activation=\"relu\"))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Flatten())\n    model.add(Dense(128, activation=\"relu\"))\n    model.add(Dense(num_classes, activation=\"softmax\"))\n    model.compile(\n        loss=keras.losses.categorical_crossentropy,\n        optimizer=keras.optimizers.Adam(),\n        metrics=[\"accuracy\"],\n    )\n    return model\n\n", "CodeBleu_score": 28.480764742426018}]}
{"instance_id": "813e3e2844f4", "generation_result": [{"code": "def extract_bboxes(mask)\n\t\n    boxes = np.zeros([mask.shape[-1], 4], dtype=np.int32)\n    for i in range(mask.shape[-1]):\n        m = mask[:, :, i]\n        # Bounding box.\n        horizontal_indicies = np.where(np.any(m, axis=0))[0]\n        vertical_indicies = np.where(np.any(m, axis=1))[0]\n        if horizontal_indicies.shape[0]:\n            x1, x2 = horizontal_indicies[[0, -1]]\n            y1, y2 = vertical_indicies[[0, -1]]\n            # x2 and y2 should not be part of the box. Increment by 1.\n            x2 += 1\n            y2 += 1\n        else:\n            # No mask for this instance. Might happen due to\n            # resizing.\n            x1, x2, y1, y2 = 0, 0, 0, 0\n        boxes[i] = np.array([y1, x1, y2, x2])\n    return boxes.astype(np.int32)\n\n", "CodeBleu_score": 79.8090520516883}, {"code": "def extract_bboxes(mask)\n\t\n    boxes = np.zeros([mask.shape[-1], 4], dtype=np.int32)\n    for i in range(mask.shape[-1]):\n        m = mask[:, :, i]\n        # Bounding box.\n        horizontal_indicies = np.where(np.any(m, axis=0))[0]\n        vertical_indicies = np.where(np.any(m, axis=1))[0]\n        if horizontal_indicies.shape[0]:\n            x1, x2 = horizontal_indicies[[0, -1]]\n            y1, y2 = vertical_indicies[[0, -1]]\n            # x2 and y2 should not be part of the box. Increment by 1.\n            x2 += 1\n            y2 += 1\n        else:\n            # No mask for this instance. Might happen due to\n            # resizing.\n            x1, x2, y1, y2 = 0, 0, 0, 0\n        boxes[i] = np.array([y1, x1, y2, x2])\n    return boxes.astype(np.int32)\n\n", "CodeBleu_score": 79.8090520516883}, {"code": "def extract_bboxes(mask)\n\t\n    boxes = np.zeros([mask.shape[-1], 4], dtype=np.int32)\n    for i in range(mask.shape[-1]):\n        m = mask[:, :, i]\n        # Bounding box.\n        horizontal_indicies = np.where(np.any(m, axis=0))[0]\n        vertical_indicies = np.where(np.any(m, axis=1))[0]\n        if horizontal_indicies.shape[0]:\n            x1, x2 = horizontal_indicies[[0, -1]]\n            y1, y2 = vertical_indicies[[0, -1]]\n            # x2 and y2 should not be part of the box. Increment by 1.\n            x2 += 1\n            y2 += 1\n        else:\n            # No mask for this instance. Might happen due to\n            # resizing.\n            x1, x2, y1, y2 = 0, 0, 0, 0\n        boxes[i] = np.array([y1, x1, y2, x2])\n    return boxes.astype(np.int32)\n\n", "CodeBleu_score": 79.8090520516883}]}
{"instance_id": "7ad5d430d6ed", "generation_result": [{"code": "def compute_iou(box, boxes, box_area, boxes_area)\n\t\n    # Calculate intersection areas\n    y1 = np.maximum(box[0], boxes[:, 0])\n    y2 = np.minimum(box[2], boxes[:, 2])\n    x1 = np.maximum(box[1], boxes[:, 1])\n    x2 = np.minimum(box[3], boxes[:, 3])\n    intersection = np.maximum(x2 - x1, 0) * np.maximum(y2 - y1, 0)\n    union = box_area + boxes_area[:] - intersection[:]\n    iou = intersection / union\n    return iou\n", "CodeBleu_score": 58.663259083089194}, {"code": "def compute_iou(box, boxes, box_area, boxes_area)\n\t\n    y1 = np.maximum(box[0], boxes[:, 0])\n    y2 = np.minimum(box[2], boxes[:, 2])\n    x1 = np.maximum(box[1], boxes[:, 1])\n    x2 = np.minimum(box[3], boxes[:, 3])\n    intersection = np.maximum(x2 - x1, 0) * np.maximum(y2 - y1, 0)\n    union = box_area + boxes_area[:] - intersection[:]\n    iou = intersection / union\n    return iou\n\n", "CodeBleu_score": 58.663259083089194}, {"code": "def compute_iou(box, boxes, box_area, boxes_area)\n\t\n    # Calculate intersection areas\n    y1 = np.maximum(box[0], boxes[:, 0])\n    y2 = np.minimum(box[2], boxes[:, 2])\n    x1 = np.maximum(box[1], boxes[:, 1])\n    x2 = np.minimum(box[3], boxes[:, 3])\n    intersection = np.maximum(x2 - x1, 0) * np.maximum(y2 - y1, 0)\n    union = box_area + boxes_area[:] - intersection[:]\n    iou = intersection / union\n    return iou\n\n", "CodeBleu_score": 58.663259083089194}]}
{"instance_id": "15941b852859", "generation_result": [{"code": "def compute_overlaps(boxes1, boxes2)\n\t\n    # Areas of anchors and GT boxes\n    area1 = (boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1])\n    area2 = (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1])\n\n    # Compute overlaps to generate matrix [boxes1 count, boxes2 count]\n    # Each cell contains the IoU value.\n    overlaps = np.zeros((boxes1.shape[0], boxes2.shape[0]))\n    for i in range(overlaps.shape[1]):\n        box2 = boxes2[i]\n        overlaps[:, i] = compute_iou(box2, boxes1, area2[i], area1)\n    return overlaps\n\n", "CodeBleu_score": 62.2853282032944}, {"code": "def compute_overlaps(boxes1, boxes2)\n\t\n    # Areas of anchors and GT boxes\n    area1 = (boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1])\n    area2 = (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1])\n\n    # Compute overlaps to generate matrix [boxes1 count, boxes2 count]\n    # Each cell contains the IoU value\n    overlaps = np.zeros((boxes1.shape[0], boxes2.shape[0]))\n    for i in range(overlaps.shape[1]):\n        box2 = boxes2[i]\n        overlaps[:, i] = compute_iou(box2, boxes1, area2[i], area1)\n    return overlaps\n\n", "CodeBleu_score": 62.2853282032944}, {"code": "def compute_overlaps(boxes1, boxes2)\n\t\n    # Areas of anchors and GT boxes\n    area1 = (boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1])\n    area2 = (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1])\n\n    # Compute overlaps to generate matrix [boxes1 count, boxes2 count]\n    # Each cell contains the IoU value.\n    overlaps = np.zeros((boxes1.shape[0], boxes2.shape[0]))\n    for i in range(overlaps.shape[1]):\n        box2 = boxes2[i]\n        overlaps[:, i] = compute_iou(box2, boxes1, area2[i], area1)\n    return overlaps\n\n", "CodeBleu_score": 62.2853282032944}]}
{"instance_id": "597533722ce3", "generation_result": [{"code": "def load_image(self, image_id):\n\t\n    # Load image information\n    image_info = self.image_info[image_id]\n    # Load image\n    image = skimage.io.imread(image_info['path'])\n    # If grayscale. Convert to RGB for consistency.\n    if image.ndim!= 3:\n        image = skimage.color.gray2rgb(image)\n    # If has an alpha channel, remove it for consistency\n    if image.shape[-1] == 4:\n        image = image[..., :3]\n    return image\n", "CodeBleu_score": 78.79495246603703}, {"code": "def load_image(self, image_id):\n\t\n    image_info = self.image_info[image_id]\n    path = image_info['path']\n    image = skimage.io.imread(path)\n    if image.shape[-1] == 4:\n        image = image[..., :3]\n    return image\n\n", "CodeBleu_score": 64.3968885716256}, {"code": "def load_image(self, image_id):\n\t\n    info = self.image_info[image_id]\n    path = info['path']\n    image = skimage.io.imread(path)\n    if len(image.shape) == 3:\n        assert image.shape[2] == 3\n    elif len(image.shape) == 2:\n        image = skimage.color.gray2rgb(image)\n    else:\n        assert len(image.shape) == 4\n        image = image[..., :3]\n    return image\n", "CodeBleu_score": 56.19744442706407}]}
{"instance_id": "9b852997ca05", "generation_result": [{"code": "def display_images(images, titles=None, cols=4, cmap=None, norm=None,\n                   interpolation=None)\n\t\n    titles = titles if titles is not None else [\"\"] * len(images)\n    rows = len(images) // cols + 1\n    plt.figure(figsize=(14, 14 * rows // cols))\n    i = 1\n    for image, title in zip(images, titles):\n        plt.subplot(rows, cols, i)\n        plt.title(title, fontsize=9)\n        plt.axis('off')\n        plt.imshow(image.astype(np.uint8), cmap=cmap,\n                   norm=norm, interpolation=interpolation)\n        i += 1\n    plt.show()\n\n", "CodeBleu_score": 76.8475453040379}, {"code": "def display_images(images, titles=None, cols=4, cmap=None, norm=None,\n                   interpolation=None)\n\t\n    titles = titles if titles is not None else [\"\"] * len(images)\n    rows = len(images) // cols + 1\n    plt.figure(figsize=(14, 14 * rows // cols))\n    i = 1\n    for image, title in zip(images, titles):\n        plt.subplot(rows, cols, i)\n        plt.title(title, fontsize=9)\n        plt.axis('off')\n        plt.imshow(image.astype(np.uint8), cmap=cmap,\n                   norm=norm, interpolation=interpolation)\n        i += 1\n    plt.show()\n\n", "CodeBleu_score": 76.8475453040379}, {"code": "def display_images(images, titles=None, cols=4, cmap=None, norm=None,\n                   interpolation=None)\n\t\n    titles = titles if titles is not None else [\"\"] * len(images)\n    rows = len(images) // cols + 1\n    plt.figure(figsize=(14, 14 * rows // cols))\n    i = 1\n    for image, title in zip(images, titles):\n        plt.subplot(rows, cols, i)\n        plt.title(title, fontsize=9)\n        plt.axis('off')\n        plt.imshow(image.astype(np.uint8), cmap=cmap,\n                   norm=norm, interpolation=interpolation)\n        i += 1\n    plt.show()\n\n", "CodeBleu_score": 76.8475453040379}]}
