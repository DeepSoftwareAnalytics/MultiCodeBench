{"instance_id": "1f9f4d85e825", "generation_result": [{"code": "def load_model_from_config(self, half_attention)\n\t\n\n    # Load model from cache if available and valid\n    if self.cache_path is not None and os.path.exists(self.cache_path):\n        try:\n            print(f\"Loading model from cache: {self.cache_path}\")\n            model = torch.load(self.cache_path, map_location=\"cpu\")\n            print(f\"Model loaded from cache: {self.cache_path}\")\n        except Exception as e:\n            print(f\"Error loading model from cache: {e}\")\n            model = None\n    else:\n        model = None\n\n    # Load model from path if not cached\n    if model is None:\n        print(f\"Loading model from path: {self.model_path}\")\n        if self.model_path.endswith(\".safetensors\"):\n            model = safetensors.torch.load(self.model_path, map_location=\"cpu\")\n        else:\n            model = torch.load(self.model_path, map_location=\"cpu\")\n        print(f\"Model loaded from path: {self.model_path}\")\n\n    # Move model to device\n    model = model.to(self.device)\n\n    # Ensure model is in evaluation mode\n    model.eval()\n\n    # Half attention if specified\n    if half_attention:\n        model.half()\n\n    # Optimize memory format if specified\n    if self.optimize_memory_format:\n        model = model.to(memory_format=torch.channels_last)\n\n    # Cache model if specified\n    if self.cache_path is not None:\n        torch.save(model, self.cache_path)\n\n    return model\n\n", "CodeBleu_score": 40.61136658515604}, {"code": "def load_model_from_config(self, half_attention)\n\t\n    # Load model from cache if available and valid\n    model_path = self.model_path\n    cache_path = self.cache_path\n    model_name = self.model_name\n    model_config = self.model_config\n    model_config_path = self.model_config_path\n    model_config_name = self.model_config_name\n    model_config_args = self.model_config_args\n    model_config_kwargs = self.model_config_kwargs\n    model_device = self.model_device\n    model_half_precision = self.model_half_precision\n    model_memory_format = self.model_memory_format\n    model_cache = self.model_cache\n    model_cache_validity = self.model_cache_validity\n    model_cache_max_size = self.model_cache_max_size\n    model_cache_max_age = self.model_cache_max_age\n    model_cache_max_count = self.model_cache_max_count\n    model_cache_max_count_per_model = self.model_cache_max_count_per_model\n    model_cache_max_count_per_device = self.model_cache_max_count_per_device\n    model_cache_max_count_per_model_per_device = self.model_cache_max_count_per_model_per_device\n    model_cache_max_count_per_model_per_device_per_process = self.model_cache_max_count_per_model_per_device_per_process\n    model_cache_max_count_per_model_per_device_per_process_per_node = self.model_cache_max_count_per_model_per_device_per_process_per_node\n    model_cache_max_count_per_model_per_device_per_process_per_node_per_process = self.model_cache_max_count_per_model_per_device_per_process_per_node_per_process\n    model_cache_max_count_per_model_per_device_per_process_per_node_per", "CodeBleu_score": 8.086130595148594}, {"code": "def load_model_from_config(self, half_attention)\n\t\n\n    # Load the model from the configuration file\n    model_config = OmegaConf.load(self.model_config_path)\n    model_config = model_config.model\n\n    # Load the model from the cache if available\n    cache_path = self.model_cache_path\n    if cache_path is not None and os.path.exists(cache_path):\n        if self.verbose:\n            print(f\"Loading model from cache at {cache_path}\")\n        model_config = OmegaConf.load(cache_path)\n        model_config = model_config.model\n\n    # Load the model from the model path\n    else:\n        if self.verbose:\n            print(f\"Loading model from {self.model_path}\")\n        model_config.model_path = self.model_path\n        model_config = instantiate_from_config(model_config)\n\n    # Move the model to the appropriate device\n    model = model_config.model\n    model = model.to(self.device)\n\n    # Set the model to evaluation mode\n    model.eval()\n\n    # Half-precision attention\n    if half_attention:\n        if self.verbose:\n            print(f\"Half-precision attention enabled\")\n        model.half()\n\n    # Memory format optimization\n    if self.memory_format == \"channels_last\":\n        if self.verbose:\n            print(f\"Channels-last memory format enabled\")\n        model.contiguous_gradients = False\n\n    # Cache the model if caching is enabled\n    if cache_path is not None:\n        if self.verbose:\n            print(f\"Caching model at {cache_path}\")\n        OmegaConf.save(model_config, cache_path)\n\n    return model\n\n", "CodeBleu_score": 36.0672543741131}]}
{"instance_id": "8dc955007aa2", "generation_result": [{"code": "def convsample_ddim(model, cond, steps, shape, eta=1.0, callback=None, normals_sequence=None,\n                    mask=None, x0=None, quantize_x0=False, temperature=1., score_corrector=None,\n                    corrector_kwargs=None, x_t=None\n                    )\n\t\n    batch_size = shape[0]\n    print(f\"eta: {eta}, steps: {steps}\")\n    sampler = DDIMSampler(model, shape, cond=cond, steps=steps, eta=eta, callback=callback,\n                          normals_sequence=normals_sequence, mask=mask, x0=x0, quantize_x0=quantize_x0,\n                          temperature=temperature, score_corrector=score_corrector,\n                          corrector_kwargs=corrector_kwargs, x_t=x_t)\n    samples = sampler.sample()\n    return samples\n\n", "CodeBleu_score": 41.577599346730885}, {"code": "def convsample_ddim(model, cond, steps, shape, eta=1.0, callback=None, normals_sequence=None,\n                    mask=None, x0=None, quantize_x0=False, temperature=1., score_corrector=None,\n                    corrector_kwargs=None, x_t=None\n                    )\n\t\n\n    batch_size = shape[0]\n    print(f'Sampling {batch_size} samples with eta={eta} and {steps} steps.')\n\n    if x0 is None:\n        x0 = torch.zeros(batch_size, 3, 64, 64, device=devices.get_device())\n\n    sampler = DDIMSampler(model, steps, x0=x0, eta=eta, quantize_x0=quantize_x0, temperature=temperature,\n                          score_corrector=score_corrector, corrector_kwargs=corrector_kwargs, x_t=x_t)\n    samples, logits = sampler.sample(batch_size, shape, cond, callback=callback, normals_sequence=normals_sequence,\n                                     mask=mask)\n    return samples, logits\n\n", "CodeBleu_score": 47.613824221117824}, {"code": "def convsample_ddim(model, cond, steps, shape, eta=1.0, callback=None, normals_sequence=None,\n                    mask=None, x0=None, quantize_x0=False, temperature=1., score_corrector=None,\n                    corrector_kwargs=None, x_t=None\n                    )\n\t\n\n    batch_size = shape[0]\n    print(f'eta: {eta}')\n    print(f'steps: {steps}')\n    print(f'batch size: {batch_size}')\n\n    sampler = DDIMSampler(model, batch_size=batch_size, steps=steps, shape=shape,\n                          cond=cond, eta=eta, callback=callback, normals_sequence=normals_sequence,\n                          mask=mask, x0=x0, quantize_x0=quantize_x0, temperature=temperature,\n                          score_corrector=score_corrector, corrector_kwargs=corrector_kwargs,\n                          x_t=x_t\n                          )\n\n    samples, scores = sampler.sample()\n    return samples, scores\n\n", "CodeBleu_score": 48.51469939745942}]}
{"instance_id": "9bdcd9081480", "generation_result": [{"code": "def ema_scope(self, context=None)\n\t\n\n    if context is None:\n        context = {}\n\n    if self.ema is None:\n        yield\n        return\n\n    if self.ema.enabled:\n        self.ema.swap()\n        context[\"ema_swap\"] = True\n        if self.ema.verbose:\n            print(f\"Switched to EMA weights\")\n\n    try:\n        yield\n    finally:\n        if context.get(\"ema_swap\", False):\n            self.ema.swap()\n            if self.ema.verbose:\n                print(f\"Switched to original weights\")\n\n", "CodeBleu_score": 41.1231089267604}, {"code": "def ema_scope(self, context=None)\n\t\n    if context is None:\n        context = self\n    if self.ema is not None:\n        if self.ema.enabled:\n            self.ema.swap()\n            if context is not self:\n                print(f\"{context.__class__.__name__} EMA weights enabled\")\n        else:\n            if context is not self:\n                print(f\"{context.__class__.__name__} EMA weights disabled\")\n    yield\n    if self.ema is not None:\n        if self.ema.enabled:\n            self.ema.swap()\n            if context is not self:\n                print(f\"{context.__class__.__name__} EMA weights restored\")\n\n", "CodeBleu_score": 43.70206394660997}, {"code": "def ema_scope(self, context=None)\n\t\n    if context is None:\n        context = self\n    if self.ema is None:\n        return context\n    if self.ema.enabled:\n        self.ema.swap()\n        if context is not self:\n            print(f\"{context.__class__.__name__} weights switched to EMA\")\n    try:\n        yield context\n    finally:\n        if self.ema is not None and self.ema.enabled:\n            self.ema.swap()\n            if context is not self:\n                print(f\"{context.__class__.__name__} weights switched back to original\")\n\n", "CodeBleu_score": 45.90524162838408}]}
{"instance_id": "14ab009e1c5c", "generation_result": [{"code": "def init_from_ckpt(self, path, ignore_keys=None)\n\t\n    sd = torch.load(path, map_location=lambda storage, loc: storage)\n    if ignore_keys is not None:\n        for k in ignore_keys:\n            sd.pop(k, None)\n    missing_keys, unexpected_keys = self.load_state_dict(sd, strict=False)\n    if missing_keys or unexpected_keys:\n        print(f\"missing_keys: {missing_keys}\")\n        print(f\"unexpected_keys: {unexpected_keys}\")\n\n", "CodeBleu_score": 28.597642117897603}, {"code": "def init_from_ckpt(self, path, ignore_keys=None)\n\t\n    if ignore_keys is None:\n        ignore_keys = []\n    sd = torch.load(path, map_location=\"cpu\")[\"state_dict\"]\n    for k in ignore_keys:\n        del sd[k]\n    missing_keys = []\n    unexpected_keys = []\n    error_msgs = []\n    # copy state_dict so _load_from_state_dict can modify it\n    metadata = getattr(self.state_dict(), \"_metadata\", None)\n    state_dict = self.state_dict()\n    for k, v in sd.items():\n        if k in state_dict:\n            if state_dict[k].shape != v.shape:\n                error_msgs.append(\n                    \"size mismatch for {}: copying a param with shape {} from checkpoint, the shape in current model is {}. \"\n                    .format(k, v.shape, state_dict[k].shape)\n                )\n                continue\n            state_dict[k] = v\n        else:\n            missing_keys.append(k)\n    for k in state_dict.keys():\n        if k not in sd:\n            unexpected_keys.append(k)\n    # pytorch newer than 1.6.0\n    if version.parse(torch.__version__) >= version.parse(\"1.6.0\"):\n        state_dict = _convert_state_dict_type(state_dict, torch.float32)\n    if len(missing_keys) > 0:\n        print(\"Weights of {} not initialized from pretrained model: {}\".format(\n            self.__class__.__name__, missing_keys))\n    if len(unexpected_keys) > 0:\n        print(\"Weights from pretrained model not used in {}: {}\".format(\n            self.__class__.__name__, unexpected_keys))\n    if len(error_msgs) > 0:\n        print(\"", "CodeBleu_score": 31.628496524811535}, {"code": "def init_from_ckpt(self, path, ignore_keys=None)\n\t\n    sd = torch.load(path, map_location=\"cpu\")[\"state_dict\"]\n    if ignore_keys is not None:\n        for key in ignore_keys:\n            sd.pop(key, None)\n    missing_keys, unexpected_keys = self.load_state_dict(sd, strict=False)\n    if missing_keys:\n        print(f\"Missing keys in state_dict: {missing_keys}\")\n    if unexpected_keys:\n        print(f\"Unexpected keys in state_dict: {unexpected_keys}\")\n\n", "CodeBleu_score": 33.586252862443416}]}
{"instance_id": "754176e27b81", "generation_result": [{"code": "def forward(self, input, return_pred_indices=False)\n\t\n    # encode\n    enc = self.encode(input)\n    # decode\n    dec = self.decode(enc)\n    # compute difference\n    diff = torch.mean((input - dec) ** 2)\n    if return_pred_indices:\n        return dec, diff, enc\n    else:\n        return dec, diff\n\n", "CodeBleu_score": 37.95819629372853}, {"code": "def forward(self, input, return_pred_indices=False)\n\t\n    if self.use_ema:\n        self.model.eval()\n        with torch.no_grad():\n            ema_model = self.ema_model.ema\n    else:\n        ema_model = self.model\n    z = ema_model.encode(input)\n    if self.use_ema:\n        self.model.train()\n    reconstruction = ema_model.decode(z)\n    if return_pred_indices:\n        return reconstruction, ema_model.get_indices(z)\n    else:\n        return reconstruction\n\n", "CodeBleu_score": 31.886321047668403}, {"code": "def forward(self, input, return_pred_indices=False)\n\t\n    if self.training:\n        # encode\n        z = self.encode(input)\n        # decode\n        output = self.decode(z)\n        # compute difference\n        diff = self.compute_diff(input, output)\n        # return\n        if return_pred_indices:\n            return output, diff, z\n        return output, diff\n\n", "CodeBleu_score": 45.03767268001922}]}
{"instance_id": "443df6fdaf0d", "generation_result": [{"code": "def get_input(self, batch, k)\n\t\n    x = batch[k]\n    if self.resize_batches:\n        x = x.resize_(self.resize_batches[0], self.resize_batches[1], self.resize_batches[2], self.resize_batches[3])\n    x = x.permute(0, 3, 1, 2).contiguous().float()\n    x = x.detach()\n    return x\n\n", "CodeBleu_score": 11.008682342692367}, {"code": "def get_input(self, batch, k)\n\t\n    x = batch[k]\n    if len(x.shape) == 3:\n        x = x.unsqueeze(1)\n    x = x.permute(0, 3, 1, 2)\n    x = x.contiguous().float()\n    if self.resize_batches:\n        x = x.resize_(x.size()[0], self.batch_size, x.size()[2], x.size()[3])\n    return x.detach()\n\n", "CodeBleu_score": 28.306435072200852}, {"code": "def get_input(self, batch, k)\n\t\n    # get the input\n    x = batch[k]\n    # adjust dimensions if necessary\n    if len(x.shape) < 4:\n        x = x.unsqueeze(0)\n    x = x.permute(0, 3, 1, 2)\n    # convert to float and detach from the computation graph\n    x = x.to(self.device).float().detach()\n    # resize if necessary\n    if self.resize_batch:\n        x = self.resize_batch_fn(x)\n    return x\n\n", "CodeBleu_score": 22.899008332121834}]}
{"instance_id": "953a2632fc15", "generation_result": [{"code": "def training_step(self, batch, batch_idx, optimizer_idx)\n\t\n    # Retrieve data and predictions\n    x, y = batch\n    x_hat = self(x)\n    if optimizer_idx == 0:\n        loss = self.loss_ae(x_hat, x)\n    elif optimizer_idx == 1:\n        loss = self.loss_disc(x_hat, y)\n    else:\n        raise ValueError(f\"Invalid optimizer index {optimizer_idx}\")\n    # Log metrics\n    self.log_dict({f\"train_loss_{optimizer_idx}\": loss}, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n    return loss\n\n", "CodeBleu_score": 18.443294399207094}, {"code": "def training_step(self, batch, batch_idx, optimizer_idx)\n\t\n\n    if self.training:\n        self.model.train()\n    else:\n        self.model.eval()\n\n    if self.current_epoch < self.hparams.warmup_epochs:\n        lr_mul = min(self.hparams.warmup_epochs, self.current_epoch + 1) / self.hparams.warmup_epochs\n        for pg in optimizer.param_groups:\n            pg['lr'] = lr_mul * self.hparams.lr\n\n    x = batch[0]\n    y = batch[1]\n\n    if self.current_epoch >= self.hparams.warmup_epochs:\n        optimizer.zero_grad()\n\n    x_hat, y_hat = self.model(x, y)\n\n    if optimizer_idx == 0:\n        loss = self.autoencoder_loss(x_hat, x)\n    else:\n        loss = self.discriminator_loss(y_hat, y)\n\n    if self.current_epoch >= self.hparams.warmup_epochs:\n        loss.backward()\n        optimizer.step()\n\n    self.log('loss', loss)\n\n    return loss\n\n", "CodeBleu_score": 31.134628508611893}, {"code": "def training_step(self, batch, batch_idx, optimizer_idx)\n\t\n    x, y = batch\n    x = x.to(self.device)\n    y = y.to(self.device)\n    if optimizer_idx == 0:\n        # autoencoder\n        x_hat = self.model(x)\n        loss = self.criterion(x_hat, x)\n        self.log_dict({'loss_ae': loss})\n    else:\n        # discriminator\n        loss = self.criterion(self.model(x), y)\n        self.log_dict({'loss_disc': loss})\n    return loss\n\n", "CodeBleu_score": 16.231889062465683}]}
{"instance_id": "b449b5e8b77f", "generation_result": [{"code": "def log_images(self, batch, only_inputs=False, plot_ema=False, **kwargs)\n\t\n    if not self.training:\n        return\n    if version.parse(torch.__version__) >= version.parse(\"1.6.0\"):\n        self.log_dict({\"train_batch\": batch}, **kwargs)\n    else:\n        self.logger.experiment.log_text(\"train_batch\", str(batch))\n\n    if only_inputs:\n        return\n\n    if self.ema is not None and plot_ema:\n        reconstructions = self.ema(batch)\n    else:\n        reconstructions = self(batch)\n\n    if not self.config.get(\"colorize\", False):\n        self.logger.experiment.log_image(\n            \"train_reconstruction\", reconstructions, **kwargs\n        )\n    else:\n        self.logger.experiment.log_image(\n            \"train_reconstruction\",\n            self.colorize(reconstructions),\n            **kwargs\n        )\n\n    if self.config.get(\"colorize\", False):\n        self.logger.experiment.log_image(\n            \"train_input\", self.colorize(batch), **kwargs\n        )\n    else:\n        self.logger.experiment.log_image(\"train_input\", batch, **kwargs)\n\n", "CodeBleu_score": 26.87222769999079}, {"code": "def log_images(self, batch, only_inputs=False, plot_ema=False, **kwargs)\n\t\n    if self.ema is not None and plot_ema:\n        self.ema.update()\n        with torch.no_grad():\n            if self.ema.model.ema_decoder is not None:\n                self.ema.model.ema_decoder.eval()\n            if self.ema.model.ema_encoder is not None:\n                self.ema.model.ema_encoder.eval()\n        self.ema.model.eval()\n        self.ema.model.ema_decoder.eval()\n        self.ema.model.ema_encoder.eval()\n        if self.ema.model.ema_decoder is not None:\n            self.ema.model.ema_decoder.eval()\n        if self.ema.model.ema_encoder is not None:\n            self.ema.model.ema_encoder.eval()\n        self.ema.model.eval()\n        if self.ema.model.ema_decoder is not None:\n            self.ema.model.ema_decoder.eval()\n        if self.ema.model.ema_encoder is not None:\n            self.ema.model.ema_encoder.eval()\n        self.ema.model.eval()\n        if self.ema.model.ema_decoder is not None:\n            self.ema.model.ema_decoder.eval()\n        if self.ema.model.ema_encoder is not None:\n            self.ema.model.ema_encoder.eval()\n        self.ema.model.eval()\n\n    if not only_inputs:\n        if self.ema is not None and plot_ema:\n            if self.ema.model.ema_decoder is not None:\n                self.ema.model.ema_decoder.eval()\n            if self.ema.model.ema_encoder is not None:\n                self.ema.model.ema_encoder.eval()\n            self.ema.model.eval()\n            if self.ema.model.ema_decoder is not None:\n                self.ema.model.ema_decoder.eval()\n            if self.ema.model.ema_encoder is not None:\n                self.ema.model", "CodeBleu_score": 15.002943495725244}, {"code": "def log_images(self, batch, only_inputs=False, plot_ema=False, **kwargs)\n\t\n\n    if only_inputs:\n        self.log_images_only_inputs(batch, plot_ema=plot_ema, **kwargs)\n    else:\n        self.log_images_inputs_and_reconstructions(batch, plot_ema=plot_ema, **kwargs)\n\n", "CodeBleu_score": 8.753488001927959}]}
{"instance_id": "540d089e3481", "generation_result": [{"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss_type == 'l1':\n        loss = pred.abs().mean() if mean else pred.abs()\n    elif self.loss_type == 'l2':\n        loss = (pred - target).pow(2).mean() if mean else (pred - target).pow(2)\n    else:\n        raise ValueError(f'Unknown loss type: {self.loss_type}')\n    return loss\n\n", "CodeBleu_score": 42.70185194237459}, {"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss_type == 'l1':\n        loss = torch.abs(pred - target)\n        if mean:\n            loss = torch.mean(loss)\n        return loss\n    elif self.loss_type == 'l2':\n        loss = (pred - target) ** 2\n        if mean:\n            loss = torch.mean(loss)\n        return loss\n    else:\n        raise ValueError(f\"Unknown loss type {self.loss_type}\")\n\n", "CodeBleu_score": 44.6303821865795}, {"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss_type == 'l1':\n        return pred.abs().mean() if mean else pred.abs()\n    elif self.loss_type == 'l2':\n        return (pred - target).pow(2).mean() if mean else (pred - target).pow(2)\n    else:\n        raise NotImplementedError(f'Unknown loss type: {self.loss_type}')\n\n", "CodeBleu_score": 31.144532930178425}]}
{"instance_id": "63a08283666f", "generation_result": [{"code": "def forward(self, x, c, *args, **kwargs)\n\t\n    # Get the time steps to sample from\n    t = self.get_timesteps(x.shape[0])\n    # Apply conditioning\n    c = self.apply_conditioning(c)\n    # Sample from the model\n    x = self.sample(x, c, t, *args, **kwargs)\n    # Compute the loss\n    loss = self.compute_loss(x, c, t, *args, **kwargs)\n    return loss\n\n", "CodeBleu_score": 11.13788044180732}, {"code": "def forward(self, x, c, *args, **kwargs)\n\t\n    # Sample time steps\n    t = self.sample_timesteps(x.shape[0])\n\n    # Apply conditioning\n    if self.conditional:\n        c = self.cond_net(c)\n        c = self.cond_proj(c)\n\n    # Apply model conditioning\n    if self.model_conditional:\n        x = self.model_cond_proj(x)\n\n    # Process inputs\n    x = self.preprocess(x, t)\n\n    # Run model\n    if self.model_type == 'pixel':\n        x = self.model(x, t, c)\n    elif self.model_type == 'class':\n        x = self.model(x, t, c)\n    elif self.model_type == 'both':\n        x = self.model(x, t, c)\n        x = self.model_class(x)\n    else:\n        raise ValueError(f'Unknown model type: {self.model_type}')\n\n    # Process outputs\n    x = self.postprocess(x, t)\n\n    # Compute loss\n    loss = self.compute_loss(x, t, c, *args, **kwargs)\n\n    return loss\n\n", "CodeBleu_score": 32.02811191508415}, {"code": "def forward(self, x, c, *args, **kwargs)\n\t\n    # extract time steps\n    t = x[:, :1]\n    x = x[:, 1:]\n    # extract conditioning\n    c = c.unsqueeze(-1) if c.dim() == 1 else c\n    # randomly sample time steps\n    t = t.floor().clamp(min=0, max=self.num_timesteps - 1)\n    t = t + torch.rand_like(t) * (1 - 1e-6)\n    t = t.long()\n    # apply conditioning\n    if c is not None:\n        x = self.condition(x, c, t)\n    # process inputs\n    x = self.preprocess(x)\n    # compute loss\n    loss = self.compute_loss(x, t)\n    return loss\n\n", "CodeBleu_score": 37.19755130697722}]}
{"instance_id": "4627e1e8ed62", "generation_result": [{"code": "def get_codebook_entry(self, indices, shape)\n\t\n\n    # get the latent vectors\n    latents = self.codebook.weight.index_select(0, indices.long())\n\n    # remap indices if necessary\n    if self.remap_indices:\n        latents = latents.view(self.codebook.num_patches, self.codebook.embed_dim)\n        latents = rearrange(latents, 'n d -> d n')\n\n    # reshape latent vectors to match the shape of the input\n    latents = latents.view(shape)\n\n    return latents\n\n", "CodeBleu_score": 31.267517667833634}, {"code": "def get_codebook_entry(self, indices, shape)\n\t\n    # get the latent vectors from the codebook\n    latents = self.codebook.weight.index_select(0, indices)\n\n    # reorder the indices if necessary\n    if self.remapping:\n        latents = latents.index_select(0, self.remapping_indices)\n\n    # reshape the latent vectors to match the input dimensions\n    latents = rearrange(latents, \"b (h d) -> b h d\", h=shape[1])\n\n    # permute the latent vectors to match the input dimensions\n    latents = latents.permute(0, 2, 1)\n\n    return latents\n\n", "CodeBleu_score": 27.595282743801885}, {"code": "def get_codebook_entry(self, indices, shape)\n\t\n    # if self.remap_indices:\n    #     indices = self.remap_indices(indices)\n\n    # rearrange the indices to match the shape of the codebook\n    # if the shape is None, we assume the codebook is 1D and we can just use the indices as-is\n    if shape is not None:\n        indices = rearrange(indices, 'b ... -> b (...)', ...)\n\n    # get the latent vectors\n    codebook_vectors = self.codebook.weight[indices]\n\n    # reshape the latent vectors to match the original input dimensions\n    if shape is not None:\n        codebook_vectors = rearrange(codebook_vectors, 'b (d j) ... -> b d j ...', j=shape[0])\n\n    return codebook_vectors\n\n", "CodeBleu_score": 33.20458778786068}]}
{"instance_id": "c55d014827ef", "generation_result": [{"code": "def factorization(dimension: int, factor:int=-1) -> tuple[int, int]\n\t\n    if factor > 0 and dimension % factor == 0:\n        return factor, dimension // factor\n    elif factor > 0:\n        for i in range(1, factor + 1):\n            if dimension % i == 0:\n                return i, dimension // i\n    elif factor < 0:\n        m, n = 1, dimension\n        while m < n:\n            if n % m == 0:\n                return m, n // m\n            else:\n                m += 1\n                n = dimension // m\n    else:\n        return 1, dimension\n\n", "CodeBleu_score": 41.25129390908062}, {"code": "def factorization(dimension: int, factor:int=-1) -> tuple[int, int]\n\t\n    if factor > 0:\n        if dimension % factor == 0:\n            return factor, dimension // factor\n        else:\n            raise ValueError(f\"{dimension} is not divisible by {factor}\")\n    elif factor == 0:\n        return 1, dimension\n    else:\n        if dimension % -factor == 0:\n            return dimension // -factor, -factor\n        else:\n            for i in range(1, dimension):\n                if dimension % i == 0:\n                    return i, dimension // i\n    raise ValueError(f\"{dimension} could not be factorized\")\n\n", "CodeBleu_score": 29.413634712997982}, {"code": "def factorization(dimension: int, factor:int=-1) -> tuple[int, int]\n\t\n    if factor > 0:\n        if dimension % factor == 0:\n            return factor, dimension // factor\n        else:\n            return factorization(dimension, factor=factor - 1)\n    else:\n        for i in range(1, dimension // 2 + 1):\n            if dimension % i == 0:\n                return i, dimension // i\n    return dimension, 1\n\n", "CodeBleu_score": 21.26974772579768}]}
{"instance_id": "5d613ec26036", "generation_result": [{"code": "def apply_weight_decompose(self, updown, orig_weight)\n\t\n    if updown == 0:\n        # downsampling\n        weight = orig_weight * self.decompose_scale\n    elif updown == 1:\n        # upsampling\n        weight = orig_weight / self.decompose_scale\n    else:\n        raise errors.DecomposeError(f'Invalid updown value {updown}')\n\n    if self.decompose_type == 'mean':\n        # mean\n        weight = weight.mean(dim=1, keepdim=True)\n    elif self.decompose_type == 'sum':\n        # sum\n        weight = weight.sum(dim=1, keepdim=True)\n    else:\n        raise errors.DecomposeError(f'Invalid decompose type {self.decompose_type}')\n\n    weight = weight.to(self.device)\n    weight = weight.type(self.dtype)\n    return weight\n\n", "CodeBleu_score": 27.99140466992876}, {"code": "def apply_weight_decompose(self, updown, orig_weight)\n\t\n    if updown == \"up\":\n        weight = orig_weight.mul(self.up_scale_factor)\n        weight = weight.add(self.up_weight)\n        weight = weight.div(self.up_scale_factor)\n        weight = weight.mul(self.up_scale_factor)\n    elif updown == \"down\":\n        weight = orig_weight.mul(self.down_scale_factor)\n        weight = weight.add(self.down_weight)\n        weight = weight.div(self.down_scale_factor)\n        weight = weight.mul(self.down_scale_factor)\n    else:\n        raise ValueError(\"updown must be either 'up' or 'down'\")\n    return weight\n\n", "CodeBleu_score": 25.25761155709737}, {"code": "def apply_weight_decompose(self, updown, orig_weight)\n\t\n    if updown == \"up\":\n        self.weight = (orig_weight + self.weight) / 2\n    elif updown == \"down\":\n        self.weight = (orig_weight - self.weight) / 2\n    else:\n        raise errors.InvalidUpDownError(\"Invalid updown value. Must be either 'up' or 'down'.\")\n\n", "CodeBleu_score": 11.20805180394658}]}
{"instance_id": "c73d0d1cd3e5", "generation_result": [{"code": "def finalize_updown(self, updown, orig_weight, output_shape, ex_bias=None)\n\t\n    # adjust the updown tensor's shape to match the original weight's shape\n    if updown.numel() == orig_weight.numel():\n        updown = updown.reshape(orig_weight.shape)\n    # apply the bias\n    if ex_bias is not None:\n        updown = updown + ex_bias\n    # apply the scalar\n    if self.scalar is not None:\n        updown = updown * self.scalar\n    # apply the weight decomposition\n    if self.weight_decomp is not None:\n        updown = updown * self.weight_decomp\n    # reshape the updown tensor to match the output shape\n    updown = updown.reshape(output_shape)\n    return updown, ex_bias\n\n", "CodeBleu_score": 39.56955852159979}, {"code": "def finalize_updown(self, updown, orig_weight, output_shape, ex_bias=None)\n\t\n\n    if ex_bias is None:\n        ex_bias = self.bias\n\n    if updown.numel() == orig_weight.numel():\n        updown = updown.view(orig_weight.shape)\n    else:\n        updown = updown.view(output_shape)\n\n    if ex_bias is not None:\n        ex_bias = ex_bias.view(output_shape)\n        ex_bias = ex_bias * self.weight_scalar\n\n    if self.weight_decomposition != 1:\n        updown = updown * self.weight_decomposition\n\n    return updown, ex_bias\n\n", "CodeBleu_score": 42.20728219698432}, {"code": "def finalize_updown(self, updown, orig_weight, output_shape, ex_bias=None)\n\t\n    # Apply bias\n    if ex_bias is not None:\n        if ex_bias.ndim == 1:\n            ex_bias = ex_bias.reshape(1, 1, 1, -1)\n        updown = updown + ex_bias\n    # Apply weight decomposition\n    if self.weight_decomposition is not None:\n        updown = updown * self.weight_decomposition\n    # Reshape updown to match output shape\n    if updown.shape == orig_weight.shape:\n        updown = updown.reshape(output_shape)\n    else:\n        updown = updown.reshape(orig_weight.shape)\n    return updown, ex_bias\n\n", "CodeBleu_score": 39.334668786488216}]}
{"instance_id": "c6a41665c62c", "generation_result": [{"code": "def calc_updown(self, orig_weight)\n\t\n    self.updown_weight = orig_weight.to(self.device)\n    self.updown_bias = self.bias.to(self.device) if self.bias is not None else None\n    self.updown_weight, self.updown_bias = self.calc_updown_final(self.updown_weight, self.updown_bias)\n    return self.updown_weight, self.updown_bias\n\n", "CodeBleu_score": 32.11252875231755}, {"code": "def calc_updown(self, orig_weight)\n\t\n    self.weight.data = orig_weight.data.to(self.weight.device)\n    self.bias.data = orig_weight.bias.data.to(self.bias.device)\n    self.weight.data = self.finalize_weight(self.weight.data, self.bias.data)\n    return self.weight.data\n\n", "CodeBleu_score": 21.66646988088754}, {"code": "def calc_updown(self, orig_weight)\n\t\n\n    # Transfer the current weight tensor and optional bias to the device of the original weight tensor\n    if self.bias is not None:\n        self.current_weight = self.current_weight.to(orig_weight.device)\n        self.current_bias = self.current_bias.to(orig_weight.device)\n    else:\n        self.current_weight = self.current_weight.to(orig_weight.device)\n\n    # Calculate the updated weight tensor\n    if self.bias is not None:\n        self.updated_weight = self.finalize_updated_weights(orig_weight, self.current_weight, self.current_bias)\n    else:\n        self.updated_weight = self.finalize_updated_weights(orig_weight, self.current_weight)\n\n    # Return the updated weight tensor\n    return self.updated_weight\n\n", "CodeBleu_score": 35.75159775212108}]}
{"instance_id": "1bb9e5d94af8", "generation_result": [{"code": "def purge_networks_from_memory()\n\t\n    networks = [network for network in network.networks.values() if network.is_loaded]\n    if len(networks) > 20:\n        for network in networks[20:]:\n            network.purge()\n        torch.cuda.empty_cache()\n\n", "CodeBleu_score": 26.578115320005885}, {"code": "def purge_networks_from_memory()\n\t\n    logger.info(\"Purging networks from memory...\")\n    if len(shared.networks) > shared.config[\"network_limit\"]:\n        logger.info(f\"Networks in memory exceeds limit of {shared.config['network_limit']}.", "CodeBleu_score": 13.408371813527511}, {"code": "def purge_networks_from_memory()\n\t\n    # TODO: Implement this function\n    pass\n\n", "CodeBleu_score": 0.8620689668435204}]}
{"instance_id": "42b48ebf4d07", "generation_result": [{"code": "def network_restore_weights_from_backup(self: Union[torch.nn.Conv2d, torch.nn.Linear, torch.nn.GroupNorm, torch.nn.LayerNorm, torch.nn.MultiheadAttention]):\n\t\n    if self.weight.shape == self.backup_weight.shape:\n        self.weight = self.backup_weight\n    if self.bias.shape == self.backup_bias.shape:\n        self.bias = self.backup_bias\n    if isinstance(self, torch.nn.MultiheadAttention):\n        self.k_proj.weight = self.backup_k_proj_weight\n        self.v_proj.weight = self.backup_v_proj_weight\n        if self.k_proj.bias.shape == self.backup_k_proj_bias.shape:\n            self.k_proj.bias = self.backup_k_proj_bias\n        if self.v_proj.bias.shape == self.backup_v_proj_bias.shape:\n            self.v_proj.bias = self.backup_v_proj_bias\n\n", "CodeBleu_score": 32.38701427040884}, {"code": "def network_restore_weights_from_backup(self: Union[torch.nn.Conv2d, torch.nn.Linear, torch.nn.GroupNorm, torch.nn.LayerNorm, torch.nn.MultiheadAttention]):\n\t\n    if isinstance(self, torch.nn.Conv2d) or isinstance(self, torch.nn.Linear):\n        if os.path.isfile(f\"{self.weight_backup_path}.weight\"):\n            self.weight.data = torch.load(f\"{self.weight_backup_path}.weight\")\n        if os.path.isfile(f\"{self.bias_backup_path}.bias\"):\n            self.bias.data = torch.load(f\"{self.bias_backup_path}.bias\")\n    elif isinstance(self, torch.nn.GroupNorm) or isinstance(self, torch.nn.LayerNorm):\n        if os.path.isfile(f\"{self.weight_backup_path}.weight\"):\n            self.weight.data = torch.load(f\"{self.weight_backup_path}.weight\")\n        if os.path.isfile(f\"{self.bias_backup_path}.bias\"):\n            self.bias.data = torch.load(f\"{self.bias_backup_path}.bias\")\n    elif isinstance(self, torch.nn.MultiheadAttention):\n        if os.path.isfile(f\"{self.q_weight_backup_path}.weight\"):\n            self.q_proj.weight.data = torch.load(f\"{self.q_weight_backup_path}.weight\")\n        if os.path.isfile(f\"{self.q_bias_backup_path}.bias\"):\n            self.q_proj.bias.data = torch.load(f\"{self.q_bias_backup_path}.bias\")\n        if os.path.isfile(f\"{self.k_weight_backup_path}.weight\"):\n            self.k_proj.weight.data = torch.load(f\"{self.k_weight_backup_path}.weight\")\n        if os.path.isfile(f\"{self.k_bias_backup_path}.bias\"", "CodeBleu_score": 34.5255033725106}, {"code": "def network_restore_weights_from_backup(self: Union[torch.nn.Conv2d, torch.nn.Linear, torch.nn.GroupNorm, torch.nn.LayerNorm, torch.nn.MultiheadAttention]):\n\t\n    if isinstance(self, torch.nn.Conv2d):\n        if os.path.exists(f\"{self.backup_path}/weights.pt\"):\n            self.weight.data = torch.load(f\"{self.backup_path}/weights.pt\")\n            if os.path.exists(f\"{self.backup_path}/biases.pt\"):\n                self.bias.data = torch.load(f\"{self.backup_path}/biases.pt\")\n            else:\n                self.bias = None\n        else:\n            logger.error(f\"No weights backup found for layer {self.name}!\")\n    elif isinstance(self, torch.nn.Linear):\n        if os.path.exists(f\"{self.backup_path}/weights.pt\"):\n            self.weight.data = torch.load(f\"{self.backup_path}/weights.pt\")\n            if os.path.exists(f\"{self.backup_path}/biases.pt\"):\n                self.bias.data = torch.load(f\"{self.backup_path}/biases.pt\")\n            else:\n                self.bias = None\n        else:\n            logger.error(f\"No weights backup found for layer {self.name}!\")\n    elif isinstance(self, torch.nn.GroupNorm):\n        if os.path.exists(f\"{self.backup_path}/weights.pt\"):\n            self.weight.data = torch.load(f\"{self.backup_path}/weights.pt\")\n            if os.path.exists(f\"{self.backup_path}/biases.pt\"):\n                self.bias.data = torch.load(f\"{self.backup_path}/biases.pt\")\n            else:\n                self.bias = None\n        else:\n            logger.error(f\"No weights backup found for layer {self.name}!\")\n    elif isinstance(self, torch.nn.LayerNorm):\n        if os.path.exists(f\"{self", "CodeBleu_score": 33.949152706802}]}
{"instance_id": "c8fcd357131a", "generation_result": [{"code": "def network_forward(org_module, input, original_forward)\n\t\n    if not org_module.is_loaded:\n        return original_forward(input)\n    else:\n        if isinstance(input, torch.Tensor):\n            input = input.to(org_module.device)\n        if org_module.is_backup:\n            org_module.restore_weights()\n        output = original_forward(input)\n        if org_module.is_backup:\n            org_module.reset_weights()\n        for network in org_module.networks:\n            if network.layer == org_module.layer:\n                output = network.forward(output)\n        return output\n\n", "CodeBleu_score": 34.91584554364556}, {"code": "def network_forward(org_module, input, original_forward)\n\t\n    logger.info(\"Running network forward pass\")\n    logger.debug(\"Input shape: {}\".format(input.shape))\n    if not shared.networks:\n        logger.debug(\"No networks loaded\")\n        return original_forward(input)\n    else:\n        logger.debug(\"Networks loaded\")\n    logger.debug(\"Networks loaded: {}\".format(shared.networks))\n    logger.debug(\"Current module: {}\".format(org_module))\n    logger.debug(\"Current module name: {}\".format(org_module.__class__.__name__))\n    logger.debug(\"Current module type: {}\".format(type(org_module)))\n    logger.debug(\"Current module device: {}\".format(org_module.device))\n    logger.debug(\"Current module weights: {}\".format(org_module.weight.shape))\n    logger.debug(\"Current module weights: {}\".format(org_module.weight))\n    logger.debug(\"Current module bias: {}\".format(org_module.bias.shape))\n    logger.debug(\"Current module bias: {}\".format(org_module.bias))\n    if not isinstance(org_module, devices.Device):\n        logger.debug(\"Current module is not a device\")\n        return original_forward(input)\n    else:\n        logger.debug(\"Current module is a device\")\n    if not isinstance(org_module, sd_models.SDModel):\n        logger.debug(\"Current module is not an SDModel\")\n        return original_forward(input)\n    else:\n        logger.debug(\"Current module is an SDModel\")\n    logger.debug(\"Current module name: {}\".format(org_module.name))\n    logger.debug(\"Current module type: {}\".format(type(org_module)))\n    logger.debug(\"Current module device: {}\".format(org_module.device))\n    logger.debug(\"Current module weights: {}\".format(org_module.weight.shape))\n    logger.debug(\"Current module weights: {}\".format(org_module.weight))\n    logger.debug(\"Current module bias: {}\".format(org_module.bias.shape))\n    logger.debug(\"Current module bias: {}\".format(org_module.bias", "CodeBleu_score": 23.109409368525075}, {"code": "def network_forward(org_module, input, original_forward)\n\t\n    # Check if any networks are loaded\n    if not len(sd_models.networks):\n        # No networks are loaded, so just use the original forward method\n        return original_forward(input)\n    # Get the device\n    device = shared.get_device()\n    # Cast the input to the appropriate device\n    input = input.to(device)\n    # Get the current module\n    module = org_module\n    # Get the current layer\n    layer = module.__class__.__name__\n    # Get the weights of the current module\n    weights = module.state_dict()\n    # Get the weights of the current module from backup\n    backup_weights = sd_models.backup_weights[layer]\n    # Restore the weights of the current module\n    module.load_state_dict(backup_weights)\n    # Get the output of the original forward method\n    output = original_forward(input)\n    # Check if the current module is a layer that has been hijacked\n    if layer in sd_hijack.hijacked_layers:\n        # The current module has been hijacked, so get the hijacked module\n        hijacked_module = sd_hijack.hijacked_layers[layer]\n        # Get the hijacked layer\n        hijacked_layer = hijacked_module.__class__.__name__\n        # Get the hijacked weights of the hijacked module\n        hijacked_weights = hijacked_module.state_dict()\n        # Get the hijacked weights of the hijacked module from backup\n        hijacked_backup_weights = sd_models.backup_weights[hijacked_layer]\n        # Restore the hijacked weights of the hijacked module\n        hijacked_module.load_state_dict(hijacked_backup_weights)\n        # Get the hijacked output of the hijacked module\n        hijacked_output = hijacked_module(output)\n        # Get the hijacked output of the hijacked module from backup\n        hijacked_backup_output = sd_models.backup_outputs[hijacked", "CodeBleu_score": 29.694460114436787}]}
{"instance_id": "1070211f5c11", "generation_result": [{"code": "def trainables(self)\n\t\n    trainables = []\n    for name, param in self.named_parameters():\n        if param.requires_grad:\n            trainables.append(param)\n    return trainables\n\n", "CodeBleu_score": 25.147289601791904}, {"code": "def trainables(self)\n\t\n    trainables = []\n    for name, param in self.named_parameters():\n        if param.requires_grad:\n            trainables.append(param)\n    return trainables\n\n", "CodeBleu_score": 25.147289601791904}, {"code": "def trainables(self)\n\t\n    trainables = []\n    for name, module in self.named_modules():\n        if isinstance(module, torch.nn.Linear) or isinstance(module, torch.nn.LayerNorm):\n            trainables.append(module)\n    return trainables\n\n", "CodeBleu_score": 33.19038196531204}]}
{"instance_id": "0c3ba62c96a7", "generation_result": [{"code": "def load_hypernetworks(names, multipliers=None)\n\t\n    if multipliers is None:\n        multipliers = [1.0] * len(names)\n    if len(names) != len(multipliers):\n        raise ValueError(\"The number of names and multipliers must be the same.\")\n    for name, multiplier in zip(names, multipliers):\n        if name not in shared.hypernetworks:\n            shared.hypernetworks[name] = sd_models.load_hypernetwork(name)\n        shared.hypernetworks[name].multiplier = multiplier\n    shared.hypernetworks.clear()\n    for name, multiplier in zip(names, multipliers):\n        shared.hypernetworks.append(shared.hypernetworks[name])\n\n", "CodeBleu_score": 31.37590447593076}, {"code": "def load_hypernetworks(names, multipliers=None)\n\t\n    if multipliers is None:\n        multipliers = [1 for _ in names]\n    for name, multiplier in zip(names, multipliers):\n        if name not in shared.hypernetworks:\n            shared.hypernetworks[name] = sd_models.load_hypernetwork(name)\n        shared.hypernetworks[name].multiplier = multiplier\n    shared.hypernetworks.clear()\n    for name, multiplier in zip(names, multipliers):\n        shared.hypernetworks.append(shared.hypernetworks[name])\n    return shared.hypernetworks\n\n", "CodeBleu_score": 27.712878982951384}, {"code": "def load_hypernetworks(names, multipliers=None)\n\t\n\n    # Check if hypernetworks are already loaded\n    if len(shared.hypernetworks) > 0:\n        print(\"Hypernetworks are already loaded. Clearing list.\")\n        shared.hypernetworks.clear()\n\n    # Load hypernetworks\n    for name in names:\n        # Check if hypernetwork is already loaded\n        if name in [hypernetwork.name for hypernetwork in shared.hypernetworks]:\n            print(f\"Hypernetwork {name} is already loaded. Skipping.\")\n            continue\n\n        # Load hypernetwork\n        print(f\"Loading hypernetwork {name}\")\n        hypernetwork = sd_models.load_hypernetwork(name)\n\n        # Set multiplier if provided\n        if multipliers is not None:\n            hypernetwork.multiplier = multipliers[name]\n\n        # Append hypernetwork to list\n        shared.hypernetworks.append(hypernetwork)\n\n", "CodeBleu_score": 36.27862674853859}]}
{"instance_id": "bea3772ed532", "generation_result": [{"code": "\ndef apply_hypernetworks(hypernetworks, context, layer=None)\n\t\n    context_k, context_v = context\n    for hypernetwork in hypernetworks:\n        context_k, context_v = apply_hypernetwork(hypernetwork, context_k, context_v, layer)\n    return context_k, context_v\n\n", "CodeBleu_score": 67.00402757474637}, {"code": "\ndef apply_hypernetworks(hypernetworks, context, layer=None)\n\t\n    context_k = context\n    context_v = context\n    for hypernetwork in hypernetworks:\n        context_k, context_v = apply_single_hypernetwork(hypernetwork, context_k, context_v, layer=layer)\n    return context_k, context_v\n\n", "CodeBleu_score": 66.82264135585403}, {"code": "\ndef apply_hypernetworks(hypernetworks, context, layer=None)\n\t\n    context_k = context\n    context_v = context\n    for hypernetwork in hypernetworks:\n        context_k, context_v = apply_hypernetwork(hypernetwork, context_k, context_v, layer=layer)\n    return context_k, context_v\n\n", "CodeBleu_score": 63.234431095063485}]}
{"instance_id": "922efa3e092f", "generation_result": [{"code": "def attention_CrossAttention_forward(self, x, context=None, mask=None, **kwargs)\n\t\n    # Compute query, key, and value projections\n    q = self.query_projection(x)\n    k = self.key_projection(context)\n    v = self.value_projection(context)\n    # Apply hypernetworks\n    q = self.query_hypernetwork(q)\n    k = self.key_hypernetwork(k)\n    v = self.value_hypernetwork(v)\n    # Reshape tensors for multi-head attention\n    q = rearrange(q, 'b n (h d) -> (b h) n d', h=self.num_heads)\n    k = rearrange(k, 'b n (h d) -> (b h) n d', h=self.num_heads)\n    v = rearrange(v, 'b n (h d) -> (b h) n d', h=self.num_heads)\n    # Calculate scaled dot-product attention\n    attention = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n    if mask is not None:\n        attention = attention.masked_fill(~mask, float('-inf'))\n    attention = attention.softmax(dim=-1)\n    # Optionally apply a mask to the attention scores\n    if self.masked_attention is not None:\n        attention = self.masked_attention(attention)\n    # Compute the attention output\n    out = einsum('b h i j, b h j d -> b h i d', attention, v)\n    # Reshape back to the original shape\n    out = rearrange(out, '(b h) n d -> b n (h d)', h=self.num_heads)\n    # Pass through a final linear layer\n    out = self.output_projection(out)\n    return out\n\n", "CodeBleu_score": 46.767172528886576}, {"code": "def attention_CrossAttention_forward(self, x, context=None, mask=None, **kwargs)\n\t\n\n    # Compute query, key, and value projections\n    query = self.query_projection(x)\n    key = self.key_projection(context)\n    value = self.value_projection(context)\n\n    # Apply hypernetworks to context\n    query = self.query_hypernetwork(query)\n    key = self.key_hypernetwork(key)\n    value = self.value_hypernetwork(value)\n\n    # Reshape tensors for multi-head attention\n    query = rearrange(query, 'b n (h d) -> (b h) n d', h=self.num_heads)\n    key = rearrange(key, 'b n (h d) -> (b h) n d', h=self.num_heads)\n    value = rearrange(value, 'b n (h d) -> (b h) n d', h=self.num_heads)\n\n    # Compute attention scores\n    attention_scores = einsum('b h i d, b h j d -> b h i j', query, key)\n    attention_scores = attention_scores / math.sqrt(query.size(-1))\n\n    # Apply mask to attention scores\n    if mask is not None:\n        attention_scores = attention_scores + mask\n\n    # Compute attention weights\n    attention_weights = torch.softmax(attention_scores, dim=-1)\n\n    # Compute attention output\n    attention_output = einsum('b h i j, b h j d -> b h i d', attention_weights, value)\n\n    # Reshape back to original shape\n    attention_output = rearrange(attention_output, '(b h) n d -> b n (h d)', h=self.num_heads)\n\n    # Pass through final linear layer\n    attention_output = self.output_projection(attention_output)\n\n    return attention_output\n\n", "CodeBleu_score": 38.58323031901403}, {"code": "def attention_CrossAttention_forward(self, x, context=None, mask=None, **kwargs)\n\t\n    # Get the input and context tensors\n    query = x\n    if context is None:\n        context = query\n    # Get the query and key projections\n    query_proj = self.query_proj(query)\n    key_proj = self.key_proj(context)\n    # Get the value projections\n    value_proj = self.value_proj(context)\n    # Get the context projections\n    context_proj = self.context_proj(context)\n    # Get the hypernetwork output\n    hyper_out = self.hyper_proj(context_proj)\n    # Reshape the tensors for multi-head attention\n    query_proj = rearrange(query_proj, 'b n (h d) -> b h n d', h=self.num_heads)\n    key_proj = rearrange(key_proj, 'b n (h d) -> b h n d', h=self.num_heads)\n    value_proj = rearrange(value_proj, 'b n (h d) -> b h n d', h=self.num_heads)\n    context_proj = rearrange(context_proj, 'b n (h d) -> b h n d', h=self.num_heads)\n    # Calculate the attention\n    attention = einsum('b h i d, b h j d -> b h i j', query_proj, key_proj)\n    # Apply the hypernetwork\n    attention = attention / self.hyper_scaling(hyper_out)\n    # Optionally apply a mask\n    if mask is not None:\n        attention = attention * mask\n    # Calculate the attention weights\n    attention = torch.softmax(attention, dim=-1)\n    # Optionally apply dropout\n    if self.dropout is not None:\n        attention = self.dropout(attention)\n    # Calculate the attention output\n    attention_out = einsum('b h i j, b h j d -> b h i d', attention, value_proj)\n    # Reshape the output\n    attention_out = rearrange(attention_out, 'b h n d -> b n (h d)')\n    # Pass the output through a final linear layer\n   ", "CodeBleu_score": 40.60075438817692}]}
{"instance_id": "8b7455a03a0e", "generation_result": [{"code": "def save_hypernetwork(hypernetwork, checkpoint, hypernetwork_name, filename)\n\t\n    hypernetwork_state = dict(hypernetwork.state_dict())\n    hypernetwork.load_state_dict(checkpoint)\n    try:\n        torch.save(hypernetwork.state_dict(), filename)\n    except:\n        hypernetwork.load_state_dict(hypernetwork_state)\n        raise\n    hypernetwork.load_state_dict(hypernetwork_state)\n    setattr(hypernetwork, hypernetwork_name, filename)\n\n", "CodeBleu_score": 21.39690143955699}, {"code": "def save_hypernetwork(hypernetwork, checkpoint, hypernetwork_name, filename)\n\t\n    original_hypernetwork_state = {}\n    for key, value in hypernetwork.__dict__.items():\n        original_hypernetwork_state[key] = value\n    hypernetwork.__dict__[\"checkpoint\"] = checkpoint\n    hypernetwork.__dict__[\"hypernetwork_name\"] = hypernetwork_name\n    try:\n        torch.save(hypernetwork.state_dict(), filename)\n    except:\n        for key, value in original_hypernetwork_state.items():\n            hypernetwork.__dict__[key] = value\n        raise\n    for key, value in original_hypernetwork_state.items():\n        hypernetwork.__dict__[key] = value\n\n", "CodeBleu_score": 26.02648200522013}, {"code": "def save_hypernetwork(hypernetwork, checkpoint, hypernetwork_name, filename)\n\t\n    hypernetwork_state = {}\n    for attribute in dir(hypernetwork):\n        if not attribute.startswith('_') and not callable(getattr(hypernetwork, attribute)):\n            hypernetwork_state[attribute] = getattr(hypernetwork, attribute)\n\n    setattr(hypernetwork, 'checkpoint', checkpoint)\n    setattr(hypernetwork, 'hypernetwork_name', hypernetwork_name)\n    try:\n        torch.save(hypernetwork.state_dict(), filename)\n    except Exception as e:\n        print(e)\n        for attribute, value in hypernetwork_state.items():\n            setattr(hypernetwork, attribute, value)\n        raise\n    for attribute, value in hypernetwork_state.items():\n        setattr(hypernetwork, attribute, value)\n\n", "CodeBleu_score": 28.942131553640195}]}
{"instance_id": "3175f835140f", "generation_result": [{"code": "def ema_scope(self, context=None)\n\t\n    if context is None:\n        context = {}\n\n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            if self.ema is not None:\n                context['ema_enabled'] = self.ema.enabled\n                context['ema_model'] = self.ema.model\n                context['ema_alpha'] = self.ema.alpha\n                context['ema_decay'] = self.ema.decay\n                context['ema_threshold'] = self.ema.threshold\n                self.ema.enabled = True\n                self.ema.model = self.model\n                self.ema.alpha = self.ema_alpha\n                self.ema.decay = self.ema_decay\n                self.ema.threshold = self.ema_threshold\n                if self.log_ema_updates:\n                    log_txt_as_img(f'Ema enabled: alpha={self.ema_alpha}, decay={self.ema_decay}, threshold={self.ema_threshold}', self.logger)\n            ret = func(*args, **kwargs)\n            if self.ema is not None:\n                self.ema.enabled = context['ema_enabled']\n                self.ema.model = context['ema_model']\n                self.ema.alpha = context['ema_alpha']\n                self.ema.decay = context['ema_decay']\n                self.ema.threshold = context['ema_threshold']\n                if self.log_ema_updates:\n                    log_txt_as_img(f'Ema disabled: alpha={self.ema_alpha}, decay={self.ema_decay}, threshold={self.ema_threshold}', self.logger)\n            return ret\n        return wrapper\n    return decorator\n\n", "CodeBleu_score": 29.80116135563813}, {"code": "def ema_scope(self, context=None)\n\t\n    if context is None:\n        context = {}\n    if self.use_ema:\n        if not self.ema_enabled:\n            self.ema_enabled = True\n            self.ema_context = context\n            self.ema_backup = {}\n            for name, param in self.named_parameters():\n                self.ema_backup[name] = param.clone()\n                param.copy_(self.ema.shadow[name])\n            if self.ema_logging:\n                self.log_ema_status(context)\n        else:\n            if self.ema_logging:\n                self.log_ema_status(context)\n    else:\n        if self.ema_enabled:\n            self.ema_enabled = False\n            if self.ema_logging:\n                self.log_ema_status(context)\n            for name, param in self.named_parameters():\n                param.copy_(self.ema_backup[name])\n    yield\n\n", "CodeBleu_score": 26.2035116031538}, {"code": "def ema_scope(self, context=None)\n\t\n    if not self.ema_enabled:\n        return context\n    else:\n        if context is None:\n            return lambda: None\n        else:\n            return lambda: self.ema_scope(context)\n\n", "CodeBleu_score": 17.37975637176709}]}
{"instance_id": "a7b5d5cd519a", "generation_result": [{"code": "def p_mean_variance(self, x, t, clip_denoised: bool)\n\t\n    # get the model output\n    model_output = self.model(x, t)\n    # get the reconstruction\n    if self.denoising_type == 'none':\n        reconstruction = x\n    elif self.denoising_type == 'predict_noise':\n        reconstruction = self.predict_noise(model_output)\n    elif self.denoising_type == 'predict_image':\n        reconstruction = self.predict_image(model_output)\n    else:\n        raise NotImplementedError\n    # clip the reconstruction if specified\n    if clip_denoised:\n        reconstruction = torch.clamp(reconstruction, -1, 1)\n    # calculate the mean and variance\n    mean = reconstruction\n    variance = self.model.output_scale * self.model.output_scale\n    log_variance = torch.zeros_like(variance)\n    return mean, variance, log_variance\n\n", "CodeBleu_score": 36.85993751285657}, {"code": "def p_mean_variance(self, x, t, clip_denoised: bool)\n\t\n    if self.model.use_ema:\n        x_mean, x_var = self.model.ema.get_ema(x, t)\n    else:\n        x_mean, x_var = self.model.model.mean_variance(x, t)\n    if self.model.use_ddim:\n        x_mean = x_mean.squeeze(-1)\n        x_var = x_var.squeeze(-1)\n    if clip_denoised:\n        x_mean = x_mean.clamp(-1, 1)\n    return x_mean, x_var\n\n", "CodeBleu_score": 31.097183541159612}, {"code": "def p_mean_variance(self, x, t, clip_denoised: bool)\n\t\n    # get the model output\n    out = self(x, t)\n    # either use the model output directly or reconstruct the input from the noise\n    if self.reconstruct_noise:\n        x_hat = self.model.reverse(out)\n    else:\n        x_hat = out\n    # clip the reconstructed input if specified\n    if clip_denoised:\n        x_hat = torch.clamp(x_hat, -1, 1)\n    # calculate the mean, posterior variance, and posterior log variance\n    mean = x_hat\n    variance = extract_into_tensor(out, \"variance\")\n    log_variance = extract_into_tensor(out, \"log_variance\")\n    return mean, variance, log_variance\n\n", "CodeBleu_score": 34.1054478276049}]}
{"instance_id": "a8652fa78473", "generation_result": [{"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss_type == 'l1':\n        if mean:\n            return pred.abs().mean()\n        else:\n            return pred.abs()\n    elif self.loss_type == 'l2':\n        if mean:\n            return pred.square().mean()\n        else:\n            return pred.square()\n    else:\n        raise ValueError('Unknown loss type.')\n\n", "CodeBleu_score": 32.85633353886641}, {"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss_type == 'l1':\n        if mean:\n            loss = torch.mean(torch.abs(pred - target))\n        else:\n            loss = torch.abs(pred - target)\n    elif self.loss_type == 'l2':\n        if mean:\n            loss = torch.mean((pred - target) ** 2)\n        else:\n            loss = (pred - target) ** 2\n    else:\n        raise ValueError(f'Unknown loss type {self.loss_type}')\n    return loss\n\n", "CodeBleu_score": 45.772721258995894}, {"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss_type == 'l1':\n        if mean:\n            return torch.mean(torch.abs(pred - target))\n        else:\n            return torch.abs(pred - target)\n    elif self.loss_type == 'l2':\n        if mean:\n            return torch.mean((pred - target) ** 2)\n        else:\n            return (pred - target) ** 2\n    else:\n        raise ValueError(f'Unknown loss type {self.loss_type}')\n\n", "CodeBleu_score": 34.197735971701654}]}
{"instance_id": "d68e4f061349", "generation_result": [{"code": "def _prior_bpd(self, x_start)\n\t\n    t = self.model.timesteps.to(x_start.device)\n    x_start = x_start.unsqueeze(0)\n    x_start = repeat(x_start, '() n c h w -> b n c h w', b=self.model.num_channels)\n    logits = self.model.prior(x_start)\n    logits = extract_into_tensor(logits, t)\n    logits = logits.squeeze(0)\n    logits = logits.view(logits.shape[0], -1)\n    return normal_kl(logits, self.model.prior_variance)\n\n", "CodeBleu_score": 19.847389887065514}, {"code": "def _prior_bpd(self, x_start)\n\t\n    # Determine batch size\n    batch_size = x_start.shape[0]\n    # Create tensor t\n    t = torch.arange(self.num_timesteps, device=self.device).view(1, -1).repeat(batch_size, 1)\n    # Compute mean and log variance of predicted distribution at t\n    mu, logvar = self.model(x_start, t)\n    # Compute KL divergence between predicted and standard normal distribution\n    kl = normal_kl(mu, logvar)\n    # Return mean KL divergence normalized by log(2)\n    return kl.mean() / torch.log(2)\n\n", "CodeBleu_score": 24.442263639921542}, {"code": "def _prior_bpd(self, x_start)\n\t\n    bpd = self.prior_bpd(x_start)\n    return bpd\n\n", "CodeBleu_score": 1.8145151655146983}]}
{"instance_id": "c695d93fdf2d", "generation_result": [{"code": "def make_cache(subsection: str) -> diskcache.Cache\n\t\n    cache_dir = os.path.join(data_path, \"cache\")\n    os.makedirs(cache_dir, exist_ok=True)\n    cache_path = os.path.join(cache_dir, f\"{subsection}.db\")\n    return diskcache.Cache(\n        cache_path,\n        size_limit=4 * 1024 * 1024 * 1024,\n        min_file_size=256 * 1024,\n    )\n\n", "CodeBleu_score": 50.664632529476236}, {"code": "def make_cache(subsection: str) -> diskcache.Cache\n\t\n    return diskcache.Cache(os.path.join(data_path, subsection), size_limit=4e9, min_file_size=256e3)\n\n", "CodeBleu_score": 44.46753919809294}, {"code": "def make_cache(subsection: str) -> diskcache.Cache\n\t\n    cache_dir = os.path.join(data_path, \"cache\")\n    if not os.path.exists(cache_dir):\n        os.makedirs(cache_dir)\n    cache = diskcache.Cache(\n        os.path.join(cache_dir, subsection),\n        size_limit=4 * 1024 * 1024 * 1024,\n        min_file_size=256 * 1024,\n    )\n    return cache\n\n", "CodeBleu_score": 57.19427534922836}]}
{"instance_id": "ed4c543f0d38", "generation_result": [{"code": "def cache(subsection)\n\t\n    global caches\n    global cache_lock\n    global cache_lock_active\n    global cache_lock_waiting\n    cache_lock_active = False\n    cache_lock_waiting = False\n    cache_lock = threading.Lock()\n    if subsection not in caches:\n        cache_lock.acquire()\n        cache_lock_active = True\n        if subsection not in caches:\n            if not os.path.exists(os.path.join(data_path, 'cache')):\n                os.mkdir(os.path.join(data_path, 'cache'))\n            if not os.path.isfile(os.path.join(data_path, 'cache', subsection + '.json')):\n                convert_old_cached_data(subsection)\n            caches[subsection] = make_cache(subsection)\n        cache_lock.release()\n    return caches[subsection]\n\n", "CodeBleu_score": 29.12864408504469}, {"code": "def cache(subsection)\n\t\n    global caches\n    global cache_lock\n    global cache_dir\n    global cache_filename\n\n    cache_lock.acquire()\n    if subsection in caches:\n        cache_lock.release()\n        return caches[subsection]\n    else:\n        if not os.path.isdir(cache_dir):\n            os.mkdir(cache_dir)\n        if not os.path.isfile(cache_filename):\n            convert_old_cached_data()\n        if not os.path.isfile(cache_filename):\n            print(\"Error: Cache file does not exist.\")\n            cache_lock.release()\n            return None\n        caches[subsection] = make_cache(subsection)\n        cache_lock.release()\n        return caches[subsection]\n\n", "CodeBleu_score": 38.41746317431036}, {"code": "def cache(subsection)\n\t\n\n    # Check if the cache directory exists and if the cache filename is a valid file. If these conditions are not met, it calls convert_old_cached_data() to convert legacy cached data.\n    if not os.path.exists(data_path):\n        os.makedirs(data_path)\n    if not os.path.isfile(os.path.join(data_path, 'cached_data.json')):\n        convert_old_cached_data()\n\n    # Re-checks the caches dictionary for the subsection. If the cache object still does not exist, it creates a new cache object using make_cache(subsection) and stores it in the caches dictionary.\n    global caches\n    if subsection not in caches:\n        with cache_lock:\n            if subsection not in caches:\n                caches[subsection] = make_cache(subsection)\n    return caches[subsection]\n\n", "CodeBleu_score": 41.36211219867517}]}
{"instance_id": "ccf5ea2d9fc1", "generation_result": [{"code": "def cached_data_for_file(subsection, title, filename, func)\n\t\n    cache = cache(subsection)\n    mtime = os.path.getmtime(filename)\n    if title in cache:\n        if mtime > cache[title][\"mtime\"]:\n            cache.pop(title)\n    if title not in cache:\n        cache[title] = {\n            \"data\": func(),\n            \"mtime\": mtime\n        }\n        dump_cache(subsection, cache)\n    return cache[title][\"data\"]\n\n", "CodeBleu_score": 27.56118484952596}, {"code": "def cached_data_for_file(subsection, title, filename, func)\n\t\n    cache = cache(subsection)\n    mtime = os.path.getmtime(os.path.join(data_path, filename))\n    if title in cache:\n        if mtime > cache[title][\"mtime\"]:\n            cache.pop(title)\n    if title not in cache:\n        cache[title] = {\"data\": func(), \"mtime\": mtime}\n        dump_cache(cache, subsection)\n    return cache[title][\"data\"]\n\n", "CodeBleu_score": 26.850847074110256}, {"code": "def cached_data_for_file(subsection, title, filename, func)\n\t\n    cache = cache(subsection)\n    try:\n        last_mtime = os.path.getmtime(filename)\n        if title in cache:\n            mtime = cache[title][\"mtime\"]\n            if last_mtime > mtime:\n                del cache[title]\n    except KeyError:\n        pass\n    if title not in cache:\n        cache[title] = {\"data\": func(), \"mtime\": last_mtime}\n        dump_cache(cache, subsection)\n    return cache[title][\"data\"]\n\n", "CodeBleu_score": 36.8831896359312}]}
{"instance_id": "e01a2d4f0ac2", "generation_result": [{"code": "def report(message: str, *, exc_info: bool = False) -> None\n\t\n    if exc_info:\n        traceback.print_exc()\n    print(f\"*** {message}\", file=sys.stderr)\n    print(\"---\", file=sys.stderr)\n\n", "CodeBleu_score": 35.64865798648555}, {"code": "def report(message: str, *, exc_info: bool = False) -> None\n\t\n    if exc_info:\n        exc_type, exc_value, exc_traceback = sys.exc_info()\n        exc_lines = traceback.format_exception(exc_type, exc_value, exc_traceback)\n        for line in exc_lines:\n            sys.stderr.write(line)\n        sys.stderr.write(\"", "CodeBleu_score": 37.25073318141604}, {"code": "def report(message: str, *, exc_info: bool = False) -> None\n\t\n    if exc_info:\n        exc_type, exc_value, exc_traceback = sys.exc_info()\n        traceback.print_exception(exc_type, exc_value, exc_traceback, file=sys.stderr)\n        print(\"---\", file=sys.stderr)\n    print(\"***\", message, file=sys.stderr)\n\n", "CodeBleu_score": 51.961313448016234}]}
{"instance_id": "125fb9d2320b", "generation_result": [{"code": "def extract_original_prompts(style: PromptStyle, prompt, negative_prompt)\n\t\n    if style.prompt == \"\" and style.negative_prompt == \"\":\n        return False, prompt, negative_prompt\n    try:\n        positive_prompt = style.prompt_extractor(prompt)\n    except:\n        return False, prompt, negative_prompt\n    try:\n        negative_prompt = style.negative_prompt_extractor(negative_prompt)\n    except:\n        return False, prompt, negative_prompt\n    return True, positive_prompt, negative_prompt\n\n", "CodeBleu_score": 48.87501635314504}, {"code": "def extract_original_prompts(style: PromptStyle, prompt, negative_prompt)\n\t\n    if style.prompt == \"\" and style.negative_prompt == \"\":\n        return False, prompt, negative_prompt\n    if style.prompt == \"\":\n        try:\n            prompt = extract_style_text(style.negative_prompt, style.negative_prompt_style)\n        except Exception:\n            return False, prompt, negative_prompt\n    else:\n        try:\n            prompt = extract_style_text(style.prompt, style.prompt_style)\n        except Exception:\n            return False, prompt, negative_prompt\n    if style.negative_prompt == \"\":\n        try:\n            negative_prompt = extract_style_text(style.prompt, style.prompt_style)\n        except Exception:\n            return False, prompt, negative_prompt\n    else:\n        try:\n            negative_prompt = extract_style_text(style.negative_prompt, style.negative_prompt_style)\n        except Exception:\n            return False, prompt, negative_prompt\n    return True, prompt, negative_prompt\n\n", "CodeBleu_score": 47.953401088353765}, {"code": "def extract_original_prompts(style: PromptStyle, prompt, negative_prompt)\n\t\n\n    if style.prompt == \"\" and style.negative_prompt == \"\":\n        return False, prompt, negative_prompt\n\n    try:\n        positive_prompt = style.extract_prompt(prompt)\n    except errors.PromptExtractionError:\n        return False, prompt, negative_prompt\n\n    try:\n        negative_prompt = style.extract_prompt(negative_prompt)\n    except errors.PromptExtractionError:\n        return False, prompt, negative_prompt\n\n    return True, positive_prompt, negative_prompt\n\n", "CodeBleu_score": 49.94233011818262}]}
{"instance_id": "0c5f729fbce2", "generation_result": [{"code": "def random_partial(self, count, n_frames)\n\t\n    if not self.utterances:\n        self._load_utterances()\n    if not self.utterance_cycler:\n        self.utterance_cycler = RandomCycler(self.utterances)\n    samples = self.utterance_cycler.sample(count)\n    partials = []\n    for u in samples:\n        partial = u.random_partial(n_frames)\n        partials.append((u, partial))\n    return partials\n\n", "CodeBleu_score": 45.88541286882947}, {"code": "def random_partial(self, count, n_frames)\n\t\n    if not self.utterances_loaded:\n        self._load_utterances()\n\n    utterances = self.utterance_cycler.sample(count)\n\n    partials = []\n\n    for u in utterances:\n        partials.append((u, u.random_partial(n_frames)))\n\n    return partials\n\n", "CodeBleu_score": 50.84384400588253}, {"code": "def random_partial(self, count, n_frames)\n\t\n    if self.utterances is None:\n        self._load_utterances()\n    utterances = self.utterance_cycler.sample(count)\n    return [(u, u.random_partial(n_frames)) for u in utterances]\n\n", "CodeBleu_score": 64.07063310277653}]}
{"instance_id": "31e8b27b8f94", "generation_result": [{"code": "def normalize_volume(wav, target_dBFS, increase_only=False, decrease_only=False)\n\t\n    if increase_only and decrease_only:\n        raise ValueError(\"Cannot increase and decrease volume only at the same time.\")\n    wav = wav.copy()\n    change_in_dBFS = target_dBFS - safe_db_to_float(wav)\n    if change_in_dBFS == 0:\n        return wav\n    elif not increase_only and not decrease_only:\n        return wav * (10 ** (change_in_dBFS / 20))\n    elif decrease_only and change_in_dBFS > 0:\n        return wav\n    elif increase_only and change_in_dBFS < 0:\n        return wav\n    else:\n        return wav * (10 ** (change_in_dBFS / 20))\n\n", "CodeBleu_score": 44.47090785197136}, {"code": "def normalize_volume(wav, target_dBFS, increase_only=False, decrease_only=False)\n\t\n    if increase_only and decrease_only:\n        raise ValueError(\"Cannot set increase_only and decrease_only to True.\")\n    if not increase_only and not decrease_only:\n        change_in_dBFS = target_dBFS - safe_db_to_float(wav)\n    elif increase_only:\n        if change_in_dBFS <= 0:\n            return wav\n        else:\n            change_in_dBFS = target_dBFS - safe_db_to_float(wav)\n    elif decrease_only:\n        if change_in_dBFS >= 0:\n            return wav\n        else:\n            change_in_dBFS = target_dBFS - safe_db_to_float(wav)\n    return wav * (10 ** (change_in_dBFS / 20))\n\n", "CodeBleu_score": 38.808389303524436}, {"code": "def normalize_volume(wav, target_dBFS, increase_only=False, decrease_only=False)\n\t\n\n    # If increase_only and decrease_only are both True, raise an error\n    if increase_only and decrease_only:\n        raise ValueError(\"Cannot set increase_only and decrease_only to True at the same time.\")\n\n    # Check if the waveform is too short to apply a volume change\n    if wav.shape[0] < 2:\n        return wav\n\n    # Compute the current dBFS\n    current_dBFS = 20 * np.log10(np.max(np.abs(wav)))\n\n    # Compute the dBFS change needed\n    dBFS_change = target_dBFS - current_dBFS\n\n    # If the change is negative and increase_only is True, or if the change is positive and decrease_only is True, do nothing\n    if (dBFS_change < 0 and increase_only) or (dBFS_change > 0 and decrease_only):\n        return wav\n\n    # Otherwise, scale the waveform by the change in dBFS\n    return wav * (10 ** (dBFS_change / 20))\n\n", "CodeBleu_score": 68.45413230610524}]}
{"instance_id": "2353831e9033", "generation_result": [{"code": "def load_model(weights_fpath: Path, device=None)\n\t\n    if device is None:\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(\"Initializing speaker encoder model...\")\n    global _model\n    _model = SpeakerEncoder(**encoder_params).to(device)\n    print(\"Loading weights from\", weights_fpath)\n    _model.load_state_dict(torch.load(weights_fpath, map_location=device))\n    _model.eval()\n    print(\"Loaded speaker encoder model trained at step {}\".format(_model.step))\n\n", "CodeBleu_score": 47.62896613663201}, {"code": "def load_model(weights_fpath: Path, device=None)\n\t\n    if device is None:\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Using device: {device}\")\n    print(\"Loading model...\")\n    model = SpeakerEncoder(rnn_dims=rnn_dims, fc_dims=fc_dims, dropout=dropout, pad_idx=pad_idx,\n                           n_speakers=n_speakers, speaker_embed_dim=speaker_embed_dim,\n                           tacotron2_pretrained_model=tacotron2_pretrained_model).to(device)\n    model.load_state_dict(torch.load(weights_fpath, map_location=device))\n    model.eval()\n    print(\"Loaded model: %s\" % weights_fpath)\n    print(\"Training step: %d\" % model.training_step)\n    return model\n\n", "CodeBleu_score": 45.6564691229221}, {"code": "def load_model(weights_fpath: Path, device=None)\n\t\n    if device is None:\n        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f'Initializing model on {device}...')\n    global _model\n    _model = SpeakerEncoder(speaker_encoder_params).to(device)\n    _model.load_state_dict(torch.load(weights_fpath, map_location=device))\n    _model.eval()\n    print(f'Loaded speaker encoder from {weights_fpath} (step {_model.step}).')\n\n", "CodeBleu_score": 41.31892805693001}]}
{"instance_id": "3231ad480199", "generation_result": [{"code": "def embed_frames_batch(frames_batch)\n\t\n    if _model is None:\n        raise Exception(\"Model not loaded\")\n    frames_batch = torch.from_numpy(frames_batch).to(_device)\n    with torch.no_grad():\n        embeddings = _model(frames_batch)\n        embeddings = embeddings.detach().cpu().numpy()\n    return embeddings\n\n", "CodeBleu_score": 62.12942324481067}, {"code": "def embed_frames_batch(frames_batch)\n\t\n    if _model is None:\n        raise RuntimeError(\"Model is not loaded. Please use load_model() first.\")\n    if not torch.is_tensor(frames_batch):\n        frames_batch = torch.from_numpy(frames_batch).to(_device)\n    with torch.no_grad():\n        embeddings = _model(frames_batch).detach().cpu().numpy()\n    return embeddings\n\n", "CodeBleu_score": 57.256441553482595}, {"code": "def embed_frames_batch(frames_batch)\n\t\n\n    if _model is None:\n        raise RuntimeError(\"Embedding model is not loaded. Please call load_model() first.\")\n    frames_batch = torch.from_numpy(frames_batch).to(_device)\n    with torch.no_grad():\n        embeddings = _model(frames_batch).detach().cpu().numpy()\n    return embeddings\n\n", "CodeBleu_score": 59.05745538940077}]}
{"instance_id": "117a41716c8d", "generation_result": [{"code": "def compute_partial_slices(n_samples, partial_utterance_n_frames=partials_n_frames,\n                           min_pad_coverage=0.75, overlap=0.5)\n\t\n    assert 0 <= overlap <= 1\n    assert 0 <= min_pad_coverage <= 1\n    assert partial_utterance_n_frames % 2 == 1\n    assert partial_utterance_n_frames > 0\n    assert n_samples > 0\n    assert partial_utterance_n_frames < n_samples\n\n    # Compute number of frames and frame step based on the sampling rate and overlap.\n    n_frames = int(n_samples / sampling_rate)\n    frame_step = int(sampling_rate * overlap)\n\n    # Compute wav and mel slices using a loop, considering the steps required based on the frames and frame step.\n    wav_slices = []\n    mel_slices = []\n    for i in range(0, n_frames - partial_utterance_n_frames, frame_step):\n        wav_slices.append(i)\n        mel_slices.append(i + frame_step)\n\n    # Evaluate the need for extra padding by checking the coverage of the last slice.\n    last_slice_wav_start = wav_slices[-1]\n    last_slice_wav_end = wav_slices[-1] + partial_utterance_n_frames\n    last_slice_mel_start = mel_slices[-1]\n    last_slice_mel_end = mel_slices[-1] + partial_utterance_n_frames\n    last_slice_wav_coverage = last_slice_wav_end / n_samples\n    last_slice_mel_coverage = last_slice_mel_end / n_frames\n    if last_slice_wav_coverage < min_pad_coverage and len(wav_slices) > 1:\n        wav_slices = wav_slices[:-1]\n        mel_slices = mel_slices[:-1]\n    return wav_slices, mel_slices\n\n", "CodeBleu_score": 37.39008022837903}, {"code": "def compute_partial_slices(n_samples, partial_utterance_n_frames=partials_n_frames,\n                           min_pad_coverage=0.75, overlap=0.5)\n\t\n    assert partial_utterance_n_frames > 0\n    assert min_pad_coverage >= 0 and min_pad_coverage <= 1\n    assert overlap >= 0 and overlap <= 1\n    assert n_samples > 0\n    assert partial_utterance_n_frames > 0\n    assert min_pad_coverage >= 0 and min_pad_coverage <= 1\n    assert overlap >= 0 and overlap <= 1\n    assert n_samples > 0\n    n_frames = partial_utterance_n_frames\n    frame_step = int(partial_utterance_n_frames * overlap)\n    wav_slices = []\n    mel_slices = []\n    for i in range(n_samples):\n        wav_slice = audio.frame_slice(i, n_frames, frame_step, n_samples)\n        mel_slice = audio.mel_slice(i, n_frames, frame_step, n_samples)\n        wav_slices.append(wav_slice)\n        mel_slices.append(mel_slice)\n    coverage = audio.coverage(n_frames, frame_step, n_samples)\n    if coverage < min_pad_coverage and len(wav_slices) > 1:\n        wav_slices.pop()\n        mel_slices.pop()\n    return wav_slices, mel_slices\n\n", "CodeBleu_score": 30.520709289637054}, {"code": "def compute_partial_slices(n_samples, partial_utterance_n_frames=partials_n_frames,\n                           min_pad_coverage=0.75, overlap=0.5)\n\t\n    assert 0.0 < overlap < 1.0\n    assert 0.0 < min_pad_coverage < 1.0\n    assert partial_utterance_n_frames >= 1\n\n    # Compute the number of frames and the frame step based on the sampling rate and overlap.\n    n_frames = int(n_samples / sampling_rate * (1 - overlap)) + 1\n    frame_step = int(sampling_rate * overlap)\n\n    # Compute the wav and mel slices.\n    wav_slices = []\n    mel_slices = []\n    for i in range(n_frames - partial_utterance_n_frames + 1):\n        wav_slice = slice(i * frame_step, (i + partial_utterance_n_frames) * frame_step)\n        mel_slice = slice(i * (n_frames - partial_utterance_n_frames) + partial_utterance_n_frames - 1,\n                          i * (n_frames - partial_utterance_n_frames) + n_frames)\n        wav_slices.append(wav_slice)\n        mel_slices.append(mel_slice)\n\n    # Check the coverage of the last slice.\n    last_wav_slice = wav_slices[-1]\n    last_mel_slice = mel_slices[-1]\n    last_wav_coverage = (last_wav_slice.stop - last_wav_slice.start) / n_samples\n    last_mel_coverage = (last_mel_slice.stop - last_mel_slice.start) / (n_frames - partial_utterance_n_frames)\n    if last_wav_coverage < min_pad_coverage and len(wav_slices) > 1:\n        # Remove the last slice from both wav and mel slices.\n        wav_slices = wav_slices[:-1]\n        mel_slices = mel_slices[:-1]\n\n    return wav_slices, mel_slices\n\n", "CodeBleu_score": 36.273178414276224}]}
{"instance_id": "34fe5c864f1c", "generation_result": [{"code": "def plot_embedding_as_heatmap(embed, ax=None, title=\"\", shape=None, color_range=(0, 0.30))\n\t\n    if ax is None:\n        ax = plt.gca()\n    if shape is None:\n        shape = (int(np.round(np.sqrt(embed.shape[0]))), int(np.round(np.sqrt(embed.shape[0]))))\n    embed = embed.reshape(shape)\n    cmap = cm.get_cmap('viridis')\n    ax.imshow(embed, interpolation='nearest', cmap=cmap, aspect='auto', vmin=color_range[0], vmax=color_range[1])\n    ax.xaxis.set_major_locator(plt.NullLocator())\n    ax.yaxis.set_major_locator(plt.NullLocator())\n    ax.set_title(title)\n    return ax\n\n", "CodeBleu_score": 49.56129242016946}, {"code": "def plot_embedding_as_heatmap(embed, ax=None, title=\"\", shape=None, color_range=(0, 0.30))\n\t\n    if ax is None:\n        ax = plt.gca()\n    if shape is None:\n        shape = (int(np.sqrt(embed.shape[0])), int(np.sqrt(embed.shape[0])))\n    embed = embed.reshape(shape)\n    cmap = cm.get_cmap(\"inferno\")\n    ax.imshow(embed, cmap=cmap, interpolation=\"nearest\", vmin=color_range[0], vmax=color_range[1])\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.set_title(title)\n    plt.colorbar(ticks=[color_range[0], color_range[1]])\n\n", "CodeBleu_score": 51.89986268530157}, {"code": "def plot_embedding_as_heatmap(embed, ax=None, title=\"\", shape=None, color_range=(0, 0.30))\n\t\n\n    if ax is None:\n        ax = plt.gca()\n\n    if shape is None:\n        shape = (int(np.sqrt(embed.shape[0])), int(np.sqrt(embed.shape[0])))\n\n    embed = embed.reshape(shape)\n\n    cmap = cm.get_cmap(\"viridis\")\n    cmap.set_bad(color=\"black\")\n\n    ax.imshow(embed, cmap=cmap, interpolation=\"nearest\", aspect=\"auto\", vmin=color_range[0], vmax=color_range[1])\n    ax.set_title(title)\n    ax.set_xticks([])\n    ax.set_yticks([])\n\n", "CodeBleu_score": 51.45512581980566}]}
{"instance_id": "92e93d50b62b", "generation_result": [{"code": "def similarity_matrix(self, embeds)\n\t\n    # Get the number of speakers and utterances per speaker\n    num_speakers = embeds.shape[0]\n    num_uttrs = embeds.shape[1]\n\n    # Initialize similarity matrix and mask\n    sim_mat = torch.zeros(num_speakers, num_speakers).to(embeds.device)\n    mask = torch.ones(num_speakers, num_speakers).to(embeds.device)\n\n    # Compute centroids\n    centroids_incl = embeds.mean(dim=1, keepdim=True)\n    centroids_excl = embeds.sum(dim=1, keepdim=True) / (num_uttrs - 1)\n\n    # Compute similarity scores and update similarity matrix\n    for i in range(num_speakers):\n        for j in range(num_speakers):\n            sim_mat[i, j] = torch.cosine_similarity(centroids_incl[i], centroids_excl[j])\n            mask[i, j] = 0\n\n    # Scale and bias similarity matrix\n    sim_mat = sim_mat / torch.max(sim_mat)\n    sim_mat = sim_mat * 0.5 + 0.5\n\n    return sim_mat, mask\n\n", "CodeBleu_score": 20.07223965298479}, {"code": "def similarity_matrix(self, embeds)\n\t\n\n    # Get the number of speakers and utterances per speaker\n    n_speakers = embeds.size(0)\n    n_uttrs = embeds.size(1)\n\n    # Initialize similarity matrix and mask\n    sim_matrix = torch.zeros(n_speakers, n_speakers, device=embeds.device)\n    mask = torch.ones(n_speakers, n_speakers, device=embeds.device)\n\n    # Compute centroids\n    centroids_incl = embeds.mean(dim=1)\n    centroids_excl = (embeds * mask.unsqueeze(1)).sum(dim=1) / (mask.sum(dim=1) - 1)\n\n    # Compute similarity scores\n    sim_scores_incl = torch.div(torch.sum(embeds * centroids_incl.unsqueeze(1), dim=2), self.hparams.centroid_norm_exponent)\n    sim_scores_excl = torch.div(torch.sum(embeds * centroids_excl.unsqueeze(1), dim=2), self.hparams.centroid_norm_exponent)\n\n    # Update similarity matrix\n    sim_matrix = sim_matrix.scatter(1, torch.arange(n_speakers).unsqueeze(1), sim_scores_incl)\n    sim_matrix = sim_matrix.scatter(0, torch.arange(n_speakers).unsqueeze(0), sim_scores_excl)\n\n    # Scale and bias similarity matrix\n    sim_matrix = sim_matrix.mul(self.hparams.similarity_matrix_scale).add(self.hparams.similarity_matrix_bias)\n\n    return sim_matrix\n\n", "CodeBleu_score": 23.428700494829318}, {"code": "def similarity_matrix(self, embeds)\n\t\n\n    # Determine number of speakers and utterances per speaker\n    num_speakers = embeds.size(0)\n    num_utts = embeds.size(1)\n\n    # Initialize similarity matrix and mask\n    sim_matrix = torch.zeros(num_speakers, num_speakers).to(embeds.device)\n    mask = torch.ones(num_speakers, num_speakers).to(embeds.device)\n\n    # Compute centroids\n    centroids_incl = embeds.mean(dim=1)\n    centroids_excl = embeds.mean(dim=1, keepdim=True)\n    centroids_excl[torch.arange(num_speakers), torch.arange(num_speakers)] = 0\n    centroids_excl = centroids_excl.mean(dim=1)\n\n    # Compute similarity scores\n    sim_matrix = torch.matmul(centroids_incl, centroids_incl.t())\n    sim_matrix = sim_matrix / sim_matrix.max()\n    mask = mask - torch.eye(num_speakers).to(embeds.device)\n    sim_matrix = sim_matrix * mask\n    sim_matrix = sim_matrix / sim_matrix.max()\n\n    # Scale and bias\n    sim_matrix = sim_matrix * (self.scale_embeds - 1) + 1\n    return sim_matrix\n\n", "CodeBleu_score": 20.595444385421416}]}
{"instance_id": "a6eba54108a1", "generation_result": [{"code": "def loss(self, embeds)\n\t\n\n    # Determine the number of speakers and utterances per speaker from the shape of the input embeddings\n    num_speakers = embeds.shape[0]\n    num_utts_per_speaker = embeds.shape[1]\n\n    # Compute a similarity matrix and reshape it\n    sim_matrix = torch.matmul(embeds, embeds.permute(0, 2, 1))\n    sim_matrix = sim_matrix.reshape(num_speakers, num_utts_per_speaker, num_utts_per_speaker)\n\n    # Generate ground truth labels\n    labels = torch.arange(num_speakers).repeat(num_utts_per_speaker)\n    labels = labels.reshape(num_speakers, num_utts_per_speaker)\n    labels = labels.permute(1, 0)\n\n    # Calculate loss and EER\n    loss = self.criterion(sim_matrix, labels)\n    eer, _ = self.eer(sim_matrix, labels)\n\n    return loss, eer\n\n", "CodeBleu_score": 12.228178797199684}, {"code": "def loss(self, embeds)\n\t\n\n    # Determine the number of speakers and utterances per speaker from the shape of the input embeddings\n    num_speakers = embeds.shape[0]\n    num_utterances = embeds.shape[1]\n\n    # Compute a similarity matrix between all embeddings\n    similarity_matrix = torch.matmul(embeds, embeds.transpose(1, 0))\n\n    # Reshape the similarity matrix\n    similarity_matrix = similarity_matrix.view(num_speakers, num_utterances, num_utterances)\n\n    # Generate ground truth labels\n    ground_truth_labels = torch.arange(num_utterances).unsqueeze(0).expand(num_speakers, num_utterances)\n\n    # Compute the loss\n    loss = self.criterion(similarity_matrix, ground_truth_labels)\n\n    # Compute the Equal Error Rate (EER)\n    eer = self.compute_eer(similarity_matrix, ground_truth_labels)\n\n    return loss, eer\n\n", "CodeBleu_score": 10.694569259002169}, {"code": "def loss(self, embeds)\n\t\n\n    # get the number of speakers and utterances per speaker\n    num_speakers = embeds.shape[0]\n    num_utterances = embeds.shape[1]\n\n    # compute a similarity matrix between all utterances\n    sim_mat = torch.matmul(embeds, embeds.transpose(1, 2))\n\n    # reshape the similarity matrix so that each row is a speaker's utterances\n    sim_mat = sim_mat.view(num_speakers, num_utterances, num_utterances)\n\n    # get the ground truth labels\n    labels = torch.arange(num_speakers).repeat(num_utterances).view(num_speakers, num_utterances)\n\n    # compute the loss\n    loss = self.criterion(sim_mat, labels)\n\n    # compute the EER\n    fpr, tpr, thresholds = roc_curve(labels.cpu().numpy(), sim_mat.cpu().numpy())\n    eer = brentq(lambda x: 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n\n    return loss, eer\n\n", "CodeBleu_score": 21.78991176976757}]}
{"instance_id": "ac7b74b9d1b1", "generation_result": [{"code": "def log_params(self)\n\t\n    if not self.disable_visdom:\n        params = self.params\n        params_string = ''\n        for param in params:\n            params_string += '{} : {}", "CodeBleu_score": 4.630147865554508}, {"code": "def log_params(self)\n\t\n    if self.args.disable_visdom:\n        return\n    # Log model parameters\n    model_parameters = self.model.named_parameters()\n    model_params_str = \"Model parameters:", "CodeBleu_score": 5.234036941768904}, {"code": "def log_params(self)\n\t\n    if self.params.visualization.visualize_params:\n        params_string = \"\"\n        for param_name, param_value in self.params.model.items():\n            params_string += f\"{param_name}: {param_value}", "CodeBleu_score": 6.468387094220233}]}
{"instance_id": "4726b7864386", "generation_result": [{"code": "def draw_projections(self, embeds, utterances_per_speaker, step, out_fpath=None, max_speakers=10)\n\t\n\n    # UMAP reduces the dimensionality of the embeddings, which are then plotted and displayed\n    # or saved to a file if specified\n    if self.vis is not None:\n        vis = self.vis\n    else:\n        vis = visdom.Visdom(port=self.port, env=self.env)\n        self.vis = vis\n\n    if self.vis_disabled:\n        return\n\n    if self.vis_step is None or self.vis_step < step:\n        self.vis_step = step\n        self.vis_embeddings = embeds\n        self.vis_utterances_per_speaker = utterances_per_speaker\n\n        self.vis_projections = umap.UMAP(\n            n_neighbors=self.vis_n_neighbors,\n            min_dist=self.vis_min_dist,\n            n_components=self.vis_n_components,\n            metric=self.vis_metric,\n            random_state=self.vis_random_state,\n        ).fit_transform(embeds)\n\n        # The plot title includes the current step\n        title = f\"{self.vis_title} (step {step})\"\n\n        # The visualization is updated unless disabled\n        vis.scatter(\n            X=self.vis_projections,\n            Y=list(range(len(self.vis_projections))),\n            win=self.vis_win,\n            opts=dict(\n                title=title,\n                markers=self.vis_markers,\n                legend=list(range(max_speakers)),\n                legend_names=self.vis_legend_names,\n                showlegend=True,\n                showgrid=True,\n                showline=True,\n                markercolor=self.vis_marker_colors,\n                markeropacity=0.5,\n                markerlinecolor=self.vis_marker_colors,\n                markerlinewidth=1,\n                xlabel=\"UMAP 1\",\n                ylabel=\"UMAP 2\",\n                margin=self.vis_margin,\n                width=self.vis_width,\n                height=self.vis_height,\n            ),\n", "CodeBleu_score": 30.414964032092374}, {"code": "def draw_projections(self, embeds, utterances_per_speaker, step, out_fpath=None, max_speakers=10)\n\t\n    if self.vis is None:\n        return\n    if self.vis_disabled:\n        return\n    if self.vis_projections:\n        return\n    if self.vis_projections_disabled:\n        return\n    if self.vis_projections_disabled_step > step:\n        return\n\n    if self.vis_projections_max_speakers is None:\n        max_speakers = max_speakers\n    else:\n        max_speakers = self.vis_projections_max_speakers\n\n    if self.vis_projections_max_speakers_step > step:\n        max_speakers = self.vis_projections_max_speakers\n\n    if self.vis_projections_max_speakers_step_override is not None:\n        if self.vis_projections_max_speakers_step_override > step:\n            max_speakers = self.vis_projections_max_speakers\n\n    if self.vis_projections_max_speakers_step_override_override is not None:\n        if self.vis_projections_max_speakers_step_override_override > step:\n            max_speakers = self.vis_projections_max_speakers\n\n    if self.vis_projections_max_speakers_step_override_override_override is not None:\n        if self.vis_projections_max_speakers_step_override_override_override > step:\n            max_speakers = self.vis_projections_max_speakers\n\n    if self.vis_projections_max_speakers_step_override_override_override_override is not None:\n        if self.vis_projections_max_speakers_step_override_override_override_override > step:\n            max_speakers = self.vis_projections_max_speakers\n\n    if self.vis_projections_max_speakers_step_override_override_override_override_override is not None:\n        if self.vis_projections_max_speakers_step_override_override_override_override_override > step:\n            max_speakers = self.vis_projections_max_speakers\n\n", "CodeBleu_score": 14.687885108023513}, {"code": "def draw_projections(self, embeds, utterances_per_speaker, step, out_fpath=None, max_speakers=10)\n\t\n\n    if self.vis is None:\n        self.vis = visdom.Visdom(env=self.name, port=self.port, server=self.server)\n        self.vis.close(env=self.name)\n        self.vis.text('', opts={'title': 'Speaker Embeddings', 'width': 1000})\n\n    if len(embeds) > max_speakers:\n        embeds = embeds[:max_speakers]\n        utterances_per_speaker = utterances_per_speaker[:max_speakers]\n\n    embeds = [x.flatten() for x in embeds]\n    embeds = [x for x in embeds if x.shape[0] > 0]\n    embeds = [x.reshape(1, -1) for x in embeds]\n    embeds = [x for x in embeds if x.shape[1] > 0]\n\n    if len(embeds) == 0:\n        return\n\n    embeds = np.concatenate(embeds, axis=0)\n    embeds = embeds.reshape(embeds.shape[0], -1)\n\n    if self.projections is None:\n        self.projections = embeds\n    else:\n        self.projections = np.concatenate((self.projections, embeds), axis=0)\n\n    if self.projection_colors is None:\n        self.projection_colors = []\n        for i in range(len(utterances_per_speaker)):\n            for j in range(utterances_per_speaker[i]):\n                self.projection_colors.append(i)\n\n    if self.projection_colors is None:\n        self.projection_colors = []\n        for i in range(len(utterances_per_speaker)):\n            for j in range(utterances_per_speaker[i]):\n                self.projection_colors.append(i)\n\n    if self.projection_colors is None:\n        self.projection_colors = []\n        for i in range(len(utterances_per_speaker)):\n", "CodeBleu_score": 35.64164969439613}]}
{"instance_id": "274fa50985c0", "generation_result": [{"code": "def add_speaker_embedding(self, x, speaker_embedding)\n\t\n    # save dimensions\n    batch_size = x.size(0)\n    sequence_length = x.size(1)\n    embedding_dim = x.size(2)\n    # determine speaker embedding index\n    if self.embedding_dim != self.speaker_embedding_dim:\n        speaker_embedding_index = 1\n    else:\n        speaker_embedding_index = 2\n    # repeat speaker embedding\n    speaker_embedding = speaker_embedding.repeat(batch_size, sequence_length, 1)\n    # reshape and transpose\n    speaker_embedding = speaker_embedding.view(batch_size, sequence_length, speaker_embedding_index, self.speaker_embedding_dim)\n    speaker_embedding = speaker_embedding.transpose(2, 3)\n    # concatenate\n    x = torch.cat([x, speaker_embedding], dim=2)\n    return x\n\n", "CodeBleu_score": 36.48116611080611}, {"code": "def add_speaker_embedding(self, x, speaker_embedding)\n\t\n    x_shape = x.shape\n    x_dims = x_shape[1:]\n    x_dims_names = [f\"dim_{i}\" for i in range(len(x_dims))]\n    if speaker_embedding.shape[0] == 1:\n        speaker_embedding = speaker_embedding.repeat(x_shape[0], 1)\n    speaker_embedding = speaker_embedding.view(x_shape[0], *speaker_embedding.shape[1:])\n    speaker_embedding = speaker_embedding.transpose(-1, -2)\n    speaker_embedding = speaker_embedding.reshape(x_shape[0], *x_dims, *speaker_embedding.shape[1:])\n    return torch.cat([x, speaker_embedding], dim=-1)\n\n", "CodeBleu_score": 19.35796417988632}, {"code": "def add_speaker_embedding(self, x, speaker_embedding)\n\t\n    # save the dimensions of the input tensor as human-readable names\n    dim_names = [\n        \"batch\",\n        \"seq_len\",\n        \"feature_dim\",\n        \"channel_dim\",\n        \"speaker_dim\",\n        \"time_dim\",\n        \"mel_dim\",\n    ]\n    dim_sizes = [\n        x.size(0),\n        x.size(1),\n        x.size(2),\n        x.size(3),\n        x.size(4),\n        x.size(5),\n        x.size(6),\n    ]\n    # determine the appropriate dimension index for the speaker embedding\n    speaker_dim_index = None\n    for i, dim_size in enumerate(dim_sizes):\n        if dim_size == 1:\n            speaker_dim_index = i\n    if speaker_dim_index is None:\n        raise ValueError(\"No speaker dimension found in the input tensor.\")\n    # repeat the speaker embedding to match the length of the input text\n    speaker_embedding = speaker_embedding.repeat(\n        dim_sizes[0], dim_sizes[1], 1\n    )\n    # reshape and transpose the speaker embedding\n    speaker_embedding = speaker_embedding.reshape(\n        dim_sizes[0], dim_sizes[1], -1\n    ).transpose(2, 1)\n    # concatenate the speaker embedding with the input tensor along the last dimension\n    return torch.cat(\n        (x, speaker_embedding), dim=speaker_dim_index + 1\n    )\n\n", "CodeBleu_score": 23.760069572749128}]}
{"instance_id": "e6742ea0fe5e", "generation_result": [{"code": "def init_attention(self, encoder_seq_proj)\n\t\n    device = next(self.parameters()).device\n    batch_size, seq_len, _ = encoder_seq_proj.shape\n    self.cum_attn = torch.zeros(batch_size, seq_len, device=device)\n    self.attn = torch.zeros(batch_size, seq_len, device=device)\n\n", "CodeBleu_score": 23.755808657513864}, {"code": "def init_attention(self, encoder_seq_proj)\n\t\n\n    self.device = next(self.parameters()).device\n    encoder_seq_proj = encoder_seq_proj.to(self.device)\n    self.cumulative_attention = torch.zeros(\n        encoder_seq_proj.size(0), encoder_seq_proj.size(1), device=self.device\n    )\n    self.attention = torch.zeros(\n        encoder_seq_proj.size(0), encoder_seq_proj.size(1), device=self.device\n    )\n\n", "CodeBleu_score": 23.962452313686057}, {"code": "def init_attention(self, encoder_seq_proj)\n\t\n\n    device = next(self.parameters()).device\n    batch_size, seq_len, _ = encoder_seq_proj.shape\n    self.cum_att = torch.zeros(batch_size, seq_len, device=device)\n    self.att = torch.zeros(batch_size, seq_len, device=device)\n\n", "CodeBleu_score": 23.822761082162977}]}
{"instance_id": "4e1fc1ed0ed7", "generation_result": [{"code": "def load(self, path, optimizer=None)\n\t\n    # TODO: add support for optimizer\n    # TODO: add support for model\n    # TODO: add support for scheduler\n    # TODO: add support for amp\n    # TODO: add support for scaler\n    # TODO: add support for amp scaler\n    # TODO: add support for amp loss scaler\n    # TODO: add support for amp loss scaler\n    # TODO: add support for amp loss scaler\n    # TODO: add support for amp loss scaler\n    # TODO: add support for amp loss scaler\n    # TODO: add support for amp loss scaler\n    # TODO: add support for amp loss scaler\n    # TODO: add support for amp loss scaler\n    # TODO: add support for amp loss scaler\n    # TODO: add support for amp loss scaler\n    # TODO: add support for amp loss scaler\n    # TODO: add support for amp loss scaler\n    # TODO: add support for amp loss scaler\n    # TODO: add support for amp loss scaler\n    # TODO: add support for amp loss scaler\n    # TODO: add support for amp loss scaler\n    # TODO: add support for amp loss scaler\n    # TODO: add support for amp loss scaler\n    # TODO: add support for amp loss scaler\n    # TODO: add support for amp loss scaler\n    # TODO: add support for amp loss scaler\n    # TODO: add support for amp loss scaler\n    # TODO: add support for amp loss scaler\n    # TODO: add support for amp loss scaler\n    # TODO: add support for amp loss scaler\n    # TODO: add support for amp loss scaler\n    # TODO: add support for amp loss scaler\n    # TODO: add support for amp loss scaler\n    # TODO: add support for amp loss scaler\n    # TODO: add support for amp loss scaler\n    # TODO: add support for amp loss scaler\n    # TODO: add support for amp loss scaler\n    # TODO: add support for amp loss scaler\n    # TODO: add support for amp loss scaler\n    # TODO: add support for amp loss scaler\n    # TODO: add support for amp loss scaler\n    # TODO: add support for amp loss scaler\n    # TODO: add support for amp", "CodeBleu_score": 4.223227752639517}, {"code": "def load(self, path, optimizer=None)\n\t\n    state = torch.load(path, map_location=self.device)\n    self.model.load_state_dict(state['model'])\n    if optimizer is not None:\n        optimizer.load_state_dict(state['optimizer'])\n    self.start_epoch = state['epoch']\n    self.best_loss = state['best_loss']\n    self.best_epoch = state['best_epoch']\n    self.best_val_loss = state['best_val_loss']\n    self.best_val_epoch = state['best_val_epoch']\n\n", "CodeBleu_score": 28.683088102062342}, {"code": "def load(self, path, optimizer=None)\n\t\n    if not os.path.exists(path):\n        raise FileNotFoundError(f\"No file or directory named '{path}'\")\n    if os.path.isdir(path):\n        path = os.path.join(path, \"checkpoint.pth\")\n    if not os.path.exists(path):\n        raise FileNotFoundError(f\"No file or directory named '{path}'\")\n    checkpoint = torch.load(path, map_location=self.device)\n    self.model.load_state_dict(checkpoint[\"model\"])\n    if optimizer is not None:\n        optimizer.load_state_dict(checkpoint[\"optimizer\"])\n    if \"epoch\" in checkpoint:\n        self.epoch = checkpoint[\"epoch\"]\n\n", "CodeBleu_score": 50.048472424901604}]}
{"instance_id": "178e6aa16c0f", "generation_result": [{"code": "def save(self, path, optimizer=None)\n\t\n    if not isinstance(path, Path):\n        path = Path(path)\n\n    if not path.parent.exists():\n        os.makedirs(path.parent)\n\n    state = {\n        \"model_state\": self.state_dict(),\n        \"optimizer_state\": optimizer.state_dict() if optimizer is not None else None\n    }\n\n    torch.save(state, path)\n\n", "CodeBleu_score": 57.513608081486254}, {"code": "def save(self, path, optimizer=None)\n\t\n    if not os.path.exists(path):\n        os.makedirs(path)\n    model_path = os.path.join(path, \"model.pth\")\n    torch.save(self.state_dict(), model_path)\n    if optimizer is not None:\n        optimizer_path = os.path.join(path, \"optimizer.pth\")\n        torch.save(optimizer.state_dict(), optimizer_path)\n\n", "CodeBleu_score": 56.75068755996795}, {"code": "def save(self, path, optimizer=None)\n\t\n    if isinstance(path, str):\n        path = Path(path)\n    if not path.parent.exists():\n        os.makedirs(path.parent)\n    torch.save(self.state_dict(), path)\n    if optimizer is not None:\n        torch.save(optimizer.state_dict(), path.parent / 'optimizer.pth')\n\n", "CodeBleu_score": 54.46448581588108}]}
{"instance_id": "6528fd80e87b", "generation_result": [{"code": "def english_cleaners(text)\n\t\n    text = unidecode(text)\n    text = re.sub(r\"[^a-zA-Z0-9(),!?\\'\\`]\", \" \", text)\n    text = re.sub(r\"\\'s\", \" \\'s\", text)\n    text = re.sub(r\"\\'ve\", \" \\'ve\", text)\n    text = re.sub(r\"n\\'t\", \" n\\'t\", text)\n    text = re.sub(r\"\\'re\", \" \\'re\", text)\n    text = re.sub(r\"\\'d\", \" \\'d\", text)\n    text = re.sub(r\"\\'ll\", \" \\'ll\", text)\n    text = re.sub(r\",\", \" , \", text)\n    text = re.sub(r\"!\", \" ! \", text)\n    text = re.sub(r\"\\(\", \" \\( \", text)\n    text = re.sub(r\"\\)\", \" \\) \", text)\n    text = re.sub(r\"\\?\", \" \\? \", text)\n    text = re.sub(r\"\\s{2,}\", \" \", text)\n    return text\n\n", "CodeBleu_score": 20.9379679011187}, {"code": "def english_cleaners(text)\n\t\n    text = unidecode(text)\n    text = re.sub(r\"([.!?])\", r\" \\1\", text)\n    text = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", text)\n    text = re.sub(r\"what's\", r\"what is \", text)\n    text = re.sub(r\"\\'s\", r\" \", text)\n    text = re.sub(r\"\\'ve\", r\" have \", text)\n    text = re.sub(r\"can't\", r\"cannot \", text)\n    text = re.sub(r\"n't\", r\" not \", text)\n    text = re.sub(r\"i'm\", r\"i am \", text)\n    text = re.sub(r\"\\'re\", r\" are \", text)\n    text = re.sub(r\"\\'d\", r\" would \", text)\n    text = re.sub(r\"\\'ll\", r\" will \", text)\n    text = re.sub(r\",\", r\" , \", text)\n    text = re.sub(r\"\\.\", r\" . \", text)\n    text = re.sub(r\"!\", r\" ! \", text)\n    text = re.sub(r\"\\(\", r\" ( \", text)\n    text = re.sub(r\"\\)\", r\" ) \", text)\n    text = re.sub(r\"\\?\", r\" ? \", text)\n    text = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", text)\n    text = re.sub(r\"\\. \", r\".\", text)\n    text = re.sub(r\" !\", r\"!\", text)\n    text = re.sub(r\" ?\", r\" \", text)\n    text = re.sub(r\" ,\", r\",\", text)\n    text = re.sub(r\" -\", r\" \", text)\n    text = re.sub(r\"  \", r\" \", text)\n    text = normalize_numbers(text)\n    return text\n\n", "CodeBleu_score": 40.09774733687253}, {"code": "def english_cleaners(text)\n\t\n    text = unidecode(text)\n    text = re.sub(r\"([,.!?])\", r\" \\1 \", text)\n    text = re.sub(r\"[^a-zA-Z,.!?]+\", r\" \", text)\n    text = re.sub(r\"what's\", r\"what is \", text)\n    text = re.sub(r\"\\'s\", r\" \", text)\n    text = re.sub(r\"\\'ve\", r\" have \", text)\n    text = re.sub(r\"can't\", r\"cannot \", text)\n    text = re.sub(r\"n't\", r\" not \", text)\n    text = re.sub(r\"i'm\", r\"i am \", text)\n    text = re.sub(r\"\\'re\", r\" are \", text)\n    text = re.sub(r\"\\'d\", r\" would \", text)\n    text = re.sub(r\"\\'ll\", r\" will \", text)\n    text = re.sub(r\"\\'scuse\", r\" excuse \", text)\n    text = re.sub(r\"\\'s\", r\" \", text)\n    text = re.sub(r\"\\'ve\", r\" have \", text)\n    text = re.sub(r\"n't\", r\" not \", text)\n    text = re.sub(r\"i'm\", r\"i am \", text)\n    text = re.sub(r\"\\'re\", r\" are \", text)\n    text = re.sub(r\"\\'d\", r\" would \", text)\n    text = re.sub(r\"\\'ll\", r\" will \", text)\n    text = re.sub(r\"\\'s\", r\" \", text)\n    text = re.sub(r\"\\'ve\", r\" have \", text)\n    text = re.sub(r\"n't\", r\" not \", text)\n    text = re.sub(r\"i'm\", r\"i am \", text)\n    text = re.sub(r\"\\'re\", r\" are \", text)\n    text = re.sub(r\"\\'d\", r\" would \", text)\n    text =", "CodeBleu_score": 17.550017820809806}]}
{"instance_id": "8a4fe08e0985", "generation_result": [{"code": "def text_to_sequence(text, cleaner_names)\n\t\n    sequence = []\n    text = re.sub(r\"\\{[^\\}]*\\}\", \"\", text)\n    phones = text_to_phonemes(text, cleaner_names)\n    for symbol in phones:\n        if symbol in symbols:\n            sequence += symbols.index(symbol)\n        else:\n            sequence += [symbols.index(\"<unk>\")]\n    sequence += [symbols.index(\"<eos>\")]\n    return sequence\n\n", "CodeBleu_score": 24.610786952098184}, {"code": "def text_to_sequence(text, cleaner_names)\n\t\n    sequence = []\n    for char in text:\n        if char == ' ':\n            sequence += [symbols.index('sp')]\n        elif char == '{' and text[text.index(char)+1] == '{':\n            sequence += [symbols.index('{')]\n            sequence += text_to_sequence(text[text.index(char)+2:text.index('}')], cleaner_names)\n            sequence += [symbols.index('}')]\n        else:\n            sequence += [symbols.index(char)]\n    sequence += [symbols.index('eos')]\n    return sequence\n\n", "CodeBleu_score": 30.84358191770069}, {"code": "def text_to_sequence(text, cleaner_names)\n\t\n    sequence = []\n    for char in text:\n        if char == '{':\n            sequence.extend(arpabet_to_sequence(text, cleaner_names))\n            break\n        sequence.append(symbols.index(char) if char in symbols else symbols.index(' '))\n    sequence.append(symbols.index('eos'))\n    return sequence\n\n", "CodeBleu_score": 25.876616630824135}]}
{"instance_id": "e1c25a37af1f", "generation_result": [{"code": "def sequence_to_text(sequence)\n\t\n    result = ''\n    for symbol_id in sequence:\n        if symbol_id in symbols:\n            s = symbols[symbol_id]\n            # Enclose ARPAbet phonemes in curly braces:\n            if len(s) > 1 and s[0] == '@':\n                s = '{%s}' % s[1:]\n            result += s\n    return result.replace('}{', ' ')\n\n", "CodeBleu_score": 66.54401148873038}, {"code": "def sequence_to_text(sequence)\n\t\n    result = ''\n    for symbol_id in sequence:\n        if symbol_id in symbols:\n            s = symbols[symbol_id]\n            # Enclose ARPAbet back in curly braces:\n            if len(s) > 1 and s[0] == '@':\n                s = '{%s}' % s[1:]\n            result += s\n    result = result.replace('}{', ' ')\n    return result.strip()\n\n", "CodeBleu_score": 64.14430667255154}, {"code": "def sequence_to_text(sequence)\n\t\n    result = ''\n    for symbol_id in sequence:\n        if symbol_id in symbols:\n            s = symbols[symbol_id]\n            # Enclose ARPAbet back in curly braces:\n            if len(s) > 1 and s[0] == '@':\n                s = '{%s}' % s[1:]\n            result += s\n    return result.replace('}{', ' ')\n\n", "CodeBleu_score": 66.54401148873038}]}
{"instance_id": "7807cad4b05d", "generation_result": [{"code": "def embed_utterance(fpaths, encoder_model_fpath)\n\t\n    if not encoder.is_loaded():\n        encoder.load_model(encoder_model_fpath)\n    wav = audio.load_wav(fpath)\n    embedding = encoder.embed_utterance(wav)\n    return embedding\n\n", "CodeBleu_score": 43.73384692487047}, {"code": "def embed_utterance(fpaths, encoder_model_fpath)\n\t\n    # Load the encoder\n    if not encoder.is_loaded():\n        encoder.load_model(encoder_model_fpath)\n\n    # Compute the speaker embedding\n    embed = encoder.embed_utterance(fpaths)\n    return embed\n\n", "CodeBleu_score": 35.899687553221874}, {"code": "def embed_utterance(fpaths, encoder_model_fpath)\n\t\n    # Load the encoder\n    if not encoder.is_loaded():\n        encoder.load_model(encoder_model_fpath)\n    # Preprocess the waveform\n    waveform, _ = librosa.load(fpaths, sr=encoder.sample_rate)\n    preprocessed_waveform = encoder.preprocess_wav(waveform)\n    # Compute the embedding\n    embedding = encoder.embed_utterance(preprocessed_waveform)\n    return embedding\n\n", "CodeBleu_score": 51.788048965689505}]}
{"instance_id": "9f860273a325", "generation_result": [{"code": "def create_embeddings(synthesizer_root: Path, encoder_model_fpath: Path, n_processes: int)\n\t\n    # Checking the existence of the required directories and files\n    if not synthesizer_root.exists():\n        raise RuntimeError(f\"Synthesizer root directory {synthesizer_root} does not exist.\")\n    if not encoder_model_fpath.exists():\n        raise RuntimeError(f\"Encoder model file {encoder_model_fpath} does not exist.\")\n    if not (synthesizer_root / \"metadata.csv\").exists():\n        raise RuntimeError(f\"Metadata file {synthesizer_root / 'metadata.csv'} does not exist.\")\n    if not (synthesizer_root / \"audio\").exists():\n        raise RuntimeError(f\"Audio directory {synthesizer_root / 'audio'} does not exist.\")\n\n    # Reading metadata and preparing file paths\n    metadata_df = pd.read_csv(synthesizer_root / \"metadata.csv\")\n    audio_paths = list(chain.from_iterable(metadata_df[\"audio_path\"].values))\n    embeddings_paths = list(chain.from_iterable(metadata_df[\"embedding_path\"].values))\n    embeddings_paths = [str(Path(embeddings_path).parent) for embeddings_path in embeddings_paths]\n\n    # Creating the output directory if it does not exist\n    if not (synthesizer_root / \"embeddings\").exists():\n        (synthesizer_root / \"embeddings\").mkdir()\n\n    # Creating the embeddings directory if it does not exist\n    if not all(Path(embeddings_path).exists() for embeddings_path in embeddings_paths):\n        for embeddings_path in embeddings_paths:\n            (Path(embeddings_path) / \"embeddings\").mkdir()\n\n    # Creating the embeddings file if it does not exist\n    if not all(Path(embeddings_path).exists() for embeddings_path in embeddings_paths):\n        for embeddings_path in embeddings_paths:\n            (Path(embeddings_path) / \"embeddings.csv\").touch()\n\n    # Creating the embeddings file if it", "CodeBleu_score": 28.273878473453166}, {"code": "def create_embeddings(synthesizer_root: Path, encoder_model_fpath: Path, n_processes: int)\n\t\n\n    # Check if the synthesizer root directory exists\n    if not synthesizer_root.exists():\n        raise ValueError(f\"The synthesizer root directory does not exist: {synthesizer_root}\")\n\n    # Check if the encoder model file path exists\n    if not encoder_model_fpath.exists():\n        raise ValueError(f\"The encoder model file path does not exist: {encoder_model_fpath}\")\n\n    # Check if the audio directory exists\n    audio_dir = synthesizer_root / \"audio\"\n    if not audio_dir.exists():\n        raise ValueError(f\"The audio directory does not exist: {audio_dir}\")\n\n    # Check if the metadata file exists\n    metadata_fpath = synthesizer_root / \"metadata.csv\"\n    if not metadata_fpath.exists():\n        raise ValueError(f\"The metadata file does not exist: {metadata_fpath}\")\n\n    # Check if the embeddings directory exists\n    embeddings_dir = synthesizer_root / \"embeddings\"\n    if not embeddings_dir.exists():\n        embeddings_dir.mkdir(parents=True, exist_ok=True)\n\n    # Check if the embeddings directory is empty\n    if list(embeddings_dir.glob(\"*.npy\")):\n        raise ValueError(f\"The embeddings directory is not empty: {embeddings_dir}\")\n\n    # Get the list of audio files\n    audio_fpaths = list(audio_dir.glob(\"*.wav\"))\n    if not audio_fpaths:\n        raise ValueError(f\"No audio files found in the audio directory: {audio_dir}\")\n\n    # Get the list of metadata\n    metadata = list(csv.DictReader(open(metadata_fpath)))\n    if not metadata:\n        raise ValueError(f\"No metadata found in the metadata file: {metadata_fpath}\")\n\n    # Get the list of metadata file paths\n    metadata_fpaths = [str(fpath) for fpath in audio_fpaths]\n\n    # Get the list of embeddings file paths\n    embeddings_fpaths = [str(", "CodeBleu_score": 32.533885956735645}, {"code": "def create_embeddings(synthesizer_root: Path, encoder_model_fpath: Path, n_processes: int)\n\t\n\n    # Check if the synthesizer root directory exists and is not empty\n    if not synthesizer_root.exists() or not synthesizer_root.is_dir():\n        raise ValueError(\"The synthesizer root directory does not exist or is empty.\")\n\n    # Check if the encoder model file exists\n    if not encoder_model_fpath.exists():\n        raise ValueError(\"The encoder model file does not exist.\")\n\n    # Check if the required directories exist\n    if not (synthesizer_root / \"audio\").exists():\n        raise ValueError(\"The audio directory does not exist.\")\n    if not (synthesizer_root / \"embeds\").exists():\n        raise ValueError(\"The embeds directory does not exist.\")\n\n    # Check if the required files exist\n    if not (synthesizer_root / \"audio\" / \"train.txt\").exists():\n        raise ValueError(\"The train.txt file does not exist.\")\n    if not (synthesizer_root / \"audio\" / \"test.txt\").exists():\n        raise ValueError(\"The test.txt file does not exist.\")\n\n    # Get the list of audio files to process\n    audio_files = list(chain(\n        (synthesizer_root / \"audio\" / \"train.txt\").open().read().splitlines(),\n        (synthesizer_root / \"audio\" / \"test.txt\").open().read().splitlines()\n    ))\n\n    # Create the embeds directory if it does not exist\n    (synthesizer_root / \"embeds\").mkdir(exist_ok=True)\n\n    # Create the embeds/train.txt and embeds/test.txt files if they do not exist\n    (synthesizer_root / \"embeds\" / \"train.txt\").touch(exist_ok=True)\n    (synthesizer_root / \"embeds\" / \"test.txt\").touch(exist_ok=True)\n\n    # Create the embeds/train and embeds/test subdirectories if they do not exist\n    (synthesizer_root / \"embeds\" / \"train\").mkdir(exist_ok=True)\n    (synthesizer_root / \"embeds\" / \"test\").mk", "CodeBleu_score": 25.69455548563408}]}
{"instance_id": "9eb0abda14a9", "generation_result": [{"code": "def eval_model(attention, mel_prediction, target_spectrogram, input_seq, step,\n               plot_dir, mel_output_dir, wav_dir, sample_num, loss, hparams)\n\t\n    # Create directories for outputs if they don't exist\n    Path(plot_dir).mkdir(exist_ok=True)\n    Path(mel_output_dir).mkdir(exist_ok=True)\n    Path(wav_dir).mkdir(exist_ok=True)\n\n    # Save the attention plot\n    plot_attention(attention, step, plot_dir)\n\n    # Save predicted mel spectrogram to disk (control frequencies)\n    mel_filename = 'mel-prediction-step-{}.npy'.format(step)\n    mel_prediction.detach().cpu().numpy().astype(np.float32).tofile(mel_output_dir + '/' + mel_filename)\n\n    # Convert mel spectrogram to audio and save it to disk (waveform)\n    wav_prediction = audio.inv_mel_spectrogram(mel_prediction.cpu().numpy(), hparams)\n    wav_prediction = audio.inv_preemphasis(wav_prediction, hparams)\n    audio.save_wav(wav_prediction, wav_dir + '/step-{}-wave-prediction.wav'.format(step), hparams)\n\n    # Save griffin lim inverted waveform using predicted linear spectrogram\n    wav_prediction = audio.inv_mel_spectrogram(mel_prediction.cpu().numpy(), hparams)\n    audio.save_wav(wav_prediction, wav_dir + '/step-{}-wave-griffin-lim.wav'.format(step), hparams)\n\n    # Save target linear spectrogram to disk (debug)\n    target_spectrogram.detach().cpu().numpy().astype(np.float32).tofile(mel_output_dir + '/target-spectrogram-step-{}.npy'.format(step))\n\n    # Generate plots of the target and predicted mel spectrograms\n    plot_spectrogram(target_spectrogram.cpu().numpy(), step, mel_output_dir, 'Target Mel Spectrogram',\n                     f'{step:06d}-target-mel-spectrogram')", "CodeBleu_score": 37.260222327895576}, {"code": "def eval_model(attention, mel_prediction, target_spectrogram, input_seq, step,\n               plot_dir, mel_output_dir, wav_dir, sample_num, loss, hparams)\n\t\n    # Get the attention plot.\n    attention_plot = attention.cpu().data.numpy()\n    # Get the mel spectrogram plot.\n    mel_output = mel_prediction.cpu().data.numpy()\n    # Get the stop token.\n    stop_token = 1 if target_spectrogram.data.numpy()[0][-1] == 1 else 0\n    # Get the predicted audio.\n    audio_prediction = audio.inv_mel_spectrogram(mel_output.T, hparams)\n    # Get the predicted audio waveform.\n    audio_prediction = audio_prediction / np.max(np.abs(audio_prediction))\n    # Get the target audio waveform.\n    audio_target = audio.inv_mel_spectrogram(target_spectrogram.cpu().data.numpy().T, hparams)\n    audio_target = audio_target / np.max(np.abs(audio_target))\n    # Save the attention plot.\n    plot_attention(attention_plot, step, plot_dir, hparams.attention_rounded_size)\n    # Save the predicted mel spectrogram plot.\n    plot_spectrogram(mel_output, step, mel_output_dir, hparams.mel_rounded_size)\n    # Save the predicted audio.\n    save_audio(audio_prediction, step, wav_dir)\n    # Save the target audio.\n    save_audio(audio_target, step, wav_dir)\n    # Save the predicted audio waveform.\n    save_audio(audio_prediction, step, wav_dir, inv=True)\n    # Save the target audio waveform.\n    save_audio(audio_target, step, wav_dir, inv=True)\n    # Save the target spectrogram plot.\n    plot_spectrogram(target_spectrogram.cpu().data.numpy(), step, mel_output_dir, hparams.mel_rounded_size)\n    # Save the target and predicted melspectrogram plots together.\n    plot_spectrogram(target_spectrogram.cpu().data.numpy(), step, mel_output_dir, hparams.", "CodeBleu_score": 33.191450990166146}, {"code": "def eval_model(attention, mel_prediction, target_spectrogram, input_seq, step,\n               plot_dir, mel_output_dir, wav_dir, sample_num, loss, hparams)\n\t\n\n    # Create directories for outputs if they don't exist\n    Path(plot_dir).mkdir(parents=True, exist_ok=True)\n    Path(mel_output_dir).mkdir(parents=True, exist_ok=True)\n    Path(wav_dir).mkdir(parents=True, exist_ok=True)\n\n    # Generate plots and save them to files\n    plot_spectrogram(mel_prediction, os.path.join(plot_dir, \"mel-prediction-step-{}.png\".format(step)),\n                     title=\"{}, step={}, loss={:.5f}\".format(attention, step, loss), target_spectrogram=target_spectrogram,\n                     max_len=hparams.max_decoder_steps)\n    plot_spectrogram(target_spectrogram, os.path.join(plot_dir, \"mel-target-step-{}.png\".format(step)),\n                     title=\"{}, step={}, loss={:.5f}\".format(attention, step, loss), max_len=hparams.max_decoder_steps)\n\n    # Save the predicted and target mel spectrograms as numpy arrays\n    np.save(os.path.join(mel_output_dir, \"mel-prediction-step-{}.npy\".format(step)), mel_prediction.cpu().numpy(), allow_pickle=False)\n    np.save(os.path.join(mel_output_dir, \"mel-target-step-{}.npy\".format(step)), target_spectrogram.cpu().numpy(), allow_pickle=False)\n\n    # Reconstruct the waveform from the predicted mel spectrogram\n    if hparams.use_lws:\n        # Forward filter the mel spectrogram using the Liquid Warp algorithm\n        reconstructed_wav = audio.inv_lws(mel_prediction.cpu().numpy(), hparams)\n    else:\n        # Reconstruct the waveform using Griffin-Lim\n        reconstructed_wav = audio.inv_mel_spectrogram(mel_prediction.cpu().numpy(), hparams)\n\n", "CodeBleu_score": 36.20466871077219}]}
{"instance_id": "6bad4f500286", "generation_result": [{"code": "def num_params(self)\n\t\n    params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n    print(f'Number of trainable parameters: {params / 1000000:.3f}M')\n\n", "CodeBleu_score": 14.416301110541246}, {"code": "def num_params(self)\n\t\n    num_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n    print(f'{num_params / 1000000:.3f} M')\n\n", "CodeBleu_score": 11.17306836437332}, {"code": "def num_params(self)\n\t\n    num_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n    num_params = num_params / 1000000\n    print(f\"Number of trainable parameters: {num_params:.3f}M\")\n\n", "CodeBleu_score": 17.250511297129506}]}
{"instance_id": "78b876054bd4", "generation_result": [{"code": "def pad_tensor(self, x, pad, side='both')\n\t\n    # Determine the new size of the tensor after padding\n    if side == 'before':\n        x_pad = x.size(0) + 2 * pad\n    elif side == 'after':\n        x_pad = x.size(0) + 2 * pad\n    elif side == 'both':\n        x_pad = x.size(0) + pad\n    else:\n        raise ValueError('Incorrect padding side entered.')\n\n    # Create a new tensor filled with zeros on either the CPU or GPU, depending on availability\n    if x.is_cuda:\n        x_pad = torch.cuda.FloatTensor(x_pad, x.size(1)).zero_()\n    else:\n        x_pad = torch.zeros(x_pad, x.size(1))\n\n    # Place the original tensor into the new padded tensor according to the specified side\n    if side == 'before':\n        x_pad = x_pad[pad:pad + x.size(0)]\n        x_pad[:, :] = x\n    elif side == 'after':\n        x_pad = x_pad[pad - x.size(0):pad]\n        x_pad[:, :] = x\n    elif side == 'both':\n        x_pad = x_pad[pad:pad + x.size(0)]\n        x_pad[pad - x.size(0):pad - x.size(0) + x.size(0), :] = x\n    else:\n        raise ValueError('Incorrect padding side entered.')\n\n    return x_pad\n\n", "CodeBleu_score": 35.56216009024196}, {"code": "def pad_tensor(self, x, pad, side='both')\n\t\n    assert side in ['before', 'after', 'both']\n    if x.is_cuda:\n        pad = pad.cuda()\n    total_length = x.size(1) + pad.size(1)\n    if side == 'before':\n        padded = torch.cat([pad, x], dim=1)\n    elif side == 'after':\n        padded = torch.cat([x, pad], dim=1)\n    elif side == 'both':\n        padded = torch.cat([pad, x, pad], dim=1)\n    padded = padded.data.new(x.size(0), total_length).fill_(0).type(x.type())\n    padded[:, :x.size(1)] = x\n    return padded\n\n", "CodeBleu_score": 36.13418896996671}, {"code": "def pad_tensor(self, x, pad, side='both')\n\t\n    assert side in ['before', 'after', 'both']\n    if side in ['before', 'both']:\n        pad_before = pad\n    else:\n        pad_before = 0\n    if side in ['after', 'both']:\n        pad_after = pad\n    else:\n        pad_after = 0\n    total_len = x.size(1) + pad_before + pad_after\n    new_tensor = torch.zeros(x.size(0), total_len, device=x.device, dtype=x.dtype)\n    new_tensor[:, pad_before:pad_before + x.size(1)] = x\n    return new_tensor\n\n", "CodeBleu_score": 27.466208586525315}]}
{"instance_id": "a7ed1715a3df", "generation_result": [{"code": "def save_attention(attn, path)\n\t\n    import matplotlib.pyplot as plt\n    import matplotlib.image as mpimg\n    fig = plt.figure(figsize=(20, 20))\n    plt.imshow(attn, interpolation=\"nearest\")\n    plt.savefig(path + \".png\")\n    plt.close(fig)\n\n", "CodeBleu_score": 62.09835914577133}, {"code": "def save_attention(attn, path)\n\t\n    import matplotlib.pyplot as plt\n    fig = plt.figure(figsize=(10, 10))\n    plt.imshow(attn, interpolation='nearest')\n    plt.savefig(path, dpi=100)\n    plt.close(fig)\n\n", "CodeBleu_score": 48.32775880247423}, {"code": "def save_attention(attn, path)\n\t\n\n    import matplotlib.pyplot as plt\n    import matplotlib.image as mpimg\n    import numpy as np\n\n    fig = plt.figure(figsize=(10, 10))\n    ax = fig.add_subplot(1, 1, 1)\n    ax.set_title('Attention Matrix', fontsize=20)\n    ax.set_xlabel('Encoder timestep', fontsize=15)\n    ax.set_ylabel('Decoder timestep', fontsize=15)\n    ax.set_xticks(np.arange(0, attn.shape[1], 1))\n    ax.set_yticks(np.arange(0, attn.shape[0], 1))\n    ax.set_xticklabels(np.arange(0, attn.shape[1], 1), fontsize=15)\n    ax.set_yticklabels(np.arange(0, attn.shape[0], 1), fontsize=15)\n    im = ax.imshow(attn, aspect='auto', cmap='viridis')\n    fig.colorbar(im, ax=ax)\n    plt.savefig(path)\n    plt.close()\n\n", "CodeBleu_score": 49.98575018931225}]}
{"instance_id": "19375c9f44a2", "generation_result": [{"code": "def plot(array)\n\t\n    # Create a figure with a specified size\n    fig = plt.figure(figsize=(10, 5))\n    # Add a subplot to the figure\n    ax = fig.add_subplot(111)\n    # Customize the axis labels and tick parameters with specific colors and font sizes\n    ax.set_xlabel('x', color='red', fontsize=16)\n    ax.set_ylabel('y', color='green', fontsize=16)\n    ax.tick_params(axis='both', colors='blue', labelsize=16)\n    # Plot the array on the customized subplot\n    plt.plot(array)\n\n", "CodeBleu_score": 24.37219877540288}, {"code": "def plot(array)\n\t\n    import matplotlib.pyplot as plt\n    fig = plt.figure(figsize=(10, 5))\n    ax = fig.add_subplot(111)\n    ax.set_title(\"Evolution of the system\", fontsize=16)\n    ax.set_xlabel(\"Time\", fontsize=16)\n    ax.set_ylabel(\"Temperature\", fontsize=16)\n    ax.tick_params(axis='both', which='major', labelsize=16)\n    ax.tick_params(axis='both', which='minor', labelsize=16)\n    ax.tick_params(axis='both', which='both', labelsize=16)\n    plt.plot(array, color='black', linewidth=2)\n    plt.show()\n\n", "CodeBleu_score": 39.57069383510715}, {"code": "def plot(array)\n\t\n    import matplotlib.pyplot as plt\n    fig = plt.figure(figsize=(10, 10))\n    ax = fig.add_subplot(111)\n    ax.set_xlabel('x', fontsize=14, color='blue')\n    ax.set_ylabel('y', fontsize=14, color='blue')\n    ax.tick_params(axis='both', which='major', labelsize=14, color='blue')\n    ax.tick_params(axis='both', which='minor', labelsize=14, color='blue')\n    plt.plot(array)\n    plt.show()\n\n", "CodeBleu_score": 35.091194875439946}]}
{"instance_id": "ff94c425920c", "generation_result": [{"code": "def write_to_csv(image_name, prediction, confidence)\n\t\n    data = {'image_name': image_name, 'prediction': prediction, 'confidence': confidence}\n    with open(os.path.join(args.project, 'predictions.csv'), 'a') as csv_file:\n        writer = csv.DictWriter(csv_file, fieldnames=['image_name', 'prediction', 'confidence'])\n        if os.stat(os.path.join(args.project, 'predictions.csv')).st_size == 0:\n            writer.writeheader()\n        writer.writerow(data)\n\n", "CodeBleu_score": 49.200283818272425}, {"code": "def write_to_csv(image_name, prediction, confidence)\n\t\n    # Create a dictionary with the image name, prediction, and confidence\n    data = {'image_name': image_name, 'prediction': prediction, 'confidence': confidence}\n\n    # Open the CSV file in append mode, write the header if the file is new, and then write the data as a new row\n    with open('predictions.csv', 'a') as file:\n        writer = csv.DictWriter(file, fieldnames=['image_name', 'prediction', 'confidence'])\n        if os.stat('predictions.csv').st_size == 0:\n            writer.writeheader()\n        writer.writerow(data)\n\n", "CodeBleu_score": 47.667603827265346}, {"code": "def write_to_csv(image_name, prediction, confidence)\n\t\n    with open('predictions.csv', 'a') as csv_file:\n        writer = csv.writer(csv_file)\n        if os.stat('predictions.csv').st_size == 0:\n            writer.writerow(['Image', 'Prediction', 'Confidence'])\n        writer.writerow([image_name, prediction, confidence])\n\n", "CodeBleu_score": 32.314803215172695}]}
{"instance_id": "436161602009", "generation_result": [{"code": "def export_torchscript(model, im, file, optimize, prefix=colorstr(\"TorchScript:\"))\n\t\n    if not os.path.exists(file):\n        os.makedirs(file)\n    file = Path(file)\n    prefix = colorstr(prefix)\n    LOGGER.info(f\"{prefix} exporting model to {file}\")\n    model.eval()\n    if optimize:\n        model = optimize_for_mobile(model, save_dir=file)\n    else:\n        model = torch.jit.trace(model, im)\n        model = model.eval()\n    model.save(file / \"model.pt\")\n    model = model.float()\n    model_info(model)\n    model.save(file / \"model.jit\")\n    LOGGER.info(f\"{prefix} exporting model to {file}\")\n    return file\n\n", "CodeBleu_score": 37.51711883598842}, {"code": "def export_torchscript(model, im, file, optimize, prefix=colorstr(\"TorchScript:\"))\n\t\n\n    if optimize:\n        model = optimize_for_mobile(model)\n    else:\n        model.eval()\n\n    try:\n        with Profile(f\"{prefix} exporting model\"):\n            torch.jit.trace(model, im)\n    except RuntimeError as e:\n        LOGGER.error(f\"{prefix} failed to export model: {e}\")\n        return None\n\n    # Save model\n    try:\n        with Profile(f\"{prefix} saving model\"):\n            if optimize:\n                file = Path(file).parent / f\"{Path(file).stem}_optimized.pt\"\n            else:\n                file = Path(file).parent / f\"{Path(file).stem}.pt\"\n            torch.jit.save(model, file)\n    except RuntimeError as e:\n        LOGGER.error(f\"{prefix} failed to save model: {e}\")\n        return None\n\n    LOGGER.info(f\"{prefix} exported model to {file} in {time.time() - t0:.2f} seconds\")\n    return file\n\n", "CodeBleu_score": 32.88049377278519}, {"code": "def export_torchscript(model, im, file, optimize, prefix=colorstr(\"TorchScript:\"))\n\t\n    with open(file, \"wb\") as f:\n        torch.jit.save(torch.jit.trace(model, im), f)\n\n    if optimize:\n        optimize_for_mobile(file)\n        LOGGER.info(f\"{prefix} {file} optimized for mobile\")\n    else:\n        LOGGER.info(f\"{prefix} {file} saved\")\n\n    return file\n\n", "CodeBleu_score": 21.248886285363998}]}
{"instance_id": "78335487ac1c", "generation_result": [{"code": "def export_coreml(model, im, file, int8, half, nms, prefix=colorstr(\"CoreML:\")):\n\t\n    # Check for coremltools\n    try:\n        import coremltools\n    except Exception:\n        raise Exception(\"CoreML export requires coremltools. To install, run 'pip install coremltools'.\")\n    # Log\n    LOGGER.info(f\"{prefix} exporting to CoreML...\")\n    # Set file suffix\n    file = Path(file).with_suffix(\".mlmodel\")\n    # Trace\n    model.eval()\n    with torch.no_grad():\n        model(im)\n    # Convert\n    try:\n        model_coreml = coremltools.convert(model, inputs_as_dicts=False)\n    except Exception as e:\n        LOGGER.info(f\"{prefix} failed to convert to CoreML: {e}\")\n        return None, None\n    # Wrap\n    if nms:\n        model_coreml = coremltools.models.datatypes.IOSModel(model_coreml)\n    # Quantize\n    if int8:\n        try:\n            model_coreml = coremltools.models.utils.convert_neural_network_weights_to_int8(model_coreml)\n        except Exception as e:\n            LOGGER.info(f\"{prefix} failed to quantize to int8: {e}\")\n    elif half:\n        try:\n            model_coreml = coremltools.models.utils.convert_neural_network_weights_to_fp16(model_coreml)\n        except Exception as e:\n            LOGGER.info(f\"{prefix} failed to quantize to fp16: {e}\")\n    # Save\n    model_coreml.save(file)\n    # Return\n    return file, model_coreml\n\n", "CodeBleu_score": 34.04486316052361}, {"code": "def export_coreml(model, im, file, int8, half, nms, prefix=colorstr(\"CoreML:\")):\n\t\n    try:\n        import coremltools\n    except Exception as e:\n        LOGGER.error(f\"{e}", "CodeBleu_score": 9.28211853256266}, {"code": "def export_coreml(model, im, file, int8, half, nms, prefix=colorstr(\"CoreML:\")):\n\t\n    # Check for coremltools\n    try:\n        import coremltools\n    except:\n        print(f\"{prefix} Please install coremltools with `pip install coremltools`\")\n        return\n    # Log start of export\n    print(f\"{prefix} Starting export to CoreML...\")\n    # Set file suffix\n    file = Path(file).with_suffix(\".mlmodel\")\n    # Wrap model for NMS if needed\n    if nms:\n        model = iOSModel(model, nms)\n    # Trace the model\n    model.eval()\n    with torch.no_grad():\n        model(torch.zeros((1, 3, im.shape[0], im.shape[1])).to(next(model.parameters()).device))\n    # Convert to CoreML\n    model_coreml = coremltools.convert(model, inputs=[(\"input\", (3, im.shape[0], im.shape[1]))])\n    # Apply quantization if needed and supported\n    if int8:\n        model_coreml = coremltools.models.utils.convert_neural_network_weights_to_integer(model_coreml)\n    if half:\n        model_coreml = coremltools.models.utils.convert_neural_network_weights_to_float16(model_coreml)\n    # Save the CoreML model\n    print(f\"{prefix} Saving CoreML model to {file}\")\n    model_coreml.save(file)\n    return file, model_coreml\n\n", "CodeBleu_score": 39.17816464731924}]}
{"instance_id": "80233cf71580", "generation_result": [{"code": "def generate_individual(input_ranges, individual_length)\n\t\n    individual = []\n    for _ in range(individual_length):\n        individual.append(random.uniform(input_ranges[0], input_ranges[1]))\n    return individual\n\n", "CodeBleu_score": 39.41234669340793}, {"code": "def generate_individual(input_ranges, individual_length)\n\t\n    individual = []\n    for _ in range(individual_length):\n        individual.append(random.uniform(input_ranges[0], input_ranges[1]))\n    return individual\n\n", "CodeBleu_score": 39.41234669340793}, {"code": "def generate_individual(input_ranges, individual_length)\n\t\n    individual = []\n    for _ in range(individual_length):\n        individual.append(random.uniform(input_ranges[0], input_ranges[1]))\n    return individual\n\n", "CodeBleu_score": 39.41234669340793}]}
{"instance_id": "5ea979cb4a8e", "generation_result": [{"code": "def marginal_log_mean_coeff(self, t)\n\t\n    if self.schedule_type == 'discrete':\n        return self.log_alpha_0 + t * self.log_alpha_1\n    elif self.schedule_type == 'linear':\n        return self.log_alpha_0 + t * self.log_alpha_1\n    elif self.schedule_type == 'cosine':\n        return self.log_alpha_0 + self.log_alpha_1 * math.cos(math.pi * t / self.T)\n    else:\n        raise ValueError('Schedule type not recognized.')\n\n", "CodeBleu_score": 30.527662421322187}, {"code": "def marginal_log_mean_coeff(self, t)\n\t\n    if self.schedule_type == 'discrete':\n        if t <= self.t_min:\n            return math.log(1.0)\n        elif t >= self.t_max:\n            return math.log(0.0)\n        else:\n            return math.log(self.alpha_min + (self.alpha_max - self.alpha_min) * (t - self.t_min) / (self.t_max - self.t_min))\n    elif self.schedule_type == 'linear':\n        if t <= self.t_min:\n            return math.log(1.0)\n        elif t >= self.t_max:\n            return math.log(0.0)\n        else:\n            return math.log(self.alpha_min + (self.alpha_max - self.alpha_min) * (t - self.t_min) / (self.t_max - self.t_min))\n    elif self.schedule_type == 'cosine':\n        if t <= self.t_min:\n            return math.log(1.0)\n        elif t >= self.t_max:\n            return math.log(0.0)\n        else:\n            return math.log(self.alpha_min + (self.alpha_max - self.alpha_min) * (1.0 + math.cos(math.pi * (t - self.t_min) / (self.t_max - self.t_min))) / 2.0)\n\n", "CodeBleu_score": 29.896772743692612}, {"code": "def marginal_log_mean_coeff(self, t)\n\t\n\n    if self.schedule_type == 'discrete':\n        return self.log_alpha_t(t)\n    elif self.schedule_type == 'linear':\n        return self.log_alpha_t(t) - math.log(self.schedule_length)\n    elif self.schedule_type == 'cosine':\n        return self.log_alpha_t(t) - math.log(\n            (1 + math.cos(math.pi * t / self.schedule_length)) / 2)\n    else:\n        raise ValueError('Schedule type not recognized.')\n\n", "CodeBleu_score": 31.100769515073907}]}
{"instance_id": "7da4fae5a278", "generation_result": [{"code": "def inverse_lambda(self, lamb)\n\t\n    if self.schedule == \"linear\":\n        t = (math.log(lamb) - math.log(self.beta[0])) / (math.log(self.beta[1]) - math.log(self.beta[0]))\n        t = self.beta[0] * (1 - t) + self.beta[1] * t\n        return t\n    elif self.schedule == \"discrete\":\n        log_alpha = math.log(lamb) - math.log(self.beta[0])\n        t = (log_alpha - self.log_alpha[0]) / (self.log_alpha[1] - self.log_alpha[0])\n        return self.beta[0] * (1 - t) + self.beta[1] * t\n    elif self.schedule == \"arccos\":\n        log_alpha = math.log(lamb) - math.log(self.beta[0])\n        t = math.acos(log_alpha / math.sqrt(self.beta[0] * self.beta[1] - log_alpha ** 2))\n        return t\n    else:\n        raise ValueError(\"The schedule is not implemented.\")\n\n", "CodeBleu_score": 30.03230464236588}, {"code": "def inverse_lambda(self, lamb)\n\t\n    if self.schedule == \"linear\":\n        t = self.beta * lamb\n    elif self.schedule == \"discrete\":\n        if self.beta == 0:\n            t = lamb\n        else:\n            t = self.beta * (lamb / self.beta) ** (1 / (1 - self.beta))\n    elif self.schedule == \"cosine\":\n        t = math.acos(lamb) / math.pi\n    elif self.schedule == \"exponential\":\n        t = (math.exp(lamb) - 1) / self.beta\n    elif self.schedule == \"logarithmic\":\n        t = math.log(lamb) / self.beta\n    else:\n        raise NotImplementedError(\"Schedule {} not implemented\".format(self.schedule))\n    return t\n\n", "CodeBleu_score": 22.933875702423954}, {"code": "def inverse_lambda(self, lamb)\n\t\n    if self.schedule_type == \"linear\":\n        return self.beta * lamb\n    elif self.schedule_type == \"discrete\":\n        return self.alpha * math.log(lamb)\n    else:\n        return math.acos(lamb)\n\n", "CodeBleu_score": 8.819301637190446}]}
{"instance_id": "a519c853a2b2", "generation_result": [{"code": "def noise_pred_fn(x, t_continuous, cond=None)\n\t\n\n    # expand time input to match batch size\n    t_continuous = t_continuous.expand(x.shape[0], -1)\n\n    # process input through model\n    if cond is None:\n        pred = model(x, t_continuous)\n    else:\n        pred = model(x, t_continuous, cond)\n\n    # apply noise schedule\n    if model_type == \"noise\":\n        pred = pred * noise_schedule(t_continuous)\n    elif model_type == \"noise_scale\":\n        pred = pred * noise_schedule(t_continuous) * scale_schedule(t_continuous)\n\n    return pred\n\n", "CodeBleu_score": 16.41170086977034}, {"code": "def noise_pred_fn(x, t_continuous, cond=None)\n\t\n    if cond is None:\n        cond = torch.ones_like(x[:, :1])\n    if x.shape[0] != 1:\n        t_continuous = t_continuous.expand(x.shape[0], -1)\n    else:\n        t_continuous = t_continuous.unsqueeze(0)\n    x = x.unsqueeze(0)\n    if cond.shape[0] != 1:\n        cond = cond.expand(x.shape[0], -1)\n    else:\n        cond = cond.unsqueeze(0)\n    x = x.expand(cond.shape[0], -1, -1)\n    t_continuous = t_continuous.expand(cond.shape[0], -1)\n    x = x.expand(cond.shape[0], -1, -1)\n    x = x.expand(cond.shape[0], -1, -1)\n    if t_continuous.shape[0] != 1:\n        t_continuous = t_continuous.expand(x.shape[0], -1)\n    else:\n        t_continuous = t_continuous.unsqueeze(0)\n    x = x.expand(t_continuous.shape[0], -1, -1)\n    x = x.expand(cond.shape[0], -1, -1)\n    if x.shape[0] != 1:\n        cond = cond.expand(x.shape[0], -1)\n    else:\n        cond = cond.unsqueeze(0)\n    x = x.expand(cond.shape[0], -1, -1)\n    x = x.expand(t_continuous.shape[0], -1, -1)\n    x = x.expand(cond.shape[0], -1, -1)\n    if x.shape[0] != 1:\n        t_continuous = t_continuous.expand(x.shape[0], -1)\n    else:\n        t_continuous = t_continuous.unsqueeze(0)\n    x = x.expand(t_continuous", "CodeBleu_score": 26.415228493965703}, {"code": "def noise_pred_fn(x, t_continuous, cond=None)\n\t\n    if t_continuous is None:\n        t_continuous = torch.arange(0, x.shape[1], device=x.device, dtype=x.dtype)\n    if cond is None:\n        cond = torch.zeros_like(x)\n    if x.shape[0] == 1:\n        x = x.expand(t_continuous.shape[0], x.shape[1], x.shape[2])\n        cond = cond.expand(t_continuous.shape[0], cond.shape[1], cond.shape[2])\n    else:\n        x = x.expand(x.shape[0], t_continuous.shape[0], x.shape[2])\n        cond = cond.expand(cond.shape[0], t_continuous.shape[0], cond.shape[2])\n    t_continuous = t_continuous.unsqueeze(1)\n    t_continuous = t_continuous.expand(x.shape[0], x.shape[1], t_continuous.shape[2])\n    x = x.permute(1, 0, 2)\n    cond = cond.permute(1, 0, 2)\n    x = x.contiguous()\n    cond = cond.contiguous()\n    t_continuous = t_continuous.contiguous()\n    if x.shape[2] == 1:\n        x = x.squeeze(2)\n        cond = cond.squeeze(2)\n        t_continuous = t_continuous.squeeze(2)\n    if x.shape[2] == 2:\n        x = x.squeeze(2)\n        cond = cond.squeeze(2)\n        t_continuous = t_continuous.squeeze(2)\n    if x.shape[2] == 3:\n        x = x.squeeze(2)\n        cond = cond.squeeze(2)\n        t_continuous = t_continuous.squeeze(2)\n    if x.shape[2] == 4:\n        x", "CodeBleu_score": 31.41067126351348}]}
{"instance_id": "96a9b7d54823", "generation_result": [{"code": "def dynamic_thresholding_fn(self, x0, t=None)\n\t\n\n    if t is None:\n        t = self.t\n\n    if self.training:\n        if self.dynamic_thresholding_mode == \"mean\":\n            t = self.dynamic_thresholding_factor * x0.abs().mean()\n        elif self.dynamic_thresholding_mode == \"median\":\n            t = self.dynamic_thresholding_factor * x0.abs().median()\n        elif self.dynamic_thresholding_mode == \"std\":\n            t = self.dynamic_thresholding_factor * x0.abs().std()\n        elif self.dynamic_thresholding_mode == \"percentile\":\n            t = self.dynamic_thresholding_factor * x0.abs().percentile(self.dynamic_thresholding_percentile)\n        elif self.dynamic_thresholding_mode == \"percentile_std\":\n            t = self.dynamic_thresholding_factor * x0.abs().percentile(self.dynamic_thresholding_percentile) * x0.abs().std()\n        elif self.dynamic_thresholding_mode == \"percentile_median\":\n            t = self.dynamic_thresholding_factor * x0.abs().percentile(self.dynamic_thresholding_percentile) * x0.abs().median()\n        elif self.dynamic_thresholding_mode == \"percentile_mean\":\n            t = self.dynamic_thresholding_factor * x0.abs().percentile(self.dynamic_thresholding_percentile) * x0.abs().mean()\n        else:\n            raise ValueError(\n                \"dynamic_thresholding_mode must be one of ['mean', 'median', 'std', 'percentile', 'percentile_std', 'percentile_median', 'percentile_mean']\"\n            )\n\n    # print(f\"dynamic_thresholding_mode: {self.dynamic_thresholding_mode}, t: {t}\")\n    # print(f\"x0.abs().mean(): {x0.abs().mean()}\")\n    # print(f\"x0.abs().median(): {x0.abs().median()}\")\n    #", "CodeBleu_score": 18.986574827099258}, {"code": "def dynamic_thresholding_fn(self, x0, t=None)\n\t\n    if t is None:\n        t = self.t\n    if t is None:\n        raise ValueError(\"Dynamic thresholding requires a threshold t.\")\n    # x0 = x0.clone()\n    x0_abs = torch.abs(x0)\n    x0_abs_quantile = torch.quantile(x0_abs, t)\n    x0_abs_quantile_threshold = x0_abs_quantile.item()\n    x0_abs_quantile_threshold = max(x0_abs_quantile_threshold, self.eps)\n    x0_abs_quantile_threshold = min(x0_abs_quantile_threshold, self.max_val)\n    x0_abs_quantile_threshold = max(x0_abs_quantile_threshold, self.eps)\n    x0_abs_quantile_threshold = min(x0_abs_quantile_threshold, self.max_val)\n    x0_abs_quantile_threshold = float(x0_abs_quantile_threshold)\n    x0_abs_quantile_threshold = max(x0_abs_quantile_threshold, self.eps)\n    x0_abs_quantile_threshold = min(x0_abs_quantile_threshold, self.max_val)\n    x0_abs_quantile_threshold = float(x0_abs_quantile_threshold)\n    x0_abs_quantile_threshold = max(x0_abs_quantile_threshold, self.eps)\n    x0_abs_quantile_threshold = min(x0_abs_quantile_threshold, self.max_val)\n    x0_abs_quantile_threshold = float(x0_abs_quantile_threshold)\n    x0_abs_quantile_threshold = max(x0_abs_quantile_threshold, self.eps)\n    x0_abs_quantile_threshold = min(x0_abs_quantile_threshold, self.max_val)\n    x0_abs_quantile_threshold = float(x0", "CodeBleu_score": 24.623611618176955}, {"code": "def dynamic_thresholding_fn(self, x0, t=None)\n\t\n    # compute quantile of absolute values\n    x0_abs = torch.abs(x0)\n    x0_abs_quantile = x0_abs.quantile(0.5, dim=0, keepdim=True)\n    # compute scaling factor\n    x0_abs_max = x0_abs.max()\n    if x0_abs_max == 0:\n        scaling_factor = torch.tensor(1.0)\n    else:\n        scaling_factor = x0_abs_quantile / x0_abs_max\n    # clamp x0 values\n    x0_clamped = x0_abs.clamp(min=scaling_factor * x0_abs_quantile)\n    # scale x0\n    x0_scaled = x0_clamped / scaling_factor\n    return x0_scaled\n\n", "CodeBleu_score": 20.029595223763007}]}
{"instance_id": "59c6ec6d7a6a", "generation_result": [{"code": "def data_prediction_fn(self, x, t)\n\t\n    x = x.clone()\n    x0 = x.clone()\n    x = x.float()\n    x0 = x0.float()\n    x0 = x0.view(x0.shape[0], -1)\n    x0 = x0.to(self.device)\n    x0 = x0.float()\n    x0 = x0.view(x0.shape[0], -1)\n    x0 = x0.to(self.device)\n    x0 = x0.float()\n    x0 = x0.view(x0.shape[0], -1)\n    x0 = x0.to(self.device)\n    x0 = x0.float()\n    x0 = x0.view(x0.shape[0], -1)\n    x0 = x0.to(self.device)\n    x0 = x0.float()\n    x0 = x0.view(x0.shape[0], -1)\n    x0 = x0.to(self.device)\n    x0 = x0.float()\n    x0 = x0.view(x0.shape[0], -1)\n    x0 = x0.to(self.device)\n    x0 = x0.float()\n    x0 = x0.view(x0.shape[0], -1)\n    x0 = x0.to(self.device)\n    x0 = x0.float()\n    x0 = x0.view(x0.shape[0], -1)\n    x0 = x0.to(self.device)\n    x0 = x0.float()\n    x0 = x0.view(x0.shape[0], -1)\n    x0 = x0.to(self.device)\n    x0 = x0.float()\n    x0 = x0.view(x0.shape[0], -1)\n    x0 = x0.to(self.device)\n    x0 = x0.float()\n    x0 = x0.view(x0.shape[0], -1)\n    x0 = x0.to(self.device)\n    x", "CodeBleu_score": 8.963316765422134}, {"code": "def data_prediction_fn(self, x, t)\n\t\n    # pylint: disable=arguments-differ\n    if not self.training:\n        x = x.detach()\n    x = x.clone()\n    if self.training:\n        # TODO: This is a temporary fix for the case when the input is a list of tensors\n        # TODO: Need to find a better way to handle this\n        x = x[0]\n\n    if self.dynamic_thresholding_ratio is not None and self.max_dynamic_threshold is not None:\n        dynamic_threshold = self.max_dynamic_threshold * self.dynamic_thresholding_ratio\n        x = torch.clamp(x, min=-dynamic_threshold, max=dynamic_threshold)\n\n    if self.normalize_method == \"batch_norm\":\n        x = self.normalization_layer(x)\n    elif self.normalize_method == \"layer_norm\":\n        x = self.normalization_layer(x)\n    elif self.normalize_method == \"none\":\n        pass\n    else:\n        raise NotImplementedError\n\n    return x\n\n", "CodeBleu_score": 24.819342629819314}, {"code": "def data_prediction_fn(self, x, t)\n\t\n    x_shape = x.shape\n    x = x.view(-1, 1)\n    x_norm = torch.norm(x, p=2, dim=1)\n    x_norm = x_norm.view(x_shape[0], 1)\n    threshold = self.dynamic_threshold_ratio * self.dynamic_threshold_max\n    threshold = threshold.view(x_shape[0], 1)\n    threshold = torch.max(threshold, x_norm)\n    threshold = threshold.view(-1)\n    x_clamped = torch.clamp(x, -threshold, threshold)\n    x_clamped = x_clamped / threshold\n    x_clamped = x_clamped.view(x_shape)\n    return x_clamped\n\n", "CodeBleu_score": 27.152135662045662}]}
{"instance_id": "65e3618468a2", "generation_result": [{"code": "def predict_eps_sigma(model, input, sigma_in, **kwargs)\n\t\n    # reshape sigma to match the dimensions of the input\n    sigma_in = sigma_in.view(input.size())\n    # scale the input\n    input = input * sigma_in\n    # compute the prediction\n    pred = model(input, **kwargs)\n    # adjust the prediction\n    pred = pred * sigma_in\n    return pred\n\n", "CodeBleu_score": 26.8488402865807}, {"code": "def predict_eps_sigma(model, input, sigma_in, **kwargs)\n\t\n    # reshape sigma to match the dimensions of the input\n    sigma_in = sigma_in.view(input.size())\n    # scale the input\n    input = input * sigma_in\n    # compute the prediction\n    output = model(input, **kwargs)\n    # adjust the model output with the scaled input and sigma\n    output = output * sigma_in\n    return output\n\n", "CodeBleu_score": 27.228167974435447}, {"code": "def predict_eps_sigma(model, input, sigma_in, **kwargs)\n\t\n\n    # Reshape sigma to match the dimensions of the input\n    sigma = sigma_in.clone()\n    if len(sigma.shape) == 1:\n        sigma = sigma.unsqueeze(0).unsqueeze(2).unsqueeze(3)\n    elif len(sigma.shape) == 2:\n        sigma = sigma.unsqueeze(0).unsqueeze(3)\n    elif len(sigma.shape) == 3:\n        sigma = sigma.unsqueeze(0)\n\n    # Scale the input\n    input = input.clone()\n    if len(input.shape) == 3:\n        input = input.unsqueeze(0)\n    input = input * sigma\n\n    # Compute the prediction\n    prediction = model(input)\n\n    # Adjust the prediction with the scaled input and sigma\n    prediction = prediction * sigma\n\n    return prediction\n\n", "CodeBleu_score": 28.51599063983814}]}
{"instance_id": "b35846dfbc66", "generation_result": [{"code": "def cal_intergrand(beta_0, beta_1, taus)\n\t\n    with torch.enable_grad():\n        taus_clone = taus.clone().requires_grad_(True)\n        alpha = torch.exp(-beta_0 - beta_1 * taus_clone)\n        log_alpha = alpha.log()\n        d_log_alpha_d_taus = torch.autograd.grad(log_alpha, taus_clone, torch.ones_like(log_alpha), create_graph=True)[0]\n        integrand = d_log_alpha_d_taus * alpha\n    return integrand\n\n", "CodeBleu_score": 41.66924583142716}, {"code": "def cal_intergrand(beta_0, beta_1, taus)\n\t\n    with torch.enable_grad():\n        taus_clone = taus.clone().requires_grad_(True)\n        alpha = torch.exp(beta_0 + beta_1 * taus_clone)\n        log_alpha = torch.log(alpha)\n        log_integrand = log_alpha * taus_clone\n        log_integrand.backward(torch.ones_like(log_integrand))\n        integrand = log_integrand.detach()\n        return integrand\n\n", "CodeBleu_score": 37.72103697359359}, {"code": "def cal_intergrand(beta_0, beta_1, taus)\n\t\n    with torch.enable_grad():\n        taus_clone = taus.clone().requires_grad_(True)\n        alpha = torch.exp(beta_0 + beta_1 * taus_clone)\n        log_alpha = alpha.log()\n        log_alpha_grad = torch.autograd.grad(log_alpha, taus_clone, create_graph=True)[0]\n        integrand = log_alpha * log_alpha_grad\n        return integrand\n\n", "CodeBleu_score": 39.77909356705376}]}
{"instance_id": "5f3667566a10", "generation_result": [{"code": "def sample_euler_ancestral(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None)\n\t\n    if extra_args is None:\n        extra_args = {}\n    if noise_sampler is None:\n        noise_sampler = torch.distributions.Normal(0, s_noise)\n    if callback is None:\n        callback = lambda x: None\n    if disable is None:\n        disable = []\n    if isinstance(disable, bool):\n        disable = [] if disable else list(range(len(model)))\n    if isinstance(disable, int):\n        disable = [disable]\n    if isinstance(disable, str):\n        disable = [model.get_index(disable)]\n    if isinstance(disable, list):\n        disable = [model.get_index(x) for x in disable]\n    if isinstance(disable, tuple):\n        disable = [model.get_index(x) for x in disable]\n    if isinstance(disable, dict):\n        disable = [model.get_index(x) for x in disable.keys()]\n    if isinstance(disable, torch.Tensor):\n        disable = [model.get_index(x) for x in disable.tolist()]\n    if isinstance(disable, torch.LongTensor):\n        disable = [model.get_index(x) for x in disable.tolist()]\n    if isinstance(disable, torch.cuda.LongTensor):\n        disable = [model.get_index(x) for x in disable.tolist()]\n    if isinstance(disable, torch.cuda.LongTensor):\n        disable = [model.get_index(x) for x in disable.tolist()]\n    if isinstance(disable, torch.cuda.LongTensor):\n        disable = [model.get_index(x) for x in disable.tolist()]\n    if isinstance(disable, torch.cuda.LongTensor):\n        disable = [model.get_index(x) for x in disable.tolist()]\n    if isinstance(disable, torch.cuda.LongTensor):\n        disable = [model.get_index(x) for x in disable.tolist()]\n    if isinstance(disable, torch.", "CodeBleu_score": 20.61641032941844}, {"code": "def sample_euler_ancestral(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None)\n\t\n    if extra_args is None:\n        extra_args = {}\n    if noise_sampler is None:\n        noise_sampler = torch.distributions.Normal(0, s_noise)\n    if callback is None:\n        callback = lambda x: None\n    if disable is None:\n        disable = []\n    if not isinstance(disable, list):\n        disable = [disable]\n    if not isinstance(sigmas, list):\n        sigmas = [sigmas]\n    for sigma in sigmas:\n        if sigma not in disable:\n            x = denoise(model, x, sigma, extra_args, noise_sampler)\n            x = ancestral_step(model, x, sigma, extra_args)\n            x = euler_step(model, x, sigma, extra_args)\n            callback(x)\n    if s_noise > 0:\n        x = x + noise_sampler.sample(x.shape).to(x) * s_noise\n    return x\n\n", "CodeBleu_score": 29.8703483738587}, {"code": "def sample_euler_ancestral(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None)\n\t\n    if extra_args is None:\n        extra_args = []\n    if noise_sampler is None:\n        noise_sampler = lambda x: x\n    if callback is None:\n        callback = lambda x: None\n    if disable is None:\n        disable = []\n    x = x.clone()\n    for i in range(len(sigmas)):\n        if not i in disable:\n            x = x + eta * sigmas[i] * model.ancestral_step(x, *extra_args)\n            callback(x)\n    x = noise_sampler(x)\n    return x + s_noise * torch.randn_like(x)\n\n", "CodeBleu_score": 25.671972830438623}]}
{"instance_id": "6261ded88df2", "generation_result": [{"code": "def linear_multistep_coeff(order, t, i, j)\n\t\n    if order > i:\n        raise ValueError(\"Order must be less than or equal to the step index.\")\n    def product_term(t):\n        return t**(j-i)\n    return integrate.quad(product_term, t[i], t[i+1])[0]\n\n", "CodeBleu_score": 18.92562612456798}, {"code": "def linear_multistep_coeff(order, t, i, j)\n\t\n    if order >= j:\n        raise ValueError(\"The order of the multistep method is too high.\")\n    def product(t, i, j):\n        if j == 0:\n            return 1\n        else:\n            return t[i+j] - t[i]\n    return integrate.quad(lambda t: product(t, i, j), t[i], t[i+1])[0]\n\n", "CodeBleu_score": 27.346492616863145}, {"code": "def linear_multistep_coeff(order, t, i, j)\n\t\n    if order == 0:\n        return 1\n    elif order == 1:\n        return t[i] - t[j]\n    elif order == 2:\n        return (t[i] - t[j]) * (t[i] - t[i + 1])\n    elif order == 3:\n        return (t[i] - t[j]) * (t[i] - t[i + 1]) * (t[i] - t[i + 2])\n    elif order == 4:\n        return (t[i] - t[j]) * (t[i] - t[i + 1]) * (t[i] - t[i + 2]) * (t[i] - t[i + 3])\n    elif order == 5:\n        return (t[i] - t[j]) * (t[i] - t[i + 1]) * (t[i] - t[i + 2]) * (t[i] - t[i + 3]) * (t[i] - t[i + 4])\n    else:\n        raise ValueError('Order too high for multistep method.')\n\n", "CodeBleu_score": 19.925566753519362}]}
{"instance_id": "4ee025e33ab1", "generation_result": [{"code": "def sample_lcm(model, x, sigmas, extra_args=None, callback=None, disable=None, noise_sampler=None)\n\t\n    if extra_args is None:\n        extra_args = {}\n    if noise_sampler is None:\n        noise_sampler = comfy.model_patcher.get_noise_sampler(model)\n    if callback is None:\n        callback = lambda *args: None\n    if disable is None:\n        disable = []\n    if not isinstance(disable, list):\n        disable = [disable]\n    if \"noise_scale\" not in disable:\n        disable.append(\"noise_scale\")\n    if \"noise_scale\" in extra_args:\n        del extra_args[\"noise_scale\"]\n    if \"noise_scale_std\" not in disable:\n        disable.append(\"noise_scale_std\")\n    if \"noise_scale_std\" in extra_args:\n        del extra_args[\"noise_scale_std\"]\n    if \"noise_scale_std_min\" not in disable:\n        disable.append(\"noise_scale_std_min\")\n    if \"noise_scale_std_min\" in extra_args:\n        del extra_args[\"noise_scale_std_min\"]\n    if \"noise_scale_std_max\" not in disable:\n        disable.append(\"noise_scale_std_max\")\n    if \"noise_scale_std_max\" in extra_args:\n        del extra_args[\"noise_scale_std_max\"]\n    if \"noise_scale_std_decay\" not in disable:\n        disable.append(\"noise_scale_std_decay\")\n    if \"noise_scale_std_decay\" in extra_args:\n        del extra_args[\"noise_scale_std_decay\"]\n    if \"noise_scale_std_decay_period\" not in disable:\n        disable.append(\"noise_scale_std_decay_period\")\n    if \"noise_scale_std_decay_period\" in extra_args:\n        del extra_args[\"noise_scale_std_decay_period\"]\n    if \"noise_scale_std_decay_period_std\" not in disable:\n        disable.", "CodeBleu_score": 21.05177145081377}, {"code": "def sample_lcm(model, x, sigmas, extra_args=None, callback=None, disable=None, noise_sampler=None)\n\t\n    if extra_args is None:\n        extra_args = {}\n    if noise_sampler is None:\n        noise_sampler = torch.distributions.Normal(0, 1)\n    if disable is None:\n        disable = []\n    if callback is None:\n        callback = lambda *args: None\n    for i, sigma in enumerate(sigmas):\n        if i in disable:\n            continue\n        if sigma <= 0:\n            x = model(x, **extra_args)\n        else:\n            x = model(x, **extra_args)\n            x = x + sigma * noise_sampler.sample(x.shape)\n        callback(x, i, sigma)\n    return x\n\n", "CodeBleu_score": 30.049305138595138}, {"code": "def sample_lcm(model, x, sigmas, extra_args=None, callback=None, disable=None, noise_sampler=None)\n\t\n    if extra_args is None:\n        extra_args = {}\n    if noise_sampler is None:\n        noise_sampler = NoiseSampler()\n    if callback is None:\n        callback = lambda x: None\n    if disable is None:\n        disable = []\n    x = x.clone()\n    for i, sigma in enumerate(sigmas):\n        if i in disable:\n            continue\n        x = model(x, sigma, extra_args)\n        if sigma > 0:\n            x = x * noise_sampler.sample(sigma)\n        callback(x)\n    return x\n\n", "CodeBleu_score": 28.000152833624036}]}
{"instance_id": "cb51f91da606", "generation_result": [{"code": "def append_dims(x, target_dims)\n\t\n    if len(target_dims) < len(x.shape):\n        raise ValueError('target dimensions must be greater than the current dimensions')\n    append_dims = target_dims[len(x.shape):]\n    for dim in append_dims:\n        x = x.unsqueeze(-1)\n    return x.detach() if x.device.type == 'mps' else x\n\n", "CodeBleu_score": 36.62190316572905}, {"code": "def append_dims(x, target_dims)\n\t\n    if x.dim() < target_dims:\n        raise ValueError(f\"The target dimensions {target_dims} are fewer than the current dimensions {x.dim()}\")\n    num_dims_to_append = target_dims - x.dim()\n    if num_dims_to_append == 0:\n        return x.detach() if x.device.type == \"mps\" else x\n    return x.unsqueeze(-1).expand(x.size() + (1,) * num_dims_to_append).detach() if x.device.type == \"mps\" else x.unsqueeze(-1).expand(x.size() + (1,) * num_dims_to_append)\n\n", "CodeBleu_score": 37.47325596715344}, {"code": "def append_dims(x, target_dims)\n\t\n    if x.dim() < target_dims:\n        raise ValueError(f\"Cannot append dimensions to a tensor of dimension {x.dim()} to reach {target_dims} dimensions.\")\n    num_dims_to_append = target_dims - x.dim()\n    if num_dims_to_append == 0:\n        return x.detach() if x.device.type == 'mps' else x\n    else:\n        return x.unsqueeze(tuple(range(num_dims_to_append)))\n\n", "CodeBleu_score": 32.23956428839754}]}
{"instance_id": "9fac874a3bc7", "generation_result": [{"code": "def download_file(path, url, digest=None)\n\t\n    path = Path(path)\n    if not path.is_dir():\n        path.mkdir(parents=True)\n    path = path / url.split('/')[-1]\n    if path.exists():\n        if digest is not None:\n            if not verify_digest(path, digest):\n                raise ValueError('File exists but has incorrect hash.')\n        return path\n    with urllib.request.urlopen(url) as response, open(path, 'wb') as f:\n        shutil.copyfileobj(response, f)\n    if digest is not None:\n        if not verify_digest(path, digest):\n            raise ValueError('File exists but has incorrect hash.')\n    return path\n\n", "CodeBleu_score": 54.28322970656421}, {"code": "def download_file(path, url, digest=None)\n\t\n    path = Path(path)\n    path.parent.mkdir(parents=True, exist_ok=True)\n    if path.exists():\n        if digest is not None:\n            if not verify_file_hash(path, digest):\n                raise RuntimeError(f\"File {path} exists but its hash does not match the expected value.\")\n        return path\n    else:\n        print(f\"Downloading {url} to {path}\")\n        with urllib.request.urlopen(url) as response, path.open(\"wb\") as out_file:\n            shutil.copyfileobj(response, out_file)\n        if digest is not None:\n            if not verify_file_hash(path, digest):\n                raise RuntimeError(f\"File {path} exists but its hash does not match the expected value.\")\n        return path\n\n", "CodeBleu_score": 41.85626799185852}, {"code": "def download_file(path, url, digest=None)\n\t\n    if not isinstance(path, Path):\n        path = Path(path)\n    if not isinstance(url, str):\n        url = str(url)\n    if path.exists():\n        if digest is not None:\n            if not verify_file(path, digest):\n                raise RuntimeError(f\"Downloaded file at {path} has an invalid hash.\")\n        return path\n    path.parent.mkdir(parents=True, exist_ok=True)\n    with urllib.request.urlopen(url) as response, open(path, \"wb\") as file:\n        shutil.copyfileobj(response, file)\n    if digest is not None:\n        if not verify_file(path, digest):\n            raise RuntimeError(f\"Downloaded file at {path} has an invalid hash.\")\n    return path\n\n", "CodeBleu_score": 50.865263968311545}]}
{"instance_id": "f23165e897ce", "generation_result": [{"code": "def train_mode(model, mode=True)\n\t\n    model.train(mode)\n    return model\n\n", "CodeBleu_score": 3.5228876052714493}, {"code": "def train_mode(model, mode=True)\n\t\n    def _enter(self):\n        self._training = self.training\n        self.train(mode)\n        return self\n    def _exit(self, type, value, traceback):\n        self.train(self._training)\n    return contextmanager(enter=_enter, exit=_exit)\n\n", "CodeBleu_score": 21.54156547868233}, {"code": "def train_mode(model, mode=True)\n\t\n    def _context_manager(model, mode):\n        if isinstance(model, nn.Module):\n            model.train(mode)\n        else:\n            raise TypeError(\"model must be an instance of nn.Module\")\n        yield\n        if isinstance(model, nn.Module):\n            model.train(not mode)\n    return contextmanager(_context_manager(model, mode))\n\n", "CodeBleu_score": 19.10109996981654}]}
{"instance_id": "c6d59c6cb8e1", "generation_result": [{"code": "def ema_update(model, averaged_model, decay)\n\t\n    with torch.no_grad():\n        for averaged_param, param in zip(averaged_model.parameters(), model.parameters()):\n            averaged_param.data.mul_(decay).add_(param.data, alpha=1 - decay)\n        for averaged_param, param in zip(averaged_model.parameters(), model.parameters()):\n            averaged_param.data.mul_(decay).add_(param.data, alpha=1 - decay)\n        for averaged_param, param in zip(averaged_model.buffers(), model.buffers()):\n            averaged_param.data.copy_(param.data)\n\n", "CodeBleu_score": 37.17912949115205}, {"code": "def ema_update(model, averaged_model, decay)\n\t\n    with torch.no_grad():\n        for averaged_param, current_param in zip(averaged_model.parameters(), model.parameters()):\n            averaged_param.data.mul_(decay).add_(current_param.data, alpha=1 - decay)\n\n        for averaged_param, current_param in zip(averaged_model.buffers(), model.buffers()):\n            averaged_param.data.copy_(current_param.data)\n\n", "CodeBleu_score": 32.745515833763804}, {"code": "def ema_update(model, averaged_model, decay)\n\t\n    with torch.no_grad():\n        for param, averaged_param in zip(model.parameters(), averaged_model.parameters()):\n            averaged_param.copy_(averaged_param * decay + param * (1 - decay))\n        for key, value in model.state_dict().items():\n            if key in averaged_model.state_dict():\n                averaged_model.state_dict()[key].copy_(averaged_model.state_dict()[key] * decay + value * (1 - decay))\n\n", "CodeBleu_score": 29.719633027172122}]}
{"instance_id": "600c39a30798", "generation_result": [{"code": "def rand_log_logistic(shape, loc=0., scale=1., min_value=0., max_value=float('inf'), device='cpu', dtype=torch.float32)\n\t\n    min_value = torch.tensor(min_value, device=device, dtype=dtype)\n    max_value = torch.tensor(max_value, device=device, dtype=dtype)\n    u = torch.rand(shape, device=device, dtype=dtype)\n    u = min_value + (max_value - min_value) * u\n    u = torch.log(u)\n    u = u * scale + loc\n    u = torch.sigmoid(u)\n    return u\n\n", "CodeBleu_score": 33.21961136890483}, {"code": "def rand_log_logistic(shape, loc=0., scale=1., min_value=0., max_value=float('inf'), device='cpu', dtype=torch.float32)\n\t\n    min_value = torch.tensor(min_value, device=device, dtype=dtype)\n    max_value = torch.tensor(max_value, device=device, dtype=dtype)\n    min_cdf = torch.log(torch.exp(scale * (torch.log(min_value) - loc)) + 1)\n    max_cdf = torch.log(torch.exp(scale * (torch.log(max_value) - loc)) + 1)\n    uniform_samples = torch.rand(shape, device=device, dtype=dtype)\n    samples = (torch.log(uniform_samples) - min_cdf) / (max_cdf - min_cdf)\n    samples = loc + scale * torch.log(torch.exp(samples) - 1)\n    return samples\n\n", "CodeBleu_score": 34.536131973634376}, {"code": "def rand_log_logistic(shape, loc=0., scale=1., min_value=0., max_value=float('inf'), device='cpu', dtype=torch.float32)\n\t\n    if not isinstance(shape, torch.Size):\n        shape = torch.Size([shape])\n    min_value = torch.tensor(min_value, device=device, dtype=dtype)\n    max_value = torch.tensor(max_value, device=device, dtype=dtype)\n    cdf_min = torch.log(torch.exp(torch.logit(min_value)) + 1)\n    cdf_max = torch.log(torch.exp(torch.logit(max_value)) - 1)\n    u = torch.rand(shape, device=device, dtype=dtype)\n    cdf_u = cdf_min + (cdf_max - cdf_min) * u\n    x = torch.logit(cdf_u) * scale + loc\n    x = torch.clamp(x, min_value, max_value)\n    return x\n\n", "CodeBleu_score": 41.073584465755914}]}
{"instance_id": "75e2def2467f", "generation_result": [{"code": "def get_activation(activation: Literal[\"elu\", \"snake\", \"none\"], antialias=False, channels=None) -> nn.Module\n\t\n    if activation == \"elu\":\n        return nn.ELU(inplace=True)\n    elif activation == \"snake\":\n        return comfy.ops.SnakeBeta(channels=channels, antialias=antialias)\n    elif activation == \"none\":\n        return nn.Identity()\n    else:\n        raise ValueError(\"Activation type must be one of 'elu', 'snake', or 'none'.\")\n\n", "CodeBleu_score": 39.05972402907318}, {"code": "def get_activation(activation: Literal[\"elu\", \"snake\", \"none\"], antialias=False, channels=None) -> nn.Module\n\t\n    if activation == \"elu\":\n        return nn.ELU()\n    elif activation == \"snake\":\n        return comfy.ops.SnakeBeta(channels)\n    elif activation == \"none\":\n        return nn.Identity()\n    else:\n        raise ValueError(\"Invalid activation type\")\n\n", "CodeBleu_score": 38.43640620778113}, {"code": "def get_activation(activation: Literal[\"elu\", \"snake\", \"none\"], antialias=False, channels=None) -> nn.Module\n\t\n    if activation == \"elu\":\n        return nn.ELU()\n    elif activation == \"snake\":\n        return comfy.ops.SnakeBeta(channels=channels, antialias=antialias)\n    elif activation == \"none\":\n        return nn.Identity()\n    else:\n        raise ValueError(f\"Invalid activation type {activation}\")\n\n", "CodeBleu_score": 42.0834546201659}]}
{"instance_id": "ef1714c79e51", "generation_result": [{"code": "def apply_rotary_pos_emb(t, freqs, scale = 1)\n\t\n    t = t.float()\n    if t.dtype == torch.float16:\n        t = t.float()\n    freqs = freqs.float()\n    if freqs.dtype == torch.float16:\n        freqs = freqs.float()\n    shape = t.shape\n    if len(shape) == 3:\n        t = rearrange(t, 'b c (h p1) -> b h c p1', p1 = shape[-1])\n        freqs = rearrange(freqs, 'f -> () f ()')\n        t = torch.cat((t, freqs), dim = -1)\n        t = rearrange(t, 'b h c (p1 p2) -> b c (h p1) p2', p1 = shape[-1])\n    elif len(shape) == 4:\n        t = rearrange(t, 'b c (h p1) (w p2) -> b h c (p1 p2) w', p1 = shape[-2], p2 = shape[-1])\n        freqs = rearrange(freqs, 'f -> () f ()')\n        t = torch.cat((t, freqs), dim = -1)\n        t = rearrange(t, 'b h c (p1 p2) (w p3) -> b h c (p1 p2) w p3', p1 = shape[-2], p2 = shape[-1])\n    else:\n        raise NotImplementedError('The rotary position embeddings are only implemented for tensors of rank 3 and 4.')\n    t = t * scale\n    t = torch.cat((t, t), dim = -1)\n    t1, t2 = t.unbind(-1)\n    t1, t2 = t1 * torch.cos(t2), t2 * torch.sin(t2)\n    return torch.cat((t1, t2), dim = -1)\n\n", "CodeBleu_score": 38.86655655341311}, {"code": "def apply_rotary_pos_emb(t, freqs, scale = 1)\n\t\n    if t.dtype == torch.float16:\n        t = t.float()\n    t = rearrange(t, 'b n d -> b d n')\n    freqs = rearrange(freqs, 'b d -> b () d ()')\n    t = t * freqs\n    t = t.sin() * scale\n    unrotated = t[..., :t.shape[-1] // 2]\n    rotated = t[..., t.shape[-1] // 2:]\n    return torch.cat((unrotated, rotated), dim = -1)\n\n", "CodeBleu_score": 29.428166791374892}, {"code": "def apply_rotary_pos_emb(t, freqs, scale = 1)\n\t\n    t = t.float()\n    freqs = freqs.float()\n    if t.dtype == torch.float16:\n        t = t.float()\n        freqs = freqs.float()\n    t_shape = t.shape\n    freqs_shape = freqs.shape\n    t = rearrange(t, 'b ... d -> b ... () d')\n    freqs = rearrange(freqs, 'b ... d -> b ... () d')\n    t = t * scale\n    freqs = freqs * scale\n    sin_t, cos_t = torch.sin(t), torch.cos(t)\n    sin_freqs, cos_freqs = torch.sin(freqs), torch.cos(freqs)\n    sin_t, cos_t = sin_t * cos_freqs, cos_t * sin_freqs\n    sin_t, cos_t = sin_t * cos_freqs, cos_t * sin_freqs\n    sin_t, cos_t = sin_t * cos_freqs, cos_t * sin_freqs\n    sin_t, cos_t = sin_t * cos_freqs, cos_t * sin_freqs\n    sin_t, cos_t = sin_t * cos_freqs, cos_t * sin_freqs\n    sin_t, cos_t = sin_t * cos_freqs, cos_t * sin_freqs\n    sin_t, cos_t = sin_t * cos_freqs, cos_t * sin_freqs\n    sin_t, cos_t = sin_t * cos_freqs, cos_t * sin_freqs\n    sin_t, cos_t = sin_t * cos_freqs, cos_t * sin_freqs\n    sin_t, cos_t = sin_t * cos_freqs, cos_t * sin_freqs\n    sin_t, cos_t = sin_t * cos_freqs, cos_t * sin_freqs", "CodeBleu_score": 31.3005709265719}]}
{"instance_id": "a7a5f178c6f6", "generation_result": [{"code": "def idx2vq(self, idx, dim=-1)\n\t\n    return self.codebook.index_select(dim, idx)\n\n", "CodeBleu_score": 17.271575374322644}, {"code": "def idx2vq(self, idx, dim=-1)\n\t\n    return self.embeddings[idx]\n\n", "CodeBleu_score": 13.027502176543567}, {"code": "def idx2vq(self, idx, dim=-1)\n\t\n    return self.codebook.embedding(idx, dim)\n\n", "CodeBleu_score": 17.31368755262934}]}
{"instance_id": "4104c041fb20", "generation_result": [{"code": "def encode(self, x, quantize=False)\n\t\n    for i, block in enumerate(self.blocks):\n        x = block(x)\n        if i == 0:\n            continue\n        if self.use_skip:\n            x = x + self.skip_layers[i - 1](x)\n    x = self.final(x)\n    if quantize:\n        x, enc, ind, loss = self.quantizer(x)\n        return x, enc, ind, loss\n    return x\n\n", "CodeBleu_score": 36.44031186255822}, {"code": "def encode(self, x, quantize=False)\n\t\n    x = self.input_block(x)\n    for i, block in enumerate(self.blocks):\n        x = block(x)\n        if i == len(self.blocks) - 1:\n            if quantize:\n                x, indices, loss = self.quantize(x)\n                return x, indices, loss\n            else:\n                return x\n\n", "CodeBleu_score": 34.31554012288598}, {"code": "def encode(self, x, quantize=False)\n\t\n    # Encode the input using a series of blocks\n    for block in self.blocks:\n        x = block(x)\n\n    # If quantization is enabled, apply a vector quantizer to the encoded output and return the quantized encoding, original encoding, indices, and combined loss\n    if quantize:\n        # Apply a vector quantizer to the encoded output\n        quantized, indices, loss = self.quantizer(x)\n\n        # Return the quantized encoding, original encoding, indices, and combined loss\n        return quantized, x, indices, loss\n\n    # If quantization is not enabled, return the encoded output\n    return x\n\n", "CodeBleu_score": 30.844941986447022}]}
{"instance_id": "403c52ba2d3b", "generation_result": [{"code": "def ema_scope(self, context=None)\n\t\n    if context is None:\n        context = {}\n    if self.ema is not None:\n        if self.ema.enabled:\n            if self.ema.log_switch:\n                self.logger.info(f\"EMA enabled. Switching to EMA weights.\")\n            for name, param in self.named_parameters():\n                if param.requires_grad:\n                    context[name] = param.data.clone()\n                    param.data = self.ema.average(name)\n        else:\n            if self.ema.log_switch:\n                self.logger.info(f\"EMA disabled. Switching to model weights.\")\n            for name, param in self.named_parameters():\n                if param.requires_grad:\n                    param.data = context[name]\n    yield\n    if self.ema is not None:\n        if self.ema.enabled:\n            if self.ema.log_switch:\n                self.logger.info(f\"EMA enabled. Switching to model weights.\")\n            for name, param in self.named_parameters():\n                if param.requires_grad:\n                    param.data = self.ema.average(name)\n        else:\n            if self.ema.log_switch:\n                self.logger.info(f\"EMA disabled. Switching to EMA weights.\")\n            for name, param in self.named_parameters():\n                if param.requires_grad:\n                    param.data = context[name]\n\n", "CodeBleu_score": 35.45398714107909}, {"code": "def ema_scope(self, context=None)\n\t\n    if context is None:\n        context = self\n    if self.ema is None:\n        yield\n        return\n    if self.ema.enabled:\n        if context.ema_switch_logging:\n            context.log(\"Switching to EMA weights\")\n        context.ema.swap_ema_params()\n    yield\n    if self.ema is None:\n        return\n    if self.ema.enabled:\n        if context.ema_switch_logging:\n            context.log(\"Switching to model weights\")\n        context.ema.swap_model_params()\n\n", "CodeBleu_score": 41.775882490076654}, {"code": "def ema_scope(self, context=None)\n\t\n    if self.ema is None:\n        return context\n    else:\n        if context is None:\n            context = {}\n        if self.ema.enabled:\n            if self.ema.log_switch:\n                context['ema_switch_on'] = self.ema.switch_on\n            self.ema.switch_on = True\n        else:\n            self.ema.switch_on = False\n        return context\n\n", "CodeBleu_score": 38.08582230606883}]}
{"instance_id": "94b7467f98e4", "generation_result": [{"code": "def post_attention(self, attn, x, gate_msa, shift_mlp, scale_mlp, gate_mlp)\n\t\n    x = x + attn * gate_msa\n    x = x / math.sqrt(self.emb_dim)\n    x = self.shift_norm(x)\n    x = x * self.scale_norm\n    x = x + self.mlp(x) * gate_mlp\n    return x\n\n", "CodeBleu_score": 41.68750348280577}, {"code": "def post_attention(self, attn, x, gate_msa, shift_mlp, scale_mlp, gate_mlp)\n\t\n    x = x + attn @ gate_msa\n    x = x / (self.num_heads ** 0.5)\n    x = x @ shift_mlp\n    x = x @ scale_mlp\n    x = x @ gate_mlp\n    return x\n\n", "CodeBleu_score": 30.37271033695199}, {"code": "def post_attention(self, attn, x, gate_msa, shift_mlp, scale_mlp, gate_mlp)\n\t\n    x = x + attn * gate_msa\n    x = self.norm1(x)\n    x = x * self.attn_mask\n    x = self.dropout(x)\n    x = x + self.shift_mlp(x) * shift_mlp\n    x = x + self.scale_mlp(x) * scale_mlp\n    x = x + self.mlp(x) * gate_mlp\n    return x\n\n", "CodeBleu_score": 44.74384169198128}]}
{"instance_id": "d9fdc05542a2", "generation_result": [{"code": "def _block_mixing(context, x, context_block, x_block, c)\n\t\n    # context_block and x_block are the pre-attention blocks\n    # c is the number of attention heads\n    # qkv are the query, key, and value tensors\n    # intermediates are the intermediate tensors\n    # context_qkv and x_qkv are the concatenated qkv tensors\n    # context_intermediates and x_intermediates are the intermediate tensors\n    # context_qkv_split and x_qkv_split are the split qkv tensors\n    # context_out and x_out are the updated context and x\n    # context_post and x_post are the updated context and x after the post-attention blocks\n    # context_out_post and x_out_post are the updated context and x after the post-attention blocks\n\n    context_qkv, context_intermediates = context_block(context)\n    x_qkv, x_intermediates = x_block(x)\n\n    context_qkv_split = rearrange(context_qkv, 'b n (h d) -> b h n d', h=c)\n    x_qkv_split = rearrange(x_qkv, 'b n (h d) -> b h n d', h=c)\n\n    context_out = rearrange(context_qkv_split, 'b h n d -> b n (h d)')\n    x_out = rearrange(x_qkv_split, 'b h n d -> b n (h d)')\n\n    context_out_post = context_post = context_out\n    x_out_post = x_post = x_out\n\n    if context_block.post_attention_block is not None:\n        context_post = context_block.post_attention_block(context_out)\n        context_out_post = context_post\n\n    if x_block.post_attention_block is not None:\n        x_post = x_block.post_attention_block(x_out)\n        x_out_post = x_post\n\n    return context_out_post, x_out_post, context_post, x_post, context_intermediates, x_inter", "CodeBleu_score": 23.472627410371132}, {"code": "def _block_mixing(context, x, context_block, x_block, c)\n\t\n    # pre-attention blocks\n    q_context, k_context, v_context = context_block(context)\n    q_x, k_x, v_x = x_block(x)\n    # concatenate qkv tensors\n    qkv_context = torch.cat([q_context, k_context, v_context], dim=1)\n    qkv_x = torch.cat([q_x, k_x, v_x], dim=1)\n    # optimized attention\n    attention_context, attention_x = c(qkv_context, qkv_x, qkv_context, qkv_x)\n    # split attention results back into context and x parts\n    attention_context_split = attention_context.split(c.out_dims, dim=1)\n    attention_x_split = attention_x.split(c.out_dims, dim=1)\n    # post-attention blocks\n    context = context_block.post_attention(context, attention_context_split[0], attention_context_split[1], attention_context_split[2])\n    x = x_block.post_attention(x, attention_x_split[0], attention_x_split[1], attention_x_split[2])\n    return context, x\n\n", "CodeBleu_score": 19.34088015833966}, {"code": "def _block_mixing(context, x, context_block, x_block, c)\n\t\n    context_qkv = context_block(context)\n    x_qkv = x_block(x)\n    qkv = torch.cat([context_qkv, x_qkv], dim=-1)\n    qkv = rearrange(qkv, 'b (h c) n d -> b h (n c) d', h=c.num_heads)\n    qkv = rearrange(qkv, 'b h (n c) d -> b h n c d', c=c.num_head_channels)\n    attn_output = c.attention(q=qkv, kv=qkv)\n    attn_output = rearrange(attn_output, 'b h n c d -> b h (n c) d', c=c.num_head_channels)\n    context_attn_output, x_attn_output = attn_output.chunk(2, dim=1)\n    context = context_block(context + context_attn_output)\n    x = x_block(x + x_attn_output)\n    return context, x\n\n", "CodeBleu_score": 23.71294858992725}]}
{"instance_id": "b219bce30160", "generation_result": [{"code": "def get_timestep_embedding(timesteps, embedding_dim)\n\t\n    assert len(timesteps.shape) == 1, \"timesteps must be a one-dimensional tensor\"\n    half_dim = embedding_dim // 2\n    emb = math.log(10000) / (half_dim - 1)\n    emb = torch.exp(torch.arange(half_dim, dtype=torch.float32) * -emb)\n    emb = timesteps.float()[:, None] * emb[None, :]\n    emb = torch.cat([emb.sin(), emb.cos()], dim=-1)\n    if embedding_dim % 2 == 1:\n        emb = torch.nn.functional.pad(emb, (0, 1), value=0.0)\n    return emb\n\n", "CodeBleu_score": 72.70820919836206}, {"code": "def get_timestep_embedding(timesteps, embedding_dim)\n\t\n    assert len(timesteps.shape) == 1, \"timestep input must be one-dimensional\"\n    half_dim = embedding_dim // 2\n    emb = math.log(10000) / (half_dim - 1)\n    emb = torch.exp(torch.arange(half_dim, dtype=torch.float32) * -emb)\n    emb = timesteps.unsqueeze(1) @ emb.unsqueeze(0)\n    emb = torch.cat([emb.sin(), emb.cos()], dim=1)\n    if embedding_dim % 2 == 1:\n        emb = torch.cat([emb, torch.zeros(emb.shape[0], 1, dtype=torch.float32, device=emb.device)], dim=1)\n    return emb\n\n", "CodeBleu_score": 64.35831491343905}, {"code": "def get_timestep_embedding(timesteps, embedding_dim)\n\t\n    assert len(timesteps.shape) == 1\n    half_dim = embedding_dim // 2\n    emb = math.log(10000) / (half_dim - 1)\n    emb = torch.exp(torch.arange(half_dim, dtype=torch.float32) * -emb)\n    emb = timesteps.float()[:, None] * emb[None, :]\n    emb = torch.cat([emb.sin(), emb.cos()], dim=-1)\n    if embedding_dim % 2 == 1:\n        emb = torch.cat([emb, torch.zeros_like(emb[:, :1])], dim=1)\n    return emb\n\n", "CodeBleu_score": 71.51668997329168}]}
{"instance_id": "6ae8453c86be", "generation_result": [{"code": "def pytorch_attention(q, k, v)\n\t\n    # Reshape and transpose q, k, v for computation\n    q = q.contiguous().view(q.size(0), q.size(1), q.size(-1)).transpose(1, 2)\n    k = k.contiguous().view(k.size(0), k.size(1), k.size(-1)).transpose(1, 2)\n    v = v.contiguous().view(v.size(0), v.size(1), v.size(-1)).transpose(1, 2)\n\n    # Compute attention\n    attention = torch.bmm(q, k.transpose(1, 2))\n    attention = attention / math.sqrt(k.size(-1))\n\n    # Apply softmax\n    attention = torch.softmax(attention, dim=-1)\n\n    # Compute attention output\n    attention_output = torch.bmm(attention, v)\n\n    # Reshape attention output back to the original dimensions\n    attention_output = attention_output.transpose(1, 2).contiguous().view(q.size(0), q.size(1), v.size(-1))\n    return attention_output\n\n", "CodeBleu_score": 16.337871033841118}, {"code": "def pytorch_attention(q, k, v)\n\t\n    # Reshape and transpose the tensors to prepare them for attention computation\n    q = q.transpose(1, 2)\n    k = k.transpose(1, 2)\n    v = v.transpose(1, 2)\n    # Compute attention using PyTorch's scaled dot-product attention\n    attention = torch.bmm(q, k.transpose(1, 2)) / math.sqrt(k.size(2))\n    # Reshape and transpose the attention tensor back to the original dimensions\n    attention = attention.transpose(1, 2)\n    attention = attention.reshape(attention.size(0), attention.size(1), v.size(1))\n    # Return the attention output reshaped back to the original dimensions\n    return attention\n\n", "CodeBleu_score": 8.251459553270212}, {"code": "def pytorch_attention(q, k, v)\n\t\n    if torch.cuda.is_available():\n        try:\n            return torch.bmm(q.transpose(1, 2), k).div(math.sqrt(k.size(-1))).transpose(1, 2).bmm(v)\n        except RuntimeError:\n            logging.warning(\"OOM exception occurred. Switching to slice attention.\")\n    return comfy.ops.slice_attention(q, k, v)\n\n", "CodeBleu_score": 15.495012112317031}]}
{"instance_id": "fe4433c987ad", "generation_result": [{"code": "def apply_control(h, control, name)\n\t\n    if not exists(control, name):\n        return h\n    if not control[name]:\n        return h\n    try:\n        h = comfy.ops.add(h, control[name])\n    except ValueError:\n        logging.warning(f\"Could not apply {name} control to {h}\")\n    control.pop(name)\n    return h\n\n", "CodeBleu_score": 37.09059797431292}, {"code": "def apply_control(h, control, name)\n\t\n    if exists(control, name):\n        try:\n            h = comfy.ops.add(h, control[name])\n        except:\n            logging.warning(f'Failed to add {name} to {h}')\n    return h\n\n", "CodeBleu_score": 31.24664362007117}, {"code": "def apply_control(h, control, name)\n\t\n    if not exists(control, name):\n        return h\n    if not control[name]:\n        return h\n    try:\n        h = comfy.ops.add(h, control[name])\n    except ValueError as e:\n        logging.warning(f\"Failed to add {name} to {h.name} due to {e}.\")\n    return h\n\n", "CodeBleu_score": 35.24930463132358}]}
{"instance_id": "56da4547c101", "generation_result": [{"code": "def get_codebook_entry(self, indices, shape)\n\t\n    # Create a one-hot encoding of the indices\n    one_hot = torch.zeros(indices.shape[0], self.num_embeddings, dtype=torch.float32, device=indices.device)\n    one_hot.scatter_(1, indices.unsqueeze(1), 1)\n\n    # Multiply the one-hot encoding with the embedding weights to get the quantized vectors\n    quantized = torch.matmul(one_hot, self.embedding.weight.T)\n\n    # If a shape is provided, reshape and permute the quantized vectors to match the original input shape\n    if shape is not None:\n        quantized = quantized.view(shape)\n        quantized = quantized.permute(0, 3, 1, 2)\n\n    return quantized\n\n", "CodeBleu_score": 37.20991341952243}, {"code": "def get_codebook_entry(self, indices, shape)\n\t\n    # print(indices.shape)\n    # print(shape)\n    # print(self.embedding.weight.shape)\n    # print(self.embedding.weight)\n    # print(self.embedding.weight[indices])\n    # print(self.embedding.weight[indices].shape)\n    # print(self.embedding.weight[indices].shape[0])\n    # print(indices.shape[0])\n    # print(torch.zeros(indices.shape[0], self.embedding.weight.shape[0]).scatter(1, indices, 1))\n    # print(torch.zeros(indices.shape[0], self.embedding.weight.shape[0]).scatter(1, indices, 1).shape)\n    # print(torch.zeros(indices.shape[0], self.embedding.weight.shape[0]).scatter(1, indices, 1).shape[0])\n    # print(torch.zeros(indices.shape[0], self.embedding.weight.shape[0]).scatter(1, indices, 1).shape[1])\n    # print(torch.zeros(indices.shape[0], self.embedding.weight.shape[0]).scatter(1, indices, 1).shape[1] == self.embedding.weight.shape[0])\n    # print(torch.zeros(indices.shape[0], self.embedding.weight.shape[0]).scatter(1, indices, 1).shape[1] == self.embedding.weight.shape[0] == indices.shape[0])\n    # print(torch.zeros(indices.shape[0], self.embedding.weight.shape[0]).scatter(1, indices, 1).shape[1] == self.embedding.weight.shape[0] == indices.shape[0] == self.embedding.weight.shape[1])\n    # print(torch.zeros(indices.shape[0], self.embedding.weight.shape[0]).scatter(1, indices, 1).shape[1] == self.embedding.", "CodeBleu_score": 2.0359848484848486}, {"code": "def get_codebook_entry(self, indices, shape)\n\t\n    assert len(indices.shape) == 2\n    assert indices.shape[1] == self.num_embeddings\n    assert len(shape) == 4\n    assert shape[0] == indices.shape[0]\n    assert shape[1] == shape[2] == shape[3] == self.embedding_dim\n    indices_one_hot = torch.zeros(shape[0], shape[1], shape[2], shape[3], device=self.device)\n    indices_one_hot.scatter_(1, indices.unsqueeze(1), 1)\n    quantized_latents = torch.matmul(indices_one_hot, self.embedding.weight.T)\n    if shape[0] != shape[1]:\n        quantized_latents = quantized_latents.permute(0, 2, 3, 1)\n    return quantized_latents\n\n", "CodeBleu_score": 38.31810103298529}]}
{"instance_id": "548f8a1fb37d", "generation_result": [{"code": "def make_noise(self)\n\t\n    noise = torch.randn(4, 4, device=self.device)\n    noise_list = [noise]\n    for i in range(3, self.log_size):\n        noise = F.interpolate(noise, scale_factor=2, mode='bilinear', align_corners=False)\n        noise_list.append(noise)\n        noise_list.append(noise)\n    return noise_list\n\n", "CodeBleu_score": 29.258022083367425}, {"code": "def make_noise(self)\n\t\n    noise_list = [torch.randn(4, 4, device=self.device)]\n    for log_size in range(3, int(math.log2(self.noise_intensity))):\n        noise_list.append(torch.randn(2 ** log_size, 2 ** log_size, device=self.device))\n        noise_list.append(torch.randn(2 ** log_size, 2 ** log_size, device=self.device))\n    return noise_list\n\n", "CodeBleu_score": 24.620298376817107}, {"code": "def make_noise(self)\n\t\n    log_size = 3\n    noise = torch.randn(4, 4, device=self.device)\n    noise_list = [noise]\n    while log_size < 6:\n        noise = torch.randn(2 ** log_size, 2 ** log_size, device=self.device)\n        noise_list.append(noise)\n        noise_list.append(noise)\n        log_size += 1\n    return noise_list\n\n", "CodeBleu_score": 28.531591884975793}]}
{"instance_id": "42fd805563c6", "generation_result": [{"code": "def mean_latent(self, num_latent)\n\t\n    latent_inputs = torch.randn(num_latent, self.latent_dim).to(self.device)\n    latent_outputs = self.style_mlp(latent_inputs)\n    mean_latent = torch.mean(latent_outputs, dim=0)\n    return mean_latent\n\n", "CodeBleu_score": 21.27316220819755}, {"code": "def mean_latent(self, num_latent)\n\t\n    latent_input = torch.randn(num_latent, self.latent_dim).to(self.device)\n    latent_output = self.latent_mlp(latent_input)\n    return torch.mean(latent_output, dim=0)\n\n", "CodeBleu_score": 21.122702760926163}, {"code": "def mean_latent(self, num_latent)\n\t\n    latent = torch.randn(num_latent, self.latent_dim, device=self.device)\n    latent = self.style_mlp(latent)\n    return torch.mean(latent, dim=0)\n\n", "CodeBleu_score": 28.459814276715576}]}
{"instance_id": "2f485fd924af", "generation_result": [{"code": "def color_jitter(img, shift)\n\t\n    img = img.astype(np.float32)\n    img_r, img_g, img_b = cv2.split(img)\n    img_r = shift_color(img_r, shift)\n    img_g = shift_color(img_g, shift)\n    img_b = shift_color(img_b, shift)\n    img = cv2.merge((img_r, img_g, img_b))\n    return img\n\n", "CodeBleu_score": 17.227942079882066}, {"code": "def color_jitter(img, shift)\n\t\n    # Randomly shift values for each channel.\n    shift_r = shift[0]\n    shift_g = shift[1]\n    shift_b = shift[2]\n    img = img.astype(float)\n    img[..., 0] += np.random.uniform(-shift_r, shift_r)\n    img[..., 1] += np.random.uniform(-shift_g, shift_g)\n    img[..., 2] += np.random.uniform(-shift_b, shift_b)\n    img = np.clip(img, 0, 1)\n    return img\n\n", "CodeBleu_score": 28.732617161118263}, {"code": "def color_jitter(img, shift)\n\t\n    img = img.astype(np.float32)\n    img = img / 255.\n    img = img.transpose((2, 0, 1))\n    img = np.reshape(img, (3, -1))\n    img = np.clip(img + shift, 0, 1)\n    img = np.reshape(img, (3, img.shape[1], img.shape[2]))\n    img = img.transpose((1, 2, 0))\n    img = img * 255.\n    return img\n\n", "CodeBleu_score": 36.840819906010665}]}
{"instance_id": "c87ccf1fbf48", "generation_result": [{"code": "def feed_data(self, data)\n\t\n    self.lq = data['lq'].to(self.device)\n    if 'gt' in data:\n        self.gt = data['gt'].to(self.device)\n    if 'eye_l' in data:\n        self.eye_l = data['eye_l'].to(self.device)\n    if 'eye_r' in data:\n        self.eye_r = data['eye_r'].to(self.device)\n    if 'mouth' in data:\n        self.mouth = data['mouth'].to(self.device)\n\n", "CodeBleu_score": 55.77031791323704}, {"code": "def feed_data(self, data)\n\t\n    self.lq = data['lq'].to(self.device)\n    if 'gt' in data:\n        self.gt = data['gt'].to(self.device)\n    if 'face' in data:\n        self.face_lq = data['face'].to(self.device)\n        self.face_gt = data['face_gt'].to(self.device)\n\n", "CodeBleu_score": 53.60130404409777}, {"code": "def feed_data(self, data)\n\t\n    self.lq = data['lq'].to(self.device)\n    if 'gt' in data:\n        self.gt = data['gt'].to(self.device)\n    if 'eye_l' in data:\n        self.eye_l = data['eye_l'].to(self.device)\n    if 'eye_r' in data:\n        self.eye_r = data['eye_r'].to(self.device)\n    if 'mouth' in data:\n        self.mouth = data['mouth'].to(self.device)\n\n", "CodeBleu_score": 55.77031791323704}]}
{"instance_id": "8f8c7cb13037", "generation_result": [{"code": "def construct_img_pyramid(self)\n\t\n    img_pyramid = [self.gt_img]\n    for _ in range(self.num_gpus):\n        img_pyramid.append(F.interpolate(img_pyramid[-1], scale_factor=0.5, mode='bilinear', align_corners=False))\n    return img_pyramid\n\n", "CodeBleu_score": 25.76454914075027}, {"code": "def construct_img_pyramid(self)\n\t\n    img_pyramid = [self.gt]\n    for i in range(self.num_scales):\n        img_pyramid.insert(0, F.interpolate(img_pyramid[0], scale_factor=1 / 2 ** (i + 1), mode='bilinear',\n                                            align_corners=False))\n    return img_pyramid\n\n", "CodeBleu_score": 25.00067179669888}, {"code": "def construct_img_pyramid(self)\n\t\n    img_pyramid = []\n    img_pyramid.append(self.gt.unsqueeze(0))\n    for i in range(self.num_scales - 1):\n        img_pyramid.insert(0, F.interpolate(img_pyramid[0], scale_factor=1 / 2, mode='bilinear', align_corners=True))\n    return img_pyramid\n\n", "CodeBleu_score": 24.76069656555871}]}
{"instance_id": "51247274d40c", "generation_result": [{"code": "def save(self, epoch, current_iter)\n\t\n    if current_iter == 0:\n        self.save_network(epoch, self.net_g, self.opt['path']['models'], 'net_g', self.opt['path']['strict'])\n        self.save_network(epoch, self.net_g_ema, self.opt['path']['models'], 'net_g_ema', self.opt['path']['strict'])\n        self.save_network(epoch, self.net_d, self.opt['path']['models'], 'net_d', self.opt['path']['strict'])\n        if self.opt['train']['gan_type'] == 'lsgan':\n            self.save_network(epoch, self.net_d_l1, self.opt['path']['models'], 'net_d_l1', self.opt['path']['strict'])\n            self.save_network(epoch, self.net_d_l2, self.opt['path']['models'], 'net_d_l2', self.opt['path']['strict'])\n        if self.opt['train']['gan_type'] == 'wgan-gp':\n            self.save_network(epoch, self.net_d_l1, self.opt['path']['models'], 'net_d_l1', self.opt['path']['strict'])\n            self.save_network(epoch, self.net_d_l2, self.opt['path']['models'], 'net_d_l2', self.opt['path']['strict'])\n            self.save_network(epoch, self.net_d_l3, self.opt['path']['models'], 'net_d_l3', self.opt['path']['strict'])\n        if self.opt['train']['gan_type'] == 'gan':\n            self.save_network(epoch, self.net_d_l1, self.opt['path']['models'], 'net_d_l1', self.opt['path']['strict'])\n            self.save_network(epoch, self.net_d_l2, self.opt['path']['models'], 'net_d_l2', self.opt['path']['strict'])\n            self.save_network(epoch", "CodeBleu_score": 35.564564576923125}, {"code": "def save(self, epoch, current_iter)\n\t\n    self.save_network(self.net_g, 'net_g', current_iter, param_key='params_ema')\n    self.save_network(self.net_g, 'net_g', current_iter)\n    self.save_network(self.net_d, 'net_d', current_iter)\n    if hasattr(self, 'net_d_l'):\n        self.save_network(self.net_d_l, 'net_d_l', current_iter)\n        self.save_network(self.net_d_r, 'net_d_r', current_iter)\n        self.save_network(self.net_d_m, 'net_d_m', current_iter)\n    self.save_training_state(epoch, current_iter)\n\n", "CodeBleu_score": 62.9703757260764}, {"code": "def save(self, epoch, current_iter)\n\t\n    if self.ema_decay > 0:\n        self.save_network(self.net_g_a, 'net_g_a', current_iter, param_key=['params', 'params_ema'],\n                          is_best=False)\n        self.save_network(self.net_g_b, 'net_g_b', current_iter, param_key=['params', 'params_ema'],\n                          is_best=False)\n    else:\n        self.save_network(self.net_g_a, 'net_g_a', current_iter, param_key='params', is_best=False)\n        self.save_network(self.net_g_b, 'net_g_b', current_iter, param_key='params', is_best=False)\n\n    if hasattr(self, 'net_d'):\n        self.save_network(self.net_d, 'net_d', current_iter, is_best=False)\n\n    if hasattr(self, 'net_d_l'):\n        self.save_network(self.net_d_l, 'net_d_l', current_iter, is_best=False)\n\n    if hasattr(self, 'net_d_r'):\n        self.save_network(self.net_d_r, 'net_d_r', current_iter, is_best=False)\n\n    if hasattr(self, 'net_d_m'):\n        self.save_network(self.net_d_m, 'net_d_m', current_iter, is_best=False)\n\n    self.save_training_state(epoch, current_iter)\n\n", "CodeBleu_score": 40.78498217708594}]}
{"instance_id": "afa0dc8128c1", "generation_result": [{"code": "def clean_folder(folder)\n\t\n    for f in os.listdir(folder):\n        f = os.path.join(folder, f)\n        try:\n            if os.path.isfile(f):\n                os.unlink(f)\n            elif os.path.isdir(f):\n                shutil.rmtree(f)\n        except Exception as e:\n            print('Failed to delete %s. Reason: %s' % (f, e))\n\n", "CodeBleu_score": 46.48678903161343}, {"code": "def clean_folder(folder)\n\t\n    for item in os.listdir(folder):\n        if os.path.isfile(os.path.join(folder, item)):\n            os.unlink(os.path.join(folder, item))\n        elif os.path.isdir(os.path.join(folder, item)):\n            shutil.rmtree(os.path.join(folder, item))\n\n", "CodeBleu_score": 22.92886878888634}, {"code": "def clean_folder(folder)\n\t\n    for the_file in os.listdir(folder):\n        file_path = os.path.join(folder, the_file)\n        try:\n            if os.path.isfile(file_path):\n                os.unlink(file_path)\n            elif os.path.isdir(file_path):\n                shutil.rmtree(file_path)\n        except Exception as e:\n            print(e)\n\n", "CodeBleu_score": 55.03918217980623}]}
{"instance_id": "53588b481bdd", "generation_result": [{"code": "def next_batch(self, sample_size=1)\n\t\n    if sample_size > len(self.experiences):\n        sample_size = len(self.experiences)\n    batch = random.sample(self.experiences, sample_size)\n    self.experiences = [exp for exp in self.experiences if exp not in batch]\n    return batch\n\n", "CodeBleu_score": 65.3232420810854}, {"code": "def next_batch(self, sample_size=1)\n\t\n    if sample_size > len(self.experience):\n        sample_size = len(self.experience)\n    sample = random.sample(self.experience, sample_size)\n    self.experience.remove(sample)\n    return sample\n\n", "CodeBleu_score": 47.84137910567096}, {"code": "def next_batch(self, sample_size=1)\n\t\n    if sample_size > len(self.experiences):\n        sample_size = len(self.experiences)\n    sample = random.sample(self.experiences, sample_size)\n    self.experiences = [x for x in self.experiences if x not in sample]\n    return sample\n\n", "CodeBleu_score": 72.83338015569785}]}
{"instance_id": "6bfa6a4e621c", "generation_result": [{"code": "def find_estimated_top_configs(self)\n\t\n\n    # Flatten all configurations\n    flat_configs = []\n    for config in self.all_configs:\n        flat_configs.append(flatten_config(config))\n\n    # Collect numerical values from all configurations\n    num_values = []\n    for config in flat_configs:\n        num_values.append(collect_numerical_values(config))\n\n    # Convert to numpy array\n    num_values = np.array(num_values)\n\n    # Predict estimates for each configuration\n    estimates = self.cost_model.predict(num_values)\n\n    # Sort configurations based on estimated cost\n    if self.metric == \"cost\":\n        sorted_indices = np.argsort(estimates)\n    elif self.metric == \"time\":\n        sorted_indices = np.argsort(estimates)[::-1]\n    else:\n        raise ValueError(\"Metric not supported: \" + self.metric)\n\n    # Return indices of top configurations\n    return sorted_indices\n\n", "CodeBleu_score": 35.558090609262045}, {"code": "def find_estimated_top_configs(self)\n\t\n\n    # Flatten all configurations\n    all_configs_flat = []\n    for config in self.all_configs:\n        all_configs_flat.append(flatten_config(config))\n\n    # Collect numerical values\n    all_configs_flat = collect_numerical_values(all_configs_flat)\n\n    # Convert to numpy array\n    all_configs_flat = np.array(all_configs_flat)\n\n    # Predict estimated costs\n    estimated_costs = self.cost_model.predict(all_configs_flat)\n\n    # Sort configurations by estimated costs\n    sorted_estimated_costs = np.argsort(estimated_costs)\n\n    # Get top configurations\n    top_configs = []\n    for config_idx in sorted_estimated_costs[-self.top_k:]:\n        top_configs.append(self.all_configs[config_idx])\n\n    return top_configs\n\n", "CodeBleu_score": 33.20814108786833}, {"code": "def find_estimated_top_configs(self)\n\t\n\n    # flatten all_configs\n    flat_configs = []\n    for config in self.all_configs:\n        flat_configs.append(self.flatten_config(config))\n\n    # collect numerical values\n    all_num_values = []\n    for flat_config in flat_configs:\n        num_values = []\n        for key in flat_config:\n            if isinstance(flat_config[key], numbers.Number):\n                num_values.append(flat_config[key])\n        all_num_values.append(num_values)\n\n    # convert to numpy array\n    all_num_values = np.array(all_num_values)\n\n    # predict estimates\n    estimates = self.cost_model.predict(all_num_values)\n\n    # sort by estimated cost\n    if self.metric == 'cost':\n        indices = np.argsort(estimates)\n    elif self.metric == 'time':\n        indices = np.argsort(estimates)[::-1]\n    else:\n        raise ValueError('metric must be either cost or time')\n\n    # return top configurations\n    top_configs = []\n    for i in indices:\n        top_configs.append(self.unflatten_config(flat_configs[i]))\n\n    return top_configs\n\n", "CodeBleu_score": 37.02603350678926}]}
{"instance_id": "d4cf48413a57", "generation_result": [{"code": "def flatten(d, parent_key='', sep='_')\n\t\n    items = []\n    for key, value in d.items():\n        new_key = parent_key + sep + key if parent_key else key\n        if isinstance(value, collections.abc.Mapping):\n            items.extend(flatten(value, new_key, sep=sep).items())\n        else:\n            items.append((new_key, value))\n    return dict(items)\n\n", "CodeBleu_score": 69.07678763532637}, {"code": "def flatten(d, parent_key='', sep='_')\n\t\n    items = []\n    for key, value in d.items():\n        new_key = parent_key + sep + key if parent_key else key\n        if isinstance(value, collections.abc.MutableMapping):\n            items.extend(flatten(value, new_key, sep=sep).items())\n        else:\n            items.append((new_key, value))\n    return dict(items)\n\n", "CodeBleu_score": 69.18147642440627}, {"code": "def flatten(d, parent_key='', sep='_')\n\t\n    items = []\n    for key, value in d.items():\n        new_key = parent_key + sep + key if parent_key else key\n        if isinstance(value, collections.abc.Mapping):\n            items.extend(flatten(value, new_key, sep=sep).items())\n        else:\n            items.append((new_key, value))\n    return dict(items)\n\n", "CodeBleu_score": 69.07678763532637}]}
{"instance_id": "c2a514249c25", "generation_result": [{"code": "def run_after_tuning(self)\n\t\n\n    if self.autotuner_results is not None and self.autotuner_results['optimal_cmd'] is not None:\n        print(\"Running autotuned command...\")\n        subprocess.Popen(self.autotuner_results['optimal_cmd'], shell=True)\n        print(\"Waiting for command to complete...\")\n        subprocess.Popen(self.autotuner_results['optimal_cmd'], shell=True).wait()\n        print(\"Command completed successfully.\")\n    else:\n        print(\"Autotuning did not find an optimal command.\")\n\n", "CodeBleu_score": 38.1643930106099}, {"code": "def run_after_tuning(self)\n\t\n    if self.optimal_cmd is not None:\n        self.logger.info(\"Running command: {}\".format(self.optimal_cmd))\n        try:\n            subprocess.Popen(self.optimal_cmd, shell=True).wait()\n            self.logger.info(\"Command completed successfully.\")\n        except Exception as e:\n            self.logger.info(\"Command failed to complete: {}\".format(e))\n    else:\n        self.logger.info(\"Autotuning did not find an optimal command.\")\n\n", "CodeBleu_score": 37.98115029920857}, {"code": "def run_after_tuning(self)\n\t\n    if self.optimal_cmd is not None:\n        print(\"Running command: {}\".format(self.optimal_cmd))\n        self.process = subprocess.Popen(self.optimal_cmd, shell=True)\n        self.process.wait()\n        print(\"Command completed successfully.\")\n    else:\n        print(\"Autotuning did not find an optimal configuration.\")\n\n", "CodeBleu_score": 46.62535823333464}]}
{"instance_id": "61802de52df0", "generation_result": [{"code": "def run_job(self, exp: dict, reservations)\n\t\n    exp_name = exp['exp_name']\n    exp_id = exp['exp_id']\n    exp_dir = exp['exp_dir']\n    exp_script = exp['exp_script']\n    exp_args = exp['exp_args']\n    exp_args_mappings = exp['exp_args_mappings']\n    exp_args_defaults = exp['exp_args_defaults']\n    exp_args_constraints = exp['exp_args_constraints']\n    exp_args_values = exp['exp_args_values']\n    exp_args_values_default = exp['exp_args_values_default']\n    exp_args_values_constraints = exp['exp_args_values_constraints']\n    exp_args_values_mappings = exp['exp_args_values_mappings']\n    exp_args_values_defaults = exp['exp_args_values_defaults']\n    exp_args_values_constraints = exp['exp_args_values_constraints']\n    exp_args_values_values = exp['exp_args_values_values']\n    exp_args_values_values_default = exp['exp_args_values_values_default']\n    exp_args_values_values_constraints = exp['exp_args_values_values_constraints']\n    exp_args_values_values_mappings = exp['exp_args_values_values_mappings']\n    exp_args_values_values_defaults = exp['exp_args_values_values_defaults']\n    exp_args_values_values_constraints = exp['exp_args_values_values_constraints']\n    exp_args_values_values_values = exp['exp_args_values_values_values']\n    exp_args_values_values_values_default = exp['exp_args_values_values_values_default']\n    exp_args_values_values_values_constraints = exp['exp_args_values_values_values_constraints']\n    exp_args_values_values_values_mappings = exp['exp_args_values_values_values_mappings']\n    exp_args_values_values_values_defaults", "CodeBleu_score": 8.268384529946104}, {"code": "def run_job(self, exp: dict, reservations)\n\t\n\n    # set experiment master port and result directory\n    exp[\"master_port\"] = self.master_port\n    exp[\"result_dir\"] = self.result_dir\n\n    # update user arguments\n    for arg_map in exp[\"arg_mapping\"]:\n        for arg in arg_map[\"args\"]:\n            if arg in exp[\"user_args\"]:\n                exp[\"user_args\"][arg] = arg_map[\"value\"]\n\n    # create new thread\n    t = threading.Thread(target=self.run_experiment, args=(exp, reservations))\n    t.start()\n\n", "CodeBleu_score": 14.763260311850848}, {"code": "def run_job(self, exp: dict, reservations)\n\t\n\n    # Set the master port and result directory\n    exp['master_port'] = self.master_port\n    exp['result_dir'] = self.result_dir\n\n    # Update user arguments based on argument mappings\n    for arg_map in self.arg_maps:\n        exp[arg_map['user_arg']] = exp[arg_map['exp_arg']]\n\n    # Create a new thread to run the experiment\n    exp_thread = threading.Thread(target=self.run_exp, args=(exp, reservations))\n    exp_thread.start()\n\n", "CodeBleu_score": 11.200795081799916}]}
{"instance_id": "4884e2e9665c", "generation_result": [{"code": "def save_exp_results_to_database(self, message, ranks=None, path=None):\n\t\n    if ranks is None:\n        ranks = list(range(dist.get_world_size()))\n    if path is None:\n        path = self.exp_results_path\n    if not os.path.exists(path):\n        os.makedirs(path)\n    if self.distributed:\n        if self.rank in ranks:\n            with open(os.path.join(path, 'exp_results.json'), 'a') as f:\n                f.write(json.dumps({'rank': self.rank, 'time': time.time(), 'message': message}) + '", "CodeBleu_score": 27.599135983989754}, {"code": "def save_exp_results_to_database(self, message, ranks=None, path=None):\n\t\n    if not self.log_to_database:\n        return\n\n    if ranks is None:\n        ranks = [self.global_rank]\n\n    if path is None:\n        path = self.exp_results_path\n\n    if not os.path.exists(path):\n        os.makedirs(path)\n\n    if self.distributed_rank is None:\n        # single-process\n        with open(path + \"/results.json\", \"a+\") as f:\n            f.write(json.dumps(message) + \"", "CodeBleu_score": 20.478755897529833}, {"code": "def save_exp_results_to_database(self, message, ranks=None, path=None):\n\t\n    if self.args.distributed_world_size > 1:\n        if ranks is None:\n            ranks = dist.get_rank()\n        else:\n            ranks = ranks\n        if ranks is not None:\n            message = f'{message} rank {ranks}'\n        if path is None:\n            path = self.args.exp_results_path\n        else:\n            path = path\n        with open(path, 'a') as f:\n            f.write(f'{message}", "CodeBleu_score": 29.073293822149004}]}
{"instance_id": "0429344c3c4d", "generation_result": [{"code": "def _build_tp_other_layer_map(self, layer_index: int)\n\t\n    # Check if there are any layer files\n    if not os.path.exists(self.layer_path(layer_index)):\n        raise ValueError(f\"Layer {layer_index} does not exist.\")\n\n    # Verify the layer index\n    if layer_index < 0 or layer_index >= self.num_layers:\n        raise ValueError(f\"Layer index {layer_index} is out of bounds.\")\n\n    # Get all files with a specific prefix for the given layer\n    layer_files = os.listdir(self.layer_path(layer_index))\n    layer_files = [f for f in layer_files if f.startswith(self.layer_prefix(layer_index))]\n\n    # Partition the files based on tp_degree\n    tp_other_layer_map: Dict[int, list] = {}\n    for file in layer_files:\n        # Get the tp_degree and partition index\n        tp_degree = int(re.search(r\"\\d+\", file).group())\n        partition_index = int(file.split(\"_\")[-1])\n\n        # Add the file to the corresponding partition\n        if partition_index not in tp_other_layer_map:\n            tp_other_layer_map[partition_index] = []\n        tp_other_layer_map[partition_index].append(file)\n\n    return tp_other_layer_map\n\n", "CodeBleu_score": 30.99418967919439}, {"code": "def _build_tp_other_layer_map(self, layer_index: int)\n\t\n\n    if not os.path.exists(self.layer_dir):\n        raise ValueError(f\"Layer directory {self.layer_dir} does not exist.\")\n\n    files = [f for f in os.listdir(self.layer_dir) if re.match(r\"^layer_[0-9]+_[0-9]+_[0-9]+_[0-9]+_[0-9]+_[0-9]+_[0-9]+_[0-9]+_[0-9]+_[0-9]+_[0-9]+_[0-9]+_[0-9]+_[0-9]+_[0-9]+_[0-9]+_[0-9]+_[0-9]+_[0-9]+_[0-9]+_[0-9]+_[0-9]+_[0-9]+_[0-9]+_[0-9]+_[0-9]+_[0-9]+_[0-9]+_[0-9]+_[0-9]+_[0-9]+_[0-9]+_[0-9]+_[0-9]+_[0-9]+_[0-9]+_[0-9]+_[0-9]+_[0-9]+_[0-9]+_[0-9]+_[0-9]+_[0-9]+_[0-9]+_[0-9]+_[0-9]+_[0-9]+_[0-9]+_[0-9]+_[0-9]+_[0-9]+_[0-9]+_[0-9]+_[0-9]+_[0-9]+_[0-9]+_[0-9]+_[0-9]+_[0-9]+_[0-9]+_[0-9]+_[0-9]+_[0-9]+_[0-9]+_[0-9]+_[0-9]+_[0-9]+_[0-9]+_[0-9]+_[0-9]+_[0-9]+_[0-9]+_[0-9]+_[0-9]+", "CodeBleu_score": 16.16302870442789}, {"code": "def _build_tp_other_layer_map(self, layer_index: int)\n\t\n    # Check if layer files exist\n    if not os.path.exists(self.layer_dir):\n        raise FileNotFoundError(f\"No layer files found in {self.layer_dir}.\")\n    # Check if layer index is valid\n    layer_file_names = os.listdir(self.layer_dir)\n    layer_file_names = [name for name in layer_file_names if re.match(r\"layer_[0-9]+.pt\", name)]\n    layer_file_names = sorted(layer_file_names, key=lambda x: int(x.split(\"_\")[1]))\n    if len(layer_file_names) == 0:\n        raise ValueError(f\"No layer files found for layer index {layer_index}.\")\n    if layer_index < 0:\n        raise ValueError(f\"Layer index {layer_index} is invalid.\")\n    elif layer_index >= len(layer_file_names):\n        raise ValueError(f\"Layer index {layer_index} is invalid.\")\n    # Get layer file names\n    layer_file_names = layer_file_names[layer_index]\n    layer_file_names = [f\"{self.layer_dir}/{layer_file_names}\" for layer_file_names in layer_file_names]\n    # Get tp_degree\n    tp_degree = self.tp_degree\n    # Get partition indices\n    partition_indices = [i for i in range(tp_degree)]\n    # Get file lists\n    file_lists = [[] for i in range(tp_degree)]\n    for file_name in layer_file_names:\n        file_name = file_name.split(\"_\")[1]\n        file_name = int(file_name.split(\".\")[0])\n        file_lists[file_name % tp_degree].append(file_name)\n    # Create data map\n    data_map = {}\n    for i in range(tp_degree):\n        data_map[i] = file_lists[i]\n    return data_map\n\n", "CodeBleu_score": 32.24983526094904}]}
{"instance_id": "1b65ecb9be93", "generation_result": [{"code": "def system_info()\n\t\n    system_info = {}\n    system_info['OS'] = platform.system()\n    system_info['Architecture'] = platform.machine()\n    system_info['Version'] = platform.version()\n    system_info['Processor'] = platform.processor()\n    system_info['Python'] = platform.python_version()\n    return system_info\n\n", "CodeBleu_score": 40.07252261814117}, {"code": "def system_info()\n\t\n    system_info = {}\n    system_info[\"OS\"] = platform.system()\n    system_info[\"Architecture\"] = platform.architecture()[0]\n    system_info[\"Version\"] = platform.version()\n    system_info[\"Processor\"] = platform.processor()\n    system_info[\"Python\"] = platform.python_version()\n    return system_info\n\n", "CodeBleu_score": 41.15484318669744}, {"code": "def system_info()\n\t\n    system_info = {\n        \"OS\": platform.platform(),\n        \"Arch\": platform.architecture()[0],\n        \"Version\": platform.version(),\n        \"Processor\": platform.processor(),\n        \"Python\": platform.python_version(),\n        \"Numpy\": numpy.__version__,\n        \"Torch\": torch.__version__,\n        \"TTS\": TTS.__version__\n    }\n    return system_info\n\n", "CodeBleu_score": 46.53284411350727}]}
{"instance_id": "969bcc5cf3b5", "generation_result": [{"code": "def cuda_info()\n\t\n\n    # CUDA is not available\n    if not torch.cuda.is_available():\n        print('CUDA is not available.')\n        return\n\n    # CUDA is available\n    print('CUDA is available.')\n\n    # CUDA version\n    print('CUDA version:', torch.version.cuda)\n\n    # GPU device names\n    device_names = torch.cuda.get_device_name(0)\n    print('GPU device name:', device_names)\n\n    # GPU device count\n    device_count = torch.cuda.device_count()\n    print('GPU device count:', device_count)\n\n    # CUDA version\n    print('CUDA version:', torch.version.cuda)\n\n    # CUDA version\n    print('CUDA version:', torch.version.cuda)\n\n    # CUDA version\n    print('CUDA version:', torch.version.cuda)\n\n    # CUDA version\n    print('CUDA version:', torch.version.cuda)\n\n", "CodeBleu_score": 40.61290546234346}, {"code": "def cuda_info()\n\t\n    cuda_device_names = []\n    for i in range(torch.cuda.device_count()):\n        cuda_device_names.append(torch.cuda.get_device_name(i))\n    cuda_device_names = \", \".join(cuda_device_names)\n    cuda_available = torch.cuda.is_available()\n    cuda_version = torch.version.cuda\n    return cuda_device_names, cuda_available, cuda_version\n\n", "CodeBleu_score": 42.57478530631974}, {"code": "def cuda_info()\n\t\n    print('GPU device names:', end=' ')\n    for i in range(torch.cuda.device_count()):\n        print(torch.cuda.get_device_name(i))\n    print()\n    print('CUDA available:', torch.cuda.is_available())\n    print('CUDA version:', torch.version.cuda)\n\n", "CodeBleu_score": 47.96581346294185}]}
{"instance_id": "a2ba908a4c70", "generation_result": [{"code": "def compute_encoder_accuracy(dataset_items, encoder_manager)\n\t\n    class_name_key = encoder_manager.config[\"class_name_key\"]\n    class_id_key = encoder_manager.config[\"class_id_key\"]\n    class_names = encoder_manager.config[\"class_names\"]\n    class_id_to_name = encoder_manager.config[\"class_id_to_name\"]\n    class_name_to_id = encoder_manager.config[\"class_name_to_id\"]\n    class_name_to_accuracy = {}\n    for class_name in class_names:\n        class_name_to_accuracy[class_name] = 0\n    total_accuracy = 0\n    total_samples = 0\n    for dataset_item in tqdm(dataset_items, desc=\"Computing encoder accuracy\"):\n        audio_file = dataset_item[0]\n        class_name = dataset_item[1][class_name_key]\n        class_id = dataset_item[1][class_id_key] if class_id_key in dataset_item[1] else None\n        embedding = encoder_manager.embedding_from_file(audio_file)\n        prediction = encoder_manager.predict_class(embedding)\n        if class_id is not None:\n            if prediction == class_id:\n                class_name_to_accuracy[class_name] += 1\n                total_accuracy += 1\n            total_samples += 1\n        else:\n            if prediction == class_name:\n                class_name_to_accuracy[class_name] += 1\n                total_accuracy += 1\n            total_samples += 1\n    for class_name in class_names:\n        class_name_to_accuracy[class_name] = class_name_to_accuracy[class_name] / total_samples\n    print(\"Encoder accuracy:\")\n    for class_name in class_names:\n        print(f\"{class_name}: {class_name_to_accuracy[class_name]:.2%}\")\n    print(f\"Average accuracy: {total_accuracy /", "CodeBleu_score": 29.320216193815153}, {"code": "def compute_encoder_accuracy(dataset_items, encoder_manager)\n\t\n\n    # Get the class name to class ID mapping from the encoder manager's configuration\n    class_name_to_class_id_mapping = encoder_manager.get_class_name_to_class_id_mapping()\n\n    # Initialize the accuracy counters\n    class_name_to_accuracy_counter_mapping = {}\n    total_accuracy_counter = 0\n\n    # For each item in the dataset\n    for item in tqdm(dataset_items, desc='Computing encoder accuracy'):\n        # Extract the audio file\n        audio_file = item['audio_file']\n\n        # Compute the embedding\n        embedding = encoder_manager.compute_embedding(audio_file)\n\n        # Predict the class label\n        predicted_label = encoder_manager.predict_class_label(embedding)\n\n        # If the predicted label is available\n        if predicted_label is not None:\n            # If the class ID to class name mapping is available\n            if class_name_to_class_id_mapping is not None:\n                # Get the class name from the class ID\n                predicted_class_name = class_name_to_class_id_mapping[predicted_label]\n\n                # If the class name is available\n                if predicted_class_name is not None:\n                    # If the class name to accuracy counter mapping is available\n                    if predicted_class_name in class_name_to_accuracy_counter_mapping:\n                        # Increment the accuracy counter\n                        class_name_to_accuracy_counter_mapping[predicted_class_name] += 1\n\n                    # Otherwise, initialize the accuracy counter\n                    else:\n                        class_name_to_accuracy_counter_mapping[predicted_class_name] = 1\n\n                    # Increment the total accuracy counter\n                    total_accuracy_counter += 1\n\n    # For each class name\n    for class_name in class_name_to_accuracy_counter_mapping:\n        # Compute the accuracy\n        accuracy = class_name_to_accuracy_counter_mapping[class_name] / total_accuracy_", "CodeBleu_score": 26.715746785430188}, {"code": "def compute_encoder_accuracy(dataset_items, encoder_manager)\n\t\n    # Get the class name key and optional class ID to class name mapping from the encoder manager's configuration\n    class_name_key = encoder_manager.config[\"class_name_key\"]\n    class_id_to_class_name_mapping = encoder_manager.config.get(\"class_id_to_class_name_mapping\", None)\n    # Initialize the accuracy counters for each class\n    class_accuracies = {}\n    for class_name in dataset_items.class_names:\n        class_accuracies[class_name] = 0\n    # Compute the encoder accuracy for each item in the dataset\n    for item in tqdm(dataset_items):\n        # Extract the audio file from the item\n        audio_file = item[dataset_items.audio_key]\n        # Compute the embedding for the audio file\n        embedding = encoder_manager.compute_embedding(audio_file)\n        # Predict the class label if criteria and mappings are available\n        if class_name_key is not None and class_id_to_class_name_mapping is not None:\n            predicted_class_label = encoder_manager.predict_class_label(embedding, class_name_key)\n            if predicted_class_label is not None:\n                # Update the accuracy counter for the predicted class label\n                class_accuracies[class_id_to_class_name_mapping[predicted_class_label]] += 1\n        # Update the accuracy counter for the true class name\n        class_accuracies[item[dataset_items.class_name_key]] += 1\n    # Calculate and print the accuracy for each class and the average accuracy across all classes\n    total_samples = len(dataset_items)\n    print(\"Accuracy for each class:\")\n    for class_name in dataset_items.class_names:\n        print(f\"{class_name}: {class_accuracies[class_name] / total_samples:.2f}\")\n    print(f\"Average accuracy across all classes: {sum(class_accuracies.values()) / total_samples:.2f}\")\n\n", "CodeBleu_score": 29.201848940479803}]}
{"instance_id": "e01ec69ad58a", "generation_result": [{"code": "def adjust_path_and_remove_silence(audio_path)\n\t\n    audio_path = pathlib.Path(audio_path)\n    if not audio_path.exists():\n        print(f\"{audio_path} does not exist. Skipping...\")\n        return None, False\n    audio_name = audio_path.name\n    audio_name = audio_name.replace(\".wav\", \"\")\n    audio_name = audio_name.replace(\".mp3\", \"\")\n    audio_name = audio_name.replace(\".flac\", \"\")\n    audio_name = audio_name.replace(\".m4a\", \"\")\n    audio_name = audio_name.replace(\".mp4\", \"\")\n    audio_name = audio_name.replace(\".aac\", \"\")\n    audio_name = audio_name.replace(\".ogg\", \"\")\n    audio_name = audio_name.replace(\".opus\", \"\")\n    audio_name = audio_name.replace(\".amr\", \"\")\n    audio_name = audio_name.replace(\".wma\", \"\")\n    audio_name = audio_name.replace(\".3gp\", \"\")\n    audio_name = audio_name.replace(\".mka\", \"\")\n    audio_name = audio_name.replace(\".mkv\", \"\")\n    audio_name = audio_name.replace(\".aiff\", \"\")\n    audio_name = audio_name.replace(\".au\", \"\")\n    audio_name = audio_name.replace(\".raw\", \"\")\n    audio_name = audio_name.replace(\".dvf\", \"\")\n    audio_name = audio_name.replace(\".gsm\", \"\")\n    audio_name = audio_name.replace(\".gsm\", \"\")\n    audio_name = audio_name.replace(\".tta\", \"\")\n    audio_name = audio_name.replace(\".tta\", \"\")\n    audio_name = audio_name.replace(\".tta\", \"\")\n    audio_name = audio_name.replace(\".tta\", \"\")\n    audio_name = audio_name.replace(\".tta\", \"\")\n    audio_name = audio_name.replace(\".tta\", \"\")\n    audio_name = audio_name.replace(\".tta\", \"\")\n    audio_name = audio_name.replace(\".tta\", \"\")\n    audio_name = audio_name.replace(\".tta\", \"\")\n    audio_name = audio_name.replace(\".", "CodeBleu_score": 21.559583447426007}, {"code": "def adjust_path_and_remove_silence(audio_path)\n\t\n\n    # Get the output path\n    audio_path_out = audio_path.replace(args.input_dir, args.output_dir)\n    audio_path_out = audio_path_out.replace(\".wav\", \".vad.wav\")\n    if os.path.exists(audio_path_out) and not args.force:\n        return audio_path_out, False\n\n    # Create the output directory if it does not exist\n    if not os.path.exists(os.path.dirname(audio_path_out)):\n        os.makedirs(os.path.dirname(audio_path_out))\n\n    # Remove silence from the audio\n    audio, speech_detected = remove_silence(audio_path, args.vad_model, args.vad_threshold, args.vad_min_silence,\n                                            args.vad_min_speech, args.vad_max_speech, args.vad_min_duration,\n                                            args.vad_max_duration, args.vad_min_pause, args.vad_max_pause,\n                                            args.vad_min_non_silence, args.vad_max_non_silence, args.vad_min_non_speech,\n                                            args.vad_max_non_speech, args.vad_min_speech_or_pause,\n                                            args.vad_max_speech_or_pause)\n\n    # Save the processed audio\n    torch.save(audio, audio_path_out)\n\n    return audio_path_out, speech_detected\n\n", "CodeBleu_score": 38.21275316459687}, {"code": "def adjust_path_and_remove_silence(audio_path)\n\t\n    # get the output path\n    audio_path = audio_path.replace(args.input_dir, args.output_dir)\n    # remove silence if necessary\n    if args.remove_silence:\n        audio, detected_speech = remove_silence(audio_path, args.vad_model, args.vad_threshold, args.vad_chunk_size, args.vad_context, args.vad_overlap)\n    else:\n        detected_speech = True\n    return audio_path, detected_speech\n\n", "CodeBleu_score": 23.956335669728237}]}
{"instance_id": "7e0bdb0feff8", "generation_result": [{"code": "def resample_files(input_dir, output_sr, output_dir=None, file_ext=\"wav\", n_jobs=10)\n\t\n\n    if not os.path.exists(input_dir):\n        raise FileNotFoundError(\"The input directory does not exist.\")\n\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    if not os.path.exists(output_dir):\n        raise FileNotFoundError(\"The output directory does not exist.\")\n\n    if not os.path.isdir(output_dir):\n        raise NotADirectoryError(\"The output directory is not a directory.\")\n\n    if not os.path.isdir(input_dir):\n        raise NotADirectoryError(\"The input directory is not a directory.\")\n\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    copytree(input_dir, output_dir)\n\n    input_files = glob.glob(os.path.join(output_dir, \"**/*.\" + file_ext), recursive=True)\n\n    if n_jobs == 1:\n        for file in tqdm(input_files):\n            librosa.resample(file, output_sr)\n    else:\n        with Pool(n_jobs) as p:\n            p.map(librosa.resample, input_files, [output_sr] * len(input_files))\n\n    print(\"Resampling complete.\")\n\n", "CodeBleu_score": 37.12487035675583}, {"code": "def resample_files(input_dir, output_sr, output_dir=None, file_ext=\"wav\", n_jobs=10)\n\t\n\n    if output_dir is None:\n        output_dir = input_dir\n    else:\n        copytree(input_dir, output_dir)\n\n    files = glob.glob(os.path.join(output_dir, \"**\", \"*.\" + file_ext), recursive=True)\n    with Pool(n_jobs) as p:\n        p.map(resample, files)\n\n    print(\"Resampling complete!\")\n\n", "CodeBleu_score": 25.902097000244744}, {"code": "def resample_files(input_dir, output_sr, output_dir=None, file_ext=\"wav\", n_jobs=10)\n\t\n\n    # Check if input directory exists\n    if not os.path.exists(input_dir):\n        raise FileNotFoundError(\"Input directory does not exist.\")\n\n    # Check if output directory exists\n    if output_dir is not None and not os.path.exists(output_dir):\n        raise FileNotFoundError(\"Output directory does not exist.\")\n\n    # Check if input directory and output directory are the same\n    if input_dir == output_dir:\n        raise ValueError(\"Input directory and output directory are the same.\")\n\n    # Check if output directory exists and is empty\n    if output_dir is not None and os.path.exists(output_dir) and os.listdir(output_dir):\n        raise FileExistsError(\"Output directory exists and is not empty.\")\n\n    # Check if file extension is valid\n    if file_ext != \"wav\" and file_ext != \"mp3\" and file_ext != \"flac\":\n        raise ValueError(\"Invalid file extension.\")\n\n    # Check if number of jobs is valid\n    if n_jobs < 1:\n        raise ValueError(\"Number of jobs must be greater than or equal to 1.\")\n\n    # Check if sample rate is valid\n    if output_sr < 8000 or output_sr > 96000:\n        raise ValueError(\"Invalid sample rate.\")\n\n    # If output directory does not exist, create it\n    if output_dir is not None:\n        os.makedirs(output_dir, exist_ok=True)\n\n    # Copy input directory to output directory\n    if output_dir is not None:\n        copytree(input_dir, output_dir)\n\n    # Find all audio files in input directory\n    audio_files = glob.glob(os.path.join(input_dir, \"*.\" + file_ext))\n\n    # Resample audio files in parallel\n    with Pool(n_jobs) as p:\n        p.map(resample_file, zip(audio_files, [output_sr] * len(audio_files)))\n\n    # Print completion message\n    print(\"Resampling completed.\")\n\n", "CodeBleu_score": 33.12758025892203}]}
{"instance_id": "a1351834ca57", "generation_result": [{"code": "def evaluation(model, criterion, data_loader, global_step)\n\t\n    model.eval()\n    loss_sum = 0.0\n    with torch.no_grad():\n        for batch_idx, (inputs, labels) in enumerate(data_loader):\n            inputs = inputs.to(model.device)\n            labels = labels.to(model.device)\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss_sum += loss.item()\n    loss_avg = loss_sum / len(data_loader)\n    print(f'[EVAL] global_step={global_step} loss={loss_avg:.4f}')\n    return loss_avg\n\n", "CodeBleu_score": 20.18131321155584}, {"code": "def evaluation(model, criterion, data_loader, global_step)\n\t\n    # Initialise average evaluation loss\n    eval_loss = 0\n\n    # Dispatch data to the GPU if available\n    data_loader.collate_fn.to(model.device)\n\n    # Evaluate the model\n    with torch.no_grad():\n        # Iterate over batches\n        for batch in data_loader:\n            # Reshape inputs and labels to group samples of each class\n            inputs = batch['inputs'].reshape(-1, batch['inputs'].shape[-1])\n            labels = batch['labels'].reshape(-1)\n\n            # Dispatch data to the GPU if available\n            inputs = inputs.to(model.device)\n            labels = labels.to(model.device)\n\n            # Compute the loss for the batch\n            loss = criterion(model(inputs), labels)\n\n            # Update average evaluation loss\n            eval_loss += loss.item()\n\n    # Calculate average evaluation loss\n    eval_loss /= len(data_loader)\n\n    # Log evaluation statistics\n    print(f\"Evaluation loss: {eval_loss:.4f}\")\n\n    # Generate and log UMAP plot of the embeddings from the last batch\n    plot_embeddings(model, data_loader, global_step)\n\n    # Return average evaluation loss\n    return eval_loss\n\n", "CodeBleu_score": 23.20932957720015}, {"code": "def evaluation(model, criterion, data_loader, global_step)\n\t\n\n    # Set model to evaluation mode\n    model.eval()\n\n    # Initialize evaluation loss\n    eval_loss = 0\n\n    # Initialize evaluation accuracy\n    eval_accuracy = 0\n\n    # Initialize evaluation accuracy\n    eval_accuracy_per_class = torch.zeros(model.num_classes)\n\n    # Initialize number of samples\n    num_samples = 0\n\n    # Initialize number of correct predictions\n    num_correct_predictions = 0\n\n    # Initialize number of correct predictions per class\n    num_correct_predictions_per_class = torch.zeros(model.num_classes)\n\n    # Initialize number of samples per class\n    num_samples_per_class = torch.zeros(model.num_classes)\n\n    # Initialize number of samples per class\n    num_samples_per_class_per_batch = torch.zeros(model.num_classes)\n\n    # Initialize number of samples per class\n    num_samples_per_class_per_batch_per_class = torch.zeros(model.num_classes)\n\n    # Initialize number of samples per class\n    num_samples_per_class_per_batch_per_class_per_batch = torch.zeros(model.num_classes)\n\n    # Initialize number of samples per class\n    num_samples_per_class_per_batch_per_class_per_batch_per_class = torch.zeros(model.num_classes)\n\n    # Initialize number of samples per class\n    num_samples_per_class_per_batch_per_class_per_batch_per_class_per_batch = torch.zeros(model.num_classes)\n\n    # Initialize number of samples per class\n    num_samples_per_class_per_batch_per_class_per_batch_per_class_per_batch_per_class = torch.zeros(model.num_classes)\n\n    # Initialize number of samples per class\n    num_samples_per_class_per_batch_per_class_per_batch_per_class_per_batch_per_class_per_class = torch.zeros(", "CodeBleu_score": 14.55612265766165}]}
{"instance_id": "565f8ed09916", "generation_result": [{"code": "def register_config(model_name: str) -> Coqpit\n\t\n\n    config_class_name = \"\".join([model_name.capitalize(), \"Config\"])\n    if model_name == \"xtts\":\n        config_class_name = \"XttsConfig\"\n\n    config_class = find_module(\n        \"TTS.config\", config_class_name, \"configs\", \"config\", \"config.py\"\n    )\n\n    return config_class\n\n", "CodeBleu_score": 20.09352241646636}, {"code": "def register_config(model_name: str) -> Coqpit\n\t\n\n    # check if model is xtts\n    if model_name == \"xtts\":\n        config_class = XttsConfig\n        return config_class\n\n    # construct configuration class name\n    config_class_name = f\"{model_name.capitalize()}Config\"\n\n    # check if model is in the predefined paths\n    for path in PREDEFINED_CONFIG_PATHS:\n        # try to import configuration class\n        try:\n            config_class = find_module(config_class_name, path)\n            return config_class\n        # if configuration class is not found, continue to next path\n        except ModuleNotFoundError:\n            pass\n\n    # if configuration class is not found, raise ModuleNotFoundError\n    raise ModuleNotFoundError(f\"Configuration class {config_class_name} not found.\")\n\n", "CodeBleu_score": 34.94055709947681}, {"code": "def register_config(model_name: str) -> Coqpit\n\t\n    # construct configuration class name\n    config_class_name = model_name + \"Config\"\n    # import configuration class\n    config_class = None\n    # try to import configuration class\n    try:\n        config_class = find_module(config_class_name, [\"TTS.config.models\", \"TTS.config.models.xtts\"])\n    except ModuleNotFoundError:\n        raise ModuleNotFoundError(\n            \"No configuration class found for model \" + model_name + \"! Please check if you have installed the model.\"\n        )\n    return config_class\n\n", "CodeBleu_score": 28.126485602600393}]}
{"instance_id": "0ea7862506a7", "generation_result": [{"code": "def load_config(config_path: str) -> Coqpit\n\t\n    if config_path is None:\n        raise ValueError(\"No configuration file specified.\")\n    if not os.path.exists(config_path):\n        raise ValueError(\"Configuration file does not exist.\")\n    config_name = os.path.basename(config_path)\n    config_name, _ = os.path.splitext(config_name)\n    config_ext = os.path.splitext(config_path)[1]\n    if config_ext == \".yaml\":\n        with open(config_path, \"r\") as f:\n            config_dict = yaml.load(f, Loader=yaml.SafeLoader)\n    elif config_ext == \".json\":\n        with open(config_path, \"r\") as f:\n            config_dict = json.load(f)\n    else:\n        raise ValueError(f\"Unknown configuration type: {config_ext}\")\n    config_dict = _process_config(config_dict)\n    config_class = _get_config_class(config_dict)\n    config = config_class(**config_dict)\n    return config\n\n", "CodeBleu_score": 47.92257808314441}, {"code": "def load_config(config_path: str) -> Coqpit\n\t\n    if not os.path.exists(config_path):\n        raise FileNotFoundError(f\"The configuration file {config_path} does not exist.\")\n    file_name = os.path.basename(config_path)\n    file_extension = file_name.split(\".\")[-1]\n    if file_extension == \"yaml\":\n        with open(config_path, \"r\") as file:\n            config_dict = yaml.safe_load(file)\n    elif file_extension == \"json\":\n        with open(config_path, \"r\") as file:\n            config_dict = json.load(file)\n    else:\n        raise ValueError(f\"The configuration file {config_path} has unknown type {file_extension}.\")\n    model_name = config_dict.pop(\"model\")\n    config_class = find_module(model_name, CONFIG_PATH)\n    config = config_class(**config_dict)\n    return config\n\n", "CodeBleu_score": 43.759647925514564}, {"code": "def load_config(config_path: str) -> Coqpit\n\t\n\n    config_extension = config_path.split('.')[-1]\n    if config_extension == 'yaml':\n        with open(config_path, 'r') as f:\n            config_dict = yaml.load(f, Loader=yaml.FullLoader)\n    elif config_extension == 'json':\n        with open(config_path, 'r') as f:\n            config_dict = json.load(f)\n    else:\n        raise ValueError(f'Unknown configuration type: {config_extension}')\n\n    # get model name\n    model_name = config_dict['model_name']\n    # register configuration class\n    config_class = find_module(f'TTS.config.{model_name}_config')\n    # create configuration object\n    config = config_class(config_dict)\n    return config\n\n", "CodeBleu_score": 37.499256447557464}]}
{"instance_id": "e0db68b63143", "generation_result": [{"code": "def list_files(basePath, validExts=None, contains=None)\n\t\n    # Check if the basePath is a directory\n    if not os.path.isdir(basePath):\n        raise NotADirectoryError(basePath)\n    # Iterate files in the base directory\n    for root, dirs, files in os.walk(basePath):\n        for file in files:\n            # Check if the file is in the valid extensions\n            if validExts is not None:\n                file_ext = os.path.splitext(file)[1]\n                if file_ext not in validExts:\n                    continue\n            # Check if the file contains a specified string\n            if contains is not None:\n                if contains not in file:\n                    continue\n            # Construct the full file path and yield it\n            full_path = os.path.join(root, file)\n            yield full_path\n\n", "CodeBleu_score": 35.07555724018496}, {"code": "def list_files(basePath, validExts=None, contains=None)\n\t\n    for root, dirs, files in os.walk(basePath):\n        for name in files:\n            if contains is not None:\n                if contains not in name:\n                    continue\n            if validExts is not None:\n                ext = os.path.splitext(name)[1]\n                if not ext.lower() in validExts:\n                    continue\n            yield os.path.join(root, name)\n\n", "CodeBleu_score": 36.84322427230353}, {"code": "def list_files(basePath, validExts=None, contains=None)\n\t\n    if validExts is None:\n        validExts = []\n    if contains is None:\n        contains = \"\"\n    for root, dirs, files in os.walk(basePath):\n        for file in files:\n            if file.endswith(tuple(validExts)):\n                if contains in file:\n                    yield os.path.join(root, file)\n\n", "CodeBleu_score": 32.56600860196446}]}
{"instance_id": "b836fcaf0a27", "generation_result": [{"code": "def load_model(xtts_checkpoint, xtts_config, xtts_vocab)\n\t\n    if not xtts_checkpoint or not xtts_config or not xtts_vocab:\n        return \"Please provide a checkpoint, config, and vocab path.\"\n    logging.info(\"Clearing GPU cache...\")\n    torch.cuda.empty_cache()\n    logging.info(\"Loading configuration...\")\n    config = XttsConfig.from_json_file(xtts_config)\n    logging.info(\"Initializing model...\")\n    model = Xtts(config, xtts_vocab)\n    logging.info(\"Loading model checkpoint...\")\n    checkpoint = torch.load(xtts_checkpoint, map_location=\"cpu\")\n    model.load_state_dict(checkpoint[\"model\"])\n    model.to(config.device)\n    logging.info(\"Model loaded!\")\n    return model\n\n", "CodeBleu_score": 51.63879491734155}, {"code": "def load_model(xtts_checkpoint, xtts_config, xtts_vocab)\n\t\n    # Clear GPU cache\n    torch.cuda.empty_cache()\n    # Check paths\n    if not os.path.exists(xtts_checkpoint):\n        raise ValueError(f\"XTTS checkpoint not found: {xtts_checkpoint}\")\n    if not os.path.exists(xtts_config):\n        raise ValueError(f\"XTTS config not found: {xtts_config}\")\n    if not os.path.exists(xtts_vocab):\n        raise ValueError(f\"XTTS vocab not found: {xtts_vocab}\")\n    # Load configuration\n    logging.info(f\"Loading XTTS configuration from {xtts_config}\")\n    xtts_config = XttsConfig.from_json_file(xtts_config)\n    # Initialize model\n    logging.info(\"Initializing XTTS model\")\n    xtts = Xtts(xtts_config)\n    # Load checkpoint\n    logging.info(f\"Loading XTTS checkpoint from {xtts_checkpoint}\")\n    xtts.load_state_dict(torch.load(xtts_checkpoint, map_location=\"cpu\"))\n    # Load vocab\n    logging.info(f\"Loading XTTS vocab from {xtts_vocab}\")\n    xtts.load_vocabulary(xtts_vocab)\n    # Move model to GPU\n    if torch.cuda.is_available():\n        logging.info(\"Moving XTTS model to GPU\")\n        xtts = xtts.cuda()\n    # Return success message\n    logging.info(\"XTTS model loaded successfully\")\n    return xtts\n\n", "CodeBleu_score": 42.334331098117886}, {"code": "def load_model(xtts_checkpoint, xtts_config, xtts_vocab)\n\t\n    if not xtts_checkpoint:\n        print(\"XTTS checkpoint path not provided!\")\n        sys.exit()\n    if not xtts_config:\n        print(\"XTTS config path not provided!\")\n        sys.exit()\n    if not xtts_vocab:\n        print(\"XTTS vocab path not provided!\")\n        sys.exit()\n    logging.info(\"Clearing GPU cache...\")\n    torch.cuda.empty_cache()\n    logging.info(\"XTTS checkpoint path: {}\".format(xtts_checkpoint))\n    logging.info(\"XTTS config path: {}\".format(xtts_config))\n    logging.info(\"XTTS vocab path: {}\".format(xtts_vocab))\n    logging.info(\"Loading XTTS model...\")\n    logging.info(\"Loading XTTS configuration...\")\n    config = XttsConfig.from_json_file(xtts_config)\n    logging.info(\"Initializing XTTS model...\")\n    model = Xtts(config)\n    logging.info(\"Loading XTTS model checkpoint...\")\n    checkpoint = torch.load(xtts_checkpoint, map_location=\"cpu\")\n    model.load_state_dict(checkpoint[\"model\"])\n    logging.info(\"Loading XTTS vocabulary...\")\n    model.load_vocabulary(xtts_vocab)\n    logging.info(\"Successfully loaded XTTS model!\")\n    return model\n\n", "CodeBleu_score": 47.79480279600586}]}
{"instance_id": "c5d63aa0df26", "generation_result": [{"code": "def compute_embedding(self, x, num_frames=250, num_eval=10, return_mean=True, l2_norm=True)\n\t\n    # Adjust number of frames based on hop length if using torch specifications\n    if self.hop_length is not None:\n        num_frames = int(num_frames * self.hop_length / self.frame_length)\n    # Ensure number of frames does not exceed input length\n    num_frames = min(num_frames, x.shape[-1])\n    # Calculate offset positions for evaluation\n    offsets = torch.arange(0, num_frames, num_eval)\n    # Extract frames at these offsets, concatenate them into a batch, and perform inference\n    x = self.extract_frames(x, offsets)\n    x = self.model(x)\n    # Compute mean of embeddings\n    if return_mean:\n        x = x.mean(dim=1)\n    # Normalize embeddings with L2 norm\n    if l2_norm:\n        x = nn.functional.normalize(x, dim=1)\n    return x\n\n", "CodeBleu_score": 27.571821963096948}, {"code": "def compute_embedding(self, x, num_frames=250, num_eval=10, return_mean=True, l2_norm=True)\n\t\n    # if self.spec_type == 'torch':\n    #     num_frames = int(num_frames * self.hop_length)\n    # elif self.spec_type == 'librosa':\n    #     num_frames = int(num_frames * self.hop_length / 1000)\n    if num_frames > x.shape[1]:\n        num_frames = x.shape[1]\n    offset_positions = torch.arange(0, num_frames, num_frames // num_eval)\n    if return_mean:\n        embeddings = []\n    for i in range(num_eval):\n        frame = x[:, offset_positions[i]:offset_positions[i] + num_frames]\n        embedding = self.forward(frame)\n        if return_mean:\n            embeddings.append(embedding)\n    if return_mean:\n        embeddings = torch.stack(embeddings)\n        return embeddings.mean(dim=0)\n    else:\n        return embeddings\n\n", "CodeBleu_score": 31.902029933133203}, {"code": "def compute_embedding(self, x, num_frames=250, num_eval=10, return_mean=True, l2_norm=True)\n\t\n    # Adjust number of frames based on hop length if using torch specifications\n    if self.hop_length is not None:\n        num_frames = int(num_frames / self.hop_length)\n    # Ensure number of frames does not exceed input length\n    if num_frames > x.shape[-1]:\n        num_frames = x.shape[-1]\n    # Calculate offsets for evaluation\n    offset = int(num_frames / 2)\n    offsets = [offset * i for i in range(num_eval)]\n    # Extract frames at offsets\n    frames = [x[:, i:i + num_frames] for i in offsets]\n    # Concatenate frames into batch\n    frames = torch.cat(frames, dim=1)\n    # Perform inference to obtain embeddings\n    embeddings = self.model(frames)\n    # Compute mean of embeddings if specified\n    if return_mean:\n        embeddings = embeddings.mean(dim=1)\n        if l2_norm:\n            embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)\n    return embeddings\n\n", "CodeBleu_score": 40.601002231353675}]}
{"instance_id": "06f5b0335b26", "generation_result": [{"code": "def create_layer(self, block, planes, blocks, stride=1)\n\t\n    downsample = None\n    if stride != 1 or self.inplanes != planes * block.expansion:\n        downsample = nn.Sequential(\n            nn.Conv2d(self.inplanes, planes * block.expansion,\n                      kernel_size=1, stride=stride, bias=False),\n            nn.BatchNorm2d(planes * block.expansion),\n        )\n\n    layers = []\n    layers.append(block(self.inplanes, planes, stride, downsample))\n    self.inplanes = planes * block.expansion\n    for i in range(1, blocks):\n        layers.append(block(self.inplanes, planes))\n\n    return nn.Sequential(*layers)\n\n", "CodeBleu_score": 85.18641475692186}, {"code": "def create_layer(self, block, planes, blocks, stride=1)\n\t\n    downsample = None\n    if stride != 1 or self.inplanes != planes * block.expansion:\n        downsample = nn.Sequential(\n            nn.Conv2d(self.inplanes, planes * block.expansion,\n                      kernel_size=1, stride=stride, bias=False),\n            nn.BatchNorm2d(planes * block.expansion),\n        )\n\n    layers = []\n    layers.append(block(self.inplanes, planes, stride, downsample))\n    self.inplanes = planes * block.expansion\n    for i in range(1, blocks):\n        layers.append(block(self.inplanes, planes))\n\n    return nn.Sequential(*layers)\n\n", "CodeBleu_score": 85.18641475692186}, {"code": "def create_layer(self, block, planes, blocks, stride=1)\n\t\n    downsample = None\n    if stride != 1 or self.inplanes != planes * block.expansion:\n        downsample = nn.Sequential(\n            nn.Conv2d(self.inplanes, planes * block.expansion,\n                      kernel_size=1, stride=stride, bias=False),\n            nn.BatchNorm2d(planes * block.expansion),\n        )\n\n    layers = []\n    layers.append(block(self.inplanes, planes, stride, downsample))\n    self.inplanes = planes * block.expansion\n    for i in range(1, blocks):\n        layers.append(block(self.inplanes, planes))\n\n    return nn.Sequential(*layers)\n\n", "CodeBleu_score": 85.18641475692186}]}
{"instance_id": "9ac60597b633", "generation_result": [{"code": "def plot_embeddings(embeddings, num_classes_in_batch)\n\t\n    # Calculate number of utterances per class\n    num_utterances = [0] * num_classes_in_batch\n    for i in range(num_classes_in_batch):\n        for j in range(len(embeddings)):\n            if embeddings[j][0] == i:\n                num_utterances[i] += 1\n\n    # Limit the number of classes to 10 if necessary\n    if num_classes_in_batch > 10:\n        num_classes = 10\n    else:\n        num_classes = num_classes_in_batch\n\n    # Plot embeddings\n    fig = matplotlib.pyplot.figure(figsize=(10, 10))\n    ax = fig.add_subplot(1, 1, 1)\n    ax.set_title(\"UMAP projection\")\n    ax.set_aspect('equal')\n    ax.set_xlabel(\"Dimension 1\")\n    ax.set_ylabel(\"Dimension 2\")\n    ax.set_xlim([-2.5, 2.5])\n    ax.set_ylim([-2.5, 2.5])\n    colors = ['#000000', '#E69F00', '#56B4E9', '#009E73', '#F0E442', '#0072B2', '#D55E00', '#CC79A7', '#999999', '#E6194B']\n    for i in range(num_classes):\n        if num_utterances[i] > 0:\n            indices = []\n            for j in range(len(embeddings)):\n                if embeddings[j][0] == i:\n                    indices.append(j)\n            embedding_subset = embeddings[indices]\n            embedding_subset = embedding_subset[:, 1:]\n            reducer = umap.UMAP(n_neighbors=10, min_dist=0.0, metric='euclidean', random_state=42, n_components=2)\n            embedding_subset = reducer.fit_", "CodeBleu_score": 35.036951145901696}, {"code": "def plot_embeddings(embeddings, num_classes_in_batch)\n\t\n    if len(embeddings) > 10:\n        num_classes_in_batch = min(num_classes_in_batch, 10)\n    embeddings = umap.UMAP().fit_transform(embeddings)\n    fig, ax = matplotlib.pyplot.subplots(figsize=(10, 10))\n    for i in range(num_classes_in_batch):\n        class_indices = [j for j, x in enumerate(\n            [x[0] for x in embeddings]) if x == i]\n        ax.scatter(embeddings[class_indices, 0],\n                   embeddings[class_indices, 1],\n                   c=list(range(num_classes_in_batch)),\n                   cmap=matplotlib.cm.get_cmap('rainbow', num_classes_in_batch),\n                   s=100,\n                   marker='o',\n                   label=i)\n    ax.set_title('UMAP projection', fontsize=20)\n    ax.set_aspect('equal')\n    fig.savefig('umap.png', dpi=300)\n    return fig\n\n", "CodeBleu_score": 34.97794571562049}, {"code": "def plot_embeddings(embeddings, num_classes_in_batch)\n\t\n    # UMAP transforms the embeddings\n    reducer = umap.UMAP(n_neighbors=10, min_dist=0.1, metric='cosine', random_state=42)\n    embeddings_transformed = reducer.fit_transform(embeddings)\n\n    # Calculates the number of utterances per class\n    num_utterances_per_class = []\n    for i in range(num_classes_in_batch):\n        num_utterances_per_class.append(len(embeddings[embeddings[:, 0] == i]))\n\n    # Limits the number of classes to 10 if necessary\n    if len(num_utterances_per_class) > 10:\n        num_utterances_per_class = num_utterances_per_class[:10]\n\n    # Creates a scatter plot with colors representing different classes\n    fig, ax = matplotlib.pyplot.subplots()\n    for i in range(num_classes_in_batch):\n        if num_utterances_per_class[i] > 0:\n            ax.scatter(embeddings_transformed[embeddings[:, 0] == i, 0], embeddings_transformed[embeddings[:, 0] == i, 1],\n                       c=num_utterances_per_class[i], s=0.5, cmap='viridis')\n\n    # Sets the title, adjusts the aspect ratio, and saves the plot as \"umap\"\n    fig.suptitle('UMAP projection', fontsize=16)\n    fig.set_aspect('equal', adjustable='box')\n    fig.savefig('umap.png', dpi=300)\n\n    # Returns the figure\n    return fig\n\n", "CodeBleu_score": 25.253203707919813}]}
{"instance_id": "19c2b4af256d", "generation_result": [{"code": "def calc_cosine_sim(self, dvecs, centroids)\n\t\n\n    # initialize the cosine similarity matrix\n    cosine_sim = torch.zeros(dvecs.shape[0], centroids.shape[0]).to(self.device)\n\n    # iterate over each speaker and their corresponding utterances\n    for spk_idx in range(dvecs.shape[0]):\n        # compute new centroids excluding the current utterance\n        new_centroids = centroids[spk_idx, :] - dvecs[spk_idx, :]\n\n        # calculate the cosine similarity with the new centroids\n        cosine_sim[spk_idx, :] = torch.mm(dvecs[spk_idx, :].unsqueeze(0), new_centroids.unsqueeze(1))\n\n        # clamp the cosine similarity values to avoid small values\n        cosine_sim[spk_idx, :] = torch.clamp(cosine_sim[spk_idx, :], min=0.0)\n\n    # concatenate and stack the cosine similarity matrix\n    cosine_sim = torch.cat(torch.split(cosine_sim, 1, dim=0), dim=0)\n    cosine_sim = torch.stack(torch.split(cosine_sim, 1, dim=0), dim=0)\n\n    return cosine_sim\n\n", "CodeBleu_score": 14.02373342194154}, {"code": "def calc_cosine_sim(self, dvecs, centroids)\n\t\n\n    # initialize cosine similarity matrix\n    cos_sim_matrix = torch.zeros(dvecs.shape[0], centroids.shape[0])\n\n    for i in range(centroids.shape[0]):\n        # get current centroid\n        current_centroid = centroids[i].unsqueeze(0)\n\n        # get utterances of current speaker\n        current_speaker = dvecs[i]\n\n        # calculate cosine similarity between utterances and centroids\n        cos_sim = torch.mm(current_speaker, current_centroid.t())\n        cos_sim = cos_sim.clamp(-1, 1)\n\n        # append cosine similarity to cosine similarity matrix\n        cos_sim_matrix = torch.cat((cos_sim_matrix, cos_sim.unsqueeze(0)), 0)\n\n    return cos_sim_matrix\n\n", "CodeBleu_score": 21.542601224668513}, {"code": "def calc_cosine_sim(self, dvecs, centroids)\n\t\n    # TODO: add docstring\n    # TODO: add type hints\n    # TODO: add type hints\n    # TODO: add type hints\n    # TODO: add type hints\n    # TODO: add type hints\n    # TODO: add type hints\n    # TODO: add type hints\n    # TODO: add type hints\n    # TODO: add type hints\n    # TODO: add type hints\n    # TODO: add type hints\n    # TODO: add type hints\n    # TODO: add type hints\n    # TODO: add type hints\n    # TODO: add type hints\n    # TODO: add type hints\n    # TODO: add type hints\n    # TODO: add type hints\n    # TODO: add type hints\n    # TODO: add type hints\n    # TODO: add type hints\n    # TODO: add type hints\n    # TODO: add type hints\n    # TODO: add type hints\n    # TODO: add type hints\n    # TODO: add type hints\n    # TODO: add type hints\n    # TODO: add type hints\n    # TODO: add type hints\n    # TODO: add type hints\n    # TODO: add type hints\n    # TODO: add type hints\n    # TODO: add type hints\n    # TODO: add type hints\n    # TODO: add type hints\n    # TODO: add type hints\n    # TODO: add type hints\n    # TODO: add type hints\n    # TODO: add type hints\n    # TODO: add type hints\n    # TODO: add type hints\n    # TODO: add type hints\n    # TODO: add type hints\n    # TODO: add type hints\n    # TODO: add type hints\n    # TODO: add type hints\n    # TODO: add type hints\n    # TODO: add type hints\n    # TODO: add type hints\n    # TODO: add type hints\n    # TODO: add type hints\n    # TODO: add type hints\n    # TODO: add type hints\n    # TODO: add type hints\n    # TODO: add type hints\n    # TODO: add type hints\n    # TODO: add type hints\n    # TODO: add type hints\n    # TODO: add type hints\n    # TODO: add type hints\n    # TODO: add type hints\n    # TODO: add type hints\n    # TODO: add type hints", "CodeBleu_score": 1.504010695187166}]}
{"instance_id": "c525b411a1e7", "generation_result": [{"code": " def embed_loss_softmax(self, dvecs, cos_sim_matrix)\n\t\n    losses = []\n    for i in range(len(dvecs)):\n        loss = -1 * torch.log(torch.nn.functional.softmax(cos_sim_matrix[i], dim=0))\n        losses.append(loss)\n    losses = torch.stack(losses)\n    return losses\n\n", "CodeBleu_score": 32.457398302181176}, {"code": " def embed_loss_softmax(self, dvecs, cos_sim_matrix)\n\t\n    dvecs = dvecs.unsqueeze(1)\n    loss = -torch.log(torch.nn.functional.softmax(cos_sim_matrix, dim=1))\n    loss = loss.view(-1, dvecs.shape[0])\n    return loss\n\n", "CodeBleu_score": 16.60289652465672}, {"code": " def embed_loss_softmax(self, dvecs, cos_sim_matrix)\n\t\n    losses = []\n    for i in range(dvecs.shape[0]):\n        loss = -torch.log(torch.nn.functional.softmax(cos_sim_matrix[i], dim=0))\n        losses.append(loss)\n    return torch.stack(losses)\n\n", "CodeBleu_score": 32.54487625443326}]}
{"instance_id": "25ac3945da2e", "generation_result": [{"code": "def embed_loss_contrast(self, dvecs, cos_sim_matrix)\n\t\n    losses = []\n    for i in range(dvecs.shape[0]):\n        loss = self.contrast_loss(dvecs[i], cos_sim_matrix[i])\n        losses.append(loss)\n    return torch.stack(losses)\n\n", "CodeBleu_score": 17.43919266019872}, {"code": "def embed_loss_contrast(self, dvecs, cos_sim_matrix)\n\t\n    losses = []\n    for dvec in dvecs:\n        # Compute the cosine similarity matrix for the current embedding\n        cos_sim = torch.mm(dvec, dvec.T)\n        # Compute the sigmoid of the cosine similarity matrix\n        sigmoid_cos_sim = torch.sigmoid(cos_sim)\n        # Compute the contrast loss\n        loss = torch.sum(sigmoid_cos_sim - cos_sim_matrix)\n        # Append the loss to the losses list\n        losses.append(loss)\n    # Stack the losses into a tensor\n    losses = torch.stack(losses)\n    # Return the stacked tensor of losses\n    return losses\n\n", "CodeBleu_score": 22.289509732436436}, {"code": "def embed_loss_contrast(self, dvecs, cos_sim_matrix)\n\t\n\n    # get the embedding loss for each embedding\n    # embeddings = self.get_embeddings(dvecs)\n    # embeddings = torch.tensor(embeddings)\n    embeddings = dvecs\n    # embeddings = embeddings.unsqueeze(1)\n    # embeddings = embeddings.repeat(1, self.centroids.shape[0], 1)\n    # embeddings = embeddings.view(-1, embeddings.shape[-1])\n    # embeddings = embeddings.view(-1, embeddings.shape[-1])\n    # embeddings = embeddings.view(-1, embeddings.shape[-1])\n    # embeddings = embeddings.view(-1, embeddings.shape[-1])\n    # embeddings = embeddings.view(-1, embeddings.shape[-1])\n    # embeddings = embeddings.view(-1, embeddings.shape[-1])\n    # embeddings = embeddings.view(-1, embeddings.shape[-1])\n    # embeddings = embeddings.view(-1, embeddings.shape[-1])\n    # embeddings = embeddings.view(-1, embeddings.shape[-1])\n    # embeddings = embeddings.view(-1, embeddings.shape[-1])\n    # embeddings = embeddings.view(-1, embeddings.shape[-1])\n    # embeddings = embeddings.view(-1, embeddings.shape[-1])\n    # embeddings = embeddings.view(-1, embeddings.shape[-1])\n    # embeddings = embeddings.view(-1, embeddings.shape[-1])\n    # embeddings = embeddings.view(-1, embeddings.shape[-1])\n    # embeddings = embeddings.view(-1, embeddings.shape[-1])\n    # embeddings = embeddings.view(-1, embeddings.shape[-1])\n    # embeddings = embeddings.view(-1, embeddings.shape", "CodeBleu_score": 1.4173993563819371}]}
{"instance_id": "878d14ffabd2", "generation_result": [{"code": "def forward(self, x, _label=None)\n\t\n\n    # input shape: (num_speakers, num_utts_per_speaker, dvec_feats)\n    assert x.shape[1] >= 2, \"The input must have at least two utterances per speaker.\"\n\n    # average the embeddings for each speaker\n    x_centroids = torch.mean(x, dim=1)\n\n    # calculate the cosine similarity matrix\n    sim_matrix = torch.mm(x_centroids, x_centroids.t())\n\n    # scale and shift the similarity matrix\n    sim_matrix = sim_matrix / (self.scale * sim_matrix.max())\n    sim_matrix = sim_matrix - 0.5\n\n    # calculate the embedding loss\n    loss = torch.sum(sim_matrix ** 2)\n    loss = loss / x.shape[0]\n\n    return loss\n\n", "CodeBleu_score": 36.159289237514905}, {"code": "def forward(self, x, _label=None)\n\t\n    if x.size(1) < 2:\n        return 0\n    x_mean = x.mean(dim=1)\n    x_mean = x_mean.unsqueeze(1)\n    x_mean = x_mean.expand(x.size(0), x.size(1), x.size(2))\n    x = x - x_mean\n    x = x.pow(2).sum(dim=2).sqrt()\n    x = x.unsqueeze(1)\n    x = x.expand(x.size(0), x.size(1), x.size(2))\n    x_mean = x_mean.unsqueeze(2)\n    x_mean = x_mean.expand(x.size(0), x.size(1), x.size(2))\n    x = x / x_mean\n    x = x.clamp(min=1e-8)\n    x = x.log()\n    x = x.sum(dim=1)\n    x = x.mean()\n    return x\n\n", "CodeBleu_score": 28.536498775206066}, {"code": "def forward(self, x, _label=None)\n\t\n    if _label is None:\n        _label = self._label\n    if _label is None:\n        raise ValueError('Labels are not provided.')\n    if x.size(1) < 2:\n        raise ValueError('The number of utterances per speaker should be at least 2.')\n    if x.size(0) != _label.size(0):\n        raise ValueError('The number of speakers should be equal to the number of labels.')\n    if x.size(2) != self.dvec_feats:\n        raise ValueError('The number of dvec_feats should be equal to the number of features.')\n    # Calculate centroids\n    centroids = []\n    for i in range(x.size(0)):\n        centroids.append(torch.mean(x[i], dim=0))\n    centroids = torch.stack(centroids, dim=0)\n    # Calculate similarity matrix\n    sim_matrix = torch.mm(x, centroids.t())\n    # Scale and shift the similarity matrix\n    sim_matrix = sim_matrix / sim_matrix.max()\n    sim_matrix = sim_matrix - 0.5\n    # Calculate the embedding loss\n    loss = 0\n    for i in range(x.size(0)):\n        loss += self.loss_func(sim_matrix[i], _label[i])\n    return loss / x.size(0)\n\n", "CodeBleu_score": 26.024581434449683}]}
{"instance_id": "86a364c72caa", "generation_result": [{"code": "def get_voices(extra_voice_dirs: List[str] = [])\n\t\n    voice_dirs = [\n        \"TTS/tts/layers/bark/hubert/voices/\",\n        \"TTS/tts/layers/bark/hubert/voices/extra/\",\n    ] + extra_voice_dirs\n    voices = {}\n    for voice_dir in voice_dirs:\n        voice_files = glob(os.path.join(voice_dir, \"**/*.npz\"), recursive=True)\n        if not voice_files:\n            voice_files = glob(os.path.join(voice_dir, \"**/*.wav\"), recursive=True)\n            if not voice_files:\n                voice_files = glob(os.path.join(voice_dir, \"**/*.mp3\"), recursive=True)\n        for voice_file in voice_files:\n            voice_name = os.path.split(os.path.split(voice_file)[0])[1]\n            if voice_name not in voices:\n                voices[voice_name] = []\n            voices[voice_name].append(voice_file)\n    return voices\n\n", "CodeBleu_score": 26.046861764369062}, {"code": "def get_voices(extra_voice_dirs: List[str] = [])\n\t\n    voice_dirs = [\n        \"voices/bark/\",\n        \"voices/bark/en/\",\n        \"voices/bark/en/female/\",\n        \"voices/bark/en/female/female_01/\",\n        \"voices/bark/en/male/\",\n        \"voices/bark/en/male/male_01/\",\n        \"voices/bark/en/male/male_02/\",\n        \"voices/bark/en/male/male_03/\",\n        \"voices/bark/en/male/male_04/\",\n        \"voices/bark/en/male/male_05/\",\n        \"voices/bark/en/male/male_06/\",\n        \"voices/bark/en/male/male_07/\",\n        \"voices/bark/en/male/male_08/\",\n        \"voices/bark/en/male/male_09/\",\n        \"voices/bark/en/male/male_10/\",\n        \"voices/bark/en/male/male_11/\",\n        \"voices/bark/en/male/male_12/\",\n        \"voices/bark/en/male/male_13/\",\n        \"voices/bark/en/male/male_14/\",\n        \"voices/bark/en/male/male_15/\",\n        \"voices/bark/en/male/male_16/\",\n        \"voices/bark/en/male/male_17/\",\n        \"voices/bark/en/male/male_18/\",\n        \"voices/bark/en/male/male_19/\",\n        \"voices/bark/en/male/male_20/\",\n        \"voices/bark/en/male/male_21/\",\n        \"voices/bark/en/male/male_22/\",\n        \"voices/bark/en/", "CodeBleu_score": 8.770779739041915}, {"code": "def get_voices(extra_voice_dirs: List[str] = [])\n\t\n    voice_dirs = [\"TTS/tts/layers/bark/hubert/voices\"]\n    voice_dirs.extend(extra_voice_dirs)\n    voices = {}\n    for voice_dir in voice_dirs:\n        for path in glob(os.path.join(voice_dir, \"**\"), recursive=True):\n            if os.path.isfile(path) and path.endswith((\".wav\", \".mp3\", \".npz\")):\n                if path.endswith(\".wav\"):\n                    voice_name = os.path.basename(os.path.dirname(path))\n                    if voice_name not in voices:\n                        voices[voice_name] = []\n                    voices[voice_name].append(path)\n                elif path.endswith(\".mp3\"):\n                    voice_name = os.path.basename(os.path.dirname(os.path.dirname(path)))\n                    if voice_name not in voices:\n                        voices[voice_name] = []\n                    voices[voice_name].append(path)\n                elif path.endswith(\".npz\"):\n                    voice_name = os.path.basename(os.path.dirname(path))\n                    if voice_name not in voices:\n                        voices[voice_name] = []\n                    voices[voice_name].append(path)\n    return voices\n\n", "CodeBleu_score": 27.775667016437634}]}
{"instance_id": "02f20c8917e6", "generation_result": [{"code": "def load_voice(model, voice: str, extra_voice_dirs: List[str] = [])\n\t\n    if voice == \"random\":\n        return None, None, None\n    voices = get_voices(model, extra_voice_dirs)\n    if voice not in voices:\n        raise ValueError(f\"Voice {voice} is not available for model {model}\")\n    voice_paths = voices[voice]\n    if len(voice_paths) > 1:\n        raise ValueError(f\"Multiple paths for voice {voice} found: {voice_paths}\")\n    voice_path = voice_paths[0]\n    if voice_path.endswith(\".npz\"):\n        return load_voice_npz(voice_path)\n    else:\n        return load_voice_audio(voice_path)\n\n", "CodeBleu_score": 44.9607576971013}, {"code": "def load_voice(model, voice: str, extra_voice_dirs: List[str] = [])\n\t\n    voices = get_voices(model, extra_voice_dirs)\n    voice_paths = voices.get(voice)\n    if not voice_paths:\n        raise ValueError(f\"Voice {voice} not found.\")\n    if len(voice_paths) > 1:\n        raise ValueError(f\"Multiple voices found for {voice}.\")\n    voice_path = voice_paths[0]\n    if voice_path.endswith(\".npz\"):\n        return torch.load(voice_path)\n    if not os.path.exists(voice_path):\n        raise ValueError(f\"Voice {voice_path} not found.\")\n    if voice_path.endswith(\".wav\"):\n        voice_npz_path = convert_audio(voice_path, \"npz\")\n        return torch.load(voice_npz_path)\n    raise ValueError(f\"Voice {voice_path} not supported.\")\n\n", "CodeBleu_score": 36.96910213733869}, {"code": "def load_voice(model, voice: str, extra_voice_dirs: List[str] = [])\n\t\n    voices = get_voices(model, extra_voice_dirs)\n    if voice == \"random\":\n        return None, None, None, None\n    voice_path = None\n    for voice_dir in voices:\n        voice_path = glob(f\"{voice_dir}/{voice}.*\")\n        if len(voice_path) == 1:\n            break\n    if len(voice_path) != 1:\n        raise ValueError(f\"Voice {voice} not found in {voices}\")\n    if voice_path[0].endswith(\".npz\"):\n        voice_data = torch.load(voice_path[0])\n    else:\n        voice_data = convert_audio(voice_path[0], model.sample_rate)\n        torch.save(voice_data, voice_path[0] + \".npz\")\n    return voice_path[0], voice_data, voices, voice\n\n", "CodeBleu_score": 49.64703805567584}]}
{"instance_id": "9458e981389f", "generation_result": [{"code": "def _relative_shift(self, pos_score: torch.Tensor) -> torch.Tensor\n\t\n    B, T, N, C = pos_score.size()\n    pos_score = pos_score.reshape(B, T, N, C)\n    pos_score = torch.cat([pos_score[:, :, 1:], torch.zeros(B, T, 1, C).to(pos_score.device)], dim=-1)\n    pos_score = pos_score.reshape(B, T + 1, N, C)[:, 1:, :, :]\n    return pos_score\n\n", "CodeBleu_score": 25.15003599800416}, {"code": "def _relative_shift(self, pos_score: torch.Tensor) -> torch.Tensor\n\t\n    # Extract the position score tensor's dimensions.\n    batch_size, seq_len, hidden_size = pos_score.size()\n    # Create a zero tensor with an additional dimension.\n    zero_tensor = torch.zeros(batch_size, seq_len, hidden_size, 1, device=pos_score.device)\n    # Concatenate the position score tensor with the zero tensor along the last dimension.\n    pos_score = torch.cat([pos_score, zero_tensor], dim=2)\n    # Reshape the concatenated tensor.\n    pos_score = pos_score.view(batch_size, seq_len, hidden_size, 2)\n    # Slice the reshaped tensor to achieve the relative shift.\n    pos_score = pos_score[:, :, :, 1:]\n    # Reshape the sliced tensor.\n    pos_score = pos_score.view(batch_size, seq_len, hidden_size)\n    return pos_score\n\n", "CodeBleu_score": 31.09668875314001}, {"code": "def _relative_shift(self, pos_score: torch.Tensor) -> torch.Tensor\n\t\n    batch_size, seq_len, _ = pos_score.size()\n    pos_score = pos_score.view(batch_size, seq_len, 1, -1)\n    pos_score = torch.cat((pos_score, torch.zeros_like(pos_score)), dim=-1)\n    pos_score = pos_score.view(batch_size, seq_len, seq_len, -1)\n    pos_score = pos_score[:, :, 1:, :]\n    pos_score = pos_score.view(batch_size, seq_len, -1)\n    return pos_score\n\n", "CodeBleu_score": 29.782154773728024}]}
{"instance_id": "c046d4f00e80", "generation_result": [{"code": "def run_padded_sequence(self, context, lens)\n\t\n    # The function is implemented in the following way:\n    # 1. The context is padded to the maximum length in the batch\n    # 2. The context is processed by the convolutional layers\n    # 3. The output is padded to the maximum length in the batch\n    # 4. The output is returned\n    # The function is implemented in the following way:\n    # 1. The context is padded to the maximum length in the batch\n    # 2. The context is processed by the convolutional layers\n    # 3. The output is padded to the maximum length in the batch\n    # 4. The output is returned\n    # The function is implemented in the following way:\n    # 1. The context is padded to the maximum length in the batch\n    # 2. The context is processed by the convolutional layers\n    # 3. The output is padded to the maximum length in the batch\n    # 4. The output is returned\n    # The function is implemented in the following way:\n    # 1. The context is padded to the maximum length in the batch\n    # 2. The context is processed by the convolutional layers\n    # 3. The output is padded to the maximum length in the batch\n    # 4. The output is returned\n    # The function is implemented in the following way:\n    # 1. The context is padded to the maximum length in the batch\n    # 2. The context is processed by the convolutional layers\n    # 3. The output is padded to the maximum length in the batch\n    # 4. The output is returned\n    # The function is implemented in the following way:\n    # 1. The context is padded to the maximum length in the batch\n    # 2. The context is processed by the convolutional layers\n    # 3. The output is padded to the maximum length in the batch\n    # 4. The output is returned\n    # The function is implemented in the following way:\n    # 1. The context is padded to the maximum length in the batch\n    # 2. The context is processed by the convolutional layers\n    # 3. The output is padded to the maximum length in the batch\n    # 4. The output is returned\n", "CodeBleu_score": 1.459942084942085}, {"code": "def run_padded_sequence(self, context, lens)\n\t\n\n    # Extract the contexts\n    contexts = []\n    for i in range(len(context)):\n        contexts.append(context[i, :lens[i]])\n\n    # Process the contexts\n    contexts = torch.stack(contexts)\n    contexts = self.conv1d_1(contexts)\n    contexts = self.relu(contexts)\n    contexts = self.dropout(contexts)\n    contexts = self.conv1d_2(contexts)\n    contexts = self.relu(contexts)\n    contexts = self.dropout(contexts)\n    contexts = self.conv1d_3(contexts)\n    contexts = self.relu(contexts)\n    contexts = self.dropout(contexts)\n    contexts = self.conv1d_4(contexts)\n    contexts = self.relu(contexts)\n    contexts = self.dropout(contexts)\n    contexts = self.conv1d_5(contexts)\n    contexts = self.relu(contexts)\n    contexts = self.dropout(contexts)\n\n    # Padding\n    max_len = max(lens)\n    contexts = torch.nn.utils.rnn.pad_sequence(contexts, batch_first=True, padding_value=0)\n    contexts = contexts[:, :max_len, :]\n\n    return contexts\n\n", "CodeBleu_score": 31.526574871181047}, {"code": "def run_padded_sequence(self, context, lens)\n\t\n\n    # Get the number of channels\n    channels = self.conv_layers[0].weight.size(0)\n\n    # Initialize the output\n    output = torch.zeros(context.size(0), context.size(1), channels)\n\n    # Initialize the mask\n    mask = torch.zeros(context.size(0), context.size(1))\n\n    # Process the context\n    for i, (context_i, lens_i) in enumerate(zip(context, lens)):\n\n        # Get the context up to the specified length\n        context_i_ = context_i[:lens_i]\n\n        # Process the context\n        for layer in self.conv_layers:\n            context_i_ = layer(context_i_.unsqueeze(1))\n\n        # Store the context\n        output[i] = context_i_\n\n        # Store the mask\n        mask[i] = torch.ones(lens_i)\n\n    # Return the processed context\n    return output, mask\n\n", "CodeBleu_score": 24.54504093036175}]}
{"instance_id": "db2644f3c681", "generation_result": [{"code": "def run_unsorted_inputs(self, fn, context, lens)\n\t\n    lens, indices = lens.sort(descending=True)\n    context = context.index_select(0, indices)\n    results = fn(context, lens)\n    results = results.index_select(0, indices)\n    return results\n\n", "CodeBleu_score": 5.428333635536123}, {"code": "def run_unsorted_inputs(self, fn, context, lens)\n\t\n\n    # Sort the lengths and indices\n    lens, indices = lens.sort(descending=True)\n    context = context[indices]\n\n    # Pack the context sequences\n    context = context.transpose(0, 1).contiguous()\n\n    # Apply the function\n    fn_output = fn(context, lens)\n\n    # Unpack the sequences\n    fn_output = fn_output.transpose(0, 1).contiguous()\n\n    # Map the results back to the original indices\n    fn_output = fn_output[indices]\n\n    return fn_output\n\n", "CodeBleu_score": 8.874943961459234}, {"code": "def run_unsorted_inputs(self, fn, context, lens)\n\t\n    lens, indices = lens.sort(0, descending=True)\n    context = context.index_select(0, indices)\n    context = context.pack_padded(lens)\n    context = fn(context)\n    context, _ = context.pad_packed(lens)\n    context = context.index_select(0, indices)\n    return context\n\n", "CodeBleu_score": 10.495653813705271}]}
{"instance_id": "ced082bffc63", "generation_result": [{"code": "def remove_weight_norm(self)\n\t\n    self.kernel_predictor.remove_weight_norm()\n    self.convt_pre.remove_weight_norm()\n    for block in self.conv_blocks:\n        block.remove_weight_norm()\n\n", "CodeBleu_score": 33.51046275391997}, {"code": "def remove_weight_norm(self)\n\t\n    self.kernel_predictor.remove_weight_norm()\n    for i in range(len(self.conv_blocks)):\n        self.conv_blocks[i].remove_weight_norm()\n\n", "CodeBleu_score": 29.147104174246408}, {"code": "def remove_weight_norm(self)\n\t\n    self.kernel_predictor.remove_weight_norm()\n    self.convt_pre.remove_weight_norm()\n    for conv_block in self.conv_blocks:\n        conv_block.remove_weight_norm()\n\n", "CodeBleu_score": 27.580633709689707}]}
{"instance_id": "b5a39fd97061", "generation_result": [{"code": "def map_mx_to_torch_model(mx_name)\n\t\n    if mx_name.startswith('se_'):\n        torch_name = 'se_' + mx_name[3:]\n    elif mx_name.startswith('senet_'):\n        torch_name = 'senet_' + mx_name[6:]\n    elif mx_name.startswith('inceptionv3'):\n        torch_name = 'inceptionv3'\n    else:\n        torch_name = mx_name.lower()\n    torch_name = torch_name.replace('_v1', '')\n    torch_name = torch_name.replace('_v2', '')\n    torch_name = torch_name.replace('_v3', '')\n    torch_name = torch_name.replace('_v4', '')\n    torch_name = torch_name.replace('_v5', '')\n    torch_name = torch_name.replace('_v6', '')\n    torch_name = torch_name.replace('_v7', '')\n    torch_name = torch_name.replace('_v8', '')\n    torch_name = torch_name.replace('_v9', '')\n    torch_name = torch_name.replace('_v10', '')\n    torch_name = torch_name.replace('_v11', '')\n    torch_name = torch_name.replace('_v12', '')\n    torch_name = torch_name.replace('_v13', '')\n    torch_name = torch_name.replace('_v14', '')\n    torch_name = torch_name.replace('_v15', '')\n    torch_name = torch_name.replace('_v16', '')\n    torch_name = torch_name.replace('_v17', '')\n    torch_name = torch_name.replace('_v18', '')\n    torch_name = torch_name.replace('_v19', '')\n    torch_name = torch_name.replace('_v20', '')\n    torch_name", "CodeBleu_score": 32.82341185522149}, {"code": "def map_mx_to_torch_model(mx_name)\n\t\n    # check if the input name is already a torch model\n    if mx_name.startswith('gluon_'):\n        return mx_name\n    # check if the input name is a MXNet model\n    if mx_name.startswith('mxnet:'):\n        mx_name = mx_name[6:]\n    # check if the input name is a MXNet model with a prefix\n    if mx_name.startswith('gluon_'):\n        mx_name = mx_name[6:]\n    # check if the input name is a MXNet model with a prefix\n    if mx_name.startswith('senet_'):\n        mx_name = mx_name.replace('senet_', 'se_')\n    # check if the input name is a MXNet model with a prefix\n    if mx_name.startswith('inceptionv3'):\n        mx_name = mx_name.replace('inceptionv3', 'inception_v3')\n    # check if the input name is a MXNet model with a prefix\n    if mx_name.startswith('resnet'):\n        mx_name = mx_name.replace('resnet', 'resnet_')\n    # check if the input name is a MXNet model with a prefix\n    if mx_name.startswith('vgg'):\n        mx_name = mx_name.replace('vgg', 'vgg_')\n    # check if the input name is a MXNet model with a prefix\n    if mx_name.startswith('densenet'):\n        mx_name = mx_name.replace('densenet', 'densenet_')\n    # check if the input name is a MXNet model with a prefix\n    if mx_name.startswith('inception'):\n        mx_name = mx_name.replace('inception', 'inception_')\n    # check if the input name is a MXNet model with a prefix\n    if mx_name.startswith('mobilenetv2'):\n        mx_name = mx_name.replace('mobilenetv2', '", "CodeBleu_score": 41.7514879491218}, {"code": "def map_mx_to_torch_model(mx_name)\n\t\n    mx_name = mx_name.lower()\n    if mx_name.startswith('se_'):\n        mx_name = mx_name.replace('se_', 'se_resnet')\n    elif mx_name.startswith('senet_'):\n        mx_name = mx_name.replace('senet_', 'senet')\n    elif mx_name.startswith('inceptionv3'):\n        mx_name = mx_name.replace('inceptionv3', 'inception')\n    return f'gluon_{mx_name}'\n\n", "CodeBleu_score": 45.92842714037122}]}
{"instance_id": "7eee5efdbf67", "generation_result": [{"code": "def _num_samples_per_worker(self)\n\t\n    # pylint: disable=protected-access\n    num_replicas = self._num_replicas_in_sync if self._use_legacy_distributed_api else self._num_replicas\n    if num_replicas > 1:\n        return math.ceil(self._total_samples / num_replicas)\n    return self._total_samples\n\n", "CodeBleu_score": 16.474497084651727}, {"code": "def _num_samples_per_worker(self)\n\t\n\n    # Calculate the number of samples per worker\n    num_samples_per_worker = self.total_samples // self.num_replicas\n\n    # If training is enabled, round up the number of samples per worker\n    if self.training:\n        num_samples_per_worker = math.ceil(num_samples_per_worker)\n\n    # If a batch size is specified during training, adjust the number of samples to be a multiple of the batch size\n    if self.training and self.batch_size > 0:\n        num_samples_per_worker = math.ceil(num_samples_per_worker / self.batch_size) * self.batch_size\n\n    return num_samples_per_worker\n\n", "CodeBleu_score": 41.48483009027598}, {"code": "def _num_samples_per_worker(self)\n\t\n    if self.shuffle:\n        num_samples = len(self) * self.num_replicas\n    else:\n        num_samples = len(self)\n    samples_per_worker = num_samples // self.num_replicas\n    samples_per_worker = samples_per_worker // self.batch_size * self.batch_size\n    if samples_per_worker < self.batch_size:\n        samples_per_worker = self.batch_size\n    return samples_per_worker\n\n", "CodeBleu_score": 30.190161571044268}]}
{"instance_id": "a698d65ae190", "generation_result": [{"code": "def find_images_and_targets(\n        folder: str,\n        types: Optional[Union[List, Tuple, Set]] = None,\n        class_to_idx: Optional[Dict] = None,\n        leaf_name_only: bool = True,\n        sort: bool = True\n)\n\t\n\n    if types is None:\n        types = ('.jpg', '.jpeg', '.png', '.tif', '.tiff', '.bmp', '.dng', '.webp')\n    if class_to_idx is None:\n        class_to_idx = {}\n\n    images = []\n    targets = []\n\n    for target in sorted(os.listdir(folder)):\n        if target in class_to_idx:\n            continue\n\n        if leaf_name_only:\n            target = os.path.basename(target)\n\n        if os.path.isdir(os.path.join(folder, target)):\n            for file in sorted(os.listdir(os.path.join(folder, target))):\n                if file.lower().endswith(tuple(types)):\n                    images.append(os.path.join(folder, target, file))\n                    targets.append(target)\n        else:\n            if target.lower().endswith(tuple(types)):\n                images.append(os.path.join(folder, target))\n                targets.append(target)\n\n    if sort:\n        images = sorted(images)\n        targets = sorted(targets)\n\n    for i, target in enumerate(targets):\n        if target not in class_to_idx:\n            class_to_idx[target] = len(class_to_idx)\n\n    return images, targets, class_to_idx\n\n", "CodeBleu_score": 36.780800894782864}, {"code": "def find_images_and_targets(\n        folder: str,\n        types: Optional[Union[List, Tuple, Set]] = None,\n        class_to_idx: Optional[Dict] = None,\n        leaf_name_only: bool = True,\n        sort: bool = True\n)\n\t\n    if types is None:\n        types = ['.jpg', '.jpeg', '.png']\n    if class_to_idx is None:\n        class_to_idx = {}\n\n    # Get a list of all the files in the folder\n    all_files = os.listdir(folder)\n\n    # Filter by file types\n    files = [f for f in all_files if os.path.splitext(f)[1].lower() in types]\n\n    # Build a list of filenames and labels based on the folder structure\n    # If a file is named like \"folder/label/image.jpg\", the label is \"label\"\n    # If a file is named like \"folder/image.jpg\", the label is \"image\"\n    # If a file is named like \"folder/label/label/image.jpg\", the label is \"label/label\"\n    # If a file is named like \"folder/label_1/label_2/image.jpg\", the label is \"label_1/label_2\"\n    # If a file is named like \"folder/label_1/label_2/image_1.jpg\", the label is \"label_1/label_2\"\n    # If a file is named like \"folder/label_1/label_2/image_2.jpg\", the label is \"label_1/label_2\"\n    # If a file is named like \"folder/label_1/image.jpg\", the label is \"label_1\"\n    # If a file is named like \"folder/label_1/label_2/image_3.jpg\", the label is \"label_1/label_2\"\n    # If a file is named like \"folder/label_1/label_2/label_3/image.jpg\", the label is \"label_1/label_2/label_3\"\n    # If a file is named like \"folder/label_1/label_2/label_3/image_4.jpg\", the label is \"label_1/label_2/label_3\"\n    # If a file is named like \"folder/label_1/label_2/label_3/image_5.jpg\", the label is \"label_1/label_2/label_3\"", "CodeBleu_score": 15.489585591082133}, {"code": "def find_images_and_targets(\n        folder: str,\n        types: Optional[Union[List, Tuple, Set]] = None,\n        class_to_idx: Optional[Dict] = None,\n        leaf_name_only: bool = True,\n        sort: bool = True\n)\n\t\n    if types is None:\n        types = ['.jpg', '.jpeg', '.png', '.tif', '.tiff']\n    if class_to_idx is None:\n        class_to_idx = {}\n    if leaf_name_only:\n        def get_name(path):\n            return os.path.basename(path)\n    else:\n        def get_name(path):\n            return path\n    # get all files in the folder\n    files = [os.path.join(folder, f) for f in os.listdir(folder) if os.path.isfile(os.path.join(folder, f))]\n    # filter by file types\n    files = [f for f in files if os.path.splitext(f)[-1] in types]\n    # sort files\n    if sort:\n        files = sorted(files, key=natural_key)\n    # build class-to-index mapping\n    for f in files:\n        label = get_name(f)\n        if label not in class_to_idx:\n            class_to_idx[label] = len(class_to_idx)\n    # get all image files\n    images = [f for f in files if os.path.splitext(f)[-1] in types]\n    # get all label files\n    labels = [f for f in files if os.path.splitext(f)[-1] not in types]\n    # pair images with their corresponding labels\n    pairs = [(images[i], labels[i]) for i in range(len(images))]\n    # sort pairs\n    if sort:\n        pairs = sorted(pairs, key=natural_key)\n    return pairs, class_to_idx\n\n", "CodeBleu_score": 39.43024664435536}]}
{"instance_id": "e7fe43bdecdc", "generation_result": [{"code": "def pytorch_worker_seed()\n\t\n    worker_info = get_worker_info()\n    if worker_info is None:\n        return int(os.getenv(\"WDS_RANK\", 0))\n    return worker_info.seed\n\n", "CodeBleu_score": 41.22528389298927}, {"code": "def pytorch_worker_seed()\n\t\n    worker_info = get_worker_info()\n    if worker_info is not None:\n        return worker_info.seed\n    else:\n        return int.from_bytes(os.urandom(4), \"big\")\n\n", "CodeBleu_score": 49.68902392954176}, {"code": "def pytorch_worker_seed()\n\t\n    worker_info = get_worker_info()\n    if worker_info is None:\n        seed = int(os.getenv(\"PYTORCH_WORKER_SEED\", \"0\"))\n    else:\n        seed = worker_info.seed\n    return seed\n\n", "CodeBleu_score": 44.86662960200425}]}
{"instance_id": "0ffc7e7ded25", "generation_result": [{"code": "def solarize_add(img, add, thresh=128, **__)\n\t\n    if img.mode in (\"L\", \"RGB\"):\n        lut = [min(255, max(0, i + add)) for i in range(256)]\n        if img.mode == \"RGB\" and len(lut) == 256:\n            lut = lut + lut + lut\n        img = img.point(lut)\n    return img\n\n", "CodeBleu_score": 52.030986412116945}, {"code": "def solarize_add(img, add, thresh=128, **__)\n\t\n    lut = []\n    for i in range(256):\n        if i < thresh:\n            lut.append(min(255, i + add))\n        else:\n            lut.append(i)\n    if img.mode in (\"L\", \"RGB\"):\n        if img.mode == \"RGB\" and len(img.getbands()) == 2:\n            img = img.convert(\"L\")\n        img = ImageOps.solarize(img, lut)\n        if img.mode == \"L\":\n            img = img.convert(\"RGB\")\n    return img\n\n", "CodeBleu_score": 65.78372708747324}, {"code": "def solarize_add(img, add, thresh=128, **__)\n\t\n    if img.mode == \"L\":\n        img = img.point(lambda i: i + add if i < thresh else i)\n    elif img.mode == \"RGB\" or img.mode == \"HSV\":\n        img = img.convert(\"RGB\")\n        img = img.point(lambda i: i + add if i < thresh else i)\n        img = img.convert(img.mode)\n    return img\n\n", "CodeBleu_score": 35.185903866278736}]}
{"instance_id": "5a0d29577a21", "generation_result": [{"code": "def _get_weighted_transforms(transforms: Dict)\n\t\n    transform_names = list(transforms.keys())\n    transform_probabilities = list(transforms.values())\n    transform_probabilities_normalized = [p / sum(transform_probabilities) for p in transform_probabilities]\n    return transform_names, transform_probabilities_normalized\n\n", "CodeBleu_score": 17.462257901225854}, {"code": "def _get_weighted_transforms(transforms: Dict)\n\t\n    transform_names = []\n    transform_probabilities = []\n    for transform_name, probability in transforms.items():\n        transform_names.append(transform_name)\n        transform_probabilities.append(probability)\n    transform_probabilities = [p / sum(transform_probabilities) for p in transform_probabilities]\n    return transform_names, transform_probabilities\n\n", "CodeBleu_score": 27.338566735132453}, {"code": "def _get_weighted_transforms(transforms: Dict)\n\t\n    transforms_names = list(transforms.keys())\n    probabilities = list(transforms.values())\n    normalized_probabilities = [p / sum(probabilities) for p in probabilities]\n    return transforms_names, normalized_probabilities\n\n", "CodeBleu_score": 19.985989881697886}]}
{"instance_id": "84bc5658c135", "generation_result": [{"code": "def _apply_basic(self, img, mixing_weights, m)\n\t\n    mixed_img = Image.new(img.mode, img.size)\n    for weight in mixing_weights:\n        op_list = []\n        for _ in range(random.randint(1, 5)):\n            op_list.append(random.choice(self.augmentations))\n        for op in op_list:\n            img = op(img)\n        mixed_img = PIL.ImageMath.eval(\n            \"convert(a * %f + b * (1 - %f), 'L')\" % (weight, 1 - weight),\n            a=mixed_img,\n            b=img,\n        )\n    mixed_img = PIL.ImageMath.eval(\n        \"convert(a * %f + b * (1 - %f), 'L')\" % (m, 1 - m), a=img, b=mixed_img\n    )\n    return mixed_img\n\n", "CodeBleu_score": 33.72490567407166}, {"code": "def _apply_basic(self, img, mixing_weights, m)\n\t\n    mixed_img = Image.new(\"RGB\", img.size)\n    for weight in mixing_weights:\n        img_copy = img.copy()\n        for _ in range(random.randint(1, 5)):\n            img_copy = self.rand_augment_seq(img_copy)\n        mixed_img = Image.blend(mixed_img, img_copy, weight)\n    mixed_img = Image.blend(img, mixed_img, m)\n    return mixed_img\n\n", "CodeBleu_score": 23.222475957521365}, {"code": "def _apply_basic(self, img, mixing_weights, m)\n\t\n    # pylint: disable=too-many-locals\n    # pylint: disable=too-many-branches\n    # pylint: disable=too-many-statements\n    # pylint: disable=too-many-arguments\n    # pylint: disable=too-many-nested-blocks\n    # pylint: disable=too-many-lines\n    # pylint: disable=too-many-function-args\n    # pylint: disable=too-many-lines\n    # pylint: disable=too-many-nested-blocks\n    # pylint: disable=too-many-nested-blocks\n    # pylint: disable=too-many-nested-blocks\n    # pylint: disable=too-many-nested-blocks\n    # pylint: disable=too-many-nested-blocks\n    # pylint: disable=too-many-nested-blocks\n    # pylint: disable=too-many-nested-blocks\n    # pylint: disable=too-many-nested-blocks\n    # pylint: disable=too-many-nested-blocks\n    # pylint: disable=too-many-nested-blocks\n    # pylint: disable=too-many-nested-blocks\n    # pylint: disable=too-many-nested-blocks\n    # pylint: disable=too-many-nested-blocks\n    # pylint: disable=too-many-nested-blocks\n    # pylint: disable=too-many-nested-blocks\n    # pylint: disable=too-many-nested-blocks\n    # pylint: disable=too-many-nested-blocks\n    # pylint: disable=too-many-nested-blocks\n    # pylint: disable=too-many-nested-blocks\n    # pylint: disable=too-many-nested-blocks\n    # pylint: disable=too-many-nested-blocks\n    # pylint", "CodeBleu_score": 1.5541835213966362}]}
{"instance_id": "84b5b752ff04", "generation_result": [{"code": "def adapt_to_chs(x, n)\n\t\n    if not isinstance(x, (tuple, list)):\n        return repeat(x, n)\n    assert len(x) == n, f\"The length of the input x {len(x)} does not match the required number of channels {n}.\"\n    return x\n\n", "CodeBleu_score": 22.38585831068395}, {"code": "def adapt_to_chs(x, n)\n\t\n    if isinstance(x, (list, tuple)):\n        if len(x) != n:\n            logging.warning(\n                \"The length of input x is not equal to the number of channels n. The mean of x is calculated and repeated n times.\"\n            )\n            x = (torch.mean(torch.stack(x)).unsqueeze(0),) * n\n        else:\n            assert len(x) == n, f\"The length of input x is not equal to the number of channels n: {len(x)} != {n}.\"\n    else:\n        x = (x,) * n\n    return x\n\n", "CodeBleu_score": 42.050734390331456}, {"code": "def adapt_to_chs(x, n)\n\t\n    if not isinstance(x, (tuple, list)):\n        x = (x,) * n\n    elif len(x) != n:\n        with suppress(Exception):\n            x = (torch.mean(torch.stack(x), dim=0),) * n\n            logging.warning(f'Adapting input to match the required number of channels.')\n    else:\n        assert len(x) == n\n    return x\n\n", "CodeBleu_score": 49.91210240041146}]}
{"instance_id": "54754b6ba394", "generation_result": [{"code": "def _worker_init(worker_id, worker_seeding='all')\n\t\n    worker_info = torch.utils.data.get_worker_info()\n    if worker_info is not None:\n        if worker_seeding == 'all':\n            random.seed(worker_info.seed)\n            torch.manual_seed(worker_info.seed)\n            torch.cuda.manual_seed(worker_info.seed)\n            np.random.seed(worker_info.seed)\n        elif callable(worker_seeding):\n            worker_seeding(worker_info.id)\n        else:\n            raise ValueError(f'worker_seeding must be a string or a callable function, but got {type(worker_seeding)}')\n\n", "CodeBleu_score": 36.12505070546133}, {"code": "def _worker_init(worker_id, worker_seeding='all')\n\t\n    if worker_seeding == 'all':\n        seed = worker_id\n        random.seed(seed)\n        torch.manual_seed(seed)\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        import numpy as np\n        np.random.seed(seed)\n    elif callable(worker_seeding):\n        worker_seeding(worker_id)\n    else:\n        raise ValueError(f'Invalid worker_seeding value: {worker_seeding}')\n\n", "CodeBleu_score": 36.657117491598015}, {"code": "def _worker_init(worker_id, worker_seeding='all')\n\t\n    # get worker info\n    worker_info = torch.utils.data.get_worker_info()\n    if worker_info is None:\n        return\n    # set the seed\n    if worker_seeding == 'all':\n        random.seed(worker_info.seed)\n        torch.manual_seed(worker_info.seed)\n        torch.cuda.manual_seed(worker_info.seed)\n        np.random.seed(worker_info.seed)\n    elif callable(worker_seeding):\n        worker_seeding(worker_info.id)\n    else:\n        raise ValueError(f'worker_seeding must be a callable or \"all\"')\n\n", "CodeBleu_score": 38.1188420028349}]}
{"instance_id": "81dfdce940db", "generation_result": [{"code": "def _decode_and_center_crop(image_bytes, image_size, resize_method)\n\t\n    image = tf.image.decode_jpeg(image_bytes, channels=3)\n    image_shape = tf.shape(image)\n    image_height = image_shape[0]\n    image_width = image_shape[1]\n\n    offset_height = (image_height - image_size) // 2\n    offset_width = (image_width - image_size) // 2\n\n    image = tf.image.crop_to_bounding_box(\n        image, offset_height, offset_width, image_size, image_size)\n    image = tf.image.resize(image, [image_size, image_size], method=resize_method)\n    return image\n\n", "CodeBleu_score": 25.153840314046587}, {"code": "def _decode_and_center_crop(image_bytes, image_size, resize_method)\n\t\n\n    image_shape = tf.io.decode_jpeg(image_bytes, channels=3)\n    image_shape = tf.image.extract_image_shape(image_shape)\n\n    height = image_shape[0]\n    width = image_shape[1]\n\n    padded_center_crop_size = tf.cast(\n        (image_size + 2 * _CROP_PADDING) * tf.cast(tf.math.minimum(height, width) / image_size, tf.float32), tf.int32)\n\n    offset_height = ((height - padded_center_crop_size) + 1) // 2\n    offset_width = ((width - padded_center_crop_size) + 1) // 2\n\n    image = tf.image.decode_jpeg(image_bytes, channels=3)\n    image = tf.image.crop_to_bounding_box(image, offset_height, offset_width, padded_center_crop_size,\n                                           padded_center_crop_size)\n    image = tf.image.resize(image, [image_size, image_size], method=resize_method)\n\n    return image\n\n", "CodeBleu_score": 46.12389979909122}, {"code": "def _decode_and_center_crop(image_bytes, image_size, resize_method)\n\t\n    image = tf.image.decode_jpeg(image_bytes, channels=3)\n    shape = tf.shape(image)\n    image_height = shape[0]\n    image_width = shape[1]\n    padded_center_crop_height = image_size\n    padded_center_crop_width = image_size\n    offset_height = ((image_height - padded_center_crop_height) + 1) // 2\n    offset_width = ((image_width - padded_center_crop_width) + 1) // 2\n    image = tf.image.crop_to_bounding_box(image, offset_height, offset_width, padded_center_crop_height, padded_center_crop_width)\n    image = tf.image.resize([image], [image_size, image_size], method=resize_method)[0]\n    return image\n\n", "CodeBleu_score": 34.70044941765849}]}
{"instance_id": "cf16a6d37111", "generation_result": [{"code": "def preprocess_for_train(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    image = tf.image.decode_image(image_bytes, channels=3)\n    image = tf.image.resize(image, [image_size, image_size], method=interpolation)\n    image = tf.image.random_crop(image, [image_size, image_size, 3])\n    image = tf.image.random_flip_left_right(image)\n    image = tf.cast(image, dtype=tf.float32)\n    image = tf.divide(image, 255.0)\n    if use_bfloat16:\n        image = tf.cast(image, dtype=tf.bfloat16)\n    return image\n\n", "CodeBleu_score": 37.400537851582364}, {"code": "def preprocess_for_train(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    image = tf.image.decode_image(image_bytes, channels=3)\n    image = tf.image.resize(image, (image_size, image_size), method=interpolation)\n    image = tf.image.random_crop(image, (image_size, image_size, 3))\n    image = tf.image.random_flip_left_right(image)\n    image = tf.image.random_flip_up_down(image)\n    image = tf.image.random_brightness(image, max_delta=0.1)\n    image = tf.image.random_contrast(image, lower=0.8, upper=1.2)\n    image = tf.image.random_hue(image, max_delta=0.1)\n    image = tf.image.random_saturation(image, lower=0.8, upper=1.2)\n    image = tf.image.random_gamma(image, max_delta=0.1)\n    image = tf.cast(image, dtype=tf.float32)\n    if use_bfloat16:\n        image = tf.cast(image, dtype=tf.bfloat16)\n    image = tf.reshape(image, [1, image_size, image_size, 3])\n    return image\n\n", "CodeBleu_score": 32.843811052454974}, {"code": "def preprocess_for_train(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    image = tf.image.decode_image(image_bytes, channels=3)\n    image = tf.image.resize(image, (image_size, image_size), interpolation)\n    image = tf.image.random_crop(image, (image_size, image_size, 3))\n    image = tf.image.random_flip_left_right(image)\n    image = tf.reshape(image, [image_size, image_size, 3])\n    image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n    if use_bfloat16:\n        image = tf.cast(image, dtype=tf.bfloat16)\n    return image\n\n", "CodeBleu_score": 39.704454747896534}]}
{"instance_id": "78b2a3bd2142", "generation_result": [{"code": "def preprocess_for_eval(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    image = tf.image.decode_image(image_bytes, channels=3)\n    image = tf.image.resize(image, [image_size, image_size], method=interpolation)\n    image = tf.image.central_crop(image, central_fraction=0.875)\n    image = tf.image.resize(image, [image_size, image_size], method=interpolation)\n    if use_bfloat16:\n        image = tf.cast(image, dtype=tf.bfloat16)\n    else:\n        image = tf.cast(image, dtype=tf.float32)\n    return image\n\n", "CodeBleu_score": 36.60017941738933}, {"code": "def preprocess_for_eval(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    image = tf.io.decode_image(image_bytes, channels=3)\n    image = tf.image.resize(image, (image_size, image_size), method=interpolation)\n    image = tf.image.central_crop(image, central_fraction=0.875)\n    image = tf.reshape(image, [image_size, image_size, 3])\n    if use_bfloat16:\n        image = tf.cast(image, dtype=tf.bfloat16)\n    else:\n        image = tf.cast(image, dtype=tf.float32)\n    return image\n\n", "CodeBleu_score": 41.06971481821222}, {"code": "def preprocess_for_eval(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    image_bytes = tf.image.decode_image(image_bytes, channels=3)\n    image_bytes = tf.image.convert_image_dtype(image_bytes, tf.float32)\n    image_bytes = tf.image.resize(image_bytes, [image_size, image_size], method=interpolation)\n    image_bytes = tf.image.central_crop(image_bytes, central_fraction=0.875)\n    image_bytes = tf.reshape(image_bytes, [image_size, image_size, 3])\n    if use_bfloat16:\n        image_bytes = tf.cast(image_bytes, dtype=tf.bfloat16)\n    else:\n        image_bytes = tf.cast(image_bytes, dtype=tf.float32)\n    return image_bytes\n\n", "CodeBleu_score": 32.23831354352662}]}
{"instance_id": "8bd4364a9a0e", "generation_result": [{"code": "def evaluate(self, model, eval_dataloader=None, output_file=\"merged\")\n\t\n\n    # Check if a dataloader was passed\n    if eval_dataloader is None:\n        eval_dataloader = self.eval_dataloader\n\n    # Check if a filename was passed\n    if output_file is None:\n        output_file = \"merged\"\n\n    # Create output directory if it doesn't exist\n    if not os.path.exists(self.output_dir):\n        os.mkdir(self.output_dir)\n\n    # Initialize output file\n    if self.output_mode == \"classification\":\n        output_file = os.path.join(self.output_dir, output_file + \".tsv\")\n        with open(output_file, \"w\") as writer:\n            writer.write(\"index\\tprediction", "CodeBleu_score": 49.2416963500247}, {"code": "def evaluate(self, model, eval_dataloader=None, output_file=\"merged\")\n\t\n    if eval_dataloader is None:\n        eval_dataloader = self.eval_dataloader\n\n    # initialize metric\n    metric = self.metric\n    metric.reset()\n\n    # set model to eval mode\n    model.eval()\n\n    # initialize progress bar\n    pbar = self.pbar\n    pbar.set_description(\"Evaluating\")\n\n    # initialize prediction file\n    prediction_file = self.get_prediction_file(output_file)\n\n    # initialize prediction file\n    prediction_file = self.get_prediction_file(output_file)\n    prediction_file.write(self.header)\n\n    # initialize prediction file\n    prediction_file = self.get_prediction_file(output_file)\n    prediction_file.write(self.header)\n\n    # initialize prediction file\n    prediction_file = self.get_prediction_file(output_file)\n    prediction_file.write(self.header)\n\n    # initialize prediction file\n    prediction_file = self.get_prediction_file(output_file)\n    prediction_file.write(self.header)\n\n    # initialize prediction file\n    prediction_file = self.get_prediction_file(output_file)\n    prediction_file.write(self.header)\n\n    # initialize prediction file\n    prediction_file = self.get_prediction_file(output_file)\n    prediction_file.write(self.header)\n\n    # initialize prediction file\n    prediction_file = self.get_prediction_file(output_file)\n    prediction_file.write(self.header)\n\n    # initialize prediction file\n    prediction_file = self.get_prediction_file(output_file)\n    prediction_file.write(self.header)\n\n    # initialize prediction file\n    prediction_file = self.get_prediction_file(output_file)\n    prediction_file.write(self.header)\n\n    # initialize prediction file\n    prediction_file = self.get_prediction_file(output_file)\n    prediction_file.write(self.header)\n\n    # initialize prediction file\n    prediction_file = self", "CodeBleu_score": 38.92774634321267}, {"code": "def evaluate(self, model, eval_dataloader=None, output_file=\"merged\")\n\t\n\n    # Setup the evaluation dataloader\n    if eval_dataloader is None:\n        eval_dataloader = self.eval_dataloader()\n\n    # Setup the output file\n    if output_file is None:\n        output_file = \"merged\"\n\n    # Setup the output directory\n    if not os.path.exists(self.output_dir):\n        os.makedirs(self.output_dir)\n\n    # Setup the output filename\n    if not os.path.exists(os.path.join(self.output_dir, output_file)):\n        os.makedirs(os.path.join(self.output_dir, output_file))\n\n    # Setup the output filename\n    output_file = os.path.join(self.output_dir, output_file, \"predictions.tsv\")\n\n    # Setup the output file\n    with open(output_file, \"w\") as f:\n        f.write(\"id\\tprediction", "CodeBleu_score": 41.97261608634176}]}
{"instance_id": "22d682c958c0", "generation_result": [{"code": " def print_computed_metrics(self, metrics)\n\t\n    print(\"Rank1: {0:.4f}\".format(metrics[\"rank1\"]))\n    print(\"Rank5: {0:.4f}\".format(metrics[\"rank5\"]))\n    print(\"mIoU: {0:.4f}\".format(metrics[\"miou\"]))\n\n\n def print_computed_metrics_json(self, metrics):\n    \"\"\"The function prints the computed metrics for rank1, rank5, and miou.", "CodeBleu_score": 27.74393872603641}, {"code": " def print_computed_metrics(self, metrics)\n\t\n    print('Rank-1: {:.4f}", "CodeBleu_score": 3.5656738493734186}, {"code": " def print_computed_metrics(self, metrics)\n\t\n    print(\"Rank-1: %.4f\" % metrics[\"rank1\"])\n    print(\"Rank-5: %.4f\" % metrics[\"rank5\"])\n    print(\"mIoU: %.4f\" % metrics[\"miou\"])\n\n", "CodeBleu_score": 20.307313824856216}]}
{"instance_id": "39c1a50b7e7d", "generation_result": [{"code": "def _eval_predictions(self, segments, data)\n\t\n\n    # Evaluate predictions\n    # Calculate IoU and rank for each prediction\n    # Calculate average IoU and rank for top predictions\n    # Calculate percentage of segments with rank 1 and 5\n    # Calculate mean IoU across all segments\n    # Return rank1, rank5, and mean IoU\n\n    # Initialize IoU and rank lists\n    iou_list = []\n    rank_list = []\n\n    # Iterate over each prediction-data pair\n    for i in range(len(segments)):\n        # Initialize IoU and rank lists for each prediction\n        iou = []\n        rank = []\n        # Iterate over each ground truth segment\n        for j in range(len(data)):\n            # Calculate IoU\n            iou.append(self.iou(segments[i], data[j]))\n            # Calculate rank\n            if iou[j] == max(iou):\n                rank.append(1)\n            else:\n                rank.append(0)\n\n        # Append IoU and rank lists\n        iou_list.append(iou)\n        rank_list.append(rank)\n\n    # Calculate average IoU and rank for top predictions\n    # Calculate percentage of segments with rank 1 and 5\n    # Calculate mean IoU across all segments\n    # Return rank1, rank5, and mean IoU\n\n    # Calculate average IoU and rank for top predictions\n    iou_list_top = []\n    rank_list_top = []\n    for i in range(len(segments)):\n        # Initialize IoU and rank lists for each prediction\n        iou_top = []\n        rank_top = []\n        # Iterate over each ground truth segment\n        for j in range(len(data)):\n            # Calculate IoU\n            iou_top.append(iou_list[i][j])\n            # Calculate rank\n            if iou_top[j] == max(iou_top):\n                rank_top.append(1)\n            else:\n                rank_top.append(0)\n\n        # Append IoU and rank lists\n        iou_list_top.append(iou_top)", "CodeBleu_score": 23.84458829801333}, {"code": "def _eval_predictions(self, segments, data)\n\t\n    if self.verbose:\n        print(\"Evaluating predictions...\")\n\n    # get the ground truth data\n    gt_data = data[\"ground_truth\"]\n    # get the prediction data\n    pred_data = data[\"prediction\"]\n\n    # get the number of segments\n    num_segments = len(segments)\n\n    # initialize the rank1 and rank5 values\n    rank1 = 0\n    rank5 = 0\n\n    # initialize the IoU values\n    IoU1 = 0\n    IoU5 = 0\n\n    # initialize the IoU values for each segment\n    IoU1s = [0] * num_segments\n    IoU5s = [0] * num_segments\n\n    # initialize the number of predictions for each segment\n    num_preds = [0] * num_segments\n\n    # get the number of predictions\n    num_preds = len(pred_data)\n\n    # get the number of ground truths\n    num_gt = len(gt_data)\n\n    # get the number of predictions per segment\n    num_preds_per_seg = [0] * num_segments\n\n    # get the number of ground truths per segment\n    num_gt_per_seg = [0] * num_segments\n\n    # get the IoU for each prediction\n    pred_IoUs = [0] * num_preds\n\n    # get the IoU for each ground truth\n    gt_IoUs = [0] * num_gt\n\n    # get the IoU for each segment\n    seg_IoUs = [0] * num_segments\n\n    # get the IoU for each segment prediction\n    seg_IoUs_pred = [0] * num_segments\n\n    # get the IoU for each segment ground truth\n    seg_IoUs_gt = [0] * num_segments\n\n    # get the segment number for each prediction\n    pred_seg_nums = [0] * num_preds\n\n    # get the segment number for each ground truth\n    gt_seg_nums = [0] * num_gt\n\n    # get the", "CodeBleu_score": 13.343857961842353}, {"code": "def _eval_predictions(self, segments, data)\n\t\n    rank1 = 0\n    rank5 = 0\n    mean_iou = 0\n    for segment, data_ in zip(segments, data):\n        if len(data_) > 0:\n            iou = self._iou(segment, data_)\n            rank = self._rank(iou)\n            rank1 += rank[0]\n            rank5 += rank[4]\n            mean_iou += iou.mean()\n    rank1 /= len(segments)\n    rank5 /= len(segments)\n    mean_iou /= len(segments)\n    return rank1, rank5, mean_iou\n\n", "CodeBleu_score": 27.457024423928367}]}
{"instance_id": "8ccf84f8d034", "generation_result": [{"code": "def predict_loop(self, model, eval_dataloader, output_file=None)\n\t\n    # Initialize the list of scores\n    scores = []\n    # Set the model to evaluation mode\n    model.eval()\n    # Transfer the model to GPU 0\n    model.to(self.device)\n    # Iterate over the dataloader\n    with torch.no_grad():\n        for batch in tqdm(eval_dataloader):\n            # Transfer the data to the appropriate context\n            batch = tuple(t.to(self.device) for t in batch)\n            # Make predictions without computing gradients\n            outputs = model(*batch)\n            # Update the outputs with the input data\n            outputs = outputs[0]\n            # Process the outputs further\n            scores.extend(outputs.tolist())\n    # Finalize and return the results\n    return self.finalize_predictions(scores, output_file)\n\n", "CodeBleu_score": 39.95678485085106}, {"code": "def predict_loop(self, model, eval_dataloader, output_file=None)\n\t\n    model.eval()\n    model = model.to(self.device)\n    predictions = []\n    with torch.no_grad():\n        for batch in tqdm(eval_dataloader):\n            for k, v in batch.items():\n                batch[k] = v.to(self.device)\n            batch_size = batch['input_ids'].shape[0]\n            logits = model(**batch)\n            logits = logits.detach().cpu().numpy()\n            predictions.extend(logits)\n    if output_file:\n        with open(output_file, 'wb') as f:\n            pickle.dump(predictions, f)\n    return predictions\n\n", "CodeBleu_score": 35.258249517690345}, {"code": "def predict_loop(self, model, eval_dataloader, output_file=None)\n\t\n    model.eval()\n    model.to(0)\n    all_scores = []\n    for batch in tqdm(eval_dataloader):\n        batch = {k: v.to(0) for k, v in batch.items()}\n        with torch.no_grad():\n            scores = model(**batch)\n        all_scores.extend(scores)\n    all_scores = torch.cat(all_scores, dim=0)\n    if output_file is not None:\n        with open(output_file, \"wb\") as f:\n            pickle.dump(all_scores.cpu().numpy(), f)\n    return all_scores\n\n", "CodeBleu_score": 36.59411965934145}]}
{"instance_id": "e68529ad727d", "generation_result": [{"code": "def _aggregate_scores(self, scores)\n\t\n    assert len(scores) == 2\n    video_scores = scores[0]\n    text_scores = scores[1]\n    assert video_scores.shape[0] == text_scores.shape[0]\n    assert video_scores.shape[1] == self.video_dim\n    assert text_scores.shape[1] == self.text_dim\n    full_scores = torch.cat((video_scores, text_scores), dim=1)\n    self.full_scores = full_scores\n    return torch.mm(full_scores, full_scores.t())\n\n", "CodeBleu_score": 33.82912508835372}, {"code": "def _aggregate_scores(self, scores)\n\t\n    assert len(scores) == 2\n    self.full_scores = torch.cat(scores, dim=0)\n    self.full_scores = self.full_scores.view(2, -1)\n    self.full_scores = self.full_scores.sum(dim=0)\n    self.full_scores = self.full_scores.view(-1)\n    self.full_scores.data.zero_()\n    return self.full_scores\n\n", "CodeBleu_score": 31.475423945311487}, {"code": "def _aggregate_scores(self, scores)\n\t\n    assert len(scores) == 2, \"The number of sets of scores must be exactly two.\"\n    self.full_scores = torch.cat(scores, dim=0)\n    self.full_scores = self.full_scores.view(len(scores[0]), -1)\n    self.full_scores = self.full_scores.cpu().numpy()\n    return self.full_scores\n\n", "CodeBleu_score": 30.459706764667015}]}
{"instance_id": "e4e1861fc357", "generation_result": [{"code": "def finalize(self, Y_pred, Y_true, output_file=None)\n\t\n\n    # concatenate predicted and true labels\n    Y_pred = torch.cat(Y_pred, dim=0).numpy()\n    Y_true = torch.cat(Y_true, dim=0).numpy()\n\n    # check for prediction errors\n    error_samples = []\n    for i in range(Y_pred.shape[0]):\n        if Y_pred[i] != Y_true[i]:\n            error_samples.append(i)\n\n    # print samples of prediction errors\n    if len(error_samples) > 0:\n        print(f\"Prediction Errors: {len(error_samples)}\")\n        for i in range(len(error_samples)):\n            print(f\"{i}: {Y_pred[error_samples[i]]} != {Y_true[error_samples[i]]}\")\n\n    # save predictions and true labels as pickle file\n    if output_file:\n        with open(output_file, \"wb\") as f:\n            pickle.dump({\"Y_pred\": Y_pred, \"Y_true\": Y_true}, f)\n\n    # return predicted and true labels\n    return {\"Y_pred\": Y_pred, \"Y_true\": Y_true}\n\n", "CodeBleu_score": 41.12329022497216}, {"code": "def finalize(self, Y_pred, Y_true, output_file=None)\n\t\n    Y_pred = torch.cat(Y_pred, dim=0).numpy()\n    Y_true = torch.cat(Y_true, dim=0).numpy()\n    if output_file is not None:\n        with open(output_file, 'wb') as f:\n            pickle.dump({'Y_pred': Y_pred, 'Y_true': Y_true}, f)\n    error_samples = []\n    for i in range(len(Y_pred)):\n        if Y_pred[i] != Y_true[i]:\n            error_samples.append((Y_pred[i], Y_true[i]))\n    if len(error_samples) > 0:\n        print(f'{len(error_samples)} prediction errors found. Printing first 10 samples:')\n        for i in range(min(10, len(error_samples))):\n            print(f'Predicted: {error_samples[i][0]}, True: {error_samples[i][1]}')\n    return {'Y_pred': Y_pred, 'Y_true': Y_true}\n\n", "CodeBleu_score": 47.39868040329676}, {"code": "def finalize(self, Y_pred, Y_true, output_file=None)\n\t\n\n    # convert to numpy arrays\n    Y_pred = torch.cat(Y_pred, dim=0).numpy()\n    Y_true = torch.cat(Y_true, dim=0).numpy()\n\n    # check for prediction errors\n    prediction_errors = []\n    for i in range(len(Y_pred)):\n        if Y_pred[i] != Y_true[i]:\n            prediction_errors.append(i)\n\n    # print prediction errors\n    if len(prediction_errors) > 0:\n        print(\"Prediction Errors:\")\n        for i in prediction_errors:\n            print(f\"{i}: {Y_pred[i]}, {Y_true[i]}\")\n\n    # save predictions and true labels as a pickle file\n    if output_file is not None:\n        with open(output_file, \"wb\") as f:\n            pickle.dump({\"Y_pred\": Y_pred, \"Y_true\": Y_true}, f)\n\n    # return predictions and true labels\n    return {\"Y_pred\": Y_pred, \"Y_true\": Y_true}\n\n", "CodeBleu_score": 42.559324202578196}]}
{"instance_id": "7df9ab9a4920", "generation_result": [{"code": "def reduce_metrics(logging_outputs) -> None\n\t\n    loss_sum = sum(log.get(\"loss\", 0) for log in logging_outputs)\n    ntokens = sum(log.get(\"ntokens\", 0) for log in logging_outputs)\n    sample_size = sum(log.get(\"sample_size\", 0) for log in logging_outputs)\n    metrics.log_scalar(\"loss\", loss_sum / sample_size / math.log(2), sample_size, round=3)\n    metrics.log_derived(\"ppl\", lambda meters: round(2**metrics.error_stats(meters)[\"loss\"].avg, 3))\n\n", "CodeBleu_score": 52.410531086202525}, {"code": "def reduce_metrics(logging_outputs) -> None\n\t\n    loss_sum = sum(log.get(\"loss\", 0) for log in logging_outputs)\n    ntokens = sum(log.get(\"ntokens\", 0) for log in logging_outputs)\n    sample_size = sum(log.get(\"sample_size\", 0) for log in logging_outputs)\n    metrics.log_scalar(\"loss\", loss_sum / sample_size / math.log(2), sample_size, round=3)\n    metrics.log_derived(\"ppl\", lambda meters: utils.get_perplexity(meters[\"loss\"].avg))\n    metrics.log_scalar(\"ntokens\", ntokens)\n    metrics.log_scalar(\"nsentences\", sample_size)\n\n", "CodeBleu_score": 51.86634958965691}, {"code": "def reduce_metrics(logging_outputs) -> None\n\t\n    # get the number of tokens\n    ntokens = sum(log.get(\"ntokens\", 0) for log in logging_outputs)\n    nsentences = sum(log.get(\"nsentences\", 0) for log in logging_outputs)\n    sample_size = sum(log.get(\"sample_size\", 0) for log in logging_outputs)\n    # sum loss values\n    loss_sum = sum(log.get(\"loss\", 0) for log in logging_outputs)\n    # sum sample sizes\n    ntokens_sum = sum(log.get(\"ntokens\", 0) for log in logging_outputs)\n    nsentences_sum = sum(log.get(\"nsentences\", 0) for log in logging_outputs)\n    sample_size_sum = sum(log.get(\"sample_size\", 0) for log in logging_outputs)\n    # calculate average loss\n    metrics.log_scalar(\"loss\", loss_sum / sample_size_sum / math.log(2), sample_size, round=3)\n    metrics.log_scalar(\"ntokens\", ntokens_sum / sample_size_sum, sample_size, round=3)\n    metrics.log_scalar(\"nsentences\", nsentences_sum / sample_size_sum, sample_size, round=3)\n\n", "CodeBleu_score": 48.51753743540519}]}
{"instance_id": "7f9c326a96f7", "generation_result": [{"code": "def _mm_on_the_fly(\n        self,\n        cmasks,\n        vmasks,\n        attention_mask\n    )\n\t\n    # Generate attention mask and token type IDs\n    if attention_mask is None:\n        attention_mask = torch.zeros(\n            (self.cmasks.size(0), self.cmasks.size(1), self.cmasks.size(2)),\n            dtype=self.cmasks.dtype,\n            device=self.cmasks.device\n        )\n        attention_mask[self.cmasks.bool()] = -1e9\n        attention_mask[self.vmasks.bool()] = -1e9\n    attention_mask = attention_mask.unsqueeze(1)\n    attention_mask = attention_mask.to(dtype=self.cmasks.dtype)\n    attention_mask = (1.0 - attention_mask) * -10000.0\n    # Generate token type IDs\n    token_type_ids = torch.zeros(\n        (self.cmasks.size(0), self.cmasks.size(1), self.cmasks.size(2)),\n        dtype=self.cmasks.dtype,\n        device=self.cmasks.device\n    )\n    token_type_ids[self.vmasks.bool()] = 1\n    return attention_mask, token_type_ids\n\n", "CodeBleu_score": 42.90865841115862}, {"code": "def _mm_on_the_fly(\n        self,\n        cmasks,\n        vmasks,\n        attention_mask\n    )\n\t\n    # Create attention mask if necessary\n    if attention_mask is None:\n        attention_mask = (cmasks + vmasks).gt(0)\n    # Create token type IDs\n    token_type_ids = torch.cat([\n        torch.zeros_like(cmasks).long(),\n        torch.ones_like(vmasks).long()\n    ], dim=-1)\n    return attention_mask, token_type_ids\n\n", "CodeBleu_score": 30.77740200263638}, {"code": "def _mm_on_the_fly(\n        self,\n        cmasks,\n        vmasks,\n        attention_mask\n    )\n\t\n    # cmasks: (batch_size, num_visual_features)\n    # vmasks: (batch_size, num_visual_features)\n    # attention_mask: (batch_size, num_visual_features + num_text_features)\n    # attention_mask.sum(-1) == num_text_features\n    # cmasks.sum(-1) == num_visual_features\n    # vmasks.sum(-1) == num_visual_features\n    # cmasks.shape == vmasks.shape == attention_mask.shape\n    # attention_mask.sum(-1) == num_text_features\n    # cmasks.sum(-1) == num_visual_features\n    # vmasks.sum(-1) == num_visual_features\n    # cmasks.shape == vmasks.shape == attention_mask.shape\n    # attention_mask.sum(-1) == num_text_features\n    # cmasks.sum(-1) == num_visual_features\n    # vmasks.sum(-1) == num_visual_features\n    # cmasks.shape == vmasks.shape == attention_mask.shape\n    if attention_mask is None:\n        attention_mask = cmasks + vmasks\n    attention_mask = attention_mask.unsqueeze(1).unsqueeze(1)\n    attention_mask = (1.0 - attention_mask) * -100000.0\n    attention_mask = attention_mask.expand(-1, -1, self.config.max_position_embeddings)\n    # attention_mask.shape == (batch_size, 1, 1, num_text_features + num_visual_features)\n    # attention_mask.sum(-1) == num_text_features + num_visual_features\n    # attention_mask.sum(-1).shape == (batch_size, 1, 1)\n    # attention_mask.shape == (batch_size, 1, 1, num_text_features + num_visual_features)\n    # attention_mask.sum(-1) == num_text_features + num_visual_features\n    # attention_", "CodeBleu_score": 26.42046474783553}]}
{"instance_id": "fa99d2a148a5", "generation_result": [{"code": "def prepare_inputs_for_generation(\n        self,\n        input_ids,\n        input_video_embeds,\n        attention_mask=None,\n        token_type_ids=None,\n        **model_kwargs\n    )\n\t\n    # The default main input is text\n    input_ids = input_ids.clone()\n    input_video_embeds = input_video_embeds.clone()\n\n    # The default input is text\n    if token_type_ids is None:\n        token_type_ids = torch.zeros(input_ids.shape, dtype=torch.long, device=input_ids.device)\n    if attention_mask is None:\n        attention_mask = torch.ones(input_ids.shape, dtype=torch.long, device=input_ids.device)\n\n    # Adjust the sequence length\n    attention_mask = attention_mask.long()\n    input_ids, input_video_embeds, attention_mask, token_type_ids = self._pad_to_max_len(\n        input_ids, input_video_embeds, attention_mask, token_type_ids\n    )\n\n    # Prepare the inputs as a dictionary\n    model_inputs = {\n        \"input_ids\": input_ids,\n        \"input_video_embeds\": input_video_embeds,\n        \"attention_mask\": attention_mask,\n        \"token_type_ids\": token_type_ids,\n    }\n    return model_inputs\n\n", "CodeBleu_score": 43.50613945451068}, {"code": "def prepare_inputs_for_generation(\n        self,\n        input_ids,\n        input_video_embeds,\n        attention_mask=None,\n        token_type_ids=None,\n        **model_kwargs\n    )\n\t\n    if attention_mask is None:\n        attention_mask = input_ids.new_ones(input_ids.shape)\n    if token_type_ids is None:\n        token_type_ids = input_ids.new_zeros(input_ids.shape)\n\n    # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n    # ourselves in which case we just need to make it broadcastable to all heads.\n    extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n\n    # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n    # masked positions, this operation will create a tensor which is 0.0 for\n    # positions we want to attend and -10000.0 for masked positions.\n    # Since we are adding it to the raw scores before the softmax, this is\n    # effectively the same as removing these entirely.\n    extended_attention_mask = extended_attention_mask.to(\n        dtype=next(self.parameters()).dtype)  # fp16 compatibility\n    extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n\n    # Prepare head mask if needed\n    # 1.0 in head_mask indicate we keep the head\n    # attention_probs has shape bsz x n_heads x N x N\n    # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n    # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x 1 x 1]\n    if isinstance(model_kwargs[\"head_mask\"], list):\n        model_kwargs[\"head_mask\"] = [None] * len(model_kwargs[\"head_mask\"])\n        for idx, head_mask in enumerate(model_kwargs[\"head_mask\"]):\n            if head_mask is not None:\n                model_kwargs[\"head_mask\"][idx] = head_mask.unsqueeze(0).unsqueeze", "CodeBleu_score": 33.71787313193017}, {"code": "def prepare_inputs_for_generation(\n        self,\n        input_ids,\n        input_video_embeds,\n        attention_mask=None,\n        token_type_ids=None,\n        **model_kwargs\n    )\n\t\n    # input_ids = input_ids.to(self.device)\n    # input_video_embeds = input_video_embeds.to(self.device)\n    # attention_mask = attention_mask.to(self.device)\n    # token_type_ids = token_type_ids.to(self.device)\n    input_ids = input_ids.unsqueeze(0)\n    input_video_embeds = input_video_embeds.unsqueeze(0)\n    attention_mask = attention_mask.unsqueeze(0)\n    token_type_ids = token_type_ids.unsqueeze(0)\n    return input_ids, input_video_embeds, attention_mask, token_type_ids\n\n", "CodeBleu_score": 13.16510551775642}]}
{"instance_id": "b1b0d25fe620", "generation_result": [{"code": "def _init_pipeline(self, cfg: ConfigType) -> Compose\n\t\n    pipeline = self.test_pipeline\n    if 'img_id' in pipeline[0]['meta_keys']:\n        pipeline[0]['meta_keys'].remove('img_id')\n    load_image_idx = [i for i, x in enumerate(pipeline) if x['type'] == 'LoadImageFromFile'][0]\n    pipeline[load_image_idx]['type'] = 'mmdet.InferencerLoader'\n    return Compose(pipeline)\n\n", "CodeBleu_score": 24.965624109550152}, {"code": "def _init_pipeline(self, cfg: ConfigType) -> Compose\n\t\n    pipeline = Compose(cfg.test_pipeline)\n    # remove img_id from meta_keys\n    meta_keys = pipeline.meta_keys\n    if 'img_id' in meta_keys:\n        meta_keys.remove('img_id')\n    pipeline.meta_keys = meta_keys\n    # set load_image transform to mmdet.InferencerLoader\n    for i, transform in enumerate(pipeline.transforms):\n        if transform['type'] == 'LoadImageFromFile':\n            pipeline.transforms[i]['type'] = 'mmdet.InferencerLoader'\n            break\n    else:\n        raise ValueError('LoadImageFromFile transform not found in test pipeline')\n    return pipeline\n\n", "CodeBleu_score": 29.263238906880733}, {"code": "def _init_pipeline(self, cfg: ConfigType) -> Compose\n\t\n    pipeline = self.test_pipeline\n    if isinstance(pipeline, dict):\n        pipeline = [pipeline]\n    pipeline = Compose(pipeline)\n    if 'img_id' in pipeline.meta_keys:\n        pipeline.meta_keys.remove('img_id')\n    idx = 0\n    for i, transform in enumerate(pipeline.transforms):\n        if transform.__class__.__name__ == 'LoadImageFromFile':\n            idx = i\n            break\n    else:\n        raise RuntimeError('Cannot find LoadImageFromFile in test pipeline')\n    pipeline.transforms[idx] = LoadImageFromFile(to_float32=True)\n    pipeline.transforms[idx].type = 'mmdet.InferencerLoader'\n    return pipeline\n\n", "CodeBleu_score": 34.536907107644296}]}
{"instance_id": "7967962856eb", "generation_result": [{"code": "def _inputs_to_list(self, inputs: InputsType) -> list\n\t\n    if isinstance(inputs, str):\n        if isdir(inputs):\n            inputs = list_dir_or_file(inputs)\n        else:\n            inputs = [inputs]\n    if not isinstance(inputs, (list, tuple)):\n        inputs = [inputs]\n    return inputs\n\n", "CodeBleu_score": 39.20852424152697}, {"code": "def _inputs_to_list(self, inputs: InputsType) -> list\n\t\n    if isinstance(inputs, str):\n        if isdir(inputs):\n            inputs = list_dir_or_file(inputs)\n        else:\n            inputs = [inputs]\n    elif not isinstance(inputs, (list, tuple)):\n        inputs = [inputs]\n    return inputs\n\n", "CodeBleu_score": 37.27664845802001}, {"code": "def _inputs_to_list(self, inputs: InputsType) -> list\n\t\n    if isinstance(inputs, str):\n        inputs = list_dir_or_file(inputs)\n    elif not isinstance(inputs, (list, tuple)):\n        inputs = [inputs]\n    return inputs\n\n", "CodeBleu_score": 29.798240086807446}]}
{"instance_id": "74f727f06fa0", "generation_result": [{"code": "def _get_chunk_data(self, inputs: Iterable, chunk_size: int)\n\t\n    if isinstance(inputs, dict):\n        inputs = [inputs]\n    for i, data in enumerate(inputs):\n        if isinstance(data, dict):\n            data = [data]\n        for j, d in enumerate(data):\n            if j >= chunk_size:\n                yield self.pipeline(d)\n            elif j == 0:\n                self.data = self.pipeline(d)\n\n    if getattr(self, 'data', None) is not None:\n        yield self.pipeline(self.data)\n\n", "CodeBleu_score": 21.845148674096542}, {"code": "def _get_chunk_data(self, inputs: Iterable, chunk_size: int)\n\t\n    if isinstance(inputs, dict):\n        if 'img_path' in inputs:\n            inputs = [inputs]\n        elif 'img' in inputs:\n            inputs = [{'img': img} for img in inputs['img']]\n        else:\n            raise ValueError('inputs must contain \"img\" or \"img_path\"')\n\n    for i in track(range(len(inputs)), total=len(inputs)):\n        if i % chunk_size == 0:\n            data = []\n        data.append(self(inputs[i]))\n        if i % chunk_size == chunk_size - 1 or i == len(inputs) - 1:\n            yield data\n\n", "CodeBleu_score": 32.29421237696536}, {"code": "def _get_chunk_data(self, inputs: Iterable, chunk_size: int)\n\t\n    if isinstance(inputs, dict):\n        inputs = [inputs]\n    for i in track(range(0, len(inputs), chunk_size)):\n        chunk = inputs[i:i + chunk_size]\n        if len(chunk) > 0:\n            yield self.process_data(chunk)\n    if len(inputs) > len(inputs) % chunk_size:\n        chunk = inputs[len(inputs) - len(inputs) % chunk_size:]\n        if len(chunk) > 0:\n            yield self.process_data(chunk)\n\n", "CodeBleu_score": 25.580099154399722}]}
{"instance_id": "a43ab51afa59", "generation_result": [{"code": "def postprocess(\n        self,\n        preds: PredType,\n        visualization: Optional[List[np.ndarray]] = None,\n        return_datasamples: bool = False,\n        print_result: bool = False,\n        no_save_pred: bool = False,\n        pred_out_dir: str = '',\n        **kwargs,\n    ) -> Dict\n\t\n    if not no_save_pred:\n        if not isdir(pred_out_dir):\n            mmcv.mkdir_or_exist(pred_out_dir)\n    if not return_datasamples:\n        results = []\n        if isinstance(preds, dict):\n            preds = [preds]\n        for pred in preds:\n            if not no_save_pred:\n                if isinstance(pred['filename'], str):\n                    filename = pred['filename']\n                else:\n                    filename = pred['filename'][0]\n                file_path = join_path(pred_out_dir, filename)\n                if not no_save_pred:\n                    if isinstance(pred['img'], np.ndarray):\n                        mmcv.imwrite(pred['img'], file_path)\n                    else:\n                        mmcv.imwrite(pred['img'].data[0].cpu().numpy(), file_path)\n                if 'ori_shape' in pred:\n                    pred['img_shape'] = pred['ori_shape']\n                else:\n                    pred['img_shape'] = pred['img'].shape\n                pred['bbox_fields'] = []\n                pred['mask_fields'] = []\n            if 'bbox_fields' in pred:\n                for key in pred['bbox_fields']:\n                    pred[key] = np.array(pred[key], dtype=np.float32)\n            if 'mask_fields' in pred:\n                for key in pred['mask_fields']:\n                    pred[key] = np.array(pred[key], dtype=np.uint8)\n            results.append(pred)\n    else:\n        warnings.warn('Saving datasamples is not supported.')\n    if print_result:\n        self.show_result(results, **kwargs)\n    return results\n\n", "CodeBleu_score": 40.12499731773824}, {"code": "def postprocess(\n        self,\n        preds: PredType,\n        visualization: Optional[List[np.ndarray]] = None,\n        return_datasamples: bool = False,\n        print_result: bool = False,\n        no_save_pred: bool = False,\n        pred_out_dir: str = '',\n        **kwargs,\n    ) -> Dict\n\t\n    if not no_save_pred:\n        if not isdir(pred_out_dir):\n            warnings.warn(f'Creating directory {pred_out_dir}')\n            mmcv.mkdir_or_exist(pred_out_dir)\n    results = {}\n    if visualization is not None:\n        results['visualization'] = visualization\n    if return_datasamples:\n        if pred_out_dir:\n            warnings.warn(\n                f'Saving predictions to {pred_out_dir} is not supported when return_datasamples is True.')\n        results['datasamples'] = []\n    for i, pred in enumerate(preds):\n        if return_datasamples:\n            if pred_out_dir:\n                warnings.warn(\n                    f'Saving predictions to {pred_out_dir} is not supported when return_datasamples is True.')\n            datasample = self.datasamples[i]\n            if isinstance(datasample, DetDataSample):\n                datasample.pred = pred\n                results['datasamples'].append(datasample)\n            else:\n                warnings.warn(\n                    f'Predictions can only be saved for DetDataSample.')\n        else:\n            pred = copy.deepcopy(pred)\n            pred['image_path'] = self.datasamples[i]['image_path']\n            results[self.datasamples[i]['image_path']] = pred\n        if print_result:\n            print(f'Image {i}: {pred[\"image_path\"]}')\n            for k, v in pred.items():\n                print(f'{k}: {v}')\n    return results\n\n", "CodeBleu_score": 42.73616139208879}, {"code": "def postprocess(\n        self,\n        preds: PredType,\n        visualization: Optional[List[np.ndarray]] = None,\n        return_datasamples: bool = False,\n        print_result: bool = False,\n        no_save_pred: bool = False,\n        pred_out_dir: str = '',\n        **kwargs,\n    ) -> Dict\n\t\n    if no_save_pred and pred_out_dir:\n        warnings.warn(\n            f'The output directory {pred_out_dir} is specified, but no predictions will be saved.')\n    if not no_save_pred and not isdir(pred_out_dir):\n        warnings.warn(\n            f'The output directory {pred_out_dir} does not exist. It will be created.')\n    if return_datasamples and pred_out_dir:\n        warnings.warn(\n            f'The output directory {pred_out_dir} is specified, but datasamples will not be saved.')\n    if not return_datasamples and pred_out_dir:\n        warnings.warn(\n            f'The output directory {pred_out_dir} is specified, but the predictions will not be returned.')\n    results = dict()\n    if return_datasamples:\n        results['datasamples'] = []\n    if return_datasamples and pred_out_dir:\n        warnings.warn(\n            f'The output directory {pred_out_dir} is specified, but datasamples will not be saved.')\n    if print_result:\n        results['print_result'] = []\n    if print_result and pred_out_dir:\n        warnings.warn(\n            f'The output directory {pred_out_dir} is specified, but the print_result will not be saved.')\n    if visualization:\n        results['visualization'] = visualization\n    if visualization and pred_out_dir:\n        warnings.warn(\n            f'The output directory {pred_out_dir} is specified, but the visualization will not be saved.')\n    if not return_datasamples and not print_result and not visualization:\n        results['predictions'] = []\n    if not return_datasamples and not print_result and not visualization and pred_out_dir:\n        warnings.warn(\n            f'The output directory {pred_out_dir} is specified, but the predictions will not be returned.')\n    if return_datasamples and not print_result and not visualization:\n        results['datasamples'] = []\n    if return_datasamples and print_result and not visualization:\n        results['", "CodeBleu_score": 34.96489592135772}]}
{"instance_id": "a3c2561f199c", "generation_result": [{"code": "def build_test_pipeline(cfg: ConfigType) -> ConfigType\n\t\n    assert isinstance(cfg, Config)\n    pipeline = copy.deepcopy(cfg.data.test.pipeline)\n    # modify the first transform in the pipeline to include only the 'Resize' transform\n    resize = None\n    for transform in pipeline[0]:\n        if transform.type == 'Resize':\n            resize = transform\n            break\n    assert resize is not None\n    pipeline[0] = [resize]\n    # combine the modified first and copied last transforms into a new test pipeline\n    test_pipeline = Compose(pipeline)\n    return test_pipeline\n\n", "CodeBleu_score": 28.581968979266232}, {"code": "def build_test_pipeline(cfg: ConfigType) -> ConfigType\n\t\n    test_pipeline = copy.deepcopy(cfg.data.test.pipeline)\n    # find and replace the first transform in test_pipeline\n    for i, transform in enumerate(test_pipeline):\n        if transform['type'] == 'Resize':\n            test_pipeline[i] = dict(type='Resize', **transform)\n            break\n    # find and replace the last transform in test_pipeline\n    for i, transform in enumerate(test_pipeline):\n        if transform['type'] == 'Collect':\n            test_pipeline[-1] = dict(type='Collect', **transform)\n            break\n    # combine the modified first and copied last transforms into a new test pipeline\n    test_pipeline = [test_pipeline[0]] + test_pipeline[-1:]\n    test_pipeline = Compose(test_pipeline)\n    return test_pipeline\n\n", "CodeBleu_score": 39.252033784777126}, {"code": "def build_test_pipeline(cfg: ConfigType) -> ConfigType\n\t\n    if isinstance(cfg, dict):\n        cfg = Config(cfg)\n    dataset_cfg = cfg.data.test\n    pipeline = copy.deepcopy(dataset_cfg.pipeline)\n    pipeline[0] = dict(type='Resize', **pipeline[0])\n    pipeline[-1] = dict(type='Collect', keys=['img'])\n    return Compose(pipeline)\n\n", "CodeBleu_score": 28.204254527135607}]}
{"instance_id": "48a358ab5a38", "generation_result": [{"code": "def inference_mot(model: nn.Module, img: np.ndarray, frame_id: int,\n                  video_len: int) -> SampleList\n\t\n    assert isinstance(img, np.ndarray), f'img should be np.ndarray, got {type(img)}'\n    assert isinstance(frame_id, int), f'frame_id should be int, got {type(frame_id)}'\n    assert isinstance(video_len, int), f'video_len should be int, got {type(video_len)}'\n\n    # build the data pipeline\n    test_pipeline = model.module.test_pipeline\n    test_pipeline = Compose(test_pipeline)\n\n    # prepare data\n    data = dict(img=img, frame_id=frame_id, video_len=video_len)\n    data = test_pipeline(data)\n    data = default_collate([data])\n\n    # forward the model\n    with torch.no_grad():\n        if next(model.parameters()).is_cuda:\n            # use RoIPool to avoid unsupported module\n            model.module.backbone.roi_layers[0] = RoIPool(\n                output_size=(7, 7),\n                spatial_scale=1 / 16,\n                sampling_ratio=2)\n        else:\n            warnings.warn(\n                'The model is on the CPU, which may cause unsupported modules '\n                'like RoIPool to be used.')\n\n        result = model(return_loss=False, **data)\n\n    return result\n\n", "CodeBleu_score": 37.24129122004188}, {"code": "def inference_mot(model: nn.Module, img: np.ndarray, frame_id: int,\n                  video_len: int) -> SampleList\n\t\n    data = dict(img=img, frame_id=frame_id, original_shape=img.shape, img_id=0, video_len=video_len)\n    data = test_pipeline(data)\n    data = default_collate([data])\n    data = revert_sync_batchnorm(data)\n    if next(model.parameters()).is_cuda:\n        data = data.cuda()\n    with torch.no_grad():\n        result = model(return_loss=False, **data)\n    return result\n\n", "CodeBleu_score": 28.642459871514152}, {"code": "def inference_mot(model: nn.Module, img: np.ndarray, frame_id: int,\n                  video_len: int) -> SampleList\n\t\n    assert isinstance(img, np.ndarray)\n    assert isinstance(frame_id, int)\n    assert isinstance(video_len, int)\n    # prepare input data\n    data = dict(\n        img=img,\n        frame_id=frame_id,\n        original_shape=img.shape[:2],\n        img_id=0,\n        video_len=video_len)\n    data = test_pipeline(data)\n    # forward\n    with torch.no_grad():\n        result = model(return_loss=False, **data)\n    return result\n\n", "CodeBleu_score": 26.21105244614123}]}
{"instance_id": "6c4ea2ce1787", "generation_result": [{"code": "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\n\t\n    while True:\n        total_length = len(tokens_a) + len(tokens_b)\n        if total_length <= max_num_tokens:\n            break\n\n        trunc_tokens = tokens_a if len(tokens_a) > len(tokens_b) else tokens_b\n        assert len(trunc_tokens) >= 1\n\n        # We want to sometimes truncate from the front and sometimes from the\n        # back to add more randomness and avoid biases.\n        if rng.random() < 0.5:\n            del trunc_tokens[0]\n        else:\n            trunc_tokens.pop()\n\n", "CodeBleu_score": 77.02217016729102}, {"code": "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\n\t\n    while True:\n        total_length = len(tokens_a) + len(tokens_b)\n        if total_length <= max_num_tokens:\n            break\n\n        trunc_tokens = tokens_a if len(tokens_a) > len(tokens_b) else tokens_b\n        assert len(trunc_tokens) >= 1\n\n        # We want to sometimes truncate from the front and sometimes from the\n        # back to add more randomness and avoid biases.\n        if rng.random() < 0.5:\n            del trunc_tokens[0]\n        else:\n            trunc_tokens.pop()\n\n", "CodeBleu_score": 77.02217016729102}, {"code": "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\n\t\n    while True:\n        total_length = len(tokens_a) + len(tokens_b)\n        if total_length <= max_num_tokens:\n            break\n\n        trunc_tokens = tokens_a if len(tokens_a) > len(tokens_b) else tokens_b\n        assert len(trunc_tokens) >= 1\n\n        # We want to sometimes truncate from the front and sometimes from the\n        # back to add more randomness and avoid biases.\n        if rng.random() < 0.5:\n            del trunc_tokens[0]\n        else:\n            trunc_tokens.pop()\n\n", "CodeBleu_score": 77.02217016729102}]}
{"instance_id": "007181b6729d", "generation_result": [{"code": "def get_activation(activation_string)\n\t\n    if not isinstance(activation_string, six.string_types):\n        return activation_string\n    if not activation_string:\n        return None\n    if activation_string.lower() == 'linear':\n        return None\n    if activation_string.lower() == 'relu':\n        return lambda x: tf.nn.relu(x)\n    if activation_string.lower() == 'leakyrelu':\n        return lambda x: tf.nn.leaky_relu(x)\n    if activation_string.lower() == 'elu':\n        return lambda x: tf.nn.elu(x)\n    if activation_string.lower() == 'selu':\n        return lambda x: tf.nn.selu(x)\n    if activation_string.lower() == 'softplus':\n        return lambda x: tf.nn.softplus(x)\n    if activation_string.lower() == 'softsign':\n        return lambda x: tf.nn.softsign(x)\n    if activation_string.lower() == 'relie':\n        return lambda x: tf.nn.relu(x)\n    if activation_string.lower() == 'ramp':\n        return lambda x: tf.nn.relu(x)\n    if activation_string.lower() == 'prelu':\n        return lambda x: tf.nn.relu(x)\n    if activation_string.lower() == 'leakyrelu':\n        return lambda x: tf.nn.leaky_relu(x)\n    if activation_string.lower() == 'elu':\n        return lambda x: tf.nn.elu(x)\n    if activation_string.lower() == 'softplus':\n        return lambda x: tf.nn.softplus(x)\n    if activation_string.lower() == 'softsign':\n        return lambda x: tf.nn.softsign(x)\n    if activation_string.lower() == 'sigmoid':\n        return lambda x: tf.nn.sigmoid(x)\n    if activation_string.lower() == 'tanh':\n        return lambda x: tf.nn.tanh(x)\n    if activation_string.lower", "CodeBleu_score": 45.366992052125056}, {"code": "def get_activation(activation_string)\n\t\n    if isinstance(activation_string, six.string_types):\n        if not activation_string:\n            return None\n        if activation_string.lower() == 'relu':\n            return tf.nn.relu\n        elif activation_string.lower() == 'tanh':\n            return tf.nn.tanh\n        elif activation_string.lower() == 'sigmoid':\n            return tf.nn.sigmoid\n        elif activation_string.lower() == 'softmax':\n            return tf.nn.softmax\n        elif activation_string.lower() == 'softplus':\n            return tf.nn.softplus\n        elif activation_string.lower() == 'softsign':\n            return tf.nn.softsign\n        elif activation_string.lower() == 'relu6':\n            return tf.nn.relu6\n        elif activation_string.lower() == 'elu':\n            return tf.nn.elu\n        elif activation_string.lower() == 'selu':\n            return tf.nn.selu\n        elif activation_string.lower() == 'crelu':\n            return tf.nn.crelu\n        elif activation_string.lower() == 'softplus_sparse':\n            return tf.nn.softplus_sparse\n        elif activation_string.lower() == 'softsign_sparse':\n            return tf.nn.softsign_sparse\n        elif activation_string.lower() == 'relu_sparse':\n            return tf.nn.relu_sparse\n        elif activation_string.lower() == 'relu6_sparse':\n            return tf.nn.relu6_sparse\n        elif activation_string.lower() == 'elu_sparse':\n            return tf.nn.elu_sparse\n        elif activation_string.lower() == 'softplus_sparse_sparse':\n            return tf.nn.softplus_sparse_sparse\n        elif activation_string.lower() == 'softsign_sparse_sparse':\n            return tf.nn.softsign_sparse_sparse\n        elif activation_string.lower() == 'relu_sparse_sparse':\n            return tf.nn.relu_sparse", "CodeBleu_score": 22.09452303950239}, {"code": "def get_activation(activation_string)\n\t\n    if not isinstance(activation_string, six.string_types):\n        return activation_string\n    if not activation_string:\n        return None\n    if activation_string.lower() == \"relu\":\n        return tf.nn.relu\n    if activation_string.lower() == \"softmax\":\n        return tf.nn.softmax\n    if activation_string.lower() == \"sigmoid\":\n        return tf.nn.sigmoid\n    if activation_string.lower() == \"tanh\":\n        return tf.nn.tanh\n    if activation_string.lower() == \"elu\":\n        return tf.nn.elu\n    if activation_string.lower() == \"softplus\":\n        return tf.nn.softplus\n    if activation_string.lower() == \"softsign\":\n        return tf.nn.softsign\n    if activation_string.lower() == \"relu6\":\n        return tf.nn.relu6\n    if activation_string.lower() == \"crelu\":\n        return tf.nn.crelu\n    if activation_string.lower() == \"softmax2d\":\n        return tf.nn.softmax\n    if activation_string.lower() == \"softmax3d\":\n        return tf.nn.softmax\n    if activation_string.lower() == \"softmax4d\":\n        return tf.nn.softmax\n    if activation_string.lower() == \"softmax5d\":\n        return tf.nn.softmax\n    raise ValueError(\"Unsupported activation function %s\" % activation_string)\n\n", "CodeBleu_score": 45.37098596426702}]}
{"instance_id": "d5d6e0315899", "generation_result": [{"code": "def dropout(input_tensor, dropout_prob)\n\t\n    if dropout_prob is None or dropout_prob == 0.0:\n        return input_tensor\n    output = input_tensor * (1.0 - dropout_prob)\n    output_shape = output.shape\n    output_shape = [output_shape[0], output_shape[1], output_shape[2]]\n    output = tf.nn.dropout(output, rate=dropout_prob)\n    output.set_shape(output_shape)\n    return output\n\n", "CodeBleu_score": 52.23650930014724}, {"code": "def dropout(input_tensor, dropout_prob)\n\t\n    if dropout_prob is None or dropout_prob == 0.0:\n        return input_tensor\n    output = input_tensor\n    scale = 1.0 / (1.0 - dropout_prob)\n    output = output * scale\n    return output\n\n", "CodeBleu_score": 53.471641696982886}, {"code": "def dropout(input_tensor, dropout_prob)\n\t\n    if dropout_prob is None or dropout_prob == 0.0:\n        return input_tensor\n    output = input_tensor\n    scale = 1.0 / (1.0 - dropout_prob)\n    if isinstance(input_tensor, collections.Mapping):\n        for k in input_tensor:\n            output[k] = dropout(input_tensor[k], dropout_prob)\n    else:\n        output = input_tensor * scale\n        if input_tensor.dtype == tf.float16:\n            output = tf.cast(output, tf.float16)\n        output = tf.nn.dropout(output, 1.0 - dropout_prob)\n    return output\n\n", "CodeBleu_score": 57.365919195054275}]}
{"instance_id": "828917b9ac53", "generation_result": [{"code": "def create_attention_mask_from_input_mask(from_tensor, to_mask)\n\t\n    from_shape = get_shape_list(from_tensor, expected_rank=[2, 3])\n    to_shape = get_shape_list(to_mask, expected_rank=[2, 3])\n    to_mask = tf.cast(tf.reshape(to_mask, [to_shape[0], to_shape[1]]), dtype=tf.float32)\n    attention_mask = (tf.ones(from_shape) * to_mask)\n    return attention_mask\n\n", "CodeBleu_score": 26.176494599453576}, {"code": "def create_attention_mask_from_input_mask(from_tensor, to_mask)\n\t\n    from_shape = get_shape_list(from_tensor, expected_rank=[2, 3])\n    batch_size = from_shape[0]\n    from_seq_length = from_shape[1]\n\n    to_shape = get_shape_list(to_mask, expected_rank=[2, 3])\n    to_seq_length = to_shape[1]\n\n    to_mask = tf.cast(\n        tf.reshape(to_mask, [batch_size, 1, to_seq_length]), tf.float32\n    )\n\n    # We don't assume that `from_tensor` is a mask (although it could be). We\n    # don't actually care if we attend *from* padding tokens (only *to* padding)\n    # tokens so we create a tensor of all ones.\n    #\n    # `broadcast_ones` = [batch_size, from_seq_length, 1]\n    broadcast_ones = tf.ones(\n        shape=[batch_size, from_seq_length, 1], dtype=tf.float32\n    )\n\n    # Here we broadcast along two dimensions to create the mask.\n    mask = broadcast_ones * to_mask\n\n    return mask\n\n", "CodeBleu_score": 57.87601399136759}, {"code": "def create_attention_mask_from_input_mask(from_tensor, to_mask)\n\t\n    from_shape = get_shape_list(from_tensor, expected_rank=[2, 3])\n    batch_size = from_shape[0]\n    from_seq_length = from_shape[1]\n\n    to_shape = get_shape_list(to_mask, expected_rank=[2, 3])\n    to_seq_length = to_shape[1]\n\n    to_mask = tf.cast(\n        tf.reshape(to_mask, [batch_size, 1, to_seq_length]), tf.float32\n    )\n\n    # We don't assume that `from_tensor` is a mask (although it could be). We\n    # don't actually care if we attend *from* padding tokens (only *to* padding)\n    # tokens so we create a tensor of all ones.\n    #\n    # `broadcast_ones` = [batch_size, from_seq_length, 1]\n    broadcast_ones = tf.ones(\n        shape=[batch_size, from_seq_length, 1], dtype=tf.float32\n    )\n\n    # Here we broadcast along two dimensions to create the mask.\n    mask = broadcast_ones * to_mask\n\n    return mask\n\n", "CodeBleu_score": 57.87601399136759}]}
{"instance_id": "d7798e41a723", "generation_result": [{"code": "def transpose_for_scores(input_tensor, batch_size, num_attention_heads,\n                           seq_length, width)\n\t\n    input_tensor = input_tensor.transpose(0, 2).reshape(\n        batch_size, num_attention_heads, seq_length, width)\n    return input_tensor.permute(0, 2, 1, 3)\n\n", "CodeBleu_score": 27.911285808132885}, {"code": "def transpose_for_scores(input_tensor, batch_size, num_attention_heads,\n                           seq_length, width)\n\t\n    input_tensor = input_tensor.reshape(batch_size, seq_length, num_attention_heads, width)\n    input_tensor = input_tensor.permute(0, 2, 1, 3)\n    return input_tensor\n\n", "CodeBleu_score": 31.505256971625855}, {"code": "def transpose_for_scores(input_tensor, batch_size, num_attention_heads,\n                           seq_length, width)\n\t\n    # input_tensor: [batch_size, seq_length, hidden_size]\n    # output_tensor: [batch_size, num_attention_heads, seq_length, width]\n    # input_tensor: [batch_size, seq_length, hidden_size]\n    # output_tensor: [batch_size, num_attention_heads, seq_length, width]\n    output_tensor = input_tensor.view(\n        batch_size, seq_length, num_attention_heads, width)\n    # output_tensor: [batch_size, seq_length, num_attention_heads, width]\n    output_tensor = output_tensor.transpose(1, 2)\n    # output_tensor: [batch_size, num_attention_heads, seq_length, width]\n    return output_tensor\n\n", "CodeBleu_score": 31.22959904819157}]}
{"instance_id": "711d466ace09", "generation_result": [{"code": "def reshape_to_matrix(input_tensor)\n\t\n    input_shape = input_tensor.get_shape().as_list()\n    if len(input_shape) < 2:\n        raise ValueError('Cannot reshape a tensor with rank {} into a matrix'.format(len(input_shape)))\n    elif len(input_shape) == 2:\n        return input_tensor\n    else:\n        width = input_shape[-1]\n        return tf.reshape(input_tensor, [-1, width])\n\n", "CodeBleu_score": 35.02971965913592}, {"code": "def reshape_to_matrix(input_tensor)\n\t\n    if input_tensor.dim() < 2:\n        raise ValueError(\"Input tensor must have rank at least 2\")\n    if input_tensor.dim() == 2:\n        return input_tensor\n    else:\n        width = input_tensor.size(input_tensor.dim() - 1)\n        return input_tensor.view(-1, width)\n\n", "CodeBleu_score": 40.12924484134604}, {"code": "def reshape_to_matrix(input_tensor)\n\t\n    input_shape = input_tensor.shape\n    input_rank = len(input_shape)\n    if input_rank < 2:\n        raise ValueError(\"Input tensor must have rank at least 2. Shape = \" + str(input_shape))\n    width = input_shape[-1]\n    if input_rank == 2:\n        return input_tensor\n    else:\n        return input_tensor[:, :width]\n\n", "CodeBleu_score": 34.63773764830938}]}
{"instance_id": "7adcd08f531e", "generation_result": [{"code": "def get_train_examples(self, data_dir)\n\t\n    examples = []\n    with open(os.path.join(data_dir, \"train.tsv\"), \"r\") as f:\n        reader = csv.reader(f, delimiter=\"\\t\", quotechar=None)\n        for line in reader:\n            guid = \"%s-%s\" % (line[0], line[1])\n            text_a = line[7]\n            text_b = line[8]\n            label = line[-1]\n            examples.append(\n                InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n    return examples\n\n", "CodeBleu_score": 33.44776805688194}, {"code": "def get_train_examples(self, data_dir)\n\t\n    examples = []\n    with open(os.path.join(data_dir, \"train.tsv\"), \"r\") as f:\n        reader = csv.reader(f, delimiter=\"\\t\")\n        for line in reader:\n            guid = line[0]\n            text_a = line[1]\n            label = line[2]\n            if label == \"contradictory\":\n                label = \"contradiction\"\n            examples.append(\n                InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n    return examples\n\n", "CodeBleu_score": 28.284893924466694}, {"code": "def get_train_examples(self, data_dir)\n\t\n    examples = []\n    for (i, line) in enumerate(csv.reader(open(os.path.join(data_dir, \"train.tsv\"), \"r\"), delimiter=\"\\t\")):\n        if i == 0:\n            continue\n        guid = \"%s-%s\" % (i, line[0])\n        text_a = line[1]\n        text_b = line[2]\n        label = line[3]\n        examples.append(\n            InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n    return examples\n\n", "CodeBleu_score": 41.825474304606104}]}
{"instance_id": "26a0bb96dab9", "generation_result": [{"code": "def metric_fn(per_example_loss, label_ids, logits, is_real_example)\n\t\n    logits = tf.argmax(logits, axis=-1)\n    logits = tf.reshape(logits, [-1])\n    label_ids = tf.reshape(label_ids, [-1])\n    label_mask = tf.cast(tf.not_equal(label_ids, modeling.PAD), tf.float32)\n    accuracy = tf.reduce_sum(tf.cast(tf.equal(logits, label_ids), tf.float32) * label_mask)\n    masked_accuracy = accuracy / tf.reduce_sum(label_mask)\n    loss = tf.reduce_sum(per_example_loss * label_mask)\n    masked_loss = loss / tf.reduce_sum(label_mask)\n    return {\n        \"eval_accuracy\": masked_accuracy,\n        \"eval_loss\": masked_loss,\n    }\n\n", "CodeBleu_score": 28.902883862579177}, {"code": "def metric_fn(per_example_loss, label_ids, logits, is_real_example)\n\t\n    logits = tf.argmax(logits, axis=-1, output_type=tf.int32)\n    accuracy = tf.reduce_mean(tf.cast(tf.equal(logits, label_ids), tf.float32))\n    loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=label_ids, logits=logits))\n    return {\n        \"eval_accuracy\": accuracy,\n        \"eval_loss\": loss,\n    }\n\n", "CodeBleu_score": 42.42080972568828}, {"code": "def metric_fn(per_example_loss, label_ids, logits, is_real_example)\n\t\n    logits = tf.cast(logits, tf.float32)\n    logits = tf.argmax(logits, axis=-1)\n    logits = tf.reshape(logits, [-1])\n    label_ids = tf.reshape(label_ids, [-1])\n    label_ids = tf.cast(label_ids, tf.int32)\n    is_real_example = tf.cast(is_real_example, dtype=tf.float32)\n    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=label_ids)\n    per_example_loss = tf.reshape(per_example_loss, [-1])\n    loss = tf.reduce_sum(per_example_loss)\n    loss = tf.reduce_mean(loss)\n    # Compute the accuracy for the current predictions.\n    correct_predictions = tf.equal(logits, label_ids)\n    correct_predictions = tf.cast(correct_predictions, tf.float32)\n    accuracy = tf.reduce_sum(tf.multiply(correct_predictions, is_real_example))\n    accuracy = accuracy / tf.reduce_sum(is_real_example)\n    return {\n        \"eval_accuracy\": accuracy,\n        \"eval_loss\": loss,\n    }\n\n", "CodeBleu_score": 37.069204847246496}]}
{"instance_id": "6154e4d28509", "generation_result": [{"code": "def convert_examples_to_features(examples, label_list, max_seq_length,\n                                 tokenizer)\n\t\n    features = []\n    for (ex_index, example) in enumerate(examples):\n        if ex_index % 10000 == 0:\n            print(\"Writing example %d of %d\" % (ex_index, len(examples)))\n        feature = convert_single_example(ex_index, example, label_list,\n                                         max_seq_length, tokenizer)\n        features.append(feature)\n    return features\n\n", "CodeBleu_score": 73.10553640588904}, {"code": "def convert_examples_to_features(examples, label_list, max_seq_length,\n                                 tokenizer)\n\t\n    features = []\n    for (ex_index, example) in enumerate(examples):\n        if ex_index % 10000 == 0:\n            print(\"Writing example %d of %d\" % (ex_index, len(examples)))\n\n        feature = convert_single_example(ex_index, example, label_list,\n                                         max_seq_length, tokenizer)\n\n        features.append(feature)\n    return features\n\n", "CodeBleu_score": 73.10553640588904}, {"code": "def convert_examples_to_features(examples, label_list, max_seq_length,\n                                 tokenizer)\n\t\n    features = []\n    for (ex_index, example) in enumerate(examples):\n        if ex_index % 10000 == 0:\n            print(\"Writing example %d of %d\" % (ex_index, len(examples)))\n        feature = convert_single_example(ex_index, example, label_list,\n                                         max_seq_length, tokenizer)\n        features.append(feature)\n    return features\n\n", "CodeBleu_score": 73.10553640588904}]}
{"instance_id": "e09c0db71ebb", "generation_result": [{"code": "def get_next_sentence_output(bert_config, input_tensor, labels)\n\t\n    bert_config = modeling.BertConfig.from_dict(bert_config.to_dict())\n    output_weights = modeling.get_next_sentence_output_weights_for_classifier(bert_config)\n    output_bias = modeling.get_next_sentence_output_bias_for_classifier(bert_config)\n    logits = tf.matmul(input_tensor, output_weights, transpose_b=True)\n    logits = tf.nn.bias_add(logits, output_bias)\n    log_probs = tf.nn.log_softmax(logits, axis=-1)\n    labels = tf.reshape(labels, [-1])\n    one_hot_labels = tf.one_hot(labels, depth=2, dtype=tf.float32)\n    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n    loss = tf.reduce_mean(per_example_loss)\n    return (loss, per_example_loss, log_probs)\n\n", "CodeBleu_score": 45.22021413536111}, {"code": "def get_next_sentence_output(bert_config, input_tensor, labels)\n\t\n\n    input_tensor = modeling.layer_norm(input_tensor)\n\n    # Scalar dimensions referenced here:\n    #   B = batch size (number of sequences)\n    #   F = `from_tensor` sequence length\n    #   T = `to_tensor` sequence length\n    #   N = `num_attention_heads`\n    #   H = `size_per_head`\n    # `query_layer`, `key_layer`, `value_layer` are the output of MLP\n    # `query_layer`, `key_layer`, `value_layer`, `attention_mask`, `attention_weights` are the output of attention\n    # `layer_output` is the output of the MLP\n    from_tensor_shape = modeling.get_shape_list(input_tensor, expected_rank=3)\n    batch_size = from_tensor_shape[0]\n    from_seq_length = from_tensor_shape[1]\n\n    to_tensor = modeling.layer_norm(labels)\n    to_tensor_shape = modeling.get_shape_list(to_tensor, expected_rank=3)\n    to_seq_length = to_tensor_shape[1]\n\n    with tf.variable_scope(\"cls/seq_relationship\"):\n        if bert_config.use_token_type:\n            token_type_table = tf.get_variable(\"token_type_table\",\n                                               [bert_config.type_vocab_size, bert_config.hidden_size],\n                                               initializer=modeling.create_initializer(bert_config.initializer_range))\n\n            token_type_embeddings = tf.nn.embedding_lookup(token_type_table, input_tensor)\n            token_type_embeddings = modeling.layer_norm(token_type_embeddings)\n            # This vocab will be small so we always do one-hot here, since it is always\n            # faster for a small vocabulary.\n            if bert_config.use_one_hot_embeddings:\n                token_type_one_hot_ids = tf.one", "CodeBleu_score": 27.89816848353352}, {"code": "def get_next_sentence_output(bert_config, input_tensor, labels)\n\t\n    # Slightly different from the TF version which uses matmul\n    input_tensor = modeling.layer_norm(input_tensor)\n    output_tensor = modeling.dropout(input_tensor, 1 - bert_config.hidden_dropout_prob)\n    logits = tf.matmul(output_tensor, bert_config.embedding_table, transpose_b=True)\n    logits = tf.nn.bias_add(logits, bert_config.embedding_bias)\n    log_probs = tf.nn.log_softmax(logits, axis=-1)\n    labels = tf.reshape(labels, [-1])\n    one_hot_labels = tf.one_hot(labels, depth=2, dtype=tf.float32)\n    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n    loss = tf.reduce_mean(per_example_loss)\n    return (loss, per_example_loss, log_probs)\n\n", "CodeBleu_score": 39.31168424149771}]}
{"instance_id": "1f66b020f485", "generation_result": [{"code": "def gather_indexes(sequence_tensor, positions)\n\t\n    batch_size = sequence_tensor.size(0)\n    seq_length = sequence_tensor.size(1)\n    offset_range = torch.arange(0, batch_size * seq_length, seq_length)\n    flat_offsets = positions + offset_range.type_as(positions)\n    flat_positions = flat_offsets.view(-1)\n    flat_sequence_tensor = sequence_tensor.view(-1, sequence_tensor.size(-1))\n    output_tensor = flat_sequence_tensor.gather(0, flat_positions)\n    return output_tensor\n\n", "CodeBleu_score": 30.85982074708124}, {"code": "def gather_indexes(sequence_tensor, positions)\n\t\n    batch_size, sequence_length, hidden_size = sequence_tensor.get_shape().as_list()\n    flat_offsets = tf.range(0, batch_size) * sequence_length\n    flat_positions = tf.reshape(positions + flat_offsets, [-1])\n    flat_sequence_tensor = tf.reshape(sequence_tensor, [-1, hidden_size])\n    output_tensor = tf.gather(flat_sequence_tensor, flat_positions)\n    return output_tensor\n\n", "CodeBleu_score": 27.25932959273277}, {"code": "def gather_indexes(sequence_tensor, positions)\n\t\n    sequence_shape = modeling.get_shape_list(sequence_tensor, expected_rank=3)\n    batch_size = sequence_shape[0]\n    seq_length = sequence_shape[1]\n    width = sequence_shape[2]\n\n    flat_offsets = modeling.get_shape_list(positions)[1]\n    flat_positions = modeling.reshape_to_matrix(positions + flat_offsets)\n    flat_sequence_tensor = modeling.reshape_to_matrix(sequence_tensor)\n    output_tensor = tf.gather(flat_sequence_tensor, flat_positions)\n    return modeling.reshape_from_matrix(output_tensor, [batch_size, seq_length, width])\n\n", "CodeBleu_score": 34.65984598131737}]}
{"instance_id": "956bffee9a33", "generation_result": [{"code": "def _decode_record(record, name_to_features)\n\t\n    example = tf.parse_single_example(record, name_to_features)\n\n    for name in list(example.keys()):\n        t = example[name]\n        if t.dtype == tf.int64:\n            t = tf.to_int32(t)\n        example[name] = t\n\n    return example\n\n", "CodeBleu_score": 71.91618952996937}, {"code": "def _decode_record(record, name_to_features)\n\t\n    example = tf.parse_single_example(record, name_to_features)\n    for name in list(example.keys()):\n        t = example[name]\n        if t.dtype == tf.int64:\n            t = tf.to_int32(t)\n        example[name] = t\n    return example\n\n", "CodeBleu_score": 71.91618952996937}, {"code": "def _decode_record(record, name_to_features)\n\t\n    example = tf.parse_single_example(record, name_to_features)\n    for name in list(example.keys()):\n        t = example[name]\n        if t.dtype == tf.int64:\n            t = tf.to_int32(t)\n        example[name] = t\n    return example\n\n", "CodeBleu_score": 71.91618952996937}]}
{"instance_id": "6b97f354e641", "generation_result": [{"code": "def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer,\n                         orig_answer_text)\n\t\n    # The SQuAD annotations are character based. We first project them to\n    # whitespace-tokenized words. But then after WordPiece tokenization, we can\n    # often find a \"better match\". For example:\n    #\n    #   Question: What year was John Smith born?\n    #   Context: The leader was John Smith (1895-1943).\n    #   Answer: 1895\n    #\n    # The original whitespace-tokenized answer will be \"(1895-1943).\". However\n    # after tokenization, our tokens will be \"( 1895 - 1943 ) .\". So we can match\n    # the exact answer, 1895.\n    #\n    # However, this is not always possible. Consider the following:\n    #\n    #   Question: What country is the top exporter of electornics?\n    #   Context: The Japanese electronics industry is the lagest in the world.\n    #   Answer: Japan\n    #\n    # In this case, the annotator chose \"Japan\" as a character sub-span of\n    # the word \"Japanese\". Since our WordPiece tokenizer does not split\n    # \"Japanese\", we just use \"Japanese\" as the annotation. This is fairly rare\n    # in SQuAD, but does happen.\n    tok_answer_text = \" \".join(tokenizer.tokenize(orig_answer_text))\n\n    for new_start in range(input_start, input_end + 1):\n        for new_end in range(input_end, new_start - 1, -1):\n            text_span = \" \".join(doc_tokens[new_start:(new_end + 1)])\n            if text_span == tok_answer_text:\n                return new_start, new_end\n\n    return input_start, input_end\n\n", "CodeBleu_score": 69.11440469388516}, {"code": "def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer,\n                         orig_answer_text)\n\t\n    # The SQuAD annotations are character based. We first project them to\n    # whitespace-tokenized words. But then after WordPiece tokenization, we can\n    # often find a \"better match\". For example:\n    #\n    #   Question: What year was John Smith born?\n    #   Context: The leader was John Smith (1895-1943).\n    #   Answer: 1895\n    #\n    # The original whitespace-tokenized answer will be \"(1895-1943).\". However\n    # after tokenization, our tokens will be \"( 1895 - 1943 ) .\". So we can match\n    # the exact answer, 1895.\n    #\n    # However, this is not always possible. Consider the following:\n    #\n    #   Question: What country is the top exporter of electornics?\n    #   Context: The Japanese electronics industry is the lagest in the world.\n    #   Answer: Japan\n    #\n    # In this case, the annotator chose \"Japan\" as a character sub-span of\n    # the word \"Japanese\". Since our WordPiece tokenizer does not split\n    # \"Japanese\", we just use \"Japanese\" as the annotation. This is fairly rare\n    # in SQuAD, but does happen.\n    tok_answer_text = \" \".join(tokenizer.tokenize(orig_answer_text))\n\n    for new_start in range(input_start, input_end + 1):\n        for new_end in range(input_end, new_start - 1, -1):\n            text_span = \" \".join(doc_tokens[new_start:(new_end + 1)])\n            if text_span == tok_answer_text:\n                return new_start, new_end\n\n    return input_start, input_end\n\n", "CodeBleu_score": 69.11440469388516}, {"code": "def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer,\n                         orig_answer_text)\n\t\n    # The SQuAD annotations are character based. We first project them to\n    # whitespace-tokenized words. But then after WordPiece tokenization, we can\n    # often find a \"better match\". For example:\n    #\n    #   Question: What year was John Smith born?\n    #   Context: The leader was John Smith (1895-1943).\n    #   Answer: 1895\n    #\n    # The original whitespace-tokenized answer will be \"(1895-1943).\". However\n    # after tokenization, our tokens will be \"( 1895 - 1943 ) .\". So we can match\n    # the exact answer, 1895.\n    #\n    # However, this is not always possible. Consider the following:\n    #\n    #   Question: What country is the top exporter of electornics?\n    #   Context: The Japanese electronics industry is the lagest in the world.\n    #   Answer: Japan\n    #\n    # In this case, the annotator chose \"Japan\" as a character sub-span of\n    # the word \"Japanese\". Since our WordPiece tokenizer does not split\n    # \"Japanese\", we just use \"Japanese\" as the annotation. This is fairly rare\n    # in SQuAD, but does happen.\n    tok_answer_text = \" \".join(tokenizer.tokenize(orig_answer_text))\n\n    for new_start in range(input_start, input_end + 1):\n        for new_end in range(input_end, new_start - 1, -1):\n            text_span = \" \".join(doc_tokens[new_start:(new_end + 1)])\n            if text_span == tok_answer_text:\n                return new_start, new_end\n\n    return input_start, input_end\n\n", "CodeBleu_score": 69.11440469388516}]}
{"instance_id": "aba334b92b34", "generation_result": [{"code": "def _check_is_max_context(doc_spans, cur_span_index, position)\n\t\n    best_score = None\n    best_span_index = None\n    for (span_index, doc_span) in enumerate(doc_spans):\n        end = doc_span[\"start\"] + doc_span[\"length\"] - 1\n        if position < doc_span[\"start\"]:\n            continue\n        if position > end:\n            continue\n        num_left_context = position - doc_span[\"start\"]\n        num_right_context = end - position\n        score = min(num_left_context, num_right_context) + 0.01 * doc_span[\"length\"]\n        if best_score is None or score > best_score:\n            best_score = score\n            best_span_index = span_index\n\n    return cur_span_index == best_span_index\n\n", "CodeBleu_score": 66.38359336403245}, {"code": "def _check_is_max_context(doc_spans, cur_span_index, position)\n\t\n    best_score = None\n    best_span_index = None\n    for (span_index, doc_span) in enumerate(doc_spans):\n        end = doc_span[\"start\"] + doc_span[\"length\"] - 1\n        if position < doc_span[\"start\"]:\n            continue\n        if position > end:\n            continue\n        num_left_context = position - doc_span[\"start\"]\n        num_right_context = end - position\n        score = min(num_left_context, num_right_context) + 0.01 * doc_span[\"length\"]\n        if best_score is None or score > best_score:\n            best_score = score\n            best_span_index = span_index\n    return cur_span_index == best_span_index\n\n", "CodeBleu_score": 66.38359336403245}, {"code": "def _check_is_max_context(doc_spans, cur_span_index, position)\n\t\n    best_score = None\n    best_span_index = None\n    for (span_index, doc_span) in enumerate(doc_spans):\n        end = doc_span[\"start\"] + doc_span[\"length\"] - 1\n        if position < doc_span[\"start\"]:\n            continue\n        if position > end:\n            continue\n        num_left_context = position - doc_span[\"start\"]\n        num_right_context = end - position\n        score = min(num_left_context, num_right_context) + 0.01 * doc_span[\"length\"]\n        if best_score is None or score > best_score:\n            best_score = score\n            best_span_index = span_index\n\n    return cur_span_index == best_span_index\n\n", "CodeBleu_score": 66.38359336403245}]}
{"instance_id": "cf92ad509f6a", "generation_result": [{"code": "def load_vocab(vocab_file)\n\t\n    vocab = collections.OrderedDict()\n    index = 0\n    with open(vocab_file, \"r\") as f:\n        for line in f:\n            token = line.strip()\n            vocab[token] = index\n            index += 1\n    return vocab\n\n", "CodeBleu_score": 43.55277640598216}, {"code": "def load_vocab(vocab_file)\n\t\n    vocab = collections.OrderedDict()\n    index = 0\n    with open(vocab_file, 'r') as f:\n        for line in f:\n            token = line.strip()\n            vocab[token] = index\n            index += 1\n    return vocab\n\n", "CodeBleu_score": 42.925448351621014}, {"code": "def load_vocab(vocab_file)\n\t\n    vocab = collections.OrderedDict()\n    index = 0\n    with open(vocab_file, \"r\", encoding=\"utf-8\") as reader:\n        while True:\n            token = convert_to_unicode(reader.readline())\n            if not token:\n                break\n            token = token.strip()\n            vocab[token] = index\n            index += 1\n    return vocab\n\n", "CodeBleu_score": 67.2956448618734}]}
{"instance_id": "6c865803ac12", "generation_result": [{"code": "def convert_by_vocab(vocab, items)\n\t\n    output = []\n    for item in items:\n        output.append(vocab[item])\n    return output\n\n", "CodeBleu_score": 79.63749907165396}, {"code": "def convert_by_vocab(vocab, items)\n\t\n    output = []\n    for item in items:\n        output.append(vocab[item])\n    return output\n\n", "CodeBleu_score": 79.63749907165396}, {"code": "def convert_by_vocab(vocab, items)\n\t\n    output = []\n    for item in items:\n        output.append(vocab[item])\n    return output\n\n", "CodeBleu_score": 79.63749907165396}]}
{"instance_id": "e93b9200dc4a", "generation_result": [{"code": "def tokenize(self, text)\n\t\n    text = convert_to_unicode(text)\n\n    # Basic tokenizer\n    # -------------------------------------------------------------------------\n    # Split the text into tokens using the basic tokenizer.\n    #\n    # The basic tokenizer divides the text into tokens using regular expressions.\n    #\n    # The regular expressions used are as follows:\n    #\n    # - Split on whitespace: the regular expression r\"\\s+\".\n    # - Split on punctuation: the regular expression r\"([.,!?])\".\n    # - Split on quotation: the regular expression r'\"'.\n    # - Split on bracket: the regular expression r\"([()])\".\n    # - Split on colon: the regular expression r\"([:])\".\n    # - Split on semicolon: the regular expression r\"([;])\".\n    # - Split on dash: the regular expression r\"([-])\".\n    # - Split on slash: the regular expression r\"([/])\".\n    # - Split on ampersand: the regular expression r\"([&])\".\n    # - Split on equals: the regular expression r\"([=])\".\n    # - Split on plus: the regular expression r\"([+])\".\n    # - Split on open bracket: the regular expression r\"([[({])\".\n    # - Split on close bracket: the regular expression r\"([])}])\".\n    # - Split on open angle bracket: the regular expression r\"([<])\".\n    # - Split on close angle bracket: the regular expression r\"([>])\".\n    # - Split on at sign: the regular expression r\"([@])\".\n    # - Split on dollar sign: the regular expression r\"([$])\".\n    # - Split on percent sign: the regular expression r\"([%])\".\n    # - Split on caret: the regular expression r\"([^])\".\n    # - Split on backslash: the regular expression r\"([\\])\".\n    # - Split on forward slash: the regular expression r\"([/])\".\n    # - Split on tilde: the regular expression r\"([~])\".\n    # - Split on underscore: the regular expression r\"([_])\".\n    # - Split on apostrophe: the regular expression r", "CodeBleu_score": 2.439771165166261}, {"code": "def tokenize(self, text)\n\t\n    split_tokens = []\n    for token in self.basic_tokenizer.tokenize(text):\n        for sub_token in self.wordpiece_tokenizer.tokenize(token):\n            split_tokens.append(sub_token)\n    return split_tokens\n\n", "CodeBleu_score": 88.10762355418231}, {"code": "def tokenize(self, text)\n\t\n    split_tokens = []\n    for token in self.basic_tokenizer.tokenize(text):\n        for sub_token in self.wordpiece_tokenizer.tokenize(token):\n            split_tokens.append(sub_token)\n    return split_tokens\n\n", "CodeBleu_score": 88.10762355418231}]}
{"instance_id": "2ff6f6db53c9", "generation_result": [{"code": "def safe_load(path, offset, duration, sample_rate, dtype)\n\t\n    try:\n        from librosa.core import load as librosa_load\n    except ModuleNotFoundError:\n        raise ModuleNotFoundError(\n            \"Could not import librosa. Please install librosa with `pip install librosa`\"\n        )\n    try:\n        from mirdata.datasets import load_audio\n    except ModuleNotFoundError:\n        raise ModuleNotFoundError(\n            \"Could not import mirdata. Please install mirdata with `pip install mirdata`\"\n        )\n    try:\n        from mirdata.utils import load_audio_to_torch\n    except ModuleNotFoundError:\n        raise ModuleNotFoundError(\n            \"Could not import mirdata.utils. Please install mirdata with `pip install mirdata`\"\n        )\n    try:\n        from mirdata.datasets.utils import load_audio_to_torch\n    except ModuleNotFoundError:\n        raise ModuleNotFoundError(\n            \"Could not import mirdata.datasets.utils. Please install mirdata with `pip install mirdata`\"\n        )\n    try:\n        from mirdata.datasets.utils import load_audio_to_torch\n    except ModuleNotFoundError:\n        raise ModuleNotFoundError(\n            \"Could not import mirdata.datasets.utils. Please install mirdata with `pip install mirdata`\"\n        )\n    try:\n        from mirdata.datasets.utils import load_audio_to_torch\n    except ModuleNotFoundError:\n        raise ModuleNotFoundError(\n            \"Could not import mirdata.datasets.utils. Please install mirdata with `pip install mirdata`\"\n        )\n    try:\n        from mirdata.datasets.utils import load_audio_to_torch\n    except ModuleNotFoundError:\n        raise ModuleNotFoundError(\n            \"Could not import mirdata.datasets.utils. Please install mirdata with `pip install mirdata`\"\n        )\n    try:\n        from mirdata.datasets.utils import load_audio_to_torch\n    except ModuleNotFoundError:\n        raise ModuleNotFoundError(\n            \"Could not import mirdata.datasets.utils. Please install mirdata with `pip install mirdata`\"\n        )\n    try:\n        from", "CodeBleu_score": 11.823637847375531}, {"code": "def safe_load(path, offset, duration, sample_rate, dtype)\n\t\n    try:\n        from spleeter.audio.adapter import get_audio_adapter\n        from spleeter.audio.adapter import AudioAdapter\n        from spleeter.audio.exceptions import AudioAdapterError\n        from spleeter.audio.exceptions import AudioAdapterRuntimeError\n        from spleeter.audio.exceptions import AudioAdapterTypeError\n        from spleeter.audio.exceptions import AudioAdapterValueError\n\n        adapter: AudioAdapter = get_audio_adapter(path, offset, duration, sample_rate, dtype)\n        audio_data: np.ndarray = adapter.load_audio()\n        return audio_data, False\n    except (AudioAdapterError, AudioAdapterRuntimeError, AudioAdapterTypeError, AudioAdapterValueError) as e:\n        print(f\"[ERROR] Failed to load audio data from {path} at {offset} with {duration} seconds duration and {sample_rate} sample rate. Reason: {e}\")\n        return -1.0, True\n\n", "CodeBleu_score": 32.0925171912676}, {"code": "def safe_load(path, offset, duration, sample_rate, dtype)\n\t\n    try:\n        import soundfile\n        data, sr = soundfile.read(path, start=offset, stop=offset+duration, dtype=dtype)\n        if sr != sample_rate:\n            raise RuntimeError(f\"Sample rate mismatch: {sr} != {sample_rate}\")\n        return data, False\n    except Exception as e:\n        print(f\"[ERROR] Failed to load audio data from {path} with offset {offset} and duration {duration} with error: {e}\")\n        return -1.0, True\n\n", "CodeBleu_score": 34.60780880651081}]}
{"instance_id": "12fbe1f03d8f", "generation_result": [{"code": "def to_n_channels(waveform: tf.Tensor, n_channels: int) -> tf.Tensor\n\t\n    if waveform.shape[-1] >= n_channels:\n        return waveform[:, :n_channels]\n    else:\n        return tf.tile(waveform, [1, n_channels])[:, :n_channels]\n\n", "CodeBleu_score": 43.109863211477325}, {"code": "def to_n_channels(waveform: tf.Tensor, n_channels: int) -> tf.Tensor\n\t\n    if n_channels == waveform.shape[1]:\n        return waveform\n    elif n_channels > waveform.shape[1]:\n        return tf.tile(waveform, [1, n_channels // waveform.shape[1], 1])[:, :n_channels, :]\n    else:\n        return waveform[:, :n_channels, :]\n\n", "CodeBleu_score": 47.09323052887244}, {"code": "def to_n_channels(waveform: tf.Tensor, n_channels: int) -> tf.Tensor\n\t\n    if waveform.shape[1] >= n_channels:\n        return waveform[:, :n_channels]\n    else:\n        return tf.tile(waveform, [1, n_channels])[:, :n_channels]\n\n", "CodeBleu_score": 43.17950792579106}]}
{"instance_id": "5e215f029b2e", "generation_result": [{"code": "def spectrogram_to_db_uint(\n    spectrogram: tf.Tensor, db_range: float = 100.0, **kwargs\n) -> tf.Tensor\n\t\n    dB_spectrogram = gain_to_db(spectrogram, **kwargs)\n    dB_spectrogram_max = tf.reduce_max(dB_spectrogram)\n    dB_spectrogram_clipped = tf.clip_by_value(\n        dB_spectrogram, -db_range, dB_spectrogram_max\n    )\n    dB_spectrogram_uint = tf.cast(\n        dB_spectrogram_clipped * 255 / db_range, tf.uint8\n    )\n    return dB_spectrogram_uint\n\n", "CodeBleu_score": 42.5250962069987}, {"code": "def spectrogram_to_db_uint(\n    spectrogram: tf.Tensor, db_range: float = 100.0, **kwargs\n) -> tf.Tensor\n\t\n    dB_spectrogram = tf.math.log(spectrogram)\n    dB_spectrogram = tf.math.divide(dB_spectrogram, tf.math.log(10.0))\n    dB_spectrogram = tf.math.multiply(dB_spectrogram, db_range)\n    dB_spectrogram = tf.math.add(dB_spectrogram, 127.0)\n    dB_spectrogram = tf.math.round(dB_spectrogram)\n    dB_spectrogram = tf.clip_by_value(dB_spectrogram, 0, 255)\n    dB_spectrogram = tf.cast(dB_spectrogram, tf.uint8)\n    return dB_spectrogram\n\n", "CodeBleu_score": 36.210982226807836}, {"code": "def spectrogram_to_db_uint(\n    spectrogram: tf.Tensor, db_range: float = 100.0, **kwargs\n) -> tf.Tensor\n\t\n    spectrogram_db = gain_to_db(spectrogram, **kwargs)\n    spectrogram_db = tf.clip_by_value(spectrogram_db, -db_range, 0.0)\n    spectrogram_db = tf.cast(spectrogram_db, tf.uint8)\n    return spectrogram_db\n\n", "CodeBleu_score": 36.26498304840796}]}
{"instance_id": "b65c0968dae7", "generation_result": [{"code": "def compute_spectrogram_tf(\n    waveform: tf.Tensor,\n    frame_length: int = 2048,\n    frame_step: int = 512,\n    spec_exponent: float = 1.0,\n    window_exponent: float = 1.0,\n) -> tf.Tensor\n\t\n    window_length = frame_length * window_exponent\n    window = hann_window(window_length)\n    window = window ** window_exponent\n    window = tf.cast(window, waveform.dtype)\n    stft_matrix = stft(\n        waveform,\n        frame_length=frame_length,\n        frame_step=frame_step,\n        fft_length=frame_length,\n        window_fn=window,\n    )\n    stft_matrix = tf.abs(stft_matrix)\n    stft_matrix = tf.transpose(stft_matrix, perm=[0, 2, 1])\n    stft_matrix = stft_matrix ** spec_exponent\n    return stft_matrix\n\n", "CodeBleu_score": 36.35481911599586}, {"code": "def compute_spectrogram_tf(\n    waveform: tf.Tensor,\n    frame_length: int = 2048,\n    frame_step: int = 512,\n    spec_exponent: float = 1.0,\n    window_exponent: float = 1.0,\n) -> tf.Tensor\n\t\n\n    # Apply Hann window function and raise to window_exponent\n    window = hann_window(frame_length, dtype=waveform.dtype)\n    window = tf.pow(window, window_exponent)\n\n    # Compute the spectrogram\n    spectrogram = stft(\n        waveform,\n        frame_length=frame_length,\n        frame_step=frame_step,\n        fft_length=frame_length,\n        window_fn=window,\n    )\n\n    # Transpose and raise to spec_exponent\n    spectrogram = tf.transpose(spectrogram, perm=[0, 2, 1])\n    spectrogram = tf.pow(tf.abs(spectrogram), spec_exponent)\n\n    return spectrogram\n\n", "CodeBleu_score": 40.43367495369545}, {"code": "def compute_spectrogram_tf(\n    waveform: tf.Tensor,\n    frame_length: int = 2048,\n    frame_step: int = 512,\n    spec_exponent: float = 1.0,\n    window_exponent: float = 1.0,\n) -> tf.Tensor\n\t\n    window = hann_window(frame_length, dtype=waveform.dtype)\n    window = tf.cast(window, waveform.dtype)\n    window = tf.math.pow(window, window_exponent)\n    spec = stft(waveform, frame_length=frame_length, frame_step=frame_step, fft_length=frame_length, window_fn=window)\n    spec = tf.math.abs(spec)\n    spec = tf.math.pow(spec, spec_exponent)\n    spec = tf.transpose(spec, perm=[0, 2, 1])\n    return spec\n\n", "CodeBleu_score": 38.60406274867325}]}
{"instance_id": "a4686140a634", "generation_result": [{"code": "def time_stretch(\n    spectrogram: tf.Tensor,\n    factor: float = 1.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n\n    # Calculate the new time dimension\n    new_time_dim = tf.cast(tf.math.round(spectrogram.shape[1] * factor), tf.int32)\n\n    # Resize the spectrogram using the specified interpolation method\n    resized_spectrogram = tf.image.resize(\n        spectrogram, [spectrogram.shape[0], new_time_dim], method=method\n    )\n\n    # Adjust the spectrogram to the original dimensions by cropping or padding\n    if new_time_dim > spectrogram.shape[1]:\n        # Crop the resized spectrogram to the original dimensions\n        resized_spectrogram = resized_spectrogram[:, : spectrogram.shape[1]]\n    else:\n        # Pad the resized spectrogram to the original dimensions\n        resized_spectrogram = tf.image.pad_to_bounding_box(\n            resized_spectrogram,\n            0,\n            0,\n            spectrogram.shape[0],\n            spectrogram.shape[1],\n            constant_values=0.0,\n        )\n\n    return resized_spectrogram\n\n", "CodeBleu_score": 34.123827442950486}, {"code": "def time_stretch(\n    spectrogram: tf.Tensor,\n    factor: float = 1.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n    if factor == 1.0:\n        return spectrogram\n    if method == tf.image.ResizeMethod.NEAREST_NEIGHBOR:\n        raise ValueError(\n            \"The method NEAREST_NEIGHBOR is not supported for time-stretching.\"\n        )\n    time_dim = spectrogram.shape[1]\n    new_time_dim = tf.cast(time_dim * factor, tf.int32)\n    resized_spectrogram = tf.image.resize(\n        spectrogram,\n        [spectrogram.shape[0], new_time_dim],\n        method=method,\n    )\n    if factor > 1.0:\n        resized_spectrogram = tf.image.crop_to_bounding_box(\n            resized_spectrogram,\n            0,\n            0,\n            spectrogram.shape[0],\n            time_dim,\n        )\n    else:\n        resized_spectrogram = tf.image.pad_to_bounding_box(\n            resized_spectrogram,\n            0,\n            0,\n            spectrogram.shape[0],\n            time_dim,\n        )\n    return resized_spectrogram\n\n", "CodeBleu_score": 36.892732628777715}, {"code": "def time_stretch(\n    spectrogram: tf.Tensor,\n    factor: float = 1.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n\n    if factor == 1.0:\n        return spectrogram\n\n    # Calculate new time dimension\n    time_dim = tf.cast(tf.math.rint(tf.cast(tf.shape(spectrogram)[1], tf.float32) * factor), tf.int32)\n\n    # Resize spectrogram\n    spectrogram = tf.image.resize(\n        spectrogram, [tf.shape(spectrogram)[0], time_dim], method=method, antialias=True\n    )\n\n    # Adjust the spectrogram to the original dimensions\n    if time_dim < tf.shape(spectrogram)[1]:\n        spectrogram = spectrogram[:, :time_dim, :]\n    else:\n        spectrogram = tf.pad(\n            spectrogram,\n            [[0, 0], [0, time_dim - tf.shape(spectrogram)[1]], [0, 0]],\n            constant_values=0.0,\n        )\n\n    return spectrogram\n\n", "CodeBleu_score": 46.208628693163625}]}
{"instance_id": "6dd01101d4ae", "generation_result": [{"code": "def pitch_shift(\n    spectrogram: tf.Tensor,\n    semitone_shift: float = 0.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n    # Calculate the resizing factor\n    resizing_factor = 2 ** semitone_shift\n\n    # Resize the spectrogram\n    resized_spectrogram = tf.image.resize(\n        spectrogram,\n        tf.shape(spectrogram)[:2],\n        method=method,\n        antialias=True,\n        preserve_aspect_ratio=True,\n        antialias_depth=1,\n    )\n\n    # Pad the resized spectrogram to maintain the original dimensions\n    resized_spectrogram = tf.pad(\n        resized_spectrogram,\n        [[0, 0], [0, 0], [0, 0], [0, tf.math.floordiv(tf.shape(spectrogram)[-1], 2)]],\n        mode=\"CONSTANT\",\n    )\n\n    # Calculate the new frequency axis\n    new_frequency_axis = tf.cast(\n        tf.linspace(\n            tf.cast(tf.shape(spectrogram)[-1] / 2, tf.float32),\n            tf.cast(tf.shape(resized_spectrogram)[-1] / 2, tf.float32),\n            tf.shape(resized_spectrogram)[-1],\n        ),\n        tf.int32,\n    )\n\n    # Reshape the spectrogram\n    resized_spectrogram = tf.reshape(\n        resized_spectrogram,\n        [\n            tf.shape(spectrogram)[0],\n            tf.shape(spectrogram)[1],\n            tf.shape(resized_spectrogram)[-1],\n        ],\n    )\n\n    # Reshape the frequency axis\n    new_frequency_axis = tf.reshape(new_frequency_axis, [1, -1, 1])\n\n    # Calculate the spectrogram's new frequency axis\n    new_frequency_axis = tf.cast(\n        tf.linspace(\n            tf.cast(tf.shape(spectrogram)[-1] / 2, tf.float32),\n            tf.cast(tf.shape", "CodeBleu_score": 26.519856727531945}, {"code": "def pitch_shift(\n    spectrogram: tf.Tensor,\n    semitone_shift: float = 0.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n    # Calculate the resizing factor\n    resizing_factor = 2 ** semitone_shift\n\n    # Resize the spectrogram\n    resized_spectrogram = tf.image.resize(\n        spectrogram,\n        tf.cast(tf.shape(spectrogram)[:2], tf.int32),\n        method=method,\n        antialias=True,\n    )\n\n    # Pad the resized spectrogram\n    resized_spectrogram = tf.pad(\n        resized_spectrogram,\n        [[0, 0], [0, 0], [0, tf.math.floordiv(tf.shape(spectrogram)[2], 2)]],\n        mode=\"CONSTANT\",\n    )\n\n    # Resize the spectrogram along the frequency axis\n    resized_spectrogram = tf.image.resize(\n        resized_spectrogram,\n        tf.cast(tf.shape(spectrogram)[:2], tf.int32),\n        method=method,\n        antialias=True,\n    )\n\n    # Calculate the number of padding values\n    num_padding_values = tf.math.floordiv(tf.shape(spectrogram)[2], 2)\n\n    # Pad the resized spectrogram\n    resized_spectrogram = tf.pad(\n        resized_spectrogram,\n        [[0, 0], [0, 0], [0, num_padding_values]],\n        mode=\"CONSTANT\",\n    )\n\n    # Calculate the spectrogram of the resized spectrogram\n    resized_spectrogram_stft = tf.abs(stft(resized_spectrogram, 1024, 256))\n\n    # Calculate the spectrogram of the spectrogram\n    spectrogram_stft = tf.abs(stft(spectrogram, 1024, 256))\n\n    # Calculate the spectrogram of the resized spectrogram\n    resized_spectrogram_stft = tf.abs(stft(resized_spectrogram, 102", "CodeBleu_score": 29.037052977027123}, {"code": "def pitch_shift(\n    spectrogram: tf.Tensor,\n    semitone_shift: float = 0.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n    # calculate the resizing factor\n    resizing_factor = tf.math.pow(2.0, semitone_shift / 12.0)\n    # calculate the resized spectrogram\n    resized_spectrogram = tf.image.resize(\n        spectrogram,\n        (tf.shape(spectrogram)[0], tf.shape(spectrogram)[1] * resizing_factor),\n        method=method,\n    )\n    # calculate the number of padding elements\n    padding_elements = tf.math.floormod(\n        tf.shape(spectrogram)[1] * resizing_factor, 2\n    )\n    # calculate the padding\n    padding = tf.constant([[0, 0], [0, padding_elements], [0, 0]])\n    # pad the resized spectrogram\n    padded_resized_spectrogram = tf.pad(resized_spectrogram, padding)\n    # return the pitch-shifted spectrogram\n    return padded_resized_spectrogram\n\n", "CodeBleu_score": 30.652577509171287}]}
{"instance_id": "612f0826e56a", "generation_result": [{"code": "def _get_conv_activation_layer(params: Dict) -> Any\n\t\n    if params[\"conv_activation\"] == \"relu\":\n        return ReLU()\n    elif params[\"conv_activation\"] == \"elu\":\n        return ELU()\n    else:\n        return LeakyReLU(alpha=0.2)\n\n", "CodeBleu_score": 37.81568218186228}, {"code": "def _get_conv_activation_layer(params: Dict) -> Any\n\t\n    activation = params[\"conv_activation\"]\n    if activation == \"relu\":\n        return ReLU()\n    elif activation == \"elu\":\n        return ELU()\n    elif activation == \"leakyrelu\":\n        return LeakyReLU(alpha=0.2)\n    else:\n        raise ValueError(f\"Activation function {activation} is not supported.\")\n\n", "CodeBleu_score": 35.45034281278795}, {"code": "def _get_conv_activation_layer(params: Dict) -> Any\n\t\n    conv_activation = params.get(\"conv_activation\", \"relu\")\n    if conv_activation == \"relu\":\n        return ReLU()\n    elif conv_activation == \"elu\":\n        return ELU()\n    elif conv_activation == \"leaky_relu\":\n        return LeakyReLU(alpha=0.2)\n    else:\n        raise ValueError(\n            f\"The value of the 'conv_activation' parameter should be one of 'relu', 'elu', or 'leaky_relu', but {conv_activation} was provided.\"\n        )\n\n", "CodeBleu_score": 40.41838353081629}]}
{"instance_id": "2f8b5a2fc0f2", "generation_result": [{"code": "def softmax_unet(\n    input_tensor: tf.Tensor, instruments: Iterable[str], params: Dict = {}\n) -> Dict\n\t\n    logging.info(\"Building softmax U-Net model...\")\n    logging.info(\"Building softmax U-Net model...\")\n    logging.info(\"Building softmax U-Net model...\")\n    logging.info(\"Building softmax U-Net model...\")\n    logging.info(\"Building softmax U-Net model...\")\n    logging.info(\"Building softmax U-Net model...\")\n    logging.info(\"Building softmax U-Net model...\")\n    logging.info(\"Building softmax U-Net model...\")\n    logging.info(\"Building softmax U-Net model...\")\n    logging.info(\"Building softmax U-Net model...\")\n    logging.info(\"Building softmax U-Net model...\")\n    logging.info(\"Building softmax U-Net model...\")\n    logging.info(\"Building softmax U-Net model...\")\n    logging.info(\"Building softmax U-Net model...\")\n    logging.info(\"Building softmax U-Net model...\")\n    logging.info(\"Building softmax U-Net model...\")\n    logging.info(\"Building softmax U-Net model...\")\n    logging.info(\"Building softmax U-Net model...\")\n    logging.info(\"Building softmax U-Net model...\")\n    logging.info(\"Building softmax U-Net model...\")\n    logging.info(\"Building softmax U-Net model...\")\n    logging.info(\"Building softmax U-Net model...\")\n    logging.info(\"Building softmax U-Net model...\")\n    logging.info(\"Building softmax U-Net model...\")\n    logging.info(\"Building softmax U-Net model...\")\n    logging.info(\"Building softmax U-Net model...\")\n    logging.info(\"Building softmax U-Net model...\")\n    logging.info(\"Building softmax U-Net model...\")\n    logging.info(\"Building softmax U-Net model...\")\n    logging.info(\"Building softmax U-Net model...\")\n    logging.info(\"Building softmax U-Net model...\")\n    logging.info(\"Building softmax U-Net model...\")", "CodeBleu_score": 11.553250524131565}, {"code": "def softmax_unet(\n    input_tensor: tf.Tensor, instruments: Iterable[str], params: Dict = {}\n) -> Dict\n\t\n    # TODO: add docstring\n    # TODO: add docstring\n    # TODO: add docstring\n    # TODO: add docstring\n    # TODO: add docstring\n    # TODO: add docstring\n    # TODO: add docstring\n    # TODO: add docstring\n    # TODO: add docstring\n    # TODO: add docstring\n    # TODO: add docstring\n    # TODO: add docstring\n    # TODO: add docstring\n    # TODO: add docstring\n    # TODO: add docstring\n    # TODO: add docstring\n    # TODO: add docstring\n    # TODO: add docstring\n    # TODO: add docstring\n    # TODO: add docstring\n    # TODO: add docstring\n    # TODO: add docstring\n    # TODO: add docstring\n    # TODO: add docstring\n    # TODO: add docstring\n    # TODO: add docstring\n    # TODO: add docstring\n    # TODO: add docstring\n    # TODO: add docstring\n    # TODO: add docstring\n    # TODO: add docstring\n    # TODO: add docstring\n    # TODO: add docstring\n    # TODO: add docstring\n    # TODO: add docstring\n    # TODO: add docstring\n    # TODO: add docstring\n    # TODO: add docstring\n    # TODO: add docstring\n    # TODO: add docstring\n    # TODO: add docstring\n    # TODO: add docstring\n    # TODO: add docstring\n    # TODO: add docstring\n    # TODO: add docstring\n    # TODO: add docstring\n    # TODO: add docstring\n    # TODO: add docstring\n    # TODO: add docstring\n    # TODO: add docstring\n    # TODO: add docstring\n    # TODO: add docstring\n    # TODO: add docstring\n    # TODO: add docstring\n    # TODO: add docstring\n    # TODO: add docstring\n    # TODO: add docstring\n    # TODO: add docstring\n    # TODO: add docstring\n    # TODO: add docstring\n    # TODO: add docstring\n    # TODO: add docstring\n    # TODO: add docstring\n    # TODO: add docstring", "CodeBleu_score": 8.96877269426289}, {"code": "def softmax_unet(\n    input_tensor: tf.Tensor, instruments: Iterable[str], params: Dict = {}\n) -> Dict\n\t\n    logging.info(\"Initializing U-Net model...\")\n    params = {\n        \"num_layers\": 3,\n        \"num_filters\": 64,\n        \"kernel_size\": 3,\n        \"activation\": \"relu\",\n        \"dropout\": 0.0,\n        \"batch_norm\": False,\n        \"initializer\": he_uniform(),\n        \"axis\": -1,\n        **params,\n    }\n\n    logging.info(\"Building U-Net model...\")\n    model = partial(\n        _unet_block,\n        num_layers=params[\"num_layers\"],\n        num_filters=params[\"num_filters\"],\n        kernel_size=params[\"kernel_size\"],\n        activation=params[\"activation\"],\n        dropout=params[\"dropout\"],\n        batch_norm=params[\"batch_norm\"],\n        initializer=params[\"initializer\"],\n    )\n\n    logging.info(\"Applying U-Net model to input tensor...\")\n    logit_masks = {\n        instrument: model(input_tensor, name=instrument)\n        for instrument in instruments\n    }\n\n    logging.info(\"Combining logit masks with softmax function...\")\n    logit_masks = {\n        instrument: Softmax(axis=params[\"axis\"])(logit_masks[instrument])\n        for instrument in instruments\n    }\n\n    logging.info(\"Multiplying logit masks with input tensor...\")\n    spectrograms = {\n        instrument: Multiply(name=f\"{instrument}_spectrogram\")(\n            [input_tensor, logit_masks[instrument]]\n        )\n        for instrument in instruments\n    }\n\n    logging.info(\"U-Net model complete.\")\n    return spectrograms\n\n", "CodeBleu_score": 23.52288299731909}]}
{"instance_id": "400b19ce8520", "generation_result": [{"code": "def compute_file_checksum(path)\n\t\n    with open(path, \"rb\") as f:\n        hash_sha256 = hashlib.sha256()\n        while True:\n            data = f.read(65536)\n            if not data:\n                break\n            hash_sha256.update(data)\n        return hash_sha256.hexdigest()\n\n", "CodeBleu_score": 40.17753787012397}, {"code": "def compute_file_checksum(path)\n\t\n    sha256_hash = hashlib.sha256()\n    with open(path, \"rb\") as f:\n        for chunk in iter(lambda: f.read(4096), b\"\"):\n            sha256_hash.update(chunk)\n    return sha256_hash.hexdigest()\n\n", "CodeBleu_score": 57.44570069380956}, {"code": "def compute_file_checksum(path)\n\t\n    sha256_hash = hashlib.sha256()\n    with open(path, \"rb\") as f:\n        for chunk in iter(lambda: f.read(4096), b\"\"):\n            sha256_hash.update(chunk)\n    return sha256_hash.hexdigest()\n\n", "CodeBleu_score": 57.44570069380956}]}
{"instance_id": "5f2e9c6e15f1", "generation_result": [{"code": "def checksum(self, name: str) -> str\n\t\n    if self.host is None:\n        raise ValueError(\"Host not specified.\")\n    if self.repository is None:\n        raise ValueError(\"Repository not specified.\")\n    if self.release_path is None:\n        raise ValueError(\"Release path not specified.\")\n    if self.checksum_index is None:\n        raise ValueError(\"Checksum index not specified.\")\n    url = f\"{self.host}/{self.repository}/{self.release_path}/{self.checksum_index}\"\n    response = httpx.get(url)\n    if response.status_code != 200:\n        raise ValueError(f\"Checksum index not found at {url}.\")\n    response_json = response.json()\n    if name not in response_json:\n        raise ValueError(f\"Model {name} not found in checksum index.\")\n    return response_json[name]\n\n", "CodeBleu_score": 38.40579695297398}, {"code": "def checksum(self, name: str) -> str\n\t\n    if name not in self.index:\n        raise ValueError(\n            f\"{name} is not a valid model name. Please check the model name and try again.\"\n        )\n    url = f\"{self.host}/{self.repository}/{self.release_path}/{self.index[name]['checksum_index']}\"\n    response = httpx.get(url)\n    if response.status_code != 200:\n        raise RuntimeError(\n            f\"Could not retrieve checksum for {name}. Please check the model name and try again.\"\n        )\n    checksum_index = response.json()\n    return checksum_index[name]\n\n", "CodeBleu_score": 39.14737084591374}, {"code": "def checksum(self, name: str) -> str\n\t\n    url = f'{self.host}/{self.repository}/{self.release_path}/{self.checksum_index}'\n    response = httpx.get(url)\n    response.raise_for_status()\n    checksum_index = response.json()\n    if name not in checksum_index:\n        raise ValueError(f'Model name {name} not found in checksum index.')\n    return checksum_index[name]\n\n", "CodeBleu_score": 35.08403953088155}]}
{"instance_id": "9312c8b6032e", "generation_result": [{"code": "def download(self, name: str, path: str) -> None:\n\t\n    url = self.url\n    if self.url.endswith('/'):\n        url = self.url + name\n    else:\n        url = self.url + '/' + name\n    if self.checksum_type == 'sha256':\n        checksum_type = 'sha256'\n    else:\n        checksum_type = 'md5'\n    checksum = self.checksum\n    if self.checksum_type == 'sha256':\n        checksum = self.sha256\n    else:\n        checksum = self.md5\n    if not os.path.exists(path):\n        os.makedirs(path)\n    tmp_file = NamedTemporaryFile(delete=False)\n    with httpx.stream('GET', url) as r:\n        r.raise_for_status()\n        with open(tmp_file.name, 'wb') as f:\n            for chunk in r.iter_bytes():\n                f.write(chunk)\n    tmp_file.close()\n    if checksum_type == 'sha256':\n        checksum_func = hashlib.sha256()\n    else:\n        checksum_func = hashlib.md5()\n    with open(tmp_file.name, 'rb') as f:\n        for chunk in iter(lambda: f.read(4096), b''):\n            checksum_func.update(chunk)\n    if checksum_func.hexdigest() != checksum:\n        raise RuntimeError(f'Checksum mismatch for {name}. Expected {checksum} but got {checksum_func.hexdigest()}')\n    with tarfile.open(tmp_file.name, 'r:gz') as tar:\n        tar.extractall(path)\n    os.unlink(tmp_file.name)\n    print(f'Extracted {name} to {path}')\n\n", "CodeBleu_score": 40.553711580197216}, {"code": "def download(self, name: str, path: str) -> None:\n\t\n    url = self.urls[name]\n    checksum = self.checksums[name]\n    with httpx.stream(\"GET\", url) as response:\n        response.raise_for_status()\n        with NamedTemporaryFile() as temp:\n            for chunk in response.iter_bytes():\n                temp.write(chunk)\n            temp.flush()\n            if hashlib.sha256(temp.read()).hexdigest() == checksum:\n                with tarfile.open(temp.name, \"r:gz\") as tar:\n                    tar.extractall(path)\n                os.remove(temp.name)\n                print(f\"Model {name} downloaded to {path}\")\n            else:\n                raise RuntimeError(f\"Checksum mismatch for {name}\")\n\n", "CodeBleu_score": 32.29585130568212}, {"code": "def download(self, name: str, path: str) -> None:\n\t\n    if self.model_urls is None:\n        raise RuntimeError(\"model_urls is not set\")\n    url = self.model_urls.get(name)\n    if url is None:\n        raise RuntimeError(f\"{name} is not found in model_urls\")\n    tmp_file = NamedTemporaryFile(delete=False)\n    with httpx.stream(\"GET\", url, timeout=None) as r:\n        r.raise_for_status()\n        total_length = r.headers.get(\"content-length\")\n        if total_length is None:\n            raise RuntimeError(\"Content-Length header is missing\")\n        total_length = int(total_length)\n        with open(tmp_file.name, \"wb\") as f:\n            for chunk in r.iter_bytes():\n                f.write(chunk)\n                f.flush()\n    self.logger.info(f\"Downloading {name} from {url}...\")\n    self.logger.info(f\"Saving to {tmp_file.name}...\")\n    self.logger.info(f\"Extracting to {path}...\")\n    self.validate_checksum(tmp_file.name, total_length)\n    self.extract_archive(tmp_file.name, path)\n    self.logger.info(f\"Deleting {tmp_file.name}...\")\n    os.remove(tmp_file.name)\n    self.logger.info(f\"{name} successfully downloaded\")\n\n", "CodeBleu_score": 41.47962364379632}]}
{"instance_id": "ee9493f37d77", "generation_result": [{"code": "def load_configuration(descriptor: str) -> Dict\n\t\n    if descriptor.startswith(\"embedded:\"):\n        name = descriptor.replace(\"embedded:\", \"\")\n        if name not in __embedded_configurations__:\n            raise FileNotFoundError(f\"No embedded configuration with name '{name}' found.\")\n        return json.loads(__embedded_configurations__[name])\n    else:\n        if not exists(descriptor):\n            raise FileNotFoundError(f\"No configuration file found at '{descriptor}'\")\n        with open(descriptor, \"r\") as file:\n            return json.loads(file.read())\n\n", "CodeBleu_score": 49.87017094731521}, {"code": "def load_configuration(descriptor: str) -> Dict\n\t\n    if descriptor.startswith(\"embedded:\"):\n        name = descriptor[9:]\n        with open(f\"{name}.json\", \"r\") as f:\n            return json.load(f)\n    else:\n        if not exists(descriptor):\n            raise ValueError(f\"Configuration file {descriptor} not found.\")\n        with open(descriptor, \"r\") as f:\n            return json.load(f)\n", "CodeBleu_score": 43.58349537603003}, {"code": "def load_configuration(descriptor: str) -> Dict\n\t\n    if descriptor.startswith(\"embedded:\"):\n        name = descriptor.split(\":\")[1]\n        if name not in __configurations__:\n            raise ValueError(f\"Embedded configuration {name} not found.\")\n        return json.loads(__configurations__[name])\n    if not exists(descriptor):\n        raise ValueError(f\"File {descriptor} not found.\")\n    with open(descriptor, \"r\") as file:\n        return json.load(file)\n\n", "CodeBleu_score": 52.6691913694347}]}
{"instance_id": "546134b67fbc", "generation_result": [{"code": "def sync_apply(\n    tensor_dict: Dict[str, tf.Tensor], func: Callable, concat_axis: int = 1\n) -> Dict[str, tf.Tensor]\n\t\n    if concat_axis not in [0, 1]:\n        raise ValueError(\n            \"The concat_axis must be either 0 or 1, but got {}\".format(concat_axis)\n        )\n\n    for key in tensor_dict.keys():\n        if tensor_dict[key].shape.rank != 2:\n            raise ValueError(\n                \"The tensor {} must have rank 2, but got {}\".format(\n                    key, tensor_dict[key].shape.rank\n                )\n            )\n\n    if concat_axis == 0:\n        tensor_dict = {\n            key: tf.reshape(tensor_dict[key], (-1, tensor_dict[key].shape[1]))\n            for key in tensor_dict.keys()\n        }\n        tensor_dict = {\n            key: func(tensor_dict[key])\n            for key in tensor_dict.keys()\n        }\n        tensor_dict = {\n            key: tf.reshape(tensor_dict[key], (tensor_dict[key].shape[0], -1))\n            for key in tensor_dict.keys()\n        }\n    else:\n        tensor_dict = {\n            key: tf.reshape(tensor_dict[key], (tensor_dict[key].shape[0], -1))\n            for key in tensor_dict.keys()\n        }\n        tensor_dict = {\n            key: func(tensor_dict[key])\n            for key in tensor_dict.keys()\n        }\n        tensor_dict = {\n            key: tf.reshape(tensor_dict[key], (-1, tensor_dict[key].shape[1]))\n            for key in tensor_dict.keys()\n        }\n\n    return tensor_dict\n\n", "CodeBleu_score": 33.852602303507076}, {"code": "def sync_apply(\n    tensor_dict: Dict[str, tf.Tensor], func: Callable, concat_axis: int = 1\n) -> Dict[str, tf.Tensor]\n\t\n\n    if concat_axis not in [0, 1]:\n        raise ValueError(\n            \"The concatenation axis must be either 0 or 1, but got {}\".format(\n                concat_axis\n            )\n        )\n\n    if concat_axis == 0:\n        tensor_dict = {\n            key: tf.reshape(\n                tensor, [tf.shape(tensor)[0], -1]\n            )  # reshape to [batch_size, -1]\n            for key, tensor in tensor_dict.items()\n        }\n\n    tensor_dict = {\n        key: tf.concat(values=tensor_dict[key], axis=concat_axis)\n        for key, tensor in tensor_dict.items()\n    }\n\n    tensor_dict = {\n        key: func(tensor_dict[key]) for key, tensor in tensor_dict.items()\n    }\n\n    if concat_axis == 0:\n        tensor_dict = {\n            key: tf.reshape(\n                tensor, [-1, tf.shape(tensor)[1]]\n            )  # reshape to [-1, feature_dim]\n            for key, tensor in tensor_dict.items()\n        }\n\n    return tensor_dict\n\n", "CodeBleu_score": 38.87432186685873}, {"code": "def sync_apply(\n    tensor_dict: Dict[str, tf.Tensor], func: Callable, concat_axis: int = 1\n) -> Dict[str, tf.Tensor]\n\t\n    if concat_axis not in [0, 1]:\n        raise ValueError(\n            \"The concatenation axis must be either 0 or 1, but it is {}\".format(\n                concat_axis\n            )\n        )\n    if concat_axis == 0:\n        tensor_dict_concat = {\n            key: tf.concat(value, axis=concat_axis)\n            for key, value in tensor_dict.items()\n        }\n        tensor_dict_processed = {\n            key: func(tensor_dict_concat[key])\n            for key, value in tensor_dict.items()\n        }\n        tensor_dict_processed = {\n            key: tf.split(tensor_dict_processed[key], value, axis=concat_axis)\n            for key, value in tensor_dict.items()\n        }\n    else:\n        tensor_dict_concat = {\n            key: tf.concat(value, axis=concat_axis)\n            for key, value in tensor_dict.items()\n        }\n        tensor_dict_processed = {\n            key: func(tensor_dict_concat[key])\n            for key, value in tensor_dict.items()\n        }\n        tensor_dict_processed = {\n            key: tf.split(tensor_dict_processed[key], value, axis=concat_axis)\n            for key, value in tensor_dict.items()\n        }\n    return tensor_dict_processed\n\n", "CodeBleu_score": 35.714380685038606}]}
{"instance_id": "df48e78d46d2", "generation_result": [{"code": "def from_float32_to_uint8(\n    tensor: tf.Tensor,\n    tensor_key: str = \"tensor\",\n    min_key: str = \"min\",\n    max_key: str = \"max\",\n) -> tf.Tensor\n\t\n\n    # Get the min and max values of the tensor\n    min_value = tf.reduce_min(tensor)\n    max_value = tf.reduce_max(tensor)\n\n    # Scale the tensor values to the uint8 range\n    scaled_tensor = (tensor - min_value) / (max_value - min_value) * 255\n\n    # Convert the scaled tensor to uint8\n    scaled_tensor = tf.cast(scaled_tensor, tf.uint8)\n\n    # Return the scaled tensor, the min value, and the max value\n    return {\n        tensor_key: scaled_tensor,\n        min_key: min_value,\n        max_key: max_value,\n    }\n\n", "CodeBleu_score": 50.13029596946588}, {"code": "def from_float32_to_uint8(\n    tensor: tf.Tensor,\n    tensor_key: str = \"tensor\",\n    min_key: str = \"min\",\n    max_key: str = \"max\",\n) -> tf.Tensor\n\t\n    min_val = tf.reduce_min(tensor)\n    max_val = tf.reduce_max(tensor)\n    scaled_tensor = (255.0 * (tensor - min_val)) / (max_val - min_val)\n    return {\n        tensor_key: tf.cast(scaled_tensor, tf.uint8),\n        min_key: min_val,\n        max_key: max_val,\n    }\n\n", "CodeBleu_score": 48.07970103079201}, {"code": "def from_float32_to_uint8(\n    tensor: tf.Tensor,\n    tensor_key: str = \"tensor\",\n    min_key: str = \"min\",\n    max_key: str = \"max\",\n) -> tf.Tensor\n\t\n\n    def _from_float32_to_uint8(tensor: tf.Tensor) -> tf.Tensor:\n        tensor = tf.cast(tensor, tf.float32)\n        min_value = tf.reduce_min(tensor)\n        max_value = tf.reduce_max(tensor)\n        tensor = (tensor - min_value) / (max_value - min_value)\n        tensor = tf.cast(tensor * 255, tf.uint8)\n        return tensor\n\n    tensor = tf.numpy_function(\n        _from_float32_to_uint8,\n        [tensor],\n        tf.uint8,\n    )\n    return {\n        tensor_key: tensor,\n        min_key: tf.constant(0.0),\n        max_key: tf.constant(1.0),\n    }\n\n", "CodeBleu_score": 49.38044019404001}]}
{"instance_id": "7f479e5de492", "generation_result": [{"code": "def pad_and_partition(tensor: tf.Tensor, segment_len: int) -> tf.Tensor\n\t\n    # Calculate the padding needed to make the tensor length a multiple of the segment length\n    padding_needed = segment_len - (tensor.shape[0] % segment_len)\n    # Pad the tensor\n    padded_tensor = tf.pad(tensor, [[0, padding_needed]])\n    # Reshape the tensor into segments of the specified length\n    return tf.reshape(padded_tensor, [-1, segment_len])\n\n", "CodeBleu_score": 16.275735623776523}, {"code": "def pad_and_partition(tensor: tf.Tensor, segment_len: int) -> tf.Tensor\n\t\n    # pylint: disable=line-too-long\n    # pylint: disable=too-many-locals\n    # pylint: disable=too-many-branches\n    # pylint: disable=too-many-statements\n    # pylint: disable=too-many-arguments\n    # pylint: disable=too-many-return-statements\n    # pylint: disable=missing-function-docstring\n    # pylint: disable=missing-module-docstring\n    # pylint: disable=missing-class-docstring\n    # pylint: disable=too-many-arguments\n    # pylint: disable=too-many-branches\n    # pylint: disable=too-many-statements\n    # pylint: disable=too-many-locals\n    # pylint: disable=too-many-arguments\n    # pylint: disable=too-many-return-statements\n    # pylint: disable=missing-function-docstring\n    # pylint: disable=missing-module-docstring\n    # pylint: disable=missing-class-docstring\n    # pylint: disable=too-many-arguments\n    # pylint: disable=too-many-branches\n    # pylint: disable=too-many-statements\n    # pylint: disable=too-many-locals\n    # pylint: disable=too-many-arguments\n    # pylint: disable=too-many-return-statements\n    # pylint: disable=missing-function-docstring\n    # pylint: disable=missing-module-docstring\n    # pylint: disable=missing-class-docstring\n    # pylint: disable=too-many-arguments\n    # pylint: disable=too-many-branches\n    # pylint: disable=too-many-statements\n    # pylint: disable=too-many-locals\n    # pylint: disable=too-many-arguments\n    # pylint: disable=too", "CodeBleu_score": 7.82608695652174}, {"code": "def pad_and_partition(tensor: tf.Tensor, segment_len: int) -> tf.Tensor\n\t\n\n    # Calculate the number of segments needed to pad the tensor\n    num_segments = (tensor.shape[0] + segment_len - 1) // segment_len\n\n    # Calculate the padding needed to make the tensor length a multiple of the segment length\n    pad_amount = num_segments * segment_len - tensor.shape[0]\n\n    # Pad the tensor\n    padded_tensor = tf.pad(tensor, [[0, pad_amount], [0, 0]])\n\n    # Reshape the padded tensor into a new tensor with the specified segment length\n    reshaped_tensor = tf.reshape(padded_tensor, [num_segments, segment_len])\n\n    return reshaped_tensor\n\n", "CodeBleu_score": 28.40739255147803}]}
{"instance_id": "1789fae7253f", "generation_result": [{"code": "def pad_and_reshape(instr_spec, frame_length, F) -> Any\n\t\n\n    def pad_and_reshape_fn(spectrogram: Any) -> Any:\n        \"\"\"The function pads and reshapes an input spectrogram tensor.", "CodeBleu_score": 3.07149705624551}, {"code": "def pad_and_reshape(instr_spec, frame_length, F) -> Any\n\t\n    pad_len = frame_length - instr_spec.shape[0]\n    pad_len_half = pad_len // 2\n    pad_len_rem = pad_len % 2\n    if pad_len_rem == 0:\n        pad_len_rem = 0\n    else:\n        pad_len_rem = 1\n    pad_len_half_rem = pad_len_half + pad_len_rem\n    pad_top = F.zeros((pad_len_half, instr_spec.shape[1]))\n    pad_bottom = F.zeros((pad_len_half_rem, instr_spec.shape[1]))\n    instr_spec = F.concat((pad_top, instr_spec, pad_bottom), axis=0)\n    instr_spec = F.reshape(instr_spec, (frame_length, -1))\n    return instr_spec\n\n", "CodeBleu_score": 26.722828632942807}, {"code": "def pad_and_reshape(instr_spec, frame_length, F) -> Any\n\t\n\n    # TODO: Implement this function\n    # raise NotImplementedError\n    pad_len = frame_length - instr_spec.shape[0]\n    pad_matrix = F.zeros(\n        (pad_len, instr_spec.shape[1])\n    )\n    padded_spec = F.concat(\n        instr_spec,\n        pad_matrix,\n        dim=0\n    )\n    padded_spec = F.reshape(\n        padded_spec,\n        (frame_length, instr_spec.shape[1] * instr_spec.shape[0])\n    )\n    return padded_spec\n\n", "CodeBleu_score": 16.543318594694213}]}
{"instance_id": "8a4f4658d97c", "generation_result": [{"code": "def dataset_from_csv(csv_path: str, **kwargs) -> Any\n\t\n    import pandas as pd\n    import tensorflow as tf\n    from tensorflow.python.data.experimental import make_csv_dataset\n\n    df = pd.read_csv(csv_path)\n\n    def _parse_csv(line: str) -> Dict[str, Any]:\n        columns = line.split(\",\")\n        return {\n            \"label\": tf.convert_to_tensor(columns[0], dtype=tf.int64),\n            \"text\": tf.convert_to_tensor(columns[1], dtype=tf.string),\n        }\n\n    return make_csv_dataset(\n        csv_path,\n        batch_size=kwargs.get(\"batch_size\", 32),\n        column_names=[\"label\", \"text\"],\n        label_name=\"label\",\n        num_epochs=kwargs.get(\"num_epochs\", 1),\n        shuffle=kwargs.get(\"shuffle\", True),\n        shuffle_buffer_size=kwargs.get(\"shuffle_buffer_size\", 1000),\n        header=kwargs.get(\"header\", True),\n        field_delim=kwargs.get(\"field_delim\", \",\"),\n        select_columns=kwargs.get(\"select_columns\", None),\n        column_defaults=kwargs.get(\"column_defaults\", None),\n        use_quote_delim=kwargs.get(\"use_quote_delim\", False),\n        na_value=kwargs.get(\"na_value\", \"\"),\n        num_parallel_reads=kwargs.get(\"num_parallel_reads\", 1),\n        sloppy=kwargs.get(\"sloppy\", False),\n        ignore_errors=kwargs.get(\"ignore_errors\", False),\n        num_parallel_batches=kwargs.get(\"num_parallel_batches\", 1),\n        drop_final_column=kwargs.get(\"drop_final_column\", False),\n        shuffle_seed=kwargs.get(\"shuffle_seed\", None),\n        prefetch_buffer_size=kwargs.get(\"prefetch_buffer_size\", 1),\n        num_threads=kwargs.get(\"num_threads\", None),\n        compression_type=kwargs.get(\"com", "CodeBleu_score": 18.743952429802786}, {"code": "def dataset_from_csv(csv_path: str, **kwargs) -> Any\n\t\n    import pandas as pd\n    import tensorflow as tf\n\n    df = pd.read_csv(csv_path)\n    dataset = tf.data.Dataset.from_tensor_slices(dict(df))\n    return dataset\n\n", "CodeBleu_score": 25.836439232608967}, {"code": "def dataset_from_csv(csv_path: str, **kwargs) -> Any\n\t\n    import pandas as pd\n    import tensorflow as tf\n\n    df = pd.read_csv(csv_path)\n    dataset = tf.data.Dataset.from_tensor_slices(\n        (\n            {\n                key: tf.convert_to_tensor(value, dtype=tf.float32)\n                for key, value in df.to_dict(\"list\").items()\n            }\n        )\n    )\n    return dataset\n\n", "CodeBleu_score": 27.953097224460617}]}
{"instance_id": "9a16e81b182f", "generation_result": [{"code": "def check_tensor_shape(tensor_tf: tf.Tensor, target_shape: Any) -> bool\n\t\n    result = True\n    for dim_idx, dim_len in enumerate(target_shape):\n        if dim_idx >= len(tensor_tf.shape):\n            result = False\n            break\n        if dim_len is not None and dim_len != tensor_tf.shape[dim_idx]:\n            result = False\n            break\n    return result\n\n", "CodeBleu_score": 32.53887400473709}, {"code": "def check_tensor_shape(tensor_tf: tf.Tensor, target_shape: Any) -> bool\n\t\n    result = True\n    for i in range(len(target_shape)):\n        if target_shape[i] is not None and tensor_tf.shape[i] != target_shape[i]:\n            result = False\n    return result\n\n", "CodeBleu_score": 28.45388678900785}, {"code": "def check_tensor_shape(tensor_tf: tf.Tensor, target_shape: Any) -> bool\n\t\n    result = True\n    for i, target_dim in enumerate(target_shape):\n        if target_dim is not None:\n            result = result and (tensor_tf.shape[i] == target_dim)\n    return result\n\n", "CodeBleu_score": 33.606374325630064}]}
{"instance_id": "49d5d3084b3a", "generation_result": [{"code": "def get_training_dataset(\n    audio_params: Dict, audio_adapter: AudioAdapter, audio_path: str\n) -> Any\n\t\n    # Initialize the dataset builder\n    dataset_builder = DatasetBuilder(audio_params, audio_adapter)\n\n    # Build the dataset\n    dataset = dataset_builder.build(\n        train_csv=audio_path,\n        cache_dir=f\"{audio_path}{SEPARATOR}cache\",\n        batch_size=audio_params[\"batch_size\"],\n        num_chunks_per_song=audio_params[\"num_chunks_per_song\"],\n        augment=audio_params[\"augment\"],\n        cache=audio_params[\"cache\"],\n    )\n\n    return dataset\n\n", "CodeBleu_score": 28.09168580794251}, {"code": "def get_training_dataset(\n    audio_params: Dict, audio_adapter: AudioAdapter, audio_path: str\n) -> Any\n\t\n    builder = DatasetBuilder(\n        audio_params=audio_params,\n        audio_adapter=audio_adapter,\n        audio_path=audio_path,\n        chunk_duration=audio_params[\"chunk_duration\"],\n        random_seed=audio_params[\"random_seed\"],\n    )\n    return builder.build(\n        train_csv=audio_params[\"train_csv\"],\n        cache_dir=audio_params[\"cache_dir\"],\n        batch_size=audio_params[\"batch_size\"],\n        chunks_per_song=audio_params[\"chunks_per_song\"],\n        augment=audio_params[\"augment\"],\n        cache=audio_params[\"cache\"],\n    )\n\n", "CodeBleu_score": 34.419942825415134}, {"code": "def get_training_dataset(\n    audio_params: Dict, audio_adapter: AudioAdapter, audio_path: str\n) -> Any\n\t\n    dataset_builder = DatasetBuilder(\n        audio_params,\n        audio_adapter,\n        audio_path,\n        chunk_duration=audio_params[\"chunk_duration\"],\n        random_seed=audio_params[\"random_seed\"],\n    )\n    return dataset_builder.build(\n        train_csv=audio_params[\"train_csv\"],\n        cache_dir=audio_params[\"cache_dir\"],\n        batch_size=audio_params[\"batch_size\"],\n        num_chunks_per_song=audio_params[\"num_chunks_per_song\"],\n        data_augmentation=audio_params[\"data_augmentation\"],\n        cache_dataset=audio_params[\"cache_dataset\"],\n    )\n\n", "CodeBleu_score": 40.644582251159626}]}
{"instance_id": "fe8bbbba06e3", "generation_result": [{"code": "def instruments(self) -> Any\n\t\n    if not self.instrument_builders:\n        self.instrument_builders = [\n            InstrumentDatasetBuilder(\n                instrument,\n                self.instrument_datasets_dir,\n                self.instrument_datasets_dir_template,\n                self.instrument_dataset_file_template,\n                self.instrument_dataset_file_template_with_date,\n                self.instrument_dataset_file_template_with_date_and_time,\n                self.instrument_dataset_file_template_with_date_and_time_and_market_type,\n                self.instrument_dataset_file_template_with_date_and_time_and_market_type_and_granularity,\n                self.instrument_dataset_file_template_with_date_and_time_and_market_type_and_granularity_and_bar_count,\n                self.instrument_dataset_file_template_with_date_and_time_and_market_type_and_granularity_and_bar_count_and_bar_number,\n                self.instrument_dataset_file_template_with_date_and_time_and_market_type_and_granularity_and_bar_count_and_bar_number_and_bar_start_time,\n                self.instrument_dataset_file_template_with_date_and_time_and_market_type_and_granularity_and_bar_count_and_bar_number_and_bar_end_time,\n                self.instrument_dataset_file_template_with_date_and_time_and_market_type_and_granularity_and_bar_count_and_bar_number_and_bar_start_time_and_bar_end_time,\n                self.instrument_dataset_file_template_with_date_and_time_and_market_type_and_granularity_and_bar_count_and_bar_number_and_bar_start_time_and_bar_end_time_and_bar_duration,\n                self.instrument_dataset_file_template_with_", "CodeBleu_score": 14.231644875325463}, {"code": "def instruments(self) -> Any\n\t\n\n    if not self._instrument_dataset_builders:\n        self._instrument_dataset_builders = []\n        for instrument in self._instruments:\n            instrument_dataset_builder = self._create_instrument_dataset_builder(instrument)\n            self._instrument_dataset_builders.append(instrument_dataset_builder)\n\n    for instrument_dataset_builder in self._instrument_dataset_builders:\n        yield instrument_dataset_builder\n\n", "CodeBleu_score": 55.69924732457089}, {"code": "def instruments(self) -> Any\n\t\n    if not self._instrument_builders:\n        self._instrument_builders = [\n            InstrumentDatasetBuilder(\n                self,\n                instrument,\n                self._dataset_path,\n                self._instrument_configs[instrument],\n                self._configs,\n            )\n            for instrument in self._instruments\n        ]\n\n    for instrument_builder in self._instrument_builders:\n        yield instrument_builder\n\n", "CodeBleu_score": 34.27318510916227}]}
{"instance_id": "438b57fd3413", "generation_result": [{"code": "def cache(self, dataset: Any, cache: Optional[str], wait: bool) -> Any\n\t\n    if cache is None:\n        return dataset\n    cache_dir = cache\n    if not exists(cache_dir):\n        os.mkdir(cache_dir)\n    cache_path = f\"{cache_dir}{SEPARATOR}{self.name}.pkl\"\n    if not exists(cache_path):\n        if wait:\n            while not exists(cache_path):\n                print(f\"Waiting for {cache_path} to be created...\")\n                time.sleep(1)\n        self.cache(dataset, cache_path)\n    return self.load(cache_path)\n\n", "CodeBleu_score": 46.216548947375344}, {"code": "def cache(self, dataset: Any, cache: Optional[str], wait: bool) -> Any\n\t\n    if not cache:\n        return dataset\n    cache_path = cache\n    if not cache_path.endswith(SEPARATOR):\n        cache_path += SEPARATOR\n    cache_path += self.cache_index_name\n    if not exists(cache_path):\n        if wait:\n            while not exists(cache_path):\n                print(f\"Waiting for {cache_path} to be created...\")\n                time.sleep(10)\n        else:\n            raise FileNotFoundError(f\"Cache index file not found: {cache_path}\")\n    cache_path = cache_path[:-len(self.cache_index_name)]\n    if not exists(cache_path):\n        raise FileNotFoundError(f\"Cache directory not found: {cache_path}\")\n    return self.cache_dataset(dataset, cache_path)\n\n", "CodeBleu_score": 42.154656902728796}, {"code": "def cache(self, dataset: Any, cache: Optional[str], wait: bool) -> Any\n\t\n\n    if cache is None:\n        return dataset\n\n    cache_path = cache\n    cache_path = cache_path.replace(\"\\\\\", SEPARATOR)\n\n    # If the cache path is a directory, append the dataset name to it\n    if os.path.isdir(cache_path):\n        cache_path = os.path.join(cache_path, self.name)\n\n    # If the cache path does not exist, create it\n    if not exists(cache_path):\n        os.makedirs(cache_path)\n\n    # If the cache path is a directory, append the dataset name to it\n    if os.path.isdir(cache_path):\n        cache_path = os.path.join(cache_path, self.name)\n\n    # If the cache path does not exist, create it\n    if not exists(cache_path):\n        os.makedirs(cache_path)\n\n    # If the cache path is a directory, append the dataset name to it\n    if os.path.isdir(cache_path):\n        cache_path = os.path.join(cache_path, self.name)\n\n    # If the cache path does not exist, create it\n    if not exists(cache_path):\n        os.makedirs(cache_path)\n\n    # If the cache path is a directory, append the dataset name to it\n    if os.path.isdir(cache_path):\n        cache_path = os.path.join(cache_path, self.name)\n\n    # If the cache path does not exist, create it\n    if not exists(cache_path):\n        os.makedirs(cache_path)\n\n    # If the cache path is a directory, append the dataset name to it\n    if os.path.isdir(cache_path):\n        cache_path = os.path.join(cache_path, self.name)\n\n    # If the cache path does not exist, create it\n    if not exists(cache_path):\n        os.makedirs(cache_path)\n\n    # If the cache path is a directory, append the dataset name to it\n    if os.path.isdir(cache_path):\n", "CodeBleu_score": 36.52571165195081}]}
{"instance_id": "8838a3a09a66", "generation_result": [{"code": "def create_estimator(params: Dict, MWF: bool) -> tf.Tensor\n\t\n    # Load the model directory\n    model_dir = ModelProvider.get_model_dir(params)\n\n    # Update the parameters\n    params = ModelProvider.update_parameters(params)\n\n    # Setup session configuration\n    session_config = tf.ConfigProto(\n        gpu_options=tf.GPUOptions(allow_growth=True, per_process_gpu_memory_fraction=0.75)\n    )\n\n    # Create run configuration\n    run_config = tf.estimator.RunConfig(\n        session_config=session_config,\n        model_dir=model_dir,\n        save_checkpoints_steps=1000,\n        save_summary_steps=1000,\n        keep_checkpoint_max=10,\n        keep_checkpoint_every_n_hours=1,\n        log_step_count_steps=1000,\n    )\n\n    # Create estimator\n    estimator = tf.estimator.Estimator(\n        model_fn=ModelProvider.get_model_fn(params, MWF), config=run_config, params=params\n    )\n\n    return estimator\n\n", "CodeBleu_score": 26.75654609138603}, {"code": "def create_estimator(params: Dict, MWF: bool) -> tf.Tensor\n\t\n    model_dir = params['model_dir']\n    model_provider = ModelProvider(model_dir)\n    model_dir = model_provider.get_model_dir()\n    params['model_dir'] = model_dir\n    if MWF:\n        model_provider.update_params(params)\n    session_config = tf.ConfigProto()\n    session_config.gpu_options.per_process_gpu_memory_fraction = 0.5\n    run_config = tf.estimator.RunConfig(session_config=session_config)\n    return tf.estimator.Estimator(model_fn=model_provider.model_fn, model_dir=model_dir, params=params, config=run_config)\n\n", "CodeBleu_score": 40.39502786498688}, {"code": "def create_estimator(params: Dict, MWF: bool) -> tf.Tensor\n\t\n    model_dir = params[\"model_dir\"]\n    model_provider = params[\"model_provider\"]\n    model_function = params[\"model_function\"]\n    params[\"model_dir\"] = model_provider.get_model_dir(model_dir)\n    if MWF:\n        model_provider.load_model_weights(model_dir)\n    params[\"session_config\"] = tf.ConfigProto(\n        gpu_options=tf.GPUOptions(per_process_gpu_memory_fraction=0.333)\n    )\n    params[\"config\"] = tf.estimator.RunConfig(\n        session_config=params[\"session_config\"],\n        model_dir=params[\"model_dir\"],\n        save_summary_steps=100,\n    )\n    return tf.estimator.Estimator(\n        model_fn=model_function, params=params, config=params[\"config\"]\n    )\n\n", "CodeBleu_score": 37.920471172484724}]}
{"instance_id": "779c812bf2cb", "generation_result": [{"code": "def _separate_tensorflow(\n        self, waveform: np.ndarray, audio_descriptor: AudioDescriptor\n    ) -> Dict\n\t\n    if audio_descriptor.channels != 2:\n        waveform = self._stereo_waveform(waveform)\n    prediction_generator = self._prediction_generator(waveform)\n    prediction = next(prediction_generator)\n    del prediction[\"audio_id\"]\n    return prediction\n\n", "CodeBleu_score": 47.31300416732932}, {"code": "def _separate_tensorflow(\n        self, waveform: np.ndarray, audio_descriptor: AudioDescriptor\n    ) -> Dict\n\t\n    # Ensure waveform is stereo\n    if waveform.ndim == 1:\n        waveform = np.tile(waveform, 2)\n\n    # Create prediction generator\n    prediction_generator = self.model.predict_generator(\n        [waveform],\n        steps=1,\n        verbose=0,\n        callbacks=[self.tensorboard_callback],\n        workers=self.workers,\n        use_multiprocessing=self.use_multiprocessing,\n        max_queue_size=self.max_queue_size,\n        workers_per_queue=self.workers_per_queue,\n    )\n\n    # Obtain prediction\n    prediction = next(prediction_generator)\n\n    # Remove audio_id key\n    prediction.pop(\"audio_id\")\n\n    return prediction\n\n", "CodeBleu_score": 53.24626448568939}, {"code": "def _separate_tensorflow(\n        self, waveform: np.ndarray, audio_descriptor: AudioDescriptor\n    ) -> Dict\n\t\n    if audio_descriptor.channel_count != 2:\n        raise ValueError(\n            f\"Expected stereo audio, got {audio_descriptor.channel_count} channels\"\n        )\n    if audio_descriptor.sample_rate != self.sample_rate:\n        raise ValueError(\n            f\"Expected sample rate {self.sample_rate}, got {audio_descriptor.sample_rate}\"\n        )\n    self._update_data_generator(waveform, audio_descriptor)\n    prediction = next(self.predictions_generator)\n    return prediction\n\n", "CodeBleu_score": 32.87869253408435}]}
{"instance_id": "cf580cd2ed93", "generation_result": [{"code": "def compute_backbone_shapes(config, image_shape)\n\t\n    if callable(config.BACKBONE):\n        return config.BACKBONE.compute_backbone_shapes(image_shape)\n    elif config.BACKBONE.lower() == \"resnet50\":\n        return ResNet.compute_backbone_shapes(config, image_shape)\n    elif config.BACKBONE.lower() == \"resnet101\":\n        return ResNet.compute_backbone_shapes(config, image_shape)\n    else:\n        raise ValueError(\"Unsupported backbone architecture.\")\n\n", "CodeBleu_score": 42.398811638777026}, {"code": "def compute_backbone_shapes(config, image_shape)\n\t\n    if callable(config.BACKBONE):\n        return config.BACKBONE.compute_backbone_shapes(\n            image_shape, config)\n\n    if config.BACKBONE.startswith('resnet'):\n        return resnet_shape_compute(config, image_shape)\n\n    else:\n        raise ValueError('Unknown backbone architecture.')\n\n", "CodeBleu_score": 41.571310597696154}, {"code": "def compute_backbone_shapes(config, image_shape)\n\t\n    if callable(config.BACKBONE):\n        return config.BACKBONE(image_shape, config=config, **config.BACKBONE_KWARGS)\n    elif config.BACKBONE.lower() == \"resnet50\":\n        return ResNet50Backbone.compute_backbone_shapes(image_shape, config)\n    elif config.BACKBONE.lower() == \"resnet101\":\n        return ResNet101Backbone.compute_backbone_shapes(image_shape, config)\n    elif config.BACKBONE.lower() == \"resnet152\":\n        return ResNet152Backbone.compute_backbone_shapes(image_shape, config)\n    elif config.BACKBONE.lower() == \"resnet50v2\":\n        return ResNet50V2Backbone.compute_backbone_shapes(image_shape, config)\n    elif config.BACKBONE.lower() == \"resnet101v2\":\n        return ResNet101V2Backbone.compute_backbone_shapes(image_shape, config)\n    elif config.BACKBONE.lower() == \"resnet152v2\":\n        return ResNet152V2Backbone.compute_backbone_shapes(image_shape, config)\n    elif config.BACKBONE.lower() == \"resnext50\":\n        return ResNeXt50Backbone.compute_backbone_shapes(image_shape, config)\n    elif config.BACKBONE.lower() == \"resnext101\":\n        return ResNeXt101Backbone.compute_backbone_shapes(image_shape, config)\n    elif config.BACKBONE.lower() == \"resnext152\":\n        return ResNeXt152Backbone.compute_backbone_shapes(image_shape, config)\n    elif config.BACKBONE.lower() == \"vgg16\":\n        return VGG16Backbone.compute_backbone_shapes(image_shape, config)\n    elif config.BACKB", "CodeBleu_score": 32.9934658150497}]}
{"instance_id": "ebd61a2e59ca", "generation_result": [{"code": "def clip_boxes_graph(boxes, window)\n\t\n    boxes = tf.cast(boxes, tf.float32)\n    window = tf.cast(window, tf.float32)\n    boxes_x1 = boxes[:, 0]\n    boxes_y1 = boxes[:, 1]\n    boxes_x2 = boxes[:, 2]\n    boxes_y2 = boxes[:, 3]\n    window_x1y1 = window[:, 0]\n    window_x2y2 = window[:, 1]\n    window_x1y1 = tf.tile(window_x1y1, [tf.shape(boxes)[0], 1])\n    window_x2y2 = tf.tile(window_x2y2, [tf.shape(boxes)[0], 1])\n    boxes_x1 = tf.maximum(tf.minimum(boxes_x1, window_x2y2[:, 0]), window_x1y1[:, 0])\n    boxes_y1 = tf.maximum(tf.minimum(boxes_y1, window_x2y2[:, 1]), window_x1y1[:, 1])\n    boxes_x2 = tf.maximum(tf.minimum(boxes_x2, window_x2y2[:, 0]), window_x1y1[:, 0])\n    boxes_y2 = tf.maximum(tf.minimum(boxes_y2, window_x2y2[:, 1]), window_x1y1[:, 1])\n    boxes = tf.stack([boxes_x1, boxes_y1, boxes_x2, boxes_y2], axis=1)\n    return boxes\n\n", "CodeBleu_score": 19.827067909056666}, {"code": "def clip_boxes_graph(boxes, window)\n\t\n    # Split the window and box coordinates into individual components\n    x_min, y_min, x_max, y_max = tf.split(boxes, 4, axis=1)\n    win_x_min, win_y_min, win_x_max, win_y_max = tf.split(window, 4, axis=1)\n\n    # Clip the bounding box coordinates to lie within the window boundaries\n    x_min_clipped = tf.maximum(x_min, win_x_min)\n    y_min_clipped = tf.maximum(y_min, win_y_min)\n    x_max_clipped = tf.minimum(x_max, win_x_max)\n    y_max_clipped = tf.minimum(y_max, win_y_max)\n\n    # Concatenate the clipped coordinates back into a tensor\n    clipped_boxes = tf.concat([x_min_clipped, y_min_clipped, x_max_clipped, y_max_clipped], axis=1)\n\n    return clipped_boxes\n\n", "CodeBleu_score": 19.72524747761039}, {"code": "def clip_boxes_graph(boxes, window)\n\t\n    # Split boxes into x, y, w, h\n    x1, y1, x2, y2 = tf.split(boxes, 4, axis=1)\n\n    # Split window into x, y, w, h\n    x1w, y1w, x2w, y2w = tf.split(window, 4, axis=1)\n\n    # Clip x1\n    x1c = tf.maximum(x1, x1w)\n\n    # Clip y1\n    y1c = tf.maximum(y1, y1w)\n\n    # Clip x2\n    x2c = tf.minimum(x2, x2w)\n\n    # Clip y2\n    y2c = tf.minimum(y2, y2w)\n\n    # Concat boxes back into x1, y1, x2, y2\n    boxes_clipped = tf.concat([x1c, y1c, x2c, y2c], axis=1)\n\n    return boxes_clipped\n\n", "CodeBleu_score": 20.39167049693067}]}
{"instance_id": "8e0411587a70", "generation_result": [{"code": "def rpn_graph(feature_map, anchors_per_location, anchor_stride)\n\t\n    # Shared convolutional base of the RPN\n    shared_4conv = keras.layers.Conv2D(512, (3, 3), activation='relu', padding='same')(feature_map)\n\n    # RPN class scores\n    num_anchors = len(anchors_per_location)\n    num_classes = 2 # background and foreground\n    rpn_class_logits = keras.layers.Conv2D(num_anchors * num_classes, (1, 1), activation=None, bias_initializer='zeros')(shared_4conv)\n    rpn_class_logits = keras.layers.Reshape((-1, num_classes))(rpn_class_logits)\n    rpn_class_probs = keras.layers.Activation('sigmoid')(rpn_class_logits)\n\n    # RPN bounding box regression deltas\n    rpn_bbox = keras.layers.Conv2D(num_anchors * 4, (1, 1), activation=None, bias_initializer='zeros')(shared_4conv)\n    rpn_bbox = keras.layers.Reshape((-1, 4))(rpn_bbox)\n\n    return rpn_class_logits, rpn_class_probs, rpn_bbox\n\n", "CodeBleu_score": 37.436711484700176}, {"code": "def rpn_graph(feature_map, anchors_per_location, anchor_stride)\n\t\n    # Shared convolutional base of the RPN\n    x = keras.layers.Conv2D(512, (3, 3), strides=1, padding='same', name='rpn_conv_3x3')(feature_map)\n    x = keras.layers.BatchNormalization(name='rpn_bn_3x3')(x)\n    x = keras.layers.Activation('relu', name='rpn_relu_3x3')(x)\n\n    # Class score predictions\n    x_class = keras.layers.Conv2D(anchors_per_location * 2, (1, 1), strides=1, padding='valid', name='rpn_out_cls')(x)\n    x_class = keras.layers.Reshape((-1, 2), name='rpn_cls_reshape')(x_class)\n    x_class = keras.layers.Activation('softmax', name='rpn_cls_act')(x_class)\n\n    # Bounding box offset predictions\n    x_regr = keras.layers.Conv2D(anchors_per_location * 4, (1, 1), strides=1, padding='valid', name='rpn_out_regr')(x)\n    x_regr = keras.layers.Reshape((-1, 4), name='rpn_regr_reshape')(x_regr)\n\n    return x_class, x_regr\n\n", "CodeBleu_score": 38.648671658479515}, {"code": "def rpn_graph(feature_map, anchors_per_location, anchor_stride)\n\t\n    # Shared convolutional base of the RPN\n    num_anchors = len(anchors_per_location)\n    x = keras.layers.Conv2D(512, (3, 3), padding='same', activation='relu', kernel_initializer='normal', name='rpn_conv_shared')(feature_map)\n\n    # Class score predictions\n    x_class = keras.layers.Conv2D(num_anchors, (1, 1), activation='linear', kernel_initializer='normal', name='rpn_out_class')(x)\n    x_class = keras.layers.Reshape((-1, num_anchors), name='rpn_out_class_reshape')(x_class)\n    x_class_softmax = keras.layers.Activation('softmax', name='rpn_out_class_softmax')(x_class)\n\n    # Bounding box regression targets\n    x_regr = keras.layers.Conv2D(num_anchors * 4, (1, 1), activation='linear', kernel_initializer='zero', name='rpn_out_regress')(x)\n    x_regr = keras.layers.Reshape((-1, 4), name='rpn_out_regress_reshape')(x_regr)\n\n    return x_class_softmax, x_regr\n\n", "CodeBleu_score": 23.67050091441976}]}
{"instance_id": "70777c9261d8", "generation_result": [{"code": "def smooth_l1_loss(y_true, y_pred)\n\t\n    abs_diff = K.abs(y_true - y_pred)\n    abs_diff_less_one = K.cast(K.less(abs_diff, 1.0), dtype='float32')\n    loss = abs_diff_less_one * 0.5 * K.square(abs_diff) + (1.0 - abs_diff_less_one) * (abs_diff - 0.5)\n    return K.sum(loss)\n\n", "CodeBleu_score": 31.230400048895383}, {"code": "def smooth_l1_loss(y_true, y_pred)\n\t\n    abs_diff = K.abs(y_true - y_pred)\n    abs_diff_lt_one = K.cast(K.less(abs_diff, 1.0), K.floatx())\n    loss = abs_diff_lt_one * 0.5 * K.square(abs_diff) + (1.0 - abs_diff_lt_one) * (abs_diff - 0.5)\n    return K.sum(loss, axis=-1)\n\n", "CodeBleu_score": 29.05825081003277}, {"code": "def smooth_l1_loss(y_true, y_pred)\n\t\n    abs_diff = K.abs(y_true - y_pred)\n    abs_diff_less_one = K.cast(K.less(abs_diff, 1.0), dtype='float32')\n    return 0.5 * (abs_diff * abs_diff_less_one + (abs_diff - 0.5) * (1.0 - abs_diff_less_one))\n\n", "CodeBleu_score": 27.878806041900674}]}
{"instance_id": "0e747b604087", "generation_result": [{"code": "def build_model(x_train, num_classes)\n\t\n    # Reset the graph\n    tf.reset_default_graph()\n    # Define the input layer\n    x = tf.placeholder(tf.float32, [None, 224, 224, 3], name='input_layer')\n    # Define the first convolutional layer\n    conv1 = tf.layers.conv2d(inputs=x, filters=32, kernel_size=[3, 3], activation=tf.nn.relu, name='conv1')\n    # Define the second convolutional layer\n    conv2 = tf.layers.conv2d(inputs=conv1, filters=64, kernel_size=[3, 3], activation=tf.nn.relu, name='conv2')\n    # Define the max-pooling layer\n    pool = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2, name='pool')\n    # Flatten the output of the max-pooling layer\n    flat = tf.reshape(pool, [-1, 7 * 7 * 64])\n    # Define the dense layer\n    dense = tf.layers.dense(inputs=flat, units=1024, activation=tf.nn.relu, name='dense')\n    # Define the output layer\n    logits = tf.layers.dense(inputs=dense, units=num_classes, activation=tf.nn.softmax, name='output_layer')\n    # Compile the model\n    model = tf.keras.Model(inputs=x, outputs=logits, name='model')\n    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n    return model\n\n", "CodeBleu_score": 34.802121555294065}, {"code": "def build_model(x_train, num_classes)\n\t\n    tf.reset_default_graph()\n    input_layer = tf.keras.layers.Input(shape=(x_train.shape[1], x_train.shape[2], x_train.shape[3]))\n    conv_layer1 = tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(input_layer)\n    conv_layer2 = tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(conv_layer1)\n    max_pool_layer = tf.keras.layers.MaxPool2D(pool_size=(2, 2))(conv_layer2)\n    flatten_layer = tf.keras.layers.Flatten()(max_pool_layer)\n    dense_layer1 = tf.keras.layers.Dense(units=512, activation='relu')(flatten_layer)\n    dense_layer2 = tf.keras.layers.Dense(units=num_classes, activation='softmax')(dense_layer1)\n    model = tf.keras.models.Model(inputs=input_layer, outputs=dense_layer2)\n    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n    return model\n\n", "CodeBleu_score": 26.194726110676985}, {"code": "def build_model(x_train, num_classes)\n\t\n\n    model = tf.keras.Sequential([\n        tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(x_train.shape[1], x_train.shape[2], 3)),\n        tf.keras.layers.MaxPooling2D(2, 2),\n        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n        tf.keras.layers.MaxPooling2D(2, 2),\n        tf.keras.layers.Flatten(),\n        tf.keras.layers.Dense(128, activation='relu'),\n        tf.keras.layers.Dense(num_classes, activation='softmax')\n    ])\n\n    model.compile(optimizer='adam',\n                  loss='sparse_categorical_crossentropy',\n                  metrics=['accuracy'])\n\n    return model\n\n", "CodeBleu_score": 27.646071267804068}]}
{"instance_id": "813e3e2844f4", "generation_result": [{"code": "def extract_bboxes(mask)\n\t\n    boxes = []\n    for i in range(mask.shape[2]):\n        m = mask[:, :, i]\n        # Bounding box.\n        horizontal_indices = numpy.where(m.any(0))[0]\n        vertical_indices = numpy.where(m.any(1))[0]\n        if horizontal_indices.shape[0]:\n            x1, x2 = horizontal_indices[[0, -1]]\n            y1, y2 = vertical_indices[[0, -1]]\n            # x2 and y2 should not be part of the box. Increment by 1.\n            x2 += 1\n            y2 += 1\n        else:\n            # No mask for this instance. Might happen due to\n            # resizing or cropping. Set bbox to zeros\n            x1, x2, y1, y2 = 0, 0, 0, 0\n        boxes.append([y1, y2, x1, x2])\n    return boxes\n\n", "CodeBleu_score": 51.326690140980865}, {"code": "def extract_bboxes(mask)\n\t\n    bboxes = []\n    for i in range(mask.shape[0]):\n        m = mask[i, :, :]\n        if m.any():\n            # Find first row and column where m is non-zero\n            first_row = 0\n            while first_row < m.shape[0] and not m[first_row, :].any():\n                first_row += 1\n            first_col = 0\n            while first_col < m.shape[1] and not m[:, first_col].any():\n                first_col += 1\n            # Find last row and column where m is non-zero\n            last_row = m.shape[0] - 1\n            while last_row >= 0 and not m[last_row, :].any():\n                last_row -= 1\n            last_col = m.shape[1] - 1\n            while last_col >= 0 and not m[:, last_col].any():\n                last_col -= 1\n            bboxes.append([first_row, last_row, first_col, last_col])\n        else:\n            bboxes.append([0, 0, 0, 0])\n    return bboxes\n\n", "CodeBleu_score": 20.89493364166392}, {"code": "def extract_bboxes(mask)\n\t\n    bboxes = []\n    for i in range(mask.shape[2]):\n        m = mask[:, :, i]\n        if not m.any():\n            bboxes.append([0, 0, 0, 0])\n            continue\n        y, x = m.nonzero()\n        yy = y.min(); yy = max(0, yy - 1)\n        xx = x.min(); xx = max(0, xx - 1)\n        yy2 = y.max(); yy2 = min(m.shape[0] - 1, yy2 + 1)\n        xx2 = x.max(); xx2 = min(m.shape[1] - 1, xx2 + 1)\n        bboxes.append([xx, yy, xx2, yy2])\n    return bboxes\n\n", "CodeBleu_score": 27.605376930861002}]}
{"instance_id": "7ad5d430d6ed", "generation_result": [{"code": "def compute_iou(box, boxes, box_area, boxes_area)\n\t\n    # determine the (x, y)-coordinates of the intersection rectangle\n    x = max(box[0], boxes[:, 0])\n    y = max(box[1], boxes[:, 1])\n    x2 = min(box[2], boxes[:, 2])\n    y2 = min(box[3], boxes[:, 3])\n\n    # compute the area of intersection rectangle\n    inter_area = max(0, x2 - x + 1) * max(0, y2 - y + 1)\n\n    # compute the area of both the prediction and ground-truth\n    # rectangles\n    box_area = box_area\n    boxes_area = boxes_area\n\n    # compute the intersection over union by taking the intersection\n    # area and dividing it by the sum of prediction + ground-truth\n    # areas - the interesection area\n    iou = inter_area / float(box_area + boxes_area - inter_area)\n\n    # return the intersection over union value\n    return iou\n\n", "CodeBleu_score": 25.168988308008}, {"code": "def compute_iou(box, boxes, box_area, boxes_area)\n\t\n    # determine the (x, y)-coordinates of the intersection rectangle\n    x = max(box[0], boxes[:, 0])\n    y = max(box[1], boxes[:, 1])\n    x2 = min(box[2], boxes[:, 2])\n    y2 = min(box[3], boxes[:, 3])\n\n    # compute the area of intersection rectangle\n    intersection_area = max(0, x2 - x + 1) * max(0, y2 - y + 1)\n\n    # compute the area of both the prediction and ground-truth rectangles\n    box_area = (box[2] - box[0] + 1) * (box[3] - box[1] + 1)\n    boxes_area = (boxes[:, 2] - boxes[:, 0] + 1) * (boxes[:, 3] - boxes[:, 1] + 1)\n\n    # compute the IoU\n    iou = intersection_area / (box_area + boxes_area - intersection_area)\n\n    return iou\n\n", "CodeBleu_score": 27.259077776340675}, {"code": "def compute_iou(box, boxes, box_area, boxes_area)\n\t\n    # determine the (x, y)-coordinates of the intersection rectangle\n    x = max(box[0], boxes[:, 0])\n    y = max(box[1], boxes[:, 1])\n    x2 = min(box[2], boxes[:, 2])\n    y2 = min(box[3], boxes[:, 3])\n    # compute the area of intersection rectangle\n    intersection_area = max(0, x2 - x + 1) * max(0, y2 - y + 1)\n    # compute the area of both the prediction and ground-truth rectangles\n    box_area = (box[2] - box[0] + 1) * (box[3] - box[1] + 1)\n    boxes_area = (boxes[:, 2] - boxes[:, 0] + 1) * (boxes[:, 3] - boxes[:, 1] + 1)\n    # compute the IoU\n    iou = intersection_area / (box_area + boxes_area - intersection_area)\n    return iou\n\n", "CodeBleu_score": 27.259077776340675}]}
{"instance_id": "15941b852859", "generation_result": [{"code": "def compute_overlaps(boxes1, boxes2)\n\t\n    # Compute areas of anchors and GT boxes\n    box_area = (boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1])\n    box_area = box_area[:, None]\n    area2 = (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1])\n    area2 = area2[None, :]\n    # Compute overlaps to generate matrix [boxes1 count] x [boxes2 count]\n    # Each cell contains the IoU value.\n    overlaps = scipy.zeros((boxes1.shape[0], boxes2.shape[0]))\n    for d in range(overlaps.shape[1]):\n        # Find intersections\n        # intersection_x1 = scipy.maximum(boxes1[:, 0][:, None], boxes2[d, 0][None, :])\n        # intersection_y1 = scipy.maximum(boxes1[:, 1][:, None], boxes2[d, 1][None, :])\n        # intersection_x2 = scipy.minimum(boxes1[:, 2][:, None], boxes2[d, 2][None, :])\n        # intersection_y2 = scipy.minimum(boxes1[:, 3][:, None], boxes2[d, 3][None, :])\n        # intersection = scipy.maximum(intersection_x2 - intersection_x1, 0.0)\n        # intersection = scipy.maximum(intersection_y2 - intersection_y1, 0.0)\n        # intersection = intersection[:, None]\n        # intersection = intersection[:, :, None]\n        #\n        # # Compute unions\n        # union = box_area[:, None] + area2[None, :] - intersection\n        # # Compute IoU and reshape [boxes1 count] x [boxes2 count]\n        # iou = intersection / union\n        # overlaps[:, d] = iou\n\n        # Compute over", "CodeBleu_score": 44.52277038713975}, {"code": "def compute_overlaps(boxes1, boxes2)\n\t\n    # Compute areas of anchors and GT boxes\n    areas1 = (boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1])\n    areas2 = (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1])\n\n    # Compute overlaps to generate matrix [boxes1 count] x [boxes2 count]\n    # Each cell contains the IoU value.\n    overlaps = scipy.zeros((boxes1.shape[0], boxes2.shape[0]))\n    for i in range(overlaps.shape[1]):\n        box2 = boxes2[i]\n        overlaps[:, i] = compute_iou(boxes1, box2, areas1, areas2)\n    return overlaps\n\n", "CodeBleu_score": 54.96573800279935}, {"code": "def compute_overlaps(boxes1, boxes2)\n\t\n    # Compute areas of anchors and GT boxes\n    box_area = (boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1])\n    area = (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1])\n    # Compute overlaps to generate matrix [boxes1 count] x [boxes2 count]\n    # Each cell contains the IoU value.\n    overlaps = scipy.zeros((boxes1.shape[0], boxes2.shape[0]))\n    for i in range(overlaps.shape[1]):\n        box2 = boxes2[i]\n        overlaps[:, i] = compute_iou(box_area, box2[0], box2[1], box2[2], box2[3], area)\n    return overlaps\n\n", "CodeBleu_score": 52.823171997905625}]}
{"instance_id": "597533722ce3", "generation_result": [{"code": "def load_image(self, image_id):\n\t\n    image_info = self.image_info[image_id]\n    image = skimage.io.imread(image_info[\"path\"])\n    if image.ndim != 3:\n        image = skimage.color.gray2rgb(image)\n    if image.shape[-1] == 4:\n        image = image[..., :3]\n    image = skimage.transform.resize(image, self.image_size + (image.ndim,))\n    return image\n\n", "CodeBleu_score": 75.73333787774536}, {"code": "def load_image(self, image_id):\n\t\n    image_info = self.image_info[image_id]\n    image = skimage.io.imread(image_info[\"path\"])\n    if image.ndim != 3:\n        image = skimage.color.gray2rgb(image)\n    if image.shape[-1] == 4:\n        image = image[..., :3]\n    image = skimage.img_as_float32(image)\n    return image\n", "CodeBleu_score": 77.06672692364141}, {"code": "def load_image(self, image_id):\n\t\n    image_info = self.image_info[image_id]\n    image = skimage.io.imread(image_info[\"path\"])\n    if image.ndim != 3:\n        raise ValueError(\"Image not of dimension 3\")\n    if image.shape[-1] != 3:\n        image = skimage.color.gray2rgb(image)\n    return image\n", "CodeBleu_score": 57.68769817672796}]}
{"instance_id": "9b852997ca05", "generation_result": [{"code": "def display_images(images, titles=None, cols=4, cmap=None, norm=None,\n                   interpolation=None)\n\t\n\n    assert ((titles is None) or (len(images) == len(titles))), \\\n        \"Number of titles is not equal to the number of images\"\n\n    rows = len(images) // cols + 1\n    fig, axes = plt.subplots(rows, cols)\n    fig.set_size_inches(18, 18)\n    for i in range(1, rows * cols):\n        ax = axes[i]\n        ax.set_axis_off()\n\n    for i in range(0, len(images)):\n        ax = axes[i]\n        ax.imshow(images[i], cmap=cmap, norm=norm, interpolation=interpolation)\n        if titles:\n            ax.set_title(titles[i])\n        ax.axis('off')\n\n", "CodeBleu_score": 39.19580652638832}, {"code": "def display_images(images, titles=None, cols=4, cmap=None, norm=None,\n                   interpolation=None)\n\t\n    rows = len(images) // cols + 1\n    plt.figure(figsize=(14, 14 * rows // cols))\n    i = 1\n    for image, title in zip(images, titles):\n        plt.subplot(rows, cols, i)\n        plt.title(title, fontsize=9)\n        plt.axis('off')\n        plt.imshow(image.astype(np.uint8), cmap=cmap,\n                   norm=norm, interpolation=interpolation)\n        i += 1\n    plt.show()\n\n", "CodeBleu_score": 65.35161349021817}, {"code": "def display_images(images, titles=None, cols=4, cmap=None, norm=None,\n                   interpolation=None)\n\t\n    titles = titles if titles is not None else [''] * len(images)\n    rows = len(images) // cols + 1\n    fig, axes = plt.subplots(rows, cols, figsize=(15, 15))\n    fig.subplots_adjust(hspace=0.3, wspace=0.05)\n    for i in range(len(images)):\n        axes[i // cols, i % cols].imshow(images[i], cmap=cmap,\n                                         norm=norm,\n                                         interpolation=interpolation)\n        axes[i // cols, i % cols].set_title(titles[i])\n        axes[i // cols, i % cols].axis('off')\n\n", "CodeBleu_score": 35.702799038989546}]}
