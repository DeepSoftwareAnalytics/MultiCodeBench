{"instance_id": "58936b2a621b", "generation_result": [{"code": "def set_model_params(model: LogisticRegression, params: NDArrays) -> LogisticRegression\n\t\n    model.coef_ = params[0]\n    if model.fit_intercept:\n        model.intercept_ = params[1]\n    return model\n\n", "CodeBleu_score": 83.87646776544821}, {"code": "def set_model_params(model: LogisticRegression, params: NDArrays) -> LogisticRegression\n\t\n    model.coef_ = params[0]\n    if model.fit_intercept:\n        model.intercept_ = params[1]\n    return model\n\n", "CodeBleu_score": 83.87646776544821}, {"code": "def set_model_params(model: LogisticRegression, params: NDArrays) -> LogisticRegression\n\t\n    model.coef_ = params[0]\n    if model.fit_intercept:\n        model.intercept_ = params[1]\n    return model\n\n", "CodeBleu_score": 83.87646776544821}]}
{"instance_id": "83b142d9d251", "generation_result": [{"code": "def support_query_split(\n    data,\n    label,\n    support_ratio: float,\n) -> Tuple[List, List, List, List]\n\t\n    x_train, x_test, y_train, y_test = train_test_split(\n        data,\n        label,\n        test_size=1 - support_ratio,\n        random_state=42,\n        stratify=label,\n    )\n    return x_train, x_test, y_train, y_test\n\n", "CodeBleu_score": 65.30740775035834}, {"code": "def support_query_split(\n    data,\n    label,\n    support_ratio: float,\n) -> Tuple[List, List, List, List]\n\t\n    x_train, x_test, y_train, y_test = train_test_split(\n        data,\n        label,\n        test_size=1 - support_ratio,\n        random_state=42,\n        stratify=label,\n    )\n    return x_train, x_test, y_train, y_test\n\n", "CodeBleu_score": 65.30740775035834}, {"code": "def support_query_split(\n    data,\n    label,\n    support_ratio: float,\n) -> Tuple[List, List, List, List]\n\t\n\n    x_train, x_test, y_train, y_test = train_test_split(\n        data,\n        label,\n        train_size=support_ratio,\n        random_state=42,\n        stratify=label,\n    )\n    return x_train, x_test, y_train, y_test\n\n", "CodeBleu_score": 67.31491587948683}]}
{"instance_id": "5fd02b540f61", "generation_result": [{"code": "def load_data() -> (\n    Tuple[List[np.ndarray], List[np.ndarray], List[np.ndarray], List[np.ndarray]]\n)\n\t\n    X, y = make_regression(n_samples=1000, n_features=3, random_state=42)\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=42\n    )\n    return X_train, y_train, X_test, y_test\n\n", "CodeBleu_score": 38.92483101817302}, {"code": "def load_data() -> (\n    Tuple[List[np.ndarray], List[np.ndarray], List[np.ndarray], List[np.ndarray]]\n)\n\t\n    X, y = make_regression(n_samples=100, n_features=3, random_state=42)\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=42\n    )\n    return X_train, y_train, X_test, y_test\n\n", "CodeBleu_score": 38.988730693271556}, {"code": "def load_data() -> (\n    Tuple[List[np.ndarray], List[np.ndarray], List[np.ndarray], List[np.ndarray]]\n)\n\t\n    X, y = make_regression(n_samples=1000, n_features=3, random_state=42)\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=42\n    )\n    return X_train, y_train, X_test, y_test\n\n", "CodeBleu_score": 38.92483101817302}]}
{"instance_id": "ad958cb31a57", "generation_result": [{"code": "def ndarrays_to_gmmparam(ndarrays: NDArray) -> GMMParameters\n\t\n    label = ndarrays[0]\n    means = ndarrays[1]\n    weights = ndarrays[2]\n    covariances = ndarrays[3]\n    num_samples = ndarrays[4]\n    return GMMParameters(label, means, weights, covariances, num_samples)\n\n", "CodeBleu_score": 40.08951228295815}, {"code": "def ndarrays_to_gmmparam(ndarrays: NDArray) -> GMMParameters\n\t\n    label = ndarrays[0]\n    means = ndarrays[1]\n    weights = ndarrays[2]\n    covariances = ndarrays[3]\n    num_samples = ndarrays[4]\n    return GMMParameters(label, means, weights, covariances, num_samples)\n\n", "CodeBleu_score": 40.08951228295815}, {"code": "def ndarrays_to_gmmparam(ndarrays: NDArray) -> GMMParameters\n\t\n    label = ndarrays[0]\n    means = ndarrays[1]\n    weights = ndarrays[2]\n    covariances = ndarrays[3]\n    num_samples = ndarrays[4]\n    return GMMParameters(label, means, weights, covariances, num_samples)\n\n", "CodeBleu_score": 40.08951228295815}]}
{"instance_id": "061d5a5b9d56", "generation_result": [{"code": "def datafiles_fusion(data_paths):\n\t\n    X, Y = load_svmlight_file(data_paths[0])\n    X = X.toarray()\n    for path in data_paths[1:]:\n        X_i, Y_i = load_svmlight_file(path)\n        assert X_i.shape[1] == X.shape[1]\n        X = np.concatenate([X, X_i.toarray()], axis=0)\n        Y = np.concatenate([Y, Y_i], axis=0)\n    return X, Y\n\n", "CodeBleu_score": 34.57813826372183}, {"code": "def datafiles_fusion(data_paths):\n\t\n    X, Y = load_svmlight_file(data_paths[0])\n    X = X.toarray()\n    for i in range(1, len(data_paths)):\n        X_new, Y_new = load_svmlight_file(data_paths[i])\n        X_new = X_new.toarray()\n        assert X.shape[1] == X_new.shape[1]\n        X = np.concatenate((X, X_new), axis=0)\n        Y = np.concatenate((Y, Y_new), axis=0)\n    return X, Y\n\n", "CodeBleu_score": 43.18354795510007}, {"code": "def datafiles_fusion(data_paths):\n\t\n    X, y = load_svmlight_file(data_paths[0])\n    X = X.toarray()\n    for path in data_paths[1:]:\n        X_new, y_new = load_svmlight_file(path)\n        X_new = X_new.toarray()\n        if X_new.shape[1] != X.shape[1]:\n            raise ValueError(\"Number of features in all files must match.\")\n        X = np.concatenate((X, X_new), axis=0)\n        y = np.concatenate((y, y_new), axis=0)\n    return X, y\n\n", "CodeBleu_score": 33.168152317854485}]}
{"instance_id": "977d3bd5d5a3", "generation_result": [{"code": "def eval_learning(y_test, y_pred)\n\t\n    accuracy = accuracy_score(y_test, y_pred)\n    recall = recall_score(y_test, y_pred, average='micro')\n    precision = precision_score(y_test, y_pred, average='micro')\n    f1 = f1_score(y_test, y_pred, average='micro')\n\n    return accuracy, recall, precision, f1\n\n", "CodeBleu_score": 33.42910286151195}, {"code": "def eval_learning(y_test, y_pred)\n\t\n    accuracy = accuracy_score(y_test, y_pred)\n    recall = recall_score(y_test, y_pred, average='micro')\n    precision = precision_score(y_test, y_pred, average='micro')\n    f1 = f1_score(y_test, y_pred, average='micro')\n\n    return accuracy, recall, precision, f1\n\n", "CodeBleu_score": 33.42910286151195}, {"code": "def eval_learning(y_test, y_pred)\n\t\n    acc = accuracy_score(y_test, y_pred)\n    rec = recall_score(y_test, y_pred, average=\"micro\")\n    prec = precision_score(y_test, y_pred, average=\"micro\")\n    f1 = f1_score(y_test, y_pred, average=\"micro\")\n    return acc, rec, prec, f1\n\n", "CodeBleu_score": 48.36297107598011}]}
{"instance_id": "f4c838f1f092", "generation_result": [{"code": "def train(model, train_loader, num_epochs=1)\n\t\n    criterion = torch.nn.BCELoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n    model.train()\n    for epoch in range(num_epochs):\n        for batch_idx, (data, target) in enumerate(train_loader):\n            optimizer.zero_grad()\n            output = model(data)\n            loss = criterion(output, target.unsqueeze(1))\n            loss.backward()\n            optimizer.step()\n\n", "CodeBleu_score": 50.603663275265085}, {"code": "def train(model, train_loader, num_epochs=1)\n\t\n    criterion = torch.nn.BCELoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n    model.train()\n    for epoch in range(num_epochs):\n        for batch_idx, (data, target) in enumerate(train_loader):\n            optimizer.zero_grad()\n            output = model(data)\n            loss = criterion(output, target.float().view(-1, 1))\n            loss.backward()\n            optimizer.step()\n\n", "CodeBleu_score": 51.20161033032922}, {"code": "def train(model, train_loader, num_epochs=1)\n\t\n    criterion = torch.nn.BCELoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n    model.train()\n    for epoch in range(num_epochs):\n        for batch_idx, (data, target) in enumerate(train_loader):\n            optimizer.zero_grad()\n            output = model(data)\n            loss = criterion(output, target.unsqueeze(1))\n            loss.backward()\n            optimizer.step()\n\n", "CodeBleu_score": 50.603663275265085}]}
{"instance_id": "35068c779c02", "generation_result": [{"code": "def _load_libsvm_dataset(cfg: DictConfig) -> Dataset\n\t\n    assert cfg.dataset.type == DatasetType.LIBSVM\n    path = train_dataset_path(cfg)\n    name = cfg.dataset.name\n    data, labels = load_svmlight_file(path)\n    data = data.todense().astype(\"float32\")\n    labels = labels.astype(\"float32\")\n    print(f\"Unique labels with counts: {dict(zip(*np.unique(labels, return_counts=True)))}\")\n    print(f\"Features shape: {data.shape}\")\n    if name == \"mushrooms\":\n        labels[labels == 1] = 0\n        labels[labels != 0] = 1\n    else:\n        raise RuntimeError(f\"Dataset name {name} is not supported\")\n    return torch.utils.data.TensorDataset(torch.from_numpy(data), torch.from_numpy(labels))\n\n", "CodeBleu_score": 31.5483466420213}, {"code": "def _load_libsvm_dataset(cfg: DictConfig) -> Dataset\n\t\n    if cfg.dataset.type != DatasetType.LIBSVM:\n        raise ValueError(f\"Invalid dataset type: {cfg.dataset.type}\")\n    path = train_dataset_path(cfg)\n    name = cfg.dataset.name\n    data, labels = load_svmlight_file(path)\n    data = data.todense().astype(\"float32\")\n    labels = labels.astype(\"float32\")\n    print(f\"Unique labels: {np.unique(labels, return_counts=True)}\")\n    print(f\"Features shape: {data.shape}\")\n    if name == \"MUSHROOMS\":\n        labels = np.where(labels == 1, 0, 1)\n    else:\n        raise RuntimeError(f\"Unknown dataset name: {name}\")\n    return torch.utils.data.TensorDataset(torch.from_numpy(data), torch.from_numpy(labels))\n\n", "CodeBleu_score": 35.74153947749251}, {"code": "def _load_libsvm_dataset(cfg: DictConfig) -> Dataset\n\t\n    if cfg.dataset.type != DatasetType.LIBSVM:\n        raise RuntimeError(\n            f\"Expected LIBSVM dataset, got {cfg.dataset.type}\"\n        )\n    dataset_path = train_dataset_path(cfg)\n    dataset_name = cfg.dataset.name\n    data, labels = load_svmlight_file(dataset_path, n_features=cfg.dataset.n_features)\n    data = data.todense().astype(\"float32\")\n    labels = labels.astype(\"float32\")\n    print(f\"Unique labels: {torch.unique(labels, return_counts=True)}\")\n    print(f\"Features shape: {data.shape}\")\n    if dataset_name == \"mushrooms\":\n        labels[labels == 1] = 0\n        labels[labels != 0] = 1\n    else:\n        raise RuntimeError(f\"Unknown dataset {dataset_name}\")\n    return torch.utils.data.TensorDataset(torch.from_numpy(data), torch.from_numpy(labels))\n\n", "CodeBleu_score": 42.146040781733454}]}
{"instance_id": "d602a96e571e", "generation_result": [{"code": "def _fit_transform_one(transformer, X, y, weight, **fit_params)\n\t\n    if hasattr(transformer, \"fit_transform\"):\n        res = transformer.fit_transform(X, y, **fit_params)\n    else:\n        res = transformer.fit(X, y, **fit_params).transform(X)\n    if weight is None:\n        return res, transformer\n    return res * weight, transformer\n\n", "CodeBleu_score": 88.84159847409744}, {"code": "def _fit_transform_one(transformer, X, y, weight, **fit_params)\n\t\n    if hasattr(transformer, \"fit_transform\"):\n        res = transformer.fit_transform(X, y, **fit_params)\n    else:\n        res = transformer.fit(X, y, **fit_params).transform(X)\n    if weight is None:\n        return res, transformer\n    return res * weight, transformer\n\n", "CodeBleu_score": 88.84159847409744}, {"code": "def _fit_transform_one(transformer, X, y, weight, **fit_params)\n\t\n    if hasattr(transformer, \"fit_transform\"):\n        res = transformer.fit_transform(X, y, **fit_params)\n    else:\n        res = transformer.fit(X, y, **fit_params).transform(X)\n    if weight is None:\n        return res, transformer\n    return res * weight, transformer\n\n", "CodeBleu_score": 88.84159847409744}]}
{"instance_id": "fcf88a773fd8", "generation_result": [{"code": "def _transform_selected(X, transform, selected=\"all\", copy=True)\n\t\n    X = check_array(X, accept_sparse=\"csc\", copy=copy, dtype=FLOAT_DTYPES)\n\n    if isinstance(transform, six.string_types):\n        if transform == \"drop\":\n            return _transform_drop_deprecated(X, selected)\n        else:\n            raise ValueError(\"Invalid transform '{0}'. \".format(transform))\n\n    if selected == \"all\":\n        return transform.transform(X)\n\n    if len(selected) == 0:\n        return X\n\n    n_features = X.shape[1]\n    ind = np.arange(n_features)\n    sel = np.zeros(n_features, dtype=bool)\n    sel[np.asarray(selected)] = True\n    not_sel = np.logical_not(sel)\n    n_selected = np.sum(sel)\n\n    if n_selected == 0:\n        # No features selected.\n        return X\n    elif n_selected == n_features:\n        # All features selected.\n        return transform.transform(X)\n    else:\n        X_sel = transform.transform(X[:, ind[sel]])\n        X_not_sel = X[:, ind[not_sel]]\n\n        if sparse.issparse(X_sel) or sparse.issparse(X_not_sel):\n            return sparse.hstack((X_sel, X_not_sel))\n        else:\n            return np.hstack((X_sel, X_not_sel))\n\n", "CodeBleu_score": 71.66665244247633}, {"code": "def _transform_selected(X, transform, selected=\"all\", copy=True)\n\t\n    X = check_array(X, accept_sparse=\"csc\", copy=copy, dtype=FLOAT_DTYPES)\n\n    if selected == \"all\":\n        return transform(X)\n\n    if len(selected) == 0:\n        return X\n\n    n_features = X.shape[1]\n    ind = np.arange(n_features)\n    sel = np.zeros(n_features, dtype=bool)\n    sel[np.asarray(selected)] = True\n    not_sel = np.logical_not(sel)\n    n_selected = np.sum(sel)\n\n    if n_selected == 0:\n        # No features selected.\n        return X\n    elif n_selected == n_features:\n        # All features selected.\n        return transform(X)\n    else:\n        X_sel = transform(X[:, ind[sel]])\n        X_not_sel = X[:, ind[not_sel]]\n\n        if sparse.issparse(X_sel) or sparse.issparse(X_not_sel):\n            return sparse.hstack((X_sel, X_not_sel))\n        else:\n            return np.hstack((X_sel, X_not_sel))\n\n", "CodeBleu_score": 79.45436857360559}, {"code": "def _transform_selected(X, transform, selected=\"all\", copy=True)\n\t\n    X = check_array(X, accept_sparse='csc', copy=copy,\n                     dtype=FLOAT_DTYPES)\n\n    if isinstance(transform, six.string_types):\n        if transform == 'drop':\n            return _transform_drop_deprecated(X, selected)\n        else:\n            raise ValueError(\"Invalid parameter transform: %s\" % transform)\n\n    if selected == \"all\":\n        X = transform.transform(X)\n    elif len(selected) == 0:\n        return X\n    else:\n        if isinstance(transform, _BaseComposition):\n            transform._validate_transformers()\n\n        transformer_list = _as_transformer_list(transform)\n        Xs = [transformer.transform(X[:, selected])\n              for transformer in transformer_list]\n        Xs = sparse.hstack(Xs).tocsr()\n        Xs.sort_indices()\n        return _hstack([X, Xs])\n\n    return X\n\n", "CodeBleu_score": 39.40769973482022}]}
{"instance_id": "c9dfb00047d1", "generation_result": [{"code": "def _transform_new(self, X)\n\t\n    X = check_array(X, accept_sparse='csc', dtype=None)\n    n_samples, n_features = X.shape\n    X_int, mask = _encode_categorical(X)\n    n_categories = []\n    for j in range(n_features):\n        mask_j = mask[:, j].toarray().ravel()\n        n_categories.append(len(np.unique(X_int[mask_j, j])))\n    indices = _transform_selected(X_int, self.categorical_features,\n                                  mask, self.categories)\n    indptr = np.hstack([[0], np.cumsum(n_categories)])\n    data = np.empty(len(indices), dtype=self.dtype)\n    data.fill(1)\n    X_trans = sparse.csr_matrix((data, indices, indptr),\n                                shape=(n_samples, sum(n_categories)),\n                                dtype=self.dtype)\n    if not self.sparse:\n        return X_trans.toarray()\n    else:\n        return X_trans\n\n", "CodeBleu_score": 39.45219218121881}, {"code": "def _transform_new(self, X)\n\t\n    X = check_array(X, accept_sparse='csc', dtype=None)\n    n_samples, n_features = X.shape\n    X_int, mask = _transform_selected(X, self._transformers,\n                                      dtype=X.dtype)\n    X_masked = X_int[mask]\n    n_values = [c.categories_.shape[0] for c in self._transformers]\n    indices = _transform_feature_indices(n_features, n_values)\n    indptr = np.hstack((0, np.cumsum(n_values)))\n    data = np.empty(X_masked.shape[0], dtype=self.dtype)\n    data.fill(1)\n    X_trans = sparse.csr_matrix((data, X_masked, indices),\n                                shape=(n_samples, indices[-1]),\n                                dtype=self.dtype)\n    if not self.sparse:\n        X_trans = X_trans.toarray()\n    return X_trans\n\n", "CodeBleu_score": 28.063191625240503}, {"code": "def _transform_new(self, X)\n\t\n    X = check_array(X, accept_sparse='csc', dtype=None)\n    n_samples, n_features = X.shape\n    X_int, mask = _encode(X)\n    n_values = []\n    for feature in range(n_features):\n        n_values.append(len(self.categories_[feature]))\n    n_values = np.array(n_values, dtype=np.intp)\n    indptr = np.zeros(n_features + 1, dtype=np.intp)\n    indices = np.zeros(n_samples, dtype=np.intp)\n    data = np.empty(n_samples, dtype=np.intp)\n    _transform_feature_loop(n_samples, n_features, X_int, X_mask,\n                            self.categories_, n_values, indptr, indices,\n                            data)\n    indptr[-1] += n_samples\n    if not np.all(mask):\n        mask = np.logical_and(~mask, X_mask)\n        n_mask = np.sum(mask)\n        indices = indices[-n_mask:]\n        indptr[-1] -= n_mask\n        if data.size > 0:\n            data = data[-n_mask:]\n    from scipy import sparse\n    A = sparse.csr_matrix((data, indices, indptr),\n                          shape=(n_samples, np.sum(n_values)),\n                          dtype=self.dtype)\n    if not self.sparse:\n        A = A.toarray()\n    return A\n\n", "CodeBleu_score": 40.10614553050332}]}
{"instance_id": "fcde391a8972", "generation_result": [{"code": "def convert_to_sklearn(self)\n\t\n    if self.log:\n        return loguniform(self.lower, self.upper)\n    else:\n        return uniform(self.lower, self.upper - self.lower)", "CodeBleu_score": 30.997576913458495}, {"code": "def convert_to_sklearn(self)\n\t\n    if self.log:\n        return scipy.stats.loguniform(self.lower, self.upper)\n    else:\n        return scipy.stats.uniform(self.lower, self.upper - self.lower)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 28.452510569922364}, {"code": "def convert_to_sklearn(self)\n\t\n    if self.log:\n        return scipy.stats.loguniform(self.lower, self.upper)\n    return scipy.stats.uniform(self.lower, self.upper)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 26.68546677519711}]}
{"instance_id": "5d5413adcca6", "generation_result": [{"code": "def _render(self, state: AnalysisState) -> None\n\t\n    additional_args = state.additional_args\n    figargs = state.figargs\n    features = state.features\n    if state.headers:\n        render_header(state, \"Partial Dependence Plots\")\n    fig, axes = plt.subplots(len(features), 1, **figargs)\n    if len(features) == 1:\n        axes = [axes]\n    kwargs = {**additional_args, **{\"ax\": axes}}\n    if state.interaction_only:\n        data = state.data[state.features].dropna()\n        if len(data) < len(state.data):\n            warnings.warn(\n                f\"Only {len(data)} samples out of {len(state.data)} were used for plotting due to null values in the dataset. Use `interaction_only=False` to plot all samples.\"\n            )\n    else:\n        data = state.data[state.features]\n    PartialDependenceDisplay.from_estimator(state.model, data, features, **kwargs)\n    plt.tight_layout()\n    plt.show()\n\n", "CodeBleu_score": 43.860392720210704}, {"code": "def _render(self, state: AnalysisState) -> None\n\t\n    additional_args = state.additional_args\n    figure_args = state.figure_args\n    features = state.features\n    if state.headers:\n        render_header(state, \"Partial Dependence Plots\")\n    fig, axes = plt.subplots(\n        nrows=len(features), ncols=len(features), **figure_args\n    )\n    kwargs = {**additional_args, **{\"ax\": axes, \"line_kw\": {\"color\": \"#000000\"}}}\n    if state.interaction:\n        data = state.data.dropna(subset=features)\n    else:\n        data = state.data\n    PartialDependenceDisplay.from_estimator(\n        state.model, data, features=features, **kwargs\n    )\n    fig.tight_layout()\n    plt.show()\n\n", "CodeBleu_score": 44.61085849621348}, {"code": "def _render(self, state: AnalysisState) -> None\n\t\n    additional_args = state.additional_args\n    figure_args = state.figure_args\n    features = state.features\n    if state.headers:\n        self.render_header(state)\n    fig, ax = plt.subplots(**figure_args)\n    kwargs = {**additional_args, **{\"ax\": ax}}\n    if state.interaction_features is not None:\n        data = state.data.dropna(subset=state.interaction_features)\n    else:\n        data = state.data\n    PartialDependenceDisplay.from_estimator(state.model, data, features, **kwargs)\n    plt.tight_layout()\n    plt.show()\n\n", "CodeBleu_score": 38.803465948756234}]}
{"instance_id": "db8aac29180b", "generation_result": [{"code": "def _preprocess(self, X, **kwargs)\n\t\n    X = super()._preprocess(X, **kwargs)\n    if self.feature_generator is None:\n        self.feature_generator = LabelEncoderFeatureGenerator(verbosity=0)\n        self.feature_generator.fit(X)\n    if self.feature_generator.feature_metadata_in is not None:\n        X_transformed = self.feature_generator.transform(X)\n        X_transformed.fillna(0, inplace=True)\n        X = X_transformed.to_numpy(dtype=\"float32\")\n    else:\n        X.fillna(0, inplace=True)\n        X = X.to_numpy(dtype=\"float32\")\n    return X\n\n", "CodeBleu_score": 40.52797999386164}, {"code": "def _preprocess(self, X, **kwargs)\n\t\n    X = super()._preprocess(X, **kwargs)\n    if self.feature_generator is None:\n        self.feature_generator = LabelEncoderFeatureGenerator(verbosity=0)\n        self.feature_generator.fit(X)\n    if self.feature_generator.feature_metadata_in is not None:\n        X_feature_gen = X.copy()\n        X_feature_gen[self.feature_generator.feature_metadata_in.get_features()] = self.feature_generator.transform(X)\n        X = X_feature_gen\n    X = X.fillna(0)\n    return X.values.astype(\"float32\")\n\n", "CodeBleu_score": 43.324076075870686}, {"code": "def _preprocess(self, X, **kwargs)\n\t\n    X = super()._preprocess(X, **kwargs)\n    if self.feature_generator is None:\n        self.feature_generator = LabelEncoderFeatureGenerator(verbosity=0)\n        self.feature_generator.fit(X)\n    if self.feature_generator.identified_features:\n        X = X.copy()\n        X[self.feature_generator.identified_features] = self.feature_generator.transform(\n            X[self.feature_generator.identified_features]\n        )\n    X = X.fillna(0)\n    X = X.to_numpy(dtype=\"float32\")\n    return X\n\n", "CodeBleu_score": 47.85982283348587}]}
{"instance_id": "d4eb45e6d013", "generation_result": [{"code": "def _estimate_memory_usage(self, X, **kwargs)\n\t\n    if self.model is None:\n        return None\n    n_jobs = self.model.n_jobs\n    n_estimators = self.model.n_estimators\n    if n_estimators is None:\n        n_estimators = self.model.get_params()[\"estimator__n_estimators\"]\n    if n_estimators < 40 or self.model.get_params().get(\"estimator__n_iter\") is not None:\n        n_estimators = 40\n    if self.model.get_params().get(\"estimator__n_iter\") is not None:\n        n_estimators = 40\n    n_trees_per_estimator = self.model.get_params().get(\"estimator__n_estimators\", 1)\n    bytes_per_estimator = self.model.get_params().get(\"estimator__max_depth\", 1) * 3 * X.shape[1] + 2000000\n    expected_min_memory_usage = n_estimators * bytes_per_estimator\n    return expected_min_memory_usage\n\n", "CodeBleu_score": 38.058122755908876}, {"code": "def _estimate_memory_usage(self, X, **kwargs)\n\t\n    if self.model is None:\n        raise ValueError(\"Model has not been trained yet.\")\n    n_jobs = self.model.n_jobs\n    if n_jobs is None:\n        n_jobs = -1\n    if n_jobs > 0:\n        n_jobs = min(n_jobs, os.cpu_count())\n    else:\n        n_jobs = os.cpu_count()\n    n_estimators = self.model.n_estimators\n    if n_estimators is None:\n        n_estimators = self.model.get_params().get(\"n_estimators\", 100)\n    if self.model.n_estimators is None:\n        self.model.n_estimators = n_estimators\n    if self.model.n_estimators < 40 or self.model.get_params().get(\"search_spaces\") is not None:\n        n_estimators = max(n_estimators, 40)\n    n_trees_per_estimator = self.model.n_estimators // n_estimators\n    bytes_per_estimator = self.model.get_booster().estimate_size() * n_trees_per_estimator\n    adjust_factor = 1.5\n    expected_min_mem_usage = bytes_per_estimator * n_estimators * adjust_factor\n    return expected_min_mem_usage\n\n", "CodeBleu_score": 37.77348073164712}, {"code": "def _estimate_memory_usage(self, X, **kwargs)\n\t\n    num_classes = self.params.get(\"num_classes\", 1)\n    n_jobs = self.params.get(\"n_jobs\", 1)\n    max_depth = self.params.get(\"max_depth\", 10)\n    num_leaves = self.params.get(\"num_leaves\", 31)\n    min_child_samples = self.params.get(\"min_child_samples\", 20)\n    subsample = self.params.get(\"subsample\", 1.0)\n    subsample_freq = self.params.get(\"subsample_freq\", 0)\n    colsample_bytree = self.params.get(\"colsample_bytree\", 1.0)\n    num_boost_round = self.params.get(\"num_boost_round\", 100)\n    num_threads = self.params.get(\"num_threads\", 1)\n    early_stopping_rounds = self.params.get(\"early_stopping_rounds\", None)\n    min_data_in_leaf = self.params.get(\"min_data_in_leaf\", 20)\n    max_bin = self.params.get(\"max_bin\", 255)\n    boosting_type = self.params.get(\"boosting_type\", \"gbdt\")\n    n_estimators = self.params.get(\"n_estimators\", 100)\n    search_spaces = self.params.get(\"search_spaces\", None)\n\n    if search_spaces is not None:\n        n_estimators = 40\n    elif n_estimators < 40:\n        n_estimators = 40\n\n    if boosting_type == \"gbdt\":\n        num_trees_per_estimator = math.ceil(num_leaves / 3)\n    elif boosting_type == \"rf\":\n        num_trees_per_estimator = 1\n    else:\n        raise ValueError(f\"Invalid boosting_type: {boosting_type}\")\n\n    num_estimators = math.ceil(n_estimators / num_trees_per_estimator)", "CodeBleu_score": 25.21105781033668}]}
{"instance_id": "6923cb007f4c", "generation_result": [{"code": "def downscale_vectorizer(vectorizer, ngram_freq, vocab_size)\n\t\n\n    # Create a Counter object from the n-gram frequency data\n    ngram_counter = Counter(ngram_freq)\n\n    # Identify the top n most common n-grams\n    top_ngrams = ngram_counter.most_common(vocab_size)\n\n    # Sort the names of the top n n-grams\n    top_ngram_names = [ngram[0] for ngram in top_ngrams]\n    top_ngram_names.sort()\n\n    # Create a new vocabulary dictionary with the downsized vocabulary\n    new_vocab = {ngram: i for i, ngram in enumerate(top_ngram_names)}\n\n    # Update the vectorizer's vocabulary with the new, downsized vocabulary\n    vectorizer.vocabulary_ = new_vocab\n\n", "CodeBleu_score": 32.44633768959788}, {"code": "def downscale_vectorizer(vectorizer, ngram_freq, vocab_size)\n\t\n    # Create a Counter object from the n-gram frequency data\n    ngram_counter = Counter(ngram_freq)\n\n    # Identify the top n most common n-grams\n    top_ngrams = ngram_counter.most_common(vocab_size)\n\n    # Sort the names of the top n n-grams\n    top_ngram_names = [ngram[0] for ngram in top_ngrams]\n    top_ngram_names.sort()\n\n    # Create a new vocabulary dictionary with the top n n-grams\n    new_vocab = {ngram: i for i, ngram in enumerate(top_ngram_names)}\n\n    # Update the vectorizer's vocabulary with the new vocabulary\n    vectorizer.vocabulary_ = new_vocab\n\n", "CodeBleu_score": 32.44633768959788}, {"code": "def downscale_vectorizer(vectorizer, ngram_freq, vocab_size)\n\t\n    ngram_counter = Counter(ngram_freq)\n    top_ngrams = [ngram for ngram, count in ngram_counter.most_common(vocab_size)]\n    top_ngrams.sort()\n    new_vocab = {ngram: i for i, ngram in enumerate(top_ngrams)}\n    vectorizer.vocabulary_ = new_vocab\n\n", "CodeBleu_score": 34.41506915015346}]}
{"instance_id": "0e837b2d8088", "generation_result": [{"code": " def _select_continuous(self, df, features)\n\t\n    continuous_features = []\n    skewed_features = []\n    skewness_threshold = self.params.get(\"skewness_threshold\", 0.95)\n    for feature in features:\n        skewness = df[feature].skew()\n        if abs(skewness) > skewness_threshold:\n            skewed_features.append(feature)\n        else:\n            continuous_features.append(feature)\n    return {\"continuous\": continuous_features, \"skewed\": skewed_features}\n\n", "CodeBleu_score": 42.97825191016125}, {"code": " def _select_continuous(self, df, features)\n\t\n    feature_types = defaultdict(list)\n    skewness_threshold = self.params.get('skewness_threshold', 1.0)\n    for feature in features:\n        skewness = df[feature].skew()\n        if abs(skewness) > skewness_threshold:\n            feature_types['skewed'].append(feature)\n        else:\n            feature_types['continuous'].append(feature)\n    return feature_types\n\n", "CodeBleu_score": 44.88520687347367}, {"code": " def _select_continuous(self, df, features)\n\t\n    feature_types = defaultdict(list)\n    skewness_threshold = self.params.get(\"skewness_threshold\", 1.0)\n    for feature in features:\n        skewness = df[feature].skew()\n        if abs(skewness) > skewness_threshold:\n            feature_types[\"skewed\"].append(feature)\n        else:\n            feature_types[\"continuous\"].append(feature)\n    return feature_types\n\n", "CodeBleu_score": 45.5591764627548}]}
{"instance_id": "bb569c10f975", "generation_result": [{"code": "def _get_weights(dist, weights):\n\t\n    if weights is None or weights == 'uniform':\n        return None\n    elif weights == 'distance':\n        dist = dist.copy()\n        inf_mask = dist == float('inf')\n        n_inf = inf_mask.sum()\n        if n_inf > 0:\n            dist[inf_mask] = dist[~inf_mask].max()\n        dist = 1 / dist\n        if n_inf > 0:\n            dist[inf_mask] = 0\n        return dist\n    elif callable(weights):\n        return weights(dist)\n    else:\n        raise ValueError(\"weights should be 'uniform', 'distance', or a callable function\")\n\n", "CodeBleu_score": 55.50026907926464}, {"code": "def _get_weights(dist, weights):\n\t\n    if weights is None or weights == \"uniform\":\n        return None\n    elif weights == \"distance\":\n        dist = dist.copy()\n        inf_mask = dist == float(\"inf\")\n        ninf_mask = dist == float(\"-inf\")\n        dist[inf_mask] = dist[~inf_mask].max()\n        dist[ninf_mask] = dist[~ninf_mask].min()\n        dist = 1 / dist\n        dist[inf_mask] = 0\n        return dist\n    elif callable(weights):\n        return weights(dist)\n    else:\n        raise ValueError(\n            f\"weights='{weights}' is not supported. weights should be one of [None, 'uniform', 'distance'] or a callable function\"\n        )\n\n", "CodeBleu_score": 48.053595092954104}, {"code": "def _get_weights(dist, weights):\n\t\n    if weights is None or weights == \"uniform\":\n        return None\n    elif weights == \"distance\":\n        if dist.dtype.kind == 'f':\n            # boolean mask of finite values of inverse of distances\n            inv_dist = 1 / dist\n            inv_dist[~np.isfinite(inv_dist)] = 0.0\n            return inv_dist\n        else:\n            raise ValueError(\"distance weights are not supported for sparse data\")\n    elif callable(weights):\n        return weights(dist)\n    else:\n        raise ValueError(\"weights should be 'uniform', 'distance', or a callable function\")\n\n", "CodeBleu_score": 44.163559672510964}]}
{"instance_id": "12aaa9c78402", "generation_result": [{"code": "def predict_loo(self)\n\t\n    logging.info(\"Performing leave-one-out (LOO) prediction using k-nearest neighbors.\")\n    try:\n        distances, indices = self.kneighbors(self._X)\n        weights = _get_weights(distances, self.weights)\n        if self._y.ndim == 1:\n            _y = self._y.reshape((-1, 1))\n        if self.weights == 'uniform':\n            y_pred = _y[indices][:, 1:].mean(axis=1)\n        else:\n            y_pred = np.zeros(self._y.shape[0])\n            for i in range(self._y.shape[0]):\n                y_pred[i] = np.dot(weights[i], _y[indices[i]][1:]) / weights[i].sum()\n        if self._y.ndim == 1:\n            y_pred = y_pred.flatten()\n        return y_pred\n    except Exception as e:\n        logging.error(f\"Error occurred during leave-one-out (LOO) prediction: {str(e)}\")\n        raise\n\n", "CodeBleu_score": 42.511545077588956}, {"code": "def predict_loo(self)\n\t\n    try:\n        distances, indices = self.kneighbors(self._fit_X)\n        weights = _get_weights(distances, self.weights)\n        if self._y.ndim == 1:\n            _y = self._y.reshape((-1, 1))\n        pred_labels = np.empty(\n            (self._fit_X.shape[0], self._y.shape[1]), dtype=self._y.dtype\n        )\n        for i, idx in enumerate(indices):\n            if weights is None:\n                pred_labels[i] = np.mean(_y[idx], axis=0)\n            else:\n                pred_labels[i] = np.sum(_y[idx] * weights[i], axis=0) / np.sum(weights[i])\n        if self._y.ndim == 1:\n            pred_labels = pred_labels.ravel()\n        return pred_labels\n    except Exception as e:\n        logging.error(f\"Error in predict_loo: {str(e)}\")\n        raise\n\n", "CodeBleu_score": 42.333023065716155}, {"code": "def predict_loo(self)\n\t\n    try:\n        distances, indices = self.kneighbors()\n        weights = _get_weights(distances, self.weights)\n        if self._y.ndim == 1:\n            _y = self._y.reshape((-1, 1))\n        pred_labels = np.empty((self._y.shape[0], self._y.shape[1]), dtype=self._y.dtype)\n        for i in range(self._y.shape[0]):\n            n_idx = indices[i]\n            if weights is None:\n                w = np.ones(n_idx.shape)\n            else:\n                w = weights[i, n_idx]\n            if self._y.ndim == 1:\n                pred_labels[i] = np.average(self._y[n_idx], weights=w, axis=0)\n            else:\n                pred_labels[i] = np.average(self._y[n_idx], weights=w, axis=0)\n        if self._y.ndim == 1:\n            pred_labels = pred_labels.ravel()\n        return pred_labels\n    except Exception as e:\n        logging.error(f\"Error in predict_loo: {str(e)}\")\n        raise\n\n", "CodeBleu_score": 40.59211357471087}]}
{"instance_id": "b316687d9e9b", "generation_result": [{"code": "def isotonic(input_data, quantile_list)\n\t\n    quantile_list = torch.reshape(quantile_list, (-1,))\n    batch_size = input_data.shape[0]\n    transformed_data = []\n    for i in range(batch_size):\n        data = input_data[i]\n        data = data.cpu().detach().numpy()\n        ir = IsotonicRegression(y_min=0.0, y_max=1.0, increasing=True)\n        y_ = ir.fit_transform(quantile_list, data)\n        transformed_data.append(y_)\n    transformed_data = torch.Tensor(transformed_data)\n    return transformed_data", "CodeBleu_score": 33.825734275967726}, {"code": "def isotonic(input_data, quantile_list)\n\t\n    quantile_list = torch.reshape(quantile_list, [-1])\n    batch_size = input_data.shape[0]\n    transformed_data = []\n    for i in range(batch_size):\n        cur_data = input_data[i, :]\n        transformed_cur_data = IsotonicRegression(\n            y_min=0, y_max=1, out_of_bounds=\"clip\", increasing=True\n        ).fit_transform(quantile_list, cur_data)\n        transformed_data.append(transformed_cur_data)\n    transformed_data = torch.stack(transformed_data)\n    return transformed_data\n\n", "CodeBleu_score": 32.63742224715651}, {"code": "def isotonic(input_data, quantile_list)\n\t\n    quantile_list = torch.Tensor(quantile_list).reshape(-1)\n    batch_size = input_data.shape[0]\n    data = []\n    for i in range(batch_size):\n        temp = input_data[i, :]\n        ir = IsotonicRegression(out_of_bounds=\"clip\")\n        ir.fit(quantile_list, temp)\n        temp = ir.transform(temp)\n        data.append(temp)\n    return torch.stack(data)\n\n", "CodeBleu_score": 36.261965325698405}]}
{"instance_id": "17566e088b4e", "generation_result": [{"code": "def _train_vectorizer(text_data: list, vectorizer)\n\t\n    vectorizer.fit(text_data)\n    vectorizer.stop_words_ = None\n    return vectorizer, vectorizer.transform(text_data)\n\n", "CodeBleu_score": 40.6826747095547}, {"code": "def _train_vectorizer(text_data: list, vectorizer)\n\t\n    vectorizer.fit(text_data)\n    vectorizer.stop_words_ = None\n    vectorized_text = vectorizer.transform(text_data)\n    return vectorizer, vectorized_text\n\n", "CodeBleu_score": 45.22807051565422}, {"code": "def _train_vectorizer(text_data: list, vectorizer)\n\t\n    vectorizer.fit(text_data)\n    vectorizer.stop_words_ = None\n    return vectorizer, vectorizer.transform(text_data)\n\n", "CodeBleu_score": 40.6826747095547}]}
{"instance_id": "be23c0afc2d6", "generation_result": [{"code": "def preprocess(df, test_size, shuffle)\n\t\n    df = df.dropna(subset=[col for col in df.columns if \"--\" not in df[col].unique()])\n    df = df.drop(\n        [col for col in df.columns if len(df[col].unique()) <= 1 or \"id\" in col],\n        axis=1,\n    )\n    df = df.drop([\"days_to_death\", \"year_of_death\"], axis=1, errors=\"ignore\")\n    train, test = train_test_split(\n        df, test_size=test_size, shuffle=shuffle, random_state=42\n    )\n    return train, test\n\n", "CodeBleu_score": 32.77834390476366}, {"code": "def preprocess(df, test_size, shuffle)\n\t\n    df = df.dropna(subset=['days_to_death'])\n    df = df.dropna(subset=['year_of_death'])\n    df = df[df.days_to_death >= 0]\n    df = df[df.year_of_death >= 0]\n    df = df.drop(columns=[col for col in df.columns if df[col].nunique() <= 1])\n    df = df.drop(columns=[col for col in df.columns if 'id' in col])\n    df = df.drop(columns=[col for col in df.columns if 'days_to' in col])\n    df = df.drop(columns=[col for col in df.columns if 'year_of' in col])\n    df = df.drop(columns=[col for col in df.columns if 'is_' in col])\n    df = df.drop(columns=[col for col in df.columns if 'has_' in col])\n    df = df.drop(columns=[col for col in df.columns if 'flag' in col])\n    df = df.drop(columns=[col for col in df.columns if 'date' in col])\n    df = df.drop(columns=[col for col in df.columns if 'time' in col])\n    df = df.drop(columns=[col for col in df.columns if 'day' in col])\n    df = df.drop(columns=[col for col in df.columns if 'week' in col])\n    df = df.drop(columns=[col for col in df.columns if 'month' in col])\n    df = df.drop(columns=[col for col in df.columns if 'year' in col])\n    df = df.drop(columns=[col for col in df.columns if 'quarter' in col])\n    df = df.drop(columns=[col for col in df.columns if 'season' in col])\n    df = df.drop(columns=[col for col in df.columns if 'era' in col])\n    df = df.drop(columns=[col for col in df.columns if 'century' in col])\n    df = df.drop(", "CodeBleu_score": 22.938217885325244}, {"code": "def preprocess(df, test_size, shuffle)\n\t\n    df = df.dropna(subset=[col for col in df.columns if df[col].dtype == 'object'])\n    df = df.drop([col for col in df.columns if df[col].nunique() <= 1], axis=1)\n    df = df.drop([col for col in df.columns if 'id' in col], axis=1)\n    df = df.drop([col for col in df.columns if 'days_to' in col], axis=1)\n    df = df.drop([col for col in df.columns if 'year_of' in col], axis=1)\n    train, test = train_test_split(df, test_size=test_size, shuffle=shuffle)\n    return train, test\n\n", "CodeBleu_score": 32.87703651301707}]}
{"instance_id": "6a82997a8c59", "generation_result": [{"code": "def _test_classifier_iterative_fit(classifier, dataset=\"iris\", sparse=False)\n\t\n    X, y = getattr(sklearn.datasets, \"load_\" + dataset)(return_X_y=True)\n    if sparse:\n        X = scipy.sparse.csr_matrix(X)\n        y = scipy.sparse.csr_matrix(y)\n\n    config_space = classifier.get_hyperparameter_search_space()\n    default_config = config_space.get_default_configuration()\n\n    classifier = classifier(\n        config=default_config,\n        random_state=1,\n    )\n\n    for i in range(2):\n        classifier.iterative_fit(X, y, n_iter=i + 1, refit=True)\n\n    while not classifier.configuration_fully_fitted:\n        classifier.iterative_fit(X, y, n_iter=classifier._iterative_fit_counter * 2)\n\n    y_pred = classifier.predict(X)\n\n    return y_pred, y, classifier\n\n", "CodeBleu_score": 39.20749916924585}, {"code": "def _test_classifier_iterative_fit(classifier, dataset=\"iris\", sparse=False)\n\t\n\n    # Load training and testing data\n    if dataset == \"iris\":\n        X, y = sklearn.datasets.load_iris(return_X_y=True)\n    elif dataset == \"digits\":\n        X, y = sklearn.datasets.load_digits(return_X_y=True)\n    else:\n        raise ValueError(\"Invalid dataset specified.\")\n\n    if sparse:\n        X = scipy.sparse.csr_matrix(X)\n\n    # Split data into training and test sets\n    X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(\n        X, y, test_size=0.2, random_state=42\n    )\n\n    # Retrieve the classifier's hyperparameter search space\n    search_space = classifier.get_hyperparameter_search_space()\n\n    # Obtain the default configuration\n    default_config = search_space.get_default_configuration()\n\n    # Initialize the classifier with the default configuration\n    classifier = classifier(\n        **default_config, random_state=42, n_jobs=1, config=default_config\n    )\n\n    # Fit the classifier to the training data for an initial two iterations with refitting enabled\n    classifier.iterative_fit(\n        X_train,\n        y_train,\n        n_iter=2,\n        refit=True,\n    )\n\n    # Continue iterative fitting, doubling the number of iterations each time, until the classifier is fully fitted\n    while not classifier.configuration_fully_fitted:\n        classifier.iterative_fit(\n            X_train,\n            y_train,\n            n_iter=2 * classifier.iteration_id,\n            refit=True,\n        )\n\n    # Make predictions on the test data\n    y_pred = classifier.predict(X_test)\n\n    # Return the predictions, true test labels, and the fitted classifier\n    return y_pred, y_test, classifier\n\n", "CodeBleu_score": 40.73261652713615}, {"code": "def _test_classifier_iterative_fit(classifier, dataset=\"iris\", sparse=False)\n\t\n    if dataset == \"iris\":\n        X, y = sklearn.datasets.load_iris(return_X_y=True)\n    elif dataset == \"digits\":\n        X, y = sklearn.datasets.load_digits(return_X_y=True)\n    else:\n        raise ValueError(\"Unsupported dataset\")\n\n    if sparse:\n        X = scipy.sparse.csr_matrix(X)\n\n    # Retrieve the classifier's hyperparameter search space\n    space = classifier.get_hyperparameter_search_space()\n\n    # Obtain the default configuration\n    default_config = space.get_default_configuration()\n\n    # Initialize the classifier with the default configuration and a random state\n    classifier = classifier(random_state=1, **default_config)\n\n    # Fit the classifier to the training data for an initial two iterations with refitting enabled\n    classifier.fit(X, y, refit=True, n_iter=2)\n\n    # Continue iterative fitting, doubling the number of iterations each time, until the classifier is fully fitted\n    while not classifier.configuration_fully_fitted():\n        classifier.fit(X, y, refit=True, n_iter=classifier.iterative_fit_inspector.get_next_iterations())\n\n    # Make predictions on the test data\n    predictions = classifier.predict(X)\n\n    return predictions, y, classifier\n\n", "CodeBleu_score": 42.56119745641741}]}
{"instance_id": "d0d8450c4c17", "generation_result": [{"code": "def _load_models(self)\n\t\n    if self.ensemble is not None:\n        self.backend.load_ensemble(self.ensemble, self.seed)\n        self.model_identifiers = self.ensemble.get_model_identifiers()\n        self.models = [self.backend.load_model(mid) for mid in self.model_identifiers]\n        self.cross_validated_models = [\n            self.backend.load_model(mid)\n            for mid in self.ensemble.get_cross_validated_model_identifiers()\n        ]\n    else:\n        self.models = []\n        self.cross_validated_models = []\n        if self.ensemble_builder_evaluator_output is not None:\n            if (\n                self.ensemble_builder_resampling_strategy is not None\n                and self.ensemble_builder_resampling_strategy != \"holdout\"\n            ):\n                self.backend.load_ensemble(\n                    self.ensemble_builder_evaluator_output, self.seed\n                )\n                self.model_identifiers = self.ensemble_builder_evaluator_output.get_model_identifiers()\n                self.models = [\n                    self.backend.load_model(mid) for mid in self.model_identifiers\n                ]\n                self.cross_validated_models = [\n                    self.backend.load_model(mid)\n                    for mid in self.ensemble_builder_evaluator_output.get_cross_validated_model_identifiers()\n                ]\n            else:\n                self.model_identifiers = [\n                    self.ensemble_builder_evaluator_output.get_best_model_identifier()\n                ]\n                self.models = [\n                    self.backend.load_model(self.model_identifiers[0])\n                ]\n                self.cross_validated_models = None\n\n", "CodeBleu_score": 38.68884129730089}, {"code": "def _load_models(self)\n\t\n    if self.ensemble_cls is not None:\n        self.ensemble = self.backend.load_ensemble(self.ensemble_cls, self.seed)\n        self.model_identifiers = self.ensemble.get_model_identifiers()\n        self.models = [\n            self.backend.load_model(identifier, self.seed)\n            for identifier in self.model_identifiers\n        ]\n        if self.resampling_strategy in (\n            \"cv\",\n            \"holdout-iterative-fit\",\n            \"holdout\",\n            \"partial-cv\",\n            \"partial-cv-iterative-fit\",\n        ):\n            self.cross_validated_models = [\n                self.backend.load_model(identifier, self.seed)\n                for identifier in self.ensemble.get_cross_validated_model_identifiers()\n            ]\n        else:\n            self.cross_validated_models = None\n    else:\n        if (\n            self.evaluator_output is not None\n            and self.resampling_strategy in (\"holdout\", \"holdout-iterative-fit\")\n            and self.ensemble_size == 1\n        ):\n            self.models = [\n                self.backend.load_model(self.evaluator_output[\"model_identifier\"], self.seed)\n            ]\n            self.model_identifiers = [self.evaluator_output[\"model_identifier\"]]\n            self.cross_validated_models = None\n        else:\n            self.models = []\n            self.model_identifiers = []\n            self.cross_validated_models = None\n\n", "CodeBleu_score": 43.7948806460422}, {"code": "def _load_models(self)\n\t\n    if self.ensemble_class is not None:\n        self.ensemble = self.backend.load_ensemble(\n            self.ensemble_class, self.seed, self.include_estimators\n        )\n        self.model_identifiers = self.ensemble.model_identifiers\n        self.models = self.backend.load_models(self.model_identifiers)\n    else:\n        self.ensemble = None\n        self.model_identifiers = []\n        self.models = []\n\n    if (\n        self.evaluator_output is not None\n        and self.evaluator_output.has_finished()\n        and self.evaluator_output.get_best_config_id() is not None\n        and self.resampling_strategy != \"holdout\"\n    ):\n        self.models = [self.backend.load_model_by_seed_and_id(self.seed, self.seed)]\n        self.model_identifiers = [self.seed]\n        self.ensemble = None\n\n    if self.resampling_strategy == \"cv\":\n        self.cross_validated_models = self.backend.load_models(\n            self.model_identifiers, cross_validation=True\n        )\n    else:\n        self.cross_validated_models = None\n\n    if self.ensemble is None and self.models == []:\n        self.models = []\n        self.model_identifiers = []\n\n", "CodeBleu_score": 45.08205763493195}]}
{"instance_id": "ef71b1cddb50", "generation_result": [{"code": "def load_task(task_id)\n\t\n    task = openml.tasks.get_task(task_id)\n    X, y = task.get_X_and_y()\n    train_indices, test_indices = task.get_train_test_split_indices()\n    X_train = X[train_indices]\n    y_train = y[train_indices]\n    X_test = X[test_indices]\n    y_test = y[test_indices]\n    dataset = task.get_dataset()\n    is_categorical = [dataset.get_feat_type(feat_name) == \"categorical\" for feat_name in dataset.features.keys()]\n    return X_train, y_train, X_test, y_test, is_categorical\n\n", "CodeBleu_score": 27.698823623153114}, {"code": "def load_task(task_id)\n\t\n    task = openml.tasks.get_task(task_id)\n    X, y = task.get_X_and_y()\n    train_indices, test_indices = task.get_train_test_split_indices()\n    X_train = X[train_indices]\n    y_train = y[train_indices]\n    X_test = X[test_indices]\n    y_test = y[test_indices]\n    dataset = task.get_dataset()\n    is_categorical = [dataset.get_feat_type(feat) == 'nominal' for feat in dataset.features.keys()]\n    return X_train, y_train, X_test, y_test, is_categorical\n\n", "CodeBleu_score": 27.095828996732752}, {"code": "def load_task(task_id)\n\t\n    task = openml.tasks.get_task(task_id)\n    X, y = task.get_X_and_y()\n    train_indices, test_indices = task.get_train_test_split_indices()\n\n    X_train = X[train_indices]\n    y_train = y[train_indices]\n    X_test = X[test_indices]\n    y_test = y[test_indices]\n\n    dataset = task.get_dataset()\n    _, _, categorical_indicator = dataset.get_data(\n        dataset_format=\"array\", target=task.target_name\n    )\n\n    # map unique label values to integer indices\n    unique_labels = list(set(y_train) | set(y_test))\n    label_mapping = {label: i for i, label in enumerate(unique_labels)}\n    y_train = [label_mapping[label] for label in y_train]\n    y_test = [label_mapping[label] for label in y_test]\n\n    return X_train, y_train, X_test, y_test, categorical_indicator\n\n", "CodeBleu_score": 41.64701370343904}]}
{"instance_id": "90bd6edbcc9e", "generation_result": [{"code": "def convert_if_sparse(\n    y: SUPPORTED_TARGET_TYPES,\n) -> Union[np.ndarray, List, pd.DataFrame, pd.Series]\n\t\n    if spmatrix.issparse(y):\n        y = y.toarray()\n        if y.shape[1] == 1:\n            y = y.flatten()\n    return y\n\n", "CodeBleu_score": 56.63069740013977}, {"code": "def convert_if_sparse(\n    y: SUPPORTED_TARGET_TYPES,\n) -> Union[np.ndarray, List, pd.DataFrame, pd.Series]\n\t\n    if spmatrix.issparse(y):\n        y = y.toarray()\n        if y.shape[1] == 1:\n            y = y.flatten()\n    return y\n\n", "CodeBleu_score": 56.63069740013977}, {"code": "def convert_if_sparse(\n    y: SUPPORTED_TARGET_TYPES,\n) -> Union[np.ndarray, List, pd.DataFrame, pd.Series]\n\t\n    if spmatrix.issparse(y):\n        y = y.toarray()\n        if y.ndim == 1:\n            y = y.flatten()\n    return y\n\n", "CodeBleu_score": 53.11788738755181}]}
{"instance_id": "5728e3d4f914", "generation_result": [{"code": "def _calculate_weights(self) -> None\n\t\n    counts = Counter(self.indices_)\n    self.weights_ = np.zeros(len(self.models))\n    for i, count in counts.items():\n        self.weights_[i] = count / len(self.indices_)\n    if np.sum(self.weights_) < 1:\n        self.weights_ /= np.sum(self.weights_)\n\n", "CodeBleu_score": 22.264643522039577}, {"code": "def _calculate_weights(self) -> None\n\t\n    weights = [0.0] * len(self.models)\n    for i in self.models:\n        weights[i] += 1\n    for i in range(len(weights)):\n        weights[i] /= len(self.models)\n    if sum(weights) < 1:\n        weights = [w / sum(weights) for w in weights]\n    self.weights_ = weights\n\n", "CodeBleu_score": 28.265544219546634}, {"code": "def _calculate_weights(self) -> None\n\t\n    weights = [0] * len(self.models)\n    for i in self.models:\n        weights[i] += 1\n    for i in range(len(weights)):\n        weights[i] /= len(self.models)\n    if sum(weights) < 1:\n        weights = [w / sum(weights) for w in weights]\n    self.weights_ = weights\n\n", "CodeBleu_score": 28.231218261688905}]}
{"instance_id": "4f9c4799fd8f", "generation_result": [{"code": "def predict_proba(self, X, batch_size=None, n_jobs=1)\n\t\n    y_pred_proba = super(AutoML, self).predict_proba(X, batch_size, n_jobs)\n    if self.target_type_ != \"multilabel-indicator\":\n        assert np.allclose(np.sum(y_pred_proba, axis=1), 1)\n    assert np.all((0 <= y_pred_proba) & (y_pred_proba <= 1))\n    return y_pred_proba\n\n", "CodeBleu_score": 30.011243076571603}, {"code": "def predict_proba(self, X, batch_size=None, n_jobs=1)\n\t\n    y_proba = super(self.__class__, self).predict_proba(X, batch_size, n_jobs)\n    if self.target_type_ != \"multilabel-indicator\":\n        assert np.allclose(np.sum(y_proba, axis=1), 1)\n    assert np.all((0 <= y_proba) & (y_proba <= 1))\n    return y_proba\n\n", "CodeBleu_score": 28.649937373989175}, {"code": "def predict_proba(self, X, batch_size=None, n_jobs=1)\n\t\n    y_proba = super().predict_proba(X, batch_size=batch_size, n_jobs=n_jobs)\n    if self.target_type_ != \"multilabel-indicator\":\n        assert np.allclose(np.sum(y_proba, axis=1), 1)\n    assert np.all((0 <= y_proba) & (y_proba <= 1))\n    return y_proba\n\n", "CodeBleu_score": 31.539810435392635}]}
{"instance_id": "a9a13150aebe", "generation_result": [{"code": "def convert_to_num(Ybin: np.ndarray) -> np.ndarray:\n\t\n    if len(Ybin.shape) == 1:\n        return Ybin\n    else:\n        return np.dot(Ybin, range(Ybin.shape[1]))\n\n", "CodeBleu_score": 37.384962417873545}, {"code": "def convert_to_num(Ybin: np.ndarray) -> np.ndarray:\n\t\n    if len(Ybin.shape) > 1:\n        return np.dot(Ybin, range(Ybin.shape[1]))\n    else:\n        return Ybin\n\n", "CodeBleu_score": 38.28572856729468}, {"code": "def convert_to_num(Ybin: np.ndarray) -> np.ndarray:\n\t\n    if len(Ybin.shape) == 1:\n        return Ybin\n    else:\n        return np.dot(Ybin, range(Ybin.shape[1]))\n\n", "CodeBleu_score": 37.384962417873545}]}
{"instance_id": "e97d38e62f81", "generation_result": [{"code": "def fit_transformer(self, X, y, fit_params=None)\n\t\n    if fit_params is None:\n        fit_params = {}\n    if self.balancing_strategy == \"weighting\":\n        balancing = Balancing(\n            random_state=self.random_state,\n            config=self.config,\n            **self.get_hyperparams(\n                params={\n                    \"balancing:strategy\": \"weighting\",\n                    \"classifier:__choice__\": self.steps[1][0],\n                    \"feature_preprocessor:__choice__\": self.steps[0][0],\n                }\n            ),\n        )\n        (\n            self.init_params[self.steps[1][0]],\n            self.init_params[self.steps[0][0]],\n        ) = balancing.get_weights()\n    self.hyperparams = {\n        \"classifier\": self.get_hyperparams(\n            params={\n                \"classifier:__choice__\": self.steps[1][0],\n                **self.init_params[self.steps[1][0]],\n            }\n        ),\n        \"feature_preprocessor\": self.get_hyperparams(\n            params={\n                \"feature_preprocessor:__choice__\": self.steps[0][0],\n                **self.init_params[self.steps[0][0]],\n            }\n        ),\n    }\n    fit_params = {**self.hyperparams[\"classifier\"], **fit_params}\n    return super(ClassificationPipeline, self).fit_transformer(X, y, fit_params=fit_params)\n\n", "CodeBleu_score": 48.01984858989153}, {"code": "def fit_transformer(self, X, y, fit_params=None)\n\t\n    if fit_params is None:\n        fit_params = {}\n    if self.config.get(\"balancing:strategy\") == \"weighting\":\n        balancing = Balancing(\n            random_state=self.config.get(\"balancing:random_state\"),\n        )\n        classifier_weights = balancing.get_weights(\n            classifier=self.steps[1][1],\n            X=X,\n            y=y,\n        )\n        self.steps[1][1].init_params[\"class_weight\"] = classifier_weights\n        feature_preprocessor_weights = balancing.get_weights(\n            classifier=self.steps[0][1],\n            X=X,\n            y=y,\n        )\n        self.steps[0][1].init_params[\"class_weight\"] = feature_preprocessor_weights\n    self.config.update(self.get_hyperparameter_search_space().sample_configuration())\n    self.set_hyperparameters(self.config)\n    fit_params.update(self.init_params)\n    return super(BalancedPipeline, self).fit_transformer(X, y, fit_params)\n\n", "CodeBleu_score": 50.64365091050925}, {"code": "def fit_transformer(self, X, y, fit_params=None)\n\t\n    if fit_params is None:\n        fit_params = {}\n    if self.configuration[\"balancing:strategy\"] == \"weighting\":\n        balancing = Balancing(random_state=self.random_state)\n        classifier_weights = balancing.get_weights(self.steps[1][1])\n        feature_preprocessor_weights = balancing.get_weights(self.steps[0][1])\n        self.steps[1][1].init_params[\n            \"classifier:__choice__\"\n        ] = self.configuration[\"classifier:__choice__\"]\n        self.steps[1][1].init_params[\n            \"classifier:{0}:__choice__\".format(\n                self.configuration[\"classifier:__choice__\"]\n            )\n        ] = self.configuration[\"classifier:{0}:__choice__\".format(self.configuration[\"classifier:__choice__\"])]\n        self.steps[1][1].init_params[\n            \"classifier:{0}:{1}:__choice__\".format(\n                self.configuration[\"classifier:__choice__\"],\n                self.configuration[\"classifier:{0}:__choice__\".format(self.configuration[\"classifier:__choice__\"])],\n            )\n        ] = self.configuration[\"classifier:{0}:{1}:__choice__\".format(self.configuration[\"classifier:__choice__\"], self.configuration[\"classifier:{0}:__choice__\".format(self.configuration[\"classifier:__choice__\"])])]\n        self.steps[1][1].init_params[\"classifier\"] = classifier_weights\n        self.steps[0][1].init_params[\"feature_preprocessor\"] = feature_preprocessor_weights\n    self.init_params[\"classifier\"] = self.configuration[\"classifier\"]\n    self.init_params[\"feature_preprocessor\"] = self.configuration[\"feature_preprocessor\"]\n    self.hyperparams = self.get_hyperparameter_search_space(\n        dataset_properties={\"sparse\": SPARSE not in self.dataset_properties[\"feat_type\"]}\n    ).sample_configuration()\n    self.hyperparams", "CodeBleu_score": 39.62564329535427}]}
{"instance_id": "b1aaad80a9c4", "generation_result": [{"code": "def test_glm_identity_link_func()\n\t\n    x = [1, 2, 3, 4, 5]\n    y = [1, 2, 3, 4, 5]\n    tweedie = ProcessMLE.from_formula(\n        \"y ~ 1 + x\", family=\"tweedie\", link=\"power\", data={\"x\": x, \"y\": y}\n    )\n    tweedie.fit()\n\n    assembler = assemblers.PythonCodeAssembler(tweedie)\n    actual = assembler.assemble()\n\n    expected = ast.BinNumExpr(\n        ast.BinNumExpr(\n            ast.NumVal(1.0),\n            ast.FeatureRef(0),\n            ast.BinNumOpType.ADD,\n        ),\n        ast.NumVal(1.0),\n        ast.BinNumOpType.POW,\n    )\n\n    assert utils.cmp_exprs(actual, expected)\n\n", "CodeBleu_score": 32.43552709255509}, {"code": "def test_glm_identity_link_func()\n\t\n    x = [1, 2, 3, 4, 5]\n    y = [2, 4, 6, 8, 10]\n    tweedie_model = ProcessMLE.from_formula(\n        ast.TweedieDistribution.identity_family_name(),\n        ast.TweedieDistribution.identity_link_func(),\n        y=y, X=x\n    )\n    tweedie_result = tweedie_model.fit()\n    assembler = assemblers.get_assembler_cls(tweedie_result)()\n    actual = assembler.assemble(tweedie_result)\n    expected = ast.BinExp(\n        ast.NumVal(2.0),\n        ast.BinNumOpType.MUL,\n        ast.FeatureRef(0)\n    )\n    utils.assert_code_equal(actual, expected)\n\n", "CodeBleu_score": 14.527186665946648}, {"code": "def test_glm_identity_link_func()\n\t\n    x = [1, 2, 3, 4, 5]\n    y = [1, 2, 3, 4, 5]\n    tweedie = ProcessMLE.from_formula(\n        \"y ~ x\",\n        family=\"Tweedie\",\n        link=\"power\",\n        vc_formula={\"shape\": \"x\"},\n    )\n    tweedie = tweedie.fit(dict(x=x, y=y), vc=[1])\n    assembler = assemblers.GLMModelAssembler(tweedie)\n    actual = assembler.assemble()\n    expected = ast.SubroutineExpr(\n        ast.BinNumExpr(\n            ast.BinNumExpr(\n                ast.NumVal(1.0),\n                ast.NumVal(0.5),\n                ast.BinNumOpType.POW,\n            ),\n            ast.BinNumExpr(\n                ast.NumVal(0.5),\n                ast.FeatureRef(0),\n                ast.BinNumOpType.POW,\n            ),\n            ast.BinNumOpType.MUL,\n        ),\n        ast.NumVal(0.5),\n        ast.BinNumOpType.MUL,\n    )\n    utils.assert_code_equal(actual, expected)\n\n", "CodeBleu_score": 29.319716085514546}]}
{"instance_id": "bcab63e592d0", "generation_result": [{"code": "def _compute_aggregation(\n        self, col: Variable, agg: Union[str, Callable, Tuple[str, Union[str, Callable]]]\n    ) -> pd.Series\n\t\n    if isinstance(agg, tuple):\n        agg_name, agg_func = agg\n    else:\n        agg_name = agg.__name__ if hasattr(agg, \"__name__\") else str(agg)\n        agg_func = agg\n    return getattr(self, agg_name)() if agg_name == \"count\" else self.agg(agg_func).rename(f\"{col.name}_{agg_name}\")\n\n", "CodeBleu_score": 49.25429111504964}, {"code": "def _compute_aggregation(\n        self, col: Variable, agg: Union[str, Callable, Tuple[str, Union[str, Callable]]]\n    ) -> pd.Series\n\t\n    if isinstance(agg, tuple):\n        name, agg = agg\n    else:\n        name = agg if isinstance(agg, str) else agg.__name__\n    new_col = f\"{col.name}_{name}\"\n    return self.agg(agg).rename(columns={col.name: new_col})\n\n", "CodeBleu_score": 74.6443247520128}, {"code": "def _compute_aggregation(\n        self, col: Variable, agg: Union[str, Callable, Tuple[str, Union[str, Callable]]]\n    ) -> pd.Series\n\t\n    if isinstance(agg, tuple):\n        agg_name, agg_func = agg\n    else:\n        agg_name = agg if isinstance(agg, str) else agg.__name__\n        agg_func = agg\n    col_name = col.name\n    return self.agg({col_name: (agg_name, agg_func)})[col_name]\n\n", "CodeBleu_score": 58.659167504420374}]}
{"instance_id": "114d623dae7b", "generation_result": [{"code": "def _corrfunc(self, x, y, **kws)\n\t\n    r, p = stats.pearsonr(x, y)\n    ax = plt.gca()\n    ax.annotate(\"r = {:.2f}\".format(r), xy=(0.1, 0.9), xycoords=ax.transAxes)\n    if r < 0.00:\n        return\n    else:\n        return\n\n", "CodeBleu_score": 8.191588348162394}, {"code": "def _corrfunc(self, x, y, **kws)\n\t\n    r, p = stats.pearsonr(x, y)\n    ax = plt.gca()\n    ax.annotate(\n        \"r = {:.2f}\".format(r),\n        xy=(0.5, 0.5),\n        xycoords=ax.transAxes,\n        ha=\"center\",\n        va=\"center\",\n        size=20 / (len(self._hue_names) + 1) * (2 + abs(r)),\n        color=\"darkgrey\",\n    )\n\n", "CodeBleu_score": 15.87347497347928}, {"code": "def _corrfunc(self, x, y, **kws)\n\t\n    r, p = stats.pearsonr(x, y)\n    ax = plt.gca()\n    ax.annotate(\"r = {:.2f}\".format(r), xy=(.1, .9), xycoords=ax.transAxes)\n    if len(self.hue_vals) < 2:\n        return\n    else:\n        hue_groups = self.hue_vals\n        avg_x = [np.mean(x[y == hue_group]) for hue_group in hue_groups]\n        avg_y = [np.mean(y[y == hue_group]) for hue_group in hue_groups]\n        for i, (x_i, y_i) in enumerate(zip(avg_x, avg_y)):\n            if len(self.hue_vals) < 2:\n                x_i = np.mean(x)\n                y_i = np.mean(y)\n            else:\n                x_i = np.mean(x[y == hue_group])\n                y_i = np.mean(y[y == hue_group])\n            ax.annotate(\"r = {:.2f}\".format(np.corrcoef(x_i, y_i)[0, 1]), xy=(.1, .9 - (i * .05)), xycoords=ax.transAxes)\n\n", "CodeBleu_score": 27.462892924375826}]}
{"instance_id": "4c1ade308953", "generation_result": [{"code": "def _round_digits(src: float, rounddigit: int = None, method='decimal')\n\t\n    if method == 'decimal':\n        return round(src, rounddigit)\n    elif method == 'sig':\n        return float(decimal.Decimal(str(src)).quantize(decimal.Decimal(str(rounddigit))))\n    elif method == 'format':\n        return float(f'{src:.{rounddigit}g}')\n\n", "CodeBleu_score": 60.286565008528314}, {"code": "def _round_digits(src: float, rounddigit: int = None, method='decimal')\n\t\n    if rounddigit is None:\n        rounddigit = 3\n    if method == 'decimal':\n        return round(src, rounddigit)\n    elif method == 'sig':\n        with decimal.localcontext() as ctx:\n            ctx.prec = rounddigit\n            return ctx.create_decimal(repr(src))\n    elif method == 'format':\n        return float(f'{src:.{rounddigit}g}')\n    else:\n        raise ValueError(\"Invalid method specified. Choose from 'decimal', 'sig', or 'format'.\")\n\n", "CodeBleu_score": 68.34449957573663}, {"code": "def _round_digits(src: float, rounddigit: int = None, method='decimal')\n\t\n    if method == 'decimal':\n        return round(src, rounddigit)\n    elif method == 'sig':\n        return float(decimal.Decimal(src).quantize(decimal.Decimal(f'1E-{rounddigit}'), rounding=decimal.ROUND_HALF_UP))\n    elif method == 'format':\n        return float(f'{src:.{rounddigit}g}')\n    else:\n        raise ValueError(f\"Invalid method: {method}. Choose from 'decimal', 'sig', or 'format'.\")\n\n", "CodeBleu_score": 58.44929175289817}]}
{"instance_id": "a9923e678320", "generation_result": [{"code": "def _round_dict_digits(cls, srcdict: Dict[str, float], rounddigit: int = None, method='decimal')\n\t\n    if rounddigit is None:\n        return srcdict\n    elif method == 'decimal':\n        return {k: decimal.Decimal(str(v)).quantize(decimal.Decimal(str(10**-rounddigit))) for k, v in srcdict.items()}\n    elif method == 'sig':\n        return {k: round(v, rounddigit) for k, v in srcdict.items()}\n    elif method == 'format':\n        return {k: '{:.{prec}f}'.format(v, prec=rounddigit) for k, v in srcdict.items()}\n    else:\n        raise ValueError(\"Invalid method specified. Choose from 'decimal', 'sig', or 'format'.\")\n\n", "CodeBleu_score": 32.90192482669694}, {"code": "def _round_dict_digits(cls, srcdict: Dict[str, float], rounddigit: int = None, method='decimal')\n\t\n    if rounddigit is None:\n        return srcdict\n    else:\n        if method == 'decimal':\n            return {key: cls._round_digits(value, rounddigit) for key, value in srcdict.items()}\n        elif method == 'sig':\n            return {key: cls._round_sig_digits(value, rounddigit) for key, value in srcdict.items()}\n        elif method == 'format':\n            return {key: cls._format_digits(value, rounddigit) for key, value in srcdict.items()}\n        else:\n            raise ValueError(\"Invalid method specified. Choose from 'decimal', 'sig', or 'format'.\")\n\n", "CodeBleu_score": 27.030491782375726}, {"code": "def _round_dict_digits(cls, srcdict: Dict[str, float], rounddigit: int = None, method='decimal')\n\t\n    if rounddigit is None:\n        return srcdict\n    if method == 'decimal':\n        return {k: _round_digits(v, rounddigit) for k, v in srcdict.items()}\n    elif method == 'sig':\n        return {k: _round_digits_sig(v, rounddigit) for k, v in srcdict.items()}\n    elif method == 'format':\n        return {k: _round_digits_format(v, rounddigit) for k, v in srcdict.items()}\n    else:\n        return srcdict\n\n", "CodeBleu_score": 31.590856926788558}]}
{"instance_id": "b6106e079dc0", "generation_result": [{"code": "def _scatterplot_ndarray(cls, x, x_name, y, y_name, hue_data, hue_name, ax, scatter_kws, legend_kws)\n\t\n    import pandas as pd\n    import seaborn as sns\n\n    # Create a DataFrame from the x and y values\n    df = pd.DataFrame({x_name: x.ravel(), y_name: y.ravel()})\n\n    # Add a hue field if hue_data is provided\n    if hue_data is not None:\n        df[hue_name] = hue_data.ravel()\n\n    # Plot the scatter plot using Seaborn\n    sns.scatterplot(data=df, x=x_name, y=y_name, hue=hue_name, ax=ax, **scatter_kws)\n\n    # Set the title of the legend if no title is specified\n    if legend_kws is not None and 'title' not in legend_kws:\n        legend_kws['title'] = hue_name\n\n    # Add the legend if legend_kws are provided\n    if legend_kws is not None:\n        ax.legend(**legend_kws)\n\n    return ax\n\n", "CodeBleu_score": 47.23063360874152}, {"code": "def _scatterplot_ndarray(cls, x, x_name, y, y_name, hue_data, hue_name, ax, scatter_kws, legend_kws)\n\t\n    df = pd.DataFrame({x_name: x, y_name: y})\n    if hue_data is not None:\n        df[hue_name] = hue_data\n        sns.scatterplot(x=x_name, y=y_name, hue=hue_name, data=df, ax=ax, **scatter_kws)\n    else:\n        sns.scatterplot(x=x_name, y=y_name, data=df, ax=ax, **scatter_kws)\n    if legend_kws is not None:\n        if 'title' not in legend_kws:\n            legend_kws['title'] = hue_name\n        ax.legend(**legend_kws)\n    else:\n        ax.legend()\n    return ax\n\n", "CodeBleu_score": 44.37982065310299}, {"code": "def _scatterplot_ndarray(cls, x, x_name, y, y_name, hue_data, hue_name, ax, scatter_kws, legend_kws)\n\t\n    import pandas as pd\n    import seaborn as sns\n    import matplotlib.pyplot as plt\n\n    # Combine x and y values into a DataFrame\n    df = pd.DataFrame({x_name: x, y_name: y})\n\n    # Add hue data to the DataFrame if provided\n    if hue_data is not None:\n        df[hue_name] = hue_data\n\n    # Plot the scatter plot using Seaborn\n    sns.scatterplot(x=x_name, y=y_name, hue=hue_name, data=df, ax=ax, **scatter_kws)\n\n    # Set the legend title if not specified\n    if legend_kws is not None and 'title' not in legend_kws:\n        legend_kws['title'] = hue_name\n\n    # Set the legend if provided\n    if legend_kws is not None:\n        ax.legend(**legend_kws)\n\n    # Set the title if provided\n    if cls.title is not None:\n        ax.set_title(cls.title)\n\n    # Set the x-axis label if provided\n    if cls.x_label is not None:\n        ax.set_xlabel(cls.x_label)\n\n    # Set the y-axis label if provided\n    if cls.y_label is not None:\n        ax.set_ylabel(cls.y_label)\n\n    # Set the x-axis limits if provided\n    if cls.x_lim is not None:\n        ax.set_xlim(cls.x_lim)\n\n    # Set the y-axis limits if provided\n    if cls.y_lim is not None:\n        ax.set_ylim(cls.y_lim)\n\n    # Set the x-axis scale if provided\n    if cls.x_scale is not None:\n        ax.set_xscale(cls.x_scale)\n\n    # Set the y-axis scale if provided\n    if cls.y_scale is not None:\n        ax.set_yscale(cls.y_", "CodeBleu_score": 42.982954147549236}]}
{"instance_id": "257e0aa5dd4d", "generation_result": [{"code": "def fit(self, **fit_args) -> \"TimeSeriesModel\"\n\t\n    endog = self.data\n    exog = None\n    if self.exog_vars is not None:\n        exog = self.data[self.exog_vars]\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\", UserWarning)\n        warnings.simplefilter(\"ignore\", ConvergenceWarning)\n        self.model = self.model_cls(endog, exog=exog, **fit_args)\n    return self.model\n\n", "CodeBleu_score": 37.798033277736266}, {"code": "def fit(self, **fit_args) -> \"TimeSeriesModel\"\n\t\n    if self.data is None:\n        raise ValueError(\n            \"Root node has no data. Please load data into the root node.\"\n        )\n\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\", UserWarning)\n        warnings.simplefilter(\"ignore\", ConvergenceWarning)\n\n        # Get the endogenous and exogenous data for the current node\n        endog = self.data[self.key]\n        exog = None\n        if self.exog is not None:\n            exog = self.data[self.exog]\n\n        # Fit the model using the endogenous and exogenous data\n        with self.model.logging_disabled():\n            fitted_model = self.model.fit(\n                endog=endog, exog=exog, **fit_args\n            )\n\n        # Return the fitted time series model object\n        return fitted_model\n\n", "CodeBleu_score": 43.35756689796803}, {"code": "def fit(self, **fit_args) -> \"TimeSeriesModel\"\n\t\n    # TODO: Implement a way to pass in a custom model\n    # TODO: Implement a way to pass in a custom model\n    # TODO: Implement a way to pass in a custom model\n    # TODO: Implement a way to pass in a custom model\n    # TODO: Implement a way to pass in a custom model\n    # TODO: Implement a way to pass in a custom model\n    # TODO: Implement a way to pass in a custom model\n    # TODO: Implement a way to pass in a custom model\n    # TODO: Implement a way to pass in a custom model\n    # TODO: Implement a way to pass in a custom model\n    # TODO: Implement a way to pass in a custom model\n    # TODO: Implement a way to pass in a custom model\n    # TODO: Implement a way to pass in a custom model\n    # TODO: Implement a way to pass in a custom model\n    # TODO: Implement a way to pass in a custom model\n    # TODO: Implement a way to pass in a custom model\n    # TODO: Implement a way to pass in a custom model\n    # TODO: Implement a way to pass in a custom model\n    # TODO: Implement a way to pass in a custom model\n    # TODO: Implement a way to pass in a custom model\n    # TODO: Implement a way to pass in a custom model\n    # TODO: Implement a way to pass in a custom model\n    # TODO: Implement a way to pass in a custom model\n    # TODO: Implement a way to pass in a custom model\n    # TODO: Implement a way to pass in a custom model\n    # TODO: Implement a way to pass in a custom model\n    # TODO: Implement a way to pass in a custom model\n    # TODO: Implement a way to pass in a custom model\n    # TODO: Implement a way to pass in a custom model\n    # TODO: Implement a way to pass in a custom model\n    # TODO: Implement a way to pass in a custom model\n    # TODO: Implement a way to pass in a custom model\n    # TODO: Implement a way to pass in a custom model\n    # TODO: Implement a way to pass in a custom model\n   ", "CodeBleu_score": 2.083333333333333}]}
{"instance_id": "6446de2223e8", "generation_result": [{"code": "def create_model(self, **kwargs)\n\t\n    if self.model_type == \"holt_winters\":\n        data = self.transformed_data\n        model = ExponentialSmoothing(data, **kwargs)\n    elif self.model_type == \"auto_arima\":\n        try:\n            from pmdarima import AutoARIMA\n        except ImportError:\n            logging.error(\"pmdarima is not installed. Please install it to use auto_arima.\")\n            exit(1)\n        model = AutoARIMA(**kwargs)\n    elif self.model_type == \"sarimax\":\n        data = self.transformed_data\n        exog = self.exog_data\n        if exog is not None:\n            model = SARIMAX(data, exog=exog, **kwargs)\n        else:\n            model = SARIMAX(data, **kwargs)\n    else:\n        raise InvalidArgumentException(f\"Unknown model type: {self.model_type}\")\n    return model\n\n", "CodeBleu_score": 40.33767444807446}, {"code": "def create_model(self, **kwargs)\n\t\n    if self.model == \"holt_winters\":\n        y = self.transformed_data\n        model = ExponentialSmoothing(y, **kwargs)\n    elif self.model == \"auto_arima\":\n        try:\n            from pmdarima.arima import AutoARIMA\n        except ImportError:\n            logging.error(\"pmdarima not installed\")\n            exit()\n        model = AutoARIMA(**kwargs)\n    elif self.model == \"sarimax\":\n        y = self.transformed_data\n        exog = self.exog\n        model = SARIMAX(y, exog=exog, **kwargs)\n    else:\n        raise InvalidArgumentException(\n            f\"Model type {self.model} is not recognized.\"\n        )\n    return model\n\n", "CodeBleu_score": 36.12363690501777}, {"code": "def create_model(self, **kwargs)\n\t\n    if self.model == \"holt_winters\":\n        transformed = self.transformed\n        model = ExponentialSmoothing(\n            transformed,\n            trend=self.trend,\n            damped=self.damped,\n            seasonal=self.seasonal,\n            seasonal_periods=self.seasonal_periods,\n        )\n    elif self.model == \"auto_arima\":\n        try:\n            from pmdarima import AutoARIMA\n\n            transformed = self.transformed\n            model = AutoARIMA(**kwargs)\n        except ImportError as e:\n            logging.error(\n                \"auto_arima requires pmdarima to be installed. Run `pip install pmdarima`\"\n            )\n            raise e\n    elif self.model == \"sarimax\":\n        transformed = self.transformed\n        exog = self.exog\n        if exog is not None:\n            model = SARIMAX(transformed, exog=exog, **kwargs)\n        else:\n            model = SARIMAX(transformed, **kwargs)\n    else:\n        raise InvalidArgumentException(\n            f\"Model type {self.model} is not recognized. Please use 'holt_winters', 'auto_arima', or 'sarimax'.\"\n        )\n    return model\n\n", "CodeBleu_score": 43.8418966431399}]}
{"instance_id": "b2f67820e27f", "generation_result": [{"code": "def _to_chx(array)\n\t\n    if isinstance(array, numpy.ndarray):\n        return _from_numpy(array)\n    elif isinstance(array, chainerx.ndarray):\n        return array\n    elif hasattr(array, '__cuda_array_interface__') and chainerx.cuda.available:\n        return _from_cupy(array)\n    else:\n        return array\n\n", "CodeBleu_score": 65.7457634288673}, {"code": "def _to_chx(array)\n\t\n    if isinstance(array, numpy.ndarray):\n        return chainerx._core._from_numpy(array)\n    elif hasattr(array, '__cuda_array_interface__'):\n        return chainerx._core._from_cupy(array)\n    else:\n        return array\n\n", "CodeBleu_score": 52.94173424224255}, {"code": "def _to_chx(array)\n\t\n    if isinstance(array, numpy.ndarray):\n        return _from_numpy(array)\n    elif isinstance(array, chainerx.ndarray):\n        return array\n    else:\n        return array\n\n", "CodeBleu_score": 61.75443891669714}]}
{"instance_id": "b96f7d9bcc95", "generation_result": [{"code": "def _fix_numpy_split(ys, x, indices_or_sections, axis)\n\t\n    if all(y.ndim == x.ndim for y in ys):\n        return ys\n    sizes = numpy.array(indices_or_sections)\n    if sizes.size == 1:\n        sections = sizes.item()\n        if x.shape[axis] % sections != 0:\n            raise ValueError('array split does not result in an equal division')\n        sizes = numpy.array([x.shape[axis] // sections] * sections)\n    else:\n        if sizes.ndim != 1:\n            raise IndexError('Only one dimension is allowed for indices_or_sections')\n        if not numpy.all(numpy.diff(sizes) >= 0):\n            raise ValueError('indices_or_sections should be a sorted sequence of increasing integers')\n        if sizes.size == 0:\n            sizes = numpy.array([0, x.shape[axis]])\n        elif sizes[0] < 0:\n            raise IndexError('indices_or_sections[0] cannot be negative')\n        elif sizes[-1] > x.shape[axis]:\n            raise IndexError('indices_or_sections[-1] cannot be greater than the number of elements of an array along the split dimension')\n        sizes = numpy.concatenate(([0], sizes, [x.shape[axis]]))\n    split_sizes = numpy.diff(sizes)\n    split_arrays = []\n    for i, y in enumerate(ys):\n        if y.ndim == x.ndim:\n            split_arrays.append(y)\n            continue\n        shape = list(x.shape)\n        shape[axis] = split_sizes[i]\n        split_arrays.append(y.reshape(shape))\n    return split_arrays\n\n", "CodeBleu_score": 40.85016972209471}, {"code": "def _fix_numpy_split(ys, x, indices_or_sections, axis)\n\t\n    if all((y.ndim == x.ndim for y in ys)):\n        return ys\n    z = numpy.empty(x.shape)\n    zs = numpy.split(z, indices_or_sections, axis)\n    new_ys = [y.reshape(z.shape) if y.ndim != x.ndim else y for (y, z) in zip(ys, zs)]\n    return new_ys\n\n", "CodeBleu_score": 32.250871340857515}, {"code": "def _fix_numpy_split(ys, x, indices_or_sections, axis)\n\t\n    if not all(y.ndim == x.ndim for y in ys):\n        # numpy<1.11 does not preserve number of dimensions.\n        # In that case we use the shape of a split along such axis.\n        shape = numpy.array(x.shape)\n        shape[axis] = 0\n        if indices_or_sections.size == 0:\n            tmp = numpy.empty(shape)\n            sections = x.shape[axis]\n        else:\n            tmp = numpy.zeros(shape)\n            sections = indices_or_sections\n        tmp_ys = numpy.split(tmp, sections, axis)\n        for y, tmp_y in zip(ys, tmp_ys):\n            if y.shape != tmp_y.shape:\n                y = y.reshape(tmp_y.shape)\n    return ys\n\n", "CodeBleu_score": 41.883204147769916}]}
{"instance_id": "cf39fb123842", "generation_result": [{"code": "def to_device_batch(batch):\n\t\n    if args.device is None:\n        return batch\n    elif args.device < 0:\n        return chainer.dataset.to_device(args.device, batch)\n    else:\n        xp = cuda.cupy.get_array_module(*batch)\n        concat_example = []\n        for example in zip(*batch):\n            concat_example.append(xp.concatenate(example, axis=0))\n        concat_examples = xp.concatenate(concat_example, axis=0)\n        sections = numpy.cumsum([len(x) for x in batch], dtype=numpy.int32)\n        concat_examples = chainer.dataset.to_device(args.device, concat_examples)\n        batch = cuda.cupy.split(concat_examples, sections)\n        return batch\n\n", "CodeBleu_score": 55.418142160817716}, {"code": "def to_device_batch(batch):\n\t\n    if batch is None:\n        return None\n    elif isinstance(batch, (list, tuple)):\n        return [to_device_batch(item) for item in batch]\n    elif isinstance(batch, dict):\n        return {key: to_device_batch(value) for key, value in six.iteritems(batch)}\n    elif isinstance(batch, chainer.dataset.concat_examples):\n        return chainer.dataset.concat_examples([to_device_batch(e) for e in batch], device=None)\n    elif batch.ndim == 0:\n        return batch\n    elif batch.ndim == 1:\n        return to_device(batch, device)\n    else:\n        xp = cuda.get_array_module(batch[0])\n        concat = xp.concatenate(batch, axis=0)\n        sections = numpy.cumsum([len(x) for x in batch[:-1]], dtype=numpy.int32)\n        concat_dev = to_device(concat, device)\n        return cuda.cupy.split(concat_dev, sections)\n\n", "CodeBleu_score": 56.23962556499711}, {"code": "def to_device_batch(batch):\n\t\n    if batch is None:\n        return None\n    elif isinstance(batch, (list, tuple)):\n        return [to_device_batch(item) for item in batch]\n    elif isinstance(batch, dict):\n        return {key: to_device_batch(value) for key, value in batch.items()}\n    elif isinstance(batch, numpy.ndarray):\n        return cuda.to_gpu(batch)\n    else:\n        raise TypeError('batch must be list, tuple, dict, or ndarray.')\n\n", "CodeBleu_score": 21.833137584979063}]}
{"instance_id": "845c744d897c", "generation_result": [{"code": "def make_image(trainer)\n\t\n    preview_dir = '{}/preview'.format(os.path.dirname(\n        trainer.out))\n    chainer.backends.cuda.get_device_from_array(trainer.updater.get_optimizer('main').target.model.z.array).use()\n    trainer.updater.get_optimizer('main').target.model.z.array[:] = chainer.backends.cuda.get_array_module(\n        trainer.updater.get_optimizer('main').target.model.z.array).random.normal(0, 1, trainer.updater.get_optimizer('main').target.model.z.shape)\n    trainer.updater.get_optimizer('main').target.model.z.to_cpu()\n    images = trainer.updater.get_optimizer('main').target.model(\n        trainer.updater.get_optimizer('main').target.model.z, test=True)\n    images = chainer.backends.cuda.to_cpu(images.data)\n    images = (images * 127.5 + 127.5).clip(0.0, 255.0).astype('uint8')\n    _, _, H, W = images.shape\n    num_rows = int(trainer.updater.epoch ** 0.5)\n    num_cols = int(trainer.updater.epoch / num_rows)\n    if num_rows * num_cols < trainer.updater.epoch:\n        num_cols += 1\n    grid = images.reshape((num_rows, num_cols, 3, H, W)).transpose(\n        0, 3, 1, 4, 2).reshape((num_rows * H, num_cols * W, 3))\n    preview_path = preview_dir + '/image{:0>8}.png'.format(trainer.updater.epoch)\n    if not os.path.exists(preview_dir):\n        os.makedirs(preview_dir)\n", "CodeBleu_score": 43.81790709322912}, {"code": "def make_image(trainer)\n\t\n    preview_dir = '{}/preview'.format(trainer.out)\n    chainer.backends.cuda.get_device_from_array(trainer.updater.get_optimizer('main').target.array).use()\n    chainer.using_config('train', False)\n    xp = chainer.backends.cuda.get_array_module(trainer.updater.get_optimizer('main').target.array)\n    xp.random.seed(314159265)\n    n_images = 10\n    z = Variable(xp.asarray(trainer.updater.get_optimizer('main').target.make_hidden(n_images)))\n    x = trainer.updater.get_optimizer('main').target.gen(z)\n    x = chainer.backends.cuda.to_cpu(x.array)\n    x = x * 255\n    x = x.transpose((0, 2, 3, 1))\n    x = x.reshape((n_images, 28, 28))\n    if not os.path.exists(preview_dir):\n        os.makedirs(preview_dir)\n    Image.fromarray(x[0]).save(preview_dir + '/{}.png'.format(trainer.updater.iteration))", "CodeBleu_score": 41.4859370637166}, {"code": "def make_image(trainer)\n\t\n    preview_dir = '{}/preview'.format(trainer.out)\n    chainer.backends.cuda.get_device_from_array(trainer.updater.get_optimizer('main').target.array).use()\n    xp = chainer.backends.cuda.get_array_module(trainer.updater.get_optimizer('main').target.array)\n    xp.random.seed(31415)\n    n_images = 10\n    z = Variable(xp.asarray(trainer.updater.get_optimizer('main').target.make_hidden(n_images)))\n    with chainer.using_config('train', False):\n        x = trainer.updater.get_optimizer('main').target.gen(z)\n    x = chainer.backends.cuda.to_cpu(x.data)\n    x = x.transpose(0, 2, 3, 1)\n    x = x * 255\n    x = x.clip(0, 255).astype('uint8')\n    _, H, W, _ = x.shape\n    x = x.reshape((n_images, H, W, 3))\n    preview_path = preview_dir + '/image{:0>8}.png'.format(trainer.updater.iteration)\n    if not os.path.exists(preview_dir):\n        os.makedirs(preview_dir)\n    Image.fromarray(x[0]).save(preview_path)\n\n", "CodeBleu_score": 49.653050352683145}]}
{"instance_id": "fc2162d43aee", "generation_result": [{"code": "def _cuda_can_cast(from_dtype: npt.DTypeLike, to_dtype: npt.DTypeLike) -> bool\n\t\n    from_dtype = numpy.dtype(from_dtype)\n    to_dtype = numpy.dtype(to_dtype)\n    _typechars = \"?bhilqpBHILQP\"\n    return _typechars.find(to_dtype.char) >= _typechars.find(from_dtype.char)\n\n", "CodeBleu_score": 60.89240861863032}, {"code": "def _cuda_can_cast(from_dtype: npt.DTypeLike, to_dtype: npt.DTypeLike) -> bool\n\t\n    _typechars = \"?bBhHiIlLqQpPfd\"\n    from_dtype = numpy.dtype(from_dtype).char\n    to_dtype = numpy.dtype(to_dtype).char\n    if from_dtype not in _typechars:\n        raise TypeError(f\"{from_dtype} is not a valid data type.\")\n    if to_dtype not in _typechars:\n        raise TypeError(f\"{to_dtype} is not a valid data type.\")\n    return _typechars.index(from_dtype) <= _typechars.index(to_dtype)\n\n", "CodeBleu_score": 40.69303275813528}, {"code": "def _cuda_can_cast(from_dtype: npt.DTypeLike, to_dtype: npt.DTypeLike) -> bool\n\t\n    from_dtype = numpy.dtype(from_dtype)\n    to_dtype = numpy.dtype(to_dtype)\n    _typechars = \"?bhilqpBHILQP\"\n    try:\n        from_pos = _typechars.index(from_dtype.char)\n        to_pos = _typechars.index(to_dtype.char)\n        return from_pos <= to_pos\n    except ValueError:\n        return False\n\n", "CodeBleu_score": 60.228298685037394}]}
{"instance_id": "5b8e2d5e6e12", "generation_result": [{"code": "def manual_legend(g, labels, colors, **legend_kwargs)\n\t\n    if g is None:\n        ax = plt.gca()\n    elif isinstance(g, Visualizer):\n        ax = g.ax\n    elif isinstance(g, Axes):\n        ax = g\n    else:\n        raise ValueError(\n            \"g must be either a Visualizer, None, or an Axes object\")\n\n    if len(labels) != len(colors):\n        raise ValueError(\"labels and colors must have the same length\")\n\n    handles = [patches.Patch(color=c, label=l)\n               for l, c in zip(labels, colors)]\n    return ax.legend(handles=handles, **legend_kwargs)", "CodeBleu_score": 50.734857703168025}, {"code": "def manual_legend(g, labels, colors, **legend_kwargs)\n\t\n    if g is None:\n        ax = plt.gca()\n    elif isinstance(g, Visualizer):\n        ax = g.ax\n    else:\n        ax = g\n\n    if len(labels) != len(colors):\n        raise ValueError(\"Length of labels and colors must match\")\n\n    legend_handles = [patches.Patch(color=c, label=l) for l, c in zip(labels, colors)]\n    return ax.legend(handles=legend_handles, **legend_kwargs)\n\n", "CodeBleu_score": 51.474252806668645}, {"code": "def manual_legend(g, labels, colors, **legend_kwargs)\n\t\n    if g is None:\n        ax = plt.gca()\n    elif isinstance(g, Visualizer):\n        ax = g.ax\n    else:\n        ax = g\n\n    if len(labels) != len(colors):\n        raise ValueError(\"Length of labels and colors must match.\")\n\n    legend_handles = [patches.Patch(color=color, label=label) for label, color in zip(labels, colors)]\n    return ax.legend(handles=legend_handles, **legend_kwargs)", "CodeBleu_score": 52.653358081260095}]}
{"instance_id": "494e7f719eb6", "generation_result": [{"code": "def pcoords_time(X, y, fast=True)\n\t\n    # Create the visualizer\n    visualizer = ParallelCoordinates(fast=fast)\n\n    # Fit and transform the visualizer\n    start_time = time.time()\n    visualizer.fit_transform(X, y)\n    end_time = time.time()\n\n    # Clear and close the plot\n    visualizer.finalize()\n    plt.clf()\n    plt.close()\n\n    # Return the elapsed time\n    return end_time - start_time\n\n", "CodeBleu_score": 21.073634373012585}, {"code": "def pcoords_time(X, y, fast=True)\n\t\n    _, ax = plt.subplots()\n    viz = ParallelCoordinates(fast=fast, ax=ax)\n    start = time.time()\n    viz.fit_transform(X, y)\n    elapsed = time.time() - start\n    plt.close()\n    return elapsed\n\n", "CodeBleu_score": 44.257797303004}, {"code": "def pcoords_time(X, y, fast=True)\n\t\n    # Create the visualizer\n    visualizer = ParallelCoordinates(fast=fast)\n\n    # Fit and transform the visualizer\n    start_time = time.time()\n    visualizer.fit_transform(X, y)\n    elapsed_time = time.time() - start_time\n\n    # Clear and close the current plot to free up resources\n    visualizer.clear()\n    visualizer.close()\n\n    return elapsed_time\n\n", "CodeBleu_score": 20.178912026872037}]}
{"instance_id": "db4143bc029f", "generation_result": [{"code": "def draw_instances(self, X, y, **kwargs):\n\t\n    alpha = kwargs.pop('alpha', self.alpha)\n    for idx, x in enumerate(X):\n        label = y[idx]\n        color = self.get_color(label)\n        self.ax.scatter(range(len(x)), x, color=color, alpha=alpha, label=label)\n    return self.ax\n\n", "CodeBleu_score": 45.33622255645471}, {"code": "def draw_instances(self, X, y, **kwargs):\n\t\n    # Get the color for this instance\n    color = self.colors[y]\n\n    # Get the alpha transparency for this instance\n    alpha = self.alphas[y]\n\n    # TODO: make the instance drawing dynamic. If the data is more than 2\n    # dimensions, we should plot the first two or three dimensions on\n    # scatter plots, then project down to the first two dimensions and\n    # do a scatter plot there.\n    #\n    # Get the values for this instance\n    x1 = X[:, 0]\n    x2 = X[:, 1]\n\n    # Plot the values of this instance\n    self.ax.scatter(\n        x1, x2, c=color, alpha=alpha, label=str(y), **kwargs\n    )\n\n    # Return the axes object\n    return self.ax\n\n", "CodeBleu_score": 30.102216004397174}, {"code": "def draw_instances(self, X, y, **kwargs):\n\t\n\n    # Get the color map for drawing the instances\n    colors = self.colors\n\n    # Get the alpha transparency value from the object's attributes or defaults to 0.25\n    alpha = kwargs.pop('alpha', 0.25)\n\n    # Iterate over the data points\n    for idx, row in enumerate(X):\n\n        # Get the label for the current instance\n        label = y[idx]\n\n        # Determine the color for the current instance based on the label\n        color = colors[label]\n\n        # Extract the feature vector for the current instance\n        x = np.array([float(v) for v in row])\n\n        # Plot the feature vector against a predefined increment sequence on the axis\n        self.ax.plot(\n            np.arange(len(x)), x, color=color, label=label, alpha=alpha, **kwargs\n        )\n\n    # Return the axis object with the plotted instances\n    return self.ax\n\n", "CodeBleu_score": 42.157874044647336}]}
{"instance_id": "676433312ffe", "generation_result": [{"code": "def draw_classes(self, X, y, **kwargs):\n\t\n    colors = self.colors\n    alpha = kwargs.pop('alpha', self.alpha)\n\n    # Convert data to numpy arrays\n    X = np.array(X)\n    y = np.array(y)\n\n    # Get the unique labels, draw each in turn\n    labels = np.unique(y)\n    for label in labels:\n        # Get the color for this label\n        color = colors[label]\n\n        # Select the instances that match this label\n        indices = np.where(y == label)[0]\n        X_ = X[indices, :]\n\n        # Increment the X values to separate the lines\n        increments = np.arange(X_.shape[0])\n        increments = np.vstack((increments, np.ones(X_.shape[0]))).T\n        X_ = X_ + increments\n\n        # Plot the line\n        self.ax.plot(X_, color=color, alpha=alpha, label=label)\n\n    return self.ax\n\n", "CodeBleu_score": 34.16070801446708}, {"code": "def draw_classes(self, X, y, **kwargs):\n\t\n    alpha = kwargs.pop(\"alpha\", self.alpha)\n    X = np.c_[X, np.ones(X.shape[0])]\n    increments = [None] + list(np.where(y[:-1] != y[1:])[0]) + [None]\n\n    colors = resolve_colors(\n        n_colors=len(self.classes_), colormap=self.colormap, colors=self.colors\n    )\n\n    for idx, grp in enumerate(self.classes_):\n        # Get the color for this index\n        color = colors[idx]\n\n        # Subset the data to instances of this class\n        # TODO: Make this work for multi-index\n        subset = X[y == grp, :]\n\n        # Iterate over each line and plot\n        for line, increment in zip(subset, increments):\n            self.ax.plot(\n                line,\n                color=color,\n                alpha=alpha,\n                label=None if idx > 0 else str(grp),\n                **kwargs,\n            )\n            if increment is not None:\n                self.ax.axvline(increment, color=\"k\", linestyle=\"--\")\n\n    return self.ax\n\n", "CodeBleu_score": 39.97898588489288}, {"code": "def draw_classes(self, X, y, **kwargs):\n\t\n    alpha = kwargs.pop('alpha', self.alpha)\n    X = np.c_[X, np.ones(X.shape[0])]\n    increments = [None] + list(range(len(X)))\n\n    classes = np.unique(y)\n    for label in classes:\n        # Get the color for this label\n        color = self.colors[label]\n\n        # Select only the instances belonging to this class\n        instances = X[y == label]\n\n        # Plot them as a line plot\n        self.ax.plot(\n            increments,\n            instances.T,\n            color=color,\n            alpha=alpha,\n            label=label,\n            **kwargs\n        )\n\n    return self.ax\n\n", "CodeBleu_score": 39.36586015771702}]}
{"instance_id": "9d297b12a673", "generation_result": [{"code": "def feature_analysis(fname=\"feature_analysis.png\")\n\t\n    # Load data\n    X, y = make_blobs(n_samples=500, n_features=12, centers=[[-2, -2], [-1, -1], [0, 0], [1, 1], [2, 2]])\n\n    # Instantiate the visualizer\n    visualizer = RadViz(size=(500, 500))\n\n    # Fit the data to the visualizer\n    visualizer.fit(X, y)\n\n    # Finalize the visualizer\n    visualizer.finalize()\n\n    # Save the figure\n    visualizer.show(outpath=fname)\n\n", "CodeBleu_score": 17.96003321550405}, {"code": "def feature_analysis(fname=\"feature_analysis.png\")\n\t\n    # Load a regression dataset\n    X, y = make_blobs(\n        n_samples=100, n_features=10, centers=3, cluster_std=1.0, random_state=42\n    )\n\n    # Instantiate the visualizer with the Covariance ranking algorithm\n    visualizer = Rank2D(features=None, algorithm=\"covariance\")\n\n    visualizer.fit(X, y)  # Fit the data to the visualizer\n    visualizer.transform(X)  # Transform the data\n    visualizer.finalize()  # Finalize the visualizer\n\n    # Instantiate the visualizer with the Pearson ranking algorithm\n    visualizer = Rank2D(features=None, algorithm=\"pearson\")\n\n    visualizer.fit(X, y)  # Fit the data to the visualizer\n    visualizer.transform(X)  # Transform the data\n    visualizer.finalize()  # Finalize the visualizer\n\n    # Load the classification data set\n    X, y = make_blobs(\n        n_samples=1000,\n        n_features=10,\n        centers=3,\n        cluster_std=1.0,\n        random_state=42,\n    )\n\n    # Instantiate the visualizer with the Shapiro-Wilk ranking algorithm\n    visualizer = Rank2D(features=None, algorithm=\"shapiro\")\n\n    visualizer.fit(X, y)  # Fit the data to the visualizer\n    visualizer.transform(X)  # Transform the data\n    visualizer.finalize()  # Finalize the visualizer\n\n    # Show the Rank2D visualization\n    visualizer.show(outpath=fname)\n\n", "CodeBleu_score": 13.00971643278588}, {"code": "def feature_analysis(fname=\"feature_analysis.png\")\n\t\n    # Generate a sample dataset\n    X, y = make_blobs(n_samples=1000, n_features=12, centers=[[-2, -2], [-1, -1], [0, 0], [1, 1], [2, 2]], cluster_std=0.5, random_state=42)\n\n    # Create a figure with a specified size\n    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\n\n    # Perform RadViz analysis on the left subplot\n    radviz = RadViz(ax=ax[0])\n    radviz.fit(X, y)\n    radviz.transform(X)\n    radviz.finalize()\n\n    # Perform Rank2D analysis on the right subplot\n    rank2d = Rank2D(ax=ax[1])\n    rank2d.fit(X, y)\n    rank2d.transform(X)\n    rank2d.finalize()\n\n    # Save the figure to a file\n    plt.savefig(fname)\n\n", "CodeBleu_score": 31.049878889829145}]}
{"instance_id": "fd1b4b9cd7ef", "generation_result": [{"code": "def regression(fname=\"regression.png\")\n\t\n    fig, ax = plt.subplots(nrows=2, ncols=1, figsize=(18, 6))\n    alphas = np.logspace(-10, 1, 400)\n\n    # Load the regression dataset\n    X, y = load_concrete()\n\n    # Create the train and test data\n    X_train, X_test, y_train, y_test = tts(X, y, test_size=0.2, random_state=42)\n\n    # Plot a RidgeCV prediction error curve\n    visualizer = PredictionError(RidgeCV(alphas=alphas, cv=12), ax=ax[0])\n    visualizer.fit(X_train, y_train)\n    visualizer.score(X_test, y_test)\n    visualizer.finalize()\n\n    # Plot a LassoCV residuals plot\n    visualizer = ResidualsPlot(LassoCV(alphas=alphas, cv=12), ax=ax[1])\n    visualizer.fit(X_train, y_train)\n    visualizer.score(X_test, y_test)\n    visualizer.finalize()\n\n    fig.tight_layout()\n    path = os.path.join(os.getcwd(), \"images\", fname)\n    fig.savefig(path)\n    return path\n\n", "CodeBleu_score": 47.89815899300167}, {"code": "def regression(fname=\"regression.png\")\n\t\n    # Create a figure with two subplots\n    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(18, 6))\n\n    # Define a range of alpha values for regularization\n    alphas = np.logspace(-10, 1, 200)\n\n    # Load the concrete dataset and split it into training and testing sets\n    X, y = load_concrete()\n    X_train, X_test, y_train, y_test = tts(X, y, test_size=0.2, random_state=42)\n\n    # Plot the prediction error using LassoCV\n    lasso = LassoCV(alphas=alphas, cv=10, max_iter=100000)\n    lasso.fit(X_train, y_train)\n    visualizer = PredictionError(lasso, ax=ax[0])\n    visualizer.fit(X_train, y_train)\n    visualizer.score(X_test, y_test)\n    visualizer.finalize()\n\n    # Plot the residuals using RidgeCV\n    ridge = RidgeCV(alphas=alphas, cv=10)\n    ridge.fit(X_train, y_train)\n    visualizer = ResidualsPlot(ridge, ax=ax[1])\n    visualizer.fit(X_train, y_train)\n    visualizer.score(X_test, y_test)\n    visualizer.finalize()\n\n    # Adjust the layout and save the figure\n    fig.tight_layout()\n    path = os.path.join(IMAGES, fname)\n    plt.savefig(path)\n    return path\n\n", "CodeBleu_score": 45.785945907158904}, {"code": "def regression(fname=\"regression.png\")\n\t\n    # Create a figure with two subplots of size 18x6 inches\n    fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(18, 6))\n\n    # Define a range of alpha values for regularization\n    alphas = np.logspace(-10, 1, 400)\n\n    # Load the concrete dataset and split it into training and testing sets\n    X, y = load_concrete()\n    X_train, X_test, y_train, y_test = tts(\n        X, y, test_size=0.2, random_state=42\n    )\n\n    # Plot the prediction error using LassoCV\n    lasso = LassoCV(alphas=alphas, cv=5)\n    lasso.fit(X_train, y_train)\n    visualizer = PredictionError(lasso, ax=axes[0])\n    visualizer.fit(X_train, y_train)\n    visualizer.score(X_test, y_test)\n    visualizer.finalize()\n\n    # Plot the residuals using RidgeCV\n    ridge = RidgeCV(alphas=alphas, cv=5)\n    ridge.fit(X_train, y_train)\n    visualizer = ResidualsPlot(ridge, ax=axes[1])\n    visualizer.fit(X_train, y_train)\n    visualizer.score(X_test, y_test)\n    visualizer.finalize()\n\n    # Adjust the layout\n    fig.tight_layout()\n\n    # Save the figure\n    path = os.path.join(os.getcwd(), \"figures\", fname)\n    plt.savefig(path)\n\n    # Return the path\n    return path\n\n", "CodeBleu_score": 46.34262648835313}]}
{"instance_id": "627126918f88", "generation_result": [{"code": "def classification(fname=\"classification.png\")\n\t\n    X, y = load_spam()\n    X_train, X_test, y_train, y_test = tts(X, y)\n    visualizer = ClassificationReport(MultinomialNB(), size=(800, 600))\n    visualizer.fit(X_train, y_train)\n    visualizer.score(X_test, y_test)\n    visualizer.finalize()\n    visualizer.set_title(\"Classification Report\")\n    visualizer.ax.set_xticklabels([\"not spam\", \"spam\"])\n    visualizer.ax.set_yticklabels([\"not spam\", \"spam\"])\n    visualizer.ax.set_xlabel(\"Predicted class\")\n    visualizer.ax.set_ylabel(\"True class\")\n    visualizer.ax.set_title(\"Classification Report\")\n    visualizer.ax.legend(loc=\"upper right\")\n    visualizer.ax.grid(False)\n    visualizer.ax.set_ylim(1.5, -0.5)\n    visualizer.ax.set_xlim(-0.5, 1.5)\n    visualizer.ax.set_xticks([0, 1])\n    visualizer.ax.set_yticks([0, 1])\n    visualizer.ax.set_xticklabels([\"not spam\", \"spam\"])\n    visualizer.ax.set_yticklabels([\"not spam\", \"spam\"])\n    visualizer.ax.set_xlabel(\"Predicted class\")\n    visualizer.ax.set_ylabel(\"True class\")\n    visualizer.ax.set_title(\"Classification Report\")\n    visualizer.ax.legend(loc=\"upper right\")\n    visualizer.ax.grid(False)\n    visualizer.ax.set_ylim(1.5, -0.5)\n    visualizer.ax.set_xlim(-0.5, 1.5)\n    visualizer.ax.set_xticks([0, 1])\n    visualizer.ax.set_yticks([0, 1])\n    visual", "CodeBleu_score": 19.881756140271275}, {"code": "def classification(fname=\"classification.png\")\n\t\n    X, y = make_blobs(n_samples=500, n_features=12, centers=4, random_state=42)\n    X_train, X_test, y_train, y_test = tts(X, y, test_size=0.2, random_state=42)\n    model = MultinomialNB()\n    model.fit(X_train, y_train)\n    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))\n    ax1, ax2 = axes.flatten()\n    visualizer = ClassificationReport(model, ax=ax1)\n    visualizer.fit(X_train, y_train)\n    visualizer.score(X_test, y_test)\n    visualizer.finalize()\n    visualizer = DiscriminationThreshold(model, ax=ax2)\n    visualizer.fit(X, y)\n    visualizer.finalize()\n    fig.tight_layout()\n    plt.savefig(fname)\n\n", "CodeBleu_score": 22.77415614116777}, {"code": "def classification(fname=\"classification.png\")\n\t\n    # Load a classification dataset\n    X, y = make_blobs(\n        n_samples=1000, n_features=10, centers=5, cluster_std=3, random_state=42\n    )\n    X_train, X_test, y_train, y_test = tts(X, y, test_size=0.2, random_state=42)\n\n    # Instantiate the classification model and visualizer\n    model = MultinomialNB()\n    visualizer = ClassificationReport(model, classes=[\"class_0\", \"class_1\"], support=True)\n\n    # Fit the visualizer and the model\n    visualizer.fit(X_train, y_train)\n    visualizer.score(X_test, y_test)\n    visualizer.finalize()\n\n    # Load a classification dataset\n    X, y = make_blobs(\n        n_samples=1000, n_features=10, centers=5, cluster_std=3, random_state=42\n    )\n\n    # Instantiate the classification model and visualizer\n    model = LogisticRegression()\n    visualizer = DiscriminationThreshold(model)\n\n    # Fit the visualizer and the model\n    visualizer.fit(X, y)\n    visualizer.finalize()\n\n    # Combine the two figures into a single grid\n    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))\n    axes[0].imshow(visualizer.classification_report_.figure_.canvas.buffer_rgba())\n    axes[1].imshow(visualizer.discrimination_threshold_.figure_.canvas.buffer_rgba())\n\n    # Adjust the layout and save the figure\n    plt.tight_layout()\n    plt.savefig(fname)\n\n", "CodeBleu_score": 23.39849965003452}]}
{"instance_id": "318d3a674694", "generation_result": [{"code": "def get_color_cycle()\n\t\n    try:\n        import matplotlib\n        if LooseVersion(matplotlib.__version__) >= LooseVersion(\"1.5.0\"):\n            from matplotlib.rcsetup import cycler\n            return [x[\"color\"] for x in cycler(matplotlib.rcParams[\"axes.prop_cycle\"])]\n        return matplotlib.rcParams[\"axes.color_cycle\"]\n    except ImportError:\n        return []\n\n", "CodeBleu_score": 48.66981913472493}, {"code": "def get_color_cycle()\n\t\n    try:\n        from matplotlib import pyplot as plt\n        from matplotlib import __version__ as mpl_version\n        if LooseVersion(mpl_version) >= LooseVersion(\"1.5.0\"):\n            return [prop[\"color\"] for prop in plt.rcParams[\"axes.prop_cycle\"]]\n        else:\n            return plt.rcParams[\"axes.color_cycle\"]\n    except ImportError:\n        return []\n\n", "CodeBleu_score": 42.296112883866456}, {"code": "def get_color_cycle()\n\t\n    try:\n        import matplotlib as mpl\n        if LooseVersion(mpl.__version__) >= LooseVersion(\"1.5.0\"):\n            cycler = mpl.rcParams[\"axes.prop_cycle\"]\n            return cycler.by_key()[\"color\"]\n        else:\n            return mpl.rcParams[\"axes.color_cycle\"]\n    except KeyError:\n        return mpl.rcParams[\"axes.color_cycle\"]\n\n", "CodeBleu_score": 40.80472881947503}]}
{"instance_id": "f3fac557334c", "generation_result": [{"code": "def plot_knee_normalized(\n        self,\n    )\n\t\n\n    # Importing the necessary libraries\n    import matplotlib.pyplot as plt\n\n    # Setting the figure size\n    plt.figure(figsize=(8, 8))\n\n    # Plotting the normalized x and y values\n    plt.plot(self.normalized_x, self.normalized_y)\n\n    # Plotting the distance curve in red\n    plt.plot(self.normalized_x, self.distance_curve, 'r')\n\n    # Setting the x and y ticks at intervals of 0.1 based on the minimum and maximum values of the respective data\n    plt.xticks(\n        [\n            i / 10 for i in range(\n                int(min(self.normalized_x) * 10),\n                int(max(self.normalized_x) * 10) + 1\n            )\n        ]\n    )\n    plt.yticks(\n        [\n            i / 10 for i in range(\n                int(min(self.normalized_y) * 10),\n                int(max(self.normalized_y) * 10) + 1\n            )\n        ]\n    )\n\n    # Drawing a vertical line at the knee point to highlight it on the plot\n    plt.vlines(self.knee_point, 0, 1, colors='r', linestyles='dashed')\n\n    # Displaying the plot\n    plt.show()\n\n", "CodeBleu_score": 42.140219464572716}, {"code": "def plot_knee_normalized(\n        self,\n    )\n\t\n    import matplotlib.pyplot as plt\n\n    # Plot the normalized curve\n    plt.figure(figsize=(8, 8))\n    plt.plot(self.normalized_x, self.normalized_y)\n    plt.xlabel('Normalized X')\n    plt.ylabel('Normalized Y')\n\n    # Plot the distance curve\n    plt.plot(self.normalized_x, self.distance_curve, 'r')\n\n    # Set the x and y ticks\n    plt.xticks(np.arange(min(self.normalized_x), max(self.normalized_x) + 0.1, 0.1))\n    plt.yticks(np.arange(min(self.normalized_y), max(self.normalized_y) + 0.1, 0.1))\n\n    # Draw a vertical line at the knee point\n    if self.knee_point is not None:\n        plt.axvline(x=self.knee_point, color='k', linestyle='--')\n\n    plt.show()\n\n", "CodeBleu_score": 47.50263250472593}, {"code": "def plot_knee_normalized(\n        self,\n    )\n\t\n    import matplotlib.pyplot as plt\n\n    # Plot the normalized curve\n    plt.figure(figsize=(8, 8))\n    plt.plot(self.normalized_x, self.normalized_y)\n\n    # Plot the distance curve in red\n    plt.plot(self.normalized_x, self.distance, color=\"red\")\n\n    # Set the x and y ticks at intervals of 0.1 based on the minimum and maximum values of the respective data\n    plt.xticks(\n        ticks=[\n            i / 10 for i in range(\n                int(min(self.normalized_x) * 10), int(max(self.normalized_x) * 10) + 1\n            )\n        ],\n        labels=[\n            i / 10 for i in range(\n                int(min(self.normalized_x) * 10), int(max(self.normalized_x) * 10) + 1\n            )\n        ],\n    )\n    plt.yticks(\n        ticks=[\n            i / 10 for i in range(\n                int(min(self.normalized_y) * 10), int(max(self.normalized_y) * 10) + 1\n            )\n        ],\n        labels=[\n            i / 10 for i in range(\n                int(min(self.normalized_y) * 10), int(max(self.normalized_y) * 10) + 1\n            )\n        ],\n    )\n\n    # Draw a vertical line at the knee point to highlight it on the plot\n    if self.knee_point is not None:\n        plt.axvline(x=self.knee_point, color=\"green\")\n\n    plt.show()\n\n", "CodeBleu_score": 38.26364522354193}]}
{"instance_id": "b2d48aae4fce", "generation_result": [{"code": "def plot_knee(\n        self,\n    )\n\t\n    import matplotlib.pyplot as plt\n\n    # Create a figure with a specific size\n    fig, ax = plt.subplots(figsize=(8, 8))\n\n    # Plot the data points\n    ax.plot(self.x, self.y, 'o-')\n\n    # Draw a vertical line at the knee point\n    if self.knee_point is not None:\n        ax.axvline(x=self.knee_point, color='r', linestyle='--')\n\n    # Set the x and y limits of the plot\n    ax.set_xlim(0, len(self.x))\n    ax.set_ylim(min(self.y), max(self.y))\n\n    # Display the plot\n    plt.show()\n\n", "CodeBleu_score": 36.986139129396435}, {"code": "def plot_knee(\n        self,\n    )\n\t\n    # Plot the curve\n    fig, ax = plt.subplots(figsize=(8, 8))\n    ax.plot(self.x, self.y)\n\n    # Draw a vertical line at the knee point\n    if self.knee_point:\n        ax.axvline(x=self.knee_point, color='r', linestyle='--')\n\n    # Display the plot\n    plt.show()\n\n", "CodeBleu_score": 40.231504232514084}, {"code": "def plot_knee(\n        self,\n    )\n\t\n    # Check if the knee point exists\n    if self.knee_point is None:\n        warnings.warn(\n            \"There is no knee point to plot. Please call the fit method first.\",\n            YellowbrickWarning,\n        )\n        return\n\n    # Create a figure and axis\n    fig, ax = plt.subplots(figsize=(8, 8))\n\n    # Plot the data points\n    ax.plot(self.x, self.y, \"o-\")\n\n    # Draw a vertical line at the knee point\n    ax.axvline(x=self.knee_point, color=\"r\", linestyle=\"--\")\n\n    # Set the title and labels\n    ax.set_title(\"Knee Point Detection\")\n    ax.set_xlabel(\"X\")\n    ax.set_ylabel(\"Y\")\n\n    # Show the plot\n    plt.show()\n\n", "CodeBleu_score": 37.875366809752634}]}
{"instance_id": "4450ba2384c9", "generation_result": [{"code": "def plot_pca_interactive(data, n_components=6)\n\t\n    from sklearn.decomposition import PCA\n    from ipywidgets import interact\n\n    pca = PCA(n_components=n_components)\n    Xproj = pca.fit_transform(data)\n\n    def show_decomp(i=0):\n        \"\"\"# This function plots the original data and its PCA components for a selected index.", "CodeBleu_score": 60.37553317381239}, {"code": "def plot_pca_interactive(data, n_components=6)\n\t\n    from sklearn.decomposition import PCA\n    from ipywidgets import interact\n\n    pca = PCA(n_components=n_components)\n    X_pca = pca.fit_transform(data)\n\n    def show_decomp(i=0):\n        \"\"\"Plot the original data and its PCA components for a selected index.\"\"\"\n        plt.figure(figsize=(12, 8))\n        plt.subplot(2, 2, 1)\n        if X.shape[1] == 2:\n            plt.scatter(X[:, 0], X[:, 1], c=y)\n            plt.xlabel('Feature 1')\n            plt.ylabel('Feature 2')\n        else:\n            plt.text(0.1, 0.5, \"Only 2D data is supported for visualization.\")\n\n        plt.subplot(2, 2, 2)\n        plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y)\n        plt.xlabel('PC1')\n        plt.ylabel('PC2')\n\n        plt.subplot(2, 2, 3)\n        plt.scatter(X_pca[:, 0], X_pca[:, 2], c=y)\n        plt.xlabel('PC1')\n        plt.ylabel('PC3')\n\n        plt.subplot(2, 2, 4)\n        plt.scatter(X_pca[:, 1], X_pca[:, 2], c=y)\n        plt.xlabel('PC2')\n        plt.ylabel('PC3')\n\n        plt.suptitle(f\"PCA Decomposition for Index {i}\")\n        plt.tight_layout()\n        plt.show()\n\n    interact(show_decomp, i=(0, len(data)-1))\n\n", "CodeBleu_score": 52.48793136708755}, {"code": "def plot_pca_interactive(data, n_components=6)\n\t\n    from sklearn.decomposition import PCA\n    from ipywidgets import interact\n\n    pca = PCA(n_components=n_components)\n    Xproj = pca.fit_transform(data)\n\n    def show_decomp(i=0):\n        \"\"\"# This function displays the original data and its PCA components for a selected index.\n        It imports the necessary pyplot module from matplotlib.", "CodeBleu_score": 61.97945657445298}]}
{"instance_id": "b09a163f3ff1", "generation_result": [{"code": "def plot_iris_knn()\n\t\n    import matplotlib.pyplot as plt\n    import numpy as np\n    from sklearn import datasets\n    from sklearn.neighbors import KNeighborsClassifier\n\n    # Load the Iris dataset\n    iris = datasets.load_iris()\n\n    # Extract the first two features and the target labels\n    X = iris.data[:, :2]\n    y = iris.target\n\n    # Create a kNN classifier with 5 neighbors\n    knn = KNeighborsClassifier(n_neighbors=5)\n\n    # Train the kNN classifier on the Iris dataset\n    knn.fit(X, y)\n\n    # Define the mesh grid for plotting decision boundaries\n    h = 0.02  # step size in the mesh\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n\n    # Predict the class for each point in the mesh grid\n    Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n\n    # Reshape the predictions and plot the decision boundaries\n    Z = Z.reshape(xx.shape)\n    plt.figure(figsize=(8, 6))\n    plt.contourf(xx, yy, Z, cmap=ListedColormap([\"orange\", \"cyan\", \"cornflowerblue\"]), alpha=0.5)\n\n    # Plot the training points\n    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors=\"k\", cmap=ListedColormap([\"red\", \"green\", \"blue\"]))\n\n    # Set the axis labels and title\n    plt.xlabel(\"Sepal length\")\n    plt.ylabel", "CodeBleu_score": 70.2528396855907}, {"code": "def plot_iris_knn()\n\t\n    import matplotlib.pyplot as plt\n    import numpy as np\n    from sklearn import datasets\n    from sklearn.neighbors import KNeighborsClassifier\n\n    # Load the Iris dataset\n    iris = datasets.load_iris()\n    X = iris.data[:, :2]  # Use the first two features\n    y = iris.target\n\n    # Create a kNN classifier with 5 neighbors\n    knn = KNeighborsClassifier(n_neighbors=5)\n\n    # Fit the classifier to the data\n    knn.fit(X, y)\n\n    # Create a mesh grid over the feature space\n    h = 0.02  # Step size in the mesh\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n\n    # Predict the class for each point in the mesh grid\n    Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n\n    # Reshape the predictions to match the mesh grid\n    Z = Z.reshape(xx.shape)\n\n    # Create a color map for the classes\n    cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n    cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n\n    # Plot the decision boundaries\n    plt.figure()\n    plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n\n    # Plot the training points\n    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold, edgecolor='k', s=", "CodeBleu_score": 69.00908026221637}, {"code": "def plot_iris_knn()\n\t\n    import matplotlib.pyplot as plt\n    import numpy as np\n\n    # Load the Iris dataset\n    iris = datasets.load_iris()\n\n    # Extract the first two features and target labels\n    X = iris.data[:, :2]\n    y = iris.target\n\n    # Define the kNN classifier with 5 neighbors\n    knn = neighbors.KNeighborsClassifier(n_neighbors=5)\n\n    # Fit the kNN classifier on the data\n    knn.fit(X, y)\n\n    # Define the mesh grid over the feature space\n    h = .02  # step size in the mesh\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n\n    # Predict the class for each point in the mesh grid\n    Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n\n    # Reshape the predictions and plot the decision boundaries\n    Z = Z.reshape(xx.shape)\n    plt.figure()\n    plt.pcolormesh(xx, yy, Z, cmap=ListedColormap(\n        ['#FFAAAA', '#AAFFAA', '#AAAAFF']))\n\n    # Plot the training points\n    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=ListedColormap(\n        ['#FF0000', '#00FF00', '#0000FF']))\n\n    # Add labels and title\n    plt.xlabel('Sepal length')\n    plt.ylabel('Sepal width')\n    plt.title('kNN decision boundaries on Iris dataset')\n\n    # Show the plot\n   ", "CodeBleu_score": 67.9814115782219}]}
{"instance_id": "8eaee3207c09", "generation_result": [{"code": "def plot_sgd_separator()\n\t\n    # we create 50 separable points\n    X, Y = make_blobs(n_samples=50, centers=2,\n                      random_state=0, cluster_std=0.60)\n    # fit the model\n    clf = SGDClassifier(loss=\"hinge\", alpha=0.01, max_iter=200, fit_intercept=True)\n    clf.fit(X, Y)\n\n    # plot the line, the points, and the nearest vectors to the plane\n    xx = np.linspace(-1, 5, 10)\n    yy = np.linspace(-1, 5, 10)\n\n    X1, X2 = np.meshgrid(xx, yy)\n    Z = np.empty(X1.shape)\n    for (i, j), val in np.ndenumerate(X1):\n        x1 = val\n        x2 = X2[i, j]\n        p = clf.decision_function([[x1, x2]])\n        Z[i, j] = p[0]\n    levels = [-1.0, 0.0, 1.0]\n    linestyles = ['dashed', 'solid', 'dashed']\n    colors = 'k'\n\n    ax = plt.axes()\n    ax.contour(X1, X2, Z, levels, colors=colors, linestyles=linestyles)\n    ax.scatter(X[:, 0], X[:, 1], c=Y, cmap=plt.cm.Paired)\n\n    ax.axis('tight')\n\n", "CodeBleu_score": 67.35610203912931}, {"code": "def plot_sgd_separator()\n\t\n    # we create 50 separable points\n    X, Y = make_blobs(n_samples=50, centers=2,\n                      random_state=0, cluster_std=0.60)\n\n    # fit the model\n    clf = SGDClassifier(loss=\"hinge\", alpha=0.01, max_iter=200, fit_intercept=True)\n    clf.fit(X, Y)\n\n    # plot the line, the points, and the nearest vectors to the plane\n    xx = np.linspace(-1, 5, 10)\n    yy = np.linspace(-1, 5, 10)\n\n    X1, X2 = np.meshgrid(xx, yy)\n    Z = np.empty(X1.shape)\n    for (i, j), val in np.ndenumerate(X1):\n        x1 = val\n        x2 = X2[i, j]\n        p = clf.decision_function([[x1, x2]])\n        Z[i, j] = p[0]\n    levels = [-1.0, 0.0, 1.0]\n    linestyles = ['dashed', 'solid', 'dashed']\n    colors = 'k'\n\n    ax = plt.axes()\n    ax.contour(X1, X2, Z, levels, colors=colors, linestyles=linestyles)\n\n    ax.scatter(X[:, 0], X[:, 1], c=Y, cmap=plt.cm.Paired)\n\n    ax.axis('tight')\n\n", "CodeBleu_score": 67.35610203912931}, {"code": "def plot_sgd_separator()\n\t\n    # we create 50 separable points\n    X, Y = make_blobs(n_samples=50, centers=2,\n                      random_state=0, cluster_std=0.60)\n\n    # fit the model\n    clf = SGDClassifier(loss=\"hinge\", alpha=0.01, max_iter=200,\n                        fit_intercept=True)\n    clf.fit(X, Y)\n\n    # plot the line, the points, and the nearest vectors to the plane\n    xx = np.linspace(-1, 5, 10)\n    yy = np.linspace(-1, 5, 10)\n\n    X1, X2 = np.meshgrid(xx, yy)\n    Z = np.empty(X1.shape)\n    for (i, j), val in np.ndenumerate(X1):\n        x1 = val\n        x2 = X2[i, j]\n        p = clf.decision_function([[x1, x2]])\n        Z[i, j] = p[0]\n    levels = [-1.0, 0.0, 1.0]\n    linestyles = ['dashed', 'solid', 'dashed']\n    colors = 'k'\n\n    ax = plt.axes()\n    ax.contour(X1, X2, Z, levels, colors=colors, linestyles=linestyles)\n\n    ax.scatter(X[:, 0], X[:, 1], c=Y, cmap=plt.cm.Paired,\n               edgecolor='black', s=20)\n\n    ax.axis('tight')\n\n", "CodeBleu_score": 66.84391211048816}]}
{"instance_id": "8b54f999b4c0", "generation_result": [{"code": "def dump_svmlight_file(self, file)\n\t\n    X = np.array(self.data)\n    y = X[:, 2]\n    X = X[:, :2]\n    dump_svmlight_file(X, y, file)\n\n", "CodeBleu_score": 30.29354181628772}, {"code": "def dump_svmlight_file(self, file)\n\t\n    X = np.asarray(self.data)\n    y = X[:, 2]\n    X = X[:, :2]\n    dump_svmlight_file(X, y, file)\n\n", "CodeBleu_score": 29.97306602712802}, {"code": "def dump_svmlight_file(self, file)\n\t\n    X = self.data[:, :2]\n    y = self.data[:, 2]\n    dump_svmlight_file(X, y, file)\n\n", "CodeBleu_score": 40.86883949371065}]}
{"instance_id": "2419013adc6e", "generation_result": [{"code": "def fit(self)\n\t\n    self.print_message(\"Fitting model\", \"Fitting model\")\n    X = self.data.data\n    y = self.data.target\n    C = self.complexity\n    gamma = self.gamma\n    coef0 = self.coef0\n    degree = self.degree\n    kernel = self.kernel\n    if all(y == y[0]):\n        self.model = svm.OneClassSVM(kernel=kernel, degree=degree, gamma=gamma, coef0=coef0, nu=0.5, shrinking=True, cache_size=200, verbose=False, max_iter=-1)\n    else:\n        self.model = svm.SVC(kernel=kernel, degree=degree, gamma=gamma, coef0=coef0, C=C, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape=None, random_state=None)\n    self.model.fit(X, y)\n    if hasattr(self.model, \"score\"):\n        self.print_message(\"Accuracy on training data: %g %%\" % (self.model.score(X, y) * 100), \"Accuracy on training data: %g %%\" % (self.model.score(X, y) * 100))\n    self.surface = self.calculate_surface()\n    self.surface_type = \"decision\"\n    self.fitted = True\n    self.trigger(\"changed_surface\")\n\n", "CodeBleu_score": 38.13487463313963}, {"code": "def fit(self)\n\t\n    print(\"Fitting the model\")\n    X = self.data[:, 0:2]\n    y = self.data[:, 2]\n    C = self.complexity\n    gamma = self.gamma\n    coef0 = self.coef0\n    degree = self.degree\n    kernel = self.kernel\n    if np.all(y == y[0]):\n        self.classifier = svm.OneClassSVM(kernel=kernel, gamma=gamma, coef0=coef0, degree=degree)\n    else:\n        self.classifier = svm.SVC(kernel=kernel, C=C, gamma=gamma, coef0=coef0, degree=degree)\n    self.classifier.fit(X, y)\n    if hasattr(self.classifier, \"score\"):\n        print(\"Accuracy:\", self.classifier.score(X, y))\n    self.calculate_surface()\n    self.surface_type = self.surface_types[self.show_surface]\n    self.fitted = True\n    self.trigger(\"changed_surface\")\n\n", "CodeBleu_score": 33.76483370101277}, {"code": "def fit(self)\n\t\n    print(\"fitting the model ...\")\n    X = self.data[:, 0:2]\n    y = self.data[:, 2]\n    C = self.complexity\n    gamma = self.gamma\n    coef0 = self.coef0\n    degree = self.degree\n    kernel = self.kernel\n    if np.all(y == y[0]):\n        clf = svm.OneClassSVM(kernel=kernel, gamma=gamma, coef0=coef0, degree=degree)\n    else:\n        clf = svm.SVC(C=C, kernel=kernel, gamma=gamma, coef0=coef0, degree=degree)\n    clf.fit(X, y)\n    if hasattr(clf, \"score\"):\n        print(\"Accuracy:\", clf.score(X, y))\n    self.decision_surface = clf.decision_function(X)\n    self.surface_type = \"svc\"\n    self.fitted = True\n    self.trigger(\"changed_surface\")\n\n", "CodeBleu_score": 33.85574067171267}]}
{"instance_id": "e887fd2a8513", "generation_result": [{"code": "def source_decode(sourcecode, verbose=0)\n\t\n    if sourcecode.startswith(\"tpot.\"):\n        import_string = sourcecode.replace(\"tpot.\", \"\")\n        object_string = import_string.split(\".\")[-1]\n        import_string = \".\".join(import_string.split(\".\")[:-1])\n        try:\n            imported_object = getattr(__import__(import_string, fromlist=[object_string]), object_string)\n        except ImportError as e:\n            if verbose > 2:\n                raise e\n            else:\n                print(\"Warning: import of {} failed. Check that the module and object are valid.\".format(sourcecode))\n                return None, None, None\n    else:\n        import_string = sourcecode\n        object_string = import_string.split(\".\")[-1]\n        import_string = \".\".join(import_string.split(\".\")[:-1])\n        try:\n            imported_object = getattr(__import__(import_string, fromlist=[object_string]), object_string)\n        except ImportError as e:\n            if verbose > 2:\n                raise e\n            else:\n                print(\"Warning: import of {} failed. Check that the module and object are valid.\".format(sourcecode))\n                return None, None, None\n    return import_string, object_string, imported_object\n\n", "CodeBleu_score": 39.38598768325423}, {"code": "def source_decode(sourcecode, verbose=0)\n\t\n    import_string, object_string = sourcecode.rsplit(\".\", 1)\n    if import_string == \"tpot\":\n        import_string = \"tpot.builtins\"\n    try:\n        imported_object = getattr(__import__(import_string, fromlist=[object_string]), object_string)\n    except ImportError as e:\n        if verbose > 2:\n            raise ImportError(\"{}: {}\".format(sourcecode, e))\n        else:\n            print(\"Warning: {} could not be imported. This pipeline will be treated as a custom classifier or regressor.\".format(sourcecode))\n            return None\n    return import_string, object_string, imported_object\n\n", "CodeBleu_score": 36.692837593620524}, {"code": "def source_decode(sourcecode, verbose=0)\n\t\n    import_string, object_string = sourcecode.split(\".\")\n    if import_string == \"tpot\":\n        import_string = \".\".join(sourcecode.split(\".\")[1:])\n    try:\n        imported_object = getattr(__import__(import_string, fromlist=[object_string]), object_string)\n    except ImportError as e:\n        if verbose > 2:\n            raise e\n        else:\n            print(\"Warning: Could not import object %s from module %s.\" % (object_string, import_string))\n            return None\n    return import_string, object_string, imported_object\n\n", "CodeBleu_score": 38.24797943650305}]}
{"instance_id": "5d6f629c3363", "generation_result": [{"code": "def set_sample_weight(pipeline_steps, sample_weight=None)\n\t\n    sample_weight_dict = {}\n    if sample_weight is not None:\n        for name, step in pipeline_steps:\n            fit_signature = inspect.signature(step.fit)\n            if \"sample_weight\" in fit_signature.parameters:\n                sample_weight_dict[name + \"__sample_weight\"] = sample_weight\n    return sample_weight_dict if sample_weight_dict else None\n\n", "CodeBleu_score": 37.46759174371139}, {"code": "def set_sample_weight(pipeline_steps, sample_weight=None)\n\t\n    sample_weight_dict = {}\n    if sample_weight is not None:\n        for step_name, step in pipeline_steps:\n            fit_method = getattr(step, \"fit\", None)\n            if fit_method is not None:\n                signature = inspect.signature(fit_method)\n                if \"sample_weight\" in signature.parameters:\n                    key = f\"{step_name}_sample_weight\"\n                    sample_weight_dict[key] = sample_weight\n    if sample_weight_dict:\n        return sample_weight_dict\n    else:\n        return None\n\n", "CodeBleu_score": 46.16808137645659}, {"code": "def set_sample_weight(pipeline_steps, sample_weight=None)\n\t\n    sample_weight_dict = {}\n    if sample_weight is not None:\n        for step_name, step in pipeline_steps:\n            fit_method = getattr(step, \"fit\", None)\n            if fit_method is not None:\n                signature = inspect.signature(fit_method)\n                if \"sample_weight\" in signature.parameters:\n                    key = f\"{step_name}__sample_weight\"\n                    sample_weight_dict[key] = sample_weight\n    return sample_weight_dict if sample_weight_dict else None\n\n", "CodeBleu_score": 36.159389950157234}]}
{"instance_id": "cae17d519818", "generation_result": [{"code": "def _summary_of_best_pipeline(self, features, target):\n\t\n    if self.pareto_front_ is None:\n        raise RuntimeError(\n            \"You must call `fit` before calling `summary`.\"\n        )\n\n    # Compile and fit the best pipeline\n    self.fitted_pipeline_ = self.compile_and_fit_pipeline(\n        self.pareto_front_[0], features, target, ignore_warnings=True\n    )\n\n    # Print out the best pipeline\n    if self.verbosity >= 1:\n        self.pareto_front_[0].print_pipeline(\n            self.operators, self.operators_as_strings, self.operators_as_strings_\n        )\n\n    # Compile and fit the entire pareto front\n    self.pareto_front_models_ = []\n    for pipeline in self.pareto_front_:\n        self.pareto_front_models_.append(\n            self.compile_and_fit_pipeline(pipeline, features, target)\n        )\n\n", "CodeBleu_score": 21.33897588827203}, {"code": "def _summary_of_best_pipeline(self, features, target):\n\t\n    if not self.pareto_front_:\n        raise RuntimeError(\n            \"You must call fit() first before calling summary().\"\n        )\n\n    if self.verbosity >= 1:\n        print(self.pareto_front_[0])\n        if self.verbosity >= 2:\n            print()\n\n    self.fitted_pipelines_ = []\n    for pipeline in self.pareto_front_:\n        # Compile and fit the pipeline\n        model = pipeline.compile(self.operators)\n        model.fit(features, target)\n        self.fitted_pipelines_.append(model)\n\n", "CodeBleu_score": 21.86250140558047}, {"code": "def _summary_of_best_pipeline(self, features, target):\n\t\n    if self.pareto_front_ is None:\n        raise RuntimeError(\"No optimized pipeline found. Please run `fit` first.\")\n\n    best_pipeline = self.pareto_front_[0]\n    self.fitted_pipeline_ = best_pipeline.compile(self.operators)\n    self.fitted_pipeline_.fit(features, target)\n\n    if self.verbosity >= 1:\n        print(\"Best pipeline:\", end=\"\")\n        if self.verbosity >= 2:\n            print()\n        print(str(self.fitted_pipeline_))\n\n    self.fitted_models_ = []\n    for pipeline in self.pareto_front_:\n        fitted_model = pipeline.compile(self.operators)\n        fitted_model.fit(features, target)\n        self.fitted_models_.append(fitted_model)\n\n", "CodeBleu_score": 29.015424451620802}]}
{"instance_id": "638683d78167", "generation_result": [{"code": "def score(self, testing_features, testing_target)\n\t\n    if not self.optimized:\n        raise ValueError('Pipeline has not been optimized.')\n    testing_features, testing_target = self._validate_data(testing_features, testing_target)\n    if callable(self.scoring):\n        scorer = self.scoring\n    else:\n        scorer = SCORERS[self.scoring]\n    return scorer(self.fitted_pipeline_, testing_features, testing_target)\n\n", "CodeBleu_score": 31.91414777801047}, {"code": "def score(self, testing_features, testing_target)\n\t\n    if self.optimized_ is False:\n        raise ValueError(\n            \"Pipeline has not been optimized yet. Please call fit() first.\"\n        )\n    testing_features, testing_target = self.validate_data(\n        testing_features, testing_target\n    )\n    if isinstance(self.scoring, str):\n        scorer = SCORERS[self.scoring]\n    else:\n        scorer = self.scoring\n    score = scorer(self.estimator_, testing_features, testing_target)\n    return score\n\n", "CodeBleu_score": 41.538489461302795}, {"code": "def score(self, testing_features, testing_target)\n\t\n    if not self.optimized:\n        raise Exception('Pipeline has not been optimized yet.')\n\n    testing_features, testing_target = check_X_y(testing_features, testing_target,\n                                                accept_sparse=['csc', 'csr', 'coo'])\n\n    if self.scoring == 'accuracy':\n        scorer = self.SCORERS[self.scoring]\n    else:\n        scorer = self.scoring\n\n    return scorer(self.fitted_pipeline, testing_features, testing_target)\n\n", "CodeBleu_score": 35.048041282591626}]}
{"instance_id": "a04382560333", "generation_result": [{"code": " def _impute_values(self, features)\n\t\n    if self.verbosity > 1:\n        print('Imputing missing values...')\n    if self.imputer is None:\n        self.imputer = SimpleImputer(strategy='median')\n        self.imputer.fit(features)\n    return self.imputer.transform(features)\n\n", "CodeBleu_score": 69.5590558000609}, {"code": " def _impute_values(self, features)\n\t\n    if self.verbosity > 1:\n        print(\"Imputing missing values\")\n    if not self.fitted:\n        self.imputer = SimpleImputer(strategy=\"median\")\n        self.imputer.fit(features)\n        self.fitted = True\n    return self.imputer.transform(features)\n\n", "CodeBleu_score": 70.86306585052078}, {"code": " def _impute_values(self, features)\n\t\n    if self.verbosity > 1:\n        print(\"Imputing missing values...\")\n    if not hasattr(self, \"imputer\"):\n        self.imputer = SimpleImputer(strategy=\"median\")\n        self.imputer.fit(features)\n    return self.imputer.transform(features)\n\n", "CodeBleu_score": 61.878337389570206}]}
{"instance_id": "0043d411c332", "generation_result": [{"code": "def _compile_to_sklearn(self, expr)\n\t\n    # Generate the pipeline code string\n    code = expr.compile(self.pset)\n    # Evaluate the code string within a specific context\n    local_dict = {\n        'make_union': make_union,\n        'make_pipeline': make_pipeline,\n        'FunctionTransformer': FunctionTransformer,\n        'SimpleImputer': SimpleImputer,\n        'check_X_y': check_X_y,\n        'check_consistent_length': check_consistent_length,\n        'check_array': check_array,\n        'check_cv': check_cv,\n        'available_if': available_if,\n        'DataFrame': DataFrame,\n        'sparse': sparse,\n        'BaseEstimator': BaseEstimator,\n        'copy': copy,\n        'deepcopy': deepcopy,\n        'train_test_split': train_test_split,\n        'random': random,\n        'inspect': inspect,\n        'warnings': warnings,\n        'sys': sys,\n        'partial': partial,\n        'datetime': datetime,\n        'cpu_count': cpu_count,\n        'os': os,\n        're': re,\n        'errno': errno,\n        'mkdtemp': mkdtemp,\n        'rmtree': rmtree,\n        'types': types,\n        'deap': deap,\n        'update_check': update_check,\n    }\n    local_dict.update(self.pset.context)\n    local_dict.update(self.pset.globals())\n    exec(code, local_dict)\n    # Set the pipeline's memory attribute\n    pipeline = local_dict['pipeline']\n    pipeline.memory = self.memory\n    # Set the random_state parameter for each step in the pipeline\n    if self.random_state is not None:\n        for step in pipeline.steps:\n            if hasattr(step[1], 'random_state'):\n                step[1].random_state = self.random_state\n    # Return the configured scikit-learn pipeline\n    return pipeline\n\n", "CodeBleu_score": 39.553838159560954}, {"code": "def _compile_to_sklearn(self, expr)\n\t\n    operators = {\n        'add': lambda x, y: x + y,\n        'sub': lambda x, y: x - y,\n        'mul': lambda x, y: x * y,\n        'div': lambda x, y: x / y,\n        'neg': lambda x: -x,\n        'inv': lambda x: 1 / x,\n        'pow': lambda x, y: x ** y,\n        'sqrt': lambda x: x ** 0.5,\n        'log': lambda x: np.log(x),\n        'abs': lambda x: np.abs(x),\n        'sin': lambda x: np.sin(x),\n        'cos': lambda x: np.cos(x),\n        'tan': lambda x: np.tan(x),\n        'exp': lambda x: np.exp(x),\n        'nth_root': lambda x, y: x ** (1 / y),\n        'log_base': lambda x, y: np.log(x) / np.log(y),\n        'protected_div': lambda x, y: x / y if y != 0 else 1,\n        'protected_log': lambda x: np.log(x) if x > 0 else 1,\n        'protected_sqrt': lambda x: np.sqrt(x) if x >= 0 else -np.sqrt(-x),\n        'protected_exp': lambda x: np.exp(x) if x <= 700 else np.exp(700),\n        'protected_pow': lambda x, y: x ** y if x >= 0 else -(-x) ** y,\n        'protected_inv': lambda x: 1 / x if x != 0 else 1,\n        'protected_div_by_zero': lambda x, y: x / y if y != 0 else 1,\n        'protected_log_by_zero': lambda x: np.log(x) if x > 0 else 1,\n        'protected_sqrt_by_neg': lambda x: np.sqrt(x) if x >= 0 else -np.sqrt(-", "CodeBleu_score": 22.183309486655553}, {"code": "def _compile_to_sklearn(self, expr)\n\t\n    # get the code string from the expression\n    code = self._get_code_from_expr(expr)\n    # get the operators\n    operators = self._get_operators()\n    # evaluate the code string in a specific context\n    context = {\n        \"SimpleImputer\": SimpleImputer,\n        \"make_union\": make_union,\n        \"make_pipeline\": make_pipeline,\n        \"FunctionTransformer\": FunctionTransformer,\n        \"DataFrame\": DataFrame,\n        \"sparse\": sparse,\n        \"np\": np,\n        \"pd\": pd,\n        \"np\": np,\n        \"pd\": pd,\n        \"np\": np,\n        \"pd\": pd,\n        \"np\": np,\n        \"pd\": pd,\n        \"np\": np,\n        \"pd\": pd,\n        \"np\": np,\n        \"pd\": pd,\n        \"np\": np,\n        \"pd\": pd,\n        \"np\": np,\n        \"pd\": pd,\n        \"np\": np,\n        \"pd\": pd,\n        \"np\": np,\n        \"pd\": pd,\n        \"np\": np,\n        \"pd\": pd,\n        \"np\": np,\n        \"pd\": pd,\n        \"np\": np,\n        \"pd\": pd,\n        \"np\": np,\n        \"pd\": pd,\n        \"np\": np,\n        \"pd\": pd,\n        \"np\": np,\n        \"pd\": pd,\n        \"np\": np,\n        \"pd\": pd,\n        \"np\": np,\n        \"pd\": pd,\n        \"np\": np,\n        \"pd\": pd,\n        \"np\": np,\n        \"pd\": pd,\n        \"np\": np,\n        \"pd\": pd,\n        \"np\": np,\n        \"pd\": pd,\n        \"np\": np,\n        \"pd\": pd,\n        \"np\": np,\n        \"pd\": pd,\n        \"np\": np,\n        \"pd\": pd,\n        \"np\": np,\n        \"pd\": p", "CodeBleu_score": 12.688439631573983}]}
{"instance_id": "ca0ce462aa76", "generation_result": [{"code": "def pick_two_individuals_eligible_for_crossover(population)\n\t\n    # Create sets of primitive nodes for each individual\n    primitive_sets = [set(individual.primitives) for individual in population]\n\n    # Convert each individual to a string representation\n    individuals_str = [str(individual) for individual in population]\n\n    # Identify pairs of individuals that share at least one primitive and are not identical\n    eligible_pairs = []\n    for i, individual_str in enumerate(individuals_str):\n        for j, other_str in enumerate(individuals_str[i+1:], i+1):\n            if i != j and not primitive_sets[i].isdisjoint(primitive_sets[j]) and individual_str != other_str:\n                eligible_pairs.append((i, j))\n                eligible_pairs.append((j, i))\n\n    # If no eligible pairs are found, return None for both individuals\n    if not eligible_pairs:\n        return None, None\n\n    # Randomly select one of the eligible pairs\n    i, j = random.choice(eligible_pairs)\n\n    # Return the corresponding individuals from the population\n    return population[i], population[j]\n\n", "CodeBleu_score": 28.923683061618682}, {"code": "def pick_two_individuals_eligible_for_crossover(population)\n\t\n\n    # Create a set of primitive nodes for each individual\n    primitives_per_individual = [\n        set(primitive_nodes(individual)) for individual in population\n    ]\n\n    # Convert each individual to a string representation\n    individuals_as_strings = [str(individual) for individual in population]\n\n    # Identify pairs of individuals that share at least one primitive and are not identical\n    eligible_pairs = [\n        (i, j)\n        for i in range(len(population))\n        for j in range(i + 1, len(population))\n        if primitives_per_individual[i] & primitives_per_individual[j]\n        and individuals_as_strings[i] != individuals_as_strings[j]\n    ]\n\n    # Consider eligible pairs in both possible orders\n    eligible_pairs += [(j, i) for i, j in eligible_pairs]\n\n    # If no eligible pairs are found, return None for both individuals\n    if not eligible_pairs:\n        return None, None\n\n    # Randomly select one of the eligible pairs\n    i, j = random.choice(eligible_pairs)\n\n    # Return the corresponding individuals from the population\n    return population[i], population[j]\n\n", "CodeBleu_score": 31.028339407720996}, {"code": "def pick_two_individuals_eligible_for_crossover(population)\n\t\n    primitives_set = [set(individual.primitives) for individual in population]\n    population_str = [str(individual) for individual in population]\n    eligible_pairs = [(i, j) for i in range(len(population)) for j in range(i + 1, len(population)) if\n                      primitives_set[i] & primitives_set[j] and population_str[i] != population_str[j]]\n    if not eligible_pairs:\n        return None, None\n    pair = random.choice(eligible_pairs)\n    return population[pair[0]], population[pair[1]]\n\n", "CodeBleu_score": 24.51436619120345}]}
{"instance_id": "4b03c843f506", "generation_result": [{"code": "def generate_export_pipeline_code(pipeline_tree, operators)\n\t\n    steps = []\n    for node in pipeline_tree:\n        operator = operators[node.operator_id]\n        step = operator.code\n        steps.append(step)\n    num_steps = len(steps)\n    if num_steps > 1:\n        code = \"make_pipeline(", "CodeBleu_score": 22.40338780567242}, {"code": "def generate_export_pipeline_code(pipeline_tree, operators)\n\t\n    steps = []\n    for node in pipeline_tree.nodes:\n        if node.type == \"operator\":\n            operator = operators[node.label]\n            steps.append(operator.export_code())\n\n    n_steps = len(steps)\n\n    if n_steps > 1:\n        code = \"make_pipeline(", "CodeBleu_score": 22.476896068335805}, {"code": "def generate_export_pipeline_code(pipeline_tree, operators)\n\t\n\n    steps = []\n    for operator in operators:\n        if operator.operator_type == \"operator\":\n            steps.append(operator.name)\n        elif operator.operator_type == \"selection\":\n            steps.append(f\"{operator.name}\")\n    n_steps = len(steps)\n\n    if n_steps > 1:\n        steps_str = \"", "CodeBleu_score": 20.256935752208967}]}
{"instance_id": "e9aec2ee39a4", "generation_result": [{"code": "def _process_operator(operator, operators, depth=0)\n\t\n\n    # Get the name of the operator\n    operator_name = operator.name\n\n    # Check if the operator is a \"CombineDFs\" operation\n    if operator_name == \"CombineDFs\":\n        # Append the result of combining dataframes\n        operators.append(operator.result)\n\n    # For other operations, get the corresponding TPOT operator\n    else:\n        tpot_operator = getattr(deap.gp, operator_name)\n\n        # Process any input operators recursively if they are not the initial input matrix\n        args = []\n        for arg in operator.args:\n            if arg.name != \"CombineDFs\":\n                args.append(_process_operator(arg, operators, depth + 1))\n            else:\n                args.append(arg.result)\n\n        # If the operator is a root operator and the depth is greater than zero, wrap it in a StackingEstimator\n        if depth > 0 or operator.name in [\"clone\", \"submodel\"]:\n            operators.append(deap.gp.Primitive(deap.gp.StackingEstimator, tpot_operator))\n        else:\n            operators.append(tpot_operator)\n\n    return operators\n\n", "CodeBleu_score": 41.468668451342765}, {"code": "def _process_operator(operator, operators, depth=0)\n\t\n    name = operator.operator.name\n    if name == \"CombineDFs\":\n        operators.append((\"CombineDFs\",))\n    else:\n        op = deap.gp.Primitive.from_string(name, [object])\n        if op.name in [\"input_matrix\", \"input_matrix_2\"]:\n            if depth > 0:\n                operators.append((\"StackingEstimator\", op))\n            else:\n                operators.append(op)\n        else:\n            operators.append(op)\n            for arg in operator.operator.arguments:\n                _process_operator(arg, operators, depth + 1)\n\n", "CodeBleu_score": 35.898174477608926}, {"code": "def _process_operator(operator, operators, depth=0)\n\t\n    operator_name = operator.name\n    if operator_name == \"CombineDFs\":\n        return [deap.gp.Primitive(operator_name, [], deap.gp.Terminal, operator.value)]\n    else:\n        tpot_operator = operators[operator_name]\n        steps = []\n        for argument in operator.arguments:\n            if isinstance(argument, deap.gp.Primitive):\n                if argument.name != \"CombineDFs\" and depth > 0:\n                    steps.append(\n                        deap.gp.Primitive(\n                            argument.name,\n                            [],\n                            deap.gp.Terminal,\n                            _process_operator(argument, operators, depth + 1),\n                        )\n                    )\n                else:\n                    steps.append(argument)\n            else:\n                steps.append(argument)\n        if operator.name in tpot_operator.root_operators and depth > 0:\n            steps = [\n                deap.gp.Primitive(\n                    \"StackingEstimator\",\n                    [tpot_operator.get_params()],\n                    deap.gp.Terminal,\n                    steps,\n                )\n            ]\n        return steps\n\n", "CodeBleu_score": 32.74743171484044}]}
{"instance_id": "0513bf969510", "generation_result": [{"code": "def transform(self, X)\n\t\n    X = check_array(X, dtype=None)\n    n_samples, n_features = X.shape\n\n    X_int = X.astype(int, copy=False)\n    X_float = X.astype(float, copy=False)\n\n    # Select only categorical features, by comparing to a threshold\n    if self.threshold is not None:\n        X_type = self.categorical_selector.fit_transform(X_int)\n        X_cat = X_type.astype(bool, copy=False)\n        n_cat = [sum(X_cat[:, j]) for j in range(X.shape[1])]\n        if not any(n > self.threshold for n in n_cat):\n            raise ValueError(\n                \"No categorical features found for threshold={0}.\".format(self.threshold)\n            )\n    else:\n        X_cat = self.categorical_selector.fit_transform(X_int).astype(bool, copy=False)\n\n    # OneHotEncode categorical features\n    X_cat = self.onehot.fit_transform(X_cat)\n    X_cat = X_cat.tocsr()\n\n    # Concatenate the categorical and numerical data\n    X = np.hstack((X_float, X_cat))\n\n    return X\n\n", "CodeBleu_score": 29.474593406297743}, {"code": "def transform(self, X)\n\t\n    X = check_array(X, dtype=None)\n    n_samples, n_features = X.shape\n\n    features = []\n    for i in range(n_features):\n        if len(set(X[:, i])) == 2:\n            features.append(i)\n\n    if len(features) == 0:\n        raise ValueError(\"No categorical features found\")\n\n    X_new = X.copy()\n    X_new = X_new[:, features]\n\n    return X_new\n\n", "CodeBleu_score": 29.35453708993435}, {"code": "def transform(self, X)\n\t\n    # Check that the input is of a type that can be converted to a numpy array\n    X = check_array(X, dtype=None)\n    # Get the number of features in the input data\n    n_features = X.shape[1]\n    # Initialize a list to store the indices of the categorical features\n    cat_features = []\n    # Iterate over each feature in the input data\n    for feature in range(n_features):\n        # Check if the feature is categorical (i.e. its data type is object)\n        if X[:, feature].dtype == object:\n            # If the feature is categorical, append its index to the list of categorical features\n            cat_features.append(feature)\n    # Check if any categorical features were found\n    if len(cat_features) == 0:\n        # If no categorical features were found, raise a ValueError\n        raise ValueError(\"No categorical features found in input data.\")\n    # If categorical features were found, apply one-hot encoding to these features\n    X_cat = X[:, cat_features]\n    X_cat = pd.get_dummies(X_cat)\n    # Concatenate the one-hot encoded categorical features with the original input data, excluding the categorical features\n    X_transformed = np.concatenate((X[:, [i for i in range(n_features) if i not in cat_features]], X_cat), axis=1)\n    # Return the transformed data\n    return X_transformed\n\n", "CodeBleu_score": 32.889744158707835}]}
{"instance_id": "5b1bcf42ca9e", "generation_result": [{"code": "def balanced_accuracy(y_true, y_pred)\n\t\n\n    classes = np.unique(y_true)\n    class_accuracies = []\n\n    for c in classes:\n        true_positives = np.sum((y_true == c) & (y_pred == c))\n        true_negatives = np.sum((y_true != c) & (y_pred != c))\n        false_positives = np.sum((y_true != c) & (y_pred == c))\n        false_negatives = np.sum((y_true == c) & (y_pred != c))\n\n        sensitivity = true_positives / (true_positives + false_negatives)\n        if true_negatives + false_positives == 0:\n            specificity = 1\n        else:\n            specificity = true_negatives / (true_negatives + false_positives)\n\n        class_accuracy = (sensitivity + specificity) / 2\n        class_accuracies.append(class_accuracy)\n\n    return np.mean(class_accuracies)\n\n", "CodeBleu_score": 25.7411368264086}, {"code": "def balanced_accuracy(y_true, y_pred)\n\t\n    classes = np.unique(y_true)\n    class_accuracies = []\n    for c in classes:\n        tp = np.sum((y_true == c) & (y_pred == c))\n        tn = np.sum((y_true != c) & (y_pred != c))\n        fp = np.sum((y_true != c) & (y_pred == c))\n        fn = np.sum((y_true == c) & (y_pred != c))\n        sensitivity = tp / (tp + fn)\n        specificity = tn / (tn + fp) if (tn + fp) > 0 else 1\n        class_accuracy = (sensitivity + specificity) / 2\n        class_accuracies.append(class_accuracy)\n    return np.mean(class_accuracies)\n\n", "CodeBleu_score": 22.19465743885857}, {"code": "def balanced_accuracy(y_true, y_pred)\n\t\n    classes = np.unique(y_true)\n    class_accuracies = []\n    for c in classes:\n        true_positives = np.sum((y_true == c) & (y_pred == c))\n        true_negatives = np.sum((y_true != c) & (y_pred != c))\n        false_positives = np.sum((y_true != c) & (y_pred == c))\n        false_negatives = np.sum((y_true == c) & (y_pred != c))\n        sensitivity = true_positives / (true_positives + false_negatives)\n        if true_negatives + false_positives == 0:\n            specificity = 1\n        else:\n            specificity = true_negatives / (true_negatives + false_positives)\n        class_accuracy = (sensitivity + specificity) / 2\n        class_accuracies.append(class_accuracy)\n    return np.mean(class_accuracies)\n\n", "CodeBleu_score": 25.7411368264086}]}
{"instance_id": "19a9ad39de0a", "generation_result": [{"code": "def transform(self, X, y=None)\n\t\n    X = check_array(X, accept_sparse=True)\n    n_samples, n_features = X.shape\n    non_zero_count = (X != 0).sum(axis=1)\n    zero_count = (X == 0).sum(axis=1)\n    non_zero_count = non_zero_count.reshape(-1, 1)\n    zero_count = zero_count.reshape(-1, 1)\n    return np.concatenate((non_zero_count, zero_count, X), axis=1)\n\n", "CodeBleu_score": 17.077617561235105}, {"code": "def transform(self, X, y=None)\n\t\n    X = check_array(X, accept_sparse='csr')\n    n_samples, n_features = X.shape\n    X_non_zero = (X != 0).sum(axis=1).astype(np.int64)\n    X_zero = (X == 0).sum(axis=1).astype(np.int64)\n    X_transformed = np.concatenate((X_non_zero.reshape(-1, 1), X_zero.reshape(-1, 1), X), axis=1)\n    return X_transformed\n\n", "CodeBleu_score": 21.413256231625223}, {"code": "def transform(self, X, y=None)\n\t\n    X = check_array(X, accept_sparse='csr')\n    n_samples, n_features = X.shape\n    X_nonzero = (X != 0).sum(axis=1).astype(np.float64)\n    X_zero = (X == 0).sum(axis=1).astype(np.float64)\n    X_transformed = np.hstack((X_nonzero.reshape(-1, 1), X_zero.reshape(-1, 1), X))\n    return X_transformed\n\n", "CodeBleu_score": 21.17009641229737}]}
{"instance_id": "5e8db0b5d447", "generation_result": [{"code": "def _init_pretest(self, features, target)\n\t\n    unique_target_classes = len(set(target))\n    if self.training_size < unique_target_classes:\n        self.training_size = unique_target_classes\n    pretest_X, pretest_y = train_test_split(features, target, train_size=self.training_size, random_state=self.random_state)\n    if len(set(pretest_y)) < unique_target_classes:\n        pretest_X, pretest_y = _safe_indexing(pretest_X, pretest_y.index), pretest_y", "CodeBleu_score": 22.916019597706743}, {"code": "def _init_pretest(self, features, target)\n\t\n    self.target_classes = len(set(target))\n    if self.train_size < self.target_classes:\n        self.train_size = self.target_classes\n    pretest_X, _, pretest_y, _ = train_test_split(\n        features, target, train_size=self.train_size, random_state=self.random_state\n    )\n    if len(set(pretest_y)) < self.target_classes:\n        unique_target_values = list(set(target))\n        pretest_y = _safe_indexing(target, unique_target_values)\n    return pretest_X, pretest_y\n\n", "CodeBleu_score": 27.61252535766332}, {"code": "def _init_pretest(self, features, target)\n\t\n\n    # Calculate the number of unique target classes\n    unique_target_classes = len(set(target))\n\n    # Ensure the training size is at least the number of unique target classes\n    if self.train_size < unique_target_classes:\n        self.train_size = unique_target_classes\n\n    # Split the features and target into pretest_X and pretest_y with a specified random state\n    pretest_X, pretest_y = train_test_split(\n        features, target, train_size=self.train_size, random_state=self.random_state\n    )\n\n    # If all target classes are not included in pretest_y, adjust pretest_y to include at least one example from each class\n    if len(set(pretest_y)) < unique_target_classes:\n        pretest_y = _safe_indexing(target, pretest_y.index)\n\n    # Return the pretest_X and pretest_y\n    return pretest_X, pretest_y\n\n", "CodeBleu_score": 24.6622221138244}]}
{"instance_id": "5e3d48667396", "generation_result": [{"code": "def _to_numpy_type(dtype)\n\t\n    if isinstance(dtype, dtypes.DType):\n        return dtype.as_numpy_dtype\n    elif isinstance(dtype, type):\n        return dtype\n    else:\n        raise TypeError('Expected a TensorFlow or NumPy data type, but got %s' % dtype)\n\n", "CodeBleu_score": 46.469769568788}, {"code": "def _to_numpy_type(dtype)\n\t\n    if isinstance(dtype, dtypes.DType):\n        return dtype.as_numpy_dtype\n    return dtype\n\n", "CodeBleu_score": 35.79545226230816}, {"code": "def _to_numpy_type(dtype)\n\t\n    if isinstance(dtype, dtypes.DType):\n        return dtype.as_numpy_dtype\n    elif isinstance(dtype, type):\n        return dtype\n    else:\n        raise TypeError(f\"Expected a TensorFlow or NumPy data type, got {dtype}.\")\n\n", "CodeBleu_score": 46.6996860226111}]}
{"instance_id": "8f1120fdd463", "generation_result": [{"code": "def _dataset_as_numpy(ds, batch_size=None)\n\t\n    try:\n        ds = ds.batch(batch_size)\n        return tnp.array([example for example in ds.as_numpy_iterator()])\n    except:\n        return tnp.array([example for example in ds])\n\n", "CodeBleu_score": 14.464521021926313}, {"code": "def _dataset_as_numpy(ds, batch_size=None)\n\t\n    try:\n        ds = ds.batch(batch_size)\n        for batch in ds:\n            yield tnp.array(batch)\n    except:\n        for batch in ds:\n            yield tnp.array(batch)\n\n", "CodeBleu_score": 11.85930658961743}, {"code": "def _dataset_as_numpy(ds, batch_size=None)\n\t\n    if batch_size is None:\n        batch_size = 1\n    try:\n        ds = ds.batch(batch_size)\n    except:\n        ds = ds.repeat().batch(batch_size)\n    batches = tfds.as_numpy(ds)\n    for batch in batches:\n        yield batch\n\n", "CodeBleu_score": 19.115459334060215}]}
{"instance_id": "282271955871", "generation_result": [{"code": "def get_prng(seed)\n\t\n    if not isinstance(seed, int):\n        raise ValueError(f\"Seed must be an integer, but got {type(seed)}\")\n\n    # Split the seed into two 32-bit unsigned integers\n    key1 = seed >> 32\n    key2 = seed & 0xFFFFFFFF\n\n    # Concatenate the two keys to form the PRNG key\n    key = (key1 << 32) | key2\n\n    return key\n\n", "CodeBleu_score": 11.441512060420768}, {"code": "def get_prng(seed)\n\t\n    if isinstance(seed, int):\n        seed = seed % 4294967296\n    elif isinstance(seed, float):\n        seed = int(seed * 4294967296)\n    else:\n        raise ValueError(f'Expected seed to be an int or float, got {type(seed)}')\n    key = (seed >> 32, seed & 4294967295)\n    return key\n\n", "CodeBleu_score": 14.183786158422256}, {"code": "def get_prng(seed)\n\t\n    if isinstance(seed, (int, float)):\n        seed = int(seed)\n    else:\n        raise TypeError(f'seed must be int or float, but was {type(seed)}')\n    key = jax.random.PRNGKey(seed)\n    return key\n\n", "CodeBleu_score": 17.95930872752221}]}
{"instance_id": "42234dccce9a", "generation_result": [{"code": "def convert_Timestamp(cls, s, dtype, pandas_type)\n\t\n    if isinstance(dtype, pd.DatetimeTZDtype):\n        return s.dt.tz_convert(dtype.tz)\n    elif isinstance(dtype, pd.DatetimeTZDtype):\n        return s.dt.tz_localize(dtype.tz)\n    elif isinstance(dtype, pd.DatetimeTZDtype):\n        return pd.to_datetime(s, unit=\"ns\", utc=True).dt.tz_localize(dtype.tz)\n    else:\n        return pd.to_datetime(s, unit=\"ns\").dt.tz_localize(dtype.tz)\n\n", "CodeBleu_score": 62.91694384072228}, {"code": "def convert_Timestamp(cls, s, dtype, pandas_type)\n\t\n    if isinstance(dtype, PandasType.DatetimeTZDtype):\n        return s.dt.tz_convert(dtype.timezone)\n    elif pandas_type == \"datetime64\":\n        return s.dt.tz_localize(\"UTC\").dt.tz_convert(dtype.timezone)\n    elif pandas_type == \"numeric\":\n        return (\n            s.astype(\"datetime64[ns]\")\n            .dt.tz_localize(\"UTC\")\n            .dt.tz_convert(dtype.timezone)\n        )\n    else:\n        return s.astype(\"datetime64[ns]\").dt.tz_localize(dtype.timezone)\n\n", "CodeBleu_score": 49.69943213525148}, {"code": "def convert_Timestamp(cls, s, dtype, pandas_type)\n\t\n\n    if isinstance(dtype, PandasType.DatetimeTZDtype):\n        return s.dt.tz_convert(dtype.timezone)\n    elif isinstance(dtype, PandasType.Datetime64TZDtype):\n        return s.dt.tz_localize(dtype.timezone)\n    elif isinstance(dtype, PandasType.NumericDtype):\n        return s.astype(\"datetime64[ns]\").dt.tz_localize(\"UTC\").dt.tz_convert(\n            dtype.timezone\n        )\n    else:\n        return s.astype(\"datetime64[ns]\").dt.tz_localize(dtype.timezone)\n\n", "CodeBleu_score": 50.96942727947944}]}
{"instance_id": "9cee7388879b", "generation_result": [{"code": "def visit(cls, op: ops.BetweenTime, arg, lower_bound, upper_bound)\n\t\n    if not isinstance(arg, dd.DataFrame):\n        raise UnsupportedOperationError(\n            \"`between_time` only applies to Dask DataFrames\"\n        )\n\n    if arg.index.tz is None:\n        raise TypeError(\n            \"Cannot apply `between_time` to a timezone-naive index\"\n        )\n\n    arg = arg.tz_convert(\"UTC\")\n    lower_bound = lower_bound.tz_convert(\"UTC\")\n    upper_bound = upper_bound.tz_convert(\"UTC\")\n\n    indexer = (arg.index >= lower_bound) & (arg.index <= upper_bound)\n    return arg[indexer]\n\n", "CodeBleu_score": 26.52076553292736}, {"code": "def visit(cls, op: ops.BetweenTime, arg, lower_bound, upper_bound)\n\t\n    if isinstance(arg, dd.DataFrame):\n        if arg.index.tz is None:\n            raise TypeError(\n                \"Cannot apply between_time to a timezone-naive DatetimeIndex without a timezone\"\n            )\n        arg = arg.tz_convert(\"UTC\")\n    lower_bound = lower_bound.tz_convert(\"UTC\")\n    upper_bound = upper_bound.tz_convert(\"UTC\")\n    indexer = (arg >= lower_bound) & (arg < upper_bound)\n    return arg.to_dask_array(lengths=True).map_blocks(\n        lambda x: indexer.to_dask_array(lengths=True), dtype=bool\n    )\n\n", "CodeBleu_score": 27.9460795230576}, {"code": "def visit(cls, op: ops.BetweenTime, arg, lower_bound, upper_bound)\n\t\n    if isinstance(arg.op(), ops.TableNode):\n        df = arg.op().data\n        if not isinstance(df.index, pd.DatetimeIndex):\n            raise TypeError(\n                \"Cannot use between_time on a non-time indexed DataFrame.\"\n            )\n        if not df.index.tzinfo:\n            raise TypeError(\n                \"Cannot use between_time on a timezone-naive DataFrame.\"\n            )\n        df = df.tz_convert(\"UTC\")\n        indexer = df.index.indexer_between_time(lower_bound, upper_bound)\n        return arg[indexer]\n    else:\n        raise TypeError(\n            \"Cannot use between_time on a non-Dask DataFrame.\"\n        )\n\n", "CodeBleu_score": 24.55727608871389}]}
{"instance_id": "0ab1cc8ccf71", "generation_result": [{"code": "def asseries(cls, value, like=None)\n\t\n    if isinstance(value, cls.series_type):\n        return value\n    elif isinstance(value, cls.scalar_type):\n        return cls.series_type(\n            value,\n            index=cls.range(1, dtype=cls.index_dtype),\n            dtype=cls.dtype,\n            name=cls.name,\n        )\n    elif isinstance(value, tuple):\n        return cls.series_type(\n            value,\n            index=cls.range(len(value), dtype=cls.index_dtype),\n            dtype=cls.dtype,\n            name=cls.name,\n        )\n    elif isinstance(value, list):\n        return cls.series_type(\n            value,\n            index=cls.range(len(value), dtype=cls.index_dtype),\n            dtype=cls.dtype,\n            name=cls.name,\n        )\n    elif isinstance(value, dict):\n        return cls.series_type(\n            value,\n            index=cls.range(len(value), dtype=cls.index_dtype),\n            dtype=cls.dtype,\n            name=cls.name,\n        )\n    elif like is not None:\n        if isinstance(like, cls.series_type):\n            return cls.series_type(\n                value,\n                index=like.index,\n                dtype=cls.dtype,\n                name=cls.name,\n            )\n        elif isinstance(like, cls.scalar_type):\n            return cls.series_type(\n                value,\n                index=cls.range(1, dtype=cls.index_dtype),\n                dtype=cls.dtype,\n                name=cls.name,\n            )\n        elif isinstance(like, tuple):\n            return cls.series_type(\n                value,\n                index=cls.range(len(like), dtype=cls.index_dtype),\n                dtype=cls.dtype,\n                name=cls.name,\n            )\n        elif isinstance(like, list):\n            return cls.series", "CodeBleu_score": 28.18775827397894}, {"code": "def asseries(cls, value, like=None)\n\t\n    if isinstance(value, cls):\n        return value\n    elif isinstance(value, dd.Series):\n        return value\n    elif isinstance(value, dd.Scalar):\n        return dd.from_array(value.value, npartitions=1)\n    elif isinstance(value, pd.Series):\n        return dd.from_pandas(value, npartitions=1)\n    elif isinstance(value, tuple):\n        return cls.from_tuple(value, like=like)\n    elif isinstance(value, list):\n        return cls.from_list(value, like=like)\n    elif isinstance(value, dict):\n        return cls.from_dict(value, like=like)\n    else:\n        return cls.from_scalar(value, like=like)\n\n", "CodeBleu_score": 32.727746464931435}, {"code": "def asseries(cls, value, like=None)\n\t\n    if isinstance(value, cls.series_type):\n        return value\n    elif isinstance(value, cls.scalar_type):\n        return cls.series_type(value)\n    elif isinstance(value, cls.array_type):\n        return cls.series_type(value)\n    elif isinstance(value, tuple):\n        return cls.series_type(value)\n    elif isinstance(value, list):\n        return cls.series_type(value)\n    elif isinstance(value, dict):\n        return cls.series_type(value)\n    elif like is not None:\n        return cls.series_type(value)\n    else:\n        return cls.series_type(value)\n\n", "CodeBleu_score": 28.052730762438237}]}
{"instance_id": "926220a06a0c", "generation_result": [{"code": "def do_connect(\n        self,\n        dictionary: MutableMapping[str, dd.DataFrame] | None = None,\n    ) -> None\n\t\n    if dictionary is None:\n        dictionary = {}\n    else:\n        for k, v in dictionary.items():\n            if not isinstance(v, (dd.DataFrame, pd.DataFrame)):\n                raise TypeError(\n                    f\"Expected value of type dask.dataframe.DataFrame or pandas.DataFrame, got {type(v)}\"\n                )\n    super().connect(dictionary)\n\n", "CodeBleu_score": 60.718122649794445}, {"code": "def do_connect(\n        self,\n        dictionary: MutableMapping[str, dd.DataFrame] | None = None,\n    ) -> None\n\t\n    if dictionary is None:\n        dictionary = {}\n    else:\n        for k, v in dictionary.items():\n            if not isinstance(v, (dd.DataFrame, pd.DataFrame)):\n                raise TypeError(\n                    f\"Expected pandas or dask DataFrame, got {type(v)}\"\n                )\n\n    super().do_connect(dictionary)\n\n", "CodeBleu_score": 59.776403929176915}, {"code": "def do_connect(\n        self,\n        dictionary: MutableMapping[str, dd.DataFrame] | None = None,\n    ) -> None\n\t\n    if dictionary is None:\n        dictionary = {}\n    else:\n        for k, v in dictionary.items():\n            if not isinstance(v, (dd.DataFrame, pd.DataFrame)):\n                raise TypeError(\n                    f\"Expected value of type dask.dataframe.DataFrame or pandas.DataFrame, got {type(v)}\"\n                )\n    super().connect(dictionary)\n\n", "CodeBleu_score": 60.718122649794445}]}
{"instance_id": "95afc38c9125", "generation_result": [{"code": "def get_estimator(library_name: str, estimator_name: str)\n\t\n    classes = get_module_members(library_name)\n    if estimator_name not in classes:\n        raise ValueError(f\"Estimator {estimator_name} not found in module {library_name}\")\n\n    matching_estimators = [\n        estimator for estimator in classes if estimator.lower() == estimator_name.lower()\n    ]\n    if len(matching_estimators) > 1:\n        logging.debug(f\"Multiple estimators found: {matching_estimators}\")\n        logging.warning(\n            f\"Multiple estimators found with name '{estimator_name}'. Using the first one.\"\n        )\n    estimator_class = classes[matching_estimators[0]]\n\n    if not inspect.isclass(estimator_class) or not issubclass(estimator_class, BaseEstimator):\n        logging.info(\n            f\"{estimator_class} is not a subclass of sklearn.base.BaseEstimator\"\n        )\n\n    return estimator_class\n\n", "CodeBleu_score": 40.312537439381096}, {"code": "def get_estimator(library_name: str, estimator_name: str)\n\t\n    try:\n        classes = get_module_members(library_name)\n    except ModuleNotFoundError:\n        raise ValueError(f\"Could not find module {library_name}\")\n\n    selected_estimators = [\n        class_ for name, class_ in classes.items() if name == estimator_name\n    ]\n    if not selected_estimators:\n        raise ValueError(f\"Could not find {estimator_name} in {library_name}\")\n    if len(selected_estimators) > 1:\n        logging.debug(f\"Multiple estimators found: {selected_estimators}\")\n        logging.warning(\n            f\"Multiple estimators found for {estimator_name}. Using the first one.\"\n        )\n    selected_estimator = selected_estimators[0]\n\n    if not issubclass(selected_estimator, BaseEstimator):\n        logging.info(\n            f\"{selected_estimator} is not a subclass of sklearn's BaseEstimator. \"\n            \"This may cause errors.\"\n        )\n\n    return selected_estimator\n\n", "CodeBleu_score": 37.63575817364692}, {"code": "def get_estimator(library_name: str, estimator_name: str)\n\t\n    classes = get_module_members(library_name)\n    if estimator_name not in classes:\n        raise ValueError(f\"Estimator {estimator_name} not found in {library_name}\")\n    if len(classes[estimator_name]) > 1:\n        logging.debug(f\"Multiple estimators found: {classes[estimator_name]}\")\n        logging.warning(\n            f\"Using the first estimator found: {classes[estimator_name][0]}\"\n        )\n    estimator_class = classes[estimator_name][0]\n    if not issubclass(estimator_class, BaseEstimator):\n        logging.info(\n            f\"{estimator_class} is not a subclass of sklearn.base.BaseEstimator\"\n        )\n    return estimator_class\n\n", "CodeBleu_score": 39.70066392634307}]}
{"instance_id": "9562a9cc043b", "generation_result": [{"code": "def estimator_to_task(estimator_name: str) -> str\n\t\n    with open(os.path.join(os.path.dirname(__file__), \"estimator_to_task_mapping.json\")) as f:\n        mapping = json.load(f)\n    for task, postfixes in mapping.items():\n        for postfix in postfixes:\n            if estimator_name.endswith(postfix):\n                return task\n    return \"unknown\"\n\n", "CodeBleu_score": 47.8519997816052}, {"code": "def estimator_to_task(estimator_name: str) -> str\n\t\n    with open(\n        os.path.join(os.path.dirname(__file__), \"estimator_to_task_mapping.json\"), \"r\"\n    ) as f:\n        estimator_to_task_mapping = json.load(f)\n\n    for task, postfixes in estimator_to_task_mapping.items():\n        if any(estimator_name.endswith(postfix) for postfix in postfixes):\n            return task\n\n    return \"unknown\"\n\n", "CodeBleu_score": 50.53684113593361}, {"code": "def estimator_to_task(estimator_name: str) -> str\n\t\n\n    # Read the mapping from a JSON file\n    with open(os.path.join(os.path.dirname(__file__), \"estimator_to_task_mapping.json\"), \"r\") as f:\n        mapping = json.load(f)\n\n    # Check if the estimator name ends with any of the postfixes in the mapping\n    for postfix, task in mapping.items():\n        if estimator_name.endswith(postfix):\n            return task\n\n    # If no match is found, return \"unknown\"\n    return \"unknown\"\n\n", "CodeBleu_score": 42.56023343273442}]}
{"instance_id": "2f50195d043a", "generation_result": [{"code": "def load_data_from_cache(data_cache: str, data_name: str) -> Dict\n\t\n    data = {}\n    for filename in os.listdir(data_cache):\n        if filename.startswith(data_name) and not filename.endswith(\".json\"):\n            component, _ = os.path.splitext(filename)\n            component = component.replace(data_name, \"\")\n            data[component] = load_data(os.path.join(data_cache, filename))\n    return data\n\n", "CodeBleu_score": 37.11031079222643}, {"code": "def load_data_from_cache(data_cache: str, data_name: str) -> Dict\n\t\n    data_dict = {}\n    for filename in os.listdir(data_cache):\n        if filename.startswith(data_name) and not filename.endswith(\".json\"):\n            component = re.search(r\"^[^_]+\", filename).group(0)\n            file_ext = re.search(r\"\\.[^.]+$\", filename).group(0)\n            data_dict[component] = load_data(\n                os.path.join(data_cache, filename), file_ext\n            )\n    return data_dict\n\n", "CodeBleu_score": 39.7909381702784}, {"code": "def load_data_from_cache(data_cache: str, data_name: str) -> Dict\n\t\n    data_dict = {}\n    for filename in os.listdir(data_cache):\n        if filename.startswith(data_name):\n            if filename.endswith(\".json\"):\n                continue\n            component = filename.split(\".\")[0]\n            file_ext = filename.split(\".\")[-1]\n            data_dict[component] = load_data_from_file(data_cache, filename, file_ext)\n    return data_dict\n\n", "CodeBleu_score": 35.90785858968991}]}
{"instance_id": "e0281206029e", "generation_result": [{"code": "def retrieve(url: str, filename: str) -> None\n\t\n    if os.path.isfile(filename):\n        print(f\"{filename} already exists\")\n        return\n    if url.startswith(\"http\"):\n        try:\n            response = requests.get(url, stream=True)\n            if response.status_code != 200:\n                raise AssertionError(\n                    f\"Failed to download {url} with status code {response.status_code}\"\n                )\n            total = int(response.headers.get(\"content-length\", 0))\n            block_size = 1024\n            written = 0\n            with open(filename, \"wb\") as f:\n                for data in response.iter_content(block_size):\n                    written += len(data)\n                    f.write(data)\n            if total > 0 and written != total:\n                raise AssertionError(\n                    f\"Failed to download {url} with status code {response.status_code}\"\n                )\n        except Exception as e:\n            raise AssertionError(f\"Failed to download {url}: {e}\")\n    else:\n        raise AssertionError(f\"{url} is not a valid URL\")\n\n", "CodeBleu_score": 65.5523459037474}, {"code": "def retrieve(url: str, filename: str) -> None\n\t\n    if os.path.isfile(filename):\n        print(\"File already exists.\")\n        return\n    if url.startswith(\"http\"):\n        try:\n            response = requests.get(url, stream=True)\n            if response.status_code != 200:\n                raise AssertionError(f\"Failed to download from {url}.\")\n            total_size = int(response.headers.get(\"content-length\", 0))\n            block_size = 1024\n            written = 0\n            with open(filename, \"wb\") as f:\n                for data in response.iter_content(block_size):\n                    written = written + len(data)\n                    f.write(data)\n            if total_size != 0 and written != total_size:\n                raise AssertionError(\n                    \"ERROR, something went wrong in the saving process\")\n        except Exception as e:\n            print(e)\n            if os.path.exists(filename):\n                os.remove(filename)\n\n", "CodeBleu_score": 59.08212618007707}, {"code": "def retrieve(url: str, filename: str) -> None\n\t\n    if os.path.isfile(filename):\n        print(\"File already exists\")\n        return\n    if url.startswith(\"http\"):\n        try:\n            response = requests.get(url, stream=True)\n            if response.status_code != 200:\n                raise AssertionError(f\"Download failed with status code {response.status_code}\")\n            total = int(response.headers.get(\"content-length\"))\n            with open(filename, \"wb\") as file:\n                for chunk in response.iter_content(chunk_size=max(int(total / 1000), 1024 * 1024)):\n                    written = file.write(chunk)\n                    print(f\"\\rDownloading {filename} {written / total:.2%}\", end=\"\")\n        except Exception as e:\n            print(f\"Download failed due to {e}\")\n            os.remove(filename)\n            return\n        finally:\n            print(\"\")\n    else:\n        raise ValueError(f\"Invalid url provided: {url}\")\n    if os.path.getsize(filename) != total:\n        raise AssertionError(\"Download size does not match the content length\")\n\n", "CodeBleu_score": 50.49843517627619}]}
{"instance_id": "a69a71cc2fe8", "generation_result": [{"code": "def load_covtype(\n    data_name: str, data_cache: str, raw_data_cache: str, dataset_params: Dict\n) -> Tuple[Dict, Dict]\n\t\n\n    if data_name == \"covtype\":\n        X, y = fetch_covtype(return_X_y=True, data_home=raw_data_cache)\n        y = y - 1\n        if dataset_params[\"binary_covtype\"]:\n            y[y != 0] = 1\n    else:\n        raise ValueError(f\"Unknown dataset {data_name}\")\n\n    description = {\n        \"n_classes\": len(set(y)),\n        \"default_split\": {\"train_size\": 0.8, \"random_state\": 71},\n    }\n\n    return {\"X\": X, \"y\": y}, description\n\n", "CodeBleu_score": 48.77126410809935}, {"code": "def load_covtype(\n    data_name: str, data_cache: str, raw_data_cache: str, dataset_params: Dict\n) -> Tuple[Dict, Dict]\n\t\n    dataset = fetch_covtype(data_cache_dir=data_cache)\n    X, y = dataset.data, dataset.target\n    y -= 1\n    if dataset_params[\"binary_classification\"]:\n        y[y != 0] = 1\n    n_classes = len(dataset.target_names)\n    default_split = {\n        \"train_size\": 0.6,\n        \"test_size\": 0.2,\n        \"valid_size\": 0.2,\n    }\n    description = {\n        \"n_classes\": n_classes,\n        \"default_split\": default_split,\n    }\n    return {\"X\": X, \"y\": y}, description\n\n", "CodeBleu_score": 36.6965218936281}, {"code": "def load_covtype(\n    data_name: str, data_cache: str, raw_data_cache: str, dataset_params: Dict\n) -> Tuple[Dict, Dict]\n\t\n    if data_name == \"covtype\":\n        dataset = fetch_covtype(data_home=raw_data_cache)\n        X, y = dataset.data, dataset.target\n        y = y - 1\n        if dataset_params[\"binary_covtype\"]:\n            y[y != 0] = 1\n        n_classes = 2 if dataset_params[\"binary_covtype\"] else 7\n        default_split = {\"train_size\": 0.8, \"test_size\": 0.1, \"valid_size\": 0.1}\n    else:\n        raise ValueError(f\"Unknown dataset: {data_name}\")\n\n    description = {\n        \"n_classes\": n_classes,\n        \"default_split\": default_split,\n    }\n\n    return (X, y), description\n\n", "CodeBleu_score": 38.260284254171715}]}
{"instance_id": "129cbd2529ef", "generation_result": [{"code": "def flatten_list(input_list: List, ensure_type_homogeneity: bool = False) -> List\n\t\n    output_list = []\n    for element in input_list:\n        if isinstance(element, list):\n            output_list.extend(flatten_list(element, ensure_type_homogeneity))\n        else:\n            output_list.append(element)\n    if ensure_type_homogeneity:\n        output_list = ensure_list_types_homogeneity(output_list)\n    return output_list\n\n", "CodeBleu_score": 39.68648195287447}, {"code": "def flatten_list(input_list: List, ensure_type_homogeneity: bool = False) -> List\n\t\n    flat_list = []\n    for item in input_list:\n        if isinstance(item, list):\n            flat_list.extend(flatten_list(item, ensure_type_homogeneity))\n        else:\n            flat_list.append(item)\n    if ensure_type_homogeneity:\n        flat_list = ensure_list_types_homogeneity(flat_list)\n    return flat_list\n\n", "CodeBleu_score": 37.389518063230646}, {"code": "def flatten_list(input_list: List, ensure_type_homogeneity: bool = False) -> List\n\t\n    flattened_list = []\n    for element in input_list:\n        if isinstance(element, list):\n            flattened_list.extend(flatten_list(element))\n        else:\n            flattened_list.append(element)\n    if ensure_type_homogeneity:\n        flattened_list = ensure_list_types_homogeneity(flattened_list)\n    return flattened_list\n\n", "CodeBleu_score": 37.049809486296326}]}
{"instance_id": "2d38c2c60b59", "generation_result": [{"code": "def logit(\n    x: np.ndarray,\n    /,\n    *,\n    eps: Optional[float] = None,\n    complex_mode: Literal[\"split\", \"magnitude\", \"jax\"] = \"jax\",\n    out: Optional[np.ndarray] = None,\n)\n\t\n    if eps is None:\n        x = np.where(np.logical_and(x >= 0, x <= 1), x, np.nan)\n    else:\n        x = np.clip(x, eps, 1 - eps)\n    return np.log(x / (1 - x))\n\n", "CodeBleu_score": 48.839089472286126}, {"code": "def logit(\n    x: np.ndarray,\n    /,\n    *,\n    eps: Optional[float] = None,\n    complex_mode: Literal[\"split\", \"magnitude\", \"jax\"] = \"jax\",\n    out: Optional[np.ndarray] = None,\n)\n\t\n    if complex_mode == \"split\":\n        x = ivy.real(x)\n    elif complex_mode == \"magnitude\":\n        x = ivy.abs(x)\n    elif complex_mode == \"jax\":\n        x = ivy.real(x)\n    else:\n        raise ivy.utils.exceptions.IvyException(\n            \"Complex mode must be one of 'split', 'magnitude', or 'jax'\"\n        )\n    if eps is None:\n        x = ivy.where((x < 0.0) | (x > 1.0), np.nan, x)\n    else:\n        x = ivy.clip(x, eps, 1 - eps)\n    return ivy.log(x / (1 - x))\n\n", "CodeBleu_score": 48.20849348640047}, {"code": "def logit(\n    x: np.ndarray,\n    /,\n    *,\n    eps: Optional[float] = None,\n    complex_mode: Literal[\"split\", \"magnitude\", \"jax\"] = \"jax\",\n    out: Optional[np.ndarray] = None,\n)\n\t\n    if eps is None:\n        x = np.where(np.logical_and(x >= 0, x <= 1), x, np.nan)\n    else:\n        x = np.clip(x, eps, 1 - eps)\n\n    if complex_mode == \"split\":\n        return np.log(x / (1 - x))\n    elif complex_mode == \"magnitude\":\n        return np.log(np.abs(x) / (1 - np.abs(x))) * np.sign(x)\n    elif complex_mode == \"jax\":\n        return np.log(x / (1 - x)) + 1j * np.log1p(-x / (1 - x))\n    else:\n        raise ivy.utils.exceptions.IvyException(\n            \"complex_mode must be one of 'split', 'magnitude', or 'jax'\"\n        )\n\n", "CodeBleu_score": 49.98708622498052}]}
{"instance_id": "205e296f8621", "generation_result": [{"code": "def hardshrink(\n    x: np.ndarray, /, *, lambd: float = 0.5, out: Optional[np.ndarray] = None\n) -> np.ndarray\n\t\n    return np.where(x < -lambd, x, np.where(x > lambd, x, 0)).astype(x.dtype)\n\n", "CodeBleu_score": 38.022240999588696}, {"code": "def hardshrink(\n    x: np.ndarray, /, *, lambd: float = 0.5, out: Optional[np.ndarray] = None\n) -> np.ndarray\n\t\n    if out is None:\n        out = np.copy(x)\n    else:\n        out = np.copy(out)\n    np.copyto(out, x, where=np.abs(x) > lambd)\n    return out\n\n", "CodeBleu_score": 37.636342240705325}, {"code": "def hardshrink(\n    x: np.ndarray, /, *, lambd: float = 0.5, out: Optional[np.ndarray] = None\n) -> np.ndarray\n\t\n    x_shape = x.shape\n    x_dtype = x.dtype\n    if not ivy.is_array(x):\n        x = np.array(x)\n    if not ivy.is_array(out):\n        out = np.array(out)\n    out = np.where(x < -lambd, x, 0)\n    out = np.where(x > lambd, out, 0)\n    out = ivy.astype(out, x_dtype, copy=False)\n    return ivy.reshape(out, x_shape)\n\n", "CodeBleu_score": 52.13536147114919}]}
{"instance_id": "5b133f6ebe2d", "generation_result": [{"code": "def vorbis_window(\n    window_length: np.ndarray,\n    *,\n    dtype: np.dtype = np.float32,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n\t\n    return np.array(\n        [\n            round(\n                math.sin(\n                    (ivy.pi / 2) * (math.sin(ivy.pi / (window_length * 2)) ** 2 * i)\n                ),\n            )\n            for i in range(1, window_length * 2)[0::2]\n        ],\n        dtype=dtype,\n    )\n\n", "CodeBleu_score": 43.876463766749026}, {"code": "def vorbis_window(\n    window_length: np.ndarray,\n    *,\n    dtype: np.dtype = np.float32,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n\t\n    return np.array(\n        [\n            round(\n                math.sin(\n                    (ivy.pi / 2) * (ivy.sin(ivy.pi / (2 * window_length)) ** 2 * i)\n                )\n                ** 2,\n                8,\n            )\n            for i in range(1, window_length * 2)[0::2]\n        ],\n        dtype=dtype,\n    )\n\n", "CodeBleu_score": 38.930691813727805}, {"code": "def vorbis_window(\n    window_length: np.ndarray,\n    *,\n    dtype: np.dtype = np.float32,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n\t\n    return np.array(\n        [\n            round(\n                math.sin(\n                    (ivy.pi / 2) * (math.sin(ivy.pi / (window_length * 2)) ** 2 * i)\n                ),\n                8,\n            )\n            for i in range(1, window_length * 2)[0::2]\n        ],\n        dtype=dtype,\n    )\n\n", "CodeBleu_score": 43.05293698111884}]}
{"instance_id": "473296f771a9", "generation_result": [{"code": "def polyval(coeffs: np.ndarray, x: np.ndarray) -> np.ndarray\n\t\n    return np.array(np.polyval(coeffs, x))\n\n", "CodeBleu_score": 13.238360167169722}, {"code": "def polyval(coeffs: np.ndarray, x: np.ndarray) -> np.ndarray\n\t\n    return np.array(np.polyval(coeffs, x), dtype=np.promote_types(coeffs.dtype, x.dtype))\n\n", "CodeBleu_score": 17.584907134390058}, {"code": "def polyval(coeffs: np.ndarray, x: np.ndarray) -> np.ndarray\n\t\n    return np.array(np.polyval(coeffs, x))\n\n", "CodeBleu_score": 13.238360167169722}]}
{"instance_id": "b8455221cb2a", "generation_result": [{"code": "def amax(\n    x: np.ndarray,\n    /,\n    *,\n    axis: Optional[Union[int, Sequence[int]]] = None,\n    keepdims: bool = False,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n\t\n    if axis is None:\n        num_dims = len(x.shape)\n        axis = tuple(range(num_dims))\n    elif isinstance(axis, list):\n        axis = tuple(axis)\n    return np.amax(a=x, axis=axis, keepdims=keepdims, out=out)\n\n", "CodeBleu_score": 50.10316053474416}, {"code": "def amax(\n    x: np.ndarray,\n    /,\n    *,\n    axis: Optional[Union[int, Sequence[int]]] = None,\n    keepdims: bool = False,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n\t\n    if axis is None:\n        num_dims = len(x.shape)\n        axis = tuple(range(num_dims))\n    elif isinstance(axis, list):\n        axis = tuple(axis)\n    return np.amax(a=x, axis=axis, keepdims=keepdims, out=out)\n\n", "CodeBleu_score": 50.10316053474416}, {"code": "def amax(\n    x: np.ndarray,\n    /,\n    *,\n    axis: Optional[Union[int, Sequence[int]]] = None,\n    keepdims: bool = False,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n\t\n    if axis is None:\n        num_dims = len(x.shape)\n        axis = tuple(range(num_dims))\n    elif isinstance(axis, list):\n        axis = tuple(axis)\n    return np.amax(a=x, axis=axis, keepdims=keepdims, out=out)\n\n", "CodeBleu_score": 50.10316053474416}]}
{"instance_id": "f72489f434a3", "generation_result": [{"code": "def copysign(\n    x1: npt.ArrayLike,\n    x2: npt.ArrayLike,\n    /,\n    *,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n\t\n    x1, x2 = promote_types_of_inputs(x1, x2)\n    if x1.dtype not in [np.float32, np.float64]:\n        x1, x2 = np.asarray(x1, np.float32), np.asarray(x2, np.float32)\n    return np.copysign(x1, x2, out=out)\n\n", "CodeBleu_score": 52.23893820558172}, {"code": "def copysign(\n    x1: npt.ArrayLike,\n    x2: npt.ArrayLike,\n    /,\n    *,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n\t\n    x1, x2 = promote_types_of_inputs(x1, x2)\n    if x1.dtype not in [np.float32, np.float64]:\n        x1, x2 = np.asarray(x1, np.float64), np.asarray(x2, np.float64)\n    return np.copysign(x1, x2, out=out)\n\n", "CodeBleu_score": 52.23893820558172}, {"code": "def copysign(\n    x1: npt.ArrayLike,\n    x2: npt.ArrayLike,\n    /,\n    *,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n\t\n    x1, x2 = promote_types_of_inputs(x1, x2)\n    if x1.dtype not in [np.float32, np.float64]:\n        x1, x2 = np.asarray(x1, dtype=np.float64), np.asarray(x2, dtype=np.float64)\n    return np.copysign(x1, x2, out=out)\n\n", "CodeBleu_score": 54.01654402944948}]}
{"instance_id": "55c0d77e4dc2", "generation_result": [{"code": "def count_nonzero(\n    a: np.ndarray,\n    /,\n    *,\n    axis: Optional[Union[int, Tuple[int, ...]]] = None,\n    keepdims: bool = False,\n    dtype: Optional[np.dtype] = None,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n\t\n    a = np.asarray(a)\n    axis = tuple(axis) if isinstance(axis, list) else axis\n    if dtype is None:\n        dtype = np.int64\n    if out is None:\n        return np.array(np.count_nonzero(a, axis=axis, keepdims=keepdims), dtype=dtype)\n    else:\n        return np.count_nonzero(a, axis=axis, keepdims=keepdims, dtype=dtype, out=out)\n\n", "CodeBleu_score": 50.554174439806964}, {"code": "def count_nonzero(\n    a: np.ndarray,\n    /,\n    *,\n    axis: Optional[Union[int, Tuple[int, ...]]] = None,\n    keepdims: bool = False,\n    dtype: Optional[np.dtype] = None,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n\t\n    if axis is None:\n        num = np.count_nonzero(a)\n    else:\n        axis = tuple(axis) if isinstance(axis, list) else axis\n        num = np.count_nonzero(a, axis=axis)\n\n    if keepdims:\n        res = np.reshape(num, a.shape)\n    else:\n        res = num\n\n    if not dtype and not out:\n        return np.asarray(res)\n\n    if out is not None:\n        return np.asarray(res, dtype=out.dtype)\n\n    if dtype:\n        return np.asarray(res, dtype=dtype)\n\n    return np.asarray(res, dtype=ivy.default_int_dtype(as_native=True))\n\n", "CodeBleu_score": 53.74761004122697}, {"code": "def count_nonzero(\n    a: np.ndarray,\n    /,\n    *,\n    axis: Optional[Union[int, Tuple[int, ...]]] = None,\n    keepdims: bool = False,\n    dtype: Optional[np.dtype] = None,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n\t\n    if axis is None:\n        num_nz = np.count_nonzero(a)\n    else:\n        axis = tuple(axis) if isinstance(axis, list) else axis\n        num_nz = np.count_nonzero(a, axis=axis)\n\n    if keepdims:\n        num_nz = np.expand_dims(num_nz, axis=axis)\n\n    # handle out\n    if out is not None:\n        out = np.asarray(out)\n        np.copyto(out, num_nz)\n        return out\n\n    # handle dtype\n    if dtype is not None:\n        num_nz = num_nz.astype(dtype)\n\n    return num_nz\n\n", "CodeBleu_score": 53.744030644922105}]}
{"instance_id": "784bf7d24fc2", "generation_result": [{"code": "def gradient(\n    x: np.ndarray,\n    /,\n    *,\n    spacing: Union[int, list, tuple] = 1,\n    axis: Optional[Union[int, list, tuple]] = None,\n    edge_order: int = 1,\n) -> Union[np.ndarray, List[np.ndarray]]\n\t\n    return np.gradient(x, spacing, axis, edge_order)\n\n", "CodeBleu_score": 39.1881060447383}, {"code": "def gradient(\n    x: np.ndarray,\n    /,\n    *,\n    spacing: Union[int, list, tuple] = 1,\n    axis: Optional[Union[int, list, tuple]] = None,\n    edge_order: int = 1,\n) -> Union[np.ndarray, List[np.ndarray]]\n\t\n    return np.gradient(x, spacing, axis, edge_order)\n\n", "CodeBleu_score": 39.1881060447383}, {"code": "def gradient(\n    x: np.ndarray,\n    /,\n    *,\n    spacing: Union[int, list, tuple] = 1,\n    axis: Optional[Union[int, list, tuple]] = None,\n    edge_order: int = 1,\n) -> Union[np.ndarray, List[np.ndarray]]\n\t\n    return np.gradient(x, spacing, axis, edge_order)\n\n", "CodeBleu_score": 39.1881060447383}]}
{"instance_id": "efa0a51af6bc", "generation_result": [{"code": "def xlogy(\n    x: np.ndarray, y: np.ndarray, /, *, out: Optional[np.ndarray] = None\n) -> np.ndarray\n\t\n    x, y = promote_types_of_inputs(x, y)\n    if ivy.any(x):\n        x, y = ivy.broadcast_arrays(x, y)\n        return ivy.asarray(np.select([x == 0, x != 0], [0.0, x * np.log(y)]), x.dtype)\n    else:\n        return ivy.zeros_like(x)\n\n", "CodeBleu_score": 56.94230708811744}, {"code": "def xlogy(\n    x: np.ndarray, y: np.ndarray, /, *, out: Optional[np.ndarray] = None\n) -> np.ndarray\n\t\n    x, y = promote_types_of_inputs(x, y)\n    if ivy.any(x):\n        x, y = ivy.broadcast_arrays(x, y)\n        return ivy.asarray(x * ivy.log(y), dtype=x.dtype)\n    else:\n        return ivy.zeros_like(y)\n\n", "CodeBleu_score": 55.3891397106378}, {"code": "def xlogy(\n    x: np.ndarray, y: np.ndarray, /, *, out: Optional[np.ndarray] = None\n) -> np.ndarray\n\t\n    x, y = promote_types_of_inputs(x, y)\n    if ivy.any(x):\n        x, y = ivy.broadcast_arrays(x, y)\n        return ivy.asarray(x * ivy.log(y), dtype=x.dtype)\n    else:\n        return ivy.zeros_like(y)\n\n", "CodeBleu_score": 55.3891397106378}]}
{"instance_id": "ac1645284260", "generation_result": [{"code": "def _determine_depth_max_pooling(x, kernel, strides, dims, data_format=\"channel_last\")\n\t\n    kernel, strides, depth_pooling = _depth_max_pooling_helper(\n        x.shape, kernel, strides, dims, data_format\n    )\n    if depth_pooling:\n        x = ivy.permute_dims(x, (0, *dims, 2, 3))\n    return x, kernel, strides, depth_pooling\n\n", "CodeBleu_score": 51.86856464374826}, {"code": "def _determine_depth_max_pooling(x, kernel, strides, dims, data_format=\"channel_last\")\n\t\n    kernel, strides, depth_pooling = _depth_max_pooling_helper(x, kernel, strides, dims, data_format)\n    if depth_pooling:\n        x = ivy.permute_dims(x, (0, *dims, 2, 3))\n    return x, kernel, strides, depth_pooling\n\n", "CodeBleu_score": 45.117616224740495}, {"code": "def _determine_depth_max_pooling(x, kernel, strides, dims, data_format=\"channel_last\")\n\t\n    kernel, strides, depth_pooling = _depth_max_pooling_helper(\n        x, kernel, strides, dims, data_format\n    )\n    if depth_pooling:\n        x = ivy.permute_dims(x, (0, *dims, 2, 3))\n    return x, kernel, strides, depth_pooling\n\n", "CodeBleu_score": 48.77605659325187}]}
{"instance_id": "18ff0a4f7550", "generation_result": [{"code": "def dropout1d(\n    x: np.ndarray,\n    prob: float,\n    /,\n    *,\n    training: bool = True,\n    data_format: str = \"NWC\",\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n\t\n    if training:\n        if data_format == \"NCW\":\n            x = np.transpose(x, (0, 2, 1))\n        mask = np.random.binomial(1, prob, size=x.shape)\n        scaled_x = np.where(mask, x * (1 / (1 - prob)), 0)\n        if data_format == \"NCW\":\n            scaled_x = np.transpose(scaled_x, (0, 2, 1))\n        ret = scaled_x\n    else:\n        ret = x\n    if ivy.exists(out):\n        return ivy.inplace_update(out, ret)\n    return ret\n\n", "CodeBleu_score": 51.9023749514113}, {"code": "def dropout1d(\n    x: np.ndarray,\n    prob: float,\n    /,\n    *,\n    training: bool = True,\n    data_format: str = \"NWC\",\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n\t\n    if training:\n        if data_format == \"NCW\":\n            x = np.transpose(x, (0, 2, 1))\n        mask = np.random.binomial(1, prob, size=x.shape)\n        scaled = np.where(mask, x * (1.0 / (1 - prob)), 0)\n        if data_format == \"NCW\":\n            scaled = np.transpose(scaled, (0, 2, 1))\n        return scaled\n    else:\n        return x\n\n", "CodeBleu_score": 46.39102213692291}, {"code": "def dropout1d(\n    x: np.ndarray,\n    prob: float,\n    /,\n    *,\n    training: bool = True,\n    data_format: str = \"NWC\",\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n\t\n    if training:\n        if data_format == \"NCW\":\n            x = np.transpose(x, (0, 2, 1))\n        mask = np.random.binomial(1, prob, size=x.shape)\n        scaled = np.where(mask, x * (1 / (1 - prob)), 0)\n        if data_format == \"NCW\":\n            scaled = np.transpose(scaled, (0, 2, 1))\n        return scaled\n    else:\n        return x\n\n", "CodeBleu_score": 46.36292828440906}]}
{"instance_id": "0ce45eb7cbd9", "generation_result": [{"code": "def top_k(\n    x: np.ndarray,\n    k: int,\n    /,\n    *,\n    axis: int = -1,\n    largest: bool = True,\n    sorted: bool = True,\n    out: Optional[Tuple[np.ndarray, np.ndarray]] = None,\n) -> Tuple[np.ndarray, np.ndarray]\n\t\n    return np.asarray(np.top_k(x, k, axis, largest, sorted, out))\n\n", "CodeBleu_score": 15.76380957416288}, {"code": "def top_k(\n    x: np.ndarray,\n    k: int,\n    /,\n    *,\n    axis: int = -1,\n    largest: bool = True,\n    sorted: bool = True,\n    out: Optional[Tuple[np.ndarray, np.ndarray]] = None,\n) -> Tuple[np.ndarray, np.ndarray]\n\t\n    return ivy.top_k(x, k, axis=axis, largest=largest, sorted=sorted, out=out)\n\n", "CodeBleu_score": 16.543112076012438}, {"code": "def top_k(\n    x: np.ndarray,\n    k: int,\n    /,\n    *,\n    axis: int = -1,\n    largest: bool = True,\n    sorted: bool = True,\n    out: Optional[Tuple[np.ndarray, np.ndarray]] = None,\n) -> Tuple[np.ndarray, np.ndarray]\n\t\n    if not largest:\n        indices = np.argpartition(x, kth=k, axis=axis)\n        indices = np.take(indices, np.arange(k), axis=axis)\n    else:\n        x *= -1\n        indices = np.argpartition(x, kth=k, axis=axis)\n        indices = np.take(indices, np.arange(k - 1, -1, -1), axis=axis)\n    topk_res = np.take_along_axis(x, indices, axis=axis)\n    return topk_res, indices\n\n", "CodeBleu_score": 52.85900643585483}]}
{"instance_id": "1424f9541998", "generation_result": [{"code": "def l1_normalize(\n    x: np.ndarray,\n    /,\n    *,\n    axis: Optional[int] = None,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n\t\n    if out is None:\n        out = np.zeros_like(x)\n    if axis is None:\n        norm = np.sum(np.abs(x.flatten()))\n        denormalization_array = np.full_like(x, norm)\n    else:\n        norm = np.sum(np.abs(x), axis=axis, keepdims=True)\n        denormalization_array = norm + np.finfo(x.dtype).eps\n    np.divide(x, denormalization_array, out=out)\n    return out\n\n", "CodeBleu_score": 57.88476440602261}, {"code": "def l1_normalize(\n    x: np.ndarray,\n    /,\n    *,\n    axis: Optional[int] = None,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n\t\n    if out is None:\n        out = np.zeros_like(x)\n    if axis is None:\n        denorm = np.abs(x).sum()\n    else:\n        denorm = np.linalg.norm(x, axis=axis, keepdims=True)\n    denorm = np.maximum(denorm, 1e-12)\n    return np.divide(x, denorm, out=out)\n\n", "CodeBleu_score": 53.531916637646425}, {"code": "def l1_normalize(\n    x: np.ndarray,\n    /,\n    *,\n    axis: Optional[int] = None,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n\t\n    if out is None:\n        out = np.zeros_like(x)\n    if axis is None:\n        norm = np.sum(np.abs(x))\n        denorm = norm * np.ones_like(x)\n    else:\n        norm = np.sum(np.abs(x), axis=axis, keepdims=True)\n        denorm = norm + np.finfo(x.dtype).eps\n    np.divide(x, denorm, out=out)\n    return out\n\n", "CodeBleu_score": 66.75976521580367}]}
{"instance_id": "4c3a386a1f05", "generation_result": [{"code": "def beta(\n    alpha: Union[float, np.ndarray],\n    beta: Union[float, np.ndarray],\n    /,\n    *,\n    shape: Optional[Union[ivy.NativeShape, Sequence[int]]] = None,\n    device: Optional[str] = None,\n    dtype: Optional[np.dtype] = None,\n    seed: Optional[int] = None,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n\t\n    shape = _check_bounds_and_get_shape(alpha, beta, shape)\n    rng = ivy.default_rng(seed)\n    out = rng.beta(alpha, beta, shape=shape, device=device, dtype=dtype, out=out)\n    return out\n\n", "CodeBleu_score": 58.236143895233894}, {"code": "def beta(\n    alpha: Union[float, np.ndarray],\n    beta: Union[float, np.ndarray],\n    /,\n    *,\n    shape: Optional[Union[ivy.NativeShape, Sequence[int]]] = None,\n    device: Optional[str] = None,\n    dtype: Optional[np.dtype] = None,\n    seed: Optional[int] = None,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n\t\n    shape = _check_bounds_and_get_shape(alpha, beta, shape)\n    if seed is not None:\n        np.random.seed(seed)\n    return np.asarray(np.random.beta(alpha, beta, shape), dtype=dtype, device=device)\n\n", "CodeBleu_score": 81.69844047861531}, {"code": "def beta(\n    alpha: Union[float, np.ndarray],\n    beta: Union[float, np.ndarray],\n    /,\n    *,\n    shape: Optional[Union[ivy.NativeShape, Sequence[int]]] = None,\n    device: Optional[str] = None,\n    dtype: Optional[np.dtype] = None,\n    seed: Optional[int] = None,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n\t\n    if seed:\n        np.random.seed(seed)\n    alpha_beta = _check_bounds_and_get_shape(alpha, beta, shape)\n    return np.random.beta(\n        alpha_beta[0], alpha_beta[1], size=alpha_beta[2], dtype=dtype, out=out\n    )\n\n", "CodeBleu_score": 60.74425884473567}]}
{"instance_id": "271200a8b261", "generation_result": [{"code": "def invert_permutation(\n    x: Union[np.ndarray, list, tuple],\n    /,\n) -> np.ndarray\n\t\n    return np.argsort(np.argsort(x))\n\n", "CodeBleu_score": 20.815369772377547}, {"code": "def invert_permutation(\n    x: Union[np.ndarray, list, tuple],\n    /,\n) -> np.ndarray\n\t\n    indices = np.argsort(x)\n    inverse = np.zeros_like(indices)\n    inverse[indices] = np.arange(len(x))\n    return np.sort(inverse)\n\n", "CodeBleu_score": 43.26743060363919}, {"code": "def invert_permutation(\n    x: Union[np.ndarray, list, tuple],\n    /,\n) -> np.ndarray\n\t\n    return np.argsort(np.argsort(x))\n\n", "CodeBleu_score": 20.815369772377547}]}
{"instance_id": "040d73b37229", "generation_result": [{"code": "def plot_medical_charts() -> None\n\t\n\n    # Set plotting style\n    register_matplotlib_converters()\n    plt.style.use('seaborn')\n\n    # Create figure with subplots\n    fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(12, 12))\n\n    # Plot total vomit per month\n    misc_data.total_vomit_per_month.plot(ax=ax[0, 0], kind='bar', color='#006666', title='Total Vomit per Month')\n    ax[0, 0].set_xlabel('Month')\n    ax[0, 0].set_ylabel('Total Vomit')\n\n    # Plot days between vomit\n    misc_data.days_between_vomit.plot(ax=ax[0, 1], kind='bar', color='#006666', title='Days Between Vomit')\n    ax[0, 1].set_xlabel('Days')\n    ax[0, 1].set_ylabel('Number of Days')\n\n    # Plot days in daycare\n    misc_data.days_in_daycare.plot(ax=ax[1, 0], kind='bar', color='#006666', title='Days in Daycare')\n    ax[1, 0].set_xlabel('Days')\n    ax[1, 0].set_ylabel('Number of Days')\n\n    # Plot doctor visits\n    misc_data.doctor_visits.plot(ax=ax[1, 1], kind='bar', color='#006666', title='Doctor Visits')\n    ax[1, 1].set_xlabel('Visits')\n    ax[1, 1].set_ylabel('Number of Visits')\n\n    # Adjust layout of subplots\n    plt.tight_layout()\n\n    # Export figure to file\n    plt.savefig(config.medical_charts_file)", "CodeBleu_score": 32.399069696482755}, {"code": "def plot_medical_charts() -> None\n\t\n\n    # Set the style\n    misc_data.plt.style.use(\"seaborn-whitegrid\")\n\n    # Create a figure with subplots\n    fig, ax = misc_data.plt.subplots(\n        nrows=2,\n        ncols=2,\n        figsize=(10, 8),\n        gridspec_kw={\"hspace\": 0.5},\n        dpi=100,\n    )\n\n    # Plot total vomit per month\n    config.total_vomit_per_month.plot(ax=ax[0, 0], kind=\"bar\", color=\"blue\")\n    ax[0, 0].set_title(\"Total Vomit per Month\")\n    ax[0, 0].set_xlabel(\"Month\")\n    ax[0, 0].set_ylabel(\"Total Vomit\")\n\n    # Plot days between vomit\n    config.days_between_vomit.plot(ax=ax[0, 1], kind=\"line\", color=\"red\")\n    ax[0, 1].set_title(\"Days Between Vomit\")\n    ax[0, 1].set_xlabel(\"Date\")\n    ax[0, 1].set_ylabel(\"Days\")\n\n    # Plot days in daycare\n    config.days_in_daycare.plot(ax=ax[1, 0], kind=\"bar\", color=\"green\")\n    ax[1, 0].set_title(\"Days in Daycare\")\n    ax[1, 0].set_xlabel(\"Date\")\n    ax[1, 0].set_ylabel(\"Days\")\n\n    # Plot doctor visits\n    config.doctor_visits.plot(ax=ax[1, 1], kind=\"line\", color=\"orange\")\n    ax[1, 1].set_title(\"Doctor Visits\")\n    ax[1, 1].set_xlabel(\"Date\")\n    ax[1, 1].set_ylabel(\"Visits\")\n\n    # Adjust the layout of the subplots\n    fig.", "CodeBleu_score": 29.878497220095714}, {"code": "def plot_medical_charts() -> None\n\t\n\n    register_matplotlib_converters()\n\n    # Set the style\n    plt.style.use(config.plot_style)\n\n    # Create a figure with subplots\n    fig, axs = plt.subplots(nrows=2, ncols=2, figsize=(10, 8))\n\n    # Plot total vomit per month\n    axs[0, 0].bar(misc_data.vomit_per_month.index, misc_data.vomit_per_month.values)\n    axs[0, 0].set_title('Total vomit per month')\n    axs[0, 0].set_xlabel('Month')\n    axs[0, 0].set_ylabel('Vomit')\n\n    # Plot days between vomit\n    axs[0, 1].plot(misc_data.days_between_vomit.index, misc_data.days_between_vomit.values)\n    axs[0, 1].set_title('Days between vomit')\n    axs[0, 1].set_xlabel('Date')\n    axs[0, 1].set_ylabel('Days')\n\n    # Plot days in daycare\n    axs[1, 0].plot(misc_data.days_in_daycare.index, misc_data.days_in_daycare.values)\n    axs[1, 0].set_title('Days in daycare')\n    axs[1, 0].set_xlabel('Date')\n    axs[1, 0].set_ylabel('Days')\n\n    # Plot doctor visits\n    axs[1, 1].plot(misc_data.doctor_visits.index, misc_data.doctor_visits.values)\n    axs[1, 1].set_title('Doctor visits')\n    axs[1, 1].set_xlabel('Date')\n    axs[1, 1].set_ylabel('Visits')\n\n    # Adjust the", "CodeBleu_score": 29.17088180389782}]}
{"instance_id": "7061f4d0c3fc", "generation_result": [{"code": "def plot_feeding_24h_viz() -> None\n\t\n    import matplotlib.pyplot as plt\n    from datetime import datetime, timedelta\n    from src.utils import export_figure, parse_raw_data\n\n    # Import and extract feeding data\n    diaper_df = parse_raw_data(diaper_data)\n    bottle_df = parse_raw_data(feeding_bottle_data)\n    solid_df = parse_raw_data(feeding_solid_data)\n\n    # Set up plot with dark grid style\n    plt.style.use(\"dark_background\")\n\n    # Compute offset from birthday based on last solid feeding date\n    offset = (\n        datetime.strptime(solid_df[\"Time\"].iloc[-1], \"%Y-%m-%d %H:%M:%S\")\n        - config.birthday\n    ).days\n\n    # Create scatter plot of feeding times\n    plt.scatter(\n        bottle_df[\"Time\"].apply(\n            lambda x: (datetime.strptime(x, \"%Y-%m-%d %H:%M:%S\") - config.birthday).days\n        ),\n        [1] * len(bottle_df),\n        c=\"red\",\n        label=\"Bottle\",\n    )\n    plt.scatter(\n        solid_df[\"Time\"].apply(\n            lambda x: (datetime.strptime(x, \"%Y-%m-%d %H:%M:%S\") - config.birthday).days\n        ),\n        [2] * len(solid_df),\n        c=\"blue\",\n        label=\"Solid\",\n    )\n\n    # Add legend to distinguish between bottle and solid feedings\n    plt.legend()\n\n    # Calculate end date for plot\n    end_date = datetime.strptime(diaper_df[\"Time\"].iloc[-1], \"%Y-%m-%d %H:%M:%S\")\n\n    # Format plot to display feeding data over a week in a 24-hour format\n    plt.xticks(\n        [i for i in range(offset, offset + 7)],\n", "CodeBleu_score": 28.99497921991697}, {"code": "def plot_feeding_24h_viz() -> None\n\t\n    import matplotlib.dates as mdates\n    import matplotlib.pyplot as plt\n    import numpy as np\n    from datetime import datetime, timedelta\n    from parse_raw_data import parse_raw_data\n    from export_figure import export_figure\n\n    # Import and extract feeding data\n    feeding_data = parse_raw_data(config.feeding_data_file_path)\n    bottle_feedings = feeding_data[feeding_bottle_data]\n    solid_feedings = feeding_data[feeding_solid_data]\n\n    # Set up plot with dark grid style\n    plt.style.use(\"dark_background\")\n    fig, ax = plt.subplots(figsize=(10, 6))\n\n    # Compute offset from birthday based on last solid feeding date\n    last_solid_feeding_date = datetime.strptime(\n        solid_feedings.index[-1], \"%Y-%m-%d %H:%M:%S\"\n    )\n    offset = (last_solid_feeding_date - config.birthday).days\n\n    # Create scatter plot of feeding times\n    ax.scatter(\n        bottle_feedings.index,\n        np.ones(len(bottle_feedings)),\n        color=\"red\",\n        label=\"Bottle Feedings\",\n    )\n    ax.scatter(\n        solid_feedings.index,\n        np.ones(len(solid_feedings)),\n        color=\"blue\",\n        label=\"Solid Feedings\",\n    )\n\n    # Add legend\n    ax.legend()\n\n    # Set plot title and axis labels\n    ax.set_title(\"24-hour Feeding Data\")\n    ax.set_xlabel(\"Time\")\n    ax.set_ylabel(\"Feeding Type\")\n\n    # Compute end date for plot\n    end_date = last_solid_feeding_date + timedelta(days=7)\n\n    # Format plot to display feeding data over a week in a 24-hour format\n    ax.set_xlim(last_solid_feeding_date,", "CodeBleu_score": 27.028711309234144}, {"code": "def plot_feeding_24h_viz() -> None\n\t\n    import matplotlib.dates as mdates\n    import matplotlib.pyplot as plt\n    import numpy as np\n    import pandas as pd\n\n    from utils import export_figure, parse_raw_data\n\n    # import and extract feeding data\n    df = parse_raw_data(config.feeding_data_path)\n\n    # set up plot with dark grid style\n    plt.style.use(\"dark_background\")\n    fig, ax = plt.subplots(figsize=(10, 5))\n\n    # compute offset from birthday based on last solid feeding date\n    offset = pd.to_datetime(df[\"solid_feeding_time\"].max()) - pd.to_datetime(\n        df[\"birthday\"]\n    )\n\n    # create scatter plot of feeding times\n    ax.scatter(\n        df[\"bottle_feeding_time\"],\n        np.ones(len(df)),\n        c=\"red\",\n        label=\"Bottle Feeding\",\n    )\n    ax.scatter(\n        df[\"solid_feeding_time\"],\n        np.zeros(len(df)),\n        c=\"blue\",\n        label=\"Solid Feeding\",\n    )\n\n    # add legend to distinguish between bottle and solid feedings\n    ax.legend()\n\n    # calculate end date for plot\n    end_date = pd.to_datetime(df[\"solid_feeding_time\"].max()) + pd.DateOffset(\n        days=7\n    )\n\n    # format plot to display feeding data over a week in a 24-hour format\n    ax.set_xlim(\n        pd.to_datetime(df[\"solid_feeding_time\"].max()) - pd.DateOffset(days=7),\n        end_date,\n    )\n    ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%H:%M\"))\n    ax.set_xlabel(\"Time of Day\")\n    ax.set_yticks([0, 1])\n    ax.set_yticklabels([\"Solid Feeding\", \"Bottle Feeding\"", "CodeBleu_score": 28.350453241546226}]}
{"instance_id": "6c1ffa63d122", "generation_result": [{"code": "def plot(self, ax, task: ClassificationTask = ClassificationTask.Result, **kwargs):\n\t\n    target_counts = self.get_target_counts(task)\n    sns.barplot(ax=ax, x=target_counts.index, y=target_counts.values, **kwargs)\n\n", "CodeBleu_score": 44.632060598876464}, {"code": "def plot(self, ax, task: ClassificationTask = ClassificationTask.Result, **kwargs):\n\t\n    target_counts = self.get_target_counts(task)\n    sns.barplot(x=target_counts.index, y=target_counts.values, ax=ax)\n    ax.set_xlabel(\"Target\")\n    ax.set_ylabel(\"Count\")\n    ax.set_title(f\"Target Counts for {task.name}\")\n    ax.tick_params(axis=\"x\", rotation=90)\n    ax.grid(True)\n    return ax\n\n", "CodeBleu_score": 36.38242602152568}, {"code": "def plot(self, ax, task: ClassificationTask = ClassificationTask.Result, **kwargs):\n\t\n    counts = self.get_target_counts(task)\n    sns.barplot(x=counts.index, y=counts.values, ax=ax, **kwargs)\n    ax.set_title(f\"{task.name} Target Counts\")\n    ax.set_xlabel(\"Target\")\n    ax.set_ylabel(\"Count\")\n    ax.tick_params(axis=\"x\", rotation=45)\n    ax.tick_params(axis=\"y\", rotation=0)\n    ax.set_xticklabels(ax.get_xticklabels(), ha=\"right\")\n    ax.set_ylim(0, counts.max() * 1.1)\n    ax.grid(True)\n    return ax\n\n", "CodeBleu_score": 38.59818681526875}]}
{"instance_id": "2f81a906bad5", "generation_result": [{"code": "def render_to_rgb(figure)\n\t\n    import matplotlib.pyplot as plt\n    plt.switch_backend('agg')\n    import numpy as np\n    import io\n    from PIL import Image\n\n    if isinstance(figure, list):\n        images = []\n        for fig in figure:\n            canvas = plt_backend_agg.FigureCanvasAgg(fig)\n            canvas.draw()\n            buf = canvas.buffer_rgba()\n            data = np.asarray(buf)\n            image = data.reshape(fig.canvas.get_width_height()[::-1] + (4,))\n            image = image[:, :, :3]\n            images.append(image)\n        images = np.stack(images, axis=0)\n        if close_figures:\n            for fig in figure:\n                plt.close(fig)\n        return images\n    else:\n        canvas = plt_backend_agg.FigureCanvasAgg(figure)\n        canvas.draw()\n        buf = canvas.buffer_rgba()\n        data = np.asarray(buf)\n        image = data.reshape(figure.canvas.get_width_height()[::-1] + (4,))\n        image = image[:, :, :3]\n        if close_figures:\n            plt.close(figure)\n        return image\n\n\n", "CodeBleu_score": 42.29571248966487}, {"code": "def render_to_rgb(figure)\n\t\n    import numpy as np\n    import matplotlib.pyplot as plt\n    from matplotlib.backends.backend_agg import FigureCanvasAgg\n\n    if isinstance(figure, list):\n        images = []\n        for fig in figure:\n            canvas = FigureCanvasAgg(fig)\n            canvas.draw()\n            buf = np.asarray(canvas.buffer_rgba())\n            image = np.reshape(buf, (fig.canvas.get_width_height()[::-1] + (4,)))[:, :, :3]\n            images.append(image)\n        return np.stack(images, axis=0)\n    else:\n        canvas = FigureCanvasAgg(figure)\n        canvas.draw()\n        buf = np.asarray(canvas.buffer_rgba())\n        image = np.reshape(buf, (figure.canvas.get_width_height()[::-1] + (4,)))[:, :, :3]\n        plt.close(figure)\n        return image\n\n\n", "CodeBleu_score": 40.33137959908597}, {"code": "def render_to_rgb(figure)\n\t\n    import numpy as np\n    import matplotlib.pyplot as plt\n    from matplotlib.backends.backend_agg import FigureCanvasAgg\n    if isinstance(figure, list):\n        images = []\n        for fig in figure:\n            canvas = FigureCanvasAgg(fig)\n            canvas.draw()\n            buf = np.asarray(canvas.buffer_rgba())\n            image = buf.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n            images.append(image)\n            plt.close(fig)\n        return np.stack(images)\n    else:\n        canvas = FigureCanvasAgg(figure)\n        canvas.draw()\n        buf = np.asarray(canvas.buffer_rgba())\n        image = buf.reshape(figure.canvas.get_width_height()[::-1] + (3,))\n        plt.close(figure)\n        return image\n\n\n\n", "CodeBleu_score": 37.0594771894944}]}
{"instance_id": "c3037b01569c", "generation_result": [{"code": "def beholder_pytorch()\n\t\n    import torch\n    from beholder.beholder import Beholder\n\n    beholder = Beholder(logdir=\"./beholder_logs\")\n    for i in range(1000):\n        tensor_list = [\n            torch.rand(3, 224, 224),\n            torch.rand(3, 224, 224),\n        ]\n        names = [\"image1\", \"image2\"]\n        frame = i\n        beholder.update(tensor_list, names, frame)\n        time.sleep(0.1)\n        print(i)\n\n", "CodeBleu_score": 26.309520014304116}, {"code": "def beholder_pytorch()\n\t\n    import torch\n    from beholder import Beholder\n\n    beholder = Beholder(logdir=\"./logs\")\n\n    for i in range(1000):\n        beholder.update(\n            [\n                torch.randn(1, 28, 28),\n                torch.randn(1, 28, 28),\n            ],\n            frame=i,\n            names=[\"random_tensor_1\", \"random_tensor_2\"],\n        )\n        time.sleep(0.1)\n        print(i)\n\n", "CodeBleu_score": 29.311152268555574}, {"code": "def beholder_pytorch()\n\t\n    import torch\n    from beholder import Beholder\n\n    beholder = Beholder(\n        \"logs/beholder_pytorch\",\n        enabled=True,\n        write_every=1,\n        model=None,\n        extra_info={},\n        max_plots=10,\n    )\n\n    for i in range(1000):\n        beholder.update(\n            [\n                torch.randn(3, 32, 32),\n                torch.randn(3, 32, 32),\n            ],\n            [\n                \"tensor1\",\n                \"tensor2\",\n            ],\n            frame=i,\n        )\n        time.sleep(0.01)\n        print(i)\n\n", "CodeBleu_score": 32.30457965218875}]}
{"instance_id": "cd57b5de8a88", "generation_result": [{"code": "def log_pr_data(self, tag, summary, num_thresholds, step=None)\n\t\n    try:\n        import tensorflow as tf\n        import numpy as np\n\n        tensor = tf.make_tensor_proto(summary.tensor)\n        dims = tensor.tensor_shape.dim\n        if len(dims) == 0:\n            return\n        elif len(dims) == 1:\n            values = np.array(tensor.float_val)\n        else:\n            values = np.array(tensor.tensor_content)\n            values = values.reshape((dims[0].size, dims[1].size))\n\n        thresholds = [i * 1.0 / num_thresholds for i in range(num_thresholds + 1)]\n        tp, fp, tn, fn, precision, recall = [], [], [], [], [], []\n        for threshold in thresholds:\n            tp.append(np.sum(values >= threshold, axis=0).flatten().tolist())\n            fp.append(np.sum(values < threshold, axis=0).flatten().tolist())\n            tn.append(np.sum(values < threshold, axis=0).flatten().tolist())\n            fn.append(np.sum(values >= threshold, axis=0).flatten().tolist())\n            precision.append(\n                np.nan_to_num(tp[-1] / (tp[-1] + fp[-1])).flatten().tolist()\n            )\n            recall.append(\n                np.nan_to_num(tp[-1] / (tp[-1] + fn[-1])).flatten().tolist()\n            )\n\n        pr_data = {\n            \"thresholds\": thresholds,\n            \"tp\": tp,\n            \"fp\": fp,\n            \"tn\": tn,\n            \"fn\": fn,\n            \"precision\": precision,\n            \"recall\": recall,\n            \"tag\": tag,\n        }\n        self.log_asset_data(pr_data, name=tag, step=step, is_input=False)\n    except Exception as e:\n        logging.error(f\"Error logging PR data: {e}\")\n", "CodeBleu_score": 39.70697916680227}, {"code": "def log_pr_data(self, tag, summary, num_thresholds, step=None)\n\t\n    try:\n        tensor_data = summary.tensor.float_val\n        if len(tensor_data) == 12 * num_thresholds:\n            tensor_data = [\n                tensor_data[i : i + 12] for i in range(0, len(tensor_data), 12)\n            ]\n        else:\n            tensor_data = [\n                tensor_data[i : i + 11] for i in range(0, len(tensor_data), 11)\n            ]\n\n        thresholds = [i / float(num_thresholds) for i in range(num_thresholds + 1)]\n        tp, fp, tn, fn, precision, recall = [\n            list(t) for t in zip(*tensor_data)\n        ]\n        pr_data = {\n            \"thresholds\": thresholds,\n            \"tp\": tp,\n            \"fp\": fp,\n            \"tn\": tn,\n            \"fn\": fn,\n            \"precision\": precision,\n            \"recall\": recall,\n        }\n        pr_data.update({\"tag\": tag})\n        self.log_asset_data(\n            json.dumps(pr_data, indent=4).encode(\"utf-8\"),\n            name=f\"{tag}.json\",\n            step=step,\n            description=f\"pr_data for {tag}\",\n        )\n    except Exception as e:\n        logging.error(f\"Error logging pr_data for {tag}: {e}\")\n\n", "CodeBleu_score": 35.39638723364257}, {"code": "def log_pr_data(self, tag, summary, num_thresholds, step=None)\n\t\n    try:\n        tensor = summary.tensor\n        dims = tensor.tensor_shape.dim\n        tensor_data = tensor.float_val\n        if len(dims) == 1:\n            tensor_data = [tensor_data]\n        else:\n            tensor_data = [tensor_data[i::dims[0].size] for i in range(dims[0].size)]\n        thresholds = [i / num_thresholds for i in range(num_thresholds + 1)]\n        tp, fp, tn, fn, precision, recall = tensor_data\n        pr_data = {\n            \"thresholds\": thresholds,\n            \"tp\": tp,\n            \"fp\": fp,\n            \"tn\": tn,\n            \"fn\": fn,\n            \"precision\": precision,\n            \"recall\": recall,\n        }\n        self.log_asset_data(\n            json.dumps(pr_data, indent=2).encode(\"utf-8\"),\n            tag,\n            file_extension=\".json\",\n            overwrite=True,\n            step=step,\n        )\n    except Exception as e:\n        logging.error(f\"Error logging PR data for tag {tag}: {e}\")\n\n", "CodeBleu_score": 37.28220163090937}]}
{"instance_id": "7c4ea9beecc9", "generation_result": [{"code": "def load_mnist(images, labels, num)\n\t\n    data = []\n    target = []\n    for filename in (images, labels):\n        with gzip.open(filename, 'rb') as f:\n            data.append(np.frombuffer(f.read(), np.uint8, offset=16).reshape(-1, 28 * 28))\n    data = np.concatenate(data, axis=0)\n    data = data.reshape(-1, 1, 28, 28)\n    data = data / np.float32(256)\n    return data\n\n", "CodeBleu_score": 27.076698734669385}, {"code": "def load_mnist(images, labels, num)\n\t\n    data = []\n    target = []\n\n    with gzip.open(labels, 'rb') as flbl:\n        magic_nr, size = struct.unpack(\">II\", flbl.read(8))\n        flbl.read(8)\n        for i in range(size):\n            target.append(six.int_from_bytes(flbl.read(1), byteorder='big'))\n\n    with gzip.open(images, 'rb') as fimg:\n        magic_nr, size, rows, cols = struct.unpack(\">IIII\", fimg.read(16))\n        fimg.read(16)\n        for i in range(size):\n            img = np.frombuffer(fimg.read(rows * cols), dtype=np.uint8).astype(float)\n            img = img.reshape(rows * cols, 1)\n            data.append(img)\n\n    return data, target\n\n", "CodeBleu_score": 40.93062055668546}, {"code": "def load_mnist(images, labels, num)\n\t\n    data = []\n    target = []\n\n    with gzip.open(labels, \"rb\") as flbl:\n        magic_nr, size = struct.unpack(\">II\", flbl.read(8))\n        lbl = np.frombuffer(flbl.read(), dtype=np.int8)\n\n    with gzip.open(images, \"rb\") as fimg:\n        magic_nr, size, rows, cols = struct.unpack(\">IIII\", fimg.read(16))\n        img = np.frombuffer(fimg.read(), dtype=np.uint8).reshape(size, rows, cols)\n\n    for i in range(num):\n        data.append([img[i]])\n        target.append(lbl[i])\n\n    return np.array(data), np.array(target)\n\n", "CodeBleu_score": 38.32706768076758}]}
{"instance_id": "bf4f1c828b25", "generation_result": [{"code": "def compute_stats(self, ldf: LuxDataFrame)\n\t\n    with LuxTracer() as t:\n        t.start()\n\n        # Initialize the dictionaries for unique values and min-max values\n        self.unique_values = {}\n        self.min_max = {}\n        self.cardinality = {}\n\n        # Compute the length of the dataframe\n        self.length = len(ldf)\n\n        # Iterate over the columns of the dataframe\n        for attr in ldf.columns:\n            # Compute unique values and cardinality for each attribute\n            self.unique_values[attr] = ldf[attr].unique()\n            self.cardinality[attr] = len(ldf[attr].unique())\n\n            # If the attribute is a timestamp, use its string representation as the dictionary key\n            if is_datetime_series(ldf[attr]):\n                self.unique_values[attr] = [str(x) for x in self.unique_values[attr]]\n\n            # If the attribute is numeric, compute the minimum and maximum values\n            if is_numeric_nan_column(ldf[attr]):\n                self.min_max[attr] = (ldf[attr].min(), ldf[attr].max())\n\n        # If the index is not of integer type, compute unique values and cardinality for the index as well\n        if not isinstance(ldf.index, pd.RangeIndex):\n            self.unique_values[ldf.index.name] = ldf.index.unique()\n            self.cardinality[ldf.index.name] = len(ldf.index.unique())\n\n        t.end()\n\n", "CodeBleu_score": 35.00213014918956}, {"code": "def compute_stats(self, ldf: LuxDataFrame)\n\t\n    self.cardinality = {}\n    self.min_max = {}\n    self.unique_values = {}\n    self.dataset_length = len(ldf)\n    self.dataset_size = ldf.shape\n    self.dataset_id = ldf.id\n    self.dataset_name = ldf.dataset_name\n    self.dataset_type = ldf.dataset_type\n    self.dataset_uuid = ldf.dataset_uuid\n    self.dataset_semantic_type = ldf.dataset_semantic_type\n    self.dataset_description = ldf.dataset_description\n    self.dataset_metadata = ldf.dataset_metadata\n    self.dataset_source = ldf.dataset_source\n    self.dataset_references = ldf.dataset_references\n    self.dataset_samples = ldf.dataset_samples\n    self.dataset_meta = ldf.dataset_meta\n    self.dataset_stats = ldf.dataset_stats\n    self.dataset_stats_metadata = ldf.dataset_stats_metadata\n    self.dataset_stats_semantic_type = ldf.dataset_stats_semantic_type\n    self.dataset_stats_description = ldf.dataset_stats_description\n    self.dataset_stats_source = ldf.dataset_stats_source\n    self.dataset_stats_references = ldf.dataset_stats_references\n    self.dataset_stats_samples = ldf.dataset_stats_samples\n    self.dataset_stats_meta = ldf.dataset_stats_meta\n    self.dataset_stats_stats = ldf.dataset_stats_stats\n    self.dataset_stats_stats_metadata = ldf.dataset_stats_stats_metadata\n    self.dataset_stats_stats_semantic_type = ldf.dataset_stats_stats_semantic_type\n    self.dataset_stats_stats_description = ldf.dataset_stats_stats_description\n    self.dataset_stats_stats_source = ldf.dataset_stats_stats_source\n    self.dataset_stats_stats_references = ldf.dataset_stats_stats_references\n    self.dataset_stats_stats_samples", "CodeBleu_score": 17.276355062283056}, {"code": "def compute_stats(self, ldf: LuxDataFrame)\n\t\n    self.unique_values = {}\n    self.min_max = {}\n    self.cardinality = {}\n    self.len = len(ldf)\n    for attr in ldf.columns:\n        if is_datetime_series(ldf[attr]):\n            self.unique_values[attr] = list(ldf[attr].dt.strftime(\"%Y-%m-%d %H:%M:%S\"))\n            self.min_max[attr] = [\n                ldf[attr].min().strftime(\"%Y-%m-%d %H:%M:%S\"),\n                ldf[attr].max().strftime(\"%Y-%m-%d %H:%M:%S\"),\n            ]\n        elif is_timedelta64_series(ldf[attr]):\n            self.unique_values[attr] = list(\n                timedelta64_to_float_seconds(ldf[attr].unique())\n            )\n            self.min_max[attr] = [\n                timedelta64_to_float_seconds(ldf[attr].min()),\n                timedelta64_to_float_seconds(ldf[attr].max()),\n            ]\n        else:\n            self.unique_values[attr] = list(ldf[attr].unique())\n            if is_numeric_nan_column(ldf[attr]):\n                self.min_max[attr] = [\n                    ldf[attr].min(),\n                    ldf[attr].max(),\n                ]\n            else:\n                self.min_max[attr] = [None, None]\n        self.cardinality[attr] = len(self.unique_values[attr])\n    if not check_if_id_like(ldf.index):\n        self.unique_values[\"index\"] = list(ldf.index.values)\n        self.cardinality[\"index\"] = len(self.unique_values[\"index\"])\n        self.min_max[\"index\"] = [\n            ldf.index.min(),\n            ldf.index.max(),\n        ]\n\n", "CodeBleu_score": 36.289174125113696}]}
{"instance_id": "3e6fc3d2f47c", "generation_result": [{"code": "def execute_preview(tbl: LuxSQLTable, preview_size=5)\n\t\n    query = f\"SELECT * FROM {tbl.table_name} LIMIT {preview_size}\"\n    result = tbl.sqlEngine.execute_query(query)\n    df = pandas.DataFrame(result, columns=result[0].keys())\n    return df\n\n", "CodeBleu_score": 19.983897196961568}, {"code": "def execute_preview(tbl: LuxSQLTable, preview_size=5)\n\t\n    # TODO: add a check for the size of the table\n    # TODO: add a check for the type of the table\n    # TODO: add a check for the number of rows in the table\n    # TODO: add a check for the number of columns in the table\n    # TODO: add a check for the number of distinct values in the table\n    # TODO: add a check for the number of null values in the table\n    # TODO: add a check for the number of outliers in the table\n    # TODO: add a check for the number of duplicates in the table\n    # TODO: add a check for the number of missing values in the table\n    # TODO: add a check for the number of unique values in the table\n    # TODO: add a check for the number of zeros in the table\n    # TODO: add a check for the number of ones in the table\n    # TODO: add a check for the number of twos in the table\n    # TODO: add a check for the number of threes in the table\n    # TODO: add a check for the number of fours in the table\n    # TODO: add a check for the number of fives in the table\n    # TODO: add a check for the number of sixes in the table\n    # TODO: add a check for the number of sevens in the table\n    # TODO: add a check for the number of eights in the table\n    # TODO: add a check for the number of nines in the table\n    # TODO: add a check for the number of tens in the table\n    # TODO: add a check for the number of hundreds in the table\n    # TODO: add a check for the number of thousands in the table\n    # TODO: add a check for the number of millions in the table\n    # TODO: add a check for the number of billions in the table\n    # TODO: add a check for the number of trillions in the table\n    # TODO: add a check for the number of quadrillions in the table\n    # TODO: add a check for the number of quintillions in the table\n    # TODO: add a check for the number of sextillions in the table\n    # TODO: add a check for the number of septillions in the table\n    # TODO: add", "CodeBleu_score": 6.3352826510721245}, {"code": "def execute_preview(tbl: LuxSQLTable, preview_size=5)\n\t\n    query = f\"SELECT * FROM {tbl.table_name} LIMIT {preview_size}\"\n    preview_df = lux.Executor.execute([query], tbl)[0]\n    return preview_df\n\n", "CodeBleu_score": 14.330880213886227}]}
{"instance_id": "b3dee87be4b8", "generation_result": [{"code": "def execute_sampling(tbl: LuxSQLTable)\n\t\n    # TODO: Make this function more modular\n    sample_flag = lux.config.sampling.SAMPLE_FLAG\n    sample_start = lux.config.sampling.SAMPLE_START\n    sample_cap = lux.config.sampling.SAMPLE_CAP\n    sample_frac = lux.config.sampling.SAMPLE_FRAC\n\n    # If sample_flag is True, execute sampling\n    if sample_flag:\n        # Get the length of the table\n        tbl_len = len(tbl)\n        # Calculate the limit based on the fraction of the table length\n        limit = math.ceil(tbl_len * sample_frac)\n        # If the limit is less than the sample_start, set the limit to the sample_start\n        if limit < sample_start:\n            limit = sample_start\n        # If the limit is greater than the sample_cap, set the limit to the sample_cap\n        if limit > sample_cap:\n            limit = sample_cap\n        # Format the query to retrieve a sample of rows from the table\n        query = f\"SELECT * FROM {tbl.tableName} LIMIT {limit}\"\n        # Execute the query and store the sampled rows in the _sampled attribute of the tbl object\n        tbl._sampled = tbl.query(query)\n\n", "CodeBleu_score": 24.000810028208626}, {"code": "def execute_sampling(tbl: LuxSQLTable)\n\t\n    if lux.config.sampling.SAMPLE_FLAG:\n        sample_start = lux.config.sampling.SAMPLE_START\n        sample_cap = lux.config.sampling.SAMPLE_CAP\n        sample_frac = lux.config.sampling.SAMPLE_FRAC\n        # Retrieve the length of the table\n        table_len = len(tbl)\n        # Calculate the limit based on the fraction of the table length\n        limit = min(math.floor(table_len * sample_frac), sample_cap)\n        # Format and execute the sample query\n        sampled_query = tbl.query_handler.get_sampled_query(sample_start, limit)\n        sampled = tbl.conn.execute_and_fetchall(sampled_query)\n        # Store the sampled rows in the _sampled attribute of the tbl object\n        tbl._sampled = sampled\n\n", "CodeBleu_score": 30.89795703764627}, {"code": "def execute_sampling(tbl: LuxSQLTable)\n\t\n    sample_flag = lux.config.sampling.SAMPLE_FLAG\n    sample_start = lux.config.sampling.SAMPLE_START\n    sample_cap = lux.config.sampling.SAMPLE_CAP\n    sample_frac = lux.config.sampling.SAMPLE_FRAC\n\n    # Retrieve the length of the table\n    query = \"SELECT COUNT(*) FROM {};\".format(tbl.table_name)\n    result = tbl.conn.execute(query).fetchone()\n    length = result[0]\n    # Calculate the limit based on the fraction of the table length\n    limit = math.ceil(length * sample_frac)\n    # If the limit is greater than the sample cap, set the limit to the sample cap\n    if limit > sample_cap:\n        limit = sample_cap\n    # If the limit is less than the sample start, set the limit to the sample start\n    if limit < sample_start:\n        limit = sample_start\n    # Format and execute the sample query\n    query = \"SELECT * FROM {} LIMIT {}\".format(tbl.table_name, limit)\n    sampled = pandas.read_sql(query, tbl.conn)\n    # Store the sampled rows in the _sampled attribute of the tbl object\n    tbl._sampled = sampled\n\n", "CodeBleu_score": 33.59480602185018}]}
{"instance_id": "485253b1088a", "generation_result": [{"code": "def draw_text(\n    params: Dict[str, dict],\n    background: BaseGeometry\n) -> None\n\t\n\n    # Override default text settings\n    params = {\n        \"text\": params.get(\"text\", \"\"),\n        \"x\": params.get(\"x\", 0),\n        \"y\": params.get(\"y\", 0),\n        **params\n    }\n\n    # Extract text, x, and y values\n    text = params.pop(\"text\")\n    x = params.pop(\"x\")\n    y = params.pop(\"y\")\n\n    # Get bounds of background\n    min_x, min_y, max_x, max_y = background.bounds\n\n    # Interpolate x and y values to fit within bounds\n    x = (x - min_x) / (max_x - min_x)\n    y = (y - min_y) / (max_y - min_y)\n\n    # Draw text at calculated position with remaining parameters\n    plt.text(x, y, text, **params)\n\n", "CodeBleu_score": 19.319110352260378}, {"code": "def draw_text(\n    params: Dict[str, dict],\n    background: BaseGeometry\n) -> None\n\t\n    params = deepcopy(params)\n    text = params.pop(\"text\")\n    x = params.pop(\"x\")\n    y = params.pop(\"y\")\n    xmin, ymin, xmax, ymax = background.bounds\n    x = (x - xmin) / (xmax - xmin)\n    y = (y - ymin) / (ymax - ymin)\n    plt.text(x, y, text, **params)\n\n", "CodeBleu_score": 16.837748308385457}, {"code": "def draw_text(\n    params: Dict[str, dict],\n    background: BaseGeometry\n) -> None\n\t\n\n    # Override default text settings with provided parameters\n    params = {**{\"text\": \"\", \"x\": 0, \"y\": 0}, **params}\n\n    # Extract text, x, and y values\n    text = params.pop(\"text\")\n    x = params.pop(\"x\")\n    y = params.pop(\"y\")\n\n    # Retrieve background's bounds\n    xmin, ymin, xmax, ymax = background.bounds\n\n    # Interpolate x and y values to fit within background's bounds\n    x = (x - xmin) / (xmax - xmin)\n    y = (y - ymin) / (ymax - ymin)\n\n    # Draw text at calculated position with remaining parameters\n    plt.text(x, y, text, **params)\n\n", "CodeBleu_score": 18.688532474374707}]}
{"instance_id": "5f70a88b6b72", "generation_result": [{"code": "def _determine_format_string( dates, datetime_format=None )\n\t\n    if datetime_format is not None:\n        return datetime_format\n    # Calculate the average number of days between consecutive dates\n    average_days = sum(\n        (dates[i + 1] - dates[i]).total_seconds() / 86400\n        for i in range(len(dates) - 1)\n    ) / (len(dates) - 1)\n    # Determine the format string based on the average number of days\n    if average_days < 0.33:\n        # Intraday data\n        if (dates[-1] - dates[0]).total_seconds() / 86400 > 1:\n            return \"%b %d, %H:%M\"\n        else:\n            return \"%H:%M\"\n    else:\n        # Daily or less frequent data\n        if (dates[-1] - dates[0]).total_seconds() / 86400 > 365:\n            return \"%Y-%b-%d\"\n        else:\n            return \"%b %d\"\n\n", "CodeBleu_score": 40.79594135155348}, {"code": "def _determine_format_string( dates, datetime_format=None )\n\t\n    if datetime_format:\n        return datetime_format\n    else:\n        dates = sorted(dates)\n        date_diffs = [(dates[i+1] - dates[i]).total_seconds() for i in range(len(dates)-1)]\n        avg_days = sum(date_diffs) / len(date_diffs) / 86400\n        if avg_days < 0.33:\n            if dates[-1].day != dates[0].day:\n                return '%b %d, %H:%M'\n            else:\n                return '%H:%M'\n        else:\n            if dates[-1].year != dates[0].year:\n                return '%Y-%b-%d'\n            else:\n                return '%b %d'\n\n", "CodeBleu_score": 39.88623900513338}, {"code": "def _determine_format_string( dates, datetime_format=None )\n\t\n    if datetime_format:\n        return datetime_format\n\n    # Calculate the average number of days between consecutive dates\n    total_days = sum( ( dates[i+1] - dates[i] ).days for i in range( len( dates ) - 1 ) )\n    avg_days = total_days / ( len( dates ) - 1 )\n\n    # Check if the data is intraday\n    if avg_days < 0.33:\n        # Check if the data spans multiple days\n        if dates[-1].date() - dates[0].date() > datetime.timedelta( days=1 ):\n            return '%b %d, %H:%M'\n        else:\n            return '%H:%M'\n    else:\n        # Check if the data spans multiple years\n        if dates[-1].year - dates[0].year > 0:\n            return '%Y-%b-%d'\n        else:\n            return '%b %d'\n\n", "CodeBleu_score": 37.68901362940754}]}
{"instance_id": "f9589bf4666d", "generation_result": [{"code": "def _mpf_to_rgba(c, alpha=None)\n\t\n    if len(c) == 3:\n        r, g, b = c\n        a = 1\n    elif len(c) == 4:\n        r, g, b, a = c\n    else:\n        raise ValueError(\"Input must be a 3 or 4-element array\")\n    if any(x > 1 for x in c):\n        r, g, b = (x / 255.0 for x in (r, g, b))\n    if alpha is not None:\n        a = alpha\n    return tuple(int(x * 255) for x in (r, g, b, a))\n\n", "CodeBleu_score": 27.201365010978108}, {"code": "def _mpf_to_rgba(c, alpha=None)\n\t\n    import mpmath\n    import numpy as np\n    from matplotlib.colors import to_rgba\n\n    if not isinstance(c, (tuple, list, np.ndarray)):\n        raise TypeError(\"Input color must be a tuple, list, or numpy array.\")\n\n    if len(c) not in [3, 4]:\n        raise ValueError(\"Input color must be a 3- or 4-element tuple/list/array.\")\n\n    if issubclass(c.dtype.type, (np.integer, int)):\n        # Input color is in RGB or RGBA format of integer type\n        if any(x > 255 for x in c):\n            # Normalize integer RGB or RGBA components if any is greater than 255\n            c = tuple(x / 255.0 for x in c)\n    elif issubclass(c.dtype.type, (np.inexact, float)):\n        # Input color is in RGB or RGBA format of inexact type (float)\n        if any(x > 1 for x in c):\n            # Normalize float RGB or RGBA components if any is greater than 1\n            c = tuple(x / 255.0 for x in c)\n    else:\n        raise TypeError(\"Input color must be an integer or float typed tuple/list/array.\")\n\n    # Convert the processed color input to an RGBA tuple\n    rgba = to_rgba(c, alpha)\n\n    return rgba\n\n", "CodeBleu_score": 20.83722243359173}, {"code": "def _mpf_to_rgba(c, alpha=None)\n\t\n    if c is None:\n        return None\n    if isinstance(c, (tuple, list)):\n        if len(c) in [3, 4]:\n            if any(x for x in c if x > 1.0):\n                red, green, blue, alpha = _to_rgba(c[0] / 255.0, c[1] / 255.0, c[2] / 255.0, alpha=c[3] if len(c) == 4 else alpha)\n            else:\n                red, green, blue, alpha = _to_rgba(c[0], c[1], c[2], alpha=c[3] if len(c) == 4 else alpha)\n        else:\n            raise ValueError(\"RGB tuples must have 3 or 4 values\")\n    elif isinstance(c, str):\n        red, green, blue, alpha = _color_names.get(c, _to_rgba(0, 0, 0, 0))\n    elif isinstance(c, int):\n        red, green, blue, alpha = _to_rgba(((c >> 16) & 255) / 255.0, ((c >> 8) & 255) / 255.0, (c & 255) / 255.0, alpha=alpha)\n    else:\n        red, green, blue, alpha = _to_rgba(c, c, c, alpha=alpha)\n    return red, green, blue, alpha\n\n", "CodeBleu_score": 27.68442557886574}]}
{"instance_id": "4202e378ab0b", "generation_result": [{"code": "def subplots(self,*args,**kwargs)\n\t\n    if 'style' in kwargs:\n        self.mpfstyle = _styles.check_for_and_apply_style(kwargs['style'])\n    elif not hasattr(self,'mpfstyle'):\n        self.mpfstyle = _styles.check_for_and_apply_style('default')\n    else:\n        _styles.apply_style(self.mpfstyle,self)\n\n    result = self.figure.subplots(*args,**kwargs)\n\n    if isinstance(result,plt.Axes):\n        result.mpfstyle = self.mpfstyle\n    elif isinstance(result,np.ndarray):\n        for ax in result.flatten():\n            ax.mpfstyle = self.mpfstyle\n    else:\n        raise TypeError(\"Unexpected type for subplots result: \" + str(type(result)))\n\n    return result", "CodeBleu_score": 41.644226660840644}, {"code": "def subplots(self,*args,**kwargs)\n\t\n    if 'style' in kwargs:\n        self.mpfstyle = _styles.check_for_and_apply_style(self,kwargs['style'])\n    elif not hasattr(self,'mpfstyle'):\n        self.mpfstyle = _styles.check_for_and_apply_style(self,'default')\n    else:\n        _styles.apply_style(self,self.mpfstyle)\n    result = self.figure.subplots(*args,**kwargs)\n    if isinstance(result,plt.Axes):\n        _styles.apply_style(result,self.mpfstyle)\n    elif isinstance(result,np.ndarray):\n        for ax in result.flat:\n            _styles.apply_style(ax,self.mpfstyle)\n    else:\n        raise TypeError(\"Unexpected type for subplots result: %s\" % type(result))\n    return result", "CodeBleu_score": 40.55872339900184}, {"code": "def subplots(self,*args,**kwargs)\n\t\n    if 'style' in kwargs:\n        self.mpfstyle = _styles.check_for_and_apply_style(kwargs['style'])\n    elif not hasattr(self,'mpfstyle'):\n        self.mpfstyle = _styles.check_for_and_apply_style('default')\n\n    sp = self.figure.subplots(*args,**kwargs)\n    if isinstance(sp,plt.Axes):\n        sp.mpfstyle = self.mpfstyle\n    elif isinstance(sp,np.ndarray):\n        for ax in sp.flatten():\n            ax.mpfstyle = self.mpfstyle\n    else:\n        raise TypeError(\"Unexpected type for 'sp': {}\".format(type(sp)))\n\n    return sp", "CodeBleu_score": 40.93163625288039}]}
{"instance_id": "3e2b6dfe4753", "generation_result": [{"code": "def _adjust_figsize(fig,config)\n\t\n    if fig is None:\n        return\n    if 'figsize' not in config:\n        fig_ratio = config['figratio']\n        if fig_ratio is None:\n            fig_ratio = (16, 9)\n        if fig_ratio[0] / fig_ratio[1] > 1.5:\n            warnings.warn('figratio is too wide, adjusting to avoid excessive whitespace')\n            fig_ratio = (fig_ratio[0] / 1.5, fig_ratio[1])\n        fig.set_size_inches(fig_ratio[0] * config['scale'], fig_ratio[1] * config['scale'])\n    else:\n        fig.set_size_inches(config['figsize'])\n\n", "CodeBleu_score": 32.31624751657811}, {"code": "def _adjust_figsize(fig,config)\n\t\n    if fig is None:\n        return\n    if config['figsize'] is None:\n        fig_ratio = config['figratio']\n        if fig_ratio is None:\n            fig_ratio = (19.7, 11.27)  # Default ratio\n        fig_width_inch = config['figscale'] * fig_ratio[0]\n        fig_height_inch = config['figscale'] * fig_ratio[1]\n    else:\n        fig_width_inch, fig_height_inch = config['figsize']\n    fig.set_size_inches(fig_width_inch, fig_height_inch)\n\n", "CodeBleu_score": 35.65533898156357}, {"code": "def _adjust_figsize(fig,config)\n\t\n    if fig is None:\n        return\n    if 'figsize' in config:\n        fig.set_size_inches(config['figsize'])\n        return\n    fig_ratio = config['figratio']\n    fig_width = config['figscale']\n    fig_height = config['figscale']\n    if fig_ratio > 0:\n        fig_height = fig_width / fig_ratio\n    if fig_height < 1:\n        fig_height = 1\n    if fig_width < 1:\n        fig_width = 1\n    fig.set_size_inches(fig_width, fig_height)\n\n", "CodeBleu_score": 24.617924683657776}]}
{"instance_id": "986e5804b941", "generation_result": [{"code": "def _plot_mav(ax,config,xdates,prices,apmav=None,apwidth=None)\n\t\n    if apmav is None:\n        apmav = config['mav']\n    if apwidth is None:\n        apwidth = config['width']\n    if isinstance(apmav,dict):\n        shift = apmav['shift']\n        periods = apmav['periods']\n    else:\n        shift = [0]\n        periods = apmav\n    if len(periods) > 7:\n        raise ValueError('Too many moving averages specified.')\n    mav_colors = cycle(config['mavcolors'])\n    mav_linewidths = cycle(config['mavlinewidths'])\n    mav_prices = []\n    for period,shift in zip(periods,shift):\n        mav_prices.append(prices.rolling(period).mean().shift(shift))\n        mav_prices[-1].plot(ax=ax,linewidth=next(mav_linewidths),color=next(mav_colors),alpha=0.8,zorder=3)\n    return mav_prices\n\n", "CodeBleu_score": 34.23883518902126}, {"code": "def _plot_mav(ax,config,xdates,prices,apmav=None,apwidth=None)\n\t\n    if apmav is not None:\n        mav_dict = apmav\n    else:\n        mav_dict = config['mav']\n    if isinstance(mav_dict,dict):\n        shift_dict = mav_dict['shift']\n        period_dict = mav_dict['period']\n    else:\n        shift_dict = None\n        period_dict = None\n    mav_prices = []\n    mav_colors = cycle(config['mavcolors'])\n    mav_linewidths = cycle(config['mavlinewidths'])\n    for i,p in enumerate(period_dict):\n        if i > 6:\n            warnings.warn('_plot_mav: More than 7 moving averages specified. Only the first 7 will be plotted.')\n            break\n        if shift_dict is not None:\n            shift = shift_dict[p]\n        else:\n            shift = 0\n        mav_prices.append(prices.rolling(p).mean().shift(shift))\n        ax.plot(xdates,mav_prices[i],linewidth=next(mav_linewidths),color=next(mav_colors))\n    return mav_prices\n", "CodeBleu_score": 39.54175161052978}, {"code": "def _plot_mav(ax,config,xdates,prices,apmav=None,apwidth=None)\n\t\n    if apmav is None:\n        mav_config = config['mav']\n    else:\n        mav_config = apmav\n    if isinstance(mav_config, dict):\n        mav_shift = mav_config['shift']\n        mav_period = mav_config['period']\n    else:\n        mav_shift = [0]\n        mav_period = mav_config\n    if len(mav_period) > 7:\n        warnings.warn('Too many moving averages. 7 maximum.')\n        mav_period = mav_period[:7]\n    mav_prices = []\n    for i, period in enumerate(mav_period):\n        if isinstance(mav_config, dict):\n            shift = mav_shift[i]\n        else:\n            shift = 0\n        if shift != 0:\n            mav_prices.append(prices.rolling(period).mean().shift(shift))\n        else:\n            mav_prices.append(prices.rolling(period).mean())\n        ax.plot(xdates, mav_prices[i],\n                linewidth=apwidth,\n                color=next(config['color_cycle']))\n    return mav_prices\n\n", "CodeBleu_score": 38.091260800618414}]}
{"instance_id": "5308e20f3c90", "generation_result": [{"code": "def flops_conv(opnode: ConvolutionForward, inputs, outputs)\n\t\n    # Extract kernel height and width from input tensor shape\n    kh, kw = opnode.kernel_shape\n    # Determine number of input channels\n    ic = inputs[0].shape[1]\n    # Calculate total number of output elements\n    total_out_elements = outputs[0].shape[2] * outputs[0].shape[3]\n    # Check if operation includes bias term\n    has_bias = len(inputs) == 3\n    # Compute FLOPs\n    if has_bias:\n        return (2 * kh * kw * ic - 1) * total_out_elements + total_out_elements\n    else:\n        return 2 * kh * kw * ic * total_out_elements\n\n", "CodeBleu_score": 20.13345362010415}, {"code": "def flops_conv(opnode: ConvolutionForward, inputs, outputs)\n\t\n    assert opnode.op_type == \"Convolution\"\n    kernel_h, kernel_w = opnode.kernel_shape\n    if len(inputs[0].shape) == 3:\n        in_c = inputs[0].shape[0]\n    else:\n        in_c = inputs[0].shape[1]\n    out_h, out_w = outputs[0].shape[2], outputs[0].shape[3]\n    total_out_elements = out_h * out_w\n    kernel_ops = kernel_h * kernel_w\n    bias_ops = 1 if len(inputs) > 1 else 0\n    conv_ops = total_out_elements * (kernel_ops * in_c + bias_ops)\n    return conv_ops\n\n", "CodeBleu_score": 32.87022751499812}, {"code": "def flops_conv(opnode: ConvolutionForward, inputs, outputs)\n\t\n    kernel_height = opnode.kernel_height\n    kernel_width = opnode.kernel_width\n    num_input_channels = inputs[0].shape[1]\n    total_num_output_elements = outputs[0].shape[0] * outputs[0].shape[1] * outputs[0].shape[2] * outputs[0].shape[3]\n    has_bias = len(inputs) == 3\n    if has_bias:\n        return total_num_output_elements * (num_input_channels * kernel_height * kernel_width + 1)\n    else:\n        return total_num_output_elements * (num_input_channels * kernel_height * kernel_width)\n\n", "CodeBleu_score": 18.125843219212395}]}
{"instance_id": "70811d3cf578", "generation_result": [{"code": "def mge_dtype_to_ir_type(mge_dtype)\n\t\n    import numpy as np\n    mge_dtype = np.dtype(mge_dtype)\n    assert isinstance(mge_dtype, np.dtype)\n    return _dtype_to_ir_type[mge_dtype](mge_dtype)\n\n", "CodeBleu_score": 23.887518703391965}, {"code": "def mge_dtype_to_ir_type(mge_dtype)\n\t\n    import numpy as np\n\n    mge_dtype = np.dtype(mge_dtype)\n    assert isinstance(mge_dtype, np.dtype)\n    return _dtype_to_ir_type[mge_dtype](mge_dtype)\n\n", "CodeBleu_score": 23.887518703391965}, {"code": "def mge_dtype_to_ir_type(mge_dtype)\n\t\n    import numpy as np\n\n    mge_dtype = np.dtype(mge_dtype)\n    assert isinstance(mge_dtype, np.dtype)\n    return _dtype_to_ir_type[mge_dtype](mge_dtype)\n\n", "CodeBleu_score": 23.887518703391965}]}
{"instance_id": "a108c9ba2c22", "generation_result": [{"code": "def _numpy_array_constant(x: np.ndarray, canonicalize_types) -> Sequence[ir.Value]\n\t\n    if canonicalize_types:\n        x = x.copy()\n        x.dtype = np.canonical_dtype(x.dtype)\n    ir_type = ir.RankedTensorType(x.shape, ir_type_from_dtype(x.dtype))\n    x = np.ascontiguousarray(x)\n    elements = ir.DenseElementsAttr.get(x.flatten(), type=ir_type)\n    return [ir.ConstantOp(elements, ir_type).result]\n\n", "CodeBleu_score": 34.80600767983582}, {"code": "def _numpy_array_constant(x: np.ndarray, canonicalize_types) -> Sequence[ir.Value]\n\t\n    if canonicalize_types:\n        x = x.copy()\n        x.dtype = canonicalize_dtype(x.dtype)\n    if x.dtype == np.bool_:\n        x = x.astype(np.int8)\n    if x.dtype == np.bfloat16:\n        x = x.astype(np.float32)\n    x = np.ascontiguousarray(x)\n    elements = ir.DenseElementsAttr.get(\n        x.shape, x.dtype, x.reshape([-1])\n    )\n    return [ir.ConstantOp(elements, ir.TypeAttr(ir.RankedTensorType(x.shape, x.dtype)))]\n\n", "CodeBleu_score": 43.448675322341884}, {"code": "def _numpy_array_constant(x: np.ndarray, canonicalize_types) -> Sequence[ir.Value]\n\t\n    if canonicalize_types:\n        x = x.astype(canonicalize_dtype(x.dtype))\n    ir_type = ir.RankedTensorType(x.shape, dtype_to_ir(x.dtype))\n    if x.dtype == np.bool_:\n        x = x.astype(np.int8)\n    elif x.dtype == np.bfloat16:\n        x = x.astype(np.float32)\n    x = np.ascontiguousarray(x)\n    elements = ir.DenseElementsAttr.get(x.ravel(), type=ir_type)\n    return ir.ConstantOp(elements, ir_type).result\n\n", "CodeBleu_score": 44.47315485708712}]}
{"instance_id": "3345fe4d008b", "generation_result": [{"code": "def _ndarray_constant_handler(\n    val: np.ndarray, canonicalize_types\n) -> Sequence[ir.Value]\n\t\n    if not val.size:\n        return [ir.Constant(val)]\n    if not val.strides:\n        return [ir.Constant(val)]\n    zero_stride_axes = []\n    non_zero_stride_axes = []\n    for i, stride in enumerate(val.strides):\n        if stride == 0:\n            zero_stride_axes.append(i)\n        else:\n            non_zero_stride_axes.append(i)\n    if zero_stride_axes:\n        collapsed_val = val\n        for i in zero_stride_axes:\n            collapsed_val = collapsed_val[0]\n        if canonicalize_types:\n            collapsed_val = collapsed_val.astype(val.dtype)\n        broadcast_op = ir.Broadcast(\n            ir.Constant(collapsed_val),\n            shape=val.shape,\n            zero_stride_axes=zero_stride_axes,\n            non_zero_stride_axes=non_zero_stride_axes,\n        )\n        return [broadcast_op]\n    else:\n        return _numpy_array_constant(val, canonicalize_types)\n\n", "CodeBleu_score": 38.051915022970654}, {"code": "def _ndarray_constant_handler(\n    val: np.ndarray, canonicalize_types\n) -> Sequence[ir.Value]\n\t\n    if not val.size:\n        return []\n    # Collapse zero strides.\n    val_shape = val.shape\n    val_strides = val.strides\n    zero_strides = [i for i, s in enumerate(val_strides) if not s]\n    non_zero_strides = [i for i, s in enumerate(val_strides) if s]\n    if zero_strides:\n        # Collapse zero strides.\n        val = val.reshape(\n            [val_shape[i] for i in non_zero_strides],\n            order=\"C\",\n        )\n        val_shape = val.shape\n        val_strides = val.strides\n\n    # Process collapsed array.\n    return _numpy_array_constant(val, canonicalize_types)\n\n", "CodeBleu_score": 21.93295963817435}, {"code": "def _ndarray_constant_handler(\n    val: np.ndarray, canonicalize_types\n) -> Sequence[ir.Value]\n\t\n    if val.strides.count(0) > 0 and val.size > 0:\n        # Collapse zero-stride axes\n        zero_stride_axes = []\n        non_zero_stride_axes = []\n        for i, stride in enumerate(val.strides):\n            if stride == 0:\n                zero_stride_axes.append(i)\n            else:\n                non_zero_stride_axes.append(i)\n        collapsed_val = val.reshape(\n            [val.shape[i] for i in non_zero_stride_axes]\n        )\n        if canonicalize_types:\n            collapsed_val = collapsed_val.astype(\n                canonicalize_dtype(collapsed_val.dtype)\n            )\n        # Broadcast the collapsed array to the original shape\n        broadcasted_val = np.broadcast_to(\n            collapsed_val,\n            [val.shape[i] for i in range(val.ndim)],\n        )\n        # Return the broadcasted array\n        return _numpy_array_constant(broadcasted_val)\n    else:\n        return _numpy_array_constant(val, canonicalize_types)\n\n", "CodeBleu_score": 39.01293145194064}]}
{"instance_id": "91b298678351", "generation_result": [{"code": "def dense_bool_elements(xs: Sequence[bool]) -> ir.DenseElementsAttr\n\t\n    if len(xs) == 1:\n        xs = [0] if xs[0] else [1]\n    else:\n        xs = [int(x) for x in xs]\n    packed = pack_bits(xs)\n    return ir.DenseElementsAttr.get(\n        np.array(packed, dtype=np.uint8), \"ui1\", shape=(len(xs),)\n    )\n\n", "CodeBleu_score": 41.125496269525854}, {"code": "def dense_bool_elements(xs: Sequence[bool]) -> ir.DenseElementsAttr\n\t\n    from .ir import DenseElementsAttr\n\n    if len(xs) == 1:\n        xs = [0] if xs[0] else [1]\n    else:\n        xs = [int(x) for x in xs]\n    packed = pack_bits(xs)\n    return DenseElementsAttr.get(\n        type=ir.IntegerType.get_signless(1),\n        shape=Shape.get(len(xs)),\n        data_buffer=packed,\n    )\n\n", "CodeBleu_score": 39.339245574582336}, {"code": "def dense_bool_elements(xs: Sequence[bool]) -> ir.DenseElementsAttr\n\t\n    if len(xs) == 1:\n        xs = [xs[0]]\n    packed = pack_bits(xs)\n    if len(packed) == 0:\n        packed = [0]\n    return ir.DenseElementsAttr.get(\n        np.array(packed, dtype=np.uint8), type=ir.IntegerType.get_signless(1)\n    ).reshape((len(xs),))\n\n", "CodeBleu_score": 42.24542217815098}]}
{"instance_id": "06ebabf1e0ff", "generation_result": [{"code": "def create_tensor_lower(ctx, *args: Union[HLOTensor, Sequence[HLOTensor]])\n\t\n    assert len(args) == 1\n    assert len(ctx.vars_in) == 1\n    assert len(ctx.vars_out) == 1\n    input_var, = ctx.vars_in\n    output_var, = ctx.vars_out\n    if input_var.bound_data is not None:\n        ctx.module.set_input(input_var.name, input_var.bound_data)\n        ctx.module.set_output(output_var.name, input_var.bound_data)\n    assert input_var.shape == output_var.shape\n    if output_var.bound_data is not None:\n        output_var.bound_data = output_var.bound_data.astype(output_var.dtype)\n        return HLOTensor(output_var.bound_data)\n    if input_var.bound_data is not None:\n        input_var.bound_data = input_var.bound_data.astype(input_var.dtype)\n        return HLOTensor(input_var.bound_data)\n    raise AssertionError(\"Only tensor creation from constants is supported\")\n\n", "CodeBleu_score": 46.08598143622594}, {"code": "def create_tensor_lower(ctx, *args: Union[HLOTensor, Sequence[HLOTensor]])\n\t\n    assert len(args) == 1\n    assert len(ctx.vars_in) == 1\n    assert len(ctx.vars_out) == 1\n    inp = ctx.vars_in[0]\n    out = ctx.vars_out[0]\n    if inp.bound_data is not None:\n        ctx.module.set_input(inp.name, inp.bound_data)\n        ctx.module.set_output(out.name, inp.bound_data)\n    assert inp.shape == out.shape\n    if out.bound_data is not None:\n        out.bound_data = out.bound_data.astype(out.dtype)\n        return HLOTensor(out.bound_data)\n    if inp.bound_data is not None:\n        inp.bound_data = inp.bound_data.astype(inp.dtype)\n        return HLOTensor(inp.bound_data)\n    raise AssertionError(\"Only tensor creation from constants is supported\")\n\n", "CodeBleu_score": 50.99177566295665}, {"code": "def create_tensor_lower(ctx, *args: Union[HLOTensor, Sequence[HLOTensor]])\n\t\n    assert len(args) == 1, \"Only one argument is supported.\"\n    assert len(ctx.vars_in) == 1, \"Only one input variable is supported.\"\n    assert len(ctx.vars_out) == 1, \"Only one output variable is supported.\"\n\n    in_var = ctx.vars_in[0]\n    out_var = ctx.vars_out[0]\n\n    if in_var.bound_data is not None:\n        ctx.module.set_initializer(in_var.name, in_var.bound_data)\n        ctx.module.set_initializer(out_var.name, in_var.bound_data)\n    else:\n        raise AssertionError(\n            \"Only tensor creation from constants is supported.\"\n        )\n\n    assert in_var.shape == out_var.shape, \"Input and output shapes must be equal.\"\n\n    if out_var.bound_data is not None:\n        data = out_var.bound_data.astype(out_var.dtype)\n    elif in_var.bound_data is not None:\n        data = in_var.bound_data.astype(in_var.dtype)\n    else:\n        raise AssertionError(\n            \"Only tensor creation from constants is supported.\"\n        )\n\n    return HLOTensor(data)\n\n", "CodeBleu_score": 51.98010316820005}]}
{"instance_id": "6cd750fa0c8d", "generation_result": [{"code": "def _device_put_scalar(x, device)\n\t\n    if not isinstance(x, (int, float)):\n        raise ValueError(\"x must be a scalar value\")\n    x = _to_numpy(x)\n    return _to_device(x, device)\n\n", "CodeBleu_score": 22.51461161115754}, {"code": "def _device_put_scalar(x, device)\n\t\n    return _device_put(\n        _as_numpy_scalar(x), device, dtype=None, like=None, copy=False, non_blocking=False\n    )\n\n", "CodeBleu_score": 13.661308670286697}, {"code": "def _device_put_scalar(x, device)\n\t\n    if not isinstance(x, (int, float, bool)):\n        raise ValueError(\"x must be a scalar value\")\n\n    if not hasattr(device, \"__module__\"):\n        raise ValueError(\"device must be a valid device\")\n\n    if device.__module__ == \"numpy\":\n        if isinstance(x, bool):\n            return device.array(x, dtype=device.bool)\n        elif isinstance(x, int):\n            return device.array(x, dtype=device.int32)\n        elif isinstance(x, float):\n            return device.array(x, dtype=device.float32)\n    elif device.__module__ == \"jax.interpreters.xla\":\n        if isinstance(x, bool):\n            return device.array(x, dtype=device.bool_)\n        elif isinstance(x, int):\n            return device.array(x, dtype=device.int32)\n        elif isinstance(x, float):\n            return device.array(x, dtype=device.float32)\n    else:\n        raise ValueError(\"device must be a valid device\")\n\n", "CodeBleu_score": 19.619172556657126}]}
{"instance_id": "140de7c2b6a8", "generation_result": [{"code": "def _shape_equal(lhs_shape, rhs_shape)\n\t\n    if isinstance(lhs_shape, np.ndarray):\n        lhs_shape = list(lhs_shape)\n    if isinstance(rhs_shape, np.ndarray):\n        rhs_shape = list(rhs_shape)\n    assert isinstance(lhs_shape, (tuple, list)), \"lhs_shape must be a tuple or list.\"\n    assert isinstance(rhs_shape, (tuple, list)), \"rhs_shape must be a tuple or list.\"\n    if len(lhs_shape) == 0 and len(rhs_shape) == 0:\n        return True\n    if len(lhs_shape) > 0 and len(rhs_shape) > 0:\n        assert isinstance(lhs_shape[0], int), \"lhs_shape must be a tuple or list of integers.\"\n        assert isinstance(rhs_shape[0], int), \"rhs_shape must be a tuple or list of integers.\"\n    if len(lhs_shape) != len(rhs_shape):\n        return False\n    for i in range(len(lhs_shape)):\n        if lhs_shape[i] != rhs_shape[i]:\n            return False\n    return True\n\n", "CodeBleu_score": 49.964155855097054}, {"code": "def _shape_equal(lhs_shape, rhs_shape)\n\t\n    if isinstance(lhs_shape, numpy.ndarray):\n        lhs_shape = list(lhs_shape)\n    if isinstance(rhs_shape, numpy.ndarray):\n        rhs_shape = list(rhs_shape)\n    assert isinstance(lhs_shape, (tuple, list)),\\\n        \"lhs_shape must be a tuple or list, but got {}\".format(type(lhs_shape))\n    assert isinstance(rhs_shape, (tuple, list)),\\\n        \"rhs_shape must be a tuple or list, but got {}\".format(type(rhs_shape))\n    if len(lhs_shape) == 0 and len(rhs_shape) == 0:\n        return True\n    if len(lhs_shape) > 0:\n        assert isinstance(lhs_shape[0], int),\\\n            \"lhs_shape must be a tuple or list of integers, but got {}\".format(type(lhs_shape[0]))\n    if len(rhs_shape) > 0:\n        assert isinstance(rhs_shape[0], int),\\\n            \"rhs_shape must be a tuple or list of integers, but got {}\".format(type(rhs_shape[0]))\n    if len(lhs_shape) != len(rhs_shape):\n        return False\n    for i in range(len(lhs_shape)):\n        if lhs_shape[i] != rhs_shape[i]:\n            return False\n    return True\n\n", "CodeBleu_score": 48.7505432150202}, {"code": "def _shape_equal(lhs_shape, rhs_shape)\n\t\n    if not isinstance(lhs_shape, (tuple, list)):\n        lhs_shape = list(lhs_shape)\n    if not isinstance(rhs_shape, (tuple, list)):\n        rhs_shape = list(rhs_shape)\n    if not lhs_shape and not rhs_shape:\n        return True\n    assert isinstance(lhs_shape, (tuple, list))\n    assert isinstance(rhs_shape, (tuple, list))\n    if len(lhs_shape) and len(rhs_shape):\n        if isinstance(lhs_shape[0], int) and isinstance(rhs_shape[0], int):\n            if lhs_shape[0] != rhs_shape[0]:\n                return False\n            lhs_shape = lhs_shape[1:]\n            rhs_shape = rhs_shape[1:]\n    if len(lhs_shape) != len(rhs_shape):\n        return False\n    for i, j in zip(lhs_shape, rhs_shape):\n        if i != j:\n            return False\n    return True\n\n", "CodeBleu_score": 45.19732594400874}]}
{"instance_id": "55d0c71a259b", "generation_result": [{"code": "def read(\n    training_set_filename,\n    test_set_filename,\n    training_set_metadata_filename,\n    test_set_metadata_filename,\n    dtypes,\n    meta_dtypes,\n)\n\t\n    start = time.time()\n    print(\"Reading training set...\")\n    train = pd.read_csv(training_set_filename, dtype=dtypes)\n    print(\"Reading test set...\")\n    test = pd.read_csv(test_set_filename, dtype=dtypes)\n    print(\"Reading training set metadata...\")\n    train_metadata = pd.read_csv(training_set_metadata_filename, dtype=meta_dtypes)\n    print(\"Reading test set metadata...\")\n    test_metadata = pd.read_csv(test_set_metadata_filename, dtype=meta_dtypes)\n    end = time.time()\n    print(f\"Time elapsed: {end - start:.2f} seconds\")\n    return train, test, train_metadata, test_metadata\n\n", "CodeBleu_score": 46.82220496824463}, {"code": "def read(\n    training_set_filename,\n    test_set_filename,\n    training_set_metadata_filename,\n    test_set_metadata_filename,\n    dtypes,\n    meta_dtypes,\n)\n\t\n    training_set = pd.read_csv(training_set_filename, dtype=dtypes)\n    test_set = pd.read_csv(test_set_filename, dtype=dtypes)\n    training_metadata = pd.read_csv(training_set_metadata_filename, dtype=meta_dtypes)\n    test_metadata = pd.read_csv(\n        test_set_metadata_filename,\n        dtype={**meta_dtypes, \"target\": None},\n    )\n    return training_set, test_set, training_metadata, test_metadata\n\n", "CodeBleu_score": 30.142402921772156}, {"code": "def read(\n    training_set_filename,\n    test_set_filename,\n    training_set_metadata_filename,\n    test_set_metadata_filename,\n    dtypes,\n    meta_dtypes,\n)\n\t\n    training_set = pd.read_csv(training_set_filename, dtype=dtypes)\n    test_set = pd.read_csv(test_set_filename, dtype=dtypes)\n    training_metadata = pd.read_csv(training_set_metadata_filename, dtype=meta_dtypes)\n    test_metadata = pd.read_csv(test_set_metadata_filename, dtype=meta_dtypes)\n    return training_set, test_set, training_metadata, test_metadata\n\n", "CodeBleu_score": 25.66820946095164}]}
{"instance_id": "9e5e02cd73e0", "generation_result": [{"code": "def _show_cmap(cmap)\n\t\n    from matplotlib import axes as axes_style\n    import matplotlib.pyplot as plt\n    import numpy as np\n    with plt.style.context(axes_style(\"white\")):\n        fig, ax = plt.subplots(1, 1, figsize=(8, 2), dpi=80)\n        ax.set_axis_off()\n        ax.set_title(\"%s colormap\" % cmap, fontsize=14)\n        ax.imshow(np.arange(cmap.N).reshape(1, -1), aspect=\"auto\", cmap=cmap)\n\n", "CodeBleu_score": 33.76753622816394}, {"code": "def _show_cmap(cmap)\n\t\n    from matplotlib import axes\n    from matplotlib import rcmod\n    with rcmod.axes_style(\"white\"):\n        fig, ax = axes.subplots(1, 1, figsize=(8, 0.5))\n        ax.set_axis_off()\n        ax.set_xlim([0, 1])\n        ax.set_ylim([0, 1])\n        ax.pcolormesh(np.linspace(0, 1, 256).reshape(1, -1), cmap=cmap)\n\n", "CodeBleu_score": 33.44359510967173}, {"code": "def _show_cmap(cmap)\n\t\n    from matplotlib import rcParams\n    from matplotlib.style import axes_style\n    rcParams[\"axes.style\"] = axes_style(rc={\"axes.grid\": False})\n    fig, ax = plt.subplots(figsize=(8, 2))\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.imshow(np.linspace(0, 1, 256).reshape(1, -1), aspect=\"auto\", cmap=cmap)\n\n", "CodeBleu_score": 37.795157826332606}]}
{"instance_id": "ad0781b5cc38", "generation_result": [{"code": "def transform_to_data_coordinates(obj, xdata, ydata)\n\t\n    if obj.axes is not None and obj.get_transform() != obj.axes.transData:\n        transData = obj.axes.transData\n        points = transData.transform(zip(xdata, ydata))\n        return points[:, 0], points[:, 1]\n    else:\n        return xdata, ydata\n\n", "CodeBleu_score": 50.670729202126566}, {"code": "def transform_to_data_coordinates(obj, xdata, ydata)\n\t\n    if obj.axes is not None and obj.get_transform() != obj.axes.transData:\n        transData = obj.axes.transData\n        points = transData.transform(zip(xdata, ydata))\n        return points[:, 0], points[:, 1]\n    else:\n        return xdata, ydata\n\n", "CodeBleu_score": 50.670729202126566}, {"code": "def transform_to_data_coordinates(obj, xdata, ydata)\n\t\n    if obj.axes is not None and obj.get_transform() != obj.axes.transData:\n        points = obj.get_xydata().copy()\n        points[:, 0] = xdata\n        points[:, 1] = ydata\n        return obj.get_transform().transform(points)\n    else:\n        return xdata, ydata\n\n", "CodeBleu_score": 51.65288534270533}]}
{"instance_id": "bf776cadef8d", "generation_result": [{"code": "def kdeplot(self, data=None, data2=None, *args, **kwargs)\n\t\n    data = self._maybe_target_series(data, 'data')\n    if data2 is not None and not _is_list_like(data2):\n        data2 = self._df[data2]\n\n    return self._module.kdeplot(data, data2, *args, **kwargs)\n\n", "CodeBleu_score": 56.128811050434834}, {"code": "def kdeplot(self, data=None, data2=None, *args, **kwargs)\n\t\n    data = self._maybe_target_series(data, 'data')\n    if data2 is not None and not _is_list_like(data2):\n        data2 = self._df[data2]\n\n    return self._module.kdeplot(data, data2, *args, **kwargs)\n\n", "CodeBleu_score": 56.128811050434834}, {"code": "def kdeplot(self, data=None, data2=None, *args, **kwargs)\n\t\n    data = self._maybe_target_series(data, 'data')\n    if data2 is not None and not _is_list_like(data2):\n        data2 = self._df[data2]\n\n    return self._module.kdeplot(data, data2, *args, **kwargs)\n\n", "CodeBleu_score": 56.128811050434834}]}
{"instance_id": "fb44964174d9", "generation_result": [{"code": "def plot_most_used_words(df)\n\t\n    df[\"token\"] = df[\"token\"].str.replace(\"programa\", \"programar\")\n    df = df[df[\"token\"].str.isalpha()]\n    df = df[df[\"token\"].str.len() > 1]\n    df = df[~df[\"token\"].isin(stopwords)]\n    df = df[\"token\"].value_counts().reset_index()\n    df.columns = [\"token\", \"count\"]\n    df = df.head(20)\n    df.plot.bar(x=\"token\", y=\"count\", legend=False)\n    plt.title(\"Most Frequent Words\")\n    plt.xlabel(\"Word\")\n    plt.ylabel(\"Frequency\")\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    plt.savefig(\"images/most_frequent_words.png\")\n\n", "CodeBleu_score": 37.55247869146102}, {"code": "def plot_most_used_words(df)\n\t\n    df.loc[df[\"word\"] == \"programa\", \"word\"] = \"programar\"\n    df = df[df[\"word\"].str.isalpha()]\n    df = df[df[\"word\"].str.len() > 1]\n    df = df[~df[\"word\"].isin(geopandas.datasets.get_path(\"naturalearth_lowres\"))]\n    df = df.groupby(\"word\").size().reset_index(name=\"count\")\n    df = df.sort_values(by=\"count\", ascending=False)\n    df = df.head(20)\n    df.plot.bar(x=\"word\", y=\"count\", legend=False)\n    plt.title(\"Most used words in the DataFrame\")\n    plt.xlabel(\"Word\")\n    plt.ylabel(\"Frequency\")\n    plt.savefig(\"most_used_words.png\")\n    plt.show()\n\n", "CodeBleu_score": 28.788935998065156}, {"code": "def plot_most_used_words(df)\n\t\n    df[\"token\"] = df[\"token\"].str.replace(\"programa\", \"programar\")\n    df = df[(df[\"token\"].str.isalpha()) & (df[\"token\"].str.len() > 1)]\n    df = df[~df[\"token\"].isin(geopandas.datasets.get_path(\"naturalearth_lowres\"))]\n    df = df[\"token\"].value_counts()[:20]\n    df.plot(kind=\"bar\", figsize=(10, 5))\n    plt.title(\"Most Used Words\")\n    plt.xlabel(\"Word\")\n    plt.ylabel(\"Frequency\")\n    plt.savefig(\"most_used_words.png\")\n\n", "CodeBleu_score": 37.26021283658821}]}
{"instance_id": "13414a7eb4e3", "generation_result": [{"code": "def dask_rolling_wrapper(moving_func, a, window, min_count=None, axis=-1)\n\t\n    import dask.array as da\n\n    a = da.from_array(a, chunks=(500, 500))\n    axis = a.ndim + axis if axis < 0 else axis\n\n    depth = {\n        dtypes.int8: 1,\n        dtypes.int16: 2,\n        dtypes.int32: 2,\n        dtypes.int64: 2,\n        dtypes.float32: 2,\n        dtypes.float64: 3,\n    }[a.dtype]\n\n    boundary = {\n        dtypes.int8: dtypes.NA,\n        dtypes.int16: dtypes.NA,\n        dtypes.int32: dtypes.NA,\n        dtypes.int64: dtypes.NA,\n        dtypes.float32: dtypes.nan,\n        dtypes.float64: dtypes.nan,\n    }[a.dtype]\n\n    ag = da.overlap.overlap(a, depth=depth, axis=axis, boundary=boundary)\n    out = da.map_blocks(\n        moving_func, ag, window, min_count=min_count, axis=axis, dtype=a.dtype\n    )\n    return da.overlap.trim_internal(out, depth, axis)\n\n", "CodeBleu_score": 34.252917091664926}, {"code": "def dask_rolling_wrapper(moving_func, a, window, min_count=None, axis=-1)\n\t\n    import dask.array as da\n\n    a = da.from_array(a, chunks=(a.shape[0], -1))\n    axis = a.ndim + axis if axis < 0 else axis\n\n    if min_count is None:\n        min_count = window\n\n    depth = {dtypes.get_dtype(k): v for k, v in dtypes.rolling_depth.items()}[\n        a.dtype.type\n    ]\n    boundary = dtypes.rolling_boundary(a.dtype.type, window, depth)\n    ag = da.overlap.overlap(a, depth=depth, boundary=boundary)\n    out = da.map_blocks(\n        moving_func, ag, window, min_count, axis=axis, dtype=a.dtype\n    )\n    out = da.overlap.trim_internal(out, depth)\n    return out\n\n", "CodeBleu_score": 49.39588064561799}, {"code": "def dask_rolling_wrapper(moving_func, a, window, min_count=None, axis=-1)\n\t\n    import dask.array as da\n\n    a = da.from_array(a, chunks=(a.shape[0], -1))\n    dtype, fill_value = dtypes.maybe_promote(a.dtype)\n    a = a.astype(dtype)\n\n    depth = {d: 0 for d in range(a.ndim)}\n    depth[axis] = window // 2\n\n    boundary = {d: fill_value for d in range(a.ndim)}\n    boundary[axis] = \"nearest\"\n\n    ag = da.overlap.overlap(a, depth=depth, boundary=boundary)\n    out = da.map_blocks(\n        moving_func, ag, window, min_count=min_count, axis=axis, dtype=a.dtype\n    )\n    out = da.overlap.trim_internal(out, depth)\n\n    return out\n\n", "CodeBleu_score": 51.766281224376975}]}
{"instance_id": "68ce25d325d4", "generation_result": [{"code": "def least_squares(lhs, rhs, rcond=None, skipna=False)\n\t\n    lhs, rhs = dask.array.core.normalize_args(lhs, rhs)\n    if rcond is None:\n        rcond = np.finfo(lhs.dtype).eps * max(lhs.shape)\n    if skipna:\n        lhs, rhs = dask.array.core.normalize_dtypes(lhs, rhs)\n        if rhs.ndim > 1:\n            rhs = rhs.reshape((rhs.shape[0], -1))\n        coeffs, residuals, rank, s = np.linalg.lstsq(lhs, rhs, rcond=rcond)\n        return coeffs, residuals\n    else:\n        lhs, rhs = dask.array.core.normalize_dtypes(lhs, rhs)\n        if lhs.ndim > 1:\n            lhs = lhs.reshape((lhs.shape[0], -1))\n        if rhs.ndim > 1:\n            rhs = rhs.reshape((rhs.shape[0], -1))\n        coeffs, residuals, rank, s = np.linalg.lstsq(lhs, rhs, rcond=rcond)\n        return coeffs, residuals\n\n", "CodeBleu_score": 30.556155411140395}, {"code": "def least_squares(lhs, rhs, rcond=None, skipna=False)\n\t\n    lhs = dtypes.promote_dtypes(lhs, rhs)\n    rhs = dtypes.promote_dtypes(lhs, rhs)\n    lhs = nputils.asarray(lhs)\n    rhs = nputils.asarray(rhs)\n    if rhs.ndim == 1:\n        rhs = rhs[:, None]\n    if lhs.ndim == 1:\n        lhs = lhs[:, None]\n    if skipna:\n        if rhs.ndim > 1:\n            rhs = rhs.T\n        if lhs.ndim > 1:\n            lhs = lhs.T\n        coeffs, residuals = nputils.polyfit(lhs, rhs, deg=1, rcond=rcond)\n    else:\n        coeffs, residuals = nputils.lstsq(lhs, rhs, rcond=rcond)\n    return coeffs, residuals", "CodeBleu_score": 23.875561266242112}, {"code": "def least_squares(lhs, rhs, rcond=None, skipna=False)\n\t\n    lhs = dask_array(lhs)\n    rhs = dask_array(rhs)\n    if skipna:\n        rhs = rhs.reshape((rhs.shape[0], -1))\n        lhs = lhs.reshape((lhs.shape[0], -1))\n        lhs, rhs = dask.compute(lhs, rhs)\n        lhs, rhs = np.asarray(lhs), np.asarray(rhs)\n        c, res, _, _ = np.linalg.lstsq(lhs, rhs, rcond=rcond)\n        c, res = da.from_array(c, chunks=(c.shape[0],)), da.from_array(res, chunks=(res.shape[0],))\n    else:\n        c, res = da.linalg.lstsq(lhs, rhs)\n    return c, res", "CodeBleu_score": 33.47785817014088}]}
{"instance_id": "c0c2c53b76e0", "generation_result": [{"code": "def from_array(\n        self, data: Any, chunks: T_Chunks | _NormalizedChunks, **kwargs: Any\n    ) -> DaskArray | Any\n\t\n    import dask.array as da\n\n    if isinstance(data, ImplicitToExplicitIndexingAdapter):\n        kwargs.setdefault(\"meta\", np.ndarray)\n\n    return da.from_array(data, chunks, **kwargs)\n\n", "CodeBleu_score": 56.7921307517903}, {"code": "def from_array(\n        self, data: Any, chunks: T_Chunks | _NormalizedChunks, **kwargs: Any\n    ) -> DaskArray | Any\n\t\n    import dask.array as da\n\n    if isinstance(data, ImplicitToExplicitIndexingAdapter):\n        kwargs[\"meta\"] = np.ndarray\n    return da.from_array(data, chunks=chunks, **kwargs)\n\n", "CodeBleu_score": 65.96475708138769}, {"code": "def from_array(\n        self, data: Any, chunks: T_Chunks | _NormalizedChunks, **kwargs: Any\n    ) -> DaskArray | Any\n\t\n    import dask.array as da\n\n    if isinstance(data, ImplicitToExplicitIndexingAdapter):\n        kwargs[\"meta\"] = np.ndarray\n    return da.from_array(data, chunks, **kwargs)\n\n", "CodeBleu_score": 70.27898286466454}]}
{"instance_id": "6559019d09a0", "generation_result": [{"code": "def __dask_postpersist__(\n        self,\n    ) -> tuple[\n        Callable[\n            [Graph, PostPersistCallable[Any], Any, Any],\n            Self,\n        ],\n        tuple[Any, ...],\n    ]\n\t\n    if is_duck_dask_array(self.data):\n        return self.data.__dask_postpersist__(self)\n    else:\n        raise AttributeError(\n            \"The data is not a dask array, so it does not have a __dask_postpersist__ method.\"\n        )\n\n", "CodeBleu_score": 38.16917053841526}, {"code": "def __dask_postpersist__(\n        self,\n    ) -> tuple[\n        Callable[\n            [Graph, PostPersistCallable[Any], Any, Any],\n            Self,\n        ],\n        tuple[Any, ...],\n    ]\n\t\n    if is_duck_dask_array(self.data):\n        return self.data.__dask_postpersist__(self)\n    else:\n        raise AttributeError(\n            f\"{type(self.data)} object has no attribute '__dask_postpersist__'\"\n        )\n\n", "CodeBleu_score": 35.393448821868546}, {"code": "def __dask_postpersist__(\n        self,\n    ) -> tuple[\n        Callable[\n            [Graph, PostPersistCallable[Any], Any, Any],\n            Self,\n        ],\n        tuple[Any, ...],\n    ]\n\t\n    if is_duck_dask_array(self.data):\n        func, args = self.data.__dask_postpersist__()\n\n        def finalize(\n                graph: Graph,\n                func: PostPersistCallable[Any],\n                *args: Any,\n                **kwargs: Any,\n            ) -> Self:\n            return self._post_persist(func, *args, **kwargs)\n\n        return finalize, (func, *args)\n    else:\n        raise AttributeError(\n            \"__dask_postpersist__ is only defined for dask arrays\",\n        )\n\n", "CodeBleu_score": 48.52414807998817}]}
{"instance_id": "4a2801bb3851", "generation_result": [{"code": "def _dask_or_eager_func(\n    name,\n    eager_module=np,\n    dask_module=\"dask.array\",\n)\n\t\n\n    def f(*args, **kwargs):\n        if any(is_duck_dask_array(arg) for arg in args):\n            if isinstance(dask_module, str):\n                dask_module = import_module(dask_module)\n            return getattr(dask_module, name)(*args, **kwargs)\n        return getattr(eager_module, name)(*args, **kwargs)\n\n    return f\n\n", "CodeBleu_score": 46.92426385128267}, {"code": "def _dask_or_eager_func(\n    name,\n    eager_module=np,\n    dask_module=\"dask.array\",\n)\n\t\n\n    def f(*args, **kwargs):\n        if any(is_duck_dask_array(arg) for arg in args):\n            if isinstance(dask_module, str):\n                dask_module = import_module(dask_module)\n            func = getattr(dask_module, name)\n        else:\n            func = getattr(eager_module, name)\n        return func(*args, **kwargs)\n\n    return f\n\n", "CodeBleu_score": 57.520993331871395}, {"code": "def _dask_or_eager_func(\n    name,\n    eager_module=np,\n    dask_module=\"dask.array\",\n)\n\t\n\n    def f(*args, **kwargs):\n        if any(is_duck_dask_array(arg) for arg in args):\n            if isinstance(dask_module, str):\n                dask_module = import_module(dask_module)\n            func = getattr(dask_module, name)\n        else:\n            func = getattr(eager_module, name)\n        return func(*args, **kwargs)\n\n    return f\n\n", "CodeBleu_score": 57.520993331871395}]}
{"instance_id": "eef8d746d01e", "generation_result": [{"code": "def lazy_array_equiv(arr1, arr2)\n\t\n    if arr1 is arr2:\n        return True\n    arr1 = nputils.asarray(arr1)\n    arr2 = nputils.asarray(arr2)\n    if arr1.shape != arr2.shape:\n        return False\n    if module_available(\"dask\"):\n        if is_duck_dask_array(arr1) and is_duck_dask_array(arr2):\n            return dask_array_ops.tokenize(arr1) == dask_array_ops.tokenize(arr2)\n        elif is_duck_array(arr1) and is_duck_array(arr2):\n            return dask_array_ops.tokenize(arr1) == dask_array_ops.tokenize(arr2)\n    return None\n\n", "CodeBleu_score": 56.44089281203115}, {"code": "def lazy_array_equiv(arr1, arr2)\n\t\n    if arr1 is arr2:\n        return True\n    arr1 = nputils.asarray(arr1)\n    arr2 = nputils.asarray(arr2)\n    if arr1.shape != arr2.shape:\n        return False\n    if module_available(\"dask\") and is_duck_dask_array(arr1) and is_duck_dask_array(arr2):\n        import dask.array\n\n        return dask.array.utils.tokenize(arr1) == dask.array.utils.tokenize(arr2)\n    return None\n\n", "CodeBleu_score": 59.727112759113595}, {"code": "def lazy_array_equiv(arr1, arr2)\n\t\n    if arr1 is arr2:\n        return True\n    arr1 = np.asarray(arr1)\n    arr2 = np.asarray(arr2)\n    if arr1.shape != arr2.shape:\n        return False\n    if module_available(\"dask\") and is_duck_dask_array(arr1) and is_duck_dask_array(arr2):\n        return dask_array_ops.tokenize(arr1) == dask_array_ops.tokenize(arr2)\n    return None\n\n", "CodeBleu_score": 57.89480827211473}]}
{"instance_id": "15e830ff5fcb", "generation_result": [{"code": "def randn(shape, frac_nan=None, chunks=None, seed=0)\n\t\n    import numpy as np\n    import dask.array as da\n    from numpy.random import default_rng\n    rng = default_rng(seed)\n    if chunks is not None:\n        x = da.random.random(shape, chunks=chunks)\n    else:\n        x = np.random.random(shape)\n    if frac_nan is not None:\n        n_nans = int(np.prod(shape) * frac_nan)\n        inds = rng.choice(np.arange(x.size), size=n_nans, replace=False)\n        x.flat[inds] = np.nan\n    return x\n\n", "CodeBleu_score": 59.2043954117309}, {"code": "def randn(shape, frac_nan=None, chunks=None, seed=0)\n\t\n    import numpy as np\n    import dask.array as da\n\n    if frac_nan is not None:\n        assert frac_nan >= 0 and frac_nan <= 1\n    if seed is not None:\n        np.random.seed(seed)\n    if chunks is not None:\n        x = da.random.random(shape, chunks=chunks)\n    else:\n        x = np.random.random(shape)\n    if frac_nan is not None:\n        ind = np.random.choice(\n            np.arange(x.size), replace=False, size=int(x.size * frac_nan)\n        )\n        x.flat[ind] = np.nan\n    return x\n\n", "CodeBleu_score": 56.15241957995081}, {"code": "def randn(shape, frac_nan=None, chunks=None, seed=0)\n\t\n    import dask.array as da\n    import numpy as np\n    import random\n\n    if frac_nan is None:\n        frac_nan = 0\n    if chunks is None:\n        chunks = shape\n    if seed is None:\n        seed = 0\n    random.seed(seed)\n    if frac_nan > 0:\n        if chunks is not None:\n            x = da.random.random(shape, chunks=chunks)\n            x[x < frac_nan] = np.nan\n        else:\n            x = np.random.random(shape)\n            x[x < frac_nan] = np.nan\n    else:\n        if chunks is not None:\n            x = da.random.random(shape, chunks=chunks)\n        else:\n            x = np.random.random(shape)\n    return x\n\n", "CodeBleu_score": 41.14146954039358}]}
{"instance_id": "5b57abbbe38b", "generation_result": [{"code": "def analyst_price_target(self) -> pd.DataFrame\n\t\n    if self._analyst_price_target is None:\n        raise YFNotImplementedError(\"analyst_price_target\")\n    return self._analyst_price_target\n\n", "CodeBleu_score": 81.06081450739325}, {"code": "def analyst_price_target(self) -> pd.DataFrame\n\t\n    if self._analyst_price_target is None:\n        raise YFNotImplementedError(\"analyst_price_target\")\n    return self._analyst_price_target\n\n", "CodeBleu_score": 81.06081450739325}, {"code": "def analyst_price_target(self) -> pd.DataFrame\n\t\n    if self._analyst_price_target is None:\n        raise YFNotImplementedError(\"analyst_price_target\")\n    return self._analyst_price_target\n\n", "CodeBleu_score": 81.06081450739325}]}
{"instance_id": "592e414671e9", "generation_result": [{"code": "def _fetch_time_series(self, name, timescale, proxy=None)\n\t\n    try:\n        self._validate_financial_statement(name)\n        self._validate_time_scale(timescale)\n    except ValueError as e:\n        raise e\n\n    try:\n        financials_table = self._create_financials_table(name, timescale)\n        return financials_table\n    except Exception as e:\n        self._logger.error(f\"Error creating financials table: {e}\")\n        return pd.DataFrame()\n\n", "CodeBleu_score": 26.69228413385846}, {"code": "def _fetch_time_series(self, name, timescale, proxy=None)\n\t\n    if name not in const.ALLOWED_FINANCIALS:\n        raise ValueError(\n            f\"{name} is not a valid financial statement. Allowed financial statements are: {', '.join(const.ALLOWED_FINANCIALS)}\"\n        )\n    if timescale not in const.ALLOWED_PERIODS:\n        raise ValueError(\n            f\"{timescale} is not a valid timescale. Allowed timescales are: {', '.join(const.ALLOWED_PERIODS)}\"\n        )\n    try:\n        financials_table = self._create_financials_table(name, timescale)\n    except Exception as e:\n        self.logger.error(f\"Error creating financials table: {e}\")\n        financials_table = pd.DataFrame()\n    return financials_table\n\n", "CodeBleu_score": 39.32291580158464}, {"code": "def _fetch_time_series(self, name, timescale, proxy=None)\n\t\n    if name not in const.ALLOWED_FINANCIAL_STATEMENTS:\n        raise ValueError(\n            f\"Invalid financial statement name: {name}. Allowed financial statements are {const.ALLOWED_FINANCIAL_STATEMENTS}\"\n        )\n    if timescale not in const.ALLOWED_PERIODS:\n        raise ValueError(\n            f\"Invalid timescale: {timescale}. Allowed periods are {const.ALLOWED_PERIODS}\"\n        )\n    try:\n        financials = self._create_financials_table(name, timescale)\n    except Exception as e:\n        self.logger.error(f\"Error creating financials table: {e}\")\n        return pd.DataFrame()\n    return financials\n\n", "CodeBleu_score": 34.77636329146717}]}
{"instance_id": "af01d9bae091", "generation_result": [{"code": "def _parse_fund_ownership(self, data)\n\t\n    ownership_details = data[\"ownershipList\"]\n    for owner in ownership_details:\n        utils.parse_raw_values(owner)\n        owner.pop(\"maxAge\")\n\n    df = utils.pd.DataFrame(ownership_details)\n\n    if not df.empty:\n        df[\"reportDate\"] = utils.pd.to_datetime(df[\"reportDate\"], unit=\"s\")\n        df = df.rename(\n            columns={\n                \"entityProperName\": \"fund_name\",\n                \"organization\": \"fund_id\",\n                \"reportedHolding\": \"fund_holding\",\n                \"reportDate\": \"report_date\",\n            }\n        )\n\n    self._mutualfund = df\n\n", "CodeBleu_score": 46.247386386643505}, {"code": "def _parse_fund_ownership(self, data)\n\t\n    ownership_details = data['ownershipList']\n    for owner in ownership_details:\n        owner['raw_values'] = owner.pop('valueChanges')\n        owner.pop('maxAge')\n    self._mutualfund = utils.convert_to_df(ownership_details)\n    if not self._mutualfund.empty:\n        self._mutualfund['reportDate'] = self._mutualfund['reportDate'].apply(utils.convert_to_datetime)\n        self._mutualfund.rename(columns={'reportDate': 'Date Reported', 'organization': 'Organization', 'pctHeld': '% Held'}, inplace=True)\n\n", "CodeBleu_score": 29.99992972804739}, {"code": "def _parse_fund_ownership(self, data)\n\t\n    ownership_data = data[\"ownershipList\"]\n    for owner in ownership_data:\n        owner[\"raw_\" + \"value\"] = owner.pop(\"value\")\n        owner[\"raw_\" + \"pctHeld\"] = owner.pop(\"pctHeld\")\n        owner.pop(\"maxAge\")\n    df = utils.pd.DataFrame(ownership_data)\n    if not df.empty:\n        df[\"reportDate\"] = df[\"reportDate\"].apply(utils.pd.to_datetime)\n        df = df.rename(\n            columns={\n                \"value\": \"Market Value\",\n                \"pctHeld\": \"% in Fund\",\n                \"name\": \"Name\",\n                \"putCall\": \"Put/Call\",\n                \"change\": \"Change\",\n                \"raw_value\": \"Raw Value\",\n                \"raw_pctHeld\": \"Raw % in Fund\",\n            }\n        )\n    self._mutualfund = df\n\n", "CodeBleu_score": 41.70196134307365}]}
{"instance_id": "2b5c6f1253db", "generation_result": [{"code": "def currency(self)\n\t\n    if hasattr(self, \"currency\"):\n        return self.currency\n    else:\n        meta = self.history_metadata\n        self.currency = meta[\"currency\"]\n        return self.currency\n\n", "CodeBleu_score": 39.12630343291635}, {"code": "def currency(self)\n\t\n    if self.currency:\n        return self.currency\n    metadata = self.history_metadata.get('currency', None)\n    if metadata:\n        self.currency = metadata\n        return self.currency\n    else:\n        raise YFNotImplementedError(\n            'currency',\n            'The currency is not available for this ticker.')\n\n", "CodeBleu_score": 35.978255536718365}, {"code": "def currency(self)\n\t\n    if self.currency is None:\n        self.history(period='1d')\n        self.currency = self.metadata['currency']\n    return self.currency\n\n", "CodeBleu_score": 32.957234922746366}]}
{"instance_id": "16b80be60e3b", "generation_result": [{"code": "def recommendations(self) -> pd.DataFrame\n\t\n    if self._recommendations is None:\n        self._recommendations = self._fetch_recommendations()\n    if self._recommendations is None:\n        self._recommendations = pd.DataFrame()\n    else:\n        try:\n            self._recommendations = pd.DataFrame(\n                self._recommendations[\"recommendationTrend\"][\"trend\"]\n            )\n        except KeyError:\n            raise YFDataException(\n                \"Could not parse recommendations data from proxy. \"\n                \"Please try again later.\"\n            )\n    return self._recommendations\n\n", "CodeBleu_score": 45.19264042872896}, {"code": "def recommendations(self) -> pd.DataFrame\n\t\n    if self._recommendations is None:\n        self._recommendations = self.get_recommendations()\n    return self._recommendations\n\n", "CodeBleu_score": 14.98949314678481}, {"code": "def recommendations(self) -> pd.DataFrame\n\t\n    if self._recommendations is None:\n        try:\n            self._recommendations = self.get_recommendations()\n        except Exception as e:\n            raise YFDataException(\n                f\"Error parsing recommendations data for {self.symbol}: {str(e)}\"\n            )\n    return self._recommendations\n\n", "CodeBleu_score": 24.447966683987204}]}
{"instance_id": "abaf8c2a3ecc", "generation_result": [{"code": "def plot_elbow_curve(clf, X, title='Elbow Plot', cluster_ranges=None, n_jobs=1,\n                     show_cluster_time=True, ax=None, figsize=None,\n                     title_fontsize=\"large\", text_fontsize=\"medium\")\n\t\n    if not hasattr(clf, 'n_clusters'):\n        raise AttributeError(\"{} does not have attribute n_clusters\".format(clf))\n    if cluster_ranges is None:\n        cluster_ranges = range(1, 15)\n    if ax is None:\n        fig, ax = plt.subplots(figsize=figsize)\n    else:\n        fig = ax.get_figure()\n    start_time = time.time()\n    silhouette_scores = Parallel(n_jobs=n_jobs)(delayed(silhouette_score)(\n        clone(clf).set_params(n_clusters=i), X) for i in cluster_ranges)\n    end_time = time.time()\n    ax.plot(cluster_ranges, silhouette_scores, marker='o')\n    ax.set_xticks(cluster_ranges)\n    ax.set_xlabel(\"Number of clusters\", fontsize=text_fontsize)\n    ax.set_ylabel(\"Silhouette Score\", fontsize=text_fontsize)\n    ax.set_title(title, fontsize=title_fontsize)\n    if show_cluster_time:\n        # Create a second y-axis for time\n        ax2 = ax.twinx()\n        ax2.set_ylabel(\"Time (s)\", fontsize=text_fontsize)\n        ax2.plot(cluster_ranges, [end_time - start_time for _ in cluster_ranges],\n                 color='red', linestyle='--')\n    return ax\n\n", "CodeBleu_score": 51.77235399610812}, {"code": "def plot_elbow_curve(clf, X, title='Elbow Plot', cluster_ranges=None, n_jobs=1,\n                     show_cluster_time=True, ax=None, figsize=None,\n                     title_fontsize=\"large\", text_fontsize=\"medium\")\n\t\n    if not hasattr(clf, 'n_clusters'):\n        raise TypeError(\"The classifier passed should have the instance variable 'n_clusters' \"\n                        \"exactly defined.\")\n    if cluster_ranges is None:\n        cluster_ranges = range(2, 15)\n    if ax is None:\n        fig, ax = plt.subplots(1, 1, figsize=figsize)\n    clf = clone(clf)\n    clf.n_jobs = n_jobs\n    start_time = time.time()\n    sse = Parallel(n_jobs=n_jobs)(delayed(get_score)(clf, X, k) for k in cluster_ranges)\n    duration = time.time() - start_time\n    ax.plot(cluster_ranges, sse, marker='o', label=clf.n_clusters)\n    ax.set_xticks(cluster_ranges)\n    ax.set_xlabel(\"Number of clusters\", fontsize=text_fontsize)\n    ax.set_ylabel(\"Sum of Squared Errors\", fontsize=text_fontsize)\n    ax.set_title(title, fontsize=title_fontsize)\n    ax.legend(loc='best')\n    if show_cluster_time:\n        ax2 = ax.twinx()\n        ax2.set_ylabel('Time in seconds', fontsize=text_fontsize)\n        ax2.plot(cluster_ranges, [duration for _ in cluster_ranges], color='g', marker='+', label='duration')\n        ax2.legend(loc=(.75, .93), fontsize=text_fontsize)\n    return ax\n\n", "CodeBleu_score": 57.03953023396047}, {"code": "def plot_elbow_curve(clf, X, title='Elbow Plot', cluster_ranges=None, n_jobs=1,\n                     show_cluster_time=True, ax=None, figsize=None,\n                     title_fontsize=\"large\", text_fontsize=\"medium\")\n\t\n\n    if not hasattr(clf, 'n_clusters'):\n        raise TypeError(\"The classifier passed should have the n_clusters attribute\")\n\n    if cluster_ranges is None:\n        cluster_ranges = range(2, 11)\n\n    if ax is None:\n        fig, ax = plt.subplots(figsize=figsize)\n\n    clf = clone(clf)\n\n    start_time = time.time()\n\n    def _fit_score(n_cluster):\n        clf.n_clusters = n_cluster\n        clf.fit(X)\n        return clf.score(X)\n\n    scores = Parallel(n_jobs=n_jobs)(delayed(_fit_score)(n_cluster) for n_cluster in cluster_ranges)\n\n    end_time = time.time()\n    duration = end_time - start_time\n\n    ax.plot(cluster_ranges, scores, 'b*-')\n    ax.set_ylim((0, 1))\n    ax.set_xlabel('Number of clusters', fontsize=text_fontsize)\n    ax.set_ylabel('Sum of Squared Errors', fontsize=text_fontsize)\n    ax.set_xticks(cluster_ranges)\n    tick_labels = ax.get_xticklabels()\n    for label in tick_labels:\n        label.set_fontsize(text_fontsize)\n    y_labels = ax.get_yticklabels()\n    for label in y_labels:\n        label.set_fontsize(text_fontsize)\n\n    if show_cluster_time:\n        ax2 = ax.twinx()\n        ax2.plot(cluster_ranges, [duration for _ in cluster_ranges], 'r*-')\n        ax2.set_ylim(0, np.max(scores) * 10)\n        ax2.set_ylabel('Clustering duration', color='r', fontsize=text_fontsize)\n        y2_labels = ax2.get_yticklabels()\n        for label in y", "CodeBleu_score": 52.123448205122}]}
{"instance_id": "8a90815c4ee1", "generation_result": [{"code": "def _clone_and_score_clusterer(clf, X, n_clusters)\n\t\n    start_time = time.time()\n    clf = clone(clf)\n    clf.set_params(n_clusters=n_clusters)\n    clf.fit(X)\n    return clf.score(X), time.time() - start_time\n\n", "CodeBleu_score": 41.88540865518867}, {"code": "def _clone_and_score_clusterer(clf, X, n_clusters)\n\t\n    start_time = time.time()\n    clf = clone(clf)\n    clf.set_params(n_clusters=n_clusters)\n    clf.fit(X)\n    return clf.score(X), time.time() - start_time\n\n", "CodeBleu_score": 41.88540865518867}, {"code": "def _clone_and_score_clusterer(clf, X, n_clusters)\n\t\n    start_time = time.time()\n    clf = clone(clf)\n    clf.set_params(n_clusters=n_clusters)\n    clf.fit(X)\n    return clf.score(X), time.time() - start_time\n\n", "CodeBleu_score": 41.88540865518867}]}
{"instance_id": "da40bd5dd81a", "generation_result": [{"code": "def clustering_factory(clf)\n\t\n    if not hasattr(clf, 'fit') or not hasattr(clf, 'fit_predict'):\n        raise TypeError('\"clf\" is not a valid clusterer instance.')\n\n    def plot_silhouette(self, X, ax=None, title='Silhouette Plot', metric='euclidean', **kwargs):\n        return plot_silhouette(self, X, ax=ax, title=title, metric=metric, **kwargs)\n\n    def plot_elbow_curve(self, X, cluster_ranges=None, ax=None, title='Elbow Plot', metric='euclidean', **kwargs):\n        return plot_elbow_curve(self, X, cluster_ranges=cluster_ranges, ax=ax, title=title, metric=metric, **kwargs)\n\n    additional_methods = [plot_silhouette, plot_elbow_curve]\n\n    for func in additional_methods:\n        if hasattr(clf, func.__name__):\n            warnings.warn('{func} already present on {clf}'.format(func=func.__name__, clf=clf))\n        setattr(clf, func.__name__, types.MethodType(func, clf))\n\n    return clf", "CodeBleu_score": 37.02174803252846}, {"code": "def clustering_factory(clf)\n\t\n    if not hasattr(clf, \"fit\") or not hasattr(clf, \"fit_predict\"):\n        raise TypeError(\n            \"{} is not a valid clusterer instance.\".format(clf.__class__.__name__)\n        )\n\n    additional_methods = {\"plot_silhouette\": plot_silhouette, \"plot_elbow_curve\": plot_elbow_curve}\n\n    for method_name, method_func in six.iteritems(additional_methods):\n        if hasattr(clf, method_name):\n            warnings.warn(\n                \"{} already has method '{}', so it will be overridden.\".format(\n                    clf.__class__.__name__, method_name\n                )\n            )\n        setattr(clf, method_name, types.MethodType(method_func, clf))\n\n    return clf\n\n", "CodeBleu_score": 41.92709159766972}, {"code": "def clustering_factory(clf)\n\t\n    required_methods = ['fit', 'fit_predict']\n    for method in required_methods:\n        if not hasattr(clf, method) or not isinstance(getattr(clf, method), types.MethodType):\n            raise TypeError('Clusterer instance must have the methods fit and fit_predict.')\n\n    additional_methods = {'plot_silhouette': plot_silhouette, 'plot_elbow_curve': plot_elbow_curve}\n\n    for method_name, method_func in six.iteritems(additional_methods):\n        if hasattr(clf, method_name):\n            warnings.warn('{method_name} already present on clf. Overriding it.'.format(method_name=method_name))\n        setattr(clf, method_name, method_func)\n\n    return clf", "CodeBleu_score": 43.418512632410895}]}
{"instance_id": "b8d8ba3c0b1b", "generation_result": [{"code": "def plot_learning_curve(clf, X, y, title='Learning Curve', cv=None,\n                        train_sizes=None, n_jobs=1, scoring=None,\n                        ax=None, figsize=None, title_fontsize=\"large\",\n                        text_fontsize=\"medium\")\n\t\n    if ax is None:\n        fig, ax = plt.subplots(1, 1, figsize=figsize)\n\n    ax.set_title(title, fontsize=title_fontsize)\n    if train_sizes is not None:\n        train_sizes = train_sizes\n\n    train_sizes, train_scores, test_scores = learning_curve(\n        clf, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes,\n        scoring=scoring)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n\n    ax.grid()\n    ax.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                    train_scores_mean + train_scores_std, alpha=0.1,\n                    color=\"r\")\n    ax.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                    test_scores_mean + test_scores_std, alpha=0.1,\n                    color=\"g\")\n    ax.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n            label=\"Training score\")\n    ax.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n            label=\"Cross-validation score\")\n    ax.legend(loc=\"best\", fontsize=text_fontsize)\n    return ax\n\n", "CodeBleu_score": 76.91904219548542}, {"code": "def plot_learning_curve(clf, X, y, title='Learning Curve', cv=None,\n                        train_sizes=None, n_jobs=1, scoring=None,\n                        ax=None, figsize=None, title_fontsize=\"large\",\n                        text_fontsize=\"medium\")\n\t\n    if ax is None:\n        fig, ax = plt.subplots(1, 1, figsize=figsize)\n\n    ax.set_title(title, fontsize=title_fontsize)\n    if train_sizes is not None:\n        train_sizes, train_scores, test_scores = \\\n            learning_curve(clf, X, y, cv=cv, n_jobs=n_jobs,\n                           train_sizes=train_sizes, scoring=scoring)\n        train_scores_mean = np.mean(train_scores, axis=1)\n        train_scores_std = np.std(train_scores, axis=1)\n        test_scores_mean = np.mean(test_scores, axis=1)\n        test_scores_std = np.std(test_scores, axis=1)\n    else:\n        train_sizes, train_scores, test_scores = \\\n            learning_curve(clf, X, y, cv=cv, n_jobs=n_jobs,\n                           scoring=scoring)\n        train_scores_mean = np.mean(train_scores, axis=1)\n        train_scores_std = np.std(train_scores, axis=1)\n        test_scores_mean = np.mean(test_scores, axis=1)\n        test_scores_std = np.std(test_scores, axis=1)\n    ax.grid()\n\n    ax.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                    train_scores_mean + train_scores_std, alpha=0.1,\n                    color=\"r\")\n    ax.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                    test_scores_mean + test_scores_std, alpha=0.1,\n                    color=\"g\")\n    ax.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n            label=\"Training", "CodeBleu_score": 64.01241688443766}, {"code": "def plot_learning_curve(clf, X, y, title='Learning Curve', cv=None,\n                        train_sizes=None, n_jobs=1, scoring=None,\n                        ax=None, figsize=None, title_fontsize=\"large\",\n                        text_fontsize=\"medium\")\n\t\n    if ax is None:\n        fig, ax = plt.subplots(1, 1, figsize=figsize)\n\n    ax.set_title(title, fontsize=title_fontsize)\n    if train_sizes is not None:\n        ax.set_xlabel(\"Training examples\", fontsize=text_fontsize)\n        ax.set_ylabel(\"Score\", fontsize=text_fontsize)\n\n        train_sizes, train_scores, test_scores = \\\n            learning_curve(clf, X, y, cv=cv, n_jobs=n_jobs,\n                           train_sizes=train_sizes, scoring=scoring)\n        train_scores_mean = np.mean(train_scores, axis=1)\n        train_scores_std = np.std(train_scores, axis=1)\n        test_scores_mean = np.mean(test_scores, axis=1)\n        test_scores_std = np.std(test_scores, axis=1)\n\n        # Plot learning curve\n        ax.grid()\n        ax.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                        train_scores_mean + train_scores_std, alpha=0.1,\n                        color=\"r\")\n        ax.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                        test_scores_mean + test_scores_std, alpha=0.1,\n                        color=\"g\")\n        ax.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n                label=\"Training score\")\n        ax.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n                label=\"Cross-validation score\")\n        ax.legend(loc=\"best\", fontsize=text_fontsize)\n        return ax\n\n", "CodeBleu_score": 74.63905610488595}]}
{"instance_id": "e6e76eb35945", "generation_result": [{"code": "def plot_elbow_curve(clf, X, title='Elbow Plot', cluster_ranges=None,\n                     ax=None, figsize=None, title_fontsize=\"large\",\n                     text_fontsize=\"medium\")\n\t\n    if cluster_ranges is None:\n        cluster_ranges = range(1, 11, 2)\n\n    if not hasattr(clf, \"n_clusters\"):\n        raise TypeError(\"The classifier passed should have a \"\n                        \"`n_clusters` parameter\")\n\n    classifiers = [\n        clone(clf).set_params(n_clusters=n)\n        for n in cluster_ranges\n    ]\n\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        scores = [\n            clf.fit(X).score(X) * X.shape[0]\n            for clf in classifiers\n        ]\n\n    if ax is None:\n        fig, ax = plt.subplots(1, 1, figsize=figsize)\n\n    ax.plot(cluster_ranges, scores, 'b*-')\n    ax.grid(True)\n    ax.set_xlabel('k', fontsize=text_fontsize)\n    ax.set_ylabel('Sum of Squared Distances', fontsize=text_fontsize)\n    ax.set_title(title, fontsize=title_fontsize)\n\n    return ax\n\n", "CodeBleu_score": 60.57096964362796}, {"code": "def plot_elbow_curve(clf, X, title='Elbow Plot', cluster_ranges=None,\n                     ax=None, figsize=None, title_fontsize=\"large\",\n                     text_fontsize=\"medium\")\n\t\n    if not hasattr(clf, 'n_clusters'):\n        raise TypeError(\"The classifier passed should have a `n_clusters` attribute\")\n\n    if cluster_ranges is None:\n        cluster_ranges = range(1, 11, 2)\n\n    if ax is None:\n        _, ax = plt.subplots(1, 1, figsize=figsize)\n\n    models = [clone(clf).set_params(n_clusters=n) for n in cluster_ranges]\n    scores = [abs(model.fit(X).score(X)) for model in models]\n\n    ax.plot(cluster_ranges, scores, 'b*-')\n    ax.grid(True)\n    ax.set_xlabel('Number of Clusters')\n    ax.set_ylabel('Score')\n    ax.set_xticks(cluster_ranges)\n    ax.set_title(title, fontsize=title_fontsize)\n    ax.tick_params(labelsize=text_fontsize)\n\n    return ax\n\n", "CodeBleu_score": 54.3385064805628}, {"code": "def plot_elbow_curve(clf, X, title='Elbow Plot', cluster_ranges=None,\n                     ax=None, figsize=None, title_fontsize=\"large\",\n                     text_fontsize=\"medium\")\n\t\n    if not hasattr(clf, 'n_clusters'):\n        raise TypeError(\"The classifier should have the instance variable \"\n                        \"'n_clusters' for the elbow method to work. Please refer to \"\n                        \"the docs for more information.\")\n\n    if cluster_ranges is None:\n        cluster_ranges = range(1, 11, 2)\n\n    clfs = [clone(clf).set_params(n_clusters=n) for n in cluster_ranges]\n    scores = [clf.fit(X).score(X) for clf in clfs]\n\n    if ax is None:\n        fig, ax = plt.subplots(1, 1, figsize=figsize)\n\n    ax.plot(cluster_ranges, np.abs(scores), 'b*-')\n    ax.grid(True)\n    ax.set_xlabel('Number of Clusters', fontsize=text_fontsize)\n    ax.set_ylabel('Score', fontsize=text_fontsize)\n    ax.set_xticks(cluster_ranges)\n    ax.tick_params(labelsize=text_fontsize)\n    ax.set_title(\"Elbow Plot\", fontsize=title_fontsize)\n\n    return ax\n\n", "CodeBleu_score": 61.551033152809964}]}
{"instance_id": "6f4530c8a20f", "generation_result": [{"code": "def plot_pca_2d_projection(clf, X, y, title='PCA 2-D Projection', ax=None,\n                           figsize=None, cmap='Spectral',\n                           title_fontsize=\"large\", text_fontsize=\"medium\")\n\t\n    X = clf.named_steps['pca'].transform(X)\n    if ax is None:\n        fig, ax = plt.subplots(figsize=figsize)\n    ax.set_title(title, fontsize=title_fontsize)\n    targets = list(set(y))\n    colors = ['r', 'g', 'b']\n    for color, target in zip(colors, targets):\n        indices_to_keep = y == target\n        ax.scatter(X[indices_to_keep, 0], X[indices_to_keep, 1], c=color, s=50, label=target)\n    ax.set_xlabel('Principal Component 1', fontsize=text_fontsize)\n    ax.set_ylabel('Principal Component 2', fontsize=text_fontsize)\n    ax.legend()\n    ax.grid()\n    return ax\n\n", "CodeBleu_score": 41.43673984951426}, {"code": "def plot_pca_2d_projection(clf, X, y, title='PCA 2-D Projection', ax=None,\n                           figsize=None, cmap='Spectral',\n                           title_fontsize=\"large\", text_fontsize=\"medium\")\n\t\n    # PCA transformation\n    pca = PCA(n_components=2)\n    X = pca.fit_transform(X)\n\n    # Set up the plot\n    if ax is None:\n        fig, ax = plt.subplots(figsize=figsize)\n\n    # Plot the data points\n    ax.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap, edgecolor='none', alpha=0.7, s=40)\n\n    # Add a legend\n    legend_handles = []\n    for i, target_name in zip(range(len(clf.classes_)), clf.classes_):\n        legend_handles.append(mpatches.Patch(color=cmap(i), label=target_name))\n    ax.legend(handles=legend_handles, framealpha=0.3)\n\n    # Label the axes with the first and second principal components\n    ax.set_xlabel('First Principal Component', fontsize=text_fontsize)\n    ax.set_ylabel('Second Principal Component', fontsize=text_fontsize)\n\n    # Add a title\n    ax.set_title(title, fontsize=title_fontsize)\n\n    return ax\n\n", "CodeBleu_score": 44.78260718970868}, {"code": "def plot_pca_2d_projection(clf, X, y, title='PCA 2-D Projection', ax=None,\n                           figsize=None, cmap='Spectral',\n                           title_fontsize=\"large\", text_fontsize=\"medium\")\n\t\n    import matplotlib.pyplot as plt\n    from sklearn.decomposition import PCA\n    pca = PCA(n_components=2)\n    X_pca = pca.fit_transform(X)\n    if ax is None:\n        fig, ax = plt.subplots(figsize=figsize)\n    ax.set_xlabel('PC1', fontsize=text_fontsize)\n    ax.set_ylabel('PC2', fontsize=text_fontsize)\n    ax.set_title(title, fontsize=title_fontsize)\n    targets = y.unique()\n    for target in targets:\n        ax.scatter(X_pca[y == target, 0], X_pca[y == target, 1],\n                   label=target, cmap=cmap)\n    ax.legend()\n    return ax\n\n", "CodeBleu_score": 42.209099264406404}]}
{"instance_id": "e7c2775d024c", "generation_result": [{"code": "def barplot(ax)\n\t\n    clip1 = ax.bar(x=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15],\n                   height=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                   color='gray',\n                   alpha=0.5,\n                   clip_path=Rectangle((0, 0), 1, 1, transform=ax.transAxes))\n    clip2 = ax.bar(x=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15],\n                   height=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                   color='gray',\n                   alpha=0.5,\n                   clip_path=Polygon([[0, 0], [0, 1], [1, 1], [1, 0]], True))\n    Y1 = np.random.randint(1, 10, 16)\n    Y2 = np.random.randint(1, 10, 16)\n    ax.bar(x=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15],\n           height=Y1,\n           color='gray',\n           alpha=0.5,\n           clip_path=clip1[10])\n    ax.bar(x=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1", "CodeBleu_score": 28.56772011976182}, {"code": "def barplot(ax)\n\t\n    clip1 = ax.bar(x, y1, color='gray', edgecolor='black', hatch='//',\n                   align='center', alpha=0.5, label='gray bar')\n    clip2 = ax.bar(x, y2, color='gray', edgecolor='black', hatch='\\\\',\n                   align='center', alpha=0.5, label='gray bar')\n    clip3 = ax.bar(x, y1, color='red', edgecolor='black',\n                   align='center', alpha=0.5, label='red bar')\n    clip4 = ax.bar(x, y2, color='red', edgecolor='black',\n                   align='center', alpha=0.5, label='red bar')\n    ax.set_xlim(-1, 16)\n    ax.text(0.01, -0.08, 'Bar plot', transform=ax.transAxes,\n            color='black', ha='left', va='top', size=20)\n    ax.text(0.01, -0.12, 'ax.bar()', transform=ax.transAxes,\n            color='blue', ha='left', va='top',\n            family='monospace', size=20)\n\n", "CodeBleu_score": 32.91941654656342}, {"code": "def barplot(ax)\n\t\n    clip1 = ax.bar(x=[1, 2, 3, 4, 5, 6, 7, 8],\n                   height=[1, 2, 3, 4, 5, 6, 7, 8],\n                   color='0.75')\n    clip2 = ax.bar(x=[1, 2, 3, 4, 5, 6, 7, 8],\n                   height=[1, 2, 3, 4, 5, 6, 7, 8],\n                   color='0.25')\n    clip3 = ax.bar(x=[1, 2, 3, 4, 5, 6, 7, 8],\n                   height=[1, 2, 3, 4, 5, 6, 7, 8],\n                   color='r')\n    clip4 = ax.bar(x=[1, 2, 3, 4, 5, 6, 7, 8],\n                   height=[1, 2, 3, 4, 5, 6, 7, 8],\n                   color='r')\n    ax.set_xlim(-1, 16)\n    ax.text(x=0.0,\n            y=-2.5,\n            s='Bar plot',\n            fontsize=18,\n            color='k')\n    ax.text(x=0.0,\n            y=-3.5,\n            s='ax.bar()',\n            fontsize=12,\n            color='b',\n            family='monospace')\n    return\n\n", "CodeBleu_score": 21.896456435862316}]}
{"instance_id": "0ec1d146a32b", "generation_result": [{"code": "def split(n_segment)\n\t\n    width_segment = int(100/n_segment)\n    padding = int((100-width_segment*n_segment)/(n_segment-1))\n    X0 = []\n    X1 = []\n    for i in range(n_segment):\n        X0.append(i*width_segment+i*padding)\n        X1.append((i+1)*width_segment+i*padding)\n    return X0, X1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 26.617623425033198}, {"code": "def split(n_segment)\n\t\n    # Calculate the width of each segment\n    width_segment = 1.0 / n_segment\n    # Calculate the padding between segments\n    padding = width_segment / 2.0\n    # Initialize the starting and ending positions of each segment\n    X0 = np.zeros(n_segment)\n    X1 = np.zeros(n_segment)\n    # Loop over the segments\n    for i in range(n_segment):\n        # Calculate the starting and ending positions of the current segment\n        X0[i] = i * width_segment\n        X1[i] = X0[i] + width_segment\n        # Add padding to the starting and ending positions\n        X0[i] += padding\n        X1[i] -= padding\n    # Return the starting and ending positions of each segment\n    return X0, X1\n\n\n", "CodeBleu_score": 27.362754655066645}, {"code": "def split(n_segment)\n\t\n    n_segment = int(n_segment)\n    width = 1/n_segment\n    pad = width/2\n    X0 = np.arange(0, 1, width)\n    X1 = np.arange(pad, 1+pad, width)\n    return X0, X1\n\n", "CodeBleu_score": 27.272311468569377}]}
{"instance_id": "604a3e93ac0d", "generation_result": [{"code": "def pixelated_text(dpi=100)\n\t\n\n    fig = Figure(figsize=(1, 1), dpi=dpi)\n    canvas = FigureCanvasAgg(fig)\n    ax = fig.add_axes([0, 0, 1, 1])\n    ax.text(0.5, 0.5, 'a', fontsize=75, ha='center', va='center')\n    ax.axis('off')\n    canvas.draw()\n    image = canvas.buffer_rgba()\n    image = np.array(image)\n    image = image.reshape(dpi, dpi, 4)\n    image = np.roll(image, 1, axis=-1)\n    return image\n\n", "CodeBleu_score": 54.33600530833351}, {"code": "def pixelated_text(dpi=100)\n\t\n    fig = Figure(figsize=(1, 1), dpi=dpi)\n    canvas = FigureCanvasAgg(fig)\n    ax = fig.add_axes([0, 0, 1, 1])\n    ax.text(0.5, 0.5, 'a', fontsize=75, ha='center', va='center')\n    ax.axis('off')\n    canvas.draw()\n    image = canvas.buffer_rgba()\n    image = np.asarray(image)\n    image = image.reshape(dpi, dpi, 4)\n    image = np.roll(image, 1, axis=-1)\n    return image\n\n", "CodeBleu_score": 54.23183708897843}, {"code": "def pixelated_text(dpi=100)\n\t\n    fig = Figure(figsize=(1, 1), dpi=dpi)\n    canvas = FigureCanvasAgg(fig)\n    ax = fig.add_axes([0, 0, 1, 1], frameon=False)\n    ax.text(0.5, 0.5, 'a', ha='center', va='center', fontsize=75)\n    ax.axis('off')\n    canvas.draw()\n    image = canvas.buffer_rgba()\n    image = np.asarray(image)\n    image = image.reshape(dpi, dpi, 4)\n    image = np.roll(image, 1, axis=-1)\n    return image\n\n", "CodeBleu_score": 53.36530856219832}]}
{"instance_id": "2a7fc7d560ea", "generation_result": [{"code": "def curve()\n\t\n    import numpy as np\n    import random\n\n    # Initialize the curve array with zeros\n    curve = np.zeros(100)\n\n    # Randomly determine the number of components\n    num_components = random.randint(1, 5)\n\n    # Randomly determine the centers, widths, and scales of each component\n    centers = np.random.uniform(0, 100, num_components)\n    widths = np.random.uniform(5, 20, num_components)\n    scales = np.random.uniform(1, 10, num_components)\n\n    # Normalize the widths and scales to ensure they sum to specific values\n    total_width = np.sum(widths)\n    total_scale = np.sum(scales)\n    widths = widths / total_width * 50\n    scales = scales / total_scale * 10\n\n    # Iteratively add scaled Gaussian components to the curve\n    for i in range(num_components):\n        curve += scales[i] * np.exp(-(np.arange(100) - centers[i])**2 / (2 * widths[i]**2))\n\n    return curve\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 34.9960253794106}, {"code": "def curve()\n\t\n    import numpy as np\n    from scipy.stats import norm\n\n    # Define the number of components\n    num_components = np.random.randint(1, 5)\n\n    # Initialize arrays to store the parameters of each component\n    centers = np.zeros(num_components)\n    widths = np.zeros(num_components)\n    scales = np.zeros(num_components)\n\n    # Randomly determine the parameters of each component\n    for i in range(num_components):\n        centers[i] = np.random.uniform(-10, 10)\n        widths[i] = np.random.uniform(0.1, 5)\n        scales[i] = np.random.uniform(0.1, 1)\n\n    # Normalize the widths and scales to ensure they sum to specific values\n    widths = widths / np.sum(widths) * 5\n    scales = scales / np.sum(scales) * 1\n\n    # Initialize an array to store the composite curve\n    curve = np.zeros(1000)\n\n    # Add scaled Gaussian components to the composite curve\n    for i in range(num_components):\n        curve += scales[i] * norm.pdf(np.linspace(-10, 10, 1000), centers[i], widths[i])\n\n    return curve\n\n\n\n", "CodeBleu_score": 36.38292634533586}, {"code": "def curve()\n\t\n    import numpy as np\n    import random\n    import matplotlib.pyplot as plt\n\n    # Define the number of components\n    num_components = random.randint(1, 10)\n\n    # Define the number of points in the curve\n    num_points = 100\n\n    # Define the x-axis values\n    x = np.linspace(0, 10, num_points)\n\n    # Initialize the curve array\n    curve = np.zeros(num_points)\n\n    # Define the width and scale normalization factors\n    width_norm_factor = 1.0\n    scale_norm_factor = 1.0\n\n    # Generate the components\n    for i in range(num_components):\n        # Randomly determine the center, width, and scale of the component\n        center = random.uniform(0, 10)\n        width = random.uniform(0.1, 1.0)\n        scale = random.uniform(0.1, 1.0)\n\n        # Normalize the width and scale to ensure they sum to specific values\n        width_norm = width / width_norm_factor\n        scale_norm = scale / scale_norm_factor\n\n        # Add the scaled Gaussian component to the curve\n        curve += scale_norm * np.exp(-((x - center) / width_norm)**2)\n\n    # Plot the curve\n    plt.plot(x, curve)\n    plt.xlabel('x')\n    plt.ylabel('y')\n    plt.title('Composite Curve')\n    plt.show()\n\n    return curve\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 26.84485856781855}]}
{"instance_id": "be694335ebfc", "generation_result": [{"code": "def difference(derivative, accuracy=1)\n\t\n    derivative = derivative + 1\n    radius = (accuracy + 1) // 2\n    points = range(-radius, radius + 1)\n    vandermonde = np.vander(points, derivative)\n    return np.linalg.inv(vandermonde)[derivative - 1, :] / factorial(derivative - 1), points\n\n", "CodeBleu_score": 49.94848043460011}, {"code": "def difference(derivative, accuracy=1)\n\t\n    derivative = derivative + 1\n    radius = int(derivative / 2) + accuracy\n    points = list(range(-radius, radius + 1))\n    coefficients = [(-1) ** i * factorial(derivative - 1) / (factorial(i) * factorial(derivative - 1 - i)) for i in range(derivative)]\n    return coefficients, points\n\n", "CodeBleu_score": 41.17726495076207}, {"code": "def difference(derivative, accuracy=1)\n\t\n    order = derivative + 1\n    radius = order // 2 + accuracy\n    points = range(-radius, radius + 1)\n    V = np.vander(points, increasing=True)\n    return np.linalg.inv(V)[:, derivative], points\n\n", "CodeBleu_score": 46.15433903826125}]}
{"instance_id": "fa3fb47f33fa", "generation_result": [{"code": "def interpolate(X, Y, T)\n\t\n    D = scipy.spatial.distance.pdist(X)\n    d = np.cumsum(np.concatenate([[0], D]))\n    t = np.interp(T, d, np.arange(len(D) + 1))\n    return np.interp(t, np.arange(len(X)), X), np.interp(t, np.arange(len(Y)), Y), d[-1]\n\n", "CodeBleu_score": 20.28334434346741}, {"code": "def interpolate(X, Y, T)\n\t\n    D = scipy.spatial.distance.cdist(X[:, None], X[None, :], 'euclidean')\n    D = np.cumsum(D, axis=1)\n    Xp = np.interp(T, D[0, :], X[:, 0])\n    Yp = np.interp(T, D[0, :], X[:, 1])\n    return Xp, Yp, D[0, -1]\n\n", "CodeBleu_score": 27.288781989996792}, {"code": "def interpolate(X, Y, T)\n\t\n    D = scipy.spatial.distance.cdist(X[:, None], X[None, :], \"euclidean\")\n    D = np.cumsum(D, axis=1)\n    X = np.interp(T, D[0, :], X[:, 0])\n    Y = np.interp(T, D[0, :], Y[:, 0])\n    T = T[-1]\n    return X, Y, T\n\n", "CodeBleu_score": 27.89026764552618}]}
{"instance_id": "870a2dad3c66", "generation_result": [{"code": "def mandelbrot_set(xmin, xmax, ymin, ymax, xn, yn, maxiter, horizon=2.0)\n\t\n    X = np.linspace(xmin, xmax, xn, dtype=np.float32)\n    Y = np.linspace(ymin, ymax, yn, dtype=np.float32)\n    C = X + Y[:, None] * 1j\n    N = np.zeros(C.shape, dtype=int)\n    Z = np.zeros(C.shape, np.complex64)\n    for n in range(maxiter):\n        I = np.less(abs(Z), horizon)\n        N[I] = n\n        Z[I] = Z[I]**2 + C[I]\n    N[N == maxiter-1] = 0\n    return Z, N\n\n", "CodeBleu_score": 72.53477004545985}, {"code": "def mandelbrot_set(xmin, xmax, ymin, ymax, xn, yn, maxiter, horizon=2.0)\n\t\n    X = np.linspace(xmin, xmax, xn, dtype=np.float32)\n    Y = np.linspace(ymin, ymax, yn, dtype=np.float32)\n    C = X + Y[:, None] * 1j\n    N = np.zeros(C.shape, dtype=int)\n    Z = np.zeros(C.shape, np.complex64)\n    for n in range(maxiter):\n        I = np.less(abs(Z), horizon)\n        N[I] = n\n        Z[I] = Z[I]**2 + C[I]\n    N[N == maxiter-1] = 0\n    return Z, N\n\n", "CodeBleu_score": 72.53477004545985}, {"code": "def mandelbrot_set(xmin, xmax, ymin, ymax, xn, yn, maxiter, horizon=2.0)\n\t\n    X = np.linspace(xmin, xmax, xn, dtype=np.float32)\n    Y = np.linspace(ymin, ymax, yn, dtype=np.float32)\n    C = X + Y[:, None] * 1j\n    N = np.zeros(C.shape, dtype=int)\n    Z = np.zeros(C.shape, np.complex64)\n    for n in range(maxiter):\n        I = np.less(abs(Z), horizon)\n        N[I] = n\n        Z[I] = Z[I]**2 + C[I]\n    N[N == maxiter-1] = 0\n    return Z, N\n\n", "CodeBleu_score": 72.53477004545985}]}
{"instance_id": "e1a908945acf", "generation_result": [{"code": "def smooth2d(A, sigma=3)\n\t\n\n    m, n = A.shape\n    wlen = int(4 * sigma + 1)  # window length\n    w = np.ones(wlen, \"d\")\n    A = np.apply_along_axis(lambda mV: np.convolve(w / w.sum(), mV, mode=\"same\"), 1, A)\n    A = np.apply_along_axis(lambda mV: np.convolve(w / w.sum(), mV, mode=\"same\"), 0, A)\n    return A\n\n", "CodeBleu_score": 22.7387733674965}, {"code": "def smooth2d(A, sigma=3)\n\t\n    window_len = sigma*4+1\n    A = np.array(A)\n    x = np.zeros(A.shape)\n    for c in range(A.shape[1]):\n        x[:,c] = smooth(A[:,c], sigma=sigma)\n    x = x.T\n    y = np.zeros(x.shape)\n    for c in range(x.shape[1]):\n        y[:,c] = smooth(x[:,c], sigma=sigma)\n    return y.T\n\n", "CodeBleu_score": 32.38208420495193}, {"code": "def smooth2d(A, sigma=3)\n\t\n    window_len = int(sigma) * 4 + 1\n    A_smooth = np.zeros(A.shape)\n    for i in range(A.shape[0]):\n        A_smooth[i, :] = smooth(A[i, :], window_len)\n    A_smooth = A_smooth.T\n    for i in range(A_smooth.shape[0]):\n        A_smooth[i, :] = smooth(A_smooth[i, :], window_len)\n    A_smooth = A_smooth.T\n    return A_smooth\n\n", "CodeBleu_score": 25.2333477804909}]}
{"instance_id": "94563c54b483", "generation_result": [{"code": "def process_image(self, padded_src, dpi)\n\t\n    ox, oy = self.ox, self.oy\n    px = 72. / dpi\n    xmin, ymin = self.axes.viewLim.intervalx.min(), self.axes.viewLim.intervaly.min()\n    xmax, ymax = self.axes.viewLim.intervalx.max(), self.axes.viewLim.intervaly.max()\n    dx = (xmax - xmin) / (px * (padded_src.shape[1] - 1))\n    dy = (ymax - ymin) / (px * (padded_src.shape[0] - 1))\n    ox, oy = int(round(ox / dx)), int(round(oy / dy))\n    return np.roll(np.roll(padded_src, -ox, axis=1), -oy, axis=0)\n\n", "CodeBleu_score": 29.626407116548855}, {"code": "def process_image(self, padded_src, dpi)\n\t\n    ox, oy = self.ox, self.oy\n    px = 1 / dpi\n    px = int(px * 72)\n    px = px / 72 * dpi\n    px = int(px)\n    if ox != 0:\n        padded_src = np.roll(padded_src, px * ox, axis=1)\n    if oy != 0:\n        padded_src = np.roll(padded_src, px * oy, axis=0)\n    return padded_src\n\n", "CodeBleu_score": 31.44656077973632}, {"code": "def process_image(self, padded_src, dpi)\n\t\n    ox, oy = self.ox, self.oy\n    ox, oy = int(ox * dpi), int(oy * dpi)\n    return np.roll(np.roll(padded_src, ox, axis=1), oy, axis=0)\n\n", "CodeBleu_score": 25.510445909614717}]}
{"instance_id": "d71d232872fe", "generation_result": [{"code": "def gaussian(shape=(25, 25), width=0.5, center=0.0)\n\t\n    shape = (shape[0], shape[1])\n    width = width / 2.0\n    center = center - 0.5\n    x, y = np.meshgrid(np.linspace(-1, 1, shape[0]) * width, np.linspace(-1, 1, shape[1]) * width)\n    d = np.sqrt(x * x + y * y)\n    g = np.exp(-(d - center) ** 2 / width)\n    return g\n\n", "CodeBleu_score": 19.44680683630528}, {"code": "def gaussian(shape=(25, 25), width=0.5, center=0.0)\n\t\n    shape = shape if isinstance(shape, tuple) else (shape,)\n    width = width if isinstance(width, tuple) else (width,)\n    center = center if isinstance(center, tuple) else (center,)\n    assert len(shape) == len(width) == len(center)\n    grid = np.mgrid[tuple(slice(-w / 2.0, w / 2.0, 1j * w) for w in width)]\n    grid = np.moveaxis(grid, 0, -1)\n    grid -= np.array(center)\n    grid = grid**2\n    grid = np.sum(grid, axis=-1)\n    return np.exp(-grid / 2.0)\n\n", "CodeBleu_score": 21.920293983763948}, {"code": "def gaussian(shape=(25, 25), width=0.5, center=0.0)\n\t\n    shape = (shape[0] // 2, shape[1] // 2) if isinstance(shape, tuple) else (shape // 2, shape // 2)\n    width = (width[0], width[1]) if isinstance(width, tuple) else (width, width)\n    center = (center[0], center[1]) if isinstance(center, tuple) else (center, center)\n    x, y = np.meshgrid(np.linspace(-shape[0], shape[0], shape[0] * 2 + 1), np.linspace(-shape[1], shape[1], shape[1] * 2 + 1))\n    x, y = (x - center[0]) / width[0], (y - center[1]) / width[1]\n    return np.exp(-0.5 * (x ** 2 + y ** 2))\n\n", "CodeBleu_score": 21.462343169897736}]}
{"instance_id": "7bbd08c987bc", "generation_result": [{"code": "def stimulus(position, size, intensity)\n\t\n    # Convert polar coordinates to Cartesian coordinates\n    x = position[0] * np.cos(position[1])\n    y = position[0] * np.sin(position[1])\n\n    # Normalize the coordinates\n    x = (x - np.min(x)) / (np.max(x) - np.min(x)) - 0.5\n    y = (y - np.min(y)) / (np.max(y) - np.min(y)) - 0.5\n\n    # Create a mesh grid representing the spatial domain\n    x_grid, y_grid = np.meshgrid(x, y)\n\n    # Compute the squared Euclidean distance from each point in the grid to the stimulus center\n    R = np.sqrt(x_grid**2 + y_grid**2)\n\n    # Compute the stimulus\n    stimulus = intensity * np.exp(-R**2 / (2 * size**2))\n\n    return stimulus\n\n", "CodeBleu_score": 15.849685977855854}, {"code": "def stimulus(position, size, intensity)\n\t\n    # Convert polar coordinates to Cartesian coordinates\n    x = position[0] * np.cos(position[1])\n    y = position[0] * np.sin(position[1])\n\n    # Normalize the coordinates\n    x_norm = (x - np.min(x)) / (np.max(x) - np.min(x)) - 0.5\n    y_norm = (y - np.min(y)) / (np.max(y) - np.min(y)) - 0.5\n\n    # Create a mesh grid representing the spatial domain\n    X, Y = np.meshgrid(x_norm, y_norm)\n\n    # Compute the squared Euclidean distance from each point in the grid to the stimulus center\n    R = X**2 + Y**2\n\n    # Compute the stimulus\n    return intensity * np.exp(-R / (2 * size**2))\n\n", "CodeBleu_score": 15.191261578043964}, {"code": "def stimulus(position, size, intensity)\n\t\n    # Convert polar coordinates to Cartesian coordinates\n    x = position[0] * np.cos(position[1])\n    y = position[0] * np.sin(position[1])\n\n    # Normalize the coordinates\n    x = (x / np.max(np.abs(x))) / 2\n    y = (y / np.max(np.abs(y))) / 2\n\n    # Create a mesh grid representing the spatial domain\n    X, Y = np.meshgrid(x, y)\n\n    # Compute the squared Euclidean distance from each point in the grid to the stimulus center\n    R = np.square(X - x) + np.square(Y - y)\n\n    # Compute the stimulus\n    return intensity * np.exp(-R / (2 * size**2))\n\n", "CodeBleu_score": 18.75838268982478}]}
{"instance_id": "2b5806238d5d", "generation_result": [{"code": "def plot(ax, text)\n\t\n    ax.set_xlim(0, 1)\n    ax.set_ylim(0, 1)\n    ax.set_xticks([0, 0.5, 1])\n    ax.set_yticks([0, 0.5, 1])\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    ax.text(0.5, 0.5, text, horizontalalignment='center', verticalalignment='center', fontdict={'family': 'serif', 'color': 'darkred', 'weight': 'bold', 'size': 12})\n    ax.set_title('Title', fontdict={'family': 'serif', 'weight': 'bold'})\n\n\n\n\n", "CodeBleu_score": 39.51792600344455}, {"code": "def plot(ax, text)\n\t\n    ax.set_xlim(0, 1)\n    ax.set_ylim(0, 1)\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    ax.text(0.5, 0.5, text, horizontalalignment='center', verticalalignment='center', transform=ax.transAxes, fontproperties=FontProperties(family='monospace', weight='bold'))\n    ax.set_title('Title', fontproperties=FontProperties(family='monospace', weight='bold'))\n\n\n\n", "CodeBleu_score": 42.42917681863141}, {"code": "def plot(ax, text)\n\t\n    ax.set_xlim(0, 1)\n    ax.set_ylim(0, 1)\n    ax.set_xticks([0, 0.5, 1])\n    ax.set_yticks([0, 0.5, 1])\n    ax.set_xlabel(\"x\")\n    ax.set_ylabel(\"y\")\n    ax.text(0.5, 0.5, text, ha=\"center\", va=\"center\", fontsize=20, color=\"red\")\n    ax.set_title(\"Title\", fontfamily=\"serif\", fontweight=\"bold\")", "CodeBleu_score": 40.759591650125174}]}
{"instance_id": "0b42aac61e21", "generation_result": [{"code": "def predict_cumulative_hazard_function(self, X, **kwargs)\n\t\n    Xt = X\n    for _, name, transform in self._iter(with_final=False):\n        Xt = transform.transform(Xt)\n    return self.steps[-1][-1].predict_cumulative_hazard_function(Xt, **kwargs)\n\n", "CodeBleu_score": 72.17854088109615}, {"code": "def predict_cumulative_hazard_function(self, X, **kwargs)\n\t\n    Xt = X\n    for _, name, transform in self._iter(with_final=False):\n        Xt = transform.transform(Xt)\n    return self.steps[-1][-1].predict_cumulative_hazard_function(Xt, **kwargs)\n\n", "CodeBleu_score": 72.17854088109615}, {"code": "def predict_cumulative_hazard_function(self, X, **kwargs)\n\t\n    Xt = X\n    for _, name, transform in self._iter(with_final=False):\n        Xt = transform.transform(Xt)\n    return self.steps[-1][-1].predict_cumulative_hazard_function(Xt, **kwargs)\n\n", "CodeBleu_score": 72.17854088109615}]}
{"instance_id": "c8439e4918ad", "generation_result": [{"code": "def predict_survival_function(self, X, **kwargs)\n\t\n    Xt = X\n    for name, transform in self.steps[:-1]:\n        Xt = transform.transform(Xt)\n    return self.steps[-1][-1].predict_survival_function(Xt, **kwargs)\n\n", "CodeBleu_score": 60.12798510017805}, {"code": "def predict_survival_function(self, X, **kwargs)\n\t\n    Xt = X\n    for _, name, transform in self._iter(with_final=False):\n        Xt = transform.transform(Xt)\n    return self.steps[-1][-1].predict_survival_function(Xt, **kwargs)\n\n", "CodeBleu_score": 72.14059024077444}, {"code": "def predict_survival_function(self, X, **kwargs)\n\t\n    Xt = X\n    for _, name, transform in self._iter(with_final=False):\n        Xt = transform.transform(Xt)\n    return self.steps[-1][-1].predict_survival_function(Xt, **kwargs)\n\n", "CodeBleu_score": 72.14059024077444}]}
{"instance_id": "216662ca7db4", "generation_result": [{"code": "def _check_success(self, results)\n\t\n    status = results.get(\"status\", 0)\n    if status == 0:\n        warnings.warn(\n            \"Solver terminated early.\", ConvergenceWarning, stacklevel=2\n        )\n    elif status != 1:\n        raise RuntimeError(\n            \"Solver terminated with exit status {0}\".format(status)\n        )\n    return results\n\n", "CodeBleu_score": 16.742300880500512}, {"code": "def _check_success(self, results)\n\t\n    if results.success:\n        return\n    if results.status == 0:\n        warnings.warn(\n            \"Optimization terminated successfully.\", ConvergenceWarning, stacklevel=2\n        )\n    elif results.status == -1:\n        raise RuntimeError(\n            \"Maximum number of iterations has been exceeded.\"\n            \" The problem may be unfeasible or \"\n            \"may be too hard. It is sometimes helpful to \"\n            \"scale the data beforehand.\"\n        )\n    elif results.status in (1, 2):\n        raise RuntimeError(\n            \"Iteration limit reached. \"\n            \"The problem may be unfeasible or \"\n            \"may be too hard. It is sometimes helpful to \"\n            \"scale the data beforehand.\"\n        )\n    elif results.status == 3:\n        raise RuntimeError(\n            \"Optimization process was interrupted by the user.\"\n        )\n    elif results.status in (4, 5, 6):\n        raise RuntimeError(\n            \"Numerical difficulties were encountered. \"\n            \"The problem may be unfeasible or \"\n            \"may be too hard. It is sometimes helpful to \"\n            \"scale the data beforehand.\"\n        )\n    elif results.status == 7:\n        raise RuntimeError(\n            \"User specified an invalid algorithm to use.\"\n        )\n    elif results.status == 8:\n        raise RuntimeError(\n            \"Convergence tolerance was reached.\"\n        )\n    elif results.status == 9:\n        raise RuntimeError(\n            \"The problem appears to be infeasible.\"\n        )\n    elif results.status == 10:\n        raise RuntimeError(\n            \"The problem appears to be unbounded.\"\n        )\n    elif results.status == 11:\n        raise RuntimeError(\n            \"A potentially singular or nearly singular matrix was encountered.\"\n        )\n    elif results.status == 12:\n        raise RuntimeError(\n            \"The solution was found to be numerically unstable.\"\n        )\n    elif results.status == 13:\n        raise RuntimeError(\n            \"The solution and its derivatives may contain NaN values.\"\n        )\n    elif results.status == 14:\n        raise Runtime", "CodeBleu_score": 23.09366250433684}, {"code": "def _check_success(self, results)\n\t\n    if results.success:\n        return\n    if results.status == 0:\n        warnings.warn(\"Optimization terminated successfully.\", ConvergenceWarning)\n    elif results.status == -1:\n        raise RuntimeError(\"Optimization failed to converge. Maximum number of iterations reached.\")\n    elif results.status == 1:\n        raise RuntimeError(\"Optimization failed to converge. Maximum number of iterations reached.\")\n    elif results.status == 2:\n        raise RuntimeError(\"Primal or dual infeasibility problem.\")\n    else:\n        raise RuntimeError(\"Optimization failed with status: %d\" % results.status)\n\n", "CodeBleu_score": 25.93483038378149}]}
{"instance_id": "1fe2e379fca0", "generation_result": [{"code": "def _decompose(self, P)\n\t\n    # Compute eigenvalues and eigenvectors\n    eig_val, eig_vec = linalg.eigh(P)\n\n    # Identify largest eigenvalue\n    max_eig_val = eig_val.max()\n\n    # Calculate condition number\n    cond_num = max_eig_val / (np.linalg.norm(P) * np.finfo(float).eps)\n\n    # Filter out eigenvalues below positive cutoff\n    eig_val_filtered = eig_val[eig_val > self.positive_cutoff]\n\n    # Normalize eigenvalues\n    eig_val_filtered /= max_eig_val\n\n    # Return decomposed matrix and largest eigenvalue\n    return eig_vec.T @ np.diag(eig_val_filtered), max_eig_val\n\n", "CodeBleu_score": 15.16765142058413}, {"code": "def _decompose(self, P)\n\t\n    eigvals, eigvecs = linalg.eigh(P)\n    eigvals = eigvals[::-1]\n    eigvecs = eigvecs[:, ::-1]\n    largest_eigval = eigvals[0]\n    cutoff = self.cutoff * largest_eigval\n    neg_cutoff = -cutoff\n    pos_cutoff = cutoff * self.shape_[0] / self.shape_[1]\n    mask = (eigvals > neg_cutoff) & (eigvals < pos_cutoff)\n    eigvals = eigvals[mask]\n    eigvecs = eigvecs[:, mask]\n    eigvals = eigvals / largest_eigval\n    return eigvecs.T, largest_eigval\n\n", "CodeBleu_score": 26.259051876575747}, {"code": "def _decompose(self, P)\n\t\n    # compute eigenvalues and eigenvectors\n    evals, evecs = linalg.eigh(P)\n\n    # identify the largest eigenvalue\n    largest_eigval = evals.max()\n\n    # calculate a condition number based on the largest eigenvalue\n    condition_number = largest_eigval / (\n        P.shape[0] * np.finfo(P.dtype).eps\n    )\n\n    # check if all eigenvalues are above the negative cutoff\n    if np.all(evals > -self.cutoff * largest_eigval):\n        warnings.warn(\n            \"All eigenvalues are above the negative cutoff. \"\n            \"This may indicate that the matrix is singular or \"\n            \"that there are more samples than features. \"\n            \"In this case, the decomposition may be unstable. \"\n            \"Consider reducing the dimensionality of the data, \"\n            \"or using a different decomposition method.\"\n        )\n\n    # filter out eigenvalues below the positive cutoff\n    evals = evals[evals > self.cutoff * largest_eigval]\n\n    # normalize the eigenvalues so that the maximum eigenvalue is set to 1\n    evals /= largest_eigval\n\n    # return the transposed decomposed matrix and the largest eigenvalue\n    return evecs.T @ np.diag(evals), largest_eigval\n\n", "CodeBleu_score": 20.190044487026196}]}
{"instance_id": "20c3f5d05d79", "generation_result": [{"code": "def _update_coef(self, coef, D)\n\t\n    self.coef_ = coef\n    self.coef_ *= D\n\n", "CodeBleu_score": 15.009637534426501}, {"code": "def _update_coef(self, coef, D)\n\t\n    # The following is equivalent to:\n    # coef = np.where(np.abs(coef) > self.alpha, coef, 0)\n    # coef *= D\n    # self.coef_ = coef\n    coef *= D\n    coef[coef < self.alpha] = 0\n    self.coef_ = coef\n\n", "CodeBleu_score": 21.048822609228875}, {"code": "def _update_coef(self, coef, D)\n\t\n    self.coef_ = coef\n    self.coef_ *= D\n\n", "CodeBleu_score": 15.009637534426501}]}
{"instance_id": "37c347d00d35", "generation_result": [{"code": "def gradient(self, y_true, raw_prediction, sample_weight=None, **kwargs)\n\t\n    event_times, event_types = y_true\n    grad = -event_types * (raw_prediction - event_times)\n    if sample_weight is not None:\n        grad *= sample_weight\n    return grad\n\n", "CodeBleu_score": 36.57513083176024}, {"code": "def gradient(self, y_true, raw_prediction, sample_weight=None, **kwargs)\n\t\n    y_true = self.label_binarizer_.transform(y_true)\n    n_samples = y_true.shape[0]\n    y_true = y_true.astype(raw_prediction.dtype, copy=False)\n    grad = self.link.gradient(raw_prediction, y_true)\n    if sample_weight is not None:\n        grad *= sample_weight[:, np.newaxis]\n    return grad\n\n", "CodeBleu_score": 31.50117338705791}, {"code": "def gradient(self, y_true, raw_prediction, sample_weight=None, **kwargs)\n\t\n    event_type = y_true[:, 0]\n    event_time = y_true[:, 1]\n    gradient = self.link.gradient(event_time, raw_prediction)\n    gradient *= event_type\n    if sample_weight is not None:\n        gradient *= sample_weight\n    return gradient\n\n", "CodeBleu_score": 48.47059926980405}]}
{"instance_id": "6be432d27e52", "generation_result": [{"code": "def _compute_missing_values_in_feature_mask(self, X, estimator_name=None)\n\t\n    if self.missing_values == \"nan\":\n        if not np.isnan(X).any():\n            return None\n        if np.isinf(X).any():\n            raise ValueError(\n                \"Input contains invalid values (infinities or a value too large for \"\n                \"dtype('float64'))\"\n            )\n        if np.isnan(X).any():\n            return np.isnan(X).any(axis=0)\n\n    if self.missing_values == \"none\":\n        if not _any_isnan_axis0(X):\n            return None\n        raise ValueError(\n            \"Input contains NaN, infinity or a value too large for \"\n            f\"{X.dtype}.", "CodeBleu_score": 26.317576419969452}, {"code": "def _compute_missing_values_in_feature_mask(self, X, estimator_name=None)\n\t\n    if not self.missing_values_in_features:\n        if not _any_isnan_axis0(X):\n            _assert_all_finite_element_wise(X)\n            return None\n\n    sum_x = X.sum(axis=0)\n    if np.isinf(sum_x).any():\n        raise ValueError(\n            \"Input contains invalid values (infinities or a value too large for \"\n            \"dtype('float64'))\"\n        )\n    if np.isnan(sum_x).any():\n        if estimator_name is None:\n            estimator_name = \"estimator\"\n        raise ValueError(\n            f\"Input contains NaN, {estimator_name} does not \"\n            f\"support missing values.\"\n        )\n    return np.isnan(sum_x)\n\n", "CodeBleu_score": 32.06603824256038}, {"code": "def _compute_missing_values_in_feature_mask(self, X, estimator_name=None)\n\t\n    if self.missing_values_in_features is None:\n        if not self.support_missing_values:\n            assert_all_finite(X)\n            return None\n        else:\n            if self.missing_values_in_features == \"nan\":\n                if np.isnan(X).any():\n                    return np.isnan(X).any(axis=0)\n                else:\n                    return None\n            elif self.missing_values_in_features == \"zero\":\n                if _any_isnan_axis0(X):\n                    return _any_isnan_axis0(X)\n                else:\n                    return None\n            else:\n                raise ValueError(\n                    \"Unknown missing_values_in_features: {}\".format(\n                        self.missing_values_in_features\n                    )\n                )\n    else:\n        if self.missing_values_in_features == \"nan\":\n            if np.isnan(X).any():\n                return np.isnan(X).any(axis=0)\n            else:\n                return None\n        elif self.missing_values_in_features == \"zero\":\n            if _any_isnan_axis0(X):\n                return _any_isnan_axis0(X)\n            else:\n                return None\n        else:\n            raise ValueError(\n                \"Unknown missing_values_in_features: {}\".format(\n                    self.missing_values_in_features\n                )\n            )\n\n", "CodeBleu_score": 22.20263528293056}]}
{"instance_id": "dc5bda895a07", "generation_result": [{"code": "def predict_survival_function(self, X, check_input=True, return_array=False)\n\t\n\n    check_is_fitted(self)\n    if check_input:\n        X = self._validate_data(X, reset=False)\n    X = self._ensure_sparse_format(X)\n\n    if _any_isnan_axis0(X):\n        raise ValueError(\"Input contains NaN.\")\n\n    # TODO: this is a hack, should be fixed\n    # https://github.com/scikit-learn/scikit-learn/issues/2693\n    if self.n_features_in_ != X.shape[1]:\n        raise ValueError(\"Number of features of the model must match the input.\")\n\n    if issparse(X):\n        # sparsity may be handled by the tree\n        X.sort_indices()\n\n    # TODO: this is a hack, should be fixed\n    # https://github.com/scikit-learn/scikit-learn/issues/2693\n    if self.n_features_in_ != X.shape[1]:\n        raise ValueError(\"Number of features of the model must match the input.\")\n\n    surv_func = self.tree_.predict(X)\n    surv_func = surv_func[:, 1]\n\n    if return_array:\n        return surv_func\n\n    times = self.unique_times_\n    step_function = StepFunction(times, surv_func, sorted=True)\n    return step_function\n\n", "CodeBleu_score": 39.97841355986795}, {"code": "def predict_survival_function(self, X, check_input=True, return_array=False)\n\t\n    check_is_fitted(self, \"tree_\")\n    if check_input:\n        X = self._validate_data(X, dtype=None, accept_sparse=\"csr\", reset=False)\n    if issparse(X):\n        X = X.tocsc()\n    else:\n        X = np.ascontiguousarray(X)\n    X = self._check_test_data(X)\n    if self.tree_ is None:\n        raise ValueError(\"The model has not been fit yet.\")\n    tree = self.tree_\n    n_samples = X.shape[0]\n    proba = np.zeros((n_samples, self.n_censored_samples_))\n    if tree.tree_.value.shape[1] == 1:\n        pred_DTYPE = np.float32\n    else:\n        pred_DTYPE = np.float64\n    pred_chunks = []\n    for i in range(tree.tree_.node_count):\n        node = tree.tree_.__getstate__()[\"nodes\"][i]\n        if node.left_child == _tree.TREE_LEAF:\n            value = tree.tree_.value[node.id].astype(pred_DTYPE, copy=False)\n        else:\n            value = None\n        pred_chunks.append((node.id, node.left_child, node.right_child, node.feature, node.threshold, value))\n    pred_chunks = np.array(pred_chunks, dtype=pred_DTYPE)\n    for i in range(n_samples):\n        ind = 0\n        while ind < tree.tree_.node_count:\n            node_id, left_child, right_child, feature, threshold, value = pred_chunks[ind]\n            if (X[i, feature] <= threshold) and left_child != _tree.TREE_LEAF:\n                ind = left_child\n            elif right_child != _tree.TREE_LEAF:\n                ind = right_child\n            else:\n                if value is None:\n                    raise ValueError(\"Unknown leaf detected. This can", "CodeBleu_score": 28.8914943973282}, {"code": "def predict_survival_function(self, X, check_input=True, return_array=False)\n\t\n    if check_input:\n        X = self._validate_data(X, reset=False)\n    check_is_fitted(self, \"tree_\")\n    X = self._get_baseline_prediction(X)\n    if issparse(X):\n        X = X.tocsc()\n    else:\n        X = np.ascontiguousarray(X)\n    proba = self.tree_.predict(X)\n    if return_array:\n        return proba[:, 1]\n    times = np.unique(proba[:, 0])\n    y_pred = np.vstack((times, proba[proba[:, 0].searchsorted(times), 1]))\n    return StepFunction(y_pred[0], y_pred[1])\n\n", "CodeBleu_score": 47.93862285905121}]}
{"instance_id": "84ae915b5933", "generation_result": [{"code": "def _pre_fit(self, X, y)\n\t\n    if not isinstance(X, (list, tuple, np.ndarray)) or len(X) == 0:\n        raise ValueError(\"X must be a list or NumPy array\")\n    if not isinstance(y, (list, tuple, np.ndarray)) or len(y) == 0:\n        raise ValueError(\"y must be a list or NumPy array\")\n    X = np.array(X, dtype=np.float64)\n    y = np.array(y, dtype=np.float64)\n    if X.ndim == 1:\n        X = np.reshape(X, (-1, 1))\n    if y.ndim == 1:\n        y = np.reshape(y, (-1, 1))\n    if X.shape[0] != y.shape[0]:\n        raise ValueError(\"X and y must have the same number of samples\")\n    if X.shape[0] < 10:\n        raise ValueError(\"X must have at least 10 samples\")\n    assert_all_finite(X)\n    assert_all_finite(y)\n    event = column_or_1d(y[:, 0], warn_on_dtype=True)\n    time = column_or_1d(y[:, 1], warn_on_dtype=True)\n    check_non_negative(time, \"Survival times\")\n    if np.any(event < 0):\n        raise ValueError(\"Event indicators must be non-negative\")\n    if np.any(event > 1):\n        raise ValueError(\"Event indicators must be binary\")\n    if np.all(event == 0):\n        raise ValueError(\"All samples are censored\")\n    X_mean = X.mean(axis=0)\n    X -= X_mean\n    if self.normalize:\n        X = f_normalize(X, axis=0, norm=self.norm)\n    X = np.asfortranarray(X)\n    y = np.asfortranarray(y)\n    order = np.argsort(-time, kind=\"mergesort\")\n    X = X[order]\n    y = y[order", "CodeBleu_score": 38.251732538428065}, {"code": "def _pre_fit(self, X, y)\n\t\n    if X.shape[0] < self.min_samples:\n        raise ValueError(\"X must have at least %d samples.\" % self.min_samples)\n    X = X.astype(np.float64)\n    y = column_or_1d(y, warn=True)\n    event, time = check_y_survival(y)\n    X, event, time = self._pre_fit_process(X, event, time)\n    X = np.asfortranarray(X)\n    if self.normalize:\n        X = f_normalize(X, axis=0)\n    if self.verbose:\n        print(\"Centering and scaling data...\")\n    X_mean = np.mean(X, axis=0)\n    X_std = np.std(X, axis=0)\n    X_std[X_std == 0.0] = 1.0\n    X -= X_mean\n    X /= X_std\n    if self.verbose:\n        print(\"Done.\")\n    if self.verbose:\n        print(\"Sorting data...\")\n    order = np.argsort(time, kind=\"mergesort\")[::-1]\n    X = X[order]\n    event = event[order]\n    time = time[order]\n    if self.verbose:\n        print(\"Done.\")\n    return X, event, time, X_mean, X_std\n\n", "CodeBleu_score": 37.19964339706648}, {"code": "def _pre_fit(self, X, y)\n\t\n    X = self._validate_data(X, accept_sparse=False)\n    X = check_array(X, ensure_min_samples=2)\n    y = check_array(y, ensure_2d=False)\n    if y.ndim == 1:\n        y = np.c_[y, np.ones_like(y)]\n    y = np.asarray(y, dtype=np.float64)\n    if y.shape[1] != 2:\n        raise ValueError(\n            \"y should have exactly 2 columns: an event indicator and a survival time.\"\n        )\n    event, time = y[:, 0], y[:, 1]\n    if not np.all(np.isfinite(time)):\n        raise ValueError(\"y must not contain non-finite values.\")\n    if not np.all(np.logical_or(event == 0, event == 1)):\n        raise ValueError(\"y must contain binary class labels.\")\n    if not np.all(event[time != 0] == 1):\n        raise ValueError(\"Non-censored observations must have event=1.\")\n    if not np.all(time[event == 0] == 0):\n        raise ValueError(\"Censored observations must have time[event == 0] = 0.\")\n    if not np.all(time >= 0):\n        raise ValueError(\"y must not contain negative values.\")\n    X = np.asarray(X, dtype=np.float64)\n    if self.fit_baseline_model:\n        self.baseline_model_ = self._fit_baseline_model(X, event, time)\n    else:\n        self.baseline_model_ = None\n    X_mean = X.mean(0)\n    X_std = X.std(0)\n    X_std[X_std == 0] = 1\n    X = (X - X_mean) / X_std\n    if self.normalize:\n        X = f_normalize(X, axis=0)\n    return X, event, time, X_mean, X_std\n\n", "CodeBleu_score": 26.1373107510402}]}
{"instance_id": "c70bb013f85f", "generation_result": [{"code": "def _check_alphas(self)\n\t\n    if self.alphas is None:\n        if self.n_alphas <= 0:\n            raise ValueError(\n                \"Invalid value for 'n_alphas': %d. The number of alphas must be \"\n                \"strictly positive.\" % self.n_alphas)\n        alphas = np.logspace(-10, -2, self.n_alphas)\n        self.alphas_ = alphas\n        return True\n    else:\n        alphas = column_or_1d(self.alphas).ravel()\n        self.alphas_ = alphas\n        return False\n\n", "CodeBleu_score": 40.70386440025344}, {"code": "def _check_alphas(self)\n\t\n    if self.alphas is None:\n        n_alphas = self.n_alphas\n        if n_alphas < 1:\n            raise ValueError(\"Invalid value for 'n_alphas': %d. The number of alphas must be a positive integer.\" % n_alphas)\n        alphas = np.linspace(self.alphas_[0], self.alphas_[-1], n_alphas)\n        return alphas, True\n    else:\n        alphas = column_or_1d(self.alphas).ravel()\n        if alphas.shape[0] < 1:\n            raise ValueError(\"Invalid value for 'alphas': %s. The number of alphas must be a positive integer.\" % alphas.shape[0])\n        alphas = np.sort(alphas)\n        check_non_negative(alphas, \"alpha\")\n        check_is_fitted(self, \"alphas_\")\n        alphas = np.interp(alphas, self.alphas_, self.coef_path_)\n        return alphas, False\n\n", "CodeBleu_score": 42.14521648277757}, {"code": "def _check_alphas(self)\n\t\n    if self.alphas is not None:\n        self.alphas = column_or_1d(self.alphas).ravel()\n        if self.alphas.shape[0] != self.n_alphas:\n            raise ValueError(f\"The number of alphas ({self.alphas.shape[0]}) does not match the number of regularization parameters ({self.n_alphas}).\")\n        if self.alphas.shape[0] < 1:\n            raise ValueError(f\"The number of alphas ({self.alphas.shape[0]}) must be greater than 0.\")\n        if not self.alphas.dtype.kind in \"fi\":\n            raise ValueError(f\"The dtype of alphas ({self.alphas.dtype}) is not supported. Supported types are float and int.\")\n        if not self.alphas.flags.c_contiguous:\n            raise ValueError(f\"The alphas array ({self.alphas}) is not C-contiguous.\")\n        if not self.alphas.flags.writeable:\n            raise ValueError(f\"The alphas array ({self.alphas}) is not writeable.\")\n        if not self.alphas.flags.aligned:\n            raise ValueError(f\"The alphas array ({self.alphas}) is not aligned.\")\n        if not self.alphas.flags.fnc:\n            raise ValueError(f\"The alphas array ({self.alphas}) is not Fortran-contiguous.\")\n        if not self.alphas.flags.forc:\n            raise ValueError(f\"The alphas array ({self.alphas}) is not aligned and Fortran-contiguous.\")\n        if not self.alphas.flags.owndata:\n            raise ValueError(f\"The alphas array ({self.alphas}) is not owned by the user.\")\n        if not self.alphas.flags.aligned:\n            raise ValueError(f\"The alphas array ({self.alphas}) is not aligned.\")\n        if not self.alphas.flags.writeable:\n            raise ValueError(f\"The", "CodeBleu_score": 19.514016264679434}]}
{"instance_id": "7b54a0554ba4", "generation_result": [{"code": "def nelson_aalen_estimator(event, time)\n\t\n    event, time = check_array(event, ensure_2d=False), check_array(time, ensure_2d=False)\n    check_consistent_length(event, time)\n    unique_times = np.unique(time)\n    n_events = np.sum(event, dtype=int)\n    n_at_risk = len(time) - np.count_nonzero(np.isnan(time))\n    if n_events > 0:\n        idx = np.argsort(time, kind=\"mergesort\")\n        total = 0\n        rate = np.empty(len(unique_times))\n        for i, t in enumerate(unique_times):\n            while idx[total] < len(time) and t == time[idx[total]]:\n                total += 1\n            rate[i] = np.sum(event[idx[:total]]) / n_at_risk\n        return unique_times, np.cumsum(rate)\n    return unique_times, np.repeat(0, len(unique_times))\n\n", "CodeBleu_score": 27.122723520767657}, {"code": "def nelson_aalen_estimator(event, time)\n\t\n    event, time = check_array(event, ensure_2d=False), check_array(time, ensure_2d=False)\n    check_consistent_length(event, time)\n\n    unique_times = np.unique(time)\n    n_times = len(unique_times)\n    n_samples = len(time)\n    n_events = np.sum(event)\n\n    if n_samples < 1:\n        raise ValueError(\"No samples found. Check the length of the time and event vectors.\")\n\n    if n_events < 1:\n        raise ValueError(\"No events found. Check the event vector.\")\n\n    if n_samples < n_events:\n        raise ValueError(\"Found more events than samples. Check the event vector.\")\n\n    if n_times < 2:\n        raise ValueError(\"Found only one unique time. Check the time vector.\")\n\n    order = np.argsort(time, kind=\"mergesort\")\n    risk_set = np.empty(n_samples, dtype=int)\n    total_events = np.empty(n_samples, dtype=int)\n    total_events[0] = event[order[0]]\n    risk_set[0] = n_events\n\n    for i in range(1, n_samples):\n        if time[order[i - 1]] != time[order[i]]:\n            total_events[i] = event[order[i]]\n        else:\n            total_events[i] = event[order[i]] + total_events[i - 1]\n\n        risk_set[i] = n_samples - i + total_events[i - 1]\n\n    # order by time\n    order = np.argsort(time[order], kind=\"mergesort\")\n    risk_set = risk_set[order]\n\n    # group by time\n    diff = np.empty(n_times, dtype=int)\n    total_events = np.empty(n_times, dtype=int)\n    n_censored = np.empty(n_times, dtype=int)\n\n    i = 0\n    j = 0\n", "CodeBleu_score": 22.539636647581617}, {"code": "def nelson_aalen_estimator(event, time)\n\t\n    event, time = check_array(event, ensure_2d=False), check_array(time, ensure_2d=False)\n    check_consistent_length(event, time)\n    unique_times = np.sort(np.unique(time))\n    n_events = event.sum()\n    n_at_risk = len(time) - event.sum()\n    if n_events == 0:\n        raise ValueError(\"No event observed. Unable to compute survival function.\")\n    if n_at_risk == 0:\n        raise ValueError(\"All individuals have experienced the event. Unable to compute survival function.\")\n    idx = np.argsort(time, kind=\"mergesort\")\n    event = event[idx]\n    time = time[idx]\n    at_risk = n_at_risk - np.cumsum(event)\n    # Compute the Nelson-Aalen estimate\n    cum_hazard = np.cumsum(event / at_risk)\n    return unique_times, cum_hazard\n\n", "CodeBleu_score": 31.292549400798563}]}
{"instance_id": "35af806300cc", "generation_result": [{"code": "def ipc_weights(event, time)\n\t\n\n    if (event == 0).all():\n        return np.ones(len(event))\n\n    km = stats.kaplan_meier(event, time)\n    km_surv = km.survival_function_\n    km_time = km.timeline\n\n    ix = np.searchsorted(km_time[::-1], time[::-1])\n    km_prob = km_surv[ix]\n    km_prob = np.clip(km_prob, a_min=1e-7, a_max=1 - 1e-7)\n\n    weights = 1. / km_prob\n\n    return weights\n\n", "CodeBleu_score": 37.670696518125034}, {"code": "def ipc_weights(event, time)\n\t\n\n    # Check if all events have occurred\n    if event.all():\n        return np.ones_like(event)\n\n    # Compute Kaplan-Meier estimator for event and time data in reverse\n    kmf = KaplanMeierFitter().fit(time, 1 - event)\n\n    # Find index positions of event times within unique time values\n    idx = np.searchsorted(kmf.timeline, time)\n\n    # Retrieve survival probabilities at event times\n    probs = kmf.survival_function_.iloc[idx].values\n\n    # Ensure all retrieved probabilities are positive\n    probs = np.where(probs > 0, probs, 1)\n\n    # Create weights array\n    weights = np.where(event == 1, 1 / probs, 0)\n\n    return weights\n\n", "CodeBleu_score": 35.99512776147874}, {"code": "def ipc_weights(event, time)\n\t\n    if all(event):\n        return np.ones_like(event)\n    km_times, km_survival = kaplan_meier_estimator(~event, time)\n    # find the indices of the event times within the unique times\n    event_idx = np.searchsorted(km_times, time[event])\n    # retrieve the survival probabilities at event time indices\n    probs_at_event = km_survival[event_idx]\n    # ensure the survival probabilities are positive\n    probs_at_event[probs_at_event <= 0] = 1e-10\n    weights = 1.0 / probs_at_event\n    return weights\n\n", "CodeBleu_score": 38.62720287653689}]}
{"instance_id": "72b19985ea11", "generation_result": [{"code": "def get_dfs(\n        self,\n        dfs: Union[\n            pd.DataFrame, BaseConnector, List[Union[pd.DataFrame, BaseConnector]]\n        ],\n    )\n\t\n    if isinstance(dfs, pd.DataFrame) or isinstance(dfs, modin.pandas.DataFrame) or isinstance(dfs, pl.DataFrame):\n        dfs = [dfs]\n    if isinstance(dfs, list):\n        for i, df in enumerate(dfs):\n            if isinstance(df, pd.DataFrame) or isinstance(df, modin.pandas.DataFrame) or isinstance(df, pl.DataFrame):\n                dfs[i] = BaseConnector(df)\n            elif isinstance(df, BaseConnector):\n                pass\n            else:\n                raise ValueError(\"Invalid dataframe type\")\n    elif isinstance(dfs, BaseConnector):\n        dfs = [dfs]\n    else:\n        raise ValueError(\"Invalid dataframe type\")\n    return dfs\n\n", "CodeBleu_score": 29.543528739190904}, {"code": "def get_dfs(\n        self,\n        dfs: Union[\n            pd.DataFrame, BaseConnector, List[Union[pd.DataFrame, BaseConnector]]\n        ],\n    )\n\t\n    if isinstance(dfs, pd.DataFrame):\n        dfs = [dfs]\n    elif isinstance(dfs, BaseConnector):\n        dfs = [dfs]\n    elif isinstance(dfs, list):\n        for i, df in enumerate(dfs):\n            if isinstance(df, pd.DataFrame):\n                dfs[i] = PandasConnector(df)\n            elif isinstance(df, modin.pandas.DataFrame):\n                dfs[i] = ModinConnector(df)\n            elif isinstance(df, pl.DataFrame):\n                dfs[i] = PolarsConnector(df)\n            elif not isinstance(df, BaseConnector):\n                raise ValueError(\n                    f\"Invalid input type: {type(df)}. Expected pandas.DataFrame, modin.pandas.DataFrame, polars.DataFrame, or BaseConnector.\"\n                )\n    else:\n        raise ValueError(\n            f\"Invalid input type: {type(dfs)}. Expected pandas.DataFrame, modin.pandas.DataFrame, polars.DataFrame, or BaseConnector.\"\n        )\n    return dfs\n\n", "CodeBleu_score": 29.285785710176203}, {"code": "def get_dfs(\n        self,\n        dfs: Union[\n            pd.DataFrame, BaseConnector, List[Union[pd.DataFrame, BaseConnector]]\n        ],\n    )\n\t\n    if isinstance(dfs, pd.DataFrame) or isinstance(dfs, modin.pandas.DataFrame) or isinstance(dfs, pl.DataFrame):\n        dfs = [dfs]\n    elif isinstance(dfs, BaseConnector):\n        dfs = [dfs]\n    elif isinstance(dfs, list):\n        pass\n    else:\n        raise ValueError(\n            f\"Invalid input type: {type(dfs)}. Expected pd.DataFrame, BaseConnector, or list.\"\n        )\n\n    connectors = []\n    for df in dfs:\n        if isinstance(df, pd.DataFrame) or isinstance(df, modin.pandas.DataFrame):\n            connector = PandasConnector(df)\n        elif isinstance(df, pl.DataFrame):\n            connector = PolarsConnector(df)\n        elif isinstance(df, BaseConnector):\n            connector = df\n        else:\n            raise ValueError(\n                f\"Invalid input type: {type(df)}. Expected pd.DataFrame, BaseConnector, or list.\"\n            )\n        connectors.append(connector)\n\n    return connectors\n\n", "CodeBleu_score": 38.490342972792234}]}
{"instance_id": "cf22fb696e90", "generation_result": [{"code": "def execute(self) -> pd.DataFrame\n\t\n    if self.cache_path is not None and self.cache_path.exists():\n        return pd.read_parquet(self.cache_path)\n    elif self.cache_path is not None and self.cache_path.with_suffix(\".parquet\").exists():\n        return pd.read_parquet(self.cache_path.with_suffix(\".parquet\"))\n    elif isinstance(self.data, pd.DataFrame):\n        return self.data\n    else:\n        self.data = self.fetch_data()\n        return self.data\n\n", "CodeBleu_score": 46.55440654762926}, {"code": "def execute(self) -> pd.DataFrame\n\t\n    if self.cache_dir is not None:\n        if self.filters is not None:\n            cache_file = os.path.join(self.cache_dir, f\"{self.name}_{self.filters}.pkl\")\n        else:\n            cache_file = os.path.join(self.cache_dir, f\"{self.name}.pkl\")\n        if os.path.exists(cache_file):\n            return pd.read_pickle(cache_file)\n    if isinstance(self.data, pd.DataFrame):\n        return self.data\n    self.data = self.fetch_data()\n    if self.cache_dir is not None:\n        if self.filters is not None:\n            cache_file = os.path.join(self.cache_dir, f\"{self.name}_{self.filters}.pkl\")\n        else:\n            cache_file = os.path.join(self.cache_dir, f\"{self.name}.pkl\")\n        self.data.to_pickle(cache_file)\n    return self.data\n\n", "CodeBleu_score": 46.083369744886994}, {"code": "def execute(self) -> pd.DataFrame\n\t\n    if self.cache_dir is not None:\n        if self.filters:\n            cache_file = os.path.join(self.cache_dir, f\"{self.name}_{self.filters_hash}.pkl\")\n            if os.path.exists(cache_file):\n                return pd.read_pickle(cache_file)\n        else:\n            cache_file = os.path.join(self.cache_dir, f\"{self.name}.pkl\")\n            if os.path.exists(cache_file):\n                return pd.read_pickle(cache_file)\n    if isinstance(self.data, pd.DataFrame):\n        return self.data\n    self.data = self.fetch()\n    if self.cache_dir is not None:\n        if self.filters:\n            cache_file = os.path.join(self.cache_dir, f\"{self.name}_{self.filters_hash}.pkl\")\n            self.data.to_pickle(cache_file)\n        else:\n            cache_file = os.path.join(self.cache_dir, f\"{self.name}.pkl\")\n            self.data.to_pickle(cache_file)\n    return self.data\n\n", "CodeBleu_score": 44.99377787115995}]}
{"instance_id": "21df5ecea95f", "generation_result": [{"code": "def column_hash(self)\n\t\n    if not isinstance(self, pd.DataFrame):\n        self = self.get_dataframe()\n    hash_string = \"|\".join(self.columns)\n    hash_string += \"|\" + self.formula\n    return hashlib.sha256(hash_string.encode()).hexdigest()\n\n", "CodeBleu_score": 42.00200766242234}, {"code": "def column_hash(self)\n\t\n    if not isinstance(self, pd.DataFrame):\n        self = self.df\n    column_names_str = \"|\".join(self.columns)\n    formula_str = \"|\".join(self.formula)\n    return hashlib.sha256(f\"{column_names_str}|{formula_str}\".encode()).hexdigest()\n\n", "CodeBleu_score": 41.20829936977617}, {"code": "def column_hash(self)\n\t\n    if not isinstance(self, pd.DataFrame):\n        self = self.get_df()\n    return hashlib.sha256(\n        (\n            \"|\".join(self.columns)\n            + \"|\"\n            + \"formula=\"\n            + self.formula\n        ).encode()\n    ).hexdigest()\n\n", "CodeBleu_score": 31.968251149490367}]}
{"instance_id": "27ee5f415000", "generation_result": [{"code": "def _load_df(self, df: Union[pd.DataFrame, pd.Series, str, list, dict])\n\t\n    if isinstance(df, pd.Series):\n        self.df = df.to_frame()\n    elif isinstance(df, pd.DataFrame):\n        self.df = df\n    elif isinstance(df, list):\n        self.df = pd.DataFrame(df)\n    elif isinstance(df, dict):\n        self.df = pd.DataFrame.from_dict(df)\n    elif isinstance(df, str):\n        if df.endswith(\".csv\"):\n            self.df = pd.read_csv(df)\n        elif df.endswith(\".xlsx\"):\n            self.df = pd.read_excel(df)\n        elif df.endswith(\".parquet\"):\n            self.df = pd.read_parquet(df)\n        else:\n            raise ValueError(\"Unsupported file type\")\n    else:\n        raise ValueError(\"Unsupported input type\")\n\n", "CodeBleu_score": 53.43039482882098}, {"code": "def _load_df(self, df: Union[pd.DataFrame, pd.Series, str, list, dict])\n\t\n    if isinstance(df, pd.Series):\n        df = df.to_frame()\n    elif isinstance(df, pd.DataFrame):\n        pass\n    elif isinstance(df, list):\n        df = pd.DataFrame(df)\n    elif isinstance(df, dict):\n        df = pd.DataFrame(df)\n    elif isinstance(df, str):\n        if df.endswith(\".csv\"):\n            df = pd.read_csv(df)\n        elif df.endswith(\".parquet\"):\n            df = pd.read_parquet(df)\n        elif df.endswith(\".json\"):\n            df = pd.read_json(df)\n        elif df.endswith(\".xlsx\"):\n            df = pd.read_excel(df)\n        elif df.endswith(\".html\"):\n            df = pd.read_html(df)\n        elif df.endswith(\".xml\"):\n            df = pd.read_xml(df)\n        elif df.endswith(\".feather\"):\n            df = pd.read_feather(df)\n        elif df.endswith(\".parquet\"):\n            df = pd.read_parquet(df)\n        elif df.endswith(\".orc\"):\n            df = pd.read_orc(df)\n        elif df.endswith(\".hdf\"):\n            df = pd.read_hdf(df)\n        elif df.endswith(\".msgpack\"):\n            df = pd.read_msgpack(df)\n        elif df.endswith(\".stata\"):\n            df = pd.read_stata(df)\n        elif df.endswith(\".sas\"):\n            df = pd.read_sas(df)\n        elif df.endswith(\".spss\"):\n            df = pd.read_spss(df)\n        elif df.endswith(\".pickle\"):\n            df = pd.read_pickle(df)\n        elif df.endswith(\".sql\"):\n            df = pd", "CodeBleu_score": 38.47756225641644}, {"code": "def _load_df(self, df: Union[pd.DataFrame, pd.Series, str, list, dict])\n\t\n    if isinstance(df, pd.Series):\n        df = pd.DataFrame(df)\n    elif isinstance(df, str):\n        if df.endswith(\".csv\"):\n            df = pd.read_csv(df)\n        elif df.endswith(\".parquet\"):\n            df = pd.read_parquet(df)\n        elif df.endswith(\".json\"):\n            df = pd.read_json(df)\n        elif df.endswith(\".xlsx\"):\n            df = pd.read_excel(df)\n        elif df.endswith(\".html\"):\n            df = pd.read_html(df)\n        elif df.endswith(\".xml\"):\n            df = pd.read_xml(df)\n        elif df.endswith(\".feather\"):\n            df = pd.read_feather(df)\n        elif df.endswith(\".hdf\"):\n            df = pd.read_hdf(df)\n        elif df.endswith(\".sql\"):\n            df = pd.read_sql(df)\n        elif df.endswith(\".gbq\"):\n            df = pd.read_gbq(df)\n        elif df.endswith(\".stata\"):\n            df = pd.read_stata(df)\n        elif df.endswith(\".sas\"):\n            df = pd.read_sas(df)\n        elif df.endswith(\".spss\"):\n            df = pd.read_spss(df)\n        elif df.endswith(\".pickle\"):\n            df = pd.read_pickle(df)\n        elif df.endswith(\".msgpack\"):\n            df = pd.read_msgpack(df)\n        elif df.endswith(\".parquet\"):\n            df = pd.read_parquet(df)\n        elif df.endswith(\".orc\"):\n            df = pd.read_orc(df)\n        elif df.endswith(\".hdf5\"):\n            df = pd.read_hdf", "CodeBleu_score": 33.929127372684675}]}
{"instance_id": "38f48a358606", "generation_result": [{"code": "def sample(self, n: int = 3) -> pd.DataFrame\n\t\n    if len(self) <= n:\n        return self.sample(frac=1).reset_index(drop=True)\n    else:\n        sample_df = pd.DataFrame()\n        for col in self.columns:\n            if self[col].dtype == \"object\":\n                sample_df[col] = self[col].sample(n=n, replace=True)\n            else:\n                sample_df[col] = self[col].sample(n=n, replace=True)\n        sample_df = sample_df.reset_index(drop=True)\n        sample_df.loc[0:2, :] = self.loc[0:2, :].apply(\n            lambda x: x.apply(lambda y: \"\".join(random.choices(y, k=len(y))))\n        )\n        return sample_df", "CodeBleu_score": 34.694076741722284}, {"code": "def sample(self, n: int = 3) -> pd.DataFrame\n\t\n    if len(self) <= n:\n        return self.sample(frac=1).reset_index(drop=True)\n\n    sample = pd.DataFrame()\n    for col in self.columns:\n        if self[col].dtype == \"object\":\n            sample[col] = self[col].sample(n, replace=True).values\n        else:\n            sample[col] = self[col].sample(n, replace=True).values\n\n    # Anonymize the first few rows of the sampled DataFrame\n    for i in range(min(n, 5)):\n        sample.iloc[i] = self.iloc[0].copy()\n\n    return sample", "CodeBleu_score": 36.99208363068095}, {"code": "def sample(self, n: int = 3) -> pd.DataFrame\n\t\n    if self.shape[0] < n:\n        return self.sample(frac=1).reset_index(drop=True)\n    else:\n        sampled_df = pd.DataFrame()\n        for col in self.columns:\n            if col in [\"Age\", \"Height\", \"Weight\", \"Salary\"]:\n                sampled_df[col] = self[col].sample(n, replace=True)\n            elif col in [\"Gender\", \"Country\", \"City\", \"State\", \"Zip\", \"Job\"]:\n                sampled_df[col] = self[col].sample(n, replace=True).reset_index(drop=True)\n            elif col in [\"Birthdate\", \"HireDate\"]:\n                sampled_df[col] = self[col].sample(n, replace=True).reset_index(drop=True)\n            else:\n                sampled_df[col] = self[col].sample(n, replace=True).reset_index(drop=True)\n        # Anonymize the first few rows of the sampled DataFrame\n        for i in range(min(n, 3)):\n            sampled_df.iloc[i, :] = self.iloc[i, :]\n        return sampled_df", "CodeBleu_score": 31.184627572500958}]}
{"instance_id": "8bddad2e31ca", "generation_result": [{"code": "def convert_df_to_csv(self, df: pd.DataFrame, extras: dict) -> str\n\t\n    name = extras.get(\"name\", \"\")\n    description = extras.get(\"description\", \"\")\n    csv = df.to_csv(index=False)\n    return f\"<dataframe name='{name}' description='{description}'>", "CodeBleu_score": 22.278691107795755}, {"code": "def convert_df_to_csv(self, df: pd.DataFrame, extras: dict) -> str\n\t\n    df_name = extras.get(\"name\", \"\")\n    df_desc = extras.get(\"desc\", \"\")\n    df_content = df.to_csv(index=False)\n    df_dims = df.shape\n    df_rows = df_dims[0]\n    df_cols = df_dims[1]\n    df_cols_names = df.columns.tolist()\n    df_cols_types = df.dtypes.tolist()\n    df_cols_types_str = [str(x) for x in df_cols_types]\n    df_cols_types_str = [x.replace(\"object\", \"string\") for x in df_cols_types_str]\n    df_cols_types_str = [x.replace(\"int64\", \"integer\") for x in df_cols_types_str]\n    df_cols_types_str = [x.replace(\"float64\", \"float\") for x in df_cols_types_str]\n    df_cols_types_str = [x.replace(\"bool\", \"boolean\") for x in df_cols_types_str]\n    df_cols_types_str = [x.replace(\"datetime64\", \"date\") for x in df_cols_types_str]\n    df_cols_types_str = [x.replace(\"timedelta\", \"time\") for x in df_cols_types_str]\n    df_cols_types_str = [x.replace(\"category\", \"string\") for x in df_cols_types_str]\n    df_cols_types_str = [x.replace(\"bytes\", \"string\") for x in df_cols_types_str]\n    df_cols_types_str = [x.replace(\"mixed\", \"string\") for x in df_cols_types_str]\n    df_cols_types_str = [x.replace(\"unicode\", \"string\") for x in df_cols_types_str]\n    df_cols_types_str = [x.replace(\"complex\", \"string\") for x in df_cols", "CodeBleu_score": 29.58558031777349}, {"code": "def convert_df_to_csv(self, df: pd.DataFrame, extras: dict) -> str\n\t\n    extras = extras or {}\n    name = extras.get(\"name\", \"\")\n    description = extras.get(\"description\", \"\")\n    csv = df.to_csv(index=False)\n    csv_lines = csv.split(\"", "CodeBleu_score": 21.73792106157056}]}
{"instance_id": "5380bb60fd39", "generation_result": [{"code": "def convert_df_sql_connector_to_str\n\t\n    def convert_df_sql_connector_to_str(df, n_rows=100):\n        \"\"\"This function converts a DataFrame into an HTML table string.", "CodeBleu_score": 3.463922773023864}, {"code": "def convert_df_sql_connector_to_str\n\t\n    def convert_df_sql_connector_to_str(df):\n        # Get the table name and description from the DataFrame metadata\n        table_name = df.table_name\n        description = df.description\n\n        # Construct the opening table tag with the table name and optional description\n        html = f\"<table name='{table_name}'>\"\n        if description:\n            html += f\"<p>{description}</p>\"\n\n        # Add the CSV representation of the DataFrame's head\n        html += df.head().to_html(index=False)\n\n        # Close the table tag\n        html += \"</table>\"\n\n        return html\n\n", "CodeBleu_score": 34.00152237766878}, {"code": "def convert_df_sql_connector_to_str\n\t\n    pass\n\n", "CodeBleu_score": 2.4514239784450392e-14}]}
{"instance_id": "0afd995716c5", "generation_result": [{"code": "def convert_df_to_yml(self, df: pd.DataFrame, extras: dict) -> str:\n\t\n    json_string = self.convert_df_to_json(df)\n    yaml_string = yaml.dump(json.loads(json_string))\n    if extras.get(\"is_direct_sql\", False):\n        return f\"<table>{yaml_string}</table>\"\n    return yaml_string\n\n", "CodeBleu_score": 48.19123802743185}, {"code": "def convert_df_to_yml(self, df: pd.DataFrame, extras: dict) -> str:\n\t\n    json_string = df.to_json(orient=\"records\")\n    yaml_string = json.dumps(json_string)\n    if extras.get(\"is_direct_sql\"):\n        yaml_string = f\"<table>{yaml_string}</table>\"\n    return yaml_string\n\n", "CodeBleu_score": 42.06009393759192}, {"code": "def convert_df_to_yml(self, df: pd.DataFrame, extras: dict) -> str:\n\t\n    json_string = self.convert_df_to_json(df, extras)\n    yml_string = yaml.dump(json.loads(json_string))\n    if \"is_direct_sql\" in extras and extras[\"is_direct_sql\"]:\n        return f\"<table>{yml_string}</table>\"\n    return yml_string\n\n", "CodeBleu_score": 69.88205497435875}]}
{"instance_id": "e2b90ebd4e5d", "generation_result": [{"code": "def validate_value(self, expected_type: str) -> bool\n\t\n    if expected_type == \"number\":\n        return isinstance(self.value, (int, float))\n    elif expected_type == \"string\":\n        return isinstance(self.value, str)\n    elif expected_type == \"dataframe\":\n        return isinstance(self.value, (pd.DataFrame, pd.Series))\n    elif expected_type == \"plot\":\n        return isinstance(self.value, str) or isinstance(self.value, dict)\n    elif expected_type == \"\":\n        return True\n    else:\n        raise ValueError(f\"Invalid expected_type: {expected_type}\")\n\n", "CodeBleu_score": 53.69309094893977}, {"code": "def validate_value(self, expected_type: str) -> bool\n\t\n    if expected_type == \"\":\n        return True\n    elif expected_type == \"number\":\n        return isinstance(self.value, (int, float))\n    elif expected_type == \"string\":\n        return isinstance(self.value, str)\n    elif expected_type == \"dataframe\":\n        return isinstance(self.value, (pd.DataFrame, pd.Series))\n    elif expected_type == \"plot\":\n        if isinstance(self.value, str):\n            return re.match(r\"^\\.\\./data/plots/.+\\.(png|jpg|jpeg|svg|gif)$\", self.value) is not None\n        elif isinstance(self.value, dict):\n            return all(isinstance(key, str) and isinstance(value, (int, float)) for key, value in self.value.items())\n        else:\n            return False\n    else:\n        raise ValueError(f\"Invalid expected_type: {expected_type}\")\n\n", "CodeBleu_score": 60.930992806468474}, {"code": "def validate_value(self, expected_type: str) -> bool\n\t\n    if expected_type == \"\":\n        return True\n    elif expected_type == \"number\":\n        return isinstance(self.value, (int, float))\n    elif expected_type == \"string\":\n        return isinstance(self.value, str)\n    elif expected_type == \"dataframe\":\n        return isinstance(self.value, (pd.DataFrame, pd.Series))\n    elif expected_type == \"plot\":\n        return (\n            isinstance(self.value, str)\n            and re.match(r\"^plots\\/.+\\.(png|jpg|jpeg|svg)$\", self.value) is not None\n        ) or isinstance(self.value, dict)\n    elif expected_type == \"list\":\n        return isinstance(self.value, Iterable)\n    else:\n        raise InvalidOutputValueMismatch(\n            f\"Invalid expected_type: {expected_type}. Expected one of 'number', 'string', 'dataframe', 'plot', 'list'.\"\n        )", "CodeBleu_score": 54.223855700973026}]}
{"instance_id": "d95828fdcbcb", "generation_result": [{"code": " def _distance(self, X, X2=None)\n\t\n    if self.distance_params is None:\n        self.distance_params = {}\n    if isinstance(self.distance, str):\n        X = convert(X, from_type=\"nested_univ\", to_type=\"numpy3D\")\n        if X2 is None:\n            sig = signature(pairwise_distance)\n            if \"X\" in sig.parameters and \"X2\" not in sig.parameters:\n                dist = pairwise_distance(\n                    X, metric=self.distance, **self.distance_params\n                )\n            else:\n                dist = pairwise_distance(\n                    X, X, metric=self.distance, **self.distance_params\n                )\n        else:\n            X2 = convert(X2, from_type=\"nested_univ\", to_type=\"numpy3D\")\n            dist = pairwise_distance(\n                X, X2, metric=self.distance, **self.distance_params\n            )\n    else:\n        dist = self.distance(X, X2, **self.distance_params)\n    return dist\n\n\n def _distance(self, X, X2=None):\n    \"\"\"The function _distance calculates the distance between two datasets, X and X2, using a specified distance metric and its parameters. If no additional parameters are provided, it initializes an empty dictionary. If the distance metric is a string, it uses the pairwise_distance function from sktime.distances. Otherwise, it directly calls the distance function with the given datasets and parameters. If X2 is None, the function checks if the distance function can handle a single dataset by inspecting its signature and calls it accordingly. The function returns the computed distance.", "CodeBleu_score": 28.32481245883717}, {"code": " def _distance(self, X, X2=None)\n\t\n    if self.distance_params is None:\n        self.distance_params = {}\n    if isinstance(self.distance, str):\n        return pairwise_distance(X, X2, metric=self.distance, **self.distance_params)\n    else:\n        if X2 is None:\n            sig = signature(self.distance)\n            if len(sig.parameters) == 1:\n                return self.distance(X)\n            else:\n                return self.distance(X, X)\n        else:\n            return self.distance(X, X2, **self.distance_params)\n\n", "CodeBleu_score": 30.079533247149264}, {"code": " def _distance(self, X, X2=None)\n\t\n    if self.distance_params is None:\n        self.distance_params = {}\n    if isinstance(self.distance, str):\n        X = convert(X, from_type=\"nested_univ\", to_type=\"numpy3D\")\n        if X2 is None:\n            sig = signature(pairwise_distance)\n            if \"X2\" in sig.parameters:\n                dist = pairwise_distance(X, metric=self.distance, **self.distance_params)\n            else:\n                dist = pairwise_distance(X, metric=self.distance, **self.distance_params)[0]\n        else:\n            X2 = convert(X2, from_type=\"nested_univ\", to_type=\"numpy3D\")\n            dist = pairwise_distance(X, X2, metric=self.distance, **self.distance_params)\n    else:\n        dist = self.distance(X, X2=X2, **self.distance_params)\n    return dist", "CodeBleu_score": 40.39830981606703}]}
{"instance_id": "497d73b298ee", "generation_result": [{"code": "def _convert_X_to_sklearn(self, X)\n\t\n    if X.shape[1] != len(self.series_columns):\n        X = X.reset_index(drop=True)\n        X = X.pivot(\n            index=self.index_columns[0], columns=self.index_columns[1], values=self.series_columns\n        )\n        X = X.fillna(0)\n        X = X.to_numpy()\n        lengths = X.shape[1] / len(self.series_columns)\n        lengths = [lengths] * len(self.series_columns)\n        lengths = [int(length) for length in lengths]\n        lengths = [length for length in lengths for _ in range(length)]\n        X = np.insert(X, 0, lengths, axis=1)\n        X = X.reshape(X.shape[0], -1)\n    else:\n        X = convert(X, from_type=type(X), to_type=\"numpy3D\")\n        X = X.reshape(X.shape[0], -1)\n    return X\n\n", "CodeBleu_score": 36.60059771888973}, {"code": "def _convert_X_to_sklearn(self, X)\n\t\n    if isinstance(X, (pd.DataFrame, pd.Series)):\n        X = convert(X, from_type=\"nested_univ\", to_type=\"numpy3D\")\n    if isinstance(X, np.ndarray):\n        if X.ndim == 2:\n            X_new = X\n        elif X.ndim == 3:\n            X_new = X.reshape(X.shape[0], -1)\n        else:\n            raise ValueError(\n                \"X must be either a 2D or 3D numpy array, or a pandas DataFrame or Series.\"\n            )\n    else:\n        raise ValueError(\n            \"X must be either a 2D or 3D numpy array, or a pandas DataFrame or Series.\"\n        )\n    return X_new\n\n", "CodeBleu_score": 27.051427102539332}, {"code": "def _convert_X_to_sklearn(self, X)\n\t\n    if self._equal_length:\n        if isinstance(X, pd.DataFrame):\n            X_flat = X.to_numpy().flatten()\n        elif isinstance(X, np.ndarray):\n            X_flat = X.flatten()\n        else:\n            X_flat = np.array(X).flatten()\n        return X_flat.reshape(1, -1)\n    else:\n        X_sklearn = convert(X, from_type=\"nested_univ\", to_type=\"numpy3D\")\n        X_sklearn = X_sklearn.reshape(X_sklearn.shape[0], -1)\n        return X_sklearn\n\n", "CodeBleu_score": 35.03594112412366}]}
{"instance_id": "8798260a6f12", "generation_result": [{"code": "def is_sklearn_estimator(obj)\n\t\n    if not isclass(obj):\n        return False\n    return issubclass(obj, SklearnBaseEstimator) and not issubclass(\n        obj, BaseObject\n    )\n\n", "CodeBleu_score": 23.30495100231496}, {"code": "def is_sklearn_estimator(obj)\n\t\n    return isclass(obj) and issubclass(obj, SklearnBaseEstimator) and not issubclass(\n        obj, BaseObject\n    )\n\n", "CodeBleu_score": 14.606920579869897}, {"code": "def is_sklearn_estimator(obj)\n\t\n    return isclass(obj) and issubclass(obj, SklearnBaseEstimator) and not issubclass(\n        obj, BaseObject\n    )\n\n", "CodeBleu_score": 14.606920579869897}]}
{"instance_id": "2ab2596d8520", "generation_result": [{"code": "def __rmul__(self, other)\n\t\n    if isinstance(other, BaseTransformer):\n        transformer_pipeline = other * self.steps\n        return SklearnRegressorPipeline(\n            steps=[\n                (\"transformer\", transformer_pipeline),\n                (\"regressor\", self.regressor),\n            ]\n        )\n    else:\n        return NotImplemented\n\n", "CodeBleu_score": 34.98678358854025}, {"code": "def __rmul__(self, other)\n\t\n    from sktime.regression.sklearn import SklearnRegressorPipeline\n\n    if isinstance(other, BaseTransformer):\n        transformer = other * self.steps[-1][1]\n        steps = self.steps[:-1] + [(self.steps[-1][0], transformer)]\n        return SklearnRegressorPipeline(self.regressor, TransformerPipeline(steps))\n\n    return NotImplemented\n\n", "CodeBleu_score": 44.56561939076693}, {"code": "def __rmul__(self, other)\n\t\n    if isinstance(other, BaseTransformer):\n        other = other.inverse_transform\n        steps = [other] + self.steps\n        other = TransformerPipeline(steps)\n        return SklearnRegressorPipeline(other, self.regressor)\n    else:\n        return NotImplemented\n\n", "CodeBleu_score": 39.721238903020875}]}
{"instance_id": "5c1c6d14ca93", "generation_result": [{"code": "def prep_skl_df(df, copy_df=False)\n\t\n    if copy_df:\n        df = df.copy()\n    df.columns = list(map(str, df.columns))\n    cols = df.columns\n    for col in cols:\n        if col != str(col):\n            try:\n                df[str(col)] = df[col]\n            except Exception:\n                print(f\"Could not convert column {col} to string\")\n            df = df.drop(columns=[col])\n    return df", "CodeBleu_score": 48.073831527142495}, {"code": "def prep_skl_df(df, copy_df=False)\n\t\n    if copy_df:\n        df = df.copy()\n    df.columns = list(map(str, df.columns))\n    if all(isinstance(col, str) for col in df.columns):\n        return df\n    else:\n        df.columns = list(map(str, df.columns))\n        return df\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 49.58575891443899}, {"code": "def prep_skl_df(df, copy_df=False)\n\t\n    if copy_df:\n        df = df.copy()\n    df.columns = list(map(str, df.columns))\n    if all(isinstance(col, str) for col in df.columns):\n        return df\n    else:\n        df.columns = list(map(str, df.columns))\n        return df\n\n\n\n", "CodeBleu_score": 49.58575891443899}]}
{"instance_id": "d5e567c20361", "generation_result": [{"code": "def predict(self, X)\n\t\n    rng = check_random_state(self.random_state)\n    probas = self.predict_proba(X)\n    max_idx = probas.argmax(axis=1)\n    rand_idx = [rng.choice(np.where(probas[i] == probas[i, idx])[0]) for i, idx in enumerate(max_idx)]\n    return np.array([self.classes_[idx] for idx in rand_idx])\n\n", "CodeBleu_score": 33.94440654477649}, {"code": "def predict(self, X)\n\t\n    check_is_fitted(self)\n    rng = check_random_state(self.random_state)\n    probs = self.predict_proba(X)\n    n_instances, _ = probs.shape\n    max_indices = np.argmax(probs, axis=1)\n    rand_indices = rng.randint(0, 1, size=n_instances)\n    y_pred = np.array([self.classes_[i] if rand_indices[i] else self.classes_[i - 1] for i in max_indices])\n    return y_pred\n\n", "CodeBleu_score": 36.39137559247925}, {"code": "def predict(self, X)\n\t\n    check_is_fitted(self)\n    rng = check_random_state(self.random_state)\n    proba = self.predict_proba(X)\n    max_idx = np.argmax(proba, axis=1)\n    return self.classes_[rng.choice(np.where(proba == proba[np.arange(proba.shape[0]), max_idx])[0])]\n\n", "CodeBleu_score": 28.90828613655232}]}
{"instance_id": "210b60dffd55", "generation_result": [{"code": "def _predict_proba_for_estimator(self, X, clf, pcas, groups)\n\t\n    X_group = []\n    for i in range(len(pcas)):\n        X_group.append(pcas[i].transform(X[:, groups[i]]))\n    X_group = np.hstack(X_group)\n    X_group = np.nan_to_num(X_group)\n    proba = clf.predict_proba(X_group)\n    if proba.shape[1] != self.n_classes_:\n        proba = np.hstack([proba, 1 - proba.sum(axis=1, keepdims=True)])\n    return proba\n\n", "CodeBleu_score": 30.724310923327845}, {"code": "def _predict_proba_for_estimator(self, X, clf, pcas, groups)\n\t\n    X_transformed = []\n    for i in range(len(pcas)):\n        X_transformed.append(pcas[i].transform(X[:, groups[i]]))\n    X_transformed = np.concatenate(X_transformed, axis=1)\n    X_transformed = np.nan_to_num(X_transformed)\n    if X_transformed.shape[1] < self.n_components:\n        X_transformed = np.concatenate(\n            [\n                X_transformed,\n                np.zeros((X_transformed.shape[0], self.n_components - X_transformed.shape[1])),\n            ],\n            axis=1,\n        )\n    probas = clf.predict_proba(X_transformed)\n    if probas.shape[1] != self.n_classes_:\n        probas = np.concatenate(\n            [\n                probas,\n                np.zeros((probas.shape[0], self.n_classes_ - probas.shape[1])),\n            ],\n            axis=1,\n        )\n    return probas\n\n", "CodeBleu_score": 36.511481592802134}, {"code": "def _predict_proba_for_estimator(self, X, clf, pcas, groups)\n\t\n    X_ = np.zeros((X.shape[0], 0))\n    for i in range(len(pcas)):\n        X_ = np.concatenate((X_, pcas[i].transform(X[:, groups[i]])), axis=1)\n    X_[np.isnan(X_)] = 0\n    if clf is None:\n        return np.zeros((X.shape[0], self.n_classes_))\n    prob = clf.predict_proba(X_)\n    if prob.shape[1] != self.n_classes_:\n        prob = self._adjust_proba(prob)\n    return prob\n\n", "CodeBleu_score": 35.61220914454605}]}
{"instance_id": "9436ee0860bb", "generation_result": [{"code": "def predict_proba(self, X)\n\t\n    if not self.is_fitted:\n        raise NotFittedError(\n            f\"This instance of {self.__class__.__name__} has not \"\n            f\"been fitted yet; please call `fit` first.\"\n        )\n    if self.n_classes_ == 1:\n        return np.ones(shape=(len(X), 1))\n    if X.ndim == 3 and X.shape[2] == 1:\n        X = X.reshape((X.shape[0], X.shape[1]))\n    elif X.ndim == 2:\n        pass\n    else:\n        raise ValueError(\n            f\"X must be a 2D or 3D numpy array, but found: {X.shape}\"\n        )\n    X = check_array(X, allow_nan=True)\n    distributions = []\n    for instance in range(X.shape[0]):\n        distributions.append(self._root.predict_proba(X[instance, :]))\n    distributions = np.array(distributions)\n    return distributions\n\n", "CodeBleu_score": 45.76052302052558}, {"code": "def predict_proba(self, X)\n\t\n    if not self.is_fitted:\n        raise NotFittedError(\n            f\"This instance of {self.__class__.__name__} has not \" f\"been fitted yet; please call `fit` first.\"\n        )\n\n    if self.n_classes == 1:\n        return np.ones((X.shape[0], 1))\n\n    if X.ndim == 3 and X.shape[2] == 1:\n        X = X.reshape((X.shape[0], X.shape[1]))\n    elif X.ndim != 2:\n        raise ValueError(\n            f\"{self.__class__.__name__} predict_proba expects a 2D numpy array or a 3D numpy array with a single feature\"\n        )\n\n    X = check_array(X, force_all_finite=\"allow-nan\", allow_nan=True)\n\n    probs = []\n    for instance in X:\n        probs.append(self.root.predict_proba(instance))\n\n    return np.array(probs)\n\n", "CodeBleu_score": 42.86398387106603}, {"code": "def predict_proba(self, X)\n\t\n    if not self.is_fitted:\n        raise NotFittedError(\n            f\"This instance of {self.__class__.__name__} has not \" f\"been fitted yet\"\n        )\n    if self.n_classes == 1:\n        return np.ones(X.shape[0])\n    if X.ndim == 3 and X.shape[2] == 1:\n        X = X.reshape((X.shape[0], X.shape[1]))\n    elif X.ndim == 2:\n        pass\n    else:\n        raise ValueError(\n            f\"{self.__class__.__name__} predict_proba expects a 2D or 3D numpy array, \"\n            f\"but found input of shape: {X.shape}\"\n        )\n    X = check_X(X)\n    dists = np.zeros((X.shape[0], self.n_classes))\n    for i in range(X.shape[0]):\n        dists[i] = self.root.predict_proba(X[i])\n    return dists\n\n", "CodeBleu_score": 48.76427276693669}]}
{"instance_id": "d7cb1cb66840", "generation_result": [{"code": "def _convert_X_to_sklearn(self, X)\n\t\n    Xt = self.transformers_.transform(X)\n    scitype = convert_to(Xt, to_type=\"scitype\")\n    if scitype == \"Primitives\":\n        Xt = convert_to(Xt, to_type=\"numpy2D\")\n    elif scitype == \"Series\":\n        Xt = convert_to(Xt, to_type=\"numpyflat\")\n    else:\n        raise TypeError(\n            \"unexpected output type from transformers, should be Series or Panel\"\n        )\n    return Xt\n\n", "CodeBleu_score": 35.24004760566145}, {"code": "def _convert_X_to_sklearn(self, X)\n\t\n    if self.transformers_ is not None:\n        Xt = self.transformers_.fit_transform(X)\n        scitype = convert_to(Xt, to_type=\"scitype\")\n        if scitype == \"Primitives\":\n            Xt = convert_to(Xt, to_type=\"numpy2D\")\n        elif scitype == \"Series\":\n            Xt = convert_to(Xt, to_type=\"numpyflat\")\n        else:\n            raise TypeError(\n                \"unexpected output type from transformers, should be \"\n                \"Series or Panel, but found {}\".format(scitype)\n            )\n        return Xt\n    else:\n        return X\n\n", "CodeBleu_score": 45.856151155869206}, {"code": "def _convert_X_to_sklearn(self, X)\n\t\n    Xt = self.transformers_.transform(X)\n    if Xt.get_scitype() == \"Primitives\":\n        Xt = convert_to(Xt, \"numpy2D\")\n    elif Xt.get_scitype() == \"Series\":\n        Xt = convert_to(Xt, \"numpyflat\")\n    else:\n        raise TypeError(\n            \"unexpected output type from transformers, should be Series or Panel\"\n        )\n    return Xt\n\n", "CodeBleu_score": 28.887340658021383}]}
{"instance_id": "497cb75f2b4d", "generation_result": [{"code": "def set_random_state(estimator, random_state=0)\n\t\n    random_state = check_random_state(random_state)\n    params = {}\n    for key in estimator.get_params(deep=True):\n        if \"random_state\" in key:\n            params[key] = random_state.randint(0, 2 ** 32 - 1)\n    if len(params) > 0:\n        estimator.set_params(**params)", "CodeBleu_score": 39.697533054039106}, {"code": "def set_random_state(estimator, random_state=0)\n\t\n    random_state = check_random_state(random_state)\n    params = dict()\n    for key in estimator.get_params(deep=True):\n        if \"random_state\" in key:\n            params[key] = random_state.randint(0, 2**32 - 1)\n    if params:\n        estimator.set_params(**params)", "CodeBleu_score": 34.43056486063273}, {"code": "def set_random_state(estimator, random_state=0)\n\t\n    random_state = check_random_state(random_state)\n    params = dict()\n    for key in estimator.get_params(deep=True):\n        if \"random_state\" in key:\n            params[key] = random_state.randint(0, 2 ** 31 - 1)\n    if params:\n        estimator.set_params(**params)", "CodeBleu_score": 34.433130749153854}]}
{"instance_id": "59ffd1d596a1", "generation_result": [{"code": "def _transform(X, intervals)\n\t\n    n_instances, _ = X.shape\n    n_intervals = len(intervals)\n    transformed = np.zeros((n_instances, 3 * n_intervals))\n\n    for i in range(n_intervals):\n        start, end = intervals[i]\n        sub_arr = X[:, start:end]\n        means = np.mean(sub_arr, axis=1)\n        std_dev = np.std(sub_arr, axis=1)\n        slope = _slope(sub_arr.T, axis=0)\n        transformed[:, 3 * i] = means\n        transformed[:, 3 * i + 1] = std_dev\n        transformed[:, 3 * i + 2] = slope\n\n    return transformed.T\n\n", "CodeBleu_score": 42.259091916700406}, {"code": "def _transform(X, intervals)\n\t\n    n_instances, _ = X.shape\n    n_intervals = len(intervals)\n    transformed = np.zeros((n_instances, 3 * n_intervals))\n    for interval_id, interval in enumerate(intervals):\n        temp = X[:, interval]\n        means = np.mean(temp, axis=1)\n        std_devs = np.std(temp, axis=1)\n        slopes = np.apply_along_axis(_slope, 1, temp)\n        transformed[:, 3 * interval_id] = means\n        transformed[:, 3 * interval_id + 1] = std_devs\n        transformed[:, 3 * interval_id + 2] = slopes\n    return transformed.transpose()\n\n", "CodeBleu_score": 35.17750551723255}, {"code": "def _transform(X, intervals)\n\t\n    n_instances, _ = X.shape\n    n_intervals = len(intervals)\n    transformed = np.zeros((n_instances, 3 * n_intervals))\n    for i, (start, end) in enumerate(intervals):\n        slice = X[:, start:end]\n        means = np.nanmean(slice, axis=1)\n        stds = np.nanstd(slice, axis=1)\n        slope = _slope(means, stds)\n        transformed[:, 3 * i] = means\n        transformed[:, 3 * i + 1] = stds\n        transformed[:, 3 * i + 2] = slope\n    return transformed.transpose()\n\n", "CodeBleu_score": 37.680547414436035}]}
{"instance_id": "5a01594eebfb", "generation_result": [{"code": "def plot_and_annotate_facets(\n            self, plot: str, plot_params: dict, configuration: dict,\n            annotation_func: str, *args, annotation_params: dict = None,\n            ax_op_before: List[Union[str, Optional[list],\n                                     Optional[dict]]] = None,\n            ax_op_after: List[Union[str, Optional[list],\n                                    Optional[dict]]] = None,\n            annotate_params: dict = None, **kwargs)\n\t\n    annotation_params = empty_dict_if_none(annotation_params)\n    annotate_params = empty_dict_if_none(annotate_params)\n    ax_op_before = empty_dict_if_none(ax_op_before)\n    ax_op_after = empty_dict_if_none(ax_op_after)\n    plot_params = empty_dict_if_none(plot_params)\n    configuration = empty_dict_if_none(configuration)\n    # Check plot type\n    check_is_in(plot, IMPLEMENTED_PLOTS)\n    # Check annotation type\n    check_is_in(annotation_func, IMPLEMENTED_ANNOTATIONS)\n    # Check p-value format\n    if 'pvalue_format' in configuration:\n        check_is_in(configuration['pvalue_format'], PVALUE_CONFIGURABLE_PARAMETERS)\n    # Check alpha\n    if 'alpha' in configuration:\n        check_alpha(configuration['alpha'])\n    # Check comparisons correction\n    if 'comparisons_correction' in configuration:\n        get_validated_comparisons_correction(\n            configuration['comparisons_correction'])\n    # Check num_comparisons\n    if 'num_comparisons' in configuration:\n        check_num_comparisons(configuration['num_comparisons'])\n    # Check pvalues\n    if 'pvalues' in configuration:\n        check_pvalues(configuration['pvalues'])\n    # Initialize annotation and operation parameters\n    if annotation_params is None:\n        annotation_params = {}\n    if annotate_params is None:\n        annotate_params = {}\n    if ax_op_before is None:\n        ax_op_before = []\n    if ax_op_after is None:\n        ax_op_after = []\n    # Create plot\n    plotter = _SeabornPlotter(self, plot, plot_params, ax_op_before)\n    plotter.plot()\n    # Apply pre-plot operations\n    plotter.apply_pre_plot_operations()\n    # Configure plot\n    plotter.configure_plot(configuration", "CodeBleu_score": 38.903385410851406}, {"code": "def plot_and_annotate_facets(\n            self, plot: str, plot_params: dict, configuration: dict,\n            annotation_func: str, *args, annotation_params: dict = None,\n            ax_op_before: List[Union[str, Optional[list],\n                                     Optional[dict]]] = None,\n            ax_op_after: List[Union[str, Optional[list],\n                                    Optional[dict]]] = None,\n            annotate_params: dict = None, **kwargs)\n\t\n    # Initialize annotation and operation parameters if they are not provided\n    annotate_params = empty_dict_if_none(annotate_params)\n    ax_op_before = empty_dict_if_none(ax_op_before)\n    ax_op_after = empty_dict_if_none(ax_op_after)\n\n    # Create the plot with given parameters and apply pre-plot axis operations\n    self.plot(**plot_params)\n    self.apply_ax_ops(ax_op_before)\n\n    # Configure and annotate the plot based on provided configuration and annotation functions\n    self.configure(**configuration)\n    self.annotate(annotation_func, *args, **annotation_params)\n\n    # Apply post-plot axis operations\n    self.apply_ax_ops(ax_op_after)\n\n    # Return the output of the plotting process\n    return self.output(**annotate_params)\n\n", "CodeBleu_score": 43.757140356381655}, {"code": "def plot_and_annotate_facets(\n            self, plot: str, plot_params: dict, configuration: dict,\n            annotation_func: str, *args, annotation_params: dict = None,\n            ax_op_before: List[Union[str, Optional[list],\n                                     Optional[dict]]] = None,\n            ax_op_after: List[Union[str, Optional[list],\n                                    Optional[dict]]] = None,\n            annotate_params: dict = None, **kwargs)\n\t\n    # Initialize parameters\n    annotation_params = empty_dict_if_none(annotation_params)\n    annotate_params = empty_dict_if_none(annotate_params)\n    ax_op_before = empty_dict_if_none(ax_op_before)\n    ax_op_after = empty_dict_if_none(ax_op_after)\n\n    # Create plot\n    ax = getattr(self, plot)(**plot_params)\n\n    # Apply pre-plot axis operations\n    for op in ax_op_before:\n        if isinstance(op, str):\n            getattr(ax, op)()\n        elif isinstance(op, list):\n            getattr(ax, op[0])(*op[1:])\n        elif isinstance(op, dict):\n            getattr(ax, op.popitem()[0])(**op)\n\n    # Configure and annotate plot\n    if configuration is not None:\n        self.configure(**configuration)\n    if annotation_func is not None:\n        getattr(self, annotation_func)(**annotation_params)\n        getattr(self, annotation_func)(**annotate_params)\n\n    # Apply post-plot axis operations\n    for op in ax_op_after:\n        if isinstance(op, str):\n            getattr(ax, op)()\n        elif isinstance(op, list):\n            getattr(ax, op[0])(*op[1:])\n        elif isinstance(op, dict):\n            getattr(ax, op.popitem()[0])(**op)\n\n    return ax\n\n", "CodeBleu_score": 46.98980141570712}]}
{"instance_id": "b5b17d312c74", "generation_result": [{"code": "def predict_proba(self, X)\n\t\n    if not hasattr(self, '_program'):\n        raise NotFittedError('Estimator not fitted. Call `fit` before'\n                             ' calling `predict_proba`.')\n    X = check_array(X)\n    if X.shape[1] != self.n_features_in_:\n        raise ValueError(f'X has {X.shape[1]} features per sample; '\n                         f'expected {self.n_features_in_}')\n    scores = self._program.execute(X)\n    scores = scores.reshape(-1, 1)\n    scores = np.hstack([1 - scores, scores])\n    return scores\n\n", "CodeBleu_score": 47.55616331047342}, {"code": "def predict_proba(self, X)\n\t\n    if not hasattr(self, \"_program\"):\n        raise NotFittedError(\"This %(name)s instance is not fitted yet\" % {\"name\": type(self).__name__})\n    X = check_array(X, accept_sparse=True)\n    if X.shape[1] != self._n_features:\n        raise ValueError(\"X.shape[1] should be %d, not %d.\" % (self._n_features, X.shape[1]))\n    scores = self._program.execute(X)\n    scores = np.asarray(scores)\n    scores -= scores.max(axis=1)[:, np.newaxis]\n    scores = np.exp(scores)\n    scores /= scores.sum(axis=1)[:, np.newaxis]\n    scores = np.hstack((1 - scores, scores))\n    return scores\n\n", "CodeBleu_score": 45.76280511827064}, {"code": "def predict_proba(self, X)\n\t\n    if not hasattr(self, '_program'):\n        raise NotFittedError(\n            'This %(name)s instance is not fitted yet. Call '\n            '`fit` before calling `predict_proba`.' % {'name': type(self).__name__})\n\n    X = check_array(X, accept_sparse='csr')\n    n_features = X.shape[1]\n    if n_features != self._n_features:\n        raise ValueError(\"X has %d features per sample; expecting %d\"\n                         % (n_features, self._n_features))\n\n    scores = self._program(X)\n    proba = scores / scores.sum(axis=1)[:, np.newaxis]\n    return np.column_stack([1 - proba, proba])\n\n", "CodeBleu_score": 49.39235868705008}]}
{"instance_id": "83f71c959df7", "generation_result": [{"code": "def point_mutation(self, random_state)\n\t\n    program_copy = copy(self)\n    mutation_probability = 0.1\n    mutation_nodes = sample_without_replacement(\n        self.n_nodes,\n        random_state.randint(1, self.n_nodes),\n        random_state=random_state,\n    )\n    mutation_nodes = [\n        node for node in mutation_nodes if random_state.uniform() < mutation_probability\n    ]\n    for node in mutation_nodes:\n        if self[node].arity == 0:\n            if random_state.uniform() < 0.5:\n                program_copy[node] = random_state.choice(self.terminals[\"var\"])\n            else:\n                program_copy[node] = random_state.choice(self.terminals[\"const\"])\n        else:\n            valid_functions = [\n                func\n                for func in self.functions\n                if self.functions[func].arity == self[node].arity\n            ]\n            program_copy[node] = random_state.choice(valid_functions)\n    return program_copy, mutation_nodes\n\n", "CodeBleu_score": 32.03618112328906}, {"code": "def point_mutation(self, random_state)\n\t\n    program = copy(self)\n    n_nodes = len(program.nodes)\n    mutation_nodes = sample_without_replacement(\n        n_nodes, self.point_mutation_prob * n_nodes, random_state=random_state\n    )\n    for i in mutation_nodes:\n        node = program.nodes[i]\n        if node.arity == 0:\n            if random_state.rand() < 0.5:\n                node.value = random_state.randint(self.constant_range[0], self.constant_range[1])\n            else:\n                node.value = random_state.randint(0, self.n_features)\n        else:\n            node.value = random_state.choice(self.functions[node.arity])\n    return program, mutation_nodes", "CodeBleu_score": 22.141840975158527}, {"code": "def point_mutation(self, random_state)\n\t\n    mutated_program = copy(self)\n    modified_nodes = []\n    for i, node in enumerate(mutated_program.nodes):\n        if random_state.uniform() < self.mutation_probability:\n            if node.arity > 0:\n                valid_functions = [\n                    f for f in self.function_set if f.arity == node.arity]\n                mutated_program.nodes[i] = random_state.choice(valid_functions)\n                modified_nodes.append(i)\n            else:\n                if random_state.uniform() < 0.5:\n                    mutated_program.nodes[i] = random_state.choice(\n                        self.terminal_set)\n                else:\n                    mutated_program.nodes[i] = random_state.randint(\n                        -self.constant_range, self.constant_range)\n                modified_nodes.append(i)\n    return mutated_program, modified_nodes\n\n", "CodeBleu_score": 25.38867528613379}]}
{"instance_id": "f1853251e671", "generation_result": [{"code": "def bias(close, length=None, mamode=None, offset=None, **kwargs)\n\t\n    # Validate Arguments\n    length = int(length) if length and length > 0 else 26\n    mamode = mamode.lower() if mamode and isinstance(mamode, str) else \"sma\"\n    close = verify_series(close, length)\n    offset = get_offset(offset)\n\n    if close is None: return\n\n    # Calculate Result\n    ma_ = ma(mamode, close, length=length, **kwargs)\n    bias = (close / ma_) - 1\n\n    # Offset\n    if offset != 0:\n        bias = bias.shift(offset)\n\n    # Handle fills\n    if \"fillna\" in kwargs:\n        bias.fillna(kwargs[\"fillna\"], inplace=True)\n    if \"fill_method\" in kwargs:\n        bias.fillna(method=kwargs[\"fill_method\"], inplace=True)\n\n    # Name and Categorize it\n    bias.name = f\"BIAS_{length}\"\n    bias.category = \"momentum\"\n\n    return bias", "CodeBleu_score": 63.07905649757812}, {"code": "def bias(close, length=None, mamode=None, offset=None, **kwargs)\n\t\n    # Validate arguments\n    length = int(length) if length and length > 0 else 26\n    mamode = mamode if isinstance(mamode, str) else \"sma\"\n    close = verify_series(close, length)\n    offset = get_offset(offset)\n\n    if close is None: return\n\n    # Calculate Result\n    ma_ = ma(mamode, close, length=length, **kwargs)\n    bias = (close / ma_) - 1\n\n    # Offset\n    if offset != 0:\n        bias = bias.shift(offset)\n\n    # Handle fills\n    if \"fillna\" in kwargs:\n        bias.fillna(kwargs[\"fillna\"], inplace=True)\n    if \"fill_method\" in kwargs:\n        bias.fillna(method=kwargs[\"fill_method\"], inplace=True)\n\n    # Name and Categorize it\n    bias.name = f\"BIAS_{length}\"\n    bias.category = \"momentum\"\n\n    return bias", "CodeBleu_score": 64.47426858746736}, {"code": "def bias(close, length=None, mamode=None, offset=None, **kwargs)\n\t\n    # Validate Arguments\n    length = int(length) if length and length > 0 else 26\n    mamode = mamode if isinstance(mamode, str) else \"sma\"\n    close = verify_series(close, length)\n    offset = get_offset(offset)\n\n    if close is None: return\n\n    # Calculate Result\n    bias = (close - ma(mamode, close, length=length, **kwargs)) / ma(mamode, close, length=length, **kwargs)\n\n    # Offset\n    if offset != 0:\n        bias = bias.shift(offset)\n\n    # Handle fills\n    if \"fillna\" in kwargs:\n        bias.fillna(kwargs[\"fillna\"], inplace=True)\n    if \"fill_method\" in kwargs:\n        bias.fillna(method=kwargs[\"fill_method\"], inplace=True)\n\n    # Name and Categorize it\n    bias.name = f\"BIAS_{length}\"\n    bias.category = \"momentum\"\n\n    return bias", "CodeBleu_score": 61.62735405438377}]}
{"instance_id": "77825be37c24", "generation_result": [{"code": "def coins_bar(broker='hb', code='btc', ktype='D', size='2000')\n\t\n    try:\n        if broker == 'hb':\n            url = 'https://api.huobi.pro/market/history/kline?period=' + ktype + '&size=' + size + '&symbol=' + code.upper() + 'usdt'\n            req = requests.get(url)\n            data = req.json()\n            df = pd.DataFrame(data['data'])\n            df.columns = ['date', 'open', 'high', 'low', 'close', 'volume', 'amount', 'count', 'bav', 'bav2', 'adl', 'adl_amount']\n            df['date'] = df['date'].apply(lambda x: time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(x / 1000)))\n            df['date'] = pd.to_datetime(df['date'])\n            return df\n        elif broker == 'bh':\n            url = 'https://api.binance.com/api/v3/klines?symbol=' + code.upper() + 'USDT&interval=' + ktype + '&limit=' + size\n            req = requests.get(url)\n            data = req.json()\n            df = pd.DataFrame(data)\n            df.columns = ['date', 'open', 'high', 'low', 'close', 'volume', 'date2', 'volume2', 'count', 'bav', 'bav2', 'ignore']\n            df['date'] = df['date'].apply(lambda x: time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(x / 1000)))\n            df['date'] = pd.to_datetime(df['date'])\n            return df\n        elif broker == 'okx':\n            url = 'https://www.okx.com/api/v5/market/candles?instId=' + code.upper() + '-USDT&bar=' + ktype + '&limit=' + size\n            req = requests.get(url)\n            data = req.json()\n           ", "CodeBleu_score": 37.71634452385531}, {"code": "def coins_bar(broker='hb', code='btc', ktype='D', size='2000')\n\t\n    try:\n        if broker == 'hb':\n            url = 'https://api.huobi.pro/market/history/kline?symbol=' + \\\n                code.upper() + 'usdt&period=' + ktype + '&size=' + size\n            data = json.loads(requests.get(url).text)\n            df = pd.DataFrame(data['data'])\n            df.columns = ['date', 'open', 'high', 'low', 'close',\n                          'volume', 'amount', 'count', 'BCH', 'BSV', 'BTC', 'DASH', 'EOS', 'ETC', 'ETH', 'LTC', 'QTUM', 'TRX', 'USDT']\n            df['date'] = df['date'].apply(lambda x: time.strftime(\n                '%Y-%m-%d %H:%M:%S', time.localtime(x/1000)))\n            df['date'] = pd.to_datetime(df['date'])\n            return df\n        elif broker == 'bh':\n            url = 'https://api.binance.com/api/v1/klines?symbol=' + \\\n                code.upper() + 'USDT&interval=' + ktype\n            data = json.loads(requests.get(url).text)\n            df = pd.DataFrame(data)\n            df.columns = ['date', 'open', 'high', 'low', 'close', 'volume']\n            df['date'] = df['date'].apply(lambda x: time.strftime(\n                '%Y-%m-%d %H:%M:%S', time.localtime(x/1000)))\n            df['date'] = pd.to_datetime(df['date'])\n            return df\n    except Exception as e:\n        traceback.print_exc()", "CodeBleu_score": 39.20384767591653}, {"code": "def coins_bar(broker='hb', code='btc', ktype='D', size='2000')\n\t\n    try:\n        if broker == 'hb':\n            url = 'https://api.huobi.pro/market/history/kline?period={}&size={}&symbol={}usdt'.format(ktype, size, code)\n            data = json.loads(requests.get(url).text)['data']\n            df = pd.DataFrame(data, columns=['date', 'open', 'high', 'low', 'close', 'volume', 'amount', 'count', 'bid', 'ask', 'bidSz', 'askSz'])\n            df['date'] = df['date'].apply(lambda x: time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(x / 1000)))\n            df['date'] = pd.to_datetime(df['date'])\n            return df\n        elif broker == 'b':\n            url = 'https://api.binance.com/api/v1/klines?symbol={}USDT&interval={}&limit={}'.format(code.upper(), ktype, size)\n            data = json.loads(requests.get(url).text)\n            df = pd.DataFrame(data, columns=['date', 'open', 'high', 'low', 'close', 'volume', 'date2', 'volume2', 'count', 'bid', 'ask', 'bidSz', 'askSz'])\n            df['date'] = df['date'].apply(lambda x: time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(x / 1000)))\n            df['date'] = pd.to_datetime(df['date'])\n            return df\n        elif broker == 'o':\n            url = 'https://www.okex.com/api/spot/v3/instruments/{}/{}-USDT/candles?granularity={}&limit={}'.format(code.upper(), code.upper(), ktype, size)\n            data = json.loads(requests.get(url).text)\n            df = pd.DataFrame(data", "CodeBleu_score": 39.34449509433655}]}
{"instance_id": "c6552851da8a", "generation_result": [{"code": "def coins_snapshot(broker='hb', code='btc', size='5')\n\t\n    try:\n        # Fetch the data using the helper function\n        data = fetch_snapshot(broker, code, size)\n\n        # Process the timestamp based on the broker\n        if broker == 'hb':\n            timestamp = data['timestamp']\n        elif broker == 'okx':\n            timestamp = data['ts']\n        else:\n            raise ValueError(f\"Unsupported broker: {broker}\")\n\n        # Convert the ask and bid data into dataframes\n        ask_df = pd.DataFrame(data['asks'], columns=['price', 'size'])\n        bid_df = pd.DataFrame(data['bids'], columns=['price', 'size'])\n\n        # Add the timestamp to each dataframe\n        ask_df['timestamp'] = timestamp\n        bid_df['timestamp'] = timestamp\n\n        # Store the dataframes in a dictionary\n        data = {'ask': ask_df, 'bid': bid_df}\n\n        # Create and return a pandas Panel containing the ask and bid data\n        return pd.Panel(data)\n    except Exception as e:\n        # Print the traceback if an exception occurs\n        traceback.print_exc()\n\n", "CodeBleu_score": 40.34304647900192}, {"code": "def coins_snapshot(broker='hb', code='btc', size='5')\n\t\n    try:\n        if broker == 'hb':\n            data = get_data_hb(code, size)\n            df_ask = pd.DataFrame(data['asks'])\n            df_ask['timestamp'] = pd.to_datetime(data['timestamp'], unit='s')\n            df_bid = pd.DataFrame(data['bids'])\n            df_bid['timestamp'] = pd.to_datetime(data['timestamp'], unit='s')\n            data = {'ask': df_ask, 'bid': df_bid}\n            df = pd.Panel(data)\n            return df\n        elif broker == 'okex':\n            data = get_data_okex(code, size)\n            df_ask = pd.DataFrame(data['asks'])\n            df_ask['timestamp'] = pd.to_datetime(data['timestamp'], unit='s')\n            df_bid = pd.DataFrame(data['bids'])\n            df_bid['timestamp'] = pd.to_datetime(data['timestamp'], unit='s')\n            data = {'ask': df_ask, 'bid': df_bid}\n            df = pd.Panel(data)\n            return df\n        elif broker == 'binance':\n            data = get_data_binance(code, size)\n            df_ask = pd.DataFrame(data['asks'])\n            df_ask['timestamp'] = pd.to_datetime(data['timestamp'], unit='s')\n            df_bid = pd.DataFrame(data['bids'])\n            df_bid['timestamp'] = pd.to_datetime(data['timestamp'], unit='s')\n            data = {'ask': df_ask, 'bid': df_bid}\n            df = pd.Panel(data)\n            return df\n        else:\n            print('The broker is not supported.')\n    except:\n        traceback.print_exc()\n\n", "CodeBleu_score": 33.45301322665266}, {"code": "def coins_snapshot(broker='hb', code='btc', size='5')\n\t\n    try:\n        if broker == 'hb':\n            data = get_snapshot(code, size)\n            df_ask = pd.DataFrame(data['asks'])\n            df_ask.columns = ['price', 'size']\n            df_bid = pd.DataFrame(data['bids'])\n            df_bid.columns = ['price', 'size']\n            df_ask['timestamp'] = pd.to_datetime(data['timestamp'], unit='s')\n            df_bid['timestamp'] = pd.to_datetime(data['timestamp'], unit='s')\n            panel = pd.Panel({'ask': df_ask, 'bid': df_bid})\n            return panel\n        elif broker == 'cb':\n            data = get_snapshot(code, size)\n            df_ask = pd.DataFrame(data['asks'])\n            df_ask.columns = ['price', 'size', 'num_orders']\n            df_bid = pd.DataFrame(data['bids'])\n            df_bid.columns = ['price', 'size', 'num_orders']\n            df_ask['timestamp'] = pd.to_datetime(data['timestamp'], unit='s')\n            df_bid['timestamp'] = pd.to_datetime(data['timestamp'], unit='s')\n            panel = pd.Panel({'ask': df_ask, 'bid': df_bid})\n            return panel\n        elif broker == 'bfx':\n            data = get_snapshot(code, size)\n            df_ask = pd.DataFrame(data['asks'])\n            df_ask.columns = ['price', 'size', 'order_id']\n            df_bid = pd.DataFrame(data['bids'])\n            df_bid.columns = ['price', 'size', 'order_id']\n            df_ask['timestamp'] = pd.to_datetime(data['timestamp'], unit='ms')\n            df_bid['timestamp'] = pd.to_datetime(data['timestamp'], unit='ms')\n            panel = pd.Panel({'ask': df_ask, 'bid': df_bid})\n            return panel\n        else:\n            print('Invalid", "CodeBleu_score": 31.76301739755535}]}
{"instance_id": "8b5fee723398", "generation_result": [{"code": "def coins_trade(broker='hb', code='btc')\n\t\n    # \u83b7\u53d6\u6570\u636e\n    if broker == 'hb':\n        url = 'https://api.huobi.pro/market/history/trade?symbol=' + code.upper() + 'usdt&size=2000'\n    elif broker == 'ok':\n        url = 'https://www.okx.com/api/v5/market/history-trades?instId=' + code.upper() + '-USDT&limit=2000'\n    elif broker == 'chbtc':\n        url = 'https://api.chbtc.com/data/v1/trades?currency=' + code.lower() + '&since=0'\n    else:\n        raise ValueError('Unknown broker')\n    data = get_data(url)\n    if data is None:\n        return None\n    # \u5904\u7406\u6570\u636e\n    if broker == 'hb':\n        data = data[['id', 'ts', 'p', 'n', 'd']]\n        data.columns = ['tid', 'datetime', 'price', 'amount', 'type']\n        data['datetime'] = data['datetime'].apply(lambda x: time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(x / 1000)))\n    elif broker == 'ok':\n        data = data['data']\n        data = data[['trade-id', 'ts', 'px', 'sz', 'side']]\n        data.columns = ['tid', 'datetime', 'price', 'amount', 'type']\n        data['datetime'] = data['datetime'].apply(lambda x: time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(x / 1000)))\n        data['type'] = data['type'].apply(lambda x: 'buy' if x == 'bid' else 'sell')\n    elif broker == 'chbtc':\n        data = data['models']\n        data = data[['id', 'date', 'price', 'amount', 'type']]\n        data.columns = ['tid', 'datetime', 'price', 'amount',", "CodeBleu_score": 41.46492165893783}, {"code": "def coins_trade(broker='hb', code='btc')\n\t\n    try:\n        if broker == 'hb':\n            url = f'https://api.huobi.pro/market/history/trade?symbol={code}usdt&size=2000'\n        elif broker == 'ok':\n            url = f'https://www.okx.com/api/v5/market/history-trades?instId={code}-USDT'\n        elif broker == 'chbtc':\n            url = f'https://api.chbtc.com/data/v1/trades?currency={code}&since=0'\n        else:\n            print('Wrong broker')\n            return None\n\n        data = pd.read_json(url)\n\n        if data is not None:\n            if broker == 'hb':\n                data = data.iloc[:, 0]\n                data = data.apply(pd.Series)\n                data = data.rename(columns={'id': 'tid', 'ts': 'datetime', 'data': 'data_list'})\n                data['datetime'] = data['datetime'].apply(lambda x: time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(x/1000)))\n                data['data_list'] = data['data_list'].apply(lambda x: json.loads(x))\n                data = data['data_list'].apply(pd.Series)\n                data = data.rename(columns={'amount': 'amount', 'id': 'tid', 'price': 'price', 'direction': 'type'})\n                data['type'] = data['type'].apply(lambda x: 'buy' if x == 'buy' else 'sell')\n                data = data[['tid', 'datetime', 'price', 'amount', 'type']]\n                return data\n\n            elif broker == 'ok':\n                data = data['data']\n                data = data.apply(pd.Series)\n                data = data.rename(columns={'instId': 'tid', 'ts': 'datetime', 'tradeId': 'tid', 'px': 'price', 'sz': 'amount', 'side", "CodeBleu_score": 39.95240595845894}, {"code": "def coins_trade(broker='hb', code='btc')\n\t\n    try:\n        if broker == 'hb':\n            url = f'https://api.hbdm.com/market/history/trade?symbol={code.upper()}usdt&size=200'\n        elif broker == 'ok':\n            url = f'https://www.okx.com/api/v5/market/history-trades?instId={code.upper()}-USDT&limit=200'\n        elif broker == 'chbtc':\n            url = f'https://api.chbtc.com/data/v1/trades?currency={code.lower()}&since=0&limit=200'\n        else:\n            raise ValueError('Invalid broker')\n\n        data = get_data(url)\n\n        if data is not None:\n            if broker == 'hb':\n                data = data[['id', 'ts', 'price', 'amount', 'direction']]\n                data['ts'] = data['ts'].apply(lambda x: int(x / 1000))\n                data['ts'] = data['ts'].apply(lambda x: time.strftime(\n                    '%Y-%m-%d %H:%M:%S', time.localtime(x)))\n                data.columns = ['tid', 'datetime', 'price', 'amount', 'type']\n            elif broker == 'ok':\n                data = data['data']\n                data = data[['trade-id', 'ts', 'px', 'sz', 'side']]\n                data['ts'] = data['ts'].apply(lambda x: time.strftime(\n                    '%Y-%m-%d %H:%M:%S', time.localtime(x / 1000)))\n                data.columns = ['tid', 'datetime', 'price', 'amount', 'type']\n            elif broker == 'chbtc':\n                data = data['data']\n                data = data[['tid', 'date', 'price', 'amount', 'type']]\n                data['date'] = data['date'].apply(lambda x: time.strftime(\n                    '%Y-%m-%d %", "CodeBleu_score": 45.537006744710716}]}
{"instance_id": "25e0a134c45b", "generation_result": [{"code": "def _parse_fund_data(url, fund_type='open')\n\t\n    print(ct.FUND_DATA_URL)\n    print(url)\n    try:\n        text = du._requests_get(url, headers=ct.headers, timeout=30)\n        text = text.decode('utf-8')\n        text = re.findall(r'\\[.*\\]', text)[0]\n        text = text.replace('datas', '\"datas\"')\n        text = text.replace('diff', '\"diff\"')\n        text = text.replace('volumn', '\"volumn\"')\n        text = text.replace('premium', '\"premium\"')\n        text = text.replace('amount', '\"amount\"')\n        text = text.replace('volume', '\"volume\"')\n        text = text.replace('ticker', '\"ticker\"')\n        text = text.replace('secid', '\"secid\"')\n        text = text.replace('name', '\"name\"')\n        text = text.replace('ipodate', '\"ipodate\"')\n        text = text.replace('listdate', '\"listdate\"')\n        text = text.replace('delistdate', '\"delistdate\"')\n        text = text.replace('nav', '\"nav\"')\n        text = text.replace('bshare', '\"bshare\"')\n        text = text.replace('turnoverratio', '\"turnoverratio\"')\n        text = text.replace('amountincm', '\"amountincm\"')\n        text = text.replace('incmpercent', '\"incmpercent\"')\n        text = text.replace('amountincmshares', '\"amountincmshares\"')\n        text = text.replace('amountoutstd', '\"amountoutstd\"')\n        text = text.replace('stdratio', '\"stdratio\"')\n        text = text.replace('sharesafter', '\"sharesafter\"')\n        text = text.replace('fundcode', '\"fundcode\"')\n        text = text.replace('dividentyield', '\"dividentyield\"')\n        text = text.replace('dividentyieldrank', '\"dividentyieldrank\"')\n        text = text.replace('dividentyieldrank5', '\"dividentyieldrank5\"')\n        text =", "CodeBleu_score": 33.585066618175695}, {"code": "def _parse_fund_data(url, fund_type='open')\n\t\n    import pandas as pd\n    import requests\n    import json\n    import re\n    import datetime\n\n    # print(url)\n    try:\n        r = requests.get(url)\n        r.encoding = 'utf-8'\n        text_data = r.text\n        json_data = json.loads(text_data[text_data.index('{'):-1])\n        if fund_type == 'open':\n            columns = ct.FUND_OPEN_FIELDS\n        else:\n            columns = ct.FUND_CLOSE_FIELDS\n        df = pd.DataFrame(json_data['Data']['LSJZList'], columns=columns)\n        df['date'] = df['FSRQ']\n        df['date'] = df['date'].apply(lambda x: datetime.datetime.strptime(x, '%Y-%m-%d'))\n        df['date'] = df['date'].apply(lambda x: x.strftime('%Y%m%d'))\n        df['date'] = df['date'].astype(int)\n        df['jjjz'] = df['DWJZ'].apply(lambda x: float(x) if x != '' else 0)\n        df['ljjz'] = df['LJJZ'].apply(lambda x: float(x) if x != '' else 0)\n        df['jjjzrq'] = df['JZZZL'].apply(lambda x: float(x) if x != '' else 0)\n        df['jjl'] = df['JJL'].apply(lambda x: float(x) if x != '' else 0)\n        df['close'] = df['LJJZ'].apply(lambda x: float(x) if x != '' else 0)\n        df = df[['date', 'jjjz', 'ljjz', 'jjjzrq', 'jjl', 'close']]\n        df.fillna(0, inplace=True)\n        return df\n    except Exception as e:\n        print(e)\n\n", "CodeBleu_score": 38.28970573357579}, {"code": "def _parse_fund_data(url, fund_type='open')\n\t\n    print(url)\n    try:\n        r = ct.HTTP_CLIENT.request('GET', url, headers=ct.HEADERS, timeout=ct.TIMEOUT)\n        r.encoding = 'utf8'\n        text_data = r.text\n        text_data = re.findall('\\((.*)\\)', text_data)[0]\n        json_data = json.loads(text_data)\n        if fund_type == 'open':\n            columns = ct.FUND_OPEN_FIELDS\n        else:\n            columns = ct.FUND_CLOSE_FIELDS\n        df = pd.DataFrame(json_data['Data'])[columns]\n        df.fillna(0.0, inplace=True)\n        return df\n    except Exception as e:\n        print(e)\n\n", "CodeBleu_score": 29.773897679789595}]}
{"instance_id": "401d18e122d8", "generation_result": [{"code": "def get_future_daily(start = None, end = None, market = 'CFFEX')\n\t\n    if market == 'CFFEX':\n        func = ct.get_cffex_daily\n    elif market == 'CZCE':\n        func = ct.get_czce_daily\n    elif market == 'DCE':\n        func = ct.get_dce_daily\n    elif market == 'SHFE':\n        func = ct.get_shfe_daily\n    else:\n        raise ValueError('market should be one of \"CFFEX\", \"CZCE\", \"DCE\", \"SHFE\"')\n\n    if start is None:\n        start = datetime.date.today()\n    else:\n        start = datetime.datetime.strptime(start, '%Y%m%d').date()\n\n    if end is None:\n        end = datetime.date.today()\n    else:\n        end = datetime.datetime.strptime(end, '%Y%m%d').date()\n\n    dfs = []\n    for date in (start + datetime.timedelta(n) for n in range((end - start).days + 1)):\n        date_str = date.strftime('%Y%m%d')\n        df = func(date_str)\n        if df is not None and not df.empty:\n            dfs.append(df)\n\n    if dfs:\n        return pd.concat(dfs)\n    else:\n        return None\n\n", "CodeBleu_score": 44.7084973906842}, {"code": "def get_future_daily(start = None, end = None, market = 'CFFEX')\n\t\n    if market == 'CFFEX':\n        func = ct.get_cffex_daily\n    elif market == 'CZCE':\n        func = ct.get_czce_daily\n    elif market == 'DCE':\n        func = ct.get_dce_daily\n    elif market == 'SHFE':\n        func = ct.get_shfe_daily\n    else:\n        return None\n\n    if start is None:\n        start = datetime.datetime.now().strftime('%Y%m%d')\n    else:\n        start = start.strftime('%Y%m%d')\n\n    if end is None:\n        end = datetime.datetime.now().strftime('%Y%m%d')\n    else:\n        end = end.strftime('%Y%m%d')\n\n    data = []\n    for date in pd.date_range(start, end):\n        date = date.strftime('%Y%m%d')\n        df = func(date)\n        if df is not None:\n            data.append(df)\n\n    if len(data) == 0:\n        return None\n    else:\n        return pd.concat(data)\n\n", "CodeBleu_score": 45.45256302316831}, {"code": "def get_future_daily(start = None, end = None, market = 'CFFEX')\n\t\n    if market == 'CFFEX':\n        func = ct.get_cffex_daily\n    elif market == 'CZCE':\n        func = ct.get_czce_daily\n    elif market == 'DCE':\n        func = ct.get_dce_daily\n    elif market == 'SHFE':\n        func = ct.get_shfe_daily\n    else:\n        raise Exception('market is not support')\n\n    if start is None:\n        start = datetime.datetime.now().strftime('%Y%m%d')\n    else:\n        start = start.strftime('%Y%m%d')\n\n    if end is None:\n        end = datetime.datetime.now().strftime('%Y%m%d')\n    else:\n        end = end.strftime('%Y%m%d')\n\n    dfs = []\n    for date in pd.date_range(start, end):\n        date = date.strftime('%Y%m%d')\n        df = func(date)\n        if df is not None and len(df) > 0:\n            dfs.append(df)\n\n    if len(dfs) > 0:\n        df = pd.concat(dfs)\n        return df\n    else:\n        return None\n\n", "CodeBleu_score": 43.97101765786699}]}
{"instance_id": "697779825988", "generation_result": [{"code": "def realtime_boxoffice(retry_count=3,pause=0.001)\n\t\n    for _ in range(retry_count):\n        try:\n            response = du.getStr('http://www.endata.com.cn/API/GetData.ashx',\n                                 params={'type': 'get_boxoffice_realtime',\n                                         'sts': int(time.time())})\n            if response:\n                data = json.loads(response)\n                df = pd.DataFrame(data['Data']['Table'])\n                df.drop(['MovieImg', 'mId'], axis=1, inplace=True)\n                df['timestamp'] = time.time()\n                return df\n        except:\n            time.sleep(pause)\n\n", "CodeBleu_score": 35.60001550653535}, {"code": "def realtime_boxoffice(retry_count=3,pause=0.001)\n\t\n    for _ in range(retry_count):\n        try:\n            df = du.get_request_stock(ct.STK_BOXOFFICE_REALTIME,\n                                      params={'src': 'web'})\n            if df is None:\n                return\n            df = df.to_json()\n            df = json.loads(df)\n            df = df['data']['data2']\n            df = pd.DataFrame(df)\n            df.drop(['MovieImg', 'mId'], axis=1, inplace=True)\n            df['time'] = time.strftime('%Y-%m-%d', time.localtime())\n            return df\n        except:\n            time.sleep(pause)\n\n", "CodeBleu_score": 35.82143923028192}, {"code": "def realtime_boxoffice(retry_count=3,pause=0.001)\n\t\n    for _ in range(retry_count):\n        time.sleep(pause)\n        try:\n            url = ct.BOXOFFICE_URL\n            response = du.get_requests(url)\n            if response.status_code != 200:\n                return None\n            data = json.loads(response.text)\n            if data['data2']:\n                df = du.json2df(data['data2'])\n                df.drop(['MovieImg', 'mId'], axis=1, inplace=True)\n                df['time'] = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(time.time()))\n                return df\n            else:\n                return None\n        except:\n            pass\n    return None\n\n", "CodeBleu_score": 40.04139043219262}]}
