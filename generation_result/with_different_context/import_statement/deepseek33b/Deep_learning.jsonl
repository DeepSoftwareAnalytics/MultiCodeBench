{"instance_id": "1f9f4d85e825", "generation_result": [{"code": "def load_model_from_config(self, half_attention)\n\t\n    model_path = self.model_path\n    if model_path is None:\n        return None\n\n    config = OmegaConf.load(f\"{self.model_path}.yaml\")\n    model = instantiate_from_config(config.model)\n    model.load_state_dict(torch.load(self.model_path, map_location=\"cpu\"), strict=False)\n    model.to(devices.device)\n    model.eval()\n\n    if half_attention:\n        model = model.half()\n\n    if shared.cmd_opts.memory_format == \"channels_last\":\n        model.to(memory_format=torch.channels_last)\n\n    return model\n\n", "CodeBleu_score": 24.787802169150453}, {"code": "def load_model_from_config(self, half_attention)\n\t\n    # Load the model from the specified path\n    model_path = self.model_path\n    config_path = os.path.join(self.model_path, \"config.yaml\")\n    config = OmegaConf.load(f\"{config_path}\")\n    model = instantiate_from_config(config.model)\n\n    # Load the state dict from the specified path\n    if model_path.endswith(\".safetensors\"):\n        from safetensors.torch import safe_open\n        with safe_open(model_path, framework=\"pt\") as f:\n            weights = f.torch()\n    else:\n        weights = torch.load(model_path)\n\n    # Load the state dict into the model\n    if weights.get(\"global_step\", 0) > 10000:\n        weights = weights[\"state_dict\"]\n    weights = weights.get(\"model\", weights)\n    model.load_state_dict(weights, strict=False)\n    model.eval()\n\n    # Move the model to the appropriate device\n    model.to(devices.device)\n\n    # Optimize the model for memory usage\n    if half_attention:\n        for layer in model.modules():\n            if isinstance(layer, torch.nn.MultiheadAttention):\n                layer.q_proj.to(torch.float16)\n                layer.k_proj.to(torch.float16)\n                layer.v_proj.to(torch.float16)\n                layer.out_proj.to(torch.float16)\n            if isinstance(layer, torch.nn.Linear):\n                layer.to(torch.float16)\n\n    # Ensure the model is in evaluation mode\n    model.eval()\n\n    # Return the model\n    return {\"model\": model}\n\n", "CodeBleu_score": 36.74475175835092}, {"code": "def load_model_from_config(self, half_attention)\n\t\n\n    # Check if a cached model is available and valid\n    cache_name = f\"{self.model_name}-{self.model_hash}.pt\"\n    cache_path = os.path.join(shared.cache_path, cache_name)\n    if os.path.exists(cache_path):\n        if os.path.getmtime(cache_path) > os.path.getmtime(self.model_path):\n            print(f\"Loading cached model: {cache_name}\")\n            return torch.load(cache_path, map_location=\"cpu\")\n\n    # Load the model from the specified path\n    print(f\"Loading model from {self.model_path}\")\n    if self.model_path.endswith(\".safetensors\"):\n        model = safetensors.torch.load_file(self.model_path, device=\"cpu\")\n    else:\n        model = torch.load(self.model_path, map_location=\"cpu\")\n\n    # Configure the model\n    model.eval()\n    if self.half_attention:\n        for name, module in model.named_modules():\n            if isinstance(module, torch.nn.MultiheadAttention):\n                module.q_proj = torch.nn.Linear(module.embed_dim, module.embed_dim, bias=False)\n                module.k_proj = torch.nn.Linear(module.embed_dim, module.embed_dim, bias=False)\n                module.v_proj = torch.nn.Linear(module.embed_dim, module.embed_dim, bias=False)\n                module.out_proj = torch.nn.Linear(module.embed_dim, module.embed_dim, bias=False)\n\n    # Move the model to the appropriate device\n    model.to(devices.device)\n\n    # Cache the model for future use\n    if self.cache_models:\n        print(f\"Caching model: {cache_name}\")\n        torch.save(model, cache_path)\n\n    return model\n\n", "CodeBleu_score": 39.20237536676574}]}
{"instance_id": "8dc955007aa2", "generation_result": [{"code": "def convsample_ddim(model, cond, steps, shape, eta=1.0, callback=None, normals_sequence=None,\n                    mask=None, x0=None, quantize_x0=False, temperature=1., score_corrector=None,\n                    corrector_kwargs=None, x_t=None\n                    )\n\t\n    # make sure model is in eval mode\n    model.eval()\n\n    # initialize the sampler\n    sampler = DDIMSampler(model)\n\n    # extract the batch size from the shape parameter\n    bs = shape[0]\n\n    # print the eta and steps values\n    print(f\"eta: {eta} steps: {steps}\")\n\n    # call the sample method of the DDIMSampler instance\n    return sampler.sample(steps, bs, shape, cond, eta, callback=callback, normals_sequence=normals_sequence,\n                          mask=mask, x0=x0, quantize_x0=quantize_x0, temperature=temperature,\n                          score_corrector=score_corrector, corrector_kwargs=corrector_kwargs, x_t=x_t)\n\n", "CodeBleu_score": 38.73041802147374}, {"code": "def convsample_ddim(model, cond, steps, shape, eta=1.0, callback=None, normals_sequence=None,\n                    mask=None, x0=None, quantize_x0=False, temperature=1., score_corrector=None,\n                    corrector_kwargs=None, x_t=None\n                    )\n\t\n    # make sure model is in eval mode\n    model.eval()\n\n    # initialize the sampler\n    sampler = DDIMSampler(model)\n\n    # extract the batch size from the shape parameter\n    bs = shape[0]\n\n    # print the eta and steps values\n    print(f\"eta: {eta}, steps: {steps}\")\n\n    # call the sample method of the DDIMSampler instance\n    return sampler.sample(steps, bs, shape,\n                          conditioning=cond,\n                          eta=eta,\n                          callback=callback,\n                          normals_sequence=normals_sequence,\n                          mask=mask,\n                          x0=x0,\n                          quantize_x0=quantize_x0,\n                          temperature=temperature,\n                          score_corrector=score_corrector,\n                          corrector_kwargs=corrector_kwargs,\n                          x_T=x_t)\n\n", "CodeBleu_score": 44.058588653749055}, {"code": "def convsample_ddim(model, cond, steps, shape, eta=1.0, callback=None, normals_sequence=None,\n                    mask=None, x0=None, quantize_x0=False, temperature=1., score_corrector=None,\n                    corrector_kwargs=None, x_t=None\n                    )\n\t\n    # make sure model is in eval mode\n    model.eval()\n\n    # initialize the sampler\n    sampler = DDIMSampler(model)\n\n    # extract the batch size from the shape parameter\n    bs = shape[0]\n\n    # print the eta and steps values\n    print(f\"Using eta = {eta} and {steps} steps\")\n\n    # call the sample method of the DDIMSampler instance\n    return sampler.sample(steps, bs, shape, cond, eta, callback=callback, normals_sequence=normals_sequence,\n                          mask=mask, x0=x0, quantize_x0=quantize_x0, temperature=temperature,\n                          score_corrector=score_corrector, corrector_kwargs=corrector_kwargs, x_t=x_t)\n\n", "CodeBleu_score": 39.76073395878781}]}
{"instance_id": "9bdcd9081480", "generation_result": [{"code": "def ema_scope(self, context=None)\n\t\n    if self.ema_scope_level == 0:\n        self.ema_scope_level += 1\n        if context:\n            print(f\"{context}: Applying EMA to model.\")\n        self.model.apply(self.ema_to_ddp if self.ema_to_ddp else self.ema_to_ddp.__wrapped__)\n        self.ema_to_ddp.__wrapped__.__self__.ema_scope_level += 1\n        yield\n        self.ema_to_ddp.__wrapped__.__self__.ema_scope_level -= 1\n        if self.ema_to_ddp.__wrapped__.__self__.ema_scope_level == 0:\n            self.model.apply(self.ddp_to_ema if self.ddp_to_ema else self.ddp_to_ema.__wrapped__)\n            self.ddp_to_ema.__wrapped__.__self__.ema_scope_level -= 1\n        if context:\n            print(f\"{context}: Removed EMA from model.\")\n        self.ema_scope_level -= 1\n    else:\n        yield\n\n", "CodeBleu_score": 34.23367763991186}, {"code": "def ema_scope(self, context=None)\n\t\n    if self.ema_scope_level == 0:\n        if context:\n            print(f\"{context}: Applying EMA to {self.model}\")\n        self.model_ema.store(self.model.state_dict())\n        self.model_ema.copy_to(self.model)\n        self.ema_scope_level += 1\n        yield\n        self.model_ema.restore(self.model.state_dict())\n        self.ema_scope_level -= 1\n    else:\n        yield\n\n", "CodeBleu_score": 44.94979223195494}, {"code": "def ema_scope(self, context=None)\n\t\n    if context:\n        print(f\"{context.__class__.__name__} using EMA weights\")\n    self.model.eval()\n    self.ema_model.train()\n    self.ema.copy_to(self.ema_model.parameters())\n    yield\n    if context:\n        print(f\"{context.__class__.__name__} done using EMA weights\")\n    self.ema_model.eval()\n    self.model.train()\n\n", "CodeBleu_score": 43.45323537703622}]}
{"instance_id": "14ab009e1c5c", "generation_result": [{"code": "def init_from_ckpt(self, path, ignore_keys=None)\n\t\n    sd = torch.load(path, map_location=\"cpu\")[\"state_dict\"]\n    if ignore_keys is not None:\n        for k in list(sd.keys()):\n            for ik in ignore_keys:\n                if k.startswith(ik):\n                    del sd[k]\n    self.load_state_dict(sd, strict=False)\n    print(f\"Loaded state_dict from [{path}]\")\n    return self\n\n", "CodeBleu_score": 27.407469228921872}, {"code": "def init_from_ckpt(self, path, ignore_keys=None)\n\t\n    sd = torch.load(path, map_location=\"cpu\")[\"state_dict\"]\n    if ignore_keys is not None:\n        for k in list(sd.keys()):\n            for ik in ignore_keys:\n                if k.startswith(ik):\n                    del sd[k]\n    self.load_state_dict(sd, strict=False)\n    print(f\"Loaded state_dict from [{path}]\")\n    missing_keys, unexpected_keys = self.load_state_dict(sd, strict=False)\n    if len(missing_keys) > 0:\n        print(f\"WARNING: missing keys:\")\n        print(missing_keys)\n    if len(unexpected_keys) > 0:\n        print(f\"WARNING: unexpected keys:\")\n        print(unexpected_keys)\n\n", "CodeBleu_score": 40.987466659264}, {"code": "def init_from_ckpt(self, path, ignore_keys=None)\n\t\n    sd = torch.load(path, map_location=\"cpu\")[\"state_dict\"]\n    if ignore_keys:\n        sd = {k: v for k, v in sd.items() if k not in ignore_keys}\n    self.load_state_dict(sd)\n    print(f\"Loaded state_dict from [{path}]\")\n\n", "CodeBleu_score": 18.59896337212342}]}
{"instance_id": "754176e27b81", "generation_result": [{"code": "def forward(self, input, return_pred_indices=False)\n\t\n    encoded = self.encoder(input)\n    idx, quantized = self.quantize(encoded)\n    pred = self.decoder(quantized)\n    if return_pred_indices:\n        return pred, idx\n    return pred\n\n", "CodeBleu_score": 26.131474422878277}, {"code": "def forward(self, input, return_pred_indices=False)\n\t\n    encoded = self.encoder(input)\n    idx, quantized = self.quantize(encoded)\n    decoded = self.decoder(quantized)\n    if return_pred_indices:\n        return decoded, idx\n    return decoded\n\n", "CodeBleu_score": 26.340585109358894}, {"code": "def forward(self, input, return_pred_indices=False)\n\t\n    encoded = self.encoder(input)\n    idx, _ = self.quant_conv(encoded)\n    quantized = self.quantize.embed(idx).transpose(1, 3)\n    decoded = self.decoder(quantized)\n    diff = torch.abs(input.add(1).log().sub(decoded.add(1).log()))\n    if return_pred_indices:\n        return decoded, diff, idx\n    else:\n        return decoded, diff\n\n", "CodeBleu_score": 24.41476906930415}]}
{"instance_id": "443df6fdaf0d", "generation_result": [{"code": "def get_input(self, batch, k)\n\t\n    x = batch[k]\n    if x.ndim == 3:\n        x = x[..., None]\n    if x.shape[2] != self.image_size:\n        x = torch.nn.functional.interpolate(x, size=(self.image_size, self.image_size), mode=\"bicubic\", align_corners=False)\n    x = x.permute(0, 2, 3, 1).to(memory_format=torch.contiguous_format).float()\n    return x.detach()\n\n", "CodeBleu_score": 35.8995271829875}, {"code": "def get_input(self, batch, k)\n\t\n    x = batch[k]\n    if len(x.shape) == 3:\n        x = x[..., None]\n    x = x.permute(0, 3, 1, 2).to(memory_format=torch.contiguous_format).float()\n    if self.resize_enabled:\n        training_step_ratio = min(self.training_step / self.resize_steps, 1)\n        resize_target = self.resize_range[0] + (self.resize_range[1] - self.resize_range[0]) * training_step_ratio\n        resize_h = int(resize_target * x.shape[-2])\n        resize_w = int(resize_target * x.shape[-1])\n        x = torch.nn.functional.interpolate(x, size=(resize_h, resize_w), mode=\"bicubic\", align_corners=False)\n    return x.contiguous()\n\n", "CodeBleu_score": 53.275282787851474}, {"code": "def get_input(self, batch, k)\n\t\n    x = batch[k]\n    if x.shape[1] != self.inner_model.img_size:\n        x = torch.nn.functional.interpolate(x, size=(self.inner_model.img_size, self.inner_model.img_size), mode=\"bicubic\", align_corners=False)\n    if x.shape[2] == 1:\n        x = x.repeat(1, 1, 2, 2)\n    x = x.permute(0, 2, 3, 1)\n    x = x.to(memory_format=torch.contiguous_format).float()\n    if self.resize_enabled:\n        # check if we should resize anything, or just use the cached version\n        # size = shape[self.inner_model.img_size]\n        # in_s = torch.randint(self.resize_range[0] * size, self.resize_range[1] * size, (1,)).item()\n        in_s = torch.randint(512, 768, (1,)).item()\n        diff = in_s % 64\n        need_resizing = self.need_resizing\n        self.need_resizing = True\n        if need_resizing:\n            self.resize_mode = \"bicubic\"\n            self.resize_module.clear()\n            self.resize_mode = \"nearest\"\n        self.in_size = (in_s - diff, in_s - diff)\n    if self.in_size is not None:\n        x = torch.nn.functional.interpolate(x, size=self.in_size, mode=self.resize_mode, align_corners=False)\n    x = x.to(memory_format=torch.contiguous_format)\n    return x.detach()\n\n", "CodeBleu_score": 41.36641488232419}]}
{"instance_id": "953a2632fc15", "generation_result": [{"code": "def training_step(self, batch, batch_idx, optimizer_idx)\n\t\n    real = batch\n    do_optimizer_step = self.get_do_optimizer_step(batch_idx)\n\n    if do_optimizer_step:\n        # autoencode\n        if optimizer_idx == 0:\n            result = self.autoencoder(real, return_dict=False)\n            train_loss = self.autoencoder.loss(real, *result)\n\n        # discriminator\n        if optimizer_idx == 1:\n            result = self.autoencoder(real, return_dict=False)\n            train_loss = self.discriminator.loss(real, *result)\n\n        if do_optimizer_step:\n            self.log(\"train/train_loss\", train_loss, prog_bar=True)\n            self.log_dict(\n                {\n                    f\"train/{k.replace('/', '_')}\": v.item()\n                    for k, v in train_loss.items()\n                }\n            )\n\n        return train_loss\n\n", "CodeBleu_score": 29.565545470400995}, {"code": "def training_step(self, batch, batch_idx, optimizer_idx)\n\t\n    real = self.get_input(batch, self.image_key)\n    loss = self.autoencoder(**batch)\n    if optimizer_idx == 0:\n        # autoencoder\n        self.log(\"train/aeloss\", loss, prog_bar=True, logger=True, on_step=True, on_epoch=True)\n        return loss\n\n    if optimizer_idx == 1:\n        # discriminator\n        d_loss = self.discriminator_loss(real, self.sample_from_encoder)\n        self.log(\"train/dloss\", d_loss, prog_bar=True, logger=True, on_step=True, on_epoch=True)\n        return d_loss\n\n", "CodeBleu_score": 30.074168240073217}, {"code": "def training_step(self, batch, batch_idx, optimizer_idx)\n\t\n    x = self.get_input(batch, self.image_key)\n    xrec, qloss = self(x)\n    aeloss, log_dict_ae = self.autoencoder(x, xrec, optimizer_idx, self.global_step,\n                                            last_layer=self.get_last_layer(), split=\"train\")\n\n    if optimizer_idx == 0:\n        # autoencode\n        if self.loss.startswith('vq'):\n            loss = aeloss['loss']\n        else:\n            loss = aeloss\n        logs = {f\"{self.image_key}_loss\": loss}\n        logs.update(log_dict_ae)\n        return loss, logs\n\n    if optimizer_idx == 1:\n        # discriminator\n        dis_loss, dis_logs = self.discriminator(x, xrec, optimizer_idx, self.global_step,\n                                               last_layer=self.get_last_layer(), split=\"train\",\n                                               conditioning=self.conditioning)\n        self.log_dict(dis_logs, prog_bar=True)\n        return dis_loss, dis_logs\n\n", "CodeBleu_score": 49.44035880019679}]}
{"instance_id": "b449b5e8b77f", "generation_result": [{"code": "def log_images(self, batch, only_inputs=False, plot_ema=False, **kwargs)\n\t\n    log = self.log\n    checkpoint = self.checkpoint\n    cfg = self.cfg\n    device = self.device\n    input_images = (batch[\"image\"]).clamp(-1, 1)\n    input_images = self.get_input(input_images, self.image_key)\n    if only_inputs:\n        log[\"reconstructions\"] = input_images\n        return\n    postprocess = cfg.model.postprocess\n    if cfg.model.use_ema:\n        if plot_ema:\n            with self.ema.ema_scope():\n                rec = self.get_recons(input_images)\n                if cfg.model.use_split_inputs:\n                    rec = rec.clamp(-1, 1)\n                rec = postprocess(rec)\n                rec = self.get_image_colorization(rec, input_images)\n                log[\"ema_reconstructions\"] = rec\n        else:\n            rec = self.get_recons(input_images)\n            if cfg.model.use_split_inputs:\n                rec = rec.clamp(-1, 1)\n            rec = postprocess(rec)\n            rec = self.get_image_colorization(rec, input_images)\n            log[\"reconstructions\"] = rec\n    else:\n        rec = self.get_recons(input_images)\n        if cfg.model.use_split_inputs:\n            rec = rec.clamp(-1, 1)\n        rec = postprocess(rec)\n        rec = self.get_image_colorization(rec, input_images)\n        log[\"reconstructions\"] = rec\n\n", "CodeBleu_score": 33.84367570722699}, {"code": "def log_images(self, batch, only_inputs=False, plot_ema=False, **kwargs)\n\t\n    log = dict()\n    input = (batch[0] if isinstance(batch, list) else batch).to(self.device)\n    input = input.permute(0, 3, 1, 2).contiguous()\n    input = input.mul(255).add_(0.5).clamp_(0, 255).to(torch.uint8)\n    if plot_ema:\n        with self.ema_scope():\n            rec = self.ema_model.reconstruct(input, **kwargs)\n            rec = torch.clamp((rec + 1.0) / 2.0, min=0.0, max=1.0)\n            rec = rec.permute(0, 2, 3, 1).cpu().numpy()\n            log[\"ema_reconstructions\"] = self.log_img(rec, name=\"reconstructions_ema\")\n    if only_inputs:\n        return log\n    rec = self.model.reconstruct(input, **kwargs)\n    rec = torch.clamp((rec + 1.0) / 2.0, min=0.0, max=1.0)\n    rec = rec.permute(0, 2, 3, 1).cpu().numpy()\n    log[\"reconstructions\"] = self.log_img(rec, name=\"reconstructions\")\n    return log\n\n", "CodeBleu_score": 33.36553617662694}, {"code": "def log_images(self, batch, only_inputs=False, plot_ema=False, **kwargs)\n\t\n    log = self.log\n    checkpoint = self.checkpoint\n    cfg = self.cfg\n    device = self.device\n    input_images = (batch[\"image\"]).clamp(-1, 1)\n    input_images = self.get_input(input_images, self.image_key)\n    if only_inputs:\n        images = [input_images]\n        names = [\"input_images\"]\n    else:\n        rec = self.ema_ reconstructions if plot_ema else self.reconstructions\n        rec = rec.clamp(-1, 1)\n        if cfg.get(\"sample_fns\", False):\n            # if we have conditional sampler, show the conditional along with the reconstruction\n            ground_truth = self.get_input(batch.get(\"ground_truth_image\"), \"image\")\n            if ground_truth is not None:\n                input_images = torch.cat([ground_truth, input_images], dim=0)\n        images = [input_images, rec]\n        names = [\"input_images\", \"rec\"]\n    for image_type, image in zip(names, images):\n        if self.use_colorization:\n            image = self.lab_transforms[\"train\"](image)\n        if image.shape[-1] == 1:\n            image = image.repeat(1, 1, 3)\n        log[f\"train/{image_type}\"] = [wandb.Image(image_grid(image[:self.num_images], **kwargs))]\n        if plot_ema and image_type == \"rec\":\n            checkpoint.add_ema_shards({f\"reconstructions\": image})\n\n", "CodeBleu_score": 25.501479953701654}]}
{"instance_id": "540d089e3481", "generation_result": [{"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss == 'l1':\n        loss = F.l1_loss(pred, target, reduction='none')\n        if mean:\n            loss = loss.mean((1, 2, 3))\n    elif self.loss == 'l2':\n        loss = F.mse_loss(pred, target, reduction='none')\n        if mean:\n            loss = loss.mean((1, 2, 3))\n    else:\n        raise NotImplementedError()\n    return loss\n\n", "CodeBleu_score": 51.24455336444132}, {"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss == 'l1':\n        loss = F.l1_loss(pred, target, reduction='none')\n        if mean:\n            loss = loss.mean([1, 2, 3], keepdim=True)\n    elif self.loss == 'l2':\n        loss = F.mse_loss(pred, target, reduction='none')\n        if mean:\n            loss = loss.mean([1, 2, 3], keepdim=True)\n    else:\n        raise NotImplementedError()\n    return loss\n\n", "CodeBleu_score": 51.64216151783767}, {"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss == 'l1':\n        loss = F.l1_loss(pred, target, reduction='none')\n    elif self.loss == 'l2':\n        loss = F.mse_loss(pred, target, reduction='none')\n    else:\n        raise NotImplementedError()\n    if mean:\n        loss = loss.mean()\n    return loss\n\n", "CodeBleu_score": 40.188951417781524}]}
{"instance_id": "63a08283666f", "generation_result": [{"code": "def forward(self, x, c, *args, **kwargs)\n\t\n    b, *_, dev = *x.shape, x.device\n    if not self.training:\n        if self.cond_stage_key == 'hybrid_ddim':\n            timesteps = torch.tensor([self.ddim_num_steps - 1] * b, device=dev, dtype=torch.long)\n        else:\n            timesteps = torch.tensor([0] * b, device=dev, dtype=torch.long)\n    else:\n        timesteps = torch.randint(0, self.num_timesteps, (b,), device=dev, dtype=torch.long)\n        if self.training and self.use_ddim:\n            timesteps = repeat(timesteps, 'b -> b 1', 1)\n    time_embed = self.time_embed(timesteps)\n    if self.training and self.use_ddim:\n        timesteps = timesteps.view(b, -1)[:, 0]\n        time_embed = time_embed.view(b, -1)[:, 0]\n    if self.cond_stage_key == 'hybrid_ddim':\n        x, c = self.hybrid_model.forward(x, c, timesteps, **kwargs)\n    else:\n        x, c = self.model.forward(x, c, timesteps, **kwargs)\n    return x, c\n\n", "CodeBleu_score": 40.44580528953603}, {"code": "def forward(self, x, c, *args, **kwargs)\n\t\n    b, *_, device = *x.shape, x.device\n    t = torch.randint(0, self.num_timesteps, (b,), device=device).long()\n    if self.cond_stage_model is not None:\n        c = self.cond_stage_model(c)\n    if self.use_timesteps:\n        x, t = self.time_embed(x, t)\n    if self.use_embedding:\n        c = self.embedding(c)\n    x = self.model(x, c, t, *args, **kwargs)\n    if self.cond_stage_model is not None:\n        x = self.cond_stage_model.output_scale * x\n    if self.use_scale_shift_norm:\n        x = self.output_scale * x\n    return x\n\n", "CodeBleu_score": 49.856640590768265}, {"code": "def forward(self, x, c, *args, **kwargs)\n\t\n    b, c, *spatial = x.shape\n    x = x.to(memory_format=torch.contiguous_format)\n    if self.use_labels:\n        assert c == self.num_classes\n        if x.dtype == torch.long and not isinstance(self, VQModelInterface):\n            x = F.one_hot(x, self.num_classes).type(torch.float32)\n        x = x * (1 - self.label_emb_dropout)\n    if self.conditioning_key is not None:\n        c = self.cond_stage_model(c)\n        if self.conditioning_key == \"hybrid\":\n            x = torch.cat([x, c], dim=1)\n        else:\n            assert self.conditioning_key == \"concat\"\n    if self.use_scale_shift_norm:\n        x = self.norm(x)\n    t = torch.empty([b], dtype=torch.long, device=x.device).random_(self.num_timesteps)\n    if self.use_timesteps:\n        x, t = self.time_embed(x, t)\n    return self.apply_model(x, t, c)\n\n", "CodeBleu_score": 41.68419572398028}]}
{"instance_id": "4627e1e8ed62", "generation_result": [{"code": "def get_codebook_entry(self, indices, shape)\n\t\n    if self.remap_index is not None:\n        indices = self.remap_index(indices)\n    # get quantized latent vectors\n    quantized = self.quantize.embedding(indices)\n    quantized = rearrange(quantized, \"b h w c -> b c h w\").contiguous()\n    return quantized.view(shape)\n\n", "CodeBleu_score": 33.886555538212015}, {"code": "def get_codebook_entry(self, indices, shape)\n\t\n    if self.remap_idx is not None:\n        indices = self.remap_idx[indices]\n    # get quantized latent vectors\n    z_q = self.embedding(indices)\n\n    if shape is not None:\n        z_q = z_q.view(shape)\n        # reshape back to match original input shape\n        z_q = z_q.permute(0, 3, 1, 2).contiguous()\n\n    return z_q\n\n", "CodeBleu_score": 59.40739852107216}, {"code": "def get_codebook_entry(self, indices, shape)\n\t\n    if self.remap_idx is not None:\n        indices = self.remap_idx[indices]\n    if self.permutation is not None:\n        indices = indices[:, self.permutation]\n    b_indices = rearrange(indices, \"b h w -> (b h w)\")\n    cb_entry = self.codebook(b_indices)\n    cb_entry = cb_entry.view(indices.shape + cb_entry.shape[1:])\n    cb_entry = cb_entry.permute(0, 3, 1, 2).contiguous()\n    return cb_entry", "CodeBleu_score": 52.77158980230833}]}
{"instance_id": "c55d014827ef", "generation_result": [{"code": "def factorization(dimension: int, factor:int=-1) -> tuple[int, int]\n\t\n    if factor > 0 and dimension % factor == 0:\n        return factor, dimension // factor\n    else:\n        if factor <= 0:\n            factor = dimension\n        for m in range(1, factor+1):\n            if dimension % m == 0:\n                n = dimension // m\n                if n <= m:\n                    return m, n\n        return 1, dimension\n\n", "CodeBleu_score": 34.88906250313542}, {"code": "def factorization(dimension: int, factor:int=-1) -> tuple[int, int]\n\t\n    if factor > 0 and dimension % factor == 0:\n        return factor, dimension // factor\n    else:\n        for i in range(1, dimension + 1):\n            if dimension % i == 0:\n                j = dimension // i\n                if i <= j and (factor == -1 or i <= factor):\n                    return i, j\n        return dimension, 1\n\n", "CodeBleu_score": 27.814889434790025}, {"code": "def factorization(dimension: int, factor:int=-1) -> tuple[int, int]\n\t\n    if factor > 0:\n        if dimension % factor == 0:\n            return factor, dimension // factor\n    for i in range(int(dimension ** 0.5), 0, -1):\n        if dimension % i == 0:\n            m, n = i, dimension // i\n            if m <= n and m <= factor:\n                return m, n\n    return dimension, 1\n\n", "CodeBleu_score": 29.3879554226535}]}
{"instance_id": "5d613ec26036", "generation_result": [{"code": "def apply_weight_decompose(self, updown, orig_weight)\n\t\n    if self.is_sd_v1:\n        return updown(orig_weight)\n\n    if orig_weight.shape != updown.weight.shape:\n        # print(f\"Weight mismatch for {orig_weight.shape} and {updown.weight.shape}, generating noise to {updown.weight.shape}\")\n        updown.weight.copy_(torch.randn(updown.weight.shape, device=orig_weight.device, dtype=orig_weight.dtype))\n\n    updown.weight.copy_(orig_weight)\n    updown.weight.data.mul_(self.scale)\n\n    updown.weight.data.div_(max(updown.weight.abs().max().item() / 127.5, 1e-8))\n    updown.weight.data.clamp_(-127, 127).round_()\n\n    updown.weight.data.add_(-orig_weight)\n    updown.weight.data.mul_(self.scale)\n\n    return updown(orig_weight)\n\n", "CodeBleu_score": 21.497681526319163}, {"code": "def apply_weight_decompose(self, updown, orig_weight)\n\t\n    assert updown in [\"up\", \"down\"], f\"Invalid updown value: {updown}\"\n    assert orig_weight.shape == self.weight.shape\n\n    # calculate the scale factor based on the updown value\n    scale_factor = 1 / (1 + self.scale_factor) if updown == \"up\" else self.scale_factor\n\n    # calculate the adjusted weight by combining the input weight with the original weight, normalizing the result, and scaling it\n    adjusted_weight = scale_factor * (self.weight + orig_weight) / torch.norm(self.weight + orig_weight) - orig_weight\n\n    # ensure that all tensors are on the same device and have the same data type\n    adjusted_weight = adjusted_weight.to(self.weight.device).type(self.weight.dtype)\n\n    # return the adjusted weight\n    return adjusted_weight\n\n", "CodeBleu_score": 22.22141470286379}, {"code": "def apply_weight_decompose(self, updown, orig_weight)\n\t\n    if self.is_sdxl:\n        return orig_weight\n    if self.is_sdxl_refiner:\n        return orig_weight\n    if self.is_sdxl_pruned:\n        return orig_weight\n    if self.is_sdxl_refiner_pruned:\n        return orig_weight\n    if self.is_sdxl_pruned_quantized:\n        return orig_weight\n    if self.is_sdxl_refiner_pruned_quantized:\n        return orig_weight\n    if self.is_sdxl_pruned_quantized_fp16:\n        return orig_weight\n    if self.is_sdxl_refiner_pruned_quantized_fp16:\n        return orig_weight\n    if self.is_sdxl_pruned_quantized_fp16_pruned:\n        return orig_weight\n    if self.is_sdxl_refiner_pruned_quantized_fp16_pruned:\n        return orig_weight\n    if self.is_sdxl_pruned_quantized_fp16_pruned_pruned:\n        return orig_weight\n    if self.is_sdxl_refiner_pruned_quantized_fp16_pruned_pruned:\n        return orig_weight\n    if self.is_sdxl_pruned_quantized_fp16_pruned_pruned_pruned:\n        return orig_weight\n    if self.is_sdxl_refiner_pruned_quantized_fp16_pruned_pruned_pruned:\n        return orig_weight\n    if self.is_sdxl_pruned_quantized_fp16_pruned_pruned_pruned_pruned:\n        return orig_weight\n    if self.is_sdxl_refiner_pruned_quantized_fp16_pruned_pruned_pruned_pruned:\n        return orig_weight\n    if self", "CodeBleu_score": 10.921484871780251}]}
{"instance_id": "c73d0d1cd3e5", "generation_result": [{"code": "def finalize_updown(self, updown, orig_weight, output_shape, ex_bias=None)\n\t\n    updown = updown.reshape(output_shape)\n    if ex_bias is not None:\n        ex_bias = ex_bias.reshape(output_shape)\n    if orig_weight.shape == updown.shape:\n        updown = updown.reshape(orig_weight.shape)\n    if self.bias is not None:\n        self.bias = self.bias * self.bias_mult\n    if self.scale:\n        updown = sd_models.get_scalar_for_tensor(self.scale, updown)\n    return updown, ex_bias\n\n", "CodeBleu_score": 41.75847864762998}, {"code": "def finalize_updown(self, updown, orig_weight, output_shape, ex_bias=None)\n\t\n    updown = updown.reshape(output_shape)\n    if orig_weight.size == updown.size:\n        updown = updown.reshape(orig_weight.shape)\n    if ex_bias is not None:\n        ex_bias = ex_bias.reshape(output_shape)\n        if orig_weight.size == ex_bias.size:\n            ex_bias = ex_bias.reshape(orig_weight.shape)\n    if self.scaler is not None:\n        updown = self.scaler.decompose(updown)\n    if ex_bias is not None:\n        ex_bias *= orig_weight.scaler.multiplier\n    return updown, ex_bias\n\n", "CodeBleu_score": 47.24408945147624}, {"code": "def finalize_updown(self, updown, orig_weight, output_shape, ex_bias=None)\n\t\n    if ex_bias is not None:\n        updown = updown + ex_bias\n    updown = updown.reshape(output_shape)\n    if orig_weight.shape == updown.shape:\n        updown = updown.reshape(orig_weight.shape)\n    if self.sd_model.configuration.scale_qunat_by_float32:\n        updown = updown / self.sd_model.scale_qunat_by_float32\n    if self.sd_model.configuration.use_attention_bias:\n        updown = updown + self.sd_model.attention_bias\n    if self.sd_model.configuration.use_model_weights_diffusion_to_noise_pred_weight:\n        updown = updown * self.sd_model.model_weights_diffusion_to_noise_pred_weight\n    if self.sd_model.configuration.use_model_weights_diffusion_to_noise_pred_slope:\n        updown = updown * self.sd_model.model_weights_diffusion_to_noise_pred_slope\n    if self.sd_model.configuration.use_model_weights_diffusion_to_noise_pred_shift:\n        updown = updown + self.sd_model.model_weights_diffusion_to_noise_pred_shift\n    if self.sd_model.configuration.use_model_weights_diffusion_to_noise_pred_power:\n        updown = updown ** self.sd_model.model_weights_diffusion_to_noise_pred_power\n    return updown, ex_bias\n\n", "CodeBleu_score": 38.20824425768438}]}
{"instance_id": "c6a41665c62c", "generation_result": [{"code": "def calc_updown(self, orig_weight)\n\t\n    self.weight.data = orig_weight.to(self.weight.device)\n    if self.bias is not None:\n        self.bias.data = self.bias.to(self.weight.device)\n    return self.finalize_updown()\n\n", "CodeBleu_score": 30.65880127109399}, {"code": "def calc_updown(self, orig_weight)\n\t\n    if self.bias is not None:\n        bias = self.bias.to(orig_weight.device)\n    else:\n        bias = None\n    if self.qconfig is not None:\n        weight = self.qconfig.activation().scale_tensor(self.weight)\n    else:\n        weight = self.weight\n    return self._make_trainable_params(orig_weight, weight, bias)\n\n", "CodeBleu_score": 44.53115838597677}, {"code": "def calc_updown(self, orig_weight)\n\t\n    if self.device != orig_weight.device:\n        self.to(orig_weight.device)\n    return self._calc_updown(orig_weight)\n\n", "CodeBleu_score": 15.76920275601284}]}
{"instance_id": "1bb9e5d94af8", "generation_result": [{"code": "def purge_networks_from_memory()\n\t\n    if len(shared.sd_model_cache) > shared.opts.lora_max_networks:\n        logger.info(f\"Purging networks from memory. Current count: {len(shared.sd_model_cache)}\")\n        while len(shared.sd_model_cache) > shared.opts.lora_max_networks:\n            shared.sd_model_cache.popitem(last=False)\n        torch.cuda.empty_cache()\n\n", "CodeBleu_score": 24.103310296471225}, {"code": "def purge_networks_from_memory()\n\t\n    global networks\n    max_networks = 100\n    while len(networks) > max_networks:\n        networks.pop(0)\n    torch.cuda.empty_cache()\n\n", "CodeBleu_score": 26.069124508575044}, {"code": "def purge_networks_from_memory()\n\t\n    logger.info(\"Purging networks from memory\")\n    # Get the number of networks in memory\n    num_networks = len(shared.sd_model.sd_checkpoint_info.filename_list)\n    # Calculate the number of networks to purge\n    num_purge = num_networks - shared.opts.max_networks_to_keep\n    # If the number of networks is within the limit, return\n    if num_purge <= 0:\n        return\n    # Get the list of network filenames\n    network_filenames = shared.sd_model.sd_checkpoint_info.filename_list\n    # Purge networks until the number of networks is within the limit\n    for i in range(num_purge):\n        # Get the network filename\n        network_filename = network_filenames[i]\n        # Get the network name\n        network_name = os.path.splitext(network_filename)[0]\n        # Remove the network from memory\n        shared.sd_model.sd_checkpoint_info.filename_list.remove(network_filename)\n        # Remove the network from the checkpoint info\n        shared.sd_model.sd_checkpoint_info.checkpoint_info.pop(network_name, None)\n    # Trigger garbage collection to free up memory\n    torch.cuda.empty_cache()\n    logger.info(f\"Purged {num_purge} networks from memory\")\n\n", "CodeBleu_score": 29.146357008291346}]}
{"instance_id": "42b48ebf4d07", "generation_result": [{"code": "def network_restore_weights_from_backup(self: Union[torch.nn.Conv2d, torch.nn.Linear, torch.nn.GroupNorm, torch.nn.LayerNorm, torch.nn.MultiheadAttention]):\n\t\n    if hasattr(self, 'backup_weights') and hasattr(self, 'backup_bias'):\n        self.weight.data.copy_(self.backup_weights)\n        if self.backup_bias is not None:\n            self.bias.data.copy_(self.backup_bias)\n        else:\n            self.bias = None\n    elif hasattr(self, 'backup_weights') and hasattr(self, 'backup_bias') and hasattr(self, 'backup_projection_weights') and hasattr(self, 'backup_projection_bias'):\n        self.weight.data.copy_(self.backup_weights)\n        if self.backup_bias is not None:\n            self.bias.data.copy_(self.backup_bias)\n        else:\n            self.bias = None\n        self.projection_weights.data.copy_(self.backup_projection_weights)\n        if self.backup_projection_bias is not None:\n            self.projection_bias.data.copy_(self.backup_projection_bias)\n        else:\n            self.projection_bias = None\n    else:\n        logging.warning(f\"No backup weights and biases found for layer {self}\")\n\n", "CodeBleu_score": 38.13867796128492}, {"code": "def network_restore_weights_from_backup(self: Union[torch.nn.Conv2d, torch.nn.Linear, torch.nn.GroupNorm, torch.nn.LayerNorm, torch.nn.MultiheadAttention]):\n\t\n    if self.backup_weights is not None:\n        self.weight = self.backup_weights.clone()\n    if self.backup_bias is not None:\n        self.bias = self.backup_bias.clone()\n    if isinstance(self, torch.nn.MultiheadAttention):\n        if self.backup_proj_weights is not None:\n            self.out_proj.weight = self.backup_proj_weights.clone()\n        if self.backup_proj_bias is not None:\n            self.out_proj.bias = self.backup_proj_bias.clone()\n    else:\n        self.bias = None\n\n", "CodeBleu_score": 34.71037734367548}, {"code": "def network_restore_weights_from_backup(self: Union[torch.nn.Conv2d, torch.nn.Linear, torch.nn.GroupNorm, torch.nn.LayerNorm, torch.nn.MultiheadAttention]):\n\t\n    if hasattr(self, 'weight_backup'):\n        self.weight.data = self.weight_backup.data\n        delattr(self, 'weight_backup')\n    if hasattr(self, 'bias_backup'):\n        self.bias.data = self.bias_backup.data\n        delattr(self, 'bias_backup')\n    if hasattr(self, 'proj_backup'):\n        self.proj.data = self.proj_backup.data\n        delattr(self, 'proj_backup')\n    if hasattr(self, 'proj_bias_backup'):\n        self.proj.bias.data = self.proj_bias_backup.data\n        delattr(self, 'proj_bias_backup')\n\n", "CodeBleu_score": 28.385224782347084}]}
{"instance_id": "c8fcd357131a", "generation_result": [{"code": "def network_forward(org_module, input, original_forward)\n\t\n    if shared.loaded_networks is None:\n        return original_forward(input)\n\n    input = input.to(devices.device)\n    for mod in shared.loaded_networks:\n        mod.restore_weights()\n\n    output = original_forward(input)\n    for mod in shared.loaded_networks:\n        if mod.layer == mod.current_layer:\n            output = mod.apply(output)\n\n    return output\n\n", "CodeBleu_score": 27.96974233105449}, {"code": "def network_forward(org_module, input, original_forward)\n\t\n    if not shared.opts.enable_network_module:\n        return original_forward(input)\n    try:\n        if not hasattr(org_module, \"backup_weights\"):\n            org_module.backup_weights = org_module.state_dict()\n        org_module.load_state_dict(org_module.backup_weights)\n        input = input.to(devices.device)\n        output = original_forward(input)\n        for network_module in shared.loaded_networks:\n            if network_module.matches_layer(org_module):\n                output = network_module.apply_to_output(org_module, output)\n        return output\n    except Exception as e:\n        errors.report(f\"Error while processing network module forward pass: {e}\", exc_info=True)\n        return original_forward(input)\n\n", "CodeBleu_score": 36.566913314956864}, {"code": "def network_forward(org_module, input, original_forward)\n\t\n    if shared.cmd_opts.no_network:\n        return original_forward(input)\n\n    if shared.opts.network_module is None:\n        return original_forward(input)\n\n    if shared.opts.network_module.networks is None:\n        return original_forward(input)\n\n    input = input.to(devices.device)\n\n    for network in shared.opts.network_module.networks:\n        if network.enabled and network.is_in_context(org_module):\n            logger.debug(f\"Applying {network.name} to {org_module.__class__.__name__}\")\n            input = network(input, org_module)\n\n    return input\n\n", "CodeBleu_score": 38.01688766526614}]}
{"instance_id": "1070211f5c11", "generation_result": [{"code": "def trainables(self)\n\t\n    return [p for p in self.parameters() if p.requires_grad]\n\n", "CodeBleu_score": 6.452447410697191}, {"code": "def trainables(self)\n\t\n    return [p for p in self.parameters() if p.requires_grad]\n\n", "CodeBleu_score": 6.452447410697191}, {"code": "def trainables(self)\n\t\n    return [p for p in self.parameters() if p.requires_grad]\n\n", "CodeBleu_score": 6.452447410697191}]}
{"instance_id": "0c3ba62c96a7", "generation_result": [{"code": "def load_hypernetworks(names, multipliers=None)\n\t\n    if multipliers is None:\n        multipliers = [1.0] * len(names)\n    shared.hypernetworks = []\n    for name, multiplier in zip(names, multipliers):\n        hypernetwork = load_hypernetwork(name, multiplier)\n        shared.hypernetworks.append(hypernetwork)\n\n", "CodeBleu_score": 18.478514205235104}, {"code": "def load_hypernetworks(names, multipliers=None)\n\t\n    if multipliers is None:\n        multipliers = [1.0 for _ in range(len(names))]\n    loaded_hypernetworks = []\n    for name, multiplier in zip(names, multipliers):\n        if name in shared.loaded_hypernetworks:\n            hypernetwork = shared.loaded_hypernetworks[name]\n        else:\n            hypernetwork = load_hypernetwork(name)\n            hypernetwork.multiplier = multiplier\n            shared.loaded_hypernetworks[name] = hypernetwork\n        loaded_hypernetworks.append(hypernetwork)\n    shared.loaded_hypernetworks_list = loaded_hypernetworks\n\n", "CodeBleu_score": 34.49765016464696}, {"code": "def load_hypernetworks(names, multipliers=None)\n\t\n    if multipliers is None:\n        multipliers = [1.0] * len(names)\n    if len(names) != len(multipliers):\n        raise errors.HypernetworkError(\"The number of hypernetworks and multipliers must be the same.\")\n    for name, multiplier in zip(names, multipliers):\n        if name in shared.hypernetworks:\n            shared.hypernetworks[name].multiplier = multiplier\n        else:\n            shared.hypernetworks[name] = load_hypernetwork(name, multiplier)\n\n", "CodeBleu_score": 24.167608912999654}]}
{"instance_id": "bea3772ed532", "generation_result": [{"code": "\ndef apply_hypernetworks(hypernetworks, context, layer=None)\n\t\n    context_k = context\n    context_v = context\n    for hypernetwork in hypernetworks:\n        context_k, context_v = apply_hypernetwork(hypernetwork, context_k, context_v, layer)\n    return context_k, context_v\n\n", "CodeBleu_score": 74.69978727224338}, {"code": "\ndef apply_hypernetworks(hypernetworks, context, layer=None)\n\t\n    context_k = context\n    context_v = context\n    for hypernetwork in hypernetworks:\n        context_k, context_v = apply_hypernetwork(hypernetwork, context_k, context_v, layer=layer)\n    return context_k, context_v\n\n", "CodeBleu_score": 61.23443109506348}, {"code": "\ndef apply_hypernetworks(hypernetworks, context, layer=None)\n\t\n    context_k = context\n    context_v = context\n    for hypernetwork in hypernetworks:\n        if layer is not None and hypernetwork.layer != layer:\n            continue\n        context_k, context_v = apply_hypernetwork(hypernetwork, context_k, context_v)\n    return context_k, context_v\n\n", "CodeBleu_score": 53.515083620796275}]}
{"instance_id": "922efa3e092f", "generation_result": [{"code": "def attention_CrossAttention_forward(self, x, context=None, mask=None, **kwargs)\n\t\n    h = self.heads\n    q = self.to_q(x)\n    context = default(context, x)\n    k = self.to_k(context) * self.scale\n    v = self.to_v(context)\n    del context\n    q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> (b h) n d', h=h), (q, k, v))\n    sim = einsum('b i d, b j d -> b i j', q, k) * self.scale\n    if exists(mask):\n        mask = rearrange(mask, 'b ... -> b (...)')\n        max_neg_value = -torch.finfo(sim.dtype).max\n        mask = repeat(mask, 'b j -> (b h) () j', h=h)\n        sim.masked_fill_(~mask, max_neg_value)\n    attn = sim.softmax(dim=-1)\n    attn = self.attn_drop(attn)\n    out = einsum('b i j, b j d -> b i d', attn, v)\n    out = rearrange(out, '(b h) n d -> b n (h d)', h=h)\n    return self.to_out(out)\n\n", "CodeBleu_score": 66.56658279489032}, {"code": "def attention_CrossAttention_forward(self, x, context=None, mask=None, **kwargs)\n\t\n    h = self.heads\n\n    q = self.to_q(x)\n\n    context = default(context, x)\n    k = self.to_k(context) * self.scale\n    v = self.to_v(context)\n\n    q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> (b h) n d', h=h), (q, k, v))\n\n    sim = einsum('b i d, b j d -> b i j', q, k) * self.scale\n\n    if exists(mask):\n        mask = rearrange(mask, 'b ... -> b (...)')\n        max_neg_value = -torch.finfo(sim.dtype).max\n        mask = repeat(mask, 'b j -> (b h) () j', h=h)\n        sim.masked_fill_(~mask, max_neg_value)\n\n    # attention, what we cannot get enough of\n    attn = sim.softmax(dim=-1)\n    attn = self.attn_drop(attn)\n\n    out = einsum('b i j, b j d -> b i d', attn, v)\n    out = rearrange(out, '(b h) n d -> b n (h d)', h=h)\n    return self.to_out(out)\n\n", "CodeBleu_score": 65.97048735345162}, {"code": "def attention_CrossAttention_forward(self, x, context=None, mask=None, **kwargs)\n\t\n    h = self.heads\n    q = self.to_q(x)\n    context = default(context, x)\n    k = self.to_k(context)\n    v = self.to_v(context)\n    q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> (b h) n d', h=h), (q, k, v))\n    if exists(self.hypernet):\n        k, v = self.hypernet(k, v)\n    sim = einsum('b i d, b j d -> b i j', q, k) * self.scale\n    if exists(mask):\n        mask = rearrange(mask, 'b ... -> b (...)')\n        max_neg_value = -torch.finfo(sim.dtype).max\n        mask = repeat(mask, 'b j -> (b h) () j', h=h)\n        sim.masked_fill_(~mask, max_neg_value)\n    attn = sim.softmax(dim=-1)\n    attn = self.attn_drop(attn)\n    out = einsum('b i j, b j d -> b i d', attn, v)\n    out = rearrange(out, '(b h) n d -> b n (h d)', h=h)\n    return self.to_out(out)\n\n", "CodeBleu_score": 71.65073187658643}]}
{"instance_id": "8b7455a03a0e", "generation_result": [{"code": "def save_hypernetwork(hypernetwork, checkpoint, hypernetwork_name, filename)\n\t\n    if hypernetwork is None:\n        return\n    try:\n        hypernetwork_name = hypernetwork_name or hypernetwork.name\n        hypernetwork_name = hypernetwork_name.replace(\" \", \"_\")\n        hypernetwork_name = hypernetwork_name.replace(\".\", \"_\")\n        hypernetwork_name = hypernetwork_name.replace(\"-\", \"_\")\n        hypernetwork_name = hypernetwork_name.replace(\":\", \"_\")\n        hypernetwork_name = hypernetwork_name.replace(\",\", \"_\")\n        hypernetwork_name = hypernetwork_name.replace(\";\", \"_\")\n        hypernetwork_name = hypernetwork_name.replace(\"|\", \"_\")\n        hypernetwork_name = hypernetwork_name.replace(\"\\\\\", \"_\")\n        hypernetwork_name = hypernetwork_name.replace(\"/\", \"_\")\n        hypernetwork_name = hypernetwork_name.replace(\"(\", \"_\")\n        hypernetwork_name = hypernetwork_name.replace(\")\", \"_\")\n        hypernetwork_name = hypernetwork_name.replace(\"[\", \"_\")\n        hypernetwork_name = hypernetwork_name.replace(\"]\", \"_\")\n        hypernetwork_name = hypernetwork_name.replace(\"{\", \"_\")\n        hypernetwork_name = hypernetwork_name.replace(\"}\", \"_\")\n        hypernetwork_name = hypernetwork_name.replace(\"<\", \"_\")\n        hypernetwork_name = hypernetwork_name.replace(\">\", \"_\")\n        hypernetwork_name = hypernetwork_name.replace(\"&\", \"_\")\n        hypernetwork_name = hypernetwork_name.replace(\"*\", \"_\")\n        hypernetwork_name = hypernetwork_name.replace(\"^\", \"_\")\n        hypernetwork_name = hypernetwork_name.replace(\"%\", \"_\")\n        hypernetwork_name = hypernetwork_name.replace(\"$\", \"_\")\n        hypernetwork_name = hypernetwork_name.replace(\"#\", \"_\")\n        hypernetwork_name = hypernetwork_name.replace(\"@\", \"_\")\n        hypernetwork_name = hypernetwork_name.replace(\"~\", \"_\")\n        hypernetwork_name = hypernetwork_name.replace(\"`\", \"_\")\n        hypernetwork_name = hypernetwork_name.replace(\"'\", \"_\")\n", "CodeBleu_score": 24.404596034827865}, {"code": "def save_hypernetwork(hypernetwork, checkpoint, hypernetwork_name, filename)\n\t\n    original_hypernetwork_name = hypernetwork.hypernetwork_name\n    original_filename = hypernetwork.filename\n    try:\n        hypernetwork.hypernetwork_name = hypernetwork_name\n        hypernetwork.filename = filename\n        hypernetwork.save(checkpoint)\n    except Exception as e:\n        hypernetwork.hypernetwork_name = original_hypernetwork_name\n        hypernetwork.filename = original_filename\n        raise e\n\n", "CodeBleu_score": 29.337001493206415}, {"code": "def save_hypernetwork(hypernetwork, checkpoint, hypernetwork_name, filename)\n\t\n    with hypernetwork.restore_hypernetwork_name_and_filename():\n        hypernetwork.hypernetwork_name = hypernetwork_name\n        hypernetwork.filename = filename\n        hypernetwork.save(checkpoint)\n\n", "CodeBleu_score": 17.847934947716368}]}
{"instance_id": "3175f835140f", "generation_result": [{"code": "def ema_scope(self, context=None)\n\t\n    if not self.ema_scope_enabled:\n        yield\n        return\n    context = default(context, '')\n    self.model.eval()\n    self.ema_model.eval()\n    self.model.to(self.device)\n    self.ema_model.to(self.device)\n    self.model_params_ema = self.ema_model.state_dict()\n    self.model_params_current = self.model.state_dict()\n    if self.ema_log:\n        self.log(f'{context} EMA: Switched to EMA weights')\n    with torch.no_grad():\n        for key in self.model_params_ema:\n            self.model_params_current[key].copy_(self.model_params_ema[key])\n    yield\n    if self.ema_log:\n        self.log(f'{context} EMA: Restored from EMA weights')\n    self.model.train()\n    self.ema_model.train()\n    with torch.no_grad():\n        for key in self.model_params_ema:\n            self.model_params_ema[key].copy_(self.model_params_current[key])\n\n", "CodeBleu_score": 37.1454529225809}, {"code": "def ema_scope(self, context=None)\n\t\n    if self.ema_scope:\n        self.ema_scope = False\n        return\n    self.ema_scope = True\n    self.model_ema.store(self.model.parameters())\n    self.model_ema.copy_to(self.model.parameters())\n    if context is not None:\n        self.log(f\"{context}/ema_enabled\", 1, prog_bar=False)\n    yield\n    self.model_ema.restore(self.model.parameters())\n    if context is not None:\n        self.log(f\"{context}/ema_enabled\", 0, prog_bar=False)\n    self.ema_scope = False\n\n", "CodeBleu_score": 49.1871931983211}, {"code": "def ema_scope(self, context=None)\n\t\n    if self.ema_scope_level > 0:\n        return\n    self.ema_scope_level += 1\n    if self.ema_scope_level == 1:\n        self.ema_model.store(self.model.parameters())\n        self.ema_model.copy_to(self.model.parameters())\n        if self.ema_scope_level == 1 and self.ema_log:\n            self.log_ema_scope_action('store_and_copy')\n    yield\n    self.ema_scope_level -= 1\n    if self.ema_scope_level == 0:\n        self.ema_model.restore(self.model.parameters())\n        if self.ema_scope_level == 0 and self.ema_log:\n            self.log_ema_scope_action('restore')\n\n", "CodeBleu_score": 40.2260746072374}]}
{"instance_id": "a7b5d5cd519a", "generation_result": [{"code": "def p_mean_variance(self, x, t, clip_denoised: bool)\n\t\n    model_output = self.model(x, t)\n    if self.model_var_type in [ModelVarType.LEARNED, ModelVarType.LEARNED_RANGE]:\n        B, C = t.shape\n        assert model_output.shape == (B, C * 2, *x.shape[2:])\n        model_output, model_var_values = torch.split(model_output, C, dim=1)\n        if self.model_var_type == ModelVarType.LEARNED:\n            model_log_variance = model_var_values\n            model_variance = torch.exp(model_log_variance)\n        else:\n            min_log = _extract_into_tensor(self.posterior_log_variance_clipped, t)\n            max_log = _extract_into_tensor(np.log(self.betas[-1]), t)\n            # The model_var_values is [-1, 1] for [min_var, max_var].\n            frac = (model_var_values + 1) / 2\n            model_log_variance = frac * max_log + (1 - frac) * min_log\n            model_variance = torch.exp(model_log_variance)\n    else:\n        model_variance, model_log_variance = {\n            # for fixedlarge, we set the initial (log-)variance like so to get a better decoder log likelihood.\n            ModelVarType.FIXED_LARGE: (\n                np.append(self.posterior_variance[1], self.posterior_variance[0]),\n                np.append(self.posterior_log_variance_clipped[1], self.posterior_log_variance_clipped[0]),\n            ),\n            ModelVarType.FIXED_SMALL: (\n                self.posterior_variance,\n                self.posterior_log_variance_clipped,\n            ),\n        }[self.model_var_type]\n        model_variance = extract_into_tensor(model_variance, t, x.shape)\n", "CodeBleu_score": 29.28164274451084}, {"code": "def p_mean_variance(self, x, t, clip_denoised: bool)\n\t\n    if self.model_var_type in ['learned', 'learned_range']:\n        assert x.shape == torch.Size([x.shape[0], self.ch, self.image_size, self.image_size])\n        model_output = self.apply_model(x, t)\n        if self.model_var_type == 'learned':\n            assert model_output.shape == torch.Size([x.shape[0], 2 * self.ch, self.image_size, self.image_size])\n            model_output, model_var_values = torch.split(model_output, self.ch, dim=1)\n            if self.clip_denoised:\n                model_output = model_output.clamp(-1., 1.)\n        else:\n            model_output, model_logvar = torch.split(model_output, self.ch, dim=1)\n            min_log = _extract_into_tensor(self.posterior_logvar_clip, t)\n            max_log = _extract_into_tensor(np.log(self.betas[t]), t)\n            if self.clip_denoised:\n                model_output = model_output.clamp(-1., 1.)\n            model_logvar = model_logvar.clamp(min_log, max_log)\n            model_var = torch.exp(model_logvar)\n        if self.model_mean_type == 'x_prev':\n            pred_xstart = self.predict_xstart_from_eps(x_t=x, t=t, eps=model_output)\n            model_mean, _, _ = self.q_posterior_mean_variance(x_start=pred_xstart, x_t=x, t=t)\n        elif self.model_mean_type == 'x_start':\n            pred_xstart = self.predict_xstart_from_eps(x_t=x, t=t, eps=model_output)\n            model_mean, _, _ = self.q_posterior_mean_variance(x_start=pred_xstart, x_", "CodeBleu_score": 39.662459610338786}, {"code": "def p_mean_variance(self, x, t, clip_denoised: bool)\n\t\n    model_output = self.model(x, t)\n\n    if self.model_var_type in [\"fixedlarge\", \"fixedsmall\"]:\n        x_recon = self.predict_start_from_noise(x, t=t, noise=model_output)\n        model_mean, posterior_log_variance = {\n            \"fixedlarge\": self.posterior_mean_coef_group[2] * x_recon,\n            \"fixedsmall\": self.posterior_mean_coef_group[2] * x_recon,\n        }[self.model_var_type]\n        posterior_variance = self.posterior_var_coef_group[2] * torch.ones_like(x)\n    else:\n        if self.model_var_type in [\"learned\", \"learned_range\"]:\n            model_log_variance_clipped = model_output\n            if self.model_var_type == \"learned\":\n                model_log_variance_clipped = {\n                    \"min\": torch.log(torch.exp(model_log_variance_clipped) + 1e-20),\n                    \"max\": torch.log(torch.exp(model_log_variance_clipped) + 1.0 - 1e-20),\n                }[self.clip_sample]\n            else:\n                model_log_variance_clipped = model_log_variance_clipped.clamp(\n                    self.min_logvar, self.max_logvar\n                )\n            model_variance = torch.exp(model_log_variance_clipped)\n            x_recon = self.predict_start_from_noise(x, t=t, noise=model_output)\n        else:\n            raise NotImplementedError(self.model_var_type)\n\n        if self.posterior_var_type in [\"fixedsmall\", \"fixedlarge\"]:\n            posterior_variance = {\n                \"fixedsmall\": self.posterior_var_coef_group[2] * torch.ones_like(x),\n                \"fixedlarge\": self.posterior_var", "CodeBleu_score": 37.865050213670465}]}
{"instance_id": "a8652fa78473", "generation_result": [{"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss == 'l1':\n        loss = torch.abs(target - pred)\n        if mean:\n            loss = loss.mean()\n    elif self.loss == 'l2':\n        loss = (target - pred) ** 2\n        if not mean:\n            loss = loss.sum()\n    else:\n        raise NotImplementedError\n    return loss\n\n", "CodeBleu_score": 43.84347775886764}, {"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss == 'l1':\n        loss = F.l1_loss(pred, target, reduction='none')\n        if mean:\n            loss = loss.mean([1, 2, 3], keepdim=True)\n    elif self.loss == 'l2':\n        loss = F.mse_loss(pred, target, reduction='none')\n        if not mean:\n            loss = loss.sum([1, 2, 3])\n    else:\n        raise NotImplementedError()\n    return loss\n\n", "CodeBleu_score": 48.75510913348342}, {"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss == 'l1':\n        loss = F.l1_loss(pred, target, reduction='none')\n        if mean:\n            loss = loss.mean([1, 2, 3], keepdim=True)\n    elif self.loss == 'l2':\n        loss = F.mse_loss(pred, target, reduction='none')\n        if not mean:\n            loss = loss.sum([1, 2, 3])\n    else:\n        raise NotImplementedError()\n    return loss\n\n", "CodeBleu_score": 48.75510913348342}]}
{"instance_id": "d68e4f061349", "generation_result": [{"code": "def _prior_bpd(self, x_start)\n\t\n    b, c, h, w = x_start.shape\n    t = torch.tensor([self.num_timesteps - 1] * b, device=x_start.device)\n    qt_mean, _, qt_log_variance = self.q_mean_variance(x_start, t)\n    kl_prior = normal_kl(mean1=qt_mean, logvar1=qt_log_variance, mean2=0.0, logvar2=0.0)\n    return torch.mean(kl_prior) / math.log(2.0)\n\n", "CodeBleu_score": 63.76487426224776}, {"code": "def _prior_bpd(self, x_start)\n\t\n    batch_size = x_start.shape[0]\n    t = torch.tensor([self.num_timesteps - 1] * batch_size, device=x_start.device)\n    qt_mean, _, qt_log_variance = self.q_mean_variance(x_start=x_start, t=t)\n    kl_prior = normal_kl(mean1=qt_mean, logvar1=qt_log_variance, mean2=0.0, logvar2=0.0)\n    return torch.mean(kl_prior) / math.log(2.0)\n\n", "CodeBleu_score": 64.15428341879867}, {"code": "def _prior_bpd(self, x_start)\n\t\n    batch, device = x_start.shape[0], x_start.device\n    t = torch.tensor([self.num_timesteps - 1] * batch, device=device)\n    qt_mean, _, qt_log_variance = self.q_mean_variance(x_start, t)\n    kl_prior = normal_kl(mean1=qt_mean, logvar1=qt_log_variance, mean2=0.0, logvar2=0.0)\n    return torch.mean(kl_prior) / math.log(2.0)\n\n", "CodeBleu_score": 63.03823422616054}]}
{"instance_id": "c695d93fdf2d", "generation_result": [{"code": "def make_cache(subsection: str) -> diskcache.Cache\n\t\n    cache_dir = os.path.join(script_path, \"cache\")\n    os.makedirs(cache_dir, exist_ok=True)\n    cache_path = os.path.join(cache_dir, f\"{subsection}.cache\")\n    cache = diskcache.Cache(cache_path, size_limit=4e9)\n    cache.settings[\"minimum_file_size\"] = 256 * 1024\n    return cache\n\n", "CodeBleu_score": 42.45424295434551}, {"code": "def make_cache(subsection: str) -> diskcache.Cache\n\t\n    cache_dir = os.path.join(data_path, \"cache\", subsection)\n    os.makedirs(cache_dir, exist_ok=True)\n    cache = diskcache.Cache(cache_dir, size_limit=4 * 1024 ** 3)\n    cache.settings[\"sqlite_min_file_size\"] = 256 * 1024\n    return cache\n\n", "CodeBleu_score": 46.81456294162296}, {"code": "def make_cache(subsection: str) -> diskcache.Cache\n\t\n    cache_path = os.path.join(data_path, \"cache\", subsection)\n    os.makedirs(cache_path, exist_ok=True)\n    cache = diskcache.Cache(cache_path, size_limit=4e9, eviction_policy=\"least-recently-used\")\n    cache.sqlite_auto_vacuum = True\n    cache.minimum_file_size = 256000\n    return cache\n\n", "CodeBleu_score": 41.94489840297773}]}
{"instance_id": "ed4c543f0d38", "generation_result": [{"code": "def cache(subsection)\n\t\n    global caches\n    global cache_lock\n\n    # Get the cache object from the global caches dictionary\n    cache = caches.get(subsection)\n\n    # If the cache object does not exist, acquire the lock to ensure thread-safety\n    if cache is None:\n        with cache_lock:\n            # Check if the cache directory exists and if the cache filename is a valid file\n            if not os.path.isdir(data_path):\n                os.mkdir(data_path)\n            if not os.path.isfile(os.path.join(data_path, \"cache.json\")):\n                convert_old_cached_data()\n\n            # Re-check the caches dictionary for the subsection\n            cache = caches.get(subsection)\n\n            # If the cache object still does not exist, create a new cache object\n            if cache is None:\n                cache = make_cache(subsection)\n                caches[subsection] = cache\n\n    # Return the retrieved or newly created cache object\n    return cache\n\n", "CodeBleu_score": 66.10936619589549}, {"code": "def cache(subsection)\n\t\n    global cache_lock\n    global caches\n\n    if subsection in caches:\n        return caches[subsection]\n\n    with cache_lock:\n        if not os.path.isdir(data_path):\n            os.mkdir(data_path)\n\n        if not os.path.isfile(os.path.join(data_path, f\"{subsection}.json\")):\n            convert_old_cached_data()\n\n        if subsection not in caches:\n            caches[subsection] = make_cache(subsection)\n\n    return caches[subsection]\n\n", "CodeBleu_score": 46.57922981973138}, {"code": "def cache(subsection)\n\t\n    global caches\n    global cache_lock\n    with cache_lock:\n        if subsection in caches:\n            return caches[subsection]\n        cache_dir = os.path.join(data_path, \"cache\")\n        cache_filename = os.path.join(cache_dir, f\"{subsection}.json\")\n        if not os.path.isdir(cache_dir) or not os.path.isfile(cache_filename):\n            convert_old_cached_data()\n        if subsection in caches:\n            return caches[subsection]\n        cache_obj = make_cache(subsection)\n        caches[subsection] = cache_obj\n        return cache_obj\n\n", "CodeBleu_score": 53.04240762729984}]}
{"instance_id": "ccf5ea2d9fc1", "generation_result": [{"code": "def cached_data_for_file(subsection, title, filename, func)\n\t\n    cache = cache(subsection)\n    mtime = os.path.getmtime(filename)\n    if title in cache:\n        if cache[title]['mtime'] < mtime:\n            del cache[title]\n    if title not in cache:\n        data = func()\n        cache[title] = {'data': data, 'mtime': mtime}\n        dump_cache(cache, subsection)\n    return cache[title]['data']\n\n", "CodeBleu_score": 25.31344716907847}, {"code": "def cached_data_for_file(subsection, title, filename, func)\n\t\n    cache = cache(subsection)\n    mtime = os.path.getmtime(filename)\n    if title in cache:\n        if cache[title][\"mtime\"] < mtime:\n            del cache[title]\n    if title not in cache:\n        data = func()\n        cache[title] = {\"data\": data, \"mtime\": mtime}\n        dump_cache(cache)\n    return cache[title][\"data\"]\n\n", "CodeBleu_score": 25.991016342703695}, {"code": "def cached_data_for_file(subsection, title, filename, func)\n\t\n    cache = cache(subsection)\n    mtime = os.path.getmtime(filename)\n    if title in cache:\n        if cache[title]['mtime'] < mtime:\n            del cache[title]\n\n    if title not in cache:\n        data = func()\n        cache[title] = {\n            'mtime': mtime,\n            'data': data\n        }\n        dump_cache(cache, subsection)\n\n    return cache[title]['data']\n\n", "CodeBleu_score": 25.943846161375582}]}
{"instance_id": "e01a2d4f0ac2", "generation_result": [{"code": "def report(message: str, *, exc_info: bool = False) -> None\n\t\n    try:\n        record_exception()\n        for line in message.splitlines():\n            print(\"*** \" + line, file=sys.stderr)\n        if exc_info:\n            print(textwrap.indent(traceback.format_exc(), \"    \"), file=sys.stderr)\n            print(\"---\", file=sys.stderr)\n    except Exception:\n        pass\n\n", "CodeBleu_score": 78.31073205941101}, {"code": "def report(message: str, *, exc_info: bool = False) -> None\n\t\n    record_exception()\n    for line in message.splitlines():\n        print(f\"*** {line}\", file=sys.stderr)\n    if exc_info:\n        print(textwrap.indent(traceback.format_exc(), \"    \"), file=sys.stderr)\n        print(\"---\", file=sys.stderr)\n\n", "CodeBleu_score": 70.73041784590174}, {"code": "def report(message: str, *, exc_info: bool = False) -> None\n\t\n    record_exception()\n    for line in message.splitlines():\n        print(f\"*** {line}\", file=sys.stderr)\n    if exc_info:\n        print(textwrap.indent(traceback.format_exc(), \"    \"), file=sys.stderr)\n        print(\"---\", file=sys.stderr)\n\n", "CodeBleu_score": 70.73041784590174}]}
{"instance_id": "125fb9d2320b", "generation_result": [{"code": "def extract_original_prompts(style: PromptStyle, prompt, negative_prompt)\n\t\n    if style.prompt == '' and style.negative_prompt == '':\n        return False, prompt, negative_prompt\n\n    try:\n        extracted_prompt = extract_style_text(prompt, style)\n    except errors.StyleExtractionError:\n        return False, prompt, negative_prompt\n\n    try:\n        extracted_negative_prompt = extract_style_text(negative_prompt, style)\n    except errors.StyleExtractionError:\n        return False, prompt, negative_prompt\n\n    return True, extracted_prompt, extracted_negative_prompt\n\n", "CodeBleu_score": 52.76714785642369}, {"code": "def extract_original_prompts(style: PromptStyle, prompt, negative_prompt)\n\t\n    if style.prompt == '' and style.negative_prompt == '':\n        return False, prompt, negative_prompt\n\n    try:\n        extracted_prompt = extract_style_text(style.prompt, prompt)\n    except:\n        return False, prompt, negative_prompt\n\n    try:\n        extracted_negative_prompt = extract_style_text(style.negative_prompt, negative_prompt)\n    except:\n        return False, prompt, negative_prompt\n\n    return True, extracted_prompt, extracted_negative_prompt\n\n", "CodeBleu_score": 55.62624741818076}, {"code": "def extract_original_prompts(style: PromptStyle, prompt, negative_prompt)\n\t\n    if style.prompt == \"\" and style.negative_prompt == \"\":\n        return False, prompt, negative_prompt\n    try:\n        extracted_prompt = extract_style_text(style.prompt, prompt)\n        extracted_negative_prompt = extract_style_text(style.negative_prompt, negative_prompt)\n    except Exception:\n        return False, prompt, negative_prompt\n    return True, extracted_prompt, extracted_negative_prompt\n\n", "CodeBleu_score": 50.9019964820378}]}
{"instance_id": "0c5f729fbce2", "generation_result": [{"code": "def random_partial(self, count, n_frames)\n\t\n    if self.n_utterances == 0:\n        self._load_utterances()\n\n    utterances = self.utterance_cycler.sample(count)\n\n    a = [(u,) + u.random_partial(n_frames) for u in utterances]\n\n    return a\n\n", "CodeBleu_score": 87.68438792375257}, {"code": "def random_partial(self, count, n_frames)\n\t\n    # First, make sure the utterances are loaded\n    self._load_utterances()\n\n    # Then randomly select the required amount of utterances\n    utterances = self.utterance_cycler.sample(count)\n\n    # Then return those utterances\n    return [(u,) for u in utterances]\n\n", "CodeBleu_score": 27.194461703254092}, {"code": "def random_partial(self, count, n_frames)\n\t\n    if not self.utterances:\n        self._load_utterances()\n\n    utterances = self.utterance_cycler.sample(count)\n\n    a = [(u,) + u.random_partial(n_frames) for u in utterances]\n\n    return a\n\n", "CodeBleu_score": 86.78768398321456}]}
{"instance_id": "31e8b27b8f94", "generation_result": [{"code": "def normalize_volume(wav, target_dBFS, increase_only=False, decrease_only=False)\n\t\n    if increase_only and decrease_only:\n        raise ValueError(\"Both increase only and decrease only are set\")\n    rms = np.sqrt(np.mean((wav * int16_max) ** 2))\n    wave_dBFS = 20 * np.log10(rms / int16_max)\n    dBFS_change = target_dBFS - wave_dBFS\n    if (dBFS_change < 0 and increase_only) or (dBFS_change > 0 and decrease_only):\n        return wav\n    return wav * (10 ** (dBFS_change / 20))\n\n", "CodeBleu_score": 74.5073023364786}, {"code": "def normalize_volume(wav, target_dBFS, increase_only=False, decrease_only=False)\n\t\n    if increase_only and decrease_only:\n        raise ValueError(\"Both increase only and decrease only are set\")\n    dBFS_change = target_dBFS - 10 * np.log10(np.mean(wav ** 2))\n    if (dBFS_change < 0 and increase_only) or (dBFS_change > 0 and decrease_only):\n        return wav\n    return wav * (10 ** (dBFS_change / 20))\n\n", "CodeBleu_score": 96.28790819755086}, {"code": "def normalize_volume(wav, target_dBFS, increase_only=False, decrease_only=False)\n\t\n    if increase_only and decrease_only:\n        raise ValueError(\"Both increase only and decrease only are set\")\n    rms = np.sqrt(np.mean((wav * SAMPLE_RATE) ** 2))\n    wave_dBFS = 20 * np.log10(rms)\n    dBFS_change = target_dBFS - wave_dBFS\n    if (dBFS_change < 0 and increase_only) or (dBFS_change > 0 and decrease_only):\n        return wav\n    return wav * (10 ** (dBFS_change / 20))\n\n", "CodeBleu_score": 75.00832535704231}]}
{"instance_id": "2353831e9033", "generation_result": [{"code": "def load_model(weights_fpath: Path, device=None)\n\t\n    global _model\n\n    if device is None:\n        _device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    elif isinstance(device, str):\n        _device = torch.device(device)\n    else:\n        _device = device\n\n    _model = SpeakerEncoder(_device, torch.device(\"cpu\"))\n    checkpoint = torch.load(weights_fpath, map_location=\"cpu\")\n    _model.load_state_dict(checkpoint[\"model_state\"])\n    _model.eval()\n    print(\"Loaded encoder \\\"%s\\\" trained to step %d\" % (weights_fpath, checkpoint[\"step\"]))\n\n", "CodeBleu_score": 79.69439499666903}, {"code": "def load_model(weights_fpath: Path, device=None)\n\t\n\n    global _model\n\n    if device is None:\n        _device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    elif isinstance(device, str):\n        _device = torch.device(device)\n    else:\n        _device = device\n\n    _model = SpeakerEncoder(_device, torch.device(\"cpu\"))\n    checkpoint = torch.load(weights_fpath, map_location=\"cpu\")\n    _model.load_state_dict(checkpoint[\"model_state\"])\n    _model.eval()\n    print(\"Loaded encoder \\\"%s\\\" trained to step %d\" % (weights_fpath, checkpoint[\"step\"]))\n\n", "CodeBleu_score": 79.69439499666903}, {"code": "def load_model(weights_fpath: Path, device=None)\n\t\n    global _model\n\n    if device is None:\n        _device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        if not _device:\n            raise Exception(\n                \"GPU not found. Add the '--cpu' option to run on CPU mode.\")\n    elif device == \"cpu\":\n        _device = torch.device(\"cpu\")\n    else:\n        _device = torch.device(device)\n\n    _model = SpeakerEncoder(_device, torch.device(\"cpu\"))\n    checkpoint = torch.load(weights_fpath, map_location=\"cpu\")\n    _model.load_state_dict(checkpoint[\"model_state\"])\n    _model.eval()\n    print(\"Loaded encoder \\\"%s\\\" trained to step %d\" %\n          (weights_fpath, checkpoint[\"step\"]))\n\n", "CodeBleu_score": 70.3739266598135}]}
{"instance_id": "3231ad480199", "generation_result": [{"code": "def embed_frames_batch(frames_batch)\n\t\n    if _model is None:\n        raise Exception(\n            \"Model was not loaded. Call load_model() before inference.\"\n        )\n\n    frames = torch.from_numpy(frames_batch).to(_device)\n    embed = _model.forward(frames).detach().cpu().numpy()\n    return embed\n\n", "CodeBleu_score": 82.7565494255655}, {"code": "def embed_frames_batch(frames_batch)\n\t\n    if _model is None:\n        raise Exception(\"Model was not loaded. Call load_model() before inference.\")\n\n    frames = torch.from_numpy(frames_batch).to(_device)\n    embed = _model.forward(frames)\n    embed = embed.detach().cpu().numpy()\n    return embed\n\n", "CodeBleu_score": 82.51942172892743}, {"code": "def embed_frames_batch(frames_batch)\n\t\n    if _model is None:\n        raise Exception(\n            \"Model was not loaded. Call load_model() before inference.\"\n        )\n\n    frames = torch.from_numpy(frames_batch).to(_device)\n    embed = _model.forward(frames).detach().cpu().numpy()\n    return embed\n\n", "CodeBleu_score": 82.7565494255655}]}
{"instance_id": "117a41716c8d", "generation_result": [{"code": "def compute_partial_slices(n_samples, partial_utterance_n_frames=partials_n_frames,\n                           min_pad_coverage=0.75, overlap=0.5)\n\t\n    assert 0 < overlap < 1\n    assert 0 < min_pad_coverage < 1\n\n    samples_per_frame = int((sampling_rate * mel_window_step / 1000))\n    n_frames = int(np.ceil((n_samples + 1) / samples_per_frame))\n    frame_step = max(int(np.round(partial_utterance_n_frames * (1 - overlap))), 1)\n\n    # Compute the slices\n    wav_slices, mel_slices = [], []\n    steps = max(1, n_frames - partial_utterance_n_frames + frame_step + 1)\n    for i in range(0, steps, frame_step):\n        mel_range = np.array([i, i + partial_utterance_n_frames])\n        wav_range = mel_range * samples_per_frame\n        mel_slices.append(slice(*mel_range))\n        wav_slices.append(slice(*wav_range))\n\n    # Evaluate whether extra padding is warranted or not\n    last_wav_range = wav_slices[-1]\n    coverage = (n_samples - last_wav_range.start) / (last_wav_range.stop - last_wav_range.start)\n    if coverage < min_pad_coverage and len(mel_slices) > 1:\n        mel_slices = mel_slices[:-1]\n        wav_slices = wav_slices[:-1]\n\n    return wav_slices, mel_slices\n\n", "CodeBleu_score": 97.67006232562707}, {"code": "def compute_partial_slices(n_samples, partial_utterance_n_frames=partials_n_frames,\n                           min_pad_coverage=0.75, overlap=0.5)\n\t\n    assert 0 < min_pad_coverage <= 1\n    assert 0 <= overlap < 1\n    assert 0 < partial_utterance_n_frames\n    # Compute number of frames per partial utterance\n    partial_utterance_n_frames = min(max(partials_n_frames, int(np.round(partials_n_frames * (1 + overlap)))),\n                                     n_samples)\n    # Compute hop size\n    hop_size = int(np.round(model_sampling_rate * 0.0125))\n    # Compute number of frames per hop\n    frame_step = int(np.round(model_sampling_rate * 0.0125))\n    # Compute number of frames\n    n_frames = int(np.ceil((n_samples + 1) / frame_step))\n    # Compute number of slices\n    n_slices = int(np.ceil((n_frames - partial_utterance_n_frames) / (partial_utterance_n_frames - frame_step))) + 1\n    # Compute the number of samples per slice\n    wav_slices, mel_slices = [], []\n    steps = max(1, n_frames - partial_utterance_n_frames + frame_step + 1)\n    for i in range(0, steps, frame_step):\n        # Compute the starting and ending indices of the slice\n        start = i * hop_size\n        end = min(start + partial_utterance_n_frames, n_samples)\n        # Compute the mel spectrogram of the slice\n        mel_slices.append(audio.wav_to_mel_spectrogram(float_32_audio[start:end]))\n        # Compute the wav slice\n        wav_slices.append(audio[start:end])\n\n    # Evaluate whether extra padding is warranted or not\n    last_wav_slice = wav_slices[-1]\n    coverage = (n_samples - last_wav_slice[-1]) / (last_wav_slice[-1] - last_wav_slice[0])", "CodeBleu_score": 53.55939268536596}, {"code": "def compute_partial_slices(n_samples, partial_utterance_n_frames=partials_n_frames,\n                           min_pad_coverage=0.75, overlap=0.5)\n\t\n    assert 0 < min_pad_coverage <= 1\n    assert 0 <= overlap < 1\n    assert 0 < partial_utterance_n_frames\n    # Compute number of frames per partial utterance\n    partial_utterance_n_frames = int(round((sampling_rate * mel_window_step / 1000 *\n                                            partial_utterance_n_frames)))\n    # Compute the slices\n    wav_slices, mel_slices = [], []\n    steps = max(1, int(partial_utterance_n_frames * (1 - overlap)))\n    for i in range(0, n_samples, steps):\n        mel_range = np.array([i, i + partial_utterance_n_frames])\n        wav_range = mel_range * mel_window_step / 1000\n        mel_slices.append(slice(*mel_range))\n        wav_slices.append(slice(*wav_range))\n    # Evaluate whether extra padding is warranted or not\n    last_wav_range = wav_slices[-1]\n    coverage = (n_samples - last_wav_range.start) / (last_wav_range.stop - last_wav_range.start)\n    if coverage < min_pad_coverage and len(mel_slices) > 1:\n        mel_slices = mel_slices[:-1]\n        wav_slices = wav_slices[:-1]\n    return wav_slices, mel_slices\n\n", "CodeBleu_score": 65.73001861236905}]}
{"instance_id": "34fe5c864f1c", "generation_result": [{"code": "def plot_embedding_as_heatmap(embed, ax=None, title=\"\", shape=None, color_range=(0, 0.30))\n\t\n    if ax is None:\n        ax = plt.gca()\n\n    if shape is None:\n        height = int(np.sqrt(len(embed)))\n        shape = (height, -1)\n\n    embed = embed.reshape(shape)\n\n    cmap = cm.get_cmap()\n    mappable = ax.imshow(embed, cmap=cmap)\n    cbar = plt.colorbar(mappable, ax=ax, fraction=0.046, pad=0.04)\n\n    cbar.set_clim(*color_range)\n    ax.set_title(title)\n    ax.set_xticks([])\n    ax.set_yticks([])\n\n", "CodeBleu_score": 82.92370119435807}, {"code": "def plot_embedding_as_heatmap(embed, ax=None, title=\"\", shape=None, color_range=(0, 0.30))\n\t\n    if ax is None:\n        ax = plt.gca()\n\n    if shape is None:\n        height = int(np.sqrt(len(embed)))\n        shape = (height, -1)\n\n    embed = embed.reshape(shape)\n\n    cmap = cm.get_cmap()\n    mappable = ax.imshow(embed, cmap=cmap)\n    cbar = plt.colorbar(mappable, ax=ax, fraction=0.046, pad=0.04)\n\n    cbar.set_clim(*color_range)\n    ax.remove_xticks()\n    ax.remove_yticks()\n    ax.set_title(title)\n\n", "CodeBleu_score": 80.39159489469004}, {"code": "def plot_embedding_as_heatmap(embed, ax=None, title=\"\", shape=None, color_range=(0, 0.30))\n\t\n    if ax is None:\n        ax = plt.gca()\n\n    if shape is None:\n        height = int(np.sqrt(len(embed)))\n        shape = (height, -1)\n\n    embed = embed.reshape(shape)\n\n    cmap = cm.get_cmap()\n    mappable = ax.imshow(embed, cmap=cmap)\n    cbar = plt.colorbar(mappable, ax=ax, fraction=0.046, pad=0.04)\n\n    cbar.set_clim(*color_range)\n    ax.set_title(title)\n    ax.set_xticks([])\n    ax.set_yticks([])\n\n", "CodeBleu_score": 82.92370119435807}]}
{"instance_id": "92e93d50b62b", "generation_result": [{"code": "def similarity_matrix(self, embeds)\n\t\n    speakers_per_batch, utterances_per_speaker = embeds.shape[0], embeds.shape[1]\n\n    # Inclusive centroids (1 per speaker).\n    centroids_incl = torch.mean(embeds, dim=1, keepdim=True)\n    centroids_incl = centroids_incl / (torch.norm(centroids_incl, dim=2, keepdim=True) + 1e-5)\n\n    # Exclusive centroids (1 per utterance).\n    centroids_excl = (torch.sum(embeds, dim=1, keepdim=True) - embeds) / (\n        speakers_per_batch - 1\n    )\n    centroids_excl = centroids_excl / (torch.norm(centroids_excl, dim=2, keepdim=True) + 1e-5)\n\n    # Similarity matrix. The cosine similarity of already 2-normed embeddings is simply the dot\n    # product of these vectors (which is just an element-wise multiplication reduced by a sum).\n    # We vectorize the computation for efficiency.\n    sim_matrix = torch.zeros(\n        speakers_per_batch, utterances_per_speaker, speakers_per_batch\n    ).to(self.loss_device)\n    mask_matrix = 1 - torch.eye(speakers_per_batch, dtype=torch.int).to(self.loss_device)\n    mask_matrix = mask_matrix.expand(\n        speakers_per_batch, utterances_per_speaker, speakers_per_batch\n    )\n\n    for j in range(speakers_per_batch):\n        mask = mask_matrix[j].view(\n            speakers_per_batch, utterances_per_speaker, 1\n        )\n        mask = mask.type(torch.bool)\n        sim_matrix[mask] = torch.matmul(embeds[mask], centroids_incl[j].transpose(0, 1))\n        sim_matrix[j, :, j] = torch.matmul(embeds[j],", "CodeBleu_score": 42.71316761364752}, {"code": "def similarity_matrix(self, embeds)\n\t\n    speakers_per_batch, utterances_per_speaker = embeds.shape\n    embeds_flat = embeds.view(speakers_per_batch * utterances_per_speaker, -1)\n    centroids_incl = torch.mean(embeds_flat, dim=1, keepdim=True)\n    centroids_excl = torch.mean(embeds_flat, dim=1, keepdim=True)\n    centroids_incl = centroids_incl / torch.norm(centroids_incl, dim=1, keepdim=True)\n    centroids_excl = centroids_excl / torch.norm(centroids_excl, dim=1, keepdim=True)\n    similarity_matrix = torch.zeros(speakers_per_batch * utterances_per_speaker, speakers_per_batch * utterances_per_speaker, device=self.loss_device)\n    mask_matrix = torch.ones_like(similarity_matrix) * -10\n    mask_matrix.to(self.loss_device)\n    for speaker_index in range(speakers_per_batch):\n        for utterance_index in range(utterances_per_speaker):\n            similarity_matrix[speaker_index * utterances_per_speaker + utterance_index, :] = torch.dot(embeds_flat[speaker_index * utterances_per_speaker + utterance_index, :], embeds_flat.T)\n    similarity_matrix = similarity_matrix / self.temperature\n    similarity_matrix = similarity_matrix - mask_matrix\n    similarity_matrix = similarity_matrix.to(self.loss_device)\n    return similarity_matrix\n\n", "CodeBleu_score": 27.634731745141117}, {"code": "def similarity_matrix(self, embeds)\n\t\n    speakers_per_batch, utterances_per_speaker = embeds.shape\n    embeds_flat = embeds.view(speakers_per_batch * utterances_per_speaker, -1)\n    centroids_incl = torch.mean(embeds_flat, dim=1, keepdim=True)\n    centroids_excl = torch.mean(embeds_flat, dim=1, keepdim=True)\n    centroids_excl[range(speakers_per_batch), range(speakers_per_batch)] = float(\"-inf\")\n    centroids_incl = centroids_incl.unsqueeze(0).expand(embeds_flat.shape[0], -1)\n    centroids_excl = centroids_excl.unsqueeze(0).expand(embeds_flat.shape[0], -1)\n    similarities_excl = torch.zeros(embeds_flat.shape[0], embeds_flat.shape[0]).to(self.device)\n    similarities_incl = torch.zeros(embeds_flat.shape[0], embeds_flat.shape[0]).to(self.device)\n    mask_matrix = torch.ones_like(similarities_excl) * -1\n    mask_matrix.to(self.device)\n    for i in range(embeds_flat.shape[0]):\n        row_sim = F.cosine_similarity(embeds_flat[i, :], embeds_flat)\n        similarities_excl[i, :] = row_sim\n        similarities_incl[i, :] = row_sim\n        mask_matrix[i, i] = 1\n    similarities_excl = similarities_excl * mask_matrix\n    similarities_incl = similarities_incl * mask_matrix\n    similarities_excl = similarities_excl / torch.abs(similarities_excl).max()\n    similarities_incl = similarities_incl / torch.abs(similarities_incl).max()\n    return similarities_incl, similarities_excl\n\n", "CodeBleu_score": 27.736198899129093}]}
{"instance_id": "a6eba54108a1", "generation_result": [{"code": "def loss(self, embeds)\n\t\n    speakers_per_batch, utterances_per_speaker = embeds.shape[:2]\n    embeds = embeds.view(\n        speakers_per_batch * utterances_per_speaker, -1\n    )  # shape: (N*M, E)\n    similarities = torch.matmul(embeds, embeds.t())  # shape: (N*M, N*M)\n    similarities = similarities.view(\n        speakers_per_batch, utterances_per_speaker, speakers_per_batch, utterances_per_speaker\n    )  # shape: (N, M, N, M)\n    loss = 0.0\n    num_valid = 0.0\n    for speaker_index in range(speakers_per_batch):\n        pos_index = torch.tensor(\n            range(speaker_index * utterances_per_speaker, (speaker_index + 1) * utterances_per_speaker)\n        )\n        pos_index = pos_index.to(self.device)\n        pos_similarity = torch.index_select(\n            torch.index_select(similarities, 0, pos_index), 1, pos_index\n        )  # shape: (M, M)\n        neg_similarity = torch.cat(\n            [\n                torch.index_select(\n                    torch.index_select(similarities, 0, pos_index), 1, neg_index\n                )\n                for neg_index in range(speakers_per_batch)\n                if neg_index != speaker_index\n            ],\n            dim=1,\n        )  # shape: (M, N*M-M)\n        similarities = torch.cat([pos_similarity, neg_similarity], dim=1)  # shape: (M, N*M)\n        target = torch.zeros(pos_similarity.size()[0]).to(self.device)\n        loss += self.loss_fn(similarities, target)\n        num_valid += pos_similarity.size(0)\n    loss = loss / num_valid\n    return loss\n\n\n", "CodeBleu_score": 29.140018695178593}, {"code": "def loss(self, embeds)\n\t\n    speakers_per_batch, utterances_per_speaker = embeds.shape[:2]\n    embeds = embeds.view(\n        speakers_per_batch * utterances_per_speaker, -1\n    )  # shape : (N, embedding_size)\n    similarity_matrix = self.similarity_weight * torch.matmul(\n        embeds, embeds.T\n    )  # shape : (N, N)\n    similarity_matrix = similarity_matrix.reshape(\n        speakers_per_batch, utterances_per_speaker, speakers_per_batch, utterances_per_speaker\n    )  # shape : (speakers_per_batch, utterances_per_speaker, speakers_per_batch, utterances_per_speaker)\n    similarity_matrix = similarity_matrix.permute(\n        1, 2, 0, 3\n    )  # shape : (utterances_per_speaker, speakers_per_batch, speakers_per_batch, utterances_per_speaker)\n    labels = torch.arange(\n        speakers_per_batch\n    ).unsqueeze(1).expand(\n        utterances_per_speaker, speakers_per_batch\n    ).reshape(\n        -1\n    )  # shape : (utterances_per_speaker * speakers_per_batch, )\n    labels = labels.to(self.loss_device)\n    loss = self.loss_fn(similarity_matrix, labels)\n    with torch.no_grad():\n        preds = similarity_matrix.detach().cpu().numpy().argmax(axis=1)\n        eer = compute_eer(labels.detach().cpu().numpy(), preds)\n    return loss, eer\n\n", "CodeBleu_score": 38.42051026419963}, {"code": "def loss(self, embeds)\n\t\n    speakers_per_batch, utterances_per_speaker = embeds.shape[:2]\n    embeds = embeds.view(\n        speakers_per_batch * utterances_per_speaker, -1\n    )  # (speakers_per_batch * utterances_per_speaker, embed_dim)\n    similarities = torch.matmul(\n        embeds, embeds.t()\n    )  # (speakers_per_batch * utterances_per_speaker, speakers_per_batch * utterances_per_speaker)\n    similarities = similarities.view(\n        speakers_per_batch,\n        utterances_per_speaker,\n        speakers_per_batch,\n        utterances_per_speaker,\n    )  # (speakers_per_batch, utterances_per_speaker, speakers_per_batch, utterances_per_speaker)\n    labels = torch.arange(\n        speakers_per_batch, dtype=torch.long\n    )  # (speakers_per_batch)\n    labels = torch.repeat_interleave(\n        labels, utterances_per_speaker\n    )  # (speakers_per_batch * utterances_per_speaker)\n    labels = labels.to(self.loss_device)  # (speakers_per_batch * utterances_per_speaker)\n    loss, _ = self.loss(similarities, labels)\n    return loss, self.eer(similarities, labels)\n\n", "CodeBleu_score": 28.53088737418126}]}
{"instance_id": "ac7b74b9d1b1", "generation_result": [{"code": "def log_params(self)\n\t\n    if not self.no_visdom:\n        self.viz.text(\n            \"<pre>\"\n            + \"<b>Model parameters</b>", "CodeBleu_score": 5.134441910286239}, {"code": "def log_params(self)\n\t\n    if not self.no_visdom:\n        timestamp = datetime.now().strftime(\"%5H%M%S\")\n        start = timer()\n        param_str = \"<br>\".join([\"%20s - %s\" % (name, param.shape) for name, param in self.model.named_parameters()])\n        self.vis.text(param_str, opts=dict(title=\"Parameters\"), win=\"params\")\n        end = timer()\n        self.logger.info(\"Logged parameter visualizations in %.1f seconds.\", end - start)\n\n", "CodeBleu_score": 20.09826221346371}, {"code": "def log_params(self)\n\t\n    if self.no_visuals:\n        return\n    self.viz.text(\n        \"<pre>\"\n        + \"<b>Model Params</b>\"\n        + \"<br/>\"\n        + self.model.model_params\n        + \"<br/>\"\n        + \"<b>Data Params</b>\"\n        + \"<br/>\"\n        + self.model.data_params\n        + \"</pre>\",\n        win=\"params\",\n    )\n\n", "CodeBleu_score": 11.014945677554486}]}
{"instance_id": "4726b7864386", "generation_result": [{"code": "def draw_projections(self, embeds, utterances_per_speaker, step, out_fpath=None, max_speakers=10)\n\t\n    if self.visdom is None:\n        self.visdom = visdom.Visdom()\n    if self.visdom_loss_plot is None:\n        self.visdom_loss_plot = self.visdom.line(\n            X=np.array([[0, 0]]),\n            Y=np.array([[0, 0]]),\n            opts=dict(\n                xlabel=\"Steps\",\n                ylabel=\"Loss\",\n                title=\"Loss\",\n                legend=[\"Total Loss\", \"Reconstruction Loss\", \"Embedding Loss\"],\n            ),\n        )\n    if self.visdom_embedding_plot is None:\n        self.visdom_embedding_plot = self.visdom.scatter(\n            X=np.random.randn(10, 2),\n            Y=(np.random.randn(10, 2) * 100).astype(int),\n            opts=dict(\n                markersize=5,\n                markercolor=np.random.randint(0, 255, (10, 3)),\n                xtickmin=-5,\n                xtickmax=5,\n                xtickstep=0.5,\n                ytickmin=-5,\n                ytickmax=5,\n                ytickstep=0.5,\n                ztickmin=-5,\n                ztickmax=5,\n                ztickstep=0.5,\n            ),\n        )\n\n    embeds = embeds.detach().cpu().numpy()\n    nrof_speakers = int(embeds.shape[0] / utterances_per_speaker)\n    if nrof_speakers > max_speakers:\n        sel_speaker_idx = np.random.choice(\n            range(nrof_speakers), size=max_speakers, replace=False\n        )\n        embeds = embeds[sel_speaker_idx * utterances_per_speaker : (sel_speaker_idx + 1) * utterances_per_speaker]\n        utterances_per_spe", "CodeBleu_score": 35.26457557896413}, {"code": "def draw_projections(self, embeds, utterances_per_speaker, step, out_fpath=None, max_speakers=10)\n\t\n    embeds = embeds.detach().cpu().numpy()\n    nrof_utterances = utterances_per_speaker.sum().item()\n    utterances_per_speaker = utterances_per_speaker[:max_speakers]\n    embeds = embeds[:max_speakers * utterances_per_speaker.max(), :]\n    labels = []\n    for i, count in enumerate(utterances_per_speaker):\n        labels += [i] * count\n    labels = np.array(labels)\n    nrof_utterances = utterances_per_speaker.sum().item()\n    utterances_per_speaker = utterances_per_speaker[:max_speakers]\n    embeds = embeds[:max_speakers * utterances_per_speaker.max(), :]\n    labels = []\n    for i, count in enumerate(utterances_per_speaker):\n        labels += [i] * count\n    labels = np.array(labels)\n    if embeds.shape[1] == 2:\n        vis = visdom.Visdom()\n        vis.scatter(\n            X=embeds,\n            Y=labels + 1,\n            opts={\n                \"legend\": [\"Speaker \" + str(i) for i in range(len(utterances_per_speaker))],\n                \"markersize\": 2,\n                \"title\": \"UMAP Projections of Speaker Embeddings (Step \" + str(step) + \")\",\n            },\n        )\n    else:\n        reducer = umap.UMAP()\n        projected = reducer.fit_transform(embeds)\n        vis = visdom.Visdom()\n        vis.scatter(\n            X=projected,\n            Y=labels + 1,\n            opts={\n                \"legend\": [\"Speaker \" + str(i) for i in range(len(utterances_per_speaker))],\n                \"markersize\": 2,\n                \"title\": \"UMAP Projections of Speaker Embeddings (Step \" + str(step) + \")\",\n            },\n        )\n\n", "CodeBleu_score": 37.23511044809409}, {"code": "def draw_projections(self, embeds, utterances_per_speaker, step, out_fpath=None, max_speakers=10)\n\t\n    if self.visdom is None:\n        return\n    embeds = embeds.detach().cpu().numpy()\n    nrof_utterances = utterances_per_speaker.sum().item()\n    utterances_per_speaker = utterances_per_speaker[:max_speakers]\n    embeds = embeds[:max_speakers * utterances_per_speaker.max(), :]\n    labels = []\n    indices_to_embed = []\n    start = 0\n    for speaker_id, nrof_utterances in enumerate(utterances_per_speaker):\n        labels.append(\n            [f\"speaker {speaker_id}\"] * nrof_utterances\n        )\n        indices_to_embed.extend(list(range(start, start + nrof_utterances)))\n        start += nrof_utterances\n    indices_to_embed = torch.LongTensor(indices_to_embed)\n    labels = np.array(labels).flatten()\n    reducer = umap.UMAP()\n    projected = reducer.fit_transform(embeds)\n    vis_title = \"UMAP projections\"\n    if step is not None:\n        vis_title = vis_title + \" at step %d\" % step\n    self.visdom.scatter(\n        projected,\n        win=\"umap_projections\",\n        opts=dict(\n            markersize=2,\n            markercolor=np.array([[0, 0, 255]]),\n            xtickmin=-5,\n            xtickmax=5,\n            xtickstep=0.5,\n            ytickmin=-5,\n            ytickmax=5,\n            ytickstep=0.5,\n            markersymbol=\"circle\",\n            textlabels=labels[indices_to_embed],\n            title=vis_title,\n        ),\n    )\n    if out_fpath is not None:\n        np.save(out_fpath, projected)\n\n", "CodeBleu_score": 41.24701987651018}]}
{"instance_id": "274fa50985c0", "generation_result": [{"code": "def add_speaker_embedding(self, x, speaker_embedding)\n\t\n    x_has_batch_dim = x.dim() == 3\n    if not x_has_batch_dim:\n        x = x.unsqueeze(0)\n\n    b, t, c = x.size()\n\n    # Append speaker embedding to input\n    speaker_embedding_size = speaker_embedding.size()\n    if speaker_embedding_size == 1:\n        speaker_embedding = speaker_embedding.repeat(b, t)\n    elif speaker_embedding_size == 2:\n        speaker_embedding = speaker_embedding.repeat(b, 1, t)\n    elif speaker_embedding_size == 3:\n        speaker_embedding = speaker_embedding.repeat(b, 1, t)\n    else:\n        raise ValueError(\n            f\"Speaker embedding has {speaker_embedding_size} dimensions, but should have 1, 2 or 3\"\n        )\n\n    speaker_embedding = speaker_embedding.view(b, -1, t)\n    speaker_embedding = speaker_embedding.transpose(1, 2)\n    x = torch.cat([x, speaker_embedding], dim=-1)\n\n    if not x_has_batch_dim:\n        x = x.squeeze(0)\n    return x\n\n", "CodeBleu_score": 36.609577069511445}, {"code": "def add_speaker_embedding(self, x, speaker_embedding)\n\t\n    dims = self.dims\n    n_ctx = dims[\"n_ctx\"]\n    n_vocab = dims[\"n_vocab\"]\n    n_text_state = dims[\"n_text_state\"]\n    n_text_layer = dims[\"n_text_layer\"]\n    n_text_head = dims[\"n_text_head\"]\n    n_text_ctx = dims[\"n_text_ctx\"]\n    n_audio_state = dims[\"n_audio_state\"]\n    n_audio_layer = dims[\"n_audio_layer\"]\n    n_audio_head = dims[\"n_audio_head\"]\n    n_audio_ctx = dims[\"n_audio_ctx\"]\n    n_audio_state_per_head = dims[\"n_audio_state_per_head\"]\n    n_audio_layer_per_block = dims[\"n_audio_layer_per_block\"]\n    n_audio_block = dims[\"n_audio_block\"]\n    n_audio_head_per_block = dims[\"n_audio_head_per_block\"]\n    n_audio_block_per_group = dims[\"n_audio_block_per_group\"]\n    n_audio_group = dims[\"n_audio_group\"]\n    n_audio_conv_state = dims[\"n_audio_conv_state\"]\n    n_audio_conv_filter = dims[\"n_audio_conv_filter\"]\n    n_audio_conv_head = dims[\"n_audio_conv_head\"]\n    n_audio_conv_layer = dims[\"n_audio_conv_layer\"]\n    n_audio_conv_ctx = dims[\"n_audio_conv_ctx\"]\n    n_audio_conv_stride = dims[\"n_audio_conv_stride\"]\n    n_audio_conv_kernel = dims[\"n_audio_conv_kernel\"]\n    n_audio_conv_dilation = dims[\"n_audio_conv_dilation\"]\n    n_audio_conv_pad = dims[\"n_audio_conv_pad\"]\n    n_audio_conv_groups =", "CodeBleu_score": 4.615466912074676}, {"code": "def add_speaker_embedding(self, x, speaker_embedding)\n\t\n    dims = x.shape\n    speaker_dim_index = dims.index(min(dims))\n    speaker_embedding = speaker_embedding.repeat(dims[speaker_dim_index], 1)\n    speaker_embedding = speaker_embedding.reshape(\n        dims[speaker_dim_index], 1, -1\n    )\n    speaker_embedding = speaker_embedding.transpose(1, 2)\n    x = torch.cat([x, speaker_embedding], dim=-1)\n    return x\n\n", "CodeBleu_score": 18.69306384000409}]}
{"instance_id": "e6742ea0fe5e", "generation_result": [{"code": "def init_attention(self, encoder_seq_proj)\n\t\n    self.device = next(self.parameters()).device\n    self.batch_size, self.seq_length = encoder_seq_proj.shape[:2]\n    self.cumulative = torch.zeros(\n        self.batch_size, self.seq_length, device=self.device\n    )\n    self.attention = torch.zeros(\n        self.batch_size, self.seq_length, device=self.device\n    )\n\n", "CodeBleu_score": 27.884647355851648}, {"code": "def init_attention(self, encoder_seq_proj)\n\t\n    device = next(self.parameters()).device\n    bs, seq_length = encoder_seq_proj.shape[:2]\n    self.cumulative = torch.zeros(bs, seq_length, device=device)\n    self.attention = torch.zeros(bs, seq_length, device=device)\n\n", "CodeBleu_score": 26.056850800814257}, {"code": "def init_attention(self, encoder_seq_proj)\n\t\n    self.device = next(self.parameters()).device\n    self.seq_len = encoder_seq_proj.shape[1]\n    self.cumulative = torch.zeros(\n        (encoder_seq_proj.shape[0], self.seq_len), device=self.device\n    )\n    self.attention = torch.zeros(\n        (encoder_seq_proj.shape[0], self.seq_len), device=self.device\n    )\n\n", "CodeBleu_score": 25.274055551961688}]}
{"instance_id": "4e1fc1ed0ed7", "generation_result": [{"code": "def load(self, path, optimizer=None)\n\t\n    self.device = torch.device(self.parameters()[0].device)\n    ckpt = torch.load(path, map_location=self.device)\n    self.load_state_dict(ckpt[\"model\"])\n    if optimizer is not None:\n        optimizer.load_state_dict(ckpt[\"optimizer\"])\n\n", "CodeBleu_score": 34.21712318680531}, {"code": "def load(self, path, optimizer=None)\n\t\n    self.device = torch.device(self.parameters()[0].device)\n    ckpt = torch.load(path, map_location=self.device)\n    self.load_state_dict(ckpt[\"model\"])\n    if optimizer is not None:\n        optimizer.load_state_dict(ckpt[\"optimizer\"])\n\n", "CodeBleu_score": 34.21712318680531}, {"code": "def load(self, path, optimizer=None)\n\t\n    if not os.path.exists(path):\n        raise FileNotFoundError(f\"File doesn't exist {path}\")\n    device = next(self.parameters()).device  # get model device\n    ckpt = torch.load(path, map_location=device)\n    self.load_state_dict(ckpt[\"model_state\"])\n    if optimizer is not None:\n        optimizer.load_state_dict(ckpt[\"optimizer_state\"])\n\n", "CodeBleu_score": 58.90517224014636}]}
{"instance_id": "178e6aa16c0f", "generation_result": [{"code": "def save(self, path, optimizer=None)\n\t\n    if optimizer is not None:\n        torch.save({\n            'model_state_dict': self.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n        }, path)\n    else:\n        torch.save({\n            'model_state_dict': self.state_dict(),\n        }, path)\n\n", "CodeBleu_score": 69.29600967425841}, {"code": "def save(self, path, optimizer=None)\n\t\n    if optimizer is not None:\n        torch.save({\"model_state_dict\": self.state_dict(), \"optimizer_state_dict\": optimizer.state_dict()}, path)\n    else:\n        torch.save({\"model_state_dict\": self.state_dict()}, path)\n\n", "CodeBleu_score": 61.12069726475413}, {"code": "def save(self, path, optimizer=None)\n\t\n    if optimizer is None:\n        torch.save(self.state_dict(), path)\n    else:\n        torch.save(\n            {\n                \"model_state_dict\": self.state_dict(),\n                \"optimizer_state_dict\": optimizer.state_dict(),\n            },\n            path,\n        )\n\n", "CodeBleu_score": 60.27996569799201}]}
{"instance_id": "6528fd80e87b", "generation_result": [{"code": "def english_cleaners(text)\n\t\n    text = convert_to_ascii(text)\n    text = lowercase(text)\n    text = expand_abbreviations(text)\n    text = expand_numbers(text)\n    text = collapse_whitespace(text)\n    return text\n\n", "CodeBleu_score": 54.508804737500895}, {"code": "def english_cleaners(text)\n\t\n    text = convert_to_ascii(text)\n    text = lowercase(text)\n    text = expand_abbreviations(text)\n    text = expand_numbers(text)\n    text = collapse_whitespace(text)\n    return text\n\n", "CodeBleu_score": 54.508804737500895}, {"code": "def english_cleaners(text)\n\t\n    text = convert_to_ascii(text)\n    text = lowercase(text)\n    text = expand_abbreviations(text)\n    text = expand_numbers(text)\n    text = collapse_whitespace(text)\n    return text\n\n", "CodeBleu_score": 54.508804737500895}]}
{"instance_id": "8a4fe08e0985", "generation_result": [{"code": "def text_to_sequence(text, cleaner_names)\n\t\n    sequence = []\n\n    # Check for curly braces and treat their contents as ARPAbet:\n    while len(text):\n        m = _curly_re.match(text)\n        if not m:\n            sequence += _symbols_to_sequence(_clean_text(text, cleaner_names))\n            break\n        sequence += _symbols_to_sequence(_clean_text(m.group(1), cleaner_names))\n        sequence += _arpabet_to_sequence(m.group(2))\n        text = m.group(3)\n\n    # Append EOS token\n    sequence.append(symbols.index(\"~\"))\n    return sequence\n\n", "CodeBleu_score": 76.93813854311537}, {"code": "def text_to_sequence(text, cleaner_names)\n\t\n    sequence = []\n\n    # Check for curly braces and treat their contents as ARPAbet:\n    while len(text):\n        m = _curly_re.match(text)\n        if not m:\n            sequence += _symbols_to_sequence(_clean_text(text, cleaner_names))\n            break\n        sequence += _symbols_to_sequence(\n            _clean_text(m.group(1), cleaner_names))\n        sequence += _arpabet_to_sequence(m.group(2))\n        text = m.group(3)\n\n    # Append EOS token\n    sequence.append(symbols.index(\"~\"))\n    return sequence\n\n", "CodeBleu_score": 77.723539527195}, {"code": "def text_to_sequence(text, cleaner_names)\n\t\n    sequence = []\n\n    # Check for curly braces and treat their contents as ARPAbet:\n    while len(text):\n        m = re.compile(r\"(?P<curly_braces>[^a-zA-Z\\s\\u00BF\\u00E9\\u00F9\\u00E8\\u00E0\\u00F2\\u00E7\\u00EA\\u00EE\\u00F4\\u00FB\\u00E2\\u00EB\\u00E9\\u00EF\\u00EE\\u00EA\\u00E0\\u00F9]*?)\\{(?P<non_default>[^}]+)\\}(?P<post_non_default>[^a-zA-Z\\s\\u00BF\\u00E9\\u00F9\\u00E8\\u00E0\\u00F2\\u00E7\\u00EA\\u00EE\\u00F4\\u00FB\\u00E2\\u00EB\\u00E9\\u00EF\\u00EE\\u00EA\\u00E0\\u00F9]*)\").match(text)\n        if m is None:\n            break\n        sequence += _symbols_to_sequence(_clean_text(m.group(\"curly_braces\"), cleaner_names))\n        sequence += _arpabet_to_sequence(m.group(\"non_default\"))\n        text = m.group(\"post_non_default\")\n\n    # Append the remaining text symbols:\n    sequence += _symbols_to_sequence(_clean_text(text, cleaner_names))\n\n    # Append the end-of-sequence token:\n    sequence.append(_symbol_to_id[\"~\"])\n    return sequence\n\n", "CodeBleu_score": 44.307460100960064}]}
{"instance_id": "e1c25a37af1f", "generation_result": [{"code": "def sequence_to_text(sequence)\n\t\n    result = ''\n    for symbol_id in sequence:\n        if symbol_id in symbols:\n            s = symbols[symbol_id]\n            # Enclose ARPAbet back in curly braces:\n            if len(s) > 1 and s[0] == '@':\n                s = '{%s}' % s[1:]\n            result += s\n    return result.replace('}{', ' ')\n\n", "CodeBleu_score": 65.50234482206369}, {"code": "def sequence_to_text(sequence)\n\t\n    result = \"\"\n    for symbol_id in sequence:\n        if symbol_id in symbols:\n            s = symbols[symbol_id]\n            # Enclose ARPAbet back in curly braces:\n            if len(s) > 1 and s[0] == \"@\":\n                s = \"{%s}\" % s[1:]\n            result += s\n    return result.replace(\"}{\", \" \")\n\n", "CodeBleu_score": 73.22396630223777}, {"code": "def sequence_to_text(sequence)\n\t\n    result = \"\"\n    for symbol_id in sequence:\n        if symbol_id in symbols:\n            s = symbols[symbol_id]\n            # Enclose ARPAbet back in curly braces:\n            if len(s) > 1 and s[0] == \"@\":\n                s = \"{%s}\" % s[1:]\n            result += s\n    return result.replace(\"}{\", \" \")\n\n", "CodeBleu_score": 73.22396630223777}]}
{"instance_id": "7807cad4b05d", "generation_result": [{"code": "def embed_utterance(fpaths, encoder_model_fpath)\n\t\n    if encoder.is_loaded() is False:\n        encoder.load_model(encoder_model_fpath)\n\n    # Compute the speaker embedding of the utterance\n    wav_fpath, embed_fpath = fpaths\n    wav = librosa.load(wav_fpath, sr=encoder.sampling_rate)[0]\n    wav = encoder.preprocess_wav(wav)\n    embed = encoder.embed_utterance(wav)\n    np.save(embed_fpath, embed, allow_pickle=False)\n\n", "CodeBleu_score": 76.29921893760458}, {"code": "def embed_utterance(fpaths, encoder_model_fpath)\n\t\n    if encoder.is_loaded() is False:\n        encoder.load_model(encoder_model_fpath)\n\n    # Compute the speaker embedding of the utterance\n    wav_fpath, embed_fpath = fpaths\n    wav = audio.preprocess_wav(wav_fpath)\n    embed = encoder.embed_utterance(wav)\n    np.save(embed_fpath, embed, allow_pickle=False)\n\n", "CodeBleu_score": 71.57359634560933}, {"code": "def embed_utterance(fpaths, encoder_model_fpath)\n\t\n    if encoder.is_loaded() is False:\n        encoder.load_model(encoder_model_fpath)\n\n    # Compute the speaker embedding of the utterance\n    wav_fpath, embed_fpath = fpaths\n    wav = librosa.load(wav_fpath, sr=encoder.sampling_rate)[0]\n    wav = encoder.preprocess_wav(wav)\n    embed = encoder.embed_utterance(wav)\n    np.save(embed_fpath, embed, allow_pickle=False)\n\n", "CodeBleu_score": 76.29921893760458}]}
{"instance_id": "9f860273a325", "generation_result": [{"code": "def create_embeddings(synthesizer_root: Path, encoder_model_fpath: Path, n_processes: int)\n\t\n    # Verify we have the correct folders\n    if not encoder_model_fpath.exists():\n        raise Exception(\"Can't find the encoder model: %s\" % encoder_model_fpath)\n    # Create the synthesizer object and load the model\n    synthesizer = Synthesizer(synthesizer_root.joinpath(\"saved_models/default/synthesizer.pt\"))\n    # Load the encoder model\n    encoder.load_model(encoder_model_fpath)\n\n    # Get the reference audio files\n    reference_files = [\n        wav_path for wav_path in wav_dir.glob(\"**/*.wav\") if \"_reference_audio\" in wav_path.name\n    ]\n    if len(reference_files) == 0:\n        raise Exception(\"No reference audio found in the directory %s\" % wav_dir)\n    # Create the reference embeddings\n    reference_embeddings = create_embeddings_helper(\n        encoder, synthesizer, reference_files, n_processes\n    )\n    # Create the test embeddings\n    test_embeddings = create_embeddings_helper(\n        encoder, synthesizer, test_files, n_processes\n    )\n\n    return reference_embeddings, test_embeddings\n\n", "CodeBleu_score": 31.463490202403825}, {"code": "def create_embeddings(synthesizer_root: Path, encoder_model_fpath: Path, n_processes: int)\n\t\n    # Create the synthesizer object and load the model\n    synthesizer = Synthesizer(synthesizer_root.joinpath(\"saved_models/logs-pretrained/taco_pretrained\"), low_mem=False, seed=None)\n\n    # Load the target encoder model\n    encoder.load_model(encoder_model_fpath)\n\n    # Preprocess the dataset\n    print(\"Preprocessing the dataset. This will take a while...\")\n    in_dir = synthesizer_root.joinpath(\"saved_models/default/train\")\n    out_dir = synthesizer_root.joinpath(\"training/\")\n    n_processes = max(1, n_processes)\n    print(\"Found %d .wav files in dataset, processing in %d parallel processes.\" %\n          (len(list(in_dir.glob(\"*.wav\"))), n_processes))\n    assert in_dir.exists() and out_dir.exists()\n    # Create the dataset\n    dataset = audio.PreprocessDataset(in_dir, out_dir, synthesizer_root, None, encoder_model_fpath, False)\n    print(\"Preprocessing done.\")\n\n    # Get the data loader\n    loader = dataset.get_data_loader(\n        synthesizer_root.joinpath(\"datasets/train.txt\"),\n        synthesizer_root.joinpath(\"datasets/val.txt\"),\n        num_workers=n_processes,\n        shuffle=False\n    )\n\n    # Preprocess and embed the dataset\n    embed_dataset(loader, encoder, synthesizer_root, n_processes)\n\n", "CodeBleu_score": 34.802891825748084}, {"code": "def create_embeddings(synthesizer_root: Path, encoder_model_fpath: Path, n_processes: int)\n\t\n    # Create a dataset\n    dataset_root = synthesizer_root.joinpath(\"datasets/SV2TTS/\")\n    meta_file_reader = encoder.inference.utils.Path_Path(dataset_root.joinpath(\"train.txt\"))\n    # Create the output directories for each output model\n    syn_dirs = [dataset_root.joinpath(\"SV2TTS\", \"embeds\")]\n    for syn_dir in syn_dirs:\n        syn_dir.mkdir(exist_ok=True)\n\n    # Gather the input wave filepath and the target output embed filepath\n    with meta_file_reader as filepaths:\n        filepaths = list(filepaths)\n        wave_fpaths = [\n            dataset_root.joinpath(\"wavs\", filepath + \".wav\") for filepath in filepaths\n        ]\n        syn_embed_fpaths = [\n            syn_dir.joinpath(filepath + \".npy\") for filepath in filepaths\n        ]\n\n    # Check for output files\n    for i, syn_embed_fpath in enumerate(syn_embed_fpaths):\n        if syn_embed_fpath.exists():\n            print(\n                f\"Embedding {syn_embed_fpath} already exists. Skipping.\"\n            )  # Prevent overwriting\n            del syn_embed_fpaths[i]\n            del wave_fpaths[i]\n\n    # Preprocess the dataset\n    print(\"Preprocessing the dataset. This will take a while...\")\n    with Pool(n_processes) as pool:\n        for i, _ in tqdm(\n            enumerate(pool.imap(preprocess_dataset, wave_fpaths)),\n            total=len(wave_fpaths),\n        ):\n            pass\n\n    # Train the encoder model\n    print(\"Training the encoder. This will take a while...\")\n    encoder.train_model(encoder_model_fpath)\n\n    # Compute the embeddings\n    print(\"Computing embeddings. This will take a while...\")\n    with Pool(n_processes) as pool:\n        for i, _ in tqdm(\n           ", "CodeBleu_score": 33.3142867467883}]}
{"instance_id": "9eb0abda14a9", "generation_result": [{"code": "def eval_model(attention, mel_prediction, target_spectrogram, input_seq, step,\n               plot_dir, mel_output_dir, wav_dir, sample_num, loss, hparams)\n\t\n\n    # Save attention plot\n    attention_filename = 'attention_step_{}_{}.png'.format(step, sample_num)\n    attention_path = os.path.join(plot_dir, attention_filename)\n    plot_attention(attention, attention_path)\n\n    # Save predicted mel spectrogram\n    mel_filename = 'mel_step_{}_{}.png'.format(step, sample_num)\n    mel_path = os.path.join(mel_output_dir, mel_filename)\n    plot_spectrogram(mel_prediction, mel_path, hparams.mel_min_value, hparams.mel_max_value)\n\n    # Save reconstructed waveform\n    wav_filename = 'wav_step_{}_{}.wav'.format(step, sample_num)\n    wav_path = os.path.join(wav_dir, wav_filename)\n    wav = audio.inv_mel_spectrogram(mel_prediction, hparams)\n    audio.save_wav(wav, wav_path, hparams)\n\n    # Save plot comparing the predicted and target mel spectrograms\n    plot_filename = 'mel_comparison_step_{}_{}.png'.format(step, sample_num)\n    plot_path = os.path.join(plot_dir, plot_filename)\n    plot_spectrogram(target_spectrogram, plot_path, hparams.mel_min_value, hparams.mel_max_value,\n                     colorbar=True)\n    plot_spectrogram(mel_prediction, plot_path, hparams.mel_min_value, hparams.mel_max_value,\n                     colorbar=True,\n                     title='Predicted Mel Spectrogram (Loss: {:.4f})'.format(loss))\n\n    # Print input sequence at current step\n    print('Input sequence: {}'.format(sequence_to_text(input_seq)))\n\n", "CodeBleu_score": 42.21918260829755}, {"code": "def eval_model(attention, mel_prediction, target_spectrogram, input_seq, step,\n               plot_dir, mel_output_dir, wav_dir, sample_num, loss, hparams)\n\t\n    # Save attention plot\n    attention_path = Path(plot_dir, f'attention_{step}_{sample_num}.png')\n    plot_attention(attention, attention_path)\n\n    # Save predicted mel spectrogram\n    mel_prediction_path = Path(mel_output_dir, f'mel_prediction_{step}_{sample_num}.npy')\n    np.save(mel_prediction_path, mel_prediction)\n\n    # Save reconstructed waveform\n    wav_path = Path(wav_dir, f'{step}_{sample_num}.wav')\n    wav = audio.inv_mel_spectrogram(mel_prediction, hparams)\n    audio.save_wav(wav, wav_path)\n\n    # Generate and save plot comparing predicted and target mel spectrograms\n    plot_mel_path = Path(plot_dir, f'mel_comparison_{step}_{sample_num}.png')\n    plot_spectrogram(target_spectrogram, mel_prediction, plot_mel_path,\n                     title=f'{hparams.model_type} {datetime.now().strftime(\"%H:%M\")} Step {step} Loss {loss}')\n\n    # Print input sequence at current step\n    print(f'Input: {sequence_to_text(input_seq)}')\n\n", "CodeBleu_score": 31.64704681750028}, {"code": "def eval_model(attention, mel_prediction, target_spectrogram, input_seq, step,\n               plot_dir, mel_output_dir, wav_dir, sample_num, loss, hparams)\n\t\n    # Save attention plot\n    attention_path = Path(plot_dir, f\"attention_step_{step}.png\")\n    plot_spectrogram(attention, attention_path)\n\n    # Save predicted mel spectrogram\n    mel_output_path = Path(mel_output_dir, f\"mel_step_{step}.png\")\n    plot_spectrogram(mel_prediction, mel_output_path)\n\n    # Save reconstructed waveform\n    wav_path = Path(wav_dir, f\"wav_step_{step}.wav\")\n    audio.save_wav(wav_path, audio.reconstruct_waveform(mel_prediction, hparams))\n\n    # Save plot comparing predicted and target mel spectrograms\n    plot_mel_path = Path(mel_output_dir, f\"mel_comparison_step_{step}.png\")\n    plot_spectrogram(target_spectrogram, plot_mel_path,\n                     predicted_spectrogram=mel_prediction,\n                     title=f\"{hparams.model_type} {datetime.now()} Step {step} Loss {loss}\")\n\n    # Print input sequence at current step\n    print(f\"Input: {sequence_to_text(input_seq[sample_num])}\")\n\n", "CodeBleu_score": 29.747809880904708}]}
{"instance_id": "6bad4f500286", "generation_result": [{"code": "def num_params(self)\n\t\n    model_params = filter(lambda p: p.requires_grad, self.parameters())\n    params = sum([np.prod(p.size()) for p in model_params])\n    print(\"Trainable parameters: {}\".format(params / 1e6))\n\n", "CodeBleu_score": 36.822000178624066}, {"code": "def num_params(self)\n\t\n    model_parameters = filter(lambda p: p.requires_grad, self.parameters())\n    params = sum([np.prod(p.size()) for p in model_parameters])\n    print(\"Trainable parameters: {}\".format(params / 1e6))\n\n", "CodeBleu_score": 38.23415227172664}, {"code": "def num_params(self)\n\t\n    model_parameters = filter(lambda p: p.requires_grad, self.parameters())\n    params = sum([np.prod(p.size()) for p in model_parameters])\n    print(\"Number of trainable parameters: %.3fM\" % (params / 1e6))\n\n", "CodeBleu_score": 38.88307277312004}]}
{"instance_id": "78b876054bd4", "generation_result": [{"code": "def pad_tensor(self, x, pad, side='both')\n\t\n    if pad > 0:\n        if side == 'both':\n            x = torch.cat((x, x.new_zeros(x.size(0), pad * 2)), 1)\n        elif side == 'after':\n            x = torch.cat((x, x.new_zeros(x.size(0), pad)), 1)\n        elif side == 'before':\n            x = torch.cat((x.new_zeros(x.size(0), pad), x), 1)\n        else:\n            raise ValueError(\"Padding side must be one of 'before', 'after' or 'both'\")\n    return x\n\n", "CodeBleu_score": 30.66450884513028}, {"code": "def pad_tensor(self, x, pad, side='both')\n\t\n    # Calculate the new size of the tensor after padding\n    if side == 'both':\n        new_size = x.shape[0] + pad * 2\n    elif side == 'before':\n        new_size = x.shape[0] + pad\n    else:\n        new_size = x.shape[0]\n\n    # Create a new tensor filled with zeros\n    if x.is_cuda:\n        # If the tensor is on the GPU, create a new tensor on the GPU\n        new_x = torch.zeros(new_size, x.shape[1], dtype=x.dtype, device=x.device)\n    else:\n        # If the tensor is on the CPU, create a new tensor on the CPU\n        new_x = torch.zeros(new_size, x.shape[1], dtype=x.dtype)\n\n    # Place the original tensor into the new padded tensor\n    if side == 'both':\n        new_x[pad:pad + x.shape[0]] = x\n    elif side == 'before':\n        new_x[:x.shape[0]] = x\n    else:\n        new_x[-x.shape[0]:] = x\n\n    return new_x\n\n", "CodeBleu_score": 33.2851023597654}, {"code": "def pad_tensor(self, x, pad, side='both')\n\t\n    # Calculate the total size of the new tensor after padding\n    if side == 'both':\n        new_size = x.size(1) + pad * 2\n    elif side == 'before':\n        new_size = x.size(1) + pad\n    elif side == 'after':\n        new_size = x.size(1) + pad\n    else:\n        raise ValueError(\"side must be 'before', 'after', or 'both'\")\n\n    # Create a new tensor filled with zeros on either the CPU or GPU\n    if x.is_cuda:\n        new_x = torch.zeros((x.size(0), new_size)).cuda()\n    else:\n        new_x = torch.zeros((x.size(0), new_size))\n\n    # Place the original tensor into the new padded tensor\n    if side == 'both':\n        new_x[:, pad:pad + x.size(1)] = x\n    elif side == 'before':\n        new_x[:, :x.size(1)] = x\n    elif side == 'after':\n        new_x[:, pad:] = x\n\n    return new_x\n\n", "CodeBleu_score": 36.00491031797395}]}
{"instance_id": "a7ed1715a3df", "generation_result": [{"code": "def save_attention(attn, path)\n\t\n    import matplotlib.pyplot as plt\n    import matplotlib.ticker as ticker\n\n    # This line ensures that the attention matrix is a 2D array.\n    attn = attn.squeeze(0).cpu().detach().numpy()\n\n    # This block of code sets up the figure and axes for the plot.\n    fig = plt.figure(figsize=(10, 10))\n    ax = fig.add_subplot(1, 1, 1)\n    ax.matshow(attn, cmap=\"viridis\")\n\n    # This block of code sets the font size and orientation for the tick labels.\n    fontdict = {\"fontsize\": 14}\n    ax.set_xticklabels([\"<sos>\"] + [\"TGT_{}\".format(i) for i in range(1, 16)] + [\"<eos>\"], rotation=90)\n    ax.set_yticklabels([\"<sos>\"] + [\"SRC_{}\".format(i) for i in range(1, 16)] + [\"<eos>\"])\n\n    # This block of code sets the tick label font size and alignment.\n    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n    ax.yaxis.set_major_formatter(\n        ticker.FuncFormatter(lambda x, pos: \"SRC_{}\".format(pos + 1))\n    )\n    ax.xaxis.set_major_formatter(\n        ticker.FuncFormatter(lambda x, pos: \"TGT_{}\".format(pos + 1))\n    )\n\n    # This block of code sets the tick label font size and alignment.\n    for label in ax.get_xticklabels() + ax.get_yticklabels():\n        label.set_fontsize(14)\n        label.set_bbox(dict(facecolor=\"white\", edgecolor=\"black\", pad=0.6))\n\n    # This block of code saves the plot", "CodeBleu_score": 46.01465667536095}, {"code": "def save_attention(attn, path)\n\t\n    import matplotlib.pyplot as plt\n\n    # This line ensures that the attention matrix is a 2D numpy array.\n    attn = attn.squeeze()\n\n    # This line ensures that the attention matrix is a 2D numpy array.\n    assert len(attn.shape) == 2\n\n    # This line ensures that the attention matrix is a 2D numpy array.\n    fig = plt.figure(figsize=(10, 10))\n\n    # This line ensures that the attention matrix is a 2D numpy array.\n    plt.imshow(attn, interpolation=\"nearest\", cmap=\"Blues\")\n\n    # This line ensures that the attention matrix is a 2D numpy array.\n    fig.savefig(path, bbox_inches=\"tight\")\n\n    # This line ensures that the attention matrix is a 2D numpy array.\n    plt.close(fig)\n\n", "CodeBleu_score": 43.968311891734565}, {"code": "def save_attention(attn, path)\n\t\n    import matplotlib.pyplot as plt\n\n    # This line ensures that the attention matrix is a 2D numpy array.\n    attn = attn.squeeze()\n\n    # This line determines the size of the figure based on the size of the attention matrix.\n    figsize = 1.0 * len(attn) / len(attn[0])\n\n    # This line creates a new figure with the specified size and resolution.\n    fig = plt.figure(figsize=(figsize, figsize), dpi=100)\n\n    # This line ensures that the axes of the figure are square, with a minimum value of 0 and a maximum value of 1.\n    ax = fig.add_subplot(111)\n    ax.matshow(attn, cmap=\"viridis\")\n\n    # This line sets the font size of the x-axis tick labels to 10.\n    fontdict = {\"fontsize\": 10}\n\n    # This line sets the font properties of the x-axis tick labels to the font dictionary defined earlier.\n    ax.set_xticklabels([\" \"] + list(range(1, len(attn) + 1)), fontdict=fontdict, rotation=90)\n\n    # This line sets the font properties of the y-axis tick labels to the font dictionary defined earlier.\n    ax.set_yticklabels([\" \"] + list(range(1, len(attn) + 1)), fontdict=fontdict)\n\n    # This line saves the figure to the specified path with a .png extension.\n    plt.savefig(path, format=\"png\")\n\n    # This line closes the figure.\n    plt.close()\n\n", "CodeBleu_score": 41.83755668904348}]}
{"instance_id": "19375c9f44a2", "generation_result": [{"code": "def plot(array)\n\t\n    import matplotlib.pyplot as plt\n    plt.figure(figsize=(10, 5))\n    plt.subplot(1, 1, 1)\n    plt.plot(array, color='black', linewidth=0.5)\n    plt.xlabel('x', color='black', fontsize=14)\n    plt.ylabel('y', color='black', fontsize=14)\n    plt.xticks(color='black', fontsize=12)\n    plt.yticks(color='black', fontsize=12)\n    plt.show()\n\n", "CodeBleu_score": 29.22506936499449}, {"code": "def plot(array)\n\t\n    import matplotlib.pyplot as plt\n    plt.figure(figsize=(15, 5))\n    plt.subplot(1, 1, 1)\n    plt.xlabel('Array Index', color='blue', fontsize=14)\n    plt.ylabel('Array Value', color='blue', fontsize=14)\n    plt.tick_params(axis='x', colors='blue', labelsize=12)\n    plt.tick_params(axis='y', colors='blue', labelsize=12)\n    plt.plot(array, color='red', marker='o', linestyle='-', linewidth=2)\n    plt.show()\n\n", "CodeBleu_score": 35.39447532317318}, {"code": "def plot(array)\n\t\n    import matplotlib.pyplot as plt\n    plt.figure(figsize=(10, 5))\n    plt.subplot(1, 1, 1)\n    plt.xlabel('X', color='white', fontsize=18)\n    plt.ylabel('Y', color='white', fontsize=18)\n    plt.xticks(color='white', fontsize=14)\n    plt.yticks(color='white', fontsize=14)\n    plt.plot(array)\n    plt.show()\n\n", "CodeBleu_score": 30.870253104943995}]}
{"instance_id": "ff94c425920c", "generation_result": [{"code": "def write_to_csv(image_name, prediction, confidence)\n\t\n    data = {\"image_name\": image_name, \"prediction\": prediction, \"confidence\": confidence}\n    with open(\"predictions.csv\", \"a\") as f:\n        writer = csv.DictWriter(f, fieldnames=data.keys())\n        if f.tell() == 0:\n            writer.writeheader()\n        writer.writerow(data)\n\n", "CodeBleu_score": 60.74020704325842}, {"code": "def write_to_csv(image_name, prediction, confidence)\n\t\n    data = {\"image_name\": image_name, \"prediction\": prediction, \"confidence\": confidence}\n    with open(\"predictions.csv\", mode=\"a\", newline=\"\") as file:\n        writer = csv.DictWriter(file, fieldnames=[\"image_name\", \"prediction\", \"confidence\"])\n        if file.tell() == 0:\n            writer.writeheader()\n        writer.writerow(data)\n\n", "CodeBleu_score": 52.40140476792803}, {"code": "def write_to_csv(image_name, prediction, confidence)\n\t\n    # Create a dictionary with the image name, prediction, and confidence\n    data = {\"image_name\": image_name, \"prediction\": prediction, \"confidence\": confidence}\n\n    # Open the CSV file in append mode\n    with open(\"predictions.csv\", \"a\", newline=\"\") as csvfile:\n        # Create a CSV writer object\n        writer = csv.DictWriter(csvfile, fieldnames=[\"image_name\", \"prediction\", \"confidence\"])\n\n        # Write the header if the file is new\n        if csvfile.tell() == 0:\n            writer.writeheader()\n\n        # Write the data as a new row\n        writer.writerow(data)\n\n", "CodeBleu_score": 50.52638633554765}]}
{"instance_id": "436161602009", "generation_result": [{"code": "def export_torchscript(model, im, file, optimize, prefix=colorstr(\"TorchScript:\"))\n\t\n    f = file.with_suffix(\".torchscript.pt\")  # add suffix\n    LOGGER.info(f\"{prefix} starting export with torch {torch.__version__}...\")\n    try:\n        LOGGER.info(f\"{prefix} export success \u2705\")\n        ts = torch.jit.trace(model, im)\n        if optimize:\n            try:\n                LOGGER.info(f\"{prefix} optimizing with torch {torch.__version__}...\")\n                ts = optimize_for_mobile(ts)\n            except Exception as e:\n                LOGGER.info(f\"{prefix} optimize failure \u274c {e}\")\n        with open(f, \"wb\") as f:\n            torch.jit.save(ts, f)\n        LOGGER.info(f\"{prefix} saved as {f} ({file_size(f):.1f} MB)\")\n        return f, None\n    except Exception as e:\n        LOGGER.info(f\"{prefix} export failure \u274c {e}\")\n        return f, e\n\n", "CodeBleu_score": 38.97332470892202}, {"code": "def export_torchscript(model, im, file, optimize, prefix=colorstr(\"TorchScript:\"))\n\t\n    try:\n        LOGGER.info(f\"", "CodeBleu_score": 9.912281340461456}, {"code": "def export_torchscript(model, im, file, optimize, prefix=colorstr(\"TorchScript:\"))\n\t\n    try:\n        LOGGER.info(f\"", "CodeBleu_score": 9.912281340461456}]}
{"instance_id": "78335487ac1c", "generation_result": [{"code": "def export_coreml(model, im, file, int8, half, nms, prefix=colorstr(\"CoreML:\")):\n\t\n    try:\n        import coremltools as ct\n\n        LOGGER.info(f\"", "CodeBleu_score": 9.958821115096491}, {"code": "def export_coreml(model, im, file, int8, half, nms, prefix=colorstr(\"CoreML:\")):\n\t\n    try:\n        import coremltools as ct\n\n        LOGGER.info(f\"{prefix} starting export with torch {torch.__version__}...\")\n        f = file.with_suffix(\".mlmodel\")  # filename\n        model.model[-1].export = True  # set Detect() layer export=True\n        model = model.model\n        model.eval()\n        for k, m in model.named_modules():\n            if isinstance(m, Detect):\n                m.inplace = False\n        model = ct.convert(\n            model,\n            inputs=[ct.ImageType(name=\"images\", shape=im.shape)],\n        )\n        if nms:\n            model = ct.models.neural_network.NeuralNetwork.deserialize(model.get_spec())\n            model = ct.models.ml_program.MLProgram.deserialize(model.get_spec())\n            model.spec.description.output[0].type.multiArrayType.shape.extend([1, 100, 6])\n            model.spec.description.output[0].type.multiArrayType.dataType = f\"Array{ct.TensorType.FLOAT32()}\"\n            model = ct.models.neural_network.NeuralNetwork.deserialize(model.get_spec())\n            model = ct.models.ml_program.MLProgram.deserialize(model.get_spec())\n            model = ct.models.neural_network.NeuralNetwork.deserialize(model.get_spec())\n            model = ct.models.ml_program.MLProgram.deserialize(model.get_spec())\n            model = ct.models.neural_network.NeuralNetwork.deserialize(model.get_spec())\n            model = ct.models.ml_program.MLProgram.deserialize(model.get_spec())\n            model = ct.models.neural_network.NeuralNetwork.deserialize(model.get_spec())\n            model = ct.models.ml_program.MLProgram.deserialize(model.get_spec())\n            model = c", "CodeBleu_score": 35.84961146377975}, {"code": "def export_coreml(model, im, file, int8, half, nms, prefix=colorstr(\"CoreML:\")):\n\t\n    try:\n        import coremltools as ct\n\n        LOGGER.info(f\"{prefix} starting export with torch {torch.__version__}...\")\n        f = file.with_suffix(\".mlmodel\")  # filename\n        model.eval()\n        for k, m in model.named_modules():\n            if isinstance(m, Detect):  # export Detect() layer\n                m.inplace = False\n                m.export = True  # onnx: operators export to onnx\n                m.dynamic = False  # ONNX: force fixed grid sampling\n                m.grid = [1] * len(m.grid)  # onnx: grid export\n                m.flatten = False  # onnx: flatten spatial dims\n                m.export_coreml = True  # coreml: convert to CoreML\n            if isinstance(m, SegmentationModel):  # upsample both segmentation and proto\n                m.export_proto = True\n            if isinstance(m, ClassificationModel):  # upsample both classification and proto\n                m.export_proto = True\n\n        if nms:\n            model = nms_wrapper(model)  # CoreML NMS\n\n        model.model[-1].export = True  # set Detect() layer export=True\n        model.eval()\n        for k, m in model.named_modules():\n            if isinstance(m, Detect):\n                if not isinstance(m.anchor_grid, list):\n                    m.anchor_grid = m.anchor_grid.cpu().float().view(1, -1, 1, 1, 2).to(model.device)\n                    m.anchor_grid = m.anchor_grid.type(model.dtype)\n                    m.anchor_grid.requires_grad_(False)\n\n        model.model[-1].inplace = False\n        y = model(im)  # dry run\n        y = y[0]\n        if int8:\n            LOGGER.info(f\"{prefix} CoreML INT8 Quantization enabled...\")\n            model.model[-1].qconfig = torch.quantization.get_default_qconfig(\"qnn", "CodeBleu_score": 32.75625142388224}]}
{"instance_id": "80233cf71580", "generation_result": [{"code": "def generate_individual(input_ranges, individual_length)\n\t\n    individual = []\n    for i in range(individual_length):\n        individual.append(random.uniform(input_ranges[i][0], input_ranges[i][1]))\n    return individual\n\n", "CodeBleu_score": 47.630972577458174}, {"code": "def generate_individual(input_ranges, individual_length)\n\t\n    individual = []\n    for i in range(individual_length):\n        individual.append(random.uniform(input_ranges[i][0], input_ranges[i][1]))\n    return individual\n\n", "CodeBleu_score": 47.630972577458174}, {"code": "def generate_individual(input_ranges, individual_length)\n\t\n    individual = []\n    for i in range(individual_length):\n        individual.append(random.uniform(input_ranges[i][0], input_ranges[i][1]))\n    return individual\n\n", "CodeBleu_score": 47.630972577458174}]}
{"instance_id": "5ea979cb4a8e", "generation_result": [{"code": "def marginal_log_mean_coeff(self, t)\n\t\n    if self.schedule == \"discrete\":\n        return self.log_alpha_t[t]\n    elif self.schedule == \"linear\":\n        return self.log_alpha_t[t]\n    elif self.schedule == \"cosine\":\n        t_ratio = t / self.T\n        return 0.5 * self.log_alpha_t[0] * (1.0 - torch.cos(torch.pi * t_ratio))\n    else:\n        raise ValueError(\"Invalid schedule type\")\n\n", "CodeBleu_score": 27.709361205066497}, {"code": "def marginal_log_mean_coeff(self, t)\n\t\n    if self.schedule_type == 'discrete':\n        return torch.log(self.alphas[t])\n    elif self.schedule_type == 'linear':\n        return torch.log(self.betas[t]) - math.log(self.num_diffusion_timesteps)\n    elif self.schedule_type == 'cosine':\n        t = t / self.num_diffusion_timesteps\n        alphas_cumprod_prev = self.alphas_cumprod[t]\n        alphas_cumprod_next = self.alphas_cumprod[t + 1]\n        alpha = (alphas_cumprod_prev + 0.0) / (1.0 - alphas_cumprod_prev) * (\n            1.0 - alphas_cumprod_next\n        )\n        return torch.log(alpha)\n    else:\n        raise NotImplementedError\n\n", "CodeBleu_score": 37.64194386334845}, {"code": "def marginal_log_mean_coeff(self, t)\n\t\n\n    # If the schedule type is 'discrete', interpolate the values of alpha_t at discrete time steps\n    if self.schedule_type == \"discrete\":\n        log_alpha_t = torch.lerp(\n            self.log_alpha_t[t], self.log_alpha_t[t + 1], self.discrete_steps[t + 1]\n        )\n\n    # If the schedule type is 'linear', use a quadratic expression to calculate alpha_t\n    elif self.schedule_type == \"linear\":\n        beta_t = t * (1.0 - self.alpha_cumprod_inv[t])\n        alpha_t = self.alpha_cumprod_inv[t] * (1.0 - beta_t)\n        alpha_t_bar = torch.sqrt(alpha_t**2 + self.beta_cumprod_inv[t])\n        log_alpha_t = torch.log(alpha_t) - torch.log(alpha_t_bar)\n\n    # If the schedule type is 'cosine', apply a cosine-based formula to calculate alpha_t\n    elif self.schedule_type == \"cosine\":\n        log_alpha_t = torch.log(\n            self.alpha_cumprod_inv[t]\n        ) + self.log_alpha_t[t + 1]\n\n    # Return the log of alpha_t\n    return log_alpha_t\n\n", "CodeBleu_score": 39.501555625597604}]}
{"instance_id": "7da4fae5a278", "generation_result": [{"code": "def inverse_lambda(self, lamb)\n\t\n    if self.schedule == \"linear\":\n        # Calculate t using beta values and a temporary variable\n        t = (1 - self.beta_0) * lamb + self.beta_0 * self.T\n    elif self.schedule == \"quadratic\":\n        # Calculate t using beta values and a temporary variable\n        t = (1 - self.beta_0) * lamb + self.beta_0 * self.T\n    elif self.schedule == \"sigmoid\":\n        # Calculate t using beta values and a temporary variable\n        t = (1 - self.beta_0) * lamb + self.beta_0 * self.T\n    elif self.schedule == \"cosine\":\n        # Calculate t using beta values and a temporary variable\n        t = (1 - self.beta_0) * lamb + self.beta_0 * self.T\n    elif self.schedule == \"jsd\":\n        # Calculate t using beta values and a temporary variable\n        t = (1 - self.beta_0) * lamb + self.beta_0 * self.T\n    elif self.schedule == \"step\":\n        # Calculate t using beta values and a temporary variable\n        t = (1 - self.beta_0) * lamb + self.beta_0 * self.T\n    elif self.schedule == \"vp\":\n        # Calculate t using beta values and a temporary variable\n        t = (1 - self.beta_0) * lamb + self.beta_0 * self.T\n    elif self.schedule == \"vp-linear\":\n        # Calculate t using beta values and a temporary variable\n        t = (1 - self.beta_0) * lamb + self.beta_0 * self.T\n    elif self.schedule == \"vp-spaced\":\n        # Calculate t using beta values and a temporary variable\n        t = (1 - self.beta_0) * lamb + self.beta_0 * self.T\n    elif self.schedule == \"vp-double\":\n        # Calculate t using beta values and a temporary variable\n        t = (1 - self.beta_0) * lamb + self.beta_0 * self.T\n    elif self.", "CodeBleu_score": 19.009055971210383}, {"code": "def inverse_lambda(self, lamb)\n\t\n    if self.schedule == \"linear\":\n        if self.schedule_mode == \"linear_beta\":\n            # Linear schedule with beta values\n            beta_t = self.schedule_params[\"beta_t\"]\n            beta_0 = self.schedule_params[\"beta_0\"]\n            beta_T = self.schedule_params[\"beta_T\"]\n            t = (beta_T ** 2 - lamb ** 2) / (2 * (beta_T ** 2 - beta_0 ** 2))\n        else:\n            # Linear schedule with temporary variable\n            tmp = self.schedule_params[\"tmp\"]\n            t = (1 + tmp ** 2) / (2 * tmp) * torch.arccos(tmp / torch.sqrt(tmp ** 2 + lamb ** 2))\n    elif self.schedule == \"quadratic\":\n        # Quadratic schedule\n        alpha_t = self.schedule_params[\"alpha_t\"]\n        alpha_0 = self.schedule_params[\"alpha_0\"]\n        t = (torch.sqrt(alpha_t ** 2 + 4 * alpha_0 * lamb) - alpha_t) / (2 * alpha_0)\n    elif self.schedule == \"sigmoid\":\n        # Sigmoid schedule\n        alpha_t = self.schedule_params[\"alpha_t\"]\n        alpha_0 = self.schedule_params[\"alpha_0\"]\n        t = torch.log(lamb + torch.sqrt(lamb ** 2 + 4 * alpha_0 * alpha_t)) / (2 * alpha_0)\n    elif self.schedule == \"cosine\":\n        # Cosine schedule\n        log_alpha_t = self.schedule_params[\"log_alpha_t\"]\n        log_alpha_0 = self.schedule_params[\"log_alpha_0\"]\n        t = (torch.arccos(lamb / torch.exp(log_alpha_t)) / math.pi) * (\n            log_alpha_t - log_alpha_0\n        ) + log_alpha_0\n    else:\n        raise ValueError(f\"Unknown schedule: {self.", "CodeBleu_score": 26.902053815788424}, {"code": "def inverse_lambda(self, lamb)\n\t\n    if self.schedule == \"linear\":\n        beta = self.schedule_params[\"beta\"]\n        temp = (self.schedule_params[\"T\"] ** 2 - lamb ** 2) / beta\n        t = (temp + math.sqrt(temp ** 2 + 4 * self.schedule_params[\"T\"] * beta)) / 2\n    elif self.schedule == \"discrete\":\n        log_alpha = torch.log(self.schedule_params[\"alpha\"])\n        t = torch.exp(torch.logsumexp(log_alpha, dim=0, keepdim=True) - lamb)\n    else:\n        log_alpha = torch.log(self.schedule_params[\"alpha\"])\n        t = torch.exp(torch.logsumexp(log_alpha, dim=0, keepdim=True) - lamb)\n        t = torch.arccos(1 - 2 * t) / math.pi\n    return t\n\n", "CodeBleu_score": 26.896282979614742}]}
{"instance_id": "a519c853a2b2", "generation_result": [{"code": "def noise_pred_fn(x, t_continuous, cond=None)\n\t\n    batch_size = x.shape[0]\n    t_expanded = t_continuous.expand(batch_size, -1)\n    if cond is not None:\n        cond = cond.expand(batch_size, -1)\n    if model_type == \"score\":\n        model_output = model(x, t_expanded, cond)\n        noise_pred = model_output\n    elif model_type == \"diffusion\":\n        model_output = model(x, t_expanded, cond)\n        noise_pred = model_output\n    elif model_type == \"hybrid\":\n        model_output = model(x, t_expanded, cond)\n        noise_pred = model_output\n    elif model_type == \"noise\":\n        model_output = model(x, t_expanded, cond)\n        noise_pred = model_output\n    elif model_type == \"v\":\n        model_output = model(x, t_expanded, cond)\n        noise_pred = model_output\n    elif model_type == \"v_hybrid\":\n        model_output = model(x, t_expanded, cond)\n        noise_pred = model_output\n    else:\n        raise ValueError(f\"Unknown model type: {model_type}\")\n\n    if model_type in [\"score\", \"hybrid\", \"v\", \"v_hybrid\"]:\n        noise_pred = noise_pred * sigma_t_expanded\n    elif model_type in [\"diffusion\", \"noise\"]:\n        noise_pred = noise_pred * (sigma_t_expanded**2 / 2)\n\n    return noise_pred\n\n", "CodeBleu_score": 25.51426662790916}, {"code": "def noise_pred_fn(x, t_continuous, cond=None)\n\t\n    batch_size = x.shape[0]\n    t_expanded = t_continuous.expand(batch_size, -1)\n    if cond is not None:\n        cond_expanded = cond.expand(batch_size, -1)\n        out = model(x, t_expanded, cond_expanded)\n    else:\n        out = model(x, t_expanded)\n    if model_type == \"score\":\n        out = -out / sigma(t_continuous)\n    elif model_type == \"hybrid\":\n        out = out / sigma(t_continuous)\n    return out\n\n", "CodeBleu_score": 21.12461236538994}, {"code": "def noise_pred_fn(x, t_continuous, cond=None)\n\t\n    if cond is not None:\n        cond = cond.to(x.device)\n    if t_continuous.ndim == 0:\n        t_continuous = t_continuous.repeat(x.shape[0])\n    t_continuous = t_continuous.to(x.device)\n    t_continuous = t_continuous.unsqueeze(-1)\n    if t_continuous.shape[0] != x.shape[0]:\n        t_continuous = t_continuous.repeat(x.shape[0], 1)\n    if model_type == 'score':\n        model_output = model(x, t_continuous, cond)\n    elif model_type == 'diffusion':\n        model_output = model(x, t_continuous)\n    elif model_type == 'noise':\n        model_output = model(x, t_continuous)\n    elif model_type == 'v':\n        model_output = model(x, t_continuous)\n    elif model_type == 'v_pred':\n        model_output = model(x, t_continuous)\n    else:\n        raise ValueError(f'Unknown model type: {model_type}')\n\n    if model_type in ['score', 'v_pred']:\n        return model_output\n    elif model_type == 'diffusion':\n        return model_output * (-0.5 * t_continuous).exp()\n    elif model_type == 'noise':\n        return model_output * (-t_continuous).exp()\n    elif model_type == 'v':\n        return model_output * t_continuous.exp()\n\n", "CodeBleu_score": 31.267417537402732}]}
{"instance_id": "96a9b7d54823", "generation_result": [{"code": "def dynamic_thresholding_fn(self, x0, t=None)\n\t\n    if t is None:\n        t = self.t\n    x0 = x0.reshape(-1)\n    q = torch.quantile(torch.abs(x0), t)\n    s = q / self.threshold\n    s = torch.max(s, torch.ones_like(s))\n    x0 = torch.clamp(x0, min=-self.threshold * s, max=self.threshold * s)\n    x0 = x0 / s\n    return x0\n\n", "CodeBleu_score": 42.054562819430394}, {"code": "def dynamic_thresholding_fn(self, x0, t=None)\n\t\n    if t is None:\n        t = self.t\n    x0 = x0.reshape(-1)\n    x0 = x0.abs()\n    x0 = x0.sort().values\n    q = x0[int(len(x0) * t)]\n    s = q / self.s\n    x0 = x0.clamp(max=q)\n    x0 = x0 / s\n    return x0\n\n", "CodeBleu_score": 33.21459347048908}, {"code": "def dynamic_thresholding_fn(self, x0, t=None)\n\t\n    if t is None:\n        t = self.t\n    x0 = x0.reshape(-1)\n    x0 = x0.abs()\n    x0 = x0.sort()[0]\n    x0 = x0[int(math.ceil(t * x0.numel()))]\n    x0 = x0.clamp(min=1e-8)\n    x0 = x0.reciprocal()\n    x0 = x0 * x0.numel()\n    x0 = x0.sqrt()\n    x0 = x0 * x0\n    x0 = x0.reshape(self.shape)\n    return x0\n\n", "CodeBleu_score": 35.17244387906481}]}
{"instance_id": "59c6ec6d7a6a", "generation_result": [{"code": "def data_prediction_fn(self, x, t)\n\t\n    x0 = x.clone().detach()\n    th = self.dynamic_thresholding_ratio * x0.abs().max()\n    x0 = torch.clamp(x0, min=-th, max=th)\n    x0 = x0 / th\n    return x0\n\n", "CodeBleu_score": 10.86602031006928}, {"code": "def data_prediction_fn(self, x, t)\n\t\n    x0 = x.clone()\n    if self.dynamic_thresholding:\n        threshold = self.max_threshold * (\n            self.dynamic_thresholding_ratio\n            * torch.max(torch.abs(x0))\n            / (torch.max(torch.abs(x0)) + self.epsilon)\n        )\n        x0 = torch.clamp(x0, min=-threshold, max=threshold)\n    x0 = x0 / (torch.max(torch.abs(x0)) + self.epsilon)\n    return x0\n\n", "CodeBleu_score": 23.925652132349196}, {"code": "def data_prediction_fn(self, x, t)\n\t\n    x0 = x.clone()\n    th = self.dynamic_thresholding_ratio * t.max()\n    x0 = torch.clamp(x0, -th, th)\n    x0 = x0 / th\n    return x0\n\n", "CodeBleu_score": 10.200292587323633}]}
{"instance_id": "65e3618468a2", "generation_result": [{"code": "def predict_eps_sigma(model, input, sigma_in, **kwargs)\n\t\n    sigma_in = sigma_in.reshape(input.shape[0], 1, 1, 1)\n    sigma_in = sigma_in.repeat(1, 3, input.shape[2], input.shape[3])\n    input = (input / sigma_in).clip(-1, 1)\n    pred = model(input, sigma_in, **kwargs)\n    pred = pred * sigma_in\n    return pred\n\n", "CodeBleu_score": 38.91016300658685}, {"code": "def predict_eps_sigma(model, input, sigma_in, **kwargs)\n\t\n    # reshape sigma to match the dimensions of the input\n    sigma = sigma_in.view(input.shape[0], *([1] * (len(input.shape) - 1)))\n    # scale the input\n    scaled_input = (input / sigma)\n    # compute the prediction\n    prediction = model(scaled_input, sigma, **kwargs)\n    # adjust the prediction\n    return prediction * sigma\n\n", "CodeBleu_score": 26.27091837693316}, {"code": "def predict_eps_sigma(model, input, sigma_in, **kwargs)\n\t\n    sigma_in = sigma_in.reshape(input.shape[0], 1, 1, 1).to(input.device)\n    scaled_input = input / sigma_in\n    eps_pred = model(scaled_input, sigma_in, **kwargs)\n    return input + sigma_in * eps_pred\n\n", "CodeBleu_score": 30.609657747334612}]}
{"instance_id": "b35846dfbc66", "generation_result": [{"code": "def cal_intergrand(beta_0, beta_1, taus)\n\t\n    alpha = beta_0 * torch.exp(-beta_1 * taus)\n    log_alpha = torch.log(alpha)\n    log_alpha.backward(gradient=torch.ones_like(log_alpha), retain_graph=True)\n    grad_log_alpha = taus.grad\n    integrand = grad_log_alpha * alpha\n    return integrand\n\n", "CodeBleu_score": 23.073405604453168}, {"code": "def cal_intergrand(beta_0, beta_1, taus)\n\t\n    taus = taus.clone().detach().requires_grad_(True)\n    alpha = torch.exp(beta_0) * torch.pow(taus, torch.exp(beta_1) - 1)\n    log_alpha = torch.log(alpha)\n    log_alpha.backward()\n    integrand = log_alpha.grad * alpha\n    return integrand\n\n", "CodeBleu_score": 15.238722889118216}, {"code": "def cal_intergrand(beta_0, beta_1, taus)\n\t\n    alpha = beta_0 + beta_1 * taus\n    log_alpha = torch.log(alpha)\n    log_alpha.backward()\n    integrand = log_alpha.grad * alpha\n    return integrand\n\n", "CodeBleu_score": 18.333822274071167}]}
{"instance_id": "5f3667566a10", "generation_result": [{"code": "def sample_euler_ancestral(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None)\n\t\n    with torch.no_grad():\n        extra_args = {} if extra_args is None else extra_args\n        noise_sampler = lambda x: torch.randn_like(x) if noise_sampler is None else noise_sampler\n        sigmas = torch.tensor(sigmas, device=x.device)\n        for sigma in tqdm(sigmas, disable=disable):\n            labels = torch.ones(x.shape[0], device=x.device)\n            euler_ancestral_sampling_fn = get_ancestral_step_fn(\n                sde, model, eps=eps, sigma_min=sigma_min, sigma_max=sigma_max,\n                extra_args=extra_args, predict_x0=predict_x0, thresholding=thresholding,\n                quantizer=quantizer, dynamic_thresholding=dynamic_thresholding,\n                eta=eta, s_noise=s_noise, noise_sampler=noise_sampler\n            )\n            x, x_mean = euler_ancestral_sampling_fn(x, sigma)\n            if callback is not None:\n                callback({'x': x, 'i': i, 'sigma': sigma, 'sigma_hat': sigmas[i], 'denoising': x})\n        x = x_mean if predict_x0 else x\n        x = x + sigma * noise_sampler(x) if add_noise else x\n        x = x.clip(0, 1)\n        return x\n\n", "CodeBleu_score": 42.03517405672402}, {"code": "def sample_euler_ancestral(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None)\n\t\n    with torch.no_grad():\n        extra_args = {} if extra_args is None else extra_args\n        noise_sampler = torch.randn if noise_sampler is None else noise_sampler\n        sigmas = sigmas.flip(0)\n        for c, sigma in tqdm(enumerate(sigmas), total=len(sigmas), disable=disable):\n            labels = torch.ones(x.shape[0], device=x.device) * c\n            labels = labels.long()\n            eps = noise_sampler(x.shape, device=x.device)\n            x_noisy = x + s_noise * eps\n            x_original = x\n            for n in range(model.num_timesteps):\n                t = torch.ones(x.shape[0], device=x.device) * n\n                vec_t = torch.ones(x.shape[0], device=x.device) * n / (model.num_timesteps - 1)\n                t = t.long()\n                vec_t = vec_t[:, None]\n                x_noisy = model.denoise_fn(x_noisy, t, x_original, labels, vec_t, **extra_args)\n            x0_pred = (x - s_noise * eps - x_noisy) / sigma\n            x = x + eta * x0_pred + math.sqrt(2 * eta) * sigma * torch.randn_like(x)\n            if callback is not None:\n                callback({'x': x, 'i': c, 'sigma': sigma, 'sigma_hat': sigmas[0], 'denoised': x_noisy})\n        x = x_original + s_noise * eps + x\n    return x\n\n", "CodeBleu_score": 39.913537552853974}, {"code": "def sample_euler_ancestral(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None)\n\t\n    with torch.no_grad():\n        extra_args = {} if extra_args is None else extra_args\n        noise_sampler = lambda x: torch.randn_like(x) if noise_sampler is None else noise_sampler\n        sigmas = torch.tensor(sigmas, device=x.device, dtype=torch.float32)\n        x = x * sigmas[0]\n        for i in trange(len(sigmas) - 1, disable=disable):\n            denoised = model(x / sigmas[i], i * torch.ones(x.shape[0], device=x.device), **extra_args)\n            d = (x - denoised) / sigmas[i]\n            dt = sigmas[i + 1] - sigmas[i]\n            x = x + d * dt\n            if callback is not None:\n                callback({'x': x, 'i': i, 'sigma': sigmas[i], 'sigma_next': sigmas[i + 1], 'denoised': denoised})\n        if noise_sampler is not None:\n            x = x + noise_sampler(x) * eta * sigmas[-1]\n        if s_noise > 0:\n            x = x + torch.randn_like(x) * s_noise\n        return x\n\n", "CodeBleu_score": 55.03946557112492}]}
{"instance_id": "6261ded88df2", "generation_result": [{"code": "def linear_multistep_coeff(order, t, i, j)\n\t\n    if order < i:\n        raise ValueError(f\"Order {order} must be at least the step index {i}\")\n\n    def integrand(tau):\n        product = 1.0\n        for k in range(j, order):\n            if k + 1 != i:\n                product *= (tau - t[k]) / (t[i] - t[k])\n        return product\n\n    return integrate.quad(integrand, t[i], t[i + 1], epsrel=1e-12)[0]\n\n", "CodeBleu_score": 42.75137515333751}, {"code": "def linear_multistep_coeff(order, t, i, j)\n\t\n    if order < 1:\n        raise ValueError(\"Linear multistep coefficients are defined in terms of positive-order polynomials. For a zero-order method, use `step_fn` instead.\")\n    if i == j:\n        return 1.\n    elif j > i:\n        raise ValueError(f\"The linear multistep coefficient with order {order} is not defined for indices i={i} and j={j}.\")\n\n    def prod_term(tau):\n        prod = 1.\n        for k in range(order):\n            if k + 1 != j:\n                prod *= (tau - t[i + k]) / (t[i + j] - t[i + k])\n        return prod\n\n    return integrate.quad(prod_term, t[i], t[i + 1], epsabs=1e-10)[0]\n\n", "CodeBleu_score": 51.5362480407526}, {"code": "def linear_multistep_coeff(order, t, i, j)\n\t\n    if order < i:\n        raise ValueError(f'Order {order} must be no less than i {i}.')\n\n    def _univariate_integral(t):\n        ret = 1.\n        for k in range(j, order):\n            ret *= (t - t[i]) / (t[k + 1] - t[k])\n        return ret\n\n    integral = integrate.quad(_univariate_integral, t[i], t[i + 1], epsrel=1e-12)[0]\n    return integral\n\n", "CodeBleu_score": 39.76997171178504}]}
{"instance_id": "4ee025e33ab1", "generation_result": [{"code": "def sample_lcm(model, x, sigmas, extra_args=None, callback=None, disable=None, noise_sampler=None)\n\t\n    extra_args = {} if extra_args is None else extra_args\n    s_in = x.new_ones([x.shape[0]])\n    sigma_fn = noise_sampler if noise_sampler is not None else torch.randn\n    with torch.enable_grad():\n        with tqdm(sigmas, disable=disable) as pbar:\n            for i, sigma in enumerate(pbar):\n                labels = s_in\n                labels = labels * sigma\n                eps = sigma_fn(x.shape, device=x.device)\n                x_in = x + eps * labels\n                denoised = model(x_in, sigma, **extra_args)\n                if callback is not None:\n                    callback({'x': x, 'i': i, 'sigma': sigma, 'sigma_hat': sigma, 'denoised': denoised})\n                d = to_d(x, sigma, denoised)\n                if sigma == 0:\n                    x = x + d\n                else:\n                    x = x + (d - eps * labels) / sigma\n    return x\n\n", "CodeBleu_score": 44.74670355600864}, {"code": "def sample_lcm(model, x, sigmas, extra_args=None, callback=None, disable=None, noise_sampler=None)\n\t\n    with torch.no_grad():\n        extra_args = {} if extra_args is None else extra_args\n        noise_sampler = lambda x: torch.randn_like(x) if noise_sampler is None else noise_sampler\n        sigmas = list(sigmas)\n        if len(sigmas) == 0:\n            sigmas = [0]\n        for sigma in tqdm(sigmas, disable=disable):\n            label = torch.tensor([sigma], device=x.device)\n            extra_args['sigma'] = label\n            output = model(x, label, **extra_args)\n            noise = noise_sampler(x)\n            x = x + output * sigma + noise * math.sqrt(sigma)\n            if callback is not None:\n                callback({'x': x, 'i': i, 'sigma': sigma, 'sigma_hat': sigmas[i], 'denoised': output})\n    return x\n\n", "CodeBleu_score": 48.198412216199436}, {"code": "def sample_lcm(model, x, sigmas, extra_args=None, callback=None, disable=None, noise_sampler=None)\n\t\n    extra_args = {} if extra_args is None else extra_args\n    s_in = x.new_ones([x.shape[0]])\n    sigma_fn = noise_sampler if noise_sampler is not None else torch.randn\n    with torch.no_grad():\n        with model.ema_scope():\n            for sigma in tqdm(sigmas, disable=disable):\n                denoised = model.apply_model(x, s_in * sigma, **extra_args)\n                d = to_d(x, sigma, denoised)\n                yield x, d\n                x = x + d\n                if callback is not None:\n                    callback(d)\n                if sigma > 0:\n                    x = x + sigma_fn(x.shape, device=x.device) * sigma\n\n", "CodeBleu_score": 43.03213529068111}]}
{"instance_id": "cb51f91da606", "generation_result": [{"code": "def append_dims(x, target_dims)\n\t\n    n_dims_to_append = target_dims - x.ndim\n    if n_dims_to_append < 0:\n        raise ValueError('Passed tensor already has more dimensions than target.')\n    for _ in range(n_dims_to_append):\n        x = x[..., None]\n    return x.detach().clone() if x.device.type == 'mps' else x\n\n", "CodeBleu_score": 40.75315049096415}, {"code": "def append_dims(x, target_dims)\n\t\n    n_dims_to_append = target_dims - x.ndim\n    if n_dims_to_append < 0:\n        raise ValueError('Passed tensor already has more dimensions than target')\n    for _ in range(n_dims_to_append):\n        x = x[..., None]\n    if x.device.type == 'mps':\n        return x.detach().clone()\n    else:\n        return x\n\n", "CodeBleu_score": 38.35523466249396}, {"code": "def append_dims(x, target_dims)\n\t\n    n_dims_to_append = target_dims - x.ndim\n    if n_dims_to_append < 0:\n        raise ValueError('input has more dimensions than target')\n    elif n_dims_to_append > 0:\n        x = x[(...,) + (None,) * n_dims_to_append]\n    if x.device.type == 'mps':\n        x = x.detach().clone()\n    return x\n\n", "CodeBleu_score": 51.40665428183326}]}
{"instance_id": "9fac874a3bc7", "generation_result": [{"code": "def download_file(path, url, digest=None)\n\t\n    path = Path(path)\n    path.parent.mkdir(exist_ok=True)\n    if path.exists():\n        if digest is None:\n            return path\n        if hashlib.sha256(path.read_bytes()).hexdigest() == digest:\n            return path\n    with urllib.request.urlopen(url) as source:\n        with path.open(\"wb\") as output:\n            with contextmanager(source) as s, contextmanager(output) as o:\n                shutil.copyfileobj(s, o)\n    if digest is not None:\n        if hashlib.sha256(path.read_bytes()).hexdigest() != digest:\n            path.unlink()\n            raise RuntimeError(\"Dataset digest mismatch\")\n    return path\n\n", "CodeBleu_score": 41.26631525848878}, {"code": "def download_file(path, url, digest=None)\n\t\n    path = Path(path)\n    path.parent.mkdir(exist_ok=True, parents=True)\n    if not path.exists():\n        print(f\"Downloading {url} to {path}\")\n        urllib.request.urlretrieve(url, path)\n    if digest is not None:\n        if digest != hashlib.sha256(path.read_bytes()).hexdigest():\n            raise RuntimeError(\"Dataset digest mismatch\")\n    return path\n\n", "CodeBleu_score": 33.88350012941343}, {"code": "def download_file(path, url, digest=None)\n\t\n    path = Path(path)\n    path.parent.mkdir(exist_ok=True, parents=True)\n    if path.exists():\n        return path\n    with urllib.request.urlopen(url) as source, path.open(\"wb\") as output:\n        with contextlib.closing(source):\n            while True:\n                buffer = source.read(8192)\n                if not buffer:\n                    break\n                output.write(buffer)\n    if digest is not None:\n        if digest != hashlib.sha256(path.read_bytes()).hexdigest():\n            path.unlink()\n            raise RuntimeError(\n                f\"Model has been downloaded but the SHA256 checksum does not not match the value provided on http://proceedings.mlr.press/v119/tolstikhin20a/tolstikhin20a.pdf. Please remove the file and try your request again.\"\n            )\n    return path\n\n", "CodeBleu_score": 33.69685307898595}]}
{"instance_id": "f23165e897ce", "generation_result": [{"code": "def train_mode(model, mode=True)\n\t\n    modes = [module.training for module in model.modules()]\n    try:\n        model.train(mode)\n        yield model\n    finally:\n        for module, mode in zip(model.modules(), modes):\n            module.training = mode\n\n", "CodeBleu_score": 49.29892349765329}, {"code": "def train_mode(model, mode=True)\n\t\n    modes = []\n    for module in model.modules():\n        modes.append(module.training)\n        module.train(mode)\n    yield\n    for module, mode in zip(model.modules(), modes):\n        module.train(mode)\n\n", "CodeBleu_score": 39.18182953024767}, {"code": "def train_mode(model, mode=True)\n\t\n    modes = [module.training for module in model.modules()]\n    try:\n        model.train(mode)\n        yield model\n    finally:\n        for module, mode in zip(model.modules(), modes):\n            module.training = mode\n\n", "CodeBleu_score": 49.29892349765329}]}
{"instance_id": "c6d59c6cb8e1", "generation_result": [{"code": "def ema_update(model, averaged_model, decay)\n\t\n    with torch.no_grad():\n        for ema_v, model_v in zip(averaged_model.parameters(), model.parameters()):\n            if model_v.requires_grad:\n                ema_v.copy_(ema_v * decay + (1.0 - decay) * model_v)\n        for ema_v, model_v in zip(averaged_model.buffers(), model.buffers()):\n            if model_v.requires_grad:\n                ema_v.copy_(model_v)\n\n", "CodeBleu_score": 29.946245564801053}, {"code": "def ema_update(model, averaged_model, decay)\n\t\n    with torch.no_grad():\n        for ema_v, model_v in zip(averaged_model.parameters(), model.parameters()):\n            if ema_v.requires_grad:\n                ema_v.mul_(decay).add_(model_v, alpha=1 - decay)\n        for ema_v, model_v in zip(averaged_model.buffers(), model.buffers()):\n            if ema_v.requires_grad:\n                ema_v.copy_(model_v)\n\n", "CodeBleu_score": 32.547520076400716}, {"code": "def ema_update(model, averaged_model, decay)\n\t\n    with torch.no_grad():\n        for ema_v, model_v in zip(\n            averaged_model.state_dict().values(), model.state_dict().values()\n        ):\n            if model_v.dtype in (torch.float16, torch.bfloat16):\n                # skip fp16 params from optimization\n                # we just need to copy the params to FP32 model\n                ema_v.copy_(model_v)\n            else:\n                ema_v.copy_(ema_v * decay + (1.0 - decay) * model_v)\n        for buffer, ema_buffer in zip(\n            model.buffers(), averaged_model.buffers()\n        ):\n            ema_buffer.copy_(buffer)\n\n", "CodeBleu_score": 31.328152698839357}]}
{"instance_id": "600c39a30798", "generation_result": [{"code": "def rand_log_logistic(shape, loc=0., scale=1., min_value=0., max_value=float('inf'), device='cpu', dtype=torch.float32)\n\t\n    min_value = torch.tensor(min_value, device=device, dtype=dtype)\n    max_value = torch.tensor(max_value, device=device, dtype=dtype)\n    min_cdf = torch.sigmoid((min_value - loc) / scale)\n    max_cdf = torch.sigmoid((max_value - loc) / scale)\n    u = torch.rand(*shape, device=device, dtype=dtype)\n    u = (max_cdf - min_cdf) * u + min_cdf\n    return loc + scale * torch.log(u / (1 - u))\n\n", "CodeBleu_score": 32.27858746180905}, {"code": "def rand_log_logistic(shape, loc=0., scale=1., min_value=0., max_value=float('inf'), device='cpu', dtype=torch.float32)\n\t\n    min_value = torch.tensor(min_value, dtype=dtype, device=device)\n    max_value = torch.tensor(max_value, dtype=dtype, device=device)\n    min_cdf = torch.sigmoid((min_value - loc) / scale)\n    max_cdf = torch.sigmoid((max_value - loc) / scale)\n    u = torch.rand(*shape, dtype=dtype, device=device)\n    u = (max_cdf - min_cdf) * u + min_cdf\n    return loc + scale * torch.log(u / (1 - u))\n\n", "CodeBleu_score": 32.02828453946061}, {"code": "def rand_log_logistic(shape, loc=0., scale=1., min_value=0., max_value=float('inf'), device='cpu', dtype=torch.float32)\n\t\n    min_value = torch.tensor(min_value, dtype=dtype, device=device)\n    max_value = torch.tensor(max_value, dtype=dtype, device=device)\n    min_cdf = torch.sigmoid((min_value - loc) / scale)\n    max_cdf = torch.sigmoid((max_value - loc) / scale)\n    u = torch.rand(shape, dtype=dtype, device=device)\n    return loc + scale * torch.logit(min_cdf + u * (max_cdf - min_cdf))\n\n", "CodeBleu_score": 34.14826735688394}]}
{"instance_id": "75e2def2467f", "generation_result": [{"code": "def get_activation(activation: Literal[\"elu\", \"snake\", \"none\"], antialias=False, channels=None) -> nn.Module\n\t\n    if activation == \"elu\":\n        return nn.ELU()\n    elif activation == \"snake\":\n        if channels is None:\n            raise ValueError(\"channels must be specified for snake activation\")\n        return comfy.ops.SnakeBeta(channels, antialias=antialias)\n    elif activation == \"none\":\n        return nn.Identity()\n    else:\n        raise ValueError(f\"activation {activation} not supported\")\n\n", "CodeBleu_score": 40.119971895075004}, {"code": "def get_activation(activation: Literal[\"elu\", \"snake\", \"none\"], antialias=False, channels=None) -> nn.Module\n\t\n    if activation == \"elu\":\n        return nn.ELU()\n    elif activation == \"snake\":\n        return comfy.ops.SnakeBeta(channels, antialias=antialias)\n    elif activation == \"none\":\n        return nn.Identity()\n    else:\n        raise ValueError(f\"Activation {activation} not supported\")\n\n", "CodeBleu_score": 40.10842114691868}, {"code": "def get_activation(activation: Literal[\"elu\", \"snake\", \"none\"], antialias=False, channels=None) -> nn.Module\n\t\n    if activation == \"elu\":\n        return nn.ELU(inplace=True)\n    elif activation == \"snake\":\n        return comfy.ops.SnakeBeta(channels=channels, antialias=antialias)\n    elif activation == \"none\":\n        return nn.Identity()\n    else:\n        raise ValueError(f\"Unknown activation type {activation}\")\n\n", "CodeBleu_score": 43.929554057812865}]}
{"instance_id": "ef1714c79e51", "generation_result": [{"code": "def apply_rotary_pos_emb(t, freqs, scale = 1)\n\t\n    if freqs.ndim == 1:\n        freqs = freqs.unsqueeze(-1)\n    t, t_pass = t[..., :freqs.shape[-2] * 2], t[..., freqs.shape[-2] * 2:]\n    t, t_pass = (t * scale).to(dtype=torch.float32), (t_pass * scale).to(dtype=torch.float32)\n    t = torch.cat([t, t_pass], dim=-1)\n    t = torch.view_as_real(t)\n    t_f = torch.view_as_real(t * freqs)\n    t_f = t_f.flatten(-2)\n    t_f = t_f.real * torch.cos(t_f.imag) + t_f.imag * torch.sin(t_f.imag)\n    t_f = torch.view_as_complex(t_f)\n    return torch.cat([t_f.real, t_f.imag], dim=-1)\n\n", "CodeBleu_score": 35.144132580181484}, {"code": "def apply_rotary_pos_emb(t, freqs, scale = 1)\n\t\n    if freqs.ndim == 1:\n        freqs = freqs.unsqueeze(-1)\n    t, t_pass = t[..., :freqs.shape[-2] * 2], t[..., freqs.shape[-2] * 2:]\n    t, t_pass = (t * scale).to(dtype=torch.float32), (t_pass * scale).to(dtype=torch.float32)\n    t = torch.cat((t, t_pass), dim=-1)\n    t = torch.view_as_complex(t.float().reshape(*t.shape[:-1], -1, 2))\n    freqs = torch.view_as_complex(freqs.contiguous())\n    t = torch.view_as_real(t * freqs).flatten(3)\n    t = t.type_as(t_pass)\n    t = torch.cat((t, t_pass), dim=-1)\n    return t\n\n", "CodeBleu_score": 39.9334158350565}, {"code": "def apply_rotary_pos_emb(t, freqs, scale = 1)\n\t\n    if freqs.ndim == 1:\n        freqs = freqs.unsqueeze(-1)\n    t, t_pass = t[..., : freqs.shape[-2] * 2, :], t[..., freqs.shape[-2] * 2 :, :]\n    t, t_pass = (t.float(), t_pass.float()) if t.dtype == torch.float16 else (t, t_pass)\n    t, t_pass = rearrange(t, \"... (d r c) -> ... d r c\", r=2).float(), t_pass\n    freqs = freqs.float() if t.dtype == torch.float16 else freqs\n    freqs = scale * freqs.to(t.device)\n    freqs = torch.cat((freqs, freqs), dim=-2)\n    t2 = t[..., 1::2, :, :]\n    t = t[..., ::2, :, :].float()\n    t2 = torch.cat([-t2[..., 1::2, :].flip(3), t2[..., ::2, :]], dim=-1)\n    t = torch.cat([t, t2], dim=-2)\n    t = t * freqs.cos() + t2 * freqs.sin()\n    t = torch.cat([t, t_pass], dim=-2)\n    return t.type(t_pass.dtype)\n\n", "CodeBleu_score": 37.59946434562594}]}
{"instance_id": "a7a5f178c6f6", "generation_result": [{"code": "def idx2vq(self, idx, dim=-1)\n\t\n    return self.codebook(idx).moveaxis(dim, -1)\n\n", "CodeBleu_score": 17.224761738162357}, {"code": "def idx2vq(self, idx, dim=-1)\n\t\n    return self.codebook.embedding(idx).moveaxis(len(idx.shape), dim)\n\n", "CodeBleu_score": 23.222903064961205}, {"code": "def idx2vq(self, idx, dim=-1)\n\t\n    idx = idx.view(idx.size(0), -1)\n    embeds = self.codebook(idx)\n    embeds = embeds.view(embeds.size(0), self.codebook.num_codebooks, -1)\n    embeds = embeds.permute(0, 2, 1).contiguous()\n    if dim != -1:\n        embeds = embeds.transpose(-1, dim)\n    return embeds\n\n", "CodeBleu_score": 46.07053152093732}]}
{"instance_id": "4104c041fb20", "generation_result": [{"code": "def encode(self, x, quantize=False)\n\t\n    if quantize:\n        quantized_output, quantized_indices, quantized_loss = self.quantize(x)\n        return quantized_output, quantized_indices, quantized_loss\n    else:\n        return self.encode(x)\n\n", "CodeBleu_score": 25.293890506862617}, {"code": "def encode(self, x, quantize=False)\n\t\n    if quantize:\n        quantized_output, encoded_output, indices, loss = self.encode_with_quantization(x)\n        return quantized_output, encoded_output, indices, loss\n    else:\n        encoded_output = self.encode_without_quantization(x)\n        return encoded_output\n\n", "CodeBleu_score": 28.0133394957108}, {"code": "def encode(self, x, quantize=False)\n\t\n    indices = []\n    combined_loss = 0\n    for block in self.blocks:\n        x, loss, index = block.encode(x)\n        combined_loss += loss\n        indices.append(index)\n    if quantize:\n        x, loss, index = self.quantizer.encode(x)\n        combined_loss += loss\n        indices.append(index)\n    return x, combined_loss, indices\n\n", "CodeBleu_score": 28.011059990715292}]}
{"instance_id": "403c52ba2d3b", "generation_result": [{"code": "def ema_scope(self, context=None)\n\t\n    if context is None:\n        context = {}\n    if not self.ema_enabled:\n        yield\n        return\n    self.model.eval()\n    self.ema_model.eval()\n    if self.ema_log_mode == \"replace\":\n        context[\"old_params\"] = {}\n        for name, params in self.model.named_parameters():\n            context[\"old_params\"][name] = params.data.clone()\n            params.data.copy_(self.ema_model.get_parameter(name).data)\n    elif self.ema_log_mode == \"swap\":\n        context[\"model\"], context[\"ema_model\"] = self.model, self.ema_model\n        self.model, self.ema_model = self.ema_model, self.model\n    else:\n        raise ValueError(f\"Invalid ema_log_mode: {self.ema_log_mode}\")\n    yield\n    if self.ema_log_mode == \"replace\":\n        for name, params in self.model.named_parameters():\n            params.data.copy_(context[\"old_params\"][name])\n    elif self.ema_log_mode == \"swap\":\n        self.model, self.ema_model = context[\"model\"], context[\"ema_model\"]\n    self.model.train()\n    self.ema_model.train()\n\n", "CodeBleu_score": 33.32845596207279}, {"code": "def ema_scope(self, context=None)\n\t\n\n    def restore(*args):\n        self.shadow_params = copy.deepcopy(self.params)\n        self.params = self.ema_params\n\n    def apply(*args):\n        self.params = self.ema_params = self.shadow_params\n        self.shadow_params = None\n\n    if self.ema_enabled:\n        if self.ema_params is None:\n            if self.ema_restore:\n                self.ema_params = copy.deepcopy(self.params)\n            else:\n                self.ema_params = self.params\n            self.ema_params_config = copy.deepcopy(self.params_config)\n\n        if self.ema_scope_level == 0:\n            if self.ema_started:\n                if self.ema_restore:\n                    self.restore_params = self.params\n                self.apply_params = self.ema_params\n            else:\n                self.restore_params = self.params\n                self.apply_params = self.ema_params\n\n        self.ema_scope_level += 1\n\n        if context is not None:\n            context.add_log_fn(lambda: f\"EMA {self.ema_scope_level}\")\n\n        return contextmanager(restore)(apply)\n    else:\n        return contextmanager(lambda: iter([lambda: None, lambda: None]))()\n\n", "CodeBleu_score": 35.5041467201028}, {"code": "def ema_scope(self, context=None)\n\t\n\n    if self.ema_scope_level == 0:\n        if context is not None:\n            context.append(\n                {\n                    \"ema_scope_level\": self.ema_scope_level,\n                    \"model_params\": [p.data.clone() for p in self.model.parameters()],\n                    \"ema_params\": [p.data.clone() for p in self.model.parameters()],\n                }\n            )\n\n        self.ema_scope_level += 1\n\n        if self.ema_decay > 0.0:\n            print(f\"EMA weights set - {self.ema_decay}\")\n            comfy.ops.set_weights(self.model, self.ema_model, self.ema_decay)\n\n    else:\n        self.ema_scope_level += 1\n\n    yield\n\n    self.ema_scope_level -= 1\n\n    if self.ema_scope_level == 0:\n        if context is not None:\n            print(f\"EMA weights restored - {self.ema_decay}\")\n            comfy.ops.restore_weights(self.model, self.ema_model, self.ema_decay)\n\n            for checkpoint in context:\n                if checkpoint[\"ema_scope_level\"] != 0:\n                    continue\n\n                for mp, ep in zip(self.model.parameters(), checkpoint[\"model_params\"]):\n                    mp.data.copy_(ep.data)\n\n", "CodeBleu_score": 43.62941863608629}]}
{"instance_id": "94b7467f98e4", "generation_result": [{"code": "def post_attention(self, attn, x, gate_msa, shift_mlp, scale_mlp, gate_mlp)\n\t\n    x = x + attn * gate_msa\n    x = self.norm_mlp(x)\n    x = x * scale_mlp + shift_mlp\n    x = self.mlp(x) * gate_mlp\n    return x\n\n", "CodeBleu_score": 34.43901548849141}, {"code": "def post_attention(self, attn, x, gate_msa, shift_mlp, scale_mlp, gate_mlp)\n\t\n    x = x + attn * gate_msa\n    x = self.norm_mlp(x)\n    x = self.mlp(self.channel_proj(x)) * gate_mlp\n    return x\n\n", "CodeBleu_score": 33.00962468232619}, {"code": "def post_attention(self, attn, x, gate_msa, shift_mlp, scale_mlp, gate_mlp)\n\t\n    x = x + attn * gate_msa\n    x = self.norm1(x)\n    x = x * scale_mlp + shift_mlp\n    x = self.mlp(x) * gate_mlp\n    return x\n\n", "CodeBleu_score": 34.229632404089685}]}
{"instance_id": "d9fdc05542a2", "generation_result": [{"code": "def _block_mixing(context, x, context_block, x_block, c)\n\t\n    # context_block is a tuple of (pre_attention_block, post_attention_block)\n    # x_block is a tuple of (pre_attention_block, post_attention_block)\n    # context_block[0] is a tuple of (pre_attention_block, None)\n    # x_block[0] is a tuple of (pre_attention_block, None)\n    # context_block[1] is a tuple of (None, post_attention_block)\n    # x_block[1] is a tuple of (None, post_attention_block)\n\n    # context_block[0][0] is a function that takes context and returns qkv and intermediates\n    # x_block[0][0] is a function that takes x and returns qkv and intermediates\n    # context_block[1][1] is a function that takes context and returns context\n    # x_block[1][1] is a function that takes x and returns x\n\n    # qkv_context is a tuple of (q, k, v)\n    # qkv_x is a tuple of (q, k, v)\n    # intermediates_context is a tuple of (intermediates)\n    # intermediates_x is a tuple of (intermediates)\n    qkv_context, intermediates_context = context_block[0][0](context)\n    qkv_x, intermediates_x = x_block[0][0](x)\n\n    # qkv_context is a tuple of (q, k, v)\n    # qkv_x is a tuple of (q, k, v)\n    # intermediates_context is a tuple of (intermediates)\n    # intermediates_x is a tuple of (intermediates)\n    qkv_context = torch.cat(qkv_context, dim=2)\n    qkv_x = torch.cat(qkv_x, dim=2)\n\n    # qkv_context is a tensor of shape (batch, heads, seq_len, 3 * dim)\n    # qkv_x is a tensor of shape (batch,", "CodeBleu_score": 5.231201474131345}, {"code": "def _block_mixing(context, x, context_block, x_block, c)\n\t\n    # Pre-attention block for context\n    context_qkv, context_intermediate = context_block(context)\n    # Pre-attention block for x\n    x_qkv, x_intermediate = x_block(x)\n    # Concatenate qkv tensors\n    qkv = torch.cat([context_qkv, x_qkv], dim=-2)\n    # Apply optimized attention\n    attention_output = c(qkv)\n    # Split attention results back into context and x parts\n    context_attention_output, x_attention_output = attention_output.chunk(2, dim=-2)\n    # Post-attention block for context\n    context = context_block.post_attention(context_attention_output, context_intermediate)\n    # Post-attention block for x\n    x = x_block.post_attention(x_attention_output, x_intermediate)\n    return context, x\n\n", "CodeBleu_score": 16.055824671817533}, {"code": "def _block_mixing(context, x, context_block, x_block, c)\n\t\n    # Apply context pre-attention block\n    context, context_intermediate = context_block.pre_attention(context)\n    # Apply x pre-attention block\n    x, x_intermediate = x_block.pre_attention(x)\n\n    # Concatenate qkv tensors\n    qkv = torch.cat([context, x], dim=-1)\n\n    # Apply optimized attention\n    qkv = _optimized_attention(qkv, c)\n\n    # Split attention results back into context and x parts\n    context, x = qkv.chunk(2, dim=-1)\n\n    # Apply context post-attention block\n    context = context_block.post_attention(context, context_intermediate)\n    # Apply x post-attention block\n    x = x_block.post_attention(x, x_intermediate)\n\n    return context, x\n\n", "CodeBleu_score": 15.655984611976878}]}
{"instance_id": "b219bce30160", "generation_result": [{"code": "def get_timestep_embedding(timesteps, embedding_dim)\n\t\n    assert len(timesteps.shape) == 1\n\n    half_dim = embedding_dim // 2\n    emb = math.log(10000) / (half_dim - 1)\n    emb = torch.exp(torch.arange(half_dim, dtype=torch.float32) * -emb)\n    emb = timesteps[:, None] * emb[None, :]\n    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n    if embedding_dim % 2 == 1:\n        # zero pad\n        emb = torch.nn.functional.pad(emb, (0, 1, 0, 0))\n    return emb\n\n", "CodeBleu_score": 77.21369170165848}, {"code": "def get_timestep_embedding(timesteps, embedding_dim)\n\t\n    assert len(timesteps.shape) == 1\n    half_dim = embedding_dim // 2\n    emb = math.log(10000) / (half_dim - 1)\n    emb = torch.exp(torch.arange(half_dim) * -emb)\n    emb = torch.tensor(timesteps).float()[:, None] * emb[None, :]\n    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n    if embedding_dim % 2 == 1:\n        emb = torch.nn.functional.pad(emb, (0, 1), value=0)\n    return emb\n\n", "CodeBleu_score": 70.79235652559443}, {"code": "def get_timestep_embedding(timesteps, embedding_dim)\n\t\n    assert len(timesteps.shape) == 1\n    half_dim = embedding_dim // 2\n    emb = math.log(10000) / (half_dim - 1)\n    emb = torch.exp(torch.arange(half_dim) * -emb)\n    emb = timesteps[:, None] * emb[None, :]\n    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n    if embedding_dim % 2 == 1:  # zero pad\n        emb = torch.nn.functional.pad(emb, (0, 1, 0, 0))\n    return emb\n\n", "CodeBleu_score": 70.42185062897063}]}
{"instance_id": "6ae8453c86be", "generation_result": [{"code": "def pytorch_attention(q, k, v)\n\t\n    try:\n        q = q.reshape(q.shape[0], q.shape[1], q.prod(dim=2)).transpose(1, 2)\n        k = k.reshape(k.shape[0], k.shape[1], k.prod(dim=2))\n        v = v.reshape(v.shape[0], v.shape[1], v.prod(dim=2))\n        return torch.matmul(q, k.transpose(1, 2)) / math.sqrt(k.shape[2])\n    except RuntimeError as e:\n        if \"out of memory\" in str(e):\n            logging.warning(\n                \"OOM exception in pytorch_attention, switching to slice attention\"\n            )\n            return comfy.ops.slice_attention(q, k, v)\n        else:\n            raise e\n\n", "CodeBleu_score": 26.684910802339466}, {"code": "def pytorch_attention(q, k, v)\n\t\n    q = q.reshape(q.shape[0], q.shape[1], q.shape[3], q.shape[2]).transpose(2, 3)\n    k = k.reshape(k.shape[0], k.shape[1], k.shape[3], k.shape[2]).transpose(2, 3)\n    v = v.reshape(v.shape[0], v.shape[1], v.shape[3], v.shape[2]).transpose(2, 3)\n    try:\n        return comfy.ops.attention(q, k, v).transpose(2, 3).reshape(q.shape)\n    except RuntimeError as e:\n        if \"out of memory\" in str(e):\n            logging.warning(\"Attention OOM, switching to slice attention\")\n            return comfy.ops.attention_slice(q, k, v).transpose(2, 3).reshape(q.shape)\n        else:\n            raise e\n\n", "CodeBleu_score": 24.381985955916214}, {"code": "def pytorch_attention(q, k, v)\n\t\n    # reshape tensors to prepare for attention computation\n    q = q.reshape(q.shape[0], q.shape[1], q.shape[2] * q.shape[3]).transpose(1, 2)\n    k = k.reshape(k.shape[0], k.shape[1], k.shape[2] * k.shape[3]).transpose(1, 2)\n    v = v.reshape(v.shape[0], v.shape[1], v.shape[2] * v.shape[3]).transpose(1, 2)\n\n    # compute attention using PyTorch's scaled dot-product attention\n    try:\n        out = torch.nn.functional.scaled_dot_product_attention(\n            q, k, v, attn_mask=None, dropout_p=0.0, is_causal=False\n        )\n    except RuntimeError as e:\n        if \"out of memory\" in str(e):\n            # switch to slice attention as a fallback\n            out = comfy.ops.slice_attention(q, k, v)\n        else:\n            raise e\n\n    # reshape output to original dimensions\n    out = out.transpose(1, 2).reshape(out.shape[0], out.shape[2], int(math.sqrt(out.shape[1])), int(math.sqrt(out.shape[1])))\n    return out\n\n", "CodeBleu_score": 39.25333751425539}]}
{"instance_id": "fe4433c987ad", "generation_result": [{"code": "def apply_control(h, control, name)\n\t\n    if exists(control) and name in control:\n        c = control.pop(name)\n        try:\n            h += c\n        except Exception as e:\n            logging.warning(f\"Failed to apply control {name} with value {c} to {h.shape}: {e}\")\n    return h\n\n", "CodeBleu_score": 36.087944333291645}, {"code": "def apply_control(h, control, name)\n\t\n    if not exists(control):\n        return h\n    if name in control:\n        c = control.pop(name)\n        try:\n            h += c\n        except Exception as e:\n            logging.warning(f\"Could not add control {name} to {h.shape} with {c.shape} due to error: {e}\")\n    return h\n\n", "CodeBleu_score": 40.013531167575756}, {"code": "def apply_control(h, control, name)\n\t\n    if exists(control) and name in control:\n        v = control[name]\n        if exists(v):\n            try:\n                h = h + v\n            except RuntimeError as e:\n                logging.warning(f\"failed to apply control {name}: {e}\")\n        control.pop(name)\n    return h\n\n", "CodeBleu_score": 39.40412581941285}]}
{"instance_id": "56da4547c101", "generation_result": [{"code": "def get_codebook_entry(self, indices, shape)\n\t\n    # shape = (batch_size, height, width, channels)\n    if self.use_conv_masks:\n        # get the codebook entry for the mask\n        indices_shape = indices.shape\n        # indices = (batch_size, channels, height, width)\n        indices = indices.permute(0, 3, 1, 2)\n        # indices = (batch_size, height, width, channels)\n        # get the one-hot encoding of the indices\n        one_hot = torch.nn.functional.one_hot(indices, num_classes=self.n_embed).float().to(indices.device)\n        # one_hot = (batch_size, height, width, channels, n_embed)\n        # get the embedding weights\n        weights = self.codebook.weight\n        # weights = (n_embed, channels)\n        # multiply the one-hot encoding with the embedding weights\n        codebook_entry = torch.einsum(\"bhwce,ce->bhwc\", one_hot, weights)\n        # codebook_entry = (batch_size, height, width, channels)\n    else:\n        # get the one-hot encoding of the indices\n        one_hot = torch.nn.functional.one_hot(indices, num_classes=self.n_embed).float().to(indices.device)\n        # one_hot = (batch_size, seq_len, n_embed)\n        # get the embedding weights\n        weights = self.codebook.weight\n        # weights = (n_embed, channels)\n        # multiply the one-hot encoding with the embedding weights\n        codebook_entry = torch.einsum(\"bse,ce->bsc\", one_hot, weights)\n        # codebook_entry = (batch_size, seq_len, channels)\n    # reshape the codebook entry to the original input shape\n    if shape is not None:\n        codebook_entry = codebook_entry.view(shape)\n        # codebook_entry = (batch_size, height, width, channels)\n    return codebook_entry\n\n", "CodeBleu_score": 44.00833357292875}, {"code": "def get_codebook_entry(self, indices, shape)\n\t\n    # shape = (batch, height, width, channels)\n    if not shape:\n        shape = indices.shape\n\n    # get the quantized latent vectors\n    latents = self.embedding(indices)\n\n    if self.latent_with_style:\n        # shape = [batch, height, width, embedding_dim]\n        latents = latents.view(latents.shape[0], *((1,) * (len(latents.shape) - 3))\n                                 , *latents.shape[2:])\n        latents = latents.expand(-1, shape[1], shape[2], -1)\n        latents = latents.reshape(latents.shape[0], latents.shape[1], latents.shape[2] * latents.shape[3])\n\n    if self.latent_linear:\n        latents = latents.permute(0, 3, 1, 2).contiguous()\n    else:\n        latents = latents.permute(0, 2, 3, 1).contiguous()\n\n    # shape = [batch, height, width, embedding_dim]\n    latents = latents.view(*shape)\n\n    return latents", "CodeBleu_score": 41.35631461969616}, {"code": "def get_codebook_entry(self, indices, shape)\n\t\n    # shape = (batch, height, width, channel)\n    if not shape:\n        shape = indices.shape\n    # get the quantized latent vectors\n    latents = self.vqvae.codebook.embedding(indices)\n    if self.latent_flattened_shape is not None:\n        # for classifier-free guidance\n        latents = latents.view([shape[0], self.latent_flattened_shape[0],\n                               self.latent_flattened_shape[1], self.latent_flattened_shape[2]])\n    # permute and return the latents\n    return latents.permute(0, 3, 1, 2).contiguous()\n\n", "CodeBleu_score": 38.25559048121817}]}
{"instance_id": "548f8a1fb37d", "generation_result": [{"code": "def make_noise(self)\n\t\n    device = self.input.device\n    noises = [torch.randn(1, 1, 4, 4, device=device)]\n\n    for i in range(3, int(self.log_size) + 1):\n        for _ in range(2):\n            noises.append(torch.randn(1, 1, 2 ** i, 2 ** i, device=device))\n\n    return noises\n\n", "CodeBleu_score": 58.90361010522965}, {"code": "def make_noise(self)\n\t\n    device = self.input.device\n    noises = [torch.randn(1, 1, 4, 4, device=device)]\n\n    for i in range(3, self.log_size + 1):\n        for _ in range(2):\n            noises.append(torch.randn(1, 1, 2 ** i, 2 ** i, device=device))\n\n    return noises\n\n", "CodeBleu_score": 62.05194749123122}, {"code": "def make_noise(self)\n\t\n    device = self.input.device\n    noises = [torch.randn(1, 1, 4, 4, device=device)]\n\n    for i in range(3, int(self.log_size) + 1):\n        for _ in range(2):\n            noises.append(torch.randn(1, 1, 2 ** i, 2 ** i, device=device))\n\n    return noises\n\n", "CodeBleu_score": 58.90361010522965}]}
{"instance_id": "42fd805563c6", "generation_result": [{"code": "def mean_latent(self, num_latent)\n\t\n    latent_in = torch.randn(\n        num_latent, self.style_dim, device=self.input.device\n    )\n    latent = self.style(latent_in)\n    return latent.mean(0)\n\n", "CodeBleu_score": 30.36057222203806}, {"code": "def mean_latent(self, num_latent)\n\t\n    latent_in = torch.randn(num_latent, self.style_dim, device=self.input.device)\n    latent = self.style(latent_in).mean(0, keepdim=True)\n    return latent\n\n", "CodeBleu_score": 36.29138592988189}, {"code": "def mean_latent(self, num_latent)\n\t\n    latent_in = torch.randn(\n        num_latent, self.style_dim, device=self.latent_avg.device\n    )\n    latent = self.style(latent_in).mean(0, keepdim=True)\n    return latent\n\n", "CodeBleu_score": 35.57462753986877}]}
{"instance_id": "2f485fd924af", "generation_result": [{"code": "def color_jitter(img, shift)\n\t\n    jitter = torch.rand(img.shape) * 2 - 1\n    jitter = jitter * shift\n    jittered_img = img + jitter\n    jittered_img = torch.clamp(jittered_img, 0, 1)\n    return jittered_img\n\n", "CodeBleu_score": 35.279570566109605}, {"code": "def color_jitter(img, shift)\n\t\n    img = img + shift\n    img = torch.clamp(img, 0, 1)\n    return img\n\n", "CodeBleu_score": 34.139323149180655}, {"code": "def color_jitter(img, shift)\n\t\n    img = adjust_brightness(img, shift)\n    img = adjust_contrast(img, shift)\n    img = adjust_saturation(img, shift)\n    img = adjust_hue(img, shift)\n    return img\n\n", "CodeBleu_score": 15.4279697125584}]}
{"instance_id": "c87ccf1fbf48", "generation_result": [{"code": "def feed_data(self, data)\n\t\n    self.lq = data['lq'].to(self.device)\n    if 'gt' in data:\n        self.gt = data['gt'].to(self.device)\n    if 'face' in data:\n        self.left_eye = data['left_eye'].to(self.device)\n        self.right_eye = data['right_eye'].to(self.device)\n        self.mouth = data['mouth'].to(self.device)\n\n", "CodeBleu_score": 58.200077050694965}, {"code": "def feed_data(self, data)\n\t\n    self.lq = data['lq'].to(self.device)\n    if 'gt' in data:\n        self.gt = data['gt'].to(self.device)\n    if 'left_eye' in data:\n        self.left_eye = data['left_eye'].to(self.device)\n        self.right_eye = data['right_eye'].to(self.device)\n        self.mouth = data['mouth'].to(self.device)\n\n", "CodeBleu_score": 58.49712029155051}, {"code": "def feed_data(self, data)\n\t\n    self.lq = data[\"lq\"].to(self.device)\n    if \"gt\" in data:\n        self.gt = data[\"gt\"].to(self.device)\n    if \"left_eye\" in data:\n        self.left_eye = data[\"left_eye\"].to(self.device)\n        self.right_eye = data[\"right_eye\"].to(self.device)\n        self.mouth = data[\"mouth\"].to(self.device)\n\n", "CodeBleu_score": 47.80785664497057}]}
{"instance_id": "8f8c7cb13037", "generation_result": [{"code": "def construct_img_pyramid(self)\n\t\n    pyramid = [self.gt]\n    for i in range(self.downgrade_iters):\n        pyramid.insert(0, F.interpolate(pyramid[0], scale_factor=0.5, mode='bilinear', align_corners=False))\n    return pyramid\n\n", "CodeBleu_score": 25.60811420734037}, {"code": "def construct_img_pyramid(self)\n\t\n    img_pyramid = [self.gt]\n    for i in range(self.downscale - 1):\n        img_pyramid.insert(\n            0,\n            F.interpolate(\n                img_pyramid[0],\n                scale_factor=0.5,\n                mode=\"bilinear\",\n                align_corners=False,\n                recompute_scale_factor=True,\n            ),\n        )\n    return img_pyramid\n\n", "CodeBleu_score": 21.393782015381618}, {"code": "def construct_img_pyramid(self)\n\t\n    img_pyramid = [self.gt]\n    for i in range(self.downgrade_iters):\n        img_pyramid.insert(0, F.interpolate(img_pyramid[0], scale_factor=0.5, mode=\"bilinear\", align_corners=False))\n    return img_pyramid\n\n", "CodeBleu_score": 25.349859131636837}]}
{"instance_id": "51247274d40c", "generation_result": [{"code": "def save(self, epoch, current_iter)\n\t\n    self.nets['generator'].save(current_iter)\n    self.nets['generator_ema'].save(current_iter)\n    self.nets['discriminator'].save(current_iter)\n    if self.opt['net_D']['use_facial_discriminator']:\n        self.nets['discriminator_left_eye'].save(current_iter)\n        self.nets['discriminator_right_eye'].save(current_iter)\n        self.nets['discriminator_mouth'].save(current_iter)\n    state_dict = self.get_current_training_state()\n    torch.save(state_dict,\n               os.path.join(self.opt['path']['training_state'],\n                            'training_state_epoch{:04d}_iter{:08d}.pth'.format(epoch, current_iter)))\n\n", "CodeBleu_score": 47.30618343546902}, {"code": "def save(self, epoch, current_iter)\n\t\n    self.nets['generator'].save(current_iter)\n    self.nets['generator_ema'].save(current_iter)\n    self.nets['discriminator'].save(current_iter)\n    if self.with_facial_discriminator:\n        self.nets['discriminator_left_eye'].save(current_iter)\n        self.nets['discriminator_right_eye'].save(current_iter)\n        self.nets['discriminator_mouth'].save(current_iter)\n    state_dict = self.state_dict()\n    for key, param in state_dict.items():\n        param = param.cpu()\n    state_dict['scheduler'] = self.schedulers[0].state_dict()\n    torch.save(state_dict,\n               os.path.join(self.opt['path']['training_state'],\n                            'training_state_{}.pt'.format(current_iter)))\n\n", "CodeBleu_score": 44.141294875296225}, {"code": "def save(self, epoch, current_iter)\n\t\n    self.nets['generator'].save(current_iter)\n    self.nets['generator_ema'].save(current_iter)\n    self.nets['discriminator'].save(current_iter)\n    if self.opt['net_D_facial']:\n        self.nets['discriminator_left_eye'].save(current_iter)\n        self.nets['discriminator_right_eye'].save(current_iter)\n        self.nets['discriminator_mouth'].save(current_iter)\n    state_dict = self.get_current_state()\n    for key, param in state_dict.items():\n        param = param.cpu()\n    state_dict_cpu = state_dict\n    save_path = str(self.opt['path']['training_state'])\n    torch.save(state_dict_cpu, save_path)\n\n", "CodeBleu_score": 47.36962228731251}]}
{"instance_id": "afa0dc8128c1", "generation_result": [{"code": "def clean_folder(folder)\n\t\n    for filename in os.listdir(folder):\n        file_path = os.path.join(folder, filename)\n        try:\n            if os.path.isfile(file_path) or os.path.islink(file_path):\n                os.unlink(file_path)\n            elif os.path.isdir(file_path):\n                shutil.rmtree(file_path)\n        except Exception as e:\n            print('Failed to delete %s. Reason: %s' % (file_path, e))\n\n", "CodeBleu_score": 70.32553386608735}, {"code": "def clean_folder(folder)\n\t\n    for filename in os.listdir(folder):\n        file_path = os.path.join(folder, filename)\n        try:\n            if os.path.isfile(file_path) or os.path.islink(file_path):\n                os.unlink(file_path)\n            elif os.path.isdir(file_path):\n                shutil.rmtree(file_path)\n        except Exception as e:\n            print('Failed to delete %s. Reason: %s' % (file_path, e))\n\n", "CodeBleu_score": 70.32553386608735}, {"code": "def clean_folder(folder)\n\t\n    for filename in os.listdir(folder):\n        file_path = os.path.join(folder, filename)\n        try:\n            if os.path.isfile(file_path) or os.path.islink(file_path):\n                os.unlink(file_path)\n            elif os.path.isdir(file_path):\n                shutil.rmtree(file_path)\n        except Exception as e:\n            print(f\"Failed to delete {file_path}. Reason: {e}\")\n\n", "CodeBleu_score": 77.24563372997861}]}
{"instance_id": "53588b481bdd", "generation_result": [{"code": "def next_batch(self, sample_size=1)\n\t\n    if self.count < sample_size:\n        return random.sample(self.buffer, self.count)\n    else:\n        batch = random.sample(self.buffer, sample_size)\n        if sample_size > 1:\n            self.buffer = [\n                experience for experience in self.buffer if experience not in batch\n            ]\n        else:\n            self.buffer.remove(batch[0])\n        self.count = self.count - sample_size\n        return batch\n\n", "CodeBleu_score": 38.667514009819925}, {"code": "def next_batch(self, sample_size=1)\n\t\n    if sample_size > len(self.experiences):\n        sample_size = len(self.experiences)\n    batch = random.sample(self.experiences, sample_size)\n    for experience in batch:\n        self.experiences.remove(experience)\n    return batch\n\n", "CodeBleu_score": 49.82036836182859}, {"code": "def next_batch(self, sample_size=1)\n\t\n    batch = []\n    sample_size = min(sample_size, len(self.memory))\n    batch = random.sample(self.memory, sample_size)\n    for sample in batch:\n        self.memory.remove(sample)\n    return batch", "CodeBleu_score": 27.61241791745912}]}
{"instance_id": "6bfa6a4e621c", "generation_result": [{"code": "def find_estimated_top_configs(self)\n\t\n    # Flatten each configuration in all_configs\n    flattened_configs = [\n        {k: v for k, v in flatten_dict(config).items() if isinstance(v, numbers.Number)}\n        for config in self.all_configs\n    ]\n    # Collect numerical values and convert them into a numpy array\n    X = np.array([list(config.values()) for config in flattened_configs])\n    # Predict estimates for these configurations\n    y_pred = self.cost_model.predict(X)\n    # Sort the configurations based on the predicted estimates\n    if self.metric == \"time\":\n        # Sort by time in ascending order\n        sorted_indices = np.argsort(y_pred)\n    elif self.metric == \"energy\":\n        # Sort by energy in ascending order\n        sorted_indices = np.argsort(y_pred)\n    elif self.metric == \"accuracy\":\n        # Sort by accuracy in descending order\n        sorted_indices = np.argsort(-y_pred)\n    else:\n        raise ValueError(\n            \"Invalid metric specified. Please choose from 'time', 'energy', or 'accuracy'.\"\n        )\n    # Return the indices of the top configurations\n    return sorted_indices[: self.top_n]\n\n", "CodeBleu_score": 28.771028965701184}, {"code": "def find_estimated_top_configs(self)\n\t\n    # Flatten each configuration in all_configs\n    flattened_configs = [\n        list(config.values()) for config in self.all_configs\n    ]\n\n    # Collect numerical values and convert to numpy array\n    numerical_values = np.array([\n        [value for value in config if isinstance(value, numbers.Number)]\n        for config in flattened_configs\n    ])\n\n    # Predict estimates for the numerical values\n    estimates = self.cost_model.predict(numerical_values)\n\n    # Sort the estimates based on the specified metric\n    if self.metric == \"mean_squared_error\":\n        sorted_indices = np.argsort(estimates)\n    elif self.metric == \"mean_absolute_error\":\n        sorted_indices = np.argsort(np.abs(estimates))\n    else:\n        raise ValueError(\n            \"Invalid metric specified. Supported metrics are 'mean_squared_error' and 'mean_absolute_error'.\"\n        )\n\n    # Return the indices of the top configurations\n    return sorted_indices\n\n", "CodeBleu_score": 28.4158696109714}, {"code": "def find_estimated_top_configs(self)\n\t\n    flattened_configs = [self.flatten_config(config) for config in self.all_configs]\n    numerical_configs = [\n        [value for value in config.values() if isinstance(value, numbers.Number)]\n        for config in flattened_configs\n    ]\n    X = np.array(numerical_configs)\n    y_pred = self.cost_model.predict(X)\n    sorted_indices = np.argsort(y_pred)\n    if self.metric == \"time\":\n        sorted_indices = sorted_indices[::-1]\n    return sorted_indices\n\n", "CodeBleu_score": 26.521697491080438}]}
{"instance_id": "d4cf48413a57", "generation_result": [{"code": "def flatten(d, parent_key='', sep='_')\n\t\n    items = []\n    for k, v in d.items():\n        new_key = parent_key + sep + k if parent_key else k\n        if isinstance(v, collections.abc.MutableMapping):\n            items.extend(flatten(v, new_key, sep=sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)\n\n", "CodeBleu_score": 83.2096634628176}, {"code": "def flatten(d, parent_key='', sep='_')\n\t\n    items = []\n    for k, v in d.items():\n        new_key = parent_key + sep + k if parent_key else k\n        if isinstance(v, collections.abc.MutableMapping):\n            items.extend(flatten(v, new_key, sep=sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)\n\n", "CodeBleu_score": 83.2096634628176}, {"code": "def flatten(d, parent_key='', sep='_')\n\t\n    items = []\n    for k, v in d.items():\n        new_key = parent_key + sep + k if parent_key else k\n        if isinstance(v, collections.abc.MutableMapping):\n            items.extend(flatten(v, new_key, sep=sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)\n\n", "CodeBleu_score": 83.2096634628176}]}
{"instance_id": "c2a514249c25", "generation_result": [{"code": "def run_after_tuning(self)\n\t\n    if self.optimal_cmd:\n        self.logger.info(f\"Running command: {self.optimal_cmd}\")\n        process = subprocess.Popen(self.optimal_cmd, shell=True)\n        process.wait()\n        self.logger.info(f\"Command completed successfully: {self.optimal_cmd}\")\n    else:\n        self.logger.info(\"Autotuning did not find an optimal configuration.\")\n\n", "CodeBleu_score": 51.31182708191482}, {"code": "def run_after_tuning(self)\n\t\n    if self.optimal_cmd:\n        # Run the command\n        start_time = time.time()\n        process = subprocess.Popen(self.optimal_cmd, shell=True)\n        process.wait()\n        end_time = time.time()\n        elapsed_time = end_time - start_time\n        # Log the success message\n        print(f\"Autotuning succeeded in {elapsed_time:.2f} seconds.\")\n        print(f\"Optimal command: {self.optimal_cmd}\")\n        # Log the optimal command to a file\n        with open(self.log_file, \"a\") as f:\n            f.write(f\"Autotuning succeeded in {elapsed_time:.2f} seconds.", "CodeBleu_score": 42.03601716473517}, {"code": "def run_after_tuning(self)\n\t\n    if self.optimal_cmd:\n        print(f\"", "CodeBleu_score": 1.3748722824052027}]}
{"instance_id": "61802de52df0", "generation_result": [{"code": "def run_job(self, exp: dict, reservations)\n\t\n    exp[\"master_port\"] = self.get_available_port()\n    exp[\"result_dir\"] = os.path.join(self.result_dir, exp[\"name\"])\n    os.makedirs(exp[\"result_dir\"], exist_ok=True)\n    # update user arguments based on argument mappings\n    exp[\"user_args\"] = self.update_user_args(exp[\"user_args\"])\n    # run the experiment in a new thread\n    t = threading.Thread(target=self.run_experiment, args=(exp,))\n    t.start()\n    # store the running experiment details with a timestamp\n    self.running_experiments[exp[\"name\"]] = {\n        \"timestamp\": time.time(),\n        \"exp\": exp,\n        \"thread\": t,\n    }\n\n", "CodeBleu_score": 18.578553508633238}, {"code": "def run_job(self, exp: dict, reservations)\n\t\n    exp[\"master_port\"] = self.master_port\n    exp[\"result_dir\"] = self.result_dir\n    exp[\"reservations\"] = reservations\n    exp[\"args\"] = update_args(exp[\"args\"], self.args_mapping)\n    exp[\"script\"] = os.path.join(self.root_dir, exp[\"script\"])\n    exp[\"timestamp\"] = time.time()\n    self.running_experiments.append(exp)\n    threading.Thread(target=run_experiment, args=(exp,)).start()\n\n", "CodeBleu_score": 12.280970543674862}, {"code": "def run_job(self, exp: dict, reservations)\n\t\n    exp[\"master_port\"] = self.next_available_port()\n    exp[\"result_dir\"] = os.path.join(self.args.result_dir, exp[\"name\"])\n    exp[\"user_args\"] = self.update_user_args(exp[\"user_args\"])\n    t = threading.Thread(target=self.run_experiment, args=(exp, reservations))\n    t.start()\n    self.running_experiments[exp[\"name\"]] = {\"thread\": t, \"timestamp\": time.time()}\n\n", "CodeBleu_score": 15.985048119170026}]}
{"instance_id": "4884e2e9665c", "generation_result": [{"code": "def save_exp_results_to_database(self, message, ranks=None, path=None):\n\t\n\n    if dist.is_initialized():\n        rank = dist.get_rank()\n    else:\n        rank = 0\n\n    if ranks is not None and rank not in ranks:\n        return\n\n    if path is None:\n        path = os.path.join(self.output_dir, \"results.jsonl\")\n\n    message = copy.deepcopy(message)\n    message[\"rank\"] = rank\n\n    with open(path, \"a\") as f:\n        f.write(json.dumps(message) + \"", "CodeBleu_score": 31.82753639248216}, {"code": "def save_exp_results_to_database(self, message, ranks=None, path=None):\n\t\n    if dist.is_initialized():\n        if dist.get_rank() == 0:\n            self.logger.info(message)\n            if ranks is None:\n                ranks = [0]\n            for rank in ranks:\n                if path is None:\n                    path = os.path.join(self.exp_path, f\"rank_{rank}.json\")\n                else:\n                    path = os.path.join(self.exp_path, path)\n                if os.path.exists(path):\n                    with open(path, \"r\") as f:\n                        data = json.load(f)\n                else:\n                    data = []\n                data.append(message)\n                with open(path, \"w\") as f:\n                    json.dump(data, f, indent=4)\n    else:\n        self.logger.info(message)\n        if path is None:\n            path = os.path.join(self.exp_path, \"rank_0.json\")\n        else:\n            path = os.path.join(self.exp_path, path)\n        if os.path.exists(path):\n            with open(path, \"r\") as f:\n                data = json.load(f)\n        else:\n            data = []\n        data.append(message)\n        with open(path, \"w\") as f:\n            json.dump(data, f, indent=4)\n\n", "CodeBleu_score": 30.09222396837702}, {"code": "def save_exp_results_to_database(self, message, ranks=None, path=None):\n\t\n    if not dist.is_initialized():\n        if ranks is None:\n            ranks = [0]\n        if path is None:\n            path = self.log_path\n        if dist.get_rank() in ranks:\n            message = {\n                \"rank\": dist.get_rank(),\n                \"message\": message,\n            }\n            with open(path, \"a\") as f:\n                json.dump(message, f)\n                f.write(\"", "CodeBleu_score": 29.786901041005148}]}
{"instance_id": "0429344c3c4d", "generation_result": [{"code": "def _build_tp_other_layer_map(self, layer_index: int)\n\t\n    layer_files = [\n        f\n        for f in os.listdir(self.path)\n        if f.startswith(f\"layer_{layer_index}_tp\")\n    ]\n    if not layer_files:\n        raise ValueError(f\"No layer files found for layer {layer_index}\")\n\n    tp_degree = int(re.search(r\"tp(\\d+)\", layer_files[0]).group(1))\n    partition_files = {}\n    for file in layer_files:\n        partition_index = int(re.search(r\"tp\\d+_(\\d+)\", file).group(1))\n        if partition_index not in partition_files:\n            partition_files[partition_index] = []\n        partition_files[partition_index].append(file)\n\n    return partition_files\n\n", "CodeBleu_score": 27.826319250375903}, {"code": "def _build_tp_other_layer_map(self, layer_index: int)\n\t\n    files = os.listdir(self.path)\n    tp_other_layer_map = {}\n    for file in files:\n        if file.startswith(f\"tp_degree_{layer_index}_\"):\n            match = re.search(r\"tp_degree_(\\d+)_(\\d+)\\.pt\", file)\n            if match:\n                tp_degree = int(match.group(1))\n                partition_index = int(match.group(2))\n                if tp_degree not in tp_other_layer_map:\n                    tp_other_layer_map[tp_degree] = {}\n                if partition_index not in tp_other_layer_map[tp_degree]:\n                    tp_other_layer_map[tp_degree][partition_index] = []\n                tp_other_layer_map[tp_degree][partition_index].append(file)\n    return tp_other_layer_map\n\n", "CodeBleu_score": 26.318061115681534}, {"code": "def _build_tp_other_layer_map(self, layer_index: int)\n\t\n    if not os.path.exists(self.layer_file_dir):\n        raise Exception(\"layer_file_dir does not exist\")\n    if layer_index > self.num_layers - 1:\n        raise Exception(\"layer_index out of range\")\n    layer_files = os.listdir(self.layer_file_dir)\n    layer_files = [f for f in layer_files if f.startswith(f\"layer_{layer_index}\")]\n    if len(layer_files) == 0:\n        raise Exception(\"no layer files found\")\n    tp_other_layer_map: Dict[int, list] = {}\n    for file in layer_files:\n        tp_degree = int(re.findall(r\"tp(\\d+)_other\", file)[0])\n        if tp_degree not in tp_other_layer_map:\n            tp_other_layer_map[tp_degree] = []\n        tp_other_layer_map[tp_degree].append(file)\n    return tp_other_layer_map\n\n", "CodeBleu_score": 33.02093105796818}]}
{"instance_id": "1b65ecb9be93", "generation_result": [{"code": "def system_info()\n\t\n    return {\n        \"OS\": platform.system(),\n        \"architecture\": platform.architecture(),\n        \"version\": platform.version(),\n        \"processor\": platform.processor(),\n        \"python\": platform.python_version(),\n    }\n\n", "CodeBleu_score": 94.74544725361}, {"code": "def system_info()\n\t\n    system_info = {}\n    system_info[\"OS\"] = platform.system()\n    system_info[\"Architecture\"] = platform.architecture()[0]\n    system_info[\"OS Version\"] = platform.version()\n    system_info[\"Processor\"] = platform.processor()\n    system_info[\"Python Version\"] = platform.python_version()\n    return system_info\n\n", "CodeBleu_score": 40.67737727859825}, {"code": "def system_info()\n\t\n    return {\n        \"OS\": platform.system(),\n        \"architecture\": platform.architecture(),\n        \"version\": platform.version(),\n        \"processor\": platform.processor(),\n        \"python_version\": platform.python_version(),\n    }\n\n", "CodeBleu_score": 89.24161063858953}]}
{"instance_id": "969bcc5cf3b5", "generation_result": [{"code": "def cuda_info()\n\t\n    info = {}\n    info[\"GPUs\"] = []\n    info[\"CUDA available\"] = torch.cuda.is_available()\n    info[\"CUDA version\"] = torch.version.cuda\n    info[\"CUDA device count\"] = torch.cuda.device_count()\n    for i in range(torch.cuda.device_count()):\n        info[\"GPUs\"].append(torch.cuda.get_device_name(i))\n    return info\n\n", "CodeBleu_score": 45.35517261918086}, {"code": "def cuda_info()\n\t\n    cuda_info = {}\n    cuda_info[\"CUDA_DEVICE_NAMES\"] = [torch.cuda.get_device_name(i) for i in range(torch.cuda.device_count())]\n    cuda_info[\"CUDA_AVAILABLE\"] = torch.cuda.is_available()\n    cuda_info[\"CUDA_VERSION\"] = torch.version.cuda\n    return cuda_info\n\n", "CodeBleu_score": 54.24324369760517}, {"code": "def cuda_info()\n\t\n    cuda_info_dict = {}\n    cuda_info_dict[\"cuda_available\"] = torch.cuda.is_available()\n    cuda_info_dict[\"cuda_version\"] = torch.version.cuda\n    cuda_info_dict[\"cuda_devices\"] = []\n    for i in range(torch.cuda.device_count()):\n        cuda_info_dict[\"cuda_devices\"].append(torch.cuda.get_device_name(i))\n    return cuda_info_dict\n\n", "CodeBleu_score": 46.0571155660343}]}
{"instance_id": "a2ba908a4c70", "generation_result": [{"code": "def compute_encoder_accuracy(dataset_items, encoder_manager)\n\t\n    class_name_key = encoder_manager.config.class_name_key\n    use_speaker_embedding = encoder_manager.config.use_speaker_embedding\n    if use_speaker_embedding:\n        if encoder_manager.speaker_manager is not None:\n            speaker_mapping = encoder_manager.speaker_manager.speaker_mapping\n        else:\n            raise RuntimeError(\"[!] Speaker Encoder Manager has not been found.\")\n    else:\n        speaker_mapping = None\n    class_accuracies = {}\n    for item in tqdm(dataset_items):\n        # load audio\n        audio = encoder_manager.ap.load_wav(item[0], sr=encoder_manager.ap.sample_rate)\n        # compute speaker embedding\n        label = item[2]\n        if speaker_mapping is not None:\n            label = speaker_mapping[label]\n        embedding = encoder_manager.compute_d_vector_from_clip(audio)\n        # predict label\n        if use_speaker_embedding:\n            pred_class_name = encoder_manager.classify_with_speaker(embedding, label)\n        else:\n            pred_class_name = encoder_manager.classify_with_speaker(embedding)\n        # record accuracy\n        true_class_name = item[class_name_key]\n        if true_class_name not in class_accuracies:\n            class_accuracies[true_class_name] = {\"correct\": 0, \"total\": 0}\n        if pred_class_name == true_class_name:\n            class_accuracies[true_class_name][\"correct\"] += 1\n        class_accuracies[true_class_name][\"total\"] += 1\n    # calculate accuracy\n    total_accuracy = 0\n    for class_name, accuracies in class_accuracies.items():\n        accuracy = accuracies[\"correct\"] / accuracies[\"total\"]\n        total_accuracy += accuracy\n        print(f\"Class {class_name}: {accuracy:.2f}\")\n    average_accuracy = total_accuracy / len(class_accuracies)\n    print(f\"", "CodeBleu_score": 39.450376570711086}, {"code": "def compute_encoder_accuracy(dataset_items, encoder_manager)\n\t\n    class_name_key = encoder_manager.config.class_name_key\n    map_class_id_to_class_name = encoder_manager.config.map_class_id_to_class_name\n    if map_class_id_to_class_name:\n        class_ids = list(map_class_id_to_class_name.keys())\n    else:\n        class_ids = None\n    accuracy_dict = {}\n    for item in tqdm(dataset_items):\n        # extract the audio file\n        audio_file = item[encoder_manager.config.audio_key]\n        # compute the embedding\n        embedding = encoder_manager.compute_embedding(audio_file)\n        # predict the class label if criteria and mappings are available\n        if class_name_key in item and (map_class_id_to_class_name or class_ids is not None):\n            true_class_name = item[class_name_key]\n            if map_class_id_to_class_name:\n                class_id = map_class_id_to_class_name[true_class_name]\n                pred_class_name = encoder_manager.classify_embedding(embedding, class_id=class_id)\n            else:\n                pred_class_name = encoder_manager.classify_embedding(embedding, class_ids=class_ids)\n            # record the accuracy for each class\n            if true_class_name not in accuracy_dict:\n                accuracy_dict[true_class_name] = {\"correct\": 0, \"total\": 0}\n            if pred_class_name == true_class_name:\n                accuracy_dict[true_class_name][\"correct\"] += 1\n            accuracy_dict[true_class_name][\"total\"] += 1\n    # calculate and print the accuracy for each class\n    for class_name in accuracy_dict:\n        correct = accuracy_dict[class_name][\"correct\"]\n        total = accuracy_dict[class_name][\"total\"]\n        accuracy = correct / total\n        print(f\"Class {class_name} accuracy: {accuracy}\")\n    # calculate and print the average", "CodeBleu_score": 36.78020246374872}, {"code": "def compute_encoder_accuracy(dataset_items, encoder_manager)\n\t\n    class_name_key = encoder_manager.config.class_name_key\n    map_class_id_to_class_name = encoder_manager.config.map_class_id_to_class_name\n    if class_name_key is None:\n        raise ValueError(\"class_name_key cannot be None.\")\n    if map_class_id_to_class_name is None:\n        map_class_id_to_class_name = {}\n\n    accuracy_dict = {}\n    for item in tqdm(dataset_items):\n        # extract the audio file\n        audio_file = item[encoder_manager.config.audio_key]\n        # compute the embedding\n        embedding = encoder_manager.compute_embedding(audio_file)\n        # predict the class label\n        predicted_class_id = encoder_manager.predict_class_id(embedding)\n        predicted_class_name = map_class_id_to_class_name.get(predicted_class_id, predicted_class_id)\n        # record the accuracy\n        true_class_name = item[class_name_key]\n        if true_class_name not in accuracy_dict:\n            accuracy_dict[true_class_name] = {\"correct\": 0, \"total\": 0}\n        if true_class_name == predicted_class_name:\n            accuracy_dict[true_class_name][\"correct\"] += 1\n        accuracy_dict[true_class_name][\"total\"] += 1\n\n    # calculate the accuracy for each class\n    for class_name, class_accuracy in accuracy_dict.items():\n        accuracy = class_accuracy[\"correct\"] / class_accuracy[\"total\"]\n        print(f\"Accuracy for class {class_name}: {accuracy:.4f}\")\n\n    # calculate the average accuracy across all classes\n    total_correct = sum(class_accuracy[\"correct\"] for class_accuracy in accuracy_dict.values())\n    total_samples = sum(class_accuracy[\"total\"] for class_accuracy in accuracy_dict.values())\n    average_accuracy = total_correct / total_samples\n    print(f\"Average accuracy", "CodeBleu_score": 35.99596228558299}]}
{"instance_id": "e01ec69ad58a", "generation_result": [{"code": "def adjust_path_and_remove_silence(audio_path)\n\t\n    output_path = audio_path.replace(input_dir, output_dir)\n    if os.path.exists(output_path) and not force:\n        return output_path, False\n    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n    speech_detected = remove_silence(\n        model,\n        audio_path,\n        output_path,\n        trim_long_silences=trim_long_silences,\n        vad_threshold=vad_threshold,\n        vad_padding=vad_padding,\n        vad_sample_rate=vad_sample_rate,\n    )\n    return output_path, speech_detected\n\n", "CodeBleu_score": 44.07850552865606}, {"code": "def adjust_path_and_remove_silence(audio_path)\n\t\n    output_path = audio_path.replace(input_dir, output_dir)\n    if os.path.exists(output_path) and not force:\n        return output_path, False\n    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n    wav = model.get_input(audio_path)\n    speech_dict = model.get_speech_timestamps(wav, vad_utils)\n    wav_nosil = remove_silence(wav, speech_dict, vad_utils, fill_in=True)\n    model.save_wav(wav_nosil, output_path)\n    return output_path, True\n\n", "CodeBleu_score": 40.759445039280564}, {"code": "def adjust_path_and_remove_silence(audio_path)\n\t\n    # adjust the output path\n    output_path = audio_path.replace(args.input_dir, args.output_dir)\n    # if the file already exists and force is not specified, return the output path and a False flag\n    if os.path.exists(output_path) and not args.force:\n        return output_path, False\n    # create the necessary directory structure\n    pathlib.Path(os.path.dirname(output_path)).mkdir(parents=True, exist_ok=True)\n    # remove silence from the audio\n    vad_model, vad_utils = get_vad_model_and_utils(args.vad_model_path, args.vad_threshold, args.vad_aggressiveness)\n    has_speech = remove_silence(\n        audio_path,\n        output_path,\n        vad_model,\n        vad_utils,\n        args.vad_min_silence_duration,\n        args.vad_min_non_speech_duration,\n        args.vad_padding_duration,\n    )\n    return output_path, has_speech\n\n", "CodeBleu_score": 49.657389569256594}]}
{"instance_id": "7e0bdb0feff8", "generation_result": [{"code": "def resample_files(input_dir, output_sr, output_dir=None, file_ext=\"wav\", n_jobs=10)\n\t\n    if output_dir is not None:\n        copytree(input_dir, output_dir)\n        input_dir = output_dir\n\n    audio_files = glob.glob(os.path.join(input_dir, \"**\", f\"*.{file_ext}\"), recursive=True)\n\n    with Pool(n_jobs) as p:\n        list(tqdm(p.imap(lambda file: resample_file(file, output_sr), audio_files), total=len(audio_files)))\n\n    print(\"Resampling complete.\")\n\n", "CodeBleu_score": 37.00053217209061}, {"code": "def resample_files(input_dir, output_sr, output_dir=None, file_ext=\"wav\", n_jobs=10)\n\t\n    if output_dir:\n        copytree(input_dir, output_dir)\n        input_dir = output_dir\n    input_files = glob.glob(os.path.join(input_dir, \"**\", f\"*.{file_ext}\"), recursive=True)\n    with Pool(n_jobs) as pool:\n        with tqdm(total=len(input_files), desc=\"Resampling audio files\") as pbar:\n            for _ in pool.imap_unordered(\n                resample_file,\n                [\n                    (input_file, output_sr, output_dir)\n                    for input_file in input_files\n                ],\n            ):\n                pbar.update()\n    print(f\"Resampled {len(input_files)} audio files to {output_sr} Hz\")\n\n", "CodeBleu_score": 46.62531935968908}, {"code": "def resample_files(input_dir, output_sr, output_dir=None, file_ext=\"wav\", n_jobs=10)\n\t\n    if output_dir:\n        copytree(input_dir, output_dir, dirs_exist_ok=True)\n        input_dir = output_dir\n\n    files = glob.glob(os.path.join(input_dir, \"**\", f\"*.{file_ext}\"), recursive=True)\n    with Pool(n_jobs) as pool:\n        list(tqdm(pool.imap(resample_file, files), total=len(files)))\n\n", "CodeBleu_score": 28.069396960593586}]}
{"instance_id": "a1351834ca57", "generation_result": [{"code": "def evaluation(model, criterion, data_loader, global_step)\n\t\n    model.eval()\n    eval_loss = 0.0\n    eval_steps = 0\n    for batch in data_loader:\n        # reshape samples to group samples of each class\n        batch = [item.unsqueeze(0) for item in batch]\n        inputs, labels = batch\n        if torch.cuda.is_available():\n            inputs = inputs.cuda()\n            labels = labels.cuda()\n        with torch.no_grad():\n            embeds = model(inputs)\n            loss = criterion(embeds, labels)\n            eval_loss += loss.item()\n        eval_steps += 1\n\n    eval_loss = eval_loss / (eval_steps * data_loader.batch_size)\n    print(\n        \"Validation loss at step {}: {:g}\".format(global_step, eval_loss)\n    )\n    plot_embeddings(embeds, labels, global_step, \"eval\")\n    return eval_loss\n\n", "CodeBleu_score": 28.70164007532917}, {"code": "def evaluation(model, criterion, data_loader, global_step)\n\t\n    model.eval()\n    eval_loss = 0.0\n    eval_steps = 0\n    for i, batch in enumerate(data_loader):\n        # reshape data\n        inputs, labels = batch\n        inputs = inputs.to(model.device)\n        labels = labels.to(model.device)\n        # compute loss\n        with torch.no_grad():\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            eval_loss += loss.item()\n            eval_steps += 1\n    eval_loss = eval_loss / eval_steps\n    print(\n        f\"Eval Loss: {eval_loss:.4f} | Eval Steps: {eval_steps} | Global Step: {global_step}\"\n    )\n    return eval_loss\n\n", "CodeBleu_score": 25.549407229380883}, {"code": "def evaluation(model, criterion, data_loader, global_step)\n\t\n    model.eval()\n    eval_loss = 0.0\n    eval_steps = 0\n    with torch.no_grad():\n        for idx, batch in enumerate(data_loader):\n            # batch\n            x, y = batch\n            x = x.to(model.device)\n            y = y.to(model.device)\n\n            # forward\n            embeddings = model(x)\n            loss = criterion(embeddings, y)\n            eval_loss += loss.item()\n            eval_steps += 1\n\n            # plot embeddings\n            if idx == len(data_loader) - 1:\n                plot_embeddings(embeddings, y, global_step, \"eval\")\n\n    eval_loss /= eval_steps\n    print(f\"Eval Loss: {eval_loss}\")\n    return eval_loss\n\n", "CodeBleu_score": 22.406927492780987}]}
{"instance_id": "565f8ed09916", "generation_result": [{"code": "def register_config(model_name: str) -> Coqpit\n\t\n    config_class_name = model_name.replace(\"_\", \"\") + \"Config\"\n    if model_name == \"xtts\":\n        config_class_name = \"XTTSConfig\"\n    try:\n        config_class = find_module(\"TTS.config.shared_configs\", config_class_name)\n    except ModuleNotFoundError:\n        config_class = find_module(\"TTS.vocoder.configs\", config_class_name)\n    return config_class\n\n", "CodeBleu_score": 26.083963228900913}, {"code": "def register_config(model_name: str) -> Coqpit\n\t\n    config_class_name = model_name.replace(\"_\", \"\") + \"Config\"\n    if model_name == \"xtts\":\n        config_class_name = \"XttsConfig\"\n    try:\n        config_class = find_module(\"TTS.config.shared_configs\", config_class_name)\n    except ModuleNotFoundError:\n        # try to find the model specific config\n        config_class = find_module(\"TTS.vocoder.configs\", config_class_name)\n    return config_class\n\n", "CodeBleu_score": 26.178010057708594}, {"code": "def register_config(model_name: str) -> Coqpit\n\t\n    config_class_name = model_name.replace(\"_\", \"\") + \"Config\"\n    config_class = None\n    if model_name == \"xtts\":\n        config_class = \"TTS.config.xtts_config.XTTSConfig\"\n    if config_class is None:\n        # try to find the model specific config\n        try:\n            config_class = find_module(\"TTS.config.model_configs.\" + config_class_name.lower())\n        except ModuleNotFoundError:\n            # try to find the shared config\n            try:\n                config_class = find_module(\"TTS.config.shared_configs.\" + config_class_name.lower())\n            except ModuleNotFoundError:\n                pass\n    if config_class is None:\n        raise ModuleNotFoundError(f\" [!] Config for {model_name} cannot be found.\")\n    return config_class\n\n", "CodeBleu_score": 49.379025913554145}]}
{"instance_id": "0ea7862506a7", "generation_result": [{"code": "def load_config(config_path: str) -> Coqpit\n\t\n    config_dict = {}\n    ext = os.path.splitext(config_path)[1]\n    if ext in (\".yml\", \".yaml\"):\n        with fsspec.open(config_path, \"r\", encoding=\"utf-8\") as yaml_file:\n            config_dict = yaml.safe_load(yaml_file)\n    elif ext == \".json\":\n        with fsspec.open(config_path, \"r\", encoding=\"utf-8\") as json_file:\n            config_dict = json.load(json_file)\n    else:\n        raise TypeError(f\"Unknown type of config file {config_path}\")\n    model_name = _process_model_name(config_dict)\n    config_class = register_config(model_name)\n    config = config_class()\n    config.from_dict(config_dict)\n    return config\n\n", "CodeBleu_score": 62.08493645213793}, {"code": "def load_config(config_path: str) -> Coqpit\n\t\n    _, ext = os.path.splitext(config_path)\n    if ext in (\".yml\", \".yaml\"):\n        with fsspec.open(config_path, \"r\", encoding=\"utf-8\") as f:\n            data = yaml.safe_load(f)\n    elif ext == \".json\":\n        with fsspec.open(config_path, \"r\", encoding=\"utf-8\") as f:\n            data = json.load(f)\n    else:\n        msg = f\"Only .json and .yaml are supported. {config_path} given.\"\n        raise ValueError(msg)\n\n    model_name = data[\"model\"]\n    model_config = data[model_name]\n    config_class = register_config(model_name.lower())\n    config = config_class()\n    config.from_dict(model_config)\n    config.model = model_name\n    return config\n\n", "CodeBleu_score": 62.691409914638626}, {"code": "def load_config(config_path: str) -> Coqpit\n\t\n    config_dict = {}\n    ext = os.path.splitext(config_path)[1]\n    if ext in (\".yaml\", \".yml\"):\n        with fsspec.open(config_path, \"r\", encoding=\"utf-8\") as yaml_file:\n            config_dict = yaml.safe_load(yaml_file)\n    elif ext == \".json\":\n        with fsspec.open(config_path, \"r\", encoding=\"utf-8\") as json_file:\n            config_dict = json.load(json_file)\n    else:\n        raise TypeError(f\".{ext} files are not supported. Use .json or .yaml\")\n    model_name = _process_model_name(config_dict)\n    config_class = register_config(model_name)\n    config = config_class()\n    config.from_dict(config_dict)\n    return config\n\n", "CodeBleu_score": 58.94561985696002}]}
{"instance_id": "e0db68b63143", "generation_result": [{"code": "def list_files(basePath, validExts=None, contains=None)\n\t\n    # loop over the directory structure\n    for (rootDir, dirNames, filenames) in os.walk(basePath):\n        # loop over the filenames in the current directory\n        for filename in filenames:\n            # if the contains string is not none and the filename does not contain\n            # the supplied string, then ignore the file\n            if contains is not None and filename.find(contains) == -1:\n                continue\n\n            # determine the file extension of the current file\n            ext = filename[filename.rfind(\".\") :].lower()\n\n            # check to see if the file is an image and should be processed\n            if validExts is None or ext.endswith(validExts):\n                # construct the path to the image and yield it\n                imagePath = os.path.join(rootDir, filename)\n                yield imagePath\n\n", "CodeBleu_score": 76.480742398644}, {"code": "def list_files(basePath, validExts=None, contains=None)\n\t\n    # Loop over the items in the base directory\n    for (rootDir, dirNames, filenames) in os.walk(basePath):\n        # Loop over the filenames in the current directory\n        for filename in filenames:\n            # If the filename contains the 'contains' string or there is no 'contains' string,\n            # and the file extension is in the valid extensions (if provided),\n            # yield the full file path\n            if contains is not None and filename.find(contains) == -1:\n                continue\n\n            # Extract the file extension from the filename\n            ext = filename[filename.rfind(\".\"):].lower()\n\n            # Check if the file should be processed based on its extension\n            if validExts is None or ext.endswith(validExts):\n                # Construct the full file path by joining the root directory and the filename\n                filePath = os.path.join(rootDir, filename)\n                yield filePath\n\n", "CodeBleu_score": 77.44616184848208}, {"code": "def list_files(basePath, validExts=None, contains=None)\n\t\n    for (rootDir, dirNames, filenames) in os.walk(basePath):\n        for filename in filenames:\n            if contains is not None and filename.find(contains) == -1:\n                continue\n\n            ext = filename[filename.rfind(\".\"):].lower()\n\n            if validExts is not None and ext.endswith(validExts):\n                filePath = os.path.join(rootDir, filename)\n                yield filePath\n\n", "CodeBleu_score": 75.10916808180212}]}
{"instance_id": "b836fcaf0a27", "generation_result": [{"code": "def load_model(xtts_checkpoint, xtts_config, xtts_vocab)\n\t\n    torch.cuda.empty_cache()\n    if not xtts_checkpoint or not xtts_config or not xtts_vocab:\n        return \"Error: xtts_checkpoint, xtts_config, and xtts_vocab are required.\"\n\n    print(\"Loading XTTS model...\")\n    config = XttsConfig.from_json_file(xtts_config)\n    model = Xtts(config)\n    model.load_checkpoint(xtts_checkpoint, eval=True)\n    model.load_vocab(xtts_vocab)\n    if torch.cuda.is_available():\n        model.cuda()\n    print(\"XTTS model loaded successfully.\")\n    return \"XTTS model loaded successfully.\"\n\n", "CodeBleu_score": 53.94684320978248}, {"code": "def load_model(xtts_checkpoint, xtts_config, xtts_vocab)\n\t\n    torch.cuda.empty_cache()\n    if not xtts_checkpoint or not xtts_config or not xtts_vocab:\n        return \"Error: Please provide all the required paths for the XTTS model.\"\n    print(\"Loading the XTTS model...\")\n    config = XttsConfig.from_json_file(xtts_config)\n    model = Xtts(config)\n    model.load_checkpoint(xtts_checkpoint, eval=True)\n    model.load_vocab(xtts_vocab)\n    if torch.cuda.is_available():\n        model.cuda()\n    print(\"XTTS model loaded successfully.\")\n    return model\n\n", "CodeBleu_score": 52.16037067873452}, {"code": "def load_model(xtts_checkpoint, xtts_config, xtts_vocab)\n\t\n    torch.cuda.empty_cache()\n    if xtts_checkpoint is None or xtts_config is None or xtts_vocab is None:\n        return \"Error: Please provide a checkpoint, config, and vocab path.\"\n    config = XttsConfig.from_json_file(xtts_config)\n    model = Xtts(config)\n    model.load_checkpoint(xtts_checkpoint, eval=True)\n    model.load_vocab(xtts_vocab)\n    if torch.cuda.is_available():\n        model.cuda()\n    print(\"XTTS model loaded successfully.\")\n    return \"XTTS model loaded successfully.\"\n\n", "CodeBleu_score": 50.28155178876208}]}
{"instance_id": "c5d63aa0df26", "generation_result": [{"code": "def compute_embedding(self, x, num_frames=250, num_eval=10, return_mean=True, l2_norm=True)\n\t\n    if self.use_torch_spec:\n        hop_length = self.ap.hop_length\n        if num_frames * hop_length > x.shape[1]:\n            num_frames = x.shape[1] // hop_length\n        offsets = torch.linspace(0, x.shape[1] - num_frames * hop_length, num_eval)\n        new_offsets = (offsets // hop_length) * hop_length\n        x = torch.stack([x[:, int(v): int(v) + num_frames * hop_length] for v in new_offsets], dim=0)\n    else:\n        offsets = torch.linspace(0, x.shape[1] - num_frames, num_eval)\n        x = torch.stack([x[:, int(v): int(v) + num_frames] for v in offsets], dim=0)\n    x = self.forward(x)\n    if return_mean:\n        x = x.mean(0)\n        if l2_norm:\n            x = x / torch.norm(x, p=2)\n    return x\n\n", "CodeBleu_score": 38.279706567901826}, {"code": "def compute_embedding(self, x, num_frames=250, num_eval=10, return_mean=True, l2_norm=True)\n\t\n    if self.use_torch_spec:\n        hop_length = self.ap.hop_length\n        num_frames = min(num_frames, x.shape[-1] // hop_length + 1)\n    else:\n        num_frames = min(num_frames, x.shape[-1])\n    offsets = torch.linspace(0, x.shape[-1] - x.shape[-1] // num_frames * num_frames, num_eval)\n    offsets = (offsets / hop_length).long()\n    frames = []\n    for offset in offsets:\n        if self.use_torch_spec:\n            frame = x[..., offset * hop_length : (offset * hop_length + num_frames * hop_length)]\n        else:\n            frame = x[..., offset : offset + num_frames]\n        frames.append(frame)\n    x = torch.cat(frames, 0)\n    embeddings = self.model.forward(x.to(self.device))\n    if return_mean:\n        embeddings = embeddings.mean(0, keepdim=True)\n    if l2_norm:\n        embeddings = embeddings / torch.norm(embeddings, p=2, dim=1, keepdim=True)\n    return embeddings\n\n", "CodeBleu_score": 42.24638380498936}, {"code": "def compute_embedding(self, x, num_frames=250, num_eval=10, return_mean=True, l2_norm=True)\n\t\n    if isinstance(x, torch.Tensor):\n        if x.ndim == 1:\n            x = x.unsqueeze(0)\n        if x.ndim == 2:\n            x = x.unsqueeze(0)\n    if self.use_torchaudio_spec:\n        offset = self.model.hop_length * num_frames\n        num_frames = int(x.shape[1] / offset) + 1\n        offset_list = [i * offset for i in range(num_frames)]\n        if x.shape[1] < offset * num_frames:\n            offset_list[-1] = x.shape[1] - offset\n        if len(offset_list) > num_eval:\n            offset_list = offset_list[:: len(offset_list) // num_eval]\n        if offset_list[-1] < x.shape[1] - offset:\n            offset_list[-1] = x.shape[1] - offset\n        mel = [\n            torchaudio.compliance.kaldi.fbank(\n                waveform=x[:, offset : offset + self.model.n_fft],\n                num_mel_bins=self.model.num_mel_bins,\n                sample_frequency=self.model.sample_rate,\n                use_energy=False,\n            ).transpose(0, 1)\n            for offset in offset_list\n        ]\n        mel = torch.nn.utils.rnn.pad_sequence(mel, batch_first=True).unsqueeze(1).to(x.device)\n    else:\n        mel = self.ap.melspec(x)\n    if self.model.normalize_emb:\n        mel = self.ap.normalize(mel)\n    embeddings = self.model.encoder(mel)\n    if return_mean:\n        embeddings = torch.mean(embeddings, dim=1)\n    if l2_norm:\n        embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=-1)\n    return embeddings\n\n", "CodeBleu_score": 35.51390238262136}]}
{"instance_id": "06f5b0335b26", "generation_result": [{"code": "def create_layer(self, block, planes, blocks, stride=1)\n\t\n    downsample = None\n    if stride != 1 or self.in_planes != planes * block.expansion:\n        downsample = nn.Sequential(\n            nn.Conv2d(\n                self.in_planes,\n                planes * block.expansion,\n                kernel_size=1,\n                stride=stride,\n                bias=False,\n            ),\n            nn.BatchNorm2d(planes * block.expansion),\n        )\n\n    layers = []\n    layers.append(block(self.in_planes, planes, stride, downsample))\n    self.in_planes = planes * block.expansion\n    for i in range(1, blocks):\n        layers.append(block(self.in_planes, planes))\n\n    return nn.Sequential(*layers)\n\n", "CodeBleu_score": 74.32936672517143}, {"code": "def create_layer(self, block, planes, blocks, stride=1)\n\t\n    downsample = None\n    if stride != 1 or self.inplanes != planes * block.expansion:\n        downsample = nn.Sequential(\n            nn.Conv1d(\n                self.inplanes,\n                planes * block.expansion,\n                kernel_size=1,\n                stride=stride,\n                bias=False,\n            ),\n            nn.BatchNorm1d(planes * block.expansion),\n        )\n\n    layers = []\n    layers.append(block(self.inplanes, planes, stride, downsample))\n    self.inplanes = planes * block.expansion\n    for _ in range(1, blocks):\n        layers.append(block(self.inplanes, planes))\n\n    return nn.Sequential(*layers)\n\n", "CodeBleu_score": 81.3700031989333}, {"code": "def create_layer(self, block, planes, blocks, stride=1)\n\t\n    downsample = None\n    if stride != 1 or self.inplanes != planes * block.expansion:\n        downsample = nn.Sequential(\n            nn.Conv2d(\n                self.inplanes,\n                planes * block.expansion,\n                kernel_size=1,\n                stride=stride,\n                bias=False,\n            ),\n            nn.BatchNorm2d(planes * block.expansion),\n        )\n\n    layers = []\n    layers.append(block(self.inplanes, planes, stride, downsample))\n    self.inplanes = planes * block.expansion\n    for _ in range(1, blocks):\n        layers.append(block(self.inplanes, planes))\n\n    return nn.Sequential(*layers)\n\n", "CodeBleu_score": 82.04951139333106}]}
{"instance_id": "9ac60597b633", "generation_result": [{"code": "def plot_embeddings(embeddings, num_classes_in_batch)\n\t\n\n    # Calculate the number of utterances per class\n    num_utterances_per_class = [\n        sum(num_classes_in_batch == i) for i in range(len(set(num_classes_in_batch)))\n    ]\n\n    # Limit the number of classes to 10 if necessary\n    num_classes_to_plot = min(len(set(num_classes_in_batch)), 10)\n\n    # Calculate the number of utterances per class to plot\n    num_utterances_per_class_to_plot = num_utterances_per_class[:num_classes_to_plot]\n\n    # Reduce the dimensionality using UMAP\n    reducer = umap.UMAP(n_components=2)\n    embeddings_umap = reducer.fit_transform(embeddings)\n\n    # Create a scatter plot with colors representing different classes\n    fig = matplotlib.pyplot.figure(figsize=(10, 10))\n    ax = fig.add_subplot(111)\n    for i, (num_utterances, color) in enumerate(\n        zip(num_utterances_per_class_to_plot, matplotlib.pyplot.cm.tab10.colors)\n    ):\n        start_index = sum(num_utterances_per_class_to_plot[:i])\n        end_index = start_index + num_utterances\n        ax.scatter(\n            embeddings_umap[start_index:end_index, 0],\n            embeddings_umap[start_index:end_index, 1],\n            c=color,\n            label=f\"Class {i}\",\n            alpha=0.5,\n        )\n    ax.legend()\n    ax.set_title(\"UMAP projection\")\n    ax.set_aspect(\"equal\")\n\n    # Save the plot\n    fig.savefig(\"umap.png\")\n\n    return fig", "CodeBleu_score": 29.805862758444047}, {"code": "def plot_embeddings(embeddings, num_classes_in_batch)\n\t\n    num_classes = len(num_classes_in_batch)\n    if num_classes > 10:\n        num_classes = 10\n        num_classes_in_batch = num_classes_in_batch[:num_classes]\n\n    num_utterances = sum(num_classes_in_batch)\n\n    # Perform dimensionality reduction with UMAP\n    reducer = umap.UMAP(n_components=2, random_state=42)\n    reduced_embeddings = reducer.fit_transform(embeddings[:num_utterances])\n\n    # Create a scatter plot with colors representing different classes\n    fig, ax = matplotlib.pyplot.subplots()\n    for i, num_utt in enumerate(num_classes_in_batch):\n        start_idx = sum(num_classes_in_batch[:i])\n        end_idx = start_idx + num_utt\n        ax.scatter(reduced_embeddings[start_idx:end_idx, 0], reduced_embeddings[start_idx:end_idx, 1], label=f\"Class {i}\")\n\n    ax.legend()\n    ax.set_title(\"UMAP projection\")\n    ax.set_aspect(\"equal\")\n    fig.savefig(\"umap\")\n    return fig", "CodeBleu_score": 36.33051327254344}, {"code": "def plot_embeddings(embeddings, num_classes_in_batch)\n\t\n    labels = [i for i in range(num_classes_in_batch) for _ in range(10)]\n    mapper = umap.UMAP().fit(embeddings)\n    embedding = mapper.transform(embeddings)\n    fig, ax = matplotlib.pyplot.subplots()\n    ax.scatter(embedding[:, 0], embedding[:, 1], c=labels)\n    ax.set_title(\"UMAP projection\")\n    ax.axis(\"equal\")\n    fig.savefig(\"umap\")\n    return fig", "CodeBleu_score": 17.89540401605746}]}
{"instance_id": "19c2b4af256d", "generation_result": [{"code": "def calc_cosine_sim(self, dvecs, centroids)\n\t\n    cos_sim_mat = []\n    for spk_idx, centroid in enumerate(centroids):\n        # Compute the cosine similarity matrix for a speaker\n        mask = torch.ones_like(dvecs)\n        mask[spk_idx * self.num_utts : (spk_idx + 1) * self.num_utts, :] = 0\n        exclude_centroids = mask * centroids\n        exclude_centroids = exclude_centroids.sum(dim=0, keepdim=True) / (\n            mask.sum(dim=0, keepdim=True) - 1\n        )\n        # exclude_centroids contains the sum of the centroids excluding the utterance of the speaker\n        # Calculate cosine similarity\n        mask = mask.to(self.device)\n        centroids = centroids.to(self.device)\n        dvecs = dvecs.to(self.device)\n        cos_sim = (\n            nn.functional.cosine_similarity(dvecs, centroids, dim=-1)\n            .view(self.num_spks, self.num_utts)\n            .masked_fill(mask.to(torch.bool), -float(\"inf\"))\n        )\n        # Clamp the cosine similarity values\n        cos_sim = cos_sim.clamp(min=1e-7)\n        cos_sim_mat.append(cos_sim)\n    cos_sim_mat = torch.cat(cos_sim_mat, dim=-1)\n    return cos_sim_mat\n\n", "CodeBleu_score": 29.589798326764083}, {"code": "def calc_cosine_sim(self, dvecs, centroids)\n\t\n    cos_sim_matrix = []\n    for spk_idx, dvecs_spk in enumerate(dvecs):\n        centroids_spk = torch.cat(\n            (centroids[:spk_idx], centroids[spk_idx + 1 :]), dim=0\n        )\n        centroids_spk = torch.mean(centroids_spk, dim=0, keepdim=True)\n        cos_sim_spk = []\n        for dvec_uttr in dvecs_spk:\n            dvec_uttr = dvec_uttr.repeat(centroids_spk.size(0), 1)\n            cos_sim = nn.CosineSimilarity(dim=1, eps=1e-6)(dvec_uttr, centroids_spk)\n            cos_sim = torch.clamp(cos_sim, 1e-7, 1 - 1e-7)\n            cos_sim_spk.append(cos_sim)\n        cos_sim_spk = torch.stack(cos_sim_spk).T\n        cos_sim_matrix.append(cos_sim_spk)\n    return torch.cat(cos_sim_matrix, dim=1)\n\n", "CodeBleu_score": 36.872940032898185}, {"code": "def calc_cosine_sim(self, dvecs, centroids)\n\t\n    sim_mats = []\n    for spk_idx, dvecs_spk in enumerate(dvecs):\n        centroids_spk = torch.cat(\n            [centroids[spk_idx], centroids[spk_idx]], dim=0)\n        dvecs_spk = torch.from_numpy(dvecs_spk).float().to(self.device)\n        centroids_spk = torch.from_numpy(\n            centroids_spk).float().to(self.device)\n        cos_sim_spk = torch.zeros(\n            (dvecs_spk.size(0), centroids_spk.size(0)), device=self.device)\n        for utt_idx, dvec_utt in enumerate(dvecs_spk):\n            centroids_spk_exc_utt = torch.cat(\n                [centroids_spk[:utt_idx], centroids_spk[utt_idx + 1:]], dim=0)\n            cos_sim_utt = nn.CosineSimilarity(dim=1, eps=1e-6)(\n                dvec_utt.unsqueeze(0), centroids_spk_exc_utt)\n            cos_sim_spk[utt_idx] = cos_sim_utt\n        sim_mats.append(cos_sim_spk)\n    sim_mats = torch.cat(sim_mats, dim=0)\n    sim_mats = torch.clamp(sim_mats, min=1e-7)\n    return sim_mats\n\n", "CodeBleu_score": 34.611636981045145}]}
{"instance_id": "c525b411a1e7", "generation_result": [{"code": " def embed_loss_softmax(self, dvecs, cos_sim_matrix)\n\t\n    N = dvecs.size(0)\n    loss = torch.empty(1).to(self.device)\n    for j in range(N):\n        dvec_j = dvecs[j].repeat((N, 1))\n        input_matrix = torch.cat((dvec_j, dvecs), 1)\n        input_matrix = self.emb_net(input_matrix)\n        input_matrix = self.emb_net_bn(input_matrix)\n        output_matrix = self.emb_net_out(input_matrix)\n        output_matrix = output_matrix.squeeze()\n        label = torch.from_numpy(cos_sim_matrix[j]).float().to(self.device)\n        loss_row = self.loss(output_matrix, label)\n        loss = torch.cat((loss, loss_row))\n    return loss\n\n\n def embed_loss_contrastive(self, dvecs, cos_sim_matrix):\n    \"\"\"The function computes the embedding loss using contrastive loss.", "CodeBleu_score": 26.94161540895158}, {"code": " def embed_loss_softmax(self, dvecs, cos_sim_matrix)\n\t\n    N = dvecs.size(0)\n    # Calculate the negative log softmax of the cosine similarity matrix for each embedding\n    loss = - torch.log(torch.exp(cos_sim_matrix) / torch.sum(torch.exp(cos_sim_matrix), dim=1))\n    # Stack the losses for each row into a tensor\n    loss = torch.stack([torch.mean(loss[i*self.num_uttrs:(i+1)*self.num_uttrs, :]) for i in range(N)])\n    return loss\n\n", "CodeBleu_score": 27.688757562889883}, {"code": " def embed_loss_softmax(self, dvecs, cos_sim_matrix)\n\t\n    N = dvecs.size(0)\n    loss = torch.empty(1).to(self.device)\n    for j in range(N):\n        row = cos_sim_matrix[j]\n        tmp_row = torch.cat((row[:j], row[j+1:]))\n        # print(row.shape, tmp_row.shape)\n        # print(row, tmp_row)\n        row_loss = - F.log_softmax(tmp_row, 0)\n        loss = torch.cat((loss, row_loss))\n    return loss[1:]\n", "CodeBleu_score": 27.24624796519077}]}
{"instance_id": "25ac3945da2e", "generation_result": [{"code": "def embed_loss_contrast(self, dvecs, cos_sim_matrix)\n\t\n    loss_row = []\n    for i in range(self.num_centroids):\n        mask = torch.cat(\n            [\n                torch.ones(i, dtype=torch.bool),\n                torch.zeros(1, dtype=torch.bool),\n                torch.ones(self.num_centroids - i - 1, dtype=torch.bool),\n            ]\n        )\n        contrast_loss = torch.sigmoid(cos_sim_matrix[i][mask])\n        loss_row.append(contrast_loss)\n    return torch.stack(loss_row)\n\n", "CodeBleu_score": 32.260559477591464}, {"code": "def embed_loss_contrast(self, dvecs, cos_sim_matrix)\n\t\n    loss_rows = []\n    for i, dvec in enumerate(dvecs):\n        centroids_sigmoids = torch.cat([torch.sigmoid(cos_sim_matrix[i, c]) for c in self.centroids if c != i])\n        contrast_losses = -torch.log(torch.sigmoid(cos_sim_matrix[i, self.centroids[i]])) + torch.mean(centroids_sigmoids)\n        loss_rows.append(contrast_losses)\n    return torch.stack(loss_rows)\n\n", "CodeBleu_score": 31.61831204164429}, {"code": "def embed_loss_contrast(self, dvecs, cos_sim_matrix)\n\t\n    # Iterate over the embeddings\n    loss_rows = []\n    for i, dvec_i in enumerate(dvecs):\n        # Exclude the current centroid\n        cent_index = torch.arange(dvecs.shape[0])\n        cent_index = cent_index.unsqueeze(0).expand_as(cos_sim_matrix)\n        cent_index = cent_index != i\n        # Compute the contrast loss\n        contrast_loss_row = torch.sigmoid(cos_sim_matrix[cent_index, i])\n        # Append the result to the loss row\n        loss_rows.append(contrast_loss_row)\n    # Stack the losses for each row into a tensor\n    contrast_loss = torch.stack(loss_rows)\n    return contrast_loss\n\n", "CodeBleu_score": 31.882618698337527}]}
{"instance_id": "878d14ffabd2", "generation_result": [{"code": "def forward(self, x, _label=None)\n\t\n    # print(x.shape)\n    # print(x)\n    # print(_label)\n    # print(_label.shape)\n    # print(_label)\n    # print(_label.shape)\n    # print(_label)\n    # print(_label.shape)\n    # print(_label)\n    # print(_label.shape)\n    # print(_label)\n    # print(_label.shape)\n    # print(_label)\n    # print(_label.shape)\n    # print(_label)\n    # print(_label.shape)\n    # print(_label)\n    # print(_label.shape)\n    # print(_label)\n    # print(_label.shape)\n    # print(_label)\n    # print(_label.shape)\n    # print(_label)\n    # print(_label.shape)\n    # print(_label)\n    # print(_label.shape)\n    # print(_label)\n    # print(_label.shape)\n    # print(_label)\n    # print(_label.shape)\n    # print(_label)\n    # print(_label.shape)\n    # print(_label)\n    # print(_label.shape)\n    # print(_label)\n    # print(_label.shape)\n    # print(_label)\n    # print(_label.shape)\n    # print(_label)\n    # print(_label.shape)\n    # print(_label)\n    # print(_label.shape)\n    # print(_label)\n    # print(_label.shape)\n    # print(_label)\n    # print(_label.shape)\n    # print(_label)\n    # print(_label.shape)\n    # print(_label)\n    # print(_label.shape)\n    # print(_label)\n    # print(_label.shape)\n    # print(_label)\n    # print(_label.shape)\n    # print(_label)\n    # print(_label.shape)\n    # print(_label)\n    # print(_label.shape)\n    # print(_label)\n    # print(_label.shape)\n    # print(_label)\n    # print(_label.shape)\n    # print(_label)\n    # print(_label.shape)", "CodeBleu_score": 3.6585365853658542}, {"code": "def forward(self, x, _label=None)\n\t\n    # print(x.shape)\n    # x = x.view(x.shape[0], -1, x.shape[-1])\n    # print(x.shape)\n    # x = x.view(x.shape[0], -1, x.shape[-1])\n    # print(x.shape)\n    # x = x.view(x.shape[0], -1, x.shape[-1])\n    # print(x.shape)\n    # x = x.view(x.shape[0], -1, x.shape[-1])\n    # print(x.shape)\n    # x = x.view(x.shape[0], -1, x.shape[-1])\n    # print(x.shape)\n    # x = x.view(x.shape[0], -1, x.shape[-1])\n    # print(x.shape)\n    # x = x.view(x.shape[0], -1, x.shape[-1])\n    # print(x.shape)\n    # x = x.view(x.shape[0], -1, x.shape[-1])\n    # print(x.shape)\n    # x = x.view(x.shape[0], -1, x.shape[-1])\n    # print(x.shape)\n    # x = x.view(x.shape[0], -1, x.shape[-1])\n    # print(x.shape)\n    # x = x.view(x.shape[0], -1, x.shape[-1])\n    # print(x.shape)\n    # x = x.view(x.shape[0], -1, x.shape[-1])\n    # print(x.shape)\n    # x = x.view(x.shape[0], -1, x.shape[-1])\n    # print(x.shape)\n    # x = x.view(x.shape[0], -1, x.shape[-1])\n    # print(x.shape)\n    # x = x.view(x.shape[0], -1, x.shape[-1])\n    # print(x.shape)\n    # x = x.view", "CodeBleu_score": 3.6585365853658542}, {"code": "def forward(self, x, _label=None)\n\t\n    (num_speakers, num_utts_per_speaker, dvec_feats) = x.shape\n    if num_speakers < 2:\n        raise ValueError(f'Incorrect number of speakers: {num_speakers}.')\n    if num_utts_per_speaker < 2:\n        raise ValueError(f'Each speaker needs multiple utterances, but received {num_utts_per_speaker}.')\n    centroids_init = torch.mean(x, dim=1)\n    centroids_init_std = torch.std(x, dim=1)\n    centroids = self.fc(centroids_init)\n    centroids = self.bn(centroids)\n    centroids_prod = torch.matmul(centroids, centroids.t())\n    x = x.reshape(-1, dvec_feats)\n    cos_sim = torch.matmul(x, centroids.t())\n    cos_sim = cos_sim.reshape(num_speakers, num_utts_per_speaker, num_speakers)\n    sim_matrix = 1.0 - cos_sim\n    per_embedding_loss = torch.mean(sim_matrix, dim=1)\n    mask_value = float(num_utts_per_speaker) / float(num_speakers)\n    mask_100 = torch.ones_like(per_embedding_loss) * mask_value\n    mask_100 = mask_100.to(dtype=sim_matrix.dtype, device=sim_matrix.device)\n    per_embedding_loss = per_embedding_loss - mask_100\n    centroids_norm = torch.norm(centroids, 2, dim=1)\n    similarity_measure = centroids_norm * centroids_prod\n    cosine_loss = torch.mean(per_embedding_loss + self.var_scale * similarity_measure)\n    return cosine_loss\n\n", "CodeBleu_score": 32.04891240985707}]}
{"instance_id": "86a364c72caa", "generation_result": [{"code": "def get_voices(extra_voice_dirs: List[str] = [])\n\t\n    voices: Dict[str, List[str]] = {}\n    for extra_voice_dir in extra_voice_dirs:\n        for root, dirs, files in os.walk(extra_voice_dir):\n            for file in files:\n                if file.endswith(\".npz\"):\n                    if not root in voices:\n                        voices[root] = []\n                    voices[root].append(os.path.join(root, file))\n            if root in voices and len(voices[root]) == 0:\n                for file in files:\n                    if file.endswith(\".wav\") or file.endswith(\".mp3\"):\n                        voices[root].append(os.path.join(root, file))\n    return voices\n\n", "CodeBleu_score": 34.87868625538617}, {"code": "def get_voices(extra_voice_dirs: List[str] = [])\n\t\n    voices = {}\n    for extra_voice_dir in extra_voice_dirs:\n        if not os.path.exists(extra_voice_dir):\n            continue\n        for root, dirs, files in os.walk(extra_voice_dir):\n            for dir in dirs:\n                voices[dir] = []\n                for file in files:\n                    if file.endswith(\".npz\") or (not os.path.exists(os.path.join(root, dir, \"hubert.npz\")) and (file.endswith(\".wav\") or file.endswith(\".mp3\"))):\n                        voices[dir].append(os.path.join(root, dir, file))\n    return voices\n\n", "CodeBleu_score": 28.325469927573295}, {"code": "def get_voices(extra_voice_dirs: List[str] = [])\n\t\n    voices = {}\n    for extra_voice_dir in extra_voice_dirs:\n        for root, dirs, files in os.walk(extra_voice_dir):\n            for dir in dirs:\n                voices[dir] = []\n                for file in files:\n                    if file.endswith(\".npz\"):\n                        voices[dir].append(os.path.join(root, dir, file))\n                if not voices[dir]:\n                    for file in files:\n                        if file.endswith(\".wav\") or file.endswith(\".mp3\"):\n                            voices[dir].append(os.path.join(root, dir, file))\n    return voices\n\n", "CodeBleu_score": 27.733207001860087}]}
{"instance_id": "02f20c8917e6", "generation_result": [{"code": "def load_voice(model, voice: str, extra_voice_dirs: List[str] = [])\n\t\n    if voice == \"random\":\n        return None, None, None, None\n    voice_paths = get_voices(voice, extra_voice_dirs)\n    if len(voice_paths) > 1:\n        raise ValueError(\n            f\"Multiple voices found for {voice}: {voice_paths}. Please specify the full path.\"\n        )\n    voice_path = voice_paths[0]\n    if not os.path.exists(voice_path):\n        raise ValueError(f\"Voice {voice} not found in {extra_voice_dirs}\")\n    if voice_path.endswith(\".npz\"):\n        voice_data = np.load(voice_path, allow_pickle=True)\n        return voice_data[\"embeds\"], voice_data[\"cluster_map\"], voice_data[\"cluster_prob\"], voice_data[\"sample_rate\"]\n    if not os.path.exists(voice_path.replace(\".wav\", \".npz\")):\n        logging.info(f\"Generating voice npz for {voice_path}\")\n        generate_voice_npz(model, voice_path)\n    return load_voice(model, voice_path.replace(\".wav\", \".npz\"), extra_voice_dirs)\n\n", "CodeBleu_score": 47.99837104148492}, {"code": "def load_voice(model, voice: str, extra_voice_dirs: List[str] = [])\n\t\n    if voice == \"random\":\n        return None, None, None, None\n    voice_paths = get_voices(voice, extra_voice_dirs)\n    if len(voice_paths) > 1:\n        raise ValueError(\n            f\"Multiple voices found for {voice}: {voice_paths}. Please specify the full path.\"\n        )\n    voice_path = voice_paths[0]\n    if not os.path.exists(voice_path):\n        raise FileNotFoundError(f\"Voice {voice} not found at {voice_path}.\")\n    if voice_path.endswith(\".npz\"):\n        voice_data = np.load(voice_path)\n        return voice_data[\"embeds\"], voice_data[\"cond_vector\"], voice_data[\"sr\"], voice_data[\"hop_length\"]\n    if voice_path.endswith(\".wav\"):\n        wav, sr = librosa.load(voice_path)\n        wav = torch.from_numpy(wav).unsqueeze(0)\n        wav = convert_audio(wav, sr, model.ap.sample_rate, model.ap.resample)\n        wav = wav.unsqueeze(0)\n        wav = wav.to(torch.float32)\n        wav = wav.to(model.device)\n        wav = model.ap.melspectrogram(wav)\n        wav = wav.squeeze(0).cpu().numpy()\n        wav = torch.from_numpy(wav).unsqueeze(0)\n        wav = wav.to(torch.float32)\n        wav = wav.to(model.device)\n        wav = model.ap.normalize(wav)\n        wav = wav.squeeze(0).cpu().numpy()\n        wav = torch.from_numpy(wav).unsqueeze(0)\n        wav = wav.to(torch.float32)\n        wav = wav.to(model.device)\n        wav", "CodeBleu_score": 40.36183807247742}, {"code": "def load_voice(model, voice: str, extra_voice_dirs: List[str] = [])\n\t\n    if voice == \"random\":\n        return None, None, None, None\n    voice_paths = get_voices(voice, extra_voice_dirs)\n    if len(voice_paths) > 1:\n        raise ValueError(f\"Multiple voices found for {voice}: {voice_paths}\")\n    voice_path = voice_paths[0]\n    if not os.path.exists(voice_path):\n        raise ValueError(f\"Voice {voice} not found at {voice_path}\")\n    if voice_path.endswith(\".npz\"):\n        voice_data = np.load(voice_path)\n        return voice_data[\"embeds\"], voice_data[\"tokens\"], voice_data[\"sr\"], voice_data[\"mean_embeds\"]\n    if voice_path.endswith(\".wav\"):\n        voice_data = load_voice(model, \"random\")\n        voice_embeds = voice_data[0]\n        voice_tokens = voice_data[1]\n        voice_sr = voice_data[2]\n        voice_mean_embeds = voice_data[3]\n        voice_embeds, voice_tokens, voice_mean_embeds = get_voice_embeds(model, voice_path, voice_sr, voice_embeds, voice_tokens, voice_mean_embeds)\n        np.savez(voice_path[:-4] + \".npz\", embeds=voice_embeds, tokens=voice_tokens, sr=voice_sr, mean_embeds=voice_mean_embeds)\n        return voice_embeds, voice_tokens, voice_sr, voice_mean_embeds\n    raise ValueError(f\"Unsupported voice file format: {voice_path}\")\n\n", "CodeBleu_score": 43.85664931962355}]}
{"instance_id": "9458e981389f", "generation_result": [{"code": "def _relative_shift(self, pos_score: torch.Tensor) -> torch.Tensor\n\t\n    batch_size, num_heads, seq_length1, seq_length2 = pos_score.size()\n    zeros = pos_score.new_zeros(batch_size, num_heads, seq_length1, 1)\n    padded_pos_score = torch.cat([zeros, pos_score], dim=-1)\n    padded_pos_score = padded_pos_score.view(batch_size, num_heads, seq_length2 + 1, seq_length1)\n    pos_score = padded_pos_score[:, :, 1:].view_as(pos_score)\n    return pos_score\n\n", "CodeBleu_score": 57.08735369887231}, {"code": "def _relative_shift(self, pos_score: torch.Tensor) -> torch.Tensor\n\t\n    batch_size, channels, t_x, t_y = pos_score.size()\n    # b x c x t_x x t_y -> b x c x t_x*t_y\n    pos_score = pos_score.view(batch_size, channels, t_x * t_y)\n    # b x c x t_x*t_y -> b x c x t_x*t_y+1\n    pos_score = torch.cat([self.zero, pos_score], dim=-1)\n    # b x c x t_x*t_y+1 -> b x c x t_x+1 x t_y+1\n    pos_score = pos_score.view(batch_size, channels, t_x + 1, t_y + 1)\n    # b x c x t_x+1 x t_y+1 -> b x c x t_x x t_y+1\n    pos_score = pos_score[:, :, :t_x, :]\n    # b x c x t_x x t_y+1 -> b x c x t_x x t_y\n    pos_score = pos_score[:, :, :, :t_y]\n    return pos_score\n\n", "CodeBleu_score": 28.54686330445292}, {"code": "def _relative_shift(self, pos_score: torch.Tensor) -> torch.Tensor\n\t\n    batch_size, n_head, seq_len1, seq_len2 = pos_score.size()\n    zeros = pos_score.new_zeros(batch_size, n_head, seq_len1, 1)\n    padded_pos_score = torch.cat([zeros, pos_score], dim=-1)\n    padded_pos_score = padded_pos_score.view(batch_size, n_head, seq_len2 + 1, seq_len1)\n    pos_score = padded_pos_score[:, :, 1:].view_as(pos_score)\n    return pos_score\n\n", "CodeBleu_score": 46.37981164395405}]}
{"instance_id": "c046d4f00e80", "generation_result": [{"code": "def run_padded_sequence(self, context, lens)\n\t\n    batch_size, seq_len, _ = context.size()\n    total_len = context.size(1)\n    context = context.transpose(1, 2)\n    lens = torch.clamp(lens, max=total_len)\n    output = []\n    for i in range(batch_size):\n        lin = lens[i]\n        tmp = self.layers(context[i, :, :lin])\n        tmp = tmp.transpose(1, 2)\n        output.append(tmp)\n    output = torch.cat(output, dim=0)\n    output = torch.nn.functional.pad(output, (0, 0, 0, batch_size - output.size(0)))\n    return output\n\n", "CodeBleu_score": 22.495967097378337}, {"code": "def run_padded_sequence(self, context, lens)\n\t\n    batch_size = context.size(1)\n    max_len = max(lens)\n    output = []\n    for i in range(batch_size):\n        curr_context = context[:, i, : lens[i]]\n        curr_context = self.conv_layers(curr_context)\n        output.append(curr_context)\n    output = torch.stack(output)\n    output = torch.nn.functional.pad(output, (0, 0, 0, max_len - output.size(2)))\n    return output\n\n", "CodeBleu_score": 28.697186872073942}, {"code": "def run_padded_sequence(self, context, lens)\n\t\n    # context: [B, C, T]\n    # lens: [B]\n    max_len = context.size(-1)\n    # [B, C, T]\n    padded_context = torch.zeros(\n        (context.size(0), context.size(1), max_len), device=context.device\n    )\n    for i, l in enumerate(lens):\n        padded_context[i, :, :l] = context[i, :, :l]\n    # [B, C, T]\n    padded_context = self.layers(padded_context)\n    # [B, C, T]\n    padded_context = padded_context.permute(2, 0, 1)\n    # [T, B, C]\n    return padded_context\n\n", "CodeBleu_score": 22.2580854935362}]}
{"instance_id": "db2644f3c681", "generation_result": [{"code": "def run_unsorted_inputs(self, fn, context, lens)\n\t\n    lens, indices = torch.sort(lens, descending=True)\n    # TODO: use indices to sort the context\n    context = torch.nn.utils.rnn.pack_padded_sequence(context, lens, batch_first=True)\n    context = fn(context)\n    context, _ = torch.nn.utils.rnn.pad_packed_sequence(context, batch_first=True)\n    context = context[indices]\n    return context\n\n", "CodeBleu_score": 13.944259812211806}, {"code": "def run_unsorted_inputs(self, fn, context, lens)\n\t\n    # sort the lengths and indices\n    sorted_lens, sorted_indices = torch.sort(lens, descending=True)\n    # pack the sequences\n    packed_context = torch.nn.utils.rnn.pack_padded_sequence(context, sorted_lens.cpu(), batch_first=True, enforce_sorted=False)\n    # apply the function\n    packed_context, _ = fn(packed_context)\n    # unpack the sequences\n    context, _ = torch.nn.utils.rnn.pad_packed_sequence(packed_context, batch_first=True)\n    # map the results back to the original indices\n    context = context[sorted_indices]\n    return context\n\n", "CodeBleu_score": 16.907544986408123}, {"code": "def run_unsorted_inputs(self, fn, context, lens)\n\t\n    sorted_lens, sorted_indices = torch.sort(lens, descending=True)\n    sorted_context = context.index_select(0, sorted_indices)\n    packed_context = torch.nn.utils.rnn.pack_padded_sequence(\n        sorted_context, sorted_lens.cpu(), batch_first=True\n    )\n    outputs, _ = fn(packed_context, None)\n    outputs, _ = torch.nn.utils.rnn.pad_packed_sequence(outputs, batch_first=True)\n    unsorted_indices = sorted_indices.index_select(0, sorted_indices)\n    return outputs.index_select(0, unsorted_indices)\n\n", "CodeBleu_score": 18.091650358872442}]}
{"instance_id": "ced082bffc63", "generation_result": [{"code": "def remove_weight_norm(self)\n\t\n    if self.kernel_predictor is not None:\n        self.kernel_predictor.remove_weight_norm()\n    parametrize.remove_weight_norm(self.convt_pre, \"weight\", \"weight_g\")\n    for l in self.conv_blocks:\n        parametrize.remove_weight_norm(l, \"weight\", \"weight_g\")\n\n", "CodeBleu_score": 54.430270430268635}, {"code": "def remove_weight_norm(self)\n\t\n    if hasattr(self, \"kernel_predictor\"):\n        parametrize.remove_parametrizations(self, \"kernel_predictor\", recurse=True)\n        self.kernel_predictor.remove_weight_norm()\n    if hasattr(self, \"convt_pre\"):\n        parametrize.remove_parametrizations(self, \"convt_pre\", recurse=True)\n    for l in self.conv_blocks:\n        parametrize.remove_parametrizations(l, \"conv\", recurse=True)\n\n", "CodeBleu_score": 45.11695132858058}, {"code": "def remove_weight_norm(self)\n\t\n    self.kernel_predictor.remove_weight_norm()\n    parametrize.remove_parametrizations(self.convt_pre, \"weight\", leave_parametrized=True)\n    for l in self.conv_blocks:\n        parametrize.remove_parametrizations(l, \"weight\", leave_parametrized=True)\n\n", "CodeBleu_score": 38.454737177081086}]}
{"instance_id": "b5a39fd97061", "generation_result": [{"code": "def map_mx_to_torch_model(mx_name)\n\t\n    mx_name = mx_name.lower()\n    mx_name = mx_name.replace('se_', 'se')\n    mx_name = mx_name.replace('senet_', 'senet')\n    mx_name = mx_name.replace('inceptionv3', 'inception_v3')\n    torch_name = 'gluon_' + mx_name\n    return torch_name\n\n", "CodeBleu_score": 39.3595838429556}, {"code": "def map_mx_to_torch_model(mx_name)\n\t\n    name = mx_name.lower()\n    if 'se_' in name:\n        name = name.replace('se_', 'se')\n    if 'senet_' in name:\n        name = name.replace('senet_', 'senet')\n    if 'inceptionv3' in name:\n        name = 'inception_v3'\n    name = 'gluon_' + name\n    return name\n\n", "CodeBleu_score": 40.15824665134494}, {"code": "def map_mx_to_torch_model(mx_name)\n\t\n    name = mx_name.lower()\n    name = name.replace('se_', 'se')\n    name = name.replace('senet_', 'senet')\n    name = name.replace('inceptionv3', 'inception_v3')\n    return 'gluon_' + name\n\n", "CodeBleu_score": 30.046365676212012}]}
{"instance_id": "7eee5efdbf67", "generation_result": [{"code": "def _num_samples_per_worker(self)\n\t\n    num_samples = len(self.dataset)\n    num_samples = math.ceil(\n        num_samples / self.num_replicas\n    )  # total samples to be processed per replica\n    num_samples = math.ceil(\n        num_samples / (self.num_workers * self.num_replicas)\n    )  # total samples to be processed per worker\n    if self.train:\n        num_samples = int(\n            math.ceil(num_samples / self.num_replicas)\n        )  # total samples to be processed per worker\n    else:\n        num_samples = int(\n            math.ceil(num_samples / self.num_replicas)\n        )  # total samples to be processed per worker\n    if self.drop_last and self.train:\n        num_samples -= 1\n    if self.batch_size is not None:\n        num_samples = ((num_samples - 1) // self.batch_size + 1) * self.batch_size\n    return num_samples\n\n", "CodeBleu_score": 36.61566706622575}, {"code": "def _num_samples_per_worker(self)\n\t\n    num_samples = len(self.dataset)\n    num_samples = math.ceil(num_samples / self.num_replicas)\n    total_size = self.num_samples * self.num_replicas\n    num_samples = ((num_samples - 1) // self.batch_size + 1) * self.batch_size\n    if self.sampler is not None:\n        if isinstance(self.sampler, DistributedSampler):\n            num_samples = num_samples // self.num_replicas\n        else:\n            num_samples = len(self.sampler)\n    else:\n        if self.shuffle:\n            num_samples = ((num_samples - 1) // self.num_replicas + 1) * self.num_replicas\n        else:\n            num_samples = num_samples // self.num_replicas\n    if self.total_size is not None:\n        num_samples = min(self.total_size, num_samples)\n    return num_samples\n\n", "CodeBleu_score": 33.836752134340074}, {"code": "def _num_samples_per_worker(self)\n\t\n    num_samples = self.total_samples\n    if self.repeats is not None:\n        num_samples *= self.repeats\n    if self.num_workers is not None:\n        num_samples = math.ceil(num_samples / self.num_workers)\n    if self.batch_size is not None:\n        num_samples = ((num_samples - 1) // self.batch_size + 1) * self.batch_size\n    return int(num_samples)\n\n", "CodeBleu_score": 38.01484433559658}]}
{"instance_id": "a698d65ae190", "generation_result": [{"code": "def find_images_and_targets(\n        folder: str,\n        types: Optional[Union[List, Tuple, Set]] = None,\n        class_to_idx: Optional[Dict] = None,\n        leaf_name_only: bool = True,\n        sort: bool = True\n)\n\t\n    labels = []\n    filenames = []\n    for root, subdirs, files in os.walk(folder, topdown=False, followlinks=True):\n        rel_path = os.path.relpath(root, folder) if (root != folder) else ''\n        label = os.path.basename(rel_path) if leaf_name_only else rel_path.replace(os.path.sep, '_')\n        for f in files:\n            base, ext = os.path.splitext(f)\n            if ext.lower() in types:\n                filenames.append(os.path.join(root, f))\n                labels.append(label)\n    if not class_to_idx:\n        # building class index\n        unique_labels = set(labels)\n        sorted_labels = list(sorted(unique_labels, key=natural_key))\n        class_to_idx = {c: idx for idx, c in enumerate(sorted_labels)}\n    images_and_targets = [(f, class_to_idx[l]) for f, l in zip(filenames, labels) if l in class_to_idx]\n    if sort:\n        images_and_targets = sorted(images_and_targets, key=lambda k: natural_key(k[0]))\n    if class_to_idx is None:\n        class_to_idx = dict((c, i) for i, c in enumerate(sorted_labels))\n    return images_and_targets, class_to_idx", "CodeBleu_score": 78.76294490659924}, {"code": "def find_images_and_targets(\n        folder: str,\n        types: Optional[Union[List, Tuple, Set]] = None,\n        class_to_idx: Optional[Dict] = None,\n        leaf_name_only: bool = True,\n        sort: bool = True\n)\n\t\n\n    labels = []\n    filenames = []\n\n    relpath = os.path.relpath(folder, folder) if (folder is not None) and leaf_name_only else ''\n\n    for root, subdirs, files in os.walk(folder, topdown=False, followlinks=True):\n        for fname in files:\n            base, ext = os.path.splitext(fname)\n            if ext.lower() in types:\n                fname_full = os.path.join(root, fname)\n                if class_to_idx is None:\n                    label = os.path.basename(root)\n                else:\n                    label = class_to_idx[os.path.basename(root)]\n\n                if label not in labels:\n                    labels.append(label)\n\n                filenames.append((relpath + '/' + fname_full) if leaf_name_only else fname_full)\n\n    if class_to_idx is None:\n        # building class index\n        label_to_idx = {target: i for i, target in enumerate(labels)}\n    else:\n        label_to_idx = class_to_idx\n\n    image_index = [(f, label_to_idx[l]) for f, l in zip(filenames, labels) if l in label_to_idx]\n    empty_classes = set(label_to_idx.keys()) - set(labels)\n\n    if sort:\n        image_index = sorted(image_index, key=lambda k: natural_key(k[0]))\n\n    if empty_classes:\n        print(\"**WARNING** dataset has empty class:\", empty_classes)\n\n    return image_index, label_to_idx\n\n", "CodeBleu_score": 51.86899770137232}, {"code": "def find_images_and_targets(\n        folder: str,\n        types: Optional[Union[List, Tuple, Set]] = None,\n        class_to_idx: Optional[Dict] = None,\n        leaf_name_only: bool = True,\n        sort: bool = True\n)\n\t\n    labels = list()\n    filenames = list()\n    for root, subdirs, files in os.walk(folder, topdown=False):\n        rel_path = os.path.relpath(root, folder) if (root != folder) else ''\n        label = os.path.basename(rel_path) if leaf_name_only else rel_path.replace(os.path.sep, '_')\n        if not class_to_idx:\n            class_to_idx = {label: idx for idx, label in enumerate(sorted(set(labels)))}\n        for f in files:\n            base, ext = os.path.splitext(f)\n            if ext.lower() in types:\n                filenames.append(os.path.join(root, f))\n                labels.append(class_to_idx[label])\n    if sort:\n        labels, filenames = (list(t) for t in zip(*sorted(zip(labels, filenames))))\n    return filenames, labels, class_to_idx\n\n", "CodeBleu_score": 49.92095677551106}]}
{"instance_id": "e7fe43bdecdc", "generation_result": [{"code": "def pytorch_worker_seed()\n\t\n    worker_info = get_worker_info()\n    if worker_info is not None:\n        return worker_info.seed % (2 ** 32 - 1)\n    else:\n        return int(os.environ.get(\"WDS_RANK\", \"0\")) * 1000\n\n", "CodeBleu_score": 45.524305301000474}, {"code": "def pytorch_worker_seed()\n\t\n    worker_info = get_worker_info()\n    if worker_info is not None:\n        return worker_info.seed % (2 ** 32 - 1)\n    else:\n        return os.environ.get(\"WDS_RANK\", 0) * 1000000000 + random.randint(0, 1000000000)\n\n", "CodeBleu_score": 41.97931093782245}, {"code": "def pytorch_worker_seed()\n\t\n    worker_info = get_worker_info()\n    if worker_info is not None:\n        return worker_info.seed % (2**32 - 1)\n    else:\n        return (\n            int(os.environ[\"WDS_RANK\"]) * 100\n            + int(os.environ.get(\"WDS_EPOCH\", \"0\"))\n            + 10\n        ) % (2**32 - 1)\n\n", "CodeBleu_score": 40.2769521999015}]}
{"instance_id": "0ffc7e7ded25", "generation_result": [{"code": "def solarize_add(img, add, thresh=128, **__)\n\t\n    if img.mode in (\"L\", \"RGB\"):\n        above_thr = img.point(lambda x: x > thresh and 255)\n        below_thr = img.point(lambda x: x > thresh and 0)\n        difference = ImageChops.subtract(img, above_thr)\n        addition = ImageChops.add(difference, add)\n        return Image.merge(img.mode, [b if a == 0 else a for a, b in zip(below_thr.split(), addition.split())])\n    return img\n\n", "CodeBleu_score": 28.295424476350128}, {"code": "def solarize_add(img, add, thresh=128, **__)\n\t\n    if img.mode not in [\"L\", \"RGB\"]:\n        return img\n    lut = [(i if i < thresh else min(255, i + add)) for i in range(256)]\n    return img.point(lut)\n\n", "CodeBleu_score": 21.099960519615195}, {"code": "def solarize_add(img, add, thresh=128, **__)\n\t\n    if img.mode == \"L\":\n        lut = []\n        for i in range(256):\n            if i < thresh:\n                lut.append(min(255, i + add))\n            else:\n                lut.append(i)\n        img = img.point(lut)\n        return img\n    elif img.mode == \"RGB\":\n        lut = []\n        for i in range(256):\n            if i < thresh:\n                lut.append(min(255, i + add))\n            else:\n                lut.append(i)\n        lut = lut + lut + lut\n        img = img.point(lut)\n        return img\n    else:\n        return img\n\n", "CodeBleu_score": 62.883120110310074}]}
{"instance_id": "5a0d29577a21", "generation_result": [{"code": "def _get_weighted_transforms(transforms: Dict)\n\t\n    transform_names, probs = zip(*transforms.items())\n    probs = [p / sum(probs) for p in probs]\n    return transform_names, probs\n\n", "CodeBleu_score": 30.47674211644673}, {"code": "def _get_weighted_transforms(transforms: Dict)\n\t\n    transform_names, probabilities = zip(*transforms.items())\n    total_probability = sum(probabilities)\n    probabilities = [probability / total_probability for probability in probabilities]\n    return transform_names, probabilities\n\n", "CodeBleu_score": 24.826511849977194}, {"code": "def _get_weighted_transforms(transforms: Dict)\n\t\n    transform_names, probs = zip(*transforms.items())\n    total_prob = sum(probs)\n    probs = [prob / total_prob for prob in probs]\n    return transform_names, probs\n\n", "CodeBleu_score": 31.097274660755243}]}
{"instance_id": "84bc5658c135", "generation_result": [{"code": "def _apply_basic(self, img, mixing_weights, m)\n\t\n    mixed = np.zeros_like(img)\n    for weight in mixing_weights:\n        mixed += weight * self.apply_sequence(img, m)\n    mixed = np.clip(mixed, 0, 255).astype(np.uint8)\n    mixed = Image.fromarray(mixed)\n    return Image.blend(img, mixed, self.alpha)\n\n", "CodeBleu_score": 20.728520379843488}, {"code": "def _apply_basic(self, img, mixing_weights, m)\n\t\n    mixed = np.zeros_like(img)\n    for weight in mixing_weights:\n        mixed += weight * self.apply_sequence(img)\n    mixed = np.clip(mixed, 0, 255).astype(np.uint8)\n    return Image.fromarray(cv2.addWeighted(img, 1 - m, mixed, m, 0))\n\n", "CodeBleu_score": 17.895627625421202}, {"code": "def _apply_basic(self, img, mixing_weights, m)\n\t\n    mixed = np.zeros_like(img)\n    for weight in mixing_weights:\n        mixed += weight * self.apply_sequence(img)\n    mixed = np.clip(mixed, 0, 255).astype(np.uint8)\n    return Image.fromarray(mixed)\n\n", "CodeBleu_score": 16.240623504549166}]}
{"instance_id": "84b5b752ff04", "generation_result": [{"code": "def adapt_to_chs(x, n)\n\t\n    if not isinstance(x, (tuple, list)):\n        x = tuple(repeat(x, n))\n    elif len(x) != n:\n        logging.warning(\n            f\"The number of channels {len(x)} does not match the required number {n}. Repeating the mean of {x}.\"\n        )\n        x = tuple(repeat(sum(x) / len(x), n))\n    else:\n        assert len(x) == n\n    return x\n\n", "CodeBleu_score": 53.55658121306225}, {"code": "def adapt_to_chs(x, n)\n\t\n    if not isinstance(x, (tuple, list)):\n        x = tuple(repeat(x, n))\n    elif len(x) != n:\n        logging.warning(\n            f\"The length of x ({len(x)}) does not match the required number of channels n ({n}). Repeating the mean of x to match n.\"\n        )\n        x = tuple(repeat(sum(x) / len(x), n))\n    else:\n        assert len(x) == n\n    return x\n\n", "CodeBleu_score": 53.23661152227642}, {"code": "def adapt_to_chs(x, n)\n\t\n    if not isinstance(x, (tuple, list)):\n        x = tuple(repeat(x, n))\n    elif len(x) != n:\n        logging.warning(\n            f\"The number of channels in the input tuple/list ({len(x)}) does not match the required number of channels ({n}). Using the mean of the input tuple/list.\"\n        )\n        x = tuple(repeat(sum(x) / len(x), n))\n    else:\n        assert len(x) == n\n    return x\n\n", "CodeBleu_score": 52.09128347392491}]}
{"instance_id": "54754b6ba394", "generation_result": [{"code": "def _worker_init(worker_id, worker_seeding='all')\n\t\n    worker_info = torch.utils.data.get_worker_info()\n    if worker_info is None:\n        return\n    if worker_id != worker_info.id:\n        return\n    if isinstance(worker_seeding, Callable):\n        seed = worker_seeding()\n        random.seed(seed)\n        torch.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n        return\n    if worker_seeding == 'all':\n        seed = worker_info.seed % (2 ** 32 - 1)\n        random.seed(seed)\n        torch.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n\n", "CodeBleu_score": 53.92080780963207}, {"code": "def _worker_init(worker_id, worker_seeding='all')\n\t\n    worker_info = torch.utils.data.get_worker_info()\n    if worker_info is None:\n        return\n    if isinstance(worker_seeding, Callable):\n        seed = worker_seeding(worker_info.seed)\n        random.seed(seed)\n        torch.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n        return\n    if worker_seeding == 'all':\n        seed = worker_info.seed % 2 ** 32\n        random.seed(seed)\n        torch.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n        return\n    raise ValueError(f'Invalid worker_seeding {worker_seeding}')\n\n", "CodeBleu_score": 49.556794062712726}, {"code": "def _worker_init(worker_id, worker_seeding='all')\n\t\n    worker_info = torch.utils.data.get_worker_info()\n    if worker_info is None:\n        return\n    if isinstance(worker_seeding, str):\n        if worker_seeding == 'all':\n            # Use worker_info.seed to generate different seeds for RNGs in all distributed workers\n            seed = worker_info.seed % (2**32 - 1)\n            random.seed(seed)\n            torch.manual_seed(seed)\n            torch.cuda.manual_seed_all(seed)\n            return\n        raise ValueError(f'Invalid worker_seeding option: {worker_seeding}')\n    # Use worker_info.seed to generate different seeds for RNGs in each distributed worker\n    worker_seed = torch.initial_seed() % (2**32 - 1)\n    if worker_seeding == 'random':\n        seed = worker_seed\n    elif worker_seeding == 'numpy':\n        import numpy as np\n        seed = np.random.randint(2**32 - 1)\n    elif isinstance(worker_seeding, Callable):\n        seed = worker_seeding(worker_seed)\n    else:\n        raise ValueError(f'Invalid worker_seeding option: {worker_seeding}')\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n", "CodeBleu_score": 48.65798162114928}]}
{"instance_id": "81dfdce940db", "generation_result": [{"code": "def _decode_and_center_crop(image_bytes, image_size, resize_method)\n\t\n    shape = tf.image.extract_jpeg_shape(image_bytes)\n    image_height = shape[0]\n    image_width = shape[1]\n    padded_center_crop_size = tf.cast(\n        ((image_size / (image_size + CROP_PADDING)) * tf.cast(tf.minimum(image_height, image_width), tf.float32)), tf.int32)\n    offset_height = ((image_height - padded_center_crop_size) + 1) // 2\n    offset_width = ((image_width - padded_center_crop_size) + 1) // 2\n    crop_window = tf.stack([offset_height, offset_width, padded_center_crop_size, padded_center_crop_size])\n    image = tf.image.decode_and_crop_jpeg(image_bytes, crop_window, channels=3)\n    image = tf.image.resize(image, [image_size, image_size], method=resize_method)\n    return image\n\n", "CodeBleu_score": 60.31450397178452}, {"code": "def _decode_and_center_crop(image_bytes, image_size, resize_method)\n\t\n    shape = tf.image.extract_jpeg_shape(image_bytes)\n    image_height = shape[0]\n    image_width = shape[1]\n    padded_center_crop_size = tf.cast(\n        ((image_size / (image_size + CROP_PADDING)) * tf.cast(tf.minimum(image_height, image_width), tf.float32)), tf.int32)\n    offset_height = ((image_height - padded_center_crop_size) + 1) // 2\n    offset_width = ((image_width - padded_center_crop_size) + 1) // 2\n    crop_window = tf.stack([offset_height, offset_width, padded_center_crop_size, padded_center_crop_size])\n    image = tf.image.decode_and_crop_jpeg(image_bytes, crop_window, channels=3)\n    image = tf.image.resize(image, [image_size, image_size], method=resize_method)\n    return image\n\n\n\n", "CodeBleu_score": 60.31450397178452}, {"code": "def _decode_and_center_crop(image_bytes, image_size, resize_method)\n\t\n    shape = tf.io.extract_jpeg_shape(image_bytes)\n    image_height = shape[0]\n    image_width = shape[1]\n    padded_center_crop_size = tf.cast(\n        ((image_size / (image_size + CROP_PADDING)) * tf.cast(tf.minimum(image_height, image_width), tf.float32)),\n        tf.int32,\n    )\n    offset_height = ((image_height - padded_center_crop_size) + 1) // 2\n    offset_width = ((image_width - padded_center_crop_size) + 1) // 2\n    crop_window = tf.stack([offset_height, offset_width, padded_center_crop_size, padded_center_crop_size])\n    image = tf.io.decode_and_crop_jpeg(image_bytes, crop_window, channels=3)\n    image = _resize_and_clip(image, image_size, resize_method)\n    return image\n\n", "CodeBleu_score": 55.56546420930795}]}
{"instance_id": "cf16a6d37111", "generation_result": [{"code": "def preprocess_for_train(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    image_bytes = tf.io.decode_raw(image_bytes, tf.uint8)\n    image_bytes = tf.reshape(image_bytes, [512, 512, 3])\n    image = tf.image.resize(image_bytes, [image_size, image_size], method=interpolation)\n    image = tf.image.random_crop(image, [IMAGE_SIZE, IMAGE_SIZE, 3])\n    image = tf.image.random_flip_left_right(image)\n    image = tf.cast(image, tf.bfloat16 if use_bfloat16 else tf.float32)\n    return image\n\n", "CodeBleu_score": 31.603721913416837}, {"code": "def preprocess_for_train(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    image = tf.image.decode_jpeg(image_bytes, channels=3)\n    image = tf.image.resize(image, image_size, method=interpolation)\n    image.set_shape((image_size[0], image_size[1], 3))\n    image = tf.image.random_flip_left_right(image)\n    image = tf.reshape(image, [image_size[0], image_size[1], 3])\n    image = tf.cast(image, tf.bfloat16 if use_bfloat16 else tf.float32)\n    return image\n\n", "CodeBleu_score": 33.21210053648751}, {"code": "def preprocess_for_train(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    image = tf.image.decode_image(image_bytes, channels=3)\n    image = tf.image.resize(image, [image_size, image_size], method=interpolation)\n    image.set_shape((image_size, image_size, 3))\n    image = tf.image.random_flip_left_right(image)\n    image = tf.cast(image, dtype=tf.bfloat16 if use_bfloat16 else tf.float32)\n    return image\n\n", "CodeBleu_score": 37.29814088585333}]}
{"instance_id": "78b2a3bd2142", "generation_result": [{"code": "def preprocess_for_eval(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    if interpolation == 'bicubic':\n        interpolation = tf.image.ResizeMethod.BICUBIC\n    elif interpolation == 'lanczos3':\n        interpolation = tf.image.ResizeMethod.LANCZOS3\n    elif interpolation == 'lanczos5':\n        interpolation = tf.image.ResizeMethod.LANCZOS5\n    elif interpolation == 'bilinear':\n        interpolation = tf.image.ResizeMethod.BILINEAR\n    elif interpolation == 'nearest':\n        interpolation = tf.image.ResizeMethod.NEAREST_NEIGHBOR\n    elif interpolation == 'area':\n        interpolation = tf.image.ResizeMethod.AREA\n    elif interpolation == 'mitchellcubic':\n        interpolation = tf.image.ResizeMethod.MITCHELLCUBIC\n    else:\n        raise ValueError('Unknown interpolation: %s' % interpolation)\n    image = preprocess_image(image_bytes=image_bytes, bbox=None, image_size=image_size, augment=False,\n                              use_bfloat16=use_bfloat16, dtype=tf.float32, interpolation=interpolation)\n    return image\n\n", "CodeBleu_score": 27.749090752795556}, {"code": "def preprocess_for_eval(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    image = tf.io.decode_image(image_bytes, channels=3, expand_animations=False)\n    if interpolation == 'bicubic':\n        image = _center_crop(image, image_size, image_size)\n    elif interpolation == 'area':\n        image = _center_crop_area(image, image_size, image_size)\n    else:\n        raise ValueError('Unknown interpolation: %s' % interpolation)\n    image = tf.image.resize(image, (image_size, image_size), method=tf.image.ResizeMethod.BICUBIC)\n    if use_bfloat16:\n        image = tf.cast(image, tf.bfloat16)\n    else:\n        image = tf.cast(image, tf.float32)\n    return image\n\n", "CodeBleu_score": 40.030376102167345}, {"code": "def preprocess_for_eval(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    image = tf.io.decode_image(image_bytes, channels=3, expand_animations=False)\n    image = _central_crop(image, image_size, image_size)\n    image = tf.image.resize(image, (image_size, image_size), method=interpolation)\n    image = tf.image.convert_image_dtype(image, dtype=tf.bfloat16 if use_bfloat16 else tf.float32)\n    return image\n\n\n", "CodeBleu_score": 40.083925666344754}]}
{"instance_id": "8bd4364a9a0e", "generation_result": [{"code": "def evaluate(self, model, eval_dataloader=None, output_file=\"merged\")\n\t\n    if eval_dataloader is None:\n        eval_dataloader = self.eval_dataloader\n\n    if output_file == \"merged\":\n        output_file = os.path.join(self.output_dir, \"predictions_merged.txt\")\n    else:\n        output_file = os.path.join(self.output_dir, output_file)\n\n    self.model = model\n    self.model.eval()\n    self.model.to(self.device)\n\n    # Evaluate\n    results = self.evaluate_loop(eval_dataloader, output_file)\n\n    return results\n\n", "CodeBleu_score": 53.1288949742857}, {"code": "def evaluate(self, model, eval_dataloader=None, output_file=\"merged\")\n\t\n    if eval_dataloader is None:\n        eval_dataloader = self.eval_dataloader\n    self.model = model\n    self.model.eval()\n    self.model.to(self.device)\n    results = []\n    for batch in eval_dataloader:\n        with torch.no_grad():\n            outputs = self.model(**batch)\n            predictions = outputs.logits.argmax(dim=-1)\n            results.append(predictions)\n    results = torch.cat(results, dim=0)\n    results = results.cpu().numpy()\n    np.save(os.path.join(self.output_dir, f\"{output_file}.npy\"), results)\n    return results\n\n", "CodeBleu_score": 48.39427423202001}, {"code": "def evaluate(self, model, eval_dataloader=None, output_file=\"merged\")\n\t\n    if eval_dataloader is None:\n        eval_dataloader = self.eval_dataloader\n    self.model.eval()\n    predictions = []\n    for batch in eval_dataloader:\n        batch = {k: v.to(self.device) for k, v in batch.items()}\n        with torch.no_grad():\n            outputs = self.model(**batch)\n        logits = outputs.logits\n        predictions.extend(logits.argmax(dim=-1).cpu().numpy())\n    predictions = np.array(predictions).flatten()\n    eval_predictions = [self.label_list[p] for p in predictions]\n    output_test_file = os.path.join(self.output_dir, f\"{output_file}.txt\")\n    output_test_file_w_ref = os.path.join(self.output_dir, f\"{output_file}_w_ref.txt\")\n    with open(output_test_file, \"w\") as writer:\n        writer.write(\"", "CodeBleu_score": 41.58251042445834}]}
{"instance_id": "22d682c958c0", "generation_result": [{"code": " def print_computed_metrics(self, metrics)\n\t\n    rank1 = metrics['rank1']\n    rank5 = metrics['rank5']\n    miou = metrics['miou']\n    print('Rank@1: {:.4f}'.format(rank1))\n    print('Rank@5: {:.4f}'.format(rank5))\n    print('mIoU: {:.4f}'.format(miou))\n\n", "CodeBleu_score": 27.694462127832907}, {"code": " def print_computed_metrics(self, metrics)\n\t\n    rank1 = metrics[\"rank1\"]\n    rank5 = metrics[\"rank5\"]\n    miou = metrics[\"miou\"]\n    print(\"Rank@1: {:.4f}\".format(rank1))\n    print(\"Rank@5: {:.4f}\".format(rank5))\n    print(\"mIoU: {:.4f}\".format(miou))", "CodeBleu_score": 37.29336163436595}, {"code": " def print_computed_metrics(self, metrics)\n\t\n    rank1 = metrics['rank1']\n    rank5 = metrics['rank5']\n    miou = metrics['miou']\n    print(\"R@1: {:.4f} - R@5: {:.4f} - mIoU: {:.4f}\".format(rank1, rank5, miou))", "CodeBleu_score": 27.85340475930947}]}
{"instance_id": "39c1a50b7e7d", "generation_result": [{"code": "def _eval_predictions(self, segments, data)\n\t\n    # Compute IoU for each segment-data pair\n    ious = []\n    for segment in segments:\n        for d in data:\n            iou = self._compute_iou(segment, d)\n            ious.append(iou)\n\n    # Compute rank for each segment-data pair\n    ranks = []\n    for segment in segments:\n        for d in data:\n            rank = self._compute_rank(segment, d)\n            ranks.append(rank)\n\n    # Compute average rank and IoU for top predictions\n    num_top_predictions = min(len(segments), self.num_top_predictions)\n    avg_rank = sum(ranks[:num_top_predictions]) / num_top_predictions\n    avg_iou = sum(ious[:num_top_predictions]) / num_top_predictions\n\n    # Compute percentage of segments with rank 1 and rank 5\n    rank1 = sum(1 for rank in ranks if rank == 1) / len(ranks)\n    rank5 = sum(1 for rank in ranks if rank <= 5) / len(ranks)\n\n    return rank1, rank5, avg_iou\n\n", "CodeBleu_score": 32.4232671576722}, {"code": "def _eval_predictions(self, segments, data)\n\t\n    ious = []\n    ranks = []\n    for segment in segments:\n        segment_data = data[segment[\"video_id\"]]\n        segment_start = segment[\"segment\"][0]\n        segment_end = segment[\"segment\"][1]\n        segment_duration = segment_end - segment_start\n        segment_start_relative = segment_start - segment_data[\"timestamps\"][0]\n        segment_end_relative = segment_end - segment_data[\"timestamps\"][0]\n        segment_start_relative_ratio = segment_start_relative / segment_duration\n        segment_end_relative_ratio = segment_end_relative / segment_duration\n        for timestamp in segment_data[\"timestamps\"]:\n            if (\n                timestamp >= segment_start_relative\n                and timestamp <= segment_end_relative\n            ):\n                iou = (segment_end_relative - timestamp) / segment_duration\n                ious.append(iou)\n                rank = (\n                    segment_end_relative_ratio - segment_start_relative_ratio\n                ) / (timestamp - segment_start_relative_ratio)\n                ranks.append(rank)\n    rank1 = 100.0 * len([x for x in ranks if x <= 1]) / len(ranks)\n    rank5 = 100.0 * len([x for x in ranks if x <= 5]) / len(ranks)\n    mean_iou = 100.0 * sum(ious) / len(ious)\n    return rank1, rank5, mean_iou\n\n", "CodeBleu_score": 26.219796851614507}, {"code": "def _eval_predictions(self, segments, data)\n\t\n\n    # Initialize variables to store the sum of ranks and IoUs for the top predictions\n    sum_ranks = 0\n    sum_ious = 0\n\n    # Initialize variables to store the number of segments with rank 1 and rank 5\n    rank1 = 0\n    rank5 = 0\n\n    # Loop through each segment and its corresponding data\n    for segment, data_dict in zip(segments, data):\n        # Get the ground truth data for the segment\n        data = data_dict[\"data\"]\n\n        # Initialize variables to store the maximum IoU and rank for the segment\n        max_iou = 0\n        max_rank = 0\n\n        # Loop through each prediction for the segment\n        for rank, pred in enumerate(segment):\n            # Calculate the IoU between the prediction and the ground truth data\n            iou = self._iou(pred, data)\n\n            # Update the maximum IoU and rank if the current prediction has a higher IoU\n            if iou > max_iou:\n                max_iou = iou\n                max_rank = rank\n\n        # Add the maximum rank and IoU to the sum of ranks and IoUs\n        sum_ranks += max_rank\n        sum_ious += max_iou\n\n        # Increment the rank1 and rank5 counters if the maximum rank is 1 or 5, respectively\n        if max_rank == 0:\n            rank1 += 1\n        if max_rank < 5:\n            rank5 += 1\n\n    # Calculate the percentage of segments with rank 1 and rank 5\n    rank1 = rank1 / len(segments)\n    rank5 = rank5 / len(segments)\n\n    # Calculate the mean IoU across all segments\n    mean_iou = sum_ious / len(segments)\n\n    # Return the rank1, rank5, and mean IoU\n    return rank1, rank5, mean_iou\n\n", "CodeBleu_score": 27.039074110586665}]}
{"instance_id": "8ccf84f8d034", "generation_result": [{"code": "def predict_loop(self, model, eval_dataloader, output_file=None)\n\t\n    results = []\n    model.eval()\n    model.to(self.device)\n    for data in tqdm(eval_dataloader, desc=\"Evaluating\"):\n        data = {k: v.to(self.device) for k, v in data.items()}\n        with torch.no_grad():\n            outputs = model(**data)\n\n        outputs = outputs.detach().cpu().tolist()\n        results.extend(outputs)\n\n    if output_file is not None:\n        with open(output_file, \"w\") as writer:\n            writer.write(json.dumps(results, indent=4) + \"", "CodeBleu_score": 40.29955811029227}, {"code": "def predict_loop(self, model, eval_dataloader, output_file=None)\n\t\n    results = []\n    model.eval()\n    model.to(self.device)\n    for step, batch in enumerate(tqdm(eval_dataloader, desc=\"Evaluating\")):\n        batch = tuple(t.to(self.device) for t in batch)\n        with torch.no_grad():\n            inputs = {\n                \"input_ids\": batch[0],\n                \"attention_mask\": batch[1],\n                \"token_type_ids\": batch[2],\n                \"labels\": batch[3],\n            }\n            outputs = model(**inputs)\n\n        for i, input_ in enumerate(inputs[\"input_ids\"]):\n            eval_feature = self.eval_features[step * self.args.eval_batch_size + i]\n            unique_id = int(eval_feature.unique_id)\n            output = [float(x) for x in outputs[1][i]]  # logits\n            output = self.softmax(output)\n            output = [float(x) for x in output]\n            results.append(\n                RawResult(unique_id=unique_id, logits=output, label=eval_feature.label)\n            )\n    if output_file is not None:\n        with open(output_file, \"w\") as writer:\n            writer.write(json.dumps(results, indent=4) + \"", "CodeBleu_score": 34.15623199117762}, {"code": "def predict_loop(self, model, eval_dataloader, output_file=None)\n\t\n    scores = []\n    model.eval()\n    model.to(\"cuda:0\")\n    for data in tqdm(eval_dataloader):\n        data = {k: v.to(\"cuda:0\") for k, v in data.items()}\n        with torch.no_grad():\n            outputs = model(**data)\n        outputs = outputs.detach().cpu().numpy()\n        scores.extend(outputs)\n    if output_file is not None:\n        with open(output_file, \"w\") as f:\n            json.dump(scores, f)\n    return scores\n\n", "CodeBleu_score": 42.87590451835241}]}
{"instance_id": "e68529ad727d", "generation_result": [{"code": "def _aggregate_scores(self, scores)\n\t\n    assert len(scores) == 2\n    scores = torch.cat(scores, dim=0)\n    self.full_scores = None\n    return torch.matmul(scores[0], scores[1].T)\n\n", "CodeBleu_score": 33.520648898450006}, {"code": "def _aggregate_scores(self, scores)\n\t\n    assert len(scores) == 2\n    scores = [torch.cat(s, dim=0) for s in scores]\n    self.full_scores = None\n    return torch.mm(scores[0], scores[1].t())\n\n", "CodeBleu_score": 34.03006170649218}, {"code": "def _aggregate_scores(self, scores)\n\t\n    assert len(scores) == 2\n    scores = torch.cat(scores, dim=0)\n    self.full_scores = None\n    return torch.matmul(scores[0], scores[1].t())\n\n", "CodeBleu_score": 33.5522664319541}]}
{"instance_id": "e4e1861fc357", "generation_result": [{"code": "def finalize(self, Y_pred, Y_true, output_file=None)\n\t\n    Y_pred = torch.cat(Y_pred, dim=0).cpu().numpy()\n    Y_true = torch.cat(Y_true, dim=0).cpu().numpy()\n    if Y_pred.shape != Y_true.shape:\n        raise ValueError('Predicted and true labels have different shapes.')\n    if Y_pred.shape[0] != len(self.data):\n        raise ValueError('Number of predictions does not match the number of samples.')\n    if Y_pred.shape[1] != self.n_tasks:\n        raise ValueError('Number of tasks does not match the number of predicted labels.')\n    if Y_true.shape[1] != self.n_tasks:\n        raise ValueError('Number of tasks does not match the number of true labels.')\n    if output_file is not None:\n        with open(output_file, 'wb') as f:\n            pickle.dump({'Y_pred': Y_pred, 'Y_true': Y_true}, f)\n    return {'Y_pred': Y_pred, 'Y_true': Y_true}\n\n", "CodeBleu_score": 34.70848340357884}, {"code": "def finalize(self, Y_pred, Y_true, output_file=None)\n\t\n    if self.prediction_errors:\n        print(f\"", "CodeBleu_score": 2.9592032078505}, {"code": "def finalize(self, Y_pred, Y_true, output_file=None)\n\t\n    Y_pred = torch.cat(Y_pred, dim=0)\n    Y_true = torch.cat(Y_true, dim=0)\n    Y_pred = Y_pred.cpu().numpy()\n    Y_true = Y_true.cpu().numpy()\n    if self.args.output_file:\n        with open(self.args.output_file, \"wb\") as f:\n            pickle.dump((Y_pred, Y_true), f)\n    Y_pred = Y_pred.tolist()\n    Y_true = Y_true.tolist()\n    for i in range(len(Y_pred)):\n        if Y_pred[i] != Y_true[i]:\n            print(f\"Prediction error: {i}\")\n            print(f\"Predicted: {Y_pred[i]}\")\n            print(f\"True: {Y_true[i]}\")\n            print(f\"Input: {self.test_data[i][0]}\")\n            print(f\"Target: {self.test_data[i][1]}\")\n            print()\n    return {\"predictions\": Y_pred, \"true_labels\": Y_true}\n\n", "CodeBleu_score": 40.60634220849034}]}
{"instance_id": "7df9ab9a4920", "generation_result": [{"code": "def reduce_metrics(logging_outputs) -> None\n\t\n    loss_sum = sum(log.get(\"loss\", 0) for log in logging_outputs)\n    sample_size = sum(log.get(\"sample_size\", 0) for log in logging_outputs)\n    metrics.log_scalar(\"loss\", loss_sum / sample_size, sample_size, round=3)\n\n", "CodeBleu_score": 62.429646714521894}, {"code": "def reduce_metrics(logging_outputs) -> None\n\t\n    loss_sum = sum(log.get('loss', 0) for log in logging_outputs)\n    sample_size = sum(log.get('sample_size', 0) for log in logging_outputs)\n    metrics.log_scalar('loss', loss_sum / sample_size, sample_size, round=3)\n\n", "CodeBleu_score": 52.7910982560264}, {"code": "def reduce_metrics(logging_outputs) -> None\n\t\n    loss_sum = sum(log.get(\"loss\", 0) for log in logging_outputs)\n    sample_size = sum(log.get(\"sample_size\", 0) for log in logging_outputs)\n    metrics.log_scalar(\n        \"loss\", loss_sum / sample_size, sample_size, round=3\n    )\n\n", "CodeBleu_score": 57.00304501759299}]}
{"instance_id": "7f9c326a96f7", "generation_result": [{"code": "def _mm_on_the_fly(\n        self,\n        cmasks,\n        vmasks,\n        attention_mask\n    )\n\t\n\n    if attention_mask is None:\n        attention_mask = torch.cat([cmasks, vmasks], dim=1)\n\n    token_type_ids = torch.cat([\n        torch.zeros_like(cmasks),\n        torch.ones_like(vmasks)\n    ], dim=1)\n\n    return attention_mask, token_type_ids\n\n", "CodeBleu_score": 26.445209272204885}, {"code": "def _mm_on_the_fly(\n        self,\n        cmasks,\n        vmasks,\n        attention_mask\n    )\n\t\n    if attention_mask is None:\n        attention_mask = torch.cat([cmasks, vmasks], dim=-1)\n    token_type_ids = torch.cat([\n        torch.zeros_like(cmasks),\n        torch.ones_like(vmasks)\n    ], dim=-1)\n    return attention_mask, token_type_ids\n\n", "CodeBleu_score": 26.483934797735564}, {"code": "def _mm_on_the_fly(\n        self,\n        cmasks,\n        vmasks,\n        attention_mask\n    )\n\t\n    if attention_mask is None:\n        attention_mask = torch.cat([cmasks, vmasks], dim=-1)\n    token_type_ids = torch.cat([\n        torch.zeros_like(cmasks),\n        torch.ones_like(vmasks)\n    ], dim=-1)\n    return attention_mask, token_type_ids\n\n", "CodeBleu_score": 26.483934797735564}]}
{"instance_id": "fa99d2a148a5", "generation_result": [{"code": "def prepare_inputs_for_generation(\n        self,\n        input_ids,\n        input_video_embeds,\n        attention_mask=None,\n        token_type_ids=None,\n        **model_kwargs\n    )\n\t\n    # get the combined length of input_ids and input_video_embeds\n    combined_length = input_ids.size(1) + input_video_embeds.size(1)\n\n    # adjust the sequence length to match the combined length\n    input_ids = input_ids[:, :combined_length]\n    input_video_embeds = input_video_embeds[:, :combined_length, :]\n\n    # modify the attention_mask and token_type_ids to fit the new sequence length\n    if attention_mask is not None:\n        attention_mask = attention_mask[:, :combined_length]\n    if token_type_ids is not None:\n        token_type_ids = token_type_ids[:, :combined_length]\n\n    # return the updated inputs as a dictionary\n    return {\n        \"input_ids\": input_ids,\n        \"input_video_embeds\": input_video_embeds,\n        \"attention_mask\": attention_mask,\n        \"token_type_ids\": token_type_ids,\n        **model_kwargs\n    }\n\n", "CodeBleu_score": 54.39842380173228}, {"code": "def prepare_inputs_for_generation(\n        self,\n        input_ids,\n        input_video_embeds,\n        attention_mask=None,\n        token_type_ids=None,\n        **model_kwargs\n    )\n\t\n    # Prepare inputs for text and video generation models\n    # Adjust sequence length to match the combined length of input_ids and input_video_embeds\n    input_ids = input_ids[:, :input_video_embeds.shape[1]]\n    if attention_mask is not None:\n        attention_mask = attention_mask[:, :input_video_embeds.shape[1]]\n    if token_type_ids is not None:\n        token_type_ids = token_type_ids[:, :input_video_embeds.shape[1]]\n\n    # Update the model_kwargs with the new inputs\n    model_kwargs[\"input_ids\"] = input_ids\n    model_kwargs[\"input_video_embeds\"] = input_video_embeds\n    model_kwargs[\"attention_mask\"] = attention_mask\n    model_kwargs[\"token_type_ids\"] = token_type_ids\n\n    return model_kwargs\n\n", "CodeBleu_score": 36.605464909263915}, {"code": "def prepare_inputs_for_generation(\n        self,\n        input_ids,\n        input_video_embeds,\n        attention_mask=None,\n        token_type_ids=None,\n        **model_kwargs\n    )\n\t\n    # get the combined length of input_ids and input_video_embeds\n    combined_length = input_ids.shape[-1] + input_video_embeds.shape[-2]\n\n    # adjust the sequence length of input_ids and input_video_embeds\n    input_ids = input_ids[:, :combined_length]\n    input_video_embeds = input_video_embeds[:, :combined_length, :]\n\n    # adjust the attention_mask and token_type_ids to fit the new sequence length\n    if attention_mask is not None:\n        attention_mask = attention_mask[:, :combined_length]\n    if token_type_ids is not None:\n        token_type_ids = token_type_ids[:, :combined_length]\n\n    # update the model_kwargs dictionary with the adjusted inputs\n    model_kwargs.update({\n        \"input_ids\": input_ids,\n        \"input_video_embeds\": input_video_embeds,\n        \"attention_mask\": attention_mask,\n        \"token_type_ids\": token_type_ids,\n    })\n\n    return model_kwargs\n", "CodeBleu_score": 51.00429555852053}]}
{"instance_id": "b1b0d25fe620", "generation_result": [{"code": "def _init_pipeline(self, cfg: ConfigType) -> Compose\n\t\n    # Adjust the pipeline config.\n    cfg = copy.deepcopy(cfg)\n    for transform in cfg.transforms:\n        if 'meta_keys' in transform.type_args:\n            if 'img_id' in transform.type_args.meta_keys:\n                transform.type_args.meta_keys.remove('img_id')\n    # Find the index of LoadImageFromFile transform.\n    for i, transform in enumerate(cfg.transforms):\n        if transform.type == 'LoadImageFromFile':\n            break\n    else:\n        raise ValueError('Cannot find the LoadImageFromFile transform')\n    # Update the transform type to InferencerLoader.\n    cfg.transforms[i].type = 'mmdet.InferencerLoader'\n    return Compose(cfg.transforms)\n\n", "CodeBleu_score": 28.345682566306007}, {"code": "def _init_pipeline(self, cfg: ConfigType) -> Compose\n\t\n    test_pipeline = copy.deepcopy(cfg.test_dataloader.dataset.pipeline)\n    for i, transform in enumerate(test_pipeline):\n        if 'meta_keys' in transform and 'img_id' in transform['meta_keys']:\n            test_pipeline[i]['meta_keys'].remove('img_id')\n    for i, transform in enumerate(test_pipeline):\n        if transform['type'] == 'LoadImageFromFile':\n            load_image_from_file_index = i\n            break\n    else:\n        raise ValueError('LoadImageFromFile is not found in '\n                         'the test pipeline.')\n    test_pipeline[load_image_from_file_index]['type'] = 'mmdet.InferencerLoader'\n    return Compose(test_pipeline)\n\n", "CodeBleu_score": 34.305079003198266}, {"code": "def _init_pipeline(self, cfg: ConfigType) -> Compose\n\t\n    # remove img_id from meta_keys\n    if 'meta_keys' in cfg.data.test.pipeline[0]:\n        cfg.data.test.pipeline[0].meta_keys = [\n            key for key in cfg.data.test.pipeline[0].meta_keys\n            if key != 'img_id'\n        ]\n    # find the index of LoadImageFromFile\n    for i, transform in enumerate(cfg.data.test.pipeline):\n        if transform.type == 'LoadImageFromFile':\n            load_image_from_file_index = i\n            break\n    else:\n        raise ValueError('Cannot find `LoadImageFromFile` in '\n                         '`cfg.data.test.pipeline`')\n    # update transform type\n    cfg.data.test.pipeline[load_image_from_file_index].type = \\\n        'mmdet.InferencerLoader'\n    return Compose(cfg.data.test.pipeline)\n\n", "CodeBleu_score": 32.015810142638415}]}
{"instance_id": "7967962856eb", "generation_result": [{"code": "def _inputs_to_list(self, inputs: InputsType) -> list\n\t\n    if isinstance(inputs, str):\n        if isdir(inputs):\n            inputs = list_dir_or_file(inputs, list_dir=False, list_file=True)\n            inputs = [join_path(inputs, img) for img in inputs]\n        else:\n            inputs = [inputs]\n    elif not isinstance(inputs, (list, tuple)):\n        inputs = [inputs]\n    return inputs\n\n", "CodeBleu_score": 49.28440431372753}, {"code": "def _inputs_to_list(self, inputs: InputsType) -> list\n\t\n    if isinstance(inputs, str) and isdir(inputs):\n        inputs = list_dir_or_file(inputs, list_dir=False, list_file=True, recursive=True)\n        inputs = [join_path(inputs, img) for img in inputs]\n    elif not isinstance(inputs, (list, tuple)):\n        inputs = [inputs]\n    return inputs\n\n", "CodeBleu_score": 49.044926573360485}, {"code": "def _inputs_to_list(self, inputs: InputsType) -> list\n\t\n    if isinstance(inputs, str) and isdir(inputs):\n        inputs = list_dir_or_file(inputs, list_dir=False, list_file=True, recursive=True)\n    if not isinstance(inputs, (list, tuple)):\n        inputs = [inputs]\n    return inputs\n\n", "CodeBleu_score": 43.55888598716806}]}
{"instance_id": "74f727f06fa0", "generation_result": [{"code": "def _get_chunk_data(self, inputs: Iterable, chunk_size: int)\n\t\n    for i in range(0, len(inputs), chunk_size):\n        chunk_data = inputs[i:i + chunk_size]\n        for input in chunk_data:\n            if isinstance(input, dict) and 'img' in input:\n                yield self.pipeline(input)\n            elif isinstance(input, dict) and 'img_path' in input:\n                yield self.pipeline(input)\n            else:\n                yield input\n\n", "CodeBleu_score": 23.23657239877271}, {"code": "def _get_chunk_data(self, inputs: Iterable, chunk_size: int)\n\t\n    chunk_data = []\n    for input in inputs:\n        if isinstance(input, dict) and ('img' in input or 'img_path' in input):\n            input = self.pipeline(input)\n        chunk_data.append(input)\n        if len(chunk_data) == chunk_size:\n            yield chunk_data\n            chunk_data = []\n    if chunk_data:\n        yield chunk_data\n\n", "CodeBleu_score": 27.7471101637419}, {"code": "def _get_chunk_data(self, inputs: Iterable, chunk_size: int)\n\t\n    for i, data in enumerate(inputs):\n        if isinstance(data, dict):\n            if 'img' in data:\n                data = data['img']\n            elif 'img_path' in data:\n                data = data['img_path']\n            else:\n                data = data['img']\n        data = self.pipeline(data)\n        if i % chunk_size == 0:\n            chunk_data = []\n        chunk_data.append(data)\n        if (i + 1) % chunk_size == 0 or (i + 1) == len(inputs):\n            yield chunk_data\n\n", "CodeBleu_score": 34.21444559281167}]}
{"instance_id": "a43ab51afa59", "generation_result": [{"code": "def postprocess(\n        self,\n        preds: PredType,\n        visualization: Optional[List[np.ndarray]] = None,\n        return_datasamples: bool = False,\n        print_result: bool = False,\n        no_save_pred: bool = False,\n        pred_out_dir: str = '',\n        **kwargs,\n    ) -> Dict\n\t\n    if not no_save_pred and pred_out_dir != '':\n        if return_datasamples:\n            warnings.warn(\n                '`pred_out_dir` is not supported when `return_datasamples` is '\n                'set to True, set `pred_out_dir` to empty string to avoid '\n                'unexpected behavior.')\n        else:\n            mmengine.mkdir_or_exist(pred_out_dir)\n\n    result = {}\n\n    if return_datasamples:\n        result['preds'] = preds\n    else:\n        preds = [pred.to_dict() for pred in preds]\n        result['preds'] = preds\n\n    if visualization is not None:\n        result['visualization'] = visualization\n\n    if print_result:\n        print_log(result, logger='current')\n\n    if not no_save_pred and pred_out_dir != '':\n        mmengine.dump(result, join_path(pred_out_dir, 'predictions.pkl'))\n\n    return result\n\n", "CodeBleu_score": 49.091490184196665}, {"code": "def postprocess(\n        self,\n        preds: PredType,\n        visualization: Optional[List[np.ndarray]] = None,\n        return_datasamples: bool = False,\n        print_result: bool = False,\n        no_save_pred: bool = False,\n        pred_out_dir: str = '',\n        **kwargs,\n    ) -> Dict\n\t\n    # TODO: support save_datasamples\n    if return_datasamples:\n        warnings.warn(\n            'save_datasamples is not supported in '\n            '`BaseDetInfer.postprocess`, please use '\n            '`BaseDetInfer.postprocess_datasample` instead.')\n    if not no_save_pred:\n        if pred_out_dir:\n            mmengine.mkdir_or_exist(pred_out_dir)\n        else:\n            pred_out_dir = self.cfg.work_dir\n    results = []\n    for pred in preds:\n        pred = self.postprocess_single_pred(pred)\n        if not no_save_pred:\n            if pred_out_dir:\n                mmengine.mkdir_or_exist(pred_out_dir)\n            else:\n                pred_out_dir = self.cfg.work_dir\n        if return_datasamples:\n            warnings.warn(\n                'save_datasamples is not supported in '\n                '`BaseDetInfer.postprocess`, please use '\n                '`BaseDetInfer.postprocess_datasample` instead.')\n        if pred_out_dir:\n            mmengine.mkdir_or_exist(pred_out_dir)\n        else:\n            pred_out_dir = self.cfg.work_dir\n        if return_datasamples:\n            warnings.warn(\n                'save_datasamples is not supported in '\n                '`BaseDetInfer.postprocess`, please use '\n                '`BaseDetInfer.postprocess_datasample` instead.')\n        if pred_out_dir:\n            mmengine.mkdir_or_exist(pred_out_dir)\n        else:\n            pred_out_dir = self.cfg.work_dir\n        if return_datasamples:\n            warnings.warn(\n                'save_datasamples is not supported in '\n                '`BaseDetInfer.postprocess`, please use '\n                '`BaseDetInfer.postprocess_datasample` instead.')\n        if pred_out_dir:\n            mmengine.mkdir_", "CodeBleu_score": 39.32063015291026}, {"code": "def postprocess(\n        self,\n        preds: PredType,\n        visualization: Optional[List[np.ndarray]] = None,\n        return_datasamples: bool = False,\n        print_result: bool = False,\n        no_save_pred: bool = False,\n        pred_out_dir: str = '',\n        **kwargs,\n    ) -> Dict\n\t\n    if not no_save_pred:\n        mmengine.mkdir_or_exist(pred_out_dir)\n    result = {}\n    if return_datasamples:\n        if pred_out_dir:\n            warnings.warn(\n                'Saving datasamples is not supported, '\n                'please set `return_datasamples=False`')\n        result['preds'] = preds\n    else:\n        result['preds'] = [\n            dict(\n                bboxes=pred.bboxes.cpu().numpy(),\n                scores=pred.scores.cpu().numpy(),\n                labels=pred.labels.cpu().numpy()) for pred in preds\n        ]\n    if visualization is not None:\n        result['visualization'] = visualization\n    if print_result:\n        print_result_mmdet(result)\n    return result\n\n", "CodeBleu_score": 46.25796639298556}]}
{"instance_id": "a3c2561f199c", "generation_result": [{"code": "def build_test_pipeline(cfg: ConfigType) -> ConfigType\n\t\n    test_pipeline = copy.deepcopy(cfg.data.test.pipeline)\n    test_pipeline[0].type = 'Resize'\n    test_pipeline[-1].transforms = [test_pipeline[0], test_pipeline[-1].transforms[-1]]\n    test_pipeline[-1].type = 'PackDetInputs'\n    return test_pipeline\n\n", "CodeBleu_score": 20.086484577126146}, {"code": "def build_test_pipeline(cfg: ConfigType) -> ConfigType\n\t\n    test_pipeline = copy.deepcopy(cfg.dataset.pipeline)\n    test_pipeline[0] = test_pipeline[0].transforms[0]\n    test_pipeline[-1] = Compose(test_pipeline[-1].transforms)\n    return test_pipeline\n\n", "CodeBleu_score": 14.413909544586886}, {"code": "def build_test_pipeline(cfg: ConfigType) -> ConfigType\n\t\n    test_pipeline = copy.deepcopy(cfg.test_dataloader.dataset.pipeline)\n    test_pipeline[0] = copy.deepcopy(test_pipeline[0])\n    test_pipeline[0].type = \"Resize\"\n    test_pipeline[-1] = copy.deepcopy(test_pipeline[-1])\n    test_pipeline = Compose(test_pipeline)\n    return test_pipeline\n\n", "CodeBleu_score": 22.518385509101346}]}
{"instance_id": "48a358ab5a38", "generation_result": [{"code": "def inference_mot(model: nn.Module, img: np.ndarray, frame_id: int,\n                  video_len: int) -> SampleList\n\t\n    cfg = model.cfg\n    device = next(model.parameters()).device  # model device\n    # prepare data\n    data = dict(img=img, frame_id=frame_id, img_id=0, img_shape=img.shape)\n    # build the data pipeline\n    test_pipeline = Compose(cfg.test_dataloader.dataset.pipeline)\n    data = test_pipeline(data)\n    data = default_collate([data])\n    if device.type == 'cpu':\n        # prevent unsupported modules from being used on CPU\n        data = replace_ImageToTensor(data)\n        data = replace_totensor(data)\n    # forward the model\n    with torch.no_grad():\n        result = model.test_step(data)[0]\n    # format result\n    result = result.pred_instances\n    result.pop('img_shape', None)\n    result.pop('img_id', None)\n    result.pop('frame_id', None)\n    result.video_len = video_len\n    return result\n\n", "CodeBleu_score": 39.568374520360344}, {"code": "def inference_mot(model: nn.Module, img: np.ndarray, frame_id: int,\n                  video_len: int) -> SampleList\n\t\n    cfg = model.cfg\n    device = next(model.parameters()).device  # model device\n\n    # build the data pipeline\n    test_pipeline = Compose(cfg.test_dataloader.dataset.pipeline)\n    # prepare data\n    data = dict(img=img, frame_id=frame_id, img_id=0, img_shape=img.shape,\n                ori_shape=img.shape, modality='RGB', img_fields=['img'],\n                video_len=video_len)\n    # build the data pipeline\n    data = test_pipeline(data)\n    data = default_collate([data])\n    # convert to cuda if device is GPU\n    if next(model.parameters()).is_cuda:\n        # scatter to specified GPU\n        data = scatter(data, [device])[0]\n    else:\n        for m in model.modules():\n            assert not isinstance(\n                m, RoIPool\n            ), 'CPU inference with RoIPool is not supported currently.'\n\n    # forward the model\n    with torch.no_grad():\n        result = model.test_step(data)[0]\n\n    return result\n\n", "CodeBleu_score": 54.81676206945123}, {"code": "def inference_mot(model: nn.Module, img: np.ndarray, frame_id: int,\n                  video_len: int) -> SampleList\n\t\n    cfg = model.cfg\n    device = next(model.parameters()).device  # model device\n\n    # prepare data\n    data = dict(img=img, frame_id=frame_id, img_id=0, img_shape=img.shape)\n    if video_len > 0:\n        data['video_len'] = video_len\n\n    # build the data pipeline\n    test_pipeline = Compose(cfg.test_pipeline)\n    data = test_pipeline(data)\n    data = default_collate([data])\n    if device.type != 'cpu':\n        # scatter to specified GPU\n        data = scatter(data, [device])[0]\n    else:\n        # This is a workaround to avoid the bug of MMDataParallel\n        data['img_metas'] = data['img_metas'][0].data\n        data['img'] = data['img'][0].data\n        # disable unsupported RoIPool\n        if cfg.get('max_per_img', 100) > 0:\n            warnings.warn(\n                'Setting `max_per_img` of MOT models > 0 may cause '\n                'unexpected bugs, please set `max_per_img` to `-1` '\n                'instead')\n            cfg.max_per_img = -1\n        # disable unsupported modules like RoIPool\n        disable_unsupported_modules(model, cfg)\n\n    # forward the model\n    with torch.no_grad():\n        result = model.test_step(data)[0]\n\n    return result\n\n", "CodeBleu_score": 44.929527164390485}]}
{"instance_id": "6c4ea2ce1787", "generation_result": [{"code": "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\n\t\n    while True:\n        total_length = len(tokens_a) + len(tokens_b)\n        if total_length <= max_num_tokens:\n            return tokens_a, tokens_b\n        trunc_tokens = tokens_a if len(tokens_a) > len(tokens_b) else tokens_b\n        assert len(trunc_tokens) >= 1\n        if rng.random() < 0.5:\n            del trunc_tokens[0]\n        else:\n            trunc_tokens.pop()\n\n", "CodeBleu_score": 70.92892981826282}, {"code": "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\n\t\n    while True:\n        total_length = len(tokens_a) + len(tokens_b)\n        if total_length <= max_num_tokens:\n            break\n\n        trunc_tokens = tokens_a if len(tokens_a) > len(tokens_b) else tokens_b\n        assert len(trunc_tokens) >= 1\n\n        # We want to sometimes truncate from the front and sometimes from the\n        # back to add more randomness and avoid biases.\n        if rng.random() < 0.5:\n            del trunc_tokens[0]\n        else:\n            trunc_tokens.pop()\n\n", "CodeBleu_score": 76.28687604964396}, {"code": "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\n\t\n    while True:\n        total_length = len(tokens_a) + len(tokens_b)\n        if total_length <= max_num_tokens:\n            break\n        trunc_tokens = (tokens_a if len(tokens_a) > len(tokens_b) else tokens_b)\n        assert len(trunc_tokens) >= 1\n        if len(trunc_tokens) == 1:\n            break\n        trunc_tokens = trunc_tokens[int(random.random() * len(trunc_tokens)):]\n        assert len(trunc_tokens) >= 1\n        if len(tokens_a) > len(tokens_b):\n            tokens_a = trunc_tokens\n        else:\n            tokens_b = trunc_tokens\n    return tokens_a, tokens_b\n\n", "CodeBleu_score": 53.760391971455256}]}
{"instance_id": "007181b6729d", "generation_result": [{"code": "def get_activation(activation_string)\n\t\n    if activation_string is None:\n        return None\n    if isinstance(activation_string, six.string_types):\n        act = activation_string\n        act_fn = None\n        if act == 'linear':\n            act_fn = lambda x: x\n        elif act == 'relu':\n            act_fn = tf.nn.relu\n        elif act == 'sigmoid':\n            act_fn = tf.sigmoid\n        elif act == 'tanh':\n            act_fn = tf.tanh\n        else:\n            raise ValueError('Unsupported activation: %s' % act)\n        return act_fn\n    elif callable(activation_string):\n        return activation_string\n    else:\n        raise ValueError('Could not interpret ' 'activation function identifier:', activation_string)\n\n", "CodeBleu_score": 42.46819762621939}, {"code": "def get_activation(activation_string)\n\t\n    if isinstance(activation_string, six.string_types):\n        if not activation_string:\n            return None\n        elif activation_string == 'linear':\n            return linear\n        elif activation_string == 'relu':\n            return relu\n        elif activation_string == 'sigmoid':\n            return sigmoid\n        elif activation_string == 'tanh':\n            return tanh\n        elif activation_string == 'softmax':\n            return softmax\n        else:\n            raise ValueError('Unsupported activation function: ' + activation_string)\n    else:\n        return activation_string\n\n", "CodeBleu_score": 35.234472350921074}, {"code": "def get_activation(activation_string)\n\t\n    if activation_string is None:\n        return None\n    if isinstance(activation_string, six.string_types):\n        act = activation_string\n        act_fn = None\n        if act == 'linear':\n            act_fn = linear\n        elif act == 'relu':\n            act_fn = relu\n        elif act == 'rrelu':\n            act_fn = rrelu\n        elif act == 'elu':\n            act_fn = elu\n        elif act == 'selu':\n            act_fn = selu\n        elif act == 'gelu':\n            act_fn = gelu\n        elif act == 'tanh':\n            act_fn = tanh\n        elif act == 'sigmoid':\n            act_fn = sigmoid\n        elif act == 'softmax':\n            act_fn = softmax\n        elif act == 'log_softmax':\n            act_fn = log_softmax\n        elif act == 'softplus':\n            act_fn = softplus\n        else:\n            raise ValueError('Unsupported activation: %s.' % act)\n        return act_fn\n    else:\n        return activation_string\n\n", "CodeBleu_score": 38.22645153646527}]}
{"instance_id": "d5d6e0315899", "generation_result": [{"code": "def dropout(input_tensor, dropout_prob)\n\t\n    if dropout_prob is None or dropout_prob == 0.0:\n        return input_tensor\n    return tf.nn.dropout(input_tensor, 1.0 - dropout_prob)\n\n", "CodeBleu_score": 60.78403505639335}, {"code": "def dropout(input_tensor, dropout_prob)\n\t\n    if dropout_prob is None or dropout_prob == 0.0:\n        return input_tensor\n    return tf.nn.dropout(input_tensor, 1.0 - dropout_prob)\n\n", "CodeBleu_score": 60.78403505639335}, {"code": "def dropout(input_tensor, dropout_prob)\n\t\n    if dropout_prob is None or dropout_prob == 0.0:\n        return input_tensor\n    return tf.nn.dropout(input_tensor, 1.0 - dropout_prob)\n\n", "CodeBleu_score": 60.78403505639335}]}
{"instance_id": "828917b9ac53", "generation_result": [{"code": "def create_attention_mask_from_input_mask(from_tensor, to_mask)\n\t\n    from_shape = from_tensor.shape.as_list()\n    batch_size = from_shape[0]\n    from_seq_length = from_shape[1]\n    to_shape = to_mask.shape.as_list()\n    to_seq_length = to_shape[1]\n    to_mask = tf.cast(tf.reshape(to_mask, [batch_size, 1, to_seq_length]), tf.float32)\n    broadcast_ones = tf.ones(shape=[batch_size, from_seq_length, 1], dtype=tf.float32)\n    mask = broadcast_ones * to_mask\n    return mask\n\n", "CodeBleu_score": 49.79420960881377}, {"code": "def create_attention_mask_from_input_mask(from_tensor, to_mask)\n\t\n    from_shape = from_tensor.shape\n    batch_size, from_seq_length = from_shape\n    to_shape = to_mask.shape\n    to_seq_length = to_shape[1]\n    to_mask = tf.cast(\n        tf.reshape(to_mask, [batch_size, 1, to_seq_length]),\n        tf.float32)\n    broadcast_ones = tf.ones(\n        shape=[batch_size, from_seq_length, 1], dtype=tf.float32)\n    mask = broadcast_ones * to_mask\n    return mask\n\n", "CodeBleu_score": 49.42846282739973}, {"code": "def create_attention_mask_from_input_mask(from_tensor, to_mask)\n\t\n    from_shape = from_tensor.shape\n    batch_size = from_shape[0]\n    from_seq_length = from_shape[1]\n    to_shape = to_mask.shape\n    to_seq_length = to_shape[1]\n    to_mask = tf.cast(tf.reshape(to_mask, [batch_size, 1, to_seq_length]), tf.float32)\n    broadcast_ones = tf.ones(shape=[batch_size, from_seq_length, 1], dtype=tf.float32)\n    mask = broadcast_ones * to_mask\n    return mask\n\n", "CodeBleu_score": 48.748979022033794}]}
{"instance_id": "d7798e41a723", "generation_result": [{"code": "def transpose_for_scores(input_tensor, batch_size, num_attention_heads,\n                           seq_length, width)\n\t\n    output_tensor = tf.reshape(\n        input_tensor,\n        [batch_size, seq_length, num_attention_heads, width])\n\n    output_tensor = tf.transpose(output_tensor, [0, 2, 1, 3])\n    return output_tensor\n\n", "CodeBleu_score": 58.86672543766436}, {"code": "def transpose_for_scores(input_tensor, batch_size, num_attention_heads,\n                           seq_length, width)\n\t\n    output_tensor = tf.reshape(\n        input_tensor,\n        [batch_size, seq_length, num_attention_heads, width])\n\n    output_tensor = tf.transpose(output_tensor, [0, 2, 1, 3])\n    return output_tensor\n\n", "CodeBleu_score": 58.86672543766436}, {"code": "def transpose_for_scores(input_tensor, batch_size, num_attention_heads,\n                           seq_length, width)\n\t\n    output_tensor = torch.reshape(\n        input_tensor,\n        [batch_size, seq_length, num_attention_heads, width])\n\n    output_tensor = torch.permute(output_tensor, [0, 2, 1, 3])\n    return output_tensor\n\n", "CodeBleu_score": 48.65352142526975}]}
{"instance_id": "711d466ace09", "generation_result": [{"code": "def reshape_to_matrix(input_tensor)\n\t\n    ndims = input_tensor.shape.ndims\n    if ndims is None or ndims < 2:\n        raise ValueError(\"Input tensor must have at least 2 dimensions, but the ndims is {}\".format(ndims))\n    if ndims == 2:\n        return input_tensor\n    width = input_tensor.shape[-1]\n    output_tensor = tf.reshape(input_tensor, [-1, width])\n    return output_tensor\n\n", "CodeBleu_score": 69.99129292462837}, {"code": "def reshape_to_matrix(input_tensor)\n\t\n    ndims = input_tensor.shape.ndims\n    if ndims is None:\n        raise ValueError('Input tensor must have a known rank.')\n    if ndims < 2:\n        raise ValueError('Input tensor must have a rank of at least 2.')\n    if ndims == 2:\n        return input_tensor\n    width = input_tensor.shape[-1]\n    output_tensor = tf.reshape(input_tensor, [-1, width])\n    return output_tensor\n\n", "CodeBleu_score": 67.05338057212073}, {"code": "def reshape_to_matrix(input_tensor)\n\t\n    ndims = input_tensor.shape.ndims\n    if ndims is None:\n        raise ValueError(\"Rank of `input_tensor` must be known.\")\n    if ndims < 2:\n        raise ValueError(\"The input tensor must have at least 2 dimensions, but the \"\n                         \"rank of `input_tensor` is %s\" % ndims)\n    if ndims == 2:\n        return input_tensor\n\n    width = input_tensor.shape[-1]\n    output_tensor = tf.reshape(input_tensor, [-1, width])\n    return output_tensor\n\n", "CodeBleu_score": 66.12663877144243}]}
{"instance_id": "7adcd08f531e", "generation_result": [{"code": "def get_train_examples(self, data_dir)\n\t\n    return self._create_examples(\n        self._read_tsv(os.path.join(data_dir, \"train.tsv\")), \"train\")\n\n", "CodeBleu_score": 6.324149162613836}, {"code": "def get_train_examples(self, data_dir)\n\t\n    return self._create_examples(\n        self._read_tsv(os.path.join(data_dir, \"train.tsv\")), \"train\")\n\n", "CodeBleu_score": 6.324149162613836}, {"code": "def get_train_examples(self, data_dir)\n\t\n    return self._create_examples(\n        self._read_tsv(os.path.join(data_dir, \"train.tsv\")), \"train\")\n\n", "CodeBleu_score": 6.324149162613836}]}
{"instance_id": "26a0bb96dab9", "generation_result": [{"code": "def metric_fn(per_example_loss, label_ids, logits, is_real_example)\n\t\n    predictions = tf.argmax(logits, axis=-1, output_type=tf.int32)\n    accuracy = tf.cast(tf.equal(predictions, label_ids), dtype=tf.float32)\n    accuracy = tf.reduce_mean(accuracy)\n    loss = tf.reduce_mean(per_example_loss)\n    return {\"eval_accuracy\": accuracy, \"eval_loss\": loss}\n\n", "CodeBleu_score": 40.48985246544493}, {"code": "def metric_fn(per_example_loss, label_ids, logits, is_real_example)\n\t\n    predictions = tf.argmax(logits, axis=-1, output_type=tf.int32)\n    accuracy = tf.reduce_sum(\n        tf.cast(tf.equal(predictions, label_ids), tf.float32)\n        * tf.cast(is_real_example, tf.float32))\n    accuracy = accuracy / tf.reduce_sum(tf.cast(is_real_example, tf.float32))\n    loss = tf.reduce_sum(per_example_loss * tf.cast(is_real_example, tf.float32))\n    loss = loss / tf.reduce_sum(tf.cast(is_real_example, tf.float32))\n    return {\"eval_accuracy\": accuracy, \"eval_loss\": loss}\n\n", "CodeBleu_score": 37.314964111967136}, {"code": "def metric_fn(per_example_loss, label_ids, logits, is_real_example)\n\t\n    predictions = tf.argmax(logits, axis=-1, output_type=tf.int32)\n    accuracy = tf.reduce_sum(\n        tf.cast(tf.equal(predictions, label_ids), dtype=tf.float32)\n    ) / tf.reduce_sum(tf.cast(is_real_example, dtype=tf.float32))\n    loss = tf.reduce_sum(per_example_loss) / tf.reduce_sum(\n        tf.cast(is_real_example, dtype=tf.float32)\n    )\n    return {\"eval_accuracy\": accuracy, \"eval_loss\": loss}\n\n", "CodeBleu_score": 37.183631919807695}]}
{"instance_id": "6154e4d28509", "generation_result": [{"code": "def convert_examples_to_features(examples, label_list, max_seq_length,\n                                 tokenizer)\n\t\n    label_map = {}\n    for (i, label) in enumerate(label_list):\n        label_map[label] = i\n    features = []\n    print('Processing {} examples...'.format(len(examples)))\n    for (ex_index, example) in enumerate(examples):\n        if ex_index % 10000 == 0:\n            print('Writing example {} of {}'.format(ex_index, len(examples)))\n        feature = convert_single_example(ex_index, example, label_map,\n                                         max_seq_length, tokenizer)\n        features.append(feature)\n    return features\n\n", "CodeBleu_score": 55.65018960834951}, {"code": "def convert_examples_to_features(examples, label_list, max_seq_length,\n                                 tokenizer)\n\t\n    label_map = {}\n    for (i, label) in enumerate(label_list):\n        label_map[label] = i\n\n    features = []\n    for (ex_index, example) in enumerate(examples):\n        if ex_index % 10000 == 0:\n            print(f\"Writing example {ex_index} of {len(examples)}\")\n\n        input_ids = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(example.text_a))\n\n        if len(input_ids) > max_seq_length:\n            input_ids = input_ids[:max_seq_length]\n\n        while len(input_ids) < max_seq_length:\n            input_ids.append(0)\n\n        assert len(input_ids) == max_seq_length\n\n        label_id = label_map[example.label]\n\n        features.append(\n            optimization.InputFeatures(input_ids=input_ids,\n                                      label_id=label_id))\n\n    return features\n\n", "CodeBleu_score": 39.19596290891938}, {"code": "def convert_examples_to_features(examples, label_list, max_seq_length,\n                                 tokenizer)\n\t\n    label_map = {}\n    for (i, label) in enumerate(label_list):\n        label_map[label] = i\n    features = []\n    for (ex_index, example) in enumerate(examples):\n        if ex_index % 10000 == 0:\n            print('Writing example %d of %d' % (ex_index, len(examples)))\n        feature = convert_single_example(ex_index, example, label_map,\n                                         max_seq_length, tokenizer)\n        features.append(feature)\n    return features\n\n", "CodeBleu_score": 64.60166938133105}]}
{"instance_id": "e09c0db71ebb", "generation_result": [{"code": "def get_next_sentence_output(bert_config, input_tensor, labels)\n\t\n    # Get the output weights\n    output_weights = tf.get_variable(\n        \"output_weights\", [bert_config.vocab_size, bert_config.hidden_size],\n        initializer=tf.truncated_normal_initializer(stddev=bert_config.initializer_range))\n\n    # Get the output bias\n    output_bias = tf.get_variable(\n        \"output_bias\", [bert_config.vocab_size], initializer=tf.zeros_initializer())\n\n    # Compute logits by matrix multiplication\n    logits = tf.matmul(input_tensor, output_weights, transpose_b=True)\n\n    # Add bias to logits\n    logits = tf.nn.bias_add(logits, output_bias)\n\n    # Convert labels to one-hot encoding\n    one_hot_labels = tf.one_hot(labels, depth=bert_config.vocab_size, dtype=tf.float32)\n\n    # Compute per-example loss\n    per_example_loss = -tf.reduce_sum(tf.multiply(one_hot_labels, tf.nn.log_softmax(logits)), axis=-1)\n\n    # Compute mean loss\n    loss = tf.reduce_mean(per_example_loss)\n\n    return (loss, per_example_loss, logits)\n\n", "CodeBleu_score": 41.040805690212395}, {"code": "def get_next_sentence_output(bert_config, input_tensor, labels)\n\t\n    output_weights = tf.get_variable(\n        \"output_weights\", [bert_config.vocab_size, bert_config.hidden_size],\n        initializer=tf.truncated_normal_initializer(stddev=bert_config.initializer_range))\n\n    output_bias = tf.get_variable(\n        \"output_bias\", [bert_config.vocab_size], initializer=tf.zeros_initializer())\n\n    logits = tf.matmul(input_tensor, output_weights, transpose_b=True)\n    logits = tf.nn.bias_add(logits, output_bias)\n    log_probs = tf.nn.log_softmax(logits, axis=-1)\n\n    labels = tf.reshape(labels, [-1])\n    one_hot_labels = tf.one_hot(labels, depth=bert_config.vocab_size, dtype=tf.float32)\n\n    per_example_loss = -tf.reduce_sum(log_probs * one_hot_labels, axis=-1)\n    loss = tf.reduce_mean(per_example_loss)\n\n    return (loss, per_example_loss, log_probs)\n\n", "CodeBleu_score": 48.26622164636139}, {"code": "def get_next_sentence_output(bert_config, input_tensor, labels)\n\t\n\n    # Compute logits.\n    output_weights = tf.get_variable(\n        \"output_weights\", [bert_config.hidden_size, 2],\n        initializer=tf.truncated_normal_initializer(stddev=0.02))\n    output_bias = tf.get_variable(\n        \"output_bias\", [2], initializer=tf.zeros_initializer())\n    logits = tf.matmul(input_tensor, output_weights)\n    logits = tf.nn.bias_add(logits, output_bias)\n\n    # Reshape labels.\n    labels = tf.reshape(labels, [-1])\n    one_hot_labels = tf.one_hot(labels, depth=2, dtype=tf.float32)\n\n    # Compute per-example loss.\n    per_example_loss = -tf.reduce_sum(tf.multiply(one_hot_labels, tf.nn.log_softmax(logits)), axis=-1)\n\n    # Compute mean loss.\n    loss = tf.reduce_mean(per_example_loss)\n\n    # Compute log probabilities.\n    log_probs = tf.nn.log_softmax(logits, axis=-1)\n\n    return loss, per_example_loss, log_probs\n\n", "CodeBleu_score": 42.006894433330224}]}
{"instance_id": "1f66b020f485", "generation_result": [{"code": "def gather_indexes(sequence_tensor, positions)\n\t\n    sequence_shape = modeling.get_shape_list(sequence_tensor, expected_rank=3)\n    batch_size = sequence_shape[0]\n    seq_length = sequence_shape[1]\n\n    flat_offsets = tf.reshape(\n        tf.range(0, batch_size, dtype=tf.int32) * seq_length, [-1, 1])\n    flat_positions = tf.reshape(positions + flat_offsets, [-1])\n    flat_sequence_tensor = tf.reshape(sequence_tensor,\n                                      [batch_size * seq_length, width])\n    output_tensor = tf.gather(flat_sequence_tensor, flat_positions)\n\n    return output_tensor\n\n", "CodeBleu_score": 54.803582831697305}, {"code": "def gather_indexes(sequence_tensor, positions)\n\t\n    sequence_shape = modeling.get_shape_list(sequence_tensor, expected_rank=3)\n    batch_size = sequence_shape[0]\n    seq_length = sequence_shape[1]\n\n    flat_offsets = tf.reshape(\n        tf.range(0, batch_size, dtype=tf.int32) * seq_length, [-1, 1]\n    )\n    flat_positions = tf.reshape(positions + flat_offsets, [-1])\n    flat_sequence_tensor = tf.reshape(sequence_tensor, [batch_size * seq_length, width])\n    output_tensor = tf.gather(flat_sequence_tensor, flat_positions)\n    return output_tensor\n\n", "CodeBleu_score": 52.21793439244114}, {"code": "def gather_indexes(sequence_tensor, positions)\n\t\n    sequence_shape = modeling.get_shape_list(sequence_tensor, expected_rank=3)\n    batch_size = sequence_shape[0]\n    seq_length = sequence_shape[1]\n\n    flat_offsets = tf.reshape(\n        tf.range(0, batch_size, dtype=tf.int32) * seq_length, [-1, 1])\n    flat_positions = tf.reshape(positions + flat_offsets, [-1])\n    flat_sequence_tensor = tf.reshape(sequence_tensor,\n                                      [batch_size * seq_length, width])\n    output_tensor = tf.gather(flat_sequence_tensor, flat_positions)\n    return output_tensor\n\n", "CodeBleu_score": 54.803582831697305}]}
{"instance_id": "956bffee9a33", "generation_result": [{"code": "def _decode_record(record, name_to_features)\n\t\n    example = tf.io.parse_single_example(record, name_to_features)\n\n    for name in list(example.keys()):\n        t = example[name]\n        if t.dtype == tf.int64:\n            t = tf.cast(t, tf.int32)\n        example[name] = t\n\n    return example\n\n", "CodeBleu_score": 60.19566542534298}, {"code": "def _decode_record(record, name_to_features)\n\t\n    example = tf.io.parse_single_example(record, name_to_features)\n\n    for name in list(example.keys()):\n        t = example[name]\n        if t.dtype == tf.int64:\n            t = tf.cast(t, tf.int32)\n        example[name] = t\n\n    return example\n\n", "CodeBleu_score": 60.19566542534298}, {"code": "def _decode_record(record, name_to_features)\n\t\n    example = tf.io.parse_single_example(record, name_to_features)\n\n    for name in list(example.keys()):\n        t = example[name]\n        if t.dtype == tf.int64:\n            t = tf.cast(t, tf.int32)\n        example[name] = t\n\n    return example\n\n", "CodeBleu_score": 60.19566542534298}]}
{"instance_id": "6b97f354e641", "generation_result": [{"code": "def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer,\n                         orig_answer_text)\n\t\n    # We don't need to check the end position because the input span is already the maximum span. We are looking for the start and end position of the answer in the original text.\n\n    tok_answer_text = \" \".join(tokenizer.tokenize(orig_answer_text))\n\n    for new_start in range(input_start, input_end + 1):\n        for new_end in range(input_end, new_start - 1, -1):\n            text_span = \" \".join(doc_tokens[new_start:(new_end + 1)])\n            if text_span == tok_answer_text:\n                return (new_start, new_end)\n\n    return (input_start, input_end)\n\n", "CodeBleu_score": 78.30625722805418}, {"code": "def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer,\n                         orig_answer_text)\n\t\n    tok_answer_text = \" \".join(tokenizer.tokenize(orig_answer_text))\n\n    for new_start in range(input_start, input_end + 1):\n        for new_end in range(input_end, new_start - 1, -1):\n            text_span = \" \".join(doc_tokens[new_start:(new_end + 1)])\n            if text_span == tok_answer_text:\n                return (new_start, new_end)\n\n    return (input_start, input_end)\n\n", "CodeBleu_score": 78.30625722805418}, {"code": "def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer,\n                         orig_answer_text)\n\t\n    tok_text = \" \".join(tokenizer.tokenize(orig_answer_text))\n\n    for new_start in range(input_start, input_end + 1):\n        for new_end in range(input_end, new_start - 1, -1):\n            text_span = \" \".join(doc_tokens[new_start:(new_end + 1)])\n            if text_span == tok_text:\n                return (new_start, new_end)\n\n    return (input_start, input_end)\n\n", "CodeBleu_score": 75.683304146581}]}
{"instance_id": "aba334b92b34", "generation_result": [{"code": "def _check_is_max_context(doc_spans, cur_span_index, position)\n\t\n    best_score = None\n    best_span_index = None\n    for (span_index, doc_span) in enumerate(doc_spans):\n        end = doc_span.start + doc_span.length - 1\n        if position < doc_span.start:\n            continue\n        if position > end:\n            continue\n        num_left_context = position - doc_span.start\n        num_right_context = end - position\n        score = min(num_left_context, num_right_context) + 0.01 * doc_span.length\n        if best_score is None or score > best_score:\n            best_score = score\n            best_span_index = span_index\n    return cur_span_index == best_span_index\n\n", "CodeBleu_score": 78.6774915248039}, {"code": "def _check_is_max_context(doc_spans, cur_span_index, position)\n\t\n    best_score = None\n    best_span_index = None\n    for (span_index, doc_span) in enumerate(doc_spans):\n        end = doc_span.start + doc_span.length - 1\n        if position < doc_span.start:\n            continue\n        if position > end:\n            continue\n        num_left_context = position - doc_span.start\n        num_right_context = end - position\n        score = min(num_left_context, num_right_context) + 0.01 * doc_span.length\n        if best_score is None or score > best_score:\n            best_score = score\n            best_span_index = span_index\n    return cur_span_index == best_span_index\n\n", "CodeBleu_score": 78.6774915248039}, {"code": "def _check_is_max_context(doc_spans, cur_span_index, position)\n\t\n    best_score = None\n    best_span_index = None\n    for (span_index, doc_span) in enumerate(doc_spans):\n        end = doc_span.start + doc_span.length - 1\n        if position < doc_span.start:\n            continue\n        if position > end:\n            continue\n        num_left_context = position - doc_span.start\n        num_right_context = end - position\n        score = min(num_left_context, num_right_context) + 0.01 * doc_span.length\n        if best_score is None or score > best_score:\n            best_score = score\n            best_span_index = span_index\n    return cur_span_index == best_span_index\n\n", "CodeBleu_score": 78.6774915248039}]}
{"instance_id": "cf92ad509f6a", "generation_result": [{"code": "def load_vocab(vocab_file)\n\t\n    vocab = collections.OrderedDict()\n    index = 0\n    with open(vocab_file, \"r\", encoding=\"utf-8\") as reader:\n        while True:\n            token = reader.readline()\n            if not token:\n                break\n            token = token.strip()\n            vocab[token] = index\n            index += 1\n    return vocab\n\n", "CodeBleu_score": 59.6145411111134}, {"code": "def load_vocab(vocab_file)\n\t\n    vocab = collections.OrderedDict()\n    index = 0\n    with open(vocab_file, \"r\", encoding=\"utf-8\") as reader:\n        while True:\n            token = reader.readline()\n            if not token:\n                break\n            token = token.strip()\n            vocab[token] = index\n            index += 1\n    return vocab\n\n", "CodeBleu_score": 59.6145411111134}, {"code": "def load_vocab(vocab_file)\n\t\n    vocab = collections.OrderedDict()\n    index = 0\n    with open(vocab_file, \"r\", encoding=\"utf-8\") as reader:\n        while True:\n            token = reader.readline()\n            if not token:\n                break\n            token = token.strip()\n            vocab[token] = index\n            index += 1\n    return vocab\n\n", "CodeBleu_score": 59.6145411111134}]}
{"instance_id": "6c865803ac12", "generation_result": [{"code": "def convert_by_vocab(vocab, items)\n\t\n    output = []\n    for item in items:\n        output.append(vocab[item])\n    return output\n\n", "CodeBleu_score": 79.63749907165396}, {"code": "def convert_by_vocab(vocab, items)\n\t\n    output = []\n    for item in items:\n        output.append(vocab[item])\n    return output\n\n", "CodeBleu_score": 79.63749907165396}, {"code": "def convert_by_vocab(vocab, items)\n\t\n    output = []\n    for item in items:\n        output.append(vocab[item])\n    return output\n\n", "CodeBleu_score": 79.63749907165396}]}
{"instance_id": "e93b9200dc4a", "generation_result": [{"code": "def tokenize(self, text)\n\t\n    text = unicodedata.normalize(\"NFD\", text)\n    text = \"\".join(\n        [ch for ch in text if unicodedata.category(ch) != \"Mn\"]\n    )  # Strip accents\n    text = text.lower()\n    text = re.sub(\"[^A-Za-z0-9]+\", \" \", text)\n    tokens = self.basic_tokenizer.tokenize(text)\n    tokens = [self.wordpiece_tokenizer.tokenize(token) for token in tokens]\n    tokens = [token for sublist in tokens for token in sublist]\n    return tokens\n\n", "CodeBleu_score": 35.47421830233809}, {"code": "def tokenize(self, text)\n\t\n    text = self.clean_text(text)\n    split_tokens = []\n    for token in self.basic_tokenizer.tokenize(text, never_split=self.all_special_tokens):\n        for sub_token in self.wordpiece_tokenizer.tokenize(token):\n            split_tokens.append(sub_token)\n    return split_tokens\n\n", "CodeBleu_score": 62.34099159945826}, {"code": "def tokenize(self, text)\n\t\n    strip_accents = False\n    lower_case = True\n    text = self.basic_tokenizer.tokenize(text, lower_case=lower_case, strip_accents=strip_accents)\n    tokens = []\n    for token in text:\n        word_pieces = self.wordpiece_tokenizer.tokenize(token)\n        tokens.extend(word_pieces)\n    return tokens\n\n", "CodeBleu_score": 46.38421094456206}]}
{"instance_id": "2ff6f6db53c9", "generation_result": [{"code": "def safe_load(path, offset, duration, sample_rate, dtype)\n\t\n    try:\n        return AudioAdapter.load(\n            str(path),\n            offset=offset,\n            duration=duration,\n            sample_rate=sample_rate,\n            dtype=dtype,\n        ), False\n    except Exception as e:\n        print(f\"Error while loading audio file {path}: {e}\")\n        return -1.0, True\n\n", "CodeBleu_score": 25.593115922413144}, {"code": "def safe_load(path, offset, duration, sample_rate, dtype)\n\t\n    try:\n        y, sr = librosa.load(\n            path,\n            offset=offset,\n            duration=duration,\n            sr=sample_rate,\n            dtype=dtype,\n        )\n        return y, False\n    except Exception as e:\n        logging.error(f\"An error occurred while loading an audio segment: {e}\")\n        return -1.0, True\n\n", "CodeBleu_score": 34.59952443045741}, {"code": "def safe_load(path, offset, duration, sample_rate, dtype)\n\t\n    try:\n        return load(path, offset, duration, sample_rate, dtype), False\n    except Exception as e:\n        logging.error(e)\n        return -1.0, True\n\n", "CodeBleu_score": 15.844852598717996}]}
{"instance_id": "12fbe1f03d8f", "generation_result": [{"code": "def to_n_channels(waveform: tf.Tensor, n_channels: int) -> tf.Tensor\n\t\n    if waveform.shape[-1] >= n_channels:\n        return waveform[..., :n_channels]\n    else:\n        return tf.tile(waveform, [1, n_channels // waveform.shape[-1] + 1])[..., :n_channels]\n\n", "CodeBleu_score": 38.10655171826537}, {"code": "def to_n_channels(waveform: tf.Tensor, n_channels: int) -> tf.Tensor\n\t\n    if waveform.shape[1] >= n_channels:\n        return waveform[:, :n_channels]\n    else:\n        return waveform[:, :n_channels]\n\n", "CodeBleu_score": 43.44141685652372}, {"code": "def to_n_channels(waveform: tf.Tensor, n_channels: int) -> tf.Tensor\n\t\n    if waveform.shape[-1] >= n_channels:\n        return waveform[..., :n_channels]\n    else:\n        return tf.tile(waveform, [1, n_channels // waveform.shape[-1] + 1])[..., :n_channels]\n\n", "CodeBleu_score": 38.10655171826537}]}
{"instance_id": "5e215f029b2e", "generation_result": [{"code": "def spectrogram_to_db_uint(\n    spectrogram: tf.Tensor, db_range: float = 100.0, **kwargs\n) -> tf.Tensor\n\t\n    db_spectrogram = gain_to_db(spectrogram, **kwargs)\n    max_db_spectrogram = tf.reduce_max(db_spectrogram)\n    db_spectrogram = tf.clip_by_value(\n        db_spectrogram, 0.0, max_db_spectrogram - db_range\n    )\n    db_spectrogram += db_range\n    db_spectrogram = tf.cast(\n        db_spectrogram * 255.0 / db_range, dtype=tf.uint8\n    )\n    return db_spectrogram\n\n\n\n", "CodeBleu_score": 47.64515466547203}, {"code": "def spectrogram_to_db_uint(\n    spectrogram: tf.Tensor, db_range: float = 100.0, **kwargs\n) -> tf.Tensor\n\t\n    if not isinstance(spectrogram, tf.Tensor):\n        raise TypeError(\"Input must be a tensor.\")\n    if len(spectrogram.shape) != 2:\n        raise ValueError(\"Input must be a 2D tensor.\")\n    if not isinstance(db_range, float):\n        raise TypeError(\"db_range must be a float.\")\n    if db_range <= 0:\n        raise ValueError(\"db_range must be positive.\")\n\n    # Convert the spectrogram to dB\n    spectrogram_db = tf.math.log1p(spectrogram)\n\n    # Find the maximum dB value\n    max_db = tf.reduce_max(spectrogram_db)\n\n    # Clip the dB spectrogram to a specified range below the maximum\n    spectrogram_db_clipped = tf.clip_by_value(\n        spectrogram_db, max_db - db_range, max_db\n    )\n\n    # Convert the clipped dB spectrogram from float32 to uint8\n    spectrogram_db_uint = tf.cast(\n        (spectrogram_db_clipped / max_db) * 255, tf.uint8\n    )\n\n    return spectrogram_db_uint\n\n", "CodeBleu_score": 43.0132980350566}, {"code": "def spectrogram_to_db_uint(\n    spectrogram: tf.Tensor, db_range: float = 100.0, **kwargs\n) -> tf.Tensor\n\t\n    spectrogram_dB = tf.math.scalar_mul(\n        20 / db_range,\n        tf.math.log(tf.math.maximum(spectrogram, 1e-10)) / tf.math.log(10.0),\n    )\n    max_value = tf.math.reduce_max(spectrogram_dB)\n    spectrogram_dB_clipped = tf.clip_by_value(\n        spectrogram_dB, max_value - db_range, max_value\n    )\n    return tf.cast(\n        tf.math.scalar_mul(255 / db_range, spectrogram_dB_clipped - max_value + db_range),\n        tf.uint8,\n    )\n\n", "CodeBleu_score": 37.85837575787501}]}
{"instance_id": "b65c0968dae7", "generation_result": [{"code": "def compute_spectrogram_tf(\n    waveform: tf.Tensor,\n    frame_length: int = 2048,\n    frame_step: int = 512,\n    spec_exponent: float = 1.0,\n    window_exponent: float = 1.0,\n) -> tf.Tensor\n\t\n    # Apply the Hann window function raised to a specified exponent\n    window = hann_window(frame_length, periodic=True) ** window_exponent\n\n    # Perform the STFT with defined frame length and step\n    stfts = tf.signal.stft(\n        waveform,\n        frame_length=frame_length,\n        frame_step=frame_step,\n        fft_length=frame_length,\n        window_fn=window,\n        pad_end=True,\n    )\n\n    # Transpose the STFT tensor and take the absolute value raised to a specified exponent\n    spectrogram = tf.transpose(tf.abs(stfts) ** spec_exponent)\n\n    return spectrogram\n\n", "CodeBleu_score": 39.06255051028532}, {"code": "def compute_spectrogram_tf(\n    waveform: tf.Tensor,\n    frame_length: int = 2048,\n    frame_step: int = 512,\n    spec_exponent: float = 1.0,\n    window_exponent: float = 1.0,\n) -> tf.Tensor\n\t\n    # Compute the spectrogram using Short-Time Fourier Transform (STFT)\n    spectrogram = tf.signal.stft(\n        waveform,\n        frame_length=frame_length,\n        frame_step=frame_step,\n        fft_length=frame_length,\n        window_fn=tf.signal.hann_window,\n        pad_end=True,\n    )\n\n    # Apply the window function raised to a specified exponent\n    window = tf.signal.hann_window(frame_length, periodic=True) ** window_exponent\n\n    # Perform the STFT with the window function\n    spectrogram = tf.signal.stft(\n        waveform,\n        frame_length=frame_length,\n        frame_step=frame_step,\n        fft_length=frame_length,\n        window_fn=lambda frame_length: window,\n        pad_end=True,\n    )\n\n    # Transpose the spectrogram tensor\n    spectrogram = tf.transpose(spectrogram, perm=[0, 2, 1])\n\n    # Compute the absolute value of the spectrogram and raise it to a specified exponent\n    spectrogram = tf.abs(spectrogram) ** spec_exponent\n\n    return spectrogram\n\n", "CodeBleu_score": 38.92497171696095}, {"code": "def compute_spectrogram_tf(\n    waveform: tf.Tensor,\n    frame_length: int = 2048,\n    frame_step: int = 512,\n    spec_exponent: float = 1.0,\n    window_exponent: float = 1.0,\n) -> tf.Tensor\n\t\n    # Define the Hann window raised to a specified exponent\n    window = hann_window(frame_length, periodic=True) ** window_exponent\n\n    # Compute the STFT of the waveform tensor\n    stfts = tf.signal.stft(\n        waveform,\n        frame_length=frame_length,\n        frame_step=frame_step,\n        fft_length=frame_length,\n        window_fn=window,\n        pad_end=True,\n    )\n\n    # Transpose the STFT tensor and raise its absolute value to a specified exponent\n    spectrogram = tf.transpose(tf.abs(stfts) ** spec_exponent)\n\n    return spectrogram\n\n", "CodeBleu_score": 39.06255051028532}]}
{"instance_id": "a4686140a634", "generation_result": [{"code": "def time_stretch(\n    spectrogram: tf.Tensor,\n    factor: float = 1.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n    if factor == 1.0:\n        return spectrogram\n    # Get the original shape of the spectrogram\n    original_shape = tf.shape(spectrogram)\n\n    # Calculate the new time dimension\n    new_time_dim = tf.cast(\n        tf.cast(original_shape[1], tf.float32) * factor, tf.int32\n    )\n\n    # Resize the spectrogram using the specified interpolation method\n    resized_spectrogram = tf.image.resize(\n        spectrogram, [original_shape[0], new_time_dim], method=method\n    )\n\n    # Adjust the resized spectrogram to the original dimensions by cropping or padding\n    if factor > 1.0:\n        # Crop the spectrogram to the original time dimension\n        cropped_spectrogram = resized_spectrogram[:, : original_shape[1]]\n        return cropped_spectrogram\n    else:\n        # Pad the spectrogram to the original time dimension\n        padded_spectrogram = tf.pad(\n            resized_spectrogram,\n            [[0, 0], [0, original_shape[1] - new_time_dim]],\n            \"CONSTANT\",\n        )\n        return padded_spectrogram\n\n", "CodeBleu_score": 41.805354198703384}, {"code": "def time_stretch(\n    spectrogram: tf.Tensor,\n    factor: float = 1.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n    if factor == 1.0:\n        return spectrogram\n    new_time_dim = int(spectrogram.shape[1] * factor)\n    resized_spectrogram = tf.image.resize(\n        spectrogram, [spectrogram.shape[0], new_time_dim], method=method\n    )\n    if factor < 1.0:\n        resized_spectrogram = resized_spectrogram[\n            :, : spectrogram.shape[1], :\n        ]  # Crop the resized spectrogram to the original time dimension\n    else:\n        padding = tf.zeros([spectrogram.shape[0], new_time_dim - spectrogram.shape[1]])\n        resized_spectrogram = tf.concat([resized_spectrogram, padding], axis=1)\n    return resized_spectrogram\n\n", "CodeBleu_score": 36.54653056358753}, {"code": "def time_stretch(\n    spectrogram: tf.Tensor,\n    factor: float = 1.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n    # Get the original shape of the spectrogram\n    original_shape = tf.shape(spectrogram)\n\n    # Calculate the new time dimension by scaling the original time dimension by the factor\n    new_time_dim = tf.cast(original_shape[1] / factor, tf.int32)\n\n    # Resize the spectrogram using the specified interpolation method\n    resized_spectrogram = tf.image.resize(\n        spectrogram, (original_shape[0], new_time_dim), method=method\n    )\n\n    # Adjust the resized spectrogram to the original dimensions by cropping or padding\n    if new_time_dim > original_shape[1]:\n        # Crop the spectrogram to the original time dimension\n        cropped_spectrogram = resized_spectrogram[:, :original_shape[1]]\n        return cropped_spectrogram\n    else:\n        # Pad the spectrogram with zeros to the original time dimension\n        padded_spectrogram = tf.pad(\n            resized_spectrogram,\n            [[0, 0], [0, original_shape[1] - new_time_dim]],\n            \"CONSTANT\",\n        )\n        return padded_spectrogram\n\n", "CodeBleu_score": 36.338029418053665}]}
{"instance_id": "6dd01101d4ae", "generation_result": [{"code": "def pitch_shift(\n    spectrogram: tf.Tensor,\n    semitone_shift: float = 0.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n    # Calculate the resizing factor\n    factor = 2 ** (semitone_shift / 12)\n\n    # Resize the spectrogram along the frequency axis\n    resized_spectrogram = tf.image.resize(\n        spectrogram, [int(spectrogram.shape[1] * factor), spectrogram.shape[2]], method=method\n    )\n\n    # Pad the resized spectrogram to maintain the original dimensions\n    paddings = tf.constant([[0, 0], [0, spectrogram.shape[1] - resized_spectrogram.shape[1]], [0, 0]])\n    padded_spectrogram = tf.pad(resized_spectrogram, paddings)\n\n    return padded_spectrogram\n\n", "CodeBleu_score": 32.14499550291337}, {"code": "def pitch_shift(\n    spectrogram: tf.Tensor,\n    semitone_shift: float = 0.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n\n    # Calculate the resizing factor based on the semitone shift\n    factor = 2 ** (semitone_shift / 12)\n\n    # Resize the spectrogram along the frequency axis\n    resized_spectrogram = tf.image.resize(\n        spectrogram,\n        [int(spectrogram.shape[1] * factor), spectrogram.shape[2]],\n        method=method,\n    )\n\n    # Pad the resized spectrogram to maintain the original dimensions\n    padded_spectrogram = tf.image.pad_to_bounding_box(\n        resized_spectrogram,\n        0,\n        0,\n        spectrogram.shape[1],\n        spectrogram.shape[2],\n    )\n\n    return padded_spectrogram\n\n", "CodeBleu_score": 28.974318992796285}, {"code": "def pitch_shift(\n    spectrogram: tf.Tensor,\n    semitone_shift: float = 0.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n    # Calculate the resizing factor based on the semitone shift\n    resizing_factor = 2 ** (semitone_shift / 12)\n\n    # Resize the spectrogram along the frequency axis using the specified method\n    resized_spectrogram = tf.image.resize(\n        spectrogram,\n        [int(spectrogram.shape[1] * resizing_factor), spectrogram.shape[2]],\n        method=method,\n    )\n\n    # Pad the resized spectrogram to maintain the original dimensions\n    padded_spectrogram = tf.image.pad_to_bounding_box(\n        resized_spectrogram,\n        0,\n        0,\n        spectrogram.shape[1],\n        spectrogram.shape[2],\n    )\n\n    return padded_spectrogram\n\n", "CodeBleu_score": 27.692115688729142}]}
{"instance_id": "612f0826e56a", "generation_result": [{"code": "def _get_conv_activation_layer(params: Dict) -> Any\n\t\n    if params[\"conv_activation\"] == \"relu\":\n        return ReLU()\n    elif params[\"conv_activation\"] == \"elu\":\n        return ELU()\n    elif params[\"conv_activation\"] == \"leaky_relu\":\n        return LeakyReLU(alpha=0.2)\n    else:\n        logging.warning(\n            \"Conv activation layer not recognized. Defaulting to ReLU.\"\n        )\n        return ReLU()\n\n", "CodeBleu_score": 36.79815943458081}, {"code": "def _get_conv_activation_layer(params: Dict) -> Any\n\t\n    conv_activation = params.get(\"conv_activation\", \"relu\")\n    if conv_activation == \"relu\":\n        return ReLU()\n    elif conv_activation == \"elu\":\n        return ELU()\n    elif conv_activation == \"leaky_relu\":\n        return LeakyReLU(alpha=0.2)\n    else:\n        logging.warning(\n            f\"{conv_activation} is not a valid option. Defaulting to ReLU activation.\"\n        )\n        return ReLU()\n\n", "CodeBleu_score": 42.222185680755906}, {"code": "def _get_conv_activation_layer(params: Dict) -> Any\n\t\n    conv_activation = params.get(\"conv_activation\", \"relu\")\n    if conv_activation == \"relu\":\n        return ReLU()\n    elif conv_activation == \"elu\":\n        return ELU()\n    elif conv_activation == \"leaky_relu\":\n        return LeakyReLU(alpha=0.2)\n    else:\n        logging.warning(\n            f\"conv_activation {conv_activation} not supported, using ReLU instead.\"\n        )\n        return ReLU()\n\n", "CodeBleu_score": 42.33632762406238}]}
{"instance_id": "2f8b5a2fc0f2", "generation_result": [{"code": "def softmax_unet(\n    input_tensor: tf.Tensor, instruments: Iterable[str], params: Dict = {}\n) -> Dict\n\t\n    logit_masks = {}\n    for instrument in instruments:\n        logit_mask = unet(input_tensor, params=params)\n        logit_masks[instrument] = logit_mask\n\n    combined_masks = tf.stack(list(logit_masks.values()), axis=-1)\n    mask = Softmax(axis=-1)(combined_masks)\n\n    output = {}\n    for i, instrument in enumerate(instruments):\n        output[instrument] = Multiply()([mask[..., i: i + 1], input_tensor])\n\n    return output\n\n", "CodeBleu_score": 40.64402330304603}, {"code": "def softmax_unet(\n    input_tensor: tf.Tensor, instruments: Iterable[str], params: Dict = {}\n) -> Dict\n\t\n    logit_masks = []\n    for instrument in instruments:\n        logit_masks.append(unet(input_tensor, instrument, params))\n\n    logit_masks = tf.stack(logit_masks, axis=-1)\n    logit_masks = Softmax(axis=-1)(logit_masks)\n\n    output = {}\n    for instrument, logit_mask in zip(instruments, tf.unstack(logit_masks, axis=-1)):\n        output[instrument] = Multiply()([input_tensor, logit_mask])\n\n    return output\n\n", "CodeBleu_score": 26.12571736940771}, {"code": "def softmax_unet(\n    input_tensor: tf.Tensor, instruments: Iterable[str], params: Dict = {}\n) -> Dict\n\t\n    logit_masks = {}\n    for instrument in instruments:\n        logit_masks[instrument] = unet(input_tensor, instrument, params)\n    logit_masks = tf.stack(list(logit_masks.values()), axis=-1)\n    mask = Softmax(axis=-1)(logit_masks)\n    output = {}\n    for i, instrument in enumerate(instruments):\n        output[instrument] = Multiply()([input_tensor, mask[..., i]])\n    return output\n\n", "CodeBleu_score": 38.06641891505773}]}
{"instance_id": "400b19ce8520", "generation_result": [{"code": "def compute_file_checksum(path)\n\t\n    sha256_hash = hashlib.sha256()\n    with open(path, \"rb\") as f:\n        for byte_block in iter(lambda: f.read(4096), b\"\"):\n            sha256_hash.update(byte_block)\n    return sha256_hash.hexdigest()\n\n", "CodeBleu_score": 52.714058032433776}, {"code": "def compute_file_checksum(path)\n\t\n    sha256_hash = hashlib.sha256()\n    with open(path, \"rb\") as f:\n        for byte_block in iter(lambda: f.read(4096), b\"\"):\n            sha256_hash.update(byte_block)\n    return sha256_hash.hexdigest()\n\n", "CodeBleu_score": 52.714058032433776}, {"code": "def compute_file_checksum(path)\n\t\n    sha256_hash = hashlib.sha256()\n    with open(path, \"rb\") as f:\n        for byte_block in iter(lambda: f.read(4096), b\"\"):\n            sha256_hash.update(byte_block)\n    return sha256_hash.hexdigest()\n\n", "CodeBleu_score": 52.714058032433776}]}
{"instance_id": "5f2e9c6e15f1", "generation_result": [{"code": "def checksum(self, name: str) -> str\n\t\n    url = f\"{self.host}/{self.repository}/{self.release_path}/{self.checksum_index}\"\n    response = httpx.get(url)\n    if response.status_code != 200:\n        raise ValueError(f\"Failed to retrieve checksum index from {url}\")\n    index = response.json()\n    if name not in index:\n        raise ValueError(f\"Model {name} not found in checksum index\")\n    return index[name]\n\n", "CodeBleu_score": 42.135316710993024}, {"code": "def checksum(self, name: str) -> str\n\t\n    url = f\"{self.host}/{self.repository}/releases/{self.release_path}/checksums.json\"\n    r = httpx.get(url)\n    if r.status_code != 200:\n        raise ValueError(f\"Failed to retrieve checksum for {name}\")\n    index = r.json()\n    if name not in index:\n        raise ValueError(f\"{name} not found in checksum index\")\n    return index[name]\n\n", "CodeBleu_score": 39.40021785441229}, {"code": "def checksum(self, name: str) -> str\n\t\n    url = f\"{self.host}/{self.repository}/{self.release_path}/checksums.json\"\n    response = httpx.get(url)\n    response.raise_for_status()\n    index = response.json()\n    if name not in index:\n        raise ValueError(f\"Model {name} not found in index\")\n    return index[name]\n\n", "CodeBleu_score": 36.13302740867803}]}
{"instance_id": "9312c8b6032e", "generation_result": [{"code": "def download(self, name: str, path: str) -> None:\n\t\n    url = f\"{environ['MODELS_BASE_URL']}/{name}\"\n    self.logger.info(f\"Downloading {url}...\")\n    with httpx.stream(\"GET\", url, follow_redirects=True) as r:\n        r.raise_for_status()\n        file = NamedTemporaryFile(delete=False)\n        current_size = 0\n        for chunk in r.iter_bytes():\n            file.write(chunk)\n            current_size += len(chunk)\n            self.logger.debug(f\"Downloaded {current_size} bytes\")\n        file.close()\n        self.logger.info(f\"Downloaded {current_size} bytes\")\n\n    self.logger.info(\"Validating checksum...\")\n    sha256_hash = hashlib.sha256()\n    with open(file.name, \"rb\") as f:\n        # Read and update hash string value in blocks of 4K\n        for byte_block in iter(lambda: f.read(4096), b\"\"):\n            sha256_hash.update(byte_block)\n            self.logger.debug(\"Validating checksum...\")\n    if sha256_hash.hexdigest() != self.checksum:\n        raise Exception(\"Checksum validation failed\")\n    self.logger.info(\"Extracting...\")\n    with tarfile.open(file.name) as f:\n        f.extractall(path)\n    os.remove(file.name)\n    self.logger.info(\"Extraction complete\")\n\n", "CodeBleu_score": 42.36201950427301}, {"code": "def download(self, name: str, path: str) -> None:\n\t\n    url = f\"{environ['MODELS_URL']}/{name}\"\n    logging.info(f\"Downloading {url}\")\n    with NamedTemporaryFile() as temp_file:\n        with httpx.stream(\"GET\", url, follow_redirects=True) as response:\n            response.raise_for_status()\n            for chunk in response.iter_bytes():\n                temp_file.write(chunk)\n        temp_file.flush()\n        temp_file.seek(0)\n        checksum = hashlib.sha256(temp_file.read()).hexdigest()\n        if checksum != self.checksum:\n            raise ValueError(\"Checksum mismatch\")\n        temp_file.seek(0)\n        with tarfile.open(fileobj=temp_file) as tar:\n            tar.extractall(path=path)\n        os.remove(temp_file.name)\n        logging.info(f\"Extracted {name} to {path}\")\n\n", "CodeBleu_score": 35.49069385828641}, {"code": "def download(self, name: str, path: str) -> None:\n\t\n    url = f\"{self.base_url}/{name}\"\n    self.logger.info(f\"Downloading {name} from {url}\")\n    with httpx.stream(\"GET\", url, follow_redirects=True) as r:\n        r.raise_for_status()\n        total_length = int(r.headers.get(\"content-length\", 0))\n        with NamedTemporaryFile(delete=False) as f:\n            with tqdm(\n                r.iter_bytes(),\n                total=total_length,\n                unit=\"B\",\n                unit_scale=True,\n                unit_divisor=1024,\n                desc=\"Downloading\",\n            ) as bar:\n                for chunk in bar:\n                    f.write(chunk)\n            f.close()\n            if self.validate_checksum:\n                self.logger.info(\"Validating checksum\")\n                if not self.validate_checksum(f.name):\n                    raise ValueError(\"Checksum validation failed\")\n            self.logger.info(\"Extracting archive\")\n            with tarfile.open(f.name, \"r:gz\") as tar:\n                tar.extractall(path)\n            os.remove(f.name)\n            self.logger.info(\"Extraction complete\")\n\n", "CodeBleu_score": 42.2414302234125}]}
{"instance_id": "ee9493f37d77", "generation_result": [{"code": "def load_configuration(descriptor: str) -> Dict\n\t\n    if descriptor.startswith(\"embedded://\"):\n        config_name = descriptor.split(\"embedded://\")[1]\n        try:\n            with open(f\"resources/configurations/{config_name}.json\", \"r\") as f:\n                config = json.load(f)\n        except FileNotFoundError:\n            raise ValueError(f\"Embedded configuration '{config_name}' not found.\")\n    else:\n        if not exists(descriptor):\n            raise ValueError(f\"Configuration file '{descriptor}' not found.\")\n        with open(descriptor, \"r\") as f:\n            config = json.load(f)\n    return config\n\n", "CodeBleu_score": 51.94549234421548}, {"code": "def load_configuration(descriptor: str) -> Dict\n\t\n    if descriptor.startswith(\"embedded:\"):\n        # Extract the embedded configuration name\n        config_name = descriptor.split(\":\")[1]\n\n        # Read the embedded configuration\n        config_data = read_embedded_configuration(config_name)\n    else:\n        # Check if the file exists\n        if not exists(descriptor):\n            raise ValueError(f\"Configuration file '{descriptor}' not found.\")\n\n        # Read the configuration from the file\n        with open(descriptor, \"r\") as file:\n            config_data = json.load(file)\n\n    return config_data\n\n", "CodeBleu_score": 43.90414836291069}, {"code": "def load_configuration(descriptor: str) -> Dict\n\t\n    if descriptor.startswith(\"embedded:\"):\n        name = descriptor.split(\":\")[1]\n        try:\n            with open(f\"resources/configurations/{name}.json\", \"r\") as file:\n                return json.load(file)\n        except FileNotFoundError:\n            raise ValueError(f\"Embedded configuration '{name}' not found.\")\n    else:\n        if not exists(descriptor):\n            raise ValueError(f\"Configuration file '{descriptor}' not found.\")\n        with open(descriptor, \"r\") as file:\n            return json.load(file)", "CodeBleu_score": 56.635081581383886}]}
{"instance_id": "546134b67fbc", "generation_result": [{"code": "def sync_apply(\n    tensor_dict: Dict[str, tf.Tensor], func: Callable, concat_axis: int = 1\n) -> Dict[str, tf.Tensor]\n\t\n    if concat_axis not in [0, 1]:\n        raise ValueError(\"concat_axis must be either 0 or 1.\")\n\n    # Concatenate the tensors along the specified axis\n    concat_tensor = tf.concat(list(tensor_dict.values()), axis=concat_axis)\n\n    # Apply the function to the concatenated tensor\n    processed_tensor = func(concat_tensor)\n\n    # Split the processed tensor back into the original dictionary format\n    processed_tensors = tf.split(processed_tensor, len(tensor_dict), axis=concat_axis)\n    processed_tensor_dict = {\n        key: tensor for key, tensor in zip(tensor_dict.keys(), processed_tensors)\n    }\n\n    return processed_tensor_dict\n\n", "CodeBleu_score": 38.4961831302293}, {"code": "def sync_apply(\n    tensor_dict: Dict[str, tf.Tensor], func: Callable, concat_axis: int = 1\n) -> Dict[str, tf.Tensor]\n\t\n    if concat_axis not in [0, 1]:\n        raise ValueError(\"concat_axis must be either 0 or 1\")\n\n    # Concatenate tensors along the specified axis\n    tensor_list = list(tensor_dict.values())\n    concat_tensor = tf.concat(tensor_list, axis=concat_axis)\n\n    # Apply function to concatenated tensor\n    processed_tensor = func(concat_tensor)\n\n    # Split processed tensor back into dictionary format based on concatenation axis\n    processed_tensor_list = tf.split(processed_tensor, len(tensor_list), axis=concat_axis)\n    processed_tensor_dict = {}\n    for i, (key, tensor) in enumerate(tensor_dict.items()):\n        processed_tensor_dict[key] = processed_tensor_list[i]\n\n    return processed_tensor_dict\n\n", "CodeBleu_score": 42.39258939790758}, {"code": "def sync_apply(\n    tensor_dict: Dict[str, tf.Tensor], func: Callable, concat_axis: int = 1\n) -> Dict[str, tf.Tensor]\n\t\n    if concat_axis not in [0, 1]:\n        raise ValueError(\"concat_axis must be either 0 or 1\")\n\n    tensors = list(tensor_dict.values())\n    if concat_axis == 0:\n        concatenated_tensor = tf.concat(tensors, axis=concat_axis)\n    else:\n        concatenated_tensor = tf.stack(tensors, axis=concat_axis)\n\n    processed_tensor = func(concatenated_tensor)\n\n    if concat_axis == 0:\n        processed_tensors = tf.split(processed_tensor, len(tensors), axis=concat_axis)\n    else:\n        processed_tensors = tf.unstack(processed_tensor, axis=concat_axis)\n\n    return dict(zip(tensor_dict.keys(), processed_tensors))\n\n", "CodeBleu_score": 42.68803048607822}]}
{"instance_id": "df48e78d46d2", "generation_result": [{"code": "def from_float32_to_uint8(\n    tensor: tf.Tensor,\n    tensor_key: str = \"tensor\",\n    min_key: str = \"min\",\n    max_key: str = \"max\",\n) -> tf.Tensor\n\t\n    min_value = tf.reduce_min(tensor)\n    max_value = tf.reduce_max(tensor)\n    tensor = tf.cast(tensor, tf.float32)\n    tensor = (tensor - min_value) / (max_value - min_value)\n    tensor = tf.cast(tensor, tf.uint8)\n    return {tensor_key: tensor, min_key: min_value, max_key: max_value}\n\n", "CodeBleu_score": 48.84227763118781}, {"code": "def from_float32_to_uint8(\n    tensor: tf.Tensor,\n    tensor_key: str = \"tensor\",\n    min_key: str = \"min\",\n    max_key: str = \"max\",\n) -> tf.Tensor\n\t\n    min_value = tf.reduce_min(tensor)\n    max_value = tf.reduce_max(tensor)\n    scaled_tensor = tf.image.convert_image_dtype(tensor, dtype=tf.uint8, saturate=True)\n    return {tensor_key: scaled_tensor, min_key: min_value, max_key: max_value}\n\n", "CodeBleu_score": 36.06280393255852}, {"code": "def from_float32_to_uint8(\n    tensor: tf.Tensor,\n    tensor_key: str = \"tensor\",\n    min_key: str = \"min\",\n    max_key: str = \"max\",\n) -> tf.Tensor\n\t\n    min_val = tf.reduce_min(tensor)\n    max_val = tf.reduce_max(tensor)\n    tensor = tf.cast(tensor, tf.float32)\n    tensor = (tensor - min_val) / (max_val - min_val)\n    tensor = tf.cast(tensor * 255, tf.uint8)\n    return {tensor_key: tensor, min_key: min_val, max_key: max_val}\n\n", "CodeBleu_score": 49.11840645660475}]}
{"instance_id": "7f479e5de492", "generation_result": [{"code": "def pad_and_partition(tensor: tf.Tensor, segment_len: int) -> tf.Tensor\n\t\n    padding_len = (-len(tensor)) % segment_len\n    padded = tf.pad(tensor, [[0, padding_len]])\n    reshaped = tf.reshape(padded, [-1, segment_len])\n    return reshaped\n\n", "CodeBleu_score": 17.40279615524241}, {"code": "def pad_and_partition(tensor: tf.Tensor, segment_len: int) -> tf.Tensor\n\t\n    # Calculate the padding needed to make the tensor length a multiple of the segment length\n    padding = tf.math.floormod(tf.shape(tensor)[0], segment_len)\n    padding = tf.cond(\n        padding > 0,\n        lambda: segment_len - padding,\n        lambda: tf.constant(0, dtype=tf.int32),\n    )\n\n    # Pad the tensor\n    tensor = tf.pad(tensor, [[0, padding], [0, 0]])\n\n    # Reshape the tensor into segments\n    tensor = tf.reshape(tensor, [-1, segment_len, tensor.shape[1]])\n\n    return tensor\n\n", "CodeBleu_score": 31.50378087950394}, {"code": "def pad_and_partition(tensor: tf.Tensor, segment_len: int) -> tf.Tensor\n\t\n    padding_len = segment_len - tensor.shape[0] % segment_len\n    padded_tensor = tf.pad(tensor, [[0, padding_len]])\n    return tf.reshape(padded_tensor, [-1, segment_len])\n\n", "CodeBleu_score": 15.89189359827541}]}
{"instance_id": "1789fae7253f", "generation_result": [{"code": "def pad_and_reshape(instr_spec, frame_length, F) -> Any\n\t\n    pad_length = frame_length - (instr_spec.shape[1] % frame_length)\n    pad = F.zeros((instr_spec.shape[0], pad_length, instr_spec.shape[2]))\n    instr_spec = F.concat(instr_spec, pad, dim=1)\n    instr_spec = instr_spec.reshape(\n        instr_spec.shape[0] * instr_spec.shape[1], instr_spec.shape[2]\n    )\n    return instr_spec\n\n", "CodeBleu_score": 16.20627136558495}, {"code": "def pad_and_reshape(instr_spec, frame_length, F) -> Any\n\t\n    n_frames = instr_spec.shape[1]\n    n_padding = frame_length - (n_frames % frame_length)\n    extension = F.zeros((instr_spec.shape[0], n_padding))\n    instr_spec = F.concat(instr_spec, extension, dim=1)\n    instr_spec = instr_spec.reshape((-1, instr_spec.shape[1] // frame_length, frame_length))\n    return instr_spec\n\n", "CodeBleu_score": 16.326913484038734}, {"code": "def pad_and_reshape(instr_spec, frame_length, F) -> Any\n\t\n    n_freq = F.shape[1]\n    n_frames = F.shape[2]\n    n_extra_frames = frame_length - n_frames % frame_length\n    if n_extra_frames > 0:\n        F_pad = F.new_zeros((F.shape[0], n_freq, n_extra_frames))\n        F = torch.cat([F, F_pad], dim=2)\n    F = F.view(-1, n_freq * frame_length)\n    return F\n\n", "CodeBleu_score": 21.801275750445292}]}
{"instance_id": "8a4f4658d97c", "generation_result": [{"code": "def dataset_from_csv(csv_path: str, **kwargs) -> Any\n\t\n    import pandas as pd\n    import tensorflow as tf\n\n    df = pd.read_csv(csv_path)\n    tensor_slices = {key: tf.convert_to_tensor(value.values) for key, value in df.items()}\n    return tf.data.Dataset.from_tensor_slices(tensor_slices, **kwargs)\n\n", "CodeBleu_score": 30.869249932431085}, {"code": "def dataset_from_csv(csv_path: str, **kwargs) -> Any\n\t\n    import pandas as pd\n    import tensorflow as tf\n\n    df = pd.read_csv(csv_path, **kwargs)\n    tensor_values = [tf.convert_to_tensor(df[col].values) for col in df.columns]\n    dataset = tf.data.Dataset.from_tensor_slices(tuple(tensor_values))\n    return dataset\n\n", "CodeBleu_score": 32.92275252832187}, {"code": "def dataset_from_csv(csv_path: str, **kwargs) -> Any\n\t\n    import pandas as pd\n    import tensorflow as tf\n\n    df = pd.read_csv(csv_path)\n    dataset = tf.data.Dataset.from_tensor_slices(\n        {key: df[key].values for key in df.keys()}, **kwargs\n    )\n    return dataset\n\n", "CodeBleu_score": 39.707678841234724}]}
{"instance_id": "9a16e81b182f", "generation_result": [{"code": "def check_tensor_shape(tensor_tf: tf.Tensor, target_shape: Any) -> bool\n\t\n    result = True\n    for i, dim in enumerate(target_shape):\n        if dim is not None:\n            result = result and (tensor_tf.shape[i] == dim)\n    return result\n\n", "CodeBleu_score": 31.182997668408653}, {"code": "def check_tensor_shape(tensor_tf: tf.Tensor, target_shape: Any) -> bool\n\t\n    result = True\n    for i, length in enumerate(target_shape):\n        if length is not None:\n            result &= tensor_tf.shape[i] == length\n    return result\n\n", "CodeBleu_score": 30.88674131158809}, {"code": "def check_tensor_shape(tensor_tf: tf.Tensor, target_shape: Any) -> bool\n\t\n    result = True\n    for i, length in enumerate(target_shape):\n        if length is not None:\n            result &= tensor_tf.shape[i] == length\n    return result\n\n", "CodeBleu_score": 30.88674131158809}]}
{"instance_id": "49d5d3084b3a", "generation_result": [{"code": "def get_training_dataset(\n    audio_params: Dict, audio_adapter: AudioAdapter, audio_path: str\n) -> Any\n\t\n    builder = DatasetBuilder(\n        audio_params,\n        audio_adapter,\n        train_csv=audio_path,\n        cache_directory=\"cache\",\n        batch_size=16,\n        num_chunks_per_song=10,\n        options={\"data_augmentation\": True, \"cache\": True},\n    )\n    return builder.build(\n        chunk_duration=1.0, random_seed=42,\n    )\n\n", "CodeBleu_score": 30.964227128367373}, {"code": "def get_training_dataset(\n    audio_params: Dict, audio_adapter: AudioAdapter, audio_path: str\n) -> Any\n\t\n\n    builder = DatasetBuilder(\n        audio_params=audio_params,\n        audio_adapter=audio_adapter,\n        dataset_path=audio_path,\n        train_csv_path=None,\n        cache_dir=None,\n        batch_size=None,\n        num_chunks_per_song=None,\n        num_chunks_per_song_in_training=None,\n        num_chunks_per_song_in_validation=None,\n        num_chunks_per_song_in_test=None,\n        augment_data=None,\n        cache_dataset=None,\n    )\n\n    return builder.build(\n        train_csv_path=None,\n        cache_dir=None,\n        batch_size=None,\n        num_chunks_per_song=None,\n        num_chunks_per_song_in_training=None,\n        num_chunks_per_song_in_validation=None,\n        num_chunks_per_song_in_test=None,\n        augment_data=None,\n        cache_dataset=None,\n    )\n\n", "CodeBleu_score": 31.724307790342653}, {"code": "def get_training_dataset(\n    audio_params: Dict, audio_adapter: AudioAdapter, audio_path: str\n) -> Any\n\t\n    builder = DatasetBuilder(\n        audio_params,\n        audio_adapter,\n        audio_path,\n        chunk_duration=1.0,\n        random_seed=42,\n    )\n    train_csv = f\"{audio_path}{SEPARATOR}train.csv\"\n    cache_dir = f\"{audio_path}{SEPARATOR}cache\"\n    return builder.build(\n        train_csv,\n        cache_dir,\n        batch_size=16,\n        num_chunks_per_song=1,\n        data_augmentation=True,\n        cache=True,\n    )\n\n", "CodeBleu_score": 38.499763482103575}]}
{"instance_id": "fe8bbbba06e3", "generation_result": [{"code": "def instruments(self) -> Any\n\t\n    if not self.instrument_builders:\n        for instrument in self.instruments:\n            self.instrument_builders.append(\n                InstrumentDatasetBuilder(self.dataset_path, instrument)\n            )\n    for builder in self.instrument_builders:\n        yield builder\n\n", "CodeBleu_score": 46.91062669030404}, {"code": "def instruments(self) -> Any\n\t\n    if self.instrument_builders is None:\n        self.instrument_builders = [\n            InstrumentDatasetBuilder(instrument, self.data_path)\n            for instrument in self.instruments\n        ]\n    for builder in self.instrument_builders:\n        yield builder\n\n", "CodeBleu_score": 35.57076775683239}, {"code": "def instruments(self) -> Any\n\t\n    for instrument in self.instruments:\n        if instrument not in self.instrument_builders:\n            self.instrument_builders[instrument] = InstrumentDatasetBuilder(\n                self.instrument_paths[instrument],\n                self.instrument_metadata[instrument],\n                self.instrument_metadata_path,\n                self.instrument_metadata_fields,\n                self.instrument_metadata_fields_types,\n                self.instrument_metadata_fields_descriptions,\n                self.instrument_metadata_fields_units,\n                self.instrument_metadata_fields_missing_values,\n                self.instrument_metadata_fields_fill_values,\n                self.instrument_metadata_fields_encoding,\n                self.instrument_metadata_fields_errors,\n                self.instrument_metadata_fields_warnings,\n                self.instrument_metadata_fields_notes,\n                self.instrument_metadata_fields_examples,\n                self.instrument_metadata_fields_links,\n                self.instrument_metadata_fields_format,\n                self.instrument_metadata_fields_pattern,\n                self.instrument_metadata_fields_min_length,\n                self.instrument_metadata_fields_max_length,\n                self.instrument_metadata_fields_minimum,\n                self.instrument_metadata_fields_maximum,\n                self.instrument_metadata_fields_exclusive_minimum,\n                self.instrument_metadata_fields_exclusive_maximum,\n                self.instrument_metadata_fields_multiple_of,\n                self.instrument_metadata_fields_enum,\n                self.instrument_metadata_fields_const,\n                self.instrument_metadata_fields_default,\n                self.instrument_metadata_fields_nullable,\n                self.instrument_metadata_fields_read_only,\n                self.instrument_metadata_fields_write_only,\n                self.instrument_metadata_fields_hidden,\n                self.instrument_metadata_fields_required,\n                self.instrument_metadata_fields_deprecated,\n                self.instrument_metadata_fields_allow_empty_", "CodeBleu_score": 15.87856269689681}]}
{"instance_id": "438b57fd3413", "generation_result": [{"code": "def cache(self, dataset: Any, cache: Optional[str], wait: bool) -> Any\n\t\n    if cache:\n        if wait:\n            while not exists(cache):\n                print(\"Waiting for cache to be created...\")\n                time.sleep(10)\n        else:\n            if not exists(cache):\n                print(\"Cache not found\")\n                return dataset\n        os.makedirs(os.path.dirname(cache), exist_ok=True)\n        dataset.to_parquet(cache)\n        return dataset\n    return dataset\n\n", "CodeBleu_score": 43.167158972101674}, {"code": "def cache(self, dataset: Any, cache: Optional[str], wait: bool) -> Any\n\t\n    if cache is not None:\n        cache_path = cache + SEPARATOR + \"index.json\"\n        if wait:\n            while not exists(cache_path):\n                print(\"Waiting for cache to be created...\")\n                time.sleep(10)\n        else:\n            if not exists(cache_path):\n                raise Exception(\"Cache not found\")\n        os.makedirs(cache, exist_ok=True)\n        dataset = dataset.cache(cache)\n    return dataset\n\n", "CodeBleu_score": 53.0671806526754}, {"code": "def cache(self, dataset: Any, cache: Optional[str], wait: bool) -> Any\n\t\n    if cache is None:\n        return dataset\n\n    cache_path = cache\n    if not cache_path.endswith(\".cache\"):\n        cache_path += \".cache\"\n    if not cache_path.startswith(SEPARATOR):\n        cache_path = os.path.join(os.getcwd(), cache_path)\n\n    if wait:\n        while not exists(cache_path):\n            print(f\"Cache file {cache_path} not found, waiting...\")\n            time.sleep(10)\n\n    os.makedirs(os.path.dirname(cache_path), exist_ok=True)\n    dataset.save_to_disk(cache_path)\n    return dataset\n\n", "CodeBleu_score": 50.688298548906985}]}
{"instance_id": "8838a3a09a66", "generation_result": [{"code": "def create_estimator(params: Dict, MWF: bool) -> tf.Tensor\n\t\n    model_dir = ModelProvider.get_model_dir(params[\"model_name\"])\n    params.update(\n        {\n            \"model_dir\": model_dir,\n            \"model_name\": params[\"model_name\"],\n            \"MWF\": MWF,\n        }\n    )\n\n    session_config = tf.compat.v1.ConfigProto()\n    session_config.gpu_options.allow_growth = True\n    session_config.gpu_options.per_process_gpu_memory_fraction = 0.45\n    config = tf.estimator.RunConfig(\n        session_config=session_config,\n        save_checkpoints_steps=params[\"save_checkpoints_steps\"],\n        keep_checkpoint_max=params[\"keep_checkpoint_max\"],\n    )\n\n    return tf.estimator.Estimator(\n        model_fn=model_fn,\n        model_dir=model_dir,\n        params=params,\n        config=config,\n    )\n\n", "CodeBleu_score": 29.644630983687236}, {"code": "def create_estimator(params: Dict, MWF: bool) -> tf.Tensor\n\t\n    model_dir = ModelProvider.get_model_dir(params[\"model_name\"])\n    params[\"model_dir\"] = model_dir\n    params[\"MWF\"] = MWF\n\n    session_config = tf.ConfigProto()\n    session_config.gpu_options.allow_growth = True\n    session_config.gpu_options.per_process_gpu_memory_fraction = 0.45\n\n    config = tf.estimator.RunConfig(\n        save_summary_steps=params[\"save_summary_steps\"],\n        save_checkpoints_steps=params[\"save_checkpoints_steps\"],\n        keep_checkpoint_max=params[\"keep_checkpoint_max\"],\n        log_step_count_steps=params[\"log_step_count_steps\"],\n        session_config=session_config,\n    )\n\n    return tf.estimator.Estimator(\n        model_fn=model_fn, model_dir=model_dir, params=params, config=config\n    )\n\n", "CodeBleu_score": 38.27514823060994}, {"code": "def create_estimator(params: Dict, MWF: bool) -> tf.Tensor\n\t\n    model_dir = ModelProvider.get_model_dir(params[\"model_name\"])\n    params[\"model_dir\"] = model_dir\n    params[\"MWF\"] = MWF\n\n    session_config = tf.ConfigProto()\n    session_config.gpu_options.per_process_gpu_memory_fraction = 0.45\n\n    config = tf.estimator.RunConfig(\n        save_summary_steps=params[\"save_summary_steps\"],\n        save_checkpoints_steps=params[\"save_checkpoints_steps\"],\n        keep_checkpoint_max=params[\"keep_checkpoint_max\"],\n        log_step_count_steps=params[\"log_step_count_steps\"],\n        session_config=session_config,\n    )\n\n    estimator = tf.estimator.Estimator(\n        model_fn=model_fn,\n        model_dir=model_dir,\n        params=params,\n        config=config,\n    )\n\n    return estimator\n\n", "CodeBleu_score": 40.12828066203002}]}
{"instance_id": "779c812bf2cb", "generation_result": [{"code": "def _separate_tensorflow(\n        self, waveform: np.ndarray, audio_descriptor: AudioDescriptor\n    ) -> Dict\n\t\n    # Ensure the input waveform is stereo\n    if len(waveform.shape) == 1:\n        waveform = np.expand_dims(waveform, axis=1)\n    elif waveform.shape[1] == 1:\n        waveform = np.repeat(waveform, 2, axis=1)\n    # Obtain a prediction generator\n    prediction_generator = self.prediction_setup.get_prediction_generator()\n    # Update the data generator with the waveform and audio descriptor\n    prediction_generator.data_generator.update_data(\n        waveform=waveform, audio_descriptor=audio_descriptor\n    )\n    # Generate a prediction\n    prediction = next(prediction_generator)\n    # Remove the \"audio_id\" key from the prediction\n    prediction.pop(\"audio_id\")\n    return prediction\n\n", "CodeBleu_score": 57.92005145888124}, {"code": "def _separate_tensorflow(\n        self, waveform: np.ndarray, audio_descriptor: AudioDescriptor\n    ) -> Dict\n\t\n    if waveform.ndim == 1:\n        waveform = np.expand_dims(waveform, axis=1)\n    if waveform.shape[1] == 1:\n        waveform = np.repeat(waveform, 2, axis=1)\n    prediction_generator = self.prediction_generator\n    prediction_generator.update(waveform=waveform, audio_descriptor=audio_descriptor)\n    prediction = next(prediction_generator)\n    prediction.pop(\"audio_id\")\n    return prediction\n\n", "CodeBleu_score": 58.527901563501004}, {"code": "def _separate_tensorflow(\n        self, waveform: np.ndarray, audio_descriptor: AudioDescriptor\n    ) -> Dict\n\t\n    waveform = self.ensure_stereo(waveform)\n    prediction_generator = self.get_prediction_generator()\n    prediction_generator.update(waveform=waveform, audio_descriptor=audio_descriptor)\n    prediction = next(prediction_generator)\n    prediction.pop(\"audio_id\")\n    return prediction\n\n", "CodeBleu_score": 44.30348932230991}]}
{"instance_id": "cf580cd2ed93", "generation_result": [{"code": "def compute_backbone_shapes(config, image_shape)\n\t\n    if callable(config.BACKBONE):\n        return config.COMPUTE_BACKBONE_SHAPE(image_shape)\n\n    # Currently supports ResNet only\n    assert config.BACKBONE in [\"resnet50\", \"resnet101\"]\n    return np.array(\n        [[int(math.ceil(image_shape[0] / stride)),\n            int(math.ceil(image_shape[1] / stride))]\n            for stride in config.BACKBONE_STRIDES])\n\n", "CodeBleu_score": 95.198357853076}, {"code": "def compute_backbone_shapes(config, image_shape)\n\t\n    if callable(config.BACKBONE):\n        return config.COMPUTE_BACKBONE_SHAPE(image_shape)\n\n    # Currently supports ResNet only\n    assert config.BACKBONE in [\"resnet50\", \"resnet101\"]\n    return np.array(\n        [[int(math.ceil(image_shape[0] / stride)),\n            int(math.ceil(image_shape[1] / stride))]\n            for stride in config.BACKBONE_STRIDES])\n\n", "CodeBleu_score": 95.198357853076}, {"code": "def compute_backbone_shapes(config, image_shape)\n\t\n    if callable(config.BACKBONE):\n        return config.COMPUTE_BACKBONE_SHAPE(image_shape)\n\n    # Currently supports ResNet only\n    assert config.BACKBONE in [\"resnet50\", \"resnet101\"]\n    return np.array(\n        [[int(math.ceil(image_shape[0] / stride)),\n            int(math.ceil(image_shape[1] / stride))]\n            for stride in config.BACKBONE_STRIDES])\n\n", "CodeBleu_score": 95.198357853076}]}
{"instance_id": "ebd61a2e59ca", "generation_result": [{"code": "def clip_boxes_graph(boxes, window)\n\t\n    wy1, wx1, wy2, wx2 = tf.split(window, 4)\n    y1, x1, y2, x2 = tf.split(boxes, 4, axis=1)\n    y1 = tf.maximum(tf.minimum(y1, wy2), wy1)\n    y2 = tf.maximum(tf.minimum(y2, wy2), wy1)\n    x1 = tf.maximum(tf.minimum(x1, wx2), wx1)\n    x2 = tf.maximum(tf.minimum(x2, wx2), wx1)\n    clipped = tf.concat([y1, x1, y2, x2], axis=1, name=\"clipped_boxes\")\n    clipped.set_shape((clipped.shape[0], 4))\n    return clipped\n\n", "CodeBleu_score": 59.804871054757946}, {"code": "def clip_boxes_graph(boxes, window)\n\t\n    boxes_shape = tf.shape(boxes)\n    boxes = tf.reshape(boxes, [-1, 4])\n    window_shape = tf.shape(window)\n    window = tf.reshape(window, [4, 1])\n    window = tf.tile(window, [1, boxes_shape[0]])\n    window = tf.transpose(window, [1, 0])\n    ymin = tf.maximum(tf.minimum(boxes[:, 0], window[:, 0]), window[:, 2])\n    xmin = tf.maximum(tf.minimum(boxes[:, 1], window[:, 1]), window[:, 3])\n    ymax = tf.minimum(tf.maximum(boxes[:, 2], window[:, 0]), window[:, 2])\n    xmax = tf.minimum(tf.maximum(boxes[:, 3], window[:, 1]), window[:, 3])\n    clipped = tf.concat([ymin, xmin, ymax, xmax], axis=1)\n    clipped = tf.reshape(clipped, boxes_shape)\n    return clipped\n\n", "CodeBleu_score": 14.718700867141186}, {"code": "def clip_boxes_graph(boxes, window)\n\t\n    boxes = tf.maximum(boxes, 0.0)\n    m = tf.tile(tf.reverse(window, [0]), [2])\n    n = tf.tile(tf.reverse([-1.0, -1.0], [0]), [2])\n    boxes = tf.minimum(boxes, m)\n    boxes = tf.maximum(boxes, n)\n    return boxes\n\n", "CodeBleu_score": 16.15729726426471}]}
{"instance_id": "8e0411587a70", "generation_result": [{"code": "def rpn_graph(feature_map, anchors_per_location, anchor_stride)\n\t\n\n    # Shared convolutional base of the RPN\n    shared = keras.layers.Conv2D(512, (3, 3), padding='same', activation='relu',\n                                strides=anchor_stride,\n                                name='rpn_conv_shared')(feature_map)\n\n    # Anchor Score. [batch, height, width, anchors per location * 2].\n    x = keras.layers.Conv2D(2 * anchors_per_location, (1, 1), padding='valid',\n                            activation='linear', name='rpn_class_raw')(shared)\n\n    # Reshape to [batch, anchors, 2]\n    rpn_class_logits = keras.layers.Lambda(\n        lambda t: tf.reshape(t, [tf.shape(t)[0], -1, 2]),\n        name='rpn_class_reshape')(x)\n\n    # Softmax on last dimension of BG/FG.\n    rpn_probs = keras.layers.Activation(\n        \"softmax\", name=\"rpn_class_xxx\")(rpn_class_logits)\n\n    # Bounding box refinement. [batch, H, W, anchors per location * depth]\n    # where depth is [x, y, log(w), log(h)]\n    x = keras.layers.Conv2D(anchors_per_location * 4, (1, 1), padding=\"valid\",\n                            activation='linear',\n                            name='rpn_bbox_pred')(shared)\n\n    # Reshape to [batch, anchors, 4]\n    rpn_bbox = keras.layers.Lambda(lambda t: tf.reshape(t, [tf.shape(t)[0], -1, 4]),\n                                   name='rpn_bbox_reshape')(x)\n\n    return rpn_class_logits, rpn_probs, rpn_bbox\n\n", "CodeBleu_score": 64.25434728972643}, {"code": "def rpn_graph(feature_map, anchors_per_location, anchor_stride)\n\t\n    # Shared convolutional base of the RPN\n    shared = keras.layers.Conv2D(512, (3, 3), padding='same', activation='relu',\n                                 kernel_initializer='normal', name='rpn_conv_shared')(feature_map)\n\n    # Anchor Score. [batch, height, width, anchors per location * 2].\n    x = keras.layers.Conv2D(2 * anchors_per_location, (1, 1), padding='valid',\n                            kernel_initializer='uniform', name='rpn_class_raw')(shared)\n\n    # Reshape to [batch, anchors, 2]\n    rpn_class_logits = keras.layers.Reshape((-1, 2), name=\"rpn_class_reshape\")(x)\n\n    # Softmax on last dimension of BG/FG.\n    rpn_probs = keras.layers.Activation(\n        \"softmax\", name=\"rpn_class_xxx\")(rpn_class_logits)\n\n    # Bounding box refinement. [batch, H, W, anchors per location * depth]\n    # where depth is [x, y, log(w), log(h)]\n    x = keras.layers.Conv2D(anchors_per_location * 4, (1, 1), padding=\"valid\",\n                            kernel_initializer='uniform',\n                            name=\"rpn_bbox_pred\")(shared)\n\n    # Reshape to [batch, anchors, 4]\n    rpn_bbox = keras.layers.Reshape((-1, 4), name=\"rpn_bbox_reshape\")(x)\n\n    return rpn_class_logits, rpn_probs, rpn_bbox\n\n", "CodeBleu_score": 49.24199640724607}, {"code": "def rpn_graph(feature_map, anchors_per_location, anchor_stride)\n\t\n    # Shared convolutional base of the RPN\n    shared = keras.layers.Conv2D(512, (3, 3), padding='same', activation='relu',\n                                strides=anchor_stride,\n                                name='rpn_conv_shared')(feature_map)\n\n    # Anchor Score. [batch, height, width, anchors per location * 2].\n    x = keras.layers.Conv2D(2 * anchors_per_location, (1, 1), padding='valid',\n                            name='rpn_class_raw')(shared)\n\n    # Reshape to [batch, anchors, 2]\n    rpn_class_logits = keras.layers.Lambda(\n        lambda t: tf.reshape(t, [tf.shape(t)[0], -1, 2]),\n        name='rpn_class_reshape')(x)\n\n    # Softmax on last dimension of BG/FG.\n    rpn_probs = keras.layers.Activation(\n        \"softmax\", name=\"rpn_class_xxx\")(rpn_class_logits)\n\n    # Bounding box refinement. [batch, H, W, anchors per location * depth]\n    # where depth is [x, y, log(w), log(h)]\n    x = keras.layers.Conv2D(anchors_per_location * 4, (1, 1), padding=\"valid\",\n                            name=\"rpn_bbox_pred\",\n                            activation='linear')(shared)\n\n    # Reshape to [batch, anchors, 4]\n    rpn_bbox = keras.layers.Lambda(lambda t: tf.reshape(t, [tf.shape(t)[0], -1, 4]),\n                                   name='rpn_bbox_reshape')(x)\n\n    return rpn_class_logits, rpn_probs, rpn_bbox\n\n", "CodeBleu_score": 60.62036093381735}]}
{"instance_id": "70777c9261d8", "generation_result": [{"code": "def smooth_l1_loss(y_true, y_pred)\n\t\n    absolute_difference = keras.backend.abs(y_true - y_pred)\n    mask = keras.backend.less(absolute_difference, 1.0)\n    square_loss = 0.5 * (absolute_difference ** 2)\n    linear_loss = absolute_difference - 0.5\n    loss = keras.backend.switch(mask, square_loss, linear_loss)\n    return keras.backend.mean(loss)\n\n", "CodeBleu_score": 19.71045512046973}, {"code": "def smooth_l1_loss(y_true, y_pred)\n\t\n    diff = keras.backend.abs(y_true - y_pred)\n    less_than_one = keras.backend.cast(keras.backend.less(diff, 1.0), \"float32\")\n    loss = (less_than_one * 0.5 * diff**2) + (1 - less_than_one) * (diff - 0.5)\n    return loss\n\n", "CodeBleu_score": 52.116200337812415}, {"code": "def smooth_l1_loss(y_true, y_pred)\n\t\n    absolute_loss = tf.abs(y_true - y_pred)\n    square_loss = 0.5 * (y_true - y_pred) ** 2\n    return tf.where(tf.less(absolute_loss, 1.0), square_loss, absolute_loss - 0.5)\n\n", "CodeBleu_score": 22.807413883185625}]}
{"instance_id": "0e747b604087", "generation_result": [{"code": "def build_model(x_train, num_classes)\n\t\n    import tensorflow as tf\n    from tensorflow.keras.models import Sequential\n    from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n\n    tf.keras.backend.clear_session()\n\n    model = Sequential()\n    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=x_train.shape[1:]))\n    model.add(Conv2D(64, (3, 3), activation='relu'))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Flatten())\n    model.add(Dense(128, activation='relu'))\n    model.add(Dense(num_classes, activation='softmax'))\n\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n    return model\n\n\n\n\n", "CodeBleu_score": 28.14955169741042}, {"code": "def build_model(x_train, num_classes)\n\t\n    model = tf.keras.Sequential([\n        tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=x_train.shape[1:]),\n        tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n        tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n        tf.keras.layers.Flatten(),\n        tf.keras.layers.Dense(128, activation='relu'),\n        tf.keras.layers.Dense(num_classes, activation='softmax')\n    ])\n    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n    return model\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 27.667668328424543}, {"code": "def build_model(x_train, num_classes)\n\t\n    import tensorflow as tf\n    from tensorflow.keras.models import Sequential\n    from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n\n    # Reset the TensorFlow graph\n    tf.keras.backend.clear_session()\n\n    # Define the input layer with the shape of the training data\n    input_shape = x_train.shape[1:]\n\n    # Create the model\n    model = Sequential()\n    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n    model.add(Conv2D(64, (3, 3), activation='relu'))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Flatten())\n    model.add(Dense(128, activation='relu'))\n    model.add(Dense(num_classes, activation='softmax'))\n\n    # Compile the model\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n    return model\n\n\n\n", "CodeBleu_score": 28.733742743136997}]}
{"instance_id": "813e3e2844f4", "generation_result": [{"code": "def extract_bboxes(mask)\n\t\n    boxes = np.zeros([mask.shape[-1], 4], dtype=np.int32)\n    for i in range(mask.shape[-1]):\n        m = mask[:, :, i]\n        horizontal_indicies = np.where(np.any(m, axis=0))[0]\n        vertical_indicies = np.where(np.any(m, axis=1))[0]\n        if horizontal_indicies.shape[0]:\n            x1, x2 = horizontal_indicies[[0, -1]]\n            y1, y2 = vertical_indicies[[0, -1]]\n            x2 += 1\n            y2 += 1\n        else:\n            x1, x2, y1, y2 = 0, 0, 0, 0\n        boxes[i] = np.array([y1, x1, y2, x2])\n    return boxes.astype(np.int32)\n\n", "CodeBleu_score": 79.98890816679622}, {"code": "def extract_bboxes(mask)\n\t\n    boxes = np.zeros([mask.shape[-1], 4], dtype=np.int32)\n    for i in range(mask.shape[-1]):\n        m = mask[:, :, i]\n        horizontal_indicies = np.where(np.any(m, axis=0))[0]\n        vertical_indicies = np.where(np.any(m, axis=1))[0]\n        if horizontal_indicies.shape[0]:\n            x1, x2 = horizontal_indicies[[0, -1]]\n            y1, y2 = vertical_indicies[[0, -1]]\n            x2 += 1\n            y2 += 1\n        else:\n            x1, x2, y1, y2 = 0, 0, 0, 0\n        boxes[i] = np.array([y1, x1, y2, x2])\n    return boxes.astype(np.int32)\n\n", "CodeBleu_score": 79.98890816679622}, {"code": "def extract_bboxes(mask)\n\t\n    boxes = np.zeros([mask.shape[-1], 4], dtype=np.int32)\n    for i in range(mask.shape[-1]):\n        m = mask[:, :, i]\n        horizontal_indicies = np.where(np.any(m, axis=0))[0]\n        vertical_indicies = np.where(np.any(m, axis=1))[0]\n        if horizontal_indicies.shape[0]:\n            x1, x2 = horizontal_indicies[[0, -1]]\n            y1, y2 = vertical_indicies[[0, -1]]\n            x2 += 1\n            y2 += 1\n        else:\n            x1, x2, y1, y2 = 0, 0, 0, 0\n        boxes[i] = np.array([y1, x1, y2, x2])\n    return boxes.astype(np.int32)\n\n", "CodeBleu_score": 79.98890816679622}]}
{"instance_id": "7ad5d430d6ed", "generation_result": [{"code": "def compute_iou(box, boxes, box_area, boxes_area)\n\t\n    y1 = np.maximum(box[0], boxes[:, 0])\n    y2 = np.minimum(box[2], boxes[:, 2])\n    x1 = np.maximum(box[1], boxes[:, 1])\n    x2 = np.minimum(box[3], boxes[:, 3])\n    intersection = np.maximum(x2 - x1, 0) * np.maximum(y2 - y1, 0)\n    union = box_area + boxes_area[:] - intersection[:]\n    iou = intersection / union\n    return iou\n\n", "CodeBleu_score": 58.663259083089194}, {"code": "def compute_iou(box, boxes, box_area, boxes_area)\n\t\n    y1 = np.maximum(box[0], boxes[:, 0])\n    y2 = np.minimum(box[2], boxes[:, 2])\n    x1 = np.maximum(box[1], boxes[:, 1])\n    x2 = np.minimum(box[3], boxes[:, 3])\n    intersection = np.maximum(x2 - x1, 0) * np.maximum(y2 - y1, 0)\n    union = box_area + boxes_area[:] - intersection[:]\n    iou = intersection / union\n    return iou\n\n", "CodeBleu_score": 58.663259083089194}, {"code": "def compute_iou(box, boxes, box_area, boxes_area)\n\t\n    y1 = np.maximum(box[0], boxes[:, 0])\n    y2 = np.minimum(box[2], boxes[:, 2])\n    x1 = np.maximum(box[1], boxes[:, 1])\n    x2 = np.minimum(box[3], boxes[:, 3])\n    intersection = np.maximum(x2 - x1, 0) * np.maximum(y2 - y1, 0)\n    union = box_area + boxes_area[:] - intersection[:]\n    iou = intersection / union\n    return iou\n\n", "CodeBleu_score": 58.663259083089194}]}
{"instance_id": "15941b852859", "generation_result": [{"code": "def compute_overlaps(boxes1, boxes2)\n\t\n    # determine the (x, y)-coordinates of the intersection rectangle\n    xA = np.maximum(boxes1[:, 0], boxes2[:, 0])\n    yA = np.maximum(boxes1[:, 1], boxes2[:, 1])\n    xB = np.minimum(boxes1[:, 2], boxes2[:, 2])\n    yB = np.minimum(boxes1[:, 3], boxes2[:, 3])\n\n    # compute the area of intersection rectangle\n    interArea = np.maximum(0, xB - xA + 1) * np.maximum(0, yB - yA + 1)\n\n    # compute the area of both the prediction and ground-truth\n    # rectangles\n    boxAArea = (boxes1[:, 2] - boxes1[:, 0] + 1) * (boxes1[:, 3] - boxes1[:, 1] + 1)\n    boxBArea = (boxes2[:, 2] - boxes2[:, 0] + 1) * (boxes2[:, 3] - boxes2[:, 1] + 1)\n\n    # compute the intersection over union by taking the intersection\n    # area and dividing it by the sum of prediction + ground-truth\n    # areas - the interesection area\n    iou = interArea / (boxAArea + boxBArea[:, np.newaxis] - interArea)\n\n    # return the intersection over union value\n    return iou\n\n", "CodeBleu_score": 31.363262777761943}, {"code": "def compute_overlaps(boxes1, boxes2)\n\t\n    area1 = (boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1])\n    area2 = (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1])\n\n    overlaps = np.zeros((boxes1.shape[0], boxes2.shape[0]))\n    for i in range(overlaps.shape[1]):\n        box2 = boxes2[i]\n        overlaps[:, i] = compute_iou(box2, boxes1, area2[i], area1)\n    return overlaps\n\n", "CodeBleu_score": 62.2853282032944}, {"code": "def compute_overlaps(boxes1, boxes2)\n\t\n    # determine the (x, y)-coordinates of the intersection rectangle\n    xA = np.maximum(boxes1[0], boxes2[:, 0])\n    yA = np.maximum(boxes1[1], boxes2[:, 1])\n    xB = np.minimum(boxes1[2], boxes2[:, 2])\n    yB = np.minimum(boxes1[3], boxes2[:, 3])\n    # compute the area of intersection rectangle\n    interArea = np.maximum(0, xB - xA + 1) * np.maximum(0, yB - yA + 1)\n    # compute the area of both the prediction and ground-truth\n    # rectangles\n    boxAArea = (boxes1[2] - boxes1[0] + 1) * (boxes1[3] - boxes1[1] + 1)\n    boxBArea = (boxes2[:, 2] - boxes2[:, 0] + 1) * (boxes2[:, 3] - boxes2[:, 1] + 1)\n    # compute the intersection over union by taking the intersection\n    # area and dividing it by the sum of prediction + ground-truth\n    # areas - the interesection area\n    iou = interArea / (boxAArea + boxBArea - interArea)\n    # return the intersection over union value\n    return iou\n\n", "CodeBleu_score": 27.063282834705326}]}
{"instance_id": "597533722ce3", "generation_result": [{"code": "def load_image(self, image_id):\n\t\n    info = self.image_info[image_id]\n    image = skimage.io.imread(info['path'])\n    if image.ndim != 3:\n        image = skimage.color.gray2rgb(image)\n    if image.shape[-1] == 4:\n        image = image[..., :3]\n    return image\n", "CodeBleu_score": 83.73585905972088}, {"code": "def load_image(self, image_id):\n\t\n    image_info = self.image_info[image_id]\n    image = skimage.io.imread(image_info['path'])\n    if image.ndim != 3:\n        image = skimage.color.gray2rgb(image)\n    if image.shape[-1] == 4:\n        image = image[..., :3]\n    return image\n\n", "CodeBleu_score": 82.5836239509772}, {"code": "def load_image(self, image_id):\n\t\n    image_info = self.image_info[image_id]\n    image_path = image_info['path']\n    image = skimage.io.imread(image_path)\n    if image.ndim != 3:\n        logging.warning(\"Image {} has invalid number of channels.\".format(image_id))\n        return None\n    if image.shape[-1] == 4:\n        image = image[..., :3]\n    return image\n", "CodeBleu_score": 65.05394177084543}]}
{"instance_id": "9b852997ca05", "generation_result": [{"code": "def display_images(images, titles=None, cols=4, cmap=None, norm=None,\n                   interpolation=None)\n\t\n    assert((titles is None)or (len(images) == len(titles)))\n    n_images = len(images)\n    if titles is None: titles = ['Image (%d)' % i for i in range(1,n_images + 1)]\n    fig = plt.figure()\n    for n, (image, title) in enumerate(zip(images, titles)):\n        a = fig.add_subplot(cols, np.ceil(n_images/float(cols)), n + 1)\n        if image.ndim == 2:\n            plt.gray()\n        plt.imshow(image, cmap = cmap,\n                   norm = norm,\n                   interpolation = interpolation)\n        a.set_title(title)\n    fig.set_size_inches(np.array(fig.get_size_inches()) * n_images)\n    plt.show()\n\n", "CodeBleu_score": 35.57816330419089}, {"code": "def display_images(images, titles=None, cols=4, cmap=None, norm=None,\n                   interpolation=None)\n\t\n    assert((titles is None)or (len(images) == len(titles)))\n    n_images = len(images)\n    if titles is None: titles = ['Image (%d)' % i for i in range(1,n_images + 1)]\n    fig = plt.figure()\n    for n, (image, title) in enumerate(zip(images, titles)):\n        a = fig.add_subplot(cols, np.ceil(n_images/float(cols)), n + 1)\n        if image.ndim == 2:\n            plt.gray()\n        plt.imshow(image, cmap = cmap,\n                   norm = norm,\n                   interpolation=interpolation)\n        a.set_title(title)\n    fig.set_size_inches(np.array(fig.get_size_inches()) * n_images)\n    plt.show()\n\n", "CodeBleu_score": 35.73575103507427}, {"code": "def display_images(images, titles=None, cols=4, cmap=None, norm=None,\n                   interpolation=None)\n\t\n    assert((titles is None)or (len(images) == len(titles)))\n    n_images = len(images)\n    if titles is None: titles = ['Image (%d)' % i for i in range(1,n_images + 1)]\n    fig = plt.figure()\n    for n, (image, title) in enumerate(zip(images, titles)):\n        a = fig.add_subplot(cols, np.ceil(n_images/float(cols)), n + 1)\n        if image.ndim == 2:\n            plt.gray()\n        plt.imshow(image, cmap = cmap,\n                   norm = norm,\n                   interpolation = interpolation)\n        a.set_title(title)\n    fig.set_size_inches(np.array(fig.get_size_inches()) * n_images)\n    plt.show()\n", "CodeBleu_score": 35.57816330419089}]}
