{"instance_id": "1f9f4d85e825", "generation_result": [{"code": "def load_model_from_config(self, half_attention)\n\t\n    if self.cached_model_path:\n        if os.path.isfile(self.cached_model_path) and time.time() - os.path.getmtime(self.cached_model_path) < 60 * 60 * 24:\n            print(\"Loading from cache\")\n            self.model = safetensors.torch.load_file(self.cached_model_path, device=self.device)\n            self.model.eval()\n            return {\"model\": self.model}\n\n    print(\"Loading model from scratch\")\n    config = OmegaConf.load(self.config_path)\n    model = instantiate_from_config(config.model)\n    sd_hijack.model_hijack(model)\n    model.load_state_dict(torch.load(self.ckpt_path, map_location=\"cpu\"), strict=False)\n    model.to(self.device)\n    model.eval()\n    if half_attention:\n        model.half()\n    if devices.str2bool(self.memory_format_optimization):\n        model.to_memory_format()\n\n    if self.cached_model_path:\n        print(\"Caching model\")\n        safetensors.torch.save_file(model, self.cached_model_path)\n        gc.collect()\n        torch.cuda.empty_cache()\n\n    return {\"model\": model}\n\n", "CodeBleu_score": 37.54073953738125}, {"code": "def load_model_from_config(self, half_attention)\n\t\n    # check if cached model is available and valid\n    if self.config.model.use_cache and os.path.exists(self.config.model.cache_path):\n        # load cached model\n        self.model = torch.load(self.config.model.cache_path)\n        # check if cached model is valid\n        if not ismap(self.model):\n            # cached model is not valid, delete it\n            os.remove(self.config.model.cache_path)\n        else:\n            # cached model is valid, return it\n            return self.model\n\n    # load model from specified path\n    if self.config.model.path.endswith(\".safetensors\"):\n        # load model from .safetensors file\n        model = safetensors.torch.load_file(self.config.model.path, device=\"cpu\")\n    else:\n        # load model from other PyTorch-compatible format\n        model = torch.load(self.config.model.path, map_location=\"cpu\")\n\n    # configure model\n    model = instantiate_from_config(model.get(\"model\", model))\n\n    # move model to device\n    model.to(devices.device)\n\n    # set model to evaluation mode\n    model.eval()\n\n    # set half-precision attention\n    if half_attention:\n        model.enable_half_precision_attention()\n\n    # optimize memory format\n    if self.config.model.optimize_memory_format:\n        model.optimize_memory_format()\n\n    # cache model for future use\n    if self.config.model.use_cache:\n        torch.save(model, self.config.model.cache_path)\n\n    # return loaded model\n    return model\n\n", "CodeBleu_score": 32.74067433246334}, {"code": "def load_model_from_config(self, half_attention)\n\t\n\n    # Load model from cache if available\n    if self.model_cache and os.path.exists(self.model_cache):\n        print(\"Loading model from cache...\")\n        model = torch.load(self.model_cache, map_location=\"cpu\")\n        model = model.to(devices.device)\n        if not self.half_precision_first:\n            model.half()\n        model.eval()\n        return {\"model\": model}\n\n    # Load model from config\n    print(f\"Loading model from {self.model_path}...\")\n    config = OmegaConf.load(self.model_path)\n    model = instantiate_from_config(config.model)\n    model.load_state_dict(torch.load(self.model_path, map_location=\"cpu\"), strict=False)\n    model = model.to(devices.device)\n    if not self.half_precision_first:\n        model.half()\n    model.eval()\n\n    # Cache model if specified\n    if self.model_cache:\n        print(\"Caching model...\")\n        torch.save(model, self.model_cache)\n\n    return {\"model\": model}\n\n", "CodeBleu_score": 33.548070813254725}]}
{"instance_id": "8dc955007aa2", "generation_result": [{"code": "def convsample_ddim(model, cond, steps, shape, eta=1.0, callback=None, normals_sequence=None,\n                    mask=None, x0=None, quantize_x0=False, temperature=1., score_corrector=None,\n                    corrector_kwargs=None, x_t=None\n                    )\n\t\n\n    batch_size = shape[0]\n    print(f\"eta: {eta}, steps: {steps}\")\n    if callback is not None:\n        callback(0)\n    assert eta == 1.0 and steps == 50\n    model.to(devices.device)\n    sampler = DDIMSampler(model)\n    sampler.make_schedule(ddim_num_steps=steps, ddim_eta=eta, verbose=False)\n    x = x0 * eta if x0 is not None else None\n    img = sampler.sample(batch_size, shape, conditioning=cond, callback=callback, normals_sequence=normals_sequence,\n                         img=x, mask=mask, quantize_x0=quantize_x0,\n                         temperature=temperature,\n                         score_corrector=score_corrector,\n                         corrector_kwargs=corrector_kwargs,\n                         x_T=x_t\n                         )\n    return img\n\n", "CodeBleu_score": 52.02561200060482}, {"code": "def convsample_ddim(model, cond, steps, shape, eta=1.0, callback=None, normals_sequence=None,\n                    mask=None, x0=None, quantize_x0=False, temperature=1., score_corrector=None,\n                    corrector_kwargs=None, x_t=None\n                    )\n\t\n    model.to(devices.device)\n    model.eval()\n    batch_size = shape[0]\n    print(f\"eta: {eta}, steps: {steps}\")\n    if callback:\n        callback(0)\n    assert eta == 1.0 and steps == 50\n    sampler = DDIMSampler(model)\n    sampler.make_schedule(ddim_num_steps=steps, ddim_eta=eta, verbose=False)\n    x, _ = sampler.sample(batch_size, shape, cond,\n                          callback=callback, normals_sequence=normals_sequence,\n                          mask=mask, x0=x0, quantize_x0=quantize_x0,\n                          temperature=temperature,\n                          score_corrector=score_corrector,\n                          corrector_kwargs=corrector_kwargs,\n                          x_T=x_t)\n    return x\n\n", "CodeBleu_score": 51.11524375009917}, {"code": "def convsample_ddim(model, cond, steps, shape, eta=1.0, callback=None, normals_sequence=None,\n                    mask=None, x0=None, quantize_x0=False, temperature=1., score_corrector=None,\n                    corrector_kwargs=None, x_t=None\n                    )\n\t\n\n    batch_size = shape[0]\n    print(f\"sampling {batch_size} images\")\n    device = next(model.parameters()).device\n\n    if normals_sequence is not None:\n        normals_sequence = repeat(normals_sequence, \"n c -> b n c\", b=batch_size)\n\n    if mask is not None:\n        mask = repeat(mask, \"b ... -> b (repeat b) ...\", repeat=batch_size)\n\n    if x0 is not None:\n        x0 = repeat(x0, \"b ... -> b (repeat b) ...\", repeat=batch_size)\n\n    if quantize_x0:\n        x0 = torch.round(x0)\n\n    if score_corrector is not None:\n        assert isinstance(\n            score_corrector, str), \"score_corrector must be a str or None\"\n\n    if corrector_kwargs is None:\n        corrector_kwargs = {}\n\n    if callback is None:\n        callback = lambda args: None\n\n    if x_t is None:\n        x_t = torch.randn(\n            (batch_size, model.channels, *model.image_size), device=device)\n\n    sampler = DDIMSampler(model)\n    return sampler.sample(\n        S=steps,\n        conditioning=cond,\n        batch_size=batch_size,\n        shape=shape,\n        verbose=False,\n        eta=eta,\n        x_T=x_t,\n        callback=callback,\n        normals_sequence=normals_sequence,\n        mask=mask,\n        x0=x0,\n        quantize_x0=quantize_x0,\n        temperature=temperature,\n        score_corrector=score_corrector,\n        corrector_kwargs=corrector_kwargs,\n    )\n\n", "CodeBleu_score": 47.05188526751882}]}
{"instance_id": "9bdcd9081480", "generation_result": [{"code": "def ema_scope(self, context=None)\n\t\n    if self.ema_enabled:\n        if context is not None:\n            print(f\"\n\n", "CodeBleu_score": 7.950674595529536}, {"code": "def ema_scope(self, context=None)\n\t\n    if self.ema_enabled:\n        if context:\n            print(f\"{context} -> EMA\")\n        self.ema_model.store(self.model)\n        self.ema_model.copy_to(self.model)\n        if context:\n            print(f\"EMA -> {context}\")\n    yield\n    if self.ema_enabled:\n        self.ema_model.restore(self.model)\n        if context:\n            print(f\"{context} -> EMA\")\n\n", "CodeBleu_score": 52.15866447637336}, {"code": "def ema_scope(self, context=None)\n\t\n    if not self.ema_enabled:\n        yield\n        return\n    if context is not None:\n        print(f\"", "CodeBleu_score": 12.21790750078505}]}
{"instance_id": "14ab009e1c5c", "generation_result": [{"code": "def init_from_ckpt(self, path, ignore_keys=None)\n\t\n    if version.parse(torch.__version__) >= version.parse(\"1.6.0\"):\n        state_dict = torch.load(path, map_location=\"cpu\")\n    else:\n        state_dict = torch.load(path, map_location=lambda storage, loc: storage)\n    if ignore_keys is not None:\n        for key in ignore_keys:\n            state_dict.pop(key, None)\n    self.load_state_dict(state_dict, strict=False)\n\n", "CodeBleu_score": 22.573954712422832}, {"code": "def init_from_ckpt(self, path, ignore_keys=None)\n\t\n    sd = torch.load(path, map_location=\"cpu\")[\"state_dict\"]\n    if ignore_keys is not None:\n        for k in ignore_keys:\n            sd.pop(k)\n    self.load_state_dict(sd, strict=False)\n\n", "CodeBleu_score": 13.17724393611101}, {"code": "def init_from_ckpt(self, path, ignore_keys=None)\n\t\n    state_dict = torch.load(path, map_location=\"cpu\")\n    if ignore_keys is not None:\n        state_dict = {k: v for k, v in state_dict.items() if k not in ignore_keys}\n        print(f\"Ignored keys: {ignore_keys}\")\n    self.load_state_dict(state_dict, strict=False)\n\n", "CodeBleu_score": 22.298178436782216}]}
{"instance_id": "754176e27b81", "generation_result": [{"code": "def forward(self, input, return_pred_indices=False)\n\t\n    # Encode input\n    z, pred_indices, indices = self.encoder(input)\n\n    # Decode z\n    decoded = self.decoder(z)\n\n    # Compute difference metric\n    diff_metric = self.diff_metric(decoded, input)\n\n    if return_pred_indices:\n        return decoded, diff_metric, pred_indices\n    return decoded, diff_metric\n\n", "CodeBleu_score": 25.280803682898}, {"code": "def forward(self, input, return_pred_indices=False)\n\t\n    quant, diff, _, _ = self.encode(input)\n    dec = self.decode(quant)\n    if return_pred_indices:\n        return dec, diff, quant\n    return dec, diff\n\n", "CodeBleu_score": 47.70820235609956}, {"code": "def forward(self, input, return_pred_indices=False)\n\t\n    # Encode\n    z, pred_indices, _ = self.encoder(input)\n    # Decode\n    output = self.decoder(z)\n    # Calculate difference metric\n    if return_pred_indices:\n        return output, pred_indices\n    else:\n        return output\n\n", "CodeBleu_score": 26.975157624096695}]}
{"instance_id": "443df6fdaf0d", "generation_result": [{"code": "def get_input(self, batch, k)\n\t\n    x = batch[k]\n    if len(x.shape) == 2:\n        x = x[:, None, :, :]\n    x = x.permute(0, 3, 1, 2).contiguous().float()\n    if self.enable_resizing:\n        x = self.resize_input(x, self.train_step)\n    x = x.detach()\n    return x\n\n", "CodeBleu_score": 26.532789886362867}, {"code": "def get_input(self, batch, k)\n\t\n    x = batch[k]\n    if len(x.shape) == 2:\n        x = x[:, None, :, :]\n    if len(x.shape) == 3:\n        x = x[:, :, None, :]\n    if x.dtype == torch.float16:\n        x = x.float()\n    if self.training and self.batch_size_fn is not None:\n        x = self.batch_size_fn(x, self.global_step)\n    if self.training and self.random_resize:\n        x = self.random_resize(x)\n    x = x.permute(0, 3, 1, 2)\n    x = x.contiguous()\n    if not self.training:\n        x = x.to(torch.float32)\n    x = x.detach()\n    return x\n\n", "CodeBleu_score": 43.56781441453318}, {"code": "def get_input(self, batch, k)\n\t\n    x = batch[k]\n    if len(x.shape) == 3:\n        x = x[..., None]\n    if x.shape[-1] == 1:\n        x = x.repeat(1, 1, 1, 3)\n    x = x.permute(0, 3, 1, 2).to(torch.float32)\n    if self.training and self.resize_mode is not None:\n        x = self.resize(x, self.training_step, self.resize_range)\n    x = x.contiguous()\n    return x.detach()\n\n", "CodeBleu_score": 36.244656924664156}]}
{"instance_id": "953a2632fc15", "generation_result": [{"code": "def training_step(self, batch, batch_idx, optimizer_idx)\n\t\n    x, _ = batch\n    x = x.to(self.device)\n    optimizer = self.optimizers()\n    optimizer_idx = optimizer_idx if optimizer_idx is not None else 0\n    optimizer = optimizer[optimizer_idx]\n    if optimizer_idx == 0:\n        # autoencoder\n        z = self.encoder(x)\n        x_tilde = self.decoder(z)\n        loss = self.loss_fn(x_tilde, x)\n    elif optimizer_idx == 1:\n        # discriminator\n        z = self.encoder(x)\n        x_tilde = self.decoder(z)\n        real = torch.ones(x.shape[0], 1).to(self.device)\n        fake = torch.zeros(x.shape[0], 1).to(self.device)\n        loss = self.loss_fn(x_tilde, x, real, fake)\n    else:\n        raise ValueError(f\"Optimizer index {optimizer_idx} is not valid\")\n\n    self.log_dict(\n        {\n            \"train/loss\": loss,\n            \"train/lr\": optimizer.param_groups[0][\"lr\"],\n        },\n        prog_bar=True,\n        logger=True,\n    )\n\n    return loss\n\n", "CodeBleu_score": 30.5934115011261}, {"code": "def training_step(self, batch, batch_idx, optimizer_idx)\n\t\n    x, y = batch\n    y = y.squeeze()\n\n    # Autoencoder\n    if optimizer_idx == 0:\n        # Encode\n        z = self.encoder(x)\n\n        # Decode\n        x_hat = self.decoder(z)\n\n        # Compute autoencoding loss\n        loss = self.loss_fn(x_hat, x)\n\n        # Logging\n        self.log(\"train/loss\", loss, on_step=True, on_epoch=True)\n\n        return loss\n\n    # Discriminator\n    if optimizer_idx == 1:\n        # Encode\n        z = self.encoder(x)\n\n        # Compute discriminator loss\n        loss = self.discriminator_loss(z, y)\n\n        # Logging\n        self.log(\"train/discriminator_loss\", loss, on_step=True, on_epoch=True)\n\n        return loss\n\n", "CodeBleu_score": 18.209926026273838}, {"code": "def training_step(self, batch, batch_idx, optimizer_idx)\n\t\n    x = batch[\"image\"]\n    x_rec, z_q, z_q_x = self.forward(x)\n    loss_autoencoder = self.loss_fn(x_rec, x, z_q, z_q_x)\n    self.log_dict({\"train/loss_autoencoder\": loss_autoencoder}, prog_bar=True)\n    if optimizer_idx == 0:\n        return loss_autoencoder\n    else:\n        return self.loss_discriminator(x, z_q)\n\n", "CodeBleu_score": 14.294797640301107}]}
{"instance_id": "b449b5e8b77f", "generation_result": [{"code": "def log_images(self, batch, only_inputs=False, plot_ema=False, **kwargs)\n\t\n    if not self.global_step % self.log_every_n_steps:\n        self.logger.experiment.log_image(\n            self.global_step,\n            \"inputs\",\n            self.make_grid(batch[\"image\"], nrow=self.log_image_samples),\n            self.global_step,\n        )\n        if not only_inputs:\n            self.logger.experiment.log_image(\n                self.global_step,\n                \"reconstructions\",\n                self.make_grid(\n                    self.reconstruction_loss(batch, return_images=True),\n                    nrow=self.log_image_samples,\n                ),\n                self.global_step,\n            )\n            if plot_ema:\n                self.logger.experiment.log_image(\n                    self.global_step,\n                    \"reconstructions_ema\",\n                    self.make_grid(\n                        self.reconstruction_loss(batch, return_images=True, ema=True),\n                        nrow=self.log_image_samples,\n                    ),\n                    self.global_step,\n                )\n\n", "CodeBleu_score": 20.838327102199237}, {"code": "def log_images(self, batch, only_inputs=False, plot_ema=False, **kwargs)\n\t\n    x = batch['image']\n    if only_inputs:\n        self.logger.experiment.add_images('inputs', x, self.global_step)\n        return\n    if self.global_step % self.opt.train.log_every_n_steps == 0:\n        self.logger.experiment.add_images('inputs', x, self.global_step)\n        with torch.no_grad():\n            if plot_ema:\n                z = self.ema_model.encode(x)\n                x_recon = self.ema_model.decode(z)\n            else:\n                z = self.model.encode(x)\n                x_recon = self.model.decode(z)\n            self.logger.experiment.add_images('reconstructions', x_recon, self.global_step)\n\n", "CodeBleu_score": 28.62585250136609}, {"code": "def log_images(self, batch, only_inputs=False, plot_ema=False, **kwargs)\n\t\n    if not self.logger:\n        return\n    if only_inputs:\n        images = batch[\"input_image\"]\n    else:\n        with torch.no_grad():\n            if plot_ema:\n                self.ema.update_average(self.model.state_dict())\n                images = self.ema.ema_model.decode(\n                    self.ema.ema_model.encode(batch[\"input_image\"])\n                )\n            else:\n                images = self.model.decode(\n                    self.model.encode(batch[\"input_image\"])\n                )\n    images = images.detach().cpu()\n    images = images.permute(0, 2, 3, 1)\n    images = images.clamp(-1, 1)\n    images = (images + 1) / 2\n    images = images.mul(255).add_(0.5).clamp_(0, 255).to(torch.uint8)\n    images = images.permute(0, 3, 1, 2)\n    images = images.contiguous()\n    images = images.view(images.shape[0], -1, 3, 32, 32)\n    images = images.view(images.shape[0] * images.shape[1], 3, 32, 32)\n    images = images.view(images.shape[0], 3, -1)\n    images = images.view(images.shape[0], 3, 32, -1)\n    images = images.view(images.shape[0], 3, -1, 32)\n    images = images.view(images.shape[0], 3, 32, -1)\n    images = images.view(images.shape[0], 3, -1, 32)\n    images = images.view(images.shape[0], 3, 32, -1)\n    images = images.view(images.shape[0], 3, -1, 32)\n    images = images.view(images.shape[0], 3, 32, -1)\n", "CodeBleu_score": 25.72318134626502}]}
{"instance_id": "540d089e3481", "generation_result": [{"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss_type == 'l1':\n        if mean:\n            return torch.mean(torch.abs(pred - target))\n        else:\n            return torch.abs(pred - target)\n    elif self.loss_type == 'l2':\n        if mean:\n            return torch.mean((pred - target) ** 2)\n        else:\n            return (pred - target) ** 2\n    else:\n        raise ValueError(f\"Unknown loss type: {self.loss_type}\")\n\n", "CodeBleu_score": 33.94350215694051}, {"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss_type == \"l1\":\n        return F.l1_loss(pred, target, reduction=\"mean\" if mean else \"none\")\n    elif self.loss_type == \"l2\":\n        return F.mse_loss(pred, target, reduction=\"mean\" if mean else \"none\")\n    else:\n        raise ValueError(f\"Unknown loss type: {self.loss_type}\")\n\n", "CodeBleu_score": 26.070519396671465}, {"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss_type == 'l1':\n        return torch.mean(torch.abs(pred - target))\n    elif self.loss_type == 'l2':\n        if mean:\n            return torch.mean((pred - target) ** 2)\n        else:\n            return (pred - target) ** 2\n    else:\n        raise ValueError(f'Unknown loss type: {self.loss_type}')\n\n", "CodeBleu_score": 31.850011927163564}]}
{"instance_id": "63a08283666f", "generation_result": [{"code": "def forward(self, x, c, *args, **kwargs)\n\t\n    # Select a random time step for each element in the batch\n    ts = torch.randint(0, self.num_timesteps, (x.shape[0],), device=x.device)\n    # Sample noise to add to the inputs\n    noises = torch.randn_like(x)\n    # Apply learned conditioning if applicable\n    if self.learned_conditioning:\n        c = self.get_learned_conditioning(c)\n    # Process the inputs and conditioning\n    x_in = self.input_timestep_embedding(ts)\n    if self.learned_conditioning:\n        x_in = torch.cat([x_in, c], dim=-1)\n    # Forward pass through the model\n    x_out = self.model(x, x_in, *args, **kwargs)\n    # Compute the loss\n    loss = self.loss_fn(x, x_out, noises)\n    return loss\n\n", "CodeBleu_score": 40.557926530692775}, {"code": "def forward(self, x, c, *args, **kwargs)\n\t\n    if self.cond_stage_model is not None:\n        c = self.cond_stage_model(c)\n    return self.model.forward(x, c, *args, **kwargs)\n\n", "CodeBleu_score": 16.28694624687665}, {"code": "def forward(self, x, c, *args, **kwargs)\n\t\n    t = torch.randint(0, self.num_timesteps, (x.shape[0],), device=x.device).long()\n    return self.forward_with_timesteps(x, t, c, *args, **kwargs)\n\n", "CodeBleu_score": 17.92932235434506}]}
{"instance_id": "4627e1e8ed62", "generation_result": [{"code": "def get_codebook_entry(self, indices, shape)\n\t\n    if self.codebook_remap is not None:\n        indices = self.codebook_remap[indices]\n    shape = [shape[0], -1] + [dim for dim in shape[1:]]\n    return self.codebook[indices].view(shape)\n\n", "CodeBleu_score": 27.470505012849287}, {"code": "def get_codebook_entry(self, indices, shape)\n\t\n    if self.codebook_capacity is not None:\n        if self.remap_indices:\n            indices = self.remap_indices_fn(indices)\n        indices = indices.clamp(min=0, max=self.codebook_capacity - 1)\n    else:\n        indices = indices.clamp(min=0)\n    shape = list(shape)\n    if len(shape) == 2:\n        shape = [shape[0], shape[1], 1, 1]\n    elif len(shape) == 4:\n        shape = [shape[0], shape[1], shape[2], shape[3]]\n    elif len(shape) == 3:\n        shape = [shape[0], shape[1], shape[2], 1]\n    elif len(shape) == 1:\n        shape = [shape[0], 1, 1, 1]\n    else:\n        raise ValueError(f\"Unsupported shape {shape}\")\n    shape[1] = shape[1] * self.codebook_dim\n    return self.codebook(indices).view(*shape)\n\n", "CodeBleu_score": 40.483456806617966}, {"code": "def get_codebook_entry(self, indices, shape)\n\t\n    # shape spec: (H, W, C)\n    # for Hybrid-VQ: shape spec: (C, H, W)\n    if self.remap_indices:\n        indices = self.remap_fn(indices)\n\n    # get quantized latent vector\n    if self.codebook_dim is None:\n        codebook_entry = self.codebook[indices]\n    else:\n        codebook_entry = self.codebook[indices, :self.codebook_dim]\n\n    # reshape\n    if shape is not None:\n        if self.remap_indices:\n            # for Hybrid-VQ: (C, H, W) -> (H, W, C)\n            codebook_entry = rearrange(codebook_entry, \"(h w) c -> c h w\", h=shape[0], w=shape[1])\n        else:\n            # for Hybrid-VQ: (H, W, C) -> (C, H, W)\n            codebook_entry = rearrange(codebook_entry, \"(c h w) -> c h w\", c=shape[2], h=shape[0], w=shape[1])\n\n    return codebook_entry\n\n", "CodeBleu_score": 46.1309389293476}]}
{"instance_id": "c55d014827ef", "generation_result": [{"code": "def factorization(dimension: int, factor:int=-1) -> tuple[int, int]\n\t\n    if factor > 0 and dimension % factor == 0:\n        return factor, dimension // factor\n    elif factor > 0:\n        raise ValueError(f\"The dimension {dimension} is not divisible by the factor {factor}.\")\n    elif factor < 0:\n        for i in range(2, int(dimension**0.5)+1):\n            if dimension % i == 0:\n                m, n = i, dimension // i\n                if m <= n and n <= -factor:\n                    return m, n\n        raise ValueError(f\"No factorization found for dimension {dimension} with factor {factor}.\")\n    else:\n        for i in range(2, int(dimension**0.5)+1):\n            if dimension % i == 0:\n                m, n = i, dimension // i\n                if m <= n:\n                    return m, n\n        raise ValueError(f\"No factorization found for dimension {dimension}.\")\n\n", "CodeBleu_score": 35.995859233674175}, {"code": "def factorization(dimension: int, factor:int=-1) -> tuple[int, int]\n\t\n    if factor > 0:\n        if dimension % factor == 0:\n            return factor, dimension // factor\n        else:\n            raise ValueError(\"The specified factor does not divide the dimension.\")\n    else:\n        factors = []\n        for i in range(1, int(dimension**0.5)+1):\n            if dimension % i == 0:\n                factors.append((i, dimension // i))\n        factors.sort(key=lambda x: x[0] + x[1])\n        return factors[0]\n\n", "CodeBleu_score": 26.659459705650303}, {"code": "def factorization(dimension: int, factor:int=-1) -> tuple[int, int]\n\t\n    if factor > 0 and dimension % factor == 0:\n        m = factor\n        n = dimension // factor\n    else:\n        m, n = dimension, 1\n        while m > n:\n            if factor > 0 and m > factor:\n                m, n = m - 1, n + 1\n            else:\n                m, n = m - 1, n + 1\n                while m > n and m * n > dimension:\n                    m, n = m - 1, n + 1\n    return m, n\n\n", "CodeBleu_score": 43.350389808474354}]}
{"instance_id": "5d613ec26036", "generation_result": [{"code": "def apply_weight_decompose(self, updown, orig_weight)\n\t\n    # Apply weight decompose\n    if updown == \"up\":\n        weight = orig_weight.to(self.device)\n        weight = weight.type_as(self.weight)\n        weight = weight * self.scale_factor\n        weight = weight.to(self.device)\n        weight = weight.type_as(self.weight)\n        weight = weight.div(weight.norm(2))\n        weight = weight.mul(self.weight.norm(2))\n        weight = weight - self.weight\n    elif updown == \"down\":\n        weight = self.weight.to(self.device)\n        weight = weight.type_as(orig_weight)\n        weight = weight * self.scale_factor\n        weight = weight.to(self.device)\n        weight = weight.type_as(orig_weight)\n        weight = weight.div(weight.norm(2))\n        weight = weight.mul(orig_weight.norm(2))\n        weight = weight - orig_weight\n    else:\n        raise ValueError(\"Invalid updown argument\")\n\n    return weight\n\n", "CodeBleu_score": 30.015638758432782}, {"code": "def apply_weight_decompose(self, updown, orig_weight)\n\t\n    assert updown in ['up', 'down']\n    if updown == 'up':\n        weight = orig_weight\n        scale = self.upscale_factor\n    else:\n        weight = orig_weight.flip(2).flip(3)\n        scale = self.downscale_factor\n    weight = weight.to(self.device, self.dtype)\n    orig_weight = orig_weight.to(self.device, self.dtype)\n    weight = weight.unsqueeze(0)\n    weight = weight.expand(self.batch_size, -1, -1, -1)\n    weight = weight.contiguous()\n    weight = weight.view(self.batch_size, -1, weight.shape[2] * weight.shape[3])\n    orig_weight = orig_weight.view(self.batch_size, -1, orig_weight.shape[2] * orig_weight.shape[3])\n    weight = weight.transpose(1, 2)\n    orig_weight = orig_weight.transpose(1, 2)\n    weight = weight.contiguous()\n    orig_weight = orig_weight.contiguous()\n    weight = weight.view(self.batch_size, weight.shape[1], -1, self.upscale_factor, self.upscale_factor)\n    orig_weight = orig_weight.view(self.batch_size, orig_weight.shape[1], -1, self.upscale_factor, self.upscale_factor)\n    weight = weight.permute(0, 3, 4, 1, 2).contiguous()\n    orig_weight = orig_weight.permute(0, 3, 4, 1, 2).contiguous()\n    weight = weight.view(self.batch_size * self.upscale_factor * self.upscale_factor, -1, weight.shape[3])\n    orig_weight = orig_weight.view(self.batch_size * self.upscale_factor * self.upscale_factor, -1, orig_weight.shape[3])\n    weight = weight.transpose(1, 2).", "CodeBleu_score": 31.435384101484686}, {"code": "def apply_weight_decompose(self, updown, orig_weight)\n\t\n    scale_factor = self.scale_factor\n    if scale_factor is None:\n        return orig_weight\n\n    if updown:\n        if self.decompose_weight_updown is None:\n            self.decompose_weight_updown = orig_weight.clone()\n        else:\n            self.decompose_weight_updown = self.decompose_weight_updown + orig_weight\n    else:\n        if self.decompose_weight_downup is None:\n            self.decompose_weight_downup = orig_weight.clone()\n        else:\n            self.decompose_weight_downup = self.decompose_weight_downup + orig_weight\n\n    if updown:\n        self.decompose_weight_updown = self.decompose_weight_updown.to(orig_weight.device)\n        self.decompose_weight_updown = self.decompose_weight_updown.to(orig_weight.dtype)\n        self.decompose_weight_updown = self.decompose_weight_updown.mul(scale_factor)\n        self.decompose_weight_updown = self.decompose_weight_updown.div(self.decompose_weight_updown.norm())\n        self.decompose_weight_updown = self.decompose_weight_updown.mul(orig_weight.norm())\n        return self.decompose_weight_updown\n    else:\n        self.decompose_weight_downup = self.decompose_weight_downup.to(orig_weight.device)\n        self.decompose_weight_downup = self.decompose_weight_downup.to(orig_weight.dtype)\n        self.decompose_weight_downup = self.decompose_weight_downup.mul(scale_factor)\n        self.decompose_weight_downup = self.decompose_weight_downup.div(self.decompose_weight_downup.norm())\n        self.decompose_weight_downup = self.de", "CodeBleu_score": 24.33540478996466}]}
{"instance_id": "c73d0d1cd3e5", "generation_result": [{"code": "def finalize_updown(self, updown, orig_weight, output_shape, ex_bias=None)\n\t\n\n    # Apply bias\n    if ex_bias is not None:\n        updown = updown + ex_bias\n\n    # Apply weight scaling\n    if orig_weight.shape != updown.shape:\n        if orig_weight.numel() == updown.numel():\n            updown = updown.reshape(orig_weight.shape)\n        else:\n            raise errors.UserError(f\"Shape mismatch: {orig_weight.shape} vs {updown.shape}\")\n\n    # Apply weight scaling\n    if self.weight_scaler is not None:\n        updown = self.weight_scaler.decompose(updown)\n\n    # Apply scalar\n    if self.scalar is not None:\n        updown = updown * self.scalar\n\n    return updown, ex_bias\n\n", "CodeBleu_score": 44.1913475829468}, {"code": "def finalize_updown(self, updown, orig_weight, output_shape, ex_bias=None)\n\t\n\n    if ex_bias is not None:\n        updown = updown + ex_bias\n\n    if updown.shape != orig_weight.shape:\n        updown = updown.reshape(orig_weight.shape)\n\n    if shared.opts.updown_scalar is not None:\n        updown = updown * shared.opts.updown_scalar\n\n    if shared.opts.updown_decomposition is not None:\n        updown = updown.reshape(-1, shared.opts.updown_decomposition)\n        updown = updown @ shared.opts.updown_decomposition_matrix\n        updown = updown.reshape(updown.shape[:-1] + (orig_weight.shape[0],))\n\n    if shared.opts.updown_bias is not None:\n        updown = updown * shared.opts.updown_bias\n\n    return updown, shared.opts.updown_bias\n\n", "CodeBleu_score": 45.81320065535389}, {"code": "def finalize_updown(self, updown, orig_weight, output_shape, ex_bias=None)\n\t\n\n    # Apply bias\n    if ex_bias is not None:\n        updown = updown + ex_bias\n\n    # Reshape updown to match the original weight's shape\n    if orig_weight.shape != updown.shape:\n        updown = updown.reshape(orig_weight.shape)\n\n    # Apply weight decomposition\n    if hasattr(self, 'weight_decomposition'):\n        updown = self.weight_decomposition(updown)\n\n    # Apply scalar\n    if hasattr(self, 'scalar'):\n        updown = updown * self.scalar\n\n    # Apply bias\n    if hasattr(self, 'bias'):\n        updown = updown + self.bias\n\n    # Reshape updown to match the output shape\n    if updown.shape != output_shape:\n        updown = updown.reshape(output_shape)\n\n    # Apply additional bias\n    if ex_bias is not None:\n        ex_bias = ex_bias * self.scalar\n\n    return updown, ex_bias\n\n", "CodeBleu_score": 46.728441319335296}]}
{"instance_id": "c6a41665c62c", "generation_result": [{"code": "def calc_updown(self, orig_weight)\n\t\n    self.weight = self.weight.to(orig_weight.device)\n    if self.bias is not None:\n        self.bias = self.bias.to(orig_weight.device)\n    return network.calc_updown(self, orig_weight)", "CodeBleu_score": 34.57329653172365}, {"code": "def calc_updown(self, orig_weight)\n\t\n    # Transfer the weight tensor and optional bias to the device of the original weight tensor\n    self.weight = self.weight.to(orig_weight.device)\n    if self.bias is not None:\n        self.bias = self.bias.to(orig_weight.device)\n\n    # Call the function to finalize the updated weights\n    return self.finalize_updown(orig_weight)\n\n", "CodeBleu_score": 34.874761335334966}, {"code": "def calc_updown(self, orig_weight)\n\t\n    if self.weight_update_mode == 'updown':\n        if self.weight_update_mode_param is not None:\n            if self.weight_update_mode_param == 'all':\n                weight_update_mode_param = 'all'\n            else:\n                weight_update_mode_param = self.weight_update_mode_param\n        else:\n            weight_update_mode_param = 'all'\n        return network.calc_updown(orig_weight, self.weight, self.bias, self.weight_update_mode_param)\n    else:\n        return self.weight\n\n", "CodeBleu_score": 20.099536728592405}]}
{"instance_id": "1bb9e5d94af8", "generation_result": [{"code": "def purge_networks_from_memory()\n\t\n    if shared.opts.get(\"lora_network_count_limit\", 0) <= 0:\n        return\n    logger.info(\"Purging networks from memory\")\n    while len(sd_models.lora_models) > shared.opts.get(\"lora_network_count_limit\", 0):\n        sd_models.lora_models.popitem()\n    torch.cuda.empty_cache()\n    torch.cuda.reset_max_memory_allocated()\n    torch.cuda.empty_cache()\n    torch.cuda.reset_max_memory_allocated()\n    torch.cuda.empty_cache()\n    torch.cuda.reset_max_memory_allocated()\n    torch.cuda.empty_cache()\n    torch.cuda.reset_max_memory_allocated()\n    torch.cuda.empty_cache()\n    torch.cuda.reset_max_memory_allocated()\n    torch.cuda.empty_cache()\n    torch.cuda.reset_max_memory_allocated()\n    torch.cuda.empty_cache()\n    torch.cuda.reset_max_memory_allocated()\n    torch.cuda.empty_cache()\n    torch.cuda.reset_max_memory_allocated()\n    torch.cuda.empty_cache()\n    torch.cuda.reset_max_memory_allocated()\n    torch.cuda.empty_cache()\n    torch.cuda.reset_max_memory_allocated()\n    torch.cuda.empty_cache()\n    torch.cuda.reset_max_memory_allocated()\n    torch.cuda.empty_cache()\n    torch.cuda.reset_max_memory_allocated()\n    torch.cuda.empty_cache()\n    torch.cuda.reset_max_memory_allocated()\n    torch.cuda.empty_cache()\n    torch.cuda.reset_max_memory_allocated()\n    torch.cuda.empty_cache()\n    torch.c", "CodeBleu_score": 21.93156890668522}, {"code": "def purge_networks_from_memory()\n\t\n    # Get the maximum number of networks allowed\n    max_networks = shared.opts.get(\"max_networks\", 10)\n\n    # Check if the number of networks exceeds the maximum\n    if len(sd_models.models) > max_networks:\n        # Remove networks from memory until the count is within the limit\n        while len(sd_models.models) > max_networks:\n            # Remove the oldest network from memory\n            sd_models.models.pop(0)\n\n        # Trigger garbage collection to free up memory\n        torch.cuda.empty_cache()\n        torch.cuda.ipc_collect()\n\n", "CodeBleu_score": 26.535530096920702}, {"code": "def purge_networks_from_memory()\n\t\n    max_networks = shared.opts.get(\"lora.max_networks\", 10)\n    if max_networks <= 0:\n        return\n    if len(sd_models.sd_model.lora_networks) <= max_networks:\n        return\n    logger.info(f\"Purging networks from memory to stay within {max_networks} limit\")\n    while len(sd_models.sd_model.lora_networks) > max_networks:\n        sd_models.sd_model.lora_networks.pop()\n    torch.cuda.empty_cache()\n\n", "CodeBleu_score": 23.01643295030621}]}
{"instance_id": "42b48ebf4d07", "generation_result": [{"code": "def network_restore_weights_from_backup(self: Union[torch.nn.Conv2d, torch.nn.Linear, torch.nn.GroupNorm, torch.nn.LayerNorm, torch.nn.MultiheadAttention]):\n\t\n    logger.info(f\"Restoring weights and biases from backup for {self.__class__.__name__}\")\n    if hasattr(self, \"weight_backup\"):\n        self.weight.data = self.weight_backup.data\n    if hasattr(self, \"bias_backup\"):\n        self.bias.data = self.bias_backup.data\n    elif hasattr(self, \"bias_backup\"):\n        self.bias = None\n\n", "CodeBleu_score": 22.998171156901513}, {"code": "def network_restore_weights_from_backup(self: Union[torch.nn.Conv2d, torch.nn.Linear, torch.nn.GroupNorm, torch.nn.LayerNorm, torch.nn.MultiheadAttention]):\n\t\n    backup_weights_name = self.name + '_backup_weights'\n    backup_bias_name = self.name + '_backup_bias'\n    if hasattr(self, backup_weights_name):\n        self.weight = getattr(self, backup_weights_name)\n        if hasattr(self, backup_bias_name):\n            self.bias = getattr(self, backup_bias_name)\n        else:\n            self.bias = None\n\n", "CodeBleu_score": 27.644215709410503}, {"code": "def network_restore_weights_from_backup(self: Union[torch.nn.Conv2d, torch.nn.Linear, torch.nn.GroupNorm, torch.nn.LayerNorm, torch.nn.MultiheadAttention]):\n\t\n    # Check if backup weights exist\n    if hasattr(self, \"weight_backup\"):\n        # Copy backup weights back to layer\n        self.weight = self.weight_backup\n        # Check if backup biases exist\n        if hasattr(self, \"bias_backup\"):\n            # Copy backup biases back to layer\n            self.bias = self.bias_backup\n        else:\n            # Set bias to None\n            self.bias = None\n    # Check if backup projection weights exist\n    elif hasattr(self, \"projection_weight_backup\"):\n        # Copy backup projection weights back to layer\n        self.projection_weight = self.projection_weight_backup\n        # Check if backup projection biases exist\n        if hasattr(self, \"projection_bias_backup\"):\n            # Copy backup projection biases back to layer\n            self.projection_bias = self.projection_bias_backup\n        else:\n            # Set projection bias to None\n            self.projection_bias = None\n    # Check if backup projection weights exist\n    elif hasattr(self, \"projection_weight_backup\"):\n        # Copy backup projection weights back to layer\n        self.projection_weight = self.projection_weight_backup\n        # Check if backup projection biases exist\n        if hasattr(self, \"projection_bias_backup\"):\n            # Copy backup projection biases back to layer\n            self.projection_bias = self.projection_bias_backup\n        else:\n            # Set projection bias to None\n            self.projection_bias = None\n    # Check if backup projection weights exist\n    elif hasattr(self, \"projection_weight_backup\"):\n        # Copy backup projection weights back to layer\n        self.projection_weight = self.projection_weight_backup\n        # Check if backup projection biases exist\n        if hasattr(self, \"projection_bias_backup\"):\n            # Copy backup projection biases back to layer\n            self.projection_bias = self.projection_bias_backup\n        else:\n            # Set projection bias to None\n            self.projection_bias = None\n   ", "CodeBleu_score": 34.39484066818766}]}
{"instance_id": "c8fcd357131a", "generation_result": [{"code": "def network_forward(org_module, input, original_forward)\n\t\n    if not hasattr(org_module, \"lora_networks\"):\n        return original_forward(input)\n\n    # Cast input to device\n    input = input.to(devices.device)\n\n    # Restore weights from backup\n    with torch.no_grad():\n        org_module.restore_weights()\n\n    # Reset weights\n    org_module.reset_weights()\n\n    # Call original forward method\n    output = original_forward(input)\n\n    # Apply loaded networks\n    for net in org_module.lora_networks:\n        if net.layer == org_module.layer:\n            output = net.apply(output)\n\n    return output\n\n", "CodeBleu_score": 32.36354642929691}, {"code": "def network_forward(org_module, input, original_forward)\n\t\n    if not hasattr(org_module, \"lora_networks\"):\n        return original_forward(input)\n\n    input = input.to(devices.device)\n\n    with torch.no_grad():\n        org_module.cpu()\n        org_module.eval()\n        org_module.weight.data = org_module.backup_weight.data.clone()\n        org_module.reset_parameters()\n        org_module.to(devices.device)\n        output = original_forward(input)\n\n        for network in org_module.lora_networks:\n            if network.target_layer == org_module:\n                output = network.apply_forward(output)\n\n    return output\n\n", "CodeBleu_score": 35.94728567456398}, {"code": "def network_forward(org_module, input, original_forward)\n\t\n    if not hasattr(org_module, \"lora_networks\"):\n        return original_forward(input)\n\n    # cast input to appropriate device\n    input = input.to(devices.device)\n\n    # backup weights\n    weights = org_module.weight.data.clone()\n\n    # restore original weights\n    org_module.weight.data = weights\n\n    # call original forward\n    output = original_forward(input)\n\n    # apply loaded networks\n    for network in org_module.lora_networks:\n        if network.match(org_module):\n            output = network.apply(output)\n\n    return output\n\n", "CodeBleu_score": 32.712514725898814}]}
{"instance_id": "1070211f5c11", "generation_result": [{"code": "def trainables(self)\n\t\n    return [p for p in self.parameters() if p.requires_grad]\n\n", "CodeBleu_score": 6.452447410697191}, {"code": "def trainables(self)\n\t\n    trainables = []\n    for m in self.modules():\n        if isinstance(m, (torch.nn.Linear, torch.nn.LayerNorm)):\n            trainables.extend(list(m.parameters()))\n    return trainables\n\n", "CodeBleu_score": 31.521436893511318}, {"code": "def trainables(self)\n\t\n    return [p for p in self.parameters() if p.requires_grad]\n\n", "CodeBleu_score": 6.452447410697191}]}
{"instance_id": "0c3ba62c96a7", "generation_result": [{"code": "def load_hypernetworks(names, multipliers=None)\n\t\n    if multipliers is None:\n        multipliers = [1.0] * len(names)\n    hypernetworks = []\n    for i, name in enumerate(names):\n        if name in shared.loaded_hypernetworks:\n            hypernetwork = shared.loaded_hypernetworks[name]\n        else:\n            hypernetwork = textual_inversion.Hypernetwork(name)\n            hypernetwork.set_multiplier(multipliers[i])\n            shared.loaded_hypernetworks[name] = hypernetwork\n        hypernetworks.append(hypernetwork)\n    return hypernetworks\n\n", "CodeBleu_score": 35.955228474494746}, {"code": "def load_hypernetworks(names, multipliers=None)\n\t\n    if multipliers is None:\n        multipliers = {}\n    if len(multipliers) > 0:\n        for name in names:\n            if name in multipliers:\n                multiplier = multipliers[name]\n                textual_inversion.hypernetworks[name].multiplier = multiplier\n\n    for name in names:\n        if name not in textual_inversion.hypernetworks:\n            textual_inversion.hypernetworks[name] = Hypernetwork(name)\n\n    for name in names:\n        textual_inversion.loaded_hypernetworks.append(textual_inversion.hypernetworks[name])\n\n", "CodeBleu_score": 29.971852288949368}, {"code": "def load_hypernetworks(names, multipliers=None)\n\t\n    # Check if the hypernetworks have already been loaded\n    if shared.loaded_hypernetworks:\n        # Clear the current list of loaded hypernetworks\n        shared.loaded_hypernetworks.clear()\n\n    # Load or retrieve each hypernetwork by name, setting its multiplier if provided\n    for name in names:\n        if multipliers is not None:\n            multiplier = multipliers.get(name, 1.0)\n        else:\n            multiplier = 1.0\n        shared.loaded_hypernetworks.append(textual_inversion.load_hypernetwork(name, multiplier))\n\n", "CodeBleu_score": 21.179778757062024}]}
{"instance_id": "bea3772ed532", "generation_result": [{"code": "\ndef apply_hypernetworks(hypernetworks, context, layer=None)\n\t\n    context_k, context_v = context, context\n    for i, hypernetwork in enumerate(hypernetworks):\n        context_k, context_v = apply_single_hypernetwork(hypernetwork, context_k, context_v, layer=layer)\n    return context_k, context_v\n\n", "CodeBleu_score": 50.496793767456246}, {"code": "\ndef apply_hypernetworks(hypernetworks, context, layer=None)\n\t\n    context_k = context\n    context_v = context\n    for hypernetwork in hypernetworks:\n        context_k, context_v = apply_single_hypernetwork(hypernetwork, context_k, context_v, layer=layer)\n    return context_k, context_v\n\n", "CodeBleu_score": 63.822641355854024}, {"code": "\ndef apply_hypernetworks(hypernetworks, context, layer=None)\n\t\n    context_k = context\n    context_v = context\n    for hypernetwork in hypernetworks:\n        context_k, context_v = apply_single_hypernetwork(hypernetwork, context_k, context_v, layer=layer)\n    return context_k, context_v\n\n", "CodeBleu_score": 63.822641355854024}]}
{"instance_id": "922efa3e092f", "generation_result": [{"code": "def attention_CrossAttention_forward(self, x, context=None, mask=None, **kwargs)\n\t\n    B, N, C = x.shape\n    if context is not None:\n        context = context.to(x.device)\n        context = context.to(x.dtype)\n        if context.dim() == 2:\n            context = repeat(context, 'n c -> b n c', b=B)\n\n    x = self.query(x)\n    context = self.key(context)\n    context = self.value(context)\n\n    context = context.to(x.dtype)\n    if context.dim() == 2:\n        context = repeat(context, 'n c -> b n c', b=B)\n\n    x = rearrange(x, 'b n (h d) -> b h n d', h=self.heads)\n    context = rearrange(context, 'b n (h d) -> b h n d', h=self.heads)\n\n    sim = einsum('b h i d, b h j d -> b h i j', x, context) * self.scale\n\n    if mask is not None:\n        mask = mask.to(x.dtype)\n        mask = mask.to(x.device)\n        sim = sim.masked_fill(~mask, -torch.finfo(sim.dtype).max)\n\n    attn = sim.softmax(dim=-1)\n    out = einsum('b h i j, b h j d -> b h i d', attn, context)\n    out = rearrange(out, 'b h n d -> b n (h d)')\n    return self.to_out(out)\n\n", "CodeBleu_score": 54.34030102631382}, {"code": "def attention_CrossAttention_forward(self, x, context=None, mask=None, **kwargs)\n\t\n    # x: [B, C, H, W]\n    # context: [B, C, H, W]\n    # mask: [B, H, W]\n    B, C, H, W = x.shape\n    context = x if context is None else context\n    B, C_, H_, W_ = context.shape\n    assert C == C_\n    assert H == H_\n    assert W == W_\n    assert B == mask.shape[0]\n\n    # [B, C, H, W] -> [B, H, W, C] -> [B, H*W, C]\n    x = rearrange(x, 'b c h w -> b (h w) c')\n    context = rearrange(context, 'b c h w -> b (h w) c')\n\n    # [B, H*W, C] -> [B, H*W, C]\n    x = self.query(x)\n    context = self.hypernet_context(context)\n\n    # [B, H*W, C] -> [B, H*W, C]\n    context = self.key(context)\n\n    # [B, H*W, C] -> [B, H*W, C]\n    context = self.value(context)\n\n    # [B, H*W, C] -> [B, C, H, W]\n    context = rearrange(context, 'b (h w) c -> b c h w', h=H, w=W)\n\n    # [B, C, H, W] -> [B, H*W, C]\n    x = rearrange(x, 'b c h w -> b (h w) c')\n\n    # [B, H*W, C] -> [B, H*W, C]\n    x = self.hypernet_query(x)\n\n    # [B, H*W, C] -> [B, H*W, C]\n    x = self.key(x)\n\n    # [B, H*W, C] -> [B, H*W, C]\n    x = self", "CodeBleu_score": 21.88859194941843}, {"code": "def attention_CrossAttention_forward(self, x, context=None, mask=None, **kwargs)\n\t\n    if context is None:\n        context = x\n    context = context.to(x.dtype)\n    b, n, device = *x.shape, x.device\n    q, k, v = self.to_qkv(x), self.to_kv(context), self.to_kv(context)\n    if self.residual_before_proj:\n        q = q + self.residual_before_proj(x)\n    q, k, v = map(lambda t: rearrange(t, \"b n (h d) -> (b h) n d\", h=self.heads), (q, k, v))\n    sim = einsum(\"b i d, b j d -> b i j\", q, k) * self.scale\n    if mask is not None:\n        mask = rearrange(mask, \"b ... -> b (...)\")\n        max_neg_value = -torch.finfo(sim.dtype).max\n        sim.masked_fill_(~mask[:, None, :], max_neg_value)\n    attn = sim.softmax(dim=-1)\n    out = einsum(\"b i j, b j d -> b i d\", attn, v)\n    out = rearrange(out, \"(b h) n d -> b n (h d)\", h=self.heads)\n    if self.residual_before_proj:\n        out = out + self.residual_before_proj(x)\n    return self.to_out(out)\n\n", "CodeBleu_score": 57.50138979852597}]}
{"instance_id": "8b7455a03a0e", "generation_result": [{"code": "def save_hypernetwork(hypernetwork, checkpoint, hypernetwork_name, filename)\n\t\n    hypernetwork_backup = {\n        \"state_dict\": hypernetwork.state_dict(),\n        \"config\": hypernetwork.config,\n        \"hypernetwork_name\": hypernetwork.hypernetwork_name,\n        \"hypernetwork_path\": hypernetwork.hypernetwork_path,\n        \"hypernetwork_model_name\": hypernetwork.hypernetwork_model_name,\n        \"hypernetwork_model_path\": hypernetwork.hypernetwork_model_path,\n        \"hypernetwork_model_hash\": hypernetwork.hypernetwork_model_hash,\n        \"hypernetwork_model_hash_name\": hypernetwork.hypernetwork_model_hash_name,\n        \"hypernetwork_model_hash_path\": hypernetwork.hypernetwork_model_hash_path,\n        \"hypernetwork_model_hash_hash\": hypernetwork.hypernetwork_model_hash_hash,\n        \"hypernetwork_model_hash_hash_name\": hypernetwork.hypernetwork_model_hash_hash_name,\n        \"hypernetwork_model_hash_hash_path\": hypernetwork.hypernetwork_model_hash_hash_path,\n        \"hypernetwork_model_hash_hash_hash\": hypernetwork.hypernetwork_model_hash_hash_hash,\n        \"hypernetwork_model_hash_hash_hash_name\": hypernetwork.hypernetwork_model_hash_hash_hash_name,\n        \"hypernetwork_model_hash_hash_hash_path\": hypernetwork.hypernetwork_model_hash_hash_hash_path,\n        \"hypernetwork_model_hash_hash_hash_hash\": hypernetwork.hypernetwork_model_hash_hash_hash_hash,\n        \"hypernetwork_model_hash_hash_hash_hash_name\": hypernetwork.hypernetwork_model_hash_hash_hash_hash_name,\n        \"hypernetwork_model_hash_hash_hash_hash_path\": hypernetwork.hypernetwork_model_hash_hash_hash_hash_path,\n        \"hypernetwork_model_hash_hash_hash_hash_hash\": hypernetwork.hypernetwork_model_hash_hash_hash_hash_hash,\n        \"hypernetwork_model_hash_hash_hash_hash_hash_name\": hypernetwork.hypernetwork_model_", "CodeBleu_score": 16.04516032035727}, {"code": "def save_hypernetwork(hypernetwork, checkpoint, hypernetwork_name, filename)\n\t\n    try:\n        original_attributes = {attr: getattr(hypernetwork, attr) for attr in dir(hypernetwork) if not attr.startswith(\"__\")}\n        hypernetwork.checkpoint = checkpoint\n        hypernetwork.hypernetwork_name = hypernetwork_name\n        hypernetwork.save(filename)\n    except Exception as e:\n        for attr, value in original_attributes.items():\n            setattr(hypernetwork, attr, value)\n        raise e\n\n", "CodeBleu_score": 32.87100010010116}, {"code": "def save_hypernetwork(hypernetwork, checkpoint, hypernetwork_name, filename)\n\t\n    try:\n        # Back up original attributes\n        original_attributes = {}\n        for attr in hypernetwork.state_dict():\n            original_attributes[attr] = getattr(hypernetwork, attr)\n\n        # Update hypernetwork with new checkpoint data\n        hypernetwork.load_state_dict(checkpoint)\n\n        # Save hypernetwork to file\n        torch.save(hypernetwork.state_dict(), filename)\n\n    except Exception as e:\n        # Restore original attributes\n        for attr, value in original_attributes.items():\n            setattr(hypernetwork, attr, value)\n\n        # Raise original exception\n        raise e\n\n", "CodeBleu_score": 24.224610200964843}]}
{"instance_id": "3175f835140f", "generation_result": [{"code": "def ema_scope(self, context=None)\n\t\n    if not self.ema_enabled:\n        yield\n        return\n    if context is None:\n        context = {}\n    original_weights = self.state_dict()\n    if self.ema_restore_on_exit:\n        context['ema_restore_on_exit'] = True\n    self.load_state_dict(self.ema_state_dict())\n    try:\n        yield\n    finally:\n        if context.get('ema_restore_on_exit', False):\n            self.load_state_dict(original_weights)\n\n", "CodeBleu_score": 34.53161268939907}, {"code": "def ema_scope(self, context=None)\n\t\n    if not self.ema_enabled:\n        yield\n        return\n    if context is None:\n        context = {}\n    ema_params = self.ema_params\n    ema_params.update(context)\n    with self.ema_scope(ema_params):\n        yield\n\n", "CodeBleu_score": 20.796760213642983}, {"code": "def ema_scope(self, context=None)\n\t\n    if context is None:\n        context = {}\n    if self.ema_enabled:\n        if self.ema_context is None:\n            self.ema_context = {}\n        if self.ema_context.get(\"ema_model\") is None:\n            self.ema_context[\"ema_model\"] = self.model.state_dict()\n        self.model.load_state_dict(self.ema_model.state_dict())\n        if self.ema_log:\n            self.ema_log(context)\n    yield\n    if self.ema_enabled:\n        if self.ema_context.get(\"ema_model\") is not None:\n            self.model.load_state_dict(self.ema_context[\"ema_model\"])\n        if self.ema_log:\n            self.ema_log(context)\n\n", "CodeBleu_score": 38.180577759020785}]}
{"instance_id": "a7b5d5cd519a", "generation_result": [{"code": "def p_mean_variance(self, x, t, clip_denoised: bool)\n\t\n    model_output = self.model(x, t)\n    if self.model.config.predict_xstart:\n        if clip_denoised:\n            x_recon = torch.clamp(model_output[\"pred_xstart\"], -1, 1)\n        else:\n            x_recon = model_output[\"pred_xstart\"]\n        model_mean, posterior_variance, posterior_log_variance = model_output[\"pred_xstart\"], model_output[\"variance\"], model_output[\"log_variance\"]\n    else:\n        x_recon = model_output[\"pred_noise\"]\n        model_mean, posterior_variance, posterior_log_variance = self.q_posterior(x_start=x, x_t=x_recon, t=t)\n    return model_mean, posterior_variance, posterior_log_variance\n\n", "CodeBleu_score": 46.6567334267695}, {"code": "def p_mean_variance(self, x, t, clip_denoised: bool)\n\t\n    model_output = self.model(x, t)\n    if self.model_var_type in [\"learned\", \"fixedlarge\", \"fixedsmall\"]:\n        h = self.get_model_var(x, t)\n        model_log_variance = h[..., 1]\n        if self.model_var_type == \"learned\":\n            model_variance = torch.exp(model_log_variance)\n        else:\n            model_variance = self.model_var_fixed_large if self.model_var_type == \"fixedlarge\" else self.model_var_fixed_small\n    else:\n        model_variance, model_log_variance = self.q_posterior_log_variance(x, t=t)\n    if self.model_mean_type == \"epsilon\":\n        eps = self.q_sample(x=x, t=t, noise=model_log_variance.exp())\n        if self.model_var_type in [\"learned\", \"fixedlarge\", \"fixedsmall\"]:\n            assert not clip_denoised\n        else:\n            x_recon = self.predict_start_from_noise(x, t=t, eps=eps)\n            if clip_denoised:\n                x_recon = x_recon.clamp(-1, 1)\n    elif self.model_mean_type == \"x_prev\":\n        x_recon = self.predict_start_from_x_prev(x_t=x, t=t, x_prev=model_output)\n        if clip_denoised:\n            x_recon = x_recon.clamp(-1, 1)\n    else:\n        raise NotImplementedError(self.model_mean_type)\n    model_mean = model_output\n    return model_mean, model_variance, model_log_variance\n\n", "CodeBleu_score": 43.59000443464552}, {"code": "def p_mean_variance(self, x, t, clip_denoised: bool)\n\t\n    model_output = self.model(x, t)\n    if self.model.config.predict_xstart:\n        xstart_pred = model_output.pred_xstart\n        model_output = model_output[0]\n    else:\n        xstart_pred = None\n    dist = DiagonalGaussianDistribution(model_output)\n    mean = dist.mean\n    variance, log_variance = dist.variance, dist.log_variance\n    if xstart_pred is not None:\n        xstart_pred = self.model.decode_first_stage(xstart_pred)\n        mean, variance, log_variance = self.q_posterior(xstart_pred, x, t)\n    if clip_denoised:\n        x_recon = torch.clamp(mean, -1.0, 1.0)\n    else:\n        x_recon = mean\n    return {\n        \"mean\": x_recon,\n        \"variance\": variance,\n        \"log_variance\": log_variance,\n    }\n\n", "CodeBleu_score": 35.29076869968454}]}
{"instance_id": "a8652fa78473", "generation_result": [{"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss_type == 'l1':\n        return torch.mean(torch.abs(pred - target)) if mean else torch.abs(pred - target)\n    elif self.loss_type == 'l2':\n        return torch.mean((pred - target) ** 2) if mean else (pred - target) ** 2\n    else:\n        raise NotImplementedError(f\"Unknown loss type: {self.loss_type}\")\n\n", "CodeBleu_score": 29.96360900214226}, {"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss_type == 'l1':\n        loss = torch.abs(pred - target)\n        if mean:\n            loss = loss.mean()\n        return loss\n    elif self.loss_type == 'l2':\n        loss = torch.nn.functional.mse_loss(pred, target, reduction='none')\n        if mean:\n            loss = loss.mean()\n        return loss\n    else:\n        raise ValueError(f\"Unknown loss type: {self.loss_type}\")\n\n", "CodeBleu_score": 61.08024594318331}, {"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss_type == 'l1':\n        return torch.abs(pred - target).mean() if mean else torch.abs(pred - target)\n    elif self.loss_type == 'l2':\n        return torch.nn.functional.mse_loss(pred, target, reduction='mean' if mean else 'none')\n    else:\n        raise ValueError(f\"Unknown loss type: {self.loss_type}\")\n\n", "CodeBleu_score": 34.59296392929026}]}
{"instance_id": "d68e4f061349", "generation_result": [{"code": "def _prior_bpd(self, x_start)\n\t\n    b, *_, device, dtype = *x_start.shape, x_start.device, x_start.dtype\n    t = torch.ones(b, device=device, dtype=dtype)\n    model_kwargs = dict(y=None, timesteps=t, return_dict=True)\n    if hasattr(self, \"get_input\"):\n        model_kwargs[\"sample\"] = self.get_input(x_start, t)\n    else:\n        model_kwargs[\"sample\"] = x_start\n    model_out = self(**model_kwargs)\n    mean, log_var = model_out[\"mean\"], model_out[\"log_variance\"]\n    std = torch.exp(0.5 * log_var)\n    mean = mean.to(dtype)\n    std = std.to(dtype)\n    var = std ** 2\n    log_std = torch.log(std)\n    kl = normal_kl(mean, log_std, var)\n    bpd = -(kl.flatten(1).sum(-1) / np.log(2) / np.prod(x_start.shape[1:]))\n    return bpd\n\n", "CodeBleu_score": 32.286425391403206}, {"code": "def _prior_bpd(self, x_start)\n\t\n    batch_size = x_start.shape[0]\n    t = torch.ones((batch_size,), device=x_start.device, dtype=torch.long)\n    t = extract_into_tensor(t, self.model.num_timesteps)\n    mean, log_var = self.model.q_posterior(x_start, t)\n    return normal_kl(mean, log_var).mean().div(math.log(2.))\n\n", "CodeBleu_score": 28.44514690238546}, {"code": "def _prior_bpd(self, x_start)\n\t\n    b, c, h, w = x_start.shape\n    t = torch.ones((b,), device=x_start.device)\n    t = extract_into_tensor(t, self.model.num_timesteps)\n    model_output = self.model(x_start, t)\n    mean, log_var = model_output[\"mean\"], model_output[\"log_var\"]\n    std = torch.exp(0.5 * log_var)\n    kl = normal_kl(mean, std, torch.zeros_like(mean), torch.ones_like(std))\n    bpd = kl / (np.log(2) * np.prod(x_start.shape[1:]))\n    return bpd.mean()\n\n", "CodeBleu_score": 22.38208228554252}]}
{"instance_id": "c695d93fdf2d", "generation_result": [{"code": "def make_cache(subsection: str) -> diskcache.Cache\n\t\n    cache_path = os.path.join(data_path, \"cache\", subsection)\n    return diskcache.Cache(cache_path, size_limit=4e9, disk_min_file_size=256e3)\n\n", "CodeBleu_score": 44.250004291325475}, {"code": "def make_cache(subsection: str) -> diskcache.Cache\n\t\n    cache_dir = os.path.join(data_path, \"cache\", subsection)\n    os.makedirs(cache_dir, exist_ok=True)\n    cache = diskcache.Cache(cache_dir, size_limit=4e9, eviction_policy=\"least-recently-used\", disk_min_file_size=256e3)\n    return cache\n\n", "CodeBleu_score": 44.26056701139382}, {"code": "def make_cache(subsection: str) -> diskcache.Cache\n\t\n    cache_dir = os.path.join(data_path, \"cache\", subsection)\n    os.makedirs(cache_dir, exist_ok=True)\n    cache = diskcache.Cache(cache_dir, size_limit=4e9, eviction_policy=\"least-recently-used\", disk_min_file_size=256 * 1024)\n    return cache\n\n", "CodeBleu_score": 49.12348659586758}]}
{"instance_id": "ed4c543f0d38", "generation_result": [{"code": "def cache(subsection)\n\t\n    caches = {}\n    cache_lock = threading.Lock()\n\n    def convert_old_cached_data():\n        \"\"\"The function convert_old_cached_data() is used to convert legacy cached data. It checks if the cache directory exists and if the cache filename is a valid file. If these conditions are not met, it returns without performing any conversion. Otherwise, it iterates through the cache directory, reads the contents of each file, and writes them to a new cache file with the same name but with a \".json\" extension. It also deletes the original cache file.", "CodeBleu_score": 15.642403892099635}, {"code": "def cache(subsection)\n\t\n    caches = cache.caches\n    cache_lock = cache.cache_lock\n    with cache_lock:\n        if subsection not in caches:\n            cache_dir = os.path.join(data_path, \"cache\")\n            if not os.path.exists(cache_dir):\n                os.makedirs(cache_dir)\n            cache_file = os.path.join(cache_dir, f\"{subsection}.json\")\n            if os.path.isfile(cache_file):\n                try:\n                    with open(cache_file, \"r\", encoding=\"utf-8\") as f:\n                        data = json.load(f)\n                        if isinstance(data, list):\n                            convert_old_cached_data(data, cache_file)\n                except json.JSONDecodeError:\n                    convert_old_cached_data(None, cache_file)\n            caches[subsection] = make_cache(subsection)\n    return caches[subsection]\n\n", "CodeBleu_score": 38.788128164078515}, {"code": "def cache(subsection)\n\t\n    caches = globals().get(\"caches\", {})\n    cache = caches.get(subsection)\n    if cache is None:\n        cache_lock.acquire()\n        cache = caches.get(subsection)\n        if cache is None:\n            convert_old_cached_data()\n            cache = make_cache(subsection)\n            caches[subsection] = cache\n        cache_lock.release()\n    return cache\n\n", "CodeBleu_score": 29.83176156665196}]}
{"instance_id": "ccf5ea2d9fc1", "generation_result": [{"code": "def cached_data_for_file(subsection, title, filename, func)\n\t\n    cache = cache(subsection)\n    mtime = os.path.getmtime(filename)\n    if title in cache:\n        if cache[title][\"mtime\"] < mtime:\n            del cache[title]\n    if title not in cache:\n        data = func()\n        cache[title] = {\"data\": data, \"mtime\": mtime}\n        dump_cache(subsection, cache)\n    return cache[title][\"data\"]\n\n", "CodeBleu_score": 25.08616082247351}, {"code": "def cached_data_for_file(subsection, title, filename, func)\n\t\n    cache = cache(subsection)\n    mtime = os.path.getmtime(filename)\n    if title in cache:\n        if mtime > cache[title][\"mtime\"]:\n            del cache[title]\n    if title not in cache:\n        data = func()\n        cache[title] = {\"mtime\": mtime, \"data\": data}\n        dump_cache(cache)\n    return cache[title][\"data\"]\n\n", "CodeBleu_score": 26.13477793117895}, {"code": "def cached_data_for_file(subsection, title, filename, func)\n\t\n    cache = cache(subsection)\n    mtime = os.path.getmtime(filename)\n    if title in cache:\n        if mtime > cache[title][\"mtime\"]:\n            del cache[title]\n    if title in cache:\n        return cache[title][\"data\"]\n    data = func()\n    cache[title] = {\"mtime\": mtime, \"data\": data}\n    dump_cache(subsection, cache)\n    return data\n\n", "CodeBleu_score": 25.18558091552574}]}
{"instance_id": "e01a2d4f0ac2", "generation_result": [{"code": "def report(message: str, *, exc_info: bool = False) -> None\n\t\n    record_exception()\n    for line in message.splitlines():\n        sys.stderr.write(\"*** {}", "CodeBleu_score": 24.605372703932012}, {"code": "def report(message: str, *, exc_info: bool = False) -> None\n\t\n    record_exception()\n    for line in textwrap.wrap(message, 70):\n        sys.stderr.write(\"*** \" + line + \"", "CodeBleu_score": 25.450653304096747}, {"code": "def report(message: str, *, exc_info: bool = False) -> None\n\t\n    record_exception()\n    lines = textwrap.wrap(message, width=70, subsequent_indent=\"*** \")\n    for line in lines:\n        print(line, file=sys.stderr)\n    if exc_info:\n        print(\"---\", file=sys.stderr)\n        traceback.print_exc(file=sys.stderr)\n\n", "CodeBleu_score": 47.609627918403056}]}
{"instance_id": "125fb9d2320b", "generation_result": [{"code": "def extract_original_prompts(style: PromptStyle, prompt, negative_prompt)\n\t\n    if style.prompt == \"\" and style.negative_prompt == \"\":\n        return False, prompt, negative_prompt\n    try:\n        prompt = extract_style_text(style.prompt, prompt)\n    except errors.StyleNotFoundError:\n        return False, prompt, negative_prompt\n    try:\n        negative_prompt = extract_style_text(style.negative_prompt, negative_prompt)\n    except errors.StyleNotFoundError:\n        return False, prompt, negative_prompt\n    return True, prompt, negative_prompt\n\n", "CodeBleu_score": 55.157742288173765}, {"code": "def extract_original_prompts(style: PromptStyle, prompt, negative_prompt)\n\t\n    if style.prompt == \"\" and style.negative_prompt == \"\":\n        return False, prompt, negative_prompt\n\n    try:\n        extracted_prompt = style.extract_prompt(prompt)\n        extracted_negative_prompt = style.extract_prompt(negative_prompt)\n    except errors.ExtractionError:\n        return False, prompt, negative_prompt\n\n    return True, extracted_prompt, extracted_negative_prompt\n\n", "CodeBleu_score": 43.929429197461076}, {"code": "def extract_original_prompts(style: PromptStyle, prompt, negative_prompt)\n\t\n    if style.prompt == \"\" and style.negative_prompt == \"\":\n        return False, prompt, negative_prompt\n    try:\n        extracted_prompt = extract_style_text(style, prompt)\n        extracted_negative_prompt = extract_style_text(style, negative_prompt)\n    except errors.StyleNotFound:\n        return False, prompt, negative_prompt\n    return True, extracted_prompt, extracted_negative_prompt\n\n", "CodeBleu_score": 44.913446475943765}]}
{"instance_id": "0c5f729fbce2", "generation_result": [{"code": "def random_partial(self, count, n_frames)\n\t\n    if not self.utterances_loaded:\n        self._load_utterances()\n    utterances = self.utterance_cycler.sample(count)\n    return [(u, u.random_partial(n_frames)) for u in utterances]\n\n", "CodeBleu_score": 56.01452748329061}, {"code": "def random_partial(self, count, n_frames)\n\t\n    if not self.utterances:\n        self._load_utterances()\n    utterances = self.utterance_cycler.sample(count)\n    return [(u, u.random_partial(n_frames)) for u in utterances]\n\n", "CodeBleu_score": 55.89879214825755}, {"code": "def random_partial(self, count, n_frames)\n\t\n    self._load_utterances()\n    utterances = self.utterance_cycler.sample(count)\n    return [(u, u.random_partial(n_frames)) for u in utterances]\n\n", "CodeBleu_score": 36.79237623664877}]}
{"instance_id": "31e8b27b8f94", "generation_result": [{"code": "def normalize_volume(wav, target_dBFS, increase_only=False, decrease_only=False)\n\t\n    dBFS_change = target_dBFS - 10 * np.log10(np.mean(wav ** 2))\n    if (dBFS_change < 0) and increase_only or (dBFS_change > 0) and decrease_only:\n        return wav\n    return wav * 10 ** (dBFS_change / 20)\n\n", "CodeBleu_score": 41.48468543262147}, {"code": "def normalize_volume(wav, target_dBFS, increase_only=False, decrease_only=False)\n\t\n    dBFS_change = target_dBFS - 10 * np.log10(np.mean(wav ** 2))\n    if (dBFS_change < 0) and increase_only or (dBFS_change > 0) and decrease_only:\n        warn(\n            \"desired target decrease_only dBFS [{}] < current mean increase_only dBFS [{}]; \"\n            \"returning the input audio as is without modification\".format(\n                target_dBFS, 10 * np.log10(np.mean(wav ** 2))\n            )\n        )\n        return wav\n    return wav * (10.0 ** (dBFS_change / 20.0))\n\n", "CodeBleu_score": 44.44970092326439}, {"code": "def normalize_volume(wav, target_dBFS, increase_only=False, decrease_only=False)\n\t\n    if increase_only and decrease_only:\n        raise ValueError(\"Both increase_only and decrease_only cannot be set\")\n    dBFS_change = target_dBFS - 10 * np.log10(np.mean(wav ** 2))\n    if (dBFS_change < 0 and increase_only) or (dBFS_change > 0 and decrease_only):\n        return wav\n    return wav * 10 ** (dBFS_change / 20)\n\n", "CodeBleu_score": 87.38287166168203}]}
{"instance_id": "2353831e9033", "generation_result": [{"code": "def load_model(weights_fpath: Path, device=None)\n\t\n    if device is None:\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    _model = SpeakerEncoder(device=device)\n    checkpoint = torch.load(weights_fpath)\n    _model.load_state_dict(checkpoint[\"model_state\"])\n    _model.eval()\n    print(\"Loaded encoder \\\"{}\\\" trained to step {}\".format(weights_fpath.name, checkpoint[\"step\"]))\n    return _model\n\n", "CodeBleu_score": 52.97013908630311}, {"code": "def load_model(weights_fpath: Path, device=None)\n\t\n    if device is None:\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    _model = SpeakerEncoder(device=device)\n    _model.load_state_dict(torch.load(weights_fpath, map_location=device))\n    _model.eval()\n    print(\"Loaded encoder \\\"{}\\\" trained to step {}\".format(weights_fpath.name, _model._step))\n    return _model\n\n", "CodeBleu_score": 43.8420538463477}, {"code": "def load_model(weights_fpath: Path, device=None)\n\t\n    if device is None:\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    _model = SpeakerEncoder(device=device)\n    checkpoint = torch.load(weights_fpath, map_location=device)\n    _model.load_state_dict(checkpoint[\"model_state\"])\n    _model.eval()\n    print(\"Loaded encoder \\\"{}\\\" trained to step {}\".format(checkpoint[\"name\"], checkpoint[\"step\"]))\n    return _model\n\n", "CodeBleu_score": 53.34925171387567}]}
{"instance_id": "3231ad480199", "generation_result": [{"code": "def embed_frames_batch(frames_batch)\n\t\n    if _model is None:\n        raise Exception(\"Model not yet loaded\")\n\n    frames_batch = torch.from_numpy(frames_batch)\n    frames_batch = frames_batch.to(_device)\n    embeds = _model.forward(frames_batch)\n    embeds = embeds.detach().cpu().numpy()\n    return embeds\n\n", "CodeBleu_score": 58.015963767990755}, {"code": "def embed_frames_batch(frames_batch)\n\t\n    if _model is None:\n        raise Exception(\"Model not yet loaded, please call load_model()\")\n\n    frames = torch.from_numpy(frames_batch).to(_device)\n    embed = _model.forward(frames)\n    embed = embed.detach().cpu().numpy()\n    return embed\n\n", "CodeBleu_score": 70.70073213797883}, {"code": "def embed_frames_batch(frames_batch)\n\t\n    if _model is None:\n        raise Exception(\"Model not yet loaded\")\n\n    frames = torch.from_numpy(frames_batch).to(_device)\n    embed = _model.forward(frames)\n    embed = embed.detach().cpu().numpy()\n\n    return embed\n\n", "CodeBleu_score": 69.54060002383939}]}
{"instance_id": "117a41716c8d", "generation_result": [{"code": "def compute_partial_slices(n_samples, partial_utterance_n_frames=partials_n_frames,\n                           min_pad_coverage=0.75, overlap=0.5)\n\t\n    assert 0 <= overlap <= 1, \"Invalid value for overlap\"\n    assert 0 <= min_pad_coverage <= 1, \"Invalid value for min_pad_coverage\"\n\n    # Compute how many frames will result in `partial_utterance_n_frames` if we cover `n_samples` in non-overlapping chunks\n    n_frames_per_step = int(np.ceil((partial_utterance_n_frames - overlap * partial_utterance_n_frames) /\n                                    (1 - overlap)))\n\n    # Make sure we cover at least `partial_utterance_n_frames`\n    n_frames_per_step = max(n_frames_per_step, 1)\n\n    # Compute how many chunks will fit in `n_samples`\n    n_steps = int(\n        np.ceil((len(wav) - n_frames_per_step + 1) / n_frames_per_step))\n\n    # Ensure we have enough frames to cover at least `partial_utterance_n_frames`\n    n_samples = n_frames_per_step * n_steps\n\n    # Pad wav with zeros to make sure we have frames to cover at least `partial_utterance_n_frames`\n    wav = np.pad(wav, (0, n_samples - len(wav)),\n                 constant_values=0)\n\n    # Re-compute `n_frames_per_step` after padding\n    n_frames_per_step = int(\n        np.ceil((partial_utterance_n_frames - overlap * partial_utterance_n_frames) / (1 - overlap)))\n\n    # Ensure `n_frames_per_step` is not above the number of frames per wav-sample\n    n_frames_per_step = min(n_frames_per_step, wav.shape[0] - 1)\n\n    # Re-compute `n_steps` after padding and `n_frames_per_step` update\n    n_steps = int(np.ceil((len(wav) - n_frames_per_step + 1) / n_frames_per_step", "CodeBleu_score": 29.303721207161377}, {"code": "def compute_partial_slices(n_samples, partial_utterance_n_frames=partials_n_frames,\n                           min_pad_coverage=0.75, overlap=0.5)\n\t\n    assert 0 <= overlap <= 1, 'Invalid value for overlap'\n    assert 0 <= min_pad_coverage <= 1, 'Invalid value for min_pad_coverage'\n\n    # Compute how many frames will result in the desired number of partials\n    n_frames_step = int((1 - overlap) * partial_utterance_n_frames)\n    assert n_frames_step >= 1, 'Invalid configuration for min_pad_coverage and/or partial_utterance_n_frames'\n\n    # Compute how many frames will result in the desired number of samples, taking into account\n    # the overlap between partials and the overlap between the first/last partial and the edge of the signal\n    min_pad_coverage *= partial_utterance_n_frames\n    n_frames_total = int((1 - overlap) * (n_samples - partial_utterance_n_frames) + partial_utterance_n_frames)\n    assert n_frames_total <= n_samples, 'Not enough samples to cover at least one partial utterance'\n    n_pad_frames = max(0, int(partial_utterance_n_frames - min_pad_coverage))\n\n    # Compute the slices frame indices\n    wav_slices, mel_slices = [], []\n    start_frame = 0\n    while start_frame < n_frames_total:\n        end_frame = min(start_frame + partial_utterance_n_frames, n_frames_total)\n        wav_slices += [(start_frame, end_frame)]\n\n        # Compute mel frames corresponding to the wav frames for this partial utterance\n        mel_start_frame = int(start_frame / n_frames_step)\n        mel_end_frame = int((end_frame + n_pad_frames) / n_frames_step)\n        mel_slices += [(mel_start_frame, mel_end_frame)]\n\n        start_frame = end_frame - n_pad_frames\n\n    return wav_slices, mel_slices\n\n", "CodeBleu_score": 26.674146660289317}, {"code": "def compute_partial_slices(n_samples, partial_utterance_n_frames=partials_n_frames,\n                           min_pad_coverage=0.75, overlap=0.5)\n\t\n    assert 0 <= min_pad_coverage <= 1\n    assert 0 < overlap < 1\n\n    samp_rate = partials_samples\n    frame_step = int(frame_length * (1 - overlap))\n    frame_length = int(frame_length)\n    assert frame_step >= 1\n\n    n_frames = int(np.ceil((n_samples + 1) / frame_step))\n    pad_frames = max(0, (n_frames - 1) * frame_step + frame_length - n_samples)\n    assert pad_frames >= 0\n\n    slices = []\n    steps = list(range(0, n_frames + 1, partials_n_frames))\n    for i in range(len(steps) - 1):\n        start_sample = steps[i] * frame_step\n        end_sample = min(n_samples, steps[i + 1] * frame_step + frame_length)\n        start_frame = int(np.ceil(start_sample / frame_step))\n        end_frame = int(np.ceil(end_sample / frame_step))\n        slices += [(start_frame, end_frame)]\n\n    # Ensure we have the right number of frames\n    if len(slices) < partials_n_frames:\n        frames_diff = partials_n_frames - len(slices)\n        if frames_diff <= pad_frames:\n            pad_begin = pad_frames - frames_diff\n            pad_begin = pad_begin // 2\n            pad_end = pad_frames - pad_begin\n\n            slices = [(0, 0)] * pad_begin + slices + [(0, 0)] * pad_end\n        else:\n            extra_pad = frames_diff - pad_frames\n            extra_pad_begin = extra_pad // 2\n            extra_pad_end = extra_pad - extra_pad_begin\n\n            slices = [(0, 0)] * extra_pad_begin + slices + [(0, 0)] * extra_pad_end\n\n    elif len(slices) > partials_n_frames:\n        frames_diff =", "CodeBleu_score": 33.03699708504832}]}
{"instance_id": "34fe5c864f1c", "generation_result": [{"code": "def plot_embedding_as_heatmap(embed, ax=None, title=\"\", shape=None, color_range=(0, 0.30))\n\t\n    if ax is None:\n        ax = plt.gca()\n    if shape is None:\n        shape = (int(np.sqrt(len(embed))), -1)\n    embed = embed.reshape(shape)\n    cmap = cm.get_cmap()\n    im = ax.imshow(embed, vmin=color_range[0], vmax=color_range[1], cmap=cmap)\n    ax.figure.colorbar(im, ax=ax)\n    ax.set(xticklabels=[], yticklabels=[], title=title)\n    return im\n\n", "CodeBleu_score": 54.82072879514111}, {"code": "def plot_embedding_as_heatmap(embed, ax=None, title=\"\", shape=None, color_range=(0, 0.30))\n\t\n    if ax is None:\n        ax = plt.gca()\n    if shape is None:\n        shape = (int(np.sqrt(len(embed))), -1)\n    embed = embed.reshape(shape)\n    cmap = cm.get_cmap()\n    im = ax.imshow(embed, vmin=color_range[0], vmax=color_range[1], cmap=cmap)\n    ax.figure.colorbar(im, ax=ax)\n    ax.set(xticklabels=[], yticklabels=[], title=title)\n    plt.setp(ax.get_xticklabels(), rotation=90)\n    plt.setp(ax.get_yticklabels(), rotation=-90)\n    ax.tick_params(bottom=False, left=False)\n\n", "CodeBleu_score": 58.62494322375991}, {"code": "def plot_embedding_as_heatmap(embed, ax=None, title=\"\", shape=None, color_range=(0, 0.30))\n\t\n    if ax is None:\n        ax = plt.gca()\n    if shape is None:\n        shape = (int(np.sqrt(len(embed))), -1)\n    if np.array(shape).prod() != len(embed):\n        raise ValueError(f\"shape {shape} has a product that is not equal to the length of embed.\")\n    if embed.ndim == 1:\n        embed = embed[None, :]\n    cmap = cm.get_cmap()\n    cax = ax.imshow(embed.reshape(shape), cmap=cmap, vmin=color_range[0], vmax=color_range[1])\n    ax.figure.colorbar(cax)\n    ax.set_xticks([]), ax.set_yticks([])\n    ax.set_title(title)\n\n", "CodeBleu_score": 53.95701019458592}]}
{"instance_id": "92e93d50b62b", "generation_result": [{"code": "def similarity_matrix(self, embeds)\n\t\n    batch_size = embeds.size(0)\n    num_speakers = batch_size // num_utterances\n    centroids_incl = torch.zeros(num_speakers, encoder_embedding_size, device=device)\n    centroids_excl = torch.zeros(num_speakers, encoder_embedding_size, device=device)\n    mask = torch.ones(num_speakers, num_speakers, device=device)\n    mask = mask.fill_diagonal_(0)\n    for i in range(num_speakers):\n        centroids_incl[i] = embeds[i * num_utterances : (i + 1) * num_utterances].mean(dim=0)\n        centroids_incl[i] /= centroids_incl[i].norm()\n        centroids_excl[i] = embeds[torch.cat((torch.arange(0, i * num_utterances), torch.arange((i + 1) * num_utterances, batch_size)))].mean(dim=0)\n        centroids_excl[i] /= centroids_excl[i].norm()\n    sim_matrix = torch.zeros(num_speakers, num_speakers, device=device)\n    for i in range(num_speakers):\n        for j in range(num_speakers):\n            sim_matrix[i, j] = (centroids_incl[i] * centroids_excl[j]).sum()\n    sim_matrix = sim_matrix * 2\n    sim_matrix = sim_matrix - sim_matrix.diag().diag()\n    sim_matrix = sim_matrix * 100\n    sim_matrix = sim_matrix + torch.eye(num_speakers, device=device) * 100\n    sim_matrix = sim_matrix * mask\n    sim_matrix = sim_matrix / sim_matrix.norm()\n    sim_matrix = sim_matrix * 100\n    sim_matrix = sim_matrix + torch.eye(num_speakers, device=device) * 10", "CodeBleu_score": 34.44318438741399}, {"code": "def similarity_matrix(self, embeds)\n\t\n    num_speakers = embeds.shape[0]\n    num_utts = embeds.shape[1]\n    centroids_incl = torch.zeros((num_speakers, embed_dim), device=device)\n    centroids_excl = torch.zeros((num_speakers, embed_dim), device=device)\n    mask = torch.ones((num_speakers, num_speakers), device=device)\n    mask = torch.tril(mask, -1)\n    for i in range(num_speakers):\n        centroids_incl[i] = torch.mean(embeds[i], dim=0)\n        centroids_incl[i] = F.normalize(centroids_incl[i], dim=0)\n        centroids_excl[i] = torch.mean(torch.cat((embeds[i, :i], embeds[i, i + 1:]), dim=0), dim=0)\n        centroids_excl[i] = F.normalize(centroids_excl[i], dim=0)\n    sim_matrix = torch.zeros((num_speakers, num_speakers), device=device)\n    for i in range(num_speakers):\n        sim_matrix[i] = torch.sum(centroids_incl[i] * centroids_incl, dim=1)\n        sim_matrix[i] = torch.sum(centroids_excl[i] * centroids_excl, dim=1)\n    sim_matrix = sim_matrix * mask\n    sim_matrix = sim_matrix / torch.max(sim_matrix)\n    sim_matrix = sim_matrix * 2 - 1\n    return sim_matrix\n\n", "CodeBleu_score": 32.74808651703096}, {"code": "def similarity_matrix(self, embeds)\n\t\n    n_speakers = embeds.shape[0]\n    n_utts = embeds.shape[1]\n    centroids_incl = torch.zeros(n_speakers, embed_dim, device=self.device)\n    centroids_excl = torch.zeros(n_speakers, embed_dim, device=self.device)\n    mask = torch.ones(n_speakers, n_speakers, device=self.device)\n    mask = mask.triu(diagonal=1)\n    for i in range(n_speakers):\n        centroids_incl[i] = torch.mean(embeds[i], dim=0)\n        centroids_excl[i] = torch.mean(torch.cat((embeds[i, 1:], embeds[i, :1]), dim=0), dim=0)\n    centroids_incl = nn.functional.normalize(centroids_incl, p=2, dim=1)\n    centroids_excl = nn.functional.normalize(centroids_excl, p=2, dim=1)\n    sim_matrix = torch.zeros(n_speakers, n_speakers, device=self.device)\n    for i in range(n_speakers):\n        sim_matrix[i] = torch.sum(centroids_incl[i] * centroids_excl, dim=1)\n    sim_matrix = sim_matrix * mask\n    sim_matrix = (sim_matrix - torch.min(sim_matrix)) / (torch.max(sim_matrix) - torch.min(sim_matrix))\n    sim_matrix = sim_matrix * (1 - self.alpha) + self.alpha\n    return sim_matrix\n\n", "CodeBleu_score": 35.01759785986632}]}
{"instance_id": "a6eba54108a1", "generation_result": [{"code": "def loss(self, embeds)\n\t\n\n    # Calculate the number of speakers and utterances per speaker\n    num_speakers = embeds.size(0)\n    num_utts = embeds.size(1)\n\n    # Compute a similarity matrix\n    sim_mat = torch.mm(embeds, embeds.transpose(0, 1))\n\n    # Reshape the similarity matrix\n    sim_mat = sim_mat.view(num_speakers, num_utts, -1)\n\n    # Generate ground truth labels\n    ground_truth = torch.cat([torch.arange(0, num_utts) for _ in range(num_speakers)], dim=0).long()\n\n    # Compute the loss\n    loss = self.criterion(sim_mat, ground_truth)\n\n    # Compute the EER\n    with torch.no_grad():\n        preds = sim_mat.detach().cpu().numpy()\n        preds = preds.reshape((num_speakers * num_utts, -1))\n        labels = ground_truth.detach().cpu().numpy()\n        fpr, tpr, thresholds = roc_curve(labels, preds, pos_label=1)\n        eer = brentq(lambda x: 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n\n    return loss, eer\n\n", "CodeBleu_score": 42.74236604906057}, {"code": "def loss(self, embeds)\n\t\n\n    # embeds: (batch_size, seq_len, embedding_size)\n    batch_size, seq_len, embedding_size = embeds.size()\n    num_speakers = batch_size // num_utterances\n    num_utterances = seq_len\n    # print(f'batch_size: {batch_size}, num_speakers: {num_speakers}, num_utterances: {num_utterances}')\n    # print(f'embeds.shape: {embeds.shape}')\n    # print(f'embeds: {embeds}')\n    # print(f'embeds.view(batch_size, num_utterances, embedding_size, 1): {embeds.view(batch_size, num_utterances, embedding_size, 1)}')\n    # print(f'embeds.view(batch_size, num_utterances, 1, embedding_size): {embeds.view(batch_size, num_utterances, 1, embedding_size)}')\n    # print(f'embeds.view(batch_size, num_utterances, embedding_size, 1) - embeds.view(batch_size, num_utterances, 1, embedding_size): {embeds.view(batch_size, num_utterances, embedding_size, 1) - embeds.view(batch_size, num_utterances, 1, embedding_size)}')\n    # print(f'embeds.view(batch_size, num_utterances, embedding_size, 1) - embeds.view(batch_size, num_utterances, 1, embedding_size): {embeds.view(batch_size, num_utterances, embedding_size, 1) - embeds.view(batch_size, num_utterances, 1, embedding_size)}')\n    # print(f'embeds.view(batch_size, num_utterances, embedding_size, 1) - embeds.view(batch_size, num_utterances, 1, embedding_size): {embeds.view(batch_size, num_utterances, embedding_size, 1) - embeds.view(batch_", "CodeBleu_score": 1.1273405989624512}, {"code": "def loss(self, embeds)\n\t\n    n_speakers = embeds.shape[0]\n    n_utts = embeds.shape[1]\n    embeds = embeds.view(-1, embeds.shape[-1])\n    sim_matrix = torch.matmul(embeds, embeds.transpose(0, 1))\n    sim_matrix = sim_matrix.view(n_speakers, n_utts, -1)\n    gt_labels = torch.zeros(n_speakers, n_utts).long().to(device)\n    gt_labels = gt_labels.view(-1, 1)\n    gt_labels = gt_labels.expand(n_speakers, n_utts)\n    gt_labels = gt_labels.contiguous().view(-1)\n    loss = self.criterion(sim_matrix, gt_labels)\n    with torch.no_grad():\n        eer = compute_eer(sim_matrix.view(-1), gt_labels.float())\n    return loss, eer\n\n", "CodeBleu_score": 29.470272405857457}]}
{"instance_id": "ac7b74b9d1b1", "generation_result": [{"code": "def log_params(self)\n\t\n    import encoder.model_embedding as model_embedding\n    import encoder.data_objects.speaker_verification_dataset as speaker_verification_dataset\n\n    if self.params[\"verbose\"]:\n        print(\"Model parameters:\")\n        for key, value in vars(model_embedding).items():\n            print(key, \":\", value)\n        print(\"Data parameters:\")\n        for key, value in vars(speaker_verification_dataset).items():\n            print(key, \":\", value)\n\n", "CodeBleu_score": 18.126488110280484}, {"code": "def log_params(self)\n\t\n    if self.log_params:\n        params = \"\"\n        params += \"<b>Model Parameters</b>", "CodeBleu_score": 1.2225222417535218}, {"code": "def log_params(self)\n\t\n    if self.config.log_params:\n        params = []\n        params.append(\"Model Parameters:\")\n        for name, value in self.model.named_parameters():\n            params.append(f\"{name}: {value.shape}\")\n        params.append(\"Data Parameters:\")\n        for name, value in self.data.named_parameters():\n            params.append(f\"{name}: {value.shape}\")\n        self.vis.text(\"", "CodeBleu_score": 15.539595251613699}]}
{"instance_id": "4726b7864386", "generation_result": [{"code": "def draw_projections(self, embeds, utterances_per_speaker, step, out_fpath=None, max_speakers=10)\n\t\n    # Limit the number of speakers\n    embeds = embeds[:max_speakers]\n    # Flatten the embeddings\n    embeds = embeds.reshape((embeds.shape[0] * embeds.shape[1], -1))\n    # Assign colors based on speaker identity\n    colors = [self.speakers[i] for i in range(embeds.shape[0])]\n    # Reduce the dimensionality of the embeddings\n    reducer = umap.UMAP(n_components=2, random_state=42, n_epochs=1000)\n    projections = reducer.fit_transform(embeds)\n    # Plot the projections\n    self.viz.scatter(\n        X=projections,\n        opts=dict(\n            title=f'Speaker embeddings at step {step}',\n            markersize=2,\n            markercolor=colors,\n            markercolorscale='Portland',\n            showlegend=False,\n            xlabel='UMAP 1',\n            ylabel='UMAP 2',\n            width=800,\n            height=600,\n            # Update the plot if not disabled\n            **({} if self.plot_update_interval is None else {'update': 'append'})\n        ),\n        win='speaker_embeddings',\n        env=self.viz_env,\n        opts=dict(\n            showlegend=False,\n            xlabel='UMAP 1',\n            ylabel='UMAP 2',\n            width=800,\n            height=600,\n            **({} if self.plot_update_interval is None else {'update': 'append'})\n        ),\n        win='speaker_embeddings',\n        env=self.viz_env\n    )\n    # Save the plot if a file path is specified\n    if out_fpath is not None:\n        self.viz.save([self.viz_env + '/speaker_embeddings'])\n\n", "CodeBleu_score": 36.96544745998508}, {"code": "def draw_projections(self, embeds, utterances_per_speaker, step, out_fpath=None, max_speakers=10)\n\t\n\n    # Reduce dimensionality of embeddings\n    reducer = umap.UMAP(n_components=2, random_state=42)\n    projections = reducer.fit_transform(embeds)\n\n    # Limit the number of speakers\n    if len(utterances_per_speaker) > max_speakers:\n        utterances_per_speaker = utterances_per_speaker[:max_speakers]\n\n    # Flatten embeddings and colors\n    flattened_projections = projections.reshape(-1, 2)\n    flattened_embeds = embeds.reshape(-1, embeds.shape[-1])\n    flattened_colors = []\n    for i, speaker_id in enumerate(utterances_per_speaker):\n        for j in range(len(utterances_per_speaker[speaker_id])):\n            flattened_colors.append(i)\n    flattened_colors = np.array(flattened_colors)\n\n    # Create visdom environment and plot\n    env_name = \"Speaker Embeddings\"\n    self.vis = visdom.Visdom(env=env_name)\n    self.vis.scatter(flattened_projections,\n                     XTitle=\"UMAP 1\",\n                     YTitle=\"UMAP 2\",\n                     win=env_name,\n                     opts=dict(\n                         markersize=3,\n                         markercolor=flattened_colors,\n                         title=f\"Speaker Embeddings (step {step})\",\n                         showlegend=False,\n                         update=\"replace\" if self.visualize else None,\n                     ))\n\n    # Save plot to file if specified\n    if out_fpath is not None:\n        self.vis.save([env_name], out_fpath)\n\n", "CodeBleu_score": 36.864118263822654}, {"code": "def draw_projections(self, embeds, utterances_per_speaker, step, out_fpath=None, max_speakers=10)\n\t\n\n    # Limit the number of speakers\n    if len(embeds) > max_speakers:\n        embeds = embeds[:max_speakers]\n\n    # Flatten the embeddings\n    flattened_embeds = embeds.reshape(embeds.shape[0] * embeds.shape[1], embeds.shape[2])\n\n    # Assign colors based on speaker identity\n    colors = []\n    for i in range(len(embeds)):\n        speaker_id = i + 1\n        speaker_colors = [(1, 0, 0)] * utterances_per_speaker[i]\n        colors.extend(speaker_colors)\n\n    # Reduce the dimensionality of the embeddings using UMAP\n    reducer = umap.UMAP(n_components=2, random_state=42, metric='cosine')\n    projections = reducer.fit_transform(flattened_embeds)\n\n    # Plot the projections\n    self.vis.scatter(\n        X=projections,\n        opts=dict(\n            markersize=5,\n            markercolor=colors,\n            title=f'Speaker Embeddings at Step {step}',\n            legend=['Speaker {}'.format(i + 1) for i in range(len(embeds))]\n        )\n    )\n\n    # Save the plot to a file if specified\n    if out_fpath is not None:\n        self.vis.save([self.env])\n\n", "CodeBleu_score": 40.453786035634174}]}
{"instance_id": "274fa50985c0", "generation_result": [{"code": "def add_speaker_embedding(self, x, speaker_embedding)\n\t\n    # Save the dimensions of the input tensor as human-readable names\n    batch_size, seq_len, dim = x.shape\n\n    # Determine the appropriate dimension index for the speaker embedding\n    if dim == 1:\n        speaker_dim_idx = 2\n    elif dim == 2:\n        speaker_dim_idx = 1\n    else:\n        raise ValueError(f\"Invalid input tensor dimension: {dim}\")\n\n    # Repeat the speaker embedding to match the length of the input text\n    speaker_embedding = speaker_embedding.repeat(1, seq_len, 1)\n\n    # Reshape and transpose the speaker embedding\n    speaker_embedding = speaker_embedding.reshape(batch_size, seq_len, -1).transpose(1, 2)\n\n    # Concatenate the speaker embedding with the input tensor\n    x = torch.cat([x, speaker_embedding], dim=speaker_dim_idx)\n\n    return x\n\n", "CodeBleu_score": 37.28791924913788}, {"code": "def add_speaker_embedding(self, x, speaker_embedding)\n\t\n    x_dims = x.dim()\n    x_dims_names = [\"batch\", \"time\"]\n    if x_dims == 3:\n        x_dims_names.append(\"features\")\n    speaker_embedding_dims = speaker_embedding.dim()\n    speaker_embedding_dims_names = [\"batch\", \"speaker_embedding\"]\n    if speaker_embedding_dims == 3:\n        speaker_embedding_dims_names.append(\"features\")\n    speaker_embedding_dims_index = x_dims_names.index(\n        speaker_embedding_dims_names[1]\n    )\n    speaker_embedding_dims_index = speaker_embedding_dims_index + 1\n    speaker_embedding = speaker_embedding.repeat(\n        x.shape[0], 1, 1\n    )  # (batch, speaker_embedding) -> (batch, speaker_embedding, features)\n    speaker_embedding = speaker_embedding.transpose(\n        1, speaker_embedding_dims_index\n    )  # (batch, speaker_embedding, features) -> (batch, features, speaker_embedding)\n    x = torch.cat([x, speaker_embedding], dim=-1)\n    return x\n\n", "CodeBleu_score": 31.480983919797296}, {"code": "def add_speaker_embedding(self, x, speaker_embedding)\n\t\n    # Save the dimensions of the input tensor as human-readable names\n    batch_size, seq_len, _ = x.size()\n\n    # Determine the appropriate dimension index for the speaker embedding\n    if self.speaker_embedding_dim == 0:\n        speaker_embedding_dim_index = 1\n    else:\n        speaker_embedding_dim_index = 2\n\n    # Repeat the speaker embedding to match the length of the input text\n    speaker_embedding = speaker_embedding.unsqueeze(1).expand(-1, seq_len, -1)\n\n    # Reshape and transpose the speaker embedding\n    speaker_embedding = speaker_embedding.reshape(batch_size * seq_len, -1)\n    speaker_embedding = speaker_embedding.transpose(0, 1)\n\n    # Concatenate the speaker embedding with the input tensor\n    x = torch.cat((x, speaker_embedding), dim=speaker_embedding_dim_index)\n\n    return x\n\n", "CodeBleu_score": 38.32188398586102}]}
{"instance_id": "e6742ea0fe5e", "generation_result": [{"code": "def init_attention(self, encoder_seq_proj)\n\t\n    device = next(self.parameters()).device\n    batch_size, seq_len = encoder_seq_proj.shape[0], encoder_seq_proj.shape[1]\n    self.cumulative_attention = torch.zeros(batch_size, seq_len, device=device)\n    self.attention = torch.zeros(batch_size, seq_len, device=device)\n\n", "CodeBleu_score": 23.669962119294947}, {"code": "def init_attention(self, encoder_seq_proj)\n\t\n    device = next(self.parameters()).device\n    batch_size, seq_len = encoder_seq_proj.shape[:2]\n    self.cumulative_attention = torch.zeros(batch_size, seq_len, device=device)\n    self.attention = torch.zeros(batch_size, seq_len, device=device)\n\n", "CodeBleu_score": 24.30512854379576}, {"code": "def init_attention(self, encoder_seq_proj)\n\t\n    device = next(self.parameters()).device\n    batch_size, seq_len = encoder_seq_proj.shape[:2]\n    self.cum_attn = torch.zeros(batch_size, seq_len, device=device)\n    self.attn = torch.zeros(batch_size, seq_len, device=device)\n\n", "CodeBleu_score": 23.629084166913785}]}
{"instance_id": "4e1fc1ed0ed7", "generation_result": [{"code": "def load(self, path, optimizer=None)\n\t\n    path = Path(path)\n    if path.is_dir():\n        path = path / \"model.pt\"\n    if not path.exists():\n        raise FileNotFoundError(f\"Model checkpoint not found at {path}\")\n    device = next(self.parameters()).device\n    checkpoint = torch.load(path, map_location=device)\n    self.load_state_dict(checkpoint[\"model\"])\n    if optimizer is not None and \"optimizer\" in checkpoint:\n        optimizer.load_state_dict(checkpoint[\"optimizer\"])\n\n", "CodeBleu_score": 61.62226988957056}, {"code": "def load(self, path, optimizer=None)\n\t\n    if not os.path.isfile(path):\n        raise FileNotFoundError(f\"No checkpoint found at {path}\")\n    checkpoint = torch.load(path, map_location=self.device)\n    self.load_state_dict(checkpoint[\"model_state_dict\"])\n    if optimizer is not None and \"optimizer_state_dict\" in checkpoint:\n        optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n\n", "CodeBleu_score": 45.06235515865795}, {"code": "def load(self, path, optimizer=None)\n\t\n    device = next(self.parameters()).device\n    checkpoint = torch.load(path, map_location=device)\n    self.load_state_dict(checkpoint[\"model_state_dict\"])\n    if optimizer is not None and \"optimizer_state_dict\" in checkpoint:\n        optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n\n", "CodeBleu_score": 43.56196850951682}]}
{"instance_id": "178e6aa16c0f", "generation_result": [{"code": "def save(self, path, optimizer=None)\n\t\n    path = Path(path)\n    path.parent.mkdir(parents=True, exist_ok=True)\n    torch.save({\n        'model_state_dict': self.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict() if optimizer else None\n    }, path)\n\n", "CodeBleu_score": 47.55486669629454}, {"code": "def save(self, path, optimizer=None)\n\t\n    if optimizer is not None:\n        torch.save({\n            'model_state_dict': self.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict()\n        }, path)\n    else:\n        torch.save(self.state_dict(), path)\n\n", "CodeBleu_score": 63.81618480133533}, {"code": "def save(self, path, optimizer=None)\n\t\n    if optimizer is None:\n        torch.save(self.state_dict(), path)\n    else:\n        torch.save({\n            'model_state_dict': self.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict()\n        }, path)\n\n", "CodeBleu_score": 57.651555886258265}]}
{"instance_id": "6528fd80e87b", "generation_result": [{"code": "def english_cleaners(text)\n\t\n    text = unidecode(text)\n    text = normalize_numbers(text)\n    text = text.lower()\n    text = re.sub(r\"([.,;:?!%'\\\"\\-_()*&])\", r\" \\1 \", text)\n    text = re.sub(r\"[^a-zA-Z.,;:?!%'\\\"\\-_()*&]+\", r\" \", text)\n    text = re.sub(r\"\\s+\", r\" \", text).strip()\n    return text\n\n", "CodeBleu_score": 28.07902202549859}, {"code": "def english_cleaners(text)\n\t\n    text = unidecode(text)\n    text = normalize_numbers(text)\n    text = text.lower()\n    text = re.sub(r\"([.,?!:;'\\\"])\", r\" \\1 \", text)\n    text = re.sub(r\"[^a-zA-Z.,?!:;'\\\" ]+\", r\" \", text)\n    text = re.sub(r\"\\s+\", r\" \", text).strip()\n    return text\n\n", "CodeBleu_score": 28.700188907427084}, {"code": "def english_cleaners(text)\n\t\n    text = unidecode(text)\n    text = normalize_numbers(text)\n    text = text.lower()\n    text = re.sub(r\"([.,?!])\", r\" \\1 \", text)\n    text = re.sub(r\"[^a-zA-Z.,?!]+\", r\" \", text)\n    text = re.sub(r\"\\s+\", r\" \", text).strip()\n    return text\n\n", "CodeBleu_score": 29.199622951402958}]}
{"instance_id": "8a4fe08e0985", "generation_result": [{"code": "def text_to_sequence(text, cleaner_names)\n\t\n    sequence = []\n    clean_text = prepare_text(text, cleaner_names)\n    for word in clean_text:\n        if word.startswith(\"@\"):\n            sequence += load_arpabet(word[1:])\n        else:\n            sequence += symbols_to_sequence(word)\n    return sequence\n\n", "CodeBleu_score": 19.610676563017144}, {"code": "def text_to_sequence(text, cleaner_names)\n\t\n    sequence = []\n    # Cleaners\n    for name in cleaner_names:\n        cleaner = getattr(cleaners, name)\n        if not cleaner:\n            raise Exception('Unknown cleaner: %s' % name)\n        text = cleaner(text)\n    # Phonemes\n    text = '{' + text + '}'\n    text = re.sub(r'([^\\s\\w]|_)+', r' \\1 ', text)\n    text = re.sub(r'\\s+', ' ', text)\n    sequence = text.split()\n    # symbols\n    for i in range(len(sequence)):\n        if sequence[i] in symbols:\n            sequence[i] = symbols[sequence[i]]\n        else:\n            sequence[i] = symbols['<unk>']\n    sequence.append(symbols['<eos>'])\n    return sequence\n\n", "CodeBleu_score": 31.311128833239348}, {"code": "def text_to_sequence(text, cleaner_names)\n\t\n    sequence = []\n\n    # Processing segments with phonetic notation\n    while \"{\" in text:\n        text_segment = text[: text.find(\"{\")]\n        if text_segment:\n            sequence += text_to_sequence(text_segment, cleaner_names)\n        text = text[text.find(\"{\") :]\n        phonemes = text[1 : text.find(\"}\")].split()\n        for phoneme in phonemes:\n            sequence += phones_to_sequence(phoneme)\n        text = text[text.find(\"}\") + 1 :]\n\n    # Processing regular text\n    for cleaner_name in cleaner_names:\n        cleaner = getattr(cleaners, cleaner_name)\n        if not cleaner:\n            raise Exception(\n                \"Unknown cleaner: {}\".format(cleaner_name)\n            )\n        text = cleaner(text)\n    text = \"{}\".format(text)\n\n    sequence += [symbols.sos_id]\n    for word in text.split():\n        for s in word:\n            sequence += [symbols.characters.index(s)]\n        sequence += [symbols.phonemes.index(\"space\")]\n    sequence.pop()\n    sequence.append(symbols.eos_id)\n\n    return sequence\n\n", "CodeBleu_score": 25.304593807475833}]}
{"instance_id": "e1c25a37af1f", "generation_result": [{"code": "def sequence_to_text(sequence)\n\t\n    result = \"\"\n    for symbol_id in sequence:\n        if symbol_id in [2, 3, 4]:\n            result += \" \"\n        elif symbol_id in [5]:\n            result += \"\u02c8\"\n        elif symbol_id in [6]:\n            result += \"\u02cc\"\n        else:\n            result += symbols[symbol_id]\n    text = re.sub(r\"\\{\", \"{ \", result)\n    text = re.sub(r\"\\}\", \" }\", text)\n    text = re.sub(r\"\\s+\", \" \", text)\n    text = text.strip()\n    return text\n\n", "CodeBleu_score": 36.096587509050146}, {"code": "def sequence_to_text(sequence)\n\t\n    result = ''\n    for symbol_id in sequence:\n        if symbol_id in [2, 3, 4]:\n            result += ' '\n        elif symbol_id in [5]:\n            result += '\u02c8'\n        elif symbol_id in [6]:\n            result += '\u02cc'\n        else:\n            result += symbols[symbol_id]\n    return result.replace('}{', ' } { ')\n\n", "CodeBleu_score": 33.403342627730694}, {"code": "def sequence_to_text(sequence)\n\t\n    result = ''\n    for symbol_id in sequence:\n        if symbol_id in [2, 3, 4]:\n            result += ' '\n        else:\n            result += symbols[symbol_id] + ' '\n    result = result.replace('}{', ' ')\n    result = re.sub(r'\\{[^}]*\\}', '', result)\n    result = result.replace('}{', ' ')\n    result = result.replace('}{', ' ')\n    result = result.replace('}{', ' ')\n    result = result.replace('}{', ' ')\n    result = result.replace('}{', ' ')\n    result = result.replace('}{', ' ')\n    result = result.replace('}{', ' ')\n    result = result.replace('}{', ' ')\n    result = result.replace('}{', ' ')\n    result = result.replace('}{', ' ')\n    result = result.replace('}{', ' ')\n    result = result.replace('}{', ' ')\n    result = result.replace('}{', ' ')\n    result = result.replace('}{', ' ')\n    result = result.replace('}{', ' ')\n    result = result.replace('}{', ' ')\n    result = result.replace('}{', ' ')\n    result = result.replace('}{', ' ')\n    result = result.replace('}{', ' ')\n    result = result.replace('}{', ' ')\n    result = result.replace('}{', ' ')\n    result = result.replace('}{', ' ')\n    result = result.replace('}{', ' ')\n    result = result.replace('}{', ' ')\n    result = result.replace('}{', ' ')\n    result = result.replace('}{', ' ')\n    result = result.replace('}{', ' ')\n    result = result.replace('}{', ' ')\n    result = result.replace('}{', ' ')\n    result = result.replace('}{', ' ')\n    result = result.replace('}{', ' ')\n    result = result.replace('}{', ' ')\n    result = result.replace('}{',", "CodeBleu_score": 29.361431498596147}]}
{"instance_id": "7807cad4b05d", "generation_result": [{"code": "def embed_utterance(fpaths, encoder_model_fpath)\n\t\n    # Load the encoder\n    encoder.load_model(encoder_model_fpath)\n\n    # Compute the speaker embedding of the utterance\n    wav = load_wav(fpaths[0])\n    embed = encoder.embed_utterance(wav)\n    return embed\n\n", "CodeBleu_score": 13.326248899065945}, {"code": "def embed_utterance(fpaths, encoder_model_fpath)\n\t\n    if not encoder.is_loaded():\n        encoder.load_model(encoder_model_fpath)\n\n    sampling_rate = 22050\n    wav = librosa.to_mono(librosa.load(fpaths, sr=sampling_rate)[0])\n    wav = encoder.preprocess_wav(wav, sampling_rate)\n    embed = encoder.embed_utterance(wav)\n    return embed\n\n", "CodeBleu_score": 56.35456344085138}, {"code": "def embed_utterance(fpaths, encoder_model_fpath)\n\t\n\n    if not encoder.is_loaded():\n        encoder.load_model(encoder_model_fpath)\n\n    sampling_rate = 16000\n    wav, _ = librosa.load(fpaths[0], sampling_rate)\n    wav = encoder.preprocess_wav(wav, sampling_rate)\n    embed = encoder.embed_utterance(wav)\n    return embed\n\n", "CodeBleu_score": 57.115764788942826}]}
{"instance_id": "9f860273a325", "generation_result": [{"code": "def create_embeddings(synthesizer_root: Path, encoder_model_fpath: Path, n_processes: int)\n\t\n    # Verify encoder model path\n    if not encoder_model_fpath.exists():\n        raise Exception(\"Encoder model file does not exist\")\n\n    # Verify synthesis directory\n    if not synthesizer_root.exists():\n        raise Exception(\"Synthesizer root directory does not exist\")\n\n    # Get the list of audio files\n    metadata_fpath = synthesizer_root.joinpath(\"train.txt\")\n    with metadata_fpath.open(\"r\") as metadata_file:\n        metadata = [line.split(\"|\") for line in metadata_file]\n    wav_dir = synthesizer_root.joinpath(metadata[0][-1])\n\n    # Create output directory\n    embedding_dir = synthesizer_root.joinpath(\"embeddings\")\n    embedding_dir.mkdir(exist_ok=True)\n\n    # Process the audio files in the dataset\n    args = [(encoder_model_fpath, audio_fpath) for audio_fpath in wav_dir.glob(\"**/*.wav\")]\n    create_embedding = partial(create_embedding_worker, embedding_dir=embedding_dir)\n    with Pool(n_processes) as pool:\n        list(tqdm(pool.imap(create_embedding, args), total=len(args), desc=\"Creating embeddings\"))\n\n", "CodeBleu_score": 54.686112638329675}, {"code": "def create_embeddings(synthesizer_root: Path, encoder_model_fpath: Path, n_processes: int)\n\t\n\n    # Check if encoder model path exists\n    if not encoder_model_fpath.exists():\n        raise Exception(\n            \"Encoder model file does not exist at specified path: {}\".format(\n                encoder_model_fpath\n            )\n        )\n\n    # Check if synthesizer root path exists\n    if not synthesizer_root.exists():\n        raise Exception(\n            \"Synthesizer root directory does not exist at specified path: {}\".format(\n                synthesizer_root\n            )\n        )\n\n    # Get the metadata file path\n    metadata_fpath = synthesizer_root.joinpath(\"train.txt\")\n\n    # Check if metadata file exists\n    if not metadata_fpath.exists():\n        raise Exception(\n            \"Metadata file does not exist at specified path: {}\".format(metadata_fpath)\n        )\n\n    # Get the output directory for the embeddings\n    embed_dir = synthesizer_root.joinpath(\"embed\")\n\n    # Create the embeddings directory if it doesn't exist\n    embed_dir.mkdir(exist_ok=True)\n\n    # Get a list of all the audio files in the metadata file\n    with metadata_fpath.open(\"r\") as metadata_file:\n        metadata = [line.split(\"|\") for line in metadata_file]\n\n    # Get the list of audio file paths and speaker IDs\n    audio_fpaths = [synthesizer_root.joinpath(m[0]) for m in metadata]\n    speaker_ids = [m[2] for m in metadata]\n\n    # Load the encoder model\n    encoder.load_model(str(encoder_model_fpath))\n\n    # Create a multiprocessing pool and use it to process the audio files in parallel\n    with Pool(n_processes) as pool:\n        # Create a function that processes a single audio file and generates an embedding\n        process_fn = partial(\n            process_wav,\n            encoder=encoder,\n            synthesizer_root=synthesizer_root,\n            embed_dir=embed_dir,\n        )\n\n        # Use the multiprocessing pool to process the audio files and generate", "CodeBleu_score": 54.19895477745188}, {"code": "def create_embeddings(synthesizer_root: Path, encoder_model_fpath: Path, n_processes: int)\n\t\n\n    # Verify encoder model path\n    if not encoder_model_fpath.exists():\n        raise Exception(\"Encoder model file does not exist\")\n\n    # Load encoder\n    encoder.load_model(encoder_model_fpath)\n\n    # Compute a fixed-quality embedding for each audio file in a dataset\n    metadata_fpath = synthesizer_root.joinpath(\"train.txt\")\n    embed_dir = synthesizer_root.joinpath(\"embeds\")\n    embed_dir.mkdir(exist_ok=True)\n    with metadata_fpath.open(\"r\") as metadata_file:\n        metadata = [line.split(\"|\") for line in metadata_file]\n        # for i, line in enumerate(metadata):\n        #     metadata[i][2] = int(line[2])\n\n    # Embed the utterances\n    # for i, (wav_fname, embed_fname, _, _, _) in enumerate(metadata):\n    #     # Embed the utterance and save it\n    #     wav_fpath = synthesizer_root.joinpath(wav_fname)\n    #     embedding = encoder.embed_utterance(wav_fpath)\n    #     embed_fpath = embed_dir.joinpath(embed_fname)\n    #     np.save(embed_fpath, embedding, allow_pickle=False)\n\n    # Multiprocessing\n    func = partial(_process_utterance, encoder_model_fpath=encoder_model_fpath)\n    job = Pool(n_processes).imap(func, metadata)\n    for i, (wav_fname, embed_fname, _, _, _) in enumerate(tqdm(job, total=len(metadata))):\n        # Save the embedding\n        embed_fpath = embed_dir.joinpath(embed_fname)\n        np.save(embed_fpath, embedding, allow_pickle=False)\n\n", "CodeBleu_score": 59.21170597545347}]}
{"instance_id": "9eb0abda14a9", "generation_result": [{"code": "def eval_model(attention, mel_prediction, target_spectrogram, input_seq, step,\n               plot_dir, mel_output_dir, wav_dir, sample_num, loss, hparams)\n\t\n    # Plot the attention.\n    plot_attention(attention, step, mel_prediction, target_spectrogram,\n                   symbols, hparams, plot_dir, sample_num, loss)\n\n    # Plot the mel spectrogram.\n    plot_mel_spectrogram(mel_prediction, step, mel_output_dir, sample_num,\n                         hparams)\n\n    # Generate and save the waveform.\n    waveform = audio.inv_mel_spectrogram(mel_prediction.data.cpu().numpy(),\n                                         hparams)\n    audio.save_wav(waveform, Path(wav_dir, '{}.wav'.format(sample_num)))\n\n    # Print the input sequence.\n    print('", "CodeBleu_score": 18.452472198721825}, {"code": "def eval_model(attention, mel_prediction, target_spectrogram, input_seq, step,\n               plot_dir, mel_output_dir, wav_dir, sample_num, loss, hparams)\n\t\n\n    # Get the input sequence and spectrogram\n    input_seq = input_seq.cpu().numpy()\n    target_spectrogram = target_spectrogram.cpu().numpy()\n\n    # Convert the input sequence to text\n    input_seq_text = sequence_to_text(input_seq, symbols)\n\n    # Plot the attention\n    plot_attention(attention, input_seq_text, step, plot_dir)\n\n    # Plot the predicted mel spectrogram\n    plot_spectrogram(mel_prediction, step, mel_output_dir)\n\n    # Convert the mel spectrogram to audio\n    wav = audio.inv_mel_spectrogram(mel_prediction)\n\n    # Save the waveform\n    audio.save_wav(wav, os.path.join(wav_dir, f\"{sample_num}.wav\"))\n\n    # Plot the target and predicted mel spectrograms\n    plot_spectrogram(target_spectrogram, step, mel_output_dir,\n                     title=f\"{hparams.model_type} | Step {step} | Loss {loss:.4f}\")\n\n    # Print the input sequence\n    print(f\"Input sequence: {input_seq_text}\")\n\n", "CodeBleu_score": 22.75248546753269}, {"code": "def eval_model(attention, mel_prediction, target_spectrogram, input_seq, step,\n               plot_dir, mel_output_dir, wav_dir, sample_num, loss, hparams)\n\t\n    # Get the predicted mel spectrogram.\n    predicted_mel_spectrogram = mel_prediction.detach().cpu().numpy()\n\n    # Get the target spectrogram.\n    target_spectrogram = target_spectrogram.detach().cpu().numpy()\n\n    # Get the input sequence.\n    input_seq = input_seq.detach().cpu().numpy()\n\n    # Get the input text.\n    input_text = sequence_to_text(input_seq, symbols)\n\n    # Save the attention plot.\n    plot_attention(attention.detach().cpu().numpy(), input_text, step, plot_dir)\n\n    # Save the predicted mel spectrogram.\n    plot_mel_spectrogram(predicted_mel_spectrogram, step, mel_output_dir)\n\n    # Save the target mel spectrogram.\n    plot_mel_spectrogram(target_spectrogram, step, mel_output_dir)\n\n    # Save the predicted waveform.\n    plot_waveform(mel_prediction.detach().cpu().numpy(), step, wav_dir)\n\n    # Save the target waveform.\n    plot_waveform(target_spectrogram, step, wav_dir)\n\n    # Save the comparison plot.\n    plot_comparison(predicted_mel_spectrogram, target_spectrogram, input_text, step, plot_dir)\n\n    # Print the input sequence.\n    print(f\"Step {step}, sample {sample_num}\")\n    print(f\"Input sequence: {input_text}\")\n\n", "CodeBleu_score": 23.311253821813786}]}
{"instance_id": "6bad4f500286", "generation_result": [{"code": "def num_params(self)\n\t\n    n_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n    n_params = n_params / 1e6\n    print(f\"Number of parameters: {n_params:.3f}M\")\n\n", "CodeBleu_score": 15.505925999266967}, {"code": "def num_params(self)\n\t\n    num_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n    num_params = num_params / 1e6\n    print(f\"Number of trainable parameters: {num_params:.3f}M\")\n\n", "CodeBleu_score": 16.701296184942937}, {"code": "def num_params(self)\n\t\n    num_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n    num_params = num_params / 1000000\n    num_params = round(num_params, 3)\n    print(\"Number of trainable parameters:\", num_params, \"million\")\n\n", "CodeBleu_score": 20.573728143426077}]}
{"instance_id": "78b876054bd4", "generation_result": [{"code": "def pad_tensor(self, x, pad, side='both')\n\t\n    if pad == 0:\n        return x\n    else:\n        if side == 'both':\n            left = pad // 2\n            right = pad - left\n        elif side == 'before':\n            left = pad\n            right = 0\n        elif side == 'after':\n            left = 0\n            right = pad\n        else:\n            raise ValueError('Unknown padding side')\n        size = x.shape[0]\n        if x.is_cuda:\n            padded = torch.zeros(size + pad, device='cuda')\n        else:\n            padded = torch.zeros(size + pad)\n        padded[left:size + right] = x\n        return padded\n\n", "CodeBleu_score": 27.180762703884575}, {"code": "def pad_tensor(self, x, pad, side='both')\n\t\n    if pad == 0:\n        return x\n    if side == 'both':\n        pad_before = pad_after = pad\n    elif side == 'before':\n        pad_before = pad\n        pad_after = 0\n    elif side == 'after':\n        pad_before = 0\n        pad_after = pad\n    else:\n        raise ValueError('Invalid side: {}'.format(side))\n    if pad_before > 0:\n        x = torch.cat([torch.zeros(x.size(0), pad_before, x.size(2)).type_as(x), x], dim=1)\n    if pad_after > 0:\n        x = torch.cat([x, torch.zeros(x.size(0), pad_after, x.size(2)).type_as(x)], dim=1)\n    return x\n\n", "CodeBleu_score": 29.90124406191968}, {"code": "def pad_tensor(self, x, pad, side='both')\n\t\n    if side == 'both':\n        size = x.size(0) + pad\n        new_tensor = torch.zeros(size).type_as(x)\n        new_tensor[:x.size(0)] = x\n        return new_tensor\n    elif side == 'after':\n        return torch.cat([x, torch.zeros(pad).type_as(x)])\n    elif side == 'before':\n        return torch.cat([torch.zeros(pad).type_as(x), x])\n\n", "CodeBleu_score": 28.324674762716484}]}
{"instance_id": "a7ed1715a3df", "generation_result": [{"code": "def save_attention(attn, path)\n\t\n    import matplotlib.pyplot as plt\n    fig = plt.figure(figsize=(10, 10))\n    ax = fig.add_subplot(111)\n    ax.imshow(attn, cmap='Blues', alpha=0.5)\n    ax.set_xticks(range(len(attn)))\n    ax.set_yticks(range(len(attn)))\n    ax.set_xticklabels(range(len(attn)))\n    ax.set_yticklabels(range(len(attn)))\n    ax.set_xlabel('Head')\n    ax.set_ylabel('Position')\n    ax.set_title('Attention Matrix')\n    plt.savefig(path + '.png')\n    plt.close(fig)\n\n", "CodeBleu_score": 44.979157512253906}, {"code": "def save_attention(attn, path)\n\t\n    import matplotlib.pyplot as plt\n    fig = plt.figure()\n    ax = fig.add_subplot(1, 1, 1)\n    ax.imshow(attn, cmap='viridis')\n    ax.set_xticks([])\n    ax.set_yticks([])\n    fig.savefig(path, format='png')\n    plt.close(fig)\n\n", "CodeBleu_score": 44.155860985833485}, {"code": "def save_attention(attn, path)\n\t\n    import matplotlib.pyplot as plt\n\n    plt.figure(figsize=(15, 15))\n    plt.imshow(attn, cmap=\"Blues\")\n    plt.savefig(path + \".png\")\n    plt.close()\n\n", "CodeBleu_score": 41.77762431578566}]}
{"instance_id": "19375c9f44a2", "generation_result": [{"code": "def plot(array)\n\t\n    import matplotlib.pyplot as plt\n    fig = plt.figure(figsize=(10, 5))\n    ax = fig.add_subplot(111)\n    ax.set_xlabel('Index', fontsize=15, color='#555555')\n    ax.set_ylabel('Value', fontsize=15, color='#555555')\n    ax.tick_params(axis='x', colors='#555555')\n    ax.tick_params(axis='y', colors='#555555')\n    ax.plot(array)\n    plt.show()\n\n", "CodeBleu_score": 35.49979698691138}, {"code": "def plot(array)\n\t\n    import matplotlib.pyplot as plt\n\n    # Create a figure with a specified size\n    fig = plt.figure(figsize=(10, 5))\n\n    # Add a subplot\n    ax = fig.add_subplot(111)\n\n    # Customize the axis labels and tick parameters with specific colors and font sizes\n    ax.set_xlabel(\"Index\", fontsize=14, color=\"blue\")\n    ax.set_ylabel(\"Value\", fontsize=14, color=\"blue\")\n    ax.tick_params(axis=\"x\", labelsize=12, color=\"blue\")\n    ax.tick_params(axis=\"y\", labelsize=12, color=\"blue\")\n\n    # Plot the array on the subplot\n    ax.plot(array, color=\"red\")\n\n    # Show the plot\n    plt.show()\n\n", "CodeBleu_score": 35.49858327201423}, {"code": "def plot(array)\n\t\n    # Create a figure with a specified size\n    fig = plt.figure(figsize=(10, 5))\n    # Add a subplot\n    ax = fig.add_subplot(111)\n    # Customize the axis labels and tick parameters with specific colors and font sizes\n    ax.set_xlabel(\"Time (s)\", color=\"white\", fontsize=14)\n    ax.set_ylabel(\"Amplitude\", color=\"white\", fontsize=14)\n    ax.tick_params(axis=\"x\", colors=\"white\", labelsize=12)\n    ax.tick_params(axis=\"y\", colors=\"white\", labelsize=12)\n    # Plot the array on the customized subplot\n    ax.plot(array)\n    # Display the plot\n    plt.show()\n\n", "CodeBleu_score": 28.64426330898324}]}
{"instance_id": "ff94c425920c", "generation_result": [{"code": "def write_to_csv(image_name, prediction, confidence)\n\t\n    data = {\"image_name\": image_name, \"prediction\": prediction, \"confidence\": confidence}\n    with open(\"predictions.csv\", \"a\", newline=\"\") as file:\n        writer = csv.DictWriter(file, fieldnames=data.keys())\n        if file.tell() == 0:\n            writer.writeheader()\n        writer.writerow(data)\n\n", "CodeBleu_score": 55.58380359476004}, {"code": "def write_to_csv(image_name, prediction, confidence)\n\t\n    data = {\"image_name\": image_name, \"prediction\": prediction, \"confidence\": confidence}\n    with open(f\"{image_name}.csv\", \"a\", newline=\"\") as f:\n        writer = csv.DictWriter(f, fieldnames=data.keys())\n        if f.tell() == 0:\n            writer.writeheader()\n        writer.writerow(data)\n\n", "CodeBleu_score": 63.869221220243624}, {"code": "def write_to_csv(image_name, prediction, confidence)\n\t\n    data = {\"image_name\": image_name, \"prediction\": prediction, \"confidence\": confidence}\n    with open(\"predictions.csv\", \"a\", newline=\"\") as csvfile:\n        fieldnames = [\"image_name\", \"prediction\", \"confidence\"]\n        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n        if csvfile.tell() == 0:\n            writer.writeheader()\n        writer.writerow(data)\n\n", "CodeBleu_score": 49.76022435370774}]}
{"instance_id": "436161602009", "generation_result": [{"code": "def export_torchscript(model, im, file, optimize, prefix=colorstr(\"TorchScript:\"))\n\t\n    # Log start\n    LOGGER.info(f\"{prefix} Exporting {file}\")\n    # Trace model\n    with torch.no_grad():\n        if optimize:\n            model = optimize_for_mobile(model)\n        model = torch.jit.trace(model, im)\n    # Save model\n    if optimize:\n        file = Path(file).with_suffix(\".pt\")\n    else:\n        file = Path(file).with_suffix(\".torchscript.pt\")\n    model.save(file)\n    # Log export\n    LOGGER.info(f\"{prefix} Export complete ({file_size(file)})\")\n    return str(file), None\n\n", "CodeBleu_score": 34.67068206322358}, {"code": "def export_torchscript(model, im, file, optimize, prefix=colorstr(\"TorchScript:\"))\n\t\n    print(f\"{prefix} Exporting {file}\")\n    t0 = time.time()\n\n    # Trace model\n    with torch.no_grad():\n        if isinstance(model, ClassificationModel):\n            model.forward(im)\n        elif isinstance(model, DetectionModel):\n            model.forward(im)\n        elif isinstance(model, SegmentationModel):\n            model.forward(im)\n        else:\n            raise ValueError(f\"Unsupported model type: {type(model)}\")\n\n    # Export\n    with torch.no_grad():\n        torch.jit.save(\n            torch.jit.trace(model, im), file\n        )  # torch.jit.trace(model, im, check_trace=False).save(file)\n\n    # Profile\n    model_info = f\"{file_size(file)} ({time.time() - t0:.1f}s)\"\n    print(f\"{prefix} Export success: {model_info}\")\n\n    if optimize:\n        model_info = optimize_for_mobile(file)\n        print(f\"{prefix} Optimized model ({model_info})\")\n\n    return file\n\n", "CodeBleu_score": 30.614697627847764}, {"code": "def export_torchscript(model, im, file, optimize, prefix=colorstr(\"TorchScript:\"))\n\t\n    LOGGER.info(f\"{prefix} exporting {file}\")\n    with torch.no_grad():\n        model.warmup(im)\n    if optimize:\n        model = optimize_for_mobile(model)\n    torch.jit.save(torch.jit.script(model), file)\n    LOGGER.info(f\"{prefix} optimized model: {file}\")\n    return file, None\n\n", "CodeBleu_score": 23.625228973279018}]}
{"instance_id": "78335487ac1c", "generation_result": [{"code": "def export_coreml(model, im, file, int8, half, nms, prefix=colorstr(\"CoreML:\")):\n\t\n    try:\n        import coremltools as ct\n    except ImportError:\n        raise ImportError(\n            \"Please install coremltools:\n  $ pip install coremltools\"\n        ) from None\n    print(f\"{prefix} Exporting to CoreML...\")\n    file = Path(file).with_suffix(\".mlmodel\")\n    if nms:\n        model = ct.convert(\n            model,\n            inputs=[\n                ct.ImageType(\n                    name=\"image\",\n                    shape=im.shape,\n                    scale=1 / 255.0,\n                    bias=[0, 0, 0],\n                )\n            ],\n        )\n    else:\n        model = ct.convert(\n            model,\n            inputs=[\n                ct.ImageType(\n                    name=\"image\",\n                    shape=im.shape,\n                    scale=1 / 255.0,\n                    bias=[-1, -1, -1],\n                )\n            ],\n        )\n    if int8:\n        model.convert(convert_to=\"INT8\")\n    if half:\n        model.convert(convert_to=\"FP16\")\n    model.save(file)\n    print(f\"{prefix} Export complete ({file_size(file)})\")\n    return file, model\n\n", "CodeBleu_score": 38.58810465894066}, {"code": "def export_coreml(model, im, file, int8, half, nms, prefix=colorstr(\"CoreML:\")):\n\t\n    try:\n        import coremltools as ct\n    except ImportError:\n        raise ImportError(\"Please install coremltools:\n  $ pip install coremltools\")\n\n    print(f\"{prefix} Exporting {file}\")\n\n    # Wrap model with iOSModel for NMS\n    if nms:\n        model = ct.models.iOSModel(model)\n\n    # Trace model\n    with torch.no_grad():\n        trace_model = torch.jit.trace(model, im)\n\n    # Convert to CoreML\n    mlmodel = ct.convert(\n        trace_model,\n        inputs=[ct.ImageType(name=\"image\", shape=im.shape, scale=1 / 255.0, bias=[0, 0, 0])],\n    )\n\n    # Apply quantization\n    if int8:\n        mlmodel = ct.models.quantization.quantize(mlmodel, calibration_dataset=im)\n\n    # Save model\n    mlmodel.save(file)\n    return file, mlmodel\n\n", "CodeBleu_score": 38.050226185130064}, {"code": "def export_coreml(model, im, file, int8, half, nms, prefix=colorstr(\"CoreML:\")):\n\t\n    # Check coremltools requirement\n    try:\n        import coremltools as ct\n    except ImportError:\n        raise ImportError(\"Please install coremltools:\n  $ pip install coremltools\")\n\n    # Log export start\n    LOGGER.info(f\"{prefix} Exporting {file}\")\n\n    # Set file suffix to .mlmodel\n    file = Path(file).with_suffix(\".mlmodel\")\n\n    # Wrap model with iOSModel for NMS support\n    if nms:\n        model = ct.models.iOSModel(model)\n\n    # Trace model\n    LOGGER.info(f\"{prefix} Tracing model\")\n    traced_model = torch.jit.trace(model, im)\n\n    # Convert to CoreML\n    LOGGER.info(f\"{prefix} Converting to CoreML\")\n    coreml_model = ct.convert(traced_model, inputs=[ct.ImageType(name=\"input_image\", shape=im.shape)])\n\n    # Apply quantization if needed and supported\n    if int8 or half:\n        LOGGER.info(f\"{prefix} Applying quantization\")\n        coreml_model = coreml_model.apply_quantization(\n            \"linear\",\n            input_scale=1 / 255.0 if half else 1 / 127.5,\n            input_bias=[0] if half else [-127.5],\n            output_scale=1 / 255.0 if half else 1 / 127.5,\n            output_bias=[0] if half else [-127.5],\n            channel_axis=1 if half else 3,\n            input_channel_axis=1 if half else 3,\n        )\n\n    # Save CoreML model\n    LOGGER.info(f\"{prefix} Saving CoreML model to {file}\")\n    coreml_model.save(file)\n\n    # Return file path and CoreML model\n    return file, coreml_model\n\n", "CodeBleu_score": 44.7269697741333}]}
{"instance_id": "80233cf71580", "generation_result": [{"code": "def generate_individual(input_ranges, individual_length)\n\t\n    individual = []\n    for _ in range(individual_length):\n        individual.append(random.uniform(*input_ranges))\n    return individual\n\n", "CodeBleu_score": 38.14759799230568}, {"code": "def generate_individual(input_ranges, individual_length)\n\t\n    individual = []\n    for i in range(individual_length):\n        individual.append(random.uniform(input_ranges[0], input_ranges[1]))\n    return individual\n\n", "CodeBleu_score": 45.11328150959641}, {"code": "def generate_individual(input_ranges, individual_length)\n\t\n    individual = []\n    for i in range(individual_length):\n        individual.append(random.uniform(*input_ranges[i]))\n    return individual\n\n", "CodeBleu_score": 45.41230804837097}]}
{"instance_id": "5ea979cb4a8e", "generation_result": [{"code": "def marginal_log_mean_coeff(self, t)\n\t\n    if self.config.scheduler_type == \"discrete\":\n        return -math.log(self.num_train_timesteps) * t\n    elif self.config.scheduler_type == \"linear\":\n        return math.log(self.num_train_timesteps) * (1 - t)\n    elif self.config.scheduler_type == \"cosine\":\n        return math.log(self.num_train_timesteps) * 0.5 * (1 + math.cos(math.pi * t))\n    else:\n        raise ValueError(\n            f\"Unknown scheduler type {self.config.scheduler_type}. Should be one of 'discrete', 'linear', or 'cosine'.\"\n        )\n\n", "CodeBleu_score": 28.527718715386264}, {"code": "def marginal_log_mean_coeff(self, t)\n\t\n    if self.config.schedule == \"discrete\":\n        return -math.log(self.config.discrete_schedule[t])\n    elif self.config.schedule == \"linear\":\n        return math.log(1 - self.config.beta_end) - t * (math.log(1 - self.config.beta_end) - math.log(self.config.beta_start)) / self.config.num_train_timesteps\n    elif self.config.schedule == \"cosine\":\n        return math.log(1 - self.config.beta_end) - math.log(self.config.beta_start)\n    else:\n        raise NotImplementedError(f\"{self.config.schedule} schedule is not implemented\")\n\n", "CodeBleu_score": 29.441416133400477}, {"code": "def marginal_log_mean_coeff(self, t)\n\t\n    if self.schedule == \"discrete\":\n        return -math.log(self.T) * t\n    elif self.schedule == \"linear\":\n        return -math.log(self.T) * (t - (1 + t) * self.beta)\n    elif self.schedule == \"cosine\":\n        return -math.log(self.T) * 0.5 * (1 + math.cos(math.pi * t))\n    else:\n        raise ValueError(f\"Unknown schedule: {self.schedule}\")\n\n", "CodeBleu_score": 26.918727445612756}]}
{"instance_id": "7da4fae5a278", "generation_result": [{"code": "def inverse_lambda(self, lamb)\n\t\n    if self.schedule == \"linear\":\n        t = (self.beta_max - lamb) / (self.beta_max - self.beta_min)\n        return t\n    elif self.schedule == \"quadratic\":\n        t = (self.beta_max - lamb) / (self.beta_max - self.beta_min)\n        return t\n    elif self.schedule == \"discrete\":\n        t = torch.argmin(torch.abs(self.log_alpha - lamb))\n        return t\n    elif self.schedule == \"cosine\":\n        t = math.acos(lamb)\n        return t\n    else:\n        raise ValueError(\n            \"Unknown schedule. Choose from 'linear', 'quadratic', 'discrete', or 'cosine'.\"\n        )\n\n", "CodeBleu_score": 23.589961315240096}, {"code": "def inverse_lambda(self, lamb)\n\t\n    if self.schedule == \"linear\":\n        return self.beta * (math.acosh(lamb) - self.beta)\n    elif self.schedule == \"quadratic\":\n        return self.beta * (math.acosh(lamb) - self.beta) ** 2\n    elif self.schedule == \"discrete\":\n        return torch.argmax(self.log_alpha >= torch.log(lamb)).item()\n    else:\n        return torch.acos(1 / lamb) / self.beta\n\n", "CodeBleu_score": 14.634765037929462}, {"code": "def inverse_lambda(self, lamb)\n\t\n    if self.schedule == \"linear\":\n        beta = self.betas[self.T - 1]\n        for i in range(self.T - 1, -1, -1):\n            beta = self.betas[i - 1] + (beta - self.betas[i - 1]) / math.exp(\n                self.log_alpha_hats[i - 1] / self.alpha_bar\n            )\n            if lamb < beta:\n                break\n        t = i\n        lamb = lamb - beta\n        for i in range(t, self.T):\n            lamb = lamb * math.exp(-self.log_alpha_hats[i] / self.alpha_bar)\n    elif self.schedule == \"quad\":\n        log_alpha = math.log(self.alpha_bar)\n        for i in range(self.T):\n            log_alpha = log_alpha + self.log_alpha_hats[i]\n            if lamb < log_alpha:\n                break\n        t = i\n        lamb = lamb - log_alpha\n    elif self.schedule == \"cosine\":\n        log_alpha = math.log(self.alpha_bar)\n        for i in range(self.T):\n            log_alpha = log_alpha + self.log_alpha_hats[i]\n            if lamb < log_alpha:\n                break\n        t = i\n        lamb = lamb - log_alpha\n        lamb = lamb / self.alpha_bar\n        lamb = math.acos(lamb) / math.pi\n        lamb = lamb * self.T\n    else:\n        raise NotImplementedError(\n            f\"The inverse lambda function is not implemented for {self.schedule} schedule.\"\n        )\n    return t, lamb\n\n", "CodeBleu_score": 26.820137242776916}]}
{"instance_id": "a519c853a2b2", "generation_result": [{"code": "def noise_pred_fn(x, t_continuous, cond=None)\n\t\n    if cond is not None:\n        x = torch.cat([x, cond], dim=1)\n    t_discrete = t_continuous.type(torch.int64)\n    t_discrete = t_discrete.view(-1, *([1] * (len(x.shape) - 1)))\n    t_discrete = t_discrete.expand(-1, *x.shape[1:])\n    t_discrete = t_discrete.contiguous()\n    noise = torch.randn_like(x)\n    return noise\n\n", "CodeBleu_score": 21.085434082351473}, {"code": "def noise_pred_fn(x, t_continuous, cond=None)\n\t\n    batch_size = x.shape[0]\n    t_discrete = torch.tensor(t_continuous, device=x.device).long()\n    t_discrete = t_discrete.view(batch_size, *([1] * (len(x.shape) - 1)))\n    t_discrete = t_discrete.expand(x.shape)\n    eps = model(x, t_discrete, cond)\n    return eps\n\n", "CodeBleu_score": 8.062629727600285}, {"code": "def noise_pred_fn(x, t_continuous, cond=None)\n\t\n    if cond is not None:\n        x = torch.cat([x, cond], dim=1)\n\n    # expand t to match batch size\n    t_continuous = t_continuous.expand(x.shape[0], -1)\n    # apply model\n    x = model(x, t_continuous)\n    # apply transform\n    if model_type == 'ddpm':\n        return x\n    elif model_type == 'ddim':\n        return x * math.sqrt(1 - alpha_cumprod[t_continuous])\n    elif model_type == 'ddpm2':\n        return x * math.sqrt(1 - alpha_cumprod[t_continuous]) + beta_cumprod[t_continuous]\n\n", "CodeBleu_score": 22.33842266067866}]}
{"instance_id": "96a9b7d54823", "generation_result": [{"code": "def dynamic_thresholding_fn(self, x0, t=None)\n\t\n    # Reshape x0 to compute the quantile of absolute values\n    x0_flat = x0.reshape(-1)\n    quantile = torch.quantile(torch.abs(x0_flat), 0.999)\n\n    # Determine the scaling factor based on the quantile\n    scaling_factor = torch.max(torch.tensor(1.0), quantile / 1.0)\n\n    # Clamp x0 values based on the scaling factor\n    x0_clamped = torch.clamp(x0, -scaling_factor, scaling_factor)\n\n    # Scale the clamped tensor and return it\n    return x0_clamped / scaling_factor\n\n", "CodeBleu_score": 15.684640071007848}, {"code": "def dynamic_thresholding_fn(self, x0, t=None)\n\t\n    if t is None:\n        t = self.t\n    x0 = x0.reshape(x0.shape[0], -1)\n    x0_abs = torch.abs(x0)\n    x0_quantile = torch.quantile(x0_abs, t, dim=1)\n    scaling_factor = x0_quantile.unsqueeze(1)\n    x0 = torch.clamp(x0, -scaling_factor, scaling_factor)\n    x0 = x0 / scaling_factor\n    return x0\n\n", "CodeBleu_score": 39.740278612500255}, {"code": "def dynamic_thresholding_fn(self, x0, t=None)\n\t\n    x = x0.reshape(-1)\n    q = torch.quantile(torch.abs(x), 0.9999)\n    scaling_factor = math.sqrt(2) * q\n    x = torch.clamp(x, -scaling_factor, scaling_factor)\n    x = x / scaling_factor\n    return x.reshape(x0.shape)\n\n", "CodeBleu_score": 14.274757723314979}]}
{"instance_id": "59c6ec6d7a6a", "generation_result": [{"code": "def data_prediction_fn(self, x, t)\n\t\n    x0 = x\n    threshold = self.threshold_ratio * torch.max(x0)\n    x0 = torch.clamp(x0, min=-threshold, max=threshold)\n    x0 = x0 / torch.max(torch.abs(x0))\n    return x0\n\n", "CodeBleu_score": 14.486032037898292}, {"code": "def data_prediction_fn(self, x, t)\n\t\n    t_ratio = self.threshold_ratio(t)\n    t_max = self.threshold_max(t)\n    x = torch.clamp(x, -t_max, t_max)\n    x = (x + t_max) / (2 * t_max)\n    return x\n\n", "CodeBleu_score": 4.065250789700892}, {"code": "def data_prediction_fn(self, x, t)\n\t\n    x = x.clone()\n    x_threshold = x.abs() * self.threshold_ratio\n    x_threshold = x_threshold.clamp(max=self.max_threshold)\n    x_normalized = x / x_threshold\n    return x_normalized\n\n", "CodeBleu_score": 9.26538120052523}]}
{"instance_id": "65e3618468a2", "generation_result": [{"code": "def predict_eps_sigma(model, input, sigma_in, **kwargs)\n\t\n    sigma_in = sigma_in.reshape(input.shape)\n    input_scaled = input / sigma_in\n    pred = model(input_scaled, **kwargs)\n    pred = pred * sigma_in\n    return pred\n\n", "CodeBleu_score": 20.068025398603424}, {"code": "def predict_eps_sigma(model, input, sigma_in, **kwargs)\n\t\n    # Reshape sigma to match the dimensions of the input\n    sigma_in = sigma_in.reshape(input.shape)\n\n    # Scale the input\n    scaled_input = input / sigma_in\n\n    # Compute the prediction\n    pred = model(scaled_input, **kwargs)\n\n    # Adjust the prediction\n    adjusted_pred = pred * sigma_in\n\n    return adjusted_pred\n\n", "CodeBleu_score": 20.76048811601193}, {"code": "def predict_eps_sigma(model, input, sigma_in, **kwargs)\n\t\n    sigma_in = sigma_in.reshape(input.shape)\n    input_scaled = input / sigma_in\n    output = model(input_scaled, **kwargs)\n    output_scaled = output * sigma_in\n    return output_scaled\n\n", "CodeBleu_score": 21.117400317197607}]}
{"instance_id": "b35846dfbc66", "generation_result": [{"code": "def cal_intergrand(beta_0, beta_1, taus)\n\t\n    taus_clone = taus.clone().requires_grad_(True)\n    alpha = torch.exp(beta_0 - beta_1 * taus_clone)\n    log_alpha = torch.log(alpha)\n    log_alpha.backward(torch.ones_like(log_alpha))\n    integrand = alpha * taus_clone.grad\n    return integrand\n\n", "CodeBleu_score": 14.545879046519325}, {"code": "def cal_intergrand(beta_0, beta_1, taus)\n\t\n    taus_clone = taus.clone().requires_grad_(True)\n    alpha = torch.exp(beta_0 + beta_1 * taus_clone)\n    log_alpha = torch.log(alpha)\n    log_alpha.backward(torch.ones_like(log_alpha))\n    integrand = taus_clone.grad * alpha\n    return integrand\n\n", "CodeBleu_score": 14.339281262769893}, {"code": "def cal_intergrand(beta_0, beta_1, taus)\n\t\n    taus_clone = taus.clone()\n    taus_clone.requires_grad = True\n    alpha = torch.exp(beta_0 + beta_1 * taus_clone)\n    log_alpha = torch.log(alpha)\n    log_alpha.backward(retain_graph=True)\n    grad_log_alpha = taus_clone.grad\n    integrand = grad_log_alpha / alpha\n    return integrand\n\n", "CodeBleu_score": 23.90248532490462}]}
{"instance_id": "5f3667566a10", "generation_result": [{"code": "def sample_euler_ancestral(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None)\n\t\n    if extra_args is None:\n        extra_args = {}\n    if noise_sampler is None:\n        noise_sampler = lambda: torch.randn_like(x)\n    x_t = x\n    for sigma in tqdm(sigmas, desc='Sampling', disable=disable):\n        if callback is not None:\n            callback({'x_t': x_t, 'sigma': sigma})\n        if sigma > 0:\n            x_t = model(x_t, sigma, **extra_args)\n        x_t = x_t + eta * s_noise * noise_sampler()\n    return x_t\n\n", "CodeBleu_score": 26.77294884053978}, {"code": "def sample_euler_ancestral(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None)\n\t\n    if extra_args is None:\n        extra_args = {}\n    if noise_sampler is None:\n        noise_sampler = lambda sigma: torch.randn_like(x) * sigma\n    for sigma in tqdm(sigmas, disable=disable):\n        x = model.denoise(x, sigma, **extra_args)\n        if callback:\n            callback({\"x\": x, \"sigma\": sigma})\n        x_ancestral = x + eta * s_noise * noise_sampler(sigma)\n        x = x + sigma * (model.predict(x, sigma, **extra_args) - model.predict(x_ancestral, sigma, **extra_args))\n    return x\n\n", "CodeBleu_score": 29.210053526152326}, {"code": "def sample_euler_ancestral(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None)\n\t\n    if extra_args is None:\n        extra_args = {}\n    if noise_sampler is None:\n        noise_sampler = lambda sigma: torch.randn_like(x) * sigma\n    for sigma in tqdm(sigmas, disable=disable):\n        if callback is not None:\n            callback(x, sigma)\n        x = model(x, sigma, **extra_args)\n        x = x + eta * s_noise * noise_sampler(sigma)\n    return x\n\n", "CodeBleu_score": 23.0678518319311}]}
{"instance_id": "6261ded88df2", "generation_result": [{"code": "def linear_multistep_coeff(order, t, i, j)\n\t\n    if order > j:\n        raise ValueError(\n            f\"Order {order} is too high for step index {j}.\"\n        )\n    def prod_term(t, i, j):\n        return math.prod([(t - t[i + k]) / (t[j] - t[i + k]) for k in range(order) if k != j])\n    return integrate.quad(lambda t: prod_term(t, i, j), t[i], t[i + 1])[0]\n\n", "CodeBleu_score": 38.4538793582788}, {"code": "def linear_multistep_coeff(order, t, i, j)\n\t\n    if order > j:\n        raise ValueError(\"Order too high for step index\")\n    def prod(k):\n        if k == j:\n            return 1\n        return (t[i+1] - t[k]) / (t[i] - t[k])\n    return integrate.quad(prod, i, i+1)[0]\n\n", "CodeBleu_score": 23.47243753716692}, {"code": "def linear_multistep_coeff(order, t, i, j)\n\t\n    if order > 1 and j >= i:\n        raise ValueError(\"Order must be less than or equal to 1.\")\n    def prod_term(t, i, j):\n        if j == i:\n            return 1\n        else:\n            return (t[i] - t[j]) / (t[i] - t[0])\n    return integrate.quad(lambda x: prod_term(t, i, j), t[i], t[i + 1])[0]\n\n", "CodeBleu_score": 26.90130752274339}]}
{"instance_id": "4ee025e33ab1", "generation_result": [{"code": "def sample_lcm(model, x, sigmas, extra_args=None, callback=None, disable=None, noise_sampler=None)\n\t\n    if extra_args is None:\n        extra_args = {}\n    if noise_sampler is None:\n        noise_sampler = lambda: torch.randn_like(x)\n    if disable is None:\n        disable = False\n    if disable:\n        pbar = None\n    else:\n        pbar = trange(len(sigmas), desc=\"LCM Sampling\", disable=disable)\n    for i, sigma in enumerate(sigmas):\n        if callback is not None:\n            callback(i, sigma, x)\n        if sigma > 0:\n            x = x + sigma * noise_sampler()\n        x = model(x, sigma, **extra_args)\n        if pbar is not None:\n            pbar.update()\n    if pbar is not None:\n        pbar.close()\n    return x\n\n", "CodeBleu_score": 36.88018816447232}, {"code": "def sample_lcm(model, x, sigmas, extra_args=None, callback=None, disable=None, noise_sampler=None)\n\t\n    if extra_args is None:\n        extra_args = {}\n    if noise_sampler is None:\n        noise_sampler = lambda: torch.randn_like(x)\n\n    for sigma in tqdm(sigmas, disable=disable):\n        x = model(x, sigma, **extra_args)\n        if sigma > 0:\n            x += sigma * noise_sampler()\n        if callback is not None:\n            callback(x)\n    return x\n\n", "CodeBleu_score": 25.82416240034977}, {"code": "def sample_lcm(model, x, sigmas, extra_args=None, callback=None, disable=None, noise_sampler=None)\n\t\n    if extra_args is None:\n        extra_args = {}\n    if noise_sampler is None:\n        noise_sampler = torch.randn_like\n    if disable is None:\n        disable = False\n    for sigma in tqdm(sigmas, disable=disable):\n        if sigma > 0:\n            noise = noise_sampler(x.shape)\n            x = model(x, sigma, **extra_args) + sigma * noise\n        else:\n            x = model(x, sigma, **extra_args)\n        if callback is not None:\n            callback(x)\n    return x\n\n", "CodeBleu_score": 30.136640864740944}]}
{"instance_id": "cb51f91da606", "generation_result": [{"code": "def append_dims(x, target_dims)\n\t\n    if x.dim() == target_dims:\n        return x\n    if x.dim() > target_dims:\n        raise ValueError(\n            f\"Cannot reduce dimensions from {x.dim()} to {target_dims}\"\n        )\n    n_dims_to_append = target_dims - x.dim()\n    return x.expand(*(n_dims_to_append * [1]), *x.shape)\n\n", "CodeBleu_score": 27.930621366828834}, {"code": "def append_dims(x, target_dims)\n\t\n    if x.dim() > target_dims:\n        raise ValueError(\n            f\"The tensor has {x.dim()} dimensions, but the target dimensions are {target_dims}.\"\n        )\n    if x.device.type == \"mps\":\n        x = x.clone().detach()\n    return x.unsqueeze(-1).expand(*x.shape, target_dims - x.dim())\n\n", "CodeBleu_score": 34.66552201360602}, {"code": "def append_dims(x, target_dims)\n\t\n    x = x.clone()\n    if x.ndim < target_dims:\n        num_dims = target_dims - x.ndim\n        x = x.unsqueeze(-1)\n        x = x.expand(*x.shape, *(1 for _ in range(num_dims)))\n    elif x.ndim > target_dims:\n        raise ValueError(\n            f\"The input tensor has {x.ndim} dimensions, but the target dimensions is {target_dims}.\"\n        )\n    if x.device.type == \"mps\":\n        x = x.detach()\n    return x\n\n", "CodeBleu_score": 42.47982377740388}]}
{"instance_id": "9fac874a3bc7", "generation_result": [{"code": "def download_file(path, url, digest=None)\n\t\n    path = Path(path)\n    path.parent.mkdir(parents=True, exist_ok=True)\n    if not path.exists():\n        with urllib.request.urlopen(url) as response:\n            with path.open(\"wb\") as file:\n                shutil.copyfileobj(response, file)\n    if digest is not None:\n        with path.open(\"rb\") as file:\n            hash = hashlib.sha256(file.read()).hexdigest()\n            if hash != digest:\n                raise RuntimeError(\n                    f\"File {path} has incorrect hash {hash}, expected {digest}\"\n                )\n    return path\n\n", "CodeBleu_score": 54.74978118953619}, {"code": "def download_file(path, url, digest=None)\n\t\n    path = Path(path)\n    path.parent.mkdir(exist_ok=True)\n    if path.exists():\n        if digest is not None:\n            if hashlib.sha256(path.read_bytes()).hexdigest() == digest:\n                return path\n            else:\n                raise RuntimeError(\n                    f\"The downloaded file {path} has an incorrect hash. \"\n                    f\"Expected {digest}, got {hashlib.sha256(path.read_bytes()).hexdigest()}.\"\n                )\n        else:\n            warnings.warn(\n                f\"The file {path} already exists. \"\n                f\"If you want to download it again, please remove it first.\"\n            )\n            return path\n    with urllib.request.urlopen(url) as source, path.open(\"wb\") as target:\n        shutil.copyfileobj(source, target)\n    if digest is not None:\n        if hashlib.sha256(path.read_bytes()).hexdigest() == digest:\n            return path\n        else:\n            raise RuntimeError(\n                f\"The downloaded file {path} has an incorrect hash. \"\n                f\"Expected {digest}, got {hashlib.sha256(path.read_bytes()).hexdigest()}.\"\n            )\n\n", "CodeBleu_score": 35.03967405117767}, {"code": "def download_file(path, url, digest=None)\n\t\n    path = Path(path)\n    path.parent.mkdir(parents=True, exist_ok=True)\n\n    if not path.exists():\n        print(f\"Downloading {url}...\")\n        urllib.request.urlretrieve(url, path)\n\n    if digest:\n        print(f\"Checking SHA-256 digest for {path}...\")\n        if hashlib.sha256(path.read_bytes()).hexdigest() != digest:\n            raise RuntimeError(f\"Downloaded file {path} has incorrect SHA-256 digest.\")\n\n    return path\n\n", "CodeBleu_score": 35.380406983929234}]}
{"instance_id": "f23165e897ce", "generation_result": [{"code": "def train_mode(model, mode=True)\n\t\n    if not isinstance(model, nn.Module):\n        raise ValueError(f\"Expected model to be an instance of nn.Module, but got {type(model)}\")\n    if not isinstance(mode, bool):\n        raise ValueError(f\"Expected mode to be a boolean, but got {type(mode)}\")\n\n    training_modes = [m.training for m in model.modules()]\n    for m in model.modules():\n        m.train(mode)\n\n    @contextmanager\n    def train_mode_context():\n        try:\n            yield\n        finally:\n            for m, tm in zip(model.modules(), training_modes):\n                m.train(tm)\n\n    return train_mode_context()\n\n", "CodeBleu_score": 30.398120469520183}, {"code": "def train_mode(model, mode=True)\n\t\n    original_modes = [m.training for m in model.modules()]\n    for m in model.modules():\n        m.train(mode)\n    yield\n    for m, o in zip(model.modules(), original_modes):\n        m.train(o)\n\n", "CodeBleu_score": 37.386818581273985}, {"code": "def train_mode(model, mode=True)\n\t\n    if mode:\n        modes = [m.training for m in model.modules()]\n        for m in model.modules():\n            m.train()\n        yield\n        for m, m_train in zip(model.modules(), modes):\n            m.train(m_train)\n    else:\n        yield\n\n", "CodeBleu_score": 38.920775417802844}]}
{"instance_id": "c6d59c6cb8e1", "generation_result": [{"code": "def ema_update(model, averaged_model, decay)\n\t\n    averaged_model.load_state_dict(model.state_dict())\n    for param in averaged_model.parameters():\n        param.detach_()\n    for buffer in averaged_model.buffers():\n        buffer.detach_()\n    for averaged_param, param in zip(averaged_model.parameters(), model.parameters()):\n        averaged_param.mul_(decay).add_(param, alpha=1 - decay)\n    for averaged_buffer, buffer in zip(averaged_model.buffers(), model.buffers()):\n        averaged_buffer.copy_(buffer)\n\n", "CodeBleu_score": 39.45928729087203}, {"code": "def ema_update(model, averaged_model, decay)\n\t\n    averaged_model.load_state_dict(model.state_dict())\n    for param_key, param in model.named_parameters():\n        averaged_model.state_dict()[param_key].copy_(\n            decay * averaged_model.state_dict()[param_key] + (1 - decay) * param\n        )\n    for buffer_key, buffer in model.named_buffers():\n        averaged_model.state_dict()[buffer_key].copy_(buffer)\n\n", "CodeBleu_score": 31.312516067531583}, {"code": "def ema_update(model, averaged_model, decay)\n\t\n    model_dict = model.state_dict()\n    avg_dict = averaged_model.state_dict()\n    for k in avg_dict.keys():\n        if avg_dict[k].dtype == torch.float32:\n            avg_dict[k] = decay * avg_dict[k] + (1 - decay) * model_dict[k]\n        else:\n            avg_dict[k] = model_dict[k]\n    averaged_model.load_state_dict(avg_dict)\n\n", "CodeBleu_score": 20.07538662745707}]}
{"instance_id": "600c39a30798", "generation_result": [{"code": "def rand_log_logistic(shape, loc=0., scale=1., min_value=0., max_value=float('inf'), device='cpu', dtype=torch.float32)\n\t\n    min_value = torch.as_tensor(min_value, device=device, dtype=dtype)\n    max_value = torch.as_tensor(max_value, device=device, dtype=dtype)\n    min_cdf = min_value.log1p() - min_value.log1p().exp()\n    max_cdf = max_value.log1p() - max_value.log1p().exp()\n    uniform = torch.rand(shape, device=device, dtype=dtype)\n    cdf = min_cdf + (max_cdf - min_cdf) * uniform\n    samples = cdf.log1p().neg().expm1()\n    return samples\n\n", "CodeBleu_score": 38.126267141186624}, {"code": "def rand_log_logistic(shape, loc=0., scale=1., min_value=0., max_value=float('inf'), device='cpu', dtype=torch.float32)\n\t\n    min_value = torch.as_tensor(min_value, dtype=dtype, device=device)\n    max_value = torch.as_tensor(max_value, dtype=dtype, device=device)\n    min_cdf = torch.logit(min_value) * scale + loc\n    max_cdf = torch.logit(max_value) * scale + loc\n    uniform_samples = torch.rand(shape, dtype=dtype, device=device)\n    uniform_samples = (max_cdf - min_cdf) * uniform_samples + min_cdf\n    samples = torch.sigmoid(uniform_samples)\n    return samples\n\n", "CodeBleu_score": 40.50457771559137}, {"code": "def rand_log_logistic(shape, loc=0., scale=1., min_value=0., max_value=float('inf'), device='cpu', dtype=torch.float32)\n\t\n    min_value = torch.as_tensor(min_value, dtype=dtype, device=device)\n    max_value = torch.as_tensor(max_value, dtype=dtype, device=device)\n    min_cdf = min_value.log1p() - min_value.exp().log()\n    max_cdf = max_value.log1p() - max_value.exp().log()\n    u = torch.rand(shape, device=device, dtype=dtype)\n    cdf = min_cdf + u * (max_cdf - min_cdf)\n    x = torch.log1p(cdf).neg().expm1()\n    return x\n\n", "CodeBleu_score": 38.3423961029052}]}
{"instance_id": "75e2def2467f", "generation_result": [{"code": "def get_activation(activation: Literal[\"elu\", \"snake\", \"none\"], antialias=False, channels=None) -> nn.Module\n\t\n    if activation == \"elu\":\n        return nn.ELU(inplace=True) if antialias else nn.ELU(inplace=True)\n    if activation == \"snake\":\n        return comfy.ops.Activation1d(comfy.ops.SnakeBeta(channels), antialias=antialias)\n    if activation == \"none\":\n        return nn.Identity()\n    raise ValueError(f\"Unknown activation: {activation}\")\n\n", "CodeBleu_score": 42.65726992562398}, {"code": "def get_activation(activation: Literal[\"elu\", \"snake\", \"none\"], antialias=False, channels=None) -> nn.Module\n\t\n    if activation == \"elu\":\n        return nn.ELU(inplace=True) if antialias else nn.ELU(inplace=True)\n    elif activation == \"snake\":\n        return comfy.ops.Activation1d(comfy.ops.SnakeBeta(channels)) if antialias else comfy.ops.SnakeBeta(channels)\n    elif activation == \"none\":\n        return nn.Identity()\n    else:\n        raise ValueError(f\"Unknown activation type: {activation}\")\n\n", "CodeBleu_score": 48.44167781388226}, {"code": "def get_activation(activation: Literal[\"elu\", \"snake\", \"none\"], antialias=False, channels=None) -> nn.Module\n\t\n    if activation == \"elu\":\n        return nn.ELU(inplace=True) if not antialias else comfy.ops.Activation1d(nn.ELU(inplace=True))\n    elif activation == \"snake\":\n        return nn.SnakeBeta(beta=-1, channels=channels) if not antialias else comfy.ops.Activation1d(nn.SnakeBeta(beta=-1, channels=channels))\n    elif activation == \"none\":\n        return nn.Identity()\n    else:\n        raise ValueError(f\"Unknown activation type: {activation}\")\n\n", "CodeBleu_score": 47.844791949407586}]}
{"instance_id": "ef1714c79e51", "generation_result": [{"code": "def apply_rotary_pos_emb(t, freqs, scale = 1)\n\t\n    t2 = t[:, 1::2]\n    t1 = t[:, ::2]\n    sin_cos = torch.exp(torch.tensor(-scale) * freqs * math.log(10000))\n    sin_cos = rearrange(sin_cos, 'n d -> n (d 2)')\n    sin_cos = sin_cos.to(t.dtype)\n    if t1.dtype == torch.float16:\n        sin_cos = sin_cos.float()\n    t1 = t1 * sin_cos[:, :t1.shape[-1] * 2:2]\n    t2 = t2 * sin_cos[:, 1:t2.shape[-1] * 2:2]\n    return torch.cat([t1, t2], dim = -1)\n\n", "CodeBleu_score": 29.386551243444696}, {"code": "def apply_rotary_pos_emb(t, freqs, scale = 1)\n\t\n    if t.dtype in (torch.float16, torch.bfloat16):\n        t = t.float()\n    freqs = rearrange(freqs, 'n d -> n () d')\n    sin, cos = torch.sin(freqs * t), torch.cos(freqs * t)\n    sin, cos = map(lambda t: rearrange(t, 'n () d -> n d'), (sin, cos))\n    return torch.cat([t[..., ::2], t[..., 1::2] * cos, t[..., ::2] * sin, t[..., 1::2]], dim=-1) * scale\n\n", "CodeBleu_score": 25.903179526275622}, {"code": "def apply_rotary_pos_emb(t, freqs, scale = 1)\n\t\n    if t.dtype == torch.float16:\n        t = t.float()\n\n    if freqs.device != t.device:\n        freqs = freqs.to(t.device)\n\n    t2 = rearrange(t, 'b ... d -> b (...) (d r)', r = 2)\n    sin, cos = torch.chunk(t2, 2, dim = -1)\n    sin, cos = map(lambda t: scale * t, (sin, cos))\n    pos_emb_sin = rearrange(sin, 'b ... (d r) -> b ... d r', r = 2) * freqs\n    pos_emb_cos = rearrange(cos, 'b ... (d r) -> b ... d r', r = 2) * freqs\n\n    return torch.cat([pos_emb_sin, pos_emb_cos], dim = -1)\n\n", "CodeBleu_score": 28.85532820438965}]}
{"instance_id": "a7a5f178c6f6", "generation_result": [{"code": "def idx2vq(self, idx, dim=-1)\n\t\n    if idx.dim() != 2:\n        raise ValueError(\n            f\"Expected 2D tensor as input, but got {idx.dim()}D tensor instead.\"\n        )\n    if dim != -1 and dim >= idx.dim():\n        raise ValueError(\n            f\"Expected dimension to be in the range [-1, {idx.dim() - 1}], but got {dim} instead.\"\n        )\n    if idx.dtype != torch.long:\n        raise ValueError(\n            f\"Expected input to have dtype long, but got {idx.dtype} instead.\"\n        )\n    if idx.device != self.codebook.device:\n        raise ValueError(\n            f\"Expected input to be on device {self.codebook.device}, but got {idx.device} instead.\"\n        )\n    if idx.shape[0] != self.codebook.shape[0]:\n        raise ValueError(\n            f\"Expected input to have {self.codebook.shape[0]} channels, but got {idx.shape[0]} instead.\"\n        )\n    if idx.shape[1] != self.codebook.shape[1]:\n        raise ValueError(\n            f\"Expected input to have {self.codebook.shape[1]} indices, but got {idx.shape[1]} instead.\"\n        )\n    if dim != -1 and idx.shape[1] != 1:\n        raise ValueError(\n            f\"Expected input to have 1 index, but got {idx.shape[1]} instead.\"\n        )\n    if dim == -1:\n        return self.codebook[idx]\n    return self.codebook[idx].permute(0, 2, 1)\n\n", "CodeBleu_score": 28.02864906096857}, {"code": "def idx2vq(self, idx, dim=-1)\n\t\n    codebook = self.codebook\n    if dim == -1:\n        return codebook[idx]\n    else:\n        return codebook[idx].permute(0, *range(2, dim + 1), 1)\n\n", "CodeBleu_score": 23.73977046614204}, {"code": "def idx2vq(self, idx, dim=-1)\n\t\n    # Retrieve the embeddings corresponding to the given indices\n    embeddings = self.codebook[idx]\n    # Move the last dimension to the given dimension\n    embeddings = embeddings.transpose(0, dim)\n    # Return the resulting embeddings\n    return embeddings\n\n", "CodeBleu_score": 24.828769866585784}]}
{"instance_id": "4104c041fb20", "generation_result": [{"code": "def encode(self, x, quantize=False)\n\t\n    # Encode\n    enc = self.encoder(x)\n    # Quantize\n    if quantize:\n        enc, quant, ind, loss = self.vector_quantizer(enc)\n        return enc, quant, ind, loss\n    return enc\n\n", "CodeBleu_score": 18.39706644081503}, {"code": "def encode(self, x, quantize=False)\n\t\n    # Encode\n    enc = self.encoder(x)\n    # Quantize\n    if quantize:\n        enc, quant, loss, ind = self.quantize(enc)\n        return enc, x, ind, loss\n    return enc\n\n", "CodeBleu_score": 18.92860881368227}, {"code": "def encode(self, x, quantize=False)\n\t\n    # Encode\n    enc = self.encoder(x)\n\n    # Quantization\n    if quantize:\n        enc = enc.contiguous()\n        enc_flatten = enc.view(-1, self.latent_dim)\n\n        # Encoded vector -> indices\n        indices = self.quantize(enc_flatten)\n        indices = indices.view(enc.shape[:-1])\n\n        # Indices -> quantized vector\n        enc_q = self.quantize.embed_code(indices)\n\n        # Loss\n        e_latent_loss = F.mse_loss(enc, enc_q.detach())\n        q_latent_loss = F.mse_loss(enc_q, enc.detach())\n        loss = q_latent_loss + self.commitment_weight * e_latent_loss\n\n        # Straight-through estimator\n        enc = enc + (enc_q - enc).detach()\n\n        return enc, enc_q, indices, loss\n\n    return enc\n\n", "CodeBleu_score": 32.476034146554426}]}
{"instance_id": "403c52ba2d3b", "generation_result": [{"code": "def ema_scope(self, context=None)\n\t\n    if context is not None:\n        context.ema_switched = False\n\n    if self.ema_enabled:\n        if self.ema_switched:\n            self.ema.restore()\n            if context is not None:\n                context.ema_switched = True\n        else:\n            self.ema.apply_to_model(self.model)\n\n", "CodeBleu_score": 35.87047744750775}, {"code": "def ema_scope(self, context=None)\n\t\n    if not self.ema_enabled:\n        yield\n        return\n\n    if context is not None:\n        if not self.ema_use_ema_params_in_context:\n            self.ema_log_switch(context)\n        self.ema_apply_ema_params()\n        yield\n        if not self.ema_use_ema_params_in_context:\n            self.ema_restore_params()\n            self.ema_log_switch(context)\n    else:\n        self.ema_apply_ema_params()\n        yield\n        self.ema_restore_params()\n\n", "CodeBleu_score": 41.071752517166935}, {"code": "def ema_scope(self, context=None)\n\t\n    if not self.ema_enabled:\n        yield\n        return\n\n    if context is not None:\n        context.set_current_scope(self)\n\n    self.ema_enabled = True\n    self.ema_model.store_average(self.model)\n    self.ema_model.copy_to(self.model)\n\n    if context is not None:\n        context.log_ema_switch()\n\n    yield\n\n    if context is not None:\n        context.set_current_scope(None)\n\n    self.ema_enabled = False\n    self.ema_model.restore()\n\n    if context is not None:\n        context.log_ema_restore()\n\n", "CodeBleu_score": 45.817474997118524}]}
{"instance_id": "94b7467f98e4", "generation_result": [{"code": "def post_attention(self, attn, x, gate_msa, shift_mlp, scale_mlp, gate_mlp)\n\t\n    x = x + gate_msa * attn\n    x = self.norm1(x)\n    x = x + self.mlp(self.norm2(x)) * gate_mlp\n    x = x * shift_mlp + scale_mlp\n    return x\n\n", "CodeBleu_score": 39.757210359090614}, {"code": "def post_attention(self, attn, x, gate_msa, shift_mlp, scale_mlp, gate_mlp)\n\t\n    x = x + gate_msa * attn\n    x = self.norm(x)\n    x = x * scale_mlp(shift_mlp(x))\n    x = x * gate_mlp(x)\n    return x\n\n", "CodeBleu_score": 34.12399195229712}, {"code": "def post_attention(self, attn, x, gate_msa, shift_mlp, scale_mlp, gate_mlp)\n\t\n    # Add attention output to input\n    x = x + gate_msa * attn\n\n    # Normalize x\n    x = x / math.sqrt(x.shape[-1])\n\n    # Modulate x using shift_mlp and scale_mlp\n    x = shift_mlp(x)\n    x = scale_mlp(x)\n\n    # Apply MLP scaled by gate_mlp\n    x = gate_mlp(x)\n\n    return x\n\n", "CodeBleu_score": 36.649669687774356}]}
{"instance_id": "d9fdc05542a2", "generation_result": [{"code": "def _block_mixing(context, x, context_block, x_block, c)\n\t\n    q, k, v, q_intermediates, k_intermediates, v_intermediates = context_block(context)\n    x_q, x_k, x_v, x_q_intermediates, x_k_intermediates, x_v_intermediates = x_block(x)\n\n    q = torch.cat([q, x_q], dim=-2)\n    k = torch.cat([k, x_k], dim=-2)\n    v = torch.cat([v, x_v], dim=-2)\n    q_intermediates = torch.cat([q_intermediates, x_q_intermediates], dim=-2)\n    k_intermediates = torch.cat([k_intermediates, x_k_intermediates], dim=-2)\n    v_intermediates = torch.cat([v_intermediates, x_v_intermediates], dim=-2)\n\n    q = rearrange(q, 'b (h d) -> b h d', h=c)\n    k = rearrange(k, 'b (h d) -> b h d', h=c)\n    v = rearrange(v, 'b (h d) -> b h d', h=c)\n    q_intermediates = rearrange(q_intermediates, 'b (h d) -> b h d', h=c)\n    k_intermediates = rearrange(k_intermediates, 'b (h d) -> b h d', h=c)\n    v_intermediates = rearrange(v_intermediates, 'b (h d) -> b h d', h=c)\n\n    attention, attention_intermediates = c.attention(q, k, v, q_intermediates, k_intermediates, v_intermediates)\n\n    context = rearrange(attention, 'b h d -> b (h d)')\n    x = rearrange(attention[:, c:], 'b h d -> b (h d)')\n\n    context, context_intermediates", "CodeBleu_score": 22.390424716592015}, {"code": "def _block_mixing(context, x, context_block, x_block, c)\n\t\n    # context block\n    qkv = context_block(context)\n    q, k, v = qkv[:, :, :c], qkv[:, :, c : 2 * c], qkv[:, :, 2 * c :]\n    q = rearrange(q, \"b n (h d) -> b h n d\", h=context_block.num_heads)\n    k = rearrange(k, \"b n (h d) -> b h d n\", h=context_block.num_heads)\n    v = rearrange(v, \"b n (h d) -> b h n d\", h=context_block.num_heads)\n\n    # x block\n    qkv = x_block(x)\n    q_x, k_x, v_x = (\n        qkv[:, :, :c],\n        qkv[:, :, c : 2 * c],\n        qkv[:, :, 2 * c :],\n    )\n    q_x = rearrange(q_x, \"b n (h d) -> b h n d\", h=x_block.num_heads)\n    k_x = rearrange(k_x, \"b n (h d) -> b h d n\", h=x_block.num_heads)\n    v_x = rearrange(v_x, \"b n (h d) -> b h n d\", h=x_block.num_heads)\n\n    # concatenate qkv\n    q = torch.cat([q, q_x], dim=2)\n    k = torch.cat([k, k_x], dim=3)\n    v = torch.cat([v, v_x], dim=2)\n\n    # attention\n    attn = torch.einsum(\"b h i d, b h d n -> b h i n\", q, k) * context_block.scale\n    attn = attn.softmax(dim=-1)\n    attn = torch.einsum(\"b h i n, b h n d -> b", "CodeBleu_score": 19.997933493165775}, {"code": "def _block_mixing(context, x, context_block, x_block, c)\n\t\n    # Process context through pre-attention blocks\n    qkv_context = context_block(context)\n    q_context, k_context, v_context, intermediates_context = qkv_context\n\n    # Process x through pre-attention blocks\n    qkv_x = x_block(x)\n    q_x, k_x, v_x, intermediates_x = qkv_x\n\n    # Concatenate qkv tensors\n    q = torch.cat([q_context, q_x], dim=1)\n    k = torch.cat([k_context, k_x], dim=1)\n    v = torch.cat([v_context, v_x], dim=1)\n\n    # Apply optimized attention\n    context_attention, x_attention = c(q, k, v)\n\n    # Split attention results back into context and x parts\n    context_attention = context_attention[:, :c.context_dim]\n    x_attention = x_attention[:, c.context_dim:]\n\n    # Process context_attention through post-attention blocks\n    context_attention = context_block.post_attention(context_attention, intermediates_context)\n\n    # Process x_attention through post-attention blocks\n    x_attention = x_block.post_attention(x_attention, intermediates_x)\n\n    return context_attention, x_attention\n\n", "CodeBleu_score": 24.263111806479106}]}
{"instance_id": "b219bce30160", "generation_result": [{"code": "def get_timestep_embedding(timesteps, embedding_dim)\n\t\n    assert len(timesteps.shape) == 1, \"Timesteps input to embedding function should be a single dimension\"\n    # create a sequence of timesteps\n    timesteps = timesteps.long()\n    # calculate the logarithmic factor\n    log_factor = math.log(10000) / (embedding_dim // 2)\n    # calculate the sine and cosine components of the timestep embeddings\n    sinusoids = torch.exp(\n        -log_factor * timesteps.type_as(torch.float32))[:, None]\n    # create a sequence of timesteps\n    timesteps = timesteps.type_as(sinusoids)\n    # calculate the sine and cosine components of the timestep embeddings\n    embedding = torch.cat([sinusoids, torch.cos(timesteps * log_factor)], dim=-1)\n    # if the embedding dimension is odd, pad it with zeros\n    if embedding_dim % 2:\n        embedding = torch.nn.functional.pad(embedding, (0, 1), value=0)\n    return embedding\n\n", "CodeBleu_score": 42.98537458829397}, {"code": "def get_timestep_embedding(timesteps, embedding_dim)\n\t\n    assert len(timesteps.shape) == 1, \"Timesteps input to embedding function should be a single dimension\"\n\n    half_dim = embedding_dim // 2\n    emb = math.log(10000) / (half_dim - 1)\n    emb = torch.exp(torch.arange(half_dim, dtype=torch.float32) * -emb)\n    emb = timesteps[:, None] * emb[None, :]\n    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=-1)\n    if embedding_dim % 2 == 1:\n        emb = torch.nn.functional.pad(emb, (0, 1), \"constant\", 0)\n    return emb\n\n", "CodeBleu_score": 70.71467056868664}, {"code": "def get_timestep_embedding(timesteps, embedding_dim)\n\t\n    assert len(timesteps.shape) == 1, \"Timesteps must be a one-dimensional tensor\"\n\n    half_dim = embedding_dim // 2\n    emb = math.log(10000) / (half_dim - 1)\n    emb = torch.exp(torch.arange(half_dim, dtype=torch.float32) * -emb)\n    emb = timesteps[:, None] * emb[None, :]\n    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=-1)\n    if embedding_dim % 2 == 1:\n        emb = torch.nn.functional.pad(emb, (0, 1), \"constant\", 0)\n    return emb\n\n", "CodeBleu_score": 71.36475915902551}]}
{"instance_id": "6ae8453c86be", "generation_result": [{"code": "def pytorch_attention(q, k, v)\n\t\n    try:\n        # Reshape and transpose the tensors to prepare them for attention computation\n        q = q.reshape(q.shape[0], q.shape[1], -1)\n        k = k.reshape(k.shape[0], k.shape[1], -1)\n        v = v.reshape(v.shape[0], v.shape[1], -1)\n\n        q = q.transpose(1, 2)\n        k = k.transpose(1, 2)\n        v = v.transpose(1, 2)\n\n        # Compute scaled dot-product attention using PyTorch's built-in function\n        attn = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=0.0)\n\n        # Reshape the attention output back to the original dimensions\n        attn = attn.reshape(attn.shape[0], attn.shape[1], -1)\n        attn = attn.transpose(1, 2)\n\n        return attn\n    except RuntimeError as e:\n        if \"CUDA out of memory\" in str(e):\n            logging.warning(\"CUDA out of memory. Switching to slice attention as a fallback.\")\n            return slice_attention(q, k, v)\n        else:\n            raise e\n\n", "CodeBleu_score": 36.59573704009822}, {"code": "def pytorch_attention(q, k, v)\n\t\n    # q: (batch_size, seq_len, hidden_size)\n    # k: (batch_size, seq_len, hidden_size)\n    # v: (batch_size, seq_len, hidden_size)\n    # q: (batch_size, seq_len, hidden_size)\n    # k: (batch_size, seq_len, hidden_size)\n    # v: (batch_size, seq_len, hidden_size)\n    try:\n        # (batch_size, seq_len, hidden_size)\n        q = q.reshape(q.shape[0], q.shape[1], -1)\n        # (batch_size, seq_len, hidden_size)\n        k = k.reshape(k.shape[0], k.shape[1], -1)\n        # (batch_size, seq_len, hidden_size)\n        v = v.reshape(v.shape[0], v.shape[1], -1)\n        # (batch_size, seq_len, seq_len)\n        attention_scores = torch.bmm(q, k.transpose(1, 2))\n        # (batch_size, seq_len, seq_len)\n        attention_scores = attention_scores / math.sqrt(q.shape[-1])\n        # (batch_size, seq_len, seq_len)\n        attention_probs = torch.softmax(attention_scores, dim=-1)\n        # (batch_size, seq_len, hidden_size)\n        attention_output = torch.bmm(attention_probs, v)\n        # (batch_size, seq_len, hidden_size)\n        attention_output = attention_output.reshape(attention_output.shape[0], attention_output.shape[1], -1)\n    except RuntimeError as e:\n        if \"CUDA out of memory\" in str(e):\n            logging.warning(\"CUDA out of memory, using slice attention as a fallback.\")\n            attention_output = slice_attention(q, k, v)\n", "CodeBleu_score": 30.933533300199823}, {"code": "def pytorch_attention(q, k, v)\n\t\n    try:\n        # Reshape and transpose the tensors for attention computation\n        q = q.reshape(q.shape[0], q.shape[1], q.shape[2] * q.shape[3]).transpose(1, 2)\n        k = k.reshape(k.shape[0], k.shape[1], k.shape[2] * k.shape[3]).transpose(1, 2)\n        v = v.reshape(v.shape[0], v.shape[1], v.shape[2] * v.shape[3]).transpose(1, 2)\n\n        # Compute attention using scaled dot-product attention\n        attn = torch.bmm(q, k.transpose(1, 2))\n        attn = attn * (1.0 / math.sqrt(k.shape[2]))\n        attn = torch.softmax(attn, dim=-1)\n        output = torch.bmm(attn, v)\n\n        # Reshape the output back to the original dimensions\n        output = output.transpose(1, 2).reshape(output.shape[0], output.shape[1], q.shape[2], q.shape[3])\n\n        return output\n    except RuntimeError as e:\n        if \"CUDA out of memory\" in str(e):\n            logging.warning(\"OOM exception occurred during attention computation, switching to slice attention as a fallback.\")\n            return slice_attention(q, k, v)\n        else:\n            raise e\n\n", "CodeBleu_score": 33.139277766717534}]}
{"instance_id": "fe4433c987ad", "generation_result": [{"code": "def apply_control(h, control, name)\n\t\n    if control is not None and exists(control) and exists(name):\n        if control.get(name) is not None:\n            logging.info(f\"Applying control for {name}\")\n            try:\n                h = control.pop(name)(h)\n            except Exception as e:\n                logging.warning(f\"Failed to apply control for {name}: {e}\")\n    return h\n\n", "CodeBleu_score": 43.082156832630844}, {"code": "def apply_control(h, control, name)\n\t\n    if exists(control) and len(control) > 0:\n        control = control.pop(name)\n        try:\n            h = control.apply(h)\n        except Exception as e:\n            logging.warning(f\"Failed to apply control {name} to {h.shape}\")\n            logging.warning(e)\n    return h\n\n", "CodeBleu_score": 35.613580739448494}, {"code": "def apply_control(h, control, name)\n\t\n    if exists(control) and len(control) > 0:\n        if name in control:\n            control_value = control.pop(name)\n            try:\n                h = control_value(h)\n            except Exception as e:\n                logging.warning(f'failed to apply control {name}: {e}')\n    return h\n\n", "CodeBleu_score": 37.43526435748027}]}
{"instance_id": "56da4547c101", "generation_result": [{"code": "def get_codebook_entry(self, indices, shape)\n\t\n    # if shape is None:\n    #     shape = indices.shape\n    #     indices = indices.flatten()\n    #     shape = shape[1:]\n    # else:\n    #     indices = indices.reshape(-1)\n    # shape = list(shape)\n    # shape[0] = len(indices)\n    # indices = torch.eye(self.num_embeddings, device=self.weight.device)[indices]\n    # indices = indices.permute(1, 0).reshape(shape)\n    # return indices\n    # indices = indices.unsqueeze(-1).expand(-1, -1, self.embedding_dim)\n    # return torch.gather(self.weight, 0, indices).reshape(shape)\n    # return torch.index_select(self.weight, 0, indices.view(-1)).view(shape)\n    return torch.index_select(self.weight, 0, indices.view(-1)).view(shape)\n\n", "CodeBleu_score": 3.7320245002163728}, {"code": "def get_codebook_entry(self, indices, shape)\n\t\n    # shape specifying (B, H, W, C)\n    # indices specifying (B, H, W)\n    # get quantized latent vectors\n    indices = indices.unsqueeze(-1).type(torch.int64)\n    embedding_onehot = torch.nn.functional.one_hot(indices, num_classes=self.num_codebook_vectors).type(torch.float32)\n    embedding = embedding_onehot @ self.codebook.weight\n\n    if shape is not None:\n        embedding = embedding.permute(0, 3, 1, 2).reshape(shape)\n\n    return embedding\n\n", "CodeBleu_score": 35.84962761810056}, {"code": "def get_codebook_entry(self, indices, shape)\n\t\n    # shape specifying (B, H, W, C)\n    if shape is not None:\n        # shape specifying (B, C, H, W)\n        indices = indices.unsqueeze(3).expand(shape)\n        indices = indices.contiguous().view(shape[0], -1, self.codebook_dim)\n        return self.codebook(indices)\n    else:\n        # shape specifying (B, H*W, C)\n        indices = indices.view(-1, self.codebook_dim)\n        return self.codebook(indices)\n\n", "CodeBleu_score": 39.39424555330414}]}
{"instance_id": "548f8a1fb37d", "generation_result": [{"code": "def make_noise(self)\n\t\n    device = self.noise_tensor.device\n    noise_tensor = self.noise_tensor\n    noise = []\n    for i in range(self.log_size, 2, -1):\n        size = 2 ** i\n        # noise in the latent space\n        tmp = noise_tensor.repeat(1, 1, size // 2, size // 2)\n        noise.append(tmp.view(-1, 1, size, size).to(device))\n        # noise in the data space\n        tmp = noise_tensor.repeat(1, 1, 2, 2)\n        noise.append(tmp.view(-1, 1, size, size).to(device))\n    return noise\n\n", "CodeBleu_score": 25.95530280702354}, {"code": "def make_noise(self)\n\t\n    noise = []\n    size = 4\n    while size <= self.log_size:\n        noise.append(torch.randn(1, self.nf, size, size, device=self.device))\n        size *= 2\n    return noise\n\n", "CodeBleu_score": 18.0915041889265}, {"code": "def make_noise(self)\n\t\n    noise = []\n    noise.append(torch.randn(4, 4, device=self.device))\n    for i in range(3, self.log_size + 3):\n        n = noise[-1]\n        n = F.interpolate(n, scale_factor=2, mode='bilinear', align_corners=False)\n        noise.append(n)\n        noise.append(n)\n    return noise\n\n", "CodeBleu_score": 34.32187399548625}]}
{"instance_id": "42fd805563c6", "generation_result": [{"code": "def mean_latent(self, num_latent)\n\t\n    # Generate random latent inputs\n    latent_in = torch.randn(1, num_latent, device=self.device)\n    # Process the latent inputs through the style MLP\n    latent_out = self.style(latent_in)\n    # Compute the mean of the processed latents along the first dimension\n    mean_latent = latent_out.mean(0, keepdim=True)\n    return mean_latent\n\n", "CodeBleu_score": 42.46811621770911}, {"code": "def mean_latent(self, num_latent)\n\t\n    mean_latent = []\n    for _ in range(num_latent):\n        # Create random latent input\n        rand_input = torch.randn(1, self.latent_dim, device=self.device)\n        # Process through the style MLP\n        rand_input = self.style_mlp(rand_input)\n        # Append to the list of mean latents\n        mean_latent.append(rand_input)\n    # Compute the mean along the first dimension\n    mean_latent = torch.mean(torch.cat(mean_latent, dim=0), dim=0, keepdim=True)\n    return mean_latent\n\n", "CodeBleu_score": 34.14664342046427}, {"code": "def mean_latent(self, num_latent)\n\t\n    mean_latent = []\n    for i in range(num_latent):\n        # generate random latent vector\n        z = torch.randn(1, self.style_dim, device=self.device)\n        # process latent vector through style MLP\n        z = self.style_mlp(z)\n        # append processed latent vector to mean latent vector\n        mean_latent.append(z)\n    # compute mean of processed latent vectors along first dimension\n    mean_latent = torch.mean(torch.cat(mean_latent, dim=0), dim=0, keepdim=True)\n    return mean_latent\n\n", "CodeBleu_score": 36.09045281429504}]}
{"instance_id": "2f485fd924af", "generation_result": [{"code": "def color_jitter(img, shift)\n\t\n    jitter = torch.rand(3) * shift\n    jitter = jitter.to(img.device)\n    img = img + jitter\n    img = torch.clamp(img, 0, 1)\n    return img\n\n", "CodeBleu_score": 48.63200486559451}, {"code": "def color_jitter(img, shift)\n\t\n    jitter = torch.rand(3) * shift\n    img = img + jitter\n    img = torch.clamp(img, 0, 1)\n    return img\n\n", "CodeBleu_score": 42.33547027266186}, {"code": "def color_jitter(img, shift)\n\t\n    jitter = torch.rand(3) * shift\n    img = img + jitter\n    img = torch.clip(img, 0, 1)\n    return img\n\n", "CodeBleu_score": 42.67348570999343}]}
{"instance_id": "c87ccf1fbf48", "generation_result": [{"code": "def feed_data(self, data)\n\t\n    if self.opt['phase'] == 'train':\n        self.lq = data['lq'].to(self.device)\n        if 'gt' in data:\n            self.gt = data['gt'].to(self.device)\n        if 'l_eye' in data:\n            self.l_eye = data['l_eye'].to(self.device)\n            self.r_eye = data['r_eye'].to(self.device)\n            self.mouth = data['mouth'].to(self.device)\n\n", "CodeBleu_score": 55.20155820345993}, {"code": "def feed_data(self, data)\n\t\n    if self.opt['phase'] == 'train':\n        self.lq = data['lq'].to(self.device)\n        if 'gt' in data:\n            self.gt = data['gt'].to(self.device)\n        if 'l_eye' in data:\n            self.l_eye = data['l_eye'].to(self.device)\n            self.r_eye = data['r_eye'].to(self.device)\n            self.mouth = data['mouth'].to(self.device)\n\n", "CodeBleu_score": 55.20155820345993}, {"code": "def feed_data(self, data)\n\t\n    self.lq = data['lq'].to(self.device)\n    if 'gt' in data:\n        self.gt = data['gt'].to(self.device)\n    if 'l_eye' in data:\n        self.l_eye = data['l_eye'].to(self.device)\n        self.r_eye = data['r_eye'].to(self.device)\n        self.mouth = data['mouth'].to(self.device)\n\n", "CodeBleu_score": 57.979397726363715}]}
{"instance_id": "8f8c7cb13037", "generation_result": [{"code": "def construct_img_pyramid(self)\n\t\n    img_pyramid = [self.gt]\n    for _ in range(self.num_pyramid_levels - 1):\n        img_pyramid.insert(0, F.interpolate(img_pyramid[0], scale_factor=0.5, mode='bilinear', align_corners=False))\n    return img_pyramid\n\n", "CodeBleu_score": 27.310434050963465}, {"code": "def construct_img_pyramid(self)\n\t\n    img_pyramid = [self.gt]\n    for i in range(self.num_pyramid_levels - 1):\n        img_pyramid.insert(0, F.interpolate(img_pyramid[0], scale_factor=0.5, mode='bilinear', align_corners=False))\n    return img_pyramid\n\n", "CodeBleu_score": 26.134877243810934}, {"code": "def construct_img_pyramid(self)\n\t\n    img_pyramid = [self.gt]\n    for _ in range(self.num_feats):\n        img_pyramid.insert(0, F.interpolate(img_pyramid[0], scale_factor=0.5, mode='bilinear', align_corners=False))\n    return img_pyramid\n\n", "CodeBleu_score": 26.801768404612204}]}
{"instance_id": "51247274d40c", "generation_result": [{"code": "def save(self, epoch, current_iter)\n\t\n    # save generator\n    save_path = self.opt['path']['pretrain_network']\n    self.save_network(self.netG, 'netG', current_iter, save_path)\n\n    # save EMA\n    if self.opt['train']['ema']:\n        self.save_network(self.netG_ema, 'netG_ema', current_iter, save_path)\n\n    # save discriminator\n    if self.opt['train']['use_discriminator']:\n        self.save_network(self.netD, 'netD', current_iter, save_path)\n\n    # save facial discriminators\n    if self.opt['train']['use_facial_discriminator']:\n        self.save_network(self.netD_l, 'netD_l', current_iter, save_path)\n        self.save_network(self.netD_r, 'netD_r', current_iter, save_path)\n        self.save_network(self.netD_m, 'netD_m', current_iter, save_path)\n\n    # save training state\n    self.save_training_state(epoch, current_iter)\n\n", "CodeBleu_score": 46.02199624910661}, {"code": "def save(self, epoch, current_iter)\n\t\n    self.save_network(self.netG, 'netG', epoch, current_iter)\n    self.save_network(self.netG_ema, 'netG_ema', epoch, current_iter)\n    self.save_network(self.netD, 'netD', epoch, current_iter)\n    self.save_network(self.netD_left, 'netD_left', epoch, current_iter)\n    self.save_network(self.netD_right, 'netD_right', epoch, current_iter)\n    self.save_network(self.netD_mouth, 'netD_mouth', epoch, current_iter)\n    self.save_training_state(epoch, current_iter)\n\n", "CodeBleu_score": 53.67955937033584}, {"code": "def save(self, epoch, current_iter)\n\t\n    # save generator and its EMA version\n    self.save_network(self.netG, \"G\", current_iter)\n    self.save_network(self.netG_ema, \"G_ema\", current_iter)\n\n    # save discriminator\n    self.save_network(self.netD, \"D\", current_iter)\n\n    # save facial discriminators\n    if self.use_facial_discriminators:\n        self.save_network(self.netD_l, \"D_l\", current_iter)\n        self.save_network(self.netD_r, \"D_r\", current_iter)\n        self.save_network(self.netD_m, \"D_m\", current_iter)\n\n    # save training state\n    self.save_training_state(epoch, current_iter)\n\n", "CodeBleu_score": 61.16760849593654}]}
{"instance_id": "afa0dc8128c1", "generation_result": [{"code": "def clean_folder(folder)\n\t\n    for item in os.listdir(folder):\n        item_path = os.path.join(folder, item)\n        try:\n            if os.path.isfile(item_path):\n                os.unlink(item_path)\n            elif os.path.isdir(item_path):\n                shutil.rmtree(item_path)\n        except Exception as e:\n            print(f\"Failed to delete {item_path}: {e}\")\n\n", "CodeBleu_score": 50.196385713352434}, {"code": "def clean_folder(folder)\n\t\n    for filename in os.listdir(folder):\n        file_path = os.path.join(folder, filename)\n        try:\n            if os.path.isfile(file_path) or os.path.islink(file_path):\n                os.unlink(file_path)\n            elif os.path.isdir(file_path):\n                shutil.rmtree(file_path)\n        except Exception as e:\n            print('Failed to delete %s. Reason: %s' % (file_path, e))\n\n", "CodeBleu_score": 69.1060216709654}, {"code": "def clean_folder(folder)\n\t\n    for filename in os.listdir(folder):\n        file_path = os.path.join(folder, filename)\n        try:\n            if os.path.isfile(file_path) or os.path.islink(file_path):\n                os.unlink(file_path)\n            elif os.path.isdir(file_path):\n                shutil.rmtree(file_path)\n        except Exception as e:\n            print('Failed to delete %s. Reason: %s' % (file_path, e))\n\n", "CodeBleu_score": 69.1060216709654}]}
{"instance_id": "53588b481bdd", "generation_result": [{"code": "def next_batch(self, sample_size=1)\n\t\n    if len(self.memory) < sample_size:\n        sample_size = len(self.memory)\n    batch = random.sample(self.memory, sample_size)\n    self.memory = [exp for exp in self.memory if exp not in batch]\n    return batch", "CodeBleu_score": 52.39890280693995}, {"code": "def next_batch(self, sample_size=1)\n\t\n    if sample_size > len(self.memory):\n        sample_size = len(self.memory)\n    samples = random.sample(self.memory, sample_size)\n    self.memory = [x for x in self.memory if x not in samples]\n    return samples\n\n", "CodeBleu_score": 71.64667068798406}, {"code": "def next_batch(self, sample_size=1)\n\t\n    if len(self.memory) < sample_size:\n        return random.sample(self.memory, len(self.memory))\n    else:\n        return random.sample(self.memory, sample_size)", "CodeBleu_score": 28.064787306412104}]}
{"instance_id": "6bfa6a4e621c", "generation_result": [{"code": "def find_estimated_top_configs(self)\n\t\n    # Flatten each configuration in all_configs\n    flattened_configs = [\n        {\n            k: v\n            for k, v in config.items()\n            if isinstance(v, (numbers.Number, bool, str))\n        }\n        for config in self.all_configs\n    ]\n\n    # Collect numerical values\n    numerical_values = [\n        [v for k, v in config.items() if isinstance(v, (numbers.Number, bool))]\n        for config in flattened_configs\n    ]\n\n    # Convert numerical values into a numpy array\n    numerical_values_array = np.array(numerical_values)\n\n    # Use the cost model to predict estimates for these configurations\n    estimates = self.cost_model.predict(numerical_values_array)\n\n    # Sort the indices of the configurations based on the estimates\n    sorted_indices = np.argsort(estimates)\n\n    # Return the indices of the top configurations\n    return sorted_indices[: self.top_configs]\n\n", "CodeBleu_score": 26.427760730464318}, {"code": "def find_estimated_top_configs(self)\n\t\n    all_configs = self.configs\n    all_configs_flattened = []\n    for config in all_configs:\n        all_configs_flattened.append(self.flatten_config(config))\n\n    all_configs_flattened_numerical = []\n    for config in all_configs_flattened:\n        all_configs_flattened_numerical.append(\n            [\n                value\n                for key, value in config.items()\n                if isinstance(value, numbers.Number)\n            ]\n        )\n\n    all_configs_flattened_numerical_array = np.array(all_configs_flattened_numerical)\n\n    all_configs_flattened_numerical_array_sorted = np.argsort(\n        self.cost_model.predict(all_configs_flattened_numerical_array)\n    )\n\n    return all_configs_flattened_numerical_array_sorted[: self.num_configs_estimated]\n\n", "CodeBleu_score": 32.07881118501023}, {"code": "def find_estimated_top_configs(self)\n\t\n\n    # Flatten each configuration in all_configs\n    flattened_configs = [self.flatten_config(config) for config in self.all_configs]\n\n    # Collect numerical values from the flattened configurations\n    numerical_values = []\n    for config in flattened_configs:\n        numerical_values.extend([value for value in config.values() if isinstance(value, numbers.Number)])\n\n    # Convert the numerical values into a numpy array\n    numerical_array = np.array(numerical_values)\n\n    # Use the cost model to predict estimates for the configurations\n    estimated_values = self.cost_model.predict(numerical_array)\n\n    # Sort the configurations based on the estimated values\n    sorted_indices = np.argsort(estimated_values)\n\n    # Return the indices of the top configurations\n    return sorted_indices[:self.top_configs]\n\n", "CodeBleu_score": 23.903108398045912}]}
{"instance_id": "d4cf48413a57", "generation_result": [{"code": "def flatten(d, parent_key='', sep='_')\n\t\n    items = []\n    for k, v in d.items():\n        new_key = parent_key + sep + k if parent_key else k\n        if isinstance(v, collections.abc.MutableMapping):\n            items.extend(flatten(v, new_key, sep=sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)\n\n", "CodeBleu_score": 83.2096634628176}, {"code": "def flatten(d, parent_key='', sep='_')\n\t\n    items = []\n    for k, v in d.items():\n        new_key = parent_key + sep + k if parent_key else k\n        if isinstance(v, collections.abc.MutableMapping):\n            items.extend(flatten(v, new_key, sep=sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)\n\n", "CodeBleu_score": 83.2096634628176}, {"code": "def flatten(d, parent_key='', sep='_')\n\t\n    items = []\n    for k, v in d.items():\n        new_key = parent_key + sep + k if parent_key else k\n        if isinstance(v, collections.abc.MutableMapping):\n            items.extend(flatten(v, new_key, sep=sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)\n\n", "CodeBleu_score": 83.2096634628176}]}
{"instance_id": "c2a514249c25", "generation_result": [{"code": "def run_after_tuning(self)\n\t\n    if self.tuner.optimal_cmd is not None:\n        cmd = self.tuner.optimal_cmd\n        print(\"Running after tuning: {}\".format(cmd))\n        process = subprocess.Popen(cmd, shell=True)\n        process.wait()\n        print(\"Finished running after tuning.\")\n    else:\n        print(\"Autotuning did not find an optimal configuration.\")\n\n", "CodeBleu_score": 43.27487976566795}, {"code": "def run_after_tuning(self)\n\t\n    if self.optimal_cmd is not None:\n        self.log.info(f\"Running command: {self.optimal_cmd}\")\n        p = subprocess.Popen(self.optimal_cmd, shell=True)\n        p.wait()\n        self.log.info(\"Command completed successfully.\")\n    else:\n        self.log.info(\"No optimal command found. Autotuning did not find an optimal configuration.\")\n\n", "CodeBleu_score": 50.65487764559554}, {"code": "def run_after_tuning(self)\n\t\n    if self.optimal_cmd:\n        print(f\"Running command: {self.optimal_cmd}\")\n        process = subprocess.Popen(self.optimal_cmd, shell=True)\n        process.wait()\n        print(f\"Command {self.optimal_cmd} completed\")\n    else:\n        print(\"Autotuning did not find an optimal configuration\")\n\n", "CodeBleu_score": 51.713965095756784}]}
{"instance_id": "61802de52df0", "generation_result": [{"code": "def run_job(self, exp: dict, reservations)\n\t\n    exp_copy = copy.deepcopy(exp)\n    exp_copy[\"master_port\"] = self.get_free_port()\n    exp_copy[\"result_dir\"] = self.get_result_dir(exp_copy)\n    exp_copy[\"user_args\"] = self.update_user_args(exp_copy)\n    exp_copy[\"reservations\"] = reservations\n    exp_copy[\"timestamp\"] = time.time()\n\n    self.running_experiments.append(exp_copy)\n    threading.Thread(target=self.run_experiment, args=(exp_copy,)).start()\n\n", "CodeBleu_score": 14.9973472715141}, {"code": "def run_job(self, exp: dict, reservations)\n\t\n    # Set the master port\n    exp[\"master_port\"] = self.get_free_port()\n\n    # Set the result directory\n    exp[\"result_dir\"] = os.path.join(self.result_dir, exp[\"name\"])\n\n    # Update user arguments based on argument mappings\n    for arg_mapping in exp.get(\"arg_mappings\", []):\n        exp[\"user_args\"] = self.update_user_args(exp[\"user_args\"], arg_mapping)\n\n    # Create a new thread to run the experiment\n    thread = threading.Thread(target=self.run_experiment, args=(exp,))\n    thread.start()\n\n    # Add the running experiment details to running_experiments\n    self.running_experiments[exp[\"name\"]] = {\n        \"exp\": exp,\n        \"thread\": thread,\n        \"start_time\": time.time(),\n    }\n\n    # Print the running experiment details\n    print(f\"Running experiment: {exp['name']}\")\n    print(f\"Start time: {time.ctime(self.running_experiments[exp['name']]['start_time'])}\")\n    print(f\"Master port: {exp['master_port']}\")\n    print(f\"Result directory: {exp['result_dir']}\")\n\n    # Print the user arguments\n    print(\"User arguments:\")\n    for arg in exp[\"user_args\"]:\n        print(f\"  {arg}: {exp['user_args'][arg]}\")\n\n    # Print the reservation details\n    print(\"Reservation details:\")\n    for reservation in reservations:\n        print(f\"  {reservation}: {reservations[reservation]}\")\n\n    # Print the experiment details\n    print(\"Experiment details:\")\n    for detail in exp:\n        print(f\"  {detail}: {exp[detail]}\")\n\n    # Print a separator\n    print(\"-\" * 80)\n\n", "CodeBleu_score": 26.68095922491247}, {"code": "def run_job(self, exp: dict, reservations)\n\t\n    # Set master port\n    exp[\"master_port\"] = self.get_master_port()\n\n    # Set result directory\n    exp[\"result_dir\"] = self.get_result_dir(exp)\n\n    # Update user arguments based on argument mappings\n    exp = self.update_user_args(exp)\n\n    # Create a new thread to run the experiment\n    thread = threading.Thread(target=self.run_experiment, args=(exp,))\n\n    # Start the thread\n    thread.start()\n\n    # Store the running experiment details with a timestamp\n    self.running_experiments[thread.name] = {\n        \"exp\": exp,\n        \"start_time\": time.time(),\n        \"reservations\": reservations,\n    }\n\n    # Print the running experiment details\n    self.print_running_experiments()\n\n", "CodeBleu_score": 17.515748120759483}]}
{"instance_id": "4884e2e9665c", "generation_result": [{"code": "def save_exp_results_to_database(self, message, ranks=None, path=None):\n\t\n    if ranks is None:\n        ranks = [0]\n    if path is None:\n        path = \"./exp_results.json\"\n    if dist.is_initialized():\n        ranks = dist.all_gather(ranks)\n        ranks = [item for sublist in ranks for item in sublist]\n    if self.rank in ranks:\n        if os.path.exists(path):\n            with open(path, \"r\") as f:\n                exp_results = json.load(f)\n        else:\n            exp_results = {}\n        exp_results[self.rank] = message\n        with open(path, \"w\") as f:\n            json.dump(exp_results, f)\n\n", "CodeBleu_score": 32.43865168682538}, {"code": "def save_exp_results_to_database(self, message, ranks=None, path=None):\n\t\n    if dist.is_initialized():\n        if ranks is None:\n            ranks = [0]\n        if dist.get_rank() in ranks:\n            if path is None:\n                path = os.path.join(self.exp_dir, \"exp_results.json\")\n            with open(path, \"a\") as f:\n                json.dump(message, f)\n                f.write(\"", "CodeBleu_score": 27.766292089753826}, {"code": "def save_exp_results_to_database(self, message, ranks=None, path=None):\n\t\n    if self.args.local_rank == 0:\n        if dist.is_initialized():\n            if ranks is not None:\n                if self.args.local_rank in ranks:\n                    if path is not None:\n                        if not os.path.exists(path):\n                            os.makedirs(path)\n                        with open(path, \"a\") as f:\n                            f.write(json.dumps(message) + \"", "CodeBleu_score": 24.934266830795725}]}
{"instance_id": "0429344c3c4d", "generation_result": [{"code": "def _build_tp_other_layer_map(self, layer_index: int)\n\t\n    if not os.path.exists(self.layer_dir):\n        raise FileNotFoundError(f\"Layer directory {self.layer_dir} not found.\")\n\n    # Check if the layer directory contains any layer files\n    layer_files = [f for f in os.listdir(self.layer_dir) if f.startswith(f\"layer_{layer_index}_\")]\n    if not layer_files:\n        raise FileNotFoundError(f\"No layer files found for layer {layer_index}.\")\n\n    # Verify the layer index\n    layer_index_regex = re.compile(r\"layer_(\\d+)_\")\n    for layer_file in layer_files:\n        match = layer_index_regex.match(layer_file)\n        if not match or int(match.group(1)) != layer_index:\n            raise ValueError(f\"Invalid layer index found in file {layer_file}.\")\n\n    # Partition the files based on tp_degree\n    tp_degree = self.tp_degree\n    partition_size = len(layer_files) // tp_degree\n    layer_files_partitioned = [layer_files[i : i + partition_size] for i in range(0, len(layer_files), partition_size)]\n\n    # Create a dictionary mapping partition indices to their corresponding file lists\n    layer_map: Dict[int, list] = {}\n    for i, files in enumerate(layer_files_partitioned):\n        layer_map[i] = files\n\n    return layer_map\n\n", "CodeBleu_score": 31.93246486559666}, {"code": "def _build_tp_other_layer_map(self, layer_index: int)\n\t\n    if not os.path.exists(self.path):\n        raise FileNotFoundError(f\"No such directory: {self.path}\")\n    if layer_index < 0 or layer_index >= self.num_layers:\n        raise ValueError(f\"Invalid layer index: {layer_index}\")\n\n    layer_files = [f for f in os.listdir(self.path) if f.startswith(f\"layer_{layer_index}\")]\n    if not layer_files:\n        raise FileNotFoundError(f\"No files found for layer {layer_index}\")\n\n    tp_degree = self.tp_degree\n    tp_layer_files = [f for f in layer_files if f.endswith(f\"_tp_{tp_degree}\")]\n    other_layer_files = [f for f in layer_files if f.endswith(f\"_tp_{tp_degree + 1}\")]\n\n    if not tp_layer_files:\n        raise FileNotFoundError(f\"No TP layer files found for layer {layer_index}\")\n\n    if not other_layer_files:\n        raise FileNotFoundError(f\"No other layer files found for layer {layer_index}\")\n\n    tp_layer_map = {}\n    other_layer_map = {}\n    for f in tp_layer_files:\n        partition_index = int(re.search(r\"_tp_(\\d+)_\", f).group(1))\n        tp_layer_map[partition_index] = [f]\n\n    for f in other_layer_files:\n        partition_index = int(re.search(r\"_tp_(\\d+)_\", f).group(1))\n        if partition_index in tp_layer_map:\n            tp_layer_map[partition_index].append(f)\n        else:\n            other_layer_map[partition_index] = [f]\n\n    return tp_layer_map, other_layer_map\n\n", "CodeBleu_score": 29.223557678898153}, {"code": "def _build_tp_other_layer_map(self, layer_index: int)\n\t\n    layer_files = os.listdir(self.model_path)\n    layer_files = [f for f in layer_files if f.startswith(f\"layer_{layer_index}\")]\n\n    if len(layer_files) == 0:\n        raise ValueError(f\"No layer files found for layer {layer_index}.\")\n\n    if layer_index not in range(self.num_layers):\n        raise ValueError(f\"Invalid layer index {layer_index}.\")\n\n    layer_files = sorted(layer_files, key=lambda x: int(x.split(\"_\")[-1]))\n\n    tp_degree = self.tp_degree\n    tp_layer_files = [f for f in layer_files if f.startswith(f\"layer_{layer_index}_tp_\")]\n    tp_layer_files = sorted(tp_layer_files, key=lambda x: int(x.split(\"_\")[-1]))\n\n    other_layer_files = [f for f in layer_files if f not in tp_layer_files]\n\n    layer_map = {}\n    for i, f in enumerate(tp_layer_files):\n        if i % tp_degree == 0:\n            layer_map[i // tp_degree] = []\n        layer_map[i // tp_degree].append(f)\n\n    for f in other_layer_files:\n        layer_map[i // tp_degree + 1].append(f)\n\n    return layer_map\n\n", "CodeBleu_score": 32.61641634286244}]}
{"instance_id": "1b65ecb9be93", "generation_result": [{"code": "def system_info()\n\t\n    return {\n        \"os\": platform.system(),\n        \"architecture\": platform.architecture()[0],\n        \"version\": platform.version(),\n        \"processor\": platform.processor(),\n        \"python_version\": sys.version,\n    }\n\n", "CodeBleu_score": 69.79418192954428}, {"code": "def system_info()\n\t\n    info = {}\n    info['os'] = os.name\n    info['architecture'] = platform.architecture()[0]\n    info['version'] = platform.version()\n    info['processor'] = platform.processor()\n    info['python'] = sys.version.split()[0]\n    return info\n\n", "CodeBleu_score": 41.27028327179229}, {"code": "def system_info()\n\t\n    info = {}\n    info['os'] = platform.system()\n    info['architecture'] = platform.architecture()\n    info['version'] = platform.version()\n    info['processor'] = platform.processor()\n    info['python'] = platform.python_version()\n    return info\n\n", "CodeBleu_score": 43.713998687862066}]}
{"instance_id": "969bcc5cf3b5", "generation_result": [{"code": "def cuda_info()\n\t\n    cuda_devices = []\n    if torch.cuda.is_available():\n        for i in range(torch.cuda.device_count()):\n            cuda_devices.append(torch.cuda.get_device_name(i))\n        return cuda_devices, True, torch.version.cuda\n    else:\n        return cuda_devices, False, None\n\n", "CodeBleu_score": 43.7541083125098}, {"code": "def cuda_info()\n\t\n    if torch.cuda.is_available():\n        print(\"CUDA is available, using device:\", torch.cuda.get_device_name(0))\n        print(\"CUDA version:\", torch.version.cuda)\n        print(\"CUDNN version:\", torch.backends.cudnn.version())\n        print(\"Number of GPUs available:\", torch.cuda.device_count())\n    else:\n        print(\"CUDA is not available, using device: cpu\")\n\n", "CodeBleu_score": 31.608918438302013}, {"code": "def cuda_info()\n\t\n    cuda_available = torch.cuda.is_available()\n    device_count = torch.cuda.device_count()\n    device_names = [torch.cuda.get_device_name(i) for i in range(device_count)]\n    cuda_version = torch.version.cuda\n    return {\"cuda_available\": cuda_available, \"device_count\": device_count, \"device_names\": device_names, \"cuda_version\": cuda_version}\n\n", "CodeBleu_score": 42.185331446722266}]}
{"instance_id": "a2ba908a4c70", "generation_result": [{"code": "def compute_encoder_accuracy(dataset_items, encoder_manager)\n\t\n    class_name_key = encoder_manager.config.class_name_key\n    class_id_to_class_name = encoder_manager.class_id_to_class_name\n\n    class_accuracies = {}\n    total_num_items = 0\n    for item in tqdm(dataset_items):\n        audio_file = item[\"audio_file\"]\n        class_name = item[class_name_key]\n        class_id = encoder_manager.class_name_to_class_id[class_name]\n        embedding = encoder_manager.compute_embedding(audio_file)\n        class_id_pred = encoder_manager.predict_class_id(embedding)\n        if class_id_pred is not None:\n            class_id_pred = class_id_pred.item()\n            class_name_pred = class_id_to_class_name[class_id_pred]\n            if class_name_pred == class_name:\n                if class_name not in class_accuracies:\n                    class_accuracies[class_name] = 0\n                class_accuracies[class_name] += 1\n            total_num_items += 1\n\n    for class_name, num_items in class_accuracies.items():\n        accuracy = num_items / total_num_items\n        print(f\"{class_name}: {accuracy:.4f}\")\n\n    avg_accuracy = sum(class_accuracies.values()) / total_num_items\n    print(f\"Average accuracy: {avg_accuracy:.4f}\")\n\n", "CodeBleu_score": 31.837908638199032}, {"code": "def compute_encoder_accuracy(dataset_items, encoder_manager)\n\t\n    # Retrieve class name key and optional class ID to class name mapping from the encoder manager's configuration\n    class_name_key = encoder_manager.config.class_name_key\n    class_id_to_class_name = encoder_manager.config.class_id_to_class_name\n\n    # Initialize counters for each class and total number of samples\n    class_correct = {class_name: 0 for class_name in class_id_to_class_name.values()}\n    total = 0\n\n    # Iterate over each item in the dataset\n    for item in tqdm(dataset_items, desc=\"Computing encoder accuracy\"):\n        # Extract the audio file from the item\n        audio_file = item[\"audio_file\"]\n\n        # Compute the embedding for the audio file\n        embedding = encoder_manager.compute_embedding(audio_file)\n\n        # If the class name key is available in the item, extract the true class name\n        if class_name_key in item:\n            true_class_name = item[class_name_key]\n\n            # If the class ID to class name mapping is available, get the class ID from the item\n            if class_id_to_class_name is not None:\n                class_id = item[class_id_to_class_name[\"key\"]]\n            else:\n                class_id = None\n\n            # Predict the class label using the encoder manager\n            predicted_class_id = encoder_manager.predict_class(embedding, class_id)\n\n            # If the predicted class ID is not None, convert it to the corresponding class name\n            if predicted_class_id is not None:\n                predicted_class_name = class_id_to_class_name[predicted_class_id]\n            else:\n                predicted_class_name = None\n\n            # If the predicted class name is not None, increment the correct count for the true class name\n            if predicted_class_name is not None:\n                class_correct[true_class_name] += 1\n\n        # Increment the total count for all samples\n        total += 1\n\n    # Print the accuracy for each class\n    for class_name,", "CodeBleu_score": 31.856076820949024}, {"code": "def compute_encoder_accuracy(dataset_items, encoder_manager)\n\t\n    # Get class name key and optional class ID to class name mapping from the encoder manager's configuration\n    class_name_key = encoder_manager.config.class_name_key\n    class_id_to_class_name_mapping = encoder_manager.config.class_id_to_class_name_mapping\n\n    # Initialize counters for correct and total predictions\n    correct_predictions = 0\n    total_predictions = 0\n\n    # Initialize a dictionary to store the accuracy for each class\n    class_accuracy = {}\n\n    # Loop through each item in the dataset\n    for item in tqdm(dataset_items, desc=\"Computing encoder accuracy\"):\n        # Load the audio file and compute the embedding\n        audio_file = item[\"audio_file\"]\n        embedding = encoder_manager.compute_embedding(audio_file)\n\n        # Get the true class name and class ID from the item\n        true_class_name = item[class_name_key]\n        true_class_id = item.get(\"class_id\")\n\n        # If the class ID is not available, skip the item\n        if true_class_id is None:\n            continue\n\n        # Predict the class label using the encoder manager\n        class_label = encoder_manager.predict_class_label(embedding)\n\n        # If the class label is not available, skip the item\n        if class_label is None:\n            continue\n\n        # Increment the total predictions counter\n        total_predictions += 1\n\n        # Check if the predicted class label matches the true class name\n        if class_label == true_class_name:\n            correct_predictions += 1\n\n        # Update the accuracy for the true class name\n        class_accuracy[true_class_name] = class_accuracy.get(true_class_name, 0) + 1\n\n    # Calculate the average accuracy across all classes\n    average_accuracy = correct_predictions / total_predictions\n\n    # Print the accuracy for each class and the average accuracy\n    print(f\"Accuracy for each class:\")\n    for class_name, count in class_accuracy.items():\n        accuracy = count / total_pred", "CodeBleu_score": 29.441165664796614}]}
{"instance_id": "e01ec69ad58a", "generation_result": [{"code": "def adjust_path_and_remove_silence(audio_path)\n\t\n    # Get the input and output directory paths\n    input_dir = os.path.dirname(audio_path)\n    output_dir = input_dir.replace(args.input_dir, args.output_dir)\n\n    # Create the output directory if it doesn't exist\n    pathlib.Path(output_dir).mkdir(parents=True, exist_ok=True)\n\n    # Get the output file path\n    output_path = os.path.join(output_dir, os.path.basename(audio_path))\n\n    # Check if the output file already exists\n    if os.path.exists(output_path) and not args.force:\n        return output_path, False\n\n    # Load the VAD model and utils\n    model, utils = get_vad_model_and_utils(args.vad_model_path, args.vad_model_config_path)\n\n    # Load the audio file\n    waveform, sample_rate = torchaudio.load(audio_path)\n\n    # Remove silence from the audio\n    speech_waveform, speech_indices = remove_silence(waveform, sample_rate, model, utils, args.vad_silence_threshold)\n\n    # Check if speech was detected\n    speech_detected = len(speech_indices) > 0\n\n    # Save the processed audio\n    if speech_detected:\n        torchaudio.save(output_path, speech_waveform, sample_rate)\n\n    return output_path, speech_detected\n\n", "CodeBleu_score": 43.76167335094657}, {"code": "def adjust_path_and_remove_silence(audio_path)\n\t\n    # Define the input and output directory paths\n    input_dir = pathlib.Path(args.input_dir)\n    output_dir = pathlib.Path(args.output_dir)\n\n    # Replace the input directory path with the output directory path to determine the new file location\n    output_path = str(output_dir / audio_path.relative_to(input_dir))\n\n    # If the file already exists and force is not specified, return the output path and a False flag\n    if os.path.exists(output_path) and not args.force:\n        return output_path, False\n\n    # Create the necessary directory structure\n    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n\n    # Load the VAD model and utils\n    vad_model, vad_utils = get_vad_model_and_utils()\n\n    # Remove silence from the audio using specified parameters\n    speech_tensor, speech_rate = remove_silence(audio_path, vad_model, vad_utils, args.vad_threshold, args.vad_min_silence_duration, args.vad_max_silence_duration)\n\n    # Save the processed audio\n    torchaudio.save(output_path, speech_tensor, speech_rate)\n\n    # Return the output path and a flag indicating if speech was detected\n    return output_path, speech_tensor.shape[1] > 0\n\n", "CodeBleu_score": 43.42542580309505}, {"code": "def adjust_path_and_remove_silence(audio_path)\n\t\n    # Get the input and output directory paths from the input path\n    input_dir, input_file = os.path.split(audio_path)\n    output_dir = input_dir.replace(args.input_dir, args.output_dir)\n\n    # Create the output directory if it doesn't exist\n    pathlib.Path(output_dir).mkdir(parents=True, exist_ok=True)\n\n    # Determine the output file path\n    output_file = os.path.join(output_dir, input_file)\n\n    # Check if the output file already exists\n    if os.path.exists(output_file) and not args.force:\n        return output_file, False\n\n    # Load the VAD model and utils\n    vad_model, vad_utils = get_vad_model_and_utils(args.vad_model_path)\n\n    # Load the audio file\n    audio, sr = torchaudio.load(audio_path)\n\n    # Remove silence from the audio using specified parameters\n    audio, speech_detected = remove_silence(audio, sr, vad_model, vad_utils, args.min_silence_duration, args.min_speech_duration, args.silence_threshold, args.speech_threshold)\n\n    # Save the processed audio\n    torchaudio.save(output_file, audio, sr)\n\n    return output_file, speech_detected\n\n", "CodeBleu_score": 39.01863131854946}]}
{"instance_id": "7e0bdb0feff8", "generation_result": [{"code": "def resample_files(input_dir, output_sr, output_dir=None, file_ext=\"wav\", n_jobs=10)\n\t\n    if output_dir is None:\n        output_dir = input_dir\n    if not os.path.exists(output_dir):\n        copytree(input_dir, output_dir)\n    file_list = glob.glob(os.path.join(input_dir, \"**\", f\"*.{file_ext}\"), recursive=True)\n    p = Pool(n_jobs)\n    for _ in tqdm(p.imap(lambda x: librosa.load(x, sr=output_sr), file_list), total=len(file_list)):\n        pass\n    print(\"Done\")\n\n", "CodeBleu_score": 37.49613980845675}, {"code": "def resample_files(input_dir, output_sr, output_dir=None, file_ext=\"wav\", n_jobs=10)\n\t\n    if output_dir is None:\n        output_dir = input_dir\n    elif os.path.exists(output_dir):\n        copytree(input_dir, output_dir, dirs_exist_ok=True)\n    else:\n        os.makedirs(output_dir)\n        copytree(input_dir, output_dir, dirs_exist_ok=True)\n\n    file_list = glob.glob(os.path.join(input_dir, \"**\", f\"*.{file_ext}\"), recursive=True)\n\n    def resample_file(file_path):\n        y, sr = librosa.load(file_path, sr=None)\n        y_resampled = librosa.resample(y, sr, output_sr)\n        output_file_path = os.path.join(output_dir, os.path.relpath(file_path, input_dir))\n        librosa.output.write_wav(output_file_path, y_resampled, output_sr)\n\n    with Pool(n_jobs) as p:\n        list(tqdm(p.imap(resample_file, file_list), total=len(file_list), desc=\"Resampling files\"))\n\n    print(f\"Resampling complete. Output files saved to {output_dir}.\")\n\n", "CodeBleu_score": 36.57843002622262}, {"code": "def resample_files(input_dir, output_sr, output_dir=None, file_ext=\"wav\", n_jobs=10)\n\t\n    if output_dir is None:\n        output_dir = input_dir\n\n    if not os.path.exists(output_dir):\n        copytree(input_dir, output_dir)\n\n    files = glob.glob(os.path.join(output_dir, \"**/*.\" + file_ext), recursive=True)\n    p = Pool(n_jobs)\n    for _ in tqdm(p.imap_unordered(resample_file, [(file, output_sr, output_dir) for file in files]), total=len(files)):\n        pass\n    p.close()\n    p.join()\n    print(\"Resampling completed.\")\n\n", "CodeBleu_score": 39.808832069853466}]}
{"instance_id": "a1351834ca57", "generation_result": [{"code": "def evaluation(model, criterion, data_loader, global_step)\n\t\n    model.eval()\n    total_loss = 0\n    with torch.no_grad():\n        for i, (inputs, labels) in enumerate(data_loader):\n            inputs = inputs.cuda()\n            labels = labels.cuda()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            total_loss += loss.item()\n\n            if i == len(data_loader) - 1:\n                plot_embeddings(\n                    outputs.cpu().numpy(),\n                    labels.cpu().numpy(),\n                    global_step,\n                    model.config.model_name,\n                )\n\n    avg_loss = total_loss / len(data_loader)\n    return avg_loss\n\n", "CodeBleu_score": 27.11031752958211}, {"code": "def evaluation(model, criterion, data_loader, global_step)\n\t\n    model.eval()\n    avg_loss = 0\n    for i, (inputs, labels) in enumerate(data_loader):\n        inputs = inputs.cuda()\n        labels = labels.cuda()\n        with torch.no_grad():\n            outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        avg_loss += loss.item()\n        if i == len(data_loader) - 1:\n            plot_embeddings(outputs, labels, global_step)\n    avg_loss /= len(data_loader)\n    return avg_loss\n\n", "CodeBleu_score": 19.3796840630303}, {"code": "def evaluation(model, criterion, data_loader, global_step)\n\t\n    model.eval()\n    avg_loss = 0\n    with torch.no_grad():\n        for i, (inputs, labels) in enumerate(data_loader):\n            inputs = inputs.squeeze(1)\n            labels = labels.squeeze(1)\n            if torch.cuda.is_available():\n                inputs = inputs.cuda()\n                labels = labels.cuda()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            avg_loss += loss.item()\n            if i == len(data_loader) - 1:\n                plot_embeddings(\n                    outputs.detach().cpu().numpy(),\n                    labels.detach().cpu().numpy(),\n                    global_step,\n                )\n    avg_loss /= len(data_loader)\n    print(\"Evaluation Loss: {:.4f}\".format(avg_loss))\n    return avg_loss\n\n", "CodeBleu_score": 31.057770565359167}]}
{"instance_id": "565f8ed09916", "generation_result": [{"code": "def register_config(model_name: str) -> Coqpit\n\t\n    config_class_name = f\"{model_name}_config\"\n    config_class = find_module(config_class_name, \"TTS.config\")\n    if config_class is None:\n        raise ModuleNotFoundError(\n            f\"{config_class_name} not found. Please register the config class for the model {model_name}.\"\n        )\n    return config_class\n\n", "CodeBleu_score": 21.649432035603017}, {"code": "def register_config(model_name: str) -> Coqpit\n\t\n    config_class_name = model_name.capitalize() + \"Config\"\n    if model_name == \"xtts\":\n        config_class_name = \"XttsConfig\"\n    try:\n        config_class = find_module(config_class_name, \"TTS.config\")\n    except ModuleNotFoundError:\n        raise ModuleNotFoundError(\n            f\"Could not find config class for {model_name}. Please make sure that the config file is present in the TTS.config folder.\"\n        )\n    return config_class\n\n", "CodeBleu_score": 31.844073481322933}, {"code": "def register_config(model_name: str) -> Coqpit\n\t\n    config_class_name = model_name.replace(\"-\", \"_\") + \"_config\"\n    config_class = find_module(config_class_name, \"TTS.config\")\n    if config_class is None:\n        raise ModuleNotFoundError(f\"No config found for model {model_name}\")\n    return config_class\n\n", "CodeBleu_score": 18.539989819021446}]}
{"instance_id": "0ea7862506a7", "generation_result": [{"code": "def load_config(config_path: str) -> Coqpit\n\t\n    if config_path is None:\n        raise ValueError(\"No config path specified.\")\n\n    if not os.path.exists(config_path):\n        raise ValueError(f\"Config path {config_path} does not exist.\")\n\n    # Get file extension\n    ext = os.path.splitext(config_path)[1].lower()\n\n    # Read configuration file\n    if ext == \".yaml\":\n        with open(config_path, \"r\") as f:\n            config_dict = yaml.safe_load(f)\n    elif ext == \".json\":\n        with open(config_path, \"r\") as f:\n            config_dict = json.load(f)\n    else:\n        raise ValueError(f\"Unsupported config file extension: {ext}\")\n\n    # Get model name\n    model_name = config_dict.get(\"model_name\", None)\n    if model_name is None:\n        raise ValueError(\"No model name specified in config.\")\n\n    # Get config class\n    config_class = find_module(model_name, \"config\")\n    if config_class is None:\n        raise ValueError(f\"No config class found for model: {model_name}\")\n\n    # Create instance of config class\n    config = config_class(**config_dict)\n\n    return config\n\n", "CodeBleu_score": 48.83889124299438}, {"code": "def load_config(config_path: str) -> Coqpit\n\t\n    config_path = os.path.expanduser(config_path)\n    ext = os.path.splitext(config_path)[-1].lower()\n    if ext == \".yaml\":\n        with fsspec.open(config_path, \"r\") as f:\n            config_dict = yaml.safe_load(f)\n    elif ext == \".json\":\n        with fsspec.open(config_path, \"r\") as f:\n            config_dict = json.load(f)\n    else:\n        raise ValueError(f\"Unknown config type: {ext}\")\n\n    model_name = config_dict.pop(\"model_name\", \"\")\n    if model_name:\n        config_cls = find_module(model_name, \"config\")\n        if config_cls is None:\n            raise ValueError(f\"Unknown model name: {model_name}\")\n        config = config_cls()\n        config.update(config_dict)\n    else:\n        config = Coqpit(config_dict)\n    return config\n\n", "CodeBleu_score": 50.69100874432364}, {"code": "def load_config(config_path: str) -> Coqpit\n\t\n    # determine file extension\n    _, ext = os.path.splitext(config_path)\n    if ext == \".json\":\n        with fsspec.open(config_path, \"r\") as f:\n            config_dict = json.load(f)\n    elif ext == \".yaml\":\n        with fsspec.open(config_path, \"r\") as f:\n            config_dict = yaml.safe_load(f)\n    else:\n        raise ValueError(f\"Unknown file extension: {ext}\")\n\n    # process config dictionary\n    model_name = config_dict.pop(\"model_name\", \"\")\n    if model_name == \"\":\n        model_name = config_dict.pop(\"name\", \"\")\n    if model_name == \"\":\n        raise ValueError(\"Model name not found in config file.\")\n\n    # register config class\n    config_class = find_module(model_name)\n    if config_class is None:\n        raise ValueError(f\"Model name not found: {model_name}\")\n    config_class.register_config(model_name)\n\n    # create config instance\n    config = config_class()\n\n    # populate config with data\n    config.from_dict(config_dict)\n\n    return config\n\n", "CodeBleu_score": 49.13233945583866}]}
{"instance_id": "e0db68b63143", "generation_result": [{"code": "def list_files(basePath, validExts=None, contains=None)\n\t\n    for (rootDir, dirNames, filenames) in os.walk(basePath):\n        for filename in filenames:\n            if contains is not None and filename.find(contains) == -1:\n                continue\n            ext = filename[filename.rfind(\".\"):].lower()\n            if validExts is None or ext.endswith(validExts):\n                yield os.path.join(rootDir, filename)\n\n", "CodeBleu_score": 68.80946695862164}, {"code": "def list_files(basePath, validExts=None, contains=None)\n\t\n    for (rootDir, dirNames, filenames) in os.walk(basePath):\n        for filename in filenames:\n            if contains is not None and filename.find(contains) == -1:\n                continue\n            ext = filename[filename.rfind(\".\"):].lower()\n            if validExts is None or ext.endswith(validExts):\n                yield os.path.join(rootDir, filename)\n\n", "CodeBleu_score": 68.80946695862164}, {"code": "def list_files(basePath, validExts=None, contains=None)\n\t\n    for (rootDir, dirNames, filenames) in os.walk(basePath):\n        for filename in filenames:\n            if contains is not None and filename.find(contains) == -1:\n                continue\n\n            ext = filename[filename.rfind(\".\"):].lower()\n            if validExts is None or ext.endswith(validExts):\n                yield os.path.join(rootDir, filename)\n\n", "CodeBleu_score": 68.80946695862164}]}
{"instance_id": "b836fcaf0a27", "generation_result": [{"code": "def load_model(xtts_checkpoint, xtts_config, xtts_vocab)\n\t\n    torch.cuda.empty_cache()\n    if not xtts_checkpoint or not xtts_config or not xtts_vocab:\n        return \"Error: Please provide all required paths.\"\n    config = XttsConfig.from_json(xtts_config)\n    model = Xtts(config)\n    model.load_state_dict(torch.load(xtts_checkpoint)['model_state'])\n    model.load_vocoder_state(xtts_vocab)\n    if torch.cuda.is_available():\n        model.cuda()\n    return \"Success: Model loaded.\"\n\n", "CodeBleu_score": 36.348453387830745}, {"code": "def load_model(xtts_checkpoint, xtts_config, xtts_vocab)\n\t\n    torch.cuda.empty_cache()\n\n    if xtts_checkpoint is None or xtts_config is None or xtts_vocab is None:\n        return \"Please provide all required paths: checkpoint, config, and vocab\"\n\n    config = XttsConfig.from_json(xtts_config)\n    model = Xtts(config)\n    model.load_state_dict(torch.load(xtts_checkpoint)[\"model\"])\n    model.eval()\n    model.decoder.load_state_dict(torch.load(xtts_vocab)[\"model\"])\n\n    if torch.cuda.is_available():\n        model.cuda()\n\n    print(\"Loaded XTTS model from checkpoint: {}\".format(xtts_checkpoint))\n    return model\n\n", "CodeBleu_score": 45.42934587276889}, {"code": "def load_model(xtts_checkpoint, xtts_config, xtts_vocab)\n\t\n    # Clear GPU cache\n    torch.cuda.empty_cache()\n\n    # Check if required paths are provided\n    if xtts_checkpoint is None:\n        logging.error(\"Please provide a path to the XTTS checkpoint.\")\n        return None\n    if xtts_config is None:\n        logging.error(\"Please provide a path to the XTTS config.\")\n        return None\n    if xtts_vocab is None:\n        logging.error(\"Please provide a path to the XTTS vocab.\")\n        return None\n\n    # Load configuration from provided path\n    config = XttsConfig.from_json(xtts_config)\n\n    # Initialize XTTS model\n    model = Xtts(config)\n\n    # Load model checkpoint and vocabulary\n    model.load_state_dict(torch.load(xtts_checkpoint)[\"model\"])\n    model.load_vocoder_state_dict(torch.load(xtts_vocab)[\"model\"])\n\n    # Move model to GPU if available\n    if torch.cuda.is_available():\n        model.cuda()\n\n    # Print loading progress\n    logging.info(\"Loaded XTTS model from checkpoint: {}\".format(xtts_checkpoint))\n    logging.info(\"Loaded XTTS config from: {}\".format(xtts_config))\n    logging.info(\"Loaded XTTS vocab from: {}\".format(xtts_vocab))\n\n    # Return success message\n    return \"XTTS model loaded successfully.\"\n\n", "CodeBleu_score": 46.15720149362189}]}
{"instance_id": "c5d63aa0df26", "generation_result": [{"code": "def compute_embedding(self, x, num_frames=250, num_eval=10, return_mean=True, l2_norm=True)\n\t\n    # Adjust number of frames based on hop length if using torch specifications\n    if isinstance(x, torch.Tensor) and x.ndim == 3:\n        hop_length = torchaudio.transforms.Resample.get_hop_length(self.hop_length)\n        num_frames = int(num_frames / hop_length)\n\n    # Ensure the number of frames does not exceed the input length\n    num_frames = min(num_frames, x.shape[-1])\n\n    # Compute offset positions for evaluation\n    offsets = torch.arange(0, num_frames) * self.hop_length\n\n    # Extract frames at these offsets, concatenate them into a batch, and perform inference to obtain embeddings\n    x = x[:, :, offsets]\n    x = x.reshape(-1, x.shape[-2], x.shape[-1])\n    emb = self.model(x)\n\n    # If specified, compute the mean of the embeddings and return the result, optionally normalizing them with L2 norm\n    if return_mean:\n        emb = emb.reshape(-1, num_eval, emb.shape[-1])\n        emb = emb.mean(dim=1)\n        if l2_norm:\n            emb = nn.functional.normalize(emb, dim=-1)\n\n    return emb\n\n", "CodeBleu_score": 37.06726611138444}, {"code": "def compute_embedding(self, x, num_frames=250, num_eval=10, return_mean=True, l2_norm=True)\n\t\n    # adjust number of frames based on hop length\n    if isinstance(x, torchaudio.spec.Spectrogram):\n        num_frames = int(num_frames * x.spec.shape[-1] / x.shape[-1])\n\n    # ensure number of frames does not exceed input length\n    num_frames = min(num_frames, x.shape[-1])\n\n    # calculate offset positions for evaluation\n    offsets = torch.arange(0, x.shape[-1] - num_frames + 1, x.shape[-1] // num_eval)\n    offsets = offsets[(offsets + num_frames) <= x.shape[-1]]\n\n    # extract frames at these offsets, concatenate them into a batch, and perform inference to obtain embeddings\n    embeddings = []\n    for offset in offsets:\n        frames = x[..., offset : offset + num_frames]\n        frames = frames.unsqueeze(0)\n        embeddings.append(self.forward(frames).squeeze(0))\n    embeddings = torch.stack(embeddings, dim=0)\n\n    # compute the mean of the embeddings and return the result, optionally normalizing them with L2 norm\n    if return_mean:\n        embeddings = embeddings.mean(dim=0)\n        if l2_norm:\n            embeddings = nn.functional.normalize(embeddings, dim=-1)\n    return embeddings\n\n", "CodeBleu_score": 42.96719773876573}, {"code": "def compute_embedding(self, x, num_frames=250, num_eval=10, return_mean=True, l2_norm=True)\n\t\n    # Adjust number of frames based on hop length if using torch specifications\n    if isinstance(x, torch.Tensor) and x.shape[1] == 1:\n        hop_length = int(torchaudio.transforms.Resample.get_hop_length(x.shape[-1], x.shape[-1] // num_frames))\n        num_frames = x.shape[-1] // hop_length\n    # Ensure number of frames does not exceed the input length\n    num_frames = min(num_frames, x.shape[-1])\n    # Calculate offset positions for evaluation\n    offsets = torch.arange(0, x.shape[-1] - num_frames + 1, x.shape[-1] // num_eval)\n    # Extract frames at these offsets, concatenate into a batch, and perform inference to obtain embeddings\n    x_frames = x[:, offsets]\n    x_frames = x_frames.reshape(-1, x_frames.shape[-2], x_frames.shape[-1])\n    embeddings = self.encoder(x_frames)\n    # Compute the mean of the embeddings and return the result, optionally normalizing them with L2 norm\n    if return_mean:\n        embeddings = embeddings.mean(dim=0)\n        if l2_norm:\n            embeddings = nn.functional.normalize(embeddings, p=2, dim=0)\n    return embeddings\n\n", "CodeBleu_score": 34.02891589452427}]}
{"instance_id": "06f5b0335b26", "generation_result": [{"code": "def create_layer(self, block, planes, blocks, stride=1)\n\t\n    downsample = None\n    if stride != 1 or self.inplanes != planes * block.expansion:\n        downsample = nn.Sequential(\n            nn.Conv2d(\n                self.inplanes, planes * block.expansion, kernel_size=1, stride=stride, bias=False\n            ),\n            nn.BatchNorm2d(planes * block.expansion),\n        )\n\n    layers = []\n    layers.append(block(self.inplanes, planes, stride, downsample))\n    self.inplanes = planes * block.expansion\n    for i in range(1, blocks):\n        layers.append(block(self.inplanes, planes))\n\n    return nn.Sequential(*layers)\n\n", "CodeBleu_score": 83.81702235562578}, {"code": "def create_layer(self, block, planes, blocks, stride=1)\n\t\n    downsample = None\n    if stride != 1 or self.inplanes != planes * block.expansion:\n        downsample = nn.Sequential(\n            nn.Conv2d(\n                self.inplanes, planes * block.expansion, kernel_size=1, stride=stride, bias=False\n            ),\n            nn.BatchNorm2d(planes * block.expansion),\n        )\n\n    layers = []\n    layers.append(block(self.inplanes, planes, stride, downsample))\n    self.inplanes = planes * block.expansion\n    for i in range(1, blocks):\n        layers.append(block(self.inplanes, planes))\n\n    return nn.Sequential(*layers)\n\n", "CodeBleu_score": 83.81702235562578}, {"code": "def create_layer(self, block, planes, blocks, stride=1)\n\t\n    downsample = None\n    if stride != 1 or self.inplanes != planes * block.expansion:\n        downsample = nn.Sequential(\n            nn.Conv2d(\n                self.inplanes, planes * block.expansion, kernel_size=1, stride=stride, bias=False\n            ),\n            nn.BatchNorm2d(planes * block.expansion),\n        )\n\n    layers = []\n    layers.append(block(self.inplanes, planes, stride, downsample))\n    self.inplanes = planes * block.expansion\n    for i in range(1, blocks):\n        layers.append(block(self.inplanes, planes))\n\n    return nn.Sequential(*layers)\n\n", "CodeBleu_score": 83.81702235562578}]}
{"instance_id": "9ac60597b633", "generation_result": [{"code": "def plot_embeddings(embeddings, num_classes_in_batch)\n\t\n    # Calculate the number of utterances per class\n    num_utterances_per_class = [\n        len(embeddings[embeddings[:, -1] == i]) for i in range(num_classes_in_batch)\n    ]\n\n    # Limit the number of classes to 10 if necessary\n    if num_classes_in_batch > 10:\n        num_classes_in_batch = 10\n\n    # UMAP transforms the embeddings\n    reducer = umap.UMAP()\n    embeddings_umap = reducer.fit_transform(embeddings[:, :-1])\n\n    # Create a scatter plot with colors representing different classes\n    fig = matplotlib.pyplot.figure(figsize=(10, 10))\n    ax = fig.add_subplot(111)\n    for i in range(num_classes_in_batch):\n        ax.scatter(\n            embeddings_umap[embeddings[:, -1] == i, 0],\n            embeddings_umap[embeddings[:, -1] == i, 1],\n            label=f\"Class {i} ({num_utterances_per_class[i]} utterances)\",\n        )\n    ax.legend()\n    ax.set_title(\"UMAP projection\")\n    ax.set_aspect(\"equal\")\n    matplotlib.pyplot.savefig(\"umap.png\")\n    return fig", "CodeBleu_score": 34.3016616345893}, {"code": "def plot_embeddings(embeddings, num_classes_in_batch)\n\t\n    # Calculate the number of utterances per class\n    num_utterances_per_class = [0] * num_classes_in_batch\n    for i in range(len(embeddings)):\n        num_utterances_per_class[i % num_classes_in_batch] += 1\n\n    # Limit the number of classes to 10 if necessary\n    if num_classes_in_batch > 10:\n        num_classes_in_batch = 10\n\n    # UMAP transforms the embeddings\n    reducer = umap.UMAP()\n    reduced_embeddings = reducer.fit_transform(embeddings)\n\n    # Create a scatter plot with colors representing different classes\n    fig = matplotlib.pyplot.figure()\n    ax = fig.add_subplot()\n    for i in range(num_classes_in_batch):\n        ax.scatter(\n            reduced_embeddings[i::num_classes_in_batch, 0],\n            reduced_embeddings[i::num_classes_in_batch, 1],\n            label=f\"Class {i} ({num_utterances_per_class[i]} utterances)\",\n        )\n    ax.legend()\n\n    # Adjust the plot for equal aspect ratio\n    ax.set_aspect(\"equal\")\n\n    # Set the title of the plot\n    ax.set_title(\"UMAP projection\")\n\n    # Save the plot as \"umap\"\n    matplotlib.pyplot.savefig(\"umap\")\n\n    return fig", "CodeBleu_score": 39.52660026953494}, {"code": "def plot_embeddings(embeddings, num_classes_in_batch)\n\t\n    # Calculate the number of utterances per class\n    num_utterances_per_class = [\n        sum(1 for _ in range(num_classes_in_batch) if _ == i)\n        for i in range(num_classes_in_batch)\n    ]\n\n    # Limit the number of classes to 10 if necessary\n    if num_classes_in_batch > 10:\n        num_classes_in_batch = 10\n\n    # UMAP transforms the embeddings\n    reducer = umap.UMAP()\n    embeddings_umap = reducer.fit_transform(embeddings)\n\n    # Create a scatter plot with colors representing different classes\n    fig = matplotlib.pyplot.figure()\n    ax = fig.add_subplot(111)\n    for i in range(num_classes_in_batch):\n        ax.scatter(\n            embeddings_umap[i * num_utterances_per_class[i] : (i + 1) * num_utterances_per_class[i], 0],\n            embeddings_umap[i * num_utterances_per_class[i] : (i + 1) * num_utterances_per_class[i], 1],\n            label=str(i),\n        )\n    ax.legend()\n\n    # Adjust the plot for equal aspect ratio\n    ax.set_aspect(\"equal\")\n\n    # Set the title of the plot\n    ax.set_title(\"UMAP projection\")\n\n    # Save the plot as \"umap\"\n    fig.savefig(\"umap.png\")\n\n    # Return the figure\n    return fig", "CodeBleu_score": 32.81023657680366}]}
{"instance_id": "19c2b4af256d", "generation_result": [{"code": "def calc_cosine_sim(self, dvecs, centroids)\n\t\n    cosine_sim = []\n    for spk_idx, spk_dvecs in enumerate(dvecs):\n        new_centroids = torch.cat([c for i, c in enumerate(centroids) if i != spk_idx])\n        cosine_sim.append(nn.CosineSimilarity(dim=1)(spk_dvecs, new_centroids))\n    cosine_sim = torch.clamp(torch.cat(cosine_sim, dim=0), min=0.0)\n    return cosine_sim\n\n", "CodeBleu_score": 25.49448642924592}, {"code": "def calc_cosine_sim(self, dvecs, centroids)\n\t\n    # Calculate cosine similarity between d-vectors and centroids\n    cosine_sim = torch.matmul(dvecs, centroids.T)\n    # Clamp the cosine similarity values to avoid small values\n    cosine_sim = cosine_sim.clamp(min=1e-6)\n    # Concatenate the cosine similarity values for each speaker\n    cosine_sim = torch.cat(cosine_sim, dim=0)\n    # Stack the cosine similarity values for each speaker\n    cosine_sim = torch.stack(cosine_sim, dim=0)\n    return cosine_sim\n\n", "CodeBleu_score": 4.930928889071918}, {"code": "def calc_cosine_sim(self, dvecs, centroids)\n\t\n    cos_sim_mat = []\n    for speaker_idx, speaker_dvecs in enumerate(dvecs):\n        speaker_cos_sim = []\n        for utterance_idx, utterance_dvec in enumerate(speaker_dvecs):\n            new_centroids = torch.cat(\n                [\n                    centroids[speaker_idx][:utterance_idx],\n                    centroids[speaker_idx][utterance_idx + 1 :],\n                ]\n            )\n            speaker_cos_sim.append(\n                (\n                    utterance_dvec.unsqueeze(0)\n                    @ new_centroids.transpose(0, 1)\n                    / (\n                        utterance_dvec.norm(dim=-1).unsqueeze(0)\n                        * new_centroids.norm(dim=-1).unsqueeze(1)\n                    )\n                )\n                .clamp(min=1e-6)\n                .squeeze()\n            )\n        speaker_cos_sim = torch.stack(speaker_cos_sim, dim=0)\n        cos_sim_mat.append(speaker_cos_sim)\n    return torch.stack(cos_sim_mat, dim=0)\n\n", "CodeBleu_score": 38.93441828001674}]}
{"instance_id": "c525b411a1e7", "generation_result": [{"code": " def embed_loss_softmax(self, dvecs, cos_sim_matrix)\n\t\n    embed_losses = []\n    for i in range(len(dvecs)):\n        embed_losses.append(nn.functional.nll_loss(nn.functional.log_softmax(cos_sim_matrix[i], dim=0), torch.tensor([i]).to(self.device)))\n    return torch.stack(embed_losses)\n\n", "CodeBleu_score": 32.65281241284866}, {"code": " def embed_loss_softmax(self, dvecs, cos_sim_matrix)\n\t\n    losses = []\n    for dvec in dvecs:\n        # Calculate the negative log softmax of the cosine similarity matrix for the current embedding\n        loss = -torch.log(torch.softmax(cos_sim_matrix, dim=1)[dvec])\n        # Append the loss to the list of losses\n        losses.append(loss)\n    # Stack the losses into a single tensor and return it\n    return torch.stack(losses)\n\n", "CodeBleu_score": 26.015984449814354}, {"code": " def embed_loss_softmax(self, dvecs, cos_sim_matrix)\n\t\n    batch_size = dvecs.shape[0]\n    losses = []\n    for i in range(batch_size):\n        loss = -torch.log_softmax(cos_sim_matrix[i], dim=0)\n        losses.append(loss)\n    return torch.stack(losses)\n\n", "CodeBleu_score": 25.519439611663802}]}
{"instance_id": "25ac3945da2e", "generation_result": [{"code": "def embed_loss_contrast(self, dvecs, cos_sim_matrix)\n\t\n    loss_row = []\n    for i, dvec in enumerate(dvecs):\n        # compute sigmoid of cosine similarity matrix for current embedding\n        sigmoid_cos_sim_matrix = nn.Sigmoid()(cos_sim_matrix)\n        # exclude the current centroid\n        sigmoid_cos_sim_matrix[i, i] = 0\n        # compute contrast loss\n        contrast_loss = -torch.log(sigmoid_cos_sim_matrix[i]).sum()\n        # append contrast loss to loss row\n        loss_row.append(contrast_loss)\n    # stack loss row into tensor\n    loss_row = torch.stack(loss_row)\n    return loss_row\n\n", "CodeBleu_score": 25.38276224651771}, {"code": "def embed_loss_contrast(self, dvecs, cos_sim_matrix)\n\t\n    sigmoid_cos_sim_matrix = torch.sigmoid(cos_sim_matrix)\n    loss_row = []\n    for i in range(dvecs.shape[0]):\n        # Exclude the current centroid\n        exclude_current_centroid = torch.cat(\n            (sigmoid_cos_sim_matrix[i, :i], sigmoid_cos_sim_matrix[i, i + 1 :])\n        )\n        loss_row.append(\n            torch.mean(\n                torch.log(1 + torch.exp(-exclude_current_centroid))\n                + torch.log(1 + torch.exp(-sigmoid_cos_sim_matrix[i, i]))\n            )\n        )\n    return torch.stack(loss_row)\n\n", "CodeBleu_score": 27.80486941096538}, {"code": "def embed_loss_contrast(self, dvecs, cos_sim_matrix)\n\t\n    # Initialize an empty list to store the loss row for each embedding\n    loss_row = []\n    # Iterate over the embeddings\n    for i in range(dvecs.shape[0]):\n        # Exclude the current centroid from the cosine similarity matrix\n        cos_sim_matrix_exclude_curr = cos_sim_matrix[i, :]\n        cos_sim_matrix_exclude_curr[i] = -1\n        # Compute the sigmoid of the cosine similarity matrix\n        sigmoid_cos_sim_matrix = torch.sigmoid(cos_sim_matrix_exclude_curr)\n        # Compute the contrast loss\n        contrast_loss = torch.sum(sigmoid_cos_sim_matrix)\n        # Append the contrast loss to the loss row\n        loss_row.append(contrast_loss)\n    # Stack the loss row into a tensor\n    loss_row = torch.stack(loss_row)\n    # Return the final stacked tensor of losses\n    return loss_row\n\n", "CodeBleu_score": 30.097536719259192}]}
{"instance_id": "878d14ffabd2", "generation_result": [{"code": "def forward(self, x, _label=None)\n\t\n    # Ensure input has at least two utterances per speaker\n    if x.shape[1] < 2:\n        raise ValueError(\n            f\"Expected input to have at least two utterances per speaker, but got {x.shape[1]} utterances per speaker.\"\n        )\n    # Compute centroids by averaging the embeddings\n    centroids = x.mean(dim=1)\n    # Calculate a cosine similarity matrix between the embeddings and centroids\n    sim_matrix = torch.matmul(x, centroids.t())\n    # Scale and shift the similarity matrix\n    sim_matrix = sim_matrix / self.scale\n    sim_matrix = sim_matrix + self.shift\n    # Compute the embedding loss\n    loss = nn.CrossEntropyLoss()(sim_matrix, torch.arange(x.shape[0]).to(x.device))\n    return loss", "CodeBleu_score": 34.593689343664316}, {"code": "def forward(self, x, _label=None)\n\t\n    x = x.reshape(x.shape[0], x.shape[1], -1)\n    x = nn.functional.normalize(x, p=2, dim=2)\n    centroids = torch.mean(x, dim=1, keepdim=True)\n    similarity_matrix = torch.matmul(x, centroids.transpose(1, 2))\n    similarity_matrix = similarity_matrix - torch.max(similarity_matrix, dim=2, keepdim=True).values\n    similarity_matrix = similarity_matrix / self.scale\n    similarity_matrix = similarity_matrix + self.shift\n    loss = nn.functional.cross_entropy(similarity_matrix, torch.arange(x.shape[0], device=x.device))\n    return loss\n\n", "CodeBleu_score": 31.5984920227531}, {"code": "def forward(self, x, _label=None)\n\t\n    x = x.reshape(x.shape[0], x.shape[1], -1)\n    centroids = torch.mean(x, dim=1)\n    sim_matrix = torch.matmul(centroids, centroids.transpose(0, 1))\n    sim_matrix = sim_matrix / torch.norm(centroids, dim=1)[:, None]\n    sim_matrix = sim_matrix / torch.norm(centroids, dim=1)[None, :]\n    sim_matrix = sim_matrix - torch.eye(sim_matrix.shape[0], device=sim_matrix.device)\n    loss = nn.functional.cross_entropy(sim_matrix, torch.arange(sim_matrix.shape[0], device=sim_matrix.device))\n    return loss", "CodeBleu_score": 31.221880911460897}]}
{"instance_id": "86a364c72caa", "generation_result": [{"code": "def get_voices(extra_voice_dirs: List[str] = [])\n\t\n    voices = {}\n    for voice_dir in extra_voice_dirs:\n        for root, dirs, files in os.walk(voice_dir):\n            if len(files) > 0:\n                voices[root] = []\n                for file in files:\n                    if file.endswith(\".npz\") or file.endswith(\".wav\") or file.endswith(\".mp3\"):\n                        voices[root].append(os.path.join(root, file))\n    if len(voices) == 0:\n        for root, dirs, files in os.walk(\"voices\"):\n            if len(files) > 0:\n                voices[root] = []\n                for file in files:\n                    if file.endswith(\".npz\") or file.endswith(\".wav\") or file.endswith(\".mp3\"):\n                        voices[root].append(os.path.join(root, file))\n    return voices\n\n", "CodeBleu_score": 26.561202561156726}, {"code": "def get_voices(extra_voice_dirs: List[str] = [])\n\t\n    voice_dirs = [\n        os.path.join(os.path.dirname(os.path.abspath(__file__)), \"voices\"),\n        *extra_voice_dirs,\n    ]\n    voices = {}\n    for voice_dir in voice_dirs:\n        for root, _, files in os.walk(voice_dir):\n            for file in files:\n                if file.endswith(\".npz\"):\n                    voices.setdefault(root, []).append(os.path.join(root, file))\n                elif file.endswith(\".wav\") or file.endswith(\".mp3\"):\n                    voices.setdefault(root, []).append(os.path.join(root, file))\n    return voices\n\n", "CodeBleu_score": 25.494353196318215}, {"code": "def get_voices(extra_voice_dirs: List[str] = [])\n\t\n    voices = {}\n    for dir in extra_voice_dirs:\n        for root, _, files in os.walk(dir):\n            for file in files:\n                if file.endswith(\".npz\"):\n                    if root not in voices:\n                        voices[root] = []\n                    voices[root].append(os.path.join(root, file))\n                elif file.endswith(\".wav\") or file.endswith(\".mp3\"):\n                    if root not in voices:\n                        voices[root] = []\n                    voices[root].append(os.path.join(root, file))\n    return voices\n\n", "CodeBleu_score": 27.22786039713718}]}
{"instance_id": "02f20c8917e6", "generation_result": [{"code": "def load_voice(model, voice: str, extra_voice_dirs: List[str] = [])\n\t\n    voice_paths = get_voices(model, voice, extra_voice_dirs)\n    if voice == \"random\":\n        return None, None, None\n    elif len(voice_paths) > 1:\n        raise ValueError(f\"Multiple voices found for {voice}: {voice_paths}\")\n    elif len(voice_paths) == 0:\n        raise ValueError(f\"No voice found for {voice}\")\n    voice_path = voice_paths[0]\n    if voice_path.endswith(\".npz\"):\n        voice_data = np.load(voice_path)\n        return voice_data[\"mean\"], voice_data[\"std\"], voice_data[\"name\"]\n    else:\n        npz_path = f\"{voice_path}.npz\"\n        if not os.path.exists(npz_path):\n            generate_npz(voice_path, npz_path)\n        voice_data = np.load(npz_path)\n        return voice_data[\"mean\"], voice_data[\"std\"], voice_data[\"name\"]\n\n", "CodeBleu_score": 44.434714155311525}, {"code": "def load_voice(model, voice: str, extra_voice_dirs: List[str] = [])\n\t\n    voice_paths = get_voices(voice, extra_voice_dirs)\n    if voice == \"random\":\n        return None, None, None, None\n    elif len(voice_paths) > 1:\n        raise ValueError(f\"Multiple voices found for {voice}: {voice_paths}\")\n    elif len(voice_paths) == 0:\n        raise ValueError(f\"No voice found for {voice}\")\n    voice_path = voice_paths[0]\n    if voice_path.endswith(\".npz\"):\n        voice_data = np.load(voice_path)\n        return voice_data[\"wav\"], voice_data[\"embeds\"], voice_data[\"rates\"], voice_data[\"embeds_d\"]\n    else:\n        npz_path = f\"{voice_path}.npz\"\n        if os.path.exists(npz_path):\n            voice_data = np.load(npz_path)\n        else:\n            wav, embeds, rates, embeds_d = generate_npz(voice_path, model)\n            np.savez(npz_path, wav=wav, embeds=embeds, rates=rates, embeds_d=embeds_d)\n            voice_data = np.load(npz_path)\n        return voice_data[\"wav\"], voice_data[\"embeds\"], voice_data[\"rates\"], voice_data[\"embeds_d\"]\n\n", "CodeBleu_score": 40.57194009860889}, {"code": "def load_voice(model, voice: str, extra_voice_dirs: List[str] = [])\n\t\n    voice_paths = get_voices(voice, extra_voice_dirs)\n    if voice == \"random\":\n        return None, None, None\n    if len(voice_paths) > 1:\n        raise ValueError(\n            f\"Multiple voices found for {voice}. Please specify the exact voice name.\"\n        )\n    if len(voice_paths) == 0:\n        raise ValueError(f\"No voice found for {voice}\")\n    voice_path = voice_paths[0]\n    if voice_path.endswith(\".npz\"):\n        voice_data = np.load(voice_path)\n        return (\n            voice_data[\"mean\"],\n            voice_data[\"std\"],\n            voice_data[\"model\"],\n        )\n    else:\n        npz_path = voice_path + \".npz\"\n        if not os.path.exists(npz_path):\n            convert_audio(voice_path, npz_path)\n        voice_data = np.load(npz_path)\n        return (\n            voice_data[\"mean\"],\n            voice_data[\"std\"],\n            voice_data[\"model\"],\n        )\n\n", "CodeBleu_score": 40.366865744191536}]}
{"instance_id": "9458e981389f", "generation_result": [{"code": "def _relative_shift(self, pos_score: torch.Tensor) -> torch.Tensor\n\t\n    b, h, w = pos_score.size()\n    zero_tensor = pos_score.new_zeros(b, h, w)\n    pos_score_cat = torch.cat([zero_tensor, pos_score], dim=-1)\n    pos_score_cat = pos_score_cat.view(b, h, w + 1)\n    pos_score = pos_score_cat[:, :, 1:].view_as(pos_score)\n    return pos_score\n\n", "CodeBleu_score": 28.7491227324298}, {"code": "def _relative_shift(self, pos_score: torch.Tensor) -> torch.Tensor\n\t\n    b, c, t = pos_score.shape\n    zero = torch.zeros((b, c, 1), device=pos_score.device, dtype=pos_score.dtype)\n    pos_score_pad = torch.cat([zero, pos_score], dim=-1)\n    pos_score_pad = pos_score_pad.view(b, c, t + 1, 2)\n    pos_score = pos_score_pad[:, :, :t, 1]\n    return pos_score\n\n", "CodeBleu_score": 26.45108971733583}, {"code": "def _relative_shift(self, pos_score: torch.Tensor) -> torch.Tensor\n\t\n    b, c, t = pos_score.size()\n    zero_pad = torch.zeros((b, c, 1), device=pos_score.device, dtype=pos_score.dtype)\n    padded = torch.cat([zero_pad, pos_score], dim=-1)\n    padded = padded.view(b, c, t + 1)\n    pos_score = padded[:, :, 1:].view_as(pos_score)\n    return pos_score\n\n", "CodeBleu_score": 30.813943991775034}]}
{"instance_id": "c046d4f00e80", "generation_result": [{"code": "def run_padded_sequence(self, context, lens)\n\t\n    batch_size = context.size(0)\n    max_len = context.size(1)\n    out = []\n    for b in range(batch_size):\n        context_b = context[b, :lens[b], :]\n        context_b = self.conv(context_b)\n        out.append(context_b)\n    out = torch.nn.utils.rnn.pad_sequence(out, batch_first=True)\n    return out\n\n", "CodeBleu_score": 28.279428764651932}, {"code": "def run_padded_sequence(self, context, lens)\n\t\n    batch_size, seq_len, _ = context.size()\n    context = context[:, :lens, :]\n    context = context.transpose(1, 2)\n    context = torch.nn.functional.pad(\n        context, (0, self.padding), \"constant\", 0\n    )\n    context = self.conv(context)\n    context = torch.nn.functional.relu(context)\n    context = context.transpose(1, 2)\n    context = context[:, :seq_len, :]\n    context = torch.nn.functional.dropout(context, p=self.dropout, training=True)\n    return context\n\n", "CodeBleu_score": 26.09339924993434}, {"code": "def run_padded_sequence(self, context, lens)\n\t\n    # Extract the context up to the specified length for each batch element\n    context = [context[i][:lens[i]] for i in range(len(context))]\n\n    # Apply the convolutional layers to the extracted context\n    for layer in self.layers:\n        context = [layer(c) for c in context]\n\n    # Pad the processed contexts to create a consistent batch size\n    max_len = max(len(c) for c in context)\n    padded_context = torch.stack([torch.nn.functional.pad(c, (0, 0, 0, max_len - len(c))) for c in context])\n\n    return padded_context\n\n", "CodeBleu_score": 22.750290478849333}]}
{"instance_id": "db2644f3c681", "generation_result": [{"code": "def run_unsorted_inputs(self, fn, context, lens)\n\t\n    lens, indices = torch.sort(lens, descending=True)\n    context = torch.index_select(context, 0, indices)\n    context = self.pack(context, lens)\n    context = fn(context)\n    context = self.unpack(context, lens)\n    context = torch.index_select(context, 0, indices)\n    return context\n\n", "CodeBleu_score": 11.452886516045618}, {"code": "def run_unsorted_inputs(self, fn, context, lens)\n\t\n    lens, indices = torch.sort(lens, descending=True)\n    context = context[indices]\n    context = torch.nn.utils.rnn.pack_padded_sequence(context, lens, batch_first=True, enforce_sorted=False)\n    context = fn(context)\n    context, _ = torch.nn.utils.rnn.pad_packed_sequence(context, batch_first=True)\n    context = context[indices.argsort()]\n    return context\n\n", "CodeBleu_score": 20.32665297966345}, {"code": "def run_unsorted_inputs(self, fn, context, lens)\n\t\n    lens, idx = torch.sort(lens, descending=True)\n    context = context[idx]\n    context = self.pack(context, lens)\n    context = fn(context)\n    context = self.unpack(context, lens)\n    context = context[torch.argsort(idx)]\n    return context\n\n", "CodeBleu_score": 12.741757846568774}]}
{"instance_id": "ced082bffc63", "generation_result": [{"code": "def remove_weight_norm(self)\n\t\n    self.kernel_predictor.remove_weight_norm()\n    for layer in self.convt_pre:\n        if \"weight_g\" in layer.named_parameters:\n            layer.remove_weight_norm()\n    for layer in self.conv_blocks:\n        if \"weight_g\" in layer.named_parameters:\n            layer.remove_weight_norm()\n\n", "CodeBleu_score": 40.87204233893763}, {"code": "def remove_weight_norm(self)\n\t\n    self.kernel_predictor.remove_weight_norm()\n    for p in self.convt_pre.parameters():\n        p.data = torch.nn.utils.parametrize.remove_weight_norm(p)\n    for block in self.conv_blocks:\n        for p in block.parameters():\n            p.data = torch.nn.utils.parametrize.remove_weight_norm(p)\n\n", "CodeBleu_score": 50.27344590880871}, {"code": "def remove_weight_norm(self)\n\t\n    self.kernel_predictor.remove_weight_norm()\n    self.convt_pre.remove_weight_norm()\n    for block in self.conv_blocks:\n        block.remove_weight_norm()\n\n", "CodeBleu_score": 33.51046275391997}]}
{"instance_id": "b5a39fd97061", "generation_result": [{"code": "def map_mx_to_torch_model(mx_name)\n\t\n    mx_name = mx_name.lower()\n    if 'se_' in mx_name:\n        mx_name = mx_name.replace('se_', 'se-')\n    if 'senet_' in mx_name:\n        mx_name = mx_name.replace('senet_', 'se-')\n    if 'inceptionv3' in mx_name:\n        mx_name = 'inceptionv3'\n    return 'gluon_' + mx_name\n\n", "CodeBleu_score": 37.47989285233571}, {"code": "def map_mx_to_torch_model(mx_name)\n\t\n    mx_name = mx_name.lower()\n    mx_name = mx_name.replace('se_', '')\n    mx_name = mx_name.replace('senet_', '')\n    mx_name = mx_name.replace('inceptionv3', 'inception_v3')\n    mx_name = 'gluon_' + mx_name\n    return mx_name\n\n", "CodeBleu_score": 34.786545241733386}, {"code": "def map_mx_to_torch_model(mx_name)\n\t\n    mx_name = mx_name.lower()\n    if 'se_' in mx_name:\n        mx_name = mx_name.replace('se_', 'se-')\n    if 'senet_' in mx_name:\n        mx_name = mx_name.replace('senet_', 'se-')\n    if 'inceptionv3' in mx_name:\n        mx_name = mx_name.replace('inceptionv3', 'inception-v3')\n    return 'gluon_' + mx_name\n\n", "CodeBleu_score": 41.578696606249075}]}
{"instance_id": "7eee5efdbf67", "generation_result": [{"code": "def _num_samples_per_worker(self)\n\t\n    if self.repeats > 0:\n        num_samples = self.num_samples * self.repeats\n    else:\n        num_samples = self.num_samples\n    if self.batch_size is not None:\n        num_samples = int(math.ceil(num_samples / float(self.batch_size))) * self.batch_size\n    if self.num_replicas > 0:\n        num_samples = int(math.ceil(num_samples / float(self.num_replicas))) * self.num_replicas\n    return num_samples\n\n", "CodeBleu_score": 40.24026027095918}, {"code": "def _num_samples_per_worker(self)\n\t\n    if self.repeats > 1:\n        num_samples = self.num_samples * self.repeats\n    else:\n        num_samples = self.num_samples\n    if self.batch_size is not None:\n        num_samples = int(math.ceil(num_samples / self.batch_size) * self.batch_size)\n    if self.num_workers > 0:\n        num_samples = int(math.ceil(num_samples / self.num_workers))\n    return num_samples\n\n", "CodeBleu_score": 39.139014747867385}, {"code": "def _num_samples_per_worker(self)\n\t\n    num_samples_per_worker = self.num_samples // self.num_replicas\n    if self.batch_size is not None:\n        num_samples_per_worker = (num_samples_per_worker + self.batch_size - 1) // self.batch_size * self.batch_size\n    if self.num_replicas > 1:\n        num_samples_per_worker = (num_samples_per_worker + self.num_replicas - 1) // self.num_replicas\n    return num_samples_per_worker\n\n", "CodeBleu_score": 32.616743281532514}]}
{"instance_id": "a698d65ae190", "generation_result": [{"code": "def find_images_and_targets(\n        folder: str,\n        types: Optional[Union[List, Tuple, Set]] = None,\n        class_to_idx: Optional[Dict] = None,\n        leaf_name_only: bool = True,\n        sort: bool = True\n)\n\t\n    if class_to_idx is None:\n        folder_to_idx = {}\n        folders = [d.name for d in os.scandir(folder) if d.is_dir()]\n        folders.sort(key=natural_key)\n        for i, class_name in enumerate(folders):\n            folder_to_idx[class_name] = i\n        class_to_idx = folder_to_idx\n\n    image_paths: List[str] = []\n    target_values: List[int] = []\n    for target_idx, (target_name, target_value) in enumerate(class_to_idx.items()):\n        target_dir = os.path.join(folder, target_name)\n        if not os.path.isdir(target_dir):\n            continue\n        for root, _, fnames in sorted(os.walk(target_dir)):\n            for fname in sorted(fnames):\n                if types is not None:\n                    if not fname.lower().endswith(types):\n                        continue\n                if leaf_name_only:\n                    path = fname\n                else:\n                    path = os.path.relpath(os.path.join(root, fname), folder)\n                image_paths.append(path)\n                target_values.append(target_value)\n\n    if sort:\n        image_paths, target_values = zip(*sorted(\n            zip(image_paths, target_values), key=lambda t: natural_key(t[0])))\n        image_paths = list(image_paths)\n        target_values = list(target_values)\n\n    return image_paths, target_values, class_to_idx", "CodeBleu_score": 38.00104032756213}, {"code": "def find_images_and_targets(\n        folder: str,\n        types: Optional[Union[List, Tuple, Set]] = None,\n        class_to_idx: Optional[Dict] = None,\n        leaf_name_only: bool = True,\n        sort: bool = True\n)\n\t\n    if class_to_idx is None:\n        folder_to_idx: Dict = {}\n        folders: List = [d.name for d in os.scandir(folder) if d.is_dir()]\n        folders.sort(key=natural_key)\n        for i, f in enumerate(folders):\n            folder_to_idx[f] = i\n        class_to_idx = folder_to_idx\n    elif class_to_idx is not None:\n        if not isinstance(class_to_idx, dict):\n            raise RuntimeError(\n                \"class_to_idx must be a dictionary mapping class names to indexes\")\n        folders = [None] * len(class_to_idx)\n        for key, value in class_to_idx.items():\n            folders[value] = key\n\n    instances: List = []\n    available_classes = set()\n    for target_class in sorted(class_to_idx.keys()):\n        class_index = class_to_idx[target_class]\n        target_dir = os.path.join(folder, target_class)\n        if not os.path.isdir(target_dir):\n            continue\n        for root, _, fnames in sorted(os.walk(target_dir, followlinks=True)):\n            for fname in sorted(fnames):\n                if leaf_name_only:\n                    path = fname\n                else:\n                    path = os.path.relpath(os.path.join(root, fname), folder)\n                item = (path, class_index)\n                instances.append(item)\n                available_classes.add(class_index)\n\n    if len(available_classes) == 0:\n        raise RuntimeError(\n            f\"Found 0 folders in subfolders of: {folder} which matches the provided classes: {', '.join(class_to_idx.keys())}\")\n    elif len(available_classes) != len(class_to_idx):\n        raise RuntimeError(\n            f\"Found {len(available_classes)} folders in subfolders of: {folder} which matches the provided classes: {', '.join(class_to_idx.keys())}.", "CodeBleu_score": 32.318235860008436}, {"code": "def find_images_and_targets(\n        folder: str,\n        types: Optional[Union[List, Tuple, Set]] = None,\n        class_to_idx: Optional[Dict] = None,\n        leaf_name_only: bool = True,\n        sort: bool = True\n)\n\t\n\n    if class_to_idx is None:\n        folder_to_idx = {}\n        folders = [d.name for d in os.scandir(folder) if d.is_dir()]\n        folders.sort(key=natural_key)\n        for i, f in enumerate(folders):\n            folder_to_idx[f] = i\n        class_to_idx = folder_to_idx\n\n    if types is not None:\n        def has_file_allowed_extension(filename, extensions):\n            filename_lower = filename.lower()\n            return any(filename_lower.endswith(ext) for ext in extensions)\n\n        extensions = types if isinstance(types, (tuple, list, set)) else [types]\n    else:\n        has_file_allowed_extension = lambda x, y: True\n\n    instances = []\n    for target_class in sorted(class_to_idx.keys()):\n        class_index = class_to_idx[target_class]\n        target_dir = os.path.join(folder, target_class)\n        if not os.path.isdir(target_dir):\n            continue\n        for root, _, fnames in sorted(os.walk(target_dir, followlinks=True)):\n            for fname in sorted(fnames):\n                path = os.path.join(root, fname)\n                if has_file_allowed_extension(path, extensions):\n                    item = path, class_index\n                    instances.append(item)\n\n    if sort:\n        instances.sort(key=lambda x: x[0])\n\n    return instances, class_to_idx\n\n", "CodeBleu_score": 37.883643441008715}]}
{"instance_id": "e7fe43bdecdc", "generation_result": [{"code": "def pytorch_worker_seed()\n\t\n    worker_info = get_worker_info()\n    if worker_info is not None:\n        seed = worker_info.seed % 2**32\n    else:\n        seed = os.getpid() % 2**32\n    return seed\n\n", "CodeBleu_score": 53.14133051592948}, {"code": "def pytorch_worker_seed()\n\t\n    worker_info = get_worker_info()\n    if worker_info is not None:\n        seed = worker_info.seed\n    else:\n        seed = os.getenv(\"WDS_RANK\", 0)\n    return seed\n\n", "CodeBleu_score": 50.78823659687767}, {"code": "def pytorch_worker_seed()\n\t\n    worker_info = get_worker_info()\n    if worker_info is not None:\n        seed = worker_info.seed\n    else:\n        seed = int(os.environ.get(\"WDS_RANK\", 0))\n    return seed\n\n", "CodeBleu_score": 51.37420146344507}]}
{"instance_id": "0ffc7e7ded25", "generation_result": [{"code": "def solarize_add(img, add, thresh=128, **__)\n\t\n    if img.mode == \"L\":\n        lut = [i + add if i < thresh else i for i in range(256)]\n        return img.point(lut)\n    elif img.mode == \"RGB\":\n        lut = [(i[0] + add, i[1] + add, i[2] + add) if i[0] < thresh else i for i in range(256)]\n        return img.point(lut)\n    else:\n        return img\n\n", "CodeBleu_score": 40.28388477203087}, {"code": "def solarize_add(img, add, thresh=128, **__)\n\t\n    if img.mode == \"L\":\n        lut = [v + add if v < thresh else v for v in range(256)]\n        return img.point(lut)\n    elif img.mode == \"RGB\":\n        lut = [(v[0] + add if v[0] < thresh else v[0],\n                v[1] + add if v[1] < thresh else v[1],\n                v[2] + add if v[2] < thresh else v[2]) for v in range(256)]\n        return img.point(lut)\n    else:\n        return img\n\n", "CodeBleu_score": 38.26273762438274}, {"code": "def solarize_add(img, add, thresh=128, **__)\n\t\n    if img.mode == \"L\":\n        lut = [min(255, i + add) for i in range(256)]\n        return img.point(lut)\n    elif img.mode == \"RGB\":\n        lut = [(min(255, r + add), min(255, g + add), min(255, b + add)) for r, g, b in img.getcolors(256)]\n        return img.point(lut)\n    else:\n        return img\n\n", "CodeBleu_score": 40.14842746658614}]}
{"instance_id": "5a0d29577a21", "generation_result": [{"code": "def _get_weighted_transforms(transforms: Dict)\n\t\n    transform_names, transform_probs = zip(*transforms.items())\n    transform_probs = [p / sum(transform_probs) for p in transform_probs]\n    return transform_names, transform_probs\n\n", "CodeBleu_score": 25.77699289615994}, {"code": "def _get_weighted_transforms(transforms: Dict)\n\t\n    transform_names = list(transforms.keys())\n    transform_weights = list(transforms.values())\n    total_weight = sum(transform_weights)\n    transform_weights = [weight / total_weight for weight in transform_weights]\n    return transform_names, transform_weights\n\n", "CodeBleu_score": 16.061346314722957}, {"code": "def _get_weighted_transforms(transforms: Dict)\n\t\n    transform_names = []\n    transform_probs = []\n    for transform_name, transform_prob in transforms.items():\n        transform_names.append(transform_name)\n        transform_probs.append(transform_prob)\n\n    transform_names = tuple(transform_names)\n    transform_probs = tuple(\n        transform_prob / sum(transform_probs) for transform_prob in transform_probs\n    )\n    return transform_names, transform_probs\n\n", "CodeBleu_score": 30.91639243822504}]}
{"instance_id": "84bc5658c135", "generation_result": [{"code": "def _apply_basic(self, img, mixing_weights, m)\n\t\n    mixed = np.zeros_like(img)\n    for w in mixing_weights:\n        mixed += self._apply_random_sequence(img, w, m)\n    mixed = np.clip(mixed, 0, 255).astype(np.uint8)\n    mixed = Image.fromarray(mixed)\n    mixed = ImageChops.blend(img, mixed, m)\n    return mixed\n\n", "CodeBleu_score": 21.209550175104834}, {"code": "def _apply_basic(self, img, mixing_weights, m)\n\t\n    mixed_img = np.zeros(img.shape, dtype=np.float32)\n    for w in mixing_weights:\n        mixed_img += self._apply_operations(img, w)\n    mixed_img = np.clip(mixed_img, 0, 255)\n    mixed_img = np.round(mixed_img).astype(np.uint8)\n    mixed_img = Image.fromarray(mixed_img)\n    return Image.blend(img, mixed_img, m)\n\n", "CodeBleu_score": 24.494852575304527}, {"code": "def _apply_basic(self, img, mixing_weights, m)\n\t\n    mixed_img = np.zeros_like(img)\n    for i, weight in enumerate(mixing_weights):\n        mixed_img += weight * self._apply_random_sequence(img, m)\n    mixed_img = np.clip(mixed_img, 0, 255)\n    mixed_img = Image.fromarray(mixed_img.astype(np.uint8))\n    mixed_img = ImageChops.blend(img, mixed_img, m)\n    return mixed_img\n\n", "CodeBleu_score": 23.868823629029094}]}
{"instance_id": "84b5b752ff04", "generation_result": [{"code": "def adapt_to_chs(x, n)\n\t\n    if isinstance(x, (tuple, list)):\n        if len(x) == n:\n            return x\n        elif len(x) == 1:\n            return tuple(x * n)\n        else:\n            logging.warning(f\"The input has {len(x)} channels, but {n} were requested. The mean of the input will be used.\")\n            return tuple(repeat(sum(x) / len(x), n))\n    else:\n        return tuple(repeat(x, n))\n\n", "CodeBleu_score": 26.926754019875382}, {"code": "def adapt_to_chs(x, n)\n\t\n    if not isinstance(x, (tuple, list)):\n        x = (x,) * n\n    elif len(x) != n:\n        x = tuple(torch.mean(torch.stack(x, dim=0), dim=0))\n        logging.warning(f\"x is not a tuple of length {n}, so it was adapted to match the required number of channels. The mean of x was calculated.\")\n    assert len(x) == n\n    return x\n\n", "CodeBleu_score": 51.3907019739049}, {"code": "def adapt_to_chs(x, n)\n\t\n    if not isinstance(x, (tuple, list)):\n        x = (x,) * n\n    elif len(x) != n:\n        logging.warning(f\"Adapting {x} to {n} channels.\")\n        x = tuple(torch.mean(torch.stack(x, dim=0), dim=0))\n    assert len(x) == n\n    return x\n\n", "CodeBleu_score": 46.392977731155526}]}
{"instance_id": "54754b6ba394", "generation_result": [{"code": "def _worker_init(worker_id, worker_seeding='all')\n\t\n    worker_info = torch.utils.data.get_worker_info()\n    if worker_info is None:\n        return\n    if worker_info.id != worker_id:\n        return\n    if isinstance(worker_seeding, str):\n        if worker_seeding == 'all':\n            numpy_seed = worker_info.seed % 2**32\n            random.seed(numpy_seed)\n            torch.manual_seed(numpy_seed)\n        else:\n            raise ValueError(f\"worker_seeding must be 'all', not {worker_seeding}\")\n    elif callable(worker_seeding):\n        worker_seeding(worker_info.seed)\n    else:\n        raise TypeError(f\"worker_seeding must be a string or callable, not {type(worker_seeding)}\")\n\n", "CodeBleu_score": 38.7521439611176}, {"code": "def _worker_init(worker_id, worker_seeding='all')\n\t\n    worker_info = torch.utils.data.get_worker_info()\n    if worker_info is None:\n        return\n    if worker_info.id != worker_id:\n        return\n    if callable(worker_seeding):\n        seed = worker_seeding(worker_info.seed)\n    elif worker_seeding == 'all':\n        seed = worker_info.seed\n    else:\n        raise ValueError(f'Invalid worker_seeding: {worker_seeding}')\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    torch.backends.cudnn.enabled = False\n    torch.use_deterministic_algorithms(True)\n    if seed is not None:\n        np.random.seed(seed)\n\n", "CodeBleu_score": 45.0663787698489}, {"code": "def _worker_init(worker_id, worker_seeding='all')\n\t\n    worker_info = torch.utils.data.get_worker_info()\n    if worker_info is None:\n        return\n    if worker_id != worker_info.id:\n        raise RuntimeError(\n            f'Worker {worker_id} is not the same as worker {worker_info.id}')\n    if worker_seeding == 'all':\n        seed = worker_info.seed\n        random.seed(seed)\n        torch.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        torch.backends.cudnn.enabled = False\n        np.random.seed(seed)\n    elif callable(worker_seeding):\n        worker_seeding(worker_id)\n    else:\n        raise ValueError(\n            f'worker_seeding should be a callable or a string, but got {worker_seeding}')\n\n", "CodeBleu_score": 36.9715665356211}]}
{"instance_id": "81dfdce940db", "generation_result": [{"code": "def _decode_and_center_crop(image_bytes, image_size, resize_method)\n\t\n    image = tf.image.decode_jpeg(image_bytes, channels=3)\n    padded_center_crop_size = tf.cast(\n        ((image_size / (image_size + PADDING)) *\n         tf.cast(tf.shape(image)[:2], tf.float32)), tf.int32)\n    offset_height = ((tf.shape(image)[0] - padded_center_crop_size) + 1) // 2\n    offset_width = ((tf.shape(image)[1] - padded_center_crop_size) + 1) // 2\n    crop_window = tf.stack([offset_height, offset_width,\n                            padded_center_crop_size, padded_center_crop_size])\n    image = tf.image.decode_and_crop_jpeg(image_bytes, crop_window, channels=3)\n    image = tf.image.resize(image, [image_size, image_size],\n                            method=resize_method)\n    return image\n\n", "CodeBleu_score": 48.459306294871105}, {"code": "def _decode_and_center_crop(image_bytes, image_size, resize_method)\n\t\n    image_shape = tf.image.extract_jpeg_shape(image_bytes)\n    padded_center_crop_size = tf.cast(\n        ((image_size / (image_size + CROP_PADDING)) *\n         tf.cast(tf.minimum(image_shape[0], image_shape[1]), tf.float32)),\n        tf.int32)\n    offset_height = ((image_shape[0] - padded_center_crop_size) + 1) // 2\n    offset_width = ((image_shape[1] - padded_center_crop_size) + 1) // 2\n    crop_window = tf.stack([offset_height, offset_width,\n                            padded_center_crop_size, padded_center_crop_size])\n    image = tf.image.decode_and_crop_jpeg(image_bytes, crop_window, channels=3)\n    image = tf.image.resize(image, [image_size, image_size],\n                            method=resize_method)\n    return image\n\n\n\n\n", "CodeBleu_score": 51.564601660882694}, {"code": "def _decode_and_center_crop(image_bytes, image_size, resize_method)\n\t\n    image = tf.image.decode_jpeg(image_bytes, channels=3)\n    shape = tf.shape(image)\n    image_height = shape[0]\n    image_width = shape[1]\n    padded_center_crop_size = tf.cast(\n        ((image_height / (image_width / image_size + PADDING)) +\n         (image_width / (image_height / image_size + PADDING))) / 2, tf.int32)\n    offset_height = ((image_height - padded_center_crop_size) + 1) // 2\n    offset_width = ((image_width - padded_center_crop_size) + 1) // 2\n    image = tf.image.crop_to_bounding_box(image, offset_height, offset_width,\n                                          padded_center_crop_size,\n                                          padded_center_crop_size)\n    image = tf.image.resize([image], [image_size, image_size],\n                            method=resize_method)[0]\n    return image\n\n\n\n\n", "CodeBleu_score": 47.02444669074556}]}
{"instance_id": "cf16a6d37111", "generation_result": [{"code": "def preprocess_for_train(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    image = tf.image.decode_image(image_bytes, channels=3)\n    image = tf.image.resize(image, image_size, method=interpolation)\n    image = tf.image.random_flip_left_right(image)\n    image = tf.cast(image, dtype=tf.bfloat16 if use_bfloat16 else tf.float32)\n    image = tf.reshape(image, (1, *image_size, 3))\n    return image\n\n\n", "CodeBleu_score": 35.73498647039729}, {"code": "def preprocess_for_train(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    image = tf.io.decode_image(image_bytes, channels=3, expand_animations=False)\n    image = tf.image.resize(image, image_size, method=interpolation)\n    image = tf.image.random_flip_left_right(image)\n    image = tf.cast(image, dtype=tf.bfloat16 if use_bfloat16 else tf.float32)\n    return image\n\n\n\n", "CodeBleu_score": 34.44398559836531}, {"code": "def preprocess_for_train(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    image = tf.image.decode_jpeg(image_bytes, channels=3)\n    image = tf.image.convert_image_dtype(image, tf.float32)\n    image = tf.image.resize(image, image_size, method=interpolation)\n    image = tf.image.random_flip_left_right(image)\n    image = tf.reshape(image, [image_size[0], image_size[1], 3])\n    if use_bfloat16:\n        image = tf.cast(image, tf.bfloat16)\n    return image\n\n\n", "CodeBleu_score": 34.84291508239926}]}
{"instance_id": "78b2a3bd2142", "generation_result": [{"code": "def preprocess_for_eval(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    image = tf.image.decode_jpeg(image_bytes, channels=3)\n    image = tf.image.convert_image_dtype(image, tf.float32)\n    image = tf.image.resize(image, image_size, method=interpolation)\n    image = tf.cast(image, tf.bfloat16 if use_bfloat16 else tf.float32)\n    return image\n\n\n", "CodeBleu_score": 30.603590875648845}, {"code": "def preprocess_for_eval(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    image = tf.image.decode_image(image_bytes, channels=3)\n    image = tf.image.convert_image_dtype(image, tf.float32)\n    image = tf.image.central_crop(image, 0.875)\n    if interpolation == 'bilinear':\n        image = tf.image.resize(image, image_size, method=tf.image.ResizeMethod.BILINEAR)\n    elif interpolation == 'bicubic':\n        image = tf.image.resize(image, image_size, method=tf.image.ResizeMethod.BICUBIC)\n    elif interpolation == 'area':\n        image = tf.image.resize(image, image_size, method=tf.image.ResizeMethod.AREA)\n    else:\n        raise ValueError('Unknown interpolation method: {}'.format(interpolation))\n    if use_bfloat16:\n        image = tf.cast(image, dtype=tf.bfloat16)\n    return image\n\n\n\n\n", "CodeBleu_score": 40.04900130154389}, {"code": "def preprocess_for_eval(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    image = tf.image.decode_jpeg(image_bytes, channels=3)\n    image = tf.cast(image, tf.float32)\n    image = tf.image.resize(image, image_size, method=interpolation)\n    image = tf.clip_by_value(image, 0.0, 255.0)\n    if use_bfloat16:\n        image = tf.cast(image, tf.bfloat16)\n    return image\n\n\n", "CodeBleu_score": 31.644269309152563}]}
{"instance_id": "8bd4364a9a0e", "generation_result": [{"code": "def evaluate(self, model, eval_dataloader=None, output_file=\"merged\")\n\t\n    if eval_dataloader is None:\n        eval_dataloader = self.eval_dataloader\n\n    # Run prediction loop\n    model.eval()\n    outputs = []\n    for batch in eval_dataloader:\n        with torch.no_grad():\n            batch = {k: v.to(self.device) for k, v in batch.items()}\n            output = model(**batch)\n        outputs.append(output)\n\n    # Save outputs to file\n    output_dir = os.path.join(self.output_dir, output_file)\n    os.makedirs(output_dir, exist_ok=True)\n    for i, output in enumerate(outputs):\n        output_path = os.path.join(output_dir, f\"output_{i}.pt\")\n        torch.save(output, output_path)\n\n    # Compute evaluation metrics\n    output_files = glob.glob(os.path.join(output_dir, \"*.pt\"))\n    metrics = self.compute_metrics(output_files)\n\n    return metrics", "CodeBleu_score": 44.79645269215534}, {"code": "def evaluate(self, model, eval_dataloader=None, output_file=\"merged\")\n\t\n\n    # Set the model to evaluation mode\n    model.eval()\n\n    # Initialize an empty list to store the predictions\n    predictions = []\n\n    # Loop over the dataloader\n    for batch in eval_dataloader:\n        # Move the batch to the appropriate device\n        batch = {k: v.to(self.device) for k, v in batch.items()}\n\n        # Run the model on the batch and get the predictions\n        with torch.no_grad():\n            output = model(**batch)\n        preds = torch.argmax(output.logits, dim=1)\n\n        # Add the predictions to the list\n        predictions.extend(preds.cpu().numpy())\n\n    # Save the predictions to a file\n    with open(f\"{output_file}.txt\", \"w\") as f:\n        for pred in predictions:\n            f.write(f\"{pred}", "CodeBleu_score": 29.197078735345137}, {"code": "def evaluate(self, model, eval_dataloader=None, output_file=\"merged\")\n\t\n    if eval_dataloader is None:\n        eval_dataloader = self.eval_dataloader\n\n    output_file = os.path.join(self.output_dir, f\"{output_file}.jsonl\")\n\n    model.eval()\n    with open(output_file, \"w\") as f:\n        for batch in eval_dataloader:\n            batch = {k: v.to(self.device) for k, v in batch.items()}\n            with torch.no_grad():\n                outputs = model(**batch)\n            for i, prediction in enumerate(outputs.predictions):\n                prediction = prediction.detach().cpu().numpy().tolist()\n                batch[\"predictions\"][i] = prediction\n            for key in batch:\n                if torch.is_tensor(batch[key]):\n                    batch[key] = batch[key].detach().cpu().numpy().tolist()\n            for example in batch[\"predictions\"]:\n                f.write(json.dumps(example) + \"", "CodeBleu_score": 45.181158797195664}]}
{"instance_id": "22d682c958c0", "generation_result": [{"code": " def print_computed_metrics(self, metrics)\n\t\n    rank1 = metrics['rank1']\n    rank5 = metrics['rank5']\n    miou = metrics['miou']\n    print('rank1: {:.4f} rank5: {:.4f} miou: {:.4f}'.format(rank1, rank5, miou))\n\n\n def print_computed_metrics(self, metrics):\n    \"\"\"The function prints the computed metrics for rank1, rank5, and miou.", "CodeBleu_score": 29.375509214467076}, {"code": " def print_computed_metrics(self, metrics)\n\t\n    rank1 = metrics['rank1']\n    rank5 = metrics['rank5']\n    miou = metrics['miou']\n    print('Rank1: {:.4f}'.format(rank1))\n    print('Rank5: {:.4f}'.format(rank5))\n    print('mIoU: {:.4f}'.format(miou))\n\n\n def print_computed_metrics(self, metrics):\n    \"\"\"The function prints the computed metrics for rank1, rank5, and miou.", "CodeBleu_score": 26.69933145881623}, {"code": " def print_computed_metrics(self, metrics)\n\t\n    print('Computed metrics:')\n    print('rank1: {:.4f}'.format(metrics['rank1']))\n    print('rank5: {:.4f}'.format(metrics['rank5']))\n    print('miou: {:.4f}'.format(metrics['miou']))\n\n\n def print_computed_metrics(self, metrics):\n    \"\"\"The function prints the computed metrics for rank1, rank5, and miou.", "CodeBleu_score": 27.92227208375897}]}
{"instance_id": "39c1a50b7e7d", "generation_result": [{"code": "def _eval_predictions(self, segments, data)\n\t\n    rank1 = 0\n    rank5 = 0\n    mean_iou = 0\n    for segment in segments:\n        iou_list = []\n        for data_segment in data:\n            iou = self._iou(segment, data_segment)\n            iou_list.append(iou)\n        iou_list.sort(reverse=True)\n        mean_iou += iou_list[0]\n        if iou_list[0] >= 0.5:\n            rank1 += 1\n        if iou_list[0] >= 0.1:\n            rank5 += 1\n    rank1 /= len(segments)\n    rank5 /= len(segments)\n    mean_iou /= len(segments)\n    return rank1, rank5, mean_iou\n\n", "CodeBleu_score": 27.901537469251146}, {"code": "def _eval_predictions(self, segments, data)\n\t\n    rank1 = 0\n    rank5 = 0\n    ious = []\n    for segment in segments:\n        segment_ious = []\n        for data_segment in data:\n            iou = self._iou(segment, data_segment)\n            segment_ious.append(iou)\n        ious.append(segment_ious)\n    for segment_ious in ious:\n        segment_ious.sort(reverse=True)\n        if segment_ious[0] >= 0.5:\n            rank1 += 1\n        if segment_ious[4] >= 0.5:\n            rank5 += 1\n    mean_iou = sum([sum(segment_ious) / len(segment_ious) for segment_ious in ious]) / len(ious)\n    return rank1, rank5, mean_iou\n\n", "CodeBleu_score": 29.003272098531703}, {"code": "def _eval_predictions(self, segments, data)\n\t\n    rank1 = 0\n    rank5 = 0\n    ious = []\n    for segment in segments:\n        iou, rank = self._eval_segment(segment, data)\n        rank1 += rank == 1\n        rank5 += rank <= 5\n        ious.append(iou)\n    rank1 = rank1 / len(segments)\n    rank5 = rank5 / len(segments)\n    iou = sum(ious) / len(segments)\n    return rank1, rank5, iou\n\n", "CodeBleu_score": 23.038439880854956}]}
{"instance_id": "8ccf84f8d034", "generation_result": [{"code": "def predict_loop(self, model, eval_dataloader, output_file=None)\n\t\n    scores = []\n    model.eval()\n    model.to(self.device)\n    for batch in tqdm(eval_dataloader):\n        batch = {k: v.to(self.device) for k, v in batch.items()}\n        with torch.no_grad():\n            output = model(**batch)\n        scores.extend(output.logits.detach().cpu().numpy())\n    if output_file is not None:\n        with open(output_file, \"wb\") as f:\n            pickle.dump(scores, f)\n    return scores\n\n", "CodeBleu_score": 36.01089974295266}, {"code": "def predict_loop(self, model, eval_dataloader, output_file=None)\n\t\n    scores = []\n    model.eval()\n    model = model.to(self.device)\n    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n        batch = tuple(t.to(self.device) for t in batch)\n        input_ids, input_mask, segment_ids, label_ids = batch\n        with torch.no_grad():\n            logits = model(input_ids, segment_ids, input_mask)\n        scores.extend(logits.detach().cpu().numpy().tolist())\n    if output_file:\n        with open(output_file, \"w\") as f:\n            json.dump(scores, f, indent=4)\n    return scores\n\n", "CodeBleu_score": 35.899691648880236}, {"code": "def predict_loop(self, model, eval_dataloader, output_file=None)\n\t\n    scores = []\n    model.eval()\n    model.to(self.device)\n    with torch.no_grad():\n        for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n            batch = tuple(t.to(self.device) for t in batch)\n            input_ids, input_mask, segment_ids, label_ids = batch\n            output = model(input_ids, token_type_ids=segment_ids, attention_mask=input_mask)\n            scores.extend(output[0].cpu().numpy())\n    if output_file:\n        with open(output_file, \"wb\") as f:\n            pickle.dump(scores, f)\n    return scores\n\n", "CodeBleu_score": 31.021062898467218}]}
{"instance_id": "e68529ad727d", "generation_result": [{"code": "def _aggregate_scores(self, scores)\n\t\n    assert len(scores) == 2\n    full_scores = torch.cat([scores[0], scores[1]], dim=0)\n    self.full_scores = None\n    return torch.matmul(full_scores[0], full_scores[1].T)\n\n", "CodeBleu_score": 38.54946767470137}, {"code": "def _aggregate_scores(self, scores)\n\t\n    assert len(scores) == 2\n    text_scores = torch.cat(scores[0], dim=0)\n    video_scores = torch.cat(scores[1], dim=0)\n    self.full_scores = None\n    return torch.matmul(text_scores, video_scores.T)\n\n", "CodeBleu_score": 60.00837001042325}, {"code": "def _aggregate_scores(self, scores)\n\t\n    assert len(scores) == 2\n    for score in scores:\n        assert len(score.shape) == 3\n    text_scores, video_scores = scores\n    full_scores = torch.cat((text_scores, video_scores), dim=1)\n    del self.full_scores\n    return torch.bmm(text_scores, video_scores.transpose(1, 2))\n\n", "CodeBleu_score": 31.422826892786215}]}
{"instance_id": "e4e1861fc357", "generation_result": [{"code": "def finalize(self, Y_pred, Y_true, output_file=None)\n\t\n    # Concatenate and convert to numpy arrays\n    Y_pred = torch.cat(Y_pred, dim=0).cpu().numpy()\n    Y_true = torch.cat(Y_true, dim=0).cpu().numpy()\n\n    # Check for prediction errors\n    errors = Y_pred != Y_true\n    if errors.any():\n        print(f\"Found {errors.sum()} errors:\")\n        for i in range(len(Y_true)):\n            if errors[i]:\n                print(f\"\\tSample {i}: Predicted: {Y_pred[i]}, True: {Y_true[i]}\")\n\n    # Save predictions and true labels to pickle file\n    if output_file is not None:\n        with open(output_file, \"wb\") as f:\n            pickle.dump({\"Y_pred\": Y_pred, \"Y_true\": Y_true}, f)\n\n    # Return dictionary of predictions and true labels\n    return {\"Y_pred\": Y_pred, \"Y_true\": Y_true}\n\n", "CodeBleu_score": 43.489748428581585}, {"code": "def finalize(self, Y_pred, Y_true, output_file=None)\n\t\n    Y_pred = torch.cat(Y_pred)\n    Y_true = torch.cat(Y_true)\n    Y_pred = Y_pred.cpu().detach().numpy()\n    Y_true = Y_true.cpu().detach().numpy()\n\n    if output_file is not None:\n        pickle.dump([Y_pred, Y_true], open(output_file, 'wb'))\n\n    return {'Y_pred': Y_pred, 'Y_true': Y_true}\n\n", "CodeBleu_score": 22.623860641208747}, {"code": "def finalize(self, Y_pred, Y_true, output_file=None)\n\t\n    Y_pred = torch.cat(Y_pred, dim=0).cpu().numpy()\n    Y_true = torch.cat(Y_true, dim=0).cpu().numpy()\n    # check for prediction errors\n    errors = []\n    for i in range(Y_pred.shape[0]):\n        if Y_pred[i] != Y_true[i]:\n            errors.append(i)\n    if len(errors) > 0:\n        print(\"Prediction errors:\")\n        for i in range(len(errors)):\n            print(errors[i], Y_pred[errors[i]], Y_true[errors[i]])\n    # save predictions and true labels\n    if output_file is not None:\n        with open(output_file, \"wb\") as f:\n            pickle.dump({\"Y_pred\": Y_pred, \"Y_true\": Y_true}, f)\n    return {\"Y_pred\": Y_pred, \"Y_true\": Y_true}\n\n", "CodeBleu_score": 42.31614836141385}]}
{"instance_id": "7df9ab9a4920", "generation_result": [{"code": "def reduce_metrics(logging_outputs) -> None\n\t\n    # Reduce logging outputs from all workers using average\n    loss_sum = sum(log.get('loss', 0) for log in logging_outputs)\n    sample_size = sum(log.get('sample_size', 0) for log in logging_outputs)\n    metrics.log_scalar('loss', loss_sum / sample_size / 1000, sample_size, round=3)\n\n", "CodeBleu_score": 52.313986771499586}, {"code": "def reduce_metrics(logging_outputs) -> None\n\t\n    loss_sum = sum(log.get(\"loss\", 0) for log in logging_outputs)\n    sample_size = sum(log.get(\"sample_size\", 0) for log in logging_outputs)\n    metrics.log_scalar(\"loss\", loss_sum / sample_size / 1000, sample_size, round=3)\n\n", "CodeBleu_score": 61.524182295454054}, {"code": "def reduce_metrics(logging_outputs) -> None\n\t\n    loss_sum = sum(log.get('loss', 0) for log in logging_outputs)\n    sample_size = sum(log.get('sample_size', 0) for log in logging_outputs)\n\n    metrics.log_scalar('loss', loss_sum / sample_size / len(logging_outputs), sample_size, round=3)\n\n", "CodeBleu_score": 51.69730414790319}]}
{"instance_id": "7f9c326a96f7", "generation_result": [{"code": "def _mm_on_the_fly(\n        self,\n        cmasks,\n        vmasks,\n        attention_mask\n    )\n\t\n\n    if attention_mask is None:\n        attention_mask = torch.cat([cmasks, vmasks], dim=-1)\n    else:\n        attention_mask = torch.cat([attention_mask, vmasks], dim=-1)\n\n    token_type_ids = torch.cat([torch.zeros_like(cmasks), torch.ones_like(vmasks)], dim=-1)\n\n    return attention_mask, token_type_ids\n\n", "CodeBleu_score": 29.447727810041595}, {"code": "def _mm_on_the_fly(\n        self,\n        cmasks,\n        vmasks,\n        attention_mask\n    )\n\t\n    if attention_mask is None:\n        attention_mask = torch.cat((cmasks, vmasks), dim=-1)\n    token_type_ids = torch.cat((torch.zeros_like(vmasks), torch.ones_like(cmasks)), dim=-1)\n    return attention_mask, token_type_ids\n\n", "CodeBleu_score": 25.043153368928046}, {"code": "def _mm_on_the_fly(\n        self,\n        cmasks,\n        vmasks,\n        attention_mask\n    )\n\t\n    # attention mask\n    if attention_mask is None:\n        attention_mask = torch.matmul(cmasks, vmasks.transpose(1, 2))\n        attention_mask = attention_mask.type(torch.float32)\n    else:\n        attention_mask = attention_mask.type(torch.float32)\n\n    # token type ids\n    token_type_ids = torch.zeros_like(attention_mask)\n    token_type_ids = token_type_ids.masked_fill(attention_mask == 1, 1)\n    token_type_ids = token_type_ids.type(torch.long)\n\n    return attention_mask, token_type_ids\n\n", "CodeBleu_score": 33.64625957920395}]}
{"instance_id": "fa99d2a148a5", "generation_result": [{"code": "def prepare_inputs_for_generation(\n        self,\n        input_ids,\n        input_video_embeds,\n        attention_mask=None,\n        token_type_ids=None,\n        **model_kwargs\n    )\n\t\n    if attention_mask is None:\n        attention_mask = torch.ones_like(input_ids)\n    if token_type_ids is None:\n        token_type_ids = torch.zeros_like(input_ids)\n    input_ids = input_ids.view(-1, input_ids.size(-1))\n    input_video_embeds = input_video_embeds.view(-1, input_video_embeds.size(-1))\n    attention_mask = attention_mask.view(-1, attention_mask.size(-1))\n    token_type_ids = token_type_ids.view(-1, token_type_ids.size(-1))\n    combined_length = input_ids.size(1) + input_video_embeds.size(1)\n    input_ids = input_ids.view(1, combined_length)\n    input_video_embeds = input_video_embeds.view(1, combined_length)\n    attention_mask = attention_mask.view(1, combined_length)\n    token_type_ids = token_type_ids.view(1, combined_length)\n    return {\n        \"input_ids\": input_ids,\n        \"input_video_embeds\": input_video_embeds,\n        \"attention_mask\": attention_mask,\n        \"token_type_ids\": token_type_ids,\n    }\n\n", "CodeBleu_score": 45.01995528088321}, {"code": "def prepare_inputs_for_generation(\n        self,\n        input_ids,\n        input_video_embeds,\n        attention_mask=None,\n        token_type_ids=None,\n        **model_kwargs\n    )\n\t\n\n    if input_ids.shape[1] != input_video_embeds.shape[1]:\n        raise ValueError(\"The sequence length of input_ids and input_video_embeds must be the same.\")\n\n    if attention_mask is not None:\n        if attention_mask.shape[1] != input_ids.shape[1]:\n            raise ValueError(\"The sequence length of attention_mask must be the same as the sequence length of input_ids.\")\n\n    if token_type_ids is not None:\n        if token_type_ids.shape[1] != input_ids.shape[1]:\n            raise ValueError(\"The sequence length of token_type_ids must be the same as the sequence length of input_ids.\")\n\n    # Adjust the sequence length to match the combined length of input_ids and input_video_embeds\n    combined_length = input_ids.shape[1] + input_video_embeds.shape[1]\n    input_ids = input_ids[:, -combined_length:]\n    input_video_embeds = input_video_embeds[:, -combined_length:]\n\n    # Modify the attention_mask and token_type_ids to fit the new sequence length\n    if attention_mask is not None:\n        attention_mask = attention_mask[:, -combined_length:]\n\n    if token_type_ids is not None:\n        token_type_ids = token_type_ids[:, -combined_length:]\n\n    return {\n        \"input_ids\": input_ids,\n        \"input_video_embeds\": input_video_embeds,\n        \"attention_mask\": attention_mask,\n        \"token_type_ids\": token_type_ids,\n        **model_kwargs,\n    }\n\n", "CodeBleu_score": 45.02519560142693}, {"code": "def prepare_inputs_for_generation(\n        self,\n        input_ids,\n        input_video_embeds,\n        attention_mask=None,\n        token_type_ids=None,\n        **model_kwargs\n    )\n\t\n    if attention_mask is None:\n        attention_mask = torch.ones_like(input_ids)\n    if token_type_ids is None:\n        token_type_ids = torch.zeros_like(input_ids)\n    if len(input_ids.shape) == 1:\n        input_ids = input_ids.unsqueeze(0)\n    if len(input_video_embeds.shape) == 1:\n        input_video_embeds = input_video_embeds.unsqueeze(0)\n\n    input_ids = input_ids.to(self.device)\n    input_video_embeds = input_video_embeds.to(self.device)\n    attention_mask = attention_mask.to(self.device)\n    token_type_ids = token_type_ids.to(self.device)\n\n    # adjust the sequence length to match the combined length of input_ids and input_video_embeds\n    combined_length = input_ids.shape[1] + input_video_embeds.shape[1]\n    if attention_mask.shape[1] != combined_length:\n        attention_mask = F.pad(attention_mask, (0, combined_length - attention_mask.shape[1]))\n    if token_type_ids.shape[1] != combined_length:\n        token_type_ids = F.pad(token_type_ids, (0, combined_length - token_type_ids.shape[1]))\n\n    # modify the attention_mask and token_type_ids to fit the new sequence length\n    attention_mask = attention_mask[:, :input_ids.shape[1]]\n    token_type_ids = token_type_ids[:, :input_ids.shape[1]]\n\n    return {\n        \"input_ids\": input_ids,\n        \"input_video_embeds\": input_video_embeds,\n        \"attention_mask\": attention_mask,\n        \"token_type_ids\": token_type_ids,\n        **model_kwargs,\n    }\n\n", "CodeBleu_score": 39.79201572427059}]}
{"instance_id": "b1b0d25fe620", "generation_result": [{"code": "def _init_pipeline(self, cfg: ConfigType) -> Compose\n\t\n    pipeline = copy.deepcopy(cfg.test_pipeline)\n    if 'meta_keys' in pipeline[0]:\n        meta_keys = pipeline[0].meta_keys\n        if 'img_id' in meta_keys:\n            meta_keys.remove('img_id')\n        pipeline[0].meta_keys = meta_keys\n    load_img_idx = next(\n        (i for i, transform in enumerate(pipeline)\n         if isinstance(transform, LoadImageFromFile)))\n    pipeline[load_img_idx].type = 'mmdet.InferencerLoader'\n    return Compose(pipeline)\n\n", "CodeBleu_score": 34.76743165453757}, {"code": "def _init_pipeline(self, cfg: ConfigType) -> Compose\n\t\n    pipeline = copy.deepcopy(cfg.test_pipeline)\n    if 'meta_keys' in pipeline[0]:\n        pipeline[0]['meta_keys'].remove('img_id')\n    loader_index = pipeline.index(\n        next(filter(lambda x: isinstance(x, LoadImageFromFile), pipeline)))\n    pipeline[loader_index]['type'] = 'mmdet.InferencerLoader'\n    return Compose(pipeline)\n\n", "CodeBleu_score": 29.379395847305478}, {"code": "def _init_pipeline(self, cfg: ConfigType) -> Compose\n\t\n    pipeline = copy.deepcopy(cfg.test_pipeline)\n    meta_keys = pipeline[0].get('meta_keys', [])\n    if 'img_id' in meta_keys:\n        meta_keys.remove('img_id')\n    pipeline[0]['meta_keys'] = meta_keys\n    idx = next(\n        (i for i, t in enumerate(pipeline) if t['type'] == 'LoadImageFromFile'),\n        None)\n    if idx is None:\n        raise ValueError('Cannot find LoadImageFromFile in pipeline')\n    pipeline[idx]['type'] = 'mmdet.InferencerLoader'\n    return Compose(pipeline)\n\n", "CodeBleu_score": 35.61123055754296}]}
{"instance_id": "7967962856eb", "generation_result": [{"code": "def _inputs_to_list(self, inputs: InputsType) -> list\n\t\n    if isinstance(inputs, (str, Path)):\n        if isdir(inputs):\n            inputs = list_dir_or_file(inputs, full_path=True)\n        else:\n            inputs = [inputs]\n    elif not isinstance(inputs, (list, tuple)):\n        inputs = [inputs]\n    return inputs\n\n", "CodeBleu_score": 35.82004344491531}, {"code": "def _inputs_to_list(self, inputs: InputsType) -> list\n\t\n    if isinstance(inputs, str):\n        if isdir(inputs):\n            inputs = list_dir_or_file(inputs, full_path=True)\n        else:\n            inputs = [inputs]\n    elif not isinstance(inputs, (list, tuple)):\n        inputs = [inputs]\n    return inputs\n\n", "CodeBleu_score": 38.18707725332514}, {"code": "def _inputs_to_list(self, inputs: InputsType) -> list\n\t\n    if isinstance(inputs, (str, Path)):\n        if isdir(inputs):\n            inputs = list_dir_or_file(inputs, full_path=True)\n        else:\n            inputs = [inputs]\n    elif not isinstance(inputs, (list, tuple)):\n        inputs = [inputs]\n    return inputs\n\n", "CodeBleu_score": 35.82004344491531}]}
{"instance_id": "74f727f06fa0", "generation_result": [{"code": "def _get_chunk_data(self, inputs: Iterable, chunk_size: int)\n\t\n    if chunk_size <= 0:\n        raise ValueError(\n            f'Invalid chunk_size {chunk_size}, must be a positive integer')\n    chunk = []\n    for input in inputs:\n        if isinstance(input, dict) and ('img' in input or 'img_path' in input):\n            input = self.pipeline(input)\n        chunk.append(input)\n        if len(chunk) == chunk_size:\n            yield chunk\n            chunk = []\n    if chunk:\n        yield chunk\n\n", "CodeBleu_score": 27.39711308770172}, {"code": "def _get_chunk_data(self, inputs: Iterable, chunk_size: int)\n\t\n    chunk_data = []\n    for idx, input in enumerate(inputs):\n        if isinstance(input, dict):\n            if 'img' in input:\n                chunk_data.append(input)\n            elif 'img_path' in input:\n                img_path = input['img_path']\n                if isinstance(img_path, str):\n                    img_path = [img_path]\n                chunk_data.extend([{'img_path': img_path_} for img_path_ in img_path])\n            else:\n                raise ValueError(f'Input data must contain either \"img\" or \"img_path\" key.')\n        else:\n            chunk_data.append(input)\n        if (idx + 1) % chunk_size == 0:\n            yield chunk_data\n            chunk_data = []\n    if chunk_data:\n        yield chunk_data\n\n", "CodeBleu_score": 42.41472794815986}, {"code": "def _get_chunk_data(self, inputs: Iterable, chunk_size: int)\n\t\n    data_chunks = []\n    for input in inputs:\n        if isinstance(input, dict) and ('img' in input or 'img_path' in input):\n            data_chunks.append(self.pipeline(input))\n            if len(data_chunks) == chunk_size:\n                yield data_chunks\n                data_chunks = []\n    if len(data_chunks) > 0:\n        yield data_chunks\n\n", "CodeBleu_score": 23.91787185858506}]}
{"instance_id": "a43ab51afa59", "generation_result": [{"code": "def postprocess(\n        self,\n        preds: PredType,\n        visualization: Optional[List[np.ndarray]] = None,\n        return_datasamples: bool = False,\n        print_result: bool = False,\n        no_save_pred: bool = False,\n        pred_out_dir: str = '',\n        **kwargs,\n    ) -> Dict\n\t\n    # init output directory\n    if not no_save_pred and pred_out_dir == '':\n        pred_out_dir = self.work_dir\n    if not no_save_pred and pred_out_dir != '':\n        if isdir(pred_out_dir):\n            mmengine.mkdir_or_exist(pred_out_dir)\n        else:\n            warnings.warn(\n                f'pred_out_dir {pred_out_dir} is not a valid directory')\n\n    # prepare result dict\n    result = dict()\n    if return_datasamples:\n        if not no_save_pred:\n            warnings.warn(\n                'Saving datasamples is not supported in postprocess, '\n                'please set `no_save_pred=True`')\n        result['datasamples'] = []\n\n    # convert predictions to dictionaries\n    for i, pred in enumerate(preds):\n        pred = self.post_process(pred)\n        if return_datasamples:\n            pred = self.convert_to_datasample(pred)\n            result['datasamples'].append(pred)\n        else:\n            pred = self.convert_to_dict(pred)\n            result.update(pred)\n\n        if visualization is not None:\n            result['visualization'] = visualization[i]\n\n        if print_result:\n            print(result)\n\n        if not no_save_pred:\n            self.save_pred(result, pred_out_dir, i)\n\n    return result\n\n", "CodeBleu_score": 41.04596811319748}, {"code": "def postprocess(\n        self,\n        preds: PredType,\n        visualization: Optional[List[np.ndarray]] = None,\n        return_datasamples: bool = False,\n        print_result: bool = False,\n        no_save_pred: bool = False,\n        pred_out_dir: str = '',\n        **kwargs,\n    ) -> Dict\n\t\n    if not return_datasamples and pred_out_dir and not no_save_pred:\n        warnings.warn(\n            'Saving datasamples is not supported when return_datasamples=False'\n        )\n\n    if pred_out_dir and not isdir(pred_out_dir):\n        mmengine.mkdir_or_exist(pred_out_dir)\n\n    result = dict()\n    if visualization is not None:\n        result['visualization'] = visualization\n\n    if return_datasamples:\n        if pred_out_dir:\n            warnings.warn(\n                'Saving datasamples is not supported when return_datasamples=True'\n            )\n        result['predictions'] = preds\n    else:\n        result['predictions'] = self.to_dict(preds)\n\n    if print_result:\n        print(result)\n\n    if pred_out_dir:\n        self.save_predictions(result, pred_out_dir)\n\n    return result\n\n", "CodeBleu_score": 45.13920001323925}, {"code": "def postprocess(\n        self,\n        preds: PredType,\n        visualization: Optional[List[np.ndarray]] = None,\n        return_datasamples: bool = False,\n        print_result: bool = False,\n        no_save_pred: bool = False,\n        pred_out_dir: str = '',\n        **kwargs,\n    ) -> Dict\n\t\n    if pred_out_dir and return_datasamples:\n        warnings.warn('Saving datasamples is not supported for inference.')\n    if return_datasamples:\n        results = {'predictions': preds}\n    else:\n        results = {}\n    if visualization:\n        results['visualization'] = visualization\n    if print_result:\n        print(results)\n    if not no_save_pred:\n        if pred_out_dir:\n            mmengine.mkdir_or_exist(pred_out_dir)\n            for i, pred in enumerate(preds):\n                if isinstance(pred, tuple):\n                    pred = pred[0]\n                if isinstance(pred, (dict, list)):\n                    pred = json.dumps(pred)\n                if not isinstance(pred, str):\n                    pred = str(pred)\n                file_name = f'{i}.txt'\n                file_path = join_path(pred_out_dir, file_name)\n                with open(file_path, 'w') as f:\n                    f.write(pred)\n    return results\n\n", "CodeBleu_score": 44.336617435785044}]}
{"instance_id": "a3c2561f199c", "generation_result": [{"code": "def build_test_pipeline(cfg: ConfigType) -> ConfigType\n\t\n    cfg = copy.deepcopy(cfg)\n    test_pipeline = cfg.data.test.pipeline\n    first_transform = test_pipeline[0]\n    first_transform = copy.deepcopy(first_transform)\n    first_transform[\"type\"] = \"Resize\"\n    first_transform[\"img_scale\"] = cfg.data.test.img_scale\n    first_transform[\"keep_ratio\"] = False\n    last_transform = test_pipeline[-1]\n    last_transform = copy.deepcopy(last_transform)\n    last_transform[\"type\"] = \"PackDetInputs\"\n    test_pipeline = [first_transform, last_transform]\n    test_pipeline = Compose(test_pipeline)\n    return test_pipeline\n\n", "CodeBleu_score": 28.094842236023798}, {"code": "def build_test_pipeline(cfg: ConfigType) -> ConfigType\n\t\n    test_pipeline = copy.deepcopy(cfg.data.test.pipeline)\n    test_pipeline[0] = dict(type='Resize', scale=(1333, 800), keep_ratio=True)\n    test_pipeline[-1] = dict(type='PackDetInputs')\n    test_pipeline = Compose(test_pipeline)\n    return test_pipeline\n\n", "CodeBleu_score": 20.522639821499872}, {"code": "def build_test_pipeline(cfg: ConfigType) -> ConfigType\n\t\n    test_pipeline = copy.deepcopy(cfg.data.test.pipeline)\n    test_pipeline[0] = dict(type='Resize', scale=(1333, 800), keep_ratio=True)\n    test_pipeline = test_pipeline[:1] + test_pipeline[-1:]\n    test_pipeline = Compose(test_pipeline)\n    return test_pipeline\n\n", "CodeBleu_score": 19.18611367633214}]}
{"instance_id": "48a358ab5a38", "generation_result": [{"code": "def inference_mot(model: nn.Module, img: np.ndarray, frame_id: int,\n                  video_len: int) -> SampleList\n\t\n    img_metas = dict(\n        img=img,\n        frame_id=frame_id,\n        ori_shape=img.shape,\n        img_id=0,\n        video_len=video_len)\n    test_pipeline = model.cfg.test_pipeline\n    data = dict(img=img)\n    data = test_pipeline(data)\n    data['img_metas'] = [img_metas]\n    with torch.no_grad():\n        result = model.forward_test(data['img'][None].to(model.device))\n    return result\n\n", "CodeBleu_score": 27.22726091375097}, {"code": "def inference_mot(model: nn.Module, img: np.ndarray, frame_id: int,\n                  video_len: int) -> SampleList\n\t\n    data = dict(img=img, frame_id=frame_id, ori_shape=img.shape,\n                img_id=0, video_len=video_len)\n    data = model.test_step(data)\n    return data\n\n", "CodeBleu_score": 11.136210602572069}, {"code": "def inference_mot(model: nn.Module, img: np.ndarray, frame_id: int,\n                  video_len: int) -> SampleList\n\t\n    img_info = dict(\n        filename=f'{frame_id:06d}.jpg',\n        frame_id=frame_id,\n        ori_shape=img.shape,\n        img_shape=img.shape[:2],\n        img_id=frame_id,\n        video_len=video_len)\n\n    test_pipeline = Compose(model.cfg.test_pipeline)\n    data = dict(img=img, img_info=img_info)\n    data = test_pipeline(data)\n    data = default_collate([data])\n    data = scatter(data, [device.index for device in model.device_ids])\n    with torch.no_grad():\n        result = model(return_loss=False, rescale=True, **data)\n    return result\n\n", "CodeBleu_score": 33.31947991836926}]}
{"instance_id": "6c4ea2ce1787", "generation_result": [{"code": "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\n\t\n    while True:\n        total_length = len(tokens_a) + len(tokens_b)\n        if total_length <= max_num_tokens:\n            break\n        if len(tokens_a) > len(tokens_b):\n            tokens_a.pop(rng.randrange(len(tokens_a)))\n        else:\n            tokens_b.pop(rng.randrange(len(tokens_b)))\n\n", "CodeBleu_score": 47.45571068188216}, {"code": "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\n\t\n    while True:\n        total_length = len(tokens_a) + len(tokens_b)\n        if total_length <= max_num_tokens:\n            break\n\n        trunc_tokens = tokens_a if len(tokens_a) > len(tokens_b) else tokens_b\n        assert len(trunc_tokens) >= 1\n\n        # We want to sometimes truncate from the front and sometimes from the\n        # back to add more randomness and avoid biases.\n        if rng.random() < 0.5:\n            del trunc_tokens[0]\n        else:\n            trunc_tokens.pop()\n\n", "CodeBleu_score": 77.02217016729102}, {"code": "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\n\t\n    while True:\n        total_length = len(tokens_a) + len(tokens_b)\n        if total_length <= max_num_tokens:\n            break\n\n        trunc_tokens = tokens_a if len(tokens_a) > len(tokens_b) else tokens_b\n        assert len(trunc_tokens) >= 1\n\n        # We want to sometimes truncate from the front and sometimes from the\n        # back to add more randomness and avoid biases.\n        if rng.random() < 0.5:\n            del trunc_tokens[0]\n        else:\n            trunc_tokens.pop()\n\n", "CodeBleu_score": 77.02217016729102}]}
{"instance_id": "007181b6729d", "generation_result": [{"code": "def get_activation(activation_string)\n\t\n    if isinstance(activation_string, six.string_types):\n        activation_string = activation_string.lower()\n        if activation_string == \"linear\":\n            return lambda x: x\n        if activation_string == \"sigmoid\":\n            return lambda x: 1 / (1 + math.exp(-x))\n        if activation_string == \"tanh\":\n            return lambda x: math.tanh(x)\n        if activation_string == \"relu\":\n            return lambda x: x if x > 0 else 0\n        if activation_string == \"leaky_relu\":\n            return lambda x: x if x > 0 else 0.01 * x\n        if activation_string == \"softmax\":\n            return lambda x: math.exp(x) / sum(\n                [math.exp(i) for i in x]\n            )  # softmax is applied to the output of the last layer\n        if activation_string == \"none\":\n            return lambda x: x\n        raise ValueError(\n            \"Unsupported activation function: {}\".format(activation_string)\n        )\n    elif callable(activation_string):\n        return activation_string\n    else:\n        raise ValueError(\n            \"Input is expected to be a string or a callable function.\"\n        )\n\n", "CodeBleu_score": 44.347903943814984}, {"code": "def get_activation(activation_string)\n\t\n    if not isinstance(activation_string, six.string_types):\n        return activation_string\n    if not activation_string:\n        return None\n    activations = {\n        'relu': 'relu',\n        'sigmoid': 'sigmoid',\n        'softmax': 'softmax',\n        'linear': 'linear',\n        'tanh': 'tanh',\n        'none': None,\n        'softplus': 'softplus',\n        'softsign': 'softsign',\n        'selu': 'selu',\n        'elu': 'elu',\n        'exponential': 'exponential',\n        'hard_sigmoid': 'hard_sigmoid',\n        'linear': 'linear',\n        'softmax': 'softmax',\n    }\n    activation_string = activation_string.lower()\n    if activation_string in activations:\n        return activations[activation_string]\n    else:\n        raise ValueError('Activation %s not found' % activation_string)\n\n", "CodeBleu_score": 44.04512959120142}, {"code": "def get_activation(activation_string)\n\t\n    if not isinstance(activation_string, six.string_types):\n        return activation_string\n    if not activation_string:\n        return None\n    act = activation_string.lower()\n    if act == \"linear\":\n        return lambda x: x\n    elif act == \"sigmoid\":\n        return lambda x: 1 / (1 + math.exp(-x))\n    elif act == \"tanh\":\n        return lambda x: math.tanh(x)\n    elif act == \"relu\":\n        return lambda x: x if x > 0 else 0\n    elif act == \"leaky_relu\":\n        return lambda x: x if x > 0 else 0.01 * x\n    elif act == \"softmax\":\n        return lambda x: math.exp(x) / sum([math.exp(y) for y in x])\n    else:\n        raise ValueError(\"Unsupported activation function: {}\".format(act))\n\n", "CodeBleu_score": 68.1706549769351}]}
{"instance_id": "d5d6e0315899", "generation_result": [{"code": "def dropout(input_tensor, dropout_prob)\n\t\n    if dropout_prob is None or dropout_prob == 0.0:\n        return input_tensor\n    else:\n        return tf.nn.dropout(input_tensor, rate=dropout_prob)\n\n", "CodeBleu_score": 53.03103381915149}, {"code": "def dropout(input_tensor, dropout_prob)\n\t\n    if dropout_prob is None or dropout_prob == 0.0:\n        return input_tensor\n    else:\n        return tf.nn.dropout(input_tensor, keep_prob=1 - dropout_prob)\n\n", "CodeBleu_score": 52.911876931017744}, {"code": "def dropout(input_tensor, dropout_prob)\n\t\n    if dropout_prob is None or dropout_prob == 0.0:\n        return input_tensor\n    return tf.nn.dropout(input_tensor, 1.0 - dropout_prob)\n\n", "CodeBleu_score": 60.78403505639335}]}
{"instance_id": "828917b9ac53", "generation_result": [{"code": "def create_attention_mask_from_input_mask(from_tensor, to_mask)\n\t\n    from_shape = from_tensor.shape\n    proj_shape = to_mask.shape\n    seq_length = from_shape[1]\n    proj_seq_length = proj_shape[1]\n    if seq_length != proj_seq_length:\n        raise ValueError(\"The length of sequence in from_tensor ({}) should be the same as the length \"\n                         \"of sequence in to_mask ({}).\".format(seq_length, proj_seq_length))\n    ones = tf.ones_like(to_mask)\n    return tf.reshape(ones, [-1]) * tf.reshape(to_mask, [-1])\n\n", "CodeBleu_score": 20.02054766031668}, {"code": "def create_attention_mask_from_input_mask(from_tensor, to_mask)\n\t\n    from_tensor = tf.cast(from_tensor, tf.float32)\n    to_mask = tf.cast(to_mask, tf.float32)\n    to_mask = tf.reshape(to_mask, [-1, 1, tf.shape(from_tensor)[-1]])\n    return to_mask * from_tensor\n\n", "CodeBleu_score": 12.374778537125913}, {"code": "def create_attention_mask_from_input_mask(from_tensor, to_mask)\n\t\n    from_shape = get_shape_list(from_tensor, expected_rank=[2, 3])\n    to_shape = get_shape_list(to_mask, expected_rank=[2, 3])\n\n    batch_size = from_shape[0] if from_shape[0] is not None else to_shape[0]\n    from_seq_length = from_shape[1]\n    to_seq_length = to_shape[1]\n\n    to_mask = tf.cast(\n        tf.reshape(to_mask, [batch_size, 1, to_seq_length]), tf.float32\n    )\n    return tf.matmul(to_mask, tf.transpose(to_mask, [0, 2, 1]))\n\n", "CodeBleu_score": 38.94787360696279}]}
{"instance_id": "d7798e41a723", "generation_result": [{"code": "def transpose_for_scores(input_tensor, batch_size, num_attention_heads,\n                           seq_length, width)\n\t\n    input_tensor = tf.reshape(input_tensor, [batch_size, seq_length, num_attention_heads, width])\n    input_tensor = tf.transpose(input_tensor, [0, 2, 1, 3])\n    return input_tensor\n\n", "CodeBleu_score": 42.61845112285781}, {"code": "def transpose_for_scores(input_tensor, batch_size, num_attention_heads,\n                           seq_length, width)\n\t\n    input_tensor = tf.reshape(input_tensor, [batch_size, seq_length, num_attention_heads, width])\n    input_tensor = tf.transpose(input_tensor, [0, 2, 1, 3])\n    return input_tensor\n\n", "CodeBleu_score": 42.61845112285781}, {"code": "def transpose_for_scores(input_tensor, batch_size, num_attention_heads,\n                           seq_length, width)\n\t\n    input_tensor.shape.assert_is_compatible_with([batch_size, seq_length, num_attention_heads * width])\n    output = tf.reshape(input_tensor, [batch_size, seq_length, num_attention_heads, width])\n    output = tf.transpose(output, [0, 2, 1, 3])\n    output.shape.assert_is_compatible_with([batch_size, num_attention_heads, seq_length, width])\n    return output\n\n", "CodeBleu_score": 33.058086409267965}]}
{"instance_id": "711d466ace09", "generation_result": [{"code": "def reshape_to_matrix(input_tensor)\n\t\n    tensor_rank = len(input_tensor.shape)\n    if tensor_rank < 2:\n        raise ValueError(\"Input tensor must have rank >= 2.\")\n    elif tensor_rank == 2:\n        return input_tensor\n    else:\n        width = input_tensor.shape[-1]\n        return tf.reshape(input_tensor, [-1, width])\n\n", "CodeBleu_score": 44.82731511681462}, {"code": "def reshape_to_matrix(input_tensor)\n\t\n    if len(input_tensor.shape) < 2:\n        raise ValueError(\n            \"The input tensor must have at least rank 2, but it has rank {}.\".format(\n                len(input_tensor.shape)\n            )\n        )\n    if len(input_tensor.shape) == 2:\n        return input_tensor\n    width = input_tensor.shape[-1]\n    return tf.reshape(input_tensor, [-1, width])\n\n", "CodeBleu_score": 48.070901317099704}, {"code": "def reshape_to_matrix(input_tensor)\n\t\n    if len(input_tensor.shape) < 2:\n        raise ValueError(\"Input tensor must have rank >= 2.\")\n    if len(input_tensor.shape) == 2:\n        return input_tensor\n    width = input_tensor.shape[-1]\n    return tf.reshape(input_tensor, [-1, width])\n\n", "CodeBleu_score": 46.199969772084394}]}
{"instance_id": "7adcd08f531e", "generation_result": [{"code": "def get_train_examples(self, data_dir)\n\t\n    train_examples = []\n    with open(os.path.join(data_dir, \"train.tsv\"), \"r\", encoding=\"utf-8\") as f:\n        reader = csv.reader(f, delimiter=\"\\t\", quotechar=None)\n        next(reader)\n        for line in reader:\n            text_a = line[1]\n            label = line[0]\n            if label == \"contradictory\":\n                label = \"contradiction\"\n            train_examples.append(\n                modeling.InputExample(guid=None, text_a=text_a, text_b=None, label=label)\n            )\n    return train_examples\n\n", "CodeBleu_score": 28.139040812120182}, {"code": "def get_train_examples(self, data_dir)\n\t\n    train_examples = []\n    with open(os.path.join(data_dir, \"train.tsv\"), \"r\", encoding=\"utf-8\") as f:\n        reader = csv.reader(f, delimiter=\"\\t\", quotechar=None)\n        next(reader)\n        for line in reader:\n            text_a = line[1]\n            text_b = line[2]\n            label = line[-1]\n            if label == \"contradictory\":\n                label = \"contradiction\"\n            train_examples.append(\n                modeling.InputExample(guid=None, text_a=text_a, text_b=text_b, label=label)\n            )\n    return train_examples\n\n", "CodeBleu_score": 29.637824352593462}, {"code": "def get_train_examples(self, data_dir)\n\t\n    train_examples = []\n    with open(os.path.join(data_dir, \"train.tsv\"), \"r\") as f:\n        reader = csv.reader(f, delimiter=\"\\t\", quotechar=None)\n        next(reader)\n        for line in reader:\n            text_a = line[1]\n            text_b = line[2]\n            label = line[-1]\n            if label == \"contradictory\":\n                label = \"contradiction\"\n            train_examples.append(\n                modeling.InputExample(guid=None, text_a=text_a, text_b=text_b, label=label)\n            )\n    return train_examples\n\n", "CodeBleu_score": 29.40376284845097}]}
{"instance_id": "26a0bb96dab9", "generation_result": [{"code": "def metric_fn(per_example_loss, label_ids, logits, is_real_example)\n\t\n    # Calculate predictions by taking the argmax of the logits\n    predictions = logits.argmax(dim=-1)\n\n    # Calculate accuracy by comparing the predictions to the true labels\n    # and taking the mean over the batch\n    accuracy = (predictions == label_ids).float().mean()\n\n    # Calculate mean loss by taking the mean over the batch\n    loss = per_example_loss.mean()\n\n    # Calculate mean loss for real examples\n    real_example_loss = (per_example_loss * is_real_example).mean()\n\n    # Return a dictionary containing the evaluation accuracy and loss\n    return {\n        \"accuracy\": accuracy.item(),\n        \"loss\": loss.item(),\n        \"real_example_loss\": real_example_loss.item(),\n    }\n\n", "CodeBleu_score": 25.23312321562286}, {"code": "def metric_fn(per_example_loss, label_ids, logits, is_real_example)\n\t\n    predictions = logits.argmax(axis=-1)\n    accuracy = (predictions == label_ids).mean()\n    loss = per_example_loss.mean()\n    return collections.OrderedDict(\n        loss=loss,\n        accuracy=accuracy,\n    )\n\n", "CodeBleu_score": 17.898207969371665}, {"code": "def metric_fn(per_example_loss, label_ids, logits, is_real_example)\n\t\n    # The function calculates evaluation metrics for a model.\n    # It computes predictions by taking the argmax of the logits and calculates accuracy using the true labels and a mask for real examples.\n    # It also calculates the mean loss for the examples, again weighted by the real example mask.\n    # The function returns a dictionary containing the evaluation accuracy and loss.\n\n    # Calculate predictions by taking the argmax of the logits\n    predictions = logits.argmax(axis=-1)\n\n    # Calculate accuracy using the true labels and a mask for real examples\n    accuracy = (predictions == label_ids).mean()\n\n    # Calculate the mean loss for the examples, again weighted by the real example mask\n    loss = per_example_loss.mean()\n\n    # Return a dictionary containing the evaluation accuracy and loss\n    return {\n        \"eval_accuracy\": accuracy,\n        \"eval_loss\": loss,\n    }\n\n", "CodeBleu_score": 29.716030213931194}]}
{"instance_id": "6154e4d28509", "generation_result": [{"code": "def convert_examples_to_features(examples, label_list, max_seq_length,\n                                 tokenizer)\n\t\n    label_map = {label: i for i, label in enumerate(label_list)}\n\n    features = []\n    for (ex_index, example) in enumerate(examples):\n        if ex_index % 10000 == 0:\n            print(\"Writing example %d of %d\" % (ex_index, len(examples)))\n        feature = convert_single_example(ex_index, example, label_list,\n                                         max_seq_length, tokenizer)\n        if feature:\n            features.append(feature)\n    return features\n\n", "CodeBleu_score": 73.15613341161145}, {"code": "def convert_examples_to_features(examples, label_list, max_seq_length,\n                                 tokenizer)\n\t\n    label_map = {label: i for i, label in enumerate(label_list)}\n\n    features = []\n    for (ex_index, example) in enumerate(examples):\n        if ex_index % 10000 == 0:\n            print(\"Writing example %d of %d\" % (ex_index, len(examples)))\n\n        feature = convert_single_example(ex_index, example, label_list,\n                                         max_seq_length, tokenizer)\n\n        features.append(feature)\n    return features\n\n", "CodeBleu_score": 74.8424828176771}, {"code": "def convert_examples_to_features(examples, label_list, max_seq_length,\n                                 tokenizer)\n\t\n    label_map = {label: i for i, label in enumerate(label_list)}\n\n    features = []\n    for (ex_index, example) in enumerate(examples):\n        if ex_index % 10000 == 0:\n            print(\"Writing example %d of %d\" % (ex_index, len(examples)))\n\n        feature = convert_single_example(\n            ex_index, example, label_list, max_seq_length, tokenizer)\n\n        features.append(feature)\n    return features\n\n", "CodeBleu_score": 73.20399098647896}]}
{"instance_id": "e09c0db71ebb", "generation_result": [{"code": "def get_next_sentence_output(bert_config, input_tensor, labels)\n\t\n    # The output weights are the same for all tokens\n    output_weights = tf.get_variable(\n        \"output_weights\", [bert_config.hidden_size, bert_config.vocab_size],\n        initializer=tf.truncated_normal_initializer(stddev=0.02))\n\n    # The output bias is the same for all tokens\n    output_bias = tf.get_variable(\n        \"output_bias\", [bert_config.vocab_size],\n        initializer=tf.zeros_initializer())\n\n    # The logits are computed by multiplying the input tensor with the output weights and adding the output bias\n    logits = tf.matmul(input_tensor, output_weights, transpose_b=True)\n    logits = tf.nn.bias_add(logits, output_bias)\n\n    # The log probabilities are computed by applying a log softmax to the logits\n    log_probs = tf.nn.log_softmax(logits, axis=-1)\n\n    # The labels are reshaped to match the shape of the log probabilities\n    labels = tf.reshape(labels, [-1])\n\n    # The one-hot encoding is computed by converting the labels to a one-hot tensor\n    one_hot_labels = tf.one_hot(labels, depth=bert_config.vocab_size, dtype=tf.float32)\n\n    # The per-example loss is computed as the negative sum of the product of one-hot labels and log probabilities\n    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n\n    # The mean loss is computed by taking the mean of the per-example loss\n    mean_loss = tf.reduce_mean(per_example_loss)\n\n    return mean_loss, per_example_loss, log_probs\n\n", "CodeBleu_score": 47.60658595991446}, {"code": "def get_next_sentence_output(bert_config, input_tensor, labels)\n\t\n\n    # Define the output weights and bias\n    output_weights = tf.get_variable(\n        \"output_weights\", [bert_config.hidden_size, bert_config.vocab_size],\n        initializer=tf.truncated_normal_initializer(stddev=0.02))\n    output_bias = tf.get_variable(\n        \"output_bias\", [bert_config.vocab_size],\n        initializer=tf.zeros_initializer())\n\n    # Compute logits through matrix multiplication and bias addition\n    logits = tf.matmul(input_tensor, output_weights, transpose_b=True)\n    logits = tf.nn.bias_add(logits, output_bias)\n\n    # Apply a log softmax to get log probabilities\n    log_probs = tf.nn.log_softmax(logits, axis=-1)\n\n    # Reshape labels and convert to one-hot encoding\n    labels = tf.reshape(labels, [-1])\n    one_hot_labels = tf.one_hot(labels, depth=bert_config.vocab_size, dtype=tf.float32)\n\n    # Compute per-example loss as the negative sum of the product of one-hot labels and log probabilities\n    per_example_loss = tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n    per_example_loss *= -1.0\n\n    # Compute mean loss as the mean of the per-example loss\n    mean_loss = tf.reduce_mean(per_example_loss)\n\n    return mean_loss, per_example_loss, log_probs\n\n", "CodeBleu_score": 45.942227879551304}, {"code": "def get_next_sentence_output(bert_config, input_tensor, labels)\n\t\n    with tf.variable_scope(\"cls/seq_relationship\"):\n        output_weights = tf.get_variable(\"output_weights\", [2, bert_config.hidden_size],\n                                         initializer=tf.truncated_normal_initializer(stddev=0.02))\n        output_bias = tf.get_variable(\"output_bias\", [2], initializer=tf.zeros_initializer())\n\n    # Reshape [batch_size, seq_length] label tensor to a [batch_size, 1] tensor\n    labels = tf.reshape(labels, [-1, 1])\n\n    # Convert one hot encoding to label indices and reshape label_ids to [batch_size, 1]\n    # label_ids = tf.argmax(labels, axis=-1)\n    # label_ids = tf.reshape(label_ids, [-1, 1])\n\n    # Map the label_ids to each position\n    one_hot_labels = tf.one_hot(labels, depth=2, dtype=tf.float32)\n\n    # Multiply one_hot_labels with the output weights and add the bias\n    logits = tf.matmul(one_hot_labels, output_weights, transpose_b=True)\n    logits = tf.nn.bias_add(logits, output_bias)\n\n    # Compute the probabilities of the logits\n    probabilities = tf.nn.softmax(logits, axis=-1)\n\n    # Compute the per example loss\n    per_example_loss = -tf.reduce_sum(one_hot_labels * tf.log(probabilities), axis=-1)\n\n    # Compute the mean loss\n    loss = tf.reduce_mean(per_example_loss)\n\n    return loss, per_example_loss, probabilities\n\n", "CodeBleu_score": 62.6851663145496}]}
{"instance_id": "1f66b020f485", "generation_result": [{"code": "def gather_indexes(sequence_tensor, positions)\n\t\n    # This function gathers vectors at specific positions from a sequence tensor over a minibatch.\n    # It retrieves the shape of the sequence tensor and computes flat offsets based on the batch size and sequence length.\n    # The positions are adjusted by these offsets and flattened.\n    # The sequence tensor is also flattened, and the function gathers the vectors at the specified positions from this flattened tensor.\n    # The function returns the gathered vectors as the output tensor.\n    # sequence_tensor: A tensor of shape [batch_size, seq_length, hidden_size]\n    # positions: A tensor of shape [batch_size, seq_length]\n    # returns: A tensor of shape [batch_size, seq_length, hidden_size]\n    batch_size, seq_length = modeling.get_shape_list(sequence_tensor, expected_rank=3)\n    flat_offsets = tf.reshape(tf.range(0, batch_size, dtype=tf.int32) * seq_length, [-1, 1])\n    flat_positions = tf.reshape(positions + flat_offsets, [-1])\n    flat_sequence_tensor = tf.reshape(sequence_tensor, [-1, hidden_size])\n    output_tensor = tf.gather(flat_sequence_tensor, flat_positions)\n    return output_tensor\n\n", "CodeBleu_score": 38.14249948751322}, {"code": "def gather_indexes(sequence_tensor, positions)\n\t\n    sequence_length = sequence_tensor.size(1)\n    batch_size = sequence_tensor.size(0)\n    flat_offsets = torch.arange(0, batch_size, dtype=torch.long) * sequence_length\n    flat_positions = positions + flat_offsets[:, None]\n    flat_sequence_tensor = sequence_tensor.reshape(-1, sequence_tensor.size(-1))\n    output_tensor = flat_sequence_tensor[flat_positions]\n    return output_tensor.reshape(batch_size, -1, sequence_tensor.size(-1))\n\n", "CodeBleu_score": 26.681797383393775}, {"code": "def gather_indexes(sequence_tensor, positions)\n\t\n    sequence_length = sequence_tensor.size(1)\n    batch_size = sequence_tensor.size(0)\n    flat_offsets = torch.arange(0, batch_size, dtype=torch.long) * sequence_length\n    flat_positions = positions + flat_offsets[:, None]\n    flat_sequence_tensor = sequence_tensor.reshape(-1, sequence_tensor.size(-1))\n    output_tensor = flat_sequence_tensor[flat_positions]\n    return output_tensor\n\n", "CodeBleu_score": 25.785997893207213}]}
{"instance_id": "956bffee9a33", "generation_result": [{"code": "def _decode_record(record, name_to_features)\n\t\n    example = tf.io.parse_single_example(record, name_to_features)\n    for name in list(example.keys()):\n        t = example[name]\n        if t.dtype == tf.int64:\n            t = tf.cast(t, tf.int32)\n        example[name] = t\n    return example\n\n", "CodeBleu_score": 59.0051892348668}, {"code": "def _decode_record(record, name_to_features)\n\t\n    example = tf.io.parse_single_example(record, name_to_features)\n    for name in list(example.keys()):\n        t = example[name]\n        if t.dtype == tf.int64:\n            t = tf.cast(t, tf.int32)\n        example[name] = t\n    return example\n\n", "CodeBleu_score": 59.0051892348668}, {"code": "def _decode_record(record, name_to_features)\n\t\n    example = tf.io.parse_single_example(record, name_to_features)\n\n    for name in list(example.keys()):\n        t = example[name]\n        if t.dtype == tf.int64:\n            t = tf.cast(t, tf.int32)\n        example[name] = t\n\n    return example\n\n", "CodeBleu_score": 59.0051892348668}]}
{"instance_id": "6b97f354e641", "generation_result": [{"code": "def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer,\n                         orig_answer_text)\n\t\n    start_position = input_start\n    end_position = input_end\n    orig_doc_len = len(doc_tokens)\n    while orig_doc_len < 3:\n        doc_tokens.append(\"[PAD]\")\n        orig_doc_len += 1\n\n    for new_start in range(input_start, input_end + 1):\n        for new_end in (input_end, new_start + len(orig_answer_text) - 1,\n                        input_end):\n            if new_start >= len(doc_tokens):\n                continue\n            if new_end >= len(doc_tokens):\n                continue\n            if tokenizer.convert_tokens_to_string(\n                    doc_tokens[new_start:new_end + 1]) == orig_answer_text:\n                start_position = new_start\n                end_position = new_end\n                return start_position, end_position\n\n    return start_position, end_position\n\n", "CodeBleu_score": 34.0952651879766}, {"code": "def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer,\n                         orig_answer_text)\n\t\n    tok_answer_text = \" \".join(tokenizer.tokenize(orig_answer_text))\n\n    for new_start in range(input_start, input_end + 1):\n        for new_end in (new_start + len(tok_answer_text), input_end + 1):\n            text_span = \" \".join(doc_tokens[new_start:new_end])\n            if text_span == tok_answer_text:\n                return (new_start, new_end)\n\n    return (input_start, input_end)\n\n", "CodeBleu_score": 61.25561248803413}, {"code": "def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer,\n                         orig_answer_text)\n\t\n    # Tokenize the original answer text to get a list of tokens\n    answer_tokens = tokenizer.tokenize(orig_answer_text)\n\n    # Find the start and end positions of the original answer text in the document tokens\n    start_position = input_start\n    end_position = input_end\n    while start_position >= 0:\n        start_position -= 1\n        if start_position < 0:\n            break\n        if doc_tokens[start_position] == answer_tokens[0]:\n            break\n\n    start_position += 1\n\n    while end_position < len(doc_tokens):\n        if doc_tokens[end_position] == answer_tokens[-1]:\n            break\n        end_position += 1\n\n    # Determine if the answer text matches the tokenized answer text\n    if input_start == start_position and input_end == end_position:\n        return (input_start, input_end)\n\n    # Iterate through possible new start and end positions within the input span\n    for new_start in range(start_position, input_start + 1):\n        for new_end in range(input_end, end_position + 1):\n            # Check if the new span matches the tokenized answer text\n            if doc_tokens[new_start:new_end] == answer_tokens:\n                return (new_start, new_end)\n\n    # If no match is found, return the original input start and end positions\n    return (input_start, input_end)\n\n", "CodeBleu_score": 35.94494708422723}]}
{"instance_id": "aba334b92b34", "generation_result": [{"code": "def _check_is_max_context(doc_spans, cur_span_index, position)\n\t\n    max_score = -1000000\n    best_span_index = -1\n    for (span_index, span) in enumerate(doc_spans):\n        end = span.start + span.length - 1\n        if position < span.start:\n            continue\n        if position > end:\n            continue\n        num_left_context = position - span.start\n        num_right_context = end - position\n        score = (num_left_context + num_right_context) / span.length\n        if score > max_score:\n            max_score = score\n            best_span_index = span_index\n    return cur_span_index == best_span_index\n\n", "CodeBleu_score": 65.15533396530992}, {"code": "def _check_is_max_context(doc_spans, cur_span_index, position)\n\t\n    best_score = -1000000\n    best_span_index = -1\n    for (span_index, doc_span) in enumerate(doc_spans):\n        end = doc_span.start + doc_span.length - 1\n        if position < doc_span.start:\n            continue\n        if position > end:\n            continue\n        num_left_context = position - doc_span.start\n        num_right_context = end - position\n        score = (num_left_context + num_right_context) / doc_span.length\n        if score > best_score:\n            best_score = score\n            best_span_index = span_index\n    return cur_span_index == best_span_index\n\n", "CodeBleu_score": 73.40671067101543}, {"code": "def _check_is_max_context(doc_spans, cur_span_index, position)\n\t\n    max_score = -1000\n    best_span_index = -1\n    for (span_index, span) in enumerate(doc_spans):\n        end = span.start + span.length - 1\n        if position < span.start:\n            continue\n        if position > end:\n            continue\n        num_left_context = position - span.start\n        num_right_context = end - position\n        score = (num_left_context + num_right_context) / span.length\n        if score > max_score:\n            max_score = score\n            best_span_index = span_index\n    return cur_span_index == best_span_index\n\n", "CodeBleu_score": 65.2298504480503}]}
{"instance_id": "cf92ad509f6a", "generation_result": [{"code": "def load_vocab(vocab_file)\n\t\n    vocab = collections.OrderedDict()\n    index = 0\n    with open(vocab_file, 'r', encoding='utf-8') as reader:\n        while True:\n            token = six.ensure_str(reader.readline())\n            if not token:\n                break\n            token = token.strip()\n            vocab[token] = index\n            index += 1\n    return vocab\n\n", "CodeBleu_score": 60.93628119773393}, {"code": "def load_vocab(vocab_file)\n\t\n    vocab = collections.OrderedDict()\n    index = 0\n    with open(vocab_file, \"rb\") as f:\n        while True:\n            token = f.readline()\n            if not token:\n                break\n            token = token.strip()\n            vocab[token] = index\n            index += 1\n    return vocab\n\n", "CodeBleu_score": 58.53273351784119}, {"code": "def load_vocab(vocab_file)\n\t\n    vocab = collections.OrderedDict()\n    index = 0\n    with open(vocab_file, \"rb\") as reader:\n        while True:\n            token = six.ensure_str(reader.readline())\n            if not token:\n                break\n            token = token.strip()\n            vocab[token] = index\n            index += 1\n    return vocab\n\n", "CodeBleu_score": 62.44653591769782}]}
{"instance_id": "6c865803ac12", "generation_result": [{"code": "def convert_by_vocab(vocab, items)\n\t\n    output = []\n    for item in items:\n        output.append(vocab.get(item, vocab.get(\"<unk>\")))\n    return output\n\n", "CodeBleu_score": 57.52029764603922}, {"code": "def convert_by_vocab(vocab, items)\n\t\n    output = []\n    for item in items:\n        output.append(vocab[item])\n    return output\n\n", "CodeBleu_score": 79.63749907165396}, {"code": "def convert_by_vocab(vocab, items)\n\t\n    output = []\n    for item in items:\n        output.append(vocab[item])\n    return output\n\n", "CodeBleu_score": 79.63749907165396}]}
{"instance_id": "e93b9200dc4a", "generation_result": [{"code": "def tokenize(self, text)\n\t\n    tokens = self.basic_tokenizer.tokenize(text)\n    if self.do_lower_case:\n        tokens = [token.lower() for token in tokens]\n    if not self.wordpiece_tokenizer.vocab:\n        raise ValueError(\"`wordpiece_tokenizer` must be an instance of `WordpieceTokenizer`.\")\n    for token in tokens:\n        if not self.wordpiece_tokenizer.vocab.get(token):\n            raise KeyError(f\"`wordpiece_tokenizer` doesn't contain {token}\")\n    output_tokens = []\n    for token in tokens:\n        sub_tokens = self.wordpiece_tokenizer.tokenize(token)\n        output_tokens.extend(sub_tokens)\n    return output_tokens\n\n", "CodeBleu_score": 46.53837572579367}, {"code": "def tokenize(self, text)\n\t\n    text = convert_to_unicode(text)\n    text = self.clean_text(text)\n    text = self.tokenize_chinese_chars(text)\n    orig_tokens = whitespace_tokenize(text)\n    split_tokens = []\n    for token in orig_tokens:\n        if self.do_lower_case:\n            token = token.lower()\n            token = self.subword_tokenize(token)\n            for sub_token in token:\n                split_tokens.append(sub_token)\n        else:\n            split_tokens.extend(self.subword_tokenize(token))\n    return split_tokens\n\n", "CodeBleu_score": 40.71467670193339}, {"code": "def tokenize(self, text)\n\t\n    text = self._clean_text(text)\n    text = self._tokenize_chinese_chars(text)\n    orig_tokens = whitespace_tokenize(text)\n    split_tokens = []\n    for token in orig_tokens:\n        if self.do_lower_case:\n            token = token.lower()\n            token = self._run_strip_accents(token)\n        if token in self._vocab:\n            split_tokens.append(token)\n        else:\n            split_tokens.extend(self._run_split_on_punc(token))\n    return split_tokens\n\n", "CodeBleu_score": 37.435895178790176}]}
{"instance_id": "2ff6f6db53c9", "generation_result": [{"code": "def safe_load(path, offset, duration, sample_rate, dtype)\n\t\n    try:\n        audio_data = load(path, offset, duration, sample_rate, dtype)\n        return audio_data, False\n    except Exception as e:\n        print(f\"Error loading audio from {path} (offset={offset}, duration={duration}, sample_rate={sample_rate}, dtype={dtype}): {e}\")\n        return -1.0, True\n\n", "CodeBleu_score": 21.686585690655672}, {"code": "def safe_load(path, offset, duration, sample_rate, dtype)\n\t\n    try:\n        audio_data = path.load(offset, duration, sample_rate, dtype)\n        return audio_data, False\n    except Exception as e:\n        print(f\"Error loading audio from {path}: {e}\")\n        return -1.0, True\n\n", "CodeBleu_score": 22.739294166477013}, {"code": "def safe_load(path, offset, duration, sample_rate, dtype)\n\t\n    try:\n        audio_data = load(path, offset, duration, sample_rate, dtype)\n        return audio_data, False\n    except Exception as e:\n        print(f\"Error loading audio from {path}: {e}\")\n        return -1.0, True\n\n", "CodeBleu_score": 18.155542463964103}]}
{"instance_id": "12fbe1f03d8f", "generation_result": [{"code": "def to_n_channels(waveform: tf.Tensor, n_channels: int) -> tf.Tensor\n\t\n    if waveform.shape[1] == n_channels:\n        return waveform\n    elif waveform.shape[1] > n_channels:\n        return waveform[:, :n_channels]\n    else:\n        return tf.tile(waveform, [1, n_channels // waveform.shape[1] + 1])[:, :n_channels]\n\n\n", "CodeBleu_score": 48.3177813955956}, {"code": "def to_n_channels(waveform: tf.Tensor, n_channels: int) -> tf.Tensor\n\t\n    waveform = tf.convert_to_tensor(waveform, dtype=tf.float32)\n    if waveform.shape[-1] == n_channels:\n        return waveform\n    elif waveform.shape[-1] > n_channels:\n        return waveform[..., :n_channels]\n    else:\n        return tf.tile(waveform[..., None], [1] * (waveform.ndim - 1) + [n_channels])[..., :n_channels]\n\n\n", "CodeBleu_score": 44.079894178609194}, {"code": "def to_n_channels(waveform: tf.Tensor, n_channels: int) -> tf.Tensor\n\t\n    waveform = tf.convert_to_tensor(waveform)\n    shape = waveform.shape\n    waveform = tf.reshape(waveform, [-1, shape[-1]])\n    if n_channels < shape[-1]:\n        waveform = waveform[..., :n_channels]\n    else:\n        waveform = tf.tile(waveform, [1, n_channels // shape[-1]])\n        waveform = waveform[..., :n_channels]\n    waveform = tf.reshape(waveform, shape)\n    return waveform\n\n\n\n", "CodeBleu_score": 45.35096203192741}]}
{"instance_id": "5e215f029b2e", "generation_result": [{"code": "def spectrogram_to_db_uint(\n    spectrogram: tf.Tensor, db_range: float = 100.0, **kwargs\n) -> tf.Tensor\n\t\n    # Convert the spectrogram to dB\n    db_spectrogram = gain_to_db(spectrogram, **kwargs)\n\n    # Find the maximum dB value\n    max_db = tf.reduce_max(db_spectrogram)\n\n    # Clip the dB spectrogram to a specified range below the maximum dB value\n    clipped_db_spectrogram = tf.clip_by_value(\n        db_spectrogram, clip_value_min=max_db - db_range, clip_value_max=max_db\n    )\n\n    # Convert the clipped dB spectrogram from float32 to uint8 format\n    db_spectrogram_uint = tf.cast(\n        clipped_db_spectrogram / (db_range / 255.0) + 128.0, tf.uint8\n    )\n\n    return db_spectrogram_uint\n\n\n", "CodeBleu_score": 43.887530693692966}, {"code": "def spectrogram_to_db_uint(\n    spectrogram: tf.Tensor, db_range: float = 100.0, **kwargs\n) -> tf.Tensor\n\t\n    spectrogram = gain_to_db(spectrogram, **kwargs)\n    max_db = tf.reduce_max(spectrogram)\n    spectrogram = tf.clip_by_value(spectrogram, -db_range, max_db)\n    spectrogram = tf.cast(\n        tf.math.round((spectrogram + db_range) / (2 * db_range) * 255), tf.uint8\n    )\n    return spectrogram\n\n\n\n", "CodeBleu_score": 45.716049639860856}, {"code": "def spectrogram_to_db_uint(\n    spectrogram: tf.Tensor, db_range: float = 100.0, **kwargs\n) -> tf.Tensor\n\t\n    spectrogram = gain_to_db(spectrogram, **kwargs)\n    max_db = tf.reduce_max(spectrogram)\n    spectrogram = tf.clip_by_value(spectrogram, -db_range, max_db)\n    spectrogram = tf.cast(\n        tf.math.round((spectrogram + db_range) * 255.0 / db_range), tf.uint8\n    )\n    return spectrogram\n\n\n", "CodeBleu_score": 45.834251687948246}]}
{"instance_id": "b65c0968dae7", "generation_result": [{"code": "def compute_spectrogram_tf(\n    waveform: tf.Tensor,\n    frame_length: int = 2048,\n    frame_step: int = 512,\n    spec_exponent: float = 1.0,\n    window_exponent: float = 1.0,\n) -> tf.Tensor\n\t\n    window_fn = lambda x: tf.pow(hann_window(x, periodic=True), window_exponent)\n    stfts = tf.transpose(\n        stft(\n            tf.pow(waveform, spec_exponent),\n            frame_length=frame_length,\n            frame_step=frame_step,\n            window_fn=window_fn,\n        ),\n        perm=[0, 2, 1],\n    )\n    return tf.abs(stfts)", "CodeBleu_score": 41.79485966295829}, {"code": "def compute_spectrogram_tf(\n    waveform: tf.Tensor,\n    frame_length: int = 2048,\n    frame_step: int = 512,\n    spec_exponent: float = 1.0,\n    window_exponent: float = 1.0,\n) -> tf.Tensor\n\t\n    window = hann_window(frame_length, window_exponent)\n    stfts = stft(waveform, frame_length, frame_step, window)\n    spectrogram = tf.abs(stfts) ** spec_exponent\n    return tf.transpose(spectrogram)", "CodeBleu_score": 29.41577063895973}, {"code": "def compute_spectrogram_tf(\n    waveform: tf.Tensor,\n    frame_length: int = 2048,\n    frame_step: int = 512,\n    spec_exponent: float = 1.0,\n    window_exponent: float = 1.0,\n) -> tf.Tensor\n\t\n    window_length = int(frame_length ** window_exponent)\n    window = hann_window(window_length)\n    window = tf.pow(window, window_exponent)\n    window = tf.cast(window, tf.float32)\n    stfts = tf.signal.stft(\n        signals=waveform,\n        frame_length=frame_length,\n        frame_step=frame_step,\n        window_fn=lambda x: window,\n    )\n    spectrogram = tf.abs(stfts)\n    spectrogram = tf.pow(spectrogram, spec_exponent)\n    return tf.transpose(spectrogram, perm=[2, 0, 1])\n\n", "CodeBleu_score": 37.49679138060109}]}
{"instance_id": "a4686140a634", "generation_result": [{"code": "def time_stretch(\n    spectrogram: tf.Tensor,\n    factor: float = 1.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n    # Calculate the new time dimension\n    time_dim = tf.cast(tf.shape(spectrogram)[1], tf.float32) * factor\n\n    # Resize the spectrogram\n    resized_spectrogram = tf.image.resize(\n        spectrogram,\n        size=(tf.cast(tf.shape(spectrogram)[0], tf.int32), tf.cast(time_dim, tf.int32)),\n        method=method,\n    )\n\n    # Adjust the resized spectrogram to the original dimensions\n    if time_dim > tf.cast(tf.shape(spectrogram)[1], tf.float32):\n        # Crop the spectrogram to the original dimensions\n        cropped_spectrogram = resized_spectrogram[:, : tf.shape(spectrogram)[1]]\n    else:\n        # Pad the spectrogram to the original dimensions\n        pad_width = [(0, 0), (0, tf.cast(tf.shape(spectrogram)[1], tf.int32) - tf.cast(time_dim, tf.int32))]\n        padded_spectrogram = tf.pad(resized_spectrogram, pad_width, mode=\"CONSTANT\")\n        cropped_spectrogram = padded_spectrogram[:, : tf.shape(spectrogram)[1]]\n\n    return cropped_spectrogram\n\n", "CodeBleu_score": 42.46064851324085}, {"code": "def time_stretch(\n    spectrogram: tf.Tensor,\n    factor: float = 1.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n    if factor == 1.0:\n        return spectrogram\n\n    if factor < 1.0:\n        raise ValueError(\"Time-stretching factor must be greater than 1.0\")\n\n    original_shape = tf.shape(spectrogram)\n    new_time_dim = tf.cast(original_shape[0] * factor, tf.int32)\n    new_spectrogram = tf.image.resize(\n        spectrogram,\n        [new_time_dim, original_shape[1]],\n        method=method,\n    )\n\n    if factor > 1.0:\n        # Crop the spectrogram to the original dimensions\n        start = tf.cast((new_time_dim - original_shape[0]) / 2, tf.int32)\n        end = start + original_shape[0]\n        new_spectrogram = new_spectrogram[start:end, :]\n\n    # Pad the spectrogram to the original dimensions\n    pad_width = [\n        (0, 0),\n        (0, 0),\n        (0, 0),\n        (0, original_shape[2] - tf.shape(new_spectrogram)[-1]),\n    ]\n    new_spectrogram = tf.pad(new_spectrogram, pad_width, mode=\"CONSTANT\")\n\n    return new_spectrogram\n\n", "CodeBleu_score": 38.176755050635826}, {"code": "def time_stretch(\n    spectrogram: tf.Tensor,\n    factor: float = 1.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n    shape = tf.shape(spectrogram)\n    time_dim = shape[1]\n    new_time_dim = tf.cast(time_dim * factor, tf.int32)\n    new_shape = tf.concat(\n        [\n            shape[:1],\n            [new_time_dim],\n            shape[2:],\n        ],\n        axis=0,\n    )\n    return tf.image.resize(\n        spectrogram,\n        new_shape[1:],\n        method=method,\n    )[..., :time_dim]\n\n", "CodeBleu_score": 31.323191734977833}]}
{"instance_id": "6dd01101d4ae", "generation_result": [{"code": "def pitch_shift(\n    spectrogram: tf.Tensor,\n    semitone_shift: float = 0.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n    # Calculate the resizing factor based on the semitone shift\n    resizing_factor = 2 ** (semitone_shift / 12)\n\n    # Resize the spectrogram along the frequency axis using the specified method\n    resized_spectrogram = tf.image.resize(\n        spectrogram,\n        tf.cast(tf.shape(spectrogram)[1] * resizing_factor, tf.int32),\n        method=method,\n    )\n\n    # Pad the resized spectrogram to maintain the original dimensions\n    padded_spectrogram = tf.pad(\n        resized_spectrogram,\n        [[0, 0], [0, tf.shape(spectrogram)[1] - tf.shape(resized_spectrogram)[1]]],\n    )\n\n    return padded_spectrogram\n\n", "CodeBleu_score": 34.87044346771958}, {"code": "def pitch_shift(\n    spectrogram: tf.Tensor,\n    semitone_shift: float = 0.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n    # Calculate the resizing factor based on the semitone shift\n    resizing_factor = 2 ** (semitone_shift / 12)\n\n    # Resize the spectrogram along the frequency axis using the specified method\n    resized_spectrogram = tf.image.resize(\n        spectrogram,\n        (tf.shape(spectrogram)[0], tf.cast(tf.shape(spectrogram)[1] * resizing_factor, tf.int32)),\n        method=method,\n    )\n\n    # Pad the resized spectrogram to maintain the original dimensions\n    padded_spectrogram = tf.pad(\n        resized_spectrogram,\n        [[0, 0], [0, tf.shape(spectrogram)[1] - tf.shape(resized_spectrogram)[1]]],\n        mode=\"CONSTANT\",\n    )\n\n    return padded_spectrogram\n\n", "CodeBleu_score": 34.74901746354257}, {"code": "def pitch_shift(\n    spectrogram: tf.Tensor,\n    semitone_shift: float = 0.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n    # Calculate the resizing factor based on the semitone shift\n    resizing_factor = 2 ** (semitone_shift / 12)\n\n    # Resize the spectrogram along the frequency axis using the specified method\n    resized_spectrogram = tf.image.resize(\n        spectrogram,\n        (spectrogram.shape[0], int(spectrogram.shape[1] * resizing_factor)),\n        method=method,\n    )\n\n    # Pad the resized spectrogram to maintain the original dimensions\n    padded_spectrogram = tf.pad(\n        resized_spectrogram,\n        [[0, 0], [0, spectrogram.shape[1] - resized_spectrogram.shape[1]]],\n        mode=\"CONSTANT\",\n    )\n\n    return padded_spectrogram\n\n", "CodeBleu_score": 30.499898386493896}]}
{"instance_id": "612f0826e56a", "generation_result": [{"code": "def _get_conv_activation_layer(params: Dict) -> Any\n\t\n    if params[\"conv_activation\"] == \"relu\":\n        return ReLU()\n    elif params[\"conv_activation\"] == \"elu\":\n        return ELU()\n    elif params[\"conv_activation\"] == \"leaky_relu\":\n        return LeakyReLU(alpha=0.2)\n    else:\n        raise ValueError(\n            f\"Invalid value for conv_activation: {params['conv_activation']}. Please use one of the following values: 'relu', 'elu', 'leaky_relu'.\"\n        )\n\n", "CodeBleu_score": 33.101312524316576}, {"code": "def _get_conv_activation_layer(params: Dict) -> Any\n\t\n    conv_activation = params.get(\"conv_activation\", \"relu\")\n    if conv_activation == \"relu\":\n        return ReLU()\n    elif conv_activation == \"elu\":\n        return ELU()\n    elif conv_activation == \"leakyrelu\":\n        return LeakyReLU(alpha=0.2)\n    else:\n        logging.warning(\n            f\"Invalid conv_activation value '{conv_activation}' provided. Using ReLU as the default activation layer.\"\n        )\n        return ReLU()\n\n", "CodeBleu_score": 41.42708923666622}, {"code": "def _get_conv_activation_layer(params: Dict) -> Any\n\t\n    if params[\"conv_activation\"] == \"relu\":\n        return ReLU\n    elif params[\"conv_activation\"] == \"elu\":\n        return ELU\n    elif params[\"conv_activation\"] == \"leakyrelu\":\n        return partial(LeakyReLU, alpha=0.2)\n    else:\n        raise ValueError(\n            f\"Invalid value for 'conv_activation': {params['conv_activation']}\"\n        )\n\n", "CodeBleu_score": 26.397996431869625}]}
{"instance_id": "2f8b5a2fc0f2", "generation_result": [{"code": "def softmax_unet(\n    input_tensor: tf.Tensor, instruments: Iterable[str], params: Dict = {}\n) -> Dict\n\t\n    logit_masks = []\n    for instrument in instruments:\n        logit_mask = unet_model(\n            input_tensor,\n            params[\"unet_params\"][instrument],\n            params[\"unet_params\"][\"input_shape\"],\n        )\n        logit_masks.append(logit_mask)\n    logit_masks = tf.stack(logit_masks, axis=-1)\n    logit_masks = tf.nn.softmax(logit_masks, axis=-1)\n    output = {}\n    for i, instrument in enumerate(instruments):\n        output[instrument] = Multiply()([input_tensor, logit_masks[:, :, :, i : i + 1]])\n    return output\n\n", "CodeBleu_score": 35.79338824042268}, {"code": "def softmax_unet(\n    input_tensor: tf.Tensor, instruments: Iterable[str], params: Dict = {}\n) -> Dict\n\t\n\n    # Iterate over instruments and apply U-Net model\n    logit_masks = {}\n    for instrument in instruments:\n        logit_masks[instrument] = unet(\n            input_tensor,\n            params[\"unet_filters\"],\n            params[\"unet_kernel_size\"],\n            params[\"unet_activation\"],\n            params[\"unet_dropout\"],\n        )\n\n    # Combine logit masks using softmax\n    softmax_masks = {}\n    for instrument in instruments:\n        softmax_masks[instrument] = Softmax(axis=-1)(logit_masks[instrument])\n\n    # Multiply input tensor with softmax masks to obtain spectrograms\n    spectrograms = {}\n    for instrument in instruments:\n        spectrograms[instrument] = Multiply()([input_tensor, softmax_masks[instrument]])\n\n    return spectrograms\n\n", "CodeBleu_score": 34.68024277059086}, {"code": "def softmax_unet(\n    input_tensor: tf.Tensor, instruments: Iterable[str], params: Dict = {}\n) -> Dict\n\t\n    logit_masks = []\n    for instrument in instruments:\n        logit_mask = unet(\n            input_tensor,\n            filters=params.get(\"filters\", 32),\n            kernel_size=params.get(\"kernel_size\", 3),\n            strides=params.get(\"strides\", 1),\n            padding=params.get(\"padding\", \"same\"),\n            activation=params.get(\"activation\", \"relu\"),\n            kernel_initializer=params.get(\"kernel_initializer\", he_uniform()),\n            dropout_rate=params.get(\"dropout_rate\", 0.5),\n            name=instrument,\n        )\n        logit_masks.append(logit_mask)\n\n    logit_masks = Concatenate(axis=-1)(logit_masks)\n    logit_masks = Softmax(axis=-1)(logit_masks)\n\n    output_dict = {}\n    for i, instrument in enumerate(instruments):\n        output_dict[instrument] = Multiply()([input_tensor, logit_masks[:, :, :, i]])\n\n    return output_dict\n\n", "CodeBleu_score": 35.392988175062754}]}
{"instance_id": "400b19ce8520", "generation_result": [{"code": "def compute_file_checksum(path)\n\t\n    sha256 = hashlib.sha256()\n    with open(path, \"rb\") as f:\n        while True:\n            data = f.read(65536)\n            if not data:\n                break\n            sha256.update(data)\n    return sha256.hexdigest()\n\n", "CodeBleu_score": 42.60781395547664}, {"code": "def compute_file_checksum(path)\n\t\n    sha256 = hashlib.sha256()\n    with open(path, \"rb\") as f:\n        for chunk in iter(lambda: f.read(4096), b\"\"):\n            sha256.update(chunk)\n    return sha256.hexdigest()\n\n", "CodeBleu_score": 64.59611031781915}, {"code": "def compute_file_checksum(path)\n\t\n    sha256 = hashlib.sha256()\n    with open(path, \"rb\") as f:\n        for chunk in iter(lambda: f.read(4096), b\"\"):\n            sha256.update(chunk)\n    return sha256.hexdigest()\n\n", "CodeBleu_score": 64.59611031781915}]}
{"instance_id": "5f2e9c6e15f1", "generation_result": [{"code": "def checksum(self, name: str) -> str\n\t\n    host = environ[\"REPO_HOST\"]\n    repository = environ[\"REPO_REPOSITORY\"]\n    release_path = environ[\"REPO_RELEASE_PATH\"]\n    checksum_index = environ[\"REPO_CHECKSUM_INDEX\"]\n\n    url = f\"{host}/{repository}/{release_path}/{checksum_index}\"\n    response = httpx.get(url)\n    response.raise_for_status()\n\n    index = response.json()\n    if name not in index:\n        raise ValueError(f\"Model {name} not found in index\")\n\n    return index[name]\n\n", "CodeBleu_score": 39.55829830019072}, {"code": "def checksum(self, name: str) -> str\n\t\n    host = environ.get(\"HOST\", \"https://api.huggingface.co\")\n    repository = environ.get(\"REPOSITORY\", \"models\")\n    release_path = environ.get(\"RELEASE_PATH\", \"\")\n    checksum_index = environ.get(\"CHECKSUM_INDEX\", \"checksums.json\")\n    url = f\"{host}/{repository}/{release_path}/{checksum_index}\"\n    response = httpx.get(url)\n    if response.status_code != 200:\n        raise ValueError(f\"Failed to retrieve checksum index: {response.text}\")\n    index = response.json()\n    if name not in index:\n        raise ValueError(f\"Model {name} not found in index\")\n    return index[name]\n\n", "CodeBleu_score": 39.070713829181344}, {"code": "def checksum(self, name: str) -> str\n\t\n    url = f\"{self.host}/{self.repository}/{self.release_path}/{self.checksum_index}\"\n    response = httpx.get(url)\n    response.raise_for_status()\n    index = response.json()\n    if name not in index:\n        raise ValueError(f\"Model {name} not found in index\")\n    return index[name]\n\n", "CodeBleu_score": 36.418427243737774}]}
{"instance_id": "9312c8b6032e", "generation_result": [{"code": "def download(self, name: str, path: str) -> None:\n\t\n    url = f\"{self.url}/{name}\"\n    self.logger.info(f\"Downloading {name} from {url}\")\n    with httpx.stream(\"GET\", url, headers=self.headers) as r:\n        r.raise_for_status()\n        with NamedTemporaryFile(mode=\"wb\", delete=False) as f:\n            for chunk in r.iter_bytes():\n                f.write(chunk)\n            f.flush()\n            f.seek(0)\n            self.logger.info(f\"Downloaded {name} to {f.name}\")\n            with tarfile.open(fileobj=f, mode=\"r:gz\") as tar:\n                tar.extractall(path=path)\n            os.remove(f.name)\n            self.logger.info(f\"Extracted {name} to {path}\")\n\n", "CodeBleu_score": 28.13918449635916}, {"code": "def download(self, name: str, path: str) -> None:\n\t\n    url = f\"{self.url}/{name}.tar.gz\"\n    self.logger.info(f\"Downloading {name} from {url}\")\n    with NamedTemporaryFile() as f:\n        with httpx.stream(\"GET\", url, follow_redirects=True) as response:\n            response.raise_for_status()\n            for chunk in response.iter_bytes():\n                f.write(chunk)\n        f.flush()\n        f.seek(0)\n        if self.checksum:\n            if self.checksum_type == \"sha256\":\n                checksum = hashlib.sha256(f.read()).hexdigest()\n            else:\n                raise ValueError(f\"Unsupported checksum type: {self.checksum_type}\")\n            if checksum != self.checksum:\n                raise ValueError(f\"Checksum mismatch: {checksum} != {self.checksum}\")\n        with tarfile.open(fileobj=f, mode=\"r:gz\") as tar:\n            tar.extractall(path)\n        f.close()\n        self.logger.info(f\"Extracted {name} to {path}\")\n\n", "CodeBleu_score": 35.76902388362748}, {"code": "def download(self, name: str, path: str) -> None:\n\t\n    url = f\"{environ['MODEL_REPO']}/{name}\"\n    self.logger.info(f\"Downloading model from {url}\")\n    with httpx.stream(\"GET\", url) as r:\n        with NamedTemporaryFile() as f:\n            for chunk in r.iter_bytes():\n                f.write(chunk)\n            f.flush()\n            self.logger.info(f\"Downloaded model to {f.name}\")\n            self.logger.info(f\"Validating checksum of {f.name}\")\n            f.seek(0)\n            sha256 = hashlib.sha256()\n            while True:\n                chunk = f.read(1024)\n                if not chunk:\n                    break\n                sha256.update(chunk)\n            if sha256.hexdigest() != name.split(\".\")[0]:\n                raise ValueError(\"Invalid checksum\")\n            self.logger.info(f\"Extracting model to {path}\")\n            with tarfile.open(f.name, \"r:gz\") as tar:\n                tar.extractall(path)\n            os.remove(f.name)\n            self.logger.info(f\"Extracted model to {path}\")\n\n", "CodeBleu_score": 39.0430276163743}]}
{"instance_id": "ee9493f37d77", "generation_result": [{"code": "def load_configuration(descriptor: str) -> Dict\n\t\n    if descriptor.startswith(\"embedded:\"):\n        name = descriptor.replace(\"embedded:\", \"\")\n        if name == \"\":\n            raise Exception(\"Missing embedded configuration name.\")\n        with open(f\"{__file__.replace('config.py', '')}configs/{name}.json\") as f:\n            return json.load(f)\n    elif exists(descriptor):\n        with open(descriptor) as f:\n            return json.load(f)\n    else:\n        raise Exception(f\"Configuration not found: {descriptor}\")\n\n", "CodeBleu_score": 47.254063394596}, {"code": "def load_configuration(descriptor: str) -> Dict\n\t\n    if descriptor.startswith(\"embedded:\"):\n        name = descriptor.replace(\"embedded:\", \"\")\n        with open(f\"configurations/{name}.json\", \"r\") as file:\n            return json.load(file)\n    elif exists(descriptor):\n        with open(descriptor, \"r\") as file:\n            return json.load(file)\n    else:\n        raise Exception(f\"Configuration file '{descriptor}' not found\")", "CodeBleu_score": 48.86942680184196}, {"code": "def load_configuration(descriptor: str) -> Dict\n\t\n    if descriptor.startswith(\"embedded:\"):\n        name = descriptor[len(\"embedded:\") :]\n        if name == \"default\":\n            import config.default_config as default_config\n\n            return default_config.config\n        elif name == \"dev\":\n            import config.dev_config as dev_config\n\n            return dev_config.config\n        elif name == \"prod\":\n            import config.prod_config as prod_config\n\n            return prod_config.config\n        else:\n            raise ValueError(f\"Unknown embedded configuration: {name}\")\n    else:\n        if not exists(descriptor):\n            raise FileNotFoundError(f\"Configuration file not found: {descriptor}\")\n        with open(descriptor, \"r\") as f:\n            return json.load(f)", "CodeBleu_score": 36.56298373054247}]}
{"instance_id": "546134b67fbc", "generation_result": [{"code": "def sync_apply(\n    tensor_dict: Dict[str, tf.Tensor], func: Callable, concat_axis: int = 1\n) -> Dict[str, tf.Tensor]\n\t\n    if concat_axis == 0:\n        concat_tensor = tf.concat(list(tensor_dict.values()), axis=0)\n        processed_tensor = func(concat_tensor)\n        processed_dict = {\n            key: value\n            for key, value in zip(\n                tensor_dict.keys(),\n                tf.split(processed_tensor, len(tensor_dict), axis=0),\n            )\n        }\n    elif concat_axis == 1:\n        concat_tensor = tf.concat(list(tensor_dict.values()), axis=1)\n        processed_tensor = func(concat_tensor)\n        processed_dict = {\n            key: value\n            for key, value in zip(\n                tensor_dict.keys(),\n                tf.split(processed_tensor, len(tensor_dict), axis=1),\n            )\n        }\n    else:\n        raise ValueError(\n            \"concat_axis must be either 0 or 1, but got {}\".format(concat_axis)\n        )\n    return processed_dict\n\n", "CodeBleu_score": 44.16618947443227}, {"code": "def sync_apply(\n    tensor_dict: Dict[str, tf.Tensor], func: Callable, concat_axis: int = 1\n) -> Dict[str, tf.Tensor]\n\t\n    if concat_axis not in [0, 1]:\n        raise ValueError(\"concat_axis must be either 0 or 1\")\n\n    # Concatenate tensors along the specified axis\n    concatenated_tensor = tf.concat(\n        [tensor for tensor in tensor_dict.values()], axis=concat_axis\n    )\n\n    # Apply the function to the concatenated tensor\n    processed_tensor = func(concatenated_tensor)\n\n    # Split the processed tensor back into the original dictionary format\n    processed_tensor_dict = {}\n    start = 0\n    for key, tensor in tensor_dict.items():\n        end = start + tensor.shape[concat_axis]\n        processed_tensor_dict[key] = processed_tensor[:, start:end]\n        start = end\n\n    return processed_tensor_dict", "CodeBleu_score": 38.772820690496324}, {"code": "def sync_apply(\n    tensor_dict: Dict[str, tf.Tensor], func: Callable, concat_axis: int = 1\n) -> Dict[str, tf.Tensor]\n\t\n    if concat_axis not in [0, 1]:\n        raise ValueError(\"Concatenation axis must be either 0 or 1.\")\n    if concat_axis == 0:\n        tensor_list = [tensor_dict[key] for key in tensor_dict]\n        concat_tensor = tf.concat(tensor_list, axis=0)\n        processed_tensor = func(concat_tensor)\n        return {\n            key: processed_tensor[i * tensor_dict[key].shape[0] : (i + 1) * tensor_dict[key].shape[0]]\n            for i, key in enumerate(tensor_dict)\n        }\n    elif concat_axis == 1:\n        tensor_list = [tensor_dict[key] for key in tensor_dict]\n        concat_tensor = tf.concat(tensor_list, axis=1)\n        processed_tensor = func(concat_tensor)\n        return {\n            key: processed_tensor[:, i * tensor_dict[key].shape[1] : (i + 1) * tensor_dict[key].shape[1]]\n            for i, key in enumerate(tensor_dict)\n        }\n\n", "CodeBleu_score": 46.60557295328296}]}
{"instance_id": "df48e78d46d2", "generation_result": [{"code": "def from_float32_to_uint8(\n    tensor: tf.Tensor,\n    tensor_key: str = \"tensor\",\n    min_key: str = \"min\",\n    max_key: str = \"max\",\n) -> tf.Tensor\n\t\n    min_value = tf.reduce_min(tensor)\n    max_value = tf.reduce_max(tensor)\n    scaled_tensor = (tensor - min_value) / (max_value - min_value) * 255.0\n    return {tensor_key: scaled_tensor, min_key: min_value, max_key: max_value}\n\n", "CodeBleu_score": 42.70356431070133}, {"code": "def from_float32_to_uint8(\n    tensor: tf.Tensor,\n    tensor_key: str = \"tensor\",\n    min_key: str = \"min\",\n    max_key: str = \"max\",\n) -> tf.Tensor\n\t\n    min_value = tf.reduce_min(tensor)\n    max_value = tf.reduce_max(tensor)\n    tensor = tf.cast(tensor, tf.float32)\n    tensor = (tensor - min_value) / (max_value - min_value)\n    tensor = tf.cast(tensor * 255.0, tf.uint8)\n    return {tensor_key: tensor, min_key: min_value, max_key: max_value}\n\n", "CodeBleu_score": 49.28269097474641}, {"code": "def from_float32_to_uint8(\n    tensor: tf.Tensor,\n    tensor_key: str = \"tensor\",\n    min_key: str = \"min\",\n    max_key: str = \"max\",\n) -> tf.Tensor\n\t\n    min_value = tf.reduce_min(tensor)\n    max_value = tf.reduce_max(tensor)\n    scaled_tensor = (tensor - min_value) / (max_value - min_value) * 255\n    return {tensor_key: scaled_tensor, min_key: min_value, max_key: max_value}\n\n", "CodeBleu_score": 42.59127931151635}]}
{"instance_id": "7f479e5de492", "generation_result": [{"code": "def pad_and_partition(tensor: tf.Tensor, segment_len: int) -> tf.Tensor\n\t\n    tensor_len = tf.shape(tensor)[0]\n    padding_len = tf.math.floormod(tensor_len, segment_len)\n    if padding_len > 0:\n        padding_len = segment_len - padding_len\n        padding = tf.zeros([padding_len], dtype=tensor.dtype)\n        tensor = tf.concat([tensor, padding], axis=0)\n    tensor_segments = tf.reshape(tensor, [-1, segment_len])\n    return tensor_segments\n\n", "CodeBleu_score": 36.77098985430785}, {"code": "def pad_and_partition(tensor: tf.Tensor, segment_len: int) -> tf.Tensor\n\t\n    padding_len = segment_len - (tensor.shape[0] % segment_len)\n    if padding_len == segment_len:\n        padding_len = 0\n    padded_tensor = tf.pad(tensor, [[0, padding_len]])\n    return tf.reshape(padded_tensor, [-1, segment_len])\n\n", "CodeBleu_score": 24.13890182601429}, {"code": "def pad_and_partition(tensor: tf.Tensor, segment_len: int) -> tf.Tensor\n\t\n    tensor_length = tf.shape(tensor)[0]\n    padding_length = segment_len - tensor_length % segment_len\n    padded_tensor = tf.pad(tensor, [[0, padding_length], [0, 0]])\n    return tf.reshape(padded_tensor, [-1, segment_len, tensor.shape[1]])\n\n", "CodeBleu_score": 22.477967716442915}]}
{"instance_id": "1789fae7253f", "generation_result": [{"code": "def pad_and_reshape(instr_spec, frame_length, F) -> Any\n\t\n    # Calculate the number of extra rows needed for padding\n    num_extra_rows = (frame_length - instr_spec.shape[0] % frame_length) % frame_length\n\n    # Create an extension of zeros\n    zeros = F.zeros((num_extra_rows, instr_spec.shape[1]))\n\n    # Concatenate the extension to the original spectrogram along the frequency axis\n    extended_spec = F.concatenate([instr_spec, zeros], axis=0)\n\n    # Reshape the extended spectrogram by merging the first two dimensions\n    processed_spec = extended_spec.reshape((-1, instr_spec.shape[1]))\n\n    return processed_spec\n\n", "CodeBleu_score": 17.749227492788762}, {"code": "def pad_and_reshape(instr_spec, frame_length, F) -> Any\n\t\n    # Calculate the number of rows needed for padding\n    pad_rows = (frame_length - (instr_spec.shape[0] % frame_length)) % frame_length\n\n    # Create an extension of zeros for padding\n    pad = F.zeros((pad_rows, instr_spec.shape[1]))\n\n    # Concatenate the extension to the original spectrogram along the frequency axis\n    instr_spec = F.concatenate([instr_spec, pad], axis=0)\n\n    # Reshape the extended spectrogram by merging the first two dimensions\n    instr_spec = instr_spec.reshape(\n        (instr_spec.shape[0] // frame_length, frame_length, instr_spec.shape[1])\n    )\n\n    return instr_spec\n\n", "CodeBleu_score": 17.168484637498768}, {"code": "def pad_and_reshape(instr_spec, frame_length, F) -> Any\n\t\n\n    # Calculate the number of extra rows needed for padding\n    num_extra_rows = frame_length - (instr_spec.shape[0] % frame_length)\n\n    # Create an extension of zeros with the same number of columns as the original spectrogram\n    extension = F.zeros((num_extra_rows, instr_spec.shape[1]))\n\n    # Concatenate the extension to the original spectrogram along the frequency axis\n    padded_spec = F.concatenate((instr_spec, extension), axis=0)\n\n    # Reshape the padded spectrogram by merging the first two dimensions\n    processed_spec = F.reshape(padded_spec, (-1, instr_spec.shape[1]))\n\n    return processed_spec\n\n", "CodeBleu_score": 17.085588620046753}]}
{"instance_id": "8a4f4658d97c", "generation_result": [{"code": "def dataset_from_csv(csv_path: str, **kwargs) -> Any\n\t\n    import pandas as pd\n    import tensorflow as tf\n\n    df = pd.read_csv(csv_path, **kwargs)\n    dataset = tf.data.Dataset.from_tensor_slices(\n        {key: df[key].values for key in df.keys()}\n    )\n    return dataset\n\n", "CodeBleu_score": 48.79159494214289}, {"code": "def dataset_from_csv(csv_path: str, **kwargs) -> Any\n\t\n    import pandas as pd\n    import tensorflow as tf\n\n    df = pd.read_csv(csv_path)\n    dataset = tf.data.Dataset.from_tensor_slices(dict(df))\n    return dataset\n\n", "CodeBleu_score": 25.836439232608967}, {"code": "def dataset_from_csv(csv_path: str, **kwargs) -> Any\n\t\n    import pandas as pd\n    import tensorflow as tf\n\n    df = pd.read_csv(csv_path, **kwargs)\n    ds = tf.data.Dataset.from_tensor_slices(\n        {\n            column: df[column].values\n            for column in df.columns\n        }\n    )\n    return ds\n\n", "CodeBleu_score": 35.21875456905618}]}
{"instance_id": "9a16e81b182f", "generation_result": [{"code": "def check_tensor_shape(tensor_tf: tf.Tensor, target_shape: Any) -> bool\n\t\n    result = True\n    for i, dim in enumerate(target_shape):\n        result = result & (dim == tensor_tf.shape[i].value)\n    return result\n\n", "CodeBleu_score": 26.982463262398415}, {"code": "def check_tensor_shape(tensor_tf: tf.Tensor, target_shape: Any) -> bool\n\t\n    result = True\n    for i, target_length in enumerate(target_shape):\n        result = result and (tensor_tf.shape[i] == target_length)\n    return result\n\n", "CodeBleu_score": 35.15391599320118}, {"code": "def check_tensor_shape(tensor_tf: tf.Tensor, target_shape: Any) -> bool\n\t\n    result = True\n    for target_dim, tensor_dim in zip(target_shape, tensor_tf.shape):\n        if target_dim is not None:\n            result = result and target_dim == tensor_dim\n    return result\n\n", "CodeBleu_score": 27.460968998490586}]}
{"instance_id": "49d5d3084b3a", "generation_result": [{"code": "def get_training_dataset(\n    audio_params: Dict, audio_adapter: AudioAdapter, audio_path: str\n) -> Any\n\t\n    dataset_builder = DatasetBuilder(\n        audio_params=audio_params,\n        audio_adapter=audio_adapter,\n        chunk_duration=audio_params[\"chunk_duration\"],\n        random_seed=audio_params[\"random_seed\"],\n    )\n\n    dataset = dataset_builder.build(\n        train_csv_path=audio_params[\"train_csv_path\"],\n        cache_directory=audio_params[\"cache_directory\"],\n        batch_size=audio_params[\"batch_size\"],\n        num_chunks_per_song=audio_params[\"num_chunks_per_song\"],\n        data_augmentation_options=audio_params[\"data_augmentation_options\"],\n        cache_data=audio_params[\"cache_data\"],\n    )\n\n    return dataset\n\n", "CodeBleu_score": 33.30189954540873}, {"code": "def get_training_dataset(\n    audio_params: Dict, audio_adapter: AudioAdapter, audio_path: str\n) -> Any\n\t\n    builder = DatasetBuilder(\n        audio_params,\n        audio_adapter,\n        audio_path,\n        chunk_duration=audio_params[\"chunk_duration\"],\n        random_seed=audio_params[\"random_seed\"],\n    )\n    dataset = builder.build(\n        train_csv_path=audio_params[\"train_csv_path\"],\n        cache_directory=audio_params[\"cache_directory\"],\n        batch_size=audio_params[\"batch_size\"],\n        num_chunks_per_song=audio_params[\"num_chunks_per_song\"],\n        augment_data=audio_params[\"augment_data\"],\n        cache_data=audio_params[\"cache_data\"],\n    )\n    return dataset\n\n", "CodeBleu_score": 42.708945450043274}, {"code": "def get_training_dataset(\n    audio_params: Dict, audio_adapter: AudioAdapter, audio_path: str\n) -> Any\n\t\n    builder = DatasetBuilder(\n        audio_params=audio_params,\n        audio_adapter=audio_adapter,\n        chunk_duration=audio_params[\"chunk_duration\"],\n        random_seed=audio_params[\"random_seed\"],\n    )\n    dataset = builder.build(\n        train_csv_path=audio_params[\"train_csv_path\"],\n        cache_directory=audio_params[\"cache_directory\"],\n        batch_size=audio_params[\"batch_size\"],\n        num_chunks_per_song=audio_params[\"num_chunks_per_song\"],\n        augment_data=audio_params[\"augment_data\"],\n        cache_data=audio_params[\"cache_data\"],\n    )\n    return dataset\n\n", "CodeBleu_score": 35.10557768965289}]}
{"instance_id": "fe8bbbba06e3", "generation_result": [{"code": "def instruments(self) -> Any\n\t\n    if not self.instrument_builders:\n        for instrument in self.instruments:\n            self.instrument_builders[instrument] = InstrumentDatasetBuilder(\n                self.instrument_dirs[instrument],\n                self.instrument_builders,\n                self.instruments,\n                self.instrument_dirs,\n                self.instrument_builders_dir,\n            )\n    for builder in self.instrument_builders.values():\n        yield builder\n\n", "CodeBleu_score": 39.09080413829671}, {"code": "def instruments(self) -> Any\n\t\n    if self.instrument_builders is None:\n        self.instrument_builders = {}\n        for instrument in self.instruments:\n            self.instrument_builders[instrument] = InstrumentDatasetBuilder(\n                self, instrument\n            )\n    for instrument in self.instruments:\n        yield self.instrument_builders[instrument]\n\n", "CodeBleu_score": 40.29551173240248}, {"code": "def instruments(self) -> Any\n\t\n    self._instrument_builders = []\n    for instrument in self._instruments:\n        self._instrument_builders.append(\n            InstrumentDatasetBuilder(\n                self._data_directory,\n                self._data_directory_path,\n                self._instruments,\n                self._instrument_builders,\n                self._instrument_builders_path,\n                self._instrument_builders_path_path,\n                instrument,\n            )\n        )\n    for builder in self._instrument_builders:\n        yield builder\n\n", "CodeBleu_score": 45.24573841978268}]}
{"instance_id": "438b57fd3413", "generation_result": [{"code": "def cache(self, dataset: Any, cache: Optional[str], wait: bool) -> Any\n\t\n    if cache is not None:\n        if wait:\n            while not exists(f\"{cache}{SEPARATOR}index.json\"):\n                print(f\"Waiting for cache index file at {cache}\")\n                time.sleep(1)\n        os.makedirs(cache, exist_ok=True)\n        return self.cache(dataset, cache)\n    return dataset\n\n", "CodeBleu_score": 50.17180812211149}, {"code": "def cache(self, dataset: Any, cache: Optional[str], wait: bool) -> Any\n\t\n    if cache is None:\n        return dataset\n\n    if not exists(cache):\n        os.makedirs(cache)\n\n    if wait:\n        while not exists(cache + SEPARATOR + \"index.json\"):\n            print(\"waiting for cache index...\")\n            time.sleep(1)\n\n    dataset.save_to_disk(cache)\n\n    return dataset\n\n", "CodeBleu_score": 44.76688778940067}, {"code": "def cache(self, dataset: Any, cache: Optional[str], wait: bool) -> Any\n\t\n    if cache:\n        cache_path = cache + SEPARATOR + dataset.cache_index\n        if not exists(cache_path):\n            if wait:\n                while not exists(cache_path):\n                    print(f\"Waiting for cache index file at {cache_path}\")\n                    time.sleep(1)\n            else:\n                raise FileNotFoundError(f\"Cache index file not found at {cache_path}\")\n        if not exists(cache):\n            os.makedirs(cache)\n        dataset.cache(cache)\n        return dataset\n    return dataset\n\n", "CodeBleu_score": 41.30969710411069}]}
{"instance_id": "8838a3a09a66", "generation_result": [{"code": "def create_estimator(params: Dict, MWF: bool) -> tf.Tensor\n\t\n    model_dir = params['model_dir']\n    model_provider = ModelProvider()\n    model_dir = model_provider.get_model_dir(model_dir)\n    params['model_dir'] = model_dir\n    params['model_provider'] = model_provider\n    params['MWF'] = MWF\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    config.gpu_options.per_process_gpu_memory_fraction = 0.9\n    session_config = tf.ConfigProto(\n        device_count={'CPU': 1, 'GPU': 1},\n        intra_op_parallelism_threads=1,\n        inter_op_parallelism_threads=1,\n        allow_soft_placement=True,\n        log_device_placement=False,\n        gpu_options=config.gpu_options\n    )\n    run_config = tf.estimator.RunConfig(\n        model_dir=model_dir,\n        session_config=session_config,\n        save_checkpoints_steps=params['save_checkpoints_steps'],\n        keep_checkpoint_max=params['keep_checkpoint_max']\n    )\n    return tf.estimator.Estimator(\n        model_fn=model_fn,\n        params=params,\n        config=run_config\n    )\n\n", "CodeBleu_score": 29.67502762023114}, {"code": "def create_estimator(params: Dict, MWF: bool) -> tf.Tensor\n\t\n    model_dir = params[\"model_dir\"]\n    model_provider = ModelProvider(model_dir)\n    model_provider.update_params(params)\n    model_dir = model_provider.model_dir\n    if MWF:\n        model_dir = model_provider.model_dir\n    else:\n        model_dir = model_provider.model_dir\n    session_config = tf.ConfigProto(\n        gpu_options=tf.GPUOptions(allow_growth=True, visible_device_list=\"0\")\n    )\n    run_config = tf.estimator.RunConfig(session_config=session_config)\n    estimator = tf.estimator.Estimator(\n        model_fn=model_provider.model_fn,\n        model_dir=model_dir,\n        params=params,\n        config=run_config,\n    )\n    return estimator\n\n", "CodeBleu_score": 46.20882086024412}, {"code": "def create_estimator(params: Dict, MWF: bool) -> tf.Tensor\n\t\n    model_dir = params[\"model_dir\"]\n    model_provider = ModelProvider()\n    model_dir = model_provider.get_model_dir(model_dir)\n    params[\"model_dir\"] = model_dir\n    if MWF:\n        params[\"model_dir\"] = join(model_dir, \"model.ckpt\")\n    config = tf.compat.v1.ConfigProto()\n    config.gpu_options.allow_growth = True\n    config.gpu_options.per_process_gpu_memory_fraction = 0.7\n    session_config = tf.compat.v1.ConfigProto(gpu_options=config)\n    run_config = tf.estimator.RunConfig(session_config=session_config)\n    estimator = tf.estimator.Estimator(\n        model_fn=model_fn, model_dir=params[\"model_dir\"], params=params, config=run_config\n    )\n    return estimator\n\n", "CodeBleu_score": 59.70568632036743}]}
{"instance_id": "779c812bf2cb", "generation_result": [{"code": "def _separate_tensorflow(\n        self, waveform: np.ndarray, audio_descriptor: AudioDescriptor\n    ) -> Dict\n\t\n\n    # Ensure the waveform is stereo\n    waveform = ensure_stereo(waveform)\n\n    # Get a prediction generator\n    prediction_generator = self.get_prediction_generator()\n\n    # Update the data generator with the waveform and audio descriptor\n    self.data_generator.update(waveform, audio_descriptor)\n\n    # Generate a prediction\n    prediction = next(prediction_generator)\n\n    # Remove the \"audio_id\" key from the prediction\n    prediction.pop(\"audio_id\")\n\n    # Return the prediction\n    return prediction\n\n", "CodeBleu_score": 43.84869157252951}, {"code": "def _separate_tensorflow(\n        self, waveform: np.ndarray, audio_descriptor: AudioDescriptor\n    ) -> Dict\n\t\n    waveform = waveform.astype(np.float32)\n    if waveform.shape[0] == 1:\n        waveform = np.repeat(waveform, 2, axis=0)\n    elif waveform.shape[0] != 2:\n        raise ValueError(\"Waveform must be mono or stereo\")\n    if audio_descriptor.sample_rate != 44100:\n        raise ValueError(\"Sample rate must be 44100\")\n    if audio_descriptor.num_channels != 2:\n        raise ValueError(\"Number of channels must be 2\")\n    if audio_descriptor.num_samples != 44100 * 30:\n        raise ValueError(\"Number of samples must be 44100 * 30\")\n    prediction_generator = self.get_prediction_generator()\n    data_generator = self.get_data_generator()\n    data_generator.update(waveform, audio_descriptor)\n    prediction = next(prediction_generator)\n    del prediction[\"audio_id\"]\n    return prediction\n\n", "CodeBleu_score": 45.834982106053594}, {"code": "def _separate_tensorflow(\n        self, waveform: np.ndarray, audio_descriptor: AudioDescriptor\n    ) -> Dict\n\t\n    # Ensure the waveform is stereo\n    waveform = ensure_stereo(waveform)\n    # Get a prediction generator\n    prediction_generator = self.get_prediction_generator()\n    # Update the data generator with the waveform and audio descriptor\n    prediction_generator.update_data_generator(\n        waveform=waveform, audio_descriptor=audio_descriptor\n    )\n    # Generate a prediction\n    prediction = prediction_generator.generate_prediction()\n    # Remove the \"audio_id\" key from the prediction\n    prediction.pop(\"audio_id\")\n    return prediction\n\n", "CodeBleu_score": 41.06883078967816}]}
{"instance_id": "cf580cd2ed93", "generation_result": [{"code": "def compute_backbone_shapes(config, image_shape)\n\t\n    if callable(config.BACKBONE):\n        return config.COMPUTE_BACKBONE_SHAPE(image_shape, config)\n\n    if config.BACKBONE == \"resnet50\":\n        return utils.compute_backbone_shapes(image_shape, config, config.BACKBONE_STRIDES, config.RESNET_LAYER_NAMES)\n    elif config.BACKBONE == \"resnet101\":\n        return utils.compute_backbone_shapes(image_shape, config, config.BACKBONE_STRIDES, config.RESNET_LAYER_NAMES)\n    else:\n        raise NotImplementedError(\"Backbone architecture not implemented: {}\".format(config.BACKBONE))\n\n", "CodeBleu_score": 39.3582366796526}, {"code": "def compute_backbone_shapes(config, image_shape)\n\t\n    if callable(config.BACKBONE):\n        return config.COMPUTE_BACKBONE_SHAPE(image_shape, config)\n\n    # C4 is used in Stage 4 and R5 is used in Stage 5\n    if config.BACKBONE == \"resnet50\":\n        return np.array([\n            [image_shape[0] // 4, image_shape[1] // 4],\n            [image_shape[0] // 8, image_shape[1] // 8],\n            [image_shape[0] // 16, image_shape[1] // 16],\n            [image_shape[0] // 32, image_shape[1] // 32],\n        ])\n\n    elif config.BACKBONE == \"resnet101\":\n        return np.array([\n            [image_shape[0] // 4, image_shape[1] // 4],\n            [image_shape[0] // 8, image_shape[1] // 8],\n            [image_shape[0] // 16, image_shape[1] // 16],\n            [image_shape[0] // 32, image_shape[1] // 32],\n            [image_shape[0] // 64, image_shape[1] // 64],\n        ])\n\n    else:\n        raise ValueError(\"Backbone (\\\"{}\\\") is not implemented.\".format(config.BACKBONE))\n\n", "CodeBleu_score": 36.69171372200474}, {"code": "def compute_backbone_shapes(config, image_shape)\n\t\n    if callable(config.BACKBONE):\n        return config.COMPUTE_BACKBONE_SHAPE(image_shape, config)\n\n    if config.BACKBONE == \"resnet50\":\n        return {\n            \"res2\": (image_shape[0] // 4, image_shape[1] // 4, 256),\n            \"res3\": (image_shape[0] // 8, image_shape[1] // 8, 512),\n            \"res4\": (image_shape[0] // 16, image_shape[1] // 16, 1024),\n            \"res5\": (image_shape[0] // 32, image_shape[1] // 32, 2048),\n        }\n    elif config.BACKBONE == \"resnet101\":\n        return {\n            \"res2\": (image_shape[0] // 4, image_shape[1] // 4, 256),\n            \"res3\": (image_shape[0] // 8, image_shape[1] // 8, 512),\n            \"res4\": (image_shape[0] // 16, image_shape[1] // 16, 1024),\n            \"res5\": (image_shape[0] // 32, image_shape[1] // 32, 2048),\n        }\n    else:\n        raise NotImplementedError(\n            \"Backbone architecture not implemented: {}\".format(config.BACKBONE)\n        )\n\n", "CodeBleu_score": 37.659764486378414}]}
{"instance_id": "ebd61a2e59ca", "generation_result": [{"code": "def clip_boxes_graph(boxes, window)\n\t\n    # split\n    wy1, wx1, wy2, wx2 = tf.split(window, 4)\n    y1, x1, y2, x2 = tf.split(boxes, 4, axis=1)\n    y1 = tf.maximum(tf.minimum(y1, wy2), wy1)\n    x1 = tf.maximum(tf.minimum(x1, wx2), wx1)\n    y2 = tf.maximum(tf.minimum(y2, wy2), wy1)\n    x2 = tf.maximum(tf.minimum(x2, wx2), wx1)\n    clipped = tf.concat([y1, x1, y2, x2], axis=1)\n    clipped.set_shape((clipped.shape[0], 4))\n    return clipped\n\n", "CodeBleu_score": 56.01484333396114}, {"code": "def clip_boxes_graph(boxes, window)\n\t\n    # Split\n    wy1, wx1, wy2, wx2 = tf.split(window, 4)\n    y1, x1, y2, x2 = tf.split(boxes, 4, axis=1)\n    # Clip\n    y1 = tf.maximum(tf.minimum(y1, wy2), wy1)\n    x1 = tf.maximum(tf.minimum(x1, wx2), wx1)\n    y2 = tf.maximum(tf.minimum(y2, wy2), wy1)\n    x2 = tf.maximum(tf.minimum(x2, wx2), wx1)\n    clipped = tf.concat([y1, x1, y2, x2], axis=1)\n    clipped.set_shape((clipped.shape[0], 4))\n    return clipped\n\n", "CodeBleu_score": 56.01484333396114}, {"code": "def clip_boxes_graph(boxes, window)\n\t\n    # Split\n    wy1, wx1, wy2, wx2 = tf.split(window, 4)\n    y1, x1, y2, x2 = tf.split(boxes, 4, axis=1)\n    y1 = tf.clip_by_value(y1, wy1, wy2)\n    y2 = tf.clip_by_value(y2, wy1, wy2)\n    x1 = tf.clip_by_value(x1, wx1, wx2)\n    x2 = tf.clip_by_value(x2, wx1, wx2)\n    # Concat\n    clipped = tf.concat([y1, x1, y2, x2], axis=1)\n    clipped.set_shape((clipped.shape[0], 4))\n    return clipped\n\n", "CodeBleu_score": 40.32718315650187}]}
{"instance_id": "8e0411587a70", "generation_result": [{"code": "def rpn_graph(feature_map, anchors_per_location, anchor_stride)\n\t\n    # Shared convolutional base of the RPN and classifier heads.\n    shared = keras.layers.Conv2D(512, (3, 3), padding='same', activation='relu', kernel_initializer='normal', name='rpn_conv_shared')(feature_map)\n\n    # Anchor Score.\n    x = keras.layers.Conv2D(anchors_per_location * 2, (1, 1), activation='linear', name='rpn_class_raw')(shared)\n    rpn_class_logits = utils.batch_slice([x], lambda x, batch_num: tf.reshape(x, [tf.shape(x)[0], -1, 2]))\n    rpn_class_logits = keras.layers.Lambda(lambda t: tf.transpose(t, (0, 2, 1)), name=\"rpn_class_logits\")(rpn_class_logits)\n    rpn_class_probs = keras.layers.Activation(\"softmax\", name=\"rpn_class_xxx\")(rpn_class_logits)\n\n    # Bounding box refinement.\n    x = keras.layers.Conv2D(anchors_per_location * 4, (1, 1), activation='linear', name='rpn_bbox_pred')(shared)\n    # Axis ordering: (batch, anchors, 4)\n    rpn_bbox = utils.batch_slice([x], lambda x, batch_num: tf.reshape(x, [tf.shape(x)[0], -1, 4]))\n    # Axis ordering: (batch, 4)\n    rpn_bbox = keras.layers.Lambda(lambda x: tf.reshape(x, [-1, 4]), name=\"rpn_bbox\")(rpn_bbox)\n\n    return [rpn_class_logits, rpn_class_probs, rpn_bbox]\n\n", "CodeBleu_score": 54.16939931946588}, {"code": "def rpn_graph(feature_map, anchors_per_location, anchor_stride)\n\t\n    shared = keras.layers.Conv2D(512, (3, 3), padding='same', activation='relu',\n                                 kernel_initializer=keras.initializers.normal(stddev=0.01),\n                                 name='rpn_conv_shared')(feature_map)\n\n    x = keras.layers.Conv2D(anchors_per_location * 4, (1, 1), activation='linear',\n                            kernel_initializer=keras.initializers.normal(stddev=0.01),\n                            name='rpn_class_raw')(shared)\n\n    rpn_class_logits = keras.layers.Reshape((-1, 4), name=\"rpn_class_logits\")(x)\n\n    rpn_probs = keras.layers.Activation(\"softmax\", name=\"rpn_class\")(rpn_class_logits)\n\n    # Bounding box refinement. [batch, num_rois, (dy, dx, log(dh), log(dw))]\n    x = keras.layers.Conv2D(anchors_per_location * 4, (1, 1), activation='linear',\n                            kernel_initializer=keras.initializers.normal(stddev=0.01),\n                            name='rpn_bbox_pred')(shared)\n\n    # Reshape back to the original 3 dimensions: [batch, num_rois, 4]\n    rpn_bbox = keras.layers.Reshape((-1, 4), name=\"rpn_bbox\")(x)\n\n    return [rpn_class_logits, rpn_probs, rpn_bbox]\n\n", "CodeBleu_score": 45.53705817969745}, {"code": "def rpn_graph(feature_map, anchors_per_location, anchor_stride)\n\t\n    # The convolutional layer to apply to the feature map\n    shared = keras.layers.Conv2D(512, (3, 3), padding=\"same\", activation=\"relu\",\n                                 kernel_initializer=keras.initializers.normal(stddev=0.01),\n                                 name=\"rpn_conv_shared\")(feature_map)\n\n    # The convolutional layer to predict class scores\n    class_logits = keras.layers.Conv2D(anchors_per_location * 2, (1, 1), activation=\"linear\",\n                                       kernel_initializer=keras.initializers.zeros(),\n                                       name=\"rpn_class_raw\")(shared)\n\n    # The convolutional layer to predict bounding box deltas\n    bbox_deltas = keras.layers.Conv2D(anchors_per_location * 4, (1, 1), activation=\"linear\",\n                                      kernel_initializer=keras.initializers.zeros(),\n                                      name=\"rpn_bbox_pred\")(shared)\n\n    # Reshape class logits and bounding box deltas\n    class_logits = utils.batch_slice(\n        [class_logits], lambda x, rpn_anchors_per_location: rpn_class_graph(\n            x, rpn_anchors_per_location, anchor_stride),\n        anchors_per_location.shape[0])\n    rpn_class_logits = tf.reshape(\n        class_logits, [tf.shape(class_logits)[0], -1, 2])\n\n    bbox_deltas = utils.batch_slice(\n        [bbox_deltas], lambda x, rpn_anchors_per_location: rpn_bbox_graph(\n            x, rpn_anchors_per_location, anchor_stride),\n        anchors_per_location.shape[0])\n    rpn_probs = tf.nn.softmax(rpn_class_logits)\n    rpn_bbox_deltas = tf.reshape", "CodeBleu_score": 44.509893557122695}]}
{"instance_id": "70777c9261d8", "generation_result": [{"code": "def smooth_l1_loss(y_true, y_pred)\n\t\n    # |x| < 1 => 0.5 * x^2\n    # |x| >= 1 => |x| - 0.5\n    abs_x = keras.backend.abs(y_pred - y_true)\n    mask = keras.backend.less(abs_x, 1.0)\n    loss = keras.backend.switch(mask, 0.5 * keras.backend.square(abs_x), abs_x - 0.5)\n    return loss\n\n", "CodeBleu_score": 23.548754295494895}, {"code": "def smooth_l1_loss(y_true, y_pred)\n\t\n    absolute_difference = keras.backend.abs(y_true - y_pred)\n    squared_difference = keras.backend.square(absolute_difference)\n    small_difference_mask = keras.backend.cast(keras.backend.less(absolute_difference, 1.0), \"float32\")\n    large_difference_mask = keras.backend.cast(keras.backend.greater_equal(absolute_difference, 1.0), \"float32\")\n    small_difference = small_difference_mask * squared_difference * 0.5\n    large_difference = large_difference_mask * (absolute_difference - 0.5)\n    return keras.backend.sum(small_difference + large_difference)\n\n", "CodeBleu_score": 21.780852645886167}, {"code": "def smooth_l1_loss(y_true, y_pred)\n\t\n    if keras.backend.backend() == 'tensorflow':\n        abs_loss = keras.backend.abs(y_true - y_pred)\n        sq_loss = 0.5 * (y_true - y_pred) ** 2\n        loss = tf.where(keras.backend.less(abs_loss, 1.0), sq_loss, abs_loss - 0.5)\n    else:\n        abs_loss = keras.backend.abs(y_true - y_pred)\n        sq_loss = 0.5 * (y_true - y_pred) ** 2\n        loss = keras.backend.switch(keras.backend.less(abs_loss, 1.0), sq_loss, abs_loss - 0.5)\n    return loss\n\n", "CodeBleu_score": 33.194610887586464}]}
{"instance_id": "0e747b604087", "generation_result": [{"code": "def build_model(x_train, num_classes)\n\t\n    # Reset the TensorFlow graph\n    tf.reset_default_graph()\n\n    # Define the input layer\n    x = tf.placeholder(tf.float32, shape=[None, x_train.shape[1], x_train.shape[2], x_train.shape[3]], name='x')\n\n    # Define the first convolutional layer\n    conv1 = tf.layers.conv2d(x, filters=32, kernel_size=[3, 3], padding='same', activation=tf.nn.relu)\n\n    # Define the second convolutional layer\n    conv2 = tf.layers.conv2d(conv1, filters=64, kernel_size=[3, 3], padding='same', activation=tf.nn.relu)\n\n    # Define the max-pooling layer\n    pool = tf.layers.max_pooling2d(conv2, pool_size=[2, 2], strides=2)\n\n    # Flatten the output of the max-pooling layer\n    flatten = tf.reshape(pool, [-1, pool.shape[1] * pool.shape[2] * pool.shape[3]])\n\n    # Define the first dense layer\n    dense1 = tf.layers.dense(flatten, units=128, activation=tf.nn.relu)\n\n    # Define the second dense layer\n    dense2 = tf.layers.dense(dense1, units=num_classes, activation=tf.nn.softmax)\n\n    # Define the output layer\n    output = dense2\n\n    # Define the loss function and optimizer\n    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=output, labels=tf.one_hot(tf.cast(tf.placeholder(tf.int32, shape=[None]), tf.int32), num_classes)))\n    optimizer = tf.train.AdamOptimizer().minimize(loss)\n\n    # Define the accuracy metric\n    correct_prediction = tf.equal(tf.", "CodeBleu_score": 28.28793601492266}, {"code": "def build_model(x_train, num_classes)\n\t\n    model = tf.keras.models.Sequential()\n    model.add(tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=x_train.shape[1:]))\n    model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n    model.add(tf.keras.layers.Conv2D(64, (3, 3), activation='relu'))\n    model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n    model.add(tf.keras.layers.Flatten())\n    model.add(tf.keras.layers.Dense(64, activation='relu'))\n    model.add(tf.keras.layers.Dense(num_classes, activation='softmax'))\n    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n    return model\n\n\n", "CodeBleu_score": 23.501707160587028}, {"code": "def build_model(x_train, num_classes)\n\t\n    # Reset the TensorFlow graph\n    tf.reset_default_graph()\n\n    # Define the input layer\n    x = tf.placeholder(tf.float32, [None, x_train.shape[1], x_train.shape[2], x_train.shape[3]], name='x')\n\n    # Define the first convolutional layer\n    conv1 = tf.layers.conv2d(x, 32, 3, activation=tf.nn.relu)\n\n    # Define the second convolutional layer\n    conv2 = tf.layers.conv2d(conv1, 64, 3, activation=tf.nn.relu)\n\n    # Define the max-pooling layer\n    pool = tf.layers.max_pooling2d(conv2, 2, 2)\n\n    # Flatten the output of the max-pooling layer\n    flatten = tf.layers.flatten(pool)\n\n    # Define the first dense layer\n    dense1 = tf.layers.dense(flatten, 128, activation=tf.nn.relu)\n\n    # Define the second dense layer\n    dense2 = tf.layers.dense(dense1, num_classes, activation=tf.nn.softmax)\n\n    # Define the output layer\n    output = dense2\n\n    # Define the loss function\n    y = tf.placeholder(tf.float32, [None, num_classes], name='y')\n    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=output))\n\n    # Define the optimizer\n    optimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss)\n\n    # Define the accuracy metric\n    correct_prediction = tf.equal(tf.argmax(output, 1), tf.argmax(y, 1))\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\n    # Define the model\n", "CodeBleu_score": 33.63820459611467}]}
{"instance_id": "813e3e2844f4", "generation_result": [{"code": "def extract_bboxes(mask)\n\t\n    bboxes = [[]]\n    # bboxes = [np.zeros((0, 4))]\n    # bboxes = [np.array([[0, 0, 0, 0]])]\n    for i in range(mask.shape[2]):\n        horiz_indices = np.where(np.any(mask[:, :, i], axis=0))\n        vert_indices = np.where(np.any(mask[:, :, i], axis=1))\n        if horiz_indices[0].shape[0]:\n            bbox = np.zeros((1, 4))\n            bbox[0, 0] = np.min(horiz_indices[0])\n            bbox[0, 1] = np.min(vert_indices[0])\n            bbox[0, 2] = np.max(horiz_indices[0])\n            bbox[0, 3] = np.max(vert_indices[0])\n            bboxes.append(bbox)\n        else:\n            bboxes.append(np.zeros((0, 4)))\n    return bboxes\n\n", "CodeBleu_score": 24.766352748238173}, {"code": "def extract_bboxes(mask)\n\t\n    bboxes = [[]]\n    # loop over the number of mask layers\n    for i in range(mask.shape[2]):\n        # find where the mask is for the given layer\n        where = skimage.morphology.where(mask[:, :, i])\n        # get coordinates of where mask is 1\n        y1, x1, y2, x2 = where[:, 0].min(), where[:, 1].min(), where[:, 0].max() + 1, where[:, 1].max() + 1\n        # catch any out of bounds below 0 or above mask_size in y\n        y1 = max(0, y1)\n        y2 = min(mask.shape[0], y2)\n        x1 = max(0, x1)\n        x2 = min(mask.shape[1], x2)\n        if y1 != y2 and x1 != x2:  # if it doesn't create a fake bounding box\n            bboxes[0].append([y1, x1, y2, x2])\n    return bboxes\n\n", "CodeBleu_score": 21.061651333422375}, {"code": "def extract_bboxes(mask)\n\t\n    boxes = np.zeros([mask.shape[-1], 4], dtype=np.int32)\n    for i in range(mask.shape[-1]):\n        m = mask[:, :, i]\n        horizontal_indicies = np.where(np.any(m, axis=0))[0]\n        vertical_indicies = np.where(np.any(m, axis=1))[0]\n        if horizontal_indicies.shape[0]:\n            x1, x2 = horizontal_indicies[[0, -1]]\n            y1, y2 = vertical_indicies[[0, -1]]\n            x2 += 1\n            y2 += 1\n        else:\n            x1, x2, y1, y2 = 0, 0, 0, 0\n        boxes[i] = [x1, y1, x2, y2]\n    return boxes.astype(np.int32)\n\n", "CodeBleu_score": 74.72572698193562}]}
{"instance_id": "7ad5d430d6ed", "generation_result": [{"code": "def compute_iou(box, boxes, box_area, boxes_area)\n\t\n    # Calculate the intersection areas\n    y1 = np.maximum(box[0], boxes[:, 0])\n    y2 = np.minimum(box[2], boxes[:, 2])\n    x1 = np.maximum(box[1], boxes[:, 1])\n    x2 = np.minimum(box[3], boxes[:, 3])\n    intersection = np.maximum(x2 - x1, 0) * np.maximum(y2 - y1, 0)\n    # Compute the union areas\n    union = box_area + boxes_area[:] - intersection[:]\n    # Compute the IoU\n    iou = intersection / union\n    return iou\n\n", "CodeBleu_score": 58.663259083089194}, {"code": "def compute_iou(box, boxes, box_area, boxes_area)\n\t\n\n    y1 = np.maximum(box[0], boxes[:, 0])\n    y2 = np.minimum(box[2], boxes[:, 2])\n    x1 = np.maximum(box[1], boxes[:, 1])\n    x2 = np.minimum(box[3], boxes[:, 3])\n    intersection = np.maximum(x2 - x1, 0) * np.maximum(y2 - y1, 0)\n    union = box_area + boxes_area[:] - intersection[:]\n    iou = intersection / union\n    return iou\n\n", "CodeBleu_score": 58.663259083089194}, {"code": "def compute_iou(box, boxes, box_area, boxes_area)\n\t\n\n    # Calculate intersection areas\n    y1 = np.maximum(box[0], boxes[:, 0])\n    y2 = np.minimum(box[2], boxes[:, 2])\n    x1 = np.maximum(box[1], boxes[:, 1])\n    x2 = np.minimum(box[3], boxes[:, 3])\n    intersection = np.maximum(x2 - x1, 0) * np.maximum(y2 - y1, 0)\n\n    # Calculate union areas\n    union = box_area + boxes_area[:] - intersection[:]\n\n    # Compute IoU values\n    iou = intersection / union\n\n    return iou\n\n", "CodeBleu_score": 58.663259083089194}]}
{"instance_id": "15941b852859", "generation_result": [{"code": "def compute_overlaps(boxes1, boxes2)\n\t\n    # If there are no boxes in both sets, the result is trivial.\n    if boxes1.shape[0] == 0 or boxes2.shape[0] == 0:\n        return np.zeros((boxes1.shape[0], boxes2.shape[0]))\n\n    # Compute intersections\n    ixmin = np.maximum(boxes1[:, 0], boxes2[:, 0])\n    iymin = np.maximum(boxes1[:, 1], boxes2[:, 1])\n    ixmax = np.minimum(boxes1[:, 2], boxes2[:, 2])\n    iymax = np.minimum(boxes1[:, 3], boxes2[:, 3])\n    iw = np.maximum(ixmax - ixmin, 0.)\n    ih = np.maximum(iymax - iymin, 0.)\n    inters = iw * ih\n\n    # Compute unions\n    uni = ((boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1]) +\n           (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1]) -\n           inters)\n\n    # Compute IoU\n    overlaps = inters / uni\n\n    return overlaps\n\n", "CodeBleu_score": 49.68883368750151}, {"code": "def compute_overlaps(boxes1, boxes2)\n\t\n    # If there are no boxes in either set of boxes return the empty matrix.\n    if len(boxes1) == 0 or len(boxes2) == 0:\n        return np.zeros((0, len(boxes1)), dtype=np.float32)\n\n    # Compute overlaps to generate matrix [boxes1 count, boxes2 count]\n    # Each cell contains the IoU value.\n    iou_matrix = np.zeros(\n        (boxes1.shape[0], boxes2.shape[0]), dtype=np.float32)\n    for i in range(iou_matrix.shape[1]):\n        iou_matrix[:, i] = compute_iou(boxes1, boxes2[i])\n    return iou_matrix\n\n", "CodeBleu_score": 31.102083332454278}, {"code": "def compute_overlaps(boxes1, boxes2)\n\t\n    warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"skimage\")\n\n    if boxes1.shape[1] != 4 or boxes2.shape[1] != 4:\n        raise IndexError(\"Boxes should be 2D arrays with shape: (N,4)\")\n    if not len(boxes1):\n        return np.zeros((0, len(boxes2)), dtype=np.float32)\n    if not len(boxes2):\n        return np.zeros((len(boxes1), 0), dtype=np.float32)\n\n    boxes1 = np.array(boxes1)\n    boxes2 = np.array(boxes2)\n\n    # Compute intersections\n    intersections = np.zeros((boxes1.shape[0], boxes2.shape[0]))\n    for i in range(boxes1.shape[0]):\n        for j in range(boxes2.shape[0]):\n            intersections[i, j] = intersection(boxes1[i], boxes2[j])\n\n    # Compute unions\n    unions = np.zeros((boxes1.shape[0], boxes2.shape[0]))\n    for i in range(boxes1.shape[0]):\n        for j in range(boxes2.shape[0]):\n            unions[i, j] = union(boxes1[i], boxes2[j])\n\n    overlaps = intersections / unions\n\n    return overlaps\n\n", "CodeBleu_score": 26.48697227418718}]}
{"instance_id": "597533722ce3", "generation_result": [{"code": "def load_image(self, image_id):\n\t\n    image_path = self.image_info[image_id]['path']\n    image = skimage.io.imread(image_path)\n    if image.ndim != 3:\n        image = image[..., np.newaxis]\n    image = image.astype(np.float32)\n    if image.shape[-1] == 4:\n        image = image[..., :3]\n    return image\n\n", "CodeBleu_score": 74.20755468951448}, {"code": "def load_image(self, image_id):\n\t\n    image_path = self.image_info[image_id]['path']\n    image = skimage.io.imread(image_path)\n    if image.ndim != 3:\n        image = image[..., :3]\n    return image\n\n", "CodeBleu_score": 60.3736848751866}, {"code": "def load_image(self, image_id):\n\t\n    image = skimage.io.imread(os.path.join(self.image_path, image_id))\n    if image.ndim == 2:\n        image = image[:, :, np.newaxis]\n        image = np.concatenate((image, image, image), axis=2)\n    elif image.shape[2] == 4:\n        image = image[:, :, :3]\n    return image\n\n", "CodeBleu_score": 49.56674395214614}]}
{"instance_id": "9b852997ca05", "generation_result": [{"code": "def display_images(images, titles=None, cols=4, cmap=None, norm=None,\n                   interpolation=None)\n\t\n    if titles is None:\n        titles = ['Image (%d)' % i for i in range(1, len(images) + 1)]\n    rows = len(images) // cols + 1\n    if rows * cols < len(images):\n        rows += 1\n    if cmap is None:\n        cmap = 'gray' if images[0].ndim == 2 else None\n    if norm is None:\n        norm = None if images[0].ndim == 2 else 'linear'\n    if interpolation is None:\n        interpolation = 'nearest' if images[0].ndim == 2 else 'bilinear'\n    fig = plt.figure(figsize=(16, 12))\n    for n, (image, title) in enumerate(zip(images, titles)):\n        ax = fig.add_subplot(rows, cols, n + 1)\n        if image.ndim == 2:\n            plt.gray()\n        plt.imshow(image, cmap=cmap, norm=norm, interpolation=interpolation)\n        plt.axis('off')\n        ax.set_title(title)\n    plt.show()\n\n", "CodeBleu_score": 43.055093811728554}, {"code": "def display_images(images, titles=None, cols=4, cmap=None, norm=None,\n                   interpolation=None)\n\t\n    if type(images[0]) == np.ndarray:\n        images = np.array(images).astype(np.uint8)\n    n_images = len(images)\n    if titles is None:\n        titles = ['Image (%d)' % i for i in range(1, n_images + 1)]\n    figure, axs = plt.subplots(nrows=1, ncols=n_images, squeeze=False)\n    figure.set_size_inches(\n        n_images * 4.5, 4.5, forward=True)\n    for i in range(n_images):\n        axs[0, i].imshow(images[i], cmap=cmap, norm=norm,\n                         interpolation=interpolation)\n        axs[0, i].set_title(titles[i])\n        axs[0, i].set_axis_off()\n    plt.tight_layout()\n    plt.show()\n\n", "CodeBleu_score": 32.76863917841164}, {"code": "def display_images(images, titles=None, cols=4, cmap=None, norm=None,\n                   interpolation=None)\n\t\n    titles = titles if titles is not None else [\"\"] * len(images)\n    rows = len(images) // cols + 1\n    plt.figure(figsize=(14, 14 * rows // cols))\n    i = 1\n    for image, title in zip(images, titles):\n        plt.subplot(rows, cols, i)\n        plt.title(title, fontsize=9)\n        plt.axis('off')\n        plt.imshow(image.astype(np.uint8), cmap=cmap,\n                   norm=norm, interpolation=interpolation)\n        i += 1\n    plt.show()\n\n", "CodeBleu_score": 76.8475453040379}]}
