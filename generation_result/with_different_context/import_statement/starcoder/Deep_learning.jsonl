{"instance_id": "1f9f4d85e825", "generation_result": [{"code": "def load_model_from_config(self, half_attention)\n\t\n    # Check if model is cached\n    if self.model_cache_path is not None:\n        if os.path.exists(self.model_cache_path):\n            # Check if model is valid\n            if self.model_cache_path.endswith('.safetensors'):\n                model = safetensors.torch.load(self.model_cache_path, map_location=self.device)\n            else:\n                model = torch.load(self.model_cache_path, map_location=self.device)\n            if not isinstance(model, dict):\n                model = {'model': model}\n            # Check if model is compatible with settings\n            if half_attention and not model['model'].half_attention:\n                model['model'].half_attention = True\n            if self.memory_format == 'channels_last' and not model['model'].channels_last:\n                model['model'].channels_last = True\n            if self.memory_format == 'channels_last' and not model['model'].channels_last:\n                model['model'].channels_last = True\n            if self.memory_format == 'channels_last' and not model['model'].channels_last:\n                model['model'].channels_last = True\n            if self.memory_format == 'channels_last' and not model['model'].channels_last:\n                model['model'].channels_last = True\n            # Move model to device\n            model['model'] = model['model'].to(self.device)\n            # Set model to evaluation mode\n            model['model'].eval()\n            # Return model\n            return model\n    # Load model from path\n    if self.model_path.endswith('.safetensors'):\n        model = safetensors.torch.load(self.model_path, map_location=self.device)\n    else:\n        model = torch.load(self.model_path, map_location=self.device)\n    if not isinstance(model, dict):\n        model = {'model': model}\n    # Check if model is compatible with settings\n    if half_attention and not model['model'].half_attention:\n        model['model'].half_attention = True\n    if self.memory_format == 'channels_last' and not model['model'].channels_last:\n        model['model'].channels_last = True\n    if self.memory_format == 'channels_last' and not model['model'].channels_", "CodeBleu_score": 29.657091841638}, {"code": "def load_model_from_config(self, half_attention)\n\t\n\n    # load model from cache if available\n    if self.load_from_cache:\n        try:\n            print('Loading model from cache.')\n            self.model = torch.load(self.load_from_cache_path, map_location=self.device)\n            self.model.eval()\n            if half_attention:\n                self.model.half_attention()\n            if self.memory_format == 'channels_last':\n                self.model.to(memory_format=torch.channels_last)\n            return self.model\n        except Exception as e:\n            print(e)\n            print('Cache not found or invalid. Loading model from file.')\n\n    # load model from file\n    print('Loading model from file.')\n    model = instantiate_from_config(self.config.model)\n    model.eval()\n    model.to(self.device)\n    if half_attention:\n        model.half_attention()\n    if self.memory_format == 'channels_last':\n        model.to(memory_format=torch.channels_last)\n\n    # cache model if requested\n    if self.save_to_cache:\n        print('Saving model to cache.')\n        torch.save(model, self.save_to_cache_path)\n\n    return model\n\n", "CodeBleu_score": 32.82382643432135}, {"code": "def load_model_from_config(self, half_attention)\n\t\n    # Load model from cache if available\n    if self.model_cache is not None:\n        cache_path = os.path.join(self.model_cache, self.model_name)\n        if os.path.exists(cache_path):\n            model_dict = torch.load(cache_path)\n            if model_dict[\"model_name\"] == self.model_name:\n                if model_dict[\"model_config\"] == self.model_config:\n                    if model_dict[\"model_ckpt\"] == self.model_ckpt:\n                        if model_dict[\"model_sd\"] == self.model_sd:\n                            if model_dict[\"model_half_attention\"] == half_attention:\n                                if model_dict[\"model_memory_format\"] == self.model_memory_format:\n                                    print(\"Loading model from cache\")\n                                    self.model = model_dict[\"model\"]\n                                    self.model.eval()\n                                    return self.model\n\n    # Load model from checkpoint\n    print(\"Loading model from checkpoint\")\n    self.model = instantiate_from_config(self.model_config)\n    if self.model_ckpt is not None:\n        sd = torch.load(self.model_ckpt, map_location=\"cpu\")\n        if \"state_dict\" in sd:\n            sd = sd[\"state_dict\"]\n        self.model.load_state_dict(sd, strict=False)\n    if self.model_sd is not None:\n        sd = torch.load(self.model_sd, map_location=\"cpu\")\n        if \"state_dict\" in sd:\n            sd = sd[\"state_dict\"]\n        self.model.load_state_dict(sd, strict=False)\n    self.model.eval()\n    self.model.to(self.device)\n\n    # Cache model if requested\n    if self.model_cache is not None:\n        cache_path = os.path.join(self.model_cache, self.model_name)\n        cache_dir = os.path.dirname(cache_path)\n        if not os.path.exists(cache_dir):\n            os.makedirs(cache_dir)\n        model_dict = {\n            \"model_name\": self.model_name,\n            \"model_config\": self.model_config,\n            \"model_ckpt\": self.model_ckpt,\n            \"model_sd\": self.", "CodeBleu_score": 37.983506456007596}]}
{"instance_id": "8dc955007aa2", "generation_result": [{"code": "def convsample_ddim(model, cond, steps, shape, eta=1.0, callback=None, normals_sequence=None,\n                    mask=None, x0=None, quantize_x0=False, temperature=1., score_corrector=None,\n                    corrector_kwargs=None, x_t=None\n                    )\n\t\n\n    # Initialize the sampler.\n    sampler = DDIMSampler(model)\n    # Extract the batch size from the shape parameter.\n    batch_size, *_ = shape\n    # Print the eta and steps values.\n    print(f'Sampling {steps} steps of size {batch_size} at eta {eta}.')\n    # Call the sample method of the DDIMSampler instance.\n    samples = sampler.sample(steps, batch_size, shape, cond, eta, callback, normals_sequence, mask, x0, quantize_x0,\n                             temperature, score_corrector, corrector_kwargs, x_t)\n    # Return the generated samples and intermediate values.\n    return samples\n\n", "CodeBleu_score": 25.936028724536037}, {"code": "def convsample_ddim(model, cond, steps, shape, eta=1.0, callback=None, normals_sequence=None,\n                    mask=None, x0=None, quantize_x0=False, temperature=1., score_corrector=None,\n                    corrector_kwargs=None, x_t=None\n                    )\n\t\n\n    batch_size = shape[0]\n    print('Batch size: ', batch_size)\n    print('Eta: ', eta)\n    print('Steps: ', steps)\n    sampler = DDIMSampler(model, batch_size=batch_size, eta=eta, score_corrector=score_corrector,\n                          corrector_kwargs=corrector_kwargs)\n    samples = sampler.sample(steps, shape, condition=cond, callback=callback, normals_sequence=normals_sequence,\n                             mask=mask, x0=x0, quantize_x0=quantize_x0, temperature=temperature, x_t=x_t)\n    return samples\n\n", "CodeBleu_score": 42.0323710024352}, {"code": "def convsample_ddim(model, cond, steps, shape, eta=1.0, callback=None, normals_sequence=None,\n                    mask=None, x0=None, quantize_x0=False, temperature=1., score_corrector=None,\n                    corrector_kwargs=None, x_t=None\n                    )\n\t\n    batch_size, channels, height, width = shape\n    assert batch_size > 0, 'batch size must be positive'\n    assert channels > 0, 'number of channels must be positive'\n    assert height > 0, 'height must be positive'\n    assert width > 0, 'width must be positive'\n    assert eta > 0.0, 'eta must be positive'\n    assert steps > 0, 'number of steps must be positive'\n    if callback is not None:\n        assert ismap(callback), 'callback must be a map'\n    if normals_sequence is not None:\n        assert ismap(normals_sequence), 'normals_sequence must be a map'\n    if mask is not None:\n        assert ismap(mask), 'mask must be a map'\n    if x0 is not None:\n        assert ismap(x0), 'x0 must be a map'\n    if score_corrector is not None:\n        assert ismap(score_corrector), 'score_corrector must be a map'\n    if corrector_kwargs is not None:\n        assert ismap(corrector_kwargs), 'corrector_kwargs must be a map'\n    if x_t is not None:\n        assert ismap(x_t), 'x_t must be a map'\n    sampler = DDIMSampler(model, cond, eta=eta)\n    print(f'Sampler initialized with eta = {eta}, steps = {steps}')\n    return sampler.sample(steps, batch_size, shape, callback=callback, normals_sequence=normals_sequence,\n                          mask=mask, x0=x0, quantize_x0=quantize_x0, temperature=temperature,\n                          score_corrector=score_corrector, corrector_kwargs=corrector_kwargs, x_t=x_t)\n\n", "CodeBleu_score": 31.537017867042515}]}
{"instance_id": "9bdcd9081480", "generation_result": [{"code": "def ema_scope(self, context=None)\n\t\n    if not self.use_ema:\n        yield\n        return\n    if context is None:\n        context = f\"for {self.__class__.__name__}\"\n    with torch.no_grad():\n        original_weights = {\n            name: param.data.clone()\n            for name, param in self.named_parameters()\n        }\n        for name, param in self.named_parameters():\n            param.data.copy_(self.ema.get(name))\n        yield\n        for name, param in self.named_parameters():\n            param.data.copy_(original_weights[name])\n        if self.verbose:\n            print(f\"EMA enabled: {context}\")\n    if self.verbose:\n        print(f\"EMA disabled: {context}\")\n\n", "CodeBleu_score": 39.35800071195591}, {"code": "def ema_scope(self, context=None)\n\t\n    if self.use_ema:\n        if context is not None:\n            context.write(\"EMA enabled, swapping weights...\")\n        with self.ema_scope:\n            yield\n        if context is not None:\n            context.write(\"Restoring weights...\")\n    else:\n        yield\n\n", "CodeBleu_score": 35.46786308052363}, {"code": "def ema_scope(self, context=None)\n\t\n    if self.ema is None:\n        yield\n        return\n    if context is None:\n        context = self.__class__.__name__\n    with self.ema:\n        yield\n        if self.ema.has_init:\n            self.ema.update(self)\n            if self.ema.verbose:\n                print(f'{context} weights restored.')\n\n", "CodeBleu_score": 43.79245891565955}]}
{"instance_id": "14ab009e1c5c", "generation_result": [{"code": "def init_from_ckpt(self, path, ignore_keys=None)\n\t\n    if ignore_keys is None:\n        ignore_keys = []\n    self.load_state_dict(torch.load(path, map_location=\"cpu\"), strict=False)\n    for k in self.state_dict().keys():\n        if k in ignore_keys:\n            del self.state_dict()[k]\n    missing_keys, unexpected_keys = self.load_state_dict(\n        self.state_dict(), strict=False\n    )\n    if len(missing_keys) > 0:\n        print(\n            \"WARNING: Missing Keys in state_dict: {}\".format(\n                \", \".join(missing_keys)\n            )\n        )\n    if len(unexpected_keys) > 0:\n        print(\n            \"WARNING: Unexpected Keys in state_dict: {}\".format(\n                \", \".join(unexpected_keys)\n            )\n        )\n\n", "CodeBleu_score": 33.191401248736184}, {"code": "def init_from_ckpt(self, path, ignore_keys=None)\n\t\n    self.load_state_dict(torch.load(path, map_location=\"cpu\")[\"state_dict\"], strict=False)\n    if ignore_keys is not None:\n        for k in self.state_dict().keys():\n            for ik in ignore_keys:\n                if k.startswith(ik):\n                    print(f\"Deleting key {k}\")\n                    del self.state_dict()[k]\n    self.to(self.device)\n\n", "CodeBleu_score": 25.105873753321283}, {"code": "def init_from_ckpt(self, path, ignore_keys=None)\n\t\n    if ignore_keys is None:\n        ignore_keys = []\n    checkpoint = torch.load(path, map_location=\"cpu\")\n    # get state_dict from checkpoint\n    if isinstance(checkpoint, dict) and \"state_dict\" in checkpoint:\n        state_dict = checkpoint[\"state_dict\"]\n    else:\n        state_dict = checkpoint\n    # strip prefix of state_dict\n    if list(state_dict.keys())[0].startswith(\"module.\"):\n        state_dict = {k[7:]: v for k, v in state_dict.items()}\n    # delete ignored params\n    for k in ignore_keys:\n        del state_dict[k]\n    # load state_dict\n    self.load_state_dict(state_dict, strict=False)\n    # print out unexpected keys and missing keys\n    missing_keys, unexpected_keys = self.load_state_dict(state_dict, strict=False)\n    if len(missing_keys) > 0:\n        print(\"Missing Keys: {}\".format(missing_keys))\n    if len(unexpected_keys) > 0:\n        print(\"Unexpected Keys: {}\".format(unexpected_keys))\n\n", "CodeBleu_score": 40.437848791420514}]}
{"instance_id": "754176e27b81", "generation_result": [{"code": "def forward(self, input, return_pred_indices=False)\n\t\n    quantized, diff, _, _ = self.encode(input)\n    dec = self.decode(quantized)\n    if return_pred_indices:\n        _, pred_indices = self.quantize.forward_idx(quantized)\n        return dec, diff, pred_indices\n    else:\n        return dec, diff\n\n", "CodeBleu_score": 31.540063678142488}, {"code": "def forward(self, input, return_pred_indices=False)\n\t\n    quantized, diff, _, _ = self.encode(input)\n    dec = self.decode(quantized)\n    if return_pred_indices:\n        _, pred_indices = self.quantize.get_indices(quantized)\n        return dec, diff, pred_indices\n    return dec, diff\n\n", "CodeBleu_score": 32.322659298882414}, {"code": "def forward(self, input, return_pred_indices=False)\n\t\n    if self.training:\n        return self.forward_train(input, return_pred_indices)\n    else:\n        return self.forward_test(input, return_pred_indices)\n", "CodeBleu_score": 22.69792400424339}]}
{"instance_id": "443df6fdaf0d", "generation_result": [{"code": "def get_input(self, batch, k)\n\t\n    input = batch[k]\n    if len(input.shape) == 3:\n        input = input.unsqueeze(0)\n    if len(input.shape) == 5:\n        input = input.squeeze(2)\n    input = input.permute(0, 3, 1, 2)\n    input = input.contiguous().float()\n    if self.resize:\n        input = F.interpolate(input, size=self.resize_range[1], mode='bicubic', align_corners=False)\n    input = input.detach()\n    return input\n\n", "CodeBleu_score": 32.775419164381326}, {"code": "def get_input(self, batch, k)\n\t\n    x = batch[k]\n    if len(x.shape) == 3:\n        x = x.unsqueeze(0)\n    if len(x.shape) == 2:\n        x = x.unsqueeze(0).unsqueeze(0)\n    if len(x.shape) == 1:\n        x = x.unsqueeze(0).unsqueeze(0).unsqueeze(0)\n    if self.resize:\n        x = F.interpolate(x, size=self.resize, mode='bicubic', align_corners=False)\n    x = x.permute(0, 3, 1, 2)\n    x = x.contiguous().float()\n    return x.detach()\n\n", "CodeBleu_score": 36.38832481439726}, {"code": "def get_input(self, batch, k)\n\t\n    x = batch[k]\n    if len(x.shape) == 3:\n        x = x[:, None]\n    x = x.permute(0, 3, 1, 2)\n    x = x.to(memory_format=torch.contiguous_format).to(torch.float32)\n    if self.resize is not None:\n        x = F.interpolate(x, size=self.resize, mode=\"bicubic\", align_corners=False)\n    return x.detach()\n\n", "CodeBleu_score": 35.22733587294869}]}
{"instance_id": "953a2632fc15", "generation_result": [{"code": "def training_step(self, batch, batch_idx, optimizer_idx)\n\t\n    # Retrieve data and predictions\n    x, y = batch\n    x_hat = self.forward(x)\n    # Compute autoencoding loss\n    if optimizer_idx == 0:\n        loss = self.loss_function(x_hat, y)\n        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n        return loss\n    # Compute discriminator loss\n    if optimizer_idx == 1:\n        loss = self.discriminator_loss(x, y, x_hat)\n        self.log(\"train_discriminator_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n        return loss\n\n", "CodeBleu_score": 15.932380825760045}, {"code": "def training_step(self, batch, batch_idx, optimizer_idx)\n\t\n    x, y = batch\n    x = x.to(self.device)\n    y = y.to(self.device)\n\n    # Autoencoder training\n    if optimizer_idx == 0:\n        # Get predictions and loss\n        x_recon, x_recon_loss, x_perplexity = self.model(x, y)\n        loss = x_recon_loss\n\n        # Log metrics\n        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n        self.log('train_x_recon_loss', x_recon_loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n        self.log('train_x_perplexity', x_perplexity, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n\n        return loss\n\n    # Discriminator training\n    elif optimizer_idx == 1:\n        # Get predictions and loss\n        y_hat = self.model(x, y)\n        loss = self.loss_fn(y_hat, y)\n\n        # Log metrics\n        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n\n        return loss\n\n", "CodeBleu_score": 33.70850325812114}, {"code": "def training_step(self, batch, batch_idx, optimizer_idx)\n\t\n    input, target = batch\n    input = input.to(self.device)\n    target = target.to(self.device)\n    x_hat, z, loss_dict = self.forward(input)\n    if optimizer_idx == 0:\n        loss = loss_dict[\"loss\"]\n        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n        return loss\n    if optimizer_idx == 1:\n        loss = loss_dict[\"loss\"]\n        self.log(\"train_loss_disc\", loss, on_step=True, on_epoch=True, prog_bar=True)\n        return loss\n\n", "CodeBleu_score": 17.52160925859661}]}
{"instance_id": "b449b5e8b77f", "generation_result": [{"code": "def log_images(self, batch, only_inputs=False, plot_ema=False, **kwargs)\n\t\n    if not self.trainer.global_step % self.trainer.log_every_n_steps:\n        if only_inputs:\n            x, y = batch\n            x = self.to_rgb(x)\n            y = self.to_rgb(y)\n            self.logger.experiment.add_image(f\"{self.name}/inputs\", x, self.trainer.global_step)\n        else:\n            x, y = batch\n            x = self.to_rgb(x)\n            y = self.to_rgb(y)\n            x_recon = self.to_rgb(self.last_x_recon)\n            y_recon = self.to_rgb(self.last_y_recon)\n            if plot_ema:\n                x_ema = self.to_rgb(self.last_x_ema)\n                y_ema = self.to_rgb(self.last_y_ema)\n                self.logger.experiment.add_image(f\"{self.name}/inputs\", x, self.trainer.global_step)\n                self.logger.experiment.add_image(f\"{self.name}/reconstructions\", x_recon, self.trainer.global_step)\n                self.logger.experiment.add_image(f\"{self.name}/ema_reconstructions\", x_ema, self.trainer.global_step)\n            else:\n                self.logger.experiment.add_image(f\"{self.name}/inputs\", x, self.trainer.global_step)\n                self.logger.experiment.add_image(f\"{self.name}/reconstructions\", x_recon, self.trainer.global_step)\n\n", "CodeBleu_score": 28.006997095949615}, {"code": "def log_images(self, batch, only_inputs=False, plot_ema=False, **kwargs)\n\t\n    # get data\n    x, y = batch\n    x, y = x.to(self.device), y.to(self.device)\n\n    # get EMA weights\n    if plot_ema:\n        x_ema = self.ema_model.module.get_input(x)\n    else:\n        x_ema = None\n\n    # get reconstructions\n    with torch.no_grad():\n        if x_ema is not None:\n            x_recon, x_recon_ema = self.forward(x_ema)\n        else:\n            x_recon = self.forward(x)\n\n    # colorize\n    if x.shape[1] == 1:\n        x = x.repeat(1, 3, 1, 1)\n        x_recon = x_recon.repeat(1, 3, 1, 1)\n        if x_ema is not None:\n            x_recon_ema = x_recon_ema.repeat(1, 3, 1, 1)\n\n    # log images\n    if only_inputs:\n        self.logger.experiment.add_images(\"inputs\", x, self.global_step, dataformats=\"NCHW\")\n    else:\n        self.logger.experiment.add_images(\"inputs\", x, self.global_step, dataformats=\"NCHW\")\n        self.logger.experiment.add_images(\"reconstructions\", x_recon, self.global_step, dataformats=\"NCHW\")\n        if x_ema is not None:\n            self.logger.experiment.add_images(\"reconstructions_ema\", x_recon_ema, self.global_step, dataformats=\"NCHW\")\n\n", "CodeBleu_score": 34.31585489652916}, {"code": "def log_images(self, batch, only_inputs=False, plot_ema=False, **kwargs)\n\t\n    if not self.training:\n        return\n    if only_inputs:\n        self.log(\"val/input\", self.input_image_logger(batch, **kwargs))\n    else:\n        self.log(\"val/input\", self.input_image_logger(batch, **kwargs))\n        self.log(\"val/recon\", self.recon_image_logger(batch, plot_ema=plot_ema, **kwargs))\n\n", "CodeBleu_score": 15.023963984153266}]}
{"instance_id": "540d089e3481", "generation_result": [{"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss_type == 'l1':\n        loss = (pred - target).abs()\n        if mean:\n            loss = loss.mean()\n    elif self.loss_type == 'l2':\n        loss = (pred - target) ** 2\n        if mean:\n            loss = loss.mean() if self.mean else loss.sum()\n    else:\n        raise NotImplementedError(f'Loss type {self.loss_type} is not implemented.')\n    return loss\n\n", "CodeBleu_score": 53.993093035538884}, {"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss_type == 'l1':\n        loss = torch.abs(pred - target).mean() if mean else torch.abs(pred - target).sum()\n    elif self.loss_type == 'l2':\n        loss = F.mse_loss(pred, target, reduction='mean') if mean else F.mse_loss(pred, target, reduction='sum')\n    else:\n        raise NotImplementedError(f'Loss type {self.loss_type} not implemented')\n    return loss\n", "CodeBleu_score": 46.73373290121409}, {"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss_type == \"l1\":\n        loss = torch.abs(pred - target)\n        if mean:\n            loss = loss.mean()\n    elif self.loss_type == \"l2\":\n        loss = (pred - target) ** 2\n        if mean:\n            loss = loss.mean() if self.mean_loss else loss.sum()\n    else:\n        raise NotImplementedError(\n            f\"Loss type {self.loss_type} is not implemented. Please use 'l1' or 'l2'.\"\n        )\n    return loss\n\n", "CodeBleu_score": 49.91421797587282}]}
{"instance_id": "63a08283666f", "generation_result": [{"code": "def forward(self, x, c, *args, **kwargs)\n\t\n    # if we're training with a conditioning model, apply it\n    if self.conditioning_model is not None:\n        c = self.conditioning_model(x, c)\n\n    # if we're training with a timestep embedding, apply it\n    if self.timestep_embed is not None:\n        t = self.get_timestep_embed(self.num_timesteps - self.current_iteration, self.model_config[\"timestep_embed_dim\"])\n        x = self.timestep_embed(x, t)\n\n    # apply the model\n    return self.model(x, c, *args, **kwargs)\n\n", "CodeBleu_score": 28.24843168465711}, {"code": "def forward(self, x, c, *args, **kwargs)\n\t\n    # Get time step selection and conditioning\n    t = self.get_t(x.shape[0], *args, **kwargs)\n    if self.conditioning_channels > 0:\n        c = self.condition(c)\n    # Process inputs\n    x = self.process_inputs(x, t)\n    # Compute loss\n    return self.compute_loss(x, t, c, *args, **kwargs)\n\n", "CodeBleu_score": 21.30042427511776}, {"code": "def forward(self, x, c, *args, **kwargs)\n\t\n    if self.num_timesteps is None:\n        timesteps = self.ddpm_num_timesteps\n    else:\n        timesteps = self.num_timesteps\n    if self.beta_schedule is not None:\n        t = torch.randint(0, timesteps, (x.shape[0],), device=x.device).long()\n    else:\n        t = torch.randint(0, timesteps, (x.shape[0],), device=x.device).long()\n    return self.p_sample_loop(x, t, c, *args, **kwargs)\n\n", "CodeBleu_score": 36.820673704243845}]}
{"instance_id": "4627e1e8ed62", "generation_result": [{"code": "def get_codebook_entry(self, indices, shape)\n\t\n    b = shape[0]\n    h = shape[1]\n    w = shape[2]\n    d = shape[3]\n    device = self.weight.device\n    indices = indices.reshape(b, h, w, d)\n    if self.remap:\n        indices = self.remap_fn(indices)\n    weight = self.weight[indices.long()]\n    weight = rearrange(weight, \"b h w d -> b d h w\")\n    weight = weight.permute(0, 3, 1, 2)\n    return weight\n\n", "CodeBleu_score": 42.55992220574053}, {"code": "def get_codebook_entry(self, indices, shape)\n\t\n    # Map the indices to the correct range\n    if self.indices_remapping is not None:\n        indices = self.indices_remapping[indices]\n    # Retrieve the latent vectors from the codebook\n    z_q = self.codebook(indices.view(-1))\n    # Reshape the latent vectors to match the original input shape\n    z_q = rearrange(z_q, '(b n) d -> b n d', b=shape[0], n=shape[1])\n    # Permute the latent vectors to match the original input dimensions\n    z_q = z_q.permute(0, 2, 1).contiguous()\n    return z_q\n\n", "CodeBleu_score": 42.492561353792034}, {"code": "def get_codebook_entry(self, indices, shape)\n\t\n    # Retrieve the latent vectors from the codebook\n    codebook_vectors = self.embedding(indices.long())\n    # Reshape the latent vectors to match the original input shape\n    codebook_vectors = rearrange(codebook_vectors, \"b (h w) c -> b c h w\", h=shape[2], w=shape[3])\n    # Permute the latent vectors to match the input format\n    codebook_vectors = codebook_vectors.permute(0, 2, 3, 1)\n    return codebook_vectors\n\n", "CodeBleu_score": 15.034424033523635}]}
{"instance_id": "c55d014827ef", "generation_result": [{"code": "def factorization(dimension: int, factor:int=-1) -> tuple[int, int]\n\t\n    if factor > 0:\n        if factor > dimension:\n            raise ValueError(f\"The specified factor ({factor}) cannot be greater than the dimension ({dimension}).\")\n        return factor, dimension // factor\n    else:\n        for i in range(1, int(dimension ** 0.5) + 1):\n            if dimension % i == 0:\n                if factor > 0 and i > factor:\n                    break\n                return i, dimension // i\n        raise ValueError(f\"The specified factor ({factor}) cannot be greater than the dimension ({dimension}) or the factor cannot be found.\")\n\n", "CodeBleu_score": 28.267036646504074}, {"code": "def factorization(dimension: int, factor:int=-1) -> tuple[int, int]\n\t\n    if factor > 0 and dimension % factor == 0:\n        return (factor, dimension // factor)\n    elif factor < 0:\n        for i in range(1, dimension):\n            if dimension % i == 0:\n                if abs(i - dimension / i) < abs(factor):\n                    factor = i\n        return (factor, dimension // factor)\n    else:\n        raise ValueError(f\"factor {factor} is invalid for dimension {dimension}\")\n", "CodeBleu_score": 29.128747907114448}, {"code": "def factorization(dimension: int, factor:int=-1) -> tuple[int, int]\n\t\n    if factor == -1:\n        factor = dimension\n    if factor > 0 and factor <= dimension:\n        return factor, dimension // factor\n    if factor < 0:\n        m = 1\n        n = dimension\n        while m * n != dimension:\n            if m + n == 0:\n                m = 1\n                n = dimension\n            elif m + n > 0 and m + n <= abs(factor):\n                m = m + 1\n                n = n - 1\n            elif m + n > abs(factor):\n                m = m - 1\n                n = n + 1\n        return m, n\n\n", "CodeBleu_score": 45.689278830020456}]}
{"instance_id": "5d613ec26036", "generation_result": [{"code": "def apply_weight_decompose(self, updown, orig_weight)\n\t\n    # Make sure that the weight is on the same device and data type as the model\n    self.to(self.weight.device)\n    self.to(self.weight.dtype)\n    # Decompose the weight\n    weight_decomposed = self.weight_decompose(updown)\n    # Normalize the weight\n    weight_normalized = self.weight_normalize(weight_decomposed)\n    # Scale the weight\n    weight_scaled = self.weight_scale(weight_normalized)\n    # Calculate the adjusted weight\n    weight_adjusted = self.weight_adjust(weight_scaled, orig_weight)\n    # Update the model's weight\n    self.weight = weight_adjusted\n\n", "CodeBleu_score": 7.9523502849280865}, {"code": "def apply_weight_decompose(self, updown, orig_weight)\n\t\n    # TODO: Add support for decomposing the weight of a model that uses shared parameters.\n    # TODO: Add support for decomposing the weight of a model that uses shared parameters with a shared parameter that is not the weight.\n    # TODO: Add support for decomposing the weight of a model that uses shared parameters with a shared parameter that is not the weight, but is a bias.\n    # TODO: Add support for decomposing the weight of a model that uses shared parameters with a shared parameter that is not the weight, but is a bias, and that has a bias.\n    # TODO: Add support for decomposing the weight of a model that uses shared parameters with a shared parameter that is not the weight, but is a bias, and that has a bias, and that has a bias.\n    # TODO: Add support for decomposing the weight of a model that uses shared parameters with a shared parameter that is not the weight, but is a bias, and that has a bias, and that has a bias, and that has a bias.\n    # TODO: Add support for decomposing the weight of a model that uses shared parameters with a shared parameter that is not the weight, but is a bias, and that has a bias, and that has a bias, and that has a bias, and that has a bias.\n    # TODO: Add support for decomposing the weight of a model that uses shared parameters with a shared parameter that is not the weight, but is a bias, and that has a bias, and that has a bias, and that has a bias, and that has a bias, and that has a bias.\n    # TODO: Add support for decomposing the weight of a model that uses shared parameters with a shared parameter that is not the weight, but is a bias, and that has a bias, and that has a bias, and that has a bias, and that has a bias, and that has a bias, and that has a bias.\n    # TODO: Add support for decomposing the weight of a model that uses shared parameters with a shared parameter that is not the weight, but is a bias, and that has a bias, and that has a bias, and that has a bias, and that has a bias, and that has a bias, and that has a bias, and that has a bias.\n    # TODO: Add support for decomposing the weight of a model that uses shared parameters with a shared parameter", "CodeBleu_score": 1.049465240641711}, {"code": "def apply_weight_decompose(self, updown, orig_weight)\n\t\n    if self.weight_decompose:\n        if self.weight_decompose_mode == 'add':\n            self.weight = self.weight + (updown * orig_weight)\n        elif self.weight_decompose_mode == 'mul':\n            self.weight = self.weight * (updown * orig_weight)\n        elif self.weight_decompose_mode == 'sub':\n            self.weight = self.weight - (updown * orig_weight)\n        elif self.weight_decompose_mode == 'div':\n            self.weight = self.weight / (updown * orig_weight)\n        else:\n            raise errors.InvalidWeightDecomposeMode(self.weight_decompose_mode)\n        self.weight = self.weight.to(self.weight.device, self.weight.dtype)\n        self.weight = self.weight / self.weight.norm(p=2, dim=1, keepdim=True)\n        self.weight = self.weight * self.weight_decompose_scale\n        self.weight = self.weight - orig_weight\n\n", "CodeBleu_score": 25.241520630039883}]}
{"instance_id": "c73d0d1cd3e5", "generation_result": [{"code": "def finalize_updown(self, updown, orig_weight, output_shape, ex_bias=None)\n\t\n    # Apply the bias if provided\n    if ex_bias is not None:\n        updown = updown + ex_bias\n\n    # Adjust the shape of the updown tensor if needed\n    if orig_weight.shape != output_shape:\n        updown = updown.reshape(output_shape)\n\n    # Adjust the shape of the updown tensor if needed\n    if orig_weight.shape != updown.shape:\n        updown = updown.reshape(orig_weight.shape)\n\n    # Decompose the updown tensor if needed\n    if self.decompose_weights:\n        updown = self.decompose_tensor(updown)\n\n    return updown\n\n", "CodeBleu_score": 33.615596639461096}, {"code": "def finalize_updown(self, updown, orig_weight, output_shape, ex_bias=None)\n\t\n    if updown is None:\n        return None, None\n    if ex_bias is not None:\n        ex_bias = ex_bias.reshape(output_shape)\n        if updown.shape != output_shape:\n            updown = updown.reshape(output_shape)\n        if orig_weight.shape != output_shape:\n            orig_weight = orig_weight.reshape(output_shape)\n        return updown, ex_bias\n    if updown.shape != output_shape:\n        updown = updown.reshape(output_shape)\n    if orig_weight.shape != output_shape:\n        orig_weight = orig_weight.reshape(output_shape)\n    if orig_weight.numel() == updown.numel():\n        updown = updown.reshape(orig_weight.shape)\n    if self.decomposition is not None:\n        updown = self.decomposition.decompose(updown, orig_weight)\n    return updown, ex_bias\n\n", "CodeBleu_score": 49.960864498304694}, {"code": "def finalize_updown(self, updown, orig_weight, output_shape, ex_bias=None)\n\t\n    if ex_bias is None:\n        ex_bias = 0\n    if updown.shape != output_shape:\n        updown = updown.reshape(output_shape)\n    if orig_weight.size != updown.size:\n        updown = updown.reshape(orig_weight.shape)\n    if self.bias is not None:\n        ex_bias += self.bias\n    if self.decomp is not None:\n        updown = self.decomp.decompose(updown)\n    if self.scalar is not None:\n        updown *= self.scalar\n    return updown, ex_bias\n\n", "CodeBleu_score": 43.73262519918563}]}
{"instance_id": "c6a41665c62c", "generation_result": [{"code": "def calc_updown(self, orig_weight)\n\t\n    # Transfer the current weight tensor to the device of the original weight tensor.\n    self.weight.data = self.weight.data.to(orig_weight.device)\n    # If the bias tensor exists, transfer it to the device of the original weight tensor.\n    if self.bias is not None:\n        self.bias.data = self.bias.data.to(orig_weight.device)\n    # Calculate and return the updated weight tensor.\n    return self.calc_updown_func(orig_weight, self.weight, self.bias)\n\n", "CodeBleu_score": 23.726289487395295}, {"code": "def calc_updown(self, orig_weight)\n\t\n    if orig_weight.is_cuda:\n        orig_weight = orig_weight.cpu()\n    new_weight = orig_weight.data.clone()\n    new_weight.copy_(self.calc_updown(orig_weight))\n    if orig_weight.is_cuda:\n        new_weight = new_weight.cuda()\n    return new_weight\n\n", "CodeBleu_score": 29.604524270749266}, {"code": "def calc_updown(self, orig_weight)\n\t\n    if self.updown_method == 'none':\n        return orig_weight\n    elif self.updown_method == 'momentum':\n        return self.calc_updown_momentum(orig_weight)\n    elif self.updown_method == 'adam':\n        return self.calc_updown_adam(orig_weight)\n    else:\n        raise ValueError('updown_method should be one of the following: none, momentum, adam')\n\n", "CodeBleu_score": 21.883625501864234}]}
{"instance_id": "1bb9e5d94af8", "generation_result": [{"code": "def purge_networks_from_memory()\n\t\n    global network_count\n    while len(network.networks) > network_count:\n        network.networks.popitem()\n\n", "CodeBleu_score": 18.37966622487103}, {"code": "def purge_networks_from_memory()\n\t\n    while len(devices.networks) > devices.network_limit:\n        logger.info(f\"Purging network from memory: {devices.networks[0].id}\")\n        devices.networks.pop(0)\n        gc.collect()\n\n", "CodeBleu_score": 19.842023211805248}, {"code": "def purge_networks_from_memory()\n\t\n    if len(network.networks) > 10:\n        for i in range(len(network.networks) - 10):\n            network.networks.popitem(last=False)\n        gc.collect()\n\n", "CodeBleu_score": 30.11157150540559}]}
{"instance_id": "42b48ebf4d07", "generation_result": [{"code": "def network_restore_weights_from_backup(self: Union[torch.nn.Conv2d, torch.nn.Linear, torch.nn.GroupNorm, torch.nn.LayerNorm, torch.nn.MultiheadAttention]):\n\t\n    if not hasattr(self, 'backup_weights'):\n        return\n    if self.backup_weights is None:\n        return\n    if not hasattr(self, 'backup_bias'):\n        self.bias = None\n    elif self.backup_bias is None:\n        self.bias = None\n    else:\n        self.bias = self.backup_bias\n    if isinstance(self, torch.nn.Conv2d):\n        self.weight = self.backup_weights\n    elif isinstance(self, torch.nn.Linear):\n        self.weight = self.backup_weights\n    elif isinstance(self, torch.nn.GroupNorm):\n        self.weight = self.backup_weights\n        self.bias = self.backup_bias\n    elif isinstance(self, torch.nn.LayerNorm):\n        self.weight = self.backup_weights\n        self.bias = self.backup_bias\n    elif isinstance(self, torch.nn.MultiheadAttention):\n        self.in_proj_weight = self.backup_weights\n        self.in_proj_bias = self.backup_bias\n        self.out_proj.weight = self.backup_weights[self.embed_dim:self.embed_dim + self.embed_dim, :]\n        self.out_proj.bias = self.backup_bias[self.embed_dim:self.embed_dim + self.embed_dim]\n\n", "CodeBleu_score": 40.806731987024506}, {"code": "def network_restore_weights_from_backup(self: Union[torch.nn.Conv2d, torch.nn.Linear, torch.nn.GroupNorm, torch.nn.LayerNorm, torch.nn.MultiheadAttention]):\n\t\n    if hasattr(self, 'backup_weight'):\n        self.weight.data = self.backup_weight\n    if hasattr(self, 'backup_bias'):\n        self.bias.data = self.backup_bias\n    if isinstance(self, torch.nn.MultiheadAttention):\n        if hasattr(self, 'in_proj_weight_backup'):\n            self.in_proj_weight.data = self.in_proj_weight_backup\n        if hasattr(self, 'in_proj_bias_backup'):\n            self.in_proj_bias.data = self.in_proj_bias_backup\n        if hasattr(self, 'out_proj_weight_backup'):\n            self.out_proj.weight.data = self.out_proj_weight_backup\n        if hasattr(self, 'out_proj_bias_backup'):\n            self.out_proj.bias.data = self.out_proj_bias_backup\n\n", "CodeBleu_score": 34.42939353938467}, {"code": "def network_restore_weights_from_backup(self: Union[torch.nn.Conv2d, torch.nn.Linear, torch.nn.GroupNorm, torch.nn.LayerNorm, torch.nn.MultiheadAttention]):\n\t\n    if self.weight_backup is not None:\n        self.weight.data = self.weight_backup.data\n    if self.bias_backup is not None:\n        self.bias.data = self.bias_backup.data\n    if isinstance(self, torch.nn.MultiheadAttention):\n        if self.in_proj_weight_backup is not None:\n            self.in_proj_weight.data = self.in_proj_weight_backup.data\n        if self.in_proj_bias_backup is not None:\n            self.in_proj_bias.data = self.in_proj_bias_backup.data\n        if self.out_proj_weight_backup is not None:\n            self.out_proj.weight.data = self.out_proj_weight_backup.data\n        if self.out_proj_bias_backup is not None:\n            self.out_proj.bias.data = self.out_proj_bias_backup.data\n\n", "CodeBleu_score": 35.271332657451694}]}
{"instance_id": "c8fcd357131a", "generation_result": [{"code": "def network_forward(org_module, input, original_forward)\n\t\n    # check if any additional networks are loaded\n    if devices.loaded_networks:\n        # check if the current layer matches any loaded networks\n        for net in devices.loaded_networks:\n            if net.layer == org_module:\n                # restore module weights from backup\n                net.restore_weights()\n                # reset module weights\n                net.reset_weights()\n                # cast input to appropriate device\n                input = input.to(net.device)\n                # perform forward pass\n                output = net.forward(input)\n                # cast output to appropriate device\n                output = output.to(devices.device)\n                # return output\n                return output\n    # no additional networks loaded, perform original forward pass\n    return original_forward(input)\n\n", "CodeBleu_score": 28.87929565425714}, {"code": "def network_forward(org_module, input, original_forward)\n\t\n    if not shared.loaded_networks:\n        return original_forward(input)\n    if org_module.weight.device != devices.get_device():\n        org_module.to(devices.get_device())\n    org_module.load_state_dict(org_module.backup_weights)\n    org_module.reset_parameters()\n    output = original_forward(input)\n    for network in shared.loaded_networks:\n        if network.layer_regex.match(org_module.__class__.__name__):\n            output = network.forward(output)\n    return output\n\n", "CodeBleu_score": 30.38075371793283}, {"code": "def network_forward(org_module, input, original_forward)\n\t\n    # Check if any additional networks are loaded\n    if not devices.loaded_networks:\n        # No additional networks loaded, use original forward method\n        return original_forward(input)\n    # Additional networks loaded, cast input to appropriate device\n    input = input.to(devices.device)\n    # Store original module weights\n    original_weights = org_module.state_dict()\n    # Restore module weights from backup\n    org_module.load_state_dict(org_module.backup_weights)\n    # Reset module weights\n    org_module.reset_weights()\n    # Call original forward method\n    output = original_forward(input)\n    # Apply additional networks\n    for network in devices.loaded_networks:\n        # Check if network should be applied to current layer\n        if network.layer == org_module.layer:\n            # Apply network to current layer\n            output = network.forward(output)\n    # Restore module weights from backup\n    org_module.load_state_dict(original_weights)\n    # Return output\n    return output\n\n", "CodeBleu_score": 35.58152601411405}]}
{"instance_id": "1070211f5c11", "generation_result": [{"code": "def trainables(self)\n\t\n    trainable_params = []\n    for name, param in self.named_parameters():\n        if param.requires_grad:\n            trainable_params.append(param)\n    return trainable_params\n\n", "CodeBleu_score": 24.577381731332526}, {"code": "def trainables(self)\n\t\n    return [param for name, param in self.named_parameters() if param.requires_grad]\n\n", "CodeBleu_score": 8.386319474073378}, {"code": "def trainables(self)\n\t\n    trainables = []\n    for name, p in self.named_parameters():\n        if 'weight' in name and 'layer_norm' not in name:\n            trainables.append(p)\n        elif 'bias' in name and 'layer_norm' not in name:\n            trainables.append(p)\n    return trainables\n\n", "CodeBleu_score": 26.841843693491228}]}
{"instance_id": "0c3ba62c96a7", "generation_result": [{"code": "def load_hypernetworks(names, multipliers=None)\n\t\n    if multipliers is None:\n        multipliers = [1.0] * len(names)\n    hypernetworks = []\n    for name, multiplier in zip(names, multipliers):\n        hypernetwork = shared.hypernetwork_dict.get(name)\n        if hypernetwork is None:\n            hypernetwork = sd_models.load_hypernetwork(name)\n        hypernetwork.set_multiplier(multiplier)\n        hypernetworks.append(hypernetwork)\n    shared.hypernetworks = hypernetworks\n\n", "CodeBleu_score": 34.566230813888154}, {"code": "def load_hypernetworks(names, multipliers=None)\n\t\n    if multipliers is None:\n        multipliers = [1 for _ in names]\n    if len(multipliers) != len(names):\n        raise errors.HyperparametersError(\"Number of hypernetwork names and number of multipliers must match.\")\n    for i, name in enumerate(names):\n        if name not in sd_models.loaded_hypernetworks:\n            sd_models.loaded_hypernetworks[name] = sd_models.load_hypernetwork(name)\n        sd_models.loaded_hypernetworks[name].set_multiplier(multipliers[i])\n    shared.hypernetworks = sd_models.loaded_hypernetworks\n\n", "CodeBleu_score": 31.64797182079708}, {"code": "def load_hypernetworks(names, multipliers=None)\n\t\n    if multipliers is None:\n        multipliers = [1.0] * len(names)\n    hypernetworks = []\n    for name, multiplier in zip(names, multipliers):\n        if name in shared.loaded_hypernetworks:\n            hypernetwork = shared.loaded_hypernetworks[name]\n            hypernetwork.multiplier = multiplier\n            hypernetworks.append(hypernetwork)\n        else:\n            hypernetwork = sd_models.load_hypernetwork(name)\n            hypernetwork.multiplier = multiplier\n            hypernetworks.append(hypernetwork)\n            shared.loaded_hypernetworks[name] = hypernetwork\n    shared.hypernetworks = hypernetworks\n\n", "CodeBleu_score": 30.823945174329936}]}
{"instance_id": "bea3772ed532", "generation_result": [{"code": "\ndef apply_hypernetworks(hypernetworks, context, layer=None)\n\t\n    context_k, context_v = context, context\n    for hypernetwork in hypernetworks:\n        context_k, context_v = apply_hypernetwork(hypernetwork, context_k, context_v, layer=layer)\n    return context_k, context_v\n\n", "CodeBleu_score": 52.02529865145008}, {"code": "\ndef apply_hypernetworks(hypernetworks, context, layer=None)\n\t\n    if layer is None:\n        layer = len(hypernetworks)\n    context_k, context_v = context\n    for i in range(layer):\n        if i == 0:\n            context_k, context_v = apply_hypernetwork(hypernetworks[i], context)\n        else:\n            context_k, context_v = apply_hypernetwork(hypernetworks[i], (context_k, context_v))\n    return context_k, context_v\n\n", "CodeBleu_score": 37.02587817384444}, {"code": "\ndef apply_hypernetworks(hypernetworks, context, layer=None)\n\t\n    context_k = context\n    context_v = context\n    for i, hypernetwork in enumerate(hypernetworks):\n        context_k, context_v = apply_hypernetwork(hypernetwork, context_k, context_v, layer=layer)\n    return context_k, context_v\n\n", "CodeBleu_score": 52.360065561115675}]}
{"instance_id": "922efa3e092f", "generation_result": [{"code": "def attention_CrossAttention_forward(self, x, context=None, mask=None, **kwargs)\n\t\n    # Query, Key, and Value projections\n    q, k, v = self.project(x, context=context, **kwargs)\n    # Apply hypernetworks to the context\n    if self.hypernet_context is not None:\n        context = self.hypernet_context(context)\n    # Reshape for multi-head attention\n    q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=self.heads), (q, k, v))\n    # Apply scaled dot-product attention\n    if mask is not None:\n        mask = rearrange(mask, 'b ... -> b (...)')\n    out, attn = self.attention(q, k, v, mask=mask)\n    # Reshape back and apply final linear layer\n    out = rearrange(out, 'b h n d -> b n (h d)')\n    return self.to_out(out)\n\n", "CodeBleu_score": 34.94336156459323}, {"code": "def attention_CrossAttention_forward(self, x, context=None, mask=None, **kwargs)\n\t\n    query = self.query_net(x)\n    key = self.key_net(x)\n    value = self.value_net(x)\n    if context is not None:\n        context = self.context_net(context)\n    if self.use_bias:\n        query = query + self.query_bias\n        key = key + self.key_bias\n        value = value + self.value_bias\n    if self.use_hypernet:\n        query = self.query_hypernet(query)\n        key = self.key_hypernet(key)\n        value = self.value_hypernet(value)\n        if context is not None:\n            context = self.context_hypernet(context)\n    query = rearrange(query, 'b n (h d) -> b h n d', h=self.heads)\n    key = rearrange(key, 'b n (h d) -> b h n d', h=self.heads)\n    value = rearrange(value, 'b n (h d) -> b h n d', h=self.heads)\n    if context is not None:\n        context = rearrange(context, 'b n (h d) -> b h n d', h=self.heads)\n    if mask is not None:\n        mask = repeat(mask, 'b n -> (b h) () n', h=self.heads)\n    out = self.attention(query, key, value, context, mask=mask, **kwargs)\n    out = rearrange(out, 'b h n d -> b n (h d)')\n    return self.out_net(out)\n\n", "CodeBleu_score": 40.85426796945437}, {"code": "def attention_CrossAttention_forward(self, x, context=None, mask=None, **kwargs)\n\t\n    # Query and key projections\n    # q = self.query_net(x)\n    # k = self.key_net(context)\n    # v = self.value_net(context)\n\n    # Query and key projections\n    q = self.query_net(x)\n    k = self.key_net(x)\n    v = self.value_net(x)\n\n    # Calculate query, key, and value projections\n    # q, k, v = self.compute_projections(x, context)\n\n    # Calculate query, key, and value projections\n    # q, k, v = self.compute_projections(x, context)\n\n    # Calculate query, key, and value projections\n    # q, k, v = self.compute_projections(x, context)\n\n    # Calculate query, key, and value projections\n    # q, k, v = self.compute_projections(x, context)\n\n    # Calculate query, key, and value projections\n    # q, k, v = self.compute_projections(x, context)\n\n    # Calculate query, key, and value projections\n    # q, k, v = self.compute_projections(x, context)\n\n    # Calculate query, key, and value projections\n    # q, k, v = self.compute_projections(x, context)\n\n    # Calculate query, key, and value projections\n    # q, k, v = self.compute_projections(x, context)\n\n    # Calculate query, key, and value projections\n    # q, k, v = self.compute_projections(x, context)\n\n    # Calculate query, key, and value projections\n    # q, k, v = self.compute_projections(x, context)\n\n    # Calculate query, key, and value projections\n    # q, k, v = self.compute_projections(x, context)\n\n    # Calculate query, key, and value projections\n    # q, k, v = self.compute_projections(x, context)\n\n    # Calculate query, key, and value projections\n    # q, k, v = self.compute_projections(x, context)\n\n    # Calculate query, key, and value projections\n    # q, k, v = self.compute_projections(x, context)\n\n    # Calculate query, key, and value", "CodeBleu_score": 2.7781092798919924}]}
{"instance_id": "8b7455a03a0e", "generation_result": [{"code": "def save_hypernetwork(hypernetwork, checkpoint, hypernetwork_name, filename)\n\t\n    if hypernetwork is None:\n        return\n    if hypernetwork_name not in checkpoint:\n        raise errors.CheckpointError(f\"Hypernetwork {hypernetwork_name} not found in checkpoint.\")\n    hypernetwork_checkpoint = checkpoint[hypernetwork_name]\n    hypernetwork_checkpoint = sd_hijack_checkpoint.remove_hypernetwork_checkpoint_metadata(hypernetwork_checkpoint)\n    hypernetwork_checkpoint = sd_hijack_checkpoint.update_hypernetwork_checkpoint_metadata(hypernetwork_checkpoint, checkpoint)\n    hypernetwork_checkpoint = sd_hijack_checkpoint.update_hypernetwork_checkpoint_metadata(hypernetwork_checkpoint, hypernetwork.state_dict())\n    hypernetwork_checkpoint = sd_hijack_checkpoint.update_hypernetwork_checkpoint_metadata(hypernetwork_checkpoint, hypernetwork.hyperparameters)\n    hypernetwork_checkpoint = sd_hijack_checkpoint.update_hypernetwork_checkpoint_metadata(hypernetwork_checkpoint, hypernetwork.hyperparameters_template)\n    hypernetwork_checkpoint = sd_hijack_checkpoint.update_hypernetwork_checkpoint_metadata(hypernetwork_checkpoint, hypernetwork.hyperparameters_template_values)\n    hypernetwork_checkpoint = sd_hijack_checkpoint.update_hypernetwork_checkpoint_metadata(hypernetwork_checkpoint, hypernetwork.hyperparameters_template_values_metadata)\n    hypernetwork_checkpoint = sd_hijack_checkpoint.update_hypernetwork_checkpoint_metadata(hypernetwork_checkpoint, hypernetwork.hyperparameters_template_metadata)\n    hypernetwork_checkpoint = sd_hijack_checkpoint.update_hypernetwork_checkpoint_metadata(hypernetwork_checkpoint, hypernetwork.hyperparameters_template_metadata_values)\n    hypernetwork_checkpoint = sd_hijack_checkpoint.update_hypernetwork_checkpoint_metadata(hypernetwork_checkpoint, hypernetwork.hyperparameters_template_metadata_values_metadata)\n    hypernetwork_checkpoint = sd_hijack_checkpoint.update_hypernetwork_checkpoint_metadata(hypernetwork_checkpoint, hypernetwork.hyperparameters_template_metadata_metadata)\n    hypernetwork_checkpoint = sd_hijack_checkpoint.update_hypernetwork_checkpoint_metadata(hypernetwork_checkpoint, hypernetwork.hyperparameters_template_metadata_metadata_values)\n    hypernetwork_checkpoint = sd_hijack_checkpoint.update_hypernetwork_checkpoint_metadata(hypernetwork_checkpoint,", "CodeBleu_score": 25.33236590865546}, {"code": "def save_hypernetwork(hypernetwork, checkpoint, hypernetwork_name, filename)\n\t\n    # backup original attributes\n    original_attributes = {}\n    for attribute in hypernetwork.attributes:\n        original_attributes[attribute] = getattr(hypernetwork, attribute)\n    # update hypernetwork attributes with new checkpoint information\n    hypernetwork.load_state_dict(checkpoint[hypernetwork_name])\n    hypernetwork.update_parameters()\n    # save hypernetwork\n    try:\n        torch.save(hypernetwork, filename)\n    except Exception as e:\n        # restore original attributes\n        for attribute in hypernetwork.attributes:\n            setattr(hypernetwork, attribute, original_attributes[attribute])\n        # re-raise exception\n        raise e\n    # restore original attributes\n    for attribute in hypernetwork.attributes:\n        setattr(hypernetwork, attribute, original_attributes[attribute])\n\n", "CodeBleu_score": 23.08415467977228}, {"code": "def save_hypernetwork(hypernetwork, checkpoint, hypernetwork_name, filename)\n\t\n    hypernetwork_state = hypernetwork.state_dict()\n    hypernetwork_state['checkpoint'] = checkpoint\n    hypernetwork_state['hypernetwork_name'] = hypernetwork_name\n    hypernetwork_state['hypernetwork_class'] = type(hypernetwork).__name__\n    hypernetwork_state['hypernetwork_module'] = hypernetwork.__class__.__module__\n    hypernetwork_state['hypernetwork_file'] = inspect.getfile(hypernetwork.__class__)\n    hypernetwork_state['hypernetwork_line_number'] = inspect.getsourcelines(hypernetwork.__class__)\n    hypernetwork_state['hypernetwork_function'] = inspect.currentframe().f_code.co_name\n    hypernetwork_state['hypernetwork_function_line_number'] = inspect.currentframe().f_lineno\n    hypernetwork_state['hypernetwork_function_code'] = inspect.getframeinfo(inspect.currentframe()).code_context[0]\n    hypernetwork_state['hypernetwork_function_code_line_number'] = inspect.getframeinfo(inspect.currentframe()).lineno\n    hypernetwork_state['hypernetwork_function_code_file'] = inspect.getframeinfo(inspect.currentframe()).filename\n    hypernetwork_state['hypernetwork_function_code_function'] = inspect.getframeinfo(inspect.currentframe()).function\n    hypernetwork_state['hypernetwork_function_code_line_number'] = inspect.getframeinfo(inspect.currentframe()).lineno\n    hypernetwork_state['hypernetwork_function_code_line_text'] = inspect.getframeinfo(inspect.currentframe()).code_context[0]\n    hypernetwork_state['hypernetwork_function_code_line_text_line_number'] = inspect.getframeinfo(inspect.currentframe()).lineno\n    hypernetwork_state['hypernetwork_function_code_line_text_file'] = inspect.getframeinfo(inspect.currentframe()).filename\n    hypernetwork_state['hypernetwork_function_code_line_text_function'] = inspect.getframeinfo(inspect.currentframe()).function\n    hypernetwork_state['hypernetwork_function_code_line_text_line_number'] = inspect.getframeinfo(inspect.currentframe()).lineno\n    hypernetwork_state['hypernetwork_function_code_line_text_line_text'] = inspect.get", "CodeBleu_score": 15.542778694564335}]}
{"instance_id": "3175f835140f", "generation_result": [{"code": "def ema_scope(self, context=None)\n\t\n    if not self.use_ema:\n        yield\n        return\n\n    if context is None:\n        context = self.ema_context\n\n    if context == 'train':\n        self.ema.store(self.parameters())\n        with torch.no_grad():\n            for p in self.parameters():\n                p.copy_(self.ema.retrieve(p))\n\n        if self.ema_log:\n            self.ema.log()\n\n    elif context == 'eval':\n        with torch.no_grad():\n            for p in self.parameters():\n                p.copy_(self.ema.retrieve(p))\n\n    else:\n        raise ValueError(f'unknown context: {context}')\n\n    yield\n\n    if context == 'train':\n        self.ema.restore(self.parameters())\n\n        if self.ema_log:\n            self.ema.log()\n\n", "CodeBleu_score": 41.97961560265043}, {"code": "def ema_scope(self, context=None)\n\t\n    context = default(context, {})\n    ema_enabled = default(context.get('ema_enabled'), self.hparams.ema_enabled)\n    if not ema_enabled:\n        yield\n        return\n    ema_start_step = default(context.get('ema_start_step'), self.hparams.ema_start_step)\n    ema_end_step = default(context.get('ema_end_step'), self.hparams.ema_end_step)\n    ema_decay = default(context.get('ema_decay'), self.hparams.ema_decay)\n    ema_fp32 = default(context.get('ema_fp32'), self.hparams.ema_fp32)\n    ema_scope_name = default(context.get('ema_scope_name'), self.hparams.ema_scope_name)\n    ema_scope_name = default(ema_scope_name, 'ema')\n    if not hasattr(self, ema_scope_name):\n        setattr(self, ema_scope_name, LitEma(self, decay=ema_decay, start_step=ema_start_step, end_step=ema_end_step, fp32=ema_fp32))\n    ema = getattr(self, ema_scope_name)\n    if not ema.enabled:\n        yield\n        return\n    ema_context = context.copy()\n    ema_context['ema_enabled'] = False\n    ema_context['log_text'] = f'EMA weights'\n    with ema.average_parameters(), ema_scope(self, context=ema_context):\n        yield\n    if self.global_step >= ema_start_step and self.global_step <= ema_end_step:\n        ema.update()\n        if self.global_step == ema_end_step:\n            ema.copy_to(self)\n            if ema.fp32:\n                self.to(torch.float32)\n        if ema.fp32:\n            self.to(torch.float16)\n        ema.log(self, context)\n\n", "CodeBleu_score": 31.623663309729967}, {"code": "def ema_scope(self, context=None)\n\t\n    if self.use_ema:\n        if self.ema_context is not None:\n            raise RuntimeError(\n                \"You have already entered an ema_scope. Please exit that scope before entering another one.\"\n            )\n        if context is None:\n            context = \"ema\"\n        self.ema_context = context\n        self.copy_ema_to_model()\n    try:\n        yield\n    finally:\n        if self.use_ema:\n            if self.ema_context != context:\n                raise RuntimeError(\n                    \"You have already exited an ema_scope. Please enter that scope before exiting another one.\"\n                )\n            self.copy_model_to_ema()\n            self.ema_context = None\n\n", "CodeBleu_score": 38.91233419904603}]}
{"instance_id": "a7b5d5cd519a", "generation_result": [{"code": "def p_mean_variance(self, x, t, clip_denoised: bool)\n\t\n    x_recon = self.p_sample(x, t, clip_denoised=clip_denoised)\n    model_mean, posterior_variance = self.q_posterior(x_recon, t)\n    return model_mean, posterior_variance\n\n", "CodeBleu_score": 13.53825541815208}, {"code": "def p_mean_variance(self, x, t, clip_denoised: bool)\n\t\n    model_output = self.p_mean_variance(x, t, clip_denoised)\n    return model_output\n\n", "CodeBleu_score": 6.910899265599993}, {"code": "def p_mean_variance(self, x, t, clip_denoised: bool)\n\t\n    model_output = self.p_mean_variance(x, t, clip_denoised)\n    assert model_output.dtype == x.dtype\n    model_mean, posterior_variance = model_output.split([self.channels, self.channels], dim=1)\n    if clip_denoised:\n        model_mean = torch.clamp(model_mean, -1.0, 1.0)\n    return model_mean, posterior_variance\n\n", "CodeBleu_score": 25.000862256904167}]}
{"instance_id": "a8652fa78473", "generation_result": [{"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss_type == 'l1':\n        if mean:\n            return torch.mean(torch.abs(pred - target))\n        else:\n            return torch.abs(pred - target)\n    elif self.loss_type == 'l2':\n        if mean:\n            return torch.mean((pred - target) ** 2)\n        else:\n            return (pred - target) ** 2\n    else:\n        raise NotImplementedError(f'Loss type {self.loss_type} not implemented.')\n\n", "CodeBleu_score": 34.498416818832176}, {"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss_type == 'l1':\n        loss = torch.abs(pred - target)\n        if mean:\n            loss = loss.mean()\n    elif self.loss_type == 'l2':\n        loss = (pred - target) ** 2\n        if mean:\n            loss = loss.mean()\n        else:\n            loss = loss.sum()\n    else:\n        raise NotImplementedError(f'Loss type {self.loss_type} is not implemented.')\n    return loss\n\n", "CodeBleu_score": 52.46364471086794}, {"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss_type == 'l1':\n        loss = torch.abs(pred - target)\n        if mean:\n            loss = loss.mean()\n    elif self.loss_type == 'l2':\n        loss = (pred - target) ** 2\n        if mean:\n            loss = loss.mean()\n        else:\n            loss = loss.sum()\n    else:\n        raise NotImplementedError(f'Loss type {self.loss_type} not implemented')\n    return loss\n\n", "CodeBleu_score": 52.42396695042022}]}
{"instance_id": "d68e4f061349", "generation_result": [{"code": "def _prior_bpd(self, x_start)\n\t\n    bpd = torch.zeros(x_start.shape[0], device=x_start.device)\n    for i in range(x_start.shape[0]):\n        t = self.num_timesteps - 1\n        mean, log_var = self.model(x_start[[i]], t)\n        mean = mean.flatten(1)\n        log_var = log_var.flatten(1)\n        kl = normal_kl(mean, log_var)\n        bpd[i] = kl.mean() / (math.log(2) * mean.shape[1])\n    return bpd\n\n", "CodeBleu_score": 32.821645515513}, {"code": "def _prior_bpd(self, x_start)\n\t\n    batch_size, *_ = x_start.shape\n    t = torch.full((batch_size,), self.num_timesteps - 1, device=x_start.device, dtype=torch.int64)\n    dist = self(x_start, t)\n    dist_prior = DiagonalGaussianDistribution(dist.mean, dist.log_variance)\n    return normal_kl(dist_prior, DiagonalGaussianDistribution(torch.zeros_like(dist.mean), torch.zeros_like(dist.log_variance))).sum(dim=-1).div(\n        torch.log(torch.tensor(2.0, device=x_start.device))).mean().item()\n\n", "CodeBleu_score": 35.8399853050812}, {"code": "def _prior_bpd(self, x_start)\n\t\n    batch_size = x_start.shape[0]\n    t = torch.full((batch_size,), self.num_timesteps - 1, device=x_start.device, dtype=torch.long)\n    dist = self.model(x_start, t)\n    assert isinstance(dist, DiagonalGaussianDistribution)\n    mean, log_variance = dist.mean, dist.log_variance\n    return torch.mean(normal_kl(mean, log_variance)).div(math.log(2.0))\n\n", "CodeBleu_score": 29.711271134912543}]}
{"instance_id": "c695d93fdf2d", "generation_result": [{"code": "def make_cache(subsection: str) -> diskcache.Cache\n\t\n    cache_dir = os.path.join(data_path, \"cache\")\n    os.makedirs(cache_dir, exist_ok=True)\n    cache = diskcache.Cache(\n        os.path.join(cache_dir, f\"{subsection}_cache\"),\n        size_limit=4 * 1024 ** 3,\n        disk=diskcache.JSONDisk,\n        disk_min_file_size=256,\n    )\n    return cache\n\n", "CodeBleu_score": 49.75580877001953}, {"code": "def make_cache(subsection: str) -> diskcache.Cache\n\t\n    cache_dir = os.path.join(data_path, \"cache\", subsection)\n    if not os.path.exists(cache_dir):\n        os.makedirs(cache_dir)\n    return diskcache.Cache(\n        cache_dir,\n        size_limit=4 * 1024 ** 3,\n        disk=diskcache.JSONDisk,\n        disk_min_file_size=256,\n    )\n\n", "CodeBleu_score": 50.83707211906696}, {"code": "def make_cache(subsection: str) -> diskcache.Cache\n\t\n    cache_path = data_path + f\"/{subsection}\"\n    if not os.path.exists(cache_path):\n        os.makedirs(cache_path)\n    return diskcache.Cache(cache_path, size_limit=4 * 1024 ** 3, disk_min_file_size=256)\n\n", "CodeBleu_score": 49.491148709355265}]}
{"instance_id": "ed4c543f0d38", "generation_result": [{"code": "def cache(subsection)\n\t\n    # Acquire a lock to ensure thread safety\n    cache_lock.acquire()\n    # Check if the cache directory exists\n    if not os.path.exists(data_path + \"/cache\"):\n        os.makedirs(data_path + \"/cache\")\n    # Check if the cache file exists\n    if not os.path.exists(data_path + \"/cache/\" + subsection + \".json\"):\n        convert_old_cached_data(subsection)\n    # Check if the subsection is already in the caches dictionary\n    if subsection in caches:\n        cache_lock.release()\n        return caches[subsection]\n    else:\n        # Create a new cache object\n        caches[subsection] = make_cache(subsection)\n        cache_lock.release()\n        return caches[subsection]\n\n", "CodeBleu_score": 35.319865165880834}, {"code": "def cache(subsection)\n\t\n    global caches\n    global cache_lock\n\n    cache_lock.acquire()\n\n    if subsection not in caches:\n        if os.path.isdir(data_path + \"/cache\"):\n            if os.path.isfile(data_path + \"/cache/\" + subsection + \".cache\"):\n                caches[subsection] = diskcache.Cache(data_path + \"/cache/\" + subsection + \".cache\")\n            else:\n                convert_old_cached_data(subsection)\n        caches[subsection] = make_cache(subsection)\n\n    cache_lock.release()\n\n    return caches[subsection]\n\n", "CodeBleu_score": 35.56297565673993}, {"code": "def cache(subsection)\n\t\n\n    # Retrieve the global caches dictionary.\n    caches = globals()[\"caches\"]\n\n    # Acquire the cache lock.\n    cache_lock.acquire()\n\n    # Check if the subsection is already cached.\n    if subsection in caches:\n        # If so, return the cached object.\n        cache_lock.release()\n        return caches[subsection]\n    else:\n        # If not, check if the cache directory exists.\n        if not os.path.exists(cache_path):\n            # If not, create the cache directory.\n            os.makedirs(cache_path)\n\n        # Check if the cache filename is a valid file.\n        if not os.path.isfile(cache_filename):\n            # If not, convert legacy cached data.\n            convert_old_cached_data()\n\n        # Re-check if the subsection is already cached.\n        if subsection in caches:\n            # If so, return the cached object.\n            cache_lock.release()\n            return caches[subsection]\n        else:\n            # If not, create a new cache object and store it in the caches dictionary.\n            caches[subsection] = make_cache(subsection)\n            cache_lock.release()\n            return caches[subsection]\n\n", "CodeBleu_score": 42.71596360952952}]}
{"instance_id": "ccf5ea2d9fc1", "generation_result": [{"code": "def cached_data_for_file(subsection, title, filename, func)\n\t\n    cache = cache_for_subsection(subsection)\n    cache_entry = cache.get(title)\n    if cache_entry:\n        mtime = os.path.getmtime(filename)\n        if cache_entry[\"mtime\"] < mtime:\n            cache.pop(title)\n            cache_entry = None\n    if not cache_entry:\n        data = func()\n        cache_entry = {\"data\": data, \"mtime\": os.path.getmtime(filename)}\n        cache.set(title, cache_entry)\n        dump_cache(cache)\n    return cache_entry[\"data\"]\n\n", "CodeBleu_score": 31.138260099313044}, {"code": "def cached_data_for_file(subsection, title, filename, func)\n\t\n    cache = cache_for_subsection(subsection)\n    mtime = os.path.getmtime(filename)\n    if title in cache:\n        if mtime > cache[title][\"mtime\"]:\n            cache.pop(title)\n    if title not in cache:\n        cache[title] = {\"mtime\": mtime, \"data\": func()}\n        dump_cache(cache)\n    return cache[title][\"data\"]\n\n", "CodeBleu_score": 20.91097909536511}, {"code": "def cached_data_for_file(subsection, title, filename, func)\n\t\n    cache = cache_for_subsection(subsection)\n    if title in cache:\n        if os.path.getmtime(filename) > cache[title]['mtime']:\n            cache[title]['valid'] = False\n    if not cache[title]['valid']:\n        cache[title]['data'] = func()\n        cache[title]['mtime'] = os.path.getmtime(filename)\n        cache[title]['valid'] = True\n        dump_cache(cache)\n    return cache[title]['data']\n\n", "CodeBleu_score": 18.592388189288318}]}
{"instance_id": "e01a2d4f0ac2", "generation_result": [{"code": "def report(message: str, *, exc_info: bool = False) -> None\n\t\n    record_exception(exc_info=exc_info)\n    for line in message.splitlines():\n        print(\"*** \" + line, file=sys.stderr)\n    if exc_info:\n        print(\"---\", file=sys.stderr)\n        traceback.print_exc(file=sys.stderr)\n\n", "CodeBleu_score": 54.177721019029946}, {"code": "def report(message: str, *, exc_info: bool = False) -> None\n\t\n    record_exception(exc_info=exc_info)\n    for line in message.splitlines():\n        print(f\"*** {line}\", file=sys.stderr)\n    if exc_info:\n        print(\"---\", file=sys.stderr)\n        traceback.print_exc(file=sys.stderr)\n\n", "CodeBleu_score": 50.50127959200967}, {"code": "def report(message: str, *, exc_info: bool = False) -> None\n\t\n    record_exception(exc_info=exc_info)\n    for line in message.splitlines():\n        print(f\"*** {line}\", file=sys.stderr)\n    if exc_info:\n        print(\"---\", file=sys.stderr)\n        print(textwrap.indent(traceback.format_exc(), \"    \"), file=sys.stderr)\n\n", "CodeBleu_score": 59.345039841997725}]}
{"instance_id": "125fb9d2320b", "generation_result": [{"code": "def extract_original_prompts(style: PromptStyle, prompt, negative_prompt)\n\t\n    if style.prompt == \"\" or style.negative_prompt == \"\":\n        return False, prompt, negative_prompt\n    else:\n        try:\n            prompt = prompt.replace(style.prompt, style.text)\n            negative_prompt = negative_prompt.replace(style.negative_prompt, style.negative_text)\n        except:\n            return False, prompt, negative_prompt\n        return True, prompt, negative_prompt\n\n", "CodeBleu_score": 48.65151122278634}, {"code": "def extract_original_prompts(style: PromptStyle, prompt, negative_prompt)\n\t\n    # If either prompt is empty, return False and the original prompts\n    if not prompt or not negative_prompt:\n        return False, prompt, negative_prompt\n\n    # Attempt to extract style text from both prompts\n    try:\n        positive_prompt = style.extract_prompt_text(prompt)\n        negative_prompt = style.extract_prompt_text(negative_prompt)\n    except errors.StyleExtractionError:\n        # If either extraction fails, return False and the original prompts\n        return False, prompt, negative_prompt\n\n    # If both extractions are successful, return True and the extracted prompts\n    return True, positive_prompt, negative_prompt\n\n", "CodeBleu_score": 44.296338670276}, {"code": "def extract_original_prompts(style: PromptStyle, prompt, negative_prompt)\n\t\n\n    if style.positive_prompt == \"\" and style.negative_prompt == \"\":\n        return False, prompt, negative_prompt\n\n    if style.positive_prompt != \"\":\n        try:\n            prompt = prompt.replace(style.positive_prompt, \"\")\n        except:\n            return False, prompt, negative_prompt\n\n    if style.negative_prompt != \"\":\n        try:\n            negative_prompt = negative_prompt.replace(style.negative_prompt, \"\")\n        except:\n            return False, prompt, negative_prompt\n\n    return True, prompt, negative_prompt\n\n", "CodeBleu_score": 52.74631182193256}]}
{"instance_id": "0c5f729fbce2", "generation_result": [{"code": "def random_partial(self, count, n_frames)\n\t\n    if not self.utterances:\n        self._load_utterances()\n    utterances = self.utterance_cycler.sample(count)\n    return [(u, u.random_partial(n_frames)) for u in utterances]\n\n", "CodeBleu_score": 55.89879214825755}, {"code": "def random_partial(self, count, n_frames)\n\t\n    if not self.utterances:\n        self._load_utterances()\n    samples = self.utterance_cycler.sample(count)\n    partials = []\n    for u in samples:\n        partials.append(u.random_partial(n_frames))\n    return partials\n\n", "CodeBleu_score": 47.0234365322299}, {"code": "def random_partial(self, count, n_frames)\n\t\n    if not self.utterances:\n        self._load_utterances()\n    return [(u, u.random_partial(n_frames)) for u in self.utterance_cycler.sample(count)]\n\n", "CodeBleu_score": 42.93472535513073}]}
{"instance_id": "31e8b27b8f94", "generation_result": [{"code": "def normalize_volume(wav, target_dBFS, increase_only=False, decrease_only=False)\n\t\n    if increase_only and decrease_only:\n        raise ValueError(\"Both increase only and decrease only are set\")\n    dBFS_change = target_dBFS - 10 * np.log10(np.mean(wav ** 2))\n    if (dBFS_change < 0 and increase_only) or (dBFS_change > 0 and decrease_only):\n        return wav\n    return wav * (10 ** (dBFS_change / 20))\n\n", "CodeBleu_score": 96.28790819755086}, {"code": "def normalize_volume(wav, target_dBFS, increase_only=False, decrease_only=False)\n\t\n    if increase_only and decrease_only:\n        raise ValueError(\"Both increase only and decrease only are set\")\n    dBFS_change = target_dBFS - 10 * np.log10(np.mean(wav ** 2))\n    if (dBFS_change < 0 and increase_only) or (dBFS_change > 0 and decrease_only):\n        return wav\n    return wav * (10 ** (dBFS_change / 20))\n\n", "CodeBleu_score": 96.28790819755086}, {"code": "def normalize_volume(wav, target_dBFS, increase_only=False, decrease_only=False)\n\t\n    if increase_only and decrease_only:\n        raise ValueError(\"Both increase only and decrease only are set\")\n    dBFS_change = target_dBFS - 10 * np.log10(np.mean(wav ** 2))\n    if (dBFS_change < 0 and increase_only) or (dBFS_change > 0 and decrease_only):\n        return wav\n    return wav * (10 ** (dBFS_change / 20))\n\n", "CodeBleu_score": 96.28790819755086}]}
{"instance_id": "2353831e9033", "generation_result": [{"code": "def load_model(weights_fpath: Path, device=None)\n\t\n    if device is None:\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(\"Available devices: {}\".format(torch.cuda.device_count()))\n    print(\"Using device: {}\".format(device))\n    model = SpeakerEncoder(device, verbose=True)\n    model.load_state_dict(torch.load(weights_fpath, map_location=device)[\"model_state\"])\n    model.eval()\n    return model\n\n", "CodeBleu_score": 44.13531467668533}, {"code": "def load_model(weights_fpath: Path, device=None)\n\t\n    device = device or \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    print(f\"Available device: {device}\")\n\n    global _model\n    _model = SpeakerEncoder(device, verbose=True)\n    _model.load(weights_fpath)\n    _model.eval()\n    print(f\"Loaded model: {_model}\")\n\n", "CodeBleu_score": 27.444969483720065}, {"code": "def load_model(weights_fpath: Path, device=None)\n\t\n\n    # Check if CUDA is available\n    if device is None:\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # Initialize the model\n    _model = SpeakerEncoder(device, verbose=False)\n\n    # Load the model's weights\n    _model.load_state_dict(torch.load(weights_fpath))\n\n    # Set model to eval mode\n    _model.eval()\n\n    print(\"Loaded model:\", weights_fpath)\n    print(\"Model step:\", _model.step)\n    return _model\n\n", "CodeBleu_score": 42.25378409032228}]}
{"instance_id": "3231ad480199", "generation_result": [{"code": "def embed_frames_batch(frames_batch)\n\t\n    if _model is None:\n        raise Exception(\"Model not loaded! Did you forget to call load_model()?\")\n    frames_batch = torch.from_numpy(frames_batch).to(_device)\n    frames_batch = frames_batch.unsqueeze(0)\n    embeddings = _model.embed_frames(frames_batch)\n    embeddings = embeddings.detach().cpu().numpy()\n    return embeddings\n\n", "CodeBleu_score": 60.58146851023448}, {"code": "def embed_frames_batch(frames_batch)\n\t\n    if not _model:\n        raise Exception(\"Model not loaded!\")\n    frames_batch = torch.from_numpy(frames_batch).to(_device)\n    frames_batch = frames_batch.unsqueeze(1)\n    with torch.no_grad():\n        embeds_batch = _model(frames_batch).detach().cpu().numpy()\n    return embeds_batch\n\n", "CodeBleu_score": 54.19308413708428}, {"code": "def embed_frames_batch(frames_batch)\n\t\n    if _model is None:\n        raise Exception(\"Model is not loaded!\")\n    frames_batch = torch.from_numpy(frames_batch).to(_device)\n    frames_batch = frames_batch.unsqueeze(1)\n    with torch.no_grad():\n        embeds_batch = _model.forward(frames_batch).detach().cpu().numpy()\n    return embeds_batch\n\n", "CodeBleu_score": 67.87211864528679}]}
{"instance_id": "117a41716c8d", "generation_result": [{"code": "def compute_partial_slices(n_samples, partial_utterance_n_frames=partials_n_frames,\n                           min_pad_coverage=0.75, overlap=0.5)\n\t\n    assert 0.0 < overlap < 1.0, \"Overlap must be between 0.0 and 1.0\"\n    assert 0.0 < min_pad_coverage < 1.0, \"Minimum pad coverage must be between 0.0 and 1.0\"\n    assert partial_utterance_n_frames > 0, \"Number of frames per partial utterance must be greater than 0\"\n    # Compute the number of frames and frame step, rounding to the nearest integer\n    n_frames = int(np.ceil(n_samples / audio_sample_rate * audio_frame_rate))\n    frame_step = int(np.round(audio_frame_rate * (1.0 - overlap)))\n    # Compute the number of audio and mel samples per partial\n    partial_utterance_n_samples = int(np.ceil(partial_utterance_n_frames / audio_frame_rate * audio_sample_rate))\n    partial_utterance_n_mel_samples = int(np.ceil(partial_utterance_n_frames / mel_frame_rate * mel_sample_rate))\n    # Compute the maximum number of partials that can be fit in the audio\n    max_partial_utterances = int(np.floor((n_samples - partial_utterance_n_samples) / frame_step)) + 1\n    # Compute the number of mel samples that will be padded\n    n_pad_mel_samples = partial_utterance_n_mel_samples - (n_frames % partial_utterance_n_mel_samples)\n    # Compute the mel padding\n    mel_pad = (0, n_pad_mel_samples)\n    # Ensure that the coverage after padding is greater than or equal to the minimum\n    assert n_pad_mel_samples >= 0, \"The coverage after padding is less than the minimum required\"\n    assert n_pad_mel_samples >= int(\n        np.ceil(partial_utterance_n_mel_samples * min_pad_coverage)), \"The coverage after padding is less than the minimum required\"\n    # Compute the number of audio samples that will be padded\n    n_pad_samples = partial_utterance_n_samples - (n_frames % partial_utterance_n_samples)\n    # Compute the audio padding\n    audio_pad = (0,", "CodeBleu_score": 22.93741009622431}, {"code": "def compute_partial_slices(n_samples, partial_utterance_n_frames=partials_n_frames,\n                           min_pad_coverage=0.75, overlap=0.5)\n\t\n    assert 0.0 < overlap < 1.0, \"Overlap must be between 0.0 and 1.0\"\n    assert 0.0 < min_pad_coverage < 1.0, \"Minimum pad coverage must be between 0.0 and 1.0\"\n    assert partial_utterance_n_frames > 0, \"Number of frames per partial utterance must be greater than 0\"\n    assert n_samples > 0, \"Number of samples must be greater than 0\"\n\n    # Calculate frame step, ensuring it does not exceed the number of samples\n    frame_step = int(np.maximum(\n        n_samples * overlap,\n        (n_samples - partial_utterance_n_frames) // (partial_utterance_n_frames - 1)))\n\n    # Calculate the number of frames, ensuring it does not exceed the number of samples\n    n_frames = int(np.minimum(\n        n_samples,\n        (n_samples - 1) // frame_step + 1))\n\n    # Calculate the coverage for the last partial\n    last_partial_coverage = (n_samples - (n_frames - 1) * frame_step) / n_samples\n\n    # If the coverage is below the minimum, we need to add an extra frame\n    # to have the required minimum coverage\n    if last_partial_coverage < min_pad_coverage and n_frames > 1:\n        n_frames -= 1\n        frame_step = int(np.maximum(\n            n_samples * overlap,\n            (n_samples - partial_utterance_n_frames) // (partial_utterance_n_frames - 1)))\n\n    # Compute the slices\n    wav_slices = []\n    mel_slices = []\n    for i in range(n_frames):\n        mel_start = i * frame_step\n        mel_end = min(n_samples, mel_start + partial_utterance_n_frames)\n\n        wav_start = partial_utterance_n_frames * i\n        wav_end = min(n_samples, wav_start + partial_utterance_n_frames)\n\n        mel_slices.append([mel_start, mel_end])\n        wav_slices.append([wav_start, wav_end])\n\n    # If the coverage is below the minimum, we need to remove the last\n    # element", "CodeBleu_score": 25.006921480057244}, {"code": "def compute_partial_slices(n_samples, partial_utterance_n_frames=partials_n_frames,\n                           min_pad_coverage=0.75, overlap=0.5)\n\t\n\n    assert overlap >= 0 and overlap < 1, \"Overlap must be between 0 and 1!\"\n    assert min_pad_coverage >= 0 and min_pad_coverage < 1, \"Min pad coverage must be between 0 and 1!\"\n\n    frame_step = int(sampling_rate * (1.0 - overlap))\n    assert frame_step > 0, \"Frame step must be greater than 0!\"\n    n_frames = int(np.ceil(float(n_samples) / frame_step))\n    assert n_frames > 0, \"Number of frames must be greater than 0!\"\n\n    # Compute the slices\n    wav_slices, mel_slices = [], []\n    for i in range(0, n_frames - partial_utterance_n_frames + 1, partial_utterance_n_frames):\n        mel_range = np.array([i, i + partial_utterance_n_frames])\n        wav_range = mel_range * frame_step\n        wav_slices.append(wav_range)\n        mel_slices.append(mel_range)\n\n    # Evaluate padding\n    last_wav_range = wav_slices[-1]\n    last_mel_range = mel_slices[-1]\n    coverage = (last_wav_range[1] - last_wav_range[0]) / float(n_samples)\n    if coverage < min_pad_coverage and len(wav_slices) > 1:\n        wav_slices = wav_slices[:-1]\n        mel_slices = mel_slices[:-1]\n\n    return wav_slices, mel_slices\n\n", "CodeBleu_score": 48.258091541443285}]}
{"instance_id": "34fe5c864f1c", "generation_result": [{"code": "def plot_embedding_as_heatmap(embed, ax=None, title=\"\", shape=None, color_range=(0, 0.30))\n\t\n    if ax is None:\n        ax = plt.gca()\n    if shape is None:\n        shape = int(np.ceil(np.sqrt(embed.shape[0])))\n    ax.imshow(embed.reshape(shape, shape).T, cmap=cm.get_cmap('magma'), aspect='auto', vmin=color_range[0], vmax=color_range[1])\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.set_title(title)\n    plt.colorbar(fraction=0.046, pad=0.04)\n", "CodeBleu_score": 40.460944332273094}, {"code": "def plot_embedding_as_heatmap(embed, ax=None, title=\"\", shape=None, color_range=(0, 0.30))\n\t\n    if ax is None:\n        ax = plt.gca()\n    if shape is None:\n        height = int(np.sqrt(embed.size(0)))\n        shape = (height, -1)\n    ax.imshow(embed.view(shape).detach().cpu().numpy(), cmap=cm.get_cmap(\"magma\"), aspect=\"auto\", vmin=color_range[0], vmax=color_range[1])\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.set_title(title)\n    plt.colorbar(ax.get_children()[0], ax=ax)\n\n", "CodeBleu_score": 45.25507820228986}, {"code": "def plot_embedding_as_heatmap(embed, ax=None, title=\"\", shape=None, color_range=(0, 0.30))\n\t\n    if ax is None:\n        ax = plt.gca()\n    if shape is None:\n        height = int(np.sqrt(len(embed)))\n        shape = (height, height)\n    if len(embed.shape) == 1:\n        embed = embed.reshape(shape)\n    if len(embed.shape) == 2:\n        embed = embed.reshape(shape)\n    if len(embed.shape) == 3:\n        embed = embed.reshape((shape[0], shape[1], -1))\n    if len(embed.shape) == 4:\n        embed = embed.reshape((shape[0], shape[1], -1))\n    if len(embed.shape) == 5:\n        embed = embed.reshape((shape[0], shape[1], -1))\n    if len(embed.shape) > 5:\n        raise ValueError(\"Embedding dimensionality is too high\")\n    if embed.shape[2] > 3:\n        raise ValueError(\"Embedding dimensionality is too high\")\n    if embed.shape[2] == 1:\n        ax.imshow(embed[:, :, 0], cmap=\"Greys\", vmin=color_range[0], vmax=color_range[1])\n    elif embed.shape[2] == 2:\n        ax.imshow(embed[:, :, 0], cmap=\"Greys\", vmin=color_range[0], vmax=color_range[1])\n        ax.imshow(embed[:, :, 1], cmap=\"Reds\", vmin=color_range[0], vmax=color_range[1])\n    elif embed.shape[2] == 3:\n        ax.imshow(embed[:, :, 0], cmap=\"Greys\", vmin=color_range[0], vmax=color_range[1])\n        ax.imshow(embed[:, :, 1], cmap=\"Reds\", vmin=color_range[0], vmax=color_range[1])\n        ax.imshow(embed[:, :, 2], cmap=\"Blues\", vmin=color_range[0], vmax=color_range[1])\n    else:\n        raise ValueError(\"Embedding dimensionality is too low\")\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.set_title(title)\n    plt.colorbar(ax.get_children()[0], ax=ax)", "CodeBleu_score": 46.841832960748306}]}
{"instance_id": "92e93d50b62b", "generation_result": [{"code": "def similarity_matrix(self, embeds)\n\t\n\n    # Determine number of speakers and utterances per speaker\n    num_speakers = embeds.shape[0]\n    num_utterances = embeds.shape[1]\n\n    # Compute centroids\n    centroids_incl = torch.sum(embeds, dim=1)\n    centroids_incl = centroids_incl / num_utterances\n    centroids_incl = F.normalize(centroids_incl, p=2, dim=1)\n\n    centroids_excl = torch.sum(embeds, dim=1)\n    centroids_excl = centroids_excl / (num_utterances - 1)\n    centroids_excl = F.normalize(centroids_excl, p=2, dim=1)\n\n    # Initialize similarity matrix\n    similarity_matrix = torch.zeros(num_speakers, num_speakers).to(device)\n\n    # Initialize mask matrix\n    mask_matrix = torch.ones(num_speakers, num_speakers).to(device)\n    mask_matrix = torch.triu(mask_matrix, diagonal=1)\n\n    # Compute similarity scores\n    for i in range(num_speakers):\n        for j in range(num_speakers):\n            if i != j:\n                similarity_matrix[i, j] = torch.dot(embeds[i], centroids_incl[j]) - torch.dot(embeds[i],\n                                                                                             centroids_excl[j])\n\n    # Scale and bias similarity matrix\n    similarity_matrix = (similarity_matrix - torch.min(similarity_matrix)) / (\n            torch.max(similarity_matrix) - torch.min(similarity_matrix))\n    similarity_matrix = similarity_matrix * mask_matrix\n    similarity_matrix = similarity_matrix + torch.eye(num_speakers).to(device)\n\n    return similarity_matrix\n\n", "CodeBleu_score": 28.14572784401843}, {"code": "def similarity_matrix(self, embeds)\n\t\n    speakers_per_batch, utterances_per_speaker = embeds.shape[:2]\n    centroids_incl = torch.mean(embeds, dim=1)\n    centroids_incl = F.normalize(centroids_incl, p=2, dim=1)\n    centroids_excl = torch.mean(embeds.clone().detach(), dim=1)\n    centroids_excl[centroids_excl == 0] = 1e-5\n    centroids_excl = F.normalize(centroids_excl, p=2, dim=1)\n    similarity_matrix = torch.zeros((speakers_per_batch, speakers_per_batch), device=embeds.device)\n    mask_matrix = torch.ones((speakers_per_batch, speakers_per_batch), device=embeds.device)\n    for speaker in range(speakers_per_batch):\n        similarity_matrix[speaker] = torch.matmul(embeds[speaker], centroids_incl.T)\n        similarity_matrix[speaker] -= torch.matmul(embeds[speaker], centroids_excl.T)\n        mask_matrix[speaker].fill_diagonal_(0)\n        similarity_matrix[speaker] /= torch.sqrt(torch.matmul(embeds[speaker], embeds[speaker].T) * torch.matmul(centroids_excl, centroids_excl.T))\n    similarity_matrix = similarity_matrix * mask_matrix\n    similarity_matrix = similarity_matrix * 0.5 + 0.5\n    return similarity_matrix\n\n", "CodeBleu_score": 35.57530925623504}, {"code": "def similarity_matrix(self, embeds)\n\t\n    # Get batch size and number of utterances per speaker\n    b, u = embeds.size()\n    # Initialize similarity matrix and mask matrix\n    sim_mat = torch.zeros((b, b), device=embeds.device)\n    mask = torch.ones((b, b), device=embeds.device)\n    # For each speaker in the batch\n    for i in range(b):\n        # Get the speaker embedding\n        embed_i = embeds[i]\n        # Get the number of utterances of the speaker\n        u_i = u[i]\n        # Compute the speaker centroid using the utterances of the speaker\n        centroid_incl = embed_i.sum(dim=0) / u_i\n        # Compute the speaker centroid excluding the utterances of the speaker\n        centroid_excl = (embeds[i] - embed_i).sum(dim=0) / (u_i - 1)\n        # Compute the similarity scores of the speaker to all other speakers\n        sim_mat[i] = 1 - (centroid_incl - centroid_excl).norm(dim=1)\n        # Set the similarity scores of the speaker to itself to zero\n        mask[i, i] = 0\n    # Scale and bias the similarity matrix\n    sim_mat = self.scale * sim_mat + self.bias\n    # Apply the mask\n    sim_mat = sim_mat * mask\n    # Return the similarity matrix\n    return sim_mat\n\n", "CodeBleu_score": 19.861462356007753}]}
{"instance_id": "a6eba54108a1", "generation_result": [{"code": "def loss(self, embeds)\n\t\n    # Number of speakers\n    n_speakers = embeds.shape[0]\n    # Number of utterances per speaker\n    n_utterances = embeds.shape[1]\n    # Reshape the embeddings\n    embeds = embeds.view(n_speakers * n_utterances, -1)\n    # Compute the similarity matrix\n    similarity_matrix = cosine_similarity(embeds, embeds)\n    # Reshape the similarity matrix\n    similarity_matrix = similarity_matrix.view(n_speakers, n_utterances, -1)\n    # Generate the ground truth labels\n    labels = torch.tensor([i for i in range(n_speakers) for _ in range(n_utterances)]).to(device)\n    # Calculate the loss\n    loss = self.loss_fn(similarity_matrix, labels)\n    # Calculate the EER\n    eer = self.get_EER(similarity_matrix, labels)\n    return loss, eer\n\n", "CodeBleu_score": 12.132789189193051}, {"code": "def loss(self, embeds)\n\t\n    # Reshape the embeddings\n    embeds = embeds.view(-1, self.model_embedding_size)\n\n    # Get the number of speakers and utterances per speaker\n    num_speakers, num_utterances = self.get_num_speakers_utterances(embeds)\n\n    # Calculate the similarity matrix\n    similarity_matrix = self.calculate_similarity_matrix(embeds, num_speakers, num_utterances)\n\n    # Calculate the ground truth labels\n    ground_truth_labels = self.calculate_ground_truth_labels(num_speakers, num_utterances)\n\n    # Calculate the loss\n    loss = self.calculate_loss(similarity_matrix, ground_truth_labels)\n\n    # Calculate the Equal Error Rate (EER)\n    eer = self.calculate_eer(similarity_matrix, ground_truth_labels)\n\n    return loss, eer\n\n", "CodeBleu_score": 18.424903888075512}, {"code": "def loss(self, embeds)\n\t\n    # Number of speakers and utterances per speaker\n    num_spkrs, num_uttrs = embeds.shape[0], embeds.shape[1]\n    # Reshape the similarity matrix\n    sim_mat = torch.matmul(embeds, embeds.transpose(0, 1))\n    sim_mat = sim_mat.reshape(num_spkrs, num_uttrs, num_spkrs, num_uttrs)\n    # Generate ground truth labels\n    labels = torch.arange(num_spkrs).repeat(num_uttrs).to(device)\n    # Calculate the loss\n    loss = self.criterion(sim_mat.reshape(-1), labels)\n    # Calculate the EER\n    eer = self.calc_eer(sim_mat.reshape(-1), labels)\n    return loss, eer\n\n", "CodeBleu_score": 7.6780664153396145}]}
{"instance_id": "ac7b74b9d1b1", "generation_result": [{"code": "def log_params(self)\n\t\n    if not self.disable_logging:\n        params_string = \"\"\n        for param_group in self.optimizer.param_groups:\n            for param in param_group['params']:\n                params_string += str(param.name) + \" \" + str(param.data) + \"", "CodeBleu_score": 9.189254833488594}, {"code": "def log_params(self)\n\t\n    if not self.disable_logging:\n        params_string = \"\"\n        for param_name, param_value in self.model_parameters.items():\n            params_string += f\"{param_name} = {param_value}", "CodeBleu_score": 5.160473147677699}, {"code": "def log_params(self)\n\t\n    if self.enable_logging:\n        if self.enable_visdom:\n            vis = visdom.Visdom(env=self.visdom_env)\n            params = self.get_params()\n            param_list = []\n            for key, value in params.items():\n                param_list.append(f'{key}={value}')\n            param_string = '", "CodeBleu_score": 11.434976045765623}]}
{"instance_id": "4726b7864386", "generation_result": [{"code": "def draw_projections(self, embeds, utterances_per_speaker, step, out_fpath=None, max_speakers=10)\n\t\n    if self.use_visdom:\n        self.vis.scatter(embeds, opts=dict(\n            title='Step: %d' % step,\n            xlabel='UMAP Component 1',\n            ylabel='UMAP Component 2',\n            legend=['%d' % i for i in range(embeds.shape[0])],\n            markersize=2,\n            markercolor=self.speaker_colors[:embeds.shape[0]],\n            showlegend=True,\n            width=800,\n            height=800\n        ), win='projections')\n    else:\n        # Flatten the embeddings\n        flattened_embeds = embeds.reshape(embeds.shape[0], -1)\n        # Reduce the dimensionality of the embeddings using UMAP\n        reducer = umap.UMAP(n_neighbors=15,\n                            min_dist=0.1,\n                            n_components=2,\n                            random_state=42)\n        umap_embeds = reducer.fit_transform(flattened_embeds)\n        # Create a plot\n        fig, ax = plt.subplots(figsize=(10, 10))\n        ax.scatter(umap_embeds[:, 0], umap_embeds[:, 1], s=2, c=self.speaker_colors[:umap_embeds.shape[0]])\n        # Set the plot title\n        ax.set_title('Step: %d' % step)\n        # Set the axis labels\n        ax.set_xlabel('UMAP Component 1')\n        ax.set_ylabel('UMAP Component 2')\n        # Set the legend\n        ax.legend(['%d' % i for i in range(umap_embeds.shape[0])])\n        # Save the plot to a file if specified\n        if out_fpath is not None:\n            plt.savefig(out_fpath)\n        # Display the plot\n        plt.show()\n\n", "CodeBleu_score": 41.68273267890202}, {"code": "def draw_projections(self, embeds, utterances_per_speaker, step, out_fpath=None, max_speakers=10)\n\t\n    start = timer()\n    print('Drawing projections...')\n    # Limit the number of speakers if needed\n    if len(embeds) > max_speakers:\n        embeds = embeds[:max_speakers]\n    # Flatten the embeddings\n    embeds = np.reshape(embeds, (len(embeds), -1))\n    # Project the embeddings\n    reducer = umap.UMAP()\n    projected = reducer.fit_transform(embeds)\n    # Assign colors based on speaker identity\n    colors = [self.speaker_to_id[speaker] for speaker in self.speakers]\n    # Draw the plot\n    viz = visdom.Visdom()\n    viz.scatter(projected,\n                opts=dict(\n                    title='Step ' + str(step) + ' - ' + str(len(embeds)) + ' Speakers',\n                    markersize=5,\n                    xtickmin=-10,\n                    xtickmax=10,\n                    ytickmin=-10,\n                    ytickmax=10,\n                    colormap='Paired',\n                    colorlist=colors,\n                    legend=self.speakers,\n                    showlegend=True\n                ),\n                win='projections')\n    if out_fpath is not None:\n        print('Saving projections to ' + out_fpath)\n        viz.save([viz.get_window_by_name('projections')], out_fpath)\n    print('Drawing projections took ' + str(timer() - start) + ' seconds')\n\n", "CodeBleu_score": 41.007490935243}, {"code": "def draw_projections(self, embeds, utterances_per_speaker, step, out_fpath=None, max_speakers=10)\n\t\n    if self.use_umap:\n        if self.use_cuda:\n            embeds = embeds.cpu()\n        # Reduce dimensionality of embeddings using UMAP\n        reducer = umap.UMAP()\n        embeds_2d = reducer.fit_transform(embeds)\n        # Set up plot\n        vis = visdom.Visdom()\n        vis.close(win=None)\n        vis.text(text=f\"Step {step}\", win=\"Step\")\n        # Create plot title\n        title = f\"Step {step}\"\n        if self.use_cuda:\n            title += \" (CUDA)\"\n        # Plot the embeddings\n        vis.scatter(\n            X=embeds_2d,\n            opts=dict(\n                xtickmin=-1,\n                xtickmax=1,\n                ytickmin=-1,\n                ytickmax=1,\n                title=title,\n                markersize=1,\n                markercolor=utterances_per_speaker,\n                colormap=\"nipy_spectral\",\n            ),\n        )\n        # Save plot to file if specified\n        if out_fpath is not None:\n            vis.save([out_fpath])\n        # Limit the number of speakers displayed\n        if len(utterances_per_speaker) > max_speakers:\n            # Sort speakers by number of utterances\n            utterances_per_speaker, speaker_ids = zip(\n                *sorted(zip(utterances_per_speaker, speaker_ids), reverse=True)\n            )\n            # Get the top max_speakers\n            utterances_per_speaker = utterances_per_speaker[:max_speakers]\n            speaker_ids = speaker_ids[:max_speakers]\n            # Get the corresponding embeddings\n            embeds_2d = embeds_2d[:max_speakers]\n            # Get the corresponding utterances\n            utterances_per_speaker = np.array(utterances_per_speaker)\n            # Create a mask for the utterances\n            mask = np.zeros(len(utterances_per_speaker), dtype=bool)\n            # Loop over the utterances\n            for i, utterance_count in enumerate(utterances_per_speaker):\n                # Set the mask to True for the utterances\n                mask[i] = True\n                # Set the remaining utterances to False\n                mask[i + 1 :] = False\n                # Create a mask for the embeddings\n               ", "CodeBleu_score": 38.61234719701342}]}
{"instance_id": "274fa50985c0", "generation_result": [{"code": "def add_speaker_embedding(self, x, speaker_embedding)\n\t\n    self.input_dim_name = x.dim()\n    self.input_dim_index = self.input_dim_name - 2\n    self.input_dim_size = x.size(self.input_dim_index)\n    self.speaker_embedding_dim_index = self.input_dim_index - 1\n    self.speaker_embedding_dim_size = x.size(self.speaker_embedding_dim_index)\n    self.speaker_embedding_dim_name = self.input_dim_name - 1\n    self.speaker_embedding_dim_size_name = self.speaker_embedding_dim_name - 1\n    self.speaker_embedding_dim_index_name = self.speaker_embedding_dim_name - 2\n    speaker_embedding = speaker_embedding.repeat(self.input_dim_size, 1)\n    speaker_embedding = speaker_embedding.transpose(0, 1)\n    speaker_embedding = speaker_embedding.reshape(\n        self.speaker_embedding_dim_size, self.speaker_embedding_dim_index, self.speaker_embedding_dim_size)\n    x = torch.cat((x, speaker_embedding), self.input_dim_index)\n    return x\n\n", "CodeBleu_score": 29.913671065541752}, {"code": "def add_speaker_embedding(self, x, speaker_embedding)\n\t\n    x_dim = x.shape\n    x_dim_names = [f\"x_dim_{i}\" for i in range(len(x_dim))]\n    x_dim_dict = dict(zip(x_dim_names, x_dim))\n    x_dim_dict[\"x_dim_names\"] = x_dim_names\n    x_dim_dict[\"x_dim_len\"] = len(x_dim)\n    x_dim_dict[\"x_dim_names_len\"] = len(x_dim_names)\n    x_dim_dict[\"x_dim_names_str\"] = \", \".join(x_dim_names)\n    x_dim_dict[\"x_dim_names_str_len\"] = len(x_dim_dict[\"x_dim_names_str\"])\n    x_dim_dict[\"x_dim_names_str_len_plus_1\"] = x_dim_dict[\"x_dim_names_str_len\"] + 1\n    x_dim_dict[\"x_dim_names_str_len_plus_2\"] = x_dim_dict[\"x_dim_names_str_len\"] + 2\n    x_dim_dict[\"x_dim_names_str_len_plus_3\"] = x_dim_dict[\"x_dim_names_str_len\"] + 3\n    x_dim_dict[\"x_dim_names_str_len_plus_4\"] = x_dim_dict[\"x_dim_names_str_len\"] + 4\n    x_dim_dict[\"x_dim_names_str_len_plus_5\"] = x_dim_dict[\"x_dim_names_str_len\"] + 5\n    x_dim_dict[\"x_dim_names_str_len_plus_6\"] = x_dim_dict[\"x_dim_names_str_len\"] + 6\n    x_dim_dict[\"x_dim_names_str_len_plus_7\"] = x_dim_dict[\"x_dim_names_str_len\"] + 7\n    x_dim_dict[\"x_dim_names_str_len_plus_8\"] = x_dim_dict[\"x_dim_names_str_len\"] + 8\n    x_dim_", "CodeBleu_score": 11.122648177176952}, {"code": "def add_speaker_embedding(self, x, speaker_embedding)\n\t\n    # Save the dimensions of the input tensor\n    batch_size, seq_len, d_model = x.size()\n\n    # Determine the appropriate dimension index for the speaker embedding\n    if self.speaker_embedding_dim == -1:\n        speaker_embedding_dim = d_model\n    elif self.speaker_embedding_dim == -2:\n        speaker_embedding_dim = d_model - 2\n    else:\n        speaker_embedding_dim = self.speaker_embedding_dim\n\n    # Repeat the speaker embedding to match the length of the input text\n    speaker_embedding = speaker_embedding.unsqueeze(1).repeat(1, seq_len, 1)\n\n    # Reshape the speaker embedding to match the input tensor\n    speaker_embedding = speaker_embedding.view(batch_size, seq_len, speaker_embedding_dim)\n\n    # Transpose the speaker embedding to match the input tensor\n    speaker_embedding = speaker_embedding.transpose(1, 2)\n\n    # Concatenate the input tensor and the speaker embedding along the last dimension\n    x = torch.cat((x, speaker_embedding), dim=2)\n\n    return x\n\n", "CodeBleu_score": 32.74095347376063}]}
{"instance_id": "e6742ea0fe5e", "generation_result": [{"code": "def init_attention(self, encoder_seq_proj)\n\t\n    self.device = next(self.parameters()).device\n    self.cum_attn = torch.zeros((self.batch_size, self.seq_len), device=self.device)\n    self.attn = torch.zeros((self.batch_size, self.seq_len), device=self.device)\n    self.encoder_seq_proj = encoder_seq_proj\n\n", "CodeBleu_score": 25.440471992450608}, {"code": "def init_attention(self, encoder_seq_proj)\n\t\n    self.device = next(self.parameters()).device\n    self.encoder_seq_proj = encoder_seq_proj\n    self.cumulative_attn = torch.zeros(self.encoder_seq_proj.shape[0], self.encoder_seq_proj.shape[1]).to(self.device)\n    self.attn = torch.zeros(self.encoder_seq_proj.shape[0], self.encoder_seq_proj.shape[1]).to(self.device)\n\n", "CodeBleu_score": 17.337377248577983}, {"code": "def init_attention(self, encoder_seq_proj)\n\t\n    device = next(self.parameters()).device\n    self.cumulative_attention = torch.zeros((self.batch_size, self.max_seq_len), device=device)\n    self.attention = torch.zeros((self.batch_size, self.max_seq_len), device=device)\n    self.encoder_seq_proj = encoder_seq_proj\n\n", "CodeBleu_score": 27.095251139686237}]}
{"instance_id": "4e1fc1ed0ed7", "generation_result": [{"code": "def load(self, path, optimizer=None)\n\t\n    if not os.path.exists(path):\n        raise FileNotFoundError(f\"File at {path} does not exist.\")\n    checkpoint = torch.load(path)\n    self.load_state_dict(checkpoint[\"model_state_dict\"])\n    if optimizer is not None and \"optimizer_state_dict\" in checkpoint:\n        optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n    self.to(self.device)\n\n", "CodeBleu_score": 41.46924756707034}, {"code": "def load(self, path, optimizer=None)\n\t\n    if isinstance(path, str):\n        path = Path(path)\n    checkpoint = torch.load(path, map_location=self.device)\n    self.load_state_dict(checkpoint['model'])\n    if optimizer is not None:\n        optimizer.load_state_dict(checkpoint['optimizer'])\n    return checkpoint\n\n", "CodeBleu_score": 50.48374662966917}, {"code": "def load(self, path, optimizer=None)\n\t\n    checkpoint = torch.load(path, map_location=self.device)\n    self.load_state_dict(checkpoint['state_dict'])\n\n    if optimizer is not None:\n        optimizer.load_state_dict(checkpoint['optimizer'])\n\n", "CodeBleu_score": 31.156237886154877}]}
{"instance_id": "178e6aa16c0f", "generation_result": [{"code": "def save(self, path, optimizer=None)\n\t\n    path = Path(path)\n    path.parent.mkdir(parents=True, exist_ok=True)\n    torch.save(self.state_dict(), path)\n    if optimizer is not None:\n        torch.save(optimizer.state_dict(), path.with_suffix('.opt'))\n\n", "CodeBleu_score": 45.49258655509077}, {"code": "def save(self, path, optimizer=None)\n\t\n    path = Path(path)\n    if not path.parent.exists():\n        path.parent.mkdir(parents=True)\n    if optimizer is None:\n        torch.save(self.state_dict(), path)\n    else:\n        torch.save({\n            'model_state_dict': self.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict()\n        }, path)\n\n", "CodeBleu_score": 57.81289106327194}, {"code": "def save(self, path, optimizer=None)\n\t\n    path = Path(path)\n    path.parent.mkdir(parents=True, exist_ok=True)\n    torch.save(self.state_dict(), path)\n    if optimizer is not None:\n        torch.save(optimizer.state_dict(), path.parent / (path.stem + \"_optim\" + path.suffix))\n\n", "CodeBleu_score": 49.52583305796042}]}
{"instance_id": "6528fd80e87b", "generation_result": [{"code": "def english_cleaners(text)\n\t\n    text = unidecode(text)\n    text = normalize_numbers(text)\n    text = re.sub(r\"[^A-Za-z0-9.!?,\\'\\`]\", \" \", text)\n    text = re.sub(r\"\\'s\", \" \\'s\", text)\n    text = re.sub(r\"\\'ve\", \" \\'ve\", text)\n    text = re.sub(r\"n\\'t\", \" n\\'t\", text)\n    text = re.sub(r\"\\'re\", \" \\'re\", text)\n    text = re.sub(r\"\\'d\", \" \\'d\", text)\n    text = re.sub(r\"\\'ll\", \" \\'ll\", text)\n    text = re.sub(r\",\", \" , \", text)\n    text = re.sub(r\"!\", \" ! \", text)\n    text = re.sub(r\"\\.\", \" . \", text)\n    text = re.sub(r\"\\?\", \" ? \", text)\n    text = re.sub(r\"\\s{2,}\", \" \", text)\n    return text.strip()\n\n", "CodeBleu_score": 21.201542530687647}, {"code": "def english_cleaners(text)\n\t\n    text = unidecode(text)\n    text = re.sub(\"[^a-zA-Z.!?]+\", \" \", text)\n    text = re.sub(r'[\" \"]+', \" \", text)\n    text = text.strip().lower()\n    text = normalize_numbers(text)\n    return text\n", "CodeBleu_score": 51.60339609346748}, {"code": "def english_cleaners(text)\n\t\n    text = unidecode(text)\n    text = re.sub(r\"[^A-Za-z0-9.!?']+\", \" \", text)\n    text = re.sub(r\"\\'s\", \" \\'s\", text)\n    text = re.sub(r\"\\'ve\", \" \\'ve\", text)\n    text = re.sub(r\"n\\'t\", \" n\\'t\", text)\n    text = re.sub(r\"\\'re\", \" \\'re\", text)\n    text = re.sub(r\"\\'d\", \" \\'d\", text)\n    text = re.sub(r\"\\'ll\", \" \\'ll\", text)\n    text = re.sub(r\",\", \" , \", text)\n    text = re.sub(r\"!\", \" ! \", text)\n    text = re.sub(r\"\\(\", \" \\( \", text)\n    text = re.sub(r\"\\)\", \" \\) \", text)\n    text = re.sub(r\"\\?\", \" \\? \", text)\n    text = re.sub(r\"\\s{2,}\", \" \", text)\n    return text.strip()\n\n", "CodeBleu_score": 19.857963185424797}]}
{"instance_id": "8a4fe08e0985", "generation_result": [{"code": "def text_to_sequence(text, cleaner_names)\n\t\n    sequence = []\n\n    # Check for curly braces and treat their contents as ARPAbet:\n    while len(text):\n        m = re.match(r\"^(.*?)\\{(.+?)\\}(.*)$\", text)\n        if not m:\n            sequence += _symbols_to_sequence(_clean_text(text, cleaner_names))\n            break\n        sequence += _symbols_to_sequence(_clean_text(m.group(1), cleaner_names))\n        sequence += _arpabet_to_sequence(m.group(2))\n        text = m.group(3)\n\n    # Append EOS token\n    sequence.append(_symbols_to_sequence([\"~\"]))\n    return sequence\n\n", "CodeBleu_score": 75.47185672740333}, {"code": "def text_to_sequence(text, cleaner_names)\n\t\n    sequence = []\n\n    # Check for curly braces and treat their contents as ARPAbet:\n    while len(text):\n        m = re.match(r\"^(.*?)\\{(.+?)\\}(.*)$\", text)\n        if not m:\n            sequence += _symbols_to_sequence(_clean_text(text, cleaner_names))\n            break\n        sequence += _symbols_to_sequence(_clean_text(m.group(1), cleaner_names))\n        sequence += _arpabet_to_sequence(m.group(2))\n        text = m.group(3)\n\n    # Append EOS token\n    sequence.append(_symbol_to_id[\"~\"])\n    return sequence\n\n", "CodeBleu_score": 80.11572938088499}, {"code": "def text_to_sequence(text, cleaner_names)\n\t\n    sequence = []\n\n    # Check for curly braces and treat their contents as ARPAbet:\n    while len(text):\n        m = re.match(r\"^(.*?)\\{(.+?)\\}(.*)$\", text)\n        if not m:\n            sequence += _symbols_to_sequence(_clean_text(text, cleaner_names))\n            break\n        sequence += _symbols_to_sequence(_clean_text(m.group(1), cleaner_names))\n        sequence += _arpabet_to_sequence(m.group(2))\n        text = m.group(3)\n\n    # Append EOS token\n    sequence.append(_symbol_to_id[\"~\"])\n    return sequence\n\n", "CodeBleu_score": 80.11572938088499}]}
{"instance_id": "e1c25a37af1f", "generation_result": [{"code": "def sequence_to_text(sequence)\n\t\n    result = \"\"\n    for symbol_id in sequence:\n        if symbol_id in [0, 1]:\n            continue\n        result += symbols[symbol_id]\n    result = re.sub(r\"}{\", \" \", result)\n    return result.replace(\"}{\", \" \")\n\n", "CodeBleu_score": 31.643230441922967}, {"code": "def sequence_to_text(sequence)\n\t\n    result = \"\"\n    for symbol_id in sequence:\n        if symbol_id in _symbol_to_id:\n            symbol = _id_to_symbol[symbol_id]\n            # Enclose ARPAbet back in curly braces:\n            if len(symbol) > 1 and symbol[0] == \"@\":\n                symbol = \"{%s}\" % symbol[1:]\n            # Replace curly braces in the text with spaces so that\n            # the symbols are separated by spaces.\n            symbol = symbol.replace(\"{\", \" \")\n            symbol = symbol.replace(\"}\", \" \")\n            result += symbol\n    return result.replace(\"}{\", \" \")\n\n", "CodeBleu_score": 54.442873883583}, {"code": "def sequence_to_text(sequence)\n\t\n    result = \"\"\n    for symbol_id in sequence:\n        if symbol_id in [sp.symbols.eos_id, sp.symbols.pad_id]:\n            break\n        result += sp.symbols.id_to_symbol[symbol_id]\n    result = re.sub(r\"{[^{}]*}\", \"\", result)\n    return result.replace(\"}{\", \" \")\n\n", "CodeBleu_score": 40.65459033466035}]}
{"instance_id": "7807cad4b05d", "generation_result": [{"code": "def embed_utterance(fpaths, encoder_model_fpath)\n\t\n    # Load the encoder model if necessary\n    if encoder.loaded_model is None:\n        encoder.load_model(encoder_model_fpath)\n\n    # Compute the speaker embedding of the utterance\n    wav = audio.preprocess_wav(fpaths[0])\n    wav = wav / np.abs(wav).max() * 0.999\n    mel_spectrogram = audio.melspectrogram(wav).astype(np.float32).T\n    embedding = encoder.embed_utterance(mel_spectrogram)\n\n    # Save the embedding\n    np.save(fpaths[1], embedding, allow_pickle=False)\n\n", "CodeBleu_score": 48.378722379950716}, {"code": "def embed_utterance(fpaths, encoder_model_fpath)\n\t\n    # Load the encoder model if necessary\n    if encoder.loaded_model is None:\n        encoder.load_model(encoder_model_fpath)\n\n    # Load the wav from the filepath\n    wav = encoder.preprocess_wav(fpaths[0])\n\n    # Compute the speaker embedding\n    embed = encoder.embed_utterance(wav)\n\n    # Save the embedding to the output filepath\n    np.save(fpaths[1], embed, allow_pickle=False)\n\n", "CodeBleu_score": 49.028002882876656}, {"code": "def embed_utterance(fpaths, encoder_model_fpath)\n\t\n    # Load the encoder model if it is not loaded already\n    if encoder.loaded_model is None:\n        encoder.load_model(encoder_model_fpath)\n\n    # Compute the speaker embedding of the utterance\n    wav = audio.preprocess_wav(fpaths[0])\n    embed = encoder.embed_utterance(wav)\n\n    # Save the embedding to the output file\n    np.save(fpaths[1], embed, allow_pickle=False)\n\n", "CodeBleu_score": 48.35547243651767}]}
{"instance_id": "9f860273a325", "generation_result": [{"code": "def create_embeddings(synthesizer_root: Path, encoder_model_fpath: Path, n_processes: int)\n\t\n\n    # Create a new directory to store the embeddings\n    embed_dir = synthesizer_root.joinpath(\"embeds\")\n    embed_dir.mkdir(exist_ok=True)\n\n    # Get the paths to the metadata files\n    train_meta = synthesizer_root.joinpath(\"train.txt\")\n    val_meta = synthesizer_root.joinpath(\"val.txt\")\n\n    # Get the paths to the audio files\n    train_audio_dir = synthesizer_root.joinpath(\"train\")\n    val_audio_dir = synthesizer_root.joinpath(\"val\")\n\n    # Get the paths to the encoder model and the synthesizer settings\n    encoder_model_fpath = encoder_model_fpath\n    encoder_settings = synthesizer_root.joinpath(\"encoder_settings.json\")\n\n    # Verify that the required directories and files exist\n    assert train_meta.exists() and train_audio_dir.exists() and encoder_settings.exists(), \\\n        f\"Could not find files at {train_meta}, {train_audio_dir}, or {encoder_settings}.\"\n\n    # Read the metadata files\n    with open(train_meta, encoding=\"utf-8\") as f:\n        train_meta = [line.strip().split(\"|\") for line in f]\n    with open(val_meta, encoding=\"utf-8\") as f:\n        val_meta = [line.strip().split(\"|\") for line in f]\n\n    # Prepare the input arguments for the multiprocessing pool\n    train_args = [(encoder_model_fpath, encoder_settings, embed_dir, train_meta, train_audio_dir, i) for i in\n                  range(len(train_meta))]\n    val_args = [(encoder_model_fpath, encoder_settings, embed_dir, val_meta, val_audio_dir, i) for i in\n                range(len(val_meta))]\n\n    # Create a multiprocessing pool and process the audio files\n    with Pool(n_processes) as pool:\n        list(\n            tqdm(pool.imap(partial(process_meta, synthesizer_root=synthesizer_root), train_args),\n                 total=len(train_args),\n                 desc=\"Generating embeddings for training files\",\n                 leave=False))\n        list(\n            tqdm(pool.imap(partial(process_meta, synthes", "CodeBleu_score": 43.53541312056124}, {"code": "def create_embeddings(synthesizer_root: Path, encoder_model_fpath: Path, n_processes: int)\n\t\n    # Verify that the encoder model file exists\n    assert encoder_model_fpath.exists()\n\n    # Verify that the synthesizer root directory exists\n    assert synthesizer_root.exists()\n\n    # Verify that the synthesizer root contains the metadata file\n    metadata_fpath = synthesizer_root.joinpath(\"train.txt\")\n    assert metadata_fpath.exists()\n\n    # Verify that the synthesizer root contains the synthesized mel spectrograms directory\n    mel_dir = synthesizer_root.joinpath(\"mels\")\n    assert mel_dir.exists()\n\n    # Verify that the synthesizer root contains the synthesized audio directory\n    audio_dir = synthesizer_root.joinpath(\"audio\")\n    assert audio_dir.exists()\n\n    # Verify that the synthesizer root contains the synthesized alignments directory\n    align_dir = synthesizer_root.joinpath(\"alignments\")\n    assert align_dir.exists()\n\n    # Verify that the synthesizer root contains the synthesized embedings directory\n    embed_dir = synthesizer_root.joinpath(\"embeds\")\n    if not embed_dir.exists():\n        embed_dir.mkdir(exist_ok=True)\n\n    # Read the metadata file\n    with open(metadata_fpath, encoding=\"utf-8\") as metadata_file:\n        metadata = [line.strip().split(\"|\") for line in metadata_file]\n\n    # Create a list of tuples containing the input and output paths for each audio file\n    # The input audio file is the mel spectrogram, the output path is the embeddings\n    audio_fpaths = [(mel_dir.joinpath(m[1] + \".npy\"), embed_dir.joinpath(m[1] + \".npy\")) for m in metadata]\n\n    # Create a pool of workers to process the audio files in parallel\n    pool = Pool(processes=n_processes)\n\n    # Create a partial function that encodes the audio file\n    # The partial function is used to pass the encoder model path to the function\n    encode_audio_file = partial(encode_audio, encoder_model_fpath=encoder_model_fpath)\n\n    # Create a partial function that generates embeddings from the audio file\n    # The partial function is used to pass the synthesizer root directory to the function\n    generate_embeddings = partial(generate_embedding, synthesizer_root=synthesizer", "CodeBleu_score": 52.91749916274841}, {"code": "def create_embeddings(synthesizer_root: Path, encoder_model_fpath: Path, n_processes: int)\n\t\n    # Verify that the encoder model file exists\n    assert encoder_model_fpath.exists()\n\n    # Create a new directory to store the embeddings\n    embedding_dir = synthesizer_root.joinpath(\"embeddings\")\n    embedding_dir.mkdir(exist_ok=True)\n\n    # Create a list of audio files\n    metadata_fpath = synthesizer_root.joinpath(\"train.txt\")\n    assert metadata_fpath.exists()\n    with metadata_fpath.open(\"r\", encoding=\"utf-8\") as metadata_file:\n        metadata = [line.split(\"|\") for line in metadata_file]\n    audio_fpaths = [synthesizer_root.joinpath(m[0]) for m in metadata]\n    assert all(f.exists() for f in audio_fpaths)\n\n    # Create a list of embedding file paths\n    embedding_fpaths = [embedding_dir.joinpath(f\"{f.stem}.npy\") for f in audio_fpaths]\n\n    # Create a pool of workers to process the audio files\n    pool = Pool(n_processes)\n    func = partial(_process_utterance, encoder_model_fpath=encoder_model_fpath)\n    list(\n        tqdm(\n            pool.imap(func, zip(audio_fpaths, embedding_fpaths)),\n            total=len(audio_fpaths),\n            desc=\"Processing audio files\",\n        )\n    )\n    pool.close()\n    pool.join()\n\n", "CodeBleu_score": 56.52782624824053}]}
{"instance_id": "9eb0abda14a9", "generation_result": [{"code": "def eval_model(attention, mel_prediction, target_spectrogram, input_seq, step,\n               plot_dir, mel_output_dir, wav_dir, sample_num, loss, hparams)\n\t\n    # Save attention plot\n    attention_path = os.path.join(plot_dir, \"step-{}_attention.png\".format(step))\n    save_attention(attention, input_seq, attention_path)\n\n    # Save predicted mel spectrogram to disk (for manual inspection)\n    mel_output_filename = \"step-{}_predicted-mel-spectrogram.png\".format(step)\n    mel_output_path = os.path.join(mel_output_dir, mel_output_filename)\n    save_mel_spectrogram(mel_prediction, mel_output_path)\n\n    # Save target mel spectrogram to disk (for manual inspection)\n    mel_output_filename = \"step-{}_target-mel-spectrogram.png\".format(step)\n    mel_output_path = os.path.join(mel_output_dir, mel_output_filename)\n    save_mel_spectrogram(target_spectrogram, mel_output_path)\n\n    # Save audio to disk\n    if hparams.predict_linear:\n        # If the model predicts linear spectrograms then we can save them to disk and\n        # use a vocoder to generate audio from them.\n        wav_path = os.path.join(wav_dir, \"step-{}_predicted.wav\".format(step))\n        save_wav(wav_path, audio.inv_spectrogram(mel_prediction.data.cpu().numpy().T))\n\n        # If the model predicts spectrograms then we can save them to disk as a plot.\n        wav_path = os.path.join(wav_dir, \"step-{}_target.wav\".format(step))\n        save_wav(wav_path, audio.inv_spectrogram(target_spectrogram.data.cpu().numpy().T))\n\n    # Save plot of mel spectrogram predictions and target spectrograms\n    plot_spectrogram(\n        [mel_prediction.data.cpu().numpy(), target_spectrogram.data.cpu().numpy()],\n        [\"Synthesizer Prediction\", \"Target Mel\"],\n        os.path.join(plot_dir, \"step-{}_mel_spectrogram.png\".format(step)),\n        figsize=(10, 10))\n\n    # Print some examples of the inputs\n    print(\"", "CodeBleu_score": 38.49880186494903}, {"code": "def eval_model(attention, mel_prediction, target_spectrogram, input_seq, step,\n               plot_dir, mel_output_dir, wav_dir, sample_num, loss, hparams)\n\t\n    # Save attention plot\n    attention_path = os.path.join(plot_dir, \"step-{}-attention.png\".format(step))\n    plot_attention(attention, input_seq, attention_path)\n\n    # Save predicted mel spectrogram to disk (debug)\n    mel_output_fpath = os.path.join(mel_output_dir, \"step-{}-predicted-mel-spectrogram.png\".format(step))\n    plot_spectrogram(mel_prediction, mel_output_fpath, title=\"{} Predicted Mel-Spectrogram at Step #{}\".format(hparams.model_type, step),\n                     target_spectrogram=target_spectrogram,\n                     max_len=target_spectrogram.size(1))\n\n    # Save target mel spectrogram to disk (control purposes)\n    mel_output_fpath = os.path.join(mel_output_dir, \"step-{}-target-mel-spectrogram.png\".format(step))\n    plot_spectrogram(target_spectrogram, mel_output_fpath, title=\"{} Target Mel-Spectrogram at Step #{}\".format(hparams.model_type, step),\n                     max_len=target_spectrogram.size(1))\n\n    # Save audio\n    if hparams.model_type == 'Tacotron':\n        # Save real and predicted audio to disk\n        _, mel_outputs_postnet, _, alignments = mel_prediction\n        real_and_predicted_audio = [real_and_predicted_mel_to_wav(mel_outputs_postnet[i].data.cpu().numpy(),\n                                                                 alignments[i].data.cpu().numpy(),\n                                                                 hparams) for i in range(sample_num)]\n        for i, ap in enumerate(real_and_predicted_audio):\n            ap.export(os.path.join(wav_dir, \"step-{}-audio-sample-{}.wav\".format(step, i)), format=\"wav\")\n\n    # Save plot of predicted vs target mel-spectrograms (control purposes)\n    plot_fpath = os.path.join(plot_dir, \"step-{}-mel-plot.png\".format(step))\n    plot_spectrogram(mel_prediction, plot_fpath, title=\"Predicted vs Target Mel-Spectrograms at Step #{}\".format(step),\n                     target", "CodeBleu_score": 41.00471249739345}, {"code": "def eval_model(attention, mel_prediction, target_spectrogram, input_seq, step,\n               plot_dir, mel_output_dir, wav_dir, sample_num, loss, hparams)\n\t\n    # Save attention plot\n    attention_path = os.path.join(plot_dir, \"step-{}-attention.png\".format(step))\n    save_attention(attention, attention_path)\n\n    # Save predicted mel spectrogram to disk (debug)\n    mel_output_fpath = os.path.join(mel_output_dir, \"step-{}-predicted-mel-spectrogram.npy\".format(step))\n    np.save(mel_output_fpath, mel_prediction.data.cpu().numpy(), allow_pickle=False)\n\n    # Save griffin lim inverted wav for debug (mel -> wav)\n    if hparams.vocoder == \"WaveGlow\":\n        wav = audio.inv_mel_spectrogram(mel_prediction.data.cpu().numpy().T, hparams)\n    elif hparams.vocoder == \"MelGAN\":\n        wav = audio.inv_mel_spectrogram(mel_prediction.data.cpu().numpy().T, hparams)\n    else:\n        raise ValueError(\"Vocoder not supported: {}\".format(hparams.vocoder))\n\n    wav = audio.inv_preemphasis(wav, hparams.preemphasis, hparams.preemphasize)\n    wav = audio.inv_spectrogram_tensorflow(wav, hparams)\n    wav = audio.inv_gain(wav, hparams)\n    wav = wav[:-(hparams.hop_size * 10)]\n    wav_fpath = os.path.join(wav_dir, \"step-{}-predicted-waveform.wav\".format(step))\n    audio.save_wav(wav, wav_fpath, sr=hparams.sample_rate)\n\n    # Save real and predicted mel-spectrogram plot to disk (control purposes)\n    spec_fpath = os.path.join(plot_dir, \"step-{}-mel-spectrogram.png\".format(step))\n    title_str = \"{}, {}, step={}, loss={:.5f}\".format(datetime.now(), hparams.model, step, loss)\n    plot_spectrogram(mel_prediction.data.cpu().numpy(), target_spectrogram.data.cpu().numpy(),\n                     input_seq, title=title_str, output_path=spec_fpath)\n\n    # Print some examples of inputs\n    print(\"\n", "CodeBleu_score": 46.14477978516135}]}
{"instance_id": "6bad4f500286", "generation_result": [{"code": "def num_params(self)\n\t\n    model_parameters = filter(lambda p: p.requires_grad, self.parameters())\n    params = sum([np.prod(p.size()) for p in model_parameters])\n    print('Trainable parameters: %.3fM' % (params / 1e6))\n\n", "CodeBleu_score": 39.44487323594}, {"code": "def num_params(self)\n\t\n    params = filter(lambda p: p.requires_grad, self.parameters())\n    params = sum([np.prod(p.size()) for p in params]) / 1000000\n    print('Trainable Parameters: %.3fM' % params)\n", "CodeBleu_score": 41.70228697618791}, {"code": "def num_params(self)\n\t\n    params = filter(lambda p: p.requires_grad, self.parameters())\n    params = sum([np.prod(p.size()) for p in params]) / 1e6\n    print('Trainable Parameters: %.3fM' % params)\n\n", "CodeBleu_score": 41.12598466187338}]}
{"instance_id": "78b876054bd4", "generation_result": [{"code": "def pad_tensor(self, x, pad, side='both')\n\t\n    if side == 'both':\n        x = F.pad(x, (pad, pad))\n    elif side == 'before':\n        x = F.pad(x, (pad, 0))\n    elif side == 'after':\n        x = F.pad(x, (0, pad))\n    return x\n\n", "CodeBleu_score": 20.71009897872612}, {"code": "def pad_tensor(self, x, pad, side='both')\n\t\n    if side == 'both':\n        x = F.pad(x, (pad, pad))\n    elif side == 'before':\n        x = F.pad(x, (pad, 0))\n    elif side == 'after':\n        x = F.pad(x, (0, pad))\n    return x\n\n", "CodeBleu_score": 20.71009897872612}, {"code": "def pad_tensor(self, x, pad, side='both')\n\t\n    if side == 'both':\n        x = F.pad(x, (pad, pad))\n    elif side == 'before':\n        x = F.pad(x, (pad, 0))\n    elif side == 'after':\n        x = F.pad(x, (0, pad))\n    return x\n\n", "CodeBleu_score": 20.71009897872612}]}
{"instance_id": "a7ed1715a3df", "generation_result": [{"code": "def save_attention(attn, path)\n\t\n    import matplotlib.pyplot as plt\n    fig = plt.figure(figsize=(10,10))\n    ax = fig.add_subplot(1, 1, 1)\n    ax.matshow(attn, cmap='viridis')\n    ax.set_xticklabels([''] + ['<sos>'] + [t.lower() for t in src_vocab] + ['<eos>'], rotation=90)\n    ax.set_yticklabels([''] + [t.lower() for t in tgt_vocab] + ['<eos>'])\n    plt.savefig(path)\n    plt.close()\n\n", "CodeBleu_score": 44.14772532078205}, {"code": "def save_attention(attn, path)\n\t\n    import matplotlib.pyplot as plt\n    fig, ax = plt.subplots(figsize=(10, 10))\n    ax.matshow(attn, cmap='viridis')\n    ax.set_xticklabels([''] + ['<sos>'] + [t.lower() for t in attn.get_text()] + ['<eos>'], rotation=90)\n    ax.set_yticklabels([''] + attn.get_text())\n    plt.savefig(path, bbox_inches='tight')\n    plt.close()\n\n", "CodeBleu_score": 43.25459534656704}, {"code": "def save_attention(attn, path)\n\t\n    import matplotlib.pyplot as plt\n    fig, ax = plt.subplots()\n    ax.matshow(attn)\n    ax.set_xticklabels([''] + ['<sos>'] + [''] + list(map(str, range(1, attn.shape[0]))))\n    ax.set_yticklabels([''] + list(map(str, range(1, attn.shape[1]))))\n    plt.xlabel('Source')\n    plt.ylabel('Target')\n    plt.savefig(path)\n    plt.close()\n\n", "CodeBleu_score": 42.2515034795514}]}
{"instance_id": "19375c9f44a2", "generation_result": [{"code": "def plot(array)\n\t\n    # Create figure\n    fig = plt.figure(figsize=(10, 7))\n    # Create subplot\n    ax = fig.add_subplot(1, 1, 1)\n    # Customize axis labels and tick parameters\n    ax.set_xlabel('x', fontsize=14, color='red')\n    ax.set_ylabel('y', fontsize=14, color='red')\n    ax.tick_params(axis='both', colors='red', labelsize=14)\n    # Plot array\n    plt.plot(array)\n    # Display plot\n    plt.show()\n\n", "CodeBleu_score": 30.40112103167677}, {"code": "def plot(array)\n\t\n    fig = plt.figure(figsize=(8, 6))\n    ax = fig.add_subplot(1, 1, 1)\n    ax.spines['top'].set_color('none')\n    ax.spines['bottom'].set_color('none')\n    ax.spines['left'].set_color('none')\n    ax.spines['right'].set_color('none')\n    ax.tick_params(labelcolor='w', top='off', bottom='off', left='off', right='off')\n    ax.set_xlabel('x', fontsize=18, color='w')\n    ax.set_ylabel('y', fontsize=18, color='w')\n    ax.set_title('Plot of Array', fontsize=22, color='w')\n    plt.plot(array, color='w')\n    plt.show()\n\n", "CodeBleu_score": 27.730168035376913}, {"code": "def plot(array)\n\t\n    fig = plt.figure(figsize=(10, 10))\n    ax = fig.add_subplot(111)\n    ax.plot(array, color='b', linewidth=3)\n    ax.set_xlabel('x', fontsize=16, color='b')\n    ax.set_ylabel('y', fontsize=16, color='b')\n    ax.tick_params(axis='x', colors='b', labelsize=16)\n    ax.tick_params(axis='y', colors='b', labelsize=16)\n    plt.show()\n\n", "CodeBleu_score": 28.130318404785825}]}
{"instance_id": "ff94c425920c", "generation_result": [{"code": "def write_to_csv(image_name, prediction, confidence)\n\t\n    with open('predictions.csv', 'a', newline='') as csvfile:\n        fieldnames = ['image_name', 'prediction', 'confidence']\n        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n        if csvfile.tell() == 0:\n            writer.writeheader()\n        writer.writerow({'image_name': image_name, 'prediction': prediction, 'confidence': confidence})\n\n", "CodeBleu_score": 45.20554409330242}, {"code": "def write_to_csv(image_name, prediction, confidence)\n\t\n    csv_file = \"predictions.csv\"\n    if not os.path.exists(csv_file):\n        with open(csv_file, \"w\", newline=\"\") as f:\n            writer = csv.DictWriter(f, fieldnames=[\"Image\", \"Prediction\", \"Confidence\"])\n            writer.writeheader()\n    with open(csv_file, \"a\", newline=\"\") as f:\n        writer = csv.writer(f)\n        writer.writerow([image_name, prediction, confidence])\n\n", "CodeBleu_score": 52.93126541135861}, {"code": "def write_to_csv(image_name, prediction, confidence)\n\t\n    with open('predictions.csv', 'a') as csvfile:\n        fieldnames = ['image_name', 'prediction', 'confidence']\n        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n        if csvfile.tell() == 0:\n            writer.writeheader()\n        writer.writerow({'image_name': image_name, 'prediction': prediction, 'confidence': confidence})\n\n", "CodeBleu_score": 44.737902641165505}]}
{"instance_id": "436161602009", "generation_result": [{"code": "def export_torchscript(model, im, file, optimize, prefix=colorstr(\"TorchScript:\"))\n\t\n    LOGGER.info(\"%s starting export with torch %s...\", prefix, torch.__version__)\n    f = file.with_suffix(\".torchscript.pt\")\n    f.parent.mkdir(parents=True, exist_ok=True)\n    ts = torch.jit.trace(model, im, strict=False)\n    if optimize:\n        LOGGER.info(\"%s optimizing with torch %s...\", prefix, torch.__version__)\n        ts = optimize_for_mobile(ts)\n    ts.save(f)\n    LOGGER.info(\"%s export success, saved as %s\", prefix, f)\n    return f\n\n", "CodeBleu_score": 39.21763762270815}, {"code": "def export_torchscript(model, im, file, optimize, prefix=colorstr(\"TorchScript:\"))\n\t\n    LOGGER.info(f\"{prefix} starting export with torch {torch.__version__}...\")\n    f = file.with_suffix(\".torchscript.pt\")\n    if f.exists():\n        f.unlink()\n    try:\n        ts = torch.jit.trace(model, im, strict=False)\n        if optimize:\n            ts = optimize_for_mobile(ts)\n        ts.save(f)\n    except Exception as e:\n        LOGGER.error(f\"{prefix} export failure: {e}\")\n        return None\n    LOGGER.info(f\"{prefix} export success, saved as {f} ({file_size(f):.1f} MB)\")\n    return f\n\n", "CodeBleu_score": 47.2085180119702}, {"code": "def export_torchscript(model, im, file, optimize, prefix=colorstr(\"TorchScript:\"))\n\t\n    LOGGER.info(f\"{prefix} starting export with torch {torch.__version__}...\")\n    f = file.with_suffix(\".torchscript.pt\")\n    f = f.relative_to(Path.cwd())\n    LOGGER.info(f\"{prefix} export success, saved as {f}\")\n    if optimize:\n        LOGGER.info(f\"{prefix} starting optimize for mobile model with torch {torch.__version__}...\")\n        f = file.with_suffix(\".pt\")\n        f = f.relative_to(Path.cwd())\n        traced = torch.jit.trace(model, im, strict=False)\n        traced = optimize_for_mobile(traced)\n        LOGGER.info(f\"{prefix} optimize for mobile model complete.\")\n        LOGGER.info(f\"{prefix} starting save for mobile model with torch {torch.__version__}...\")\n        traced.save(f)\n        LOGGER.info(f\"{prefix} save for mobile model complete.\")\n        return f, None\n    else:\n        traced = torch.jit.trace(model, im, strict=False)\n        traced.save(f)\n        return f, None\n\n", "CodeBleu_score": 48.738243625934174}]}
{"instance_id": "78335487ac1c", "generation_result": [{"code": "def export_coreml(model, im, file, int8, half, nms, prefix=colorstr(\"CoreML:\")):\n\t\n    check_requirements(('coremltools'), prefix)\n    LOGGER.info(\n        \"%s Exporting to CoreML%s%s%s%s\"\n        % (\n            prefix,\n            \" with NMS support\" if nms else \"\",\n            \" with quantization\" if int8 else \"\",\n            \" in FP16\" if half else \"\",\n            \" from %s\" % file if file else \"\",\n        )\n    )\n    if half:\n        im = im.half()\n    if nms:\n        model = iOSModel(model, file)\n    model = torch.jit.trace(model, im)\n    mlmodel = ct.convert(model, inputs=[ct.ImageType(name=\"image\", shape=im.shape)])\n    if int8:\n        mlmodel = mlmodel.quantize()\n    if file:\n        file = Path(file).stem + \".mlmodel\"\n    if not file:\n        file = \"model.mlmodel\"\n    mlmodel.save(file)\n    return file, mlmodel\n\n", "CodeBleu_score": 38.168376652829515}, {"code": "def export_coreml(model, im, file, int8, half, nms, prefix=colorstr(\"CoreML:\")):\n\t\n    try:\n        import coremltools as ct\n    except ImportError:\n        LOGGER.error(\"CoreML export requires `coremltools` - see https://github.com/apple/coremltools\")\n        raise SystemExit(1)\n    if half:\n        model.half()\n    if int8:\n        model.qconfig = torch.quantization.get_default_qconfig(\"qnnpack\")\n        torch.backends.quantized.engine = \"qnnpack\"\n        model.eval()\n        torch.quantization.prepare(model, inplace=True)\n    if nms:\n        LOGGER.info(\"Wrapping model with NMS support.\")\n        model = Detect(model, conf_thres=0.25, iou_thres=0.45)\n    traced_model = torch.jit.trace(model, im, strict=False)\n    if int8:\n        LOGGER.info(\"Quantizing model for CoreML export.\")\n        torch.quantization.convert(model, inplace=True)\n    if half:\n        traced_model = traced_model.half()\n    coreml_model = ct.convert(\n        traced_model,\n        inputs=[ct.ImageType(name=\"image\", shape=im.shape, scale=1 / 255.0, bias=[0, 0, 0])],\n    )\n    file = Path(file).with_suffix(\".mlmodel\")\n    coreml_model.save(file)\n    LOGGER.info(f\"Export complete ({file_size(file)}).\")\n    return file, coreml_model\n\n", "CodeBleu_score": 45.17993019236282}, {"code": "def export_coreml(model, im, file, int8, half, nms, prefix=colorstr(\"CoreML:\")):\n\t\n    try:\n        import coremltools as ct\n    except ImportError as e:\n        LOGGER.error(\n            \"CoreML export requires 'coremltools' to be installed. \"\n            \"Install it with 'pip install coremltools' or 'pip3 install coremltools'.\"\n        )\n        raise e\n    try:\n        import onnxmltools as onnxmltools\n    except ImportError as e:\n        LOGGER.error(\n            \"CoreML export requires 'onnxmltools' to be installed. \"\n            \"Install it with 'pip install onnxmltools' or 'pip3 install onnxmltools'.\"\n        )\n        raise e\n\n    # Check if NMS is supported\n    if nms and not isinstance(model, Detect):\n        LOGGER.error(\n            \"NMS is only supported for YOLOv5 Detect models. \"\n            \"Exporting as CoreML without NMS.\"\n        )\n        nms = False\n\n    # Check if model is in eval mode\n    if model.training:\n        LOGGER.warning(\n            \"Model is in training mode. Converting to eval mode. \"\n            \"This may affect accuracy and is not recommended.\"\n        )\n        model.eval()\n\n    # Check if model is a Detect model\n    if not isinstance(model, Detect):\n        LOGGER.error(\n            \"CoreML export is only supported for YOLOv5 Detect models. \"\n            \"Exporting as CoreML without NMS.\"\n        )\n        nms = False\n\n    # Check if model is a Classification model\n    if isinstance(model, ClassificationModel):\n        LOGGER.error(\n            \"CoreML export is not supported for YOLOv5 Classification models. \"\n            \"Exporting as CoreML without NMS.\"\n        )\n        nms = False\n\n    # Check if model is a Segmentation model\n    if isinstance(model, SegmentationModel):\n        LOGGER.error(\n            \"CoreML export is not supported for YOLOv5 Segmentation models. \"\n            \"Exporting as CoreML without NMS.\"\n        )\n        nms = False\n\n    # Check if model is a Detection model\n    if isinstance(model, DetectionModel):\n        LOGGER.error(\n            \"CoreML export is not supported for YOLOv5 Detection models. \"\n            \"Exporting as CoreML without NMS.\"\n        )\n        nms = False\n\n    # Check if model is a Tiny model\n    if model.yaml.get(\"nc\") == 1:\n        LOGGER.error(\n            \"", "CodeBleu_score": 21.76749835836109}]}
{"instance_id": "80233cf71580", "generation_result": [{"code": "def generate_individual(input_ranges, individual_length)\n\t\n    individual = []\n    for i in range(individual_length):\n        individual.append(random.uniform(input_ranges[i][0], input_ranges[i][1]))\n    return individual\n\n", "CodeBleu_score": 47.630972577458174}, {"code": "def generate_individual(input_ranges, individual_length)\n\t\n    individual = []\n    for i in range(individual_length):\n        individual.append(random.uniform(input_ranges[i][0], input_ranges[i][1]))\n    return individual\n\n", "CodeBleu_score": 47.630972577458174}, {"code": "def generate_individual(input_ranges, individual_length)\n\t\n    individual = []\n    for _ in range(individual_length):\n        individual.append(random.uniform(input_ranges[0], input_ranges[1]))\n    return individual\n\n", "CodeBleu_score": 39.41234669340793}]}
{"instance_id": "5ea979cb4a8e", "generation_result": [{"code": "def marginal_log_mean_coeff(self, t)\n\t\n    if self.schedule_type == 'discrete':\n        return torch.log(self.alpha_0)\n    elif self.schedule_type == 'linear':\n        return torch.log(self.alpha_0) + (self.alpha_f - self.alpha_0) / (self.T - 1) * t\n    elif self.schedule_type == 'cosine':\n        return torch.log(self.alpha_0) + (self.alpha_f - self.alpha_0) / 2 * (1 + torch.cos(math.pi * t / (self.T - 1)))\n    else:\n        raise NotImplementedError\n\n", "CodeBleu_score": 33.034168698486624}, {"code": "def marginal_log_mean_coeff(self, t)\n\t\n    if self.schedule_type == 'discrete':\n        return torch.log(self.alpha[t])\n    elif self.schedule_type == 'linear':\n        return torch.log(self.alpha[0]) + t / (self.T - 1) * (torch.log(self.alpha[1]) - torch.log(self.alpha[0]))\n    elif self.schedule_type == 'cosine':\n        return torch.log(self.alpha[0]) + 0.5 * (1 + torch.cos(math.pi * t / (self.T - 1))) * (torch.log(self.alpha[1]) - torch.log(self.alpha[0]))\n    else:\n        raise NotImplementedError\n\n", "CodeBleu_score": 31.142086198330983}, {"code": "def marginal_log_mean_coeff(self, t)\n\t\n    if self.schedule_type == \"discrete\":\n        return (\n            torch.log(self.alpha_0)\n            + (torch.log(self.alpha_1) - torch.log(self.alpha_0))\n            * t\n            / (self.T - 1)\n        )\n    elif self.schedule_type == \"linear\":\n        return (\n            torch.log(self.alpha_0)\n            + (torch.log(self.alpha_1) - torch.log(self.alpha_0))\n            * t\n            / self.T\n        )\n    elif self.schedule_type == \"cosine\":\n        return (\n            torch.log(self.alpha_0)\n            + 0.5\n            * (torch.log(self.alpha_1) - torch.log(self.alpha_0))\n            * (1 + torch.cos(math.pi * t / self.T))\n        )\n    else:\n        raise NotImplementedError(\n            \"Schedule type must be either 'discrete', 'linear', or 'cosine'.\"\n        )\n\n", "CodeBleu_score": 28.606959362035155}]}
{"instance_id": "7da4fae5a278", "generation_result": [{"code": "def inverse_lambda(self, lamb)\n\t\n    if self.schedule == 'linear':\n        return (self.beta_0 - self.beta_1) / (1 - torch.exp(self.log_alpha)) * lamb + self.beta_1\n    elif self.schedule == 'discrete':\n        return torch.exp(self.log_alpha[0] * lamb + self.log_alpha[-1] * (1 - lamb))\n    elif self.schedule == 'logarithmic':\n        return torch.acos(torch.exp(-lamb * self.log_alpha)) / math.pi\n    elif self.schedule == 'exponential':\n        return 1 - torch.exp(-lamb * self.log_alpha)\n    elif self.schedule == 'quadratic':\n        return torch.sqrt(1 - torch.exp(-2 * lamb * self.log_alpha))\n    elif self.schedule == 'cubic':\n        return 1 - torch.exp(-3 * lamb * self.log_alpha)\n    else:\n        raise NotImplementedError\n\n", "CodeBleu_score": 25.450711077859122}, {"code": "def inverse_lambda(self, lamb)\n\t\n    if self.schedule == \"linear\":\n        beta = self.beta\n        t = torch.log(2.0) / beta * torch.log(torch.log(2.0) / beta + lamb)\n    elif self.schedule == \"discrete\":\n        log_alpha = self.log_alpha\n        t = (torch.log(lamb) - log_alpha) / (torch.log(torch.tensor(1.0)) - log_alpha)\n    else:\n        log_alpha = self.log_alpha\n        t = torch.acos(torch.exp(-lamb * torch.exp(log_alpha))) / math.pi\n    return t\n\n", "CodeBleu_score": 21.295611770147364}, {"code": "def inverse_lambda(self, lamb)\n\t\n    if self.schedule == 'linear':\n        beta = self.beta\n        t = torch.log(lamb) / beta\n        t = torch.clamp(t, min=0, max=self.T)\n        return t\n    elif self.schedule == 'discrete':\n        log_alpha = self.log_alpha\n        t = torch.lerp(torch.log(lamb), log_alpha, self.T)\n        return t\n    else:\n        log_alpha = self.log_alpha\n        t = torch.acos(torch.cos(torch.log(lamb) - log_alpha))\n        return t\n\n", "CodeBleu_score": 24.919840914133577}]}
{"instance_id": "a519c853a2b2", "generation_result": [{"code": "def noise_pred_fn(x, t_continuous, cond=None)\n\t\n    if cond is None:\n        cond = torch.ones_like(t_continuous)\n    t_continuous = t_continuous.view(-1, 1).repeat(1, x.shape[1])\n    cond = cond.view(-1, 1).repeat(1, x.shape[1])\n    x = torch.cat([x, t_continuous, cond], dim=2)\n    noise = model(x)\n    if model_type == \"linear\":\n        noise = noise.view(noise.shape[0], -1)\n        noise = noise * noise_scale + noise_shift\n        return noise\n    elif model_type == \"linear_additive\":\n        noise = noise.view(noise.shape[0], -1)\n        noise = noise * noise_scale + noise_shift\n        return x + noise\n    elif model_type == \"linear_multiplicative\":\n        noise = noise.view(noise.shape[0], -1)\n        noise = noise * noise_scale + noise_shift\n        return x * (1 + noise)\n    elif model_type == \"linear_multiplicative_additive\":\n        noise = noise.view(noise.shape[0], -1)\n        noise = noise * noise_scale + noise_shift\n        return x * (1 + noise) + noise\n    elif model_type == \"linear_additive_multiplicative\":\n        noise = noise.view(noise.shape[0], -1)\n        noise = noise * noise_scale + noise_shift\n        return x + noise * noise\n    elif model_type == \"linear_additive_multiplicative_additive\":\n        noise = noise.view(noise.shape[0], -1)\n        noise = noise * noise_scale + noise_shift\n        return x + noise * noise + noise\n    elif model_type == \"linear_multiplicative_additive_multiplicative\":\n        noise = noise.view(noise.shape[0], -1)\n        noise = noise * noise_scale + noise_shift\n        return x * (1 + noise) + noise * noise\n    elif model_type == \"linear_additive_multiplicative_additive_multiplicative\":\n        noise = noise.view(noise.shape[0], -1)\n        noise = noise * noise_scale + noise_shift\n        return x + noise * noise + noise *", "CodeBleu_score": 27.688361333429988}, {"code": "def noise_pred_fn(x, t_continuous, cond=None)\n\t\n    if cond is None:\n        cond = torch.ones(x.shape[0], 1, device=x.device)\n    t_continuous = t_continuous.expand(x.shape[0], -1)\n    t_continuous = t_continuous.to(x.dtype)\n    if cond.shape[0] != x.shape[0]:\n        cond = cond.expand(x.shape[0], -1)\n    t_continuous = t_continuous * cond\n    if model_type == 'linear':\n        return t_continuous\n    elif model_type == 'quadratic':\n        return t_continuous ** 2\n    elif model_type == 'cubic':\n        return t_continuous ** 3\n    elif model_type == 'logarithmic':\n        return torch.log(t_continuous + 1e-10)\n    elif model_type == 'exponential':\n        return torch.exp(t_continuous)\n    elif model_type == 'sinusoidal':\n        return torch.sin(t_continuous)\n    elif model_type == 'cosinusoidal':\n        return torch.cos(t_continuous)\n    elif model_type == 'sawtooth':\n        return torch.sin(2 * math.pi * t_continuous)\n    elif model_type == 'triangular':\n        return torch.abs(torch.sin(2 * math.pi * t_continuous))\n    elif model_type == 'sigmoid':\n        return torch.sigmoid(t_continuous)\n    else:\n        raise ValueError('Unknown model type: ' + model_type)\n\n", "CodeBleu_score": 27.81879403685952}, {"code": "def noise_pred_fn(x, t_continuous, cond=None)\n\t\n    if t_continuous:\n        t = torch.linspace(0, 1, x.shape[0]).to(x.device)\n        t = t.expand(x.shape[0], -1).transpose(0, 1).reshape(-1, 1)\n        x = torch.cat([x, t], dim=1)\n    if cond is not None:\n        x = torch.cat([x, cond], dim=1)\n    return noise_fn(x)\n\n", "CodeBleu_score": 17.954576468594823}]}
{"instance_id": "96a9b7d54823", "generation_result": [{"code": "def dynamic_thresholding_fn(self, x0, t=None)\n\t\n    if t is None:\n        t = self.threshold\n    x0 = x0.view(x0.shape[0], -1)\n    q = torch.quantile(torch.abs(x0), t, dim=1).view(-1, 1)\n    scaling_factor = torch.max(q, torch.ones_like(q) * self.epsilon)\n    return torch.clamp(x0 / scaling_factor, -self.clip_value, self.clip_value)\n\n", "CodeBleu_score": 40.810611583238455}, {"code": "def dynamic_thresholding_fn(self, x0, t=None)\n\t\n    if t is None:\n        t = self.threshold\n    x0 = x0.view(x0.size(0), -1)\n    q = x0.abs().mean(dim=1).view(x0.size(0), 1, 1, 1)\n    x = x0.div(q)\n    x = x.clamp(min=-t, max=t)\n    x = x.mul(q)\n    return x\n\n", "CodeBleu_score": 35.22922983261655}, {"code": "def dynamic_thresholding_fn(self, x0, t=None)\n\t\n    # Reshape x0 to compute the quantile of absolute values\n    x0 = x0.view(-1)\n    x0_abs = torch.abs(x0)\n    q = torch.quantile(x0_abs, self.quantile)\n    # Determine the scaling factor\n    s = 1 / q\n    # Clamp x0 values based on this factor\n    x0_scaled = torch.clamp(x0, min=-s, max=s)\n    # Scale the resulting tensor\n    x0_scaled = x0_scaled * self.scale\n    return x0_scaled\n\n", "CodeBleu_score": 15.553550844099234}]}
{"instance_id": "59c6ec6d7a6a", "generation_result": [{"code": "def data_prediction_fn(self, x, t)\n\t\n\n    # Calculate the threshold value\n    threshold = self.dynamic_threshold * torch.max(x)\n\n    # Clamp the input tensor values within the threshold\n    x = torch.clamp(x, min=threshold, max=self.max_threshold)\n\n    # Normalize the clamped values\n    x = (x - threshold) / (self.max_threshold - threshold)\n\n    return x\n\n", "CodeBleu_score": 13.133819495994537}, {"code": "def data_prediction_fn(self, x, t)\n\t\n\n    # Calculate the threshold value\n    threshold = self.dynamic_threshold * self.max_threshold\n\n    # Clamp the input tensor values\n    x_clamp = torch.clamp(x, min=0, max=threshold)\n\n    # Normalize the clamped values\n    x_clamp_norm = x_clamp / threshold\n\n    return x_clamp_norm\n\n", "CodeBleu_score": 10.25381856035735}, {"code": "def data_prediction_fn(self, x, t)\n\t\n    threshold = self.threshold_value * self.threshold_ratio\n    threshold = min(threshold, self.threshold_value)\n    x_clamped = torch.clamp(x, min=threshold)\n    x_clamped = x_clamped / self.threshold_value\n    return x_clamped\n\n", "CodeBleu_score": 12.295072513530748}]}
{"instance_id": "65e3618468a2", "generation_result": [{"code": "def predict_eps_sigma(model, input, sigma_in, **kwargs)\n\t\n    sigma_in = sigma_in.reshape(sigma_in.shape[0], 1, 1, 1)\n    input = input * sigma_in\n    pred = model(input, **kwargs)\n    return pred / sigma_in\n\n", "CodeBleu_score": 27.311660833755358}, {"code": "def predict_eps_sigma(model, input, sigma_in, **kwargs)\n\t\n    sigma_in = sigma_in.view(sigma_in.shape[0], 1, 1, 1)\n    input_scaled = input * sigma_in\n    output = model(input_scaled, **kwargs)\n    output_scaled = output / sigma_in\n    return output_scaled\n\n", "CodeBleu_score": 26.04328837664755}, {"code": "def predict_eps_sigma(model, input, sigma_in, **kwargs)\n\t\n    sigma_in = sigma_in.reshape(sigma_in.shape[0], 1, 1, 1)\n    input_scaled = input / sigma_in\n    output_scaled = model(input_scaled, **kwargs)\n    output = output_scaled * sigma_in\n    return output\n\n", "CodeBleu_score": 24.355230048347902}]}
{"instance_id": "b35846dfbc66", "generation_result": [{"code": "def cal_intergrand(beta_0, beta_1, taus)\n\t\n    with torch.no_grad():\n        taus = taus.clone().requires_grad_(True)\n        alpha = beta_0 + beta_1 * taus\n        log_alpha = torch.log(alpha)\n        log_alpha.backward(torch.ones_like(log_alpha))\n        grad = taus.grad\n        taus.grad = None\n        integrand = (torch.exp(beta_0) * torch.exp(-beta_1 * taus) * grad).sum(dim=1)\n    return integrand\n\n", "CodeBleu_score": 40.16337494118991}, {"code": "def cal_intergrand(beta_0, beta_1, taus)\n\t\n    # clone taus and set the gradient to 1\n    taus_ = taus.clone().detach().requires_grad_(True)\n    # calculate alpha and log_alpha\n    alpha = torch.exp(beta_0 + beta_1 * taus_)\n    log_alpha = torch.log(alpha)\n    # calculate the gradient of log_alpha with respect to taus\n    log_alpha.backward(torch.ones_like(log_alpha))\n    # calculate the integrand using the gradient of log_alpha and alpha\n    integrand = taus_ * torch.exp(beta_1 * taus_ - log_alpha)\n    return integrand\n\n", "CodeBleu_score": 17.073066902102514}, {"code": "def cal_intergrand(beta_0, beta_1, taus)\n\t\n    alpha = torch.exp(beta_0 + beta_1 * taus)\n    log_alpha = beta_0 + beta_1 * taus\n    log_alpha.backward(torch.ones_like(log_alpha))\n    grad_log_alpha = taus.grad\n    taus.grad.zero_()\n    return alpha * grad_log_alpha\n\n", "CodeBleu_score": 20.73535418150331}]}
{"instance_id": "5f3667566a10", "generation_result": [{"code": "def sample_euler_ancestral(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None)\n\t\n    if extra_args is None:\n        extra_args = []\n    if noise_sampler is None:\n        noise_sampler = lambda x, s: torch.randn_like(x) * s\n    if disable is None:\n        disable = []\n    if callback is None:\n        callback = lambda x: None\n    x = x.clone()\n    x.requires_grad_(True)\n    for sigma in sigmas:\n        callback(x)\n        x_noisy = x + noise_sampler(x, s_noise)\n        x_denoised = model(x_noisy, *extra_args)\n        x_denoised.backward(eta * sigma)\n        x = x - x.grad\n        x.grad.zero_()\n    x = x + noise_sampler(x, s_noise)\n    return x\n\n", "CodeBleu_score": 26.95158182830407}, {"code": "def sample_euler_ancestral(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None)\n\t\n    if extra_args is None:\n        extra_args = {}\n    if noise_sampler is None:\n        noise_sampler = torch.randn_like\n    if disable is None:\n        disable = []\n    if not isinstance(disable, list):\n        disable = [disable]\n    if not isinstance(sigmas, list):\n        sigmas = [sigmas]\n    if callback is not None:\n        callback(x, **extra_args)\n    for sigma in tqdm(sigmas, disable=disable):\n        x_noise = x + noise_sampler(x.shape) * s_noise\n        x_noise = model(x_noise, sigma=sigma, **extra_args)\n        x_noise = x_noise + noise_sampler(x.shape) * s_noise\n        x = x + eta * (x_noise - x)\n        if callback is not None:\n            callback(x, **extra_args)\n    return x + noise_sampler(x.shape) * s_noise\n\n", "CodeBleu_score": 32.476556370870874}, {"code": "def sample_euler_ancestral(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None)\n\t\n    if extra_args is None:\n        extra_args = {}\n    if noise_sampler is None:\n        noise_sampler = torch.randn_like\n    if callback is None:\n        callback = lambda *args, **kwargs: None\n    if disable is None:\n        disable = False\n    x_shape = x.shape\n    x = x.reshape(-1, *x_shape[-3:])\n    x = x.repeat(len(sigmas), 1, 1, 1, 1)\n    x = x.reshape(-1, *x_shape[-3:])\n    x_noise = noise_sampler(x)\n    x_noise = x_noise.reshape(-1, *x_shape[-3:])\n    x_noise = x_noise.repeat(len(sigmas), 1, 1, 1, 1)\n    x_noise = x_noise.reshape(-1, *x_shape[-3:])\n    x_denoised = model(x_noise, **extra_args)\n    x_denoised = x_denoised.reshape(-1, *x_shape[-3:])\n    x_denoised = x_denoised.repeat(len(sigmas), 1, 1, 1, 1)\n    x_denoised = x_denoised.reshape(-1, *x_shape[-3:])\n    for i, sigma in enumerate(sigmas):\n        if not disable:\n            callback(i, x_denoised, x_noise, x)\n        x_noise_denoised = x_noise + eta * (x_denoised - x_noise)\n        x_noise = x_noise + sigma * x_noise_denoised\n        x = x + sigma * x_noise\n    if s_noise > 0:\n        x = x + s_noise * noise_sampler(x)\n    return x.reshape(*x_shape)\n\n", "CodeBleu_score": 32.669704506837626}]}
{"instance_id": "6261ded88df2", "generation_result": [{"code": "def linear_multistep_coeff(order, t, i, j)\n\t\n    if order == 0:\n        return 1.0\n    if order == 1:\n        if i == j:\n            return 0.5\n        if i == j - 1:\n            return 0.5\n        raise ValueError(f\"Order {order} not defined for indices {i} and {j}.\")\n    if order == 2:\n        if i == j:\n            return 1.0 / 6.0\n        if i == j - 1:\n            return 4.0 / 6.0\n        if i == j - 2:\n            return 1.0 / 6.0\n        raise ValueError(f\"Order {order} not defined for indices {i} and {j}.\")\n    if order == 3:\n        if i == j:\n            return 1.0 / 24.0\n        if i == j - 1:\n            return 19.0 / 24.0\n        if i == j - 2:\n            return -5.0 / 24.0\n        if i == j - 3:\n            return 1.0 / 24.0\n        raise ValueError(f\"Order {order} not defined for indices {i} and {j}.\")\n    if order == 4:\n        if i == j:\n            return 1.0 / 120.0\n        if i == j - 1:\n            return 9.0 / 120.0\n        if i == j - 2:\n            return -9.0 / 120.0\n        if i == j - 3:\n            return 9.0 / 120.0\n        if i == j - 4:\n            return 1.0 / 120.0\n        raise ValueError(f\"Order {order} not defined for indices {i} and {j}.\")\n    if order == 5:\n        if i == j:\n            return 1.0 / 720.0\n        if i == j - 1:\n            return 361.0 / 720.0\n        if i == j - 2:\n            return -1387.0 / 720.0\n        if i == j - 3:\n            return 109.0 / 720.0\n        if i == j - 4:\n            return", "CodeBleu_score": 13.49752544971467}, {"code": "def linear_multistep_coeff(order, t, i, j)\n\t\n    if order == 1:\n        return 1.0\n    elif order == 2:\n        if i == j:\n            return 1.0 / 2.0\n        elif i == j - 1:\n            return 1.0 / 2.0\n        else:\n            raise ValueError(\"The order is too high for the step index.\")\n    elif order == 3:\n        if i == j:\n            return 1.0 / 6.0\n        elif i == j - 1:\n            return 2.0 / 3.0\n        elif i == j - 2:\n            return 1.0 / 6.0\n        else:\n            raise ValueError(\"The order is too high for the step index.\")\n    else:\n        if i == j:\n            return integrate.quad(\n                lambda t: _linear_multistep_coeff(order, t, i, j), t[i], t[i + 1]\n            )[0]\n        elif i == j - 1:\n            return integrate.quad(\n                lambda t: _linear_multistep_coeff(order, t, i, j), t[i], t[i + 1]\n            )[0]\n        elif i == j - 2:\n            return integrate.quad(\n                lambda t: _linear_multistep_coeff(order, t, i, j), t[i], t[i + 1]\n            )[0]\n        else:\n            raise ValueError(\"The order is too high for the step index.\")\n\n", "CodeBleu_score": 17.98352606127736}, {"code": "def linear_multistep_coeff(order, t, i, j)\n\t\n    if order == 1:\n        return 1 / 2\n    if order == 2:\n        return 1 / 2 * (1 - t) ** 2\n    if order == 3:\n        return 1 / 2 * (2 * t - 3 * t ** 2 + 1)\n    if order == 4:\n        return 1 / 8 * (1 - t) ** 4 + 1 / 8 * (4 - 6 * t + 3 * t ** 2)\n    if order == 5:\n        return 1 / 8 * (1 - t) ** 5 + 1 / 8 * (5 - 10 * t + 15 * t ** 2 - 6 * t ** 3)\n    if order == 6:\n        return 1 / 8 * (1 - t) ** 6 + 1 / 8 * (6 - 15 * t + 20 * t ** 2 - 10 * t ** 3 + 1)\n    if order == 7:\n        return 1 / 8 * (1 - t) ** 7 + 1 / 8 * (7 - 21 * t + 35 * t ** 2 - 35 * t ** 3 + 21 * t ** 4 - 5 * t ** 5)\n    if order == 8:\n        return 1 / 16 * (1 - t) ** 8 + 1 / 16 * (8 - 28 * t + 56 * t ** 2 - 70 * t ** 3 + 56 * t ** 4 - 28 * t ** 5 + 7 * t ** 6)\n    if order == 9:\n        return 1 / 16 * (1 - t) ** 9 + 1 / 16 * (9 - 36 * t + 84 * t ** 2 - 126 * t ** 3 + 126 * t ** 4 - 84 * t ** 5 + 36 * t ** 6 - 9 * t ** 7)\n    if order == 10:\n        return 1 / 16 * (1 - t) ** 10 +", "CodeBleu_score": 11.52478429908203}]}
{"instance_id": "4ee025e33ab1", "generation_result": [{"code": "def sample_lcm(model, x, sigmas, extra_args=None, callback=None, disable=None, noise_sampler=None)\n\t\n    if extra_args is None:\n        extra_args = {}\n    if noise_sampler is None:\n        noise_sampler = torch.distributions.Normal(0, 1)\n    if disable is None:\n        disable = False\n    if callback is None:\n        callback = lambda x: None\n    x = x.clone()\n    for i in trange(len(sigmas), disable=disable):\n        sigma = sigmas[i]\n        if sigma == 0:\n            continue\n        x_ = x.clone()\n        x_ = x_ + sigma * noise_sampler.sample(x_.shape).to(x.device)\n        x_ = model(x_, **extra_args)\n        x = x_ + (sigma / math.sqrt(2)) * noise_sampler.sample(x_.shape).to(x.device)\n        callback(x)\n    return x\n\n", "CodeBleu_score": 37.06190726810901}, {"code": "def sample_lcm(model, x, sigmas, extra_args=None, callback=None, disable=None, noise_sampler=None)\n\t\n\n    # Initialize extra arguments if not provided\n    if extra_args is None:\n        extra_args = {}\n\n    # Initialize noise sampler if not provided\n    if noise_sampler is None:\n        noise_sampler = lambda size: torch.randn(size, device=x.device)\n\n    # Initialize the output\n    x_out = x.clone()\n\n    # Initialize the denoised value\n    x_denoised = x.clone()\n\n    # Initialize the sigma index\n    sigma_idx = 0\n\n    # Initialize the current sigma\n    sigma = sigmas[sigma_idx]\n\n    # Initialize the current noise level\n    noise_level = 0.0\n\n    # Initialize the current noise level\n    noise_level_max = 0.0\n\n    # Initialize the current sigma index\n    sigma_idx_max = 0\n\n    # Initialize the current sigma\n    sigma_max = 0.0\n\n    # Initialize the current step\n    step = 0\n\n    # Initialize the progress bar\n    pbar = trange(len(sigmas), desc='Sampling', leave=False, disable=disable)\n\n    # Iterate over the sigmas\n    for sigma in pbar:\n\n        # Update the sigma index\n        sigma_idx += 1\n\n        # Update the current sigma\n        sigma_max = sigma\n\n        # Update the current sigma index\n        sigma_idx_max = sigma_idx\n\n        # Update the noise level\n        noise_level = sigma_idx * sigma\n\n        # Update the noise level max\n        noise_level_max = sigma_idx_max * sigma_max\n\n        # Update the denoised value\n        x_denoised = x_out.clone()\n\n        # Update the step\n        step += 1\n\n        # Execute the callback function if provided\n        if callback is not None:\n            callback(x_out, x_denoised, sigma, sigma_idx, step, noise_level, noise_level_max, sigma_max, sigma_idx_max)\n\n        # Stop sampling if the next sigma is greater than zero and the noise level is greater than the noise level max\n        if sigma > 0 and noise_level > noise_level_max:\n            break\n\n        # Execute the model\n        x_out = model(x_out, **extra_args)\n\n        # Denoise the output\n        x_out = x_denoised + noise_sampler(x_out.shape) * math.sqrt(noise_level)\n\n    # Close the progress bar\n    p", "CodeBleu_score": 33.600977798950225}, {"code": "def sample_lcm(model, x, sigmas, extra_args=None, callback=None, disable=None, noise_sampler=None)\n\t\n    # Initialize extra arguments\n    if extra_args is None:\n        extra_args = {}\n    # Initialize noise sampler\n    if noise_sampler is None:\n        noise_sampler = torch.distributions.normal.Normal(0, 1)\n    # Initialize output\n    x_out = x\n    # Initialize model\n    model.eval()\n    # Iterate over sigmas\n    for sigma in tqdm(sigmas, disable=disable):\n        # Denoise input\n        x_out = model(x_out, **extra_args)\n        # Add noise\n        if sigma > 0:\n            x_out += sigma * noise_sampler.sample(x_out.shape).to(x_out.device)\n        # Execute callback\n        if callback is not None:\n            callback(x_out, sigma)\n    # Return output\n    return x_out\n\n", "CodeBleu_score": 29.48010153162588}]}
{"instance_id": "cb51f91da606", "generation_result": [{"code": "def append_dims(x, target_dims)\n\t\n    if not isinstance(target_dims, tuple):\n        target_dims = (target_dims,)\n    if x.dim() == len(target_dims):\n        return x.detach().clone() if x.device.type == 'mps' else x\n    if x.dim() > len(target_dims):\n        raise ValueError('The target dimensions must be equal to or less than the current dimensions.')\n    dims_to_append = len(target_dims) - x.dim()\n    for _ in range(dims_to_append):\n        x = x.unsqueeze(-1)\n    if x.device.type == 'mps':\n        return x.detach().clone()\n    return x\n\n", "CodeBleu_score": 35.80614442416296}, {"code": "def append_dims(x, target_dims)\n\t\n    dims_to_add = target_dims - len(x.shape)\n    if dims_to_add < 0:\n        raise ValueError(f\"Target dimensions ({target_dims}) cannot be smaller than the current dimensions ({len(x.shape)})\")\n    if dims_to_add > 0:\n        if x.device.type == 'mps':\n            return x.detach().reshape(x.shape + (1,) * dims_to_add)\n        else:\n            return x.reshape(x.shape + (1,) * dims_to_add)\n    else:\n        return x\n\n", "CodeBleu_score": 26.747790637660078}, {"code": "def append_dims(x, target_dims)\n\t\n    dims_to_add = target_dims - len(x.shape)\n    if dims_to_add < 0:\n        raise ValueError(f\"The target dimensions must be greater than or equal to the current dimensions ({target_dims} >= {len(x.shape)}).\")\n    if dims_to_add > 0:\n        if x.device.type == \"mps\":\n            return x.detach().clone().expand(x.shape + (1,) * dims_to_add)\n        else:\n            return x.expand(x.shape + (1,) * dims_to_add)\n    else:\n        return x\n\n", "CodeBleu_score": 27.447576679968883}]}
{"instance_id": "9fac874a3bc7", "generation_result": [{"code": "def download_file(path, url, digest=None)\n\t\n    path = Path(path)\n    if not path.parent.is_dir():\n        path.parent.mkdir(parents=True)\n    if path.is_file():\n        if digest is not None:\n            with open(path, \"rb\") as f:\n                file_hash = hashlib.sha256(f.read()).hexdigest()\n            if file_hash != digest:\n                raise RuntimeError(\n                    f\"The SHA-256 hash of the file at {path} does not match the expected value of {digest}.\"\n                )\n        return path\n    with urllib.request.urlopen(url) as response:\n        with open(path, \"wb\") as f:\n            shutil.copyfileobj(response, f)\n    if digest is not None:\n        with open(path, \"rb\") as f:\n            file_hash = hashlib.sha256(f.read()).hexdigest()\n        if file_hash != digest:\n            raise RuntimeError(\n                f\"The SHA-256 hash of the file at {path} does not match the expected value of {digest}.\"\n            )\n    return path\n\n", "CodeBleu_score": 53.41402953875534}, {"code": "def download_file(path, url, digest=None)\n\t\n    path = Path(path)\n    path.parent.mkdir(parents=True, exist_ok=True)\n    if not path.exists():\n        print(f\"Downloading {url} to {path}\")\n        urllib.request.urlretrieve(url, path)\n    else:\n        print(f\"Using existing file {path}\")\n    if digest is not None:\n        print(f\"Checking SHA-256 hash of {path}\")\n        sha256 = hashlib.sha256()\n        with open(path, \"rb\") as f:\n            while True:\n                data = f.read(65536)\n                if not data:\n                    break\n                sha256.update(data)\n        if sha256.hexdigest() != digest:\n            raise ValueError(f\"SHA-256 hash of {path} does not match the expected value\")\n    return path\n\n", "CodeBleu_score": 45.4711729301871}, {"code": "def download_file(path, url, digest=None)\n\t\n    path = Path(path)\n    if not path.parent.exists():\n        path.parent.mkdir(parents=True)\n    if not path.exists():\n        urllib.request.urlretrieve(url, path)\n    if digest is not None:\n        with open(path, \"rb\") as f:\n            file_hash = hashlib.sha256(f.read()).hexdigest()\n        if file_hash != digest:\n            raise ValueError(\n                f\"The downloaded file at {path} has an unexpected SHA-256 hash. Please try again.\"\n            )\n    return path\n\n", "CodeBleu_score": 49.23006754119442}]}
{"instance_id": "f23165e897ce", "generation_result": [{"code": "def train_mode(model, mode=True)\n\t\n    prev_states = {}\n    for name, module in model.named_modules():\n        if isinstance(module, nn.Module):\n            prev_states[name] = module.training\n    model.train(mode)\n    try:\n        yield\n    finally:\n        for name, module in model.named_modules():\n            if isinstance(module, nn.Module):\n                module.training = prev_states[name]\n\n", "CodeBleu_score": 32.288548757234345}, {"code": "def train_mode(model, mode=True)\n\t\n    prev_states = []\n    for module in model.modules():\n        prev_states.append(module.training)\n        module.train(mode)\n    try:\n        yield\n    finally:\n        for module, state in zip(model.modules(), prev_states):\n            module.train(state)\n\n", "CodeBleu_score": 35.625521533943804}, {"code": "def train_mode(model, mode=True)\n\t\n    prev_states = []\n    for module in model.modules():\n        prev_states.append(module.training)\n        module.train(mode)\n    yield\n    for module, state in zip(model.modules(), prev_states):\n        module.train(state)\n\n", "CodeBleu_score": 35.10303923793591}]}
{"instance_id": "c6d59c6cb8e1", "generation_result": [{"code": "def ema_update(model, averaged_model, decay)\n\t\n    for ema_param, param in zip(averaged_model.parameters(), model.parameters()):\n        ema_param.data.mul_(decay).add_(param.data, alpha=1 - decay)\n    for ema_buffer, buffer in zip(averaged_model.buffers(), model.buffers()):\n        ema_buffer.data.copy_(buffer.data)\n\n", "CodeBleu_score": 27.485946931039855}, {"code": "def ema_update(model, averaged_model, decay)\n\t\n    for current_params, averaged_params in zip(model.parameters(), averaged_model.parameters()):\n        # The update is defined as: current_params = decay * averaged_params + (1 - decay) * current_params\n        # Note that this is not exactly the same as the exact update that is used in the Polyak-Ruppert Averaging algorithm.\n        # However, the difference is not noticable in most cases.\n        averaged_params.data.mul_(decay).add_(current_params.data, alpha=1 - decay)\n\n    for current_buffer, averaged_buffer in zip(model.buffers(), averaged_model.buffers()):\n        averaged_buffer.data.copy_(current_buffer.data)\n\n", "CodeBleu_score": 30.210788334820627}, {"code": "def ema_update(model, averaged_model, decay)\n\t\n    for ema_param, param in zip(averaged_model.parameters(), model.parameters()):\n        ema_param.data.mul_(decay).add_(param.data, alpha=1 - decay)\n    for ema_buffer, buffer in zip(averaged_model.buffers(), model.buffers()):\n        ema_buffer.data.copy_(buffer.data)\n\n", "CodeBleu_score": 27.485946931039855}]}
{"instance_id": "600c39a30798", "generation_result": [{"code": "def rand_log_logistic(shape, loc=0., scale=1., min_value=0., max_value=float('inf'), device='cpu', dtype=torch.float32)\n\t\n    min_value = torch.as_tensor(min_value, device=device, dtype=dtype)\n    max_value = torch.as_tensor(max_value, device=device, dtype=dtype)\n    if min_value.ndim > 0:\n        min_value = min_value.expand(shape)\n    if max_value.ndim > 0:\n        max_value = max_value.expand(shape)\n    u = torch.rand(shape, device=device, dtype=dtype)\n    u_cdf = torch.log(u) - torch.log1p(-u)\n    min_cdf = torch.log(min_value) - torch.log1p(-min_value)\n    max_cdf = torch.log(max_value) - torch.log1p(-max_value)\n    return torch.exp(torch.logit(u_cdf * (max_cdf - min_cdf) + min_cdf) * scale + loc)\n\n", "CodeBleu_score": 48.25976910901251}, {"code": "def rand_log_logistic(shape, loc=0., scale=1., min_value=0., max_value=float('inf'), device='cpu', dtype=torch.float32)\n\t\n    min_value = torch.as_tensor(min_value, device=device, dtype=dtype)\n    max_value = torch.as_tensor(max_value, device=device, dtype=dtype)\n    cdf_min = torch.log(torch.exp(min_value - loc) - 1) / scale\n    cdf_max = torch.log(torch.exp(max_value - loc) - 1) / scale\n    uniform_samples = torch.rand(shape, device=device, dtype=dtype)\n    cdf_samples = uniform_samples * (cdf_max - cdf_min) + cdf_min\n    samples = torch.exp(cdf_samples * scale + loc)\n    return samples\n\n", "CodeBleu_score": 36.688173995574566}, {"code": "def rand_log_logistic(shape, loc=0., scale=1., min_value=0., max_value=float('inf'), device='cpu', dtype=torch.float32)\n\t\n    # convert min and max values to tensors\n    min_value = torch.as_tensor(min_value, device=device, dtype=dtype)\n    max_value = torch.as_tensor(max_value, device=device, dtype=dtype)\n    # compute CDF values\n    min_cdf = 1. / (1. + torch.exp(-(min_value - loc) / scale))\n    max_cdf = 1. / (1. + torch.exp((max_value - loc) / scale))\n    # generate uniform random values\n    rand_uniform = torch.rand(shape, device=device, dtype=dtype)\n    # transform uniform random values using the logit function\n    rand_logit = torch.log(rand_uniform) - torch.log1p(-rand_uniform)\n    # scale and exponentiate the values\n    rand_log_logistic = torch.exp(rand_logit * scale + loc)\n    # truncate the values\n    rand_log_logistic = torch.min(torch.max(rand_log_logistic, min_cdf), max_cdf)\n    return rand_log_logistic\n\n", "CodeBleu_score": 33.238597677791944}]}
{"instance_id": "75e2def2467f", "generation_result": [{"code": "def get_activation(activation: Literal[\"elu\", \"snake\", \"none\"], antialias=False, channels=None) -> nn.Module\n\t\n    if activation == \"elu\":\n        act = nn.ELU()\n    elif activation == \"snake\":\n        act = comfy.ops.SnakeBeta(channels)\n    elif activation == \"none\":\n        act = nn.Identity()\n    else:\n        raise ValueError(f\"Activation type {activation} is not supported.\")\n\n    if antialias:\n        act = comfy.ops.Activation1d(act)\n\n    return act\n\n", "CodeBleu_score": 64.76344264567442}, {"code": "def get_activation(activation: Literal[\"elu\", \"snake\", \"none\"], antialias=False, channels=None) -> nn.Module\n\t\n    if activation == \"elu\":\n        act = nn.ELU()\n    elif activation == \"snake\":\n        act = comfy.ops.SnakeBeta(channels)\n    else:\n        act = nn.Identity()\n    if antialias:\n        act = comfy.ops.Activation1d(act)\n    return act\n\n", "CodeBleu_score": 55.06618401880936}, {"code": "def get_activation(activation: Literal[\"elu\", \"snake\", \"none\"], antialias=False, channels=None) -> nn.Module\n\t\n    if activation == \"elu\":\n        if antialias:\n            return comfy.ops.Activation1d(nn.ELU(inplace=True))\n        else:\n            return nn.ELU(inplace=True)\n    elif activation == \"snake\":\n        if antialias:\n            return comfy.ops.Activation1d(comfy.ops.SnakeBeta(channels))\n        else:\n            return comfy.ops.SnakeBeta(channels)\n    elif activation == \"none\":\n        return nn.Identity()\n    else:\n        raise NotImplementedError(f\"Activation type {activation} not implemented.\")\n\n", "CodeBleu_score": 39.61306996653728}]}
{"instance_id": "ef1714c79e51", "generation_result": [{"code": "def apply_rotary_pos_emb(t, freqs, scale = 1)\n\t\n    if t.dtype == torch.float16:\n        t = t.to(torch.float32)\n    freqs = freqs.to(t)\n    if len(t.shape) == 3:\n        t = rearrange(t, 'b h l -> b (h l)')\n    elif len(t.shape) == 2:\n        t = rearrange(t, 'h l -> (h l)')\n    else:\n        raise ValueError('Input tensor must have 2 or 3 dimensions.')\n    t = t.float()\n    freqs = freqs.float()\n    t = t * freqs\n    sin = torch.sin(t)\n    cos = torch.cos(t)\n    if len(t.shape) == 2:\n        sin = rearrange(sin, '(h l) -> h l', h = t.shape[0])\n        cos = rearrange(cos, '(h l) -> h l', h = t.shape[0])\n    elif len(t.shape) == 3:\n        sin = rearrange(sin, 'b (h l) -> b h l', h = t.shape[1])\n        cos = rearrange(cos, 'b (h l) -> b h l', h = t.shape[1])\n    else:\n        raise ValueError('Input tensor must have 2 or 3 dimensions.')\n    sin = sin * scale\n    cos = cos * scale\n    return torch.cat([sin, cos], dim = -1)\n\n", "CodeBleu_score": 34.50325351352455}, {"code": "def apply_rotary_pos_emb(t, freqs, scale = 1)\n\t\n    if t.dtype == torch.float64:\n        t = t.to(torch.float32)\n    t = rearrange(t, 'b n (f d) -> b n f d', f = freqs.shape[0])\n    t = (t * freqs.cos()) + (t * freqs.sin())\n    t = rearrange(t, 'b n f d -> b n (f d)')\n    return t\n\n", "CodeBleu_score": 19.803614140113773}, {"code": "def apply_rotary_pos_emb(t, freqs, scale = 1)\n\t\n    if t.dtype == torch.float16:\n        t = t.float()\n    t = t.reshape(-1, t.shape[-1])\n    freqs = freqs.reshape(-1)\n    t = t[:, None, :] * freqs[None, :, None]\n    sin, cos = t.sin(), t.cos()\n    sin, cos = rearrange(sin, 'b n d -> b d n'), rearrange(cos, 'b n d -> b d n')\n    t = torch.cat((sin, cos), dim=-1)\n    t = rearrange(t, 'b d (n r) -> b n d r', r=2)\n    return t * scale\n\n", "CodeBleu_score": 35.11717068075712}]}
{"instance_id": "a7a5f178c6f6", "generation_result": [{"code": "def idx2vq(self, idx, dim=-1)\n\t\n    return self.codebook(idx.to(self.codebook.weight.device)).movedim(1, dim)\n\n", "CodeBleu_score": 24.469841004854114}, {"code": "def idx2vq(self, idx, dim=-1)\n\t\n    return self.codebook(idx).movedim(1, dim)\n\n", "CodeBleu_score": 18.88410605975309}, {"code": "def idx2vq(self, idx, dim=-1)\n\t\n    return self.vq_embeddings(idx).movedim(1, dim)\n\n", "CodeBleu_score": 18.332692797594518}]}
{"instance_id": "4104c041fb20", "generation_result": [{"code": "def encode(self, x, quantize=False)\n\t\n    if quantize:\n        z = self.encoder(x)\n        z_q, z_p, z_indices = self.vq(z)\n        z_q = z_q.view(z_q.size(0), -1)\n        z_p = z_p.view(z_p.size(0), -1)\n        z_indices = z_indices.view(z_indices.size(0), -1)\n        return z_q, z_p, z_indices, self.vq.loss(z_q, z_indices)\n    else:\n        z = self.encoder(x)\n        return z\n\n", "CodeBleu_score": 33.21678249276933}, {"code": "def encode(self, x, quantize=False)\n\t\n    x = self.conv1(x)\n    x = self.block1(x)\n    x = self.block2(x)\n    x = self.block3(x)\n    x = self.block4(x)\n    x = self.block5(x)\n    x = self.block6(x)\n    x = self.block7(x)\n    x = self.block8(x)\n    x = self.block9(x)\n    x = self.block10(x)\n    x = self.block11(x)\n    x = self.block12(x)\n    x = self.block13(x)\n    x = self.block14(x)\n    x = self.block15(x)\n    x = self.block16(x)\n    x = self.block17(x)\n    x = self.block18(x)\n    x = self.block19(x)\n    x = self.block20(x)\n    x = self.block21(x)\n    x = self.block22(x)\n    x = self.block23(x)\n    x = self.block24(x)\n    x = self.block25(x)\n    x = self.block26(x)\n    x = self.block27(x)\n    x = self.block28(x)\n    x = self.block29(x)\n    x = self.block30(x)\n    x = self.block31(x)\n    x = self.block32(x)\n    x = self.block33(x)\n    x = self.block34(x)\n    x = self.block35(x)\n    x = self.block36(x)\n    x = self.block37(x)\n    x = self.block38(x)\n    x = self.block39(x)\n    x = self.block40(x)\n    x = self.block41(x)\n    x = self.block42(x)\n    x = self.block43(x)\n    x = self.block44(x)\n    x = self.block45(x)\n    x = self.block46(x)\n    x = self.", "CodeBleu_score": 8.66828436869452}, {"code": "def encode(self, x, quantize=False)\n\t\n    quantize = self.quantize if quantize is None else quantize\n    quantize = quantize and self.training\n    if quantize:\n        z, z_orig, indices, loss = self.encoder(x)\n        return z, z_orig, indices, loss\n    else:\n        z = self.encoder(x)\n        return z\n\n", "CodeBleu_score": 31.980175262124256}]}
{"instance_id": "403c52ba2d3b", "generation_result": [{"code": "def ema_scope(self, context=None)\n\t\n    if self.ema_decay == 0.0:\n        yield\n        return\n    with torch.no_grad():\n        if self.ema_decay < 1.0:\n            for p, ema_p in zip(self.parameters(), self.ema_parameters()):\n                ema_p.copy_(ema_p * self.ema_decay + p * (1.0 - self.ema_decay))\n        else:\n            for p, ema_p in zip(self.parameters(), self.ema_parameters()):\n                ema_p.copy_(p)\n    if context is not None:\n        context.log_ema_switch(self.ema_decay)\n    yield\n    with torch.no_grad():\n        for p, ema_p in zip(self.parameters(), self.ema_parameters()):\n            p.copy_(ema_p)\n    if context is not None:\n        context.log_ema_restore(self.ema_decay)\n\n", "CodeBleu_score": 43.840316528074055}, {"code": "def ema_scope(self, context=None)\n\t\n    if context is None:\n        return self.ema_scope(context=self.ema_context)\n    else:\n        return self.ema_scope(context=context)\n\n", "CodeBleu_score": 9.987944971685861}, {"code": "def ema_scope(self, context=None)\n\t\n    if context is None:\n        context = self.ema_context\n    if context is None:\n        yield\n        return\n    with context.ema_scope():\n        yield\n\n", "CodeBleu_score": 19.20394527307439}]}
{"instance_id": "94b7467f98e4", "generation_result": [{"code": "def post_attention(self, attn, x, gate_msa, shift_mlp, scale_mlp, gate_mlp)\n\t\n    x = x + gate_msa * attn\n    x = self.norm(x)\n    x = self.mlp(x)\n    x = gate_mlp * x\n    return x\n\n", "CodeBleu_score": 29.422425993474587}, {"code": "def post_attention(self, attn, x, gate_msa, shift_mlp, scale_mlp, gate_mlp)\n\t\n    gate_msa = self.gate_msa(gate_msa)\n    x = x + gate_msa * attn\n    x = self.norm(x)\n    x = self.shift_mlp(x)\n    x = self.scale_mlp(x)\n    gate_mlp = self.gate_mlp(gate_mlp)\n    x = gate_mlp * x\n    return x\n\n", "CodeBleu_score": 39.99293171002566}, {"code": "def post_attention(self, attn, x, gate_msa, shift_mlp, scale_mlp, gate_mlp)\n\t\n    gate_msa = self.gate_msa(gate_msa)\n    x = x + gate_msa * attn\n    x = self.norm(x)\n    x = self.shift_scale(x, shift_mlp, scale_mlp)\n    x = self.gate_mlp(x, gate_mlp)\n    return x\n\n", "CodeBleu_score": 39.59608439381146}]}
{"instance_id": "d9fdc05542a2", "generation_result": [{"code": "def _block_mixing(context, x, context_block, x_block, c)\n\t\n\n    # Process context through context block\n    context_block_output = context_block(context)\n    context_block_output = rearrange(context_block_output, 'b h n d -> b n (h d)')\n\n    # Process x through x block\n    x_block_output = x_block(x)\n    x_block_output = rearrange(x_block_output, 'b h n d -> b n (h d)')\n\n    # Concatenate context and x parts\n    x_and_context_block_output = torch.cat((context_block_output, x_block_output), dim=1)\n\n    # Apply attention\n    qkv = rearrange(x_and_context_block_output, 'b (n qkv) d -> (qkv) b n d', qkv=3)\n    q, k, v = qkv[0], qkv[1], qkv[2]\n\n    # Apply attention\n    context_block_output, x_block_output = _optimized_attention(q, k, v, c)\n\n    # Split context and x parts\n    context_block_output = rearrange(context_block_output, '(qkv) b n d -> b (n qkv) d', qkv=3)\n    x_block_output = rearrange(x_block_output, '(qkv) b n d -> b (n qkv) d', qkv=3)\n\n    # Process context and x parts through their respective post-attention blocks\n    if context_block.post_attention_layer_norm is not None:\n        context_block_output = context_block.post_attention_layer_norm(context_block_output)\n    if context_block.post_attention_dropout is not None:\n        context_block_output = context_block.post_attention_dropout(context_block_output)\n    context_block_output = context_block.post_attention_layer(context_block_output)\n    if x_block.post_attention_layer_norm is not None:\n        x_block_output = x_block.post_attention_layer_norm(x_block_output)\n    if x_block.post_attention_dropout is not None:\n        x_block_output = x_block.post_attention_dropout(x_block_output)\n    x_block", "CodeBleu_score": 27.54332303674234}, {"code": "def _block_mixing(context, x, context_block, x_block, c)\n\t\n    # context_block.pre_attention_block(context)\n    # x_block.pre_attention_block(x)\n    q, k, v = context_block.pre_attention(context)\n    q_, k_, v_ = x_block.pre_attention(x)\n    q = torch.cat((q, q_), dim=1)\n    k = torch.cat((k, k_), dim=1)\n    v = torch.cat((v, v_), dim=1)\n    if c is not None:\n        c = torch.cat((c, c), dim=1)\n    context, x = context_block.attention(q, k, v, c)\n    context, x = context_block.post_attention(context, x)\n    context = context_block.post_attention_block(context)\n    x = x_block.post_attention_block(x)\n    return context, x\n\n", "CodeBleu_score": 19.15591972229172}, {"code": "def _block_mixing(context, x, context_block, x_block, c)\n\t\n    # context and x are split into blocks\n    context_block = context_block(context)\n    x_block = x_block(x)\n    # context and x are processed through their respective pre-attention blocks\n    context_block, x_block = context_block(context_block), x_block(x_block)\n    # context and x are concatenated\n    context_x = torch.cat([context_block, x_block], dim=1)\n    # context_x is processed through the optimized attention mechanism\n    context_x = c(context_x)\n    # context_x is split into context and x\n    context_block, x_block = context_x[:, :context_block.shape[1]], context_x[:, context_block.shape[1]:]\n    # context and x are processed through their respective post-attention blocks\n    context_block, x_block = context_block(context_block), x_block(x_block)\n    return context_block, x_block\n\n", "CodeBleu_score": 16.162917423684185}]}
{"instance_id": "b219bce30160", "generation_result": [{"code": "def get_timestep_embedding(timesteps, embedding_dim)\n\t\n    assert len(timesteps.shape) == 1  # and timesteps.dtype == tf.int32\n    half_dim = embedding_dim // 2\n    emb = math.log(10000) / (half_dim - 1)\n    emb = torch.exp(torch.arange(half_dim, dtype=torch.float32) * -emb)\n    emb = timesteps.float()[:, None] * emb[None, :]\n    emb = torch.cat([torch.sin(emb), torch.cos(emb)], axis=1)\n    if embedding_dim % 2 == 1:\n        emb = torch.nn.functional.pad(emb, (0, 1, 0, 0))\n    assert emb.shape == (timesteps.shape[0], embedding_dim)\n    return emb\n\n", "CodeBleu_score": 81.26037883566804}, {"code": "def get_timestep_embedding(timesteps, embedding_dim)\n\t\n    assert len(timesteps.shape) == 1  # and timesteps.dtype == tf.int32\n    half_dim = embedding_dim // 2\n    # magic number 10000 is from transformers\n    emb = math.log(10000) / (half_dim - 1)\n    # emb = math.log(10000) / (half_dim - 1)\n    emb = torch.exp(torch.arange(half_dim, dtype=torch.float32) * -emb)\n    # emb = tf.range(num_embeddings, dtype=tf.float32)[:, None] * emb[None, :]\n    emb = timesteps.float()[:, None] * emb[None, :]\n    emb = torch.cat([torch.sin(emb), torch.cos(emb)], axis=1)\n    if embedding_dim % 2 == 1:  # zero pad\n        emb = torch.nn.functional.pad(emb, (0, 1, 0, 0))\n    assert emb.shape == (timesteps.shape[0], embedding_dim)\n    return emb\n\n", "CodeBleu_score": 80.79553721789459}, {"code": "def get_timestep_embedding(timesteps, embedding_dim)\n\t\n    assert len(timesteps.shape) == 1  # and timesteps.dtype == tf.int32\n    half_dim = embedding_dim // 2\n    emb = math.log(10000) / (half_dim - 1)\n    emb = torch.exp(torch.arange(half_dim, dtype=torch.float32) * -emb)\n    emb = torch.arange(timesteps.shape[0], dtype=torch.float32).unsqueeze(1) * emb.unsqueeze(0)\n    emb = torch.cat([torch.sin(emb), torch.cos(emb)], axis=1)\n    if embedding_dim % 2 == 1:  # zero pad\n        emb = torch.nn.functional.pad(emb, (0,1,0,0))\n    assert emb.shape == (timesteps.shape[0], embedding_dim)\n    return emb\n\n", "CodeBleu_score": 74.13191912524022}]}
{"instance_id": "6ae8453c86be", "generation_result": [{"code": "def pytorch_attention(q, k, v)\n\t\n    try:\n        # Reshape and transpose the tensors to prepare them for attention computation\n        q = q.transpose(-2, -1)\n        k = k.transpose(-2, -1)\n        v = v.transpose(-2, -1)\n\n        # Compute the attention\n        attention = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(k.shape[-1])\n\n        # Mask the attention\n        attention = comfy.ops.mask_attention(attention)\n\n        # Compute the attention output\n        attention_output = torch.matmul(attention, v)\n\n        # Reshape the attention output back to the original dimensions\n        attention_output = attention_output.transpose(-2, -1)\n        attention_output = attention_output.reshape(q.shape[:-1] + (v.shape[-1],))\n\n        return attention_output\n    except RuntimeError as e:\n        if 'out of memory' in str(e):\n            # Switch to slice attention as a fallback\n            logging.warning(f'Attention computation failed due to OOM. Switching to slice attention as a fallback.')\n            return comfy.ops.slice_attention(q, k, v)\n        else:\n            raise e\n\n", "CodeBleu_score": 29.39849401823742}, {"code": "def pytorch_attention(q, k, v)\n\t\n    try:\n        attn_output, _ = torch.nn.functional.multi_head_attention_forward(\n            query=q, key=k, value=v,\n            embed_dim_to_check=q.shape[1],\n            num_heads=1,\n            q_proj_weight=None,\n            k_proj_weight=None,\n            v_proj_weight=None,\n            in_proj_weight=None,\n            in_proj_bias=None,\n            bias_k=None,\n            bias_v=None,\n            add_zero_attn=False,\n            dropout_p=0,\n            out_proj_weight=None,\n            out_proj_bias=None,\n            use_separate_proj_weight=False,\n            training=True,\n            need_weights=False,\n            attn_mask=None,\n            use_separate_proj_bias=False,\n            q_proj_bias=None,\n            k_proj_bias=None,\n            v_proj_bias=None,\n            static_k=None,\n            static_v=None)\n        return attn_output\n    except RuntimeError as e:\n        if 'out of memory' in str(e):\n            logging.error(\n                'Attention computation failed due to insufficient memory. Switching to slice attention as a fallback.')\n            return comfy.ops.attention(q, k, v)\n        else:\n            raise e\n\n", "CodeBleu_score": 25.28299726175097}, {"code": "def pytorch_attention(q, k, v)\n\t\n    try:\n        attn_output, _ = torch.nn.functional.multi_head_attention_forward(q, k, v, None, None, None, True, 0.0, None, False, True, torch.backends.cudnn.benchmark, False, None, None)\n        return attn_output\n    except RuntimeError as e:\n        if 'out of memory' in str(e):\n            logging.error('Attention: OOM exception occurred. Switching to slice attention.')\n            return comfy.ops.slice_attention(q, k, v)\n        else:\n            raise e\n\n", "CodeBleu_score": 21.577790232310605}]}
{"instance_id": "fe4433c987ad", "generation_result": [{"code": "def apply_control(h, control, name)\n\t\n    if exists(control) and exists(name):\n        if control[name]:\n            try:\n                h = h + control[name]\n            except:\n                logging.warning(f\"Failed to add control {name} to h\")\n    return h\n\n", "CodeBleu_score": 32.951928103561976}, {"code": "def apply_control(h, control, name)\n\t\n    if exists(control) and exists(name):\n        if len(control) == 0:\n            return h\n        try:\n            h = h + control.pop(0)\n            logging.info(f\"Applied control {name} to {h}\")\n        except:\n            logging.warning(f\"Failed to apply control {name} to {h}\")\n    return h\n\n", "CodeBleu_score": 36.06333018208824}, {"code": "def apply_control(h, control, name)\n\t\n    if exists(control) and exists(name) and len(name) > 0:\n        try:\n            h = h + control\n        except:\n            logging.warning(f'Failed to add control {name} to h, skipping.')\n    return h\n\n", "CodeBleu_score": 27.89949195612772}]}
{"instance_id": "56da4547c101", "generation_result": [{"code": "def get_codebook_entry(self, indices, shape)\n\t\n    indices = indices.unsqueeze(-1).to(self.embedding.weight.device)\n    shape = torch.Size(shape) if shape is not None else None\n    quantized = F.embedding(indices, self.embedding.weight)\n    if shape is not None:\n        quantized = quantized.view(shape)\n        quantized = quantized.permute(0, 3, 1, 2)\n    return quantized\n\n", "CodeBleu_score": 24.57713633434094}, {"code": "def get_codebook_entry(self, indices, shape)\n\t\n    indices = indices.unsqueeze(-1).to(self.embedding.weight.device)\n    shape = torch.tensor(shape).to(self.embedding.weight.device)\n    shape = shape.view(1, -1)\n    shape = shape.repeat(indices.size(0), 1)\n    one_hot = torch.zeros(indices.size(0), self.embedding_dim).to(self.embedding.weight.device)\n    one_hot.scatter_(1, indices, 1)\n    quantized = torch.matmul(one_hot, self.embedding.weight)\n    if len(shape) > 0:\n        quantized = quantized.view(shape)\n        perm = list(range(len(shape)))\n        perm[-1], perm[-2] = perm[-2], perm[-1]\n        quantized = quantized.permute(perm)\n    return quantized\n\n", "CodeBleu_score": 33.346606650398265}, {"code": "def get_codebook_entry(self, indices, shape)\n\t\n    indices = indices.unsqueeze(-1).t()\n    shape = torch.tensor(shape).unsqueeze(0)\n    quantized = F.embedding(indices, self.embedding.weight)\n    quantized = quantized.permute(0, 3, 1, 2)\n    return quantized.view(shape)\n\n", "CodeBleu_score": 21.09736691290335}]}
{"instance_id": "548f8a1fb37d", "generation_result": [{"code": "def make_noise(self)\n\t\n    size = 4\n    noise_list = [torch.randn(1, 1, size, size, device=self.device)]\n    while size < self.log_size:\n        size *= 2\n        noise_list.extend(\n            [\n                torch.randn(1, 1, size, size, device=self.device),\n                torch.randn(1, 1, size, size, device=self.device),\n            ]\n        )\n    return noise_list\n\n", "CodeBleu_score": 28.854969275174913}, {"code": "def make_noise(self)\n\t\n    size = 4\n    noise_tensors = [torch.randn(1, 1, size, size, device=self.device)]\n    while size < self.log_size:\n        size *= 2\n        noise_tensors += [torch.randn(1, 1, size, size, device=self.device)]\n        noise_tensors += [torch.randn(1, 1, size, size, device=self.device)]\n    return noise_tensors\n\n", "CodeBleu_score": 30.859816574848242}, {"code": "def make_noise(self)\n\t\n    size = 4\n    noise_list = [torch.randn(1, 1, size, size, device=self.device)]\n    while size < self.log_size:\n        size *= 2\n        noise = torch.randn(1, 1, size, size, device=self.device)\n        noise_list.append(noise)\n        noise_list.append(noise.flip(3))\n    return noise_list\n\n", "CodeBleu_score": 31.110833017909556}]}
{"instance_id": "42fd805563c6", "generation_result": [{"code": "def mean_latent(self, num_latent)\n\t\n    latent_in = torch.randn(num_latent, self.latent_dim).to(self.device)\n    latent = self.style(latent_in).mean(0, keepdim=True)\n    return latent\n\n", "CodeBleu_score": 25.562829813281862}, {"code": "def mean_latent(self, num_latent)\n\t\n    latent_in = torch.randn(num_latent, self.latent_dim, device=self.device)\n    latent = self.style(latent_in).mean(dim=0, keepdim=True)\n    return latent\n\n", "CodeBleu_score": 34.15310890199879}, {"code": "def mean_latent(self, num_latent)\n\t\n    latent_in = torch.randn(num_latent, self.latent_dim).to(self.device)\n    latent_out = self.style(latent_in).mean(0, keepdim=True)\n    return latent_out\n\n", "CodeBleu_score": 24.635452450881974}]}
{"instance_id": "2f485fd924af", "generation_result": [{"code": "def color_jitter(img, shift)\n\t\n    b = shift * torch.randn(1).item()\n    g = shift * torch.randn(1).item()\n    r = shift * torch.randn(1).item()\n    img = adjust_brightness(img, b)\n    img = adjust_contrast(img, g)\n    img = adjust_hue(img, r)\n    img = adjust_saturation(img, r)\n    return img\n\n", "CodeBleu_score": 31.236834390345653}, {"code": "def color_jitter(img, shift)\n\t\n    img = img.astype(np.float32)\n    # Randomly shift the values for each channel.\n    img[:, :, 0] += np.random.uniform(-shift, shift)\n    img[:, :, 1] += np.random.uniform(-shift, shift)\n    img[:, :, 2] += np.random.uniform(-shift, shift)\n    # Clip the pixel values to ensure they remain between 0 and 1.\n    img = np.clip(img, 0, 255)\n    return img.astype(np.uint8)\n\n", "CodeBleu_score": 26.17635163787806}, {"code": "def color_jitter(img, shift)\n\t\n    # Convert the image from [0, 1] to [0, 255]\n    img = img * 255\n\n    # Generate random jitter values\n    jitter = np.random.uniform(-shift, shift, 3)\n\n    # Apply the jitter shift\n    img = img + jitter\n\n    # Clip pixel values to [0, 255]\n    img = np.clip(img, 0, 255)\n\n    # Convert the image back to [0, 1]\n    img = img / 255\n\n    return img\n\n", "CodeBleu_score": 57.198737392816845}]}
{"instance_id": "c87ccf1fbf48", "generation_result": [{"code": "def feed_data(self, data)\n\t\n    self.lq = data['lq'].to(self.device)\n    if 'gt' in data:\n        self.gt = data['gt'].to(self.device)\n\n    if 'facial_component_locations' in data:\n        self.left_eye_location = data['facial_component_locations']['left_eye']\n        self.right_eye_location = data['facial_component_locations']['right_eye']\n        self.mouth_location = data['facial_component_locations']['mouth']\n\n", "CodeBleu_score": 53.46993814213762}, {"code": "def feed_data(self, data)\n\t\n    self.lq = data['lq'].to(self.device)\n    if 'gt' in data:\n        self.gt = data['gt'].to(self.device)\n    if 'left_eye' in data:\n        self.left_eye = data['left_eye'].to(self.device)\n    if 'right_eye' in data:\n        self.right_eye = data['right_eye'].to(self.device)\n    if 'mouth' in data:\n        self.mouth = data['mouth'].to(self.device)\n\n", "CodeBleu_score": 56.77114126112309}, {"code": "def feed_data(self, data)\n\t\n    self.lq = data['lq'].to(self.device)\n    if 'gt' in data:\n        self.gt = data['gt'].to(self.device)\n\n    if 'facial_component_locations' in data:\n        self.facial_component_locations = data['facial_component_locations'].to(self.device)\n        self.left_eye_location = self.facial_component_locations[..., 0:2]\n        self.right_eye_location = self.facial_component_locations[..., 2:4]\n        self.mouth_location = self.facial_component_locations[..., 4:6]\n\n", "CodeBleu_score": 50.59120359976389}]}
{"instance_id": "8f8c7cb13037", "generation_result": [{"code": "def construct_img_pyramid(self)\n\t\n    img_pyramid = [self.gt]\n    for i in range(self.num_scales - 1):\n        img_pyramid.insert(0, F.interpolate(img_pyramid[0], scale_factor=0.5, mode='bilinear', align_corners=False))\n    return img_pyramid\n\n", "CodeBleu_score": 25.88876705957451}, {"code": "def construct_img_pyramid(self)\n\t\n    img_pyramid = [self.gt]\n    for i in range(self.num_scales - 1):\n        img_pyramid.insert(0, F.interpolate(img_pyramid[0], scale_factor=0.5, mode='bilinear', align_corners=False))\n    return img_pyramid\n\n", "CodeBleu_score": 25.88876705957451}, {"code": "def construct_img_pyramid(self)\n\t\n    img_pyramid = [self.lq]\n    for i in range(self.num_scales - 1):\n        img_pyramid.insert(0, F.interpolate(img_pyramid[0], scale_factor=0.5, mode='bilinear', align_corners=False))\n    return img_pyramid\n\n", "CodeBleu_score": 25.490128656324384}]}
{"instance_id": "51247274d40c", "generation_result": [{"code": "def save(self, epoch, current_iter)\n\t\n    state_dict = {\n        'epoch': epoch,\n        'iter': current_iter,\n        'gen_state_dict': self.gen.state_dict(),\n        'gen_ema_state_dict': self.gen_ema.state_dict(),\n        'dis_state_dict': self.dis.state_dict(),\n        'opt_state_dict': self.opt.state_dict(),\n    }\n    if self.dis_left is not None:\n        state_dict['dis_left_state_dict'] = self.dis_left.state_dict()\n    if self.dis_right is not None:\n        state_dict['dis_right_state_dict'] = self.dis_right.state_dict()\n    if self.dis_mouth is not None:\n        state_dict['dis_mouth_state_dict'] = self.dis_mouth.state_dict()\n    if self.dis_face is not None:\n        state_dict['dis_face_state_dict'] = self.dis_face.state_dict()\n    if self.dis_face_mask is not None:\n        state_dict['dis_face_mask_state_dict'] = self.dis_face_mask.state_dict()\n    torch.save(state_dict, self.opt['path']['model_state_file'])\n\n", "CodeBleu_score": 28.711379283716358}, {"code": "def save(self, epoch, current_iter)\n\t\n    state_dict = OrderedDict()\n    state_dict['epoch'] = epoch\n    state_dict['iter'] = current_iter\n    state_dict['net_g'] = self.net_g.state_dict()\n    state_dict['net_g_ema'] = self.net_g_ema.state_dict()\n    state_dict['net_d'] = self.net_d.state_dict()\n    state_dict['net_d_left'] = self.net_d_left.state_dict()\n    state_dict['net_d_right'] = self.net_d_right.state_dict()\n    state_dict['net_d_mouth'] = self.net_d_mouth.state_dict()\n    state_dict['optim_g'] = self.optim_g.state_dict()\n    state_dict['optim_d'] = self.optim_d.state_dict()\n    state_dict['optim_d_left'] = self.optim_d_left.state_dict()\n    state_dict['optim_d_right'] = self.optim_d_right.state_dict()\n    state_dict['optim_d_mouth'] = self.optim_d_mouth.state_dict()\n    state_dict['logger'] = self.logger\n    state_dict['ema_decay'] = self.ema_decay\n    state_dict['current_lr_g'] = self.current_lr_g\n    state_dict['current_lr_d'] = self.current_lr_d\n    state_dict['current_lr_d_left'] = self.current_lr_d_left\n    state_dict['current_lr_d_right'] = self.current_lr_d_right\n    state_dict['current_lr_d_mouth'] = self.current_lr_d_mouth\n    torch.save(state_dict, self.checkpoint_path)\n\n", "CodeBleu_score": 24.772142829964515}, {"code": "def save(self, epoch, current_iter)\n\t\n    # Save generators, discriminators, and optimizers\n    net_g_ema = self.net_g_ema.module if hasattr(self.net_g_ema, 'module') else self.net_g_ema\n    state_dict = OrderedDict()\n    state_dict['net_g'] = self.net_g.state_dict()\n    state_dict['net_g_ema'] = net_g_ema.state_dict()\n    state_dict['net_d'] = self.net_d.state_dict()\n    state_dict['net_d_left'] = self.net_d_left.state_dict()\n    state_dict['net_d_right'] = self.net_d_right.state_dict()\n    state_dict['net_d_mouth'] = self.net_d_mouth.state_dict()\n    state_dict['optimizer_g'] = self.optimizer_g.state_dict()\n    state_dict['optimizer_d'] = self.optimizer_d.state_dict()\n    state_dict['optimizer_d_left'] = self.optimizer_d_left.state_dict()\n    state_dict['optimizer_d_right'] = self.optimizer_d_right.state_dict()\n    state_dict['optimizer_d_mouth'] = self.optimizer_d_mouth.state_dict()\n    state_dict['epoch'] = epoch\n    state_dict['iter'] = current_iter\n    state_dict['current_lr_g'] = self.current_lr_g\n    state_dict['current_lr_d'] = self.current_lr_d\n    state_dict['current_lr_d_left'] = self.current_lr_d_left\n    state_dict['current_lr_d_right'] = self.current_lr_d_right\n    state_dict['current_lr_d_mouth'] = self.current_lr_d_mouth\n    torch.save(state_dict, self.opt['path']['model_state_file'])\n\n    # Save training states\n    state_dict = OrderedDict()\n    state_dict['train_opt'] = self.opt['train']\n    state_dict['val_opt'] = self.opt['val']\n    torch.save(state_dict, self.", "CodeBleu_score": 32.11624480853453}]}
{"instance_id": "afa0dc8128c1", "generation_result": [{"code": "def clean_folder(folder)\n\t\n    for filename in os.listdir(folder):\n        file_path = os.path.join(folder, filename)\n        try:\n            if os.path.isfile(file_path) or os.path.islink(file_path):\n                os.unlink(file_path)\n            elif os.path.isdir(file_path):\n                shutil.rmtree(file_path)\n        except Exception as e:\n            print('Failed to delete %s. Reason: %s' % (file_path, e))\n\n", "CodeBleu_score": 67.88650947584345}, {"code": "def clean_folder(folder)\n\t\n    for filename in os.listdir(folder):\n        file_path = os.path.join(folder, filename)\n        try:\n            if os.path.isfile(file_path) or os.path.islink(file_path):\n                os.unlink(file_path)\n            elif os.path.isdir(file_path):\n                shutil.rmtree(file_path)\n        except Exception as e:\n            print('Failed to delete %s. Reason: %s' % (file_path, e))\n\n", "CodeBleu_score": 67.88650947584345}, {"code": "def clean_folder(folder)\n\t\n    for filename in os.listdir(folder):\n        file_path = os.path.join(folder, filename)\n        try:\n            if os.path.isfile(file_path) or os.path.islink(file_path):\n                os.unlink(file_path)\n            elif os.path.isdir(file_path):\n                shutil.rmtree(file_path)\n        except Exception as e:\n            print('Failed to delete %s. Reason: %s' % (file_path, e))\n\n", "CodeBleu_score": 67.88650947584345}]}
{"instance_id": "53588b481bdd", "generation_result": [{"code": "def next_batch(self, sample_size=1)\n\t\n    if sample_size > len(self.buffer):\n        sample_size = len(self.buffer)\n    batch = random.sample(self.buffer, sample_size)\n    self.buffer = [x for x in self.buffer if x not in batch]\n    return batch\n", "CodeBleu_score": 70.95631070572395}, {"code": "def next_batch(self, sample_size=1)\n\t\n    batch = []\n    if sample_size > len(self.memory):\n        sample_size = len(self.memory)\n    for i in range(sample_size):\n        batch.append(self.memory.pop(random.randint(0, len(self.memory)-1)))\n    return batch\n\n", "CodeBleu_score": 41.19446067547071}, {"code": "def next_batch(self, sample_size=1)\n\t\n    batch = []\n    for i in range(sample_size):\n        if len(self.memory) > 0:\n            batch.append(self.memory.pop(random.randint(0, len(self.memory) - 1)))\n        else:\n            break\n    return batch\n", "CodeBleu_score": 27.47771832924448}]}
{"instance_id": "6bfa6a4e621c", "generation_result": [{"code": "def find_estimated_top_configs(self)\n\t\n    all_configs = self.all_configs\n    cost_model = self.cost_model\n    metric = self.metric\n    if metric == 'time':\n        metric = 0\n    elif metric == 'energy':\n        metric = 1\n    elif metric == 'area':\n        metric = 2\n    elif metric == 'power':\n        metric = 3\n    elif metric == 'latency':\n        metric = 4\n    elif metric == 'resources':\n        metric = 5\n    else:\n        print(\"Error: Invalid metric specified. Please choose one of the following: time, energy, area, power, latency, resources\")\n        return\n    # Flatten all configurations\n    flattened_configs = []\n    for config in all_configs:\n        flattened_configs.append(list(itertools.chain.from_iterable(config)))\n    # Collect numerical values\n    numerical_values = []\n    for config in flattened_configs:\n        numerical_values.append(collect_numerical_values(config))\n    # Convert to numpy array\n    numerical_values = np.array(numerical_values)\n    # Predict cost for each configuration\n    costs = cost_model.predict(numerical_values)\n    # Sort configurations based on cost\n    sorted_indices = np.argsort(costs)\n    return sorted_indices\n\n", "CodeBleu_score": 28.689472773920038}, {"code": "def find_estimated_top_configs(self)\n\t\n    # Flatten all_configs\n    all_configs_flattened = [item for sublist in self.all_configs for item in sublist]\n    # Collect numerical values from all_configs_flattened\n    all_configs_flattened_numerical = []\n    for config in all_configs_flattened:\n        all_configs_flattened_numerical.append(\n            [config[key] for key in self.search_space])\n    # Convert all_configs_flattened_numerical into a numpy array\n    all_configs_flattened_numerical = np.array(all_configs_flattened_numerical)\n    # Predict the cost of each configuration\n    costs = self.cost_model.predict(all_configs_flattened_numerical)\n    # Sort the configurations based on the specified metric\n    if self.metric == 'val_acc':\n        sorted_indices = np.argsort(costs)[::-1]\n    elif self.metric == 'flops':\n        sorted_indices = np.argsort(costs)\n    elif self.metric == 'latency':\n        sorted_indices = np.argsort(costs)\n    elif self.metric == 'params':\n        sorted_indices = np.argsort(costs)\n    elif self.metric == 'latency_reg':\n        sorted_indices = np.argsort(costs)\n    elif self.metric == 'latency_pipelined':\n        sorted_indices = np.argsort(costs)\n    elif self.metric == 'latency_pipelined_reg':\n        sorted_indices = np.argsort(costs)\n    elif self.metric == 'latency_pipelined_reg_multithread':\n        sorted_indices = np.argsort(costs)\n    elif self.metric == 'latency_pipelined_reg_multithread_single_batch':\n        sorted_indices = np.argsort(costs)\n    elif self.metric == 'latency_pipelined_reg_multithread_single_batch_double_buffer':\n        sorted_indices = np.argsort(costs)\n    elif self.metric == 'latency_pipelined_reg_multithread_single_batch_double_buffer_no_reuse':\n        sorted_indices = np.argsort(costs)\n    elif self.metric == 'latency_pipelined_reg_multithread_single_batch_double", "CodeBleu_score": 17.076900266176857}, {"code": "def find_estimated_top_configs(self)\n\t\n    all_configs = self.all_configs\n    all_configs_flattened = [config for config_list in all_configs for config in config_list]\n    all_configs_flattened_numerical = [\n        [config[key] for key in self.cost_model_config_keys]\n        for config in all_configs_flattened\n    ]\n    all_configs_flattened_numerical = np.array(all_configs_flattened_numerical)\n    all_configs_flattened_numerical_estimates = self.cost_model.predict(\n        all_configs_flattened_numerical\n    )\n    all_configs_flattened_numerical_estimates = np.array(\n        all_configs_flattened_numerical_estimates\n    )\n    if self.cost_model_metric == \"min\":\n        all_configs_flattened_numerical_estimates = -1 * all_configs_flattened_numerical_estimates\n    all_configs_flattened_numerical_estimates_sorted_indices = np.argsort(\n        all_configs_flattened_numerical_estimates\n    )\n    all_configs_flattened_numerical_estimates_sorted = all_configs_flattened_numerical_estimates[\n        all_configs_flattened_numerical_estimates_sorted_indices\n    ]\n    estimated_top_configs_indices = all_configs_flattened_numerical_estimates_sorted_indices[\n        : self.cost_model_top_k\n    ]\n    return estimated_top_configs_indices\n\n", "CodeBleu_score": 20.746990956246837}]}
{"instance_id": "d4cf48413a57", "generation_result": [{"code": "def flatten(d, parent_key='', sep='_')\n\t\n    items = []\n    for k, v in d.items():\n        new_key = parent_key + sep + k if parent_key else k\n        if isinstance(v, collections.abc.MutableMapping):\n            items.extend(flatten(v, new_key, sep=sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)\n\n", "CodeBleu_score": 82.56863782179195}, {"code": "def flatten(d, parent_key='', sep='_')\n\t\n    items = []\n    for k, v in d.items():\n        new_key = parent_key + sep + k if parent_key else k\n        if isinstance(v, collections.abc.MutableMapping):\n            items.extend(flatten(v, new_key, sep=sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)\n\n", "CodeBleu_score": 82.56863782179195}, {"code": "def flatten(d, parent_key='', sep='_')\n\t\n    items = []\n    for k, v in d.items():\n        new_key = parent_key + sep + k if parent_key else k\n        if isinstance(v, collections.abc.MutableMapping):\n            items.extend(flatten(v, new_key, sep=sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)\n\n", "CodeBleu_score": 82.56863782179195}]}
{"instance_id": "c2a514249c25", "generation_result": [{"code": "def run_after_tuning(self)\n\t\n\n    if self.optimal_cmd:\n        self.logger.info(\n            f\"Optimal command found: {self.optimal_cmd}", "CodeBleu_score": 14.266684910814037}, {"code": "def run_after_tuning(self)\n\t\n    if self.optimal_cmd:\n        self.logger.info(f\"Running command: {self.optimal_cmd}\")\n        self.optimal_cmd = self.optimal_cmd.split()\n        process = subprocess.Popen(self.optimal_cmd)\n        process.wait()\n        self.logger.info(f\"Command completed: {self.optimal_cmd}\")\n    else:\n        self.logger.info(f\"Autotuning did not find an optimal configuration\")\n\n", "CodeBleu_score": 53.5060528478718}, {"code": "def run_after_tuning(self)\n\t\n    if self.optimal_cmd is not None:\n        self.logger.info(\n            \"Optimal command found, running command: {}\".format(self.optimal_cmd)\n        )\n        subprocess.Popen(self.optimal_cmd, shell=True)\n        self.logger.info(\"Command completed.\")\n    else:\n        self.logger.info(\n            \"No optimal command found. Please run autotuning again to find an optimal configuration.\"\n        )\n\n", "CodeBleu_score": 33.4093644210819}]}
{"instance_id": "61802de52df0", "generation_result": [{"code": "def run_job(self, exp: dict, reservations)\n\t\n    # Set the experiment's master port and result directory\n    exp[\"master_port\"] = self.master_port\n    exp[\"result_dir\"] = self.result_dir\n    exp[\"reservations\"] = reservations\n    # Update user arguments based on argument mappings\n    for arg in exp[\"args\"]:\n        if arg in self.argument_mappings:\n            exp[\"args\"][arg] = self.argument_mappings[arg]\n    # Start the experiment\n    thread = threading.Thread(target=run_experiment, args=(exp, self.script, self.args))\n    thread.start()\n    # Store the running experiment details with a timestamp\n    self.running_experiments.append(exp)\n    self.running_experiments[-1][\"start_time\"] = time.time()\n\n", "CodeBleu_score": 19.577980227617715}, {"code": "def run_job(self, exp: dict, reservations)\n\t\n\n    # Set experiment's master port\n    self.master_port = str(self.master_port)\n\n    # Set experiment's result directory\n    exp_name = exp[\"name\"]\n    exp_result_dir = self.result_dir + \"/\" + exp_name\n    exp[\"result_dir\"] = exp_result_dir\n    exp[\"master_port\"] = self.master_port\n\n    # Update user arguments based on argument mappings\n    for arg in exp[\"args\"]:\n        if \"arg_mapping\" in arg:\n            for mapping in arg[\"arg_mapping\"]:\n                if mapping[\"arg\"] in self.args:\n                    arg[\"value\"] = mapping[\"value\"]\n\n    # Create a new thread to run the experiment with the specified script and arguments, and start the thread\n    thread = threading.Thread(target=run_exp, args=(self, exp))\n    thread.start()\n\n    # Store running experiment details\n    running_experiments = []\n    if os.path.exists(self.running_experiments_path):\n        with open(self.running_experiments_path, \"r\") as f:\n            running_experiments = json.load(f)\n    running_experiments.append(exp)\n    with open(self.running_experiments_path, \"w\") as f:\n        json.dump(running_experiments, f)\n\n    # Increment master port\n    self.master_port = int(self.master_port) + 1\n\n", "CodeBleu_score": 32.96859972629041}, {"code": "def run_job(self, exp: dict, reservations)\n\t\n    # Set the master port\n    exp[\"master_port\"] = self.master_port\n\n    # Set the result directory\n    exp[\"result_dir\"] = self.result_dir\n\n    # Update user arguments based on argument mappings\n    if \"args\" in exp:\n        for arg in exp[\"args\"]:\n            if arg in self.argument_mappings:\n                exp[\"args\"][arg] = self.argument_mappings[arg]\n\n    # Run the experiment in a new thread\n    thread = threading.Thread(target=self.run_experiment, args=(exp, reservations))\n    thread.start()\n    self.running_experiments.append((exp, thread))\n\n", "CodeBleu_score": 22.353134721846047}]}
{"instance_id": "4884e2e9665c", "generation_result": [{"code": "def save_exp_results_to_database(self, message, ranks=None, path=None):\n\t\n    if self.distributed_rank is None:\n        if ranks is None:\n            ranks = [0]\n        if self.rank in ranks:\n            print(message)\n    else:\n        if ranks is None:\n            ranks = list(range(self.world_size))\n        if self.distributed_rank in ranks:\n            message[\"rank\"] = self.distributed_rank\n            if self.rank == 0:\n                if path is None:\n                    path = self.exp_path\n                with open(path, \"a\") as file:\n                    file.write(json.dumps(message) + \"", "CodeBleu_score": 26.268133139940307}, {"code": "def save_exp_results_to_database(self, message, ranks=None, path=None):\n\t\n    if self.is_distributed:\n        if ranks is None:\n            ranks = [0]\n        if self.rank in ranks:\n            message['rank'] = self.rank\n            if self.is_master:\n                if path is None:\n                    path = self.log_path\n                with open(path, 'a') as f:\n                    f.write(json.dumps(message) + '", "CodeBleu_score": 24.74454665538867}, {"code": "def save_exp_results_to_database(self, message, ranks=None, path=None):\n\t\n    if ranks is None:\n        ranks = [0]\n    if path is None:\n        path = \"results.json\"\n    if dist.is_initialized() and dist.get_rank() in ranks:\n        message[\"rank\"] = dist.get_rank()\n        with open(path, \"a\") as f:\n            f.write(json.dumps(message) + \"", "CodeBleu_score": 24.625435300929077}]}
{"instance_id": "0429344c3c4d", "generation_result": [{"code": "def _build_tp_other_layer_map(self, layer_index: int)\n\t\n    layer_file_prefix = f\"layer_{layer_index}_\"\n    layer_files = [f for f in os.listdir(self.data_path) if f.startswith(layer_file_prefix)]\n    if not layer_files:\n        raise ValueError(\"No layer files found for the given layer index.\")\n    if layer_index not in [int(re.search(r\"\\d+\", f).group()) for f in layer_files]:\n        raise ValueError(\"Layer index not found in layer files.\")\n    layer_files = sorted(layer_files, key=lambda x: int(re.search(r\"\\d+\", x).group()))\n    layer_files = [os.path.join(self.data_path, f) for f in layer_files]\n    layer_files = [f for f in layer_files if os.path.isfile(f)]\n    layer_files = [f for f in layer_files if not os.path.getsize(f) == 0]\n    layer_files_partitioned = []\n    for i in range(self.tp_degree):\n        layer_files_partitioned.append([f for f in layer_files if int(re.search(r\"\\d+\", f).group()) % self.tp_degree == i])\n    return {i: layer_files_partitioned[i] for i in range(self.tp_degree)}\n\n", "CodeBleu_score": 31.18947194598652}, {"code": "def _build_tp_other_layer_map(self, layer_index: int)\n\t\n    # check if there are any layer files\n    layer_files = self.get_layer_files(layer_index)\n    if len(layer_files) == 0:\n        raise ValueError(\"No layer files found for layer index \" + str(layer_index) + \".\")\n\n    # check if layer index is valid\n    if layer_index < 0 or layer_index >= len(self.layer_files):\n        raise ValueError(\"Layer index \" + str(layer_index) + \" is invalid.\")\n\n    # build map\n    layer_file_map = {}\n    for file in layer_files:\n        # get partition index\n        partition_index = int(file.split(\"_\")[1])\n        # add file to map\n        if partition_index in layer_file_map:\n            layer_file_map[partition_index].append(file)\n        else:\n            layer_file_map[partition_index] = [file]\n\n    return layer_file_map\n\n", "CodeBleu_score": 34.547643327446934}, {"code": "def _build_tp_other_layer_map(self, layer_index: int)\n\t\n    layer_files = self.get_layer_files(layer_index)\n    if len(layer_files) == 0:\n        raise RuntimeError(\"No layer files found for layer index: \" + str(layer_index))\n    if layer_index >= len(self.layer_files):\n        raise RuntimeError(\"Layer index: \" + str(layer_index) + \" is out of bounds.\")\n    layer_files = [f for f in layer_files if f.startswith(self.prefix + str(layer_index) + \"_\")]\n    if len(layer_files) == 0:\n        raise RuntimeError(\"No layer files found for layer index: \" + str(layer_index))\n    tp_degree = self.tp_degree\n    if tp_degree == 0:\n        raise RuntimeError(\"TP degree is not set.\")\n    partition_files = [layer_files[i:i + tp_degree] for i in range(0, len(layer_files), tp_degree)]\n    return {i: partition_files[i] for i in range(len(partition_files))}\n\n", "CodeBleu_score": 34.552037558390545}]}
{"instance_id": "1b65ecb9be93", "generation_result": [{"code": "def system_info()\n\t\n    sysinfo = {}\n    sysinfo[\"system\"] = platform.system()\n    sysinfo[\"architecture\"] = platform.architecture()\n    sysinfo[\"version\"] = platform.version()\n    sysinfo[\"processor\"] = platform.processor()\n    sysinfo[\"python_version\"] = platform.python_version()\n    return sysinfo\n\n", "CodeBleu_score": 42.52877312612896}, {"code": "def system_info()\n\t\n    system_info = {\n        \"Operating System\": platform.system(),\n        \"Architecture\": platform.machine(),\n        \"Version\": platform.version(),\n        \"Processor\": platform.processor(),\n        \"Python Version\": sys.version,\n    }\n    return system_info\n\n", "CodeBleu_score": 49.46055865992138}, {"code": "def system_info()\n\t\n    return {\n        \"Operating System\": platform.system(),\n        \"Architecture\": platform.machine(),\n        \"Version\": platform.version(),\n        \"Processor\": platform.processor(),\n        \"Python Version\": platform.python_version(),\n        \"PyTorch Version\": torch.__version__,\n        \"Numpy Version\": numpy.__version__,\n        \"TTS Version\": TTS.__version__,\n    }\n\n", "CodeBleu_score": 60.94491882393018}]}
{"instance_id": "969bcc5cf3b5", "generation_result": [{"code": "def cuda_info()\n\t\n    try:\n        device_count = torch.cuda.device_count()\n        devices = []\n        for i in range(device_count):\n            devices.append(torch.cuda.get_device_name(i))\n        cuda_available = True\n    except:\n        devices = ['None']\n        cuda_available = False\n    return devices, cuda_available, torch.version.cuda\n\n", "CodeBleu_score": 46.73872400454092}, {"code": "def cuda_info()\n\t\n    try:\n        import torch\n    except ImportError:\n        print('PyTorch is not installed. Please install it first.')\n        return\n    print('PyTorch version: {}'.format(torch.__version__))\n    print('CUDA version: {}'.format(torch.version.cuda))\n    print('cuDNN version: {}'.format(torch.backends.cudnn.version()))\n    print('Number of GPUs: {}'.format(torch.cuda.device_count()))\n    print('CUDA devices: {}'.format(torch.cuda.get_device_name(0)))\n\n", "CodeBleu_score": 31.86519616382433}, {"code": "def cuda_info()\n\t\n    print(\"CUDA available: \", torch.cuda.is_available())\n    print(\"CUDA version: \", torch.version.cuda)\n    print(\"GPU devices: \")\n    for i in range(torch.cuda.device_count()):\n        print(\"    \", torch.cuda.get_device_name(i))\n\n", "CodeBleu_score": 52.138029548472865}]}
{"instance_id": "a2ba908a4c70", "generation_result": [{"code": "def compute_encoder_accuracy(dataset_items, encoder_manager)\n\t\n    # get class name key and optional class ID to class name mapping\n    class_name_key = encoder_manager.config['preprocessing']['class_name_key']\n    class_id_to_name = encoder_manager.config['preprocessing']['class_id_to_name']\n    # create a dictionary to record the accuracy for each class\n    class_accuracy_dict = {}\n    # loop through the dataset items\n    for item in tqdm(dataset_items):\n        # get audio file path\n        audio_file_path = item[0]\n        # get class name\n        class_name = item[1][class_name_key]\n        # compute embedding\n        embedding = encoder_manager.compute_embedding(audio_file_path)\n        # predict class label\n        predicted_class_id = encoder_manager.predict_class_label(embedding)\n        # get class name from class ID if class ID to class name mapping is available\n        if class_id_to_name is not None:\n            predicted_class_name = class_id_to_name[predicted_class_id]\n        else:\n            predicted_class_name = predicted_class_id\n        # record accuracy\n        if class_name not in class_accuracy_dict:\n            class_accuracy_dict[class_name] = 0\n        if predicted_class_name == class_name:\n            class_accuracy_dict[class_name] += 1\n    # calculate and print accuracy for each class\n    for class_name in class_accuracy_dict:\n        class_accuracy = class_accuracy_dict[class_name] / len(dataset_items)\n        print('Class: {}, Accuracy: {:.4f}'.format(class_name, class_accuracy))\n    # calculate and print average accuracy across all classes\n    total_accuracy = sum(class_accuracy_dict.values()) / len(dataset_items)\n    print('Total Accuracy: {:.4f}'.format(total_accuracy))\n\n", "CodeBleu_score": 31.09245022168134}, {"code": "def compute_encoder_accuracy(dataset_items, encoder_manager)\n\t\n    # get class name key and optional class ID to class name mapping\n    class_name_key = encoder_manager.config['class_name_key']\n    class_id_to_class_name = encoder_manager.config['class_id_to_class_name']\n    # initialize accuracy dictionary\n    accuracy = {}\n    # iterate over dataset items\n    for item in tqdm(dataset_items):\n        # get audio file path\n        audio_file = item['audio_filepath']\n        # compute embedding\n        embedding = encoder_manager.get_embedding(audio_file)\n        # predict class label\n        predicted_label = encoder_manager.predict_class_label(embedding)\n        # get true class name\n        true_class_name = item[class_name_key]\n        # get true class ID\n        true_class_id = item['speaker_id']\n        # if class ID to class name mapping is available\n        if class_id_to_class_name is not None:\n            # get true class name from mapping\n            true_class_name = class_id_to_class_name[true_class_id]\n        # if true class name is not in accuracy dictionary\n        if true_class_name not in accuracy:\n            # add true class name to accuracy dictionary\n            accuracy[true_class_name] = 0\n        # if predicted label matches true class name\n        if predicted_label == true_class_name:\n            # increment accuracy\n            accuracy[true_class_name] += 1\n    # calculate average accuracy\n    total_accuracy = 0\n    for key in accuracy:\n        total_accuracy += accuracy[key]\n    average_accuracy = total_accuracy / len(accuracy)\n    # print accuracy for each class\n    for key in accuracy:\n        print(f'{key} accuracy: {accuracy[key] / len(dataset_items) * 100:.2f}%')\n    # print average accuracy\n    print(f'average accuracy: {average_accuracy * 100:.2f}%')\n\n", "CodeBleu_score": 27.685801945650017}, {"code": "def compute_encoder_accuracy(dataset_items, encoder_manager)\n\t\n    # get class name to class ID mapping\n    class_name_to_id_map = encoder_manager.get_class_name_to_id_map()\n    # get class ID to class name mapping\n    class_id_to_name_map = encoder_manager.get_class_id_to_name_map()\n    # initialize accuracy dictionary\n    accuracy = {}\n    # iterate over all items in the dataset\n    for item in tqdm(dataset_items, desc='Computing encoder accuracy'):\n        # get audio file path\n        audio_file_path = item['audio_filepath']\n        # get true class name\n        true_class_name = item['speaker_name']\n        # get true class ID\n        true_class_id = class_name_to_id_map[true_class_name]\n        # get true class ID\n        true_class_id = class_name_to_id_map[true_class_name]\n        # get true class ID\n        true_class_id = class_name_to_id_map[true_class_name]\n        # get true class ID\n        true_class_id = class_name_to_id_map[true_class_name]\n        # get true class ID\n        true_class_id = class_name_to_id_map[true_class_name]\n        # get true class ID\n        true_class_id = class_name_to_id_map[true_class_name]\n        # get true class ID\n        true_class_id = class_name_to_id_map[true_class_name]\n        # get true class ID\n        true_class_id = class_name_to_id_map[true_class_name]\n        # get true class ID\n        true_class_id = class_name_to_id_map[true_class_name]\n        # get true class ID\n        true_class_id = class_name_to_id_map[true_class_name]\n        # get true class ID\n        true_class_id = class_name_to_id_map[true_class_name]\n        # get true class ID\n        true_class_id = class_name_to_id_map[true_class_name]\n        # get true class ID\n        true_class_id = class_name_to_", "CodeBleu_score": 15.490137916279552}]}
{"instance_id": "e01ec69ad58a", "generation_result": [{"code": "def adjust_path_and_remove_silence(audio_path)\n\t\n    # get input and output paths\n    audio_path = pathlib.Path(audio_path)\n    output_path = audio_path.parent.parent / 'no_silence' / audio_path.parent.name / audio_path.name\n    output_path.parent.mkdir(parents=True, exist_ok=True)\n    # check if file already exists\n    if output_path.exists():\n        if args.force:\n            os.remove(output_path)\n        else:\n            return output_path, False\n    # load model\n    model, utils = get_vad_model_and_utils(args.model_path)\n    # load audio\n    audio, sr = torch.load(audio_path)\n    # predict voice activity\n    voice_prob = model(audio)\n    voice_mask = utils.get_voice_mask(voice_prob, args.threshold)\n    # remove silence\n    audio = remove_silence(audio, voice_mask)\n    # save audio\n    torch.save(audio, output_path)\n    return output_path, True\n\n", "CodeBleu_score": 40.99441160894337}, {"code": "def adjust_path_and_remove_silence(audio_path)\n\t\n    audio_path = pathlib.Path(audio_path)\n    output_path = audio_path.parent.parent.parent / \"silence_removed\" / audio_path.parent.name / audio_path.name\n    output_path.parent.mkdir(parents=True, exist_ok=True)\n    if output_path.exists() and not args.force:\n        return output_path, False\n    else:\n        vad_model, vad_utils = get_vad_model_and_utils(args.vad_model_path)\n        audio, sr = vad_utils.load_wav(audio_path)\n        audio = vad_utils.normalize_volume(audio, audio_norm_target_dBFS=-30)\n        audio = vad_utils.trim_long_silences(audio, vad_model, args.silence_threshold, args.min_silence_duration)\n        audio = vad_utils.trim_short_silences(audio, args.silence_threshold, args.min_silence_duration)\n        vad_utils.save_wav(audio, output_path)\n        return output_path, True\n\n", "CodeBleu_score": 43.92279212909759}, {"code": "def adjust_path_and_remove_silence(audio_path)\n\t\n    # get audio file name and extension\n    audio_path = pathlib.Path(audio_path)\n    audio_name = audio_path.stem\n    audio_ext = audio_path.suffix\n    # get output path\n    output_path = audio_path.parent.parent.parent.parent.joinpath(\n        \"datasets\", \"VCTK\", \"processed\", audio_name + \".wav\")\n    # check if file already exists\n    if output_path.exists():\n        if args.force:\n            output_path.unlink()\n        else:\n            print(f\"File {output_path} already exists.\")\n            return output_path, False\n    # create output directory if not exists\n    output_path.parent.mkdir(parents=True, exist_ok=True)\n    # load model and utils\n    model, utils = get_vad_model_and_utils()\n    # load audio\n    audio, rate = torchaudio.load(audio_path)\n    # normalize audio\n    audio = audio.numpy()\n    audio = audio / abs(max(audio.flatten()))\n    # apply voice activity detection\n    audio = audio.squeeze(0)\n    vad = model(audio)\n    vad = vad.numpy()\n    # remove silence\n    audio = remove_silence(audio, vad, rate, utils)\n    # save audio\n    torchaudio.save(output_path, torch.from_numpy(audio), rate)\n    return output_path, True\n\n", "CodeBleu_score": 37.04906462146944}]}
{"instance_id": "7e0bdb0feff8", "generation_result": [{"code": "def resample_files(input_dir, output_sr, output_dir=None, file_ext=\"wav\", n_jobs=10)\n\t\n    if output_dir is not None:\n        copytree(input_dir, output_dir)\n    audio_files = glob.glob(os.path.join(input_dir, \"**\", f\"*.{file_ext}\"), recursive=True)\n    if len(audio_files) == 0:\n        print(f\"No audio files found in {input_dir}\")\n        return\n    with Pool(processes=n_jobs) as pool:\n        resampled_files = list(\n            tqdm(\n                pool.imap_unordered(\n                    librosa.load,\n                    audio_files,\n                    chunksize=1,\n                    desc=f\"Resampling audio files to {output_sr} Hz\",\n                ),\n                total=len(audio_files),\n            )\n        )\n    for audio_file, (audio, sr) in zip(audio_files, resampled_files):\n        if sr != output_sr:\n            audio_sr = librosa.get_samplerate(audio_file)\n            audio = librosa.resample(audio, sr, output_sr)\n            librosa.output.write_wav(audio_file, audio, output_sr)\n    print(f\"Resampled {len(audio_files)} audio files to {output_sr} Hz\")\n\n", "CodeBleu_score": 41.68218285661157}, {"code": "def resample_files(input_dir, output_sr, output_dir=None, file_ext=\"wav\", n_jobs=10)\n\t\n    if output_dir:\n        copytree(input_dir, output_dir)\n\n    files = glob.glob(os.path.join(input_dir, \"**\", f\"*.{file_ext}\"), recursive=True)\n    print(f\"Resampling {len(files)} files to {output_sr}...\")\n\n    with Pool(n_jobs) as pool:\n        list(tqdm(pool.imap_unordered(resample_file, [(file, output_sr, output_dir) for file in files]), total=len(files)))\n\n    print(\"Done!\")\n\n", "CodeBleu_score": 35.457301668471295}, {"code": "def resample_files(input_dir, output_sr, output_dir=None, file_ext=\"wav\", n_jobs=10)\n\t\n\n    # Check if input directory exists\n    if not os.path.isdir(input_dir):\n        raise NotADirectoryError(f\"Input directory does not exist: {input_dir}\")\n\n    # Check if output directory exists\n    if output_dir is not None:\n        if not os.path.isdir(output_dir):\n            raise NotADirectoryError(f\"Output directory does not exist: {output_dir}\")\n\n    # Check if output sample rate is valid\n    if output_sr <= 0:\n        raise ValueError(f\"Output sample rate must be a positive integer: {output_sr}\")\n\n    # Check if number of jobs is valid\n    if n_jobs <= 0:\n        raise ValueError(f\"Number of jobs must be a positive integer: {n_jobs}\")\n\n    # If output directory is not specified, use the same directory as the input directory\n    if output_dir is None:\n        output_dir = input_dir\n\n    # Copy input directory to output directory\n    copytree(input_dir, output_dir)\n\n    # Find all audio files with the specified extension in the input directory\n    audio_files = glob.glob(f\"{input_dir}/**/*.{file_ext}\", recursive=True)\n\n    # Resample audio files in parallel\n    with Pool(n_jobs) as p:\n        list(\n            tqdm(\n                p.imap_unordered(\n                    resample_file,\n                    [(file, output_sr, output_dir) for file in audio_files],\n                ),\n                total=len(audio_files),\n                desc=\"Resampling\",\n            )\n        )\n\n    # Print completion message\n    print(f\"Resampling complete. Output directory: {output_dir}\")\n\n", "CodeBleu_score": 30.683120300815357}]}
{"instance_id": "a1351834ca57", "generation_result": [{"code": "def evaluation(model, criterion, data_loader, global_step)\n\t\n    model.eval()\n    total_loss = 0\n    total_mel_loss = 0\n    total_gate_loss = 0\n    total_mel_frames = 0\n    total_text_frames = 0\n    total_time = 0\n    total_steps = 0\n    with torch.no_grad():\n        for i, batch in enumerate(data_loader):\n            start_time = time.perf_counter()\n            (text_input, input_lengths, mel_specs, gate_targets,\n             output_lengths) = model.parse_batch(batch)\n            if torch.cuda.is_available():\n                text_input = text_input.cuda()\n                input_lengths = input_lengths.cuda()\n                mel_specs = mel_specs.cuda()\n                gate_targets = gate_targets.cuda()\n                output_lengths = output_lengths.cuda()\n            (mel_outputs, mel_outputs_postnet, gate_outputs,\n             alignments) = model(text_input, input_lengths, mel_specs)\n            mel_loss, gate_loss = criterion(\n                mel_outputs, mel_outputs_postnet, gate_outputs, gate_targets,\n                alignments, output_lengths)\n            loss = mel_loss + gate_loss\n            total_loss += loss.item()\n            total_mel_loss += mel_loss.item()\n            total_gate_loss += gate_loss.item()\n            total_mel_frames += mel_outputs.size(0)\n            total_text_frames += text_input.size(0)\n            total_time += time.perf_counter() - start_time\n            total_steps += 1\n            if i % 100 == 0:\n                print(\"Evaluation step {}/{}\".format(i, len(data_loader)))\n    avg_loss = total_loss / total_steps\n    avg_mel_loss = total_mel_loss / total_steps\n    avg_gate_loss = total_gate_loss / total_steps\n    avg_time = total_time / total_steps\n    print(\"Evaluation loss {} at step {}: mel loss = {}, gate loss = {}, time = {}\"\n          .format(avg_loss, global_step, avg_mel_loss, avg_gate_loss, avg_time))\n    model.train()\n    return avg_loss\n\n", "CodeBleu_score": 27.055051213577975}, {"code": "def evaluation(model, criterion, data_loader, global_step)\n\t\n    # set model to evaluation mode\n    model.eval()\n    device = next(model.parameters()).device\n    # init loss\n    eval_loss = 0.0\n    # init embeddings\n    embeddings = []\n    # init labels\n    labels = []\n    # init data loader\n    for i, batch in enumerate(data_loader):\n        # get data\n        x, y = batch\n        # reshape data\n        x = x.reshape(-1, x.shape[-1])\n        y = y.reshape(-1)\n        # move data to device\n        x = x.to(device)\n        y = y.to(device)\n        # forward pass\n        y_pred = model(x)\n        # compute loss\n        loss = criterion(y_pred, y)\n        # compute embeddings\n        embeddings.append(y_pred.detach().cpu().numpy())\n        # compute labels\n        labels.append(y.detach().cpu().numpy())\n        # accumulate loss\n        eval_loss += loss.item()\n    # compute average loss\n    eval_loss = eval_loss / len(data_loader)\n    # compute average embeddings\n    embeddings = np.concatenate(embeddings, axis=0)\n    # compute average labels\n    labels = np.concatenate(labels, axis=0)\n    # compute UMAP plot\n    if global_step > 0:\n        plot_embeddings(embeddings, labels, global_step, 'eval')\n    # log evaluation statistics\n    print(f'Evaluation loss: {eval_loss:.3f}')\n    # return average loss\n    return eval_loss\n\n", "CodeBleu_score": 30.728203277116762}, {"code": "def evaluation(model, criterion, data_loader, global_step)\n\t\n    # switch to evaluate mode\n    model.eval()\n    # init loss\n    eval_loss = 0.0\n    # init embeddings\n    embeddings = None\n    # init labels\n    labels = None\n    # init sample counter\n    sample_counter = 0\n    # init eval sample counter\n    eval_sample_counter = 0\n    # init eval sample counter\n    eval_sample_counter = 0\n    # init eval sample counter\n    eval_sample_counter = 0\n    # init eval sample counter\n    eval_sample_counter = 0\n    # init eval sample counter\n    eval_sample_counter = 0\n    # init eval sample counter\n    eval_sample_counter = 0\n    # init eval sample counter\n    eval_sample_counter = 0\n    # init eval sample counter\n    eval_sample_counter = 0\n    # init eval sample counter\n    eval_sample_counter = 0\n    # init eval sample counter\n    eval_sample_counter = 0\n    # init eval sample counter\n    eval_sample_counter = 0\n    # init eval sample counter\n    eval_sample_counter = 0\n    # init eval sample counter\n    eval_sample_counter = 0\n    # init eval sample counter\n    eval_sample_counter = 0\n    # init eval sample counter\n    eval_sample_counter = 0\n    # init eval sample counter\n    eval_sample_counter = 0\n    # init eval sample counter\n    eval_sample_counter = 0\n    # init eval sample counter\n    eval_sample_counter = 0\n    # init eval sample counter\n    eval_sample_counter = 0\n    # init eval sample counter\n    eval_sample_counter = 0\n    # init eval sample counter\n    eval_sample_counter = 0\n    # init eval sample counter\n    eval_sample_counter = 0\n    # init eval sample counter\n    eval_sample_counter = 0\n    # init eval sample counter\n    eval_sample_counter = 0\n    # init eval sample counter\n    eval_sample_counter = 0\n    # init eval sample counter\n    eval_sample_counter = 0\n    # init eval sample counter\n    eval_sample_counter = 0\n    # init eval sample counter\n    eval_sample_counter = 0\n    # init eval sample counter\n    eval_sample_counter = 0\n    # init eval sample counter\n    eval_sample_counter = 0\n    # init eval sample counter\n    eval_sample", "CodeBleu_score": 7.7852641988431674}]}
{"instance_id": "565f8ed09916", "generation_result": [{"code": "def register_config(model_name: str) -> Coqpit\n\t\n    config_name = model_name.upper() + \"Config\"\n    if model_name == \"xtts\":\n        config_name = \"XTTSConfig\"\n    try:\n        config_class = find_module(config_name, [os.path.join(os.path.dirname(__file__), \"config\")])\n        return config_class\n    except ModuleNotFoundError:\n        raise ModuleNotFoundError(f\"Configuration class for model {model_name} not found.\")\n\n", "CodeBleu_score": 33.75126081186682}, {"code": "def register_config(model_name: str) -> Coqpit\n\t\n    config_class_name = model_name.replace(\"-\", \"\") + \"Config\"\n    try:\n        config_class = find_module(config_class_name, [os.path.join(os.path.dirname(__file__), \"config\")])\n    except ModuleNotFoundError:\n        if model_name.lower() == \"xtts\":\n            config_class = find_module(config_class_name, [os.path.join(os.path.dirname(__file__), \"config\", \"x-tts\")])\n        else:\n            raise ModuleNotFoundError(f\"Configuration class not found for model: {model_name}\")\n\n    return config_class\n\n", "CodeBleu_score": 35.99104947187161}, {"code": "def register_config(model_name: str) -> Coqpit\n\t\n    model_name = model_name.lower()\n    # construct the configuration class name\n    config_class_name = model_name.upper() + \"Config\"\n    # check if the model is \"xtts\" to import its specific configuration\n    if model_name == \"xtts\":\n        from TTS.tts.utils.generic_utils import get_module_from_path\n        config_class_name = get_module_from_path(config_class_name)\n    # search through predefined paths to find and assign the configuration class\n    for config_path in CONFIG_PATHS:\n        try:\n            config_class = find_module(config_class_name, config_path)\n            break\n        except ModuleNotFoundError:\n            pass\n    else:\n        raise ModuleNotFoundError(f\"Configuration class {config_class_name} not found.\")\n    # return the found configuration class\n    return config_class\n\n", "CodeBleu_score": 41.33772500268316}]}
{"instance_id": "0ea7862506a7", "generation_result": [{"code": "def load_config(config_path: str) -> Coqpit\n\t\n    # check if the configuration file exists\n    if not os.path.isfile(config_path):\n        raise FileNotFoundError(f\"The configuration file {config_path} does not exist.\")\n    # determine the file extension\n    file_extension = os.path.splitext(config_path)[1]\n    # read the content of the file\n    with open(config_path, \"r\") as file:\n        if file_extension in [\".yml\", \".yaml\"]:\n            # load the configuration as YAML\n            config_dict = yaml.load(file, Loader=yaml.FullLoader)\n        elif file_extension in [\".json\"]:\n            # load the configuration as JSON\n            config_dict = json.load(file)\n        else:\n            raise ValueError(f\"Unknown file extension {file_extension}.\")\n    # process the configuration dictionary\n    # determine the model name\n    model_name = config_dict[\"model_name\"]\n    # register the appropriate configuration class\n    config_class = find_module(f\"{model_name}_config\", \"TTS.config\")\n    # create an instance of the configuration class\n    config = config_class()\n    # populate the configuration object with the loaded data\n    config.load_from_dict(config_dict)\n    # return the configuration object\n    return config\n\n", "CodeBleu_score": 46.72856082225919}, {"code": "def load_config(config_path: str) -> Coqpit\n\t\n    config_path = os.path.abspath(config_path)\n    config_name = os.path.basename(config_path)\n    config_name = os.path.splitext(config_name)[0]\n    config_ext = os.path.splitext(config_path)[1]\n    if config_ext == \".yaml\":\n        with open(config_path, \"r\") as f:\n            config_dict = yaml.load(f, Loader=yaml.FullLoader)\n    elif config_ext == \".json\":\n        with open(config_path, \"r\") as f:\n            config_dict = json.load(f)\n    else:\n        raise ValueError(f\"Unknown file extension {config_ext} for configuration file {config_path}\")\n    if \"model_name\" not in config_dict:\n        raise ValueError(f\"No model name found in configuration file {config_path}\")\n    model_name = config_dict[\"model_name\"]\n    config_class = find_module(f\"TTS.config.{model_name.lower()}.{model_name.lower()}_config\")\n    config = config_class()\n    config.load_from_dict(config_dict)\n    return config\n\n", "CodeBleu_score": 42.21301631778908}, {"code": "def load_config(config_path: str) -> Coqpit\n\t\n    # check if the path exists\n    if not os.path.exists(config_path):\n        raise ValueError(f\"The path {config_path} does not exist.\")\n    # check the file extension\n    if config_path.endswith(\".yaml\"):\n        with open(config_path, \"r\") as f:\n            config_dict = yaml.load(f, Loader=yaml.FullLoader)\n    elif config_path.endswith(\".json\"):\n        with open(config_path, \"r\") as f:\n            config_dict = json.load(f)\n    else:\n        raise ValueError(\n            f\"The file extension of the configuration file is not supported. Supported extensions are .yaml and .json\"\n        )\n    # get the model name from the configuration file\n    model_name = config_dict[\"model_name\"]\n    # register the configuration class\n    config_class = find_module(f\"TTS.config.{model_name}_config\")\n    # create the configuration object\n    config = config_class()\n    # populate the configuration object with the data from the configuration file\n    config.populate_from_dict(config_dict)\n    # return the configuration object\n    return config\n\n", "CodeBleu_score": 42.84644060457302}]}
{"instance_id": "e0db68b63143", "generation_result": [{"code": "def list_files(basePath, validExts=None, contains=None)\n\t\n    # loop over the directory structure\n    for (rootDir, dirNames, filenames) in os.walk(basePath):\n        # loop over the filenames in the current directory\n        for filename in filenames:\n            # if the contains string is not none and the filename does not contain\n            # the supplied string, then ignore the file\n            if contains is not None and filename.find(contains) == -1:\n                continue\n\n            # determine the file extension of the current file\n            ext = filename[filename.rfind(\".\"):].lower()\n\n            # check to see if the file is an image and should be processed\n            if validExts is None or ext.endswith(validExts):\n                # construct the path to the image and yield it\n                imagePath = os.path.join(rootDir, filename)\n                yield imagePath\n\n", "CodeBleu_score": 78.4028354587536}, {"code": "def list_files(basePath, validExts=None, contains=None)\n\t\n    # loop over the directory structure\n    for (rootDir, dirNames, filenames) in os.walk(basePath):\n        # loop over the filenames in the current directory\n        for filename in filenames:\n            # if the contains string is not none and the filename does not contain\n            # the supplied string, then ignore the file\n            if contains is not None and filename.find(contains) == -1:\n                continue\n\n            # determine the file extension of the current file\n            ext = filename[filename.rfind(\".\"):].lower()\n\n            # check to see if the file is an image and should be processed\n            if validExts is None or ext.endswith(validExts):\n                # construct the path to the image and yield it\n                imagePath = os.path.join(rootDir, filename).replace(\"\\\\\", \"/\")\n                yield imagePath\n\n", "CodeBleu_score": 73.74255077788857}, {"code": "def list_files(basePath, validExts=None, contains=None)\n\t\n    # loop over the directory structure\n    for (rootDir, dirNames, filenames) in os.walk(basePath):\n        # loop over the filenames in the current directory\n        for filename in filenames:\n            # if the contains string is not none and the filename does not contain\n            # the supplied string, then ignore the file\n            if contains is not None and filename.find(contains) == -1:\n                continue\n\n            # determine the file extension of the current file\n            ext = filename[filename.rfind(\".\"):].lower()\n\n            # check to see if the file is an image and should be processed\n            if validExts is None or ext.endswith(validExts):\n                # construct the path to the image and yield it\n                imagePath = os.path.join(rootDir, filename).replace(\"\\\\\", \"/\")\n                yield imagePath\n\n", "CodeBleu_score": 73.74255077788857}]}
{"instance_id": "b836fcaf0a27", "generation_result": [{"code": "def load_model(xtts_checkpoint, xtts_config, xtts_vocab)\n\t\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n    if xtts_checkpoint is None:\n        print(\"Error: No XTTS checkpoint path provided.\")\n        return False\n    if xtts_config is None:\n        print(\"Error: No XTTS config path provided.\")\n        return False\n    if xtts_vocab is None:\n        print(\"Error: No XTTS vocab path provided.\")\n        return False\n    if not os.path.exists(xtts_checkpoint):\n        print(\"Error: XTTS checkpoint path does not exist.\")\n        return False\n    if not os.path.exists(xtts_config):\n        print(\"Error: XTTS config path does not exist.\")\n        return False\n    if not os.path.exists(xtts_vocab):\n        print(\"Error: XTTS vocab path does not exist.\")\n        return False\n    try:\n        config = XttsConfig.from_json_file(xtts_config)\n        model = Xtts(config)\n        model.load_checkpoint(checkpoint_path=xtts_checkpoint, model_name=\"XTTS\")\n        model.load_vocabulary(vocab_path=xtts_vocab)\n        if torch.cuda.is_available():\n            model.cuda()\n        print(\"XTTS model loaded successfully.\")\n        return model\n    except Exception as e:\n        print(\"Error: Failed to load XTTS model.\")\n        print(e)\n        traceback.print_exc()\n        return False\n\n", "CodeBleu_score": 44.09526396942719}, {"code": "def load_model(xtts_checkpoint, xtts_config, xtts_vocab)\n\t\n    # clear gpu cache\n    torch.cuda.empty_cache()\n    # check if paths are provided\n    if xtts_checkpoint is None or xtts_config is None or xtts_vocab is None:\n        print(\"Please provide paths to the checkpoint, config, and vocab files.\")\n        return\n    # load config\n    config = XttsConfig.load_config(xtts_config)\n    # init model\n    model = Xtts(config)\n    # load checkpoint and vocab\n    model.load_checkpoint(checkpoint_path=xtts_checkpoint, load_model_only=True)\n    model.load_vocabulary(vocab_path=xtts_vocab)\n    # move model to gpu\n    if torch.cuda.is_available():\n        model = model.cuda()\n    # print success message\n    print(\"XTTS model loaded successfully.\")\n    return model\n\n", "CodeBleu_score": 47.57214943493388}, {"code": "def load_model(xtts_checkpoint, xtts_config, xtts_vocab)\n\t\n    # clear cache\n    torch.cuda.empty_cache()\n    # check paths\n    if not os.path.exists(xtts_checkpoint):\n        print(\"The path to the XTTS checkpoint does not exist. Please check the path and try again.\")\n        sys.exit()\n    if not os.path.exists(xtts_config):\n        print(\"The path to the XTTS config does not exist. Please check the path and try again.\")\n        sys.exit()\n    if not os.path.exists(xtts_vocab):\n        print(\"The path to the XTTS vocabulary does not exist. Please check the path and try again.\")\n        sys.exit()\n    # load config\n    config = XttsConfig(xtts_config)\n    # init model\n    model = Xtts(config)\n    # load checkpoint and vocab\n    checkpoint = torch.load(xtts_checkpoint, map_location=torch.device('cpu'))\n    model.load_state_dict(checkpoint['model'])\n    model.eval()\n    # move to GPU\n    if torch.cuda.is_available():\n        model.cuda()\n    # print success\n    print(\"The XTTS model has been loaded successfully.\")\n    return model\n\n", "CodeBleu_score": 44.044297026419}]}
{"instance_id": "c5d63aa0df26", "generation_result": [{"code": "def compute_embedding(self, x, num_frames=250, num_eval=10, return_mean=True, l2_norm=True)\n\t\n    # adjust number of frames based on hop length\n    if self.use_torch_spec:\n        num_frames = (num_frames - 1) * self.hop_length + self.win_length\n    # ensure number of frames does not exceed input length\n    num_frames = min(num_frames, x.shape[1])\n    # calculate offset positions for evaluation\n    offset_eval = np.arange(0, x.shape[1] - num_frames, num_frames // num_eval)\n    # extract frames at these offsets, concatenate them into a batch, and perform inference to obtain embeddings\n    x_eval = torch.cat([x[:, offset:offset + num_frames] for offset in offset_eval], dim=0)\n    with torch.no_grad():\n        emb = self.model(x_eval)\n    # compute mean of the embeddings and return the result, optionally normalizing them with L2 norm\n    emb = emb.mean(dim=0) if return_mean else emb\n    if l2_norm:\n        emb = emb / emb.norm(dim=-1, keepdim=True)\n    return emb, offset_eval\n\n", "CodeBleu_score": 37.9094967537391}, {"code": "def compute_embedding(self, x, num_frames=250, num_eval=10, return_mean=True, l2_norm=True)\n\t\n    if self.use_torch_spec:\n        num_frames = (num_frames - 1) * self.hop_length + self.filter_length\n    num_frames = min(num_frames, x.shape[1])\n    num_eval = min(num_eval, num_frames)\n    eval_offset = np.linspace(0, num_frames - num_eval, num_eval, dtype=int)\n    eval_frames = []\n    for i in eval_offset:\n        eval_frames.append(x[:, i : i + num_eval])\n    eval_frames = torch.stack(eval_frames, dim=0)\n    eval_frames = eval_frames.to(self.device)\n    with torch.no_grad():\n        embeddings = self.model(eval_frames)\n    embeddings = embeddings.cpu().numpy()\n    if return_mean:\n        embeddings = np.mean(embeddings, axis=0, keepdims=True)\n        if l2_norm:\n            embeddings = embeddings / np.linalg.norm(embeddings)\n    return embeddings\n\n", "CodeBleu_score": 45.57323522272516}, {"code": "def compute_embedding(self, x, num_frames=250, num_eval=10, return_mean=True, l2_norm=True)\n\t\n    if x.shape[1] < num_frames:\n        num_frames = x.shape[1]\n    if self.use_cuda:\n        x = x.cuda()\n    # adjust number of frames for hop length\n    num_frames = (num_frames // self.hop_length) * self.hop_length\n    # ensure num frames is not greater than input length\n    if num_frames > x.shape[1]:\n        num_frames = x.shape[1]\n    # compute offset positions for evaluation\n    eval_offset = [i for i in range(num_frames, x.shape[1], num_frames // num_eval)]\n    # extract frames at offsets\n    x_frames = [x[:, i - num_frames:i] for i in eval_offset]\n    # concatenate frames into a batch\n    x_frames = torch.cat(x_frames, dim=0)\n    # perform inference\n    embeddings = self.model(x_frames)\n    if l2_norm:\n        embeddings = nn.functional.normalize(embeddings, p=2, dim=1)\n    if return_mean:\n        embeddings = torch.mean(embeddings, dim=0)\n        embeddings = nn.functional.normalize(embeddings, p=2, dim=0)\n    return embeddings, eval_offset\n\n", "CodeBleu_score": 43.949382711605125}]}
{"instance_id": "06f5b0335b26", "generation_result": [{"code": "def create_layer(self, block, planes, blocks, stride=1)\n\t\n    downsample = None\n    if stride != 1 or self.inplanes != planes * block.expansion:\n        downsample = nn.Sequential(\n            nn.Conv1d(self.inplanes, planes * block.expansion,\n                      kernel_size=1, stride=stride, bias=False),\n            nn.BatchNorm1d(planes * block.expansion),\n        )\n\n    layers = []\n    layers.append(block(self.inplanes, planes, stride, downsample))\n    self.inplanes = planes * block.expansion\n    for i in range(1, blocks):\n        layers.append(block(self.inplanes, planes))\n\n    return nn.Sequential(*layers)\n\n", "CodeBleu_score": 82.29069686612789}, {"code": "def create_layer(self, block, planes, blocks, stride=1)\n\t\n    downsample = None\n    if stride != 1 or self.inplanes != planes * block.expansion:\n        downsample = nn.Sequential(\n            nn.Conv1d(self.inplanes, planes * block.expansion,\n                      kernel_size=1, stride=stride, bias=False),\n            nn.BatchNorm1d(planes * block.expansion),\n        )\n\n    layers = []\n    layers.append(block(self.inplanes, planes, stride, downsample))\n    self.inplanes = planes * block.expansion\n    for i in range(1, blocks):\n        layers.append(block(self.inplanes, planes))\n\n    return nn.Sequential(*layers)\n\n", "CodeBleu_score": 82.29069686612789}, {"code": "def create_layer(self, block, planes, blocks, stride=1)\n\t\n    downsample = None\n    if stride != 1 or self.inplanes != planes * block.expansion:\n        downsample = nn.Sequential(\n            nn.Conv1d(self.inplanes, planes * block.expansion,\n                      kernel_size=1, stride=stride, bias=False),\n            nn.BatchNorm1d(planes * block.expansion),\n        )\n\n    layers = []\n    layers.append(block(self.inplanes, planes, stride, downsample))\n    self.inplanes = planes * block.expansion\n    for i in range(1, blocks):\n        layers.append(block(self.inplanes, planes))\n\n    return nn.Sequential(*layers)\n\n", "CodeBleu_score": 82.29069686612789}]}
{"instance_id": "9ac60597b633", "generation_result": [{"code": "def plot_embeddings(embeddings, num_classes_in_batch)\n\t\n    num_classes = min(num_classes_in_batch, 10)\n    num_utterances_per_class = [0] * num_classes\n    for i in range(num_classes):\n        num_utterances_per_class[i] = len(\n            [j for j in range(len(embeddings)) if embeddings[j][1] == i])\n    print(\"Number of utterances per class:\", num_utterances_per_class)\n\n    if num_classes == 1:\n        reducer = umap.UMAP(random_state=42)\n        embeddings = reducer.fit_transform(embeddings)\n        fig = plt.figure(figsize=(10, 10))\n        plt.scatter(embeddings[:, 0], embeddings[:, 1])\n        plt.title(\"UMAP projection\")\n        plt.axis(\"off\")\n        plt.savefig(\"umap.png\")\n        return fig\n\n    reducer = umap.UMAP(random_state=42)\n    embeddings = reducer.fit_transform(embeddings)\n    fig = plt.figure(figsize=(10, 10))\n    plt.scatter(embeddings[:, 0], embeddings[:, 1], c=embeddings[:, 2])\n    plt.title(\"UMAP projection\")\n    plt.axis(\"off\")\n    plt.savefig(\"umap.png\")\n    return fig\n", "CodeBleu_score": 34.76699795509984}, {"code": "def plot_embeddings(embeddings, num_classes_in_batch)\n\t\n    if num_classes_in_batch > 10:\n        num_classes_in_batch = 10\n    colors = matplotlib.cm.get_cmap(\"tab20\", num_classes_in_batch)\n    fig = plt.figure(figsize=(10, 10))\n    ax = fig.add_subplot(1, 1, 1)\n    scatter = ax.scatter(\n        embeddings[:, 0],\n        embeddings[:, 1],\n        c=[colors(i) for i in range(num_classes_in_batch)],\n        s=0.5,\n        alpha=0.5,\n    )\n    ax.axis(\"off\")\n    ax.set_title(\"UMAP projection\")\n    ax.axis(\"equal\")\n    fig.savefig(\"umap.png\")\n    return fig\n", "CodeBleu_score": 40.49938691696634}, {"code": "def plot_embeddings(embeddings, num_classes_in_batch)\n\t\n\n    # Calculate the number of utterances per class\n    num_utterances_per_class = np.array([len(np.where(embeddings.labels == i)[0]) for i in range(num_classes_in_batch)])\n\n    # Limit the number of classes to 10\n    if num_classes_in_batch > 10:\n        print(\"The number of classes in this batch is {}. It is limited to 10 for visualization purposes.\".format(num_classes_in_batch))\n        num_utterances_per_class = num_utterances_per_class[:10]\n        num_classes_in_batch = 10\n\n    # Calculate the number of utterances in the batch\n    num_utterances_in_batch = sum(num_utterances_per_class)\n\n    # Calculate the number of utterances in each class\n    num_utterances_in_each_class = num_utterances_per_class / num_utterances_in_batch\n\n    # Create a color palette\n    colors = sns.color_palette(\"hls\", num_classes_in_batch)\n\n    # Initialize the plot\n    fig = plt.figure(figsize=(10, 10))\n    ax = fig.add_subplot(1, 1, 1)\n\n    # Transform the embeddings\n    reducer = umap.UMAP()\n    embeddings_transformed = reducer.fit_transform(embeddings.embeddings)\n\n    # Plot the embeddings\n    for i in range(num_classes_in_batch):\n        ax.scatter(embeddings_transformed[np.where(embeddings.labels == i)[0], 0], embeddings_transformed[np.where(embeddings.labels == i)[0], 1], s=num_utterances_in_each_class[i] * 200, c=colors[i], alpha=0.5, label=i)\n\n    # Set the title\n    ax.set_title(\"UMAP projection\", fontsize=24)\n\n    # Adjust the aspect ratio\n    ax.set_aspect('equal', 'datalim')\n\n    # Set the x and y limits\n    ax.set_xlim(embeddings_transformed[:, 0].min() - 0.1, embeddings_transformed[:, 0].max() + 0.1)\n    ax.set_ylim(embeddings_transformed[:, 1].min() - 0.", "CodeBleu_score": 38.71606754466287}]}
{"instance_id": "19c2b4af256d", "generation_result": [{"code": "def calc_cosine_sim(self, dvecs, centroids)\n\t\n    cos_sim = []\n    for dvec in dvecs:\n        new_centroids = []\n        for centroid in centroids:\n            new_centroid = centroid - dvec\n            new_centroid = torch.clamp(new_centroid, min=0.0)\n            new_centroids.append(new_centroid)\n        new_centroids = torch.stack(new_centroids)\n        cos_sim.append(torch.mm(dvec, new_centroids.T))\n    cos_sim = torch.stack(cos_sim)\n    return cos_sim\n\n", "CodeBleu_score": 26.670767771613935}, {"code": "def calc_cosine_sim(self, dvecs, centroids)\n\t\n    # dvecs: (n_speakers, n_utterances, d_vector)\n    # centroids: (n_speakers, d_vector)\n    n_speakers, n_utterances, d_vector = dvecs.shape\n    n_speakers, d_vector = centroids.shape\n    assert d_vector == dvecs.shape[-1] == centroids.shape[-1]\n\n    cosine_sim = []\n    for i in range(n_speakers):\n        # cosine_sim.append([])\n        for j in range(n_utterances):\n            # cosine_sim[i].append([])\n            # cosine_sim[i][j] = torch.clamp(torch.mm(dvecs[i][j].unsqueeze(0), centroids[i].unsqueeze(1).transpose(0, 1)), min=-1.0, max=1.0)\n            cosine_sim.append(torch.clamp(torch.mm(dvecs[i][j].unsqueeze(0), centroids[i].unsqueeze(1).transpose(0, 1)), min=-1.0, max=1.0))\n\n    return torch.stack(cosine_sim)\n\n", "CodeBleu_score": 28.41602665558826}, {"code": "def calc_cosine_sim(self, dvecs, centroids)\n\t\n    sim_mat = []\n    for dvec in dvecs:\n        sim_row = []\n        for centroid in centroids:\n            sim_row.append(torch.clamp(torch.dot(dvec, centroid) / (torch.norm(dvec) * torch.norm(centroid)), min=-1.0, max=1.0))\n        sim_mat.append(torch.stack(sim_row))\n    return torch.stack(sim_mat)\n\n", "CodeBleu_score": 19.9876530021649}]}
{"instance_id": "c525b411a1e7", "generation_result": [{"code": " def embed_loss_softmax(self, dvecs, cos_sim_matrix)\n\t\n    cos_sim_matrix = cos_sim_matrix.float()\n    dvecs = dvecs.float()\n    losses = []\n    for i in range(dvecs.shape[0]):\n        neg_log_softmax = nn.LogSoftmax(dim=0)(-cos_sim_matrix[i])\n        losses.append(neg_log_softmax)\n    losses = torch.stack(losses)\n    return losses\n", "CodeBleu_score": 25.112380350272073}, {"code": " def embed_loss_softmax(self, dvecs, cos_sim_matrix)\n\t\n    loss = torch.tensor([], device=dvecs.device)\n    for i in range(dvecs.shape[0]):\n        loss = torch.cat((loss, -torch.log_softmax(cos_sim_matrix[i], dim=0)))\n    return loss\n\n def embed_loss_sigmoid(self, dvecs, cos_sim_matrix):\n    \"\"\"The function calculates the embedding loss using sigmoid.", "CodeBleu_score": 23.47938124353825}, {"code": " def embed_loss_softmax(self, dvecs, cos_sim_matrix)\n\t\n    batch_size = dvecs.shape[0]\n    losses = []\n    for i in range(batch_size):\n        loss = -torch.log_softmax(cos_sim_matrix[i], dim=0)\n        losses.append(loss)\n    losses = torch.stack(losses)\n    return losses\n", "CodeBleu_score": 26.375800954663198}]}
{"instance_id": "25ac3945da2e", "generation_result": [{"code": "def embed_loss_contrast(self, dvecs, cos_sim_matrix)\n\t\n    # Calculate the sigmoid of the cosine similarity matrix\n    sigmoid_matrix = torch.sigmoid(cos_sim_matrix)\n\n    # Create a tensor of zeros to store the loss for each embedding\n    loss_row = torch.zeros(dvecs.shape[0])\n\n    # Iterate over the embeddings\n    for i in range(dvecs.shape[0]):\n        # Exclude the current centroid\n        sigmoid_matrix_row = torch.cat(\n            (sigmoid_matrix[i, :i], sigmoid_matrix[i, i + 1:]))\n\n        # Calculate the contrast loss\n        loss_row[i] = torch.mean(sigmoid_matrix_row)\n\n    # Stack the loss rows into a tensor\n    loss_tensor = torch.stack(loss_row)\n\n    return loss_tensor\n\n", "CodeBleu_score": 29.51607842228586}, {"code": "def embed_loss_contrast(self, dvecs, cos_sim_matrix)\n\t\n    batch_size = dvecs.shape[0]\n    num_centroids = self.num_centroids\n    loss = torch.zeros((batch_size, num_centroids)).to(self.device)\n    for i in range(batch_size):\n        for j in range(num_centroids):\n            cos_sim_matrix_i = cos_sim_matrix[i, j]\n            cos_sim_matrix_i_row = cos_sim_matrix[i, :]\n            cos_sim_matrix_i_row[j] = -1\n            cos_sim_matrix_i_row = cos_sim_matrix_i_row.reshape((1, -1))\n            cos_sim_matrix_i_row = torch.sigmoid(cos_sim_matrix_i_row)\n            loss[i, j] = torch.sum(cos_sim_matrix_i_row)\n    return loss\n\n", "CodeBleu_score": 28.948133743437367}, {"code": "def embed_loss_contrast(self, dvecs, cos_sim_matrix)\n\t\n    losses = []\n    for i in range(len(dvecs)):\n        cos_sim = cos_sim_matrix[i]\n        cos_sim[i] = 0\n        cos_sim = torch.sigmoid(cos_sim)\n        losses.append(torch.sum(cos_sim))\n    losses = torch.stack(losses)\n    return losses\n\n", "CodeBleu_score": 22.89482046481548}]}
{"instance_id": "878d14ffabd2", "generation_result": [{"code": "def forward(self, x, _label=None)\n\t\n    # input: (num_speakers, num_utts_per_speaker, dvec_feats)\n    # output: loss\n\n    # check input dimensions\n    if len(x.shape) != 3:\n        raise ValueError(\"Input tensor must have 3 dimensions: (num_speakers, num_utts_per_speaker, dvec_feats)\")\n    num_speakers, num_utts_per_speaker, dvec_feats = x.shape\n\n    # check that there are at least two utterances per speaker\n    if num_utts_per_speaker < 2:\n        raise ValueError(\"There must be at least two utterances per speaker\")\n\n    # compute the centroids by averaging the embeddings\n    centroids = x.mean(dim=1)\n\n    # compute a cosine similarity matrix between the embeddings and centroids\n    sim_matrix = torch.matmul(x, centroids.transpose(0, 1))\n\n    # scale and shift the similarity matrix\n    sim_matrix = (self.alpha * 2) * (torch.exp(sim_matrix / self.alpha) - 1)\n\n    # compute the embedding loss\n    if _label is not None:\n        # if labels are provided, use them to compute the loss\n        pos_sim = torch.exp(self.margin - sim_matrix[range(num_speakers), _label])\n        neg_sim = torch.sum(torch.exp(sim_matrix), dim=1) - pos_sim\n        loss = torch.mean(torch.log(1 + neg_sim) - torch.log(1 + pos_sim))\n    else:\n        # if no labels are provided, use the similarity matrix for clustering\n        loss = torch.mean(torch.sum(sim_matrix, dim=1))\n\n    return loss\n\n", "CodeBleu_score": 26.1865084941191}, {"code": "def forward(self, x, _label=None)\n\t\n    # Check if input is valid\n    if x.shape[1] < 2:\n        raise ValueError(\n            \"GE2E loss requires at least two utterances per speaker\"\n        )\n    # Calculate the centroids\n    centroids = torch.mean(x, dim=1)\n    # Calculate the similarity matrix\n    sim_mat = torch.matmul(x, centroids.transpose(0, 1))\n    # Scale and shift the similarity matrix\n    sim_mat = 10.0 * (sim_mat - 1.0)\n    # Calculate the loss\n    if _label is not None:\n        # If labels are provided, calculate the loss\n        loss = torch.mean(\n            torch.sum(\n                -F.log_softmax(sim_mat, dim=1) * _label,\n                dim=1\n            )\n        )\n    else:\n        # If labels are not provided, return the similarity matrix\n        loss = sim_mat\n    return loss\n\n", "CodeBleu_score": 32.82955466937452}, {"code": "def forward(self, x, _label=None)\n\t\n    # Check if the input has at least two utterances per speaker\n    if x.size()[1] < 2:\n        raise ValueError(\n            \"The input tensor must have at least two utterances per speaker.\"\n        )\n\n    # Compute the centroids by averaging the embeddings\n    centroids = torch.mean(x, dim=1)\n\n    # Compute the cosine similarity matrix\n    sim_matrix = torch.matmul(x, centroids.t())\n\n    # Scale and shift the similarity matrix\n    sim_matrix = 10.0 * sim_matrix - 5.0\n\n    # Compute the embedding loss\n    if _label is not None:\n        _label = _label.view(-1)\n        speaker_centroids = torch.index_select(\n            centroids, 0, _label.long()\n        )  # Select the centroids for the speakers\n        sim_matrix = torch.exp(sim_matrix)\n        pos_sim = torch.sum(sim_matrix, dim=1) - 1.0\n        neg_sim = torch.sum(\n            torch.exp(torch.mm(x, speaker_centroids.t())), dim=1\n        )\n        diff = pos_sim - neg_sim + 1.0\n        loss = torch.mean(diff)\n    else:\n        loss = torch.mean(torch.sum(sim_matrix, dim=1) - 1.0)\n\n    return loss\n\n", "CodeBleu_score": 31.026312802456758}]}
{"instance_id": "86a364c72caa", "generation_result": [{"code": "def get_voices(extra_voice_dirs: List[str] = [])\n\t\n    voice_files = {}\n    for voice_dir in extra_voice_dirs:\n        for root, _, files in os.walk(voice_dir):\n            if len(files) == 0:\n                logging.warning(f'No files found in {root}!')\n                continue\n            voice_files[os.path.basename(root)] = []\n            for file in files:\n                if file.endswith('.npz'):\n                    voice_files[os.path.basename(root)].append(os.path.join(root, file))\n                elif file.endswith('.wav') or file.endswith('.mp3'):\n                    voice_files[os.path.basename(root)].append(os.path.join(root, file))\n    return voice_files\n\n", "CodeBleu_score": 28.81476546198115}, {"code": "def get_voices(extra_voice_dirs: List[str] = [])\n\t\n    voice_dirs = [\n        os.path.join('data', 'tts', 'bark', 'hubert', 'voices'),\n        os.path.join('data', 'tts', 'bark', 'hubert', 'custom_voices')\n    ]\n    voice_dirs.extend(extra_voice_dirs)\n    voices = {}\n    for voice_dir in voice_dirs:\n        for dirpath, dirnames, filenames in os.walk(voice_dir):\n            if filenames:\n                if '.npz' in filenames:\n                    voices[dirpath.split('/')[-1]] = [\n                        os.path.join(dirpath, f) for f in filenames if f.endswith('.npz')\n                    ]\n                else:\n                    voices[dirpath.split('/')[-1]] = [\n                        os.path.join(dirpath, f) for f in filenames if f.endswith('.wav') or f.endswith('.mp3')\n                    ]\n    return voices\n\n", "CodeBleu_score": 22.01323513115745}, {"code": "def get_voices(extra_voice_dirs: List[str] = [])\n\t\n    voices = {}\n    for voice_dir in extra_voice_dirs:\n        for root, dirs, files in os.walk(voice_dir):\n            if len(files) > 0:\n                if root.split('/')[-1] not in voices:\n                    voices[root.split('/')[-1]] = []\n                for file in files:\n                    if file.endswith('.npz'):\n                        voices[root.split('/')[-1]].append(os.path.join(root, file))\n                    elif file.endswith('.wav') or file.endswith('.mp3'):\n                        voices[root.split('/')[-1]].append(os.path.join(root, file))\n    return voices\n\n", "CodeBleu_score": 28.360508489606794}]}
{"instance_id": "02f20c8917e6", "generation_result": [{"code": "def load_voice(model, voice: str, extra_voice_dirs: List[str] = [])\n\t\n    # get voice paths\n    voice_paths = get_voices(voice, extra_voice_dirs)\n    # check if there are multiple paths\n    if len(voice_paths) > 1:\n        raise ValueError(f\"There are multiple paths for voice {voice}: {voice_paths}\")\n    # load voice\n    voice_path = voice_paths[0]\n    if voice_path.endswith(\".npz\"):\n        # load npz file\n        voice_data = np.load(voice_path, allow_pickle=True)\n        # load voice\n        voice_data = {key: voice_data[key].item() for key in voice_data}\n        return voice_data\n    else:\n        # load audio\n        audio, sr = librosa.load(voice_path, sr=None)\n        # convert audio\n        audio, _ = convert_audio(audio, sr, model.audio_config)\n        # generate npz file\n        voice_data = generate_npz(audio, sr, model.audio_config)\n        # load voice\n        voice_data = {key: voice_data[key].item() for key in voice_data}\n        return voice_data\n\n", "CodeBleu_score": 41.256580328856586}, {"code": "def load_voice(model, voice: str, extra_voice_dirs: List[str] = [])\n\t\n    if voice == \"random\":\n        return None\n    voice_paths = get_voices(voice, extra_voice_dirs)\n    if len(voice_paths) > 1:\n        raise ValueError(\n            f\"Multiple voices found for {voice} in {extra_voice_dirs}.", "CodeBleu_score": 17.145323764782606}, {"code": "def load_voice(model, voice: str, extra_voice_dirs: List[str] = [])\n\t\n    if voice == \"random\":\n        return None\n    voice_paths = get_voices(voice, extra_voice_dirs)\n    if len(voice_paths) > 1:\n        raise ValueError(f\"Multiple voices found for {voice}.\")\n    voice_path = voice_paths[0]\n    if voice_path.endswith(\".npz\"):\n        voice_data = np.load(voice_path)\n        return voice_data\n    elif os.path.exists(voice_path):\n        voice_data = convert_audio(voice_path, model.sampling_rate)\n        return voice_data\n    else:\n        raise ValueError(f\"Voice {voice} not found.\")\n\n", "CodeBleu_score": 39.114103546827536}]}
{"instance_id": "9458e981389f", "generation_result": [{"code": "def _relative_shift(self, pos_score: torch.Tensor) -> torch.Tensor\n\t\n    batch_size, seq_len, _ = pos_score.size()\n    zero_pad = torch.zeros((batch_size, 1, seq_len), device=pos_score.device)\n    padded_pos_score = torch.cat([zero_pad, pos_score], dim=1)\n    pos_score = padded_pos_score.view(batch_size, seq_len + 1, seq_len)[:, 1:, :]\n    return pos_score\n\n", "CodeBleu_score": 28.38731192622416}, {"code": "def _relative_shift(self, pos_score: torch.Tensor) -> torch.Tensor\n\t\n    batch_size, sequence_length, _ = pos_score.shape\n    zero_pad = torch.zeros((batch_size, sequence_length, 1), device=pos_score.device, dtype=pos_score.dtype)\n    padded_pos_score = torch.cat((zero_pad, pos_score), dim=2)\n    shifted_pos_score = padded_pos_score[:, :, :-1].reshape(batch_size, sequence_length, sequence_length)\n    return shifted_pos_score\n\n", "CodeBleu_score": 28.183615682276386}, {"code": "def _relative_shift(self, pos_score: torch.Tensor) -> torch.Tensor\n\t\n    batch_size, num_heads, seq_length1, seq_length2 = pos_score.size()\n    zeros = torch.zeros((batch_size, num_heads, seq_length1, 1), device=pos_score.device, dtype=pos_score.dtype)\n    padded_pos_score = torch.cat([zeros, pos_score], dim=-1)\n    pos_score = padded_pos_score.view(batch_size, num_heads, seq_length2 + 1, seq_length1)\n    pos_score = pos_score[:, :, 1:].view_as(pos_score)\n    return pos_score\n\n", "CodeBleu_score": 57.793227511653065}]}
{"instance_id": "c046d4f00e80", "generation_result": [{"code": "def run_padded_sequence(self, context, lens)\n\t\n    # context: [B, T, C]\n    # lens: [B]\n    B, T, C = context.size()\n    max_len = lens.max().item()\n    context_out = context.new_zeros(B, max_len, C)\n    for i in range(B):\n        context_out[i, :lens[i]] = context[i, :lens[i]]\n    context_out = self.conv_layers(context_out.transpose(1, 2)).transpose(1, 2)\n    return context_out\n\n", "CodeBleu_score": 21.707078559845264}, {"code": "def run_padded_sequence(self, context, lens)\n\t\n    # Extract the context up to the specified length\n    context = context[:, :lens.max()]\n    # Apply the convolutional layers\n    context = self.conv_layers(context)\n    # Pad the processed contexts to create a consistent batch size\n    context = self.pad_packed_sequence(context, batch_first=True)\n    return context\n\n", "CodeBleu_score": 14.97897139604355}, {"code": "def run_padded_sequence(self, context, lens)\n\t\n    # extract the context up to the specified length\n    batch_size = context.shape[0]\n    max_len = context.shape[1]\n    max_len_mask = torch.arange(max_len, device=lens.device)[None, :] < lens[:, None]\n    max_len_mask = max_len_mask.unsqueeze(1).expand(-1, batch_size, -1)\n    context = context[max_len_mask].view(batch_size, -1, context.shape[2])\n\n    # apply the convolutional layers\n    context = self.conv_layers(context)\n\n    # pad the processed contexts to create a consistent batch size\n    max_len = context.shape[1]\n    max_len_mask = torch.arange(max_len, device=lens.device)[None, :] < lens[:, None]\n    max_len_mask = max_len_mask.unsqueeze(1).expand(-1, batch_size, -1)\n    context = context[max_len_mask].view(batch_size, -1, context.shape[2])\n    return context\n\n", "CodeBleu_score": 18.941679430499427}]}
{"instance_id": "db2644f3c681", "generation_result": [{"code": "def run_unsorted_inputs(self, fn, context, lens)\n\t\n    # sort the inputs by length\n    sorted_lens, sorted_idx = torch.sort(lens, descending=True)\n    sorted_context = context.index_select(0, sorted_idx)\n    # run the function on the sorted inputs\n    sorted_context = fn(sorted_context)\n    # unsort the results to match the original order\n    unsorted_context = sorted_context.index_select(0, sorted_idx.flip(0))\n    return unsorted_context\n\n", "CodeBleu_score": 9.12964445141006}, {"code": "def run_unsorted_inputs(self, fn, context, lens)\n\t\n    if not self.sorted:\n        sorted_lens, sorted_idx = torch.sort(lens, descending=True)\n        sorted_context = context[sorted_idx]\n        sorted_context = fn(sorted_context)\n        sorted_context = sorted_context[torch.argsort(sorted_idx)]\n        return sorted_context\n    return fn(context)\n\n", "CodeBleu_score": 24.045289978167823}, {"code": "def run_unsorted_inputs(self, fn, context, lens)\n\t\n    sorted_lens, sorted_indices = torch.sort(lens, descending=True)\n    sorted_context = context[sorted_indices]\n    sorted_context = pack_padded_sequence(sorted_context, sorted_lens, batch_first=True)\n    sorted_context, _ = fn(sorted_context)\n    sorted_context, _ = pad_packed_sequence(sorted_context, batch_first=True)\n    sorted_context = sorted_context[sorted_indices]\n    return sorted_context\n\n", "CodeBleu_score": 15.448334165907566}]}
{"instance_id": "ced082bffc63", "generation_result": [{"code": "def remove_weight_norm(self)\n\t\n    for block in self.conv_blocks:\n        block.remove_weight_norm()\n    self.convt_pre.remove_weight_norm()\n    self.kernel_predictor.remove_weight_norm()\n\n", "CodeBleu_score": 42.51046275391997}, {"code": "def remove_weight_norm(self)\n\t\n    self.kernel_predictor.remove_weight_norm()\n    parametrize.remove_parametrizations(self, \"weight\")\n    for block in self.conv_blocks:\n        parametrize.remove_parametrizations(block, \"weight\")\n\n", "CodeBleu_score": 51.75943866287916}, {"code": "def remove_weight_norm(self)\n\t\n    self.kernel_predictor.remove_weight_norm()\n    if hasattr(self, \"convt_pre\"):\n        parametrize.remove_parametrizations(self.convt_pre, \"weight\", keep_parametrized=True)\n    for block in self.conv_blocks:\n        parametrize.remove_parametrizations(block, \"weight\", keep_parametrized=True)\n\n", "CodeBleu_score": 42.88279310672617}]}
{"instance_id": "b5a39fd97061", "generation_result": [{"code": "def map_mx_to_torch_model(mx_name)\n\t\n    mx_name = mx_name.lower()\n    if 'se_' in mx_name:\n        mx_name = mx_name.replace('se_', 'senet_')\n    if 'inceptionv3' in mx_name:\n        mx_name = mx_name.replace('inceptionv3', 'inception_v3')\n    return 'gluon_' + mx_name\n\n", "CodeBleu_score": 34.68815924684549}, {"code": "def map_mx_to_torch_model(mx_name)\n\t\n    # Convert to lowercase\n    name = mx_name.lower()\n    # Replace se_ with senet_\n    name = name.replace('se_', 'senet_')\n    # Replace inceptionv3 with inception_v3\n    name = name.replace('inceptionv3', 'inception_v3')\n    # Prefix with gluon_\n    name = 'gluon_' + name\n    return name\n\n", "CodeBleu_score": 28.19679032241249}, {"code": "def map_mx_to_torch_model(mx_name)\n\t\n    mx_name = mx_name.lower()\n    if 'se_' in mx_name:\n        mx_name = mx_name.replace('se_', 'senet_')\n    if 'inceptionv3' in mx_name:\n        mx_name = mx_name.replace('inceptionv3', 'inception_v3')\n    return 'gluon_' + mx_name\n\n", "CodeBleu_score": 34.68815924684549}]}
{"instance_id": "7eee5efdbf67", "generation_result": [{"code": "def _num_samples_per_worker(self)\n\t\n    # Get the initial number of samples per worker\n    num_samples_per_worker = math.ceil(len(self.dataset) / self.num_replicas)\n    # Adjust the number of samples per worker if a batch size is specified\n    if self.batch_size:\n        num_samples_per_worker = math.ceil(\n            num_samples_per_worker / self.batch_size)\n    # If training or multiple replicas are used, round up the number of samples\n    if (self.is_training or self.num_replicas > 1):\n        num_samples_per_worker = math.ceil(num_samples_per_worker)\n    # Return the number of samples as an integer\n    return int(num_samples_per_worker)\n\n", "CodeBleu_score": 38.664312271549726}, {"code": "def _num_samples_per_worker(self)\n\t\n    if self.num_replicas > 1:\n        if self.is_train:\n            # If training is used, we round up the number of samples\n            # to be a multiple of the batch size, so that each\n            # process receives the same number of samples each step.\n            total_samples = self.total_samples\n            num_samples = int(\n                math.ceil((total_samples - self.num_samples_per_replica) /\n                          float(self.num_replicas)))\n            num_samples = max(num_samples, 1)\n            num_samples = (num_samples // self.batch_size) * self.batch_size\n        else:\n            # If training is not used, we just divide the total number\n            # of samples by the number of replicas to determine the\n            # number of samples per worker.\n            num_samples = self.total_samples // self.num_replicas\n    else:\n        # If only a single replica is used, the number of samples per\n        # worker is the total number of samples.\n        num_samples = self.total_samples\n\n    return num_samples\n\n", "CodeBleu_score": 35.57667332999318}, {"code": "def _num_samples_per_worker(self)\n\t\n    if self.num_workers > 0:\n        if self.num_replicas > 1:\n            num_samples = math.ceil(\n                (len(self.dataset) * self.num_replicas) / float(self.num_workers)\n            )\n        else:\n            num_samples = math.ceil(len(self.dataset) / float(self.num_workers))\n    else:\n        num_samples = len(self.dataset)\n    if self.batch_size is not None:\n        num_samples = int(num_samples / self.batch_size) * self.batch_size\n    return num_samples\n\n", "CodeBleu_score": 44.154310134943074}]}
{"instance_id": "a698d65ae190", "generation_result": [{"code": "def find_images_and_targets(\n        folder: str,\n        types: Optional[Union[List, Tuple, Set]] = None,\n        class_to_idx: Optional[Dict] = None,\n        leaf_name_only: bool = True,\n        sort: bool = True\n)\n\t\n    if types is None:\n        types = ('jpg', 'jpeg', 'png')\n    else:\n        types = tuple(types)\n    images = []\n    dir = os.path.expanduser(folder)\n    for root, _, fnames in sorted(os.walk(dir, followlinks=True)):\n        for fname in sorted(fnames):\n            if fname.lower().endswith(types):\n                path = os.path.join(root, fname)\n                if leaf_name_only:\n                    path = os.path.basename(path)\n                item = path\n                if class_to_idx is not None:\n                    cname = os.path.basename(root)\n                    if cname in class_to_idx:\n                        label = class_to_idx[cname]\n                    else:\n                        label = len(class_to_idx)\n                        class_to_idx[cname] = label\n                else:\n                    label = None\n                images.append((item, label))\n    if sort:\n        images = sorted(images, key=lambda x: natural_key(x[0]))\n    return images, class_to_idx\n\n", "CodeBleu_score": 39.76780666735515}, {"code": "def find_images_and_targets(\n        folder: str,\n        types: Optional[Union[List, Tuple, Set]] = None,\n        class_to_idx: Optional[Dict] = None,\n        leaf_name_only: bool = True,\n        sort: bool = True\n)\n\t\n    if types is None:\n        types = ('jpg', 'jpeg', 'png')\n    else:\n        types = tuple(types)\n    images = []\n    labels = []\n    for root, subdirs, files in os.walk(folder, topdown=False):\n        for name in files:\n            if name.lower().endswith(types):\n                path = os.path.join(root, name)\n                images.append(path)\n                leaf = os.path.split(root)[-1]\n                if leaf_name_only:\n                    label = leaf\n                else:\n                    label = os.path.relpath(root, folder)\n                if label not in labels:\n                    labels.append(label)\n    if class_to_idx is None:\n        class_to_idx = {label: idx for idx, label in enumerate(labels)}\n    else:\n        for label in labels:\n            if label not in class_to_idx:\n                raise ValueError(f'Label {label} not found in class-to-index mapping')\n    images_and_labels = []\n    for image, label in zip(images, labels):\n        images_and_labels.append((image, class_to_idx[label]))\n    if sort:\n        images_and_labels.sort(key=lambda x: natural_key(x[0]))\n    return images_and_labels, class_to_idx\n\n", "CodeBleu_score": 44.05916719808483}, {"code": "def find_images_and_targets(\n        folder: str,\n        types: Optional[Union[List, Tuple, Set]] = None,\n        class_to_idx: Optional[Dict] = None,\n        leaf_name_only: bool = True,\n        sort: bool = True\n)\n\t\n    # If no types are provided, default to all file types\n    if types is None:\n        types = ['.jpg', '.jpeg', '.png', '.ppm', '.bmp', '.pgm', '.tif']\n    # If no class-to-index mapping is provided, create one from the labels found in the folder\n    if class_to_idx is None:\n        # Get all subfolders in the given folder\n        classes = [d.name for d in os.scandir(folder) if d.is_dir()]\n        # Sort the classes alphabetically\n        if sort:\n            classes.sort(key=natural_key)\n        # Create the class-to-index mapping\n        class_to_idx = {classes[i]: i for i in range(len(classes))}\n    # Get all files in the given folder\n    files = [os.path.join(folder, f) for f in os.listdir(folder) if os.path.isfile(os.path.join(folder, f))]\n    # Filter files based on their file type\n    files = [f for f in files if os.path.splitext(f)[1].lower() in types]\n    # If leaf_name_only is set to True, strip the folder structure from the file names\n    if leaf_name_only:\n        files = [os.path.split(f)[-1] for f in files]\n    # Get all labels from the file names\n    labels = [os.path.splitext(f)[0].split(os.path.sep)[-1] for f in files]\n    # Get all image-path and class-index pairs\n    images_and_targets = [(f, class_to_idx[l]) for f, l in zip(files, labels)]\n    # Sort the pairs alphabetically by image name\n    if sort:\n        images_and_targets.sort(key=lambda k: natural_key(k[0]))\n    return images_and_targets, class_to_idx\n\n", "CodeBleu_score": 44.32689466774152}]}
{"instance_id": "e7fe43bdecdc", "generation_result": [{"code": "def pytorch_worker_seed()\n\t\n    worker_info = get_worker_info()\n    if worker_info is not None:\n        seed = worker_info.seed % (2 ** 32 - 1)\n    else:\n        seed = int(os.environ.get(\"PL_GLOBAL_SEED\", 0))\n    return seed\n\n", "CodeBleu_score": 49.411818879179435}, {"code": "def pytorch_worker_seed()\n\t\n    worker_info = get_worker_info()\n    if worker_info is not None:\n        return worker_info.seed\n    else:\n        worker_info = torch.utils.data.get_worker_info()\n        if worker_info is not None:\n            return worker_info.id\n        else:\n            return 0\n\n", "CodeBleu_score": 58.88844413674863}, {"code": "def pytorch_worker_seed()\n\t\n    worker_info = get_worker_info()\n    if worker_info is not None:\n        # worker_id is an int in [0, num_workers - 1]\n        worker_id = worker_info.id\n        # num_workers is the number of workers spawned by the dataloader\n        num_workers = worker_info.num_workers\n        # return a seed based on the worker_id and the number of workers\n        return worker_id + num_workers * 100\n    else:\n        # return a seed based on the rank\n        return torch.distributed.get_rank()\n\n", "CodeBleu_score": 49.80476938653339}]}
{"instance_id": "0ffc7e7ded25", "generation_result": [{"code": "def solarize_add(img, add, thresh=128, **__)\n\t\n    if img.mode != 'L' and img.mode != 'RGB':\n        return img\n    lut = []\n    for i in range(256):\n        if i < thresh:\n            lut.append(min(255, i + add))\n        else:\n            lut.append(i)\n    if img.mode == 'RGB':\n        if len(lut) != 768:\n            raise ValueError(\"LUT length must be 768\")\n        return img.point(lut)\n    if img.mode == 'L':\n        if len(lut) != 256:\n            raise ValueError(\"LUT length must be 256\")\n        return img.point(lut, 'L')\n\n", "CodeBleu_score": 61.018095312639595}, {"code": "def solarize_add(img, add, thresh=128, **__)\n\t\n    if img.mode == \"L\" or img.mode == \"RGB\":\n        lut = []\n        for i in range(256):\n            if i < thresh:\n                lut.append(min(255, i + add))\n            else:\n                lut.append(i)\n        if img.mode == \"L\":\n            img = img.point(lut)\n        elif img.mode == \"RGB\":\n            img = img.point(lut * 3)\n        return img\n    else:\n        return img\n\n", "CodeBleu_score": 61.2683888788377}, {"code": "def solarize_add(img, add, thresh=128, **__)\n\t\n    if img.mode == \"L\":\n        lut = []\n        for i in range(256):\n            if i < thresh:\n                lut.append(min(255, i + add))\n            else:\n                lut.append(i)\n        if img.mode == \"I\":\n            lut = array(lut, dtype=uint8)\n        elif img.mode == \"F\":\n            lut = array(lut, dtype=float32)\n        return img.point(lut)\n    elif img.mode == \"RGB\":\n        return ImageOps.solarize(img, thresh, add)\n    else:\n        return img\n\n", "CodeBleu_score": 57.311439305012144}]}
{"instance_id": "5a0d29577a21", "generation_result": [{"code": "def _get_weighted_transforms(transforms: Dict)\n\t\n    transform_names = list(transforms.keys())\n    transform_probs = list(transforms.values())\n    transform_probs = [p / sum(transform_probs) for p in transform_probs]\n    return transform_names, transform_probs\n\n", "CodeBleu_score": 20.698957173702425}, {"code": "def _get_weighted_transforms(transforms: Dict)\n\t\n    # Extract the transform names and probabilities\n    transform_names = list(transforms.keys())\n    transform_probs = [transforms[name] for name in transform_names]\n\n    # Normalize the probabilities so they sum to one\n    transform_probs = [prob / sum(transform_probs) for prob in transform_probs]\n\n    return transform_names, transform_probs\n\n", "CodeBleu_score": 20.445005858465667}, {"code": "def _get_weighted_transforms(transforms: Dict)\n\t\n    transform_names = list(transforms.keys())\n    transform_weights = [transforms[name] for name in transform_names]\n    transform_weights = [weight / sum(transform_weights) for weight in transform_weights]\n    return transform_names, transform_weights\n\n", "CodeBleu_score": 17.741689075869058}]}
{"instance_id": "84bc5658c135", "generation_result": [{"code": "def _apply_basic(self, img, mixing_weights, m)\n\t\n    mixed_image = np.zeros(img.shape, dtype=np.uint8)\n    for i in range(m):\n        # Generate random operations\n        ops = np.random.choice(\n            [\n                \"identity\",\n                \"autocontrast\",\n                \"equalize\",\n                \"invert\",\n                \"rotate\",\n                \"posterize\",\n                \"solarize\",\n                \"color\",\n                \"contrast\",\n                \"brightness\",\n                \"sharpness\",\n                \"shear_x\",\n                \"shear_y\",\n                \"translate_x\",\n                \"translate_y\",\n            ],\n            p=mixing_weights,\n        )\n\n        # Apply the operations\n        if ops == \"identity\":\n            pass\n        elif ops == \"autocontrast\":\n            img = ImageOps.autocontrast(img)\n        elif ops == \"equalize\":\n            img = ImageOps.equalize(img)\n        elif ops == \"invert\":\n            img = ImageOps.invert(img)\n        elif ops == \"rotate\":\n            img = img.rotate(np.random.uniform(-30, 30))\n        elif ops == \"posterize\":\n            img = ImageOps.posterize(img, np.random.randint(4, 8))\n        elif ops == \"solarize\":\n            img = ImageOps.solarize(img, np.random.randint(256))\n        elif ops == \"color\":\n            img = ImageEnhance.Color(img).enhance(\n                np.random.uniform(0.1, 2.0)\n            )\n        elif ops == \"contrast\":\n            img = ImageEnhance.Contrast(img).enhance(\n                np.random.uniform(0.1, 2.0)\n            )\n        elif ops == \"brightness\":\n            img = ImageEnhance.Brightness(img).enhance(\n                np.random.uniform(0.1, 2.0)\n            )\n        elif ops == \"sharpness\":\n            img = ImageEnhance.Sharpness(img).enhance(\n                np.random.uniform(0.1, 2.0)\n            )\n        elif ops == \"shear_x\":\n            img = img.transform(\n                img.size,\n                Image.AFFINE,\n                (\n                    1,\n                    np.random.uniform(-0.3, 0.3),\n                    0,\n                    0,\n                    1,\n                    0,\n                ),\n                resample=Image", "CodeBleu_score": 26.84568600863418}, {"code": "def _apply_basic(self, img, mixing_weights, m)\n\t\n    # Initialize an empty array for the mixed image\n    mixed_image = np.zeros(img.shape, dtype=np.float32)\n    # Iterate through given mixing weights\n    for i in range(len(mixing_weights)):\n        # Generate random operations\n        operations = random.choices(\n            m, k=random.randint(1, len(m))\n        )  # Generate a list of random operations\n        # Apply the operations to the image\n        for op in operations:\n            img = op(img)\n        # Accumulate the results\n        mixed_image += img * mixing_weights[i]\n    # Clip the results to valid pixel values\n    mixed_image = np.clip(mixed_image, 0, 255)\n    # Convert the mixed image back to an image\n    mixed_image = Image.fromarray(mixed_image.astype(np.uint8))\n    # Blend the original image with the mixed image\n    blended = ImageChops.blend(img, mixed_image, alpha=1.0 - m.factor)\n    return blended\n\n", "CodeBleu_score": 36.69910084247133}, {"code": "def _apply_basic(self, img, mixing_weights, m)\n\t\n    # create an empty array for the mixed image\n    mixed_image = np.zeros_like(img)\n    for i in range(m):\n        # randomly choose a sequence of operations to apply to the image\n        seq = random.choice(self.seq_list)\n        # apply the sequence of operations to the image\n        img = seq(img)\n        # add the image to the mixed image array\n        mixed_image += img * mixing_weights[i]\n    # clip the pixel values of the mixed image\n    mixed_image = np.clip(mixed_image, 0, 255)\n    # convert the mixed image array back to an image\n    mixed_image = Image.fromarray(mixed_image.astype(np.uint8))\n    # blend the original image with the mixed image\n    blended = ImageChops.blend(img, mixed_image, self.alpha)\n    return blended\n\n", "CodeBleu_score": 27.66325100892263}]}
{"instance_id": "84b5b752ff04", "generation_result": [{"code": "def adapt_to_chs(x, n)\n\t\n    if isinstance(x, (tuple, list)):\n        if len(x) == n:\n            return tuple(x)\n        else:\n            assert len(x) == 1\n            logging.warning(\n                f\"The channel dimension of the input is not equal to {n}. The input is adapted to match the required number of channels.\"\n            )\n            return tuple(repeat(x[0], n))\n    else:\n        return tuple(repeat(x, n))\n\n", "CodeBleu_score": 29.764224194913286}, {"code": "def adapt_to_chs(x, n)\n\t\n    if not isinstance(x, (tuple, list)):\n        x = repeat(x, n)\n    else:\n        if len(x) == n:\n            assert len(x) == n, \"If x is a tuple or list, its length must match n.\"\n        else:\n            logging.warning(f\"The input tuple or list has length {len(x)}, but n is {n}. The input will be repeated to match n.\")\n            x = [torch.mean(torch.stack(x, dim=0), dim=0) for _ in range(n)]\n    return x\n\n", "CodeBleu_score": 44.83910187445988}, {"code": "def adapt_to_chs(x, n)\n\t\n    if isinstance(x, (tuple, list)):\n        if len(x) == n:\n            assert len(x) == n\n        elif len(x) == 1:\n            x = list(repeat(x[0], n))\n        else:\n            logging.warning(\n                f\"The input {x} has length {len(x)}, which is not equal to {n}. The input is averaged and then repeated to match the required length {n}.\"\n            )\n            x = [sum(x) / len(x)] * n\n    else:\n        x = list(repeat(x, n))\n    return x\n\n", "CodeBleu_score": 40.3542608321883}]}
{"instance_id": "54754b6ba394", "generation_result": [{"code": "def _worker_init(worker_id, worker_seeding='all')\n\t\n    worker_info = torch.utils.data.get_worker_info()\n    if worker_info is not None:\n        worker_id = worker_info.id\n        if worker_seeding == 'all':\n            seed = worker_info.seed % (2**32 - 1)\n            random.seed(seed)\n            torch.manual_seed(seed)\n            if torch.cuda.is_available():\n                torch.cuda.manual_seed_all(seed)\n            # numpy.random.seed(seed)\n        elif callable(worker_seeding):\n            worker_seeding(worker_info)\n        elif isinstance(worker_seeding, str):\n            np.random.seed(worker_info.seed % (2**32 - 1))\n        else:\n            raise ValueError('The worker_seeding parameter must be a callable or a string.')\n\n", "CodeBleu_score": 51.49030800953178}, {"code": "def _worker_init(worker_id, worker_seeding='all')\n\t\n    worker_info = torch.utils.data.get_worker_info()\n    if worker_info is not None:\n        worker_id = worker_info.id\n        if worker_seeding is not None:\n            if callable(worker_seeding):\n                worker_seeding(worker_info.seed)\n            elif worker_seeding == 'all':\n                random.seed(worker_info.seed)\n                torch.manual_seed(worker_info.seed)\n                torch.cuda.manual_seed_all(worker_info.seed)\n                torch.backends.cudnn.deterministic = True\n                torch.backends.cudnn.benchmark = False\n                np.random.seed(worker_info.seed)\n            else:\n                raise ValueError(\"worker_seeding must be callable or 'all'\")\n\n", "CodeBleu_score": 34.49762456963398}, {"code": "def _worker_init(worker_id, worker_seeding='all')\n\t\n    worker_info = torch.utils.data.get_worker_info()\n    if worker_info is None:\n        return\n\n    worker_id = worker_info.id\n    if worker_id == 0:\n        print(f'Using {worker_info.num_workers} workers for data loading')\n\n    worker_seed = worker_info.seed % (2 ** 32 - 1)\n    random.seed(worker_seed)\n    if worker_seeding is not None:\n        if callable(worker_seeding):\n            worker_seeding(worker_seed)\n        elif worker_seeding == 'all':\n            torch.manual_seed(worker_seed)\n            torch.cuda.manual_seed_all(worker_seed)\n            torch.backends.cudnn.deterministic = True\n            torch.backends.cudnn.benchmark = False\n            np.random.seed(worker_seed)\n\n", "CodeBleu_score": 50.3876754126419}]}
{"instance_id": "81dfdce940db", "generation_result": [{"code": "def _decode_and_center_crop(image_bytes, image_size, resize_method)\n\t\n    shape = tf.image.extract_jpeg_shape(image_bytes)\n    image_height = shape[0]\n    image_width = shape[1]\n\n    padded_center_crop_size = tf.cast(\n        ((image_size / (image_size + CROP_PADDING)) *\n        tf.cast(tf.minimum(image_height, image_width), tf.float32)),\n        tf.int32)\n\n    offset_height = ((image_height - padded_center_crop_size) + 1) // 2\n    offset_width = ((image_width - padded_center_crop_size) + 1) // 2\n    crop_window = tf.stack([offset_height, offset_width, padded_center_crop_size, padded_center_crop_size])\n    image = tf.image.decode_and_crop_jpeg(image_bytes, crop_window, channels=3)\n    image = tf.image.resize([image], [image_size, image_size], method=resize_method)[0]\n    return image\n", "CodeBleu_score": 62.62075327384204}, {"code": "def _decode_and_center_crop(image_bytes, image_size, resize_method)\n\t\n    # 1. Extract image shape and calculate padded center crop size\n    shape = tf.image.extract_jpeg_shape(image_bytes)\n    image_height = shape[0]\n    image_width = shape[1]\n    padded_center_crop_size = tf.cast(\n        ((image_size / (image_size + CROP_PADDING)) *\n         tf.cast(tf.minimum(image_height, image_width), tf.float32)),\n        tf.int32)\n\n    # 2. Determine the offset for the center crop\n    offset_height = ((image_height - padded_center_crop_size) + 1) // 2\n    offset_width = ((image_width - padded_center_crop_size) + 1) // 2\n\n    # 3. Crop the center of the image\n    image = tf.image.crop_to_bounding_box(\n        image_bytes, offset_height, offset_width, padded_center_crop_size,\n        padded_center_crop_size)\n\n    # 4. Resize the image with the provided resize method\n    image = _resize_image(image, image_size, resize_method)\n\n    return image\n\n", "CodeBleu_score": 50.53489075892732}, {"code": "def _decode_and_center_crop(image_bytes, image_size, resize_method)\n\t\n    # 1. Extract image shape\n    shape = tf.image.extract_jpeg_shape(image_bytes)\n    # 2. Calculate padded center crop size\n    padded_center_crop_size = tf.cast(\n        ((shape[0] / shape[1]) * (image_size / 2)), tf.int32)\n\n    offset_height = ((shape[0] - padded_center_crop_size) + 1) // 2\n    offset_width = ((shape[1] - padded_center_crop_size) + 1) // 2\n    crop_window = tf.stack([offset_height, offset_width,\n                            padded_center_crop_size, padded_center_crop_size])\n    # 3. Crop the image\n    image = tf.image.decode_and_crop_jpeg(image_bytes, crop_window, channels=3)\n    # 4. Resize the image\n    image = tf.image.resize([image], [image_size, image_size],\n                            method=resize_method)[0]\n    return image\n\n", "CodeBleu_score": 40.92030545305846}]}
{"instance_id": "cf16a6d37111", "generation_result": [{"code": "def preprocess_for_train(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    image = tf.image.decode_image(image_bytes, channels=3)\n    image = tf.image.resize(image, [image_size, image_size], method=interpolation)\n    image = tf.image.random_flip_left_right(image)\n    image = tf.reshape(image, [image_size, image_size, 3])\n    if use_bfloat16:\n        image = tf.cast(image, dtype=tf.bfloat16)\n    else:\n        image = tf.cast(image, dtype=tf.float32)\n    return image\n\n", "CodeBleu_score": 40.28857676266979}, {"code": "def preprocess_for_train(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    # Decode the jpeg image\n    image = tf.io.decode_jpeg(image_bytes, channels=3)\n    # Resize the image to the specified size\n    image = tf.image.resize(image, [image_size, image_size], method=interpolation)\n    # Randomly crop the image\n    image = tf.image.random_crop(image, [image_size, image_size, 3])\n    # Randomly flip the image\n    image = tf.image.random_flip_left_right(image)\n    # Rescale to [-1,1]\n    image = (image / 127.5) - 1\n    # Convert to the specified dtype\n    if use_bfloat16:\n        image = tf.cast(image, dtype=tf.bfloat16)\n    else:\n        image = tf.cast(image, dtype=tf.float32)\n    return image\n\n", "CodeBleu_score": 37.59833858469213}, {"code": "def preprocess_for_train(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    # Decode the image\n    image = tf.image.decode_image(image_bytes, channels=3)\n    # Crop the image to the desired size\n    image = tf.image.random_crop(image, size=[image_size, image_size, 3])\n    # Flip the image\n    image = tf.image.random_flip_left_right(image)\n    # Reshape the image to the target size\n    image = tf.reshape(image, [image_size, image_size, 3])\n    # Convert the image to the desired dtype\n    image = tf.image.convert_image_dtype(image, dtype=tf.bfloat16 if use_bfloat16 else tf.float32)\n    return image\n\n", "CodeBleu_score": 44.571544968700465}]}
{"instance_id": "78b2a3bd2142", "generation_result": [{"code": "def preprocess_for_eval(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    image = tf.io.decode_jpeg(image_bytes, channels=3)\n    image = tf.image.convert_image_dtype(image, tf.float32)\n    image = tf.image.central_crop(image, central_fraction=CENTRAL_FRACTION)\n    image = tf.expand_dims(image, 0)\n    image = tf.image.resize(image, [image_size, image_size], method=interpolation)\n    image = tf.squeeze(image, [0])\n    if use_bfloat16:\n        image = tf.cast(image, tf.bfloat16)\n    return image\n\n", "CodeBleu_score": 32.56973270789553}, {"code": "def preprocess_for_eval(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    if use_bfloat16:\n        image = tf.image.decode_and_crop_jpeg(image_bytes, crop_window=_CROP_PADDING)\n        image = tf.image.convert_image_dtype(image, dtype=tf.bfloat16)\n    else:\n        image = tf.image.decode_and_crop_jpeg(image_bytes, crop_window=_CROP_PADDING)\n        image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n    image = tf.image.resize(image, [image_size, image_size], method=interpolation)\n    return image\n\n", "CodeBleu_score": 42.121708215153085}, {"code": "def preprocess_for_eval(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    image = tf.io.decode_image(image_bytes, channels=3)\n    image = tf.image.central_crop(image, central_fraction=CROP_FRACTION)\n    image = tf.image.resize(image, [image_size, image_size], method=interpolation)\n    image = tf.image.convert_image_dtype(image, tf.bfloat16 if use_bfloat16 else tf.float32)\n    return image\n\n", "CodeBleu_score": 31.73489849065033}]}
{"instance_id": "8bd4364a9a0e", "generation_result": [{"code": "def evaluate(self, model, eval_dataloader=None, output_file=\"merged\")\n\t\n    if eval_dataloader is None:\n        eval_dataloader = self.eval_dataloader\n    else:\n        self.eval_dataloader = eval_dataloader\n\n    if output_file == \"merged\":\n        output_file = self.output_dir + \"/merged.csv\"\n\n    predictions, labels, _ = self.prediction_loop(model, eval_dataloader, description=\"Evaluation\")\n    results = self.compute_metrics(eval_dataloader, predictions, labels)\n\n    if self.is_world_process_zero():\n        self.log(results)\n\n    return results\n\n", "CodeBleu_score": 50.19763032133076}, {"code": "def evaluate(self, model, eval_dataloader=None, output_file=\"merged\")\n\t\n    if eval_dataloader is None:\n        eval_dataloader = self.val_dataloader()\n    os.makedirs(self.args.output_dir, exist_ok=True)\n    output_eval_file = os.path.join(self.args.output_dir, f\"eval_results_{output_file}.txt\")\n    # if os.path.exists(output_eval_file):\n    #     os.remove(output_eval_file)\n    # with open(output_eval_file, \"w\") as writer:\n    #     print(f\"***** Running evaluation {output_file} *****\")\n    #     print(f\"  Num examples = {len(eval_dataset)}\")\n    #     print(f\"  Batch size = {args.eval_batch_size}\")\n    # eval_loss = 0.0\n    # nb_eval_steps = 0\n    # preds = None\n    # out_label_ids = None\n    # for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n    #     model.eval()\n    #     batch = tuple(t.to(args.device) for t in batch)\n    #     with torch.no_grad():\n    #         inputs = {\"input_ids\": batch[0], \"attention_mask\": batch[1], \"labels\": batch[3]}\n    #         if args.model_type != \"distilbert\":\n    #             inputs[\"token_type_ids\"] = (\n    #                 batch[2] if args.model_type in [\"bert\", \"xlnet\"] else None\n    #             )  # XLM, DistilBERT, RoBERTa, and XLM-RoBERTa don't use segment_ids\n    #         outputs = model(**inputs)\n    #         tmp_eval_loss, logits = outputs[:2]\n    #         eval_loss += tmp_eval_loss.mean().item()\n    #     nb_eval_steps += 1\n    #     if preds is None:\n    #         preds = logits.detach().cpu().numpy()\n    #     else:\n    #         preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n    #     if out_label_ids is None:\n    #         out_label_ids = inputs[\"labels\"].detach().cpu().numpy()\n    #     else:\n    #         out", "CodeBleu_score": 47.45095404518668}, {"code": "def evaluate(self, model, eval_dataloader=None, output_file=\"merged\")\n\t\n    if eval_dataloader is None:\n        eval_dataloader = self.test_dataloader\n    else:\n        self.test_dataloader = eval_dataloader\n    self.model = model\n    self.compute_metrics = self.compute_metrics_fn\n    self.eval_dataset = eval_dataloader.dataset\n    self.eval_examples = self.eval_dataset\n    self.eval_features = self.convert_examples_to_features(\n        self.eval_examples, self.tokenizer, self.max_seq_length, self.doc_stride, self.max_query_length\n    )\n    os.makedirs(self.args.output_dir, exist_ok=True)\n    self.eval_output_dir = os.path.join(self.args.output_dir, \"eval\")\n    os.makedirs(self.eval_output_dir, exist_ok=True)\n    self.eval_batch_size = self.args.eval_batch_size\n    self.eval_sampler = SequentialSampler(self.eval_dataset)\n    self.eval_dataloader = DataLoader(\n        self.eval_dataset, sampler=self.eval_sampler, batch_size=self.eval_batch_size\n    )\n    self.evaluate()\n    result = self.eval_results\n    self.eval_results = {}\n    return result\n\n", "CodeBleu_score": 38.82783436918272}]}
{"instance_id": "22d682c958c0", "generation_result": [{"code": " def print_computed_metrics(self, metrics)\n\t\n    rank1 = metrics['rank1']\n    rank5 = metrics['rank5']\n    mIoU = metrics['mIoU']\n    print(\"Rank-1: {0:.4f} \\t Rank-5: {1:.4f} \\t mIoU: {2:.4f}\".format(rank1, rank5, mIoU))\n\n def print_metrics_table(self, metrics_list):\n    \"\"\"The function prints the computed metrics for rank1, rank5, and miou.", "CodeBleu_score": 24.3676862416132}, {"code": " def print_computed_metrics(self, metrics)\n\t\n    print(\"Rank1: {:.4f}\".format(metrics[\"rank1\"]))\n    print(\"Rank5: {:.4f}\".format(metrics[\"rank5\"]))\n    print(\"mIoU: {:.4f}\".format(metrics[\"miou\"]))\n\n def get_metrics(self, predictions, labels):\n    \"\"\"The function computes the rank1, rank5, and miou metrics for the provided predictions and labels.\n    It returns the computed metrics in a dictionary.\"\"\"\n    rank1 = metrics.rank1(predictions, labels)\n    rank5 = metrics.rank5(predictions, labels)\n    miou = metrics.miou(predictions, labels)\n    return {\"rank1\": rank1, \"rank5\": rank5, \"miou\": miou}\n\n def print_metrics(self, metrics):\n    \"\"\"The function prints the provided metrics dictionary to the console.\n    It retrieves these values from the provided metrics dictionary and prints them formatted to four decimal places.\"\"\"\n    print(\"Rank1: {:.4f}\".format(metrics[\"rank1\"]))\n    print(\"Rank5: {:.4f}\".format(metrics[\"rank5\"]))\n    print(\"mIoU: {:.4f}\".format(metrics[\"miou\"]))\n\n def save_metrics(self, metrics, path):\n    \"\"\"The function saves the provided metrics dictionary to the provided path.\n    It retrieves these values from the provided metrics dictionary and saves them in the provided path.\"\"\"\n    with open(path, \"w\") as f:\n        json.dump(metrics, f)\n\n def load_metrics(self, path):\n    \"\"\"The function loads the metrics dictionary from the provided path.\n    It loads the metrics dictionary from the provided path and returns it.\"\"\"\n    with open(path, \"r\") as f:\n        return json.load(f)\n\n def get_metrics_from_log(self, log_path):\n    \"\"\"The function retrieves the metrics from the provided log path.\n    It loads the metrics dictionary from the provided log path and returns it.\"\"\"\n    with open(log_path, \"r\") as f:\n        for line in f:\n            if \"mIoU\" in line:\n                return json.loads(line.split(\" \")[-1])", "CodeBleu_score": 28.913282248485668}, {"code": " def print_computed_metrics(self, metrics)\n\t\n    rank1 = metrics['rank1']\n    rank5 = metrics['rank5']\n    miou = metrics['miou']\n    print('Rank@1: {0:.4f}, Rank@5: {1:.4f}, mIoU: {2:.4f}'.format(rank1, rank5, miou))\n", "CodeBleu_score": 27.58935091746976}]}
{"instance_id": "39c1a50b7e7d", "generation_result": [{"code": "def _eval_predictions(self, segments, data)\n\t\n    ious = []\n    ranks = []\n    for segment, data_instance in zip(segments, data):\n        iou, rank = self.eval_segment(segment, data_instance)\n        ious.append(iou)\n        ranks.append(rank)\n    ious = np.array(ious)\n    ranks = np.array(ranks)\n    return np.mean(ious[ranks == 1]), np.mean(ious[ranks <= 5]), np.mean(ious)\n\n", "CodeBleu_score": 21.203254760882455}, {"code": "def _eval_predictions(self, segments, data)\n\t\n    # Get the IoU and rank for the top predictions for each segment-data pair\n    ious = []\n    ranks = []\n    for segment, data in zip(segments, data):\n        iou, rank = self._eval_predictions_for_segment(segment, data)\n        ious.append(iou)\n        ranks.append(rank)\n    # Calculate the mean IoU and rank for the top predictions for all segments\n    mean_iou = np.mean(ious)\n    mean_rank = np.mean(ranks)\n    # Calculate the percentage of segments with rank 1 and rank 5\n    rank1 = np.mean([1 if r == 1 else 0 for r in ranks])\n    rank5 = np.mean([1 if r <= 5 else 0 for r in ranks])\n    return rank1, rank5, mean_iou\n\n", "CodeBleu_score": 26.266242304162944}, {"code": "def _eval_predictions(self, segments, data)\n\t\n    # TODO: add support for other metrics\n    ious = []\n    ranks = []\n    for segment, data in zip(segments, data):\n        # TODO: add support for other metrics\n        iou = segment.iou(data)\n        rank = segment.rank(data)\n        ious.append(iou)\n        ranks.append(rank)\n    rank1 = np.mean([1 if rank == 1 else 0 for rank in ranks])\n    rank5 = np.mean([1 if rank <= 5 else 0 for rank in ranks])\n    mean_iou = np.mean(ious)\n    return rank1, rank5, mean_iou\n\n", "CodeBleu_score": 23.317440485119544}]}
{"instance_id": "8ccf84f8d034", "generation_result": [{"code": "def predict_loop(self, model, eval_dataloader, output_file=None)\n\t\n    model.eval()\n    model.to(0)\n    scores = []\n    for batch in tqdm(eval_dataloader):\n        batch = {key: value.to(0) for key, value in batch.items()}\n        with torch.no_grad():\n            output = model(**batch)\n        scores.extend(output[\"scores\"].tolist())\n    scores = [math.exp(score) for score in scores]\n    if output_file is not None:\n        with open(output_file, \"w\") as f:\n            json.dump(scores, f)\n    return scores\n\n", "CodeBleu_score": 36.81643136600089}, {"code": "def predict_loop(self, model, eval_dataloader, output_file=None)\n\t\n    model.eval()\n    model.to(self.device)\n    scores = []\n    for step, batch in enumerate(tqdm(eval_dataloader)):\n        batch = tuple(t.to(self.device) for t in batch)\n        with torch.no_grad():\n            inputs = self.prepare_inputs(batch)\n            outputs = model(**inputs)\n        scores.extend(self.finalize_outputs(batch, outputs))\n    if output_file is not None:\n        with open(output_file, 'w') as f:\n            json.dump(scores, f)\n    return scores\n\n", "CodeBleu_score": 35.7307455112093}, {"code": "def predict_loop(self, model, eval_dataloader, output_file=None)\n\t\n\n    model.eval()\n    model.cuda()\n\n    scores = []\n    for step, batch in enumerate(tqdm(eval_dataloader)):\n        input_ids = batch['input_ids'].cuda()\n        attention_mask = batch['attention_mask'].cuda()\n        token_type_ids = batch['token_type_ids'].cuda()\n        labels = batch['labels'].cuda()\n\n        with torch.no_grad():\n            outputs = model(input_ids, attention_mask, token_type_ids, labels)\n            scores.append(outputs.cpu().numpy())\n\n    scores = np.concatenate(scores)\n\n    if output_file is not None:\n        with open(output_file, 'w') as f:\n            for score in scores:\n                f.write(json.dumps(score.tolist()) + '", "CodeBleu_score": 33.09898670780518}]}
{"instance_id": "e68529ad727d", "generation_result": [{"code": "def _aggregate_scores(self, scores)\n\t\n    assert len(scores) == 2, \"scores must have length 2\"\n    text_scores, video_scores = scores\n    text_scores = text_scores.view(text_scores.shape[0], -1)\n    video_scores = video_scores.view(video_scores.shape[0], -1)\n    scores = torch.cat([text_scores, video_scores], dim=1)\n    self.full_scores = None\n    return torch.mm(text_scores, video_scores.t())\n\n", "CodeBleu_score": 31.600991144200396}, {"code": "def _aggregate_scores(self, scores)\n\t\n    assert len(scores) == 2, \"There should be exactly two sets of scores.\"\n    video_scores, text_scores = scores[0], scores[1]\n    self.full_scores = torch.cat((video_scores, text_scores), 0)\n    return torch.dot(text_scores.mean(0), video_scores.mean(0))\n\n", "CodeBleu_score": 34.627289686710874}, {"code": "def _aggregate_scores(self, scores)\n\t\n    assert len(scores) == 2, \"There should be two sets of scores.\"\n    self.full_scores = torch.cat(scores, dim=0)\n    self.full_scores = self.full_scores.cpu().numpy()\n    self.full_scores = self.full_scores.tolist()\n    return self.full_scores\n\n", "CodeBleu_score": 29.379730551536998}]}
{"instance_id": "e4e1861fc357", "generation_result": [{"code": "def finalize(self, Y_pred, Y_true, output_file=None)\n\t\n    Y_pred = torch.cat(Y_pred, dim=0).cpu().numpy()\n    Y_true = torch.cat(Y_true, dim=0).cpu().numpy()\n    assert Y_pred.shape == Y_true.shape, \"Predictions and true labels have different shapes.\"\n    print(\"Prediction errors:\")\n    for i in range(len(Y_pred)):\n        if Y_pred[i] != Y_true[i]:\n            print(f\"Predicted: {Y_pred[i]}, True: {Y_true[i]}\")\n    if output_file is not None:\n        with open(output_file, \"wb\") as f:\n            pickle.dump({\"Y_pred\": Y_pred, \"Y_true\": Y_true}, f)\n    return {\"Y_pred\": Y_pred, \"Y_true\": Y_true}\n\n", "CodeBleu_score": 40.92831871485463}, {"code": "def finalize(self, Y_pred, Y_true, output_file=None)\n\t\n\n    # Concatenate predictions and true labels\n    Y_pred = torch.cat(Y_pred, dim=0).numpy()\n    Y_true = torch.cat(Y_true, dim=0).numpy()\n\n    # Check for prediction errors\n    if Y_pred.shape[0] != Y_true.shape[0]:\n        print(\"Error: Number of predicted and true labels differ.\")\n        return\n\n    # Print samples of prediction errors\n    for i in range(Y_pred.shape[0]):\n        if Y_pred[i] != Y_true[i]:\n            print(f\"Error: Prediction for sample {i} is wrong. Predicted: {Y_pred[i]}, True: {Y_true[i]}\")\n\n    # Save predictions and true labels\n    if output_file is not None:\n        with open(output_file, \"wb\") as f:\n            pickle.dump({\"Y_pred\": Y_pred, \"Y_true\": Y_true}, f)\n\n    return {\"Y_pred\": Y_pred, \"Y_true\": Y_true}\n\n", "CodeBleu_score": 42.45618052608993}, {"code": "def finalize(self, Y_pred, Y_true, output_file=None)\n\t\n    Y_pred = torch.cat(Y_pred).cpu().numpy()\n    Y_true = torch.cat(Y_true).cpu().numpy()\n\n    # Check for errors\n    errors = []\n    for i in range(len(Y_pred)):\n        if Y_pred[i] != Y_true[i]:\n            errors.append(i)\n\n    # Print errors\n    if len(errors) > 0:\n        print(\"Found\", len(errors), \"errors:\")\n        for i in errors:\n            print(\"Predicted:\", Y_pred[i], \"True:\", Y_true[i])\n            print(\"Text:\", self.data[\"text\"][i])\n\n    # Save predictions and true labels\n    if output_file is not None:\n        with open(output_file, \"wb\") as f:\n            pickle.dump({\"Y_pred\": Y_pred, \"Y_true\": Y_true}, f)\n\n    return {\"Y_pred\": Y_pred, \"Y_true\": Y_true}\n\n", "CodeBleu_score": 41.87591161551474}]}
{"instance_id": "7df9ab9a4920", "generation_result": [{"code": "def reduce_metrics(logging_outputs) -> None\n\t\n    loss_sum = sum(log.get(\"loss\", 0) for log in logging_outputs)\n    sample_size = sum(log.get(\"sample_size\", 0) for log in logging_outputs)\n\n    metrics.log_scalar(\n        \"loss\", loss_sum / sample_size / math.log(2), sample_size, round=3\n    )\n\n", "CodeBleu_score": 55.969294100204195}, {"code": "def reduce_metrics(logging_outputs) -> None\n\t\n    loss_sum = sum(log.get('loss', 0) for log in logging_outputs)\n    sample_size = sum(log.get('sample_size', 0) for log in logging_outputs)\n\n    metrics.log_scalar('loss', loss_sum / sample_size, sample_size, round=3)\n\n", "CodeBleu_score": 52.7910982560264}, {"code": "def reduce_metrics(logging_outputs) -> None\n\t\n    loss_sum = sum(log.get(\"loss\", 0) for log in logging_outputs)\n    sample_size = sum(log.get(\"sample_size\", 0) for log in logging_outputs)\n\n    metrics.log_scalar(\n        \"loss\", loss_sum / sample_size / math.log(2), sample_size, round=3\n    )\n\n", "CodeBleu_score": 55.969294100204195}]}
{"instance_id": "7f9c326a96f7", "generation_result": [{"code": "def _mm_on_the_fly(\n        self,\n        cmasks,\n        vmasks,\n        attention_mask\n    )\n\t\n    if attention_mask is None:\n        attention_mask = torch.cat([cmask, vmask], dim=1)\n    attention_mask = attention_mask.to(dtype=torch.float)\n    attention_mask = (1.0 - attention_mask) * -10000.0\n    token_type_ids = torch.cat([torch.zeros_like(cmask), torch.ones_like(vmask)], dim=1)\n    return attention_mask, token_type_ids\n\n", "CodeBleu_score": 34.73632803599072}, {"code": "def _mm_on_the_fly(\n        self,\n        cmasks,\n        vmasks,\n        attention_mask\n    )\n\t\n    if attention_mask is None:\n        attention_mask = (cmasks.sum(dim=-1) + vmasks.sum(dim=-1)).gt(0)\n    token_type_ids = torch.cat([cmasks.new_zeros(cmasks.shape[:-1] + (1,)), vmasks.new_ones(vmasks.shape[:-1] + (1,))], dim=-1)\n    return attention_mask, token_type_ids\n\n", "CodeBleu_score": 34.7899797006832}, {"code": "def _mm_on_the_fly(\n        self,\n        cmasks,\n        vmasks,\n        attention_mask\n    )\n\t\n\n    if attention_mask is None:\n        attention_mask = (cmasks.sum(dim=1) + vmasks.sum(dim=1)).gt(0)\n\n    token_type_ids = torch.cat((\n        torch.zeros(cmasks.shape[0], cmasks.shape[1], dtype=torch.long),\n        torch.ones(vmasks.shape[0], vmasks.shape[1], dtype=torch.long)\n    ), dim=1)\n\n    return attention_mask, token_type_ids\n\n", "CodeBleu_score": 42.0127092133719}]}
{"instance_id": "fa99d2a148a5", "generation_result": [{"code": "def prepare_inputs_for_generation(\n        self,\n        input_ids,\n        input_video_embeds,\n        attention_mask=None,\n        token_type_ids=None,\n        **model_kwargs\n    )\n\t\n    # adjust sequence length to match the combined length of input_ids and input_video_embeds\n    if input_ids is not None and input_video_embeds is not None:\n        combined_input_ids = torch.cat([input_ids, input_video_embeds], dim=-1)\n        attention_mask = torch.cat([attention_mask, attention_mask], dim=-1) if attention_mask is not None else None\n        token_type_ids = torch.cat([token_type_ids, token_type_ids], dim=-1) if token_type_ids is not None else None\n        model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(input_ids, model_kwargs)\n        input_ids, attention_mask, token_type_ids = self._pad_to_max_len(combined_input_ids, attention_mask, token_type_ids, self.config.max_length)\n        input_ids = input_ids[:, :input_ids.size(-1) - input_video_embeds.size(-1)]\n        attention_mask = attention_mask[:, :attention_mask.size(-1) - input_video_embeds.size(-1)]\n        token_type_ids = token_type_ids[:, :token_type_ids.size(-1) - input_video_embeds.size(-1)]\n    elif input_ids is None and input_video_embeds is not None:\n        input_ids = input_video_embeds\n        model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(input_ids, model_kwargs)\n        attention_mask = torch.cat([attention_mask, attention_mask], dim=-1) if attention_mask is not None else None\n        token_type_ids = torch.cat([token_type_ids, token_type_ids], dim=-1) if token_type_ids is not None else None\n        input_ids, attention_mask, token_type_ids = self._pad_to_max_len(input_ids, attention_mask, token_type_ids, self.config.max_length)\n    elif input_ids is not None and input_video_embeds is None:\n        input_video_embeds = input_ids\n        model_kwargs = self", "CodeBleu_score": 35.15588487149485}, {"code": "def prepare_inputs_for_generation(\n        self,\n        input_ids,\n        input_video_embeds,\n        attention_mask=None,\n        token_type_ids=None,\n        **model_kwargs\n    )\n\t\n    if input_video_embeds is None:\n        input_video_embeds = torch.zeros(input_ids.shape[0], 1, 1, 1, 1, device=input_ids.device)\n    if attention_mask is None:\n        attention_mask = torch.ones(input_ids.shape, device=input_ids.device)\n    if token_type_ids is None:\n        token_type_ids = torch.zeros(input_ids.shape, dtype=torch.long, device=input_ids.device)\n\n    input_ids, input_video_embeds, attention_mask, token_type_ids = self._pad_to_max_length(\n        input_ids,\n        input_video_embeds,\n        attention_mask,\n        token_type_ids,\n        max_length=self.config.max_length,\n        pad_to_multiple_of=self.config.pad_to_multiple_of,\n        return_attention_mask=True,\n        return_tensors=None,\n    )\n\n    return {\n        \"input_ids\": input_ids,\n        \"input_video_embeds\": input_video_embeds,\n        \"attention_mask\": attention_mask,\n        \"token_type_ids\": token_type_ids,\n    }\n\n", "CodeBleu_score": 42.096468843494485}, {"code": "def prepare_inputs_for_generation(\n        self,\n        input_ids,\n        input_video_embeds,\n        attention_mask=None,\n        token_type_ids=None,\n        **model_kwargs\n    )\n\t\n    if attention_mask is None:\n        attention_mask = torch.ones_like(input_ids)\n    if token_type_ids is None:\n        token_type_ids = torch.zeros_like(input_ids)\n\n    # get the sequence length\n    seq_length = input_ids.shape[-1]\n\n    # get the combined sequence length\n    combined_length = seq_length + input_video_embeds.shape[-2]\n\n    # adjust the sequence length\n    if combined_length > self.config.max_position_embeddings:\n        raise ValueError(\n            f\"The combined sequence length is {combined_length}, but it cannot be greater than the maximum sequence length of the model ({self.config.max_position_embeddings}).\"\n        )\n    elif combined_length < self.config.max_position_embeddings:\n        pad_length = self.config.max_position_embeddings - combined_length\n        input_ids = F.pad(input_ids, (0, pad_length), value=self.config.pad_token_id)\n        attention_mask = F.pad(attention_mask, (0, pad_length), value=0)\n        token_type_ids = F.pad(token_type_ids, (0, pad_length), value=0)\n\n    # update the input_ids, input_video_embeds, attention_mask, and token_type_ids\n    inputs = {\n        \"input_ids\": input_ids,\n        \"input_video_embeds\": input_video_embeds,\n        \"attention_mask\": attention_mask,\n        \"token_type_ids\": token_type_ids,\n    }\n\n    return inputs\n\n", "CodeBleu_score": 37.15545977074686}]}
{"instance_id": "b1b0d25fe620", "generation_result": [{"code": "def _init_pipeline(self, cfg: ConfigType) -> Compose\n\t\n    # TODO: Add support for other pipelines\n    # TODO: Add support for other pipelines\n    if cfg.get('pipeline', None) is None:\n        raise ValueError(f\"The configuration file {cfg} does not have a \"\n                         \"'pipeline' key.\")\n    pipeline = cfg['pipeline']\n    if 'LoadImageFromFile' not in [\n            transform['type'] for transform in pipeline\n    ]:\n        raise ValueError(f\"The configuration file {cfg} does not have a \"\n                         \"'LoadImageFromFile' transform in the pipeline.\")\n    load_image_from_file_index = [\n        index for index, transform in enumerate(pipeline)\n        if transform['type'] == 'LoadImageFromFile'\n    ][0]\n    pipeline[load_image_from_file_index]['type'] = 'mmdet.InferencerLoader'\n    if 'meta_keys' in pipeline[load_image_from_file_index]:\n        if 'img_id' in pipeline[load_image_from_file_index]['meta_keys']:\n            pipeline[load_image_from_file_index]['meta_keys'].remove(\n                'img_id')\n    return Compose(pipeline)\n\n", "CodeBleu_score": 31.29283320648828}, {"code": "def _init_pipeline(self, cfg: ConfigType) -> Compose\n\t\n    pipeline = cfg.copy()\n    if 'img_id' in pipeline.keys():\n        pipeline.pop('img_id')\n    pipeline = Compose(pipeline)\n    for idx, transform in enumerate(pipeline.transforms):\n        if isinstance(transform, LoadImageFromFile):\n            transform.type = 'mmdet.InferencerLoader'\n            pipeline.transforms[idx] = transform\n            break\n    else:\n        raise ValueError('The pipeline does not contain a LoadImageFromFile '\n                         'transform, which is required for the Inferencer.')\n    return pipeline\n\n", "CodeBleu_score": 27.387270048823968}, {"code": "def _init_pipeline(self, cfg: ConfigType) -> Compose\n\t\n    # adjust pipeline config\n    if 'img_id' in cfg['data']['test']['pipeline'][0]['meta_keys']:\n        cfg['data']['test']['pipeline'][0]['meta_keys'].remove('img_id')\n    # find the index of the 'LoadImageFromFile' transform\n    for i, transform in enumerate(cfg['data']['test']['pipeline']):\n        if transform['type'] == 'LoadImageFromFile':\n            load_image_from_file_index = i\n            break\n    else:\n        raise RuntimeError('The test pipeline must include a transform of '\n                           'type \"LoadImageFromFile\".')\n    # update transform type\n    cfg['data']['test']['pipeline'][load_image_from_file_index][\n        'type'] = 'mmdet.InferencerLoader'\n    # return the pipeline\n    return Compose(cfg['data']['test']['pipeline'])\n\n", "CodeBleu_score": 27.144225090304268}]}
{"instance_id": "7967962856eb", "generation_result": [{"code": "def _inputs_to_list(self, inputs: InputsType) -> list\n\t\n    if isinstance(inputs, str):\n        inputs = list_dir_or_file(inputs)\n    if not isinstance(inputs, (list, tuple)):\n        inputs = [inputs]\n    return inputs\n\n", "CodeBleu_score": 32.234335486847}, {"code": "def _inputs_to_list(self, inputs: InputsType) -> list\n\t\n    if isinstance(inputs, (str, Path)):\n        if isdir(inputs):\n            inputs = list_dir_or_file(inputs)\n        else:\n            raise FileNotFoundError(f\"The input is not a directory: {inputs}\")\n    elif isinstance(inputs, (tuple, list)):\n        pass\n    else:\n        inputs = [inputs]\n    return inputs\n\n", "CodeBleu_score": 33.05544463822853}, {"code": "def _inputs_to_list(self, inputs: InputsType) -> list\n\t\n    if isinstance(inputs, (str, Path)):\n        inputs = list_dir_or_file(inputs)\n    elif not isinstance(inputs, (list, tuple)):\n        inputs = [inputs]\n    return inputs\n\n", "CodeBleu_score": 27.59974701643205}]}
{"instance_id": "74f727f06fa0", "generation_result": [{"code": "def _get_chunk_data(self, inputs: Iterable, chunk_size: int)\n\t\n    chunk = []\n    for input in inputs:\n        if isinstance(input, dict) and 'img' in input:\n            input = input['img']\n        elif isinstance(input, dict) and 'img_path' in input:\n            input = input['img_path']\n        chunk.append(input)\n        if len(chunk) == chunk_size:\n            yield self.pipeline(chunk)\n            chunk = []\n    if len(chunk) > 0:\n        yield self.pipeline(chunk)\n\n", "CodeBleu_score": 26.91332190947189}, {"code": "def _get_chunk_data(self, inputs: Iterable, chunk_size: int)\n\t\n    if chunk_size is None:\n        yield from map(self.pipeline, inputs)\n    else:\n        for i in range(0, len(inputs), chunk_size):\n            yield from map(self.pipeline, inputs[i:i + chunk_size])\n\n", "CodeBleu_score": 10.70081287828468}, {"code": "def _get_chunk_data(self, inputs: Iterable, chunk_size: int)\n\t\n    if chunk_size <= 0:\n        raise ValueError(\"Chunk size must be a positive integer.\")\n\n    for i, input in enumerate(inputs):\n        if i % chunk_size == 0:\n            if i > 0:\n                yield chunk_data\n            chunk_data = []\n\n        if isinstance(input, dict):\n            if \"img\" in input:\n                chunk_data.append(self.pipeline(input[\"img\"], input[\"img_path\"]))\n            elif \"img_path\" in input:\n                chunk_data.append(self.pipeline(input[\"img_path\"]))\n            else:\n                raise ValueError(\n                    \"The input dictionary must contain an 'img' or 'img_path' key.\"\n                )\n        else:\n            chunk_data.append(self.pipeline(input))\n\n        if i == len(inputs) - 1:\n            yield chunk_data\n\n", "CodeBleu_score": 37.858006527107655}]}
{"instance_id": "a43ab51afa59", "generation_result": [{"code": "def postprocess(\n        self,\n        preds: PredType,\n        visualization: Optional[List[np.ndarray]] = None,\n        return_datasamples: bool = False,\n        print_result: bool = False,\n        no_save_pred: bool = False,\n        pred_out_dir: str = '',\n        **kwargs,\n    ) -> Dict\n\t\n    # Initialize output directory if necessary\n    if not no_save_pred and pred_out_dir:\n        mmcv.mkdir_or_exist(pred_out_dir)\n\n    # Initialize result dictionary\n    results = {}\n\n    # Convert predictions to dictionaries and append them to results\n    if return_datasamples:\n        if pred_out_dir:\n            warnings.warn(\n                'Saving datasamples is not supported for now. '\n                'Predictions will be saved to the specified directory.')\n        results['datasamples'] = [\n            self.convert_to_datasample(pred) for pred in preds\n        ]\n    else:\n        results['preds'] = [self.convert_to_dict(pred) for pred in preds]\n\n    # Append visualizations to results\n    if visualization is not None:\n        results['visualization'] = visualization\n\n    # Print results\n    if print_result:\n        self.print_results(results)\n\n    return results\n\n", "CodeBleu_score": 45.19758965295508}, {"code": "def postprocess(\n        self,\n        preds: PredType,\n        visualization: Optional[List[np.ndarray]] = None,\n        return_datasamples: bool = False,\n        print_result: bool = False,\n        no_save_pred: bool = False,\n        pred_out_dir: str = '',\n        **kwargs,\n    ) -> Dict\n\t\n    if self.pred_out_dir and not no_save_pred:\n        warnings.warn(\n            'Saving predictions is not supported. '\n            'Predictions will not be saved.'\n        )\n    if return_datasamples and pred_out_dir:\n        warnings.warn(\n            'Saving datasamples is not supported. '\n            'Predictions will not be saved.'\n        )\n    if return_datasamples:\n        warnings.warn(\n            'Returning datasamples is not supported. '\n            'Predictions will not be saved.'\n        )\n    if self.pred_out_dir:\n        mmcv.mkdir_or_exist(self.pred_out_dir)\n    results = dict()\n    if return_datasamples:\n        results['datasamples'] = []\n    if visualization is not None:\n        results['visualization'] = visualization\n    if preds is not None:\n        if isinstance(preds, tuple):\n            preds = list(preds)\n        elif not isinstance(preds, list):\n            preds = [preds]\n        if return_datasamples:\n            for pred in preds:\n                results['datasamples'].append(self.convert_pred_to_datasample(pred))\n        else:\n            for pred in preds:\n                results.update(self.convert_pred_to_dict(pred))\n    if print_result:\n        self.print_result(results)\n    return results\n\n", "CodeBleu_score": 41.74766736888236}, {"code": "def postprocess(\n        self,\n        preds: PredType,\n        visualization: Optional[List[np.ndarray]] = None,\n        return_datasamples: bool = False,\n        print_result: bool = False,\n        no_save_pred: bool = False,\n        pred_out_dir: str = '',\n        **kwargs,\n    ) -> Dict\n\t\n    # Initialize output directory\n    if pred_out_dir and not no_save_pred:\n        mmcv.mkdir_or_exist(pred_out_dir)\n\n    # Initialize results\n    results = dict()\n    if return_datasamples:\n        results['datasamples'] = []\n\n    # Convert predictions to dictionaries\n    if not return_datasamples:\n        results['preds'] = []\n\n    # Print results\n    if print_result:\n        self.print_results(preds)\n\n    # Save predictions\n    if pred_out_dir and not no_save_pred:\n        self.save_preds(preds, pred_out_dir)\n\n    # Save visualization\n    if visualization is not None:\n        self.save_visualization(visualization, pred_out_dir)\n\n    # Convert predictions to dictionaries and append them to results\n    if not return_datasamples:\n        for pred in preds:\n            results['preds'].append(self.convert_pred_to_dict(pred))\n\n    # Add predictions and visualization to results\n    if visualization is not None:\n        results['visualization'] = visualization\n\n    return results\n\n", "CodeBleu_score": 48.71299838045219}]}
{"instance_id": "a3c2561f199c", "generation_result": [{"code": "def build_test_pipeline(cfg: ConfigType) -> ConfigType\n\t  # noqa: E501\n    test_pipeline = copy.deepcopy(cfg.data.test.pipeline)\n    test_pipeline[0] = copy.deepcopy(cfg.data.test.pipeline[0])\n    test_pipeline[0].type = 'Resize'\n    test_pipeline[-1] = copy.deepcopy(cfg.data.test.pipeline[-1])\n    test_pipeline[-1].type = 'Compose'\n    test_pipeline[-1].transforms = [test_pipeline[0]] + \\\n        test_pipeline[-1].transforms\n    return test_pipeline\n\n", "CodeBleu_score": 28.92784204399973}, {"code": "def build_test_pipeline(cfg: ConfigType) -> ConfigType\n\t  # noqa: E501\n    cfg = copy.deepcopy(cfg)\n    resize_cfg = cfg[0].copy()\n    resize_cfg['type'] = 'Resize'\n    cfg.insert(0, resize_cfg)\n    cfg.append(cfg[-1])\n    cfg.pop(-2)\n    return Compose(cfg)\n\n", "CodeBleu_score": 14.517820207577367}, {"code": "def build_test_pipeline(cfg: ConfigType) -> ConfigType\n\t\n    # copy first transform in dataset pipeline\n    pipeline = copy.deepcopy(cfg.data.test.pipeline[0])\n    # modify first transform to include only Resize\n    pipeline.transforms = [\n        transform for transform in pipeline.transforms\n        if transform.type == 'Resize'\n    ]\n    # copy last transform in dataset pipeline\n    last_transform = copy.deepcopy(cfg.data.test.pipeline[-1])\n    # combine first and last transforms into test pipeline\n    test_pipeline = Compose(pipeline.transforms + last_transform.transforms)\n    return test_pipeline\n\n", "CodeBleu_score": 33.64525594969589}]}
{"instance_id": "48a358ab5a38", "generation_result": [{"code": "def inference_mot(model: nn.Module, img: np.ndarray, frame_id: int,\n                  video_len: int) -> SampleList\n\t\n    cfg = model.cfg\n    device = next(model.parameters()).device\n    # prepare data\n    data = dict(img=img, frame_id=frame_id, video_len=video_len)\n    # build the data pipeline\n    test_pipeline = cfg.data.test.pipeline\n    test_pipeline = Compose(test_pipeline)\n    data = test_pipeline(data)\n    data = collate([data], samples_per_gpu=1)\n    if next(model.parameters()).is_cuda:\n        # scatter to specified GPU\n        data = scatter(data, [device])[0]\n    else:\n        for m in model.modules():\n            assert not isinstance(\n                m, RoIPool\n            ), 'CPU inference with RoIPool is not supported currently.'\n    # forward the model\n    with torch.no_grad():\n        result = model(return_loss=False, rescale=True, **data)\n    return result\n\n", "CodeBleu_score": 52.899938175431316}, {"code": "def inference_mot(model: nn.Module, img: np.ndarray, frame_id: int,\n                  video_len: int) -> SampleList\n\t\n    # prepare data\n    data = dict(img=img, frame_id=frame_id, img_shape=img.shape,\n                ori_shape=img.shape, img_id=frame_id, video_len=video_len)\n    # build the data pipeline\n    cfg = model.cfg\n    device = next(model.parameters()).device  # model device\n    if cfg.data.test.pipeline[0].type == 'MultiScaleFlipAug':\n        # TODO: support batch inference\n        data = data['img'][None]\n        cfg = cfg.copy()\n        # set loading pipeline type\n        cfg.data.test.pipeline[0].type = cfg.data.test.pipeline[0].__class__\n        # set loading pipeline for each img\n        for pipeline in cfg.data.test.pipeline[0].transforms:\n            if pipeline.type == 'LoadImageFromFile':\n                pipeline.type = pipeline.__class__\n        test_pipeline = Compose(cfg.data.test.pipeline)\n    else:\n        test_pipeline = Compose(cfg.data.test.pipeline)\n    data = test_pipeline(data)\n    data = collate([data], samples_per_gpu=1)\n    if next(model.parameters()).is_cuda:\n        # scatter to specified GPU\n        data = scatter(data, [device])[0]\n    else:\n        for m in model.modules():\n            assert not isinstance(\n                m, RoIPool\n            ), 'CPU inference with RoIPool is not supported currently.'\n    # forward the model\n    with torch.no_grad():\n        result = model(return_loss=False, rescale=True, **data)\n    return result\n\n", "CodeBleu_score": 57.33171334080578}, {"code": "def inference_mot(model: nn.Module, img: np.ndarray, frame_id: int,\n                  video_len: int) -> SampleList\n\t\n    # prepare input data\n    data = dict(\n        img=img,\n        frame_id=frame_id,\n        original_shape=img.shape,\n        img_id=frame_id,\n        video_len=video_len\n    )\n    # process data\n    if model.device.type == 'cpu':\n        # ensure no unsupported modules are used\n        model = revert_sync_batchnorm(model)\n        model = model.eval()\n        with torch.no_grad():\n            result = model(return_loss=False, rescale=True, **data)\n    else:\n        result = model(return_loss=False, rescale=True, **data)\n    # return result\n    return result\n\n", "CodeBleu_score": 32.4549173338482}]}
{"instance_id": "6c4ea2ce1787", "generation_result": [{"code": "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\n\t\n    while True:\n        total_length = len(tokens_a) + len(tokens_b)\n        if total_length <= max_num_tokens:\n            break\n\n        trunc_tokens = tokens_a if len(tokens_a) > len(tokens_b) else tokens_b\n        assert len(trunc_tokens) >= 1\n\n        # We want to sometimes truncate from the front and sometimes from the\n        # back to add more randomness and avoid biases.\n        if rng.random() < 0.5:\n            del trunc_tokens[0]\n        else:\n            trunc_tokens.pop()\n\n", "CodeBleu_score": 78.49275840258514}, {"code": "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\n\t\n    while True:\n        total_length = len(tokens_a) + len(tokens_b)\n        if total_length <= max_num_tokens:\n            break\n        if len(tokens_a) > len(tokens_b):\n            trunc_tokens = tokens_a\n            assert len(trunc_tokens) >= 1\n            # We want to sometimes truncate from the front and sometimes from the\n            # back to add more randomness and avoid biases.\n            if rng.random() < 0.5:\n                del trunc_tokens[0]\n            else:\n                trunc_tokens.pop()\n        else:\n            trunc_tokens = tokens_b\n            assert len(trunc_tokens) >= 1\n            # We want to sometimes truncate from the front and sometimes from the\n            # back to add more randomness and avoid biases.\n            if rng.random() < 0.5:\n                del trunc_tokens[0]\n            else:\n                trunc_tokens.pop()\n    return tokens_a, tokens_b\n\n", "CodeBleu_score": 61.15039146723701}, {"code": "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\n\t\n    while True:\n        total_length = len(tokens_a) + len(tokens_b)\n        if total_length <= max_num_tokens:\n            break\n\n        trunc_tokens = tokens_a if len(tokens_a) > len(tokens_b) else tokens_b\n        assert len(trunc_tokens) >= 1\n\n        # We want to sometimes truncate from the front and sometimes from the\n        # back to add more randomness and avoid biases.\n        if rng.random() < 0.5:\n            del trunc_tokens[0]\n        else:\n            trunc_tokens.pop()\n\n", "CodeBleu_score": 78.49275840258514}]}
{"instance_id": "007181b6729d", "generation_result": [{"code": "def get_activation(activation_string)\n\t\n    if isinstance(activation_string, six.string_types):\n        if not activation_string:\n            return None\n        act = activation_string.lower()\n        if act == 'linear':\n            return None\n        elif act == 'relu':\n            return tf.nn.relu\n        elif act == 'sigmoid':\n            return tf.nn.sigmoid\n        elif act == 'tanh':\n            return tf.nn.tanh\n        elif act == 'elu':\n            return tf.nn.elu\n        elif act == 'selu':\n            return tf.nn.selu\n        elif act == 'softplus':\n            return tf.nn.softplus\n        elif act == 'softsign':\n            return tf.nn.softsign\n        elif act == 'swish':\n            return lambda x: x * tf.sigmoid(x)\n        else:\n            raise ValueError('Unsupported activation: %s' % act)\n    else:\n        return activation_string\n\n", "CodeBleu_score": 64.43226195692937}, {"code": "def get_activation(activation_string)\n\t\n    if not isinstance(activation_string, six.string_types):\n        return activation_string\n    if not activation_string:\n        return None\n    act = activation_string.lower()\n    if act == \"linear\":\n        return None\n    elif act == \"relu\":\n        return tf.nn.relu\n    elif act == \"elu\":\n        return tf.nn.elu\n    elif act == \"selu\":\n        return tf.nn.selu\n    elif act == \"softplus\":\n        return tf.nn.softplus\n    elif act == \"softsign\":\n        return tf.nn.softsign\n    elif act == \"tanh\":\n        return tf.nn.tanh\n    elif act == \"sigmoid\":\n        return tf.nn.sigmoid\n    else:\n        raise ValueError(\"Unsupported activation: %s\" % act)\n\n", "CodeBleu_score": 79.88853111232605}, {"code": "def get_activation(activation_string)\n\t\n    if isinstance(activation_string, six.string_types):\n        if not activation_string:\n            return None\n        act = activation_string.lower()\n        if act == \"linear\":\n            return linear\n        elif act == \"relu\":\n            return relu\n        elif act == \"sigmoid\":\n            return sigmoid\n        elif act == \"tanh\":\n            return tanh\n        elif act == \"elu\":\n            return elu\n        elif act == \"selu\":\n            return selu\n        elif act == \"softplus\":\n            return softplus\n        elif act == \"softsign\":\n            return softsign\n        elif act == \"hard_sigmoid\":\n            return hard_sigmoid\n        elif act == \"exponential\":\n            return exponential\n        elif act == \"linear_hard_sigmoid\":\n            return linear_hard_sigmoid\n        else:\n            raise ValueError('Unsupported activation: %s' % act)\n    return activation_string\n\n", "CodeBleu_score": 65.07051144460092}]}
{"instance_id": "d5d6e0315899", "generation_result": [{"code": "def dropout(input_tensor, dropout_prob)\n\t\n    if dropout_prob is not None and dropout_prob != 0.0:\n        return tf.nn.dropout(input_tensor, keep_prob=1.0 - dropout_prob)\n    return input_tensor\n\n", "CodeBleu_score": 43.216351555431984}, {"code": "def dropout(input_tensor, dropout_prob)\n\t\n    if dropout_prob is None or dropout_prob == 0.0:\n        return input_tensor\n    else:\n        return tf.nn.dropout(input_tensor, rate=dropout_prob)\n\n", "CodeBleu_score": 53.03103381915149}, {"code": "def dropout(input_tensor, dropout_prob)\n\t\n    if dropout_prob is not None and dropout_prob > 0.0:\n        return tf.nn.dropout(input_tensor, rate=dropout_prob)\n    return input_tensor\n\n", "CodeBleu_score": 40.30140557757546}]}
{"instance_id": "828917b9ac53", "generation_result": [{"code": "def create_attention_mask_from_input_mask(from_tensor, to_mask)\n\t\n    from_shape = from_tensor.shape.as_list()\n    batch_size = from_shape[0]\n    from_seq_length = from_shape[1]\n\n    to_shape = to_mask.shape.as_list()\n    to_seq_length = to_shape[1]\n\n    to_mask = tf.cast(\n        tf.reshape(to_mask, [batch_size, 1, to_seq_length]), tf.float32\n    )\n\n    # We don't assume that `from_tensor` is a mask (although it could be). We\n    # don't actually care if we attend *from* padding tokens (only *to* padding)\n    # tokens so we create a tensor of all ones.\n    #\n    # `broadcast_ones` = [batch_size, from_seq_length, 1]\n    broadcast_ones = tf.ones(\n        shape=[batch_size, from_seq_length, 1], dtype=tf.float32\n    )\n\n    # Here we broadcast along two dimensions to create the mask.\n    mask = broadcast_ones * to_mask\n\n    return mask\n\n", "CodeBleu_score": 51.04795108440414}, {"code": "def create_attention_mask_from_input_mask(from_tensor, to_mask)\n\t\n    from_shape = get_shape_list(from_tensor, expected_rank=[2, 3])\n    batch_size = from_shape[0]\n    from_seq_length = from_shape[1]\n\n    to_shape = get_shape_list(to_mask, expected_rank=2)\n    to_seq_length = to_shape[1]\n\n    to_mask = tf.cast(\n        tf.reshape(to_mask, [batch_size, 1, to_seq_length]), tf.float32)\n\n    # We don't assume that `from_tensor` is a mask (although it could be). We\n    # don't actually care if we attend *from* padding tokens (only *to* padding)\n    # tokens so we create a tensor of all ones.\n    #\n    # `broadcast_ones` = [batch_size, from_seq_length, 1]\n    broadcast_ones = tf.ones(\n        shape=[batch_size, from_seq_length, 1], dtype=tf.float32)\n\n    # Here we broadcast along two dimensions to create the mask.\n    mask = broadcast_ones * to_mask\n\n    return mask\n\n", "CodeBleu_score": 64.6737227741467}, {"code": "def create_attention_mask_from_input_mask(from_tensor, to_mask)\n\t\n    from_shape = from_tensor.shape.as_list()\n    batch_size = from_shape[0]\n    from_seq_length = from_shape[1]\n    to_shape = to_mask.shape.as_list()\n    to_seq_length = to_shape[1]\n\n    to_mask = tf.cast(\n        tf.reshape(to_mask, [batch_size, 1, to_seq_length]), tf.float32\n    )\n\n    broadcast_ones = tf.ones(\n        shape=[batch_size, from_seq_length, 1], dtype=tf.float32\n    )\n\n    mask = broadcast_ones * to_mask\n\n    return mask\n\n", "CodeBleu_score": 51.04795108440414}]}
{"instance_id": "d7798e41a723", "generation_result": [{"code": "def transpose_for_scores(input_tensor, batch_size, num_attention_heads,\n                           seq_length, width)\n\t\n    output_tensor = tf.reshape(input_tensor, [batch_size, seq_length, num_attention_heads, width])\n    output_tensor = tf.transpose(output_tensor, [0, 2, 1, 3])\n    return output_tensor\n\n", "CodeBleu_score": 54.22065594588168}, {"code": "def transpose_for_scores(input_tensor, batch_size, num_attention_heads,\n                           seq_length, width)\n\t\n    output_tensor = tf.reshape(input_tensor, [batch_size, seq_length, num_attention_heads, width])\n    output_tensor = tf.transpose(output_tensor, [0, 2, 1, 3])\n    return output_tensor\n\n", "CodeBleu_score": 54.22065594588168}, {"code": "def transpose_for_scores(input_tensor, batch_size, num_attention_heads,\n                           seq_length, width)\n\t\n    output_tensor = tf.reshape(input_tensor, [batch_size, seq_length, num_attention_heads, width])\n\n    output_tensor = tf.transpose(output_tensor, [0, 2, 1, 3])\n    return output_tensor\n\n", "CodeBleu_score": 54.22065594588168}]}
{"instance_id": "711d466ace09", "generation_result": [{"code": "def reshape_to_matrix(input_tensor)\n\t\n    ndims = input_tensor.shape.ndims\n    if ndims < 2:\n        raise ValueError(\"Input tensor must have at least rank 2. Shape = %s\" %\n                         (input_tensor.shape))\n    if ndims == 2:\n        return input_tensor\n\n    width = input_tensor.shape[-1].value\n    output_tensor = tf.reshape(input_tensor, [-1, width])\n    return output_tensor\n\n", "CodeBleu_score": 81.62979837100144}, {"code": "def reshape_to_matrix(input_tensor)\n\t\n    ndims = input_tensor.shape.ndims\n    if ndims < 2:\n        raise ValueError(\"Input tensor must have at least rank 2. Shape = %s\" %\n                         (input_tensor.shape))\n    if ndims == 2:\n        return input_tensor\n\n    width = input_tensor.shape[-1].value\n    output_tensor = tf.reshape(input_tensor, [-1, width])\n    return output_tensor\n\n", "CodeBleu_score": 81.62979837100144}, {"code": "def reshape_to_matrix(input_tensor)\n\t\n    ndims = input_tensor.shape.ndims\n    if ndims < 2:\n        raise ValueError(\"Input tensor must have at least rank 2. Shape = %s\" %\n                         (input_tensor.shape))\n    if ndims == 2:\n        return input_tensor\n\n    width = input_tensor.shape[-1].value\n    output_tensor = tf.reshape(input_tensor, [-1, width])\n    return output_tensor\n\n", "CodeBleu_score": 81.62979837100144}]}
{"instance_id": "7adcd08f531e", "generation_result": [{"code": "def get_train_examples(self, data_dir)\n\t\n    train_examples = []\n    with open(os.path.join(data_dir, \"train.tsv\"), \"r\", encoding=\"utf-8\") as f:\n        reader = csv.reader(f, delimiter=\"\\t\")\n        for i, line in enumerate(reader):\n            if i == 0:\n                continue\n            guid = \"%s-%s\" % (\"train\", i)\n            text_a = tokenization.convert_to_unicode(line[3])\n            text_b = tokenization.convert_to_unicode(line[4])\n            label = tokenization.convert_to_unicode(line[5])\n            if label == \"contradictory\":\n                label = \"contradiction\"\n            train_examples.append(InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n    return train_examples\n\n", "CodeBleu_score": 48.21310677747353}, {"code": "def get_train_examples(self, data_dir)\n\t\n    train_file = os.path.join(data_dir, \"train.tsv\")\n    train_examples = []\n    with open(train_file, \"r\", encoding=\"utf-8\") as f:\n        reader = csv.reader(f, delimiter=\"\\t\")\n        for i, line in enumerate(reader):\n            if i == 0:\n                continue\n            guid = \"%s-%s\" % (\"train\", i)\n            text_a = tokenization.convert_to_unicode(line[3])\n            text_b = tokenization.convert_to_unicode(line[4])\n            label = tokenization.convert_to_unicode(line[5])\n            if label == \"contradictory\":\n                label = \"contradiction\"\n            train_examples.append(\n                InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n    return train_examples\n\n", "CodeBleu_score": 48.434201240578304}, {"code": "def get_train_examples(self, data_dir)\n\t\n    train_examples = []\n    with open(os.path.join(data_dir, \"train.tsv\"), \"r\", encoding=\"utf-8\") as reader:\n        reader = csv.reader(reader, delimiter=\"\\t\")\n        for i, line in enumerate(reader):\n            if i == 0:\n                continue\n            guid = \"%s-%s\" % (\"train\", i)\n            text_a = tokenization.convert_to_unicode(line[3])\n            text_b = tokenization.convert_to_unicode(line[4])\n            label = tokenization.convert_to_unicode(line[5])\n            if label == \"contradictory\":\n                label = \"contradiction\"\n            train_examples.append(\n                InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label)\n            )\n    return train_examples\n\n", "CodeBleu_score": 46.70723592826717}]}
{"instance_id": "26a0bb96dab9", "generation_result": [{"code": "def metric_fn(per_example_loss, label_ids, logits, is_real_example)\n\t\n    predictions = tf.argmax(logits, axis=-1, output_type=tf.int32)\n    accuracy = tf.metrics.accuracy(\n        labels=label_ids, predictions=predictions, weights=is_real_example)\n    loss = tf.metrics.mean(values=per_example_loss, weights=is_real_example)\n    return {\n        \"eval_accuracy\": accuracy,\n        \"eval_loss\": loss,\n    }\n\n", "CodeBleu_score": 65.47238279114057}, {"code": "def metric_fn(per_example_loss, label_ids, logits, is_real_example)\n\t\n    predictions = tf.argmax(logits, axis=-1, output_type=tf.int32)\n    accuracy = tf.metrics.accuracy(\n        labels=label_ids, predictions=predictions, weights=is_real_example)\n    loss = tf.metrics.mean(values=per_example_loss, weights=is_real_example)\n    return {\n        \"eval_accuracy\": accuracy,\n        \"eval_loss\": loss,\n    }\n\n", "CodeBleu_score": 65.47238279114057}, {"code": "def metric_fn(per_example_loss, label_ids, logits, is_real_example)\n\t\n    predictions = tf.argmax(logits, axis=-1, output_type=tf.int32)\n    accuracy = tf.metrics.accuracy(\n        labels=label_ids, predictions=predictions, weights=is_real_example)\n    loss = tf.metrics.mean(values=per_example_loss, weights=is_real_example)\n    return {\n        \"eval_accuracy\": accuracy,\n        \"eval_loss\": loss,\n    }\n\n", "CodeBleu_score": 65.47238279114057}]}
{"instance_id": "6154e4d28509", "generation_result": [{"code": "def convert_examples_to_features(examples, label_list, max_seq_length,\n                                 tokenizer)\n\t\n    print(\"convert_examples_to_features\")\n    features = []\n    for (ex_index, example) in enumerate(examples):\n        if ex_index % 10000 == 0:\n            print(\"Writing example %d of %d\" % (ex_index, len(examples)))\n        feature = convert_single_example(ex_index, example, label_list,\n                                         max_seq_length, tokenizer)\n        features.append(feature)\n    return features\n\n", "CodeBleu_score": 74.75940693490833}, {"code": "def convert_examples_to_features(examples, label_list, max_seq_length,\n                                 tokenizer)\n\t\n    print(len(examples))\n    features = []\n    for (ex_index, example) in enumerate(examples):\n        if ex_index % 10000 == 0:\n            tf.logging.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n        feature = convert_single_example(ex_index, example, label_list,\n                                         max_seq_length, tokenizer)\n        features.append(feature)\n    return features\n\n", "CodeBleu_score": 69.01923850011092}, {"code": "def convert_examples_to_features(examples, label_list, max_seq_length,\n                                 tokenizer)\n\t\n    features = []\n    for (ex_index, example) in enumerate(examples):\n        if ex_index % 10000 == 0:\n            tf.logging.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n\n        feature = convert_single_example(ex_index, example, label_list,\n                                         max_seq_length, tokenizer)\n\n        features.append(feature)\n    return features\n\n", "CodeBleu_score": 82.83594654369867}]}
{"instance_id": "e09c0db71ebb", "generation_result": [{"code": "def get_next_sentence_output(bert_config, input_tensor, labels)\n\t\n    # Simple binary classification. Note that 0 is \"next sentence\" and 1 is \"random sentence\". This weight matrix is not used after pre-training.\n    with tf.variable_scope(\"cls/seq_relationship\"):\n        output_weights = tf.get_variable(\n            \"output_weights\",\n            shape=[2, bert_config.hidden_size],\n            initializer=modeling.create_initializer(bert_config.initializer_range))\n        output_bias = tf.get_variable(\n            \"output_bias\", shape=[2], initializer=tf.zeros_initializer())\n\n    logits = tf.matmul(input_tensor, output_weights, transpose_b=True)\n    logits = tf.nn.bias_add(logits, output_bias)\n    log_probs = tf.nn.log_softmax(logits, axis=-1)\n\n    labels = tf.reshape(labels, [-1])\n    one_hot_labels = tf.one_hot(labels, depth=2, dtype=tf.float32)\n\n    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n    loss = tf.reduce_mean(per_example_loss)\n    return (loss, per_example_loss, log_probs)\n\n", "CodeBleu_score": 87.87003311020517}, {"code": "def get_next_sentence_output(bert_config, input_tensor, labels)\n\t\n    # Simple binary classification. Note that 0 is \"next sentence\" and 1 is \"random sentence\". This weight matrix is not used after pre-training.\n    with tf.variable_scope(\"cls/seq_relationship\"):\n        output_weights = tf.get_variable(\n            \"output_weights\",\n            shape=[2, bert_config.hidden_size],\n            initializer=modeling.create_initializer(bert_config.initializer_range))\n        output_bias = tf.get_variable(\n            \"output_bias\", shape=[2], initializer=tf.zeros_initializer())\n        logits = tf.matmul(input_tensor, output_weights, transpose_b=True)\n        logits = tf.nn.bias_add(logits, output_bias)\n        log_probs = tf.nn.log_softmax(logits, axis=-1)\n        labels = tf.reshape(labels, [-1])\n        one_hot_labels = tf.one_hot(labels, depth=2, dtype=tf.float32)\n        per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n        loss = tf.reduce_mean(per_example_loss)\n        return (loss, per_example_loss, log_probs)\n\n", "CodeBleu_score": 86.94900271179796}, {"code": "def get_next_sentence_output(bert_config, input_tensor, labels)\n\t\n    with tf.variable_scope(\"cls/seq_relationship\"):\n        output_weights = tf.get_variable(\n            \"output_weights\",\n            shape=[2, bert_config.hidden_size],\n            initializer=modeling.create_initializer(bert_config.initializer_range),\n        )\n        output_bias = tf.get_variable(\n            \"output_bias\",\n            shape=[2],\n            initializer=tf.zeros_initializer(),\n        )\n        logits = tf.matmul(input_tensor, output_weights, transpose_b=True)\n        logits = tf.nn.bias_add(logits, output_bias)\n        log_probs = tf.nn.log_softmax(logits, axis=-1)\n        labels = tf.reshape(labels, [-1])\n        one_hot_labels = tf.one_hot(labels, depth=2, dtype=tf.float32)\n        per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n        loss = tf.reduce_mean(per_example_loss)\n        return (loss, per_example_loss, log_probs)\n\n", "CodeBleu_score": 82.89242271321334}]}
{"instance_id": "1f66b020f485", "generation_result": [{"code": "def gather_indexes(sequence_tensor, positions)\n\t\n    sequence_shape = modeling.get_shape_list(sequence_tensor, expected_rank=3)\n    batch_size = sequence_shape[0]\n    seq_length = sequence_shape[1]\n    width = sequence_shape[2]\n\n    flat_offsets = tf.reshape(tf.range(0, batch_size, dtype=tf.int32) * seq_length, [-1, 1])\n    flat_positions = tf.reshape(positions + flat_offsets, [-1])\n    flat_sequence_tensor = tf.reshape(sequence_tensor, [batch_size * seq_length, width])\n    output_tensor = tf.gather(flat_sequence_tensor, flat_positions)\n    return output_tensor\n\n", "CodeBleu_score": 55.18468812590758}, {"code": "def gather_indexes(sequence_tensor, positions)\n\t\n    shape = modeling.get_shape_list(sequence_tensor, expected_rank=3)\n    batch_size = shape[0]\n    seq_length = shape[1]\n    width = shape[2]\n\n    flat_offsets = tf.reshape(tf.range(0, batch_size, dtype=tf.int32) * seq_length, [-1, 1])\n    flat_positions = tf.reshape(positions + flat_offsets, [-1])\n    flat_sequence_tensor = tf.reshape(sequence_tensor, [batch_size * seq_length, width])\n    output_tensor = tf.gather(flat_sequence_tensor, flat_positions)\n    return output_tensor\n\n", "CodeBleu_score": 48.44086041370425}, {"code": "def gather_indexes(sequence_tensor, positions)\n\t\n\n    sequence_shape = modeling.get_shape_list(sequence_tensor, expected_rank=3)\n    batch_size = sequence_shape[0]\n    seq_length = sequence_shape[1]\n    width = sequence_shape[2]\n\n    flat_offsets = tf.reshape(tf.range(0, batch_size, dtype=tf.int32) * seq_length, [-1, 1])\n    flat_positions = tf.reshape(positions + flat_offsets, [-1])\n    flat_sequence_tensor = tf.reshape(sequence_tensor, [batch_size * seq_length, width])\n    output_tensor = tf.gather(flat_sequence_tensor, flat_positions)\n    return output_tensor\n\n", "CodeBleu_score": 55.18468812590758}]}
{"instance_id": "956bffee9a33", "generation_result": [{"code": "def _decode_record(record, name_to_features)\n\t\n    example = tf.parse_single_example(record, name_to_features)\n    for name in list(example.keys()):\n        t = example[name]\n        if t.dtype == tf.int64:\n            t = tf.to_int32(t)\n        example[name] = t\n    return example\n\n", "CodeBleu_score": 71.32095143473128}, {"code": "def _decode_record(record, name_to_features)\n\t\n    example = tf.io.parse_single_example(record, name_to_features)\n    for name in list(example.keys()):\n        t = example[name]\n        if t.dtype == tf.int64:\n            t = tf.cast(t, tf.int32)\n        example[name] = t\n    return example\n\n", "CodeBleu_score": 60.19566542534298}, {"code": "def _decode_record(record, name_to_features)\n\t\n    example = tf.io.parse_single_example(record, name_to_features)\n    for name in list(example.keys()):\n        t = example[name]\n        if t.dtype == tf.int64:\n            t = tf.cast(t, tf.int32)\n        example[name] = t\n    return example\n\n", "CodeBleu_score": 60.19566542534298}]}
{"instance_id": "6b97f354e641", "generation_result": [{"code": "def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer,\n                         orig_answer_text)\n\t\n    tok_answer_text = \" \".join(tokenizer.tokenize(orig_answer_text))\n\n    for new_start in range(input_start, input_end + 1):\n        for new_end in range(input_end, new_start - 1, -1):\n            text_span = \" \".join(doc_tokens[new_start:(new_end + 1)])\n            if text_span == tok_answer_text:\n                return (new_start, new_end)\n\n    return (input_start, input_end)\n\n", "CodeBleu_score": 79.1127088409574}, {"code": "def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer,\n                         orig_answer_text)\n\t\n    tok_answer_text = \" \".join(tokenizer.tokenize(orig_answer_text))\n\n    for new_start in range(input_start, input_end + 1):\n        for new_end in range(input_end, new_start - 1, -1):\n            text_span = \" \".join(doc_tokens[new_start:(new_end + 1)])\n            if text_span == tok_answer_text:\n                return (new_start, new_end)\n\n    return (input_start, input_end)\n\n", "CodeBleu_score": 79.1127088409574}, {"code": "def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer,\n                         orig_answer_text)\n\t\n    tok_answer_text = \" \".join(tokenizer.tokenize(orig_answer_text))\n\n    for new_start in range(input_start, input_end + 1):\n        for new_end in range(input_end, new_start - 1, -1):\n            text_span = \" \".join(doc_tokens[new_start:(new_end + 1)])\n            if text_span == tok_answer_text:\n                return (new_start, new_end)\n\n    return (input_start, input_end)\n\n", "CodeBleu_score": 79.1127088409574}]}
{"instance_id": "aba334b92b34", "generation_result": [{"code": "def _check_is_max_context(doc_spans, cur_span_index, position)\n\t\n    best_score = None\n    best_span_index = None\n    for (span_index, doc_span) in enumerate(doc_spans):\n        end = doc_span.start + doc_span.length - 1\n        if position < doc_span.start:\n            continue\n        if position > end:\n            continue\n        num_left_context = position - doc_span.start\n        num_right_context = end - position\n        score = min(num_left_context, num_right_context) + 0.01 * doc_span.length\n        if best_score is None or score > best_score:\n            best_score = score\n            best_span_index = span_index\n\n    return cur_span_index == best_span_index\n\n", "CodeBleu_score": 76.04591257743547}, {"code": "def _check_is_max_context(doc_spans, cur_span_index, position)\n\t\n    best_score = None\n    best_span_index = None\n    for (span_index, doc_span) in enumerate(doc_spans):\n        end = doc_span.start + doc_span.length - 1\n        if position < doc_span.start:\n            continue\n        if position > end:\n            continue\n        num_left_context = position - doc_span.start\n        num_right_context = end - position\n        score = min(num_left_context, num_right_context) + 0.01 * doc_span.length\n        if best_score is None or score > best_score:\n            best_score = score\n            best_span_index = span_index\n\n    return cur_span_index == best_span_index\n\n", "CodeBleu_score": 76.04591257743547}, {"code": "def _check_is_max_context(doc_spans, cur_span_index, position)\n\t\n    best_score = None\n    best_span_index = None\n    for (span_index, doc_span) in enumerate(doc_spans):\n        end = doc_span.start + doc_span.length - 1\n        if position < doc_span.start:\n            continue\n        if position > end:\n            continue\n        num_left_context = position - doc_span.start\n        num_right_context = end - position\n        score = min(num_left_context, num_right_context) + 0.01 * doc_span.length\n        if best_score is None or score > best_score:\n            best_score = score\n            best_span_index = span_index\n\n    return cur_span_index == best_span_index\n\n", "CodeBleu_score": 76.04591257743547}]}
{"instance_id": "cf92ad509f6a", "generation_result": [{"code": "def load_vocab(vocab_file)\n\t\n    vocab = collections.OrderedDict()\n    index = 0\n    with tf.gfile.GFile(vocab_file, \"r\") as reader:\n        while True:\n            token = convert_to_unicode(reader.readline())\n            if not token:\n                break\n            token = token.strip()\n            vocab[token] = index\n            index += 1\n    return vocab\n\n", "CodeBleu_score": 73.46545204327492}, {"code": "def load_vocab(vocab_file)\n\t\n    vocab = collections.OrderedDict()\n    index = 0\n    with open(vocab_file, \"r\", encoding=\"utf-8\") as reader:\n        while True:\n            token = reader.readline()\n            if not token:\n                break\n            token = token.strip()\n            vocab[token] = index\n            index += 1\n    return vocab\n\n", "CodeBleu_score": 59.6145411111134}, {"code": "def load_vocab(vocab_file)\n\t\n    vocab = collections.OrderedDict()\n    index = 0\n    with tf.gfile.GFile(vocab_file, \"r\") as reader:\n        while True:\n            token = convert_to_unicode(reader.readline())\n            if not token:\n                break\n            token = token.strip()\n            vocab[token] = index\n            index += 1\n    return vocab\n\n", "CodeBleu_score": 73.46545204327492}]}
{"instance_id": "6c865803ac12", "generation_result": [{"code": "def convert_by_vocab(vocab, items)\n\t\n    output = []\n    for item in items:\n        output.append(vocab[item])\n    return output\n\n", "CodeBleu_score": 79.63749907165396}, {"code": "def convert_by_vocab(vocab, items)\n\t\n    output = []\n    for item in items:\n        output.append(vocab[item])\n    return output\n\n", "CodeBleu_score": 79.63749907165396}, {"code": "def convert_by_vocab(vocab, items)\n\t\n    output = []\n    for item in items:\n        output.append(vocab[item])\n    return output\n\n", "CodeBleu_score": 79.63749907165396}]}
{"instance_id": "e93b9200dc4a", "generation_result": [{"code": "def tokenize(self, text)\n\t\n    text = convert_to_unicode(text)\n\n    # This was added on November 1st, 2018 for the multilingual and Chinese\n    # models. This is also applied to the English models now, but it doesn't\n    # matter since the English models were not trained on any Chinese data\n    # and generally don't have any Chinese data in them (there are Chinese\n    # characters in the vocabulary because Wikipedia does have some Chinese\n    # words in the English Wikipedia.).\n    text = self.basic_tokenizer.tokenize(text)\n\n    text = [self.wordpiece_tokenizer.tokenize(token) for token in text]\n    return [token for sublist in text for token in sublist]\n\n", "CodeBleu_score": 37.609936147208394}, {"code": "def tokenize(self, text)\n\t\n    text = convert_to_unicode(text)\n    text = self.basic_tokenizer.tokenize(text)\n    text = self.wordpiece_tokenizer.tokenize(text)\n    return text\n", "CodeBleu_score": 19.7783917281408}, {"code": "def tokenize(self, text)\n\t\n    text = convert_to_unicode(text)\n\n    # This was added on November 1st, 2018 for the multilingual and Chinese\n    # models. This is also applied to the English models now, but it doesn't\n    # matter since the English models were not trained on any Chinese data\n    # and generally don't have any Chinese data in them (there are Chinese\n    # characters in the vocabulary because Wikipedia does have some Chinese\n    # words in the English Wikipedia.).\n    text = self.basic_tokenizer.tokenize(text)\n\n    text = [self.wordpiece_tokenizer.tokenize(token) for token in text]\n    text = [item for sublist in text for item in sublist]\n    return text\n\n", "CodeBleu_score": 34.970468204217354}]}
{"instance_id": "2ff6f6db53c9", "generation_result": [{"code": "def safe_load(path, offset, duration, sample_rate, dtype)\n\t\n    try:\n        data, _ = librosa.load(\n            path,\n            offset=offset,\n            duration=duration,\n            sr=sample_rate,\n            mono=True,\n            dtype=dtype,\n        )\n        return data, False\n    except Exception as e:\n        logger.error(f\"Error loading audio file {path}: {e}\")\n        return -1.0, True\n\n", "CodeBleu_score": 35.766783004567685}, {"code": "def safe_load(path, offset, duration, sample_rate, dtype)\n\t\n    try:\n        audio_data, _ = Codec.load(path, offset, duration, sample_rate, dtype)\n        return audio_data, False\n    except Exception as e:\n        logger.error(f'An error occurred while loading audio data from {path} at offset {offset} and duration {duration} with sample rate {sample_rate} and dtype {dtype}: {e}')\n        return -1.0, True\n\n", "CodeBleu_score": 28.2994026859719}, {"code": "def safe_load(path, offset, duration, sample_rate, dtype)\n\t\n    try:\n        audio_data, _ = librosa.load(path, offset=offset, duration=duration, sr=sample_rate, dtype=dtype)\n        return audio_data, False\n    except Exception as error:\n        logger.error(f\"An error occurred while loading audio data from {path} at {offset}s - {duration}s.\")\n        logger.error(error)\n        return -1.0, True\n\n", "CodeBleu_score": 33.49265470428996}]}
{"instance_id": "12fbe1f03d8f", "generation_result": [{"code": "def to_n_channels(waveform: tf.Tensor, n_channels: int) -> tf.Tensor\n\t\n    if n_channels == waveform.shape[-1]:\n        return waveform\n    if n_channels < waveform.shape[-1]:\n        return waveform[..., :n_channels]\n    n_channels_to_add = n_channels - waveform.shape[-1]\n    return tf.tile(waveform, [1, 1, n_channels_to_add])[..., :n_channels]\n\n", "CodeBleu_score": 43.6366526987425}, {"code": "def to_n_channels(waveform: tf.Tensor, n_channels: int) -> tf.Tensor\n\t\n    if n_channels == 1:\n        return tf.squeeze(waveform, axis=-1)\n    elif n_channels == 2:\n        return tf.tile(waveform, [1, 1, 2])\n    elif n_channels == 4:\n        return tf.tile(waveform, [1, 1, 4])\n    else:\n        raise ValueError(f\"Invalid number of channels: {n_channels}\")\n\n", "CodeBleu_score": 45.109523963128865}, {"code": "def to_n_channels(waveform: tf.Tensor, n_channels: int) -> tf.Tensor\n\t\n    if n_channels == waveform.shape[-1]:\n        return waveform\n    elif n_channels > waveform.shape[-1]:\n        return tf.tile(waveform, [1, 1, n_channels // waveform.shape[-1]])[:, :, :n_channels]\n    else:\n        return waveform[:, :, :n_channels]\n\n", "CodeBleu_score": 46.93272914740814}]}
{"instance_id": "5e215f029b2e", "generation_result": [{"code": "def spectrogram_to_db_uint(\n    spectrogram: tf.Tensor, db_range: float = 100.0, **kwargs\n) -> tf.Tensor\n\t\n    return tf.cast(\n        tf.clip_by_value(\n            tf.cast(\n                tf.clip_by_value(\n                    tf.cast(\n                        tf.math.round(\n                            tf.clip_by_value(\n                                gain_to_db(spectrogram, **kwargs),\n                                -db_range,\n                                tf.cast(tf.reduce_max(spectrogram), tf.float32),\n                            )\n                        ),\n                        tf.float32,\n                    ),\n                    0,\n                    255,\n                ),\n                tf.uint8,\n            ),\n            0,\n            255,\n        ),\n        tf.uint8,\n    )\n\n", "CodeBleu_score": 32.69671390113232}, {"code": "def spectrogram_to_db_uint(\n    spectrogram: tf.Tensor, db_range: float = 100.0, **kwargs\n) -> tf.Tensor\n\t\n    return tf.cast(\n        tf.clip_by_value(\n            tf.cast(\n                tf.clip_by_value(\n                    tf.cast(\n                        gain_to_db(spectrogram, **kwargs),\n                        tf.float32,\n                    ),\n                    -db_range,\n                    0.0,\n                ),\n                tf.int32,\n            ),\n            0,\n            255,\n        ),\n        tf.uint8,\n    )\n\n", "CodeBleu_score": 31.11300141933395}, {"code": "def spectrogram_to_db_uint(\n    spectrogram: tf.Tensor, db_range: float = 100.0, **kwargs\n) -> tf.Tensor\n\t\n    spectrogram_db = gain_to_db(spectrogram, **kwargs)\n    max_db = tf.reduce_max(spectrogram_db, axis=[-3, -2, -1], keepdims=True)\n    min_db = tf.reduce_min(spectrogram_db, axis=[-3, -2, -1], keepdims=True)\n    spectrogram_db = tf.clip_by_value(spectrogram_db, min_db, max_db - db_range)\n    spectrogram_db = tf.cast(spectrogram_db, tf.uint8)\n    return spectrogram_db\n\n", "CodeBleu_score": 41.88779669914432}]}
{"instance_id": "b65c0968dae7", "generation_result": [{"code": "def compute_spectrogram_tf(\n    waveform: tf.Tensor,\n    frame_length: int = 2048,\n    frame_step: int = 512,\n    spec_exponent: float = 1.0,\n    window_exponent: float = 1.0,\n) -> tf.Tensor\n\t\n    # Hann window\n    window = hann_window(frame_length, periodic=True) ** window_exponent\n\n    # STFT\n    stft_tensor = stft(\n        waveform,\n        frame_length=frame_length,\n        frame_step=frame_step,\n        fft_length=frame_length,\n        window_fn=lambda _, __: window,\n        pad_end=True,\n    )\n\n    # Transpose and take absolute value\n    stft_tensor = tf.abs(tf.transpose(stft_tensor, perm=[1, 2, 0])) ** spec_exponent\n\n    return stft_tensor\n", "CodeBleu_score": 40.89472337855874}, {"code": "def compute_spectrogram_tf(\n    waveform: tf.Tensor,\n    frame_length: int = 2048,\n    frame_step: int = 512,\n    spec_exponent: float = 1.0,\n    window_exponent: float = 1.0,\n) -> tf.Tensor\n\t\n    window_length = int(frame_length * 2 ** (window_exponent - 1))\n    window_step = int(frame_step * 2 ** (window_exponent - 1))\n    window = hann_window(window_length, periodic=True)\n    stft_matrix = tf.signal.stft(\n        waveform,\n        frame_length=frame_length,\n        frame_step=frame_step,\n        window_fn=lambda _, dtype: tf.cast(window, dtype),\n        pad_end=False,\n    )\n    stft_matrix = tf.abs(stft_matrix) ** spec_exponent\n    stft_matrix = tf.transpose(stft_matrix)\n    return stft_matrix\n\n", "CodeBleu_score": 37.18847521031886}, {"code": "def compute_spectrogram_tf(\n    waveform: tf.Tensor,\n    frame_length: int = 2048,\n    frame_step: int = 512,\n    spec_exponent: float = 1.0,\n    window_exponent: float = 1.0,\n) -> tf.Tensor\n\t\n    window = hann_window(frame_length, periodic=True) ** window_exponent\n    stft_matrix = tf.abs(\n        stft(\n            waveform,\n            frame_length=frame_length,\n            frame_step=frame_step,\n            fft_length=frame_length,\n            window_fn=lambda frame_length, dtype: window,\n            pad_end=True,\n        )\n    )\n    return tf.pow(stft_matrix, spec_exponent)\n", "CodeBleu_score": 40.47497811053764}]}
{"instance_id": "a4686140a634", "generation_result": [{"code": "def time_stretch(\n    spectrogram: tf.Tensor,\n    factor: float = 1.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n    # Get the original dimensions of the spectrogram\n    original_shape = tf.shape(spectrogram)\n\n    # Get the new time dimension\n    new_time = tf.cast(original_shape[1] * factor, tf.int32)\n\n    # Resize the spectrogram\n    resized_spectrogram = tf.image.resize(\n        spectrogram,\n        size=[original_shape[0], new_time],\n        method=method,\n    )\n\n    # Crop or pad the spectrogram to match the original dimensions\n    if method == tf.image.ResizeMethod.BILINEAR:\n        # Crop the spectrogram\n        cropped_spectrogram = resized_spectrogram[:, :original_shape[1]]\n    elif method == tf.image.ResizeMethod.NEAREST_NEIGHBOR:\n        # Pad the spectrogram\n        padded_spectrogram = tf.pad(\n            resized_spectrogram,\n            paddings=[\n                [0, 0],\n                [0, original_shape[1] - new_time],\n            ],\n        )\n        cropped_spectrogram = padded_spectrogram[:, :original_shape[1]]\n    else:\n        raise ValueError(\n            \"The specified interpolation method is not supported by the time stretch function.\"\n        )\n\n    return cropped_spectrogram\n\n", "CodeBleu_score": 36.308545912257685}, {"code": "def time_stretch(\n    spectrogram: tf.Tensor,\n    factor: float = 1.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n    original_time_dimension = spectrogram.shape[1]\n    new_time_dimension = int(original_time_dimension * factor)\n    resized_spectrogram = tf.image.resize(\n        spectrogram,\n        size=[spectrogram.shape[0], new_time_dimension],\n        method=method,\n    )\n    resized_spectrogram = tf.image.resize_with_crop_or_pad(\n        resized_spectrogram,\n        target_height=original_time_dimension,\n        target_width=spectrogram.shape[2],\n    )\n    return resized_spectrogram\n\n", "CodeBleu_score": 29.11515885140138}, {"code": "def time_stretch(\n    spectrogram: tf.Tensor,\n    factor: float = 1.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n    # Calculate the new time dimension\n    new_time_dim = tf.cast(\n        tf.round(tf.shape(spectrogram)[1] * factor), dtype=tf.int32\n    )\n\n    # Resize the spectrogram\n    spectrogram = tf.image.resize(\n        spectrogram,\n        size=[tf.shape(spectrogram)[0], new_time_dim],\n        method=method,\n    )\n\n    # Adjust the spectrogram to the original dimensions\n    spectrogram = tf.image.resize_with_crop_or_pad(\n        spectrogram,\n        target_height=tf.shape(spectrogram)[0],\n        target_width=tf.shape(spectrogram)[1],\n    )\n\n    return spectrogram\n\n", "CodeBleu_score": 34.86623296252893}]}
{"instance_id": "6dd01101d4ae", "generation_result": [{"code": "def pitch_shift(\n    spectrogram: tf.Tensor,\n    semitone_shift: float = 0.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n\n    # Get the number of semitones to shift\n    semitone_shift = tf.cast(semitone_shift, tf.float32)\n\n    # Calculate the resizing factor\n    resize_factor = tf.math.exp(semitone_shift * (tf.math.log(2.0) / 12.0))\n\n    # Resize the spectrogram along the frequency axis\n    spectrogram = tf.image.resize(\n        spectrogram,\n        [\n            tf.cast(tf.shape(spectrogram)[0], tf.int32),\n            tf.cast(tf.cast(tf.shape(spectrogram)[1], tf.float32) * resize_factor, tf.int32),\n        ],\n        method=method,\n    )\n\n    # Pad the resized spectrogram to maintain the original dimensions\n    spectrogram = tf.pad(\n        spectrogram,\n        [\n            [0, 0],\n            [\n                tf.cast(\n                    (tf.shape(spectrogram)[1] - tf.shape(spectrogram)[0]) // 2, tf.int32\n                ),\n                tf.cast(\n                    (tf.shape(spectrogram)[1] - tf.shape(spectrogram)[0]) // 2, tf.int32\n                ),\n            ],\n        ],\n        mode=\"CONSTANT\",\n        constant_values=0.0,\n    )\n\n    return spectrogram\n\n", "CodeBleu_score": 27.475595053926817}, {"code": "def pitch_shift(\n    spectrogram: tf.Tensor,\n    semitone_shift: float = 0.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n\n    # Calculating the resizing factor\n    factor = 2.0 ** (semitone_shift / 12.0)\n\n    # Resizing the spectrogram\n    resized_spectrogram = tf.image.resize(\n        spectrogram,\n        [int(spectrogram.shape[0] * factor), spectrogram.shape[1]],\n        method=method,\n    )\n\n    # Paddind the resized spectrogram to maintain the original dimensions\n    padded_spectrogram = tf.pad(\n        resized_spectrogram,\n        [\n            [\n                int(\n                    (spectrogram.shape[0] * factor)\n                    - resized_spectrogram.shape[0]\n                    + spectrogram.shape[0]\n                )\n                // 2,\n                int(\n                    (spectrogram.shape[0] * factor)\n                    - resized_spectrogram.shape[0]\n                    + spectrogram.shape[0]\n                )\n                // 2,\n            ],\n            [0, 0],\n        ],\n    )\n\n    return padded_spectrogram\n\n", "CodeBleu_score": 24.320604500826498}, {"code": "def pitch_shift(\n    spectrogram: tf.Tensor,\n    semitone_shift: float = 0.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n    # Calculate the resizing factor\n    factor = 2.0 ** (semitone_shift / 12.0)\n    # Resize the spectrogram along the frequency axis\n    resized_spectrogram = tf.image.resize(\n        spectrogram,\n        [tf.shape(spectrogram)[0], tf.cast(tf.shape(spectrogram)[1] * factor, tf.int32)],\n        method=method,\n    )\n    # Pad the resized spectrogram to maintain the original dimensions\n    padded_spectrogram = tf.pad(\n        resized_spectrogram,\n        [[0, 0], [0, tf.shape(spectrogram)[1] - tf.shape(resized_spectrogram)[1]]],\n    )\n    return padded_spectrogram\n\n", "CodeBleu_score": 35.48135502686325}]}
{"instance_id": "612f0826e56a", "generation_result": [{"code": "def _get_conv_activation_layer(params: Dict) -> Any\n\t\n    if params[\"conv_activation\"] == \"relu\":\n        return ReLU()\n    elif params[\"conv_activation\"] == \"elu\":\n        return ELU()\n    elif params[\"conv_activation\"] == \"leaky_relu\":\n        return LeakyReLU(alpha=0.2)\n    else:\n        raise ValueError(\n            \"The `conv_activation` parameter should be one of the following: `relu`, `elu`, or `leaky_relu`.\"\n        )\n\n", "CodeBleu_score": 34.72861878302487}, {"code": "def _get_conv_activation_layer(params: Dict) -> Any\n\t\n    conv_activation = params.get(\"conv_activation\", \"relu\")\n    if conv_activation == \"relu\":\n        return ReLU()\n    elif conv_activation == \"elu\":\n        return ELU()\n    elif conv_activation == \"leaky_relu\":\n        return LeakyReLU(alpha=0.2)\n    else:\n        raise ValueError(\n            f\"Unknown convolutional activation layer: {conv_activation}\"\n        )\n\n", "CodeBleu_score": 42.39037536623832}, {"code": "def _get_conv_activation_layer(params: Dict) -> Any\n\t\n    conv_activation = params.get(\"conv_activation\", \"relu\")\n    if conv_activation == \"relu\":\n        return ReLU()\n    if conv_activation == \"elu\":\n        return ELU()\n    if conv_activation == \"leaky_relu\":\n        return LeakyReLU(alpha=0.2)\n    raise ValueError(\n        f\"Unknown activation function {conv_activation}. Available options are 'relu', 'elu', 'leaky_relu'.\"\n    )\n\n", "CodeBleu_score": 36.23877676801328}]}
{"instance_id": "2f8b5a2fc0f2", "generation_result": [{"code": "def softmax_unet(\n    input_tensor: tf.Tensor, instruments: Iterable[str], params: Dict = {}\n) -> Dict\n\t\n    output_dict: Dict[str, tf.Tensor] = {}\n    for instrument in instruments:\n        output_dict[instrument] = tf.math.reduce_sum(\n            tf.math.multiply(\n                unet(\n                    input_tensor,\n                    instruments=[instrument],\n                    params=params,\n                )[instrument],\n                tf.ones_like(input_tensor),\n            ),\n            axis=-1,\n        )\n    return output_dict\n\n", "CodeBleu_score": 33.98992220875863}, {"code": "def softmax_unet(\n    input_tensor: tf.Tensor, instruments: Iterable[str], params: Dict = {}\n) -> Dict\n\t\n    instrument_spectrograms = {}\n    for instrument in instruments:\n        instrument_spectrograms[instrument] = softmax_unet_single_instrument(\n            input_tensor, instrument, params\n        )\n    return instrument_spectrograms\n\n", "CodeBleu_score": 17.078493519868704}, {"code": "def softmax_unet(\n    input_tensor: tf.Tensor, instruments: Iterable[str], params: Dict = {}\n) -> Dict\n\t\n    if not isinstance(input_tensor, tf.Tensor):\n        raise TypeError(\"The input tensor must be a Tensor.\")\n    if not isinstance(instruments, Iterable):\n        raise TypeError(\"The instruments must be an Iterable.\")\n    if not isinstance(params, dict):\n        raise TypeError(\"The parameters must be a dictionary.\")\n    if not all(isinstance(instrument, str) for instrument in instruments):\n        raise TypeError(\"The instruments must be strings.\")\n    if not all(instrument in params for instrument in instruments):\n        raise ValueError(\"The instruments must be in the parameters dictionary.\")\n    if not all(isinstance(param, dict) for param in params.values()):\n        raise TypeError(\"The parameters must be dictionaries.\")\n    if not all(\n        \"n_filters\" in param and \"n_layers\" in param for param in params.values()\n    ):\n        raise ValueError(\n            \"The parameters must contain the keys 'n_filters' and 'n_layers'.\"\n        )\n    if not all(\n        isinstance(param[\"n_filters\"], int) and isinstance(param[\"n_layers\"], int)\n        for param in params.values()\n    ):\n        raise TypeError(\n            \"The parameters must contain the keys 'n_filters' and 'n_layers', and the values must be integers.\"\n        )\n    if not all(param[\"n_filters\"] > 0 for param in params.values()):\n        raise ValueError(\n            \"The parameters must contain the keys 'n_filters' and 'n_layers', and the values must be positive.\"\n        )\n    if not all(param[\"n_layers\"] > 0 for param in params.values()):\n        raise ValueError(\n            \"The parameters must contain the keys 'n_filters' and 'n_layers', and the values must be positive.\"\n        )\n    if not all(\n        \"activation\" in param and \"kernel_size\" in param\n        for param in params.values()\n    ):\n        raise ValueError(\n            \"The parameters must contain the keys 'activation' and 'kernel_size'.\"\n        )\n    if not all(\n        isinstance(param[\"activation\"], str) and isinstance(param[\"kernel_size\"], int)\n        for param in params.values()\n    ):\n        raise TypeError(\n            \"The parameters must contain the keys 'activation' and 'kernel_size', and the values must be strings and integers.\"\n        )\n    if not all(\n        param[\"activation\"]", "CodeBleu_score": 17.49254524369137}]}
{"instance_id": "400b19ce8520", "generation_result": [{"code": "def compute_file_checksum(path)\n\t\n    sha256_hash = hashlib.sha256()\n    with open(path, \"rb\") as f:\n        for byte_block in iter(lambda: f.read(4096), b\"\"):\n            sha256_hash.update(byte_block)\n    return sha256_hash.hexdigest()\n\n", "CodeBleu_score": 53.675596493972236}, {"code": "def compute_file_checksum(path)\n\t\n    sha256_hash = hashlib.sha256()\n    with open(path, \"rb\") as f:\n        for byte_block in iter(lambda: f.read(4096), b\"\"):\n            sha256_hash.update(byte_block)\n    return sha256_hash.hexdigest()\n\n", "CodeBleu_score": 53.675596493972236}, {"code": "def compute_file_checksum(path)\n\t\n    sha256 = hashlib.sha256()\n    with open(path, \"rb\") as f:\n        for chunk in iter(lambda: f.read(4096), b\"\"):\n            sha256.update(chunk)\n    return sha256.hexdigest()\n\n", "CodeBleu_score": 63.6345718562807}]}
{"instance_id": "5f2e9c6e15f1", "generation_result": [{"code": "def checksum(self, name: str) -> str\n\t\n    url = f\"{self.host}/{self.repository}/{self.release_path}/{self.checksum_index}\"\n    response = httpx.get(url)\n    if response.status_code != 200:\n        raise ValueError(\n            f\"Cannot retrieve checksum index from {url}:", "CodeBleu_score": 20.91805228189936}, {"code": "def checksum(self, name: str) -> str\n\t\n    # Construct URL\n    url = (\n        environ[\"MLSERVER_STORAGE_URI\"]\n        + f\"/models/{self.repository}/{self.release}/{name}.json\"\n    )\n\n    # Send GET request\n    response = httpx.get(url)\n\n    # Check for successful response\n    if response.status_code != 200:\n        raise ValueError(\n            f\"Failed to get checksum for model {name}: {response.text}\"\n        )\n\n    # Parse response as JSON\n    response_json = response.json()\n\n    # Check if model name is in index\n    if name not in response_json[\"models\"]:\n        raise ValueError(\n            f\"Model {name} not found in index for repository {self.repository}\"\n        )\n\n    # Return checksum\n    return response_json[\"models\"][name][\"checksum\"]\n\n", "CodeBleu_score": 36.58101336582716}, {"code": "def checksum(self, name: str) -> str\n\t\n    host = environ.get(\"VITESSCE_REPO_HOST\", \"https://www.github.com\")\n    repository = environ.get(\"VITESSCE_REPO_NAME\", \"vitessce\")\n    release_path = environ.get(\"VITESSCE_RELEASE_PATH\", \"releases/download\")\n    checksum_index = environ.get(\"VITESSCE_CHECKSUM_INDEX\", \"checksums.json\")\n    url = f\"{host}/{repository}/{release_path}/{name}/{checksum_index}\"\n    response = httpx.get(url)\n    if response.status_code != 200:\n        raise ValueError(f\"Error retrieving checksum for {name}.\")\n    checksums = response.json()\n    if name not in checksums:\n        raise ValueError(f\"Checksum not found for {name}.\")\n    return checksums[name]\n\n", "CodeBleu_score": 35.746351464199066}]}
{"instance_id": "9312c8b6032e", "generation_result": [{"code": "def download(self, name: str, path: str) -> None:\n\t\n    # Construct the download URL.\n    url = self.url(name)\n\n    # Log the download initiation.\n    self.log.info(f\"Downloading {name} from {url}...\")\n\n    # Stream the file using HTTP/2.\n    with httpx.stream(\"GET\", url) as response:\n        # Raise an exception if the response status code is not 200.\n        response.raise_for_status()\n\n        # Create a temporary file to store the archive.\n        with NamedTemporaryFile(delete=False) as archive_file:\n            # Stream the file in chunks.\n            for chunk in response.iter_bytes():\n                archive_file.write(chunk)\n\n    # Log the archive download completion.\n    self.log.info(f\"Download complete.\")\n\n    # Compute the checksum of the downloaded archive.\n    checksum = hashlib.sha256(open(archive_file.name, \"rb\").read()).hexdigest()\n\n    # Raise an exception if the checksum does not match the expected value.\n    if checksum != self.checksum(name):\n        raise ValueError(f\"Checksum mismatch for {name}.\")\n\n    # Extract the archive to the specified path.\n    with tarfile.open(archive_file.name) as archive:\n        archive.extractall(path)\n\n    # Delete the temporary file.\n    os.remove(archive_file.name)\n\n    # Log the archive extraction completion.\n    self.log.info(f\"Extraction complete.\")\n\n", "CodeBleu_score": 41.46507220711316}, {"code": "def download(self, name: str, path: str) -> None:\n\t\n    if name not in self.models:\n        raise ValueError(\n            f\"Model {name} is not available. Available models are: {', '.join(self.models.keys())}\"\n        )\n\n    url = self.models[name][\"url\"]\n    checksum = self.models[name][\"checksum\"]\n    archive = self.models[name][\"archive\"]\n\n    # Construct the download URL\n    download_url = url + \"/\" + archive\n\n    # Log the download initiation\n    self.logger.info(f\"Downloading {name} from {download_url}\")\n\n    # Stream the file using HTTP/2\n    with httpx.stream(\"GET\", download_url) as response:\n        # Raise an exception if the status code is not 200\n        response.raise_for_status()\n\n        # Create a temporary file\n        with NamedTemporaryFile(delete=False) as file:\n            # Stream the response body to the temporary file\n            for chunk in response.iter_bytes():\n                file.write(chunk)\n\n        # Get the temporary file's checksum\n        file_checksum = hashlib.sha256(open(file.name, \"rb\").read()).hexdigest()\n\n        # Compare the checksums\n        if file_checksum != checksum:\n            # Delete the temporary file\n            os.remove(file.name)\n\n            # Raise an exception if the checksums don't match\n            raise ValueError(\n                f\"Checksum for {file.name} does not match the expected value. Please try again.\"\n            )\n\n        # Extract the archive to the specified path\n        with tarfile.open(file.name) as tar:\n            tar.extractall(path)\n\n        # Delete the temporary file\n        os.remove(file.name)\n\n    # Log the extraction completion\n    self.logger.info(f\"Extracted {name} to {path}\")\n\n", "CodeBleu_score": 43.7955013836413}, {"code": "def download(self, name: str, path: str) -> None:\n\t\n    url = self.url(name)\n    expected_checksum = self.checksum(name)\n    archive_path = os.path.join(path, name)\n    with NamedTemporaryFile(delete=False) as tmp:\n        with httpx.stream(\"GET\", url) as response:\n            if response.status_code != 200:\n                raise RuntimeError(\n                    f\"Failed to download {name} from {url}.", "CodeBleu_score": 20.87716757577923}]}
{"instance_id": "ee9493f37d77", "generation_result": [{"code": "def load_configuration(descriptor: str) -> Dict\n\t\n    if descriptor.startswith(\"embedded:\"):\n        config_name = descriptor[len(\"embedded:\") :]\n        config_file = f\"config/{config_name}.json\"\n        if exists(config_file):\n            with open(config_file) as f:\n                return json.load(f)\n        else:\n            raise Exception(f\"Configuration file '{config_file}' not found\")\n    else:\n        if exists(descriptor):\n            with open(descriptor) as f:\n                return json.load(f)\n        else:\n            raise Exception(f\"Configuration file '{descriptor}' not found\")\n", "CodeBleu_score": 41.19353568403031}, {"code": "def load_configuration(descriptor: str) -> Dict\n\t\n    if descriptor.startswith('embedded:'):\n        config_name = descriptor[9:]\n        config_path = f'configs/{config_name}.json'\n        if exists(config_path):\n            with open(config_path) as config_file:\n                return json.load(config_file)\n        else:\n            raise FileNotFoundError(f'Embedded configuration {config_name} not found.')\n    else:\n        if exists(descriptor):\n            with open(descriptor) as config_file:\n                return json.load(config_file)\n        else:\n            raise FileNotFoundError(f'Configuration file {descriptor} not found.')\n", "CodeBleu_score": 38.35377884218909}, {"code": "def load_configuration(descriptor: str) -> Dict\n\t\n    if descriptor.startswith('{'):\n        return json.loads(descriptor)\n    else:\n        if exists(descriptor):\n            with open(descriptor, 'r') as f:\n                return json.load(f)\n        else:\n            raise FileNotFoundError(f'Configuration file {descriptor} not found.')\n", "CodeBleu_score": 25.661893192723724}]}
{"instance_id": "546134b67fbc", "generation_result": [{"code": "def sync_apply(\n    tensor_dict: Dict[str, tf.Tensor], func: Callable, concat_axis: int = 1\n) -> Dict[str, tf.Tensor]\n\t\n    if concat_axis not in (0, 1):\n        raise ValueError(\n            \"The concatenation axis must be either 0 or 1, but the specified axis is {}\".format(\n                concat_axis\n            )\n        )\n    concat_tensors = tf.concat(list(tensor_dict.values()), concat_axis)\n    processed_tensor = func(concat_tensors)\n    processed_tensors = tf.split(processed_tensor, len(tensor_dict), concat_axis)\n    return dict(zip(tensor_dict.keys(), processed_tensors))\n\n", "CodeBleu_score": 35.15432348598552}, {"code": "def sync_apply(\n    tensor_dict: Dict[str, tf.Tensor], func: Callable, concat_axis: int = 1\n) -> Dict[str, tf.Tensor]\n\t\n    if concat_axis not in [0, 1]:\n        raise ValueError(\"The concatenation axis must be either 0 or 1.\")\n    concat_tensors = tf.concat(list(tensor_dict.values()), axis=concat_axis)\n    processed_tensor = func(concat_tensors)\n    split_tensors = tf.split(processed_tensor, len(tensor_dict), axis=concat_axis)\n    return {\n        key: split_tensor\n        for key, split_tensor in zip(tensor_dict.keys(), split_tensors)\n    }\n\n", "CodeBleu_score": 36.820650908121536}, {"code": "def sync_apply(\n    tensor_dict: Dict[str, tf.Tensor], func: Callable, concat_axis: int = 1\n) -> Dict[str, tf.Tensor]\n\t\n    if concat_axis not in [0, 1]:\n        raise ValueError(\n            f\"The concatenation axis must be either 0 or 1, but the value is {concat_axis}\"\n        )\n\n    tensor_list = list(tensor_dict.values())\n    if concat_axis == 0:\n        tensor_concat = tf.concat(tensor_list, axis=0)\n    else:\n        tensor_concat = tf.concat(tensor_list, axis=1)\n    tensor_processed = func(tensor_concat)\n    tensor_list_processed = tf.split(tensor_processed, len(tensor_list), axis=concat_axis)\n    tensor_dict_processed = dict(zip(tensor_dict.keys(), tensor_list_processed))\n    return tensor_dict_processed\n\n", "CodeBleu_score": 44.6112994896366}]}
{"instance_id": "df48e78d46d2", "generation_result": [{"code": "def from_float32_to_uint8(\n    tensor: tf.Tensor,\n    tensor_key: str = \"tensor\",\n    min_key: str = \"min\",\n    max_key: str = \"max\",\n) -> tf.Tensor\n\t\n    # Get the minimum and maximum values of the tensor.\n    tensor_min = tf.reduce_min(tensor)\n    tensor_max = tf.reduce_max(tensor)\n    # Scale the tensor to the uint8 range.\n    tensor_scaled = (tensor - tensor_min) / (tensor_max - tensor_min) * 255\n    # Cast the tensor to uint8.\n    tensor_scaled = tf.cast(tensor_scaled, tf.uint8)\n    # Create a dictionary with the scaled tensor, the minimum value, and the maximum value.\n    tensor_dict = {tensor_key: tensor_scaled, min_key: tensor_min, max_key: tensor_max}\n    return tensor_dict\n\n", "CodeBleu_score": 59.35272825925747}, {"code": "def from_float32_to_uint8(\n    tensor: tf.Tensor,\n    tensor_key: str = \"tensor\",\n    min_key: str = \"min\",\n    max_key: str = \"max\",\n) -> tf.Tensor\n\t\n    min_value = tf.reduce_min(tensor)\n    max_value = tf.reduce_max(tensor)\n    return {\n        tensor_key: tf.cast(\n            tf.round((tensor - min_value) * 255 / (max_value - min_value)), tf.uint8\n        ),\n        min_key: min_value,\n        max_key: max_value,\n    }\n\n", "CodeBleu_score": 53.188496673655216}, {"code": "def from_float32_to_uint8(\n    tensor: tf.Tensor,\n    tensor_key: str = \"tensor\",\n    min_key: str = \"min\",\n    max_key: str = \"max\",\n) -> tf.Tensor\n\t\n    tensor_dtype = tensor.dtype\n    if tensor_dtype != tf.float32:\n        raise ValueError(\n            f\"Expected a tf.float32 tensor, but got a {tensor_dtype} tensor.\"\n        )\n    min_value = tf.reduce_min(tensor)\n    max_value = tf.reduce_max(tensor)\n    tensor = tf.divide(tf.subtract(tensor, min_value), tf.subtract(max_value, min_value))\n    tensor = tf.multiply(tensor, 255)\n    tensor = tf.cast(tensor, tf.uint8)\n    return {\n        tensor_key: tensor,\n        min_key: min_value,\n        max_key: max_value,\n    }\n\n", "CodeBleu_score": 48.79712366735497}]}
{"instance_id": "7f479e5de492", "generation_result": [{"code": "def pad_and_partition(tensor: tf.Tensor, segment_len: int) -> tf.Tensor\n\t\n    # calculate the amount of padding needed\n    padding = segment_len - (tf.shape(tensor)[0] % segment_len)\n    # pad the tensor\n    padded_tensor = tf.pad(tensor, ((0, padding), (0, 0)))\n    # reshape the tensor\n    reshaped_tensor = tf.reshape(padded_tensor, (-1, segment_len))\n    return reshaped_tensor\n\n", "CodeBleu_score": 21.205714110317814}, {"code": "def pad_and_partition(tensor: tf.Tensor, segment_len: int) -> tf.Tensor\n\t\n    if len(tensor.shape) != 1:\n        raise ValueError(\"The tensor must have rank 1.\")\n    if segment_len <= 0:\n        raise ValueError(\"The segment length must be positive.\")\n    if tensor.shape[0] % segment_len != 0:\n        pad_len = segment_len - (tensor.shape[0] % segment_len)\n        padded_tensor = tf.pad(tensor, [[0, pad_len]])\n    else:\n        padded_tensor = tensor\n    return tf.reshape(padded_tensor, [-1, segment_len])\n\n", "CodeBleu_score": 30.758440441807995}, {"code": "def pad_and_partition(tensor: tf.Tensor, segment_len: int) -> tf.Tensor\n\t\n    padded_len = tf.math.ceil(tf.math.divide(tf.shape(tensor)[0], segment_len)) * segment_len\n    padded_tensor = tf.pad(tensor, [[0, padded_len - tf.shape(tensor)[0]]])\n    return tf.reshape(padded_tensor, [padded_len // segment_len, segment_len])\n\n", "CodeBleu_score": 28.36483338836311}]}
{"instance_id": "1789fae7253f", "generation_result": [{"code": "def pad_and_reshape(instr_spec, frame_length, F) -> Any\n\t\n    # pad to frame_length\n    pad_size = frame_length - instr_spec.shape[-1]\n    if pad_size > 0:\n        pad = torch.zeros(instr_spec.shape[0], instr_spec.shape[1], pad_size)\n        instr_spec = torch.cat((instr_spec, pad), dim=2)\n    # reshape\n    instr_spec = instr_spec.reshape(instr_spec.shape[0] * instr_spec.shape[1], instr_spec.shape[2])\n    return instr_spec\n\n", "CodeBleu_score": 22.902078013437144}, {"code": "def pad_and_reshape(instr_spec, frame_length, F) -> Any\n\t\n    # calculate the number of rows to pad\n    num_rows_to_pad = frame_length - instr_spec.shape[1]\n    # create an extension of zeros\n    padding = np.zeros((instr_spec.shape[0], num_rows_to_pad, instr_spec.shape[2]))\n    # concatenate the extension to the original spectrogram along the frequency axis\n    instr_spec_padded = np.concatenate((instr_spec, padding), axis=1)\n    # reshape the extended spectrogram by merging the first two dimensions\n    instr_spec_padded_reshaped = instr_spec_padded.reshape(\n        (instr_spec_padded.shape[0] * instr_spec_padded.shape[1], instr_spec_padded.shape[2])\n    )\n    return instr_spec_padded_reshaped\n\n", "CodeBleu_score": 18.013973147515973}, {"code": "def pad_and_reshape(instr_spec, frame_length, F) -> Any\n\t\n    # Calculate the number of extra rows needed for padding\n    num_rows_to_pad = frame_length - (instr_spec.shape[0] % frame_length)\n    # Create an extension of zeros\n    padding_extension = np.zeros((num_rows_to_pad, instr_spec.shape[1]))\n    # Concatenate the extension to the original spectrogram along the frequency axis\n    instr_spec_padded = np.concatenate((instr_spec, padding_extension), axis=0)\n    # Reshape the extended spectrogram by merging the first two dimensions\n    instr_spec_padded_reshaped = np.reshape(instr_spec_padded, (instr_spec_padded.shape[0] * instr_spec_padded.shape[1], 1))\n    return instr_spec_padded_reshaped\n\n", "CodeBleu_score": 17.804924315095576}]}
{"instance_id": "8a4f4658d97c", "generation_result": [{"code": "def dataset_from_csv(csv_path: str, **kwargs) -> Any\n\t\n    import pandas as pd\n    import tensorflow as tf\n\n    df = pd.read_csv(csv_path)\n    return tf.data.Dataset.from_tensor_slices(df.values, **kwargs)\n\n", "CodeBleu_score": 29.103141597989012}, {"code": "def dataset_from_csv(csv_path: str, **kwargs) -> Any\n\t\n    import pandas as pd\n\n    df = pd.read_csv(csv_path, **kwargs)\n    return tf.data.Dataset.from_tensor_slices(tuple(df.values.T))\n\n", "CodeBleu_score": 33.50626321719972}, {"code": "def dataset_from_csv(csv_path: str, **kwargs) -> Any\n\t\n\n    import pandas as pd\n    import tensorflow as tf\n\n    df = pd.read_csv(csv_path)\n\n    dataset = tf.data.Dataset.from_tensor_slices(\n        (\n            df[\"image\"].values,\n            df[\"label\"].values,\n        )\n    )\n\n    return dataset\n\n", "CodeBleu_score": 22.666283625247107}]}
{"instance_id": "9a16e81b182f", "generation_result": [{"code": "def check_tensor_shape(tensor_tf: tf.Tensor, target_shape: Any) -> bool\n\t\n    result = True\n    for i in range(len(target_shape)):\n        result = result and (target_shape[i] == tensor_tf.shape[i])\n    return result\n\n", "CodeBleu_score": 26.81887297268905}, {"code": "def check_tensor_shape(tensor_tf: tf.Tensor, target_shape: Any) -> bool\n\t\n    result = True\n    for i, dim in enumerate(target_shape):\n        if dim is not None:\n            result = result and (tensor_tf.shape[i] == dim)\n    return result\n\n", "CodeBleu_score": 30.692801589977275}, {"code": "def check_tensor_shape(tensor_tf: tf.Tensor, target_shape: Any) -> bool\n\t\n    result = True\n    for i, dim in enumerate(target_shape):\n        if dim is not None:\n            result = result and (tensor_tf.shape[i] == dim)\n    return result\n\n", "CodeBleu_score": 30.692801589977275}]}
{"instance_id": "49d5d3084b3a", "generation_result": [{"code": "def get_training_dataset(\n    audio_params: Dict, audio_adapter: AudioAdapter, audio_path: str\n) -> Any\n\t\n    builder = DatasetBuilder(\n        audio_adapter=audio_adapter,\n        audio_params=audio_params,\n        chunk_duration=audio_params[\"duration\"],\n        random_seed=audio_params[\"seed\"],\n    )\n    return builder.build(\n        train_csv=audio_path,\n        cache_dir=audio_params[\"cache_dir\"],\n        batch_size=audio_params[\"batch_size\"],\n        num_chunks_per_song=audio_params[\"num_chunks_per_song\"],\n        augment=audio_params[\"augment\"],\n        cache=audio_params[\"cache\"],\n    )\n\n", "CodeBleu_score": 33.141632851432256}, {"code": "def get_training_dataset(\n    audio_params: Dict, audio_adapter: AudioAdapter, audio_path: str\n) -> Any\n\t\n    builder = DatasetBuilder(\n        audio_adapter=audio_adapter,\n        audio_params=audio_params,\n        chunk_duration=audio_params[\"chunk_duration\"],\n        random_seed=audio_params[\"random_seed\"],\n    )\n    return builder.build(\n        train_csv=audio_params[\"train_csv_path\"],\n        cache_dir=audio_params[\"cache_dir\"],\n        batch_size=audio_params[\"batch_size\"],\n        num_chunks_per_song=audio_params[\"num_chunks_per_song\"],\n        augment=audio_params[\"augment\"],\n        cache=audio_params[\"cache\"],\n    )\n\n", "CodeBleu_score": 34.00734758102686}, {"code": "def get_training_dataset(\n    audio_params: Dict, audio_adapter: AudioAdapter, audio_path: str\n) -> Any\n\t\n    dataset_builder = DatasetBuilder(\n        audio_adapter=audio_adapter,\n        audio_params=audio_params,\n        chunk_duration=audio_params[\"duration\"],\n        random_seed=audio_params[\"seed\"],\n    )\n    dataset = dataset_builder.build(\n        train_csv=audio_path,\n        cache_dir=audio_params[\"cache_dir\"],\n        batch_size=audio_params[\"batch_size\"],\n        num_chunks_per_song=audio_params[\"num_chunks_per_song\"],\n        augment=audio_params[\"augment\"],\n        cache=audio_params[\"cache\"],\n    )\n    return dataset\n\n", "CodeBleu_score": 33.436614542674}]}
{"instance_id": "fe8bbbba06e3", "generation_result": [{"code": "def instruments(self) -> Any\n\t\n    if not hasattr(self, \"_instruments\"):\n        self._instruments = {}\n    if not hasattr(self, \"_instrument_builders\"):\n        self._instrument_builders = {}\n    for instrument in self.instruments_list:\n        if instrument not in self._instruments:\n            self._instruments[instrument] = self.dataset_builder(\n                data_frequency=self.data_frequency,\n                path=self.path,\n                calendar=self.calendar,\n                start_session=self.start_session,\n                end_session=self.end_session,\n                create_writers=False,\n            )\n            self._instrument_builders[instrument] = self._instruments[instrument].get_dataset_writer()\n        yield self._instrument_builders[instrument]\n\n", "CodeBleu_score": 26.49721205327541}, {"code": "def instruments(self) -> Any\n\t\n    # Create the instrument dataset builders if they don't already exist.\n    if not hasattr(self, \"_instruments\"):\n        self._instruments: Dict[str, InstrumentDatasetBuilder] = {}\n        for instrument in self._instruments_list:\n            self._instruments[instrument] = InstrumentDatasetBuilder(\n                self, instrument\n            )\n\n    # Yield each instrument dataset builder.\n    for instrument in self._instruments_list:\n        yield self._instruments[instrument]\n\n", "CodeBleu_score": 37.03040159532979}, {"code": "def instruments(self) -> Any\n\t\n    # Check if instruments are already built\n    if self.instruments_built:\n        return\n\n    # Initialize instrument dataset builders\n    instrument_dataset_builders = []\n    for instrument in self.instruments:\n        instrument_dataset_builder = InstrumentDatasetBuilder(instrument, self.exchange, self.symbol, self.timeframe)\n        instrument_dataset_builders.append(instrument_dataset_builder)\n\n    # Build instruments\n    for instrument_dataset_builder in instrument_dataset_builders:\n        instrument_dataset_builder.build()\n\n    # Set instruments built flag\n    self.instruments_built = True\n\n    # Yield instrument dataset builders\n    for instrument_dataset_builder in instrument_dataset_builders:\n        yield instrument_dataset_builder\n\n", "CodeBleu_score": 35.303408614657876}]}
{"instance_id": "438b57fd3413", "generation_result": [{"code": "def cache(self, dataset: Any, cache: Optional[str], wait: bool) -> Any\n\t\n    if cache is not None:\n        if not wait:\n            self.cache_dataset(dataset, cache)\n        else:\n            while not exists(cache):\n                self.log.info(\n                    f\"Cache file {cache} not found. Waiting for {self.cache_wait} seconds.\"\n                )\n                time.sleep(self.cache_wait)\n            self.cache_dataset(dataset, cache)\n    return dataset\n\n", "CodeBleu_score": 50.12160370429228}, {"code": "def cache(self, dataset: Any, cache: Optional[str], wait: bool) -> Any\n\t\n\n    if cache:\n        if wait:\n            while not exists(cache):\n                time.sleep(10)\n                print(\"Waiting for cache index file to exist...\")\n\n        cache_dir = SEPARATOR.join(cache.split(SEPARATOR)[:-1])\n        if not exists(cache_dir):\n            os.makedirs(cache_dir)\n\n        with open(cache, \"wb\") as f:\n            pickle.dump(dataset, f)\n\n        print(f\"Cached dataset to {cache}\")\n\n    return dataset\n\n", "CodeBleu_score": 48.18461206437354}, {"code": "def cache(self, dataset: Any, cache: Optional[str], wait: bool) -> Any\n\t\n    if cache:\n        cache_dir = os.path.dirname(cache)\n        if not os.path.exists(cache_dir):\n            os.makedirs(cache_dir)\n        if wait:\n            while not exists(cache):\n                print(\n                    f\"Waiting for cache file to be created at {cache}... (this may take a few seconds)\"\n                )\n                time.sleep(5)\n        if not exists(cache):\n            print(f\"Caching dataset to {cache}...\")\n            dataset.save_to_disk(cache)\n        else:\n            print(f\"Loading cached dataset from {cache}...\")\n            dataset = dataset.load_from_disk(cache)\n    return dataset\n\n", "CodeBleu_score": 40.021825782328726}]}
{"instance_id": "8838a3a09a66", "generation_result": [{"code": "def create_estimator(params: Dict, MWF: bool) -> tf.Tensor\n\t\n\n    # Load the model directory\n    model_dir = params[\"model_dir\"]\n    model_provider = ModelProvider(model_dir)\n\n    # Set up the session configuration\n    session_config = tf.ConfigProto()\n    session_config.gpu_options.allow_growth = True\n    session_config.gpu_options.per_process_gpu_memory_fraction = 0.4\n\n    # Set up the RunConfig\n    run_config = tf.estimator.RunConfig(\n        session_config=session_config,\n        save_summary_steps=100,\n        save_checkpoints_steps=1000,\n        keep_checkpoint_max=1000,\n    )\n\n    # Initialize the estimator\n    estimator = tf.estimator.Estimator(\n        model_fn=model_provider.get_model_fn(MWF),\n        model_dir=model_dir,\n        params=params,\n        config=run_config,\n    )\n\n    return estimator\n\n", "CodeBleu_score": 29.479307455309893}, {"code": "def create_estimator(params: Dict, MWF: bool) -> tf.Tensor\n\t\n\n    # Get the model directory\n    model_dir = params.get(\"model_dir\")\n\n    # Get the model provider\n    model_provider = params.get(\"model_provider\")\n\n    # Get the model function\n    model_fn = params.get(\"model_fn\")\n\n    # Get the model parameters\n    model_params = params.get(\"model_params\")\n\n    # Get the model configuration\n    model_config = params.get(\"model_config\")\n\n    # Get the model hooks\n    model_hooks = params.get(\"model_hooks\")\n\n    # Get the model warm start\n    model_warm_start = params.get(\"model_warm_start\")\n\n    # Get the model warm start settings\n    model_warm_start_settings = params.get(\"model_warm_start_settings\")\n\n    # Get the model warm start from\n    model_warm_start_from = params.get(\"model_warm_start_from\")\n\n    # Get the model warm start from settings\n    model_warm_start_from_settings = params.get(\"model_warm_start_from_settings\")\n\n    # Get the model warm start from checkpoint\n    model_warm_start_from_checkpoint = params.get(\"model_warm_start_from_checkpoint\")\n\n    # Get the model warm start from checkpoint settings\n    model_warm_start_from_checkpoint_settings = params.get(\n        \"model_warm_start_from_checkpoint_settings\"\n    )\n\n    # Get the model warm start from checkpoint name\n    model_warm_start_from_checkpoint_name = params.get(\n        \"model_warm_start_from_checkpoint_name\"\n    )\n\n    # Get the model warm start from checkpoint name settings\n    model_warm_start_from_checkpoint_name_settings = params.get(\n        \"model_warm_start_from_checkpoint_name_settings\"\n    )\n\n    # Get the model warm start from checkpoint global step\n    model_warm_start_from_checkpoint_global_step = params.get(\n        \"model_warm_start_from_checkpoint_global_step\"\n    )\n\n    # Get the model warm start from checkpoint global step settings\n    model_warm_start_from_checkpoint_global_step_settings = params.get(\n        \"model_warm_start_from_checkpoint_global_step_settings\"\n    )\n\n    # Get the model warm start from checkpoint all\n    model_", "CodeBleu_score": 14.753070787754865}, {"code": "def create_estimator(params: Dict, MWF: bool) -> tf.Tensor\n\t\n    model_provider = ModelProvider(params)\n    model_provider.load_model_dir(MWF)\n    params = model_provider.update_params(params)\n    session_config = tf.ConfigProto()\n    session_config.gpu_options.allow_growth = True\n    run_config = tf.estimator.RunConfig(session_config=session_config)\n    estimator = tf.estimator.Estimator(model_fn=params[\"model_fn\"], model_dir=params[\"model_dir\"], params=params, config=run_config)\n    return estimator\n\n", "CodeBleu_score": 33.07180754387496}]}
{"instance_id": "779c812bf2cb", "generation_result": [{"code": "def _separate_tensorflow(\n        self, waveform: np.ndarray, audio_descriptor: AudioDescriptor\n    ) -> Dict\n\t\n\n    # Ensure the waveform is stereo\n    if waveform.ndim == 1:\n        waveform = np.expand_dims(waveform, axis=0)\n\n    # Obtain a prediction generator\n    prediction_generator = self.predict(\n        data_generator=self._data_generator.update(\n            waveform=waveform, audio_descriptor=audio_descriptor\n        )\n    )\n\n    # Generate a prediction\n    prediction = next(prediction_generator)\n\n    # Remove the \"audio_id\" key from the prediction\n    prediction.pop(\"audio_id\")\n\n    return prediction\n\n", "CodeBleu_score": 60.863700099826865}, {"code": "def _separate_tensorflow(\n        self, waveform: np.ndarray, audio_descriptor: AudioDescriptor\n    ) -> Dict\n\t\n    if len(waveform.shape) > 1 and waveform.shape[1] == 1:\n        waveform = np.repeat(waveform, 2, axis=1)\n    predictions = self.predict(x=waveform, audio_descriptor=audio_descriptor)\n    predictions.pop(\"audio_id\")\n    return predictions\n\n", "CodeBleu_score": 41.242493869391915}, {"code": "def _separate_tensorflow(\n        self, waveform: np.ndarray, audio_descriptor: AudioDescriptor\n    ) -> Dict\n\t\n    if len(waveform.shape) == 1:\n        waveform = np.expand_dims(waveform, axis=0)\n    if waveform.shape[1] != 2:\n        waveform = np.transpose(waveform)\n    self.data_generator.update_data(waveform, audio_descriptor)\n    predictions = next(self.prediction_generator)\n    predictions.pop(\"audio_id\", None)\n    return predictions\n\n", "CodeBleu_score": 48.44544988077054}]}
{"instance_id": "cf580cd2ed93", "generation_result": [{"code": "def compute_backbone_shapes(config, image_shape)\n\t\n    if callable(config.BACKBONE):\n        return config.COMPUTE_BACKBONE_SHAPE(image_shape)\n\n    # Currently supports ResNet only\n    assert config.BACKBONE in [\"resnet50\", \"resnet101\"]\n    return np.array(\n        [[int(math.ceil(image_shape[0] / stride)),\n            int(math.ceil(image_shape[1] / stride))]\n            for stride in config.BACKBONE_STRIDES])\n\n", "CodeBleu_score": 95.198357853076}, {"code": "def compute_backbone_shapes(config, image_shape)\n\t\n    if callable(config.BACKBONE):\n        return config.COMPUTE_BACKBONE_SHAPE(image_shape)\n    # Currently supports ResNet only\n    assert config.BACKBONE in [\"resnet50\", \"resnet101\"]\n    return np.array(\n        [[int(math.ceil(image_shape[0] / stride)), int(math.ceil(image_shape[1] / stride))] for stride in config.BACKBONE_STRIDES])\n\n", "CodeBleu_score": 93.05088425331716}, {"code": "def compute_backbone_shapes(config, image_shape)\n\t\n    assert config.BACKBONE in [\"resnet50\", \"resnet101\"]\n    if callable(config.BACKBONE):\n        return config.COMPUTE_BACKBONE_SHAPE(image_shape)\n    return np.array(\n        [[int(math.ceil(image_shape[0] / stride)), int(math.ceil(image_shape[1] / stride))] for stride in config.BACKBONE_STRIDES])\n\n", "CodeBleu_score": 87.1434728739045}]}
{"instance_id": "ebd61a2e59ca", "generation_result": [{"code": "def clip_boxes_graph(boxes, window)\n\t\n    # Split\n    wy1, wx1, wy2, wx2 = tf.split(window, 4)\n    y1, x1, y2, x2 = tf.split(boxes, 4, axis=2)\n\n    # Clip\n    y1 = tf.maximum(tf.minimum(y1, wy2), wy1)\n    x1 = tf.maximum(tf.minimum(x1, wx2), wx1)\n    y2 = tf.maximum(tf.minimum(y2, wy2), wy1)\n    x2 = tf.maximum(tf.minimum(x2, wx2), wx1)\n    clipped = tf.concat([y1, x1, y2, x2], axis=2, name=\"clipped_boxes\")\n\n    clipped.set_shape((clipped.shape[0], clipped.shape[1], 4))\n    return clipped\n\n", "CodeBleu_score": 55.60362680082691}, {"code": "def clip_boxes_graph(boxes, window)\n\t\n    # Split\n    wy1, wx1, wy2, wx2 = tf.split(window, 4)\n    y1, x1, y2, x2 = tf.split(boxes, 4, axis=2)\n    # Clip\n    y1 = tf.maximum(tf.minimum(y1, wy2), wy1)\n    x1 = tf.maximum(tf.minimum(x1, wx2), wx1)\n    y2 = tf.maximum(tf.minimum(y2, wy2), wy1)\n    x2 = tf.maximum(tf.minimum(x2, wx2), wx1)\n    clipped = tf.concat([y1, x1, y2, x2], axis=2)\n    clipped.set_shape((clipped.shape[0], clipped.shape[1], 4))\n    return clipped\n\n", "CodeBleu_score": 54.67274953842913}, {"code": "def clip_boxes_graph(boxes, window)\n\t\n    # Split\n    wy1, wx1, wy2, wx2 = tf.split(window, 4)\n    y1, x1, y2, x2 = tf.split(boxes, 4, axis=2)\n    # Clip\n    y1 = tf.maximum(tf.minimum(y1, wy2), wy1)\n    x1 = tf.maximum(tf.minimum(x1, wx2), wx1)\n    y2 = tf.maximum(tf.minimum(y2, wy2), wy1)\n    x2 = tf.maximum(tf.minimum(x2, wx2), wx1)\n    clipped = tf.concat([y1, x1, y2, x2], axis=2, name=\"clipped_boxes\")\n    clipped.set_shape((clipped.shape[0], clipped.shape[1], 4))\n    return clipped\n\n", "CodeBleu_score": 55.60362680082691}]}
{"instance_id": "8e0411587a70", "generation_result": [{"code": "def rpn_graph(feature_map, anchors_per_location, anchor_stride)\n\t\n    # Shared convolutional base of the RPN\n    shared = keras.layers.Conv2D(512, (3, 3), padding='same', activation='relu', strides=anchor_stride,\n                                name='rpn_conv_shared')(feature_map)\n\n    # Anchor Score. [batch, height, width, anchors per location * 2].\n    x = keras.layers.Conv2D(2 * anchors_per_location, (1, 1), padding='valid', activation='linear', name='rpn_class_raw')(\n        shared)\n\n    # Reshape to [batch, anchors, 2]\n    rpn_class_logits = keras.layers.Lambda(\n        lambda t: tf.reshape(t, [tf.shape(t)[0], -1, 2]))(x)\n\n    # Softmax on last dimension of BG/FG.\n    rpn_probs = keras.layers.Activation(\n        \"softmax\", name=\"rpn_class_xxx\")(rpn_class_logits)\n\n    # Bounding box refinement. [batch, H, W, anchors per location * depth]\n    # where depth is [x, y, log(w), log(h)]\n    x = keras.layers.Conv2D(anchors_per_location * 4, (1, 1), padding=\"valid\", activation='linear',\n                           name='rpn_bbox_pred')(shared)\n\n    # Reshape to [batch, anchors, 4]\n    rpn_bbox = keras.layers.Lambda(lambda t: tf.reshape(t, [tf.shape(t)[0], -1, 4]))(x)\n\n    return [rpn_class_logits, rpn_probs, rpn_bbox]\n\n", "CodeBleu_score": 70.29693890983083}, {"code": "def rpn_graph(feature_map, anchors_per_location, anchor_stride)\n\t\n    shared = keras.layers.Conv2D(512, (3, 3), padding='same', activation='relu', strides=anchor_stride,\n                                name='rpn_conv_shared')(feature_map)\n    x = keras.layers.Conv2D(256, (1, 1), padding='valid', activation='relu', name='rpn_class_raw')(shared)\n    x_class = keras.layers.Conv2D(anchors_per_location * 2, (1, 1), padding='valid', activation='linear',\n                                 name='rpn_class_xxx')(x)\n    x_regr = keras.layers.Conv2D(anchors_per_location * 4, (1, 1), padding='valid', activation='linear',\n                                name='rpn_bbox_pred')(shared)\n    return [x_class, x_regr, shared]\n\n", "CodeBleu_score": 42.902445734912995}, {"code": "def rpn_graph(feature_map, anchors_per_location, anchor_stride)\n\t\n    shared = keras.layers.Conv2D(512, (3, 3), padding='same', activation='relu', strides=anchor_stride,\n                                 name='rpn_conv_shared')(feature_map)\n    x = keras.layers.Conv2D(256, (1, 1), padding='valid', activation='relu', name='rpn_class_raw')(shared)\n    x_class = keras.layers.Conv2D(anchors_per_location * 2, (1, 1), padding='valid', strides=anchor_stride,\n                                  activation='linear', name='rpn_class_xxx')(x)\n    x_regr = keras.layers.Conv2D(anchors_per_location * 4, (1, 1), padding='valid', strides=anchor_stride,\n                                 activation='linear', name='rpn_bbox_pred')(shared)\n    return x_class, x_regr\n\n", "CodeBleu_score": 42.42640204307519}]}
{"instance_id": "70777c9261d8", "generation_result": [{"code": "def smooth_l1_loss(y_true, y_pred)\n\t\n    abs_loss = tf.abs(y_true - y_pred)\n    sqr_loss = 0.5 * (y_true - y_pred)**2\n    l1_loss = tf.where(tf.less(abs_loss, 1.0), sqr_loss, abs_loss - 0.5)\n    return tf.reduce_sum(l1_loss)\n\n", "CodeBleu_score": 31.180515322262664}, {"code": "def smooth_l1_loss(y_true, y_pred)\n\t\n    abs_loss = K.abs(y_true - y_pred)\n    sq_loss = 0.5 * (y_true - y_pred)**2\n    l1_loss = tf.where(K.less(abs_loss, 1.0), sq_loss, abs_loss - 0.5)\n    return K.sum(l1_loss, axis=-1)\n\n", "CodeBleu_score": 34.77161987881745}, {"code": "def smooth_l1_loss(y_true, y_pred)\n\t\n    abs_loss = keras.backend.abs(y_true - y_pred)\n    sq_loss = 0.5 * (y_true - y_pred)**2\n    l1_loss = tf.where(keras.backend.less(abs_loss, 1.0), sq_loss, abs_loss - 0.5)\n    return keras.backend.sum(l1_loss, axis=-1)\n\n", "CodeBleu_score": 30.433561802374637}]}
{"instance_id": "0e747b604087", "generation_result": [{"code": "def build_model(x_train, num_classes)\n\t\n    tf.reset_default_graph()\n    x = tf.placeholder(tf.float32, (None, x_train.shape[1], x_train.shape[2], 1))\n    y = tf.placeholder(tf.int32, (None))\n    keep_prob = tf.placeholder(tf.float32)\n\n    # Convolutional Layer #1\n    conv1 = tf.layers.conv2d(\n        inputs=x,\n        filters=16,\n        kernel_size=[5, 5],\n        padding=\"same\",\n        activation=tf.nn.relu)\n\n    # Pooling Layer #1\n    pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\n\n    # Convolutional Layer #2 and Pooling Layer #2\n    conv2 = tf.layers.conv2d(\n        inputs=pool1,\n        filters=32,\n        kernel_size=[5, 5],\n        padding=\"same\",\n        activation=tf.nn.relu)\n    pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n\n    # Flatten tensor into a batch of vectors\n    pool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 32])\n\n    # Dense Layer\n    dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)\n    dropout = tf.layers.dropout(\n        inputs=dense, rate=keep_prob, training=True)\n\n    # Logits layer\n    logits = tf.layers.dense(inputs=dropout, units=num_classes)\n\n    # Define loss and optimizer\n    loss_op = tf.reduce_mean(\n        tf.nn.sparse_softmax_cross_entropy_with_logits(\n            logits=logits, labels=y))\n    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n    train_op = optimizer.minimize(loss_op)\n\n    # Evaluate model (with test logits, for dropout to be disabled)\n    correct_pred = tf.nn.in_top_k(logits, y, 1)\n    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.", "CodeBleu_score": 22.72708209055121}, {"code": "def build_model(x_train, num_classes)\n\t\n    tf.reset_default_graph()\n    x = tf.placeholder(tf.float32, shape=[None, x_train.shape[1], x_train.shape[2], x_train.shape[3]])\n    y = tf.placeholder(tf.float32, shape=[None, num_classes])\n\n    # Convolutional layer 1\n    conv1 = tf.layers.conv2d(x, filters=16, kernel_size=5, padding='same', activation=tf.nn.relu)\n    # Max-pooling layer 1\n    pool1 = tf.layers.max_pooling2d(conv1, pool_size=2, strides=2)\n    # Convolutional layer 2\n    conv2 = tf.layers.conv2d(pool1, filters=32, kernel_size=5, padding='same', activation=tf.nn.relu)\n    # Max-pooling layer 2\n    pool2 = tf.layers.max_pooling2d(conv2, pool_size=2, strides=2)\n    # Flatten layer\n    pool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 32])\n    # Dense layer 1\n    dense1 = tf.layers.dense(pool2_flat, 1024, activation=tf.nn.relu)\n    # Dense layer 2\n    logits = tf.layers.dense(dense1, num_classes)\n    # Probability distribution\n    y_pred = tf.nn.softmax(logits)\n    # Loss function\n    cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(labels=y, logits=logits)\n    loss = tf.reduce_mean(cross_entropy)\n    # Optimizer\n    optimizer = tf.train.AdamOptimizer()\n    train_step = optimizer.minimize(loss)\n    # Accuracy metric\n    correct_prediction = tf.equal(tf.argmax(y_pred, 1), tf.argmax(y, 1))\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n    # Build model\n    model = Model(x, y, y_pred, accuracy, loss, train_step)\n    return model\n\n", "CodeBleu_score": 22.77379693855672}, {"code": "def build_model(x_train, num_classes)\n\t\n\n    # Reset the TensorFlow graph\n    tf.reset_default_graph()\n\n    # Start the computational graph session\n    sess = tf.Session()\n\n    # Define the input layer\n    x = tf.placeholder(tf.float32, shape=(None, x_train.shape[1], x_train.shape[2], x_train.shape[3]), name='input')\n\n    # Define the model\n    model = tf.keras.models.Sequential()\n    model.add(tf.keras.layers.Conv2D(filters=16, kernel_size=3, strides=1, padding='same', activation='relu', input_shape=x_train.shape[1:], name='conv_1'))\n    model.add(tf.keras.layers.MaxPooling2D(pool_size=2, strides=2, name='max_pool_1'))\n    model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, strides=1, padding='same', activation='relu', name='conv_2'))\n    model.add(tf.keras.layers.MaxPooling2D(pool_size=2, strides=2, name='max_pool_2'))\n    model.add(tf.keras.layers.Flatten(name='flatten'))\n    model.add(tf.keras.layers.Dense(units=128, activation='relu', name='dense_1'))\n    model.add(tf.keras.layers.Dense(units=num_classes, activation='softmax', name='dense_2'))\n\n    # Compile the model\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n    return model, x, sess\n\n", "CodeBleu_score": 27.976101761944115}]}
{"instance_id": "813e3e2844f4", "generation_result": [{"code": "def extract_bboxes(mask)\n\t\n    boxes = []\n    for i in range(mask.shape[-1]):\n        m = mask[:, :, i]\n        # Bounding box.\n        horizontal_indicies = np.where(np.any(m, axis=0))[0]\n        vertical_indicies = np.where(np.any(m, axis=1))[0]\n        if horizontal_indicies.shape[0]:\n            x1, x2 = horizontal_indicies[[0, -1]]\n            y1, y2 = vertical_indicies[[0, -1]]\n            # x2 and y2 should not be part of the box. Increment by 1.\n            x2 += 1\n            y2 += 1\n        else:\n            # No mask for this layer, therefore set the bbox to zeros\n            x1, x2, y1, y2 = 0, 0, 0, 0\n        boxes.append([x1, x2, y1, y2])\n    return boxes\n\n", "CodeBleu_score": 67.22627968806758}, {"code": "def extract_bboxes(mask)\n\t\n    boxes = []\n    for mask_layer in mask:\n        m = mask_layer > 0\n        # Bounding box.\n        horizontal_indicies = np.where(np.any(m, axis=0))[0]\n        vertical_indicies = np.where(np.any(m, axis=1))[0]\n        if horizontal_indicies.shape[0]:\n            x1, x2 = horizontal_indicies[[0, -1]]\n            y1, y2 = vertical_indicies[[0, -1]]\n            # x2 and y2 should not be part of the box. Increment by 1.\n            x2 += 1\n            y2 += 1\n        else:\n            # No mask for this instance. Might happen due to\n            # resizing or cropping. Set bbox to zeros\n            x1, x2, y1, y2 = 0, 0, 0, 0\n        boxes.append([x1, x2, y1, y2])\n    return boxes\n\n", "CodeBleu_score": 59.619736035145735}, {"code": "def extract_bboxes(mask)\n\t\n    boxes = []\n    for m in mask:\n        # Bounding box.\n        horizontal_indicies = np.where(np.any(m, axis=0))[0]\n        vertical_indicies = np.where(np.any(m, axis=1))[0]\n        if horizontal_indicies.shape[0]:\n            x1, x2 = horizontal_indicies[[0, -1]]\n            y1, y2 = vertical_indicies[[0, -1]]\n            # x2 and y2 should not be part of the box. Increment by 1.\n            x2 += 1\n            y2 += 1\n        else:\n            # No mask for this instance. Might happen due to\n            # resizing or cropping. Set bbox to zeros\n            x1, x2, y1, y2 = 0, 0, 0, 0\n        boxes.append([x1, y1, x2, y2])\n    return boxes\n\n", "CodeBleu_score": 58.2804866350806}]}
{"instance_id": "7ad5d430d6ed", "generation_result": [{"code": "def compute_iou(box, boxes, box_area, boxes_area)\n\t\n    # Calculate intersection areas\n    y1 = np.maximum(box[0], boxes[:, 0])\n    y2 = np.minimum(box[2], boxes[:, 2])\n    x1 = np.maximum(box[1], boxes[:, 1])\n    x2 = np.minimum(box[3], boxes[:, 3])\n    intersection = np.maximum(x2 - x1, 0) * np.maximum(y2 - y1, 0)\n    union = box_area + boxes_area[:] - intersection[:]\n    iou = intersection / union\n    return iou\n\n", "CodeBleu_score": 58.663259083089194}, {"code": "def compute_iou(box, boxes, box_area, boxes_area)\n\t\n    # Calculate intersection areas\n    y1 = np.maximum(box[0], boxes[:, 0])\n    y2 = np.minimum(box[2], boxes[:, 2])\n    x1 = np.maximum(box[1], boxes[:, 1])\n    x2 = np.minimum(box[3], boxes[:, 3])\n    intersection = np.maximum(x2 - x1, 0) * np.maximum(y2 - y1, 0)\n    union = box_area + boxes_area[:] - intersection[:]\n    iou = intersection / union\n    return iou\n\n", "CodeBleu_score": 58.663259083089194}, {"code": "def compute_iou(box, boxes, box_area, boxes_area)\n\t\n    # Calculate intersection areas\n    y1 = np.maximum(box[0], boxes[:, 0])\n    y2 = np.minimum(box[2], boxes[:, 2])\n    x1 = np.maximum(box[1], boxes[:, 1])\n    x2 = np.minimum(box[3], boxes[:, 3])\n    intersection = np.maximum(x2 - x1, 0) * np.maximum(y2 - y1, 0)\n    union = box_area + boxes_area[:] - intersection[:]\n    iou = intersection / union\n    return iou\n\n", "CodeBleu_score": 58.663259083089194}]}
{"instance_id": "15941b852859", "generation_result": [{"code": "def compute_overlaps(boxes1, boxes2)\n\t\n    # Areas of anchors and GT boxes\n    area1 = (boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1])\n    area2 = (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1])\n\n    # Compute overlaps to generate matrix [boxes1 count, boxes2 count]\n    # Each cell contains the IoU value.\n    overlaps = scipy.sparse.csr_matrix(\n        np.zeros((boxes1.shape[0], boxes2.shape[0]), dtype=np.float32))\n    for i in range(overlaps.shape[1]):\n        box2 = boxes2[i]\n        overlaps[:, i] = compute_iou(box2, boxes1, area2[i], area1)\n    return overlaps\n\n", "CodeBleu_score": 57.29713895839933}, {"code": "def compute_overlaps(boxes1, boxes2)\n\t\n    # Areas of anchors and GT boxes\n    area1 = (boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1])\n    area2 = (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1])\n\n    # Compute overlaps to generate matrix [boxes1 count, boxes2 count]\n    # Each cell contains the IoU value.\n    overlaps = scipy.sparse.csr_matrix(\n        np.zeros((boxes1.shape[0], boxes2.shape[0]))\n    )\n    for i in range(overlaps.shape[1]):\n        box2 = boxes2[i]\n        overlaps[:, i] = compute_iou(box2, boxes1, area2[i], area1)\n    return overlaps\n\n", "CodeBleu_score": 58.288889488546545}, {"code": "def compute_overlaps(boxes1, boxes2)\n\t\n    area1 = (boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1])\n    area2 = (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1])\n\n    lt = np.maximum(boxes1[:, None, :2], boxes2[:, :2])  # [N,M,2]\n    rb = np.minimum(boxes1[:, None, 2:], boxes2[:, 2:])  # [N,M,2]\n\n    wh = (rb - lt).clip(min=0)  # [N,M,2]\n    inter = wh[:, :, 0] * wh[:, :, 1]  # [N,M]\n\n    union = area1[:, None] + area2 - inter\n\n    return inter / union  # [N,M]\n\n", "CodeBleu_score": 45.11345402744934}]}
{"instance_id": "597533722ce3", "generation_result": [{"code": "def load_image(self, image_id):\n\t\n    # Load image\n    image = skimage.io.imread(self.image_info[image_id]['path'])\n    # If grayscale. Convert to RGB for consistency.\n    if image.ndim != 3:\n        image = skimage.color.gray2rgb(image)\n    # If has an alpha channel, remove it for consistency\n    if image.shape[-1] == 4:\n        image = image[..., :3]\n    return image\n\n", "CodeBleu_score": 100.0}, {"code": "def load_image(self, image_id):\n\t\n    # Load image\n    image = skimage.io.imread(self.image_info[image_id]['path'])\n    # If grayscale. Convert to RGB for consistency.\n    if image.ndim != 3:\n        image = skimage.color.gray2rgb(image)\n    # If has an alpha channel, remove it for consistency\n    if image.shape[-1] == 4:\n        image = image[..., :3]\n    return image\n", "CodeBleu_score": 100.0}, {"code": "def load_image(self, image_id):\n\t\n    # Load image\n    image = skimage.io.imread(self.image_info[image_id]['path'])\n    # If grayscale. Convert to RGB for consistency.\n    if image.ndim != 3:\n        image = skimage.color.gray2rgb(image)\n    # If has an alpha channel, remove it for consistency\n    if image.shape[-1] == 4:\n        image = image[..., :3]\n    return image\n\n", "CodeBleu_score": 100.0}]}
{"instance_id": "9b852997ca05", "generation_result": [{"code": "def display_images(images, titles=None, cols=4, cmap=None, norm=None,\n                   interpolation=None)\n\t\n    if titles is None:\n        titles = ['(%d)' % i for i in range(1, len(images) + 1)]\n    rows = len(images) // cols + 1\n    plt.figure(figsize=(14, 14 * rows // cols))\n    i = 1\n    for image, title in zip(images, titles):\n        plt.subplot(rows, cols, i)\n        plt.title(title, fontsize=9)\n        plt.axis('off')\n        plt.imshow(image, cmap=cmap,\n                   norm=norm, interpolation=interpolation)\n        i += 1\n    plt.show()\n\n", "CodeBleu_score": 74.69730524293347}, {"code": "def display_images(images, titles=None, cols=4, cmap=None, norm=None,\n                   interpolation=None)\n\t\n    # Set default titles if none are provided.\n    if titles is None:\n        titles = ['(%d)' % i for i in range(1, len(images) + 1)]\n    # Calculate the number of rows needed.\n    rows = len(images) // cols + 1\n    # Create a figure and set display parameters.\n    fig = plt.figure()\n    fig.set_size_inches(14, 14 * rows // cols)\n    if cmap is None:\n        cmap = 'gray'\n    if norm is None:\n        norm = mcolors.Normalize(vmin=0, vmax=255)\n    if interpolation is None:\n        interpolation = 'none'\n    # Display the images in a grid format.\n    for i, (image, title) in enumerate(zip(images, titles)):\n        a = fig.add_subplot(rows, cols, i + 1)\n        plt.imshow(image, cmap=cmap, norm=norm, interpolation=interpolation)\n        a.set_title(title)\n    # Display the figure.\n    plt.show()\n\n", "CodeBleu_score": 45.45764444367632}, {"code": "def display_images(images, titles=None, cols=4, cmap=None, norm=None,\n                   interpolation=None)\n\t\n    # Create a figure with a default title and the specified number of rows and columns.\n    # Set the default title if none is provided.\n    plt.figure()\n    # Set the number of rows and columns.\n    # Use the number of provided images and the number of columns to calculate the number of rows.\n    # Round up the number of rows to the nearest integer.\n    nrows = len(images) // cols + 1\n    # Create a grid of subplots.\n    # Use the number of rows and columns calculated earlier and the current index of the image to determine the position of the subplot.\n    # Set the title of the subplot.\n    # Show the image in the subplot.\n    # Set the color map, normalization, and interpolation.\n    for i in range(len(images)):\n        plt.subplot(nrows, cols, i + 1)\n        # Set the title.\n        if titles is not None:\n            plt.title(titles[i], fontsize=9)\n        # Remove ticks from the plot.\n        plt.axis('off')\n        plt.imshow(images[i], cmap=cmap, norm=norm, interpolation=interpolation)\n    # Display the figure.\n    plt.show()\n\n", "CodeBleu_score": 32.500178593506476}]}
