{"instance_id": "1f9f4d85e825", "generation_result": [{"code": "def load_model_from_config(self, half_attention)\n\t\n\n    # Check if the model has already been loaded and cached\n    if os.path.isfile(self.model_cache_path):\n        print(f\"Loading model from cache: {self.model_cache_path}\")\n        model = torch.load(self.model_cache_path)\n        model.eval()\n        return model\n\n    # Otherwise, load the model from the specified path\n    print(f\"Loading model from path: {self.model_path}\")\n    if self.model_path.endswith(\".safetensors\"):\n        model = safetensors.torch.load(self.model_path)\n    else:\n        model = torch.load(self.model_path)\n\n    # Configure the model using the settings in the YAML file\n    model = instantiate_from_config(self.model_config)\n    model.load_state_dict(model_dict)\n\n    # Move the model to the appropriate device\n    model = model.to(self.device)\n\n    # Convert the model to half-precision if specified\n    if half_attention:\n        model.convert_to_half_precision()\n\n    # Optimize the model for memory format\n    if self.memory_format == \"channels_last\":\n        model = model.to(memory_format=torch.channels_last)\n\n    # Ensure the model is in evaluation mode\n    model.eval()\n\n    # Cache the model for future use\n    torch.save(model, self.model_cache_path)\n\n    return model\n\n", "CodeBleu_score": 34.45301324272709}, {"code": "def load_model_from_config(self, half_attention)\n\t\n    # Load model from cache if available and valid\n    if self.model_path in self.cache:\n        model = self.cache[self.model_path]\n        if model is not None:\n            print(\"Using cached model.\")\n            return model\n    # Load model from file\n    print(\"Loading model from file.\")\n    if self.model_path.endswith(\".safetensors\"):\n        model = safetensors.torch.load(self.model_path, map_location=self.device)\n    else:\n        model = torch.load(self.model_path, map_location=self.device)\n    # Configure model\n    if half_attention:\n        model = model.half()\n    model.to(self.device)\n    model.eval()\n    # Cache model\n    self.cache[self.model_path] = model\n    return model\n\n", "CodeBleu_score": 29.338155832127345}, {"code": "def load_model_from_config(self, half_attention)\n\t\n\n    # Check if a cached model is available and valid\n    if os.path.isfile(self.cache_path):\n        print(f\"Loading model from cache: {self.cache_path}\")\n        model = torch.load(self.cache_path)\n        if self.validate_cache:\n            print(\"Validating cache...\")\n            model.eval()\n            sample = model.sample(self.sample_batch_size, self.sample_batch_size)\n            if torch.cuda.is_available():\n                torch.cuda.synchronize()\n            print(\"Cache validated!\")\n    # Otherwise, load the model from the specified path\n    else:\n        print(f\"Loading model from path: {self.model_path}\")\n        # Load the model\n        model = torch.load(self.model_path)\n        # If the model is in a safetensors file, load it\n        if isinstance(model, safetensors.torch.SafeTensor):\n            model = model.to_pytorch()\n        # Otherwise, load it as a PyTorch model\n        else:\n            model = model.eval()\n        # Move the model to the appropriate device\n        model = model.to(self.device)\n        # If half-precision attention is enabled, convert the model to half-precision\n        if half_attention:\n            model = model.half()\n        # If memory format optimization is enabled, optimize the model\n        if self.memory_format_optimization:\n            model = model.to_memory_format(torch.channels_last)\n        # Cache the model if requested\n        if self.cache_model:\n            print(\"Caching model...\")\n            torch.save(model, self.cache_path)\n            print(\"Model cached!\")\n    return model\n\n", "CodeBleu_score": 40.22608510350608}]}
{"instance_id": "8dc955007aa2", "generation_result": [{"code": "def convsample_ddim(model, cond, steps, shape, eta=1.0, callback=None, normals_sequence=None,\n                    mask=None, x0=None, quantize_x0=False, temperature=1., score_corrector=None,\n                    corrector_kwargs=None, x_t=None\n                    )\n\t\n    if not isinstance(model, torch.nn.Module):\n        model = instantiate_from_config(model)\n\n    sampler = DDIMSampler(model, eta=eta, callback=callback, normals_sequence=normals_sequence,\n                          score_corrector=score_corrector, **corrector_kwargs)\n\n    if mask is not None:\n        assert mask.shape[0] == 1\n        mask = repeat(mask, '1 n -> b n', b=cond.shape[0])\n\n    if x0 is not None:\n        assert x0.shape[0] == 1\n        x0 = repeat(x0, '1 c h w -> b c h w', b=cond.shape[0])\n        x0 = x0.to(cond.device)\n\n    if quantize_x0:\n        x0 = torch.round(x0 * 255) / 255\n\n    b, *_ = cond.shape\n    shape = [b] + shape\n\n    print(f'sampling with eta={eta}, {steps} steps')\n    t0 = time.time()\n    x, intermediates = sampler.sample(steps, shape, cond=cond, mask=mask, x0=x0, temperature=temperature, x_t=x_t)\n    print(f'sampling took {time.time() - t0} seconds')\n    return x, intermediates\n\n", "CodeBleu_score": 46.03928920400595}, {"code": "def convsample_ddim(model, cond, steps, shape, eta=1.0, callback=None, normals_sequence=None,\n                    mask=None, x0=None, quantize_x0=False, temperature=1., score_corrector=None,\n                    corrector_kwargs=None, x_t=None\n                    )\n\t\n    device = devices.get_device()\n    model = model.to(device)\n    if x0 is not None:\n        x0 = x0.to(device)\n    if cond is not None:\n        cond = cond.to(device)\n    if mask is not None:\n        mask = mask.to(device)\n    if normals_sequence is not None:\n        normals_sequence = normals_sequence.to(device)\n    if score_corrector is not None:\n        score_corrector = score_corrector.to(device)\n    if corrector_kwargs is not None:\n        corrector_kwargs = corrector_kwargs.to(device)\n    if x_t is not None:\n        x_t = x_t.to(device)\n\n    sampler = DDIMSampler(model, eta=eta, callback=callback)\n    batch_size = shape[0]\n    print(f\"Sampling with eta={eta} for {steps} steps\")\n    print(f\"Sampling with shape={shape}\")\n    t = torch.randint(0, steps, (batch_size,), device=device)\n    x_shape = [batch_size] + list(shape[1:])\n    if x0 is None:\n        x_t = torch.randn(x_shape, device=device)\n    else:\n        x_t = x0.detach().clone()\n    x_t = x_t.to(device)\n    assert x_t.shape == x_shape\n    if quantize_x0:\n        x_t, _, _, _ = model(x_t, t, cond=cond, eta=eta, mask=mask, normals_sequence=normals_sequence,\n                             quantize_x0=True, temperature=temperature, score_corrector=score_corrector,\n                             corrector_kwargs=corrector_kwargs, x_t=x_t)\n    for i in range(t.max() + 1):\n        assert x_t.shape == x_shape\n        ts = t == i\n        x_t_ = x_t[ts]\n        cond_ =", "CodeBleu_score": 40.43354103397293}, {"code": "def convsample_ddim(model, cond, steps, shape, eta=1.0, callback=None, normals_sequence=None,\n                    mask=None, x0=None, quantize_x0=False, temperature=1., score_corrector=None,\n                    corrector_kwargs=None, x_t=None\n                    )\n\t\n    # Initialize the DDIM sampler\n    sampler = DDIMSampler(model, eta=eta, callback=callback, normals_sequence=normals_sequence,\n                          score_corrector=score_corrector, **corrector_kwargs)\n    # Extract the batch size from the shape parameter\n    batch_size = shape[0]\n    # Print the eta and steps values\n    print(f'eta: {eta}, steps: {steps}')\n    # Call the sample method of the DDIM sampler instance\n    x, intermediates = sampler.sample(steps, batch_size, shape, cond, mask=mask, x0=x0,\n                                      quantize_x0=quantize_x0, temperature=temperature, x_t=x_t)\n    # Return the generated samples and intermediate values\n    return x, intermediates\n\n", "CodeBleu_score": 36.23619322349901}]}
{"instance_id": "9bdcd9081480", "generation_result": [{"code": "def ema_scope(self, context=None)\n\t\n    if self.ema is None:\n        return\n    if self.ema_apply_in_infer:\n        self.ema.swap_parameters()\n        if context is not None:\n            print(f\"{context}: using EMA weights\")\n    try:\n        yield\n    finally:\n        if self.ema_apply_in_infer:\n            self.ema.swap_parameters()\n            if context is not None:\n                print(f\"{context}: restoring original weights\")\n\n", "CodeBleu_score": 64.54464280410981}, {"code": "def ema_scope(self, context=None)\n\t\n    if self.ema is None:\n        yield\n    else:\n        if context is not None:\n            context.info(\"Using EMA weights.\")\n        self.ema.store()\n        self.ema.copy_to()\n        try:\n            yield\n        finally:\n            if context is not None:\n                context.info(\"Restoring original weights.\")\n            self.ema.restore()\n\n", "CodeBleu_score": 41.056617554960695}, {"code": "def ema_scope(self, context=None)\n\t\n    if self.ema is None:\n        return contextmanager(lambda x: x)()\n\n    if context is not None:\n        context.info(\"Using EMA weights\")\n\n    # Store the parameters of the model\n    params = self.model.parameters()\n    ema_params = list(self.ema.parameters())\n\n    # Replace the parameters of the model with those of the EMA\n    index = 0\n    for param in params:\n        param.data = ema_params[index].data.clone()\n        index += 1\n\n    yield\n\n    if context is not None:\n        context.info(\"Restoring original weights\")\n\n    # Replace the parameters of the model with the stored parameters\n    index = 0\n    for param in params:\n        param.data = ema_params[index].data.clone()\n        index += 1\n\n", "CodeBleu_score": 44.52688324514632}]}
{"instance_id": "14ab009e1c5c", "generation_result": [{"code": "def init_from_ckpt(self, path, ignore_keys=None)\n\t\n\n    if ignore_keys is None:\n        ignore_keys = []\n\n    # Load the state dict from the checkpoint\n    checkpoint = torch.load(path, map_location=\"cpu\")\n    if \"state_dict\" in checkpoint:\n        state_dict = checkpoint[\"state_dict\"]\n    else:\n        state_dict = checkpoint\n\n    # Delete any keys that match the ignore_keys list\n    for key in ignore_keys:\n        if key in state_dict:\n            del state_dict[key]\n\n    # Update the model's state with the loaded state dict\n    unexpected_keys = []\n    missing_keys = []\n    error_msgs = []\n    # copy state_dict so _load_from_state_dict can modify it\n    metadata = getattr(state_dict, \"_metadata\", None)\n    state_dict = state_dict.copy()\n    if metadata is not None:\n        state_dict._metadata = metadata\n\n    def load(module, prefix=\"\"):\n        local_metadata = {} if metadata is None else metadata.get(prefix[:-1], {})\n        module._load_from_state_dict(\n            state_dict,\n            prefix,\n            local_metadata,\n            True,\n            missing_keys,\n            unexpected_keys,\n            error_msgs,\n        )\n        for name, child in module._modules.items():\n            if child is not None:\n                load(child, prefix + name + \".\")\n\n    load(self)\n    del checkpoint\n    torch.cuda.empty_cache()\n\n    # Print out any missing or unexpected keys after the update\n    if len(missing_keys) > 0:\n        print(\n            \"Some module weights or buffers (keys) are missing in the checkpoint: {}\".format(\n                \", \".join([\"'{}'\".format(k) for k in missing_keys])\n            )\n        )\n    if len(unexpected_keys) > 0:\n        print(\n            \"Some module weights or buffers (keys) in the checkpoint are not used by the model: {}\".format(\n                \", \".join([\"'{}'\".format(k", "CodeBleu_score": 29.734403257523333}, {"code": "def init_from_ckpt(self, path, ignore_keys=None)\n\t\n    print(f\"Initializing model from checkpoint at {path}\")\n    sd = torch.load(path, map_location=\"cpu\")\n    if ignore_keys is not None:\n        print(f\"Ignoring keys: {ignore_keys}\")\n        for k in ignore_keys:\n            if k in sd:\n                del sd[k]\n    self.load_state_dict(sd, strict=False)\n    print(f\"Missing keys: {self.missing_keys}\")\n    print(f\"Unexpected keys: {self.unexpected_keys}\")\n\n", "CodeBleu_score": 30.51297941687816}, {"code": "def init_from_ckpt(self, path, ignore_keys=None)\n\t\n    print(f'Initializing model from checkpoint {path}')\n    checkpoint = torch.load(path, map_location='cpu')\n    if 'state_dict' in checkpoint:\n        state_dict = checkpoint['state_dict']\n    else:\n        state_dict = checkpoint\n\n    if ignore_keys is not None:\n        state_dict = {k: v for k, v in state_dict.items() if k not in ignore_keys}\n\n    self.load_state_dict(state_dict, strict=False)\n\n    missing_keys = self.load_state_dict(state_dict, strict=False)\n    if len(missing_keys.missing_keys) > 0:\n        print(f'Missing keys in state dict: {missing_keys.missing_keys}')\n    if len(missing_keys.unexpected_keys) > 0:\n        print(f'Unexpected keys in state dict: {missing_keys.unexpected_keys}')\n\n", "CodeBleu_score": 34.265751631961564}]}
{"instance_id": "754176e27b81", "generation_result": [{"code": "def forward(self, input, return_pred_indices=False)\n\t\n    if self.training:\n        input = input.permute(1, 0, 2)\n    input = input.contiguous()\n    x = self.encoder(input)\n    quant, diff, _, _ = self.vector_quantizer(x)\n    quant_conv = torch.zeros_like(quant)\n    if self.training:\n        quant_conv = self.ema_encoder(quant_conv)\n    else:\n        quant_conv = self.ema_encoder(quant)\n    dec = self.decoder(quant_conv)\n    if self.training:\n        dec = dec.permute(1, 0, 2)\n    dec = dec.contiguous()\n    if return_pred_indices:\n        return dec, diff, x\n    else:\n        return dec, diff\n\n", "CodeBleu_score": 44.47896500917765}, {"code": "def forward(self, input, return_pred_indices=False)\n\t\n    # Encode\n    z, _, (_, _, indices) = self.encoder(input)\n    # Decode\n    recon = self.decoder(z)\n    # Calculate difference metric\n    diff = (input - recon).abs().mean()\n    if return_pred_indices:\n        return recon, diff, indices\n    else:\n        return recon, diff\n\n", "CodeBleu_score": 29.83545919380178}, {"code": "def forward(self, input, return_pred_indices=False)\n\t\n    quant, quant_pred, indices = self.encode(input)\n    dec = self.decode(quant_pred)\n    diff = (dec - input).abs().mean()\n    if return_pred_indices:\n        return dec, diff, indices\n    else:\n        return dec, diff\n\n", "CodeBleu_score": 37.45416695941239}]}
{"instance_id": "443df6fdaf0d", "generation_result": [{"code": "def get_input(self, batch, k)\n\t\n    x = batch[k]\n    if len(x.shape) == 3:\n        x = x[..., None]\n    x = x.permute(0, 3, 1, 2).contiguous().float()\n    if self.resize:\n        factor = self.resize_factors[self.training_step]\n        x = torch.nn.functional.interpolate(x, scale_factor=factor, mode='bicubic', align_corners=False)\n    x = x.detach()\n    return x\n\n", "CodeBleu_score": 40.44564297118966}, {"code": "def get_input(self, batch, k)\n\t\n    x = batch[k]\n    if len(x.shape) == 3:\n        x = x[..., None]\n    x = x.permute(0, 3, 1, 2).contiguous().float()\n    if self.args.resize_factors is not None:\n        size = x.shape[-1]\n        assert size % 64 == 0\n        if self.args.resize_factors == '1024':\n            size = 1024\n        elif self.args.resize_factors == '512':\n            size = 512\n        elif self.args.resize_factors == '256':\n            size = 256\n        else:\n            raise ValueError(f'invalid resize factors {self.args.resize_factors}')\n        if size != x.shape[-1]:\n            x = torch.nn.functional.interpolate(x, size=size, mode='bicubic')\n    x = x.detach()\n    return x\n\n", "CodeBleu_score": 49.40641739309029}, {"code": "def get_input(self, batch, k)\n\t\n    x = batch[k]\n    if len(x.shape) == 2:\n        x = x[..., None]\n    x = x.permute(0, 3, 1, 2).contiguous().float()\n    if self.resize_enabled:\n        x = self.resize_transform(x)\n    x = x.detach()\n    return x\n\n", "CodeBleu_score": 27.833931011457825}]}
{"instance_id": "953a2632fc15", "generation_result": [{"code": "def training_step(self, batch, batch_idx, optimizer_idx)\n\t\n    x, y = batch\n    x = x.to(self.device)\n    y = y.to(self.device)\n    z, _, _ = self.model(x)\n    if optimizer_idx == 0:\n        loss = self.autoencoder_loss(z, x)\n        self.log(\"train/autoencoder_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n    elif optimizer_idx == 1:\n        loss = self.discriminator_loss(z, y)\n        self.log(\"train/discriminator_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n    return loss\n\n", "CodeBleu_score": 16.871193024230962}, {"code": "def training_step(self, batch, batch_idx, optimizer_idx)\n\t\n    x, y = batch\n    x = x.to(self.device)\n    y = y.to(self.device)\n    z, qloss = self.model(x, y)\n    if optimizer_idx == 0:\n        loss = self.autoencoder_loss(z, x, y)\n    elif optimizer_idx == 1:\n        loss = self.discriminator_loss(z, y)\n    else:\n        raise ValueError(f'Invalid optimizer index: {optimizer_idx}')\n    self.log('train_loss', loss, on_step=True, on_epoch=False, prog_bar=True, logger=True)\n    return loss\n\n", "CodeBleu_score": 22.652987241866196}, {"code": "def training_step(self, batch, batch_idx, optimizer_idx)\n\t\n    x, y = batch\n    x = x.to(self.device)\n    y = y.to(self.device)\n\n    if optimizer_idx == 0:\n        z, delta_logits, eps = self(x, y)\n        loss_ae = self.loss_fn(z, x, y)\n        self.log(\"train_loss_ae\", loss_ae, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n        return loss_ae\n    else:\n        z, delta_logits, eps = self(x, y, compute_delta_loss=True)\n        loss_disc = self.loss_fn(z, x, y, delta_logits)\n        self.log(\"train_loss_disc\", loss_disc, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n        return loss_disc\n\n", "CodeBleu_score": 22.483182876619484}]}
{"instance_id": "b449b5e8b77f", "generation_result": [{"code": "def log_images(self, batch, only_inputs=False, plot_ema=False, **kwargs)\n\t\n    if not self.logger.experiment:\n        return\n\n    if plot_ema:\n        model = self.ema_model.module\n    else:\n        model = self.model\n\n    # Get the input\n    x = batch[\"x_t\"]\n    x = x.to(self.device)\n\n    # Process the input\n    if self.model_type == \"disc\":\n        x_in = x\n    elif self.model_type == \"enc\":\n        x_in = model.q_sample(x)\n    elif self.model_type == \"dec\":\n        x_in = model.p_sample(x)\n    elif self.model_type == \"ae\":\n        x_in = model.decode(x)\n    else:\n        raise ValueError(f\"Model type {self.model_type} not recognized\")\n\n    # Get the reconstruction\n    if self.model_type == \"disc\":\n        x_rec = x_in\n    elif self.model_type == \"enc\":\n        x_rec = model.decode(x_in)\n    elif self.model_type == \"dec\":\n        x_rec = model.p_sample(x_in)\n    elif self.model_type == \"ae\":\n        x_rec = model.decode(x_in)\n    else:\n        raise ValueError(f\"Model type {self.model_type} not recognized\")\n\n    # Log the input\n    self.logger.experiment.add_images(\n        \"input\",\n        x_in.detach().cpu(),\n        self.global_step,\n        dataformats=\"NCHW\",\n    )\n\n    # Log the reconstruction\n    if not only_inputs:\n        self.logger.experiment.add_images(\n            \"reconstruction\",\n            x_rec.detach().cpu(),\n            self.global_step,\n            dataformats=\"NCHW\",\n        )\n\n    # Log the EMA reconstruction\n    if plot_ema:\n        x_rec = model.decode(x_in)\n        self.logger.experiment.add_images(\n            \"reconstruction_ema", "CodeBleu_score": 33.14514795334287}, {"code": "def log_images(self, batch, only_inputs=False, plot_ema=False, **kwargs)\n\t\n    if not self.logger:\n        return\n\n    if only_inputs:\n        log_data = batch\n    else:\n        if plot_ema:\n            log_data = torch.cat([batch, self.ema(batch)], dim=0)\n        else:\n            log_data = torch.cat([batch, self(batch)], dim=0)\n\n    log_data = log_data.detach().cpu()\n\n    if self.colorize_fn is not None:\n        log_data = self.colorize_fn(log_data)\n\n    self.logger.experiment.add_images(\n        \"samples\", log_data, self.global_step, dataformats=\"NCHW\"\n    )\n\n", "CodeBleu_score": 31.347990024313578}, {"code": "def log_images(self, batch, only_inputs=False, plot_ema=False, **kwargs)\n\t\n    # Process input data\n    x, c = self.preprocess_batch(batch)\n    if not only_inputs:\n        # Reconstructions\n        with torch.no_grad():\n            x_rec, x_rec_ema = self(x, c, return_logits=False, return_all_activations=False, return_all_latents=False, return_all_preds=False, return_preds_ema=True)\n        x_rec_ema = x_rec_ema.cpu()\n        x_rec = x_rec.cpu()\n    x = x.cpu()\n    # Log inputs\n    self.logger.experiment.add_images('inputs', x, self.global_step, dataformats='NCHW')\n    if not only_inputs:\n        # Log reconstructions\n        self.logger.experiment.add_images('reconstructions', x_rec, self.global_step, dataformats='NCHW')\n        if plot_ema:\n            self.logger.experiment.add_images('reconstructions_ema', x_rec_ema, self.global_step, dataformats='NCHW')\n\n", "CodeBleu_score": 29.88526824203186}]}
{"instance_id": "540d089e3481", "generation_result": [{"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss_type == 'l1':\n        loss = torch.abs(pred - target)\n        if mean:\n            loss = loss.mean()\n    elif self.loss_type == 'l2':\n        loss = torch.pow(pred - target, 2)\n        if mean:\n            loss = loss.mean()\n    else:\n        raise Exception(\"unknown loss type\")\n    return loss\n\n", "CodeBleu_score": 52.61976348428175}, {"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss_type == 'l1':\n        loss = torch.abs(pred - target)\n        if mean:\n            loss = loss.mean()\n    elif self.loss_type == 'l2':\n        loss = torch.pow(pred - target, 2)\n        if mean:\n            loss = loss.mean()\n    else:\n        raise Exception('Unknown loss type!')\n    return loss\n\n", "CodeBleu_score": 52.48911854699448}, {"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss_type == 'l1':\n        loss = torch.abs(pred - target)\n        if mean:\n            loss = loss.mean()\n    elif self.loss_type == 'l2':\n        loss = torch.pow(pred - target, 2)\n        if mean:\n            loss = loss.mean()\n    else:\n        raise NotImplementedError\n    return loss\n\n", "CodeBleu_score": 49.990536905437175}]}
{"instance_id": "63a08283666f", "generation_result": [{"code": "def forward(self, x, c, *args, **kwargs)\n\t\n    x = self.q_sample(x, c, *args, **kwargs)\n    x, *args, kwargs = self.q_sample_loop(x, c, *args, **kwargs)\n    x = self.q_posterior(x, c, *args, **kwargs)\n    x = self.p_mean_var(x, c, *args, **kwargs)\n    x = self.p(x, c, *args, **kwargs)\n    x = self.p_sample(x, c, *args, **kwargs)\n    x = self.p_sample_loop(x, c, *args, **kwargs)\n    x = self.p_sample_loop_progressive(x, c, *args, **kwargs)\n    x = self.p_sample_loop_ddim(x, c, *args, **kwargs)\n    x = self.p_sample_loop_ddpm(x, c, *args, **kwargs)\n    x = self.p_sample_loop_ddim_ddpm(x, c, *args, **kwargs)\n    x = self.p_sample_loop_ddim_ddpm_v2(x, c, *args, **kwargs)\n    x = self.p_sample_loop_ddim_ddpm_v3(x, c, *args, **kwargs)\n    x = self.p_sample_loop_ddim_ddpm_v4(x, c, *args, **kwargs)\n    x = self.p_sample_loop_ddim_ddpm_v5(x, c, *args, **kwargs)\n    x = self.p_sample_loop_ddim_ddpm_v6(x, c, *args, **kwargs)\n    x = self.p_sample_loop_ddim_ddpm_v7(x, c, *args, **kwargs)\n    x = self.p_sample_loop_ddim_ddpm_v8(x, c, *args, **kwargs)\n    x = self.p_sample_loop_ddim_ddpm_v9(x, c, *args, **kwargs)\n    x = self.p_sample", "CodeBleu_score": 9.6376558309859}, {"code": "def forward(self, x, c, *args, **kwargs)\n\t\n    # Sample time steps for the forward pass\n    ts = self.sample_timesteps(x.shape[0])\n    # If conditioning is applicable, process it\n    if self.conditioning_type == 'discrete':\n        c = self.process_discrete_conditioning(c)\n    elif self.conditioning_type == 'continuous':\n        c = self.process_continuous_conditioning(c)\n    else:\n        c = None\n    # Apply the forward pass\n    loss = self.model(x, ts, c, *args, **kwargs)\n    # Return the loss\n    return loss\n\n", "CodeBleu_score": 26.613117183978595}, {"code": "def forward(self, x, c, *args, **kwargs)\n\t\n    # sample time steps\n    ts = self.betas.sample(x.shape[:2], device=x.device)\n    # apply learned conditioning\n    if self.condition_x:\n        x = self.cond_fn(x, c)\n    # process inputs according to model's conditioning requirements\n    if self.condition_x:\n        x = self.cond_fn(x, c)\n    # compute loss\n    loss = self.compute_loss(x, ts)\n    return loss\n\n", "CodeBleu_score": 17.121918625443165}]}
{"instance_id": "4627e1e8ed62", "generation_result": [{"code": "def get_codebook_entry(self, indices, shape)\n\t\n    if self.remap is not None:\n        indices = self.remap[indices]\n    z = self.embed_code(indices)\n    if shape is not None:\n        z = rearrange(z, 'b (h w) c -> b c h w', h=shape[0], w=shape[1])\n    return z\n\n", "CodeBleu_score": 42.58198279616959}, {"code": "def get_codebook_entry(self, indices, shape)\n\t\n    if self.remap is not None:\n        indices = self.remap[indices]\n    z = self.codebook.embed_code(indices, shape=shape)\n    if self.permutation is not None:\n        z = z.permute(self.permutation)\n    return z\n\n", "CodeBleu_score": 31.327390186616334}, {"code": "def get_codebook_entry(self, indices, shape)\n\t\n    if self.remap is not None:\n        indices = self.remap[indices]\n    z = self.codebook[indices]\n    B, H, W, C = shape\n    z = rearrange(z, \"b h w (g d) -> b g h w d\", g=self.groups)\n    z = z.view(B, self.groups, H, W, -1)\n    if self.permutation is not None:\n        z = z[:, :, :, :, self.permutation]\n    return z\n\n", "CodeBleu_score": 41.31090063235228}]}
{"instance_id": "c55d014827ef", "generation_result": [{"code": "def factorization(dimension: int, factor:int=-1) -> tuple[int, int]\n\t\n    if factor > 0 and dimension % factor == 0:\n        return (factor, dimension // factor)\n    elif factor < 0:\n        factor = -factor\n        for i in range(factor, dimension // 2 + 1):\n            if dimension % i == 0:\n                return (i, dimension // i)\n    else:\n        raise ValueError(\"The factor must be positive and divide the dimension or negative.\")\n\n", "CodeBleu_score": 25.579205855601284}, {"code": "def factorization(dimension: int, factor:int=-1) -> tuple[int, int]\n\t\n    if factor > 0 and dimension % factor == 0:\n        return factor, dimension // factor\n    elif factor < 0:\n        if dimension % 2 == 0:\n            m, n = 2, dimension // 2\n        else:\n            m, n = 1, dimension\n        while n > abs(factor):\n            if n % 2 == 0:\n                m, n = m + 1, n // 2\n            else:\n                m, n = m + 1, n - 1\n        return m, n\n    else:\n        raise ValueError(\"factor must be positive or negative\")\n\n", "CodeBleu_score": 39.24929504935324}, {"code": "def factorization(dimension: int, factor:int=-1) -> tuple[int, int]\n\t\n    if factor >= 0:\n        if dimension % factor == 0:\n            return factor, dimension // factor\n        else:\n            raise ValueError(f\"The factor {factor} does not divide the dimension {dimension}\")\n    else:\n        factor = -factor\n        if dimension % factor == 0:\n            return dimension // factor, factor\n        else:\n            m = dimension // factor\n            while m > factor:\n                if dimension % m == 0:\n                    return m, dimension // m\n                m -= 1\n            raise ValueError(f\"The factor {factor} is too small to divide the dimension {dimension}\")\n\n", "CodeBleu_score": 32.763992095670815}]}
{"instance_id": "5d613ec26036", "generation_result": [{"code": "def apply_weight_decompose(self, updown, orig_weight)\n\t\n    if not isinstance(updown, bool):\n        raise errors.ModelError(\n            f\"updown must be a boolean value, got {type(updown)}\")\n    if not isinstance(orig_weight, torch.Tensor):\n        raise errors.ModelError(\n            f\"orig_weight must be a torch.Tensor, got {type(orig_weight)}\")\n    if not isinstance(self.weight, torch.Tensor):\n        raise errors.ModelError(\n            f\"self.weight must be a torch.Tensor, got {type(self.weight)}\")\n    if orig_weight.device != self.weight.device:\n        raise errors.ModelError(\n            f\"orig_weight and self.weight must be on the same device, got {orig_weight.device} and {self.weight.device}\")\n    if orig_weight.dtype != self.weight.dtype:\n        raise errors.ModelError(\n            f\"orig_weight and self.weight must have the same data type, got {orig_weight.dtype} and {self.weight.dtype}\")\n    if orig_weight.shape != self.weight.shape:\n        raise errors.ModelError(\n            f\"orig_weight and self.weight must have the same shape, got {orig_weight.shape} and {self.weight.shape}\")\n    if not isinstance(self.weight_decompose_scale, float):\n        raise errors.ModelError(\n            f\"self.weight_decompose_scale must be a float, got {type(self.weight_decompose_scale)}\")\n    if self.weight_decompose_scale < 0:\n        raise errors.ModelError(\n            f\"self.weight_decompose_scale must be greater than or equal to 0, got {self.weight_decompose_scale}\")\n    if self.weight_decompose_scale > 1:\n        raise errors.ModelError(\n            f\"self.weight_decompose_scale must be less than or equal to 1, got {self.weight_decompose_scale}\")\n    if updown:\n        weight_decompose = orig_weight + self.weight_decompose_scale * self.weight", "CodeBleu_score": 15.90476810611877}, {"code": "def apply_weight_decompose(self, updown, orig_weight)\n\t\n\n    # Check if the input weight is None\n    if updown is None:\n        # If the input weight is None, then return the original weight\n        return orig_weight\n\n    # Check if the original weight is None\n    if orig_weight is None:\n        # If the original weight is None, then return the input weight\n        return updown\n\n    # Check if the input weight is on the same device as the original weight\n    if updown.device != orig_weight.device:\n        # If the input weight is not on the same device, then move it to the same device\n        updown = updown.to(orig_weight.device)\n\n    # Check if the input weight is the same data type as the original weight\n    if updown.dtype != orig_weight.dtype:\n        # If the input weight is not the same data type, then convert it to the same data type\n        updown = updown.type(orig_weight.dtype)\n\n    # Normalize the combined weight\n    combined_weight = updown + orig_weight\n    combined_weight = combined_weight / combined_weight.norm(p=2)\n\n    # Scale the combined weight\n    combined_weight = combined_weight * self.scale_factor\n\n    # Calculate the adjusted weight\n    adjusted_weight = combined_weight - orig_weight\n\n    # Return the adjusted weight\n    return adjusted_weight\n\n", "CodeBleu_score": 31.480476566174097}, {"code": "def apply_weight_decompose(self, updown, orig_weight)\n\t\n    # Check if the original weight is a tensor\n    if not isinstance(orig_weight, torch.Tensor):\n        raise TypeError(\"The original weight must be a torch.Tensor object.\")\n\n    # Check if the original weight is on the same device as the model\n    if self.device != orig_weight.device:\n        raise errors.DeviceError(\n            \"The original weight must be on the same device as the model.\"\n        )\n\n    # Check if the original weight is the same data type as the model\n    if self.dtype != orig_weight.dtype:\n        raise errors.DataTypeError(\n            \"The original weight must be the same data type as the model.\"\n        )\n\n    # Decompose the original weight into a tensor of the same shape as the input weight\n    orig_weight = torch.zeros_like(updown) + orig_weight\n\n    # Normalize the input weight\n    updown = updown / torch.linalg.norm(updown)\n\n    # Scale the input weight\n    updown = updown * self.weight_scale\n\n    # Normalize the scaled input weight\n    updown = updown / torch.linalg.norm(updown)\n\n    # Calculate the adjusted weight\n    weight = updown - orig_weight\n\n    # Return the adjusted weight\n    return weight\n\n", "CodeBleu_score": 25.769168974219575}]}
{"instance_id": "c73d0d1cd3e5", "generation_result": [{"code": "def finalize_updown(self, updown, orig_weight, output_shape, ex_bias=None)\n\t\n    if ex_bias is not None:\n        updown = updown + ex_bias\n    if updown.size == orig_weight.size:\n        updown = updown.reshape(orig_weight.shape)\n    else:\n        updown = updown.reshape(output_shape)\n    if self.scaling_factor != 1:\n        updown = self.weight_decomp(updown)\n    return updown, ex_bias\n\n", "CodeBleu_score": 31.094305884732563}, {"code": "def finalize_updown(self, updown, orig_weight, output_shape, ex_bias=None)\n\t\n    if ex_bias is None:\n        ex_bias = 0\n    if updown.numel() == orig_weight.numel():\n        updown = updown.view(orig_weight.shape)\n    else:\n        updown = updown.view(output_shape)\n        if ex_bias != 0:\n            ex_bias = ex_bias.view(output_shape)\n        if self.scalar is not None:\n            updown = updown.view(-1)\n            updown = self.scalar.apply(updown)\n            updown = updown.view(output_shape)\n    return updown, ex_bias\n\n", "CodeBleu_score": 43.01904614762408}, {"code": "def finalize_updown(self, updown, orig_weight, output_shape, ex_bias=None)\n\t\n    if ex_bias is not None:\n        ex_bias = ex_bias * updown.bias\n    if updown.weight.numel() == orig_weight.numel():\n        updown.weight = updown.weight.reshape(orig_weight.shape)\n    else:\n        updown.weight = updown.weight.reshape(output_shape)\n    if self.scaling_factor is not None:\n        updown.weight = self.decompose(updown.weight, self.scaling_factor)\n    if ex_bias is not None:\n        return updown.weight, ex_bias\n    return updown.weight\n\n", "CodeBleu_score": 41.890034253878504}]}
{"instance_id": "c6a41665c62c", "generation_result": [{"code": "def calc_updown(self, orig_weight)\n\t\n    if self.bias is not None:\n        bias = self.bias.to(orig_weight.device)\n    else:\n        bias = None\n    if self.device is None:\n        self.device = orig_weight.device\n    else:\n        self.weight = self.weight.to(self.device)\n        if self.bias is not None:\n            bias = self.bias.to(self.device)\n    self.weight = self.weight.to(orig_weight.device)\n    if self.bias is not None:\n        bias = bias.to(orig_weight.device)\n    self.weight = self.finalize_updown(orig_weight, self.weight, bias)\n    self.weight = self.weight.to(self.device)\n    if self.bias is not None:\n        self.bias = self.bias.to(self.device)\n    return self.weight\n\n", "CodeBleu_score": 39.17638599609169}, {"code": "def calc_updown(self, orig_weight)\n\t\n    if self.bias is not None:\n        self.bias = self.bias.to(orig_weight.device)\n        self.weight = self.weight.to(orig_weight.device)\n        return self.calc_updown_no_bias(orig_weight)\n    else:\n        self.weight = self.weight.to(orig_weight.device)\n        return self.calc_updown_no_bias(orig_weight)\n\n", "CodeBleu_score": 41.87506804488611}, {"code": "def calc_updown(self, orig_weight)\n\t\n    if self.current_weight is None:\n        self.current_weight = self.weight.clone().detach().requires_grad_(True)\n        self.current_bias = self.bias.clone().detach().requires_grad_(True)\n        self.current_weight = self.current_weight.to(orig_weight.device)\n        self.current_bias = self.current_bias.to(orig_weight.device)\n\n    self.current_weight = self.current_weight.to(orig_weight.device)\n    self.current_bias = self.current_bias.to(orig_weight.device)\n\n    self.current_weight = network.updown_weight(self.current_weight, orig_weight, self.updown_type)\n\n    if self.bias is not None:\n        self.current_bias = network.updown_bias(self.current_bias, self.bias, self.updown_type)\n\n    return self.current_weight, self.current_bias\n\n", "CodeBleu_score": 34.82349985993505}]}
{"instance_id": "1bb9e5d94af8", "generation_result": [{"code": "def purge_networks_from_memory()\n\t\n    if len(shared.networks) > shared.config[\"memory\"][\"max_networks\"]:\n        logger.warning(f\"Networks in memory exceeds limit of {shared.config['memory']['max_networks']}, purging...\")\n        while len(shared.networks) > shared.config[\"memory\"][\"max_networks\"]:\n            network_to_purge = shared.networks[0]\n            del shared.networks[0]\n            logger.debug(f\"Purged network {network_to_purge.name} from memory\")\n        logger.debug(\"Purged all networks from memory, triggering garbage collection...\")\n        torch.cuda.empty_cache()\n        logger.debug(\"Garbage collection complete\")\n\n", "CodeBleu_score": 25.026046345743957}, {"code": "def purge_networks_from_memory()\n\t\n    if len(network.networks) > shared.MAX_NETWORK_COUNT:\n        logger.debug(f\"Purging networks from memory...\")\n        while len(network.networks) > shared.MAX_NETWORK_COUNT:\n            network.networks.popitem()\n        logger.debug(f\"Garbage collection...\")\n        torch.cuda.empty_cache()\n\n", "CodeBleu_score": 26.20678431901633}, {"code": "def purge_networks_from_memory()\n\t\n    logger.info(\"Purging networks from memory...\")\n    networks = list(shared.networks.keys())\n    networks.sort(key=lambda x: shared.networks[x].last_used)\n    for network in networks:\n        if len(shared.networks) <= shared.network_limit:\n            break\n        del shared.networks[network]\n    logger.info(\"Purged {} networks from memory.\".format(len(networks) - shared.network_limit))\n    logger.info(\"Currently using {}/{} networks.\".format(len(shared.networks), shared.network_limit))\n    torch.cuda.empty_cache()\n\n", "CodeBleu_score": 31.28306942052689}]}
{"instance_id": "42b48ebf4d07", "generation_result": [{"code": "def network_restore_weights_from_backup(self: Union[torch.nn.Conv2d, torch.nn.Linear, torch.nn.GroupNorm, torch.nn.LayerNorm, torch.nn.MultiheadAttention]):\n\t\n    if self.weight.requires_grad:\n        # Restore weights\n        if os.path.exists(self.weight.backup_path):\n            self.weight.data = torch.load(self.weight.backup_path)\n        else:\n            raise errors.BackupNotFoundError(self.weight.backup_path)\n\n        # Restore bias\n        if self.bias is not None:\n            if os.path.exists(self.bias.backup_path):\n                self.bias.data = torch.load(self.bias.backup_path)\n            else:\n                self.bias = None\n\n        # Restore projection weights and biases\n        if isinstance(self, torch.nn.MultiheadAttention):\n            if os.path.exists(self.in_proj_weight.backup_path):\n                self.in_proj_weight.data = torch.load(self.in_proj_weight.backup_path)\n            else:\n                raise errors.BackupNotFoundError(self.in_proj_weight.backup_path)\n\n            if os.path.exists(self.in_proj_bias.backup_path):\n                self.in_proj_bias.data = torch.load(self.in_proj_bias.backup_path)\n            else:\n                self.in_proj_bias = None\n\n", "CodeBleu_score": 41.18676824783784}, {"code": "def network_restore_weights_from_backup(self: Union[torch.nn.Conv2d, torch.nn.Linear, torch.nn.GroupNorm, torch.nn.LayerNorm, torch.nn.MultiheadAttention]):\n\t\n    if self.bias is not None:\n        bias_backup = self.bias.data.clone()\n    else:\n        bias_backup = None\n    if self.weight.data.shape == self.weight_backup.shape:\n        self.weight.data = self.weight_backup.clone()\n    else:\n        self.weight.data = torch.zeros(self.weight.data.shape)\n        self.weight.data[:self.weight_backup.shape[0], :self.weight_backup.shape[1]] = self.weight_backup.clone()\n    if bias_backup is not None:\n        self.bias.data = bias_backup.clone()\n    else:\n        self.bias = None\n    if type(self) == torch.nn.MultiheadAttention:\n        if self.in_proj_weight.data.shape == self.in_proj_weight_backup.shape:\n            self.in_proj_weight.data = self.in_proj_weight_backup.clone()\n        else:\n            self.in_proj_weight.data = torch.zeros(self.in_proj_weight.data.shape)\n            self.in_proj_weight.data[:self.in_proj_weight_backup.shape[0], :self.in_proj_weight_backup.shape[1]] = self.in_proj_weight_backup.clone()\n        if self.in_proj_bias is not None:\n            self.in_proj_bias.data = self.in_proj_bias_backup.clone()\n        else:\n            self.in_proj_bias = None\n\n", "CodeBleu_score": 41.804261643088815}, {"code": "def network_restore_weights_from_backup(self: Union[torch.nn.Conv2d, torch.nn.Linear, torch.nn.GroupNorm, torch.nn.LayerNorm, torch.nn.MultiheadAttention]):\n\t\n    if self.bias is not None:\n        bias_backup = getattr(self, \"bias_backup\", None)\n        if bias_backup is not None:\n            self.bias.data = bias_backup\n            self.bias_backup = None\n    if isinstance(self, torch.nn.MultiheadAttention):\n        proj_weight_backup = getattr(self, \"proj_weight_backup\", None)\n        if proj_weight_backup is not None:\n            self.in_proj_weight = proj_weight_backup\n            self.proj_weight_backup = None\n        proj_bias_backup = getattr(self, \"proj_bias_backup\", None)\n        if proj_bias_backup is not None:\n            self.in_proj_bias = proj_bias_backup\n            self.proj_bias_backup = None\n    else:\n        weight_backup = getattr(self, \"weight_backup\", None)\n        if weight_backup is not None:\n            self.weight.data = weight_backup\n            self.weight_backup = None\n\n", "CodeBleu_score": 49.36733142633043}]}
{"instance_id": "c8fcd357131a", "generation_result": [{"code": "def network_forward(org_module, input, original_forward)\n\t\n    if len(network.loaded_networks) == 0:\n        return original_forward(input)\n\n    # Cast input to appropriate device\n    if isinstance(input, torch.Tensor):\n        input = input.to(org_module.device)\n    elif isinstance(input, tuple):\n        input = tuple(i.to(org_module.device) for i in input)\n\n    # Restore weights and reset module\n    org_module.load_backup()\n    org_module.reset()\n\n    # Perform forward pass through original network\n    output = original_forward(input)\n\n    # Apply each loaded network to the output if it matches the current layer\n    for network in network.loaded_networks:\n        if network.layer == org_module.layer:\n            output = network.forward(output)\n\n    return output\n\n", "CodeBleu_score": 38.67626343173636}, {"code": "def network_forward(org_module, input, original_forward)\n\t\n    if len(shared.loaded_networks) == 0:\n        return original_forward(input)\n\n    input = input.to(shared.device)\n    org_module.weight.data = shared.loaded_networks[0].weight.data\n    org_module.bias.data = shared.loaded_networks[0].bias.data\n    output = original_forward(input)\n    for network in shared.loaded_networks:\n        if network.layer_id == shared.layer_id:\n            output = network.forward(output)\n    return output\n\n", "CodeBleu_score": 35.57036034647675}, {"code": "def network_forward(org_module, input, original_forward)\n\t\n    if not shared.networks:\n        return original_forward(input)\n    device = shared.networks[0].device\n    if device == 'cuda':\n        device = 'cuda:0'\n    input = input.to(device)\n    for net in shared.networks:\n        net.restore_weights()\n        net.reset_weights()\n    output = original_forward(input)\n    for net in shared.networks:\n        if net.layer == shared.layer:\n            output = net.forward(output)\n    return output\n\n", "CodeBleu_score": 34.36838612091484}]}
{"instance_id": "1070211f5c11", "generation_result": [{"code": "def trainables(self)\n\t\n    return [p for p in self.parameters() if p.requires_grad]\n\n", "CodeBleu_score": 6.452447410697191}, {"code": "def trainables(self)\n\t\n    return [p for p in self.parameters() if p.requires_grad]\n\n", "CodeBleu_score": 6.452447410697191}, {"code": "def trainables(self)\n\t\n    return [p for p in self.parameters() if p.requires_grad]\n\n", "CodeBleu_score": 6.452447410697191}]}
{"instance_id": "0c3ba62c96a7", "generation_result": [{"code": "def load_hypernetworks(names, multipliers=None)\n\t\n    if not shared.hypernetworks:\n        shared.hypernetworks = []\n    else:\n        shared.hypernetworks.clear()\n    if not multipliers:\n        multipliers = [1] * len(names)\n    for name, multiplier in zip(names, multipliers):\n        if name in shared.hypernetworks:\n            hypernetwork = shared.hypernetworks[shared.hypernetworks.index(name)]\n        else:\n            hypernetwork = sd_models.HyperNetwork(name)\n        hypernetwork.multiplier = multiplier\n        shared.hypernetworks.append(hypernetwork)\n\n", "CodeBleu_score": 29.587453985729045}, {"code": "def load_hypernetworks(names, multipliers=None)\n\t\n    if multipliers is None:\n        multipliers = [1] * len(names)\n    if len(multipliers) != len(names):\n        raise ValueError(\"multipliers must be the same length as names\")\n    shared.loaded_hypernetworks.clear()\n    for name, multiplier in zip(names, multipliers):\n        if name in shared.loaded_hypernetworks:\n            hypernetwork = shared.loaded_hypernetworks[name]\n        else:\n            hypernetwork = sd_models.HyperNetwork(name)\n            shared.loaded_hypernetworks[name] = hypernetwork\n        hypernetwork.multiplier = multiplier\n        shared.loaded_hypernetworks.append(hypernetwork)\n\n", "CodeBleu_score": 34.0104161753233}, {"code": "def load_hypernetworks(names, multipliers=None)\n\t\n    if shared.loaded_hypernetworks is None:\n        shared.loaded_hypernetworks = []\n    else:\n        shared.loaded_hypernetworks.clear()\n    for name, multiplier in zip(names, multipliers):\n        if name in shared.loaded_hypernetworks:\n            hypernetwork = shared.loaded_hypernetworks[shared.loaded_hypernetworks.index(name)]\n        else:\n            hypernetwork = sd_models.HyperNetwork(name)\n        if multiplier is not None:\n            hypernetwork.multiplier = multiplier\n        shared.loaded_hypernetworks.append(hypernetwork)\n\n", "CodeBleu_score": 36.3542420083234}]}
{"instance_id": "bea3772ed532", "generation_result": [{"code": "\ndef apply_hypernetworks(hypernetworks, context, layer=None)\n\t\n    if layer is None:\n        layer = len(hypernetworks) - 1\n    context_k, context_v = context, context\n    for hypernetwork in hypernetworks[:layer]:\n        context_k, context_v = apply_hypernetwork(hypernetwork, context_k, context_v)\n    return context_k, context_v\n\n", "CodeBleu_score": 38.32872620816024}, {"code": "\ndef apply_hypernetworks(hypernetworks, context, layer=None)\n\t\n    if layer is None:\n        layer = len(hypernetworks)\n    context_k, context_v = context, context\n    for i in range(layer):\n        context_k, context_v = apply_hypernetwork(hypernetworks[i], context_k, context_v)\n    return context_k, context_v\n", "CodeBleu_score": 37.927568675742585}, {"code": "\ndef apply_hypernetworks(hypernetworks, context, layer=None)\n\t\n    context_k = context\n    context_v = context\n    for hypernetwork in hypernetworks:\n        context_k, context_v = apply_single_hypernetwork(hypernetwork, context_k, context_v, layer)\n    return context_k, context_v\n", "CodeBleu_score": 76.04946886953897}]}
{"instance_id": "922efa3e092f", "generation_result": [{"code": "def attention_CrossAttention_forward(self, x, context=None, mask=None, **kwargs)\n\t\n    # x: [batch_size, seq_len, input_dim]\n    # context: [batch_size, seq_len, context_dim]\n    # mask: [batch_size, seq_len]\n    # output: [batch_size, seq_len, output_dim]\n\n    # Get hyperparameters\n    d_model = self.d_model\n    n_heads = self.n_heads\n    d_qk = self.d_qk\n    d_v = self.d_v\n    d_out = self.d_out\n    dropout = self.dropout\n    mask_value = self.mask_value\n\n    # Apply hypernetworks\n    x = self.x_proj(x)\n    context = self.context_proj(context)\n\n    # Reshape for multi-head attention\n    x = rearrange(x, 'b s (n h) -> b n s h', h=d_qk)\n    context = rearrange(context, 'b s (n h) -> b n s h', h=d_v)\n\n    # Calculate query, key, and value tensors\n    q = self.q(x)\n    k = self.k(context)\n    v = self.v(context)\n\n    # Calculate scaled dot-product attention\n    scale = d_qk ** -0.5\n    scores = einsum('bn i d, bn j d -> bn i j', q, k) * scale\n\n    # Optionally apply mask\n    if mask is not None:\n        scores = scores.masked_fill(~mask[:, :, None, :], mask_value)\n\n    # Calculate attention weights\n    weights = self.softmax(scores)\n\n    # Dropout\n    weights = self.dropout(weights)\n\n    # Apply attention weights\n    out = einsum('bn i j, bn j d -> bn i d', weights, v)\n\n    # Reshape and pass through final linear layer\n    out = rearrange(out, 'b n s h -> b s (n h)')\n    out = self.out_proj(out)\n\n   ", "CodeBleu_score": 39.043053174992274}, {"code": "def attention_CrossAttention_forward(self, x, context=None, mask=None, **kwargs)\n\t\n    # Compute query, key, and value projections\n    query = self.query(x)\n    context_key = self.context_key(context)\n    context_value = self.context_value(context)\n    # Apply hypernetworks\n    if self.hypers_initialized:\n        query = self.hyper_w_q(query)\n        context_key = self.hyper_w_k(context_key)\n        context_value = self.hyper_w_v(context_value)\n    # Reshape for multi-head attention\n    query = rearrange(query, 'b n (h d) -> b h n d', h=self.n_heads)\n    context_key = rearrange(context_key, 'b n (h d) -> b h n d', h=self.n_heads)\n    context_value = rearrange(context_value, 'b n (h d) -> b h n d', h=self.n_heads)\n    # Calculate scaled dot-product attention\n    dots = einsum('b h i d, b h j d -> b h i j', query, context_key) * self.scale\n    # Optionally apply mask\n    if mask is not None:\n        dots.masked_fill_(~mask, float('-inf'))\n    # Calculate attention weights\n    attn = dots.softmax(dim=-1)\n    # Apply attention weights\n    out = einsum('b h i j, b h j d -> b h i d', attn, context_value)\n    # Reshape for final linear layer\n    out = rearrange(out, 'b h n d -> b n (h d)')\n    # Pass through final linear layer\n    return self.to_out(out)\n\n", "CodeBleu_score": 44.80191285714501}, {"code": "def attention_CrossAttention_forward(self, x, context=None, mask=None, **kwargs)\n\t\n    # TODO: add support for context_mask\n    # TODO: add support for attn_mask\n    # TODO: add support for context_key_padding_mask\n    # TODO: add support for need_weights\n    # TODO: add support for attn_mask_hijack\n    # TODO: add support for context_attn_mask_hijack\n    # TODO: add support for context_attn_mask\n    # TODO: add support for output_attn\n    # TODO: add support for output_weights\n    # TODO: add support for output_context\n    # TODO: add support for output_context_weights\n    # TODO: add support for output_attn_weights\n    # TODO: add support for output_context_attn_weights\n    # TODO: add support for output_context_attn\n    # TODO: add support for output_context_attn_weights\n    # TODO: add support for output_context_attn\n    # TODO: add support for output_context_attn_weights\n    # TODO: add support for output_context_attn\n    # TODO: add support for output_context_attn_weights\n    # TODO: add support for output_context_attn\n    # TODO: add support for output_context_attn_weights\n    # TODO: add support for output_context_attn\n    # TODO: add support for output_context_attn_weights\n    # TODO: add support for output_context_attn\n    # TODO: add support for output_context_attn_weights\n    # TODO: add support for output_context_attn\n    # TODO: add support for output_context_attn_weights\n    # TODO: add support for output_context_attn\n    # TODO: add support for output_context_attn_weights\n    # TODO: add support for output_context_attn\n    # TODO: add support for output_context_attn_weights\n    # TODO: add support for output_context_attn\n    # TODO: add support for output_context_attn_weights\n    # TODO: add support for output_context_attn\n    # TODO: add support for output", "CodeBleu_score": 1.9834764933446987}]}
{"instance_id": "8b7455a03a0e", "generation_result": [{"code": "def save_hypernetwork(hypernetwork, checkpoint, hypernetwork_name, filename)\n\t\n    # Backup the original attributes\n    backup_attributes = {}\n    for key in checkpoint.keys():\n        backup_attributes[key] = getattr(hypernetwork, key)\n\n    # Update the hypernetwork with the new checkpoint data\n    for key in checkpoint.keys():\n        setattr(hypernetwork, key, checkpoint[key])\n\n    # Attempt to save the hypernetwork\n    try:\n        torch.save(hypernetwork.state_dict(), filename)\n    except Exception as e:\n        # Restore the original attributes\n        for key in checkpoint.keys():\n            setattr(hypernetwork, key, backup_attributes[key])\n        raise e\n\n    # Restore the original attributes\n    for key in checkpoint.keys():\n        setattr(hypernetwork, key, backup_attributes[key])\n\n", "CodeBleu_score": 22.502184177569568}, {"code": "def save_hypernetwork(hypernetwork, checkpoint, hypernetwork_name, filename)\n\t\n    original_attributes = dict()\n    try:\n        for key, value in checkpoint[hypernetwork_name].items():\n            original_attributes[key] = getattr(hypernetwork, key)\n            setattr(hypernetwork, key, value)\n        torch.save(hypernetwork.state_dict(), filename)\n    except Exception as e:\n        for key, value in original_attributes.items():\n            setattr(hypernetwork, key, value)\n        raise e\n\n", "CodeBleu_score": 23.712309791166998}, {"code": "def save_hypernetwork(hypernetwork, checkpoint, hypernetwork_name, filename)\n\t\n    original_attributes = {}\n    for attribute_name in dir(hypernetwork):\n        if attribute_name[0] != '_':\n            original_attributes[attribute_name] = getattr(hypernetwork, attribute_name)\n            setattr(hypernetwork, attribute_name, getattr(checkpoint, attribute_name))\n    try:\n        torch.save(hypernetwork.state_dict(), filename)\n    except Exception as e:\n        for attribute_name, attribute_value in original_attributes.items():\n            setattr(hypernetwork, attribute_name, attribute_value)\n        raise e\n    for attribute_name, attribute_value in original_attributes.items():\n        setattr(hypernetwork, attribute_name, attribute_value)\n\n", "CodeBleu_score": 25.746767478624218}]}
{"instance_id": "3175f835140f", "generation_result": [{"code": "def ema_scope(self, context=None)\n\t\n    if not hasattr(self, 'ema'):\n        raise RuntimeError('This function requires the model to have an EMA module.')\n    if context is None:\n        context = self.ema_context\n    if context is None:\n        raise RuntimeError('This function requires a context to be passed.')\n    if context:\n        self.ema.store(self.parameters())\n        self.ema.copy_to(self.parameters())\n        if self.log_ema:\n            self.log_ema(True)\n    try:\n        yield\n    finally:\n        if context:\n            self.ema.restore(self.parameters())\n            if self.log_ema:\n                self.log_ema(False)\n\n", "CodeBleu_score": 37.770565071773376}, {"code": "def ema_scope(self, context=None)\n\t\n    if not self.use_ema:\n        yield\n        return\n    if context is None:\n        context = f\"ema_scope_{self.ema_model.updates}\"\n    if self.verbose:\n        print(f\"Switching to EMA weights ({context}).\")\n    self.ema_model.store()\n    self.ema_model.copy_to()\n    try:\n        yield\n    finally:\n        if self.verbose:\n            print(f\"Restoring original weights ({context}).\")\n        self.ema_model.restore()\n\n", "CodeBleu_score": 44.828642903966724}, {"code": "def ema_scope(self, context=None)\n\t\n    if self.ema is not None:\n        if context is None:\n            context = f\"{self.__class__.__name__} context\"\n        self.model.load_state_dict(self.ema.state_dict())\n        self.log(f\"Switched to EMA weights for {context}.\")\n        yield\n        self.model.load_state_dict(self.ema_params)\n        self.log(f\"Restored original weights after {context}.\")\n    else:\n        yield\n\n", "CodeBleu_score": 48.20553167422733}]}
{"instance_id": "a7b5d5cd519a", "generation_result": [{"code": "def p_mean_variance(self, x, t, clip_denoised: bool)\n\t\n    model_out = self(x, t)\n    eps, _, _, model_mean, posterior_variance, posterior_log_variance = self.model_output_to_parameters(model_out)\n\n    # Predict the noise if we're not given it directly.\n    if self.model_output_std == \"learned\":\n        if self.model_output_range is not None:\n            raise ValueError(\"Learned noise level and range are mutually exclusive.\")\n        noise_level = self.model_mean_n2(eps)\n    else:\n        noise_level = self.model_output_std\n\n    if self.model_output_range is not None:\n        noise_level = self.model_output_range * noise_level\n\n    # Optionally clip the reconstructed image to the range -1 to 1.\n    if clip_denoised:\n        model_mean = torch.clamp(model_mean, -1.0, 1.0)\n\n    return model_mean, posterior_variance, posterior_log_variance, noise_level\n\n", "CodeBleu_score": 37.0254848632735}, {"code": "def p_mean_variance(self, x, t, clip_denoised: bool)\n\t\n    assert x.shape == self.shape\n\n    # Get the model output for the input and time step\n    model_output = self.model(x, t)\n\n    # Reconstruct the input either directly or from predicted noise\n    if self.model_output_stds:\n        noise = self.model.get_variance(model_output)\n        if clip_denoised:\n            noise = noise.clamp(-1, 1)\n        x_recon = self.predict_start_from_noise(noise, t)\n    else:\n        x_recon = self.predict_start(model_output, t)\n\n    # Calculate and return the model mean, posterior variance, and posterior log variance\n    model_mean = self.q_posterior(x, x_recon, t)\n    if self.model_output_stds:\n        posterior_variance = self.q_posterior_variance(x, x_recon, t)\n        posterior_log_variance_clipped = self.q_posterior_log_variance_clipped(x, x_recon, t)\n        return model_mean, posterior_variance, posterior_log_variance_clipped\n    else:\n        return model_mean\n\n", "CodeBleu_score": 39.8154348658187}, {"code": "def p_mean_variance(self, x, t, clip_denoised: bool)\n\t\n    assert x.shape == self.shape\n\n    # get the model output for the input and time step\n    model_out = self.model(x, t)\n    if self.model_output_denoised:\n        # if the model output is the denoised input, just return the model output\n        return model_out\n    else:\n        # otherwise, reconstruct the input from the predicted noise\n        noise = self.model.get_variance_from_output(model_out)\n        x_recon = self.model.predict_start_from_noise(x, t, noise)\n        if clip_denoised:\n            x_recon.clamp_(-1., 1.)\n        return x_recon\n\n", "CodeBleu_score": 30.522617326334256}]}
{"instance_id": "a8652fa78473", "generation_result": [{"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss_type == 'l1':\n        if mean:\n            return torch.mean(torch.abs(pred - target))\n        else:\n            return torch.abs(pred - target)\n    elif self.loss_type == 'l2':\n        if mean:\n            return torch.mean((pred - target)**2)\n        else:\n            return (pred - target)**2\n    else:\n        raise ValueError(f\"Unknown loss type: {self.loss_type}\")\n\n", "CodeBleu_score": 33.93271613747534}, {"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss_type == 'l1':\n        loss = (pred - target).abs()\n        if mean:\n            loss = loss.mean()\n        return loss\n    elif self.loss_type == 'l2':\n        loss = (pred - target).pow(2)\n        if mean:\n            loss = loss.mean()\n        return loss\n    else:\n        raise NotImplementedError(f'Unknown loss type {self.loss_type}')\n\n", "CodeBleu_score": 52.30604460780689}, {"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss_type == 'l1':\n        loss = torch.abs(pred - target)\n        if mean:\n            loss = loss.mean()\n    elif self.loss_type == 'l2':\n        loss = torch.pow(pred - target, 2)\n        if not mean:\n            loss = loss.sum(1).mean(0)\n    else:\n        raise ValueError(f'unknown loss type: {self.loss_type}')\n    return loss\n\n", "CodeBleu_score": 52.76502197532751}]}
{"instance_id": "d68e4f061349", "generation_result": [{"code": "def _prior_bpd(self, x_start)\n\t\n    batch_size = x_start.shape[0]\n    t = repeat(self.num_timesteps - 1, 't -> b t', b=batch_size)\n    x_pred = self.p_mean_var(x_start, t)\n    x_pred_dist = DiagonalGaussianDistribution(x_pred)\n    prior_dist = DiagonalGaussianDistribution(\n        torch.zeros_like(x_pred), torch.ones_like(x_pred))\n    kl = normal_kl(x_pred_dist, prior_dist).sum(dim=1)\n    return kl.mean() / float(np.log(2.))\n\n", "CodeBleu_score": 18.70554936757369}, {"code": "def _prior_bpd(self, x_start)\n\t\n    # x_start: (B, C, H, W)\n    # x_t: (B, C, H, W)\n    # t: (B)\n    x_t = self.p_x_start(x_start)\n    t = self.num_timesteps - 1\n    mean, log_var = self.q_x(x_t, t)\n    mean, log_var = mean.mean(dim=0), log_var.mean(dim=0)\n    kl = normal_kl(mean, log_var)\n    return kl.sum() / (mean.shape[0] * np.log(2))\n\n", "CodeBleu_score": 28.24743807126505}, {"code": "def _prior_bpd(self, x_start)\n\t\n    b, *_, device = *x_start.shape, x_start.device\n    t = repeat(self.num_timesteps - 1, '-> b ()', b = b)\n    x_recon = self(x_start, t)\n    x_dist = self.model.first_stage_model(x_recon, t)\n    bpd_loss = normal_kl(x_dist, DiagonalGaussianDistribution(x_recon.shape))\n    return bpd_loss.sum() / b / np.log(2.)\n\n", "CodeBleu_score": 37.21579635627927}]}
{"instance_id": "c695d93fdf2d", "generation_result": [{"code": "def make_cache(subsection: str) -> diskcache.Cache\n\t\n    cache_path = os.path.join(data_path, \"cache\", subsection)\n    cache = diskcache.Cache(cache_path, size_limit=4 * 1024 ** 3, min_file_size=256 * 1024)\n    return cache\n\n", "CodeBleu_score": 49.79282892567368}, {"code": "def make_cache(subsection: str) -> diskcache.Cache\n\t\n    return diskcache.Cache(\n        os.path.join(data_path, \"cache\", subsection),\n        size_limit=4 * 1024 * 1024 * 1024,\n        min_file_size=256 * 1024,\n    )\n\n", "CodeBleu_score": 54.77522694293578}, {"code": "def make_cache(subsection: str) -> diskcache.Cache\n\t\n    cache = diskcache.Cache(\n        os.path.join(data_path, subsection),\n        diskcache.DEFAULT_MAX_DISK_USAGE,\n        diskcache.DEFAULT_MIN_DISK_FREE,\n        diskcache.DEFAULT_FILE_MAX_SIZE,\n        diskcache.DEFAULT_CACHE_NAME,\n        diskcache.DEFAULT_DISK,\n        diskcache.DEFAULT_DISK_MIN_FREE,\n        diskcache.DEFAULT_DISK_PAGE_SIZE,\n        diskcache.DEFAULT_DISK_PAGE_COUNTER,\n        diskcache.DEFAULT_DISK_CACHE_SIZE,\n        diskcache.DEFAULT_DISK_CACHE_MIN_FREE,\n        diskcache.DEFAULT_DISK_CACHE_MAX_FREE,\n        diskcache.DEFAULT_DISK_CACHE_ROUND_UP,\n        diskcache.DEFAULT_DISK_CACHE_ROUND_DOWN,\n        diskcache.DEFAULT_DISK_CACHE_IN_MEMORY,\n        diskcache.DEFAULT_DISK_CACHE_TRUNCATE,\n        diskcache.DEFAULT_DISK_CACHE_LOCK_TIMEOUT,\n        diskcache.DEFAULT_DISK_CACHE_LOCK_REFRESH,\n        diskcache.DEFAULT_DISK_CACHE_LOCK_CHECK,\n        diskcache.DEFAULT_DISK_CACHE_LOCK_STALE,\n        diskcache.DEFAULT_DISK_CACHE_LOCK_RETRY,\n        diskcache.DEFAULT_DISK_CACHE_LOCK_RETRY_TIMEOUT,\n        diskcache.DEFAULT_DISK_CACHE_LOCK_RETRY_DELAY,\n        diskcache.DEFAULT_DISK_CACHE_LOCK_RETRY_JITTER,\n        diskcache.DEFAULT_DISK_CACHE_LOCK_RETRY_MAX_JITTER,\n        diskcache.DEFAULT_DISK_CACHE_LOCK_RETRY_EXPONENTIAL_BACKOFF,\n        diskcache.DEFAULT_DISK_CACHE_LOCK_RETRY", "CodeBleu_score": 39.71487503035364}]}
{"instance_id": "ed4c543f0d38", "generation_result": [{"code": "def cache(subsection)\n\t\n    global caches\n    if subsection in caches:\n        return caches[subsection]\n    else:\n        cache_lock.acquire()\n        if not os.path.isdir(data_path):\n            os.mkdir(data_path)\n        cache_filename = os.path.join(data_path, subsection + '.cache')\n        if not os.path.isfile(cache_filename):\n            convert_old_cached_data()\n        if not os.path.isfile(cache_filename):\n            caches[subsection] = make_cache(subsection)\n        else:\n            caches[subsection] = diskcache.Cache(cache_filename)\n        cache_lock.release()\n        return caches[subsection]\n\n", "CodeBleu_score": 37.81054429690317}, {"code": "def cache(subsection)\n\t\n    if subsection in caches:\n        return caches[subsection]\n    else:\n        cache_lock.acquire()\n        try:\n            if not os.path.isdir(cache_path) or not os.path.isfile(cache_filename):\n                convert_old_cached_data()\n            if subsection not in caches:\n                caches[subsection] = make_cache(subsection)\n        finally:\n            cache_lock.release()\n        return caches[subsection]\n\n", "CodeBleu_score": 40.32499879326114}, {"code": "def cache(subsection)\n\t\n\n    global caches\n    global cache_lock\n\n    if subsection not in caches:\n        with cache_lock:\n            if not os.path.exists(data_path):\n                os.makedirs(data_path)\n            if not os.path.isfile(data_path + subsection + \".json\"):\n                convert_old_cached_data()\n\n            if subsection not in caches:\n                caches[subsection] = make_cache(subsection)\n\n    return caches[subsection]\n\n", "CodeBleu_score": 44.792781807909456}]}
{"instance_id": "ccf5ea2d9fc1", "generation_result": [{"code": "def cached_data_for_file(subsection, title, filename, func)\n\t\n\n    cache = cache_for_subsection(subsection)\n    mtime = os.path.getmtime(filename)\n    if title in cache:\n        if mtime > cache[title][0]:\n            del cache[title]\n            dump_cache(subsection)\n    if title not in cache:\n        data = func()\n        cache[title] = (mtime, data)\n        dump_cache(subsection)\n    return cache[title][1]\n\n", "CodeBleu_score": 16.53673092055183}, {"code": "def cached_data_for_file(subsection, title, filename, func)\n\t\n    cache = cache_for_subsection(subsection)\n    mtime = os.path.getmtime(filename)\n    if title in cache and cache[title]['mtime'] >= mtime:\n        return cache[title]['data']\n    else:\n        data = func()\n        cache[title] = {\n            'mtime': mtime,\n            'data': data\n        }\n        dump_cache(subsection, cache)\n        return data\n\n", "CodeBleu_score": 20.40542420195623}, {"code": "def cached_data_for_file(subsection, title, filename, func)\n\t\n    cache = cache_for_subsection(subsection)\n    file_path = os.path.join(data_path, subsection, filename)\n    mtime = os.path.getmtime(file_path)\n    if title in cache:\n        if cache[title][1] < mtime:\n            del cache[title]\n            dump_cache(subsection)\n    if title not in cache:\n        data = func()\n        cache[title] = (data, mtime)\n        dump_cache(subsection)\n    return cache[title][0]\n\n", "CodeBleu_score": 19.053929252849073}]}
{"instance_id": "e01a2d4f0ac2", "generation_result": [{"code": "def report(message: str, *, exc_info: bool = False) -> None\n\t\n    record_exception()\n    message = textwrap.dedent(message).strip()\n    message = f\"", "CodeBleu_score": 16.867318347588267}, {"code": "def report(message: str, *, exc_info: bool = False) -> None\n\t\n    record_exception()\n    for line in message.splitlines():\n        print(\"***\", line, file=sys.stderr)\n    if exc_info:\n        for line in traceback.format_exc().splitlines():\n            print(\"    \" + line, file=sys.stderr)\n        print(\"---\", file=sys.stderr)\n\n", "CodeBleu_score": 66.3863160302386}, {"code": "def report(message: str, *, exc_info: bool = False) -> None\n\t\n    record_exception()\n    for line in message.splitlines():\n        print(\"***\", line, file=sys.stderr)\n    if exc_info:\n        for line in traceback.format_exc().splitlines():\n            print(\"    \" + line, file=sys.stderr)\n        print(\"---\", file=sys.stderr)\n\n", "CodeBleu_score": 66.3863160302386}]}
{"instance_id": "125fb9d2320b", "generation_result": [{"code": "def extract_original_prompts(style: PromptStyle, prompt, negative_prompt)\n\t\n\n    # Check if style's prompt and negative prompt are empty.\n    if style.prompt == \"\" and style.negative_prompt == \"\":\n        return False, prompt, negative_prompt\n\n    # Extract style text from the positive and negative prompts.\n    try:\n        prompt = extract_text(style.prompt, prompt)\n        negative_prompt = extract_text(style.negative_prompt, negative_prompt)\n    except errors.TextNotFoundError:\n        return False, prompt, negative_prompt\n\n    return True, prompt, negative_prompt\n\n", "CodeBleu_score": 47.82270371592635}, {"code": "def extract_original_prompts(style: PromptStyle, prompt, negative_prompt)\n\t\n\n    # Check if style prompt and negative prompt are empty\n    if style.prompt == \"\" and style.negative_prompt == \"\":\n        return False, prompt, negative_prompt\n\n    # Attempt to extract style text from positive prompt\n    try:\n        prompt = prompt[prompt.index(style.prompt):]\n    except ValueError:\n        return False, prompt, negative_prompt\n\n    # Attempt to extract style text from negative prompt\n    try:\n        negative_prompt = negative_prompt[negative_prompt.index(style.negative_prompt):]\n    except ValueError:\n        return False, prompt, negative_prompt\n\n    return True, prompt, negative_prompt\n\n", "CodeBleu_score": 51.82933976462139}, {"code": "def extract_original_prompts(style: PromptStyle, prompt, negative_prompt)\n\t\n    if style.prompt == \"\" and style.negative_prompt == \"\":\n        return False, prompt, negative_prompt\n    elif style.prompt == \"\":\n        if style.negative_prompt in negative_prompt:\n            negative_prompt = negative_prompt.replace(style.negative_prompt, \"\")\n        else:\n            return False, prompt, negative_prompt\n    elif style.negative_prompt == \"\":\n        if style.prompt in prompt:\n            prompt = prompt.replace(style.prompt, \"\")\n        else:\n            return False, prompt, negative_prompt\n    else:\n        if style.prompt in prompt:\n            prompt = prompt.replace(style.prompt, \"\")\n        else:\n            return False, prompt, negative_prompt\n        if style.negative_prompt in negative_prompt:\n            negative_prompt = negative_prompt.replace(style.negative_prompt, \"\")\n        else:\n            return False, prompt, negative_prompt\n    return True, prompt, negative_prompt\n\n", "CodeBleu_score": 46.915259337532056}]}
{"instance_id": "0c5f729fbce2", "generation_result": [{"code": "def random_partial(self, count, n_frames)\n\t\n    if not self.utterances:\n        self._load_utterances()\n\n    return [(u, u.random_partial(n_frames)) for u in self.utterance_cycler.sample(count)]\n\n", "CodeBleu_score": 42.93472535513073}, {"code": "def random_partial(self, count, n_frames)\n\t\n    if not self.utterances:\n        self._load_utterances()\n\n    return [(u, u.random_partial(n_frames)) for u in self.utterance_cycler.sample(count)]\n\n", "CodeBleu_score": 42.93472535513073}, {"code": "def random_partial(self, count, n_frames)\n\t\n    if not self.utterances:\n        self._load_utterances()\n\n    return [(u, u.random_partial(n_frames)) for u in self.utterance_cycler.sample(count)]\n\n", "CodeBleu_score": 42.93472535513073}]}
{"instance_id": "31e8b27b8f94", "generation_result": [{"code": "def normalize_volume(wav, target_dBFS, increase_only=False, decrease_only=False)\n\t\n    if increase_only and decrease_only:\n        raise ValueError(\"Both increase_only and decrease_only are set\")\n    rms = wav.std()\n    change_in_dBFS = target_dBFS - 20 * np.log10(rms)\n    if (change_in_dBFS < 0 and increase_only) or (change_in_dBFS > 0 and decrease_only):\n        return wav\n    return wav * (10 ** (change_in_dBFS / 20))\n\n", "CodeBleu_score": 66.59805314153188}, {"code": "def normalize_volume(wav, target_dBFS, increase_only=False, decrease_only=False)\n\t\n    if increase_only and decrease_only:\n        raise ValueError(\"Both increase_only and decrease_only are set\")\n    rms = wav.std()\n    change_in_dBFS = target_dBFS - 20 * np.log10(rms)\n    if (change_in_dBFS < 0 and increase_only) or (change_in_dBFS > 0 and decrease_only):\n        return wav\n    return wav * (10 ** (change_in_dBFS / 20))\n\n", "CodeBleu_score": 66.59805314153188}, {"code": "def normalize_volume(wav, target_dBFS, increase_only=False, decrease_only=False)\n\t\n    if increase_only and decrease_only:\n        raise ValueError(\"Both increase only and decrease only are set\")\n    dBFS_change = target_dBFS - 10 * np.log10(np.mean(np.abs(wav) ** 2))\n    if (dBFS_change < 0 and increase_only) or (dBFS_change > 0 and decrease_only):\n        return wav\n    return wav * (10 ** (dBFS_change / 20))\n\n", "CodeBleu_score": 83.71468878119516}]}
{"instance_id": "2353831e9033", "generation_result": [{"code": "def load_model(weights_fpath: Path, device=None)\n\t\n    device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = SpeakerEncoder(device)\n    checkpoint = torch.load(weights_fpath)\n    model.load_state_dict(checkpoint[\"model_state\"])\n    model.eval()\n    print(\"Loaded voice encoder \\\"{}\\\" trained to step {}\".format(weights_fpath, checkpoint[\"step\"]))\n    return model\n\n", "CodeBleu_score": 37.091692696638155}, {"code": "def load_model(weights_fpath: Path, device=None)\n\t\n    device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    global _model\n    _model = SpeakerEncoder(device, **params).to(device)\n    checkpoint = torch.load(weights_fpath, map_location=device)\n    _model.load_state_dict(checkpoint[\"model_state\"])\n    step = checkpoint[\"step\"]\n    _model.eval()\n    print(\"Loaded step %d\" % step)\n\n", "CodeBleu_score": 44.53165747308917}, {"code": "def load_model(weights_fpath: Path, device=None)\n\t\n    device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = SpeakerEncoder(device)\n    checkpoint = torch.load(weights_fpath)\n    model.load_state_dict(checkpoint[\"model_state\"])\n    model.eval()\n    print(\"Loaded pre-trained model for speaker encoder from {}\".format(\n        weights_fpath))\n    print(\"Training step for loaded model: {}\".format(\n        checkpoint[\"step\"]))\n    return model\n\n", "CodeBleu_score": 38.48809300972455}]}
{"instance_id": "3231ad480199", "generation_result": [{"code": "def embed_frames_batch(frames_batch)\n\t\n    if _model is None:\n        raise Exception(\"Model must be loaded before calling embed_frames_batch!\")\n    frames_batch = torch.tensor(frames_batch).to(_device)\n    embeddings = _model(frames_batch).detach().cpu()\n    return embeddings.numpy()\n\n", "CodeBleu_score": 57.808921334428476}, {"code": "def embed_frames_batch(frames_batch)\n\t\n    if not _model:\n        raise Exception(\"Model was not loaded\")\n\n    # Preprocess frames\n    frames_batch = torch.tensor(frames_batch).float()\n    frames_batch = frames_batch.to(_device)\n\n    # Compute embeddings\n    with torch.no_grad():\n        embeddings = _model(frames_batch)\n        embeddings = embeddings.detach().cpu()\n\n    return embeddings.numpy()\n\n", "CodeBleu_score": 53.156472732213004}, {"code": "def embed_frames_batch(frames_batch)\n\t\n    # Check that the model is loaded\n    if not _model:\n        raise Exception(\"Model must be loaded before inference\")\n\n    # Move input to the appropriate device\n    frames_batch = frames_batch.to(_device)\n\n    # Perform a forward pass through the model\n    embeddings = _model(frames_batch)\n\n    # Move embeddings to the CPU and convert them to a numpy array\n    embeddings = embeddings.detach().cpu().numpy()\n\n    return embeddings\n\n", "CodeBleu_score": 48.69745067567206}]}
{"instance_id": "117a41716c8d", "generation_result": [{"code": "def compute_partial_slices(n_samples, partial_utterance_n_frames=partials_n_frames,\n                           min_pad_coverage=0.75, overlap=0.5)\n\t\n    assert 0 <= overlap < 1\n    assert 0 <= min_pad_coverage <= 1\n    assert partial_utterance_n_frames % 2 == 1\n\n    frame_step = int((1 - overlap) * partial_utterance_n_frames)\n    n_frames = int(np.ceil(n_samples / hop_length))\n\n    # Compute the slices\n    wav_slices, mel_slices = [], []\n    steps = max(1, int(np.round(n_frames / partial_utterance_n_frames)))\n    for i in range(steps):\n        mel_range = np.array([i * frame_step, i * frame_step + partial_utterance_n_frames])\n        mel_range = mel_range.clip(0, n_frames)\n        wav_range = mel_range * hop_length\n\n        mel_slices.append(slice(*mel_range))\n        wav_slices.append(slice(*wav_range))\n\n    # Evaluate whether extra padding is necessary\n    last_wav_range = wav_slices[-1]\n    coverage = (n_samples - last_wav_range.start) / (last_wav_range.stop - last_wav_range.start)\n    if coverage < min_pad_coverage and len(mel_slices) > 1:\n        wav_slices = wav_slices[:-1]\n        mel_slices = mel_slices[:-1]\n\n    return wav_slices, mel_slices\n\n", "CodeBleu_score": 62.84935365432861}, {"code": "def compute_partial_slices(n_samples, partial_utterance_n_frames=partials_n_frames,\n                           min_pad_coverage=0.75, overlap=0.5)\n\t\n    assert 0 <= overlap < 1\n    assert 0 <= min_pad_coverage <= 1\n\n    frame_step = int((sampling_rate * overlap) // hop_length)\n    n_frames = int((n_samples + hop_length * (partial_utterance_n_frames - 1)) // hop_length)\n    mel_win = int(sampling_rate * partial_utterance_n_frames / 1000)\n    mel_hop = int(sampling_rate * partial_utterance_n_frames * (1 - overlap) / 1000)\n\n    wav_slices, mel_slices = [], []\n    steps = max(1, n_frames - partial_utterance_n_frames + frame_step + 1)\n    for i in range(0, steps, frame_step):\n        mel_range = np.array([i * mel_hop, i * mel_hop + mel_win])\n        wav_range = np.array([i * hop_length, i * hop_length + mel_win * 1000 / sampling_rate])\n\n        mel_slices.append(mel_range)\n        wav_slices.append(wav_range)\n\n    last_wav_range = wav_slices[-1]\n    coverage = (n_samples - last_wav_range[0]) / (last_wav_range[1] - last_wav_range[0])\n    if coverage < min_pad_coverage and len(mel_slices) > 1:\n        wav_slices = wav_slices[:-1]\n        mel_slices = mel_slices[:-1]\n\n    return wav_slices, mel_slices\n\n", "CodeBleu_score": 58.31268276995355}, {"code": "def compute_partial_slices(n_samples, partial_utterance_n_frames=partials_n_frames,\n                           min_pad_coverage=0.75, overlap=0.5)\n\t\n    assert 0 <= min_pad_coverage <= 1\n    assert 0 <= overlap < 1\n\n    # Compute the number of frames and frame step\n    frames_per_partial = partial_utterance_n_frames\n    frame_step = int(partial_utterance_n_frames * (1 - overlap))\n\n    # Compute the number of partials\n    n_partials = int(np.ceil((n_samples - frames_per_partial) / frame_step)) + 1\n\n    # Compute the wav and mel slices\n    wav_slices, mel_slices = [], []\n    for i in range(n_partials):\n        start = i * frame_step\n        end = start + frames_per_partial\n        wav_slices.append((start, end))\n        mel_slices.append((start * hop_length, end * hop_length))\n\n    # Evaluate whether extra padding is needed for the last partial\n    coverage = (n_samples - wav_slices[-1][0]) / (wav_slices[-1][1] - wav_slices[-1][0])\n    if coverage < min_pad_coverage and len(mel_slices) > 1:\n        wav_slices = wav_slices[:-1]\n        mel_slices = mel_slices[:-1]\n\n    return wav_slices, mel_slices\n\n", "CodeBleu_score": 42.54534774171814}]}
{"instance_id": "34fe5c864f1c", "generation_result": [{"code": "def plot_embedding_as_heatmap(embed, ax=None, title=\"\", shape=None, color_range=(0, 0.30))\n\t\n    if ax is None:\n        fig, ax = plt.subplots()\n    if shape is None:\n        shape = int(np.sqrt(len(embed)))\n    embed = embed.reshape(shape, shape)\n    cmap = cm.get_cmap()\n    cmap.set_bad(color=\"black\")\n    img = ax.imshow(embed, cmap=cmap, vmin=color_range[0], vmax=color_range[1])\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.set_title(title)\n    plt.colorbar(img)\n    return ax\n\n", "CodeBleu_score": 51.989732218274}, {"code": "def plot_embedding_as_heatmap(embed, ax=None, title=\"\", shape=None, color_range=(0, 0.30))\n\t\n    if ax is None:\n        ax = plt.gca()\n    if shape is None:\n        shape = int(np.sqrt(len(embed)))\n    embed = embed.reshape(shape, shape)\n    cmap = cm.get_cmap('RdBu_r')\n    cnorm = colors.Normalize(vmin=color_range[0], vmax=color_range[1])\n    scalarmappaple = cm.ScalarMappable(norm=cnorm, cmap=cmap)\n    ax.imshow(embed, cmap=cmap)\n    ax.set_aspect('equal')\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.set_title(title)\n    ax.figure.colorbar(scalarmappaple)\n\n", "CodeBleu_score": 60.51805687844746}, {"code": "def plot_embedding_as_heatmap(embed, ax=None, title=\"\", shape=None, color_range=(0, 0.30))\n\t\n    if ax is None:\n        ax = plt.gca()\n    if shape is None:\n        shape = int(np.sqrt(len(embed)))\n    embed = embed.reshape(shape, shape)\n    cmap = cm.get_cmap('RdBu_r')\n    cnorm = colors.Normalize(vmin=color_range[0], vmax=color_range[1])\n    scalarmappaple = cm.ScalarMappable(norm=cnorm, cmap=cmap)\n    ax.imshow(embed, cmap=cmap)\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.set_title(title)\n    ax.set_aspect('equal')\n    plt.colorbar(scalarmappaple)\n\n", "CodeBleu_score": 60.942552428182196}]}
{"instance_id": "92e93d50b62b", "generation_result": [{"code": "def similarity_matrix(self, embeds)\n\t\n    num_speakers = embeds.shape[0]\n    num_utts = embeds.shape[1]\n\n    centroids_incl = torch.zeros(num_speakers, num_utts, self.speaker_embedding_size, device=self.device)\n    centroids_excl = torch.zeros(num_speakers, num_utts, self.speaker_embedding_size, device=self.device)\n\n    for i in range(num_speakers):\n        centroids_incl[i] = torch.mean(embeds[i], dim=0, keepdim=True)\n        centroids_excl[i] = torch.mean(embeds[i], dim=0, keepdim=True)\n\n    centroids_incl = nn.functional.normalize(centroids_incl, p=2, dim=2)\n    centroids_excl = nn.functional.normalize(centroids_excl, p=2, dim=2)\n\n    sim_matrix = torch.zeros(num_speakers, num_utts, num_utts, device=self.device)\n    mask = torch.eye(num_utts, device=self.device)\n\n    for i in range(num_speakers):\n        sim_matrix[i] = torch.matmul(centroids_incl[i], centroids_excl[i].T)\n        sim_matrix[i] -= mask * 1e12\n\n    sim_matrix = sim_matrix.view(-1, num_utts)\n    sim_matrix = nn.functional.softmax(sim_matrix, dim=1)\n    sim_matrix = sim_matrix.view(num_speakers, num_utts, num_utts)\n\n    sim_matrix = (sim_matrix + sim_matrix.T) / 2\n\n    sim_matrix = sim_matrix * (1 - mask)\n    sim_matrix = sim_matrix * 20\n    sim_matrix = sim_matrix - mask * 1e12\n\n   ", "CodeBleu_score": 36.085273763869445}, {"code": "def similarity_matrix(self, embeds)\n\t\n    n_speakers = embeds.shape[0]\n    n_utts = embeds.shape[1]\n    centroids_incl = torch.zeros(n_speakers, n_utts, self.embedding_size).to(self.device)\n    centroids_excl = torch.zeros(n_speakers, n_utts, self.embedding_size).to(self.device)\n    for i in range(n_speakers):\n        for j in range(n_utts):\n            centroids_incl[i, j, :] = embeds[i, j, :]\n            centroids_excl[i, j, :] = torch.mean(torch.cat((embeds[i, :j, :], embeds[i, j+1:, :]), dim=0), dim=0)\n    centroids_incl = F.normalize(centroids_incl, p=2, dim=2)\n    centroids_excl = F.normalize(centroids_excl, p=2, dim=2)\n    sim_matrix = torch.zeros(n_speakers, n_utts, n_utts).to(self.device)\n    mask = torch.eye(n_utts).bool().to(self.device)\n    for i in range(n_speakers):\n        sim_matrix[i, :, :] = torch.mm(centroids_incl[i, :, :], centroids_excl[i, :, :].T)\n        sim_matrix[i, :, :][mask] = -1000\n    sim_matrix = sim_matrix * self.scale + self.bias\n    return sim_matrix\n\n", "CodeBleu_score": 35.517292476386956}, {"code": "def similarity_matrix(self, embeds)\n\t\n\n    # Calculate the number of speakers and utterances per speaker\n    n_spkrs, n_utts = embeds.shape[:2]\n\n    # Initialize a similarity matrix\n    sim_matrix = torch.zeros(n_spkrs, n_spkrs, device=self.device)\n\n    # Initialize a mask matrix for excluding self-similarity\n    mask = torch.ones(n_spkrs, n_spkrs, device=self.device)\n    mask = mask.fill_diagonal_(0)\n\n    # Calculate the centroids\n    centroids_incl = embeds.mean(dim=1, keepdim=True)\n    centroids_excl = (\n        embeds.sum(dim=1, keepdim=True) - embeds\n    ) / (n_utts - 1)\n\n    # Calculate the similarities for each speaker\n    for i in range(n_spkrs):\n        # Calculate the similarities for the current speaker\n        sim_matrix[i] = self.similarity_func(\n            embeds[i], centroids_incl[i], centroids_excl[i]\n        )\n\n    # Scale and bias the similarity matrix\n    sim_matrix = sim_matrix / self.scale\n    sim_matrix = sim_matrix + 1\n\n    # Apply the mask\n    sim_matrix = sim_matrix * mask\n\n    return sim_matrix\n\n", "CodeBleu_score": 26.72384096674132}]}
{"instance_id": "a6eba54108a1", "generation_result": [{"code": "def loss(self, embeds)\n\t\n    self.reset_gradients()\n    spker_embeds, utt_embeds = embeds\n    spker_embeds = spker_embeds.detach()\n    utt_embeds = utt_embeds.detach()\n    num_spkers = spker_embeds.shape[0]\n    num_utts = utt_embeds.shape[0]\n    num_utts_per_spker = num_utts // num_spkers\n    spker_embeds = spker_embeds.reshape(num_spkers, num_utts_per_spker, -1)\n    spker_embeds = spker_embeds.mean(dim=1)\n    spker_embeds = F.normalize(spker_embeds, dim=1)\n    utt_embeds = F.normalize(utt_embeds, dim=1)\n    sim_matrix = torch.mm(spker_embeds, utt_embeds.t())\n    sim_matrix = sim_matrix.reshape(num_spkers, num_utts_per_spker, -1)\n    labels = torch.from_numpy(np.asarray(range(num_spkers))).long().to(self.device)\n    labels = labels.repeat_interleave(num_utts_per_spker)\n    loss = self.loss_function(sim_matrix, labels)\n    eer = self.compute_eer(sim_matrix, labels)\n    return loss, eer\n\n", "CodeBleu_score": 18.88241267888079}, {"code": "def loss(self, embeds)\n\t\n\n    # Get the number of speakers and utterances per speaker from the shape of the embeddings\n    num_speakers = embeds.shape[0]\n    num_utterances = embeds.shape[1]\n\n    # Compute the similarity matrix\n    similarity_matrix = self.compute_similarity_matrix(embeds)\n\n    # Reshape the similarity matrix to a 1D vector\n    similarity_matrix = similarity_matrix.view(-1)\n\n    # Generate the ground truth labels\n    labels = torch.from_numpy(self.generate_labels(num_speakers, num_utterances))\n\n    # Compute the loss\n    loss = self.loss_fn(similarity_matrix, labels)\n\n    # Compute the Equal Error Rate (EER)\n    eer = self.compute_eer(similarity_matrix, labels)\n\n    return loss, eer\n\n", "CodeBleu_score": 7.744210078378172}, {"code": "def loss(self, embeds)\n\t\n    self.similarity_matrix = self.similarity_matrix(embeds)\n    self.labels = self.labels(embeds)\n    self.loss = self.loss_fn(self.similarity_matrix, self.labels)\n    self.eer = self.get_eer(self.similarity_matrix, self.labels)\n    return self.loss, self.eer\n\n", "CodeBleu_score": 3.156242124037595}]}
{"instance_id": "ac7b74b9d1b1", "generation_result": [{"code": "def log_params(self)\n\t\n    if not self.no_visdom:\n        # Prepare a string to display the model parameters\n        model_params = \"Model Parameters", "CodeBleu_score": 1.8757491470599634}, {"code": "def log_params(self)\n\t\n    if not self.disable_visualization:\n        self.visualizer.log_parameters(self.model_parameters, self.data_parameters)\n\n", "CodeBleu_score": 3.180700904004532}, {"code": "def log_params(self)\n\t\n    if self.visualization_disabled:\n        return\n    self.visualization_window_params = self.visualization_window_params or self.visualizer.text(\"Parameters\", opts=dict(title=\"Parameters\"))\n\n    # Iterate through the model and data parameters, appending each name and value to a formatted string\n    param_string = \"\"\n    for key, value in self.model_parameters.items():\n        param_string += \"{}: {}", "CodeBleu_score": 15.163041521281103}]}
{"instance_id": "4726b7864386", "generation_result": [{"code": "def draw_projections(self, embeds, utterances_per_speaker, step, out_fpath=None, max_speakers=10)\n\t\n\n    # Select the speakers to display\n    n_speakers = min(len(self.speaker_encoder.speakers), max_speakers)\n    speaker_names = self.speaker_encoder.speakers[:n_speakers]\n\n    # Create the embedding matrix\n    n_utts_per_speaker = [len(utterances) for speaker_name, utterances in utterances_per_speaker.items()]\n    max_utts = max(n_utts_per_speaker)\n    n_speakers = len(speaker_names)\n    speaker_embeddings = np.zeros((n_speakers * max_utts, self.speaker_encoder.embedding_size), np.float32)\n    speaker_labels = np.zeros(n_speakers * max_utts, np.int32)\n    speaker_utterance_indices = np.zeros(n_speakers * max_utts, np.int32)\n    for i, speaker_name in enumerate(speaker_names):\n        speaker_embeddings[i * max_utts: (i + 1) * max_utts] = np.vstack(utterances_per_speaker[speaker_name])[:max_utts]\n        speaker_labels[i * max_utts: (i + 1) * max_utts] = i\n        speaker_utterance_indices[i * max_utts: (i + 1) * max_utts] = np.arange(max_utts)\n\n    # Project the speaker embeddings to 2D\n    reducer = umap.UMAP(int(self.hparams.projection_dimensions), metric='cosine', n_epochs=500)\n    projection = reducer.fit_transform(speaker_embeddings)\n\n    # Visualize the projections\n    vis_path = self.vis_dir.joinpath(\"projections.png\")\n    self.plot_projections(projection, speaker_labels, speaker_utterance_indices, speaker_names, vis_path, step)\n    if out_f", "CodeBleu_score": 34.29325582998174}, {"code": "def draw_projections(self, embeds, utterances_per_speaker, step, out_fpath=None, max_speakers=10)\n\t\n\n    # Select a subset of the speakers to plot\n    speaker_ids = list(utterances_per_speaker.keys())[:max_speakers]\n    selected_embeddings = []\n    for speaker_id in speaker_ids:\n        utterances = utterances_per_speaker[speaker_id]\n        utterances = [embeds[utterance_id] for utterance_id in utterances]\n        selected_embeddings.extend(utterances)\n    selected_embeddings = [emb.detach().cpu().numpy() for emb in selected_embeddings]\n\n    # Perform UMAP projection\n    reducer = umap.UMAP(random_state=42)\n    projection = reducer.fit_transform(selected_embeddings)\n\n    # Assign a color to each speaker\n    speaker_colors = {speaker_id: i for i, speaker_id in enumerate(speaker_ids)}\n    speaker_colors = {speaker_id: color for speaker_id, color in speaker_colors.items()}\n    colors = [speaker_colors[emb.utterance.speaker_id] for emb in selected_embeddings]\n\n    # Draw the projection\n    fig = self.visdom.figure(title=f\"Speaker Embeddings (step {step})\")\n    self.visdom.scatter(projection[:, 0], projection[:, 1], win=fig, opts=dict(\n        legend=list(speaker_colors.keys()),\n        markersize=1,\n        markercolor=colors,\n        xlabel=\"UMAP dim 0\",\n        ylabel=\"UMAP dim 1\",\n        ytype=\"log\",\n        xaxis=\"true\",\n        yaxis=\"true\",\n    ))\n\n    # Save to disk\n    if out_fpath is not None:\n        self.visdom.save([fig])\n\n", "CodeBleu_score": 40.116893247547814}, {"code": "def draw_projections(self, embeds, utterances_per_speaker, step, out_fpath=None, max_speakers=10)\n\t\n    # Select a subset of speakers\n    max_speakers = min(len(embeds), max_speakers)\n    selected_speakers = np.random.choice(list(embeds.keys()), size=max_speakers, replace=False)\n    flat_embeds = np.array([embed for speaker, embed in embeds.items() if speaker in selected_speakers])\n    flat_utterances = np.array([utterances_per_speaker[speaker] for speaker in selected_speakers])\n\n    # Project the embeddings using UMAP\n    start = timer()\n    reducer = umap.UMAP(n_neighbors=10, min_dist=0.0, metric=\"cosine\")\n    projected = reducer.fit_transform(flat_embeds)\n    end = timer()\n    print(\"UMAP projection took %.2f seconds\" % (end - start))\n\n    # Prepare the visualization\n    vmin, vmax = 0, np.amax(flat_utterances)\n    speaker_colors = {speaker: i / len(selected_speakers) for i, speaker in enumerate(selected_speakers)}\n    speaker_labels = {speaker: speaker.name for speaker in selected_speakers}\n\n    # Draw and show/save the projection\n    fig, ax = plt.subplots(figsize=(16, 10))\n    im = ax.scatter(projected[:, 0], projected[:, 1], c=flat_utterances, cmap=\"magma\", vmin=vmin, vmax=vmax)\n    legend_labels = {speaker: speaker_labels[speaker] for speaker in selected_speakers}\n    legend = ax.legend(im.legend_elements()[0], legend_labels.values(), loc=\"upper right\", title=\"Speakers\")\n    ax.add_artist(legend)\n    ax.set_title(\"UMAP projection, step %d\" % step)\n    fig.colorbar(im, fraction=0.027, pad=0.04)\n    if out_fpath:\n        plt.savefig(out_fpath)\n        plt.", "CodeBleu_score": 29.86385122569068}]}
{"instance_id": "274fa50985c0", "generation_result": [{"code": "def add_speaker_embedding(self, x, speaker_embedding)\n\t\n    # Save the dimensions of the input tensor as human-readable names\n    batch_size, sequence_length, embedding_dimension = x.shape\n\n    # Determine the appropriate dimension index for the speaker embedding\n    if self.speaker_embedding_dimension == \"sequence\":\n        speaker_embedding_dimension_index = 1\n    elif self.speaker_embedding_dimension == \"embedding\":\n        speaker_embedding_dimension_index = 2\n    else:\n        raise ValueError(f\"Speaker embedding dimension '{self.speaker_embedding_dimension}' not supported.\")\n\n    # Repeat the speaker embedding to match the length of the input text, reshape, and transpose it\n    speaker_embedding = speaker_embedding.repeat(sequence_length, 1)\n    speaker_embedding = speaker_embedding.reshape(batch_size, sequence_length, -1)\n    speaker_embedding = speaker_embedding.transpose(1, speaker_embedding_dimension_index)\n\n    # Concatenate the speaker embedding with the input tensor along the last dimension and return the result\n    return torch.cat((x, speaker_embedding), dim=speaker_embedding_dimension_index)\n\n", "CodeBleu_score": 25.7020737488411}, {"code": "def add_speaker_embedding(self, x, speaker_embedding)\n\t\n    if speaker_embedding is not None:\n        if self.speaker_embedding_type == \"spk_emb\":\n            if speaker_embedding.dim() == 2:\n                speaker_embedding = speaker_embedding.unsqueeze(1)\n            speaker_embedding = speaker_embedding.expand(\n                -1, x.size(1), -1\n            )  # expand speaker embedding to match text length\n            x = torch.cat([x, speaker_embedding], dim=-1)\n        elif self.speaker_embedding_type == \"spk_id\":\n            x = torch.cat([x, speaker_embedding.unsqueeze(-1)], dim=-1)\n        else:\n            raise ValueError(\n                \"speaker embedding type should be either 'spk_emb' or 'spk_id'.\"\n            )\n    return x\n\n", "CodeBleu_score": 30.59247789365651}, {"code": "def add_speaker_embedding(self, x, speaker_embedding)\n\t\n    if speaker_embedding is None:\n        return x\n    if not isinstance(x, torch.Tensor):\n        x = torch.tensor(x)\n    if len(x.shape) == 3:\n        x = x.transpose(1, 2)\n    elif len(x.shape) == 2:\n        x = x.unsqueeze(1)\n    if len(x.shape) != 3:\n        raise ValueError(f\"Input tensor must be 3 dimensional, not {len(x.shape)}\")\n    if x.shape[1] != speaker_embedding.shape[0]:\n        raise ValueError(\n            f\"Input tensor must have same sequence length as speaker embedding, not {x.shape[1]} and {speaker_embedding.shape[0]}\"\n        )\n    x_dims = {\n        \"batch_size\": x.shape[0],\n        \"sequence_length\": x.shape[1],\n        \"embedding_dimension\": x.shape[2],\n    }\n    speaker_embedding_dims = {\n        \"batch_size\": speaker_embedding.shape[0],\n        \"sequence_length\": speaker_embedding.shape[1],\n        \"embedding_dimension\": speaker_embedding.shape[2],\n    }\n    if x_dims[\"batch_size\"] != speaker_embedding_dims[\"batch_size\"]:\n        raise ValueError(\n            f\"Input tensor and speaker embedding must have same batch size, not {x_dims['batch_size']} and {speaker_embedding_dims['batch_size']}\"\n        )\n    if x_dims[\"embedding_dimension\"] != speaker_embedding_dims[\"embedding_dimension\"]:\n        raise ValueError(\n            f\"Input tensor and speaker embedding must have same embedding dimension, not {x_dims['embedding_dimension']} and {speaker_embedding_dims['embedding_dimension']}\"\n        )\n    if x_dims[\"sequence_length\"] != speaker_embedding_dims[\"sequence_length\"]:\n        raise ValueError(\n            f\"Input", "CodeBleu_score": 24.536304198471313}]}
{"instance_id": "e6742ea0fe5e", "generation_result": [{"code": "def init_attention(self, encoder_seq_proj)\n\t\n    self.device = next(self.parameters()).device\n    self.batch_size, self.seq_len, self.encoder_size = encoder_seq_proj.shape\n    self.cumulative_weights = torch.zeros((self.batch_size, self.seq_len), device=self.device)\n    self.attention_weights = torch.zeros((self.batch_size, self.seq_len), device=self.device)\n\n", "CodeBleu_score": 23.89755251209772}, {"code": "def init_attention(self, encoder_seq_proj)\n\t\n    self.device = next(self.parameters()).device\n    self.seq_proj_size = encoder_seq_proj.size(2)\n    self.cumulative_weights = torch.zeros(\n        encoder_seq_proj.size(0), encoder_seq_proj.size(1), self.seq_proj_size\n    ).to(self.device)\n    self.attention_weights = torch.zeros(\n        encoder_seq_proj.size(0), encoder_seq_proj.size(1), self.seq_proj_size\n    ).to(self.device)\n\n", "CodeBleu_score": 17.334928498577966}, {"code": "def init_attention(self, encoder_seq_proj)\n\t\n    self.device = next(self.parameters()).device\n    self.seq_proj_shape = encoder_seq_proj.shape\n    self.batch_size, self.seq_len = self.seq_proj_shape[0], self.seq_proj_shape[1]\n\n    self.cumulative_weights = torch.zeros(self.batch_size, self.seq_len).to(self.device)\n    self.attention_weights = torch.zeros(self.batch_size, self.seq_len).to(self.device)\n\n", "CodeBleu_score": 17.03822161795915}]}
{"instance_id": "4e1fc1ed0ed7", "generation_result": [{"code": "def load(self, path, optimizer=None)\n\t\n\n    path = Path(path)\n    if not path.exists():\n        raise FileNotFoundError(f\"The path {path} does not exist.\")\n\n    checkpoint = torch.load(path, map_location=self.device)\n    self.load_state_dict(checkpoint[\"state_dict\"])\n\n    if optimizer is not None and \"optimizer\" in checkpoint:\n        optimizer.load_state_dict(checkpoint[\"optimizer\"])\n\n", "CodeBleu_score": 43.91227943440259}, {"code": "def load(self, path, optimizer=None)\n\t\n    path = Path(path)\n    if not path.exists():\n        raise FileNotFoundError(f\"The checkpoint path {path} does not exist.\")\n    if not path.is_file():\n        raise TypeError(f\"The checkpoint path {path} is not a file.\")\n    if not path.suffix == \".pth\":\n        raise TypeError(f\"The checkpoint path {path} does not have a .pth extension.\")\n\n    checkpoint = torch.load(path, map_location=self.device)\n    self.load_state_dict(checkpoint[\"state_dict\"])\n\n    if optimizer is not None:\n        optimizer.load_state_dict(checkpoint[\"optimizer\"])\n\n    return checkpoint\n\n", "CodeBleu_score": 40.81907722630101}, {"code": "def load(self, path, optimizer=None)\n\t\n    if not isinstance(path, Path):\n        path = Path(path)\n    if not path.exists():\n        raise FileNotFoundError(f\"The path {path} does not exist.\")\n    device = next(self.parameters()).device\n    checkpoint = torch.load(path, map_location=device)\n    self.load_state_dict(checkpoint[\"state_dict\"])\n    if optimizer is not None and \"optimizer_state_dict\" in checkpoint:\n        optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n    return self\n\n", "CodeBleu_score": 61.0437392674091}]}
{"instance_id": "178e6aa16c0f", "generation_result": [{"code": "def save(self, path, optimizer=None)\n\t\n    if isinstance(path, str):\n        path = Path(path)\n\n    if optimizer is not None:\n        torch.save({\"model_state_dict\": self.state_dict(), \"optimizer_state_dict\": optimizer.state_dict()}, path)\n    else:\n        torch.save(self.state_dict(), path)\n\n", "CodeBleu_score": 60.468114484875024}, {"code": "def save(self, path, optimizer=None)\n\t\n    if not os.path.exists(os.path.dirname(path)):\n        os.makedirs(os.path.dirname(path))\n    if optimizer:\n        torch.save({'model_state_dict': self.state_dict(), 'optimizer_state_dict': optimizer.state_dict()}, path)\n    else:\n        torch.save(self.state_dict(), path)\n\n", "CodeBleu_score": 55.74301357551585}, {"code": "def save(self, path, optimizer=None)\n\t\n\n    if optimizer is not None:\n        torch.save({\n            'model_state_dict': self.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict()\n        }, path)\n    else:\n        torch.save(self.state_dict(), path)\n\n", "CodeBleu_score": 63.81618480133533}]}
{"instance_id": "6528fd80e87b", "generation_result": [{"code": "def english_cleaners(text)\n\t\n    text = convert_to_ascii(text)\n    text = lowercase(text)\n    text = remove_empty_lines(text)\n    text = collapse_whitespace(text)\n    return text\n\n", "CodeBleu_score": 39.40477979186552}, {"code": "def english_cleaners(text)\n\t\n    text = convert_to_ascii(text)\n    text = lowercase(text)\n    text = expand_numbers(text)\n    text = collapse_whitespace(text)\n    return text\n\n", "CodeBleu_score": 44.3918781219196}, {"code": "def english_cleaners(text)\n\t\n    text = convert_to_ascii(text)\n    text = collapse_whitespace(text)\n    return text\n\n", "CodeBleu_score": 22.48650305096744}]}
{"instance_id": "8a4fe08e0985", "generation_result": [{"code": "def text_to_sequence(text, cleaner_names)\n\t\n    sequence = []\n    # Check for curly braces and treat their contents as ARPAbet:\n    while len(text):\n        m = re.match(r\"(.*?)\\{(.+?)\\}(.*)\", text)\n        if not m:\n            sequence += _symbols_to_sequence(_clean_text(text, cleaner_names))\n            break\n        sequence += _symbols_to_sequence(_clean_text(m.group(1), cleaner_names))\n        sequence += _arpabet_to_sequence(m.group(2))\n        text = m.group(3)\n    # Append EOS token\n    sequence.append(symbols.index(\"EOS\"))\n    return sequence\n\n", "CodeBleu_score": 76.7062565475911}, {"code": "def text_to_sequence(text, cleaner_names)\n\t\n    sequence = []\n\n    # Check for curly braces and treat their contents as ARPAbet:\n    while len(text):\n        m = re.match(r\"(.*?)\\{(.+?)\\}(.*)\", text)\n        if not m:\n            sequence += _symbols_to_sequence(_clean_text(text, cleaner_names))\n            break\n        sequence += _symbols_to_sequence(_clean_text(m.group(1), cleaner_names))\n        sequence += _arpabet_to_sequence(m.group(2))\n        text = m.group(3)\n\n    # Append EOS token\n    sequence.append(symbols.index(\"EOS\"))\n    return sequence\n\n", "CodeBleu_score": 76.7062565475911}, {"code": "def text_to_sequence(text, cleaner_names)\n\t\n    sequence = []\n    # Check for curly braces and treat their contents as ARPAbet:\n    while len(text):\n        m = re.match(r\"^(.*?)\\{(.+?)\\}(.*)$\", text)\n        if not m:\n            sequence += _symbols_to_sequence(_clean_text(text, cleaner_names))\n            break\n        sequence += _symbols_to_sequence(_clean_text(m.group(1), cleaner_names))\n        sequence += _arpabet_to_sequence(m.group(2))\n        text = m.group(3)\n    # Append EOS token\n    sequence.append(symbols.index(\"EOS\"))\n    return sequence\n\n", "CodeBleu_score": 76.60057179462588}]}
{"instance_id": "e1c25a37af1f", "generation_result": [{"code": "def sequence_to_text(sequence)\n\t\n    result = \"\"\n    for symbol_id in sequence:\n        if symbol_id in symbols:\n            s = symbols[symbol_id]\n            if len(s) > 1:\n                s = \"{%s}\" % s\n            result += s\n    result = result.replace(\"}{\", \" \")\n    return result\n\n", "CodeBleu_score": 52.63323050681999}, {"code": "def sequence_to_text(sequence)\n\t\n    result = ''\n    for symbol_id in sequence:\n        if symbol_id in symbols:\n            s = symbols[symbol_id]\n            if len(s) > 1:\n                s = '{%s}' % s\n            result += s\n    result = re.sub(r'\\{[^\\}]*\\}', ' ', result)\n    result = result.replace('}{', ' ')\n    result = result.strip()\n    return result\n\n", "CodeBleu_score": 45.80078155697904}, {"code": "def sequence_to_text(sequence)\n\t\n    result = \"\"\n    for symbol_id in sequence:\n        if symbol_id in symbols:\n            s = symbols[symbol_id]\n            if len(s) > 1:\n                s = \"{%s}\" % s\n            result += s\n    result = result.replace(\"}{\", \" \")\n    return result.strip()\n\n", "CodeBleu_score": 51.63219276314456}]}
{"instance_id": "7807cad4b05d", "generation_result": [{"code": "def embed_utterance(fpaths, encoder_model_fpath)\n\t\n    if not encoder.is_loaded():\n        encoder.load_model(encoder_model_fpath)\n\n    for fpath in fpaths:\n        wav_fpath = Path(fpath).with_suffix(\".wav\")\n        if not wav_fpath.exists():\n            raise FileNotFoundError(\"The specified file does not exist: {}\".format(wav_fpath))\n\n        wav = audio.load_wav(wav_fpath)\n        preprocessed_wav = encoder.preprocess_wav(wav)\n        embed = encoder.embed_utterance(preprocessed_wav)\n        np_path = str(fpath) + \".npy\"\n        np.save(np_path, embed, allow_pickle=False)\n\n", "CodeBleu_score": 42.80329246420326}, {"code": "def embed_utterance(fpaths, encoder_model_fpath)\n\t\n    if not encoder.is_loaded():\n        encoder.load_model(encoder_model_fpath)\n\n    for fpath in fpaths:\n        wav_fpath, embed_fpath = fpath\n        wav = audio.load_wav(wav_fpath, encode=False)\n        wav = wav / np.abs(wav).max() * 0.999\n        embed = encoder.embed_utterance(wav)\n        np.save(embed_fpath, embed, allow_pickle=False)\n\n", "CodeBleu_score": 55.462637408162585}, {"code": "def embed_utterance(fpaths, encoder_model_fpath)\n\t\n    if not encoder.is_loaded():\n        encoder.load_model(encoder_model_fpath)\n\n    wav_fpath, embed_fpath = fpaths\n    wav = audio.load_wav(wav_fpath, encode=False)\n    wav = wav / max(0.01, np.max(np.abs(wav)))\n    embed = encoder.embed_utterance(wav)\n    np.save(embed_fpath, embed, allow_pickle=False)\n\n", "CodeBleu_score": 77.27717038435243}]}
{"instance_id": "9f860273a325", "generation_result": [{"code": "def create_embeddings(synthesizer_root: Path, encoder_model_fpath: Path, n_processes: int)\n\t\n\n    # Verify the existence of directories and files\n    synthesizer_root = Path(synthesizer_root).expanduser()\n    assert synthesizer_root.exists()\n    metadata_fpath = synthesizer_root.joinpath(\"train.txt\")\n    assert metadata_fpath.exists()\n    embed_dir = synthesizer_root.joinpath(\"embeds\")\n    embed_dir.mkdir(exist_ok=True)\n    encoder_model_fpath = Path(encoder_model_fpath).expanduser()\n    assert encoder_model_fpath.exists()\n\n    # Read metadata\n    with metadata_fpath.open(\"r\", encoding=\"utf-8\") as metadata_file:\n        metadata = [line.split(\"|\") for line in metadata_file]\n        frame_shift_ms = float(metadata[0][4])\n        sampling_rate = int(metadata[0][2])\n        hop_length = int(frame_shift_ms / 1000 * sampling_rate)\n\n    # Prepare file paths for audio and embeddings\n    file_paths = synthesizer_root.joinpath(\"audio\").rglob(\"*.wav\")\n    file_paths = [Path(fp) for fp in file_paths]\n    embed_fpaths = [embed_dir.joinpath(f\"{fp.stem}.npy\") for fp in file_paths]\n    assert all(not fp.exists() for fp in embed_fpaths)\n\n    # Process files in parallel\n    func = partial(\n        _process_utterance,\n        encoder_model_fpath=encoder_model_fpath,\n        hop_length=hop_length,\n    )\n    job = Pool(n_processes).imap(func, tqdm(zip(file_paths, embed_fpaths)))\n    list(tqdm(job, \"Creating embeddings\", len(file_paths), unit=\"utterances\"))\n\n", "CodeBleu_score": 54.156488909030706}, {"code": "def create_embeddings(synthesizer_root: Path, encoder_model_fpath: Path, n_processes: int)\n\t\n    ## Verify the existence of required directories and files\n    synthesizer_root = Path(synthesizer_root).expanduser()\n    assert synthesizer_root.exists()\n    embed_dir = synthesizer_root.joinpath(\"encoder/embeddings\")\n    assert embed_dir.exists()\n    encoder_model_fpath = Path(encoder_model_fpath).expanduser()\n    assert encoder_model_fpath.exists()\n    metadata_fpath = synthesizer_root.joinpath(\"encoder/metadata.csv\")\n    assert metadata_fpath.exists()\n    ## Prepare file paths for audio and embeddings\n    with metadata_fpath.open(\"r\", encoding=\"utf-8\") as metadata_file:\n        metadata = [line.split(\"|\") for line in metadata_file]\n        fpaths = [m[0] for m in metadata]\n        text_lengths = [int(m[1]) for m in metadata]\n        speakers = [int(m[2]) for m in metadata]\n        fpaths = [str(synthesizer_root.joinpath(\"audio\", fpath + \".wav\")) for fpath in fpaths]\n        embed_fpaths = [str(embed_dir.joinpath(fpath + \".npy\")) for fpath in fpaths]\n    ## Process files in parallel\n    func = partial(embed_utterance, encoder_model_fpath=encoder_model_fpath)\n    job = Pool(n_processes).imap(func, zip(fpaths, embed_fpaths, text_lengths, speakers))\n    list(tqdm(job, \"Creating embeddings\", len(fpaths), unit=\"utterances\"))\n\n", "CodeBleu_score": 57.49124004724511}, {"code": "def create_embeddings(synthesizer_root: Path, encoder_model_fpath: Path, n_processes: int)\n\t\n    # Verify the existence of required directories and files\n    assert synthesizer_root.exists(), \"The synthesizer root directory does not exist.\"\n    assert encoder_model_fpath.exists(), \"The encoder model does not exist.\"\n    embed_dir = synthesizer_root.joinpath(\"embed\")\n    assert embed_dir.exists(), \"The embed directory does not exist.\"\n    metadata_fpath = synthesizer_root.joinpath(\"train.txt\")\n    assert metadata_fpath.exists(), \"The metadata file does not exist.\"\n\n    # Read metadata\n    with metadata_fpath.open(\"r\", encoding=\"utf-8\") as metadata_file:\n        metadata = [line.split(\"|\") for line in metadata_file]\n        frame_shift_ms = int(metadata[0][7])\n        sample_rate = int(metadata[0][4])\n        hop_length = int(frame_shift_ms / 1000 * sample_rate)\n\n    # Prepare file paths for audio and embeddings\n    file_paths = synthesizer_root.joinpath(\"audio\").glob(\"**/*.wav\")\n    file_paths = [Path(str(fpath).replace(str(synthesizer_root.joinpath(\"audio\")), str(embed_dir))) for fpath in file_paths]\n    file_paths = [fpath.parent.joinpath(fpath.stem + \".npy\") for fpath in file_paths]\n\n    # Create embeddings using a multiprocessing pool\n    if n_processes == 1:\n        embeddings = [encoder.embed_utterance(fpath, encoder_model_fpath) for fpath in tqdm(file_paths)]\n    else:\n        partial_fn = partial(encoder.embed_utterance, encoder_model_fpath=encoder_model_fpath)\n        with Pool(n_processes) as pool:\n            embeddings = list(tqdm(pool.imap(partial_fn, file_paths), total=len(file_paths)))\n\n    # Save embeddings\n    for embedding, fpath in zip(embedd", "CodeBleu_score": 50.77050424422027}]}
{"instance_id": "9eb0abda14a9", "generation_result": [{"code": "def eval_model(attention, mel_prediction, target_spectrogram, input_seq, step,\n               plot_dir, mel_output_dir, wav_dir, sample_num, loss, hparams)\n\t\n    # Generate wav and plot spectrograms\n    if hparams.predict_linear:\n        wav = audio.inv_spectrogram(mel_prediction.data.cpu().numpy().T)\n    else:\n        wav = audio.inv_mel_spectrogram(mel_prediction.data.cpu().numpy().T)\n    # wav = audio.inv_mel_spectrogram(mel_prediction.data.cpu().numpy().T)\n    target_wav = audio.inv_mel_spectrogram(target_spectrogram.data.cpu().numpy().T)\n    # wav_target = audio.inv_spectrogram(target_spectrogram.data.cpu().numpy().T)\n    # wav_target = audio.inv_mel_spectrogram(target_spectrogram.data.cpu().numpy().T)\n    # wav_predicted = audio.inv_spectrogram(mel_prediction.data.cpu().numpy().T)\n    wav_predicted = audio.inv_mel_spectrogram(mel_prediction.data.cpu().numpy().T)\n    # wav_predicted = audio.inv_mel_spectrogram(mel_prediction.data.cpu().numpy().T)\n    # wav_predicted = audio.inv_mel_spectrogram(mel_prediction.data.cpu().numpy().T)\n\n    # Save audio\n    audio.save_wav(wav, os.path.join(wav_dir, \"step-{}-sample-{}.wav\".format(step, sample_num)))\n    audio.save_wav(target_wav, os.path.join(wav_dir, \"target-step-{}-sample-{}.wav\".format(step, sample_num)))\n    audio.save_wav(wav_predicted, os.path.join(wav_dir, \"predicted-step-{}-sample-{}.wav\".format(step, sample_num)))\n\n    # Save spectrograms\n    # plot_spectrogram(mel_prediction.data.cpu().numpy(), os.path.join(mel_output_", "CodeBleu_score": 31.734413762423568}, {"code": "def eval_model(attention, mel_prediction, target_spectrogram, input_seq, step,\n               plot_dir, mel_output_dir, wav_dir, sample_num, loss, hparams)\n\t\n    if attention is not None:\n        # Save attention plot to disk\n        plot_path = plot_dir.joinpath(f\"step{step}_sample{sample_num}_attention.png\")\n        save_attention(attention, plot_path)\n\n    # Save predicted mel spectrogram to disk\n    mel_prediction_path = mel_output_dir.joinpath(f\"step{step}_sample{sample_num}_predicted.npy\")\n    np.save(mel_prediction_path, mel_prediction.T, allow_pickle=False)\n\n    # Save the reconstructed wav for both the predicted and target mel spectrograms\n    wav_reconstruction_path = wav_dir.joinpath(f\"step{step}_sample{sample_num}_predicted.wav\")\n    audio.save_wav(audio.inv_mel_spectrogram(mel_prediction.T, hparams),\n                   wav_reconstruction_path)\n\n    # Save the target mel spectrogram to disk\n    mel_target_path = mel_output_dir.joinpath(f\"step{step}_sample{sample_num}_target.npy\")\n    np.save(mel_target_path, target_spectrogram.T, allow_pickle=False)\n\n    # Save the reconstructed wav for the target mel spectrogram\n    wav_reconstruction_path = wav_dir.joinpath(f\"step{step}_sample{sample_num}_target.wav\")\n    audio.save_wav(audio.inv_mel_spectrogram(target_spectrogram.T, hparams),\n                   wav_reconstruction_path)\n\n    # Generate and save a plot comparing the predicted and target mel spectrograms\n    plot_path = plot_dir.joinpath(f\"step{step}_sample{sample_num}_mel_spectrograms.png\")\n    title = f\"{hparams.model_type} model, synthesizer step {step}, loss {loss:.5f}, {datetime.now().strftime('%Y-%m-%d %H:%M')}\"\n", "CodeBleu_score": 35.21775589589389}, {"code": "def eval_model(attention, mel_prediction, target_spectrogram, input_seq, step,\n               plot_dir, mel_output_dir, wav_dir, sample_num, loss, hparams)\n\t\n    if attention is not None:\n        attention = attention.cpu().detach().numpy()\n        plot_spectrogram(attention, filename=plot_dir.joinpath(f\"step{step:09d}_sample{sample_num:02d}_attention.png\"))\n    if mel_prediction is not None:\n        mel_prediction = mel_prediction.cpu().detach().numpy()\n        plot_spectrogram(mel_prediction, filename=plot_dir.joinpath(f\"step{step:09d}_sample{sample_num:02d}_predicted_mel.png\"))\n    if target_spectrogram is not None:\n        target_spectrogram = target_spectrogram.cpu().detach().numpy()\n        plot_spectrogram(target_spectrogram, filename=plot_dir.joinpath(f\"step{step:09d}_sample{sample_num:02d}_target_mel.png\"))\n        plot_spectrogram(target_spectrogram - mel_prediction, filename=plot_dir.joinpath(f\"step{step:09d}_sample{sample_num:02d}_mel_diff.png\"))\n    if wav_dir is not None:\n        wav_dir.mkdir(exist_ok=True)\n        wav_path = wav_dir.joinpath(f\"step{step:09d}_sample{sample_num:02d}_predicted.wav\")\n        audio.save_wav(audio.inv_preemphasis(mel_prediction.T), wav_path, hparams.sample_rate)\n    if mel_output_dir is not None:\n        mel_output_dir.mkdir(exist_ok=True)\n        mel_output_path = mel_output_dir.joinpath(f\"step{step:09d}_sample{sample_num:02d}_predicted.npy\")\n        np.save(mel_output_path, mel_prediction.T, allow_pickle=False)\n    if input_seq is not None:\n        input_seq = input_seq.cpu().detach().numpy()\n       ", "CodeBleu_score": 29.239840315296554}]}
{"instance_id": "6bad4f500286", "generation_result": [{"code": "def num_params(self)\n\t\n    params = filter(lambda p: p.requires_grad, self.parameters())\n    params = sum([np.prod(p.size()) for p in params])\n    return print(f\"Trainable Parameters: {params/1e6:.3f} million\")\n\n", "CodeBleu_score": 38.54074900967153}, {"code": "def num_params(self)\n\t\n    parameters = filter(lambda p: p.requires_grad, self.parameters())\n    parameters = sum([np.prod(p.size()) for p in parameters]) / 1_000_000\n    print('Trainable Parameters: %.3fM' % parameters)\n\n", "CodeBleu_score": 55.68162377003812}, {"code": "def num_params(self)\n\t\n    model_parameters = filter(lambda p: p.requires_grad, self.parameters())\n    params = sum([np.prod(p.size()) for p in model_parameters])\n    return \"Trainable Parameters: %.3f million\" % (params / 1e6)\n\n", "CodeBleu_score": 39.943761989402766}]}
{"instance_id": "78b876054bd4", "generation_result": [{"code": "def pad_tensor(self, x, pad, side='both')\n\t\n    # Determine the size of the new tensor\n    if side == 'both':\n        new_size = (x.size(0), x.size(1) + 2 * pad)\n    elif side == 'after':\n        new_size = (x.size(0), x.size(1) + pad)\n    elif side == 'before':\n        new_size = (x.size(0), x.size(1) + pad)\n    else:\n        raise ValueError(\"side must be either 'before', 'after', or 'both'\")\n\n    # Create a new tensor\n    device = x.device\n    dtype = x.dtype\n    if device == torch.device('cpu'):\n        new_tensor = torch.zeros(new_size, dtype=dtype)\n    else:\n        new_tensor = torch.zeros(new_size, dtype=dtype, device=device)\n\n    # Place the original tensor into the new tensor\n    if side == 'both':\n        new_tensor[:, pad:x.size(1) + pad] = x\n    elif side == 'after':\n        new_tensor[:, :x.size(1)] = x\n    elif side == 'before':\n        new_tensor[:, -x.size(1):] = x\n    else:\n        raise ValueError(\"side must be either 'before', 'after', or 'both'\")\n\n    return new_tensor\n\n", "CodeBleu_score": 30.372255501127754}, {"code": "def pad_tensor(self, x, pad, side='both')\n\t\n    # Determine new size\n    if side == 'both':\n        new_size = x.size(0) + 2 * pad\n    else:\n        new_size = x.size(0) + pad\n\n    # Create new tensor\n    if x.is_cuda:\n        x_pad = torch.cuda.FloatTensor(new_size, x.size(1), x.size(2))\n    else:\n        x_pad = torch.zeros(new_size, x.size(1), x.size(2))\n\n    # Place original tensor into new tensor\n    if side == 'after':\n        x_pad[:x.size(0)] = x\n    elif side == 'both':\n        x_pad[pad:x.size(0) + pad] = x\n    else:\n        x_pad[-x.size(0):] = x\n\n    return x_pad\n\n", "CodeBleu_score": 36.11147759214111}, {"code": "def pad_tensor(self, x, pad, side='both')\n\t\n    # Determine the size of the new tensor\n    if side == 'both':\n        new_size = x.size()[1] + 2 * pad\n    else:\n        new_size = x.size()[1] + pad\n\n    # Create a tensor filled with zeros of the correct size\n    if x.is_cuda:\n        device = x.get_device()\n        x_pad = torch.cuda.FloatTensor(x.size()[0], new_size, x.size()[2]).fill_(0)\n    else:\n        x_pad = torch.zeros(x.size()[0], new_size, x.size()[2])\n\n    # Place the original tensor into the new tensor\n    if side == 'after':\n        x_pad[:, :x.size()[1], :] = x\n    elif side == 'before':\n        x_pad[:, -x.size()[1]:, :] = x\n    else:\n        first_half = x_pad[:, pad:pad + x.size()[1], :]\n        first_half[:] = x\n\n    return x_pad\n\n", "CodeBleu_score": 38.95332366730939}]}
{"instance_id": "a7ed1715a3df", "generation_result": [{"code": "def save_attention(attn, path)\n\t\n    import matplotlib.pyplot as plt\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    cax = ax.matshow(attn, cmap='bone')\n    fig.colorbar(cax)\n    ax.set_xticklabels([''] + ['Source'] + [''] * (attn.shape[1] - 2) + ['Target'], rotation=90)\n    ax.set_yticklabels([''] + ['Target'] + [''] * (attn.shape[0] - 2) + ['Source'])\n    ax.xaxis.set_major_locator(plt.MultipleLocator(1))\n    ax.yaxis.set_major_locator(plt.MultipleLocator(1))\n    plt.savefig(path + '.png', bbox_inches='tight')\n    plt.close(fig)\n\n", "CodeBleu_score": 45.98790032937306}, {"code": "def save_attention(attn, path)\n\t\n    import matplotlib.pyplot as plt\n    fig = plt.figure(figsize=(12, 6))\n    ax = fig.add_subplot(1, 1, 1)\n    ax.matshow(attn.T, cmap='viridis')\n    ax.set_xticklabels([''] + ['<sos>'] + [i for i in 'hello'])\n    ax.set_yticklabels([''] + [i for i in 'english'])\n    plt.savefig(path)\n    plt.close(fig)\n\n", "CodeBleu_score": 48.55195559162579}, {"code": "def save_attention(attn, path)\n\t\n    import matplotlib.pyplot as plt\n    fig = plt.figure(figsize=(10, 10))\n    ax = fig.add_subplot(1, 1, 1)\n    ax.matshow(attn, cmap='viridis')\n    ax.set_xticklabels([''] + ['<sos>'] + [i for i in range(attn.shape[1] - 2)])\n    ax.set_yticklabels([''] + ['<sos>'] + [i for i in range(attn.shape[0] - 2)])\n    plt.tight_layout()\n    plt.savefig(path)\n    plt.close(fig)\n\n", "CodeBleu_score": 44.247584786184404}]}
{"instance_id": "19375c9f44a2", "generation_result": [{"code": "def plot(array)\n\t\n    import matplotlib.pyplot as plt\n\n    fig = plt.figure(figsize=(12, 7))\n    ax = fig.add_subplot(111)\n    ax.set_xlabel('$x$', fontsize=15)\n    ax.set_ylabel('$y$', fontsize=15)\n    ax.tick_params(axis='both', labelsize=15)\n    ax.plot(array)\n    plt.show()\n\n", "CodeBleu_score": 28.658714002468127}, {"code": "def plot(array)\n\t\n    import matplotlib.pyplot as plt\n    from matplotlib import rc\n    rc('text', usetex=True)\n    plt.rcParams['text.latex.preamble'] = [r'\\usepackage{amsmath}']\n    fig = plt.figure(figsize=(10, 6))\n    ax = fig.add_subplot(111)\n    ax.set_xlabel(r'$\\mathbf{x}$', fontsize=20)\n    ax.set_ylabel(r'$\\mathbf{y}$', fontsize=20)\n    ax.tick_params(axis='both', which='major', labelsize=15)\n    ax.tick_params(axis='both', which='minor', labelsize=15)\n    ax.plot(array, linewidth=2)\n    plt.show()\n\n", "CodeBleu_score": 46.353454687242625}, {"code": "def plot(array)\n\t\n\n    import matplotlib.pyplot as plt\n\n    # Create a figure with a specified size\n    fig = plt.figure(figsize=(12, 12))\n\n    # Add a subplot with a specified size\n    ax = fig.add_subplot(111, aspect='equal')\n\n    # Customize the axis labels and tick parameters with specific colors and font sizes\n    ax.set_xlabel('X axis', color='#000000', fontsize=16)\n    ax.set_ylabel('Y axis', color='#000000', fontsize=16)\n    ax.tick_params(axis='x', colors='#000000', labelsize=12)\n    ax.tick_params(axis='y', colors='#000000', labelsize=12)\n\n    # Plot the array on the customized subplot\n    ax.plot(array, color='#000000', linewidth=2)\n\n    # Show the plot\n    plt.show()\n\n", "CodeBleu_score": 33.79035328311766}]}
{"instance_id": "ff94c425920c", "generation_result": [{"code": "def write_to_csv(image_name, prediction, confidence)\n\t\n    data = {\n        \"image_name\": image_name,\n        \"prediction\": prediction,\n        \"confidence\": confidence,\n    }\n    with open(\"predictions.csv\", \"a\", newline=\"\") as f:\n        writer = csv.DictWriter(f, fieldnames=data.keys())\n        if os.stat(\"predictions.csv\").st_size == 0:\n            writer.writeheader()\n        writer.writerow(data)\n\n", "CodeBleu_score": 60.268262473497714}, {"code": "def write_to_csv(image_name, prediction, confidence)\n\t\n    prediction_dict = {\"image_name\": image_name, \"prediction\": prediction, \"confidence\": confidence}\n    with open(\"predictions.csv\", \"a\") as csv_file:\n        fieldnames = [\"image_name\", \"prediction\", \"confidence\"]\n        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n        if os.stat(\"predictions.csv\").st_size == 0:\n            writer.writeheader()\n        writer.writerow(prediction_dict)\n\n", "CodeBleu_score": 46.22599533283551}, {"code": "def write_to_csv(image_name, prediction, confidence)\n\t\n\n    # Create a dictionary with the image name, prediction, and confidence\n    data = {\"image_name\": image_name, \"prediction\": prediction, \"confidence\": confidence}\n\n    # Open the CSV file in append mode\n    with open(\"predictions.csv\", \"a\") as csvfile:\n        # Create a CSV writer\n        writer = csv.writer(csvfile)\n\n        # Check if the file is new, and write the header\n        is_new = os.stat(\"predictions.csv\").st_size == 0\n        if is_new:\n            writer.writerow(data.keys())\n\n        # Write the data as a new row\n        writer.writerow(data.values())\n\n", "CodeBleu_score": 48.220008073618}]}
{"instance_id": "436161602009", "generation_result": [{"code": "def export_torchscript(model, im, file, optimize, prefix=colorstr(\"TorchScript:\"))\n\t\n    if optimize:\n        LOGGER.info(f\"{prefix} starting export with torch.jit.save...\")\n        f = model.torchscript(im)\n        LOGGER.info(f\"{prefix} export success, saving to {file}\")\n        f.save(file)\n        return file, None\n\n    LOGGER.info(f\"{prefix} starting export with torch.jit.script...\")\n    try:\n        f = torch.jit.script(model)\n    except Exception as e:\n        LOGGER.info(f\"{prefix} export failure: {e}\")\n        return None, e\n    LOGGER.info(f\"{prefix} export success, saving to {file}\")\n    f.save(file)\n    return file, None\n\n", "CodeBleu_score": 32.63496556702571}, {"code": "def export_torchscript(model, im, file, optimize, prefix=colorstr(\"TorchScript:\"))\n\t\n    # YOLOv5 TorchScript model export\n    with torch.jit.optimized_execution(True):\n        try:\n            LOGGER.info(f\"{prefix} starting export with torch {torch.__version__}...\")\n            f = str(file).endswith('.pt')\n            model.model[-1].export = not f  # set Detect() layer export=False if saving as .torchscript.pt\n            traced_model = torch.jit.trace(model, im, strict=False)\n            traced_model.eval()\n            LOGGER.info(f\"{prefix} export success, saving {file}...\")\n            if optimize:\n                LOGGER.info(f\"{prefix} saving for mobile optimization...\")\n                model = optimize_for_mobile(traced_model)\n            else:\n                model = traced_model\n            torch.jit.save(model, file)\n            LOGGER.info(f\"{prefix} saved {file_size(file):.1f}MB\")\n        except Exception as e:\n            LOGGER.info(f\"{prefix} export failure: saving {file}... {e}\")\n        finally:\n            model.model[-1].export = True  # reset Detect() layer export=True\n\n", "CodeBleu_score": 42.50697577176864}, {"code": "def export_torchscript(model, im, file, optimize, prefix=colorstr(\"TorchScript:\"))\n\t\n    LOGGER.info(f\"{prefix} starting export with torch {torch.__version__}...\")\n    f = str(file).replace(\".pt\", \".torchscript.pt\")  # make sure output filename is a .pt\n    ts = torch.jit.trace(model, im, strict=False)\n    extra_files = {\"config.txt\": \"\", \"build.txt\": \"\"}  # extra TorchScript files\n    extra_files[\"config.txt\"] = \"", "CodeBleu_score": 42.00372424303883}]}
{"instance_id": "78335487ac1c", "generation_result": [{"code": "def export_coreml(model, im, file, int8, half, nms, prefix=colorstr(\"CoreML:\")):\n\t\n    try:\n        import coremltools as ct\n    except ImportError:\n        raise ImportError(\"CoreML export requires coremltools. Install with 'pip install coremltools'.\")\n    suffix = \".mlmodel\"\n    file = Path(file).with_suffix(suffix)\n    LOGGER.info(f\"{prefix} exporting to {file}...\")\n    model = model.eval()\n    with torch.no_grad():\n        if half:\n            im = im.half()  # for FP16\n        # trace model\n        o = model(im) if isinstance(o, str) else o  # outputs\n        if isinstance(o, (list, tuple)):\n            o = o[0]  # get first item from list/tuple\n        o = o.float()  # to float32\n        o = o.permute(0, 2, 3, 1).cpu().numpy()  # BCHW to BHWC\n        # convert\n        mlmodel = ct.convert(\n            ct.models.MLModel(o),\n            inputs=[ct.TensorType(shape=im.shape)],\n            minimum_deployment_target=\"13\",\n        )\n        if int8:\n            mlmodel = ct.models.MLModel(\n                ct.quantization_utils.quantize_weights(\n                    mlmodel, nbits=8, quant_type=ct.quantization_utils.QuantizationType.QUANTIZATION_TYPE_WEIGHT_ONLY\n                )\n            )\n        # save\n        mlmodel.save(file)\n        LOGGER.info(f\"{prefix} export success, saved to {file}\")\n    return file, mlmodel\n\n", "CodeBleu_score": 43.59496604005455}, {"code": "def export_coreml(model, im, file, int8, half, nms, prefix=colorstr(\"CoreML:\")):\n\t\n    try:\n        import coremltools as ct\n    except ImportError:\n        raise ImportError(f'CoreML export requires coremltools. Install with \"pip install coremltools\".')\n\n    suffix = '.mlmodel'\n    file = Path(file)\n    file.suffix = suffix if file.suffix else suffix\n    LOGGER.info(f\"{prefix} exporting {file}\")\n\n    if half:\n        model.half()\n    if int8:\n        model.fuse()\n        model.qconfig = torch.quantization.get_default_qconfig(\"qnnpack\")\n        torch.quantization.prepare(model, inplace=True)\n        torch.quantization.convert(model, inplace=True)\n\n    with torch.no_grad():\n        model.eval()\n        img = im.unsqueeze(0)\n        if half:\n            img = img.half()\n        out = model(img)\n        out = out.cpu().numpy()\n\n    # wrap model for NMS\n    if nms:\n        from models.common import Detections\n        from models.yolo import DetectMultiBackend\n        from utils.general import non_max_suppression\n        detect = DetectMultiBackend(model=model, nc=model.nc)\n        detect.stride = model.stride\n        detect.names = model.names\n        detect.onnx = model.onnx\n        detect.forward = detect.forward_export\n        detect.inplace = model.inplace\n        detect.dynamic = model.dynamic\n        detect.export = model.export\n        detect.fuse = model.fuse\n        detect.half = model.half\n        detect.device = model.device\n        detect.yaml = model.yaml\n        detect.training = model.training\n        detect.inplace = model.inplace\n        detect.iou_thres = 0.5\n        detect.agnostic = False\n        detect.multi_label = nms == 2\n        detect.merge = False\n        detect.classes = model.nc\n        detect.auto_classes = model.nc\n        detect.autos", "CodeBleu_score": 36.51411238241556}, {"code": "def export_coreml(model, im, file, int8, half, nms, prefix=colorstr(\"CoreML:\")):\n\t\n    try:\n        import coremltools as ct\n    except ImportError:\n        raise ImportError('CoreML export requires coremltools. Install with `pip install coremltools`.')\n    if int8:\n        LOGGER.info(f'{prefix} quantizing model')\n        model = quantize(model)\n    stride, names = (\n        model.stride,\n        model.names,\n    )  # only YOLOv3 model supports strides and names\n    model = attempt_load(weights, map_location=device)  # load FP32 model\n    im = im.unsqueeze(0)\n    if half:\n        im = im.half()  # to FP16\n        model.half()  # to FP16\n    if int8:\n        model = model.fuse()\n    model = model.eval()\n    LOGGER.info(f'{prefix} starting export with torch {torch.__version__}...')\n    with torch.no_grad():\n        # trace model\n        LOGGER.info(f'{prefix} tracing with {im.shape}...')\n        traced = torch.jit.trace(model, im)\n        LOGGER.info(f'{prefix} exporting...')\n        # export model\n        if half:\n            traced = torch.jit.freeze(traced)\n            mlmodel = ct.convert(traced, inputs=[ct.ImageType(shape=im.shape[2:], bias=[-x / 2 for x in im.shape[2:]])])\n        else:\n            mlmodel = ct.convert(traced, inputs=[ct.ImageType(shape=im.shape[2:])])\n        mlmodel.save(file)\n        LOGGER.info(f'{prefix} export success, saved as {file} ({file_size(file):.1f} MB)')\n        return file, mlmodel\n\n", "CodeBleu_score": 43.10279313225712}]}
{"instance_id": "80233cf71580", "generation_result": [{"code": "def generate_individual(input_ranges, individual_length)\n\t\n    individual = []\n    for i in range(individual_length):\n        individual.append(random.uniform(input_ranges[i][0], input_ranges[i][1]))\n    return individual\n\n", "CodeBleu_score": 47.630972577458174}, {"code": "def generate_individual(input_ranges, individual_length)\n\t\n    individual = []\n    for i in range(individual_length):\n        individual.append(random.uniform(input_ranges[i][0], input_ranges[i][1]))\n    return individual\n\n", "CodeBleu_score": 47.630972577458174}, {"code": "def generate_individual(input_ranges, individual_length)\n\t\n    individual = []\n    for i in range(individual_length):\n        individual.append(random.uniform(input_ranges[i][0], input_ranges[i][1]))\n    return individual\n\n", "CodeBleu_score": 47.630972577458174}]}
{"instance_id": "5ea979cb4a8e", "generation_result": [{"code": "def marginal_log_mean_coeff(self, t)\n\t\n    if self.schedule_type == 'discrete':\n        return self.log_alpha_t[t]\n    elif self.schedule_type == 'linear':\n        t_min, t_max = self.t_min, self.t_max\n        return (t - t_min) / (t_max - t_min) * (self.log_alpha_t[t_max] - self.log_alpha_t[t_min]) + self.log_alpha_t[t_min]\n    elif self.schedule_type == 'cosine':\n        t_min, t_max = self.t_min, self.t_max\n        return (1 - math.cos(math.pi * (t - t_min) / (t_max - t_min))) / 2 * (self.log_alpha_t[t_max] - self.log_alpha_t[t_min]) + self.log_alpha_t[t_min]\n    else:\n        raise ValueError(\"Schedule type must be 'discrete', 'linear', or 'cosine'.\")\n\n", "CodeBleu_score": 33.278331451622414}, {"code": "def marginal_log_mean_coeff(self, t)\n\t\n    if self.schedule_type == 'discrete':\n        return self.log_alpha_t[int(t)]\n    elif self.schedule_type == 'linear':\n        return self.log_alpha_t[0] + (self.log_alpha_t[-1] - self.log_alpha_t[0]) * t\n    elif self.schedule_type == 'cosine':\n        return self.log_alpha_t[0] + (self.log_alpha_t[-1] - self.log_alpha_t[0]) * (1 + math.cos(math.pi * t)) / 2\n    else:\n        raise ValueError('Schedule type must be one of \"discrete\", \"linear\", or \"cosine\"')\n\n", "CodeBleu_score": 32.06171027195556}, {"code": "def marginal_log_mean_coeff(self, t)\n\t\n    if self.schedule == \"discrete\":\n        return self.log_alpha_t[t]\n    elif self.schedule == \"linear\":\n        return self.log_alpha_t[0] + (self.log_alpha_t[-1] - self.log_alpha_t[0]) * t\n    elif self.schedule == \"cosine\":\n        return (\n            self.log_alpha_t[0]\n            + (self.log_alpha_t[-1] - self.log_alpha_t[0]) * (1 + math.cos(math.pi * t)) / 2\n        )\n    else:\n        raise ValueError(f\"Schedule {self.schedule} not implemented.\")\n\n", "CodeBleu_score": 30.727651758727266}]}
{"instance_id": "7da4fae5a278", "generation_result": [{"code": "def inverse_lambda(self, lamb)\n\t\n    if self.schedule == \"linear\":\n        if self.beta_t is None:\n            self.beta_t = torch.linspace(0, 1, self.T, device=self.device)\n        return self.beta_t.inverse_transform(lamb)\n    elif self.schedule == \"discrete\":\n        return self.alpha_t.inverse_transform(lamb)\n    else:\n        # inverse of arccos\n        return torch.acos(1 - 2 * lamb) / math.pi\n\n", "CodeBleu_score": 15.382863112588701}, {"code": "def inverse_lambda(self, lamb)\n\t\n    if self.schedule == \"linear\":\n        t = (\n            self.beta_0\n            + (self.beta_1 - self.beta_0) * (1 - math.exp(-lamb * self.alpha_t))\n        )\n    elif self.schedule == \"discrete\":\n        t = self.t_list[\n            torch.argmin(torch.abs(lamb.unsqueeze(1) - self.log_alpha))\n        ].squeeze()\n    else:\n        log_alpha = torch.log(self.alpha_t)\n        t = math.acos((1 - lamb * log_alpha) / math.sqrt(1 + lamb ** 2 * log_alpha ** 2))\n        t /= math.pi\n    return t\n\n", "CodeBleu_score": 27.491722973003345}, {"code": "def inverse_lambda(self, lamb)\n\t\n    if self.schedule == \"linear\":\n        t = (\n            self.beta_0\n            + (self.beta_1 - self.beta_0)\n            * (1 - math.exp(-lamb * self.T / self.n_steps))\n        )\n    elif self.schedule == \"discrete\":\n        t = self.T * (1 - math.exp(-lamb * self.T / self.n_steps))\n    else:\n        log_alpha = math.log(1 - math.exp(-lamb * self.T / self.n_steps))\n        t = self.T * math.acos(\n            (math.cos(self.T * log_alpha) - self.cos_beta_0)\n            / (self.cos_beta_1 - self.cos_beta_0)\n        ) / log_alpha\n    return t\n\n", "CodeBleu_score": 26.262234530766072}]}
{"instance_id": "a519c853a2b2", "generation_result": [{"code": "def noise_pred_fn(x, t_continuous, cond=None)\n\t\n    if cond is not None:\n        cond = cond.unsqueeze(1).repeat(1, x.shape[1], 1)\n        x = torch.cat([x, cond], dim=2)\n    x = x.unsqueeze(1).repeat(1, t_continuous.shape[0], 1)\n    t_continuous = t_continuous.unsqueeze(0).repeat(x.shape[0], 1, 1)\n    x = torch.cat([x, t_continuous], dim=2)\n    return x\n\n", "CodeBleu_score": 17.23631583081284}, {"code": "def noise_pred_fn(x, t_continuous, cond=None)\n\t\n    if cond is not None:\n        if cond.ndim == 1:\n            cond = cond.unsqueeze(0).repeat(x.shape[0], 1)\n        elif cond.ndim == 2:\n            cond = cond.repeat(x.shape[0], 1)\n        elif cond.ndim > 2:\n            raise ValueError(\n                \"Conditional vector must have 1 or 2 dimensions, got {}\".format(\n                    cond.ndim\n                )\n            )\n\n    if t_continuous.ndim == 0:\n        t_continuous = t_continuous.unsqueeze(0).repeat(x.shape[0], 1)\n    elif t_continuous.ndim == 1:\n        t_continuous = t_continuous.unsqueeze(0).repeat(x.shape[0], 1)\n    elif t_continuous.ndim == 2:\n        pass\n    elif t_continuous.ndim > 2:\n        raise ValueError(\n            \"Time vector must have 1 or 2 dimensions, got {}\".format(\n                t_continuous.ndim\n            )\n        )\n\n    if cond is None:\n        return x + model(t_continuous, x)\n    else:\n        return x + model(t_continuous, x, cond)\n\n", "CodeBleu_score": 25.498999936953226}, {"code": "def noise_pred_fn(x, t_continuous, cond=None)\n\t\n    if cond is None:\n        cond = torch.zeros_like(x)\n    if x.shape[0] != t_continuous.shape[0]:\n        t_continuous = t_continuous.repeat(x.shape[0], 1)\n    if x.shape[0] != cond.shape[0]:\n        cond = cond.repeat(x.shape[0], 1)\n    if x.shape[0] != 1:\n        x = x.repeat(t_continuous.shape[0], 1)\n    if cond.shape[0] != 1:\n        cond = cond.repeat(t_continuous.shape[0], 1)\n    if x.shape[0] != 1:\n        x = x.repeat(t_continuous.shape[0], 1)\n    if cond.shape[0] != 1:\n        cond = cond.repeat(t_continuous.shape[0], 1)\n    if x.shape[0] != 1:\n        x = x.repeat(t_continuous.shape[0], 1)\n    if cond.shape[0] != 1:\n        cond = cond.repeat(t_continuous.shape[0], 1)\n    if x.shape[0] != 1:\n        x = x.repeat(t_continuous.shape[0], 1)\n    if cond.shape[0] != 1:\n        cond = cond.repeat(t_continuous.shape[0], 1)\n    if x.shape[0] != 1:\n        x = x.repeat(t_continuous.shape[0], 1)\n    if cond.shape[0] != 1:\n        cond = cond.repeat(t_continuous.shape[0], 1)\n    if x.shape[0] != 1:\n        x = x.repeat(t_continuous.shape[0], 1)\n    if cond.shape[0] != 1:\n        cond = cond.repeat(t_continuous.shape[0], 1)\n    if x.shape[0] != 1:\n        x = x.repeat", "CodeBleu_score": 20.486482926105626}]}
{"instance_id": "96a9b7d54823", "generation_result": [{"code": "def dynamic_thresholding_fn(self, x0, t=None)\n\t\n    if t is None:\n        t = self.t\n    x0 = x0.reshape(-1)\n    x0_abs = x0.abs()\n    x0_abs_sorted, _ = x0_abs.sort()\n    x0_abs_quantile = x0_abs_sorted[int(x0_abs_sorted.numel() * t)]\n    scaling_factor = x0_abs_quantile / self.s\n    x0 = torch.where(x0.abs() < x0_abs_quantile, x0, scaling_factor * torch.sign(x0))\n    x0 = x0 / scaling_factor\n    return x0\n\n", "CodeBleu_score": 33.255786650312736}, {"code": "def dynamic_thresholding_fn(self, x0, t=None)\n\t\n    if t is None:\n        t = self.threshold\n    if self.dynamic:\n        x0_abs = torch.abs(x0)\n        x0_abs_flat = x0_abs.flatten()\n        x0_abs_sorted, _ = torch.sort(x0_abs_flat)\n        k = int(t * x0_abs_sorted.numel())\n        if k == 0:\n            threshold = x0_abs_sorted[0]\n        else:\n            threshold = x0_abs_sorted[k - 1]\n        scale_factor = threshold / self.n_levels\n        x0 = torch.clamp(x0, -threshold, threshold)\n        x0 = x0 / scale_factor\n    return x0\n\n", "CodeBleu_score": 34.23585125196582}, {"code": "def dynamic_thresholding_fn(self, x0, t=None)\n\t\n    if t is None:\n        t = self.threshold\n    if self.threshold_type == \"dynamic\":\n        x0_abs = x0.abs()\n        x0_abs_flat = x0_abs.flatten()\n        x0_abs_flat_sorted, _ = x0_abs_flat.sort()\n        k = int(t * x0_abs_flat_sorted.numel())\n        if k == 0:\n            threshold = x0_abs_flat_sorted[0]\n        else:\n            threshold = x0_abs_flat_sorted[k - 1]\n        threshold = threshold.item()\n    else:\n        threshold = self.threshold\n    if self.threshold_value == \"max\":\n        scaling_factor = threshold / x0.abs().max()\n    elif self.threshold_value == \"mean\":\n        scaling_factor = threshold / x0.abs().mean()\n    x0 = torch.where(x0.abs() > threshold, x0, torch.zeros_like(x0))\n    x0 = x0 * scaling_factor\n    return x0\n\n", "CodeBleu_score": 32.674414920772264}]}
{"instance_id": "59c6ec6d7a6a", "generation_result": [{"code": "def data_prediction_fn(self, x, t)\n\t\n    x0 = x.clone()\n    x0 = x0.view(x0.shape[0], -1)\n    x0 = x0.t()\n    x0 = x0.contiguous().view(-1)\n    x0 = x0.detach().cpu().numpy()\n    x0 = x0.reshape(-1, 1)\n\n    t0 = t.clone()\n    t0 = t0.view(t0.shape[0], -1)\n    t0 = t0.t()\n    t0 = t0.contiguous().view(-1)\n    t0 = t0.detach().cpu().numpy()\n    t0 = t0.reshape(-1, 1)\n\n    x0 = self.normalize(x0, t0)\n    x0 = self.clamp(x0, t0)\n    x0 = self.normalize(x0, t0)\n\n    x0 = torch.from_numpy(x0).float()\n    x0 = x0.view(x.shape[0], -1)\n    x0 = x0.t()\n    x0 = x0.contiguous().view(x.shape)\n    x0 = x0.to(self.device)\n\n    return x0\n\n", "CodeBleu_score": 14.69760016539447}, {"code": "def data_prediction_fn(self, x, t)\n\t\n    if self.dynamic_thresholding:\n        # Dynamic thresholding\n        # Calculate the threshold value\n        # max_threshold = torch.max(x0)\n        max_threshold = torch.max(x)\n        threshold = max_threshold * self.dynamic_thresholding_ratio\n        # Clamp the tensor values within the threshold\n        x = torch.clamp(x, 0, threshold)\n        # Normalize the tensor values\n        x = x / threshold\n    return x\n\n", "CodeBleu_score": 13.782928965311026}, {"code": "def data_prediction_fn(self, x, t)\n\t\n    if self.dynamic_thresholding:\n        if self.dynamic_thresholding_ratio == 1:\n            threshold = self.max_threshold\n        else:\n            threshold = self.max_threshold * math.exp(\n                -self.dynamic_thresholding_ratio * t\n            )\n        x = torch.clamp(x, max=threshold)\n        x = x / threshold\n    return x\n\n", "CodeBleu_score": 17.400922026835993}]}
{"instance_id": "65e3618468a2", "generation_result": [{"code": "def predict_eps_sigma(model, input, sigma_in, **kwargs)\n\t\n    sigma = sigma_in.reshape(input.shape)\n    scaled_input = input * sigma\n    pred = model(scaled_input, **kwargs)\n    pred = pred * sigma\n    return pred\n\n", "CodeBleu_score": 23.828449379829635}, {"code": "def predict_eps_sigma(model, input, sigma_in, **kwargs)\n\t\n    sigma = sigma_in.view(sigma_in.size(0), 1, 1, 1).repeat(1, input.size(1), input.size(2), input.size(3))\n    input = input / sigma\n    prediction = model(input, **kwargs)\n    prediction = prediction * sigma\n    return prediction\n\n", "CodeBleu_score": 28.015870605475463}, {"code": "def predict_eps_sigma(model, input, sigma_in, **kwargs)\n\t\n    # Reshape sigma\n    sigma_in = sigma_in.reshape(input.shape[0], 1, 1, 1)\n\n    # Scale the input\n    input = input * sigma_in\n\n    # Predict\n    pred = model(input, **kwargs)\n\n    # Adjust the prediction\n    pred = pred / sigma_in\n\n    return pred\n\n", "CodeBleu_score": 27.439359373386086}]}
{"instance_id": "b35846dfbc66", "generation_result": [{"code": "def cal_intergrand(beta_0, beta_1, taus)\n\t\n\n    taus_ = taus.clone().requires_grad_(True)\n    alpha = torch.exp(beta_0 * taus_ + beta_1 * taus_ ** 2)\n    log_alpha = torch.log(alpha)\n    log_alpha.backward(torch.ones_like(log_alpha))\n    grad_log_alpha = taus_.grad\n    integrand = torch.exp(log_alpha - grad_log_alpha * taus_)\n    return integrand\n\n", "CodeBleu_score": 22.917751987636766}, {"code": "def cal_intergrand(beta_0, beta_1, taus)\n\t\n    # clone taus to avoid changing them in-place\n    taus = taus.clone()\n    # set the gradient of taus to 1\n    taus.requires_grad = True\n    # calculate alpha\n    alpha = torch.sigmoid(beta_0 + beta_1 * taus)\n    # calculate log_alpha\n    log_alpha = torch.log(alpha)\n    # calculate the gradient of log_alpha with respect to taus\n    log_alpha.backward(torch.ones_like(taus))\n    # calculate the integrand\n    integrand = (alpha * taus.grad).sum(0)\n    # reset the gradient of taus to 0\n    taus.grad.zero_()\n    return integrand\n\n", "CodeBleu_score": 28.897382642744173}, {"code": "def cal_intergrand(beta_0, beta_1, taus)\n\t\n    taus_clone = taus.clone()\n    taus_clone.requires_grad = True\n    alpha = torch.exp(beta_0 + beta_1 * taus_clone)\n    log_alpha = torch.log(alpha)\n    log_alpha.backward(torch.ones_like(log_alpha))\n    grad_log_alpha = taus_clone.grad.clone()\n    taus_clone.grad.zero_()\n    integrand = grad_log_alpha * alpha\n    return integrand\n\n", "CodeBleu_score": 30.015989398924088}]}
{"instance_id": "5f3667566a10", "generation_result": [{"code": "def sample_euler_ancestral(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None)\n\t\n    if extra_args is None:\n        extra_args = {}\n    if noise_sampler is None:\n        noise_sampler = lambda x: x.new_zeros(x.shape)\n    if disable is None:\n        disable = []\n    if callback is None:\n        callback = lambda x, extra_args: None\n    x = x.detach()\n    x.requires_grad = True\n    for sigma in sigmas:\n        extra_args = callback(x, extra_args)\n        x = denoise(model, x, sigma, disable=disable, extra_args=extra_args)\n        x = euler_ancestral_step(model, x, sigma, disable=disable, extra_args=extra_args)\n        x = x + eta * s_noise * noise_sampler(x)\n    return x\n\n", "CodeBleu_score": 25.977737369563865}, {"code": "def sample_euler_ancestral(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None)\n\t\n    if extra_args is None:\n        extra_args = {}\n    if noise_sampler is None:\n        noise_sampler = lambda: torch.randn_like(x) * s_noise\n    for sigma in sigmas:\n        if callback is not None:\n            callback(x)\n        x = denoise(model, x, sigma, extra_args, disable=disable)\n        x = euler_step(model, x, sigma, extra_args, disable=disable)\n        x = x + eta * noise_sampler()\n    if callback is not None:\n        callback(x)\n    return x\n\n", "CodeBleu_score": 22.02823026359832}, {"code": "def sample_euler_ancestral(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None)\n\t\n    if extra_args is None:\n        extra_args = {}\n    if noise_sampler is None:\n        noise_sampler = lambda sigma: torch.randn_like(x) * sigma\n    if disable is None:\n        disable = []\n    if callback is None:\n        callback = lambda x, extra_args: None\n    for sigma in tqdm(sigmas, desc=\"Sampling\"):\n        if \"x\" in extra_args:\n            extra_args[\"x\"] = x\n        if \"sigma\" in extra_args:\n            extra_args[\"sigma\"] = sigma\n        if \"eta\" in extra_args:\n            extra_args[\"eta\"] = eta\n        if \"s_noise\" in extra_args:\n            extra_args[\"s_noise\"] = s_noise\n        x = x - model.denoise(x, **extra_args)\n        x = x + eta * model.ancestral_sample(x, **extra_args)\n        callback(x, extra_args)\n    if s_noise > 0:\n        x = x + s_noise * noise_sampler(sigma)\n    return x\n\n", "CodeBleu_score": 31.863693487672297}]}
{"instance_id": "6261ded88df2", "generation_result": [{"code": "def linear_multistep_coeff(order, t, i, j)\n\t\n    def prod_term(s):\n        return math.prod([t[i] - t[k] for k in range(i, j) if k != i])\n\n    if j - i > order + 1:\n        raise ValueError(\"The order is too high for the step index.\")\n    else:\n        return integrate.quad(prod_term, t[i], t[j])[0]\n\n", "CodeBleu_score": 25.25130721166765}, {"code": "def linear_multistep_coeff(order, t, i, j)\n\t\n    if order > j:\n        raise ValueError(\"Order is too high for the given step index.\")\n    if order == 0:\n        return 1\n    if order == 1:\n        return t[i] - t[j]\n    def product(t):\n        prod = 1\n        for k in range(order):\n            if k != j:\n                prod *= t - t[k]\n        return prod\n    return integrate.quad(product, t[i], t[i+1])[0]\n\n", "CodeBleu_score": 36.53172915620936}, {"code": "def linear_multistep_coeff(order, t, i, j)\n\t\n    def product(k):\n        return (t[k] - t[j]) / (t[i] - t[j])\n\n    def integral(k):\n        return integrate.quad(product, t[j], t[k])[0]\n\n    if order > 2:\n        raise ValueError(\"Order must be 1 or 2\")\n    elif order == 1:\n        return integral(i)\n    elif order == 2:\n        return integral(i) - integral(j)\n\n", "CodeBleu_score": 24.503392356665053}]}
{"instance_id": "4ee025e33ab1", "generation_result": [{"code": "def sample_lcm(model, x, sigmas, extra_args=None, callback=None, disable=None, noise_sampler=None)\n\t\n    if extra_args is None:\n        extra_args = {}\n    if noise_sampler is None:\n        noise_sampler = NoiseSampler(x.shape)\n    for sigma in tqdm(sigmas):\n        if callback is not None:\n            callback(x, sigma)\n        if sigma > 0:\n            noise = noise_sampler(sigma)\n            if disable is None or not disable(x, sigma, noise):\n                x = model(x, noise, **extra_args)\n    return x\n\n", "CodeBleu_score": 27.03018403950037}, {"code": "def sample_lcm(model, x, sigmas, extra_args=None, callback=None, disable=None, noise_sampler=None)\n\t\n    if extra_args is None:\n        extra_args = {}\n    if noise_sampler is None:\n        noise_sampler = torch.randn_like\n    if disable is None:\n        disable = []\n    for sigma in sigmas:\n        if callback is not None:\n            callback(x, sigma)\n        if sigma > 0:\n            x = model(x, noise_sampler(x.shape), **extra_args)\n        else:\n            x = model(x, **extra_args)\n        if \"noise_scale\" in extra_args and sigma > 0:\n            x = x * (sigma / extra_args[\"noise_scale\"])\n    return x\n\n", "CodeBleu_score": 31.440559747710743}, {"code": "def sample_lcm(model, x, sigmas, extra_args=None, callback=None, disable=None, noise_sampler=None)\n\t\n    if extra_args is None:\n        extra_args = {}\n    if noise_sampler is None:\n        noise_sampler = torch.randn_like\n    for sigma in sigmas:\n        x = model(x, sigma=sigma, **extra_args)\n        if callback is not None:\n            callback(x)\n        if sigma > 0:\n            noise = noise_sampler(x.shape)\n            if disable is not None:\n                noise[:, disable] = 0\n            x = x + sigma * noise\n    return x\n\n", "CodeBleu_score": 28.050168298073913}]}
{"instance_id": "cb51f91da606", "generation_result": [{"code": "def append_dims(x, target_dims)\n\t\n    if len(x.shape) > target_dims:\n        raise ValueError(f\"The target dimensions ({target_dims}) must be greater than the current dimensions ({len(x.shape)})\")\n    elif len(x.shape) == target_dims:\n        return x\n    else:\n        dims_to_add = target_dims - len(x.shape)\n        if x.device.type == \"mps\":\n            x = x.detach().clone()\n        else:\n            x = x.clone()\n        return x.reshape(*x.shape, *[1 for _ in range(dims_to_add)])\n\n", "CodeBleu_score": 36.29381210155056}, {"code": "def append_dims(x, target_dims)\n\t\n    if len(x.shape) > target_dims:\n        raise ValueError(f'The target dimensions ({target_dims}) must be greater than or equal to the current dimensions ({len(x.shape)}).')\n    elif len(x.shape) == target_dims:\n        return x\n    else:\n        if x.device.type == 'mps':\n            return x.clone().detach()\n        else:\n            return x.expand(tuple(x.shape) + (1,)*(target_dims - len(x.shape)))\n\n", "CodeBleu_score": 25.91772507162225}, {"code": "def append_dims(x, target_dims)\n\t\n    if len(x.shape) > target_dims:\n        raise ValueError(f'The target dimensions ({target_dims}) must be equal or greater than the current dimensions ({len(x.shape)}).')\n    elif len(x.shape) == target_dims:\n        return x.detach().clone() if x.device.type == 'mps' else x\n    else:\n        return x.detach().clone().unsqueeze(dim=len(x.shape)) if x.device.type == 'mps' else x.unsqueeze(dim=len(x.shape))\n\n", "CodeBleu_score": 34.708223665039235}]}
{"instance_id": "9fac874a3bc7", "generation_result": [{"code": "def download_file(path, url, digest=None)\n\t\n\n    if not path.parent.exists():\n        path.parent.mkdir(parents=True)\n\n    if path.exists():\n        if digest is not None:\n            with open(path, 'rb') as f:\n                if hashlib.sha256(f.read()).hexdigest() != digest:\n                    raise RuntimeError(f'File {path} has an SHA-256 hash of {hashlib.sha256(f.read()).hexdigest()}, but {digest} was expected.')\n    else:\n        print(f'Downloading {url} to {path}...')\n        urllib.request.urlretrieve(url, path)\n\n        if digest is not None:\n            with open(path, 'rb') as f:\n                if hashlib.sha256(f.read()).hexdigest() != digest:\n                    raise RuntimeError(f'File {path} has an SHA-256 hash of {hashlib.sha256(f.read()).hexdigest()}, but {digest} was expected.')\n\n    return path\n\n", "CodeBleu_score": 34.71689422923355}, {"code": "def download_file(path, url, digest=None)\n\t\n    path = Path(path)\n    path.parent.mkdir(parents=True, exist_ok=True)\n    if path.exists():\n        if digest is None:\n            return path\n        digest_ = hashlib.sha256(path.read_bytes()).hexdigest()\n        if digest_ == digest:\n            return path\n        else:\n            warnings.warn(f\"File {path} exists but its digest does not match. Re-downloading.\")\n    with urllib.request.urlopen(url) as response, path.open('wb') as f:\n        shutil.copyfileobj(response, f)\n    if digest is not None:\n        digest_ = hashlib.sha256(path.read_bytes()).hexdigest()\n        if digest_ != digest:\n            raise RuntimeError(f\"Digest mismatch for {path}. Expected {digest}, got {digest_}.\")\n    return path\n\n", "CodeBleu_score": 50.35247523960273}, {"code": "def download_file(path, url, digest=None)\n\t\n    path = Path(path)\n    if not path.parent.exists():\n        path.parent.mkdir(parents=True)\n    if path.exists():\n        if digest is not None:\n            assert digest == hashlib.sha256(path.read_bytes()).hexdigest(), f'{path} does not match the expected hash.'\n    else:\n        print(f'Downloading {url} to {path}...')\n        urllib.request.urlretrieve(url, path)\n        if digest is not None:\n            assert digest == hashlib.sha256(path.read_bytes()).hexdigest(), f'{path} does not match the expected hash.'\n    return path\n\n", "CodeBleu_score": 31.534605561284636}]}
{"instance_id": "f23165e897ce", "generation_result": [{"code": "def train_mode(model, mode=True)\n\t\n    if mode:\n        return model.train()\n    else:\n        return model.eval()\n\n", "CodeBleu_score": 13.777709534837959}, {"code": "def train_mode(model, mode=True)\n\t\n    if not isinstance(model, nn.Module):\n        raise TypeError(\"'model' must be a torch.nn.Module\")\n\n    if mode is None:\n        raise ValueError(\"'mode' must be True or False\")\n\n    if not isinstance(mode, bool):\n        raise TypeError(\"'mode' must be True or False\")\n\n    if model.training is mode:\n        yield\n        return\n\n    original_modes = {}\n    for module in model.modules():\n        original_modes[module] = module.training\n        module.train(mode)\n\n    try:\n        yield\n    finally:\n        for module, mode in original_modes.items():\n            module.train(mode)\n\n", "CodeBleu_score": 27.14767578318593}, {"code": "def train_mode(model, mode=True)\n\t\n    if not isinstance(model, nn.Module):\n        raise TypeError(\"'{}' object is not a PyTorch module\".format(type(model)))\n\n    def get_mode(module):\n        return module.training\n\n    def set_mode(module, mode):\n        module.train(mode)\n\n    return _mode_context(model, get_mode, set_mode, mode)\n\n", "CodeBleu_score": 20.591210485041046}]}
{"instance_id": "c6d59c6cb8e1", "generation_result": [{"code": "def ema_update(model, averaged_model, decay)\n\t\n    for p1, p2 in zip(model.parameters(), averaged_model.parameters()):\n        p2.data.mul_(decay).add_(1 - decay, p1.data)\n    for b1, b2 in zip(model.buffers(), averaged_model.buffers()):\n        b2.data.copy_(b1.data)\n\n", "CodeBleu_score": 24.424394216858026}, {"code": "def ema_update(model, averaged_model, decay)\n\t\n    for p, p_avg in zip(model.parameters(), averaged_model.parameters()):\n        p_avg.data = decay * p_avg.data + (1 - decay) * p.data\n    for b, b_avg in zip(model.buffers(), averaged_model.buffers()):\n        b_avg.data.copy_(b.data)\n\n", "CodeBleu_score": 25.28937634367468}, {"code": "def ema_update(model, averaged_model, decay)\n\t\n    for p, p_avg in zip(model.parameters(), averaged_model.parameters()):\n        p_avg.data.mul_(decay).add_(1 - decay, p.data)\n    for b, b_avg in zip(model.buffers(), averaged_model.buffers()):\n        b_avg.data.copy_(b.data)\n\n", "CodeBleu_score": 24.745430718528027}]}
{"instance_id": "600c39a30798", "generation_result": [{"code": "def rand_log_logistic(shape, loc=0., scale=1., min_value=0., max_value=float('inf'), device='cpu', dtype=torch.float32)\n\t\n    min_value = torch.tensor(min_value, device=device, dtype=dtype)\n    max_value = torch.tensor(max_value, device=device, dtype=dtype)\n    log_cdf_min = torch.log(torch.logistic(loc - min_value / scale))\n    log_cdf_max = torch.log(torch.logistic(loc - max_value / scale))\n    log_uniform = torch.empty(shape, device=device, dtype=dtype).uniform_(log_cdf_min, log_cdf_max)\n    return torch.exp(torch.log(torch.logistic(loc - scale * torch.log(torch.exp(log_uniform) - 1))) * scale)\n\n", "CodeBleu_score": 31.588490424568207}, {"code": "def rand_log_logistic(shape, loc=0., scale=1., min_value=0., max_value=float('inf'), device='cpu', dtype=torch.float32)\n\t\n    if isinstance(shape, int):\n        shape = (shape,)\n    if isinstance(min_value, float):\n        min_value = torch.tensor(min_value, dtype=dtype, device=device)\n    if isinstance(max_value, float):\n        max_value = torch.tensor(max_value, dtype=dtype, device=device)\n    if min_value.dtype != dtype or max_value.dtype != dtype or min_value.device != device or max_value.device != device:\n        raise ValueError('min_value and max_value must have the same dtype and device as the output')\n    if min_value.shape != shape or max_value.shape != shape:\n        raise ValueError('min_value and max_value must have the same shape as the output')\n    if min_value >= max_value:\n        raise ValueError('min_value must be less than max_value')\n    if min_value > 0:\n        raise ValueError('min_value must be less than or equal to 0')\n    if max_value < 0:\n        raise ValueError('max_value must be greater than or equal to 0')\n    if not (loc == 0 and scale == 1):\n        raise NotImplementedError('loc and scale must be 0 and 1, respectively')\n    # Convert min_value and max_value to CDF values\n    min_value = torch.log(-min_value / scale + 1)\n    max_value = torch.log(max_value / scale + 1)\n    # Generate uniform random values within this range\n    u = torch.rand(shape, dtype=dtype, device=device)\n    # Transform using the logit function, scale, and exponentiate\n    return torch.exp(torch.log(u) / loc + min_value) * scale\n\n", "CodeBleu_score": 32.69694246978198}, {"code": "def rand_log_logistic(shape, loc=0., scale=1., min_value=0., max_value=float('inf'), device='cpu', dtype=torch.float32)\n\t\n    if isinstance(shape, int):\n        shape = (shape,)\n    if isinstance(loc, int):\n        loc = torch.tensor(loc, dtype=dtype, device=device)\n    if isinstance(scale, int):\n        scale = torch.tensor(scale, dtype=dtype, device=device)\n    if isinstance(min_value, int):\n        min_value = torch.tensor(min_value, dtype=dtype, device=device)\n    if isinstance(max_value, int):\n        max_value = torch.tensor(max_value, dtype=dtype, device=device)\n    if min_value == max_value:\n        return min_value\n    if min_value > max_value:\n        raise ValueError('Minimum value must be less than maximum value.')\n    if min_value != 0:\n        loc = loc + (min_value / scale) * math.log(1 + math.exp(-loc))\n        scale = scale * (max_value / min_value)\n    if min_value == 0:\n        min_value = min_value.to(dtype=dtype, device=device)\n    if max_value == float('inf'):\n        max_value = max_value.to(dtype=dtype, device=device)\n    min_value = torch.log(min_value / scale) - loc\n    max_value = torch.log(max_value / scale) - loc\n    u = torch.rand(shape, dtype=dtype, device=device)\n    x = scale * torch.log(u) + loc\n    x = torch.where(u < math.exp(-loc), min_value, x)\n    x = torch.where(u > math.exp(-loc) * (1 + scale), max_value, x)\n    return x\n\n", "CodeBleu_score": 33.451259764699635}]}
{"instance_id": "75e2def2467f", "generation_result": [{"code": "def get_activation(activation: Literal[\"elu\", \"snake\", \"none\"], antialias=False, channels=None) -> nn.Module\n\t\n    if activation == \"elu\":\n        return nn.ELU()\n    elif activation == \"snake\":\n        return SnakeBeta(channels)\n    elif activation == \"none\":\n        return nn.Identity()\n    else:\n        raise ValueError(f\"Activation type {activation} is not supported\")\n\n", "CodeBleu_score": 40.87459991014086}, {"code": "def get_activation(activation: Literal[\"elu\", \"snake\", \"none\"], antialias=False, channels=None) -> nn.Module\n\t\n    if activation == \"elu\":\n        return nn.ELU(inplace=True)\n    elif activation == \"snake\":\n        return SnakeBeta(channels, antialias=antialias)\n    elif activation == \"none\":\n        return nn.Identity()\n    else:\n        raise ValueError(f\"Activation type {activation} not recognized.\")\n\n", "CodeBleu_score": 39.25127565740499}, {"code": "def get_activation(activation: Literal[\"elu\", \"snake\", \"none\"], antialias=False, channels=None) -> nn.Module\n\t\n    if activation == \"elu\":\n        if antialias:\n            return Activation1d(nn.ELU())\n        return nn.ELU()\n    elif activation == \"snake\":\n        if antialias:\n            return Activation1d(SnakeBeta(channels))\n        return SnakeBeta(channels)\n    elif activation == \"none\":\n        return nn.Identity()\n    else:\n        raise ValueError(f\"Unknown activation type {activation}\")\n\n", "CodeBleu_score": 48.548979121287914}]}
{"instance_id": "ef1714c79e51", "generation_result": [{"code": "def apply_rotary_pos_emb(t, freqs, scale = 1)\n\t\n    if t.dtype != torch.float32:\n        t = t.float()\n    freqs = freqs.float()\n    if len(freqs.shape) == 1:\n        freqs = freqs.unsqueeze(1)\n    if len(t.shape) == 3:\n        t = rearrange(t, 'b s d -> b s () d')\n    if len(t.shape) == 2:\n        t = rearrange(t, 'b s -> b s ()')\n    t_shape, t_axes = t.shape, t.shape[-1]\n    freqs = repeat(freqs, 'n d -> () n d', n = t_shape[1])\n    t, t_pass = t[..., :t_axes // 2], t[..., t_axes // 2:]\n    t = (t * scale).cos()\n    return torch.cat((t, t_pass), dim = -1)\n\n", "CodeBleu_score": 39.26516475947046}, {"code": "def apply_rotary_pos_emb(t, freqs, scale = 1)\n\t\n    t = t.float()\n    freqs = freqs.float()\n    rot_dim = freqs.shape[-1]\n    t, orig_shape, tail_shape = _cast_and_flatten(t, rot_dim)\n    freqs, _ = _cast_and_flatten(freqs, rot_dim)\n    t = apply_rotary_pos_emb_nd(t, freqs, scale)\n    t = _unflatten(t, orig_shape, tail_shape)\n    return t\n\n", "CodeBleu_score": 11.595889156862055}, {"code": "def apply_rotary_pos_emb(t, freqs, scale = 1)\n\t\n    t = t.float()\n    freqs = freqs.float()\n    rot_dim = freqs.shape[-1]\n    t, orig_shape, tail_shape = _cast_and_split(t, rot_dim)\n    freqs, _ = _cast_and_split(freqs, rot_dim)\n    t = _apply_rotary_pos_emb(t, freqs, scale)\n    t = _merge_rotated_with_unrotated(t, orig_shape, tail_shape)\n    return t\n", "CodeBleu_score": 12.151444651008141}]}
{"instance_id": "a7a5f178c6f6", "generation_result": [{"code": "def idx2vq(self, idx, dim=-1)\n\t\n    if dim == -1:\n        return self.codebook.weight[idx]\n    else:\n        return self.codebook.weight[idx].movedim(-1, dim)\n\n", "CodeBleu_score": 29.564357945754953}, {"code": "def idx2vq(self, idx, dim=-1)\n\t\n    return self.codebook.embedding(idx, dim=dim)\n\n", "CodeBleu_score": 18.0408908271316}, {"code": "def idx2vq(self, idx, dim=-1)\n\t\n    emb = self.codebook(idx)\n    if dim != -1:\n        emb = emb.transpose(-1, dim)\n    return emb\n\n", "CodeBleu_score": 41.097048603360335}]}
{"instance_id": "4104c041fb20", "generation_result": [{"code": "def encode(self, x, quantize=False)\n\t\n\n    # Initialize the encoded output\n    enc = x\n\n    # Encode the input\n    for block in self.blocks:\n        enc = block(enc)\n\n    # If quantization is enabled, apply the vector quantizer\n    if quantize:\n        enc, enc_q, indices, loss = self.quantizer(enc)\n        return enc, enc_q, indices, loss\n\n    # If quantization is not enabled, return the encoded output\n    return enc\n\n", "CodeBleu_score": 26.6936286419849}, {"code": "def encode(self, x, quantize=False)\n\t\n    x = self.blocks(x)\n    x = self.quantizer(x, quantize)\n    return x\n\n", "CodeBleu_score": 9.605483102526492}, {"code": "def encode(self, x, quantize=False)\n\t\n    x = self.blocks(x)\n    if quantize:\n        x_hat, x_q, loss = self.quantize(x)\n        return x_hat, x_q, loss\n    else:\n        return x\n\n", "CodeBleu_score": 20.168886283903998}]}
{"instance_id": "403c52ba2d3b", "generation_result": [{"code": "def ema_scope(self, context=None)\n\t\n    if self.ema is not None:\n        if context is not None:\n            context.log(\"Switching to EMA weights\")\n        self.model.load_state_dict(self.ema.get_averages())\n        self.model.train()\n    yield\n    if self.ema is not None:\n        if context is not None:\n            context.log(\"Restoring original weights\")\n        self.model.load_state_dict(self.model_state)\n        self.model.train()\n\n", "CodeBleu_score": 56.053093514339736}, {"code": "def ema_scope(self, context=None)\n\t\n    if self.ema is None:\n        return contextmanager(lambda _: iter([]))()\n    else:\n        return self.ema.average_parameters(self.model, context=context)\n\n", "CodeBleu_score": 13.511279527273487}, {"code": "def ema_scope(self, context=None)\n\t\n    if self.ema is not None:\n        if context is not None:\n            context.log(f\"Using EMA weights for {context.name}\")\n        self.model.apply(self.ema.assign)\n        yield\n        if context is not None:\n            context.log(f\"Restoring original weights for {context.name}\")\n        self.model.apply(self.ema.resume)\n    else:\n        yield\n\n", "CodeBleu_score": 49.85251063672066}]}
{"instance_id": "94b7467f98e4", "generation_result": [{"code": "def post_attention(self, attn, x, gate_msa, shift_mlp, scale_mlp, gate_mlp)\n\t\n    x = x + attn * gate_msa\n    x = self.norm(x)\n    x = x * shift_mlp + scale_mlp\n    x = x + self.mlp(x) * gate_mlp\n    return x\n\n", "CodeBleu_score": 35.84919383205091}, {"code": "def post_attention(self, attn, x, gate_msa, shift_mlp, scale_mlp, gate_mlp)\n\t\n    # print(\"post_attention\")\n    # print(attn.shape)\n    # print(x.shape)\n    # print(gate_msa.shape)\n    # print(shift_mlp.shape)\n    # print(scale_mlp.shape)\n    # print(gate_mlp.shape)\n    x = x + gate_msa * attn\n    x = self.norm_msa(x)\n    x = x + shift_mlp * (x - x.mean(dim=-1, keepdim=True))\n    x = x + scale_mlp * (x - x.mean(dim=-1, keepdim=True))\n    x = x * gate_mlp\n    return x\n\n", "CodeBleu_score": 43.160002728305514}, {"code": "def post_attention(self, attn, x, gate_msa, shift_mlp, scale_mlp, gate_mlp)\n\t\n    # Add the attention output to the input\n    x = x + gate_msa * attn\n    # Normalize the result\n    x = x / x.sum(dim=-1, keepdim=True)\n    # Modulate the result\n    x = x * shift_mlp + scale_mlp\n    # Apply the MLP\n    x = x * gate_mlp + x\n    return x\n\n", "CodeBleu_score": 33.09478742924207}]}
{"instance_id": "d9fdc05542a2", "generation_result": [{"code": "def _block_mixing(context, x, context_block, x_block, c)\n\t\n\n    # Pre-attention processing\n    context_qkv, context_intermediates = context_block.pre_attention(context)\n    x_qkv, x_intermediates = x_block.pre_attention(x)\n\n    # Concatenate qkv tensors\n    qkv = torch.cat([context_qkv, x_qkv], dim=1)\n\n    # Optimized attention\n    qkv = _attention(qkv, c)\n\n    # Split qkv tensors\n    context_qkv, x_qkv = torch.split(qkv, context_qkv.shape[1], dim=1)\n\n    # Post-attention processing\n    context = context_block.post_attention(context, context_qkv, context_intermediates)\n    x = x_block.post_attention(x, x_qkv, x_intermediates)\n\n    return context, x\n\n", "CodeBleu_score": 19.388986457252543}, {"code": "def _block_mixing(context, x, context_block, x_block, c)\n\t\n\n    # Process the context and x through their respective pre-attention blocks to obtain query, key, and value (qkv) tensors and intermediates.\n    context_pre_attention, context_intermediates = context_block.pre_attention(context)\n    x_pre_attention, x_intermediates = x_block.pre_attention(x)\n\n    # Concatenate the qkv tensors.\n    qkv = torch.cat((context_pre_attention, x_pre_attention), dim=2)\n\n    # Apply optimized attention.\n    qkv = qkv * (c ** -0.5)\n    qkv = rearrange(qkv, 'b n (h d) -> (b h) n d', h=2)\n    q, k, v = qkv.chunk(3, dim=2)\n    attn_mask = torch.zeros(q.shape[0], q.shape[1], dtype=torch.bool).to(q.device)\n    attn_mask = rearrange(attn_mask, 'b n -> b () n ()')\n    out = torch.bmm(q, k.transpose(1, 2))\n    out = torch.softmax(out, dim=-1)\n    out = torch.bmm(out, v)\n    out = rearrange(out, '(b h) n d -> b n (h d)', h=2)\n\n    # Split the attention results back into context and x parts.\n    context_attn_out, x_attn_out = out.chunk(2, dim=2)\n\n    # Process the context and x through their respective post-attention blocks, depending on the configuration of the context block, and return the updated context and x.\n    context_post_attention = context_block.post_attention(context_attn_out, context_intermediates)\n    x_post_attention = x_block.post_attention(x_attn_out, x_intermediates)\n    return context_post_attention, x_post_attention\n\n", "CodeBleu_score": 30.367815155909383}, {"code": "def _block_mixing(context, x, context_block, x_block, c)\n\t\n    # pre-attention blocks\n    context_qkv, context_intermediates = context_block(context, c)\n    x_qkv, x_intermediates = x_block(x, c)\n\n    # block mixing\n    qkv = torch.cat([context_qkv, x_qkv], dim=2)\n    qkv = rearrange(qkv, 'b (h d) n -> b h n d', h=2)\n    q, k, v = qkv.unbind(dim=1)\n    attn_out = optimized_attention(q, k, v, c)\n    attn_out = rearrange(attn_out, 'b h n d -> b (h d) n')\n    context_attn_out, x_attn_out = attn_out.split([context_qkv.shape[-1], x_qkv.shape[-1]], dim=1)\n\n    # post-attention blocks\n    if context_block.post_attention is not None:\n        context = context_block.post_attention(context, context_attn_out, context_intermediates, c)\n    if x_block.post_attention is not None:\n        x = x_block.post_attention(x, x_attn_out, x_intermediates, c)\n\n    return context, x\n\n", "CodeBleu_score": 34.00797716009516}]}
{"instance_id": "b219bce30160", "generation_result": [{"code": "def get_timestep_embedding(timesteps, embedding_dim)\n\t\n    assert (\n        timesteps.dim() == 1\n    ), f\"timesteps must be one-dimensional, got {timesteps.dim()} dimensions\"\n    assert embedding_dim % 2 == 0, f\"embedding dimension must be even, got {embedding_dim}\"\n\n    half_dim = embedding_dim // 2\n    emb = math.log(10000) / (half_dim - 1)\n    emb = torch.exp(torch.arange(half_dim, dtype=torch.float32) * -emb)\n    emb = timesteps.float()[:, None] * emb[None, :]\n    emb = torch.cat((torch.sin(emb), torch.cos(emb)), dim=1)\n    if embedding_dim % 2 == 1:\n        emb = torch.nn.functional.pad(emb, (0, 1))\n    return emb\n\n", "CodeBleu_score": 70.48493849950681}, {"code": "def get_timestep_embedding(timesteps, embedding_dim)\n\t\n    assert len(timesteps.shape) == 1, \"timesteps input must be one-dimensional\"\n\n    dim_div_2 = embedding_dim // 2\n    dim_mod_2 = embedding_dim % 2\n\n    # Calculate the sine and cosine embedding\n    embed = torch.zeros(timesteps.shape[0], embedding_dim)\n    embed[:, :dim_div_2] = (\n        math.log(10000) / (dim_div_2 - 1) * timesteps.unsqueeze(1).float()\n    ).sin()\n    embed[:, dim_div_2:] = (\n        math.log(10000) / (dim_div_2 - 1) * timesteps.unsqueeze(1).float()\n    ).cos()\n\n    # Zero-pad the embedding if the dimension is odd\n    if dim_mod_2 == 1:\n        embed = torch.nn.functional.pad(embed, (0, 1))\n\n    return embed\n\n", "CodeBleu_score": 37.517818019725816}, {"code": "def get_timestep_embedding(timesteps, embedding_dim)\n\t\n    assert (\n        timesteps.dim() == 1\n    ), f\"Timestep embedding must be one-dimensional, but got shape {timesteps.shape}\"\n    assert (\n        embedding_dim % 2 == 0\n    ), f\"Embedding dimension must be even, but got {embedding_dim}\"\n    half_dim = embedding_dim // 2\n    emb = math.log(10000) / (half_dim - 1)\n    emb = torch.exp(torch.arange(half_dim, dtype=torch.float32) * -emb)\n    emb = timesteps.float()[:, None] * emb[None, :]\n    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n    if emb.shape[1] < embedding_dim:\n        pad = torch.zeros([emb.shape[0], embedding_dim - emb.shape[1]])\n        emb = torch.cat([emb, pad], dim=1)\n    return emb\n\n", "CodeBleu_score": 70.28218141559046}]}
{"instance_id": "6ae8453c86be", "generation_result": [{"code": "def pytorch_attention(q, k, v)\n\t\n    batch_size, num_heads, seq_len, hidden_size_per_head = q.shape\n    assert k.shape == v.shape == q.shape\n    assert hidden_size_per_head * num_heads == v.shape[-1]\n\n    try:\n        return _pytorch_attention_impl(q, k, v)\n    except RuntimeError as e:\n        if \"CUDA out of memory\" in str(e):\n            logging.warning(\n                \"Attention OOM, switching to slice attention. \"\n                \"This will be slower but will use less memory.\"\n            )\n            return _pytorch_attention_slice_impl(q, k, v)\n        raise\n\n", "CodeBleu_score": 18.778027326509736}, {"code": "def pytorch_attention(q, k, v)\n\t\n    if q.dim() == 4:\n        return pytorch_attention_2d(q, k, v)\n    elif q.dim() == 5:\n        return pytorch_attention_3d(q, k, v)\n    else:\n        raise ValueError(f\"Unsupported number of dimensions: {q.dim()}\")\n\n", "CodeBleu_score": 7.4202576673596115}, {"code": "def pytorch_attention(q, k, v)\n\t\n    b, h, s, d = q.shape\n    q = q.reshape(b * h, s, d).transpose(1, 2)\n    k = k.reshape(b * h, s, d).transpose(1, 2)\n    v = v.reshape(b * h, s, d).transpose(1, 2)\n    try:\n        attn = torch.bmm(q, k.transpose(1, 2))\n    except RuntimeError as e:\n        if \"CUDA out of memory\" in str(e):\n            logging.warning(\n                \"CUDA out of memory. Switching to slice attention.\"\n            )\n            attn = comfy.ops.slice_attention(q, k, v)\n        else:\n            raise e\n    attn = attn.transpose(1, 2).reshape(b, h, s, s)\n    return attn\n\n", "CodeBleu_score": 27.174016021226322}]}
{"instance_id": "fe4433c987ad", "generation_result": [{"code": "def apply_control(h, control, name)\n\t\n    if control is not None and name in control:\n        c = control.pop(name)\n        if c is not None:\n            try:\n                h = h + c\n            except Exception as e:\n                logging.warning(f\"Failed to add control {name} to {h.shape} with {c.shape}.", "CodeBleu_score": 46.65440415370304}, {"code": "def apply_control(h, control, name)\n\t\n    if control is not None and name in control:\n        c = control.pop(name)\n        if exists(c):\n            try:\n                h = h + c\n            except Exception as e:\n                logging.warning(f\"Failed to add control {name} to {h.name}: {e}\")\n    return h\n\n", "CodeBleu_score": 43.72560427144552}, {"code": "def apply_control(h, control, name)\n\t\n    if not exists(control) or not exists(name):\n        return h\n    if not exists(control[name]):\n        return h\n    control = control[name]\n    if not exists(control):\n        return h\n    try:\n        h = h + control\n    except:\n        logging.warning(f\"Could not apply control {name} to h.\")\n    return h\n\n", "CodeBleu_score": 38.14724474810443}]}
{"instance_id": "56da4547c101", "generation_result": [{"code": "def get_codebook_entry(self, indices, shape)\n\t\n    one_hot = torch.nn.functional.one_hot(indices.view(-1), num_classes=self.embedding.shape[0])\n    quantized = one_hot @ self.embedding.T\n    if shape is not None:\n        return quantized.view(shape).permute(0, 3, 1, 2).contiguous()\n    else:\n        return quantized\n\n", "CodeBleu_score": 34.449138004775634}, {"code": "def get_codebook_entry(self, indices, shape)\n\t\n    one_hot = F.one_hot(indices, self.num_embeddings)\n    quantized = torch.einsum(\"...nd,ncd->...nc\", one_hot.type(self.weight.dtype), self.weight)\n    if shape is not None:\n        return quantized.reshape(shape).permute(0, 3, 1, 2).contiguous()\n    else:\n        return quantized\n\n", "CodeBleu_score": 29.411577717985953}, {"code": "def get_codebook_entry(self, indices, shape)\n\t\n    one_hot = torch.nn.functional.one_hot(indices.view(-1), self.num_embeddings).type(self.weight.dtype)\n    quantized = torch.matmul(one_hot, self.weight)\n    quantized = quantized.view(shape) if shape else quantized\n    quantized = quantized.permute(0, 3, 1, 2) if shape else quantized\n    return quantized\n\n", "CodeBleu_score": 17.756840157122806}]}
{"instance_id": "548f8a1fb37d", "generation_result": [{"code": "def make_noise(self)\n\t\n    noise = []\n    # Initialization\n    noise.append(torch.randn(4, 4, device=self.device))\n    # Resize\n    for i in range(3, self.log_size):\n        noise.append(F.interpolate(noise[0], scale_factor=2, mode='bilinear'))\n        noise.append(F.interpolate(noise[0], scale_factor=0.5, mode='bilinear'))\n    return noise\n\n", "CodeBleu_score": 24.357076472082376}, {"code": "def make_noise(self)\n\t\n    # initialize the list of noise tensors\n    noise = [torch.randn(4, 4, device=self.device)]\n    # generate the noise tensors for the other sizes\n    for i in range(3, self.log_size):\n        size = 2 ** i\n        noise.append(self.upsample(noise[-1], size))\n        noise.append(self.upsample(noise[-1], size))\n    return noise\n\n", "CodeBleu_score": 27.569931359593873}, {"code": "def make_noise(self)\n\t\n    noise = torch.randn(4, 4, device=self.device)\n    noise_list = [noise]\n    for i in range(3, self.log_size):\n        noise_list.append(F.interpolate(noise, scale_factor=2, mode='bilinear'))\n        noise_list.append(F.interpolate(noise, scale_factor=0.5, mode='bilinear'))\n        noise = noise_list[-1]\n    return noise_list\n\n", "CodeBleu_score": 24.44498229525314}]}
{"instance_id": "42fd805563c6", "generation_result": [{"code": "def mean_latent(self, num_latent)\n\t\n    device = self.device\n    # Create random latent inputs\n    latent_input = torch.randn(num_latent, self.latent_dim, device=device)\n    # Process the latent inputs through the style MLP\n    latent_output = self.style_mlp(latent_input)\n    # Compute the mean of the processed latents\n    mean_latent = torch.mean(latent_output, dim=0, keepdim=True)\n    return mean_latent\n\n", "CodeBleu_score": 22.43036355611099}, {"code": "def mean_latent(self, num_latent)\n\t\n    # Create random latent inputs\n    latents = torch.randn(num_latent, self.latent_dim, device=self.device)\n    # Process them through the style MLP\n    latents = self.style_mlp(latents)\n    # Compute the mean of the processed latents\n    return torch.mean(latents, dim=0)\n\n", "CodeBleu_score": 27.97624968098124}, {"code": "def mean_latent(self, num_latent)\n\t\n    z_sample = torch.randn(1, num_latent, 1, 1, device=self.device)\n    z_sample = self.style_mlp(z_sample)\n    z_sample = z_sample.view(1, -1)\n    return z_sample\n\n", "CodeBleu_score": 32.2460118656891}]}
{"instance_id": "2f485fd924af", "generation_result": [{"code": "def color_jitter(img, shift)\n\t\n    img = img.astype('float32')\n    img += shift\n    img = np.clip(img, 0, 1)\n    return img\n\n", "CodeBleu_score": 40.182139884787574}, {"code": "def color_jitter(img, shift)\n\t\n    img = img.permute(1, 2, 0).numpy()\n    img = img * 255\n    img = img.astype('uint8')\n    img = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)\n\n    # hsv_shift = np.random.uniform(-shift, shift, 3)\n    hsv_shift = [shift, shift, shift]\n    hsv_shift = np.array(hsv_shift, dtype=np.uint8)\n\n    img = img + hsv_shift\n    img = np.clip(img, 0, 255)\n\n    img = cv2.cvtColor(img.astype('uint8'), cv2.COLOR_HSV2RGB)\n    img = img / 255\n    img = img.permute(2, 0, 1)\n    img = torch.from_numpy(img).float()\n    return img\n\n", "CodeBleu_score": 44.64188585450183}, {"code": "def color_jitter(img, shift)\n\t\n    img = img.astype(float)\n    color_shift = [\n        np.random.uniform(-shift, shift),\n        np.random.uniform(-shift, shift),\n        np.random.uniform(-shift, shift),\n    ]\n    img[:, :, 0] = np.clip(img[:, :, 0] + color_shift[0], 0, 1)\n    img[:, :, 1] = np.clip(img[:, :, 1] + color_shift[1], 0, 1)\n    img[:, :, 2] = np.clip(img[:, :, 2] + color_shift[2], 0, 1)\n    return img\n\n", "CodeBleu_score": 26.696929926755626}]}
{"instance_id": "c87ccf1fbf48", "generation_result": [{"code": "def feed_data(self, data)\n\t\n    self.lq = data['lq'].to(self.device)\n    if 'gt' in data:\n        self.gt = data['gt'].to(self.device)\n    if 'left_eye' in data:\n        self.left_eye = data['left_eye'].to(self.device)\n    if 'right_eye' in data:\n        self.right_eye = data['right_eye'].to(self.device)\n    if 'mouth' in data:\n        self.mouth = data['mouth'].to(self.device)\n\n", "CodeBleu_score": 56.77114126112309}, {"code": "def feed_data(self, data)\n\t\n    self.lq = data['lq'].to(self.device)\n    if 'gt' in data:\n        self.gt = data['gt'].to(self.device)\n    if 'face_landmarks' in data:\n        self.face_landmarks = data['face_landmarks'].to(self.device)\n        self.left_eye = data['face_landmarks'][:, 0, :]\n        self.right_eye = data['face_landmarks'][:, 1, :]\n        self.mouth = data['face_landmarks'][:, 2, :]\n\n", "CodeBleu_score": 54.06077588630861}, {"code": "def feed_data(self, data)\n\t\n    self.lq = data['lq'].to(self.device)\n    if 'gt' in data:\n        self.gt = data['gt'].to(self.device)\n    if 'left_eye' in data:\n        self.left_eye = data['left_eye']\n    if 'right_eye' in data:\n        self.right_eye = data['right_eye']\n    if 'mouth' in data:\n        self.mouth = data['mouth']\n\n", "CodeBleu_score": 59.83417022228472}]}
{"instance_id": "8f8c7cb13037", "generation_result": [{"code": "def construct_img_pyramid(self)\n\t\n    img_pyramid = [self.gt]\n    for _ in range(self.opt.get('num_intermediate_restoration_loss', 0)):\n        img_pyramid.insert(0, F.interpolate(img_pyramid[0], scale_factor=0.5, mode='bilinear', align_corners=False))\n    return img_pyramid\n\n", "CodeBleu_score": 27.6665559926035}, {"code": "def construct_img_pyramid(self)\n\t\n    img_pyramid = [self.gt]\n    for i in range(1, self.num_feats):\n        img_pyramid.insert(0, F.interpolate(img_pyramid[0], scale_factor=0.5, mode='bilinear', align_corners=False))\n    return img_pyramid\n\n", "CodeBleu_score": 25.888283559629404}, {"code": "def construct_img_pyramid(self)\n\t\n    img_pyramid = [self.gt]\n    for i in range(self.opt['num_intermediate_restoration']):\n        img_pyramid.insert(0, F.interpolate(img_pyramid[0], scale_factor=0.5, mode='bilinear', align_corners=False))\n    return img_pyramid\n\n", "CodeBleu_score": 24.077268589376864}]}
{"instance_id": "51247274d40c", "generation_result": [{"code": "def save(self, epoch, current_iter)\n\t\n\n    state_dict = OrderedDict()\n\n    # save the state of the model\n    state_dict['net_g'] = self.net_g.state_dict()\n    state_dict['net_g_ema'] = self.net_g_ema.state_dict()\n    state_dict['net_d'] = self.net_d.state_dict()\n    if self.opt['use_face_discriminator']:\n        state_dict['net_d_eye_left'] = self.net_d_eye_left.state_dict()\n        state_dict['net_d_eye_right'] = self.net_d_eye_right.state_dict()\n        state_dict['net_d_mouth'] = self.net_d_mouth.state_dict()\n\n    # save the overall training state\n    state_dict['epoch'] = epoch\n    state_dict['iter'] = current_iter\n    state_dict['optimizers'] = []\n    for optimizer in self.optimizers:\n        state_dict['optimizers'].append(optimizer.state_dict())\n    state_dict['schedulers'] = []\n    for scheduler in self.schedulers:\n        state_dict['schedulers'].append(scheduler.state_dict())\n    state_dict['lr'] = self.get_current_learning_rate()\n    state_dict['best_pred'] = self.best_pred\n\n    # save the model\n    model_path = os.path.join(self.opt['path']['models'], f'{self.opt[\"name\"]}_{current_iter}.pth')\n    torch.save(state_dict, model_path)\n    self.print_network(self.net_g)\n\n    # save the best model\n    if self.best_pred < self.metric_worst:\n        self.best_pred = self.metric_worst\n        best_model_path = os.path.join(self.opt['path']['models'], f'{self.opt[\"name\"]}_best.pth')\n        torch.save(state_dict, best_model_path)\n", "CodeBleu_score": 35.81399395871451}, {"code": "def save(self, epoch, current_iter)\n\t\n    if not hasattr(self, 'net_g_ema'):\n        self.net_g_ema = self.net_g.clone().detach()\n        for p in self.net_g_ema.parameters():\n            p.requires_grad = False\n\n    state_dict = OrderedDict()\n    state_dict['net_g'] = self.net_g.state_dict()\n    state_dict['net_g_ema'] = self.net_g_ema.state_dict()\n    state_dict['net_d'] = self.net_d.state_dict()\n    if self.opt['use_eye_discrim']:\n        state_dict['net_d_eye'] = self.net_d_eye.state_dict()\n    if self.opt['use_mouth_discrim']:\n        state_dict['net_d_mouth'] = self.net_d_mouth.state_dict()\n    state_dict['optimizer_g'] = self.optimizer_g.state_dict()\n    state_dict['optimizer_d'] = self.optimizer_d.state_dict()\n    state_dict['epoch'] = epoch\n    state_dict['iter'] = current_iter\n    state_dict['best_pred'] = self.best_pred\n    state_dict['records'] = self.records\n    state_dict['net_g_best'] = self.net_g_best.state_dict()\n    state_dict['net_g_best_ema'] = self.net_g_best_ema.state_dict()\n    torch.save(state_dict, self.save_dir / f'{self.opt[\"name\"]}_{epoch:04d}_{current_iter:08d}.pth')\n\n", "CodeBleu_score": 28.65552355470029}, {"code": "def save(self, epoch, current_iter)\n\t\n    # save network\n    torch.save(self.net_g.state_dict(), self.opt['path']['models'] + f'/net_g_{epoch:04d}.pth')\n    torch.save(self.net_g_ema.state_dict(), self.opt['path']['models'] + f'/net_g_ema_{epoch:04d}.pth')\n    torch.save(self.net_d.state_dict(), self.opt['path']['models'] + f'/net_d_{epoch:04d}.pth')\n\n    # save discriminators for facial parts\n    if self.opt['facial_discriminator']:\n        torch.save(self.net_d_left_eye.state_dict(), self.opt['path']['models'] + f'/net_d_left_eye_{epoch:04d}.pth')\n        torch.save(self.net_d_right_eye.state_dict(), self.opt['path']['models'] + f'/net_d_right_eye_{epoch:04d}.pth')\n        torch.save(self.net_d_mouth.state_dict(), self.opt['path']['models'] + f'/net_d_mouth_{epoch:04d}.pth')\n\n    # save training state\n    state = {\n        'epoch': epoch,\n        'iter': current_iter,\n        'optimizers': {},\n        'schedulers': {}\n    }\n    state['optimizers']['G'] = self.optimizer_g.state_dict()\n    state['optimizers']['D'] = self.optimizer_d.state_dict()\n    state['schedulers']['G'] = self.scheduler_g.state_dict()\n    state['schedulers']['D'] = self.scheduler_d.state_dict()\n    torch.save(state, self.opt['path']['training_state'] + f'/training_state_{epoch:04d}.pth')\n\n", "CodeBleu_score": 35.635911433553176}]}
{"instance_id": "afa0dc8128c1", "generation_result": [{"code": "def clean_folder(folder)\n\t\n    for filename in os.listdir(folder):\n        file_path = os.path.join(folder, filename)\n        try:\n            if os.path.isfile(file_path) or os.path.islink(file_path):\n                os.unlink(file_path)\n            elif os.path.isdir(file_path):\n                shutil.rmtree(file_path)\n        except Exception as e:\n            print('Failed to delete %s. Reason: %s' % (file_path, e))\n\n", "CodeBleu_score": 69.71577776852638}, {"code": "def clean_folder(folder)\n\t\n    for filename in os.listdir(folder):\n        file_path = os.path.join(folder, filename)\n        try:\n            if os.path.isfile(file_path) or os.path.islink(file_path):\n                os.unlink(file_path)\n            elif os.path.isdir(file_path):\n                shutil.rmtree(file_path)\n        except Exception as e:\n            print('Failed to delete %s. Reason: %s' % (file_path, e))\n\n", "CodeBleu_score": 69.71577776852638}, {"code": "def clean_folder(folder)\n\t\n    for filename in os.listdir(folder):\n        file_path = os.path.join(folder, filename)\n        try:\n            if os.path.isfile(file_path) or os.path.islink(file_path):\n                os.unlink(file_path)\n            elif os.path.isdir(file_path):\n                shutil.rmtree(file_path)\n        except Exception as e:\n            print('Failed to delete %s. Reason: %s' % (file_path, e))\n\n", "CodeBleu_score": 69.71577776852638}]}
{"instance_id": "53588b481bdd", "generation_result": [{"code": "def next_batch(self, sample_size=1)\n\t\n    if sample_size >= len(self.experiences):\n        sample_size = len(self.experiences)\n        self.experiences = []\n    else:\n        self.experiences = self.experiences[sample_size:]\n\n    batch = random.sample(self.experiences, sample_size)\n    return batch\n", "CodeBleu_score": 46.26633993211271}, {"code": "def next_batch(self, sample_size=1)\n\t\n\n    if len(self.memory) < sample_size:\n        sample_size = len(self.memory)\n\n    sample_indexes = random.sample(range(len(self.memory)), sample_size)\n    sample_batch = [self.memory[i] for i in sample_indexes]\n    for i in sorted(sample_indexes, reverse=True):\n        del self.memory[i]\n\n    return sample_batch\n", "CodeBleu_score": 42.95292958394693}, {"code": "def next_batch(self, sample_size=1)\n\t\n\n    if sample_size > len(self.experiences):\n        sample_size = len(self.experiences)\n\n    batch = random.sample(self.experiences, sample_size)\n    for experience in batch:\n        self.experiences.remove(experience)\n    return batch\n", "CodeBleu_score": 49.82036836182859}]}
{"instance_id": "6bfa6a4e621c", "generation_result": [{"code": "def find_estimated_top_configs(self)\n\t\n\n    if self.cost_model is None:\n        raise ValueError(\"Cost model is not defined.\")\n\n    if self.metric not in [\"min\", \"max\"]:\n        raise ValueError(\"Metric must be either 'min' or 'max'.\")\n\n    if self.top_configs is None:\n        raise ValueError(\"Number of top configurations must be defined.\")\n\n    if self.top_configs > len(self.all_configs):\n        raise ValueError(\"Number of top configurations cannot exceed total number of configurations.\")\n\n    if self.top_configs <= 0:\n        raise ValueError(\"Number of top configurations must be greater than zero.\")\n\n    # Flatten all configurations and collect numerical values\n    all_configs_flat = []\n    for config in self.all_configs:\n        config_flat = flatten_config(config)\n        all_configs_flat.append(config_flat)\n\n    # Convert to numpy array\n    all_configs_flat = np.array(all_configs_flat)\n\n    # Predict estimates for all configurations\n    estimates = self.cost_model.predict(all_configs_flat)\n\n    # Sort estimates\n    if self.metric == \"min\":\n        sort_index = np.argsort(estimates)\n    else:\n        sort_index = np.argsort(-estimates)\n\n    # Return indices of top configurations\n    top_config_indices = sort_index[:self.top_configs]\n\n    return top_config_indices\n\n", "CodeBleu_score": 31.962670735978772}, {"code": "def find_estimated_top_configs(self)\n\t\n\n    # Flatten all configurations\n    all_configs = []\n    for config in self.all_configs:\n        all_configs.append(self.flatten_config(config))\n\n    # Find numerical values\n    numerical_values = []\n    for config in all_configs:\n        numerical_values.append(self.get_numerical_values(config))\n\n    # Get cost estimates\n    cost_estimates = self.cost_model.predict(numerical_values)\n\n    # Sort by cost estimates\n    sorted_indices = []\n    if self.metric == \"min\":\n        sorted_indices = cost_estimates.argsort()\n    elif self.metric == \"max\":\n        sorted_indices = cost_estimates.argsort()[::-1]\n    else:\n        raise ValueError(\n            \"The metric must be either 'min' or 'max', but is '%s'.\" % self.metric\n        )\n\n    # Return top configurations\n    top_configs = []\n    for index in sorted_indices:\n        top_configs.append(self.all_configs[index])\n\n    return top_configs\n\n", "CodeBleu_score": 31.723592830746423}, {"code": "def find_estimated_top_configs(self)\n\t\n\n    # Flatten all configurations in all_configs\n    all_configs = []\n    for config in self.all_configs:\n        flat_config = self.flatten_config(config)\n        all_configs.append(flat_config)\n\n    # Convert all configurations into a numpy array\n    all_configs_array = self.convert_configs_to_array(all_configs)\n\n    # Predict the cost model estimates for all configurations\n    estimates = self.cost_model.predict(all_configs_array)\n\n    # Sort the estimates and return the top configurations\n    if self.metric == \"cost\":\n        top_configs = self.get_top_configs_cost(estimates)\n    elif self.metric == \"time\":\n        top_configs = self.get_top_configs_time(estimates)\n    elif self.metric == \"accuracy\":\n        top_configs = self.get_top_configs_accuracy(estimates)\n    else:\n        raise ValueError(\"Invalid metric: \" + self.metric)\n\n    return top_configs\n\n", "CodeBleu_score": 29.729921034591182}]}
{"instance_id": "d4cf48413a57", "generation_result": [{"code": "def flatten(d, parent_key='', sep='_')\n\t\n    items = []\n    for k, v in d.items():\n        new_key = parent_key + sep + k if parent_key else k\n        if isinstance(v, collections.abc.MutableMapping):\n            items.extend(flatten(v, new_key, sep=sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)\n\n", "CodeBleu_score": 82.56863782179195}, {"code": "def flatten(d, parent_key='', sep='_')\n\t\n    items = []\n    for k, v in d.items():\n        new_key = parent_key + sep + k if parent_key else k\n        if isinstance(v, collections.abc.MutableMapping):\n            items.extend(flatten(v, new_key, sep=sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)\n\n", "CodeBleu_score": 82.56863782179195}, {"code": "def flatten(d, parent_key='', sep='_')\n\t\n    items = []\n    for k, v in d.items():\n        new_key = parent_key + sep + k if parent_key else k\n        if isinstance(v, collections.abc.MutableMapping):\n            items.extend(flatten(v, new_key, sep=sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)\n\n", "CodeBleu_score": 82.56863782179195}]}
{"instance_id": "c2a514249c25", "generation_result": [{"code": "def run_after_tuning(self)\n\t\n\n    if self.optimal_cmd is not None:\n        self.log.info(f\"Optimal command: {self.optimal_cmd}\")\n        self.log.info(f\"Running: {self.optimal_cmd}\")\n        self.log.info(f\"Output:\")\n        self.log.info(f\"{subprocess.check_output(self.optimal_cmd, shell=True)}\")\n        self.log.info(f\"Successfully ran command: {self.optimal_cmd}\")\n    else:\n        self.log.info(f\"No optimal command found\")\n\n", "CodeBleu_score": 30.629386926947976}, {"code": "def run_after_tuning(self)\n\t\n    if self.optimal_cmd:\n        start_time = time.time()\n        self.log(f\"Running the optimal command: {self.optimal_cmd}\")\n        self.log(f\"Command start time: {datetime.datetime.now()}\")\n        self.log(f\"Command end time: {datetime.datetime.now() + datetime.timedelta(seconds=self.optimal_duration)}\")\n        self.log(f\"Command duration: {self.optimal_duration}\")\n        self.log(f\"Command throughput: {self.optimal_throughput}\")\n        self.log(f\"Command accuracy: {self.optimal_accuracy}\")\n        self.log(f\"Command loss: {self.optimal_loss}\")\n        self.log(f\"Command hyperparameters: {self.optimal_hyperparameters}\")\n        self.log(f\"Command hyperparameters_flat: {self.optimal_hyperparameters_flat}\")\n        self.log(f\"Command hyperparameters_flat_valid: {self.optimal_hyperparameters_flat_valid}\")\n        self.log(f\"Command hyperparameters_flat_invalid: {self.optimal_hyperparameters_flat_invalid}\")\n        self.log(f\"Command hyperparameters_flat_valid_keys: {self.optimal_hyperparameters_flat_valid_keys}\")\n        self.log(f\"Command hyperparameters_flat_invalid_keys: {self.optimal_hyperparameters_flat_invalid_keys}\")\n        self.log(f\"Command hyperparameters_flat_valid_values: {self.optimal_hyperparameters_flat_valid_values}\")\n        self.log(f\"Command hyperparameters_flat_invalid_values: {self.optimal_hyperparameters_flat_invalid_values}\")\n        self.log(f\"Command hyperparameters_flat_valid_values_str: {self.optimal_hyperparameters_flat_valid_values_str}\")\n        self.log(f\"Command hyperparameters_flat_invalid_values_str: {self.optimal_hyperparameters_flat_invalid_values_str}\")\n        self.log(f\"Command hyperparameters_flat_valid_values_str_valid: {self.", "CodeBleu_score": 34.679078570448304}, {"code": "def run_after_tuning(self)\n\t\n\n    if self.optimal_cmd:\n        self.log_info(f'Autotuning found an optimal configuration: {self.optimal_cmd}')\n        self.log_info('Running the optimal configuration ...')\n        subprocess.Popen(self.optimal_cmd, shell=True).wait()\n        self.log_info('Autotuning completed successfully.')\n    else:\n        self.log_info('Autotuning did not find an optimal configuration.')\n\n", "CodeBleu_score": 34.68669327121376}]}
{"instance_id": "61802de52df0", "generation_result": [{"code": "def run_job(self, exp: dict, reservations)\n\t\n    if 'args' not in exp:\n        exp['args'] = {}\n    exp['args']['master_port'] = self.master_port\n    exp['args']['result_dir'] = self.result_dir\n    exp['args']['reservations'] = reservations\n    exp['args']['exp_id'] = exp['exp_id']\n    exp['args']['reservation_id'] = exp['reservation_id']\n    exp['args']['exp_dir'] = exp['exp_dir']\n    exp['args']['exp_name'] = exp['exp_name']\n    exp['args']['exp_type'] = exp['exp_type']\n    exp['args']['exp_config'] = exp['exp_config']\n    exp['args']['exp_config_path'] = exp['exp_config_path']\n    exp['args']['exp_config_name'] = exp['exp_config_name']\n    exp['args']['exp_config_dir'] = exp['exp_config_dir']\n    exp['args']['exp_config_ext'] = exp['exp_config_ext']\n    exp['args']['exp_config_file'] = exp['exp_config_file']\n    exp['args']['exp_config_json'] = exp['exp_config_json']\n    exp['args']['exp_config_json_path'] = exp['exp_config_json_path']\n    exp['args']['exp_config_json_name'] = exp['exp_config_json_name']\n    exp['args']['exp_config_json_dir'] = exp['exp_config_json_dir']\n    exp['args']['exp_config_json_ext'] = exp['exp_config_json_ext']\n    exp['args']['exp_config_json_file'] = exp['exp_config_json_file']\n    exp['args']['exp_config_yaml'] = exp['exp_config_yaml']\n    exp['args']['exp_config_yaml_path'] = exp['exp_config_yaml_path']\n    exp['args']['exp_config_yaml_name'] = exp['exp_config_yaml_name']\n    exp['args']['exp_config_yaml_dir'] = exp['exp_config_yaml", "CodeBleu_score": 17.520437446054903}, {"code": "def run_job(self, exp: dict, reservations)\n\t\n\n    # get experiment details\n    script = exp['script']\n    args = exp['args']\n    port = exp['port']\n    result_dir = exp['result_dir']\n    name = exp['name']\n    log_dir = exp['log_dir']\n    gpu = exp['gpu']\n    args = self.update_args(args, exp['arg_mappings'])\n\n    # update experiment details\n    exp['port'] = port\n    exp['result_dir'] = result_dir\n    exp['log_dir'] = log_dir\n    exp['gpu'] = gpu\n    exp['args'] = args\n\n    # start experiment in new thread\n    thread = threading.Thread(target=self.run_experiment,\n                              args=(script, args, port, result_dir, name, log_dir, gpu))\n    thread.start()\n\n    # update running experiments\n    self.running_experiments.append(exp)\n\n", "CodeBleu_score": 21.25161542465986}, {"code": "def run_job(self, exp: dict, reservations)\n\t\n\n    if not reservations:\n        reservations = {}\n\n    # Set master port\n    if \"master_port\" not in exp:\n        exp[\"master_port\"] = 29500\n    else:\n        exp[\"master_port\"] += 1\n\n    # Set result directory\n    if \"result_dir\" not in exp:\n        exp[\"result_dir\"] = self.result_dir\n\n    # Update user arguments\n    if \"user_args\" in exp:\n        for arg_name, arg_value in exp[\"user_args\"].items():\n            if arg_name in self.user_args:\n                self.user_args[arg_name] = arg_value\n\n    # Create thread\n    thread = threading.Thread(target=self.run_experiment, args=(exp, reservations))\n    thread.start()\n\n    # Add experiment to running experiments\n    self.running_experiments.append({\"exp\": exp, \"thread\": thread})\n\n", "CodeBleu_score": 27.688498164359217}]}
{"instance_id": "4884e2e9665c", "generation_result": [{"code": "def save_exp_results_to_database(self, message, ranks=None, path=None):\n\t\n    if dist.is_initialized():\n        if dist.get_rank() == 0:\n            if ranks is None:\n                ranks = [0]\n            for rank in ranks:\n                if rank == 0:\n                    message[\"rank\"] = rank\n                    message[\"message\"] = message[\"message\"]\n                    if path is None:\n                        path = self.log_path\n                    with open(path, \"a\") as f:\n                        json.dump(message, f)\n                        f.write(\"", "CodeBleu_score": 28.598084512751843}, {"code": "def save_exp_results_to_database(self, message, ranks=None, path=None):\n\t\n    if ranks is None:\n        ranks = []\n    if path is None:\n        path = self.args.save_dir\n    if dist.is_initialized():\n        if dist.get_rank() in ranks:\n            if self.args.distributed_logging:\n                with open(os.path.join(path, 'logs.json'), 'a+') as f:\n                    f.write(json.dumps(message) + '", "CodeBleu_score": 23.27503452412615}, {"code": "def save_exp_results_to_database(self, message, ranks=None, path=None):\n\t\n    if self.distributed_logging and ranks is not None and dist.get_rank() in ranks:\n        if self.distributed_logging_rank == 0:\n            self.logger.log_message(message)\n        if self.distributed_logging_rank == 1:\n            self.logger.log_message(message)\n    elif not self.distributed_logging:\n        self.logger.log_message(message)\n\n", "CodeBleu_score": 20.47386498323804}]}
{"instance_id": "0429344c3c4d", "generation_result": [{"code": "def _build_tp_other_layer_map(self, layer_index: int)\n\t\n    tp_other_layer_map = {}\n    layer_index = str(layer_index)\n    layer_files = [f for f in os.listdir(self.model_dir) if re.match(r'layer_' + layer_index + '_tp_degree_[0-9]+_[0-9]+', f)]\n    if not layer_files:\n        raise ValueError(\"No layer files found for layer index \" + layer_index)\n    for f in layer_files:\n        tp_degree = int(re.findall(r'layer_' + layer_index + '_tp_degree_([0-9]+)_[0-9]+', f)[0])\n        if tp_degree not in tp_other_layer_map:\n            tp_other_layer_map[tp_degree] = []\n        tp_other_layer_map[tp_degree].append(f)\n    return tp_other_layer_map\n\n", "CodeBleu_score": 31.384873178907945}, {"code": "def _build_tp_other_layer_map(self, layer_index: int)\n\t\n    if not os.path.exists(self.layer_path):\n        raise Exception(f\"{self.layer_path} does not exist\")\n\n    # Retrieve layer files\n    layer_files = os.listdir(self.layer_path)\n    layer_files = [f for f in layer_files if re.search(r\"layer_[0-9]+\", f)]\n    if len(layer_files) == 0:\n        raise Exception(f\"No layer files found in {self.layer_path}\")\n\n    # Verify layer index\n    layer_index_list = [int(re.search(r\"layer_([0-9]+)\", f).group(1)) for f in layer_files]\n    if layer_index not in layer_index_list:\n        raise Exception(f\"Layer index {layer_index} not found in {self.layer_path}\")\n\n    # Partition layer files based on tp_degree\n    layer_file_map = {}\n    for f in layer_files:\n        tp_degree = int(re.search(r\"tp_degree_([0-9]+)\", f).group(1))\n        if tp_degree not in layer_file_map:\n            layer_file_map[tp_degree] = []\n        layer_file_map[tp_degree].append(f)\n\n    # Create layer file map\n    layer_file_map = {k: sorted(v) for k, v in layer_file_map.items()}\n    return layer_file_map\n\n", "CodeBleu_score": 35.51935015759426}, {"code": "def _build_tp_other_layer_map(self, layer_index: int)\n\t\n    layer_files = [f for f in os.listdir(self.layer_path) if f.startswith(f\"layer_{layer_index}_\")]\n    if not layer_files:\n        raise ValueError(f\"No layer files found for layer {layer_index}.\")\n    if layer_index < 0 or layer_index > len(layer_files):\n        raise ValueError(f\"Layer index {layer_index} out of bounds.\")\n    layer_files = [f for f in layer_files if f.startswith(f\"layer_{layer_index}_tp_\")]\n    layer_files = [f for f in layer_files if f.endswith(f\"_degree_{self.tp_degree}.pt\")]\n    tp_files = [f for f in layer_files if f.startswith(f\"layer_{layer_index}_tp_degree_{self.tp_degree}_\")]\n    other_files = [f for f in layer_files if f not in tp_files]\n    tp_files_dict = {}\n    other_files_dict = {}\n    for f in tp_files:\n        tp_degree = re.findall(r\"_tp_degree_(\\d+)\", f)[0]\n        tp_files_dict[tp_degree] = f\n    for f in other_files:\n        tp_degree = re.findall(r\"_tp_degree_(\\d+)\", f)[0]\n        other_files_dict[tp_degree] = f\n    return tp_files_dict, other_files_dict\n\n", "CodeBleu_score": 27.466121906315223}]}
{"instance_id": "1b65ecb9be93", "generation_result": [{"code": "def system_info()\n\t\n    sysinfo = {\n        \"os\": platform.system(),\n        \"arch\": platform.machine(),\n        \"version\": platform.version(),\n        \"processor\": platform.processor(),\n        \"python_version\": platform.python_version(),\n    }\n    return sysinfo\n\n", "CodeBleu_score": 58.5706202109451}, {"code": "def system_info()\n\t\n    system_info = {}\n    system_info[\"OS\"] = platform.system()\n    system_info[\"OS_version\"] = platform.release()\n    system_info[\"architecture\"] = platform.machine()\n    system_info[\"processor\"] = platform.processor()\n    system_info[\"Python_version\"] = platform.python_version()\n    return system_info\n\n", "CodeBleu_score": 40.29335189136175}, {"code": "def system_info()\n\t\n    system_info = {\n        \"os\": platform.system(),\n        \"architecture\": platform.machine(),\n        \"version\": platform.version(),\n        \"processor\": platform.processor(),\n        \"python_version\": sys.version,\n    }\n    return system_info\n\n", "CodeBleu_score": 56.942614119095744}]}
{"instance_id": "969bcc5cf3b5", "generation_result": [{"code": "def cuda_info()\n\t\n    if torch.cuda.is_available():\n        device_names = [torch.cuda.get_device_name(i) for i in range(torch.cuda.device_count())]\n        return device_names, True, torch.version.cuda\n    else:\n        return [], False, None\n\n", "CodeBleu_score": 57.055489385936234}, {"code": "def cuda_info()\n\t\n    try:\n        from torch.utils.cpp_extension import CUDA_HOME\n        if CUDA_HOME is not None:\n            cuda_home = CUDA_HOME\n        else:\n            cuda_home = ''\n    except:\n        cuda_home = ''\n    try:\n        from torch.utils.cpp_extension import CUDA_HOME\n        if CUDA_HOME is not None:\n            cuda_home = CUDA_HOME\n        else:\n            cuda_home = ''\n    except:\n        cuda_home = ''\n\n    try:\n        from torch.version import cuda\n        if cuda is not None:\n            cuda_version = torch.version.cuda\n        else:\n            cuda_version = ''\n    except:\n        cuda_version = ''\n\n    try:\n        import torch.cuda\n        if torch.cuda.is_available():\n            cuda_available = True\n        else:\n            cuda_available = False\n    except:\n        cuda_available = False\n\n    try:\n        import torch.cuda\n        if torch.cuda.is_available():\n            gpu_names = torch.cuda.get_device_name(None)\n        else:\n            gpu_names = ''\n    except:\n        gpu_names = ''\n\n    return cuda_home, cuda_available, cuda_version, gpu_names\n\n", "CodeBleu_score": 32.76422865184857}, {"code": "def cuda_info()\n\t\n    if torch.cuda.is_available():\n        cuda_version = torch.version.cuda\n        cuda_device_count = torch.cuda.device_count()\n        cuda_device_name = torch.cuda.get_device_name(0)\n        cuda_info = {'cuda_version': cuda_version, 'cuda_device_count': cuda_device_count, 'cuda_device_name': cuda_device_name}\n        return cuda_info\n    else:\n        print('CUDA is not available.')\n\n", "CodeBleu_score": 43.34332697074154}]}
{"instance_id": "a2ba908a4c70", "generation_result": [{"code": "def compute_encoder_accuracy(dataset_items, encoder_manager)\n\t\n    # Retrieve the class name key and optional class ID to class name mapping from the encoder manager's configuration\n    class_name_key = encoder_manager.config.class_name_key\n    class_id_to_name = encoder_manager.config.class_id_to_name\n\n    # Initialize the class name to accuracy dictionary\n    class_name_to_accuracy = {}\n\n    # Iterate over the dataset items\n    for item in tqdm(dataset_items, desc='Computing encoder accuracy'):\n        # Extract the audio file\n        audio_file = item.audio_file\n\n        # Compute the embedding\n        embedding = encoder_manager.compute_embedding(audio_file)\n\n        # Predict the class label if criteria and mappings are available\n        if class_name_key in item.keys and class_id_to_name is not None:\n            # Retrieve the true class name\n            true_class_name = item[class_name_key]\n\n            # Predict the class label\n            predicted_class_name = encoder_manager.predict_class_name(embedding)\n\n            # Update the class name to accuracy dictionary\n            if true_class_name in class_name_to_accuracy:\n                class_name_to_accuracy[true_class_name] = class_name_to_accuracy[true_class_name] + 1 if true_class_name == predicted_class_name else class_name_to_accuracy[true_class_name]\n            else:\n                class_name_to_accuracy[true_class_name] = 1 if true_class_name == predicted_class_name else 0\n\n    # Calculate and print the accuracy for each class\n    for class_name in class_name_to_accuracy:\n        # Calculate the accuracy\n        accuracy = class_name_to_accuracy[class_name] / len(dataset_items)\n\n        # Print the accuracy\n        print(f'Accuracy for class \"{class_name}\": {accuracy}')\n\n    # Calculate and print the average accuracy across all classes\n   ", "CodeBleu_score": 28.24458814529131}, {"code": "def compute_encoder_accuracy(dataset_items, encoder_manager)\n\t\n    # get class name key and optional class ID to class name mapping\n    class_name_key = encoder_manager.config[\"class_name_key\"]\n    class_id_to_class_name = encoder_manager.config.get(\"class_id_to_class_name\", None)\n\n    # initialize counters\n    num_samples = 0\n    num_correct_predictions = 0\n    num_correct_predictions_per_class = {}\n    num_samples_per_class = {}\n\n    # iterate over dataset items\n    for item in tqdm(dataset_items):\n        # get class name\n        class_name = item[class_name_key]\n\n        # get class ID if class ID to class name mapping is available\n        if class_id_to_class_name is not None:\n            class_id = item[\"class_id\"]\n            class_name = class_id_to_class_name[class_id]\n\n        # get embedding\n        embedding = encoder_manager.get_embedding(item[\"audio_path\"])\n\n        # predict class label\n        predicted_class_name = encoder_manager.predict_class(embedding)\n\n        # update counters\n        num_samples += 1\n        num_samples_per_class[class_name] = num_samples_per_class.get(class_name, 0) + 1\n        if class_name == predicted_class_name:\n            num_correct_predictions += 1\n            num_correct_predictions_per_class[class_name] = num_correct_predictions_per_class.get(class_name, 0) + 1\n\n    # compute accuracy for each class\n    accuracy_per_class = {}\n    for class_name, num_correct_predictions in num_correct_predictions_per_class.items():\n        accuracy_per_class[class_name] = num_correct_predictions / num_samples_per_class[class_name]\n\n    # compute overall accuracy\n    accuracy = num_correct_predictions / num_samples\n\n    # print results\n    print(\"Accuracy per class:\")\n    for class", "CodeBleu_score": 32.92683087467598}, {"code": "def compute_encoder_accuracy(dataset_items, encoder_manager)\n\t\n    # get the class name key and optional class ID to class name mapping from the encoder manager's configuration\n    class_name_key = encoder_manager.config[\"class_name_key\"]\n    class_id_to_name_map = encoder_manager.config.get(\"class_id_to_name_map\", None)\n    # initialize the accuracy dictionary\n    class_accuracy_dict = {}\n    # iterate over all dataset items\n    for dataset_item in tqdm(dataset_items, desc=\"Computing encoder accuracy\"):\n        # get the class name from the dataset item\n        class_name = dataset_item[class_name_key]\n        # get the class ID from the dataset item\n        class_id = dataset_item[\"class_id\"]\n        # get the audio file path from the dataset item\n        audio_file_path = dataset_item[\"audio_file_path\"]\n        # get the embedding from the encoder manager\n        embedding = encoder_manager.get_embedding(audio_file_path)\n        # predict the class label\n        predicted_class_name = encoder_manager.predict_class(embedding)\n        # get the predicted class ID\n        predicted_class_id = encoder_manager.get_class_id(predicted_class_name)\n        # check if the predicted class ID and the actual class ID are equal\n        if class_id == predicted_class_id:\n            # check if the class name is in the accuracy dictionary\n            if class_name not in class_accuracy_dict:\n                # initialize the class accuracy dictionary entry\n                class_accuracy_dict[class_name] = {\"correct\": 0, \"total\": 0}\n            # increment the correct count\n            class_accuracy_dict[class_name][\"correct\"] += 1\n        # increment the total count\n        class_accuracy_dict[class_name][\"total\"] += 1\n    # iterate over all class names\n    for class_name in class_accuracy_dict:\n        # get the accuracy for the class\n        accuracy = class_accuracy_dict[class_name][\"correct\"] / class_accuracy_dict[class_", "CodeBleu_score": 24.859329687514094}]}
{"instance_id": "e01ec69ad58a", "generation_result": [{"code": "def adjust_path_and_remove_silence(audio_path)\n\t\n    audio_path = pathlib.Path(audio_path)\n    output_path = audio_path.parent.replace(args.input_dir, args.output_dir)\n    output_path.mkdir(parents=True, exist_ok=True)\n    output_path = output_path / audio_path.name\n    if output_path.exists() and not args.force:\n        return output_path, False\n    speech_detected = False\n    if args.vad:\n        speech_detected = remove_silence(audio_path, output_path, args.vad_model, args.vad_utils, args.vad_threshold)\n    else:\n        torch.hub.load_state_dict_from_url(args.model_url, map_location=torch.device('cpu'))\n        torch.hub.save_state_dict_to_file(output_path, map_location=torch.device('cpu'))\n    return output_path, speech_detected\n\n", "CodeBleu_score": 41.86078908619984}, {"code": "def adjust_path_and_remove_silence(audio_path)\n\t\n    # Adjust output path\n    audio_path = audio_path.replace(args.input_dir, args.output_dir)\n    # Check if file already exists\n    if os.path.exists(audio_path) and not args.force:\n        return audio_path, False\n    # Create directory structure\n    pathlib.Path(os.path.dirname(audio_path)).mkdir(parents=True, exist_ok=True)\n    # Remove silence\n    speech_detected = remove_silence(audio_path, args.vad_model, args.vad_utils, args.vad_threshold, args.vad_frame_length, args.vad_frame_shift, args.vad_window_length, args.vad_padding, args.vad_skip_sec, args.vad_noise_floor_db)\n    return audio_path, speech_detected\n\n", "CodeBleu_score": 42.64237911932304}, {"code": "def adjust_path_and_remove_silence(audio_path)\n\t\n    audio_path = pathlib.Path(audio_path)\n    output_path = output_dir / audio_path.relative_to(input_dir)\n    output_path.parent.mkdir(parents=True, exist_ok=True)\n    if output_path.exists() and not force:\n        return str(output_path), False\n    speech_detected = remove_silence(str(audio_path), str(output_path), vad_model, vad_utils, min_speech_duration,\n                                      silence_threshold, aggressiveness, min_silence_duration, padding_end,\n                                      padding_start)\n    return str(output_path), speech_detected\n\n", "CodeBleu_score": 29.182744122555782}]}
{"instance_id": "7e0bdb0feff8", "generation_result": [{"code": "def resample_files(input_dir, output_sr, output_dir=None, file_ext=\"wav\", n_jobs=10)\n\t\n    if output_dir is None:\n        output_dir = input_dir\n    else:\n        copytree(input_dir, output_dir)\n    audio_files = glob.glob(os.path.join(output_dir, \"**/*.\" + file_ext), recursive=True)\n    with Pool(n_jobs) as p:\n        list(\n            tqdm(\n                p.imap(\n                    resample_file,\n                    zip(audio_files, [output_sr] * len(audio_files)),\n                ),\n                total=len(audio_files),\n                desc=\"Resampling files\",\n            )\n        )\n    print(\"Resampling complete.\")\n\n", "CodeBleu_score": 35.16406750418011}, {"code": "def resample_files(input_dir, output_sr, output_dir=None, file_ext=\"wav\", n_jobs=10)\n\t\n    if output_dir is None:\n        output_dir = input_dir\n    else:\n        copytree(input_dir, output_dir)\n\n    pool = Pool(processes=n_jobs)\n    audio_files = glob.glob(os.path.join(output_dir, \"**\", f\"*.{file_ext}\"), recursive=True)\n    for audio_file in tqdm(pool.imap_unordered(resample_file, audio_files), total=len(audio_files)):\n        pass\n\n    pool.close()\n    pool.join()\n    print(\"Finished resampling.\")\n\n", "CodeBleu_score": 38.275858898253944}, {"code": "def resample_files(input_dir, output_sr, output_dir=None, file_ext=\"wav\", n_jobs=10)\n\t\n    if output_dir is None:\n        output_dir = input_dir\n    else:\n        copytree(input_dir, output_dir)\n    files = glob.glob(os.path.join(output_dir, \"**\", \"*.\" + file_ext), recursive=True)\n    if n_jobs == 1:\n        for file in tqdm(files):\n            resample_file(file, output_sr)\n    else:\n        with Pool(n_jobs) as p:\n            list(\n                tqdm(\n                    p.imap(\n                        lambda file: resample_file(file, output_sr),\n                        files,\n                    ),\n                    total=len(files),\n                )\n            )\n    print(\"Resampling completed!\")\n\n", "CodeBleu_score": 36.13896537852948}]}
{"instance_id": "a1351834ca57", "generation_result": [{"code": "def evaluation(model, criterion, data_loader, global_step)\n\t\n    model.eval()\n    with torch.no_grad():\n        eval_loss = 0\n        for batch_idx, batch in enumerate(data_loader):\n            batch = tuple(b.to(device) for b in batch)\n            inputs, labels = batch\n            inputs = inputs.view(-1, inputs.size(-1))\n            labels = labels.view(-1)\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            eval_loss += loss.item()\n        eval_loss /= len(data_loader)\n        print(f\"Evaluation loss at step {global_step}: {eval_loss}\")\n        return eval_loss\n\n", "CodeBleu_score": 26.06434388736794}, {"code": "def evaluation(model, criterion, data_loader, global_step)\n\t\n    model.eval()\n    device = next(model.parameters()).device\n    eval_loss = 0\n    with torch.no_grad():\n        for batch_idx, batch in enumerate(data_loader):\n            input, input_lengths, labels, label_lengths = batch\n            input = input.to(device)\n            input_lengths = input_lengths.to(device)\n            labels = labels.to(device)\n            label_lengths = label_lengths.to(device)\n            output, output_lengths = model(input, input_lengths)\n            loss = criterion(output, labels, output_lengths, label_lengths)\n            eval_loss += loss.item()\n            if batch_idx == 0:\n                plot_embeddings(output, labels, label_lengths, global_step, 'eval')\n    eval_loss /= len(data_loader)\n    print('Evaluation loss: {:.4f}'.format(eval_loss))\n    return eval_loss\n\n", "CodeBleu_score": 26.02655993882704}, {"code": "def evaluation(model, criterion, data_loader, global_step)\n\t\n    model.eval()\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    eval_loss = 0\n    for batch in data_loader:\n        batch = [x.to(device) for x in batch]\n        with torch.no_grad():\n            input_data, input_lengths, labels = batch\n            output = model(input_data, input_lengths)\n            loss = criterion(output, labels)\n            eval_loss += loss.item()\n    eval_loss /= len(data_loader)\n    print(\"[%d] Evaluation loss: %.4f\" % (global_step, eval_loss))\n    return eval_loss\n\n", "CodeBleu_score": 19.582487655637777}]}
{"instance_id": "565f8ed09916", "generation_result": [{"code": "def register_config(model_name: str) -> Coqpit\n\t\n    # construct the configuration class name\n    config_class_name = model_name + \"_config\"\n    # check if the model is \"xtts\" to import its specific configuration\n    if model_name == \"xtts\":\n        config_class_name = \"xtts_config\"\n    # search through predefined paths to find and assign the configuration class\n    for path in CONFIG_CLASS_PATHS:\n        try:\n            config_class = find_module(config_class_name, path)\n            break\n        except ModuleNotFoundError:\n            pass\n    else:\n        raise ModuleNotFoundError(\n            f\"Could not find {config_class_name} in any of the following paths: {CONFIG_CLASS_PATHS}\"\n        )\n    # return the found configuration class\n    return config_class\n\n", "CodeBleu_score": 36.270245700865644}, {"code": "def register_config(model_name: str) -> Coqpit\n\t\n    config_class_name = model_name.split(\"-\")[0].title() + \"Config\"\n    if model_name == \"xtts\":\n        config_class = find_module(config_class_name, [\"TTS.config\"])\n    else:\n        config_class = find_module(config_class_name, [\"TTS.config\", \"TTS.config.custom_configs\"])\n    return config_class\n\n", "CodeBleu_score": 24.162171916177737}, {"code": "def register_config(model_name: str) -> Coqpit\n\t\n    config_class_name = model_name.upper() + \"_CONFIG\"\n    try:\n        config_class = find_module(config_class_name)\n    except ModuleNotFoundError:\n        try:\n            config_class = find_module(config_class_name, \"TTS.config\")\n        except ModuleNotFoundError:\n            try:\n                config_class = find_module(config_class_name, \"TTS.config.xtts_config\")\n            except ModuleNotFoundError:\n                raise ModuleNotFoundError(f\"Could not find configuration class {config_class_name}\")\n\n    return config_class()\n\n", "CodeBleu_score": 31.568460998236812}]}
{"instance_id": "0ea7862506a7", "generation_result": [{"code": "def load_config(config_path: str) -> Coqpit\n\t\n\n    # Load the configuration file\n    _, file_extension = os.path.splitext(config_path)\n    if file_extension == \".yaml\" or file_extension == \".yml\":\n        with fsspec.open(config_path) as f:\n            config_dict = yaml.load(f, Loader=yaml.FullLoader)\n    elif file_extension == \".json\":\n        with fsspec.open(config_path) as f:\n            config_dict = json.load(f)\n    else:\n        raise ValueError(f\"Unknown configuration file extension: {file_extension}\")\n\n    # Process the configuration dictionary\n    model_name = config_dict[\"model_name\"]\n    config_class = find_module(model_name + \"Config\", config_path)\n    config_dict[\"config_path\"] = config_path\n\n    # Create the configuration object\n    config = config_class(**config_dict)\n\n    # Return the configuration object\n    return config\n\n", "CodeBleu_score": 40.94812530161849}, {"code": "def load_config(config_path: str) -> Coqpit\n\t\n\n    if config_path.startswith(\"gs://\"):\n        fs, path = fsspec.core.url_to_fs(config_path)\n        with fs.open(path, \"r\") as f:\n            if path.endswith(\".json\"):\n                config_dict = json.load(f)\n            elif path.endswith(\".yaml\"):\n                config_dict = yaml.load(f, Loader=yaml.FullLoader)\n            else:\n                raise ValueError(\n                    f\"Unknown file extension for config file: {path}. Only json and yaml are supported.\"\n                )\n    else:\n        if config_path.endswith(\".json\"):\n            with open(config_path, \"r\") as f:\n                config_dict = json.load(f)\n        elif config_path.endswith(\".yaml\"):\n            with open(config_path, \"r\") as f:\n                config_dict = yaml.load(f, Loader=yaml.FullLoader)\n        else:\n            raise ValueError(\n                f\"Unknown file extension for config file: {config_path}. Only json and yaml are supported.\"\n            )\n\n    # get the model name\n    model_name = config_dict.get(\"model_name\", None)\n    if model_name is None:\n        raise ValueError(\n            \"Model name is not specified in the config file. Please specify 'model_name' in the config file.\"\n        )\n\n    # get the config class\n    config_class = find_module(f\"TTS.config.{model_name}_config\")\n\n    # create config object\n    config = config_class(**config_dict)\n\n    return config\n\n", "CodeBleu_score": 37.908019683724056}, {"code": "def load_config(config_path: str) -> Coqpit\n\t\n    if config_path is None:\n        raise ValueError(\"No config file path provided.\")\n\n    # load config file\n    with fsspec.open(config_path, \"r\") as f:\n        if config_path.endswith(\".json\"):\n            config_dict = json.load(f)\n        elif config_path.endswith(\".yaml\"):\n            config_dict = yaml.safe_load(f)\n        else:\n            raise ValueError(f\"Unknown config file type: {config_path}\")\n\n    # get model name\n    model_name = config_dict[\"model_name\"]\n    if model_name not in MODEL_CLASSES:\n        raise ValueError(f\"Unknown model: {model_name}\")\n\n    # register config class\n    config_class = MODEL_CLASSES[model_name]\n    Coqpit.register_config_class(config_class)\n\n    # create config object\n    config = config_class()\n    config.load_dict(config_dict)\n\n    return config\n\n", "CodeBleu_score": 40.38476884211443}]}
{"instance_id": "e0db68b63143", "generation_result": [{"code": "def list_files(basePath, validExts=None, contains=None)\n\t\n    for root, _, files in os.walk(basePath):\n        for file in files:\n            if contains and contains not in file:\n                continue\n            if validExts and os.path.splitext(file)[1] not in validExts:\n                continue\n            yield os.path.join(root, file)\n\n", "CodeBleu_score": 28.35672846366537}, {"code": "def list_files(basePath, validExts=None, contains=None)\n\t\n    # For all files, walk the tree\n    for root, _, files in os.walk(basePath):\n        for file in files:\n            if contains is not None and contains not in file:\n                continue\n            if validExts is None or os.path.splitext(file)[1].lower() in validExts:\n                yield os.path.join(root, file)\n\n", "CodeBleu_score": 37.588329877743355}, {"code": "def list_files(basePath, validExts=None, contains=None)\n\t\n    if validExts is None:\n        validExts = set()\n    if contains is None:\n        contains = set()\n    else:\n        contains = set(contains)\n\n    for root, _, files in os.walk(basePath):\n        for file in files:\n            if len(contains) > 0 and not any(x in file for x in contains):\n                continue\n            if len(validExts) > 0 and os.path.splitext(file)[1][1:] not in validExts:\n                continue\n            yield os.path.join(root, file)\n\n", "CodeBleu_score": 34.47371614657458}]}
{"instance_id": "b836fcaf0a27", "generation_result": [{"code": "def load_model(xtts_checkpoint, xtts_config, xtts_vocab)\n\t\n    try:\n        torch.cuda.empty_cache()\n        if xtts_checkpoint is None or xtts_config is None or xtts_vocab is None:\n            raise Exception(\"Checkpoint, config, and vocab paths are required.\")\n        print(\"Loading XTTS model...\")\n        config = XttsConfig.load(xtts_config)\n        model = Xtts(config)\n        model.load_state_dict(torch.load(xtts_checkpoint))\n        model.load_vocab(xtts_vocab)\n        if torch.cuda.is_available():\n            model = model.cuda()\n        print(\"XTTS model loaded.\")\n        return model\n    except Exception as e:\n        traceback.print_exc()\n        print(e)\n        sys.exit(1)\n\n", "CodeBleu_score": 49.40444198719185}, {"code": "def load_model(xtts_checkpoint, xtts_config, xtts_vocab)\n\t\n    try:\n        torch.cuda.empty_cache()\n        if xtts_checkpoint is None or xtts_config is None or xtts_vocab is None:\n            raise ValueError(\n                \"The xtts_checkpoint, xtts_config, and xtts_vocab paths are required to load the XTTS model.\"\n            )\n        print(\"Loading the XTTS model configuration...\")\n        config = XttsConfig.load(xtts_config)\n        print(\"Initializing the XTTS model...\")\n        model = Xtts(config)\n        print(\"Loading the XTTS model checkpoint...\")\n        model.load_state_dict(torch.load(xtts_checkpoint))\n        print(\"Loading the XTTS model vocabulary...\")\n        model.load_vocab(xtts_vocab)\n        if torch.cuda.is_available():\n            model.cuda()\n        return model\n    except Exception as e:\n        traceback.print_exc()\n        return None\n\n", "CodeBleu_score": 45.678165274508196}, {"code": "def load_model(xtts_checkpoint, xtts_config, xtts_vocab)\n\t\n    try:\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n        else:\n            print(\"GPU not available, using CPU instead.\")\n\n        if xtts_config is None:\n            print(\"Please provide the path to the XTTS config.\")\n            return\n        elif xtts_checkpoint is None:\n            print(\"Please provide the path to the XTTS checkpoint.\")\n            return\n        elif xtts_vocab is None:\n            print(\"Please provide the path to the XTTS vocab.\")\n            return\n\n        config = XttsConfig.load(xtts_config)\n        model = Xtts(config)\n        model.load_state_dict(torch.load(xtts_checkpoint))\n        model.load_vocab(xtts_vocab)\n        if torch.cuda.is_available():\n            model.cuda()\n        print(\"XTTS model loaded successfully.\")\n        return model\n    except:\n        traceback.print_exc()\n        print(\"Error loading XTTS model.\")\n        return\n\n", "CodeBleu_score": 43.977988983073665}]}
{"instance_id": "c5d63aa0df26", "generation_result": [{"code": "def compute_embedding(self, x, num_frames=250, num_eval=10, return_mean=True, l2_norm=True)\n\t\n\n    # adjust number of frames based on hop length\n    if self.hop_length is not None:\n        num_frames = int(num_frames * self.hop_length)\n\n    # ensure the number of frames does not exceed the input length\n    num_frames = min(num_frames, x.shape[-1])\n\n    # calculate offset positions for evaluation\n    offsets = torch.arange(0, x.shape[-1] - num_frames + 1, step=int(num_frames / num_eval))\n\n    # extract frames at these offsets, concatenate them into a batch, and perform inference to obtain embeddings\n    x_batch = x[..., offsets]\n    embeddings = self.infer(x_batch)\n\n    # if specified, compute the mean of the embeddings and return the result, optionally normalizing them with L2 norm\n    if return_mean:\n        embeddings = embeddings.mean(dim=-1)\n        if l2_norm:\n            embeddings = torch.nn.functional.normalize(embeddings, dim=-1)\n\n    return embeddings\n\n", "CodeBleu_score": 35.57199710824784}, {"code": "def compute_embedding(self, x, num_frames=250, num_eval=10, return_mean=True, l2_norm=True)\n\t\n    if isinstance(x, torch.Tensor):\n        x = x.unsqueeze(0)\n    x_len = x.shape[2]\n    if num_frames > x_len:\n        num_frames = x_len\n    offset = (x_len - num_frames) // (num_eval - 1)\n    x_emb = []\n    for i in range(num_eval):\n        x_emb.append(self(x[:, :, i * offset : i * offset + num_frames]))\n    x_emb = torch.cat(x_emb, dim=0)\n    if return_mean:\n        x_emb = x_emb.mean(dim=0, keepdim=True)\n    if l2_norm:\n        x_emb = nn.functional.normalize(x_emb, dim=-1)\n    return x_emb\n\n", "CodeBleu_score": 36.61168041262944}, {"code": "def compute_embedding(self, x, num_frames=250, num_eval=10, return_mean=True, l2_norm=True)\n\t\n\n    # get the number of frames for the input x\n    num_frames = min(num_frames, x.shape[-1] // self.hop_length)\n\n    # get the offset positions for evaluation\n    offsets = torch.linspace(\n        0,\n        x.shape[-1] - self.hop_length * num_frames,\n        num_eval,\n        device=x.device,\n        dtype=torch.long,\n    )\n\n    # extract frames at the offset positions\n    x_frames = [x[:, offset : offset + self.hop_length * num_frames] for offset in offsets]\n\n    # concatenate the frames into a batch and perform inference\n    x_frames = torch.cat(x_frames, dim=0)\n    with torch.no_grad():\n        embeddings = self.inference(x_frames)\n\n    # compute the mean of the embeddings and return the result\n    if return_mean:\n        embeddings = torch.mean(embeddings, dim=0, keepdim=True)\n    if l2_norm:\n        embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)\n    return embeddings\n\n", "CodeBleu_score": 43.61938487898955}]}
{"instance_id": "06f5b0335b26", "generation_result": [{"code": "def create_layer(self, block, planes, blocks, stride=1)\n\t\n    downsample = None\n    if stride != 1 or self.inplanes != planes * block.expansion:\n        downsample = nn.Sequential(\n            nn.Conv1d(self.inplanes, planes * block.expansion,\n                      kernel_size=1, stride=stride, bias=False),\n            nn.BatchNorm1d(planes * block.expansion),\n        )\n\n    layers = []\n    layers.append(block(self.inplanes, planes, stride, downsample))\n    self.inplanes = planes * block.expansion\n    for i in range(1, blocks):\n        layers.append(block(self.inplanes, planes))\n\n    return nn.Sequential(*layers)\n\n", "CodeBleu_score": 82.8588786843097}, {"code": "def create_layer(self, block, planes, blocks, stride=1)\n\t\n    downsample = None\n    if stride != 1 or self.inplanes != planes * block.expansion:\n        downsample = nn.Sequential(\n            nn.Conv2d(self.inplanes, planes * block.expansion,\n                      kernel_size=1, stride=stride, bias=False),\n            nn.BatchNorm2d(planes * block.expansion),\n        )\n\n    layers = []\n    layers.append(block(self.inplanes, planes, stride, downsample))\n    self.inplanes = planes * block.expansion\n    for i in range(1, blocks):\n        layers.append(block(self.inplanes, planes))\n\n    return nn.Sequential(*layers)\n\n", "CodeBleu_score": 85.75459657510368}, {"code": "def create_layer(self, block, planes, blocks, stride=1)\n\t\n    downsample = None\n    if stride != 1 or self.inplanes != planes * block.expansion:\n        downsample = nn.Sequential(\n            nn.Conv2d(self.inplanes, planes * block.expansion,\n                      kernel_size=1, stride=stride, bias=False),\n            nn.BatchNorm2d(planes * block.expansion),\n        )\n\n    layers = []\n    layers.append(block(self.inplanes, planes, stride, downsample))\n    self.inplanes = planes * block.expansion\n    for i in range(1, blocks):\n        layers.append(block(self.inplanes, planes))\n\n    return nn.Sequential(*layers)\n\n", "CodeBleu_score": 85.75459657510368}]}
{"instance_id": "9ac60597b633", "generation_result": [{"code": "def plot_embeddings(embeddings, num_classes_in_batch)\n\t\n\n    # Get the number of utterances per class\n    num_utterances_per_class = [0] * num_classes_in_batch\n    for i in range(len(embeddings)):\n        num_utterances_per_class[i % num_classes_in_batch] += 1\n\n    # Limit the number of classes to 10 if necessary\n    if num_classes_in_batch > 10:\n        num_utterances_per_class = num_utterances_per_class[:10]\n        num_classes_in_batch = 10\n\n    # Calculate the number of utterances per class\n    num_utterances_per_class = [\n        num_utterances_per_class[i] / sum(num_utterances_per_class)\n        for i in range(num_classes_in_batch)\n    ]\n\n    # Transform the embeddings\n    reducer = umap.UMAP(n_neighbors=5, min_dist=0.0, metric=\"cosine\")\n    embedding_2d = reducer.fit_transform(embeddings)\n\n    # Plot the embeddings\n    matplotlib.rcParams[\"figure.figsize\"] = [10, 10]\n    fig, ax = matplotlib.pyplot.subplots(figsize=(10, 10))\n    ax.set_title(\"UMAP projection\")\n    ax.scatter(\n        embedding_2d[:, 0],\n        embedding_2d[:, 1],\n        c=list(range(num_classes_in_batch)),\n        cmap=matplotlib.cm.get_cmap(\"tab10\", num_classes_in_batch),\n        s=25,\n    )\n    ax.set_aspect(\"equal\", \"box\")\n    fig.savefig(\"umap.png\")\n    return fig\n", "CodeBleu_score": 42.540102114146734}, {"code": "def plot_embeddings(embeddings, num_classes_in_batch)\n\t\n\n    # Calculate the number of utterances per class\n    num_utterances_per_class = [0] * num_classes_in_batch\n    for i in range(len(embeddings)):\n        num_utterances_per_class[i % num_classes_in_batch] += 1\n\n    # Limit the number of classes to 10 if necessary\n    if num_classes_in_batch > 10:\n        num_classes_in_batch = 10\n\n    # Transform the embeddings\n    embeddings_transformed = umap.UMAP(n_neighbors=10, min_dist=0.5,\n                                       metric='cosine').fit_transform(embeddings)\n\n    # Create a scatter plot with colors representing different classes\n    fig = matplotlib.pyplot.figure()\n    ax = fig.add_subplot(111)\n    for i in range(num_classes_in_batch):\n        ax.scatter(embeddings_transformed[i * num_utterances_per_class[i]:(i + 1) * num_utterances_per_class[i], 0],\n                   embeddings_transformed[i * num_utterances_per_class[i]:(i + 1) * num_utterances_per_class[i], 1],\n                   c=i * num_utterances_per_class[i] * [i])\n\n    # Adjust the aspect ratio of the plot\n    ax.set_aspect('equal')\n\n    # Title the plot\n    ax.set_title(\"UMAP projection\")\n\n    # Save the plot as \"umap\"\n    fig.savefig(\"umap\")\n\n    # Return the figure\n    return fig\n", "CodeBleu_score": 37.161361022946636}, {"code": "def plot_embeddings(embeddings, num_classes_in_batch)\n\t\n    matplotlib.use(\"Agg\")\n    import matplotlib.pyplot as plt\n    import seaborn as sns\n\n    # Calculate the number of utterances per class\n    num_utterances_per_class = [0] * num_classes_in_batch\n    for embedding in embeddings:\n        num_utterances_per_class[embedding[1]] += 1\n\n    # Limit the number of classes to 10 if necessary\n    if num_classes_in_batch > 10:\n        num_utterances_per_class = num_utterances_per_class[:10]\n        num_classes_in_batch = 10\n\n    # Transform the embeddings\n    reducer = umap.UMAP(n_neighbors=10, min_dist=0.0, metric=\"cosine\")\n    transformed_embeddings = reducer.fit_transform(embeddings)\n\n    # Create a scatter plot with colors representing different classes\n    fig = plt.figure(figsize=(10, 10))\n    ax = fig.add_subplot(1, 1, 1)\n    scatter = ax.scatter(\n        transformed_embeddings[:, 0],\n        transformed_embeddings[:, 1],\n        c=embeddings[:, 1],\n        s=25,\n        cmap=\"tab10\",\n        vmin=0,\n        vmax=num_classes_in_batch - 1,\n    )\n    ax.set_title(\"UMAP projection\")\n    ax.set_aspect(\"equal\", \"datalim\")\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.grid(False)\n    fig.colorbar(scatter, ax=ax, ticks=range(num_classes_in_batch))\n    fig.tight_layout()\n\n    # Save the plot as \"umap\"\n    fig.savefig(\"umap.png\")\n\n    return fig\n", "CodeBleu_score": 45.01591883079968}]}
{"instance_id": "19c2b4af256d", "generation_result": [{"code": "def calc_cosine_sim(self, dvecs, centroids)\n\t\n    cos_sim_matrix = []\n    for i in range(dvecs.shape[0]):\n        # Get the d-vector for the current utterance\n        dvec = dvecs[i, :]\n        # Get the centroids for the current speaker\n        centroids_speaker = centroids[i, :]\n        # Calculate the cosine similarity between the d-vector and the centroids for the current speaker\n        cos_sim = torch.nn.functional.cosine_similarity(dvec, centroids_speaker, dim=0)\n        # Clamp the cosine similarity values to avoid small values\n        cos_sim = torch.clamp(cos_sim, min=1e-10, max=1.0)\n        # Append the cosine similarity values to the cosine similarity matrix\n        cos_sim_matrix.append(cos_sim)\n    # Stack the cosine similarity matrix\n    cos_sim_matrix = torch.stack(cos_sim_matrix, dim=0)\n    return cos_sim_matrix\n\n", "CodeBleu_score": 28.630634703835966}, {"code": "def calc_cosine_sim(self, dvecs, centroids)\n\t\n\n    # create a cosine similarity matrix\n    cos_sim_matrix = torch.empty(\n        (dvecs.shape[0], centroids.shape[0]),\n        dtype=torch.float32,\n        device=self.device,\n    )\n\n    # iterate over each speaker\n    for spk_idx in range(dvecs.shape[0]):\n\n        # iterate over each utterance of the current speaker\n        for ut_idx in range(dvecs.shape[1]):\n\n            # compute new centroids excluding the current utterance\n            new_centroids = centroids.clone()\n            new_centroids[spk_idx, :] = (\n                torch.sum(centroids[spk_idx, :], dim=0) - dvecs[spk_idx, ut_idx, :]\n            ) / (centroids.shape[1] - 1)\n\n            # calculate cosine similarity between the current utterance and the new centroids\n            cos_sim = torch.cosine_similarity(\n                dvecs[spk_idx, ut_idx, :].unsqueeze(0),\n                new_centroids,\n                dim=1,\n            )\n\n            # clamp the cosine similarity values to avoid small values\n            cos_sim = torch.clamp(cos_sim, min=1e-8)\n\n            # concatenate the cosine similarity values\n            cos_sim_matrix[spk_idx, :] = torch.cat(\n                (cos_sim_matrix[spk_idx, :], cos_sim), dim=0\n            )\n\n    # stack the cosine similarity matrix\n    cos_sim_matrix = torch.stack(cos_sim_matrix, dim=0)\n\n    return cos_sim_matrix\n\n", "CodeBleu_score": 28.670286758821682}, {"code": "def calc_cosine_sim(self, dvecs, centroids)\n\t\n    cos_sim_matrix = []\n    for i in range(len(dvecs)):\n        # Iterate over each speaker\n        speaker_dvecs = dvecs[i]\n        speaker_centroids = centroids[i]\n        speaker_cos_sim_matrix = []\n        for j in range(len(speaker_dvecs)):\n            # Iterate over each utterance\n            utterance_dvec = speaker_dvecs[j]\n            utterance_centroids = speaker_centroids.clone()\n            utterance_centroids[j] = 0\n            utterance_centroids = utterance_centroids / torch.norm(utterance_centroids, dim=1, keepdim=True)\n            # Calculate cosine similarity between utterance and new centroids\n            utterance_cos_sim = torch.matmul(utterance_dvec, utterance_centroids.T)\n            utterance_cos_sim = torch.clamp(utterance_cos_sim, min=1e-5, max=1.0)\n            speaker_cos_sim_matrix.append(utterance_cos_sim)\n        cos_sim_matrix.append(speaker_cos_sim_matrix)\n    return cos_sim_matrix\n\n", "CodeBleu_score": 34.13976898089902}]}
{"instance_id": "c525b411a1e7", "generation_result": [{"code": " def embed_loss_softmax(self, dvecs, cos_sim_matrix)\n\t\n    losses = []\n    for i in range(dvecs.shape[0]):\n        loss = -nn.functional.log_softmax(cos_sim_matrix[i], dim=0)[i]\n        losses.append(loss)\n    return torch.stack(losses)\n\n", "CodeBleu_score": 32.7201924433387}, {"code": " def embed_loss_softmax(self, dvecs, cos_sim_matrix)\n\t\n    # The following is the loss function for the cosine similarity matrix\n    # The cosine similarity matrix is a matrix of size (num_embeddings, num_embeddings)\n    # The loss is calculated by iterating over the rows of the matrix, and computing the negative log softmax of each row\n    # The losses for each row are stacked into tensors, and the function returns the final stacked tensor of losses\n    # This loss function is used in the paper \"In Defense of the Triplet Loss for Person Re-Identification\"\n    # https://arxiv.org/pdf/1703.07737.pdf\n    # The loss function is implemented in the following code\n    loss_matrix = []\n    for i in range(len(dvecs)):\n        # The following code calculates the negative log softmax of the cosine similarity matrix for each embedding\n        # The cosine similarity matrix is a matrix of size (num_embeddings, num_embeddings)\n        # The loss is calculated by iterating over the rows of the matrix, and computing the negative log softmax of each row\n        # The losses for each row are stacked into tensors, and the function returns the final stacked tensor of losses\n        # This loss function is used in the paper \"In Defense of the Triplet Loss for Person Re-Identification\"\n        # https://arxiv.org/pdf/1703.07737.pdf\n        # The loss function is implemented in the following code\n        cos_sim_matrix_i = cos_sim_matrix[i]\n        # The following code calculates the negative log softmax of the cosine similarity matrix for each embedding\n        # The cosine similarity matrix is a matrix of size (num_embeddings, num_embeddings)\n        # The loss is calculated by iterating over the rows of the matrix, and computing the negative log softmax of each row\n        # The losses for each row are stacked into tensors, and the function returns the final stacked tensor of losses\n        # This loss function is used in the paper \"In Defense of the Triplet Loss for Person Re-Identification\"\n        # https://arxiv.org/pdf/1703.07", "CodeBleu_score": 13.2688312423028}, {"code": " def embed_loss_softmax(self, dvecs, cos_sim_matrix)\n\t\n\n    # Initialize the loss tensor\n    loss = torch.zeros(self.batch_size)\n\n    # Iterate over the embeddings\n    for i in range(self.batch_size):\n        # Get the cosine similarity matrix for the current embedding\n        cos_sim_matrix_i = cos_sim_matrix[i]\n        # Get the diagonal of the cosine similarity matrix\n        cos_sim_matrix_i_diag = cos_sim_matrix_i.diag()\n        # Get the cosine similarity matrix for the current embedding without the diagonal\n        cos_sim_matrix_i_no_diag = cos_sim_matrix_i.clone()\n        cos_sim_matrix_i_no_diag[torch.eye(self.batch_size) == 1] = 0\n        # Get the maximum value in the cosine similarity matrix\n        max_value = torch.max(cos_sim_matrix_i_no_diag)\n        # Get the sum of the exponential values of the cosine similarity matrix\n        exp_sum = torch.sum(torch.exp(cos_sim_matrix_i_no_diag - max_value))\n        # Get the softmax values of the cosine similarity matrix\n        softmax = torch.exp(cos_sim_matrix_i_no_diag - max_value) / exp_sum\n        # Get the loss value\n        loss[i] = -torch.log(softmax[i][i])\n\n    return loss\n\n", "CodeBleu_score": 21.865530629287083}]}
{"instance_id": "25ac3945da2e", "generation_result": [{"code": "def embed_loss_contrast(self, dvecs, cos_sim_matrix)\n\t\n\n    # The function iterates over the embeddings, computing the sigmoid of the cosine similarity matrix for each embedding.\n    # For each embedding, it excludes the current centroid, computes the contrast loss, and appends the result to the loss row.\n    # The losses for each row are stacked into tensors, and the function returns the final stacked tensor of losses.\n\n    # The loss matrix is initialized with zeros.\n    loss_matrix = torch.zeros(dvecs.shape[0], device=self.device)\n\n    # The function iterates over the embeddings.\n    for idx in range(dvecs.shape[0]):\n\n        # The function excludes the current centroid from the cosine similarity matrix.\n        cos_sim_matrix[idx, idx] = -1\n\n        # The function computes the sigmoid of the cosine similarity matrix.\n        cos_sim_matrix_sigmoid = torch.sigmoid(cos_sim_matrix)\n\n        # The function computes the contrast loss.\n        contrast_loss = torch.log(\n            torch.sum(cos_sim_matrix_sigmoid[idx])\n            / torch.sum(cos_sim_matrix_sigmoid[idx, idx])\n        )\n\n        # The function appends the result to the loss row.\n        loss_matrix[idx] = contrast_loss\n\n    # The losses for each row are stacked into tensors, and the function returns the final stacked tensor of losses.\n    return loss_matrix\n\n", "CodeBleu_score": 27.42938095944979}, {"code": "def embed_loss_contrast(self, dvecs, cos_sim_matrix)\n\t\n    loss = torch.zeros(len(dvecs))\n    for i, dvec in enumerate(dvecs):\n        cos_sim_matrix_i = cos_sim_matrix[i]\n        cos_sim_matrix_i[i] = -1000\n        loss[i] = torch.sigmoid(-cos_sim_matrix_i).mean()\n    return loss.mean()\n\n", "CodeBleu_score": 12.811313763201179}, {"code": "def embed_loss_contrast(self, dvecs, cos_sim_matrix)\n\t\n    loss_row = []\n    for i in range(dvecs.shape[0]):\n        loss_row.append(nn.functional.binary_cross_entropy_with_logits(\n            input=cos_sim_matrix[i, :],\n            target=torch.zeros(cos_sim_matrix.shape[1], device=self.device),\n            weight=torch.ones(cos_sim_matrix.shape[1], device=self.device),\n            reduction='none'))\n        loss_row[i][i] = nn.functional.binary_cross_entropy_with_logits(\n            input=cos_sim_matrix[i, i],\n            target=torch.ones(1, device=self.device),\n            weight=torch.ones(1, device=self.device),\n            reduction='none')\n    return torch.stack(loss_row, dim=0)\n\n", "CodeBleu_score": 26.03681064427433}]}
{"instance_id": "878d14ffabd2", "generation_result": [{"code": "def forward(self, x, _label=None)\n\t\n    assert len(x.shape) == 3, 'Input tensor must be 3D.'\n    assert x.shape[1] >= 2, 'Input tensor must have at least two utterances per speaker.'\n\n    num_speakers = x.shape[0]\n    num_utts_per_speaker = x.shape[1]\n    dvec_feats = x.shape[2]\n\n    # Compute centroids of each speaker\n    centroids = torch.mean(x, dim=1, keepdim=True).expand(num_speakers, num_utts_per_speaker, dvec_feats)\n\n    # Compute similarity matrix\n    similarity_matrix = torch.matmul(x, centroids.transpose(1, 2))\n\n    # Scale and shift similarity matrix\n    scaled_similarity_matrix = (self.scale * similarity_matrix - 1) * self.margin\n\n    # Compute embedding loss\n    embedding_loss = torch.sum(torch.log(torch.exp(scaled_similarity_matrix) + 1))\n\n    # Compute mean loss\n    mean_loss = embedding_loss / (num_speakers * num_utts_per_speaker)\n\n    return mean_loss\n\n", "CodeBleu_score": 29.719308743838557}, {"code": "def forward(self, x, _label=None)\n\t\n    if x.size(0) < 2:\n        raise ValueError(\n            \"GE2E loss requires at least two embeddings per speaker (got {})\".format(x.size(0))\n        )\n    if x.size(1) < 2:\n        raise ValueError(\n            \"GE2E loss requires at least two utterances per speaker (got {})\".format(\n                x.size(1)\n            )\n        )\n\n    # Compute the centroids\n    centroids = x.mean(dim=1, keepdim=True)\n\n    # Compute the cosine similarity matrix\n    x = x.squeeze(1)\n    x = x.unsqueeze(1).expand(x.size(0), centroids.size(0), x.size(1))\n    centroids = centroids.expand(centroids.size(0), x.size(1), centroids.size(2))\n    cos_matrix = torch.nn.functional.cosine_similarity(x, centroids, dim=2)\n\n    # Scale and shift the cosine matrix\n    scale = torch.sqrt(torch.tensor(x.size(1)).float())\n    cos_matrix *= scale\n    cos_matrix -= 1.0\n\n    # Compute the embedding loss\n    embedding_loss = torch.nn.functional.softmax(cos_matrix, dim=1)\n    embedding_loss = torch.log(embedding_loss)\n    embedding_loss = -embedding_loss.mean(dim=1)\n    embedding_loss = embedding_loss.mean()\n\n    return embedding_loss\n\n", "CodeBleu_score": 30.52622258012746}, {"code": "def forward(self, x, _label=None)\n\t\n    num_speakers = x.shape[0]\n    num_utts_per_speaker = x.shape[1]\n    dvec_feats = x.shape[2]\n\n    # Ensure there are at least two utterances per speaker\n    if num_utts_per_speaker < 2:\n        raise ValueError(\"GE2ELoss requires at least two utterances per speaker.\")\n\n    # Compute centroids by averaging embeddings\n    centroids = x.mean(dim=1, keepdim=True)\n\n    # Compute cosine similarity matrix\n    cos_sim_matrix = torch.matmul(x, centroids.transpose(1, 2))\n\n    # Scale and shift cosine similarity matrix\n    cos_sim_matrix = cos_sim_matrix * 10\n    cos_sim_matrix = cos_sim_matrix - 5\n\n    # Compute embedding loss\n    embedding_loss = torch.zeros(num_speakers, num_utts_per_speaker)\n    for i in range(num_speakers):\n        for j in range(num_utts_per_speaker):\n            embedding_loss[i, j] = torch.relu(cos_sim_matrix[i, j, i] - cos_sim_matrix[i, j].max())\n\n    # Average embedding loss\n    embedding_loss = embedding_loss.mean()\n\n    return embedding_loss\n\n", "CodeBleu_score": 36.82047298714859}]}
{"instance_id": "86a364c72caa", "generation_result": [{"code": "def get_voices(extra_voice_dirs: List[str] = [])\n\t\n    voice_dirs = [\n        \"voices/\",\n        \"voices/bark/\",\n        \"voices/bark/hubert/\",\n        \"voices/bark/hubert/hubert_base_ls960_espeak_eng_v1/\",\n        \"voices/bark/hubert/hubert_base_ls960_espeak_eng_v1/hubert_base_ls960_espeak_eng_v1/\",\n        \"voices/bark/hubert/hubert_base_ls960_espeak_eng_v1/hubert_base_ls960_espeak_eng_v1/hubert_base_ls960_espeak_eng_v1/\",\n    ]\n    voice_dirs.extend(extra_voice_dirs)\n    voices = {}\n    for voice_dir in voice_dirs:\n        for subdir in os.listdir(voice_dir):\n            subdir_path = os.path.join(voice_dir, subdir)\n            if os.path.isdir(subdir_path):\n                voice_files = []\n                for file in os.listdir(subdir_path):\n                    file_path = os.path.join(subdir_path, file)\n                    if file.endswith(\".npz\") or file.endswith(\".wav\") or file.endswith(\".mp3\"):\n                        voice_files.append(file_path)\n                if voice_files:\n                    voices[subdir] = voice_files\n    return voices\n\n", "CodeBleu_score": 26.04461583898521}, {"code": "def get_voices(extra_voice_dirs: List[str] = [])\n\t\n    voices = {}\n    for voice_dir in extra_voice_dirs:\n        for subdir in os.listdir(voice_dir):\n            subdir_path = os.path.join(voice_dir, subdir)\n            if os.path.isdir(subdir_path):\n                voice_files = [\n                    os.path.join(subdir_path, voice_file)\n                    for voice_file in os.listdir(subdir_path)\n                    if re.match(r\"^.*\\.(npz|wav|mp3)$\", voice_file)\n                ]\n                if voice_files:\n                    voices[subdir] = voice_files\n    return voices\n\n", "CodeBleu_score": 28.480226375947936}, {"code": "def get_voices(extra_voice_dirs: List[str] = [])\n\t\n    voices = {}\n    for voice_dir in extra_voice_dirs:\n        for subdir in os.listdir(voice_dir):\n            subdir_path = os.path.join(voice_dir, subdir)\n            if os.path.isdir(subdir_path):\n                voices[subdir] = []\n                for file in os.listdir(subdir_path):\n                    file_path = os.path.join(subdir_path, file)\n                    if file.endswith(\".npz\") or file.endswith(\".wav\") or file.endswith(\".mp3\"):\n                        voices[subdir].append(file_path)\n    return voices\n\n", "CodeBleu_score": 30.111158779060936}]}
{"instance_id": "02f20c8917e6", "generation_result": [{"code": "def load_voice(model, voice: str, extra_voice_dirs: List[str] = [])\n\t\n    voices = get_voices(model, voice, extra_voice_dirs)\n    if len(voices) > 1:\n        raise ValueError(f\"Multiple voices found for {voice} in {extra_voice_dirs}\")\n    if len(voices) == 0:\n        return None, None, None\n    voice_path = voices[0]\n    if voice_path.endswith(\".npz\"):\n        voice_data = np.load(voice_path)\n        return voice_data[\"mels\"], voice_data[\"wavs\"], voice_data[\"embeds\"]\n    else:\n        voice_data = load_voice(model, voice, extra_voice_dirs)\n        return voice_data\n\n", "CodeBleu_score": 38.9392038582466}, {"code": "def load_voice(model, voice: str, extra_voice_dirs: List[str] = [])\n\t\n    voice_paths = get_voices(voice, extra_voice_dirs)\n    if len(voice_paths) > 1:\n        raise ValueError(f\"Multiple voices found for {voice}\")\n    if len(voice_paths) == 0:\n        return None, None, None\n    voice_path = voice_paths[0]\n    if voice_path.endswith(\".npz\"):\n        voice_data = np.load(voice_path)\n        return voice_data[\"embeddings\"], voice_data[\"speaker_id\"], voice_data[\"gender\"]\n    else:\n        voice_data = load_voice(model, voice, extra_voice_dirs)\n        return voice_data\n\n", "CodeBleu_score": 36.400860869842816}, {"code": "def load_voice(model, voice: str, extra_voice_dirs: List[str] = [])\n\t\n    voice_paths = get_voices(voice, extra_voice_dirs)\n    if voice_paths:\n        if len(voice_paths) > 1:\n            raise ValueError(f\"Multiple paths for voice {voice}: {voice_paths}\")\n        voice_path = voice_paths[0]\n        if voice_path.endswith(\".npz\"):\n            return load_npz_voice(voice_path)\n        else:\n            return load_voice(model, voice, extra_voice_dirs)\n    else:\n        return None\n\n", "CodeBleu_score": 32.36089284622879}]}
{"instance_id": "9458e981389f", "generation_result": [{"code": "def _relative_shift(self, pos_score: torch.Tensor) -> torch.Tensor\n\t\n    batch_size, seq_len, _ = pos_score.size()\n    zero_tensor = torch.zeros((batch_size, seq_len, 1)).to(self.device)\n    pos_score_padded = torch.cat([zero_tensor, pos_score], dim=2)\n    pos_score_padded = pos_score_padded.view(batch_size, 2 * seq_len, -1)\n    pos_score_shifted = pos_score_padded[:, :seq_len, :]\n    pos_score_shifted = pos_score_shifted.view(batch_size, seq_len, 2 * self.num_heads, self.relative_shift_size + 1)\n    pos_score_shifted = pos_score_shifted[:, :, :, 1:]\n    return pos_score_shifted\n\n", "CodeBleu_score": 29.29202684579676}, {"code": "def _relative_shift(self, pos_score: torch.Tensor) -> torch.Tensor\n\t\n    batch_size, head_num, length_1, length_2 = pos_score.shape\n    zero_tensor = torch.zeros(\n        (batch_size, head_num, length_1, 1), dtype=pos_score.dtype, device=pos_score.device)\n    pos_score = torch.cat([zero_tensor, pos_score], dim=-1)\n    pos_score = pos_score.view(batch_size, head_num, length_2 + 1, length_1)\n    pos_score = pos_score[:, :, 1:, :]\n    return pos_score\n\n", "CodeBleu_score": 34.474863020853256}, {"code": "def _relative_shift(self, pos_score: torch.Tensor) -> torch.Tensor\n\t\n    batch_size, num_heads, seq_len, seq_len2 = pos_score.size()\n    zero_pad = pos_score.new_zeros(batch_size, num_heads, seq_len, 1)\n    pos_score = torch.cat([zero_pad, pos_score], dim=-1)\n    pos_score = pos_score.view(batch_size, num_heads, seq_len2 + 1, seq_len)[:, :, 1:]\n    return pos_score\n\n", "CodeBleu_score": 32.022173224650444}]}
{"instance_id": "c046d4f00e80", "generation_result": [{"code": "def run_padded_sequence(self, context, lens)\n\t\n    # Get the batch size\n    batch_size = context.shape[0]\n\n    # Extract the contexts up to the specified lengths\n    contexts = [context[i, :l, :] for i, l in enumerate(lens)]\n\n    # Process each context\n    processed_contexts = []\n    for c in contexts:\n        # Apply the convolutional layers\n        c = self.conv1(c)\n        c = self.relu(c)\n        c = self.dropout(c)\n        c = self.conv2(c)\n        c = self.relu(c)\n        c = self.dropout(c)\n        c = self.conv3(c)\n        c = self.relu(c)\n        c = self.dropout(c)\n        c = self.conv4(c)\n        c = self.relu(c)\n        c = self.dropout(c)\n        c = self.conv5(c)\n        c = self.relu(c)\n        c = self.dropout(c)\n        c = self.conv6(c)\n        c = self.relu(c)\n        c = self.dropout(c)\n        c = self.conv7(c)\n        c = self.relu(c)\n        c = self.dropout(c)\n        c = self.conv8(c)\n        c = self.relu(c)\n        c = self.dropout(c)\n        c = self.conv9(c)\n        c = self.relu(c)\n        c = self.dropout(c)\n        c = self.conv10(c)\n        c = self.relu(c)\n        c = self.dropout(c)\n        c = self.conv11(c)\n        c = self.relu(c)\n        c = self.dropout(c)\n        c = self.conv12(c)\n        c = self.relu(c)\n        c = self.dropout(c)\n        c = self.conv13(c)\n        c = self.relu(c)", "CodeBleu_score": 12.747724598252816}, {"code": "def run_padded_sequence(self, context, lens)\n\t\n    # Extract contexts\n    context = context[:, :max(lens), :]\n    # Apply convolutions\n    context = self.conv_layers(context)\n    # Pad contexts\n    context = self.pad_sequence(context, lens)\n    return context\n\n", "CodeBleu_score": 10.929206496708483}, {"code": "def run_padded_sequence(self, context, lens)\n\t\n    # Extract the context up to the specified length for each batch element\n    context = context[:, :max(lens)]\n    # Apply the convolutional layers to the context\n    context = self.conv(context)\n    # Apply dropout\n    context = self.dropout(context)\n    # Apply ReLU\n    context = self.relu(context)\n    # Pad the contexts to create a consistent batch size\n    context = torch.nn.utils.rnn.pad_sequence(context.unbind(1), batch_first=True)\n    return context\n\n", "CodeBleu_score": 23.097776417945575}]}
{"instance_id": "db2644f3c681", "generation_result": [{"code": "def run_unsorted_inputs(self, fn, context, lens)\n\t\n\n    # Sort lengths and indices\n    lengths = lens.tolist()\n    indices = list(range(len(lengths)))\n    indices.sort(key=lambda x: lengths[x], reverse=True)\n    indices = torch.tensor(indices, dtype=torch.long, device=context.device)\n    lengths = torch.tensor(lengths, dtype=torch.long, device=context.device)\n\n    # Pack context sequences\n    context = context.index_select(0, indices)\n    context = torch.nn.utils.rnn.pack_padded_sequence(context, lengths, batch_first=True)\n\n    # Apply function\n    context, _ = fn(context)\n\n    # Unpack sequences\n    context, _ = torch.nn.utils.rnn.pad_packed_sequence(context, batch_first=True)\n\n    # Map results back to original indices\n    context = context.index_select(0, indices)\n\n    return context\n\n", "CodeBleu_score": 27.84628742660818}, {"code": "def run_unsorted_inputs(self, fn, context, lens)\n\t\n    lens, indices = torch.sort(lens, descending=True)\n    context = context.index_select(1, indices)\n    context = fn(context, lens)\n    _, indices = torch.sort(indices, descending=False)\n    context = context.index_select(1, indices)\n    return context\n\n", "CodeBleu_score": 9.943138965617246}, {"code": "def run_unsorted_inputs(self, fn, context, lens)\n\t\n    lens, indices = torch.sort(lens, descending=True)\n    context = context[indices]\n    context = torch.nn.utils.rnn.pack_padded_sequence(context, lens, batch_first=True)\n    context, _ = fn(context)\n    context, _ = torch.nn.utils.rnn.pad_packed_sequence(context, batch_first=True)\n    _, indices = torch.sort(indices)\n    context = context[indices]\n    return context\n\n", "CodeBleu_score": 17.754321722781455}]}
{"instance_id": "ced082bffc63", "generation_result": [{"code": "def remove_weight_norm(self)\n\t\n    if hasattr(self, \"kernel_predictor\"):\n        self.kernel_predictor.remove_weight_norm()\n    if hasattr(self, \"convt_pre\"):\n        for param in self.convt_pre.parameters():\n            if hasattr(param, \"parametrizations\"):\n                del param.parametrizations\n    if hasattr(self, \"conv_blocks\"):\n        for block in self.conv_blocks:\n            for param in block.parameters():\n                if hasattr(param, \"parametrizations\"):\n                    del param.parametrizations\n\n", "CodeBleu_score": 45.728635984604715}, {"code": "def remove_weight_norm(self)\n\t\n    self.kernel_predictor.remove_weight_norm()\n    self.convt_pre.remove_weight_norm()\n    for block in self.conv_blocks:\n        block.remove_weight_norm()\n\n", "CodeBleu_score": 33.51046275391997}, {"code": "def remove_weight_norm(self)\n\t\n    self.kernel_predictor.remove_weight_norm()\n    self.convt_pre.remove_weight_norm()\n    for block in self.conv_blocks:\n        block.remove_weight_norm()\n\n", "CodeBleu_score": 33.51046275391997}]}
{"instance_id": "b5a39fd97061", "generation_result": [{"code": "def map_mx_to_torch_model(mx_name)\n\t\n    mx_name = mx_name.lower()\n    mx_name = mx_name.replace('se_', 'se-')\n    mx_name = mx_name.replace('senet_', 'se-')\n    mx_name = mx_name.replace('inceptionv3', 'inception_v3')\n    return 'gluon_' + mx_name\n\n", "CodeBleu_score": 31.067628596309138}, {"code": "def map_mx_to_torch_model(mx_name)\n\t\n    mx_name = mx_name.lower()\n    if mx_name.startswith('se_'):\n        mx_name = mx_name.replace('se_', 'se-')\n    elif mx_name.startswith('senet_'):\n        mx_name = mx_name.replace('senet_', 'senet-')\n    elif mx_name.startswith('inceptionv3'):\n        mx_name = mx_name.replace('inceptionv3', 'inception_v3')\n    return 'gluon_' + mx_name\n\n", "CodeBleu_score": 47.127021514535}, {"code": "def map_mx_to_torch_model(mx_name)\n\t\n    name = mx_name.lower()\n    name = name.replace('se_', 'se-')\n    name = name.replace('senet_', 'senet-')\n    name = name.replace('inceptionv3', 'inception_v3')\n    return 'gluon_' + name\n\n", "CodeBleu_score": 29.97347810140898}]}
{"instance_id": "7eee5efdbf67", "generation_result": [{"code": "def _num_samples_per_worker(self)\n\t\n    num_workers = self.num_workers\n    num_samples = self.num_samples\n    repeats = self.repeats\n    if num_workers > 1:\n        num_samples = math.ceil(num_samples / num_workers)\n    if repeats > 1:\n        num_samples = math.ceil(num_samples / repeats)\n    if self.batch_size > 0:\n        num_samples = math.ceil(num_samples / self.batch_size) * self.batch_size\n    return int(num_samples)\n\n", "CodeBleu_score": 41.240314313889535}, {"code": "def _num_samples_per_worker(self)\n\t\n    if self.repeats > 1:\n        num_samples = math.ceil(self.repeats / self.num_workers)\n    else:\n        num_samples = math.ceil(self.num_samples / self.num_workers)\n\n    if self.training:\n        num_samples = math.ceil(num_samples / self.num_replicas)\n\n    if self.batch_size > 0:\n        num_samples = math.ceil(num_samples / self.batch_size) * self.batch_size\n\n    return int(num_samples)\n\n", "CodeBleu_score": 47.67989960252865}, {"code": "def _num_samples_per_worker(self)\n\t\n    num_samples = self.num_samples\n    num_workers = self.num_workers\n    batch_size = self.batch_size\n\n    if self.repeats > 1:\n        num_samples = num_samples * self.repeats\n\n    if num_workers > 1:\n        num_samples = math.ceil(num_samples / num_workers)\n\n    if batch_size > 0:\n        num_samples = math.ceil(num_samples / batch_size) * batch_size\n\n    return int(num_samples)\n\n", "CodeBleu_score": 33.76136055295569}]}
{"instance_id": "a698d65ae190", "generation_result": [{"code": "def find_images_and_targets(\n        folder: str,\n        types: Optional[Union[List, Tuple, Set]] = None,\n        class_to_idx: Optional[Dict] = None,\n        leaf_name_only: bool = True,\n        sort: bool = True\n)\n\t\n    if types is None:\n        types = ['.jpg', '.jpeg', '.png', '.ppm', '.bmp', '.pgm', '.tif']\n    if class_to_idx is None:\n        class_to_idx = {}\n        found_classes = set()\n    images_and_targets = []\n    for root, subdirs, files in os.walk(folder):\n        for file in files:\n            base, ext = os.path.splitext(file)\n            if ext.lower() in types:\n                full_path = os.path.join(root, file)\n                if leaf_name_only:\n                    path_without_ext = os.path.join(os.path.basename(root), base)\n                else:\n                    path_without_ext = full_path[: -len(ext)]\n                if class_to_idx:\n                    class_name = path_without_ext\n                else:\n                    class_name = os.path.basename(root)\n                if class_name not in class_to_idx:\n                    found_classes.add(class_name)\n                    class_to_idx[class_name] = len(class_to_idx)\n                target = class_to_idx[class_name]\n                item = (full_path, target)\n                images_and_targets.append(item)\n    if sort:\n        images_and_targets = sorted(images_and_targets, key=lambda k: natural_key(k[0]))\n    if not class_to_idx:\n        class_to_idx = {v: k for k, v in enumerate(sorted(found_classes))}\n    return images_and_targets, class_to_idx\n", "CodeBleu_score": 46.40683371534094}, {"code": "def find_images_and_targets(\n        folder: str,\n        types: Optional[Union[List, Tuple, Set]] = None,\n        class_to_idx: Optional[Dict] = None,\n        leaf_name_only: bool = True,\n        sort: bool = True\n)\n\t\n\n    if types is None:\n        types = ['.jpg', '.jpeg', '.png', '.ppm', '.bmp', '.pgm', '.tif']\n    elif isinstance(types, str):\n        types = [types]\n\n    labels = []\n    filenames = []\n    for root, subdirs, files in os.walk(folder, topdown=False, followlinks=True):\n        rel_path = os.path.relpath(root, folder) if (root != folder) else ''\n        label = os.path.basename(rel_path) if leaf_name_only else rel_path.replace(os.path.sep, '_')\n        for f in files:\n            base, ext = os.path.splitext(f)\n            if ext.lower() in types:\n                filenames.append(os.path.join(root, f))\n                labels.append(label)\n    if class_to_idx is None:\n        unique_labels = set(labels)\n        sorted_labels = list(sorted(unique_labels, key=natural_key))\n        class_to_idx = {c: idx for idx, c in enumerate(sorted_labels)}\n    images_and_targets = [(f, class_to_idx[l]) for f, l in zip(filenames, labels) if l in class_to_idx]\n    if sort:\n        images_and_targets = sorted(images_and_targets, key=lambda k: natural_key(k[0]))\n    return images_and_targets, class_to_idx\n", "CodeBleu_score": 82.52381307907008}, {"code": "def find_images_and_targets(\n        folder: str,\n        types: Optional[Union[List, Tuple, Set]] = None,\n        class_to_idx: Optional[Dict] = None,\n        leaf_name_only: bool = True,\n        sort: bool = True\n)\n\t\n\n    if types is None:\n        types = ['.jpg', '.jpeg', '.png', '.ppm', '.bmp', '.pgm', '.tif']\n    elif not isinstance(types, (list, tuple, set)):\n        types = [types]\n    else:\n        types = list(types)\n\n    labels = []\n    filenames = []\n    for root, subdirs, files in os.walk(folder, topdown=False):\n        rel_path = os.path.relpath(root, folder) if (root != folder) else ''\n        label = os.path.basename(rel_path) if leaf_name_only else rel_path.replace(os.path.sep, '_')\n        for f in files:\n            base, ext = os.path.splitext(f)\n            if ext.lower() in types:\n                filenames.append(os.path.join(root, f))\n                labels.append(label)\n\n    if class_to_idx is None:\n        unique_labels = set(labels)\n        sorted_labels = list(sorted(unique_labels, key=natural_key))\n        class_to_idx = {c: idx for idx, c in enumerate(sorted_labels)}\n    else:\n        unique_labels = set(labels)\n        assert unique_labels.issubset(\n            class_to_idx.keys()\n        ), \"labels {} are not a subset of the class_to_idx keys {}\".format(unique_labels, class_to_idx.keys())\n\n    if sort:\n        labels, filenames = zip(*sorted(zip(labels, filenames)))\n    return filenames, [class_to_idx[l] for l in labels], class_to_idx\n", "CodeBleu_score": 65.08909362159267}]}
{"instance_id": "e7fe43bdecdc", "generation_result": [{"code": "def pytorch_worker_seed()\n\t\n    worker_info = get_worker_info()\n    if worker_info is not None:\n        return worker_info.seed\n    else:\n        return int(torch.utils.data.get_worker_info().id) + int(os.getenv(\"WDS_RANK\", 0))\n\n", "CodeBleu_score": 51.77214720352592}, {"code": "def pytorch_worker_seed()\n\t\n    worker_info = get_worker_info()\n    if worker_info is not None:\n        return worker_info.seed\n    else:\n        return int(os.environ.get(\"WDS_RANK\", 0))\n\n", "CodeBleu_score": 51.0145924255473}, {"code": "def pytorch_worker_seed()\n\t\n    worker_info = get_worker_info()\n    if worker_info is not None:\n        return worker_info.seed\n    else:\n        return random.randint(0, 2 ** 32 - 1)\n\n", "CodeBleu_score": 49.950150092858806}]}
{"instance_id": "0ffc7e7ded25", "generation_result": [{"code": "def solarize_add(img, add, thresh=128, **__)\n\t\n    if img.mode in (\"L\", \"RGB\"):\n        lut = []\n        for i in range(256):\n            if i < thresh:\n                lut.append(min(255, i + add))\n            else:\n                lut.append(i)\n        if img.mode == \"L\":\n            img = img.point(lut)\n        elif img.mode == \"RGB\":\n            channels = img.split()\n            channels[0] = channels[0].point(lut)\n            channels[1] = channels[1].point(lut)\n            channels[2] = channels[2].point(lut)\n            img = Image.merge(img.mode, channels)\n        return img\n    else:\n        return img\n\n", "CodeBleu_score": 57.70578855878553}, {"code": "def solarize_add(img, add, thresh=128, **__)\n\t\n    if img.mode not in (\"L\", \"RGB\"):\n        return img\n    lut = []\n    for i in range(256):\n        if i < thresh:\n            lut.append(min(255, i + add))\n        else:\n            lut.append(i)\n    if img.mode == \"L\":\n        return img.point(lut)\n    else:\n        channels = img.split()\n        channels[0] = channels[0].point(lut)\n        return Image.merge(img.mode, channels)\n\n", "CodeBleu_score": 63.26982437416857}, {"code": "def solarize_add(img, add, thresh=128, **__)\n\t\n    if img.mode in (\"L\", \"RGB\"):\n        lut = []\n        for i in range(256):\n            if i < thresh:\n                lut.append(min(255, i + add))\n            else:\n                lut.append(i)\n        if img.mode == \"L\":\n            img = img.point(lut)\n        elif img.mode == \"RGB\":\n            img = img.point(lut * 3)\n        return img\n    else:\n        return img\n\n", "CodeBleu_score": 63.6285734466565}]}
{"instance_id": "5a0d29577a21", "generation_result": [{"code": "def _get_weighted_transforms(transforms: Dict)\n\t\n    transform_names = []\n    transform_probabilities = []\n    for transform_name, probability in transforms.items():\n        transform_names.append(transform_name)\n        transform_probabilities.append(probability)\n    transform_probabilities = [probability / sum(transform_probabilities) for probability in transform_probabilities]\n    return transform_names, transform_probabilities\n\n", "CodeBleu_score": 27.822806855337383}, {"code": "def _get_weighted_transforms(transforms: Dict)\n\t\n    transform_names = list(transforms.keys())\n    transform_probs = list(transforms.values())\n    transform_probs = [float(prob) for prob in transform_probs]\n    total_prob = sum(transform_probs)\n    transform_probs = [prob / total_prob for prob in transform_probs]\n    return transform_names, transform_probs\n\n", "CodeBleu_score": 20.334068605880244}, {"code": "def _get_weighted_transforms(transforms: Dict)\n\t\n    transforms_names = list(transforms.keys())\n    transforms_probs = list(transforms.values())\n    transforms_probs = [float(prob) for prob in transforms_probs]\n    transforms_probs = [prob / sum(transforms_probs) for prob in transforms_probs]\n    return transforms_names, transforms_probs\n\n", "CodeBleu_score": 20.059041251760252}]}
{"instance_id": "84bc5658c135", "generation_result": [{"code": "def _apply_basic(self, img, mixing_weights, m)\n\t\n\n    # Initialize the mixed image\n    mixed_img = Image.new(img.mode, img.size)\n\n    # Iterate through the mixing weights\n    for mixing_weight in mixing_weights:\n\n        # Get the random sequence of operations\n        random_sequence = self.get_random_sequence(m)\n\n        # Apply the sequence of operations to the image\n        augmented_img = self.apply_sequence(img, random_sequence)\n\n        # Accumulate the results\n        mixed_img = ImageChops.add(mixed_img, augmented_img)\n\n    # Clip the mixed image to valid pixel values\n    mixed_img = mixed_img.convert(img.mode)\n    mixed_img = ImageOps.autocontrast(mixed_img)\n\n    # Blend the mixed image with the original image\n    mixed_img = ImageChops.blend(img, mixed_img, mixing_weight)\n\n    # Return the blended image\n    return mixed_img\n\n", "CodeBleu_score": 25.50887865497174}, {"code": "def _apply_basic(self, img, mixing_weights, m)\n\t\n\n    # Initialize the mixed image\n    mixed = Image.new(img.mode, img.size)\n\n    # Iterate through the mixing weights\n    for w in mixing_weights:\n\n        # Initialize the augmented image\n        augmented = img\n\n        # Apply a random sequence of operations to the image\n        for _ in range(m):\n            augmented = self.augment(augmented)\n\n        # Accumulate the results\n        mixed = ImageChops.add(mixed, augmented)\n\n    # Clip the mixed image to valid pixel values\n    mixed = ImageOps.clip(mixed)\n\n    # Convert the mixed image back to an image\n    mixed = Image.fromarray(mixed)\n\n    # Blend the mixed image with the original image using a specified blending factor\n    mixed = ImageChops.blend(img, mixed, w)\n\n    return mixed\n\n", "CodeBleu_score": 23.638741584099115}, {"code": "def _apply_basic(self, img, mixing_weights, m)\n\t\n\n    # Initialize mixed image\n    mixed_img = torch.zeros_like(img)\n\n    # Iterate through mixing weights\n    for w in mixing_weights:\n\n        # Apply random sequence of operations to image\n        img_aug = self._apply_ops(img)\n\n        # Accumulate results\n        mixed_img = mixed_img + w * img_aug\n\n    # Clip mixed image to valid pixel values\n    mixed_img = mixed_img.clamp(0.0, 1.0)\n\n    # Convert mixed image to image and blend with original image\n    mixed_img = self.to_image(mixed_img)\n    img_blended = self.blend(img, mixed_img, m)\n\n    return img_blended\n\n", "CodeBleu_score": 20.984715885714316}]}
{"instance_id": "84b5b752ff04", "generation_result": [{"code": "def adapt_to_chs(x, n)\n\t\n    if not isinstance(x, (tuple, list)):\n        x = tuple(repeat(x, n))\n    elif len(x) == n:\n        x = tuple(x)\n    elif len(x) == 1:\n        x = tuple(repeat(x[0], n))\n    else:\n        logging.warning(\n            f\"The length of x ({len(x)}) is not equal to the number of channels ({n}). The mean of x will be used to adapt x to the number of channels.\"\n        )\n        x = tuple(repeat(sum(x) / len(x), n))\n    return x\n\n", "CodeBleu_score": 49.32650918739554}, {"code": "def adapt_to_chs(x, n)\n\t\n    if not isinstance(x, (tuple, list)):\n        x = tuple(repeat(x, n))\n    else:\n        if len(x) != n:\n            logging.warning(f\"The number of channels of the input {len(x)} does not match the required number of channels {n}.\")\n            x = tuple(repeat(sum(x) / len(x), n))\n        assert len(x) == n, f\"The number of channels of the input {len(x)} does not match the required number of channels {n}.\"\n    return x\n\n", "CodeBleu_score": 47.616120895438044}, {"code": "def adapt_to_chs(x, n)\n\t\n    if not isinstance(x, (tuple, list)):\n        x = tuple(repeat(x, n))\n    elif len(x) != n:\n        logging.warning(\n            \"The number of channels of input and output does not match. \"\n            \"The input will be repeated to match the required number of channels.\"\n        )\n        x = tuple(repeat(sum(x) / len(x), n))\n    else:\n        assert len(x) == n\n    return x\n\n", "CodeBleu_score": 52.10118429762487}]}
{"instance_id": "54754b6ba394", "generation_result": [{"code": "def _worker_init(worker_id, worker_seeding='all')\n\t\n\n    if worker_seeding == 'all':\n        worker_info = torch.utils.data.get_worker_info()\n        if worker_info is None:\n            raise RuntimeError('Expected to be called with worker_info, but got None.')\n        if worker_info.id != worker_id:\n            raise RuntimeError('Expected the same worker_id as worker_info.id')\n\n        seed = worker_info.seed % (2 ** 32)\n        random.seed(seed)\n        torch.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n        import numpy as np\n        np.random.seed(seed)\n    elif isinstance(worker_seeding, Callable):\n        worker_info = torch.utils.data.get_worker_info()\n        if worker_info is None:\n            raise RuntimeError('Expected to be called with worker_info, but got None.')\n        if worker_info.id != worker_id:\n            raise RuntimeError('Expected the same worker_id as worker_info.id')\n        worker_seeding(worker_info.seed % (2 ** 32))\n    elif isinstance(worker_seeding, str) and worker_seeding == 'numpy':\n        worker_info = torch.utils.data.get_worker_info()\n        if worker_info is None:\n            raise RuntimeError('Expected to be called with worker_info, but got None.')\n        if worker_info.id != worker_id:\n            raise RuntimeError('Expected the same worker_id as worker_info.id')\n        import numpy as np\n        np.random.seed(worker_info.seed % (2 ** 32))\n    else:\n        raise TypeError('worker_seeding must be a callable, string or None, but got {}'.format(type(worker_seeding)))\n\n", "CodeBleu_score": 44.622366673215915}, {"code": "def _worker_init(worker_id, worker_seeding='all')\n\t\n    worker_info = torch.utils.data.get_worker_info()\n    if worker_info is None:  # single-process data loading, return the seed directly\n        return None\n    if worker_seeding == 'all':\n        # for the numpy.random seed, we only set the seed for the first worker,\n        # otherwise it will be the same across all workers, defeating the purpose\n        if worker_info.id != 0:\n            worker_seeding = None\n        else:\n            worker_seeding = worker_info.seed\n    elif worker_seeding == 'numpy':\n        worker_seeding = worker_info.seed\n    elif isinstance(worker_seeding, Callable):\n        worker_seeding = worker_seeding(worker_info)\n    elif worker_seeding is not None:\n        raise ValueError(f'worker_seeding must be None, a string or a callable, but got {type(worker_seeding)}')\n    if worker_seeding is not None:\n        random.seed(worker_seeding)\n        torch.manual_seed(worker_seeding)\n        torch.cuda.manual_seed_all(worker_seeding)\n        if worker_seeding == 'all':\n            torch.backends.cudnn.deterministic = True\n            torch.backends.cudnn.benchmark = False\n    return None\n\n", "CodeBleu_score": 39.71495714774378}, {"code": "def _worker_init(worker_id, worker_seeding='all')\n\t\n    worker_info = torch.utils.data.get_worker_info()\n    if worker_info is None:  # single-process data loading, return the seed directly\n        return None\n    if worker_seeding == 'all':\n        # set the seed of random, torch, and numpy module for each worker\n        torch.manual_seed(worker_info.seed % torch.cuda.device_count())\n        random.seed(worker_info.seed % torch.cuda.device_count())\n        import numpy as np\n        np.random.seed(worker_info.seed % torch.cuda.device_count())\n    elif callable(worker_seeding):\n        worker_seeding(worker_info.seed % torch.cuda.device_count())\n    elif worker_seeding == 'numpy':\n        import numpy as np\n        np.random.seed(worker_info.seed % torch.cuda.device_count())\n    else:\n        raise ValueError('worker seeding must be \"all\", \"numpy\", or a callable')\n\n", "CodeBleu_score": 38.125209840878604}]}
{"instance_id": "81dfdce940db", "generation_result": [{"code": "def _decode_and_center_crop(image_bytes, image_size, resize_method)\n\t\n    image_shape = tf.image.extract_jpeg_shape(image_bytes)\n    image_height = image_shape[0]\n    image_width = image_shape[1]\n    padded_center_crop_size = tf.cast(\n        ((image_size / (image_size + CROP_PADDING)) *\n         tf.cast(tf.minimum(image_height, image_width), tf.float32)),\n        tf.int32)\n    offset_height = ((image_height - padded_center_crop_size) + 1) // 2\n    offset_width = ((image_width - padded_center_crop_size) + 1) // 2\n    crop_window = tf.stack([offset_height, offset_width,\n                            padded_center_crop_size, padded_center_crop_size])\n    image = tf.image.decode_and_crop_jpeg(image_bytes, crop_window, channels=3)\n    image = tf.image.resize([image], [image_size, image_size],\n                            method=resize_method)[0]\n    return image\n\n", "CodeBleu_score": 59.56705134977531}, {"code": "def _decode_and_center_crop(image_bytes, image_size, resize_method)\n\t\n\n    # extract image shape\n    image_shape = tf.image.extract_jpeg_shape(image_bytes)\n\n    # calculate padded center crop size\n    padded_center_crop_size = _get_padded_center_crop_size(image_size, image_shape)\n\n    # determine offset for crop window\n    offset = _get_offset(image_size, padded_center_crop_size)\n\n    # decode and crop image\n    image = tf.image.decode_and_crop_jpeg(image_bytes, offset, padded_center_crop_size)\n\n    # resize image\n    image = _resize(image, image_size, resize_method)\n\n    # return processed image\n    return image\n\n", "CodeBleu_score": 12.215296137488888}, {"code": "def _decode_and_center_crop(image_bytes, image_size, resize_method)\n\t\n    image_shape = tf.image.extract_jpeg_shape(image_bytes)\n    image_height = image_shape[0]\n    image_width = image_shape[1]\n    padded_center_crop_size = tf.cast(\n        ((image_size / (image_size + _CROP_PADDING)) *\n         tf.cast(tf.minimum(image_height, image_width), tf.float32)),\n        tf.int32)\n\n    offset_height = ((image_height - padded_center_crop_size) + 1) // 2\n    offset_width = ((image_width - padded_center_crop_size) + 1) // 2\n    crop_window = tf.stack([offset_height, offset_width,\n                             padded_center_crop_size, padded_center_crop_size])\n    image = tf.image.decode_and_crop_jpeg(image_bytes, crop_window, channels=3)\n    image = tf.image.resize(\n        image, [image_size, image_size], method=resize_method)\n    return image\n\n", "CodeBleu_score": 57.62508244320761}]}
{"instance_id": "cf16a6d37111", "generation_result": [{"code": "def preprocess_for_train(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n\n    # Decode the image\n    image = tf.io.decode_jpeg(image_bytes, channels=3)\n\n    # Resize the image\n    image = tf.image.resize(image, [image_size, image_size], method=interpolation)\n\n    # Randomly flip the image horizontally\n    image = tf.image.random_flip_left_right(image)\n\n    # Reshape the image to the target size\n    image = tf.reshape(image, [image_size, image_size, 3])\n\n    # Convert the image to the specified dtype\n    if use_bfloat16:\n        image = tf.image.convert_image_dtype(image, tf.bfloat16)\n    else:\n        image = tf.image.convert_image_dtype(image, tf.float32)\n\n    return image\n\n", "CodeBleu_score": 41.30514383898486}, {"code": "def preprocess_for_train(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    image = tf.io.decode_jpeg(image_bytes, channels=3)\n    image = tf.image.resize_with_crop_or_pad(image, image_size, image_size)\n    image = tf.image.random_flip_left_right(image)\n    image = tf.image.resize(image, [image_size, image_size], method=interpolation)\n    image = tf.image.convert_image_dtype(image, tf.bfloat16 if use_bfloat16 else tf.float32)\n    return image\n", "CodeBleu_score": 32.45267992570936}, {"code": "def preprocess_for_train(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    image = tf.io.decode_jpeg(image_bytes, channels=3)\n    image = tf.image.random_crop(image, size=(image_size, image_size, 3))\n    image = tf.image.random_flip_left_right(image)\n    image = tf.image.resize(image, [image_size, image_size], method=interpolation)\n    image /= 255.0  # normalize to [0,1] range\n\n    if use_bfloat16:\n        image = tf.cast(image, tf.bfloat16)\n    else:\n        image = tf.cast(image, tf.float32)\n\n    return image\n", "CodeBleu_score": 34.470097099400746}]}
{"instance_id": "78b2a3bd2142", "generation_result": [{"code": "def preprocess_for_eval(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    # Decode the image.\n    image = tf.io.decode_image(image_bytes, channels=3)\n    image = tf.image.convert_image_dtype(image, tf.float32)\n\n    # Resize the image to the desired size.\n    if interpolation == 'bicubic':\n        image = tf.image.resize(image, [image_size, image_size], method=tf.image.ResizeMethod.BICUBIC)\n    elif interpolation == 'bilinear':\n        image = tf.image.resize(image, [image_size, image_size], method=tf.image.ResizeMethod.BILINEAR)\n    elif interpolation == 'nearest':\n        image = tf.image.resize(image, [image_size, image_size], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n    elif interpolation == 'area':\n        image = tf.image.resize(image, [image_size, image_size], method=tf.image.ResizeMethod.AREA)\n    else:\n        raise ValueError('Invalid interpolation method.')\n\n    # Center the image crop.\n    image = tf.image.resize_with_crop_or_pad(image, image_size, image_size)\n\n    # Convert the image data type to bfloat16 or float32.\n    if use_bfloat16:\n        image = tf.cast(image, tf.bfloat16)\n    else:\n        image = tf.cast(image, tf.float32)\n\n    return image\n\n", "CodeBleu_score": 41.02480662870377}, {"code": "def preprocess_for_eval(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    image = tf.image.decode_image(image_bytes, channels=3)\n    image = tf.image.resize_with_pad(image, image_size, image_size, method=interpolation, antialias=True)\n    image = tf.cast(image, tf.float32)\n    image = image / 255.0\n    image = tf.image.resize_with_crop_or_pad(image, image_size, image_size)\n    image = tf.image.resize(image, [image_size, image_size], method=interpolation, antialias=True)\n    if use_bfloat16:\n        image = tf.cast(image, tf.bfloat16)\n    return image\n\n", "CodeBleu_score": 40.10526808589902}, {"code": "def preprocess_for_eval(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    # Decode and center the crop\n    image = tf.io.decode_jpeg(image_bytes, channels=3, dct_method='INTEGER_FAST')\n    image = tf.image.resize_with_pad(image, image_size, image_size, method=interpolation)\n    # Resize the image to the specified height and width.\n    image = tf.image.resize(image, [image_size, image_size], method=interpolation)\n    # Convert the image to bfloat16 if necessary.\n    if use_bfloat16:\n        image = tf.cast(image, dtype=tf.bfloat16)\n    else:\n        image = tf.cast(image, dtype=tf.float32)\n    return image\n\n", "CodeBleu_score": 35.65574172817061}]}
{"instance_id": "8bd4364a9a0e", "generation_result": [{"code": "def evaluate(self, model, eval_dataloader=None, output_file=\"merged\")\n\t\n\n    # Setup the eval_dataloader\n    if eval_dataloader is None:\n        eval_dataloader = self.eval_dataloader\n\n    # Run prediction\n    self.predict(model, eval_dataloader, output_file)\n\n    # Load the output file\n    pred_file = os.path.join(self.output_dir, output_file + \".pred\")\n    assert os.path.exists(pred_file), \"The prediction file does not exist.\"\n    pred = []\n    with open(pred_file, \"r\") as f:\n        for line in f:\n            pred.append(line.strip())\n\n    # Load the ground truth file\n    gold_file = os.path.join(self.output_dir, output_file + \".gold\")\n    assert os.path.exists(gold_file), \"The ground truth file does not exist.\"\n    gold = []\n    with open(gold_file, \"r\") as f:\n        for line in f:\n            gold.append(line.strip())\n\n    # Compute evaluation metrics\n    metrics = self.evaluate_metrics(pred, gold)\n\n    # Print results\n    print(\"Evaluation results:\")\n    for metric in metrics:\n        print(f\"{metric}: {metrics[metric]}\")\n\n    # Return results\n    return metrics\n\n", "CodeBleu_score": 49.529685907432466}, {"code": "def evaluate(self, model, eval_dataloader=None, output_file=\"merged\")\n\t\n\n    if eval_dataloader is None:\n        eval_dataloader = self.get_eval_dataloader()\n\n    # Prediction loop\n    outputs = self.predict(model, eval_dataloader)\n\n    # Saving predictions to file\n    self.save_predictions(outputs, output_file)\n\n    # Computing evaluation metrics\n    results = self.compute_metrics(outputs)\n\n    return results\n\n", "CodeBleu_score": 56.50510573407954}, {"code": "def evaluate(self, model, eval_dataloader=None, output_file=\"merged\")\n\t\n    if eval_dataloader is None:\n        eval_dataloader = self.eval_dataloader\n    model.eval()\n    output_file = os.path.join(self.output_dir, output_file)\n    with open(output_file, \"w\") as f:\n        for batch in eval_dataloader:\n            with torch.no_grad():\n                input_ids = batch[\"input_ids\"].to(self.device)\n                attention_mask = batch[\"attention_mask\"].to(self.device)\n                token_type_ids = batch[\"token_type_ids\"].to(self.device)\n                outputs = model(\n                    input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids\n                )\n            logits = outputs[0]\n            logits = logits.detach().cpu().numpy()\n            for i in range(logits.shape[0]):\n                for j in range(logits.shape[1]):\n                    f.write(str(logits[i][j]) + \" \")\n                f.write(\"", "CodeBleu_score": 42.28630851386021}]}
{"instance_id": "22d682c958c0", "generation_result": [{"code": " def print_computed_metrics(self, metrics)\n\t\n    print(\"Rank1: \", round(metrics[\"rank1\"], 4))\n    print(\"Rank5: \", round(metrics[\"rank5\"], 4))\n    print(\"mIoU: \", round(metrics[\"miou\"], 4))\n\n\n def print_metrics(self, metrics):\n    \"\"\"The function prints the computed metrics for rank1, rank5, and miou.", "CodeBleu_score": 26.57280993192926}, {"code": " def print_computed_metrics(self, metrics)\n\t\n    rank1 = metrics['rank1']\n    rank5 = metrics['rank5']\n    miou = metrics['miou']\n    print(\"Rank@1: {0:.4f}\".format(rank1))\n    print(\"Rank@5: {0:.4f}\".format(rank5))\n    print(\"mIoU: {0:.4f}\".format(miou))\n\n", "CodeBleu_score": 27.579107254724356}, {"code": " def print_computed_metrics(self, metrics)\n\t\n    print(\"Rank1: \", format(metrics[\"rank1\"], \".4f\"))\n    print(\"Rank5: \", format(metrics[\"rank5\"], \".4f\"))\n    print(\"mIoU: \", format(metrics[\"mean_iou\"], \".4f\"))\n\n", "CodeBleu_score": 23.492872327534997}]}
{"instance_id": "39c1a50b7e7d", "generation_result": [{"code": "def _eval_predictions(self, segments, data)\n\t\n    rank1 = 0\n    rank5 = 0\n    iou_sum = 0\n    for segment, data in zip(segments, data):\n        # get top 5 predictions\n        preds = [\n            (self.class_names[i], p)\n            for i, p in enumerate(segment[\"preds\"])\n            if p > 0.05\n        ][:5]\n\n        # get ground truth data\n        gt_classes = data[\"gt_classes\"]\n        gt_boxes = data[\"gt_boxes\"]\n\n        # get top 5 IoUs\n        ious = []\n        for gt_class, gt_box in zip(gt_classes, gt_boxes):\n            for pred_class, pred_box in preds:\n                if gt_class == pred_class:\n                    iou = self._iou(pred_box, gt_box)\n                    ious.append(iou)\n        ious.sort(reverse=True)\n\n        # get top 5 ranks\n        ranks = []\n        for pred_class, _ in preds:\n            for i, gt_class in enumerate(gt_classes):\n                if gt_class == pred_class:\n                    ranks.append(i + 1)\n        ranks.sort()\n\n        # update rank1 and rank5\n        if ranks[0] == 1:\n            rank1 += 1\n        if ranks[0] <= 5:\n            rank5 += 1\n\n        # update iou_sum\n        iou_sum += sum(ious[:5])\n\n    # calculate mean IoU and rank percentages\n    mean_iou = iou_sum / len(segments)\n    rank1_percent = rank1 / len(segments)\n    rank5_percent = rank5 / len(segments)\n\n    return rank1_percent, rank5_percent, mean_iou\n\n", "CodeBleu_score": 28.650374913716377}, {"code": "def _eval_predictions(self, segments, data)\n\t\n    rank1, rank5, mean_iou = 0, 0, 0\n    for i, segment in enumerate(segments):\n        ious = []\n        for j, data_segment in enumerate(data):\n            iou = self._get_iou(segment, data_segment)\n            ious.append(iou)\n        ious.sort()\n        if ious[-1] > 0.5:\n            rank1 += 1\n        if ious[-5] > 0.5:\n            rank5 += 1\n        mean_iou += ious[-1]\n    rank1 /= len(segments)\n    rank5 /= len(segments)\n    mean_iou /= len(segments)\n    return rank1, rank5, mean_iou\n\n", "CodeBleu_score": 27.43397645568539}, {"code": "def _eval_predictions(self, segments, data)\n\t\n    ranks = []\n    ious = []\n    for segment, data in zip(segments, data):\n        if segment is None:\n            continue\n        iou = self._get_iou(segment, data)\n        ious.append(iou)\n        rank = self._get_rank(segment, data)\n        ranks.append(rank)\n    return self._get_ranks_and_iou(ranks, ious)\n\n", "CodeBleu_score": 18.23523478203601}]}
{"instance_id": "8ccf84f8d034", "generation_result": [{"code": "def predict_loop(self, model, eval_dataloader, output_file=None)\n\t\n    model.eval()\n    model.to(self.device)\n    preds = []\n    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n        batch = tuple(t.to(self.device) for t in batch)\n        with torch.no_grad():\n            inputs = {\"input_ids\": batch[0], \"attention_mask\": batch[1], \"labels\": batch[3]}\n            if self.args.model_type != \"distilbert\":\n                inputs[\"token_type_ids\"] = (\n                    batch[2] if self.args.model_type in [\"bert\", \"xlnet\"] else None\n                )  # XLM, DistilBERT and RoBERTa don't use segment_ids\n            outputs = model(**inputs)\n            _, logits = outputs[:2]\n            preds.append(logits.cpu().numpy())\n    preds = np.vstack(preds)\n    if output_file:\n        np.save(output_file, preds)\n    return preds\n\n", "CodeBleu_score": 31.832658439165012}, {"code": "def predict_loop(self, model, eval_dataloader, output_file=None)\n\t\n\n    # initialize the list for scores\n    scores = []\n\n    # set the model to evaluation mode\n    model.eval()\n\n    # transfer the model to GPU 0\n    model.to(self.device)\n\n    # iterate over the dataloader\n    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n\n        # transfer the data to the appropriate context\n        batch = tuple(t.to(self.device) for t in batch)\n\n        # unpack the data\n        input_ids, input_mask, segment_ids, label_ids = batch\n\n        # make predictions without computing gradients\n        with torch.no_grad():\n            # make predictions\n            logits = model(input_ids, segment_ids, input_mask)\n\n        # update the list of scores\n        scores.extend(logits.cpu().numpy())\n\n    # finalize the results\n    scores = self.finalize_results(scores)\n\n    # optionally save the results to an output file\n    if output_file:\n        self.save_results(scores, output_file)\n\n    # return the results\n    return scores\n\n", "CodeBleu_score": 36.44851043609358}, {"code": "def predict_loop(self, model, eval_dataloader, output_file=None)\n\t\n    # Initialize the list of scores\n    scores = []\n\n    # Set the model to evaluation mode\n    model.eval()\n\n    # Transfer the model to GPU 0\n    model = model.to(0)\n\n    # Iterate over the dataloader\n    for batch in tqdm(eval_dataloader):\n        # Transfer the data to GPU 0\n        batch = tuple(t.to(0) for t in batch)\n\n        # Unpack the data\n        input_ids, attention_mask, token_type_ids, label_ids = batch\n\n        # Make predictions without computing gradients\n        with torch.no_grad():\n            outputs = model(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                token_type_ids=token_type_ids,\n                labels=label_ids,\n            )\n\n        # Update the scores\n        logits = outputs[0]\n        scores.extend(logits.cpu().tolist())\n\n    # Finalize the results\n    results = {\"scores\": scores}\n\n    # Optionally save the results to an output file\n    if output_file:\n        with open(output_file, \"w\") as f:\n            json.dump(results, f)\n\n    # Return the results\n    return results\n\n", "CodeBleu_score": 35.061811529199346}]}
{"instance_id": "e68529ad727d", "generation_result": [{"code": "def _aggregate_scores(self, scores)\n\t\n    assert len(scores) == 2, \"There should be exactly two sets of scores\"\n    self.full_scores = torch.cat(scores, dim=0)\n    return torch.mm(self.full_scores[:, 0], self.full_scores[:, 1].T)\n\n", "CodeBleu_score": 28.425673886138235}, {"code": "def _aggregate_scores(self, scores)\n\t\n    assert len(scores) == 2, \"There should be exactly two sets of scores.\"\n    for i in range(len(scores)):\n        scores[i] = torch.cat(scores[i], 0)\n    self.full_scores = []\n    return torch.mm(scores[0], scores[1].t())\n\n", "CodeBleu_score": 36.937030793522716}, {"code": "def _aggregate_scores(self, scores)\n\t\n    assert len(scores) == 2\n    self.full_scores = torch.cat(scores, dim=0)\n    return torch.mm(self.full_scores[0:self.n_videos], self.full_scores[self.n_videos:].t())\n\n", "CodeBleu_score": 31.076461089858608}]}
{"instance_id": "e4e1861fc357", "generation_result": [{"code": "def finalize(self, Y_pred, Y_true, output_file=None)\n\t\n\n    Y_pred = torch.cat(Y_pred).cpu().detach().numpy()\n    Y_true = torch.cat(Y_true).cpu().detach().numpy()\n\n    if output_file is not None:\n        with open(output_file, \"wb\") as f:\n            pickle.dump([Y_pred, Y_true], f)\n\n    return {\"Y_pred\": Y_pred, \"Y_true\": Y_true}\n\n", "CodeBleu_score": 23.94005605611464}, {"code": "def finalize(self, Y_pred, Y_true, output_file=None)\n\t\n    Y_pred = torch.cat(Y_pred, 0).cpu().numpy()\n    Y_true = torch.cat(Y_true, 0).cpu().numpy()\n\n    if output_file:\n        if os.path.isfile(output_file):\n            os.remove(output_file)\n        with open(output_file, \"wb\") as f:\n            pickle.dump((Y_pred, Y_true), f)\n\n    return {\"Y_pred\": Y_pred, \"Y_true\": Y_true}\n\n", "CodeBleu_score": 26.485849271364405}, {"code": "def finalize(self, Y_pred, Y_true, output_file=None)\n\t\n\n    Y_pred = torch.cat(Y_pred, dim=0).cpu().numpy()\n    Y_true = torch.cat(Y_true, dim=0).cpu().numpy()\n\n    if output_file is not None:\n        with open(output_file, \"wb\") as f:\n            pickle.dump((Y_pred, Y_true), f)\n\n    return {\"Y_pred\": Y_pred, \"Y_true\": Y_true}\n\n", "CodeBleu_score": 25.839786411799032}]}
{"instance_id": "7df9ab9a4920", "generation_result": [{"code": "def reduce_metrics(logging_outputs) -> None\n\t\n    loss_sum = sum(log.get('loss', 0) for log in logging_outputs)\n    sample_size = sum(log.get('sample_size', 0) for log in logging_outputs)\n\n    metrics.log_scalar('loss', loss_sum / sample_size, sample_size, round=3)\n    metrics.log_derived('ppl', lambda meters: utils.get_perplexity(meters['loss'].avg))\n\n", "CodeBleu_score": 48.19936533971999}, {"code": "def reduce_metrics(logging_outputs) -> None\n\t\n    loss_sum = sum(log.get(\"loss\", 0) for log in logging_outputs)\n    sample_size = sum(log.get(\"sample_size\", 0) for log in logging_outputs)\n    metrics.log_scalar(\"loss\", loss_sum / sample_size, sample_size, round=3)\n\n", "CodeBleu_score": 62.429646714521894}, {"code": "def reduce_metrics(logging_outputs) -> None\n\t\n    loss_sum = sum(log.get(\"loss\", 0) for log in logging_outputs)\n    nll_loss_sum = sum(log.get(\"nll_loss\", 0) for log in logging_outputs)\n    sample_size = sum(log.get(\"sample_size\", 0) for log in logging_outputs)\n\n    metrics.log_scalar(\"loss\", loss_sum / sample_size / math.log(2), sample_size, round=3)\n    metrics.log_scalar(\"nll_loss\", nll_loss_sum / sample_size / math.log(2), nll_loss_sum, round=3)\n    metrics.log_derived(\"ppl\", lambda meters: utils.get_perplexity(meters[\"nll_loss\"].avg))\n\n", "CodeBleu_score": 53.922162989815845}]}
{"instance_id": "7f9c326a96f7", "generation_result": [{"code": "def _mm_on_the_fly(\n        self,\n        cmasks,\n        vmasks,\n        attention_mask\n    )\n\t\n\n    # get batch size and sequence length\n    batch_size, seq_len = cmasks.shape\n\n    # generate attention mask\n    if attention_mask is None:\n        attention_mask = torch.cat(\n            (\n                torch.ones(batch_size, seq_len, device=self.device),\n                torch.zeros(batch_size, seq_len, device=self.device)\n            ),\n            dim=-1\n        )\n        attention_mask = torch.where(\n            torch.cat(\n                (\n                    torch.ones(batch_size, seq_len, device=self.device),\n                    vmasks\n                ),\n                dim=-1\n            ) == 1,\n            torch.zeros(batch_size, seq_len, device=self.device),\n            attention_mask\n        )\n\n    # generate token type IDs\n    token_type_ids = torch.cat(\n        (\n            torch.zeros(batch_size, seq_len, device=self.device),\n            torch.ones(batch_size, seq_len, device=self.device)\n        ),\n        dim=-1\n    )\n\n    return attention_mask, token_type_ids\n\n", "CodeBleu_score": 38.238393974392146}, {"code": "def _mm_on_the_fly(\n        self,\n        cmasks,\n        vmasks,\n        attention_mask\n    )\n\t\n    if attention_mask is None:\n        attention_mask = torch.cat(\n            (\n                torch.ones(\n                    (\n                        cmasks.shape[0],\n                        cmasks.shape[1]\n                    ),\n                    dtype=torch.long\n                ),\n                torch.zeros(\n                    (\n                        cmasks.shape[0],\n                        vmasks.shape[1]\n                    ),\n                    dtype=torch.long\n                )\n            ),\n            dim=-1\n        )\n        attention_mask = (\n            (attention_mask == 0)\n            | (\n                (attention_mask == 1)\n                & (\n                    (\n                        cmasks.bool()\n                        & vmasks.bool()\n                    )\n                    .any(dim=1)\n                )\n            )\n        ).long()\n    token_type_ids = torch.cat(\n        (\n            torch.zeros(\n                (\n                    cmasks.shape[0],\n                    cmasks.shape[1]\n                ),\n                dtype=torch.long\n            ),\n            torch.ones(\n                (\n                    cmasks.shape[0],\n                    vmasks.shape[1]\n                ),\n                dtype=torch.long\n            )\n        ),\n        dim=-1\n    )\n    return attention_mask, token_type_ids\n\n", "CodeBleu_score": 38.92783432266411}, {"code": "def _mm_on_the_fly(\n        self,\n        cmasks,\n        vmasks,\n        attention_mask\n    )\n\t\n    if attention_mask is not None:\n        return attention_mask, None\n\n    # Create attention mask\n    # --------------------\n    # The attention mask is a binary matrix of shape (batch_size, N) where N = len(cmasks) + len(vmasks).\n    # The first len(cmasks) columns of the matrix are 1s if the corresponding caption token is not a padding token.\n    # The remaining columns are 1s if the corresponding visual token is not a padding token.\n    attention_mask = torch.cat(\n        [\n            torch.cat(cmasks, dim=1),\n            torch.cat(vmasks, dim=1),\n        ],\n        dim=1\n    )\n    attention_mask = (attention_mask > 0).long()\n\n    # Create token type IDs\n    # ---------------------\n    # The token type IDs are a binary matrix of shape (batch_size, N) where N = len(cmasks) + len(vmasks).\n    # The first len(cmasks) columns of the matrix are 1s.\n    # The remaining columns are 0s.\n    token_type_ids = torch.cat(\n        [\n            torch.ones(\n                (attention_mask.shape[0], len(cmasks)),\n                dtype=attention_mask.dtype,\n                device=attention_mask.device\n            ),\n            torch.zeros(\n                (attention_mask.shape[0], len(vmasks)),\n                dtype=attention_mask.dtype,\n                device=attention_mask.device\n            ),\n        ],\n        dim=1\n    )\n\n    return attention_mask, token_type_ids\n\n", "CodeBleu_score": 46.00138679352763}]}
{"instance_id": "fa99d2a148a5", "generation_result": [{"code": "def prepare_inputs_for_generation(\n        self,\n        input_ids,\n        input_video_embeds,\n        attention_mask=None,\n        token_type_ids=None,\n        **model_kwargs\n    )\n\t\n    if input_video_embeds is not None:\n        input_ids = torch.cat([input_ids, torch.zeros_like(input_video_embeds).to(torch.long)], dim=1)\n        input_video_embeds = torch.cat([input_video_embeds, torch.zeros_like(input_ids)], dim=1)\n        attention_mask = torch.cat([attention_mask, torch.ones_like(input_video_embeds[:, :, 0]).to(torch.long)], dim=1)\n        if token_type_ids is not None:\n            token_type_ids = torch.cat([token_type_ids, torch.zeros_like(input_video_embeds[:, :, 0]).to(torch.long)], dim=1)\n    else:\n        input_ids = torch.cat([input_ids, torch.zeros_like(input_ids)], dim=1)\n        input_video_embeds = torch.cat([torch.zeros_like(input_ids), torch.zeros_like(input_ids)], dim=1)\n        attention_mask = torch.cat([attention_mask, torch.ones_like(input_ids[:, :, 0]).to(torch.long)], dim=1)\n        if token_type_ids is not None:\n            token_type_ids = torch.cat([token_type_ids, torch.zeros_like(input_ids[:, :, 0]).to(torch.long)], dim=1)\n\n    return {\n        \"input_ids\": input_ids,\n        \"input_video_embeds\": input_video_embeds,\n        \"attention_mask\": attention_mask,\n        \"token_type_ids\": token_type_ids,\n    }\n\n", "CodeBleu_score": 46.01147352244585}, {"code": "def prepare_inputs_for_generation(\n        self,\n        input_ids,\n        input_video_embeds,\n        attention_mask=None,\n        token_type_ids=None,\n        **model_kwargs\n    )\n\t\n    if input_ids is not None and input_video_embeds is not None:\n        input_ids = input_ids[:, -self.max_text_len:]\n        input_video_embeds = input_video_embeds[:, -self.max_video_len:]\n        attention_mask = attention_mask[:, -self.max_text_len-self.max_video_len:]\n        token_type_ids = token_type_ids[:, -self.max_text_len-self.max_video_len:]\n        input_ids = torch.cat((input_ids, input_video_embeds), dim=1)\n        attention_mask = torch.cat((attention_mask, attention_mask), dim=1)\n        token_type_ids = torch.cat((token_type_ids, token_type_ids), dim=1)\n    elif input_ids is not None:\n        input_ids = input_ids[:, -self.max_text_len:]\n        attention_mask = attention_mask[:, -self.max_text_len:]\n        token_type_ids = token_type_ids[:, -self.max_text_len:]\n    elif input_video_embeds is not None:\n        input_video_embeds = input_video_embeds[:, -self.max_video_len:]\n        attention_mask = attention_mask[:, -self.max_video_len:]\n        token_type_ids = token_type_ids[:, -self.max_video_len:]\n        input_ids = input_video_embeds\n        attention_mask = torch.cat((attention_mask, attention_mask), dim=1)\n        token_type_ids = torch.cat((token_type_ids, token_type_ids), dim=1)\n    else:\n        raise ValueError(\"You have to specify either input_ids or input_video_embeds\")\n    return {\n        \"input_ids\": input_ids,\n        \"attention_mask\": attention_mask,\n        \"token_type_ids\": token_type_ids,\n    }\n\n", "CodeBleu_score": 37.63871825952558}, {"code": "def prepare_inputs_for_generation(\n        self,\n        input_ids,\n        input_video_embeds,\n        attention_mask=None,\n        token_type_ids=None,\n        **model_kwargs\n    )\n\t\n\n    # make masks if user doesn't supply\n    if attention_mask is None:\n        attention_mask = torch.ones_like(input_ids)\n    if token_type_ids is None:\n        token_type_ids = torch.zeros_like(input_ids)\n\n    # make sure that input_video_embeds and input_ids have the same sequence length\n    if input_video_embeds is not None and input_video_embeds.shape[1] != input_ids.shape[1]:\n        if input_video_embeds.shape[1] < input_ids.shape[1]:\n            input_video_embeds = input_video_embeds.expand(-1, input_ids.shape[1], -1)\n        else:\n            input_ids = input_ids.expand(-1, input_video_embeds.shape[1])\n            attention_mask = attention_mask.expand(-1, input_video_embeds.shape[1])\n            token_type_ids = token_type_ids.expand(-1, input_video_embeds.shape[1])\n\n    # make sure that input_video_embeds and input_ids have the same batch size\n    if input_video_embeds is not None and input_video_embeds.shape[0] != input_ids.shape[0]:\n        if input_video_embeds.shape[0] < input_ids.shape[0]:\n            input_video_embeds = input_video_embeds.expand(input_ids.shape[0], -1, -1)\n        else:\n            input_ids = input_ids.expand(input_video_embeds.shape[0], -1)\n            attention_mask = attention_mask.expand(input_video_embeds.shape[0], -1)\n            token_type_ids = token_type_ids.expand(input_video_embeds.shape[0], -1)\n\n    # make sure that input_video_embeds and input_ids have the same device\n    if input_video_embeds is not None and input_video_embeds.device != input_ids.device:\n        input_video", "CodeBleu_score": 26.927258428676247}]}
{"instance_id": "b1b0d25fe620", "generation_result": [{"code": "def _init_pipeline(self, cfg: ConfigType) -> Compose\n\t\n    # remove img_id from meta_keys\n    meta_keys = cfg.get('meta_keys', None)\n    if meta_keys is not None and 'img_id' in meta_keys:\n        meta_keys.remove('img_id')\n\n    # find the index of the LoadImageFromFile transform\n    load_idx = -1\n    for idx, transform in enumerate(cfg.transforms):\n        if transform['type'] == 'LoadImageFromFile':\n            load_idx = idx\n            break\n    if load_idx == -1:\n        raise RuntimeError('Cannot find LoadImageFromFile transform in '\n                           'pipeline configuration')\n\n    # update the transform type to InferencerLoader\n    cfg.transforms[load_idx]['type'] = 'mmdet.InferencerLoader'\n\n    # initialize the pipeline\n    pipeline = Compose(cfg.transforms)\n\n    return pipeline\n\n", "CodeBleu_score": 32.50812159821512}, {"code": "def _init_pipeline(self, cfg: ConfigType) -> Compose\n\t\n    pipeline = copy.deepcopy(cfg.test_pipeline)\n    if 'img_id' in pipeline[0].get('meta_keys', []):\n        pipeline[0]['meta_keys'].remove('img_id')\n    # find the index of the 'LoadImageFromFile' transform\n    for idx, transform in enumerate(pipeline):\n        if transform['type'] == 'LoadImageFromFile':\n            break\n    else:\n        raise ValueError(\n            'Cannot find the \"LoadImageFromFile\" transform in the pipeline')\n    pipeline[idx]['type'] = 'mmdet.InferencerLoader'\n    return Compose(pipeline)\n\n", "CodeBleu_score": 30.269470851091}, {"code": "def _init_pipeline(self, cfg: ConfigType) -> Compose\n\t\n    pipeline = copy.deepcopy(cfg.pipeline)\n    if 'img_id' in pipeline[0]['meta_keys']:\n        pipeline[0]['meta_keys'].remove('img_id')\n    if 'LoadImageFromFile' not in pipeline[0]['type']:\n        raise TypeError(f\"The first transform in the pipeline must be of type 'LoadImageFromFile', found '{pipeline[0]['type']}'\")\n    pipeline[0]['type'] = 'mmdet.InferencerLoader'\n    return Compose(pipeline)\n\n", "CodeBleu_score": 23.694222625824654}]}
{"instance_id": "7967962856eb", "generation_result": [{"code": "def _inputs_to_list(self, inputs: InputsType) -> list\n\t\n    if isinstance(inputs, str) and isdir(inputs):\n        inputs = list_dir_or_file(inputs)\n    if not isinstance(inputs, (list, tuple)):\n        inputs = [inputs]\n    return inputs\n\n", "CodeBleu_score": 35.31349002348356}, {"code": "def _inputs_to_list(self, inputs: InputsType) -> list\n\t\n    if isinstance(inputs, str):\n        if isdir(inputs):\n            inputs = list_dir_or_file(inputs)\n        else:\n            inputs = [inputs]\n    if not isinstance(inputs, (list, tuple)):\n        inputs = [inputs]\n    return inputs\n\n", "CodeBleu_score": 39.20852424152697}, {"code": "def _inputs_to_list(self, inputs: InputsType) -> list\n\t\n    if isinstance(inputs, str) and isdir(inputs):\n        inputs = list_dir_or_file(inputs, suffix=\".jpg\", recursive=True)\n    if not isinstance(inputs, (list, tuple)):\n        inputs = [inputs]\n    return inputs\n\n", "CodeBleu_score": 39.92441045894952}]}
{"instance_id": "74f727f06fa0", "generation_result": [{"code": "def _get_chunk_data(self, inputs: Iterable, chunk_size: int)\n\t\n    chunk = []\n    for input in inputs:\n        if isinstance(input, dict) and ('img' in input or 'img_path' in input):\n            input = self.pipeline(input)\n        chunk.append(input)\n        if len(chunk) == chunk_size:\n            yield chunk\n            chunk = []\n    if chunk:\n        yield chunk\n\n", "CodeBleu_score": 23.59926429365173}, {"code": "def _get_chunk_data(self, inputs: Iterable, chunk_size: int)\n\t\n    chunk = []\n    for input in inputs:\n        if isinstance(input, dict):\n            if 'img' in input:\n                input = input['img']\n            elif 'img_path' in input:\n                input = input['img_path']\n            else:\n                raise ValueError(\n                    'The input data must contain the `img` or `img_path` key')\n        chunk.append(input)\n        if len(chunk) == chunk_size:\n            yield chunk\n            chunk = []\n    if len(chunk) > 0:\n        yield chunk\n\n", "CodeBleu_score": 35.04387029036339}, {"code": "def _get_chunk_data(self, inputs: Iterable, chunk_size: int)\n\t\n    chunk_data = []\n    for data in inputs:\n        if isinstance(data, dict):\n            if 'img' in data:\n                data = data['img']\n            elif 'img_path' in data:\n                data = data['img_path']\n            else:\n                raise TypeError(\n                    'The input data is expected to be a dictionary containing an \"img\" or \"img_path\" key, but got {}'\n                    .format(data))\n        chunk_data.append(data)\n        if len(chunk_data) == chunk_size:\n            yield chunk_data\n            chunk_data = []\n    if len(chunk_data) > 0:\n        yield chunk_data\n\n", "CodeBleu_score": 37.98004267871511}]}
{"instance_id": "a43ab51afa59", "generation_result": [{"code": "def postprocess(\n        self,\n        preds: PredType,\n        visualization: Optional[List[np.ndarray]] = None,\n        return_datasamples: bool = False,\n        print_result: bool = False,\n        no_save_pred: bool = False,\n        pred_out_dir: str = '',\n        **kwargs,\n    ) -> Dict\n\t\n    if not no_save_pred:\n        if not isdir(pred_out_dir):\n            os.makedirs(pred_out_dir)\n\n    if return_datasamples:\n        warnings.warn('Saving datasamples is not supported.')\n\n    results = dict()\n\n    if visualization is not None:\n        results['visualization'] = visualization\n\n    if return_datasamples:\n        results['datasamples'] = preds\n    else:\n        results['preds'] = []\n        for i, pred in enumerate(preds):\n            if self.cfg.get('save_pred_to_file', False):\n                pred_out_file = join_path(pred_out_dir, f'pred_{i}.pkl')\n                mmcv.dump(pred, pred_out_file)\n            results['preds'].append(pred.to_dict())\n\n    if print_result:\n        print(results)\n\n    return results\n\n", "CodeBleu_score": 48.21180883079834}, {"code": "def postprocess(\n        self,\n        preds: PredType,\n        visualization: Optional[List[np.ndarray]] = None,\n        return_datasamples: bool = False,\n        print_result: bool = False,\n        no_save_pred: bool = False,\n        pred_out_dir: str = '',\n        **kwargs,\n    ) -> Dict\n\t\n    if not no_save_pred and not pred_out_dir:\n        pred_out_dir = self.work_dir\n    if return_datasamples and pred_out_dir:\n        warnings.warn('Saving datasamples is not supported.')\n    results = dict()\n    if not no_save_pred:\n        results['pred_out_dir'] = pred_out_dir\n    if visualization is not None:\n        results['visualization'] = visualization\n    if not return_datasamples:\n        results['preds'] = []\n        for pred in preds:\n            results['preds'].append(pred.to_dict())\n    if print_result:\n        self.print_result(results)\n    return results\n\n", "CodeBleu_score": 50.45561647918}, {"code": "def postprocess(\n        self,\n        preds: PredType,\n        visualization: Optional[List[np.ndarray]] = None,\n        return_datasamples: bool = False,\n        print_result: bool = False,\n        no_save_pred: bool = False,\n        pred_out_dir: str = '',\n        **kwargs,\n    ) -> Dict\n\t\n    if not no_save_pred:\n        if not pred_out_dir:\n            pred_out_dir = self.cfg.work_dir\n        else:\n            pred_out_dir = join_path(self.cfg.work_dir, pred_out_dir)\n        if not isdir(pred_out_dir):\n            mmcv.mkdir_or_exist(pred_out_dir)\n    results = dict()\n    if return_datasamples:\n        if pred_out_dir:\n            warnings.warn(\n                'Saving datasamples is not supported. Returning datasamples.')\n        results['datasamples'] = preds\n    else:\n        results['preds'] = []\n        for i, pred in enumerate(preds):\n            result = self.model.module.show_result(\n                pred,\n                visualization[i],\n                dataset=self.dataset,\n                score_thr=self.cfg.score_thr,\n                **kwargs,\n            )\n            results['preds'].append(result)\n    if visualization:\n        results['visualization'] = visualization\n    if print_result:\n        self.print_result(results)\n    if not no_save_pred:\n        self.save_pred(results, pred_out_dir)\n    return results\n\n", "CodeBleu_score": 43.88880910170171}]}
{"instance_id": "a3c2561f199c", "generation_result": [{"code": "def build_test_pipeline(cfg: ConfigType) -> ConfigType\n\t\n    pipeline = copy.deepcopy(cfg.data.test.pipeline)\n    pipeline[0] = pipeline[0][0]\n    pipeline[0][\"transforms\"] = [pipeline[0][\"transforms\"][0]]\n    pipeline[0][\"transforms\"][0][\"keep_ratio\"] = False\n    pipeline[-1] = pipeline[-1][-1]\n    pipeline = Compose(pipeline)\n    return pipeline\n\n", "CodeBleu_score": 19.105892092148483}, {"code": "def build_test_pipeline(cfg: ConfigType) -> ConfigType\n\t\n    test_pipeline = copy.deepcopy(cfg.data.test.pipeline)\n    test_pipeline[0] = test_pipeline[0].copy()\n    test_pipeline[0].pop('type')\n    test_pipeline[0]['type'] = 'Resize'\n    test_pipeline.insert(-1, test_pipeline[-1].copy())\n    test_pipeline = Compose(test_pipeline)\n    return test_pipeline\n\n", "CodeBleu_score": 22.644691349612618}, {"code": "def build_test_pipeline(cfg: ConfigType) -> ConfigType\n\t\n    test_pipeline = []\n    dataset_pipeline = copy.deepcopy(cfg.data.test.pipeline)\n    if cfg.data.test.pipeline[0]['type'] != 'LoadImageFromFile':\n        raise ValueError('The first transform in the test pipeline must be LoadImageFromFile')\n    if cfg.data.test.pipeline[-1]['type'] != 'MultiScaleFlipAug':\n        raise ValueError('The last transform in the test pipeline must be MultiScaleFlipAug')\n    resize_transform = dataset_pipeline[0]\n    resize_transform['transforms'] = [resize_transform['transforms'][0]]\n    test_pipeline.append(resize_transform)\n    test_pipeline.append(dataset_pipeline[-1])\n    return Compose(test_pipeline)\n\n", "CodeBleu_score": 28.214750367996643}]}
{"instance_id": "48a358ab5a38", "generation_result": [{"code": "def inference_mot(model: nn.Module, img: np.ndarray, frame_id: int,\n                  video_len: int) -> SampleList\n\t\n    input_dict = {\n        'img': img,\n        'frame_id': frame_id,\n        'original_shape': img.shape,\n        'img_id': frame_id,\n        'video_len': video_len\n    }\n    test_pipeline = model.test_pipeline\n    if torch.cuda.is_available():\n        device = 'cuda:0'\n    else:\n        device = 'cpu'\n        if isinstance(test_pipeline, Compose):\n            test_pipeline = copy.deepcopy(test_pipeline)\n            for t in test_pipeline.transforms:\n                if isinstance(t, RoIPool):\n                    warnings.warn(\n                        'CPU inference with RoIPool is not supported')\n                    test_pipeline.transforms.remove(t)\n    if isinstance(test_pipeline, Compose):\n        test_pipeline = Compose(test_pipeline.transforms)\n    data = test_pipeline(input_dict)\n    data = default_collate([data])\n    data = data.to(device)\n    with torch.no_grad():\n        result = model(data['img'], data['img_metas'])\n    return result\n\n", "CodeBleu_score": 45.13520412981811}, {"code": "def inference_mot(model: nn.Module, img: np.ndarray, frame_id: int,\n                  video_len: int) -> SampleList\n\t\n    data = dict(\n        img=img,\n        frame_id=frame_id,\n        original_shape=img.shape,\n        img_id=frame_id,\n        video_len=video_len,\n    )\n    cfg = model.cfg\n    device = next(model.parameters()).device  # model device\n    test_pipeline = Compose(cfg.test_pipeline)\n    data = test_pipeline(data)\n    data = collate([data], samples_per_gpu=1)\n    if next(model.parameters()).is_cuda:\n        # scatter to specified GPU\n        data = scatter(data, [device])[0]\n    else:\n        for m in model.modules():\n            assert not isinstance(\n                m, RoIPool\n            ), 'CPU inference with RoIPool is not supported currently.'\n        warnings.warn(\n            'The model is on CPU. It would be significantly faster if you put it on GPU.'\n        )\n    # forward the model\n    with torch.no_grad():\n        result = model(return_loss=False, rescale=True, **data)\n    return result\n\n", "CodeBleu_score": 50.66610559314413}, {"code": "def inference_mot(model: nn.Module, img: np.ndarray, frame_id: int,\n                  video_len: int) -> SampleList\n\t\n    if isinstance(img, np.ndarray):\n        img = torch.from_numpy(img)\n    data = {\n        'img': img,\n        'frame_id': frame_id,\n        'original_shape': img.shape,\n        'img_info': {\n            'video_len': video_len\n        }\n    }\n    cfg = model.cfg\n    device = next(model.parameters()).device  # model device\n    cfg = model.cfg\n    # prepare data\n    data = model.cfg.data.test.pipeline(data)\n    data = collate([data], samples_per_gpu=1)\n    if next(model.parameters()).is_cuda:\n        # scatter to specified GPU\n        data = scatter(data, [device])[0]\n    else:\n        # remove sync_batchnorm\n        model = revert_sync_batchnorm(model)\n    # forward the model\n    with torch.no_grad():\n        result = model(return_loss=False, rescale=True, **data)\n    return result\n\n", "CodeBleu_score": 49.515034991456034}]}
{"instance_id": "6c4ea2ce1787", "generation_result": [{"code": "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\n\t\n\n    while True:\n        total_length = len(tokens_a) + len(tokens_b)\n        if total_length <= max_num_tokens:\n            break\n\n        trunc_tokens = tokens_a if len(tokens_a) > len(tokens_b) else tokens_b\n        assert len(trunc_tokens) >= 1\n\n        # We want to sometimes truncate from the front and sometimes from the\n        # back to add more randomness and avoid biases.\n        if rng.random() < 0.5:\n            del trunc_tokens[0]\n        else:\n            trunc_tokens.pop()\n\n", "CodeBleu_score": 78.49275840258514}, {"code": "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\n\t\n\n    while True:\n        total_length = len(tokens_a) + len(tokens_b)\n        if total_length <= max_num_tokens:\n            break\n\n        trunc_tokens = tokens_a if len(tokens_a) > len(tokens_b) else tokens_b\n        assert len(trunc_tokens) >= 1\n\n        # We want to sometimes truncate from the front and sometimes from the\n        # back to add more randomness and avoid biases.\n        if rng.random() < 0.5:\n            del trunc_tokens[0]\n        else:\n            trunc_tokens.pop()\n\n", "CodeBleu_score": 78.49275840258514}, {"code": "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\n\t\n    while True:\n        total_length = len(tokens_a) + len(tokens_b)\n        if total_length <= max_num_tokens:\n            break\n\n        trunc_tokens = tokens_a if len(tokens_a) > len(tokens_b) else tokens_b\n        assert len(trunc_tokens) >= 1\n\n        # We want to sometimes truncate from the front and sometimes from the\n        # back to add more randomness and avoid biases.\n        if rng.random() < 0.5:\n            del trunc_tokens[0]\n        else:\n            trunc_tokens.pop()\n\n", "CodeBleu_score": 78.49275840258514}]}
{"instance_id": "007181b6729d", "generation_result": [{"code": "def get_activation(activation_string)\n\t\n    if not isinstance(activation_string, six.string_types):\n        return activation_string\n\n    if not activation_string:\n        return None\n\n    act = activation_string.lower()\n    if act == 'linear':\n        return None\n    elif act == 'relu':\n        return tf.nn.relu\n    elif act == 'leaky_relu':\n        return tf.nn.leaky_relu\n    elif act == 'elu':\n        return tf.nn.elu\n    elif act == 'tanh':\n        return tf.tanh\n    elif act == 'sigmoid':\n        return tf.sigmoid\n    elif act == 'softmax':\n        return tf.nn.softmax\n    else:\n        raise ValueError('Unsupported activation function: ' + act)\n\n", "CodeBleu_score": 75.26343751951026}, {"code": "def get_activation(activation_string)\n\t\n    if not isinstance(activation_string, six.string_types):\n        return activation_string\n\n    activation_string = activation_string.lower()\n\n    if not activation_string:\n        return None\n\n    if activation_string == 'linear':\n        return None\n    elif activation_string == 'relu':\n        from keras.layers import ReLU\n        return ReLU\n    elif activation_string == 'elu':\n        from keras.layers import ELU\n        return ELU\n    elif activation_string == 'selu':\n        from keras.layers import SELU\n        return SELU\n    elif activation_string == 'tanh':\n        from keras.layers import Activation\n        return Activation('tanh')\n    elif activation_string == 'sigmoid':\n        from keras.layers import Activation\n        return Activation('sigmoid')\n    elif activation_string == 'hard_sigmoid':\n        from keras.layers import Activation\n        return Activation('hard_sigmoid')\n    elif activation_string == 'exponential':\n        from keras.layers import Activation\n        return Activation('exponential')\n    elif activation_string == 'softmax':\n        from keras.layers import Activation\n        return Activation('softmax')\n    elif activation_string == 'softplus':\n        from keras.layers import Activation\n        return Activation('softplus')\n    elif activation_string == 'softsign':\n        from keras.layers import Activation\n        return Activation('softsign')\n    elif activation_string == 'relu6':\n        from keras.layers import ReLU\n        return ReLU(max_value=6)\n    elif activation_string == 'crelu':\n        from keras.layers import Activation\n        return Activation('crelu')\n    elif activation_string == 'leakyrelu':\n        from keras.layers import LeakyReLU\n        return LeakyReLU\n    elif activation_string == 'prelu':\n        from keras.layers import PReLU\n        return PReLU\n    elif activation_string == 'swish':\n", "CodeBleu_score": 39.750170847666496}, {"code": "def get_activation(activation_string)\n\t\n    if not isinstance(activation_string, six.string_types):\n        return activation_string\n\n    if not activation_string:\n        return None\n\n    act = activation_string.lower()\n    if act == \"linear\":\n        return None\n    elif act == \"relu\":\n        return tf.nn.relu\n    elif act == \"leaky_relu\":\n        return tf.nn.leaky_relu\n    elif act == \"elu\":\n        return tf.nn.elu\n    elif act == \"selu\":\n        return tf.nn.selu\n    elif act == \"tanh\":\n        return tf.tanh\n    elif act == \"sigmoid\":\n        return tf.sigmoid\n    elif act == \"hard_sigmoid\":\n        return hard_sigmoid\n    else:\n        raise ValueError(\"Unsupported activation:\", activation_string)\n\n", "CodeBleu_score": 77.93344197163266}]}
{"instance_id": "d5d6e0315899", "generation_result": [{"code": "def dropout(input_tensor, dropout_prob)\n\t\n    if dropout_prob is None or dropout_prob == 0.0:\n        return input_tensor\n\n    output = tf.nn.dropout(input_tensor, rate=dropout_prob)\n    return output\n\n", "CodeBleu_score": 70.84479337256124}, {"code": "def dropout(input_tensor, dropout_prob)\n\t\n    if dropout_prob is None or dropout_prob == 0.0:\n        return input_tensor\n\n    output = tf.nn.dropout(input_tensor, rate=dropout_prob)\n    return output\n\n", "CodeBleu_score": 70.84479337256124}, {"code": "def dropout(input_tensor, dropout_prob)\n\t\n    if dropout_prob is None or dropout_prob == 0.0:\n        return input_tensor\n\n    output = tf.nn.dropout(input_tensor, rate=dropout_prob)\n    return output\n\n", "CodeBleu_score": 70.84479337256124}]}
{"instance_id": "828917b9ac53", "generation_result": [{"code": "def create_attention_mask_from_input_mask(from_tensor, to_mask)\n\t\n    from_shape = get_shape_list(from_tensor, expected_rank=[2, 3])\n    batch_size = from_shape[0]\n    from_seq_length = from_shape[1]\n\n    to_shape = get_shape_list(to_mask, expected_rank=2)\n    to_seq_length = to_shape[1]\n\n    to_mask = tf.cast(tf.reshape(to_mask, [batch_size, 1, to_seq_length]), tf.float32)\n\n    # We don't assume that `from_tensor` is a mask (although it could be). We\n    # don't actually care if we attend *from* padding tokens (only *to* padding)\n    # tokens so we create a tensor of all ones.\n    #\n    # `broadcast_ones` = [batch_size, from_seq_length, 1]\n    broadcast_ones = tf.ones(shape=[batch_size, from_seq_length, 1], dtype=tf.float32)\n\n    # Here we broadcast along two dimensions to create the mask.\n    mask = broadcast_ones * to_mask\n\n    return mask\n\n", "CodeBleu_score": 60.166084759694584}, {"code": "def create_attention_mask_from_input_mask(from_tensor, to_mask)\n\t\n    from_shape = get_shape_list(from_tensor, expected_rank=[2, 3])\n    batch_size = from_shape[0]\n    from_seq_length = from_shape[1]\n\n    to_shape = get_shape_list(to_mask, expected_rank=2)\n    to_seq_length = to_shape[1]\n\n    to_mask = tf.cast(tf.reshape(to_mask, [batch_size, 1, to_seq_length]), tf.float32)\n\n    # We don't assume that `from_tensor` is a mask (although it could be). We\n    # don't actually care if we attend *from* padding tokens (only *to* padding)\n    # tokens so we create a tensor of all ones.\n    #\n    # `broadcast_ones` = [batch_size, from_seq_length, 1]\n    broadcast_ones = tf.ones(shape=[batch_size, from_seq_length, 1], dtype=tf.float32)\n\n    # Here we broadcast along two dimensions to create the mask.\n    mask = broadcast_ones * to_mask\n\n    return mask\n\n", "CodeBleu_score": 60.166084759694584}, {"code": "def create_attention_mask_from_input_mask(from_tensor, to_mask)\n\t\n    from_shape = get_shape_list(from_tensor, expected_rank=[2, 3, 4])\n    batch_size = from_shape[0]\n    from_seq_length = from_shape[1]\n\n    to_shape = get_shape_list(to_mask, expected_rank=2)\n    to_seq_length = to_shape[1]\n\n    to_mask = tf.cast(tf.reshape(to_mask, [batch_size, 1, to_seq_length]), tf.float32)\n\n    # We don't assume that `from_tensor` is a mask (although it could be). We\n    # don't actually care if we attend *from* padding tokens (only *to* padding)\n    # tokens so we create a tensor of all ones.\n    #\n    # `broadcast_ones` = [batch_size, from_seq_length, 1]\n    broadcast_ones = tf.ones(shape=[batch_size, from_seq_length, 1], dtype=tf.float32)\n\n    # Here we broadcast along two dimensions to create the mask.\n    mask = broadcast_ones * to_mask\n\n    return mask\n\n", "CodeBleu_score": 58.278542193419206}]}
{"instance_id": "d7798e41a723", "generation_result": [{"code": "def transpose_for_scores(input_tensor, batch_size, num_attention_heads,\n                           seq_length, width)\n\t\n    input_tensor = tf.reshape(\n        input_tensor, [batch_size, seq_length, num_attention_heads, width])\n\n    return tf.transpose(input_tensor, [0, 2, 1, 3])\n\n", "CodeBleu_score": 49.719542738107044}, {"code": "def transpose_for_scores(input_tensor, batch_size, num_attention_heads,\n                           seq_length, width)\n\t\n    output_tensor = tf.reshape(\n        input_tensor, [batch_size, seq_length, num_attention_heads, width])\n\n    output_tensor = tf.transpose(output_tensor, [0, 2, 1, 3])\n    return output_tensor\n", "CodeBleu_score": 59.99845781533536}, {"code": "def transpose_for_scores(input_tensor, batch_size, num_attention_heads,\n                           seq_length, width)\n\t\n    input_tensor = tf.reshape(\n        input_tensor, [batch_size, seq_length, num_attention_heads, width])\n\n    return tf.transpose(input_tensor, [0, 2, 1, 3])\n\n", "CodeBleu_score": 49.719542738107044}]}
{"instance_id": "711d466ace09", "generation_result": [{"code": "def reshape_to_matrix(input_tensor)\n\t\n    if len(input_tensor.shape) < 2:\n        raise ValueError('Input tensor to reshape_to_matrix must have at least rank 2.')\n    if len(input_tensor.shape) == 2:\n        return input_tensor\n    else:\n        return tf.reshape(input_tensor, [-1, input_tensor.shape[-1]])\n\n", "CodeBleu_score": 34.509389629387734}, {"code": "def reshape_to_matrix(input_tensor)\n\t\n    if len(input_tensor.shape) < 2:\n        raise ValueError(\"Input tensor to reshape_to_matrix must have at least rank 2. Input tensor \"\n                         \"rank: {}\".format(len(input_tensor.shape)))\n    if len(input_tensor.shape) == 2:\n        return input_tensor\n    width = input_tensor.shape[-1]\n    return tf.reshape(input_tensor, [-1, width])\n\n", "CodeBleu_score": 49.510898374408484}, {"code": "def reshape_to_matrix(input_tensor)\n\t\n    if input_tensor.get_shape().ndims < 2:\n        raise ValueError('The input tensor must have at least rank 2.')\n    elif input_tensor.get_shape().ndims == 2:\n        return input_tensor\n    else:\n        return tf.reshape(input_tensor, [-1, input_tensor.get_shape().as_list()[-1]])\n\n", "CodeBleu_score": 33.794550914547784}]}
{"instance_id": "7adcd08f531e", "generation_result": [{"code": "def get_train_examples(self, data_dir)\n\t\n    train_data = []\n    with open(os.path.join(data_dir, \"train.tsv\")) as f:\n        reader = csv.reader(f, delimiter=\"\\t\")\n        header = next(reader)\n        for line in reader:\n            train_data.append(InputExample(guid=line[0], text_a=line[3], text_b=line[4], label=line[1]))\n    return train_data\n\n", "CodeBleu_score": 18.86901299875493}, {"code": "def get_train_examples(self, data_dir)\n\t\n    train_data = []\n    with open(os.path.join(data_dir, \"train.tsv\"), \"r\") as f:\n        reader = csv.reader(f, delimiter=\"\\t\")\n        next(reader)\n        for line in reader:\n            train_data.append(InputExample(guid=None, text_a=line[3], text_b=line[4], label=line[0]))\n    return train_data\n\n", "CodeBleu_score": 16.951636011973356}, {"code": "def get_train_examples(self, data_dir)\n\t\n    train_data = []\n    with open(os.path.join(data_dir, \"train.tsv\")) as f:\n        reader = csv.reader(f, delimiter=\"\\t\")\n        header = next(reader)\n        for line in reader:\n            train_data.append(InputExample(guid=line[0], text_a=line[3], text_b=line[4], label=line[1]))\n    return train_data\n\n", "CodeBleu_score": 18.86901299875493}]}
{"instance_id": "26a0bb96dab9", "generation_result": [{"code": "def metric_fn(per_example_loss, label_ids, logits, is_real_example)\n\t\n    predictions = []\n    for (i, logit) in enumerate(logits):\n        if i not in label_ids:\n            continue\n        if logit[label_ids[i]] == max(logit):\n            predictions.append(1)\n        else:\n            predictions.append(0)\n\n    eval_accuracy = sum(predictions) / len(predictions)\n    eval_loss = sum(per_example_loss) / len(per_example_loss)\n    return {\n        \"eval_accuracy\": eval_accuracy,\n        \"eval_loss\": eval_loss,\n    }\n\n", "CodeBleu_score": 28.738519295734143}, {"code": "def metric_fn(per_example_loss, label_ids, logits, is_real_example)\n\t\n    predictions = []\n    for (i, logit) in enumerate(logits):\n        if i not in is_real_example:\n            continue\n        predictions.append(logit)\n    predictions = np.argmax(predictions, axis=-1)\n    accuracy = np.sum(np.equal(predictions, label_ids)) / len(label_ids)\n    loss = np.sum(per_example_loss * is_real_example) / np.sum(is_real_example)\n    return {\n        \"eval_accuracy\": accuracy,\n        \"eval_loss\": loss,\n    }\n\n", "CodeBleu_score": 43.66451135990476}, {"code": "def metric_fn(per_example_loss, label_ids, logits, is_real_example)\n\t\n    predictions = []\n    for (i, logit) in enumerate(logits):\n        if i not in label_ids:\n            continue\n        predictions.append(\n            int(logit[label_ids[i]] >= 0.5)\n        )  # 0.5 is the threshold for positive sentiment\n    eval_accuracy = sum(\n        predictions[i] == label_ids[i] for i in range(len(predictions))\n    ) / len(predictions)\n    eval_loss = sum(per_example_loss[i] for i in range(len(predictions)))\n    return {\n        \"eval_accuracy\": eval_accuracy,\n        \"eval_loss\": eval_loss,\n    }\n\n", "CodeBleu_score": 30.029671498367144}]}
{"instance_id": "6154e4d28509", "generation_result": [{"code": "def convert_examples_to_features(examples, label_list, max_seq_length,\n                                 tokenizer)\n\t\n    features = []\n    for (ex_index, example) in enumerate(examples):\n        if ex_index % 10000 == 0:\n            print('Writing example %d of %d' % (ex_index, len(examples)))\n\n        feature = convert_single_example(ex_index, example, label_list,\n                                         max_seq_length, tokenizer)\n\n        features.append(feature)\n    return features\n\n", "CodeBleu_score": 70.77147918438398}, {"code": "def convert_examples_to_features(examples, label_list, max_seq_length,\n                                 tokenizer)\n\t\n    features = []\n    for (ex_index, example) in enumerate(examples):\n        if ex_index % 10000 == 0:\n            print(\"Writing example %d of %d\" % (ex_index, len(examples)))\n        feature = convert_single_example(ex_index, example, label_list, max_seq_length, tokenizer)\n        features.append(feature)\n    return features\n\n", "CodeBleu_score": 72.69382994957441}, {"code": "def convert_examples_to_features(examples, label_list, max_seq_length,\n                                 tokenizer)\n\t\n    features = []\n    for (ex_index, example) in enumerate(examples):\n        if ex_index % 10000 == 0:\n            print('Writing example %d of %d' % (ex_index, len(examples)))\n\n        feature = convert_single_example(ex_index, example, label_list,\n                                         max_seq_length, tokenizer)\n\n        features.append(feature)\n    return features\n\n", "CodeBleu_score": 70.77147918438398}]}
{"instance_id": "e09c0db71ebb", "generation_result": [{"code": "def get_next_sentence_output(bert_config, input_tensor, labels)\n\t\n    # STEP 1: Define the weights and bias for the output layer\n    with tf.variable_scope(\"cls/seq_relationship\"):\n        output_weights = tf.get_variable(\n            \"output_weights\",\n            shape=[2, bert_config.hidden_size],\n            initializer=modeling.create_initializer(bert_config.initializer_range))\n        output_bias = tf.get_variable(\n            \"output_bias\", shape=[2], initializer=tf.zeros_initializer())\n\n    # STEP 2: Compute logits\n    logits = tf.matmul(input_tensor, output_weights, transpose_b=True)\n    logits = tf.nn.bias_add(logits, output_bias)\n\n    # STEP 3: Convert labels to one-hot encoding\n    labels = tf.reshape(labels, [-1])\n    one_hot_labels = tf.one_hot(labels, depth=2, dtype=tf.float32)\n\n    # STEP 4: Compute per-example loss\n    per_example_loss = -tf.reduce_sum(one_hot_labels * tf.nn.log_softmax(logits, axis=-1), axis=-1)\n\n    # STEP 5: Compute the mean loss\n    loss = tf.reduce_mean(per_example_loss)\n\n    return (loss, per_example_loss, logits)\n\n", "CodeBleu_score": 80.76669573079775}, {"code": "def get_next_sentence_output(bert_config, input_tensor, labels)\n\t\n\n    # STEP 1: get the output weights and bias\n    with tf.variable_scope(\"cls/seq_relationship\"):\n        output_weights = tf.get_variable(\n            \"output_weights\",\n            shape=[2, bert_config.hidden_size],\n            initializer=modeling.create_initializer(bert_config.initializer_range))\n        output_bias = tf.get_variable(\n            \"output_bias\", shape=[2], initializer=tf.zeros_initializer())\n\n    # STEP 2: compute logits through matrix multiplication and bias addition\n    logits = tf.matmul(input_tensor, output_weights, transpose_b=True)\n    logits = tf.nn.bias_add(logits, output_bias)\n\n    # STEP 3: reshape labels and convert to one-hot encoding\n    logits = tf.reshape(logits, [-1, 2])\n    labels = tf.reshape(labels, [-1])\n    one_hot_labels = tf.one_hot(labels, depth=2, dtype=tf.float32)\n\n    # STEP 4: compute the per-example loss\n    per_example_loss = -tf.reduce_sum(one_hot_labels * tf.nn.log_softmax(logits, axis=-1), axis=-1)\n    loss = tf.reduce_mean(per_example_loss)\n\n    # STEP 5: compute the log probabilities\n    probabilities = tf.nn.softmax(logits, axis=-1)\n\n    return (loss, per_example_loss, logits, probabilities)\n\n", "CodeBleu_score": 78.86937934665747}, {"code": "def get_next_sentence_output(bert_config, input_tensor, labels)\n\t\n\n    # Sets the output weights. The weights are initialized from a normal distribution with mean = 0 and stddev = 1e-2.\n    with tf.variable_scope(\"cls/seq_relationship\"):\n        output_weights = tf.get_variable(\n            \"output_weights\",\n            shape=[2, bert_config.hidden_size],\n            initializer=modeling.create_initializer(bert_config.initializer_range))\n        output_bias = tf.get_variable(\n            \"output_bias\", shape=[2], initializer=tf.zeros_initializer())\n\n    # Gets logits through matrix multiplication and bias addition.\n    logits = tf.matmul(input_tensor, output_weights, transpose_b=True)\n    logits = tf.nn.bias_add(logits, output_bias)\n\n    # Reshapes logits and labels to get per-example loss.\n    logits = tf.reshape(logits, [-1, 2])\n    labels = tf.reshape(labels, [-1])\n\n    # Converts labels to one-hot encoding.\n    one_hot_labels = tf.one_hot(labels, depth=2, dtype=tf.float32)\n\n    # Computes per-example loss.\n    per_example_loss = -tf.reduce_sum(one_hot_labels * tf.nn.log_softmax(logits, axis=-1), axis=-1)\n\n    # Computes the mean loss.\n    loss = tf.reduce_mean(per_example_loss)\n\n    # Computes log probabilities.\n    log_probs = tf.nn.log_softmax(logits, axis=-1)\n\n    return (loss, per_example_loss, log_probs)\n\n", "CodeBleu_score": 81.18600539705028}]}
{"instance_id": "1f66b020f485", "generation_result": [{"code": "def gather_indexes(sequence_tensor, positions)\n\t\n    sequence_shape = modeling.get_shape_list(sequence_tensor, expected_rank=3)\n    batch_size = sequence_shape[0]\n    seq_length = sequence_shape[1]\n    width = sequence_shape[2]\n\n    flat_offsets = tf.reshape(tf.range(0, batch_size, dtype=tf.int32) * seq_length, [-1, 1])\n    flat_positions = tf.reshape(positions + flat_offsets, [-1])\n    flat_sequence_tensor = tf.reshape(sequence_tensor, [batch_size * seq_length, width])\n    output_tensor = tf.gather(flat_sequence_tensor, flat_positions)\n    return output_tensor\n\n", "CodeBleu_score": 55.18468812590758}, {"code": "def gather_indexes(sequence_tensor, positions)\n\t\n    sequence_shape = modeling.get_shape_list(sequence_tensor, expected_rank=3)\n    batch_size = sequence_shape[0]\n    seq_length = sequence_shape[1]\n    width = sequence_shape[2]\n\n    flat_offsets = tf.reshape(tf.range(0, batch_size, dtype=tf.int32) * seq_length, [-1, 1])\n    flat_positions = tf.reshape(positions + flat_offsets, [-1])\n    flat_sequence_tensor = tf.reshape(sequence_tensor, [batch_size * seq_length, width])\n    output_tensor = tf.gather(flat_sequence_tensor, flat_positions)\n    return output_tensor\n\n", "CodeBleu_score": 55.18468812590758}, {"code": "def gather_indexes(sequence_tensor, positions)\n\t\n    sequence_shape = modeling.get_shape_list(sequence_tensor, expected_rank=3)\n    batch_size = sequence_shape[0]\n    seq_length = sequence_shape[1]\n    width = sequence_shape[2]\n\n    flat_offsets = tf.reshape(tf.range(0, batch_size, dtype=tf.int32) * seq_length, [-1, 1])\n    flat_positions = tf.reshape(positions + flat_offsets, [-1])\n    flat_sequence_tensor = tf.reshape(sequence_tensor, [batch_size * seq_length, width])\n    output_tensor = tf.gather(flat_sequence_tensor, flat_positions)\n    return output_tensor\n\n", "CodeBleu_score": 55.18468812590758}]}
{"instance_id": "956bffee9a33", "generation_result": [{"code": "def _decode_record(record, name_to_features)\n\t\n    example = tf.parse_single_example(record, name_to_features)\n\n    # tf.Example only supports tf.int64, but the TPU only supports tf.int32.\n    # So cast all int64 to int32.\n    for name in list(example.keys()):\n        t = example[name]\n        if t.dtype == tf.int64:\n            t = tf.to_int32(t)\n        example[name] = t\n\n    return example\n\n", "CodeBleu_score": 70.7257133394932}, {"code": "def _decode_record(record, name_to_features)\n\t\n\n    example = tf.parse_single_example(record, name_to_features)\n\n    # tf.Example only supports tf.int64, but the TPU only supports tf.int32.\n    # So cast all int64 to int32.\n    for name in list(example.keys()):\n        t = example[name]\n        if t.dtype == tf.int64:\n            t = tf.to_int32(t)\n        example[name] = t\n\n    return example\n\n", "CodeBleu_score": 70.7257133394932}, {"code": "def _decode_record(record, name_to_features)\n\t\n    example = tf.parse_single_example(record, name_to_features)\n    # tf.Example only supports tf.int64, but the TPU only supports tf.int32.\n    # So cast all int64 to int32.\n    for name in list(example.keys()):\n        t = example[name]\n        if t.dtype == tf.int64:\n            t = tf.to_int32(t)\n        example[name] = t\n    return example\n\n", "CodeBleu_score": 70.7257133394932}]}
{"instance_id": "6b97f354e641", "generation_result": [{"code": "def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer,\n                         orig_answer_text)\n\t\n    tok_answer_text = tokenizer.tokenize(orig_answer_text)\n\n    for new_start in range(input_start, input_end + 1):\n        for new_end in range(input_end, new_start - 1, -1):\n            text_span = tokenizer.convert_tokens_to_string(doc_tokens[new_start:(new_end + 1)])\n            if text_span == orig_answer_text:\n                return new_start, new_end\n\n    return input_start, input_end\n\n", "CodeBleu_score": 54.69391401950459}, {"code": "def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer,\n                         orig_answer_text)\n\t\n    tok_answer_text = \" \".join(tokenizer.tokenize(orig_answer_text))\n\n    for new_start in range(input_start, input_end + 1):\n        for new_end in range(input_end, new_start - 1, -1):\n            text_span = \" \".join(doc_tokens[new_start:(new_end + 1)])\n            if text_span == tok_answer_text:\n                return (new_start, new_end)\n\n    return (input_start, input_end)\n\n", "CodeBleu_score": 78.7094830345058}, {"code": "def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer,\n                         orig_answer_text)\n\t\n\n    tok_answer_text = tokenizer.tokenize(orig_answer_text)\n\n    for new_start in range(input_start, input_end + 1):\n        for new_end in range(input_end, new_start - 1, -1):\n            text_span = tokenizer.convert_tokens_to_string(\n                doc_tokens[new_start:(new_end + 1)])\n            if text_span == orig_answer_text:\n                return new_start, new_end\n\n    return input_start, input_end\n\n", "CodeBleu_score": 52.98891959326194}]}
{"instance_id": "aba334b92b34", "generation_result": [{"code": "def _check_is_max_context(doc_spans, cur_span_index, position)\n\t\n    best_score = None\n    best_span_index = None\n    for (span_index, doc_span) in enumerate(doc_spans):\n        end = doc_span.start + doc_span.length - 1\n        if position < doc_span.start:\n            continue\n        if position > end:\n            continue\n        num_left_context = position - doc_span.start\n        num_right_context = end - position\n        score = min(num_left_context, num_right_context) + 0.01 * doc_span.length\n        if best_score is None or score > best_score:\n            best_score = score\n            best_span_index = span_index\n\n    return cur_span_index == best_span_index\n\n", "CodeBleu_score": 76.92310555989161}, {"code": "def _check_is_max_context(doc_spans, cur_span_index, position)\n\t\n    best_score = None\n    best_span_index = None\n    for (span_index, doc_span) in enumerate(doc_spans):\n        end = doc_span.start + doc_span.length - 1\n        if position < doc_span.start:\n            continue\n        if position > end:\n            continue\n        num_left_context = position - doc_span.start\n        num_right_context = end - position\n        score = min(num_left_context, num_right_context) + 0.01 * doc_span.length\n        if best_score is None or score > best_score:\n            best_score = score\n            best_span_index = span_index\n\n    return cur_span_index == best_span_index\n\n", "CodeBleu_score": 76.92310555989161}, {"code": "def _check_is_max_context(doc_spans, cur_span_index, position)\n\t\n    best_score = None\n    best_span_index = None\n    for (span_index, doc_span) in enumerate(doc_spans):\n        end = doc_span.start + doc_span.length - 1\n        if position < doc_span.start:\n            continue\n        if position > end:\n            continue\n        num_left_context = position - doc_span.start\n        num_right_context = end - position\n        score = min(num_left_context, num_right_context) + 0.01 * doc_span.length\n        if best_score is None or score > best_score:\n            best_score = score\n            best_span_index = span_index\n\n    return cur_span_index == best_span_index\n\n", "CodeBleu_score": 76.92310555989161}]}
{"instance_id": "cf92ad509f6a", "generation_result": [{"code": "def load_vocab(vocab_file)\n\t\n    vocab = collections.OrderedDict()\n    index = 0\n    with open(vocab_file, 'r', encoding='utf-8') as reader:\n        while True:\n            token = convert_to_unicode(reader.readline())\n            if not token:\n                break\n            token = token.strip()\n            vocab[token] = index\n            index += 1\n    return vocab\n\n", "CodeBleu_score": 69.34817212829458}, {"code": "def load_vocab(vocab_file)\n\t\n    vocab = collections.OrderedDict()\n    with open(vocab_file, 'r', encoding='utf-8') as reader:\n        while True:\n            token = convert_to_unicode(reader.readline())\n            if not token:\n                break\n            token = token.strip()\n            vocab[token] = len(vocab)\n    return vocab\n\n", "CodeBleu_score": 38.21680241242412}, {"code": "def load_vocab(vocab_file)\n\t\n    vocab = collections.OrderedDict()\n    index = 0\n    with open(vocab_file, 'r', encoding='utf-8') as reader:\n        while True:\n            token = convert_to_unicode(reader.readline())\n            if not token:\n                break\n            token = token.strip()\n            vocab[token] = index\n            index += 1\n    return vocab\n\n", "CodeBleu_score": 69.34817212829458}]}
{"instance_id": "6c865803ac12", "generation_result": [{"code": "def convert_by_vocab(vocab, items)\n\t\n    output = []\n    for item in items:\n        output.append(vocab[item])\n    return output\n\n", "CodeBleu_score": 79.63749907165396}, {"code": "def convert_by_vocab(vocab, items)\n\t\n    output = []\n    for item in items:\n        output.append(vocab[item])\n    return output\n\n", "CodeBleu_score": 79.63749907165396}, {"code": "def convert_by_vocab(vocab, items)\n\t\n    output = []\n    for item in items:\n        output.append(vocab[item])\n    return output\n\n", "CodeBleu_score": 79.63749907165396}]}
{"instance_id": "e93b9200dc4a", "generation_result": [{"code": "def tokenize(self, text)\n\t\n    tokens = self.basic_tokenizer.tokenize(text)\n    split_tokens = []\n    for token in tokens:\n        for sub_token in self.wordpiece_tokenizer.tokenize(token):\n            split_tokens.append(sub_token)\n    return split_tokens\n\n", "CodeBleu_score": 56.71761519883849}, {"code": "def tokenize(self, text)\n\t\n    tokens = self.basic_tokenizer.tokenize(text)\n    split_tokens = []\n    for token in tokens:\n        for sub_token in self.wordpiece_tokenizer.tokenize(token):\n            split_tokens.append(sub_token)\n\n    return split_tokens\n\n", "CodeBleu_score": 56.71761519883849}, {"code": "def tokenize(self, text)\n\t\n\n    text = convert_to_unicode(text)\n    text = clean_text(text)\n\n    # This was added on November 1st, 2018 for the multilingual and Chinese\n    # models. This is also applied to the English models now, but it doesn't\n    # matter since the English models were not trained on any Chinese data\n    # and generally don't have any Chinese data in them (there are Chinese\n    # characters in the vocabulary because Wikipedia does have some Chinese\n    # words in the English Wikipedia.).\n    text = self.tokenize_chinese_chars(text)\n\n    orig_tokens = whitespace_tokenize(text)\n    split_tokens = []\n    for token in orig_tokens:\n        if self.do_lower_case:\n            token = token.lower()\n            token = self.run_strip_accents(token)\n        split_tokens.extend(self.run_split_on_punc(token))\n\n    output_tokens = whitespace_tokenize(\" \".join(split_tokens))\n    return output_tokens\n\n", "CodeBleu_score": 31.970079713616016}]}
{"instance_id": "2ff6f6db53c9", "generation_result": [{"code": "def safe_load(path, offset, duration, sample_rate, dtype)\n\t\n    try:\n        audio = load(path, offset, duration, sample_rate, dtype)\n        return audio, False\n    except Exception as e:\n        print(f\"Error loading {path}: {e}\")\n        return -1.0, True\n\n", "CodeBleu_score": 16.87913110162328}, {"code": "def safe_load(path, offset, duration, sample_rate, dtype)\n\t\n    try:\n        audio_data = AudioAdapter.load(path, offset, duration, sample_rate, dtype)\n        return audio_data, False\n    except Exception as e:\n        print(f\"Error while loading file {path}: {e}\")\n        return -1.0, True\n\n", "CodeBleu_score": 23.451539889833494}, {"code": "def safe_load(path, offset, duration, sample_rate, dtype)\n\t\n    try:\n        print(f\"Loading {path} from {offset} to {duration} with sample rate {sample_rate} and dtype {dtype}\")\n        return import_module(\"spleeter.audio.adapter\").load(path, offset, duration, sample_rate, dtype), False\n    except Exception as e:\n        print(f\"Error while loading {path}: {e}\")\n        return -1.0, True\n\n", "CodeBleu_score": 23.64441293414088}]}
{"instance_id": "12fbe1f03d8f", "generation_result": [{"code": "def to_n_channels(waveform: tf.Tensor, n_channels: int) -> tf.Tensor\n\t\n    waveform_shape = tf.shape(waveform)\n    waveform_channels = waveform_shape[-1]\n    if waveform_channels == n_channels:\n        return waveform\n    elif waveform_channels > n_channels:\n        return waveform[..., :n_channels]\n    else:\n        return tf.tile(waveform, [1, n_channels // waveform_channels])[:, :n_channels]\n\n\n", "CodeBleu_score": 46.90383880706293}, {"code": "def to_n_channels(waveform: tf.Tensor, n_channels: int) -> tf.Tensor\n\t\n    waveform = tf.convert_to_tensor(waveform)\n    n_channels = int(n_channels)\n    if waveform.shape[0] == n_channels:\n        return waveform\n    elif waveform.shape[0] < n_channels:\n        waveform = tf.tile(waveform, [n_channels // waveform.shape[0], 1])\n        waveform = waveform[:n_channels, :]\n        return waveform\n    else:\n        waveform = waveform[:n_channels, :]\n        return waveform\n\n", "CodeBleu_score": 43.705430639624275}, {"code": "def to_n_channels(waveform: tf.Tensor, n_channels: int) -> tf.Tensor\n\t\n    waveform = tf.convert_to_tensor(waveform)\n    shape = tf.shape(waveform)\n    if shape[0] >= n_channels:\n        return waveform[:n_channels]\n    else:\n        return tf.tile(waveform, [n_channels // shape[0]] + [1])[:n_channels]\n\n\n\n", "CodeBleu_score": 45.09603427499438}]}
{"instance_id": "5e215f029b2e", "generation_result": [{"code": "def spectrogram_to_db_uint(\n    spectrogram: tf.Tensor, db_range: float = 100.0, **kwargs\n) -> tf.Tensor\n\t\n    # Convert to dB\n    spectrogram = tf.math.maximum(spectrogram, 1e-10)\n    spectrogram = gain_to_db(spectrogram, **kwargs)\n\n    # Find maximum dB value\n    max_db = tf.math.reduce_max(spectrogram)\n    if max_db < 0:\n        max_db = 0.0\n\n    # Clip to range\n    spectrogram = tf.math.maximum(spectrogram, max_db - db_range)\n\n    # Convert to uint8\n    spectrogram = tf.cast(\n        tf.math.round(\n            (spectrogram - max_db + db_range) / db_range * 255.0\n        ),\n        tf.uint8,\n    )\n\n    return spectrogram\n\n", "CodeBleu_score": 50.75761079684281}, {"code": "def spectrogram_to_db_uint(\n    spectrogram: tf.Tensor, db_range: float = 100.0, **kwargs\n) -> tf.Tensor\n\t\n\n    spectrogram = tf.math.maximum(spectrogram, 1e-8)\n    spectrogram = tf.math.log(spectrogram)\n    spectrogram = tf.math.divide(spectrogram, tf.math.log(10.0))\n    spectrogram = tf.math.multiply(spectrogram, 20.0)\n    spectrogram = tf.math.subtract(spectrogram, 100.0)\n    spectrogram = tf.math.maximum(spectrogram, -db_range)\n    spectrogram = tf.math.minimum(spectrogram, db_range)\n    spectrogram = tf.math.divide(spectrogram, db_range)\n    spectrogram = tf.math.add(spectrogram, 1.0)\n    spectrogram = tf.math.multiply(spectrogram, 128.0)\n    spectrogram = tf.cast(spectrogram, tf.uint8)\n    return spectrogram\n\n", "CodeBleu_score": 34.68503629294961}, {"code": "def spectrogram_to_db_uint(\n    spectrogram: tf.Tensor, db_range: float = 100.0, **kwargs\n) -> tf.Tensor\n\t\n    # Convert to dB\n    spectrogram_db = gain_to_db(spectrogram, **kwargs)\n    # Find the maximum dB value\n    max_db = tf.reduce_max(spectrogram_db)\n    # Clip the dB spectrogram to the range below the maximum dB value\n    spectrogram_db = tf.clip_by_value(spectrogram_db, max_db - db_range, max_db)\n    # Convert to uint8\n    spectrogram_db = tf.cast(spectrogram_db, tf.uint8)\n    return spectrogram_db\n\n", "CodeBleu_score": 44.07838487100963}]}
{"instance_id": "b65c0968dae7", "generation_result": [{"code": "def compute_spectrogram_tf(\n    waveform: tf.Tensor,\n    frame_length: int = 2048,\n    frame_step: int = 512,\n    spec_exponent: float = 1.0,\n    window_exponent: float = 1.0,\n) -> tf.Tensor\n\t\n\n    # Apply Hann window function to the waveform tensor\n    window = hann_window(frame_length, periodic=False, dtype=tf.float32)\n    window = tf.math.pow(window, window_exponent)\n    window = tf.cast(window, dtype=tf.float32)\n    windowed_waveform = tf.multiply(window, waveform)\n\n    # Compute STFT\n    stft_waveform = stft(\n        windowed_waveform,\n        frame_length=frame_length,\n        frame_step=frame_step,\n        fft_length=frame_length,\n        window_fn=None,\n        pad_end=True,\n    )\n\n    # Compute spectrogram\n    spectrogram = tf.math.pow(tf.abs(stft_waveform), spec_exponent)\n\n    # Transpose the spectrogram\n    spectrogram = tf.transpose(spectrogram)\n\n    return spectrogram\n", "CodeBleu_score": 40.5738040901259}, {"code": "def compute_spectrogram_tf(\n    waveform: tf.Tensor,\n    frame_length: int = 2048,\n    frame_step: int = 512,\n    spec_exponent: float = 1.0,\n    window_exponent: float = 1.0,\n) -> tf.Tensor\n\t\n\n    window = hann_window(frame_length, periodic=True, dtype=waveform.dtype) ** window_exponent\n    stft_tensor = stft(\n        waveform,\n        frame_length=frame_length,\n        frame_step=frame_step,\n        fft_length=frame_length,\n        window_fn=window,\n    )\n    spectrogram = tf.abs(stft_tensor) ** spec_exponent\n    return tf.transpose(spectrogram)\n", "CodeBleu_score": 40.105286971308054}, {"code": "def compute_spectrogram_tf(\n    waveform: tf.Tensor,\n    frame_length: int = 2048,\n    frame_step: int = 512,\n    spec_exponent: float = 1.0,\n    window_exponent: float = 1.0,\n) -> tf.Tensor\n\t\n\n    # Apply Hann window function\n    window = hann_window(\n        frame_length, periodic=True, dtype=waveform.dtype\n    ) ** window_exponent\n\n    # Compute STFT\n    stft_tensor = stft(\n        waveform,\n        frame_length=frame_length,\n        frame_step=frame_step,\n        fft_length=frame_length,\n        window_fn=None,\n        pad_end=False,\n        window_frame=window,\n    )\n\n    # Compute spectrogram\n    spectrogram = tf.math.abs(stft_tensor) ** spec_exponent\n\n    # Transpose spectrogram\n    spectrogram = tf.transpose(spectrogram, perm=[2, 0, 1])\n\n    return spectrogram\n", "CodeBleu_score": 43.59710623055418}]}
{"instance_id": "a4686140a634", "generation_result": [{"code": "def time_stretch(\n    spectrogram: tf.Tensor,\n    factor: float = 1.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n\n    # Calculate new time dimension\n    new_time_dim = tf.cast(tf.shape(spectrogram)[1] * factor, tf.int32)\n\n    # Resize spectrogram\n    spectrogram = tf.image.resize(\n        spectrogram,\n        [tf.shape(spectrogram)[0], new_time_dim],\n        method=method,\n        preserve_aspect_ratio=False,\n    )\n\n    # Adjust to original dimensions\n    if new_time_dim > tf.shape(spectrogram)[1]:\n        # Crop\n        spectrogram = spectrogram[:, : tf.shape(spectrogram)[1] - 1]\n    elif new_time_dim < tf.shape(spectrogram)[1]:\n        # Pad\n        spectrogram = tf.pad(\n            spectrogram,\n            [[0, 0], [0, tf.shape(spectrogram)[1] - new_time_dim]],\n            constant_values=0,\n        )\n\n    return spectrogram\n\n", "CodeBleu_score": 38.921738544900954}, {"code": "def time_stretch(\n    spectrogram: tf.Tensor,\n    factor: float = 1.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n    if factor == 1.0:\n        return spectrogram\n\n    if factor < 1.0:\n        raise ValueError(\n            f\"The factor must be greater than or equal to 1.0, but got {factor}.\"\n        )\n\n    # Calculate the new time dimension.\n    new_time_dim = tf.cast(tf.shape(spectrogram)[1] * factor, tf.int32)\n\n    # Resize the spectrogram.\n    resized = tf.image.resize(\n        spectrogram, [tf.shape(spectrogram)[0], new_time_dim], method=method\n    )\n\n    # Adjust the resized spectrogram to the original dimensions by cropping or padding.\n    if new_time_dim > tf.shape(spectrogram)[1]:\n        # Crop the resized spectrogram to the original dimensions.\n        cropped = resized[:, : tf.shape(spectrogram)[1]]\n        return cropped\n    else:\n        # Pad the resized spectrogram to the original dimensions.\n        pad_width = tf.shape(spectrogram)[1] - new_time_dim\n        paddings = [[0, 0], [0, pad_width], [0, 0]]\n        padded = tf.pad(resized, paddings)\n        return padded\n\n", "CodeBleu_score": 40.98633941341481}, {"code": "def time_stretch(\n    spectrogram: tf.Tensor,\n    factor: float = 1.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n    # get the shape of the input tensor\n    spectrogram_shape = tf.shape(spectrogram)\n    # calculate the new time dimension\n    new_time_dim = tf.cast(\n        tf.cast(spectrogram_shape[0], tf.float32) * factor, tf.int32\n    )\n    # resize the spectrogram\n    spectrogram_resized = tf.image.resize(\n        spectrogram,\n        [new_time_dim, spectrogram_shape[1]],\n        method=method,\n        antialias=True,\n    )\n    # calculate the difference in time dimension between the resized and the original spectrogram\n    time_diff = spectrogram_shape[0] - new_time_dim\n    # crop or pad the resized spectrogram to match the original dimensions\n    if time_diff > 0:\n        spectrogram_resized = spectrogram_resized[:-time_diff, :]\n    else:\n        spectrogram_resized = tf.pad(\n            spectrogram_resized, [[0, -time_diff], [0, 0]]\n        )\n    # return the resized spectrogram\n    return spectrogram_resized\n\n", "CodeBleu_score": 37.45939601975586}]}
{"instance_id": "6dd01101d4ae", "generation_result": [{"code": "def pitch_shift(\n    spectrogram: tf.Tensor,\n    semitone_shift: float = 0.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n    if semitone_shift == 0.0:\n        return spectrogram\n\n    frequency_ratio = 2 ** (semitone_shift / 12)\n    resized_spectrogram = tf.image.resize(\n        spectrogram,\n        [spectrogram.shape[0], int(spectrogram.shape[1] / frequency_ratio)],\n        method=method,\n    )\n\n    padded_spectrogram = tf.image.resize_with_pad(\n        resized_spectrogram,\n        spectrogram.shape[0],\n        spectrogram.shape[1],\n        method=method,\n    )\n\n    return padded_spectrogram\n\n", "CodeBleu_score": 33.79104913056181}, {"code": "def pitch_shift(\n    spectrogram: tf.Tensor,\n    semitone_shift: float = 0.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n\n    # Calculate the resizing factor\n    resize_factor = 2 ** (semitone_shift / 12)\n\n    # Resize the spectrogram along the frequency axis\n    resized_spectrogram = tf.image.resize(\n        spectrogram,\n        [spectrogram.shape[0], int(spectrogram.shape[1] * resize_factor)],\n        method=method,\n    )\n\n    # Pad the resized spectrogram to maintain the original dimensions\n    padded_spectrogram = tf.image.pad_to_bounding_box(\n        resized_spectrogram, 0, 0, spectrogram.shape[0], spectrogram.shape[1]\n    )\n\n    return padded_spectrogram\n\n", "CodeBleu_score": 28.606797844550364}, {"code": "def pitch_shift(\n    spectrogram: tf.Tensor,\n    semitone_shift: float = 0.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n\n    # Calculate the resizing factor\n    resize_factor = 2 ** (semitone_shift / 12)\n\n    # Resize the spectrogram along the frequency axis\n    resized_spectrogram = tf.image.resize(\n        spectrogram,\n        [spectrogram.shape[0], int(spectrogram.shape[1] * resize_factor)],\n        method=method,\n    )\n\n    # Pad the resized spectrogram to maintain the original dimensions\n    pitch_shifted_spectrogram = tf.image.pad_to_bounding_box(\n        resized_spectrogram,\n        0,\n        0,\n        spectrogram.shape[0],\n        spectrogram.shape[1],\n    )\n\n    return pitch_shifted_spectrogram\n\n", "CodeBleu_score": 27.334581097972492}]}
{"instance_id": "612f0826e56a", "generation_result": [{"code": "def _get_conv_activation_layer(params: Dict) -> Any\n\t\n    conv_activation = params.get(\"conv_activation\", \"ReLU\")\n    if conv_activation == \"ReLU\":\n        return ReLU()\n    elif conv_activation == \"ELU\":\n        return ELU()\n    elif conv_activation == \"LeakyReLU\":\n        return LeakyReLU(0.2)\n    else:\n        raise ValueError(\n            \"The provided conv_activation parameter is not valid. Please use one of the following: ReLU, ELU, or LeakyReLU.\"\n        )\n\n", "CodeBleu_score": 53.684061587319164}, {"code": "def _get_conv_activation_layer(params: Dict) -> Any\n\t\n    if params[\"conv_activation\"] == \"relu\":\n        return ReLU()\n    elif params[\"conv_activation\"] == \"elu\":\n        return ELU()\n    else:\n        return LeakyReLU(alpha=0.2)\n\n", "CodeBleu_score": 37.81568218186228}, {"code": "def _get_conv_activation_layer(params: Dict) -> Any\n\t\n\n    if params[\"conv_activation\"] == \"ReLU\":\n        return ReLU()\n    elif params[\"conv_activation\"] == \"ELU\":\n        return ELU()\n    else:\n        return LeakyReLU(alpha=0.2)\n\n", "CodeBleu_score": 46.02205608514587}]}
{"instance_id": "2f8b5a2fc0f2", "generation_result": [{"code": "def softmax_unet(\n    input_tensor: tf.Tensor, instruments: Iterable[str], params: Dict = {}\n) -> Dict\n\t\n\n    # set default parameters\n    params_default = {\n        \"kernel_size\": 3,\n        \"num_layers\": 3,\n        \"num_filters\": 64,\n        \"dropout_rate\": 0.0,\n        \"batch_norm\": False,\n        \"activation\": \"relu\",\n        \"activation_kwargs\": {},\n        \"output_activation\": \"softmax\",\n        \"output_activation_kwargs\": {},\n        \"axis\": 1,\n    }\n    params_default.update(params)\n\n    # initialize the model\n    model = {}\n\n    # iterate over the instruments\n    for instrument in instruments:\n\n        # initialize the model for the current instrument\n        model[instrument] = {}\n\n        # apply the U-Net model to the input tensor\n        model[instrument][\"logit_mask\"] = unet(\n            input_tensor=input_tensor,\n            kernel_size=params_default[\"kernel_size\"],\n            num_layers=params_default[\"num_layers\"],\n            num_filters=params_default[\"num_filters\"],\n            dropout_rate=params_default[\"dropout_rate\"],\n            batch_norm=params_default[\"batch_norm\"],\n            activation=params_default[\"activation\"],\n            activation_kwargs=params_default[\"activation_kwargs\"],\n            output_activation=params_default[\"output_activation\"],\n            output_activation_kwargs=params_default[\"output_activation_kwargs\"],\n        )\n\n    # combine the logit masks\n    logit_masks = Concatenate(axis=params_default[\"axis\"])(\n        [model[instrument][\"logit_mask\"] for instrument in instruments]\n    )\n\n    # apply softmax to the logit masks\n    masks = Softmax(**params_default[\"output_activation_kwargs\"])(logit_masks)\n\n    # create the output dictionary\n    output = {}\n\n    # iterate over the instruments\n    for instrument in instruments:\n\n        # create the spectrogram for the current instrument\n        output[instrument] = Multiply()([masks[..., i], input_tensor", "CodeBleu_score": 34.8801863598685}, {"code": "def softmax_unet(\n    input_tensor: tf.Tensor, instruments: Iterable[str], params: Dict = {}\n) -> Dict\n\t\n    # Set default parameters\n    params.setdefault(\"unet_n_layers\", 5)\n    params.setdefault(\"unet_n_filters_start\", 32)\n    params.setdefault(\"unet_filter_length\", 3)\n    params.setdefault(\"unet_pool_size\", 2)\n    params.setdefault(\"unet_n_convs_per_layer\", 2)\n    params.setdefault(\"unet_dropout\", 0.1)\n    params.setdefault(\"unet_activation\", \"relu\")\n    params.setdefault(\"unet_normalization\", \"batch\")\n    params.setdefault(\"unet_normalization_axis\", 1)\n    params.setdefault(\"unet_last_activation\", \"linear\")\n    params.setdefault(\"unet_mask_activation\", \"softmax\")\n    params.setdefault(\"unet_mask_axis\", 1)\n    params.setdefault(\"unet_mask_temperature\", 1)\n    params.setdefault(\"unet_output_activation\", \"linear\")\n    params.setdefault(\"unet_output_temperature\", 1)\n    params.setdefault(\"unet_output_axis\", 1)\n    params.setdefault(\"unet_output_from_logits\", False)\n    params.setdefault(\"unet_output_mask_separately\", False)\n    params.setdefault(\"unet_output_normalization\", None)\n    params.setdefault(\"unet_output_normalization_axis\", 1)\n\n    # Create a dictionary of logit masks for each instrument\n    logit_masks = {}\n    for instrument in instruments:\n        logit_masks[instrument] = unet(\n            input_tensor,\n            n_layers=params[\"unet_n_layers\"],\n            n_filters_start=params[\"unet_n_filters_start\"],\n            filter_length=params[\"unet_filter_length\"],\n            pool_size=params[\"unet_pool_size\"],\n            n_convs_per_layer=params[\"unet_n_convs_per_layer\"],\n            drop", "CodeBleu_score": 19.514908714534958}, {"code": "def softmax_unet(\n    input_tensor: tf.Tensor, instruments: Iterable[str], params: Dict = {}\n) -> Dict\n\t\n\n    # Initialize the parameters\n    axis: int = params.get(\"axis\", 1)\n    input_shape: Optional[Iterable[int]] = params.get(\"input_shape\")\n    output_shape: Optional[Iterable[int]] = params.get(\"output_shape\")\n    n_filters: int = params.get(\"n_filters\", 32)\n    kernel_size: int = params.get(\"kernel_size\", 3)\n    dropout: float = params.get(\"dropout\", 0.2)\n    batch_norm: bool = params.get(\"batch_norm\", True)\n    activation: str = params.get(\"activation\", \"relu\")\n    initializer: Any = params.get(\"initializer\", he_uniform())\n    padding: str = params.get(\"padding\", \"same\")\n    strides: int = params.get(\"strides\", 1)\n    kernel_regularizer: Any = params.get(\"kernel_regularizer\", None)\n    bias_regularizer: Any = params.get(\"bias_regularizer\", None)\n    activity_regularizer: Any = params.get(\"activity_regularizer\", None)\n    kernel_constraint: Any = params.get(\"kernel_constraint\", None)\n    bias_constraint: Any = params.get(\"bias_constraint\", None)\n    use_bias: bool = params.get(\"use_bias\", True)\n    return_logits: bool = params.get(\"return_logits\", False)\n\n    # Create the U-Net model for each instrument\n    unet_models: Dict[str, tf.keras.Model] = {}\n    for instrument in instruments:\n        unet_models[instrument] = unet(\n            input_shape=input_shape,\n            output_shape=output_shape,\n            n_filters=n_filters,\n            kernel_size=kernel_size,\n            dropout=dropout,\n            batch_norm=batch_norm,\n            activation=activation,\n            initializer=initializer,\n            padding=padding,\n            strides=strides,\n            kernel_regularizer=kernel_regularizer,\n            bias_regularizer=bias_regular", "CodeBleu_score": 26.08677868318141}]}
{"instance_id": "400b19ce8520", "generation_result": [{"code": "def compute_file_checksum(path)\n\t\n    sha256_hash = hashlib.sha256()\n    with open(path, \"rb\") as f:\n        # Read and update hash string value in blocks of 4K\n        for byte_block in iter(lambda: f.read(4096), b\"\"):\n            sha256_hash.update(byte_block)\n        return sha256_hash.hexdigest()\n\n", "CodeBleu_score": 54.9700780341427}, {"code": "def compute_file_checksum(path)\n\t\n    sha256_hash = hashlib.sha256()\n    with open(path, \"rb\") as f:\n        for byte_block in iter(lambda: f.read(4096), b\"\"):\n            sha256_hash.update(byte_block)\n        return sha256_hash.hexdigest()\n\n", "CodeBleu_score": 54.9700780341427}, {"code": "def compute_file_checksum(path)\n\t\n    sha256_hash = hashlib.sha256()\n    with open(path, \"rb\") as f:\n        # Read and update hash string value in blocks of 4K\n        for byte_block in iter(lambda: f.read(4096), b\"\"):\n            sha256_hash.update(byte_block)\n        return sha256_hash.hexdigest()\n\n", "CodeBleu_score": 54.9700780341427}]}
{"instance_id": "5f2e9c6e15f1", "generation_result": [{"code": "def checksum(self, name: str) -> str\n\t\n    url = (\n        f\"{self.host}/{self.repository}/{self.release}/{self.checksum_index}\"\n    )\n    response = httpx.get(url)\n    if response.status_code != 200:\n        raise ValueError(\n            f\"Failed to get checksum index. Status code: {response.status_code}\"\n        )\n    checksum_index = response.json()\n    if name not in checksum_index:\n        raise ValueError(f\"Checksum not found for model: {name}\")\n    return checksum_index[name]\n\n", "CodeBleu_score": 42.26813392588536}, {"code": "def checksum(self, name: str) -> str\n\t\n    url = f\"{self.host}/{self.repository}/{self.release}/{self.checksum_index}\"\n    response = httpx.get(url)\n    response.raise_for_status()\n    checksums = response.json()\n    if name in checksums:\n        return checksums[name]\n    raise ValueError(f\"Model name {name} not found in checksum index.\")\n\n", "CodeBleu_score": 32.33363450990194}, {"code": "def checksum(self, name: str) -> str\n\t\n\n    url = f\"{self.host}/{self.repository}/{self.release}/{self.checksum_index}\"\n    response = httpx.get(url)\n    response.raise_for_status()\n    checksum_index = response.json()\n    if name in checksum_index:\n        return checksum_index[name]\n    else:\n        raise ValueError(f\"Model {name} not found in checksum index.\")\n\n", "CodeBleu_score": 33.63780528887032}]}
{"instance_id": "9312c8b6032e", "generation_result": [{"code": "def download(self, name: str, path: str) -> None:\n\t\n\n    # Construct the download URL\n    url = f\"https://github.com/huggingface/transformers/blob/main/examples/research_projects/{name}/{name}.tar.gz?raw=true\"\n\n    # Log the download initiation\n    self.logger.info(f\"Downloading {url}...\")\n\n    # Stream the file using HTTP/2\n    with httpx.stream(\"GET\", url) as response:\n        response.raise_for_status()\n\n        # Create a temporary file to store the downloaded archive\n        with NamedTemporaryFile() as f:\n\n            # Log the download completion\n            self.logger.info(f\"Download completed. Extracting to {path}...\")\n\n            # Extract the archive to the specified path\n            with tarfile.open(fileobj=f, mode=\"w:gz\") as tar:\n                tar.add(path, arcname=os.path.basename(path))\n\n            # Compute the MD5 checksum of the downloaded archive\n            md5 = hashlib.md5()\n            f.seek(0)\n            while chunk := f.read(8192):\n                md5.update(chunk)\n\n            # Compute the expected MD5 checksum\n            expected_md5 = hashlib.md5(\n                environ.get(f\"TRANSFORMERS_{name.upper()}_CHECKPOINT_SHA256\").encode()\n            ).hexdigest()\n\n            # Compare the checksums\n            if md5.hexdigest() != expected_md5:\n                raise ValueError(\"The archive's MD5 checksum is invalid.\")\n\n            # Extract the archive to the specified path\n            with tarfile.open(fileobj=f, mode=\"r:gz\") as tar:\n                tar.extractall(path)\n\n            # Log the extraction completion\n            self.logger.info(f\"Extraction completed.\")\n", "CodeBleu_score": 43.45461722789411}, {"code": "def download(self, name: str, path: str) -> None:\n\t\n\n    url = environ.get(\"MODEL_URL\")\n    checksum = environ.get(\"MODEL_CHECKSUM\")\n\n    print(f\"Downloading model archive from {url}\")\n    with httpx.stream(\"GET\", url) as response:\n        response.raise_for_status()\n        with NamedTemporaryFile() as tmp_file:\n            for chunk in response.iter_bytes():\n                tmp_file.write(chunk)\n            tmp_file.flush()\n            print(f\"Model archive downloaded to {tmp_file.name}\")\n            if not validate_checksum(tmp_file.name, checksum):\n                raise Exception(\"Checksum validation failed\")\n            extract_archive(tmp_file.name, path)\n\n", "CodeBleu_score": 31.611386358240246}, {"code": "def download(self, name: str, path: str) -> None:\n\t\n    url = self.url + name\n    checksum = self.checksums[name]\n    print(f\"Downloading {name} from {url}\")\n    with NamedTemporaryFile() as tmp_file:\n        with httpx.stream(\"GET\", url) as response:\n            response.raise_for_status()\n            for chunk in response.iter_bytes():\n                tmp_file.write(chunk)\n        tmp_file.seek(0)\n        print(f\"Downloaded {name} to {tmp_file.name}\")\n        if checksum is not None:\n            print(f\"Verifying checksum for {name}\")\n            hasher = hashlib.sha256()\n            with open(tmp_file.name, \"rb\") as f:\n                for chunk in iter(lambda: f.read(4096), b\"\"):\n                    hasher.update(chunk)\n            if hasher.hexdigest() != checksum:\n                raise RuntimeError(\n                    f\"Checksum verification failed for {name}.\"\n                    f\"Expected {checksum}, got {hasher.hexdigest()}\"\n                )\n            print(f\"Verified checksum for {name}\")\n        print(f\"Extracting {name} to {path}\")\n        with tarfile.open(tmp_file.name, \"r:gz\") as tar:\n            tar.extractall(path)\n        print(f\"Extracted {name} to {path}\")\n\n", "CodeBleu_score": 42.975674624477065}]}
{"instance_id": "ee9493f37d77", "generation_result": [{"code": "def load_configuration(descriptor: str) -> Dict\n\t\n    if descriptor.startswith(\"embedded:\"):\n        name = descriptor[9:]\n        if not exists(f\"resources/configurations/{name}.json\"):\n            raise FileNotFoundError(f\"The configuration '{name}' is not available.\")\n        with open(f\"resources/configurations/{name}.json\", \"r\") as file:\n            return json.load(file)\n    else:\n        if not exists(descriptor):\n            raise FileNotFoundError(f\"The configuration file '{descriptor}' is not available.\")\n        with open(descriptor, \"r\") as file:\n            return json.load(file)\n\n", "CodeBleu_score": 49.161318115776226}, {"code": "def load_configuration(descriptor: str) -> Dict\n\t\n    if not descriptor:\n        raise Exception(\"Configuration descriptor is required\")\n\n    if descriptor.startswith(\"embedded:\"):\n        config_name = descriptor[9:]\n        if not config_name:\n            raise Exception(\"Configuration name is required\")\n        config_path = f\"resources/configs/{config_name}.json\"\n        if not exists(config_path):\n            raise Exception(f\"Embedded configuration {config_name} not found\")\n        with open(config_path, \"r\") as config_file:\n            config_content = config_file.read()\n    else:\n        if not exists(descriptor):\n            raise Exception(f\"Configuration file {descriptor} not found\")\n        with open(descriptor, \"r\") as config_file:\n            config_content = config_file.read()\n\n    config = json.loads(config_content)\n    return config\n", "CodeBleu_score": 49.49161189059309}, {"code": "def load_configuration(descriptor: str) -> Dict\n\t\n    if descriptor.startswith('embedded:'):\n        # extract the name of the embedded configuration\n        name = descriptor[9:]\n        # check if the configuration exists\n        if not exists(f'configurations/{name}.json'):\n            raise Exception(f'Embedded configuration \"{name}\" not found.')\n        # load the configuration\n        with open(f'configurations/{name}.json', 'r') as f:\n            configuration = json.load(f)\n    else:\n        # check if the file exists\n        if not exists(descriptor):\n            raise Exception(f'Configuration file \"{descriptor}\" not found.')\n        # load the configuration\n        with open(descriptor, 'r') as f:\n            configuration = json.load(f)\n    return configuration\n", "CodeBleu_score": 47.46009463577287}]}
{"instance_id": "546134b67fbc", "generation_result": [{"code": "def sync_apply(\n    tensor_dict: Dict[str, tf.Tensor], func: Callable, concat_axis: int = 1\n) -> Dict[str, tf.Tensor]\n\t\n    if concat_axis not in [0, 1]:\n        raise ValueError(\n            f\"concat_axis must be either 0 or 1, but got {concat_axis} instead.\"\n        )\n    if concat_axis == 0:\n        return {\n            key: tf.split(func(tf.concat(tensor_dict[key], axis=0)), len(tensor_dict))[\n                i\n            ]\n            for i, key in enumerate(tensor_dict.keys())\n        }\n    elif concat_axis == 1:\n        return {\n            key: tf.split(func(tf.concat(tensor_dict[key], axis=1)), len(tensor_dict))[\n                i\n            ]\n            for i, key in enumerate(tensor_dict.keys())\n        }\n\n", "CodeBleu_score": 37.17028672168277}, {"code": "def sync_apply(\n    tensor_dict: Dict[str, tf.Tensor], func: Callable, concat_axis: int = 1\n) -> Dict[str, tf.Tensor]\n\t\n\n    if concat_axis not in (0, 1):\n        raise ValueError(f\"Concatenation axis must be either 0 or 1, not {concat_axis}\")\n\n    if concat_axis == 0:\n        axis = 1\n    else:\n        axis = 0\n\n    concat_tensor = tf.concat([tf.expand_dims(v, axis=axis) for v in tensor_dict.values()], axis=axis)\n    processed_tensor = func(concat_tensor)\n    if concat_axis == 0:\n        processed_tensor = tf.expand_dims(processed_tensor, axis=1)\n    else:\n        processed_tensor = tf.expand_dims(processed_tensor, axis=0)\n    processed_tensor_dict = tf.split(processed_tensor, [v.shape[concat_axis] for v in tensor_dict.values()], axis=concat_axis)\n    processed_tensor_dict = {k: v for k, v in zip(tensor_dict.keys(), processed_tensor_dict)}\n    return processed_tensor_dict\n\n", "CodeBleu_score": 41.96215925153994}, {"code": "def sync_apply(\n    tensor_dict: Dict[str, tf.Tensor], func: Callable, concat_axis: int = 1\n) -> Dict[str, tf.Tensor]\n\t\n\n    if concat_axis not in [0, 1]:\n        raise ValueError(\n            f\"The specified concatenation axis ({concat_axis}) is not supported. Please specify either 0 or 1.\"\n        )\n\n    if concat_axis == 0:\n        tensor_dict_values = list(tensor_dict.values())\n        tensor_dict_keys = list(tensor_dict.keys())\n        tensor_dict_concat = tf.concat(tensor_dict_values, axis=concat_axis)\n        tensor_dict_processed = func(tensor_dict_concat)\n        tensor_dict_processed = tf.split(tensor_dict_processed, len(tensor_dict_values), axis=concat_axis)\n        tensor_dict_processed = dict(zip(tensor_dict_keys, tensor_dict_processed))\n    else:\n        tensor_dict_values = list(tensor_dict.values())\n        tensor_dict_keys = list(tensor_dict.keys())\n        tensor_dict_concat = tf.concat(tensor_dict_values, axis=concat_axis)\n        tensor_dict_processed = func(tensor_dict_concat)\n        tensor_dict_processed = tf.split(tensor_dict_processed, len(tensor_dict_values), axis=concat_axis)\n        tensor_dict_processed = dict(zip(tensor_dict_keys, tensor_dict_processed))\n\n    return tensor_dict_processed\n\n", "CodeBleu_score": 42.01740128906586}]}
{"instance_id": "df48e78d46d2", "generation_result": [{"code": "def from_float32_to_uint8(\n    tensor: tf.Tensor,\n    tensor_key: str = \"tensor\",\n    min_key: str = \"min\",\n    max_key: str = \"max\",\n) -> tf.Tensor\n\t\n\n    min_value = tf.reduce_min(tensor)\n    max_value = tf.reduce_max(tensor)\n    tensor = (tensor - min_value) / (max_value - min_value)\n    tensor = tf.cast(tensor * 255, dtype=tf.uint8)\n\n    return {tensor_key: tensor, min_key: min_value, max_key: max_value}\n\n", "CodeBleu_score": 48.223458171913094}, {"code": "def from_float32_to_uint8(\n    tensor: tf.Tensor,\n    tensor_key: str = \"tensor\",\n    min_key: str = \"min\",\n    max_key: str = \"max\",\n) -> tf.Tensor\n\t\n    tensor_min = tf.reduce_min(tensor)\n    tensor_max = tf.reduce_max(tensor)\n    tensor_normalized = tf.cast(\n        (tensor - tensor_min) / (tensor_max - tensor_min) * 255, tf.uint8\n    )\n    return {\n        tensor_key: tensor_normalized,\n        min_key: tensor_min,\n        max_key: tensor_max,\n    }\n\n", "CodeBleu_score": 60.7504470906253}, {"code": "def from_float32_to_uint8(\n    tensor: tf.Tensor,\n    tensor_key: str = \"tensor\",\n    min_key: str = \"min\",\n    max_key: str = \"max\",\n) -> tf.Tensor\n\t\n\n    min_value = tf.reduce_min(tensor)\n    max_value = tf.reduce_max(tensor)\n\n    tensor = tf.cast(tensor, tf.float32)\n    tensor = (tensor - min_value) / (max_value - min_value)\n    tensor = tf.cast(tensor * 255, tf.uint8)\n\n    return {tensor_key: tensor, min_key: min_value, max_key: max_value}\n\n", "CodeBleu_score": 49.21164030304975}]}
{"instance_id": "7f479e5de492", "generation_result": [{"code": "def pad_and_partition(tensor: tf.Tensor, segment_len: int) -> tf.Tensor\n\t\n    pad_len = tf.cast(tf.math.ceil(tf.cast(tf.shape(tensor)[0], tf.float32) / segment_len), tf.int32) * segment_len - tf.shape(tensor)[0]\n    padded_tensor = tf.pad(tensor, [[0, pad_len], [0, 0]])\n    return tf.reshape(padded_tensor, [-1, segment_len] + padded_tensor.shape[1:])\n\n", "CodeBleu_score": 29.68585553063474}, {"code": "def pad_and_partition(tensor: tf.Tensor, segment_len: int) -> tf.Tensor\n\t\n    padded_len = tf.math.ceil(tf.shape(tensor)[0] / segment_len) * segment_len\n    paddings = [[0, padded_len - tf.shape(tensor)[0]], [0, 0]]\n    padded_tensor = tf.pad(tensor, paddings)\n    return tf.reshape(padded_tensor, [-1, segment_len, tf.shape(tensor)[1]])\n\n", "CodeBleu_score": 27.729503335157613}, {"code": "def pad_and_partition(tensor: tf.Tensor, segment_len: int) -> tf.Tensor\n\t\n    pad_len = -tf.shape(tensor)[0] % segment_len\n    padded = tf.pad(tensor, [[0, pad_len]])\n    return tf.reshape(padded, [-1, segment_len])\n\n", "CodeBleu_score": 17.465601041643446}]}
{"instance_id": "1789fae7253f", "generation_result": [{"code": "def pad_and_reshape(instr_spec, frame_length, F) -> Any\n\t\n\n    # Calculate the number of extra rows needed for padding\n    n_rows = int(frame_length - instr_spec.shape[1] % frame_length)\n    # Create an extension of zeros\n    extension = F.zeros((instr_spec.shape[0], n_rows, instr_spec.shape[2]))\n    # Concatenate the extension to the original spectrogram along the frequency axis\n    padded_spec = F.concat(instr_spec, extension, dim=1)\n    # Reshape the extended spectrogram by merging the first two dimensions\n    padded_spec = F.reshape(padded_spec, (instr_spec.shape[0] * (instr_spec.shape[1] + n_rows), instr_spec.shape[2]))\n\n    return padded_spec\n\n", "CodeBleu_score": 18.194607702211876}, {"code": "def pad_and_reshape(instr_spec, frame_length, F) -> Any\n\t\n\n    # Calculate the number of extra rows needed for padding\n    extra_rows = (frame_length - instr_spec.shape[1]) // 2\n\n    # Create an extension of zeros\n    zeros = F.zeros((extra_rows, instr_spec.shape[2]), dtype=instr_spec.dtype)\n\n    # Concatenate the extension to the original spectrogram along the frequency axis\n    instr_spec = F.concat(zeros, instr_spec, zeros, dim=0)\n\n    # Reshape the extended spectrogram by merging the first two dimensions\n    instr_spec = F.reshape(instr_spec, shape=(-1, instr_spec.shape[2]))\n\n    return instr_spec\n\n", "CodeBleu_score": 15.58679964342826}, {"code": "def pad_and_reshape(instr_spec, frame_length, F) -> Any\n\t\n    # Calculate the number of extra rows needed for padding\n    num_extra_rows = frame_length - instr_spec.shape[1] % frame_length\n\n    # Create an extension of zeros\n    extension = F.zeros((instr_spec.shape[0], num_extra_rows, instr_spec.shape[2]))\n\n    # Concatenate the extension to the original spectrogram along the frequency axis\n    instr_spec = F.concat(instr_spec, extension, dim=1)\n\n    # Reshape the extended spectrogram by merging the first two dimensions\n    instr_spec = instr_spec.reshape((-1, instr_spec.shape[2]))\n\n    return instr_spec\n\n", "CodeBleu_score": 15.834445657045041}]}
{"instance_id": "8a4f4658d97c", "generation_result": [{"code": "def dataset_from_csv(csv_path: str, **kwargs) -> Any\n\t\n\n    import pandas as pd\n    import tensorflow as tf\n\n    df = pd.read_csv(csv_path)\n    dataset = tf.data.Dataset.from_tensor_slices(dict(df))\n    return dataset\n\n", "CodeBleu_score": 25.836439232608967}, {"code": "def dataset_from_csv(csv_path: str, **kwargs) -> Any\n\t\n    import pandas as pd\n    import tensorflow as tf\n\n    df = pd.read_csv(csv_path, **kwargs)\n    dataset = tf.data.Dataset.from_tensor_slices(dict(df))\n\n    return dataset\n\n", "CodeBleu_score": 36.47017546402109}, {"code": "def dataset_from_csv(csv_path: str, **kwargs) -> Any\n\t\n    import pandas as pd\n    import tensorflow as tf\n\n    # read the CSV file into a pandas DataFrame\n    df = pd.read_csv(csv_path, **kwargs)\n\n    # convert the DataFrame columns into a TensorFlow dataset by slicing the tensor values for each column\n    dataset = tf.data.Dataset.from_tensor_slices(dict(df))\n\n    return dataset\n\n", "CodeBleu_score": 36.47017546402109}]}
{"instance_id": "9a16e81b182f", "generation_result": [{"code": "def check_tensor_shape(tensor_tf: tf.Tensor, target_shape: Any) -> bool\n\t\n    result: bool = True\n    for i, target_length in enumerate(target_shape):\n        if target_length is not None:\n            result = result and (tensor_tf.shape[i] == target_length)\n    return result\n\n", "CodeBleu_score": 41.31591581483508}, {"code": "def check_tensor_shape(tensor_tf: tf.Tensor, target_shape: Any) -> bool\n\t\n    result = True\n    for i, dim in enumerate(target_shape):\n        if dim is not None:\n            result = result and (tensor_tf.shape[i] == dim)\n    return result\n\n", "CodeBleu_score": 30.692801589977275}, {"code": "def check_tensor_shape(tensor_tf: tf.Tensor, target_shape: Any) -> bool\n\t\n    result = True\n    for target_dim, target_len in enumerate(target_shape):\n        if target_len is not None:\n            result = result and (tensor_tf.shape[target_dim] == target_len)\n    return result\n\n", "CodeBleu_score": 34.20553557536255}]}
{"instance_id": "49d5d3084b3a", "generation_result": [{"code": "def get_training_dataset(\n    audio_params: Dict, audio_adapter: AudioAdapter, audio_path: str\n) -> Any\n\t\n    builder = DatasetBuilder(audio_params, audio_adapter)\n    return builder.build(\n        audio_path,\n        train_csv=audio_params[\"train_csv\"],\n        cache_dir=audio_params[\"cache_dir\"],\n        batch_size=audio_params[\"batch_size\"],\n        num_chunks_per_song=audio_params[\"num_chunks_per_song\"],\n        data_augmentation=audio_params[\"data_augmentation\"],\n        cache=audio_params[\"cache\"],\n    )\n\n", "CodeBleu_score": 29.75886392295859}, {"code": "def get_training_dataset(\n    audio_params: Dict, audio_adapter: AudioAdapter, audio_path: str\n) -> Any\n\t\n    builder = DatasetBuilder(\n        audio_params, audio_adapter, audio_path, chunk_duration=10, seed=42\n    )\n    return builder.build(\n        train_csv=audio_path + SEPARATOR + \"train.csv\",\n        cache_dir=audio_path + SEPARATOR + \"cache\",\n        batch_size=16,\n        num_chunks_per_song=10,\n        augment=True,\n        cache=True,\n    )\n\n", "CodeBleu_score": 34.960246178538604}, {"code": "def get_training_dataset(\n    audio_params: Dict, audio_adapter: AudioAdapter, audio_path: str\n) -> Any\n\t\n\n    builder = DatasetBuilder(\n        audio_params,\n        audio_adapter,\n        chunk_duration=audio_params[\"chunk_duration\"],\n        seed=audio_params[\"seed\"],\n    )\n\n    return builder.build(\n        train_csv=audio_path,\n        cache_dir=audio_params[\"cache_dir\"],\n        batch_size=audio_params[\"batch_size\"],\n        num_chunks_per_song=audio_params[\"num_chunks_per_song\"],\n        augment=audio_params[\"augment\"],\n        cache=audio_params[\"cache\"],\n    )\n\n", "CodeBleu_score": 38.110954759866054}]}
{"instance_id": "fe8bbbba06e3", "generation_result": [{"code": "def instruments(self) -> Any\n\t\n    if not hasattr(self, \"instrument_builders\"):\n        self.instrument_builders: List[Any] = []\n        for instrument in self.instruments:\n            self.instrument_builders.append(\n                InstrumentDatasetBuilder(\n                    instrument=instrument,\n                    dataset_dir=self.dataset_dir,\n                    dataset_name=self.dataset_name,\n                    dataset_description=self.dataset_description,\n                    dataset_version=self.dataset_version,\n                    dataset_type=self.dataset_type,\n                    dataset_reference=self.dataset_reference,\n                    dataset_license=self.dataset_license,\n                    dataset_terms_of_use=self.dataset_terms_of_use,\n                    dataset_acknowledgements=self.dataset_acknowledgements,\n                    dataset_citation=self.dataset_citation,\n                    dataset_documentation=self.dataset_documentation,\n                    dataset_metadata=self.dataset_metadata,\n                    dataset_tags=self.dataset_tags,\n                    dataset_icon=self.dataset_icon,\n                    dataset_cover=self.dataset_cover,\n                    dataset_papers=self.dataset_papers,\n                    dataset_siblings=self.dataset_siblings,\n                    dataset_childs=self.dataset_childs,\n                    dataset_parent=self.dataset_parent,\n                    dataset_script=self.dataset_script,\n                    dataset_sample=self.dataset_sample,\n                    dataset_assets=self.dataset_assets,\n                    dataset_data=self.dataset_data,\n                    dataset_readme=self.dataset_readme,\n                    dataset_manifest=self.dataset_manifest,\n                    dataset_diff=self.dataset_diff,\n                    dataset_config=self.dataset_config,\n                    dataset_ignore=self.dataset_ignore,\n                    dataset_parameters=self.dataset_parameters,\n                    dataset_dependencies=self.dataset_dependencies,\n                    dataset_requirements=self.dataset_requirements,\n                   ", "CodeBleu_score": 16.691115603972015}, {"code": "def instruments(self) -> Any\n\t\n    if not hasattr(self, \"_instrument_builders\"):\n        self._instrument_builders: List[Any] = []\n        for instrument in self.instruments:\n            self._instrument_builders.append(\n                InstrumentDatasetBuilder(\n                    self.root,\n                    self.instruments,\n                    self.instrument_to_idx,\n                    self.instrument_to_program,\n                    self.instrument_to_is_drum,\n                    self.velocity_bins,\n                    self.pitch_bins,\n                    self.drum_pitch_bins,\n                    self.min_pitch,\n                    self.max_pitch,\n                    self.min_velocity,\n                    self.max_velocity,\n                    self.hop_length,\n                    self.fps,\n                    self.max_duration,\n                    self.filter_drum_roll,\n                    self.filter_program,\n                    self.filter_pitch,\n                    self.filter_velocity,\n                    self.filter_beat,\n                    self.filter_duration,\n                    self.filter_onset,\n                    self.filter_offset,\n                    self.filter_frame,\n                    self.filter_track,\n                    self.filter_instrument,\n                    self.filter_fps,\n                    self.filter_hop_length,\n                    self.filter_max_duration,\n                    self.filter_min_duration,\n                    self.filter_max_pitch,\n                    self.filter_min_pitch,\n                    self.filter_max_velocity,\n                    self.filter_min_velocity,\n                    self.filter_beat_range,\n                    self.filter_duration_range,\n                    self.filter_onset_range,\n                    self.filter_offset_range,\n                    self.filter_frame_range,\n                    self.filter_track_range,\n                    self.filter_instrument_range,\n                    self.filter_fps_range", "CodeBleu_score": 17.051731983388542}, {"code": "def instruments(self) -> Any\n\t\n    if not hasattr(self, \"_instrument_builders\"):\n        self._instrument_builders = []\n        for instrument in self.instruments:\n            self._instrument_builders.append(\n                InstrumentDatasetBuilder(self, instrument)\n            )\n    for instrument_builder in self._instrument_builders:\n        yield instrument_builder\n\n", "CodeBleu_score": 75.4222787024696}]}
{"instance_id": "438b57fd3413", "generation_result": [{"code": "def cache(self, dataset: Any, cache: Optional[str], wait: bool) -> Any\n\t\n    if cache is None:\n        return dataset\n    else:\n        while True:\n            if exists(cache):\n                break\n            elif wait:\n                print(f\"Waiting for cache at {cache}\")\n                time.sleep(60)\n            else:\n                raise FileNotFoundError(f\"Cache not found at {cache}\")\n        dataset.cache(cache)\n        return dataset\n\n", "CodeBleu_score": 37.10735923128014}, {"code": "def cache(self, dataset: Any, cache: Optional[str], wait: bool) -> Any\n\t\n    if cache is not None:\n        if wait:\n            while not exists(cache + SEPARATOR + \"index\"):\n                print(f\"Waiting for cache at {cache} to be created...\")\n                time.sleep(10)\n        self.ensure_cache_dir(cache)\n        dataset.save_to_disk(cache)\n    return dataset\n\n", "CodeBleu_score": 48.73431424603948}, {"code": "def cache(self, dataset: Any, cache: Optional[str], wait: bool) -> Any\n\t\n\n    if cache is not None:\n        if wait:\n            while not exists(cache + SEPARATOR + \"index.json\"):\n                print(\n                    \"The cache is not ready. Please wait for a moment. \"\n                    \"If you want to stop waiting, press Ctrl+C.\"\n                )\n                time.sleep(60)\n\n        os.makedirs(cache, exist_ok=True)\n        return dataset.cache(cache)\n\n    return dataset\n\n", "CodeBleu_score": 53.42136786737643}]}
{"instance_id": "8838a3a09a66", "generation_result": [{"code": "def create_estimator(params: Dict, MWF: bool) -> tf.Tensor\n\t\n    model_provider = ModelProvider(params[\"model_dir\"])\n    params = model_provider.update_params(params)\n    session_config = tf.ConfigProto(\n        allow_soft_placement=True,\n        log_device_placement=False,\n        gpu_options=tf.GPUOptions(allow_growth=True),\n    )\n    run_config = tf.estimator.RunConfig(\n        session_config=session_config, save_checkpoints_steps=1000\n    )\n    estimator = tf.estimator.Estimator(\n        model_fn=model_provider.model_fn,\n        model_dir=params[\"model_dir\"],\n        params=params,\n        config=run_config,\n    )\n    return estimator\n\n", "CodeBleu_score": 32.480408912992495}, {"code": "def create_estimator(params: Dict, MWF: bool) -> tf.Tensor\n\t\n    # Load the model directory using a ModelProvider\n    model_provider = ModelProvider(params['model_dir'])\n    model_dir = model_provider.get_model_dir(params['model_dir'])\n    # Update the parameters\n    params['model_dir'] = model_dir\n    params['model_params']['model_dir'] = model_dir\n    params['model_params']['MWF'] = MWF\n    # Set up the session configuration to limit GPU memory usage\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    # Create a RunConfig with the session configuration\n    run_config = tf.estimator.RunConfig(session_config=config)\n    # Initialize and return a TensorFlow estimator using the specified model function, model directory, parameters, and configuration\n    return tf.estimator.Estimator(\n        model_fn=model_provider.get_model_fn(params['model_dir']),\n        model_dir=model_dir,\n        params=params['model_params'],\n        config=run_config\n    )\n\n", "CodeBleu_score": 35.08298911401525}, {"code": "def create_estimator(params: Dict, MWF: bool) -> tf.Tensor\n\t\n    model_provider = ModelProvider(params, MWF)\n    model_dir = model_provider.get_model_dir()\n    params = model_provider.get_params()\n    session_config = tf.ConfigProto(allow_soft_placement=True,\n                                    gpu_options=tf.GPUOptions(allow_growth=True))\n    run_config = tf.estimator.RunConfig(session_config=session_config,\n                                        save_checkpoints_steps=params['save_checkpoints_steps'],\n                                        keep_checkpoint_max=params['keep_checkpoint_max'],\n                                        log_step_count_steps=params['log_step_count_steps'],\n                                        model_dir=model_dir)\n    estimator = tf.estimator.Estimator(model_fn=model_provider.get_model_fn(),\n                                       model_dir=model_dir,\n                                       params=params,\n                                       config=run_config)\n    return estimator\n\n", "CodeBleu_score": 26.796328664006413}]}
{"instance_id": "779c812bf2cb", "generation_result": [{"code": "def _separate_tensorflow(\n        self, waveform: np.ndarray, audio_descriptor: AudioDescriptor\n    ) -> Dict\n\t\n    if len(waveform.shape) == 1:\n        waveform = np.expand_dims(waveform, axis=1)\n    elif waveform.shape[1] != 2:\n        raise ValueError(\n            \"TensorFlow model only supports mono or stereo waveforms.\"\n        )\n\n    prediction_generator = self.get_prediction_generator()\n    prediction_generator.update(waveform, audio_descriptor)\n    prediction = next(prediction_generator)\n    del prediction[\"audio_id\"]\n    return prediction\n\n", "CodeBleu_score": 49.849467609790274}, {"code": "def _separate_tensorflow(\n        self, waveform: np.ndarray, audio_descriptor: AudioDescriptor\n    ) -> Dict\n\t\n    # Ensure the input waveform is stereo\n    if audio_descriptor.num_channels == 1:\n        waveform = np.array([waveform, waveform])\n\n    # Obtain a prediction generator\n    data_gen = self._data_gen(waveform, audio_descriptor)\n\n    # Update the data generator with the waveform and audio descriptor\n    data_gen.update(waveform, audio_descriptor)\n\n    # Generate a prediction\n    prediction = next(data_gen)\n\n    # Remove the \"audio_id\" key from the prediction before returning it\n    del prediction[\"audio_id\"]\n\n    return prediction\n\n", "CodeBleu_score": 46.404263017878705}, {"code": "def _separate_tensorflow(\n        self, waveform: np.ndarray, audio_descriptor: AudioDescriptor\n    ) -> Dict\n\t\n    if len(waveform.shape) == 1:\n        waveform = np.array([waveform, waveform])\n    elif len(waveform.shape) == 2 and waveform.shape[1] == 1:\n        waveform = np.array([waveform, waveform])\n    elif len(waveform.shape) == 2 and waveform.shape[1] == 2:\n        pass\n    else:\n        raise ValueError(\n            f\"Waveform must be mono or stereo, but received shape {waveform.shape}.\"\n        )\n\n    data_gen = self.data_generators[audio_descriptor.source_type]\n    data_gen.update(waveform, audio_descriptor)\n    prediction = next(data_gen)\n    prediction.pop(\"audio_id\")\n\n    return prediction\n\n", "CodeBleu_score": 46.149677119088416}]}
{"instance_id": "cf580cd2ed93", "generation_result": [{"code": "def compute_backbone_shapes(config, image_shape)\n\t\n    if callable(config.BACKBONE):\n        return config.COMPUTE_BACKBONE_SHAPE(image_shape)\n    # Currently supports ResNet only.\n    assert config.BACKBONE in [\"resnet50\", \"resnet101\"]\n    return np.array(\n        [[int(math.ceil(image_shape[0] / stride)),\n          int(math.ceil(image_shape[1] / stride))]\n         for stride in config.BACKBONE_STRIDES])\n\n", "CodeBleu_score": 94.81077990178257}, {"code": "def compute_backbone_shapes(config, image_shape)\n\t\n    if callable(config.BACKBONE):\n        return config.COMPUTE_BACKBONE_SHAPE(image_shape)\n    # Currently supports ResNet only\n    assert config.BACKBONE in [\"resnet50\", \"resnet101\"]\n    return np.array(\n        [[int(math.ceil(image_shape[0] / stride)),\n          int(math.ceil(image_shape[1] / stride))]\n         for stride in config.BACKBONE_STRIDES])\n\n", "CodeBleu_score": 94.81077990178257}, {"code": "def compute_backbone_shapes(config, image_shape)\n\t\n    if callable(config.BACKBONE):\n        return config.COMPUTE_BACKBONE_SHAPE(image_shape)\n    # Currently supports ResNet only. TensorFlow backend.\n    assert config.BACKBONE in [\"resnet50\", \"resnet101\"]\n    return np.array(\n        [[int(math.ceil(image_shape[0] / stride)),\n          int(math.ceil(image_shape[1] / stride))]\n         for stride in config.BACKBONE_STRIDES])\n\n", "CodeBleu_score": 94.81077990178257}]}
{"instance_id": "ebd61a2e59ca", "generation_result": [{"code": "def clip_boxes_graph(boxes, window)\n\t\n    # Split\n    wy1, wy2 = window[0]\n    wx1, wx2 = window[1]\n    y1, x1, y2, x2 = tf.split(boxes, 4, axis=-1)\n    # Clip\n    y1 = tf.maximum(tf.minimum(y1, wy2), wy1)\n    y2 = tf.maximum(tf.minimum(y2, wy2), wy1)\n    x1 = tf.maximum(tf.minimum(x1, wx2), wx1)\n    x2 = tf.maximum(tf.minimum(x2, wx2), wx1)\n    # Return\n    clipped = tf.concat([y1, x1, y2, x2], axis=-1, name=\"clipped_boxes\")\n    clipped.set_shape((clipped.shape[0], 4))\n    return clipped\n\n", "CodeBleu_score": 52.061021762568075}, {"code": "def clip_boxes_graph(boxes, window)\n\t\n    # Split\n    wy1, wy2 = tf.split(window, 2, axis=0)\n    wx1, wx2 = tf.split(window, 2, axis=1)\n\n    y1, x1, y2, x2 = tf.split(boxes, 4, axis=-1)\n    # Clip\n    y1 = tf.maximum(tf.minimum(y1, wy2), wy1)\n    x1 = tf.maximum(tf.minimum(x1, wx2), wx1)\n    y2 = tf.maximum(tf.minimum(y2, wy2), wy1)\n    x2 = tf.maximum(tf.minimum(x2, wx2), wx1)\n    clipped = tf.concat([y1, x1, y2, x2], axis=-1)\n    clipped.set_shape(boxes.shape)\n    return clipped\n\n", "CodeBleu_score": 44.711524188853204}, {"code": "def clip_boxes_graph(boxes, window)\n\t\n    # Split up and reshape the window and box tensors\n    win = tf.reshape(window, [-1, 1, 4])\n    win_xmin = win[:, 0, 0]\n    win_ymin = win[:, 0, 1]\n    win_xmax = win[:, 0, 2]\n    win_ymax = win[:, 0, 3]\n\n    boxes = tf.reshape(boxes, [-1, 1, 4])\n    xmin = boxes[:, 0, 0]\n    ymin = boxes[:, 0, 1]\n    xmax = boxes[:, 0, 2]\n    ymax = boxes[:, 0, 3]\n\n    # Clip the x and y coordinates\n    xmin = tf.maximum(tf.minimum(xmin, win_xmax), win_xmin)\n    ymin = tf.maximum(tf.minimum(ymin, win_ymax), win_ymin)\n    xmax = tf.maximum(tf.minimum(xmax, win_xmax), win_xmin)\n    ymax = tf.maximum(tf.minimum(ymax, win_ymax), win_ymin)\n\n    clipped_boxes = tf.concat([xmin, ymin, xmax, ymax], axis=1, name=\"clipped_boxes\")\n    clipped_boxes = tf.reshape(clipped_boxes, [-1, 4])\n    return clipped_boxes\n\n", "CodeBleu_score": 22.020608737944748}]}
{"instance_id": "8e0411587a70", "generation_result": [{"code": "def rpn_graph(feature_map, anchors_per_location, anchor_stride)\n\t\n    shared = keras.layers.Conv2D(512, (3, 3), padding=\"same\", activation=\"relu\", strides=anchor_stride, name=\"rpn_conv_shared\")(feature_map)\n\n    x = keras.layers.Conv2D(2 * anchors_per_location, (1, 1), padding=\"valid\", activation=\"linear\", name=\"rpn_class_raw\")(shared)\n    x = keras.layers.Reshape((-1, 2), name=\"rpn_class_logits\")(x)\n    x = keras.layers.Activation(\"softmax\", name=\"rpn_class_xxx\")(x)\n    rpn_class_logits = x\n\n    x = keras.layers.Conv2D(anchors_per_location * 4, (1, 1), padding=\"valid\", activation=\"linear\", name=\"rpn_bbox_pred\")(shared)\n    x = keras.layers.Reshape((-1, 4), name=\"rpn_bbox\")(x)\n    rpn_bbox = x\n\n    return rpn_class_logits, rpn_bbox\n\n", "CodeBleu_score": 41.69347602742097}, {"code": "def rpn_graph(feature_map, anchors_per_location, anchor_stride)\n\t\n\n    shared = keras.layers.Conv2D(512, (3, 3), padding=\"same\", activation=\"relu\", kernel_initializer=\"normal\", name=\"rpn_conv_shared\")(feature_map)\n\n    x = keras.layers.Conv2D(2 * anchors_per_location, (1, 1), padding=\"valid\", activation=\"linear\", kernel_initializer=\"zero\", name=\"rpn_class_raw\")(shared)\n    x = keras.layers.Reshape((-1, 2), name=\"rpn_class_logits\")(x)\n    x = keras.layers.Activation(\"softmax\", name=\"rpn_class\")(x)\n\n    x = keras.layers.Conv2D(2 * anchors_per_location, (1, 1), padding=\"valid\", activation=\"linear\", kernel_initializer=\"zero\", name=\"rpn_bbox_pred\")(shared)\n    x = keras.layers.Reshape((-1, 4), name=\"rpn_bbox\")(x)\n\n    return x, x\n\n", "CodeBleu_score": 38.010283094854636}, {"code": "def rpn_graph(feature_map, anchors_per_location, anchor_stride)\n\t\n\n    x = keras.layers.Conv2D(512, (3, 3), padding=\"same\", activation=\"relu\", kernel_initializer=\"normal\", name=\"rpn_conv1\")(feature_map)\n\n    x_class = keras.layers.Conv2D(2 * anchors_per_location, (1, 1), activation=\"linear\", kernel_initializer=\"zero\", name=\"rpn_out_class\")(x)\n    x_class = keras.layers.Reshape((-1, 2), name=\"rpn_out_class_reshape\")(x_class)\n    x_regr = keras.layers.Conv2D(2 * anchors_per_location, (1, 1), activation=\"linear\", kernel_initializer=\"zero\", name=\"rpn_out_regress\")(x)\n    x_regr = keras.layers.Reshape((-1, 4), name=\"rpn_out_regress_reshape\")(x_regr)\n\n    return [x_class, x_regr, feature_map]\n\n", "CodeBleu_score": 33.88508273450328}]}
{"instance_id": "70777c9261d8", "generation_result": [{"code": "def smooth_l1_loss(y_true, y_pred)\n\t\n    diff = K.abs(y_true - y_pred)\n    less_than_one = K.cast(K.less(diff, 1.0), \"float32\")\n    loss = (less_than_one * 0.5 * diff**2) + (1 - less_than_one) * (diff - 0.5)\n    return loss\n\n", "CodeBleu_score": 60.928513203413225}, {"code": "def smooth_l1_loss(y_true, y_pred)\n\t\n    diff = K.abs(y_true - y_pred)\n    less_than_one = K.cast(K.less(diff, 1.0), \"float32\")\n    loss = (less_than_one * 0.5 * diff**2) + (1 - less_than_one) * (diff - 0.5)\n    return loss\n\n", "CodeBleu_score": 60.928513203413225}, {"code": "def smooth_l1_loss(y_true, y_pred)\n\t\n    diff = K.abs(y_true - y_pred)\n    less_than_one = K.cast(K.less(diff, 1.0), \"float32\")\n    loss = (less_than_one * 0.5 * diff**2) + (1 - less_than_one) * (diff - 0.5)\n    return loss\n\n", "CodeBleu_score": 60.928513203413225}]}
{"instance_id": "0e747b604087", "generation_result": [{"code": "def build_model(x_train, num_classes)\n\t\n    \n    # Reset the TensorFlow graph\n    tf.reset_default_graph()\n\n    # Define the input layer\n    x = tf.placeholder(tf.float32, shape=(None, x_train.shape[1], x_train.shape[2], x_train.shape[3]))\n\n    # Define the first convolutional layer\n    W1 = tf.get_variable('W1', [3, 3, 1, 16], initializer=tf.contrib.layers.xavier_initializer())\n    b1 = tf.get_variable('b1', [16], initializer=tf.zeros_initializer())\n    conv1 = tf.nn.conv2d(x, W1, strides=[1, 1, 1, 1], padding='SAME') + b1\n    h1 = tf.nn.relu(conv1)\n\n    # Define the max-pooling layer\n    h1_pool = tf.nn.max_pool(h1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n\n    # Define the second convolutional layer\n    W2 = tf.get_variable('W2', [3, 3, 16, 32], initializer=tf.contrib.layers.xavier_initializer())\n    b2 = tf.get_variable('b2', [32], initializer=tf.zeros_initializer())\n    conv2 = tf.nn.conv2d(h1_pool, W2, strides=[1, 1, 1, 1], padding='SAME') + b2\n    h2 = tf.nn.relu(conv2)\n\n    # Define the max-pooling layer\n    h2_pool = tf.nn.max_pool(h2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n\n    # Flatten the output of the max-pooling layer\n    h2_pool_flat = tf.reshape(h2_pool, [-1, 7 *", "CodeBleu_score": 32.054981336668654}, {"code": "def build_model(x_train, num_classes)\n\t\n    # Reset the TensorFlow graph\n    tf.reset_default_graph()\n\n    # Define the input layer\n    X_input = tf.placeholder(tf.float32, shape = (None, x_train.shape[1], x_train.shape[2], x_train.shape[3]))\n\n    # Define the first convolutional layer\n    conv1 = tf.layers.conv2d(inputs = X_input, filters = 32, kernel_size = 5, strides = 1, padding = 'same', activation = tf.nn.relu)\n\n    # Define the first max-pooling layer\n    pool1 = tf.layers.max_pooling2d(inputs = conv1, pool_size = 2, strides = 2)\n\n    # Define the second convolutional layer\n    conv2 = tf.layers.conv2d(inputs = pool1, filters = 64, kernel_size = 5, strides = 1, padding = 'same', activation = tf.nn.relu)\n\n    # Define the second max-pooling layer\n    pool2 = tf.layers.max_pooling2d(inputs = conv2, pool_size = 2, strides = 2)\n\n    # Flatten the output of the second convolutional layer\n    pool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 64])\n\n    # Define the first dense layer\n    dense1 = tf.layers.dense(inputs = pool2_flat, units = 1024, activation = tf.nn.relu)\n\n    # Define the second dense layer\n    logits = tf.layers.dense(inputs = dense1, units = num_classes)\n\n    # Define the output layer\n    y_pred = tf.nn.softmax(logits)\n\n    # Define the model\n    model = tf.estimator.Estimator(model_fn = model_fn)\n\n    return model\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 24.047621223206477}, {"code": "def build_model(x_train, num_classes)\n\t\n    # Reset the graph\n    tf.reset_default_graph()\n\n    # Define the input layer\n    X = tf.placeholder(tf.float32, shape=(None, x_train.shape[1], x_train.shape[2], x_train.shape[3]))\n    y = tf.placeholder(tf.int32, shape=(None))\n\n    # Define the convolutional layers\n    conv1 = tf.layers.conv2d(inputs=X, filters=32, kernel_size=[5, 5], padding=\"same\", activation=tf.nn.relu)\n    conv2 = tf.layers.conv2d(inputs=conv1, filters=64, kernel_size=[5, 5], padding=\"same\", activation=tf.nn.relu)\n\n    # Define the max-pooling layer\n    pool = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n\n    # Flatten the output of the convolutional layers\n    pool_flat = tf.reshape(pool, [-1, 7 * 7 * 64])\n\n    # Define the dense layers\n    dense = tf.layers.dense(inputs=pool_flat, units=1024, activation=tf.nn.relu)\n    dropout = tf.layers.dropout(inputs=dense, rate=0.4)\n    logits = tf.layers.dense(inputs=dropout, units=num_classes)\n\n    # Define the model\n    model = tf.estimator.Estimator(model_fn=cnn_model_fn, model_dir=\"./cnn_model\")\n\n    return model\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 27.17869644723298}]}
{"instance_id": "813e3e2844f4", "generation_result": [{"code": "def extract_bboxes(mask)\n\t\n    # if mask is None:\n    #     return None\n\n    # mask = np.squeeze(mask)\n    # if len(mask.shape) == 3:\n    #     mask = np.squeeze(mask[:, :, 0])\n    # mask_shape = mask.shape\n    # bbox = np.zeros([mask_shape[0], mask_shape[1], 4], dtype=np.int16)\n    # for i, m in enumerate(mask):\n    #     for j, n in enumerate(m):\n    #         if n:\n    #             row_min = i\n    #             row_max = i\n    #             col_min = j\n    #             col_max = j\n    #             while row_min >= 0 and mask[row_min, j]:\n    #                 row_min -= 1\n    #             while row_max < mask_shape[0] and mask[row_max, j]:\n    #                 row_max += 1\n    #             while col_min >= 0 and mask[i, col_min]:\n    #                 col_min -= 1\n    #             while col_max < mask_shape[1] and mask[i, col_max]:\n    #                 col_max += 1\n    #             bbox[i, j] = np.array(\n    #                 [row_min, row_max, col_min, col_max], dtype=np.int16)\n    # return bbox\n\n", "CodeBleu_score": 0.29411764705882354}, {"code": "def extract_bboxes(mask)\n\t\n    # if mask.shape[-1] != 1:\n    #     raise ValueError(\"Invalid mask shape: {}\".format(mask.shape))\n\n    bbox = []\n    for i in range(mask.shape[-1]):\n        m = mask[:, :, i]\n        # Bounding box.\n        horizontal_indices = np.where(np.any(m, axis=0))[0]\n        vertical_indices = np.where(np.any(m, axis=1))[0]\n        if horizontal_indices.shape[0]:\n            x1, x2 = horizontal_indices[[0, -1]]\n            y1, y2 = vertical_indices[[0, -1]]\n            # x2 and y2 should not be part of the box. Increment by 1.\n            x2 += 1\n            y2 += 1\n        else:\n            # No mask for this instance. Might happen due to\n            # resizing or cropping. Set bbox to zeros\n            x1, x2, y1, y2 = 0, 0, 0, 0\n        bbox.append([y1, x1, y2, x2])\n    return np.array(bbox).astype(np.int32)\n\n", "CodeBleu_score": 63.400616896168536}, {"code": "def extract_bboxes(mask)\n\t\n\n    # if mask is None:\n    #     return None\n\n    # mask = np.squeeze(mask)\n    # if len(mask.shape) == 3:\n    #     mask = np.squeeze(mask[:, :, 0])\n    # bbox = []\n    # for i, mask_layer in enumerate(mask):\n    #     binary_mask = mask_layer.astype(bool)\n    #     if not binary_mask.any():\n    #         bbox.append([0, 0, 0, 0])\n    #         continue\n    #     vertical_indices = np.where(np.any(binary_mask, axis=0))[0]\n    #     horizontal_indices = np.where(np.any(binary_mask, axis=1))[0]\n    #     assert vertical_indices.shape[0], 'No mask for mask id {}'.format(i)\n    #     x1, x2 = vertical_indices[[0, -1]]\n    #     y1, y2 = horizontal_indices[[0, -1]]\n    #     bbox.append([x1, y1, x2, y2])\n    # bbox = np.array(bbox)\n    # return bbox\n    pass\n\n", "CodeBleu_score": 0.29411764705882354}]}
{"instance_id": "7ad5d430d6ed", "generation_result": [{"code": "def compute_iou(box, boxes, box_area, boxes_area)\n\t\n    # box: [y1, x1, y2, x2]\n    # boxes: [N, (y1, x1, y2, x2)]\n    # box_area: float. the area of 'box'\n    # boxes_area: array of length N. the areas of 'boxes'\n\n    # Calculate intersection areas\n    y1 = np.maximum(box[0], boxes[:, 0])\n    y2 = np.minimum(box[2], boxes[:, 2])\n    x1 = np.maximum(box[1], boxes[:, 1])\n    x2 = np.minimum(box[3], boxes[:, 3])\n    intersection = np.maximum(y2 - y1, 0) * np.maximum(x2 - x1, 0)\n    union = box_area + boxes_area[:] - intersection[:]\n    iou = intersection / union\n    return iou\n\n", "CodeBleu_score": 56.12768883353916}, {"code": "def compute_iou(box, boxes, box_area, boxes_area)\n\t\n    # Calculate intersection areas\n    y1 = np.maximum(box[0], boxes[:, 0])\n    y2 = np.minimum(box[2], boxes[:, 2])\n    x1 = np.maximum(box[1], boxes[:, 1])\n    x2 = np.minimum(box[3], boxes[:, 3])\n    intersection = np.maximum(y2 - y1, 0) * np.maximum(x2 - x1, 0)\n    union = box_area + boxes_area[:] - intersection[:]\n    iou = intersection / union\n    return iou\n\n", "CodeBleu_score": 56.12768883353916}, {"code": "def compute_iou(box, boxes, box_area, boxes_area)\n\t\n    # Calculate intersection areas\n    y1 = np.maximum(box[0], boxes[:, 0])\n    y2 = np.minimum(box[2], boxes[:, 2])\n    x1 = np.maximum(box[1], boxes[:, 1])\n    x2 = np.minimum(box[3], boxes[:, 3])\n    intersection = np.maximum(y2 - y1, 0) * np.maximum(x2 - x1, 0)\n    union = box_area + boxes_area[:] - intersection[:]\n    iou = intersection / union\n    return iou\n\n", "CodeBleu_score": 56.12768883353916}]}
{"instance_id": "15941b852859", "generation_result": [{"code": "def compute_overlaps(boxes1, boxes2)\n\t\n    # if there are no boxes, return an empty result\n    if boxes1.shape[0] == 0 or boxes2.shape[0] == 0:\n        return np.zeros((boxes1.shape[0], boxes2.shape[0]))\n\n    # if the boxes are the same, return 1 as the iou value\n    if boxes1 is boxes2:\n        return np.ones((boxes1.shape[0], boxes1.shape[0]))\n\n    # compute the area of each box in both sets\n    area1 = (boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1])\n    area2 = (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1])\n\n    # compute the intersection areas\n    xx1 = np.maximum(boxes1[:, 0], boxes2[:, 0].T)\n    yy1 = np.maximum(boxes1[:, 1], boxes2[:, 1].T)\n    xx2 = np.minimum(boxes1[:, 2], boxes2[:, 2].T)\n    yy2 = np.minimum(boxes1[:, 3], boxes2[:, 3].T)\n    w = np.maximum(0, xx2 - xx1)\n    h = np.maximum(0, yy2 - yy1)\n    intersection = w * h\n\n    # compute the union areas\n    union = area1[:, None] + area2[None, :] - intersection\n\n    # compute the intersection over union\n    iou = intersection / union\n\n    # return the intersection over union matrix\n    return iou\n\n", "CodeBleu_score": 52.759267996887395}, {"code": "def compute_overlaps(boxes1, boxes2)\n\t\n    # if there are no boxes, return an empty result\n    if boxes1.shape[0] == 0 or boxes2.shape[0] == 0:\n        return np.zeros((boxes1.shape[0], boxes2.shape[0]))\n\n    # if the boxes are vectors, convert them to matrices\n    if len(boxes1.shape) == 1:\n        boxes1 = np.expand_dims(boxes1, 0)\n    if len(boxes2.shape) == 1:\n        boxes2 = np.expand_dims(boxes2, 0)\n\n    # compute area of each box in both boxes1 and boxes2\n    area1 = (boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1])\n    area2 = (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1])\n\n    # compute the intersection areas\n    xx1 = np.maximum(boxes1[:, 0], boxes2[:, 0])\n    yy1 = np.maximum(boxes1[:, 1], boxes2[:, 1])\n    xx2 = np.minimum(boxes1[:, 2], boxes2[:, 2])\n    yy2 = np.minimum(boxes1[:, 3], boxes2[:, 3])\n    w = np.maximum(0, xx2 - xx1)\n    h = np.maximum(0, yy2 - yy1)\n    inter = w * h\n\n    # compute the union areas\n    union = area1 + area2 - inter\n\n    # compute the overlaps\n    overlaps = inter / union\n    return overlaps\n\n", "CodeBleu_score": 52.76409627580514}, {"code": "def compute_overlaps(boxes1, boxes2)\n\t\n    # if there are no boxes, return an empty result\n    if boxes1.shape[0] == 0 or boxes2.shape[0] == 0:\n        return np.zeros((boxes1.shape[0], boxes2.shape[0]))\n\n    # if the boxes are empty, return an empty result\n    if len(boxes1) == 0 or len(boxes2) == 0:\n        return np.zeros((len(boxes1), len(boxes2)))\n\n    # compute area of each box in both sets\n    area1 = (boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1])\n    area2 = (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1])\n\n    # compute the intersection area of each combination of boxes in both sets\n    xx1 = np.maximum(boxes1[:, 0], boxes2[:, 0].T)\n    yy1 = np.maximum(boxes1[:, 1], boxes2[:, 1].T)\n    xx2 = np.minimum(boxes1[:, 2], boxes2[:, 2].T)\n    yy2 = np.minimum(boxes1[:, 3], boxes2[:, 3].T)\n    w = np.maximum(0, xx2 - xx1)\n    h = np.maximum(0, yy2 - yy1)\n    inter = w * h\n    # compute the union area of each combination of boxes in both sets\n    union = area1[:, None] + area2[None, :] - inter\n    # compute the intersection over union by taking the ratio of intersection to union areas\n    iou = inter / union\n    # return the intersection over union value\n    return iou\n\n", "CodeBleu_score": 54.186623273720414}]}
{"instance_id": "597533722ce3", "generation_result": [{"code": "def load_image(self, image_id):\n\t\n\n    image_info = self.image_info[image_id]\n    if image_info[\"source\"] != \"rgb\":\n        return skimage.io.imread(image_info[\"path\"])\n    image = skimage.io.imread(image_info[\"path\"])\n    if image.ndim != 3:\n        image = skimage.color.gray2rgb(image)\n    if image.shape[-1] == 4:\n        image = image[..., :3]\n    return image\n\n", "CodeBleu_score": 76.2525636077214}, {"code": "def load_image(self, image_id):\n\t\n\n    image = skimage.io.imread(self.image_info[image_id]['path'])\n\n    if image.ndim != 3:\n        warnings.warn('Image ' + str(image_id) + ' is not RGB. Converting to RGB.')\n        if image.ndim == 2:\n            image = skimage.color.gray2rgb(image)\n        if image.shape[-1] == 4:\n            image = image[..., :3]\n\n    return image\n\n", "CodeBleu_score": 83.02506405370335}, {"code": "def load_image(self, image_id):\n\t\n    info = self.image_info[image_id]\n    image = skimage.io.imread(info['path'])\n    if image.ndim != 3:\n        warnings.warn('Image ' + image_id + ' is not RGB')\n        image = skimage.color.gray2rgb(image)\n    if image.shape[-1] == 4:\n        image = image[..., :3]\n    return image\n\n", "CodeBleu_score": 75.2637536755988}]}
{"instance_id": "9b852997ca05", "generation_result": [{"code": "def display_images(images, titles=None, cols=4, cmap=None, norm=None,\n                   interpolation=None)\n\t\n    titles = titles if titles is not None else [\"Image (%d)\" % i\n                                                for i in range(1, len(images) + 1)]\n    rows = len(images) // cols + 1\n    plt.figure(figsize=(14, 14 * rows // cols))\n    i = 1\n    for image, title in zip(images, titles):\n        plt.subplot(rows, cols, i)\n        plt.title(title, fontsize=9)\n        plt.axis('off')\n        plt.imshow(image.astype(np.uint8), cmap=cmap,\n                   norm=norm, interpolation=interpolation)\n        i += 1\n    plt.show()\n\n", "CodeBleu_score": 71.8647826847557}, {"code": "def display_images(images, titles=None, cols=4, cmap=None, norm=None,\n                   interpolation=None)\n\t\n    titles = titles if titles is not None else [\"Image (%d)\" % i for i in range(len(images))]\n    rows = len(images) // cols + 1\n    plt.figure(figsize=(14, 14 * rows // cols))\n    i = 1\n    for image, title in zip(images, titles):\n        plt.subplot(rows, cols, i)\n        plt.title(title, fontsize=9)\n        plt.axis('off')\n        plt.imshow(image.astype(np.uint8), cmap=cmap,\n                   norm=norm, interpolation=interpolation)\n        i += 1\n    plt.show()\n\n", "CodeBleu_score": 74.53110451250471}, {"code": "def display_images(images, titles=None, cols=4, cmap=None, norm=None,\n                   interpolation=None)\n\t\n    titles = titles if titles is not None else [\"Image (%d)\" % i for i in range(len(images))]\n    rows = len(images) // cols + 1\n    plt.figure(figsize=(14, 14 * rows // cols))\n    i = 1\n    for image, title in zip(images, titles):\n        plt.subplot(rows, cols, i)\n        plt.title(title, fontsize=9)\n        plt.axis('off')\n        plt.imshow(image.astype(np.uint8), cmap=cmap,\n                   norm=norm, interpolation=interpolation)\n        i += 1\n    plt.show()\n\n", "CodeBleu_score": 74.53110451250471}]}
