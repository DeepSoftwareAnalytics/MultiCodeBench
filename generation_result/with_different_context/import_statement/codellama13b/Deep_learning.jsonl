{"instance_id": "1f9f4d85e825", "generation_result": [{"code": "def load_model_from_config(self, half_attention)\n\t\n    # Load model from cache if available\n    if self.model_cache_path is not None and os.path.isfile(self.model_cache_path):\n        print('Loading cached model...')\n        model = torch.load(self.model_cache_path, map_location=self.device)\n        # Check if model is valid\n        if model['valid']:\n            print('Model loaded from cache!')\n            # Set model to evaluation mode\n            model['model'].eval()\n            # Set model to half precision if requested\n            if half_attention:\n                model['model'].half()\n            # Set memory format to channels last if requested\n            if self.memory_format == 'channels_last':\n                model['model'].to_channels_last()\n            # Return model\n            return model\n        else:\n            print('Cached model invalid, reloading from path...')\n\n    # Load model from path\n    print('Loading model from path...')\n    if self.model_path.endswith('.safetensors'):\n        model = torch.load(self.model_path, map_location=self.device)\n    else:\n        model = torch.load(self.model_path, map_location=self.device, pickle_module=safetensors.torch)\n\n    # Set model to evaluation mode\n    model.eval()\n    # Set model to half precision if requested\n    if half_attention:\n        model.half()\n    # Set memory format to channels last if requested\n    if self.memory_format == 'channels_last':\n        model.to_channels_last()\n\n    # Cache model if requested\n    if self.model_cache_path is not None:\n        print('Caching model...')\n        torch.save({'model': model, 'valid': True}, self.model_cache_path)\n\n    # Return model\n    return model\n\n", "CodeBleu_score": 35.57609226112752}, {"code": "def load_model_from_config(self, half_attention)\n\t\n    if self.config.model.cache_model and os.path.exists(self.config.model.cache_path) and os.path.isfile(self.config.model.cache_path):\n        model = torch.load(self.config.model.cache_path, map_location=self.device)\n    else:\n        model = instantiate_from_config(self.config.model)\n        if self.config.model.path.endswith(\".safetensors\"):\n            model.load_state_dict(torch.load(self.config.model.path, map_location=self.device))\n        else:\n            model.load_state_dict(torch.load(self.config.model.path, map_location=self.device)[\"state_dict\"])\n    if self.config.model.half_attention:\n        model.half()\n    if self.config.model.memory_format == \"channels_last\":\n        model = model.to(memory_format=torch.channels_last)\n    model.eval()\n    if self.config.model.cache_model:\n        torch.save(model, self.config.model.cache_path)\n    return model\n\n", "CodeBleu_score": 30.772523631854483}, {"code": "def load_model_from_config(self, half_attention)\n\t\n    # Load model from cache if available\n    model_path = os.path.join(self.cache_dir, self.model_name)\n    if os.path.isfile(model_path):\n        self.logger.info(f\"Loading model from cache: {model_path}\")\n        model = torch.load(model_path, map_location=devices.map_location)\n        if self.model_name == \"ddim.pt\":\n            model = DDIMSampler(\n                model.model,\n                model.diffusion,\n                model.ema_model,\n                model.ema_diffusion,\n                model.ema_params,\n                model.config.timestep_respacing,\n                model.config.model_type,\n                model.config.diffusion_steps,\n                model.config.noise_schedule,\n                model.config.use_kl,\n                model.config.predict_xstart,\n                model.config.rescale_timesteps,\n                model.config.rescale_learned_sigmas,\n                model.config.use_checkpoint_ema,\n                model.config.ema_rate,\n                model.config.ema_eval,\n                model.config.use_scale_shift_norm,\n                model.config.resblock_updown,\n                model.config.use_fp16,\n                model.config.fp16_scale_given,\n                model.config.fp16_scale,\n                model.config.attention_resolutions,\n                model.config.padding_mode,\n                model.config.resblock_updown,\n                model.config.use_new_attention_order,\n                model.config.use_checkpoint_ema,\n                model.config.ema_rate,\n                model.config.ema_eval,\n                model.config.use_scale_shift_norm,\n                model.config.resblock_updown,\n                model.config.use_new_attention_order,\n                model.config.use_checkpoint_ema,\n                model.config.ema_rate,\n                model.config.ema_eval,\n                model.config.use", "CodeBleu_score": 22.395094423093454}]}
{"instance_id": "8dc955007aa2", "generation_result": [{"code": "def convsample_ddim(model, cond, steps, shape, eta=1.0, callback=None, normals_sequence=None,\n                    mask=None, x0=None, quantize_x0=False, temperature=1., score_corrector=None,\n                    corrector_kwargs=None, x_t=None\n                    )\n\t\n    print(f\"Sampling with eta {eta:.3f} and {steps} steps\")\n    batch_size = shape[0]\n    device = next(model.parameters()).device\n    sampler = DDIMSampler(model, batch_size=batch_size, device=device)\n    samples = sampler.sample(steps, shape, cond=cond, eta=eta, callback=callback,\n                             normals_sequence=normals_sequence, mask=mask, x0=x0,\n                             quantize_x0=quantize_x0, temperature=temperature,\n                             score_corrector=score_corrector,\n                             corrector_kwargs=corrector_kwargs, x_t=x_t)\n    return samples\n\n", "CodeBleu_score": 52.1090676579043}, {"code": "def convsample_ddim(model, cond, steps, shape, eta=1.0, callback=None, normals_sequence=None,\n                    mask=None, x0=None, quantize_x0=False, temperature=1., score_corrector=None,\n                    corrector_kwargs=None, x_t=None\n                    )\n\t\n    print(f'eta: {eta:.3f}')\n    print(f'steps: {steps}')\n    batch_size = shape[0]\n    sampler = DDIMSampler(model, batch_size=batch_size, device=devices.get_device())\n    return sampler.sample(steps, shape, cond, eta=eta, callback=callback,\n                          normals_sequence=normals_sequence, mask=mask, x0=x0,\n                          quantize_x0=quantize_x0, temperature=temperature,\n                          score_corrector=score_corrector, corrector_kwargs=corrector_kwargs,\n                          x_t=x_t)\n\n", "CodeBleu_score": 39.366014156570905}, {"code": "def convsample_ddim(model, cond, steps, shape, eta=1.0, callback=None, normals_sequence=None,\n                    mask=None, x0=None, quantize_x0=False, temperature=1., score_corrector=None,\n                    corrector_kwargs=None, x_t=None\n                    )\n\t\n    print(f'eta: {eta:.3f}')\n    print(f'steps: {steps}')\n    batch_size = shape[0]\n    device = next(model.parameters()).device\n    sampler = DDIMSampler(model, batch_size=batch_size, device=device)\n    samples = sampler.sample(steps, shape, cond, eta, callback=callback,\n                             normals_sequence=normals_sequence, mask=mask, x0=x0, quantize_x0=quantize_x0,\n                             temperature=temperature, score_corrector=score_corrector,\n                             corrector_kwargs=corrector_kwargs, x_t=x_t)\n    return samples\n\n", "CodeBleu_score": 50.09132679704473}]}
{"instance_id": "9bdcd9081480", "generation_result": [{"code": "def ema_scope(self, context=None)\n\t\n    if self.ema is None:\n        return\n    if context is None:\n        context = self\n    if self.ema.use_ema:\n        self.ema.store(self.ema.ema_params)\n        self.ema.copy_to()\n        if hasattr(context, \"print\") and context.print:\n            print(\"EMA enabled\")\n    else:\n        if hasattr(context, \"print\") and context.print:\n            print(\"EMA disabled\")\n\n", "CodeBleu_score": 34.39978029864535}, {"code": "def ema_scope(self, context=None)\n\t\n    if self.ema is not None:\n        if context is not None:\n            print(f\"{context}: Switching to EMA weights.\")\n        self.model.load_state_dict(self.ema.get_current_dict(), strict=True)\n        yield\n        if context is not None:\n            print(f\"{context}: Restoring original weights.\")\n        self.model.load_state_dict(self.ema.get_original_dict(), strict=True)\n    else:\n        yield\n\n", "CodeBleu_score": 60.66744441603766}, {"code": "def ema_scope(self, context=None)\n\t\n    if self.ema_enabled:\n        if context is not None:\n            print(f\"{context}: switching to EMA weights.\")\n        self.model.load_state_dict(self.ema.ema.state_dict())\n        yield\n        if context is not None:\n            print(f\"{context}: restoring original weights.\")\n        self.model.load_state_dict(self.ema.model.state_dict())\n    else:\n        yield\n\n", "CodeBleu_score": 60.45337773359748}]}
{"instance_id": "14ab009e1c5c", "generation_result": [{"code": "def init_from_ckpt(self, path, ignore_keys=None)\n\t\n    if not path:\n        return\n\n    if version.parse(torch.__version__) >= version.parse(\"1.6.0\"):\n        state_dict = torch.load(path, map_location=\"cpu\")[\"state_dict\"]\n    else:\n        state_dict = torch.load(path, map_location=\"cpu\")\n\n    if ignore_keys is not None:\n        for k in ignore_keys:\n            del state_dict[k]\n\n    self.load_state_dict(state_dict, strict=False)\n    return self\n\n", "CodeBleu_score": 26.586995089908143}, {"code": "def init_from_ckpt(self, path, ignore_keys=None)\n\t\n    if not path:\n        return\n\n    # Load the state_dict from checkpoint file.\n    state_dict = torch.load(path, map_location=\"cpu\")[\"state_dict\"]\n\n    # Delete any unwanted keys.\n    if ignore_keys is not None:\n        for key in ignore_keys:\n            del state_dict[key]\n\n    # Update the model's state_dict with the loaded state_dict.\n    self.load_state_dict(state_dict, strict=False)\n\n    # Print out any missing or unexpected keys.\n    missing_keys = self.load_state_dict(state_dict, strict=True).missing_keys\n    unexpected_keys = self.load_state_dict(state_dict, strict=True).unexpected_keys\n    if len(missing_keys) > 0:\n        print(f\"Missing keys: {missing_keys}\")\n    if len(unexpected_keys) > 0:\n        print(f\"Unexpected keys: {unexpected_keys}\")\n\n", "CodeBleu_score": 40.592676357634836}, {"code": "def init_from_ckpt(self, path, ignore_keys=None)\n\t\n    # Load the state dictionary\n    state_dict = torch.load(path, map_location=\"cpu\")[\"state_dict\"]\n    # Remove any ignored keys\n    if ignore_keys is not None:\n        for k in ignore_keys:\n            del state_dict[k]\n    # Update the model's state dictionary\n    self.load_state_dict(state_dict, strict=False)\n    # Print out any missing or unexpected keys\n    missing_keys = [k for k in self.state_dict().keys() if k not in state_dict.keys()]\n    unexpected_keys = [k for k in state_dict.keys() if k not in self.state_dict().keys()]\n    if len(missing_keys) > 0:\n        print(f\"Missing keys: {missing_keys}\")\n    if len(unexpected_keys) > 0:\n        print(f\"Unexpected keys: {unexpected_keys}\")\n\n", "CodeBleu_score": 38.620750636294126}]}
{"instance_id": "754176e27b81", "generation_result": [{"code": "def forward(self, input, return_pred_indices=False)\n\t\n    assert input.dim() == 5, 'Input must be 5D (B, C, T, H, W)'\n    batch_size = input.shape[0]\n    input = input.view(-1, *input.shape[2:])\n    z, z_indices = self.encoder(input)\n    z = z.view(batch_size, -1, *z.shape[1:])\n    out = self.decoder(z)\n    out = out.view(batch_size, *out.shape[1:])\n    if return_pred_indices:\n        return out, z_indices\n    return out\n\n", "CodeBleu_score": 35.97992070579159}, {"code": "def forward(self, input, return_pred_indices=False)\n\t\n    z, z_indices = self.encode(input)\n    x_hat = self.decode(z)\n    if return_pred_indices:\n        return x_hat, z_indices\n    return x_hat\n\n", "CodeBleu_score": 30.55873204286945}, {"code": "def forward(self, input, return_pred_indices=False)\n\t\n    z = self.encoder(input)\n    x_recon = self.decoder(z)\n    loss = self.loss_function(x_recon, input)\n    if return_pred_indices:\n        return x_recon, loss, z.argmax(dim=1)\n    return x_recon, loss\n\n", "CodeBleu_score": 24.58786745936975}]}
{"instance_id": "443df6fdaf0d", "generation_result": [{"code": "def get_input(self, batch, k)\n\t\n    x = batch[k]\n    if x.dim() == 2:\n        x = x.unsqueeze(2).unsqueeze(3)\n    if x.dim() == 3:\n        x = x.unsqueeze(1)\n    if self.resize_batch and self.batch_size_range is not None:\n        x = self.resize_fn(x, self.batch_size_range, self.global_step)\n    x = x.permute(0, 2, 3, 1).contiguous().float()\n    x.requires_grad_(False)\n    return x\n\n", "CodeBleu_score": 30.68832803750241}, {"code": "def get_input(self, batch, k)\n\t\n    x = batch[k]\n    if x.ndim == 3:\n        x = x.unsqueeze(0)\n    if x.ndim == 2:\n        x = x.unsqueeze(1)\n    if x.ndim == 4 and x.size(1) == 1:\n        x = x.repeat(1, 3, 1, 1)\n    if self.resize_batch:\n        size = x.size()\n        if self.resize_batch_t == 0:\n            self.resize_batch_t = self.resize_batch_t_max\n        else:\n            self.resize_batch_t -= self.resize_batch_t_max / self.resize_batch_size\n        size = list(size)\n        size[2] = int(size[2] * self.resize_batch_t)\n        size[3] = int(size[3] * self.resize_batch_t)\n        x = F.interpolate(x, size=size, mode=\"bicubic\", align_corners=False)\n    x = x.permute(0, 2, 3, 1).contiguous().float()\n    return x.detach()\n\n", "CodeBleu_score": 42.920210384602484}, {"code": "def get_input(self, batch, k)\n\t\n    x = batch[k]\n    if x.dim() == 3:\n        x = x.unsqueeze(0)\n    if x.dim() == 2:\n        x = x.unsqueeze(0).unsqueeze(0)\n    if x.dim() != 4:\n        raise ValueError(f\"Input {k} has {x.dim()} dimensions, but must have 4 dimensions.\")\n    if self.resize_batch and self.resize_batch_size > 1:\n        if self.resize_batch_size != self.batch_size:\n            x = F.interpolate(x, size=self.batch_size, mode=\"bicubic\")\n    x = x.permute(0, 2, 3, 1).contiguous().float()\n    return x.detach()\n\n", "CodeBleu_score": 35.72327127944805}]}
{"instance_id": "953a2632fc15", "generation_result": [{"code": "def training_step(self, batch, batch_idx, optimizer_idx)\n\t\n    if self.hparams.train_on_gpu:\n        batch = tuple(t.cuda(non_blocking=True) for t in batch)\n    x, y = batch\n    if self.hparams.discriminator_loss_weight > 0:\n        x_hat, z = self.forward(x)\n        if self.hparams.discriminator_loss_weight > 0:\n            discriminator_loss = self.discriminator_loss(x_hat, y)\n            self.log(\n                \"train/discriminator_loss\",\n                discriminator_loss,\n                on_step=True,\n                on_epoch=True,\n                prog_bar=True,\n            )\n            if optimizer_idx == 0:\n                return discriminator_loss\n        if self.hparams.autoencoder_loss_weight > 0:\n            autoencoder_loss = self.autoencoder_loss(x_hat, x)\n            self.log(\n                \"train/autoencoder_loss\",\n                autoencoder_loss,\n                on_step=True,\n                on_epoch=True,\n                prog_bar=True,\n            )\n            if optimizer_idx == 1:\n                return autoencoder_loss\n    else:\n        x_hat, z = self.forward(x)\n        autoencoder_loss = self.autoencoder_loss(x_hat, x)\n        self.log(\n            \"train/autoencoder_loss\",\n            autoencoder_loss,\n            on_step=True,\n            on_epoch=True,\n            prog_bar=True,\n        )\n        return autoencoder_loss\n\n", "CodeBleu_score": 34.19504427444088}, {"code": "def training_step(self, batch, batch_idx, optimizer_idx)\n\t\n    x, y = batch\n    x = x.to(self.device)\n    y = y.to(self.device)\n    x_hat, z = self.forward(x)\n    if optimizer_idx == 0:\n        loss = self.loss(x, x_hat)\n        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n        return loss\n    else:\n        loss = self.loss(y, x_hat)\n        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n        return loss\n\n", "CodeBleu_score": 15.29561069828681}, {"code": "def training_step(self, batch, batch_idx, optimizer_idx)\n\t\n    x, y = batch\n    x = x.to(self.device)\n    y = y.to(self.device)\n    self.current_epoch = self.trainer.current_epoch\n    self.global_step = self.trainer.global_step\n\n    if self.hparams.use_amp:\n        with torch.cuda.amp.autocast():\n            x_hat, x_hat_uncond, x_hat_uncond_no_quant, x_hat_uncond_no_quant_no_conv = self(x)\n            loss = self.loss_function(x, x_hat, x_hat_uncond, x_hat_uncond_no_quant, x_hat_uncond_no_quant_no_conv, y)\n    else:\n        x_hat, x_hat_uncond, x_hat_uncond_no_quant, x_hat_uncond_no_quant_no_conv = self(x)\n        loss = self.loss_function(x, x_hat, x_hat_uncond, x_hat_uncond_no_quant, x_hat_uncond_no_quant_no_conv, y)\n\n    if optimizer_idx == 0:\n        self.log_dict({f\"train/loss\": loss}, on_step=True, on_epoch=True)\n        self.log_dict({f\"train/recon_loss\": loss.recon_loss}, on_step=True, on_epoch=True)\n        self.log_dict({f\"train/kl_loss\": loss.kl_loss}, on_step=True, on_epoch=True)\n        self.log_dict({f\"train/nll\": loss.nll}, on_step=True, on_epoch=True)\n        self.log_dict({f\"train/bpd\": loss.bpd}, on_step=True, on_epoch=True)\n    elif optimizer_idx == 1:\n        self.log_dict({f\"train/disc_loss\": loss}, on_step=True, on_epoch=True)\n        self.log", "CodeBleu_score": 28.925338794259787}]}
{"instance_id": "b449b5e8b77f", "generation_result": [{"code": "def log_images(self, batch, only_inputs=False, plot_ema=False, **kwargs)\n\t\n    # Inputs\n    x = batch\n    if self.hparams.input_channels == 1:\n        x = x.unsqueeze(1)\n    x = x.to(self.device)\n    x = 2 * x - 1.0\n    x = x.permute(0, 2, 3, 1)\n    if self.hparams.log_inputs:\n        self.logger.experiment.add_images(\"inputs\", x, self.current_epoch)\n    if only_inputs:\n        return\n\n    # Reconstructions\n    if self.ema is not None and plot_ema:\n        with torch.no_grad():\n            self.ema.store(self.model.parameters())\n            self.ema.copy_to()\n            out = self.model(x)\n            self.ema.restore()\n    else:\n        out = self.model(x)\n\n    if self.hparams.log_reconstructions:\n        if self.hparams.log_inputs:\n            self.logger.experiment.add_images(\n                \"reconstructions\", out, self.current_epoch, dataformats=\"CHW\"\n            )\n        else:\n            self.logger.experiment.add_images(\n                \"reconstructions\", out, self.current_epoch, dataformats=\"CHW\"\n            )\n\n", "CodeBleu_score": 34.075588322522}, {"code": "def log_images(self, batch, only_inputs=False, plot_ema=False, **kwargs)\n\t\n    # process input data\n    if self.hparams.colorize:\n        batch = batch.float() / 255.\n    else:\n        batch = batch.float()\n    if self.hparams.channels_last:\n        batch = batch.permute(0, 3, 1, 2)\n\n    # log inputs\n    self.logger.experiment.add_images(\n        \"inputs\",\n        batch,\n        self.current_epoch,\n        dataformats=\"NCHW\" if self.hparams.channels_last else \"NHWC\",\n    )\n\n    # log reconstructions\n    if not only_inputs:\n        if plot_ema:\n            with torch.no_grad():\n                # plot reconstructions with EMA weights\n                self.model.eval()\n                ema_model = LitEma(self.model, self.ema_model)\n                ema_model.eval()\n                ema_model.freeze()\n                ema_reconstructions = ema_model(batch)\n                self.model.train()\n        else:\n            # plot reconstructions with current weights\n            ema_reconstructions = self.model(batch)\n\n        # colorize reconstructions\n        if self.hparams.colorize:\n            ema_reconstructions = ema_reconstructions.permute(\n                0, 2, 3, 1\n            )  # NHWC -> NCHW\n            ema_reconstructions = ema_reconstructions * 255\n\n        self.logger.experiment.add_images(\n            \"reconstructions\",\n            ema_reconstructions,\n            self.current_epoch,\n            dataformats=\"NCHW\" if self.hparams.channels_last else \"NHWC\",\n        )\n\n", "CodeBleu_score": 30.37418401009494}, {"code": "def log_images(self, batch, only_inputs=False, plot_ema=False, **kwargs)\n\t\n\n    # Process input data\n    if self.hparams.log_inputs:\n        if self.hparams.log_colorize:\n            batch = self.colorize(batch)\n        if self.hparams.log_normalize:\n            batch = self.normalize(batch)\n\n        # Log inputs\n        self.logger.experiment.add_images(\n            \"inputs\",\n            batch,\n            global_step=self.global_step,\n            dataformats=\"NCHW\",\n            **kwargs,\n        )\n\n    # Log reconstructions\n    if not only_inputs:\n        if self.hparams.log_colorize:\n            batch = self.colorize(batch)\n        if self.hparams.log_normalize:\n            batch = self.normalize(batch)\n\n        # Log reconstructions\n        self.logger.experiment.add_images(\n            \"reconstructions\",\n            batch,\n            global_step=self.global_step,\n            dataformats=\"NCHW\",\n            **kwargs,\n        )\n\n        # Log reconstructions using EMA weights\n        if plot_ema:\n            with torch.no_grad():\n                self.eval()\n                if self.hparams.ema_decay > 0:\n                    self.ema.store(self.model.parameters())\n                    self.ema.copy_to(self.model.parameters())\n                out = self.forward(batch)\n                if self.hparams.ema_decay > 0:\n                    self.ema.restore(self.model.parameters())\n                self.train()\n            if self.hparams.log_colorize:\n                out = self.colorize(out)\n            if self.hparams.log_normalize:\n                out = self.normalize(out)\n            self.logger.experiment.add_images(\n                \"reconstructions_ema\",\n                out,\n                global_step=self.global_step,\n                dataformats=\"NCHW\",\n                **kwargs,\n            )\n\n", "CodeBleu_score": 28.319765276419805}]}
{"instance_id": "540d089e3481", "generation_result": [{"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss_type == 'l1':\n        if mean:\n            return torch.mean(torch.abs(target - pred))\n        else:\n            return torch.mean(torch.abs(target - pred), dim=(1, 2, 3))\n    elif self.loss_type == 'l2':\n        if mean:\n            return torch.mean((target - pred) ** 2)\n        else:\n            return torch.mean((target - pred) ** 2, dim=(1, 2, 3))\n    else:\n        raise ValueError(f'Unknown loss type: {self.loss_type}')\n\n", "CodeBleu_score": 34.296256838358914}, {"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss_type == \"l1\":\n        if mean:\n            return torch.mean(torch.abs(pred - target))\n        else:\n            return torch.mean((pred - target) ** 2)\n    elif self.loss_type == \"l2\":\n        if mean:\n            return torch.mean((pred - target) ** 2)\n        else:\n            return (pred - target) ** 2\n    else:\n        raise ValueError(f\"Unknown loss type: {self.loss_type}\")\n\n", "CodeBleu_score": 30.076823793152556}, {"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss_type == 'l1':\n        if mean:\n            return torch.mean(torch.abs(pred - target))\n        else:\n            return torch.abs(pred - target)\n    elif self.loss_type == 'l2':\n        if mean:\n            return torch.mean((pred - target)**2)\n        else:\n            return (pred - target)**2\n    else:\n        raise ValueError(f'Loss type {self.loss_type} not recognized.')\n\n", "CodeBleu_score": 33.69026630975652}]}
{"instance_id": "63a08283666f", "generation_result": [{"code": "def forward(self, x, c, *args, **kwargs)\n\t\n    if self.training:\n        # time step\n        if self.time_dependent:\n            t = self.timestep_embed(c)\n        else:\n            t = None\n\n        # condition\n        if self.condition_dim > 0:\n            assert c is not None, 'condition is expected to be not None'\n            c = self.cond_embed(c)\n\n        # sample\n        x = self.q_sample(x, t, c)\n\n        # loss\n        loss = self.q_loss(x, t, c)\n        return loss\n\n    else:\n        # deterministic generation\n        if self.condition_dim > 0:\n            assert c is not None, 'condition is expected to be not None'\n            c = self.cond_embed(c)\n\n        # sample\n        x = self.p_sample(x, t, c)\n\n        # loss\n        loss = self.p_loss(x, t, c)\n        return loss\n\n", "CodeBleu_score": 32.73976923839298}, {"code": "def forward(self, x, c, *args, **kwargs)\n\t\n    if self.training:\n        # Sample time steps\n        if self.training_steps is None:\n            self.training_steps = self.num_timesteps - self.num_diffusion_steps\n        time_steps = self.time_sampler(self.training_steps)\n        # Apply learned conditioning\n        if self.learned_conditioning:\n            c = self.conditioning(c)\n        # Process inputs\n        x = self.input_process(x, c)\n        # Sample diffusion\n        x_t, t_samples = self.diffusion(x, time_steps, conditioning=c)\n        # Compute loss\n        loss = self.loss_fn(x, x_t, t_samples, c)\n    else:\n        # Sample time steps\n        if self.inference_steps is None:\n            self.inference_steps = self.num_timesteps - self.num_diffusion_steps\n        time_steps = self.time_sampler(self.inference_steps)\n        # Apply learned conditioning\n        if self.learned_conditioning:\n            c = self.conditioning(c)\n        # Process inputs\n        x = self.input_process(x, c)\n        # Sample diffusion\n        x_t, t_samples = self.diffusion(x, time_steps, conditioning=c)\n        # Compute loss\n        loss = self.loss_fn(x, x_t, t_samples, c)\n    return loss\n\n", "CodeBleu_score": 29.612418285184816}, {"code": "def forward(self, x, c, *args, **kwargs)\n\t\n    if self.timestep is None:\n        self.timestep = 0\n    if self.cond_fn is not None:\n        c = self.cond_fn(c, self.timestep)\n    if self.model_config.get(\"time_in_channels\", False):\n        x = rearrange(x, \"b c t h w -> b t c h w\")\n    if self.model_config.get(\"channel_in_channels\", False):\n        x = rearrange(x, \"b t c h w -> b c t h w\")\n    if self.model_config.get(\"noise_in_channels\", False):\n        x = rearrange(x, \"b t c h w -> b c h w t\")\n    if self.model_config.get(\"noise_channels\", False):\n        x = repeat(x, \"b c h w -> b c h w t\", t=self.model_config[\"noise_channels\"])\n    if self.model_config.get(\"time_channels\", False):\n        x = repeat(x, \"b c h w -> b c h w t\", t=self.model_config[\"time_channels\"])\n    if self.model_config.get(\"noise_channels\", False):\n        x = repeat(x, \"b c h w -> b c h w t\", t=self.model_config[\"noise_channels\"])\n    if self.model_config.get(\"time_channels\", False):\n        x = repeat(x, \"b c h w -> b c h w t\", t=self.model_config[\"time_channels\"])\n    if self.model_config.get(\"noise_channels\", False):\n        x = repeat(x, \"b c h w -> b c h w t\", t=self.model_config[\"noise_channels\"])\n    if self.model_config.get(\"time_channels\", False):\n        x = repeat(x, \"b c h w -> b c h w t\", t=self.model_config[\"time_channels\"])\n    if self.model_config.get(\"noise_channels\", False):\n        x", "CodeBleu_score": 22.471515016141367}]}
{"instance_id": "4627e1e8ed62", "generation_result": [{"code": "def get_codebook_entry(self, indices, shape)\n\t\n\n    # get the codebook entry at the given indices\n    latents = self.codebook[indices.view(-1)].view(*shape)\n\n    # optionally remap the indices\n    if exists(self.remap_indices):\n        indices = self.remap_indices(indices)\n\n    # optionally permute the latent vectors\n    if exists(self.permute_dims):\n        latents = latents.permute(*self.permute_dims).contiguous()\n\n    return latents\n\n", "CodeBleu_score": 35.753374713838426}, {"code": "def get_codebook_entry(self, indices, shape)\n\t\n    if self.remap_indices:\n        indices = self.remap_indices(indices)\n    return self.codebook.weight.index_select(0, indices).view(*shape)\n\n", "CodeBleu_score": 19.685984226497766}, {"code": "def get_codebook_entry(self, indices, shape)\n\t\n    # if the codebook is empty, return the empty tensor\n    if self.num_embeddings == 0:\n        return torch.empty(\n            *shape,\n            dtype=self.dtype,\n            device=self.weight.device,\n            requires_grad=False,\n        )\n\n    # if the indices are empty, return the empty tensor\n    if indices.numel() == 0:\n        return torch.empty(\n            *shape,\n            dtype=self.dtype,\n            device=self.weight.device,\n            requires_grad=False,\n        )\n\n    # if the indices are longer than the number of embeddings,\n    # then we have to remap the indices\n    if indices.numel() > self.num_embeddings:\n        # remap indices to the range [0, num_embeddings)\n        indices = indices % self.num_embeddings\n\n    # get the quantized latent vectors from the codebook\n    latents = self.weight[indices.view(-1)]\n\n    # reshape the latent vectors to match the shape of the input\n    latents = latents.view(*shape)\n\n    # if the input is a 4D tensor, then permute the dimensions to match the input\n    if len(shape) == 4:\n        # permute the dimensions to match the input\n        latents = latents.permute(0, 3, 1, 2)\n\n    return latents\n\n", "CodeBleu_score": 37.596186126040315}]}
{"instance_id": "c55d014827ef", "generation_result": [{"code": "def factorization(dimension: int, factor:int=-1) -> tuple[int, int]\n\t\n    if factor > 0 and dimension % factor == 0:\n        return factor, dimension // factor\n    elif factor < 0:\n        for i in range(1, dimension):\n            if dimension % i == 0 and dimension // i <= i:\n                return i, dimension // i\n    else:\n        for i in range(1, dimension):\n            if dimension % i == 0:\n                return i, dimension // i\n\n", "CodeBleu_score": 25.128240035755383}, {"code": "def factorization(dimension: int, factor:int=-1) -> tuple[int, int]\n\t\n    if factor > 0:\n        if dimension % factor == 0:\n            return factor, dimension // factor\n        else:\n            raise ValueError(f\"The dimension {dimension} does not divide by the specified factor {factor}.\")\n    elif factor < 0:\n        if dimension % -factor == 0:\n            return -factor, dimension // -factor\n        else:\n            raise ValueError(f\"The dimension {dimension} does not divide by the specified factor {factor}.\")\n    else:\n        for i in range(1, dimension):\n            if dimension % i == 0:\n                if dimension // i <= i:\n                    return dimension // i, i\n                else:\n                    return i, dimension // i\n    raise ValueError(f\"The dimension {dimension} does not have a factorization with factors less than or equal to the dimension.\")\n\n", "CodeBleu_score": 24.796940619227023}, {"code": "def factorization(dimension: int, factor:int=-1) -> tuple[int, int]\n\t\n    if factor > 0:\n        if dimension % factor == 0:\n            return factor, dimension // factor\n        else:\n            raise ValueError(f\"{factor} does not divide {dimension}\")\n    elif factor < 0:\n        for i in range(dimension // 2, 1, -1):\n            if dimension % i == 0:\n                return i, dimension // i\n    else:\n        for i in range(dimension // 2, 1, -1):\n            if dimension % i == 0:\n                return i, dimension // i\n\n", "CodeBleu_score": 26.851634542012214}]}
{"instance_id": "5d613ec26036", "generation_result": [{"code": "def apply_weight_decompose(self, updown, orig_weight)\n\t\n    # Combine the input weight with the original weight\n    combined_weight = updown.combine_weights(orig_weight, self.weight)\n    # Normalize the combined weight\n    combined_weight = updown.normalize_weight(combined_weight)\n    # Scale the combined weight\n    combined_weight = updown.scale_weight(combined_weight)\n    # Calculate the adjusted weight\n    adjusted_weight = combined_weight - orig_weight\n    return adjusted_weight\n\n", "CodeBleu_score": 7.048822993343339}, {"code": "def apply_weight_decompose(self, updown, orig_weight)\n\t\n    # Check that the input and original weights are on the same device and have the same data type\n    assert orig_weight.device == updown.device and orig_weight.dtype == updown.dtype\n\n    # Decompose the input weight into a scale factor and a bias\n    scale_factor, bias = self.weight_decompose(updown)\n\n    # Normalize the combined weight\n    combined_weight = self.weight_normalize(updown)\n\n    # Scale the combined weight\n    scaled_weight = self.weight_scale(combined_weight, scale_factor)\n\n    # Calculate the adjusted weight\n    adjusted_weight = self.weight_adjust(scaled_weight, orig_weight, bias)\n\n    return adjusted_weight\n\n", "CodeBleu_score": 11.366210028400904}, {"code": "def apply_weight_decompose(self, updown, orig_weight)\n\t\n    # TODO: This function needs to be tested.\n    # TODO: This function needs to be refactored to use the new weight decomposer.\n    # TODO: This function needs to be refactored to use the new weight decomposer.\n    # TODO: This function needs to be refactored to use the new weight decomposer.\n    # TODO: This function needs to be refactored to use the new weight decomposer.\n    # TODO: This function needs to be refactored to use the new weight decomposer.\n    # TODO: This function needs to be refactored to use the new weight decomposer.\n    # TODO: This function needs to be refactored to use the new weight decomposer.\n    # TODO: This function needs to be refactored to use the new weight decomposer.\n    # TODO: This function needs to be refactored to use the new weight decomposer.\n    # TODO: This function needs to be refactored to use the new weight decomposer.\n    # TODO: This function needs to be refactored to use the new weight decomposer.\n    # TODO: This function needs to be refactored to use the new weight decomposer.\n    # TODO: This function needs to be refactored to use the new weight decomposer.\n    # TODO: This function needs to be refactored to use the new weight decomposer.\n    # TODO: This function needs to be refactored to use the new weight decomposer.\n    # TODO: This function needs to be refactored to use the new weight decomposer.\n    # TODO: This function needs to be refactored to use the new weight decomposer.\n    # TODO: This function needs to be refactored to use the new weight decomposer.\n    # TODO: This function needs to be refactored to use the new weight decomposer.\n    # TODO: This function needs to be refactored to use the new weight decomposer.\n    # TODO: This function needs to be refactored to use the new weight decomposer.\n    # TODO: This function needs to be refactored to use the new weight decomposer.\n    # TODO: This function needs to be refactored to use the", "CodeBleu_score": 1.049465240641711}]}
{"instance_id": "c73d0d1cd3e5", "generation_result": [{"code": "def finalize_updown(self, updown, orig_weight, output_shape, ex_bias=None)\n\t\n    if ex_bias is not None:\n        updown = updown + ex_bias\n    if orig_weight.numel() == updown.numel():\n        updown = updown.view(orig_weight.shape)\n    else:\n        updown = updown.view(output_shape)\n    if self.scale_factor is not None:\n        updown = self.decompose(updown, self.scale_factor)\n    return updown, ex_bias\n\n", "CodeBleu_score": 37.794624965389026}, {"code": "def finalize_updown(self, updown, orig_weight, output_shape, ex_bias=None)\n\t\n    bias = None\n    if ex_bias is not None:\n        bias = ex_bias.clone()\n    if orig_weight.numel() == updown.numel():\n        updown = updown.view(orig_weight.shape)\n    else:\n        updown = updown.view(output_shape)\n    if bias is not None:\n        bias *= self.scalar\n    if self.scalar != 1.0:\n        updown = self.decomp(updown)\n    return updown, bias\n\n", "CodeBleu_score": 37.15701992224674}, {"code": "def finalize_updown(self, updown, orig_weight, output_shape, ex_bias=None)\n\t\n    if orig_weight.shape != updown.shape:\n        if orig_weight.numel() == updown.numel():\n            updown = updown.view(orig_weight.shape)\n        else:\n            raise errors.ModelError(\n                \"The updown tensor has a different shape than the original weight tensor. \"\n                \"The updown tensor must have the same shape as the original weight tensor \"\n                \"or the same number of elements as the original weight tensor.\"\n            )\n    if ex_bias is not None:\n        if ex_bias.shape != updown.shape:\n            raise errors.ModelError(\n                \"The additional bias has a different shape than the updown tensor. \"\n                \"The additional bias must have the same shape as the updown tensor.\"\n            )\n        if orig_weight.shape != ex_bias.shape:\n            raise errors.ModelError(\n                \"The additional bias has a different shape than the original weight tensor. \"\n                \"The additional bias must have the same shape as the original weight tensor.\"\n            )\n        if orig_weight.numel() != ex_bias.numel():\n            raise errors.ModelError(\n                \"The additional bias has a different number of elements than the original weight tensor. \"\n                \"The additional bias must have the same number of elements as the original weight tensor.\"\n            )\n        updown = updown + ex_bias\n    if self.scaling_factor is not None:\n        updown = updown * self.scaling_factor\n    if self.decomposition is not None:\n        updown = self.decomposition.apply(updown)\n    return updown\n\n", "CodeBleu_score": 37.54253383481741}]}
{"instance_id": "c6a41665c62c", "generation_result": [{"code": "def calc_updown(self, orig_weight)\n\t\n    # Transfer current weight and bias to device of original weight\n    current_weight = self.weight.to(orig_weight.device)\n    current_bias = self.bias.to(orig_weight.device) if self.bias is not None else None\n\n    # Call another function to finalize the updated weights\n    return self.calc_updown_finalize(orig_weight, current_weight, current_bias)\n\n", "CodeBleu_score": 26.3801757582028}, {"code": "def calc_updown(self, orig_weight)\n\t\n    # transfer the current weight tensor and optional bias to the device of the original weight tensor\n    if self.bias is not None:\n        self.bias = self.bias.to(orig_weight.device)\n    self.weight = self.weight.to(orig_weight.device)\n    # call another function to finalize the updated weights\n    return self._calc_updown_impl(orig_weight)\n\n", "CodeBleu_score": 37.122762072223985}, {"code": "def calc_updown(self, orig_weight)\n\t\n    # transfer the current weight tensor and optional bias to the device of the original weight tensor\n    orig_weight = orig_weight.to(self.device)\n    # call another function to finalize the updated weights\n    return self.calc_updown_final(orig_weight)\n\n", "CodeBleu_score": 5.713620001835118}]}
{"instance_id": "1bb9e5d94af8", "generation_result": [{"code": "def purge_networks_from_memory()\n\t\n    networks = get_networks()\n    if len(networks) > 100:\n        for n in networks:\n            networks.pop(n)\n        torch.cuda.empty_cache()\n\n", "CodeBleu_score": 22.44819369897664}, {"code": "def purge_networks_from_memory()\n\t\n    if len(network.Network.networks) > 1000:\n        # Remove networks from memory\n        for net in network.Network.networks:\n            if net.is_in_memory:\n                net.is_in_memory = False\n\n        # Trigger garbage collection\n        torch.cuda.empty_cache()\n\n", "CodeBleu_score": 24.82863051657993}, {"code": "def purge_networks_from_memory()\n\t\n    networks_to_purge = []\n    while len(network.networks) > 1000:\n        networks_to_purge.append(network.networks.popitem()[0])\n    for network_id in networks_to_purge:\n        del network.networks[network_id]\n    torch.cuda.empty_cache()\n\n", "CodeBleu_score": 27.43155239242482}]}
{"instance_id": "42b48ebf4d07", "generation_result": [{"code": "def network_restore_weights_from_backup(self: Union[torch.nn.Conv2d, torch.nn.Linear, torch.nn.GroupNorm, torch.nn.LayerNorm, torch.nn.MultiheadAttention]):\n\t\n    if self.weight is not None:\n        weight_backup = self.weight.backup\n        if weight_backup is not None:\n            self.weight.data.copy_(weight_backup)\n            self.weight.backup = None\n    if self.bias is not None:\n        bias_backup = self.bias.backup\n        if bias_backup is not None:\n            self.bias.data.copy_(bias_backup)\n            self.bias.backup = None\n    if isinstance(self, torch.nn.MultiheadAttention):\n        if self.in_proj_weight is not None:\n            in_proj_weight_backup = self.in_proj_weight.backup\n            if in_proj_weight_backup is not None:\n                self.in_proj_weight.data.copy_(in_proj_weight_backup)\n                self.in_proj_weight.backup = None\n        if self.in_proj_bias is not None:\n            in_proj_bias_backup = self.in_proj_bias.backup\n            if in_proj_bias_backup is not None:\n                self.in_proj_bias.data.copy_(in_proj_bias_backup)\n                self.in_proj_bias.backup = None\n    if isinstance(self, torch.nn.GroupNorm) or isinstance(self, torch.nn.LayerNorm):\n        if self.weight is not None:\n            weight_backup = self.weight.backup\n            if weight_backup is not None:\n                self.weight.data.copy_(weight_backup)\n                self.weight.backup = None\n        if self.bias is not None:\n            bias_backup = self.bias.backup\n            if bias_backup is not None:\n                self.bias.data.copy_(bias_backup)\n                self.bias.backup = None\n\n", "CodeBleu_score": 44.47042925991153}, {"code": "def network_restore_weights_from_backup(self: Union[torch.nn.Conv2d, torch.nn.Linear, torch.nn.GroupNorm, torch.nn.LayerNorm, torch.nn.MultiheadAttention]):\n\t\n    if self.backup_weights_exist:\n        self.weight.data = self.backup_weights.data\n        self.backup_weights = None\n        self.backup_weights_exist = False\n    if self.backup_biases_exist:\n        self.bias.data = self.backup_biases.data\n        self.backup_biases = None\n        self.backup_biases_exist = False\n    if isinstance(self, torch.nn.MultiheadAttention):\n        if self.backup_proj_weights_exist:\n            self.out_proj.weight.data = self.backup_proj_weights.data\n            self.backup_proj_weights = None\n            self.backup_proj_weights_exist = False\n        if self.backup_proj_biases_exist:\n            self.out_proj.bias.data = self.backup_proj_biases.data\n            self.backup_proj_biases = None\n            self.backup_proj_biases_exist = False\n\n", "CodeBleu_score": 35.14656300840093}, {"code": "def network_restore_weights_from_backup(self: Union[torch.nn.Conv2d, torch.nn.Linear, torch.nn.GroupNorm, torch.nn.LayerNorm, torch.nn.MultiheadAttention]):\n\t\n    if not hasattr(self, 'backup_weights'):\n        return\n    if self.backup_weights is None:\n        return\n    if self.backup_weights.get('weight') is not None:\n        self.weight.data = self.backup_weights.get('weight')\n    if self.backup_weights.get('bias') is not None:\n        self.bias.data = self.backup_weights.get('bias')\n    if isinstance(self, torch.nn.MultiheadAttention):\n        if self.backup_weights.get('q_proj_weight') is not None:\n            self.q_proj_weight.data = self.backup_weights.get('q_proj_weight')\n        if self.backup_weights.get('k_proj_weight') is not None:\n            self.k_proj_weight.data = self.backup_weights.get('k_proj_weight')\n        if self.backup_weights.get('v_proj_weight') is not None:\n            self.v_proj_weight.data = self.backup_weights.get('v_proj_weight')\n        if self.backup_weights.get('out_proj_weight') is not None:\n            self.out_proj_weight.data = self.backup_weights.get('out_proj_weight')\n        if self.backup_weights.get('out_proj_bias') is not None:\n            self.out_proj_bias.data = self.backup_weights.get('out_proj_bias')\n    if self.backup_weights.get('running_mean') is not None:\n        self.running_mean.data = self.backup_weights.get('running_mean')\n    if self.backup_weights.get('running_var') is not None:\n        self.running_var.data = self.backup_weights.get('running_var')\n\n", "CodeBleu_score": 35.11423199871251}]}
{"instance_id": "c8fcd357131a", "generation_result": [{"code": "def network_forward(org_module, input, original_forward)\n\t\n    if len(network.loaded_networks) == 0:\n        return original_forward(input)\n    input = input.to(devices.device)\n    if hasattr(org_module, \"backup_weights\"):\n        org_module.load_state_dict(org_module.backup_weights)\n    output = original_forward(input)\n    for network in network.loaded_networks:\n        if network.current_layer == org_module:\n            output = network.forward(output)\n    return output\n\n", "CodeBleu_score": 32.35203622365149}, {"code": "def network_forward(org_module, input, original_forward)\n\t\n\n    # Check if any additional networks are loaded\n    if len(network.loaded_networks) == 0:\n        return original_forward(input)\n\n    # Cast input to appropriate device\n    input = input.to(devices.device)\n\n    # Restore weights and reset\n    sd_models.restore_backup(org_module)\n    org_module.reset()\n\n    # Perform original forward pass\n    output = original_forward(input)\n\n    # Apply additional networks\n    for net in network.loaded_networks:\n        if net.layer_name == org_module.name:\n            output = net.forward(output)\n\n    # Return output\n    return output\n\n", "CodeBleu_score": 30.424625980675152}, {"code": "def network_forward(org_module, input, original_forward)\n\t\n    if not lora_patches.networks:\n        return original_forward(*input)\n    input = input[0]\n    input = input.to(lora_patches.device)\n    org_module.load_state_dict(lora_patches.backup_state_dict)\n    org_module.reset_parameters()\n    output = original_forward(*input)\n    for net in lora_patches.networks:\n        if net.layer == org_module:\n            output = net.forward(output)\n    return output\n\n", "CodeBleu_score": 29.409768467830933}]}
{"instance_id": "1070211f5c11", "generation_result": [{"code": "def trainables(self)\n\t\n    trainables = []\n    for layer in self.layers:\n        if isinstance(layer, torch.nn.Linear):\n            trainables.extend([layer.weight, layer.bias])\n        elif isinstance(layer, torch.nn.LayerNorm):\n            trainables.extend([layer.weight, layer.bias])\n    return trainables\n\n", "CodeBleu_score": 38.711162911548925}, {"code": "def trainables(self)\n\t\n    trainables = []\n    for name, param in self.named_parameters():\n        if isinstance(param, torch.nn.Parameter):\n            if \"weight\" in name:\n                trainables.append(param)\n            if \"bias\" in name:\n                trainables.append(param)\n    return trainables\n\n", "CodeBleu_score": 27.967018323736642}, {"code": "def trainables(self)\n\t\n    return [p for p in self.parameters() if p.requires_grad]\n\n", "CodeBleu_score": 6.452447410697191}]}
{"instance_id": "0c3ba62c96a7", "generation_result": [{"code": "def load_hypernetworks(names, multipliers=None)\n\t\n    global loaded_hypernetworks\n    loaded_hypernetworks.clear()\n    for name, multiplier in zip(names, multipliers):\n        if name in loaded_hypernetworks:\n            hypernetwork = loaded_hypernetworks[name]\n        else:\n            hypernetwork = sd_models.HyperNetwork(name, sd_models.HyperNetwork.load(name))\n            loaded_hypernetworks[name] = hypernetwork\n        if multiplier is not None:\n            hypernetwork.multiplier = multiplier\n        loaded_hypernetworks.append(hypernetwork)\n\n", "CodeBleu_score": 29.48003924966412}, {"code": "def load_hypernetworks(names, multipliers=None)\n\t\n    if not names:\n        return\n    shared.loaded_hypernetworks.clear()\n    for name in names:\n        if name not in shared.loaded_hypernetworks:\n            shared.loaded_hypernetworks.append(load_hypernetwork(name, multipliers))\n        else:\n            shared.loaded_hypernetworks.append(shared.loaded_hypernetworks[name])\n\n", "CodeBleu_score": 21.533177478255606}, {"code": "def load_hypernetworks(names, multipliers=None)\n\t\n    shared.loaded_hypernetworks.clear()\n    for name in names:\n        if name in shared.loaded_hypernetworks:\n            shared.loaded_hypernetworks.append(shared.loaded_hypernetworks[name])\n        else:\n            shared.loaded_hypernetworks.append(load_hypernetwork(name, multipliers))\n\n", "CodeBleu_score": 15.756986566065773}]}
{"instance_id": "bea3772ed532", "generation_result": [{"code": "\ndef apply_hypernetworks(hypernetworks, context, layer=None)\n\t\n\n    context_k = context\n    context_v = context\n    for i, hypernetwork in enumerate(hypernetworks):\n        context_k, context_v = apply_single_hypernetwork(hypernetwork, context_k, context_v, layer=layer, i=i)\n    return context_k, context_v\n\n", "CodeBleu_score": 54.59377447792981}, {"code": "\ndef apply_hypernetworks(hypernetworks, context, layer=None)\n\t\n    context_k = context_v = context\n    for h in hypernetworks:\n        context_k, context_v = apply_hypernetwork(h, context_k, context_v, layer)\n    return context_k, context_v\n\n", "CodeBleu_score": 58.84022520822493}, {"code": "\ndef apply_hypernetworks(hypernetworks, context, layer=None)\n\t\n    context_k, context_v = context, context\n    for hypernetwork in hypernetworks:\n        context_k, context_v = apply_single_hypernetwork(hypernetwork, context_k, context_v, layer)\n    return context_k, context_v\n\n", "CodeBleu_score": 68.4402416707376}]}
{"instance_id": "922efa3e092f", "generation_result": [{"code": "def attention_CrossAttention_forward(self, x, context=None, mask=None, **kwargs)\n\t\n    if context is None:\n        context = x\n    q, k, v = self.compute_qkv(x, context)\n    q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=self.heads), (q, k, v))\n    q, k, v = map(lambda t: self.scale_attn_weights(t, self.heads), (q, k, v))\n    out = self.attn(q, k, v, mask=mask)\n    out = rearrange(out, 'b h n d -> b n (h d)')\n    out = self.to_out(out)\n    return out\n\n", "CodeBleu_score": 29.67512823724292}, {"code": "def attention_CrossAttention_forward(self, x, context=None, mask=None, **kwargs)\n\t\n    # compute query, key, and value projections\n    q = self.query(x)\n    k = self.key(context)\n    v = self.value(context)\n\n    # apply hypernetworks to the context\n    if self.hyper_w_qs is not None:\n        q = einsum('b x d, h -> b h x d', q, self.hyper_w_qs(context))\n    if self.hyper_w_ks is not None:\n        k = einsum('b x d, h -> b h x d', k, self.hyper_w_ks(context))\n    if self.hyper_w_vs is not None:\n        v = einsum('b x d, h -> b h x d', v, self.hyper_w_vs(context))\n\n    # reshape for multi-head attention\n    b, n, d = q.shape\n    h = self.num_heads\n    q = rearrange(q, 'b n (h d) -> b h n d', h=h)\n    k = rearrange(k, 'b n (h d) -> b h n d', h=h)\n    v = rearrange(v, 'b n (h d) -> b h n d', h=h)\n\n    # calculate scaled dot-product attention\n    dots = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n\n    # apply mask to the attention scores\n    if mask is not None:\n        mask = rearrange(mask, 'b j -> b () () j')\n        dots = dots.masked_fill(mask, -1e9)\n\n    # apply attention\n    attn = dots.softmax(dim=-1)\n    attn = self.attn_dropout(attn)\n    out = einsum('b h i j, b h j d -> b h i d', attn, v)\n\n    # reshape back and apply final linear layer\n    out = rearrange(out, 'b h n d -> b n (h d)')\n    out = self.to_out(out)\n    return out\n\n\n", "CodeBleu_score": 53.83491965303693}, {"code": "def attention_CrossAttention_forward(self, x, context=None, mask=None, **kwargs)\n\t\n    if context is None:\n        context = x\n    if self.context_dim is None:\n        context_dim = context.shape[-1]\n    else:\n        context_dim = self.context_dim\n    if self.query_dim is None:\n        query_dim = x.shape[-1]\n    else:\n        query_dim = self.query_dim\n    if self.value_dim is None:\n        value_dim = context.shape[-1]\n    else:\n        value_dim = self.value_dim\n    if self.heads is None:\n        heads = self.num_heads\n    else:\n        heads = self.heads\n    if self.head_dim is None:\n        head_dim = context_dim // heads\n    else:\n        head_dim = self.head_dim\n    if self.scale is None:\n        scale = head_dim ** -0.5\n    else:\n        scale = self.scale\n    if self.bias is None:\n        bias = self.bias\n    else:\n        bias = self.bias\n    if self.bias_context is None:\n        bias_context = self.bias_context\n    else:\n        bias_context = self.bias_context\n    if self.bias_output is None:\n        bias_output = self.bias_output\n    else:\n        bias_output = self.bias_output\n    if self.bias_value is None:\n        bias_value = self.bias_value\n    else:\n        bias_value = self.bias_value\n    if self.bias_query is None:\n        bias_query = self.bias_query\n    else:\n        bias_query = self.bias_query\n    if self.bias_key is None:\n        bias_key = self.bias_key\n    else:\n        bias_key = self.bias_key\n    if self.context_dim is None:\n        context_dim = context.shape[-1]\n    else:\n        context_dim = self.context_dim\n    if self.query_dim is None:\n        query_dim =", "CodeBleu_score": 16.541847096822245}]}
{"instance_id": "8b7455a03a0e", "generation_result": [{"code": "def save_hypernetwork(hypernetwork, checkpoint, hypernetwork_name, filename)\n\t\n    original_hypernetwork_state = hypernetwork.state_dict()\n    original_hypernetwork_name = hypernetwork.name\n    original_hypernetwork_checkpoint = hypernetwork.checkpoint\n    original_hypernetwork_filename = hypernetwork.filename\n    original_hypernetwork_save_path = hypernetwork.save_path\n    original_hypernetwork_save_filename = hypernetwork.save_filename\n    original_hypernetwork_save_extension = hypernetwork.save_extension\n    original_hypernetwork_save_directory = hypernetwork.save_directory\n    original_hypernetwork_save_filepath = hypernetwork.save_filepath\n    original_hypernetwork_load_path = hypernetwork.load_path\n    original_hypernetwork_load_filename = hypernetwork.load_filename\n    original_hypernetwork_load_extension = hypernetwork.load_extension\n    original_hypernetwork_load_directory = hypernetwork.load_directory\n    original_hypernetwork_load_filepath = hypernetwork.load_filepath\n    original_hypernetwork_save_on_forward = hypernetwork.save_on_forward\n    original_hypernetwork_load_on_forward = hypernetwork.load_on_forward\n    original_hypernetwork_save_on_backward = hypernetwork.save_on_backward\n    original_hypernetwork_load_on_backward = hypernetwork.load_on_backward\n    original_hypernetwork_save_on_step = hypernetwork.save_on_step\n    original_hypernetwork_load_on_step = hypernetwork.load_on_step\n    original_hypernetwork_save_on_epoch = hypernetwork.save_on_epoch\n    original_hypernetwork_load_on_epoch = hypernetwork.load_on_epoch\n    original_hypernetwork_save_on_batch = hypernetwork.save_on_batch\n    original_hypernetwork_load_on_batch = hypernetwork.load_on_batch\n    original_hypernetwork_save_on_sample = hypernetwork.save_on_sample\n    original_hypernetwork_load_on_sample = hypernetwork.load_on_sample\n    original_hypernetwork_save_on_validation = hypernetwork.save_on_validation", "CodeBleu_score": 9.63802101980648}, {"code": "def save_hypernetwork(hypernetwork, checkpoint, hypernetwork_name, filename)\n\t\n    original_state_dict = hypernetwork.state_dict()\n    original_checkpoint = hypernetwork.checkpoint\n    hypernetwork.checkpoint = checkpoint\n    try:\n        torch.save(hypernetwork.state_dict(), os.path.join(filename, hypernetwork_name + '.pt'))\n    except Exception as e:\n        hypernetwork.checkpoint = original_checkpoint\n        hypernetwork.load_state_dict(original_state_dict)\n        raise e\n    hypernetwork.checkpoint = original_checkpoint\n    hypernetwork.load_state_dict(original_state_dict)\n\n", "CodeBleu_score": 30.738468465668973}, {"code": "def save_hypernetwork(hypernetwork, checkpoint, hypernetwork_name, filename)\n\t\n    try:\n        original_checkpoint = hypernetwork.checkpoint\n        original_checkpoint_name = hypernetwork.checkpoint_name\n        original_checkpoint_dir = hypernetwork.checkpoint_dir\n        original_checkpoint_path = hypernetwork.checkpoint_path\n        original_checkpoint_version = hypernetwork.checkpoint_version\n        original_checkpoint_version_dir = hypernetwork.checkpoint_version_dir\n        original_checkpoint_version_path = hypernetwork.checkpoint_version_path\n        original_checkpoint_version_path_dir = hypernetwork.checkpoint_version_path_dir\n        original_checkpoint_version_path_dir_filename = hypernetwork.checkpoint_version_path_dir_filename\n        original_checkpoint_version_path_dir_filename_ext = hypernetwork.checkpoint_version_path_dir_filename_ext\n        original_checkpoint_version_path_dir_filename_ext_hash = hypernetwork.checkpoint_version_path_dir_filename_ext_hash\n        original_checkpoint_version_path_dir_filename_ext_hash_path = hypernetwork.checkpoint_version_path_dir_filename_ext_hash_path\n        original_checkpoint_version_path_dir_filename_ext_hash_path_filename = hypernetwork.checkpoint_version_path_dir_filename_ext_hash_path_filename\n        original_checkpoint_version_path_dir_filename_ext_hash_path_filename_ext = hypernetwork.checkpoint_version_path_dir_filename_ext_hash_path_filename_ext\n        original_checkpoint_version_path_dir_filename_ext_hash_path_filename_ext_hash = hypernetwork.checkpoint_version_path_dir_filename_ext_hash_path_filename_ext_hash\n        original_checkpoint_version_path_dir_filename_ext_hash_path_filename_ext_hash_path = hypernetwork.checkpoint_version_path_dir_filename_ext_hash_path_filename_ext_hash_path\n        original_checkpoint_version_path_dir_filename_ext_hash_path_filename_ext_hash_path_filename = hyper", "CodeBleu_score": 25.890684342802857}]}
{"instance_id": "3175f835140f", "generation_result": [{"code": "def ema_scope(self, context=None)\n\t\n    if self.ema is not None:\n        # Store current parameters and replace with EMA parameters\n        original_params = [p.clone() for p in self.parameters()]\n        self.ema.apply_shadow()\n        # Log EMA usage\n        if context is not None:\n            self.log(f'{context}_ema', True)\n    try:\n        yield\n    finally:\n        # Restore original parameters\n        if self.ema is not None:\n            self.ema.restore()\n            # Log EMA restoration\n            if context is not None:\n                self.log(f'{context}_ema', False)\n\n", "CodeBleu_score": 50.6746662711921}, {"code": "def ema_scope(self, context=None)\n\t\n    if self.ema is None:\n        return context\n    else:\n        self.ema.store()\n        self.ema.copy_to()\n        if context is not None:\n            context.log('EMA enabled')\n        yield\n        self.ema.restore()\n        if context is not None:\n            context.log('EMA restored')\n\n", "CodeBleu_score": 42.132224337062915}, {"code": "def ema_scope(self, context=None)\n\t\n    if self.ema is None:\n        return context()\n    with torch.no_grad():\n        self.ema.store(self.model.parameters())\n        self.ema.copy_to(self.model.parameters())\n        if self.log_weights:\n            self.log(\"ema/store_ema\", self.ema.ema_value, on_step=False, on_epoch=True, prog_bar=True)\n        context()\n        if self.log_weights:\n            self.log(\"ema/restore_ema\", self.ema.ema_value, on_step=False, on_epoch=True, prog_bar=True)\n        self.ema.restore(self.model.parameters())\n\n", "CodeBleu_score": 50.049816098622266}]}
{"instance_id": "a7b5d5cd519a", "generation_result": [{"code": "def p_mean_variance(self, x, t, clip_denoised: bool)\n\t\n    model_out = self.net(x, t)\n    if self.model_output_mean is not None:\n        mean = self.model_output_mean(model_out)\n    else:\n        mean = model_out\n    if self.model_output_variance is not None:\n        variance = self.model_output_variance(model_out)\n    else:\n        variance = torch.ones_like(mean)\n    if self.model_output_log_variance is not None:\n        log_variance = self.model_output_log_variance(model_out)\n    else:\n        log_variance = torch.zeros_like(mean)\n    if self.clip_denoised:\n        mean = mean.clamp(-1., 1.)\n    return mean, variance, log_variance\n\n", "CodeBleu_score": 33.08452261735248}, {"code": "def p_mean_variance(self, x, t, clip_denoised: bool)\n\t\n    model_out = self.model(x, t)\n    if self.model.out_channels == 2 * self.model.in_channels:\n        mean, log_variance = model_out.chunk(2, dim=1)\n    else:\n        mean = model_out\n        log_variance = torch.zeros_like(mean)\n    if clip_denoised:\n        mean = mean.clamp(min=-1, max=1)\n    return mean, log_variance.exp()\n\n", "CodeBleu_score": 35.743125188789435}, {"code": "def p_mean_variance(self, x, t, clip_denoised: bool)\n\t\n    x_recon = self.denoise_fn(x, t, noise_like(x.shape, device=x.device))\n    if clip_denoised:\n        x_recon.clamp_(-1., 1.)\n    model_mean, posterior_variance, posterior_log_variance = self.p_mean_variance(x_recon, t)\n    return model_mean, posterior_variance, posterior_log_variance\n\n", "CodeBleu_score": 36.957102689822676}]}
{"instance_id": "a8652fa78473", "generation_result": [{"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss_type == 'l1':\n        loss = torch.abs(pred - target)\n        if mean:\n            loss = loss.mean()\n        return loss\n    elif self.loss_type == 'l2':\n        loss = (pred - target) ** 2\n        if not self.reduction:\n            return loss\n        return loss.mean()\n    else:\n        raise ValueError('Unknown loss type: {}'.format(self.loss_type))\n\n", "CodeBleu_score": 46.28327391645468}, {"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss_type == 'l1':\n        loss = torch.abs(pred - target)\n        if mean:\n            loss = loss.mean()\n    elif self.loss_type == 'l2':\n        loss = (pred - target)**2\n        if not self.reduction:\n            loss = loss.mean(dim=1)\n        else:\n            loss = loss.mean()\n    else:\n        raise ValueError(f'Unknown loss type: {self.loss_type}')\n    return loss\n\n", "CodeBleu_score": 52.43549789492199}, {"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss_type == 'l1':\n        loss = torch.abs(pred - target)\n        if mean:\n            return loss.mean()\n        else:\n            return loss\n    elif self.loss_type == 'l2':\n        loss = (pred - target) ** 2\n        if mean:\n            return loss.mean()\n        else:\n            return loss\n    else:\n        raise ValueError(f'Unknown loss type: {self.loss_type}')\n\n", "CodeBleu_score": 41.2135438485047}]}
{"instance_id": "d68e4f061349", "generation_result": [{"code": "def _prior_bpd(self, x_start)\n\t\n    batch_size = x_start.shape[0]\n    t = torch.full((batch_size,), self.num_timesteps - 1, device=x_start.device)\n    x_t = self.p_sample_x_t(x_start, t)\n    mean, log_var = self.p_x_t_given_t(x_t, t)\n    kl = normal_kl(mean, log_var).mean(dim=(1, 2, 3))\n    return kl.mean() / torch.log(torch.tensor(2.0))\n\n", "CodeBleu_score": 31.15355147947033}, {"code": "def _prior_bpd(self, x_start)\n\t\n    x_start = x_start.to(self.device)\n    batch_size = x_start.size(0)\n    t = torch.ones(batch_size, 1, 1, 1, device=self.device) * self.num_timesteps\n    t = t.long()\n    x_t = self.net(x_start, t)\n    x_t = x_t.view(batch_size, -1)\n    mean, log_var = self.predict_x_t_given_t(x_t, t)\n    kl = normal_kl(mean, log_var)\n    bpd = kl.mean() / torch.log(torch.tensor(2.0))\n    return bpd\n\n", "CodeBleu_score": 38.008101248682195}, {"code": "def _prior_bpd(self, x_start)\n\t\n    batch_size = x_start.shape[0]\n    t = torch.ones(batch_size, device=x_start.device, dtype=torch.long) * (self.num_timesteps - 1)\n    x_t, _ = self.p_sample_loop(self.model, x_start, t, clip_denoised=True, model_kwargs={\"reverse\": True})\n    x_t_log_var = self.model.log_var(x_t)\n    x_t_mean = self.model.mean(x_t)\n    return -0.5 * torch.mean(torch.sum(1 + 2 * x_t_log_var - torch.pow(x_t_mean, 2) - torch.exp(x_t_log_var), dim=[1, 2, 3])) / torch.log(torch.tensor(2.))\n\n", "CodeBleu_score": 28.703995124724123}]}
{"instance_id": "c695d93fdf2d", "generation_result": [{"code": "def make_cache(subsection: str) -> diskcache.Cache\n\t\n    cache_dir = os.path.join(data_path, \"cache\", subsection)\n    if not os.path.exists(cache_dir):\n        os.makedirs(cache_dir)\n    return diskcache.Cache(cache_dir, size_limit=4e9, disk_min_file_size=256e3)\n\n", "CodeBleu_score": 47.720381195363736}, {"code": "def make_cache(subsection: str) -> diskcache.Cache\n\t\n    cache_path = os.path.join(data_path, \"cache\")\n    return diskcache.Cache(\n        os.path.join(cache_path, subsection),\n        size_limit=4e9,\n        disk_min_file_size=2 ** 18,\n    )\n\n", "CodeBleu_score": 58.92720377141602}, {"code": "def make_cache(subsection: str) -> diskcache.Cache\n\t\n    cache = diskcache.Cache(os.path.join(data_path, \"cache\", subsection), size_limit=4e9, min_size=256e3)\n    return cache\n\n", "CodeBleu_score": 43.217571121902324}]}
{"instance_id": "ed4c543f0d38", "generation_result": [{"code": "def cache(subsection)\n\t\n    global caches\n    global cache_lock\n    if subsection in caches:\n        return caches[subsection]\n    else:\n        cache_lock.acquire()\n        if subsection in caches:\n            cache_lock.release()\n            return caches[subsection]\n        else:\n            if not os.path.isdir(data_path + 'cache'):\n                os.mkdir(data_path + 'cache')\n            if not os.path.isfile(data_path + 'cache/' + subsection + '.json'):\n                convert_old_cached_data(subsection)\n            if not os.path.isfile(data_path + 'cache/' + subsection + '.json'):\n                make_cache(subsection)\n            caches[subsection] = diskcache.Cache(data_path + 'cache/' + subsection + '.json')\n            cache_lock.release()\n            return caches[subsection]\n\n", "CodeBleu_score": 32.664795343295374}, {"code": "def cache(subsection)\n\t\n    global caches\n    if subsection not in caches:\n        cache_lock.acquire()\n        if subsection not in caches:\n            if not os.path.isdir(os.path.join(data_path, \"cache\")):\n                os.mkdir(os.path.join(data_path, \"cache\"))\n            if not os.path.isfile(os.path.join(data_path, \"cache\", subsection + \".cache\")):\n                convert_old_cached_data(subsection)\n            caches[subsection] = make_cache(subsection)\n        cache_lock.release()\n    return caches[subsection]\n\n", "CodeBleu_score": 38.166166623292305}, {"code": "def cache(subsection)\n\t\n    global caches, cache_lock\n    if subsection not in caches:\n        cache_lock.acquire()\n        if subsection not in caches:\n            if not os.path.exists(data_path + \"cache/\"):\n                os.makedirs(data_path + \"cache/\")\n            if not os.path.isfile(data_path + \"cache/\" + subsection + \".cache\"):\n                convert_old_cached_data(subsection)\n            if not os.path.isfile(data_path + \"cache/\" + subsection + \".cache\"):\n                make_cache(subsection)\n            caches[subsection] = diskcache.Cache(data_path + \"cache/\" + subsection + \".cache\")\n        cache_lock.release()\n    return caches[subsection]\n\n", "CodeBleu_score": 37.64580269245249}]}
{"instance_id": "ccf5ea2d9fc1", "generation_result": [{"code": "def cached_data_for_file(subsection, title, filename, func)\n\t\n    cache = cached_data(subsection)\n    mtime = os.path.getmtime(filename)\n    if title in cache:\n        if cache[title][0] < mtime:\n            del cache[title]\n    if title not in cache:\n        cache[title] = (mtime, func())\n    dump_cache()\n    return cache[title][1]\n\n", "CodeBleu_score": 17.382131156394355}, {"code": "def cached_data_for_file(subsection, title, filename, func)\n\t\n    cache = cache_for_subsection(subsection)\n    mtime = os.path.getmtime(filename)\n    if title in cache:\n        if mtime > cache[title][1]:\n            cache[title] = (None, mtime)\n    else:\n        cache[title] = (None, mtime)\n    if cache[title][0] is None:\n        data = func()\n        cache[title] = (data, mtime)\n        dump_cache()\n    return cache[title][0]\n\n", "CodeBleu_score": 22.207083489783702}, {"code": "def cached_data_for_file(subsection, title, filename, func)\n\t\n\n    # Get the cache for the given subsection\n    cache = cached_data(subsection)\n\n    # Get the last modification time of the file\n    mtime = os.path.getmtime(filename)\n\n    # If there is a cache entry for the given title, check if the file's mtime is more recent than the cached mtime\n    if title in cache and mtime > cache[title]['mtime']:\n        # If so, invalidate the cache entry\n        del cache[title]\n\n    # If no valid cache entry exists, call the provided function to generate new data\n    if title not in cache:\n        cache[title] = { 'data': func(), 'mtime': mtime }\n\n    # Update the cache storage\n    dump_cache()\n\n    # Return the cached or newly generated data\n    return cache[title]['data']\n\n", "CodeBleu_score": 21.53313902969455}]}
{"instance_id": "e01a2d4f0ac2", "generation_result": [{"code": "def report(message: str, *, exc_info: bool = False) -> None\n\t\n    record_exception()\n    for line in message.splitlines():\n        print(\"***\", line, file=sys.stderr)\n    if exc_info:\n        traceback.print_exc(file=sys.stderr)\n        print(\"---\", file=sys.stderr)\n\n", "CodeBleu_score": 63.65335006274419}, {"code": "def report(message: str, *, exc_info: bool = False) -> None\n\t\n    record_exception()\n    for line in message.splitlines():\n        print(\"***\", line, file=sys.stderr)\n    if exc_info:\n        traceback.print_exc(file=sys.stderr)\n        print(\"---\", file=sys.stderr)\n\n", "CodeBleu_score": 63.65335006274419}, {"code": "def report(message: str, *, exc_info: bool = False) -> None\n\t\n    record_exception()\n    print(\"***\", message, file=sys.stderr, end=\"\")\n    if exc_info:\n        print(file=sys.stderr)\n        print(textwrap.indent(traceback.format_exc(), \"    \"), file=sys.stderr)\n        print(\"---\", file=sys.stderr)\n\n", "CodeBleu_score": 58.495351899622726}]}
{"instance_id": "125fb9d2320b", "generation_result": [{"code": "def extract_original_prompts(style: PromptStyle, prompt, negative_prompt)\n\t\n    if style.prompt == \"\" or style.negative_prompt == \"\":\n        return False, prompt, negative_prompt\n    try:\n        positive_prompt = style.prompt.strip()\n        negative_prompt = style.negative_prompt.strip()\n        return True, positive_prompt, negative_prompt\n    except AttributeError:\n        return False, prompt, negative_prompt\n\n", "CodeBleu_score": 40.244202315936825}, {"code": "def extract_original_prompts(style: PromptStyle, prompt, negative_prompt)\n\t\n    if style.prompt == \"\" or style.negative_prompt == \"\":\n        return False, prompt, negative_prompt\n\n    try:\n        positive_prompt = style.prompt.format(prompt)\n    except KeyError as e:\n        raise errors.InvalidPromptError(f\"The prompt style '{style.name}' requires the following keys: {e}\")\n\n    try:\n        negative_prompt = style.negative_prompt.format(negative_prompt)\n    except KeyError as e:\n        raise errors.InvalidPromptError(f\"The prompt style '{style.name}' requires the following keys: {e}\")\n\n    return True, positive_prompt, negative_prompt\n\n", "CodeBleu_score": 39.20018102677319}, {"code": "def extract_original_prompts(style: PromptStyle, prompt, negative_prompt)\n\t\n    if not style.prompt or not style.negative_prompt:\n        return False, prompt, negative_prompt\n\n    try:\n        positive_prompt = style.extract_prompt(prompt)\n    except errors.PromptStyleError:\n        return False, prompt, negative_prompt\n\n    try:\n        negative_prompt = style.extract_prompt(negative_prompt)\n    except errors.PromptStyleError:\n        return False, prompt, negative_prompt\n\n    return True, positive_prompt, negative_prompt\n\n", "CodeBleu_score": 53.86295175921221}]}
{"instance_id": "0c5f729fbce2", "generation_result": [{"code": "def random_partial(self, count, n_frames)\n\t\n    if self.utterances is None:\n        self._load_utterances()\n    samples = self.utterance_cycler.sample(count)\n    partials = []\n    for u in samples:\n        partial = u.random_partial(n_frames)\n        partials.append((u, partial))\n    return partials\n\n", "CodeBleu_score": 53.80597907586736}, {"code": "def random_partial(self, count, n_frames)\n\t\n    if not self.utterances:\n        self._load_utterances()\n    samples = self.utterance_cycler.sample(count)\n    partials = []\n    for u in samples:\n        partial = u.random_partial(n_frames)\n        partials.append((u, partial))\n    return partials\n\n", "CodeBleu_score": 46.720971908126764}, {"code": "def random_partial(self, count, n_frames)\n\t\n\n    if self.utterances is None:\n        self._load_utterances()\n    samples = self.utterance_cycler.sample(count)\n    partials = []\n    for u in samples:\n        partials.append((u, u.random_partial(n_frames)))\n    return partials\n\n", "CodeBleu_score": 53.889304984332064}]}
{"instance_id": "31e8b27b8f94", "generation_result": [{"code": "def normalize_volume(wav, target_dBFS, increase_only=False, decrease_only=False)\n\t\n\n    if increase_only and decrease_only:\n        raise ValueError(\"Both increase only and decrease only are set to True. Please set either increase_only or decrease_only to False.\")\n\n    change_in_dBFS = target_dBFS - 20 * np.log10(np.maximum(1e-5, np.mean(np.square(wav))))\n\n    if np.isnan(change_in_dBFS) or np.isinf(change_in_dBFS):\n        if np.isnan(change_in_dBFS):\n            warn(\"Audio contains NaNs. Normalizing volume has no effect.\")\n        elif np.isinf(change_in_dBFS):\n            warn(\"Audio contains Infs. Normalizing volume has no effect.\")\n        return wav\n\n    if increase_only and change_in_dBFS < 0:\n        return wav\n    if decrease_only and change_in_dBFS > 0:\n        return wav\n\n    return wav * (10 ** (change_in_dBFS / 20))\n\n", "CodeBleu_score": 51.92978077757899}, {"code": "def normalize_volume(wav, target_dBFS, increase_only=False, decrease_only=False)\n\t\n    if increase_only and decrease_only:\n        raise ValueError(\"Both increase_only and decrease_only are set to True. Please set one of these to False.\")\n\n    change_in_dBFS = target_dBFS - wav_dBFS(wav)\n\n    if increase_only and change_in_dBFS < 0:\n        return wav\n    elif decrease_only and change_in_dBFS > 0:\n        return wav\n    else:\n        return wav * (10 ** (change_in_dBFS / 20))\n\n", "CodeBleu_score": 44.10918850336677}, {"code": "def normalize_volume(wav, target_dBFS, increase_only=False, decrease_only=False)\n\t\n    if increase_only and decrease_only:\n        raise ValueError(\"Both increase_only and decrease_only are set to True. Please set either increase_only or decrease_only to True, or set both to False.\")\n    rms = librosa.core.get_root_mean_square(wav)\n    wave_dBFS = 20 * np.log10(rms)\n    dBFS_change = target_dBFS - wave_dBFS\n    if dBFS_change < 0 and increase_only or dBFS_change > 0 and decrease_only:\n        return wav\n    return wav * (10 ** (dBFS_change / 20))\n\n", "CodeBleu_score": 58.299822033994175}]}
{"instance_id": "2353831e9033", "generation_result": [{"code": "def load_model(weights_fpath: Path, device=None)\n\t\n\n    # Initialize the model and load the state dict\n    model = SpeakerEncoder(device=device)\n    model.load_state_dict(torch.load(weights_fpath, map_location=device))\n\n    # Set to eval mode\n    model.eval()\n\n    print(f\"Model loaded from step {model.step.item()}\")\n    return model\n\n", "CodeBleu_score": 13.366215031230707}, {"code": "def load_model(weights_fpath: Path, device=None)\n\t\n    global _model\n    if device is None:\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    _model = SpeakerEncoder(device=device)\n    _model.load_state_dict(torch.load(weights_fpath, map_location=device))\n    _model.eval()\n    print(f\"Loaded model trained to step {_model.step} from {weights_fpath}\")\n\n", "CodeBleu_score": 42.04037146674012}, {"code": "def load_model(weights_fpath: Path, device=None)\n\t\n    global _model\n    if device is None:\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(\"Using device:\", device)\n    _model = SpeakerEncoder(device=device)\n    _model.load_state_dict(torch.load(weights_fpath, map_location=device))\n    _model.eval()\n    print(\"Loaded model trained to step {} from checkpoint {}\".format(_model.step, weights_fpath))\n\n", "CodeBleu_score": 45.202709030361405}]}
{"instance_id": "3231ad480199", "generation_result": [{"code": "def embed_frames_batch(frames_batch)\n\t\n    if _model is None:\n        raise Exception(\"The model is not loaded. Please load the model first.\")\n    frames_batch = torch.from_numpy(frames_batch).to(_device)\n    embed_batch = _model(frames_batch).detach().cpu().numpy()\n    return embed_batch\n\n", "CodeBleu_score": 61.39182373526617}, {"code": "def embed_frames_batch(frames_batch)\n\t\n    if _model is None:\n        raise Exception(\"The model is not loaded. Load it by calling the load_model() function.\")\n    frames_batch = torch.from_numpy(frames_batch).to(_device)\n    embed_batch = _model(frames_batch).detach().cpu().numpy()\n    return embed_batch\n\n", "CodeBleu_score": 60.73911489869606}, {"code": "def embed_frames_batch(frames_batch)\n\t\n    if _model is None:\n        raise Exception(\"Model is not loaded. Load it by calling load_model()\")\n    frames_batch = torch.from_numpy(frames_batch).to(_device)\n    embed_batch = _model(frames_batch).detach().cpu().numpy()\n    return embed_batch\n\n", "CodeBleu_score": 63.45007796326687}]}
{"instance_id": "117a41716c8d", "generation_result": [{"code": "def compute_partial_slices(n_samples, partial_utterance_n_frames=partials_n_frames,\n                           min_pad_coverage=0.75, overlap=0.5)\n\t\n    assert 0 <= overlap <= 1\n    assert 0 <= min_pad_coverage <= 1\n    assert partial_utterance_n_frames > 0\n\n    # Compute the number of frames at once\n    n_frames = int(np.ceil(n_samples / hop_size))\n    n_frames_partial = int(np.ceil(partial_utterance_n_frames / hop_size))\n    if n_frames_partial > n_frames:\n        raise ValueError(\"Partial utterance cannot be longer than %f seconds\" % (n_samples / sampling_rate))\n\n    # Compute the number of slices\n    if min_pad_coverage == 0:\n        n_slices = 1\n    else:\n        n_slices = max(1, int(np.ceil((1 - min_pad_coverage) * n_frames / n_frames_partial)))\n\n    # Compute slice step\n    slice_step = int(np.ceil((1 - overlap) * n_frames_partial))\n\n    # Compute slice indices\n    slices = [slice(i * slice_step, min(n_samples, (i + 1) * slice_step + 1))\n              for i in range(n_slices)]\n\n    # If last slice is too short, remove it\n    if slices[-1].stop - slices[-1].start < partial_utterance_n_frames:\n        slices = slices[:-1]\n\n    # Compute slice lengths\n    slice_lengths = [slice.stop - slice.start for slice in slices]\n\n    # Compute total length\n    total_length = sum(slice_lengths)\n\n    # Compute padding\n    if total_length < n_samples:\n        padding = n_samples - total_length\n        padding_start = slice_lengths[0] - padding\n        padding_stop = slice_lengths[-1] + padding\n        slices = [(slice.start + padding_start, slice.stop + padding_stop) for slice in slices]\n\n    # Convert to list of tuples\n    slices = [(slice.start, slice.stop) for slice in", "CodeBleu_score": 36.06374845427709}, {"code": "def compute_partial_slices(n_samples, partial_utterance_n_frames=partials_n_frames,\n                           min_pad_coverage=0.75, overlap=0.5)\n\t\n    assert 0 <= overlap < 1\n    assert 0 <= min_pad_coverage <= 1\n    assert partial_utterance_n_frames > 0\n\n    # Compute the number of frames at once\n    n_frames = int(np.ceil(n_samples / hop_size))\n    n_frames_partial = int(np.ceil(partial_utterance_n_frames / hop_size))\n\n    # Compute the number of slices\n    if min_pad_coverage == 0:\n        n_slices = (n_frames - n_frames_partial + 1)\n    else:\n        n_slices = max(\n            (n_frames - n_frames_partial + 1) * (1 - min_pad_coverage), 1)\n\n    # Compute step size between slices\n    step_size = int(np.floor((1 - overlap) * n_frames_partial))\n\n    # Compute frame step\n    frame_step = int(np.round(hop_size * step_size))\n\n    # Compute slice indices\n    slice_starts = [\n        int(np.round(frame_step / 2.0)) + frame_step * i for i in range(n_slices)\n    ]\n\n    # Compute slice sizes\n    slice_sizes = [n_frames_partial] * n_slices\n\n    # Compute slice indices\n    slice_ends = [s + f for s, f in zip(slice_starts, slice_sizes)]\n\n    # Compute slice coverage\n    slice_coverage = [(s, e, e - s) for s, e in zip(slice_starts, slice_ends)]\n\n    # Check for valid slices (at least min_pad_coverage)\n    valid_slice_mask = [\n        c > min_pad_coverage * partial_utterance_n_frames for s, e, c in\n        slice_coverage\n    ]\n\n    # Get valid slices\n    valid_slices = [\n        (s, e, c) for s, e, c in zip(slice_starts, slice_ends, valid_slice_mask)\n", "CodeBleu_score": 35.16342195943431}, {"code": "def compute_partial_slices(n_samples, partial_utterance_n_frames=partials_n_frames,\n                           min_pad_coverage=0.75, overlap=0.5)\n\t\n\n    # Force the right number of frames, and right frame step\n    n_frames = (n_samples + 1) // hop_size\n    frame_step = hop_size * overlap\n\n    # Ensure we can satisfy min_pad_coverage\n    assert 1 - min_pad_coverage < overlap\n\n    # Compute all slices\n    wav_slices = []\n    mel_slices = []\n    for slice_start in range(0, n_frames, partial_utterance_n_frames // frame_step):\n        slice_end = min(n_frames, slice_start + partial_utterance_n_frames)\n        wav_slices.append(slice(slice_start * hop_size, slice_end * hop_size))\n        mel_slices.append(slice(slice_start, slice_end))\n\n    # Ensure we have enough frames. This could happen if the number of samples\n    # is not a multiple of the hop size\n    if len(wav_slices) == 0 or wav_slices[-1].stop > n_samples:\n        wav_slices = wav_slices[:-1]\n        mel_slices = mel_slices[:-1]\n\n    # Ensure we have enough frames if time_steps is not a multiple of frame_step\n    remainder = (wav_slices[-1].stop - wav_slices[0].start) % frame_step\n    if remainder > 0:\n        wav_slices = wav_slices[:-1]\n        mel_slices = mel_slices[:-1]\n        assert len(wav_slices) > 0\n\n    # Take off the end piece that doesn't fit the frame_step\n    wav_slices = wav_slices[:-1]\n    mel_slices = mel_slices[:-1]\n\n    # Ensure all slices can be combined to cover the whole audio\n    assert len(wav_slices) > 0\n    assert wav_slices[0].start == 0\n    assert wav_slices[-1].stop == n_samples\n\n    # Check", "CodeBleu_score": 31.619702154695382}]}
{"instance_id": "34fe5c864f1c", "generation_result": [{"code": "def plot_embedding_as_heatmap(embed, ax=None, title=\"\", shape=None, color_range=(0, 0.30))\n\t\n    if ax is None:\n        ax = plt.gca()\n    if shape is None:\n        shape = (int(np.sqrt(len(embed))), -1)\n    embed = embed.reshape(*shape)\n    cmap = cm.get_cmap(\"RdYlBu\")\n    cax = ax.imshow(embed, interpolation=\"nearest\", cmap=cmap)\n    cax.set_clim(color_range)\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.set_title(title)\n    plt.colorbar(cax)\n\n", "CodeBleu_score": 54.82948342862526}, {"code": "def plot_embedding_as_heatmap(embed, ax=None, title=\"\", shape=None, color_range=(0, 0.30))\n\t\n\n    # Check if ax is provided, if not use current ax\n    if ax is None:\n        ax = plt.gca()\n\n    # Check if shape is provided, if not calculate it\n    if shape is None:\n        shape = (int(np.sqrt(len(embed))), int(np.sqrt(len(embed))))\n\n    # Reshape the embedding\n    embed = embed.reshape(shape)\n\n    # Plot the embedding as a heatmap\n    cmap = cm.get_cmap(\"RdYlBu_r\")\n    im = ax.imshow(embed, cmap=cmap)\n\n    # Add a color bar for reference\n    cbar = ax.figure.colorbar(im, ax=ax)\n    cbar.ax.set_ylabel(\"Embedding value\", rotation=-90, va=\"bottom\")\n\n    # Remove x and y ticks\n    ax.set_xticks([])\n    ax.set_yticks([])\n\n    # Set the title\n    ax.set_title(title)\n\n    # Set the color range\n    cbar.set_clim(color_range)\n\n", "CodeBleu_score": 61.25501143380939}, {"code": "def plot_embedding_as_heatmap(embed, ax=None, title=\"\", shape=None, color_range=(0, 0.30))\n\t\n    if ax is None:\n        ax = plt.gca()\n\n    if shape is None:\n        shape = (int(np.sqrt(len(embed))), -1)\n    embed = embed.reshape(shape)\n\n    cmap = cm.get_cmap(\"RdYlBu\")\n    cax = ax.imshow(embed, interpolation=\"nearest\", cmap=cmap)\n\n    cbar = plt.colorbar(cax, ticks=[0, 0.30])\n    cbar.set_label(\"Embedding value\", rotation=270, labelpad=15, fontsize=10)\n    cbar.set_ticklabels([round(x, 2) for x in color_range])\n\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.set_title(title, fontsize=10)\n\n", "CodeBleu_score": 61.92046643696748}]}
{"instance_id": "92e93d50b62b", "generation_result": [{"code": "def similarity_matrix(self, embeds)\n\t\n    device = embeds.device\n    num_speakers = embeds.shape[0]\n    num_utterances = embeds.shape[1]\n    centroids_incl = torch.zeros(num_speakers, embeds.shape[2], device=device)\n    centroids_excl = torch.zeros(num_speakers, embeds.shape[2], device=device)\n    similarity_matrix = torch.zeros(num_speakers, num_speakers, device=device)\n    mask = torch.eye(num_speakers, device=device)\n    for i in range(num_speakers):\n        centroids_incl[i] = torch.mean(embeds[i], dim=0)\n        centroids_incl[i] /= torch.norm(centroids_incl[i])\n        centroids_excl[i] = torch.mean(embeds[i][1:], dim=0)\n        centroids_excl[i] /= torch.norm(centroids_excl[i])\n        for j in range(num_speakers):\n            if j != i:\n                similarity_matrix[i, j] = torch.dot(centroids_incl[i], centroids_excl[j])\n                similarity_matrix[j, i] = torch.dot(centroids_excl[j], centroids_incl[i])\n    similarity_matrix = self.similarity_scaling * similarity_matrix + self.similarity_bias\n    similarity_matrix = torch.exp(similarity_matrix)\n    similarity_matrix = similarity_matrix * (1 - mask) + mask\n    return similarity_matrix\n\n", "CodeBleu_score": 30.405583725163293}, {"code": "def similarity_matrix(self, embeds)\n\t\n\n    # Determine number of speakers and utterances per speaker\n    n_speakers = embeds.shape[0]\n    n_utts_per_speaker = embeds.shape[1]\n\n    # Initialize similarity matrix\n    sim_matrix = torch.zeros(n_speakers, n_speakers, device=self.device)\n    sim_mask = torch.ones(n_speakers, n_speakers, device=self.device)\n\n    # Compute centroids\n    centroids_incl = torch.zeros(n_speakers, self.model_embedding_size, device=self.device)\n    centroids_excl = torch.zeros(n_speakers, self.model_embedding_size, device=self.device)\n    for s in range(n_speakers):\n        centroids_incl[s, :] = torch.mean(embeds[s, :, :], dim=0)\n        centroids_excl[s, :] = torch.mean(embeds[s, 1:, :], dim=0)\n    centroids_incl = torch.nn.functional.normalize(centroids_incl, p=2, dim=1)\n    centroids_excl = torch.nn.functional.normalize(centroids_excl, p=2, dim=1)\n\n    # Compute similarity scores\n    for s in range(n_speakers):\n        sim_matrix[s, :] = torch.nn.functional.cosine_similarity(centroids_incl[s, :], centroids_excl, dim=1)\n        sim_mask[s, s] = 0\n\n    # Scale and bias similarity matrix\n    sim_matrix = self.similarity_scale * sim_matrix + self.similarity_bias\n\n    return sim_matrix, sim_mask\n\n", "CodeBleu_score": 30.91037883637674}, {"code": "def similarity_matrix(self, embeds)\n\t\n    # determine number of speakers and utterances per speaker\n    n_spkrs, n_utts_per_spkr = embeds.size(0), embeds.size(1)\n    # initialize similarity matrix\n    sim_mat = torch.zeros(n_spkrs, n_spkrs, device=self.device)\n    # initialize mask matrix\n    mask = torch.eye(n_spkrs, dtype=torch.bool, device=self.device)\n    # iterate over speakers\n    for spkr_idx in range(n_spkrs):\n        # compute centroids\n        centroids_incl = embeds[:, spkr_idx].unsqueeze(1)\n        centroids_excl = torch.mean(embeds[:, :spkr_idx] + embeds[:, spkr_idx + 1:], dim=1, keepdim=True)\n        # compute similarity scores\n        sim_score_incl = torch.nn.functional.cosine_similarity(centroids_incl, embeds, dim=2)\n        sim_score_excl = torch.nn.functional.cosine_similarity(centroids_excl, embeds, dim=2)\n        # update similarity matrix\n        sim_mat[spkr_idx, :spkr_idx] = sim_score_excl[:spkr_idx]\n        sim_mat[spkr_idx, spkr_idx + 1:] = sim_score_excl[spkr_idx + 1:]\n        sim_mat[spkr_idx, spkr_idx] = sim_score_incl\n        # update mask\n        mask[spkr_idx, :spkr_idx] = 1\n        mask[spkr_idx, spkr_idx + 1:] = 1\n    # scale and bias similarity matrix\n    sim_mat = self.similarity_scaling * sim_mat\n    sim_mat = sim_mat + self.similarity_bias\n    # apply mask\n    sim_mat = sim_mat.masked_fill(mask, -1e30)\n    return sim_mat\n", "CodeBleu_score": 30.43395657595351}]}
{"instance_id": "a6eba54108a1", "generation_result": [{"code": "def loss(self, embeds)\n\t\n    # Determine the number of speakers and utterances per speaker\n    num_speakers = embeds.shape[0]\n    num_utterances = embeds.shape[1]\n\n    # Compute the similarity matrix\n    sim_mat = self.similarity_fn(embeds, embeds)\n    sim_mat = sim_mat.reshape(num_speakers * num_utterances, num_speakers * num_utterances)\n\n    # Generate ground truth labels\n    labels = torch.arange(num_speakers * num_utterances).long()\n    labels = (labels.reshape(num_speakers, num_utterances) == labels.reshape(num_speakers, num_utterances).transpose(0, 1)).long()\n    labels = labels.reshape(num_speakers * num_utterances)\n\n    # Calculate loss\n    loss = self.loss_fn(sim_mat, labels)\n\n    # Compute EER\n    scores = sim_mat.detach().cpu().numpy()\n    labels = labels.detach().cpu().numpy()\n    fpr, tpr, thresholds = roc_curve(labels, scores, pos_label=1)\n    eer = brentq(lambda x: 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n\n    return loss, eer\n\n", "CodeBleu_score": 27.266123101112544}, {"code": "def loss(self, embeds)\n\t\n    # Determine the number of speakers and utterances per speaker\n    n_spks, n_utts_per_spk = embeds.shape[:2]\n\n    # Compute the similarity matrix\n    sim_matrix = self.similarity_function(embeds.view(n_spks * n_utts_per_spk, -1))\n    sim_matrix = sim_matrix.view(n_spks, n_utts_per_spk, -1)\n\n    # Reshape the similarity matrix to compute the loss\n    labels = torch.arange(n_spks * n_utts_per_spk).cuda()\n    labels = (labels // n_utts_per_spk).long()\n    labels = labels.repeat(n_utts_per_spk).view(n_spks, n_utts_per_spk)\n    sim_matrix = sim_matrix.permute(0, 2, 1).contiguous()\n    sim_matrix = sim_matrix.view(-1, n_spks)\n\n    # Compute the loss\n    loss = self.loss_function(sim_matrix, labels)\n\n    # Compute the EER\n    with torch.no_grad():\n        # Generate the predicted labels\n        predicted_labels = torch.argmax(sim_matrix, dim=1)\n\n        # Compute the ROC curve\n        fpr, tpr, thresholds = roc_curve(labels.cpu(), sim_matrix.cpu())\n\n        # Find the threshold that gives the EER\n        eer = brentq(lambda x: 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n        thresh = interp1d(fpr, thresholds)(eer)\n\n        # Compute the EER\n        y_pred = interp1d(thresholds, tpr)(thresh)\n        y_true = (predicted_labels == labels).float()\n        diff = y_true - y_pred\n        acc = torch.sqrt(torch.pow(diff, 2).mean())\n\n   ", "CodeBleu_score": 51.411138593379825}, {"code": "def loss(self, embeds)\n\t\n    # Determine the number of speakers and utterances per speaker\n    batch_size, n_utts_per_speaker, n_dims = embeds.shape\n    n_speakers = batch_size // n_utts_per_speaker\n    # Compute the similarity matrix\n    sim_matrix = self.similarity_function(embeds, embeds)\n    # Reshape the similarity matrix to [n_speakers, n_utts_per_speaker, n_utts_per_speaker]\n    sim_matrix = sim_matrix.view(n_speakers, n_utts_per_speaker, n_utts_per_speaker)\n    # Generate the ground truth labels\n    labels = torch.arange(n_speakers).unsqueeze(1).repeat(1, n_utts_per_speaker).view(-1)\n    # Compute the loss\n    loss = self.criterion(sim_matrix, labels)\n    # Compute the EER\n    with torch.no_grad():\n        # Get the scores for the positive and negative pairs\n        pos_scores = torch.diag(sim_matrix)\n        neg_scores = sim_matrix.flatten().view(n_speakers, -1)[:, 1:]\n        # Concatenate the positive and negative scores\n        scores = torch.cat([pos_scores, neg_scores])\n        # Get the binary labels\n        labels = torch.cat([torch.ones(n_speakers), torch.zeros(n_speakers * (n_utts_per_speaker - 1))])\n        # Compute the ROC curve\n        fpr, tpr, thresholds = roc_curve(labels, scores)\n        # Compute the EER\n        eer = brentq(lambda x: 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n    return loss, eer\n\n", "CodeBleu_score": 47.29387835656516}]}
{"instance_id": "ac7b74b9d1b1", "generation_result": [{"code": "def log_params(self)\n\t\n    if self.log_parameters:\n        params = self.model.get_config()\n        params.update(self.data.get_config())\n        params_str = \"Parameters:", "CodeBleu_score": 4.606302316350449}, {"code": "def log_params(self)\n\t\n\n    if not self.log_params:\n        return\n\n    # Get the parameters from the model and data classes\n    model_params = vars(self.model)\n    data_params = vars(self.data)\n\n    # Get the parameter names and values from the model and data classes\n    model_params_names = [key for key in model_params.keys()]\n    model_params_values = [str(model_params[key]) for key in model_params.keys()]\n\n    data_params_names = [key for key in data_params.keys()]\n    data_params_values = [str(data_params[key]) for key in data_params.keys()]\n\n    # Join the names and values into a single string\n    model_params_string = \"Model Parameters", "CodeBleu_score": 19.423631218532194}, {"code": "def log_params(self)\n\t\n    if self.params.log_params:\n        params = self.params\n        print('", "CodeBleu_score": 3.3024138286396907}]}
{"instance_id": "4726b7864386", "generation_result": [{"code": "def draw_projections(self, embeds, utterances_per_speaker, step, out_fpath=None, max_speakers=10)\n\t\n    if self.config.visdom:\n        if self.visdom_win is None:\n            self.visdom_win = self.viz.line(X=torch.zeros((1, 2)), Y=torch.zeros((1, 1)), opts=dict(title=\"UMAP Projections\", markers=False))\n        self.viz.line(X=torch.zeros((1, 2)), Y=torch.zeros((1, 1)), win=self.visdom_win, update='replace')\n    if self.config.tensorboard:\n        self.writer.add_embedding(embeds, metadata=utterances_per_speaker, label_img=None, global_step=step)\n    if self.config.wandb:\n        self.wandb.log({\"UMAP Projections\": [wandb.Image(self.viz.get_image(self.visdom_win))]}, step=step)\n    if self.config.umap:\n        embeds = embeds.cpu().detach().numpy()\n        embeds = embeds.reshape(embeds.shape[0], -1)\n        if self.config.umap_speaker_identity:\n            embeds = embeds.reshape(embeds.shape[0], -1)\n            embeds = embeds.reshape(embeds.shape[0], -1)\n            embeds = embeds.reshape(embeds.shape[0], -1)\n            embeds = embeds.reshape(embeds.shape[0], -1)\n            embeds = embeds.reshape(embeds.shape[0], -1)\n            embeds = embeds.reshape(embeds.shape[0], -1)\n            embeds = embeds.reshape(embeds.shape[0], -1)\n            embeds = embeds.reshape(embeds.shape[0], -1)\n            embeds = embeds.reshape(embeds.shape[0], -1)\n            embeds = embeds.reshape(embeds.shape[0], -1)\n            embeds = embeds.reshape(embeds.shape[", "CodeBleu_score": 31.50332235181521}, {"code": "def draw_projections(self, embeds, utterances_per_speaker, step, out_fpath=None, max_speakers=10)\n\t\n    # Limit the number of speakers\n    speakers = embeds.keys()\n    speakers = speakers[:min(len(speakers), max_speakers)]\n    embeds = {speaker: embeds[speaker] for speaker in speakers}\n\n    # Flatten the embeddings\n    embeds = [embed for speaker in embeds for embed in embeds[speaker]]\n\n    # Assign colors based on speaker identity\n    colors = [self.colors[speaker] for speaker in speakers for _ in embeds[speaker]]\n\n    # UMAP\n    reducer = umap.UMAP(\n        n_neighbors=10,\n        min_dist=0.0,\n        metric=\"cosine\",\n        low_memory=False,\n        random_state=self.seed,\n    )\n    embeds = reducer.fit_transform(embeds)\n\n    # Plot\n    if self.visdom:\n        self.vis.line(\n            X=embeds[:, 0],\n            Y=embeds[:, 1],\n            opts=dict(\n                title=f\"UMAP Projections (Step {step})\",\n                markers=True,\n                markersymbol=\"circle\",\n                markersize=10,\n                markercolor=colors,\n            ),\n        )\n    else:\n        plt.scatter(embeds[:, 0], embeds[:, 1], c=colors)\n        plt.title(f\"UMAP Projections (Step {step})\")\n        plt.show()\n\n    # Save\n    if out_fpath:\n        plt.savefig(out_fpath)\n\n", "CodeBleu_score": 40.01369976707794}, {"code": "def draw_projections(self, embeds, utterances_per_speaker, step, out_fpath=None, max_speakers=10)\n\t\n\n    # Limit the number of speakers to max_speakers\n    speaker_ids = list(self.speaker_embeddings.keys())\n    if len(speaker_ids) > max_speakers:\n        speaker_ids = sorted(speaker_ids)[-max_speakers:]\n\n    # Flatten the embeddings\n    embeds = [self.speaker_embeddings[speaker_id] for speaker_id in speaker_ids]\n    embeds = [embed for embed_utterances in embeds for embed in embed_utterances]\n\n    # Assign colors based on speaker identity\n    colors = [self.speaker_colors[speaker_id] for speaker_id in speaker_ids for _ in range(utterances_per_speaker)]\n\n    # UMAP reduces the dimensionality of the embeddings, which are then plotted and displayed\n    embeds = umap.UMAP(n_neighbors=5, min_dist=0.3, metric=\"cosine\").fit_transform(embeds)\n    self.vis.scatter(\n        X=embeds,\n        Y=colors,\n        win=\"speaker_projections\",\n        opts=dict(\n            title=\"Speaker Embeddings (Step \" + str(step) + \")\",\n            markersize=4,\n            xlabel=\"UMAP 1\",\n            ylabel=\"UMAP 2\",\n        ),\n    )\n\n", "CodeBleu_score": 32.74407483177217}]}
{"instance_id": "274fa50985c0", "generation_result": [{"code": "def add_speaker_embedding(self, x, speaker_embedding)\n\t\n    # Save the dimensions of the input tensor as human-readable names and determine the appropriate dimension index for the speaker embedding\n    x_dimensions = {\n        \"batch_size\": x.size(0),\n        \"sequence_length\": x.size(1),\n        \"speaker_embedding_dimension\": speaker_embedding.size(1),\n    }\n    x_dimension_names = {\n        \"batch_size\": \"Batch size\",\n        \"sequence_length\": \"Sequence length\",\n        \"speaker_embedding_dimension\": \"Speaker embedding dimension\",\n    }\n    x_dimension_index = {\n        \"batch_size\": 0,\n        \"sequence_length\": 1,\n        \"speaker_embedding_dimension\": 2,\n    }\n\n    # Repeat the speaker embedding to match the length of the input text, reshape, and transpose it\n    speaker_embedding = speaker_embedding.repeat(\n        x_dimensions[\"sequence_length\"], 1\n    ).reshape(\n        x_dimensions[\"batch_size\"], x_dimensions[\"sequence_length\"], -1\n    ).transpose(\n        1, 2\n    )\n\n    # Concatenate the speaker embedding with the input tensor along the last dimension and return the result\n    return torch.cat([x, speaker_embedding], dim=x_dimension_index[\"speaker_embedding_dimension\"])\n\n", "CodeBleu_score": 23.88936980182464}, {"code": "def add_speaker_embedding(self, x, speaker_embedding)\n\t\n    # save the dimensions of the input tensor as human-readable names\n    input_dimensions = {\n        \"batch_size\": x.shape[0],\n        \"sequence_length\": x.shape[1],\n        \"num_features\": x.shape[2],\n    }\n\n    # determine the appropriate dimension index for the speaker embedding\n    if self.use_speaker_embedding:\n        if self.use_gst:\n            speaker_embedding_dimension = 1\n        else:\n            speaker_embedding_dimension = 0\n    else:\n        speaker_embedding_dimension = None\n\n    # repeat the speaker embedding to match the length of the input text\n    if speaker_embedding_dimension is not None:\n        speaker_embedding = speaker_embedding.repeat(\n            input_dimensions[\"sequence_length\"], 1, 1\n        )\n\n    # reshape the speaker embedding\n    if speaker_embedding_dimension is not None:\n        speaker_embedding = speaker_embedding.reshape(\n            (\n                input_dimensions[\"batch_size\"],\n                input_dimensions[\"sequence_length\"],\n                -1,\n            )\n        )\n\n    # transpose the speaker embedding\n    if speaker_embedding_dimension is not None:\n        speaker_embedding = speaker_embedding.transpose(\n            speaker_embedding_dimension, 2\n        )\n\n    # concatenate the speaker embedding with the input tensor\n    if speaker_embedding_dimension is not None:\n        x = torch.cat((x, speaker_embedding), dim=2)\n\n    return x\n\n", "CodeBleu_score": 29.1859995579294}, {"code": "def add_speaker_embedding(self, x, speaker_embedding)\n\t\n    # Save the dimensions of the input tensor as human-readable names\n    batch_size = x.size(0)\n    num_chars = x.size(1)\n    num_features = x.size(2)\n    # Determine the appropriate dimension index for the speaker embedding\n    if self.embedding_dim is None:\n        embedding_dim = speaker_embedding.size(1)\n        if self.use_cuda:\n            x = x.cuda()\n            speaker_embedding = speaker_embedding.cuda()\n        x = torch.cat([x, speaker_embedding.repeat(batch_size, num_chars, 1)], dim=2)\n    else:\n        embedding_dim = self.embedding_dim\n        x = torch.cat([x, speaker_embedding], dim=2)\n    return x\n\n", "CodeBleu_score": 36.24432343596194}]}
{"instance_id": "e6742ea0fe5e", "generation_result": [{"code": "def init_attention(self, encoder_seq_proj)\n\t\n    self.attention_cumulative = torch.zeros(\n        encoder_seq_proj.shape[0], encoder_seq_proj.shape[1], device=self.device\n    )\n    self.attention_weights = torch.zeros(\n        encoder_seq_proj.shape[0], encoder_seq_proj.shape[1], device=self.device\n    )\n\n", "CodeBleu_score": 18.644755393600057}, {"code": "def init_attention(self, encoder_seq_proj)\n\t\n    device = next(self.parameters()).device\n    self.attention = torch.zeros(encoder_seq_proj.shape[0], encoder_seq_proj.shape[1], device=device)\n    self.cumulative_attention = torch.zeros(encoder_seq_proj.shape[0], encoder_seq_proj.shape[1], device=device)\n\n", "CodeBleu_score": 22.259474120055685}, {"code": "def init_attention(self, encoder_seq_proj)\n\t\n    device = next(self.parameters()).device\n    self.cumulative_attention = torch.zeros(encoder_seq_proj.shape[0], encoder_seq_proj.shape[1], device=device)\n    self.attention = torch.zeros(encoder_seq_proj.shape[0], encoder_seq_proj.shape[1], device=device)\n\n", "CodeBleu_score": 22.75459479285599}]}
{"instance_id": "4e1fc1ed0ed7", "generation_result": [{"code": "def load(self, path, optimizer=None)\n\t\n    if not os.path.exists(path):\n        raise FileNotFoundError(f\"The model checkpoint {path} does not exist\")\n    device = next(self.parameters()).device\n    checkpoint = torch.load(path, map_location=device)\n    self.load_state_dict(checkpoint[\"state_dict\"])\n    if optimizer is not None:\n        optimizer.load_state_dict(checkpoint[\"optimizer\"])\n\n", "CodeBleu_score": 63.395526616168006}, {"code": "def load(self, path, optimizer=None)\n\t\n\n    # Set the device to the model's parameter device\n    self.to(next(self.parameters()).device)\n\n    # Load the checkpoint\n    checkpoint = torch.load(path, map_location=self.device)\n\n    # Update the model's state dictionary\n    self.load_state_dict(checkpoint[\"model_state_dict\"])\n\n    # Update the optimizer's state dictionary if an optimizer is provided\n    if optimizer is not None:\n        optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n\n", "CodeBleu_score": 41.51184613744698}, {"code": "def load(self, path, optimizer=None)\n\t\n    if isinstance(path, str):\n        path = Path(path)\n\n    if not path.exists():\n        raise FileNotFoundError(f\"Checkpoint file {path} does not exist.\")\n\n    checkpoint = torch.load(path, map_location=self.device)\n\n    self.load_state_dict(checkpoint[\"state_dict\"])\n\n    if optimizer is not None:\n        optimizer.load_state_dict(checkpoint[\"optimizer\"])\n\n", "CodeBleu_score": 49.00177645339294}]}
{"instance_id": "178e6aa16c0f", "generation_result": [{"code": "def save(self, path, optimizer=None)\n\t\n    if isinstance(path, str):\n        path = Path(path)\n    elif isinstance(path, Path):\n        pass\n    else:\n        raise TypeError(f\"The path argument must be a str or a Path object, but got {type(path)}\")\n    if not path.exists():\n        path.mkdir(parents=True)\n    torch.save(self.state_dict(), os.path.join(path, \"model.pth\"))\n    if optimizer is not None:\n        torch.save(optimizer.state_dict(), os.path.join(path, \"optimizer.pth\"))\n\n", "CodeBleu_score": 55.3951895295307}, {"code": "def save(self, path, optimizer=None)\n\t\n    if optimizer is not None:\n        torch.save({\n            'model_state_dict': self.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict()\n        }, path)\n    else:\n        torch.save({\n            'model_state_dict': self.state_dict()\n        }, path)\n\n", "CodeBleu_score": 68.20396178291797}, {"code": "def save(self, path, optimizer=None)\n\t\n    path = Path(path)\n    path.parent.mkdir(parents=True, exist_ok=True)\n    torch.save(self.state_dict(), path)\n    if optimizer is not None:\n        torch.save(optimizer.state_dict(), path.with_suffix(\".opt\"))\n\n", "CodeBleu_score": 45.52557392155883}]}
{"instance_id": "6528fd80e87b", "generation_result": [{"code": "def english_cleaners(text)\n\t\n    text = unidecode(text)\n    text = normalize_numbers(text)\n    text = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", text)\n    text = re.sub(r\"\\'s\", \" \\'s\", text)\n    text = re.sub(r\"\\'ve\", \" \\'ve\", text)\n    text = re.sub(r\"n\\'t\", \" n\\'t\", text)\n    text = re.sub(r\"\\'re\", \" \\'re\", text)\n    text = re.sub(r\"\\'d\", \" \\'d\", text)\n    text = re.sub(r\"\\'ll\", \" \\'ll\", text)\n    text = re.sub(r\",\", \" , \", text)\n    text = re.sub(r\"!\", \" ! \", text)\n    text = re.sub(r\"\\(\", \" \\( \", text)\n    text = re.sub(r\"\\)\", \" \\) \", text)\n    text = re.sub(r\"\\?\", \" \\? \", text)\n    text = re.sub(r\"\\s{2,}\", \" \", text)\n    return text.strip().lower()\n\n", "CodeBleu_score": 20.91927645508429}, {"code": "def english_cleaners(text)\n\t\n    text = unidecode(text)\n    text = normalize_numbers(text)\n    text = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", text)\n    text = re.sub(r\"\\s{2,}\", \" \", text)\n    return text.strip().lower()\n\n", "CodeBleu_score": 22.205127064548215}, {"code": "def english_cleaners(text)\n\t\n    text = unidecode(text)\n    text = normalize_numbers(text)\n    text = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", text)\n    text = re.sub(r\"\\'s\", \" \\'s\", text)\n    text = re.sub(r\"\\'ve\", \" \\'ve\", text)\n    text = re.sub(r\"n\\'t\", \" n\\'t\", text)\n    text = re.sub(r\"\\'re\", \" \\'re\", text)\n    text = re.sub(r\"\\'d\", \" \\'d\", text)\n    text = re.sub(r\"\\'ll\", \" \\'ll\", text)\n    text = re.sub(r\",\", \" , \", text)\n    text = re.sub(r\"!\", \" ! \", text)\n    text = re.sub(r\"\\(\", \" \\( \", text)\n    text = re.sub(r\"\\)\", \" \\) \", text)\n    text = re.sub(r\"\\?\", \" \\? \", text)\n    text = re.sub(r\"\\s{2,}\", \" \", text)\n    return text.strip().lower()\n\n", "CodeBleu_score": 20.91927645508429}]}
{"instance_id": "8a4fe08e0985", "generation_result": [{"code": "def text_to_sequence(text, cleaner_names)\n\t\n    sequence = []\n\n    # Check for curly braces and treat their contents as ARPAbet:\n    while len(text):\n        m = re.match(r'([^{]+)(.*)', text)\n        if not m:\n            clean_text = cleaners.clean_text(text, cleaner_names)\n            sequence += _symbols_to_sequence(clean_text)\n            break\n        clean_text, text = m.groups()\n        clean_text = cleaners.clean_text(clean_text, cleaner_names)\n        sequence += _symbols_to_sequence(clean_text)\n\n    # Append EOS token\n    sequence.append(_symbol_to_id['~'])\n    return sequence\n\n", "CodeBleu_score": 42.06408597305362}, {"code": "def text_to_sequence(text, cleaner_names)\n\t\n    sequence = []\n    # Check for curly braces and treat their contents as ARPAbet:\n    pattern = '([^{]+)(({[^}]+})+)?(.)'\n    matches = re.finditer(pattern, text)\n    for _, groups in enumerate(matches):\n        # Regular text segment.\n        start, end = groups.span(1)\n        sequence += _symbols_to_sequence(_clean_text(text[start:end], cleaner_names))\n        # ARPAbet segment.\n        start, end = groups.span(2)\n        if start > 0:\n            sequence += _arpabet_to_sequence(text[start:end])\n        # Regular text segment.\n        start, end = groups.span(4)\n        sequence += _symbols_to_sequence(_clean_text(text[start:end], cleaner_names))\n    # Append EOS token\n    sequence.append(_symbol_to_id['~'])\n    return sequence\n\n", "CodeBleu_score": 37.54031173074337}, {"code": "def text_to_sequence(text, cleaner_names)\n\t\n    sequence = []\n\n    # Check for curly braces and treat their contents as ARPAbet:\n    while len(text):\n        m = re.match(r'([^{]+)(.*)', text)\n        if not m:\n            clean_text = cleaner_names[0](text)\n            sequence += _symbols_to_sequence(clean_text)\n            break\n        clean_text, text = m.groups()\n        clean_text = cleaner_names[0](clean_text)\n        sequence += _symbols_to_sequence(clean_text)\n\n    # Append EOS token\n    sequence.append(_symbol_to_id['~'])\n    return sequence\n\n", "CodeBleu_score": 42.33316180258523}]}
{"instance_id": "e1c25a37af1f", "generation_result": [{"code": "def sequence_to_text(sequence)\n\t\n    result = ''\n    for symbol_id in sequence:\n        if symbol_id in symbols:\n            s = symbols[symbol_id]\n            # Enclose ARPAbet back in curly braces:\n            if len(s) > 1 and s[0] == '@':\n                s = '{%s}' % s[1:]\n            result += s\n    return result.replace('}{', ' ')\n\n", "CodeBleu_score": 64.46067815539703}, {"code": "def sequence_to_text(sequence)\n\t\n    result = ''\n    for symbol_id in sequence:\n        if symbol_id == 0:\n            break\n        symbol = symbols[symbol_id]\n        if symbol == '|':\n            result += ' '\n        else:\n            result += symbol\n    return result.replace('}{', ' ')\n\n", "CodeBleu_score": 35.83381526467824}, {"code": "def sequence_to_text(sequence)\n\t\n    result = ''\n    for symbol_id in sequence:\n        if symbol_id in symbols:\n            s = symbols[symbol_id]\n            # Enclose ARPAbet back in curly braces.\n            if len(s) > 1 and s[0] == '@':\n                s = '{%s}' % s[1:]\n            result += s\n    return result.replace('}{', ' ')\n\n", "CodeBleu_score": 64.46067815539703}]}
{"instance_id": "7807cad4b05d", "generation_result": [{"code": "def embed_utterance(fpaths, encoder_model_fpath)\n\t\n    # Load the encoder model if it is not already loaded\n    encoder.load_model(encoder_model_fpath)\n    # Compute the speaker embedding of the utterance\n    embed = encoder.embed_utterance(fpaths)\n    # Save the speaker embedding\n    return embed\n\n", "CodeBleu_score": 9.501590929463836}, {"code": "def embed_utterance(fpaths, encoder_model_fpath)\n\t\n    # Load the encoder model\n    encoder.load_model(encoder_model_fpath)\n\n    # Compute the speaker embedding of the utterance\n    embeds = encoder.embed_utterance(fpaths)\n\n    # Save the speaker embedding to disk\n    np.save(fpaths.with_suffix(\".embeds\"), embeds)\n\n", "CodeBleu_score": 12.372164516041273}, {"code": "def embed_utterance(fpaths, encoder_model_fpath)\n\t\n    # Load the encoder model\n    encoder.load_model(encoder_model_fpath)\n\n    # Loop over all utterances\n    for utterance_fpath in fpaths:\n        # Load the utterance\n        wav = audio.load_wav(utterance_fpath)\n\n        # Compute the utterance embedding\n        wav = encoder.preprocess_wav(wav)\n        embed = encoder.embed_utterance(wav)\n\n        # Save the utterance embedding\n        np.save(utterance_fpath, embed, allow_pickle=False)\n\n", "CodeBleu_score": 57.20556915307367}]}
{"instance_id": "9f860273a325", "generation_result": [{"code": "def create_embeddings(synthesizer_root: Path, encoder_model_fpath: Path, n_processes: int)\n\t\n    # Verify we have the correct files in the synthesizer folder\n    assert (synthesizer_root / \"metadata.csv\").exists(), \"metadata.csv file does not exist in the synthesizer folder\"\n    assert (synthesizer_root / \"wavs\").exists(), \"wavs folder does not exist in the synthesizer folder\"\n    assert encoder_model_fpath.exists(), \"Encoder model does not exist\"\n\n    # Read the metadata file\n    metadata_fpath = synthesizer_root / \"metadata.csv\"\n    with metadata_fpath.open(\"r\") as metadata_file:\n        metadata = [line for line in metadata_file]\n    metadata = [line.split(\"|\") for line in metadata]\n    mel_fpaths = [synthesizer_root / \"mels\" / f\"{row[1]}.npy\" for row in metadata]\n    wav_fpaths = [synthesizer_root / \"wavs\" / f\"{row[0]}.wav\" for row in metadata]\n\n    # Prepare the encoder, pool and function for processing an audio file\n    encoder.load_model(encoder_model_fpath)\n    encoder.model.eval()\n    encoder.inference_batch_size = 1\n    pool = Pool(n_processes)\n    partial_process_utterance = partial(process_utterance, encoder_model_fpath)\n\n    # Process the utterances\n    with tqdm(total=len(wav_fpaths)) as pbar:\n        for i, (mel, wav) in enumerate(pool.imap(partial_process_utterance, wav_fpaths)):\n            mel_fpath = mel_fpaths[i]\n            mel_fpath.parent.mkdir(exist_ok=True)\n            mel_fpath.write_bytes(mel)\n            pbar.update(1)\n\n    # Cleanup\n    pool.close()\n    pool.join()\n    encoder.clean_up()\n\n", "CodeBleu_score": 44.398955215160576}, {"code": "def create_embeddings(synthesizer_root: Path, encoder_model_fpath: Path, n_processes: int)\n\t\n\n    # Verify we have the correct files in the synthesizer root\n    assert (synthesizer_root / \"preprocessed_data\").is_dir(), \\\n        f\"{synthesizer_root / 'preprocessed_data'} not found. Was synthesizer preprocessed?\"\n    assert encoder_model_fpath.is_file(), f\"{encoder_model_fpath} not found. Was encoder trained?\"\n\n    # Read the metadata\n    with open(synthesizer_root.joinpath(\"preprocessed_data/train_speakers.txt\"), encoding=\"utf-8\") as f:\n        speakers = [line.strip() for line in f if line.strip()]\n    with open(synthesizer_root.joinpath(\"preprocessed_data/train_texts.txt\"), encoding=\"utf-8\") as f:\n        texts = [line.strip() for line in f if line.strip()]\n    assert all(speaker in speakers for speaker in texts)\n\n    # Prepare the directories for the embeddings\n    embed_dir = synthesizer_root.joinpath(\"embeddings\")\n    embed_dir.mkdir(exist_ok=True)\n    for speaker in speakers:\n        speaker_dir = embed_dir.joinpath(speaker)\n        speaker_dir.mkdir(exist_ok=True)\n\n    # Prepare the file paths\n    wav_fpaths = [\n        synthesizer_root.joinpath(\"preprocessed_data/wavs/clean\", speaker, f\"{speaker}_{text.replace(' ', '_')}.wav\")\n        for speaker in speakers for text in texts\n    ]\n    mel_fpaths = [\n        synthesizer_root.joinpath(\"preprocessed_data/mels/clean\", speaker, f\"{speaker}_{text.replace(' ', '_')}.npy\")\n        for speaker in speakers for text in texts\n    ]\n    embed_fpaths = [\n        embed_dir.joinpath(speaker, f\"{speaker}_{text.replace(' ', '_')}.npy\")\n        for speaker in speakers for text in texts\n    ]\n", "CodeBleu_score": 34.840286528710415}, {"code": "def create_embeddings(synthesizer_root: Path, encoder_model_fpath: Path, n_processes: int)\n\t\n\n    # Checks files and directories\n    assert encoder_model_fpath.exists(), f\"Encoder model does not exist at {encoder_model_fpath}\"\n    assert (\n        synthesizer_root.exists()\n    ), f\"Synthesizer root directory does not exist at {synthesizer_root}\"\n    assert (\n        synthesizer_root / \"train\" / \"mels\"\n    ).exists(), f\"Training mels directory does not exist at {synthesizer_root / 'train' / 'mels'}\"\n    assert (\n        synthesizer_root / \"test\" / \"mels\"\n    ).exists(), f\"Test mels directory does not exist at {synthesizer_root / 'test' / 'mels'}\"\n    assert (\n        synthesizer_root / \"train\" / \"audio\"\n    ).exists(), f\"Training audio directory does not exist at {synthesizer_root / 'train' / 'audio'}\"\n    assert (\n        synthesizer_root / \"test\" / \"audio\"\n    ).exists(), f\"Test audio directory does not exist at {synthesizer_root / 'test' / 'audio'}\"\n\n    # Prepares file paths\n    metadata_fpath = synthesizer_root.joinpath(\"train/metadata.csv\")\n    mel_fpaths = list(synthesizer_root.joinpath(\"train/mels\").glob(\"*.npy\"))\n    embed_fpaths = [\n        synthesizer_root.joinpath(\"train/embeddings/speaker_encoder\", fpath.name)\n        for fpath in mel_fpaths\n    ]\n\n    # Prepares the encoder\n    encoder.load_model(encoder_model_fpath)\n\n    # Processes the metadata\n    with metadata_fpath.open(\"r\") as metadata_file:\n        metadata = [line.split(\"|\") for line in metadata_file]\n    speakers = list(set([m[4] for m in metadata]))\n    speaker_to_idx = {speaker: i for i, speaker in enumerate(speakers)}\n\n    # Processes each file in parallel\n    with Pool(n_", "CodeBleu_score": 46.2334077608005}]}
{"instance_id": "9eb0abda14a9", "generation_result": [{"code": "def eval_model(attention, mel_prediction, target_spectrogram, input_seq, step,\n               plot_dir, mel_output_dir, wav_dir, sample_num, loss, hparams)\n\t\n    # Save attention plot (mel_prediction is used as target for plotting)\n    attention_path = plot_dir.joinpath(\"step_{}_sample_{}_attention.png\".format(step, sample_num))\n    plot_spectrogram(attention[sample_num].cpu().data.numpy(),\n                     str(attention_path),\n                     title=\"Attention\",\n                     target=mel_prediction.cpu().data.numpy()[sample_num])\n\n    # Save predicted mel spectrogram plot\n    mel_prediction_path = plot_dir.joinpath(\"step_{}_sample_{}_mel_prediction.png\".format(step, sample_num))\n    plot_spectrogram(mel_prediction[sample_num].cpu().data.numpy(),\n                     str(mel_prediction_path),\n                     title=\"Predicted mel spectrogram\",\n                     target=target_spectrogram[sample_num].cpu().data.numpy())\n\n    # Save griffin lim inverted wav for predicted mel spectrogram\n    wav_prediction_path = wav_dir.joinpath(\"step_{}_sample_{}_predicted.wav\".format(step, sample_num))\n    audio.save_wav(wav_prediction_path,\n                   audio.inv_mel_spectrogram(mel_prediction[sample_num].cpu().data.numpy()),\n                   hparams.sample_rate)\n\n    # Save target mel spectrogram plot\n    mel_target_path = plot_dir.joinpath(\"step_{}_sample_{}_mel_target.png\".format(step, sample_num))\n    plot_spectrogram(target_spectrogram[sample_num].cpu().data.numpy(),\n                     str(mel_target_path),\n                     title=\"Target mel spectrogram\",\n                     target=target_spectrogram[sample_num].cpu().data.numpy())\n\n    # Save griffin lim inverted wav for target mel spectrogram\n    wav_target_path = wav_dir.joinpath(\"step_{}_sample_{}_target.wav\".format(step, sample_num))\n    audio.save", "CodeBleu_score": 41.70716550147141}, {"code": "def eval_model(attention, mel_prediction, target_spectrogram, input_seq, step,\n               plot_dir, mel_output_dir, wav_dir, sample_num, loss, hparams)\n\t\n    # Save attention plot to disk\n    Path(plot_dir).mkdir(exist_ok=True)\n    attention_path = str(plot_dir.joinpath(\"step_{:09d}_sample_{}_attention.png\".format(step, sample_num)))\n    save_attention(attention_path, attention, input_seq)\n\n    # Save predicted mel spectrogram to disk\n    Path(mel_output_dir).mkdir(exist_ok=True)\n    mel_output_path = str(mel_output_dir.joinpath(\"step_{:09d}_sample_{}_mel_output.npy\".format(step, sample_num)))\n    np.save(mel_output_path, mel_prediction.cpu().data.numpy(), allow_pickle=False)\n\n    # Save griffin lim inverted wav for debugging\n    if wav_dir is not None:\n        wav_output_path = str(wav_dir.joinpath(\"step_{:09d}_sample_{}_predicted.wav\".format(step, sample_num)))\n        audio.save_wav(wav_output_path, audio.inv_mel_spectrogram(mel_prediction.cpu().data.numpy()), hparams)\n\n    # Save real and predicted mel-spectrogram plot to disk (control purposes)\n    Path(mel_output_dir).mkdir(exist_ok=True)\n    mel_output_path = str(mel_output_dir.joinpath(\"step_{:09d}_sample_{}_mel_output.png\".format(step, sample_num)))\n    save_mel(mel_output_path, mel_prediction, target_spectrogram)\n\n    # Save loss plot to disk\n    Path(plot_dir).mkdir(exist_ok=True)\n    loss_path = str(plot_dir.joinpath(\"step_{:09d}_sample_{}_loss.png\".format(step, sample_num)))\n    save_loss(loss_path, loss.cpu().data.numpy())\n\n    # Save log to disk\n    Path(plot_dir).mkdir(exist_ok=True)\n    log_path =", "CodeBleu_score": 35.998169434651984}, {"code": "def eval_model(attention, mel_prediction, target_spectrogram, input_seq, step,\n               plot_dir, mel_output_dir, wav_dir, sample_num, loss, hparams)\n\t\n    # Save some logs and plots\n    attention_path = str(plot_dir.joinpath(\"step_{:09d}_sample_{:05d}_attention.png\".format(step, sample_num)))\n    attention_plot = plot_spectrogram(attention[0].cpu().data.numpy(),\n                                      title=\"Attention\",\n                                      save_path=attention_path,\n                                      info=\"Step {}: {}\".format(step, sequence_to_text(input_seq)),\n                                      max_len=target_spectrogram.size(1))\n    mel_path = str(mel_output_dir.joinpath(\"step_{:09d}_sample_{:05d}_mel.png\".format(step, sample_num)))\n    mel_plot = plot_spectrogram(mel_prediction[0].data.cpu().numpy(),\n                                title=\"Predicted mel spectrogram\",\n                                save_path=mel_path,\n                                info=\"Step {}: {}\".format(step, sequence_to_text(input_seq)),\n                                max_len=target_spectrogram.size(1))\n    mel_target_path = str(mel_output_dir.joinpath(\"step_{:09d}_sample_{:05d}_mel_target.png\".format(step, sample_num)))\n    mel_target_plot = plot_spectrogram(target_spectrogram.cpu().data.numpy(),\n                                       title=\"Target mel spectrogram\",\n                                       save_path=mel_target_path,\n                                       info=\"Step {}: {}\".format(step, sequence_to_text(input_seq)),\n                                       max_len=target_spectrogram.size(1))\n    mel_diff_path = str(mel_output_dir.joinpath(\"step_{:09d}_sample_{:05d}_mel_diff.png\".format(step, sample_num)))\n    mel_diff_plot = plot_spectrogram(mel_prediction[0].data.cpu().numpy() - target_spectrogram.cpu().data.numpy(),\n                                     title=\"Predicted mel spectrogram diff", "CodeBleu_score": 43.09970981747038}]}
{"instance_id": "6bad4f500286", "generation_result": [{"code": "def num_params(self)\n\t\n    return sum(p.numel() for p in self.parameters() if p.requires_grad) / 1e6\n\n", "CodeBleu_score": 5.637300874658717}, {"code": "def num_params(self)\n\t\n    num_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n    return num_params / 1e6\n\n", "CodeBleu_score": 9.363037280413323}, {"code": "def num_params(self)\n\t\n    return sum(p.numel() for p in self.parameters() if p.requires_grad)/1e6\n\n", "CodeBleu_score": 5.307274095446838}]}
{"instance_id": "78b876054bd4", "generation_result": [{"code": "def pad_tensor(self, x, pad, side='both')\n\t\n    # Determine padding\n    if side == 'before' or side == 'both':\n        before = pad\n        after = 0\n    elif side == 'after':\n        before = 0\n        after = pad\n    else:\n        raise ValueError(\"side must be one of 'before', 'after' or 'both'\")\n\n    # Determine new size\n    if side == 'before' or side == 'both':\n        size = list(x.size())\n        size[1] += before\n    elif side == 'after':\n        size = list(x.size())\n        size[1] += after\n\n    # Create new tensor\n    if x.is_cuda:\n        out = x.new_zeros(size)\n    else:\n        out = x.new_zeros(size).cpu()\n\n    # Copy into new tensor\n    if side == 'before' or side == 'both':\n        out[:, before:] = x\n    elif side == 'after':\n        out[:, :after] = x\n\n    return out\n\n", "CodeBleu_score": 33.053998109289545}, {"code": "def pad_tensor(self, x, pad, side='both')\n\t\n    # Determine padding\n    if side == 'before' or side == 'both':\n        before = pad\n        after = 0\n    elif side == 'after':\n        before = 0\n        after = pad\n    else:\n        raise ValueError('side must be either \"before\", \"after\", or \"both\"')\n    # Determine total size of new tensor\n    if side == 'before' or side == 'both':\n        size = list(x.size())\n        size[1] += pad\n    elif side == 'after':\n        size = list(x.size())\n        size[1] += pad\n    else:\n        raise ValueError('side must be either \"before\", \"after\", or \"both\"')\n    # Create new tensor\n    if x.is_cuda:\n        out = x.new(size).zero_().cuda()\n    else:\n        out = x.new(size).zero_()\n    # Place original tensor into new tensor\n    if side == 'before' or side == 'both':\n        out[:, before:before + x.size(1)] = x\n    elif side == 'after':\n        out[:, :-after] = x\n    else:\n        raise ValueError('side must be either \"before\", \"after\", or \"both\"')\n    return out\n\n", "CodeBleu_score": 33.0831820913021}, {"code": "def pad_tensor(self, x, pad, side='both')\n\t\n    # Determine padding\n    if side == 'before' or side == 'both':\n        before = pad\n        after = 0\n    elif side == 'after':\n        before = 0\n        after = pad\n    else:\n        raise ValueError('side must be either \\'before\\', \\'after\\', or \\'both\\'')\n    # Determine new size\n    if side == 'before' or side == 'both':\n        size = list(x.size())\n        size[1] += before\n    elif side == 'after':\n        size = list(x.size())\n        size[1] += after\n    else:\n        size = list(x.size())\n        size[1] += before + after\n    # Create new tensor\n    if x.is_cuda:\n        out = x.new(size).zero_().cuda()\n    else:\n        out = x.new(size).zero_()\n    # Place original tensor into new tensor\n    if side == 'before' or side == 'both':\n        out[:, before:, :] = x\n    elif side == 'after':\n        out[:, :after, :] = x\n    else:\n        out[:, before:size[1] - after, :] = x\n    return out\n\n", "CodeBleu_score": 37.2424584999953}]}
{"instance_id": "a7ed1715a3df", "generation_result": [{"code": "def save_attention(attn, path)\n\t\n    try:\n        import matplotlib.pyplot as plt\n        import matplotlib.cm as cm\n        import matplotlib.colors as colors\n        import matplotlib.ticker as ticker\n    except ImportError:\n        print(\"This function requires matplotlib. Please install it by running 'pip install matplotlib'\")\n        sys.exit(1)\n\n    fig = plt.figure(figsize=(12, 12))\n    ax = fig.add_subplot(1, 1, 1)\n    cax = ax.matshow(attn, cmap='bone')\n    fig.colorbar(cax)\n\n    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n\n    plt.savefig(path + \".png\")\n    plt.close()\n\n", "CodeBleu_score": 51.262312060613816}, {"code": "def save_attention(attn, path)\n\t\n    import matplotlib.pyplot as plt\n\n    plt.figure(figsize=(10, 10))\n    plt.imshow(attn, interpolation='nearest', cmap='Blues')\n    plt.xlabel('Decoder timestep')\n    plt.ylabel('Encoder timestep')\n    plt.savefig(path)\n    plt.close()\n\n", "CodeBleu_score": 48.14790273676293}, {"code": "def save_attention(attn, path)\n\t\n\n    import matplotlib.pyplot as plt\n\n    plt.imshow(attn)\n    plt.xlabel('Decoder timestep')\n    plt.ylabel('Encoder timestep')\n    plt.savefig(path)\n    plt.close()\n\n", "CodeBleu_score": 32.39547708235106}]}
{"instance_id": "19375c9f44a2", "generation_result": [{"code": "def plot(array)\n\t\n    import matplotlib.pyplot as plt\n    from matplotlib.ticker import FuncFormatter\n\n    fig = plt.figure(figsize=(10, 10))\n    ax = fig.add_subplot(111)\n    ax.set_xlabel('x', color='black', fontsize=20)\n    ax.set_ylabel('y', color='black', fontsize=20)\n    ax.xaxis.set_major_formatter(FuncFormatter(lambda x, pos: '{:.0f}'.format(x)))\n    ax.yaxis.set_major_formatter(FuncFormatter(lambda x, pos: '{:.0f}'.format(x)))\n    ax.tick_params(axis='both', which='major', labelsize=15, colors='black')\n    ax.tick_params(axis='both', which='minor', labelsize=15, colors='black')\n    plt.imshow(array, cmap='gray', origin='lower')\n    plt.show()\n\n", "CodeBleu_score": 55.56304614006602}, {"code": "def plot(array)\n\t\n    import matplotlib.pyplot as plt\n    from matplotlib.ticker import MaxNLocator\n    import numpy as np\n    import matplotlib.cm as cm\n\n    # create a figure with a specified size\n    fig = plt.figure(figsize=(10, 6))\n    # add a subplot\n    ax = fig.add_subplot(111)\n    # customize the axis labels and tick parameters with specific colors and font sizes\n    ax.set_xlabel('x', color='black', fontsize=14)\n    ax.set_ylabel('y', color='black', fontsize=14)\n    ax.xaxis.label.set_color('black')\n    ax.yaxis.label.set_color('black')\n    ax.tick_params(axis='both', colors='black', labelsize=14)\n    ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n    ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n    # plot the array on the customized subplot\n    plt.imshow(array, cmap=cm.Greys_r, vmin=0, vmax=1)\n\n", "CodeBleu_score": 62.448130412568695}, {"code": "def plot(array)\n\t\n    import matplotlib.pyplot as plt\n\n    plt.figure(figsize=(10, 10))\n    plt.subplot()\n    plt.xlabel(\"$x$\", color=\"black\", fontsize=18)\n    plt.ylabel(\"$y$\", color=\"black\", fontsize=18)\n    plt.tick_params(axis=\"both\", colors=\"black\", labelsize=16)\n    plt.plot(array)\n\n", "CodeBleu_score": 23.82316263749951}]}
{"instance_id": "ff94c425920c", "generation_result": [{"code": "def write_to_csv(image_name, prediction, confidence)\n\t\n    # Create a dictionary with the image name, prediction, and confidence\n    data = {\n        \"Image\": image_name,\n        \"Prediction\": prediction,\n        \"Confidence\": confidence,\n    }\n\n    # Open the CSV file in append mode\n    with open(\"prediction_data.csv\", \"a\", newline=\"\") as f:\n        # Initialize the CSV writer\n        writer = csv.writer(f)\n\n        # If the file is new, write the header\n        if os.stat(\"prediction_data.csv\").st_size == 0:\n            writer.writerow(data.keys())\n\n        # Write the data as a new row\n        writer.writerow(data.values())\n\n", "CodeBleu_score": 49.01392779779366}, {"code": "def write_to_csv(image_name, prediction, confidence)\n\t\n    # Create a dictionary with the image name, prediction, and confidence\n    data = {\"Image\": image_name, \"Prediction\": prediction, \"Confidence\": confidence}\n    # Open the CSV file in append mode\n    with open(\"predictions.csv\", \"a\", newline=\"\") as f:\n        # Create the CSV writer\n        writer = csv.DictWriter(f, fieldnames=data.keys())\n        # If the file is new, write the header\n        if os.path.getsize(\"predictions.csv\") == 0:\n            writer.writeheader()\n        # Write the data as a new row\n        writer.writerow(data)\n\n", "CodeBleu_score": 67.35727356002576}, {"code": "def write_to_csv(image_name, prediction, confidence)\n\t\n    data = {\n        \"image_name\": image_name,\n        \"prediction\": prediction,\n        \"confidence\": confidence,\n    }\n    with open(\"output.csv\", \"a\", newline=\"\") as csvfile:\n        fieldnames = [\"image_name\", \"prediction\", \"confidence\"]\n        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n        if os.stat(\"output.csv\").st_size == 0:\n            writer.writeheader()\n        writer.writerow(data)\n\n", "CodeBleu_score": 47.90586740035964}]}
{"instance_id": "436161602009", "generation_result": [{"code": "def export_torchscript(model, im, file, optimize, prefix=colorstr(\"TorchScript:\"))\n\t\n    t = time.time()\n    LOGGER.info(f\"{prefix} starting export with {im.shape if im else 'random'} image input...\")\n    try:\n        ts = torch.jit.trace(model, im)  # input shape [1, 3, 640, 640]\n        ts = optimize_for_mobile(ts) if optimize else ts\n        ts.save(file)\n        torch.jit.save(ts, file)\n        LOGGER.info(f\"{prefix} export success, saved as {file}\")\n    except Exception as e:\n        LOGGER.info(f\"{prefix} export failure: {e}\")\n    return file, None\n\n", "CodeBleu_score": 34.468132580381535}, {"code": "def export_torchscript(model, im, file, optimize, prefix=colorstr(\"TorchScript:\"))\n\t\n    LOGGER.info(f\"{prefix} starting export with {'' if optimize else 'un'}optimized model...\")\n    f = file.with_suffix(\".torchscript.pt\")\n    ts = torch.jit.trace(model, im)\n    if optimize:\n        optimize_for_mobile(ts)\n        f = file.with_suffix(\".torchscript.optimized.pt\")\n        LOGGER.info(f\"{prefix} exported to {f} ({file_size(f):.1f} MB)\")\n    else:\n        ts.save(f)\n        LOGGER.info(f\"{prefix} exported to {f} ({file_size(f):.1f} MB)\")\n    return f, None\n\n", "CodeBleu_score": 35.691539949604646}, {"code": "def export_torchscript(model, im, file, optimize, prefix=colorstr(\"TorchScript:\"))\n\t\n    LOGGER.info(f\"{prefix} starting export with input shape {im.shape}.\")\n    f = file.with_suffix(\".torchscript.pt\")\n    optimize_for_mobile(model, im) if optimize else torch.jit.trace(model, im, strict=False)\n    model.save(f)\n    files = [f, *model.files]\n    weights = getattr(model, \"weights\", model.pt)\n    if weights is not None:\n        files.extend(Path(weights).parent.glob(\"*.*\"))\n    LOGGER.info(f\"{prefix} export success, saved as {f}\")\n    return f, files\n\n", "CodeBleu_score": 39.22230022497799}]}
{"instance_id": "78335487ac1c", "generation_result": [{"code": "def export_coreml(model, im, file, int8, half, nms, prefix=colorstr(\"CoreML:\")):\n\t\n    try:\n        import coremltools as ct\n    except ImportError:\n        LOGGER.error(f\"{prefix} coremltools is not installed, run {colorstr('pip install coremltools')}.\")\n        return\n    if nms:\n        from coremltools.models import datatypes, neural_network as nn\n\n        class iOSModel(nn.NeuralNetwork):\n            def __init__(self, *args, **kwargs):\n                super(iOSModel, self).__init__(*args, **kwargs)\n\n            def nms(self, boxes, conf, iou_threshold):\n                boxes, conf, labels = boxes.value, conf.value, labels.value\n                boxes, conf, labels = list(boxes), list(conf), list(labels)\n                return nms(boxes, conf, labels, iou_threshold)\n\n        inputs = [\n            ct.TensorType(name=\"image\", shape=im.shape),\n            ct.TensorType(name=\"confidence\", shape=conf.shape),\n            ct.TensorType(name=\"labels\", shape=labels.shape),\n        ]\n        outputs = [\n            ct.TensorType(name=\"boxes\", shape=boxes.shape),\n            ct.TensorType(name=\"confidence\", shape=conf.shape),\n            ct.TensorType(name=\"labels\", shape=labels.shape),\n        ]\n        spec = ct.utils.convert_neural_network_spec(iOSModel(inputs, outputs))\n        builder = ct.models.model.ModelBuilder(spec, use_float_arraytype=True)\n        model = builder.spec\n\n    LOGGER.info(f\"{prefix} Starting export with inputs: {im.shape}, {im.dtype}, {im.max()}\")\n    model.eval()\n    if isinstance(model, Detect):\n        model.model[-1].export = True\n    elif isinstance(model, ClassificationModel):\n        model.model.export = True\n    else:\n        model.export = True\n    if int8:\n        model.int8()\n    if half", "CodeBleu_score": 39.02624176633458}, {"code": "def export_coreml(model, im, file, int8, half, nms, prefix=colorstr(\"CoreML:\")):\n\t\n    try:\n        import coremltools as ct\n    except ImportError:\n        LOGGER.warning(\n            f\"{prefix}coremltools not installed, run 'pip install coremltools' to enable CoreML export.\"\n        )\n        return\n    t = time.time()\n    LOGGER.info(f\"{prefix}Starting CoreML export with inputs: {im.shape} and half: {half}.\")\n    if int8:\n        LOGGER.info(f\"{prefix}Applying quantization to CoreML model.\")\n    if nms:\n        LOGGER.info(f\"{prefix}Applying NMS to CoreML model.\")\n    if half:\n        im = im.half()\n    model.eval()\n    if nms:\n        model = iOSModel(model)\n    model = model.to(im.device)\n    model(im.unsqueeze(0))\n    if int8:\n        model = ct.convert(\n            model,\n            inputs=[ct.ImageType(name=\"image\", shape=im.shape)],\n            minimum_ios_deployment_target=\"13\",\n        )\n    else:\n        model = ct.convert(\n            model,\n            inputs=[ct.TensorType(name=\"image\", shape=im.shape)],\n            minimum_ios_deployment_target=\"13\",\n        )\n    if int8:\n        model = ct.convert(\n            model,\n            inputs=[ct.ImageType(name=\"image\", shape=im.shape)],\n            minimum_ios_deployment_target=\"13\",\n        )\n    else:\n        model = ct.convert(\n            model,\n            inputs=[ct.TensorType(name=\"image\", shape=im.shape)],\n            minimum_ios_deployment_target=\"13\",\n        )\n    if int8:\n        model = ct.convert(\n            model,\n            inputs=[ct.ImageType(name=\"image\", shape=im.shape)],\n            minimum_ios_deployment_target=\"13\",\n        )\n    else:\n        model = ct.convert(\n            model,\n           ", "CodeBleu_score": 34.270715653706304}, {"code": "def export_coreml(model, im, file, int8, half, nms, prefix=colorstr(\"CoreML:\")):\n\t\n    # Checks\n    check_requirements((\"coremltools\",))\n    check_version(coremltools=True)\n    if nms and not isinstance(model, (DetectionModel, DetectionModel)):\n        LOGGER.info(f\"{prefix}NMS disabled for non-YOLOv5 models\")\n        nms = False\n    if not isinstance(model, (DetectionModel, DetectionModel)):\n        LOGGER.info(f\"{prefix}NMS not supported for non-YOLOv5 models\")\n        nms = False\n    if half:\n        LOGGER.info(f\"{prefix}FP16 export not supported for non-YOLOv5 models\")\n        half = False\n    if int8:\n        LOGGER.info(f\"{prefix}INT8 export not supported for non-YOLOv5 models\")\n        int8 = False\n    # Export\n    LOGGER.info(f\"{prefix}Starting export with inputs {im}\")\n    file = file.replace(\".mlmodel\", \"\")  # remove suffix\n    if nms:\n        model = model.module.model[-1].iOSModel()  # wrap model\n    model.eval()\n    if isinstance(model, ClassificationModel):\n        model.model[-1].export = True  # set export flag\n    else:\n        model.model[-1].export = True  # set export flag\n    model = model.to(device=\"cpu\")\n    model.model[-1].half = half  # set FP16\n    model.model[-1].quant = int8  # set quantization\n    model.model[-1].dynamic = True  # set dynamic\n    model.model[-1].profile = False  # set profile\n    if half:\n        im = im.half()  # set FP16\n    elif int8:\n        im = im.to(torch.int8)  # set INT8\n    im = im.to(device=\"cpu\")\n    im = im[None].cpu()\n    if isinstance(model, ClassificationModel):\n        im = im[None]  # class models expect lists\n    script_model = torch", "CodeBleu_score": 33.55006328379732}]}
{"instance_id": "80233cf71580", "generation_result": [{"code": "def generate_individual(input_ranges, individual_length)\n\t\n    individual = []\n    for i in range(individual_length):\n        individual.append(random.uniform(input_ranges[i][0], input_ranges[i][1]))\n    return individual\n\n", "CodeBleu_score": 47.630972577458174}, {"code": "def generate_individual(input_ranges, individual_length)\n\t\n    individual = []\n    for i in range(individual_length):\n        individual.append(random.uniform(input_ranges[0], input_ranges[1]))\n    return individual\n\n", "CodeBleu_score": 45.11328150959641}, {"code": "def generate_individual(input_ranges, individual_length)\n\t\n    individual = []\n    for i in range(individual_length):\n        individual.append(random.uniform(input_ranges[i][0], input_ranges[i][1]))\n    return individual\n\n", "CodeBleu_score": 47.630972577458174}]}
{"instance_id": "5ea979cb4a8e", "generation_result": [{"code": "def marginal_log_mean_coeff(self, t)\n\t\n    if self.schedule == 'discrete':\n        if t > self.T:\n            raise ValueError(f\"t must be less than T. t: {t}, T: {self.T}\")\n        if t == 0:\n            return 0\n        if t == self.T:\n            return self.log_alpha_T\n        if t > self.T:\n            return self.log_alpha_T\n        if t < 0:\n            return 0\n        if t == 1:\n            return self.log_alpha_T\n        if t < 1:\n            return self.log_alpha_T * t\n        if t > self.T - 1:\n            return self.log_alpha_T * (self.T - t)\n        if t > 1:\n            return self.log_alpha_T * (1 - t)\n    elif self.schedule == 'linear':\n        if t > self.T:\n            raise ValueError(f\"t must be less than T. t: {t}, T: {self.T}\")\n        if t == 0:\n            return 0\n        if t == self.T:\n            return self.log_alpha_T\n        if t > self.T:\n            return self.log_alpha_T\n        if t < 0:\n            return 0\n        if t == 1:\n            return self.log_alpha_T * (t - 1)\n        if t > 1:\n            return self.log_alpha_T * (1 - t)\n    elif self.schedule == 'cosine':\n        if t > self.T:\n            raise ValueError(f\"t must be less than T. t: {t}, T: {self.T}\")\n        if t == 0:\n            return 0\n        if t == self.T:\n            return self.log_alpha_T\n        if t > self.T:\n            return self.log_alpha_T\n        if t < 0:\n            return 0\n        if t == 1:\n            return self.log_alpha_T * (1 - math.cos(math.pi / self.T))\n       ", "CodeBleu_score": 26.15159481417787}, {"code": "def marginal_log_mean_coeff(self, t)\n\t\n    if self.schedule == 'discrete':\n        alpha_t = self.alpha_0 * (1 - self.gamma) ** (t - 1)\n    elif self.schedule == 'linear':\n        alpha_t = self.alpha_0 * (1 - self.gamma) ** (t - 1) + self.alpha_0 * (1 - self.gamma) ** (t - 2) * (\n                1 - self.gamma) / 2\n    elif self.schedule == 'cosine':\n        alpha_t = self.alpha_0 * (1 + self.gamma * (math.cos(math.pi * (t - 1) / self.T) + 1) / 2)\n    else:\n        raise ValueError('Unknown schedule type: ' + self.schedule)\n    return math.log(alpha_t)\n\n", "CodeBleu_score": 41.008992179034855}, {"code": "def marginal_log_mean_coeff(self, t)\n\t\n    if self.schedule_type == \"discrete\":\n        if t >= self.num_classes:\n            return 0\n        else:\n            return math.log(1 / self.num_classes)\n    elif self.schedule_type == \"linear\":\n        if t >= self.num_classes - 1:\n            return 0\n        else:\n            return math.log(1 / self.num_classes) + math.log(\n                (t + 1) / self.num_classes - ((t + 1) ** 2 - t ** 2) / (2 * self.num_classes ** 2)\n            )\n    elif self.schedule_type == \"cosine\":\n        if t >= self.num_classes - 1:\n            return 0\n        else:\n            return math.log(1 / self.num_classes) + math.log(\n                (1 + math.cos(math.pi * t / self.num_classes)) / 2\n            )\n    else:\n        raise ValueError(\"Unknown schedule type.\")\n\n", "CodeBleu_score": 27.134062064657066}]}
{"instance_id": "7da4fae5a278", "generation_result": [{"code": "def inverse_lambda(self, lamb)\n\t\n    if self.schedule == \"linear\":\n        t = (\n            torch.arccos(\n                (\n                    (\n                        (1 + self.beta_1)\n                        / (1 - self.beta_1)\n                        * torch.cos(math.pi * lamb)\n                    )\n                    + self.beta_1\n                )\n                / (1 + self.beta_1)\n            )\n            / math.pi\n        )\n    elif self.schedule == \"discrete\":\n        t = (\n            torch.arccos(\n                (\n                    (\n                        (1 + self.beta_1)\n                        / (1 - self.beta_1)\n                        * torch.cos(math.pi * lamb)\n                    )\n                    + self.beta_1\n                )\n                / (1 + self.beta_1)\n            )\n            / math.pi\n        )\n    else:\n        log_alpha = self.log_alpha.to(lamb.device)\n        t = torch.acos(\n            (\n                (\n                    (\n                        (1 + self.beta_1)\n                        / (1 - self.beta_1)\n                        * torch.cos(math.pi * lamb)\n                    )\n                    + self.beta_1\n                )\n                / (1 + self.beta_1)\n            )\n            / (1 + self.beta_1)\n        )\n        t = torch.acos(\n            (\n                (\n                    (\n                        (1 + self.beta_1)\n                        / (1 - self.beta_1)\n                        * torch.cos(math.pi * lamb)\n                    )\n                    + self.beta_1\n                )\n                / (1 + self.beta_1)\n            )\n            / (1 + self.beta_1)\n        )\n        t = torch.arange(\n            self.T, dtype=lamb.dtype, device=lamb.device\n        ).unsqueeze(1) * (1 / self.T)", "CodeBleu_score": 23.261568895849777}, {"code": "def inverse_lambda(self, lamb)\n\t\n    if self.beta is not None:\n        if self.beta[0] == 0:\n            t = (lamb + 1) / (2 * self.beta[1])\n        else:\n            t = (lamb + 1) / (2 * self.beta[0])\n    elif self.log_alpha is not None:\n        t = (\n            torch.arccos(\n                (\n                    lamb\n                    + 1\n                    - (\n                        self.log_alpha[0]\n                        + self.log_alpha[1]\n                        - 2 * torch.log(torch.tensor(math.e))\n                    )\n                )\n                / (self.log_alpha[1] - self.log_alpha[0])\n            )\n            / math.pi\n        )\n        t = (t + 1) / 2\n    else:\n        raise ValueError(\n            \"The label schedule is not defined. Please define it using the beta or log_alpha parameters.\"\n        )\n    return t\n\n", "CodeBleu_score": 25.028012823818397}, {"code": "def inverse_lambda(self, lamb)\n\t\n    if self.schedule == 'linear':\n        beta = 0.5 * (1 + math.sqrt(1 + 4 * lamb))\n        t = (beta - 1) / (beta + 1)\n    elif self.schedule == 'discrete':\n        if lamb <= 0:\n            t = 0\n        elif lamb >= self.log_alpha[-1]:\n            t = 1\n        else:\n            t = (self.log_alpha.index(lamb) + 1) / len(self.log_alpha)\n    else:\n        alpha = math.acos(1 - 2 * lamb)\n        t = alpha / math.pi\n    return t\n\n", "CodeBleu_score": 23.139942447486312}]}
{"instance_id": "a519c853a2b2", "generation_result": [{"code": "def noise_pred_fn(x, t_continuous, cond=None)\n\t\n    t_continuous = t_continuous.unsqueeze(-1).unsqueeze(-1)\n    x = x.unsqueeze(0)\n    if cond is not None:\n        cond = cond.unsqueeze(0)\n        x = torch.cat([x, cond], dim=1)\n    x = model.model.denoise_fn(x, t_continuous, cond=cond)\n    if model.model.model_type == \"noise\":\n        x = x.squeeze()\n        x = x.clamp(-1.0, 1.0)\n        return x\n    elif model.model.model_type == \"scale\":\n        x = x.squeeze()\n        x = x.clamp(0.0, 1.0)\n        return x\n    elif model.model.model_type == \"scale_shift\":\n        x = x.squeeze()\n        x = x.clamp(0.0, 1.0)\n        return x\n    elif model.model.model_type == \"scale_shift_reverse\":\n        x = x.squeeze()\n        x = x.clamp(0.0, 1.0)\n        return x\n    elif model.model.model_type == \"scale_shift_reverse_sigmoid\":\n        x = x.squeeze()\n        x = x.clamp(0.0, 1.0)\n        return x\n    elif model.model.model_type == \"scale_shift_reverse_sigmoid_scale\":\n        x = x.squeeze()\n        x = x.clamp(0.0, 1.0)\n        return x\n    elif model.model.model_type == \"scale_shift_reverse_sigmoid_scale_shift\":\n        x = x.squeeze()\n        x = x.clamp(0.0, 1.0)\n        return x\n    elif model.model.model_type == \"scale_shift_reverse_sigmoid_scale_shift_sigmoid\":\n        x = x.squeeze()\n        x", "CodeBleu_score": 29.12275744131621}, {"code": "def noise_pred_fn(x, t_continuous, cond=None)\n\t\n    if t_continuous is None:\n        t_continuous = torch.ones(x.shape[0], device=x.device)\n    t_continuous = t_continuous.to(x.device)\n    t_continuous = t_continuous.view(\n        t_continuous.shape[0], *([1] * (len(x.shape) - 1)))\n    if cond is None:\n        cond = torch.ones(x.shape[0], device=x.device)\n    cond = cond.to(x.device)\n    cond = cond.view(cond.shape[0], *([1] * (len(x.shape) - 1)))\n    if x.shape[1] != 3:\n        x = x.repeat(1, 3, 1, 1)\n    if cond.shape[1] != 3:\n        cond = cond.repeat(1, 3, 1, 1)\n    with torch.no_grad():\n        if noise_schedule == \"linear\":\n            noise = noise_amount * t_continuous\n            noise = noise.clamp(min=0., max=noise_amount)\n            x_noised = x + noise\n            cond_noised = cond + noise\n        elif noise_schedule == \"quadratic\":\n            t_continuous = t_continuous.clamp(min=0., max=1.)\n            t_continuous = t_continuous ** 2\n            noise = noise_amount * t_continuous\n            noise = noise.clamp(min=0., max=noise_amount)\n            x_noised = x + noise\n            cond_noised = cond + noise\n        elif noise_schedule == \"exp\":\n            t_continuous = t_continuous.clamp(min=0., max=1.)\n            t_continuous = t_continuous ** 2\n            noise = noise_amount * t_continuous\n            noise = noise.clamp(min=0., max=noise_amount)\n            x_noised = x + noise\n            cond_noised = cond + noise\n        elif noise_schedule == \"sin", "CodeBleu_score": 28.148492187972394}, {"code": "def noise_pred_fn(x, t_continuous, cond=None)\n\t\n    if cond is not None:\n        x = torch.cat((x, cond), dim=1)\n    if len(t_continuous.shape) == 1:\n        t_continuous = t_continuous.unsqueeze(1)\n    t_continuous = t_continuous.to(x.device)\n    noise_pred = model.model.denoise_fn(x, t_continuous)\n    if model.model.model_type == \"unet\":\n        noise_pred = noise_pred.squeeze()\n    if model.model.model_type == \"glow\":\n        noise_pred = noise_pred.squeeze()\n        noise_pred = noise_pred.permute(0, 2, 1)\n        noise_pred = noise_pred.reshape(noise_pred.shape[0], -1, noise_pred.shape[-1])\n    return noise_pred\n\n", "CodeBleu_score": 28.788036005618224}]}
{"instance_id": "96a9b7d54823", "generation_result": [{"code": "def dynamic_thresholding_fn(self, x0, t=None)\n\t\n    # reshape x0 to 1D\n    x0 = x0.view(x0.numel())\n    # compute quantile\n    quantile = self.quantile(x0.abs())\n    # determine scaling factor\n    scale = quantile.clamp(min=1e-12).reciprocal()\n    # clamp x0 values based on this factor\n    x0 = x0.clamp_min(scale)\n    # reshape x0 to its original shape\n    x0 = x0.view(self.shape)\n    # scale x0\n    x0 = x0 * scale\n    # return x0\n    return x0\n\n", "CodeBleu_score": 12.751626171482325}, {"code": "def dynamic_thresholding_fn(self, x0, t=None)\n\t\n    if t is None:\n        t = self.t\n    if self.training:\n        self.threshold = self.quantile_fn(x0.abs().view(x0.shape[0], -1), t)\n    else:\n        self.threshold = self.threshold.detach()\n    x0 = torch.clamp(x0, -self.threshold, self.threshold)\n    return self.scale_fn(x0)\n\n", "CodeBleu_score": 35.82166248501566}, {"code": "def dynamic_thresholding_fn(self, x0, t=None)\n\t\n    # reshape the input tensor to compute the quantile of absolute values\n    x0 = x0.reshape(x0.shape[0], -1)\n    # determine the scaling factor\n    if t is None:\n        t = 1 - 1 / (2 * math.sqrt(x0.shape[1]))\n    # compute the quantile of absolute values\n    quantile = torch.quantile(torch.abs(x0), t, dim=1, keepdim=True)\n    # clamp the input tensor based on the quantile\n    x0 = torch.clamp(x0, min=-quantile, max=quantile)\n    # scale the tensor and return it\n    return self.scale * x0\n\n", "CodeBleu_score": 35.886960788393594}]}
{"instance_id": "59c6ec6d7a6a", "generation_result": [{"code": "def data_prediction_fn(self, x, t)\n\t\n    # apply dynamic thresholding\n    threshold = self.dynamic_thresholding_ratio * (self.max_threshold - self.min_threshold) + self.min_threshold\n    x0 = torch.clamp(x, min=self.min_threshold, max=threshold)\n    x0 = self.normalization_fn(x0)\n    return x0\n\n", "CodeBleu_score": 10.982404949782014}, {"code": "def data_prediction_fn(self, x, t)\n\t\n    threshold = self.dynamic_threshold_ratio * (self.max_threshold - self.min_threshold) + self.min_threshold\n    x_clamped = torch.clamp(x, min=threshold)\n    x_normalized = x_clamped / x_clamped.max()\n    return x_normalized\n\n", "CodeBleu_score": 9.726406156470075}, {"code": "def data_prediction_fn(self, x, t)\n\t\n    # apply dynamic thresholding\n    x0 = x.clone()\n    x0 = x0.view(x0.size(0), -1)\n    x0 = x0.transpose(0, 1)\n    x0 = x0.contiguous().view(-1)\n    x0 = x0.view(x0.size(0), 1)\n    x0 = x0.transpose(0, 1)\n    x0 = x0.contiguous().view(x0.size(0), -1)\n\n    x0 = torch.clamp(x0, 0, self.dynamic_thresholding_ratio * self.max_threshold)\n    x0 = x0.view(x0.size(0), -1)\n    x0 = x0.transpose(0, 1)\n    x0 = x0.contiguous().view(-1)\n    x0 = x0.view(x0.size(0), 1)\n    x0 = x0.transpose(0, 1)\n    x0 = x0.contiguous().view(x0.size(0), -1)\n\n    # normalize the clamped values\n    x0 = x0 / self.dynamic_thresholding_ratio\n\n    return x0\n\n", "CodeBleu_score": 16.62988278316112}]}
{"instance_id": "65e3618468a2", "generation_result": [{"code": "def predict_eps_sigma(model, input, sigma_in, **kwargs)\n\t\n    # Reshape sigma to match the dimensions of the input\n    sigma_in = sigma_in.reshape(input.shape[0], 1, 1, 1)\n    # Scale the input\n    input = input * sigma_in\n    # Compute the prediction\n    output = model(input, **kwargs)\n    # Adjust the prediction with the scaled input and sigma\n    output = output * sigma_in\n    return output\n\n", "CodeBleu_score": 27.622119735468704}, {"code": "def predict_eps_sigma(model, input, sigma_in, **kwargs)\n\t\n\n    # Reshape sigma to match the dimensions of the input\n    sigma_in = sigma_in.reshape(input.shape[0], -1)\n\n    # Scale the input and sigma\n    input = input.reshape(input.shape[0], -1)\n    input = input / sigma_in.repeat(input.shape[1], 1).T\n    sigma_in = sigma_in.repeat(input.shape[1], 1).T\n\n    # Compute the prediction\n    prediction = model(input)\n    prediction = prediction * sigma_in\n\n    return prediction\n\n", "CodeBleu_score": 20.079655928346636}, {"code": "def predict_eps_sigma(model, input, sigma_in, **kwargs)\n\t\n    # Reshape sigma to match the dimensions of the input\n    sigma_in = sigma_in.reshape(input.shape[0], 1, 1, 1)\n\n    # Scale the input\n    input = input * sigma_in\n\n    # Compute the prediction\n    prediction = model(input, **kwargs)\n\n    # Adjust the prediction with the scaled input and sigma\n    prediction = prediction * sigma_in\n\n    return prediction\n\n", "CodeBleu_score": 27.361014670927585}]}
{"instance_id": "b35846dfbc66", "generation_result": [{"code": "def cal_intergrand(beta_0, beta_1, taus)\n\t\n    # set the required gradients for taus\n    taus_grad = torch.ones(taus.shape)\n    taus.requires_grad = True\n    taus.grad = taus_grad\n    # compute alpha and its logarithm\n    alpha = torch.exp(taus)\n    log_alpha = taus\n    # backpropagate to obtain the gradient of log_alpha with respect to taus\n    grad_log_alpha = torch.autograd.grad(log_alpha, taus, grad_outputs=taus_grad)[0]\n    # compute the integrand\n    integrand = alpha * (beta_0 + beta_1 * alpha)\n    # return the integrand and its gradient with respect to taus\n    return integrand, grad_log_alpha\n\n", "CodeBleu_score": 26.773568245386585}, {"code": "def cal_intergrand(beta_0, beta_1, taus)\n\t\n    taus = taus.clone().requires_grad_(True)\n    alpha = torch.exp(beta_0 + beta_1 * taus)\n    log_alpha = torch.log(alpha)\n    log_alpha.backward(torch.ones_like(log_alpha))\n    grad_log_alpha = taus.grad\n    integrand = alpha * grad_log_alpha\n    return integrand\n\n", "CodeBleu_score": 19.512515098642922}, {"code": "def cal_intergrand(beta_0, beta_1, taus)\n\t\n    taus_clone = taus.clone()\n    taus_clone.requires_grad = True\n    alpha = torch.exp(beta_0 + beta_1 * taus_clone)\n    log_alpha = torch.log(alpha)\n    log_alpha.backward()\n    return log_alpha * alpha\n\n", "CodeBleu_score": 21.32243385015054}]}
{"instance_id": "5f3667566a10", "generation_result": [{"code": "def sample_euler_ancestral(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None)\n\t\n    if extra_args is None:\n        extra_args = {}\n    if noise_sampler is None:\n        noise_sampler = torch.distributions.Normal(0, s_noise)\n    if disable is None:\n        disable = []\n    with torch.no_grad():\n        for k in disable:\n            extra_args[k] = False\n        x_noise = noise_sampler.sample(x.shape)\n        x = x + x_noise\n        for sigma in sigmas:\n            x_denoised = model(x, sigma, **extra_args)\n            x_ancestral = x_denoised - x\n            x = x + eta * x_ancestral\n            if callback is not None:\n                callback(x)\n        if s_noise > 0:\n            x_noise = noise_sampler.sample(x.shape)\n            x = x + x_noise\n    return x\n\n", "CodeBleu_score": 34.16553490715592}, {"code": "def sample_euler_ancestral(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None)\n\t\n    if extra_args is None:\n        extra_args = {}\n    if noise_sampler is None:\n        noise_sampler = torch.distributions.Normal(0, s_noise)\n    if disable is None:\n        disable = []\n    x = x.detach().clone().requires_grad_(True)\n    for i in range(len(sigmas)):\n        x = x.detach().clone().requires_grad_(True)\n        x = denoise(model, x, sigmas[i], extra_args=extra_args, disable=disable)\n        x = ancestral_step(model, x, sigmas[i], extra_args=extra_args, disable=disable)\n        if callback is not None:\n            callback(x, i)\n        if i < len(sigmas) - 1:\n            x = x + eta * noise_sampler.sample(x.shape)\n    return x\n\n", "CodeBleu_score": 33.10217386404754}, {"code": "def sample_euler_ancestral(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None)\n\t\n    if extra_args is None:\n        extra_args = {}\n    if noise_sampler is None:\n        noise_sampler = torch.distributions.normal.Normal(0, s_noise)\n    x_ancestral = x.clone()\n    for sigma in sigmas:\n        x_ancestral = denoise(model, x_ancestral, sigma, extra_args)\n        if callback is not None:\n            callback(x_ancestral, sigma)\n        if disable is not None:\n            x_ancestral[disable] = 0.\n        x_ancestral = eta * x_ancestral + noise_sampler.sample(x_ancestral.shape)\n    return x_ancestral\n\n", "CodeBleu_score": 24.299243266910242}]}
{"instance_id": "6261ded88df2", "generation_result": [{"code": "def linear_multistep_coeff(order, t, i, j)\n\t\n    if i == j:\n        return 1\n    elif i == j - 1:\n        return t[i]\n    elif i == j - 2:\n        return t[i]**2 / 2\n    elif i == j - 3:\n        return t[i]**3 / 6\n    elif i == j - 4:\n        return t[i]**4 / 24\n    elif i == j - 5:\n        return t[i]**5 / 120\n    elif i == j - 6:\n        return t[i]**6 / 720\n    elif i == j - 7:\n        return t[i]**7 / 5040\n    elif i == j - 8:\n        return t[i]**8 / 40320\n    elif i == j - 9:\n        return t[i]**9 / 362880\n    elif i == j - 10:\n        return t[i]**10 / 3628800\n    else:\n        raise ValueError(\n            f\"Order {order} is too high for the step index {j - i - 1}.\"\n        )\n\n    def prod_term(k):\n        if k == i:\n            return 1\n        elif k == i + 1:\n            return t[k]\n        elif k == i + 2:\n            return t[k]**2 / 2\n        elif k == i + 3:\n            return t[k]**3 / 6\n        elif k == i + 4:\n            return t[k]**4 / 24\n        elif k == i + 5:\n            return t[k]**5 / 120\n        elif k == i + 6:\n            return t[k]**6 / 720\n        elif k == i + 7:\n            return t[k]**7 / 5040\n        elif k == i + 8:\n            return t[k]**8 / 40320\n        elif k == i + 9:\n            return t[k]**", "CodeBleu_score": 20.272519185517385}, {"code": "def linear_multistep_coeff(order, t, i, j)\n\t\n    if order > i:\n        raise ValueError(f\"The order {order} is higher than the step index {i}.\")\n    def product(t):\n        for k in range(order):\n            if k == j:\n                continue\n            else:\n                yield (t - t[k]) / (t[i] - t[k])\n    return integrate.quad(lambda t: math.prod(product(t)), t[i], t[i + 1])[0]\n\n", "CodeBleu_score": 40.64476147933732}, {"code": "def linear_multistep_coeff(order, t, i, j)\n\t\n    if order > i:\n        raise ValueError('The order is too high for the step index.')\n    def product_term(t):\n        product = 1\n        for k in range(order):\n            if k != j:\n                product *= (t - (i + 1 - k)) / (i + 1 - k)\n        return product\n    return integrate.quad(product_term, t[i], t[i + 1])[0]\n\n", "CodeBleu_score": 35.68457947239732}]}
{"instance_id": "4ee025e33ab1", "generation_result": [{"code": "def sample_lcm(model, x, sigmas, extra_args=None, callback=None, disable=None, noise_sampler=None)\n\t\n\n    # Initialize extra arguments and noise sampler if not provided\n    if extra_args is None:\n        extra_args = {}\n    if noise_sampler is None:\n        noise_sampler = torch.distributions.normal.Normal(0, 1)\n\n    # Iterate over the sigmas\n    for sigma in sigmas:\n\n        # If the next sigma is greater than zero, noise scaling is applied to the updated input\n        if sigma > 0:\n            noise = noise_sampler.sample(x.shape).to(x.device)\n            x = x + sigma * noise\n\n        # Apply the model to the input and update it with denoised values\n        x = model(x, extra_args=extra_args)\n        x = torchsde.sdeint(model, x, t=torch.tensor([0, 1]), sigma=sigma, extra_args=extra_args, noise_sampler=noise_sampler)\n\n        # Execute callback function to monitor progress\n        if callback is not None:\n            callback(x)\n\n    # Return the final sampled value\n    return x\n\n", "CodeBleu_score": 32.52642665390895}, {"code": "def sample_lcm(model, x, sigmas, extra_args=None, callback=None, disable=None, noise_sampler=None)\n\t\n    if extra_args is None:\n        extra_args = {}\n    if noise_sampler is None:\n        noise_sampler = torch.distributions.normal.Normal(0, 1)\n    if disable is None:\n        disable = []\n    if callback is None:\n        callback = lambda *args: None\n    extra_args = {**extra_args, **{\n        \"noise_sampler\": noise_sampler,\n        \"disable\": disable\n    }}\n    for sigma in sigmas:\n        callback(sigma, x)\n        x = model(x, sigma, **extra_args)\n        if sigma > 0:\n            x = x * math.sqrt(sigma)\n    return x\n\n", "CodeBleu_score": 33.95333970998025}, {"code": "def sample_lcm(model, x, sigmas, extra_args=None, callback=None, disable=None, noise_sampler=None)\n\t\n    if extra_args is None:\n        extra_args = {}\n    if noise_sampler is None:\n        noise_sampler = torchsde.SDEIto(model.sde.noise_type, model.sde.noise_dim)\n    x = x.clone().detach().requires_grad_(True)\n    for sigma in sigmas:\n        if callback is not None:\n            callback(x, sigma)\n        if sigma > 0:\n            x = model(x, sigma, extra_args=extra_args, noise_sampler=noise_sampler, disable=disable)\n        else:\n            x = model(x, extra_args=extra_args, disable=disable)\n    return x\n\n", "CodeBleu_score": 32.949626578627814}]}
{"instance_id": "cb51f91da606", "generation_result": [{"code": "def append_dims(x, target_dims)\n\t\n    if not isinstance(x, torch.Tensor):\n        raise TypeError(\"Expected a tensor, but got: \" + str(type(x)))\n    if not isinstance(target_dims, int):\n        raise TypeError(\"Expected an integer, but got: \" + str(type(target_dims)))\n    if not isinstance(x.device, torch.device):\n        raise TypeError(\"Expected a torch.device, but got: \" + str(type(x.device)))\n    if x.device.type == \"mps\":\n        if x.dim() < target_dims:\n            raise ValueError(\"The target dimensions are fewer than the current dimensions: \" + str(x.dim()) + \" < \" + str(target_dims))\n        if x.dim() == target_dims:\n            return x.detach().clone()\n        else:\n            return x.detach().clone().unsqueeze(-1).expand(*x.shape, *([1] * (target_dims - x.dim())))\n    else:\n        if x.dim() > target_dims:\n            raise ValueError(\"The target dimensions are fewer than the current dimensions: \" + str(x.dim()) + \" < \" + str(target_dims))\n        if x.dim() == target_dims:\n            return x\n        else:\n            return x.unsqueeze(-1).expand(*x.shape, *([1] * (target_dims - x.dim())))\n\n", "CodeBleu_score": 27.029398472654044}, {"code": "def append_dims(x, target_dims)\n\t\n    if not isinstance(target_dims, int):\n        raise TypeError(f'target_dims must be an int, got {type(target_dims)}.')\n    if not isinstance(x, torch.Tensor):\n        raise TypeError(f'x must be a torch.Tensor, got {type(x)}.')\n    if x.ndim >= target_dims:\n        raise ValueError(f'target_dims must be greater than the current number of dimensions of x, got {target_dims} and {x.ndim}.')\n    num_dims = target_dims - x.ndim\n    expanded = x.unsqueeze(*[None]*num_dims)\n    if x.device.type == 'mps':\n        expanded = expanded.detach().clone()\n    return expanded\n\n", "CodeBleu_score": 40.14142002799376}, {"code": "def append_dims(x, target_dims)\n\t\n    assert len(target_dims) >= len(x.shape), \"Target dimensions must be greater than or equal to the current dimensions.\"\n    num_dims = len(target_dims) - len(x.shape)\n    for _ in range(num_dims):\n        x = x.unsqueeze(-1)\n    if x.device.type == 'mps':\n        x = x.detach().clone()\n    return x\n\n", "CodeBleu_score": 37.136457500122745}]}
{"instance_id": "9fac874a3bc7", "generation_result": [{"code": "def download_file(path, url, digest=None)\n\t\n    if path.exists():\n        if digest and not check_sha256(path, digest):\n            raise RuntimeError(f\"The digest of the file at {path} does not match the expected digest\")\n        return path\n    path.parent.mkdir(parents=True, exist_ok=True)\n    with urllib.request.urlopen(url) as response, open(path, \"wb\") as out_file:\n        shutil.copyfileobj(response, out_file)\n    if digest and not check_sha256(path, digest):\n        raise RuntimeError(f\"The digest of the file at {path} does not match the expected digest\")\n    return path\n\n", "CodeBleu_score": 44.82080090578133}, {"code": "def download_file(path, url, digest=None)\n\t\n    if path.exists():\n        if digest is not None:\n            if not check_sha256(path, digest):\n                raise RuntimeError(f\"The file at {path} does not have the expected SHA-256 hash of {digest}.\")\n        return path\n    path.parent.mkdir(parents=True, exist_ok=True)\n    urllib.request.urlretrieve(url, path)\n    if digest is not None:\n        if not check_sha256(path, digest):\n            raise RuntimeError(f\"The file at {path} does not have the expected SHA-256 hash of {digest}.\")\n    return path\n\n", "CodeBleu_score": 35.784849738195405}, {"code": "def download_file(path, url, digest=None)\n\t\n    if not Path(path).exists():\n        Path(path).parent.mkdir(parents=True, exist_ok=True)\n        with urllib.request.urlopen(url) as source, open(path, \"wb\") as output:\n            shutil.copyfileobj(source, output)\n    if digest is not None:\n        if not check_sha256(path, digest):\n            raise RuntimeError(f\"Downloaded file {path} has an invalid SHA-256 hash\")\n    return path\n\n", "CodeBleu_score": 42.16461816353981}]}
{"instance_id": "f23165e897ce", "generation_result": [{"code": "def train_mode(model, mode=True)\n\t\n    if isinstance(model, nn.Module):\n        model.train(mode)\n    else:\n        for m in model:\n            m.train(mode)\n    return model\n\n", "CodeBleu_score": 18.834169719595888}, {"code": "def train_mode(model, mode=True)\n\t\n    model_modes = []\n    for module in model.modules():\n        model_modes.append(module.training)\n    if mode:\n        model.train()\n    else:\n        model.eval()\n    try:\n        yield\n    finally:\n        for module, mode in zip(model.modules(), model_modes):\n            module.training = mode\n\n", "CodeBleu_score": 32.504996698219315}, {"code": "def train_mode(model, mode=True)\n\t\n    # Save current training mode and set to training mode\n    training_modes = []\n    for module in model.modules():\n        training_modes.append(module.training)\n        module.train(mode)\n    # Yield control to the caller\n    yield\n    # Restore original training mode\n    for module, training_mode in zip(model.modules(), training_modes):\n        module.train(training_mode)\n\n", "CodeBleu_score": 34.571402493731895}]}
{"instance_id": "c6d59c6cb8e1", "generation_result": [{"code": "def ema_update(model, averaged_model, decay)\n\t\n    assert 0 <= decay <= 1, \"EMA decay must be in the range [0, 1]\"\n    averaged_model.load_state_dict(model.state_dict())\n    for name, param in model.named_parameters():\n        averaged_param = averaged_model.state_dict()[name]\n        averaged_param.mul_(1 - decay).add_(param, alpha=decay)\n    for name, buf in model.named_buffers():\n        averaged_model.register_buffer(name, buf)\n\n", "CodeBleu_score": 37.67867981049487}, {"code": "def ema_update(model, averaged_model, decay)\n\t\n    # Exponential moving average of parameters and buffers\n    averaged_model.load_state_dict(\n        {\n            name: decay * averaged_model.state_dict()[name].clone()\n            + (1 - decay) * model.state_dict()[name].clone()\n            for name, _ in model.named_parameters()\n        }\n    )\n    for name, _ in model.named_buffers():\n        averaged_model.register_buffer(name, model.state_dict()[name].clone())\n\n", "CodeBleu_score": 27.973146490258223}, {"code": "def ema_update(model, averaged_model, decay)\n\t\n    averaged_model.load_state_dict(\n        {k: decay * averaged_model.state_dict()[k].clone() + (1 - decay) * model.state_dict()[k].clone()\n         for k in model.state_dict().keys()})\n    for name, buf in model.named_buffers():\n        averaged_model.register_buffer(name, buf)\n\n", "CodeBleu_score": 25.45394186883313}]}
{"instance_id": "600c39a30798", "generation_result": [{"code": "def rand_log_logistic(shape, loc=0., scale=1., min_value=0., max_value=float('inf'), device='cpu', dtype=torch.float32)\n\t\n    min_value = torch.as_tensor(min_value, dtype=dtype, device=device)\n    max_value = torch.as_tensor(max_value, dtype=dtype, device=device)\n    min_value_cdf = torch.log1p(torch.exp(-min_value))\n    max_value_cdf = torch.log1p(torch.exp(-max_value))\n    rand_uniform = torch.rand(shape, dtype=dtype, device=device) * (max_value_cdf - min_value_cdf) + min_value_cdf\n    rand_logit = torch.log(rand_uniform / (1. - rand_uniform))\n    return scale * torch.exp(loc - rand_logit)\n\n", "CodeBleu_score": 33.1972628356558}, {"code": "def rand_log_logistic(shape, loc=0., scale=1., min_value=0., max_value=float('inf'), device='cpu', dtype=torch.float32)\n\t\n    min_value, max_value = torch.tensor(min_value, device=device, dtype=dtype), torch.tensor(max_value, device=device, dtype=dtype)\n    min_value, max_value = torch.min(min_value, max_value), torch.max(min_value, max_value)\n    min_value, max_value = torch.log(min_value), torch.log(max_value)\n    min_value, max_value = torch.log1p(torch.exp(min_value - loc) - 1) / scale + loc, torch.log1p(torch.exp(max_value - loc) - 1) / scale + loc\n    min_value, max_value = torch.clamp(min_value, min=0., max=1.), torch.clamp(max_value, min=0., max=1.)\n    uniform_values = torch.empty(shape, device=device, dtype=dtype).uniform_(min_value, max_value)\n    logit_values = torch.log(uniform_values) - torch.log1p(-uniform_values)\n    logit_values = (logit_values - loc) / scale\n    return torch.exp(logit_values)\n\n", "CodeBleu_score": 28.24406853106054}, {"code": "def rand_log_logistic(shape, loc=0., scale=1., min_value=0., max_value=float('inf'), device='cpu', dtype=torch.float32)\n\t\n    min_value = torch.as_tensor(min_value, device=device, dtype=dtype)\n    max_value = torch.as_tensor(max_value, device=device, dtype=dtype)\n    min_value = torch.clamp(min_value, min=1e-6, max=max_value)\n    max_value = torch.clamp(max_value, min=min_value, max=float('inf'))\n    min_value_cdf = torch.log(min_value) - loc\n    max_value_cdf = torch.log(max_value) - loc\n    uniform = torch.rand(shape, device=device, dtype=dtype)\n    cdf = min_value_cdf + (max_value_cdf - min_value_cdf) * uniform\n    logit = cdf - scale * torch.log1p(-torch.exp(-torch.abs(cdf)))\n    return torch.exp(loc + scale * torch.sign(logit) * torch.log1p(torch.exp(-torch.abs(logit))))\n\n", "CodeBleu_score": 40.47514984738745}]}
{"instance_id": "75e2def2467f", "generation_result": [{"code": "def get_activation(activation: Literal[\"elu\", \"snake\", \"none\"], antialias=False, channels=None) -> nn.Module\n\t\n    if activation == \"elu\":\n        activation = nn.ELU()\n    elif activation == \"snake\":\n        activation = SnakeBeta(channels)\n    elif activation == \"none\":\n        activation = nn.Identity()\n    if antialias:\n        activation = Activation1d(activation)\n    return activation\n\n", "CodeBleu_score": 55.359197403696555}, {"code": "def get_activation(activation: Literal[\"elu\", \"snake\", \"none\"], antialias=False, channels=None) -> nn.Module\n\t\n    if activation == \"elu\":\n        if antialias:\n            return comfy.ops.Activation1d(nn.ELU())\n        else:\n            return nn.ELU()\n    elif activation == \"snake\":\n        if antialias:\n            return comfy.ops.Activation1d(comfy.ops.SnakeBeta(channels))\n        else:\n            return comfy.ops.SnakeBeta(channels)\n    elif activation == \"none\":\n        return nn.Identity()\n    else:\n        raise ValueError(f\"Invalid activation type: {activation}\")\n\n", "CodeBleu_score": 42.92933242623149}, {"code": "def get_activation(activation: Literal[\"elu\", \"snake\", \"none\"], antialias=False, channels=None) -> nn.Module\n\t\n    if activation == \"elu\":\n        activation = nn.ELU()\n    elif activation == \"snake\":\n        activation = SnakeBeta(channels=channels)\n    elif activation == \"none\":\n        activation = nn.Identity()\n    else:\n        raise ValueError(f\"Invalid activation type: {activation}\")\n    if antialias:\n        activation = Activation1d(activation)\n    return activation\n\n", "CodeBleu_score": 57.52550061104903}]}
{"instance_id": "ef1714c79e51", "generation_result": [{"code": "def apply_rotary_pos_emb(t, freqs, scale = 1)\n\t\n    # cast to float32 if necessary\n    if t.dtype != torch.float32:\n        t = t.float()\n    # ensure that the input tensor and frequency tensor have the same number of dimensions\n    if t.ndim > freqs.ndim:\n        freqs = freqs.unsqueeze(0)\n    elif freqs.ndim > t.ndim:\n        t = t.unsqueeze(0)\n    # get the number of dimensions of the input tensor\n    ndims = t.ndim\n    # get the number of dimensions of the frequency tensor\n    nfreqs = freqs.ndim\n    # get the size of the input tensor\n    size = t.shape[-ndims:]\n    # get the size of the frequency tensor\n    freq_size = freqs.shape[-nfreqs:]\n    # get the size of the frequency tensor in the last dimension\n    freq_size_last = freqs.shape[-1]\n    # get the size of the frequency tensor in the second-to-last dimension\n    freq_size_second_to_last = freqs.shape[-2]\n    # get the size of the frequency tensor in the third-to-last dimension\n    freq_size_third_to_last = freqs.shape[-3]\n    # get the size of the frequency tensor in the fourth-to-last dimension\n    freq_size_fourth_to_last = freqs.shape[-4]\n    # get the size of the frequency tensor in the fifth-to-last dimension\n    freq_size_fifth_to_last = freqs.shape[-5]\n    # get the size of the frequency tensor in the sixth-to-last dimension\n    freq_size_sixth_to_last = freqs.shape[-6]\n    # get the size of the frequency tensor in the seventh-to-last dimension\n    freq_size_seventh_to_last = freqs.shape[-7]\n    # get the size of the frequency tensor in the eighth-to-last dimension\n    freq_size_eighth_to_last = freqs", "CodeBleu_score": 29.667773843007687}, {"code": "def apply_rotary_pos_emb(t, freqs, scale = 1)\n\t\n    if t.dtype != torch.float32:\n        t = t.float()\n    t = t.reshape(-1, t.shape[-1])\n    freqs = freqs.reshape(1, -1)\n    t = t[:, :freqs.shape[-1]]\n    t = t.reshape(t.shape[0], t.shape[1], 2)\n    t = t * scale\n    t = t.permute(0, 2, 1)\n    t = t.reshape(t.shape[0], t.shape[1], -1)\n    t = t * freqs\n    t = t.reshape(t.shape[0], t.shape[1], 2, -1)\n    t = t.permute(0, 2, 3, 1)\n    t = t.reshape(t.shape[0], t.shape[1], -1)\n    return t\n\n", "CodeBleu_score": 26.382413762399498}, {"code": "def apply_rotary_pos_emb(t, freqs, scale = 1)\n\t\n    # cast to float32 if necessary\n    if t.dtype != torch.float32:\n        t = t.float()\n\n    # adjust shapes\n    t = t.view(*t.shape[:2], -1)\n    freqs = freqs.view(*freqs.shape[:2], -1)\n\n    # apply rotary embeddings to a subset of the tensor\n    t1, t2 = t[..., :t.shape[-1] // 2], t[..., t.shape[-1] // 2:]\n    t1 = t1 * scale\n    freqs1, freqs2 = freqs[..., :freqs.shape[-1] // 2], freqs[..., freqs.shape[-1] // 2:]\n    t1 = t1 + freqs1 * torch.cos(t2)\n    t2 = t2 + freqs2 * torch.sin(t1)\n    t = torch.cat((t1, t2), dim = -1)\n\n    # concatenate processed and unrotated parts of the tensor\n    t = torch.cat((t, t[..., :t.shape[-1] // 2]), dim = -1)\n\n    # return\n    return t\n\n", "CodeBleu_score": 37.64385053467753}]}
{"instance_id": "a7a5f178c6f6", "generation_result": [{"code": "def idx2vq(self, idx, dim=-1)\n\t\n    # retrieve the embeddings corresponding to the given indices\n    vq_embeddings = self.weight.index_select(0, idx)\n    # if specified, move the last dimension to the given dimension\n    if dim != -1:\n        vq_embeddings = vq_embeddings.transpose(0, dim)\n    return vq_embeddings\n\n", "CodeBleu_score": 41.16331208416464}, {"code": "def idx2vq(self, idx, dim=-1)\n\t\n    if self.codebook_size is None:\n        raise ValueError(\"The codebook size is not defined.\")\n    if self.codebook is None:\n        raise ValueError(\"The codebook is not defined.\")\n    if self.codebook_dim is None:\n        raise ValueError(\"The codebook dimension is not defined.\")\n    if idx.ndim != 2:\n        raise ValueError(\"The indices must be a 2D tensor.\")\n    if idx.size(1) != 1:\n        raise ValueError(\"The indices must be a 2D tensor with the second dimension of size 1.\")\n    if idx.dtype != torch.long:\n        raise ValueError(\"The indices must be a long tensor.\")\n    if idx.size(0) != self.codebook_size:\n        raise ValueError(\"The indices must be of size equal to the codebook size.\")\n    if dim == -1:\n        dim = idx.ndim - 1\n    if dim < 0 or dim >= idx.ndim:\n        raise ValueError(\"The dimension must be between 0 and the number of dimensions of the indices.\")\n    if idx.size(0) != self.codebook_size:\n        raise ValueError(\"The indices must be of size equal to the codebook size.\")\n    if self.codebook_dim != self.codebook.size(1):\n        raise ValueError(\"The codebook dimension must be equal to the codebook embedding dimension.\")\n    if idx.size(1) != 1:\n        raise ValueError(\"The indices must be a 2D tensor with the second dimension of size 1.\")\n    if idx.dtype != torch.long:\n        raise ValueError(\"The indices must be a long tensor.\")\n    if idx.size(0) != self.codebook_size:\n        raise ValueError(\"The indices must be of size equal to the codebook size.\")\n    if dim == -1:\n        dim = idx.ndim - 1\n    if dim < 0 or dim >= idx.ndim:\n        raise ValueError(\"The dimension must be between 0 and the number of dimensions of the indices.\")\n    if idx.size(0) != self.codebook_size:\n        raise ValueError(\"", "CodeBleu_score": 31.704192204180316}, {"code": "def idx2vq(self, idx, dim=-1)\n\t\n    if idx.dtype != torch.int64:\n        raise TypeError(\"idx must be of type torch.int64\")\n    if idx.dim() != 2:\n        raise ValueError(\"idx must be 2D\")\n    if self.codebook.dtype != idx.dtype:\n        raise TypeError(\"idx and codebook must have the same dtype\")\n    if self.codebook.device != idx.device:\n        raise TypeError(\"idx and codebook must be on the same device\")\n    if self.codebook.size(0) != idx.size(0):\n        raise ValueError(\"idx and codebook must have the same number of rows\")\n    if self.codebook.size(1) != idx.size(1):\n        raise ValueError(\"idx and codebook must have the same number of columns\")\n    if dim != -1 and (dim < -idx.dim() or dim >= idx.dim()):\n        raise ValueError(\"dim must be in the range [-idx.dim(), idx.dim())\")\n    if dim != -1:\n        idx = idx.transpose(dim, -1)\n    return self.codebook.index_select(0, idx)\n\n", "CodeBleu_score": 38.417202491442325}]}
{"instance_id": "4104c041fb20", "generation_result": [{"code": "def encode(self, x, quantize=False)\n\t\n    x = self.input_layer(x)\n    x = self.blocks(x)\n    x = self.output_layer(x)\n    if quantize:\n        x, x_orig, indices, loss = self.quantizer(x)\n        return x, x_orig, indices, loss\n    return x\n\n", "CodeBleu_score": 26.262040583131174}, {"code": "def encode(self, x, quantize=False)\n\t\n    for block in self.blocks:\n        x = block(x)\n    if quantize:\n        x, orig_x, indices = self.quantizer(x)\n        return x, orig_x, indices, self.quantizer.loss\n    else:\n        return x\n\n", "CodeBleu_score": 36.70156437627176}, {"code": "def encode(self, x, quantize=False)\n\t\n    for i, block in enumerate(self.blocks):\n        x = block(x)\n    if quantize:\n        x, orig_x, indices, loss = self.quantizer(x)\n        return x, orig_x, indices, loss\n    else:\n        return x\n\n", "CodeBleu_score": 34.72509964239972}]}
{"instance_id": "403c52ba2d3b", "generation_result": [{"code": "def ema_scope(self, context=None)\n\t\n    if self.use_ema:\n        if context:\n            context.log(\"EMA weights applied\")\n        self.ema.store_params()\n        self.ema.apply_params()\n    yield\n    if self.use_ema:\n        if context:\n            context.log(\"EMA weights restored\")\n        self.ema.restore_params()\n\n", "CodeBleu_score": 40.26864890072211}, {"code": "def ema_scope(self, context=None)\n\t\n    if self.ema is None:\n        return context\n    if self.ema.enabled:\n        if context is not None:\n            context.log(f\"EMA enabled for {self.__class__.__name__}\")\n        self.ema.store_and_apply()\n        return EmaContext(self, context)\n    return context\n\n", "CodeBleu_score": 32.054377759333036}, {"code": "def ema_scope(self, context=None)\n\t\n    if self.ema is not None:\n        if context is not None:\n            self.ema.store_parameters()\n            self.ema.apply_parameters()\n            context.log_artifact(self.ema)\n        else:\n            self.ema.apply_parameters()\n    yield\n    if self.ema is not None:\n        if context is not None:\n            self.ema.restore_parameters()\n            context.log_artifact(self.ema)\n        else:\n            self.ema.restore_parameters()\n\n", "CodeBleu_score": 54.45872526569813}]}
{"instance_id": "94b7467f98e4", "generation_result": [{"code": "def post_attention(self, attn, x, gate_msa, shift_mlp, scale_mlp, gate_mlp)\n\t\n    # x + attn * gate_msa\n    x = x + attn * gate_msa\n    # Normalize x\n    x = self.norm(x)\n    # Modulate x\n    x = self.shift_scale_mlp(x) * scale_mlp + shift_mlp\n    # Apply MLP\n    x = self.gate_mlp(x) * gate_mlp\n    return x\n\n", "CodeBleu_score": 36.815490700333044}, {"code": "def post_attention(self, attn, x, gate_msa, shift_mlp, scale_mlp, gate_mlp)\n\t\n    # x = x + attn\n    x = x + attn * gate_msa\n    # x = self.norm(x)\n    x = self.norm(x)\n    # x = x * scale_mlp + shift_mlp\n    x = x * scale_mlp + shift_mlp\n    # x = self.final_layer_norm(x)\n    x = self.final_layer_norm(x)\n    # x = self.activation(x)\n    x = self.activation(x)\n    # x = self.feed_forward(x) * gate_mlp\n    x = self.feed_forward(x) * gate_mlp\n    return x\n\n", "CodeBleu_score": 41.04034730410918}, {"code": "def post_attention(self, attn, x, gate_msa, shift_mlp, scale_mlp, gate_mlp)\n\t\n    # x is the input tensor (dims: N, L, C)\n    # attn is the attention output (dims: N, L, L)\n    # gate_msa is the MSA gate (dims: N, L, L)\n    # shift_mlp is the shift MLP (dims: N, L, C)\n    # scale_mlp is the scale MLP (dims: N, L, C)\n    # gate_mlp is the MLP gate (dims: N, L, C)\n    x = x + attn * gate_msa\n    x = self.norm(x)\n    x = x * gate_mlp + shift_mlp * gate_mlp\n    x = x * scale_mlp + scale_mlp\n    return x\n\n", "CodeBleu_score": 32.46886362125467}]}
{"instance_id": "d9fdc05542a2", "generation_result": [{"code": "def _block_mixing(context, x, context_block, x_block, c)\n\t\n    # Pre-attention blocks\n    context_qkv = context_block(context)\n    x_qkv = x_block(x)\n    # Concat qkv tensors\n    qkv = torch.cat([context_qkv, x_qkv], dim=1)\n    # Optimized attention\n    qkv = qkv * c\n    # Split qkv tensors\n    context_qkv, x_qkv = torch.chunk(qkv, 2, dim=1)\n    # Post-attention blocks\n    context = context_block(context, context_qkv)\n    x = x_block(x, x_qkv)\n    return context, x\n\n", "CodeBleu_score": 9.806780492751798}, {"code": "def _block_mixing(context, x, context_block, x_block, c)\n\t\n\n    # get qkv tensors and intermediates\n    qkv_context, intermediates_context = context_block(context)\n    qkv_x, intermediates_x = x_block(x)\n\n    # concatenate qkv tensors\n    qkv_context = rearrange(qkv_context, \"b n (h d) -> b h n d\", h=c.num_heads)\n    qkv_x = rearrange(qkv_x, \"b n (h d) -> b h n d\", h=c.num_heads)\n    qkv = torch.cat([qkv_context, qkv_x], dim=2)\n\n    # optimized attention\n    qkv = qkv.permute(0, 2, 1, 3).contiguous()\n    qkv = qkv.view(qkv.shape[0], qkv.shape[1], -1)\n    qkv = qkv.permute(0, 2, 1).contiguous()\n    qkv = qkv.view(qkv.shape[0], qkv.shape[1], c.num_heads, -1)\n    qkv = qkv.permute(0, 2, 1, 3).contiguous()\n\n    # split attention results\n    qkv_context = qkv[:, :context.shape[1], :, :]\n    qkv_x = qkv[:, context.shape[1]:, :, :]\n\n    # process results through post-attention blocks\n    context = context_block.post_attention(qkv_context, intermediates_context)\n    x = x_block.post_attention(qkv_x, intermediates_x)\n\n    return context, x\n\n", "CodeBleu_score": 24.69160599083805}, {"code": "def _block_mixing(context, x, context_block, x_block, c)\n\t\n    # pre-attention block\n    qkv_context = context_block(context)\n    qkv_x = x_block(x)\n    # concatenate\n    qkv = torch.cat([qkv_context, qkv_x], dim=2)\n    # split\n    qkv_context, qkv_x = torch.split(qkv, [context.size(2), x.size(2)], dim=2)\n    # optimized attention\n    qkv_context = qkv_context + qkv_x\n    # post-attention block\n    context = context_block(qkv_context)\n    x = x_block(qkv_x)\n    return context, x\n\n", "CodeBleu_score": 12.974912611578482}]}
{"instance_id": "b219bce30160", "generation_result": [{"code": "def get_timestep_embedding(timesteps, embedding_dim)\n\t\n    timesteps = timesteps.long().view(-1)\n    assert timesteps.dim() == 1, \"timesteps must be one-dimensional\"\n    max_time = timesteps.max()\n    min_time = timesteps.min()\n    assert max_time <= 10000, \"max_time must be less than 10000\"\n    assert min_time >= 0, \"min_time must be greater than 0\"\n    num_embeddings = max_time + 1\n    device = timesteps.device\n    half_dim = embedding_dim // 2\n    emb = math.log(10000) / (half_dim - 1)\n    emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n    emb = torch.arange(num_embeddings, device=device).type_as(emb)[:, None] * emb[None, :]\n    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n    if embedding_dim % 2 == 1:\n        # zero pad\n        emb = torch.nn.functional.pad(emb, (0, 1, 0, 0))\n    return emb[timesteps]\n\n", "CodeBleu_score": 66.39631418389222}, {"code": "def get_timestep_embedding(timesteps, embedding_dim)\n\t\n    timesteps = timesteps.long().view(-1)\n    half_dim = embedding_dim // 2\n    emb = torch.log(timesteps.float() + 1) / math.log(10000)\n    emb = torch.pow(10000, emb)\n    emb = torch.arange(half_dim, dtype=torch.float32, device=timesteps.device)\n    emb = 2 * math.pi * emb / half_dim\n    emb = torch.stack((torch.sin(emb), torch.cos(emb)), dim=1).view(half_dim, 2)\n    emb = emb.repeat(timesteps.shape[0], 1, 1)\n    emb = emb.transpose(0, 1)\n    emb = emb.view(timesteps.shape[0], -1)\n    if embedding_dim % 2 == 1:\n        # zero pad\n        emb = torch.cat([emb, torch.zeros(timesteps.shape[0], 1, device=emb.device)], dim=1)\n    return emb\n\n", "CodeBleu_score": 52.99351105092326}, {"code": "def get_timestep_embedding(timesteps, embedding_dim)\n\t\n    timesteps = timesteps.long().view(-1)\n    assert timesteps.dim() == 1 and timesteps.dtype == torch.long\n\n    half_dim = embedding_dim // 2\n    emb = timesteps.float() / (10000.0)\n    emb = torch.log(1.0 + emb) / math.log(10000.0)\n    emb = torch.sin(emb[:, None] * torch.exp(torch.arange(half_dim, dtype=torch.float) / half_dim * -math.pi))\n    emb = torch.cat((emb.sin(), emb.cos()), dim=1)\n    if embedding_dim % 2 == 1:\n        # zero pad\n        emb = torch.nn.functional.pad(emb, (0, 1, 0, 0))\n    return emb\n\n", "CodeBleu_score": 58.95922027553524}]}
{"instance_id": "6ae8453c86be", "generation_result": [{"code": "def pytorch_attention(q, k, v)\n\t\n    # Reshape and transpose\n    q = q.reshape(q.shape[0], q.shape[1], q.shape[2], -1).transpose(1, 2)\n    k = k.reshape(k.shape[0], k.shape[1], k.shape[2], -1).transpose(1, 2)\n    v = v.reshape(v.shape[0], v.shape[1], v.shape[2], -1).transpose(1, 2)\n\n    # Scaled dot product attention\n    dk = torch.tensor(k.shape[-1], dtype=torch.float32, device=k.device)\n    attention = torch.matmul(q, k.transpose(-1, -2)) / torch.sqrt(dk)\n\n    # Masking\n    mask = torch.triu(torch.ones_like(attention), diagonal=1)\n    attention = attention.masked_fill(mask == 0, float('-inf'))\n\n    # Softmax\n    attention = torch.softmax(attention, dim=-1)\n\n    # Output\n    output = torch.matmul(attention, v)\n    output = output.transpose(1, 2).reshape(q.shape[0], q.shape[1], q.shape[2], v.shape[-1])\n\n    return output\n\n", "CodeBleu_score": 16.756252607109175}, {"code": "def pytorch_attention(q, k, v)\n\t\n    # Reshape and transpose to prepare for attention computation\n    q = q.contiguous().view(q.size(0), -1, q.size(-1))\n    k = k.contiguous().view(k.size(0), -1, k.size(-1))\n    v = v.contiguous().view(v.size(0), -1, v.size(-1))\n\n    # Compute attention\n    try:\n        attn = torch.bmm(q, k.transpose(1, 2))\n    except RuntimeError as e:\n        if \"out of memory\" in str(e):\n            logging.warning(\"Out of memory error occurred. Switching to slice attention.\")\n            attn = torch.bmm(q, k.transpose(1, 2))\n        else:\n            raise e\n\n    # Scale attention\n    dk = torch.sqrt(torch.tensor(k.size(-1), dtype=torch.float32))\n    attn = attn / dk\n\n    # Compute attention weights\n    attn = torch.softmax(attn, dim=-1)\n\n    # Compute attention output\n    attn = torch.bmm(attn, v)\n\n    # Reshape back to original dimensions\n    attn = attn.view(q.size(0), q.size(1), q.size(2), v.size(2))\n\n    return attn\n\n", "CodeBleu_score": 28.80052375767891}, {"code": "def pytorch_attention(q, k, v)\n\t\n    # Prepare the query, key, and value tensors for attention computation.\n    # Transpose the tensors to shape [batch_size, num_heads, seq_len, head_size]\n    q = q.transpose(1, 2).contiguous()\n    k = k.transpose(1, 2).contiguous()\n    v = v.transpose(1, 2).contiguous()\n\n    # Reshape the tensors to shape [batch_size, num_heads, seq_len, head_size]\n    q = q.view(q.size(0), q.size(1), q.size(2), -1)\n    k = k.view(k.size(0), k.size(1), k.size(2), -1)\n    v = v.view(v.size(0), v.size(1), v.size(2), -1)\n\n    # Compute the attention scores.\n    # Scale the query and key tensors.\n    q = q / math.sqrt(math.sqrt(q.size(-1)))\n    k = k / math.sqrt(math.sqrt(k.size(-1)))\n\n    # Compute the dot product between the query and key tensors.\n    # The attention scores are in the shape [batch_size, num_heads, seq_len, seq_len]\n    scores = torch.matmul(q, k.transpose(-2, -1))\n\n    # Apply the attention mask to the attention scores.\n    scores.masked_fill_(~comfy.ops.get_attention_mask(q.size(1), q.size(2), k.size(1), k.size(2), scores.device), -1e9)\n\n    # Normalize the attention scores to the range [0, 1].\n    # The attention probabilities are in the shape [batch_size, num_heads, seq_len, seq_len]\n    attention_probs = torch.nn.Softmax(dim=-1)(scores)\n\n    # Compute the attention output.\n    # The attention output is in the shape [batch_size, num", "CodeBleu_score": 18.955920045305245}]}
{"instance_id": "fe4433c987ad", "generation_result": [{"code": "def apply_control(h, control, name)\n\t\n    if exists(control, name):\n        if control[name] is not None:\n            try:\n                h = h.add(control[name])\n            except Exception as e:\n                logging.warning(f\"Failed to add control {name} to {h.name}: {e}\")\n    return h\n\n", "CodeBleu_score": 41.91860227630105}, {"code": "def apply_control(h, control, name)\n\t\n    if exists(control, name):\n        if control[name] != \"\":\n            try:\n                h = comfy.ops.add_control(h, control[name])\n            except Exception as e:\n                logging.warning(f\"Failed to add control {name} to {h.name}: {e}\")\n    return h\n\n", "CodeBleu_score": 37.39100537406019}, {"code": "def apply_control(h, control, name)\n\t\n    if exists(control) and control[name]:\n        h = h.add(control[name])\n        control.pop(name)\n    return h\n\n", "CodeBleu_score": 13.94469899318183}]}
{"instance_id": "56da4547c101", "generation_result": [{"code": "def get_codebook_entry(self, indices, shape)\n\t\n    # Create one-hot encoding of indices\n    indices = indices.type(torch.int64)\n    one_hot = torch.zeros(\n        indices.shape[0], self.num_embeddings, dtype=torch.float32, device=indices.device\n    )\n    one_hot.scatter_(1, indices.unsqueeze(1), 1)\n    quantized = torch.matmul(one_hot, self.embedding.weight)\n    if shape is not None:\n        quantized = quantized.permute(0, 2, 1).view(shape)\n    return quantized\n\n", "CodeBleu_score": 48.280586686532814}, {"code": "def get_codebook_entry(self, indices, shape)\n\t\n    # create one-hot encoding of indices\n    one_hot = torch.zeros(self.num_embeddings, *shape, device=self.weight.device)\n    one_hot.scatter_(0, indices.unsqueeze(0), 1)\n    # quantized vectors\n    quantized = torch.matmul(one_hot, self.weight.t())\n    return quantized\n\n", "CodeBleu_score": 24.728647209584242}, {"code": "def get_codebook_entry(self, indices, shape)\n\t\n\n    # Create one-hot encoding of indices\n    indices = indices.type(self.indices_dtype)\n    one_hot = torch.zeros(\n        (indices.shape[0], self.num_embeddings),\n        dtype=self.indices_dtype,\n        device=self.weight.device,\n    )\n    one_hot.scatter_(1, indices.unsqueeze(1), 1)\n\n    # Multiply one-hot encoding with embedding weights\n    quantized = torch.matmul(one_hot, self.weight)\n\n    # Reshape and permute if needed\n    if shape is not None:\n        quantized = quantized.reshape(shape)\n        if len(shape) == 4:\n            quantized = quantized.permute(0, 3, 1, 2)\n\n    return quantized\n\n", "CodeBleu_score": 47.349882165545154}]}
{"instance_id": "548f8a1fb37d", "generation_result": [{"code": "def make_noise(self)\n\t\n    noise_list = []\n    noise = torch.randn(4, 4, device=self.device)\n    noise_list.append(noise)\n    for i in range(3, self.log_size + 1):\n        noise = torch.randn(2 ** i, 2 ** i, device=self.device)\n        noise_list.append(noise)\n        noise = torch.randn(2 ** i, 2 ** i, device=self.device)\n        noise_list.append(noise)\n    return noise_list\n\n", "CodeBleu_score": 37.87703250728872}, {"code": "def make_noise(self)\n\t\n    noise_list = [\n        torch.randn(1, 1, 4, 4, device=self.device).expand(self.channels, -1, -1, -1)\n    ]\n    for i in range(3, int(math.log(self.kernel_size, 2)) + 1):\n        noise_list.append(\n            torch.randn(self.channels, 1, 2 ** i, 2 ** i, device=self.device)\n        )\n        noise_list.append(\n            torch.randn(self.channels, 1, 2 ** i, 2 ** i, device=self.device)\n        )\n    return noise_list\n\n", "CodeBleu_score": 25.782005309928955}, {"code": "def make_noise(self)\n\t\n    noise_list = []\n    noise = torch.zeros(4, 4, device=self.device)\n    noise_list.append(noise)\n    for i in range(3, self.log_size + 1):\n        noise_list.append(\n            F.interpolate(noise, size=2**i, mode='nearest'))\n        noise_list.append(\n            F.interpolate(noise, size=2**i, mode='nearest'))\n    return noise_list\n\n", "CodeBleu_score": 31.599906754963943}]}
{"instance_id": "42fd805563c6", "generation_result": [{"code": "def mean_latent(self, num_latent)\n\t\n    latent_inputs = torch.randn(num_latent, self.latent_dim, device=self.device)\n    latent_inputs = self.style_mlp(latent_inputs)\n    latent_inputs = latent_inputs.mean(dim=0)\n    return latent_inputs\n\n", "CodeBleu_score": 27.739880631810404}, {"code": "def mean_latent(self, num_latent)\n\t\n    with torch.no_grad():\n        latent = torch.randn(1, num_latent, device=self.device)\n        latent = self.style_mlp(latent)\n        latent = latent.mean(dim=0)\n        return latent\n\n", "CodeBleu_score": 43.55154494945701}, {"code": "def mean_latent(self, num_latent)\n\t\n    latents = torch.randn(num_latent, self.latent_dim, device=self.device)\n    latents = self.style_mlp(latents)\n    latents = latents.mean(dim=0, keepdim=True)\n    return latents\n\n", "CodeBleu_score": 30.539990567046182}]}
{"instance_id": "2f485fd924af", "generation_result": [{"code": "def color_jitter(img, shift)\n\t\n    jitter_value = torch.rand(3) * shift\n    jitter_value = jitter_value.reshape(1, 3, 1, 1).to(img.device)\n    img = img + jitter_value\n    img = torch.clamp(img, 0, 1)\n    return img\n\n", "CodeBleu_score": 55.799129902106024}, {"code": "def color_jitter(img, shift)\n\t\n    r, g, b = img[..., 0], img[..., 1], img[..., 2]\n    r = r + torch.randn(r.size()) * shift\n    g = g + torch.randn(g.size()) * shift\n    b = b + torch.randn(b.size()) * shift\n    r = torch.clamp(r, 0, 1)\n    g = torch.clamp(g, 0, 1)\n    b = torch.clamp(b, 0, 1)\n    img[..., 0] = r\n    img[..., 1] = g\n    img[..., 2] = b\n    return img\n\n", "CodeBleu_score": 35.309117014530756}, {"code": "def color_jitter(img, shift)\n\t\n    shift = np.random.uniform(-shift, shift, size=3)\n    img = img.astype(np.float32)\n    img[:, :, 0] = np.clip(img[:, :, 0] + shift[0], 0, 1)\n    img[:, :, 1] = np.clip(img[:, :, 1] + shift[1], 0, 1)\n    img[:, :, 2] = np.clip(img[:, :, 2] + shift[2], 0, 1)\n    return img\n\n", "CodeBleu_score": 42.40414877238249}]}
{"instance_id": "c87ccf1fbf48", "generation_result": [{"code": "def feed_data(self, data)\n\t\n    self.lq = data['lq'].to(self.device)\n    if 'gt' in data:\n        self.gt = data['gt'].to(self.device)\n\n", "CodeBleu_score": 38.202084040060775}, {"code": "def feed_data(self, data)\n\t\n    for k, v in data.items():\n        if k == 'lq':\n            self.lq = v.to(self.device)\n        elif k == 'gt':\n            self.gt = v.to(self.device)\n        elif k == 'lq_path':\n            self.lq_path = v\n        elif k == 'gt_path':\n            self.gt_path = v\n        elif k in ['l_eye', 'r_eye', 'mouth']:\n            setattr(self, k, v.to(self.device))\n\n", "CodeBleu_score": 28.87620394498481}, {"code": "def feed_data(self, data)\n\t\n    for k, v in data.items():\n        if k in ['lq', 'gt']:\n            setattr(self, k, v.to(self.device))\n        elif k in ['lq_path', 'gt_path']:\n            setattr(self, k, v)\n        elif k in ['lq_face_loc', 'gt_face_loc']:\n            setattr(self, k, v.to(self.device))\n\n", "CodeBleu_score": 24.655596434775322}]}
{"instance_id": "8f8c7cb13037", "generation_result": [{"code": "def construct_img_pyramid(self)\n\t\n    img_pyramid = [self.gt]\n    for i in range(self.n_intermediate_steps):\n        img_pyramid.insert(0, F.interpolate(img_pyramid[0], scale_factor=0.5, mode=\"bilinear\", align_corners=False))\n    return img_pyramid\n\n", "CodeBleu_score": 25.355333904264434}, {"code": "def construct_img_pyramid(self)\n\t\n    img_pyramid = []\n    img_pyramid.append(self.gt)\n    for i in range(self.num_intermediate_steps):\n        img_pyramid.insert(0, F.interpolate(img_pyramid[0], scale_factor=0.5, mode=\"bilinear\", align_corners=False))\n    return img_pyramid\n\n", "CodeBleu_score": 26.52353298171396}, {"code": "def construct_img_pyramid(self)\n\t\n    img_pyramid = [self.gt]\n    for i in range(self.n_inter_steps):\n        img_pyramid.insert(0, F.interpolate(img_pyramid[0], scale_factor=0.5, mode=\"bilinear\", align_corners=False))\n    return img_pyramid\n\n", "CodeBleu_score": 25.221386001140495}]}
{"instance_id": "51247274d40c", "generation_result": [{"code": "def save(self, epoch, current_iter)\n\t\n    # save models and training states\n    current_states = {\n        'epoch': epoch,\n        'iter': current_iter,\n        'model_G': self.model_G.state_dict(),\n        'model_G_ema': self.model_G_ema.state_dict(),\n        'model_D': self.model_D.state_dict(),\n        'model_D_ema': self.model_D_ema.state_dict(),\n        'optimizer_G': self.optimizer_G.state_dict(),\n        'optimizer_D': self.optimizer_D.state_dict(),\n        'scheduler_G': self.scheduler_G.state_dict(),\n        'scheduler_D': self.scheduler_D.state_dict(),\n        'best_metric_D': self.best_metric_D\n    }\n\n    if self.opt['train']['gan_type'] in ['wgan-gp', 'ragan']:\n        current_states['model_D_A'] = self.model_D_A.state_dict()\n        current_states['model_D_B'] = self.model_D_B.state_dict()\n        current_states['model_D_A_ema'] = self.model_D_A_ema.state_dict()\n        current_states['model_D_B_ema'] = self.model_D_B_ema.state_dict()\n\n    if self.opt['train']['gan_type'] == 'ragan':\n        current_states['model_D_C'] = self.model_D_C.state_dict()\n        current_states['model_D_C_ema'] = self.model_D_C_ema.state_dict()\n\n    if self.opt['train']['gan_type'] in ['lsgan', 'ragan']:\n        current_states['optimizer_D_A'] = self.optimizer_D_A.state_dict()\n        current_states['optimizer_D_B'] = self.optimizer_D_B.state_dict()\n        current_states['scheduler_D_A'] = self.scheduler_D_A", "CodeBleu_score": 29.096671474312295}, {"code": "def save(self, epoch, current_iter)\n\t\n    self.save_network(self.net_g, 'net_g', current_iter)\n    self.save_network(self.net_g_ema, 'net_g_ema', current_iter)\n    self.save_network(self.net_d, 'net_d', current_iter)\n    if self.opt['lambda_fa'] > 0:\n        self.save_network(self.net_d_eye, 'net_d_eye', current_iter)\n        self.save_network(self.net_d_mouth, 'net_d_mouth', current_iter)\n    self.save_training_state(epoch, current_iter)\n\n", "CodeBleu_score": 67.80935912222374}, {"code": "def save(self, epoch, current_iter)\n\t\n    if self.opt['is_train']:\n        self.save_network(self.netG, 'netG', current_iter,\n                          param_key=['params', 'params_ema'])\n        if self.opt['dist']:\n            self.save_network(self.netD, 'netD', current_iter)\n            if self.opt['fa_net_d']:\n                self.save_network(self.netD_A, 'netD_A', current_iter)\n                self.save_network(self.netD_B, 'netD_B', current_iter)\n                self.save_network(self.netD_C, 'netD_C', current_iter)\n\n", "CodeBleu_score": 56.18228574849945}]}
{"instance_id": "afa0dc8128c1", "generation_result": [{"code": "def clean_folder(folder)\n\t\n    for item in os.listdir(folder):\n        item_path = os.path.join(folder, item)\n        try:\n            if os.path.isfile(item_path):\n                os.unlink(item_path)\n            elif os.path.isdir(item_path):\n                shutil.rmtree(item_path)\n        except Exception as e:\n            print('Failed to delete %s. Reason: %s' % (item_path, e))\n\n", "CodeBleu_score": 46.18355306961337}, {"code": "def clean_folder(folder)\n\t\n    for item in os.listdir(folder):\n        item_path = os.path.join(folder, item)\n        if os.path.isfile(item_path):\n            try:\n                os.unlink(item_path)\n            except Exception as e:\n                print(f'Failed to delete {item_path}. Reason: {e}')\n        elif os.path.isdir(item_path):\n            try:\n                shutil.rmtree(item_path)\n            except Exception as e:\n                print(f'Failed to delete {item_path}. Reason: {e}')\n\n", "CodeBleu_score": 51.29956898269044}, {"code": "def clean_folder(folder)\n\t\n    for filename in os.listdir(folder):\n        file_path = os.path.join(folder, filename)\n        try:\n            if os.path.isfile(file_path) or os.path.islink(file_path):\n                os.unlink(file_path)\n            elif os.path.isdir(file_path):\n                shutil.rmtree(file_path)\n        except Exception as e:\n            print('Failed to delete %s. Reason: %s' % (file_path, e))\n\n", "CodeBleu_score": 67.88650947584345}]}
{"instance_id": "53588b481bdd", "generation_result": [{"code": "def next_batch(self, sample_size=1)\n\t\n    if len(self.experiences) < sample_size:\n        sample_size = len(self.experiences)\n    batch = random.sample(self.experiences, sample_size)\n    self.experiences = [x for x in self.experiences if x not in batch]\n    return batch\n\n", "CodeBleu_score": 60.38298568558618}, {"code": "def next_batch(self, sample_size=1)\n\t\n\n    # Check if the sample size exceeds the number of available experiences\n    if sample_size > len(self.experiences):\n        sample_size = len(self.experiences)\n\n    # Randomly select the specified number of samples\n    sampled_batch = random.sample(self.experiences, sample_size)\n\n    # Remove the sampled batch from the original list\n    self.experiences = [experience for experience in self.experiences if experience not in sampled_batch]\n\n    # Return the sampled batch\n    return sampled_batch\n\n", "CodeBleu_score": 69.72789706879618}, {"code": "def next_batch(self, sample_size=1)\n\t\n    if sample_size > len(self.experiences):\n        sample_size = len(self.experiences)\n    batch = random.sample(self.experiences, sample_size)\n    for e in batch:\n        self.experiences.remove(e)\n    return batch\n\n", "CodeBleu_score": 49.57300423715275}]}
{"instance_id": "6bfa6a4e621c", "generation_result": [{"code": "def find_estimated_top_configs(self)\n\t\n    # flatten all configurations\n    flattened_configs = []\n    for config in self.all_configs:\n        flattened_config = {}\n        for key, value in config.items():\n            if isinstance(value, dict):\n                for sub_key, sub_value in value.items():\n                    flattened_config[key + \"_\" + sub_key] = sub_value\n            else:\n                flattened_config[key] = value\n        flattened_configs.append(flattened_config)\n\n    # collect numerical values\n    numerical_values = []\n    for config in flattened_configs:\n        numerical_value = []\n        for key, value in config.items():\n            if isinstance(value, numbers.Number):\n                numerical_value.append(value)\n        numerical_values.append(numerical_value)\n\n    # convert to numpy array\n    numerical_values = np.array(numerical_values)\n\n    # predict estimates\n    estimates = self.cost_model.predict(numerical_values)\n\n    # sort\n    sorted_indices = np.argsort(estimates)\n    sorted_indices = sorted_indices[::-1]\n\n    # return indices\n    return sorted_indices\n\n", "CodeBleu_score": 37.02811881873453}, {"code": "def find_estimated_top_configs(self)\n\t\n\n    # flatten the configurations\n    flattened_configs = [\n        [\n            item\n            for sublist in [\n                [\n                    item\n                    for item in config.values()\n                    if isinstance(item, numbers.Number)\n                ]\n                for config in self.all_configs\n            ]\n            for item in sublist\n        ]\n    ]\n\n    # convert the flattened configurations to a numpy array\n    flattened_configs = np.array(flattened_configs)\n\n    # get the predicted cost for each configuration\n    predicted_costs = self.cost_model.predict(flattened_configs)\n\n    # sort the configurations based on the metric\n    if self.metric == \"min\":\n        sorted_indices = predicted_costs.argsort()\n    elif self.metric == \"max\":\n        sorted_indices = predicted_costs.argsort()[::-1]\n\n    # return the indices of the top configurations\n    return sorted_indices[: self.num_configs]\n\n", "CodeBleu_score": 25.814943364084993}, {"code": "def find_estimated_top_configs(self)\n\t\n\n    # flatten all configurations\n    flat_configs = [\n        [\n            value\n            for value in config.values()\n            if isinstance(value, numbers.Number) or isinstance(value, bool)\n        ]\n        for config in self.all_configs\n    ]\n\n    # convert to numpy array\n    flat_configs = np.array(flat_configs)\n\n    # predict estimates\n    estimates = self.cost_model.predict(flat_configs)\n\n    # sort configurations according to the metric\n    if self.metric == \"minimize\":\n        indices = np.argsort(estimates)\n    else:\n        indices = np.argsort(estimates)[::-1]\n\n    return indices\n\n", "CodeBleu_score": 30.012828410380454}]}
{"instance_id": "d4cf48413a57", "generation_result": [{"code": "def flatten(d, parent_key='', sep='_')\n\t\n    items = []\n    for k, v in d.items():\n        new_key = parent_key + sep + k if parent_key else k\n        if isinstance(v, collections.abc.Mapping):\n            items.extend(flatten(v, new_key, sep=sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)\n\n", "CodeBleu_score": 80.44699777155867}, {"code": "def flatten(d, parent_key='', sep='_')\n\t\n    items = []\n    for k, v in d.items():\n        new_key = parent_key + sep + k if parent_key else k\n        if isinstance(v, collections.abc.MutableMapping):\n            items.extend(flatten(v, new_key, sep=sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)\n\n", "CodeBleu_score": 83.2096634628176}, {"code": "def flatten(d, parent_key='', sep='_')\n\t\n\n    items = []\n    for k, v in d.items():\n        new_key = parent_key + sep + k if parent_key else k\n        if isinstance(v, collections.abc.Mapping):\n            items.extend(flatten(v, new_key, sep=sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)\n\n", "CodeBleu_score": 80.44699777155867}]}
{"instance_id": "c2a514249c25", "generation_result": [{"code": "def run_after_tuning(self)\n\t\n\n    if self.optimal_cmd:\n        self.log(f\"Tuning completed successfully. Running command: {self.optimal_cmd}\")\n        process = subprocess.Popen(self.optimal_cmd, shell=True)\n        process.wait()\n        self.log(f\"Command completed successfully.\")\n    else:\n        self.log(\"Tuning completed successfully, but no optimal command found.\")\n\n", "CodeBleu_score": 52.17104030510127}, {"code": "def run_after_tuning(self)\n\t\n    if self.optimal_cmd is None:\n        print(\"Autotuning did not find an optimal configuration.\")\n        return\n\n    print(f\"Running the following command: {self.optimal_cmd}\")\n    subprocess.Popen(self.optimal_cmd, shell=True)\n    time.sleep(self.sleep_time)\n    print(\"Autotuning complete.\")\n\n", "CodeBleu_score": 33.01956013713203}, {"code": "def run_after_tuning(self)\n\t\n    if self.optimal_cmd:\n        # Run the optimal command\n        subprocess.Popen(self.optimal_cmd, shell=True)\n        print(\"Tuning complete. Optimal command is:\")\n        print(self.optimal_cmd)\n        print(\"Tuning took:\", self.tuning_time)\n        print(\"Optimal command took:\", self.optimal_time)\n        print(\"Optimal command is:\", self.optimal_cmd)\n        print(\"Optimal command time:\", self.optimal_time)\n        print(\"Optimal command time (relative):\", self.optimal_time / self.tuning_time)\n        print(\"Optimal command time (relative, 100x):\", self.optimal_time / self.tuning_time * 100)\n        print(\"Optimal command time (relative, 1000x):\", self.optimal_time / self.tuning_time * 1000)\n        print(\"Optimal command time (relative, 10000x):\", self.optimal_time / self.tuning_time * 10000)\n        print(\"Optimal command time (relative, 100000x):\", self.optimal_time / self.tuning_time * 100000)\n        print(\"Optimal command time (relative, 1000000x):\", self.optimal_time / self.tuning_time * 1000000)\n        print(\"Optimal command time (relative, 10000000x):\", self.optimal_time / self.tuning_time * 10000000)\n        print(\"Optimal command time (relative, 100000000x):\", self.optimal_time / self.tuning_time * 100000000)\n        print(\"Optimal command time (relative, 1000000000x):\", self.optimal_time / self.tuning_time * 1000000000)\n        print(\"Optimal", "CodeBleu_score": 34.66267284809905}]}
{"instance_id": "61802de52df0", "generation_result": [{"code": "def run_job(self, exp: dict, reservations)\n\t\n    exp_id = exp['id']\n    script = exp['script']\n    args = exp['args']\n    args_mapping = exp['args_mapping']\n    exp['result_dir'] = os.path.join(self.result_dir, exp_id)\n    exp['master_port'] = self.get_free_port()\n    exp['master_addr'] = self.master_addr\n    exp['rank'] = self.rank\n    exp['world_size'] = self.world_size\n    exp['local_rank'] = self.local_rank\n    exp['local_world_size'] = self.local_world_size\n    exp['local_master_addr'] = self.local_master_addr\n    exp['local_master_port'] = self.local_master_port\n    exp['local_rank_start'] = self.local_rank_start\n    exp['local_world_size_start'] = self.local_world_size_start\n    exp['local_world_size_end'] = self.local_world_size_end\n    exp['local_world_size_step'] = self.local_world_size_step\n    exp['local_rank_end'] = self.local_rank_end\n    exp['local_rank_step'] = self.local_rank_step\n    exp['local_rank_offset'] = self.local_rank_offset\n    exp['local_world_size_offset'] = self.local_world_size_offset\n    exp['local_world_size_max'] = self.local_world_size_max\n    exp['local_world_size_min'] = self.local_world_size_min\n    exp['local_world_size_list'] = self.local_world_size_list\n    exp['local_rank_list'] = self.local_rank_list\n    exp['local_world_size_list_str'] = self.local_world_size_list_str\n    exp['local_rank_list_str'] = self.local_rank_list_str\n    exp['local_world_size_max_str'] = self.local_world_size_max_str\n    exp['local_world_", "CodeBleu_score": 9.795717027100613}, {"code": "def run_job(self, exp: dict, reservations)\n\t\n    # set the master port\n    exp['args']['master_port'] = self.master_port\n    # set the result directory\n    exp['args']['result_dir'] = self.result_dir\n    # update the user arguments based on argument mappings\n    exp['args'] = self.update_args(exp['args'], exp['mappings'])\n    # set the experiment id\n    exp['id'] = self.experiment_id\n    # set the experiment time\n    exp['time'] = time.time()\n    # set the experiment status\n    exp['status'] = 'running'\n    # set the experiment reservation\n    exp['reservation'] = reservations\n    # append the experiment to running_experiments\n    self.running_experiments.append(exp)\n    # increment the experiment id\n    self.experiment_id += 1\n    # start the experiment\n    exp_thread = threading.Thread(target=self.start_experiment, args=(exp,))\n    exp_thread.start()\n\n", "CodeBleu_score": 16.827014062785505}, {"code": "def run_job(self, exp: dict, reservations)\n\t\n    exp_id = exp['id']\n    exp_name = exp['name']\n    exp_script = exp['script']\n    exp_args = exp['args']\n    exp_reservation = exp['reservation']\n    exp_reservation_id = reservations[exp_reservation]['id']\n    exp_reservation_name = reservations[exp_reservation]['name']\n    exp_reservation_user = reservations[exp_reservation]['user']\n    exp_reservation_group = reservations[exp_reservation]['group']\n    exp_reservation_nodes = reservations[exp_reservation]['nodes']\n    exp_reservation_cores = reservations[exp_reservation]['cores']\n    exp_reservation_gpus = reservations[exp_reservation]['gpus']\n    exp_reservation_gpu_model = reservations[exp_reservation]['gpu_model']\n    exp_reservation_walltime = reservations[exp_reservation]['walltime']\n    exp_reservation_partition = reservations[exp_reservation]['partition']\n    exp_reservation_account = reservations[exp_reservation]['account']\n    exp_reservation_nodelist = reservations[exp_reservation]['nodelist']\n    exp_reservation_exclude = reservations[exp_reservation]['exclude']\n    exp_reservation_features = reservations[exp_reservation]['features']\n    exp_reservation_comment = reservations[exp_reservation]['comment']\n    exp_reservation_starttime = reservations[exp_reservation]['starttime']\n    exp_reservation_endtime = reservations[exp_reservation]['endtime']\n    exp_reservation_jobname = reservations[exp_reservation]['jobname']\n    exp_reservation_jobid = reservations[exp_reservation]['jobid']\n    exp_reservation_jobowner = reservations[exp_reservation]['jobowner']\n    exp_reservation_jobgroup = reservations[exp_reservation]['jobgroup']\n    exp_reservation_jobstate =", "CodeBleu_score": 8.058197500093414}]}
{"instance_id": "4884e2e9665c", "generation_result": [{"code": "def save_exp_results_to_database(self, message, ranks=None, path=None):\n\t\n    if self.rank == 0:\n        # check if distributed training is initialized\n        if dist.is_initialized():\n            # check if message is a string\n            if type(message) == str:\n                # check if message is not empty\n                if message != \"\":\n                    # check if ranks are specified\n                    if ranks is not None:\n                        # check if ranks is a list\n                        if type(ranks) == list:\n                            # check if message is not empty\n                            if message != \"\":\n                                # check if path is specified\n                                if path is not None:\n                                    # check if path is a string\n                                    if type(path) == str:\n                                        # check if path is not empty\n                                        if path != \"\":\n                                            # check if path exists\n                                            if os.path.exists(path):\n                                                # check if path is a directory\n                                                if os.path.isdir(path):\n                                                    # check if path is writable\n                                                    if os.access(path, os.W_OK):\n                                                        # check if message is not empty\n                                                        if message != \"\":\n                                                            # check if ranks are specified\n                                                            if ranks is not None:\n                                                                # check if ranks is a list\n                                                                if type(ranks) == list:\n                                                                    # check if message is not empty\n                                                                    if message != \"\":\n                                                                        # check if path is specified\n                                                                        if path is not None:\n                                                                            # check if path is a string\n                                                                            if type(path) == str:\n                                                                                # check if path is not empty\n                                                                                if path != \"\":\n                                                                                    # check if path exists\n                                                                                    if os.path.exists(path):\n                                                                                        # check if path is a directory\n                                                                                        if os.path.isdir(path):\n                                                                                            # check if path is writable\n                                                                                            if os.", "CodeBleu_score": 17.609646058949423}, {"code": "def save_exp_results_to_database(self, message, ranks=None, path=None):\n\t\n    if self.distributed_initialized:\n        if ranks is None:\n            ranks = [self.global_rank]\n        else:\n            ranks = ranks\n        for rank in ranks:\n            if rank == self.global_rank:\n                if path is None:\n                    path = self.exp_results_path\n                with open(path, \"a+\") as f:\n                    f.write(json.dumps(message) + \"", "CodeBleu_score": 24.365215883989134}, {"code": "def save_exp_results_to_database(self, message, ranks=None, path=None):\n\t\n    if not self.log_to_db:\n        return\n\n    if self.is_distributed:\n        if ranks is None:\n            ranks = [0]\n        if self.rank not in ranks:\n            return\n\n    if path is None:\n        path = self.log_path\n\n    if not os.path.exists(path):\n        os.makedirs(path)\n\n    with open(os.path.join(path, 'log.json'), 'a') as f:\n        message['rank'] = self.rank\n        json.dump(message, f)\n        f.write('", "CodeBleu_score": 26.08455028897332}]}
{"instance_id": "0429344c3c4d", "generation_result": [{"code": "def _build_tp_other_layer_map(self, layer_index: int)\n\t\n    layer_files = [\n        f for f in os.listdir(self.checkpoint_folder) if f.startswith(f\"layer{layer_index}\")\n    ]\n    if len(layer_files) == 0:\n        raise Exception(\n            f\"No layer files found for layer {layer_index} in checkpoint folder {self.checkpoint_folder}.\"\n        )\n    if layer_index not in range(len(self.layer_partition_map)):\n        raise Exception(\n            f\"Layer index {layer_index} is out of range. The layer partition map has {len(self.layer_partition_map)} layers.\"\n        )\n    tp_other_layer_map: Dict[int, list] = {}\n    for layer_file in layer_files:\n        tp_degree = int(\n            re.findall(r\"\\d+\", layer_file)[0]\n        )  # find the tp degree of the layer file\n        tp_other_layer_map.setdefault(tp_degree, []).append(layer_file)\n    return tp_other_layer_map\n\n", "CodeBleu_score": 25.261517294670544}, {"code": "def _build_tp_other_layer_map(self, layer_index: int)\n\t\n    if not self.layer_files:\n        raise ValueError(\n            \"No layer files found. Please call 'save_layer_files' function first.\"\n        )\n\n    if layer_index > self.num_layers:\n        raise ValueError(\n            f\"Layer index {layer_index} is greater than the number of layers {self.num_layers}.\"\n        )\n\n    layer_files = [\n        file\n        for file in self.layer_files\n        if re.search(f\"layer_{layer_index}_\", file)\n    ]\n\n    tp_other_layer_map = {}\n    for file in layer_files:\n        tp_degree = int(file.split(\"_\")[-1].split(\".\")[0])\n        if tp_degree in tp_other_layer_map:\n            tp_other_layer_map[tp_degree].append(file)\n        else:\n            tp_other_layer_map[tp_degree] = [file]\n\n    return tp_other_layer_map\n\n", "CodeBleu_score": 31.35325552946695}, {"code": "def _build_tp_other_layer_map(self, layer_index: int)\n\t\n    if not self.layer_files:\n        raise ValueError(\n            \"No layer files were found. Please make sure the layer files are in the correct directory and have the correct prefix.\"\n        )\n\n    if layer_index < 0 or layer_index >= len(self.layer_files):\n        raise ValueError(\n            f\"Layer index {layer_index} is out of bounds. The layer index must be between 0 and {len(self.layer_files) - 1}.\"\n        )\n\n    layer_file_list = self.layer_files[layer_index]\n    tp_other_layer_map = {}\n\n    for layer_file in layer_file_list:\n        tp_degree = re.search(r\"_tp(\\d+)_\", layer_file).group(1)\n\n        if tp_degree not in tp_other_layer_map:\n            tp_other_layer_map[tp_degree] = []\n\n        tp_other_layer_map[tp_degree].append(layer_file)\n\n    return tp_other_layer_map\n\n", "CodeBleu_score": 31.86012439553053}]}
{"instance_id": "1b65ecb9be93", "generation_result": [{"code": "def system_info()\n\t\n    system = dict()\n    system['os'] = platform.system()\n    system['architecture'] = platform.machine()\n    system['version'] = platform.version()\n    system['processor'] = platform.processor()\n    system['python'] = sys.version\n    return system\n\n", "CodeBleu_score": 28.870814659816048}, {"code": "def system_info()\n\t\n    info = {}\n    info['OS'] = platform.system()\n    info['Architecture'] = platform.machine()\n    info['Version'] = platform.version()\n    info['Processor'] = platform.processor()\n    info['Python'] = sys.version\n    return info\n\n", "CodeBleu_score": 41.54592071373325}, {"code": "def system_info()\n\t\n    info = {}\n    info[\"os\"] = platform.system()\n    info[\"architecture\"] = platform.architecture()[0]\n    info[\"version\"] = platform.version()\n    info[\"processor\"] = platform.processor()\n    info[\"python_version\"] = platform.python_version()\n    return info\n\n", "CodeBleu_score": 43.81320733289156}]}
{"instance_id": "969bcc5cf3b5", "generation_result": [{"code": "def cuda_info()\n\t\n    try:\n        import torch\n        import torch.cuda\n        import torch.backends.cudnn\n        import torch.utils.dlpack\n        import cupy\n        import cupy.cuda\n        import cupy.cuda.runtime\n        import cupyx\n        import cupyx.scipy\n        import cupyx.scipy.ndimage\n        import cupyx.scipy.ndimage.filters\n        import cupyx.scipy.ndimage.interpolation\n        import cupyx.scipy.ndimage.measurements\n        import cupyx.scipy.ndimage.morphology\n        import cupyx.scipy.ndimage.measurements\n        import cupyx.scipy.ndimage.measurements\n        import cupyx.scipy.ndimage.measurements\n        import cupyx.scipy.ndimage.measurements\n        import cupyx.scipy.ndimage.measurements\n        import cupyx.scipy.ndimage.measurements\n        import cupyx.scipy.ndimage.measurements\n        import cupyx.scipy.ndimage.measurements\n        import cupyx.scipy.ndimage.measurements\n        import cupyx.scipy.ndimage.measurements\n        import cupyx.scipy.ndimage.measurements\n        import cupyx.scipy.ndimage.measurements\n        import cupyx.scipy.ndimage.measurements\n        import cupyx.scipy.ndimage.measurements\n        import cupyx.scipy.ndimage.measurements\n        import cupyx.scipy.ndimage.measurements\n        import cupyx.scipy.ndimage.measurements\n        import cupyx.scipy.ndimage.measurements\n        import cupyx.scipy.ndimage.measurements\n        import cupyx.scipy.ndimage.measurements\n        import cupyx.scipy.ndimage.measurements\n        import cupyx.scipy.ndimage.measurements\n        import cupyx.scipy.ndimage.measurements\n        import cupyx.scipy.ndimage.measurements\n       ", "CodeBleu_score": 17.062524312912995}, {"code": "def cuda_info()\n\t\n    print(\"CUDA Information:\")\n    print(\"----------------------------------------------------------------------------------------------------------------------------------------------------------------\")\n    print(\"CUDA Available: \", torch.cuda.is_available())\n    print(\"CUDA Version: \", torch.version.cuda)\n    print(\"CUDA Device Names: \", torch.cuda.get_device_name())\n    print(\"----------------------------------------------------------------------------------------------------------------------------------------------------------------\")\n\n", "CodeBleu_score": 26.900173611569265}, {"code": "def cuda_info()\n\t\n    import subprocess\n\n    gpu_names = []\n    if torch.cuda.is_available():\n        for i in range(torch.cuda.device_count()):\n            gpu_names.append(torch.cuda.get_device_name(i))\n    return {\"GPU_names\": gpu_names, \"CUDA_available\": torch.cuda.is_available(), \"CUDA_version\": torch.version.cuda}\n\n", "CodeBleu_score": 53.52492054734726}]}
{"instance_id": "a2ba908a4c70", "generation_result": [{"code": "def compute_encoder_accuracy(dataset_items, encoder_manager)\n\t\n    # retrieve class name key and optional class ID to class name mapping\n    class_name_key = encoder_manager.config.class_name_key\n    class_id_to_class_name = encoder_manager.config.class_id_to_class_name\n    # iterate over all dataset items\n    accuracies = {}\n    for item in tqdm(dataset_items, desc=\"Computing encoder accuracy\"):\n        # extract audio file\n        audio_file = item[encoder_manager.config.audio_key]\n        # compute embedding\n        embedding = encoder_manager.compute_embedding(audio_file)\n        # predict class label\n        if class_name_key in item:\n            true_class_name = item[class_name_key]\n            if class_id_to_class_name:\n                true_class_id = class_id_to_class_name[true_class_name]\n                predicted_class_id = encoder_manager.predict_class_id(embedding)\n                predicted_class_name = class_id_to_class_name[predicted_class_id]\n            else:\n                predicted_class_name = encoder_manager.predict_class_name(embedding)\n            # record accuracy\n            if true_class_name in accuracies:\n                accuracies[true_class_name] += 1\n            else:\n                accuracies[true_class_name] = 1\n            if predicted_class_name == true_class_name:\n                accuracies[true_class_name] += 1\n    # calculate and print average accuracy\n    total_accuracy = 0\n    for class_name, accuracy in accuracies.items():\n        accuracy /= len(dataset_items)\n        total_accuracy += accuracy\n        print(f\"Accuracy for class '{class_name}': {accuracy:.2f}\")\n    print(f\"Average accuracy: {total_accuracy / len(accuracies):.2f}\")\n\n", "CodeBleu_score": 33.31706750783483}, {"code": "def compute_encoder_accuracy(dataset_items, encoder_manager)\n\t\n    # Get the class name key and optional class ID to class name mapping from the encoder manager's configuration\n    class_name_key = encoder_manager.config.class_name_key\n    class_id_to_class_name = encoder_manager.config.class_id_to_class_name\n\n    # For each item in the dataset, extract the audio file, compute the embedding, and predict the class label\n    # if criteria and mappings are available\n    correct_predictions = 0\n    total_predictions = 0\n    for item in tqdm(dataset_items):\n        # Extract the audio file\n        audio_file = item['audio_file']\n\n        # Compute the embedding\n        embeddings = encoder_manager.embed_utterance(audio_file)\n\n        # Predict the class label if criteria and mappings are available\n        if class_name_key in item and class_id_to_class_name:\n            # Get the true class name\n            true_class_name = item[class_name_key]\n\n            # Predict the class label\n            predicted_class_name = encoder_manager.predict_class_label(embeddings)\n\n            # Increment the correct predictions and total predictions if the true class name matches the predicted label\n            if true_class_name == predicted_class_name:\n                correct_predictions += 1\n            total_predictions += 1\n\n    # Calculate and print the accuracy for each class and the average accuracy across all classes\n    class_accuracies = []\n    for class_name in class_id_to_class_name.values():\n        accuracy = correct_predictions / total_predictions * 100 if total_predictions > 0 else 0\n        print(f'Accuracy for class \"{class_name}\": {accuracy:.2f}%')\n        class_accuracies.append(accuracy)\n    average_accuracy = sum(class_accuracies) / len(class_accuracies)\n    print(f'Average accuracy: {average_accuracy:.2f}%')\n\n", "CodeBleu_score": 28.996007435481495}, {"code": "def compute_encoder_accuracy(dataset_items, encoder_manager)\n\t\n    class_name_key = encoder_manager.config['class_name_key']\n    class_id_to_name_mapping = encoder_manager.config.get('class_id_to_name_mapping')\n    class_name_to_id_mapping = encoder_manager.config.get('class_name_to_id_mapping')\n    class_accuracy = {}\n    total_accuracy = 0\n    total_samples = 0\n    for item in tqdm(dataset_items):\n        audio_file = item['audio_file']\n        class_name = item[class_name_key]\n        if class_name_to_id_mapping is not None:\n            class_id = class_name_to_id_mapping[class_name]\n        else:\n            class_id = class_name\n        if class_id_to_name_mapping is not None:\n            class_name = class_id_to_name_mapping[class_id]\n        embedding = encoder_manager.compute_embedding(audio_file)\n        predicted_class_name = encoder_manager.predict_class_name(embedding)\n        if predicted_class_name == class_name:\n            if class_name not in class_accuracy:\n                class_accuracy[class_name] = 0\n            class_accuracy[class_name] += 1\n            total_accuracy += 1\n        total_samples += 1\n    print('Accuracy per class:')\n    for class_name, accuracy in class_accuracy.items():\n        print('{:<30s} {:>5.2f}'.format(class_name, 100 * accuracy / total_samples))\n    print('Average accuracy: {:>5.2f}'.format(100 * total_accuracy / total_samples))\n\n", "CodeBleu_score": 34.454351213353654}]}
{"instance_id": "e01ec69ad58a", "generation_result": [{"code": "def adjust_path_and_remove_silence(audio_path)\n\t\n    # Get the output path\n    output_path = audio_path.replace(args.input_dir, args.output_dir)\n    output_path = output_path.replace(\".wav\", \"_trimmed.wav\")\n    output_path = output_path.replace(\".mp3\", \"_trimmed.wav\")\n    output_path = output_path.replace(\".m4a\", \"_trimmed.wav\")\n    output_path = output_path.replace(\".flac\", \"_trimmed.wav\")\n    output_path = output_path.replace(\".ogg\", \"_trimmed.wav\")\n\n    # Check if the file already exists\n    if os.path.exists(output_path):\n        return output_path, False\n\n    # Create the output directory if it doesn't exist\n    output_path_dir = os.path.dirname(output_path)\n    pathlib.Path(output_path_dir).mkdir(parents=True, exist_ok=True)\n\n    # Remove silence\n    trimmed_audio, detected = remove_silence(audio_path, model, device, args.threshold, args.min_duration, args.max_duration)\n\n    # Save the trimmed audio\n    torch.save(trimmed_audio, output_path)\n\n    return output_path, detected\n\n", "CodeBleu_score": 43.13245364181532}, {"code": "def adjust_path_and_remove_silence(audio_path)\n\t\n\n    # Replace input directory with output directory\n    output_path = audio_path.replace(args.input_dir, args.output_dir)\n\n    # Check if file already exists\n    if os.path.exists(output_path) and not args.force:\n        return output_path, False\n\n    # Create directory structure\n    pathlib.Path(os.path.dirname(output_path)).mkdir(parents=True, exist_ok=True)\n\n    # Remove silence from audio\n    audio, sampling_rate, speech_detected = remove_silence(audio_path, args.vad_model, args.vad_sensitivity, args.vad_chunk_size, args.vad_chunk_overlap)\n\n    # Save audio\n    torch.save(audio, output_path)\n\n    return output_path, speech_detected\n\n", "CodeBleu_score": 43.989951200019654}, {"code": "def adjust_path_and_remove_silence(audio_path)\n\t\n\n    # Replace the input directory path with the output directory path to determine the new file location\n    audio_path = audio_path.replace(args.input_dir, args.output_dir)\n\n    # If the file already exists and force is not specified, return the output path and a False flag\n    if os.path.exists(audio_path) and not args.force:\n        return audio_path, False\n\n    # Create the necessary directory structure\n    os.makedirs(os.path.dirname(audio_path), exist_ok=True)\n\n    # Remove silence from the audio using specified parameters\n    audio, fs = torchaudio.load(audio_path)\n    audio = remove_silence(audio, fs, args.vad_threshold, args.vad_frame_length, args.vad_frame_step, args.vad_max_silence_length)\n\n    # Save the processed audio\n    torchaudio.save(audio_path, audio, fs)\n\n    # Return the output path and a flag indicating if speech was detected\n    return audio_path, True\n\n", "CodeBleu_score": 38.14792220184731}]}
{"instance_id": "7e0bdb0feff8", "generation_result": [{"code": "def resample_files(input_dir, output_sr, output_dir=None, file_ext=\"wav\", n_jobs=10)\n\t\n    if output_dir is None:\n        output_dir = input_dir\n    else:\n        copytree(input_dir, output_dir)\n    input_files = glob.glob(os.path.join(input_dir, \"**\", \"*.{}\".format(file_ext)), recursive=True)\n    output_files = [os.path.join(output_dir, os.path.relpath(f, input_dir)) for f in input_files]\n    with Pool(n_jobs) as p:\n        list(tqdm(p.imap(resample_file, input_files, output_files, [output_sr] * len(input_files)), total=len(input_files)))\n    print(\"Done!\")\n\n", "CodeBleu_score": 41.756487758733044}, {"code": "def resample_files(input_dir, output_sr, output_dir=None, file_ext=\"wav\", n_jobs=10)\n\t\n    if output_dir is None:\n        output_dir = input_dir\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    else:\n        copytree(input_dir, output_dir)\n    file_paths = glob.glob(os.path.join(output_dir, \"**\", \"*.\" + file_ext), recursive=True)\n    with Pool(n_jobs) as pool:\n        list(tqdm(pool.imap(resample_file, file_paths), total=len(file_paths)))\n    print(\"Resampling complete!\")\n\n", "CodeBleu_score": 34.484776376808945}, {"code": "def resample_files(input_dir, output_sr, output_dir=None, file_ext=\"wav\", n_jobs=10)\n\t\n    if output_dir is None:\n        output_dir = input_dir\n    else:\n        copytree(input_dir, output_dir)\n\n    files = glob.glob(os.path.join(output_dir, \"**\", \"*.\" + file_ext), recursive=True)\n    files = [file for file in files if os.path.isfile(file)]\n\n    p = Pool(n_jobs)\n    p.map(\n        lambda file: librosa.resample(\n            librosa.load(file, sr=None)[0],\n            librosa.get_samplerate(file),\n            output_sr,\n        ),\n        files,\n    )\n    p.close()\n    p.join()\n\n    for file in tqdm(files):\n        librosa.output.write_wav(file, librosa.resample(librosa.load(file, sr=None)[0], librosa.get_samplerate(file), output_sr), output_sr)\n\n    print(f\"Successfully resampled {len(files)} files to {output_sr} Hz.\")\n\n", "CodeBleu_score": 36.6285556102936}]}
{"instance_id": "a1351834ca57", "generation_result": [{"code": "def evaluation(model, criterion, data_loader, global_step)\n\t\n    # Initialize variables\n    model.eval()\n    total_loss = 0\n    total_num = 0\n    # Initialize UMAP plot\n    embeddings = []\n    labels = []\n    # Iterate over data\n    for i, (inputs, labels) in enumerate(data_loader):\n        # Move data to device\n        inputs = inputs.to(device)\n        labels = labels.to(device)\n        # Reshape data\n        inputs = inputs.view(inputs.size(0) * inputs.size(1), inputs.size(2))\n        labels = labels.view(labels.size(0) * labels.size(1))\n        # Compute loss\n        with torch.no_grad():\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n        # Accumulate loss\n        total_loss += loss.item()\n        total_num += 1\n        # Add embeddings to UMAP plot\n        embeddings.append(outputs.detach().cpu())\n        labels.append(labels.detach().cpu())\n    # Compute average loss\n    avg_loss = total_loss / total_num\n    # Log evaluation statistics\n    print('Evaluation loss at step {}: {:.4f}'.format(global_step, avg_loss))\n    # Generate and log UMAP plot\n    embeddings = torch.cat(embeddings, dim=0)\n    labels = torch.cat(labels, dim=0)\n    plot_embeddings(embeddings, labels, global_step, model.module.name)\n    return avg_loss\n\n", "CodeBleu_score": 34.30164186447695}, {"code": "def evaluation(model, criterion, data_loader, global_step)\n\t\n    model.eval()\n    total_loss = 0.\n    total_sample_size = 0\n    with torch.no_grad():\n        for i, (inputs, labels) in enumerate(data_loader):\n            inputs = inputs.to(model.device)\n            labels = labels.to(model.device)\n            inputs = inputs.view(inputs.size(0), -1)\n            labels = labels.view(labels.size(0), -1)\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            total_loss += loss.item() * labels.size(0)\n            total_sample_size += labels.size(0)\n            if i == len(data_loader) - 1:\n                embeddings = outputs.detach().cpu().numpy()\n                labels = labels.detach().cpu().numpy()\n                plot_embeddings(embeddings, labels, global_step)\n    return total_loss / total_sample_size\n\n", "CodeBleu_score": 32.469206883285665}, {"code": "def evaluation(model, criterion, data_loader, global_step)\n\t\n    model.eval()\n    losses = []\n    with torch.no_grad():\n        for batch_idx, (inputs, labels) in enumerate(data_loader):\n            if torch.cuda.is_available():\n                inputs, labels = inputs.cuda(), labels.cuda()\n            inputs, labels = inputs.view(inputs.size(0), -1), labels.view(labels.size(0), -1)\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            losses.append(loss.item())\n            if batch_idx == len(data_loader) - 1:\n                plot_embeddings(outputs, labels, global_step)\n    return sum(losses) / len(losses)\n\n", "CodeBleu_score": 21.29327595994836}]}
{"instance_id": "565f8ed09916", "generation_result": [{"code": "def register_config(model_name: str) -> Coqpit\n\t\n\n    # construct class name\n    config_class_name = model_name.capitalize() + \"Config\"\n\n    # check if model is xtts\n    if model_name == \"xtts\":\n        # import xtts specific configs\n        from TTS.config.xtts_configs import *\n\n    # search for config class\n    try:\n        # find module\n        config_module = find_module(config_class_name)\n        # import config class\n        config_class = getattr(config_module, config_class_name)\n    except ModuleNotFoundError:\n        # raise error\n        raise ModuleNotFoundError(f\"Could not find config class '{config_class_name}' for model '{model_name}'\")\n\n    # return config class\n    return config_class\n\n", "CodeBleu_score": 32.67046838516973}, {"code": "def register_config(model_name: str) -> Coqpit\n\t\n    config_name = f\"{model_name.title()}Config\"\n    if model_name == \"xtts\":\n        try:\n            config_module = find_module(model_name, \"config\")\n            config_class = getattr(config_module, config_name)\n            return config_class\n        except ModuleNotFoundError:\n            raise ModuleNotFoundError(\n                f\"{model_name} configuration class not found in {model_name}/config.py\"\n            )\n    else:\n        try:\n            config_module = find_module(model_name, \"config\")\n            config_class = getattr(config_module, config_name)\n            return config_class\n        except ModuleNotFoundError:\n            try:\n                config_module = find_module(\"shared_configs\", \"TTS/config\")\n                config_class = getattr(config_module, config_name)\n                return config_class\n            except ModuleNotFoundError:\n                raise ModuleNotFoundError(\n                    f\"{model_name} configuration class not found in {model_name}/config.py\"\n                )\n\n", "CodeBleu_score": 34.82912595105262}, {"code": "def register_config(model_name: str) -> Coqpit\n\t\n    config_class_name = f\"{model_name.capitalize()}Config\"\n    try:\n        config_class = find_module(config_class_name, \"TTS.config.model_configs\")\n    except ModuleNotFoundError:\n        config_class = find_module(config_class_name, \"TTS.config.model_configs.xtts\")\n    return config_class\n\n", "CodeBleu_score": 21.47655131144221}]}
{"instance_id": "0ea7862506a7", "generation_result": [{"code": "def load_config(config_path: str) -> Coqpit\n\t\n    # load config\n    config_path = os.path.expanduser(config_path)\n    if config_path.endswith(\".yaml\") or config_path.endswith(\".yml\"):\n        with open(config_path, \"r\") as f:\n            config_dict = yaml.safe_load(f)\n    elif config_path.endswith(\".json\"):\n        with open(config_path, \"r\") as f:\n            config_dict = json.load(f)\n    else:\n        raise ValueError(f\"Unknown file type: {config_path}\")\n    # process config\n    model_name = config_dict.pop(\"model_name\")\n    config_class = find_module(model_name, \"TTS.config\")\n    config = config_class()\n    config.populate(config_dict)\n    return config\n\n", "CodeBleu_score": 39.5680929396478}, {"code": "def load_config(config_path: str) -> Coqpit\n\t\n    # Determine file type\n    if config_path.endswith(\".yaml\") or config_path.endswith(\".yml\"):\n        config_dict = yaml.load(open(config_path, \"r\"), Loader=yaml.FullLoader)\n    elif config_path.endswith(\".json\"):\n        config_dict = json.load(open(config_path, \"r\"))\n    else:\n        raise ValueError(\"Unknown file type of config file.\")\n\n    # Get model name\n    model_name = config_dict[\"model\"]\n    del config_dict[\"model\"]\n\n    # Register config class\n    config_class = find_module(model_name, \"configs\")\n    config_class.register_config()\n\n    # Create config object\n    config = config_class()\n\n    # Populate config object\n    config.populate(config_dict)\n\n    return config\n\n", "CodeBleu_score": 36.377586054286155}, {"code": "def load_config(config_path: str) -> Coqpit\n\t\n    # Load config file\n    if config_path.endswith(\".yaml\") or config_path.endswith(\".yml\"):\n        with open(config_path, \"r\") as f:\n            config = yaml.safe_load(f)\n    elif config_path.endswith(\".json\"):\n        with open(config_path, \"r\") as f:\n            config = json.load(f)\n    else:\n        raise ValueError(f\"Unknown file type for config file: {config_path}\")\n\n    # Load model name\n    if \"model_name\" not in config:\n        raise ValueError(f\"No model_name specified in config file: {config_path}\")\n    model_name = config[\"model_name\"]\n\n    # Register config class\n    config_class = find_module(model_name, \"configs\", \"Config\")\n    if config_class is None:\n        raise ValueError(f\"Config class for model {model_name} not found\")\n\n    # Create config instance\n    config = config_class()\n    config.update(config)\n    return config\n\n", "CodeBleu_score": 41.36843209808054}]}
{"instance_id": "e0db68b63143", "generation_result": [{"code": "def list_files(basePath, validExts=None, contains=None)\n\t\n    for root, dirs, files in os.walk(basePath):\n        for file in files:\n            if contains and contains not in file:\n                continue\n            if validExts and os.path.splitext(file)[1] not in validExts:\n                continue\n            yield os.path.join(root, file)\n\n", "CodeBleu_score": 29.09415646710923}, {"code": "def list_files(basePath, validExts=None, contains=None)\n\t\n    if validExts is None:\n        validExts = []\n    if contains is None:\n        contains = ''\n\n    if os.path.isfile(basePath):\n        if basePath.endswith(tuple(validExts)):\n            if contains in basePath:\n                yield basePath\n    elif os.path.isdir(basePath):\n        for path, subdirs, files in os.walk(basePath):\n            for name in files:\n                if name.endswith(tuple(validExts)):\n                    if contains in name:\n                        yield os.path.join(path, name)\n\n", "CodeBleu_score": 30.69508890532255}, {"code": "def list_files(basePath, validExts=None, contains=None)\n\t\n    if validExts is None:\n        validExts = []\n    if contains is None:\n        contains = []\n    for root, dirs, files in os.walk(basePath):\n        for f in files:\n            if not f.endswith(validExts):\n                continue\n            if contains:\n                if not contains in f:\n                    continue\n            yield os.path.join(root, f)\n\n", "CodeBleu_score": 33.22291567020356}]}
{"instance_id": "b836fcaf0a27", "generation_result": [{"code": "def load_model(xtts_checkpoint, xtts_config, xtts_vocab)\n\t\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n    if not xtts_checkpoint or not os.path.exists(xtts_checkpoint):\n        raise Exception(\"The specified checkpoint path does not exist.\")\n    if not xtts_config or not os.path.exists(xtts_config):\n        raise Exception(\"The specified config path does not exist.\")\n    if not xtts_vocab or not os.path.exists(xtts_vocab):\n        raise Exception(\"The specified vocab path does not exist.\")\n    logging.info(\"Loading configuration.\")\n    config = XttsConfig.from_json_file(xtts_config)\n    logging.info(\"Loading model.\")\n    model = Xtts(config)\n    logging.info(\"Loading checkpoint.\")\n    checkpoint = torch.load(xtts_checkpoint, map_location=\"cpu\")\n    model.load_state_dict(checkpoint[\"model\"])\n    model.eval()\n    logging.info(\"Loading vocabulary.\")\n    vocab = torch.load(xtts_vocab, map_location=\"cpu\")\n    logging.info(\"Loading complete.\")\n    if torch.cuda.is_available():\n        logging.info(\"Moving model to GPU.\")\n        model.cuda()\n    return model, config, vocab\n\n", "CodeBleu_score": 46.77061492360799}, {"code": "def load_model(xtts_checkpoint, xtts_config, xtts_vocab)\n\t\n    # Clear GPU cache\n    torch.cuda.empty_cache()\n\n    # Check if required paths are provided\n    if not xtts_checkpoint:\n        raise ValueError(\"Please provide a checkpoint path for XTTS\")\n    if not xtts_config:\n        raise ValueError(\"Please provide a config path for XTTS\")\n    if not xtts_vocab:\n        raise ValueError(\"Please provide a vocabulary path for XTTS\")\n\n    # Load config\n    config = XttsConfig(xtts_config)\n    config.freeze()\n\n    # Initialize XTTS model\n    model = Xtts(config)\n\n    # Load checkpoint\n    checkpoint = torch.load(xtts_checkpoint, map_location=torch.device(\"cpu\"))\n    model.load_state_dict(checkpoint[\"model\"])\n\n    # Load vocabulary\n    vocab = torch.load(xtts_vocab, map_location=torch.device(\"cpu\"))\n    model.vocab = vocab\n\n    # Move model to GPU if available\n    if torch.cuda.is_available():\n        model = model.cuda()\n\n    # Print success message\n    logging.info(\"XTTS model loaded\")\n\n    return model\n\n", "CodeBleu_score": 44.206420622289}, {"code": "def load_model(xtts_checkpoint, xtts_config, xtts_vocab)\n\t\n\n    # clear GPU cache\n    torch.cuda.empty_cache()\n\n    # check if required paths are provided\n    if xtts_checkpoint is None:\n        raise Exception(\"No checkpoint path provided.\")\n    if xtts_config is None:\n        raise Exception(\"No config path provided.\")\n    if xtts_vocab is None:\n        raise Exception(\"No vocab path provided.\")\n\n    # load config\n    config = XttsConfig(xtts_config)\n\n    # initialize model\n    model = Xtts(config)\n\n    # load checkpoint\n    try:\n        checkpoint = torch.load(xtts_checkpoint, map_location=\"cpu\")\n        model.load_state_dict(checkpoint[\"model\"])\n        print(f\"Model checkpoint loaded from {xtts_checkpoint}\")\n    except Exception as e:\n        print(f\"Unable to load checkpoint from {xtts_checkpoint}\")\n        print(traceback.format_exc())\n        raise e\n\n    # load vocab\n    try:\n        vocab = torch.load(xtts_vocab, map_location=\"cpu\")\n        model.load_vocab(vocab)\n        print(f\"Vocab loaded from {xtts_vocab}\")\n    except Exception as e:\n        print(f\"Unable to load vocab from {xtts_vocab}\")\n        print(traceback.format_exc())\n        raise e\n\n    # move model to GPU\n    if torch.cuda.is_available():\n        model.cuda()\n\n    return model\n\n", "CodeBleu_score": 40.66443341150988}]}
{"instance_id": "c5d63aa0df26", "generation_result": [{"code": "def compute_embedding(self, x, num_frames=250, num_eval=10, return_mean=True, l2_norm=True)\n\t\n\n    # get hop length and adjust number of frames\n    hop_length = self.hparams.hop_length\n    if hop_length is not None:\n        num_frames = int((x.shape[1] - self.hparams.n_fft) / hop_length) + 1\n\n    # check if number of frames exceeds input length\n    if num_frames > x.shape[1]:\n        num_frames = x.shape[1]\n\n    # calculate offsets\n    offsets = torch.arange(0, num_frames - num_eval, num_eval).long().to(x.device)\n\n    # extract frames and concatenate them into a batch\n    embeddings = []\n    for offset in offsets:\n        frames = x[:, offset : offset + num_eval]\n        frames = frames.unsqueeze(1)\n        embeddings.append(frames)\n    embeddings = torch.cat(embeddings, dim=1)\n\n    # perform inference and compute mean\n    embeddings = self.forward(embeddings)\n    embeddings = embeddings.mean(dim=1)\n\n    # normalize embeddings with L2 norm\n    if l2_norm:\n        embeddings = embeddings / torch.norm(embeddings, dim=1, keepdim=True)\n\n    # return mean embeddings\n    if return_mean:\n        return embeddings\n\n    # return all embeddings\n    else:\n        return embeddings\n\n", "CodeBleu_score": 44.95946496110515}, {"code": "def compute_embedding(self, x, num_frames=250, num_eval=10, return_mean=True, l2_norm=True)\n\t\n    # Adjust number of frames based on hop length\n    if self.hop_length is not None:\n        num_frames = int(num_frames / self.hop_length)\n\n    # Ensure that number of frames does not exceed input length\n    if x.shape[1] < num_frames:\n        num_frames = x.shape[1]\n\n    # Calculate offset positions for evaluation\n    offsets = torch.arange(0, x.shape[1] - num_frames + 1, num_frames)\n    if len(offsets) > num_eval:\n        offsets = torch.randperm(len(offsets))[:num_eval]\n\n    # Extract frames at these offsets, concatenate them into a batch, and perform inference\n    embeddings = []\n    for offset in offsets:\n        embeddings.append(self.forward(x[:, offset:offset + num_frames]))\n    embeddings = torch.cat(embeddings, dim=0)\n\n    # Compute mean of embeddings and return result, optionally normalizing them with L2 norm\n    if return_mean:\n        embeddings = embeddings.mean(dim=0)\n    if l2_norm:\n        embeddings = embeddings / torch.norm(embeddings)\n\n    return embeddings\n\n", "CodeBleu_score": 46.250485922083016}, {"code": "def compute_embedding(self, x, num_frames=250, num_eval=10, return_mean=True, l2_norm=True)\n\t\n    if self.use_torch_spec:\n        num_frames = int(num_frames * self.hop_length / self.sample_rate)\n        num_frames = min(num_frames, x.shape[-1] - num_frames)\n        num_eval = min(num_eval, num_frames - 1)\n        offsets = torch.arange(0, num_frames, num_frames / num_eval, dtype=torch.long)\n    else:\n        offsets = torch.arange(0, num_frames, num_frames / num_eval, dtype=torch.long)\n    x = x.unsqueeze(0)\n    embeddings = []\n    for offset in offsets:\n        x_frame = x[:, :, offset : offset + num_frames]\n        x_frame = x_frame.view(x_frame.shape[0], -1)\n        x_frame = self.model(x_frame)\n        embeddings.append(x_frame)\n    embeddings = torch.cat(embeddings, dim=0)\n    if return_mean:\n        embeddings = embeddings.mean(dim=0)\n    if l2_norm:\n        embeddings = torch.nn.functional.normalize(embeddings)\n    return embeddings\n\n", "CodeBleu_score": 42.71866144150346}]}
{"instance_id": "06f5b0335b26", "generation_result": [{"code": "def create_layer(self, block, planes, blocks, stride=1)\n\t\n    downsample = None\n    if stride != 1 or self.inplanes != planes * block.expansion:\n        downsample = nn.Sequential(\n            nn.Conv1d(self.inplanes, planes * block.expansion,\n                      kernel_size=1, stride=stride, bias=False),\n            nn.BatchNorm1d(planes * block.expansion))\n\n    layers = []\n    layers.append(block(self.inplanes, planes, stride, downsample))\n    self.inplanes = planes * block.expansion\n    for i in range(1, blocks):\n        layers.append(block(self.inplanes, planes))\n\n    return nn.Sequential(*layers)\n\n", "CodeBleu_score": 82.00967983613901}, {"code": "def create_layer(self, block, planes, blocks, stride=1)\n\t\n    downsample = None\n    if stride != 1 or self.inplanes != planes * block.expansion:\n        downsample = nn.Sequential(\n            nn.Conv1d(self.inplanes, planes * block.expansion,\n                      kernel_size=1, stride=stride, bias=False),\n            nn.BatchNorm1d(planes * block.expansion),\n        )\n\n    layers = []\n    layers.append(block(self.inplanes, planes, stride, downsample))\n    self.inplanes = planes * block.expansion\n    for i in range(1, blocks):\n        layers.append(block(self.inplanes, planes))\n\n    return nn.Sequential(*layers)\n\n", "CodeBleu_score": 82.8588786843097}, {"code": "def create_layer(self, block, planes, blocks, stride=1)\n\t\n    downsample = None\n    if stride != 1 or self.inplanes != planes * block.expansion:\n        downsample = nn.Sequential(\n            nn.Conv1d(self.inplanes, planes * block.expansion,\n                      kernel_size=1, stride=stride, bias=False),\n            nn.BatchNorm1d(planes * block.expansion),\n        )\n\n    layers = []\n    layers.append(block(self.inplanes, planes, stride, downsample))\n    self.inplanes = planes * block.expansion\n    for i in range(1, blocks):\n        layers.append(block(self.inplanes, planes))\n\n    return nn.Sequential(*layers)\n\n", "CodeBleu_score": 82.8588786843097}]}
{"instance_id": "9ac60597b633", "generation_result": [{"code": "def plot_embeddings(embeddings, num_classes_in_batch)\n\t\n    num_classes = num_classes_in_batch\n    if num_classes > 10:\n        num_classes = 10\n    num_utterances_per_class = num_classes_in_batch // num_classes\n    embeddings_per_class = []\n    for i in range(num_classes):\n        embeddings_per_class.append(embeddings[i * num_utterances_per_class: (i + 1) * num_utterances_per_class])\n    embeddings_per_class = np.concatenate(embeddings_per_class, axis=0)\n    umap_embeddings = umap.UMAP().fit_transform(embeddings_per_class)\n    fig = plt.figure(figsize=(10, 10))\n    plt.scatter(umap_embeddings[:, 0], umap_embeddings[:, 1], c=np.arange(num_classes))\n    plt.title(\"UMAP projection\", fontsize=16)\n    plt.axis(\"equal\")\n    plt.savefig(\"umap.png\")\n    plt.close(fig)\n    return fig\n\n", "CodeBleu_score": 39.345125969980636}, {"code": "def plot_embeddings(embeddings, num_classes_in_batch)\n\t\n    import matplotlib.pyplot as plt\n    import matplotlib.cm as cm\n    import numpy as np\n\n    # Calculate number of utterances per class\n    num_classes = num_classes_in_batch\n    num_utts = np.zeros(num_classes)\n    for i in range(num_classes):\n        num_utts[i] = len(embeddings[i])\n\n    # Limit number of classes to 10 if necessary\n    if num_classes > 10:\n        num_classes = 10\n        num_utts = np.zeros(num_classes)\n        for i in range(num_classes):\n            num_utts[i] = np.sum(embeddings[i])\n        num_utts = num_utts.argsort()[::-1]\n        embeddings = embeddings[num_utts]\n\n    # UMAP transforms the embeddings\n    reducer = umap.UMAP(n_neighbors=15, min_dist=0.3, metric='correlation')\n    embedding_2d = reducer.fit_transform(embeddings)\n\n    # A scatter plot is created with colors representing different classes\n    plt.figure(figsize=(15, 15))\n    plt.scatter(embedding_2d[:, 0], embedding_2d[:, 1], c=np.arange(num_classes), cmap=cm.get_cmap('tab20', num_classes))\n    plt.title('UMAP projection', y=1.08)\n    plt.axis('equal')\n    plt.savefig('umap.png')\n    plt.show()\n\n    return plt.gcf()\n\n", "CodeBleu_score": 39.28916171926014}, {"code": "def plot_embeddings(embeddings, num_classes_in_batch)\n\t\n\n    # Calculate the number of utterances per class\n    num_utterances_per_class = [0] * num_classes_in_batch\n    for i in range(num_classes_in_batch):\n        num_utterances_per_class[i] = embeddings[i].shape[0]\n\n    # Limit the number of classes to 10 if necessary\n    if num_classes_in_batch > 10:\n        num_classes_in_batch = 10\n\n    # UMAP transforms the embeddings\n    reducer = umap.UMAP(n_neighbors=10, min_dist=0.3, n_components=2, metric='cosine')\n    embeddings_2d = reducer.fit_transform(embeddings)\n\n    # A scatter plot is created with colors representing different classes\n    matplotlib.use('Agg')\n    import matplotlib.pyplot as plt\n    fig = plt.figure(figsize=(10, 10))\n    plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], c=range(num_classes_in_batch), cmap='rainbow')\n    plt.title('UMAP projection', fontsize=20)\n    plt.axis('equal')\n    plt.axis('off')\n    plt.savefig('umap.png', bbox_inches='tight', pad_inches=0)\n    plt.close(fig)\n\n    return fig\n\n", "CodeBleu_score": 43.159799665291196}]}
{"instance_id": "19c2b4af256d", "generation_result": [{"code": "def calc_cosine_sim(self, dvecs, centroids)\n\t\n    cos_sim = []\n    for i, dvec in enumerate(dvecs):\n        dvec = dvec.unsqueeze(0)\n        centroids_exclude = centroids.clone()\n        centroids_exclude[i] = 0\n        cos_sim.append(\n            torch.clamp(\n                torch.mm(dvec, centroids_exclude.transpose(0, 1)),\n                min=1e-6,\n            )\n        )\n    cos_sim = torch.cat(cos_sim, dim=0)\n    return cos_sim\n\n", "CodeBleu_score": 29.22623143059685}, {"code": "def calc_cosine_sim(self, dvecs, centroids)\n\t\n    cosine_sim = []\n    for i in range(len(centroids)):\n        new_centroids = centroids.clone()\n        new_centroids[i, :] = 0\n        new_centroids = new_centroids / new_centroids.norm(dim=1, keepdim=True)\n        cosine_sim.append(torch.clamp(torch.mm(dvecs[i, :], new_centroids.t()), min=1e-10))\n    cosine_sim = torch.cat(cosine_sim, dim=0)\n    return cosine_sim\n\n", "CodeBleu_score": 23.422825272169824}, {"code": "def calc_cosine_sim(self, dvecs, centroids)\n\t\n\n    # Compute cosine similarity matrix\n    cosine_sim = []\n    for i in range(len(dvecs)):\n        # Get the centroids for the current speaker\n        centroids_i = centroids[i]\n\n        # Get the d-vectors for the current speaker\n        dvecs_i = dvecs[i]\n\n        # Compute the cosine similarity for each utterance of the current speaker\n        cosine_sim_i = []\n        for j in range(len(dvecs_i)):\n            # Get the d-vector for the current utterance\n            dvec_j = dvecs_i[j]\n\n            # Compute the cosine similarity with the centroids\n            # Exclude the current utterance\n            cosine_sim_j = torch.cosine_similarity(dvec_j, centroids_i, dim=1)\n\n            # Clamp the cosine similarity to avoid small values\n            cosine_sim_j = torch.clamp(cosine_sim_j, min=self.min_cosine_sim)\n\n            # Append the cosine similarity to the list\n            cosine_sim_i.append(cosine_sim_j)\n\n        # Concatenate the cosine similarity for each utterance of the current speaker\n        cosine_sim_i = torch.cat(cosine_sim_i, dim=0)\n\n        # Append the cosine similarity for the current speaker\n        cosine_sim.append(cosine_sim_i)\n\n    # Concatenate the cosine similarity for each speaker\n    cosine_sim = torch.cat(cosine_sim, dim=0)\n\n    # Stack the cosine similarity matrix\n    cosine_sim = torch.stack(cosine_sim, dim=0)\n\n    return cosine_sim\n\n", "CodeBleu_score": 32.56563590694323}]}
{"instance_id": "c525b411a1e7", "generation_result": [{"code": " def embed_loss_softmax(self, dvecs, cos_sim_matrix)\n\t\n    embed_loss = []\n    for i in range(dvecs.size(0)):\n        embed_loss.append(torch.log(1 - cos_sim_matrix[i, :]))\n    embed_loss = torch.stack(embed_loss)\n    return embed_loss\n\n\n def embed_loss_triplet(self, dvecs, cos_sim_matrix):\n    \"\"\"The function calculates the embedding loss using triplet loss.", "CodeBleu_score": 28.803696792987243}, {"code": " def embed_loss_softmax(self, dvecs, cos_sim_matrix)\n\t\n    loss = 0\n    for i in range(dvecs.shape[0]):\n        loss += -torch.log(cos_sim_matrix[i, i])\n    return loss\n\n\n def embed_loss_triplet(self, dvecs, cos_sim_matrix):\n    \"\"\"The function calculates the embedding loss using triplet loss.", "CodeBleu_score": 20.811780177825824}, {"code": " def embed_loss_softmax(self, dvecs, cos_sim_matrix)\n\t\n    losses = []\n    for dvec in dvecs:\n        dvec = dvec.unsqueeze(0)\n        cos_sim_matrix = cos_sim_matrix.unsqueeze(0)\n        loss = nn.LogSoftmax(dim=1)(cos_sim_matrix)\n        loss = -loss[:, 0]\n        losses.append(loss)\n    return torch.cat(losses, dim=0)\n\n", "CodeBleu_score": 30.29204439900302}]}
{"instance_id": "25ac3945da2e", "generation_result": [{"code": "def embed_loss_contrast(self, dvecs, cos_sim_matrix)\n\t\n    # get the centroids\n    centroids = self.centroids.weight\n    # get the batch size\n    batch_size = dvecs.shape[0]\n    # initialize the loss tensor\n    loss = torch.zeros(batch_size, device=self.device)\n    # iterate over the embeddings\n    for i in range(batch_size):\n        # get the embedding\n        dvec = dvecs[i]\n        # get the cosine similarity matrix for the embedding\n        cos_sim = cos_sim_matrix[i]\n        # get the closest centroid\n        closest_centroid = centroids[cos_sim.argmax()]\n        # get the cosine similarity of the embedding and the closest centroid\n        cos_sim_closest_centroid = cos_sim[cos_sim.argmax()]\n        # get the contrast loss\n        loss[i] = torch.log(1 + torch.exp(cos_sim_closest_centroid - cos_sim_matrix[i][cos_sim.argmax()]))\n    # return the loss\n    return loss\n\n", "CodeBleu_score": 23.257620874709545}, {"code": "def embed_loss_contrast(self, dvecs, cos_sim_matrix)\n\t\n    self.loss = torch.tensor([], device=self.device)\n    for i in range(len(dvecs)):\n        dvec = dvecs[i]\n        dvec = dvec.unsqueeze(0)\n        cos_sim_matrix = cos_sim_matrix.unsqueeze(0)\n        cos_sim_matrix = cos_sim_matrix.expand(dvec.shape[0], cos_sim_matrix.shape[1], cos_sim_matrix.shape[2])\n        dvec = dvec.expand(cos_sim_matrix.shape[0], cos_sim_matrix.shape[1], cos_sim_matrix.shape[2])\n        cos_sim_matrix = cos_sim_matrix.permute(0, 2, 1)\n        cos_sim_matrix = cos_sim_matrix.contiguous()\n        cos_sim_matrix = cos_sim_matrix.view(-1, cos_sim_matrix.shape[-1])\n        dvec = dvec.permute(0, 2, 1)\n        dvec = dvec.contiguous()\n        dvec = dvec.view(-1, dvec.shape[-1])\n        cos_sim_matrix = cos_sim_matrix.unsqueeze(1)\n        dvec = dvec.unsqueeze(1)\n        cos_sim_matrix = cos_sim_matrix.expand(cos_sim_matrix.shape[0], cos_sim_matrix.shape[1], dvec.shape[2], dvec.shape[3])\n        dvec = dvec.expand(dvec.shape[0], dvec.shape[1], cos_sim_matrix.shape[2], cos_sim_matrix.shape[3])\n        cos_sim_matrix = cos_sim_matrix.permute(0, 1, 3, 2)\n        cos_sim_matrix = cos_sim_matrix.contiguous()\n        cos_sim_matrix = cos_sim_matrix.view(-1, cos_sim_matrix.shape[-1])\n        dvec = dvec.permute(0, 1, 3, 2)\n        dvec", "CodeBleu_score": 13.516217656113133}, {"code": "def embed_loss_contrast(self, dvecs, cos_sim_matrix)\n\t\n    loss_row = []\n    for i, dvec in enumerate(dvecs):\n        cos_sim = cos_sim_matrix[i]\n        cos_sim = cos_sim[i+1:]\n        cos_sim = torch.sigmoid(cos_sim)\n        loss = 1 - cos_sim\n        loss = loss.sum()\n        loss_row.append(loss)\n    loss_row = torch.stack(loss_row)\n    return loss_row\n\n", "CodeBleu_score": 25.740783493722834}]}
{"instance_id": "878d14ffabd2", "generation_result": [{"code": "def forward(self, x, _label=None)\n\t\n    if self.embedding_loss_weight > 0:\n        # x is of shape (num_speakers, num_utts_per_speaker, dvec_feats)\n        # reshape to (num_speakers * num_utts_per_speaker, dvec_feats)\n        x = x.view(x.size(0) * x.size(1), x.size(2))\n        # get centroids by averaging embeddings\n        centroids = torch.mean(x, dim=0, keepdim=True)\n        # reshape to (num_speakers, num_utts_per_speaker, dvec_feats)\n        x = x.view(x.size(0) // x.size(1), x.size(1), x.size(2))\n        # get cosine similarities between embeddings and centroids\n        sim_matrix = torch.nn.functional.cosine_similarity(x, centroids, dim=2)\n        # scale and shift the similarities\n        sim_matrix = (sim_matrix - self.shift) / self.scale\n        # compute embedding loss\n        embedding_loss = torch.mean(torch.log(1 + torch.exp(-sim_matrix)))\n        # return the mean loss\n        return embedding_loss\n    else:\n        return 0\n\n", "CodeBleu_score": 33.65412636510898}, {"code": "def forward(self, x, _label=None)\n\t\n    # Check input shape\n    if x.dim() != 3:\n        raise ValueError(\"Input tensor should have 3 dimensions.\")\n    num_speakers, num_utts_per_speaker, dvec_feats = x.size()\n    if num_utts_per_speaker < 2:\n        raise ValueError(\"Input tensor should have at least 2 utterances per speaker.\")\n\n    # Calculate centroids\n    centroids = x.mean(dim=1)\n    # Calculate cosine similarities\n    sim_matrix = nn.functional.cosine_similarity(x, centroids.unsqueeze(1), dim=2)\n    # Scale and shift the cosine similarities\n    sim_matrix = sim_matrix * 0.5 + 0.5\n    # Calculate embedding loss\n    loss = torch.mean(1 - sim_matrix)\n    return loss\n\n", "CodeBleu_score": 32.25475161714645}, {"code": "def forward(self, x, _label=None)\n\t\n    # check input\n    if len(x.shape) != 3:\n        raise ValueError(\"Input must have 3 dimensions (num_speakers, num_utts_per_speaker, dvec_feats)\")\n\n    # get number of speakers and utterances per speaker\n    num_speakers, num_utts_per_speaker, dvec_feats = x.shape\n\n    # check if there are at least two utterances per speaker\n    if num_utts_per_speaker < 2:\n        raise ValueError(\"Input must have at least two utterances per speaker\")\n\n    # compute centroids by averaging the embeddings\n    centroids = torch.mean(x, dim=1)\n\n    # compute cosine similarities between embeddings and centroids\n    similarities = nn.functional.cosine_similarity(x, centroids.unsqueeze(1), dim=-1)\n\n    # scale and shift similarities\n    similarities = similarities - similarities.mean() + 1\n\n    # compute embedding loss\n    loss = torch.mean(similarities)\n\n    return loss\n\n", "CodeBleu_score": 33.08327509133743}]}
{"instance_id": "86a364c72caa", "generation_result": [{"code": "def get_voices(extra_voice_dirs: List[str] = [])\n\t\n    voices = {}\n    voice_dirs = [os.path.join(os.path.dirname(__file__), 'voices'), *extra_voice_dirs]\n    for voice_dir in voice_dirs:\n        for subdir in os.listdir(voice_dir):\n            subdir_path = os.path.join(voice_dir, subdir)\n            if os.path.isdir(subdir_path):\n                voices[subdir] = []\n                for file in os.listdir(subdir_path):\n                    file_path = os.path.join(subdir_path, file)\n                    if file.endswith('.npz'):\n                        voices[subdir].append(file_path)\n                    elif file.endswith(('.wav', '.mp3')):\n                        voices[subdir].append(file_path)\n                    elif os.path.isdir(file_path):\n                        for file_ in os.listdir(file_path):\n                            file_path_ = os.path.join(file_path, file_)\n                            if file_.endswith('.npz'):\n                                voices[subdir].append(file_path_)\n                            elif file_.endswith(('.wav', '.mp3')):\n                                voices[subdir].append(file_path_)\n    return voices\n\n", "CodeBleu_score": 24.00788617388413}, {"code": "def get_voices(extra_voice_dirs: List[str] = [])\n\t\n    voices = {}\n    for voice_dir in [os.path.join(os.path.dirname(__file__), \"voices\")] + extra_voice_dirs:\n        for dirpath, dirnames, filenames in os.walk(voice_dir):\n            for filename in filenames:\n                if filename.endswith(\".npz\"):\n                    voices.setdefault(dirpath.split(\"/\")[-1], []).append(os.path.join(dirpath, filename))\n    for dirpath, dirnames, filenames in os.walk(voice_dir):\n        for filename in filenames:\n            if filename.endswith(\".wav\") or filename.endswith(\".mp3\"):\n                voices.setdefault(dirpath.split(\"/\")[-1], []).append(os.path.join(dirpath, filename))\n    return voices\n\n", "CodeBleu_score": 27.096867782355304}, {"code": "def get_voices(extra_voice_dirs: List[str] = [])\n\t\n\n    voices = {}\n    for voice_dir in [os.path.join(os.path.dirname(os.path.abspath(__file__)), \"voices\")] + extra_voice_dirs:\n        for root, dirs, files in os.walk(voice_dir):\n            for file in files:\n                if file.endswith(\".npz\"):\n                    if root not in voices:\n                        voices[root] = []\n                    voices[root].append(os.path.join(root, file))\n                elif file.endswith(\".wav\") or file.endswith(\".mp3\"):\n                    if root not in voices:\n                        voices[root] = []\n                    voices[root].append(os.path.join(root, file))\n\n    return voices\n\n", "CodeBleu_score": 27.963509580428394}]}
{"instance_id": "02f20c8917e6", "generation_result": [{"code": "def load_voice(model, voice: str, extra_voice_dirs: List[str] = [])\n\t\n    voice_dirs = [os.path.join(os.path.dirname(__file__), \"voices\")] + extra_voice_dirs\n    voice_dirs = [d for d in voice_dirs if os.path.isdir(d)]\n    voice_paths = get_voices(voice, voice_dirs)\n    if len(voice_paths) > 1:\n        raise Exception(f\"Multiple voices found for {voice}: {voice_paths}\")\n    voice_path = voice_paths[0]\n    if voice_path is None:\n        return None, None, None, None\n    if os.path.isfile(voice_path) and voice_path.endswith(\".npz\"):\n        voice_data = np.load(voice_path)\n        return voice_data[\"audio\"], voice_data[\"mel\"], voice_data[\"mel_postnet\"], voice_data[\"duration_control\"]\n    if os.path.isfile(voice_path):\n        voice_data = convert_audio(voice_path)\n        return voice_data[\"audio\"], voice_data[\"mel\"], voice_data[\"mel_postnet\"], voice_data[\"duration_control\"]\n    if os.path.isdir(voice_path):\n        voice_data = convert_audio(voice_path, model.hparams.sample_rate)\n        return voice_data[\"audio\"], voice_data[\"mel\"], voice_data[\"mel_postnet\"], voice_data[\"duration_control\"]\n    return None, None, None, None\n\n", "CodeBleu_score": 39.498308923212164}, {"code": "def load_voice(model, voice: str, extra_voice_dirs: List[str] = [])\n\t\n    voice_dirs = [model.voice_dir] + extra_voice_dirs\n    voice_paths = get_voices(voice_dirs, voice)\n    if len(voice_paths) > 1:\n        raise ValueError(f\"Multiple voices found for {voice} in {voice_dirs}\")\n    if len(voice_paths) == 0:\n        raise ValueError(f\"No voice found for {voice} in {voice_dirs}\")\n    voice_path = voice_paths[0]\n    if voice_path.endswith(\".npz\"):\n        voice_data = torch.load(voice_path)\n    else:\n        voice_data = generate_voice(model, voice_path)\n    return voice_data\n\n", "CodeBleu_score": 38.506064145724636}, {"code": "def load_voice(model, voice: str, extra_voice_dirs: List[str] = [])\n\t\n    voice_dirs = get_voices(model, extra_voice_dirs)\n    voice_path = None\n    for voice_dir in voice_dirs:\n        if voice == \"random\":\n            voice_path = None\n            break\n        voice_path = os.path.join(voice_dir, voice + \".npz\")\n        if os.path.exists(voice_path):\n            break\n        voice_path = None\n    if voice_path is None:\n        return None\n    if os.path.splitext(voice_path)[1] == \".npz\":\n        voice_data = np.load(voice_path)\n        return voice_data[\"wav\"], voice_data[\"mel\"], voice_data[\"pitch\"], voice_data[\"energy\"]\n    wav, mel, pitch, energy = generate_voice(model, voice_path, voice)\n    return wav, mel, pitch, energy\n\n", "CodeBleu_score": 38.268596607616956}]}
{"instance_id": "9458e981389f", "generation_result": [{"code": "def _relative_shift(self, pos_score: torch.Tensor) -> torch.Tensor\n\t\n    pos_dim = pos_score.size()\n    zero_tensor = torch.zeros(\n        (pos_dim[0], pos_dim[1], 1), dtype=torch.float32, device=pos_score.device\n    )\n    pos_score = torch.cat((zero_tensor, pos_score), dim=2)\n    pos_score = pos_score.view(pos_dim[0], pos_dim[1], pos_dim[2] + 1)\n    pos_score = pos_score[:, :, 1:]\n    return pos_score\n\n", "CodeBleu_score": 24.951799996709994}, {"code": "def _relative_shift(self, pos_score: torch.Tensor) -> torch.Tensor\n\t\n    pos_score_dim = pos_score.shape\n    pos_score_zero = torch.zeros(\n        (pos_score_dim[0], pos_score_dim[1], 1),\n        dtype=pos_score.dtype,\n        device=pos_score.device,\n    )\n    pos_score_zero = pos_score_zero.expand(pos_score_dim[0], pos_score_dim[1], pos_score_dim[2] + 1)\n    pos_score_zero[:, :, 0] = pos_score[:, :, 0]\n    pos_score_zero = pos_score_zero.reshape(\n        (pos_score_dim[0], pos_score_dim[1], pos_score_dim[2] + 1)\n    )\n    pos_score_zero = pos_score_zero[:, :, 1:]\n    return pos_score_zero\n\n", "CodeBleu_score": 28.869278676650044}, {"code": "def _relative_shift(self, pos_score: torch.Tensor) -> torch.Tensor\n\t\n    pos_score_dims = pos_score.size()\n    zero_tensor = torch.zeros(\n        (pos_score_dims[0], pos_score_dims[1], 1), device=pos_score.device\n    )\n    pos_score_padded = torch.cat(\n        (zero_tensor, pos_score), dim=2\n    )  # (batch_size, n_mel_channels, n_frames+1)\n    pos_score_padded = pos_score_padded.view(\n        pos_score_dims[0], pos_score_dims[1], pos_score_dims[2] + 1, 1\n    )\n    pos_score = pos_score_padded[:, :, 1:].view(pos_score_dims)\n    return pos_score\n\n", "CodeBleu_score": 27.184552629665422}]}
{"instance_id": "c046d4f00e80", "generation_result": [{"code": "def run_padded_sequence(self, context, lens)\n\t\n    # Get the batch size\n    batch_size = context.size(0)\n    # Create the output tensor\n    out = torch.zeros(batch_size, self.out_channels, context.size(1), device=context.device)\n    # Iterate over the batch elements\n    for i in range(batch_size):\n        # Get the length of the current sequence\n        seq_len = lens[i]\n        # Get the current sequence\n        seq = context[i][:seq_len]\n        # Apply the convolutional layers\n        for layer in self.layers:\n            seq = layer(seq)\n        # Store the result\n        out[i] = seq\n    # Return the padded sequence\n    return out\n\n", "CodeBleu_score": 21.196093748914436}, {"code": "def run_padded_sequence(self, context, lens)\n\t\n    # Extract the context up to the specified length\n    context = context[:, :lens.max()]\n    # Apply the convolutional layers\n    context = self.conv1(context)\n    context = self.conv2(context)\n    context = self.conv3(context)\n    context = self.conv4(context)\n    context = self.conv5(context)\n    # Pad the sequence to create a consistent batch size\n    context = parametrize.pad_sequence(context, batch_first=True)\n    return context\n\n", "CodeBleu_score": 23.73635474288534}, {"code": "def run_padded_sequence(self, context, lens)\n\t\n\n    def _pad_to_batch(tensors: Tuple[torch.Tensor], batch_size: int):\n        \"\"\"Pads the tensors to the specified batch size.\"\"\"\n        pad_len = batch_size - tensors[0].size(0)\n        if pad_len > 0:\n            tensors = tuple(\n                torch.cat([tensor, tensor.new_full((pad_len,) + tensor.size()[1:], 0)])\n                for tensor in tensors\n            )\n        return tensors\n\n    def _process_context(context: torch.Tensor, lens: torch.Tensor):\n        \"\"\"Processes the context with the convolutional layers.\"\"\"\n        lens = lens.to(torch.int64)\n        context = context[:, :lens.max()]\n        context = self.conv1(context)\n        context = self.conv2(context)\n        context = self.conv3(context)\n        context = self.conv4(context)\n        context = self.conv5(context)\n        return context\n\n    def _process_contexts(contexts: torch.Tensor, lens: torch.Tensor):\n        \"\"\"Processes the contexts with the convolutional layers.\"\"\"\n        contexts = contexts.unfold(1, self.context_len, self.context_len)\n        contexts = contexts.permute(0, 2, 1, 3)\n        contexts = contexts.contiguous().view(\n            contexts.size(0), contexts.size(1), -1\n        )\n        contexts = _process_context(contexts, lens)\n        return contexts\n\n    def _process_context_pairs(contexts: torch.Tensor, lens: torch.Tensor):\n        \"\"\"Processes the context pairs with the convolutional layers.\"\"\"\n        contexts = contexts.unfold(1, self.context_len, self.context_len)\n        contexts = contexts.permute(0, 2, 1, 3)\n        contexts = contexts.contiguous().view(\n", "CodeBleu_score": 27.9046325498012}]}
{"instance_id": "db2644f3c681", "generation_result": [{"code": "def run_unsorted_inputs(self, fn, context, lens)\n\t\n    # Sort the inputs based on the length of the sequences\n    lens, indices = lens.sort(0, descending=True)\n    context = context[indices]\n    # Pack the context sequences\n    context = context.transpose(0, 1).contiguous()\n    context = context.view(context.size(0), -1)\n    # Apply the function\n    output = fn(context)\n    # Unpack the sequences\n    output = output.view(lens.size(0), lens.max().item(), -1)\n    output = output.transpose(0, 1).contiguous()\n    # Map the results back to the original indices\n    output = output[indices]\n    return output\n\n", "CodeBleu_score": 12.630477240432894}, {"code": "def run_unsorted_inputs(self, fn, context, lens)\n\t\n    sorted_lens, sorted_idx = torch.sort(lens, descending=True)\n    sorted_context = context[sorted_idx]\n    packed_context = torch.nn.utils.rnn.pack_padded_sequence(sorted_context, sorted_lens.tolist(), batch_first=True)\n    packed_output, _ = fn(packed_context)\n    output, _ = torch.nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n    _, unsorted_idx = torch.sort(sorted_idx, descending=False)\n    return output[unsorted_idx]\n\n", "CodeBleu_score": 17.05158711627007}, {"code": "def run_unsorted_inputs(self, fn, context, lens)\n\t\n    sorted_lens, sorted_indices = torch.sort(lens, descending=True)\n    sorted_context = context[sorted_indices]\n    packed_context = torch.nn.utils.rnn.pack_padded_sequence(sorted_context, sorted_lens.tolist(), batch_first=True)\n    packed_outputs, _ = fn(packed_context)\n    outputs, _ = torch.nn.utils.rnn.pad_packed_sequence(packed_outputs, batch_first=True)\n    outputs = outputs[torch.arange(lens.size(0)).long(), sorted_indices]\n    return outputs\n\n", "CodeBleu_score": 20.338114687531338}]}
{"instance_id": "ced082bffc63", "generation_result": [{"code": "def remove_weight_norm(self)\n\t\n    self.kernel_predictor.remove_weight_norm()\n    for name, param in self.named_parameters():\n        if name == \"convt_pre.weight\":\n            del self._parameters[name]\n            self.register_parameter(name, param)\n        if \"conv_blocks\" in name:\n            for block_name, block_param in self.conv_blocks.named_parameters():\n                if block_name == name:\n                    del self.conv_blocks._parameters[block_name]\n                    self.conv_blocks.register_parameter(block_name, block_param)\n\n", "CodeBleu_score": 41.73852898986634}, {"code": "def remove_weight_norm(self)\n\t\n    self.kernel_predictor.remove_weight_norm()\n    self.convt_pre.remove_weight_norm()\n    for conv_block in self.conv_blocks:\n        conv_block.remove_weight_norm()\n\n", "CodeBleu_score": 27.580633709689707}, {"code": "def remove_weight_norm(self)\n\t\n    self.kernel_predictor.remove_weight_norm()\n    self.convt_pre.remove_weight_norm()\n    for conv_block in self.conv_blocks:\n        conv_block.remove_weight_norm()\n\n", "CodeBleu_score": 27.580633709689707}]}
{"instance_id": "b5a39fd97061", "generation_result": [{"code": "def map_mx_to_torch_model(mx_name)\n\t\n    mx_name = mx_name.lower()\n    if mx_name.startswith('se_'):\n        mx_name = 'se_' + mx_name[3:]\n    elif mx_name.startswith('senet_'):\n        mx_name = 'senet_' + mx_name[6:]\n    elif mx_name == 'inceptionv3':\n        mx_name = 'inception_v3'\n    return 'gluon_' + mx_name\n\n", "CodeBleu_score": 33.91410226834835}, {"code": "def map_mx_to_torch_model(mx_name)\n\t\n    mx_name = mx_name.lower()\n    mx_name = mx_name.replace('se_', 'se_preresnet')\n    mx_name = mx_name.replace('senet_', 'senet_154')\n    mx_name = mx_name.replace('inceptionv3', 'inception_v3')\n    return 'gluon_' + mx_name\n\n", "CodeBleu_score": 32.03386367555648}, {"code": "def map_mx_to_torch_model(mx_name)\n\t\n    if mx_name.startswith('se_'):\n        mx_name = mx_name.replace('se_', 'se_resnext')\n    elif mx_name.startswith('senet_'):\n        mx_name = mx_name.replace('senet_', 'se_resnet')\n    elif mx_name.startswith('inceptionv3'):\n        mx_name = mx_name.replace('inceptionv3', 'inception_v3')\n    return 'gluon_' + mx_name\n\n", "CodeBleu_score": 44.862636813704185}]}
{"instance_id": "7eee5efdbf67", "generation_result": [{"code": "def _num_samples_per_worker(self)\n\t\n    if self.num_workers > 0:\n        num_samples = self.num_samples // self.num_workers\n        if self.num_replicas > 1:\n            num_samples = math.ceil(num_samples / self.num_replicas)\n        if self.batch_size > 1:\n            num_samples = math.ceil(num_samples / self.batch_size) * self.batch_size\n        return num_samples\n    else:\n        return self.num_samples\n\n", "CodeBleu_score": 39.716913907691705}, {"code": "def _num_samples_per_worker(self)\n\t\n    if self.num_workers is None:\n        return None\n    if self.num_workers == 0:\n        return None\n\n    num_samples_per_worker = int(math.ceil(self.num_samples / self.num_workers))\n    if self.repeats is not None:\n        num_samples_per_worker = int(math.ceil(num_samples_per_worker / self.repeats))\n    if self.batch_size is not None:\n        num_samples_per_worker = int(math.ceil(num_samples_per_worker / self.batch_size)) * self.batch_size\n    return num_samples_per_worker\n\n", "CodeBleu_score": 43.09932304991215}, {"code": "def _num_samples_per_worker(self)\n\t\n    if self.num_samples is not None:\n        return self.num_samples\n\n    if self.num_workers is None:\n        return 1\n\n    if self.repeats is None:\n        num_samples = self.num_samples_per_worker\n    else:\n        num_samples = self.num_samples_per_worker * self.repeats\n\n    if self.num_workers > 1:\n        num_samples = math.ceil(num_samples / self.num_workers) * self.num_workers\n\n    if self.batch_size is not None:\n        num_samples = math.ceil(num_samples / self.batch_size) * self.batch_size\n\n    return num_samples\n\n", "CodeBleu_score": 41.60741319700179}]}
{"instance_id": "a698d65ae190", "generation_result": [{"code": "def find_images_and_targets(\n        folder: str,\n        types: Optional[Union[List, Tuple, Set]] = None,\n        class_to_idx: Optional[Dict] = None,\n        leaf_name_only: bool = True,\n        sort: bool = True\n)\n\t\n\n    if types is None:\n        types = ('.jpg', '.jpeg', '.png')\n\n    if class_to_idx is None:\n        class_to_idx = {}\n\n    images_and_targets = []\n    for target in sorted(os.listdir(folder)):\n        d = os.path.join(folder, target)\n        if os.path.isdir(d):\n            if leaf_name_only:\n                images = [os.path.join(d, f) for f in os.listdir(d) if os.path.splitext(f)[1].lower() in types]\n            else:\n                images = [os.path.join(d, f) for f in os.listdir(d)]\n\n            if images:\n                class_to_idx[target] = len(class_to_idx)\n                for fname in images:\n                    item = (fname, class_to_idx[target])\n                    images_and_targets.append(item)\n\n    if sort:\n        images_and_targets.sort(key=lambda k: natural_key(k[0]))\n\n    return images_and_targets, class_to_idx\n\n", "CodeBleu_score": 36.82134154388221}, {"code": "def find_images_and_targets(\n        folder: str,\n        types: Optional[Union[List, Tuple, Set]] = None,\n        class_to_idx: Optional[Dict] = None,\n        leaf_name_only: bool = True,\n        sort: bool = True\n)\n\t\n    if not os.path.isdir(folder):\n        raise ValueError(f'{folder} is not a valid directory')\n    if not types:\n        types = ('.jpg', '.jpeg', '.png', '.ppm', '.bmp', '.pgm', '.tif', '.tiff', '.webp')\n    if not isinstance(types, (list, tuple, set)):\n        raise ValueError(f'{types} is not a valid type')\n    if not class_to_idx:\n        class_to_idx = {}\n    if not isinstance(class_to_idx, dict):\n        raise ValueError(f'{class_to_idx} is not a valid type')\n    if not isinstance(leaf_name_only, bool):\n        raise ValueError(f'{leaf_name_only} is not a valid type')\n    if not isinstance(sort, bool):\n        raise ValueError(f'{sort} is not a valid type')\n\n    def get_filename(filepath):\n        if leaf_name_only:\n            return os.path.basename(filepath)\n        return filepath\n\n    def get_class(filepath):\n        dirname = os.path.dirname(filepath)\n        if leaf_name_only:\n            return os.path.basename(dirname)\n        return dirname\n\n    def get_label(filepath):\n        dirname = os.path.dirname(filepath)\n        if leaf_name_only:\n            return os.path.basename(dirname)\n        return dirname.replace(os.path.dirname(dirname), '')\n\n    images = []\n    targets = []\n    for root, _, fnames in sorted(os.walk(folder)):\n        for fname in sorted(fnames):\n            if has_file_allowed_extension(fname, types):\n                path = os.path.join(root, fname)\n                item = (path, get_class(path))\n                if class_to_idx:\n                    target = class_to_idx[get_label(path)]\n                else:\n                    target = class_to_idx.get(get_label(path))\n                ", "CodeBleu_score": 28.23517425301193}, {"code": "def find_images_and_targets(\n        folder: str,\n        types: Optional[Union[List, Tuple, Set]] = None,\n        class_to_idx: Optional[Dict] = None,\n        leaf_name_only: bool = True,\n        sort: bool = True\n)\n\t\n\n    if not os.path.isdir(folder):\n        raise FileNotFoundError(f\"{folder} is not a valid directory\")\n\n    if types is None:\n        types = ('.jpg', '.jpeg', '.png')\n\n    if not isinstance(types, (list, tuple, set)):\n        raise TypeError(f\"{types} is not a valid type\")\n\n    if class_to_idx is None:\n        class_to_idx = {}\n\n    if not isinstance(class_to_idx, dict):\n        raise TypeError(f\"{class_to_idx} is not a valid type\")\n\n    if leaf_name_only:\n        folder = os.path.basename(folder)\n\n    images = []\n    dirs = []\n    for root, _, fnames in sorted(os.walk(folder, followlinks=True)):\n        for fname in sorted(fnames):\n            if has_file_allowed_extension(fname, types):\n                path = os.path.join(root, fname)\n                item = path\n                target = class_from_target_path(path, class_to_idx, leaf_name_only)\n                images.append((item, target))\n                dirs.append(os.path.basename(root))\n\n    if sort:\n        images.sort(key=lambda k: natural_key(k[0]))\n\n    return images, dirs, class_to_idx\n\n", "CodeBleu_score": 36.929347135346845}]}
{"instance_id": "e7fe43bdecdc", "generation_result": [{"code": "def pytorch_worker_seed()\n\t\n    if get_worker_info() is not None:\n        return get_worker_info().seed % 2**32\n    return random.randint(0, 2**32)\n\n", "CodeBleu_score": 33.07061158166509}, {"code": "def pytorch_worker_seed()\n\t\n    worker_info = get_worker_info()\n    if worker_info is not None:\n        return worker_info.seed\n    return random.randint(0, 2**32)\n\n", "CodeBleu_score": 52.87984585668606}, {"code": "def pytorch_worker_seed()\n\t\n    worker_info = get_worker_info()\n    if worker_info is not None:\n        return worker_info.seed % 2**32\n    else:\n        return random.randint(0, 2**32 - 1)\n\n", "CodeBleu_score": 46.231120631646505}]}
{"instance_id": "0ffc7e7ded25", "generation_result": [{"code": "def solarize_add(img, add, thresh=128, **__)\n\t\n    lut = []\n    for i in range(256):\n        if i < thresh:\n            lut.append(min(255, i + add))\n        else:\n            lut.append(i)\n    if img.mode in [\"L\", \"RGB\"]:\n        return img.point(lut)\n    return img\n\n", "CodeBleu_score": 57.11216591506537}, {"code": "def solarize_add(img, add, thresh=128, **__)\n\t\n    lut = []\n    for i in range(256):\n        if i < thresh:\n            lut.append(min(255, i + add))\n        else:\n            lut.append(i)\n    if img.mode in (\"L\", \"RGB\"):\n        return img.point(lut)\n    return img\n\n", "CodeBleu_score": 59.559831640690675}, {"code": "def solarize_add(img, add, thresh=128, **__)\n\t\n    lut = []\n    for i in range(256):\n        if i < thresh:\n            lut.append(min(255, i + add))\n        else:\n            lut.append(i)\n    if img.mode in [\"L\", \"RGB\"]:\n        return img.point(lut)\n    return img\n\n", "CodeBleu_score": 57.11216591506537}]}
{"instance_id": "5a0d29577a21", "generation_result": [{"code": "def _get_weighted_transforms(transforms: Dict)\n\t\n    transform_names = []\n    transform_probs = []\n    for name, prob in transforms.items():\n        transform_names.append(name)\n        transform_probs.append(prob)\n\n    # Normalize transform probabilities to sum to one\n    transform_probs = [prob / sum(transform_probs) for prob in transform_probs]\n\n    return transform_names, transform_probs\n\n", "CodeBleu_score": 30.533629324050015}, {"code": "def _get_weighted_transforms(transforms: Dict)\n\t\n    transforms_names = []\n    transforms_probs = []\n    for transform, prob in transforms.items():\n        transforms_names.append(transform)\n        transforms_probs.append(prob)\n    transforms_probs = [prob / sum(transforms_probs) for prob in transforms_probs]\n    return transforms_names, transforms_probs\n\n", "CodeBleu_score": 30.099264617236543}, {"code": "def _get_weighted_transforms(transforms: Dict)\n\t\n    transform_names = []\n    probabilities = []\n    for transform_name, probability in transforms.items():\n        if transform_name not in transform_names:\n            transform_names.append(transform_name)\n            probabilities.append(probability)\n    total_probability = sum(probabilities)\n    probabilities = [probability / total_probability for probability in probabilities]\n    return transform_names, probabilities\n\n", "CodeBleu_score": 27.76708238780044}]}
{"instance_id": "84bc5658c135", "generation_result": [{"code": "def _apply_basic(self, img, mixing_weights, m)\n\t\n    if not self.augmentations:\n        return img\n    if mixing_weights is None:\n        mixing_weights = [1.0] * len(self.augmentations)\n    if len(mixing_weights) != len(self.augmentations):\n        raise ValueError(\n            \"The number of mixing weights must be the same as the number of augmentations.\"\n        )\n    if sum(mixing_weights) == 0:\n        return img\n    if not isinstance(img, Image.Image):\n        img = Image.fromarray(img)\n    mixed_img = np.zeros(img.size, dtype=np.float32)\n    for i, aug in enumerate(self.augmentations):\n        img_aug = aug(img.copy())\n        mixed_img = mixed_img + mixing_weights[i] * np.asarray(img_aug)\n    mixed_img = np.clip(mixed_img, 0, 255).astype(np.uint8)\n    mixed_img = Image.fromarray(mixed_img)\n    return Image.blend(img, mixed_img, m)\n\n", "CodeBleu_score": 33.38096913619006}, {"code": "def _apply_basic(self, img, mixing_weights, m)\n\t\n    self.randomize_parameters()\n    mixed = []\n    for mw in mixing_weights:\n        mixed.append(img.copy())\n        m.augment_image(mw, mixed[-1])\n    if self.blend:\n        if self.blend_original_first:\n            mixed.insert(0, img.copy())\n        else:\n            mixed.append(img.copy())\n        img = Image.blend(mixed[0], mixed[1], self.blend)\n    else:\n        img = random.choice(mixed)\n    return img\n\n", "CodeBleu_score": 27.06961522765095}, {"code": "def _apply_basic(self, img, mixing_weights, m)\n\t\n    if mixing_weights is None:\n        mixing_weights = [1]\n    if not isinstance(mixing_weights, list):\n        mixing_weights = [mixing_weights]\n    if len(mixing_weights) == 0:\n        return img\n    if len(mixing_weights) != len(m):\n        raise ValueError(\"The number of mixing weights should be equal to the number of augmentations\")\n    mixed_img = None\n    for mixing_weight in mixing_weights:\n        if mixed_img is None:\n            mixed_img = img.copy()\n        else:\n            mixed_img = Image.blend(mixed_img, img.copy(), mixing_weight)\n    return mixed_img\n\n", "CodeBleu_score": 23.068450296118943}]}
{"instance_id": "84b5b752ff04", "generation_result": [{"code": "def adapt_to_chs(x, n)\n\t\n    if not isinstance(x, (list, tuple)):\n        x = tuple(repeat(x, n))\n    elif len(x) != n:\n        logging.warning(f\"The length of the input x is {len(x)}, but {n} channels are required. The mean of x will be used.\")\n        x = tuple(repeat(sum(x) / len(x), n))\n    else:\n        assert len(x) == n\n    return x\n\n", "CodeBleu_score": 49.4342803887781}, {"code": "def adapt_to_chs(x, n)\n\t\n    if isinstance(x, (tuple, list)):\n        if len(x) == n:\n            return x\n        else:\n            logging.warning(f\"The length of x ({len(x)}) does not match the required number of channels n ({n}). Calculating the mean of x and repeating it n times.\")\n            x = torch.mean(torch.stack(x), dim=0)\n            return repeat(x, n)\n    else:\n        return repeat(x, n)\n\n", "CodeBleu_score": 36.99094914885824}, {"code": "def adapt_to_chs(x, n)\n\t\n    if not isinstance(x, (tuple, list)):\n        x = repeat(x, n)\n    elif len(x) != n:\n        logging.warning(\n            f\"The length of input x is {len(x)} but the required number of channels is {n}. The mean of x will be repeated {n-len(x)} times to form a tuple.\"\n        )\n        x = (sum(x) / len(x), *repeat(x[-1], n - len(x)))\n    else:\n        assert len(x) == n\n    return x\n\n", "CodeBleu_score": 47.13899910915143}]}
{"instance_id": "54754b6ba394", "generation_result": [{"code": "def _worker_init(worker_id, worker_seeding='all')\n\t\n    worker_info = torch.utils.data.get_worker_info()\n    if worker_info is None:\n        raise RuntimeError('The worker_info is None.')\n    if worker_info.id != worker_id:\n        raise RuntimeError(f'The worker ID {worker_id} is not equal to the worker ID {worker_info.id}.')\n    if isinstance(worker_seeding, str):\n        if worker_seeding == 'all':\n            seed = worker_info.seed\n        elif worker_seeding == 'torch':\n            seed = worker_info.seed % 2**32\n        elif worker_seeding == 'numpy':\n            seed = worker_info.seed % 2**32 - 2**31\n        else:\n            raise ValueError(f'The worker_seeding {worker_seeding} is not supported.')\n    elif callable(worker_seeding):\n        seed = worker_seeding(worker_id)\n    else:\n        raise ValueError(f'The worker_seeding {worker_seeding} is not supported.')\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    torch.backends.cudnn.enabled = True\n    np_type = np.random.RandomState\n    if isinstance(np_type, type):\n        np_type = np_type or np.random.RandomState\n    np_state = getattr(np_type, '_state', None)\n    if (np_state is not None) and (np_state.has_own_rand):\n        np_type.seed(seed)\n\n", "CodeBleu_score": 43.13639817110034}, {"code": "def _worker_init(worker_id, worker_seeding='all')\n\t\n    if worker_seeding is None:\n        return\n    if worker_seeding == 'all':\n        worker_seeding = torch.initial_seed() % 2**32\n    elif not isinstance(worker_seeding, int):\n        raise ValueError(f'worker_seeding must be an integer or a string, but got {type(worker_seeding)}')\n    worker_info = torch.utils.data.get_worker_info()\n    if worker_info is None:\n        raise ValueError(f'worker_info must be set when using worker_init_fn')\n    if worker_info.id != worker_id:\n        raise ValueError(f'worker_id must be {worker_id} but got {worker_info.id}')\n    if isinstance(worker_seeding, int):\n        torch.manual_seed(worker_seeding)\n        random.seed(worker_seeding)\n        torch.cuda.manual_seed_all(worker_seeding)\n    else:\n        raise ValueError(f'worker_seeding must be an integer or a string, but got {type(worker_seeding)}')\n\n", "CodeBleu_score": 45.54538561385625}, {"code": "def _worker_init(worker_id, worker_seeding='all')\n\t\n    if worker_seeding == 'all':\n        # worker_id is a tuple of (pid, index)\n        worker_info = torch.utils.data.get_worker_info()\n        if worker_info is None:\n            raise RuntimeError('Failed to get worker info')\n        if worker_info.id != worker_id:\n            raise RuntimeError('Failed to get correct worker info')\n        worker_seed = worker_info.seed\n        torch.manual_seed(worker_seed)\n        random.seed(worker_seed)\n        torch.cuda.manual_seed_all(worker_seed)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n    elif worker_seeding == 'numpy':\n        # worker_id is a tuple of (pid, index)\n        worker_info = torch.utils.data.get_worker_info()\n        if worker_info is None:\n            raise RuntimeError('Failed to get worker info')\n        if worker_info.id != worker_id:\n            raise RuntimeError('Failed to get correct worker info')\n        worker_seed = worker_info.seed\n        np.random.seed(worker_seed)\n    elif callable(worker_seeding):\n        worker_seeding(worker_id)\n    else:\n        raise ValueError('worker_seeding must be a callable or a string')\n\n", "CodeBleu_score": 42.113785687626226}]}
{"instance_id": "81dfdce940db", "generation_result": [{"code": "def _decode_and_center_crop(image_bytes, image_size, resize_method)\n\t\n    image_shape = tf.image.extract_jpeg_shape(image_bytes)\n    image_height = image_shape[0]\n    image_width = image_shape[1]\n    padded_center_crop_size = tf.cast(\n        ((image_size / (image_size + CROP_PADDING)) *\n         tf.cast(tf.minimum(image_height, image_width), tf.float32)),\n        tf.int32)\n    offset_height = ((image_height - padded_center_crop_size) + 1) // 2\n    offset_width = ((image_width - padded_center_crop_size) + 1) // 2\n    crop_window = tf.stack([offset_height, offset_width,\n                            padded_center_crop_size, padded_center_crop_size])\n    image = tf.image.decode_and_crop_jpeg(image_bytes, crop_window, channels=3)\n    image = tf.image.resize(image, [image_size, image_size],\n                            method=resize_method)\n    return image\n\n", "CodeBleu_score": 58.07503797129272}, {"code": "def _decode_and_center_crop(image_bytes, image_size, resize_method)\n\t\n    # Extract shape of image\n    image_shape = tf.image.extract_jpeg_shape(image_bytes)\n    # Calculate padded center crop size\n    image_height = image_shape[0]\n    image_width = image_shape[1]\n    padded_center_crop_size = tf.cast(\n        ((image_size / (image_size + CROP_PADDING)) *\n         tf.cast(tf.minimum(image_height, image_width), tf.float32)),\n        tf.int32)\n    # Determine offset for center crop\n    offset_height = ((image_height - padded_center_crop_size) + 1) // 2\n    offset_width = ((image_width - padded_center_crop_size) + 1) // 2\n    # Crop and resize image\n    cropped_image = tf.image.crop_to_bounding_box(\n        image_bytes, offset_height, offset_width,\n        padded_center_crop_size, padded_center_crop_size)\n    return tf.image.resize(cropped_image, [image_size, image_size],\n                           method=resize_method)\n\n", "CodeBleu_score": 49.247782360099855}, {"code": "def _decode_and_center_crop(image_bytes, image_size, resize_method)\n\t\n    # Extract the image shape and the crop window size.\n    image_shape = tf.image.extract_jpeg_shape(image_bytes)\n    image_height = image_shape[0]\n    image_width = image_shape[1]\n    crop_window_size = tf.cast(\n        ((image_size / (image_size + _PADDING)) *\n         tf.cast(tf.minimum(image_height, image_width), tf.float32)),\n        tf.int32)\n\n    # Calculate the upper-left corner of the crop window to be centered in the image.\n    offset_height = ((image_height - crop_window_size) + 1) // 2\n    offset_width = ((image_width - crop_window_size) + 1) // 2\n\n    # Decode and crop the image.\n    cropped_image = tf.image.decode_and_crop_jpeg(\n        image_bytes,\n        [offset_height, offset_width, crop_window_size, crop_window_size],\n        channels=3)\n\n    # Resize the image.\n    resized_image = tf.image.resize(cropped_image, [image_size, image_size],\n                                    method=resize_method)\n    return resized_image\n\n", "CodeBleu_score": 44.672639734368225}]}
{"instance_id": "cf16a6d37111", "generation_result": [{"code": "def preprocess_for_train(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    image = decode_and_random_crop(image_bytes, image_size, interpolation)\n    image = flip_and_resize(image, image_size)\n    image = tf.image.convert_image_dtype(image, dtype=tf.bfloat16 if use_bfloat16 else tf.float32)\n    return image\n", "CodeBleu_score": 31.890271311451922}, {"code": "def preprocess_for_train(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n\n    # Decode image\n    image = tf.image.decode_image(image_bytes, channels=3)\n\n    # Randomly crop image\n    image = tf.image.random_crop(image, size=[image_size, image_size, 3])\n\n    # Flip image\n    image = tf.image.random_flip_left_right(image)\n\n    # Resize image\n    image = tf.image.resize(image, size=[image_size, image_size], method=interpolation)\n\n    # Convert to bfloat16 or float32\n    if use_bfloat16:\n        image = tf.image.convert_image_dtype(image, dtype=tf.bfloat16)\n    else:\n        image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n\n    return image\n\n\n\n", "CodeBleu_score": 42.25809591515481}, {"code": "def preprocess_for_train(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    # Decode the image bytes into a tensor.\n    image = tf.io.decode_jpeg(image_bytes, channels=3)\n\n    # Randomly crop the image.\n    image = tf.image.random_crop(image, size=[image_size, image_size, 3])\n\n    # Randomly flip the image horizontally.\n    image = tf.image.random_flip_left_right(image)\n\n    # Resize the image to the target size.\n    image = tf.image.resize(image, [image_size, image_size], method=interpolation)\n\n    # Convert the image to the specified data type.\n    image = tf.image.convert_image_dtype(image, dtype=tf.bfloat16 if use_bfloat16 else tf.float32)\n\n    return image\n\n", "CodeBleu_score": 42.6744447872392}]}
{"instance_id": "78b2a3bd2142", "generation_result": [{"code": "def preprocess_for_eval(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n\n    # Decode the image data\n    image = tf.image.decode_jpeg(image_bytes, channels=3)\n\n    # Center crop the image\n    image = tf.image.resize_with_crop_or_pad(image, image_size, image_size)\n\n    # Resize the image to the specified size\n    if interpolation == 'bicubic':\n        image = tf.image.resize(image, [image_size, image_size],\n                                method=tf.image.ResizeMethod.BICUBIC)\n    else:\n        image = tf.image.resize(image, [image_size, image_size],\n                                method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n\n    # Convert the image data type\n    if use_bfloat16:\n        image = tf.image.convert_image_dtype(image, dtype=tf.bfloat16)\n    else:\n        image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n\n    return image\n\n", "CodeBleu_score": 42.763334096364744}, {"code": "def preprocess_for_eval(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    # Decode the image\n    image = tf.io.decode_image(image_bytes, channels=3, dtype=tf.float32)\n    # Center crop and resize the image to the specified size\n    image = tf.image.resize(image, (image_size, image_size), method=interpolation)\n    # Convert to bfloat16 if needed\n    if use_bfloat16:\n        image = tf.cast(image, tf.bfloat16)\n    return image\n\n", "CodeBleu_score": 28.79967462387585}, {"code": "def preprocess_for_eval(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    import tensorflow as tf\n    import tensorflow_addons as tfa\n    import numpy as np\n    # Convert the image to a float tensor.\n    image = tf.image.decode_jpeg(image_bytes, channels=3)\n    # Crop the central region of the image with an area containing 87.5% of\n    # the original image.\n    image = tf.image.central_crop(image, central_fraction=0.875)\n    # Resize the image to the original height and width.\n    image = tf.expand_dims(image, 0)\n    image = tf.image.resize(image, [image_size, image_size],\n                            method=tf.image.ResizeMethod.BICUBIC)\n    image = tf.squeeze(image, [0])\n    # Convert to float and normalize the pixel values to the range [0, 1].\n    image = tf.cast(image, tf.float32)\n    image /= 255.0\n    # If bfloat16 is used, cast to bfloat16.\n    if use_bfloat16:\n        image = tf.cast(image, tf.bfloat16)\n    return image\n\n", "CodeBleu_score": 43.39698679645438}]}
{"instance_id": "8bd4364a9a0e", "generation_result": [{"code": "def evaluate(self, model, eval_dataloader=None, output_file=\"merged\")\n\t\n\n    if eval_dataloader is None:\n        eval_dataloader = self.get_eval_dataloader(model)\n\n    if self.args.past_index >= 0:\n        self._past = None\n\n    # Evaluation\n    self.logger.info(\"***** Running evaluation *****\")\n    self.logger.info(\"  Num examples = %d\", len(eval_dataloader.dataset))\n    self.logger.info(\"  Num Epochs = %d\", 1)\n    self.logger.info(\"  Batch size = %d\", self.args.eval_batch_size)\n\n    model.eval()\n\n    self.callback_handler.eval_dataloader = eval_dataloader\n    eval_dataloader = self.callback_handler.get_eval_dataloader(eval_dataloader)\n\n    eval_loss = 0.0\n    nb_eval_steps = 0\n    preds = None\n    out_label_ids = None\n    for batch in eval_dataloader:\n        self.callback_handler.eval_step_start(self.args, self.state, self.control, model, batch)\n\n        with torch.no_grad():\n            inputs = self._prepare_inputs(batch)\n            outputs = model(**inputs)\n            tmp_eval_loss, logits = outputs[:2]\n\n            if self.args.past_index >= 0:\n                self._past = outputs[self.args.past_index]\n\n            if self.args.n_gpu > 1:\n                tmp_eval_loss = tmp_eval_loss.mean()  # mean() to average on multi-gpu parallel evaluating\n\n            eval_loss += tmp_eval_loss.item()\n        nb_eval_steps += 1\n\n        if preds is None:\n            preds = logits.detach().cpu().numpy()\n            out_label_ids = inputs[\"labels\"].detach().cpu().numpy()\n        else:\n            preds = np.append(preds, logits.detach().", "CodeBleu_score": 38.06754973008499}, {"code": "def evaluate(self, model, eval_dataloader=None, output_file=\"merged\")\n\t\n    if eval_dataloader is None:\n        eval_dataloader = self.get_eval_dataloader(self.eval_dataset)\n    model.eval()\n    eval_dataloader = self.get_eval_dataloader(self.eval_dataset)\n    self.write_prediction_file(model, eval_dataloader, output_file)\n    results = self.compute_metrics(output_file)\n    return results\n\n", "CodeBleu_score": 51.75359697035431}, {"code": "def evaluate(self, model, eval_dataloader=None, output_file=\"merged\")\n\t\n    # If no evaluation dataloader is specified, use the default one\n    if eval_dataloader is None:\n        eval_dataloader = self.eval_dataloader\n\n    # Create a temporary directory for the evaluation outputs\n    eval_output_dir = self.get_temp_dir()\n\n    # Create a temporary file for the evaluation results\n    eval_results_file = os.path.join(eval_output_dir, \"eval_results.txt\")\n\n    # Create a temporary file for the evaluation predictions\n    eval_predictions_file = os.path.join(eval_output_dir, \"eval_predictions.txt\")\n\n    # Create a temporary file for the evaluation labels\n    eval_labels_file = os.path.join(eval_output_dir, \"eval_labels.txt\")\n\n    # Create a temporary file for the evaluation probabilities\n    eval_probabilities_file = os.path.join(eval_output_dir, \"eval_probabilities.txt\")\n\n    # Create a temporary file for the evaluation probabilities\n    eval_probabilities_file = os.path.join(eval_output_dir, \"eval_probabilities.txt\")\n\n    # Create a temporary file for the evaluation probabilities\n    eval_probabilities_file = os.path.join(eval_output_dir, \"eval_probabilities.txt\")\n\n    # Create a temporary file for the evaluation probabilities\n    eval_probabilities_file = os.path.join(eval_output_dir, \"eval_probabilities.txt\")\n\n    # Create a temporary file for the evaluation probabilities\n    eval_probabilities_file = os.path.join(eval_output_dir, \"eval_probabilities.txt\")\n\n    # Create a temporary file for the evaluation probabilities\n    eval_probabilities_file = os.path.join(eval_output_dir, \"eval_probabilities.txt\")\n\n    # Create a temporary file for the evaluation probabilities\n    eval_probabilities_file = os.path.join(eval_output_dir, \"eval_probabilities.txt\")\n\n    # Create a temporary file for the evaluation probabilities\n    eval_probabilities_file = os.path.", "CodeBleu_score": 42.224739078687676}]}
{"instance_id": "22d682c958c0", "generation_result": [{"code": " def print_computed_metrics(self, metrics)\n\t\n    print(\"Rank1: {:.4f}\".format(metrics[\"rank1\"]))\n    print(\"Rank5: {:.4f}\".format(metrics[\"rank5\"]))\n    print(\"mIoU: {:.4f}\".format(metrics[\"miou\"]))\n\n\n def print_computed_metrics_json(self, metrics):\n    \"\"\"The function prints the computed metrics for rank1, rank5, and miou.", "CodeBleu_score": 28.34202488399683}, {"code": " def print_computed_metrics(self, metrics)\n\t\n    rank1 = metrics['rank1']\n    rank5 = metrics['rank5']\n    miou = metrics['miou']\n    print('Rank-1: {:.4f}'.format(rank1))\n    print('Rank-5: {:.4f}'.format(rank5))\n    print('mIoU:   {:.4f}'.format(miou))\n\n", "CodeBleu_score": 27.1629525276255}, {"code": " def print_computed_metrics(self, metrics)\n\t\n\n    rank1 = metrics['rank1']\n    rank5 = metrics['rank5']\n    miou = metrics['miou']\n\n    print('Rank-1: {:.4f}, Rank-5: {:.4f}, mIoU: {:.4f}'.format(rank1, rank5, miou))\n\n", "CodeBleu_score": 27.477747159264577}]}
{"instance_id": "39c1a50b7e7d", "generation_result": [{"code": "def _eval_predictions(self, segments, data)\n\t\n    rank1 = 0\n    rank5 = 0\n    ious = []\n    for segment in segments:\n        segment_ious = []\n        for data_idx in range(len(data)):\n            data_segment = data[data_idx]\n            iou = self._compute_iou(segment, data_segment)\n            if iou > 0:\n                segment_ious.append(iou)\n        if len(segment_ious) > 0:\n            rank = segment_ious.index(max(segment_ious)) + 1\n            rank1 += rank == 1\n            rank5 += rank <= 5\n            ious.append(max(segment_ious))\n    return rank1 / len(segments), rank5 / len(segments), sum(ious) / len(ious)\n\n", "CodeBleu_score": 30.70179638040885}, {"code": "def _eval_predictions(self, segments, data)\n\t\n    if self.segment_iou_threshold is None:\n        self.segment_iou_threshold = 0.5\n\n    if self.rank_iou_threshold is None:\n        self.rank_iou_threshold = 0.5\n\n    rank1, rank5, ious = 0, 0, []\n    for seg, seg_data in zip(segments, data):\n        seg_ious = []\n        for pred in seg:\n            max_iou = 0\n            for gt in seg_data:\n                iou = self._compute_iou(pred, gt)\n                if iou > max_iou:\n                    max_iou = iou\n            seg_ious.append(max_iou)\n        seg_ious = sorted(seg_ious, reverse=True)\n        rank = seg_ious.index(max(seg_ious)) + 1\n        ious.append(max(seg_ious))\n        if rank == 1:\n            rank1 += 1\n        if rank <= 5:\n            rank5 += 1\n\n    rank1 /= len(segments)\n    rank5 /= len(segments)\n    ious = sum(ious) / len(ious)\n\n    return rank1, rank5, ious\n\n", "CodeBleu_score": 31.42036085343307}, {"code": "def _eval_predictions(self, segments, data)\n\t\n    # Create a list of segment-data pairs\n    pairs = []\n    for segment in segments:\n        for data_item in data:\n            pairs.append((segment, data_item))\n\n    # For each segment-data pair, compute IoUs and ranks\n    ious = []\n    ranks = []\n    for segment, data_item in pairs:\n        # Compute IoU\n        iou = self.compute_iou(segment, data_item)\n        ious.append(iou)\n\n        # Compute rank\n        rank = self.compute_rank(iou, data_item)\n        ranks.append(rank)\n\n    # Compute mean IoU\n    mean_iou = sum(ious) / len(ious)\n\n    # Compute rank1 and rank5\n    rank1 = len([r for r in ranks if r == 1]) / len(ranks)\n    rank5 = len([r for r in ranks if r <= 5]) / len(ranks)\n\n    return rank1, rank5, mean_iou\n\n", "CodeBleu_score": 29.97870210062429}]}
{"instance_id": "8ccf84f8d034", "generation_result": [{"code": "def predict_loop(self, model, eval_dataloader, output_file=None)\n\t\n    # Initialize empty lists for scores\n    preds = []\n    labels = []\n    ids = []\n    # Set model to evaluation mode\n    model.eval()\n    # Transfer model to GPU 0\n    model.cuda(0)\n    # Iterate over the data\n    for input_ids, input_mask, segment_ids, label_ids, qas_id in tqdm(eval_dataloader, desc=\"Evaluating\"):\n        # Transfer data to GPU 0\n        input_ids = input_ids.cuda(0)\n        input_mask = input_mask.cuda(0)\n        segment_ids = segment_ids.cuda(0)\n        # Make predictions\n        with torch.no_grad():\n            logits = model(input_ids, segment_ids, input_mask)\n        # Update the scores\n        logits = logits.detach().cpu().numpy()\n        label_ids = label_ids.cpu().numpy()\n        qas_id = qas_id.cpu().numpy()\n        preds.append(logits)\n        labels.append(label_ids)\n        ids.append(qas_id)\n    # Finalize and return the results\n    preds = np.concatenate(preds, axis=0)\n    labels = np.concatenate(labels, axis=0)\n    ids = np.concatenate(ids, axis=0)\n    if output_file:\n        output_dir = os.path.dirname(output_file)\n        if not os.path.exists(output_dir):\n            os.makedirs(output_dir)\n        with open(output_file, \"w\") as writer:\n            writer.write(\"index\\tprediction", "CodeBleu_score": 33.2091238574443}, {"code": "def predict_loop(self, model, eval_dataloader, output_file=None)\n\t\n    self.logger.info(\"***** Running Prediction *****\")\n    self.logger.info(\"  Num examples = %d\", len(eval_dataloader.dataset))\n    self.logger.info(\"  Batch size = %d\", self.args.eval_batch_size)\n\n    model.eval()\n\n    self.logger.info(\"Starting Prediction\")\n    scores = []\n    for step, inputs in enumerate(tqdm(eval_dataloader)):\n        for k, v in inputs.items():\n            inputs[k] = v.to(self.args.device)\n\n        with torch.no_grad():\n            outputs = model(**inputs)\n\n        for output in outputs:\n            if self.args.output_mode == \"classification\":\n                scores += [output.to(\"cpu\").numpy()]\n            elif self.args.output_mode == \"regression\":\n                scores += [output.to(\"cpu\").numpy()[:, 0]]\n\n    self.logger.info(\"Finished Prediction\")\n\n    if output_file is not None:\n        self.logger.info(f\"Saving results to {output_file}\")\n        with open(output_file, \"w\") as writer:\n            writer.write(\"\".join([str(x) + \"", "CodeBleu_score": 31.15932824367859}, {"code": "def predict_loop(self, model, eval_dataloader, output_file=None)\n\t\n    # Initialize empty lists for scores\n    all_predictions = []\n    all_nbest_json = []\n    all_input_ids = []\n    all_input_mask = []\n    all_segment_ids = []\n    all_example_indices = []\n\n    # Set model to evaluation mode\n    model.eval()\n\n    # Transfer model to GPU 0\n    if self.args.local_rank != -1:\n        model = model.to(self.args.device)\n\n    # Iterate over the dataloader\n    for step, batch in enumerate(tqdm(eval_dataloader, desc=\"Evaluating\")):\n        # Transfer data to appropriate context\n        batch = tuple(t.to(self.args.device) for t in batch)\n\n        # Unpack the inputs from our dataloader\n        input_ids, input_mask, segment_ids, example_indices = batch\n\n        # Make predictions without computing gradients\n        with torch.no_grad():\n            # Forward pass\n            outputs = model(input_ids=input_ids, attention_mask=input_mask, token_type_ids=segment_ids)\n            logits = outputs[0]\n\n            # Move logits and inputs to CPU\n            logits = logits.detach().cpu().numpy()\n            input_ids = input_ids.detach().cpu().numpy()\n            input_mask = input_mask.detach().cpu().numpy()\n            segment_ids = segment_ids.detach().cpu().numpy()\n            example_indices = example_indices.detach().cpu().numpy()\n\n        # Process outputs\n        for i, example_index in enumerate(example_indices):\n            # Unpack predictions\n            start_logits = logits[i, :, 0]\n            end_logits = logits[i, :, 1]\n            cls_logits = logits[i, 0, 2]\n\n            # Compute scores\n            start_scores = start_logits.tolist()\n            end_scores = end_logits.tolist()\n            cls_scores = cls_logits.tol", "CodeBleu_score": 28.822257445521387}]}
{"instance_id": "e68529ad727d", "generation_result": [{"code": "def _aggregate_scores(self, scores)\n\t\n    assert len(scores) == 2, 'There must be exactly two sets of scores.'\n    text_scores = torch.cat([s[0] for s in scores], dim=0)\n    video_scores = torch.cat([s[1] for s in scores], dim=0)\n    self.full_scores = None\n    return torch.matmul(text_scores, video_scores.t())\n\n", "CodeBleu_score": 37.303377667351754}, {"code": "def _aggregate_scores(self, scores)\n\t\n    assert len(scores) == 2, \"scores should have length 2\"\n    text_scores, video_scores = scores\n    self.full_scores = torch.cat([text_scores, video_scores], dim=1)\n    return torch.matmul(text_scores, video_scores.transpose(0, 1))\n\n", "CodeBleu_score": 31.323042612555962}, {"code": "def _aggregate_scores(self, scores)\n\t\n\n    # Aggregate scores\n    self.full_scores = torch.cat([self.full_scores, scores], dim=0)\n\n    # Reset the scores\n    self.scores = []\n\n    # Return the dot product of the text and video hidden representations\n    return self.full_scores[:, 0] * self.full_scores[:, 1]\n\n", "CodeBleu_score": 18.819879289950553}]}
{"instance_id": "e4e1861fc357", "generation_result": [{"code": "def finalize(self, Y_pred, Y_true, output_file=None)\n\t\n    # concatenate predicted and true labels\n    Y_pred = np.concatenate(Y_pred, axis=0)\n    Y_true = np.concatenate(Y_true, axis=0)\n\n    # check for prediction errors\n    if (Y_pred != Y_true).any():\n        print(\"Prediction errors:\")\n        for i in range(len(Y_pred)):\n            if Y_pred[i] != Y_true[i]:\n                print(\"Predicted: {}, True: {}\".format(Y_pred[i], Y_true[i]))\n\n    # save predictions and true labels as a pickle file\n    if output_file is not None:\n        with open(output_file, \"wb\") as f:\n            pickle.dump({\"Y_pred\": Y_pred, \"Y_true\": Y_true}, f)\n\n    return {\"Y_pred\": Y_pred, \"Y_true\": Y_true}\n\n", "CodeBleu_score": 37.33086426338345}, {"code": "def finalize(self, Y_pred, Y_true, output_file=None)\n\t\n    Y_pred = torch.cat(Y_pred, 0)\n    Y_true = torch.cat(Y_true, 0)\n    assert len(Y_pred) == len(Y_true)\n\n    if output_file is not None:\n        with open(output_file, 'wb') as f:\n            pickle.dump((Y_pred, Y_true), f)\n\n    errors = []\n    for i in range(len(Y_pred)):\n        if Y_pred[i] != Y_true[i]:\n            errors.append(i)\n\n    if len(errors) > 0:\n        print('Found {} errors in predictions'.format(len(errors)))\n        print('Sample errors:')\n        for i in random.sample(errors, min(10, len(errors))):\n            print('{} (predicted: {}, true: {})'.format(i, Y_pred[i], Y_true[i]))\n\n    return {'Y_pred': Y_pred, 'Y_true': Y_true}\n\n", "CodeBleu_score": 46.52263472890327}, {"code": "def finalize(self, Y_pred, Y_true, output_file=None)\n\t\n    # concatenate predicted and true labels\n    Y_pred = torch.cat(Y_pred, dim=0)\n    Y_true = torch.cat(Y_true, dim=0)\n\n    # check for prediction errors\n    if (Y_pred != Y_true).sum() > 0:\n        print(\"Prediction errors:\")\n        for i in range(Y_pred.shape[0]):\n            if Y_pred[i] != Y_true[i]:\n                print(f\"Sample {i}: Predicted {Y_pred[i].item()}, True {Y_true[i].item()}\")\n\n    # save predictions and true labels as pickle file\n    if output_file is not None:\n        with open(output_file, \"wb\") as f:\n            pickle.dump({\"Y_pred\": Y_pred, \"Y_true\": Y_true}, f)\n\n    # return dictionary containing predicted and true labels\n    return {\"Y_pred\": Y_pred, \"Y_true\": Y_true}\n\n", "CodeBleu_score": 38.57578960171819}]}
{"instance_id": "7df9ab9a4920", "generation_result": [{"code": "def reduce_metrics(logging_outputs) -> None\n\t\n    sample_size = 0\n    loss = 0\n    for log in logging_outputs:\n        sample_size += log.get('sample_size', 0)\n        loss += log.get('loss', 0)\n    metrics.log_scalar('loss', loss / sample_size, sample_size, round=3)\n\n", "CodeBleu_score": 35.183762683389766}, {"code": "def reduce_metrics(logging_outputs) -> None\n\t\n    sample_size = 0\n    loss = 0.\n    for log in logging_outputs:\n        loss += log['loss']\n        sample_size += log['sample_size']\n    metrics.log_scalar('loss', loss / sample_size, sample_size, round=3)\n\n", "CodeBleu_score": 31.2868335815823}, {"code": "def reduce_metrics(logging_outputs) -> None\n\t\n    loss_sum = sum(log.get('loss', 0) for log in logging_outputs)\n    sample_size = sum(log.get('sample_size', 0) for log in logging_outputs)\n    metrics.log_scalar('loss', loss_sum / sample_size, sample_size, round=3)\n\n", "CodeBleu_score": 52.7910982560264}]}
{"instance_id": "7f9c326a96f7", "generation_result": [{"code": "def _mm_on_the_fly(\n        self,\n        cmasks,\n        vmasks,\n        attention_mask\n    )\n\t\n\n    # create attention mask\n    if attention_mask is None:\n        attention_mask = torch.cat([cmasks, vmasks], dim=1)\n\n    # create token type IDs\n    token_type_ids = torch.cat([\n        torch.zeros_like(cmasks),\n        torch.ones_like(vmasks)\n    ], dim=1)\n\n    return attention_mask, token_type_ids\n\n", "CodeBleu_score": 26.445209272204885}, {"code": "def _mm_on_the_fly(\n        self,\n        cmasks,\n        vmasks,\n        attention_mask\n    )\n\t\n\n    # generate attention mask if not provided\n    if attention_mask is None:\n        attention_mask = torch.cat(\n            [\n                cmasks.unsqueeze(-2),\n                vmasks.unsqueeze(-2),\n            ],\n            dim=-2,\n        )\n\n    # generate token type IDs\n    token_type_ids = torch.cat(\n        [\n            torch.zeros_like(cmasks).unsqueeze(-1),\n            torch.ones_like(vmasks).unsqueeze(-1),\n        ],\n        dim=-1,\n    )\n\n    return attention_mask, token_type_ids\n\n", "CodeBleu_score": 41.11098564859345}, {"code": "def _mm_on_the_fly(\n        self,\n        cmasks,\n        vmasks,\n        attention_mask\n    )\n\t\n    if attention_mask is None:\n        attention_mask = torch.cat([cmasks, vmasks], dim=1)\n    token_type_ids = torch.cat([\n        torch.zeros_like(cmasks),\n        torch.ones_like(vmasks)\n    ], dim=1)\n    return attention_mask, token_type_ids\n\n", "CodeBleu_score": 26.445209272204885}]}
{"instance_id": "fa99d2a148a5", "generation_result": [{"code": "def prepare_inputs_for_generation(\n        self,\n        input_ids,\n        input_video_embeds,\n        attention_mask=None,\n        token_type_ids=None,\n        **model_kwargs\n    )\n\t\n    # adjust sequence length to match the combined length of input_ids and input_video_embeds\n    sequence_length = input_ids.shape[1]\n    sequence_length_video = input_video_embeds.shape[1]\n    sequence_length_new = sequence_length + sequence_length_video\n    input_ids = F.pad(input_ids, (0, sequence_length_new - sequence_length), value=self.config.pad_token_id)\n    input_video_embeds = F.pad(input_video_embeds, (0, sequence_length_new - sequence_length_video), value=0)\n\n    # adjust attention_mask and token_type_ids to fit the new sequence length\n    if attention_mask is not None:\n        attention_mask = F.pad(attention_mask, (0, sequence_length_new - sequence_length), value=True)\n    if token_type_ids is not None:\n        token_type_ids = F.pad(token_type_ids, (0, sequence_length_new - sequence_length), value=0)\n\n    return {\n        \"input_ids\": input_ids,\n        \"input_video_embeds\": input_video_embeds,\n        \"attention_mask\": attention_mask,\n        \"token_type_ids\": token_type_ids,\n    }\n\n", "CodeBleu_score": 44.456184493561274}, {"code": "def prepare_inputs_for_generation(\n        self,\n        input_ids,\n        input_video_embeds,\n        attention_mask=None,\n        token_type_ids=None,\n        **model_kwargs\n    )\n\t\n\n    # Set the updated sequence length\n    sequence_length = input_ids.shape[1] + input_video_embeds.shape[1]\n\n    # Modify attention_mask and token_type_ids\n    if attention_mask is not None:\n        attention_mask = attention_mask.repeat(1, 2)\n    if token_type_ids is not None:\n        token_type_ids = token_type_ids.repeat(1, 2)\n\n    # Modify input_ids and input_video_embeds\n    input_ids = torch.cat([input_ids, input_video_embeds], dim=1)\n\n    return {\n        \"input_ids\": input_ids,\n        \"attention_mask\": attention_mask,\n        \"token_type_ids\": token_type_ids,\n    }\n\n", "CodeBleu_score": 38.91727834687345}, {"code": "def prepare_inputs_for_generation(\n        self,\n        input_ids,\n        input_video_embeds,\n        attention_mask=None,\n        token_type_ids=None,\n        **model_kwargs\n    )\n\t\n    input_ids, input_video_embeds = self._pad_inputs_for_generation(\n        input_ids, input_video_embeds, padding=self.pad_token_id, max_length=self.model_max_length\n    )\n    attention_mask, token_type_ids = self._pad_inputs_for_generation(\n        attention_mask, token_type_ids, padding=False, max_length=self.model_max_length\n    )\n    return {\n        \"input_ids\": input_ids,\n        \"input_video_embeds\": input_video_embeds,\n        \"attention_mask\": attention_mask,\n        \"token_type_ids\": token_type_ids,\n    }\n\n", "CodeBleu_score": 32.536057385422815}]}
{"instance_id": "b1b0d25fe620", "generation_result": [{"code": "def _init_pipeline(self, cfg: ConfigType) -> Compose\n\t\n    if 'img_id' in cfg.data.test.pipeline[1]['meta_keys']:\n        cfg.data.test.pipeline[1]['meta_keys'].remove('img_id')\n    loader_index = None\n    for i, t in enumerate(cfg.data.test.pipeline):\n        if t['type'] == 'LoadImageFromFile':\n            loader_index = i\n            break\n    if loader_index is None:\n        raise ValueError('The pipeline does not contain a loader.')\n    cfg.data.test.pipeline[loader_index]['type'] = 'mmdet.InferencerLoader'\n    return Compose(cfg.data.test.pipeline)\n\n", "CodeBleu_score": 30.713295806622448}, {"code": "def _init_pipeline(self, cfg: ConfigType) -> Compose\n\t\n    # remove 'img_id' from meta_keys if present\n    if 'img_id' in cfg['meta_keys']:\n        cfg['meta_keys'].remove('img_id')\n    # find the index of the 'LoadImageFromFile' transform\n    try:\n        index = cfg.index('LoadImageFromFile')\n    except ValueError:\n        raise ValueError('Cannot find \"LoadImageFromFile\" transform in test_pipeline.')\n    # update the transform type to 'mmdet.InferencerLoader'\n    cfg[index] = dict(type='mmdet.InferencerLoader', **cfg[index])\n    # return the modified pipeline configuration wrapped in a Compose object\n    return Compose(cfg)\n\n", "CodeBleu_score": 27.146208381631425}, {"code": "def _init_pipeline(self, cfg: ConfigType) -> Compose\n\t\n\n    # remove img_id from meta_keys\n    for t in cfg.data.test.pipeline:\n        if t['type'] == 'LoadImageFromFile':\n            if 'img_id' in t['meta_keys']:\n                t['meta_keys'].remove('img_id')\n            break\n\n    # find the index of the LoadImageFromFile transform\n    idx = -1\n    for i, t in enumerate(cfg.data.test.pipeline):\n        if t['type'] == 'LoadImageFromFile':\n            idx = i\n            break\n    if idx == -1:\n        raise ValueError(\n            'LoadImageFromFile transform not found in the test pipeline')\n\n    # update the transform type to mmdet.InferencerLoader\n    cfg.data.test.pipeline[idx]['type'] = 'mmdet.InferencerLoader'\n\n    # initialize the test pipeline\n    return Compose(cfg.data.test.pipeline)\n\n", "CodeBleu_score": 40.98969877957855}]}
{"instance_id": "7967962856eb", "generation_result": [{"code": "def _inputs_to_list(self, inputs: InputsType) -> list\n\t\n    if isinstance(inputs, (str, bytes)):\n        if isdir(inputs):\n            inputs = list_dir_or_file(inputs)\n        else:\n            inputs = [inputs]\n    elif not isinstance(inputs, (list, tuple)):\n        inputs = [inputs]\n    return inputs\n\n", "CodeBleu_score": 35.003510438734196}, {"code": "def _inputs_to_list(self, inputs: InputsType) -> list\n\t\n    if isinstance(inputs, (str, bytes)):\n        if isdir(inputs):\n            inputs = list_dir_or_file(inputs, backend=get_file_backend())\n        else:\n            inputs = [inputs]\n    elif not isinstance(inputs, (list, tuple)):\n        inputs = [inputs]\n    return inputs\n\n", "CodeBleu_score": 37.52460957775731}, {"code": "def _inputs_to_list(self, inputs: InputsType) -> list\n\t\n    if isinstance(inputs, str):\n        if isdir(inputs):\n            inputs = list_dir_or_file(inputs, join_path=True)\n        else:\n            inputs = [inputs]\n    elif not isinstance(inputs, (list, tuple)):\n        inputs = [inputs]\n    return inputs\n\n", "CodeBleu_score": 38.34136227455633}]}
{"instance_id": "74f727f06fa0", "generation_result": [{"code": "def _get_chunk_data(self, inputs: Iterable, chunk_size: int)\n\t\n    chunk_data = []\n    for idx, input in enumerate(inputs):\n        if isinstance(input, dict):\n            input = [input]\n        if idx % chunk_size == 0:\n            chunk_data.append(input)\n        else:\n            chunk_data[-1].append(input)\n    for idx, chunk_data_item in enumerate(chunk_data):\n        if idx == len(chunk_data) - 1 and len(chunk_data_item) < chunk_size:\n            yield chunk_data_item\n        else:\n            yield chunk_data_item\n\n", "CodeBleu_score": 27.053550861085412}, {"code": "def _get_chunk_data(self, inputs: Iterable, chunk_size: int)\n\t\n    chunk_data = []\n    for input_data in inputs:\n        if isinstance(input_data, dict):\n            if 'img' in input_data:\n                input_data = self.pipeline(input_data)\n            elif 'img_path' in input_data:\n                input_data = self.pipeline(self.load_image(input_data['img_path']))\n            else:\n                raise TypeError('input data must contain an `img` or `img_path` key')\n        else:\n            input_data = self.pipeline(input_data)\n        chunk_data.append(input_data)\n        if len(chunk_data) == chunk_size:\n            yield chunk_data\n            chunk_data = []\n    if chunk_data:\n        yield chunk_data\n\n", "CodeBleu_score": 38.36137706481544}, {"code": "def _get_chunk_data(self, inputs: Iterable, chunk_size: int)\n\t\n    chunk_data = []\n    for idx, input in enumerate(inputs):\n        if idx % chunk_size == 0:\n            yield chunk_data\n            chunk_data = []\n        chunk_data.append(input)\n    if chunk_data:\n        yield chunk_data\n\n", "CodeBleu_score": 20.525146323932574}]}
{"instance_id": "a43ab51afa59", "generation_result": [{"code": "def postprocess(\n        self,\n        preds: PredType,\n        visualization: Optional[List[np.ndarray]] = None,\n        return_datasamples: bool = False,\n        print_result: bool = False,\n        no_save_pred: bool = False,\n        pred_out_dir: str = '',\n        **kwargs,\n    ) -> Dict\n\t\n    # initialize output directory\n    if pred_out_dir:\n        if not isdir(pred_out_dir):\n            os.makedirs(pred_out_dir, exist_ok=True)\n\n    # prepare output dictionary\n    results = dict(\n        predictions=[],\n        visualization=visualization,\n    )\n\n    # convert predictions to dictionaries and append them to results\n    if not return_datasamples:\n        for pred in preds:\n            results['predictions'].append(self.convert_pred_to_dict(pred))\n\n    # issue a warning if saving datasamples is not supported\n    elif pred_out_dir:\n        warnings.warn(\n            'Saving datasamples is not supported. '\n            'Please set return_datasamples to False.'\n        )\n\n    # print results if required\n    if print_result:\n        self.print_results(results)\n\n    # save predictions if required\n    if not no_save_pred:\n        self.save_preds(results, pred_out_dir)\n\n    return results\n\n", "CodeBleu_score": 49.845170468172896}, {"code": "def postprocess(\n        self,\n        preds: PredType,\n        visualization: Optional[List[np.ndarray]] = None,\n        return_datasamples: bool = False,\n        print_result: bool = False,\n        no_save_pred: bool = False,\n        pred_out_dir: str = '',\n        **kwargs,\n    ) -> Dict\n\t\n    if not no_save_pred:\n        if pred_out_dir:\n            mmcv.mkdir_or_exist(pred_out_dir)\n        else:\n            pred_out_dir = self.out_dir\n    else:\n        pred_out_dir = ''\n\n    results = []\n    if return_datasamples:\n        if pred_out_dir:\n            warnings.warn(\n                'Saving datasamples is not supported. '\n                'Please set no_save_pred=True to avoid this warning.'\n            )\n        results = []\n    else:\n        results = [{} for _ in range(len(preds))]\n\n    if visualization is None:\n        visualization = [None] * len(preds)\n\n    for i, (pred, vis) in enumerate(zip(preds, visualization)):\n        if return_datasamples:\n            results.append(pred)\n        else:\n            results[i].update(pred)\n\n        if vis is not None:\n            results[i]['visualization'] = vis\n\n    if print_result:\n        self.print_result(results, **kwargs)\n\n    return results\n\n", "CodeBleu_score": 41.573914821084436}, {"code": "def postprocess(\n        self,\n        preds: PredType,\n        visualization: Optional[List[np.ndarray]] = None,\n        return_datasamples: bool = False,\n        print_result: bool = False,\n        no_save_pred: bool = False,\n        pred_out_dir: str = '',\n        **kwargs,\n    ) -> Dict\n\t\n    if not no_save_pred:\n        if pred_out_dir:\n            mmcv.mkdir_or_exist(pred_out_dir)\n        else:\n            pred_out_dir = self.cfg.work_dir\n\n    if return_datasamples:\n        if pred_out_dir:\n            warnings.warn('Saving datasamples is not supported.')\n            return_datasamples = False\n        else:\n            warnings.warn('Saving datasamples is not supported. '\n                          'Prediction results will be saved as files.')\n\n    results = []\n    if return_datasamples:\n        for i, data in enumerate(self.data_loader):\n            if i >= len(preds):\n                break\n            results.append(data)\n            results[-1]['pred_bboxes'] = preds[i]['bboxes']\n            results[-1]['pred_labels'] = preds[i]['labels']\n            results[-1]['pred_scores'] = preds[i]['scores']\n            if 'bboxes_ignore' in preds[i]:\n                results[-1]['pred_bboxes_ignore'] = preds[i]['bboxes_ignore']\n            if 'masks' in preds[i]:\n                results[-1]['pred_masks'] = preds[i]['masks']\n            if 'seg_map' in preds[i]:\n                results[-1]['pred_seg_map'] = preds[i]['seg_map']\n            if 'keypoints' in preds[i]:\n                results[-1]['pred_keypoints'] = preds[i]['keypoints']\n            if 'track_ids' in preds[i]:\n                results[-1]['pred_track_ids'] = preds[i]['track_ids']\n            if 'track_scores' in preds[i]:\n                results[-1]['pred_track_scores'] = preds[i]['track_scores']\n            if 'bboxes_ignore' in preds[i]:\n                results[-1]['pred_bboxes_ignore'] = preds[i]['bboxes_ignore']\n            if 'gt_labels' in data:", "CodeBleu_score": 34.584447333322046}]}
{"instance_id": "a3c2561f199c", "generation_result": [{"code": "def build_test_pipeline(cfg: ConfigType) -> ConfigType\n\t\n    if cfg.get('test_pipeline', None) is None:\n        raise ValueError('test_pipeline is not specified in the configuration file.')\n    test_pipeline = copy.deepcopy(cfg.test_pipeline)\n    first_transform = test_pipeline[0]\n    last_transform = test_pipeline[-1]\n    if first_transform.get('type', None) != 'Resize':\n        raise ValueError('The first transform in the test pipeline must be Resize.')\n    if last_transform.get('type', None) != 'ToTensor':\n        raise ValueError('The last transform in the test pipeline must be ToTensor.')\n    first_transform.pop('type')\n    last_transform.pop('type')\n    test_pipeline = Compose([first_transform, last_transform])\n    return test_pipeline\n\n", "CodeBleu_score": 32.25663884207029}, {"code": "def build_test_pipeline(cfg: ConfigType) -> ConfigType\n\t\n    cfg = copy.deepcopy(cfg)\n    transforms = cfg.data.test.pipeline\n    first_transform = transforms[0]\n    last_transform = transforms[-1]\n    first_transform.pop('type')\n    first_transform.pop('transforms')\n    test_pipeline = [first_transform]\n    test_pipeline.extend(last_transform)\n    cfg.data.test.pipeline = Compose(test_pipeline)\n    return cfg\n\n", "CodeBleu_score": 26.307441075202476}, {"code": "def build_test_pipeline(cfg: ConfigType) -> ConfigType\n\t\n    dataset_pipeline = cfg.data.test.pipeline\n    test_pipeline = copy.deepcopy(dataset_pipeline)\n    # remove the first transform in the dataset pipeline\n    # the first transform in the dataset pipeline is usually `LoadImage`\n    # but we need to check it explicitly\n    first_transform = test_pipeline.pop(0)\n    if first_transform['type'] == 'LoadImageFromFile':\n        first_transform['to_float32'] = False\n        first_transform['color_type'] = 'unchanged'\n        test_pipeline.insert(0, first_transform)\n    elif first_transform['type'] == 'LoadImageFromWebcam':\n        first_transform['to_float32'] = False\n        first_transform['color_type'] = 'unchanged'\n        test_pipeline.insert(0, first_transform)\n    elif first_transform['type'] == 'LoadImageFromFolder':\n        first_transform['to_float32'] = False\n        first_transform['color_type'] = 'unchanged'\n        test_pipeline.insert(0, first_transform)\n    elif first_transform['type'] == 'LoadImageFromMemcached':\n        first_transform['to_float32'] = False\n        first_transform['color_type'] = 'unchanged'\n        test_pipeline.insert(0, first_transform)\n    elif first_transform['type'] == 'LoadImageFromList':\n        first_transform['to_float32'] = False\n        first_transform['color_type'] = 'unchanged'\n        test_pipeline.insert(0, first_transform)\n    else:\n        raise TypeError(\n            'The first transform in the test pipeline must be '\n            '`LoadImageFromFile` or `LoadImageFromWebcam`. '\n            f'But received {first_transform[\"type\"]}.')\n\n    # remove the last transform in the dataset pipeline\n    # the last transform in the dataset pipeline is usually `Normalize`\n    # but we need to check it explicitly\n    last_transform = test_pipeline.pop()\n    if last_transform['type'] == 'Normalize':\n        # add", "CodeBleu_score": 19.735174446643516}]}
{"instance_id": "48a358ab5a38", "generation_result": [{"code": "def inference_mot(model: nn.Module, img: np.ndarray, frame_id: int,\n                  video_len: int) -> SampleList\n\t\n    # construct the input data\n    img_meta = dict(\n        ori_shape=(img.shape[1], img.shape[0]),\n        img_shape=(img.shape[1], img.shape[0]),\n        pad_shape=(img.shape[1], img.shape[0]),\n        frame_id=frame_id,\n        video_id=0,\n        video_len=video_len)\n    img_meta = dict(img_meta, **img_meta)\n    data = dict(img=img, img_meta=img_meta)\n\n    # prepare the data pipeline\n    test_pipeline = copy.deepcopy(model.cfg.data.test.pipeline)\n    test_pipeline = Compose(test_pipeline)\n    if next(model.parameters()).device == torch.device('cpu'):\n        test_pipeline = revert_sync_batchnorm(test_pipeline)\n        test_pipeline.transforms = [\n            x for x in test_pipeline.transforms\n            if not isinstance(x, RoIPool)\n        ]\n\n    # perform inference\n    data = test_pipeline(data)\n    data = default_collate([data])\n    data['img_metas'] = [img_meta]\n    with torch.no_grad():\n        result = model(return_loss=False, rescale=True, **data)\n    return result\n\n", "CodeBleu_score": 42.498777610657555}, {"code": "def inference_mot(model: nn.Module, img: np.ndarray, frame_id: int,\n                  video_len: int) -> SampleList\n\t\n    # Prepare the input data\n    img_meta = dict(\n        ori_shape=(img.shape[1], img.shape[0], 3),\n        frame_id=frame_id,\n        img_id=frame_id,\n        video_len=video_len)\n    data = dict(img=img, img_meta=img_meta)\n    data = test_pipeline(data)\n    # Ensure the model is on the CPU\n    if next(model.parameters()).device != torch.device('cpu'):\n        model = model.cpu()\n        model.eval()\n    # Perform forward pass\n    with torch.no_grad():\n        result = model(return_loss=False, rescale=True, **data)\n    return result\n\n", "CodeBleu_score": 33.180109899044965}, {"code": "def inference_mot(model: nn.Module, img: np.ndarray, frame_id: int,\n                  video_len: int) -> SampleList\n\t\n    from mmdet.datasets import build_dataset\n    from mmdet.datasets.pipelines import Compose\n\n    model.eval()\n    if not isinstance(img, np.ndarray):\n        raise TypeError('img must be numpy array. Got {}'.format(type(img)))\n\n    if img.ndim != 3:\n        raise ValueError('img must be 3-dimensional. Got {}-dimensional.'.format(img.ndim))\n\n    if img.dtype != np.uint8:\n        raise ValueError('img must be np.uint8 type. Got {}'.format(img.dtype))\n\n    if img.shape[2] != 3:\n        raise ValueError('img must have 3 channels. Got {}'.format(img.shape[2]))\n\n    # build the data pipeline\n    cfg = model.cfg\n    test_pipeline = Compose(cfg.test_pipeline)\n    # prepare data\n    data = dict(img=img, frame_id=frame_id, video_len=video_len)\n    data = test_pipeline(data)\n    data = collate([data], samples_per_gpu=1)\n    if next(model.parameters()).is_cuda:\n        # scatter to specified GPU\n        data = scatter(data, [cfg.gpu_ids[0]])[0]\n    else:\n        # just get the actual data from DataContainer\n        data['img_metas'] = data['img_metas'][0].data\n        data['img'] = data['img'][0].data\n\n    with torch.no_grad():\n        # forward the model\n        result = model(return_loss=False, rescale=True, **data)\n    return result\n\n", "CodeBleu_score": 44.20903593827175}]}
{"instance_id": "6c4ea2ce1787", "generation_result": [{"code": "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\n\t\n    while True:\n        total_length = len(tokens_a) + len(tokens_b)\n        if total_length <= max_num_tokens:\n            break\n\n        trunc_tokens = tokens_a if len(tokens_a) > len(tokens_b) else tokens_b\n        assert len(trunc_tokens) >= 1\n\n        # We want to sometimes truncate from the front and sometimes from the\n        # back to add more randomness and avoid biases.\n        if rng.random() < 0.5:\n            del trunc_tokens[0]\n        else:\n            trunc_tokens.pop()\n\n", "CodeBleu_score": 78.49275840258514}, {"code": "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\n\t\n    while True:\n        total_length = len(tokens_a) + len(tokens_b)\n        if total_length <= max_num_tokens:\n            break\n\n        trunc_tokens = tokens_a if len(tokens_a) > len(tokens_b) else tokens_b\n        assert len(trunc_tokens) >= 1\n\n        # We want to sometimes truncate from the front and sometimes from the\n        # back to add more randomness and avoid biases.\n        if rng.random() < 0.5:\n            del trunc_tokens[0]\n        else:\n            trunc_tokens.pop()\n\n", "CodeBleu_score": 78.49275840258514}, {"code": "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\n\t\n    while True:\n        total_length = len(tokens_a) + len(tokens_b)\n        if total_length <= max_num_tokens:\n            break\n\n        trunc_tokens = tokens_a if len(tokens_a) > len(tokens_b) else tokens_b\n        assert len(trunc_tokens) >= 1\n\n        # We want to sometimes truncate from the front and sometimes from the\n        # back to add more randomness and avoid biases.\n        if rng.random() < 0.5:\n            del trunc_tokens[0]\n        else:\n            trunc_tokens.pop()\n\n", "CodeBleu_score": 78.49275840258514}]}
{"instance_id": "007181b6729d", "generation_result": [{"code": "def get_activation(activation_string)\n\t\n    if isinstance(activation_string, six.string_types):\n        if not activation_string:\n            return None\n        act = activation_string.lower()\n        if act == \"linear\":\n            return None\n        elif act == \"relu\":\n            return tf.nn.relu\n        elif act == \"leaky_relu\":\n            return tf.nn.leaky_relu\n        elif act == \"tanh\":\n            return tf.nn.tanh\n        elif act == \"sigmoid\":\n            return tf.nn.sigmoid\n        elif act == \"softmax\":\n            return tf.nn.softmax\n        elif act == \"softplus\":\n            return tf.nn.softplus\n        elif act == \"elu\":\n            return tf.nn.elu\n        elif act == \"selu\":\n            return tf.nn.selu\n        elif act == \"softsign\":\n            return tf.nn.softsign\n        elif act == \"crelu\":\n            return tf.nn.crelu\n        elif act == \"lrelu\":\n            return tf.nn.lrelu\n        elif act == \"relu6\":\n            return tf.nn.relu6\n        elif act == \"relu_x\":\n            return tf.nn.relu_x\n        elif act == \"relu_x_x\":\n            return tf.nn.relu_x_x\n        elif act == \"relu_x_x_x\":\n            return tf.nn.relu_x_x_x\n        elif act == \"relu_x_x_x_x\":\n            return tf.nn.relu_x_x_x_x\n        elif act == \"relu_x_x_x_x_x\":\n            return tf.nn.relu_x_x_x_x_x\n        elif act == \"relu_x_x_x_x_x_x\":\n            return tf.nn.relu_x_x_x_x_x_x\n        elif act == \"relu_x_x_x_x_x_x_x\":\n            return tf.nn.relu_x_x_x_", "CodeBleu_score": 56.26722755960814}, {"code": "def get_activation(activation_string)\n\t\n    if not isinstance(activation_string, six.string_types):\n        return activation_string\n    if not activation_string:\n        return None\n    act = activation_string.lower()\n    if act == 'linear':\n        return None\n    elif act == 'relu':\n        return tf.nn.relu\n    elif act == 'elu':\n        return tf.nn.elu\n    elif act == 'tanh':\n        return tf.tanh\n    else:\n        raise ValueError('Unsupported activation: {}'.format(act))\n\n", "CodeBleu_score": 74.87029497560374}, {"code": "def get_activation(activation_string)\n\t\n    if not isinstance(activation_string, six.string_types):\n        return activation_string\n    if not activation_string:\n        return None\n    act = activation_string.lower()\n    if act == \"linear\":\n        return None\n    elif act == \"relu\":\n        return \"relu\"\n    elif act == \"sigmoid\":\n        return \"sigmoid\"\n    elif act == \"tanh\":\n        return \"tanh\"\n    elif act == \"softmax\":\n        return \"softmax\"\n    elif act == \"softplus\":\n        return \"softplus\"\n    elif act == \"softsign\":\n        return \"softsign\"\n    elif act == \"relu6\":\n        return \"relu6\"\n    elif act == \"elu\":\n        return \"elu\"\n    elif act == \"selu\":\n        return \"selu\"\n    elif act == \"softmax\":\n        return \"softmax\"\n    elif act == \"softmax2\":\n        return \"softmax2\"\n    elif act == \"log_sigmoid\":\n        return \"log_sigmoid\"\n    elif act == \"hard_sigmoid\":\n        return \"hard_sigmoid\"\n    elif act == \"swish\":\n        return \"swish\"\n    elif act == \"mish\":\n        return \"mish\"\n    elif act == \"gelu\":\n        return \"gelu\"\n    elif act == \"prelu\":\n        return \"prelu\"\n    elif act == \"leaky_relu\":\n        return \"leaky_relu\"\n    elif act == \"thresholded_relu\":\n        return \"thresholded_relu\"\n    elif act == \"softshrink\":\n        return \"softshrink\"\n    elif act == \"tanhshrink\":\n        return \"tanhshrink\"\n    elif act == \"hardshrink\":\n        return \"hardshrink\"\n    elif act == \"hardtanh\":\n        return \"hardtanh\"\n    elif act == \"log_softmax\":\n        return \"log_softmax\"\n    elif act == \"rrelu\":\n        return \"rrel", "CodeBleu_score": 58.965828608947454}]}
{"instance_id": "d5d6e0315899", "generation_result": [{"code": "def dropout(input_tensor, dropout_prob)\n\t\n    if dropout_prob is None or dropout_prob == 0.0:\n        return input_tensor\n    return tf.nn.dropout(input_tensor, 1.0 - dropout_prob)\n\n", "CodeBleu_score": 60.78403505639335}, {"code": "def dropout(input_tensor, dropout_prob)\n\t\n    if dropout_prob is None or dropout_prob == 0.0:\n        return input_tensor\n    return tf.nn.dropout(input_tensor, 1.0 - dropout_prob)\n\n", "CodeBleu_score": 60.78403505639335}, {"code": "def dropout(input_tensor, dropout_prob)\n\t\n    if dropout_prob is None or dropout_prob == 0.0:\n        return input_tensor\n    return tf.nn.dropout(input_tensor, 1.0 - dropout_prob)\n\n", "CodeBleu_score": 60.78403505639335}]}
{"instance_id": "828917b9ac53", "generation_result": [{"code": "def create_attention_mask_from_input_mask(from_tensor, to_mask)\n\t\n    from_shape = from_tensor.shape\n    batch_size = from_shape[0]\n    from_seq_length = from_shape[1]\n    to_shape = to_mask.shape\n    to_seq_length = to_shape[1]\n    to_mask = tf.cast(\n        tf.reshape(to_mask, (batch_size, 1, to_seq_length)), tf.float32)\n    return tf.ones((batch_size, from_seq_length, to_seq_length)) * to_mask\n\n", "CodeBleu_score": 30.83471178539609}, {"code": "def create_attention_mask_from_input_mask(from_tensor, to_mask)\n\t\n    from_shape = from_tensor.shape\n    batch_size = from_shape[0]\n    from_seq_length = from_shape[1]\n\n    to_shape = to_mask.shape\n    to_seq_length = to_shape[1]\n\n    to_mask = tf.cast(\n        tf.reshape(to_mask, (batch_size, 1, to_seq_length)), tf.float32)\n\n    # We don't assume that `from_tensor` is a mask (although it could be). We\n    # don't actually care if we attend *from* padding tokens (only *to* padding)\n    # tokens so we create a tensor of all ones.\n    #\n    # `broadcast_ones` = [batch_size, from_seq_length, 1]\n    broadcast_ones = tf.ones(\n        shape=(batch_size, from_seq_length, 1), dtype=tf.float32)\n\n    # Here we broadcast along two dimensions to create the mask.\n    mask = broadcast_ones * to_mask\n\n    return mask\n\n", "CodeBleu_score": 45.24516635871454}, {"code": "def create_attention_mask_from_input_mask(from_tensor, to_mask)\n\t\n    from_shape = from_tensor.shape\n    to_shape = to_mask.shape\n    # broadcast to [batch_size, from_seq_length, 1, 1, to_seq_length]\n    to_mask = tf.cast(to_mask, dtype=tf.float32)\n    to_mask = tf.reshape(to_mask, (to_shape[0], 1, 1, to_shape[1]))\n    # broadcast to [batch_size, from_seq_length, to_seq_length]\n    # to_mask = tf.broadcast_to(to_mask, (from_shape[0], from_shape[1], to_shape[1]))\n    to_mask = tf.broadcast_to(to_mask, (from_shape[0], from_shape[1], to_shape[1]))\n    return to_mask\n\n", "CodeBleu_score": 20.09274868001559}]}
{"instance_id": "d7798e41a723", "generation_result": [{"code": "def transpose_for_scores(input_tensor, batch_size, num_attention_heads,\n                           seq_length, width)\n\t\n    output_tensor = input_tensor.reshape(\n        [batch_size, seq_length, num_attention_heads, width])\n    output_tensor = output_tensor.transpose([0, 2, 1, 3])\n    return output_tensor\n\n", "CodeBleu_score": 46.22669641176388}, {"code": "def transpose_for_scores(input_tensor, batch_size, num_attention_heads,\n                           seq_length, width)\n\t\n    output_tensor = input_tensor\n\n    if len(output_tensor.shape) != 3:\n        raise ValueError(\"The input tensor must be a 3D tensor of shape [batch_size, seq_length, width].\")\n\n    batch_size = output_tensor.shape[0]\n    seq_length = output_tensor.shape[1]\n    width = output_tensor.shape[2]\n\n    output_tensor = output_tensor.reshape(batch_size, seq_length, num_attention_heads, width)\n\n    return output_tensor.transpose(0, 2, 1, 3)\n", "CodeBleu_score": 32.505582381432056}, {"code": "def transpose_for_scores(input_tensor, batch_size, num_attention_heads,\n                           seq_length, width)\n\t\n    output_tensor = input_tensor\n\n    if len(input_tensor.shape) != 3:\n        raise ValueError(\"Wrong shape for input tensor: {0}\".format(input_tensor.shape))\n    if batch_size is None:\n        batch_size = -1\n    if seq_length is None:\n        seq_length = -1\n    if width is None:\n        width = -1\n\n    if batch_size != -1:\n        output_tensor = output_tensor.view(batch_size, seq_length, num_attention_heads, width)\n    else:\n        batch_size = output_tensor.size(0)\n    if seq_length != -1:\n        output_tensor = output_tensor.view(batch_size, seq_length, num_attention_heads, width)\n    else:\n        seq_length = output_tensor.size(1)\n    if width != -1:\n        output_tensor = output_tensor.view(batch_size, seq_length, num_attention_heads, width)\n    else:\n        width = output_tensor.size(3)\n\n    output_tensor = output_tensor.permute(0, 2, 1, 3)\n    return output_tensor\n\n", "CodeBleu_score": 30.489422954488766}]}
{"instance_id": "711d466ace09", "generation_result": [{"code": "def reshape_to_matrix(input_tensor)\n\t\n    if len(input_tensor.get_shape()) < 2:\n        raise ValueError(\"The tensor must have rank at least 2. Received input with shape: \" + str(input_tensor.get_shape()))\n    if len(input_tensor.get_shape()) == 2:\n        return input_tensor\n    else:\n        width = input_tensor.get_shape()[-1]\n        output_tensor = tf.reshape(input_tensor, [-1, width])\n        return output_tensor\n\n", "CodeBleu_score": 58.16464612664504}, {"code": "def reshape_to_matrix(input_tensor)\n\t\n    if len(input_tensor.shape) < 2:\n        raise ValueError('Input tensor must have rank at least 2.')\n    if len(input_tensor.shape) == 2:\n        return input_tensor\n    width = input_tensor.shape[-1]\n    output_tensor = tf.reshape(input_tensor, [-1, width])\n    return output_tensor\n\n", "CodeBleu_score": 62.25867677675316}, {"code": "def reshape_to_matrix(input_tensor)\n\t\n    input_tensor_shape = input_tensor.get_shape().as_list()\n    if len(input_tensor_shape) < 2:\n        raise ValueError(\"input_tensor must have at least 2 dimensions.\")\n    if len(input_tensor_shape) == 2:\n        return input_tensor\n    else:\n        width = input_tensor_shape[-1]\n        output_tensor = tf.reshape(input_tensor, [-1, width])\n        return output_tensor\n\n", "CodeBleu_score": 52.61527437048855}]}
{"instance_id": "7adcd08f531e", "generation_result": [{"code": "def get_train_examples(self, data_dir)\n\t\n    lines = []\n    with open(os.path.join(data_dir, \"train.tsv\"), \"r\") as f:\n        reader = csv.reader(f, delimiter=\"\\t\")\n        for line in reader:\n            lines.append(line)\n    examples = []\n    for (i, line) in enumerate(lines):\n        guid = \"%s-%s\" % (self.name, i)\n        text_a = tokenization.convert_to_unicode(line[0])\n        label = tokenization.convert_to_unicode(line[1])\n        if label == \"contradictory\":\n            label = \"contradiction\"\n        examples.append(\n            InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n    return examples\n\n", "CodeBleu_score": 47.63170885937137}, {"code": "def get_train_examples(self, data_dir)\n\t\n    train_examples = []\n    with open(os.path.join(data_dir, \"train.tsv\"), \"r\") as f:\n        reader = csv.reader(f, delimiter=\"\\t\", quotechar=None)\n        next(reader)\n        for line in reader:\n            if sys.version_info[0] == 2:\n                text_a = unicode(line[3], \"utf-8\")\n                text_b = unicode(line[4], \"utf-8\")\n                label = unicode(line[5], \"utf-8\")\n            else:\n                text_a = line[3]\n                text_b = line[4]\n                label = line[5]\n            examples = self.create_examples(\n                [text_a, text_b], [label], \"train\"\n            )\n            train_examples.extend(examples)\n    return train_examples\n\n", "CodeBleu_score": 29.803153660336946}, {"code": "def get_train_examples(self, data_dir)\n\t\n    train_examples = []\n    with open(os.path.join(data_dir, \"train.tsv\"), \"r\", encoding=\"utf-8\") as f:\n        reader = csv.reader(f, delimiter=\"\\t\", quotechar=None)\n        for line in reader:\n            if sys.version_info[0] == 2:\n                line = list(unicode(cell, \"utf-8\") for cell in line)\n            text_a = line[3]\n            text_b = line[4]\n            label = line[5]\n            examples = self.create_examples(\n                text_a, text_b, label, \"train\"\n            )\n            train_examples.extend(examples)\n        return train_examples\n\n", "CodeBleu_score": 26.772595993189256}]}
{"instance_id": "26a0bb96dab9", "generation_result": [{"code": "def metric_fn(per_example_loss, label_ids, logits, is_real_example)\n\t\n    predictions = tf.argmax(logits, axis=-1, output_type=tf.int32)\n    accuracy = tf.metrics.accuracy(label_ids, predictions, weights=is_real_example)\n    loss = tf.metrics.mean(per_example_loss, weights=is_real_example)\n    return {\n        \"eval_accuracy\": accuracy,\n        \"eval_loss\": loss,\n    }\n\n", "CodeBleu_score": 54.387272564649315}, {"code": "def metric_fn(per_example_loss, label_ids, logits, is_real_example)\n\t\n    predictions = logits\n    predictions = predictions[:, 1]\n    label_ids = label_ids[:, 1]\n    accuracy = tf.metrics.accuracy(label_ids, predictions, weights=is_real_example)\n    loss = tf.metrics.mean(per_example_loss, weights=is_real_example)\n    return {\n        \"eval_accuracy\": accuracy,\n        \"eval_loss\": loss,\n    }\n\n", "CodeBleu_score": 56.928509945047765}, {"code": "def metric_fn(per_example_loss, label_ids, logits, is_real_example)\n\t\n    predictions = tf.argmax(logits, axis=-1, output_type=tf.int32)\n    accuracy = tf.metrics.accuracy(label_ids, predictions, weights=is_real_example)\n    loss = tf.metrics.mean(per_example_loss, weights=is_real_example)\n    return {\n        \"eval_accuracy\": accuracy,\n        \"eval_loss\": loss,\n    }\n\n", "CodeBleu_score": 54.387272564649315}]}
{"instance_id": "6154e4d28509", "generation_result": [{"code": "def convert_examples_to_features(examples, label_list, max_seq_length,\n                                 tokenizer)\n\t\n    features = []\n    for (ex_index, example) in enumerate(examples):\n        if ex_index % 10000 == 0:\n            print('Writing example %d of %d' % (ex_index, len(examples)))\n\n        feature = convert_single_example(ex_index, example, label_list,\n                                         max_seq_length, tokenizer)\n\n        features.append(feature)\n    return features\n\n", "CodeBleu_score": 71.35287453322118}, {"code": "def convert_examples_to_features(examples, label_list, max_seq_length,\n                                 tokenizer)\n\t\n    features = []\n    for (ex_index, example) in enumerate(examples):\n        if ex_index % 10000 == 0:\n            print(\"Writing example %d of %d\" % (ex_index, len(examples)))\n\n        feature = convert_single_example(ex_index, example, label_list,\n                                         max_seq_length, tokenizer)\n\n        features.append(feature)\n    return features\n\n", "CodeBleu_score": 73.68693175472625}, {"code": "def convert_examples_to_features(examples, label_list, max_seq_length,\n                                 tokenizer)\n\t\n    features = []\n    for (ex_index, example) in enumerate(examples):\n        if ex_index % 10000 == 0:\n            print(\"Writing example %d of %d\" % (ex_index, len(examples)))\n        feature = convert_single_example(example, label_list, max_seq_length, tokenizer)\n        features.append(feature)\n    return features\n\n", "CodeBleu_score": 66.85809553613763}]}
{"instance_id": "e09c0db71ebb", "generation_result": [{"code": "def get_next_sentence_output(bert_config, input_tensor, labels)\n\t\n    with tf.variable_scope(\"cls/seq_relationship\"):\n        output_weights = tf.get_variable(\n            \"output_weights\",\n            shape=[2, bert_config.hidden_size],\n            initializer=modeling.create_initializer(bert_config.initializer_range))\n        output_bias = tf.get_variable(\n            \"output_bias\", shape=[2], initializer=tf.zeros_initializer())\n\n        logits = tf.matmul(input_tensor, output_weights, transpose_b=True)\n        logits = tf.nn.bias_add(logits, output_bias)\n        log_probs = tf.nn.log_softmax(logits, axis=-1)\n\n        labels = tf.reshape(labels, [-1])\n        one_hot_labels = tf.one_hot(labels, depth=2, dtype=tf.float32)\n\n        per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n        loss = tf.reduce_mean(per_example_loss)\n        return (loss, per_example_loss, log_probs)\n\n", "CodeBleu_score": 86.94900271179796}, {"code": "def get_next_sentence_output(bert_config, input_tensor, labels)\n\t\n    with tf.variable_scope(\"cls/seq_relationship\"):\n        output_weights = tf.get_variable(\n            \"output_weights\",\n            shape=[2, bert_config.hidden_size],\n            initializer=modeling.create_initializer(\n                bert_config.initializer_range))\n        output_bias = tf.get_variable(\n            \"output_bias\", shape=[2], initializer=tf.zeros_initializer())\n\n        logits = tf.matmul(input_tensor, output_weights, transpose_b=True)\n        logits = tf.nn.bias_add(logits, output_bias)\n        log_probs = tf.nn.log_softmax(logits, axis=-1)\n\n        one_hot_labels = tf.one_hot(labels, depth=2, dtype=tf.float32)\n        per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n        loss = tf.reduce_mean(per_example_loss)\n        return (loss, per_example_loss, log_probs)\n\n", "CodeBleu_score": 82.71025874056485}, {"code": "def get_next_sentence_output(bert_config, input_tensor, labels)\n\t\n    with tf.variable_scope(\"cls/seq_relationship\"):\n        output_weights = tf.get_variable(\n            \"output_weights\",\n            shape=[2, bert_config.hidden_size],\n            initializer=modeling.create_initializer(bert_config.initializer_range))\n        output_bias = tf.get_variable(\n            \"output_bias\", shape=[2], initializer=tf.zeros_initializer())\n\n        logits = tf.matmul(input_tensor, output_weights, transpose_b=True)\n        logits = tf.nn.bias_add(logits, output_bias)\n        log_probs = tf.nn.log_softmax(logits, axis=-1)\n\n        labels = tf.reshape(labels, [-1])\n        one_hot_labels = tf.one_hot(labels, depth=2, dtype=tf.float32)\n\n        per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n        loss = tf.reduce_mean(per_example_loss)\n\n        return loss, per_example_loss, log_probs\n\n", "CodeBleu_score": 84.97588967239635}]}
{"instance_id": "1f66b020f485", "generation_result": [{"code": "def gather_indexes(sequence_tensor, positions)\n\t\n    sequence_shape = modeling.get_shape_list(sequence_tensor, expected_rank=3)\n    batch_size = sequence_shape[0]\n    seq_length = sequence_shape[1]\n    width = sequence_shape[2]\n\n    flat_offsets = batch_size * seq_length * tf.range(width)\n    flat_positions = tf.reshape(positions + flat_offsets, [-1])\n    flat_sequence_tensor = tf.reshape(sequence_tensor, [batch_size * seq_length, width])\n    output_tensor = tf.gather(flat_sequence_tensor, flat_positions)\n\n    return output_tensor\n\n", "CodeBleu_score": 46.45802866456157}, {"code": "def gather_indexes(sequence_tensor, positions)\n\t\n    sequence_shape = modeling.get_shape_list(sequence_tensor, expected_rank=3)\n    batch_size = sequence_shape[0]\n    seq_length = sequence_shape[1]\n    width = sequence_shape[2]\n\n    flat_offsets = batch_size * seq_length * width\n    flat_positions = tf.range(flat_offsets, flat_offsets + positions.shape[0], dtype=tf.int32)\n    flat_sequence_tensor = tf.reshape(sequence_tensor, [batch_size * seq_length, width])\n    output_tensor = tf.gather(flat_sequence_tensor, flat_positions)\n    return output_tensor\n\n", "CodeBleu_score": 46.22147197502393}, {"code": "def gather_indexes(sequence_tensor, positions)\n\t\n    sequence_shape = modeling.get_shape_list(sequence_tensor, expected_rank=3)\n    batch_size = sequence_shape[0]\n    seq_length = sequence_shape[1]\n    width = sequence_shape[2]\n\n    flat_offsets = batch_size * seq_length * list(positions)\n    flat_positions = list(\n        map(lambda i: i + flat_offsets,\n            list(range(0, batch_size * seq_length))))\n    flat_positions = tf.reshape(flat_positions, [-1])\n    flat_sequence_tensor = tf.reshape(sequence_tensor,\n                                      [batch_size * seq_length, width])\n    output_tensor = tf.gather(flat_sequence_tensor, flat_positions)\n    return output_tensor\n\n", "CodeBleu_score": 43.33373959624774}]}
{"instance_id": "956bffee9a33", "generation_result": [{"code": "def _decode_record(record, name_to_features)\n\t\n    example = tf.parse_single_example(record, name_to_features)\n\n    # tf.Example only supports tf.int64, but TPU only supports tf.int32.\n    # So cast all int64 to int32.\n    for name in list(example.keys()):\n        t = example[name]\n        if t.dtype == tf.int64:\n            t = tf.to_int32(t)\n        example[name] = t\n\n    return example\n\n", "CodeBleu_score": 71.32095143473128}, {"code": "def _decode_record(record, name_to_features)\n\t\n    example = tf.parse_single_example(record, name_to_features)\n\n    # tf.Example only supports tf.int64, but TPUs only support tf.int32.\n    # So cast all int64 to int32.\n    for name in list(example.keys()):\n        t = example[name]\n        if t.dtype == tf.int64:\n            t = tf.to_int32(t)\n        example[name] = t\n\n    return example\n\n", "CodeBleu_score": 71.32095143473128}, {"code": "def _decode_record(record, name_to_features)\n\t\n    example = tf.io.parse_single_example(record, name_to_features)\n    for name in list(example.keys()):\n        t = example[name]\n        if t.dtype == tf.int64:\n            t = tf.cast(t, tf.int32)\n        example[name] = t\n    return example\n\n", "CodeBleu_score": 59.0051892348668}]}
{"instance_id": "6b97f354e641", "generation_result": [{"code": "def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer,\n                         orig_answer_text)\n\t\n    tok_answer_text = \" \".join(tokenizer.tokenize(orig_answer_text))\n\n    for new_start in range(input_start, input_end + 1):\n        for new_end in range(input_end, new_start - 1, -1):\n            text_span = \" \".join(doc_tokens[new_start:(new_end + 1)])\n            if text_span == tok_answer_text:\n                return new_start, new_end\n\n    return input_start, input_end\n\n", "CodeBleu_score": 69.51763050033676}, {"code": "def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer,\n                         orig_answer_text)\n\t\n    tok_answer_text = \" \".join(tokenizer.tokenize(orig_answer_text))\n\n    for new_start in range(input_start, input_end + 1):\n        for new_end in range(input_end, new_start - 1, -1):\n            text_span = \" \".join(doc_tokens[new_start:(new_end + 1)])\n            if text_span == tok_answer_text:\n                return new_start, new_end\n\n    return input_start, input_end\n\n", "CodeBleu_score": 69.51763050033676}, {"code": "def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer,\n                         orig_answer_text)\n\t\n    tok_answer_text = \" \".join(tokenizer.tokenize(orig_answer_text))\n\n    for new_start in range(input_start, input_end + 1):\n        for new_end in range(input_end, new_start - 1, -1):\n            text_span = \" \".join(doc_tokens[new_start:(new_end + 1)])\n            if text_span == tok_answer_text:\n                return new_start, new_end\n\n    return input_start, input_end\n\n", "CodeBleu_score": 69.51763050033676}]}
{"instance_id": "aba334b92b34", "generation_result": [{"code": "def _check_is_max_context(doc_spans, cur_span_index, position)\n\t\n    best_score = None\n    best_span_index = None\n    for (span_index, doc_span) in enumerate(doc_spans):\n        end = doc_span.start + doc_span.length - 1\n        if position < doc_span.start:\n            continue\n        if position > end:\n            continue\n        num_left_context = position - doc_span.start\n        num_right_context = end - position\n        score = min(num_left_context, num_right_context) + 0.01 * doc_span.length\n        if best_score is None or score > best_score:\n            best_score = score\n            best_span_index = span_index\n\n    return cur_span_index == best_span_index\n\n", "CodeBleu_score": 78.23889503357583}, {"code": "def _check_is_max_context(doc_spans, cur_span_index, position)\n\t\n    best_score = None\n    best_span_index = None\n    for (span_index, doc_span) in enumerate(doc_spans):\n        end = doc_span.start + doc_span.length - 1\n        if position < doc_span.start:\n            continue\n        if position > end:\n            continue\n        num_left_context = position - doc_span.start\n        num_right_context = end - position\n        score = min(num_left_context, num_right_context) + 0.01 * doc_span.length\n        if best_score is None or score > best_score:\n            best_score = score\n            best_span_index = span_index\n\n    return cur_span_index == best_span_index\n\n", "CodeBleu_score": 78.23889503357583}, {"code": "def _check_is_max_context(doc_spans, cur_span_index, position)\n\t\n    best_score = None\n    best_span_index = None\n    for (span_index, doc_span) in enumerate(doc_spans):\n        end = doc_span.start + doc_span.length - 1\n        if position < doc_span.start:\n            continue\n        if position > end:\n            continue\n        num_left_context = position - doc_span.start\n        num_right_context = end - position\n        score = min(num_left_context, num_right_context) + 0.01 * doc_span.length\n        if best_score is None or score > best_score:\n            best_score = score\n            best_span_index = span_index\n\n    return cur_span_index == best_span_index\n\n", "CodeBleu_score": 78.23889503357583}]}
{"instance_id": "cf92ad509f6a", "generation_result": [{"code": "def load_vocab(vocab_file)\n\t\n    vocab = collections.OrderedDict()\n    index = 0\n    with open(vocab_file, \"r\", encoding=\"utf-8\") as reader:\n        while True:\n            token = reader.readline()\n            if not token:\n                break\n            token = token.strip()\n            token = six.ensure_text(token, \"utf-8\")\n            vocab[token] = index\n            index += 1\n    return vocab\n\n", "CodeBleu_score": 57.76825467597657}, {"code": "def load_vocab(vocab_file)\n\t\n    vocab = collections.OrderedDict()\n    index = 0\n    with open(vocab_file, \"r\") as reader:\n        while True:\n            token = convert_to_unicode(reader.readline())\n            if not token:\n                break\n            token = token.strip()\n            vocab[token] = index\n            index += 1\n    return vocab\n\n", "CodeBleu_score": 69.79588590161403}, {"code": "def load_vocab(vocab_file)\n\t\n    vocab = collections.OrderedDict()\n    index = 0\n    with open(vocab_file, \"r\", encoding=\"utf8\") as reader:\n        while True:\n            token = reader.readline()\n            if not token:\n                break\n            token = token.strip()\n            vocab[token] = index\n            index += 1\n    return vocab\n\n", "CodeBleu_score": 58.24527325133367}]}
{"instance_id": "6c865803ac12", "generation_result": [{"code": "def convert_by_vocab(vocab, items)\n\t\n    if vocab is None:\n        return items\n    return [vocab[item] for item in items]\n\n", "CodeBleu_score": 25.594639430907844}, {"code": "def convert_by_vocab(vocab, items)\n\t\n\n    if isinstance(items, six.string_types):\n        return vocab[items]\n\n    if isinstance(items, collections.Iterable):\n        return [vocab[item] for item in items]\n\n    raise ValueError(\"Input must be a string or a list of strings.\")\n\n", "CodeBleu_score": 20.762372293811893}, {"code": "def convert_by_vocab(vocab, items)\n\t\n    if not isinstance(vocab, collections.Mapping):\n        vocab = vocab.idx_to_token\n    return [vocab[item] for item in items]\n\n", "CodeBleu_score": 24.762714485739878}]}
{"instance_id": "e93b9200dc4a", "generation_result": [{"code": "def tokenize(self, text)\n\t\n    text = convert_to_unicode(text)\n\n    text = self._clean_text(text)\n\n    orig_tokens = whitespace_tokenize(text)\n    split_tokens = []\n    for token in orig_tokens:\n        if self.do_lower_case:\n            token = token.lower()\n            token = self._run_strip_accents(token)\n        split_tokens.extend(self._run_split_on_punc(token))\n\n    output_tokens = whitespace_tokenize(\" \".join(split_tokens))\n    return output_tokens\n\n", "CodeBleu_score": 31.459048416614245}, {"code": "def tokenize(self, text)\n\t\n    text = self._clean_text(text)\n    orig_tokens = self._basic_tokenizer.tokenize(text)\n    split_tokens = []\n    for token in orig_tokens:\n        if self.do_lower_case:\n            token = token.lower()\n            token = self._run_strip_accents(token)\n        split_tokens.extend(self._run_split_on_punc(token))\n    return split_tokens\n\n", "CodeBleu_score": 28.449637042263653}, {"code": "def tokenize(self, text)\n\t\n    text = self.basic_tokenizer.tokenize(text)\n\n    # For Chinese, we should split words if the word contains Chinese characters.\n    # We use a regex to identify the Chinese characters.\n    # For example, \"\u6211\u662f\u4e2d\u56fd\u4eba\" will be split into \"\u6211\", \"\u662f\", \"\u4e2d\", \"\u56fd\", \"\u4eba\".\n    # Note that this regex will split \"\u4e2d\u56fd\u4eba\" into \"\u4e2d\", \"\u56fd\", \"\u4eba\".\n    # So we will not split Chinese characters in the middle of a word.\n    # For example, \"\u4e2d\u56fd\u4eba\" will not be split into \"\u4e2d\", \"\u56fd\", \"\u4eba\".\n    # The regex will only split Chinese characters that appear in the beginning of a word.\n    # For example, \"\u6211\u662f\u4e2d\u56fd\u4eba\" will be split into \"\u6211\", \"\u662f\", \"\u4e2d\u56fd\u4eba\".\n    # If we don't have this regex, \"\u6211\u662f\u4e2d\u56fd\u4eba\" will be split into \"\u6211\", \"\u662f\", \"\u4e2d\", \"\u56fd\", \"\u4eba\".\n    # Note that this regex will not split Chinese characters that are not in the beginning of a word.\n    # For example, \"\u6211\u662f\u4e2d\u56fd\u4eba\" will not be split into \"\u6211\", \"\u662f\", \"\u4e2d\", \"\u56fd\", \"\u4eba\".\n    # We use the regex to split Chinese characters that appear in the beginning of a word.\n    # For example, \"\u6211\u662f\u4e2d\u56fd\u4eba\" will be split into \"\u6211\", \"\u662f\", \"\u4e2d\u56fd\u4eba\".\n    # If we don't have this regex, \"\u6211\u662f\u4e2d\u56fd\u4eba\" will be split into \"\u6211\", \"\u662f\", \"\u4e2d\", \"\u56fd\", \"\u4eba\".\n    # Note that this regex will not split Chinese characters that appear in the middle of a word.\n    # For example, \"\u6211\u662f\u4e2d\u56fd\u4eba\" will not be split into \"\u6211\", \"\u662f\", \"\u4e2d\", \"\u56fd\", \"\u4eba\".\n    # We use the regex to split Chinese characters that appear in the beginning of a word.\n    # For example, \"\u6211\u662f\u4e2d\u56fd\u4eba\" will be split into \"\u6211\", \"\u662f\", \"\u4e2d\u56fd\u4eba\".\n    # If we don't have this regex, \"\u6211\u662f\u4e2d\u56fd\u4eba\" will be split into \"\u6211\", \"\u662f\", \"\u4e2d\", \"\u56fd\", \"\u4eba\".\n", "CodeBleu_score": 3.2530064079796377}]}
{"instance_id": "2ff6f6db53c9", "generation_result": [{"code": "def safe_load(path, offset, duration, sample_rate, dtype)\n\t\n\n    try:\n        audio_data = load_audio(path, offset, duration, sample_rate, dtype)\n        return audio_data, False\n    except Exception as e:\n        print(f\"{e}\")\n        return -1.0, True\n\n", "CodeBleu_score": 16.42910150184692}, {"code": "def safe_load(path, offset, duration, sample_rate, dtype)\n\t\n    try:\n        audio_data = load(path, offset, duration, sample_rate, dtype)\n        return audio_data, False\n    except Exception as e:\n        print(f\"Error: {e}\")\n        return -1.0, True\n\n", "CodeBleu_score": 16.489955630477485}, {"code": "def safe_load(path, offset, duration, sample_rate, dtype)\n\t\n    try:\n        audio = import_module(\"spleeter.audio.%s\" % dtype)\n        data = audio.load(path, offset, duration, sample_rate)\n        return data, False\n    except Exception as e:\n        print(e)\n        return -1.0, True\n\n", "CodeBleu_score": 24.032926944254513}]}
{"instance_id": "12fbe1f03d8f", "generation_result": [{"code": "def to_n_channels(waveform: tf.Tensor, n_channels: int) -> tf.Tensor\n\t\n    if waveform.shape[1] == n_channels:\n        return waveform\n    elif waveform.shape[1] < n_channels:\n        waveform = tf.tile(waveform, [1, n_channels // waveform.shape[1], 1])\n        waveform = waveform[:, :n_channels, :]\n        return waveform\n    else:\n        waveform = waveform[:, :n_channels, :]\n        return waveform\n\n", "CodeBleu_score": 43.487021698756266}, {"code": "def to_n_channels(waveform: tf.Tensor, n_channels: int) -> tf.Tensor\n\t\n    waveform_channels = waveform.shape[-1]\n    if waveform_channels < n_channels:\n        waveform = tf.tile(waveform, [1, 1, n_channels // waveform_channels])\n    return waveform[:, :, :n_channels]\n\n", "CodeBleu_score": 46.514819447739995}, {"code": "def to_n_channels(waveform: tf.Tensor, n_channels: int) -> tf.Tensor\n\t\n    waveform_channels = waveform.shape[-1]\n    if waveform_channels == n_channels:\n        return waveform\n    elif waveform_channels > n_channels:\n        return waveform[:, :, :n_channels]\n    else:\n        waveform = tf.tile(waveform, [1, 1, n_channels // waveform_channels])\n        return waveform[:, :, :n_channels]\n\n", "CodeBleu_score": 43.99171320304804}]}
{"instance_id": "5e215f029b2e", "generation_result": [{"code": "def spectrogram_to_db_uint(\n    spectrogram: tf.Tensor, db_range: float = 100.0, **kwargs\n) -> tf.Tensor\n\t\n    spectrogram = tf.convert_to_tensor(spectrogram, **kwargs)\n    spectrogram = tf.math.pow(10.0, tf.math.maximum(spectrogram, -100.0) / 20.0)\n    spectrogram = tf.math.pow(10.0, tf.math.minimum(spectrogram, db_range) / 20.0)\n    return tf.cast(tf.math.round(spectrogram * 255.0), tf.uint8)\n\n", "CodeBleu_score": 39.573711168726376}, {"code": "def spectrogram_to_db_uint(\n    spectrogram: tf.Tensor, db_range: float = 100.0, **kwargs\n) -> tf.Tensor\n\t\n    spectrogram = tf.math.pow(10.0, gain_to_db(spectrogram, **kwargs) / 20.0)\n    spectrogram = tf.clip_by_value(spectrogram, 0.0, 1.0)\n    spectrogram = tf.math.pow(spectrogram, 1.0 / 2.0)\n    spectrogram = tf.image.convert_image_dtype(spectrogram, tf.uint8)\n    return spectrogram\n\n", "CodeBleu_score": 40.75493487770233}, {"code": "def spectrogram_to_db_uint(\n    spectrogram: tf.Tensor, db_range: float = 100.0, **kwargs\n) -> tf.Tensor\n\t\n    spectrogram = tf.math.pow(10.0, spectrogram / 20.0)\n    spectrogram = tf.math.log10(spectrogram) * 20.0\n    spectrogram = tf.clip_by_value(spectrogram, -db_range, 0.0)\n    spectrogram = (\n        spectrogram * (255.0 / db_range)\n    )  # convert to range [0, 255]\n    spectrogram = tf.cast(spectrogram, dtype=tf.uint8)\n    return spectrogram\n\n", "CodeBleu_score": 36.96910513414586}]}
{"instance_id": "b65c0968dae7", "generation_result": [{"code": "def compute_spectrogram_tf(\n    waveform: tf.Tensor,\n    frame_length: int = 2048,\n    frame_step: int = 512,\n    spec_exponent: float = 1.0,\n    window_exponent: float = 1.0,\n) -> tf.Tensor\n\t\n\n    # Compute the spectrogram\n    waveform = tf.cast(waveform, dtype=tf.float32)\n    window = tf.cast(hann_window(frame_length, periodic=False), dtype=tf.float32)\n    window = tf.pow(window, window_exponent)\n    stft = tf.signal.stft(waveform, frame_length, frame_step, fft_length=frame_length, window_fn=window)\n\n    # Transpose and compute the absolute value\n    spectrogram = tf.transpose(stft)\n    spectrogram = tf.abs(spectrogram)\n\n    # Apply the specified exponent\n    spectrogram = tf.pow(spectrogram, spec_exponent)\n\n    return spectrogram\n\n", "CodeBleu_score": 37.65649243604113}, {"code": "def compute_spectrogram_tf(\n    waveform: tf.Tensor,\n    frame_length: int = 2048,\n    frame_step: int = 512,\n    spec_exponent: float = 1.0,\n    window_exponent: float = 1.0,\n) -> tf.Tensor\n\t\n    # Create the Hann window function\n    window = hann_window(frame_length, periodic=True)\n    # Apply the window function to the waveform\n    waveform *= window\n    # Compute the STFT\n    stft_matrix = stft(\n        waveform,\n        frame_length=frame_length,\n        frame_step=frame_step,\n        fft_length=frame_length,\n        window_fn=None,\n        pad_end=False,\n    )\n    # Transpose the STFT matrix\n    stft_matrix = tf.transpose(stft_matrix)\n    # Compute the absolute value of the STFT matrix\n    stft_matrix = tf.abs(stft_matrix)\n    # Raise the absolute value to the specified exponent\n    stft_matrix = tf.pow(stft_matrix, spec_exponent)\n    # Raise the window function to the specified exponent\n    window = tf.pow(window, window_exponent)\n    # Apply the window function to the STFT matrix\n    stft_matrix *= window\n    # Return the computed spectrogram\n    return stft_matrix\n\n", "CodeBleu_score": 37.39185393423071}, {"code": "def compute_spectrogram_tf(\n    waveform: tf.Tensor,\n    frame_length: int = 2048,\n    frame_step: int = 512,\n    spec_exponent: float = 1.0,\n    window_exponent: float = 1.0,\n) -> tf.Tensor\n\t\n    # Compute the STFT\n    stft_tensor = tf.abs(\n        tf.signal.stft(\n            waveform,\n            frame_length=frame_length,\n            frame_step=frame_step,\n            fft_length=frame_length,\n            window_fn=lambda frame_length: tf.signal.hann_window(\n                frame_length, window_exponent\n            ),\n            pad_end=True,\n        )\n    )\n    # Transpose the tensor\n    stft_tensor = tf.transpose(stft_tensor)\n    # Raise the tensor to the specified exponent\n    stft_tensor = tf.math.pow(stft_tensor, spec_exponent)\n    return stft_tensor\n\n", "CodeBleu_score": 39.374423794475724}]}
{"instance_id": "a4686140a634", "generation_result": [{"code": "def time_stretch(\n    spectrogram: tf.Tensor,\n    factor: float = 1.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n\n    # Get the original shape of the spectrogram\n    spectrogram_shape = spectrogram.shape\n\n    # Calculate the new time dimension by scaling the original time dimension by the factor\n    new_time_dim = tf.cast(tf.math.ceil(spectrogram_shape[1] * factor), tf.int32)\n\n    # Resize the spectrogram using the specified interpolation method\n    spectrogram = tf.image.resize(spectrogram, (spectrogram_shape[0], new_time_dim), method)\n\n    # Calculate the difference between the new and the original time dimension\n    time_dim_diff = new_time_dim - spectrogram_shape[1]\n\n    # Crop or pad the spectrogram to match the original dimensions\n    if time_dim_diff > 0:\n        spectrogram = tf.pad(spectrogram, ((0, 0), (0, time_dim_diff), (0, 0)))\n    elif time_dim_diff < 0:\n        spectrogram = spectrogram[:, :spectrogram_shape[1], :]\n\n    return spectrogram\n\n", "CodeBleu_score": 35.305763737990034}, {"code": "def time_stretch(\n    spectrogram: tf.Tensor,\n    factor: float = 1.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n    if factor == 1.0:\n        return spectrogram\n    spectrogram = tf.cast(spectrogram, tf.float32)\n    spectrogram_shape = tf.shape(spectrogram)\n    time_dimension = spectrogram_shape[1]\n    new_time_dimension = tf.cast(time_dimension * factor, tf.int32)\n    spectrogram = tf.image.resize(spectrogram, [new_time_dimension, spectrogram_shape[2]], method)\n    if new_time_dimension > time_dimension:\n        spectrogram = tf.image.pad_to_bounding_box(spectrogram, 0, 0, spectrogram_shape[1], spectrogram_shape[2])\n    elif new_time_dimension < time_dimension:\n        spectrogram = tf.image.crop_to_bounding_box(spectrogram, 0, 0, spectrogram_shape[1], spectrogram_shape[2])\n    return spectrogram\n\n", "CodeBleu_score": 41.90126467437931}, {"code": "def time_stretch(\n    spectrogram: tf.Tensor,\n    factor: float = 1.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n\n    if not isinstance(spectrogram, tf.Tensor):\n        raise ValueError(\"spectrogram must be a tf.Tensor\")\n\n    if not isinstance(factor, float):\n        raise ValueError(\"factor must be a float\")\n\n    if factor <= 0:\n        raise ValueError(\"factor must be greater than 0\")\n\n    if not isinstance(method, tf.image.ResizeMethod):\n        raise ValueError(\"method must be a tf.image.ResizeMethod\")\n\n    if spectrogram.shape[1] == 1:\n        return spectrogram\n\n    # Calculate the new time dimension\n    new_time_dim = tf.cast(tf.math.round(spectrogram.shape[1] * factor), tf.int32)\n\n    # Resize the spectrogram\n    resized_spectrogram = tf.image.resize(\n        spectrogram, (spectrogram.shape[0], new_time_dim), method=method\n    )\n\n    # Adjust the spectrogram to the original dimensions\n    return tf.image.resize_with_crop_or_pad(\n        resized_spectrogram, spectrogram.shape[0], spectrogram.shape[1]\n    )\n\n", "CodeBleu_score": 37.21898909972916}]}
{"instance_id": "6dd01101d4ae", "generation_result": [{"code": "def pitch_shift(\n    spectrogram: tf.Tensor,\n    semitone_shift: float = 0.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n\n    # Calculate the resizing factor\n    factor = 2 ** (semitone_shift / 12)\n\n    # Resize the spectrogram along the frequency axis\n    spectrogram = tf.image.resize(spectrogram, [spectrogram.shape[0], int(spectrogram.shape[1] * factor)], method)\n\n    # Pad the spectrogram to maintain the original dimensions\n    spectrogram = tf.pad(spectrogram, [[0, 0], [0, int(factor * spectrogram.shape[1]) - spectrogram.shape[1]]])\n\n    return spectrogram\n\n", "CodeBleu_score": 27.384430067537807}, {"code": "def pitch_shift(\n    spectrogram: tf.Tensor,\n    semitone_shift: float = 0.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n    # Calculate the resizing factor\n    factor = 2 ** (semitone_shift / 12)\n    # Resize the spectrogram\n    spectrogram = tf.image.resize(spectrogram, [spectrogram.shape[0], int(spectrogram.shape[1] * factor)], method)\n    # Pad the spectrogram\n    spectrogram = tf.pad(spectrogram, [[0, 0], [0, int(spectrogram.shape[1] * (1 - factor))]])\n    return spectrogram\n\n", "CodeBleu_score": 27.24206179204179}, {"code": "def pitch_shift(\n    spectrogram: tf.Tensor,\n    semitone_shift: float = 0.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n    # Calculate the resizing factor\n    freq_axis = spectrogram.shape[0]\n    factor = 2 ** (semitone_shift / 12)\n    new_freq_axis = int(freq_axis * factor)\n    # Resize the spectrogram\n    spectrogram = tf.image.resize(spectrogram, [new_freq_axis, freq_axis], method=method)\n    # Pad the spectrogram\n    spectrogram = tf.pad(\n        spectrogram,\n        [[0, freq_axis - new_freq_axis], [0, 0]],\n        mode=\"CONSTANT\",\n        constant_values=0,\n    )\n    return spectrogram\n\n", "CodeBleu_score": 31.54644418832539}]}
{"instance_id": "612f0826e56a", "generation_result": [{"code": "def _get_conv_activation_layer(params: Dict) -> Any\n\t\n    if params[\"conv_activation\"] == \"relu\":\n        return ReLU()\n    elif params[\"conv_activation\"] == \"elu\":\n        return ELU()\n    elif params[\"conv_activation\"] == \"leaky_relu\":\n        return LeakyReLU(alpha=0.2)\n    else:\n        raise ValueError(\n            f\"The provided convolutional activation function {params['conv_activation']} is not supported. Please provide one of the following: 'relu', 'elu', 'leaky_relu'\"\n        )\n\n", "CodeBleu_score": 32.527263162557425}, {"code": "def _get_conv_activation_layer(params: Dict) -> Any\n\t\n    if params[\"conv_activation\"] == \"ReLU\":\n        return ReLU()\n    elif params[\"conv_activation\"] == \"ELU\":\n        return ELU()\n    elif params[\"conv_activation\"] == \"LeakyReLU\":\n        return LeakyReLU(alpha=0.2)\n    else:\n        raise ValueError(\n            f\"The convolutional activation {params['conv_activation']} is not supported.\"\n        )\n\n", "CodeBleu_score": 42.35436102127581}, {"code": "def _get_conv_activation_layer(params: Dict) -> Any\n\t\n    if params[\"conv_activation\"] == \"relu\":\n        return ReLU()\n    elif params[\"conv_activation\"] == \"elu\":\n        return ELU()\n    else:\n        return LeakyReLU(alpha=0.2)\n\n", "CodeBleu_score": 37.81568218186228}]}
{"instance_id": "2f8b5a2fc0f2", "generation_result": [{"code": "def softmax_unet(\n    input_tensor: tf.Tensor, instruments: Iterable[str], params: Dict = {}\n) -> Dict\n\t\n    # Set default values for parameters\n    params = {\n        \"num_filters\": 32,\n        \"kernel_size\": 3,\n        \"activation\": \"relu\",\n        \"dropout_rate\": 0.2,\n        \"batch_norm\": True,\n        \"kernel_initializer\": he_uniform,\n        \"kernel_regularizer\": None,\n        \"bias_regularizer\": None,\n        \"instance_normalization\": False,\n        \"instance_normalization_epsilon\": 1e-5,\n        \"instance_normalization_axis\": 1,\n        \"output_axis\": 2,\n        \"output_activation\": \"softmax\",\n        \"output_axis_name\": \"instrument\",\n        \"output_activation_name\": \"mask\",\n        \"output_axis_name_separator\": \"_\",\n        \"output_activation_name_separator\": \"_\",\n        \"output_axis_name_prefix\": \"instrument\",\n        \"output_activation_name_prefix\": \"mask\",\n        \"output_axis_name_suffix\": None,\n        \"output_activation_name_suffix\": None,\n        \"output_axis_name_separator_suffix\": None,\n        \"output_activation_name_separator_suffix\": None,\n        \"output_axis_name_prefix_suffix\": None,\n        \"output_activation_name_prefix_suffix\": None,\n        \"output_axis_name_suffix_separator\": None,\n        \"output_activation_name_suffix_separator\": None,\n        \"output_axis_name_prefix_suffix_separator\": None,\n        \"output_activation_name_prefix_suffix_separator\": None,\n        \"output_axis_name_prefix_suffix_separator_suffix\": None,\n        \"output_activation_name_prefix_suffix_separator_suffix\": None,\n        \"output_axis_name_prefix_suffix_separator_suffix_suffix\": None,\n        \"output_activation_name", "CodeBleu_score": 14.618771917190971}, {"code": "def softmax_unet(\n    input_tensor: tf.Tensor, instruments: Iterable[str], params: Dict = {}\n) -> Dict\n\t\n    # Set default parameters\n    params = {\n        \"activation\": \"relu\",\n        \"dropout\": 0.0,\n        \"filters\": 32,\n        \"kernel_size\": 3,\n        \"strides\": 1,\n        \"padding\": \"same\",\n        \"kernel_initializer\": he_uniform(),\n        \"activation_params\": {},\n        \"dropout_params\": {},\n        \"axis\": 1,\n        **params,\n    }\n\n    # Create a dictionary of logit masks\n    logit_masks = {}\n\n    # Iterate over the instruments\n    for instrument in instruments:\n        # Apply the U-Net model to the input tensor for the current instrument\n        logit_masks[instrument] = unet(\n            input_tensor,\n            params[\"activation\"],\n            params[\"dropout\"],\n            params[\"filters\"],\n            params[\"kernel_size\"],\n            params[\"strides\"],\n            params[\"padding\"],\n            params[\"kernel_initializer\"],\n            params[\"activation_params\"],\n            params[\"dropout_params\"],\n        )\n\n    # Create a dictionary of spectrograms\n    spectrograms = {}\n\n    # Iterate over the instruments\n    for instrument in instruments:\n        # Multiply the logit mask for the current instrument with the input tensor\n        spectrograms[instrument] = Multiply(name=f\"{instrument}_spectrogram\")(\n            [input_tensor, logit_masks[instrument]]\n        )\n\n    # Return the dictionary of spectrograms\n    return spectrograms\n\n", "CodeBleu_score": 30.033818729032784}, {"code": "def softmax_unet(\n    input_tensor: tf.Tensor, instruments: Iterable[str], params: Dict = {}\n) -> Dict\n\t\n\n    # initialize the model parameters\n    params = {\n        \"num_filters\": 32,\n        \"kernel_size\": 5,\n        \"pool_size\": 2,\n        \"dropout_rate\": 0.2,\n        \"activation\": \"relu\",\n        \"axis\": 3,\n        \"name\": \"softmax_unet\",\n        **params,\n    }\n\n    # get the number of filters\n    num_filters = params[\"num_filters\"]\n\n    # get the kernel size\n    kernel_size = params[\"kernel_size\"]\n\n    # get the pooling size\n    pool_size = params[\"pool_size\"]\n\n    # get the dropout rate\n    dropout_rate = params[\"dropout_rate\"]\n\n    # get the activation function\n    activation = params[\"activation\"]\n\n    # get the axis along which the softmax function is applied\n    axis = params[\"axis\"]\n\n    # get the name of the model\n    name = params[\"name\"]\n\n    # initialize the input tensor\n    input_tensor = tf.expand_dims(input_tensor, axis=axis)\n\n    # initialize the list of logit masks\n    logit_masks = []\n\n    # iterate over the instruments\n    for instrument in instruments:\n        # get the name of the instrument\n        instrument_name = instrument.name\n\n        # get the input tensor for the current instrument\n        instrument_input_tensor = input_tensor[:, :, :, instrument.index]\n\n        # apply the U-Net model to the input tensor for the current instrument\n        logit_mask = unet(\n            instrument_input_tensor,\n            num_filters=num_filters,\n            kernel_size=kernel_size,\n            pool_size=pool_size,\n            dropout_rate=dropout_rate,\n            activation=activation,\n            name=f\"{name}_{instrument_name}\",\n        )\n\n        # add the logit mask to the list of logit masks\n        logit_masks.append(logit_mask)\n\n    # stack the logit masks along the specified axis\n    logit_masks = tf.", "CodeBleu_score": 25.882676880721846}]}
{"instance_id": "400b19ce8520", "generation_result": [{"code": "def compute_file_checksum(path)\n\t\n    sha256 = hashlib.sha256()\n    with open(path, \"rb\") as f:\n        while True:\n            data = f.read(65536)\n            if not data:\n                break\n            sha256.update(data)\n    return sha256.hexdigest()\n\n", "CodeBleu_score": 41.64627549393817}, {"code": "def compute_file_checksum(path)\n\t\n    sha256 = hashlib.sha256()\n    with open(path, 'rb') as f:\n        for chunk in iter(lambda: f.read(4096), b''):\n            sha256.update(chunk)\n    return sha256.hexdigest()\n\n", "CodeBleu_score": 56.88014708665297}, {"code": "def compute_file_checksum(path)\n\t\n    sha256 = hashlib.sha256()\n    with open(path, \"rb\") as f:\n        while True:\n            data = f.read(65536)\n            if not data:\n                break\n            sha256.update(data)\n    return sha256.hexdigest()\n\n", "CodeBleu_score": 41.64627549393817}]}
{"instance_id": "5f2e9c6e15f1", "generation_result": [{"code": "def checksum(self, name: str) -> str\n\t\n    checksum_url = f\"{self.host}/{self.repository}/{self.release_path}/{self.checksum_index}\"\n    response = httpx.get(checksum_url)\n    response.raise_for_status()\n    checksum_index = response.json()\n    if name not in checksum_index:\n        raise ValueError(f\"{name} is not in checksum index\")\n    return checksum_index[name]\n\n", "CodeBleu_score": 34.102685815009934}, {"code": "def checksum(self, name: str) -> str\n\t\n    url = f\"{self.host}/{self.repository}/{self.release_path}/checksums.json\"\n    response = httpx.get(url)\n    if response.status_code != 200:\n        raise ValueError(f\"Checksums file not found at {url}\")\n    checksums = response.json()\n    if name not in checksums:\n        raise ValueError(f\"Checksum for {name} not found in {url}\")\n    return checksums[name]\n\n", "CodeBleu_score": 39.34856254073982}, {"code": "def checksum(self, name: str) -> str\n\t\n    url = f\"{self.host}/{self.repository}/{self.release_path}/{self.checksum_index}\"\n    response = httpx.get(url)\n    if response.status_code != 200:\n        raise ValueError(f\"Failed to retrieve checksum index from {url}\")\n    checksum_index = response.json()\n    if name not in checksum_index:\n        raise ValueError(f\"Model {name} not found in checksum index\")\n    return checksum_index[name]\n\n", "CodeBleu_score": 40.24984678064435}]}
{"instance_id": "9312c8b6032e", "generation_result": [{"code": "def download(self, name: str, path: str) -> None:\n\t\n    # Construct the download URL.\n    url = self.url + name + \".tar.gz\"\n\n    # Log the download initiation.\n    self.logger.info(f\"Downloading model {name} from {url}...\")\n\n    # Stream the file using HTTP/2.\n    with httpx.stream(\"GET\", url, timeout=None) as response:\n        response.raise_for_status()\n\n        # Write the response to a temporary file.\n        with NamedTemporaryFile(delete=False) as file:\n            for chunk in response.iter_bytes():\n                file.write(chunk)\n            file_path = file.name\n\n    # Check the file's integrity by comparing its checksum with the expected value.\n    checksum = hashlib.sha256()\n    with open(file_path, \"rb\") as file:\n        for chunk in iter(lambda: file.read(4096), b\"\"):\n            checksum.update(chunk)\n    if checksum.hexdigest() != self.checksums[name]:\n        raise RuntimeError(f\"The checksum of the downloaded file does not match the expected value.\")\n\n    # Extract the archive to the specified path and delete the temporary file.\n    with tarfile.open(file_path) as file:\n        file.extractall(path)\n    os.remove(file_path)\n\n    # Log the extraction completion.\n    self.logger.info(f\"Model {name} downloaded and extracted to {path}.\")\n\n", "CodeBleu_score": 38.433161489685055}, {"code": "def download(self, name: str, path: str) -> None:\n\t\n    # Construct the download URL.\n    url = f\"{self.base_url}/models/{name}/download\"\n\n    # Log the download initiation.\n    self.logger.info(f\"Downloading model {name} from {url}...\")\n\n    # Stream the file using HTTP/2.\n    response = httpx.get(url, stream=True)\n\n    # Check the status code.\n    response.raise_for_status()\n\n    # Get the expected checksum.\n    expected_checksum = response.headers[\"X-Checksum\"]\n\n    # Create a temporary file.\n    with NamedTemporaryFile() as tmp_file:\n        # Log the file creation.\n        self.logger.info(f\"Created temporary file {tmp_file.name}...\")\n\n        # Write the file's contents.\n        for chunk in response.iter_bytes():\n            tmp_file.write(chunk)\n\n        # Log the file write completion.\n        self.logger.info(f\"Wrote {tmp_file.tell()} bytes to {tmp_file.name}...\")\n\n        # Check the file's integrity.\n        if checksum(tmp_file.name) != expected_checksum:\n            raise Exception(\n                f\"Checksum mismatch: expected {expected_checksum}, got {checksum(tmp_file.name)}\"\n            )\n\n        # Extract the archive to the specified path.\n        with tarfile.open(tmp_file.name) as tar:\n            tar.extractall(path)\n\n        # Log the extraction completion.\n        self.logger.info(f\"Extracted {tmp_file.name} to {path}...\")\n\n", "CodeBleu_score": 35.564779603543876}, {"code": "def download(self, name: str, path: str) -> None:\n\t\n    # Construct the download URL\n    url = f\"{self.base_url}/{self.version}/{name}\"\n\n    # Log the download initiation\n    self.logger.info(f\"Downloading model archive from {url} to {path}\")\n\n    # Stream the file using HTTP/2\n    with httpx.stream(\"GET\", url) as response:\n        response.raise_for_status()\n        with NamedTemporaryFile(delete=False) as file:\n            for chunk in response.iter_bytes():\n                file.write(chunk)\n\n    # Check the file's integrity by comparing its checksum with the expected value\n    checksum = self.checksums[name]\n    hash = hashlib.sha256()\n    with open(file.name, \"rb\") as file:\n        for chunk in iter(lambda: file.read(4096), b\"\"):\n            hash.update(chunk)\n    if hash.hexdigest() != checksum:\n        raise RuntimeError(f\"Checksum mismatch for {name}.\")\n\n    # Extract the archive to the specified path\n    with tarfile.open(file.name) as file:\n        file.extractall(path)\n\n    # Delete the temporary file\n    os.remove(file.name)\n\n    # Log the extraction completion\n    self.logger.info(f\"Extraction of {name} complete.\")\n\n", "CodeBleu_score": 39.46761396980543}]}
{"instance_id": "ee9493f37d77", "generation_result": [{"code": "def load_configuration(descriptor: str) -> Dict\n\t\n    if descriptor.startswith(\"embedded:\"):\n        name = descriptor.split(\":\")[1]\n        return json.loads(\n            pkgutil.get_data(__package__, f\"config/{name}.json\").decode(\"utf-8\")\n        )\n    elif exists(descriptor):\n        with open(descriptor) as f:\n            return json.load(f)\n    else:\n        raise FileNotFoundError(f\"The configuration file '{descriptor}' does not exist.\")\n\n", "CodeBleu_score": 41.06828412392464}, {"code": "def load_configuration(descriptor: str) -> Dict\n\t\n\n    if descriptor.startswith('embedded:'):\n        name = descriptor[9:]\n        configuration = json.loads(f'{{\"{name}\": {json.dumps(eval(f\"configuration.{name}\"))}}}.{name}')\n    elif exists(descriptor):\n        with open(descriptor) as file:\n            configuration = json.load(file)\n    else:\n        raise FileNotFoundError(f'The descriptor \"{descriptor}\" does not point to an embedded configuration or an existing file.')\n\n    return configuration\n\n", "CodeBleu_score": 40.76463048279725}, {"code": "def load_configuration(descriptor: str) -> Dict\n\t\n    if descriptor.startswith('embedded://'):\n        name = descriptor[11:]\n        try:\n            return json.loads(getattr(config, name))\n        except AttributeError:\n            raise ValueError(f'Embedded configuration \"{name}\" not found.')\n    elif exists(descriptor):\n        with open(descriptor) as f:\n            return json.load(f)\n    else:\n        raise ValueError(f'File \"{descriptor}\" not found.')\n\n", "CodeBleu_score": 39.133282957367285}]}
{"instance_id": "546134b67fbc", "generation_result": [{"code": "def sync_apply(\n    tensor_dict: Dict[str, tf.Tensor], func: Callable, concat_axis: int = 1\n) -> Dict[str, tf.Tensor]\n\t\n    import tensorflow as tf\n\n    if concat_axis == 0:\n        raise ValueError(\n            \"Concatenation axis cannot be 0. Please choose 1 to concatenate along the batch dimension.\"\n        )\n    elif concat_axis == 1:\n        pass\n    else:\n        raise ValueError(\"Concatenation axis must be 0 or 1.\")\n\n    tensor_list = []\n    for key in tensor_dict:\n        tensor_list.append(tensor_dict[key])\n\n    concat_tensor = tf.concat(tensor_list, concat_axis)\n    processed_tensor = func(concat_tensor)\n\n    split_tensor_list = tf.split(processed_tensor, len(tensor_dict), concat_axis)\n\n    processed_tensor_dict = {}\n    for i in range(len(tensor_dict)):\n        processed_tensor_dict[list(tensor_dict.keys())[i]] = split_tensor_list[i]\n\n    return processed_tensor_dict\n\n", "CodeBleu_score": 44.035830456714194}, {"code": "def sync_apply(\n    tensor_dict: Dict[str, tf.Tensor], func: Callable, concat_axis: int = 1\n) -> Dict[str, tf.Tensor]\n\t\n    import tensorflow as tf\n\n    assert concat_axis in [0, 1], \"The concat_axis must be either 0 or 1.\"\n\n    # Concatenate tensors\n    if concat_axis == 0:\n        tensor_concat = tf.concat(list(tensor_dict.values()), axis=0)\n    else:\n        tensor_concat = tf.concat(list(tensor_dict.values()), axis=1)\n\n    # Apply the function to the concatenated tensor\n    tensor_processed = func(tensor_concat)\n\n    # Split the processed tensor back into the original dictionary format\n    if concat_axis == 0:\n        tensor_dict_processed = {\n            key: tensor_processed[i : i + tensor_dict[key].shape[0]]\n            for i, key in enumerate(tensor_dict.keys())\n        }\n    else:\n        tensor_dict_processed = {\n            key: tensor_processed[i : i + tensor_dict[key].shape[1]]\n            for i, key in enumerate(tensor_dict.keys())\n        }\n\n    return tensor_dict_processed\n\n", "CodeBleu_score": 46.00739583659437}, {"code": "def sync_apply(\n    tensor_dict: Dict[str, tf.Tensor], func: Callable, concat_axis: int = 1\n) -> Dict[str, tf.Tensor]\n\t\n    if concat_axis not in [0, 1]:\n        raise ValueError(\n            \"The concatenation axis must be either 0 or 1, but it is {}.\".format(\n                concat_axis\n            )\n        )\n    if concat_axis == 0:\n        tensor_dict = {\n            key: tf.expand_dims(tensor, axis=0)\n            for key, tensor in tensor_dict.items()\n        }\n        tensor_dict = {\n            key: tf.concat(list(tensor_dict.values()), axis=0)\n            for key, tensor in tensor_dict.items()\n        }\n        tensor_dict = {\n            key: tf.squeeze(tensor, axis=0)\n            for key, tensor in tensor_dict.items()\n        }\n    else:\n        tensor_dict = {\n            key: tf.expand_dims(tensor, axis=1)\n            for key, tensor in tensor_dict.items()\n        }\n        tensor_dict = {\n            key: tf.concat(list(tensor_dict.values()), axis=1)\n            for key, tensor in tensor_dict.items()\n        }\n        tensor_dict = {\n            key: tf.squeeze(tensor, axis=1)\n            for key, tensor in tensor_dict.items()\n        }\n    return tensor_dict\n\n", "CodeBleu_score": 37.705972827962874}]}
{"instance_id": "df48e78d46d2", "generation_result": [{"code": "def from_float32_to_uint8(\n    tensor: tf.Tensor,\n    tensor_key: str = \"tensor\",\n    min_key: str = \"min\",\n    max_key: str = \"max\",\n) -> tf.Tensor\n\t\n    return tf.map_fn(\n        lambda x: tf.image.convert_image_dtype(\n            x[tensor_key], dtype=tf.uint8, saturate=True\n        ),\n        tensor,\n        fn_output_signature=tf.RaggedTensorSpec(\n            shape=[None], dtype=tf.uint8, ragged_rank=1\n        ),\n    )\n\n", "CodeBleu_score": 35.854124427211396}, {"code": "def from_float32_to_uint8(\n    tensor: tf.Tensor,\n    tensor_key: str = \"tensor\",\n    min_key: str = \"min\",\n    max_key: str = \"max\",\n) -> tf.Tensor\n\t\n    min_value = tf.math.reduce_min(tensor)\n    max_value = tf.math.reduce_max(tensor)\n    tensor = tf.cast(tensor, tf.float32)\n    tensor = tf.math.divide(\n        tf.math.multiply(tensor - min_value, 255), max_value - min_value\n    )\n    tensor = tf.cast(tensor, tf.uint8)\n    return {tensor_key: tensor, min_key: min_value, max_key: max_value}\n\n", "CodeBleu_score": 43.42066598933581}, {"code": "def from_float32_to_uint8(\n    tensor: tf.Tensor,\n    tensor_key: str = \"tensor\",\n    min_key: str = \"min\",\n    max_key: str = \"max\",\n) -> tf.Tensor\n\t\n    min_value = tf.math.reduce_min(tensor)\n    max_value = tf.math.reduce_max(tensor)\n    scaled_tensor = tf.cast(\n        tf.math.round((tensor - min_value) / (max_value - min_value) * 255),\n        dtype=tf.uint8,\n    )\n    return {tensor_key: scaled_tensor, min_key: min_value, max_key: max_value}\n\n", "CodeBleu_score": 42.371204320709765}]}
{"instance_id": "7f479e5de492", "generation_result": [{"code": "def pad_and_partition(tensor: tf.Tensor, segment_len: int) -> tf.Tensor\n\t\n    # Calculate the padding needed to make the tensor length a multiple of the segment length\n    pad_len = segment_len - (tf.shape(tensor)[0] % segment_len)\n\n    # Pad the tensor\n    tensor = tf.pad(tensor, [[0, pad_len]])\n\n    # Reshape the tensor into a new tensor with the specified segment length\n    return tf.reshape(tensor, (-1, segment_len))\n\n", "CodeBleu_score": 18.832321706982967}, {"code": "def pad_and_partition(tensor: tf.Tensor, segment_len: int) -> tf.Tensor\n\t\n    padded_len = segment_len * (tensor.shape[0] // segment_len)\n    padded_tensor = tf.pad(tensor, [[0, padded_len - tensor.shape[0]]])\n    return tf.reshape(padded_tensor, [-1, segment_len])\n\n", "CodeBleu_score": 17.608232831805207}, {"code": "def pad_and_partition(tensor: tf.Tensor, segment_len: int) -> tf.Tensor\n\t\n    tensor_length = tensor.shape[0]\n    segment_length = segment_len\n    # Calculate the number of segments\n    num_segments = (tensor_length + segment_length - 1) // segment_length\n    # Calculate the padding needed to make the tensor length a multiple of the segment length\n    pad_length = (num_segments * segment_length) - tensor_length\n    # Pad the tensor with zeros\n    tensor = tf.pad(tensor, [[0, pad_length]])\n    # Reshape the tensor into a new tensor with the specified segment length\n    tensor = tf.reshape(tensor, [num_segments, segment_length])\n    return tensor\n\n", "CodeBleu_score": 25.94205923105863}]}
{"instance_id": "1789fae7253f", "generation_result": [{"code": "def pad_and_reshape(instr_spec, frame_length, F) -> Any\n\t\n    # Calculate the number of extra rows needed for padding\n    n_extra_rows = frame_length - instr_spec.shape[1]\n    # Create an extension of zeros\n    extra_rows = F.zeros((instr_spec.shape[0], n_extra_rows, instr_spec.shape[2]))\n    # Concatenate the extension to the original spectrogram along the frequency axis\n    instr_spec_extended = F.concat(instr_spec, extra_rows, dim=1)\n    # Reshape the extended spectrogram by merging the first two dimensions\n    instr_spec_reshaped = F.reshape(instr_spec_extended, (-1, instr_spec_extended.shape[2]))\n    return instr_spec_reshaped\n\n", "CodeBleu_score": 17.219073590214656}, {"code": "def pad_and_reshape(instr_spec, frame_length, F) -> Any\n\t\n    # calculate number of extra rows needed for padding\n    pad_rows = (frame_length - instr_spec.shape[0]) % frame_length\n    # create extension of zeros\n    pad_extension = np.zeros((pad_rows, instr_spec.shape[1]))\n    # concatenate extension to the original spectrogram\n    padded_spec = np.concatenate((instr_spec, pad_extension), axis=0)\n    # reshape the extended spectrogram by merging the first two dimensions\n    padded_spec = padded_spec.reshape((-1, instr_spec.shape[1]))\n    # return the processed spectrogram\n    return padded_spec\n\n", "CodeBleu_score": 15.380916063182292}, {"code": "def pad_and_reshape(instr_spec, frame_length, F) -> Any\n\t\n\n    # calculate the number of extra rows needed for padding\n    n_extra = (frame_length - instr_spec.shape[1]) // 2\n\n    # create an extension of zeros\n    pad = F.zeros((instr_spec.shape[0], n_extra, instr_spec.shape[2]))\n\n    # concatenate this extension to the original spectrogram along the frequency axis\n    padded_spec = F.concat((pad, instr_spec, pad), dim=1)\n\n    # reshape the extended spectrogram by merging the first two dimensions\n    return F.reshape(padded_spec, (-1, instr_spec.shape[2]))\n\n", "CodeBleu_score": 13.752770593950697}]}
{"instance_id": "8a4f4658d97c", "generation_result": [{"code": "def dataset_from_csv(csv_path: str, **kwargs) -> Any\n\t\n    import pandas as pd\n    import tensorflow as tf\n\n    df = pd.read_csv(csv_path)\n    return tf.data.Dataset.from_tensor_slices(df.to_dict(orient=\"list\"))\n\n", "CodeBleu_score": 24.105006658089277}, {"code": "def dataset_from_csv(csv_path: str, **kwargs) -> Any\n\t\n    import pandas as pd\n    import tensorflow as tf\n\n    df = pd.read_csv(csv_path)\n    dataset = tf.data.Dataset.from_tensor_slices(df.to_dict(orient=\"list\"))\n    return dataset\n\n", "CodeBleu_score": 26.19841597825769}, {"code": "def dataset_from_csv(csv_path: str, **kwargs) -> Any\n\t\n    import pandas as pd\n    import tensorflow as tf\n\n    df = pd.read_csv(csv_path, **kwargs)\n    dataset = tf.data.Dataset.from_tensor_slices(df.to_dict(orient=\"list\"))\n    return dataset\n\n", "CodeBleu_score": 36.85947531008321}]}
{"instance_id": "9a16e81b182f", "generation_result": [{"code": "def check_tensor_shape(tensor_tf: tf.Tensor, target_shape: Any) -> bool\n\t\n    result = True\n    for i, dim in enumerate(target_shape):\n        if isinstance(dim, int):\n            result = result and (tensor_tf.shape[i] == dim)\n    return result\n\n", "CodeBleu_score": 31.18355152739751}, {"code": "def check_tensor_shape(tensor_tf: tf.Tensor, target_shape: Any) -> bool\n\t\n    tensor_shape = tensor_tf.shape\n    result = True\n    for i, target_dimension in enumerate(target_shape):\n        if target_dimension:\n            result = result and (tensor_shape[i] == target_dimension)\n    return result\n\n", "CodeBleu_score": 34.65462287541358}, {"code": "def check_tensor_shape(tensor_tf: tf.Tensor, target_shape: Any) -> bool\n\t\n    result = True\n    for dim, length in enumerate(target_shape):\n        if length is not None:\n            result = result and (tensor_tf.shape[dim] == length)\n    return result\n\n", "CodeBleu_score": 32.61913248656539}]}
{"instance_id": "49d5d3084b3a", "generation_result": [{"code": "def get_training_dataset(\n    audio_params: Dict, audio_adapter: AudioAdapter, audio_path: str\n) -> Any\n\t\n    dataset_builder = DatasetBuilder(\n        audio_params,\n        audio_adapter,\n        chunk_duration=audio_params[\"chunk_duration\"],\n        random_seed=audio_params[\"random_seed\"],\n    )\n    dataset = dataset_builder.build(\n        audio_path,\n        cache_dir=audio_params[\"cache_dir\"],\n        batch_size=audio_params[\"batch_size\"],\n        num_chunks_per_song=audio_params[\"num_chunks_per_song\"],\n        augment=audio_params[\"augment\"],\n        cache=audio_params[\"cache\"],\n    )\n    return dataset\n\n", "CodeBleu_score": 38.54807996495755}, {"code": "def get_training_dataset(\n    audio_params: Dict, audio_adapter: AudioAdapter, audio_path: str\n) -> Any\n\t\n\n    dataset_builder = DatasetBuilder(audio_params, audio_adapter)\n    dataset_builder.set_chunk_duration(audio_params[\"chunk_duration\"])\n    dataset_builder.set_random_seed(audio_params[\"random_seed\"])\n    dataset = dataset_builder.build(\n        audio_path,\n        audio_params[\"train_csv\"],\n        audio_params[\"cache_dir\"],\n        audio_params[\"batch_size\"],\n        audio_params[\"chunks_per_song\"],\n        audio_params[\"augment\"],\n        audio_params[\"cache\"],\n    )\n\n    return dataset\n\n", "CodeBleu_score": 33.22928141518289}, {"code": "def get_training_dataset(\n    audio_params: Dict, audio_adapter: AudioAdapter, audio_path: str\n) -> Any\n\t\n    dataset_builder = DatasetBuilder(audio_params, audio_adapter)\n    dataset_builder.set_chunk_duration(audio_params[\"chunk_duration\"])\n    dataset_builder.set_random_seed(audio_params[\"random_seed\"])\n    dataset = dataset_builder.build(\n        audio_path,\n        audio_params[\"train_csv\"],\n        audio_params[\"cache_dir\"],\n        audio_params[\"batch_size\"],\n        audio_params[\"num_chunks_per_song\"],\n        audio_params[\"augment\"],\n        audio_params[\"cache_data\"],\n    )\n    return dataset\n\n", "CodeBleu_score": 33.361269985248356}]}
{"instance_id": "fe8bbbba06e3", "generation_result": [{"code": "def instruments(self) -> Any\n\t\n    for instrument in self.instruments:\n        if instrument not in self.instrument_builders:\n            self.instrument_builders[instrument] = InstrumentDatasetBuilder(\n                self.data_path, instrument\n            )\n        yield self.instrument_builders[instrument]\n\n", "CodeBleu_score": 34.57436653968803}, {"code": "def instruments(self) -> Any\n\t\n    from qf_lib.common.utils.dateutils.string_to_date import str_to_date\n    from qf_lib.containers.dataframe.qf_dataframe import QFDataFrame\n    from qf_lib.containers.dataframe.simple_returns_dataframe import SimpleReturnsDataFrame\n    from qf_lib.containers.dataframe.simple_returns_dataframe import to_simple_returns_dataframe\n    from qf_lib.containers.dataframe.time_series_dataframe import TimeSeriesDataFrame\n    from qf_lib.containers.qf_data_array import QFDataArray\n    from qf_lib.containers.series.qf_series import QFSeries\n    from qf_lib.data_providers.helpers import get_data_provider\n    from qf_lib.data_providers.helpers import get_data_provider_from_config\n    from qf_lib.documents_utils.excel.excel_document import ExcelDocument\n    from qf_lib.documents_utils.excel.excel_document_creator import ExcelDocumentCreator\n    from qf_lib.documents_utils.excel.excel_document_creator import ExcelStyle\n    from qf_lib.settings import Settings\n    from qf_lib.settings import SettingsIni\n\n    def _get_instrument_data_provider(instrument: str, data_provider: str) -> Any:\n        \"\"\"\n        Returns the data provider for the given instrument.\n\n        Parameters\n        ----------\n        instrument: str\n            Instrument ticker\n        data_provider: str\n            Data provider name\n\n        Returns\n        -------\n        Any\n            Data provider for the given instrument\n        \"\"\"\n        settings = SettingsIni.get_settings()\n        data_provider_settings = settings.get_data_provider_settings(data_provider)\n        return get_data_provider_from_config(data_provider_settings, data_provider_settings.get(instrument))\n\n    def _get_instrument_data(instrument: str, data_provider: str, field: str, start_date: Optional[str", "CodeBleu_score": 24.193481271535543}, {"code": "def instruments(self) -> Any\n\t\n    if not self.instrument_builders:\n        for instrument in self.instruments:\n            self.instrument_builders.append(InstrumentDatasetBuilder(instrument))\n    for builder in self.instrument_builders:\n        yield builder\n\n", "CodeBleu_score": 36.51889704297139}]}
{"instance_id": "438b57fd3413", "generation_result": [{"code": "def cache(self, dataset: Any, cache: Optional[str], wait: bool) -> Any\n\t\n    if cache is None:\n        return dataset\n    cache_path = os.path.join(cache, \"index.json\")\n    if not exists(cache_path):\n        if wait:\n            print(f\"Waiting for cache index file {cache_path} to exist...\")\n            while not exists(cache_path):\n                time.sleep(1)\n            print(f\"Found cache index file {cache_path}\")\n        else:\n            raise ValueError(f\"Cache index file {cache_path} not found.\")\n    if not exists(cache):\n        os.makedirs(cache)\n    return self.cache(dataset, cache_path)\n\n", "CodeBleu_score": 41.94637097728682}, {"code": "def cache(self, dataset: Any, cache: Optional[str], wait: bool) -> Any\n\t\n\n    # If no cache path is provided, return the original dataset\n    if cache is None:\n        return dataset\n\n    # Ensure the cache directory exists\n    cache_dir = os.path.dirname(cache)\n    if not os.path.exists(cache_dir):\n        os.makedirs(cache_dir)\n\n    # Cache the dataset to the specified path\n    cache_dataset = dataset.cache(cache)\n\n    # If wait is True, repeatedly check for the existence of the cache index file, logging a message and waiting for a specified period if not found\n    if wait:\n        while not exists(cache + \".idx\"):\n            print(\"Waiting for cache index file to be created...\")\n            time.sleep(10)\n\n    return cache_dataset\n\n", "CodeBleu_score": 48.16580055715235}, {"code": "def cache(self, dataset: Any, cache: Optional[str], wait: bool) -> Any\n\t\n    if cache is None:\n        return dataset\n    if not exists(cache):\n        os.makedirs(cache)\n    if not exists(cache + SEPARATOR + \"index.pkl\"):\n        self.log(\n            \"Caching dataset to \" + cache + SEPARATOR + \"index.pkl\",\n            \"INFO\",\n            \"Caching\",\n        )\n        dataset.cache(cache)\n    else:\n        self.log(\n            \"Using cached dataset from \" + cache + SEPARATOR + \"index.pkl\",\n            \"INFO\",\n            \"Caching\",\n        )\n        dataset = dataset.load_from_cache(cache)\n    return dataset\n\n", "CodeBleu_score": 35.80887165282213}]}
{"instance_id": "8838a3a09a66", "generation_result": [{"code": "def create_estimator(params: Dict, MWF: bool) -> tf.Tensor\n\t\n    model_dir = params[\"model_dir\"]\n    model_provider = ModelProvider(model_dir)\n    model_provider.update_params(params)\n    params[\"model_dir\"] = model_provider.model_dir\n    params[\"model_fn\"] = model_provider.model_fn\n    params[\"model_params\"] = model_provider.model_params\n    params[\"config\"] = tf.ConfigProto(\n        gpu_options=tf.GPUOptions(\n            allow_growth=True,\n            per_process_gpu_memory_fraction=0.9,\n        )\n    )\n    run_config = tf.estimator.RunConfig(**params[\"config\"])\n    return tf.estimator.Estimator(\n        model_fn=params[\"model_fn\"],\n        model_dir=params[\"model_dir\"],\n        params=params[\"model_params\"],\n        config=run_config,\n    )\n\n", "CodeBleu_score": 30.108001203124683}, {"code": "def create_estimator(params: Dict, MWF: bool) -> tf.Tensor\n\t\n    model_dir = params[\"model_dir\"]\n    model_provider = ModelProvider(model_dir)\n    model_function = model_provider.model_function\n    model_params = model_provider.model_params\n    model_params.update(params)\n    session_config = tf.ConfigProto()\n    session_config.gpu_options.allow_growth = True\n    run_config = tf.estimator.RunConfig(\n        model_dir=model_dir,\n        session_config=session_config,\n        save_summary_steps=params[\"save_summary_steps\"],\n        save_checkpoints_steps=params[\"save_checkpoints_steps\"],\n        keep_checkpoint_max=params[\"keep_checkpoint_max\"],\n        log_step_count_steps=params[\"log_step_count_steps\"],\n    )\n    if MWF:\n        return tf.estimator.Estimator(\n            model_fn=model_function,\n            model_dir=model_dir,\n            config=run_config,\n            params=model_params,\n        )\n    return tf.estimator.Estimator(\n        model_fn=model_function,\n        model_dir=model_dir,\n        config=run_config,\n        params=model_params,\n    )\n\n", "CodeBleu_score": 28.05888308634629}, {"code": "def create_estimator(params: Dict, MWF: bool) -> tf.Tensor\n\t\n    # Load model directory using a ModelProvider\n    model_dir = params[\"model_dir\"]\n    model_provider = ModelProvider(model_dir)\n    model_dir = model_provider.get_model_dir(params[\"model_name\"], params[\"model_version\"])\n\n    # Update parameters\n    params[\"model_dir\"] = model_dir\n    params[\"model_name\"] = params[\"model_name\"]\n    params[\"model_version\"] = params[\"model_version\"]\n    params[\"model_provider\"] = model_provider\n\n    # Set up session configuration to limit GPU memory usage\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    config.gpu_options.per_process_gpu_memory_fraction = 0.95\n\n    # Create RunConfig with this session configuration\n    run_config = tf.estimator.RunConfig(session_config=config)\n\n    # Initialize and return a TensorFlow estimator\n    return tf.estimator.Estimator(\n        model_fn=model_provider.model_fn,\n        model_dir=model_dir,\n        params=params,\n        config=run_config,\n    )\n\n", "CodeBleu_score": 33.09757225650481}]}
{"instance_id": "779c812bf2cb", "generation_result": [{"code": "def _separate_tensorflow(\n        self, waveform: np.ndarray, audio_descriptor: AudioDescriptor\n    ) -> Dict\n\t\n    # check the waveform is stereo\n    if waveform.ndim != 2 or waveform.shape[1] != 2:\n        raise ValueError(\"The waveform must be stereo\")\n\n    # obtain the prediction generator\n    prediction_gen = self.predict_generator\n\n    # update the data generator\n    prediction_gen.update(waveform, audio_descriptor)\n\n    # obtain a prediction\n    prediction = next(prediction_gen)\n\n    # remove the audio id from the prediction\n    del prediction[\"audio_id\"]\n\n    return prediction\n\n", "CodeBleu_score": 43.7974779792541}, {"code": "def _separate_tensorflow(\n        self, waveform: np.ndarray, audio_descriptor: AudioDescriptor\n    ) -> Dict\n\t\n    # Ensure the waveform is stereo\n    waveform = ensure_stereo(waveform)\n    # Obtain a prediction generator\n    prediction_generator = self.model.predict(\n        self.data_generator.flow(\n            waveform, audio_descriptor, batch_size=self.batch_size\n        )\n    )\n    # Update the data generator with the waveform and audio descriptor\n    self.data_generator.update(waveform, audio_descriptor)\n    # Generate a prediction\n    prediction = next(prediction_generator)\n    # Remove the \"audio_id\" key from the prediction\n    del prediction[\"audio_id\"]\n    return prediction\n\n", "CodeBleu_score": 42.1495483703041}, {"code": "def _separate_tensorflow(\n        self, waveform: np.ndarray, audio_descriptor: AudioDescriptor\n    ) -> Dict\n\t\n    waveform = self._to_stereo(waveform)\n    prediction_gen = self.model.predict(\n        self.data_gen.flow(waveform, audio_descriptor)\n    )\n    prediction = next(prediction_gen)\n    del prediction[\"audio_id\"]\n    return prediction\n\n", "CodeBleu_score": 35.05863431118083}]}
{"instance_id": "cf580cd2ed93", "generation_result": [{"code": "def compute_backbone_shapes(config, image_shape)\n\t\n    if callable(config.BACKBONE):\n        return config.COMPUTE_BACKBONE_SHAPE(image_shape)\n    assert config.BACKBONE in [\"resnet50\", \"resnet101\"]\n    return np.array(\n        [[int(math.ceil(image_shape[0] / stride)), int(math.ceil(image_shape[1] / stride))]\n         for stride in config.BACKBONE_STRIDES])\n\n", "CodeBleu_score": 93.90166041473587}, {"code": "def compute_backbone_shapes(config, image_shape)\n\t\n    if callable(config.BACKBONE):\n        return config.COMPUTE_BACKBONE_SHAPE(image_shape)\n    assert config.BACKBONE in [\"resnet50\", \"resnet101\"], \"Backbone \\\"{}\\\" not recognized.\".format(config.BACKBONE)\n    return np.array(\n        [\n            image_shape[0] // stride,\n            image_shape[1] // stride,\n        ]\n    )\n\n", "CodeBleu_score": 60.89130560791259}, {"code": "def compute_backbone_shapes(config, image_shape)\n\t\n    if callable(config.BACKBONE):\n        return config.COMPUTE_BACKBONE_SHAPE(image_shape)\n    assert config.BACKBONE in [\"resnet50\", \"resnet101\"]\n    return np.array(\n        [\n            image_shape // stride\n            for stride in config.BACKBONE_STRIDES\n        ]\n    )\n\n", "CodeBleu_score": 57.108342847480074}]}
{"instance_id": "ebd61a2e59ca", "generation_result": [{"code": "def clip_boxes_graph(boxes, window)\n\t\n    # Split the box and window dimensions\n    # Tensorflow requires a float tensor, so convert the window to float\n    # Also convert to float to avoid overflow errors\n    window = tf.cast(window, tf.float32)\n    wh = window[2:]\n    xy = window[:2]\n    # Split each box and window side into start and end indices\n    # xywh are the box coordinates, xy are the window coordinates\n    # wh are the window dimensions\n    x1y1 = boxes[:, :2]\n    x2y2 = boxes[:, 2:4]\n    xywh = tf.concat([xy - x1y1, x2y2 - x1y1], axis=1)\n    # Split each in start and end indices\n    x1, y1, x2, y2 = tf.split(xywh, 4, axis=1)\n    # Clip each of the coordinates\n    x1 = tf.clip_by_value(x1, 0, wh[0])\n    y1 = tf.clip_by_value(y1, 0, wh[1])\n    x2 = tf.clip_by_value(x2, 0, wh[0])\n    y2 = tf.clip_by_value(y2, 0, wh[1])\n    # Calculate the new box coordinates\n    xy1 = tf.concat([x1, y1], axis=1)\n    xy2 = tf.concat([x2, y2], axis=1)\n    # Calculate the new box dimensions\n    wh = tf.concat([x2 - x1, y2 - y1], axis=1)\n    # Return the clipped boxes\n    return tf.concat([xy1, wh], axis=1)\n\n", "CodeBleu_score": 15.695145134285859}, {"code": "def clip_boxes_graph(boxes, window)\n\t\n    # Split the window and boxes coordinates into their components\n    window_start = window[:, 0:2]\n    window_size = window[:, 2:4]\n    boxes_start = boxes[:, 0:2]\n    boxes_size = boxes[:, 2:4]\n\n    # Compute the box coordinates clipped to lie within the window boundaries\n    boxes_end = tf.minimum(boxes_start + boxes_size, window_start + window_size)\n    boxes_start = tf.maximum(boxes_start, window_start)\n\n    # Concatenate the clipped coordinates and return them\n    return tf.concat([boxes_start, boxes_end - boxes_start], axis=1)\n\n", "CodeBleu_score": 21.85147567196464}, {"code": "def clip_boxes_graph(boxes, window)\n\t\n    # Split the window and boxes coordinates into start and end indices\n    window_start = window[:, 0]\n    window_end = window[:, 1]\n    box_start = boxes[:, 0]\n    box_end = boxes[:, 1]\n    # Compute the clipped coordinates\n    clipped_box_start = tf.maximum(box_start, window_start)\n    clipped_box_end = tf.minimum(box_end, window_end)\n    # If the clipped coordinates are the same as the box coordinates, return the box index\n    box_indices = tf.to_int32(tf.where(tf.equal(clipped_box_start, box_start))[:, 0])\n    clipped_coordinates = tf.stack([clipped_box_start, clipped_box_end], axis=1)\n    clipped_coordinates = tf.gather(clipped_coordinates, box_indices)\n    clipped_coordinates.set_shape([None, 2])\n    return clipped_coordinates\n\n", "CodeBleu_score": 28.233447556397362}]}
{"instance_id": "8e0411587a70", "generation_result": [{"code": "def rpn_graph(feature_map, anchors_per_location, anchor_stride)\n\t\n    # Shared convolutional base layer\n    x = keras.layers.Conv2D(512, (3, 3), padding='same', activation='relu', kernel_initializer='normal', name='rpn_conv1')(feature_map)\n\n    # Class prediction head\n    x_class = keras.layers.Conv2D(anchors_per_location, (1, 1), activation='sigmoid', kernel_initializer='uniform', name='rpn_out_class')(x)\n    # Reshape to [batch, num_anchors, 1]\n    x_class = keras.layers.Reshape((-1, 1), name='rpn_out_class_reshape')(x_class)\n\n    # Bounding box prediction head\n    x_regr = keras.layers.Conv2D(anchors_per_location * 4, (1, 1), activation='linear', kernel_initializer='zero', name='rpn_out_regress')(x)\n    # Reshape to [batch, num_anchors, 4]\n    x_regr = keras.layers.Reshape((-1, 4), name='rpn_out_regress_reshape')(x_regr)\n\n    return [x_class, x_regr]\n\n", "CodeBleu_score": 34.94681160716125}, {"code": "def rpn_graph(feature_map, anchors_per_location, anchor_stride)\n\t\n    # Shared convolutional base layer\n    x = keras.layers.Conv2D(512, (3, 3), padding='same', activation='relu', kernel_initializer='normal', name='rpn_conv1')(feature_map)\n\n    # Class score and bounding box branch\n    x_class = keras.layers.Conv2D(anchors_per_location, (1, 1), activation='sigmoid', kernel_initializer='uniform', name='rpn_out_class')(x)\n    x_regr = keras.layers.Conv2D(anchors_per_location * 4, (1, 1), activation='linear', kernel_initializer='zero', name='rpn_out_regress')(x)\n\n    # Reshape to [batch, num_anchors, 4]\n    x_class = keras.layers.Reshape((-1, anchors_per_location), name='rpn_out_class_reshape')(x_class)\n    x_regr = keras.layers.Reshape((-1, anchors_per_location, 4), name='rpn_out_regress_reshape')(x_regr)\n\n    return [x_class, x_regr]\n\n", "CodeBleu_score": 36.06984673079453}, {"code": "def rpn_graph(feature_map, anchors_per_location, anchor_stride)\n\t\n    # ROI Proposal feature map\n    # The feature map is 17x17x(2k * 5)\n    # 17x17x(2k * 5) -> 17x17x2k\n    # 17x17x2k -> 17x17x2k\n    # 17x17x2k -> 17x17x4k\n    # 17x17x4k -> 17x17x4k\n    # 17x17x4k -> 17x17x4k\n    # 17x17x4k -> 17x17x4k\n    # 17x17x4k -> 17x17x4k\n    # 17x17x4k -> 17x17x4k\n    # 17x17x4k -> 17x17x4k\n    # 17x17x4k -> 17x17x4k\n    # 17x17x4k -> 17x17x4k\n    # 17x17x4k -> 17x17x4k\n    # 17x17x4k -> 17x17x4k\n    # 17x17x4k -> 17x17x4k\n    # 17x17x4k -> 17x17x4k\n    # 17x17x4k -> 17x17x4k\n    # 17x17x4k -> 17x17x4k\n    # 17x17x4k -> 17x17x4k\n    # 17x17x4k -> 17x17x4k\n    # 17x17x4k -> 17x17x4k\n    # 17x17x4k -> 17x17x4k\n    # 17x17x4k -> ", "CodeBleu_score": 0.8163911766399816}]}
{"instance_id": "70777c9261d8", "generation_result": [{"code": "def smooth_l1_loss(y_true, y_pred)\n\t\n    absolute_loss = keras.backend.abs(y_true - y_pred)\n    square_loss = 0.5 * (y_true - y_pred)**2\n    linear_loss = absolute_loss - 0.5\n    loss_mask = keras.backend.cast(absolute_loss < 1, 'float32')\n    return keras.backend.mean(loss_mask * square_loss + (1 - loss_mask) * linear_loss)\n\n", "CodeBleu_score": 29.55075336468202}, {"code": "def smooth_l1_loss(y_true, y_pred)\n\t\n    absolute_loss = keras.backend.abs(y_true - y_pred)\n    square_loss = 0.5 * (y_true - y_pred) ** 2\n    linear_loss = absolute_loss - 0.5\n    return keras.backend.where(absolute_loss < 1.0, square_loss, linear_loss)\n\n", "CodeBleu_score": 20.780010181191482}, {"code": "def smooth_l1_loss(y_true, y_pred)\n\t\n    absolute_loss = keras.backend.abs(y_true - y_pred)\n    loss = (absolute_loss < 1.0).astype(keras.backend.floatx()) * (\n        absolute_loss ** 2) / 2 + (absolute_loss >= 1.0).astype(\n            keras.backend.floatx()) * (absolute_loss - 0.5)\n    return loss\n\n", "CodeBleu_score": 22.71314838745426}]}
{"instance_id": "0e747b604087", "generation_result": [{"code": "def build_model(x_train, num_classes)\n\t\n    # Reset the TensorFlow graph\n    tf.reset_default_graph()\n\n    # Input layer\n    x = tf.placeholder(tf.float32, (None, 32, 32, 3), name='x')\n\n    # Convolutional layer 1\n    W1 = tf.Variable(tf.random_normal([3, 3, 3, 16], stddev=0.01), name='W1')\n    L1 = tf.nn.conv2d(x, W1, strides=[1, 1, 1, 1], padding='SAME')\n    L1 = tf.nn.relu(L1)\n    L1 = tf.nn.max_pool(L1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n\n    # Convolutional layer 2\n    W2 = tf.Variable(tf.random_normal([3, 3, 16, 32], stddev=0.01), name='W2')\n    L2 = tf.nn.conv2d(L1, W2, strides=[1, 1, 1, 1], padding='SAME')\n    L2 = tf.nn.relu(L2)\n    L2 = tf.nn.max_pool(L2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n\n    # Flatten the output for the fully connected layer\n    L2 = tf.contrib.layers.flatten(L2)\n\n    # Fully connected layer 1\n    W3 = tf.Variable(tf.random_normal([32 * 32 * 32, 128], stddev=0.01), name='W3')\n    L3 = tf.matmul(L2, W3)\n    L3 = tf.nn.relu(L3)\n\n    # Fully connected layer 2 (logits)\n    W4 = tf.Variable(tf.random_normal([128, num_", "CodeBleu_score": 18.03855948737273}, {"code": "def build_model(x_train, num_classes)\n\t\n    \n    # Reset the graph\n    K.clear_session()\n    \n    # Define the input layer\n    input_layer = Input(shape=(32, 32, 3))\n    \n    # Define the first convolutional layer\n    conv_layer1 = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(input_layer)\n    \n    # Define the first max-pooling layer\n    pool_layer1 = MaxPooling2D(pool_size=(2, 2))(conv_layer1)\n    \n    # Define the second convolutional layer\n    conv_layer2 = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(pool_layer1)\n    \n    # Define the second max-pooling layer\n    pool_layer2 = MaxPooling2D(pool_size=(2, 2))(conv_layer2)\n    \n    # Flatten the output of the second max-pooling layer\n    flatten_layer = Flatten()(pool_layer2)\n    \n    # Define the first dense layer\n    dense_layer1 = Dense(units=512, activation='relu')(flatten_layer)\n    \n    # Define the second dense layer\n    dense_layer2 = Dense(units=num_classes, activation='softmax')(dense_layer1)\n    \n    # Define the model\n    model = Model(inputs=input_layer, outputs=dense_layer2)\n    \n    # Compile the model\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n    \n    return model\n\n\n\n\n", "CodeBleu_score": 22.015279017503133}, {"code": "def build_model(x_train, num_classes)\n\t\n    # Reset the graph\n    tf.reset_default_graph()\n\n    # Input layer\n    input_layer = tf.reshape(x_train, [-1, 32, 32, 3])\n\n    # Convolutional layer 1\n    conv1 = tf.layers.conv2d(inputs=input_layer,\n                             filters=32,\n                             kernel_size=[5, 5],\n                             padding=\"same\",\n                             activation=tf.nn.relu)\n\n    # Max pooling layer 1\n    pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\n\n    # Convolutional layer 2\n    conv2 = tf.layers.conv2d(inputs=pool1,\n                             filters=64,\n                             kernel_size=[5, 5],\n                             padding=\"same\",\n                             activation=tf.nn.relu)\n\n    # Max pooling layer 2\n    pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n\n    # Flatten the data to a 1-D vector for the fully connected layer\n    pool2_flat = tf.reshape(pool2, [-1, 8 * 8 * 64])\n\n    # Dense layer\n    dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)\n\n    # Dropout\n    dropout = tf.layers.dropout(inputs=dense, rate=0.4)\n\n    # Logits layer\n    logits = tf.layers.dense(inputs=dropout, units=num_classes)\n\n    # Create the model\n    model = tf.estimator.Estimator(model_fn=cnn_model_fn, model_dir=\"./models\")\n\n    return model\n\n", "CodeBleu_score": 22.78298083380708}]}
{"instance_id": "813e3e2844f4", "generation_result": [{"code": "def extract_bboxes(mask)\n\t\n    boxes = []\n    for i in range(mask.shape[-1]):\n        m = mask[:, :, i]\n        horizontal_indicies = np.where(np.any(m, axis=0))[0]\n        vertical_indicies = np.where(np.any(m, axis=1))[0]\n        if horizontal_indicies.shape[0]:\n            x1, x2 = horizontal_indicies[[0, -1]]\n            y1, y2 = vertical_indicies[[0, -1]]\n            x2 += 1\n            y2 += 1\n        else:\n            x1, x2, y1, y2 = 0, 0, 0, 0\n        boxes.append((x1, y1, x2, y2))\n    return boxes\n\n", "CodeBleu_score": 65.91623935707317}, {"code": "def extract_bboxes(mask)\n\t\n    boxes = []\n    for mask_layer in mask:\n        horizontal_indices = np.where(np.any(mask_layer, axis=0))[0]\n        vertical_indices = np.where(np.any(mask_layer, axis=1))[0]\n        if not horizontal_indices.any() or not vertical_indices.any():\n            bounding_box = [0, 0, 0, 0]\n        else:\n            x1, x2 = horizontal_indices[[0, -1]]\n            y1, y2 = vertical_indices[[0, -1]]\n            bounding_box = np.array([y1, x1, y2, x2])\n        boxes.append(bounding_box)\n    return boxes\n\n", "CodeBleu_score": 41.761943995079044}, {"code": "def extract_bboxes(mask)\n\t\n    boxes = []\n    for i in range(mask.shape[-1]):\n        m = mask[:, :, i]\n        horizontal_indices = scipy.where(m.any(axis=0))[0]\n        vertical_indices = scipy.where(m.any(axis=1))[0]\n        if horizontal_indices.shape[0]:\n            x1, x2 = horizontal_indices[[0, -1]]\n            y1, y2 = vertical_indices[[0, -1]]\n            # x2 and y2 should not be part of the box. Increment by 1.\n            x2 += 1\n            y2 += 1\n        else:\n            # No mask for this instance. Might happen due to\n            # resizing or cropping. Set bbox to zeros\n            x1, x2, y1, y2 = 0, 0, 0, 0\n        boxes.append([x1, y1, x2, y2])\n    return np.array(boxes, dtype=np.int32)\n\n", "CodeBleu_score": 55.63401264607387}]}
{"instance_id": "7ad5d430d6ed", "generation_result": [{"code": "def compute_iou(box, boxes, box_area, boxes_area)\n\t\n    # Calculate the intersection areas\n    iou = np.zeros((boxes.shape[0]))\n    for i in range(boxes.shape[0]):\n        iou[i] = box_intersection_area(box, boxes[i]) / float(box_area + boxes_area[i] - box_intersection_area(box, boxes[i]))\n    return iou\n\n", "CodeBleu_score": 15.78912914508945}, {"code": "def compute_iou(box, boxes, box_area, boxes_area)\n\t\n    # Compute the intersection areas\n    iou = np.zeros((boxes.shape[0]))\n    for i in range(boxes.shape[0]):\n        iou[i] = box_intersection_area(box, boxes[i]) / float(box_area + boxes_area[i] - box_intersection_area(box, boxes[i]))\n    return iou\n", "CodeBleu_score": 15.78912914508945}, {"code": "def compute_iou(box, boxes, box_area, boxes_area)\n\t\n    # Compute the intersection areas\n    iou = np.zeros((boxes.shape[0]))\n    for i in range(boxes.shape[0]):\n        iou[i] = box_intersection_area(box, boxes[i]) / float(box_area + boxes_area[i] - box_intersection_area(box, boxes[i]))\n    return iou\n\n", "CodeBleu_score": 15.78912914508945}]}
{"instance_id": "15941b852859", "generation_result": [{"code": "def compute_overlaps(boxes1, boxes2)\n\t\n    # Compute area of each bounding box\n    area1 = (boxes1[:, 2] - boxes1[:, 0] + 1) * (boxes1[:, 3] - boxes1[:, 1] + 1)\n    area2 = (boxes2[:, 2] - boxes2[:, 0] + 1) * (boxes2[:, 3] - boxes2[:, 1] + 1)\n\n    # Compute overlaps to generate matrix [boxes1 count, boxes2 count]\n    # Each cell contains the IoU value.\n    overlaps = np.zeros((boxes1.shape[0], boxes2.shape[0]))\n    for i in range(overlaps.shape[1]):\n        box2 = boxes2[i]\n        overlaps[:, i] = compute_iou(box2, boxes1, area2[i], area1)\n    return overlaps\n\n", "CodeBleu_score": 55.22725520258685}, {"code": "def compute_overlaps(boxes1, boxes2)\n\t\n    # Compute areas of both boxes\n    areas1 = (boxes1[:, 2] - boxes1[:, 0] + 1) * (boxes1[:, 3] - boxes1[:, 1] + 1)\n    areas2 = (boxes2[:, 2] - boxes2[:, 0] + 1) * (boxes2[:, 3] - boxes2[:, 1] + 1)\n\n    # Compute overlaps to generate matrix [boxes1 count, boxes2 count]\n    # Each cell contains the IoU value.\n    overlaps = np.zeros((boxes1.shape[0], boxes2.shape[0]))\n    for i in range(overlaps.shape[1]):\n        box2 = boxes2[i]\n        overlaps[:, i] = compute_iou(box2, boxes1, areas2[i], areas1)\n    return overlaps\n\n", "CodeBleu_score": 51.62879550873449}, {"code": "def compute_overlaps(boxes1, boxes2)\n\t\n    # Compute areas of both boxes\n    areas1 = (boxes1[:, 2] - boxes1[:, 0] + 1) * (boxes1[:, 3] - boxes1[:, 1] + 1)\n    areas2 = (boxes2[:, 2] - boxes2[:, 0] + 1) * (boxes2[:, 3] - boxes2[:, 1] + 1)\n\n    # Compute overlaps to generate matrix [boxes1 count, boxes2 count]\n    # Each cell contains the IoU value.\n    overlaps = np.zeros((boxes1.shape[0], boxes2.shape[0]))\n    for i in range(overlaps.shape[1]):\n        box2 = boxes2[i]\n        overlaps[:, i] = compute_iou(box2, boxes1, areas2[i], areas1)\n    return overlaps\n\n", "CodeBleu_score": 51.62879550873449}]}
{"instance_id": "597533722ce3", "generation_result": [{"code": "def load_image(self, image_id):\n\t\n    image_info = self.image_info[image_id]\n    image = skimage.io.imread(image_info['path'])\n    # If grayscale. Convert to RGB for consistency.\n    if image.ndim != 3:\n        image = skimage.color.gray2rgb(image)\n    # If has an alpha channel, remove it for consistency\n    if image.shape[-1] == 4:\n        image = image[..., :3]\n    return image\n", "CodeBleu_score": 82.5836239509772}, {"code": "def load_image(self, image_id):\n\t\n    # Load image\n    image = skimage.io.imread(self.image_info[image_id]['path'])\n    # If grayscale. Convert to RGB for consistency.\n    if image.ndim != 3:\n        image = skimage.color.gray2rgb(image)\n    # If has an alpha channel, remove it for consistency\n    if image.shape[-1] == 4:\n        image = image[..., :3]\n    return image\n\n", "CodeBleu_score": 100.0}, {"code": "def load_image(self, image_id):\n\t\n    # Load image\n    image = skimage.io.imread(self.image_info[image_id]['path'])\n    # If grayscale. Convert to RGB for consistency.\n    if image.ndim != 3:\n        image = skimage.color.gray2rgb(image)\n    # If has an alpha channel, remove it for consistency\n    if image.shape[-1] == 4:\n        image = image[..., :3]\n    return image\n\n", "CodeBleu_score": 100.0}]}
{"instance_id": "9b852997ca05", "generation_result": [{"code": "def display_images(images, titles=None, cols=4, cmap=None, norm=None,\n                   interpolation=None)\n\t\n    # Number of images\n    n_images = len(images)\n    # Create a figure with a grid of sub-plots\n    fig, axes = plt.subplots(nrows=n_images // cols + 1, ncols=cols,\n                             figsize=(cols * 2, n_images // cols + 1))\n    # Iterate over the images and titles\n    for i, (image, title) in enumerate(zip(images, titles)):\n        # Set title if none is provided\n        title = title if title is not None else \"\"\n        # Set colormap if none is provided\n        cmap = cmap if cmap is not None else plt.cm.gray\n        # Set interpolation if none is provided\n        interpolation = interpolation if interpolation is not None else 'nearest'\n        # Show each image in a subplot\n        axes[i // cols, i % cols].imshow(image, cmap=cmap, norm=norm,\n                                        interpolation=interpolation)\n        # Remove ticks from the plot\n        axes[i // cols, i % cols].set_xticks([])\n        axes[i // cols, i % cols].set_yticks([])\n        # Set title\n        axes[i // cols, i % cols].set_title(title)\n    # Ensure the plot is shown correctly with multiple plots\n    # in a single Notebook cell.\n    plt.show()\n\n", "CodeBleu_score": 37.008951046048}, {"code": "def display_images(images, titles=None, cols=4, cmap=None, norm=None,\n                   interpolation=None)\n\t\n    # Number of rows in the grid\n    N = len(images)\n    if titles is None:\n        titles = ['(%d)' % i for i in range(1, N + 1)]\n    else:\n        assert len(titles) == N\n    # Create a figure with a grid of sub-plots\n    fig, axes = plt.subplots(ncols=cols, nrows=N // cols, figsize=(cols * 2, N // cols * 2))\n    # Adjust the scaling to fit the window\n    fig.subplots_adjust(wspace=0.05, hspace=0.05)\n    # Iterate over the images and titles\n    for i, (image, title) in enumerate(zip(images, titles)):\n        # Set the title\n        axes[i // cols, i % cols].set_title(title, fontsize=9)\n        # Show the image\n        pimg = axes[i // cols, i % cols].imshow(image, cmap=cmap, norm=norm,\n                                                interpolation=interpolation)\n    # Return the figure\n    return fig\n\n", "CodeBleu_score": 35.16502829011957}, {"code": "def display_images(images, titles=None, cols=4, cmap=None, norm=None,\n                   interpolation=None)\n\t\n    assert ((titles is None) or (len(images) == len(titles)))\n    n_images = len(images)\n    if titles is None:\n        titles = ['Image (%d)' % i for i in range(1, n_images + 1)]\n    else:\n        titles = [('%s (%d)' % (t, i)) for i, t in enumerate(titles, 1)]\n    n_cols = min(n_images, cols)\n    n_rows = n_images // n_cols + 1\n\n    fig = plt.figure()\n    fig.suptitle('Image display')\n    for i, (image, title) in enumerate(zip(images, titles)):\n        sp = plt.subplot(n_rows, n_cols, i + 1)\n        if image.ndim == 2:\n            plt.gray()\n        plt.imshow(image, cmap=cmap, norm=norm, interpolation=interpolation)\n        sp.set_title(title)\n        sp.set_xticks([])\n        sp.set_yticks([])\n    plt.show()\n\n", "CodeBleu_score": 38.98749476536834}]}
