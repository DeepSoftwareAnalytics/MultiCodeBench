{"instance_id": "58936b2a621b", "generation_result": [{"code": "def set_model_params(model: LogisticRegression, params: NDArrays) -> LogisticRegression\n\tmodel.coef_ = params[0]\n    if model.fit_intercept:\n        model.intercept_ = params[1]\n    return model\n\n", "CodeBleu_score": 82.78484165299523}, {"code": "def set_model_params(model: LogisticRegression, params: NDArrays) -> LogisticRegression\n\tmodel.coef_ = params[0]\n    if model.fit_intercept:\n        model.intercept_ = params[1]\n    return model\n\n", "CodeBleu_score": 82.78484165299523}, {"code": "def set_model_params(model: LogisticRegression, params: NDArrays) -> LogisticRegression\n\tmodel.coef_ = params[0]\n    if model.fit_intercept:\n        model.intercept_ = params[1]\n    return model\n\n", "CodeBleu_score": 82.78484165299523}]}
{"instance_id": "83b142d9d251", "generation_result": [{"code": "def support_query_split(\n    data,\n    label,\n    support_ratio: float,\n) -> Tuple[List, List, List, List]\n\tfrom sklearn.model_selection import train_test_split\n\n    x_train, x_test, y_train, y_test = train_test_split(\n        data, label, test_size=1 - support_ratio, random_state=42, stratify=label\n    )\n\n    return x_train, x_test, y_train, y_test\n\n", "CodeBleu_score": 63.65342001799503}, {"code": "def support_query_split(\n    data,\n    label,\n    support_ratio: float,\n) -> Tuple[List, List, List, List]\n\tfrom sklearn.model_selection import train_test_split\n\n    x_train, x_test, y_train, y_test = train_test_split(\n        data,\n        label,\n        test_size=1 - support_ratio,\n        random_state=42,\n        stratify=label,\n    )\n\n    return x_train, x_test, y_train, y_test\n\n", "CodeBleu_score": 61.90281236732943}, {"code": "def support_query_split(\n    data,\n    label,\n    support_ratio: float,\n) -> Tuple[List, List, List, List]\n\tx_train, x_test, y_train, y_test = train_test_split(\n        data, label, test_size=1 - support_ratio, stratify=label, random_state=42\n    )\n    return x_train, x_test, y_train, y_test\n\n", "CodeBleu_score": 71.90266706946817}]}
{"instance_id": "5fd02b540f61", "generation_result": [{"code": "def load_data() -> (\n    Tuple[List[np.ndarray], List[np.ndarray], List[np.ndarray], List[np.ndarray]]\n)\n\tX, y = make_regression(\n        n_samples=100, n_features=3, noise=20, random_state=1\n    )\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.33, random_state=42\n    )\n    return X_train, X_test, y_train, y_test\n\n", "CodeBleu_score": 37.99215071804369}, {"code": "def load_data() -> (\n    Tuple[List[np.ndarray], List[np.ndarray], List[np.ndarray], List[np.ndarray]]\n)\n\tX, y = make_regression(\n        n_samples=100, n_features=3, noise=10, random_state=1\n    )\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.33, random_state=1\n    )\n    return X_train, X_test, y_train, y_test\n\n", "CodeBleu_score": 38.04511624168531}, {"code": "def load_data() -> (\n    Tuple[List[np.ndarray], List[np.ndarray], List[np.ndarray], List[np.ndarray]]\n)\n\tX, y = make_regression(\n        n_samples=100, n_features=3, n_informative=2, random_state=0, shuffle=False\n    )\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=0\n    )\n    return X_train, X_test, y_train, y_test\n\n", "CodeBleu_score": 37.016946670733894}]}
{"instance_id": "ad958cb31a57", "generation_result": [{"code": "def ndarrays_to_gmmparam(ndarrays: NDArray) -> GMMParameters\n\tgmm_param = GMMParameters()\n    gmm_param.label = ndarrays[0]\n    gmm_param.means = ndarrays[1]\n    gmm_param.weights = ndarrays[2]\n    gmm_param.covariances = ndarrays[3]\n    gmm_param.num_samples = ndarrays[4]\n    return gmm_param\n\n", "CodeBleu_score": 44.014097423493624}, {"code": "def ndarrays_to_gmmparam(ndarrays: NDArray) -> GMMParameters\n\tgmm_params = GMMParameters()\n\n    gmm_params.label = ndarrays[0]\n    gmm_params.means = ndarrays[1]\n    gmm_params.weights = ndarrays[2]\n    gmm_params.covariances = ndarrays[3]\n    gmm_params.num_samples = ndarrays[4]\n\n    return gmm_params\n\n", "CodeBleu_score": 43.62839141074365}, {"code": "def ndarrays_to_gmmparam(ndarrays: NDArray) -> GMMParameters\n\tgmmparam = GMMParameters()\n    gmmparam.label = ndarrays[0]\n    gmmparam.means = ndarrays[1]\n    gmmparam.weights = ndarrays[2]\n    gmmparam.covariances = ndarrays[3]\n    gmmparam.num_samples = ndarrays[4]\n    return gmmparam\n\n", "CodeBleu_score": 44.42375056449129}]}
{"instance_id": "061d5a5b9d56", "generation_result": [{"code": "def datafiles_fusion(data_paths):\n\tfrom sklearn.datasets import load_svmlight_file\n    import numpy as np\n\n    X, y = load_svmlight_file(data_paths[0])\n    X = X.todense()\n    for path in data_paths[1:]:\n        X_new, y_new = load_svmlight_file(path)\n        X_new = X_new.todense()\n        if X_new.shape[1] != X.shape[1]:\n            raise ValueError('Number of features in {} does not match'.format(path))\n        X = np.concatenate((X, X_new), axis=0)\n        y = np.concatenate((y, y_new), axis=0)\n    return X, y\n\n\n", "CodeBleu_score": 31.573234139850182}, {"code": "def datafiles_fusion(data_paths):\n\t# Load the first data file\n    X, Y = load_svmlight_file(data_paths[0])\n    X = X.todense()\n    # Load each subsequent data file\n    for i in range(1, len(data_paths)):\n        X_temp, Y_temp = load_svmlight_file(data_paths[i])\n        # Check that the number of features matches the first file\n        if X_temp.shape[1] == X.shape[1]:\n            # Concatenate the features and labels\n            X = np.concatenate((X, X_temp.todense()), axis=0)\n            Y = np.concatenate((Y, Y_temp), axis=0)\n        else:\n            raise ValueError(\"Number of features in file {} does not match.\".format(i))\n    return X, Y\n\n\n", "CodeBleu_score": 40.31147331647874}, {"code": "def datafiles_fusion(data_paths):\n\tX = None\n    Y = None\n    for data_path in data_paths:\n        X_new, Y_new = load_svmlight_file(data_path)\n        X_new = X_new.todense()\n        if X is None:\n            X = X_new\n            Y = Y_new\n        else:\n            assert X.shape[1] == X_new.shape[1]\n            X = np.concatenate((X, X_new))\n            Y = np.concatenate((Y, Y_new))\n    return X, Y\n\n", "CodeBleu_score": 24.682695832190152}]}
{"instance_id": "977d3bd5d5a3", "generation_result": [{"code": "def eval_learning(y_test, y_pred)\n\tfrom sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n    accuracy = accuracy_score(y_test, y_pred)\n    recall = recall_score(y_test, y_pred, average='micro')\n    precision = precision_score(y_test, y_pred, average='micro')\n    f1 = f1_score(y_test, y_pred, average='micro')\n    return accuracy, recall, precision, f1\n\n\n", "CodeBleu_score": 54.50795865138787}, {"code": "def eval_learning(y_test, y_pred)\n\taccuracy = accuracy_score(y_test, y_pred)\n    recall = recall_score(y_test, y_pred, average='micro')\n    precision = precision_score(y_test, y_pred, average='micro')\n    f1 = f1_score(y_test, y_pred, average='micro')\n    return accuracy, recall, precision, f1\n\n\n", "CodeBleu_score": 33.002675070865436}, {"code": "def eval_learning(y_test, y_pred)\n\tfrom sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n    accuracy = accuracy_score(y_test, y_pred)\n    recall = recall_score(y_test, y_pred, average=\"micro\")\n    precision = precision_score(y_test, y_pred, average=\"micro\")\n    f1 = f1_score(y_test, y_pred, average=\"micro\")\n    return accuracy, recall, precision, f1\n\n", "CodeBleu_score": 61.30450430641786}]}
{"instance_id": "f4c838f1f092", "generation_result": [{"code": "def train(model, train_loader, num_epochs=1)\n\tcriterion = nn.BCELoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n    model.train()\n    for epoch in range(num_epochs):\n        for X_batch, y_batch in train_loader:\n            optimizer.zero_grad()\n            outputs = model(X_batch)\n            loss = criterion(outputs, y_batch)\n            loss.backward()\n            optimizer.step()", "CodeBleu_score": 72.56317172205301}, {"code": "def train(model, train_loader, num_epochs=1)\n\tcriterion = nn.BCELoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n    model.train()\n    for epoch in range(num_epochs):\n        for X_batch, y_batch in train_loader:\n            optimizer.zero_grad()\n            outputs = model(X_batch)\n            loss = criterion(outputs, y_batch)\n            loss.backward()\n            optimizer.step()\n\n", "CodeBleu_score": 72.56317172205301}, {"code": "def train(model, train_loader, num_epochs=1)\n\tcriterion = nn.BCELoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n    model.train()\n    for epoch in range(num_epochs):\n        for X_batch, y_batch in train_loader:\n            optimizer.zero_grad()\n            outputs = model(X_batch)\n            loss = criterion(outputs, y_batch)\n            loss.backward()\n            optimizer.step()", "CodeBleu_score": 72.56317172205301}]}
{"instance_id": "35068c779c02", "generation_result": [{"code": "def _load_libsvm_dataset(cfg: DictConfig) -> Dataset\n\tassert cfg.dataset.type == \"libsvm\", \"Dataset type must be libsvm\"\n    dataset_path = cfg.dataset.path\n    dataset_name = cfg.dataset.name\n    data, labels = load_svmlight_file(dataset_path)\n    data = data.todense().astype(np.float32)\n    labels = labels.astype(np.float32)\n    print(f\"Unique labels: {np.unique(labels, return_counts=True)}\")\n    print(f\"Features shape: {data.shape}\")\n    if dataset_name == \"MUSHROOMS\":\n        labels[labels == 1] = 0\n        labels[labels != 0] = 1\n    else:\n        raise RuntimeError(f\"Dataset {dataset_name} not supported\")\n    dataset = TensorDataset(torch.from_numpy(data), torch.from_numpy(labels))\n    return dataset\n\n", "CodeBleu_score": 38.46880024666374}, {"code": "def _load_libsvm_dataset(cfg: DictConfig) -> Dataset\n\tif cfg.dataset.type != \"LIBSVM\":\n        raise RuntimeError(\n            \"This function only supports loading LIBSVM datasets. Please use another function to load your dataset.\"\n        )\n    dataset_path = cfg.dataset.path\n    dataset_name = cfg.dataset.name\n    print(f\"Loading {dataset_name} dataset from {dataset_path}\")\n    data, labels = load_svmlight_file(dataset_path)\n    data = data.todense().astype(np.float32)\n    labels = labels.astype(np.float32)\n    print(f\"Unique labels: {np.unique(labels, return_counts=True)}\")\n    print(f\"Shape of features: {data.shape}\")\n    if dataset_name == \"MUSHROOMS\":\n        labels[labels == 1] = 0\n        labels[labels != 0] = 1\n    else:\n        raise RuntimeError(\n            \"This function only supports loading the MUSHROOMS dataset. Please use another function to load your dataset.\"\n        )\n    dataset = TensorDataset(torch.from_numpy(data), torch.from_numpy(labels))\n    return dataset\n\n\n", "CodeBleu_score": 48.37468663963145}, {"code": "def _load_libsvm_dataset(cfg: DictConfig) -> Dataset\n\tif cfg.dataset.type != 'libsvm':\n        raise RuntimeError('Dataset type must be LIBSVM')\n    path = cfg.dataset.path\n    name = cfg.dataset.name\n    data, labels = load_svmlight_file(os.path.join(path, name))\n    data = data.todense().astype(np.float32)\n    if name == 'mushrooms':\n        labels[labels == 1] = 0\n        labels[labels != 0] = 1\n    print(f'Unique labels: {np.unique(labels, return_counts=True)}')\n    print(f'Shape of features: {data.shape}')\n    dataset = TensorDataset(torch.from_numpy(data), torch.from_numpy(labels))\n    return dataset\n\n", "CodeBleu_score": 40.38340810761506}]}
{"instance_id": "d602a96e571e", "generation_result": [{"code": "def _fit_transform_one(transformer, X, y, weight, **fit_params)\n\tres = transformer.fit_transform(X, y, **fit_params)\n    if weight is not None:\n        res *= weight\n    return res, transformer\n\n", "CodeBleu_score": 23.580853657870595}, {"code": "def _fit_transform_one(transformer, X, y, weight, **fit_params)\n\tres = transformer.fit_transform(X, y, **fit_params)\n    if weight is None:\n        return res, transformer\n    return res * weight, transformer\n\n", "CodeBleu_score": 38.82392474296739}, {"code": "def _fit_transform_one(transformer, X, y, weight, **fit_params)\n\tres = transformer.fit_transform(X, y, **fit_params)\n    if weight is None:\n        return res, transformer\n    return res * weight, transformer\n\n", "CodeBleu_score": 38.82392474296739}]}
{"instance_id": "fcf88a773fd8", "generation_result": [{"code": "def _transform_selected(X, transform, selected=\"all\", copy=True)\n\tX = check_array(X, accept_sparse='csc', copy=copy, dtype=FLOAT_DTYPES)\n    n_samples, n_features = X.shape\n\n    if isinstance(selected, six.string_types) and selected == \"all\":\n        return transform(X)\n    elif (not hasattr(selected, \"__len__\") or\n          not len(selected) or\n          len(selected) == n_features):\n        return X\n\n    if not hasattr(selected, '__len__'):\n        raise ValueError(\"Selected features must be either the string 'all' \"\n                         \"or a list/array of integers or strings.\")\n    if len(selected) > n_features:\n        raise ValueError(\"The number of selected features must be less than \"\n                         \"the number of features of the array to transform.\")\n\n    mask = np.ones(n_features, dtype=np.bool)\n    mask[selected] = False\n\n    if X.nnz != 0:\n        if is_scalar_nan(X.data[0]):\n            # If the array's first value is nan, scipy.sparse.hstack will\n            # raise a ValueError. We work around that by first adding a\n            # non-nan value to X.data, and then removing it from the result.\n            X.data = np.hstack([[0.], X.data])\n            X.data[0] = np.nan\n            to_remove = 1\n        else:\n            to_remove = 0\n\n        # We use hstack to horizontally stack the transformed selected features\n        # and the non-selected features of X.\n        result = sparse_hstack((transform(X[:, selected]), X[:, mask]))\n        if to_remove:\n            result.data = result.data[1:]\n    else:\n        # If X is a sparse matrix of dtype object, then the transformed\n        # selected features will be of dtype float, while the non-selected\n        # features remain of dtype object. As a result, the concatenation\n        # would fail. We work around that by first converting the selected\n        # features to dtype object.", "CodeBleu_score": 42.29512447337376}, {"code": "def _transform_selected(X, transform, selected=\"all\", copy=True)\n\tif not isinstance(X, (list, tuple, np.ndarray, sparse.spmatrix)):\n        raise ValueError(\"X needs to be an array, list, tuple, or sparse matrix.\")\n    if len(X.shape) != 2:\n        raise ValueError(\"X needs to be 2-dimensional.\")\n    if sparse.issparse(X):\n        if X.format != \"csc\":\n            raise ValueError(\"X needs to be in csc format.\")\n        if copy:\n            X = X.copy()\n        else:\n            X = X.tocsc()\n    else:\n        if copy:\n            X = X.copy()\n    if selected == \"all\":\n        return transform(X)\n    elif len(selected) == 0:\n        return X\n    else:\n        X_selected = transform(X[:, selected])\n        n_selected = len(selected)\n        n_features = X.shape[1]\n        if sparse.issparse(X_selected):\n            X_not_selected = X[:, np.arange(n_features) != selected]\n            return sparse.hstack((X_selected, X_not_selected))\n        else:\n            X_not_selected = X[:, np.arange(n_features) != selected]\n            return np.hstack((X_selected, X_not_selected))\n\n", "CodeBleu_score": 41.531683606357305}, {"code": "def _transform_selected(X, transform, selected=\"all\", copy=True)\n\tX = check_array(X, accept_sparse='csc', copy=copy, dtype=FLOAT_DTYPES)\n    n_samples, n_features = X.shape\n\n    if selected == \"all\":\n        return transform(X)\n    elif not selected:\n        return X\n\n    if (not hasattr(selected, \"__len__\") or\n            not isinstance(selected[0], (int, np.integer))):\n        raise ValueError(\"'selected' should be a list of integers\")\n\n    mask = np.ones(n_features, dtype=np.bool)\n    mask[np.asarray(selected)] = False\n\n    if len(selected) == 0:\n        return X[:, mask]\n\n    indices = np.cumsum([0] + [len(t) for t in transform(X[:, selected])])\n    indptr = np.zeros(n_samples + 1, dtype=np.int)\n    indptr[1:] = indices\n    data = np.hstack([t.toarray().ravel() for t in transform(X[:, selected])])\n    X_tr = sp.csr_matrix((data, indices, indptr), shape=(n_samples, len(indices) - 1))\n    return sp.hstack((X[:, mask], X_tr)).tocsr()\n\n", "CodeBleu_score": 44.88130102289942}]}
{"instance_id": "c9dfb00047d1", "generation_result": [{"code": "def _transform_new(self, X)\n\t# Check the input array X and adjust its data type if necessary\n    X_temp = check_array(X, dtype=None)\n    if not hasattr(X, 'dtype') and np.issubdtype(X_temp.dtype, np.str_):\n        X = check_array(X, dtype=np.object)\n    else:\n        X = X_temp\n\n    # Retrieve the number of samples and features in X\n    n_samples, n_features = X.shape\n\n    # Transform the input array into integer-encoded values and generate a mask for valid entries\n    X_int, X_mask = self._transform(X, handle_unknown='error')\n\n    # Calculate the number of categories for each feature\n    n_values = []\n    for i in range(n_features):\n        n_values.append(len(self.categories_[i]))\n\n    # Compute feature indices\n    indices = []\n    indptr = []\n    for i in range(n_features):\n        indices.append(X_int[:, i])\n        indptr.append(np.arange(n_samples + 1))\n\n    # Construct indices and indptr arrays for creating a sparse matrix\n    indices = np.concatenate(indices)\n    indptr = np.concatenate(indptr)\n\n    # The output is a sparse CSR matrix with binary data\n    data = np.ones(n_samples * n_features)\n    X_bin = sparse.csr_matrix((data, indices, indptr),\n                              shape=(n_samples, np.sum(n_values)),\n                              dtype=np.int)\n\n    # Return the transformed data\n    return X_bin\n\n", "CodeBleu_score": 48.020590828763105}, {"code": "def _transform_new(self, X)\n\tX_temp = check_array(X, dtype=None)\n    if not hasattr(X, 'dtype') and np.issubdtype(X_temp.dtype, np.str_):\n        X = check_array(X, dtype=np.object)\n    else:\n        X = X_temp\n\n    n_samples, n_features = X.shape\n    X_int = np.zeros((n_samples, n_features), dtype=np.int)\n    X_mask = np.ones((n_samples, n_features), dtype=np.bool)\n\n    for i in range(n_features):\n        Xi = X[:, i]\n        valid_mask = np.in1d(Xi, self.categories_[i])\n\n        if not np.all(valid_mask):\n            diff = np.unique(X[~valid_mask, i])\n            msg = (\"Found unknown categories {0} in column {1}\"\n                   \" during transform\".format(diff, i))\n            raise ValueError(msg)\n        else:\n            # Set the problematic rows to an acceptable value and\n            # continue `The rows are marked `X_mask` and will be\n            # removed later.\n            X_mask[:, i] = valid_mask\n            Xi = Xi.copy()\n            Xi[~valid_mask] = self.categories_[i][0]\n        X_int[:, i] = self._label_encoders_[i].transform(Xi)\n\n    n_values = [len(cats) for cats in self.categories_]\n    indices = np.zeros(np.sum(n_values), dtype=np.int)\n    indptr = np.zeros(n_samples + 1, dtype=np.int)\n    indptr[1:] = np.cumsum(n_values)\n\n    for i in range(n_features):\n        indices[indptr[0]:indptr[1]] = i\n        indptr[1:] += n_values[i]\n\n    data = np.ones(n_samples * n_features)\n    data[~X_", "CodeBleu_score": 45.788800112353975}, {"code": "def _transform_new(self, X)\n\t# Check input array X and adjust its data type if necessary\n    X_temp = check_array(X, dtype=None)\n    if not hasattr(X, 'dtype') and np.issubdtype(X_temp.dtype, np.str_):\n        X = check_array(X, dtype=np.object)\n    else:\n        X = X_temp\n\n    # Retrieve the number of samples and features in X\n    n_samples, n_features = X.shape\n\n    # Transform the input array into integer-encoded values and generate a mask for valid entries\n    X_int, X_mask = self._transform(X, handle_unknown='error')\n\n    # Calculate the number of categories for each feature\n    n_values = [len(le.classes_) for le in self._label_encoders_]\n\n    # Compute feature indices\n    indices = np.cumsum([0] + n_values)\n\n    # Construct indices and indptr arrays for creating a sparse matrix\n    indptr = (n_samples * indices).astype(np.int)\n    indptr[1:] = np.cumsum(X_mask.sum(axis=0) * n_values)\n\n    # Construct a sparse matrix with binary data\n    data = np.ones(n_samples * sum(n_values))\n    data[~np.ravel(X_mask)] = 0\n    data = data.astype(np.int)\n\n    # Return the transformed data\n    return sparse.csr_matrix((data, X_int.ravel(), indptr),\n                             shape=(n_samples, sum(n_values)),\n                             dtype=self.dtype)\n\n", "CodeBleu_score": 49.61373207142577}]}
{"instance_id": "fcde391a8972", "generation_result": [{"code": "def convert_to_sklearn(self)\n\timport scipy.stats\n    if self.log:\n        return scipy.stats.uniform(self.lower, self.upper)\n    else:\n        return scipy.stats.uniform(self.lower, self.upper)\n\n", "CodeBleu_score": 26.504290662559598}, {"code": "def convert_to_sklearn(self)\n\tfrom scipy.stats import uniform, loguniform\n    if self.log:\n        return loguniform(self.lower, self.upper)\n    else:\n        return uniform(self.lower, self.upper)\n\n", "CodeBleu_score": 33.9907366177407}, {"code": "def convert_to_sklearn(self)\n\timport scipy.stats\n    if self.log:\n        return scipy.stats.uniform(self.lower, self.upper)\n    else:\n        return scipy.stats.uniform(self.lower, self.upper)", "CodeBleu_score": 26.504290662559598}]}
{"instance_id": "5d5413adcca6", "generation_result": [{"code": "def _render(self, state: AnalysisState) -> None\n\tadditional_kwargs, fig_args, features = self._get_args()\n    if self.header is not None:\n        self._render_header(state)\n    fig, axes = plt.subplots(**fig_args)\n    kwargs = {**self.predefined_kwargs, **additional_kwargs}\n    if self.two_way:\n        state.data = state.data.dropna()\n    PartialDependenceDisplay.from_estimator(\n        state.model,\n        state.data,\n        features,\n        **kwargs,\n    ).plot(ax=axes)\n    plt.tight_layout()\n    plt.show()\n\n", "CodeBleu_score": 39.87402033757105}, {"code": "def _render(self, state: AnalysisState) -> None\n\tadditional_kwargs, fig_args, features = self._get_args()\n    if self.headers:\n        self._render_header()\n    fig, axes = plt.subplots(**fig_args)\n    kwargs = {**self.kwargs, **additional_kwargs}\n    if self.two_way:\n        data = state.data.dropna()\n    else:\n        data = state.data\n    PartialDependenceDisplay.from_estimator(\n        state.model,\n        data,\n        features,\n        **kwargs,\n    ).plot(ax=axes)\n    fig.tight_layout()\n    plt.show()\n\n", "CodeBleu_score": 42.33898315787393}, {"code": "def _render(self, state: AnalysisState) -> None\n\tadditional_kwargs, fig_args, features = self._get_args()\n    if self.header:\n        self._render_header()\n    fig, axes = plt.subplots(**fig_args)\n    kwargs = {**self.kwargs, **additional_kwargs}\n    if self.two_way:\n        state.data = state.data.dropna()\n    PartialDependenceDisplay.from_estimator(\n        state.model,\n        state.data,\n        features,\n        **kwargs,\n    ).plot(ax=axes)\n    plt.tight_layout()\n    plt.show()", "CodeBleu_score": 38.75719854446301}]}
{"instance_id": "db8aac29180b", "generation_result": [{"code": "def _preprocess(self, X, **kwargs)\n\tX = super()._preprocess(X, **kwargs)\n    if self._feature_generator is None:\n        self._feature_generator = LabelEncoderFeatureGenerator(verbosity=0)\n        self._feature_generator.fit(X=X)\n    if self._feature_generator.features_in:\n        X = X.copy()\n        X[self._feature_generator.features_in] = self._feature_generator.transform(X=X)\n    X = X.fillna(0).to_numpy(dtype=np.float32)\n    return X\n\n", "CodeBleu_score": 79.24938401754183}, {"code": "def _preprocess(self, X, **kwargs)\n\tX = super()._preprocess(X, **kwargs)\n    if self._feature_generator is None:\n        self._feature_generator = LabelEncoderFeatureGenerator(verbosity=0)\n        self._feature_generator.fit(X=X)\n    if self._feature_generator.features_in:\n        X = X.copy()\n        X[self._feature_generator.features_in] = self._feature_generator.transform(X=X)\n    X = X.fillna(0).to_numpy(dtype=np.float32)\n    return X\n\n", "CodeBleu_score": 79.24938401754183}, {"code": "def _preprocess(self, X, **kwargs)\n\tX = super()._preprocess(X, **kwargs)\n    if self._feature_generator is None:\n        self._feature_generator = LabelEncoderFeatureGenerator(verbosity=0)\n        self._feature_generator.fit(X=X)\n    if self._feature_generator.features_in:\n        X = X.copy()\n        X[self._feature_generator.features_in] = self._feature_generator.transform(X=X)\n    X = X.fillna(0).to_numpy(dtype=np.float32)\n    return X\n\n", "CodeBleu_score": 79.24938401754183}]}
{"instance_id": "d4eb45e6d013", "generation_result": [{"code": "def _estimate_memory_usage(self, X, **kwargs)\n\t# Retrieve model parameters\n    model_params = self.get_params()\n    # Determine the final number of estimators\n    if self.estimator_params.get('n_estimators', None):\n        n_estimators = self.estimator_params['n_estimators']\n    elif self.estimator_params.get('max_iter', None):\n        n_estimators = self.estimator_params['max_iter']\n    elif self.estimator_params.get('max_rounds', None):\n        n_estimators = self.estimator_params['max_rounds']\n    elif self.estimator_params.get('n_iter', None):\n        n_estimators = self.estimator_params['n_iter']\n    else:\n        n_estimators = model_params['n_estimators']\n    # Set a minimum number of estimators\n    if n_estimators < 40 or self.estimator_params.get('n_estimators', None):\n        n_estimators = 40\n    # Determine the number of trees per estimator\n    num_trees_per_estimator = self._get_num_trees_per_estimator()\n    # Estimate the bytes used per estimator\n    bytes_per_estimator = X.memory_usage(index=False).sum() / len(X) * num_trees_per_estimator * 2.5\n    # Calculate the expected minimum memory usage\n    expected_minimum_memory_usage = bytes_per_estimator * n_estimators\n    return expected_minimum_memory_usage", "CodeBleu_score": 46.473687587681475}, {"code": "def _estimate_memory_usage(self, X, **kwargs)\n\t# Retrieve model parameters\n    params = self.get_params()\n    # Determine number of estimators\n    num_estimators = params['n_estimators']\n    if self.problem_type in [MULTICLASS, SOFTCLASS]:\n        if self.num_classes is None:\n            num_estimators = 10  # Guess since it wasn't passed in, could also check y for a better value\n        else:\n            num_estimators = self.num_classes\n    elif self.problem_type == REGRESSION:\n        num_estimators = 100  # Guess since it wasn't passed in\n    elif self.problem_type == BINARY:\n        num_estimators = 100  # Guess since it wasn't passed in\n    # If there is a search space, use the maximum number of estimators\n    if self.estimator_hyperparameter_space is not None:\n        num_estimators = self.estimator_hyperparameter_space['n_estimators'].max()\n    # If there are less than 40 estimators, use 40\n    if num_estimators < 40:\n        num_estimators = 40\n    # Determine number of trees per estimator\n    num_trees_per_estimator = self._get_num_trees_per_estimator()\n    # Estimate memory usage per estimator\n    bytes_per_estimator = X.memory_usage(index=False).sum() * num_trees_per_estimator * 0.000001\n    # Adjust for hyperparameters\n    if self.problem_type in [MULTICLASS, SOFTCLASS]:\n        bytes_per_estimator *= 1.5\n    elif self.problem_type == REGRESSION:\n        bytes_per_estimator *= 1.5\n    elif self.problem_type == BINARY:\n        bytes_per_estimator *= 1.5\n    # Estimate expected memory usage\n    expected_memory_usage = bytes_per_est", "CodeBleu_score": 38.84573758811305}, {"code": "def _estimate_memory_usage(self, X, **kwargs)\n\t# Retrieve model parameters\n    num_classes = self.num_classes\n    problem_type = self.problem_type\n    num_cols = X.shape[1]\n    num_estimators = self.get_num_estimators()\n    if num_estimators is None:\n        num_estimators = 100\n    max_depth = self.max_depth\n    if max_depth is None:\n        max_depth = 6\n    max_features = self.max_features\n    if max_features is None:\n        max_features = 0.5\n    max_bins = self.max_bins\n    if max_bins is None:\n        max_bins = 256\n\n    # Determine the number of estimators\n    if self.estimator_list is not None:\n        num_estimators = len(self.estimator_list)\n    elif self.num_estimators is not None:\n        num_estimators = self.num_estimators\n    elif self.hyperparameter_tune_kwargs is not None and 'num_estimators' in self.hyperparameter_tune_kwargs:\n        num_estimators = self.hyperparameter_tune_kwargs['num_estimators']\n    elif self.hyperparameter_tune_kwargs is not None and 'max_repeats' in self.hyperparameter_tune_kwargs:\n        num_estimators = self.hyperparameter_tune_kwargs['max_repeats']\n    else:\n        num_estimators = 100\n    if num_estimators < 40:\n        num_estimators = 40\n\n    # Determine the number of trees per estimator\n    if problem_type in [MULTICLASS, SOFTCLASS]:\n        if num_classes is None:\n            num_trees_per_estimator = 10  # Guess since it wasn't passed in, could also check y for a better value\n        else:\n            num_trees_per_estimator = num_classes\n    else:\n        num_trees_per_", "CodeBleu_score": 25.602933626353312}]}
{"instance_id": "6923cb007f4c", "generation_result": [{"code": "def downscale_vectorizer(vectorizer, ngram_freq, vocab_size)\n\t# create a Counter object from the n-gram frequency data\n    counter = Counter(ngram_freq)\n\n    # identify the top n most common n-grams\n    top_ngrams = counter.most_common(vocab_size)\n\n    # get the n-grams from the top n most common n-grams\n    top_ngrams = [ngram[0] for ngram in top_ngrams]\n\n    # sort the n-grams\n    top_ngrams.sort()\n\n    # create a new vocabulary dictionary, mapping each n-gram to a unique index\n    new_vocab = {ngram: idx for idx, ngram in enumerate(top_ngrams, start=1)}\n\n    # update the vectorizer's vocabulary with the new, downsized vocabulary\n    vectorizer.vocabulary_ = new_vocab\n\n    return vectorizer\n\n", "CodeBleu_score": 27.792134308710075}, {"code": "def downscale_vectorizer(vectorizer, ngram_freq, vocab_size)\n\tvocab_counter = Counter(ngram_freq)\n    vocab_counter_top_n = vocab_counter.most_common(vocab_size)\n    vocab_top_n = [ngram for ngram, freq in vocab_counter_top_n]\n    vocab_top_n_sorted = sorted(vocab_top_n)\n    vocab_top_n_dict = {ngram: index for index, ngram in enumerate(vocab_top_n_sorted)}\n    vectorizer.vocabulary_ = vocab_top_n_dict\n    return vectorizer\n\n\n", "CodeBleu_score": 25.6624433622775}, {"code": "def downscale_vectorizer(vectorizer, ngram_freq, vocab_size)\n\t# Create a Counter object from the n-gram frequency data\n    counter = Counter(ngram_freq)\n    # Identify the top n most common n-grams according to the specified vocabulary size\n    top_ngrams = counter.most_common(vocab_size)\n    # Create a list of the names of these top n n-grams\n    top_ngram_names = [ngram[0] for ngram in top_ngrams]\n    # Sort the names of these top n n-grams\n    sorted_top_ngram_names = sorted(top_ngram_names)\n    # Create a new vocabulary dictionary, mapping each n-gram to a unique index\n    new_vocab = {ngram: idx for idx, ngram in enumerate(sorted_top_ngram_names, start=1)}\n    # Update the vectorizer's vocabulary with this new, downsized vocabulary\n    vectorizer.vocabulary_ = new_vocab", "CodeBleu_score": 26.95206194327968}]}
{"instance_id": "0e837b2d8088", "generation_result": [{"code": " def _select_continuous(self, df, features)\n\tskewness_threshold = self.parameters[\"skewness_threshold\"]\n    skewness = df[features].skew().to_dict()\n    continuous_features = []\n    skewed_features = []\n\n    for feature in skewness:\n        if skewness[feature] < skewness_threshold:\n            continuous_features.append(feature)\n        else:\n            skewed_features.append(feature)\n\n    return {\"continuous\": continuous_features, \"skewed\": skewed_features}", "CodeBleu_score": 33.14827745128355}, {"code": " def _select_continuous(self, df, features)\n\tskewness_threshold = self.parameters[\"skewness_threshold\"]\n    skewness = df[features].skew().sort_values(ascending=False)\n    continuous_features = skewness[skewness < skewness_threshold].index\n    skewed_features = skewness[skewness >= skewness_threshold].index\n    return {\"continuous\": continuous_features, \"skewed\": skewed_features}", "CodeBleu_score": 23.82693229644533}, {"code": " def _select_continuous(self, df, features)\n\tskewness_threshold = self.parameters['skewness_threshold']\n    skewed_features = {}\n    skewed_features['skewed'] = []\n    skewed_features['continuous'] = []\n\n    for feature in features:\n        if df[feature].skew() > skewness_threshold:\n            skewed_features['skewed'].append(feature)\n        else:\n            skewed_features['continuous'].append(feature)\n\n    return skewed_features", "CodeBleu_score": 43.873114016852455}]}
{"instance_id": "bb569c10f975", "generation_result": [{"code": "def _get_weights(dist, weights):\n\tif weights is None or weights == \"uniform\":\n        return None\n    elif weights == \"distance\":\n        # Check for invalid values in distance matrix\n        if np.any(~np.isfinite(dist)):\n            dist = np.ma.masked_invalid(dist).filled(np.inf)\n        # Calculate weights as 1/distance\n        w = 1 / dist\n        # Set self-distances to 0\n        w[np.diag_indices_from(w)] = 0\n        # Set any rows with bad values to all 0s\n        w[np.any(~np.isfinite(w), axis=1)] = 0\n        # Normalize weights\n        w = w / w.sum(axis=1, keepdims=True)\n        return w\n    elif callable(weights):\n        return weights(dist)\n    else:\n        raise ValueError(\n            \"weights not recognized: should be 'uniform', 'distance', or a callable function\"\n        )\n\n", "CodeBleu_score": 52.90392697738001}, {"code": "def _get_weights(dist, weights):\n\tif weights is None or weights == \"uniform\"::\n        return None\n    elif weights == \"distance\"::\n        if dist.dtype is np.dtype(object)::\n            raise ValueError(\"cannot use 'distance' weights with object dtype distances\")\n        if np.any(dist == 0.0)::\n            raise ValueError(\"cannot use 'distance' weights when distances contain 0.0\")\n        if np.any(np.isinf(dist))::\n            dist = dist.copy()\n            dist[np.isinf(dist)] = np.nan\n            w = 1.0 / dist\n            w[np.isnan(w)] = 0.0\n            return w\n        else::\n            return 1.0 / dist\n    elif callable(weights)::\n        return weights(dist)\n    else::\n        raise ValueError(\"weights not recognized: should be 'uniform', \"\n                         \"'distance', or a callable function\")\n\n", "CodeBleu_score": 44.073076734227705}, {"code": "def _get_weights(dist, weights):\n\tif weights is None or weights == \"uniform\":\n        return None\n    elif weights == \"distance\":\n        if dist.dtype.kind == \"f\":\n            dist = np.where(np.isfinite(dist), dist, 0)\n            if dist.size == 0 or dist.max() == 0:\n                return None\n            dist = 1.0 / dist\n        elif dist.dtype.kind in \"iu\":\n            dist = 1.0 / dist\n        else:\n            raise ValueError(\"weights='distance' not valid for dtype %s\" % dist.dtype)\n        return dist\n    elif callable(weights):\n        return weights(dist)\n    else:\n        raise ValueError(\"weights not recognized: should be 'uniform', 'distance', or a callable function\")\n\n", "CodeBleu_score": 55.879039933017204}]}
{"instance_id": "12aaa9c78402", "generation_result": [{"code": "def predict_loo(self)\n\t# Get the distances and indices of the k-nearest neighbors\n    distances, indices = self.kneighbors(self._X)\n\n    # Get the weights for the neighbors\n    weights = self._get_weights(distances)\n\n    # If the target variable _y is one-dimensional, reshape it to a two-dimensional array\n    if len(self._y.shape) == 1:\n        self._y = self._y.reshape(self._y.shape[0], 1)\n\n    # Make predictions\n    predictions = np.zeros(self._y.shape)\n    for i in range(self._y.shape[0]):\n        if self.weights == 'uniform':\n            predictions[i] = np.mean(self._y[indices[i]], axis=0)\n        elif self.weights == 'distance':\n            predictions[i] = np.sum(self._y[indices[i]] * weights[i], axis=0) / np.sum(weights[i])\n\n    # If the original target variable _y was one-dimensional, flatten the predictions\n    if len(self._y.shape) == 1:\n        predictions = predictions.flatten()\n\n    return predictions", "CodeBleu_score": 32.660778840788154}, {"code": "def predict_loo(self)\n\t# Compute distances and indices of k-nearest neighbors\n    distances, indices = self.kneighbors(self._X)\n    # Get weights for neighbors\n    weights = self._get_weights(distances)\n    # Reshape target variable if necessary\n    if self._y.ndim == 1:\n        self._y = self._y.reshape(-1, 1)\n    # Get predictions\n    if weights is None:\n        predictions = np.mean(self._y[indices], axis=1)\n    else:\n        predictions = np.sum(self._y[indices] * weights, axis=1) / np.sum(weights, axis=1)\n    # Reshape predictions if necessary\n    if self._y.ndim == 1:\n        predictions = predictions.reshape(-1)\n    return predictions\n\n\n", "CodeBleu_score": 30.51259791497921}, {"code": "def predict_loo(self)\n\t# get the distances and indices of the k-nearest neighbors\n    distances, indices = self.kneighbors()\n\n    # get the weights for the neighbors\n    weights = self._get_weights(distances)\n\n    # reshape the target variable if necessary\n    if len(self._y.shape) == 1:\n        self._y = self._y.reshape(-1, 1)\n\n    # compute the predictions\n    predictions = np.zeros(self._y.shape)\n    for i in range(len(self._y)):\n        if self.weights == 'uniform':\n            predictions[i] = np.mean(self._y[indices[i]], axis=0)\n        else:\n            predictions[i] = np.sum(self._y[indices[i]] * weights[i], axis=0) / np.sum(weights[i])\n\n    # reshape the predictions if necessary\n    if len(self._y.shape) == 1:\n        predictions = predictions.reshape(-1)\n\n    return predictions\n\n", "CodeBleu_score": 31.4909058151564}]}
{"instance_id": "b316687d9e9b", "generation_result": [{"code": "def isotonic(input_data, quantile_list)\n\tquantile_list = quantile_list.flatten()\n    batch_size = input_data.shape[0]\n    iso_transformed = []\n    for i in range(batch_size):\n        iso = IsotonicRegression(quantile_list)\n        iso.fit(input_data[i])\n        iso_transformed.append(iso.transform(input_data[i]))\n    iso_transformed = np.stack(iso_transformed)\n    return iso_transformed\n\n\n\n", "CodeBleu_score": 34.890508299035716}, {"code": "def isotonic(input_data, quantile_list)\n\tquantile_list = quantile_list.flatten()\n    batch_size = input_data.shape[0]\n    transformed_data = np.zeros(input_data.shape)\n\n    for i in range(batch_size):\n        iso = IsotonicRegression(quantile_list)\n        transformed_data[i, :] = iso.fit_transform(input_data[i, :])\n\n    return transformed_data\n\n", "CodeBleu_score": 39.58422369149105}, {"code": "def isotonic(input_data, quantile_list)\n\timport numpy as np\n    from sklearn.isotonic import IsotonicRegression\n\n    quantile_list = np.reshape(quantile_list, (len(quantile_list),))\n    batch_size = np.shape(input_data)[0]\n    output_data = np.zeros((batch_size, len(quantile_list)))\n\n    for i in range(batch_size):\n        iso = IsotonicRegression(y_min=0, y_max=1, out_of_bounds='clip')\n        iso.fit(np.arange(0, len(quantile_list)), quantile_list)\n        output_data[i] = iso.transform(input_data[i])\n\n    return output_data\n\n", "CodeBleu_score": 38.706104047545594}]}
{"instance_id": "17566e088b4e", "generation_result": [{"code": "def _train_vectorizer(text_data: list, vectorizer)\n\tvectorizer.fit(text_data)\n    vectorized_data = vectorizer.transform(text_data)\n    vectorizer.stop_words_ = None\n    return vectorizer, vectorized_data\n\n", "CodeBleu_score": 41.26427907092894}, {"code": "def _train_vectorizer(text_data: list, vectorizer)\n\tvectorizer.fit(text_data)\n    vectorizer.stop_words_ = None\n    matrix = vectorizer.transform(text_data)\n    return vectorizer, matrix\n\n", "CodeBleu_score": 45.03098034172488}, {"code": "def _train_vectorizer(text_data: list, vectorizer)\n\tvectorizer.fit(text_data)\n    vectorizer.stop_words_ = None\n    matrix = vectorizer.transform(text_data)\n    return vectorizer, matrix\n\n", "CodeBleu_score": 45.03098034172488}]}
{"instance_id": "be23c0afc2d6", "generation_result": [{"code": "def preprocess(df, test_size, shuffle)\n\tdf = df.dropna()\n    df = df.drop(df.columns[df.nunique() <= 1], axis=1)\n    df = df.drop(df.columns[df.columns.str.contains('id', case=False)], axis=1)\n    df = df.drop(df.columns[df.columns.str.contains('year_of_death', case=False)], axis=1)\n    df = df.drop(df.columns[df.columns.str.contains('days_to_death', case=False)], axis=1)\n    df = df.drop(df.columns[df.columns.str.contains('days_to_death', case=False)], axis=1)\n    df = df.drop(df.columns[df.columns.str.contains('days_to_death', case=False)], axis=1)\n    df = df.drop(df.columns[df.columns.str.contains('days_to_death', case=False)], axis=1)\n    df = df.drop(df.columns[df.columns.str.contains('days_to_death', case=False)], axis=1)\n    df = df.drop(df.columns[df.columns.str.contains('days_to_death', case=False)], axis=1)\n    df = df.drop(df.columns[df.columns.str.contains('days_to_death', case=False)], axis=1)\n    df = df.drop(df.columns[df.columns.str.contains('days_to_death', case=False)], axis=1)\n    df = df.drop(df.columns[df.columns.str.contains('days_to_death', case=False)], axis=1)\n    df = df.drop(df.columns[df.columns.str.contains('days_to_death', case=False)], axis=1)\n    df = df.drop(df.columns[df.columns.str.contains('days_to_death', case=False)], axis=1)\n    df = df.drop(df.columns[df.columns", "CodeBleu_score": 15.938067703916403}, {"code": "def preprocess(df, test_size, shuffle)\n\t# Remove rows with missing entries\n    df = df.replace('--', np.nan)\n    df = df.dropna()\n\n    # Remove columns with unique values less than or equal to one\n    df = df.loc[:, (df != df.iloc[0]).any()]\n\n    # Remove columns with \"id\" in their name\n    df = df.loc[:, ~df.columns.str.contains('id')]\n\n    # Remove shortcut columns\n    df = df.drop(['days_to_death', 'year_of_death', 'year_of_birth', 'days_to_birth'], axis=1)\n\n    # Split data into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(df.drop(['death_cause'], axis=1), df['death_cause'], test_size=test_size, shuffle=shuffle)\n\n    return X_train, X_test, y_train, y_test\n\n", "CodeBleu_score": 22.969710020746636}, {"code": "def preprocess(df, test_size, shuffle)\n\t# Remove rows with missing entries\n    df = df.dropna(axis=0, how='any', subset=df.columns[1:])\n\n    # Remove columns with unique values less than or equal to one\n    df = df.loc[:, (df != df.iloc[0]).any()]\n\n    # Remove columns with \"id\" in their name\n    df = df.loc[:, ~df.columns.str.contains('id')]\n\n    # Remove columns that are shortcuts\n    df = df.drop(columns=['days_to_death', 'year_of_death'])\n\n    # Split data into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(df.drop(columns=['death_event']),\n                                                        df['death_event'],\n                                                        test_size=test_size,\n                                                        shuffle=shuffle)\n\n    # Return training and testing dataframes\n    return X_train, X_test, y_train, y_test\n\n", "CodeBleu_score": 29.486170014459223}]}
{"instance_id": "6a82997a8c59", "generation_result": [{"code": "def _test_classifier_iterative_fit(classifier, dataset=\"iris\", sparse=False)\n\tX_train, Y_train, X_test, Y_test = get_dataset(\n        dataset=dataset, make_sparse=sparse, return_target_as_string=True\n    )\n    cs = classifier.get_hyperparameter_search_space()\n    default = cs.get_default_configuration()\n    classifier = classifier(random_state=1, **default)\n    classifier.fit(X_train, Y_train, refit=True)\n    classifier.fit(X_train, Y_train, refit=True)\n    while not classifier.configuration_fully_fitted():\n        classifier.iterative_fit(X_train, Y_train)\n    predictions = classifier.predict(X_test)\n    return predictions, Y_test, classifier\n\n", "CodeBleu_score": 36.46582290364619}, {"code": "def _test_classifier_iterative_fit(classifier, dataset=\"iris\", sparse=False)\n\tX_train, Y_train, X_test, Y_test = get_dataset(dataset, sparse)\n\n    # obtain the classifier's default configuration\n    cs = classifier.get_hyperparameter_search_space()\n    default = cs.get_default_configuration()\n\n    # initialize the classifier\n    classifier = classifier(random_state=1, **default)\n\n    # fit the classifier iteratively\n    classifier.iterative_fit(X_train, Y_train, n_iter=2, refit=True)\n    n_iter = 2\n    while not classifier.configuration_fully_fitted():\n        n_iter *= 2\n        classifier.iterative_fit(X_train, Y_train, n_iter=n_iter, refit=False)\n\n    # make predictions on the test data\n    predictions = classifier.predict(X_test)\n\n    return predictions, Y_test, classifier\n\n", "CodeBleu_score": 42.57426969995289}, {"code": "def _test_classifier_iterative_fit(classifier, dataset=\"iris\", sparse=False)\n\tX_train, Y_train, X_test, Y_test = get_dataset(\n        dataset=dataset, make_sparse=sparse\n    )\n    cs = classifier.get_hyperparameter_search_space()\n    default = cs.get_default_configuration()\n    classifier = classifier(random_state=1, **{hp_name: default[hp_name] for hp_name in default})\n    classifier.fit(X_train, Y_train, refit=True)\n    for i in range(2, 10):\n        classifier.iterative_fit(X_train, Y_train)\n    predictions = classifier.predict(X_test)\n    return predictions, Y_test, classifier\n\n", "CodeBleu_score": 27.478115026208105}]}
{"instance_id": "d0d8450c4c17", "generation_result": [{"code": "def _load_models(self)\n\tif self._ensemble_class:\n        self._ensemble = self._ensemble_class(\n            ensemble_size=self._ensemble_size,\n            task_type=self._task,\n            metric=self._metric,\n            random_state=self._seed,\n        )\n        self._ensemble.fit(\n            self.runhistory_,\n            self._backend,\n            self._seed,\n            self._resampling_strategy,\n            self._resampling_strategy_arguments,\n        )\n        if self._ensemble.get_selected_model_identifiers():\n            self._identifiers = self._ensemble.get_selected_model_identifiers()\n            self._models = self._backend.load_models_by_identifiers(\n                self._seed, self._identifiers\n            )\n            self._cv_models = self._backend.load_cv_models_by_identifiers(\n                self._seed, self._identifiers\n            )\n        else:\n            self._ensemble = None\n            self._logger.warning(\n                \"No valid ensemble was created. Please check the log file for errors. \"\n                \"Default to the best individual estimator.\"\n            )\n            self._models = []\n            self._cv_models = []\n            self._identifiers = []\n            self._identifiers_cv = []\n            self._weights = []\n            self._weights_cv = []\n\n            if self._resampling_strategy in [\"holdout\", \"holdout-iterative-fit\"]:\n                self._identifiers = self._backend.load_best_individual_model_identifiers(\n                    self._seed\n                )\n                self._models = self._backend.load_models_by_identifiers(\n                    self._seed, self._identifiers\n                )\n            elif self._resampling_strategy in [\n                \"cv\",\n                \"cv-iterative-fit\",\n                \"partial-cv\",\n                \"partial-cv-iterative-fit\",\n            ]:\n                self._identifiers_cv = self._backend.load_best_cv_model_identifiers(\n                    self._seed\n                )\n                self._cv_models = self._backend", "CodeBleu_score": 35.877366451614485}, {"code": "def _load_models(self)\n\t# If ensemble class is defined, load ensemble using backend and seed\n    if self.ensemble_ is not None:\n        self.ensemble_ = self._backend.load_ensemble(self.ensemble_)\n        self.identifiers_with_weights_ = self.ensemble_.get_identifiers_with_weights()\n        self.identifiers_ = [identifier for identifier, _ in self.identifiers_with_weights_]\n        self.weights_ = [weight for _, weight in self.identifiers_with_weights_]\n    else:\n        # If ensemble class is not defined, load best individual model\n        # if conditions are met\n        if (\n            self._resampling_strategy in [\"holdout\", \"holdout-iterative-fit\"]\n            and self._metric._optimum == 1\n            and self._metric._sign == 1\n            and self._metric._maximize\n        ):\n            self.ensemble_ = self._load_best_individual_model()\n            if self.ensemble_ is not None:\n                self.identifiers_with_weights_ = self.ensemble_.get_identifiers_with_weights()\n                self.identifiers_ = [\n                    identifier for identifier, _ in self.identifiers_with_weights_\n                ]\n                self.weights_ = [weight for _, weight in self.identifiers_with_weights_]\n        else:\n            # If no ensemble is loaded, initialize empty lists\n            self.identifiers_with_weights_ = []\n            self.identifiers_ = []\n            self.weights_ = []\n\n    # If ensemble is successfully loaded, retrieve model identifiers and\n    # load corresponding models using backend\n    if self.ensemble_ is not None:\n        self.models_ = [self._backend.load_model(identifier) for identifier in self.identifiers_]\n    else:\n        self.models_ = []\n\n    # For specific resampling strategies, load cross-validated models;\n    # otherwise, set cross-validated models to None\n    if self._resampling_strategy in [\"cv\", \"cv-iterative-fit\"]:\n        self.cv_models_ = [", "CodeBleu_score": 44.79237253250066}, {"code": "def _load_models(self)\n\t# If ensemble class is defined, load the ensemble\n    if self.ensemble_ is not None:\n        self.ensemble_ = self._backend.load_ensemble(\n            self.ensemble_,\n            self._seed,\n            self._metrics,\n            self._task,\n            self._runhistory,\n            self._logger,\n        )\n        self._logger.info(\n            \"Loaded ensemble %s, %s\",\n            self.ensemble_,\n            \"metrics: %s\" % self._metric,\n        )\n\n        # If ensemble is loaded, retrieve model identifiers and load the corresponding models\n        if self.ensemble_:\n            self.identifiers_ = self.ensemble_.get_model_identifiers()\n            self.weights_ = self.ensemble_.get_weights()\n            self.models_ = [\n                self._backend.load_model_by_seed_and_id(seed, identifier)\n                for identifier in self.identifiers_\n            ]\n\n            # For specific resampling strategies, load cross-validated models; otherwise, set to None\n            if self._resampling_strategy in [\n                \"holdout\",\n                \"holdout-iterative-fit\",\n                \"cv\",\n                \"cv-iterative-fit\",\n            ]:\n                self.cv_models_ = [\n                    self._backend.load_cv_model_by_seed_and_id(seed, identifier)\n                    for identifier in self.identifiers_\n                ]\n            else:\n                self.cv_models_ = [None] * len(self.identifiers_)\n\n        # If ensemble is not loaded, load the best individual model\n        else:\n            self._logger.info(\n                \"No valid ensemble was loaded. Default to the best individual estimator\"\n            )\n            self.identifiers_ = self._load_best_individual_model()\n            self.weights_ = [1.0]\n            self.models_ = [self._backend.load_model_by_seed_and_id(seed, self.identifiers_)]\n            self.cv_models_ = [None]\n\n    # If ensemble class is not defined, load the best individual model", "CodeBleu_score": 39.64858010709942}]}
{"instance_id": "ef71b1cddb50", "generation_result": [{"code": "def load_task(task_id)\n\ttask = oml.tasks.get_task(task_id)\n    X, y = task.get_X_and_y()\n    train_indices, test_indices = task.get_train_test_split_indices()\n    X_train = X[train_indices]\n    y_train = y[train_indices]\n    X_test = X[test_indices]\n    y_test = y[test_indices]\n    dataset = oml.datasets.get_dataset(task.dataset_id)\n    categorical_indicator = dataset.get_categorical_indicator()\n    return X_train, y_train, X_test, y_test, categorical_indicator", "CodeBleu_score": 20.6870891284565}, {"code": "def load_task(task_id)\n\t# Import the OpenML library\n    from openml import datasets, tasks\n\n    # Load the task\n    task = tasks.get_task(task_id)\n\n    # Get the dataset\n    dataset = datasets.get_dataset(task.dataset_id)\n\n    # Get the features and labels\n    X, y, categorical_indicator, _ = dataset.get_data(\n        dataset_format=\"array\", target=dataset.default_target_attribute\n    )\n\n    # Get the train-test split indices\n    train_indices, test_indices = task.get_train_test_split_indices()\n\n    # Create training and testing features and labels\n    X_train, y_train = X[train_indices], y[train_indices]\n    X_test, y_test = X[test_indices], y[test_indices]\n\n    # Map unique label values to integer indices\n    y_train = np.searchsorted(np.unique(y_train), y_train)\n    y_test = np.searchsorted(np.unique(y_test), y_test)\n\n    # Return the training features, training labels, testing features, testing labels, and a list indicating whether each feature is categorical or numerical\n    return X_train, y_train, X_test, y_test, categorical_indicator\n\n", "CodeBleu_score": 38.66690095687872}, {"code": "def load_task(task_id)\n\t# Load the task\n    task = openml.tasks.get_task(task_id)\n    # Extract features and labels\n    X, y = task.get_X_and_y()\n    # Get train-test split indices\n    train_indices, test_indices = task.get_train_test_split_indices()\n    # Create training and testing features and labels\n    X_train = X[train_indices]\n    y_train = y[train_indices]\n    X_test = X[test_indices]\n    y_test = y[test_indices]\n    # Get dataset\n    dataset = openml.datasets.get_dataset(task.dataset_id)\n    # Determine categorical or numerical features\n    categorical = [False] * X_train.shape[1]\n    for i in range(X_train.shape[1]):\n        if dataset.features[i].data_type == 'nominal':\n            categorical[i] = True\n    # Map unique label values to integer indices\n    label_encoder = sklearn.preprocessing.LabelEncoder()\n    y_train = label_encoder.fit_transform(y_train)\n    y_test = label_encoder.transform(y_test)\n    return X_train, y_train, X_test, y_test, categorical\n\n", "CodeBleu_score": 40.139011028653904}]}
{"instance_id": "90bd6edbcc9e", "generation_result": [{"code": "def convert_if_sparse(\n    y: SUPPORTED_TARGET_TYPES,\n) -> Union[np.ndarray, List, pd.DataFrame, pd.Series]\n\tif issparse(y):\n        y = y.toarray()\n        if y.ndim == 1:\n            y = y.flatten()\n    return y\n\n", "CodeBleu_score": 51.028960355030314}, {"code": "def convert_if_sparse(\n    y: SUPPORTED_TARGET_TYPES,\n) -> Union[np.ndarray, List, pd.DataFrame, pd.Series]\n\tif isinstance(y, (pd.DataFrame, pd.Series)):\n        y = y.to_numpy()\n\n    if sp.issparse(y):\n        y = y.toarray()\n        if y.ndim == 2 and y.shape[1] == 1:\n            y = y.flatten()\n    return y\n\n", "CodeBleu_score": 53.98627062913768}, {"code": "def convert_if_sparse(\n    y: SUPPORTED_TARGET_TYPES,\n) -> Union[np.ndarray, List, pd.DataFrame, pd.Series]\n\tif sp.issparse(y):\n        y = y.toarray()\n        if y.ndim == 1:\n            y = y.flatten()\n    return y\n\n", "CodeBleu_score": 51.1196969260423}]}
{"instance_id": "5728e3d4f914", "generation_result": [{"code": "def _calculate_weights(self) -> None\n\t# Initialize the weights array\n    weights = np.zeros(self.num_models)\n    # Count the occurrences of each ensemble member\n    for ensemble_member in self.ensemble:\n        weights[ensemble_member] += 1\n    # Calculate the weights\n    weights = weights / self.ensemble.shape[0]\n    # Normalize the weights\n    weights = weights / np.sum(weights)\n    # Store the weights\n    self.weights_ = weights\n\n", "CodeBleu_score": 33.277271802160115}, {"code": "def _calculate_weights(self) -> None\n\t# count occurrences\n    occurrences = np.zeros(self.ensemble_size)\n    for i in range(self.ensemble_size):\n        occurrences[self.ensemble[i]] += 1\n    # calculate weights\n    weights = np.zeros(self.ensemble_size)\n    for i in range(self.ensemble_size):\n        weights[i] = occurrences[i] / self.ensemble_size\n    # normalize weights\n    if sum(weights) < 1:\n        weights = weights / sum(weights)\n    # store weights\n    self.weights_ = weights\n\n", "CodeBleu_score": 33.735293464994186}, {"code": "def _calculate_weights(self) -> None\n\t# Count the occurrences of each ensemble member index\n    occurrences = self.ensemble_members_.value_counts(sort=False)\n\n    # Initialize a weights array with zeros, having a length equal to the number of input models\n    weights = np.zeros(len(self.models_))\n\n    # For each ensemble member, calculate the weight as the frequency divided by the ensemble size and assign it to the corresponding index in the weights array\n    for index, occurrence in occurrences.items():\n        weights[index] = occurrence / len(self.ensemble_members_)\n\n    # If the sum of weights is less than 1, normalize the weights to ensure their sum equals 1\n    if weights.sum() < 1:\n        weights = weights / weights.sum()\n\n    # Store the calculated weights in the instance variable self.weights_\n    self.weights_ = weights\n\n", "CodeBleu_score": 31.4476002682815}]}
{"instance_id": "4f9c4799fd8f", "generation_result": [{"code": "def predict_proba(self, X, batch_size=None, n_jobs=1)\n\tproba = super().predict_proba(X, batch_size=batch_size, n_jobs=n_jobs)\n    if self.automl_.dataset_properties.task_type != MULTILABEL_CLASSIFICATION:\n        assert np.allclose(proba.sum(axis=1), 1), \\\n            \"The sum of probabilities for each instance must be 1.\"\n    assert np.allclose(proba, proba.clip(min=0, max=1)), \\\n        \"All probability values must lie between 0 and 1.\"\n    return proba\n\n", "CodeBleu_score": 33.87665802619939}, {"code": "def predict_proba(self, X, batch_size=None, n_jobs=1)\n\tpredicted_probabilities = super().predict_proba(X, batch_size=batch_size, n_jobs=n_jobs)\n    if self.target_type != \"multilabel-indicator\":\n        assert np.allclose(predicted_probabilities.sum(axis=1), 1)\n    assert np.all(predicted_probabilities >= 0)\n    assert np.all(predicted_probabilities <= 1)\n    return predicted_probabilities\n\n", "CodeBleu_score": 35.20332299842654}, {"code": "def predict_proba(self, X, batch_size=None, n_jobs=1)\n\tproba = super().predict_proba(X, batch_size=batch_size, n_jobs=n_jobs)\n    if self.automl_.dataset_properties.type_of_target != \"multilabel-indicator\":\n        assert np.allclose(proba.sum(axis=1), 1), \"The sum of probabilities for each instance must be 1.\"\n    assert np.all(proba >= 0) and np.all(proba <= 1), \"All probability values must lie between 0 and 1.\"\n    return proba", "CodeBleu_score": 35.60350101228714}]}
{"instance_id": "a9a13150aebe", "generation_result": [{"code": "def convert_to_num(Ybin: np.ndarray) -> np.ndarray:\n\tif len(Ybin.shape) == 1:\n        return Ybin\n    else:\n        return np.dot(Ybin, range(Ybin.shape[1]))\n\n", "CodeBleu_score": 34.975079710234944}, {"code": "def convert_to_num(Ybin: np.ndarray) -> np.ndarray:\n\tif len(Ybin.shape) == 1:\n        return Ybin\n    else:\n        return np.dot(Ybin, range(Ybin.shape[1]))\n\n", "CodeBleu_score": 34.975079710234944}, {"code": "def convert_to_num(Ybin: np.ndarray) -> np.ndarray:\n\tif len(Ybin.shape) == 1:\n        return Ybin\n    else:\n        return np.dot(Ybin, range(Ybin.shape[1]))\n\n", "CodeBleu_score": 34.975079710234944}]}
{"instance_id": "e97d38e62f81", "generation_result": [{"code": "def fit_transformer(self, X, y, fit_params=None)\n\tif fit_params is None:\n        fit_params = {}\n\n    if self.config[\"balancing:strategy\"] == \"weighting\":\n        balancing = Balancing(strategy=\"weighting\")\n        _init_params, _fit_params = balancing.get_weights(\n            y,\n            self.config[\"classifier:__choice__\"],\n            self.config[\"feature_preprocessor:__choice__\"],\n            {},\n            {},\n        )\n        _init_params.update(self.init_params)\n        self.set_hyperparameters(\n            feat_type=self.feat_type,\n            configuration=self.config,\n            init_params=_init_params,\n        )\n\n        if _fit_params is not None:\n            fit_params.update(_fit_params)\n\n    X, fit_params = super().fit_transformer(X, y, fit_params=fit_params)\n\n    return X, fit_params\n\n", "CodeBleu_score": 97.87308038655074}, {"code": "def fit_transformer(self, X, y, fit_params=None)\n\tif fit_params is None:\n        fit_params = {}\n\n    if self.config[\"balancing:strategy\"] == \"weighting\":\n        balancing = Balancing(strategy=\"weighting\")\n        _init_params, _fit_params = balancing.get_weights(\n            y,\n            self.config[\"classifier:__choice__\"],\n            self.config[\"feature_preprocessor:__choice__\"],\n            {},\n            {},\n        )\n        _init_params.update(self.init_params)\n        self.set_hyperparameters(\n            feat_type=self.feat_type,\n            configuration=self.config,\n            init_params=_init_params,\n        )\n\n        if _fit_params is not None:\n            fit_params.update(_fit_params)\n\n    X, fit_params = super().fit_transformer(X, y, fit_params=fit_params)\n\n    return X, fit_params\n\n", "CodeBleu_score": 97.87308038655074}, {"code": "def fit_transformer(self, X, y, fit_params=None)\n\tif fit_params is None:\n        fit_params = {}\n\n    if self.config[\"balancing:strategy\"] == \"weighting\":\n        balancing = Balancing(strategy=\"weighting\")\n        _init_params, _fit_params = balancing.get_weights(\n            y,\n            self.config[\"classifier:__choice__\"],\n            self.config[\"feature_preprocessor:__choice__\"],\n            {},\n            {},\n        )\n        _init_params.update(self.init_params)\n        self.set_hyperparameters(\n            feat_type=self.feat_type,\n            configuration=self.config,\n            init_params=_init_params,\n        )\n\n        if _fit_params is not None:\n            fit_params.update(_fit_params)\n\n    X, fit_params = super().fit_transformer(X, y, fit_params=fit_params)\n\n    return X, fit_params", "CodeBleu_score": 97.87308038655074}]}
{"instance_id": "b1aaad80a9c4", "generation_result": [{"code": "def test_glm_identity_link_func()\n\tfrom statsmodels.genmod.generalized_linear_model import GLM\n    from statsmodels.genmod.families.family import Tweedie\n    from statsmodels.genmod.families.links import Power\n    from statsmodels.api import add_constant\n    from pysd.tools.assemble import assemble_from_dict\n    from pysd.tools.pysd_tools import ModelExpression\n    from pysd.py_backend.functions import Integ, Delay\n    from pysd.py_backend.functions import cache\n    from pysd.py_backend.functions import Lookup\n    from pysd.py_backend.functions import Interpolate\n    from pysd.py_backend.functions import Smooth\n    from pysd.py_backend.functions import Trend\n    from pysd.py_backend.functions import Step\n    from pysd.py_backend.functions import Initial\n    from pysd.py_backend.functions import Pulse\n    from pysd.py_backend.functions import PulseTrain\n    from pysd.py_backend.functions import Monotonic\n    from pysd.py_backend.functions import SampleIfTrue\n    from pysd.py_backend.functions import Time\n    from pysd.py_backend.functions import TimePrevious\n    from pysd.py_backend.functions import TimeStep\n    from pysd.py_backend.functions import TimeChain\n    from pysd.py_backend.functions import Smooth\n    from pysd.py_backend.functions import Macro\n    from pysd.py_backend.functions import While\n    from pysd.py_backend.functions import For\n    from pysd.py_backend.functions import If\n    from pysd.py_backend.functions import Logic\n    from pysd.py_backend.functions import RealityCheck\n    from pysd.py_backend.functions import Log\n    from pysd.py_backend.functions import Lookup\n    from pysd.py_backend.functions import DelayN\n    from pysd.py_backend.functions import DelayFixed\n    from pysd", "CodeBleu_score": 6.202780636311037}, {"code": "def test_glm_identity_link_func()\n\tfrom sklearn import datasets\n    from sklearn.model_selection import train_test_split\n    from sklearn.preprocessing import StandardScaler\n    import pandas as pd\n    import numpy as np\n    from pyspark.sql import SparkSession\n    from pyspark.ml.linalg import Vectors\n    from pyspark.ml.feature import VectorAssembler\n    from pyspark.ml.regression import GeneralizedLinearRegression\n    from pyspark.ml.evaluation import RegressionEvaluator\n    from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n    from pyspark.ml.stat import Correlation\n    import statsmodels.api as sm\n    from statsmodels.formula.api import glm\n    from statsmodels.genmod.families.family import Tweedie\n    from statsmodels.genmod.families.links import Power\n    import statsmodels.formula.api as smf\n    from pyspark.ml.linalg import DenseVector\n    from pyspark.ml.linalg import Vectors\n    from pyspark.ml.linalg import VectorUDT\n    from pyspark.sql.types import *\n    from pyspark.sql.functions import udf\n    import math\n    from pyspark.sql.functions import pandas_udf\n    from pyspark.sql.functions import PandasUDFType\n    import pyspark.sql.functions as F\n    from pyspark.sql.types import StructType, StructField, DoubleType, StringType\n    from pyspark.sql.types import StructType, StructField, DoubleType, StringType\n    import pyspark.sql.functions as F\n    from pyspark.sql.functions import pandas_udf\n    from pyspark.sql.functions import PandasUDFType\n    import pyspark.sql.functions as F\n    from pyspark.sql.types import StructType, StructField, DoubleType, StringType\n    import pyspark.sql.functions as F\n    from pyspark.sql.functions import pandas_udf\n    from pyspark.sql.functions import PandasUDFType\n    import pyspark.sql.functions", "CodeBleu_score": 8.966174541863262}, {"code": "def test_glm_identity_link_func()\n\timport pandas as pd\n    from sklearn.model_selection import train_test_split\n    from sklearn.metrics import mean_squared_error\n    from sklearn.metrics import r2_score\n    from statsmodels.api import GLM\n    from statsmodels.genmod.families.family import Tweedie\n    from statsmodels.genmod.families.links import Power\n    from statsmodels.genmod.generalized_linear_model import GLMResultsWrapper\n    from sklearn.datasets import load_boston\n    from sklearn.preprocessing import StandardScaler\n    from sklearn.pipeline import Pipeline\n    from sklearn.compose import ColumnTransformer\n    from sklearn.preprocessing import OneHotEncoder\n    from sklearn.compose import make_column_transformer\n    from sklearn.preprocessing import FunctionTransformer\n    from sklearn.preprocessing import LabelEncoder\n    from sklearn.preprocessing import OrdinalEncoder\n    from sklearn.impute import SimpleImputer\n    from sklearn.preprocessing import QuantileTransformer\n    from sklearn.pipeline import FeatureUnion\n    from sklearn.base import BaseEstimator, TransformerMixin\n    from sklearn.linear_model import LinearRegression\n    from sklearn.ensemble import RandomForestRegressor\n    from sklearn.svm import SVR\n    from sklearn.neural_network import MLPRegressor\n    from sklearn.neighbors import KNeighborsRegressor\n    from sklearn.linear_model import Ridge\n    from sklearn.linear_model import Lasso\n    from sklearn.linear_model import ElasticNet\n    from sklearn.linear_model import BayesianRidge\n    from sklearn.linear_model import HuberRegressor\n    from sklearn.linear_model import LinearRegression\n    from sklearn.linear_model import RANSACRegressor\n    from sklearn.linear_model import TheilSenRegressor\n    from sklearn.linear_model import ARDRegression\n    from sklearn.gaussian_process import GaussianProcessRegressor\n    from sklearn.tree import DecisionTreeRegressor", "CodeBleu_score": 6.355431627167883}]}
{"instance_id": "bcab63e592d0", "generation_result": [{"code": "def _compute_aggregation(\n        self, col: Variable, agg: Union[str, Callable, Tuple[str, Union[str, Callable]]]\n    ) -> pd.Series\n\tif isinstance(agg, tuple):\n        agg_name, agg_func = agg\n    else:\n        agg_func = agg\n        agg_name = self._get_aggregation_name(agg_func)\n    col_name = col.name\n    new_col_name = f\"{col_name}_{agg_name}\"\n    return self.agg({new_col_name: agg_func})[new_col_name]\n\n", "CodeBleu_score": 52.377989483548724}, {"code": "def _compute_aggregation(\n        self, col: Variable, agg: Union[str, Callable, Tuple[str, Union[str, Callable]]]\n    ) -> pd.Series\n\t# If the aggregation is a string, use it as the function name\n    if isinstance(agg, str):\n        agg_func = getattr(col, agg)\n        agg_name = agg\n    # If the aggregation is a function, use it as the function\n    elif callable(agg):\n        agg_func = agg\n        agg_name = agg.__name__\n    # If the aggregation is a tuple, extract the function name and function\n    elif isinstance(agg, tuple) and len(agg) == 2:\n        agg_name, agg_func = agg\n    # If the aggregation is invalid, raise an exception\n    else:\n        raise ValueError(\n            f\"Invalid aggregation {agg} for column {col.name}; aggregation must be a string, function, or tuple\"\n        )\n    # If the aggregation is a string, use it as the function name\n    if isinstance(agg, str):\n        agg_func = getattr(col, agg)\n        agg_name = agg\n    # If the aggregation is a function, use it as the function\n    elif callable(agg):\n        agg_func = agg\n        agg_name = agg.__name__\n    # If the aggregation is a tuple, extract the function name and function\n    elif isinstance(agg, tuple) and len(agg) == 2:\n        agg_name, agg_func = agg\n    # If the aggregation is invalid, raise an exception\n    else:\n        raise ValueError(\n            f\"Invalid aggregation {agg} for column {col.name}; aggregation must be a string, function, or tuple\"\n        )\n    # Construct the new column name\n    new_col_name = f\"{col.name}_{agg_name}\"\n    # Apply the aggregation\n    agg_result = agg_func()\n    # Rename the column\n    agg_result.name = new_col_name\n    # Return the result\n    return agg_result\n\n\n\n", "CodeBleu_score": 41.58372149278012}, {"code": "def _compute_aggregation(\n        self, col: Variable, agg: Union[str, Callable, Tuple[str, Union[str, Callable]]]\n    ) -> pd.Series\n\tif isinstance(agg, str):\n        name = agg\n        agg = lambda x: getattr(x, agg)()\n    elif isinstance(agg, Callable):\n        name = agg.__name__\n        agg = lambda x: agg(x)\n    elif isinstance(agg, tuple):\n        name, agg = agg\n        if isinstance(agg, str):\n            agg = lambda x: getattr(x, agg)()\n        elif isinstance(agg, Callable):\n            agg = lambda x: agg(x)\n        else:\n            raise ValueError(f\"Invalid aggregation: {agg}\")\n    else:\n        raise ValueError(f\"Invalid aggregation: {agg}\")\n    new_col_name = f\"{col.name}_{name}\"\n    return self.agg({new_col_name: agg})[new_col_name]\n\n", "CodeBleu_score": 50.99113623793494}]}
{"instance_id": "114d623dae7b", "generation_result": [{"code": "def _corrfunc(self, x, y, **kws)\n\timport numpy as np\n    import pandas as pd\n    import matplotlib.pyplot as plt\n    import seaborn as sns\n    import warnings\n    warnings.filterwarnings('ignore')\n    # get the axis\n    ax = kws.pop('ax', plt.gca())\n    # get the hue groups\n    hue = kws.pop('hue', None)\n    # get the data\n    data = pd.concat([x, y], axis=1)\n    # drop missing values\n    data = data.dropna()\n    # calculate the correlation\n    r = np.corrcoef(data[x.name], data[y.name])[0][1]\n    # get the size of the hue groups\n    if hue is not None:\n        n_hue = len(data[hue].unique())\n    else:\n        n_hue = 1\n    # get the font size\n    if n_hue > 1:\n        fontsize = 20 - n_hue\n    else:\n        fontsize = 20\n    # get the position of the annotation\n    if hue is not None:\n        pos = data[hue].unique().tolist().index(data[hue].iloc[0])\n    else:\n        pos = 0\n    # annotate the axis\n    ax.annotate(f'r = {r:.2f}', xy=(0.1, 0.9 - pos*0.05), xycoords='axes fraction', fontsize=fontsize)\n    # return the axis\n    return ax\n\n", "CodeBleu_score": 31.304252625105267}, {"code": "def _corrfunc(self, x, y, **kws)\n\timport seaborn as sns\n    import pandas as pd\n    import numpy as np\n    import matplotlib.pyplot as plt\n    import matplotlib.font_manager as fm\n    import matplotlib.patheffects as pe\n    import matplotlib.lines as mlines\n    import matplotlib.transforms as transforms\n    import matplotlib.patches as mpatches\n    from scipy.stats import pearsonr\n\n    # Get the hue group, if provided\n    hue = kws.pop('hue', None)\n\n    # Get the palette, if provided\n    palette = kws.pop('palette', None)\n\n    # Get the data frame\n    df = kws.pop('data', None)\n\n    # Get the axes\n    ax = kws.pop('ax', None)\n\n    # Get the font size\n    fontsize = kws.pop('fontsize', 12)\n\n    # Get the font color\n    fontcolor = kws.pop('fontcolor', 'black')\n\n    # Get the font weight\n    fontweight = kws.pop('fontweight', 'normal')\n\n    # Get the font family\n    fontfamily = kws.pop('fontfamily', 'sans-serif')\n\n    # Get the linewidth\n    linewidth = kws.pop('linewidth', 1.5)\n\n    # Get the linecolor\n    linecolor = kws.pop('linecolor', 'black')\n\n    # Get the line style\n    linestyle = kws.pop('linestyle', '-')\n\n    # Get the line alpha\n    linealpha = kws.pop('linealpha', 1)\n\n    # Get the linewidth\n    marker = kws.pop('marker', 'o')\n\n    # Get the marker size\n    markersize = kws.pop('markersize', 5)\n\n    # Get the marker edge color\n    markeredgecolor = kws.pop('markeredgecolor', 'black')\n\n    # Get the marker edge width\n    markeredgewidth = kws.pop('markeredgewidth', 1)\n\n    # Get the marker face color\n    markerfacecolor = kws.pop('markerfacecolor',", "CodeBleu_score": 16.47573978090716}, {"code": "def _corrfunc(self, x, y, **kws)\n\timport pandas as pd\n    import numpy as np\n    import matplotlib.pyplot as plt\n    from scipy.stats import pearsonr\n    import seaborn as sns\n    from matplotlib.patheffects import withStroke\n\n    # Calculate the correlation coefficient\n    r, p = pearsonr(x, y)\n    # Handle optional hue groups\n    if 'hue' in kws:\n        hue = kws['hue']\n        # Remove rows where x or y is NaN\n        df = pd.DataFrame({'x': x, 'y': y, 'hue': hue})\n        df = df.dropna()\n        x = df['x']\n        y = df['y']\n        hue = df['hue']\n        # Calculate the correlation coefficient\n        r, p = pearsonr(x, y)\n        # Get the number of hue groups\n        n_hue = len(np.unique(hue))\n        # Calculate the font size based on the number of hue groups\n        fontsize = 16 if n_hue > 1 else 12\n    else:\n        # Calculate the correlation coefficient\n        r, p = pearsonr(x, y)\n        # Get the number of hue groups\n        n_hue = 1\n        # Calculate the font size based on the number of hue groups\n        fontsize = 12\n    # Adjust the font size based on the correlation value\n    if r > 0.9:\n        fontsize += 2\n    elif r > 0.75:\n        fontsize += 1\n    elif r > 0.5:\n        fontsize += 0\n    elif r > 0.25:\n        fontsize -= 1\n    elif r > 0.1:\n        fontsize -= 2\n    else:\n        fontsize -= 3\n    # Get the appropriate axis\n    if 'ax' in kws:\n        ax = kws['ax']\n    else:\n        ax = plt.gca()\n    # Annotate the axis\n    ax.annotate(f'r = {r:.2f}',", "CodeBleu_score": 35.56728981269148}]}
{"instance_id": "4c1ade308953", "generation_result": [{"code": "def _round_digits(src: float, rounddigit: int = None, method='decimal')\n\tif rounddigit is None:\n        return src\n    if method == 'decimal':\n        return round(src, rounddigit)\n    elif method == 'sig':\n        return round(src, rounddigit)\n    elif method == 'format':\n        return round(src, rounddigit)\n    else:\n        raise ValueError(f\"Method '{method}' not recognized. Must be one of 'decimal', 'sig', or 'format'.\")\n\n", "CodeBleu_score": 51.07267233614723}, {"code": "def _round_digits(src: float, rounddigit: int = None, method='decimal')\n\tif rounddigit is None:\n        return src\n    if method == 'decimal':\n        return round(src, rounddigit)\n    elif method == 'sig':\n        return round(Decimal(src), rounddigit)\n    elif method == 'format':\n        return format(src, f'.{rounddigit}g')\n    else:\n        raise ValueError(f'Method {method} not recognized.')\n\n", "CodeBleu_score": 53.30021677624609}, {"code": "def _round_digits(src: float, rounddigit: int = None, method='decimal')\n\tif rounddigit is None:\n        return src\n    if method == 'decimal':\n        return round(src, rounddigit)\n    elif method == 'sig':\n        return round(Decimal(src).quantize(Decimal(10) ** -rounddigit, rounding=ROUND_HALF_UP), rounddigit)\n    elif method == 'format':\n        return round(float(format(src, '.%sf' % rounddigit)), rounddigit)\n    else:\n        raise ValueError(f\"Method must be one of 'decimal', 'sig', or 'format'\")\n\n", "CodeBleu_score": 53.16076452448609}]}
{"instance_id": "a9923e678320", "generation_result": [{"code": "def _round_dict_digits(cls, srcdict: Dict[str, float], rounddigit: int = None, method='decimal')\n\tdstdict = {}\n    for k, v in srcdict.items():\n        if rounddigit is not None and isinstance(v, float):\n            dstdict[k] = _round_digits(v, rounddigit=rounddigit, method=method)\n        else:\n            dstdict[k] = v\n    return dstdict\n", "CodeBleu_score": 75.10124930330284}, {"code": "def _round_dict_digits(cls, srcdict: Dict[str, float], rounddigit: int = None, method='decimal')\n\tdstdict = {}\n    for k, v in srcdict.items():\n        if rounddigit is not None and isinstance(v, float):\n            dstdict[k] = _round_digits(v, rounddigit=rounddigit, method=method)\n        else:\n            dstdict[k] = v\n    return dstdict\n\n", "CodeBleu_score": 75.10124930330284}, {"code": "def _round_dict_digits(cls, srcdict: Dict[str, float], rounddigit: int = None, method='decimal')\n\tdstdict = {}\n    for k, v in srcdict.items():\n        if rounddigit is not None and isinstance(v, float):\n            dstdict[k] = _round_digits(v, rounddigit=rounddigit, method=method)\n        else:\n            dstdict[k] = v\n    return dstdict\n\n", "CodeBleu_score": 75.10124930330284}]}
{"instance_id": "b6106e079dc0", "generation_result": [{"code": "def _scatterplot_ndarray(cls, x, x_name, y, y_name, hue_data, hue_name, ax, scatter_kws, legend_kws)\n\tif hue_data is None:\n        data = pd.DataFrame({x_name: x, y_name: y})\n    else:\n        data = pd.DataFrame({x_name: x, y_name: y, hue_name: hue_data})\n    scatter = sns.scatterplot(x=x_name, y=y_name, hue=hue_name, data=data, ax=ax, **scatter_kws)\n    if hue_data is not None:\n        legend = ax.legend(**legend_kws)\n        if legend_kws.get('title', None) is None:\n            legend.set_title(hue_name)\n    return scatter\n\n", "CodeBleu_score": 46.95798187895895}, {"code": "def _scatterplot_ndarray(cls, x, x_name, y, y_name, hue_data, hue_name, ax, scatter_kws, legend_kws)\n\t# Combine x and y values into a DataFrame\n    df = pd.DataFrame({x_name: x, y_name: y})\n\n    # Add a hue field to the DataFrame if specified\n    if hue_data is not None:\n        df[hue_name] = hue_data\n\n    # Create a scatter plot\n    cls.scatterplot(x=x_name, y=y_name, hue=hue_name, data=df, ax=ax, **scatter_kws)\n\n    # Set the legend title if specified\n    if legend_kws is not None and \"title\" in legend_kws:\n        ax.legend(title=legend_kws[\"title\"])\n    elif hue_name is not None:\n        ax.legend(title=hue_name)\n\n    return ax\n\n", "CodeBleu_score": 33.31746556948108}, {"code": "def _scatterplot_ndarray(cls, x, x_name, y, y_name, hue_data, hue_name, ax, scatter_kws, legend_kws)\n\tif hue_data is not None:\n        df = pd.DataFrame({x_name: x, y_name: y, hue_name: hue_data})\n    else:\n        df = pd.DataFrame({x_name: x, y_name: y})\n\n    sns.scatterplot(x=x_name, y=y_name, hue=hue_name, data=df, ax=ax, **scatter_kws)\n    if hue_data is not None and legend_kws.get(\"title\") is None:\n        legend_kws[\"title\"] = hue_name\n    ax.legend(**legend_kws)\n\n", "CodeBleu_score": 41.206038575463545}]}
{"instance_id": "257e0aa5dd4d", "generation_result": [{"code": "def fit(self, **fit_args) -> \"TimeSeriesModel\"\n\tas_df = self.node.item\n    end = self._get_transformed_data(as_series=True)\n    if self.node.exogenous:\n        ex = as_df[self.node.exogenous]\n    else:\n        ex = None\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\"ignore\", category=UserWarning)\n        warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n        self.model = self.model.fit(y=end, X=ex, **fit_args)\n    return self\n\n", "CodeBleu_score": 81.38523406593907}, {"code": "def fit(self, **fit_args) -> \"TimeSeriesModel\"\n\tas_df = self.node.item\n    end = self._get_transformed_data(as_series=True)\n    if self.node.exogenous:\n        ex = as_df[self.node.exogenous]\n    else:\n        ex = None\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\"ignore\", category=UserWarning)\n        warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n        self.model = self.model.fit(y=end, X=ex, **fit_args)\n    return self\n\n", "CodeBleu_score": 81.38523406593907}, {"code": "def fit(self, **fit_args) -> \"TimeSeriesModel\"\n\tas_df = self.node.item\n    end = self._get_transformed_data(as_series=True)\n    if self.node.exogenous:\n        ex = as_df[self.node.exogenous]\n    else:\n        ex = None\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\"ignore\", category=UserWarning)\n        warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n        self.model = self.model.fit(y=end, X=ex, **fit_args)\n    return self\n\n", "CodeBleu_score": 81.38523406593907}]}
{"instance_id": "6446de2223e8", "generation_result": [{"code": "def create_model(self, **kwargs)\n\tif self.model_type == \"holt_winters\":\n        transformed_data = self.transformed_data.values.flatten()\n        model = ExponentialSmoothing(transformed_data, **kwargs)\n    elif self.model_type == \"auto_arima\":\n        try:\n            from pmdarima import AutoARIMA\n        except ImportError:\n            logger.error(\n                \"pmdarima is not installed. Please install pmdarima to use the auto_arima model type.\"\n            )\n            sys.exit()\n        transformed_data = self.transformed_data.values.flatten()\n        model = AutoARIMA(transformed_data, **kwargs)\n    elif self.model_type == \"sarimax\":\n        transformed_data = self.transformed_data\n        if self.exogenous_variables:\n            model = SARIMAX(\n                transformed_data,\n                exog=self.exogenous_variables,\n                **kwargs,\n            )\n        else:\n            model = SARIMAX(transformed_data, **kwargs)\n    else:\n        raise Exception(\n            f\"{self.model_type} is not a valid model type. Valid model types are 'holt_winters', 'auto_arima', and 'sarimax'.\"\n        )\n    return model\n\n", "CodeBleu_score": 42.72007430629231}, {"code": "def create_model(self, **kwargs)\n\tif self.model_type == \"holt_winters\":\n        transformed_data = self.transformed_data.get_transformed_data(\n            as_series=True\n        )\n        model = ExponentialSmoothing(\n            transformed_data,\n            trend=self.trend,\n            seasonal=self.seasonal,\n            seasonal_periods=self.seasonal_periods,\n        )\n    elif self.model_type == \"auto_arima\":\n        try:\n            from pmdarima import AutoARIMA\n        except ModuleNotFoundError:\n            self.logger.error(\n                \"pmdarima is not installed. Please install pmdarima to use AutoARIMA.\"\n            )\n            sys.exit(1)\n        transformed_data = self.transformed_data.get_transformed_data(\n            as_series=True\n        )\n        model = AutoARIMA(transformed_data, **kwargs)\n    elif self.model_type == \"sarimax\":\n        transformed_data = self.transformed_data.get_transformed_data(\n            as_series=False\n        )\n        if self.exogenous_variables:\n            exogenous_data = self.transformed_data.get_transformed_data(\n                as_series=False\n            )\n            exogenous_data = exogenous_data[self.exogenous_variables]\n            exogenous_data = pandas.concat(\n                [transformed_data, exogenous_data], axis=1\n            )\n        else:\n            exogenous_data = None\n        model = SARIMAX(\n            transformed_data,\n            exogenous=exogenous_data,\n            order=self.order,\n            seasonal_order=self.seasonal_order,\n        )\n    else:\n        raise Exception(\n            f\"Model type {self.model_type} is not recognized. Please specify a valid model type.\"\n        )\n    return model\n\n", "CodeBleu_score": 41.70067637274653}, {"code": "def create_model(self, **kwargs)\n\tmodel_type = self.model_type\n    if model_type == \"holt_winters\":\n        transformed = self._get_transformed_data(as_series=True)\n        model = ExponentialSmoothing(transformed, **kwargs)\n    elif model_type == \"auto_arima\":\n        try:\n            from pmdarima import AutoARIMA\n        except ModuleNotFoundError:\n            self.logger.error(\n                \"pmdarima is not installed. Please install it using the command 'pip install pmdarima'.\"\n            )\n            sys.exit(1)\n        transformed = self._get_transformed_data(as_series=True)\n        model = AutoARIMA(transformed, **kwargs)\n    elif model_type == \"sarimax\":\n        transformed = self._get_transformed_data()\n        exogenous = self.exogenous_variables\n        if exogenous is not None:\n            exogenous = self._get_transformed_data(as_series=False)\n        model = SARIMAX(transformed, exogenous=exogenous, **kwargs)\n    else:\n        raise Exception(f\"Model type {model_type} not recognized.\")\n    return model\n\n", "CodeBleu_score": 45.707029572627874}]}
{"instance_id": "b2f67820e27f", "generation_result": [{"code": "def _to_chx(array)\n\tif isinstance(array, numpy.ndarray):\n        return _from_numpy(array)\n    if cupy is not None and isinstance(array, cupy.ndarray):\n        return _from_cupy(array)\n    return array\n\n", "CodeBleu_score": 87.97048848572042}, {"code": "def _to_chx(array)\n\tif isinstance(array, numpy.ndarray):\n        return _from_numpy(array)\n    elif cupy is not None and isinstance(array, cupy.ndarray):\n        return _from_cupy(array)\n    else:\n        return array\n\n", "CodeBleu_score": 89.76276169788974}, {"code": "def _to_chx(array)\n\tif isinstance(array, numpy.ndarray):\n        return _from_numpy(array)\n    elif cupy is not None and isinstance(array, cupy.ndarray):\n        return _from_cupy(array)\n    else:\n        return array\n\n", "CodeBleu_score": 89.76276169788974}]}
{"instance_id": "b96f7d9bcc95", "generation_result": [{"code": "def _fix_numpy_split(ys, x, indices_or_sections, axis)\n\tif len(ys) == 0:\n        return ys\n    if axis is None:\n        axis = 0\n    if axis < 0:\n        axis += x.ndim\n    if not all(y.ndim == x.ndim for y in ys):\n        shape = list(x.shape)\n        shape[axis] = 0\n        tmp = np.empty(shape, dtype=x.dtype)\n        splits = np.split(tmp, indices_or_sections, axis)\n        sections = [s.shape[axis] for s in splits]\n        for i, y in enumerate(ys):\n            if y.shape[axis] != sections[i]:\n                new_shape = list(y.shape)\n                new_shape[axis] = sections[i]\n                ys[i] = y.reshape(new_shape)\n    return ys\n\n", "CodeBleu_score": 49.019454808468446}, {"code": "def _fix_numpy_split(ys, x, indices_or_sections, axis)\n\tif len(ys) == 0:\n        return ys\n    if axis >= 0:\n        axis = axis - len(x.shape)\n    if axis < 0:\n        axis = axis + len(x.shape)\n    if axis == 0:\n        return ys\n    if axis == 1:\n        if len(ys[0].shape) == 2:\n            return ys\n        else:\n            if indices_or_sections is None:\n                indices_or_sections = x.shape[axis]\n            if isinstance(indices_or_sections, int):\n                indices_or_sections = np.linspace(0, x.shape[axis], indices_or_sections + 1, dtype=np.int32)\n            else:\n                indices_or_sections = np.array(indices_or_sections, dtype=np.int32)\n            split_size = indices_or_sections[1:] - indices_or_sections[:-1]\n            expected_shape = (split_size[0],) + x.shape[1:]\n            for i in range(len(ys)):\n                if ys[i].shape != expected_shape:\n                    ys[i] = ys[i].reshape(expected_shape)\n            return ys\n    else:\n        if len(ys[0].shape) == 3:\n            return ys\n        else:\n            if indices_or_sections is None:\n                indices_or_sections = x.shape[axis]\n            if isinstance(indices_or_sections, int):\n                indices_or_sections = np.linspace(0, x.shape[axis], indices_or_sections + 1, dtype=np.int32)\n            else:\n                indices_or_sections = np.array(indices_or_sections, dtype=np.int32)\n            split_size = indices_or_sections[1:] - indices_or_sections[:-1]\n            expected_shape = (split_size[0],) + x.shape[1:]\n            for i in range(len(ys)):\n                if ys[i].shape != expected_shape", "CodeBleu_score": 31.084309172518537}, {"code": "def _fix_numpy_split(ys, x, indices_or_sections, axis)\n\timport numpy as np\n    if indices_or_sections is None:\n        indices_or_sections = np.array(x.shape[axis])\n    if isinstance(indices_or_sections, int):\n        indices_or_sections = np.array(np.ceil(np.array(x.shape[axis]) / indices_or_sections))\n    else:\n        indices_or_sections = np.array(indices_or_sections)\n    if not (indices_or_sections == np.sort(indices_or_sections)).all():\n        raise ValueError(\"The indices or sections need to be sorted.\")\n    if not (np.array(x.shape[axis]) >= indices_or_sections).all():\n        raise ValueError(\"The length of the axis along which to split elements must be bigger than the number of splits.\")\n    if (indices_or_sections == 0).any():\n        raise ValueError(\"The length of the axis along which to split elements must be bigger than the number of splits.\")\n    if not (indices_or_sections > 0).all():\n        raise ValueError(\"The length of the axis along which to split elements must be bigger than the number of splits.\")\n    if not (np.array(x.shape[axis]) >= indices_or_sections).all():\n        raise ValueError(\"The length of the axis along which to split elements must be bigger than the number of splits.\")\n    if (indices_or_sections == 0).any():\n        raise ValueError(\"The length of the axis along which to split elements must be bigger than the number of splits.\")\n    if not (indices_or_sections > 0).all():\n        raise ValueError(\"The length of the axis along which to split elements must be bigger than the number of splits.\")\n    if not (np.array(x.shape[axis]) >= indices_or_sections).all():\n        raise ValueError(\"The length of the axis along which to split elements must be bigger than the number of splits.\")\n    if (indices_or_sections == 0).any():\n        raise ValueError(\"The length of the axis along which to split elements must be bigger than the number of splits.\")\n    if not", "CodeBleu_score": 19.217637268823328}]}
{"instance_id": "cf39fb123842", "generation_result": [{"code": "def to_device_batch(batch):\n\tif device is None:\n        return batch\n    elif device < 0:\n        return [chainer.dataset.to_device(device, x) for x in batch]\n    else:\n        xp = chainer.cuda.cupy if device >= 0 else np\n        concat = xp.concatenate(batch, axis=0)\n        sections = np.cumsum([len(x) for x in batch[:-1]], dtype=np.int32)\n        res = xp.split(concat, sections)\n        return res", "CodeBleu_score": 66.23440716142147}, {"code": "def to_device_batch(batch):\n\tif device is None:\n        return batch\n    elif device < 0:\n        return [to_device(device, x) for x in batch]\n    else:\n        xp = cuda.cupy.get_array_module(batch[0])\n        concat = xp.concatenate(batch, axis=0)\n        sections = [len(x) for x in batch]\n        device_concat = cuda.to_gpu(concat, device=device)\n        device_batch = cuda.cupy.split(device_concat, sections)\n        return device_batch", "CodeBleu_score": 61.070512559683785}, {"code": "def to_device_batch(batch):\n\tif device is None:\n        return batch\n    elif device < 0:\n        return [chainer.dataset.to_device(device, x) for x in batch]\n    else:\n        xp = chainer.cuda.cupy if device >= 0 else np\n        batch = xp.concatenate(batch, axis=0)\n        sections = np.cumsum([len(x) for x in batch])\n        sections = np.r_[0, sections]\n        batches = xp.split(batch, sections)[:-1]\n        return batches", "CodeBleu_score": 60.646256666945206}]}
{"instance_id": "845c744d897c", "generation_result": [{"code": "def make_image(trainer)\n\timport torch\n    import torchvision.utils as utils\n    import os\n    import numpy as np\n\n    # Set a random seed for reproducibility\n    seed = 123\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n\n    # Determine the number of images to generate\n    n_images = 64\n\n    # Create a latent variable z\n    z = torch.randn(n_images, trainer.z_dim)\n\n    # Generate images using a generator model with the training mode turned off\n    with torch.no_grad():\n        images = trainer.generator(z)\n\n    # Move the generated images to the CPU and scale them to a [0, 255] range\n    images = images.cpu() * 0.5 + 0.5\n\n    # Convert the images to uint8 type\n    images = images.type(torch.uint8)\n\n    # Reshape and transpose the images to form a grid of images\n    images = images.view(n_images, trainer.ch, trainer.image_size, trainer.image_size)\n    images = images.permute(0, 2, 3, 1)\n\n    # Create a directory for saving the preview image if it doesn't exist\n    if not os.path.exists(\"preview\"):\n        os.mkdir(\"preview\")\n\n    # Save the generated image as a PNG file named according to the current training iteration\n    filename = \"preview/preview_%05d.png\" % trainer.iteration\n    utils.save_image(images, filename, nrow=8)\n\n", "CodeBleu_score": 30.052597517681818}, {"code": "def make_image(trainer)\n\tseed = trainer.seed\n    torch.manual_seed(seed)\n    n_images = 16\n    z = torch.randn(n_images, trainer.z_dim)\n    z = z.to(trainer.device)\n    with torch.no_grad():\n        generated_images = trainer.G(z)\n    generated_images = generated_images.detach().cpu()\n    generated_images = generated_images * 0.5 + 0.5\n    generated_images = generated_images.numpy()\n    generated_images = generated_images.transpose(0, 2, 3, 1)\n    generated_images = generated_images * 255\n    generated_images = generated_images.astype(np.uint8)\n    n_rows = 4\n    n_cols = n_images // n_rows\n    generated_images = Image.fromarray(generated_images)\n    generated_images = generated_images.resize((n_cols * 64, n_rows * 64))\n    path = trainer.config.get('preview_dir', 'preview')\n    if not os.path.exists(path):\n        os.makedirs(path)\n    generated_images.save(os.path.join(path, f'{trainer.iteration}.png'))\n\n\n", "CodeBleu_score": 33.21191552124857}, {"code": "def make_image(trainer)\n\t# Set a random seed\n    torch.manual_seed(42)\n\n    # Determine the number of images to generate\n    n_images = 10\n\n    # Create a latent variable z\n    z = torch.randn(n_images,\n                    trainer.model.latent_dim).to(trainer.device)\n\n    # Generate images\n    with torch.no_grad():\n        images = trainer.model.generate(z,\n                                        deterministic=True,\n                                        training=False)\n\n    # Move the images to the CPU and scale them to a [0, 255] range\n    images = images.to(torch.uint8).cpu()\n\n    # Reshape and transpose the images\n    images = images.view(n_images,\n                         trainer.model.channels,\n                         trainer.model.image_size,\n                         trainer.model.image_size)\n    images = images.transpose(2, 3).transpose(1, 2).numpy()\n\n    # Create a directory for saving the preview image if it doesn't exist\n    preview_dir = os.path.join(trainer.log_dir, 'preview')\n    if not os.path.exists(preview_dir):\n        os.makedirs(preview_dir)\n\n    # Save the generated image as a PNG file\n    # named according to the current training iteration\n    filename = os.path.join(preview_dir,\n                            f'preview_{trainer.iteration:05d}.png')\n    imageio.imwrite(filename, images)\n\n", "CodeBleu_score": 35.79969484432052}]}
{"instance_id": "fc2162d43aee", "generation_result": [{"code": "def _cuda_can_cast(from_dtype: npt.DTypeLike, to_dtype: npt.DTypeLike) -> bool\n\tfrom_dtype = np.dtype(from_dtype).type\n    to_dtype = np.dtype(to_dtype).type\n    _typechars = '?bhilqBHILQefdFD'\n    return _typechars.find(from_dtype) <= _typechars.find(to_dtype)\n\n\n\n", "CodeBleu_score": 46.78356545390561}, {"code": "def _cuda_can_cast(from_dtype: npt.DTypeLike, to_dtype: npt.DTypeLike) -> bool\n\tfrom_dtype = np.dtype(from_dtype).type\n    to_dtype = np.dtype(to_dtype).type\n    _typechars = \"efdgFDG\"\n    return _typechars.find(from_dtype.char) <= _typechars.find(to_dtype.char)\n\n\n", "CodeBleu_score": 55.89220554230152}, {"code": "def _cuda_can_cast(from_dtype: npt.DTypeLike, to_dtype: npt.DTypeLike) -> bool\n\tfrom_dtype = np.dtype(from_dtype)\n    to_dtype = np.dtype(to_dtype)\n    if from_dtype == to_dtype:\n        return True\n    # TODO: This is a workaround for the fact that torch.can_cast doesn't\n    # work on CUDA-enabled devices.\n    _typechars = \"?bhilqBHILQefdgFDGOV\"\n    if from_dtype.char not in _typechars:\n        raise TypeError(f\"can_cast: {from_dtype} is not a valid input type\")\n    if to_dtype.char not in _typechars:\n        raise TypeError(f\"can_cast: {to_dtype} is not a valid input type\")\n    return _typechars.find(from_dtype.char) <= _typechars.find(to_dtype.char)", "CodeBleu_score": 47.21072664408185}]}
{"instance_id": "5b8e2d5e6e12", "generation_result": [{"code": "def manual_legend(g, labels, colors, **legend_kwargs)\n\tif isinstance(g, Visualizer):\n        ax = g.ax\n    elif isinstance(g, type(None)):\n        ax = plt.gca()\n    elif isinstance(g, plt.Axes):\n        ax = g\n    else:\n        raise ValueError(\"The input must be either a Visualizer, None, or an Axes object.\")\n    if len(labels) != len(colors):\n        raise ValueError(\"The length of the labels and colors lists must match.\")\n    legend_handles = [patches.Patch(color=c, label=l) for c, l in zip(colors, labels)]\n    return ax.legend(handles=legend_handles, **legend_kwargs)\n\n", "CodeBleu_score": 49.796901582006484}, {"code": "def manual_legend(g, labels, colors, **legend_kwargs)\n\t# Obtain the matplotlib Axes object from the input\n    if g is None:\n        ax = plt.gca()\n    elif isinstance(g, Visualizer):\n        ax = g.ax\n    elif isinstance(g, Axes):\n        ax = g\n    else:\n        raise TypeError(\"The input must be a Visualizer, None, or an Axes object.\")\n\n    # Check that the lengths of the labels and colors lists match\n    if len(labels) != len(colors):\n        raise ValueError(\"The labels and colors lists must be the same length.\")\n\n    # Create legend handles\n    legend_handles = [Patch(color=color, label=label) for color, label in zip(colors, labels)]\n\n    # Return the legend object created with the specified legend handles and additional keyword arguments\n    return ax.legend(handles=legend_handles, **legend_kwargs)", "CodeBleu_score": 53.70820267623492}, {"code": "def manual_legend(g, labels, colors, **legend_kwargs)\n\tfrom matplotlib.patches import Patch\n    from matplotlib.pyplot import gca\n    from sklearn.base import BaseEstimator\n\n    # Obtain the matplotlib Axes object from the input\n    if isinstance(g, BaseEstimator):\n        ax = g.ax\n    elif g is None:\n        ax = gca()\n    elif isinstance(g, type(gca())):\n        ax = g\n    else:\n        raise TypeError(\n            \"The provided plot is not a Visualizer, matplotlib Axes, or None!\"\n        )\n\n    # Check that the lengths of the labels and colors lists match\n    if len(labels) != len(colors):\n        raise ValueError(\n            \"The number of labels and colors must be equal!\"\n        )\n\n    # Create legend handles by pairing each label with its corresponding color\n    legend_handles = [\n        Patch(color=color, label=label) for color, label in zip(colors, labels)\n    ]\n\n    # Create and return the legend\n    return ax.legend(\n        handles=legend_handles,\n        **legend_kwargs\n    )\n\n", "CodeBleu_score": 57.329310873545936}]}
{"instance_id": "494e7f719eb6", "generation_result": [{"code": "def pcoords_time(X, y, fast=True)\n\tfrom yellowbrick.features.pcoords import ParallelCoordinates\n    from time import time\n    from matplotlib import pyplot as plt\n\n    start = time()\n    plt.figure()\n    pcoords = ParallelCoordinates(fast=fast)\n    pcoords.fit(X, y)\n    pcoords.transform(X)\n    end = time()\n    plt.close()\n\n    return end - start\n\n", "CodeBleu_score": 37.920671520874336}, {"code": "def pcoords_time(X, y, fast=True)\n\t# Create a plot axis\n    fig, ax = plt.subplots()\n\n    # Initialize the ParallelCoordinates visualizer with the specified speed setting\n    visualizer = ParallelCoordinates(ax, fast=fast)\n\n    # Record the start time\n    start = time.time()\n\n    # Fit and transform the data\n    visualizer.fit_transform(X, y)\n\n    # Calculate the elapsed time\n    elapsed = time.time() - start\n\n    # Clear and close the plot to free up resources\n    plt.cla()\n    plt.clf()\n    plt.close()\n\n    # Return the elapsed time\n    return elapsed\n\n", "CodeBleu_score": 42.67905785060382}, {"code": "def pcoords_time(X, y, fast=True)\n\timport time\n    from yellowbrick.features import ParallelCoordinates\n    from sklearn.datasets import load_iris\n    from sklearn.model_selection import train_test_split\n\n    # Create a visualizer\n    visualizer = ParallelCoordinates(fast=fast)\n\n    # Create a dataset\n    X, y = load_iris(return_X_y=True)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\n    # Fit and transform the visualizer\n    start = time.time()\n    visualizer.fit(X_train, y_train)\n    visualizer.transform(X_test)\n    end = time.time()\n\n    # Clear and close the figure to free up resources\n    visualizer.poof()\n\n    return end - start\n\n", "CodeBleu_score": 38.7909912812455}]}
{"instance_id": "db4143bc029f", "generation_result": [{"code": "def draw_instances(self, X, y, **kwargs):\n\talpha = kwargs.pop(\"alpha\", self.alpha)\n    ax = kwargs.pop(\"ax\", None)\n    ax = self.ax if ax is None else ax\n    ax.set_xticks(self.xticks)\n\n    # Draw the instances\n    for i, (x, yi) in enumerate(zip(X, y)):\n        color = self.get_colors(yi)\n        ax.scatter(self.xticks, x, color=color, alpha=alpha, **kwargs)\n\n    return ax\n\n", "CodeBleu_score": 37.30455595058985}, {"code": "def draw_instances(self, X, y, **kwargs):\n\t# Retrieve the alpha value from the object's attributes or defaults to 0.25\n    alpha = kwargs.pop(\"alpha\", self.alpha)\n\n    # Get the colors for each of the classes\n    colors = self.get_colors(y)\n\n    # Iterate over each of the instances and plot them\n    for xi, yi, ci in zip(X, y, colors):\n        self.ax.plot(self.increment_, xi, alpha=alpha, color=ci, **kwargs)\n\n    return self.ax\n\n", "CodeBleu_score": 35.60951928552195}, {"code": "def draw_instances(self, X, y, **kwargs):\n\t# Retrieve the alpha value from the object's attributes or defaults to 0.25\n    alpha = getattr(self, \"alpha\", 0.25)\n    # For each instance, extract the feature vector and corresponding label\n    for x, yi in zip(X, y):\n        # Determine the color based on the label\n        color = self.get_colors(yi)\n        # Plot the feature vector against a predefined increment sequence on the axis\n        self.ax.plot(self.feature_sequence, x, color=color, alpha=alpha, **kwargs)\n    # Return the axis object with the plotted instances\n    return self.ax\n\n", "CodeBleu_score": 41.26972854410441}]}
{"instance_id": "676433312ffe", "generation_result": [{"code": "def draw_classes(self, X, y, **kwargs):\n\t# Set the alpha transparency value from the object's attributes or defaults to 0.5\n    alpha = self.alpha\n    if alpha is None:\n        alpha = 0.5\n\n    # Add a column of ones to X to separate the instances visually\n    X = np.c_[X, np.ones(X.shape[0])]\n\n    # Append None to the increments list to create breaks between instances\n    increments = self.increments\n    if increments is not None:\n        increments = increments + [None]\n\n    # Identify unique class labels in y\n    classes = np.unique(y)\n\n    # Iterate over the unique class labels\n    for c in classes:\n\n        # Select the instances of X belonging to that class\n        X_c = X[y == c]\n\n        # Determine the color\n        color = self.get_colors(c)\n\n        # Plot the instances as a single line plot with the specified color and alpha transparency\n        self.ax.plot(X_c[:, :-1], X_c[:, -1], color=color, alpha=alpha, **kwargs)\n\n    # Return the axis object with the plotted classes\n    return self.ax\n\n", "CodeBleu_score": 32.46206115541194}, {"code": "def draw_classes(self, X, y, **kwargs):\n\t# Set alpha transparency from object's attributes or default to 0.5\n    alpha = getattr(self, \"alpha\", 0.5)\n\n    # Add a column of ones to X to separate the instances visually\n    # and append None to the increments list to create breaks between instances\n    X = np.c_[X, np.ones(X.shape[0])]\n    increments = self.increments + [None]\n\n    # Identify unique class labels in y\n    classes = np.unique(y)\n\n    # Iterate over the unique class labels\n    for label in classes:\n\n        # Determine the color for the class\n        color = self.get_colors(label)\n\n        # Select the instances of X belonging to the class\n        instances = X[y == label]\n\n        # Plot the instances as a single line plot with the specified color and alpha transparency\n        self.ax.plot(increments, instances, color=color, alpha=alpha)\n\n    # Return the axis object with the plotted classes\n    return self.ax\n\n", "CodeBleu_score": 28.632966393573554}, {"code": "def draw_classes(self, X, y, **kwargs):\n\t# Set alpha transparency value\n    alpha = self.alpha\n    if alpha is None:\n        alpha = 0.5\n\n    # Add a column of ones to X to separate the instances visually\n    X = np.hstack([X, np.ones((X.shape[0], 1))])\n    increments = []\n\n    # Get the unique class labels in y\n    classes = np.unique(y)\n\n    # Iterate over the unique class labels in y\n    for c in classes:\n        # Determine the color for the class\n        color = self.get_colors(c)[0]\n\n        # Select the instances of X belonging to the class\n        X_c = X[y == c, :]\n\n        # Plot the instances as a single line plot with the specified color and alpha transparency\n        self.ax.plot(X_c[:, 0], X_c[:, 1], color=color, alpha=alpha, **kwargs)\n\n        # Append None to the increments list to create breaks between instances\n        increments.append(None)\n\n    return self.ax\n\n", "CodeBleu_score": 40.11467983489128}]}
{"instance_id": "9d297b12a673", "generation_result": [{"code": "def feature_analysis(fname=\"feature_analysis.png\")\n\tfig, axs = plt.subplots(1, 2, figsize=(12, 6))\n    fig.suptitle(\"Feature Analysis\", fontsize=20)\n\n    # Fit and transform the concrete data\n    concrete_data = load_concrete(split=True)\n    concrete_X_train, concrete_X_test, concrete_y_train, concrete_y_test = concrete_data\n    concrete_X_train = StandardScaler().fit_transform(concrete_X_train)\n    concrete_X_test = StandardScaler().fit_transform(concrete_X_test)\n\n    # Perform Rank2D analysis on the concrete data\n    concrete_ranks = Rank2D(n_features=2)\n    concrete_ranks.fit(concrete_X_train, concrete_y_train)\n    concrete_ranks.transform(concrete_X_train)\n    concrete_ranks.fit_transform(concrete_X_test, concrete_y_test)\n    concrete_ranks.poof(outpath=\"images/concrete_ranks.png\")\n\n    # Perform RadViz analysis on the occupancy data\n    occupancy_data = load_occupancy(split=True)\n    occupancy_X_train, occupancy_X_test, occupancy_y_train, occupancy_y_test = occupancy_data\n    occupancy_ranks = RadViz(classes=[\"unoccupied\", \"occupied\"])\n    occupancy_ranks.fit(occupancy_X_train, occupancy_y_train)\n    occupancy_ranks.transform(occupancy_X_train)\n    occupancy_ranks.fit_transform(occupancy_X_test, occupancy_y_test)\n    occupancy_ranks.poof(outpath=\"images/occupancy_ranks.png\")\n\n    # Plot the concrete data on the left subplot\n    concrete_ranks.poof(ax=axs[0])\n    axs[0].set_title(\"Concrete Data\")\n\n    # Plot the occupancy data on the right subplot\n    occupancy_ranks.poof(ax=axs[", "CodeBleu_score": 31.197840019939484}, {"code": "def feature_analysis(fname=\"feature_analysis.png\")\n\tfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n\n    # Load the data\n    X, y = load_occupancy(split=False)\n\n    # Perform RadViz analysis\n    viz = RadViz(classes=[\"unoccupied\", \"occupied\"], features=X.columns)\n    viz.fit(X, y)\n    viz.finalize(ax1)\n\n    # Load the data\n    X, y = load_concrete(split=False)\n\n    # Perform Rank2D analysis\n    viz = Rank2D(features=X.columns)\n    viz.fit(X, y)\n    viz.transform(X)\n    viz.finalize(ax2)\n\n    # Save the figure\n    fig.savefig(fname, bbox_inches=\"tight\")\n\n", "CodeBleu_score": 30.146858133310644}, {"code": "def feature_analysis(fname=\"feature_analysis.png\")\n\t# Set up the figure and axes grid\n    fig, axes = plt.subplots(ncols=2, figsize=(16, 6))\n\n    # Perform RadViz analysis on the left subplot\n    X, y = load_occupancy(split=False)\n    radviz = RadViz(X, y, ax=axes[0])\n    radviz.fit()\n    radviz.finalize()\n\n    # Perform Rank2D analysis on the right subplot\n    X, y = load_concrete(split=False)\n    rank2d = Rank2D(X, y, ax=axes[1])\n    rank2d.fit()\n    rank2d.transform()\n    rank2d.finalize()\n\n    # Save the figure\n    fig.savefig(fname)\n\n", "CodeBleu_score": 25.361628391820535}]}
{"instance_id": "fd1b4b9cd7ef", "generation_result": [{"code": "def regression(fname=\"regression.png\")\n\tfig, axs = plt.subplots(1, 2, figsize=(18, 6))\n    X, y = load_concrete(split=True)\n    alphas = np.logspace(-5, -2, 50)\n    for a, ax in zip(alphas, axs):\n        lasso = LassoCV(alphas=[a], random_state=0).fit(X.train, y.train)\n        ax.plot(\n            alphas,\n            -cross_val_score(lasso, X.test, y.test, scoring=\"neg_mean_squared_error\"),\n        )\n        ax.set_xscale(\"log\")\n        ax.set_xlabel(\"alpha\")\n        ax.set_ylabel(\"MSE\")\n        ax.set_title(\"LassoCV\")\n\n        ridge = RidgeCV(alphas=[a], random_state=0).fit(X.train, y.train)\n        ax.plot(\n            alphas,\n            -cross_val_score(ridge, X.test, y.test, scoring=\"neg_mean_squared_error\"),\n        )\n        ax.set_xscale(\"log\")\n        ax.set_xlabel(\"alpha\")\n        ax.set_ylabel(\"MSE\")\n        ax.set_title(\"RidgeCV\")\n\n    fig.tight_layout()\n    fig.savefig(os.path.join(IMAGES, fname))\n    return os.path.join(IMAGES, fname)\n\n", "CodeBleu_score": 34.74348937913386}, {"code": "def regression(fname=\"regression.png\")\n\talphas = [0.01, 0.03, 0.05, 0.07, 0.1, 0.3, 0.5, 0.7, 1, 3, 5, 7, 10]\n    X_train, X_test, y_train, y_test = load_concrete(split=True)\n\n    fig, ax = plt.subplots(1, 2, figsize=(18, 6))\n\n    for alpha in alphas:\n        model = LassoCV(alphas=[alpha], random_state=1).fit(X_train, y_train)\n        y_pred = model.predict(X_test)\n        ax[0].plot(\n            np.log10(alpha),\n            mean_squared_error(y_test, y_pred),\n            \"o-\",\n            label=\"alpha = {}\".format(alpha),\n        )\n\n        model = RidgeCV(alphas=[alpha], random_state=1).fit(X_train, y_train)\n        y_pred = model.predict(X_test)\n        ax[1].plot(\n            np.log10(alpha),\n            mean_squared_error(y_test, y_pred),\n            \"o-\",\n            label=\"alpha = {}\".format(alpha),\n        )\n\n    ax[0].set_xlabel(\"log(alpha)\")\n    ax[0].set_ylabel(\"MSE\")\n    ax[0].legend()\n    ax[0].set_title(\"LassoCV\")\n\n    ax[1].set_xlabel(\"log(alpha)\")\n    ax[1].set_ylabel(\"MSE\")\n    ax[1].legend()\n    ax[1].set_title(\"RidgeCV\")\n\n    fig.tight_layout()\n    fig.savefig(fname)\n    return fname", "CodeBleu_score": 29.60721979003027}, {"code": "def regression(fname=\"regression.png\")\n\talphas = np.logspace(-10, 10, 100)\n    X_train, X_test, y_train, y_test = load_concrete(split=True)\n\n    fig, axs = plt.subplots(2, 1, figsize=(18, 6))\n\n    for alpha, ax in zip(alphas, axs):\n        model = LassoCV(alphas=[alpha], cv=5, random_state=42)\n        model.fit(X_train, y_train)\n        ax.plot(\n            alphas,\n            -cross_val_score(model, X_test, y_test, cv=5, scoring=\"neg_mean_squared_error\"),\n            label=f\"alpha={alpha:.2f}\",\n        )\n        ax.set_xscale(\"log\")\n        ax.set_xlabel(\"alpha\")\n        ax.set_ylabel(\"MSE\")\n        ax.legend()\n\n    model = RidgeCV(alphas=alphas, cv=5, random_state=42)\n    model.fit(X_train, y_train)\n    axs[1].plot(\n        alphas,\n        -cross_val_score(model, X_test, y_test, cv=5, scoring=\"neg_mean_squared_error\"),\n    )\n    axs[1].set_xscale(\"log\")\n    axs[1].set_xlabel(\"alpha\")\n    axs[1].set_ylabel(\"MSE\")\n\n    fig.tight_layout()\n    fig.savefig(fname)\n    return fname\n\n", "CodeBleu_score": 34.17560667000643}]}
{"instance_id": "627126918f88", "generation_result": [{"code": "def classification(fname=\"classification.png\")\n\tfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n    X_train, X_test, y_train, y_test = load_spam(split=True)\n    model = MultinomialNB()\n    model.fit(X_train, y_train)\n    plot_classification_report(model, X_test, y_test, ax=ax1)\n    X, y = load_spam()\n    model = LogisticRegression()\n    model.fit(X, y)\n    plot_discrimination_threshold(model, X, y, ax=ax2)\n    fig.tight_layout()\n    fig.savefig(fname)\n\n", "CodeBleu_score": 21.386674137575017}, {"code": "def classification(fname=\"classification.png\")\n\tfig, axes = plt.subplots(ncols=2, figsize=(12, 4))\n\n    X, y, X_train, X_test, y_train, y_test = load_spam(split=True)\n    clf = MultinomialNB()\n    clf.fit(X_train, y_train)\n    y_pred = clf.predict(X_test)\n    cr = classification_report(y_test, y_pred)\n    plot_classification_report(cr, ax=axes[0])\n\n    X, y = load_spam(split=False)\n    clf = LogisticRegression()\n    clf.fit(X, y)\n    plot_discrimination_threshold(clf, X, y, ax=axes[1])\n\n    fig.tight_layout()\n    fig.savefig(fname)\n\n", "CodeBleu_score": 26.810261206150116}, {"code": "def classification(fname=\"classification.png\")\n\tfig, ax = plt.subplots(1, 2, figsize=(15, 5))\n\n    X_train, X_test, y_train, y_test = load_spam(split=True)\n    clf = MultinomialNB()\n    clf.fit(X_train, y_train)\n    y_pred = clf.predict(X_test)\n\n    cr = classification_report(y_test, y_pred, output_dict=True)\n    cr = pd.DataFrame(cr).transpose()\n    cr.index = [\"F1-score\", \"Precision\", \"Recall\", \"Support\"]\n    cr.plot.barh(ax=ax[0], title=\"MultinomialNB\")\n\n    X, y = load_spam()\n    clf = LogisticRegression()\n    clf.fit(X, y)\n    y_pred = clf.predict_proba(X)[:, 1]\n\n    dt = discrimination_threshold(y, y_pred)\n    dt.plot(ax=ax[1], title=\"LogisticRegression\")\n\n    plt.tight_layout()\n    plt.savefig(fname)\n\n", "CodeBleu_score": 42.2571025714161}]}
{"instance_id": "318d3a674694", "generation_result": [{"code": "def get_color_cycle()\n\timport matplotlib as mpl\n    import matplotlib.pyplot as plt\n    import sys\n    import warnings\n\n    if sys.version_info[0] > 2:\n        string_types = (str,)\n    else:\n        string_types = (basestring,)  # noqa: F821\n\n    if mpl.__version__ < '1.5.0' or not isinstance(plt.rcParams['axes.prop_cycle'], string_types):\n        # version of matplotlib is < 1.5.0:\n        # the old default color cycle:\n        return plt.rcParams['axes.color_cycle']\n    else:\n        # the new default color cycle:\n        try:\n            return plt.rcParams['axes.prop_cycle'].by_key()['color']\n        except KeyError:\n            warnings.warn(\"Please set the color cycle using \"\n                          \"rcParams['axes.prop_cycle']\")\n            return plt.rcParams['axes.color_cycle']\n\n", "CodeBleu_score": 33.38566752457899}, {"code": "def get_color_cycle()\n\timport matplotlib as mpl\n    import matplotlib.pyplot as plt\n    import warnings\n    import numpy as np\n\n    # matplotlib 1.5.0 and above\n    if int(mpl.__version__[0]) >= 2:\n        try:\n            color_cycle = plt.rcParams['axes.prop_cycle'].by_key()['color']\n            return color_cycle\n        except KeyError:\n            warnings.warn(\"Unable to get color cycle from matplotlib.axes.prop_cycle. \"\n                          \"Falling back to matplotlib.axes.color_cycle\")\n            return plt.rcParams['axes.color_cycle']\n    # matplotlib 1.4.x and below\n    else:\n        return plt.rcParams['axes.color_cycle']\n\n", "CodeBleu_score": 34.51117674663783}, {"code": "def get_color_cycle()\n\timport matplotlib as mpl\n    import matplotlib.pyplot as plt\n    try:\n        if mpl.__version__ >= '1.5.0':\n            return plt.rcParams['axes.prop_cycle'].by_key()['color']\n    except KeyError:\n        pass\n    return plt.rcParams['axes.color_cycle']\n\n", "CodeBleu_score": 38.613353337696}]}
{"instance_id": "f3fac557334c", "generation_result": [{"code": "def plot_knee_normalized(\n        self,\n    )\n\timport matplotlib.pyplot as plt\n    plt.figure(figsize=(8,8))\n    plt.plot(self.normalized_x, self.normalized_y, 'b', label='Normalized Curve')\n    plt.plot(self.x, self.distance, 'r', label='Distance Curve')\n    plt.xticks(np.arange(min(self.x), max(self.x)+0.1, 0.1))\n    plt.yticks(np.arange(min(self.y), max(self.y)+0.1, 0.1))\n    plt.xlabel('x')\n    plt.ylabel('y')\n    plt.legend()\n    if self.knee:\n        plt.axvline(x=self.knee, color='k', linestyle='--')\n    plt.show()", "CodeBleu_score": 43.51903137373052}, {"code": "def plot_knee_normalized(\n        self,\n    )\n\timport matplotlib.pyplot as plt\n    fig, ax = plt.subplots(figsize=(8,8))\n    ax.plot(self.x_normalized, self.y_normalized, color='blue')\n    ax.plot(self.x_normalized, self.distance_curve, color='red')\n    ax.set_xlabel('Normalized X')\n    ax.set_ylabel('Normalized Y')\n    ax.set_xticks(np.arange(0, 1.2, 0.1))\n    ax.set_yticks(np.arange(0, 1.2, 0.1))\n    if self.knee_point:\n        ax.axvline(x=self.knee_point, color='black', linestyle='--')\n    plt.show()\n\n\n", "CodeBleu_score": 37.574495575930186}, {"code": "def plot_knee_normalized(\n        self,\n    )\n\timport matplotlib.pyplot as plt\n    fig = plt.figure(figsize=(8,8))\n    plt.plot(self.normalized_x, self.normalized_y, 'b')\n    plt.plot(self.normalized_x, self.distance, 'r')\n    plt.xlabel('Normalized x')\n    plt.ylabel('Normalized y')\n    plt.xticks(np.arange(np.min(self.normalized_x), np.max(self.normalized_x)+0.1, 0.1))\n    plt.yticks(np.arange(np.min(self.normalized_y), np.max(self.normalized_y)+0.1, 0.1))\n    if self.knee_point:\n        plt.axvline(x=self.knee_point, color='r', linestyle='--')\n    plt.show()\n\n\n", "CodeBleu_score": 44.10151931528144}]}
{"instance_id": "b2d48aae4fce", "generation_result": [{"code": "def plot_knee(\n        self,\n    )\n\timport matplotlib.pyplot as plt\n    import matplotlib.patches as mpatches\n\n    fig, ax = plt.subplots(figsize=(8, 8))\n    ax.plot(self.x, self.y, color='blue', marker='o')\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    ax.set_title('Knee Point')\n\n    if self.knee_x is not None:\n        ax.axvline(self.knee_x, color='red')\n\n    ax.legend(handles=[\n        mpatches.Patch(color='blue', label='Data'),\n        mpatches.Patch(color='red', label='Knee')\n    ])\n\n    plt.show()", "CodeBleu_score": 41.276641216084265}, {"code": "def plot_knee(\n        self,\n    )\n\t# Import modules ...\n    import matplotlib\n    import matplotlib.pyplot\n    import numpy\n\n    # Load sub-functions ...\n    from .plot_curve import plot_curve\n\n    # Create figure ...\n    fgu = matplotlib.pyplot.figure(figsize = (8.0, 8.0))                      # [cm]\n    ax1 = fgu.gca()\n\n    # Plot curve ...\n    plot_curve(\n        x = self.x,\n        y = self.y,\n        xlab = self.xlab,\n        ylab = self.ylab,\n        fgu = fgu,\n        ax = ax1,\n    )\n\n    # Find knee point ...\n    x = self.x\n    y = self.y\n    i = numpy.argmax(y)\n    x1 = x[i]\n    y1 = y[i]\n    i = numpy.argmin(numpy.abs(y - y1))\n    x2 = x[i]\n    y2 = y[i]\n    i = numpy.argmax(numpy.abs(y - y1))\n    x3 = x[i]\n    y3 = y[i]\n    x4 = (x2 + x3) / 2.0\n    y4 = (y2 + y3) / 2.0\n    x5 = (x1 + x4) / 2.0\n    y5 = (y1 + y4) / 2.0\n    x6 = (x1 + x5) / 2.0\n    y6 = (y1 + y5) / 2.0\n    x7 = (x6 + x4) / 2.0\n    y7 = (y6 + y4) / 2.0\n    x8 = (x7 + x3) / 2.0\n    y8 = (y7 + y3) / 2.0\n    x9 = (x8 + x2) / 2.0\n    y9 = (y8 + y2) / 2.0\n    x10 = (x9 + x1) / 2.0\n    y10", "CodeBleu_score": 29.108231420900065}, {"code": "def plot_knee(\n        self,\n    )\n\timport matplotlib.pyplot as plt\n    fig = plt.figure(figsize=(8,8))\n    ax = fig.add_subplot(111)\n    ax.plot(self.x, self.y, 'b-', label='curve')\n    ax.axvline(self.knee, color='r', label='knee')\n    ax.legend()\n    plt.show()\n    return", "CodeBleu_score": 30.817137466239586}]}
{"instance_id": "4450ba2384c9", "generation_result": [{"code": "def plot_pca_interactive(data, n_components=6)\n\tfrom sklearn.decomposition import PCA\n    pca = PCA(n_components=n_components)\n    pca.fit(data)\n    X_pca = pca.transform(data)\n\n    def show_decomp(index):\n        \"\"\"# This function plots the original data and its PCA components for a selected index.", "CodeBleu_score": 45.839080965931515}, {"code": "def plot_pca_interactive(data, n_components=6)\n\timport IPython.display as ipd\n    from sklearn.decomposition import PCA\n\n    pca = PCA(n_components=n_components)\n    pca.fit(data)\n    data = pca.transform(data)\n\n    def show_decomp(idx):\n        \"\"\"# This internal function plots the original data and its PCA components for a selected index.", "CodeBleu_score": 49.12032391361274}, {"code": "def plot_pca_interactive(data, n_components=6)\n\tfrom sklearn.decomposition import PCA\n    from IPython.html.widgets import interact\n\n    pca = PCA(n_components=n_components)\n    pca.fit(data)\n    transformed = pca.transform(data)\n    coef = pca.components_\n\n    def show_decomp(index):\n        x = data[index]\n        coefficients = transformed[index]\n        plot_image_components(x, coefficients, components=coef,\n                              n_components=n_components)\n\n    interact(show_decomp, index=(0, len(data) - 1))\n\n", "CodeBleu_score": 61.868142767563214}]}
{"instance_id": "b09a163f3ff1", "generation_result": [{"code": "def plot_iris_knn()\n\tfrom sklearn.datasets import load_iris\n    from sklearn.neighbors import KNeighborsClassifier\n    from matplotlib import pyplot as plt\n    import numpy as np\n    import matplotlib as mpl\n    # Load the iris dataset\n    iris = load_iris()\n    # Extract the first two features and the target labels\n    X = iris.data[:, :2]\n    y = iris.target\n    # Create a kNN classifier with 5 neighbors\n    knn = KNeighborsClassifier(n_neighbors=5)\n    # Fit the classifier to the data\n    knn.fit(X, y)\n    # Define a mesh over the feature space\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n                         np.arange(y_min, y_max, 0.02))\n    # Predict the class for each point in the mesh\n    Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n    # Reshape the predictions to a 2D grid\n    Z = Z.reshape(xx.shape)\n    # Plot the decision boundary\n    plt.figure(figsize=(10, 8))\n    plt.pcolormesh(xx, yy, Z, cmap=mpl.cm.Paired)\n    # Plot the training points\n    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', cmap=mpl.cm.Paired)\n    plt.xlabel('Sepal length')\n    plt.ylabel('Sepal width')\n    plt.xlim(xx.min(), xx.max())\n    plt.ylim(yy.min(), yy.max())\n    plt.", "CodeBleu_score": 63.83125493922154}, {"code": "def plot_iris_knn()\n\timport numpy as np\n    import matplotlib.pyplot as plt\n    from sklearn import datasets\n    from sklearn.neighbors import KNeighborsClassifier\n    from sklearn.model_selection import train_test_split\n    from sklearn.metrics import accuracy_score\n    from sklearn.metrics import classification_report\n\n    # Load the Iris dataset\n    iris = datasets.load_iris()\n    X = iris.data\n    y = iris.target\n\n    # Extract the first two features\n    X = X[:, :2]\n\n    # Split the data into training and test sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n\n    # Create the kNN classifier\n    knn = KNeighborsClassifier(n_neighbors=5)\n\n    # Train the classifier\n    knn.fit(X_train, y_train)\n\n    # Predict the class labels of the test data\n    y_pred = knn.predict(X_test)\n\n    # Compute the accuracy of the classifier\n    acc = accuracy_score(y_test, y_pred)\n    print(\"Accuracy:\", acc)\n\n    # Create the mesh grid\n    h = 0.2\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n\n    # Predict the class for each point in the grid\n    Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n\n    # Reshape the prediction grid\n    Z = Z.reshape(xx.shape)\n\n    # Plot the decision boundary\n    plt.", "CodeBleu_score": 49.41950230171147}, {"code": "def plot_iris_knn()\n\t# Load the Iris dataset\n    iris = datasets.load_iris()\n\n    # Create a mesh grid\n    x_min, x_max = iris.data[:, 0].min() - 1, iris.data[:, 0].max() + 1\n    y_min, y_max = iris.data[:, 1].min() - 1, iris.data[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n                         np.arange(y_min, y_max, 0.1))\n\n    # Create a kNN classifier\n    knn = KNeighborsClassifier(n_neighbors=5)\n\n    # Fit the classifier to the data\n    knn.fit(iris.data[:, :2], iris.target)\n\n    # Predict the labels of the data points in the mesh grid\n    z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n\n    # Reshape the predictions to a 2D grid\n    z = z.reshape(xx.shape)\n\n    # Plot the mesh grid and the predicted labels\n    plt.figure()\n    plt.pcolormesh(xx, yy, z, cmap=plt.cm.Paired)\n\n    # Plot the training points\n    plt.scatter(iris.data[:, 0], iris.data[:, 1], c=iris.target,\n                cmap=plt.cm.Paired)\n    plt.xlabel('Sepal length')\n    plt.ylabel('Sepal width')\n\n    plt.xlim(xx.min(), xx.max())\n    plt.ylim(yy.min(), yy.max())\n\n    plt.show()", "CodeBleu_score": 44.3589080340403}]}
{"instance_id": "8eaee3207c09", "generation_result": [{"code": "def plot_sgd_separator()\n\tfrom sklearn.linear_model import SGDClassifier\n    from sklearn.datasets import make_blobs\n    from sklearn.metrics import plot_decision_boundary\n    from matplotlib import pyplot as plt\n    import numpy as np\n\n    # Create 50 separable points\n    X, y = make_blobs(n_samples=50, centers=2, random_state=10,\n                      cluster_std=1.5)\n\n    # Fit the classifier\n    clf = SGDClassifier(loss=\"hinge\", alpha=0.01, max_iter=200,\n                        fit_intercept=True)\n    clf.fit(X, y)\n\n    # Prepare the meshgrid for plotting\n    h = .01\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n\n    # Plot the decision boundary in 2D\n    plt.figure()\n    Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    plt.contour(xx, yy, Z, levels=[-1, 0, 1], linestyles=['--', '-', '--'],\n                linewidths=[2, 2, 2])\n\n    # Plot the training points\n    plt.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap=plt.cm.Paired,\n                edgecolors='black')\n    plt.xlabel('x')\n    plt.ylabel('y')\n    plt.xlim(xx.min(), xx.max())\n    plt.ylim(yy.min", "CodeBleu_score": 41.29189522865222}, {"code": "def plot_sgd_separator()\n\tfrom sklearn.datasets import make_blobs\n    from sklearn.linear_model import SGDClassifier\n    import matplotlib.pyplot as plt\n    import numpy as np\n\n    # Create the data\n    X, y = make_blobs(n_samples=50, centers=2, random_state=42)\n\n    # Create and fit the model\n    model = SGDClassifier(loss='hinge', alpha=0.01, max_iter=200, fit_intercept=True)\n    model.fit(X, y)\n\n    # Plot the decision boundary\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1), np.arange(y_min, y_max, 0.1))\n    Z = model.decision_function(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    plt.contour(xx, yy, Z, levels=[-1, 0, 1], linestyles=['--', '-', '--'], colors=['red', 'green', 'red'])\n\n    # Plot the training points\n    plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)\n    plt.xlim(xx.min(), xx.max())\n    plt.ylim(yy.min(), yy.max())\n    plt.show()\n\n", "CodeBleu_score": 37.684962438754276}, {"code": "def plot_sgd_separator()\n\tfrom sklearn.datasets import make_blobs\n    from sklearn.linear_model import SGDClassifier\n    from sklearn.metrics import plot_confusion_matrix\n    import matplotlib.pyplot as plt\n    import numpy as np\n\n    X, y = make_blobs(n_samples=50, centers=2, random_state=42)\n    clf = SGDClassifier(loss=\"hinge\", alpha=0.01, max_iter=200, fit_intercept=True)\n    clf.fit(X, y)\n\n    # Plot the decision boundary. For that, we will assign a color to each\n    # point in the mesh [x_min, x_max]x[y_min, y_max].\n    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n    h = 0.01\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n    Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n\n    # Put the result into a color plot\n    Z = Z.reshape(xx.shape)\n    plt.figure()\n    plt.contour(xx, yy, Z, levels=[-1, 0, 1], linestyles=[\"--\", \"-\", \"--\"], colors=[\"red\", \"blue\", \"red\"])\n    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired)\n    plt.xlabel(\"x\")\n    plt.ylabel(\"y\")\n    plt.axis(\"tight\")\n    plt.show()\n\n", "CodeBleu_score": 40.21524008028997}]}
{"instance_id": "8b54f999b4c0", "generation_result": [{"code": "def dump_svmlight_file(self, file)\n\tdata = np.array(self.data)\n    X = data[:, 0:2]\n    y = data[:, 2]\n    dump_svmlight_file(X, y, file)\n\n", "CodeBleu_score": 53.37186017015665}, {"code": "def dump_svmlight_file(self, file)\n\tdata = np.array(self.data)\n    X = data[:, 0:2]\n    y = data[:, 2]\n    dump_svmlight_file(X, y, file)\n\n", "CodeBleu_score": 53.37186017015665}, {"code": "def dump_svmlight_file(self, file)\n\tdata = np.array(self.data)\n    X = data[:, 0:2]\n    y = data[:, 2]\n    dump_svmlight_file(X, y, file)\n\n", "CodeBleu_score": 53.37186017015665}]}
{"instance_id": "2419013adc6e", "generation_result": [{"code": "def fit(self)\n\tprint(\"fit the model\")\n    train = np.array(self.model.data)\n    X = train[:, 0:2]\n    y = train[:, 2]\n\n    C = float(self.complexity.get())\n    gamma = float(self.gamma.get())\n    coef0 = float(self.coef0.get())\n    degree = int(self.degree.get())\n    kernel_map = {0: \"linear\", 1: \"rbf\", 2: \"poly\"}\n    if len(np.unique(y)) == 1:\n        clf = svm.OneClassSVM(kernel=kernel_map[self.kernel.get()],\n                              gamma=gamma, coef0=coef0, degree=degree)\n        clf.fit(X)\n    else:\n        clf = svm.SVC(kernel=kernel_map[self.kernel.get()], C=C,\n                      gamma=gamma, coef0=coef0, degree=degree)\n        clf.fit(X, y)\n    if hasattr(clf, 'score'):\n        print(\"Accuracy:\", clf.score(X, y) * 100)\n    X1, X2, Z = self.decision_surface(clf)\n    self.model.clf = clf\n    self.model.set_surface((X1, X2, Z))\n    self.model.surface_type = self.surface_type.get()\n    self.fitted = True\n    self.model.changed(\"surface\")\n\n", "CodeBleu_score": 98.7568182783107}, {"code": "def fit(self)\n\tprint(\"fit the model\")\n    train = np.array(self.model.data)\n    X = train[:, 0:2]\n    y = train[:, 2]\n\n    C = float(self.complexity.get())\n    gamma = float(self.gamma.get())\n    coef0 = float(self.coef0.get())\n    degree = int(self.degree.get())\n    kernel_map = {0: \"linear\", 1: \"rbf\", 2: \"poly\"}\n    if len(np.unique(y)) == 1:\n        clf = svm.OneClassSVM(kernel=kernel_map[self.kernel.get()],\n                              gamma=gamma, coef0=coef0, degree=degree)\n        clf.fit(X)\n    else:\n        clf = svm.SVC(kernel=kernel_map[self.kernel.get()], C=C,\n                      gamma=gamma, coef0=coef0, degree=degree)\n        clf.fit(X, y)\n    if hasattr(clf, 'score'):\n        print(\"Accuracy:\", clf.score(X, y) * 100)\n    X1, X2, Z = self.decision_surface(clf)\n    self.model.clf = clf\n    self.model.set_surface((X1, X2, Z))\n    self.model.surface_type = self.surface_type.get()\n    self.fitted = True\n    self.model.changed(\"surface\")\n\n", "CodeBleu_score": 98.7568182783107}, {"code": "def fit(self)\n\tprint(\"fit the model\")\n    train = np.array(self.model.data)\n    X = train[:, 0:2]\n    y = train[:, 2]\n\n    C = float(self.complexity.get())\n    gamma = float(self.gamma.get())\n    coef0 = float(self.coef0.get())\n    degree = int(self.degree.get())\n    kernel_map = {0: \"linear\", 1: \"rbf\", 2: \"poly\"}\n    if len(np.unique(y)) == 1:\n        clf = svm.OneClassSVM(kernel=kernel_map[self.kernel.get()],\n                              gamma=gamma, coef0=coef0, degree=degree)\n        clf.fit(X)\n    else:\n        clf = svm.SVC(kernel=kernel_map[self.kernel.get()], C=C,\n                      gamma=gamma, coef0=coef0, degree=degree)\n        clf.fit(X, y)\n    if hasattr(clf, 'score'):\n        print(\"Accuracy:\", clf.score(X, y) * 100)\n    X1, X2, Z = self.decision_surface(clf)\n    self.model.clf = clf\n    self.model.set_surface((X1, X2, Z))\n    self.model.surface_type = self.surface_type.get()\n    self.fitted = True\n    self.model.changed(\"surface\")\n\n", "CodeBleu_score": 98.7568182783107}]}
{"instance_id": "e887fd2a8513", "generation_result": [{"code": "def source_decode(sourcecode, verbose=0)\n\t# Split the source code into module and object components\n    if sourcecode.startswith(\"tpot.\"):\n        module_name = sourcecode[5:].split(\".\", 1)[0]\n        object_name = sourcecode.split(\".\", 1)[1]\n    else:\n        module_name = sourcecode.split(\".\", 1)[0]\n        object_name = sourcecode.split(\".\", 1)[1]\n\n    # Import the object from the module\n    try:\n        imported_object = getattr(__import__(module_name, fromlist=[object_name]), object_name)\n    except ImportError as e:\n        if verbose > 2:\n            raise e\n        else:\n            print(\"Warning: Could not import {} from module {}\".format(object_name, module_name))\n            return sourcecode, object_name, None\n\n    return sourcecode, object_name, imported_object\n\n", "CodeBleu_score": 38.68632130585043}, {"code": "def source_decode(sourcecode, verbose=0)\n\tif sourcecode.startswith(\"tpot.\"):\n        module_name, object_name = sourcecode.split(\".\", 1)\n        try:\n            module = __import__(module_name, fromlist=[object_name])\n            object = getattr(module, object_name)\n        except ImportError as e:\n            if verbose > 2:\n                raise ImportError(\"Error importing %s: %s\" % (sourcecode, e))\n            else:\n                print(\"Warning: Could not import %s\" % sourcecode)\n            return sourcecode, sourcecode, None\n    else:\n        module_name, object_name = sourcecode.rsplit(\".\", 1)\n        try:\n            module = __import__(module_name, fromlist=[object_name])\n            object = getattr(module, object_name)\n        except ImportError as e:\n            if verbose > 2:\n                raise ImportError(\"Error importing %s: %s\" % (sourcecode, e))\n            else:\n                print(\"Warning: Could not import %s\" % sourcecode)\n            return sourcecode, sourcecode, None\n    return module_name, object_name, object\n\n", "CodeBleu_score": 39.18338394112797}, {"code": "def source_decode(sourcecode, verbose=0)\n\timport importlib\n    import re\n\n    # Split the source code into its module and object components\n    module_name, object_name = re.findall(r\"^([a-zA-Z0-9_\\.]+)\\.([a-zA-Z0-9_\\.]+)$\", sourcecode)[0]\n\n    # If the module name starts with \"tpot.\", remove the prefix\n    if module_name.startswith(\"tpot.\"):\n        module_name = module_name[5:]\n\n    # Attempt to import the object\n    try:\n        module = importlib.import_module(module_name)\n        obj = getattr(module, object_name)\n\n        # If the object is a class, instantiate it\n        if isinstance(obj, type):\n            obj = obj()\n\n        # Return the import string, object string, and the object\n        return \"%s.%s\" % (module_name, object_name), object_name, obj\n\n    except ImportError as e:\n        # If verbose is higher than 2, raise the error\n        if verbose > 2:\n            raise\n\n        # Otherwise, print a warning\n        print(\"WARNING: Could not import %s (%s)\" % (sourcecode, e))\n\n        # Return the import string, object string, and None\n        return \"%s.%s\" % (module_name, object_name), object_name, None\n\n", "CodeBleu_score": 39.049640709346804}]}
{"instance_id": "5d6f629c3363", "generation_result": [{"code": "def set_sample_weight(pipeline_steps, sample_weight=None)\n\tsample_weight_dict = {}\n    if sample_weight is not None:\n        for step in pipeline_steps:\n            step_name = step[0]\n            step_obj = step[1]\n            step_fit_method = step_obj.fit\n            step_fit_args = inspect.getfullargspec(step_fit_method).args\n            if \"sample_weight\" in step_fit_args:\n                sample_weight_dict[step_name + \"_sample_weight\"] = sample_weight\n        if len(sample_weight_dict) > 0:\n            return sample_weight_dict\n        else:\n            return None\n    else:\n        return None\n\n\n\n\n", "CodeBleu_score": 38.43343579811233}, {"code": "def set_sample_weight(pipeline_steps, sample_weight=None)\n\tsample_weight_dict = {}\n    if sample_weight is not None:\n        for step in pipeline_steps:\n            step_name = step[0]\n            step_obj = step[1]\n            step_fit_args = inspect.getfullargspec(step_obj.fit).args\n            if 'sample_weight' in step_fit_args:\n                sample_weight_dict[step_name + '__sample_weight'] = sample_weight\n        if sample_weight_dict:\n            return sample_weight_dict\n        else:\n            return None\n    else:\n        return None\n\n", "CodeBleu_score": 45.56493548923103}, {"code": "def set_sample_weight(pipeline_steps, sample_weight=None)\n\tsample_weight_dict = {}\n    if sample_weight is not None:\n        for step in pipeline_steps:\n            step_name = step[0]\n            step_obj = step[1]\n            fit_args = inspect.getfullargspec(step_obj.fit)\n            if \"sample_weight\" in fit_args.args:\n                sample_weight_dict[step_name + \"_sample_weight\"] = sample_weight\n        if sample_weight_dict:\n            return sample_weight_dict\n        else:\n            return None\n    else:\n        return None\n\n", "CodeBleu_score": 46.41633289374607}]}
{"instance_id": "cae17d519818", "generation_result": [{"code": "def _summary_of_best_pipeline(self, features, target):\n\twe do not mess up indices\n        pretty = dirty_string\n        for (start, end) in reversed(parameter_prefixes):\n            pretty = pretty[: start + 2] + pretty[end:]\n\n        return pretty", "CodeBleu_score": 5.146947504305843}, {"code": "def _summary_of_best_pipeline(self, features, target):\n\twe do not mess up indices\n        pretty = dirty_string\n        for (start, end) in reversed(parameter_prefixes):\n            pretty = pretty[: start + 2] + pretty[end:]\n\n        return pretty", "CodeBleu_score": 5.146947504305843}, {"code": "def _summary_of_best_pipeline(self, features, target):\n\twe do not mess up indices\n        pretty = dirty_string\n        for (start, end) in reversed(parameter_prefixes):\n            pretty = pretty[: start + 2] + pretty[end:]\n\n        return pretty", "CodeBleu_score": 5.146947504305843}]}
{"instance_id": "638683d78167", "generation_result": [{"code": "def score(self, testing_features, testing_target)\n\tif self._optimized is False:\n        raise ValueError(\n            \"Error: This pipeline has not been optimized yet. Please call the fit function before scoring.\"\n        )\n\n    testing_features, testing_target = self._check_dataset(\n        testing_features, testing_target\n    )\n\n    if callable(self.scoring):\n        scorer = self.scoring\n    elif self.scoring in SCORERS:\n        scorer = SCORERS[self.scoring]\n    else:\n        raise ValueError(\n            \"Error: The provided scoring function is not defined in the predefined SCORERS dictionary and is not callable.\"\n        )\n\n    return scorer(self._fitted_pipeline, testing_features, testing_target)\n\n", "CodeBleu_score": 45.83473366756646}, {"code": "def score(self, testing_features, testing_target)\n\tif not self._optimized:\n        raise RuntimeError(\n            \"This pipeline has not been optimized yet. Call the optimize method before scoring.\"\n        )\n\n    if self._scorer is None:\n        raise ValueError(\"No scorer has been provided for this pipeline.\")\n\n    testing_features, testing_target = self._check_dataset(\n        testing_features, testing_target\n    )\n\n    if isinstance(self._scorer, str):\n        try:\n            scorer = SCORERS[self._scorer]\n        except KeyError:\n            raise ValueError(\n                \"{} is not a valid scoring function. Valid scoring functions are {}\".format(\n                    self._scorer, list(SCORERS.keys())\n                )\n            )\n    else:\n        scorer = self._scorer\n\n    return scorer(self._pipeline, testing_features, testing_target)\n\n", "CodeBleu_score": 47.65690229859019}, {"code": "def score(self, testing_features, testing_target)\n\tif not self.fitted_pipeline:\n        raise NotFittedError(\"This %s instance is not fitted yet. Call 'fit' with \"\n                             \"appropriate arguments before using this estimator.\"\n                             % type(self).__name__)\n    if not self.optimized:\n        raise NotOptimizedError(\"This %s instance is not optimized yet. Call 'optimize' \"\n                                \"with appropriate arguments before using this estimator.\"\n                                % type(self).__name__)\n    testing_features, testing_target = self._check_dataset(testing_features,\n                                                           testing_target)\n    if callable(self.scoring):\n        scorer = self.scoring\n    else:\n        scorer = SCORERS[self.scoring]\n    return scorer(self.fitted_pipeline, testing_features, testing_target)", "CodeBleu_score": 38.49531779742815}]}
{"instance_id": "a04382560333", "generation_result": [{"code": " def _impute_values(self, features)\n\tif self.verbosity > 1:\n        print(\n            \"Imputing missing values in feature set with strategy: {}\".format(\n                self.imputer_strategy\n            )\n        )\n    if not self.imputer:\n        self.imputer = SimpleImputer(strategy=self.imputer_strategy)\n        self.imputer.fit(features)\n    features = self.imputer.transform(features)\n    return features\n\n", "CodeBleu_score": 49.98311830432469}, {"code": " def _impute_values(self, features)\n\tif self.verbosity > 1:\n        print(\n            \"Imputing missing values using median imputation strategy.", "CodeBleu_score": 10.881917381810457}, {"code": " def _impute_values(self, features)\n\tif self.verbosity > 1:\n        print(\"Imputing missing values...\")\n    if not self._imputer:\n        self._imputer = SimpleImputer(strategy=\"median\")\n        self._imputer.fit(features)\n    features = self._imputer.transform(features)\n    return features\n\n", "CodeBleu_score": 71.5353391933778}]}
{"instance_id": "0043d411c332", "generation_result": [{"code": "def _compile_to_sklearn(self, expr)\n\tfrom . import _pipeline_code\n    from . import _pipeline_context\n    from . import _pipeline_operators\n\n    # generate pipeline code string from expression\n    pipeline_code = _pipeline_code.generate(expr, _pipeline_operators)\n\n    # evaluate pipeline code string within context to obtain pipeline object\n    pipeline = eval(pipeline_code, _pipeline_context)\n\n    # set pipeline memory attribute\n    pipeline.memory = self.memory\n\n    # recursively set random_state parameter for each step in pipeline\n    if self.random_state is not None:\n        _set_random_state(pipeline, self.random_state)\n\n    # return configured pipeline\n    return pipeline\n\n", "CodeBleu_score": 42.905269054769846}, {"code": "def _compile_to_sklearn(self, expr)\n\t# Generate the pipeline code string\n    pipeline_code = self._generate_pipeline_code(expr)\n\n    # Evaluate the pipeline code string in the context of the current module\n    pipeline = eval(pipeline_code, globals(), locals())\n\n    # Set the memory attribute\n    pipeline.memory = self._memory\n\n    # Set the random state for each step in the pipeline\n    if self._random_state is not None:\n        self._set_random_state(pipeline)\n\n    # Return the configured pipeline\n    return pipeline\n\n\n", "CodeBleu_score": 15.861299261474441}, {"code": "def _compile_to_sklearn(self, expr)\n\timport sys\n    import os\n    import importlib\n    import inspect\n    import re\n    import ast\n    import astor\n    import types\n    from sklearn.pipeline import Pipeline\n    from sklearn.base import BaseEstimator, TransformerMixin\n    from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n    from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler, RobustScaler\n    from sklearn.preprocessing import Imputer, FunctionTransformer\n    from sklearn.decomposition import PCA, TruncatedSVD\n    from sklearn.feature_selection import RFE, RFECV, SelectKBest, SelectPercentile, VarianceThreshold\n    from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n    from sklearn.feature_extraction import DictVectorizer\n    from sklearn.feature_selection import chi2, f_classif, mutual_info_classif\n    from sklearn.linear_model import LogisticRegression, LinearRegression, SGDClassifier, SGDRegressor\n    from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, GradientBoostingClassifier, GradientBoostingRegressor, ExtraTreesClassifier, ExtraTreesRegressor\n    from sklearn.svm import LinearSVC, LinearSVR, SVC, SVR, NuSVC, NuSVR\n    from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB\n    from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n    from sklearn.neural_network import MLPClassifier, MLPRegressor\n    from sklearn.cluster import KMeans, MiniBatchKMeans\n    from sklearn.decomposition import PCA, TruncatedSVD\n    from sklearn.feature_selection import SelectKBest, SelectPercentile, VarianceThreshold\n    from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n    from sklearn.preprocessing import StandardScaler, MinMaxScaler", "CodeBleu_score": 22.582172306954735}]}
{"instance_id": "ca0ce462aa76", "generation_result": [{"code": "def pick_two_individuals_eligible_for_crossover(population)\n\t# Create sets of primitive nodes for each individual.\n    primitive_sets = [set() for individual in population]\n    for individual in population:\n        primitive_sets[individual.id] = individual.primitive_set()\n    # Convert each individual to a string representation.\n    string_representations = [str(individual) for individual in population]\n    # Identify pairs of individuals that share at least one primitive and are not identical.\n    eligible_pairs = []\n    for i in range(len(population)):\n        for j in range(i + 1, len(population)):\n            if primitive_sets[i].intersection(primitive_sets[j]) and string_representations[i] != string_representations[j]:\n                eligible_pairs.append((i, j))\n    # If no eligible pairs are found, return None for both individuals.\n    if not eligible_pairs:\n        return None, None\n    # Otherwise, randomly select one of the eligible pairs and return the corresponding individuals from the population.\n    else:\n        i, j = random.choice(eligible_pairs)\n        return population[i], population[j]\n\n\n", "CodeBleu_score": 27.176832902407167}, {"code": "def pick_two_individuals_eligible_for_crossover(population)\n\t# Create sets of primitive nodes for each individual and convert each individual to a string representation\n    primitive_nodes = {}\n    primitive_nodes_str = {}\n    for individual in population:\n        primitive_nodes[individual] = set()\n        primitive_nodes_str[individual] = \"\"\n        for node in individual:\n            if node.type == \"primitive\":\n                primitive_nodes[individual].add(node)\n                primitive_nodes_str[individual] += str(node)\n\n    # Identify pairs of individuals that share at least one primitive and are not identical\n    eligible_pairs = []\n    for individual_1 in population:\n        for individual_2 in population:\n            if (individual_1 != individual_2) and (primitive_nodes_str[individual_1] in primitive_nodes_str[individual_2]):\n                eligible_pairs.append((individual_1, individual_2))\n\n    # If no eligible pairs are found, return None for both individuals\n    if len(eligible_pairs) == 0:\n        return None, None\n\n    # Otherwise, randomly select one of the eligible pairs and return the corresponding individuals from the population\n    else:\n        return eligible_pairs[np.random.randint(len(eligible_pairs))]\n\n\n", "CodeBleu_score": 24.166808994130403}, {"code": "def pick_two_individuals_eligible_for_crossover(population)\n\t# Create sets of primitive nodes for each individual and convert each individual to a string representation\n    primitive_sets = []\n    str_representations = []\n    for individual in population:\n        primitive_sets.append(set(individual.primitive_nodes))\n        str_representations.append(str(individual))\n\n    # Identify pairs of individuals that share at least one primitive and are not identical\n    pairs = []\n    for i in range(len(population)):\n        for j in range(i + 1, len(population)):\n            if primitive_sets[i].intersection(primitive_sets[j]) and str_representations[i] != str_representations[j]:\n                pairs.append((i, j))\n\n    # If no eligible pairs are found, return None for both individuals\n    if len(pairs) == 0:\n        return None, None\n\n    # Otherwise, randomly select one of the eligible pairs and return the corresponding individuals from the population\n    pair = random.choice(pairs)\n    return population[pair[0]], population[pair[1]]\n\n", "CodeBleu_score": 24.292337162214096}]}
{"instance_id": "4b03c843f506", "generation_result": [{"code": "def generate_export_pipeline_code(pipeline_tree, operators)\n\tsteps = _process_operator(pipeline_tree, operators)\n    n_steps = len(steps)\n\n    if n_steps > 1:\n        pipeline_code = \"make_pipeline(", "CodeBleu_score": 12.69022647722485}, {"code": "def generate_export_pipeline_code(pipeline_tree, operators)\n\tsteps = _process_operator(pipeline_tree, operators)\n    if len(steps) == 1:\n        code = steps[0]\n    else:\n        code = \"make_pipeline(", "CodeBleu_score": 13.203636880839737}, {"code": "def generate_export_pipeline_code(pipeline_tree, operators)\n\tsteps = _process_operator(pipeline_tree, operators)\n    num_steps = len(steps)\n\n    if num_steps > 1:\n        steps_str = \",\n    \".join(steps)\n        pipeline_code = \"make_pipeline(\n    {}", "CodeBleu_score": 20.0679187951816}]}
{"instance_id": "e9aec2ee39a4", "generation_result": [{"code": "def _process_operator(operator, operators, depth=0)\n\tsteps = []\n    op_name = operator[0]\n\n    if op_name == \"CombineDFs\":\n        steps.append(_combine_dfs(operator[1], operator[2], operators))\n    else:\n        input_name, args = operator[1], operator[2:]\n        tpot_op = get_by_name(op_name, operators)\n\n        if input_name != \"input_matrix\":\n            steps.extend(_process_operator(input_name, operators, depth + 1))\n\n        # If the step is an estimator and is not the last step then we must\n        # add its guess as synthetic feature(s)\n        # classification prediction for both regression and classification\n        # classification probabilities for classification if available\n        if tpot_op.root and depth > 0:\n            steps.append(\n                \"StackingEstimator(estimator={})\".format(tpot_op.export(*args))\n            )\n        else:\n            steps.append(tpot_op.export(*args))\n    return steps\n\n", "CodeBleu_score": 85.07455460075597}, {"code": "def _process_operator(operator, operators, depth=0)\n\tsteps = []\n    op_name = operator[0]\n\n    if op_name == \"CombineDFs\":\n        steps.append(_combine_dfs(operator[1], operator[2], operators))\n    else:\n        input_name, args = operator[1], operator[2:]\n        tpot_op = get_by_name(op_name, operators)\n\n        if input_name != \"input_matrix\":\n            steps.extend(_process_operator(input_name, operators, depth + 1))\n\n        # If the step is an estimator and is not the last step then we must\n        # add its guess as synthetic feature(s)\n        # classification prediction for both regression and classification\n        # classification probabilities for classification if available\n        if tpot_op.root and depth > 0:\n            steps.append(\n                \"StackingEstimator(estimator={})\".format(tpot_op.export(*args))\n            )\n        else:\n            steps.append(tpot_op.export(*args))\n    return steps\n\n", "CodeBleu_score": 85.07455460075597}, {"code": "def _process_operator(operator, operators, depth=0)\n\tsteps = []\n    op_name = operator[0]\n\n    if op_name == \"CombineDFs\":\n        steps.append(_combine_dfs(operator[1], operator[2], operators))\n    else:\n        input_name, args = operator[1], operator[2:]\n        tpot_op = get_by_name(op_name, operators)\n\n        if input_name != \"input_matrix\":\n            steps.extend(_process_operator(input_name, operators, depth + 1))\n\n        # If the step is an estimator and is not the last step then we must\n        # add its guess as synthetic feature(s)\n        # classification prediction for both regression and classification\n        # classification probabilities for classification if available\n        if tpot_op.root and depth > 0:\n            steps.append(\n                \"StackingEstimator(estimator={})\".format(tpot_op.export(*args))\n            )\n        else:\n            steps.append(tpot_op.export(*args))\n    return steps\n\n", "CodeBleu_score": 85.07455460075597}]}
{"instance_id": "0513bf969510", "generation_result": [{"code": "def transform(self, X)\n\tif self.categorical_features == 'auto':\n        categorical_features = []\n        for feature in range(X.shape[1]):\n            if X[:, feature].dtype == object:\n                categorical_features.append(feature)\n            elif X[:, feature].dtype == bool:\n                categorical_features.append(feature)\n            elif X[:, feature].dtype == 'category':\n                categorical_features.append(feature)\n        if len(categorical_features) == 0:\n            raise ValueError(\"No categorical features found.\")\n        else:\n            self.categorical_features = categorical_features\n\n    if self.categorical_features == 'all':\n        categorical_features = list(range(X.shape[1]))\n    else:\n        categorical_features = self.categorical_features\n\n    # get categorical feature names\n    if self.categorical_features_names is None:\n        self.categorical_features_names = categorical_features\n\n    # get categorical feature values\n    if self.categorical_features_values is None:\n        self.categorical_features_values = {}\n        for feature in categorical_features:\n            self.categorical_features_values[feature] = list(set(X[:, feature]))\n\n    # get categorical feature encoding\n    if self.categorical_features_encoding is None:\n        self.categorical_features_encoding = {}\n        for feature in categorical_features:\n            self.categorical_features_encoding[feature] = {}\n            for value in self.categorical_features_values[feature]:\n                self.categorical_features_encoding[feature][value] = 0\n\n    # transform categorical features\n    X_transformed = X.copy()\n    for feature in categorical_features:\n        for value in self.categorical_features_values[feature]:\n            X_transformed[X[:, feature] == value, feature] = self.categorical_features_encoding[feature][value]\n\n    return X_transformed", "CodeBleu_score": 23.577267073000858}, {"code": "def transform(self, X)\n\tX_new = X.copy()\n    categorical_features = self.categorical_features(X)\n    if len(categorical_features) == 0:\n        raise ValueError(\"No categorical features found in the input data.\")\n    X_new = pd.get_dummies(X_new, columns=categorical_features)\n    return X_new\n\n\n", "CodeBleu_score": 24.115988115764317}, {"code": "def transform(self, X)\n\t# Check if X is a dataframe\n    if isinstance(X, pd.DataFrame):\n        # Get the categorical features\n        categorical_features = self.get_categorical_features(X)\n        # Check if there are categorical features\n        if len(categorical_features) == 0:\n            raise ValueError(\"No categorical features found in the data\")\n        # Get the threshold\n        threshold = self.threshold\n        # Get the number of samples\n        n_samples = X.shape[0]\n        # Get the number of features\n        n_features = X.shape[1]\n        # Create a boolean mask to select the categorical features\n        categorical_mask = categorical_features.isin(categorical_features)\n        # Create a boolean mask to select the numerical features\n        numerical_mask = ~categorical_mask\n        # Get the categorical data\n        categorical_data = X.loc[:, categorical_mask]\n        # Get the numerical data\n        numerical_data = X.loc[:, numerical_mask]\n        # Get the number of categorical features\n        n_categorical_features = categorical_data.shape[1]\n        # Get the number of numerical features\n        n_numerical_features = numerical_data.shape[1]\n        # Create a list to store the transformed data\n        transformed_data = []\n        # Iterate over the categorical features\n        for feature in categorical_features:\n            # Get the number of unique values\n            n_unique_values = X[feature].nunique()\n            # Check if the number of unique values is greater than the threshold\n            if n_unique_values > threshold:\n                # Add the feature to the list of transformed data\n                transformed_data.append(feature)\n        # Check if there are categorical features\n        if len(transformed_data) == 0:\n            raise ValueError(\"No categorical features found in the data\")\n        # Create a dataframe to store the transformed data\n        transformed_data = pd.DataFrame(index=range(n_samples), columns=transformed_data)\n        # Iterate over the categorical features\n        for feature in transformed_data:\n            # Create a one", "CodeBleu_score": 28.248876101179025}]}
{"instance_id": "5b1bcf42ca9e", "generation_result": [{"code": "def balanced_accuracy(y_true, y_pred)\n\t# Find unique classes\n    unique_classes = np.unique(np.concatenate([y_true, y_pred]))\n\n    # Initialize balanced accuracy\n    balanced_accuracy = 0\n\n    # Iterate over all classes\n    for unique_class in unique_classes:\n        # Identify true and predicted labels for the current class\n        y_true_class = y_true == unique_class\n        y_pred_class = y_pred == unique_class\n\n        # Calculate sensitivity and specificity\n        sensitivity = np.sum(y_true_class & y_pred_class) / np.sum(y_true_class)\n        specificity = np.sum(~y_true_class & ~y_pred_class) / np.sum(~y_true_class)\n\n        # Set specificity to 1 if there are no negative samples\n        if np.sum(~y_true_class) == 0:\n            specificity = 1\n\n        # Calculate class accuracy\n        class_accuracy = (sensitivity + specificity) / 2\n\n        # Update balanced accuracy\n        balanced_accuracy += class_accuracy\n\n    # Return mean balanced accuracy\n    return balanced_accuracy / len(unique_classes)\n\n", "CodeBleu_score": 18.631029387910868}, {"code": "def balanced_accuracy(y_true, y_pred)\n\t# Identify all unique classes present in the true and predicted labels\n    classes = np.union1d(y_true, y_pred)\n    \n    # Initialize the balanced accuracy\n    balanced_accuracy = 0\n    \n    # Loop through all unique classes\n    for c in classes:\n        \n        # Identify all indices in y_true and y_pred where the class c appears\n        idx_c_true = np.where(y_true == c)[0]\n        idx_c_pred = np.where(y_pred == c)[0]\n        \n        # Calculate sensitivity and specificity for class c\n        sensitivity = len(idx_c_true) / len(y_true)\n        specificity = len(idx_c_pred) / len(y_pred)\n        \n        # If the true labels consist of only one class, specificity is set to 1\n        if len(y_true) == len(idx_c_true):\n            specificity = 1\n        \n        # Calculate the class accuracy\n        class_accuracy = (sensitivity + specificity) / 2\n        \n        # Add the class accuracy to the balanced accuracy\n        balanced_accuracy += class_accuracy\n        \n    # Return the mean of the class accuracies\n    return balanced_accuracy / len(classes)\n\n", "CodeBleu_score": 19.229506703489914}, {"code": "def balanced_accuracy(y_true, y_pred)\n\t# Get the unique classes\n    classes = np.unique(np.concatenate((y_true, y_pred)))\n    # Initialize the balanced accuracy\n    balanced_accuracy = 0\n    # Loop over the classes\n    for c in classes:\n        # Get the true and predicted labels for this class\n        c_true = y_true[y_true == c]\n        c_pred = y_pred[y_pred == c]\n        # Calculate the sensitivity and specificity\n        sensitivity = np.sum(c_true == c_pred) / len(c_true)\n        specificity = np.sum(c_true != c_pred) / len(c_pred)\n        # If the true labels have only one class, set specificity to 1\n        if len(c_true) == len(y_true):\n            specificity = 1\n        # Calculate the class accuracy\n        class_accuracy = (sensitivity + specificity) / 2\n        # Add the class accuracy to the balanced accuracy\n        balanced_accuracy += class_accuracy\n    # Return the mean balanced accuracy\n    return balanced_accuracy / len(classes)\n\n", "CodeBleu_score": 18.478058872283263}]}
{"instance_id": "19a9ad39de0a", "generation_result": [{"code": "def transform(self, X, y=None)\n\t# Check if the input is of the correct format\n    X = check_array(X)\n\n    # Initialize the number of non-zero elements and zero elements per row\n    non_zero_elements_per_row = np.zeros(X.shape[0])\n    zero_elements_per_row = np.zeros(X.shape[0])\n\n    # Iterate through each row of the input array\n    for row in range(X.shape[0]):\n        # Calculate the number of non-zero elements per row\n        non_zero_elements_per_row[row] = np.count_nonzero(X[row])\n        # Calculate the number of zero elements per row\n        zero_elements_per_row[row] = X.shape[1] - non_zero_elements_per_row[row]\n\n    # Add the number of non-zero and zero elements per row to the beginning of the transformed array\n    transformed_X = np.insert(X, 0, zero_elements_per_row, axis=1)\n    transformed_X = np.insert(transformed_X, 0, non_zero_elements_per_row, axis=1)\n\n    return transformed_X", "CodeBleu_score": 18.448340514799362}, {"code": "def transform(self, X, y=None)\n\tX = check_array(X)\n    nonzero_per_row = np.count_nonzero(X, axis=1)\n    zero_per_row = X.shape[1] - nonzero_per_row\n    return np.c_[zero_per_row, nonzero_per_row, X]\n\n", "CodeBleu_score": 8.681964184059488}, {"code": "def transform(self, X, y=None)\n\tX = check_array(X)\n    non_zero_counts = np.count_nonzero(X, axis=1)\n    zero_counts = X.shape[1] - non_zero_counts\n    return np.insert(X, 0, values=[zero_counts, non_zero_counts], axis=1)\n\n", "CodeBleu_score": 12.078648620966106}]}
{"instance_id": "5e8db0b5d447", "generation_result": [{"code": "def _init_pretest(self, features, target)\n\t# Calculate the number of unique target classes\n    self.classes_ = np.unique(target)\n    self.n_classes_ = len(self.classes_)\n\n    # Ensure the training size is at least the number of unique target classes\n    if self.n_classes_ > self.train_size:\n        self.train_size = self.n_classes_\n\n    # Split the features and target into pretest_X and pretest_y with a specified random state\n    self.pretest_X, self.pretest_y, _, _ = train_test_split(features, target, train_size=self.train_size, random_state=self.random_state)\n\n    # If all target classes are not included in pretest_y, adjust pretest_y to include at least one example from each class by indexing unique target values\n    if len(np.unique(self.pretest_y)) < self.n_classes_:\n        for i in range(self.n_classes_):\n            if self.classes_[i] not in self.pretest_y:\n                idx = np.where(target == self.classes_[i])[0][0]\n                self.pretest_X = np.vstack((self.pretest_X, features[idx].reshape(1, -1)))\n                self.pretest_y = np.append(self.pretest_y, target[idx])\n\n\n", "CodeBleu_score": 31.557529717210485}, {"code": "def _init_pretest(self, features, target)\n\t# Calculate number of unique target classes\n    unique_target_values = np.unique(target)\n    num_unique_target_values = len(unique_target_values)\n\n    # Ensure training size is at least number of unique target classes\n    if features.shape[0] < num_unique_target_values:\n        print(\"Warning: Training size is less than number of unique target classes. Training size has been increased to match.\")\n        num_unique_target_values = features.shape[0]\n\n    # Split features and target into pretest_X and pretest_y with a specified random state\n    pretest_X, pretest_y = train_test_split(features, target, test_size=num_unique_target_values, random_state=self.random_state)\n\n    # If all target classes are not included in pretest_y, adjust pretest_y to include at least one example from each class by indexing unique target values\n    if len(np.unique(pretest_y)) != len(unique_target_values):\n        for target_value in unique_target_values:\n            if target_value not in pretest_y:\n                pretest_y[np.where(pretest_y == unique_target_values[0])[0][0]] = target_value\n\n    return pretest_X, pretest_y\n\n\n", "CodeBleu_score": 30.566819289565593}, {"code": "def _init_pretest(self, features, target)\n\tself.pretest_y = target\n    self.pretest_X = features\n    self.pretest_y_unique = np.unique(self.pretest_y)\n    self.pretest_size = len(self.pretest_y)\n    self.pretest_n_classes = len(self.pretest_y_unique)\n    self.pretest_n_samples = self.pretest_size\n    if self.pretest_n_samples < self.pretest_n_classes:\n        self.pretest_n_samples = self.pretest_n_classes\n    self.pretest_X, self.pretest_y = train_test_split(\n        self.pretest_X,\n        self.pretest_y,\n        train_size=self.pretest_n_samples,\n        random_state=self.random_state,\n    )\n    self.pretest_y_unique = np.unique(self.pretest_y)\n    if len(self.pretest_y_unique) < self.pretest_n_classes:\n        self.pretest_y_unique = np.unique(self.pretest_y)\n        self.pretest_y = []\n        for i in self.pretest_y_unique:\n            self.pretest_y.append(i)\n        self.pretest_y = np.array(self.pretest_y)\n    self.pretest_y_unique = np.unique(self.pretest_y)\n    self.pretest_n_classes = len(self.pretest_y_unique)\n    self.pretest_n_samples = len(self.pretest_y)\n\n", "CodeBleu_score": 22.929843596547496}]}
{"instance_id": "5e3d48667396", "generation_result": [{"code": "def _to_numpy_type(dtype)\n\tif isinstance(dtype, tf.DType):\n        return dtype.as_numpy_dtype\n    return dtype\n", "CodeBleu_score": 48.68057248011396}, {"code": "def _to_numpy_type(dtype)\n\tif isinstance(dtype, str):\n        dtype = tf.as_dtype(dtype)\n    if isinstance(dtype, tf.DType):\n        dtype = tf.as_dtype(dtype).as_numpy_dtype\n    return dtype\n", "CodeBleu_score": 54.20156094937215}, {"code": "def _to_numpy_type(dtype)\n\tif dtype is None:\n        return None\n    if isinstance(dtype, tf.DType):\n        return dtype.as_numpy_dtype\n    return dtype\n\n", "CodeBleu_score": 74.03496494949367}]}
{"instance_id": "8f1120fdd463", "generation_result": [{"code": "def _dataset_as_numpy(ds, batch_size=None)\n\tif batch_size is None:\n        return [_to_numpy(x) for x in ds]\n    try:\n        return [_to_numpy(x) for x in ds.batch(batch_size)]\n    except AttributeError:\n        return [_to_numpy(x) for x in ds.apply(tf.data.experimental.dense_to_sparse_batch(batch_size))]\n\n", "CodeBleu_score": 23.603979897801743}, {"code": "def _dataset_as_numpy(ds, batch_size=None)\n\ttry:\n        if batch_size is not None:\n            ds = ds.batch(batch_size)\n        return [_to_numpy(x) for x in iter(ds)]\n    except AttributeError:\n        return [_to_numpy(x) for x in ds.make_one_shot_iterator().get_next()]\n\n", "CodeBleu_score": 17.145487583286283}, {"code": "def _dataset_as_numpy(ds, batch_size=None)\n\ttry:\n        if batch_size is None:\n            batch_size = ds.cardinality().numpy()\n        ds = ds.batch(batch_size)\n        return [_to_numpy(x) for x in ds]\n    except:\n        return [_to_numpy(x) for x in ds]\n\n", "CodeBleu_score": 19.277609804166534}]}
{"instance_id": "282271955871", "generation_result": [{"code": "def get_prng(seed)\n\tif seed is None:\n        seed = 0\n    if isinstance(seed, int):\n        seed = np.uint32(seed)\n    elif isinstance(seed, np.ndarray):\n        seed = np.uint32(seed)\n    elif isinstance(seed, jnp.ndarray):\n        seed = jnp.uint32(seed)\n    elif isinstance(seed, float):\n        seed = np.uint32(int(seed))\n    else:\n        raise ValueError(\"Seed must be a scalar.\")\n    key = jax.random.PRNGKey(seed)\n    return key\n\n", "CodeBleu_score": 27.592062452193243}, {"code": "def get_prng(seed)\n\timport jax\n    import numpy as np\n    seed = np.array(seed, dtype=np.uint32)\n    if seed.ndim == 0:\n        seed = np.array([seed, seed])\n    elif seed.ndim == 1:\n        if seed.shape[0] == 1:\n            seed = np.array([seed[0], seed[0]])\n        elif seed.shape[0] == 2:\n            seed = seed.astype(np.uint32)\n        else:\n            raise ValueError(\"Seed must be a scalar or a vector of length 1 or 2.\")\n    else:\n        raise ValueError(\"Seed must be a scalar or a vector of length 1 or 2.\")\n    return jax.random.PRNGKey(seed[0], seed[1])\n\n", "CodeBleu_score": 29.05072412429221}, {"code": "def get_prng(seed)\n\tfrom jax import random\n    if isinstance(seed, int):\n        seed = random.PRNGKey(seed)\n    elif isinstance(seed, (tuple, list)):\n        seed = random.PRNGKey(seed)\n    elif isinstance(seed, np.ndarray):\n        seed = random.PRNGKey(seed)\n    elif isinstance(seed, random.PRNGKey):\n        pass\n    else:\n        raise TypeError(\"Seed must be an integer, tuple, list, numpy array, or PRNGKey.\")\n    return seed\n\n", "CodeBleu_score": 19.08746679453082}]}
{"instance_id": "42234dccce9a", "generation_result": [{"code": "def convert_Timestamp(cls, s, dtype, pandas_type)\n\tif isinstance(dtype, pd.DatetimeTZDtype):\n        return s.dt.tz_convert(dtype.tz)\n    elif isinstance(dtype, pd.DatetimeTZDtype):\n        return s.dt.tz_localize(dtype.tz)\n    elif isinstance(dtype, pd.DatetimeTZDtype):\n        return s.dt.tz_localize(dtype.tz)\n    elif isinstance(dtype, pd.DatetimeTZDtype):\n        return s.dt.tz_localize(dtype.tz)\n    else:\n        return s.dt.tz_localize(dtype.tz)\n\n", "CodeBleu_score": 49.118479474346714}, {"code": "def convert_Timestamp(cls, s, dtype, pandas_type)\n\tif dtype is pd.DatetimeTZDtype:\n        return s.dt.tz_convert(dtype.tz)\n    elif dtype is np.dtype('datetime64[ns]'):\n        return s.dt.tz_localize(dtype.tz)\n    elif np.issubdtype(dtype, np.number):\n        return pd.to_datetime(s, unit='s').dt.tz_localize(dtype.tz)\n    else:\n        return pd.to_datetime(s).dt.tz_localize(dtype.tz)\n\n", "CodeBleu_score": 56.684430476439886}, {"code": "def convert_Timestamp(cls, s, dtype, pandas_type)\n\tif dtype == pd.DatetimeTZDtype:\n        return s.dt.tz_convert(s.dtype.tz)\n    elif dtype == np.dtype(\"datetime64[ns]\"):\n        return s.dt.tz_localize(s.dtype.tz)\n    elif dtype == np.dtype(\"float64\"):\n        return pd.to_datetime(s, unit=\"s\").dt.tz_localize(s.dtype.tz)\n    else:\n        return pd.to_datetime(s, unit=\"s\").dt.tz_localize(\"UTC\").dt.tz_convert(s.dtype.tz)\n\n", "CodeBleu_score": 51.15924890238326}]}
{"instance_id": "9cee7388879b", "generation_result": [{"code": "def visit(cls, op: ops.BetweenTime, arg, lower_bound, upper_bound)\n\tif op.column_name in arg.columns:\n        if not isinstance(arg[op.column_name].dtype, datetime):\n            raise ValueError(\n                \"BetweenTime operation can only be applied to datetime columns.\"\n            )\n        if not arg[op.column_name].dtype.tz:\n            arg[op.column_name] = arg[op.column_name].dt.tz_localize(\"utc\")\n        return arg[op.column_name].between(lower_bound, upper_bound)\n    else:\n        raise ValueError(f\"Column {op.column_name} not found in dataframe.\")", "CodeBleu_score": 22.008447040366196}, {"code": "def visit(cls, op: ops.BetweenTime, arg, lower_bound, upper_bound)\n\tif not isinstance(arg, dd.DataFrame):\n        raise ValueError(f\"Expected a Dask DataFrame, got {type(arg)}\")\n    if not isinstance(lower_bound, datetime):\n        raise ValueError(f\"Expected a datetime object, got {type(lower_bound)}\")\n    if not isinstance(upper_bound, datetime):\n        raise ValueError(f\"Expected a datetime object, got {type(upper_bound)}\")\n\n    if arg[op.column].dtype == \"datetime64[ns, UTC]\":\n        df = arg\n    else:\n        df = arg.map_partitions(convert_to_utc, meta=arg)\n\n    lower_bound = lower_bound.replace(tzinfo=timezone.utc)\n    upper_bound = upper_bound.replace(tzinfo=timezone.utc)\n\n    indexer = df[op.column].between(lower_bound, upper_bound, inclusive=\"both\")\n    return dd.from_array(indexer, chunks=indexer.chunksize)\n\n", "CodeBleu_score": 32.16135373911374}, {"code": "def visit(cls, op: ops.BetweenTime, arg, lower_bound, upper_bound)\n\tif not isinstance(arg, dd.DataFrame):\n        raise ValueError(f\"Expected a Dask DataFrame as argument, got {type(arg)}.\")\n\n    if not isinstance(lower_bound, datetime.time):\n        raise ValueError(f\"Expected a datetime.time as lower bound, got {type(lower_bound)}.\")\n\n    if not isinstance(upper_bound, datetime.time):\n        raise ValueError(f\"Expected a datetime.time as upper bound, got {type(upper_bound)}.\")\n\n    if not arg.columns.contains(op.column):\n        raise ValueError(f\"Column {op.column} not found in DataFrame.\")\n\n    if not arg.dtypes[op.column] == \"datetime64[ns]\":\n        raise ValueError(f\"Expected a datetime column, got {arg.dtypes[op.column]}.\")\n\n    if not arg.dtypes[op.column].tz:\n        arg = arg.assign(\n            **{op.column: lambda x: x[op.column].dt.tz_localize(\"UTC\")}\n        )\n\n    lower_bound = lower_bound.replace(tzinfo=pytz.utc)\n    upper_bound = upper_bound.replace(tzinfo=pytz.utc)\n\n    lower_bound = np.datetime64(lower_bound)\n    upper_bound = np.datetime64(upper_bound)\n\n    indexer = (\n        (arg[op.column].dt.time >= lower_bound)\n        & (arg[op.column].dt.time <= upper_bound)\n    ).to_dask_array()\n\n    return dd.from_array(indexer, columns=[op.column])\n\n", "CodeBleu_score": 34.25493498627327}]}
{"instance_id": "0ab1cc8ccf71", "generation_result": [{"code": "def asseries(cls, value, like=None)\n\tif isinstance(value, dd.Series):\n        return value\n    elif isinstance(value, dd.Scalar):\n        return dd.from_array(value.to_dask_array())\n    elif isinstance(value, pd.Series):\n        return dd.from_pandas(value, 1)\n    elif isinstance(value, tuple) or isinstance(value, list):\n        if like is None:\n            return dd.from_pandas(pd.Series(value), 1)\n        else:\n            return dd.from_pandas(pd.Series(value), like.npartitions)\n    elif isinstance(value, dict):\n        if like is None:\n            return dd.from_pandas(pd.Series(value), 1)\n        else:\n            return dd.from_pandas(pd.Series(value), like.npartitions)\n    else:\n        if like is None:\n            return dd.from_pandas(pd.Series([value]), 1)\n        else:\n            return dd.from_pandas(pd.Series([value]), like.npartitions)\n\n", "CodeBleu_score": 38.457249197423934}, {"code": "def asseries(cls, value, like=None)\n\timport dask.dataframe as dd\n    import pandas as pd\n    import numpy as np\n    if isinstance(value, dd.Series):\n        return value\n    elif isinstance(value, dd.core.Scalar):\n        return dd.from_dask_array(value.to_dask_array())\n    elif isinstance(value, pd.Series):\n        return dd.from_pandas(value, npartitions=1)\n    elif isinstance(value, (tuple, list)):\n        if like is None:\n            raise ValueError(\"If value is a tuple or list, 'like' must be provided\")\n        else:\n            return dd.from_pandas(pd.Series(value), like.npartitions)\n    elif isinstance(value, dict):\n        if like is None:\n            raise ValueError(\"If value is a dict, 'like' must be provided\")\n        else:\n            return dd.from_pandas(pd.Series(value), like.npartitions).rename(index=value.keys())\n    elif isinstance(value, (int, float, np.number)):\n        return dd.from_pandas(pd.Series([value]), 1)\n    else:\n        raise ValueError(\"Unable to convert value to a Dask Series\")\n", "CodeBleu_score": 37.39686474169398}, {"code": "def asseries(cls, value, like=None)\n\tif isinstance(value, da.Array):\n        return dd.from_dask_array(value)\n    elif isinstance(value, dd.Series):\n        return value\n    elif isinstance(value, (pd.Series, pd.DataFrame)):\n        return dd.from_pandas(value, npartitions=1)\n    elif isinstance(value, (list, tuple, dict)):\n        if like is not None:\n            return dd.from_pandas(pd.Series(value, index=like.index), npartitions=1)\n        else:\n            return dd.from_pandas(pd.Series(value), npartitions=1)\n    elif isinstance(value, da.core.Scalar):\n        return dd.from_dask_array(da.from_array(value))\n    else:\n        return dd.from_pandas(pd.Series([value]), npartitions=1)\n\n", "CodeBleu_score": 43.67267666626623}]}
{"instance_id": "926220a06a0c", "generation_result": [{"code": "def do_connect(\n        self,\n        dictionary: MutableMapping[str, dd.DataFrame] | None = None,\n    ) -> None\n\tif dictionary is None:\n        dictionary = {}\n\n    for k, v in dictionary.items():\n        if not isinstance(v, (dd.DataFrame, pd.DataFrame)):\n            raise TypeError(\n                f\"Expected an instance of 'dask.dataframe.DataFrame' for {k!r},\"\n                f\" got an instance of '{type(v).__name__}' instead.\"\n            )\n    super().do_connect(dictionary)", "CodeBleu_score": 96.84942538636408}, {"code": "def do_connect(\n        self,\n        dictionary: MutableMapping[str, dd.DataFrame] | None = None,\n    ) -> None\n\tif dictionary is None:\n        dictionary = {}\n\n    for k, v in dictionary.items():\n        if not isinstance(v, (dd.DataFrame, pd.DataFrame)):\n            raise TypeError(\n                f\"Expected an instance of 'dask.dataframe.DataFrame' for {k!r},\"\n                f\" got an instance of '{type(v).__name__}' instead.\"\n            )\n    super().do_connect(dictionary)\n\n", "CodeBleu_score": 96.84942538636408}, {"code": "def do_connect(\n        self,\n        dictionary: MutableMapping[str, dd.DataFrame] | None = None,\n    ) -> None\n\tif dictionary is None:\n        dictionary = {}\n\n    for k, v in dictionary.items():\n        if not isinstance(v, (dd.DataFrame, pd.DataFrame)):\n            raise TypeError(\n                f\"Expected an instance of 'dask.dataframe.DataFrame' for {k!r},\"\n                f\" got an instance of '{type(v).__name__}' instead.\"\n            )\n    super().do_connect(dictionary)\n\n", "CodeBleu_score": 96.84942538636408}]}
{"instance_id": "95afc38c9125", "generation_result": [{"code": "def get_estimator(library_name: str, estimator_name: str)\n\tclasses = get_module_members(library_name)\n    if estimator_name not in classes:\n        raise ValueError(f\"{estimator_name} not found in {library_name}\")\n    estimator = classes[estimator_name]\n    if not issubclass(estimator, BaseEstimator):\n        logger.info(f\"{estimator_name} is not a subclass of BaseEstimator\")\n    return estimator\n\n\n", "CodeBleu_score": 25.57710101826421}, {"code": "def get_estimator(library_name: str, estimator_name: str)\n\tclasses = get_module_members(library_name)\n    if estimator_name not in classes:\n        raise ValueError(f\"{estimator_name} not found in {library_name} module\")\n    estimator_class = classes[estimator_name]\n    if len(classes) > 1:\n        logger.debug(f\"Multiple classes found in {library_name} module: {list(classes.keys())}\")\n        logger.warning(f\"Multiple classes found in {library_name} module, using {estimator_name}\")\n    if not issubclass(estimator_class, sklearn.base.BaseEstimator):\n        logger.info(f\"{estimator_name} is not a subclass of sklearn.base.BaseEstimator\")\n    return estimator_class\n\n", "CodeBleu_score": 31.61697788459193}, {"code": "def get_estimator(library_name: str, estimator_name: str)\n\tclasses = get_module_members(library_name)\n    if estimator_name not in classes:\n        raise ValueError(f\"The estimator '{estimator_name}' could not be found in the module '{library_name}'.\")\n    estimator_classes = [classes[estimator_name]]\n    if len(estimator_classes) > 1:\n        logger.debug(f\"Multiple estimators found with the name '{estimator_name}': {estimator_classes}\")\n        logger.warning(f\"Multiple estimators found with the name '{estimator_name}'. The first one will be used.\")\n    if not issubclass(estimator_classes[0], BaseEstimator):\n        logger.info(f\"The estimator '{estimator_name}' is not a subclass of sklearn's BaseEstimator.\")\n    return estimator_classes[0]\n\n", "CodeBleu_score": 34.06121415476291}]}
{"instance_id": "9562a9cc043b", "generation_result": [{"code": "def estimator_to_task(estimator_name: str) -> str\n\twith open(\"ml_task_mapping.json\", \"r\") as file:\n        mapping = json.load(file)\n    for postfix in mapping:\n        if postfix in estimator_name:\n            return mapping[postfix]\n    return \"unknown\"\n\n", "CodeBleu_score": 25.50441433956584}, {"code": "def estimator_to_task(estimator_name: str) -> str\n\twith open('./config/estimator_to_task.json', 'r') as f:\n        mapping = json.load(f)\n    for postfix in mapping.keys():\n        if postfix in estimator_name:\n            return mapping[postfix]\n    return \"unknown\"\n\n", "CodeBleu_score": 29.78422299236751}, {"code": "def estimator_to_task(estimator_name: str) -> str\n\tmapping = {\n        \"Classifier\": \"classification\",\n        \"Regressor\": \"regression\",\n        \"Clusterer\": \"clustering\",\n        \"AnomalyDetector\": \"anomaly_detection\",\n        \"FeatureExtractor\": \"feature_extraction\",\n        \"FeatureSelector\": \"feature_selection\",\n        \"Sampler\": \"sampling\",\n        \"Embedder\": \"embedding\",\n        \"Evaluator\": \"evaluation\",\n        \"Transformer\": \"transformation\",\n        \"Estimator\": \"unknown\",\n    }\n    for key, value in mapping.items():\n        if estimator_name.endswith(key):\n            return value\n    return \"unknown\"", "CodeBleu_score": 23.224821268706304}]}
{"instance_id": "2f50195d043a", "generation_result": [{"code": "def load_data_from_cache(data_cache: str, data_name: str) -> Dict\n\tassert os.path.isdir(data_cache)\n    data_files = get_filenames_by_prefix(data_cache, data_name)\n    data = {}\n    for file in data_files:\n        if file.endswith(\".json\"):\n            continue\n        component, extension = file.split(\".\", 1)\n        data[component] = load_data_file(os.path.join(data_cache, file), extension)\n    return data\n\n", "CodeBleu_score": 46.87018977993817}, {"code": "def load_data_from_cache(data_cache: str, data_name: str) -> Dict\n\tfilenames = get_filenames_by_prefix(data_cache, data_name)\n    data_dict = {}\n    for filename in filenames:\n        component, extension = filename.split(\".\")[0].split(\"_\")[1:]\n        if extension == \"json\":\n            continue\n        data_dict[component] = load_data_file(\n            os.path.join(data_cache, filename), extension\n        )\n    return data_dict\n\n", "CodeBleu_score": 43.23834108143594}, {"code": "def load_data_from_cache(data_cache: str, data_name: str) -> Dict\n\tfilenames = get_filenames_by_prefix(data_cache, data_name)\n    data = {}\n    for filename in filenames:\n        component, extension = get_component_and_extension(filename)\n        if extension != \"json\":\n            filepath = os.path.join(data_cache, filename)\n            data[component] = load_data_file(filepath, extension)\n    return data\n\n", "CodeBleu_score": 37.11417375538615}]}
{"instance_id": "e0281206029e", "generation_result": [{"code": "def retrieve(url: str, filename: str) -> None\n\timport os\n    import requests\n\n    if os.path.isfile(filename):\n        return\n\n    if not url.startswith(\"http\"):\n        raise ValueError(\"URL must start with http\")\n\n    response = requests.get(url)\n    if response.status_code != 200:\n        raise AssertionError(f\"Download failed, status code: {response.status_code}\")\n\n    content_length = int(response.headers.get(\"Content-Length\"))\n    block_size = 1024\n    written = 0\n\n    with open(filename, \"wb\") as f:\n        for data in response.iter_content(block_size):\n            written += len(data)\n            f.write(data)\n\n    if written != content_length:\n        raise AssertionError(f\"Download failed, content length mismatch: {written} != {content_length}\")", "CodeBleu_score": 50.65748068127396}, {"code": "def retrieve(url: str, filename: str) -> None\n\timport os\n    import requests\n\n    if os.path.isfile(filename):\n        print(f\"File {filename} already exists. Skipping download.\")\n        return\n\n    response = requests.get(url)\n    assert response.status_code == 200, f\"Download failed: {response.status_code}\"\n\n    total_size = int(response.headers.get(\"content-length\", 0))\n    block_size = 1024\n    wrote = 0\n    with open(filename, \"wb\") as f:\n        for data in response.iter_content(block_size):\n            wrote = wrote + len(data)\n            f.write(data)\n\n    assert total_size == wrote, f\"Download failed: {total_size} != {wrote}\"\n\n    print(f\"Downloaded {filename} from {url}\")", "CodeBleu_score": 48.937956867090584}, {"code": "def retrieve(url: str, filename: str) -> None\n\tif os.path.isfile(filename):\n        return\n\n    if url.startswith(\"http\"):\n        response = requests.get(url)\n        assert response.status_code == 200, f\"Download failed, status code: {response.status_code}\"\n        total_size = int(response.headers.get('content-length', 0))\n        block_size = 1024\n        with open(filename, 'wb') as f:\n            for data in tqdm(response.iter_content(block_size), total=math.ceil(total_size // block_size), unit='KB', unit_scale=True):\n                written = f.write(data)\n                total_size -= written\n\n        assert total_size == 0, f\"File not downloaded completely, missing {total_size} bytes\"", "CodeBleu_score": 46.7557848679313}]}
{"instance_id": "a69a71cc2fe8", "generation_result": [{"code": "def load_covtype(\n    data_name: str, data_cache: str, raw_data_cache: str, dataset_params: Dict\n) -> Tuple[Dict, Dict]\n\t# load the data\n    data = pd.read_csv(f\"{raw_data_cache}/{data_name}.data\", header=None)\n    X = data.iloc[:, :-1]\n    y = data.iloc[:, -1]\n\n    # convert the labels to binary\n    if dataset_params[\"binary\"]:\n        y = (y == 2).astype(int)\n\n    # convert the labels to zero-indexed\n    y = y - 1\n\n    # create a description dictionary\n    description = {\n        \"n_classes\": 2,\n        \"split_params\": {\n            \"split_pct\": 0.8,\n            \"val_pct\": 0.1,\n            \"test_pct\": 0.1,\n        },\n    }\n\n    return ({\"X\": X, \"y\": y}, description)", "CodeBleu_score": 41.30824176011438}, {"code": "def load_covtype(\n    data_name: str, data_cache: str, raw_data_cache: str, dataset_params: Dict\n) -> Tuple[Dict, Dict]\n\t# Load the data from the cache\n    data_path = os.path.join(data_cache, \"covtype.npz\")\n    if not os.path.exists(data_path):\n        # Download the data if it doesn't exist\n        data_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/covtype/covtype.data.gz\"\n        data_file = os.path.join(raw_data_cache, \"covtype.data.gz\")\n        if not os.path.exists(data_file):\n            print(\"Downloading the Covertype dataset...\")\n            urllib.request.urlretrieve(data_url, data_file)\n        print(\"Processing the Covertype dataset...\")\n        # Read the data\n        data = np.loadtxt(data_file, delimiter=\",\", skiprows=1)\n        # Split the data into features and labels\n        features = data[:, :-1]\n        labels = data[:, -1]\n        # Convert the labels to zero-indexed\n        labels -= 1\n        # Save the data to the cache\n        np.savez(data_path, features=features, labels=labels)\n    else:\n        # Load the data from the cache\n        data = np.load(data_path)\n        features = data[\"features\"]\n        labels = data[\"labels\"]\n    # Convert the labels to binary if needed\n    if dataset_params[\"binary\"]:\n        labels = np.where(labels > 3, 1, 0)\n    # Create a description dictionary\n    data_desc = {\n        \"n_classes\": np.max(labels) + 1,\n        \"n_features\": features.shape[1],\n        \"train_size\": 581012,\n        \"val_size\": 104857,\n        \"test_size\": 104857,\n    }\n    # Return the data and the description\n    return features, labels, data_desc", "CodeBleu_score": 36.784401260658825}, {"code": "def load_covtype(\n    data_name: str, data_cache: str, raw_data_cache: str, dataset_params: Dict\n) -> Tuple[Dict, Dict]\n\t# load the dataset\n    dataset = load_dataset(\n        data_name,\n        cache_dir=data_cache,\n        data_dir=raw_data_cache,\n        download_mode=\"force_redownload\",\n        split=\"train+test\",\n    )\n\n    # process the target labels\n    dataset[\"train\"][\"label\"] = dataset[\"train\"][\"label\"].apply(lambda x: x - 1)\n    dataset[\"test\"][\"label\"] = dataset[\"test\"][\"label\"].apply(lambda x: x - 1)\n    if dataset_params[\"binary_target\"]:\n        dataset[\"train\"][\"label\"] = dataset[\"train\"][\"label\"].apply(\n            lambda x: 1 if x >= 2 else 0\n        )\n        dataset[\"test\"][\"label\"] = dataset[\"test\"][\"label\"].apply(\n            lambda x: 1 if x >= 2 else 0\n        )\n\n    # create a description dictionary\n    description = dict()\n    description[\"n_classes\"] = len(dataset[\"train\"][\"label\"].unique())\n    description[\"split_params\"] = dict()\n    description[\"split_params\"][\"split_name\"] = \"default\"\n    description[\"split_params\"][\"n_splits\"] = 1\n    description[\"split_params\"][\"train_frac\"] = 0.8\n    description[\"split_params\"][\"test_frac\"] = 0.1\n    description[\"split_params\"][\"val_frac\"] = 0.1\n    description[\"split_params\"][\"seed\"] = 123\n    return dataset, description\n\n", "CodeBleu_score": 28.3864189947014}]}
{"instance_id": "129cbd2529ef", "generation_result": [{"code": "def flatten_list(input_list: List, ensure_type_homogeneity: bool = False) -> List\n\toutput_list = list()\n    # iteration with inner recursion\n    for value in input_list:\n        if isinstance(value, list):\n            inner_flat_list = flatten_list(value)\n            for inner_value in inner_flat_list:\n                output_list.append(inner_value)\n        else:\n            output_list.append(value)\n    if ensure_type_homogeneity:\n        ensure_list_types_homogeneity(output_list)\n    return output_list\n\n", "CodeBleu_score": 81.18553468541374}, {"code": "def flatten_list(input_list: List, ensure_type_homogeneity: bool = False) -> List\n\toutput_list = list()\n    # iteration with inner recursion\n    for value in input_list:\n        if isinstance(value, list):\n            inner_flat_list = flatten_list(value)\n            for inner_value in inner_flat_list:\n                output_list.append(inner_value)\n        else:\n            output_list.append(value)\n    if ensure_type_homogeneity:\n        ensure_list_types_homogeneity(output_list)\n    return output_list\n\n", "CodeBleu_score": 81.18553468541374}, {"code": "def flatten_list(input_list: List, ensure_type_homogeneity: bool = False) -> List\n\toutput_list = list()\n    # iteration with inner recursion\n    for value in input_list:\n        if isinstance(value, list):\n            inner_flat_list = flatten_list(value)\n            for inner_value in inner_flat_list:\n                output_list.append(inner_value)\n        else:\n            output_list.append(value)\n    if ensure_type_homogeneity:\n        ensure_list_types_homogeneity(output_list)\n    return output_list\n\n", "CodeBleu_score": 81.18553468541374}]}
{"instance_id": "2d38c2c60b59", "generation_result": [{"code": "def logit(\n    x: np.ndarray,\n    /,\n    *,\n    eps: Optional[float] = None,\n    complex_mode: Literal[\"split\", \"magnitude\", \"jax\"] = \"jax\",\n    out: Optional[np.ndarray] = None,\n)\n\tif x.dtype == np.dtype(\"complex64\") or x.dtype == np.dtype(\"complex128\"):\n        if complex_mode == \"split\":\n            return np.log(x.real / (1 - x.real)) + 1j * np.log(x.imag / (1 - x.imag))\n        elif complex_mode == \"magnitude\":\n            return np.log(np.abs(x) / (1 - np.abs(x)))\n        elif complex_mode == \"jax\":\n            return np.log(x / (1 - x))\n        else:\n            raise ValueError(\n                \"complex_mode must be one of 'split', 'magnitude', or 'jax'\"\n            )\n    else:\n        if eps is None:\n            x = np.clip(x, 0, 1)\n        else:\n            x = np.clip(x, eps, 1 - eps)\n        return np.log(x / (1 - x))\n\n", "CodeBleu_score": 46.50966317938207}, {"code": "def logit(\n    x: np.ndarray,\n    /,\n    *,\n    eps: Optional[float] = None,\n    complex_mode: Literal[\"split\", \"magnitude\", \"jax\"] = \"jax\",\n    out: Optional[np.ndarray] = None,\n)\n\treturn _cur_framework(x).logit(x, eps=eps, complex_mode=complex_mode, out=out)\n\n\n", "CodeBleu_score": 17.182357016260518}, {"code": "def logit(\n    x: np.ndarray,\n    /,\n    *,\n    eps: Optional[float] = None,\n    complex_mode: Literal[\"split\", \"magnitude\", \"jax\"] = \"jax\",\n    out: Optional[np.ndarray] = None,\n)\n\treturn np.logit(\n        x, eps=eps, complex_mode=complex_mode, out=out\n    )\n\n", "CodeBleu_score": 18.549931992355713}]}
{"instance_id": "205e296f8621", "generation_result": [{"code": "def hardshrink(\n    x: np.ndarray, /, *, lambd: float = 0.5, out: Optional[np.ndarray] = None\n) -> np.ndarray\n\tif out is None:\n        out = np.zeros_like(x)\n    else:\n        out = np.copyto(out, x)\n    out[np.abs(x) < lambd] = 0\n    return out", "CodeBleu_score": 36.03164006172286}, {"code": "def hardshrink(\n    x: np.ndarray, /, *, lambd: float = 0.5, out: Optional[np.ndarray] = None\n) -> np.ndarray\n\tif out is None:\n        out = np.zeros_like(x)\n    else:\n        out[:] = 0\n    np.copyto(out, x, where=(x > lambd) | (x < -lambd))\n    return out\n\n", "CodeBleu_score": 38.37886342945522}, {"code": "def hardshrink(\n    x: np.ndarray, /, *, lambd: float = 0.5, out: Optional[np.ndarray] = None\n) -> np.ndarray\n\tif out is None:\n        out = np.zeros_like(x)\n    if lambd == 0:\n        return out\n    np.copyto(out, x)\n    out[x > lambd] = x[x > lambd]\n    out[x < -lambd] = x[x < -lambd]\n    out[x <= lambd] = 0\n    out[x >= -lambd] = 0\n    return out\n\n", "CodeBleu_score": 40.48368388507534}]}
{"instance_id": "5b133f6ebe2d", "generation_result": [{"code": "def vorbis_window(\n    window_length: np.ndarray,\n    *,\n    dtype: np.dtype = np.float32,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n\tif out is None:\n        out = np.empty(window_length, dtype=dtype)\n    else:\n        assert out.dtype == dtype\n        assert out.shape[0] == window_length\n\n    for n in range(window_length):\n        out[n] = np.sin(np.pi * np.sin(np.pi * 2 * n / window_length) ** 2)\n\n    return out", "CodeBleu_score": 37.43980429730121}, {"code": "def vorbis_window(\n    window_length: np.ndarray,\n    *,\n    dtype: np.dtype = np.float32,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n\tif out is None:\n        out = np.empty(window_length, dtype=dtype)\n    else:\n        assert len(out) == window_length\n    for i in range(window_length):\n        out[i] = np.sin(np.pi * np.sin(np.pi * i / window_length) / (2 * np.pi))\n    return out\n\n", "CodeBleu_score": 40.22531686814506}, {"code": "def vorbis_window(\n    window_length: np.ndarray,\n    *,\n    dtype: np.dtype = np.float32,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n\tif out is None:\n        out = np.empty(window_length, dtype=dtype)\n    else:\n        out = np.array(out, copy=False)\n        if out.dtype != dtype:\n            raise ValueError(f\"out array has incorrect dtype: {out.dtype}; expected {dtype}\")\n        if out.shape != (window_length,):\n            raise ValueError(f\"out array has incorrect shape: {out.shape}; expected ({window_length},)\")\n    # Vorbis window formula\n    for i in range(window_length):\n        out[i] = (1 - np.cos(2 * np.pi * i / window_length)) ** 2\n    return out\n\n", "CodeBleu_score": 33.50091801746286}]}
{"instance_id": "473296f771a9", "generation_result": [{"code": "def polyval(coeffs: np.ndarray, x: np.ndarray) -> np.ndarray\n\treturn np.polyval(coeffs, x)", "CodeBleu_score": 13.666184671336335}, {"code": "def polyval(coeffs: np.ndarray, x: np.ndarray) -> np.ndarray\n\treturn np.polyval(coeffs, x)", "CodeBleu_score": 13.666184671336335}, {"code": "def polyval(coeffs: np.ndarray, x: np.ndarray) -> np.ndarray\n\treturn np.polyval(coeffs, x)", "CodeBleu_score": 13.666184671336335}]}
{"instance_id": "b8455221cb2a", "generation_result": [{"code": "def amax(\n    x: np.ndarray,\n    /,\n    *,\n    axis: Optional[Union[int, Sequence[int]]] = None,\n    keepdims: bool = False,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n\taxis = tuple(axis) if isinstance(axis, list) else axis\n    ret = np.amax(a=x, axis=axis, out=out, keepdims=keepdims)\n    return np.asarray(ret) if np.isscalar(ret) else ret", "CodeBleu_score": 77.68208199905418}, {"code": "def amax(\n    x: np.ndarray,\n    /,\n    *,\n    axis: Optional[Union[int, Sequence[int]]] = None,\n    keepdims: bool = False,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n\taxis = tuple(axis) if isinstance(axis, list) else axis\n    ret = np.amax(a=x, axis=axis, out=out, keepdims=keepdims)\n    return np.asarray(ret) if np.isscalar(ret) else ret", "CodeBleu_score": 77.68208199905418}, {"code": "def amax(\n    x: np.ndarray,\n    /,\n    *,\n    axis: Optional[Union[int, Sequence[int]]] = None,\n    keepdims: bool = False,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n\taxis = tuple(axis) if isinstance(axis, list) else axis\n    ret = np.amax(a=x, axis=axis, out=out, keepdims=keepdims)\n    return np.asarray(ret) if np.isscalar(ret) else ret\n\n", "CodeBleu_score": 77.68208199905418}]}
{"instance_id": "f72489f434a3", "generation_result": [{"code": "def copysign(\n    x1: npt.ArrayLike,\n    x2: npt.ArrayLike,\n    /,\n    *,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n\t...\n\n", "CodeBleu_score": 16.203703703703702}, {"code": "def copysign(\n    x1: npt.ArrayLike,\n    x2: npt.ArrayLike,\n    /,\n    *,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n\t...\n\n", "CodeBleu_score": 16.203703703703702}, {"code": "def copysign(\n    x1: npt.ArrayLike,\n    x2: npt.ArrayLike,\n    /,\n    *,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n\t...\n\n", "CodeBleu_score": 16.203703703703702}]}
{"instance_id": "55c0d77e4dc2", "generation_result": [{"code": "def count_nonzero(\n    a: np.ndarray,\n    /,\n    *,\n    axis: Optional[Union[int, Tuple[int, ...]]] = None,\n    keepdims: bool = False,\n    dtype: Optional[np.dtype] = None,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n\ta = np.asarray(a)\n    if dtype is None:\n        dtype = np.int64\n    if axis is None:\n        return np.count_nonzero(a, dtype=dtype, keepdims=keepdims, out=out)\n    else:\n        axis = np.asarray(axis)\n        if axis.ndim == 0:\n            axis = np.expand_dims(axis, 0)\n        axis = axis.astype(np.int64)\n        if axis.ndim > 1:\n            raise ValueError(\"axis must be an integer or tuple of integers\")\n        axis = tuple(axis)\n        result = np.count_nonzero(a, axis=axis, keepdims=keepdims, dtype=dtype, out=out)\n        if result.ndim == 0:\n            result = np.expand_dims(result, 0)\n        return result\n\n", "CodeBleu_score": 51.07126695614708}, {"code": "def count_nonzero(\n    a: np.ndarray,\n    /,\n    *,\n    axis: Optional[Union[int, Tuple[int, ...]]] = None,\n    keepdims: bool = False,\n    dtype: Optional[np.dtype] = None,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n\tif dtype is None:\n        dtype = np.dtype(np.int64)\n    if axis is None:\n        axis = None\n    elif isinstance(axis, int):\n        axis = (axis,)\n    else:\n        axis = tuple(axis)\n    if out is None:\n        out = np.count_nonzero(a, axis=axis, keepdims=keepdims, dtype=dtype)\n    else:\n        np.count_nonzero(a, axis=axis, keepdims=keepdims, dtype=dtype, out=out)\n    if out.ndim == 0:\n        out = np.array(out, dtype=dtype)\n    return out", "CodeBleu_score": 54.915033167547975}, {"code": "def count_nonzero(\n    a: np.ndarray,\n    /,\n    *,\n    axis: Optional[Union[int, Tuple[int, ...]]] = None,\n    keepdims: bool = False,\n    dtype: Optional[np.dtype] = None,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n\tif axis is None:\n        axis = tuple(range(a.ndim))\n    elif isinstance(axis, int):\n        axis = (axis,)\n    elif isinstance(axis, tuple):\n        axis = tuple(axis)\n    else:\n        raise TypeError(\"Axis must be None, int, or tuple\")\n\n    if dtype is None:\n        dtype = np.dtype(np.int64)\n\n    if out is None:\n        out = np.count_nonzero(a, axis=axis, keepdims=keepdims, dtype=dtype)\n    else:\n        np.count_nonzero(a, axis=axis, keepdims=keepdims, dtype=dtype, out=out)\n\n    if out.ndim == 0:\n        out = np.array(out, dtype=dtype)\n\n    return out", "CodeBleu_score": 53.386594391758194}]}
{"instance_id": "784bf7d24fc2", "generation_result": [{"code": "def gradient(\n    x: np.ndarray,\n    /,\n    *,\n    spacing: Union[int, list, tuple] = 1,\n    axis: Optional[Union[int, list, tuple]] = None,\n    edge_order: int = 1,\n) -> Union[np.ndarray, List[np.ndarray]]\n\tif type(spacing) in (int, float):\n        return np.gradient(x, spacing, axis=axis, edge_order=edge_order)\n    return np.gradient(x, *spacing, axis=axis, edge_order=edge_order)\n\n", "CodeBleu_score": 95.90395527731977}, {"code": "def gradient(\n    x: np.ndarray,\n    /,\n    *,\n    spacing: Union[int, list, tuple] = 1,\n    axis: Optional[Union[int, list, tuple]] = None,\n    edge_order: int = 1,\n) -> Union[np.ndarray, List[np.ndarray]]\n\tif type(spacing) in (int, float):\n        return np.gradient(x, spacing, axis=axis, edge_order=edge_order)\n    return np.gradient(x, *spacing, axis=axis, edge_order=edge_order)\n\n", "CodeBleu_score": 95.90395527731977}, {"code": "def gradient(\n    x: np.ndarray,\n    /,\n    *,\n    spacing: Union[int, list, tuple] = 1,\n    axis: Optional[Union[int, list, tuple]] = None,\n    edge_order: int = 1,\n) -> Union[np.ndarray, List[np.ndarray]]\n\tif type(spacing) in (int, float):\n        return np.gradient(x, spacing, axis=axis, edge_order=edge_order)\n    return np.gradient(x, *spacing, axis=axis, edge_order=edge_order)\n\n", "CodeBleu_score": 95.90395527731977}]}
{"instance_id": "efa0a51af6bc", "generation_result": [{"code": "def xlogy(\n    x: np.ndarray, y: np.ndarray, /, *, out: Optional[np.ndarray] = None\n) -> np.ndarray\n\tx, y = promote_types_of_inputs(x, y)\n    if out is None:\n        out = np.empty_like(x)\n    elif out.shape != x.shape:\n        raise ValueError(\"out must have the same shape as x and y\")\n    x_is_zero = x == 0\n    out[~x_is_zero] = x[~x_is_zero] * np.log(y[~x_is_zero])\n    out[x_is_zero] = 0.0\n    return out\n\n", "CodeBleu_score": 50.77272734831805}, {"code": "def xlogy(\n    x: np.ndarray, y: np.ndarray, /, *, out: Optional[np.ndarray] = None\n) -> np.ndarray\n\tx, y = promote_types_of_inputs(x, y)\n    if out is None:\n        out = np.empty_like(x)\n    if np.all(x == 0):\n        return 0.0\n    else:\n        return np.multiply(x, np.log(y), out=out)\n\n", "CodeBleu_score": 61.86483504809457}, {"code": "def xlogy(\n    x: np.ndarray, y: np.ndarray, /, *, out: Optional[np.ndarray] = None\n) -> np.ndarray\n\tx, y = promote_types_of_inputs(x, y)\n    if out is None:\n        out = np.zeros_like(x)\n    if x.size == 0:\n        return out\n    if x.size == 1:\n        if x[0] == 0:\n            return out\n    np.multiply(np.log(y), x, out=out)\n    return out\n\n", "CodeBleu_score": 52.301060511215844}]}
{"instance_id": "ac1645284260", "generation_result": [{"code": "def _determine_depth_max_pooling(x, kernel, strides, dims, data_format=\"channel_last\")\n\t# Determine depth pooling.\n    # We assume that the kernel and the data have the same data_format.\n    depth_pooling = False\n    CHANNEL_LAST = \"channel_last\"\n    channel_idx = -1 if data_format == CHANNEL_LAST else 1\n    if len(kernel) == dims + 2:\n        spatial_kernel = kernel[1:-1] if data_format == CHANNEL_LAST else kernel[2:]\n        if kernel[channel_idx] != 1:\n            depth_pooling = True\n            if any(i != 1 for i in spatial_kernel):\n                raise NotImplementedError(\n                    \"MaxPooling supports exactly one of pooling across\"\n                    \" depth or pooling across width/height.\"\n                )\n            if len(strides) != dims + 2 or strides[channel_idx] != kernel[channel_idx]:\n                raise NotImplementedError(\n                    \"Depthwise max pooling requires the depth window to equal the depth\"\n                    \" stride\"\n                )\n            if x.shape[channel_idx] % kernel[channel_idx] != 0:\n                raise NotImplementedError(\n                    \"Depthwise max pooling requires the depth window to evenly divide\"\n                    \" the input depth\"\n                )\n            kernel = [kernel[channel_idx], *[1] * (dims - 1)]\n            strides = [strides[channel_idx], *[1] * (dims - 1)]\n        else:\n            kernel = spatial_kernel\n            if len(strides) == dims + 2:\n                strides = strides[1:-1] if data_format == CHANNEL_LAST else strides[2:]\n    return kernel, strides, depth_pooling\n\n", "CodeBleu_score": 25.007791313961913}, {"code": "def _determine_depth_max_pooling(x, kernel, strides, dims, data_format=\"channel_last\")\n\tkernel, strides, depth_pooling = _depth_max_pooling_helper(\n        kernel, strides, dims, data_format\n    )\n    if depth_pooling:\n        x = tf.transpose(x, [0, 3, 1, 2] if data_format == \"channel_last\" else [0, 2, 3, 1])\n    return x, kernel, strides, depth_pooling\n\n", "CodeBleu_score": 50.66499185469966}, {"code": "def _determine_depth_max_pooling(x, kernel, strides, dims, data_format=\"channel_last\")\n\t# Determine depth pooling.\n    # We assume that the kernel and the data have the same data_format.\n    depth_pooling = False\n    CHANNEL_LAST = \"channel_last\"\n    channel_idx = -1 if data_format == CHANNEL_LAST else 1\n    if len(kernel) == dims + 2:\n        spatial_kernel = kernel[1:-1] if data_format == CHANNEL_LAST else kernel[2:]\n        if kernel[channel_idx] != 1:\n            depth_pooling = True\n            if any(i != 1 for i in spatial_kernel):\n                raise NotImplementedError(\n                    \"MaxPooling supports exactly one of pooling across\"\n                    \" depth or pooling across width/height.\"\n                )\n            if len(strides) != dims + 2 or strides[channel_idx] != kernel[channel_idx]:\n                raise NotImplementedError(\n                    \"Depthwise max pooling requires the depth window to equal the depth\"\n                    \" stride\"\n                )\n            if x.shape[channel_idx] % kernel[channel_idx] != 0:\n                raise NotImplementedError(\n                    \"Depthwise max pooling requires the depth window to evenly divide\"\n                    \" the input depth\"\n                )\n            kernel = [kernel[channel_idx], *[1] * (dims - 1)]\n            strides = [strides[channel_idx], *[1] * (dims - 1)]\n        else:\n            kernel = spatial_kernel\n            if len(strides) == dims + 2:\n                strides = strides[1:-1] if data_format == CHANNEL_LAST else strides[2:]\n    return kernel, strides, depth_pooling\n\n", "CodeBleu_score": 25.007791313961913}]}
{"instance_id": "18ff0a4f7550", "generation_result": [{"code": "def dropout1d(\n    x: np.ndarray,\n    prob: float,\n    /,\n    *,\n    training: bool = True,\n    data_format: str = \"NWC\",\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n\tif not training:\n        return x\n    if data_format == \"NCW\":\n        x = np.transpose(x, (0, 2, 1))\n    if out is None:\n        out = np.empty_like(x)\n    else:\n        assert out.shape == x.shape\n    n, w, c = x.shape\n    if n == 1:\n        mask = np.random.binomial(1, 1 - prob, size=(w, c))\n        out[:] = x\n        out[mask == 0] = 0\n        out /= 1 - prob\n    else:\n        mask = np.random.binomial(1, 1 - prob, size=(n, w, c))\n        out[:] = x\n        out[mask == 0] = 0\n        out /= 1 - prob\n    if data_format == \"NCW\":\n        out = np.transpose(out, (0, 2, 1))\n    return out\n\n", "CodeBleu_score": 45.68917745947713}, {"code": "def dropout1d(\n    x: np.ndarray,\n    prob: float,\n    /,\n    *,\n    training: bool = True,\n    data_format: str = \"NWC\",\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n\tif not training:\n        return x\n    if data_format == \"NCW\":\n        x = x.transpose(0, 2, 1)\n    if out is None:\n        out = np.empty_like(x)\n    if x.ndim == 2:\n        x_shape = x.shape\n        x = x.reshape(-1, x_shape[-1])\n        out = out.reshape(-1, x_shape[-1])\n    mask = np.random.binomial(n=1, p=1 - prob, size=x.shape)\n    out[:] = x * mask / (1 - prob)\n    if data_format == \"NCW\":\n        out = out.transpose(0, 2, 1)\n    return out\n\n", "CodeBleu_score": 51.24627100760777}, {"code": "def dropout1d(\n    x: np.ndarray,\n    prob: float,\n    /,\n    *,\n    training: bool = True,\n    data_format: str = \"NWC\",\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n\tif not training:\n        return x\n    if data_format == \"NCW\":\n        x = x.transpose(0, 2, 1)\n    x = x.copy()\n    x.setflags(write=True)\n    if out is None:\n        out = x\n    if prob == 0.0:\n        return x\n    if x.ndim == 2:\n        x_shape = x.shape\n        x = x.reshape(-1, x_shape[-1])\n    elif x.ndim == 3:\n        x_shape = x.shape\n        x = x.reshape(-1, x_shape[-2], x_shape[-1])\n    else:\n        raise ValueError(\"x must have 2 or 3 dimensions\")\n    if prob == 1.0:\n        x.fill(0.0)\n        return x\n    mask = np.random.binomial(1, 1 - prob, x.shape)\n    x *= mask\n    x /= 1 - prob\n    if x.ndim == 2:\n        x = x.reshape(x_shape)\n    elif x.ndim == 3:\n        x = x.reshape(x_shape)\n    if data_format == \"NCW\":\n        x = x.transpose(0, 2, 1)\n    return x\n\n", "CodeBleu_score": 47.03425260779823}]}
{"instance_id": "0ce45eb7cbd9", "generation_result": [{"code": "def top_k(\n    x: np.ndarray,\n    k: int,\n    /,\n    *,\n    axis: int = -1,\n    largest: bool = True,\n    sorted: bool = True,\n    out: Optional[Tuple[np.ndarray, np.ndarray]] = None,\n) -> Tuple[np.ndarray, np.ndarray]\n\t...", "CodeBleu_score": 14.79044834307992}, {"code": "def top_k(\n    x: np.ndarray,\n    k: int,\n    /,\n    *,\n    axis: int = -1,\n    largest: bool = True,\n    sorted: bool = True,\n    out: Optional[Tuple[np.ndarray, np.ndarray]] = None,\n) -> Tuple[np.ndarray, np.ndarray]\n\tif not isinstance(x, np.ndarray):\n        raise TypeError(f\"x must be a np.ndarray, not {type(x)}\")\n    if not isinstance(k, int):\n        raise TypeError(f\"k must be an int, not {type(k)}\")\n    if not isinstance(axis, int):\n        raise TypeError(f\"axis must be an int, not {type(axis)}\")\n    if not isinstance(largest, bool):\n        raise TypeError(f\"largest must be a bool, not {type(largest)}\")\n    if not isinstance(sorted, bool):\n        raise TypeError(f\"sorted must be a bool, not {type(sorted)}\")\n    if not isinstance(out, Optional[Tuple[np.ndarray, np.ndarray]]):\n        raise TypeError(\n            f\"out must be an Optional[Tuple[np.ndarray, np.ndarray]], not {type(out)}\"\n        )\n\n    if axis < 0:\n        axis = x.ndim + axis\n    if axis >= x.ndim:\n        raise ValueError(f\"axis {axis} is out of range for x of shape {x.shape}\")\n    if k <= 0:\n        raise ValueError(f\"k must be positive, not {k}\")\n    if k > x.shape[axis]:\n        raise ValueError(\n            f\"k {k} must not be greater than the size of the specified axis {x.shape[axis]}\"\n        )\n    if out is not None and not isinstance(out, tuple):\n        raise TypeError(f\"out must be a tuple, not {type(out)}\")\n    if out is not None and len(out) != 2:\n        raise ValueError(f\"out must have length 2, not {len(out)}\")\n    if out is not None and not isinstance(out[0], np.ndarray):\n        raise TypeError(f\"out[0] must be a np.ndarray, not {type(out[0])}\")\n    if out is not None and not isinstance(out[1], np.ndarray):\n        raise TypeError(f\"out[1] must be a np", "CodeBleu_score": 24.62356398384128}, {"code": "def top_k(\n    x: np.ndarray,\n    k: int,\n    /,\n    *,\n    axis: int = -1,\n    largest: bool = True,\n    sorted: bool = True,\n    out: Optional[Tuple[np.ndarray, np.ndarray]] = None,\n) -> Tuple[np.ndarray, np.ndarray]\n\t...\n\n", "CodeBleu_score": 14.79044834307992}]}
{"instance_id": "1424f9541998", "generation_result": [{"code": "def l1_normalize(\n    x: np.ndarray,\n    /,\n    *,\n    axis: Optional[int] = None,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n\tif out is None:\n        out = np.empty_like(x)\n    if axis is None:\n        norm = np.linalg.norm(x.reshape(-1), ord=1)\n        out[:] = x / norm\n    else:\n        norm = np.linalg.norm(x, ord=1, axis=axis, keepdims=True)\n        out[:] = x / (norm + 1e-8)\n    return out\n\n", "CodeBleu_score": 51.52221287364036}, {"code": "def l1_normalize(\n    x: np.ndarray,\n    /,\n    *,\n    axis: Optional[int] = None,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n\tnorm = np.abs(x).sum(axis=axis, keepdims=True)\n    if out is None:\n        out = np.empty_like(x)\n    out = np.divide(x, norm + EPSILON, out=out)\n    return out\n\n", "CodeBleu_score": 33.34159551689936}, {"code": "def l1_normalize(\n    x: np.ndarray,\n    /,\n    *,\n    axis: Optional[int] = None,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n\tnorm = np.sum(np.abs(x), axis=axis, keepdims=True)\n    if out is None:\n        out = np.empty_like(x)\n    out[:] = x\n    out /= (norm + EPSILON)\n    return out\n\n", "CodeBleu_score": 34.927450512446654}]}
{"instance_id": "4c3a386a1f05", "generation_result": [{"code": "def beta(\n    alpha: Union[float, np.ndarray],\n    beta: Union[float, np.ndarray],\n    /,\n    *,\n    shape: Optional[Union[ivy.NativeShape, Sequence[int]]] = None,\n    device: Optional[str] = None,\n    dtype: Optional[np.dtype] = None,\n    seed: Optional[int] = None,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n\tshape = _check_bounds_and_get_shape(alpha, beta, shape).shape\n    if seed is not None:\n        np.random.seed(seed)\n    return np.asarray(np.random.beta(alpha, beta, shape), dtype=dtype)\n\n", "CodeBleu_score": 89.70998520485102}, {"code": "def beta(\n    alpha: Union[float, np.ndarray],\n    beta: Union[float, np.ndarray],\n    /,\n    *,\n    shape: Optional[Union[ivy.NativeShape, Sequence[int]]] = None,\n    device: Optional[str] = None,\n    dtype: Optional[np.dtype] = None,\n    seed: Optional[int] = None,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n\tshape = _check_bounds_and_get_shape(alpha, beta, shape).shape\n    if seed is not None:\n        np.random.seed(seed)\n    return np.asarray(np.random.beta(alpha, beta, shape), dtype=dtype)\n\n", "CodeBleu_score": 89.70998520485102}, {"code": "def beta(\n    alpha: Union[float, np.ndarray],\n    beta: Union[float, np.ndarray],\n    /,\n    *,\n    shape: Optional[Union[ivy.NativeShape, Sequence[int]]] = None,\n    device: Optional[str] = None,\n    dtype: Optional[np.dtype] = None,\n    seed: Optional[int] = None,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n\tshape = _check_bounds_and_get_shape(alpha, beta, shape).shape\n    if seed is not None:\n        np.random.seed(seed)\n    return np.asarray(np.random.beta(alpha, beta, shape), dtype=dtype)\n\n", "CodeBleu_score": 89.70998520485102}]}
{"instance_id": "271200a8b261", "generation_result": [{"code": "def invert_permutation(\n    x: Union[np.ndarray, list, tuple],\n    /,\n) -> np.ndarray\n\tif not isinstance(x, np.ndarray):\n        x = np.array(x)\n    sorted_indices = np.argsort(x)\n    inverse = np.zeros(shape=sorted_indices.shape, dtype=np.int64)\n    inverse[sorted_indices] = np.arange(0, len(x), 1)\n    return np.argsort(inverse)\n\n\n", "CodeBleu_score": 62.5354101906424}, {"code": "def invert_permutation(\n    x: Union[np.ndarray, list, tuple],\n    /,\n) -> np.ndarray\n\tsorted_indices = np.argsort(x)\n    inverse = np.zeros(sorted_indices.shape, dtype=int)\n    inverse[sorted_indices] = np.arange(len(x))\n    return np.sort(inverse)\n\n\n", "CodeBleu_score": 49.56533520244379}, {"code": "def invert_permutation(\n    x: Union[np.ndarray, list, tuple],\n    /,\n) -> np.ndarray\n\tif isinstance(x, np.ndarray):\n        sorted_indices = np.argsort(x)\n        inverse = np.zeros(sorted_indices.shape, dtype=int)\n        inverse[sorted_indices] = np.arange(len(x))\n        return np.argsort(inverse)\n    elif isinstance(x, list) or isinstance(x, tuple):\n        sorted_indices = np.argsort(x)\n        inverse = np.zeros(sorted_indices.shape, dtype=int)\n        inverse[sorted_indices] = np.arange(len(x))\n        return np.argsort(inverse).tolist()\n    else:\n        raise TypeError(\"x must be either a NumPy array, list, or tuple.\")", "CodeBleu_score": 58.656415750229606}]}
{"instance_id": "040d73b37229", "generation_result": [{"code": "def plot_medical_charts() -> None\n\t# Create figure and subplots\n    fig, axs = plt.subplots(2, 2, figsize=(12, 12))\n\n    # Plot data\n    plot_monthly_vomit(axs[0, 0], data)\n    plot_days_between_vomit(axs[0, 1], data)\n    plot_daycare_days(axs[1, 0], data)\n    plot_doctor_visit_monthly(axs[1, 1], data)\n\n    # Adjust layout\n    fig.tight_layout()\n\n    # Export figure\n    fig.savefig(\"medical_charts.png\")\n\n", "CodeBleu_score": 23.527864736703037}, {"code": "def plot_medical_charts() -> None\n\t# Set style and create figure\n    plt.style.use(\"seaborn-whitegrid\")\n    fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(10, 10))\n\n    # Plot vomit per month\n    plot_monthly_vomit(axes[0, 0], data)\n\n    # Plot days between vomit\n    plot_days_between_vomit(axes[0, 1], data)\n\n    # Plot daycare days\n    plot_daycare_days(axes[1, 0], data)\n\n    # Plot doctor visits\n    plot_doctor_visit_monthly(axes[1, 1], data)\n\n    # Adjust layout and save\n    fig.tight_layout()\n    fig.savefig(\"./images/medical_charts.png\", dpi=300)\n\n", "CodeBleu_score": 26.570103392079325}, {"code": "def plot_medical_charts() -> None\n\t# Set style and create figure with subplots\n    plt.style.use(\"seaborn-whitegrid\")\n    fig, ax = plt.subplots(2, 2)\n\n    # Read in data\n    data = pd.read_csv(\"data/medical.csv\")\n\n    # Plot data on subplots\n    plot_monthly_vomit(ax[0][0], data)\n    plot_days_between_vomit(ax[0][1], data)\n    plot_daycare_days(ax[1][0], data)\n    plot_doctor_visit_monthly(ax[1][1], data)\n\n    # Adjust layout and save figure\n    fig.tight_layout()\n    fig.savefig(\"figures/medical_charts.png\", dpi=300)\n\n", "CodeBleu_score": 23.244041855010263}]}
{"instance_id": "7061f4d0c3fc", "generation_result": [{"code": "def plot_feeding_24h_viz() -> None\n\t# Import and extract data\n    data = parse_raw_data(\n        pd.read_csv(\"data/raw/baby_feeding_data.csv\", parse_dates=[\"Date\"]),\n        key=[\"Date\"],\n    )\n\n    # Set up plot\n    plt.style.use(\"dark_background\")\n    plt.figure(figsize=(16, 9), dpi=80)\n    plt.grid(True)\n\n    # Compute offset from birthday\n    offset = data.iloc[-1][\"day_number\"] - data.iloc[-1][\"timestamp_hour\"]\n\n    # Create scatter plot\n    plt.scatter(\n        data[\"day_number\"] - offset,\n        data[\"timestamp_hour\"],\n        c=data[\"Type\"],\n        cmap=plt.cm.get_cmap(\"plasma\", 2),\n        edgecolors=\"none\",\n        alpha=0.5,\n    )\n\n    # Format plot\n    end_date = get_end_date(data, first_year_only=True)\n    plt.xlim(0, end_date)\n    plt.ylim(0, 24)\n    plt.xticks(np.arange(0, end_date, 7))\n    plt.yticks(np.arange(0, 24, 2))\n    plt.xlabel(\"Day\")\n    plt.ylabel(\"Hour\")\n    plt.title(\"Bottle and Solid Feeding Times\")\n    plt.legend([\"Bottle\", \"Solid\"])\n\n    # Export figure\n    export_figure(\"figures/feeding_24h.png\")\n\n", "CodeBleu_score": 24.89324856763691}, {"code": "def plot_feeding_24h_viz() -> None\n\t# Import and extract data\n    data = parse_raw_data(data=data, key=[\"Date\", \"Time\"])\n\n    # Set up plot\n    plt.style.use(\"dark_grid\")\n    plt.figure(figsize=(12, 8))\n\n    # Compute offset\n    offset = get_end_date(data=data, first_year_only=True)\n\n    # Create scatter plot\n    plt.scatter(\n        data[\"timestamp_hour\"] + data[\"day_number\"] - offset,\n        data[\"Bottle\"],\n        color=\"red\",\n        label=\"Bottle\",\n    )\n    plt.scatter(\n        data[\"timestamp_hour\"] + data[\"day_number\"] - offset,\n        data[\"Solid\"],\n        color=\"blue\",\n        label=\"Solid\",\n    )\n\n    # Format plot\n    plt.legend()\n    plt.xlabel(\"Day\")\n    plt.ylabel(\"Hour\")\n    plt.title(\"Bottle and Solid Feeding Times\")\n    plt.xticks(range(0, 365, 7))\n    plt.xlim(0, 365)\n    plt.ylim(0, 24)\n\n    # Export figure\n    export_figure(\"plot_feeding_24h_viz.png\")\n\n", "CodeBleu_score": 20.127342707106866}, {"code": "def plot_feeding_24h_viz() -> None\n\t# Import and extract data\n    data = parse_raw_data(baby_data, [\"Date\", \"Start\", \"Feeding type\"])\n\n    # Set up the plot\n    sns.set(style=\"darkgrid\")\n\n    # Compute offset from birthday\n    offset = (data[\"Date\"].iloc[-1] - data[\"Date\"].iloc[0]).days\n\n    # Create a scatter plot of feeding times\n    sns.scatterplot(\n        x=\"timestamp_hour\",\n        y=\"day_number\",\n        hue=\"Feeding type\",\n        data=data,\n        palette=[\"r\", \"b\"],\n        edgecolor=\"none\",\n        s=50,\n    )\n\n    # Format plot to display feeding data over a week in 24-hour format\n    plt.xlabel(\"Time of day\")\n    plt.ylabel(\"Day\")\n    plt.xlim(0, 24)\n    plt.ylim(0, offset + 7)\n    plt.xticks(np.arange(0, 24, 2))\n    plt.yticks(np.arange(0, offset + 7, 1))\n    plt.legend(loc=\"upper right\")\n\n    # Export figure\n    export_figure(\"plot_feeding_24h_viz\", \"png\")\n\n", "CodeBleu_score": 19.88945402394414}]}
{"instance_id": "6c1ffa63d122", "generation_result": [{"code": "def plot(self, ax, task: ClassificationTask = ClassificationTask.Result, **kwargs):\n\ttarget_counts, target_names = self._get_target_counts(task)\n    sns.barplot(x=target_names, y=target_counts, ax=ax, **kwargs)\n\n", "CodeBleu_score": 45.064674385803194}, {"code": "def plot(self, ax, task: ClassificationTask = ClassificationTask.Result, **kwargs):\n\ttarget_counts, target_names = self._get_target_counts(task)\n    sns.barplot(target_names, target_counts, ax=ax, **kwargs)", "CodeBleu_score": 43.81321740077998}, {"code": "def plot(self, ax, task: ClassificationTask = ClassificationTask.Result, **kwargs):\n\ttarget_counts, target_names = self._get_target_counts(task)\n    sns.barplot(x=target_names, y=target_counts, ax=ax, **kwargs)\n\n", "CodeBleu_score": 45.064674385803194}]}
{"instance_id": "2f81a906bad5", "generation_result": [{"code": "def render_to_rgb(figure)\n\timport matplotlib\n    matplotlib.use('Agg')\n    import matplotlib.pyplot as plt\n    from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n    import numpy as np\n\n    if isinstance(figure, list):\n        img_list = []\n        for fig in figure:\n            canvas = FigureCanvas(fig)\n            canvas.draw()\n            img = np.array(canvas.buffer_rgba()).reshape(fig.bbox.bounds[3], fig.bbox.bounds[2], 4)[:, :, :3]\n            img_list.append(img)\n        return np.stack(img_list, axis=0)\n    else:\n        canvas = FigureCanvas(figure)\n        canvas.draw()\n        img = np.array(canvas.buffer_rgba()).reshape(figure.bbox.bounds[3], figure.bbox.bounds[2], 4)[:, :, :3]\n        return img\n\n", "CodeBleu_score": 36.58343251716502}, {"code": "def render_to_rgb(figure)\n\timport matplotlib\n    matplotlib.use('Agg')\n    import matplotlib.pyplot as plt\n    import numpy as np\n    from PIL import Image\n\n    if isinstance(figure, list):\n        rgb_images = []\n        for fig in figure:\n            fig.canvas = plt.FigureCanvas(fig)\n            fig.canvas.draw()\n            w, h = fig.canvas.get_width_height()\n            buf = np.fromstring(fig.canvas.tostring_rgb(), dtype=np.uint8)\n            buf.shape = (h, w, 3)\n            rgb_images.append(buf)\n            if fig.get_visible():\n                fig.clf()\n                plt.close(fig)\n        return np.stack(rgb_images)\n    else:\n        figure.canvas = plt.FigureCanvas(figure)\n        figure.canvas.draw()\n        w, h = figure.canvas.get_width_height()\n        buf = np.fromstring(figure.canvas.tostring_rgb(), dtype=np.uint8)\n        buf.shape = (h, w, 3)\n        if figure.get_visible():\n            figure.clf()\n            plt.close(figure)\n        return buf\n\n", "CodeBleu_score": 34.41963836399025}, {"code": "def render_to_rgb(figure)\n\timport numpy as np\n    import matplotlib\n    matplotlib.use('Agg')\n    import matplotlib.pyplot as plt\n    from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n    canvas = FigureCanvas(figure)\n    canvas.draw()\n    width, height = figure.get_size_inches() * figure.get_dpi()\n    image = np.frombuffer(canvas.tostring_rgb(), dtype='uint8').reshape(int(height), int(width), 3)\n    return image\n\n\n", "CodeBleu_score": 31.245341470361026}]}
{"instance_id": "c3037b01569c", "generation_result": [{"code": "def beholder_pytorch()\n\timport time\n    import random\n    import torch\n    from beholder.beholder import Beholder\n\n    beholder = Beholder(log_dir='.')\n    for i in range(1000):\n        tensors = [torch.rand(100, 100) for _ in range(2)]\n        names = ['tensor1', 'tensor2']\n        frame = random.randint(0, 1000000000)\n        beholder.update(tensors, names, frame)\n        time.sleep(0.01)\n        print(i)\n\n", "CodeBleu_score": 25.947215967609576}, {"code": "def beholder_pytorch()\n\timport torch\n    import time\n    import random\n    import beholder\n\n    for i in range(1000):\n        tensors = [torch.rand(3, 3, 128, 128) for j in range(random.randint(1, 5))]\n        names = [f\"Tensor {j}\" for j in range(len(tensors))]\n        bh = beholder.Beholder(log_dir=\"logs/pytorch\")\n        bh.update(tensors, names, frame=random.randint(0, 1000))\n        time.sleep(0.01)\n        print(i)\n\n", "CodeBleu_score": 25.813432075931473}, {"code": "def beholder_pytorch()\n\timport beholder\n    import torch\n    import numpy as np\n    import time\n\n    for i in range(1000):\n        a = torch.rand(10, 10)\n        b = torch.rand(10, 10)\n        beholder.Beholder(log_dir='beholder_pytorch').update({'a': a, 'b': b}, frame=np.random.randint(10000))\n        time.sleep(0.01)\n        print(i)\n\n\n", "CodeBleu_score": 22.867894723400063}]}
{"instance_id": "cd57b5de8a88", "generation_result": [{"code": "def log_pr_data(self, tag, summary, num_thresholds, step=None)\n\ttensor_data = summary.tensor_data\n    tensor_shape = summary.tensor_shape\n    tensor_data = np.array(tensor_data)\n    tensor_data = np.reshape(tensor_data, tensor_shape.dims)\n\n    thresholds = np.linspace(0, 1, num_thresholds)\n\n    tp = np.array(tensor_data[0]).flatten()\n    fp = np.array(tensor_data[1]).flatten()\n    tn = np.array(tensor_data[2]).flatten()\n    fn = np.array(tensor_data[3]).flatten()\n    precision = np.array(tensor_data[4]).flatten()\n    recall = np.array(tensor_data[5]).flatten()\n\n    pr_data = {\n        'thresholds': thresholds,\n        'tp': tp,\n        'fp': fp,\n        'tn': tn,\n        'fn': fn,\n        'precision': precision,\n        'recall': recall\n    }\n\n    self.log_asset_data(tag, pr_data, step=step)\n\n", "CodeBleu_score": 29.594175522227218}, {"code": "def log_pr_data(self, tag, summary, num_thresholds, step=None)\n\ttensor_data = summary.get_tensor_data()\n    tensor_shape = summary.get_tensor_shape()\n    tensor_size = summary.get_tensor_size()\n    tensor_dtype = summary.get_tensor_dtype()\n    tensor_data = tensor_data.reshape(tensor_shape)\n    thresholds = list(np.linspace(0, 1, num_thresholds))\n    tp = tensor_data[:, 0].flatten().tolist()\n    fp = tensor_data[:, 1].flatten().tolist()\n    tn = tensor_data[:, 2].flatten().tolist()\n    fn = tensor_data[:, 3].flatten().tolist()\n    precision = tensor_data[:, 4].flatten().tolist()\n    recall = tensor_data[:, 5].flatten().tolist()\n    pr_data = {\n        \"thresholds\": thresholds,\n        \"tp\": tp,\n        \"fp\": fp,\n        \"tn\": tn,\n        \"fn\": fn,\n        \"precision\": precision,\n        \"recall\": recall,\n    }\n    self.log_asset_data(tag, pr_data, step=step)\n\n", "CodeBleu_score": 26.80846039639008}, {"code": "def log_pr_data(self, tag, summary, num_thresholds, step=None)\n\t# extract tensor data from summary\n    tensor_data = summary.value[0].tensor\n    # reshape tensor data based on its dimensions\n    tensor_data = np.reshape(tensor_data, (tensor_data.shape[0], tensor_data.shape[1]))\n    # compute thresholds as a list of values from 0 to 1 divided by the number of thresholds\n    thresholds = [i / num_thresholds for i in range(0, num_thresholds + 1)]\n    # split tensor data into true positives (TP), false positives (FP), true negatives (TN), false negatives (FN), precision, and recall, flattening each into a list\n    TP = tensor_data[:, 0].tolist()\n    FP = tensor_data[:, 1].tolist()\n    TN = tensor_data[:, 2].tolist()\n    FN = tensor_data[:, 3].tolist()\n    precision = tensor_data[:, 4].tolist()\n    recall = tensor_data[:, 5].tolist()\n    # store the thresholds, TP, FP, TN, FN, precision, and recall in a dictionary named pr_data\n    pr_data = {\n        'thresholds': thresholds,\n        'TP': TP,\n        'FP': FP,\n        'TN': TN,\n        'FN': FN,\n        'precision': precision,\n        'recall': recall\n    }\n    # log the pr_data using the log_asset_data method with the provided tag and optional step\n    self.log_asset_data(tag, pr_data, step=step)\n\n\n", "CodeBleu_score": 31.952706149955773}]}
{"instance_id": "7c4ea9beecc9", "generation_result": [{"code": "def load_mnist(images, labels, num)\n\timages = images.reshape(-1, 28 * 28)\n    labels = np.array(labels)\n    return images[:num], labels[:num]\n\n", "CodeBleu_score": 6.624164623001155}, {"code": "def load_mnist(images, labels, num)\n\timages = images.reshape(-1, 28, 28)\n    labels = np.asarray(labels)\n\n    return images[:num], labels[:num]\n\n", "CodeBleu_score": 7.813066038567296}, {"code": "def load_mnist(images, labels, num)\n\t# Initialize the data and target arrays.\n    data = np.zeros((num, 784), dtype=np.float32)\n    target = np.zeros(num, dtype=np.int32)\n\n    # Open the images and labels with gzip in binary read mode.\n    with gzip.open(images, 'rb') as f_images, gzip.open(labels, 'rb') as f_labels:\n        # Skip the header bytes.\n        f_images.read(16)\n        f_labels.read(8)\n\n        # Read the data and labels one byte at a time.\n        for i in range(num):\n            # Read a single image label.\n            target[i] = ord(f_labels.read(1))\n\n            # Read a 28x28 image.\n            for j in range(784):\n                # Read a single byte from the image file and add it to the data array.\n                data[i, j] = ord(f_images.read(1))\n\n    # Return the data and target arrays.\n    return data, target\n\n", "CodeBleu_score": 54.31016073721987}]}
{"instance_id": "bf4f1c828b25", "generation_result": [{"code": "def compute_stats(self, ldf: LuxDataFrame)\n\tself.unique_values = {}\n    self.cardinality = {}\n    self.min_max = {}\n    self.length = len(ldf)\n    for attr in ldf.columns:\n        if pd.api.types.is_numeric_dtype(ldf[attr]):\n            self.min_max[attr] = (ldf[attr].min(), ldf[attr].max())\n        if pd.api.types.is_datetime64_any_dtype(ldf[attr]):\n            self.unique_values[attr] = ldf[attr].dt.strftime(\"%Y-%m-%d %H:%M:%S\").unique()\n        else:\n            self.unique_values[attr] = ldf[attr].unique()\n        self.cardinality[attr] = len(self.unique_values[attr])\n    if not pd.api.types.is_integer_dtype(ldf.index):\n        self.unique_values[\"index\"] = ldf.index.unique()\n        self.cardinality[\"index\"] = len(self.unique_values[\"index\"])", "CodeBleu_score": 39.03935567476849}, {"code": "def compute_stats(self, ldf: LuxDataFrame)\n\tself.unique_values = {}\n    self.cardinality = {}\n    self.min_max = {}\n    self.length = len(ldf)\n    for attr in ldf.columns:\n        if ldf.data_type[attr] == \"temporal\":\n            # temporal attributes are stored as datetimes, but converted to strings for display\n            self.unique_values[attr] = ldf[attr].unique().astype(str).tolist()\n        else:\n            self.unique_values[attr] = ldf[attr].unique().tolist()\n        self.cardinality[attr] = len(self.unique_values[attr])\n        if ldf.data_type[attr] == \"quantitative\":\n            self.min_max[attr] = (ldf[attr].min(), ldf[attr].max())\n    if ldf.index.dtype != np.int64:\n        self.unique_values[ldf.index.name] = ldf.index.unique().tolist()\n        self.cardinality[ldf.index.name] = len(self.unique_values[ldf.index.name])\n\n", "CodeBleu_score": 34.3608852074378}, {"code": "def compute_stats(self, ldf: LuxDataFrame)\n\tself.unique_values = {}\n    self.min_max = {}\n    self.cardinality = {}\n    self.length = len(ldf)\n    for attribute in ldf.columns:\n        if ldf[attribute].dtype == \"object\":\n            self.unique_values[attribute] = list(ldf[attribute].unique())\n            self.cardinality[attribute] = len(self.unique_values[attribute])\n        else:\n            self.min_max[attribute] = (ldf[attribute].min(), ldf[attribute].max())\n        if ldf[attribute].dtype == \"datetime64[ns]\":\n            self.unique_values[attribute] = list(ldf[attribute].unique())\n            self.cardinality[attribute] = len(self.unique_values[attribute])\n    if ldf.index.dtype != \"int64\":\n        self.unique_values[ldf.index.name] = list(ldf.index.unique())\n        self.cardinality[ldf.index.name] = len(self.unique_values[ldf.index.name])\n\n", "CodeBleu_score": 40.29026528113185}]}
{"instance_id": "3e6fc3d2f47c", "generation_result": [{"code": "def execute_preview(tbl: LuxSQLTable, preview_size=5)\n\ttbl_name = tbl.table_name\n    tbl_type = tbl.table_type\n    preview_query = \"SELECT * FROM {tbl_name} LIMIT {preview_size}\"\n    preview_query = preview_query.format(tbl_name=tbl_name, preview_size=preview_size)\n    preview_df = pd.read_sql(preview_query, tbl.connection)\n    return preview_df\n\n", "CodeBleu_score": 23.362219361929732}, {"code": "def execute_preview(tbl: LuxSQLTable, preview_size=5)\n\tquery = f\"SELECT * FROM {tbl.table_name} LIMIT {preview_size}\"\n    df = tbl.preview_df\n    return df\n\n", "CodeBleu_score": 11.184308071944194}, {"code": "def execute_preview(tbl: LuxSQLTable, preview_size=5)\n\ttbl.preview_size = preview_size\n    query = tbl.preview_query()\n    tbl._preview = tbl._preview_cache.get(query)\n    if tbl._preview is None:\n        tbl._preview = tbl.preview_df()\n        tbl._preview_cache[query] = tbl._preview\n    return tbl._preview\n\n", "CodeBleu_score": 17.779707081386604}]}
{"instance_id": "b3dee87be4b8", "generation_result": [{"code": "def execute_sampling(tbl: LuxSQLTable)\n\timport pandas as pd\n    import numpy as np\n    import warnings\n    import lux\n    from lux.utils import utils\n    from lux.utils.utils import check_import_lux_widget\n    from lux.utils.date_utils import compute_date_granularity\n    from lux.utils.date_utils import compute_granularity_from_str\n    from lux.utils.utils import get_agg_title\n    from lux.utils.utils import get_filter_specs\n    from lux.utils.utils import check_import_lux_widget\n    from lux.utils.utils import check_if_id_like\n    from lux.utils.utils import check_import_lux_widget\n    from lux.utils.utils import check_if_id_like\n    from lux.utils.utils import check_import_lux_widget\n    from lux.utils.utils import check_if_id_like\n    from lux.utils.utils import check_import_lux_widget\n    from lux.utils.utils import check_if_id_like\n    from lux.utils.utils import check_import_lux_widget\n    from lux.utils.utils import check_if_id_like\n    from lux.utils.utils import check_import_lux_widget\n    from lux.utils.utils import check_if_id_like\n    from lux.utils.utils import check_import_lux_widget\n    from lux.utils.utils import check_if_id_like\n    from lux.utils.utils import check_import_lux_widget\n    from lux.utils.utils import check_if_id_like\n    from lux.utils.utils import check_import_lux_widget\n    from lux.utils.utils import check_if_id_like\n    from lux.utils.utils import check_import_lux_widget\n    from lux.utils.utils import check_if_id_like\n    from lux.utils.utils import check_import_lux_widget\n    from lux.utils.utils import check_if_id_like\n    from lux.utils.utils import check_import_lux_widget\n    from lux.utils.utils import check_if_id_like", "CodeBleu_score": 5.524307709560168}, {"code": "def execute_sampling(tbl: LuxSQLTable)\n\timport pandas as pd\n    import lux\n    from lux.utils import utils\n    from lux.utils.utils import get_filter_specs\n    import numpy as np\n\n    sample_flag = lux.config.SAMPLE_FLAG\n    sample_start = lux.config.SAMPLE_START\n    sample_cap = lux.config.SAMPLE_CAP\n    sample_frac = lux.config.SAMPLE_FRAC\n\n    if sample_flag:\n        tbl._sampled = pd.DataFrame()\n        tbl._sampled_df_len = 0\n        tbl._sampled_len = 0\n        tbl._sampled_query = \"\"\n\n        # Retrieve table length\n        tbl_len_query = \"SELECT COUNT(*) FROM \" + tbl.table_name\n        tbl_len_query += \" \" + tbl.get_filter_query()\n        tbl_len_query += \";\"\n        tbl_len_df = pd.read_sql_query(tbl_len_query, tbl.con)\n        tbl_len = tbl_len_df.iloc[0][0]\n        tbl._tbl_len = tbl_len\n\n        # Calculate limit based on fraction of table length\n        limit = max(min(int(tbl_len * sample_frac), sample_cap), sample_start)\n\n        # Execute sample query\n        sample_query = \"SELECT * FROM \" + tbl.table_name\n        sample_query += \" \" + tbl.get_filter_query()\n        sample_query += \" LIMIT \" + str(limit) + \";\"\n        tbl._sampled = pd.read_sql_query(sample_query, tbl.con)\n        tbl._sampled_df_len = len(tbl._sampled)\n        tbl._sampled_len = tbl_len\n        tbl._sampled_query = sample_query\n        tbl._sampled = utils.pandas_to_lux(tbl._sampled)\n        tbl._sampled._intent = []\n        tbl._sampled._inferred_intent = []\n        tbl._sampled.table_name = tbl.table_name\n        tbl._sampled._original_df = tbl._sampled.copy()", "CodeBleu_score": 30.902730748262652}, {"code": "def execute_sampling(tbl: LuxSQLTable)\n\timport pandas as pd\n    import psycopg2\n    from psycopg2.extensions import ISOLATION_LEVEL_AUTOCOMMIT\n    from lux.utils import utils\n    from lux.utils.utils import get_filter_specs\n    import numpy as np\n    import warnings\n    warnings.filterwarnings('ignore')\n    import lux\n    import random\n    import string\n    import time\n    import psycopg2.extras as extras\n\n    SAMPLE_FLAG = lux.config.sampling\n    SAMPLE_START = lux.config.sampling_start\n    SAMPLE_CAP = lux.config.sampling_cap\n    SAMPLE_FRAC = lux.config.sampling_fraction\n\n    # Check if table has already been sampled\n    if hasattr(tbl, \"_sampled\"):\n        return\n\n    # Check if table has a primary key\n    if tbl.primary_key is None:\n        tbl._sampled = tbl.df\n        return\n\n    # Check if table is too small to be sampled\n    if tbl.size < SAMPLE_CAP:\n        tbl._sampled = tbl.df\n        return\n\n    # Check if sampling is disabled\n    if SAMPLE_FLAG == False:\n        tbl._sampled = tbl.df\n        return\n\n    # Check if sampling is enabled\n    if SAMPLE_FLAG == True:\n        # Check if sampling is enabled for this table\n        if tbl.name in SAMPLE_START:\n            # Check if sampling is enabled for this table and if it has already been sampled\n            if hasattr(tbl, \"_sampled\"):\n                return\n            # Check if table has a primary key\n            if tbl.primary_key is None:\n                tbl._sampled = tbl.df\n                return\n            # Check if table is too small to be sampled\n            if tbl.size < SAMPLE_CAP:\n                tbl._sampled = tbl.df\n                return\n            # Check if sampling is disabled for this table\n            if SAMPLE_START[tbl.name] == False:\n                tbl._sampled = tbl.df\n                return\n            # Check", "CodeBleu_score": 29.937865788507022}]}
{"instance_id": "485253b1088a", "generation_result": [{"code": "def draw_text(\n    params: Dict[str, dict],\n    background: BaseGeometry\n) -> None\n\t# override default parameters\n    text_params = override_params(default_text_params, params)\n\n    # extract text, x, and y values\n    text = text_params['text']\n    x = text_params['x']\n    y = text_params['y']\n\n    # get background bounds\n    x_min, x_max, y_min, y_max = background.bounds\n\n    # interpolate x and y values\n    x_interp = (x - x_min) / (x_max - x_min)\n    y_interp = (y - y_min) / (y_max - y_min)\n\n    # draw text\n    plt.text(x_interp, y_interp, text, **text_params)", "CodeBleu_score": 13.892957812588556}, {"code": "def draw_text(\n    params: Dict[str, dict],\n    background: BaseGeometry\n) -> None\n\ttext_params = override_params(\n        default_dict=default_text_params,\n        new_dict=params\n    )\n\n    text = text_params[\"text\"]\n    x = text_params[\"x\"]\n    y = text_params[\"y\"]\n    color = text_params[\"color\"]\n    font_size = text_params[\"font_size\"]\n    font_weight = text_params[\"font_weight\"]\n    font_family = text_params[\"font_family\"]\n    alpha = text_params[\"alpha\"]\n    horizontal_alignment = text_params[\"horizontal_alignment\"]\n    vertical_alignment = text_params[\"vertical_alignment\"]\n    rotation = text_params[\"rotation\"]\n\n    x_bounds = background.bounds[0]\n    y_bounds = background.bounds[1]\n\n    x_interp = interpolate.interp1d(x_bounds, [0, 1])\n    y_interp = interpolate.interp1d(y_bounds, [0, 1])\n\n    x_interp_value = x_interp(x)\n    y_interp_value = y_interp(y)\n\n    plt.text(\n        x_interp_value,\n        y_interp_value,\n        text,\n        color=color,\n        fontsize=font_size,\n        fontweight=font_weight,\n        fontfamily=font_family,\n        alpha=alpha,\n        horizontalalignment=horizontal_alignment,\n        verticalalignment=vertical_alignment,\n        rotation=rotation\n    )\n\n", "CodeBleu_score": 18.865613806242525}, {"code": "def draw_text(\n    params: Dict[str, dict],\n    background: BaseGeometry\n) -> None\n\t# Override default text settings with provided parameters\n    text_params = override_params(default_text_params, params)\n\n    # Extract text, x, and y values\n    text = text_params[\"text\"]\n    x = text_params[\"x\"]\n    y = text_params[\"y\"]\n\n    # Retrieve background bounds\n    background_bounds = background.bounds\n    x_min = background_bounds[0]\n    x_max = background_bounds[2]\n    y_min = background_bounds[1]\n    y_max = background_bounds[3]\n\n    # Interpolate x and y values to fit within background bounds\n    x = x_min + (x_max - x_min) * x\n    y = y_min + (y_max - y_min) * y\n\n    # Remove x and y values from text_params\n    del text_params[\"text\"]\n    del text_params[\"x\"]\n    del text_params[\"y\"]\n\n    # Draw text at calculated position with remaining parameters\n    plt.text(x, y, text, **text_params)\n\n", "CodeBleu_score": 17.504106674409353}]}
{"instance_id": "5f70a88b6b72", "generation_result": [{"code": "def _determine_format_string( dates, datetime_format=None )\n\timport numpy as np\n    from datetime import datetime\n    if datetime_format:\n        return datetime_format\n    else:\n        date_diffs = np.array([(datetime.strptime(dates[i+1], '%Y-%m-%d') - datetime.strptime(dates[i], '%Y-%m-%d')).days for i in range(len(dates)-1)])\n        if np.mean(date_diffs) < 0.33:\n            if np.max(date_diffs) > 1:\n                return '%b %d, %H:%M'\n            else:\n                return '%H:%M'\n        else:\n            if np.max(date_diffs) > 365:\n                return '%Y-%b-%d'\n            else:\n                return '%b %d'\n\n", "CodeBleu_score": 36.70171014343182}, {"code": "def _determine_format_string( dates, datetime_format=None )\n\tif datetime_format is not None:\n        return datetime_format\n\n    if len(dates) < 2:\n        return '%Y-%m-%d'\n\n    # Calculate the average number of days between consecutive dates\n    days = np.diff(dates)\n    avg_days = np.mean(days)\n\n    if avg_days < 0.33:\n        if np.max(days) > 1:\n            return '%b %d, %H:%M'\n        else:\n            return '%H:%M'\n    else:\n        if np.max(dates) - np.min(dates) > 365:\n            return '%Y-%m-%d'\n        else:\n            return '%b %d'\n\n", "CodeBleu_score": 32.23736530499552}, {"code": "def _determine_format_string( dates, datetime_format=None )\n\tif datetime_format is not None:\n        return datetime_format\n    days = []\n    for i in range( len( dates ) - 1 ):\n        days.append( ( dates[i+1] - dates[i] ).days )\n    avg_days = sum( days ) / len( days )\n    if avg_days < 0.33:\n        if dates[0].year != dates[-1].year:\n            return '%b %d, %H:%M'\n        else:\n            return '%H:%M'\n    else:\n        if dates[0].year != dates[-1].year:\n            return '%Y-%b-%d'\n        else:\n            return '%b %d'\n\n", "CodeBleu_score": 32.65968035461616}]}
{"instance_id": "f9589bf4666d", "generation_result": [{"code": "def _mpf_to_rgba(c, alpha=None)\n\tif _is_uint8_rgb_or_rgba(c):\n        c = [float(v) / 255 for v in c]\n    if alpha is not None:\n        c = list(c)\n        c.append(alpha)\n    return mpl.colors.to_rgba(c)\n\n", "CodeBleu_score": 26.107911426956647}, {"code": "def _mpf_to_rgba(c, alpha=None)\n\tif _is_uint8_rgb_or_rgba(c):\n        c = [v / 255.0 for v in c]\n    if alpha is not None:\n        c = list(c)\n        c.append(alpha)\n    return mpl.colors.to_rgba(c)\n\n", "CodeBleu_score": 24.53665595280982}, {"code": "def _mpf_to_rgba(c, alpha=None)\n\tif _is_uint8_rgb_or_rgba(c):\n        c = tuple(v/255.0 for v in c)\n    if alpha is not None:\n        c = c + (alpha,)\n    return mpl.colors.to_rgba(c)\n\n", "CodeBleu_score": 25.29807557939397}]}
{"instance_id": "4202e378ab0b", "generation_result": [{"code": "def subplots(self,*args,**kwargs)\n\tif 'style' in kwargs or not hasattr(self,'mpfstyle'):\n        style = _check_for_and_apply_style(kwargs)\n        self.mpfstyle = style\n    else:\n        style = _check_for_and_apply_style(dict(style=self.mpfstyle))\n\n    axlist = mplfigure.Figure.subplots(self,*args,**kwargs)\n\n    if isinstance(axlist,mpl_axes.Axes):\n        axlist.mpfstyle = style\n    elif isinstance(axlist,np.ndarray):\n        for ax in axlist.flatten():\n            ax.mpfstyle = style\n    else:\n       raise TypeError('Unexpected type ('+str(type(axlist))+') '+\n                       'returned from \"matplotlib.figure.Figure.subplots()\"')\n    return axlist", "CodeBleu_score": 91.3032669441917}, {"code": "def subplots(self,*args,**kwargs)\n\tif 'style' in kwargs or not hasattr(self,'mpfstyle'):\n        style = _check_for_and_apply_style(kwargs)\n        self.mpfstyle = style\n    else:\n        style = _check_for_and_apply_style(dict(style=self.mpfstyle))\n\n    axlist = mplfigure.Figure.subplots(self,*args,**kwargs)\n\n    if isinstance(axlist,mpl_axes.Axes):\n        axlist.mpfstyle = style\n    elif isinstance(axlist,np.ndarray):\n        for ax in axlist.flatten():\n            ax.mpfstyle = style\n    else:\n       raise TypeError('Unexpected type ('+str(type(axlist))+') '+\n                       'returned from \"matplotlib.figure.Figure.subplots()\"')\n    return axlist", "CodeBleu_score": 91.3032669441917}, {"code": "def subplots(self,*args,**kwargs)\n\tif 'style' in kwargs or not hasattr(self,'mpfstyle'):\n        style = _check_for_and_apply_style(kwargs)\n        self.mpfstyle = style\n    else:\n        style = _check_for_and_apply_style(dict(style=self.mpfstyle))\n\n    axlist = mplfigure.Figure.subplots(self,*args,**kwargs)\n\n    if isinstance(axlist,mpl_axes.Axes):\n        axlist.mpfstyle = style\n    elif isinstance(axlist,np.ndarray):\n        for ax in axlist.flatten():\n            ax.mpfstyle = style\n    else:\n       raise TypeError('Unexpected type ('+str(type(axlist))+') '+\n                       'returned from \"matplotlib.figure.Figure.subplots()\"')\n    return axlist\n\n", "CodeBleu_score": 91.3032669441917}]}
{"instance_id": "3e2b6dfe4753", "generation_result": [{"code": "def _adjust_figsize(fig,config)\n\tif fig is None:\n        return\n    if 'figsize' in config:\n        fig.set_size_inches(config['figsize'])\n    else:\n        if 'figsize_ratio' in config:\n            ratio = config['figsize_ratio']\n        else:\n            ratio = 1.6\n        width = fig.get_size_inches()[0]\n        height = width / ratio\n        if height > 10:\n            height = 10\n        if height < 3:\n            height = 3\n        fig.set_size_inches([width, height])\n", "CodeBleu_score": 30.676733767910534}, {"code": "def _adjust_figsize(fig,config)\n\tif fig is None:\n        return\n\n    if 'figure_size' in config:\n        figsize = config['figure_size']\n    else:\n        figsize = [10,10]\n        ratio = fig.get_figwidth() / fig.get_figheight()\n        if ratio > 1.5:\n            figsize[0] = 15\n        elif ratio < 0.7:\n            figsize[1] = 15\n\n    fig.set_size_inches(figsize)\n\n", "CodeBleu_score": 30.430858003460504}, {"code": "def _adjust_figsize(fig,config)\n\tif fig is None:\n        return\n    if 'figsize' in config:\n        fig.set_size_inches(config['figsize'])\n    else:\n        if 'ratio' in config:\n            ratio = config['ratio']\n        else:\n            ratio = 1.618\n        if 'width' in config:\n            width = config['width']\n        else:\n            width = 10\n        if 'height' in config:\n            height = config['height']\n        else:\n            height = 10\n        if 'min_width' in config:\n            min_width = config['min_width']\n        else:\n            min_width = 2\n        if 'max_width' in config:\n            max_width = config['max_width']\n        else:\n            max_width = 20\n        if 'min_height' in config:\n            min_height = config['min_height']\n        else:\n            min_height = 2\n        if 'max_height' in config:\n            max_height = config['max_height']\n        else:\n            max_height = 20\n        if width < min_width:\n            width = min_width\n        if width > max_width:\n            width = max_width\n        if height < min_height:\n            height = min_height\n        if height > max_height:\n            height = max_height\n        if ratio < 0.5:\n            ratio = 0.5\n        if ratio > 2.0:\n            ratio = 2.0\n        if width/height > ratio:\n            width = ratio * height\n        else:\n            height = width / ratio\n        fig.set_size_inches(width,height)\n\n\n", "CodeBleu_score": 26.555851493650668}]}
{"instance_id": "986e5804b941", "generation_result": [{"code": "def _plot_mav(ax,config,xdates,prices,apmav=None,apwidth=None)\n\tto me that there may be some or significant overlap\n    # TODO:  between what the following functions actually do:\n    # TODO:  At the very least, all four of them appear to communicate \n    # TODO:  to matplotlib that the xaxis should be treated as dates:\n    # TODO:   ->  'ax.autoscale_view()'\n    # TODO:   ->  'ax.xaxis_dates()'\n    # TODO:   ->  'plt.autofmt_xdates()'\n    # TODO:   ->  'fig.autofmt_xdate()'\n    # TODO: ================================================================\n    \n\n    #if config['autofmt_xdate']:\n        #print('CALLING fig.autofmt_xdate()')\n        #fig.autofmt_xdate()\n\n    axA1.autoscale_view()  # Is this really necessary??\n                           # It appears to me, based on experience coding types 'ohlc' and 'candle'\n                           # for `addplot`, that this IS necessary when the only thing done to the\n                           # the axes is .add_collection().  (However, if ax.plot() .scatter() or\n                           # .bar() was called, then possibly this is not necessary; not entirely\n                           # sure, but it definitely was necessary to get 'ohlc' and 'candle' \n                           # working in `addplot`).\n\n    axA1.set_ylabel(config['ylabel'])\n\n    if config['volume']:\n        if external_axes_mode:\n            volumeAxes.tick_params(axis='x',rotation=xrotation)\n            volumeAxes.xaxis.set_major_formatter(formatter)\n\n        vscale = 'linear'\n        ysd = config['volume_yscale']\n        if isinstance(ysd,dict):\n            yscale = ysd['yscale']\n            del      ysd['yscale']\n            volumeAxes.set_yscale(yscale,**ysd)\n            vscale = yscale\n        elif isinstance(ysd,str):\n            volumeAxes.set_yscale(ysd)\n            vscale = ysd\n        offset = ''\n        if vscale == 'linear':\n            vxp = config['volume_exponent']\n            if vxp == 'legacy':\n                volumeAxes.figure.canvas.draw()  # This is needed to calculate offset\n                offset = volumeAxes.yaxis.get_major_formatter().get_offset()\n                if len(offset) > 0:\n                    offset = (' x '+offset)\n            elif isinstance(vxp,int) and vxp > 0:\n                volumeAxes.ticklabel_format(useOffset=False,scilimits=(vxp,vxp),axis='y')\n                offset = '  $10^{'+str(vxp)+'}$'\n            elif isinstance(vxp,int) and vxp == 0:\n                volumeAxes.ticklabel_format(useOffset=False,style='plain',axis='y')\n                offset = ''\n            else:\n                offset = ''\n                scilims = plt.rcParams['axes.formatter.limits']\n                if scilims[0] < scilims[1]:\n                    for power in (5,4,3,2,1):\n                        xp = scilims[1]*power\n                        if vymax >= 10.**xp:\n                            volumeAxes.ticklabel_format(useOffset=False,scilimits=(xp,xp),axis='y')\n                            offset = '  $10^{'+str(xp)+'}$'\n                            break\n                elif scilims[0] == scilims[1] and scilims[1] != 0:\n                    volumeAxes.ticklabel_format(useOffset=False,scilimits=scilims,axis='y')\n                    offset = ' $10^'+str(scilims[1])+'$'\n            volumeAxes.yaxis.offsetText.set_visible(False)\n\n        if config['ylabel_lower'] is None:\n            vol_label = 'Volume'+offset\n        else:\n            if len(offset) > 0:\n                offset = '", "CodeBleu_score": 29.60059685514193}, {"code": "def _plot_mav(ax,config,xdates,prices,apmav=None,apwidth=None)\n\tto me that there may be some or significant overlap\n    # TODO:  between what the following functions actually do:\n    # TODO:  At the very least, all four of them appear to communicate \n    # TODO:  to matplotlib that the xaxis should be treated as dates:\n    # TODO:   ->  'ax.autoscale_view()'\n    # TODO:   ->  'ax.xaxis_dates()'\n    # TODO:   ->  'plt.autofmt_xdates()'\n    # TODO:   ->  'fig.autofmt_xdate()'\n    # TODO: ================================================================\n    \n\n    #if config['autofmt_xdate']:\n        #print('CALLING fig.autofmt_xdate()')\n        #fig.autofmt_xdate()\n\n    axA1.autoscale_view()  # Is this really necessary??\n                           # It appears to me, based on experience coding types 'ohlc' and 'candle'\n                           # for `addplot`, that this IS necessary when the only thing done to the\n                           # the axes is .add_collection().  (However, if ax.plot() .scatter() or\n                           # .bar() was called, then possibly this is not necessary; not entirely\n                           # sure, but it definitely was necessary to get 'ohlc' and 'candle' \n                           # working in `addplot`).\n\n    axA1.set_ylabel(config['ylabel'])\n\n    if config['volume']:\n        if external_axes_mode:\n            volumeAxes.tick_params(axis='x',rotation=xrotation)\n            volumeAxes.xaxis.set_major_formatter(formatter)\n\n        vscale = 'linear'\n        ysd = config['volume_yscale']\n        if isinstance(ysd,dict):\n            yscale = ysd['yscale']\n            del      ysd['yscale']\n            volumeAxes.set_yscale(yscale,**ysd)\n            vscale = yscale\n        elif isinstance(ysd,str):\n            volumeAxes.set_yscale(ysd)\n            vscale = ysd\n        offset = ''\n        if vscale == 'linear':\n            vxp = config['volume_exponent']\n            if vxp == 'legacy':\n                volumeAxes.figure.canvas.draw()  # This is needed to calculate offset\n                offset = volumeAxes.yaxis.get_major_formatter().get_offset()\n                if len(offset) > 0:\n                    offset = (' x '+offset)\n            elif isinstance(vxp,int) and vxp > 0:\n                volumeAxes.ticklabel_format(useOffset=False,scilimits=(vxp,vxp),axis='y')\n                offset = '  $10^{'+str(vxp)+'}$'\n            elif isinstance(vxp,int) and vxp == 0:\n                volumeAxes.ticklabel_format(useOffset=False,style='plain',axis='y')\n                offset = ''\n            else:\n                offset = ''\n                scilims = plt.rcParams['axes.formatter.limits']\n                if scilims[0] < scilims[1]:\n                    for power in (5,4,3,2,1):\n                        xp = scilims[1]*power\n                        if vymax >= 10.**xp:\n                            volumeAxes.ticklabel_format(useOffset=False,scilimits=(xp,xp),axis='y')\n                            offset = '  $10^{'+str(xp)+'}$'\n                            break\n                elif scilims[0] == scilims[1] and scilims[1] != 0:\n                    volumeAxes.ticklabel_format(useOffset=False,scilimits=scilims,axis='y')\n                    offset = ' $10^'+str(scilims[1])+'$'\n            volumeAxes.yaxis.offsetText.set_visible(False)\n\n        if config['ylabel_lower'] is None:\n            vol_label = 'Volume'+offset\n        else:\n            if len(offset) > 0:\n                offset = '", "CodeBleu_score": 29.60059685514193}, {"code": "def _plot_mav(ax,config,xdates,prices,apmav=None,apwidth=None)\n\tto me that there may be some or significant overlap\n    # TODO:  between what the following functions actually do:\n    # TODO:  At the very least, all four of them appear to communicate \n    # TODO:  to matplotlib that the xaxis should be treated as dates:\n    # TODO:   ->  'ax.autoscale_view()'\n    # TODO:   ->  'ax.xaxis_dates()'\n    # TODO:   ->  'plt.autofmt_xdates()'\n    # TODO:   ->  'fig.autofmt_xdate()'\n    # TODO: ================================================================\n    \n\n    #if config['autofmt_xdate']:\n        #print('CALLING fig.autofmt_xdate()')\n        #fig.autofmt_xdate()\n\n    axA1.autoscale_view()  # Is this really necessary??\n                           # It appears to me, based on experience coding types 'ohlc' and 'candle'\n                           # for `addplot`, that this IS necessary when the only thing done to the\n                           # the axes is .add_collection().  (However, if ax.plot() .scatter() or\n                           # .bar() was called, then possibly this is not necessary; not entirely\n                           # sure, but it definitely was necessary to get 'ohlc' and 'candle' \n                           # working in `addplot`).\n\n    axA1.set_ylabel(config['ylabel'])\n\n    if config['volume']:\n        if external_axes_mode:\n            volumeAxes.tick_params(axis='x',rotation=xrotation)\n            volumeAxes.xaxis.set_major_formatter(formatter)\n\n        vscale = 'linear'\n        ysd = config['volume_yscale']\n        if isinstance(ysd,dict):\n            yscale = ysd['yscale']\n            del      ysd['yscale']\n            volumeAxes.set_yscale(yscale,**ysd)\n            vscale = yscale\n        elif isinstance(ysd,str):\n            volumeAxes.set_yscale(ysd)\n            vscale = ysd\n        offset = ''\n        if vscale == 'linear':\n            vxp = config['volume_exponent']\n            if vxp == 'legacy':\n                volumeAxes.figure.canvas.draw()  # This is needed to calculate offset\n                offset = volumeAxes.yaxis.get_major_formatter().get_offset()\n                if len(offset) > 0:\n                    offset = (' x '+offset)\n            elif isinstance(vxp,int) and vxp > 0:\n                volumeAxes.ticklabel_format(useOffset=False,scilimits=(vxp,vxp),axis='y')\n                offset = '  $10^{'+str(vxp)+'}$'\n            elif isinstance(vxp,int) and vxp == 0:\n                volumeAxes.ticklabel_format(useOffset=False,style='plain',axis='y')\n                offset = ''\n            else:\n                offset = ''\n                scilims = plt.rcParams['axes.formatter.limits']\n                if scilims[0] < scilims[1]:\n                    for power in (5,4,3,2,1):\n                        xp = scilims[1]*power\n                        if vymax >= 10.**xp:\n                            volumeAxes.ticklabel_format(useOffset=False,scilimits=(xp,xp),axis='y')\n                            offset = '  $10^{'+str(xp)+'}$'\n                            break\n                elif scilims[0] == scilims[1] and scilims[1] != 0:\n                    volumeAxes.ticklabel_format(useOffset=False,scilimits=scilims,axis='y')\n                    offset = ' $10^'+str(scilims[1])+'$'\n            volumeAxes.yaxis.offsetText.set_visible(False)\n\n        if config['ylabel_lower'] is None:\n            vol_label = 'Volume'+offset\n        else:\n            if len(offset) > 0:\n                offset = '", "CodeBleu_score": 29.60059685514193}]}
{"instance_id": "5308e20f3c90", "generation_result": [{"code": "def flops_conv(opnode: ConvolutionForward, inputs, outputs)\n\t# Extract the kernel height and width from the input tensor's shape\n    kernel_height = inputs[0].shape[2]\n    kernel_width = inputs[0].shape[3]\n    # Determine the number of input channels\n    num_input_channels = inputs[0].shape[1]\n    # Calculate the product of the output tensor's dimensions\n    num_output_elements = np.prod(outputs[0].shape)\n    # Check if the operation includes a bias term\n    if len(inputs) == 3:\n        # Compute the FLOPs by multiplying the total number of output elements by the number of input channels and the kernel size, adding the bias if present\n        flops = num_output_elements * num_input_channels * (kernel_height * kernel_width + 1)\n    else:\n        # Compute the FLOPs by multiplying the total number of output elements by the number of input channels and the kernel size\n        flops = num_output_elements * num_input_channels * (kernel_height * kernel_width)\n    # Return the computed FLOPs\n    return flops\n\n", "CodeBleu_score": 20.397950754185064}, {"code": "def flops_conv(opnode: ConvolutionForward, inputs, outputs)\n\tin_shape = inputs[0].shape\n    out_shape = outputs[0].shape\n    kernel_h = in_shape[2]\n    kernel_w = in_shape[3]\n    in_channels = in_shape[1]\n    out_channels = out_shape[1]\n    out_elements = np.prod(out_shape[1:])\n    flops = out_elements * (in_channels * kernel_h * kernel_w + 1 if opnode.has_bias else in_channels * kernel_h * kernel_w)\n    return flops\n", "CodeBleu_score": 21.008148129133435}, {"code": "def flops_conv(opnode: ConvolutionForward, inputs, outputs)\n\tkernel_h = opnode.kernel_shape[0]\n    kernel_w = opnode.kernel_shape[1]\n    num_input_channels = inputs[0].shape[1]\n    num_output_elements = np.prod(outputs[0].shape)\n    num_output_channels = outputs[0].shape[1]\n    flops = num_output_elements * num_input_channels * kernel_h * kernel_w\n    if opnode.bias_term:\n        flops += num_output_elements * num_output_channels\n    return flops\n\n", "CodeBleu_score": 19.22577525474652}]}
{"instance_id": "70811d3cf578", "generation_result": [{"code": "def mge_dtype_to_ir_type(mge_dtype)\n\timport numpy as np\n    import tvm\n\n    mge_dtype = np.dtype(mge_dtype)\n    assert isinstance(mge_dtype, np.dtype)\n    return _dtype_to_ir_type[mge_dtype](mge_dtype)\n\n", "CodeBleu_score": 42.18215800411124}, {"code": "def mge_dtype_to_ir_type(mge_dtype)\n\tmge_dtype = np.dtype(mge_dtype)\n    assert isinstance(mge_dtype, np.dtype), \"mge_dtype must be a NumPy data type\"\n    return _dtype_to_ir_type[mge_dtype](mge_dtype)\n\n", "CodeBleu_score": 24.244610243826497}, {"code": "def mge_dtype_to_ir_type(mge_dtype)\n\tmge_dtype = np.dtype(mge_dtype)\n    assert isinstance(mge_dtype, np.dtype)\n    return _dtype_to_ir_type[mge_dtype](mge_dtype)\n\n", "CodeBleu_score": 20.773363693926143}]}
{"instance_id": "a108c9ba2c22", "generation_result": [{"code": "def _numpy_array_constant(x: np.ndarray, canonicalize_types) -> Sequence[ir.Value]\n\tif canonicalize_types:\n        x = np.ascontiguousarray(x, np.dtype(np.float32))\n    else:\n        x = np.ascontiguousarray(x)\n    if x.dtype == np.bool_:\n        x = x.astype(np.uint8)\n    if x.dtype == np.bfloat16:\n        x = x.astype(np.float32)\n    return ir.ConstantOp(\n        ir.DenseElementsAttr.get(x.reshape(-1), mge_dtype_to_ir_type(x.dtype))\n    ).results\n\n", "CodeBleu_score": 37.50064477162074}, {"code": "def _numpy_array_constant(x: np.ndarray, canonicalize_types) -> Sequence[ir.Value]\n\tif canonicalize_types:\n        x = np.ascontiguousarray(x)\n        x = np.canonicalize_dtype(x)\n    if x.dtype == np.bool_:\n        x = x.astype(np.uint8)\n    if x.dtype == np.bfloat16:\n        x = x.astype(np.float32)\n    return [\n        hlo_builder.ConstOp(\n            ir.RankedTensorType.get(x.shape, mge_dtype_to_ir_type(x.dtype)),\n            ir.DenseElementsAttr.get(x),\n        )\n    ]\n\n", "CodeBleu_score": 37.89866544556406}, {"code": "def _numpy_array_constant(x: np.ndarray, canonicalize_types) -> Sequence[ir.Value]\n\tif canonicalize_types:\n        x = np.array(x, copy=False)\n        x = np.ascontiguousarray(x)\n        x = np.canonicalize_to_dtype(x, np.float32)\n\n    if x.dtype == np.bool_:\n        x = x.astype(np.uint8)\n    elif x.dtype == np.bfloat16:\n        x = x.astype(np.float32)\n    ir_type = mge_dtype_to_ir_type(x.dtype)\n    dense_attr = ir.DenseElementsAttr.get(x.flatten(), type=ir_type)\n    return [\n        hcl.make_constant_op(\n            dense_attr, ir.RankedTensorType.get(x.shape, ir_type)\n        ).result\n    ]\n\n", "CodeBleu_score": 50.10485590450835}]}
{"instance_id": "3345fe4d008b", "generation_result": [{"code": "def _ndarray_constant_handler(\n    val: np.ndarray, canonicalize_types\n) -> Sequence[ir.Value]\n\tif val.size == 0:\n        return _numpy_array_constant(val, canonicalize_types)\n    zero_stride_axes, nonzero_stride_axes = _get_zero_and_nonzero_stride_axes(val)\n    if not zero_stride_axes:\n        return _numpy_array_constant(val, canonicalize_types)\n    collapsed_val = val.reshape(\n        [val.shape[axis] for axis in nonzero_stride_axes]\n        + [-1]\n        * len(zero_stride_axes)\n    )\n    collapsed_val = collapsed_val.reshape(\n        [np.prod(collapsed_val.shape[: len(nonzero_stride_axes)])]\n        + [collapsed_val.shape[-1]]\n    )\n    collapsed_val = collapsed_val.reshape(\n        [collapsed_val.shape[0]] + [1] * len(zero_stride_axes)\n    )\n    collapsed_val = np.broadcast_to(\n        collapsed_val,\n        [val.shape[axis] for axis in nonzero_stride_axes]\n        + [val.shape[axis] for axis in zero_stride_axes],\n    )\n    return _numpy_array_constant(collapsed_val, canonicalize_types)\n\n", "CodeBleu_score": 37.22572417930073}, {"code": "def _ndarray_constant_handler(\n    val: np.ndarray, canonicalize_types\n) -> Sequence[ir.Value]\n\tif np.any(np.equal(0, val.strides)) and val.size != 0:\n        zero_stride_axes = np.equal(0, val.strides)\n        non_zero_stride_axes = np.logical_not(zero_stride_axes)\n        # Collapse the array along the zero-stride axes.\n        collapsed_val = val.reshape(\n            np.delete(val.shape, zero_stride_axes)\n        )\n        # Optionally canonicalize the data type.\n        if canonicalize_types:\n            collapsed_val = np.asarray(\n                collapsed_val, dtype.canonicalize_dtype(collapsed_val.dtype)\n            )\n        # Create a broadcast operation for the collapsed array, matching the original shape and non-zero-stride axes.\n        return (\n            hlo.BroadcastInDimOp(\n                _numpy_array_constant(collapsed_val, canonicalize_types)[0],\n                dense_int_elements(val.shape),\n                dense_int_elements(np.nonzero(non_zero_stride_axes)[0]),\n            ).result,\n        )\n    return _numpy_array_constant(val, canonicalize_types)\n\n", "CodeBleu_score": 47.67865499889709}, {"code": "def _ndarray_constant_handler(\n    val: np.ndarray, canonicalize_types\n) -> Sequence[ir.Value]\n\tif val.size == 0:\n        return _numpy_array_constant(val, canonicalize_types)\n    zero_stride_axes, nonzero_stride_axes = _find_zero_stride_axes(val)\n    if zero_stride_axes:\n        collapsed_val = _collapse_zero_stride_axes(val, zero_stride_axes)\n        return _numpy_array_constant(collapsed_val, canonicalize_types) + _create_broadcast_op(\n            collapsed_val.shape, val.shape, nonzero_stride_axes\n        )\n    return _numpy_array_constant(val, canonicalize_types)\n", "CodeBleu_score": 22.29532484059615}]}
{"instance_id": "91b298678351", "generation_result": [{"code": "def dense_bool_elements(xs: Sequence[bool]) -> ir.DenseElementsAttr\n\treturn ir.DenseElementsAttr.get(np.array(xs, dtype=np.int8),\n                                    ir.IntegerType.get_signless(1))\n\n", "CodeBleu_score": 18.507487005780305}, {"code": "def dense_bool_elements(xs: Sequence[bool]) -> ir.DenseElementsAttr\n\treturn ir.DenseElementsAttr.get(np.array(xs, dtype=np.uint8), ir.IntegerType.get_signless(1))\n\n", "CodeBleu_score": 14.779166950758343}, {"code": "def dense_bool_elements(xs: Sequence[bool]) -> ir.DenseElementsAttr\n\t...\n\n", "CodeBleu_score": 7.211538461538461}]}
{"instance_id": "06ebabf1e0ff", "generation_result": [{"code": "def create_tensor_lower(ctx, *args: Union[HLOTensor, Sequence[HLOTensor]])\n\tassert len(args) == 1, \"create_tensor_lower only accepts one argument\"\n    assert len(ctx.vars_in) == 1, \"create_tensor_lower only accepts one argument\"\n    assert len(ctx.vars_out) == 1, \"create_tensor_lower only accepts one argument\"\n    in_var = ctx.vars_in[0]\n    out_var = ctx.vars_out[0]\n    if in_var.bound:\n        out_var.set_data(in_var.data)\n    elif out_var.bound:\n        in_var.set_data(out_var.data)\n    else:\n        raise AssertionError(\"create_tensor_lower only accepts tensor creation from constants\")\n    assert in_var.shape == out_var.shape, \"Input and output shapes must match\"\n    data = out_var.data.asnumpy()\n    return HLOTensor(data, in_var.shape, in_var.dtype)\n\n", "CodeBleu_score": 37.40418939031181}, {"code": "def create_tensor_lower(ctx, *args: Union[HLOTensor, Sequence[HLOTensor]])\n\tassert len(args) == 1 and len(ctx.vars_in) == 1 and len(ctx.vars_out) == 1\n    in_var, out_var = ctx.vars_in[0], ctx.vars_out[0]\n    if in_var.bound_data is not None:\n        out_var.bound_data = in_var.bound_data\n    elif out_var.bound_data is not None:\n        in_var.bound_data = out_var.bound_data\n    else:\n        raise AssertionError(\"Only tensor creation from constant is supported\")\n    assert in_var.shape == out_var.shape\n    data = out_var.bound_data.asnumpy().astype(out_var.dtype)\n    return HLOTensor(data)\n\n", "CodeBleu_score": 51.33932673864776}, {"code": "def create_tensor_lower(ctx, *args: Union[HLOTensor, Sequence[HLOTensor]])\n\tassert len(args) == 1\n    assert len(ctx.vars_in) == 1\n    assert len(ctx.vars_out) == 1\n    in_var = ctx.vars_in[0]\n    out_var = ctx.vars_out[0]\n    if in_var.bound_data is not None:\n        out_var.bound_data = in_var.bound_data\n    elif out_var.bound_data is not None:\n        in_var.bound_data = out_var.bound_data\n    else:\n        raise AssertionError(\"Only tensor creation from constants is supported.\")\n    assert in_var.shape == out_var.shape\n    data = out_var.bound_data.asnumpy().astype(out_var.dtype)\n    return HLOTensor(data, out_var.shape)\n\n", "CodeBleu_score": 46.7795869352811}]}
{"instance_id": "6cd750fa0c8d", "generation_result": [{"code": "def _device_put_scalar(x, device)\n\t# TODO: add type checking\n    if device is None:\n        return x\n    elif device == 'cpu':\n        return x\n    else:\n        return x\n\n", "CodeBleu_score": 10.659703004068989}, {"code": "def _device_put_scalar(x, device)\n\tx = _numpy_array_from_scalar(x)\n    return _device_put_array(x, device)\n\n", "CodeBleu_score": 7.656571152652836}, {"code": "def _device_put_scalar(x, device)\n\tif not isinstance(x, np.ndarray):\n        x = np.array(x)\n    if device is not None:\n        x = _device_put(x, device)\n    return x\n\n", "CodeBleu_score": 27.15734359253822}]}
{"instance_id": "140de7c2b6a8", "generation_result": [{"code": "def _shape_equal(lhs_shape, rhs_shape)\n\t# Convert to lists if necessary\n    if isinstance(lhs_shape, np.ndarray):\n        lhs_shape = lhs_shape.tolist()\n    if isinstance(rhs_shape, np.ndarray):\n        rhs_shape = rhs_shape.tolist()\n    # Check that both shapes are tuples or lists\n    assert isinstance(lhs_shape, tuple) or isinstance(lhs_shape, list), 'lhs_shape must be a tuple or a list'\n    assert isinstance(rhs_shape, tuple) or isinstance(rhs_shape, list), 'rhs_shape must be a tuple or a list'\n    # Check that both shapes are not empty\n    if len(lhs_shape) == 0 and len(rhs_shape) == 0:\n        return True\n    # Check that the first elements of both shapes are integers\n    assert isinstance(lhs_shape[0], int), 'The first element of lhs_shape must be an integer'\n    assert isinstance(rhs_shape[0], int), 'The first element of rhs_shape must be an integer'\n    # Check that the shapes have the same length\n    if len(lhs_shape) != len(rhs_shape):\n        return False\n    # Check that the corresponding elements of both shapes are equal\n    for i in range(len(lhs_shape)):\n        if lhs_shape[i] != rhs_shape[i]:\n            return False\n    return True\n\n", "CodeBleu_score": 50.824545300823}, {"code": "def _shape_equal(lhs_shape, rhs_shape)\n\tif isinstance(lhs_shape, np.ndarray):\n        lhs_shape = lhs_shape.tolist()\n    if isinstance(rhs_shape, np.ndarray):\n        rhs_shape = rhs_shape.tolist()\n    assert isinstance(lhs_shape, (tuple, list))\n    assert isinstance(rhs_shape, (tuple, list))\n    if len(lhs_shape) == 0 and len(rhs_shape) == 0:\n        return True\n    if len(lhs_shape) == 0 or len(rhs_shape) == 0:\n        return False\n    if not isinstance(lhs_shape[0], int) or not isinstance(rhs_shape[0], int):\n        return False\n    if len(lhs_shape) != len(rhs_shape):\n        return False\n    for lhs_dim, rhs_dim in zip(lhs_shape, rhs_shape):\n        if lhs_dim != rhs_dim:\n            return False\n    return True\n\n", "CodeBleu_score": 53.725046197076985}, {"code": "def _shape_equal(lhs_shape, rhs_shape)\n\tif isinstance(lhs_shape, np.ndarray):\n        lhs_shape = lhs_shape.tolist()\n    if isinstance(rhs_shape, np.ndarray):\n        rhs_shape = rhs_shape.tolist()\n    assert isinstance(lhs_shape, (tuple, list)), \"lhs_shape is not a tuple or a list\"\n    assert isinstance(rhs_shape, (tuple, list)), \"rhs_shape is not a tuple or a list\"\n    if len(lhs_shape) == 0 and len(rhs_shape) == 0:\n        return True\n    assert isinstance(lhs_shape[0], int), \"lhs_shape[0] is not an integer\"\n    assert isinstance(rhs_shape[0], int), \"rhs_shape[0] is not an integer\"\n    if len(lhs_shape) != len(rhs_shape):\n        return False\n    for i in range(len(lhs_shape)):\n        if lhs_shape[i] != rhs_shape[i]:\n            return False\n    return True\n\n", "CodeBleu_score": 50.635260230159716}]}
{"instance_id": "55d0c71a259b", "generation_result": [{"code": "def read(\n    training_set_filename,\n    test_set_filename,\n    training_set_metadata_filename,\n    test_set_metadata_filename,\n    dtypes,\n    meta_dtypes,\n)\n\t# Read training set\n    training_set = pd.read_csv(training_set_filename, dtype=dtypes)\n\n    # Read test set\n    test_set = pd.read_csv(test_set_filename, dtype=dtypes)\n\n    # Read training set metadata\n    training_set_metadata = pd.read_csv(training_set_metadata_filename, dtype=meta_dtypes)\n\n    # Read test set metadata\n    test_set_metadata = pd.read_csv(test_set_metadata_filename, dtype=meta_dtypes)\n\n    # Return tuple of dataframes\n    return (training_set, test_set, training_set_metadata, test_set_metadata)\n\n", "CodeBleu_score": 25.869871342160955}, {"code": "def read(\n    training_set_filename,\n    test_set_filename,\n    training_set_metadata_filename,\n    test_set_metadata_filename,\n    dtypes,\n    meta_dtypes,\n)\n\t# Read training data\n    training_set = pd.read_csv(\n        training_set_filename,\n        dtype=dtypes,\n        parse_dates=[\"date\"],\n        skipinitialspace=True,\n        skiprows=1,\n    )\n    # Read test data\n    test_set = pd.read_csv(\n        test_set_filename,\n        dtype=dtypes,\n        parse_dates=[\"date\"],\n        skipinitialspace=True,\n        skiprows=1,\n    )\n    # Read training metadata\n    training_set_metadata = pd.read_csv(\n        training_set_metadata_filename,\n        dtype=meta_dtypes,\n        parse_dates=[\"date\"],\n        skipinitialspace=True,\n        skiprows=1,\n    )\n    # Read test metadata\n    meta_dtypes[\"target\"] = str\n    test_set_metadata = pd.read_csv(\n        test_set_metadata_filename,\n        dtype=meta_dtypes,\n        parse_dates=[\"date\"],\n        skipinitialspace=True,\n        skiprows=1,\n    )\n    meta_dtypes[\"target\"] = np.int64\n    return training_set, test_set, training_set_metadata, test_set_metadata\n\n", "CodeBleu_score": 24.446436161423815}, {"code": "def read(\n    training_set_filename,\n    test_set_filename,\n    training_set_metadata_filename,\n    test_set_metadata_filename,\n    dtypes,\n    meta_dtypes,\n)\n\t# Read training set into a pandas DataFrame\n    training_set = pd.read_csv(training_set_filename, dtype=dtypes)\n\n    # Read test set into a pandas DataFrame\n    test_set = pd.read_csv(test_set_filename, dtype=dtypes)\n\n    # Read training metadata into a pandas DataFrame\n    training_set_metadata = pd.read_csv(\n        training_set_metadata_filename, dtype=meta_dtypes\n    )\n\n    # Remove 'target' from the metadata dictionary\n    test_set_metadata_dtypes = meta_dtypes.copy()\n    test_set_metadata_dtypes.pop(\"target\")\n\n    # Read test metadata into a pandas DataFrame\n    test_set_metadata = pd.read_csv(\n        test_set_metadata_filename, dtype=test_set_metadata_dtypes\n    )\n\n    # Add 'target' back into the metadata dictionary\n    test_set_metadata_dtypes[\"target\"] = meta_dtypes[\"target\"]\n\n    return training_set, test_set, training_set_metadata, test_set_metadata\n\n\n", "CodeBleu_score": 41.673417412021855}]}
{"instance_id": "9e5e02cd73e0", "generation_result": [{"code": "def _show_cmap(cmap)\n\tfrom .rcmod import axes_style\n    axes_style(\"white\")\n    fig, ax = plt.subplots(figsize=(8, 1))\n    ax.set_axis_off()\n    x = np.linspace(0, 1, 256)\n    x = np.repeat(x, 10).reshape(10, 256)\n    ax.pcolormesh(x, cmap=cmap)\n    plt.show()\n\n", "CodeBleu_score": 56.45972802740978}, {"code": "def _show_cmap(cmap)\n\tfrom rcmod import axes_style\n    axes_style(\"white\")\n    fig, ax = plt.subplots(figsize=(10, 2))\n    ax.set_xticks([])\n    ax.set_yticks([])\n    x = np.linspace(0, 1, 200)\n    y = np.linspace(0, 1, 200)\n    X, Y = np.meshgrid(x, y)\n    ax.pcolormesh(X, Y, cmap(X))\n    plt.show()\n\n", "CodeBleu_score": 49.64892594741337}, {"code": "def _show_cmap(cmap)\n\tfrom rcmod import axes_style\n    import matplotlib.pyplot as plt\n    with axes_style('white'):\n        fig, ax = plt.subplots(figsize=(10, 2))\n        ax.pcolormesh(np.linspace(0, 1, 100), np.linspace(0, 1, 100), np.linspace(0, 1, 10000).reshape(100, 100), cmap=cmap)\n        ax.set_xticks([])\n        ax.set_yticks([])\n\n", "CodeBleu_score": 39.03432999516711}]}
{"instance_id": "ad0781b5cc38", "generation_result": [{"code": "def transform_to_data_coordinates(obj, xdata, ydata)\n\tif obj.axes is None:\n        return xdata, ydata\n    else:\n        xydata = np.column_stack([xdata, ydata])\n        trans = obj.axes.transData.inverted()\n        if obj.get_transform() != obj.axes.transData:\n            trans = obj.get_transform() + trans\n        return trans.transform(xydata).T\n\n", "CodeBleu_score": 53.46017000855588}, {"code": "def transform_to_data_coordinates(obj, xdata, ydata)\n\timport numpy as np\n    import matplotlib.transforms as transforms\n    if obj.axes is None:\n        raise RuntimeError('Object does not have a parent axes.')\n    if obj.axes.transData.transform(np.array([[0, 0]])) == np.array([[0, 0]]):\n        return xdata, ydata\n    else:\n        data_transform = obj.axes.transData.inverted().transform\n        return data_transform(np.array([xdata, ydata]))\n\n", "CodeBleu_score": 42.74417628412102}, {"code": "def transform_to_data_coordinates(obj, xdata, ydata)\n\tif obj.axes is None:\n        return xdata, ydata\n    else:\n        xdata, ydata = obj.axes.transData.transform(np.vstack([xdata, ydata]).T).T\n        xydata = np.vstack([xdata, ydata]).T\n        inv = obj.axes.transData.inverted()\n        return inv.transform(xydata).T\n\n", "CodeBleu_score": 51.20251342752348}]}
{"instance_id": "bf776cadef8d", "generation_result": [{"code": "def kdeplot(self, data=None, data2=None, *args, **kwargs)\n\tdata = self._maybe_target_series(data, key='data')\n\n    if data2 is not None:\n        if not pd.api.types.is_list_like(data2):\n            data2 = self._df[data2]\n    return self._module.kdeplot(data, data2=data2, *args, **kwargs)\n\n", "CodeBleu_score": 78.44847260741699}, {"code": "def kdeplot(self, data=None, data2=None, *args, **kwargs)\n\tdata = self._maybe_target_series(data, key='data')\n\n    if data2 is not None:\n        if not pd.api.types.is_list_like(data2):\n            data2 = self._df[data2]\n    return self._module.kdeplot(data, data2=data2, *args, **kwargs)\n\n", "CodeBleu_score": 78.44847260741699}, {"code": "def kdeplot(self, data=None, data2=None, *args, **kwargs)\n\tdata = self._maybe_target_series(data, key='data')\n\n    if data2 is not None:\n        if not pd.api.types.is_list_like(data2):\n            data2 = self._df[data2]\n    return self._module.kdeplot(data, data2=data2, *args, **kwargs)", "CodeBleu_score": 78.44847260741699}]}
{"instance_id": "fb44964174d9", "generation_result": [{"code": "def plot_most_used_words(df)\n\tdf['token'] = df['token'].replace('programa', 'programar')\n    df = df.loc[df['token'].str.isalpha() & df['token'].str.len() > 1 & df['token'].isin(stopwords.words('portuguese')) == False]\n    df = df.groupby('token').size().sort_values(ascending=False).reset_index()\n    df = df.head(20)\n    plt.figure(figsize=(12, 8))\n    sns.barplot(x='token', y=0, data=df)\n    plt.title('Most used words')\n    plt.xlabel('Word')\n    plt.ylabel('Frequency')\n    plt.savefig('most_used_words.png')\n", "CodeBleu_score": 36.03480073861996}, {"code": "def plot_most_used_words(df)\n\tdf['word'] = df['word'].apply(lambda x: 'programar' if x == 'programa' else x)\n    df = df[df['is_alpha'] & (df['is_stop'] == False) & (df['len'] > 1)]\n    top_20 = df['word'].value_counts()[:20]\n    top_20.plot.bar(figsize=(10, 10), title='Most used words')\n    plt.xlabel('Word')\n    plt.ylabel('Frequency')\n    plt.savefig('../output/most_used_words.png')\n\n", "CodeBleu_score": 24.006005524001168}, {"code": "def plot_most_used_words(df)\n\tdf['token'] = df['token'].replace('programa', 'programar')\n    df = df[(df['token'].str.isalpha()) & (df['token'].str.len() > 1) & (~df['token'].isin(stop_words))]\n    df = df.groupby('token').size().sort_values(ascending=False).head(20)\n    df.plot(kind='bar', title='Most Frequent Words', figsize=(10, 6))\n    plt.xlabel('Words')\n    plt.ylabel('Frequency')\n    plt.savefig('most_frequent_words.png')\n\n", "CodeBleu_score": 35.519139634381034}]}
{"instance_id": "13414a7eb4e3", "generation_result": [{"code": "def dask_rolling_wrapper(moving_func, a, window, min_count=None, axis=-1)\n\timport dask.array as da\n    import numpy as np\n    import bottleneck as bn\n\n    # promote data type to handle edge cases\n    if np.issubdtype(a.dtype, np.integer):\n        a = a.astype(np.float64)\n    elif np.issubdtype(a.dtype, np.bool_):\n        a = a.astype(np.int_)\n    elif np.issubdtype(a.dtype, np.number):\n        a = a.astype(np.float64)\n\n    # calculate the depth of overlap\n    depth = (window - 1) // 2\n\n    # set boundary fill values\n    boundary = 'periodic'\n\n    # create an overlapped array\n    ag = da.overlap.overlap(a, depth=depth, boundary=boundary, axis=axis)\n\n    # apply the moving_func to the overlapped array\n    if min_count is None:\n        agg = da.map_blocks(moving_func, ag, window, axis=axis)\n    else:\n        agg = da.map_blocks(moving_func, ag, window, min_count, axis=axis)\n\n    # trim the overlapped regions\n    result = da.overlap.trim_internal(agg, depth)\n\n    return result\n\n", "CodeBleu_score": 39.44075539271077}, {"code": "def dask_rolling_wrapper(moving_func, a, window, min_count=None, axis=-1)\n\timport dask.array as da\n\n    # Promote the data type to handle edge cases\n    a = da.promote_types(a, a.dtype)\n\n    # Set the fill value for the specified axis\n    if a.dtype == 'float64':\n        fill_value = np.nan\n    elif a.dtype == 'float32':\n        fill_value = np.nan\n    elif a.dtype == 'int64':\n        fill_value = -999999\n    elif a.dtype == 'int32':\n        fill_value = -999999\n    elif a.dtype == 'int16':\n        fill_value = -9999\n    elif a.dtype == 'int8':\n        fill_value = -99\n    elif a.dtype == 'uint64':\n        fill_value = 999999\n    elif a.dtype == 'uint32':\n        fill_value = 999999\n    elif a.dtype == 'uint16':\n        fill_value = 9999\n    elif a.dtype == 'uint8':\n        fill_value = 99\n    else:\n        fill_value = 0\n\n    # Set the depth of overlap for the specified axis\n    depth = (window - 1) // 2\n\n    # Create an overlapped array\n    ag = da.overlap.overlap(a, depth=depth, boundary=fill_value, axis=axis)\n\n    # Apply the moving_func to the overlapped array\n    ag = da.map_blocks(moving_func, ag, window, axis=axis, min_count=min_count)\n\n    # Trim the overlapped regions\n    ag = da.overlap.trim_internal(ag)\n\n    return ag\n\n", "CodeBleu_score": 30.348904678646214}, {"code": "def dask_rolling_wrapper(moving_func, a, window, min_count=None, axis=-1)\n\timport dask.array as da\n    import numpy as np\n    from bottleneck import nanmin, nanmax\n\n    # promote data type to handle edge cases\n    if np.issubdtype(a.dtype, np.integer):\n        a = a.astype(float)\n    elif np.issubdtype(a.dtype, np.bool_):\n        a = a.astype(np.int_)\n\n    # set boundary fill value\n    if np.issubdtype(a.dtype, np.inexact):\n        fill = np.nan\n    elif np.issubdtype(a.dtype, np.integer):\n        fill = 0\n    elif np.issubdtype(a.dtype, np.bool_):\n        fill = False\n\n    # calculate depth of overlap\n    depth = {}\n    depth[axis] = window - 1\n\n    # create overlapped array\n    ag = da.overlap.overlap(a, depth=depth, boundary=fill, trim=False)\n\n    # apply moving window function\n    moving_func = np.nanmin if moving_func == 'nanmin' else moving_func\n    moving_func = np.nanmax if moving_func == 'nanmax' else moving_func\n    if min_count is None:\n        agg = da.map_blocks(moving_func, ag, window, axis=axis)\n    else:\n        agg = da.map_blocks(moving_func, ag, window, min_count, axis=axis)\n\n    # trim overlapped regions\n    result = da.overlap.trim_internal(agg)\n\n    return result\n\n\n", "CodeBleu_score": 36.525434085644875}]}
{"instance_id": "68ce25d325d4", "generation_result": [{"code": "def least_squares(lhs, rhs, rcond=None, skipna=False)\n\timport numpy as np\n    import dask.array as da\n    from dask.array import linalg\n    from dask.array import polyval\n    from dask.array import polyfit\n    from dask.array import where\n    from dask.array import isnan\n\n    if not isinstance(lhs, da.Array):\n        lhs = da.from_array(lhs, chunks=lhs.shape)\n    if not isinstance(rhs, da.Array):\n        rhs = da.from_array(rhs, chunks=rhs.shape)\n    if skipna:\n        lhs = lhs.reshape(lhs.shape[0], -1)\n        rhs = rhs.reshape(rhs.shape[0], -1)\n        lhs = lhs.map_blocks(polyfit, rhs, deg=1, rcond=rcond, skipna=skipna)\n        coeffs = lhs[:, 0]\n        residuals = lhs[:, 1]\n        return coeffs, residuals\n    else:\n        lhs = lhs.reshape(lhs.shape[0], -1)\n        rhs = rhs.reshape(rhs.shape[0], -1)\n        coeffs = linalg.lstsq(lhs, rhs)\n        residuals = lhs - coeffs @ rhs.T\n        return coeffs, residuals", "CodeBleu_score": 40.25848809840403}, {"code": "def least_squares(lhs, rhs, rcond=None, skipna=False)\n\timport numpy as np\n    import dask.array as da\n\n    if not isinstance(lhs, da.Array):\n        lhs = da.from_array(lhs, chunks=lhs.shape)\n    if not isinstance(rhs, da.Array):\n        rhs = da.from_array(rhs, chunks=rhs.shape)\n\n    if skipna:\n        lhs = lhs.map_blocks(lambda x: np.polynomial.polynomial.polyfit(x, np.ones(x.shape[0]), 1)[0])\n        rhs = rhs.map_blocks(lambda x: np.polynomial.polynomial.polyfit(x, np.ones(x.shape[0]), 1)[1])\n\n    return da.linalg.lstsq(lhs, rhs, rcond=rcond)\n\n", "CodeBleu_score": 31.022917925103}, {"code": "def least_squares(lhs, rhs, rcond=None, skipna=False)\n\timport numpy as np\n    import dask.array as da\n    from scipy.linalg import lstsq\n    from scipy.interpolate import interp1d\n\n    if not isinstance(lhs, da.Array):\n        lhs = da.from_array(lhs, chunks=lhs.shape)\n    if not isinstance(rhs, da.Array):\n        rhs = da.from_array(rhs, chunks=rhs.shape)\n\n    if skipna:\n        if lhs.ndim > 1:\n            lhs = lhs.reshape(lhs.shape[0], -1)\n        if rhs.ndim > 1:\n            rhs = rhs.reshape(rhs.shape[0], -1)\n        if rcond is None:\n            rcond = np.finfo(lhs.dtype).eps * max(lhs.shape)\n        coeffs = da.map_blocks(interp1d, lhs, rhs, axis=0, kind='polynomial', assume_sorted=True, bounds_error=False, fill_value=0.0, rcond=rcond)\n        residuals = rhs - lhs.dot(coeffs)\n    else:\n        coeffs, residuals, rank, s = da.linalg.lstsq(lhs, rhs)\n\n    return coeffs, residuals\n\n", "CodeBleu_score": 45.95560864081357}]}
{"instance_id": "c0c2c53b76e0", "generation_result": [{"code": "def from_array(\n        self, data: Any, chunks: T_Chunks | _NormalizedChunks, **kwargs: Any\n    ) -> DaskArray | Any\n\treturn self._from_array(data, chunks, **kwargs)\n\n", "CodeBleu_score": 25.783901938454157}, {"code": "def from_array(\n        self, data: Any, chunks: T_Chunks | _NormalizedChunks, **kwargs: Any\n    ) -> DaskArray | Any\n\treturn self._from_array(data, chunks, **kwargs)\n\n", "CodeBleu_score": 25.783901938454157}, {"code": "def from_array(\n        self, data: Any, chunks: T_Chunks | _NormalizedChunks, **kwargs: Any\n    ) -> DaskArray | Any\n\timport dask.array as da\n\n    if isinstance(data, ImplicitToExplicitIndexingAdapter):\n        # lazily loaded backend array classes should use NumPy array operations.\n        kwargs[\"meta\"] = np.ndarray\n\n    return da.from_array(\n        data,\n        chunks,\n        **kwargs,\n    )  # type: ignore[no-untyped-call]\n\n", "CodeBleu_score": 92.3542706764267}]}
{"instance_id": "6559019d09a0", "generation_result": [{"code": "def __dask_postpersist__(\n        self,\n    ) -> tuple[\n        Callable[\n            [Graph, PostPersistCallable[Any], Any, Any],\n            Self,\n        ],\n        tuple[Any, ...],\n    ]\n\tif is_duck_dask_array(self._data):\n        a: tuple[PostPersistCallable[Any], tuple[Any, ...]]\n        a = self._data.__dask_postpersist__()  # type: ignore[no-untyped-call]\n        array_func, array_args = a\n\n        return self._dask_finalize, (array_func,) + array_args\n    else:\n        raise AttributeError(\"Method requires self.data to be a dask array.\")", "CodeBleu_score": 94.10588162387555}, {"code": "def __dask_postpersist__(\n        self,\n    ) -> tuple[\n        Callable[\n            [Graph, PostPersistCallable[Any], Any, Any],\n            Self,\n        ],\n        tuple[Any, ...],\n    ]\n\tif is_duck_dask_array(self._data):\n        a: tuple[PostPersistCallable[Any], tuple[Any, ...]]\n        a = self._data.__dask_postpersist__()  # type: ignore[no-untyped-call]\n        array_func, array_args = a\n\n        return self._dask_finalize, (array_func,) + array_args\n    else:\n        raise AttributeError(\"Method requires self.data to be a dask array.\")", "CodeBleu_score": 94.10588162387555}, {"code": "def __dask_postpersist__(\n        self,\n    ) -> tuple[\n        Callable[\n            [Graph, PostPersistCallable[Any], Any, Any],\n            Self,\n        ],\n        tuple[Any, ...],\n    ]\n\tif is_duck_dask_array(self._data):\n        a: tuple[PostPersistCallable[Any], tuple[Any, ...]]\n        a = self._data.__dask_postpersist__()  # type: ignore[no-untyped-call]\n        array_func, array_args = a\n\n        return self._dask_finalize, (array_func,) + array_args\n    else:\n        raise AttributeError(\"Method requires self.data to be a dask array.\")", "CodeBleu_score": 94.10588162387555}]}
{"instance_id": "4a2801bb3851", "generation_result": [{"code": "def _dask_or_eager_func(\n    name,\n    eager_module=np,\n    dask_module=\"dask.array\",\n)\n\tdef f(*args, **kwargs):\n        if is_duck_dask_array(args[0]):\n            import importlib\n            module = importlib.import_module(dask_module)\n            func = getattr(module, name)\n        else:\n            func = getattr(eager_module, name)\n        return func(*args, **kwargs)\n    return f\n\n", "CodeBleu_score": 49.02316021727541}, {"code": "def _dask_or_eager_func(\n    name,\n    eager_module=np,\n    dask_module=\"dask.array\",\n)\n\tdef f(*args, **kwargs):\n        if is_duck_dask_array(args[0]):\n            module = importlib.import_module(dask_module)\n            func = getattr(module, name)\n        else:\n            func = getattr(eager_module, name)\n        return func(*args, **kwargs)\n    return f\n\n", "CodeBleu_score": 47.56310644451446}, {"code": "def _dask_or_eager_func(\n    name,\n    eager_module=np,\n    dask_module=\"dask.array\",\n)\n\timport numpy as np\n    import dask.array as dask_array\n\n    def f(*args, **kwargs):\n        if is_duck_dask_array(args[0]):\n            module = importlib.import_module(dask_module)\n            func = getattr(module, name)\n        else:\n            module = eager_module\n            func = getattr(module, name)\n        return func(*args, **kwargs)\n\n    return f\n\n", "CodeBleu_score": 47.75458333998907}]}
{"instance_id": "eef8d746d01e", "generation_result": [{"code": "def lazy_array_equiv(arr1, arr2)\n\tif arr1 is arr2:\n        return True\n\n    arr1, arr2 = asarray(arr1), asarray(arr2)\n\n    if arr1.shape != arr2.shape:\n        return False\n\n    if is_duck_dask_array(arr1) and is_duck_dask_array(arr2):\n        return dask_array_tokenize(arr1) == dask_array_tokenize(arr2)\n\n    return None\n\n", "CodeBleu_score": 64.31059767986386}, {"code": "def lazy_array_equiv(arr1, arr2)\n\tif arr1 is arr2:\n        return True\n    arr1 = asarray(arr1)\n    arr2 = asarray(arr2)\n    if arr1.shape != arr2.shape:\n        return False\n    if hasattr(arr1, \"chunks\") and hasattr(arr2, \"chunks\"):\n        return dask.base.tokenize(arr1) == dask.base.tokenize(arr2)\n    return None", "CodeBleu_score": 57.17936566433271}, {"code": "def lazy_array_equiv(arr1, arr2)\n\tif arr1 is arr2:\n        return True\n\n    arr1 = asarray(arr1)\n    arr2 = asarray(arr2)\n\n    if arr1.shape != arr2.shape:\n        return False\n\n    if has_dask:\n        from dask.base import tokenize\n\n        arr1_token = tokenize(arr1)\n        arr2_token = tokenize(arr2)\n\n        if arr1_token == arr2_token:\n            return True\n        else:\n            return None\n    else:\n        return None\n\n", "CodeBleu_score": 78.64452170915757}]}
{"instance_id": "15e830ff5fcb", "generation_result": [{"code": "def randn(shape, frac_nan=None, chunks=None, seed=0)\n\timport numpy as np\n    import dask.array as da\n    import random\n\n    if chunks is None:\n        return np.random.randn(*shape) * (np.random.rand(*shape) < frac_nan)\n    else:\n        random.seed(seed)\n        chunks = tuple([chunks[i] if i < len(chunks) else shape[i] for i in range(len(shape))])\n        return da.random.RandomState(seed).random_sample(shape, chunks=chunks) * (da.random.RandomState(seed).random_sample(shape, chunks=chunks) < frac_nan)\n\n", "CodeBleu_score": 42.386457644830266}, {"code": "def randn(shape, frac_nan=None, chunks=None, seed=0)\n\timport numpy as np\n    import dask.array as da\n    import math\n\n    if chunks is None:\n        np.random.seed(seed)\n        return np.random.randn(*shape)\n    else:\n        if frac_nan is None:\n            frac_nan = 0\n        if frac_nan > 0:\n            n_elements = math.prod(shape)\n            n_nan = int(n_elements * frac_nan)\n            n_not_nan = n_elements - n_nan\n            arr = np.random.choice(\n                [np.nan, 0], size=n_elements, p=[frac_nan, 1 - frac_nan]\n            )\n            arr[:n_not_nan] = np.random.randn(n_not_nan)\n        else:\n            arr = np.random.randn(*shape)\n        return da.from_array(arr, chunks=chunks)", "CodeBleu_score": 40.73154598481821}, {"code": "def randn(shape, frac_nan=None, chunks=None, seed=0)\n\timport numpy as np\n    import dask.array as da\n    np.random.seed(seed)\n    if chunks is None:\n        return np.random.randn(*shape) * (1 - frac_nan) + frac_nan * np.nan\n    else:\n        return da.random.randn(shape, chunks=chunks) * (1 - frac_nan) + frac_nan * np.nan\n\n", "CodeBleu_score": 31.521257941360915}]}
{"instance_id": "5b57abbbe38b", "generation_result": [{"code": "def analyst_price_target(self) -> pd.DataFrame\n\tif self._analyst_price_target is None:\n        raise YFNotImplementedError('analyst_price_target')\n    return self._analyst_price_target\n\n", "CodeBleu_score": 89.72656559975366}, {"code": "def analyst_price_target(self) -> pd.DataFrame\n\tif self._analyst_price_target is None:\n        raise YFNotImplementedError(\"analyst_price_target\")\n    return self._analyst_price_target\n\n", "CodeBleu_score": 80.21012753152378}, {"code": "def analyst_price_target(self) -> pd.DataFrame\n\tif self._analyst_price_target is None:\n        raise YFNotImplementedError(\"analyst_price_target\")\n    return self._analyst_price_target\n\n", "CodeBleu_score": 80.21012753152378}]}
{"instance_id": "592e414671e9", "generation_result": [{"code": "def _fetch_time_series(self, name, timescale, proxy=None)\n\tif name not in const.fundamentals_keys:\n        raise ValueError(\n            \"Invalid financial statement. Use one of the following: \"\n            + \", \".join(const.fundamentals_keys)\n        )\n\n    if timescale not in const.timescales:\n        raise ValueError(\n            \"Invalid timescale. Use one of the following: \"\n            + \", \".join(const.timescales)\n        )\n\n    try:\n        return self._create_financials_table(name, timescale, proxy)\n    except Exception:\n        logger.error(\n            \"An error occurred while retrieving the time series for \"\n            + name\n            + \" statement and \"\n            + timescale\n            + \" timescale.\"\n        )\n        return pd.DataFrame()\n\n", "CodeBleu_score": 33.99299539446612}, {"code": "def _fetch_time_series(self, name, timescale, proxy=None)\n\tif name not in const.fundamentals_keys:\n        raise ValueError(\n            \"Invalid financial statement name. Please use one of the following: \"\n            + \", \".join(const.fundamentals_keys.keys())\n        )\n\n    if timescale not in const.fundamentals_periods:\n        raise ValueError(\n            \"Invalid timescale. Please use one of the following: \"\n            + \", \".join(const.fundamentals_periods)\n        )\n\n    try:\n        return self._create_financials_table(name, timescale, proxy)\n    except Exception as e:\n        log.error(e)\n        return pd.DataFrame()\n\n", "CodeBleu_score": 33.569188058461286}, {"code": "def _fetch_time_series(self, name, timescale, proxy=None)\n\tif name not in const.fundamentals_keys.keys():\n        raise ValueError(f\"Invalid financial statement: {name}\")\n    if timescale not in const.timescales:\n        raise ValueError(f\"Invalid timescale: {timescale}\")\n\n    try:\n        return self._create_financials_table(name, timescale, proxy)\n    except Exception:\n        logger.error(f\"Failed to retrieve {name} data for {self.ticker}\")\n        return pd.DataFrame()\n\n", "CodeBleu_score": 32.89189615002006}]}
{"instance_id": "af01d9bae091", "generation_result": [{"code": "def _parse_fund_ownership(self, data)\n\t# Extract the list of ownership details\n    ownership_details = data['fundOwnershipList']\n    # Initialize a list to store the processed data\n    ownership_data = []\n    # Iterate through each owner's data\n    for owner in ownership_details:\n        # Parse raw values\n        owner = self._parse_raw_values(owner)\n        # Remove the 'maxAge' field\n        del owner['maxAge']\n        # Append the owner's data to the list\n        ownership_data.append(owner)\n    # Convert the list into a DataFrame\n    ownership_data = pd.DataFrame(ownership_data)\n    # Check if the DataFrame is empty\n    if not ownership_data.empty:\n        # Convert the 'reportDate' to a datetime format\n        ownership_data['reportDate'] = pd.to_datetime(ownership_data['reportDate'])\n        # Rename specific columns for clarity\n        ownership_data.rename(columns={'reportDate': 'Date', 'reportedHolding': 'Holding', 'reportedShares': 'Shares', 'reportedValue': 'Value', 'reportedName': 'Name'}, inplace=True)\n    # Assign the DataFrame to the instance variable\n    self._mutualfund = ownership_data\n\n", "CodeBleu_score": 29.21254445589698}, {"code": "def _parse_fund_ownership(self, data)\n\tself._mutualfund = pd.DataFrame()\n    for owner in data['ownershipList']:\n        owner.pop('maxAge')\n        self._mutualfund = self._mutualfund.append(owner, ignore_index=True)\n    if not self._mutualfund.empty:\n        self._mutualfund['reportDate'] = pd.to_datetime(self._mutualfund['reportDate'])\n        self._mutualfund.rename(columns={'reportDate': 'Date', 'reportedHolding': 'Reported Holding', 'reportedShares': 'Reported Shares', 'reportedValue': 'Reported Value'}, inplace=True)\n\n\n", "CodeBleu_score": 29.199358947230163}, {"code": "def _parse_fund_ownership(self, data)\n\tself._mutualfund = pd.DataFrame()\n    if 'ownershipList' in data:\n        for owner in data['ownershipList']:\n            del owner['maxAge']\n            self._mutualfund = self._mutualfund.append(owner, ignore_index=True)\n    if not self._mutualfund.empty:\n        self._mutualfund['reportDate'] = pd.to_datetime(self._mutualfund['reportDate'])\n        self._mutualfund.rename(columns={'reportDate': 'Date', 'reportedOwner': 'Owner', 'reportedTitle': 'Title'}, inplace=True)\n        self._mutualfund.set_index('Date', inplace=True)\n    return self._mutualfund\n\n", "CodeBleu_score": 30.705085680881727}]}
{"instance_id": "2b5c6f1253db", "generation_result": [{"code": "def currency(self)\n\tif self.currency:\n        return self.currency\n    else:\n        metadata = self.get_metadata()\n        self.currency = metadata['currency']\n        return self.currency\n\n\n", "CodeBleu_score": 41.99109440945549}, {"code": "def currency(self)\n\tif self.currency is not None:\n        return self.currency\n    else:\n        metadata = self.get_metadata()\n        self.currency = metadata['currency']\n        return self.currency", "CodeBleu_score": 51.005917904699395}, {"code": "def currency(self)\n\tif self.currency is not None:\n        return self.currency\n    else:\n        metadata = self.get_metadata()\n        self.currency = metadata['currency']\n        return self.currency\n\n", "CodeBleu_score": 51.005917904699395}]}
{"instance_id": "16b80be60e3b", "generation_result": [{"code": "def recommendations(self) -> pd.DataFrame\n\tif self._recommendations is None:\n        self._recommendations = pd.DataFrame()\n        if self._quote_summary_data is None:\n            self._quote_summary_data = self._fetch_quote_summary_data(self._proxy, [\"recommendationTrend\"])\n        if self._quote_summary_data is None:\n            return self._recommendations\n        try:\n            self._recommendations = pd.DataFrame(self._quote_summary_data[\"result\"][0][\"recommendationTrend\"][\"trend\"]).T\n            self._recommendations[\"date\"] = pd.to_datetime(self._recommendations[\"date\"], format=\"%Y-%m-%d\")\n        except KeyError as e:\n            raise YFDataException(\"Failed to parse recommendation data, error message: \" + str(e))\n    return self._recommendations\n\n", "CodeBleu_score": 47.160395330341345}, {"code": "def recommendations(self) -> pd.DataFrame\n\tif self._recommendations is None:\n        data = self._fetch_quote_summary_data(modules=[\"recommendationTrend\"])\n        if data is None:\n            self._recommendations = pd.DataFrame()\n            return self._recommendations\n        try:\n            self._recommendations = pd.DataFrame(data[\"recommendationTrend\"][\"trend\"])\n        except (KeyError, TypeError) as e:\n            raise YFDataException(f\"Failed to parse recommendation data from Yahoo Finance. Error: {e}\")\n\n    return self._recommendations\n\n", "CodeBleu_score": 52.13592425715912}, {"code": "def recommendations(self) -> pd.DataFrame\n\tif self._recommendations is None:\n        self._recommendations = self._fetch_recommendations()\n    if self._recommendations is None:\n        self._recommendations = pd.DataFrame()\n    return self._recommendations\n\n", "CodeBleu_score": 22.86278520788251}]}
{"instance_id": "abaf8c2a3ecc", "generation_result": [{"code": "def plot_elbow_curve(clf, X, title='Elbow Plot', cluster_ranges=None, n_jobs=1,\n                     show_cluster_time=True, ax=None, figsize=None,\n                     title_fontsize=\"large\", text_fontsize=\"medium\")\n\tif not hasattr(clf, 'n_clusters'):\n        raise Exception('The classifier must have an attribute n_clusters.')\n\n    if cluster_ranges is None:\n        cluster_ranges = range(1, 11)\n\n    if ax is None:\n        if figsize is None:\n            figsize = (10, 6)\n        fig, ax = plt.subplots(figsize=figsize)\n\n    ax.set_title(title, fontsize=title_fontsize)\n    ax.set_xlabel('Number of clusters', fontsize=text_fontsize)\n    ax.set_ylabel('Sum of squared errors', fontsize=text_fontsize)\n\n    sse = []\n    durations = []\n    for n_clusters in cluster_ranges:\n        clf.set_params(n_clusters=n_clusters)\n        start_time = time.time()\n        clf.fit(X)\n        end_time = time.time()\n        sse.append(clf.inertia_)\n        durations.append(end_time - start_time)\n\n    ax.plot(cluster_ranges, sse, 'bx-')\n    ax.grid()\n\n    if show_cluster_time:\n        ax2 = ax.twinx()\n        ax2.plot(cluster_ranges, durations, 'r--')\n        ax2.set_ylabel('Clustering duration (sec)', fontsize=text_fontsize)\n\n    return ax\n\n", "CodeBleu_score": 48.75628539538502}, {"code": "def plot_elbow_curve(clf, X, title='Elbow Plot', cluster_ranges=None, n_jobs=1,\n                     show_cluster_time=True, ax=None, figsize=None,\n                     title_fontsize=\"large\", text_fontsize=\"medium\")\n\tif not hasattr(clf, 'n_clusters'):\n        raise ValueError('Classifier must have the attribute n_clusters')\n\n    if cluster_ranges is None:\n        cluster_ranges = [2, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n\n    if ax is None:\n        fig, ax = plt.subplots(1, 1, figsize=figsize)\n\n    # Generate the elbow curve\n    sse = []\n    cluster_time = []\n    for n_clusters in cluster_ranges:\n        clf.set_params(n_clusters=n_clusters)\n        start_time = time.time()\n        clf.fit(X)\n        end_time = time.time()\n        sse.append(clf.inertia_)\n        cluster_time.append(end_time - start_time)\n\n    # Plot the elbow curve\n    ax.plot(cluster_ranges, sse, 'bx-')\n    ax.set_xlabel('Number of clusters', fontsize=text_fontsize)\n    ax.set_ylabel('Sum of squared errors', fontsize=text_fontsize)\n    ax.set_title(title, fontsize=title_fontsize)\n    ax.grid()\n\n    # Plot the clustering time on a secondary y-axis\n    if show_cluster_time:\n        ax2 = ax.twinx()\n        ax2.plot(cluster_ranges, cluster_time, 'r.-')\n        ax2.set_ylabel('Clustering time (s)', fontsize=text_fontsize)\n\n    return ax", "CodeBleu_score": 52.5884013194264}, {"code": "def plot_elbow_curve(clf, X, title='Elbow Plot', cluster_ranges=None, n_jobs=1,\n                     show_cluster_time=True, ax=None, figsize=None,\n                     title_fontsize=\"large\", text_fontsize=\"medium\")\n\tif not hasattr(clf, 'n_clusters'):\n        raise ValueError('The classifier must have an attribute \"n_clusters\"')\n    \n    if cluster_ranges is None:\n        cluster_ranges = range(1, 11)\n    \n    if ax is None:\n        fig, ax = plt.subplots(1, 1, figsize=figsize)\n    \n    ax.set_title(title, fontsize=title_fontsize)\n    ax.set_xlabel('Number of Clusters', fontsize=text_fontsize)\n    ax.set_ylabel('Sum of Squared Errors', fontsize=text_fontsize)\n    ax.tick_params(axis='both', which='major', labelsize=text_fontsize)\n    \n    X = np.array(X)\n    sse = []\n    times = []\n    for n_clusters in cluster_ranges:\n        clf.set_params(n_clusters=n_clusters)\n        start = time.time()\n        clf.fit(X)\n        end = time.time()\n        sse.append(clf.inertia_)\n        times.append(end - start)\n    \n    ax.plot(cluster_ranges, sse, 'bx-')\n    ax.grid()\n    \n    if show_cluster_time:\n        ax2 = ax.twinx()\n        ax2.plot(cluster_ranges, times, 'r.-')\n        ax2.set_ylabel('Clustering Time (in seconds)',\n                       fontsize=text_fontsize, color='r')\n        ax2.tick_params(axis='y', which='major', labelsize=text_fontsize)\n    \n    return ax", "CodeBleu_score": 55.60693395877971}]}
{"instance_id": "8a90815c4ee1", "generation_result": [{"code": "def _clone_and_score_clusterer(clf, X, n_clusters)\n\tstart = time.time()\n    clf_clone = clone(clf)\n    clf_clone.set_params(n_clusters=n_clusters)\n    clf_clone.fit(X)\n    end = time.time()\n    return clf_clone.score(X), end - start\n\n", "CodeBleu_score": 34.47100359306438}, {"code": "def _clone_and_score_clusterer(clf, X, n_clusters)\n\tfrom time import time\n    t0 = time()\n    clf_clone = clf.set_params(n_clusters=n_clusters)\n    clf_clone.fit(X)\n    score = clf_clone.score(X)\n    t1 = time()\n    return score, t1 - t0\n\n\n", "CodeBleu_score": 38.45392272009352}, {"code": "def _clone_and_score_clusterer(clf, X, n_clusters)\n\tstart_time = time.time()\n    clf_clone = clone(clf)\n    clf_clone.set_params(n_clusters=n_clusters)\n    clf_clone.fit(X)\n    score = clf_clone.score(X)\n    end_time = time.time()\n    time_taken = end_time - start_time\n    return score, time_taken\n\n\n", "CodeBleu_score": 30.817534765942945}]}
{"instance_id": "da40bd5dd81a", "generation_result": [{"code": "def clustering_factory(clf)\n\tif not hasattr(clf, 'fit'):\n        raise TypeError(\"The object does not have a method 'fit'\")\n    if not hasattr(clf, 'fit_predict'):\n        raise TypeError(\"The object does not have a method 'fit_predict'\")\n\n    def plot_silhouette(self, X, labels, metric='euclidean', **kwargs):\n        \"\"\"The function plot_silhouette takes the following arguments:", "CodeBleu_score": 12.81453294899385}, {"code": "def clustering_factory(clf)\n\t# Check if the clustering object has the required methods\n    if not hasattr(clf, 'fit'):\n        raise TypeError(\"The clustering object should have a method 'fit'\")\n    if not hasattr(clf, 'fit_predict'):\n        raise TypeError(\"The clustering object should have a method 'fit_predict'\")\n\n    # Define additional methods\n    def plot_silhouette(self, X, labels, **kwargs):\n        \"\"\"The method plot_silhouette takes two arguments X and labels.", "CodeBleu_score": 13.097898367599885}, {"code": "def clustering_factory(clf)\n\tif not hasattr(clf, 'fit'):\n        raise TypeError(\"The clustering object should have a 'fit' method\")\n    if not hasattr(clf, 'fit_predict'):\n        raise TypeError(\"The clustering object should have a 'fit_predict' method\")\n\n    def plot_silhouette(self, X, y=None, **kwargs):\n        \"\"\"The method plot_silhouette takes two arguments X and y. It plots the silhouette plot for the data X.", "CodeBleu_score": 15.692434316157877}]}
{"instance_id": "b8d8ba3c0b1b", "generation_result": [{"code": "def plot_learning_curve(clf, X, y, title='Learning Curve', cv=None,\n                        train_sizes=None, n_jobs=1, scoring=None,\n                        ax=None, figsize=None, title_fontsize=\"large\",\n                        text_fontsize=\"medium\")\n\tif ax is None:\n        fig, ax = plt.subplots(1, 1, figsize=figsize)\n\n    train_sizes, train_scores, test_scores = learning_curve(clf, X, y, cv=cv,\n                                                            n_jobs=n_jobs,\n                                                            scoring=scoring,\n                                                            train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n\n    ax.grid()\n    ax.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                    train_scores_mean + train_scores_std, alpha=0.1,\n                    color=\"r\")\n    ax.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                    test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    ax.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n            label=\"Training score\")\n    ax.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n            label=\"Cross-validation score\")\n    ax.legend(loc=\"best\")\n    ax.set_title(title, fontsize=title_fontsize)\n    ax.set_xlabel(\"Training examples\", fontsize=text_fontsize)\n    ax.set_ylabel(\"Score\", fontsize=text_fontsize)\n    return ax\n\n\n\n", "CodeBleu_score": 70.92509452546335}, {"code": "def plot_learning_curve(clf, X, y, title='Learning Curve', cv=None,\n                        train_sizes=None, n_jobs=1, scoring=None,\n                        ax=None, figsize=None, title_fontsize=\"large\",\n                        text_fontsize=\"medium\")\n\tfrom sklearn.model_selection import learning_curve\n    from sklearn.model_selection import ShuffleSplit\n    from sklearn.model_selection import cross_val_score\n    from sklearn.metrics import make_scorer\n    from sklearn.model_selection import train_test_split\n    from sklearn.metrics import accuracy_score\n    from sklearn.model_selection import GridSearchCV\n    from sklearn.metrics import confusion_matrix\n    from sklearn.metrics import classification_report\n    from sklearn.metrics import plot_confusion_matrix\n    import matplotlib.pyplot as plt\n    import numpy as np\n\n    if ax is None:\n        if figsize is not None:\n            fig, ax = plt.subplots(figsize=figsize)\n        else:\n            fig, ax = plt.subplots()\n\n    if cv is None:\n        cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\n\n    if train_sizes is None:\n        train_sizes = np.linspace(0.1, 1.0, 5)\n\n    if scoring is None:\n        scoring = make_scorer(accuracy_score)\n\n    train_sizes, train_scores, test_scores = learning_curve(\n        clf, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes, scoring=scoring)\n\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n\n    ax.grid()\n    ax.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                    train_scores_mean + train_scores_std, alpha=0.1,\n                    color=\"r\")\n    ax.", "CodeBleu_score": 60.54286679817616}, {"code": "def plot_learning_curve(clf, X, y, title='Learning Curve', cv=None,\n                        train_sizes=None, n_jobs=1, scoring=None,\n                        ax=None, figsize=None, title_fontsize=\"large\",\n                        text_fontsize=\"medium\")\n\tif ax is None:\n        if figsize is None:\n            figsize = (10, 5)\n        fig, ax = plt.subplots(1, 1, figsize=figsize)\n\n    ax.set_title(title, fontsize=title_fontsize)\n    ax.set_xlabel(\"Training examples\", fontsize=text_fontsize)\n    ax.set_ylabel(\"Score\", fontsize=text_fontsize)\n\n    train_sizes, train_scores, test_scores = learning_curve(clf, X, y, cv=cv,\n                                                            train_sizes=train_sizes,\n                                                            n_jobs=n_jobs, scoring=scoring)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n\n    ax.grid()\n\n    ax.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                    train_scores_mean + train_scores_std, alpha=0.1,\n                    color=\"r\")\n    ax.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                    test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    ax.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n            label=\"Training score\")\n    ax.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n            label=\"Cross-validation score\")\n\n    ax.legend(loc=\"best\")\n\n    return ax\n\n", "CodeBleu_score": 74.17659103800817}]}
{"instance_id": "e6e76eb35945", "generation_result": [{"code": "def plot_elbow_curve(clf, X, title='Elbow Plot', cluster_ranges=None,\n                     ax=None, figsize=None, title_fontsize=\"large\",\n                     text_fontsize=\"medium\")\n\t# Check if the classifier has an n_clusters attribute\n    if not hasattr(clf, 'n_clusters'):\n        raise TypeError(f\"The classifier {clf} does not have an n_clusters attribute.\")\n\n    # If no cluster_ranges are provided, set the default\n    if cluster_ranges is None:\n        cluster_ranges = range(1, 12, 2)\n\n    # Create a list of classifiers with different cluster numbers\n    clusterers = [clf.__class__(n_clusters=n) for n in cluster_ranges]\n\n    # Fit the classifiers to the data and store the scores\n    scores = [c.fit(X).score(X) for c in clusterers]\n\n    # If no axes object is provided, create a new one\n    if ax is None:\n        if figsize is None:\n            figsize = (6, 4)\n        _, ax = plt.subplots(figsize=figsize)\n\n    # Plot the number of clusters against the absolute value of the scores\n    ax.plot(cluster_ranges, np.abs(scores), 'bx-')\n\n    # Add grid lines\n    ax.grid(True, linestyle='--', color='.3')\n\n    # Add labels\n    ax.set_xlabel('Number of clusters', fontsize=text_fontsize)\n    ax.set_ylabel('Score', fontsize=text_fontsize)\n\n    # Set the title and font sizes\n    ax.set_title(title, fontsize=title_fontsize)\n\n    return ax", "CodeBleu_score": 52.633393178781276}, {"code": "def plot_elbow_curve(clf, X, title='Elbow Plot', cluster_ranges=None,\n                     ax=None, figsize=None, title_fontsize=\"large\",\n                     text_fontsize=\"medium\")\n\tif not hasattr(clf, \"n_clusters\"):\n        raise TypeError(\"The classifier must have an n_clusters attribute\")\n    if cluster_ranges is None:\n        cluster_ranges = range(1, 12, 2)\n    clfs = [clf.__class__(n_clusters=n) for n in cluster_ranges]\n    scores = [clf.fit(X).score(X) for clf in clfs]\n    if ax is None:\n        fig, ax = plt.subplots(figsize=figsize)\n    ax.plot(cluster_ranges, np.abs(scores))\n    ax.grid(True)\n    ax.set_xlabel(\"Number of Clusters\", fontsize=text_fontsize)\n    ax.set_ylabel(\"Score\", fontsize=text_fontsize)\n    ax.set_title(title, fontsize=title_fontsize)\n    return ax", "CodeBleu_score": 49.63546557509973}, {"code": "def plot_elbow_curve(clf, X, title='Elbow Plot', cluster_ranges=None,\n                     ax=None, figsize=None, title_fontsize=\"large\",\n                     text_fontsize=\"medium\")\n\tif not hasattr(clf, 'n_clusters'):\n        raise TypeError(\"clf does not have an n_clusters attribute.\")\n    if cluster_ranges is None:\n        cluster_ranges = range(1, 12, 2)\n    clfs = [clf.__class__(n_clusters=i) for i in cluster_ranges]\n    scores = [clf.fit(X).score(X) for clf in clfs]\n    if ax is None:\n        fig, ax = plt.subplots(figsize=figsize)\n    ax.plot(cluster_ranges, np.abs(scores))\n    ax.set_xlabel(\"Number of clusters\", fontsize=text_fontsize)\n    ax.set_ylabel(\"Score\", fontsize=text_fontsize)\n    ax.set_title(title, fontsize=title_fontsize)\n    ax.grid(True)\n    return ax", "CodeBleu_score": 50.85649179243013}]}
{"instance_id": "6f4530c8a20f", "generation_result": [{"code": "def plot_pca_2d_projection(clf, X, y, title='PCA 2-D Projection', ax=None,\n                           figsize=None, cmap='Spectral',\n                           title_fontsize=\"large\", text_fontsize=\"medium\")\n\tif ax is None:\n        fig, ax = plt.subplots(figsize=figsize)\n    X_r = clf.transform(X)\n    x1 = X_r[:, 0]\n    x2 = X_r[:, 1]\n    ax.scatter(x1, x2, c=y, s=40, cmap=cmap)\n    ax.set_xlabel(\"Principal Component 1\", fontsize=text_fontsize)\n    ax.set_ylabel(\"Principal Component 2\", fontsize=text_fontsize)\n    ax.set_title(title, fontsize=title_fontsize)\n    ax.legend(loc='lower left', fontsize=text_fontsize)\n    return ax\n\n\n", "CodeBleu_score": 36.926287406145185}, {"code": "def plot_pca_2d_projection(clf, X, y, title='PCA 2-D Projection', ax=None,\n                           figsize=None, cmap='Spectral',\n                           title_fontsize=\"large\", text_fontsize=\"medium\")\n\tif ax is None:\n        fig, ax = plt.subplots(figsize=figsize)\n    \n    X_transformed = clf.fit_transform(X)\n    ax.scatter(X_transformed[:, 0], X_transformed[:, 1], c=y, cmap=cmap)\n    ax.set_title(title, fontsize=title_fontsize)\n    ax.set_xlabel('Principal Component 1', fontsize=text_fontsize)\n    ax.set_ylabel('Principal Component 2', fontsize=text_fontsize)\n    ax.legend(loc='lower left')\n    \n    return ax\n\n", "CodeBleu_score": 34.83241167265034}, {"code": "def plot_pca_2d_projection(clf, X, y, title='PCA 2-D Projection', ax=None,\n                           figsize=None, cmap='Spectral',\n                           title_fontsize=\"large\", text_fontsize=\"medium\")\n\t# Perform PCA\n    pca = PCA(n_components=2)\n    pca_transformed = pca.fit_transform(X)\n\n    # Set up the figure\n    if ax is None:\n        fig, ax = plt.subplots(figsize=figsize)\n\n    # Plot the transformed data\n    colors = [cmap(i) for i in np.linspace(0, 1, len(np.unique(y)))]\n    for label, color in zip(np.unique(y), colors):\n        ix = np.where(y == label)\n        ax.scatter(pca_transformed[ix, 0], pca_transformed[ix, 1],\n                   c=color, label=label)\n\n    # Set up the plot\n    ax.set_title(title, fontsize=title_fontsize)\n    ax.set_xlabel('Principal Component 1', fontsize=text_fontsize)\n    ax.set_ylabel('Principal Component 2', fontsize=text_fontsize)\n    ax.legend(fontsize=text_fontsize)\n\n    return ax", "CodeBleu_score": 50.258788513897876}]}
{"instance_id": "e7c2775d024c", "generation_result": [{"code": "def barplot(ax)\n\tclip1, clip2 = clip_path(ax)\n    Y1 = np.random.rand(16)\n    Y2 = np.random.rand(16)\n    ax.bar(np.arange(16)-0.25, Y1, 0.5, bottom=0, color='#999999',\n           linewidth=0, edgecolor='black', zorder=10, clip_path=clip1)\n    ax.bar(np.arange(16)+0.25, Y2, 0.5, bottom=0, color='#999999',\n           linewidth=0, edgecolor='black', zorder=10, clip_path=clip2)\n    ax.bar(np.arange(16)-0.25, Y1, 0.5, bottom=0, color='#ff0000',\n           linewidth=0, edgecolor='black', zorder=10, clip_path=clip1)\n    ax.bar(np.arange(16)+0.25, Y2, 0.5, bottom=0, color='#ff0000',\n           linewidth=0, edgecolor='black', zorder=10, clip_path=clip2)\n    ax.set_xlim(-1, 16)\n    ax.set_ylim(0, 1)\n    ax.text(0, -0.05, \"Bar plot\", size=14, color='black')\n    ax.text(0, -0.1, \"ax.bar()\", size=14, family='monospace', color='blue')\n\n", "CodeBleu_score": 32.02321907415923}, {"code": "def barplot(ax)\n\tclip1, clip2 = clip_path(ax)\n\n    Y1 = np.random.rand(16)*20\n    Y2 = np.random.rand(16)*20\n    X = np.arange(16)\n\n    ax.bar(X, Y1, 0.5, bottom=0, color='lightgray', clip_path=clip1)\n    ax.bar(X, Y2, 0.5, bottom=0, color='gray', clip_path=clip2)\n    ax.bar(X, Y1, 0.5, bottom=Y2, color='red', clip_path=clip1)\n    ax.bar(X, Y2, 0.5, bottom=Y2, color='darkred', clip_path=clip2)\n\n    ax.set_xlim(-1, 16)\n\n    ax.text(0.5, -0.05, \"Bar plot\", ha='center', va='center',\n            transform=ax.transAxes)\n    ax.text(0.5, -0.15, r\"$\\mathdefault{ax.bar()}$\",\n            ha='center', va='center', transform=ax.transAxes,\n            fontproperties=font_manager.FontProperties(family=\"monospace\"))\n\n", "CodeBleu_score": 36.13766437829119}, {"code": "def barplot(ax)\n\tclip1, clip2 = clip_path(ax)\n\n    Y1 = np.random.rand(16)*0.5+0.25\n    Y2 = np.random.rand(16)*0.5+0.25\n\n    ax.bar(np.arange(16)-0.25, Y1, 0.5, color='0.75',\n           edgecolor='black', linewidth=1,\n           label='Y1', clip_path=clip1)\n    ax.bar(np.arange(16)+0.25, Y2, 0.5, color='0.95',\n           edgecolor='black', linewidth=1,\n           label='Y2', clip_path=clip2)\n\n    ax.set_xlim(-1, 16)\n\n    ax.text(0.025, 0.025, 'Bar plot', transform=ax.transAxes,\n            fontsize=14, fontweight='bold', color='black')\n    ax.text(0.025, 0.01, r'$\\mathtt{ax.bar()}$', transform=ax.transAxes,\n            fontsize=14, color='blue')\n\n", "CodeBleu_score": 33.23051949306151}]}
{"instance_id": "0ec1d146a32b", "generation_result": [{"code": "def split(n_segment)\n\twidth = 1/n_segment\n    padding = width/2\n    X0 = np.arange(0, 1-width, width)\n    X1 = X0 + width\n    return X0, X1\n\n", "CodeBleu_score": 28.41916816630753}, {"code": "def split(n_segment)\n\t# Calculate the width of each segment\n    width = 2 / n_segment\n    # Calculate the padding between segments\n    padding = width / 1000\n    # Determine the starting (X0) and ending (X1) positions of each segment\n    X0 = np.arange(0, 2, width)\n    X1 = X0 + width - 2 * padding\n    return X0, X1\n\n", "CodeBleu_score": 26.5566263856648}, {"code": "def split(n_segment)\n\twidth = 1 / n_segment\n    padding = width / 2\n    X0 = np.arange(0, 1, width) - padding\n    X1 = np.arange(0, 1, width) + padding\n    return X0, X1", "CodeBleu_score": 19.687526850292244}]}
{"instance_id": "604a3e93ac0d", "generation_result": [{"code": "def pixelated_text(dpi=100)\n\timport matplotlib.pyplot as plt\n    fig = plt.figure(figsize=(1,1), dpi=dpi)\n    canvas = plt.gca().figure.canvas\n    plt.text(0.5, 0.5, 'a', fontsize=75, ha='center', va='center')\n    plt.axis('off')\n    canvas.draw()\n    img = np.array(canvas.renderer.buffer_rgba())\n    img = np.reshape(img, (dpi, dpi, 4))\n    img = np.roll(img, 2, axis=2)\n    return img", "CodeBleu_score": 43.12313085179645}, {"code": "def pixelated_text(dpi=100)\n\timport matplotlib.pyplot as plt\n    fig = plt.figure(figsize=(1, 1), dpi=dpi)\n    canvas = fig.canvas\n    fig.text(0.5, 0.5, 'a', fontsize=75)\n    ax = plt.gca()\n    ax.set_axis_off()\n    canvas.draw()\n    data = np.frombuffer(canvas.tostring_rgb(), dtype=np.uint8)\n    data = data.reshape(canvas.get_width_height()[::-1] + (3,))\n    data = np.roll(data, 3, axis=2)\n    return data\n\n", "CodeBleu_score": 47.57163112865497}, {"code": "def pixelated_text(dpi=100)\n\timport matplotlib.pyplot as plt\n    import numpy as np\n\n    fig = plt.figure()\n    canvas = fig.canvas\n    plt.axis('off')\n    plt.text(0.5, 0.5, 'a', fontsize=75)\n    canvas.draw()\n    image = np.array(canvas.renderer.buffer_rgba())\n    image = image.reshape(dpi, dpi, 4)\n    image = np.roll(image, 1, axis=2)\n    return image\n\n", "CodeBleu_score": 50.645815663360416}]}
{"instance_id": "2a7fc7d560ea", "generation_result": [{"code": "def curve()\n\timport numpy as np\n    import random\n    import matplotlib.pyplot as plt\n    import scipy.stats as stats\n    import matplotlib.mlab as mlab\n    import matplotlib.ticker as ticker\n    import scipy.optimize as optimize\n    import scipy.special as special\n    import scipy.integrate as integrate\n    import scipy.interpolate as interpolate\n    import scipy.signal as signal\n    import scipy.stats as stats\n    import scipy.misc as misc\n    import math\n    import sys\n    import os\n\n    def gaussian(x, mu, sig):\n        return np.exp(-np.power(x - mu, 2.) / (2 * np.power(sig, 2.)))\n\n    def gaussian_integral(x, mu, sig):\n        return np.sqrt(2 * np.pi) * sig * np.exp(-np.power(x - mu, 2.) / (2 * np.power(sig, 2.)))\n\n    def gaussian_integral_normalized(x, mu, sig):\n        return np.sqrt(2 * np.pi) * sig * np.exp(-np.power(x - mu, 2.) / (2 * np.power(sig, 2.))) / np.sqrt(2 * np.pi) * sig\n\n    def gaussian_integral_normalized_square(x, mu, sig):\n        return np.sqrt(2 * np.pi) * sig * np.exp(-np.power(x - mu, 2.) / (2 * np.power(sig, 2.))) / np.sqrt(2 * np.pi) * sig / np.sqrt(2 * np.pi) * sig\n\n    def gaussian_integral_normalized_cube(x, mu, sig):\n        return np.sqrt(2 * np.pi) * sig * np.exp(-np.power(x - mu, 2.) / (2 * np.power(sig, 2.))) / np.sqrt(2 * np.pi) * sig / np.sqrt(2 * np.pi) * sig / np.sqrt(2 * np.pi) * sig", "CodeBleu_score": 11.010763792825976}, {"code": "def curve()\n\t# Randomly determine the number of components\n    n = np.random.randint(2, 10)\n\n    # Randomly determine the centers of the components\n    centers = np.sort(np.random.rand(n))\n\n    # Randomly determine the widths of the components\n    widths = np.abs(np.random.randn(n))\n\n    # Normalize the widths to sum to 1\n    widths = widths / np.sum(widths)\n\n    # Randomly determine the scales of the components\n    scales = np.abs(np.random.randn(n))\n\n    # Normalize the scales to sum to 1\n    scales = scales / np.sum(scales)\n\n    # Initialize an array of zeros\n    x = np.zeros(1000)\n\n    # Iteratively add scaled Gaussian components\n    for i in range(n):\n        x += scales[i] * np.exp(-0.5 * ((np.linspace(0, 1, 1000) - centers[i]) / widths[i]) ** 2)\n\n    return x\n\n", "CodeBleu_score": 28.315416908507096}, {"code": "def curve()\n\t# Generate the number of components, their centers, widths, and scales\n    #\n    # The number of components is random, with a minimum of 2 and a maximum of 10\n    #\n    # The centers are random, with a minimum of 10 and a maximum of 190\n    #\n    # The widths are random, with a minimum of 5 and a maximum of 30\n    #\n    # The scales are random, with a minimum of 0.1 and a maximum of 0.9\n\n    # Initialize an array of zeros\n    #\n    # The array has a length of 200, with a step size of 0.1\n    #\n    # The array is named curve\n\n    # Iteratively add scaled Gaussian components to the array\n    #\n    # The components are generated from the calculated parameters\n    #\n    # The components are added to the array\n\n    # Return the resulting composite curve\n\n", "CodeBleu_score": 0.3424657534246575}]}
{"instance_id": "be694335ebfc", "generation_result": [{"code": "def difference(derivative, accuracy=1)\n\t# Increment the derivative order\n    derivative = derivative + 1\n\n    # Calculate the radius of the finite difference\n    radius = accuracy * derivative\n\n    # Define a range of points centered around zero\n    points = np.arange(-radius, radius + 1)\n\n    # Calculate the inverse of the Vandermonde matrix for these points\n    vandermonde = np.vander(points, derivative)\n    inverse = np.linalg.inv(vandermonde)\n\n    # Return the coefficients scaled by the factorial of one less than the derivative order\n    return (inverse[derivative - 1] / np.math.factorial(derivative - 1), points)\n\n", "CodeBleu_score": 44.93985073976666}, {"code": "def difference(derivative, accuracy=1)\n\tderivative += 1\n    radius = int(np.ceil((derivative + accuracy) / 2))\n    x = np.arange(-radius, radius + 1)\n    V = vandermonde(x, derivative)\n    Vinv = np.linalg.inv(V)\n    coefficients = Vinv[derivative, :] / np.math.factorial(derivative - 1)\n    return coefficients, x\n\n", "CodeBleu_score": 48.48379450340196}, {"code": "def difference(derivative, accuracy=1)\n\tderivative = int(derivative)\n    accuracy = int(accuracy)\n    radius = derivative + accuracy\n    points = np.linspace(-radius, radius, 2 * radius + 1)\n    vandermonde = np.vander(points, derivative + 1)\n    coefficients = np.linalg.inv(vandermonde)[derivative]\n    coefficients = coefficients / np.math.factorial(derivative)\n    return coefficients, points\n\n", "CodeBleu_score": 42.079502350968255}]}
{"instance_id": "fa3fb47f33fa", "generation_result": [{"code": "def interpolate(X, Y, T)\n\t# Calculate the distances between consecutive points\n    distances = np.sqrt(np.sum((X[1:] - X[:-1])**2 + (Y[1:] - Y[:-1])**2, axis=0))\n    # Calculate the cumulative distances\n    cumulative_distances = np.cumsum(distances)\n    # Find the cumulative distance corresponding to each target point\n    cumulative_distances_at_targets = np.interp(T, cumulative_distances, cumulative_distances)\n    # Calculate the fraction of total distance traveled\n    fractions = cumulative_distances_at_targets / cumulative_distances[-1]\n    # Calculate the interpolated X and Y coordinates\n    interpolated_X = np.interp(T, cumulative_distances, X)\n    interpolated_Y = np.interp(T, cumulative_distances, Y)\n    # Return the interpolated X and Y coordinates along with the total distance covered by the original coordinates\n    return interpolated_X, interpolated_Y, cumulative_distances[-1]\n\n", "CodeBleu_score": 15.34067923097966}, {"code": "def interpolate(X, Y, T)\n\t# Calculate the distances between consecutive points\n    distances = np.sqrt(np.sum(np.diff(X, axis=0)**2 + np.diff(Y, axis=0)**2, axis=1))\n    # Calculate the cumulative distances\n    cum_distances = np.insert(np.cumsum(distances), 0, 0)\n    # Get the distances covered by the original coordinates\n    total_distance = cum_distances[-1]\n    # Interpolate the X and Y coordinates\n    X_interp = np.interp(T, cum_distances, X)\n    Y_interp = np.interp(T, cum_distances, Y)\n    return X_interp, Y_interp, total_distance\n\n", "CodeBleu_score": 22.048409869942997}, {"code": "def interpolate(X, Y, T)\n\t# Compute the distances between consecutive points\n    D = np.sqrt((X[1:] - X[:-1])**2 + (Y[1:] - Y[:-1])**2)\n    # Compute the cumulative distances\n    D = np.concatenate(([0], np.cumsum(D)))\n    # Find the indices of the two consecutive points for each value of T\n    idx = np.searchsorted(D, T)\n    # Compute the X and Y coordinates using linear interpolation\n    x = X[idx] + (T - D[idx-1]) * (X[idx] - X[idx-1]) / (D[idx] - D[idx-1])\n    y = Y[idx] + (T - D[idx-1]) * (Y[idx] - Y[idx-1]) / (D[idx] - D[idx-1])\n    # Return the X and Y coordinates along with the total distance\n    return x, y, D[-1]", "CodeBleu_score": 27.56385202062014}]}
{"instance_id": "870a2dad3c66", "generation_result": [{"code": "def mandelbrot_set(xmin, xmax, ymin, ymax, xn, yn, maxiter, horizon=2.0)\n\t# Create a grid of complex numbers C from the specified ranges\n    x = np.linspace(xmin, xmax, xn, dtype=np.float32)\n    y = np.linspace(ymin, ymax, yn, dtype=np.float32)\n    C = x + 1j*y[:,None]\n\n    # Initialize iteration count array N and complex array Z\n    N = np.zeros(C.shape, dtype=np.int32)\n    Z = np.zeros(C.shape, dtype=np.complex64)\n\n    # Iteratively compute the Mandelbrot sequence, updating N with the iteration count where the magnitude of Z exceeds the horizon\n    for n in range(maxiter):\n        Z[N==0] = Z[N==0]**2 + C[N==0]\n        N[N==0] += 1\n        N[np.abs(Z) > horizon] = 0\n\n    return Z, N\n\n", "CodeBleu_score": 44.1324023126349}, {"code": "def mandelbrot_set(xmin, xmax, ymin, ymax, xn, yn, maxiter, horizon=2.0)\n\t# Create a grid of complex numbers\n    x = np.linspace(xmin, xmax, xn, dtype=np.float32)\n    y = np.linspace(ymin, ymax, yn, dtype=np.float32)\n    c = x + 1j*y[:, None]\n\n    # Initialize iteration count array and complex array\n    N = np.zeros(c.shape, dtype=np.int32)\n    Z = np.zeros(c.shape, dtype=np.complex64)\n\n    # Compute the Mandelbrot set\n    for n in range(maxiter):\n        Z[N == 0] = Z[N == 0]**2 + c[N == 0]\n        N[N == 0] += 1\n        # Escape radius\n        N[np.abs(Z) > horizon] = 0\n\n    return Z, N\n\n", "CodeBleu_score": 42.33973600050825}, {"code": "def mandelbrot_set(xmin, xmax, ymin, ymax, xn, yn, maxiter, horizon=2.0)\n\t# Create a grid of complex numbers C from the specified ranges\n    C = np.linspace(xmin, xmax, xn) + 1j*np.linspace(ymin, ymax, yn)\n    # Initialize iteration count array N and complex array Z\n    N = np.zeros(C.shape, dtype=int)\n    Z = np.zeros(C.shape, dtype=complex)\n    # Iteratively compute the Mandelbrot sequence, updating N with the iteration count where the magnitude of Z exceeds the horizon\n    for n in range(maxiter):\n        Z[N==0] = Z[N==0]**2 + C[N==0]\n        N[N==0] += 1\n        N[np.abs(Z)>=horizon] = 0\n    # Points that remain bounded after maxiter iterations are set to 0 in N\n    N[N==maxiter] = 0\n    return Z, N", "CodeBleu_score": 42.42336093890531}]}
{"instance_id": "e1a908945acf", "generation_result": [{"code": "def smooth2d(A, sigma=3)\n\twindow_len = int(2 * sigma + 1)\n    window_len = window_len if window_len % 2 == 1 else window_len + 1\n    A = smooth1d(A, window_len)\n    A = A.T\n    A = smooth1d(A, window_len)\n    A = A.T\n    return A\n\n", "CodeBleu_score": 22.276423579323936}, {"code": "def smooth2d(A, sigma=3)\n\twindow_len = int(2 * np.ceil(sigma) + 1)\n    window_len = window_len if window_len % 2 == 1 else window_len + 1\n    A = smooth1d(A, window_len)\n    A = A.T\n    A = smooth1d(A, window_len)\n    A = A.T\n    return A\n\n", "CodeBleu_score": 26.695148487096276}, {"code": "def smooth2d(A, sigma=3)\n\twindow_len = int(2 * np.ceil(sigma) + 1)\n    A = smooth1d(A, window_len)\n    A = A.T\n    A = smooth1d(A, window_len)\n    A = A.T\n    return A\n\n", "CodeBleu_score": 23.235826700801447}]}
{"instance_id": "94563c54b483", "generation_result": [{"code": "def process_image(self, padded_src, dpi)\n\t# print(\"process_image\")\n    # print(\"ox:\", self.ox)\n    # print(\"oy:\", self.oy)\n    # print(\"dpi:\", dpi)\n    # print(\"padded_src.shape:\", padded_src.shape)\n    # print(\"padded_src.dtype:\", padded_src.dtype)\n    # print(\"padded_src:\", padded_src)\n\n    # ox = int(self.ox * dpi)\n    # oy = int(self.oy * dpi)\n    # print(\"ox:\", ox)\n    # print(\"oy:\", oy)\n\n    # # shift the image\n    # padded_src = np.roll(padded_src, ox, axis=1)\n    # padded_src = np.roll(padded_src, oy, axis=0)\n    # print(\"padded_src:\", padded_src)\n\n    # return padded_src", "CodeBleu_score": 2.9609929078014185}, {"code": "def process_image(self, padded_src, dpi)\n\timport numpy as np\n    # get the offsets from the instance's attributes\n    ox = self.ox\n    oy = self.oy\n    # convert the offsets to pixels\n    ox = int(ox * dpi)\n    oy = int(oy * dpi)\n    # shift the image horizontally and vertically by the offsets\n    # the offsets are converted to pixels\n    padded_src = np.roll(padded_src, shift=(ox, oy), axis=(1, 0))\n    return padded_src\n\n", "CodeBleu_score": 23.530776014284026}, {"code": "def process_image(self, padded_src, dpi)\n\tpixels_per_inch = dpi\n    pixels_per_mm = pixels_per_inch / 25.4\n    ox_px = int(round(self.ox * pixels_per_mm))\n    oy_px = int(round(self.oy * pixels_per_mm))\n    return np.roll(padded_src, (oy_px, ox_px), axis=(0, 1))\n\n", "CodeBleu_score": 21.40957663236519}]}
{"instance_id": "d71d232872fe", "generation_result": [{"code": "def gaussian(shape=(25, 25), width=0.5, center=0.0)\n\tshape = (shape,) if isinstance(shape, int) else tuple(shape)\n    width = (width,) * len(shape) if isinstance(width, int) else tuple(width)\n    center = (center,) * len(shape) if isinstance(center, int) else tuple(center)\n    grid = np.meshgrid(*[np.arange(s) for s in shape], indexing='ij')\n    grid = np.stack(grid, axis=-1)\n    grid = np.float64(grid)\n    width = np.float64(width)\n    center = np.float64(center)\n    return np.exp(-((grid - center) / width) ** 2 / 2)", "CodeBleu_score": 31.14235657909914}, {"code": "def gaussian(shape=(25, 25), width=0.5, center=0.0)\n\tshape = tuple(shape)\n    width = tuple(width)\n    center = tuple(center)\n\n    grid = np.mgrid[[slice(0, s, 1) for s in shape]]\n    grid = np.reshape(grid, (len(shape), -1))\n    grid = grid.T\n\n    A = ((grid - center) / width) ** 2\n    A = A.sum(axis=1)\n    A = np.exp(-0.5 * A)\n\n    return A.reshape(shape)\n\n", "CodeBleu_score": 21.17869841008682}, {"code": "def gaussian(shape=(25, 25), width=0.5, center=0.0)\n\tshape = tuple(shape)\n    width = tuple(width)\n    center = tuple(center)\n    grid = np.meshgrid(*[np.arange(0, s) for s in shape], indexing='ij')\n    grid = np.stack(grid, axis=-1)\n    grid = grid.reshape(-1, len(shape))\n    grid -= center\n    grid /= width\n    grid = np.sum(grid**2, axis=-1)\n    grid = np.exp(-0.5 * grid)\n    grid = grid.reshape(shape)\n    return grid\n\n", "CodeBleu_score": 21.954249001113464}]}
{"instance_id": "7bbd08c987bc", "generation_result": [{"code": "def stimulus(position, size, intensity)\n\t# convert polar coordinates to Cartesian coordinates\n    x = position[0] * np.cos(position[1])\n    y = position[0] * np.sin(position[1])\n\n    # normalize and center the coordinates\n    x = (x + 1) / 2\n    y = (y + 1) / 2\n\n    # create a mesh grid representing the spatial domain\n    x_grid, y_grid = np.meshgrid(np.linspace(0, 1, 100), np.linspace(0, 1, 100))\n\n    # compute the squared Euclidean distance (R) from each point in the grid to the stimulus center\n    r = ((x - x_grid) ** 2 + (y - y_grid) ** 2)\n\n    # return an exponential decay function of these distances, scaled by the size parameter\n    return np.exp(-r / (2 * (size ** 2))) * intensity\n\n", "CodeBleu_score": 19.742070135206607}, {"code": "def stimulus(position, size, intensity)\n\t# convert polar coordinates to Cartesian coordinates\n    x = position[0] * np.cos(position[1])\n    y = position[0] * np.sin(position[1])\n    # normalize coordinates\n    x = (x + 1) / 2\n    y = (y + 1) / 2\n    # create a mesh grid representing the spatial domain, normalized and centered\n    x_grid, y_grid = np.meshgrid(np.linspace(0, 1, 100), np.linspace(0, 1, 100))\n    # compute the squared Euclidean distance (R) from each point in the grid to the stimulus center\n    r_squared = (x_grid - x) ** 2 + (y_grid - y) ** 2\n    # return an exponential decay function of these distances, scaled by the size parameter\n    return intensity * np.exp(-r_squared / size ** 2)", "CodeBleu_score": 18.987966091556864}, {"code": "def stimulus(position, size, intensity)\n\tx, y = position\n    x_grid, y_grid = np.meshgrid(np.linspace(-1, 1, size), np.linspace(-1, 1, size))\n    R = np.sqrt((x_grid - x) ** 2 + (y_grid - y) ** 2)\n    return intensity * np.exp(-R ** 2 / (size / 10) ** 2)\n\n", "CodeBleu_score": 19.505575678120664}]}
{"instance_id": "2b5806238d5d", "generation_result": [{"code": "def plot(ax, text)\n\tax.set_xlim(0, 1)\n    ax.set_ylim(0, 1)\n    ax.set_xticks([0, 0.5, 1])\n    ax.set_yticks([0, 0.5, 1])\n    ax.set_xlabel(\"x\", fontsize=16)\n    ax.set_ylabel(\"y\", fontsize=16)\n    ax.text(0.5, 0.5, text, fontsize=16, horizontalalignment=\"center\", verticalalignment=\"center\")\n    ax.set_title(\"Plot Title\", fontsize=16, fontfamily=\"serif\", fontweight=\"bold\")\n\n\n", "CodeBleu_score": 32.92721101696189}, {"code": "def plot(ax, text)\n\tax.set_xlim(0, 1)\n    ax.set_ylim(0, 1)\n    ax.set_xticks([0, 1])\n    ax.set_yticks([0, 1])\n    ax.set_xlabel('X')\n    ax.set_ylabel('Y')\n    ax.text(0.5, 0.5, text, ha='center', va='center', fontsize=30, fontweight='bold')\n    ax.set_title('Title', fontfamily='serif', fontweight='bold')", "CodeBleu_score": 41.65616518335251}, {"code": "def plot(ax, text)\n\tax.set_xlim(0, 1)\n    ax.set_ylim(0, 1)\n    ax.set_xticks([0, 1])\n    ax.set_yticks([0, 1])\n    ax.set_xlabel('X')\n    ax.set_ylabel('Y')\n    ax.text(0.5, 0.5, text, horizontalalignment='center', verticalalignment='center', fontsize=12, fontweight='bold')\n    ax.set_title('Title', fontfamily='serif', fontweight='bold')\n\n", "CodeBleu_score": 41.94905513337237}]}
{"instance_id": "0b42aac61e21", "generation_result": [{"code": "def predict_cumulative_hazard_function(self, X, **kwargs)\n\tXt = self.transform(X, **kwargs)\n    return self.steps[-1][-1].predict_cumulative_hazard_function(Xt, **kwargs)\n\n", "CodeBleu_score": 21.660299952763772}, {"code": "def predict_cumulative_hazard_function(self, X, **kwargs)\n\treturn self.predict(X, **kwargs)\n\n\n", "CodeBleu_score": 6.0905565474746375}, {"code": "def predict_cumulative_hazard_function(self, X, **kwargs)\n\tXt = self.transform(X)\n    return self.steps[-1][-1].predict_cumulative_hazard_function(Xt, **kwargs)\n\n", "CodeBleu_score": 20.745140761663585}]}
{"instance_id": "c8439e4918ad", "generation_result": [{"code": "def predict_survival_function(self, X, **kwargs)\n\treturn self.steps[-1][-1].predict_survival_function(self.transform(X), **kwargs)\n\n\n", "CodeBleu_score": 14.883865946533339}, {"code": "def predict_survival_function(self, X, **kwargs)\n\tXt = self.transform(X)\n    return self.steps[-1][-1].predict_survival_function(Xt, **kwargs)\n\n", "CodeBleu_score": 19.907528630225745}, {"code": "def predict_survival_function(self, X, **kwargs)\n\tX = self._preprocess(X)\n    return self._final_estimator.predict_survival_function(X, **kwargs)\n\n", "CodeBleu_score": 14.779373878020447}]}
{"instance_id": "216662ca7db4", "generation_result": [{"code": "def _check_success(self, results)\n\t# Check success\n    if results['success']:\n        return\n    else:\n        if results['status'] == 1:\n            # Max iterations reached\n            print('Max iterations reached.')\n        elif results['status'] == 2:\n            # Primal infeasible\n            raise RuntimeError('Primal infeasible.')\n        elif results['status'] == 3:\n            # Dual infeasible\n            raise RuntimeError('Dual infeasible.')\n        else:\n            # Unknown exit flag\n            raise RuntimeError('Unknown exit flag: %s' % results['status'])\n\n", "CodeBleu_score": 17.12725425766348}, {"code": "def _check_success(self, results)\n\tif results.solver.termination_condition == TerminationCondition.optimal:\n        return\n    elif results.solver.termination_condition == TerminationCondition.maxIterations:\n        warnings.warn('Solver terminated - maximum iterations reached.', RuntimeWarning)\n    elif results.solver.termination_condition == TerminationCondition.primalInfeasible:\n        raise RuntimeError('Solver terminated - primal infeasibility.')\n    elif results.solver.termination_condition == TerminationCondition.dualInfeasible:\n        raise RuntimeError('Solver terminated - dual infeasibility.')\n    else:\n        raise RuntimeError('Solver terminated - unknown exit status.')\n\n\n", "CodeBleu_score": 31.23444573439099}, {"code": "def _check_success(self, results)\n\tif results['success']:\n        return\n    if results['status'] == 1:\n        raise RuntimeError('Optimization terminated successfully.')\n    elif results['status'] == 2:\n        raise RuntimeError('Iteration limit reached.')\n    elif results['status'] == 3:\n        raise RuntimeError('Problem appears to be infeasible.')\n    elif results['status'] == 4:\n        raise RuntimeError('Problem appears to be unbounded.')\n    elif results['status'] == 5:\n        raise RuntimeError('Numerical difficulties encountered.')\n    else:\n        raise RuntimeError('Unknown exit flag.')\n\n", "CodeBleu_score": 19.452050522117005}]}
{"instance_id": "1fe2e379fca0", "generation_result": [{"code": "def _decompose(self, P)\n\t# Calculate eigenvalues and eigenvectors of P\n    eigenvalues, eigenvectors = np.linalg.eigh(P)\n\n    # Check if all eigenvalues are above the negative cutoff\n    if np.any(eigenvalues < -self.cutoff):\n        raise ValueError('Eigenvalues of P are below the negative cutoff.')\n\n    # Filter out eigenvalues below the positive cutoff\n    eigenvalues = eigenvalues[eigenvalues > self.cutoff]\n    eigenvectors = eigenvectors[:, eigenvalues > self.cutoff]\n\n    # Calculate condition number\n    condition_number = np.max(eigenvalues) / np.min(eigenvalues)\n    if condition_number > 10 ** 14:\n        raise ValueError('Condition number of P is too large.')\n\n    # Normalize eigenvalues\n    eigenvalues = eigenvalues / np.max(eigenvalues)\n\n    # Transpose eigenvectors\n    eigenvectors = eigenvectors.T\n\n    # Return transposed eigenvectors and largest eigenvalue\n    return eigenvectors, eigenvalues\n\n", "CodeBleu_score": 27.68721351982216}, {"code": "def _decompose(self, P)\n\t# Compute eigenvalues and eigenvectors\n    eigvals, eigvecs = np.linalg.eigh(P)\n    # Find largest eigenvalue\n    eigval_max = np.max(eigvals)\n    # Compute condition number\n    condition_number = eigval_max / (np.finfo(float).eps * P.shape[0])\n    # Check if all eigenvalues are above the negative cutoff\n    if np.min(eigvals) < -condition_number:\n        raise Exception(\"Eigenvalues are below the negative cutoff.\")\n    # Filter out eigenvalues below the positive cutoff\n    eigvals = np.where(eigvals > condition_number, eigvals, 0)\n    # Normalize eigenvalues\n    eigvals = eigvals / eigval_max\n    # Compute decomposed matrix\n    P_decomp = eigvecs @ np.diag(eigvals) @ eigvecs.T\n    return P_decomp.T, eigval_max\n\n", "CodeBleu_score": 23.126471653510517}, {"code": "def _decompose(self, P)\n\t# calculate eigenvalues and eigenvectors of P\n    eigvals, eigvecs = np.linalg.eigh(P)\n    # identify largest eigenvalue\n    max_eigval = np.max(eigvals)\n    # calculate condition number\n    cond_num = max_eigval / np.finfo(float).eps * np.max(P.shape)\n    # filter out eigenvalues below negative cutoff\n    eigvals = eigvals[eigvals > -cond_num]\n    # normalize eigenvalues\n    eigvals = eigvals / max_eigval\n    # calculate decomposed matrix\n    P_decomp = np.dot(eigvecs, np.dot(np.diag(eigvals), np.linalg.inv(eigvecs)))\n    # return decomposed matrix and largest eigenvalue\n    return P_decomp.T, max_eigval\n\n", "CodeBleu_score": 17.61509136557466}]}
{"instance_id": "20c3f5d05d79", "generation_result": [{"code": "def _update_coef(self, coef, D)\n\tself.coef_ = coef[D > self.threshold] * D[D > self.threshold]\n\n", "CodeBleu_score": 19.617225017697965}, {"code": "def _update_coef(self, coef, D)\n\tself.coef_ = np.zeros(self.n_features_)\n    self.coef_[coef > self.tol] = coef[coef > self.tol] * D[coef > self.tol]\n\n", "CodeBleu_score": 20.42144868306305}, {"code": "def _update_coef(self, coef, D)\n\t# Select coefficients greater than a small threshold\n    coef = coef[coef > 1e-8]\n    # Scale coefficients by corresponding elements in D\n    coef = coef * D[coef.index]\n    # Store the updated coefficients in the model's coef attribute\n    self.coef_ = coef", "CodeBleu_score": 22.862335257206322}]}
{"instance_id": "37c347d00d35", "generation_result": [{"code": "def gradient(self, y_true, raw_prediction, sample_weight=None, **kwargs)\n\t# Check if the input is a pd.DataFrame or pd.Series\n    if isinstance(y_true, pd.DataFrame):\n        y_true = y_true.iloc[:, 0]\n    elif isinstance(y_true, pd.Series):\n        y_true = y_true\n    else:\n        raise TypeError(\"The input for y_true must be a pd.DataFrame or pd.Series\")\n\n    # Check if the input is a pd.DataFrame or pd.Series\n    if isinstance(raw_prediction, pd.DataFrame):\n        raw_prediction = raw_prediction.iloc[:, 0]\n    elif isinstance(raw_prediction, pd.Series):\n        raw_prediction = raw_prediction\n    else:\n        raise TypeError(\"The input for raw_prediction must be a pd.DataFrame or pd.Series\")\n\n    # Check if the input is a pd.DataFrame or pd.Series\n    if isinstance(sample_weight, pd.DataFrame):\n        sample_weight = sample_weight.iloc[:, 0]\n    elif isinstance(sample_weight, pd.Series):\n        sample_weight = sample_weight\n    elif sample_weight is None:\n        sample_weight = pd.Series(np.ones(len(y_true)))\n    else:\n        raise TypeError(\"The input for sample_weight must be a pd.DataFrame, pd.Series, or None\")\n\n    # Check if the length of y_true, raw_prediction, and sample_weight are the same\n    if len(y_true) != len(raw_prediction) or len(y_true) != len(sample_weight):\n        raise ValueError(\"The length of y_true, raw_prediction, and sample_weight must be the same\")\n\n    # Check if the values of y_true are only 0 and 1\n    if not all(i in [0, 1] for i in y_true):\n        raise ValueError(\"The values of y_true must be only 0 and 1\")\n\n    # Calculate the negative gradient\n    gradient = (-y_true + np.exp(-raw_prediction)) * sample_weight\n\n    return gradient", "CodeBleu_score": 32.15084813335005}, {"code": "def gradient(self, y_true, raw_prediction, sample_weight=None, **kwargs)\n\t# TODO: add docstring\n    # TODO: add example\n    # TODO: add tests\n\n    # check inputs\n    if sample_weight is None:\n        sample_weight = np.ones(y_true.shape[0])\n\n    # compute gradient\n    gradient = - (y_true - np.exp(-raw_prediction))\n    gradient *= sample_weight[:, np.newaxis]\n\n    return gradient\n\n", "CodeBleu_score": 40.12979658854328}, {"code": "def gradient(self, y_true, raw_prediction, sample_weight=None, **kwargs)\n\t# Check that y_true and raw_prediction have the same shape\n    if y_true.shape != raw_prediction.shape:\n        raise ValueError(\"y_true and raw_prediction must have the same shape\")\n\n    # Check that y_true is a 1-D array\n    if len(y_true.shape) != 1:\n        raise ValueError(\"y_true must be a 1-D array\")\n\n    # Check that y_true is binary\n    if not np.all(y_true == 0) or not np.all(y_true == 1):\n        raise ValueError(\"y_true must be binary\")\n\n    # Check that raw_prediction is a 1-D array\n    if len(raw_prediction.shape) != 1:\n        raise ValueError(\"raw_prediction must be a 1-D array\")\n\n    # Check that sample_weight is a 1-D array\n    if sample_weight is not None and len(sample_weight.shape) != 1:\n        raise ValueError(\"sample_weight must be a 1-D array\")\n\n    # Check that sample_weight has the same length as y_true\n    if sample_weight is not None and len(y_true) != len(sample_weight):\n        raise ValueError(\"sample_weight must have the same length as y_true\")\n\n    # Check that y_true and raw_prediction have the same length\n    if len(y_true) != len(raw_prediction):\n        raise ValueError(\"y_true and raw_prediction must have the same length\")\n\n    # Calculate the gradient\n    gradient = -y_true * np.exp(raw_prediction)\n\n    # Apply sample weights\n    if sample_weight is not None:\n        gradient = gradient * sample_weight\n\n    return gradient\n\n", "CodeBleu_score": 34.83213176682125}]}
{"instance_id": "6be432d27e52", "generation_result": [{"code": "def _compute_missing_values_in_feature_mask(self, X, estimator_name=None)\n\tif self._support_missing_values(X):\n        mask = np.isnan(X).any(axis=0)\n        if not np.any(mask):\n            return None\n        else:\n            return mask\n    else:\n        # Check if there are infinite values\n        if not np.isfinite(X).all():\n            raise ValueError(\"Estimator %s doesn't support infinite values in %s\"\n                             % (estimator_name, X))\n        return None\n\n", "CodeBleu_score": 28.049351895607305}, {"code": "def _compute_missing_values_in_feature_mask(self, X, estimator_name=None)\n\tif not self._support_missing_values(X):\n        check_array(X, estimator_name, allow_nan=True, force_all_finite=True)\n        return None\n    missing_mask = np.isnan(np.sum(X, axis=0))\n    if np.any(missing_mask):\n        missing_columns = np.arange(X.shape[1])[missing_mask]\n        if estimator_name is not None:\n            warnings.warn(\n                \"Estimator \"\n                + estimator_name\n                + \" cannot handle missing values. The features corresponding to \"\n                + \"missing values will be dropped.\",\n                UserWarning,\n            )\n        return missing_mask\n    return None\n\n", "CodeBleu_score": 41.85926311526441}, {"code": "def _compute_missing_values_in_feature_mask(self, X, estimator_name=None)\n\tif not self._support_missing_values(X):\n        # X does not support missing values\n        _assert_all_finite(X, allow_nan=True, estimator_name=estimator_name)\n        return None\n\n    missing_mask = np.isnan(X).sum(axis=0)\n    missing_count = missing_mask.sum()\n    if missing_count == 0:\n        return None\n\n    if not np.isfinite(X).all():\n        # X contains infinite values\n        raise ValueError(\n            \"Estimator %s cannot accept infinite values as input\" % estimator_name\n        )\n\n    if not np.isfinite(missing_mask).all():\n        # X contains infinite values\n        raise ValueError(\n            \"Estimator %s cannot accept infinite values as input\" % estimator_name\n        )\n\n    return missing_mask\n\n\n\n", "CodeBleu_score": 43.373280675025896}]}
{"instance_id": "dc5bda895a07", "generation_result": [{"code": "def predict_survival_function(self, X, check_input=True, return_array=False)\n\tself._check_low_memory: heck_low_memory(self, function):\n        \"\"\"Check if `function` is supported in low memory mode and throw if it is not.\"\"\"\n        if self.low_memory:\n            raise NotImplementedError(\n                f\"{function} is not implemented in low memory mode.\"\n                + \" run fit with low_memory=False to disable low memory mode.\"\n            )\n\n    ", "CodeBleu_score": 21.37066465265666}, {"code": "def predict_survival_function(self, X, check_input=True, return_array=False)\n\tself._check_low_memory: heck_low_memory(self, function):\n        \"\"\"Check if `function` is supported in low memory mode and throw if it is not.\"\"\"\n        if self.low_memory:\n            raise NotImplementedError(\n                f\"{function} is not implemented in low memory mode.\"\n                + \" run fit with low_memory=False to disable low memory mode.\"\n            )\n\n    self._validate_X_predict: alidate_X_predict(self, X, check_input, accept_sparse=\"csr\"):\n        \"\"\"Validate X whenever one tries to predict\"\"\"\n        if check_input:\n            if self._support_missing_values(X):\n                force_all_finite = \"allow-nan\"\n            else:\n                force_all_finite = True\n            X = self._validate_data(\n                X,\n                dtype=DTYPE,\n                accept_sparse=accept_sparse,\n                reset=False,\n                force_all_finite=force_all_finite,\n            )\n        else:\n            # The number of features is checked regardless of `check_input`\n            self._check_n_features(X, reset=False)\n\n        return X\n\n    if self.low_memory:\n        check_is_fitted(self, \"tree_\")\n        X = self._validate_X_predict(X, check_input, accept_sparse=\"csr\")\n        pred = self.tree_.predict(X)\n        return pred[..., 1]\n\n    chf = self.predict_cumulative_hazard_function(X, check_input, return_array=True)\n    if return_array:\n        return 1 - chf[:, self.is_event_time_]\n    return _array_to_step_function(self.time_points_, 1 - chf[:, self.is_event_time_])\n\n", "CodeBleu_score": 54.359562492425404}, {"code": "def predict_survival_function(self, X, check_input=True, return_array=False)\n\tself._check_low_memory: heck_low_memory(self, function):\n        \"\"\"Check if `function` is supported in low memory mode and throw if it is not.\"\"\"\n        if self.low_memory:\n            raise NotImplementedError(\n                f\"{function} is not implemented in low memory mode.\"\n                + \" run fit with low_memory=False to disable low memory mode.\"\n            )\n\n    ", "CodeBleu_score": 21.37066465265666}]}
{"instance_id": "84ae915b5933", "generation_result": [{"code": "def _pre_fit(self, X, y)\n\tif not isinstance(X, pd.DataFrame):\n        raise ValueError(\"X must be a pandas dataframe\")\n    if not isinstance(y, pd.DataFrame):\n        raise ValueError(\"y must be a pandas dataframe\")\n    if X.shape[0] < 2:\n        raise ValueError(\"X must contain at least 2 samples\")\n    if y.shape[0] < 2:\n        raise ValueError(\"y must contain at least 2 samples\")\n    if X.shape[0] != y.shape[0]:\n        raise ValueError(\"X and y must have the same number of samples\")\n    if X.shape[1] < 1:\n        raise ValueError(\"X must contain at least 1 feature\")\n    if y.shape[1] != 1:\n        raise ValueError(\"y must contain exactly 1 feature\")\n    if not np.issubdtype(X.dtypes[0], np.number):\n        raise ValueError(\"X must contain only numeric features\")\n    if not np.issubdtype(y.dtypes[0], np.number):\n        raise ValueError(\"y must contain only numeric features\")\n    if (y.iloc[:, 0] < 0).any():\n        raise ValueError(\"y must not contain any negative values\")\n    if (y.iloc[:, 0] == 0).all():\n        raise ValueError(\"y must contain at least one non-zero value\")\n    if not np.isfinite(X).all().all():\n        raise ValueError(\"X must contain only finite values\")\n    if not np.isfinite(y).all().all():\n        raise ValueError(\"y must contain only finite values\")\n\n    X = X.astype(np.float64)\n    y = y.astype(np.float64)\n\n    if self.normalize:\n        X_mean = X.mean()\n        X_std = X.std()\n        X = (X - X_mean) / X_std\n    else:\n        X_mean = 0\n        X_std = 1\n\n    y_events = (y.iloc[:, 0] == 0).astype(", "CodeBleu_score": 17.677384246241118}, {"code": "def _pre_fit(self, X, y)\n\tif not isinstance(X, pd.DataFrame):\n        raise ValueError(\"X must be a pandas DataFrame\")\n\n    if not isinstance(y, pd.DataFrame):\n        raise ValueError(\"y must be a pandas DataFrame\")\n\n    if not isinstance(self.alpha, float) and not isinstance(self.alpha, int):\n        raise ValueError(\"alpha must be a float or an integer\")\n\n    if not isinstance(self.beta, float) and not isinstance(self.beta, int):\n        raise ValueError(\"beta must be a float or an integer\")\n\n    if not isinstance(self.gamma, float) and not isinstance(self.gamma, int):\n        raise ValueError(\"gamma must be a float or an integer\")\n\n    if not isinstance(self.epsilon, float) and not isinstance(self.epsilon, int):\n        raise ValueError(\"epsilon must be a float or an integer\")\n\n    if not isinstance(self.eta, float) and not isinstance(self.eta, int):\n        raise ValueError(\"eta must be a float or an integer\")\n\n    if not isinstance(self.lambda_1, float) and not isinstance(self.lambda_1, int):\n        raise ValueError(\"lambda_1 must be a float or an integer\")\n\n    if not isinstance(self.lambda_2, float) and not isinstance(self.lambda_2, int):\n        raise ValueError(\"lambda_2 must be a float or an integer\")\n\n    if not isinstance(self.lambda_3, float) and not isinstance(self.lambda_3, int):\n        raise ValueError(\"lambda_3 must be a float or an integer\")\n\n    if not isinstance(self.lambda_4, float) and not isinstance(self.lambda_4, int):\n        raise ValueError(\"lambda_4 must be a float or an integer\")\n\n    if not isinstance(self.lambda_5, float) and not isinstance(self.lambda_5, int):\n        raise ValueError(\"lambda_5 must be a float or an integer\")\n\n    if not isinstance(self.lambda_6, float) and not isinstance(self.lambda_6", "CodeBleu_score": 11.68076983073377}, {"code": "def _pre_fit(self, X, y)\n\tX, y = self._validate_data(X, y, multi_output=True)\n    self.n_samples_, self.n_features_ = X.shape\n    self.classes_, y = np.unique(y, return_inverse=True)\n    self.n_classes_ = self.classes_.size\n    self.event_counts_ = np.bincount(y)\n    self.n_events_ = np.sum(self.event_counts_)\n    self.event_class_counts_ = np.bincount(y, weights=self.event_counts_)\n    self.event_class_counts_ /= self.event_class_counts_.sum()\n    self.n_non_events_ = self.n_samples_ - self.n_events_\n    self.n_censors_ = self.n_non_events_ - np.sum(self.event_counts_ - self.event_class_counts_ * self.n_samples_)\n    self.n_censors_ = int(self.n_censors_)\n    self.n_unique_events_ = np.unique(y).size\n    self.event_observed_ = y.astype(np.bool)\n    self.event_time_ = X[:, 0]\n    self.event_indices_ = np.argsort(self.event_time_, kind='mergesort')\n    self.event_times_ = self.event_time_[self.event_indices_]\n    self.event_observed_ = self.event_observed_[self.event_indices_]\n    self.n_samples_ = self.event_time_.shape[0]\n    self.n_features_ = self.event_time_.shape[1]\n    self.feature_matrix_ = X.copy()\n    self.feature_matrix_ = self.feature_matrix_[self.event_indices_, :]\n    self.feature_matrix_ = self.feature_matrix_.astype(np.float64)\n    self.feature_means_ = self.feature_matrix_.mean(axis=0)", "CodeBleu_score": 18.015209399394458}]}
{"instance_id": "c70bb013f85f", "generation_result": [{"code": "def _check_alphas(self)\n\tif self.alphas is None:\n        self.alphas = np.array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95, 0.99, 0.999, 0.9999, 0.99999])\n        new_path = True\n    else:\n        if not isinstance(self.alphas, Iterable):\n            raise TypeError('alphas must be iterable')\n        if not isinstance(self.alphas, np.ndarray):\n            self.alphas = np.array(self.alphas)\n        if len(self.alphas.shape) > 1:\n            raise ValueError('alphas must be 1D')\n        if self.alphas.shape[0] == 0:\n            raise ValueError('alphas must contain at least one value')\n        if not np.isfinite(self.alphas).all():\n            raise ValueError('alphas must all be finite')\n        if (self.alphas < 0).any():\n            raise ValueError('alphas must all be non-negative')\n        new_path = False\n    return self.alphas, new_path\n\n", "CodeBleu_score": 37.15764251421625}, {"code": "def _check_alphas(self)\n\tif self.alphas is None:\n        # Create a new alpha path\n        self.alphas = np.linspace(0, 1, 20)\n        new_path = True\n    else:\n        # Use an existing alpha path\n        self.alphas = np.atleast_1d(self.alphas)\n        if not np.all(np.isfinite(self.alphas)):\n            raise ValueError(\"All alpha values must be finite.\")\n        if np.any(self.alphas < 0):\n            raise ValueError(\"All alpha values must be non-negative.\")\n        new_path = False\n    return self.alphas, new_path\n\n", "CodeBleu_score": 33.61221822621059}, {"code": "def _check_alphas(self)\n\tif self.alphas is None:\n        self.alphas = np.linspace(0.01, 0.99, 50)\n        new_path = True\n    else:\n        self.alphas = np.array(self.alphas)\n        if self.alphas.ndim != 1:\n            raise ValueError(\"alphas must be a 1D array\")\n        if self.alphas.size == 0:\n            raise ValueError(\"alphas must be non-empty\")\n        if np.any(~np.isfinite(self.alphas)):\n            raise ValueError(\"alphas must be finite\")\n        if np.any(self.alphas < 0):\n            raise ValueError(\"alphas must be non-negative\")\n        new_path = False\n\n    return self.alphas, new_path\n\n", "CodeBleu_score": 32.32701616759391}]}
{"instance_id": "7b54a0554ba4", "generation_result": [{"code": "def nelson_aalen_estimator(event, time)\n\t# validate event\n    event = np.asarray(event, dtype=bool)\n    if event.ndim != 1:\n        raise ValueError(\"event must be a 1-d array.\")\n\n    # validate time\n    time = np.asarray(time, dtype=float)\n    if time.ndim != 1:\n        raise ValueError(\"time must be a 1-d array.\")\n    if time.shape[0] != event.shape[0]:\n        raise ValueError(\"time and event must have the same length.\")\n\n    # compute counts\n    times, n_events, n_at_risk, _ = _compute_counts(event, time)\n\n    # compute cumulative hazard\n    hazard = n_events / n_at_risk\n    hazard = np.r_[0, hazard]\n    cum_hazard = np.cumsum(hazard)\n\n    return times, cum_hazard\n\n", "CodeBleu_score": 42.04961065196146}, {"code": "def nelson_aalen_estimator(event, time)\n\tevent, time = _validate_inputs(event, time)\n    times, n_events, n_at_risk, _ = _compute_counts(event, time)\n\n    # calculate cumulative hazard\n    hazard = n_events / n_at_risk\n    cumulative_hazard = np.cumsum(hazard)\n\n    return times, cumulative_hazard\n\n", "CodeBleu_score": 35.213157820238074}, {"code": "def nelson_aalen_estimator(event, time)\n\tevent, time = _validate_inputs(event, time)\n    times, n_events, n_at_risk, _ = _compute_counts(event, time)\n    cumulative_hazard = np.cumsum(n_events / n_at_risk)\n    return times, cumulative_hazard\n\n", "CodeBleu_score": 39.11105757052884}]}
{"instance_id": "35af806300cc", "generation_result": [{"code": "def ipc_weights(event, time)\n\t# Check if all events have occurred\n    if np.all(event):\n        return np.ones(event.shape[0])\n\n    # Otherwise, compute Kaplan-Meier estimator for event and time data in reverse\n    else:\n        time_rev = time[::-1]\n        event_rev = event[::-1]\n        time_rev, prob_survival, _ = kaplan_meier_estimator(event_rev, time_rev, reverse=True)\n\n    # Find the index positions of event times within the unique time values\n    event_times = time[event]\n    index = np.searchsorted(time_rev, event_times)\n\n    # Retrieve the corresponding survival probabilities\n    prob_survival = prob_survival[index]\n\n    # Ensure all retrieved probabilities are positive\n    prob_survival = np.clip(prob_survival, 1e-10, None)\n\n    # Create a weights array, assigning inverse probabilities to events\n    weights = 1 / prob_survival\n\n    return weights\n\n", "CodeBleu_score": 39.343233490521136}, {"code": "def ipc_weights(event, time)\n\t# Check if all events have occurred\n    if np.all(event):\n        return np.ones(len(event))\n\n    # Calculate Kaplan-Meier estimator for event and time data in reverse\n    uniq_times, prob_survival, _ = kaplan_meier_estimator(\n        event, time, reverse=True, conf_type=None, conf_level=0.95\n    )\n\n    # Find index positions of event times within unique time values\n    event_index = np.searchsorted(uniq_times, time)\n\n    # Retrieve corresponding survival probabilities\n    prob_survival = prob_survival[event_index]\n\n    # Ensure retrieved probabilities are positive\n    prob_survival = np.clip(prob_survival, 0, np.inf)\n\n    # Create weights array, assigning inverse probabilities to events\n    weights = 1 / prob_survival\n\n    return weights\n\n", "CodeBleu_score": 36.916991614334734}, {"code": "def ipc_weights(event, time)\n\tif event.all():\n        return np.ones(len(event))\n\n    else:\n        km_times, km_survival = kaplan_meier_estimator(event, time, reverse=True)\n        event_times_index = np.searchsorted(km_times, time)\n        event_times_survival = km_survival[event_times_index]\n        event_times_survival[event_times_survival < 0] = 0\n        weights = 1 / event_times_survival\n\n        return weights\n\n", "CodeBleu_score": 36.686514399954284}]}
{"instance_id": "72b19985ea11", "generation_result": [{"code": "def get_dfs(\n        self,\n        dfs: Union[\n            pd.DataFrame, BaseConnector, List[Union[pd.DataFrame, BaseConnector]]\n        ],\n    )\n\tif isinstance(dfs, pd.DataFrame):\n        return [PandasConnector(dfs)]\n    elif isinstance(dfs, BaseConnector):\n        return [dfs]\n    elif isinstance(dfs, list):\n        dfs_list = []\n        for df in dfs:\n            if isinstance(df, pd.DataFrame):\n                dfs_list.append(PandasConnector(df))\n            elif isinstance(df, BaseConnector):\n                dfs_list.append(df)\n            elif isinstance(df, list):\n                dfs_list.extend(get_dfs(df))\n            else:\n                raise ValueError(\n                    \"Invalid data type. Accepted data types are pandas, modin, polars, and custom dataframes, as well as connector objects.\"\n                )\n        return dfs_list\n    else:\n        raise ValueError(\n            \"Invalid data type. Accepted data types are pandas, modin, polars, and custom dataframes, as well as connector objects.\"\n        )\n\n", "CodeBleu_score": 40.89682411021304}, {"code": "def get_dfs(\n        self,\n        dfs: Union[\n            pd.DataFrame, BaseConnector, List[Union[pd.DataFrame, BaseConnector]]\n        ],\n    )\n\tif isinstance(dfs, BaseConnector):\n        dfs = [dfs]\n    elif isinstance(dfs, pd.DataFrame):\n        dfs = [PandasConnector(dfs)]\n    elif isinstance(dfs, list):\n        dfs = [get_dfs(self, df) for df in dfs]\n    else:\n        raise ValueError(\n            f\"Invalid input type. Input must be a dataframe, connector, or list of dataframes or connectors. You provided {type(dfs)}.\"\n        )\n    return dfs", "CodeBleu_score": 18.9136901259249}, {"code": "def get_dfs(\n        self,\n        dfs: Union[\n            pd.DataFrame, BaseConnector, List[Union[pd.DataFrame, BaseConnector]]\n        ],\n    )\n\tif isinstance(dfs, (pd.DataFrame, BaseConnector)):\n        dfs = [dfs]\n    elif isinstance(dfs, list):\n        pass\n    else:\n        raise ValueError(\n            \"The input must be a single dataframe, a list of dataframes, or a list of connectors.\"\n        )\n    dfs_list = []\n    for df in dfs:\n        if isinstance(df, pd.DataFrame):\n            dfs_list.append(PandasConnector(df))\n        elif isinstance(df, BaseConnector):\n            dfs_list.append(df)\n        else:\n            raise ValueError(\n                \"The input must be a single dataframe, a list of dataframes, or a list of connectors.\"\n            )\n    return dfs_list\n\n", "CodeBleu_score": 33.33675509891221}]}
{"instance_id": "cf22fb696e90", "generation_result": [{"code": "def execute(self) -> pd.DataFrame\n\tif isinstance(self._data, pd.DataFrame):\n        return self._data\n\n    if self.config.cache_enabled:\n        df = self._cached()\n        if df is not None:\n            self._data = df\n            return self._data\n\n    self._data = self._fetch_data()\n\n    if self.config.cache_enabled:\n        self._data.to_csv(self._get_cache_path(), index=False)\n\n    return self._data\n\n", "CodeBleu_score": 43.69903586306161}, {"code": "def execute(self) -> pd.DataFrame\n\tif isinstance(self.data, pd.DataFrame):\n        return self.data\n\n    # If a cached version exists, read and return it.\n    cached = self._cached()\n    if cached is not None:\n        return pd.read_csv(cached)\n\n    # If no cached version exists, fetch the data and return it.\n    self.data = self._fetch_data()\n    self._cache_data()\n    return self.data\n\n", "CodeBleu_score": 50.95536270738673}, {"code": "def execute(self) -> pd.DataFrame\n\tif isinstance(self.data, pd.DataFrame):\n        return self.data\n\n    # Check if there is a cached version of the data.\n    cached_data = self._cached()\n    if cached_data is not None:\n        return pd.read_csv(cached_data)\n\n    # Fetch the data.\n    self.data = self._fetch_data()\n\n    # Cache the data.\n    self._cache_data()\n\n    return self.data\n\n", "CodeBleu_score": 50.852502200090996}]}
{"instance_id": "21df5ecea95f", "generation_result": [{"code": "def column_hash(self)\n\tif not isinstance(self._instance, pd.DataFrame):\n        self.execute()\n\n    column_names = \"|\".join(self._instance.columns.tolist())\n    formula = self._build_formula()\n    return hashlib.sha256(f\"{column_names}{formula}\".encode()).hexdigest()\n\n", "CodeBleu_score": 52.997995787796725}, {"code": "def column_hash(self)\n\tif isinstance(self._instance, pd.DataFrame):\n        df = self._instance\n    else:\n        df = self.execute()\n    column_names = \"|\".join(df.columns)\n    formula = self._build_formula()\n    combined_string = column_names + formula\n    hashed = hashlib.sha256(combined_string.encode()).hexdigest()\n    return hashed\n\n", "CodeBleu_score": 41.86857320778675}, {"code": "def column_hash(self)\n\tif isinstance(self._instance, pd.DataFrame):\n        column_string = \"|\".join(self._instance.columns)\n    else:\n        column_string = \"|\".join(self.execute().columns)\n\n    column_string += self._build_formula()\n    return hashlib.sha256(column_string.encode()).hexdigest()\n\n", "CodeBleu_score": 50.26999676177031}]}
{"instance_id": "27ee5f415000", "generation_result": [{"code": "def _load_df(self, df: Union[pd.DataFrame, pd.Series, str, list, dict])\n\tif isinstance(df, pd.DataFrame):\n        self.df = df\n    elif isinstance(df, pd.Series):\n        self.df = df.to_frame()\n    elif isinstance(df, str):\n        self.df = pd.read_csv(df)\n    elif isinstance(df, list):\n        self.df = pd.DataFrame(df)\n    elif isinstance(df, dict):\n        self.df = pd.DataFrame(df)\n    else:\n        raise ValueError(\n            \"df must be a pandas DataFrame, Series, string, list, or dictionary\"\n        )\n\n", "CodeBleu_score": 49.87068808819076}, {"code": "def _load_df(self, df: Union[pd.DataFrame, pd.Series, str, list, dict])\n\tif isinstance(df, pd.DataFrame):\n        self.df = df\n    elif isinstance(df, pd.Series):\n        self.df = df.to_frame()\n    elif isinstance(df, str):\n        if df.endswith('.csv'):\n            self.df = pd.read_csv(df)\n        elif df.endswith('.xlsx'):\n            self.df = pd.read_excel(df)\n        elif df.endswith('.json'):\n            self.df = pd.read_json(df)\n        elif df.endswith('.parquet'):\n            self.df = pd.read_parquet(df)\n        elif df.endswith('.h5'):\n            self.df = pd.read_hdf(df)\n        elif df.endswith('.feather'):\n            self.df = pd.read_feather(df)\n        elif df.endswith('.stata'):\n            self.df = pd.read_stata(df)\n        elif df.endswith('.pickle'):\n            self.df = pd.read_pickle(df)\n        elif df.endswith('.sas7bdat'):\n            self.df = pd.read_sas(df)\n        elif df.endswith('.sav'):\n            self.df = pd.read_spss(df)\n        elif df.endswith('.dta'):\n            self.df = pd.read_stata(df)\n        elif df.endswith('.html'):\n            self.df = pd.read_html(df)\n        else:\n            raise ValueError(\n                \"File extension not supported. Supported file extensions are .csv, .xlsx, .json, .parquet, .h5, .feather, .stata, .pickle, .sas7bdat, .sav, .dta, and .html.\")\n    elif isinstance(df, list):\n        self.df = pd.DataFrame(df)\n    elif isinstance(df, dict):\n        self.df = pd.DataFrame(df)\n    else:\n        raise ValueError(\n            \"Input type not supported. Supported input types are pandas DataFrame, pandas Series, string representing a file", "CodeBleu_score": 44.17392339801699}, {"code": "def _load_df(self, df: Union[pd.DataFrame, pd.Series, str, list, dict])\n\tif isinstance(df, pd.DataFrame):\n        self._df = df\n    elif isinstance(df, pd.Series):\n        self._df = df.to_frame()\n    elif isinstance(df, str):\n        self._df = pd.read_csv(df)\n    elif isinstance(df, list):\n        self._df = pd.DataFrame(df)\n    elif isinstance(df, dict):\n        self._df = pd.DataFrame(df)\n    else:\n        raise ValueError(\n            \"The input must be a pandas DataFrame, Series, string, list, or dictionary.\"\n        )\n    return self._df\n\n", "CodeBleu_score": 50.87613862827446}]}
{"instance_id": "38f48a358606", "generation_result": [{"code": "def sample(self, n: int = 3) -> pd.DataFrame\n\tsampled_df = pd.DataFrame()\n    if len(self.df) <= n:\n        sampled_df = self.df.sample(frac=1)\n    else:\n        for col in self.df.columns:\n            col_sample = self._sample_column(col, n)\n            sampled_df[col] = col_sample\n\n    # anonymize the sampled dataframe head\n    sampled_df = Anonymizer.anonymize_dataframe_head(sampled_df)\n\n    return sampled_df\n\n", "CodeBleu_score": 76.5886131503128}, {"code": "def sample(self, n: int = 3) -> pd.DataFrame\n\tsampled_df = pd.DataFrame()\n    if len(self.df) <= n:\n        sampled_df = self.df.sample(frac=1)\n    else:\n        for col in self.df.columns:\n            col_sample = self._sample_column(col, n)\n            sampled_df[col] = col_sample\n\n    # anonymize the sampled dataframe head\n    sampled_df = Anonymizer.anonymize_dataframe_head(sampled_df)\n\n    return sampled_df\n\n", "CodeBleu_score": 76.5886131503128}, {"code": "def sample(self, n: int = 3) -> pd.DataFrame\n\tsampled_df = pd.DataFrame()\n    if len(self.df) <= n:\n        sampled_df = self.df.sample(frac=1)\n    else:\n        for col in self.df.columns:\n            col_sample = self._sample_column(col, n)\n            sampled_df[col] = col_sample\n\n    # anonymize the sampled dataframe head\n    sampled_df = Anonymizer.anonymize_dataframe_head(sampled_df)\n\n    return sampled_df\n\n", "CodeBleu_score": 76.5886131503128}]}
{"instance_id": "8bddad2e31ca", "generation_result": [{"code": "def convert_df_to_csv(self, df: pd.DataFrame, extras: dict) -> str\n\t# TODO: Add a docstring with more details about the extras dictionary.\n    # TODO: Add a docstring with more details about the return value.\n    # TODO: Add a docstring with more details about the return value.\n    # TODO: Add a docstring with more details about the return value.\n    # TODO: Add a docstring with more details about the return value.\n    # TODO: Add a docstring with more details about the return value.\n    # TODO: Add a docstring with more details about the return value.\n    # TODO: Add a docstring with more details about the return value.\n    # TODO: Add a docstring with more details about the return value.\n    # TODO: Add a docstring with more details about the return value.\n    # TODO: Add a docstring with more details about the return value.\n    # TODO: Add a docstring with more details about the return value.\n    # TODO: Add a docstring with more details about the return value.\n    # TODO: Add a docstring with more details about the return value.\n    # TODO: Add a docstring with more details about the return value.\n    # TODO: Add a docstring with more details about the return value.\n    # TODO: Add a docstring with more details about the return value.\n    # TODO: Add a docstring with more details about the return value.\n    # TODO: Add a docstring with more details about the return value.\n    # TODO: Add a docstring with more details about the return value.\n    # TODO: Add a docstring with more details about the return value.\n    # TODO: Add a docstring with more details about the return value.\n    # TODO: Add a docstring with more details about the return value.\n    # TODO: Add a docstring with more details about the return value.\n    # TODO: Add a docstring with more details about the return value.\n    # TODO: Add a docstring with more details about the return value.\n    # TODO: Add a docstring with more details about the return value.\n    # TODO: Add a docstring with more details about the return value.\n    # TODO: Add a docstring with more details about the return value.\n    # TODO: Add a docstring with more details about the return value.", "CodeBleu_score": 12.885375494071145}, {"code": "def convert_df_to_csv(self, df: pd.DataFrame, extras: dict) -> str\n\t# Get the DataFrame's name and description\n    df_name = df.name\n    df_description = df.description\n    # Get the DataFrame's dimensions\n    df_rows = df.shape[0]\n    df_cols = df.shape[1]\n    # Get the DataFrame's content\n    df_content = df.to_csv(index=False, header=False)\n    # Get the DataFrame's column names\n    df_col_names = df.columns.values.tolist()\n    # Get the DataFrame's column data types\n    df_col_types = df.dtypes.values.tolist()\n    # Get the DataFrame's column descriptions\n    df_col_descriptions = df.describe_columns()\n    # Get the DataFrame's column units\n    df_col_units = df.units.values.tolist()\n    # Get the DataFrame's column formats\n    df_col_formats = df.formats.values.tolist()\n    # Get the DataFrame's column widths\n    df_col_widths = df.widths.values.tolist()\n    # Get the DataFrame's column precisions\n    df_col_precisions = df.precisions.values.tolist()\n    # Get the DataFrame's column roles\n    df_col_roles = df.roles.values.tolist()\n    # Get the DataFrame's column labels\n    df_col_labels = df.labels.values.tolist()\n    # Get the DataFrame's column categories\n    df_col_categories = df.categories.values.tolist()\n    # Get the DataFrame's column references\n    df_col_references = df.references.values.tolist()\n    # Get the DataFrame's column attributes\n    df_col_attributes = df.attributes.values.tolist()\n    # Get the DataFrame's column notes\n    df_col_notes = df.notes.values.tolist()\n    # Get the DataFrame's column tags\n    df_col_tags = df.tags.values.tolist()\n    # Get the DataFrame", "CodeBleu_score": 24.799763746898147}, {"code": "def convert_df_to_csv(self, df: pd.DataFrame, extras: dict) -> str\n\t# Add name and description attributes to the opening tag if available\n    if 'name' in df.attrs:\n        opening_tag = f'<{df.attrs[\"name\"]}'\n        if 'description' in df.attrs:\n            opening_tag += f' description=\"{df.attrs[\"description\"]}\"'\n        opening_tag += '>'\n    else:\n        opening_tag = '<DataFrame>'\n    # Add details about the DataFrame's dimensions and content\n    details = f'{df.shape[0]} rows x {df.shape[1]} columns", "CodeBleu_score": 32.55984220396536}]}
{"instance_id": "5380bb60fd39", "generation_result": [{"code": "def convert_df_sql_connector_to_str\n\tpass\n\n\n", "CodeBleu_score": 1.1679952229833652e-22}, {"code": "def convert_df_sql_connector_to_str\n\t# TODO: Add the table name and description to the table tag\n    # TODO: Convert the DataFrame to CSV\n    # TODO: Add the CSV representation to the table\n    # TODO: Close the table tag\n    # TODO: Return the HTML string\n\n    # TODO: Add the table name and description to the table tag\n    # TODO: Convert the DataFrame to CSV\n    # TODO: Add the CSV representation to the table\n    # TODO: Close the table tag\n    # TODO: Return the HTML string\n\n    # TODO: Add the table name and description to the table tag\n    # TODO: Convert the DataFrame to CSV\n    # TODO: Add the CSV representation to the table\n    # TODO: Close the table tag\n    # TODO: Return the HTML string\n\n    # TODO: Add the table name and description to the table tag\n    # TODO: Convert the DataFrame to CSV\n    # TODO: Add the CSV representation to the table\n    # TODO: Close the table tag\n    # TODO: Return the HTML string\n\n    # TODO: Add the table name and description to the table tag\n    # TODO: Convert the DataFrame to CSV\n    # TODO: Add the CSV representation to the table\n    # TODO: Close the table tag\n    # TODO: Return the HTML string\n\n    # TODO: Add the table name and description to the table tag\n    # TODO: Convert the DataFrame to CSV\n    # TODO: Add the CSV representation to the table\n    # TODO: Close the table tag\n    # TODO: Return the HTML string\n\n    # TODO: Add the table name and description to the table tag\n    # TODO: Convert the DataFrame to CSV\n    # TODO: Add the CSV representation to the table\n    # TODO: Close the table tag\n    # TODO: Return the HTML string\n\n    # TODO: Add the table name and description to the table tag\n    # TODO: Convert the DataFrame to CSV\n    # TODO: Add the CSV representation to the table\n    # TODO: Close the table tag\n    # TODO: Return the HTML string\n\n    # TODO: Add the table name and description to the table tag\n    # TODO: Convert the DataFrame to CSV\n    # TODO: Add the CSV representation to the table\n    # TODO: Close the table tag\n    # TODO: Return the HTML", "CodeBleu_score": 0.0}, {"code": "def convert_df_sql_connector_to_str\n\tpass\n\n", "CodeBleu_score": 1.1679952229833652e-22}]}
{"instance_id": "0afd995716c5", "generation_result": [{"code": "def convert_df_to_yml(self, df: pd.DataFrame, extras: dict) -> str:\n\t# Initialize the result dictionary\n    df_number_key = f\"dfs[{extras['index']}]\"\n\n    # Create a dictionary representing the data structure\n    df_info = {\n        \"name\": df.name,\n        \"description\": df.description,\n        \"type\": (\n            df.type\n            if \"is_direct_sql\" in extras and extras[\"is_direct_sql\"]\n            else extras[\"type\"]\n        ),\n    }\n    # Add DataFrame details to the result\n    data = {\n        \"rows\": df.rows_count,\n        \"columns\": df.columns_count,\n        \"schema\": {\"fields\": []},\n    }\n\n    # Iterate over DataFrame columns\n    df_head = df.get_head()\n    for col_name, col_dtype in df_head.dtypes.items():\n        col_info = {\n            \"name\": col_name,\n            \"type\": str(col_dtype),\n        }\n\n        if not extras.get(\"enforce_privacy\") or df.custom_head is not None:\n            col_info[\"samples\"] = df_head[col_name].head().tolist()\n\n        # Add column description if available\n        if df.field_descriptions and isinstance(df.field_descriptions, dict):\n            if col_description := df.field_descriptions.get(col_name, None):\n                col_info[\"description\"] = col_description\n\n        if df.connector_relations:\n            for relation in df.connector_relations:\n                from pandasai.ee.connectors.relations import ForeignKey, PrimaryKey\n\n                if (\n                    isinstance(relation, PrimaryKey) and relation.name == col_name\n                ) or (\n                    isinstance(relation, ForeignKey) and relation.field == col_name\n                ):\n                    col_info[\"constraints\"] = relation.to_string()\n\n        data[\"schema\"][\"fields\"].append(col_info)\n\n    result = df_info | data\n\n    if \"is_direct_sql\" in extras and extras[\"is_direct_sql", "CodeBleu_score": 37.091443260290646}, {"code": "def convert_df_to_yml(self, df: pd.DataFrame, extras: dict) -> str:\n\tdf_json = self.convert_df_to_json(df, extras)\n    df_yml = yaml.dump(df_json, default_flow_style=False)\n    if \"is_direct_sql\" in extras and extras[\"is_direct_sql\"]:\n        df_yml = f\"<table>{df_yml}</table>\"\n    return df_yml\n\n", "CodeBleu_score": 71.41822619002951}, {"code": "def convert_df_to_yml(self, df: pd.DataFrame, extras: dict) -> str:\n\t# Initialize the result dictionary\n    df_number_key = f\"dfs[{extras['index']}]\"\n\n    # Create a dictionary representing the data structure\n    df_info = {\n        \"name\": df.name,\n        \"description\": df.description,\n        \"type\": (\n            df.type\n            if \"is_direct_sql\" in extras and extras[\"is_direct_sql\"]\n            else extras[\"type\"]\n        ),\n    }\n    # Add DataFrame details to the result\n    data = {\n        \"rows\": df.rows_count,\n        \"columns\": df.columns_count,\n        \"schema\": {\"fields\": []},\n    }\n\n    # Iterate over DataFrame columns\n    df_head = df.get_head()\n    for col_name, col_dtype in df_head.dtypes.items():\n        col_info = {\n            \"name\": col_name,\n            \"type\": str(col_dtype),\n        }\n\n        if not extras.get(\"enforce_privacy\") or df.custom_head is not None:\n            col_info[\"samples\"] = df_head[col_name].head().tolist()\n\n        # Add column description if available\n        if df.field_descriptions and isinstance(df.field_descriptions, dict):\n            if col_description := df.field_descriptions.get(col_name, None):\n                col_info[\"description\"] = col_description\n\n        if df.connector_relations:\n            for relation in df.connector_relations:\n                from pandasai.ee.connectors.relations import ForeignKey, PrimaryKey\n\n                if (\n                    isinstance(relation, PrimaryKey) and relation.name == col_name\n                ) or (\n                    isinstance(relation, ForeignKey) and relation.field == col_name\n                ):\n                    col_info[\"constraints\"] = relation.to_string()\n\n        data[\"schema\"][\"fields\"].append(col_info)\n\n    result = df_info | data\n\n    if \"is_direct_sql\" in extras and extras[\"is_direct_sql\"]", "CodeBleu_score": 37.08867831677715}]}
{"instance_id": "e2b90ebd4e5d", "generation_result": [{"code": "def validate_value(self, expected_type: str) -> bool\n\tif not expected_type:\n        return True\n    elif expected_type == \"number\":\n        return isinstance(self, (int, float))\n    elif expected_type == \"string\":\n        return isinstance(self, str)\n    elif expected_type == \"dataframe\":\n        return isinstance(self, (pd.DataFrame, pd.Series))\n    elif expected_type == \"plot\":\n        if isinstance(self, dict):\n            return True\n        elif isinstance(self, str):\n            return bool(re.match(r\"^.*\\.png$\", self))\n        else:\n            return False\n    else:\n        raise ValueError(f\"Expected type {expected_type} is not supported.\")\n\n\n", "CodeBleu_score": 76.35385981374849}, {"code": "def validate_value(self, expected_type: str) -> bool\n\tif expected_type == \"\":\n        return True\n    elif expected_type == \"number\":\n        return isinstance(self.value, (int, float))\n    elif expected_type == \"string\":\n        return isinstance(self.value, str)\n    elif expected_type == \"dataframe\":\n        return isinstance(self.value, (pd.DataFrame, pd.Series))\n    elif expected_type == \"plot\":\n        return isinstance(self.value, (str, dict))\n    else:\n        raise ValueError(f\"Invalid expected_type: {expected_type}\")", "CodeBleu_score": 53.597878874430464}, {"code": "def validate_value(self, expected_type: str) -> bool\n\tif expected_type == \"\":\n        return True\n    elif expected_type == \"number\":\n        return isinstance(self.value, (int, float))\n    elif expected_type == \"string\":\n        return isinstance(self.value, str)\n    elif expected_type == \"dataframe\":\n        return isinstance(self.value, (pd.DataFrame, pd.Series))\n    elif expected_type == \"plot\":\n        return isinstance(self.value, (str, dict))\n    else:\n        raise ValueError(f\"Invalid expected_type: {expected_type}\")\n\n", "CodeBleu_score": 53.597878874430464}]}
{"instance_id": "d95828fdcbcb", "generation_result": [{"code": " def _distance(self, X, X2=None)\n\tif isinstance(self.distance_metric, str):\n        if X2 is None:\n            dist = pairwise_distance(X, metric=self.distance_metric, **self.distance_params)\n        else:\n            dist = pairwise_distance(X, X2, metric=self.distance_metric, **self.distance_params)\n    else:\n        if X2 is None:\n            if len(signature(self.distance_metric).parameters) == 1:\n                dist = self.distance_metric(X, **self.distance_params)\n            else:\n                raise ValueError(\n                    \"Distance metric function should only have one parameter for X or two parameters for X and X2.\"\n                )\n        else:\n            dist = self.distance_metric(X, X2, **self.distance_params)\n\n    return dist", "CodeBleu_score": 34.707922017480044}, {"code": " def _distance(self, X, X2=None)\n\tif isinstance(self.distance, str):\n        dist_fn = pairwise_distance\n        params = {}\n    else:\n        dist_fn = self.distance\n        params = self.params\n    if X2 is None:\n        try:\n            signature = inspect.signature(dist_fn)\n            if len(signature.parameters) == 2:\n                return dist_fn(X, **params)\n        except ValueError:\n            pass\n        X2 = X\n    return dist_fn(X, X2, **params)", "CodeBleu_score": 36.215768221742145}, {"code": " def _distance(self, X, X2=None)\n\tif isinstance(self.distance, str):\n        if X2 is None:\n            X2 = X\n        return pairwise_distance(X, X2, metric=self.distance, **self.distance_params)\n    else:\n        if X2 is None:\n            if len(signature(self.distance).parameters) == 1:\n                return self.distance(X)\n            else:\n                raise ValueError(\n                    \"The distance function must accept a single dataset or two datasets.\"\n                )\n        else:\n            return self.distance(X, X2, **self.distance_params)", "CodeBleu_score": 24.913466260040533}]}
{"instance_id": "497d73b298ee", "generation_result": [{"code": "def _convert_X_to_sklearn(self, X)\n\tif self.equal_length:\n        if isinstance(X, pd.Series):\n            X = X.values\n        elif isinstance(X, pd.DataFrame):\n            X = X.values\n        elif isinstance(X, np.ndarray):\n            X = X.reshape(X.shape[0], -1)\n        else:\n            raise ValueError('X must be a pandas DataFrame, Series, or numpy array.')\n        X = X.reshape(X.shape[0], -1)\n        self.n_features_in_ = X.shape[1]\n    else:\n        if isinstance(X, pd.Series):\n            X = X.reset_index().pivot(index='index', columns='level_1').fillna(0)\n        elif isinstance(X, pd.DataFrame):\n            X = X.reset_index().pivot(index='index', columns='level_1').fillna(0)\n        elif isinstance(X, np.ndarray):\n            X = pd.DataFrame(X).reset_index().pivot(index='index', columns='level_1').fillna(0)\n        else:\n            raise ValueError('X must be a pandas DataFrame, Series, or numpy array.')\n        X = X.values\n        self.n_features_in_ = X.shape[1] - 1\n        lengths = np.array(X[:, 0]).reshape(-1, 1)\n        X = np.concatenate([lengths, X[:, 1:]], axis=1)\n    return X\n\n", "CodeBleu_score": 35.02627551059261}, {"code": "def _convert_X_to_sklearn(self, X)\n\tif self.unequal:\n        if isinstance(X, pd.DataFrame):\n            X = X.reset_index(drop=True)\n            X = pd.DataFrame(X.T)\n            X = X.fillna(0)\n            X = X.values\n            X = np.concatenate((np.array([X.shape[1]]*X.shape[0]).reshape(-1,1), X), axis=1)\n        elif isinstance(X, np.ndarray):\n            X = np.array([X.shape[1]]*X.shape[0]).reshape(-1,1)\n            X = np.concatenate((X, X), axis=1)\n        else:\n            raise TypeError(\"X must be a pandas DataFrame or a numpy array.\")\n    else:\n        if isinstance(X, pd.DataFrame):\n            X = X.values\n            X = X.flatten()\n        elif isinstance(X, np.ndarray):\n            X = X.flatten()\n        else:\n            raise TypeError(\"X must be a pandas DataFrame or a numpy array.\")\n    return X\n\n\n", "CodeBleu_score": 38.53469540614148}, {"code": "def _convert_X_to_sklearn(self, X)\n\tif self.equal_length:\n        if isinstance(X, pd.DataFrame):\n            X = X.values.flatten()\n        elif isinstance(X, np.ndarray):\n            X = X.flatten()\n        elif isinstance(X, list):\n            X = np.array(X).flatten()\n        else:\n            raise ValueError(\n                \"X must be a list, numpy array, or pandas dataframe for equal length series.\"\n            )\n        return X\n    else:\n        if isinstance(X, pd.DataFrame):\n            X = X.reset_index(drop=True)\n            X = pd.pivot_table(\n                X, values=X.columns, index=X.index, columns=\"variable\"\n            ).fillna(0)\n            X = X.values\n            X_lengths = X.shape[1]\n            X = np.concatenate((np.array([X_lengths]).T, X), axis=1)\n        elif isinstance(X, np.ndarray):\n            X = pd.DataFrame(X)\n            X = X.reset_index(drop=True)\n            X = pd.pivot_table(\n                X, values=X.columns, index=X.index, columns=\"variable\"\n            ).fillna(0)\n            X = X.values\n            X_lengths = X.shape[1]\n            X = np.concatenate((np.array([X_lengths]).T, X), axis=1)\n        elif isinstance(X, list):\n            X = pd.DataFrame(X)\n            X = X.reset_index(drop=True)\n            X = pd.pivot_table(\n                X, values=X.columns, index=X.index, columns=\"variable\"\n            ).fillna(0)\n            X = X.values\n            X_lengths = X.shape[1]\n            X = np.concatenate((np.array([X_lengths]).T, X), axis=1)\n        else:\n            raise ValueError(\n                \"X must be a list, numpy array, or pandas dataframe for unequal length series.\"\n            )", "CodeBleu_score": 33.401363991838785}]}
{"instance_id": "8798260a6f12", "generation_result": [{"code": "def is_sklearn_estimator(obj)\n\tif not isinstance(obj, type):\n        raise TypeError(\"is_sklearn_estimator requires a class, but an object was passed\")\n    is_sklearn_estimator = issubclass(obj, SklearnBaseEstimator) and not issubclass(obj, BaseObject)\n    return is_sklearn_estimator\n\n", "CodeBleu_score": 32.03769494616498}, {"code": "def is_sklearn_estimator(obj)\n\treturn isinstance(obj, type) and issubclass(obj, SklearnBaseEstimator) and not issubclass(obj, BaseObject)\n\n", "CodeBleu_score": 15.999554501972845}, {"code": "def is_sklearn_estimator(obj)\n\tif not isinstance(obj, type):\n        raise TypeError(\"object should be a class, type object given\")\n    if issubclass(obj, SklearnBaseEstimator) and not issubclass(obj, BaseObject):\n        return True\n    else:\n        return False\n\n", "CodeBleu_score": 25.68768621471948}]}
{"instance_id": "2ab2596d8520", "generation_result": [{"code": "def __rmul__(self, other)\n\tif isinstance(other, BaseTransformer):\n        new_transformers = [other] + self.transformers\n        new_transformer_pipeline = TransformerPipeline(new_transformers)\n        new_pipeline = SklearnRegressorPipeline(\n            self.regressor, new_transformer_pipeline\n        )\n        return new_pipeline\n    else:\n        return NotImplemented\n\n", "CodeBleu_score": 50.14666093014537}, {"code": "def __rmul__(self, other)\n\tif isinstance(other, BaseTransformer):\n        transformers = self.transformers.copy()\n        transformers.append(other)\n        transformer_pipeline = TransformerPipeline(transformers)\n        return SklearnRegressorPipeline(self.regressor, transformer_pipeline)\n    return NotImplemented\n\n", "CodeBleu_score": 38.028306686148014}, {"code": "def __rmul__(self, other)\n\tif isinstance(other, BaseTransformer):\n        t = TransformerPipeline(steps=[other, self])\n        return SklearnRegressorPipeline(regressor=self.regressor, transformer=t)\n    else:\n        return NotImplemented\n\n", "CodeBleu_score": 33.46779166395017}]}
{"instance_id": "5c1c6d14ca93", "generation_result": [{"code": "def prep_skl_df(df, copy_df=False)\n\tif copy_df:\n        df = df.copy()\n    if not all([isinstance(col, str) for col in df.columns]):\n        df.columns = [str(col) for col in df.columns]\n    return df\n\n", "CodeBleu_score": 47.723556909419315}, {"code": "def prep_skl_df(df, copy_df=False)\n\tif copy_df:\n        df = df.copy()\n    if not all(isinstance(col, str) for col in df.columns):\n        df.columns = df.columns.astype(str)\n    return df\n\n\n", "CodeBleu_score": 48.47904373253273}, {"code": "def prep_skl_df(df, copy_df=False)\n\tif copy_df:\n        df = df.copy()\n    if not all(isinstance(col, str) for col in df.columns):\n        df.columns = [str(col) for col in df.columns]\n    return df\n\n", "CodeBleu_score": 47.81480576724148}]}
{"instance_id": "d5e567c20361", "generation_result": [{"code": "def predict(self, X)\n\tif not self._is_fitted:\n        raise NotFittedError(\n            f\"This instance of {self.__class__.__name__} has not \"\n            f\"been fitted yet; please call `fit` first.\"\n        )\n\n    # treat case of single class seen in fit\n    if self.n_classes_ == 1:\n        return np.repeat([self.classes_[0]], X.shape[0], axis=0)\n\n    if isinstance(X, np.ndarray) and len(X.shape) == 3 and X.shape[1] == 1:\n        X = np.reshape(X, (X.shape[0], -1))\n    elif isinstance(X, pd.DataFrame) and len(X.shape) == 2:\n        X = X.to_numpy()\n    elif not isinstance(X, np.ndarray) or len(X.shape) > 2:\n        raise ValueError(\n            \"RotationForest is not a time series classifier. \"\n            \"A valid sklearn input such as a 2d numpy array is required.\"\n            \"Sparse input formats are currently not supported.\"\n        )\n    X = self._validate_data(X=X, reset=False)\n\n    # replace missing values with 0 and remove useless attributes\n    X = X[:, self._useful_atts]\n\n    # normalise the data.\n    X = (X - self._min) / self._ptp\n\n    y_probas = Parallel(n_jobs=self._n_jobs)(\n        delayed(self._predict_proba_for_estimator)(\n            X,\n            self.estimators_[i],\n            self._pcas[i],\n            self._groups[i],\n        )\n        for i in range(self._n_estimators)\n    )\n\n    output = np.sum(y_probas, axis=0) / (\n        np.ones(self.n_classes_) * self._n_estimators\n    )\n    return self.classes_[np.argmax(output, axis=1)]", "CodeBleu_score": 34.674521056401}, {"code": "def predict(self, X)\n\tif not self._is_fitted:\n        raise NotFittedError(\n            f\"This instance of {self.__class__.__name__} has not \"\n            f\"been fitted yet; please call `fit` first.\"\n        )\n\n    # treat case of single class seen in fit\n    if self.n_classes_ == 1:\n        return np.repeat([self.classes_[0]], X.shape[0])\n\n    if isinstance(X, np.ndarray) and len(X.shape) == 3 and X.shape[1] == 1:\n        X = np.reshape(X, (X.shape[0], -1))\n    elif isinstance(X, pd.DataFrame) and len(X.shape) == 2:\n        X = X.to_numpy()\n    elif not isinstance(X, np.ndarray) or len(X.shape) > 2:\n        raise ValueError(\n            \"RotationForest is not a time series classifier. \"\n            \"A valid sklearn input such as a 2d numpy array is required.\"\n            \"Sparse input formats are currently not supported.\"\n        )\n    X = self._validate_data(X=X, reset=False)\n\n    # replace missing values with 0 and remove useless attributes\n    X = X[:, self._useful_atts]\n\n    # normalise the data.\n    X = (X - self._min) / self._ptp\n\n    y_probas = Parallel(n_jobs=self._n_jobs)(\n        delayed(self._predict_proba_for_estimator)(\n            X,\n            self.estimators_[i],\n            self._pcas[i],\n            self._groups[i],\n        )\n        for i in range(self._n_estimators)\n    )\n\n    output = np.sum(y_probas, axis=0) / (\n        np.ones(self.n_classes_) * self._n_estimators\n    )\n    return self.classes_[np.argmax(output, axis=1)]", "CodeBleu_score": 34.69214501541771}, {"code": "def predict(self, X)\n\tif not self._is_fitted:\n        raise NotFittedError(\n            f\"This instance of {self.__class__.__name__} has not \"\n            f\"been fitted yet; please call `fit` first.\"\n        )\n\n    # treat case of single class seen in fit\n    if self.n_classes_ == 1:\n        return np.repeat([self.classes_[0]], X.shape[0])\n\n    if isinstance(X, np.ndarray) and len(X.shape) == 3 and X.shape[1] == 1:\n        X = np.reshape(X, (X.shape[0], -1))\n    elif isinstance(X, pd.DataFrame) and len(X.shape) == 2:\n        X = X.to_numpy()\n    elif not isinstance(X, np.ndarray) or len(X.shape) > 2:\n        raise ValueError(\n            \"RotationForest is not a time series classifier. \"\n            \"A valid sklearn input such as a 2d numpy array is required.\"\n            \"Sparse input formats are currently not supported.\"\n        )\n    X = self._validate_data(X=X, reset=False)\n\n    # replace missing values with 0 and remove useless attributes\n    X = X[:, self._useful_atts]\n\n    # normalise the data.\n    X = (X - self._min) / self._ptp\n\n    y_probas = Parallel(n_jobs=self._n_jobs)(\n        delayed(self._predict_proba_for_estimator)(\n            X,\n            self.estimators_[i],\n            self._pcas[i],\n            self._groups[i],\n        )\n        for i in range(self._n_estimators)\n    )\n\n    output = np.sum(y_probas, axis=0) / (\n        np.ones(self.n_classes_) * self._n_estimators\n    )\n\n    # select the class with the maximum probability\n    output = np.argmax(output, axis=1)\n\n    # map the", "CodeBleu_score": 33.13364210378633}]}
{"instance_id": "210b60dffd55", "generation_result": [{"code": "def _predict_proba_for_estimator(self, X, clf, pcas, groups)\n\t# transform the data using the corresponding pca for each group\n    X_transformed = np.concatenate(\n        [\n            pca.transform(X[:, group])\n            for pca, group in zip(pcas, groups)\n        ],\n        axis=1,\n    )\n\n    # replace missing values with 0\n    X_transformed[np.isnan(X_transformed)] = 0\n\n    # compute the class probabilities\n    y_probas = clf.predict_proba(X_transformed)\n\n    # adjust the class probabilities if necessary\n    if y_probas.shape[1] != self.n_classes_:\n        y_probas = _adjust_probas(y_probas, self.n_classes_)\n\n    return y_probas\n\n", "CodeBleu_score": 25.70032049155037}, {"code": "def _predict_proba_for_estimator(self, X, clf, pcas, groups)\n\t# transform the data by applying the corresponding PCA to each group and concatenating the results.\n    X_transformed = np.concatenate(\n        [\n            pca.transform(X[:, group])\n            for pca, group in zip(pcas, groups)\n        ],\n        axis=1,\n    )\n\n    # replace missing values with 0\n    X_transformed = np.nan_to_num(X_transformed)\n\n    # predict class probabilities\n    y_probas = clf.predict_proba(X_transformed)\n\n    # adjust the probabilities to match the expected class structure\n    if y_probas.shape[1] != self.n_classes_:\n        if self.n_classes_ == 2:\n            if y_probas.shape[1] == 1:\n                y_probas = np.concatenate(\n                    [1 - y_probas, y_probas], axis=1\n                )\n            else:\n                y_probas = np.concatenate(\n                    [1 - y_probas[:, 0:1], y_probas[:, 1:]], axis=1\n                )\n        else:\n            raise ValueError(\n                \"The number of classes predicted by the classifier does not match the expected number of classes.\"\n            )\n\n    return y_probas\n\n", "CodeBleu_score": 28.641722444590478}, {"code": "def _predict_proba_for_estimator(self, X, clf, pcas, groups)\n\t# transform the data\n    X_transformed = np.concatenate(\n        [\n            pca.transform(X[:, group])\n            for pca, group in zip(pcas, groups)\n        ],\n        axis=1,\n    )\n    # replace missing values with zeros\n    X_transformed = np.nan_to_num(X_transformed)\n    # compute class probabilities\n    y_proba = clf.predict_proba(X_transformed)\n    # adjust class probabilities if necessary\n    if y_proba.shape[1] != self.n_classes_:\n        # create a new array of the correct shape\n        y_proba_adjusted = np.zeros((y_proba.shape[0], self.n_classes_))\n        # copy the probabilities from the original array\n        y_proba_adjusted[:, self._classes_] = y_proba\n        # normalise the probabilities\n        y_proba_adjusted = y_proba_adjusted / np.sum(y_proba_adjusted, axis=1)[:, np.newaxis]\n        # return the adjusted probabilities\n        return y_proba_adjusted\n    else:\n        # return the original probabilities\n        return y_proba\n\n\n", "CodeBleu_score": 27.75158000924623}]}
{"instance_id": "9436ee0860bb", "generation_result": [{"code": "def predict_proba(self, X)\n\tif not self._is_fitted:\n        raise NotFittedError(\n            f\"This instance of {self.__class__.__name__} has not \"\n            f\"been fitted yet; please call `fit` first.\"\n        )\n\n    # treat case of single class seen in fit\n    if self.n_classes_ == 1:\n        return np.repeat([[1]], X.shape[0], axis=0)\n\n    if isinstance(X, np.ndarray) and len(X.shape) == 3 and X.shape[1] == 1:\n        X = np.reshape(X, (X.shape[0], -1))\n    elif not isinstance(X, np.ndarray) or len(X.shape) > 2:\n        raise ValueError(\n            \"ContinuousIntervalTree is not a time series classifier. \"\n            \"A valid sklearn input such as a 2d numpy array is required.\"\n            \"Sparse input formats are currently not supported.\"\n        )\n    X = self._validate_data(X=X, reset=False, force_all_finite=\"allow-nan\")\n\n    dists = np.zeros((X.shape[0], self.n_classes_))\n    for i in range(X.shape[0]):\n        dists[i] = self._root.predict_proba(X[i], self.n_classes_)\n    return dists\n\n", "CodeBleu_score": 88.43560719097412}, {"code": "def predict_proba(self, X)\n\tif not self._is_fitted:\n        raise NotFittedError(\n            f\"This instance of {self.__class__.__name__} has not \"\n            f\"been fitted yet; please call `fit` first.\"\n        )\n\n    # treat case of single class seen in fit\n    if self.n_classes_ == 1:\n        return np.repeat([[1]], X.shape[0], axis=0)\n\n    if isinstance(X, np.ndarray) and len(X.shape) == 3 and X.shape[1] == 1:\n        X = np.reshape(X, (X.shape[0], -1))\n    elif not isinstance(X, np.ndarray) or len(X.shape) > 2:\n        raise ValueError(\n            \"ContinuousIntervalTree is not a time series classifier. \"\n            \"A valid sklearn input such as a 2d numpy array is required.\"\n            \"Sparse input formats are currently not supported.\"\n        )\n    X = self._validate_data(X=X, reset=False, force_all_finite=\"allow-nan\")\n\n    dists = np.zeros((X.shape[0], self.n_classes_))\n    for i in range(X.shape[0]):\n        dists[i] = self._root.predict_proba(X[i], self.n_classes_)\n    return dists", "CodeBleu_score": 88.43560719097412}, {"code": "def predict_proba(self, X)\n\tif not self._is_fitted:\n        raise NotFittedError(\n            f\"This instance of {self.__class__.__name__} has not \"\n            f\"been fitted yet; please call `fit` first.\"\n        )\n\n    # treat case of single class seen in fit\n    if self.n_classes_ == 1:\n        return np.repeat([[1]], X.shape[0], axis=0)\n\n    if isinstance(X, np.ndarray) and len(X.shape) == 3 and X.shape[1] == 1:\n        X = np.reshape(X, (X.shape[0], -1))\n    elif not isinstance(X, np.ndarray) or len(X.shape) > 2:\n        raise ValueError(\n            \"ContinuousIntervalTree is not a time series classifier. \"\n            \"A valid sklearn input such as a 2d numpy array is required.\"\n            \"Sparse input formats are currently not supported.\"\n        )\n    X = self._validate_data(X=X, reset=False, force_all_finite=\"allow-nan\")\n\n    dists = np.zeros((X.shape[0], self.n_classes_))\n    for i in range(X.shape[0]):\n        dists[i] = self._root.predict_proba(X[i], self.n_classes_)\n    return dists\n\n", "CodeBleu_score": 88.43560719097412}]}
{"instance_id": "d7cb1cb66840", "generation_result": [{"code": "def _convert_X_to_sklearn(self, X)\n\tif self.transformers_ == \"Primitives\":\n        X_sklearn = np.array(X)\n\n    elif self.transformers_ == \"Series\":\n        X_sklearn = np.array(X.flatten())\n\n    else:\n        raise TypeError(\n            \"The output of the transformers must be of scitype \"\n            + \"Primitives or Series. Please ensure that the \"\n            + \"transformers are valid.\"\n        )\n\n    return X_sklearn", "CodeBleu_score": 27.782576286770382}, {"code": "def _convert_X_to_sklearn(self, X)\n\t# Check if the transformers_ attribute is empty\n    if not self.transformers_:\n        raise ValueError(\"The transformers_ attribute is empty. The fit function must be called before the transform function.\")\n\n    # Check if the transformers_ attribute contains a single transformer\n    if len(self.transformers_) == 1:\n        # Check if the transformer is a ColumnTransformer\n        if isinstance(self.transformers_[0][1], ColumnTransformer):\n            # Check if the transformer is a ColumnTransformer with multiple transformers\n            if len(self.transformers_[0][1].transformers) > 1:\n                # Check if the transformers_ attribute contains a single transformer\n                if len(self.transformers_) == 1:\n                    # Check if the transformer is a ColumnTransformer\n                    if isinstance(self.transformers_[0][1], ColumnTransformer):\n                        # Check if the transformer is a ColumnTransformer with multiple transformers\n                        if len(self.transformers_[0][1].transformers) > 1:\n                            # Check if the scitype is \"Primitives\"\n                            if self.transformers_[0][1].scitype == \"Primitives\":\n                                # Convert X to a 2D numpy array\n                                X = self.transformers_[0][1].transform(X)\n                                # Return X\n                                return X\n                            # Check if the scitype is \"Series\"\n                            elif self.transformers_[0][1].scitype == \"Series\":\n                                # Convert X to a flat 2D numpy array\n                                X = self.transformers_[0][1].transform(X)\n                                # Return X\n                                return X\n                            # Raise a TypeError indicating an unexpected output type\n                            else:\n                                raise TypeError(\"The output type of the transformer is not supported.\")\n                        # Check if the scitype is \"Primitives\"\n                        elif self.transformers_[0][1].scitype == \"Primitives\":\n                            # Convert X to a 2D numpy array\n                            X = self.transformers_", "CodeBleu_score": 33.57617367833527}, {"code": "def _convert_X_to_sklearn(self, X)\n\tif self.transformers_ is None:\n        raise ValueError(\"The transformers_ attribute is None. Please fit the transformer before calling this function.\")\n    if self.transformers_[0][0] == \"TableTransformer\":\n        X_transformed = self.transformers_[0][1].transform(X)\n        return X_transformed\n    elif self.transformers_[0][0] == \"SeriesTransformer\":\n        X_transformed = self.transformers_[0][1].transform(X)\n        X_transformed = X_transformed.to_numpy()\n        return X_transformed\n    else:\n        raise TypeError(\"The scitype of the output is neither Primitives nor Series. Please check the transformers_ attribute.\")\n\n", "CodeBleu_score": 31.876677074731717}]}
{"instance_id": "497cb75f2b4d", "generation_result": [{"code": "def set_random_state(estimator, random_state=0)\n\t# Ensure random_state is a valid random state object\n    if not isinstance(random_state, np.random.RandomState):\n        random_state = np.random.RandomState(random_state)\n\n    # Initialize dictionary to hold parameters that need updating\n    params = {}\n\n    # Iterate over estimator's parameters, checking for those related to random state\n    for key, value in estimator.get_params().items():\n        if key.endswith('random_state') and value is None:\n            params[key] = random_state.randint(1000)\n\n    # Update the estimator with the new random state values if any were found\n    if params:\n        estimator.set_params(**params)\n\n    return estimator\n\n", "CodeBleu_score": 38.841287089897115}, {"code": "def set_random_state(estimator, random_state=0)\n\tfrom sklearn.utils.validation import check_random_state\n    random_state = check_random_state(random_state)\n    params = {}\n    for key, value in estimator.get_params().items():\n        if 'random_state' in key:\n            params[key] = random_state.randint(0, 1000000)\n    if params:\n        estimator.set_params(**params)\n    return estimator\n\n", "CodeBleu_score": 42.55602016496366}, {"code": "def set_random_state(estimator, random_state=0)\n\tfrom sklearn.utils.validation import check_random_state\n    from sklearn.base import clone\n    import numpy as np\n    random_state = check_random_state(random_state)\n    params_to_update = {}\n    for param in estimator.get_params():\n        if 'random_state' in param:\n            params_to_update[param] = random_state.randint(np.iinfo(np.int32).max)\n    if len(params_to_update) > 0:\n        estimator = clone(estimator)\n        estimator.set_params(**params_to_update)\n    return estimator\n\n", "CodeBleu_score": 40.31097096069932}]}
{"instance_id": "59ffd1d596a1", "generation_result": [{"code": "def _transform(X, intervals)\n\tn_instances, n_intervals = X.shape[0], len(intervals)\n    transformed = np.empty((n_instances, 3 * n_intervals))\n    for i, interval in enumerate(intervals):\n        transformed[:, 3 * i : 3 * i + 3] = [\n            X[:, interval[0] : interval[1]].mean(axis=1),\n            X[:, interval[0] : interval[1]].std(axis=1),\n            _slope(X[:, interval[0] : interval[1]], axis=1),\n        ]\n    return transformed.T\n\n", "CodeBleu_score": 26.62461118923139}, {"code": "def _transform(X, intervals)\n\t# Initialize the number of instances and intervals\n    n_instances, n_intervals = X.shape[0], len(intervals)\n\n    # Initialize an empty array to store the transformed features\n    transformed = np.empty((n_instances, 3 * n_intervals))\n\n    # For each interval, slice the input matrix, compute the mean, standard deviation, and slope of the slice, and store these values in the transformed array\n    for i, (a, b) in enumerate(intervals):\n        transformed[:, 3 * i] = np.mean(X[:, a:b], axis=1)\n        transformed[:, 3 * i + 1] = np.std(X[:, a:b], axis=1)\n        transformed[:, 3 * i + 2] = _slope(X[:, a:b], axis=1)\n\n    # Return the transposed transformed array\n    return transformed.T\n\n", "CodeBleu_score": 26.02486649546907}, {"code": "def _transform(X, intervals)\n\t# Initialize the number of instances and intervals\n    n_instances, n_intervals = X.shape[0], len(intervals)\n\n    # Initialize an empty array to store the transformed features\n    features = np.zeros((n_instances, 3 * n_intervals))\n\n    # For each interval, slice the input matrix, compute the mean, standard deviation, and slope of the slice, and store these values in the transformed array\n    for i, interval in enumerate(intervals):\n        # Slice the input matrix\n        X_slice = X[:, interval[0] : interval[1]]\n\n        # Compute the mean, standard deviation, and slope of the slice\n        mean = np.mean(X_slice, axis=1)\n        std = np.std(X_slice, axis=1)\n        slope = _slope(X_slice, axis=1)\n\n        # Store these values in the transformed array\n        features[:, 3 * i] = mean\n        features[:, 3 * i + 1] = std\n        features[:, 3 * i + 2] = slope\n\n    # Return the transposed transformed array\n    return features.T\n\n", "CodeBleu_score": 39.65549340645097}]}
{"instance_id": "5a01594eebfb", "generation_result": [{"code": "def plot_and_annotate_facets(\n            self, plot: str, plot_params: dict, configuration: dict,\n            annotation_func: str, *args, annotation_params: dict = None,\n            ax_op_before: List[Union[str, Optional[list],\n                                     Optional[dict]]] = None,\n            ax_op_after: List[Union[str, Optional[list],\n                                    Optional[dict]]] = None,\n            annotate_params: dict = None, **kwargs)\n\tif annotation_params is None:\n        annotation_params = {}\n\n    if annotate_params is None:\n        annotate_params = {}\n\n    ax_op_before = ax_op_before or []\n    ax_op_after = ax_op_after or []\n\n    if not isinstance(ax_op_before, list):\n        raise InvalidParametersError(\n            'ax_op_before must be a list of tuples or lists.')\n    if not isinstance(ax_op_after, list):\n        raise InvalidParametersError(\n            'ax_op_after must be a list of tuples or lists.')\n\n    self.new_plot(ax=kwargs['ax'], plot=plot, **plot_params)\n    self.configure(**configuration)\n\n    _apply_ax_operations(kwargs['ax'], ax_op_before)\n\n    if annotation_func is not None:\n        getattr(self, annotation_func)(*args, **annotation_params)\n\n    self.annotate(**annotate_params)\n\n    _apply_ax_operations(kwargs['ax'], ax_op_after)\n\n    return self._get_output()\n\n", "CodeBleu_score": 49.15243572622985}, {"code": "def plot_and_annotate_facets(\n            self, plot: str, plot_params: dict, configuration: dict,\n            annotation_func: str, *args, annotation_params: dict = None,\n            ax_op_before: List[Union[str, Optional[list],\n                                     Optional[dict]]] = None,\n            ax_op_after: List[Union[str, Optional[list],\n                                    Optional[dict]]] = None,\n            annotate_params: dict = None, **kwargs)\n\tif annotation_params is None:\n        annotation_params = {}\n    if annotate_params is None:\n        annotate_params = {}\n\n    if ax_op_before is None:\n        ax_op_before = []\n    if ax_op_after is None:\n        ax_op_after = []\n\n    if annotation_func == 'annotate_pairs':\n        annotation_func = self.annotate_pairs\n    elif annotation_func == 'annotate_stats':\n        annotation_func = self.annotate_stats\n    else:\n        raise ValueError(f\"{annotation_func} is not a valid annotation function.\")\n\n    plot_params['ax'] = self.ax\n    self.new_plot(**plot_params)\n    _apply_ax_operations(self.ax, ax_op_before)\n    self.configure(**configuration)\n    annotation_func(*args, **annotation_params)\n    self.annotate(**annotate_params)\n    _apply_ax_operations(self.ax, ax_op_after)\n\n    return self._get_output()", "CodeBleu_score": 47.04324190296247}, {"code": "def plot_and_annotate_facets(\n            self, plot: str, plot_params: dict, configuration: dict,\n            annotation_func: str, *args, annotation_params: dict = None,\n            ax_op_before: List[Union[str, Optional[list],\n                                     Optional[dict]]] = None,\n            ax_op_after: List[Union[str, Optional[list],\n                                    Optional[dict]]] = None,\n            annotate_params: dict = None, **kwargs)\n\tif annotation_params is None:\n        annotation_params = {}\n    if annotate_params is None:\n        annotate_params = {}\n\n    if not isinstance(plot_params, dict):\n        raise TypeError(\"plot_params must be a dictionary\")\n    if not isinstance(configuration, dict):\n        raise TypeError(\"configuration must be a dictionary\")\n    if not isinstance(annotation_params, dict):\n        raise TypeError(\"annotation_params must be a dictionary\")\n    if not isinstance(annotate_params, dict):\n        raise TypeError(\"annotate_params must be a dictionary\")\n    if not isinstance(ax_op_before, list):\n        raise TypeError(\"ax_op_before must be a list\")\n    if not isinstance(ax_op_after, list):\n        raise TypeError(\"ax_op_after must be a list\")\n\n    if annotation_func == 'stat_test':\n        annotation_func = self.stat_test\n    elif annotation_func == 'pairwise_ttests':\n        annotation_func = self.pairwise_ttests\n    elif annotation_func == 'pairwise_tukeys_hsd':\n        annotation_func = self.pairwise_tukeys_hsd\n    elif annotation_func == 'pairwise_tukeys_hsd_with_multiple_comparisons':\n        annotation_func = self.pairwise_tukeys_hsd_with_multiple_comparisons\n    elif annotation_func == 'pairwise_anova':\n        annotation_func = self.pairwise_anova\n    elif annotation_func == 'pairwise_anova_with_multiple_comparisons':\n        annotation_func = self.pairwise_anova_with_multiple_comparisons\n    elif annotation_func == 'pairwise_wilcoxon':\n        annotation_func = self.pairwise_wilcoxon\n    elif annotation_func == 'pairwise_wilcoxon_with_multiple_comparisons':\n        annotation_func = self.pairwise_wilcoxon_with_multiple_comparisons\n    elif annotation_func == 'pairwise_mann_whitney':\n        annotation_func = self.pairwise_mann_", "CodeBleu_score": 37.36807704274209}]}
{"instance_id": "b5b17d312c74", "generation_result": [{"code": "def predict_proba(self, X)\n\tif not hasattr(self, '_program'):\n        raise Exception('The model has not been fitted yet.')\n    X = self._validate_data(X)\n    if X.shape[1] != self.n_features_:\n        raise Exception('The number of features in the data does not match the number of features used to fit the model.')\n    scores = self._program.execute(X)\n    probabilities = self._transform_scores_to_probabilities(scores)\n    return probabilities", "CodeBleu_score": 37.14094360631993}, {"code": "def predict_proba(self, X)\n\tif not hasattr(self, '_program'):\n        raise AttributeError(\"Model has not been fitted yet.\")\n    X = self._validate_input(X)\n    if X.shape[1] != self._n_features:\n        raise ValueError(\"Number of features in input data does not match number of features in model.\")\n    scores = self._program(X)\n    probas = self._softmax(scores)\n    return probas\n\n\n", "CodeBleu_score": 32.20175126094904}, {"code": "def predict_proba(self, X)\n\tif not hasattr(self, '_program'):\n        raise ValueError(\"Model has not been fitted.\")\n    X = self._validate_data(X, dtype=np.float64)\n    if X.shape[1] != self.n_features_in_:\n        raise ValueError(\"Number of features in input data does not match the number of features used to fit the model.\")\n    scores = self._program.compute(X)\n    proba = np.zeros((X.shape[0], 2))\n    proba[:, 1] = 1 / (1 + np.exp(-scores))\n    proba[:, 0] = 1 - proba[:, 1]\n    return proba\n\n", "CodeBleu_score": 46.10592872630248}]}
{"instance_id": "83f71c959df7", "generation_result": [{"code": "def point_mutation(self, random_state)\n\t# Create a copy of the program\n    program = self.copy()\n\n    # Initialize the list of modified node indices\n    modified_node_indices = []\n\n    # Identify nodes to be mutated\n    for node_index in range(len(program)):\n\n        # If the node is to be mutated\n        if random_state.uniform() < self.mutation_prob:\n\n            # Add the node index to the list of modified node indices\n            modified_node_indices.append(node_index)\n\n            # If the node is a function\n            if program[node_index].arity != 0:\n\n                # Find a valid replacement function with the same arity\n                replacement_function = self.find_valid_replacement_function(program[node_index].arity, random_state)\n\n                # Update the node\n                program[node_index] = replacement_function\n\n            # If the node is a terminal\n            else:\n\n                # If the node is a constant\n                if program[node_index].is_constant:\n\n                    # Replace the node with a variable\n                    program[node_index] = self.find_valid_replacement_variable(random_state)\n\n                # If the node is a variable\n                else:\n\n                    # Replace the node with a constant\n                    program[node_index] = self.find_valid_replacement_constant(random_state)\n\n    # Return the mutated program and the list of modified node indices\n    return program, modified_node_indices\n\n\n", "CodeBleu_score": 20.920457881275556}, {"code": "def point_mutation(self, random_state)\n\t# Create a copy of the program\n    program_copy = self.copy()\n\n    # Get the number of nodes in the program\n    n_nodes = len(program_copy)\n\n    # Get the number of features\n    n_features = len(self.feature_names)\n\n    # Get the number of constants\n    n_constants = len(self.constant_range)\n\n    # Get the number of functions\n    n_functions = len(self.function_set)\n\n    # Get the number of terminals\n    n_terminals = n_features + n_constants\n\n    # Get the number of nodes to mutate\n    n_mutate = int(self.mutation_rate * n_nodes)\n\n    # Get the mutation probability threshold\n    mutation_prob = self.mutation_prob\n\n    # Initialize the list of nodes to mutate\n    nodes_to_mutate = []\n\n    # Loop through the nodes in the program\n    for i in range(n_nodes):\n\n        # Sample a uniform random variable\n        r = random_state.uniform()\n\n        # If the random variable is less than the mutation probability, add the node to the list of nodes to mutate\n        if r < mutation_prob:\n            nodes_to_mutate.append(i)\n\n    # Get the number of nodes to mutate\n    n_mutate = len(nodes_to_mutate)\n\n    # Initialize the list of modified node indices\n    modified_nodes = []\n\n    # Loop through the nodes to mutate\n    for i in range(n_mutate):\n\n        # Get the node index\n        node_idx = nodes_to_mutate[i]\n\n        # Get the node\n        node = program_copy[node_idx]\n\n        # Get the node type\n        node_type = node[0]\n\n        # If the node is a function\n        if node_type == 'function':\n\n            # Get the node arity\n            node_arity = node[1]\n\n            # Get the valid functions with the same arity\n            valid_functions = self.valid_functions[node_arity]\n\n            # Get the", "CodeBleu_score": 24.957294643745502}, {"code": "def point_mutation(self, random_state)\n\t# Create a copy of the program\n    mutated_program = copy.deepcopy(self)\n\n    # Identify nodes to mutate\n    mutation_indices = []\n    for i, node in enumerate(mutated_program.nodes):\n        if random_state.uniform() < self.mutation_prob:\n            mutation_indices.append(i)\n\n    # Mutate the nodes\n    for i in mutation_indices:\n        node = mutated_program.nodes[i]\n        if node.type == \"function\":\n            # If the node is a function, find a valid replacement function with the same arity\n            valid_replacement_functions = [\n                f for f in mutated_program.function_set if f.arity == node.arity\n            ]\n            replacement_function = random_state.choice(valid_replacement_functions)\n            mutated_program.nodes[i] = replacement_function\n        elif node.type == \"terminal\":\n            # If the node is a terminal, replace it with either a constant or a variable\n            if random_state.uniform() < self.constant_prob:\n                # If the constant probability is greater than the variable probability, replace the node with a constant\n                mutated_program.nodes[i] = random_state.uniform(\n                    *self.constant_range\n                )\n            else:\n                # If the variable probability is greater than the constant probability, replace the node with a variable\n                mutated_program.nodes[i] = random_state.randint(\n                    self.n_features\n                )\n\n    return mutated_program, mutation_indices\n\n", "CodeBleu_score": 28.32813128954048}]}
{"instance_id": "f1853251e671", "generation_result": [{"code": "def bias(close, length=None, mamode=None, offset=None, **kwargs)\n\t# Validate Arguments\n    close = verify_series(close)\n    length = int(length) if length and length > 0 else 14\n    mamode = mamode if mamode and isinstance(mamode, str) else \"sma\"\n    offset = get_offset(offset)\n\n    # Calculate Result\n    bias = close.rolling(length, min_periods=length).apply(\n        lambda x: x[-1] - x.mean(), raw=True\n    )\n\n    # Offset\n    if offset != 0:\n        bias = bias.shift(offset)\n\n    # Handle fills\n    if \"fillna\" in kwargs:\n        bias.fillna(kwargs[\"fillna\"], inplace=True)\n    if \"fill_method\" in kwargs:\n        bias.fillna(method=kwargs[\"fill_method\"], inplace=True)\n\n    # Name & Category\n    bias.name = f\"BIAS_{length}\"\n    bias.category = \"momentum\"\n\n    return bias\n\n\n", "CodeBleu_score": 51.3634733249815}, {"code": "def bias(close, length=None, mamode=None, offset=None, **kwargs)\n\treturn\n\n", "CodeBleu_score": 1.76762510841005}, {"code": "def bias(close, length=None, mamode=None, offset=None, **kwargs)\n\treturn _ta_lib.bias(close, length=length, mamode=mamode, offset=offset, **kwargs)\n\n", "CodeBleu_score": 5.267912132703985}]}
{"instance_id": "77825be37c24", "generation_result": [{"code": "def coins_bar(broker='hb', code='btc', ktype='D', size='2000')\n\ttry:\n        if broker == 'hb':\n            url = f'https://api.huobi.pro/market/history/kline?period={ktype}&size={size}&symbol={code}'\n            data = requests.get(url).json()\n            df = pd.DataFrame(data['data'])\n            df.columns = ['date', 'open', 'high', 'low', 'close', 'volume', 'amount', 'count']\n            df['date'] = df['date'].apply(lambda x: int2time(x))\n            df = df.set_index('date')\n            df = df.astype(float)\n            return df\n        elif broker == 'bt':\n            url = f'https://api.btctradeua.com/api/v1/trades/{code}?limit={size}'\n            data = requests.get(url).json()\n            df = pd.DataFrame(data)\n            df.columns = ['date', 'price', 'amount', 'tid']\n            df['date'] = df['date'].apply(lambda x: int2time(x))\n            df = df.set_index('date')\n            df = df.astype(float)\n            return df\n        elif broker == 'ok':\n            url = f'https://www.okex.com/api/v1/kline.do?symbol={code}&type={ktype}&size={size}'\n            data = requests.get(url).json()\n            df = pd.DataFrame(data['kline'])\n            df.columns = ['date', 'open', 'high', 'low', 'close', 'volume']\n            df['date'] = df['date'].apply(lambda x: int2time(x))\n            df = df.set_index('date')\n            df = df.astype(float)\n            return df\n        elif broker == 'zb':\n            url = f'http://api.zb.com/data/v1/kline?type=1min&size={size}&symbol={code}'\n            data = requests.get(url).json()\n            df = pd.DataFrame(data['datas'])\n            df.columns = ['date', 'open', 'high',", "CodeBleu_score": 34.30298864982132}, {"code": "def coins_bar(broker='hb', code='btc', ktype='D', size='2000')\n\ttry:\n        if broker == 'hb':\n            df = _hb_coins_bar(code, ktype, size)\n            df = df.rename(columns={'vol': 'volume', 'amount': 'turnover'})\n            df['date'] = df['date'].apply(int2time)\n            df['date'] = pd.to_datetime(df['date'])\n        elif broker == 'binance':\n            df = _binance_coins_bar(code, ktype, size)\n            df = df.rename(columns={'volume': 'volume', 'quoteVolume': 'turnover'})\n            df['date'] = pd.to_datetime(df['date'])\n        elif broker == 'bitfinex':\n            df = _bitfinex_coins_bar(code, ktype, size)\n            df = df.rename(columns={'vol': 'volume', 'amount': 'turnover'})\n            df['date'] = df['date'].apply(int2time)\n            df['date'] = pd.to_datetime(df['date'])\n        elif broker == 'okex':\n            df = _okex_coins_bar(code, ktype, size)\n            df = df.rename(columns={'vol': 'volume', 'amount': 'turnover'})\n            df['date'] = pd.to_datetime(df['date'])\n        else:\n            raise ValueError('Broker not supported')\n        return df\n    except Exception:\n        print(traceback.print_exc())\n\n", "CodeBleu_score": 33.510854183185515}, {"code": "def coins_bar(broker='hb', code='btc', ktype='D', size='2000')\n\turl = f'http://api.coincap.io/v2/assets/{code}/history?interval={ktype}&limit={size}'\n    try:\n        r = requests.get(url)\n        js = r.json()\n        data = js['data']\n        df = pd.DataFrame(data)\n        df['date'] = pd.to_datetime(df['date'])\n        df.set_index('date', inplace=True)\n        return df\n    except Exception:\n        print(traceback.print_exc())\n\n", "CodeBleu_score": 23.985038715858163}]}
{"instance_id": "c6552851da8a", "generation_result": [{"code": "def coins_snapshot(broker='hb', code='btc', size='5')\n\ttry:\n        js = get_data(broker, code, size)\n        if broker == 'hb':\n            ask_time = [int2time(int(i)) for i in js['asks'][0::2]]\n            bid_time = [int2time(int(i)) for i in js['bids'][0::2]]\n        elif broker == 'ok':\n            ask_time = [int2time(int(i)) for i in js['timestamp']]\n            bid_time = ask_time\n        ask_data = pd.DataFrame(js['asks'][1::2], columns=['ask_price', 'ask_amount'])\n        bid_data = pd.DataFrame(js['bids'][1::2], columns=['bid_price', 'bid_amount'])\n        ask_data['ask_time'] = ask_time\n        bid_data['bid_time'] = bid_time\n        data = {'ask': ask_data, 'bid': bid_data}\n        panel = pd.Panel(data)\n        return panel\n    except Exception:\n        print(traceback.print_exc())\n\n", "CodeBleu_score": 47.9622541890116}, {"code": "def coins_snapshot(broker='hb', code='btc', size='5')\n\ttry:\n        # Create a dictionary to store the data\n        data = {}\n        # Fetch the data\n        js = get_snapshot_data(broker, code, size)\n        # Process the data\n        if broker == 'hb':\n            # Get the timestamp\n            dt = int2time(js['timestamp'])\n            # Get the ask data\n            ask = pd.DataFrame(js['asks'])\n            ask.columns = ['ask', 'size']\n            # Get the bid data\n            bid = pd.DataFrame(js['bids'])\n            bid.columns = ['bid', 'size']\n        elif broker == 'ok':\n            # Get the timestamp\n            dt = int2time(js['timestamp'])\n            # Get the ask data\n            ask = pd.DataFrame(js['asks'])\n            ask.columns = ['size', 'ask']\n            # Get the bid data\n            bid = pd.DataFrame(js['bids'])\n            bid.columns = ['size', 'bid']\n        # Add the timestamp to each dataframe\n        ask['timestamp'] = dt\n        bid['timestamp'] = dt\n        # Store the data in the dictionary\n        data['ask'] = ask\n        data['bid'] = bid\n        # Create and return a pandas Panel\n        return pd.Panel(data)\n    except Exception:\n        print(traceback.print_exc())\n\n\n\n", "CodeBleu_score": 38.75376788849457}, {"code": "def coins_snapshot(broker='hb', code='btc', size='5')\n\ttry:\n        # get data from broker\n        data = get_data(broker, code, size)\n        # process data\n        if broker == 'hb':\n            data = data['tick']['data']\n            ask = pd.DataFrame(data[0:int(len(data)/2)])\n            bid = pd.DataFrame(data[int(len(data)/2):])\n            ask['time'] = ask['id']\n            bid['time'] = bid['id']\n            ask['time'] = ask['time'].apply(int2time)\n            bid['time'] = bid['time'].apply(int2time)\n        elif broker == 'ok':\n            data = data['ticker']\n            ask = pd.DataFrame(data['asks'])\n            bid = pd.DataFrame(data['bids'])\n            ask['time'] = ask['id']\n            bid['time'] = bid['id']\n            ask['time'] = ask['time'].apply(int2time)\n            bid['time'] = bid['time'].apply(int2time)\n        # store data in dictionary\n        data_dict = {'ask': ask, 'bid': bid}\n        # create and return Panel\n        data_panel = pd.Panel.from_dict(data_dict)\n        return data_panel\n    except Exception:\n        print(traceback.print_exc())\n\n", "CodeBleu_score": 43.34194395131754}]}
{"instance_id": "8b5fee723398", "generation_result": [{"code": "def coins_trade(broker='hb', code='btc')\n\tif broker == 'hb':\n        url = 'https://api.huobi.pro/market/history/trade?symbol=' + code + 'usdt&size=100'\n    elif broker == 'ok':\n        url = 'https://www.okex.com/api/v1/trades.do?symbol=' + code + '_usdt'\n    elif broker == 'chbtc':\n        url = 'https://api.btc.com/v1/market/trades?symbol=' + code + 'usdt&size=100'\n    else:\n        print('Broker not supported')\n        return None\n\n    try:\n        r = requests.get(url)\n        if r.status_code != 200:\n            print('Broker error')\n            return None\n        js = r.json()\n        if broker == 'hb':\n            data = pd.DataFrame(js['data'])\n            data = data[['id', 'ts', 'price', 'amount', 'direction']]\n            data.columns = ['tid', 'datetime', 'price', 'amount', 'type']\n            data['datetime'] = data['datetime'].apply(int2time)\n        elif broker == 'ok':\n            data = pd.DataFrame(js['data'])\n            data = data[['tid', 'date', 'price', 'amount', 'type']]\n            data.columns = ['tid', 'datetime', 'price', 'amount', 'type']\n            data['datetime'] = data['datetime'].apply(int2time)\n        elif broker == 'chbtc':\n            data = pd.DataFrame(js['data'])\n            data = data[['tid', 'date', 'price', 'amount', 'type']]\n            data.columns = ['tid', 'datetime', 'price', 'amount', 'type']\n            data['datetime'] = data['datetime'].apply(int2time)\n        else:\n            print('Broker not supported')\n            return None\n        return data\n    except Exception:\n        print(traceback.print_exc())\n\n", "CodeBleu_score": 43.13991562123752}, {"code": "def coins_trade(broker='hb', code='btc')\n\tif broker == 'hb':\n        url = 'http://api.huobi.com/staticmarket/ticker_%s_json.js' % code\n        data = get_data(url)\n        df = pd.DataFrame(data['ticker'])\n        df = df[['id', 'price', 'amount', 'ts']]\n        df.columns = ['tid', 'price', 'amount', 'datetime']\n        df['datetime'] = df['datetime'].apply(int2time)\n        df = df[['tid', 'datetime', 'price', 'amount']]\n        return df\n    elif broker == 'ok':\n        url = 'https://www.okcoin.cn/api/v1/trades.do?symbol=%s' % code\n        data = get_data(url)\n        df = pd.DataFrame(data['data'])\n        df = df[['tid', 'price', 'amount', 'date']]\n        df.columns = ['tid', 'price', 'amount', 'datetime']\n        df['datetime'] = df['datetime'].apply(int2time)\n        df = df[['tid', 'datetime', 'price', 'amount']]\n        return df\n    elif broker == 'chbtc':\n        url = 'https://api.chbtc.com/data/v1/trades?currency=%s' % code\n        data = get_data(url)\n        df = pd.DataFrame(data['data'])\n        df = df[['tid', 'price', 'amount', 'date']]\n        df.columns = ['tid', 'price', 'amount', 'datetime']\n        df['datetime'] = df['datetime'].apply(int2time)\n        df = df[['tid', 'datetime', 'price', 'amount']]\n        return df\n\n\n\n", "CodeBleu_score": 37.71772647006622}, {"code": "def coins_trade(broker='hb', code='btc')\n\tif broker == 'hb':\n        url = 'http://api.huobi.com/staticmarket/ticker_btc_json.js'\n        data = get_data(url)\n        if data:\n            data = json.loads(data)\n            df = pd.DataFrame(data)\n            df = df.rename(columns={'buy': 'price', 'sell': 'amount', 'time': 'datetime'})\n            df['datetime'] = df['datetime'].apply(int2time)\n            df = df[['price', 'amount', 'datetime']]\n            df = df.rename(columns={'price': 'price', 'amount': 'amount', 'datetime': 'datetime'})\n            return df\n    elif broker == 'ok':\n        url = 'https://www.okcoin.cn/api/v1/trades.do?symbol=' + code\n        data = get_data(url)\n        if data:\n            data = json.loads(data)\n            df = pd.DataFrame(data['trades'])\n            df = df.rename(columns={'price': 'price', 'amount': 'amount', 'date': 'datetime'})\n            df['datetime'] = df['datetime'].apply(int2time)\n            df = df[['price', 'amount', 'datetime']]\n            df = df.rename(columns={'price': 'price', 'amount': 'amount', 'datetime': 'datetime'})\n            return df\n    elif broker == 'chbtc':\n        url = 'https://api.btcchina.com/data/historydata?market=btccny&last=5000&since='\n        data = get_data(url)\n        if data:\n            data = json.loads(data)\n            df = pd.DataFrame(data['trades'])\n            df = df.rename(columns={'price': 'price', 'amount': 'amount', 'date': 'datetime'})\n            df['datetime'] = df['datetime'].apply(int2time)\n            df = df[['price', 'amount', 'datetime']]\n            df = df.rename(columns={'price': 'price', 'amount':", "CodeBleu_score": 38.8490565266534}]}
{"instance_id": "25e0a134c45b", "generation_result": [{"code": "def _parse_fund_data(url, fund_type='open')\n\timport pandas as pd\n    import requests\n    from bs4 import BeautifulSoup\n    import json\n\n    try:\n        response = requests.get(url)\n        response.raise_for_status()\n    except requests.exceptions.HTTPError as err:\n        print(err)\n    except requests.exceptions.RequestException as err:\n        print(err)\n    else:\n        print(\"Request was successful!\")\n        soup = BeautifulSoup(response.text, 'html.parser')\n        text = soup.find('script', type='text/javascript').string\n        text = text.replace('var data = ', '')\n        text = text.replace(';', '')\n        text = text.replace('\"', '')\n        text = text.replace(':', '')\n        text = text.replace('[', '')\n        text = text.replace(']', '')\n        text = text.replace('{', '')\n        text = text.replace('}', '')\n        text = text.replace('null', '')\n        text = text.replace('\"', '')\n        text = text.replace(':', '')\n        text = text.replace('[', '')\n        text = text.replace(']', '')\n        text = text.replace('{', '')\n        text = text.replace('}', '')\n        text = text.replace('null', '')\n        text = text.replace('\"', '')\n        text = text.replace(':', '')\n        text = text.replace('[', '')\n        text = text.replace(']', '')\n        text = text.replace('{', '')\n        text = text.replace('}', '')\n        text = text.replace('null', '')\n        text = text.replace('\"', '')\n        text = text.replace(':', '')\n        text = text.replace('[', '')\n        text = text.replace(']', '')\n        text = text.replace('{', '')\n        text = text.replace('}', '')\n        text = text.replace('null', '')\n        text = text.replace('\"', '')\n        text = text.replace(':', '')\n        text = text.replace('[', '')\n        text = text.replace(']', '')\n        text", "CodeBleu_score": 25.546509706180775}, {"code": "def _parse_fund_data(url, fund_type='open')\n\timport requests\n    import pandas as pd\n    import json\n    from datetime import datetime\n\n    # Write to console\n    print('Retrieving data...')\n\n    # Attempt to request the URL\n    try:\n        response = requests.get(url)\n    except:\n        print('Error: Failed to retrieve data.')\n        return\n\n    # Read the response\n    try:\n        response.encoding = 'utf-8'\n        data = response.text\n    except:\n        print('Error: Failed to read data.')\n        return\n\n    # Extract relevant data\n    try:\n        # Remove the first and last characters\n        data = data[1:-1]\n\n        # Remove the first and last lines\n        data = '", "CodeBleu_score": 18.031814931045922}, {"code": "def _parse_fund_data(url, fund_type='open')\n\timport requests\n    import pandas as pd\n    import json\n    import re\n    from datetime import datetime\n    import os\n    from dotenv import load_dotenv\n    load_dotenv()\n    api_key = os.getenv(\"API_KEY\")\n    # print(api_key)\n    # print(url)\n    try:\n        response = requests.get(url)\n        response.raise_for_status()\n        # print(response.text)\n        # print(response.encoding)\n        response.encoding = 'utf-8'\n        # print(response.text)\n        # print(response.json())\n        data = response.json()\n        # print(data)\n        if fund_type == 'open':\n            # print(data)\n            fund_data = data['openFundData']\n            # print(fund_data)\n            # print(fund_data['fundData'])\n            fund_data = fund_data['fundData']\n            # print(fund_data)\n            # print(fund_data['fundName'])\n            fund_name = fund_data['fundName']\n            # print(fund_name)\n            # print(fund_data['fundType'])\n            fund_type = fund_data['fundType']\n            # print(fund_type)\n            # print(fund_data['fundManager'])\n            fund_manager = fund_data['fundManager']\n            # print(fund_manager)\n            # print(fund_data['fundManagerTenure'])\n            fund_manager_tenure = fund_data['fundManagerTenure']\n            # print(fund_manager_tenure)\n            # print(fund_data['fundManagerBio'])\n            fund_manager_bio = fund_data['fundManagerBio']\n            # print(fund_manager_bio)\n            # print(fund_data['fundManagerTwitter'])\n            fund_manager_twitter = fund_data['fundManagerTwitter']\n            # print(fund_manager_twitter)\n            # print(fund_data['fundManagerLinkedin'])\n            fund_manager_linkedin = fund_data['fundManagerLinkedin']\n            # print(fund_manager_link", "CodeBleu_score": 27.5552412500054}]}
{"instance_id": "401d18e122d8", "generation_result": [{"code": "def get_future_daily(start = None, end = None, market = 'CFFEX')\n\tif market == 'CFFEX':\n        return get_future_daily_CFFEX(start, end)\n    elif market == 'SHFE':\n        return get_future_daily_SHFE(start, end)\n    elif market == 'DCE':\n        return get_future_daily_DCE(start, end)\n    elif market == 'CZCE':\n        return get_future_daily_CZCE(start, end)\n    else:\n        raise ValueError(\"Invalid market, please choose from 'CFFEX', 'SHFE', 'DCE', 'CZCE'\")\n", "CodeBleu_score": 12.845786654115965}, {"code": "def get_future_daily(start = None, end = None, market = 'CFFEX')\n\tif market == 'CFFEX':\n        func = get_future_daily_cffex\n    elif market == 'SHFE':\n        func = get_future_daily_shfe\n    elif market == 'DCE':\n        func = get_future_daily_dce\n    elif market == 'CZCE':\n        func = get_future_daily_czce\n    else:\n        raise ValueError('Invalid market.')\n\n    if start is None:\n        start = datetime.date.today()\n    if end is None:\n        end = datetime.date.today()\n\n    start = start.strftime('%Y%m%d')\n    end = end.strftime('%Y%m%d')\n\n    data = []\n\n    for date in pd.date_range(start, end):\n        df = func(date)\n        if df is not None:\n            data.append(df)\n\n    if len(data) > 0:\n        return pd.concat(data)\n    else:\n        return None\n\n", "CodeBleu_score": 51.64788486072207}, {"code": "def get_future_daily(start = None, end = None, market = 'CFFEX')\n\tif market == 'CFFEX':\n        return get_future_daily_CFFEX(start, end)\n    elif market == 'SHFE':\n        return get_future_daily_SHFE(start, end)\n    elif market == 'DCE':\n        return get_future_daily_DCE(start, end)\n    elif market == 'CZCE':\n        return get_future_daily_CZCE(start, end)\n    else:\n        raise ValueError(\"Invalid market code.\")\n\n\n", "CodeBleu_score": 12.035529527281506}]}
{"instance_id": "697779825988", "generation_result": [{"code": "def realtime_boxoffice(retry_count=3,pause=0.001)\n\timport requests\n    import time\n    import pandas as pd\n    import json\n    from datetime import datetime\n    import warnings\n    warnings.filterwarnings(\"ignore\")\n    url = 'https://www.cgv.co.kr/common/showtimes/ajaxGetShowtimeList.do'\n    for i in range(retry_count):\n        try:\n            response = requests.get(url)\n            if response.status_code == 200:\n                data = json.loads(response.text)\n                df = pd.DataFrame(data['data2'])\n                df = df.drop(['MovieImg','mId'],axis=1)\n                df['timestamp'] = datetime.now()\n                return df\n            else:\n                time.sleep(pause)\n        except:\n            time.sleep(pause)", "CodeBleu_score": 31.930729549413144}, {"code": "def realtime_boxoffice(retry_count=3,pause=0.001)\n\timport pandas as pd\n    import requests\n    import time\n    import json\n    from requests.exceptions import HTTPError\n    from requests.exceptions import ConnectionError\n    from requests.exceptions import Timeout\n    from requests.exceptions import RequestException\n    url = 'https://www.cgv.co.kr/common/showtimes/ajaxGetShowtimeAndTicketInfo.json'\n    headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.93 Safari/537.36'}\n    for i in range(retry_count):\n        try:\n            response = requests.get(url, headers=headers)\n            response.raise_for_status()\n            if len(response.text) > 0:\n                data = json.loads(response.text)\n                df = pd.json_normalize(data['data2'])\n                df = df.drop(columns=['MovieImg', 'mId'])\n                df['timestamp'] = pd.to_datetime('now')\n                return df\n            else:\n                raise ValueError('No data found')\n        except (HTTPError, ConnectionError, Timeout, RequestException, ValueError) as e:\n            print(e)\n            time.sleep(pause)\n    return None", "CodeBleu_score": 31.432107491392625}, {"code": "def realtime_boxoffice(retry_count=3,pause=0.001)\n\timport requests\n    import json\n    import time\n    import pandas as pd\n    import numpy as np\n    from datetime import datetime\n\n    # Specify URL and parameters\n    url = 'https://www.cgv.co.kr/common/showtimes/iframeTheater.aspx'\n    params = {'areacode': '01',\n              'theatercode': '0074'}\n\n    # Make request and parse JSON\n    while True:\n        try:\n            response = requests.get(url, params=params)\n            data = json.loads(response.text)\n            df = pd.DataFrame(data['data2'])\n            df = df.drop(columns=['MovieImg', 'mId'])\n            df['timestamp'] = datetime.now()\n            return df\n        except (json.decoder.JSONDecodeError, KeyError, requests.exceptions.RequestException):\n            if retry_count > 0:\n                print(f'Request failed. Retrying in {pause} seconds...')\n                retry_count -= 1\n                time.sleep(pause)\n            else:\n                raise", "CodeBleu_score": 31.081132192491758}]}
