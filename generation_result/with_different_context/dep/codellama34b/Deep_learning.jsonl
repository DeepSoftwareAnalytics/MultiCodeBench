{"instance_id": "1f9f4d85e825", "generation_result": [{"code": "def load_model_from_config(self, half_attention)\n\timport os\n    import torch\n    import yaml\n    from pathlib import Path\n    from .utils import load_model\n    from .utils import get_model_config\n    from .utils import get_model_cache\n    from .utils import get_model_cache_path\n    from .utils import get_model_cache_status\n    from .utils import get_model_cache_timestamp\n    from .utils import get_model_cache_validity\n    from .utils import get_model_cache_version\n    from .utils import get_model_path\n    from .utils import get_model_version\n    from .utils import get_model_config_path\n    from .utils import get_model_cache_path\n    from .utils import get_model_cache_status\n    from .utils import get_model_cache_timestamp\n    from .utils import get_model_cache_validity\n    from .utils import get_model_cache_version\n    from .utils import get_model_path\n    from .utils import get_model_version\n    from .utils import get_model_config_path\n    from .utils import get_model_cache_path\n    from .utils import get_model_cache_status\n    from .utils import get_model_cache_timestamp\n    from .utils import get_model_cache_validity\n    from .utils import get_model_cache_version\n    from .utils import get_model_path\n    from .utils import get_model_version\n    from .utils import get_model_config_path\n    from .utils import get_model_cache_path\n    from .utils import get_model_cache_status\n    from .utils import get_model_cache_timestamp\n    from .utils import get_model_cache_validity\n    from .utils import get_model_cache_version\n    from .utils import get_model_path\n    from .utils import get_model_version\n    from .utils import get_model_config_path\n    from .utils import get_model_cache_path\n    from .utils import get_model_cache_status\n    from .utils import get_model_cache_timestamp\n    from .utils import get_model_cache_validity\n    from", "CodeBleu_score": 8.681131068099788}, {"code": "def load_model_from_config(self, half_attention)\n\t# Check if a cached model is available and valid\n    if self.cache_model:\n        if self.model_path in self.cached_models:\n            if self.cached_models[self.model_path] is not None:\n                if self.verbose:\n                    print(\"Loading model from cache\")\n                return self.cached_models[self.model_path]\n    # Load the model from the specified path\n    if self.verbose:\n        print(\"Loading model from {}\".format(self.model_path))\n    # Load the model from a .safetensors file\n    if self.model_path.endswith(\".safetensors\"):\n        with open(self.model_path, \"rb\") as f:\n            model = torch.load(f)\n    # Load the model from a PyTorch-compatible file\n    else:\n        model = torch.load(self.model_path, map_location=self.device)\n    # Configure the model\n    if self.verbose:\n        print(\"Configuring model\")\n    model = self.configure_model(model, self.model_config_path, self.device, self.verbose)\n    # Move the model to the appropriate device\n    if self.verbose:\n        print(\"Moving model to {}\".format(self.device))\n    model = model.to(self.device)\n    # Ensure the model is in evaluation mode\n    if self.verbose:\n        print(\"Ensuring model is in evaluation mode\")\n    model.eval()\n    # Optionally cache the loaded model for future use\n    if self.cache_model:\n        self.cached_models[self.model_path] = model\n    # Optionally optimize the model's memory format\n    if self.memory_format is not None:\n        if self.verbose:\n            print(\"Optimizing model's memory format\")\n        model = model.to(memory_format=self.memory_format)\n    # Optionally convert the model to half-precision\n    if half_attention:\n        if self.verbose:\n            print(\"Converting model to half-precision\")\n        model", "CodeBleu_score": 39.87157763546464}, {"code": "def load_model_from_config(self, half_attention)\n\timport os\n    import sys\n    import time\n    import yaml\n    import torch\n    from . import safetensors\n    from . import safetorch\n    from . import safelayers\n    from . import safepath\n    from . import safecache\n    from . import safetorch\n\n    # Load the configuration file.\n    with open(self.config_path, \"r\") as stream:\n        try:\n            config = yaml.safe_load(stream)\n        except yaml.YAMLError as exc:\n            print(exc)\n            sys.exit(1)\n\n    # Set the default device.\n    device = torch.device(\"cpu\")\n\n    # Set the default memory format.\n    memory_format = torch.contiguous_format\n\n    # Set the default precision.\n    precision = torch.float32\n\n    # Set the default cache path.\n    cache_path = None\n\n    # Set the default cache expiration.\n    cache_expiration = None\n\n    # Set the default cache mode.\n    cache_mode = None\n\n    # Set the default cache size.\n    cache_size = None\n\n    # Set the default cache directory.\n    cache_directory = None\n\n    # Set the default cache file.\n    cache_file = None\n\n    # Set the default cache file path.\n    cache_file_path = None\n\n    # Set the default cache file directory.\n    cache_file_directory = None\n\n    # Set the default cache file name.\n    cache_file_name = None\n\n    # Set the default cache file extension.\n    cache_file_extension = None\n\n    # Set the default cache file format.\n    cache_file_format = None\n\n    # Set the default cache file type.\n    cache_file_type = None\n\n    # Set the default cache file directory path.\n    cache_file_directory_path = None\n\n    # Set the default cache file directory name.\n    cache_file_directory_name = None\n\n    # Set the default cache file directory extension.\n    cache_file_directory_extension = None\n\n    # Set the default cache file directory format.", "CodeBleu_score": 20.087526658900966}]}
{"instance_id": "8dc955007aa2", "generation_result": [{"code": "def convsample_ddim(model, cond, steps, shape, eta=1.0, callback=None, normals_sequence=None,\n                    mask=None, x0=None, quantize_x0=False, temperature=1., score_corrector=None,\n                    corrector_kwargs=None, x_t=None\n                    )\n\tsampler = DDIMSampler(model, eta=eta)\n    batch_size = shape[0]\n    print(f\"eta: {eta}\")\n    print(f\"steps: {steps}\")\n    x, intermediates = sampler.sample(\n        steps, batch_size, shape, cond, callback=callback, normals_sequence=normals_sequence, mask=mask, x0=x0, quantize_x0=quantize_x0, temperature=temperature, score_corrector=score_corrector, corrector_kwargs=corrector_kwargs, x_t=x_t\n    )\n    return x, intermediates\n\n", "CodeBleu_score": 38.0924466249506}, {"code": "def convsample_ddim(model, cond, steps, shape, eta=1.0, callback=None, normals_sequence=None,\n                    mask=None, x0=None, quantize_x0=False, temperature=1., score_corrector=None,\n                    corrector_kwargs=None, x_t=None\n                    )\n\tsampler = DDIMSampler(model, eta=eta)\n    batch_size = shape[0]\n    print(f'eta: {eta}')\n    print(f'steps: {steps}')\n    x, intermediates = sampler.sample(steps, batch_size, shape, cond, callback=callback, normals_sequence=normals_sequence,\n                                      mask=mask, x0=x0, quantize_x0=quantize_x0, temperature=temperature, score_corrector=score_corrector,\n                                      corrector_kwargs=corrector_kwargs, x_t=x_t)\n    return x, intermediates\n\n", "CodeBleu_score": 43.78412511946697}, {"code": "def convsample_ddim(model, cond, steps, shape, eta=1.0, callback=None, normals_sequence=None,\n                    mask=None, x0=None, quantize_x0=False, temperature=1., score_corrector=None,\n                    corrector_kwargs=None, x_t=None\n                    )\n\tprint(f'eta: {eta}')\n    print(f'steps: {steps}')\n    print(f'batch size: {shape[0]}')\n    sampler = DDIMSampler(model, eta=eta)\n    return sampler.sample(steps, shape, cond, callback=callback, normals_sequence=normals_sequence, mask=mask,\n                          x0=x0, quantize_x0=quantize_x0, temperature=temperature, score_corrector=score_corrector,\n                          corrector_kwargs=corrector_kwargs, x_t=x_t)\n\n", "CodeBleu_score": 32.18528687783308}]}
{"instance_id": "9bdcd9081480", "generation_result": [{"code": "def ema_scope(self, context=None)\n\tif not self.ema_weights:\n        return\n    if context:\n        print(f\"Switching to EMA weights for {context}\")\n    self.ema_weights.copy_to(self.weights)\n    if context:\n        print(f\"Switched to EMA weights for {context}\")\n\n\n", "CodeBleu_score": 24.699537722130874}, {"code": "def ema_scope(self, context=None)\n\tif not self.ema_enabled:\n        return\n    if context is not None:\n        print(f\"{context}: Using EMA weights\")\n    self.ema.restore()\n    yield\n    self.ema.store()\n    if context is not None:\n        print(f\"{context}: Restored original weights\")\n\n", "CodeBleu_score": 48.40183459192033}, {"code": "def ema_scope(self, context=None)\n\tif not self.ema_enabled:\n        return\n    if context:\n        print(f\"Using EMA weights for {context}\")\n    self.ema_weights = [w.clone() for w in self.model.parameters()]\n    for ema_w, w in zip(self.ema_weights, self.model.parameters()):\n        ema_w.data.copy_(w.data)\n    self.model.parameters()\n    for ema_w, w in zip(self.ema_weights, self.model.parameters()):\n        w.data.copy_(ema_w.data)\n    if context:\n        print(f\"Restored original weights after {context}\")\n\n\n", "CodeBleu_score": 40.87313427243019}]}
{"instance_id": "14ab009e1c5c", "generation_result": [{"code": "def init_from_ckpt(self, path, ignore_keys=None)\n\tif ignore_keys is None:\n        ignore_keys = []\n    state_dict = torch.load(path, map_location='cpu')\n    for key in ignore_keys:\n        state_dict.pop(key)\n    self.load_state_dict(state_dict, strict=False)\n    missing_keys, unexpected_keys = self.load_state_dict(state_dict, strict=False)\n    if len(missing_keys) > 0:\n        print('Missing keys:')\n        print(missing_keys)\n    if len(unexpected_keys) > 0:\n        print('Unexpected keys:')\n        print(unexpected_keys)\n\n\n", "CodeBleu_score": 34.43733649393552}, {"code": "def init_from_ckpt(self, path, ignore_keys=None)\n\tif ignore_keys is None:\n        ignore_keys = []\n    sd = torch.load(path, map_location=\"cpu\")[\"state_dict\"]\n    keys = list(sd.keys())\n    for k in keys:\n        for ik in ignore_keys:\n            if k.startswith(ik):\n                print(\"Deleting key {} from state_dict.\".format(k))\n                del sd[k]\n    self.load_state_dict(sd, strict=False)\n    missing_keys = self.load_state_dict(sd, strict=False)\n    if len(missing_keys) > 0:\n        print(\"Missing Keys: {}\".format(missing_keys))\n    unexpected_keys = [k for k in sd.keys() if k not in self.state_dict()]\n    if len(unexpected_keys) > 0:\n        print(\"Unexpected Keys: {}\".format(unexpected_keys))\n\n", "CodeBleu_score": 61.39458293664097}, {"code": "def init_from_ckpt(self, path, ignore_keys=None)\n\tstate_dict = torch.load(path, map_location=self.device)\n\n    if ignore_keys is not None:\n        for key in ignore_keys:\n            if key in state_dict:\n                del state_dict[key]\n\n    missing_keys = []\n    unexpected_keys = []\n    error_msgs = []\n    # copy state_dict so _load_from_state_dict can modify it\n    metadata = getattr(state_dict, '_metadata', None)\n    state_dict = state_dict.copy()\n    if metadata is not None:\n        state_dict._metadata = metadata\n\n    def load(module, prefix=''):\n        local_metadata = {} if metadata is None else metadata.get(prefix[:-1], {})\n        module._load_from_state_dict(\n            state_dict, prefix, local_metadata, True, missing_keys, unexpected_keys, error_msgs)\n        for name, child in module._modules.items():\n            if child is not None:\n                load(child, prefix + name + '.')\n\n    load(self)\n    print(\"missing keys: \", missing_keys)\n    print(\"unexpected keys: \", unexpected_keys)\n    if len(missing_keys) > 0:\n        print(\"missing keys: \", missing_keys)\n        raise KeyError(f\"Missing keys in state_dict: {missing_keys}\")\n    if len(unexpected_keys) > 0:\n        print(\"unexpected keys: \", unexpected_keys)\n        raise KeyError(f\"Unexpected keys in state_dict: {unexpected_keys}\")\n\n", "CodeBleu_score": 31.1404582976488}]}
{"instance_id": "754176e27b81", "generation_result": [{"code": "def forward(self, input, return_pred_indices=False)\n\tquant, diff, info = self.encode(input)\n    dec = self.decode(quant)\n    if return_pred_indices:\n        return dec, diff, info\n    else:\n        return dec, diff\n\n", "CodeBleu_score": 50.97775916518488}, {"code": "def forward(self, input, return_pred_indices=False)\n\tquant, diff, _ = self.encode(input)\n    dec = self.decode(quant)\n    if return_pred_indices:\n        return dec, diff, quant\n    else:\n        return dec, diff\n\n", "CodeBleu_score": 50.32552376744825}, {"code": "def forward(self, input, return_pred_indices=False)\n\tquant, diff, info = self.encode(input)\n    dec = self.decode(quant)\n    if return_pred_indices:\n        return dec, diff, info[\"pred_indices\"]\n    else:\n        return dec, diff\n\n", "CodeBleu_score": 45.21616644425962}]}
{"instance_id": "443df6fdaf0d", "generation_result": [{"code": "def get_input(self, batch, k)\n\tx = batch[k]\n    if len(x.shape) == 3:\n        x = x[..., None]\n    if self.opt.resize_factors is not None:\n        x = self.resize(x)\n    if self.opt.resize_factors is not None:\n        x = self.resize(x)\n    x = x.permute(0, 3, 1, 2).contiguous().float()\n    x = x.detach()\n    return x\n\n", "CodeBleu_score": 34.03074907284658}, {"code": "def get_input(self, batch, k)\n\tx = batch[k]\n    if x.dim() == 3:\n        x = x[None]\n    x = x.permute(0, 3, 1, 2).contiguous().to(self.device)\n    if self.opt.resize_batch:\n        x = self.resize_batch(x)\n    x = x.detach()\n    return x\n\n", "CodeBleu_score": 25.52192133627948}, {"code": "def get_input(self, batch, k)\n\tx = batch[k]\n    if len(x.shape) == 3:\n        x = x[None, :, :, :]\n    if self.opt.resize_factors is not None:\n        x = self.resize_input(x)\n    x = x.permute(0, 3, 1, 2)\n    x = x.contiguous().float()\n    x = x.detach()\n    return x\n\n", "CodeBleu_score": 27.950944137674366}]}
{"instance_id": "953a2632fc15", "generation_result": [{"code": "def training_step(self, batch, batch_idx, optimizer_idx)\n\t# get input and target\n    x = self.get_input(batch, \"x\")\n    y = self.get_input(batch, \"y\")\n    # get prediction\n    if self.global_step <= self.warmup_steps:\n        # use the prediction from the encoder\n        z = self.encoder(x)\n    else:\n        # use the prediction from the decoder\n        z = self.decoder(x)\n    # compute loss\n    if optimizer_idx == 0:\n        # compute autoencoding loss\n        loss = self.autoencoder_loss(z, x)\n        self.log(\"train_autoencoding_loss\", loss, on_epoch=True, prog_bar=True, logger=True)\n    else:\n        # compute discriminator loss\n        loss = self.discriminator_loss(z, y)\n        self.log(\"train_discriminator_loss\", loss, on_epoch=True, prog_bar=True, logger=True)\n    # log images\n    if self.global_step % self.log_image_interval == 0:\n        self.log_images(x, y, z)\n    return loss\n", "CodeBleu_score": 30.934132237811525}, {"code": "def training_step(self, batch, batch_idx, optimizer_idx)\n\tx = self.get_input(batch, \"x\")\n    x_hat = self.forward(x)\n    loss = self.compute_loss(x, x_hat, optimizer_idx)\n    self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n    return loss", "CodeBleu_score": 15.049107083296914}, {"code": "def training_step(self, batch, batch_idx, optimizer_idx)\n\tx = self.get_input(batch, self.image_key)\n    xrec, qloss = self(x)\n\n    if optimizer_idx == 0:\n        # autoencoder\n        aeloss, log_dict_ae = self.loss(qloss, x, xrec, optimizer_idx, self.global_step,\n                                        last_layer=self.get_last_layer(), split=\"train\")\n\n        self.log_dict(log_dict_ae, prog_bar=False, logger=True, on_step=True, on_epoch=True)\n        return aeloss\n\n    if optimizer_idx == 1:\n        # discriminator\n        discloss, log_dict_disc = self.loss(qloss, x, xrec, optimizer_idx, self.global_step,\n                                            last_layer=self.get_last_layer(), split=\"train\")\n        self.log_dict(log_dict_disc, prog_bar=False, logger=True, on_step=True, on_epoch=True)\n        return discloss\n\n", "CodeBleu_score": 68.83590450483578}]}
{"instance_id": "b449b5e8b77f", "generation_result": [{"code": "def log_images(self, batch, only_inputs=False, plot_ema=False, **kwargs)\n\tif self.global_step == 0:\n        return\n    if self.global_step % self.log_images_every != 0:\n        return\n    if not self.logging_root_dir:\n        return\n    if not hasattr(self, \"first_batch\"):\n        self.first_batch = batch\n    if not hasattr(self, \"x_mean\"):\n        self.x_mean = 0.\n    if not hasattr(self, \"x_std\"):\n        self.x_std = 0.\n    if not hasattr(self, \"x_logvar\"):\n        self.x_logvar = 0.\n    if not hasattr(self, \"x_min\"):\n        self.x_min = 0.\n    if not hasattr(self, \"x_max\"):\n        self.x_max = 0.\n    if not hasattr(self, \"x_histogram_bins\"):\n        self.x_histogram_bins = 0.\n    if not hasattr(self, \"x_histogram_values\"):\n        self.x_histogram_values = 0.\n    if not hasattr(self, \"x_histogram_max\"):\n        self.x_histogram_max = 0.\n    if not hasattr(self, \"x_histogram_min\"):\n        self.x_histogram_min = 0.\n    if not hasattr(self, \"x_histogram_width\"):\n        self.x_histogram_width = 0.\n    if not hasattr(self, \"x_histogram_height\"):\n        self.x_histogram_height = 0.\n    if not hasattr(self, \"x_image_shape\"):\n        self.x_image_shape = 0.\n    if not hasattr(self, \"x_range\"):\n        self.x_range = 0.\n    if not hasattr(self, \"x_num_channels\"):\n        self.x_num_channels = 0.\n    if not hasattr(self,", "CodeBleu_score": 13.93147635956784}, {"code": "def log_images(self, batch, only_inputs=False, plot_ema=False, **kwargs)\n\tif not self.log_input:\n        return\n    if only_inputs:\n        return\n    if not hasattr(self, \"rec_x\"):\n        self.rec_x = self.get_input(batch, self.image_key)\n    if plot_ema and hasattr(self, \"ema\"):\n        self.rec_x_ema = self.ema(self.rec_x)\n        self.rec_x_ema = self.to_rgb(self.rec_x_ema)\n        self.rec_x = self.to_rgb(self.rec_x)\n        self.log_kimgs(self.rec_x_ema, \"rec_x_ema\", self.global_step)\n    self.rec_x = self.to_rgb(self.rec_x)\n    self.log_kimgs(self.rec_x, \"rec_x\", self.global_step)\n\n", "CodeBleu_score": 27.45737600347117}, {"code": "def log_images(self, batch, only_inputs=False, plot_ema=False, **kwargs)\n\tlog_images_kwargs = self.log_images_settings.copy()\n    log_images_kwargs.update(kwargs)\n    log_input = log_images_kwargs.pop(\"log_input\")\n    log_recon = log_images_kwargs.pop(\"log_recon\")\n    log_rec_target = log_images_kwargs.pop(\"log_rec_target\")\n    if log_rec_target:\n        log_recon = True\n    log_input_ema = log_images_kwargs.pop(\"log_input_ema\")\n    log_recon_ema = log_images_kwargs.pop(\"log_recon_ema\")\n    if log_recon_ema:\n        log_recon = True\n    if only_inputs:\n        log_recon = False\n    if not log_input and not log_recon:\n        return\n    if log_input:\n        x = self.get_input(batch, self.image_key)\n        if self.image_key == \"segmentation\":\n            x = self.to_rgb(x)\n        x = x.detach().cpu()\n        if log_input_ema:\n            x_ema = self.get_input(batch, self.image_key)\n            if self.image_key == \"segmentation\":\n                x_ema = self.to_rgb(x_ema)\n            x_ema = x_ema.detach().cpu()\n    if log_recon:\n        if self.image_key == \"segmentation\":\n            x_rec = self.decode(self.forward(batch)[0])\n            x_rec = self.to_rgb(x_rec)\n        else:\n            x_rec = self.decode(self.forward(batch)[0])\n        x_rec = x_rec.detach().cpu()\n        if log_recon_ema:\n            if self.image_key == \"segmentation\":\n                x_rec_ema = self.decode(self.ema_forward(batch)[0])\n                x_rec_ema = self.to_rgb(x_rec_ema)\n            else:\n                x_rec_ema = self.decode(self", "CodeBleu_score": 33.48580071940326}]}
{"instance_id": "540d089e3481", "generation_result": [{"code": "def get_loss(self, pred, target, mean=True)\n\tif self.loss_type == 'l1':\n        loss = torch.abs(pred - target)\n        if mean:\n            loss = torch.mean(loss)\n    elif self.loss_type == 'l2':\n        loss = torch.pow(pred - target, 2)\n        if mean:\n            loss = torch.mean(loss)\n    else:\n        raise ValueError('Unknown loss type')\n\n    return loss\n\n", "CodeBleu_score": 46.88028889900488}, {"code": "def get_loss(self, pred, target, mean=True)\n\tif self.loss_type == 'l1':\n        loss = torch.abs(pred - target)\n        if mean:\n            loss = torch.mean(loss)\n    elif self.loss_type == 'l2':\n        loss = torch.pow(pred - target, 2)\n        if mean:\n            loss = torch.mean(loss)\n    else:\n        raise ValueError(f'Unknown loss type: {self.loss_type}')\n    return loss\n\n", "CodeBleu_score": 45.53203143489907}, {"code": "def get_loss(self, pred, target, mean=True)\n\tif self.loss_type == 'l1':\n        loss = torch.mean(torch.abs(pred - target), dim=0)\n        if mean:\n            loss = torch.mean(loss)\n    elif self.loss_type == 'l2':\n        loss = torch.mean((pred - target)**2, dim=0)\n        if mean:\n            loss = torch.mean(loss)\n    else:\n        raise ValueError('Unknown loss type: {}'.format(self.loss_type))\n    return loss", "CodeBleu_score": 49.06294271260337}]}
{"instance_id": "63a08283666f", "generation_result": [{"code": "def forward(self, x, c, *args, **kwargs)\n\tx_start = x\n    x_cond = self.get_conditioning(c)\n    if self.cond_stage_forward is not None:\n        x_cond = self.get_learned_conditioning(x_cond)\n    x_cond = self.get_conditioned_input(x_start, x_cond)\n    loss, loss_dict = self.p_losses(x_start, x_cond)\n    return loss, loss_dict\n\n", "CodeBleu_score": 12.791880414968965}, {"code": "def forward(self, x, c, *args, **kwargs)\n\tif self.cond_stage_model is not None:\n        c = self.get_learned_conditioning(c)\n    else:\n        c = self.get_conditioning(c)\n\n    if self.cond_stage_model is not None:\n        x = self.cond_stage_model(x)\n\n    if self.cond_stage_model is not None:\n        x = self.model(x, c, *args, **kwargs)\n    else:\n        x = self.model(x, c=c, *args, **kwargs)\n    return x\n\n", "CodeBleu_score": 34.2982642009994}, {"code": "def forward(self, x, c, *args, **kwargs)\n\tif self.cond_stage_model is not None:\n        if self.cond_stage_model_type == 'learned':\n            c = self.get_learned_conditioning(c)\n        else:\n            raise NotImplementedError(f\"Conditioning model type {self.cond_stage_model_type} not yet supported\")\n    if self.cond_stage_forward is not None:\n        assert hasattr(self.cond_stage_model, self.cond_stage_forward)\n        c = getattr(self.cond_stage_model, self.cond_stage_forward)(c)\n    if self.cond_stage_model_type == 'learned':\n        c = c.mean\n    x = self.process_input(x, c)\n    return super().forward(x, *args, **kwargs)\n", "CodeBleu_score": 35.719070065751374}]}
{"instance_id": "4627e1e8ed62", "generation_result": [{"code": "def get_codebook_entry(self, indices, shape)\n\tif self.remap:\n        indices = self.unmap_to_all(indices)\n    latents = F.embedding(indices, self.get_codebook_tensor())\n    if self.permutation is not None:\n        latents = latents.permute(self.permutation)\n    return latents.reshape(shape)\n\n", "CodeBleu_score": 34.4864473435544}, {"code": "def get_codebook_entry(self, indices, shape)\n\tif self.remap_to_used is not None:\n        indices = self.remap_to_used(indices)\n    shape = list(shape)\n    shape_prefix = shape[:-1]\n    length = shape[-1]\n    assert length == self.embedding_dim\n    flat_indices = indices.view(-1, length)\n    used_size = self.used_size\n    if self.used is not None:\n        used = self.used.to(indices)\n        mask = used[flat_indices]\n        mask = mask.view(*shape_prefix, length)\n        mask = mask.permute(0, 2, 1, 3)\n        mask = mask.reshape(shape_prefix + [length, -1])\n        indices = indices.unsqueeze(-1)\n        indices = indices.expand(*shape_prefix, length, -1)\n        indices = indices.reshape(-1, length)\n        embeddings = self.embeddings(indices)\n        embeddings = embeddings.view(*shape_prefix, length, -1)\n        embeddings = embeddings.permute(0, 2, 1, 3)\n        embeddings = embeddings.reshape(shape_prefix + [length, -1])\n        embeddings = embeddings * mask\n    else:\n        embeddings = self.embeddings(indices)\n        embeddings = embeddings.view(*shape_prefix, length, -1)\n        embeddings = embeddings.permute(0, 2, 1, 3)\n        embeddings = embeddings.reshape(shape_prefix + [length, -1])\n    return embeddings\n\n", "CodeBleu_score": 42.65546581030721}, {"code": "def get_codebook_entry(self, indices, shape)\n\tassert len(indices.shape) == len(shape) + 1\n    assert shape[0] % self.groups == 0\n    batch_size = shape[0] // self.groups\n    assert indices.shape[1] == batch_size\n    if self.remap is not None:\n        indices = self.unmap_to_all(indices)\n    assert (indices >= 0).all()\n    assert (indices < self.used.shape[0]).all()\n    codes = self.used[indices]\n    if self.groups > 1:\n        codes = rearrange(codes, 'g b c -> (g b) c')\n        shape_suffix = shape[2:]\n        shape = (shape[0], -1) + shape_suffix\n        codes = codes.reshape(shape)\n        codes = rearrange(codes, '(g b) c ... -> g b c ...', b=batch_size)\n    return codes", "CodeBleu_score": 46.33552459750375}]}
{"instance_id": "c55d014827ef", "generation_result": [{"code": "def factorization(dimension: int, factor:int=-1) -> tuple[int, int]\n\tif factor < 0:\n        m = dimension\n        n = 1\n        while m > 1:\n            if dimension % m == 0:\n                n = dimension // m\n                break\n            m -= 1\n        return m, n\n    elif dimension % factor == 0:\n        return factor, dimension // factor\n    else:\n        raise ValueError(\"The factor must be a divisor of the dimension.\")\n\n\n", "CodeBleu_score": 34.11229889584174}, {"code": "def factorization(dimension: int, factor:int=-1) -> tuple[int, int]\n\tif factor > 0:\n        if dimension % factor == 0:\n            return factor, dimension // factor\n        else:\n            raise ValueError(f\"factor {factor} does not divide dimension {dimension}\")\n    else:\n        if factor == 0:\n            return 1, dimension\n        else:\n            factors = [i for i in range(1, dimension+1) if dimension % i == 0]\n            if factor == -1:\n                return min(factors), dimension // min(factors)\n            else:\n                return min([f for f in factors if f <= -factor]), dimension // min([f for f in factors if f <= -factor])\n", "CodeBleu_score": 28.70906379263237}, {"code": "def factorization(dimension: int, factor:int=-1) -> tuple[int, int]\n\tif factor < 0:\n        m, n = dimension, 1\n        while n < m:\n            m, n = n, m\n            n += 1\n        while n > 1 and n % 2 == 0:\n            m, n = n, m\n            n //= 2\n        while n > 1 and n % 3 == 0:\n            m, n = n, m\n            n //= 3\n        while n > 1 and n % 5 == 0:\n            m, n = n, m\n            n //= 5\n        while n > 1 and n % 7 == 0:\n            m, n = n, m\n            n //= 7\n        while n > 1 and n % 11 == 0:\n            m, n = n, m\n            n //= 11\n        while n > 1 and n % 13 == 0:\n            m, n = n, m\n            n //= 13\n        while n > 1 and n % 17 == 0:\n            m, n = n, m\n            n //= 17\n        while n > 1 and n % 19 == 0:\n            m, n = n, m\n            n //= 19\n        while n > 1 and n % 23 == 0:\n            m, n = n, m\n            n //= 23\n        while n > 1 and n % 29 == 0:\n            m, n = n, m\n            n //= 29\n        while n > 1 and n % 31 == 0:\n            m, n = n, m\n            n //= 31\n        while n > 1 and n % 37 == 0:\n            m, n = n, m\n            n //= 37\n        while n > 1 and n % 41 == 0:\n            m, n = n, m\n            n //= 41\n        while n > 1 and n % 43 == 0:\n            m, n = n, m\n            n //= 43\n        while n", "CodeBleu_score": 26.64298774965118}]}
{"instance_id": "5d613ec26036", "generation_result": [{"code": "def apply_weight_decompose(self, updown, orig_weight)\n\t# Ensure that the original weight and the input weight are on the same device\n    orig_weight = orig_weight.to(self.device)\n    updown = updown.to(self.device)\n\n    # Ensure that the original weight and the input weight are of the same data type\n    orig_weight = orig_weight.type(updown.type())\n\n    # Combine the original weight with the input weight\n    combined_weight = orig_weight + updown\n\n    # Normalize the combined weight\n    combined_weight = combined_weight / torch.norm(combined_weight)\n\n    # Scale the normalized combined weight\n    scaled_combined_weight = combined_weight * self.scale_factor\n\n    # Calculate the adjusted weight by subtracting the original weight from the scaled and normalized combined weight\n    adjusted_weight = scaled_combined_weight - orig_weight\n\n    return adjusted_weight\n\n", "CodeBleu_score": 21.267972679390144}, {"code": "def apply_weight_decompose(self, updown, orig_weight)\n\t# Get device\n    device = orig_weight.device\n\n    # Ensure tensors are on the same device\n    if updown.device != device:\n        updown = updown.to(device)\n\n    # Ensure tensors are the same data type\n    if updown.dtype != orig_weight.dtype:\n        updown = updown.type(orig_weight.dtype)\n\n    # Decompose the weight\n    weight_decomp = updown - orig_weight\n\n    # Normalize the weight\n    weight_decomp_norm = self.normalize_weight(weight_decomp)\n\n    # Scale the weight\n    weight_decomp_scaled = self.scale_weight(weight_decomp_norm)\n\n    # Get the adjusted weight\n    adjusted_weight = weight_decomp_scaled - orig_weight\n\n    return adjusted_weight\n\n\n", "CodeBleu_score": 28.017830219214364}, {"code": "def apply_weight_decompose(self, updown, orig_weight)\n\tif updown == 'up':\n        scale_factor = self.scale_factor\n    elif updown == 'down':\n        scale_factor = 1 / self.scale_factor\n    else:\n        raise ValueError('updown must be either \"up\" or \"down\"')\n    # Ensure that the input weight and original weight are on the same device\n    if orig_weight.device != orig_weight.device:\n        orig_weight = orig_weight.to(orig_weight.device)\n    # Ensure that the input weight and original weight have the same data type\n    if orig_weight.dtype != orig_weight.dtype:\n        orig_weight = orig_weight.type(orig_weight.dtype)\n    # Combine the input weight and original weight\n    combined_weight = torch.add(orig_weight, orig_weight)\n    # Normalize the combined weight\n    combined_weight = torch.nn.functional.normalize(combined_weight)\n    # Scale the normalized combined weight\n    scaled_combined_weight = torch.mul(combined_weight, scale_factor)\n    # Calculate the final adjusted weight\n    adjusted_weight = torch.sub(scaled_combined_weight, orig_weight)\n    return adjusted_weight\n\n\n", "CodeBleu_score": 28.862318921665796}]}
{"instance_id": "c73d0d1cd3e5", "generation_result": [{"code": "def finalize_updown(self, updown, orig_weight, output_shape, ex_bias=None)\n\tif ex_bias is not None:\n        ex_bias = ex_bias.to(updown.dtype)\n        updown = updown * self.multiplier + ex_bias\n    else:\n        updown = updown * self.multiplier\n\n    if updown.numel() == orig_weight.numel():\n        updown = updown.reshape(orig_weight.shape)\n    else:\n        updown = updown.reshape(output_shape)\n\n    if self.scale is not None:\n        updown = self.apply_weight_decompose(updown, orig_weight)\n\n    return updown, ex_bias\n\n", "CodeBleu_score": 50.72368338283122}, {"code": "def finalize_updown(self, updown, orig_weight, output_shape, ex_bias=None)\n\tif updown.shape[0] == 1 and len(updown.shape) > 2:\n        updown = updown.squeeze(0)\n\n    if updown.shape[0] != orig_weight.shape[0]:\n        if updown.shape[0] == 1:\n            updown = updown.repeat(orig_weight.shape[0], 1, 1, 1)\n        else:\n            updown = updown.repeat(orig_weight.shape[0], 1, 1)\n\n    if ex_bias is not None:\n        if updown.shape[0] == 1:\n            ex_bias = ex_bias.repeat(orig_weight.shape[0], 1, 1, 1)\n        else:\n            ex_bias = ex_bias.repeat(orig_weight.shape[0], 1, 1)\n\n    if updown.shape[0] == orig_weight.shape[0]:\n        updown = updown.reshape(output_shape)\n\n    if self.scale is not None:\n        updown = self.apply_weight_decompose(updown, orig_weight)\n\n    if ex_bias is not None:\n        ex_bias = self.apply_bias_decompose(ex_bias)\n\n    return updown, ex_bias\n\n", "CodeBleu_score": 49.4305138382374}, {"code": "def finalize_updown(self, updown, orig_weight, output_shape, ex_bias=None)\n\tif ex_bias is not None:\n        if ex_bias.shape[0] == 1:\n            ex_bias = ex_bias.reshape(-1)\n        ex_bias = ex_bias.to(updown.dtype)\n        updown = updown + ex_bias\n\n    if updown.shape == orig_weight.shape:\n        updown = updown.reshape(output_shape)\n    else:\n        updown = updown.reshape(output_shape)\n\n    scale = self.calc_scale()\n    if scale != 1.0:\n        updown = self.apply_weight_decompose(updown, orig_weight)\n\n    return updown, ex_bias\n\n", "CodeBleu_score": 46.16307452071645}]}
{"instance_id": "c6a41665c62c", "generation_result": [{"code": "def calc_updown(self, orig_weight)\n\tupdown = self.weight.to(orig_weight.device, dtype=orig_weight.dtype) - orig_weight\n    updown = updown.reshape(orig_weight.shape)\n    updown, ex_bias = self.finalize_updown(updown, orig_weight, orig_weight.shape)\n    return updown, ex_bias\n\n", "CodeBleu_score": 35.63457448154327}, {"code": "def calc_updown(self, orig_weight)\n\tif self.bias is not None:\n        bias = self.bias.to(orig_weight.device, dtype=orig_weight.dtype)\n    else:\n        bias = None\n\n    if self.dora_scale is not None:\n        weight = self.apply_weight_decompose(self.weight, orig_weight)\n    else:\n        weight = self.weight\n\n    weight = weight.to(orig_weight.device, dtype=orig_weight.dtype)\n\n    return self.finalize_updown(weight, orig_weight, self.weight.shape, bias)\n\n", "CodeBleu_score": 42.36229302377581}, {"code": "def calc_updown(self, orig_weight)\n\tif self.dora_scale is not None:\n        updown = self.apply_weight_decompose(self.weight, orig_weight)\n    else:\n        updown = self.weight\n\n    updown = updown.to(orig_weight.device, dtype=orig_weight.dtype)\n    updown = updown.reshape(orig_weight.shape)\n    updown = updown * self.calc_scale() * self.multiplier()\n\n    if self.bias is not None:\n        updown += self.bias.to(orig_weight.device, dtype=updown.dtype)\n\n    return updown\n\n", "CodeBleu_score": 38.63491921420747}]}
{"instance_id": "1bb9e5d94af8", "generation_result": [{"code": "def purge_networks_from_memory()\n\timport gc\n    import torch\n    import sys\n    import os\n    import psutil\n    from psutil._common import bytes2human\n    import numpy as np\n    import time\n    import pickle\n    import random\n\n    # Get the number of networks in memory\n    networks_in_memory = len(torch.jit.loaded_modules)\n\n    # Get the limit for networks in memory\n    networks_in_memory_limit = int(os.environ[\"NETWORKS_IN_MEMORY_LIMIT\"])\n\n    # If the number of networks exceeds the limit, purge networks from memory\n    if networks_in_memory > networks_in_memory_limit:\n\n        # Get the number of networks to purge\n        networks_to_purge = networks_in_memory - networks_in_memory_limit\n\n        # Get the number of networks in memory before purging\n        networks_in_memory_before_purge = networks_in_memory\n\n        # Get the number of bytes in memory before purging\n        bytes_in_memory_before_purge = psutil.Process(os.getpid()).memory_info().rss\n\n        # Get the number of bytes in memory before purging\n        bytes_in_memory_before_purge_human = bytes2human(bytes_in_memory_before_purge)\n\n        # Get the number of bytes in memory before purging\n        print(\"Number of bytes in memory before purging: \" + str(bytes_in_memory_before_purge_human))\n\n        # Get the number of bytes in memory before purging\n        print(\"Number of networks in memory before purging: \" + str(networks_in_memory_before_purge))\n\n        # Get the number of bytes in memory before purging\n        print(\"Number of networks to purge: \" + str(networks_to_purge))\n\n        # Get the number of bytes in memory before purging\n        print(\"Purging networks from memory...\")\n\n        # Get a list of loaded modules\n        loaded_modules = torch.jit.loaded_modules\n\n        # Get a list of loaded modules\n        loaded_modules", "CodeBleu_score": 21.97133541040844}, {"code": "def purge_networks_from_memory()\n\timport gc\n    from . import network_manager\n    from . import config\n    # Purge networks from memory if the number of networks exceeds a specified limit\n    if len(network_manager.networks) > config.max_networks_in_memory:\n        # Purge networks from memory until the count is within the limit\n        while len(network_manager.networks) > config.max_networks_in_memory:\n            network_manager.networks.popitem()\n        # Trigger garbage collection to free up memory\n        gc.collect()\n\n\n", "CodeBleu_score": 26.168549995966234}, {"code": "def purge_networks_from_memory()\n\t# TODO: Add limit to config\n    limit = 50\n    while len(get_networks()) > limit:\n        network = get_networks()[0]\n        network.purge_from_memory()\n    gc.collect()\n\n", "CodeBleu_score": 26.49379264328059}]}
{"instance_id": "42b48ebf4d07", "generation_result": [{"code": "def network_restore_weights_from_backup(self: Union[torch.nn.Conv2d, torch.nn.Linear, torch.nn.GroupNorm, torch.nn.LayerNorm, torch.nn.MultiheadAttention]):\n\tif self.bias is not None:\n        if os.path.exists(self.bias_backup_path):\n            self.bias.data = torch.load(self.bias_backup_path)\n        else:\n            self.bias = None\n    if os.path.exists(self.weight_backup_path):\n        self.weight.data = torch.load(self.weight_backup_path)\n    if isinstance(self, torch.nn.MultiheadAttention):\n        if os.path.exists(self.in_proj_weight_backup_path):\n            self.in_proj_weight.data = torch.load(self.in_proj_weight_backup_path)\n        if os.path.exists(self.in_proj_bias_backup_path):\n            self.in_proj_bias.data = torch.load(self.in_proj_bias_backup_path)\n\n\n\n", "CodeBleu_score": 36.70256716523909}, {"code": "def network_restore_weights_from_backup(self: Union[torch.nn.Conv2d, torch.nn.Linear, torch.nn.GroupNorm, torch.nn.LayerNorm, torch.nn.MultiheadAttention]):\n\tif self.weight.shape[0] > 1:\n        if self.weight.shape[0] == 2:\n            if self.weight.shape[1] == 2:\n                if self.weight.shape[2] == 3:\n                    self.weight = torch.nn.Parameter(self.weight_backup.clone())\n                    self.bias = torch.nn.Parameter(self.bias_backup.clone())\n                    return\n            if self.weight.shape[1] == 1:\n                if self.weight.shape[2] == 3:\n                    self.weight = torch.nn.Parameter(self.weight_backup.clone())\n                    self.bias = torch.nn.Parameter(self.bias_backup.clone())\n                    return\n        if self.weight.shape[0] == 4:\n            if self.weight.shape[1] == 1:\n                if self.weight.shape[2] == 3:\n                    self.weight = torch.nn.Parameter(self.weight_backup.clone())\n                    self.bias = torch.nn.Parameter(self.bias_backup.clone())\n                    return\n    if self.weight.shape[0] == 1:\n        if self.weight.shape[1] == 1:\n            if self.weight.shape[2] == 3:\n                self.weight = torch.nn.Parameter(self.weight_backup.clone())\n                self.bias = torch.nn.Parameter(self.bias_backup.clone())\n                return\n    if self.weight.shape[0] == 2:\n        if self.weight.shape[1] == 2:\n            if self.weight.shape[2] == 1:\n                self.weight = torch.nn.Parameter(self.weight_backup.clone())\n                self.bias = torch.nn.Parameter(self.bias_backup.clone())\n                return\n    if self.weight.shape[0] == 4:\n        if self.weight.shape[1] == 2:\n            if", "CodeBleu_score": 33.847302428553846}, {"code": "def network_restore_weights_from_backup(self: Union[torch.nn.Conv2d, torch.nn.Linear, torch.nn.GroupNorm, torch.nn.LayerNorm, torch.nn.MultiheadAttention]):\n\tif self.weight.shape == self.backup_weight.shape:\n        self.weight.data = self.backup_weight.data\n    if self.bias is not None:\n        if self.bias.shape == self.backup_bias.shape:\n            self.bias.data = self.backup_bias.data\n        else:\n            self.bias = None\n    if isinstance(self, torch.nn.MultiheadAttention):\n        if self.in_proj_weight.shape == self.backup_in_proj_weight.shape:\n            self.in_proj_weight.data = self.backup_in_proj_weight.data\n        if self.in_proj_bias is not None:\n            if self.in_proj_bias.shape == self.backup_in_proj_bias.shape:\n                self.in_proj_bias.data = self.backup_in_proj_bias.data\n            else:\n                self.in_proj_bias = None\n\n\n\n", "CodeBleu_score": 40.40049715555251}]}
{"instance_id": "c8fcd357131a", "generation_result": [{"code": "def network_forward(org_module, input, original_forward)\n\tif len(org_module.sd_modules) == 0:\n        return original_forward(input)\n\n    input = input.to(org_module.device)\n    org_module.network_reset_cached_weight()\n\n    output = original_forward(input)\n\n    for sd_module in org_module.sd_modules:\n        if sd_module.current_name in org_module.network_current_names:\n            continue\n\n        updown, ex_bias = sd_module.calc_updown(sd_module.weight)\n        output = sd_module.ops(output, weight=updown, bias=ex_bias, **sd_module.extra_kwargs)\n\n        org_module.network_current_names += (sd_module.current_name,)\n\n    return output\n\n", "CodeBleu_score": 34.65808254149634}, {"code": "def network_forward(org_module, input, original_forward)\n\tif not hasattr(org_module, \"network_loaded_weights\"):\n        return original_forward(input)\n\n    if isinstance(input, tuple):\n        input = input[0]\n\n    if isinstance(input, list):\n        input = input[0]\n\n    if isinstance(input, dict):\n        input = list(input.values())[0]\n\n    device = input.device\n    if device.type == \"cpu\":\n        device = torch.device(\"cuda\")\n\n    for network_name, network_weights in org_module.network_loaded_weights.items():\n        if network_name in org_module.network_current_names:\n            continue\n        org_module.network_current_names += (network_name,)\n        org_module.network_weights_backup = org_module.weight.data.clone().detach().to(device)\n        org_module.network_bias_backup = org_module.bias.data.clone().detach().to(device) if org_module.bias is not None else None\n        org_module.weight.data.copy_(network_weights)\n        if org_module.bias is not None:\n            org_module.bias.data.copy_(torch.zeros_like(org_module.bias))\n        output = original_forward(input)\n        org_module.network_restore_weights_from_backup()\n        return output\n\n    output = original_forward(input)\n    org_module.network_reset_cached_weight()\n    return output\n\n", "CodeBleu_score": 32.52078137958222}, {"code": "def network_forward(org_module, input, original_forward)\n\tif not isinstance(org_module, (torch.nn.Conv2d, torch.nn.Linear, torch.nn.GroupNorm, torch.nn.LayerNorm, torch.nn.MultiheadAttention)):\n        return original_forward(input)\n\n    if not isinstance(input, torch.Tensor):\n        input = torch.tensor(input, device=org_module.weight.device)\n    else:\n        input = input.to(org_module.weight.device)\n\n    if isinstance(org_module, torch.nn.MultiheadAttention):\n        input = (input,) * 3\n\n    network_reset_cached_weight(org_module)\n\n    if org_module.network_current_names is None:\n        org_module.network_current_names = ()\n\n    if not isinstance(org_module, torch.nn.MultiheadAttention):\n        input = input.unsqueeze(0)\n\n    output = original_forward(input)\n\n    if not isinstance(org_module, torch.nn.MultiheadAttention):\n        output = output.squeeze(0)\n\n    for name in org_module.network_current_names:\n        network = org_module.networks[name]\n        if isinstance(output, torch.Tensor):\n            output = network(output)\n        else:\n            output = [network(x) for x in output]\n\n    return output\n\n", "CodeBleu_score": 33.83363036596795}]}
{"instance_id": "1070211f5c11", "generation_result": [{"code": "def trainables(self)\n\ttrainables = []\n    for layer in self.layers:\n        if isinstance(layer, torch.nn.Linear):\n            trainables.append(layer.weight)\n            trainables.append(layer.bias)\n        elif isinstance(layer, torch.nn.LayerNorm):\n            trainables.append(layer.weight)\n            trainables.append(layer.bias)\n    return trainables\n\n\n", "CodeBleu_score": 37.1781736126104}, {"code": "def trainables(self)\n\ttrainables = []\n    for _, layer in enumerate(self.layers):\n        if isinstance(layer, torch.nn.Linear):\n            trainables.append(layer.weight)\n            trainables.append(layer.bias)\n        elif isinstance(layer, torch.nn.LayerNorm):\n            trainables.append(layer.weight)\n            trainables.append(layer.bias)\n    return trainables\n\n\n\n\n", "CodeBleu_score": 28.817382803312036}, {"code": "def trainables(self)\n\ttrainable_parameters = []\n    for name, parameter in self.named_parameters():\n        if isinstance(self, torch.nn.Linear) or isinstance(self, torch.nn.LayerNorm):\n            trainable_parameters.append(parameter)\n    return trainable_parameters\n\n", "CodeBleu_score": 32.929259739796926}]}
{"instance_id": "0c3ba62c96a7", "generation_result": [{"code": "def load_hypernetworks(names, multipliers=None)\n\tif multipliers is None:\n        multipliers = [None] * len(names)\n\n    hypernetworks = []\n    for name, multiplier in zip(names, multipliers):\n        hypernetwork = shared.loaded_hypernetworks.get(name, None)\n\n        if hypernetwork is None:\n            hypernetwork = load_hypernetwork(name)\n\n            if hypernetwork is None:\n                continue\n\n            hypernetwork.set_multiplier(multiplier)\n            shared.loaded_hypernetworks[name] = hypernetwork\n\n        hypernetworks.append(hypernetwork)\n\n    return hypernetworks\n\n", "CodeBleu_score": 43.579548865354944}, {"code": "def load_hypernetworks(names, multipliers=None)\n\t# Check for already loaded hypernetworks.\n    if len(shared.hypernetworks) > 0:\n        errors.report(\"Clearing loaded hypernetworks\", level=\"debug\")\n        shared.hypernetworks.clear()\n\n    # Load or retrieve each hypernetwork by name.\n    for name in names:\n        errors.report(f\"Loading hypernetwork {name}\", level=\"debug\")\n\n        # Load hypernetwork.\n        hypernetwork = load_hypernetwork(name)\n\n        # Set hypernetwork multiplier if provided.\n        if hypernetwork is not None and multipliers is not None:\n            try:\n                multiplier = multipliers[names.index(name)]\n                hypernetwork.set_multiplier(multiplier)\n            except Exception:\n                errors.report(\n                    f\"Error setting multiplier for hypernetwork {name}\",\n                    exc_info=True,\n                )\n\n        # Append hypernetwork to list.\n        if hypernetwork is not None:\n            shared.hypernetworks.append(hypernetwork)\n\n", "CodeBleu_score": 38.32356261313262}, {"code": "def load_hypernetworks(names, multipliers=None)\n\tif not isinstance(names, (list, tuple)):\n        names = [names]\n\n    if multipliers is not None:\n        if not isinstance(multipliers, (list, tuple)):\n            multipliers = [multipliers] * len(names)\n\n        if len(multipliers) != len(names):\n            errors.report(f\"The number of multipliers provided ({len(multipliers)}) does not match the number of hypernetworks ({len(names)})\", error=True)\n\n    hypernetworks = []\n\n    for i, name in enumerate(names):\n        hypernetwork = load_hypernetwork(name)\n\n        if hypernetwork is None:\n            errors.report(f\"Failed to load hypernetwork {name}\", error=True)\n\n        if multipliers is not None:\n            hypernetwork.set_multiplier(multipliers[i])\n\n        hypernetworks.append(hypernetwork)\n\n    shared.hypernetworks.clear()\n    shared.hypernetworks.extend(hypernetworks)\n\n    return hypernetworks\n\n", "CodeBleu_score": 41.6526604370234}]}
{"instance_id": "bea3772ed532", "generation_result": [{"code": "\ndef apply_hypernetworks(hypernetworks, context, layer=None)\n\tcontext_k, context_v = context, context\n    for hypernetwork in hypernetworks:\n        context_k, context_v = apply_single_hypernetwork(hypernetwork, context_k, context_v, layer=layer)\n    return context_k, context_v\n\n\n", "CodeBleu_score": 56.977956367118345}, {"code": "\ndef apply_hypernetworks(hypernetworks, context, layer=None)\n\tcontext_k, context_v = context, context\n\n    for hypernetwork in hypernetworks:\n        context_k, context_v = apply_single_hypernetwork(hypernetwork, context_k, context_v, layer=layer)\n\n    return context_k, context_v\n\n\n", "CodeBleu_score": 56.977956367118345}, {"code": "\ndef apply_hypernetworks(hypernetworks, context, layer=None)\n\tcontext_k, context_v = context, context\n    for hypernetwork in hypernetworks:\n        context_k, context_v = apply_single_hypernetwork(hypernetwork, context_k, context_v, layer)\n    return context_k, context_v\n\n\n", "CodeBleu_score": 70.97333483627517}]}
{"instance_id": "922efa3e092f", "generation_result": [{"code": "def attention_CrossAttention_forward(self, x, context=None, mask=None, **kwargs)\n\tbatch_size, q_seq_len, d_model = x.size()\n    if context is None:\n        context = x\n    batch_size, k_seq_len, d_model = context.size()\n\n    q_seq_len = q_seq_len // self.h\n    k_seq_len = k_seq_len // self.h\n\n    q = self.q(x).view(batch_size, q_seq_len, self.h, self.d_k).permute(0, 2, 1, 3)\n    k = self.k(context).view(batch_size, k_seq_len, self.h, self.d_k).permute(0, 2, 1, 3)\n    v = self.v(context).view(batch_size, k_seq_len, self.h, self.d_k).permute(0, 2, 1, 3)\n\n    if self.hyper_layers_k is not None:\n        k, v = apply_hypernetworks(self.hyper_layers_k, k, v)\n\n    q = q / math.sqrt(self.d_k)\n    scores = torch.matmul(q, k.permute(0, 1, 3, 2))\n    if mask is not None:\n        mask = mask.unsqueeze(1)\n        scores = scores.masked_fill(mask == 0, -1e9)\n    scores = F.softmax(scores, dim=-1)\n    scores = self.dropout(scores)\n\n    h = torch.matmul(scores, v)\n    h = h.permute(0, 2, 1, 3).contiguous()\n    h = h.view(batch_size, q_seq_len, self.h * self.d_k)\n    h = self.o(h)\n    return h\n\n", "CodeBleu_score": 38.63819694289283}, {"code": "def attention_CrossAttention_forward(self, x, context=None, mask=None, **kwargs)\n\tif self.layer is not None:\n        x = self.layer(x)\n\n    x = x.transpose(1, 2).contiguous()\n    if context is not None:\n        context = context.transpose(1, 2).contiguous()\n\n    query_layer, key_layer, value_layer = self.in_projection(x, context)\n\n    if self.hyper_act_func is not None:\n        if context is not None:\n            context_k, context_v = apply_hypernetworks(self.hyper_act_func, context, self.layer)\n        else:\n            context_k, context_v = None, None\n\n        query_layer, key_layer, value_layer = apply_hypernetworks(self.hyper_act_func, [query_layer, key_layer, value_layer], self.layer)\n\n    if self.hyper_query_key_func is not None:\n        query_layer, key_layer = apply_hypernetworks(self.hyper_query_key_func, [query_layer, key_layer], self.layer)\n\n    if self.hyper_value_func is not None:\n        value_layer = apply_hypernetworks(self.hyper_value_func, value_layer, self.layer)\n\n    if self.hyper_out_func is not None:\n        query_layer = apply_hypernetworks(self.hyper_out_func, query_layer, self.layer)\n\n    if self.in_proj_bias is not None:\n        query_layer = query_layer + self.in_proj_bias[0:self.embed_dim]\n        key_layer = key_layer + self.in_proj_bias[self.embed_dim:(self.embed_dim * 2)]\n        value_layer = value_layer + self.in_proj_bias[(self.embed_dim * 2):]\n\n    if self.bias_k is not None:\n        assert self.bias_v is not None\n        key_layer = torch.cat([key_layer, self.bias_k.repeat(1, bsz, 1", "CodeBleu_score": 27.86821405941979}, {"code": "def attention_CrossAttention_forward(self, x, context=None, mask=None, **kwargs)\n\t# x: (batch_size, seq_len, embedding_size)\n    # context: (batch_size, seq_len, embedding_size)\n    # mask: (batch_size, seq_len)\n    # self.embedding_size: int\n    # self.num_heads: int\n    # self.head_size: int\n    # self.dropout: float\n\n    # Apply hypernetworks to context\n    if self.hypernetworks is not None:\n        context_k, context_v = apply_hypernetworks(self.hypernetworks, context)\n    else:\n        context_k, context_v = context, context\n\n    # Reshape input and context for multi-head attention\n    x_k, x_v = self.split_heads(x)\n    context_k, context_v = self.split_heads(context_k), self.split_heads(context_v)\n\n    # Calculate scaled dot-product attention\n    attention_scores = torch.matmul(x_k, context_k.transpose(-1, -2)) / np.sqrt(self.head_size)\n\n    # Apply mask to attention scores\n    if mask is not None:\n        mask = mask.unsqueeze(1)\n        attention_scores = attention_scores.masked_fill(mask == 0, -1e9)\n\n    # Calculate attention weights\n    attention_weights = torch.softmax(attention_scores, dim=-1)\n\n    # Dropout\n    attention_weights = self.dropout(attention_weights)\n\n    # Apply attention weights\n    attention_output = torch.matmul(attention_weights, context_v)\n\n    # Reshape output for linear layer\n    attention_output = self.combine_heads(attention_output)\n\n    # Apply final linear layer\n    attention_output = self.linear(attention_output)\n\n    return attention_output\n\n", "CodeBleu_score": 36.14373304810161}]}
{"instance_id": "8b7455a03a0e", "generation_result": [{"code": "def save_hypernetwork(hypernetwork, checkpoint, hypernetwork_name, filename)\n\tif not isinstance(hypernetwork, HyperNetwork):\n        raise TypeError(\"save_hypernetwork() only accepts HyperNetwork objects.\")\n\n    hypernetwork.sd_checkpoint = checkpoint\n    hypernetwork.sd_checkpoint_name = hypernetwork_name\n    hypernetwork.save(filename)\n\n", "CodeBleu_score": 21.891573404172334}, {"code": "def save_hypernetwork(hypernetwork, checkpoint, hypernetwork_name, filename)\n\ttry:\n        hypernetwork.update_checkpoint(checkpoint, hypernetwork_name)\n        hypernetwork.save(filename)\n    except Exception as e:\n        hypernetwork.restore_checkpoint()\n        raise e\n\n", "CodeBleu_score": 15.562084448628177}, {"code": "def save_hypernetwork(hypernetwork, checkpoint, hypernetwork_name, filename)\n\thypernetwork_name = hypernetwork_name.replace(' ', '_')\n    hypernetwork_name = hypernetwork_name.replace(':', '_')\n\n    if checkpoint is not None:\n        hypernetwork.sd_checkpoint = checkpoint\n        hypernetwork.sd_checkpoint_name = hypernetwork_name\n\n    try:\n        hypernetwork.save(filename)\n    except Exception as e:\n        if checkpoint is not None:\n            hypernetwork.sd_checkpoint = None\n            hypernetwork.sd_checkpoint_name = None\n        raise e\n\n    if checkpoint is not None:\n        hypernetwork.sd_checkpoint = None\n        hypernetwork.sd_checkpoint_name = None\n\n", "CodeBleu_score": 38.73169108469634}]}
{"instance_id": "3175f835140f", "generation_result": [{"code": "def ema_scope(self, context=None)\n\tif context is None:\n        context = {}\n    if not isinstance(context, dict):\n        raise TypeError(f'context must be a dict, got {type(context)}')\n    if not self.ema_enabled:\n        return\n    if self.ema_is_current:\n        raise RuntimeError('ema_scope is not reentrant')\n    if self.ema_is_initialized:\n        self.ema_model.load_state_dict(self.model.state_dict())\n        self.model.load_state_dict(self.ema_model.state_dict())\n        self.ema_is_current = True\n    if self.ema_log_interval > 0:\n        self.ema_log_count += 1\n        if self.ema_log_count >= self.ema_log_interval:\n            self.ema_log_count = 0\n            self.ema_log_latest = True\n    else:\n        self.ema_log_latest = False\n    if self.ema_log_latest:\n        self.ema_log_latest = False\n        self.ema_log(context)\n\n", "CodeBleu_score": 38.15775259728388}, {"code": "def ema_scope(self, context=None)\n\tif not self.ema:\n        yield\n        return\n\n    if context is None:\n        context = {}\n\n    if self.ema_momentum == 0:\n        yield\n        return\n\n    # store original weights\n    original_weights = self.model.state_dict()\n    self.model.load_state_dict(self.ema_model.state_dict())\n\n    # log the switch\n    if self.logger is not None:\n        self.logger.log_message(f'ema_scope: switching to EMA weights')\n\n    # execute the code block\n    yield\n\n    # restore original weights\n    self.model.load_state_dict(original_weights)\n\n    # log the restoration\n    if self.logger is not None:\n        self.logger.log_message(f'ema_scope: restoring original weights')", "CodeBleu_score": 43.59072980067905}, {"code": "def ema_scope(self, context=None)\n\tif context is None:\n        context = self.context\n    if context is None:\n        raise ValueError('No context provided.')\n    if context.get('ema_enabled', False):\n        if context.get('ema_restore', False):\n            self.ema_restore()\n        else:\n            self.ema_store()\n    try:\n        yield\n    finally:\n        if context.get('ema_enabled', False):\n            self.ema_restore()\n\n", "CodeBleu_score": 38.54223136910198}]}
{"instance_id": "a7b5d5cd519a", "generation_result": [{"code": "def p_mean_variance(self, x, t, clip_denoised: bool)\n\t# TODO: Implement this function.\n    return None, None, None\n\n", "CodeBleu_score": 4.964171707203214}, {"code": "def p_mean_variance(self, x, t, clip_denoised: bool)\n\tmodel_output = self.model(x, t)\n    if self.model_output_denoise:\n        x_recon = self.predict_start_from_noise(x, t, model_output)\n        if clip_denoised:\n            x_recon.clamp_(-1., 1.)\n    else:\n        x_recon = model_output\n    posterior_mean, posterior_variance, posterior_log_variance_clipped = self.q_posterior(x_start=x, x_t=x_recon, t=t)\n    return posterior_mean, posterior_variance, posterior_log_variance_clipped\n\n", "CodeBleu_score": 53.75002209409192}, {"code": "def p_mean_variance(self, x, t, clip_denoised: bool)\n\t# TODO: add the option to use the predicted noise instead of the predicted start\n    # TODO: add the option to use the predicted noise instead of the predicted start\n    # TODO: add the option to use the predicted noise instead of the predicted start\n    # TODO: add the option to use the predicted noise instead of the predicted start\n    # TODO: add the option to use the predicted noise instead of the predicted start\n    # TODO: add the option to use the predicted noise instead of the predicted start\n    # TODO: add the option to use the predicted noise instead of the predicted start\n    # TODO: add the option to use the predicted noise instead of the predicted start\n    # TODO: add the option to use the predicted noise instead of the predicted start\n    # TODO: add the option to use the predicted noise instead of the predicted start\n    # TODO: add the option to use the predicted noise instead of the predicted start\n    # TODO: add the option to use the predicted noise instead of the predicted start\n    # TODO: add the option to use the predicted noise instead of the predicted start\n    # TODO: add the option to use the predicted noise instead of the predicted start\n    # TODO: add the option to use the predicted noise instead of the predicted start\n    # TODO: add the option to use the predicted noise instead of the predicted start\n    # TODO: add the option to use the predicted noise instead of the predicted start\n    # TODO: add the option to use the predicted noise instead of the predicted start\n    # TODO: add the option to use the predicted noise instead of the predicted start\n    # TODO: add the option to use the predicted noise instead of the predicted start\n    # TODO: add the option to use the predicted noise instead of the predicted start\n    # TODO: add the option to use the predicted noise instead of the predicted start\n    # TODO: add the option to use the predicted noise instead of the predicted start\n    # TODO: add the option to use the predicted noise instead of the predicted start\n    # TODO: add the option to use the predicted noise instead of the predicted start\n    # TODO: add the option to use the predicted noise instead of the predicted start\n    # TODO: add the option to use the predicted noise instead of the predicted start\n    # TODO: add the option to use the predicted noise instead of the predicted start\n    # TODO: add the option", "CodeBleu_score": 4.788306451612903}]}
{"instance_id": "a8652fa78473", "generation_result": [{"code": "def get_loss(self, pred, target, mean=True)\n\tif self.loss_type == 'l1':\n        loss = torch.abs(pred - target)\n        if mean:\n            loss = torch.mean(loss)\n    elif self.loss_type == 'l2':\n        loss = torch.pow(pred - target, 2)\n        if mean:\n            loss = torch.mean(loss)\n    else:\n        raise ValueError(\"Unknown loss type\")\n    return loss\n\n", "CodeBleu_score": 46.88028889900488}, {"code": "def get_loss(self, pred, target, mean=True)\n\tif self.loss_type == 'l1':\n        loss = torch.abs(pred - target)\n        if mean:\n            loss = loss.mean()\n    elif self.loss_type == 'l2':\n        loss = torch.pow(pred - target, 2)\n        if not mean:\n            loss = loss.mean(1)\n    else:\n        raise ValueError('Unknown loss type: {}'.format(self.loss_type))\n    return loss\n\n\n", "CodeBleu_score": 50.51556659366596}, {"code": "def get_loss(self, pred, target, mean=True)\n\tif self.loss_type == 'l1':\n        if mean:\n            return torch.mean(torch.abs(pred - target))\n        else:\n            return torch.abs(pred - target)\n    elif self.loss_type == 'l2':\n        if mean:\n            return torch.mean((pred - target) ** 2)\n        else:\n            return (pred - target) ** 2\n    else:\n        raise ValueError('Unknown loss type: {}'.format(self.loss_type))\n\n", "CodeBleu_score": 33.80349717781917}]}
{"instance_id": "d68e4f061349", "generation_result": [{"code": "def _prior_bpd(self, x_start)\n\tbatch_size = x_start.shape[0]\n    t = torch.full((batch_size,), self.num_timesteps - 1, dtype=torch.long, device=x_start.device)\n    mean, _, log_variance = self.q_mean_variance(x_start=x_start, t=t)\n    kl = 0.5 * (mean.pow(2) + torch.exp(log_variance) - log_variance - 1)\n    return kl.sum(dim=(1, 2, 3)).mean() / np.log(2)\n\n", "CodeBleu_score": 31.286949552204923}, {"code": "def _prior_bpd(self, x_start)\n\tbatch_size = x_start.shape[0]\n    t = torch.ones(batch_size, device=x_start.device, dtype=torch.long) * self.num_timesteps\n    mean, _, log_variance = self.q_mean_variance(x_start=x_start, t=t)\n    kl = 0.5 * (mean.pow(2) + torch.exp(log_variance) - log_variance - 1)\n    return kl.mean(dim=[1, 2, 3]) / np.log(2)\n\n", "CodeBleu_score": 40.254448907956586}, {"code": "def _prior_bpd(self, x_start)\n\tbatch_size = x_start.shape[0]\n    t = torch.ones(batch_size, dtype=torch.long, device=x_start.device) * self.num_timesteps\n    mean, _, log_variance = self.q_mean_variance(x_start=x_start, t=t)\n    kl = 0.5 * (mean ** 2 + torch.exp(log_variance)) - log_variance - 0.5\n    return kl.mean(dim=0).sum() / np.log(2.0)\n\n", "CodeBleu_score": 39.23868114265855}]}
{"instance_id": "c695d93fdf2d", "generation_result": [{"code": "def make_cache(subsection: str) -> diskcache.Cache\n\tcache_dir = os.path.join(os.path.expanduser(\"~\"), \".cache\", \"py_cached_requests\")\n    if not os.path.exists(cache_dir):\n        os.makedirs(cache_dir)\n    cache_path = os.path.join(cache_dir, subsection)\n    return diskcache.Cache(cache_path, size_limit=4 * 1024 * 1024 * 1024, disk_min_file_size=256 * 1024)\n\n", "CodeBleu_score": 51.42832920961536}, {"code": "def make_cache(subsection: str) -> diskcache.Cache\n\tcache_dir = Path(os.path.expanduser('~')) / '.cache' / 'bw_processing' / 'data'\n    cache_dir.mkdir(parents=True, exist_ok=True)\n    cache = diskcache.Cache(cache_dir / subsection, size_limit=4e9, min_file_size=256e3)\n    return cache\n\n", "CodeBleu_score": 44.05289403733842}, {"code": "def make_cache(subsection: str) -> diskcache.Cache\n\tcache_dir = os.path.join(os.path.dirname(__file__), '..', '..', 'cache')\n    cache = diskcache.Cache(os.path.join(cache_dir, subsection), size_limit=4 * 1024 * 1024 * 1024, min_file_size=256 * 1024)\n    return cache\n", "CodeBleu_score": 53.305348207089786}]}
{"instance_id": "ed4c543f0d38", "generation_result": [{"code": "def cache(subsection)\n\tcache_obj = caches.get(subsection)\n    if cache_obj is None:\n        with cache_lock:\n            if not os.path.exists(cache_dir):\n                os.mkdir(cache_dir)\n            if not os.path.isfile(cache_filename):\n                convert_old_cached_data()\n\n            cache_obj = caches.get(subsection)\n            if cache_obj is None:\n                cache_obj = make_cache(subsection)\n                caches[subsection] = cache_obj\n\n    return cache_obj\n\n", "CodeBleu_score": 67.4523095740611}, {"code": "def cache(subsection)\n\tcache_obj = caches.get(subsection)\n    if cache_obj is None:\n        with cache_lock:\n            if not os.path.exists(cache_dir):\n                os.mkdir(cache_dir)\n            if not os.path.isfile(cache_filename):\n                convert_old_cached_data()\n\n            cache_obj = caches.get(subsection)\n            if cache_obj is None:\n                cache_obj = make_cache(subsection)\n                caches[subsection] = cache_obj\n\n    return cache_obj\n\n", "CodeBleu_score": 67.4523095740611}, {"code": "def cache(subsection)\n\tcache_obj = caches.get(subsection)\n    if cache_obj is None:\n        with cache_lock:\n            if not os.path.exists(cache_dir):\n                os.makedirs(cache_dir)\n            if not os.path.isfile(cache_filename):\n                convert_old_cached_data()\n\n            cache_obj = caches.get(subsection)\n            if cache_obj is None:\n                cache_obj = make_cache(subsection)\n                caches[subsection] = cache_obj\n\n    return cache_obj\n\n", "CodeBleu_score": 67.3139918660477}]}
{"instance_id": "ccf5ea2d9fc1", "generation_result": [{"code": "def cached_data_for_file(subsection, title, filename, func)\n\tcache_obj = cache(subsection)\n\n    # get the last modification time of the file\n    file_mtime = os.path.getmtime(filename)\n\n    # if a cache entry exists for the given title, check if the file's mtime is more recent than the cached mtime\n    if cache_obj.get(title):\n        if cache_obj[title][0] < file_mtime:\n            # if so, invalidate the cache entry\n            cache_obj.delete(title)\n\n    # if no valid cache entry exists, call the provided function func() to generate new data\n    if not cache_obj.get(title):\n        data = func()\n        # store this data along with the file's mtime in the cache\n        cache_obj[title] = (file_mtime, data)\n        # update the cache storage with dump_cache()\n        dump_cache()\n    else:\n        # otherwise, return the cached data\n        data = cache_obj[title][1]\n\n    return data\n\n", "CodeBleu_score": 25.945627324643596}, {"code": "def cached_data_for_file(subsection, title, filename, func)\n\tcache_obj = cache(subsection)\n    mtime = os.path.getmtime(filename)\n\n    if cache_obj.get(title):\n        if cache_obj.get(title)[0] < mtime:\n            cache_obj.invalidate(title)\n\n    if not cache_obj.get(title):\n        data = func()\n        cache_obj.set(title, (mtime, data))\n        dump_cache()\n\n    return cache_obj.get(title)[1]\n\n", "CodeBleu_score": 20.69103302765909}, {"code": "def cached_data_for_file(subsection, title, filename, func)\n\tcache_obj = cache(subsection)\n    mtime = os.path.getmtime(filename)\n    cached_data = cache_obj.get(title)\n    if cached_data and cached_data[\"mtime\"] >= mtime:\n        return cached_data[\"data\"]\n    else:\n        data = func()\n        cache_obj.set(title, {\"mtime\": mtime, \"data\": data}, expire=cache_expire)\n        dump_cache()\n        return data\n\n", "CodeBleu_score": 20.26787611501008}]}
{"instance_id": "e01a2d4f0ac2", "generation_result": [{"code": "def report(message: str, *, exc_info: bool = False) -> None\n\trecord_exception()\n\n    for line in message.splitlines():\n        print(\"***\", line, file=sys.stderr)\n\n    if exc_info:\n        for line in format_exception(*sys.exc_info()).splitlines():\n            print(\"    \" + line, file=sys.stderr)\n\n        print(\"---\", file=sys.stderr)\n\n", "CodeBleu_score": 65.71413794467728}, {"code": "def report(message: str, *, exc_info: bool = False) -> None\n\trecord_exception()\n\n    if exc_info:\n        message += \"", "CodeBleu_score": 13.121442595626531}, {"code": "def report(message: str, *, exc_info: bool = False) -> None\n\trecord_exception()\n\n    if exc_info:\n        _, e, tb = sys.exc_info()\n        if e is not None:\n            print(\"---\", file=sys.stderr)\n            print(*format_exception(e, tb), sep=\"", "CodeBleu_score": 40.95983875593271}]}
{"instance_id": "125fb9d2320b", "generation_result": [{"code": "def extract_original_prompts(style: PromptStyle, prompt, negative_prompt)\n\tif style.prompt == \"\" and style.negative_prompt == \"\":\n        return False, prompt, negative_prompt\n\n    prompt_extracted, prompt = extract_style_text_from_prompt(style.prompt, prompt)\n    negative_prompt_extracted, negative_prompt = extract_style_text_from_prompt(style.negative_prompt, negative_prompt)\n\n    if prompt_extracted and negative_prompt_extracted:\n        return True, prompt, negative_prompt\n\n    return False, prompt, negative_prompt\n", "CodeBleu_score": 54.76864126909206}, {"code": "def extract_original_prompts(style: PromptStyle, prompt, negative_prompt)\n\tif style.prompt == \"\" or style.negative_prompt == \"\":\n        return False, prompt, negative_prompt\n\n    prompt_extracted, prompt = extract_style_text_from_prompt(style.prompt, prompt)\n    negative_prompt_extracted, negative_prompt = extract_style_text_from_prompt(style.negative_prompt, negative_prompt)\n\n    if prompt_extracted and negative_prompt_extracted:\n        return True, prompt, negative_prompt\n    else:\n        return False, prompt, negative_prompt\n\n", "CodeBleu_score": 56.97794012171571}, {"code": "def extract_original_prompts(style: PromptStyle, prompt, negative_prompt)\n\tif style.prompt == \"\" and style.negative_prompt == \"\":\n        return False, prompt, negative_prompt\n\n    if style.prompt != \"\":\n        success, prompt = extract_style_text_from_prompt(style.prompt, prompt)\n        if not success:\n            return False, prompt, negative_prompt\n\n    if style.negative_prompt != \"\":\n        success, negative_prompt = extract_style_text_from_prompt(style.negative_prompt, negative_prompt)\n        if not success:\n            return False, prompt, negative_prompt\n\n    return True, prompt, negative_prompt\n\n", "CodeBleu_score": 66.69600350213094}]}
{"instance_id": "0c5f729fbce2", "generation_result": [{"code": "def random_partial(self, count, n_frames)\n\tif self.utterances is None:\n        self._load_utterances()\n\n    utterances = self.utterance_cycler.sample(count)\n\n    a = [(u,) + u.random_partial(n_frames) for u in utterances]\n\n    return a", "CodeBleu_score": 93.58876149224903}, {"code": "def random_partial(self, count, n_frames)\n\tif self.utterances is None:\n        self._load_utterances()\n\n    utterances = self.utterance_cycler.sample(count)\n\n    a = [(u,) + u.random_partial(n_frames) for u in utterances]\n\n    return a\n\n", "CodeBleu_score": 93.58876149224903}, {"code": "def random_partial(self, count, n_frames)\n\tif self.utterances is None:\n        self._load_utterances()\n\n    utterances = self.utterance_cycler.sample(count)\n\n    a = [(u,) + u.random_partial(n_frames) for u in utterances]\n\n    return a\n\n", "CodeBleu_score": 93.58876149224903}]}
{"instance_id": "31e8b27b8f94", "generation_result": [{"code": "def normalize_volume(wav, target_dBFS, increase_only=False, decrease_only=False)\n\tif increase_only and decrease_only:\n        raise ValueError(\"Both increase_only and decrease_only are set\")\n    dBFS_change = target_dBFS - wav.dBFS\n    if dBFS_change < 0 and increase_only or dBFS_change > 0 and decrease_only:\n        return wav\n    return wav.apply_gain(dBFS_change)\n\n", "CodeBleu_score": 46.01746977323644}, {"code": "def normalize_volume(wav, target_dBFS, increase_only=False, decrease_only=False)\n\tif increase_only and decrease_only:\n        raise ValueError(\"Both increase_only and decrease_only are set\")\n    dBFS_change = target_dBFS - 10 * np.log10(np.mean(wav ** 2))\n    if (dBFS_change < 0 and increase_only) or (dBFS_change > 0 and decrease_only):\n        return wav\n    return wav * (10 ** (dBFS_change / 20))\n\n", "CodeBleu_score": 92.12905740681829}, {"code": "def normalize_volume(wav, target_dBFS, increase_only=False, decrease_only=False)\n\tif increase_only and decrease_only:\n        raise ValueError(\"Both increase_only and decrease_only are set\")\n    dBFS_change = target_dBFS - 10 * np.log10(np.mean(wav ** 2))\n    if (dBFS_change < 0 and increase_only) or (dBFS_change > 0 and decrease_only):\n        return wav\n    return wav * (10 ** (dBFS_change / 20))\n\n", "CodeBleu_score": 92.12905740681829}]}
{"instance_id": "2353831e9033", "generation_result": [{"code": "def load_model(weights_fpath: Path, device=None)\n\tif device is None:\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    global _model\n    _model = SpeakerEncoder(device).to(device)\n    _model.load_state_dict(torch.load(weights_fpath, map_location=device))\n    _model.eval()\n    print(\"Loaded speaker encoder \\\"{}\\\" trained to step {}\".format(\n        weights_fpath, _model.step))\n\n", "CodeBleu_score": 45.92916990421093}, {"code": "def load_model(weights_fpath: Path, device=None)\n\t# set the device to either a specified one or defaults to CUDA if available, otherwise CPU\n    device = device or torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(\"Using device:\", device)\n\n    # initialize a global speaker encoder model (_model) with the specified device\n    global _model\n    _model = SpeakerEncoder(device).to(device)\n\n    # load the model's state dictionary from a checkpoint file located at weights_fpath\n    _model.load_state_dict(torch.load(weights_fpath, map_location=device))\n\n    # set the model to evaluation mode\n    _model.eval()\n\n    # a confirmation message is printed, indicating the loaded model and the training step it was trained to\n    step = _model.get_step()\n    print(f\"Loaded step {step} from {weights_fpath}\")\n\n", "CodeBleu_score": 36.25848131721354}, {"code": "def load_model(weights_fpath: Path, device=None)\n\tif device is None:\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    global _model\n    _model = SpeakerEncoder(device).to(device)\n    _model.load_state_dict(torch.load(weights_fpath, map_location=device))\n\n    _model.eval()\n    print(\"Loaded speaker encoder \\\"{}\\\" trained to step {}\".format(\n        weights_fpath, _model.step))\n\n\n", "CodeBleu_score": 45.92916990421093}]}
{"instance_id": "3231ad480199", "generation_result": [{"code": "def embed_frames_batch(frames_batch)\n\t# Check if the model is loaded, raise an exception if not\n    if _model is None:\n        raise Exception(\"You need to load the model first!\")\n\n    # Convert the input from a NumPy array to a PyTorch tensor, and move it to the appropriate device\n    frames_batch = torch.from_numpy(frames_batch).to(_device)\n\n    # Perform a forward pass through the model to compute the embeddings\n    embeds = _model.forward(frames_batch)\n\n    # Detach the embeddings from the computation graph\n    embeds = embeds.detach()\n\n    # Move the embeddings to the CPU, convert them back to a NumPy array, and return them\n    return embeds.cpu().numpy()\n\n", "CodeBleu_score": 57.44396790986443}, {"code": "def embed_frames_batch(frames_batch)\n\t# Check if the model is loaded\n    if _model is None:\n        raise Exception(\"You must first load a model.\")\n    \n    # Convert the input frames_batch from a NumPy array to a PyTorch tensor and transfer it to the appropriate device\n    frames_batch_tensor = torch.tensor(frames_batch, dtype=torch.float32).to(_device)\n    \n    # Perform a forward pass through the model to compute the embeddings\n    embeds_batch = _model.forward(frames_batch_tensor)\n    \n    # Detach the embeddings from the computation graph\n    embeds_batch = embeds_batch.detach()\n    \n    # Move the embeddings to the CPU and convert them back to a NumPy array\n    embeds_batch = embeds_batch.cpu().numpy()\n    \n    return embeds_batch\n\n", "CodeBleu_score": 51.03220958551977}, {"code": "def embed_frames_batch(frames_batch)\n\t# Check that the model is loaded\n    if _model is None:\n        raise Exception('Model not loaded!')\n\n    # Convert the input frames_batch from a NumPy array to a PyTorch tensor\n    frames_batch_tensor = torch.from_numpy(frames_batch)\n\n    # Transfer the input frames_batch to the appropriate device\n    frames_batch_tensor = frames_batch_tensor.to(_device)\n\n    # Perform a forward pass through the model to compute the embeddings\n    embeds_batch = _model.forward(frames_batch_tensor)\n\n    # Detach the embeddings from the computation graph\n    embeds_batch = embeds_batch.detach()\n\n    # Move the embeddings to the CPU\n    embeds_batch = embeds_batch.to('cpu')\n\n    # Convert the embeddings from a PyTorch tensor to a NumPy array\n    embeds_batch = embeds_batch.numpy()\n\n    return embeds_batch\n\n", "CodeBleu_score": 50.45295062881616}]}
{"instance_id": "117a41716c8d", "generation_result": [{"code": "def compute_partial_slices(n_samples, partial_utterance_n_frames=partials_n_frames,\n                           min_pad_coverage=0.75, overlap=0.5)\n\tassert 0.0 < overlap < 1.0, \"Overlap must be between 0 and 1\"\n    assert 0.0 <= min_pad_coverage <= 1.0, \"Pad coverage must be between 0 and 1\"\n    assert partial_utterance_n_frames % 2 == 1, \"partial_utterance_n_frames must be odd\"\n\n    # Compute the number of frames and step\n    frames = n_samples // hparams.hop_length + 1\n    step = max(int(partial_utterance_n_frames * (1 - overlap)), 1)\n\n    # Compute the slices\n    wav_slices, mel_slices = [], []\n    i = 0\n    while 1:\n        start = i * step\n        end = min(start + partial_utterance_n_frames, frames)\n\n        if start >= end:\n            break\n\n        wav_slices.append((start, end))\n        mel_slices.append((start, end))\n        i += 1\n\n    # Evaluate whether extra padding is needed for the last slice\n    if len(mel_slices) > 1 and mel_slices[-1][1] != frames:\n        last_coverage = (frames - mel_slices[-1][0]) / (mel_slices[-1][1] - mel_slices[-1][0])\n        if last_coverage < min_pad_coverage:\n            wav_slices = wav_slices[:-1]\n            mel_slices = mel_slices[:-1]\n\n    return wav_slices, mel_slices\n\n\n\n", "CodeBleu_score": 39.10478883074819}, {"code": "def compute_partial_slices(n_samples, partial_utterance_n_frames=partials_n_frames,\n                           min_pad_coverage=0.75, overlap=0.5)\n\tassert 0.0 < overlap < 1.0, \"Overlap must be between 0 and 1\"\n    assert 0 < partial_utterance_n_frames, \"partial_utterance_n_frames must be strictly positive\"\n    assert 0.0 <= min_pad_coverage <= 1.0, \"min_pad_coverage must be between 0 and 1\"\n\n    # Compute the number of frames and frame step\n    frame_step = int(partial_utterance_n_frames * (1 - overlap))\n    n_frames = int((n_samples + frame_step - 1) / frame_step)\n\n    # Compute the wav and mel slices\n    wav_slices = []\n    mel_slices = []\n    for i in range(n_frames):\n        mel_range = partial_utterance_n_frames * i\n        mel_range = mel_range, mel_range + partial_utterance_n_frames\n\n        wav_range = frame_step * i\n        wav_range = wav_range, wav_range + partial_utterance_n_frames * sample_rate\n        wav_slices.append(wav_range)\n        mel_slices.append(mel_range)\n\n    # Evaluate whether extra padding is needed for the last frame\n    coverage = (n_samples - frame_step * (n_frames - 1)) / partial_utterance_n_frames\n    if coverage < min_pad_coverage and len(mel_slices) > 1:\n        wav_slices = wav_slices[:-1]\n        mel_slices = mel_slices[:-1]\n\n    return wav_slices, mel_slices", "CodeBleu_score": 42.05232261508733}, {"code": "def compute_partial_slices(n_samples, partial_utterance_n_frames=partials_n_frames,\n                           min_pad_coverage=0.75, overlap=0.5)\n\tassert 0 <= overlap < 1\n    assert 0 < min_pad_coverage <= 1\n    assert partial_utterance_n_frames % 2 == 0\n\n    # Compute the number of frames and frame step.\n    frame_step = int(partial_utterance_n_frames * (1 - overlap))\n    n_frames = int(np.ceil(n_samples / hop_length))\n\n    # Compute the slices.\n    wav_slices, mel_slices = [], []\n    steps = max(1, int(np.rint((n_frames - partial_utterance_n_frames) / frame_step)))\n    for i in range(steps):\n        start = int(i * frame_step)\n        end = int(start + partial_utterance_n_frames)\n        wav_slices.append((start, end))\n        mel_slices.append((start, end))\n\n    # Evaluate whether extra padding is needed for the last slice.\n    coverage = (n_samples - wav_slices[-1][0]) / (wav_slices[-1][1] - wav_slices[-1][0])\n    if coverage < min_pad_coverage and len(mel_slices) > 1:\n        wav_slices = wav_slices[:-1]\n        mel_slices = mel_slices[:-1]\n\n    return wav_slices, mel_slices\n\n\n", "CodeBleu_score": 48.16837227511635}]}
{"instance_id": "34fe5c864f1c", "generation_result": [{"code": "def plot_embedding_as_heatmap(embed, ax=None, title=\"\", shape=None, color_range=(0, 0.30))\n\timport matplotlib.pyplot as plt\n    if ax is None:\n        ax = plt.gca()\n    if shape is None:\n        shape = int(np.sqrt(embed.shape[0]))\n    embed = embed.reshape(shape, shape)\n    cmap = plt.cm.get_cmap('RdBu')\n    cmap.set_over('white')\n    cmap.set_under('black')\n    cmap.set_bad('white')\n    im = ax.imshow(embed, cmap=cmap, interpolation='none', vmin=color_range[0], vmax=color_range[1])\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.set_title(title)\n    ax.figure.colorbar(im)\n\n\n", "CodeBleu_score": 60.751587932546556}, {"code": "def plot_embedding_as_heatmap(embed, ax=None, title=\"\", shape=None, color_range=(0, 0.30))\n\tif ax is None:\n        ax = plt.gca()\n    if shape is None:\n        shape = int(np.sqrt(embed.shape[0]))\n    embed = embed.reshape(shape, shape)\n    cmap = sns.cubehelix_palette(start=color_range[0], rot=0.4,\n                                 gamma=1.0, as_cmap=True)\n    sns.heatmap(embed, cmap=cmap, xticklabels=False, yticklabels=False, ax=ax, cbar=False)\n    ax.set_title(title)\n    ax.set_xticks([])\n    ax.set_yticks([])\n\n", "CodeBleu_score": 52.15581492243337}, {"code": "def plot_embedding_as_heatmap(embed, ax=None, title=\"\", shape=None, color_range=(0, 0.30))\n\t# Check if an Axes object is provided\n    if ax is None:\n        ax = plt.gca()\n\n    # If the shape of the embedding is not specified, calculate it\n    if shape is None:\n        shape = int(np.sqrt(embed.shape[0]))\n        embed = embed.reshape(shape, shape)\n\n    # Plot the embedding as a heatmap\n    cmap = sns.diverging_palette(220, 10, as_cmap=True)\n    sns.heatmap(embed, cmap=cmap, cbar=True, xticklabels=False, yticklabels=False, ax=ax,\n                vmin=color_range[0], vmax=color_range[1])\n\n    # Set the title of the plot\n    ax.set_title(title)", "CodeBleu_score": 45.12067256854125}]}
{"instance_id": "92e93d50b62b", "generation_result": [{"code": "def similarity_matrix(self, embeds)\n\t# number of speakers per batch\n    n_speakers = embeds.shape[0]\n    # number of utterances per speaker\n    n_utterances = embeds.shape[1]\n    # initialize a similarity matrix with zeros\n    sim_matrix = torch.zeros(n_speakers, n_speakers, device=self.device)\n    # initialize a mask matrix to exclude self-similarity\n    mask = torch.eye(n_speakers, n_speakers, device=self.device)\n    # iterate over all speakers\n    for i in range(n_speakers):\n        # initialize centroids_incl and centroids_excl\n        centroids_incl = torch.zeros(n_utterances, self.embed_dim, device=self.device)\n        centroids_excl = torch.zeros(n_utterances, self.embed_dim, device=self.device)\n        # iterate over all utterances\n        for j in range(n_utterances):\n            # calculate centroids_incl and centroids_excl\n            centroids_incl[j] = (embeds[i, j] + torch.sum(embeds[i, :j], dim=0) + torch.sum(embeds[i, j + 1:], dim=0)) / (j + torch.sum(embeds[i, :j] != 0, dim=0) + torch.sum(embeds[i, j + 1:] != 0, dim=0) + 1)\n            centroids_excl[j] = torch.sum(embeds[i, :j], dim=0) / torch.sum(embeds[i, :j] != 0, dim=0) + torch.sum(embeds[i, j + 1:], dim=0) / torch.sum(embeds[i, j + 1:] != 0, dim=0)\n        # calculate the similarity scores\n        sim_scores_incl = torch.sum(embeds[i] * centroids_incl, dim=1)\n        sim_sc", "CodeBleu_score": 27.94728983804187}, {"code": "def similarity_matrix(self, embeds)\n\t# get shape of embeddings\n    n_speakers = embeds.shape[0]\n    n_utterances = embeds.shape[1]\n\n    # initialize similarity matrix and mask matrix\n    similarity_matrix = torch.zeros((n_speakers, n_speakers), device=self.device)\n    mask = torch.ones((n_speakers, n_speakers), device=self.device)\n    mask = mask.fill_diagonal_(0)\n\n    # compute centroids for each speaker\n    centroids_incl = torch.zeros((n_speakers, n_utterances, embeds.shape[2]), device=self.device)\n    centroids_excl = torch.zeros((n_speakers, n_utterances, embeds.shape[2]), device=self.device)\n\n    for i in range(n_speakers):\n        for j in range(n_utterances):\n            centroids_incl[i, j] = embeds[i, j].mean(dim=0)\n            centroids_excl[i, j] = embeds[i, :j].mean(dim=0)\n\n    # compute similarity scores for each speaker\n    for i in range(n_speakers):\n        for j in range(n_speakers):\n            # compute similarity scores\n            similarity_scores_incl = self.similarity(centroids_incl[i], centroids_incl[j])\n            similarity_scores_excl = self.similarity(centroids_excl[i], centroids_excl[j])\n\n            # update similarity matrix\n            similarity_matrix[i, j] = (similarity_scores_incl + similarity_scores_excl).mean()\n\n    # scale and bias similarity matrix\n    similarity_matrix = similarity_matrix * self.scale\n    similarity_matrix = similarity_matrix + self.bias\n\n    return similarity_matrix, mask", "CodeBleu_score": 29.262116915887955}, {"code": "def similarity_matrix(self, embeds)\n\t# get number of speakers and utterances per speaker\n    num_speakers = embeds.shape[0]\n    num_utts_per_speaker = embeds.shape[1]\n\n    # initialize similarity matrix\n    similarity_matrix = torch.zeros(num_speakers, num_speakers, device=self.device)\n\n    # initialize mask matrix\n    mask = torch.ones(num_speakers, num_speakers, device=self.device)\n    mask = mask.fill_diagonal_(0)\n\n    # calculate centroids\n    centroids_incl = torch.zeros(num_speakers, num_utts_per_speaker, self.embed_dim, device=self.device)\n    centroids_excl = torch.zeros(num_speakers, num_utts_per_speaker, self.embed_dim, device=self.device)\n\n    for i in range(num_speakers):\n        for j in range(num_utts_per_speaker):\n            centroids_incl[i, j, :] = torch.mean(embeds[i, :, :], dim=0)\n            centroids_excl[i, j, :] = torch.mean(embeds[i, :, :], dim=0)\n\n    # calculate similarity scores\n    for i in range(num_speakers):\n        for j in range(num_speakers):\n            for k in range(num_utts_per_speaker):\n                similarity_matrix[i, j] += torch.cosine_similarity(embeds[i, k, :], centroids_incl[j, k, :], dim=0)\n                similarity_matrix[i, j] += torch.cosine_similarity(embeds[i, k, :], centroids_excl[j, k, :], dim=0)\n            similarity_matrix[i, j] /= (2 * num_utts_per_speaker)\n\n    # apply mask\n    similarity_matrix = similarity_matrix * mask\n\n    # scale and bias\n    similarity_matrix =", "CodeBleu_score": 25.482627577690366}]}
{"instance_id": "a6eba54108a1", "generation_result": [{"code": "def loss(self, embeds)\n\tspeakers_per_batch, utterances_per_speaker = embeds.shape[:2]\n    similarity_matrix = self.similarity_matrix(embeds)\n    labels = torch.from_numpy(np.array(range(speakers_per_batch))).repeat(utterances_per_speaker).to(self.loss_device)\n    labels = labels.view(-1, 1).repeat(1, speakers_per_batch).view(-1)\n    labels = labels.long()\n    loss = self.loss_fn(similarity_matrix.view(-1, speakers_per_batch), labels)\n    eer = self.compute_eer(similarity_matrix.view(-1, speakers_per_batch), labels)\n    return loss, eer\n\n", "CodeBleu_score": 30.13803667808083}, {"code": "def loss(self, embeds)\n\tspeakers_per_batch, utterances_per_speaker = embeds.shape[:2]\n    similarity_matrix = self.similarity_matrix(embeds)\n    # Reshape similarity matrix to a 1D vector of size speakers_per_batch * (speakers_per_batch - 1).\n    # Each row is a speaker, and each column is a comparison between two speakers.\n    similarity_matrix_reshaped = similarity_matrix.view(-1, 1)\n    # Compute labels: 1 if the two speakers belong to the same class, 0 otherwise.\n    labels = torch.from_numpy(np.repeat(np.arange(speakers_per_batch), speakers_per_batch)).to(self.loss_device)\n    labels = (labels == labels.T).float()\n    # Compute the loss.\n    loss = self.loss_fn(similarity_matrix_reshaped, labels)\n    # Compute the Equal Error Rate (EER).\n    with torch.no_grad():\n        preds = similarity_matrix_reshaped.sigmoid().cpu().data.numpy()\n        labels = labels.cpu().data.numpy()\n        labels[np.isnan(labels)] = 0\n        fpr, tpr, thresholds = roc_curve(labels.flatten(), preds.flatten())\n        eer = brentq(lambda x: 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n        eer = np.mean(labels.flatten() == (preds.flatten() >= eer).astype(int))\n    return loss, eer\n\n", "CodeBleu_score": 56.36472147564129}, {"code": "def loss(self, embeds)\n\tspeakers_per_batch, utterances_per_speaker = embeds.shape[:2]\n    sim_matrix = self.similarity_matrix(embeds)\n    sim_matrix = sim_matrix.reshape(-1, speakers_per_batch)\n\n    # Ground truth labels are generated\n    labels = torch.arange(speakers_per_batch).repeat(utterances_per_speaker).view(-1, 1)\n    labels = labels.to(self.loss_device)\n\n    # Loss is calculated\n    loss = self.loss_fn(sim_matrix, labels)\n\n    # EER is calculated\n    with torch.no_grad():\n        preds = F.softmax(sim_matrix, dim=1)\n        preds = preds.reshape(-1, speakers_per_batch)[:, 1].cpu().numpy()\n        labels = labels.reshape(-1).cpu().numpy()\n        eer, _, _ = calculate_eer(labels, preds)\n\n    return loss, eer\n\n", "CodeBleu_score": 40.22270045460318}]}
{"instance_id": "ac7b74b9d1b1", "generation_result": [{"code": "def log_params(self)\n\tif not self.disable_logging:\n\n        # import parameter configurations\n        import mlflow\n        from mlflow.tracking import MlflowClient\n        client = MlflowClient()\n        experiment = client.get_experiment_by_name(self.experiment_name)\n        experiment_id = experiment.experiment_id\n        runs = mlflow.search_runs(experiment_ids=[experiment_id], order_by=[\"attributes.start_time DESC\"])\n        artifact_uri = runs[0].info.artifact_uri\n        artifact_uri = artifact_uri.replace('file://','')\n\n        # iterate through model and data parameters\n        model_params = \"\"\n        data_params = \"\"\n        for name, value in self.model_params.items():\n            model_params += f\"{name}: {value}", "CodeBleu_score": 21.83063578350518}, {"code": "def log_params(self)\n\tif self.log_params:\n        import json\n        import pandas as pd\n        import plotly.express as px\n\n        # Import parameters\n        with open(self.params_file) as json_file:\n            params = json.load(json_file)\n\n        # Iterate through model parameters\n        model_params = []\n        for key, value in self.model.get_params().items():\n            model_params.append(f\"{key}: {value}\")\n\n        # Iterate through data parameters\n        data_params = []\n        for key, value in params.items():\n            data_params.append(f\"{key}: {value}\")\n\n        # Create dataframe\n        df = pd.DataFrame({\"Model Parameters\": model_params, \"Data Parameters\": data_params})\n\n        # Visualize parameters\n        fig = px.bar(df, x=\"Model Parameters\", y=\"Data Parameters\", orientation=\"h\", title=\"Parameters\")\n        fig.show()\n\n", "CodeBleu_score": 25.858682105666485}, {"code": "def log_params(self)\n\t# Import parameter configurations\n    from . import params\n\n    # Initialize the string\n    string = \"\"\n\n    # Iterate through the model parameters\n    for key in params.model:\n        # Append the name and value to the string\n        string += f\"{key} = {params.model[key]}", "CodeBleu_score": 4.109545889273027}]}
{"instance_id": "4726b7864386", "generation_result": [{"code": "def draw_projections(self, embeds, utterances_per_speaker, step, out_fpath=None, max_speakers=10)\n\tif not self.show_embeddings:\n        return\n    import matplotlib.pyplot as plt\n    import matplotlib.cm as cm\n    import umap\n    import numpy as np\n    from sklearn.manifold import TSNE\n\n    if len(embeds) == 0:\n        return\n    # Limit the number of speakers to plot\n    max_speakers = min(max_speakers, len(embeds))\n    # Flatten the embeddings\n    flat_embeds = np.array([embed.flatten() for embed in embeds[:max_speakers]])\n    # Assign a color to each speaker\n    speaker_colors = cm.rainbow(np.linspace(0, 1, max_speakers))\n    # Reduce dimensionality with UMAP\n    reducer = umap.UMAP()\n    projection = reducer.fit_transform(flat_embeds)\n    # Plot the embeddings\n    fig, ax = plt.subplots(figsize=(10, 10))\n    ax.scatter(projection[:, 0], projection[:, 1], c=speaker_colors, marker='.')\n    # Set the plot title\n    ax.set_title(f'Speaker Embeddings at Step {step}')\n    # Set the axis labels\n    ax.set_xlabel('UMAP Dimension 1')\n    ax.set_ylabel('UMAP Dimension 2')\n    # Draw the legend\n    labels = list(embeds[:max_speakers].keys())\n    legend = ax.legend(labels, loc='upper left', bbox_to_anchor=(1, 1))\n    # Display the plot if enabled\n    if self.display_embeddings:\n        plt.show()\n    # Save the plot if a file path is specified\n    if out_fpath is not None:\n        fig.savefig(out_fpath, bbox_extra_artists=(legend,), bbox_inches='tight')", "CodeBleu_score": 47.10499267078719}, {"code": "def draw_projections(self, embeds, utterances_per_speaker, step, out_fpath=None, max_speakers=10)\n\tif len(embeds) == 0:\n        print(\"No embeddings to visualize.\")\n        return\n    if len(embeds) > max_speakers:\n        print(f\"Visualizing {max_speakers} of {len(embeds)} speakers.\")\n        embeds = embeds[:max_speakers]\n        utterances_per_speaker = utterances_per_speaker[:max_speakers]\n    embeds = np.concatenate(embeds)\n    umap_embeds = umap.UMAP(random_state=1).fit_transform(embeds)\n    plt.figure(figsize=(10, 10))\n    plt.title(f\"Step {step}\")\n    plt.scatter(umap_embeds[:, 0], umap_embeds[:, 1], c=np.arange(len(utterances_per_speaker)))\n    plt.colorbar()\n    if out_fpath:\n        plt.savefig(out_fpath)\n    else:\n        plt.show()\n\n", "CodeBleu_score": 35.49178847443259}, {"code": "def draw_projections(self, embeds, utterances_per_speaker, step, out_fpath=None, max_speakers=10)\n\t# Limit the number of speakers to visualize\n    if len(embeds) > max_speakers:\n        embeds = embeds[:max_speakers]\n        utterances_per_speaker = utterances_per_speaker[:max_speakers]\n\n    # Flatten the embeddings\n    embeds = embeds.flatten(1)\n\n    # Assign colors based on speaker identity\n    colors = np.array([i for i in range(len(embeds))])\n\n    # Reduce the dimensionality of the embeddings\n    umap_embeddings = umap.UMAP().fit_transform(embeds)\n\n    # Plot the embeddings\n    fig = plt.figure(figsize=(10, 10))\n    plt.gca().set_prop_cycle(None)\n    plt.scatter(umap_embeddings[:, 0], umap_embeddings[:, 1], c=colors)\n    plt.colorbar()\n    plt.title(f'Step {step}')\n    plt.xlabel('UMAP 1')\n    plt.ylabel('UMAP 2')\n\n    # Display the plot if not disabled\n    if self.display_visuals:\n        plt.show()\n\n    # Save the plot if a file path is provided\n    if out_fpath is not None:\n        fig.savefig(out_fpath)\n\n    # Close the plot\n    plt.close(fig)", "CodeBleu_score": 43.17737209875724}]}
{"instance_id": "274fa50985c0", "generation_result": [{"code": "def add_speaker_embedding(self, x, speaker_embedding)\n\t# save the dimensions of the input tensor as human-readable names\n    batch_size, sequence_length, embedding_dimension = x.shape\n\n    # determine the appropriate dimension index for the speaker embedding\n    if self.speaker_embedding_index == 0:\n        # the speaker embedding is the first dimension\n        speaker_embedding_dimension = batch_size\n    elif self.speaker_embedding_index == 1:\n        # the speaker embedding is the second dimension\n        speaker_embedding_dimension = sequence_length\n    else:\n        # the speaker embedding is the third dimension\n        speaker_embedding_dimension = embedding_dimension\n\n    # repeat the speaker embedding to match the length of the input text\n    speaker_embedding = speaker_embedding.repeat(speaker_embedding_dimension)\n\n    # reshape and transpose the speaker embedding\n    speaker_embedding = speaker_embedding.reshape(batch_size, sequence_length, embedding_dimension)\n    speaker_embedding = speaker_embedding.transpose(0, 1)\n\n    # concatenate the speaker embedding with the input tensor along the last dimension\n    x = torch.cat([x, speaker_embedding], dim=2)\n\n    return x", "CodeBleu_score": 31.713846410632463}, {"code": "def add_speaker_embedding(self, x, speaker_embedding)\n\t# get the dimensions of the input tensor\n    batch_size, time_steps, n_features = x.shape\n    # determine the dimension index for the speaker embedding\n    if self.speaker_embedding_dim == \"prepend\":\n        # prepend the speaker embedding to the input tensor\n        speaker_embedding = speaker_embedding.repeat(time_steps, 1)\n        speaker_embedding = speaker_embedding.reshape(time_steps, batch_size, -1)\n        speaker_embedding = speaker_embedding.transpose(0, 1)\n        x = torch.cat([speaker_embedding, x], dim=2)\n    elif self.speaker_embedding_dim == \"append\":\n        # append the speaker embedding to the input tensor\n        speaker_embedding = speaker_embedding.repeat(time_steps, 1)\n        speaker_embedding = speaker_embedding.reshape(time_steps, batch_size, -1)\n        x = torch.cat([x, speaker_embedding], dim=2)\n    else:\n        # raise an error if the speaker embedding dimension is not valid\n        raise ValueError(f\"The speaker embedding dimension {self.speaker_embedding_dim} is not valid!\")\n    return x\n\n", "CodeBleu_score": 28.658518413356393}, {"code": "def add_speaker_embedding(self, x, speaker_embedding)\n\tif self.speaker_embedding_dim == 0:\n        return x\n    if self.speaker_embedding_dim == 1:\n        x = torch.unsqueeze(x, 1)\n        speaker_embedding = torch.unsqueeze(speaker_embedding, 1)\n    if self.speaker_embedding_dim == 2:\n        x = torch.unsqueeze(x, 1).transpose(1, 2)\n        speaker_embedding = torch.unsqueeze(speaker_embedding, 1).transpose(1, 2)\n    speaker_embedding = speaker_embedding.repeat(1, x.shape[self.speaker_embedding_dim] // speaker_embedding.shape[1], 1)\n    speaker_embedding = speaker_embedding.reshape(speaker_embedding.shape[0], speaker_embedding.shape[1], -1).transpose(1, 2)\n    x = torch.cat([x, speaker_embedding], dim=2)\n    return x\n\n", "CodeBleu_score": 33.00777345249082}]}
{"instance_id": "e6742ea0fe5e", "generation_result": [{"code": "def init_attention(self, encoder_seq_proj)\n\tself.device = next(self.parameters()).device\n    self.batch_size, self.seq_len, self.hidden_size = encoder_seq_proj.shape\n    self.cumulative_attention = torch.zeros((self.batch_size, self.seq_len)).to(self.device)\n    self.attention = torch.zeros((self.batch_size, self.seq_len)).to(self.device)\n", "CodeBleu_score": 23.657874579463023}, {"code": "def init_attention(self, encoder_seq_proj)\n\tdevice = next(self.parameters()).device\n    batch_size, seq_len, _ = encoder_seq_proj.shape\n    self.cumulative_alpha = torch.zeros((batch_size, seq_len)).to(device)\n    self.alpha = torch.zeros((batch_size, seq_len)).to(device)\n\n", "CodeBleu_score": 22.433601183472707}, {"code": "def init_attention(self, encoder_seq_proj)\n\tself.device = next(self.parameters()).device\n    self.encoder_seq_proj_shape = encoder_seq_proj.shape\n    self.batch_size, self.seq_len = encoder_seq_proj.shape[:2]\n\n    self.cumulative_weights = torch.zeros(self.batch_size, self.seq_len).to(self.device)\n    self.attention_weights = torch.zeros(self.batch_size, self.seq_len).to(self.device)\n\n\n", "CodeBleu_score": 26.955493720066748}]}
{"instance_id": "4e1fc1ed0ed7", "generation_result": [{"code": "def load(self, path, optimizer=None)\n\tdevice = next(self.parameters()).device\n    checkpoint = torch.load(str(path), map_location=device)\n    self.load_state_dict(checkpoint[\"model_state\"])\n\n    if \"optimizer_state\" in checkpoint and optimizer is not None:\n        optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n\n", "CodeBleu_score": 67.57285718969018}, {"code": "def load(self, path, optimizer=None)\n\t# Use device of model params as location for loaded state\n    device = next(self.parameters()).device\n    checkpoint = torch.load(str(path), map_location=device)\n    self.load_state_dict(checkpoint[\"model_state\"])\n\n    if \"optimizer_state\" in checkpoint and optimizer is not None:\n        optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n\n", "CodeBleu_score": 67.98839425729693}, {"code": "def load(self, path, optimizer=None)\n\t# Use device of model params as location for loaded state\n    device = next(self.parameters()).device\n    checkpoint = torch.load(str(path), map_location=device)\n    self.load_state_dict(checkpoint[\"model_state\"])\n\n    if \"optimizer_state\" in checkpoint and optimizer is not None:\n        optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n\n", "CodeBleu_score": 67.98839425729693}]}
{"instance_id": "178e6aa16c0f", "generation_result": [{"code": "def save(self, path, optimizer=None)\n\tif optimizer is not None:\n        torch.save({\n            \"model_state\": self.state_dict(),\n            \"optimizer_state\": optimizer.state_dict(),\n        }, str(path))\n    else:\n        torch.save({\n            \"model_state\": self.state_dict(),\n        }, str(path))\n\n", "CodeBleu_score": 93.03451014294475}, {"code": "def save(self, path, optimizer=None)\n\tif optimizer is not None:\n        torch.save({\n            \"model_state\": self.state_dict(),\n            \"optimizer_state\": optimizer.state_dict(),\n        }, str(path))\n    else:\n        torch.save({\n            \"model_state\": self.state_dict(),\n        }, str(path))\n\n", "CodeBleu_score": 93.03451014294475}, {"code": "def save(self, path, optimizer=None)\n\tif optimizer is not None:\n        torch.save({\n            \"model_state\": self.state_dict(),\n            \"optimizer_state\": optimizer.state_dict(),\n        }, str(path))\n    else:\n        torch.save({\n            \"model_state\": self.state_dict(),\n        }, str(path))\n\n", "CodeBleu_score": 93.03451014294475}]}
{"instance_id": "6528fd80e87b", "generation_result": [{"code": "def english_cleaners(text)\n\ttext = convert_to_ascii(text)\n    text = lowercase(text)\n    text = expand_numbers(text)\n    text = expand_abbreviations(text)\n    text = collapse_whitespace(text)\n    return text\n\n", "CodeBleu_score": 55.236418047148106}, {"code": "def english_cleaners(text)\n\ttext = convert_to_ascii(text)\n    text = lowercase(text)\n    text = expand_numbers(text)\n    text = expand_abbreviations(text)\n    text = collapse_whitespace(text)\n    return text\n\n", "CodeBleu_score": 55.236418047148106}, {"code": "def english_cleaners(text)\n\ttext = convert_to_ascii(text)\n    text = lowercase(text)\n    text = expand_numbers(text)\n    text = expand_abbreviations(text)\n    text = collapse_whitespace(text)\n    return text\n\n", "CodeBleu_score": 55.236418047148106}]}
{"instance_id": "8a4fe08e0985", "generation_result": [{"code": "def text_to_sequence(text, cleaner_names)\n\ttext = _clean_text(text, cleaner_names)\n    sequence = []\n    # Check for curly braces and treat their contents as ARPAbet:\n    while len(text):\n        m = _curly_re.match(text)\n        if not m:\n            sequence += _symbols_to_sequence(_clean_text(text, cleaner_names).split())\n            break\n        sequence += _arpabet_to_sequence(m.group(1))\n        text = m.group(2)\n    # Append EOS token\n    sequence += [EOS_ID]\n    return sequence\n\n", "CodeBleu_score": 48.0833469971252}, {"code": "def text_to_sequence(text, cleaner_names)\n\ttext = _clean_text(text, cleaner_names)\n    if not text:\n        return None\n    sequence = []\n    # Check for curly braces and treat their contents as ARPAbet:\n    while len(text):\n        m = _curly_re.match(text)\n        if not m:\n            sequence += _symbols_to_sequence(_clean_text(text, cleaner_names).split())\n            break\n        sequence += _arpabet_to_sequence(m.group(1))\n        text = m.group(2)\n    # Append EOS token\n    sequence.append(_eos_id)\n    return sequence\n\n", "CodeBleu_score": 60.01720137671107}, {"code": "def text_to_sequence(text, cleaner_names)\n\tsequence = []\n    # Check for curly braces and treat their contents as ARPAbet:\n    while len(text):\n        m = _curly_re.match(text)\n        if not m:\n            sequence += _symbols_to_sequence(_clean_text(text, cleaner_names))\n            break\n        sequence += _arpabet_to_sequence(m.group(1))\n        text = m.group(2)\n    # Append EOS token\n    sequence.append(_symbol_to_id[\"~\"])\n    return sequence\n\n", "CodeBleu_score": 62.32559722902214}]}
{"instance_id": "e1c25a37af1f", "generation_result": [{"code": "def sequence_to_text(sequence)\n\ttext = \"\"\n    for symbol_id in sequence:\n        if symbol_id in _arpabet:\n            text += \"{\" + _arpabet[symbol_id] + \"} \"\n        else:\n            text += _symbols[symbol_id]\n    return text\n\n", "CodeBleu_score": 30.517666133100306}, {"code": "def sequence_to_text(sequence)\n\tresult = \"\"\n    for symbol_id in sequence:\n        if symbol_id in _symbol_to_id:\n            s = _id_to_symbol[symbol_id]\n            # Enclose ARPAbet back in curly braces:\n            if len(s) > 1 and s[0] == \"@\":\n                s = \"{%s}\" % s[1:]\n            result += s\n    result = result.replace(\"}{\", \" \")\n    return result.strip()\n\n", "CodeBleu_score": 70.85427749639473}, {"code": "def sequence_to_text(sequence)\n\tresult = \"\"\n    for symbol_id in sequence:\n        if symbol_id in _arpabet_symbols:\n            result += \"{%s}\" % _arpabet_symbols[symbol_id]\n        else:\n            result += _symbols[symbol_id]\n    return result.replace(\"}{\", \" \")", "CodeBleu_score": 36.715794013718096}]}
{"instance_id": "7807cad4b05d", "generation_result": [{"code": "def embed_utterance(fpaths, encoder_model_fpath)\n\tif not encoder.is_loaded():\n        encoder.load_model(encoder_model_fpath)\n\n    # Compute the speaker embedding of the utterance\n    wav_fpath, embed_fpath = fpaths\n    wav = np.load(wav_fpath)\n    wav = encoder.preprocess_wav(wav)\n    embed = encoder.embed_utterance(wav)\n    np.save(embed_fpath, embed, allow_pickle=False)\n\n", "CodeBleu_score": 94.89398130768922}, {"code": "def embed_utterance(fpaths, encoder_model_fpath)\n\tif not encoder.is_loaded():\n        encoder.load_model(encoder_model_fpath)\n\n    # Compute the speaker embedding of the utterance\n    wav_fpath, embed_fpath = fpaths\n    wav = np.load(wav_fpath)\n    wav = encoder.preprocess_wav(wav)\n    embed = encoder.embed_utterance(wav)\n    np.save(embed_fpath, embed, allow_pickle=False)\n\n", "CodeBleu_score": 94.89398130768922}, {"code": "def embed_utterance(fpaths, encoder_model_fpath)\n\tif not encoder.is_loaded():\n        encoder.load_model(encoder_model_fpath)\n\n    # Compute the speaker embedding of the utterance\n    wav_fpath, embed_fpath = fpaths\n    wav = np.load(wav_fpath)\n    wav = encoder.preprocess_wav(wav)\n    embed = encoder.embed_utterance(wav)\n    np.save(embed_fpath, embed, allow_pickle=False)\n\n", "CodeBleu_score": 94.89398130768922}]}
{"instance_id": "9f860273a325", "generation_result": [{"code": "def create_embeddings(synthesizer_root: Path, encoder_model_fpath: Path, n_processes: int)\n\t# Verify synthesizer_root\n    if not synthesizer_root.exists():\n        raise Exception(\"The specified root directory for the synthesizer does not exist: %s\" % synthesizer_root)\n    if not encoder_model_fpath.exists():\n        raise Exception(\"The specified path to an encoder model does not exist: %s\" % encoder_model_fpath)\n\n    # Verify the encoder\n    encoder_model_fpath = synthesizer_root.joinpath(\"encoder\", \"saved_model.pb\")\n    if not encoder_model_fpath.exists():\n        raise Exception(\"The specified encoder model does not exist: %s\" % encoder_model_fpath)\n\n    # Gather the input directories\n    datasets_root = synthesizer_root.joinpath(\"datasets\")\n    input_dirs = [datasets_root.joinpath(\"audio\")]\n    input_dirs += [datasets_root.joinpath(\"audio\", name) for name in os.listdir(datasets_root.joinpath(\"audio\"))]\n    print(\"\n    \".join([\"Using data from:\"] + [\"- %s\" % s for s in input_dirs]))\n\n    # Create the output directories for embeddings\n    embeddings_root = synthesizer_root.joinpath(\"encoder\", \"embeddings\")\n    for input_dir in input_dirs:\n        name = os.path.basename(input_dir)\n        if name == \"audio\":\n            continue\n        embeddings_dir = embeddings_root.joinpath(name)\n        if not embeddings_dir.exists():\n            os.makedirs(embeddings_dir)\n\n    # Gather the list of audio files and the corresponding output files\n    file_paths = []\n    for input_dir in input_dirs:\n        for file_name in os.listdir(input_dir):\n            if file_name.endswith(\".wav\"):\n                file_path = os.path.join(input_dir, file_name)\n                file_paths.append(file_path)\n\n    # Prepare the", "CodeBleu_score": 32.77703756134109}, {"code": "def create_embeddings(synthesizer_root: Path, encoder_model_fpath: Path, n_processes: int)\n\t# verify synthesizer root directory\n    if not synthesizer_root.exists():\n        raise Exception(f\"Synthesizer root directory not found at {synthesizer_root}\")\n\n    # verify encoder model\n    if not encoder_model_fpath.exists():\n        raise Exception(f\"Encoder model not found at {encoder_model_fpath}\")\n\n    # verify encoder model\n    if not encoder_model_fpath.exists():\n        raise Exception(f\"Encoder model not found at {encoder_model_fpath}\")\n\n    # verify synthesizer root directory\n    if not synthesizer_root.exists():\n        raise Exception(f\"Synthesizer root directory not found at {synthesizer_root}\")\n\n    # verify encoder model\n    if not encoder_model_fpath.exists():\n        raise Exception(f\"Encoder model not found at {encoder_model_fpath}\")\n\n    # verify encoder model\n    if not encoder_model_fpath.exists():\n        raise Exception(f\"Encoder model not found at {encoder_model_fpath}\")\n\n    # verify encoder model\n    if not encoder_model_fpath.exists():\n        raise Exception(f\"Encoder model not found at {encoder_model_fpath}\")\n\n    # verify encoder model\n    if not encoder_model_fpath.exists():\n        raise Exception(f\"Encoder model not found at {encoder_model_fpath}\")\n\n    # verify encoder model\n    if not encoder_model_fpath.exists():\n        raise Exception(f\"Encoder model not found at {encoder_model_fpath}\")\n\n    # verify encoder model\n    if not encoder_model_fpath.exists():\n        raise Exception(f\"Encoder model not found at {encoder_model_fpath}\")\n\n    # verify encoder model\n    if not encoder_model_fpath.exists():\n        raise Exception(f\"Encoder model not found at {encoder_model_fpath}\")\n\n    # verify encoder model\n    if not encoder_model_fpath.exists():\n        raise Exception(f\"", "CodeBleu_score": 15.268259642916625}, {"code": "def create_embeddings(synthesizer_root: Path, encoder_model_fpath: Path, n_processes: int)\n\t# Verify that the synthesizer root directory exists\n    synthesizer_root = Path(synthesizer_root).expanduser()\n    if not synthesizer_root.exists():\n        raise Exception(\"Synthesizer root directory does not exist: {}\".format(synthesizer_root))\n    \n    # Verify that the encoder model file exists\n    encoder_model_fpath = Path(encoder_model_fpath).expanduser()\n    if not encoder_model_fpath.exists():\n        raise Exception(\"The specified encoder model does not exist: {}\".format(encoder_model_fpath))\n    \n    # Load the metadata\n    metadata_fpath = synthesizer_root.joinpath(\"train.txt\")\n    if not metadata_fpath.exists():\n        raise Exception(\"Could not find the metadata file: {}\".format(metadata_fpath))\n    with metadata_fpath.open(\"r\", encoding=\"utf-8\") as metadata_file:\n        metadata = [line.split(\"|\") for line in metadata_file]\n    \n    # Create the output directories for embeddings\n    embed_dir = synthesizer_root.joinpath(\"embed\")\n    embed_dir.mkdir(exist_ok=True)\n    \n    # Create a multiprocessing pool to encode the utterances in parallel\n    pool = multiprocessing.Pool(processes=n_processes)\n    metadata_chunks = [metadata[i:i + n_processes] for i in range(0, len(metadata), n_processes)]\n    func = partial(embed_utterance, encoder_model_fpath, embed_dir)\n    for metadata_chunk in tqdm(metadata_chunks):\n        pool.map(func, metadata_chunk)\n    \n    # Clean up the multiprocessing pool\n    pool.close()\n    pool.join()\n\n", "CodeBleu_score": 49.523252871898734}]}
{"instance_id": "9eb0abda14a9", "generation_result": [{"code": "def eval_model(attention, mel_prediction, target_spectrogram, input_seq, step,\n               plot_dir, mel_output_dir, wav_dir, sample_num, loss, hparams)\n\t# save attention plot\n    if attention is not None:\n        save_attention(attention, f\"{plot_dir}/step{step}_sample{sample_num}_attention\")\n\n    # save predicted mel spectrogram plot\n    if mel_prediction is not None:\n        plot_spectrogram(mel_prediction, f\"{plot_dir}/step{step}_sample{sample_num}_predicted_spectrogram.png\",\n                         title=f\"Step {step}, loss {loss:.4f}\", target_spectrogram=target_spectrogram)\n\n    # save predicted wav file\n    if mel_prediction is not None:\n        mel_prediction = mel_prediction.T\n        mel_prediction = mel_prediction[:, :target_spectrogram.shape[1]]\n        wav = inv_mel_spectrogram(mel_prediction, hparams)\n        wav_path = f\"{wav_dir}/step{step}_sample{sample_num}_predicted.wav\"\n        save_wav(wav, wav_path)\n\n    # save reconstructed wav file\n    if mel_output_dir is not None:\n        mel_output_path = f\"{mel_output_dir}/step{step}_sample{sample_num}_predicted.npy\"\n        np.save(mel_output_path, mel_prediction, allow_pickle=False)\n\n    # print input sequence\n    if input_seq is not None:\n        print(f\"Input Sequence: {sequence_to_text(input_seq)}\")\n\n", "CodeBleu_score": 33.342713107900124}, {"code": "def eval_model(attention, mel_prediction, target_spectrogram, input_seq, step,\n               plot_dir, mel_output_dir, wav_dir, sample_num, loss, hparams)\n\t#save_attention(attention, os.path.join(plot_dir, \"step_{}_{}_attention.png\".format(step, sample_num)))\n    #save_spectrogram(mel_prediction, os.path.join(mel_output_dir, \"step_{}_{}_predicted.png\".format(step, sample_num)))\n    #save_spectrogram(target_spectrogram, os.path.join(mel_output_dir, \"step_{}_{}_target.png\".format(step, sample_num)))\n    #save_spectrogram(target_spectrogram, os.path.join(mel_output_dir, \"step_{}_{}_target.npy\".format(step, sample_num)))\n    #save_spectrogram(mel_prediction, os.path.join(mel_output_dir, \"step_{}_{}_predicted.npy\".format(step, sample_num)))\n    #plot_spectrogram(mel_prediction, os.path.join(plot_dir, \"step_{}_{}_predicted.png\".format(step, sample_num)), title=\"{}, {}, step={}, loss={:.5f}\".format(hparams.model, time_string(), step, loss), target_spectrogram=target_spectrogram)\n\n    # save wav (mel -> wav)\n    #wav = inv_mel_spectrogram(mel_prediction.T, hparams)\n    #librosa.output.write_wav(os.path.join(wav_dir, \"step_{}_{}_{}.wav\".format(step, sample_num, hparams.model)), wav, hparams.sample_rate)\n\n    # print(\"Input at step {}: {}\".format(step, sequence_to_text(input_seq)))\n\n", "CodeBleu_score": 4.500446030330062}, {"code": "def eval_model(attention, mel_prediction, target_spectrogram, input_seq, step,\n               plot_dir, mel_output_dir, wav_dir, sample_num, loss, hparams)\n\timport matplotlib\n    matplotlib.use(\"Agg\")\n    import matplotlib.pyplot as plt\n\n    # save the attention plot\n    attention_path = \"{}/step_{}_{}_attention.png\".format(plot_dir, step, sample_num)\n    save_attention(attention, attention_path)\n\n    # save the predicted mel spectrogram plot\n    mel_output_path = \"{}/step_{}_{}_mel_spectrogram.png\".format(mel_output_dir, step, sample_num)\n    plot_spectrogram(pred_spectrogram=mel_prediction, path=mel_output_path, title=\"Predicted Mel Spectrogram\", split_title=True, target_spectrogram=target_spectrogram)\n\n    # save the reconstructed waveform plot\n    wav = audio.inv_mel_spectrogram(mel_prediction.T, hparams)\n    audio.save_wav(wav, \"{}/step_{}_{}_predicted.wav\".format(wav_dir, step, sample_num))\n\n    # save the reconstructed spectrogram plot\n    reconstructed_spectrogram = audio.melspectrogram(wav, hparams).T\n    reconstructed_spectrogram_path = \"{}/step_{}_{}_reconstructed_spectrogram.png\".format(mel_output_dir, step, sample_num)\n    plot_spectrogram(pred_spectrogram=reconstructed_spectrogram, path=reconstructed_spectrogram_path, title=\"Reconstructed Mel Spectrogram\", split_title=True, target_spectrogram=target_spectrogram)\n\n    # save the input sequence\n    input_path = \"{}/step_{}_{}_input.txt\".format(plot_dir, step, sample_num)\n    with open(input_path, \"w\") as f:\n        f.write(sequence_to_text(input_seq))\n\n    # save the loss plot\n    loss_path = \"{}/step_{}_{}_loss.png\".format(plot_dir, step, sample_num)\n    plot_loss(loss, loss_path, title=\"Loss\")", "CodeBleu_score": 42.75001992761865}]}
{"instance_id": "6bad4f500286", "generation_result": [{"code": "def num_params(self)\n\treturn sum(p.numel() for p in self.parameters() if p.requires_grad) / 1000000.0\n\n\n", "CodeBleu_score": 5.810683927435598}, {"code": "def num_params(self)\n\tnum_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n    print(f\"Number of parameters: {num_params/1e6:.3f}M\")\n\n", "CodeBleu_score": 12.167218696143722}, {"code": "def num_params(self)\n\tnum_params = sum([p.numel() for p in self.parameters() if p.requires_grad])\n    print(f\"Number of parameters: {num_params / 1e6:.3f}M\")\n\n", "CodeBleu_score": 18.151642790803525}]}
{"instance_id": "78b876054bd4", "generation_result": [{"code": "def pad_tensor(self, x, pad, side='both')\n\t# Determine the new size of the tensor.\n    new_size = x.shape[1] + pad*2\n    # Create a new tensor of the correct size and fill it with zeros.\n    if self.use_cuda:\n        x_padded = torch.zeros(x.shape[0], new_size, x.shape[2], device=self.device)\n    else:\n        x_padded = torch.zeros(x.shape[0], new_size, x.shape[2])\n    # Determine the correct pad indices.\n    if side == 'both':\n        left_pad = pad\n        right_pad = pad\n    elif side == 'before':\n        left_pad = pad\n        right_pad = 0\n    elif side == 'after':\n        left_pad = 0\n        right_pad = pad\n    # Place the original tensor into the padded tensor.\n    x_padded[:, left_pad:left_pad+x.shape[1], :] = x\n    return x_padded\n\n\n", "CodeBleu_score": 30.93631951248818}, {"code": "def pad_tensor(self, x, pad, side='both')\n\tif side not in ['before', 'after', 'both']:\n        raise ValueError(\"side must be either 'before', 'after', or 'both'.\")\n\n    if side == 'both':\n        pad = (pad, pad)\n    elif side == 'before':\n        pad = (pad, 0)\n    elif side == 'after':\n        pad = (0, pad)\n\n    x_size = x.size()\n    new_size = (x_size[0], x_size[1] + pad[0] + pad[1])\n    x_padded = torch.zeros(new_size, device=x.device, dtype=x.dtype)\n    if side == 'before':\n        x_padded[:, pad[0]:] = x\n    elif side == 'after':\n        x_padded[:, :x_size[1]] = x\n    else:\n        x_padded[:, pad[0]:x_size[1]+pad[0]] = x\n    return x_padded\n\n", "CodeBleu_score": 32.97589895090078}, {"code": "def pad_tensor(self, x, pad, side='both')\n\t# Determine the size of the new tensor\n    if side == 'both':\n        new_size = x.size()[0] + 2 * pad\n    else:\n        new_size = x.size()[0] + pad\n    # Create a new tensor filled with zeros\n    if self.use_cuda:\n        device = torch.device('cuda')\n    else:\n        device = torch.device('cpu')\n    x_pad = torch.zeros(new_size, *x.size()[1:]).to(device)\n    # Place the original tensor into the new tensor\n    if side == 'both':\n        x_pad[pad:x.size()[0] + pad] = x\n    elif side == 'before':\n        x_pad[pad:x.size()[0] + pad] = x\n    elif side == 'after':\n        x_pad[:x.size()[0]] = x\n    return x_pad\n\n", "CodeBleu_score": 37.218617385020224}]}
{"instance_id": "a7ed1715a3df", "generation_result": [{"code": "def save_attention(attn, path)\n\timport matplotlib.pyplot as plt\n    fig = plt.figure(figsize=(10, 10))\n    ax = fig.add_subplot(1, 1, 1)\n    ax.matshow(attn, cmap='viridis')\n    ax.set_xticklabels([''] + list(range(1, attn.shape[1] + 1)))\n    ax.set_yticklabels([''] + list(range(1, attn.shape[0] + 1)))\n    plt.tight_layout()\n    plt.savefig(path + '.png', bbox_inches='tight', pad_inches=0)\n    plt.close(fig)\n", "CodeBleu_score": 47.21692560566818}, {"code": "def save_attention(attn, path)\n\timport matplotlib.pyplot as plt\n    fig = plt.figure(figsize=(10,10))\n    plt.imshow(attn.T, cmap='Blues', interpolation='nearest', aspect='auto')\n    plt.colorbar()\n    plt.xticks(range(len(attn)), range(len(attn)))\n    plt.yticks(range(len(attn)), range(len(attn)))\n    plt.xlabel('Input')\n    plt.ylabel('Output')\n    plt.savefig(path)\n    plt.close(fig)\n\n\n", "CodeBleu_score": 49.68249278965643}, {"code": "def save_attention(attn, path)\n\timport matplotlib.pyplot as plt\n    fig = plt.figure(figsize=(10, 10))\n    ax = fig.add_subplot(1, 1, 1)\n    ax.matshow(attn, cmap='viridis')\n    ax.set_xticklabels([''] + list(range(1, attn.shape[1] + 1)))\n    ax.set_yticklabels([''] + list(range(1, attn.shape[0] + 1)))\n    plt.tight_layout()\n    plt.savefig(path)\n    plt.close()\n\n", "CodeBleu_score": 44.66022011233702}]}
{"instance_id": "19375c9f44a2", "generation_result": [{"code": "def plot(array)\n\timport matplotlib.pyplot as plt\n\n    fig = plt.figure(figsize=(30, 5))\n    ax = fig.add_subplot(111)\n    ax.xaxis.label.set_color('grey')\n    ax.yaxis.label.set_color('grey')\n    ax.xaxis.label.set_fontsize(23)\n    ax.yaxis.label.set_fontsize(23)\n    ax.tick_params(axis='x', colors='grey', labelsize=23)\n    ax.tick_params(axis='y', colors='grey', labelsize=23)\n    plt.plot(array)", "CodeBleu_score": 77.42583347278438}, {"code": "def plot(array)\n\timport matplotlib.pyplot as plt\n\n    fig = plt.figure(figsize=(30, 5))\n    ax = fig.add_subplot(111)\n    ax.xaxis.label.set_color('grey')\n    ax.yaxis.label.set_color('grey')\n    ax.xaxis.label.set_fontsize(23)\n    ax.yaxis.label.set_fontsize(23)\n    ax.tick_params(axis='x', colors='grey', labelsize=23)\n    ax.tick_params(axis='y', colors='grey', labelsize=23)\n    plt.plot(array)\n\n", "CodeBleu_score": 77.42583347278438}, {"code": "def plot(array)\n\timport matplotlib.pyplot as plt\n\n    fig = plt.figure(figsize=(30, 5))\n    ax = fig.add_subplot(111)\n    ax.xaxis.label.set_color('grey')\n    ax.yaxis.label.set_color('grey')\n    ax.xaxis.label.set_fontsize(23)\n    ax.yaxis.label.set_fontsize(23)\n    ax.tick_params(axis='x', colors='grey', labelsize=23)\n    ax.tick_params(axis='y', colors='grey', labelsize=23)\n    plt.plot(array)\n\n", "CodeBleu_score": 77.42583347278438}]}
{"instance_id": "ff94c425920c", "generation_result": [{"code": "def write_to_csv(image_name, prediction, confidence)\n\timport csv\n\n    data = {\n        \"image_name\": image_name,\n        \"prediction\": prediction,\n        \"confidence\": confidence\n    }\n\n    with open(\"predictions.csv\", \"a\") as csv_file:\n        fieldnames = [\"image_name\", \"prediction\", \"confidence\"]\n        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n\n        if csv_file.tell() == 0:\n            writer.writeheader()\n\n        writer.writerow(data)\n\n", "CodeBleu_score": 45.189533074514664}, {"code": "def write_to_csv(image_name, prediction, confidence)\n\twith open('predictions.csv', 'a') as csv_file:\n        fieldnames = ['image_name', 'prediction', 'confidence']\n        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n        if os.stat('predictions.csv').st_size == 0:\n            writer.writeheader()\n        writer.writerow({'image_name': image_name, 'prediction': prediction, 'confidence': confidence})\n\n", "CodeBleu_score": 44.065165712664964}, {"code": "def write_to_csv(image_name, prediction, confidence)\n\tdata = {'image_name': image_name, 'prediction': prediction, 'confidence': confidence}\n    with open('predictions.csv', 'a') as csv_file:\n        fieldnames = ['image_name', 'prediction', 'confidence']\n        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n        if csv_file.tell() == 0:\n            writer.writeheader()\n        writer.writerow(data)\n\n\n", "CodeBleu_score": 47.61832553399379}]}
{"instance_id": "436161602009", "generation_result": [{"code": "def export_torchscript(model, im, file, optimize, prefix=colorstr(\"TorchScript:\"))\n\ttry:\n        print(f\"{prefix} starting export with torch {torch.__version__}...\")\n        f = opt.weights.replace(\".pt\", \".torchscript.pt\")  # filename\n        ts = torch.jit.trace(model, im, strict=False)  # trace\n        extra_files = {\"config.txt\": f\"{model.yaml}\"}  # extra files\n        if optimize:\n            ts = torch.jit.optimize_for_mobile(ts)\n            extra_files[\"model.json\"] = f\"{model.model.json()}\"  # extra files\n        print(f\"{prefix} export success, saving to {f}...\")\n        ts.save(f, _extra_files=extra_files)\n        print(f\"{prefix} export success: {f} ({file_size(f):.1f} MB)\")\n        return f, None\n    except Exception as e:\n        print(f\"{prefix} export failure: {e}\")\n        return None, e\n\n", "CodeBleu_score": 53.32318708092411}, {"code": "def export_torchscript(model, im, file, optimize, prefix=colorstr(\"TorchScript:\"))\n\ttry:\n        print(f\"{prefix} starting export with torch {torch.__version__}...\")\n        f = opt.weights.replace(\".pt\", \".torchscript.pt\")  # filename\n        ts = torch.jit.trace(model, im, strict=False)\n        extra_files = {\"config.txt\": \"\", \"names.txt\": \"\"}  # extra files\n        ts.save(f, _extra_files=extra_files)\n        print(f\"{prefix} export success, saved as {f}\")\n        return f, None\n    except Exception as e:\n        print(f\"{prefix} export failure: {e}\")\n        return None, e\n\n", "CodeBleu_score": 48.86016648133694}, {"code": "def export_torchscript(model, im, file, optimize, prefix=colorstr(\"TorchScript:\"))\n\ttry:\n        LOGGER.info(f\"{prefix} starting export with torch {torch.__version__}...\")\n        f = str(file).lower()\n        if f.endswith(\".pt\"):\n            file = Path(file).with_suffix(\".torchscript.pt\")\n        else:\n            file = Path(file).with_suffix(\".torchscript.pth\")\n        model.model[-1].export = not optimize  # set Detect() layer export=True\n        if optimize:\n            LOGGER.info(f\"{prefix} saving optimized model\")\n            torch.jit.optimized_execution(True)\n            model.eval()\n            with torch.no_grad():\n                model = torch.jit.trace(model, im, strict=False)  # trace model\n        else:\n            LOGGER.info(f\"{prefix} saving unoptimized model\")\n            model.eval()\n            model = torch.jit.script(model)\n\n        LOGGER.info(f\"{prefix} saving {file}\")\n        with open(file, \"wb\") as f:\n            torch.jit.save(model, f)\n\n        LOGGER.info(f\"{prefix} export success, saved to {file}\")\n        return file, None\n    except Exception as e:\n        LOGGER.info(f\"{prefix} export failure: {e}\")\n        return None, e\n\n", "CodeBleu_score": 46.52880772980619}]}
{"instance_id": "78335487ac1c", "generation_result": [{"code": "def export_coreml(model, im, file, int8, half, nms, prefix=colorstr(\"CoreML:\")):\n\ttry:\n        import coremltools as ct\n    except ImportError:\n        raise ImportError(\"CoreML export requires 'coremltools>=4.0' to be installed. Run 'pip install coremltools>=4.0'\")\n\n    if half:\n        im = im.half()  # for more precise half-precision export\n    im = im.cpu()  # to CPU\n    model.eval()\n    traced = torch.jit.trace(model, [im], strict=False)  # run once\n    model = torch.jit.freeze(traced)  # FIXME https://github.com/ultralytics/yolov5/issues/406\n    jit = str(model).replace(\"nn.Sequential\", \"nn.ModuleList\")\n    jit = re.sub(r\"(\\w+).to(\\([\\w\\s,]+\\))\", r\"\\1\", jit)  # remove .to()\n    if \"Half\" in jit:\n        print(\"Warning: CoreML export does not support torch.Half tensors, using torch.FloatTensor instead.\")\n        jit = re.sub(r\"HalfTensor\", \"FloatTensor\", jit)\n    if \"__constants__ = \" in jit:\n        jit = re.sub(r\"__constants__ = .*?\\]\", r\"__constants__ = []\", jit)\n    if int8:\n        print(f\"{prefix} running model to automatically determine quantization ranges...\")\n        model = torch.quantization.quantize_dynamic(model, {torch.nn.Linear}, dtype=torch.qint8)\n        print(f\"{prefix} exporting quantized model to CoreML...\")\n    if nms:\n        from models.export import iOSModel\n\n        model = iOSModel(model, nms)\n    print(f\"{prefix} exporting with torch 1.8.1...\")\n    mlmodel = ct.convert(model, inputs=[ct.ImageType(shape=im.shape, bias=[-255, -255, -255], scale", "CodeBleu_score": 41.61084801985214}, {"code": "def export_coreml(model, im, file, int8, half, nms, prefix=colorstr(\"CoreML:\")):\n\ttry:\n        import coremltools as ct\n    except ImportError:\n        raise ImportError(f\"CoreML export requires coremltools. Install with `pip install coremltools`.\")\n    suffix = \".mlmodel\"\n    file = Path(file).with_suffix(suffix)\n\n    if isinstance(im, torch.Tensor):\n        im = im.shape[-2:]  # [C,H,W]\n    # Checks\n    assert isinstance(model, nn.Module), f\"{prefix} export requires a PyTorch model, not {type(model)}\"\n    assert im.max() <= 255, f\"{prefix} export requires 8-bit integer images, not 32-bit float images\"\n    assert im.shape[0] < 512, f\"{prefix} export requires input images <= 512 pixels\"\n    assert len(next(model.parameters()).shape) == 4, f\"{prefix} export requires 4D weights, not 2D or 5D\"\n    if int8:\n        assert not half, f\"{prefix} export with int8 must not set half=True\"\n        assert (\n            type(model) in (nn.Sequential, Detect)\n        ), f\"{prefix} export with int8 must use a model of type nn.Sequential or Detect, not {type(model)}\"\n    # Wrap model\n    if half:\n        model = model.half()  # to FP16\n    if int8 or half:\n        from models.common import iOSModel\n\n        model = iOSModel(model, nc=nc, anchors=getattr(model, \"anchors\", None))  # convert\n    # Trace model\n    with set_logging():\n        traced_model = (\n            export_torchscript(model, im, file, optimize=False)\n            if half\n            else trace_model(model, im, file, optimize=False)\n        )\n    # Convert to CoreML\n    mlmodel = ct.convert(\n        traced_model,\n        inputs=[ct.TensorType(shape=im, dtype=np", "CodeBleu_score": 34.448991081294686}, {"code": "def export_coreml(model, im, file, int8, half, nms, prefix=colorstr(\"CoreML:\")):\n\ttry:\n        import coremltools as ct\n    except ImportError:\n        raise ImportError(\"CoreML export requires 'coremltools' package. Install with 'pip install coremltools'.\")\n\n    suffix = \".mlmodel\"\n    file = Path(file).with_suffix(suffix)\n    LOGGER.info(f\"{prefix} exporting {file}...\")\n    mlmodel = ct.convert(\n        traced_model(model, im, int8=int8, half=half),\n        inputs=[ct.ImageType(shape=im.shape, scale=1 / 255.0, bias=[0, 0, 0], color_layout=\"RGB\")],\n    )\n    if nms:\n        mlmodel = ct.models.model.MLModel(IOSModel(mlmodel, nms=True))\n    mlmodel.save(file)\n    LOGGER.info(f\"{prefix} saved {file}\")\n    return file, mlmodel\n\n", "CodeBleu_score": 37.5647516845779}]}
{"instance_id": "80233cf71580", "generation_result": [{"code": "def generate_individual(input_ranges, individual_length)\n\tindividual = []\n    for i in range(individual_length):\n        individual.append(random.uniform(input_ranges[i][0], input_ranges[i][1]))\n    return individual\n\n", "CodeBleu_score": 46.97407887252343}, {"code": "def generate_individual(input_ranges, individual_length)\n\tindividual = []\n    for i in range(individual_length):\n        individual.append(random.uniform(input_ranges[i][0], input_ranges[i][1]))\n    return individual\n\n", "CodeBleu_score": 46.97407887252343}, {"code": "def generate_individual(input_ranges, individual_length)\n\tindividual = []\n    for i in range(individual_length):\n        individual.append(random.uniform(input_ranges[i][0], input_ranges[i][1]))\n    return individual\n\n", "CodeBleu_score": 46.97407887252343}]}
{"instance_id": "5ea979cb4a8e", "generation_result": [{"code": "def marginal_log_mean_coeff(self, t)\n\tif self.schedule == \"discrete\":\n        return interpolate_fn(t, self.t_label, self.log_alpha_label)\n    elif self.schedule == \"linear\":\n        return (\n            interpolate_fn(t, self.t_label, self.log_alpha_label)\n            - (t - self.t_label) ** 2 / (2 * self.sigma ** 2)\n        )\n    elif self.schedule == \"cosine\":\n        return (\n            interpolate_fn(t, self.t_label, self.log_alpha_label)\n            - torch.log(\n                torch.cos(\n                    torch.acos(\n                        torch.clamp(\n                            (t - self.t_label) / self.sigma,\n                            min=-1.0 + 1e-6,\n                            max=1.0 - 1e-6,\n                        )\n                    )\n                    / 2\n                )\n                + 1e-6\n            )\n            + torch.log(self.sigma)\n        )\n    else:\n        raise ValueError(\"schedule must be one of ['discrete', 'linear', 'cosine']\")\n\n", "CodeBleu_score": 29.919286479898123}, {"code": "def marginal_log_mean_coeff(self, t)\n\tif self.schedule_type == \"discrete\":\n        return interpolate_fn(t, self.schedule_timesteps, self.schedule_coeffs.log())\n    elif self.schedule_type == \"linear\":\n        return (\n            interpolate_fn(t, self.schedule_timesteps, self.schedule_coeffs.log())\n            - interpolate_fn(t, self.schedule_timesteps, self.schedule_coeffs.log()) ** 2\n        )\n    elif self.schedule_type == \"cosine\":\n        return (\n            interpolate_fn(t, self.schedule_timesteps, self.schedule_coeffs.log())\n            * torch.cos(t * np.pi / self.schedule_timesteps[-1])\n        )\n    else:\n        raise NotImplementedError(\n            \"schedule_type must be one of ['discrete', 'linear', 'cosine']\"\n        )\n\n", "CodeBleu_score": 29.481723896220156}, {"code": "def marginal_log_mean_coeff(self, t)\n\tif self.schedule_type == \"discrete\":\n        return interpolate_fn(t, self.t_knots, self.log_alpha_knots)\n    elif self.schedule_type == \"linear\":\n        return (\n            interpolate_fn(t, self.t_knots, self.log_alpha_knots)\n            + (t - self.t_knots) * self.log_alpha_grads\n        )\n    elif self.schedule_type == \"cosine\":\n        return (\n            interpolate_fn(t, self.t_knots, self.log_alpha_knots)\n            + (t - self.t_knots) * self.log_alpha_grads\n            + (\n                (1.0 - torch.cos(np.pi * (t - self.t_knots) / (self.t_knots[-1] - self.t_knots[0])))\n                * self.log_alpha_hessians\n            )\n        )\n    else:\n        raise NotImplementedError(\"Schedule type not implemented.\")\n\n", "CodeBleu_score": 30.955521332208335}]}
{"instance_id": "7da4fae5a278", "generation_result": [{"code": "def inverse_lambda(self, lamb)\n\tif self.schedule == \"linear\":\n        t = self.beta_0 + self.beta_1 * lamb\n    elif self.schedule == \"discrete\":\n        t = interpolate_fn(lamb, self.log_alpha, self.t_values)\n    else:\n        log_alpha = torch.log(torch.exp(lamb) + 1)\n        t = torch.acos(torch.cos(log_alpha) / torch.cos(self.log_alpha_max)) / torch.pi\n    return t\n\n", "CodeBleu_score": 18.951983465991944}, {"code": "def inverse_lambda(self, lamb)\n\tif self.schedule == \"linear\":\n        return self.beta * (1 - torch.exp(-lamb))\n    elif self.schedule == \"discrete\":\n        return interpolate_fn(lamb, self.log_alpha, self.log_alpha_t)\n    elif self.schedule == \"logarithmic\":\n        return torch.acos(torch.exp(-lamb)) / np.pi\n    else:\n        raise NotImplementedError(f\"schedule {self.schedule} not implemented\")\n\n", "CodeBleu_score": 12.762303124572608}, {"code": "def inverse_lambda(self, lamb)\n\tif self.schedule == \"linear\":\n        # linear schedule\n        if self.beta_schedule == \"constant\":\n            t = self.T * lamb / self.beta\n        elif self.beta_schedule == \"linear\":\n            t = self.T * lamb / (self.beta + self.beta_start * (1 - lamb))\n        elif self.beta_schedule == \"quadratic\":\n            t = self.T * lamb / (self.beta + self.beta_start * (1 - lamb) ** 2)\n        elif self.beta_schedule == \"cubic\":\n            t = self.T * lamb / (self.beta + self.beta_start * (1 - lamb) ** 3)\n        else:\n            raise ValueError(\"Invalid beta_schedule: \" + self.beta_schedule)\n    elif self.schedule == \"discrete\":\n        # discrete schedule\n        xp = torch.tensor([0, 1], device=lamb.device)\n        yp = torch.tensor([self.beta_start, self.beta], device=lamb.device)\n        log_alpha = interpolate_fn(lamb, xp, yp)\n        t = torch.acos(1 - log_alpha) / math.pi\n    else:\n        # other schedules\n        log_alpha = 1 - torch.cos(lamb * math.pi)\n        t = torch.acos(1 - log_alpha) / math.pi\n    return t\n\n", "CodeBleu_score": 30.328953798340326}]}
{"instance_id": "a519c853a2b2", "generation_result": [{"code": "def noise_pred_fn(x, t_continuous, cond=None)\n\t# expand the time input to match the batch size\n    t_continuous = expand_dims(t_continuous, x.shape[1])\n    # process the input through the model\n    if cond is not None:\n        x = torch.cat((x, cond), 1)\n    x = model(x)\n    # process the model output\n    if model_type == 'linear':\n        # linear model\n        x = x.squeeze()\n    elif model_type == 'nonlinear':\n        # non-linear model\n        x = x.tanh()\n    elif model_type == 'resnet':\n        # resnet model\n        x = x.tanh()\n    elif model_type == 'resnet_fc':\n        # resnet model with linear output\n        x = x.squeeze()\n    else:\n        raise ValueError(f'Model type {model_type} not supported.')\n    # apply noise schedule parameters\n    if noise_schedule == 'linear':\n        # linear noise schedule\n        x = noise_schedule.marginal_alpha(t_continuous) * x\n    elif noise_schedule == 'nonlinear':\n        # non-linear noise schedule\n        x = noise_schedule.marginal_alpha(t_continuous) * x\n    elif noise_schedule == 'resnet':\n        # resnet noise schedule\n        x = noise_schedule.marginal_alpha(t_continuous) * x\n    elif noise_schedule == 'resnet_fc':\n        # resnet noise schedule with linear output\n        x = noise_schedule.marginal_alpha(t_continuous) * x\n    else:\n        raise ValueError(f'Noise schedule {noise_schedule} not supported.')\n    return x\n\n", "CodeBleu_score": 35.239972701535685}, {"code": "def noise_pred_fn(x, t_continuous, cond=None)\n\tif cond is None:\n        cond = x\n    # expand time to match batch size\n    t_continuous = expand_dims(t_continuous, x.ndim)\n    # compute noise prediction\n    if args.model_type == 'linear':\n        pred_noise = x\n    elif args.model_type == 'nonlinear':\n        pred_noise = model(x, t_continuous)\n    elif args.model_type == 'resnet':\n        pred_noise = model(x, t_continuous)\n    elif args.model_type == 'wavenet':\n        pred_noise = model(x, t_continuous)\n    elif args.model_type == 'transformer':\n        pred_noise = model(x, t_continuous)\n    else:\n        raise NotImplementedError\n\n    # compute prediction of alpha and sigma\n    if args.model_type in ['linear', 'nonlinear', 'resnet']:\n        alpha = pred_noise\n        sigma = noise_schedule.marginal_std(t_continuous)\n    elif args.model_type == 'wavenet':\n        alpha = pred_noise[:, :, :, 0]\n        sigma = noise_schedule.marginal_std(t_continuous)\n    elif args.model_type == 'transformer':\n        alpha = pred_noise[:, :, :, 0]\n        sigma = noise_schedule.marginal_std(t_continuous)\n    else:\n        raise NotImplementedError\n\n    # compute prediction of x\n    if args.model_type in ['linear', 'nonlinear', 'resnet']:\n        x_pred = alpha * x + sigma * torch.randn_like(x)\n    elif args.model_type == 'wavenet':\n        x_pred = alpha * x + sigma * torch.randn_like(x)\n    elif args.model_type == 'transformer':\n        x_pred = alpha * x + sigma * torch.randn_like(x)\n    else:\n        raise NotImplementedError", "CodeBleu_score": 31.68429632676358}, {"code": "def noise_pred_fn(x, t_continuous, cond=None)\n\tif cond is None:\n        cond = x\n    t = t_continuous * noise_schedule.t_scale\n    t = expand_dims(t, x.ndim)\n    if model_type == \"linear\":\n        return noise_schedule.marginal_alpha(t) * x + noise_schedule.marginal_std(t) * cond\n    elif model_type == \"nonlinear\":\n        return noise_schedule.marginal_alpha(t) * x + noise_schedule.marginal_std(t) * torch.tanh(cond)\n    elif model_type == \"transformer\":\n        return noise_schedule.marginal_alpha(t) * x + noise_schedule.marginal_std(t) * torch.tanh(cond)\n    else:\n        raise NotImplementedError\n\n", "CodeBleu_score": 21.05432261586903}]}
{"instance_id": "96a9b7d54823", "generation_result": [{"code": "def dynamic_thresholding_fn(self, x0, t=None)\n\tx0 = x0.detach().clone()\n    x0 = x0.flatten()\n    x0 = x0.abs()\n    x0 = torch.sort(x0, descending=True)[0]\n    if t is None:\n        t = self.t\n    q = x0[int(x0.numel() * t)]\n    x0 = x0 / q\n    x0[x0 > 1.0] = 1.0\n    x0 = x0 * q\n    x0 = x0.reshape(self.shape)\n    return x0\n\n", "CodeBleu_score": 22.416286077735208}, {"code": "def dynamic_thresholding_fn(self, x0, t=None)\n\tif t is None:\n        t = self.threshold\n    if t == 0:\n        return x0\n    x0 = x0.clone()\n    x0_shape = x0.shape\n    x0_abs = torch.abs(x0)\n    x0_abs_flat = x0_abs.flatten()\n    x0_abs_flat_sorted, _ = torch.sort(x0_abs_flat)\n    x0_abs_flat_sorted = x0_abs_flat_sorted[1:]\n    x0_abs_flat_sorted = x0_abs_flat_sorted[x0_abs_flat_sorted != 0]\n    if x0_abs_flat_sorted.numel() == 0:\n        return x0\n    x0_abs_flat_sorted_quantile = x0_abs_flat_sorted[int(t * x0_abs_flat_sorted.numel())]\n    scaling_factor = x0_abs_flat_sorted_quantile / t\n    x0[x0_abs > scaling_factor] = torch.sign(x0[x0_abs > scaling_factor]) * scaling_factor\n    x0 = x0 / scaling_factor\n    x0 = x0.view(x0_shape)\n    return x0\n\n", "CodeBleu_score": 32.63125142408493}, {"code": "def dynamic_thresholding_fn(self, x0, t=None)\n\tx0_shape = x0.shape\n    x0_flat = x0.flatten()\n    x0_flat_abs = torch.abs(x0_flat)\n    x0_flat_abs_sorted, _ = torch.sort(x0_flat_abs)\n    x0_flat_abs_sorted_cumsum = torch.cumsum(x0_flat_abs_sorted, dim=0)\n    x0_flat_abs_sorted_cumsum_div = x0_flat_abs_sorted_cumsum / x0_flat_abs_sorted_cumsum[-1]\n    x0_flat_abs_sorted_cumsum_div_t = expand_dims(x0_flat_abs_sorted_cumsum_div, dims=x0_shape.numel())\n    x0_flat_abs_sorted_cumsum_div_t_gt = torch.gt(x0_flat_abs_sorted_cumsum_div_t, t)\n    x0_flat_abs_sorted_cumsum_div_t_gt_float = x0_flat_abs_sorted_cumsum_div_t_gt.to(torch.float32)\n    x0_flat_abs_sorted_cumsum_div_t_gt_float_cumsum = torch.cumsum(x0_flat_abs_sorted_cumsum_div_t_gt_float, dim=0)\n    x0_flat_abs_sorted_cumsum_div_t_gt_float_cumsum_div = x0_flat_abs_sorted_cumsum_div_t_gt_float_cumsum / x0_flat_abs_sorted_cumsum_div_t_gt_float_cumsum[-1]\n    x0_flat_abs_sorted_cumsum_div_t_gt_float_cumsum_div_t = expand_dims(x0_flat_abs_sorted_cumsum_div_t_gt_float_cumsum_div, dims=x0_shape.numel())\n    x0", "CodeBleu_score": 15.72726390353281}]}
{"instance_id": "59c6ec6d7a6a", "generation_result": [{"code": "def data_prediction_fn(self, x, t)\n\treturn self.data_prediction_fn(x, t)\n\n", "CodeBleu_score": 1.769989109671654}, {"code": "def data_prediction_fn(self, x, t)\n\treturn self.model(x, t)\n", "CodeBleu_score": 1.7699693687413316}, {"code": "def data_prediction_fn(self, x, t)\n\tif self.dynamic_thresholding:\n        if self.dynamic_thresholding_ratio == 1.0:\n            threshold = self.max_threshold\n        else:\n            threshold = torch.max(x) * self.dynamic_thresholding_ratio\n        x = torch.clamp(x, max=threshold)\n        x = x / torch.max(x)\n    return x\n", "CodeBleu_score": 17.24940517444427}]}
{"instance_id": "65e3618468a2", "generation_result": [{"code": "def predict_eps_sigma(model, input, sigma_in, **kwargs)\n\t# Reshape sigma\n    sigma = sigma_in.reshape(input.shape)\n    # Scale input\n    scaled_input = scale(input, sigma)\n    # Predict epsilon\n    prediction = model(scaled_input, **kwargs)\n    # Adjust prediction\n    adjusted_prediction = unscale(prediction, sigma)\n    return adjusted_prediction\n\n", "CodeBleu_score": 18.50937031253229}, {"code": "def predict_eps_sigma(model, input, sigma_in, **kwargs)\n\tsigma = sigma_in.reshape(input.shape)\n    scaled_input = input / sigma\n    prediction = model(scaled_input, **kwargs)\n    return prediction * sigma\n\n\n", "CodeBleu_score": 19.3394070763781}, {"code": "def predict_eps_sigma(model, input, sigma_in, **kwargs)\n\t# Reshape sigma to match the dimensions of the input\n    sigma = sigma_in.reshape(input.shape)\n\n    # Scale the input\n    input_scaled = input * sigma\n\n    # Predict the output\n    output_scaled = model.predict(input_scaled, **kwargs)\n\n    # Adjust the prediction by scaling it with the input and sigma\n    output = output_scaled / (input * sigma)\n\n    return output\n\n", "CodeBleu_score": 28.092967313030904}]}
{"instance_id": "b35846dfbc66", "generation_result": [{"code": "def cal_intergrand(beta_0, beta_1, taus)\n\ttaus = taus.clone().requires_grad_(True)\n    alpha = t2alpha_fn(beta_0, beta_1, taus)\n    log_alpha = alpha.log()\n    log_alpha.backward()\n    return (log_alpha * alpha).grad\n\n", "CodeBleu_score": 13.405659208022541}, {"code": "def cal_intergrand(beta_0, beta_1, taus)\n\ttaus_clone = taus.clone().detach().requires_grad_(True)\n    alpha = t2alpha_fn(beta_0, beta_1, taus_clone)\n    log_alpha = alpha.log()\n    log_alpha.backward()\n    grad_log_alpha = taus_clone.grad\n    return grad_log_alpha * alpha\n\n", "CodeBleu_score": 15.931989506716365}, {"code": "def cal_intergrand(beta_0, beta_1, taus)\n\ttaus_ = taus.clone().requires_grad_(True)\n    alphas = t2alpha_fn(beta_0, beta_1, taus_)\n    log_alphas = alphas.log()\n    log_alphas.backward(torch.ones_like(taus_))\n    grad_taus = taus_.grad.clone()\n    integrand = log_alphas + grad_taus * taus_\n    return integrand\n\n", "CodeBleu_score": 18.335830001167537}]}
{"instance_id": "5f3667566a10", "generation_result": [{"code": "def sample_euler_ancestral(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None)\n\tif extra_args is None:\n        extra_args = {}\n    if noise_sampler is None:\n        noise_sampler = default_noise_sampler(x)\n    if disable is None:\n        disable = []\n    sigma_from, sigma_to = sigmas[0], sigmas[-1]\n    x_shape = x.shape\n    x = x.view(-1, *x.shape[1:])\n    if sigma_from == 0.:\n        x_noise = x\n    else:\n        x_noise = noise_sampler(sigma_from, sigma_to)(x.shape)\n    x_noise = x_noise.view(-1, *x.shape[1:])\n    x_noise.requires_grad_()\n    x_noise_grad = None\n    for sigma_i, sigma_next in zip(sigmas[:-1], sigmas[1:]):\n        if sigma_i not in disable:\n            x_denoised = model(x_noise, sigma_i, **extra_args)\n            sigma_down, sigma_up = get_ancestral_step(sigma_i, sigma_next, eta)\n            x_noise_grad = to_d(x_noise, sigma_i, x_denoised)\n            x_noise = x_noise - sigma_down * x_noise_grad\n            x_noise = x_noise + sigma_up * noise_sampler(sigma_i, sigma_next)(x_noise.shape)\n        if callback is not None:\n            callback(sigma_i, x_noise, x_noise_grad, x_denoised)\n    if s_noise > 0.:\n        x_noise = x_noise + s_noise * noise_sampler(sigma_to, 0.)(x_noise.shape)\n    return x_noise.view(*x_shape)\n\n", "CodeBleu_score": 33.87268410360472}, {"code": "def sample_euler_ancestral(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None)\n\tif extra_args is None:\n        extra_args = {}\n    if noise_sampler is None:\n        noise_sampler = default_noise_sampler(x)\n    x_shape = x.shape\n    x = x.reshape(-1, *x.shape[-3:])\n    sigmas = sigmas.reshape(-1, *sigmas.shape[-3:])\n    if disable is None:\n        disable = [False] * len(sigmas)\n    if len(disable) != len(sigmas):\n        raise ValueError(\"disable and sigmas must have the same length.\")\n    x_noise = noise_sampler(x, sigmas[0])\n    x = x + x_noise\n    for i, sigma in enumerate(sigmas):\n        if disable[i]:\n            continue\n        x = x.reshape(-1, *x.shape[-3:])\n        x_shape = x.shape\n        x_denoised = model(x, sigma, **extra_args).reshape(-1, *x.shape[-3:])\n        sigma_down, sigma_up = get_ancestral_step(sigma, sigmas[i + 1], eta)\n        x = x - sigma_up * to_d(x, sigma, x_denoised)\n        if callback is not None:\n            callback(x, sigma, x_denoised)\n    if s_noise:\n        x_noise = noise_sampler(x, sigmas[-1])\n        x = x + x_noise\n    return x.reshape(x_shape)\n\n", "CodeBleu_score": 40.72123529026795}, {"code": "def sample_euler_ancestral(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None)\n\tif extra_args is None:\n        extra_args = {}\n    if noise_sampler is None:\n        noise_sampler = default_noise_sampler(x)\n    if disable is None:\n        disable = []\n    if callback is None:\n        callback = lambda x, sigma, extra_args: None\n    sigma = sigmas[0]\n    x = x.detach().clone()\n    for sigma_next in sigmas[1:]:\n        with torch.no_grad():\n            x_denoised, extra_args = model(x, sigma, **extra_args)\n            sigma_down, sigma_up = get_ancestral_step(sigma, sigma_next, eta=eta)\n            if 'sigma' not in disable:\n                x = x - to_d(x, sigma, x_denoised) * sigma_down ** 2 / 2\n            if 'noise' not in disable:\n                x = x + noise_sampler(sigma_up, sigma_down) * s_noise\n            x = x.detach()\n            callback(x, sigma_next, extra_args)\n            sigma = sigma_next\n    return x\n\n", "CodeBleu_score": 38.266873737997166}]}
{"instance_id": "6261ded88df2", "generation_result": [{"code": "def linear_multistep_coeff(order, t, i, j)\n\tdef product_term(x):\n        \"\"\"The function computes the product term for the coefficient, excluding the j-th term.\"\"\"\n        prod = 1\n        for k in range(order):\n            if k != j:\n                prod *= (x - t[k]) / (t[i] - t[k])\n        return prod\n\n    if order > i:\n        raise ValueError(\"Order too high for step index.\")\n    else:\n        return integrate.quad(product_term, t[i], t[i + 1])[0]\n\n", "CodeBleu_score": 42.98463951900596}, {"code": "def linear_multistep_coeff(order, t, i, j)\n\tdef product(k):\n        \"\"\"The function computes the product term for the linear multistep coefficient, excluding the j-th term.\"\"\"\n        if k != j:\n            return (t[i+1] - t[k])/(t[i+1] - t[j])\n        else:\n            return 0\n    if i+order < j:\n        raise ValueError(\"The order is too high for the step index.\")\n    elif i+order == j:\n        return 1\n    else:\n        return sum([product(k) for k in range(i+1, j+1)])\n\n\n", "CodeBleu_score": 29.63052729721978}, {"code": "def linear_multistep_coeff(order, t, i, j)\n\tdef product(k):\n        \"\"\"The nested function computes the product term for the coefficient, excluding the j-th term.\"\"\"\n        return (t[k] - t[i]) / (t[j] - t[i])\n\n    if j - i > order:\n        raise ValueError('The order is too high for the step index')\n    return integrate.quad(product, t[i], t[j])[0]\n\n", "CodeBleu_score": 20.32176084098176}]}
{"instance_id": "4ee025e33ab1", "generation_result": [{"code": "def sample_lcm(model, x, sigmas, extra_args=None, callback=None, disable=None, noise_sampler=None)\n\tif extra_args is None:\n        extra_args = {}\n    if noise_sampler is None:\n        noise_sampler = default_noise_sampler(x)\n\n    for i, sigma in enumerate(sigmas):\n        x_next = model(x, sigma, **extra_args)\n        if callback is not None:\n            callback(x_next, i, sigma, extra_args)\n        if disable is not None and disable(x_next, i, sigma, extra_args):\n            break\n        if sigma_next > 0:\n            noise = noise_sampler(x, sigma_next)\n            x = x_next + noise\n        else:\n            x = x_next\n    return x\n\n", "CodeBleu_score": 30.42284640659511}, {"code": "def sample_lcm(model, x, sigmas, extra_args=None, callback=None, disable=None, noise_sampler=None)\n\tif extra_args is None:\n        extra_args = {}\n    if noise_sampler is None:\n        noise_sampler = default_noise_sampler(x)\n    if disable is None:\n        disable = []\n    for i, sigma in enumerate(sigmas):\n        x = model(x, sigma, **extra_args)\n        if callback is not None:\n            callback(x, i, sigma)\n        if i < len(sigmas) - 1 and sigmas[i + 1] > 0:\n            noise = noise_sampler(x, sigmas[i + 1])\n            x = x + sigmas[i + 1] * noise\n    return x", "CodeBleu_score": 35.008248972556146}, {"code": "def sample_lcm(model, x, sigmas, extra_args=None, callback=None, disable=None, noise_sampler=None)\n\tif extra_args is None:\n        extra_args = {}\n    if disable is None:\n        disable = []\n    if noise_sampler is None:\n        noise_sampler = default_noise_sampler(x)\n    for i, sigma in enumerate(sigmas):\n        if callback is not None:\n            callback(i, sigma)\n        if sigma > 0:\n            x_noise = noise_sampler(sigma, sigmas[i + 1])\n            x = model(x, x_noise, **extra_args)\n        else:\n            x = model(x, **extra_args)\n        if 'noise_scale' not in disable:\n            x = x * sigma\n    return x\n\n", "CodeBleu_score": 35.02519666212008}]}
{"instance_id": "cb51f91da606", "generation_result": [{"code": "def append_dims(x, target_dims)\n\tif x.dim() >= target_dims:\n        raise ValueError(\"Target dimensions must be greater than the current dimensions.\")\n    dims_to_append = target_dims - x.dim()\n    if x.device.type == \"mps\":\n        x = x.detach().clone()\n    else:\n        x = x.clone()\n    for i in range(dims_to_append):\n        x = x.unsqueeze(-1)\n    return x\n\n", "CodeBleu_score": 40.12468471476098}, {"code": "def append_dims(x, target_dims)\n\tif len(x.shape) > target_dims:\n        raise ValueError(\n            f\"The target dimensions ({target_dims}) must be greater than the current dimensions ({len(x.shape)})\")\n\n    if x.device.type == \"mps\":\n        x = x.detach().clone()\n    else:\n        x = x.clone()\n\n    x = x.expand(*x.shape, *([1] * (target_dims - len(x.shape))))\n\n    return x\n\n", "CodeBleu_score": 38.464373976251366}, {"code": "def append_dims(x, target_dims)\n\tif len(x.shape) > target_dims:\n        raise ValueError(f\"The target dimensions ({target_dims}) must be greater than or equal to the current dimensions ({len(x.shape)})\")\n    if len(x.shape) == target_dims:\n        return x.clone().detach()\n    else:\n        return x.expand(*x.shape, *([1]*(target_dims-len(x.shape))))\n\n", "CodeBleu_score": 23.633305172811507}]}
{"instance_id": "9fac874a3bc7", "generation_result": [{"code": "def download_file(path, url, digest=None)\n\timport os\n    import hashlib\n    import urllib.request\n    from tqdm.auto import tqdm\n\n    if os.path.exists(path):\n        print(f\"File already exists at {path}\")\n        return path\n\n    print(f\"Downloading {url} to {path}\")\n\n    os.makedirs(os.path.dirname(path), exist_ok=True)\n\n    with urllib.request.urlopen(url) as response, open(path, \"wb\") as file:\n        with tqdm(total=int(response.info()[\"Content-Length\"]), unit=\"B\", unit_scale=True) as progress:\n            for chunk in iter(lambda: response.read(4096), b\"\"):\n                file.write(chunk)\n                progress.update(len(chunk))\n\n    if digest is not None:\n        print(\"Verifying hash\")\n        digest_algorithm = digest[:64].lower()\n        hasher = hashlib.new(digest_algorithm)\n        with open(path, \"rb\") as file:\n            while chunk := file.read(4096):\n                hasher.update(chunk)\n        if digest[64:] != hasher.hexdigest():\n            raise ValueError(f\"File at {path} has incorrect hash\")\n\n    return path\n\n", "CodeBleu_score": 45.45143000541565}, {"code": "def download_file(path, url, digest=None)\n\timport os\n    import hashlib\n    import requests\n    import tqdm\n    import logging\n    import sys\n    import pathlib\n\n    logger = logging.getLogger(__name__)\n\n    if not os.path.exists(path):\n        os.makedirs(path)\n\n    filename = os.path.join(path, os.path.basename(url))\n\n    if not os.path.isfile(filename):\n        logger.info(\"Downloading file from {}\".format(url))\n        r = requests.get(url, stream=True)\n        total_size = int(r.headers.get('content-length', 0))\n        block_size = 1024\n        t = tqdm.tqdm(total=total_size, unit='iB', unit_scale=True)\n        with open(filename, 'wb') as f:\n            for data in r.iter_content(block_size):\n                t.update(len(data))\n                f.write(data)\n        t.close()\n        if total_size != 0 and t.n != total_size:\n            raise RuntimeError(\"Error downloading file from {}\".format(url))\n        else:\n            logger.info(\"Successfully downloaded file from {}\".format(url))\n\n    if digest is not None:\n        logger.info(\"Verifying hash of downloaded file\")\n        h = hashlib.sha256()\n        with open(filename, 'rb') as f:\n            for block in iter(lambda: f.read(h.block_size), b''):\n                h.update(block)\n        if h.hexdigest() != digest:\n            raise RuntimeError(\"Digest of downloaded file does not match expected value\")\n        else:\n            logger.info(\"Successfully verified hash of downloaded file\")\n\n    return filename\n\n", "CodeBleu_score": 40.26672787592831}, {"code": "def download_file(path, url, digest=None)\n\timport os\n    import requests\n    import hashlib\n    import tqdm\n\n    if os.path.exists(path):\n        return path\n\n    os.makedirs(os.path.dirname(path), exist_ok=True)\n\n    response = requests.get(url, stream=True)\n    response.raise_for_status()\n\n    with open(path, 'wb') as file:\n        with tqdm.tqdm(total=int(response.headers.get('content-length')), unit='B', unit_scale=True) as progress:\n            for chunk in response.iter_content(chunk_size=1024):\n                file.write(chunk)\n                progress.update(len(chunk))\n\n    if digest is not None:\n        with open(path, 'rb') as file:\n            actual_digest = hashlib.sha256(file.read()).hexdigest()\n        if digest != actual_digest:\n            raise RuntimeError('Digest mismatch!')\n\n    return path\n\n", "CodeBleu_score": 43.59753212988981}]}
{"instance_id": "f23165e897ce", "generation_result": [{"code": "def train_mode(model, mode=True)\n\treturn model.train(mode)\n", "CodeBleu_score": 3.2536385283412086}, {"code": "def train_mode(model, mode=True)\n\tif mode:\n        model.train()\n    else:\n        model.eval()\n    return model", "CodeBleu_score": 14.080780715645588}, {"code": "def train_mode(model, mode=True)\n\tif mode:\n        return model.train()\n    else:\n        return model.eval()\n\n", "CodeBleu_score": 13.369412496655281}]}
{"instance_id": "c6d59c6cb8e1", "generation_result": [{"code": "def ema_update(model, averaged_model, decay)\n\tfor averaged_param, param in zip(averaged_model.parameters(), model.parameters()):\n        averaged_param.data = decay * averaged_param.data + (1 - decay) * param.data\n    for averaged_buffer, buffer in zip(averaged_model.buffers(), model.buffers()):\n        averaged_buffer.data.copy_(buffer.data)\n\n", "CodeBleu_score": 29.59065565433672}, {"code": "def ema_update(model, averaged_model, decay)\n\tfor ema_param, param in zip(averaged_model.parameters(), model.parameters()):\n        ema_param.data.mul_(decay).add_(1 - decay, param.data)\n    for ema_buffer, buffer in zip(averaged_model.buffers(), model.buffers()):\n        if buffer.data is not None:\n            ema_buffer.data.copy_(buffer.data)\n\n\n", "CodeBleu_score": 28.511531634991428}, {"code": "def ema_update(model, averaged_model, decay)\n\tmodel_params = list(model.parameters())\n    averaged_model_params = list(averaged_model.parameters())\n\n    if len(model_params) != len(averaged_model_params):\n        raise ValueError(\"The number of parameters of the averaged model is not the same as the number of parameters of the current model.\")\n\n    for model_param, averaged_model_param in zip(model_params, averaged_model_params):\n        averaged_model_param.data = decay * averaged_model_param.data + (1 - decay) * model_param.data\n\n    # Copy the buffers of the current model to the averaged model\n    for model_buffer, averaged_model_buffer in zip(model.buffers(), averaged_model.buffers()):\n        averaged_model_buffer.data.copy_(model_buffer.data)\n\n", "CodeBleu_score": 35.16299033277758}]}
{"instance_id": "600c39a30798", "generation_result": [{"code": "def rand_log_logistic(shape, loc=0., scale=1., min_value=0., max_value=float('inf'), device='cpu', dtype=torch.float32)\n\tmin_value, max_value = torch.tensor(min_value, device=device), torch.tensor(max_value, device=device)\n    min_value, max_value = min_value.type(dtype), max_value.type(dtype)\n    min_logit, max_logit = torch.log(min_value / loc) - scale * torch.log(min_value / loc), torch.log(max_value / loc) - scale * torch.log(max_value / loc)\n    return torch.exp(loc + scale * torch.log(torch.rand(shape, device=device, dtype=dtype) * (max_value / min_value) + min_value / max_value))\n\n\n", "CodeBleu_score": 31.371258800917246}, {"code": "def rand_log_logistic(shape, loc=0., scale=1., min_value=0., max_value=float('inf'), device='cpu', dtype=torch.float32)\n\tif min_value == 0:\n        min_value = torch.tensor(min_value, device=device, dtype=dtype)\n    else:\n        min_value = torch.tensor(min_value, device=device, dtype=dtype).log()\n    if max_value == float('inf'):\n        max_value = torch.tensor(max_value, device=device, dtype=dtype)\n    else:\n        max_value = torch.tensor(max_value, device=device, dtype=dtype).log()\n    u = torch.rand(shape, device=device, dtype=dtype)\n    return torch.exp(torch.log(u * (max_value - min_value) + min_value) / scale + loc)\n\n", "CodeBleu_score": 40.21475297762321}, {"code": "def rand_log_logistic(shape, loc=0., scale=1., min_value=0., max_value=float('inf'), device='cpu', dtype=torch.float32)\n\tif not torch.is_tensor(min_value):\n        min_value = torch.tensor(min_value, device=device, dtype=dtype)\n    if not torch.is_tensor(max_value):\n        max_value = torch.tensor(max_value, device=device, dtype=dtype)\n    min_value = min_value.to(device=device, dtype=dtype)\n    max_value = max_value.to(device=device, dtype=dtype)\n    if min_value == max_value:\n        return min_value\n    if torch.is_tensor(loc):\n        loc = loc.to(device=device, dtype=dtype)\n    else:\n        loc = torch.tensor(loc, device=device, dtype=dtype)\n    if torch.is_tensor(scale):\n        scale = scale.to(device=device, dtype=dtype)\n    else:\n        scale = torch.tensor(scale, device=device, dtype=dtype)\n    if loc.shape != shape:\n        loc = torch.broadcast_to(loc, shape)\n    if scale.shape != shape:\n        scale = torch.broadcast_to(scale, shape)\n    min_cdf = logit(torch.log(min_value) - loc)\n    max_cdf = logit(torch.log(max_value) - loc)\n    if min_cdf > max_cdf:\n        min_cdf, max_cdf = max_cdf, min_cdf\n    u = torch.rand(shape, device=device, dtype=dtype)\n    u = u * (max_cdf - min_cdf) + min_cdf\n    v = logit(u)\n    x = torch.exp(loc + scale * v)\n    return x\n\n", "CodeBleu_score": 40.76031547588208}]}
{"instance_id": "75e2def2467f", "generation_result": [{"code": "def get_activation(activation: Literal[\"elu\", \"snake\", \"none\"], antialias=False, channels=None) -> nn.Module\n\tif activation == \"elu\":\n        return nn.ELU()\n    elif activation == \"snake\":\n        return Activation1d(SnakeBeta(channels), antialias=antialias)\n    elif activation == \"none\":\n        return nn.Identity()\n    else:\n        raise ValueError(f\"Invalid activation type {activation}\")\n\n", "CodeBleu_score": 43.171687007955796}, {"code": "def get_activation(activation: Literal[\"elu\", \"snake\", \"none\"], antialias=False, channels=None) -> nn.Module\n\tif activation == \"elu\":\n        return nn.ELU()\n    elif activation == \"snake\":\n        return SnakeBeta(channels=channels)\n    elif activation == \"none\":\n        return nn.Identity()\n    else:\n        raise ValueError(\"Activation type not recognized.\")\n\n", "CodeBleu_score": 36.52378093435809}, {"code": "def get_activation(activation: Literal[\"elu\", \"snake\", \"none\"], antialias=False, channels=None) -> nn.Module\n\tif activation == \"elu\":\n        if antialias:\n            return Activation1d(nn.ELU())\n        else:\n            return nn.ELU()\n    elif activation == \"snake\":\n        if antialias:\n            return Activation1d(SnakeBeta(channels))\n        else:\n            return SnakeBeta(channels)\n    elif activation == \"none\":\n        return nn.Identity()\n    else:\n        raise ValueError(\"Activation type must be one of 'elu', 'snake', or 'none'.\")\n\n\n\n", "CodeBleu_score": 40.58611432356681}]}
{"instance_id": "ef1714c79e51", "generation_result": [{"code": "def apply_rotary_pos_emb(t, freqs, scale = 1)\n\tif t.dtype != torch.float32:\n        t = t.float()\n\n    freqs = freqs.type_as(t)\n\n    if t.dim() == 2:\n        t = t.unsqueeze(0)\n\n    if freqs.dim() == 1:\n        freqs = freqs.unsqueeze(0)\n\n    rot_dim = 1\n    offset = t.shape[rot_dim] - freqs.shape[rot_dim]\n    t = torch.cat((t[:, :offset], t[:, offset:] * freqs.cos(), t[:, offset:] * freqs.sin()), dim = rot_dim)\n    return t\n\n", "CodeBleu_score": 29.627733346016804}, {"code": "def apply_rotary_pos_emb(t, freqs, scale = 1)\n\tt = t.type_as(freqs)\n    freqs = freqs.type_as(t)\n    freqs = expand_dims_to(freqs, t.ndim - 1)\n    t, freqs = map(lambda t: rearrange(t, '... n d -> ... (n d)'), (t, freqs))\n    t, t_pass = map(lambda t: (t[..., :t.shape[-1] // 2], t[..., t.shape[-1] // 2:]), (t, t))\n    t = (t * freqs.cos()) + (rotate_half(t) * freqs.sin())\n    t = torch.cat((t, t_pass), dim = -1)\n    return t * scale\n\n", "CodeBleu_score": 27.884829420164397}, {"code": "def apply_rotary_pos_emb(t, freqs, scale = 1)\n\t# if the input tensor is not a float, cast it to one\n    if not isinstance(t, torch.cuda.FloatTensor):\n        t = t.float()\n\n    # if the frequency tensor is not a float, cast it to one\n    if not isinstance(freqs, torch.cuda.FloatTensor):\n        freqs = freqs.float()\n\n    # if the input tensor is not contiguous, make it contiguous\n    if not t.is_contiguous():\n        t = t.contiguous()\n\n    # if the frequency tensor is not contiguous, make it contiguous\n    if not freqs.is_contiguous():\n        freqs = freqs.contiguous()\n\n    # determine the shape of the input tensor\n    shape = t.shape\n\n    # determine the shape of the frequency tensor\n    freq_shape = freqs.shape\n\n    # if the input tensor is not 2-dimensional, reshape it to 2-dimensional\n    if len(shape) != 2:\n        t = t.reshape(-1, shape[-1])\n\n    # if the frequency tensor is not 2-dimensional, reshape it to 2-dimensional\n    if len(freq_shape) != 2:\n        freqs = freqs.reshape(-1, freq_shape[-1])\n\n    # if the frequency tensor is not the same shape as the input tensor, reshape it to the same shape as the input tensor\n    if freqs.shape != t.shape:\n        freqs = freqs[:, :t.shape[1]]\n\n    # if the frequency tensor is not contiguous, make it contiguous\n    if not freqs.is_contiguous():\n        freqs = freqs.contiguous()\n\n    # determine the number of elements in the input tensor\n    num_el = t.shape[1]\n\n    # if the frequency tensor is not the same size as the input tensor, repeat it until it is\n    if freqs.shape[1] != num_el:\n        freqs = freqs.", "CodeBleu_score": 31.166707581725905}]}
{"instance_id": "a7a5f178c6f6", "generation_result": [{"code": "def idx2vq(self, idx, dim=-1)\n\tvq_embeddings = self.codebook.embedding(idx)\n    if dim != -1:\n        vq_embeddings = vq_embeddings.movedim(-1, dim)\n    return vq_embeddings\n\n", "CodeBleu_score": 33.097288944895745}, {"code": "def idx2vq(self, idx, dim=-1)\n\tembeddings = self.codebook[idx]\n    if dim != -1:\n        embeddings = embeddings.movedim(-1, dim)\n    return embeddings", "CodeBleu_score": 34.30773438926686}, {"code": "def idx2vq(self, idx, dim=-1)\n\t# Retrieve the embeddings corresponding to the given indices\n    embeddings = self.codebook[idx]\n\n    # Move the last dimension to the given dimension\n    if dim != -1:\n        embeddings = torch.moveaxis(embeddings, -1, dim)\n\n    # Return the resulting embeddings\n    return embeddings\n\n\n", "CodeBleu_score": 32.85541038642389}]}
{"instance_id": "4104c041fb20", "generation_result": [{"code": "def encode(self, x, quantize=False)\n\tx = self.conv_in(x)\n    x = self.blocks(x)\n    x = self.conv_out(x)\n    if quantize:\n        x, (vq_loss, commit_loss), indices = self.vq(x)\n        return x, vq_loss, commit_loss, indices\n    else:\n        return x\n\n", "CodeBleu_score": 33.403767437008675}, {"code": "def encode(self, x, quantize=False)\n\tif self.model_type == 'encoder':\n        return self.forward(x)\n    elif self.model_type == 'decoder':\n        return self.forward(x)\n    else:\n        if quantize:\n            z, (vq_loss, commit_loss), indices = self.quantize(x)\n            return z, x, indices, (vq_loss, commit_loss)\n        else:\n            return self.forward(x)\n\n", "CodeBleu_score": 34.99600522804186}, {"code": "def encode(self, x, quantize=False)\n\tx = self.conv_in(x)\n    x = self.blocks(x)\n    x = self.conv_out(x)\n    if quantize:\n        x, (vq_loss, commit_loss), indices = self.vq(x)\n        return x, x.detach(), indices, (vq_loss, commit_loss)\n    return x\n\n", "CodeBleu_score": 31.62604211436119}]}
{"instance_id": "403c52ba2d3b", "generation_result": [{"code": "def ema_scope(self, context=None)\n\tif context is None:\n        context = suppress()\n    with context:\n        self.ema.store(self.parameters())\n        self.ema.copy_to(self)\n        yield\n        self.ema.restore(self.parameters())\n\n", "CodeBleu_score": 34.78840762195773}, {"code": "def ema_scope(self, context=None)\n\tif not hasattr(self, \"ema\"):\n        raise ValueError(\"EMA is not initialized\")\n    self.ema.store(self.parameters())\n    self.ema.copy_to(self)\n    if context is not None:\n        context.log(\"Switched to EMA weights\")\n    yield\n    self.ema.restore(self.parameters())\n    if context is not None:\n        context.log(\"Restored original weights\")\n\n", "CodeBleu_score": 53.71943171192062}, {"code": "def ema_scope(self, context=None)\n\tif context is None:\n        context = self.logger\n    with context.scope('ema'):\n        self.ema.store(self.parameters())\n        self.ema.copy_to(self)\n        yield\n        self.ema.restore(self.parameters())\n\n", "CodeBleu_score": 36.70631210584955}]}
{"instance_id": "94b7467f98e4", "generation_result": [{"code": "def post_attention(self, attn, x, gate_msa, shift_mlp, scale_mlp, gate_mlp)\n\tassert not self.pre_only\n    x = x + attn * gate_msa\n    x = self.norm(x)\n    x = modulate(x, shift_mlp, scale_mlp)\n    x = self.mlp(x) * gate_mlp\n    return x\n\n", "CodeBleu_score": 46.846589774313394}, {"code": "def post_attention(self, attn, x, gate_msa, shift_mlp, scale_mlp, gate_mlp)\n\tassert not self.pre_only\n    x = self.proj(attn)\n    x = self.proj_drop(x)\n    x = modulate(x, shift_mlp, scale_mlp)\n    x = x + gate_msa * x\n    x = self.post_proj(x)\n    x = self.post_proj_drop(x)\n    x = modulate(x, shift_mlp, scale_mlp)\n    x = x + gate_mlp * x\n    return x\n\n", "CodeBleu_score": 49.86996468952745}, {"code": "def post_attention(self, attn, x, gate_msa, shift_mlp, scale_mlp, gate_mlp)\n\tx = x + attn * gate_msa\n    x = x.transpose(1, 2).contiguous()\n    x = self.norm1(x)\n    x = x.transpose(1, 2).contiguous()\n    x = modulate(x, shift_mlp, scale_mlp)\n    x = x + self.post_proj(x)\n    x = x * gate_mlp\n    return x\n\n", "CodeBleu_score": 48.97097943670011}]}
{"instance_id": "d9fdc05542a2", "generation_result": [{"code": "def _block_mixing(context, x, context_block, x_block, c)\n\tcontext_q, context_k, context_v = context_block.pre_attention(context)\n    x_q, x_k, x_v = x_block.pre_attention(x)\n    qkv = torch.cat([context_q, context_k, context_v, x_q, x_k, x_v], dim=2)\n    attn_out = optimized_attention(qkv, num_heads=c.num_heads)\n    context_qkv, x_qkv = split_qkv(attn_out, c.head_dim)\n    context = context_block.post_attention(context_qkv)\n    x = x_block.post_attention(x_qkv)\n    return context, x\n\n", "CodeBleu_score": 19.006840542824467}, {"code": "def _block_mixing(context, x, context_block, x_block, c)\n\t# Process context and x through their respective pre-attention blocks\n    # to obtain query, key, and value (qkv) tensors and intermediates\n    q, k, v, context_intermediates, x_intermediates = _pre_attention(context, x, context_block, x_block)\n\n    # Concatenate the qkv tensors\n    qkv = torch.cat([q, k, v], dim=2)\n\n    # Apply optimized attention\n    x_attn = optimized_attention(qkv, context_block.num_heads)\n\n    # Split the attention results back into context and x parts\n    context_attn, x_attn = split_qkv(x_attn, context_block.head_dim)\n\n    # Process the context and x parts through their respective post-attention blocks\n    context = _post_attention(context, context_attn, context_intermediates, context_block)\n    x = _post_attention(x, x_attn, x_intermediates, x_block)\n\n    # Return the updated context and x\n    return context, x\n\n", "CodeBleu_score": 18.51923487107134}, {"code": "def _block_mixing(context, x, context_block, x_block, c)\n\t# Pre-attention\n    q_context, k_context, v_context = context_block.pre_attention(context)\n    q_x, k_x, v_x = x_block.pre_attention(x)\n    # Block mixing\n    qkv = torch.cat([q_context, q_x, k_context, k_x, v_context, v_x], dim=-1)\n    qkv = optimized_attention(qkv, c.num_heads)\n    q_context, q_x, k_context, k_x, v_context, v_x = split_qkv(qkv, c.head_dim)\n    # Post-attention\n    context = context_block.post_attention(q_context + v_context)\n    x = x_block.post_attention(q_x + v_x)\n    return context, x\n\n", "CodeBleu_score": 17.22269197025468}]}
{"instance_id": "b219bce30160", "generation_result": [{"code": "def get_timestep_embedding(timesteps, embedding_dim)\n\tassert len(timesteps.shape) == 1, \"Timesteps must be one-dimensional\"\n    assert embedding_dim % 2 == 0, \"Embedding dimension must be even\"\n    half_dim = embedding_dim // 2\n    emb = np.log(10000) / (half_dim - 1)\n    emb = np.exp(np.arange(half_dim, dtype=np.float32) * -emb)\n    emb = np.expand_dims(np.arange(len(timesteps), dtype=np.float32), 1) * emb\n    emb = np.concatenate([np.sin(emb), np.cos(emb)], axis=1)\n    if embedding_dim % 2 == 1:\n        emb = np.pad(emb, [[0, 0], [0, 1]])\n    assert emb.shape[1] == embedding_dim\n    return emb", "CodeBleu_score": 57.78575167535759}, {"code": "def get_timestep_embedding(timesteps, embedding_dim)\n\tassert len(timesteps.shape) == 1, \"Timesteps must be a one-dimensional tensor.\"\n    assert embedding_dim % 2 == 0, \"Embedding dimension must be even.\"\n    # Calculate the positional embeddings using sine and cosine functions.\n    # We use a logarithmic scale for the positional embeddings.\n    # The values are normalized by the maximum value of the logarithmic scale.\n    timesteps = tf.cast(timesteps, tf.float32)\n    log_timescale_increment = math.log(10000) / (embedding_dim // 2 - 1)\n    inv_timescales = tf.exp(tf.range(0, embedding_dim // 2, 2) * -log_timescale_increment)\n    scaled_time = timesteps[:, None] * tf.cast(inv_timescales, tf.float32)\n    # We use `tf.concat` to create the full embedding.\n    # For even dimensions, the concatenated axis is the embedding dimension.\n    # For odd dimensions, we zero-pad the embedding by one element on the right.\n    embedding = tf.concat([tf.sin(scaled_time), tf.cos(scaled_time)], axis=-1)\n    if embedding_dim % 2 != 0:\n        embedding = tf.pad(embedding, [[0, 0], [0, 1]])\n    return embedding\n\n", "CodeBleu_score": 41.373669398789794}, {"code": "def get_timestep_embedding(timesteps, embedding_dim)\n\tassert len(timesteps.shape) == 1, 'The timesteps tensor must be one-dimensional.'\n\n    assert embedding_dim % 2 == 0, 'The embedding dimension must be even.'\n\n    half_dim = embedding_dim // 2\n\n    emb = np.log(10000) / (half_dim - 1)\n    emb = np.exp(np.arange(half_dim, dtype=np.float32) * -emb)\n    emb = np.expand_dims(np.arange(num_positions, dtype=np.float32), 1) * np.expand_dims(emb, 0)\n    emb = np.concatenate([np.sin(emb), np.cos(emb)], axis=1)\n    emb = np.expand_dims(emb, 0)\n    emb = np.tile(emb, [batch_size, 1, 1])\n\n    if embedding_dim % 2 == 1:\n        emb = np.pad(emb, [[0, 0], [0, 1], [0, 0]])\n\n    assert emb.shape == (batch_size, num_positions, embedding_dim)\n    return emb\n\n", "CodeBleu_score": 57.00420195903211}]}
{"instance_id": "6ae8453c86be", "generation_result": [{"code": "def pytorch_attention(q, k, v)\n\t# prepare tensors for attention computation\n    q = q.transpose(1,2).contiguous()\n    k = k.transpose(1,2).contiguous()\n    v = v.transpose(1,2).contiguous()\n\n    # compute attention\n    try:\n        r1 = torch.bmm(q, k.transpose(1,2)) * (int(q.shape[-1])**(-0.5))\n    except model_management.OOM_EXCEPTION as e:\n        logging.warning(\"out of memory error, switching to slice attention\")\n        r1 = slice_attention(q, k, v)\n\n    # compute attention weights\n    s1 = torch.nn.functional.softmax(r1, dim=2)\n\n    # compute attention output\n    r2 = torch.bmm(v, s1.permute(0,2,1))\n\n    # reshape tensors back to the original dimensions\n    r2 = r2.transpose(1,2).contiguous()\n    s1 = s1.transpose(1,2).contiguous()\n\n    return r2, s1\n\n", "CodeBleu_score": 33.30440338382642}, {"code": "def pytorch_attention(q, k, v)\n\tif not torch.is_tensor(q) or not torch.is_tensor(k) or not torch.is_tensor(v):\n        raise TypeError(\"Inputs must be PyTorch tensors.\")\n\n    if len(q.shape) != 3 or len(k.shape) != 3 or len(v.shape) != 3:\n        raise ValueError(\"Inputs must be 3-dimensional.\")\n\n    if q.shape[0] != k.shape[0] or q.shape[0] != v.shape[0]:\n        raise ValueError(\"Inputs must have equal batch sizes.\")\n\n    if q.shape[2] != k.shape[2] or q.shape[2] != v.shape[2]:\n        raise ValueError(\"Inputs must have equal feature sizes.\")\n\n    if k.shape[1] != v.shape[1]:\n        raise ValueError(\"Key and value must have equal sequence lengths.\")\n\n    if q.shape[1] != k.shape[1]:\n        raise ValueError(\"Query and key must have equal sequence lengths.\")\n\n    if q.shape[1] == 0 or k.shape[1] == 0:\n        raise ValueError(\"Sequence lengths must be greater than 0.\")\n\n    if q.device != k.device or q.device != v.device:\n        raise ValueError(\"Inputs must be on the same device.\")\n\n    if q.dtype != k.dtype or q.dtype != v.dtype:\n        raise TypeError(\"Inputs must have the same dtype.\")\n\n    if q.dtype != torch.float16 and q.dtype != torch.float32 and q.dtype != torch.float64:\n        raise TypeError(\"Inputs must be of type float16, float32, or float64.\")\n\n    if q.dtype == torch.float16:\n        logging.warning(\"Attention computation is not supported for float16. Switching to float32.\")\n        q = q.float()\n        k = k.float()\n        v = v.float()\n\n    if q.shape[1] == 1:\n        return v", "CodeBleu_score": 14.545194984737151}, {"code": "def pytorch_attention(q, k, v)\n\tif q.shape[1] < 1024:\n        return torch.bmm(torch.nn.functional.softmax(torch.bmm(q, k.transpose(1, 2)) / math.sqrt(q.shape[-1]), dim=2), v)\n    else:\n        return slice_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2)).transpose(1, 2)\n\n", "CodeBleu_score": 12.8155358738434}]}
{"instance_id": "fe4433c987ad", "generation_result": [{"code": "def apply_control(h, control, name)\n\tif control is not None:\n        if name in control:\n            if control[name] is not None:\n                try:\n                    h = h.add(control[name])\n                except ValueError as e:\n                    print(\"Warning: {}\".format(e))\n    return h\n\n", "CodeBleu_score": 45.8297592851349}, {"code": "def apply_control(h, control, name)\n\tif control is None:\n        return h\n    if name in control:\n        c = control[name]\n        if c is not None:\n            try:\n                h = h.add(c)\n            except:\n                print(\"WARNING: Failed to apply control {} to {}\".format(c, h))\n        control.pop(name)\n    return h\n\n", "CodeBleu_score": 45.15412854718546}, {"code": "def apply_control(h, control, name)\n\tif control is not None and name in control and len(control[name]) > 0:\n        try:\n            h.add(control[name].pop(0))\n        except:\n            print(\"Warning: Failed to add {} to {}\".format(name, h.name))\n    return h\n\n\n", "CodeBleu_score": 43.14979826416307}]}
{"instance_id": "56da4547c101", "generation_result": [{"code": "def get_codebook_entry(self, indices, shape)\n\t# Create a one-hot encoding of the indices\n    one_hot_encoding = tf.one_hot(indices, self.codebook_size, dtype=self.dtype)\n    # Multiply the one-hot encoding with the embedding weights to get the quantized vectors\n    quantized_vectors = tf.matmul(one_hot_encoding, self.embedding_weights)\n    # Reshape and permute the quantized vectors to match the original input shape\n    if shape is not None:\n        quantized_vectors = tf.reshape(quantized_vectors, shape=shape)\n        quantized_vectors = tf.transpose(quantized_vectors, perm=self.permute_indices)\n    return quantized_vectors\n\n\n", "CodeBleu_score": 30.783365940885552}, {"code": "def get_codebook_entry(self, indices, shape)\n\t# Create one-hot encoding\n    one_hot = tf.one_hot(indices, self.latent_dim)\n\n    # Quantize\n    quantized = tf.matmul(one_hot, self.embeddings)\n\n    # Reshape and permute\n    if shape is not None:\n        quantized = tf.reshape(quantized, shape)\n        quantized = tf.transpose(quantized, [0, 3, 1, 2])\n\n    return quantized\n\n", "CodeBleu_score": 29.931445246138082}, {"code": "def get_codebook_entry(self, indices, shape)\n\t# Create one-hot encoding of the indices\n    one_hot = F.one_hot(indices, self.num_embeddings).float()\n    # Multiply one-hot encoding with embedding weights to get quantized vectors\n    quantized_vectors = torch.matmul(one_hot, self.weight.t())\n    # Reshape and permute quantized vectors to match original input shape\n    if shape is not None:\n        quantized_vectors = quantized_vectors.view(*shape)\n        quantized_vectors = quantized_vectors.permute(0, 3, 1, 2)\n    return quantized_vectors\n\n", "CodeBleu_score": 32.276328037946165}]}
{"instance_id": "548f8a1fb37d", "generation_result": [{"code": "def make_noise(self)\n\tnoise = [torch.randn(4, 4, device=self.device)]\n    for i in range(3, self.log_size):\n        noise.append(torch.randn(2 ** i, 2 ** i, device=self.device))\n        noise.append(torch.randn(2 ** i, 2 ** i, device=self.device))\n    return noise\n\n", "CodeBleu_score": 29.98301280892784}, {"code": "def make_noise(self)\n\tnoise_list = []\n    noise_list.append(self.noise_init(4, 4))\n    for i in range(3, self.log_size):\n        noise_list.append(self.noise_init(2 ** i, 2 ** i))\n        noise_list.append(self.noise_init(2 ** i, 2 ** i))\n    return noise_list\n\n", "CodeBleu_score": 23.988372871624232}, {"code": "def make_noise(self)\n\tnoise_list = []\n    noise_list.append(torch.randn(4, 4, device=self.device))\n    for size in range(3, self.log_size):\n        noise_list.append(torch.randn(2**size, 2**size, device=self.device))\n        noise_list.append(torch.randn(2**size, 2**size, device=self.device))\n    return noise_list\n\n", "CodeBleu_score": 25.89172422223001}]}
{"instance_id": "42fd805563c6", "generation_result": [{"code": "def mean_latent(self, num_latent)\n\tlatent_input = torch.randn(num_latent, self.latent_dim)\n    latent_output = self.style_mlp(latent_input)\n    mean_latent = torch.mean(latent_output, dim=0)\n\n    return mean_latent", "CodeBleu_score": 21.38490592089945}, {"code": "def mean_latent(self, num_latent)\n\t# Generate random latent inputs\n    latent_inputs = torch.randn(num_latent, self.latent_dim)\n    # Process latent inputs through the style MLP\n    latent_outputs = self.style_mlp(latent_inputs)\n    # Compute the mean of the latent outputs\n    mean_latent = torch.mean(latent_outputs, dim=0)\n    return mean_latent\n\n", "CodeBleu_score": 21.23766524661599}, {"code": "def mean_latent(self, num_latent)\n\tlatent_input = torch.randn(1, num_latent, 1, 1, device=self.device)\n    mean_latent = self.style_mlp(latent_input)\n    mean_latent = mean_latent.mean(dim=0, keepdim=True)\n    return mean_latent\n\n", "CodeBleu_score": 40.44939664659616}]}
{"instance_id": "2f485fd924af", "generation_result": [{"code": "def color_jitter(img, shift)\n\tjitter = np.random.randint(shift, size=3)\n    img = img + jitter\n    img = np.clip(img, 0, 1)\n    return img\n\n\n", "CodeBleu_score": 47.26911698680271}, {"code": "def color_jitter(img, shift)\n\t# Generate random jitter values for each channel\n    r_jitter = np.random.uniform(-shift, shift)\n    g_jitter = np.random.uniform(-shift, shift)\n    b_jitter = np.random.uniform(-shift, shift)\n\n    # Add the jitter values to the RGB channels\n    img[:, :, 0] += r_jitter\n    img[:, :, 1] += g_jitter\n    img[:, :, 2] += b_jitter\n\n    # Clip the values to ensure they remain between 0 and 1\n    img[:, :, 0] = np.clip(img[:, :, 0], 0, 1)\n    img[:, :, 1] = np.clip(img[:, :, 1], 0, 1)\n    img[:, :, 2] = np.clip(img[:, :, 2], 0, 1)\n\n    return img", "CodeBleu_score": 34.738710400507856}, {"code": "def color_jitter(img, shift)\n\tjitter = np.random.randint(-shift, shift, 3)\n    img = img + jitter\n    img = np.clip(img, 0, 1)\n    return img\n\n", "CodeBleu_score": 52.17816271604195}]}
{"instance_id": "c87ccf1fbf48", "generation_result": [{"code": "def feed_data(self, data)\n\tself.lq = data['lq'].to(self.device)\n    if 'gt' in data:\n        self.gt = data['gt'].to(self.device)\n    if 'lq_path' in data:\n        self.lq_path = data['lq_path']\n    if 'gt_path' in data:\n        self.gt_path = data['gt_path']\n    if 'face_location' in data:\n        self.left_eye = data['face_location']['left_eye']\n        self.right_eye = data['face_location']['right_eye']\n        self.mouth = data['face_location']['mouth']", "CodeBleu_score": 54.63985997286262}, {"code": "def feed_data(self, data)\n\tself.lq = data['lq'].to(self.device)\n    if 'gt' in data:\n        self.gt = data['gt'].to(self.device)\n    if 'gt_facial' in data:\n        self.gt_facial = data['gt_facial'].to(self.device)\n        self.gt_facial_mask = data['gt_facial_mask'].to(self.device)\n        self.left_eye = data['left_eye'].to(self.device)\n        self.right_eye = data['right_eye'].to(self.device)\n        self.mouth = data['mouth'].to(self.device)\n\n", "CodeBleu_score": 52.901916997315965}, {"code": "def feed_data(self, data)\n\tself.lq = data['lq'].to(self.device)\n    if 'gt' in data.keys():\n        self.gt = data['gt'].to(self.device)\n    if 'lq_path' in data.keys():\n        self.lq_path = data['lq_path']\n    if 'gt_path' in data.keys():\n        self.gt_path = data['gt_path']\n    if 'key' in data.keys():\n        self.key = data['key']\n    if 'left_eye' in data.keys():\n        self.left_eye = data['left_eye']\n    if 'right_eye' in data.keys():\n        self.right_eye = data['right_eye']\n    if 'mouth' in data.keys():\n        self.mouth = data['mouth']\n\n", "CodeBleu_score": 46.49628724817669}]}
{"instance_id": "8f8c7cb13037", "generation_result": [{"code": "def construct_img_pyramid(self)\n\timg_pyramid = []\n    img_pyramid.append(self.gt_img)\n    for i in range(self.num_scales - 1):\n        img_pyramid.insert(0, self.downscale(img_pyramid[0]))\n    return img_pyramid\n", "CodeBleu_score": 17.629494240578232}, {"code": "def construct_img_pyramid(self)\n\tpyramid = [self.gt]\n    for i in range(self.n_scales):\n        pyramid.insert(0, self.downscale(pyramid[0]))\n    return pyramid\n", "CodeBleu_score": 14.116883361152349}, {"code": "def construct_img_pyramid(self)\n\timg_pyramid = [self.gt_img]\n    for i in range(self.num_steps):\n        img_pyramid.insert(0, cv2.resize(img_pyramid[0], (int(img_pyramid[0].shape[1]/2), int(img_pyramid[0].shape[0]/2)), interpolation=cv2.INTER_LINEAR))\n    return img_pyramid\n", "CodeBleu_score": 18.165314452050712}]}
{"instance_id": "51247274d40c", "generation_result": [{"code": "def save(self, epoch, current_iter)\n\tself.logger.info(\"Saving model at epoch {} and iteration {}\".format(epoch, current_iter))\n\n    self.model_saver.save_model(self.model, epoch, current_iter)\n    self.model_saver.save_model(self.model_ema, epoch, current_iter, ema=True)\n    self.model_saver.save_model(self.discriminator, epoch, current_iter, discriminator=True)\n\n    if self.args.use_facial_discriminator:\n        self.model_saver.save_model(self.discriminator_eyes, epoch, current_iter, discriminator=True, eyes=True)\n        self.model_saver.save_model(self.discriminator_mouth, epoch, current_iter, discriminator=True, mouth=True)\n\n    self.model_saver.save_training_state(self.optimizer, self.optimizer_discriminator, epoch, current_iter)", "CodeBleu_score": 44.26129557798204}, {"code": "def save(self, epoch, current_iter)\n\tself.save_network(self.G, 'G', epoch, current_iter)\n    self.save_network(self.G_ema, 'G_ema', epoch, current_iter)\n    self.save_network(self.D, 'D', epoch, current_iter)\n    if self.use_face_disc:\n        self.save_network(self.D_left_eye, 'D_left_eye', epoch, current_iter)\n        self.save_network(self.D_right_eye, 'D_right_eye', epoch, current_iter)\n        self.save_network(self.D_mouth, 'D_mouth', epoch, current_iter)\n    self.save_training_state(epoch, current_iter)\n\n\n", "CodeBleu_score": 55.50724124711677}, {"code": "def save(self, epoch, current_iter)\n\tself.save_model(self.netG, epoch, current_iter, \"G\")\n    self.save_model(self.netG_ema, epoch, current_iter, \"G_ema\")\n    self.save_model(self.netD, epoch, current_iter, \"D\")\n    if self.use_eyes:\n        self.save_model(self.netD_eye_left, epoch, current_iter, \"D_eye_left\")\n        self.save_model(self.netD_eye_right, epoch, current_iter, \"D_eye_right\")\n    if self.use_mouth:\n        self.save_model(self.netD_mouth, epoch, current_iter, \"D_mouth\")\n    self.save_training_state(epoch, current_iter)\n\n\n", "CodeBleu_score": 48.6530554977978}]}
{"instance_id": "afa0dc8128c1", "generation_result": [{"code": "def clean_folder(folder)\n\timport os\n    import shutil\n    for filename in os.listdir(folder):\n        file_path = os.path.join(folder, filename)\n        try:\n            if os.path.isfile(file_path) or os.path.islink(file_path):\n                os.unlink(file_path)\n            elif os.path.isdir(file_path):\n                shutil.rmtree(file_path)\n        except Exception as e:\n            print('Failed to delete %s. Reason: %s' % (file_path, e))\n\n", "CodeBleu_score": 65.03483738613971}, {"code": "def clean_folder(folder)\n\timport os, shutil\n    for the_file in os.listdir(folder):\n        file_path = os.path.join(folder, the_file)\n        try:\n            if os.path.isfile(file_path):\n                os.unlink(file_path)\n            elif os.path.isdir(file_path): shutil.rmtree(file_path)\n        except Exception as e:\n            print(e)\n\n\n", "CodeBleu_score": 48.81840938641271}, {"code": "def clean_folder(folder)\n\timport os\n    import shutil\n    for item in os.listdir(folder):\n        path = os.path.join(folder, item)\n        try:\n            if os.path.isfile(path) or os.path.islink(path):\n                os.unlink(path)\n            elif os.path.isdir(path):\n                shutil.rmtree(path)\n        except Exception as e:\n            print(\"Failed to delete %s. Reason: %s\" % (path, e))\n\n", "CodeBleu_score": 47.32995718229675}]}
{"instance_id": "53588b481bdd", "generation_result": [{"code": "def next_batch(self, sample_size=1)\n\tif sample_size > len(self.experiences):\n        sample_size = len(self.experiences)\n\n    batch = random.sample(self.experiences, sample_size)\n    self.experiences = [exp for exp in self.experiences if exp not in batch]\n\n    return batch", "CodeBleu_score": 64.81909829345986}, {"code": "def next_batch(self, sample_size=1)\n\tif sample_size > len(self.experiences):\n        sample_size = len(self.experiences)\n    batch = random.sample(self.experiences, sample_size)\n    for experience in batch:\n        self.experiences.remove(experience)\n    return batch\n\n", "CodeBleu_score": 49.55684932968932}, {"code": "def next_batch(self, sample_size=1)\n\tif self.size < sample_size:\n        sample_size = self.size\n        print('Sample size exceeds the size of the experience list.')\n    idx = np.random.choice(self.size, sample_size, replace=False)\n    batch = self.experiences[idx]\n    self.experiences = np.delete(self.experiences, idx, 0)\n    self.size -= sample_size\n    return batch", "CodeBleu_score": 33.56536570687818}]}
{"instance_id": "6bfa6a4e621c", "generation_result": [{"code": "def find_estimated_top_configs(self)\n\tall_configs = self.all_configs\n    all_configs_flattened = self.all_configs_flattened\n    cost_model = self.cost_model\n    metric = self.metric\n    top_configs = self.top_configs\n    top_configs_flattened = self.top_configs_flattened\n    top_configs_flattened_estimated = self.top_configs_flattened_estimated\n    top_configs_estimated = self.top_configs_estimated\n    top_configs_estimated_costs = self.top_configs_estimated_costs\n    top_configs_estimated_costs_sorted = self.top_configs_estimated_costs_sorted\n    top_configs_estimated_costs_sorted_indices = self.top_configs_estimated_costs_sorted_indices\n    top_configs_estimated_sorted = self.top_configs_estimated_sorted\n    top_configs_estimated_sorted_indices = self.top_configs_estimated_sorted_indices\n    top_configs_sorted = self.top_configs_sorted\n    top_configs_sorted_indices = self.top_configs_sorted_indices\n    top_configs_sorted_indices_original = self.top_configs_sorted_indices_original\n    top_configs_sorted_indices_original_costs = self.top_configs_sorted_indices_original_costs\n    top_configs_sorted_indices_original_costs_sorted = self.top_configs_sorted_indices_original_costs_sorted\n    top_configs_sorted_indices_original_costs_sorted_indices = self.top_configs_sorted_indices_original_costs_sorted_indices\n    top_configs_sorted_indices_original_costs_sorted_indices_original = self.top_configs_sorted_indices_original_costs_sorted_indices", "CodeBleu_score": 5.329985871923866}, {"code": "def find_estimated_top_configs(self)\n\t# Flatten each configuration in all_configs, collect numerical values, and convert them into a numpy array\n    all_configs_flattened = []\n    for config in self.all_configs:\n        config_flattened = []\n        for param in config:\n            if type(param) in [int, float, np.int64, np.float64]:\n                config_flattened.append(param)\n        all_configs_flattened.append(config_flattened)\n    all_configs_flattened = np.array(all_configs_flattened)\n\n    # Predict estimates for all configurations\n    all_configs_estimates = self.cost_model.predict(all_configs_flattened)\n\n    # Sort the estimates and return the indices of the top configurations\n    if self.metric == \"cost\":\n        top_config_indices = np.argsort(all_configs_estimates)[:self.top_k]\n    else:\n        top_config_indices = np.argsort(all_configs_estimates)[::-1][:self.top_k]\n\n    return top_config_indices", "CodeBleu_score": 37.54824971133497}, {"code": "def find_estimated_top_configs(self)\n\timport numpy as np\n    from sklearn.preprocessing import MinMaxScaler\n    from sklearn.metrics import mean_squared_error\n    from sklearn.metrics import mean_absolute_error\n    from sklearn.metrics import median_absolute_error\n    from sklearn.metrics import r2_score\n    from sklearn.metrics import explained_variance_score\n\n    all_configs = self.all_configs\n    metric = self.metric\n    if metric == \"mean_squared_error\":\n        metric_func = mean_squared_error\n    elif metric == \"mean_absolute_error\":\n        metric_func = mean_absolute_error\n    elif metric == \"median_absolute_error\":\n        metric_func = median_absolute_error\n    elif metric == \"r2_score\":\n        metric_func = r2_score\n    elif metric == \"explained_variance_score\":\n        metric_func = explained_variance_score\n    else:\n        raise ValueError(\"Metric {} not supported\".format(metric))\n    # flatten configs\n    flattened_configs = []\n    for config in all_configs:\n        flattened_configs.append(self.flatten_config(config))\n    # convert flattened configs to numpy array\n    flattened_configs = np.array(flattened_configs)\n    # scale flattened configs\n    scaler = MinMaxScaler()\n    flattened_configs = scaler.fit_transform(flattened_configs)\n    # predict estimates for flattened configs\n    estimates = self.estimator.predict(flattened_configs)\n    # sort estimates\n    sorted_indices = np.argsort(estimates)\n    # return top configs\n    return sorted_indices\n\n", "CodeBleu_score": 30.446980143158452}]}
{"instance_id": "d4cf48413a57", "generation_result": [{"code": "def flatten(d, parent_key='', sep='_')\n\titems = []\n    for k, v in d.items():\n        new_key = parent_key + sep + k if parent_key else k\n        if isinstance(v, collections.abc.MutableMapping):\n            items.extend(flatten(v, new_key, sep=sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)\n\n", "CodeBleu_score": 82.16984618063623}, {"code": "def flatten(d, parent_key='', sep='_')\n\titems = []\n    for k, v in d.items():\n        new_key = parent_key + sep + k if parent_key else k\n        if isinstance(v, collections.abc.MutableMapping):\n            items.extend(flatten(v, new_key, sep=sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)\n\n", "CodeBleu_score": 82.16984618063623}, {"code": "def flatten(d, parent_key='', sep='_')\n\titems = []\n    for k, v in d.items():\n        new_key = parent_key + sep + k if parent_key else k\n        if isinstance(v, collections.abc.MutableMapping):\n            items.extend(flatten(v, new_key, sep=sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)", "CodeBleu_score": 82.16984618063623}]}
{"instance_id": "c2a514249c25", "generation_result": [{"code": "def run_after_tuning(self)\n\tif self.optimal_cmd is not None:\n        self.logger.info(\"Running the optimal command: {}\".format(self.optimal_cmd))\n        self.logger.info(\"Waiting for the command to complete...\")\n        process = subprocess.Popen(self.optimal_cmd, shell=True)\n        process.wait()\n        self.logger.info(\"The command completed.\")\n    else:\n        self.logger.info(\"Autotuning did not find an optimal configuration.\")", "CodeBleu_score": 46.989061807246706}, {"code": "def run_after_tuning(self)\n\tif self.optimal_cmd is not None:\n        proc = subprocess.Popen(self.optimal_cmd, shell=True)\n        proc.wait()\n        self.logger.info(\"Autotuning found an optimal configuration. Command {} was executed.\".format(self.optimal_cmd))\n    else:\n        self.logger.info(\"Autotuning did not find an optimal configuration.\")", "CodeBleu_score": 50.22072622545072}, {"code": "def run_after_tuning(self)\n\tif self.optimal_cmd:\n        self.logger.info(f\"Optimal command found: {self.optimal_cmd}\")\n        self.logger.info(\"Executing the optimal command...\")\n        self.logger.info(\"------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------", "CodeBleu_score": 19.917492992175656}]}
{"instance_id": "61802de52df0", "generation_result": [{"code": "def run_job(self, exp: dict, reservations)\n\t# Set experiment master port and result directory\n    exp[\"master_port\"] = self.master_port\n    exp[\"result_dir\"] = self.result_dir\n\n    # Update user arguments based on argument mappings\n    for mapping in exp[\"argument_mappings\"]:\n        for arg in exp[\"user_args\"]:\n            if arg[\"name\"] == mapping[\"arg_name\"]:\n                arg[\"value\"] = mapping[\"value\"]\n\n    # Start experiment in a new thread\n    thread = threading.Thread(target=self.run_experiment, args=(exp, reservations))\n    thread.start()\n\n    # Store running experiment details with a timestamp\n    self.running_experiments.append({\"exp_id\": exp[\"id\"], \"timestamp\": time.time()})\n\n    return thread\n\n", "CodeBleu_score": 20.88813687985197}, {"code": "def run_job(self, exp: dict, reservations)\n\t# Set the master port and result directory\n    exp['master_port'] = self.master_port\n    exp['result_dir'] = self.result_dir\n\n    # Update user arguments based on argument mappings\n    if 'argument_mappings' in exp:\n        for mapping in exp['argument_mappings']:\n            if mapping['type'] == 'user_argument':\n                if mapping['name'] in exp['user_arguments']:\n                    exp['user_arguments'][mapping['name']] = mapping['value']\n\n    # Start the experiment in a new thread\n    thread = threading.Thread(target=self.run_experiment, args=(exp,))\n    thread.start()\n\n    # Store the running experiment details with a timestamp in running_experiments\n    exp['timestamp'] = time.time()\n    self.running_experiments.append(exp)\n\n    # Update the number of running experiments\n    self.running_experiments_count += 1\n\n    # Update the number of running experiments for each reservation\n    for reservation in reservations:\n        reservation['running_experiments_count'] += 1\n\n    return\n\n", "CodeBleu_score": 28.825121814535414}, {"code": "def run_job(self, exp: dict, reservations)\n\tfrom threading import Thread\n    from datetime import datetime\n    from os import mkdir\n    from os.path import join\n    from time import sleep\n    from ..utils import get_master_port\n    from ..utils import get_result_dir\n    from ..utils import get_script_path\n    from ..utils import get_script_args\n    from ..utils import update_args\n    from ..utils import get_experiment_dir\n    from ..utils import get_experiment_reservation\n    from ..utils import get_experiment_name\n    from ..utils import get_experiment_id\n    from ..utils import get_experiment_reservation_id\n    from ..utils import get_experiment_reservation_name\n    from ..utils import get_experiment_reservation_id\n    from ..utils import get_experiment_reservation_name\n    from ..utils import get_experiment_reservation_id\n    from ..utils import get_experiment_reservation_name\n    from ..utils import get_experiment_reservation_id\n    from ..utils import get_experiment_reservation_name\n    from ..utils import get_experiment_reservation_id\n    from ..utils import get_experiment_reservation_name\n    from ..utils import get_experiment_reservation_id\n    from ..utils import get_experiment_reservation_name\n    from ..utils import get_experiment_reservation_id\n    from ..utils import get_experiment_reservation_name\n    from ..utils import get_experiment_reservation_id\n    from ..utils import get_experiment_reservation_name\n    from ..utils import get_experiment_reservation_id\n    from ..utils import get_experiment_reservation_name\n    from ..utils import get_experiment_reservation_id\n    from ..utils import get_experiment_reservation_name\n    from ..utils import get_experiment_reservation_id\n    from ..utils import get_experiment_reservation_name\n    from ..utils import get_experiment_reservation_id\n    from ..utils import get_experiment_reservation_name\n    from ..utils import get_experiment_reservation_id", "CodeBleu_score": 6.580959776877143}]}
{"instance_id": "4884e2e9665c", "generation_result": [{"code": "def save_exp_results_to_database(self, message, ranks=None, path=None):\n\tif self.distributed_training:\n        if self.rank == 0:\n            if ranks is not None:\n                message = ranks + \" \" + message\n            if path is not None:\n                self.save_to_database(message, path)\n    else:\n        if path is not None:\n            self.save_to_database(message, path)\n\n\n", "CodeBleu_score": 18.06339501705878}, {"code": "def save_exp_results_to_database(self, message, ranks=None, path=None):\n\tif self.distributed_training_initialized:\n        if self.log_experiment_results:\n            if self.rank == 0:\n                if ranks is None:\n                    message = json.dumps(message)\n                else:\n                    message = json.dumps(message, indent=4)\n                with open(path, \"a\") as f:\n                    f.write(message)\n                    f.write(\"", "CodeBleu_score": 24.64205001123246}, {"code": "def save_exp_results_to_database(self, message, ranks=None, path=None):\n\tif self.args.distributed:\n        if self.args.rank == 0:\n            if ranks is None:\n                self.logger.experiment.log_text(message)\n            else:\n                self.logger.experiment.log_text(message, ranks)\n    else:\n        if ranks is None:\n            self.logger.experiment.log_text(message)\n        else:\n            self.logger.experiment.log_text(message, ranks)\n\n\n", "CodeBleu_score": 17.4900978467229}]}
{"instance_id": "0429344c3c4d", "generation_result": [{"code": "def _build_tp_other_layer_map(self, layer_index: int)\n\tlayer_files = self.get_layer_files(layer_index)\n    if not layer_files:\n        raise ValueError(f\"No layer files found for layer {layer_index}\")\n    if not self.tp_degree:\n        raise ValueError(f\"No tp_degree set\")\n    if self.tp_degree <= 0:\n        raise ValueError(f\"tp_degree must be greater than 0\")\n    if not self.tp_degree_prefix:\n        raise ValueError(f\"No tp_degree_prefix set\")\n    if not self.tp_degree_suffix:\n        raise ValueError(f\"No tp_degree_suffix set\")\n    if self.tp_degree_suffix.startswith(\".\"):\n        raise ValueError(f\"tp_degree_suffix must not start with a dot\")\n    if self.tp_degree_suffix.endswith(\".\"):\n        raise ValueError(f\"tp_degree_suffix must not end with a dot\")\n\n    # check if layer index is valid\n    if layer_index < 0:\n        raise ValueError(f\"Layer index must be greater than 0\")\n    if layer_index >= len(layer_files):\n        raise ValueError(f\"Layer index must be smaller than the number of layer files\")\n\n    # partition layer files based on tp_degree\n    layer_files_by_tp_degree = {}\n    for layer_file in layer_files:\n        layer_file_name = layer_file.name\n        if layer_file_name.startswith(self.tp_degree_prefix) and layer_file_name.endswith(self.tp_degree_suffix):\n            # extract tp degree from file name\n            tp_degree = int(layer_file_name[len(self.tp_degree_prefix):-len(self.tp_degree_suffix)])\n            # add file to tp degree list\n            if tp_degree not in layer_files_by_tp_degree:\n                layer_files_by_tp_deg", "CodeBleu_score": 28.121594002654447}, {"code": "def _build_tp_other_layer_map(self, layer_index: int)\n\tlayer_files = self.get_layer_files(layer_index)\n    if len(layer_files) == 0:\n        return {}\n    if layer_index < 0 or layer_index >= self.get_num_layers():\n        raise ValueError(f\"Layer index {layer_index} is out of range. There are {self.get_num_layers()} layers.\")\n    tp_degree = self.get_tp_degree(layer_index)\n    if tp_degree is None:\n        raise ValueError(f\"Layer index {layer_index} has no tp_degree.\")\n    tp_degree_str = str(tp_degree)\n    layer_files = [f for f in layer_files if f.startswith(tp_degree_str + \"_\")]\n    if len(layer_files) == 0:\n        return {}\n    partition_map = {}\n    for f in layer_files:\n        partition_index = int(f.split(\"_\")[1])\n        if partition_index not in partition_map:\n            partition_map[partition_index] = []\n        partition_map[partition_index].append(f)\n    return partition_map\n\n", "CodeBleu_score": 31.81031938983514}, {"code": "def _build_tp_other_layer_map(self, layer_index: int)\n\t# Check if there are any layer files\n    if not self.layer_files:\n        raise Exception('No layer files found.')\n    # Check if the layer index is valid\n    if layer_index < 0 or layer_index >= len(self.layer_files):\n        raise Exception('The layer index is not valid.')\n    # Get the layer files\n    layer_files = self.layer_files[layer_index]\n    # Get the layer prefix\n    layer_prefix = layer_files[0].split('_')[0]\n    # Partition the layer files based on tp_degree\n    layer_files_by_tp_degree = {}\n    for layer_file in layer_files:\n        # Get the tp_degree\n        tp_degree = int(layer_file.split('_')[1].split('.')[0])\n        # Add the file to the corresponding list\n        if tp_degree in layer_files_by_tp_degree:\n            layer_files_by_tp_degree[tp_degree].append(layer_file)\n        else:\n            layer_files_by_tp_degree[tp_degree] = [layer_file]\n    # Create a dictionary mapping partition indices to their corresponding file lists\n    layer_files_by_partition = {}\n    for tp_degree in layer_files_by_tp_degree:\n        # Get the file list\n        file_list = layer_files_by_tp_degree[tp_degree]\n        # Get the partition index\n        partition_index = int(file_list[0].split('_')[2].split('.')[0])\n        # Add the file list to the dictionary\n        layer_files_by_partition[partition_index] = file_list\n    # Return the dictionary\n    return layer_files_by_partition\n\n", "CodeBleu_score": 34.46350212522356}]}
{"instance_id": "1b65ecb9be93", "generation_result": [{"code": "def system_info()\n\timport platform\n    import psutil\n    import sys\n    import os\n    import cpuinfo\n\n    info = {}\n    info['Operating System'] = platform.system()\n    info['Architecture'] = platform.machine()\n    info['Version'] = platform.version()\n    info['Processor'] = platform.processor()\n    info['Python Version'] = sys.version\n    info['CPU Count'] = psutil.cpu_count()\n    info['CPU Model'] = cpuinfo.get_cpu_info()['brand_raw']\n    info['RAM'] = str(round(psutil.virtual_memory().total / (1024.0 ** 3))) + \" GB\"\n    info['Disk'] = str(round(psutil.disk_usage('/').total / (1024.0 ** 3))) + \" GB\"\n    info['Python Version'] = sys.version\n\n    return info\n\n", "CodeBleu_score": 28.57315465203902}, {"code": "def system_info()\n\timport platform\n    import sys\n\n    return {\n        \"Operating System\": platform.system(),\n        \"Architecture\": platform.machine(),\n        \"Version\": platform.version(),\n        \"Processor\": platform.processor(),\n        \"Python Version\": sys.version\n    }\n\n\n", "CodeBleu_score": 60.42184138849749}, {"code": "def system_info()\n\timport platform\n    import sys\n\n    system_info = {\n        'Operating System': platform.system(),\n        'Architecture': platform.architecture(),\n        'Version': platform.version(),\n        'Processor': platform.processor(),\n        'Python Version': sys.version\n    }\n\n    return system_info\n\n", "CodeBleu_score": 37.14912630995032}]}
{"instance_id": "969bcc5cf3b5", "generation_result": [{"code": "def cuda_info()\n\timport torch\n    print(\"CUDA is available: \", torch.cuda.is_available())\n    print(\"CUDA version: \", torch.version.cuda)\n    print(\"List of GPU devices: \", torch.cuda.get_device_name(0))\n\n", "CodeBleu_score": 35.92005144085921}, {"code": "def cuda_info()\n\timport pynvml\n    pynvml.nvmlInit()\n    deviceCount = pynvml.nvmlDeviceGetCount()\n    print(\"Cuda version: \", pynvml.nvmlSystemGetCudaDriverVersion())\n    print(\"Cuda available: \", pynvml.nvmlSystemGetCudaDriverVersion())\n    print(\"GPU device names:\")\n    for i in range(deviceCount):\n        handle = pynvml.nvmlDeviceGetHandleByIndex(i)\n        print(pynvml.nvmlDeviceGetName(handle))\n    pynvml.nvmlShutdown()\n\n", "CodeBleu_score": 32.16131159824352}, {"code": "def cuda_info()\n\timport pynvml\n    from pynvml import *\n    import torch\n    pynvml.nvmlInit()\n    gpu_count = pynvml.nvmlDeviceGetCount()\n    gpu_info = []\n    for i in range(gpu_count):\n        handle = pynvml.nvmlDeviceGetHandleByIndex(i)\n        name = pynvml.nvmlDeviceGetName(handle)\n        gpu_info.append(name)\n    cuda_available = torch.cuda.is_available()\n    cuda_version = torch.version.cuda\n    return gpu_info, cuda_available, cuda_version\n\n", "CodeBleu_score": 36.837415344105075}]}
{"instance_id": "a2ba908a4c70", "generation_result": [{"code": "def compute_encoder_accuracy(dataset_items, encoder_manager)\n\t# Retrieve the class name key and optional class ID to class name mapping from the encoder manager's configuration\n    class_name_key = encoder_manager.get_class_name_key()\n    class_id_to_class_name_mapping = encoder_manager.get_class_id_to_class_name_mapping()\n\n    # Initialize the accuracy dictionary\n    accuracy = {}\n\n    # Iterate over the dataset\n    for item in dataset_items:\n        # Extract the audio file\n        audio_file = item[\"audio_file\"]\n\n        # Compute the embedding\n        embedding = encoder_manager.compute_embedding(audio_file)\n\n        # Predict the class label if criteria and mappings are available\n        predicted_class_name = encoder_manager.predict_class_name(embedding, class_name_key, class_id_to_class_name_mapping)\n\n        # Record the accuracy for each class by comparing the true class name with the predicted label\n        if predicted_class_name is not None:\n            true_class_name = item[class_name_key]\n            if true_class_name not in accuracy:\n                accuracy[true_class_name] = {}\n                accuracy[true_class_name][\"count\"] = 0\n                accuracy[true_class_name][\"correct\"] = 0\n            accuracy[true_class_name][\"count\"] += 1\n            if true_class_name == predicted_class_name:\n                accuracy[true_class_name][\"correct\"] += 1\n\n    # Calculate and print the accuracy for each class\n    for true_class_name in accuracy:\n        count = accuracy[true_class_name][\"count\"]\n        correct = accuracy[true_class_name][\"correct\"]\n        accuracy_ = correct / count\n        print(f\"{true_class_name}: {accuracy_}\")\n\n    # Calculate and print the average accuracy across all classes\n    accuracy_values = list(accuracy.values())\n    count_values = [accuracy_value[\"count\"] for accuracy_value in accuracy_values]\n    correct_values = [accuracy_value[\"correct\"]", "CodeBleu_score": 30.22519052183711}, {"code": "def compute_encoder_accuracy(dataset_items, encoder_manager)\n\tclass_name_key = encoder_manager.config[\"class_name_key\"]\n    class_id_to_class_name = encoder_manager.config.get(\"class_id_to_class_name\", None)\n\n    class_names = set()\n    class_accuracy = {}\n    for item in dataset_items:\n        class_names.add(item[class_name_key])\n\n    class_names = list(class_names)\n    class_names.sort()\n\n    for class_name in class_names:\n        class_accuracy[class_name] = {\"correct\": 0, \"total\": 0}\n\n    for item in dataset_items:\n        audio_file = item[\"audio_file\"]\n        predicted_class_name = encoder_manager.predict_class(audio_file)\n        true_class_name = item[class_name_key]\n\n        if predicted_class_name is not None:\n            class_accuracy[true_class_name][\"total\"] += 1\n            if class_id_to_class_name is not None:\n                predicted_class_name = class_id_to_class_name[predicted_class_name]\n            if predicted_class_name == true_class_name:\n                class_accuracy[true_class_name][\"correct\"] += 1\n\n    for class_name in class_names:\n        accuracy = class_accuracy[class_name][\"correct\"] / class_accuracy[class_name][\"total\"]\n        print(f\"Class '{class_name}' accuracy: {accuracy}\")\n\n    total_accuracy = 0\n    total_count = 0\n    for class_name in class_names:\n        accuracy = class_accuracy[class_name][\"correct\"] / class_accuracy[class_name][\"total\"]\n        total_accuracy += accuracy * class_accuracy[class_name][\"total\"]\n        total_count += class_accuracy[class_name][\"total\"]\n    print(f\"Total accuracy: {total_accuracy / total_count}\")\n\n", "CodeBleu_score": 32.153490507952405}, {"code": "def compute_encoder_accuracy(dataset_items, encoder_manager)\n\t# Get the class name key and optional class ID to class name mapping from the encoder manager's configuration\n    class_name_key = encoder_manager.config.class_name_key\n    class_id_to_name = encoder_manager.config.class_id_to_name\n\n    # Get the class name to class ID mapping from the encoder manager's configuration\n    class_name_to_id = encoder_manager.config.class_name_to_id\n\n    # Initialize the accuracy dictionary\n    accuracy = {}\n\n    # Loop over the dataset items\n    for dataset_item in dataset_items:\n        # Get the class name and class ID from the dataset item\n        class_name = dataset_item[class_name_key]\n        class_id = class_name_to_id[class_name]\n\n        # Get the audio file path from the dataset item\n        audio_file_path = dataset_item[\"audio_file_path\"]\n\n        # Compute the embedding\n        embedding = encoder_manager.compute_embedding(audio_file_path)\n\n        # Predict the class label if criteria and mappings are available\n        if class_id_to_name is not None and class_name_to_id is not None:\n            predicted_class_id = encoder_manager.predict_class_id(embedding)\n            predicted_class_name = class_id_to_name[predicted_class_id]\n        else:\n            predicted_class_name = None\n\n        # Record the accuracy for each class by comparing the true class name with the predicted label\n        if class_name not in accuracy:\n            accuracy[class_name] = {\"true_positives\": 0, \"false_positives\": 0, \"false_negatives\": 0}\n        if predicted_class_name == class_name:\n            accuracy[class_name][\"true_positives\"] += 1\n        elif predicted_class_name is not None:\n            accuracy[class_name][\"false_positives\"] += 1\n            accuracy[predicted_class_name][\"false_negatives\"] += 1\n\n    # Calculate and print the accuracy for", "CodeBleu_score": 27.20411622369674}]}
{"instance_id": "e01ec69ad58a", "generation_result": [{"code": "def adjust_path_and_remove_silence(audio_path)\n\t# get the output path\n    out_path = audio_path.replace(in_dir, out_dir)\n\n    # if force is not specified and the file already exists, return the path and False\n    if not force and os.path.exists(out_path):\n        print(f\"> \u2757 The file {out_path} already exists, skipping\")\n        return out_path, False\n\n    # if force is not specified and the file already exists, return the path and False\n    if not force and os.path.exists(out_path):\n        print(f\"> \u2757 The file {out_path} already exists, skipping\")\n        return out_path, False\n\n    # create the necessary directory structure\n    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n\n    # remove silence from the audio\n    out_path, is_speech = remove_silence(model_and_utils, audio_path, out_path, vad_sample_rate, trim_just_beginning_and_end, use_cuda)\n\n    # return the output path and is_speech flag\n    return out_path, is_speech\n\n\n", "CodeBleu_score": 34.87104662715533}, {"code": "def adjust_path_and_remove_silence(audio_path)\n\t# get the output path\n    audio_path = Path(audio_path)\n    out_path = Path(args.output_dir) / audio_path.relative_to(args.input_dir)\n    out_path.parent.mkdir(parents=True, exist_ok=True)\n\n    # remove silence from the audio\n    out_path, is_speech = remove_silence(model_and_utils, audio_path, out_path, use_cuda=args.use_cuda)\n    return out_path, is_speech\n\n", "CodeBleu_score": 20.904670094934307}, {"code": "def adjust_path_and_remove_silence(audio_path)\n\t# get the input and output paths\n    audio_path = Path(audio_path)\n    out_path = Path(audio_path.parent.replace(in_dir, out_dir)).joinpath(audio_path.name)\n    out_path.parent.mkdir(parents=True, exist_ok=True)\n\n    # if the file already exists and force is not specified, return the output path and a False flag\n    if out_path.exists() and not force:\n        return out_path, False\n\n    # remove silence from the audio using specified parameters\n    out_path, is_speech = remove_silence(model_and_utils, audio_path, out_path, vad_sample_rate, trim_just_beginning_and_end, use_cuda)\n\n    return out_path, is_speech\n\n", "CodeBleu_score": 27.54572684097291}]}
{"instance_id": "7e0bdb0feff8", "generation_result": [{"code": "def resample_files(input_dir, output_sr, output_dir=None, file_ext=\"wav\", n_jobs=10)\n\t# import modules\n    from tqdm import tqdm\n    from joblib import Parallel, delayed\n    from pydub import AudioSegment\n    import os\n\n    # if no output directory is provided, set it to the input directory\n    if output_dir is None:\n        output_dir = input_dir\n\n    # get all audio files in the input directory\n    audio_files = []\n    for root, dirs, files in os.walk(input_dir):\n        for file in files:\n            if file.endswith(file_ext):\n                audio_files.append(os.path.join(root, file))\n\n    # resample audio files in parallel\n    Parallel(n_jobs=n_jobs)(delayed(resample_file)(audio_file, output_sr, output_dir) for audio_file in tqdm(audio_files))\n\n    # print completion message\n    print(\"Audio files resampled successfully!\")\n\n", "CodeBleu_score": 29.14515414490714}, {"code": "def resample_files(input_dir, output_sr, output_dir=None, file_ext=\"wav\", n_jobs=10)\n\timport os\n    import soundfile as sf\n    from tqdm.auto import tqdm\n    from joblib import Parallel, delayed\n    import multiprocessing\n    import shutil\n\n    if output_dir is None:\n        output_dir = input_dir\n    else:\n        shutil.copytree(input_dir, output_dir)\n\n    audio_files = []\n    for root, dirs, files in os.walk(output_dir):\n        for file in files:\n            if file.endswith(file_ext):\n                audio_files.append(os.path.join(root, file))\n\n    def resample_file(file):\n        data, sr = sf.read(file)\n        sf.write(file, data, output_sr)\n\n    n_jobs = min(n_jobs, multiprocessing.cpu_count())\n    Parallel(n_jobs=n_jobs)(delayed(resample_file)(file) for file in tqdm(audio_files))\n    print(\"Resampling complete.\")", "CodeBleu_score": 35.190837379757646}, {"code": "def resample_files(input_dir, output_sr, output_dir=None, file_ext=\"wav\", n_jobs=10)\n\timport os\n    import librosa\n    from joblib import Parallel, delayed\n\n    if output_dir is None:\n        output_dir = input_dir\n\n    print(\"Copying input directory to output directory...\")\n    os.system(f\"cp -r {input_dir} {output_dir}\")\n\n    print(\"Finding audio files...\")\n    audio_files = []\n    for root, dirs, files in os.walk(output_dir):\n        for file in files:\n            if file.endswith(file_ext):\n                audio_files.append(os.path.join(root, file))\n\n    print(\"Resampling audio files...\")\n    Parallel(n_jobs=n_jobs)(delayed(resample_file)(audio_file, output_sr) for audio_file in audio_files)\n\n    print(\"Done!\")\n\n", "CodeBleu_score": 31.929279385855345}]}
{"instance_id": "a1351834ca57", "generation_result": [{"code": "def evaluation(model, criterion, data_loader, global_step)\n\tmodel.eval()\n    eval_loss = 0.0\n    for i, (inputs, labels) in enumerate(data_loader):\n        inputs = inputs.view(-1, inputs.shape[-1])\n        labels = labels.view(-1)\n        inputs, labels = inputs.to(device), labels.to(device)\n        with torch.no_grad():\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            eval_loss += loss.item()\n    eval_loss /= len(data_loader)\n    logger.info(\n        f\"Evaluation Loss at step {global_step} : {eval_loss:.6f} ", "CodeBleu_score": 23.11282586899457}, {"code": "def evaluation(model, criterion, data_loader, global_step)\n\tmodel.eval()\n    running_loss = 0.0\n    num_batches = len(data_loader)\n    for batch_idx, (inputs, labels) in enumerate(data_loader):\n        inputs = inputs.to(device)\n        labels = labels.to(device)\n        inputs = inputs.reshape(-1, 1, 1024)\n        labels = labels.reshape(-1, 1)\n        embeddings, outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        running_loss += loss.item()\n        if batch_idx == num_batches - 1:\n            num_classes_in_batch = len(np.unique(labels.cpu()))\n            embeddings = embeddings.detach().cpu().numpy()\n            fig = plot_embeddings(embeddings, num_classes_in_batch)\n            writer.add_figure(\"umap\", fig, global_step)\n    writer.add_scalar(\"eval_loss\", running_loss / num_batches, global_step)\n    return running_loss / num_batches\n\n", "CodeBleu_score": 32.08446689136568}, {"code": "def evaluation(model, criterion, data_loader, global_step)\n\tmodel.eval()\n    eval_loss = 0\n    with torch.no_grad():\n        for batch_idx, (data, target) in enumerate(data_loader):\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            loss = criterion(output, target)\n            eval_loss += loss.item()\n\n            # log embeddings and umap projection\n            if batch_idx == len(data_loader) - 1:\n                embeddings = model.get_embeddings(data)\n                num_utter_per_class = data.shape[0] // target.shape[0]\n                num_classes_in_batch = target.shape[0]\n                fig = plot_embeddings(embeddings, num_classes_in_batch)\n                writer.add_figure(\"umap\", fig, global_step)\n\n    eval_loss /= len(data_loader)\n    writer.add_scalar(\"Loss/Eval\", eval_loss, global_step)\n    print(\n        \"Evaluation Loss at step {}: {:.6f}\".format(global_step, eval_loss),\n        flush=True,\n    )\n    return eval_loss\n\n", "CodeBleu_score": 31.024735858891066}]}
{"instance_id": "565f8ed09916", "generation_result": [{"code": "def register_config(model_name: str) -> Coqpit\n\tconfig_class_name = to_camel(model_name) + \"Config\"\n    config_class = None\n    try:\n        config_class = find_module(\"configs\", config_class_name)\n    except ModuleNotFoundError:\n        try:\n            config_class = find_module(\"xtts.configs\", config_class_name)\n        except ModuleNotFoundError:\n            raise ModuleNotFoundError(\n                \"Could not find the configuration class for the model: {}\".format(\n                    model_name\n                )\n            )\n    return config_class\n\n", "CodeBleu_score": 28.215878059321764}, {"code": "def register_config(model_name: str) -> Coqpit\n\tconfig_class_name = to_camel(model_name) + \"Config\"\n    config_class = None\n    try:\n        config_class = find_module(\"configs\", config_class_name)\n    except ModuleNotFoundError:\n        try:\n            config_class = find_module(\"configs.xtts\", config_class_name)\n        except ModuleNotFoundError:\n            raise ModuleNotFoundError(\n                f\"Could not find a configuration class for the model '{model_name}'\"\n            )\n    return config_class\n\n", "CodeBleu_score": 29.701138527705474}, {"code": "def register_config(model_name: str) -> Coqpit\n\tconfig_class_name = to_camel(model_name) + \"Config\"\n    config_class = None\n    for module_path in CONFIG_MODULE_PATHS:\n        try:\n            config_class = find_module(module_path, config_class_name)\n            break\n        except ModuleNotFoundError:\n            continue\n    if config_class is None:\n        raise ModuleNotFoundError(\n            f\"Could not find a configuration class for model {model_name}\"\n        )\n    return config_class\n\n", "CodeBleu_score": 34.20432979760323}]}
{"instance_id": "0ea7862506a7", "generation_result": [{"code": "def load_config(config_path: str) -> Coqpit\n\tif config_path is None:\n        raise ValueError(\" [!] No config file path is given.\")\n    if not os.path.exists(config_path):\n        raise FileNotFoundError(f\" [!] Config file {config_path} does not exist.\")\n    ext = config_path.split(\".\")[-1]\n    if ext == \"json\":\n        with fsspec.open(config_path, \"r\", encoding=\"utf-8\") as f:\n            config_dict = json.load(f)\n    elif ext == \"yaml\":\n        with fsspec.open(config_path, \"r\", encoding=\"utf-8\") as f:\n            config_dict = yaml.load(f, Loader=yaml.FullLoader)\n    else:\n        raise ValueError(f\" [!] Config file {config_path} has unknown extension {ext}.\")\n    model_name = _process_model_name(config_dict)\n    config_class = register_config(model_name)\n    config = config_class()\n    config.load_from_dict(config_dict)\n    return config\n\n", "CodeBleu_score": 60.06334774178407}, {"code": "def load_config(config_path: str) -> Coqpit\n\tconfig_dict = {}\n    if config_path.endswith(\".json\"):\n        config_dict = read_json_with_comments(config_path)\n    elif config_path.endswith(\".yaml\"):\n        with fsspec.open(config_path, \"r\", encoding=\"utf-8\") as f:\n            config_dict = yaml.safe_load(f)\n    else:\n        raise ValueError(f\" [!] Unknown config extension: {config_path}\")\n\n    model_name = _process_model_name(config_dict)\n    config_class = register_config(model_name)\n    config = config_class()\n    config.load_from_dict(config_dict)\n    return config\n\n", "CodeBleu_score": 47.746020999547575}, {"code": "def load_config(config_path: str) -> Coqpit\n\tconfig_dict = load_config_from_file(config_path)\n    model_name = _process_model_name(config_dict)\n    config_class = register_config(model_name)\n    config = config_class()\n    config.load_from_dict(config_dict)\n    return config\n\n", "CodeBleu_score": 9.832093113017047}]}
{"instance_id": "e0db68b63143", "generation_result": [{"code": "def list_files(basePath, validExts=None, contains=None)\n\timport os\n\n    # walk through the base directory\n    for root, dirs, files in os.walk(basePath):\n        for file in files:\n            # check if the file contains the string\n            if contains is not None and contains not in file:\n                continue\n\n            # check the file extension\n            if validExts is not None and os.path.splitext(file)[1] not in validExts:\n                continue\n\n            # get the full file path\n            filePath = os.path.join(root, file)\n\n            # yield the full file path\n            yield filePath\n\n", "CodeBleu_score": 41.63588553877028}, {"code": "def list_files(basePath, validExts=None, contains=None)\n\timport os\n\n    # For all files, sub-directories in basePath\n    for root, dirs, files in os.walk(basePath):\n        for file in files:\n            # Check if the file contains the specified string\n            if contains is not None:\n                if contains in file:\n                    # Form the full file path\n                    fullPath = os.path.join(root, file)\n                    # Check if the file extension is valid\n                    if validExts is None:\n                        yield fullPath\n                    elif os.path.splitext(fullPath)[1] in validExts:\n                        yield fullPath\n            else:\n                # Form the full file path\n                fullPath = os.path.join(root, file)\n                # Check if the file extension is valid\n                if validExts is None:\n                    yield fullPath\n                elif os.path.splitext(fullPath)[1] in validExts:\n                    yield fullPath\n\n", "CodeBleu_score": 31.81193431994007}, {"code": "def list_files(basePath, validExts=None, contains=None)\n\timport os\n    for root, dirs, files in os.walk(basePath, topdown=True):\n        for name in files:\n            if contains is not None and contains not in name:\n                continue\n            if validExts is not None:\n                skip = True\n                for ext in validExts:\n                    if name.endswith(ext):\n                        skip = False\n                        break\n                if skip:\n                    continue\n            yield os.path.join(root, name)\n\n", "CodeBleu_score": 32.17458503041358}]}
{"instance_id": "b836fcaf0a27", "generation_result": [{"code": "def load_model(xtts_checkpoint, xtts_config, xtts_vocab)\n\t# clear the GPU cache\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n\n    # check if the required paths are provided\n    if xtts_checkpoint is None:\n        return \"XTTS checkpoint path is required.\"\n    if xtts_config is None:\n        return \"XTTS config path is required.\"\n    if xtts_vocab is None:\n        return \"XTTS vocab path is required.\"\n\n    # load the configuration from the provided path\n    config = load_config(xtts_config)\n\n    # initialize the XTTS model\n    model = XTTSModel(config)\n\n    # load the model checkpoint and vocabulary\n    model.load_checkpoint(\n        config,\n        checkpoint_path=xtts_checkpoint,\n        vocab_path=xtts_vocab,\n        eval=True,\n    )\n\n    # move the model to the GPU if available\n    if torch.cuda.is_available():\n        model.to(\"cuda\")\n\n    return model\n\n", "CodeBleu_score": 44.416245056394985}, {"code": "def load_model(xtts_checkpoint, xtts_config, xtts_vocab)\n\tclear_gpu_cache()\n    if xtts_checkpoint is None:\n        return \"Please provide a valid checkpoint path.\"\n    if xtts_config is None:\n        return \"Please provide a valid config path.\"\n    if xtts_vocab is None:\n        return \"Please provide a valid vocab path.\"\n    try:\n        model = XTTS_MODEL(xtts_config, xtts_checkpoint, xtts_vocab)\n    except:\n        return \"Error while loading the model.\"\n    if torch.cuda.is_available():\n        model.to(\"cuda\")\n    return \"Model loaded successfully.\"\n\n", "CodeBleu_score": 30.96378447394216}, {"code": "def load_model(xtts_checkpoint, xtts_config, xtts_vocab)\n\tclear_gpu_cache()\n\n    if xtts_checkpoint is None or xtts_config is None or xtts_vocab is None:\n        return \"Please provide the path to the checkpoint, config, and vocabulary files.\"\n\n    print(\"Loading the XTTS model...\")\n    XTTS_MODEL.load_checkpoint(xtts_config, xtts_checkpoint, xtts_vocab)\n\n    if torch.cuda.is_available():\n        XTTS_MODEL.cuda()\n\n    print(\"XTTS model loaded successfully.\")\n\n", "CodeBleu_score": 33.50077082877244}]}
{"instance_id": "c5d63aa0df26", "generation_result": [{"code": "def compute_embedding(self, x, num_frames=250, num_eval=10, return_mean=True, l2_norm=True)\n\t# compute number of frames\n    num_frames = int(num_frames * self.sample_rate / self.hop_length)\n    num_frames = min(num_frames, x.shape[1])\n    # compute offsets\n    offsets = np.linspace(0, x.shape[1] - num_frames, num_eval)\n    offsets = np.round(offsets).astype(int)\n    # extract frames and concatenate into batch\n    frames = [x[:, o : o + num_frames] for o in offsets]\n    frames = torch.cat(frames, dim=0)\n    # perform inference\n    with torch.no_grad():\n        embeddings = self.inference(frames)\n    # compute mean\n    if return_mean:\n        embeddings = torch.mean(embeddings, dim=0, keepdim=True)\n    # normalize\n    if l2_norm:\n        embeddings = torch.nn.functional.normalize(embeddings, dim=1)\n    return embeddings\n\n", "CodeBleu_score": 41.809826645493764}, {"code": "def compute_embedding(self, x, num_frames=250, num_eval=10, return_mean=True, l2_norm=True)\n\tif self.hop_length is not None:\n        num_frames = (num_frames - self.filter_length) / self.hop_length + 1\n        num_frames = int(num_frames)\n\n    if num_frames > x.shape[1]:\n        num_frames = x.shape[1]\n\n    eval_offsets = np.linspace(0, x.shape[1] - num_frames, num=num_eval)\n    eval_offsets = [int(offset) for offset in eval_offsets]\n\n    embeddings = []\n    for offset in eval_offsets:\n        x_frame = x[:, offset: offset + num_frames]\n        x_frame = torch.unsqueeze(x_frame, 0)\n        x_frame = x_frame.to(self.device)\n        embedding = self.inference(x_frame)\n        embeddings.append(embedding)\n\n    embeddings = torch.stack(embeddings)\n    embeddings = embeddings.to(self.device)\n    embeddings = embeddings.squeeze(1)\n\n    if return_mean:\n        embeddings = torch.mean(embeddings, dim=0)\n\n    if l2_norm:\n        embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=0)\n\n    return embeddings\n\n", "CodeBleu_score": 43.24818958673018}, {"code": "def compute_embedding(self, x, num_frames=250, num_eval=10, return_mean=True, l2_norm=True)\n\tif self.hop_length is not None:\n        num_frames = int(np.ceil(num_frames / self.hop_length))\n    num_frames = min(num_frames, x.shape[1])\n    offsets = np.linspace(0, x.shape[1] - num_frames, num_eval, dtype=int)\n    embeddings = []\n    for offset in offsets:\n        x_batch = x[:, offset : offset + num_frames]\n        embedding = self.inference(x_batch)\n        embeddings.append(embedding)\n    embeddings = torch.stack(embeddings, dim=0)\n    if return_mean:\n        embeddings = embeddings.mean(dim=0)\n        if l2_norm:\n            embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=0)\n    return embeddings\n\n", "CodeBleu_score": 47.2933175912433}]}
{"instance_id": "06f5b0335b26", "generation_result": [{"code": "def create_layer(self, block, planes, blocks, stride=1)\n\tdownsample = None\n    if stride != 1 or self.inplanes != planes * block.expansion:\n        downsample = nn.Sequential(\n            nn.Conv2d(self.inplanes, planes * block.expansion,\n                      kernel_size=1, stride=stride, bias=False),\n            nn.BatchNorm2d(planes * block.expansion),\n        )\n\n    layers = []\n    layers.append(block(self.inplanes, planes, stride, downsample))\n    self.inplanes = planes * block.expansion\n    for i in range(1, blocks):\n        layers.append(block(self.inplanes, planes))\n\n    return nn.Sequential(*layers)", "CodeBleu_score": 84.96307378168089}, {"code": "def create_layer(self, block, planes, blocks, stride=1)\n\tdownsample = None\n    if stride != 1 or self.inplanes != planes * block.expansion:\n        downsample = nn.Sequential(\n            nn.Conv2d(self.inplanes, planes * block.expansion,\n                      kernel_size=1, stride=stride, bias=False),\n            nn.BatchNorm2d(planes * block.expansion),\n        )\n\n    layers = []\n    layers.append(block(self.inplanes, planes, stride, downsample))\n    self.inplanes = planes * block.expansion\n    for i in range(1, blocks):\n        layers.append(block(self.inplanes, planes))\n\n    return nn.Sequential(*layers)\n\n", "CodeBleu_score": 84.96307378168089}, {"code": "def create_layer(self, block, planes, blocks, stride=1)\n\tdownsample = None\n    if stride != 1 or self.inplanes != planes * block.expansion:\n        downsample = nn.Sequential(\n            nn.Conv2d(self.inplanes, planes * block.expansion,\n                      kernel_size=1, stride=stride, bias=False),\n            nn.BatchNorm2d(planes * block.expansion),\n        )\n\n    layers = []\n    layers.append(block(self.inplanes, planes, stride, downsample))\n    self.inplanes = planes * block.expansion\n    for i in range(1, blocks):\n        layers.append(block(self.inplanes, planes))\n\n    return nn.Sequential(*layers)\n\n\n", "CodeBleu_score": 84.96307378168089}]}
{"instance_id": "9ac60597b633", "generation_result": [{"code": "def plot_embeddings(embeddings, num_classes_in_batch)\n\timport numpy as np\n    import matplotlib.pyplot as plt\n    from umap import UMAP\n    from sklearn.manifold import TSNE\n    from sklearn.decomposition import PCA\n    import umap.plot\n    from matplotlib.colors import ListedColormap\n    from matplotlib.colors import BoundaryNorm\n    from matplotlib.ticker import MaxNLocator\n    from matplotlib.ticker import StrMethodFormatter\n    import matplotlib.ticker as ticker\n    from matplotlib.ticker import LogLocator\n    from matplotlib.ticker import LogFormatter\n    from matplotlib.ticker import LogFormatterExponent\n    from matplotlib.ticker import LogFormatterMathtext\n    from matplotlib.ticker import LogFormatterSciNotation\n    from matplotlib.ticker import LogLocator\n    from matplotlib.ticker import NullLocator\n    from matplotlib.ticker import NullFormatter\n    from matplotlib.ticker import ScalarFormatter\n    from matplotlib.ticker import FixedLocator\n    from matplotlib.ticker import FixedFormatter\n    from matplotlib.ticker import FormatStrFormatter\n    from matplotlib.ticker import PercentFormatter\n    from matplotlib.ticker import LogitFormatter\n    from matplotlib.ticker import EngFormatter\n    from matplotlib.ticker import FuncFormatter\n    from matplotlib.ticker import MultipleLocator\n    from matplotlib.ticker import AutoMinorLocator\n    from matplotlib.ticker import SymmetricalLogLocator\n    from matplotlib.ticker import LogitLocator\n    from matplotlib.ticker import LogLocator\n    from matplotlib.ticker import AutoMinorLocator\n    from matplotlib.ticker import MaxNLocator\n    from matplotlib.ticker import AutoMinorLocator\n    from matplotlib.ticker import MultipleLocator\n    from matplotlib.ticker import FixedLocator\n    from matplotlib.ticker import FixedFormatter\n    from matplotlib.ticker import FormatStrFormatter\n    from matplotlib.ticker import NullFormatter\n    from matplotlib.ticker import NullLocator\n    from matplotlib.ticker import ScalarFormatter\n    from matplotlib.ticker import LogFormatter\n    from matplotlib.ticker import LogFormatterExponent\n    from matplotlib.ticker import LogFormatterMathtext\n    from matplotlib.ticker import Log", "CodeBleu_score": 6.484891416757196}, {"code": "def plot_embeddings(embeddings, num_classes_in_batch)\n\timport matplotlib.pyplot as plt\n    from sklearn.manifold import TSNE\n    from umap import UMAP\n    from sklearn.decomposition import PCA\n    import numpy as np\n\n    # Calculate the number of utterances per class\n    num_utterances_per_class = np.zeros(num_classes_in_batch)\n    for i in range(len(embeddings)):\n        num_utterances_per_class[i] = len(embeddings[i])\n\n    # Limit the number of classes to 10\n    if num_classes_in_batch > 10:\n        num_classes_in_batch = 10\n\n    # Transform the embeddings\n    embeddings = np.concatenate(embeddings)\n    umap = UMAP(n_neighbors=10, min_dist=0.1, n_components=2, metric='cosine')\n    embeddings_transformed = umap.fit_transform(embeddings)\n\n    # Create a scatter plot with colors representing different classes\n    fig, ax = plt.subplots()\n    colors = ['#1f78b4', '#33a02c', '#e31a1c', '#ff7f00', '#6a3d9a', '#b15928', '#a6cee3', '#b2df8a', '#fb9a99', '#fdbf6f']\n    for i in range(num_classes_in_batch):\n        ax.scatter(embeddings_transformed[np.arange(len(embeddings))[num_utterances_per_class == i], 0],\n                   embeddings_transformed[np.arange(len(embeddings))[num_utterances_per_class == i], 1],\n                   s=1, alpha=0.5, color=colors[i])\n\n    # Adjust for equal aspect ratio and save the plot\n    ax.set_aspect('equal', 'datalim')\n    ax.set_title('UMAP projection')\n    fig.savefig('umap.png", "CodeBleu_score": 41.74824229950642}, {"code": "def plot_embeddings(embeddings, num_classes_in_batch)\n\timport matplotlib.pyplot as plt\n    from sklearn.manifold import TSNE\n    from sklearn.manifold import MDS\n    from sklearn.manifold import Isomap\n    from sklearn.manifold import SpectralEmbedding\n    from sklearn.manifold import LocallyLinearEmbedding\n    from sklearn.manifold import MDS\n    from sklearn.decomposition import PCA\n    from sklearn.manifold import TSNE\n    from sklearn.manifold import MDS\n    from sklearn.manifold import Isomap\n    from sklearn.manifold import SpectralEmbedding\n    from sklearn.manifold import LocallyLinearEmbedding\n    from sklearn.manifold import MDS\n    from sklearn.decomposition import PCA\n    from umap import UMAP\n    import seaborn as sns\n    import numpy as np\n\n    # Calculate the number of utterances per class\n    num_utterances_per_class = [0] * num_classes_in_batch\n    for i in range(len(embeddings)):\n        num_utterances_per_class[embeddings[i][1]] += 1\n\n    # Limit the number of classes to 10 if necessary\n    if len(num_utterances_per_class) > 10:\n        num_utterances_per_class = num_utterances_per_class[:10]\n        embeddings = [embeddings[i] for i in range(len(embeddings)) if embeddings[i][1] < 10]\n\n    # Transform the embeddings\n    umap = UMAP(n_neighbors=20, min_dist=0.01, metric='cosine', random_state=42)\n    embeddings_transformed = umap.fit_transform(embeddings)\n\n    # Create a scatter plot with colors representing different classes\n    plt.figure(figsize=(10, 10))\n    plt.scatter(embeddings_transformed[:, 0], embeddings_transformed[:, 1], c", "CodeBleu_score": 29.677548585178137}]}
{"instance_id": "19c2b4af256d", "generation_result": [{"code": "def calc_cosine_sim(self, dvecs, centroids)\n\tcos_sim_matrix = []\n    for spkr, utts in enumerate(self.utt_per_spkr):\n        for utt in range(utts):\n            new_centroids = self.calc_new_centroids(dvecs, centroids, spkr, utt)\n            cos_sim = torch.nn.functional.cosine_similarity(\n                dvecs[spkr, utt], new_centroids, dim=1\n            )\n            cos_sim = torch.clamp(cos_sim, min=self.eps)\n            cos_sim_matrix.append(cos_sim)\n    return torch.stack(cos_sim_matrix)\n\n", "CodeBleu_score": 36.26668879202284}, {"code": "def calc_cosine_sim(self, dvecs, centroids)\n\tcos_sim_matrix = []\n    for spkr in range(dvecs.shape[0]):\n        new_centroids = self.calc_new_centroids(dvecs, centroids, spkr)\n        cos_sim = []\n        for utt in range(dvecs.shape[1]):\n            cos_sim.append(\n                torch.nn.functional.cosine_similarity(\n                    dvecs[spkr, utt], new_centroids, dim=0\n                )\n            )\n        cos_sim = torch.stack(cos_sim)\n        cos_sim = torch.clamp(cos_sim, min=self.eps)\n        cos_sim_matrix.append(cos_sim)\n    cos_sim_matrix = torch.stack(cos_sim_matrix)\n    return cos_sim_matrix\n\n", "CodeBleu_score": 38.21236173314889}, {"code": "def calc_cosine_sim(self, dvecs, centroids)\n\tcos_sim_matrix = []\n    for spkr in range(dvecs.shape[0]):\n        new_centroids = self.calc_new_centroids(dvecs, centroids, spkr)\n        cos_sim_matrix.append(\n            torch.clamp(\n                torch.cosine_similarity(\n                    dvecs[spkr], new_centroids, dim=-1, eps=1e-6\n                ),\n                min=0,\n            )\n        )\n    return torch.stack(cos_sim_matrix)\n\n", "CodeBleu_score": 28.198171249692855}]}
{"instance_id": "c525b411a1e7", "generation_result": [{"code": " def embed_loss_softmax(self, dvecs, cos_sim_matrix)\n\t\"\"\"\n    Args:\n        dvecs: The embeddings of the dataset.\n        cos_sim_matrix: The cosine similarity matrix.\n\n    Returns:\n        The embedding loss.\n    \"\"\"\n\n    \"\"\"\n    loss = 0\n    for dvec in dvecs:\n        loss += F.log_softmax(cos_sim_matrix[dvec], dim=0)\n\n    return -loss\n\n\n\n def embed_loss_contrastive(self, dvecs, cos_sim_matrix):\n    \"\"\"The function calculates the embedding loss using contrastive loss.", "CodeBleu_score": 11.523218914629227}, {"code": " def embed_loss_softmax(self, dvecs, cos_sim_matrix)\n\tlosses = []\n    for dvec in dvecs:\n        loss = -F.log_softmax(cos_sim_matrix[dvec], dim=0)\n        losses.append(loss)\n    return torch.stack(losses)\n\n\n", "CodeBleu_score": 25.760282366643477}, {"code": " def embed_loss_softmax(self, dvecs, cos_sim_matrix)\n\tbatch_size = dvecs.size(0)\n    cos_sim_matrix = cos_sim_matrix.float()\n    # dvecs = dvecs.float()\n    # dvecs = dvecs.view(batch_size, 1, -1)\n    # cos_sim_matrix = cos_sim_matrix.view(batch_size, -1)\n    # cos_sim_matrix = torch.bmm(dvecs, dvecs.transpose(1, 2))\n    # cos_sim_matrix = cos_sim_matrix.view(batch_size, -1)\n    cos_sim_matrix = F.softmax(cos_sim_matrix, dim=1)\n    # print(cos_sim_matrix.shape)\n    # print(cos_sim_matrix)\n    # print(cos_sim_matrix.sum(dim=1))\n    # print(cos_sim_matrix.sum(dim=1).shape)\n    # print(torch.diagonal(cos_sim_matrix, dim1=0, dim2=1))\n    # print(torch.diagonal(cos_sim_matrix, dim1=0, dim2=1).shape)\n    # print(torch.diagonal(cos_sim_matrix, dim1=0, dim2=1).sum(dim=1))\n    # print(torch.diagonal(cos_sim_matrix, dim1=0, dim2=1).sum(dim=1).shape)\n    # print(torch.diagonal(cos_sim_matrix, dim1=0, dim2=1).sum(dim=1).view(batch_size, 1))\n    # print(torch.diagonal(cos_sim_matrix, dim1=0, dim2=1).sum(dim=1).view(batch_size, 1).shape)\n    # print(cos_sim_matrix - torch.diagonal(cos_sim_matrix, dim1=0, dim2=1).sum(dim=1).view(batch_size, 1))\n    # print(cos_sim_matrix - torch.diagonal(cos", "CodeBleu_score": 6.506659404326001}]}
{"instance_id": "25ac3945da2e", "generation_result": [{"code": "def embed_loss_contrast(self, dvecs, cos_sim_matrix)\n\tloss_row = []\n    for i in range(dvecs.shape[0]):\n        # exclude the current centroid\n        cos_sim_matrix_i = cos_sim_matrix[i].clone()\n        cos_sim_matrix_i[i] = -100000\n        # compute the contrast loss\n        loss_row.append(self.contrast_loss(dvecs[i], cos_sim_matrix_i))\n    return torch.stack(loss_row)\n\n", "CodeBleu_score": 20.412094342261867}, {"code": "def embed_loss_contrast(self, dvecs, cos_sim_matrix)\n\tlosses = []\n    for i, dvec in enumerate(dvecs):\n        # Exclude current centroid\n        cos_sim_matrix_i = cos_sim_matrix[i]\n        cos_sim_matrix_i[i] = -1\n        # Compute contrast loss\n        losses.append(self.contrast_loss(dvec, cos_sim_matrix_i))\n    return torch.stack(losses)\n\n", "CodeBleu_score": 16.457025679063694}, {"code": "def embed_loss_contrast(self, dvecs, cos_sim_matrix)\n\t# cos_sim_matrix = torch.matmul(dvecs, dvecs.t())\n    # cos_sim_matrix = torch.div(cos_sim_matrix, self.temperature)\n    # cos_sim_matrix = torch.exp(cos_sim_matrix)\n    # cos_sim_matrix = torch.div(cos_sim_matrix, torch.sum(cos_sim_matrix, dim=1).unsqueeze(1))\n    # cos_sim_matrix = torch.div(cos_sim_matrix, torch.sum(cos_sim_matrix, dim=1).unsqueeze(1))\n    # cos_sim_matrix = cos_sim_matrix.t()\n    # print(cos_sim_matrix.shape)\n    # print(cos_sim_matrix)\n    # print(torch.argmax(cos_sim_matrix, dim=1))\n    # print(torch.argmax(cos_sim_matrix, dim=1).shape)\n    # print(torch.argmax(cos_sim_matrix, dim=1).unsqueeze(1).shape)\n    # print(cos_sim_matrix.gather(1, torch.argmax(cos_sim_matrix, dim=1).unsqueeze(1)))\n    # print(cos_sim_matrix.gather(1, torch.argmax(cos_sim_matrix, dim=1).unsqueeze(1)).shape)\n    # print(cos_sim_matrix.gather(1, torch.argmax(cos_sim_matrix, dim=1).unsqueeze(1)).squeeze(1).shape)\n    # print(cos_sim_matrix.gather(1, torch.argmax(cos_sim_matrix, dim=1).unsqueeze(1)).squeeze(1))\n    # print(cos_sim_matrix.gather(1, torch.argmax(cos_sim_matrix, dim=1).unsqueeze(1)).squeeze(1).unsqueeze(1).shape)\n    # print(cos_sim_matrix.gather(", "CodeBleu_score": 1.3221153846153846}]}
{"instance_id": "878d14ffabd2", "generation_result": [{"code": "def forward(self, x, _label=None)\n\t# x: (num_speakers, num_utts_per_speaker, dvec_feats)\n    # label: (num_speakers, num_utts_per_speaker)\n    # cos_sim: (num_speakers, num_utts_per_speaker, num_speakers)\n    # cos_sim_scaled: (num_speakers, num_utts_per_speaker, num_speakers)\n    # cos_sim_scaled_shifted: (num_speakers, num_utts_per_speaker, num_speakers)\n    # loss: (num_speakers, num_utts_per_speaker)\n    # loss_mean: ()\n    x = self.calc_cosine_sim(x, self.calc_centroids(x))\n    x = self.calc_cosine_sim(x, self.calc_centroids(x))\n    x = self.calc_cosine_sim(x, self.calc_centroids(x))\n    x = self.calc_cosine_sim(x, self.calc_centroids(x))\n    x = self.calc_cosine_sim(x, self.calc_centroids(x))\n    x = self.calc_cosine_sim(x, self.calc_centroids(x))\n    x = self.calc_cosine_sim(x, self.calc_centroids(x))\n    x = self.calc_cosine_sim(x, self.calc_centroids(x))\n    x = self.calc_cosine_sim(x, self.calc_centroids(x))\n    x = self.calc_cosine_sim(x, self.calc_centroids(x))\n    x = self.calc_cosine_sim(x, self.calc_centroids(x))\n    x = self.calc_cosine_sim(x, self.calc_centroids(x))\n    x = self.calc_cosine_sim(x, self.calc_centroids(x))\n    x = self.calc_cosine_sim(x, self.", "CodeBleu_score": 16.198921176013403}, {"code": "def forward(self, x, _label=None)\n\t# check for the correct dimensions\n    if x.ndim != 3:\n        raise ValueError(f\"Expected 3 dimensions for x, got {x.ndim}\")\n    # check for at least 2 utterances per speaker\n    if x.shape[1] < 2:\n        raise ValueError(f\"Expected at least 2 utterances per speaker, got {x.shape[1]}\")\n\n    # calculate the cosine similarity matrix\n    cos_sim_matrix = self.calc_cosine_sim(x, self.centroids)\n\n    # scale and shift the matrix\n    scaled_sim_matrix = (cos_sim_matrix * self.scale) - 1\n\n    # calculate the loss\n    loss = torch.mean(torch.log(torch.sum(torch.exp(scaled_sim_matrix), dim=2)))\n\n    return loss\n\n", "CodeBleu_score": 33.09608281248926}, {"code": "def forward(self, x, _label=None)\n\t# check for at least two utterances per speaker\n    assert x.shape[1] > 1\n\n    # compute centroids\n    centroids = self.calc_centroids(x)\n\n    # compute cosine similarity matrix\n    cos_sim_matrix = self.calc_cosine_sim(x, centroids)\n\n    # scale and shift the cosine similarity matrix\n    scaled_cos_sim_matrix = self.scale_and_shift_matrix(cos_sim_matrix)\n\n    # compute the embedding loss\n    embedding_loss = self.calc_embedding_loss(scaled_cos_sim_matrix)\n\n    # compute the mean loss\n    mean_loss = self.calc_mean_loss(embedding_loss)\n\n    return mean_loss", "CodeBleu_score": 44.46113817617855}]}
{"instance_id": "86a364c72caa", "generation_result": [{"code": "def get_voices(extra_voice_dirs: List[str] = [])\n\tvoices = {}\n    for voice_dir in extra_voice_dirs:\n        for root, dirs, files in os.walk(voice_dir):\n            for file in files:\n                if file.endswith(\".npz\"):\n                    if root not in voices:\n                        voices[root] = []\n                    voices[root].append(os.path.join(root, file))\n                elif file.endswith(\".wav\") or file.endswith(\".mp3\"):\n                    if root not in voices:\n                        voices[root] = []\n                    voices[root].append(os.path.join(root, file))\n    return voices\n\n", "CodeBleu_score": 28.19472393734901}, {"code": "def get_voices(extra_voice_dirs: List[str] = [])\n\tvoices = {}\n    for voice_dir in [*extra_voice_dirs, *[os.path.join(os.path.dirname(__file__), \"voices\")]]:\n        for root, dirs, files in os.walk(voice_dir):\n            for file in files:\n                if file.endswith(\".npz\"):\n                    if root not in voices:\n                        voices[root] = []\n                    voices[root].append(os.path.join(root, file))\n                elif file.endswith(\".wav\") or file.endswith(\".mp3\"):\n                    if root not in voices:\n                        voices[root] = []\n                    voices[root].append(os.path.join(root, file))\n    return voices\n\n", "CodeBleu_score": 27.76429051030519}, {"code": "def get_voices(extra_voice_dirs: List[str] = [])\n\tvoices = {}\n    for voice_dir in extra_voice_dirs:\n        for subdir, dirs, files in os.walk(voice_dir):\n            voice_files = []\n            for file in files:\n                if file.endswith(\".npz\") or file.endswith(\".wav\") or file.endswith(\".mp3\"):\n                    voice_files.append(os.path.join(subdir, file))\n            if voice_files:\n                voices[subdir] = voice_files\n    return voices\n\n", "CodeBleu_score": 27.919420227345903}]}
{"instance_id": "02f20c8917e6", "generation_result": [{"code": "def load_voice(model, voice: str, extra_voice_dirs: List[str] = [])\n\tif voice == \"random\":\n        return None, None, None\n\n    voices = get_voices(extra_voice_dirs)\n    paths = voices[voice]\n\n    # bark only uses a single sample for cloning\n    if len(paths) > 1:\n        raise ValueError(f\"Voice {voice} has multiple paths: {paths}\")\n\n    try:\n        path = voices[voice]\n    except KeyError as e:\n        raise KeyError(f\"Voice {voice} not found in {extra_voice_dirs}\") from e\n\n    if len(paths) == 1 and paths[0].endswith(\".npz\"):\n        return load_npz(path[0])\n\n    audio_path = paths[0]\n    # replace the file extension with .npz\n    output_path = os.path.splitext(audio_path)[0] + \".npz\"\n    generate_voice(audio=audio_path, model=model, output_path=output_path)\n    return load_voice(model, voice, extra_voice_dirs)", "CodeBleu_score": 96.01808764220647}, {"code": "def load_voice(model, voice: str, extra_voice_dirs: List[str] = [])\n\tif voice == \"random\":\n        return None, None, None\n\n    voices = get_voices(extra_voice_dirs)\n    paths = voices[voice]\n\n    # bark only uses a single sample for cloning\n    if len(paths) > 1:\n        raise ValueError(f\"Voice {voice} has multiple paths: {paths}\")\n\n    try:\n        path = voices[voice]\n    except KeyError as e:\n        raise KeyError(f\"Voice {voice} not found in {extra_voice_dirs}\") from e\n\n    if len(paths) == 1 and paths[0].endswith(\".npz\"):\n        return load_npz(path[0])\n\n    audio_path = paths[0]\n    # replace the file extension with .npz\n    output_path = os.path.splitext(audio_path)[0] + \".npz\"\n    generate_voice(audio=audio_path, model=model, output_path=output_path)\n    return load_voice(model, voice, extra_voice_dirs)\n\n", "CodeBleu_score": 96.01808764220647}, {"code": "def load_voice(model, voice: str, extra_voice_dirs: List[str] = [])\n\tif voice == \"random\":\n        return None, None, None\n\n    voices = get_voices(extra_voice_dirs)\n    paths = voices[voice]\n\n    # bark only uses a single sample for cloning\n    if len(paths) > 1:\n        raise ValueError(f\"Voice {voice} has multiple paths: {paths}\")\n\n    try:\n        path = voices[voice]\n    except KeyError as e:\n        raise KeyError(f\"Voice {voice} not found in {extra_voice_dirs}\") from e\n\n    if len(paths) == 1 and paths[0].endswith(\".npz\"):\n        return load_npz(path[0])\n\n    audio_path = paths[0]\n    # replace the file extension with .npz\n    output_path = os.path.splitext(audio_path)[0] + \".npz\"\n    generate_voice(audio=audio_path, model=model, output_path=output_path)\n    return load_voice(model, voice, extra_voice_dirs)\n\n", "CodeBleu_score": 96.01808764220647}]}
{"instance_id": "9458e981389f", "generation_result": [{"code": "def _relative_shift(self, pos_score: torch.Tensor) -> torch.Tensor\n\tbatch_size, num_heads, seq_len, _ = pos_score.size()\n    zero_tensor = torch.zeros((batch_size, num_heads, seq_len, 1), device=pos_score.device, dtype=pos_score.dtype)\n    pos_score = torch.cat((zero_tensor, pos_score), dim=-1)\n    pos_score = pos_score.view(batch_size, num_heads, seq_len * 2, self.d_model // self.num_heads)\n    pos_score = pos_score[:, :, :seq_len, :]\n    return pos_score\n\n", "CodeBleu_score": 34.273350148636275}, {"code": "def _relative_shift(self, pos_score: torch.Tensor) -> torch.Tensor\n\tbatch_size, num_heads, seq_len, _ = pos_score.shape\n    zero_tensor = torch.zeros((batch_size, num_heads, seq_len, 1), device=self.device, dtype=self.dtype)\n    concat_tensor = torch.cat((zero_tensor, pos_score), -1)\n    reshaped_tensor = concat_tensor.reshape(batch_size, num_heads, 2 * seq_len, -1)\n    sliced_tensor = reshaped_tensor[:, :, seq_len:, :]\n    return sliced_tensor", "CodeBleu_score": 26.750923360072278}, {"code": "def _relative_shift(self, pos_score: torch.Tensor) -> torch.Tensor\n\tbatch_size, num_heads, seq_length, _ = pos_score.size()\n\n    zeros_shape = (batch_size, num_heads, seq_length, 1)\n    zeros = pos_score.new_zeros(zeros_shape)\n\n    shifted_pos_score = torch.cat([zeros, pos_score], dim=-1)\n    shifted_pos_score = shifted_pos_score.view(batch_size, num_heads, seq_length * 2, -1)\n    shifted_pos_score = shifted_pos_score[:, :, :seq_length, :]\n\n    return shifted_pos_score\n\n", "CodeBleu_score": 35.74182502017905}]}
{"instance_id": "c046d4f00e80", "generation_result": [{"code": "def run_padded_sequence(self, context, lens)\n\t# extract context up to the specified length\n    context = context[:, :lens, :]\n    # apply convolutional layers with ReLU activation and dropout\n    context = self.conv1(context)\n    context = self.relu(context)\n    context = self.dropout(context)\n    context = self.conv2(context)\n    context = self.relu(context)\n    context = self.dropout(context)\n    # pad the context to create a consistent batch size\n    context = self.pad_sequence(context)\n    return context\n\n", "CodeBleu_score": 18.951314174797464}, {"code": "def run_padded_sequence(self, context, lens)\n\t# Extract the context up to the specified length\n    context = context[:, :lens.max(), :]\n    # Apply convolutional layers\n    for i in range(len(self.convolutions)):\n        context = self.convolutions[i](context)\n        context = self.relu(context)\n        context = self.dropout(context)\n    # Pad the sequence to create a consistent batch size\n    context = self.pad_sequence(context, lens)\n    return context\n\n", "CodeBleu_score": 25.235820599245717}, {"code": "def run_padded_sequence(self, context, lens)\n\t# Extract context up to the specified length\n    context = context[:, :lens]\n    # Apply convolutional layers with ReLU activation and dropout\n    context = self.conv1(context)\n    context = self.relu1(context)\n    context = self.dropout1(context)\n    context = self.conv2(context)\n    context = self.relu2(context)\n    context = self.dropout2(context)\n    # Pad the contexts to create a consistent batch size\n    context = nn.utils.rnn.pad_sequence(context.unbind(), batch_first=True)\n    return context\n\n", "CodeBleu_score": 28.003941482444695}]}
{"instance_id": "db2644f3c681", "generation_result": [{"code": "def run_unsorted_inputs(self, fn, context, lens)\n\t# Sort the lengths and indices\n    lengths, indices = zip(*sorted(zip(lens, range(len(lens))), reverse=True))\n\n    # Pack the context sequences\n    context_packed = pack_padded_sequence(context, lengths, batch_first=True)\n\n    # Apply the function\n    context_packed = fn(context_packed)\n\n    # Unpack the sequences\n    context_unpacked, lengths_unpacked = pad_packed_sequence(context_packed, batch_first=True)\n\n    # Map the results back to the original indices\n    indices_unsorted = [indices[i] for i in indices]\n    context_unsorted = context_unpacked[indices_unsorted]\n\n    return context_unsorted", "CodeBleu_score": 25.32704123910702}, {"code": "def run_unsorted_inputs(self, fn, context, lens)\n\tsorted_lens, indices = torch.sort(lens, descending=True)\n    context = context[indices]\n    context_packed = nn.utils.rnn.pack_padded_sequence(context, sorted_lens, batch_first=True)\n    context_packed_out, _ = fn(context_packed)\n    context_unpacked, _ = nn.utils.rnn.pad_packed_sequence(context_packed_out, batch_first=True)\n    _, unsort_indices = torch.sort(indices)\n    return context_unpacked[unsort_indices]\n\n\n", "CodeBleu_score": 17.434373363924674}, {"code": "def run_unsorted_inputs(self, fn, context, lens)\n\tsorted_lens, indices = torch.sort(lens, descending=True)\n    sorted_context = context.index_select(0, indices)\n    sorted_context = sorted_context.split(sorted_lens.tolist())\n    packed_context = rnn_utils.pack_sequence(sorted_context)\n    packed_output, _ = fn(packed_context)\n    sorted_output, _ = rnn_utils.pad_packed_sequence(packed_output)\n    unsorted_output = sorted_output.index_select(0, indices)\n    return unsorted_output", "CodeBleu_score": 15.069665150619272}]}
{"instance_id": "ced082bffc63", "generation_result": [{"code": "def remove_weight_norm(self)\n\tself.kernel_predictor.remove_weight_norm()\n    parametrize.remove_parametrizations(self.convt_pre[1], \"weight\")\n    for block in self.conv_blocks:\n        parametrize.remove_parametrizations(block[1], \"weight\")", "CodeBleu_score": 69.598046653168}, {"code": "def remove_weight_norm(self)\n\tself.kernel_predictor.remove_weight_norm()\n    parametrize.remove_parametrizations(self.convt_pre[1], \"weight\")\n    for block in self.conv_blocks:\n        parametrize.remove_parametrizations(block[1], \"weight\")", "CodeBleu_score": 69.598046653168}, {"code": "def remove_weight_norm(self)\n\tself.kernel_predictor.remove_weight_norm()\n    parametrize.remove_parametrizations(self.convt_pre[1], \"weight\")\n    for block in self.conv_blocks:\n        parametrize.remove_parametrizations(block[1], \"weight\")\n\n\n", "CodeBleu_score": 69.598046653168}]}
{"instance_id": "b5a39fd97061", "generation_result": [{"code": "def map_mx_to_torch_model(mx_name)\n\tmx_name = mx_name.lower()\n    mx_name = mx_name.replace('se_', 'se-')\n    mx_name = mx_name.replace('senet_', 'se-')\n    mx_name = mx_name.replace('inceptionv3', 'inception-v3')\n    return 'gluon_' + mx_name\n\n", "CodeBleu_score": 30.672634216927726}, {"code": "def map_mx_to_torch_model(mx_name)\n\t# Convert to lowercase\n    name = mx_name.lower()\n    # Replace 'se_' with 'squeeze_excitation_'\n    name = name.replace('se_', 'squeeze_excitation_')\n    # Replace 'senet_' with 'se_'\n    name = name.replace('senet_', 'se_')\n    # Replace 'inceptionv3' with 'inception_v3'\n    name = name.replace('inceptionv3', 'inception_v3')\n    # Prefix with 'gluon_'\n    name = 'gluon_' + name\n    return name\n\n", "CodeBleu_score": 34.542970267644904}, {"code": "def map_mx_to_torch_model(mx_name)\n\tmx_name = mx_name.lower()\n    mx_name = mx_name.replace('se_', 'se-')\n    mx_name = mx_name.replace('inceptionv3', 'inception-v3')\n    mx_name = mx_name.replace('senet_', 'se-')\n    return 'gluon_' + mx_name\n", "CodeBleu_score": 30.672634216927726}]}
{"instance_id": "7eee5efdbf67", "generation_result": [{"code": "def _num_samples_per_worker(self)\n\tif self.repeats > 0:\n        num_samples = self.repeats\n    else:\n        num_samples = self.total_samples\n    num_samples = num_samples // self.num_workers\n    if self.training:\n        num_samples = num_samples // self.num_replicas\n    if self.batch_size > 0:\n        num_samples = (num_samples // self.batch_size) * self.batch_size\n    return num_samples\n\n", "CodeBleu_score": 32.059573760630045}, {"code": "def _num_samples_per_worker(self)\n\tif self.repeats > 1:\n        num_samples_per_worker = int(np.ceil(self.num_samples / self.repeats))\n    else:\n        num_samples_per_worker = int(np.ceil(self.num_samples / self.num_workers))\n\n    if self.training:\n        num_samples_per_worker = int(np.ceil(num_samples_per_worker / self.num_replicas))\n\n    if self.batch_size > 0:\n        num_samples_per_worker = int(np.ceil(num_samples_per_worker / self.batch_size) * self.batch_size)\n\n    return num_samples_per_worker\n\n", "CodeBleu_score": 41.01461895279087}, {"code": "def _num_samples_per_worker(self)\n\tnum_workers = self.num_workers\n    num_replicas = self.num_replicas\n    num_samples = self.num_samples\n    batch_size = self.batch_size\n    repeats = self.repeats\n\n    if num_samples == 0:\n        return 0\n\n    if num_replicas > 1:\n        num_samples = num_samples * repeats\n\n    if num_workers > 1:\n        num_samples = num_samples // num_workers\n\n    if batch_size > 0:\n        num_samples = int(math.ceil(num_samples / batch_size)) * batch_size\n\n    return num_samples", "CodeBleu_score": 31.15317850410811}]}
{"instance_id": "a698d65ae190", "generation_result": [{"code": "def find_images_and_targets(\n        folder: str,\n        types: Optional[Union[List, Tuple, Set]] = None,\n        class_to_idx: Optional[Dict] = None,\n        leaf_name_only: bool = True,\n        sort: bool = True\n)\n\tif types is None:\n        types = set(IMG_EXTENSIONS)\n    else:\n        if isinstance(types, (list, tuple)):\n            types = set(types)\n        elif not isinstance(types, set):\n            raise TypeError(\"The 'types' argument must be a set, list, or tuple. Got {}.\".format(type(types)))\n\n    labels = []\n    filenames = []\n    for root, subdirs, files in os.walk(folder):\n        for fname in files:\n            if fname.lower().endswith(tuple(types)):\n                if leaf_name_only:\n                    path = os.path.join(root, fname)\n                else:\n                    path = os.path.join(root, os.path.join(fname))\n                filenames.append(path)\n                label = os.path.basename(os.path.dirname(path))\n                labels.append(label)\n\n    if class_to_idx is None:\n        unique_labels = set(labels)\n        sorted_labels = list(sorted(unique_labels, key=natural_key))\n        class_to_idx = {c: idx for idx, c in enumerate(sorted_labels)}\n    images_and_targets = list(zip(filenames, [class_to_idx[l] for l in labels]))\n    if sort:\n        images_and_targets = sorted(images_and_targets, key=lambda k: natural_key(k[0]))\n    return images_and_targets, class_to_idx\n\n", "CodeBleu_score": 55.4679906813434}, {"code": "def find_images_and_targets(\n        folder: str,\n        types: Optional[Union[List, Tuple, Set]] = None,\n        class_to_idx: Optional[Dict] = None,\n        leaf_name_only: bool = True,\n        sort: bool = True\n)\n\tif types is None:\n        types = IMG_EXTENSIONS\n    if class_to_idx is None:\n        class_to_idx = {}\n\n    images_and_targets = []\n    labels = []\n\n    for root, subdirs, files in os.walk(folder):\n        for file in files:\n            base, ext = os.path.splitext(file)\n            if ext.lower() in types:\n                if leaf_name_only:\n                    label = os.path.basename(root.rstrip(os.sep))\n                else:\n                    label = os.path.relpath(root, folder).replace(os.sep, '.')\n                label = label.lower()\n                if label not in class_to_idx:\n                    class_to_idx[label] = len(class_to_idx)\n                target = class_to_idx[label]\n                images_and_targets.append((os.path.join(root, file), target))\n                labels.append(label)\n\n    if sort:\n        images_and_targets.sort(key=lambda k: natural_key(k[0]))\n\n    return images_and_targets, class_to_idx\n\n", "CodeBleu_score": 45.12922932707321}, {"code": "def find_images_and_targets(\n        folder: str,\n        types: Optional[Union[List, Tuple, Set]] = None,\n        class_to_idx: Optional[Dict] = None,\n        leaf_name_only: bool = True,\n        sort: bool = True\n)\n\tif types is None:\n        types = IMG_EXTENSIONS\n    if class_to_idx is None:\n        class_to_idx = {}\n    images_and_targets = []\n    leaf_name_only = leaf_name_only and (folder.count(os.path.sep) == 0)\n    # Sorting helps the order of class labels be consistent across runs\n    classes = sorted(entry.name for entry in os.scandir(folder) if entry.is_dir())\n    n_classes = len(classes)\n    for i, class_ in enumerate(classes):\n        class_idx = class_to_idx.get(class_, None)\n        if class_idx is None:\n            class_idx = i\n        class_folder = os.path.join(folder, class_)\n        for entry in os.scandir(class_folder):\n            if entry.name.startswith('.') or entry.is_dir():\n                continue\n            file_type = entry.name[entry.name.rfind('.') + 1:].lower()\n            if file_type not in types:\n                continue\n            if leaf_name_only:\n                path = entry.name\n            else:\n                path = os.path.join(class_, entry.name)\n            images_and_targets.append((path, class_idx))\n    if sort:\n        images_and_targets.sort()\n    for i, (_, class_idx) in enumerate(images_and_targets):\n        if class_idx >= n_classes:\n            assert class_idx == i\n            class_to_idx[images_and_targets[i][0]] = i\n    return images_and_targets, class_to_idx\n\n", "CodeBleu_score": 36.46902357546612}]}
{"instance_id": "e7fe43bdecdc", "generation_result": [{"code": "def pytorch_worker_seed()\n\tworker_info = get_worker_info()\n    if worker_info is not None:\n        # favour the seed already created for pytorch dataloader workers if it exists\n        return worker_info.seed\n    # fallback to wds rank based seed\n    return wds.utils.pytorch_worker_seed()\n\n", "CodeBleu_score": 65.96393825034185}, {"code": "def pytorch_worker_seed()\n\tworker_info = get_worker_info()\n    if worker_info is not None:\n        # favour the seed already created for pytorch dataloader workers if it exists\n        return worker_info.seed\n    # fallback to wds rank based seed\n    return wds.utils.pytorch_worker_seed()\n\n", "CodeBleu_score": 65.96393825034185}, {"code": "def pytorch_worker_seed()\n\treturn pytorch_worker_seed()", "CodeBleu_score": 7.342351708070089}]}
{"instance_id": "0ffc7e7ded25", "generation_result": [{"code": "def solarize_add(img, add, thresh=128, **__)\n\tlut = []\n    for i in range(256):\n        if i < thresh:\n            lut.append(min(255, i + add))\n        else:\n            lut.append(i)\n    if img.mode in (\"L\", \"RGB\"):\n        if img.mode == \"RGB\" and len(lut) == 256:\n            lut = lut + lut + lut\n        return img.point(lut)\n    else:\n        return img\n\n", "CodeBleu_score": 85.83676294835062}, {"code": "def solarize_add(img, add, thresh=128, **__)\n\tif img.mode in (\"L\", \"RGB\"):\n        lut = []\n        for i in range(256):\n            if i < thresh:\n                lut.append(min(255, i + add))\n            else:\n                lut.append(i)\n        if img.mode == \"RGB\":\n            img = img.point(lut)\n        else:\n            img = img.point(lut, \"L\")\n    return img\n\n", "CodeBleu_score": 64.54619998826618}, {"code": "def solarize_add(img, add, thresh=128, **__)\n\tlut = []\n    for i in range(256):\n        if i < thresh:\n            lut.append(min(255, i + add))\n        else:\n            lut.append(i)\n\n    if img.mode in (\"L\", \"RGB\"):\n        if img.mode == \"RGB\" and len(lut) == 256:\n            lut = lut + lut + lut\n        return img.point(lut)\n    else:\n        return img\n\n\n", "CodeBleu_score": 85.83676294835062}]}
{"instance_id": "5a0d29577a21", "generation_result": [{"code": "def _get_weighted_transforms(transforms: Dict)\n\ttransform_names = list(transforms.keys())\n    transform_probs = [transforms[t] for t in transform_names]\n    transform_probs = [p / sum(transform_probs) for p in transform_probs]\n    return transform_names, transform_probs\n\n", "CodeBleu_score": 20.37129793845985}, {"code": "def _get_weighted_transforms(transforms: Dict)\n\ttransforms_names = []\n    transforms_probs = []\n    for name, prob in transforms.items():\n        transforms_names.append(name)\n        transforms_probs.append(prob)\n    transforms_probs = np.array(transforms_probs) / np.sum(transforms_probs)\n    return transforms_names, transforms_probs\n\n", "CodeBleu_score": 31.804121791582958}, {"code": "def _get_weighted_transforms(transforms: Dict)\n\ttransforms_names = list(transforms.keys())\n    transforms_probs = list(transforms.values())\n    transforms_probs = np.array(transforms_probs) / np.sum(transforms_probs)\n    return transforms_names, transforms_probs\n\n\n", "CodeBleu_score": 20.947824909884122}]}
{"instance_id": "84bc5658c135", "generation_result": [{"code": "def _apply_basic(self, img, mixing_weights, m)\n\t# Create a copy of the image for the mixed image\n    mixed_img = img.copy()\n\n    # Iterate through the mixing weights\n    for mixing_weight in mixing_weights:\n        # Apply a random sequence of operations to the image\n        img = self.apply_sequence(img, m)\n\n        # Blend the mixed image with the image\n        mixed_img = self.blend(mixed_img, img, mixing_weight)\n\n    # Clip the mixed image to valid pixel values\n    mixed_img = np.clip(mixed_img, 0, 255)\n\n    # Convert the mixed image to an image\n    mixed_img = Image.fromarray(mixed_img)\n\n    # Blend the mixed image with the original image\n    mixed_img = self.blend(mixed_img, img, 1 - m)\n\n    # Return the mixed image\n    return mixed_img\n\n", "CodeBleu_score": 21.09511893559154}, {"code": "def _apply_basic(self, img, mixing_weights, m)\n\th, w, c = img.shape\n    mixed = np.zeros((h, w, c), dtype=np.float32)\n    for weight in mixing_weights:\n        mixed += weight * self._apply_ops(img, m)\n    mixed = np.clip(mixed, 0, 255).astype(np.uint8)\n    return cv2.addWeighted(img, 1 - m, mixed, m, 0)\n\n", "CodeBleu_score": 15.973236247361017}, {"code": "def _apply_basic(self, img, mixing_weights, m)\n\tmixed_img = np.zeros(img.shape, dtype=np.float32)\n    for w in mixing_weights:\n        mixed_img += w * self.apply_ops(img, m)\n    mixed_img = np.clip(mixed_img, 0, 255).astype(np.uint8)\n    mixed_img = Image.fromarray(mixed_img)\n    return Image.blend(img, mixed_img, m)\n\n", "CodeBleu_score": 22.07208814942633}]}
{"instance_id": "84b5b752ff04", "generation_result": [{"code": "def adapt_to_chs(x, n)\n\tif not isinstance(x, (tuple, list)):\n        x = [x] * n\n    else:\n        if len(x) != n:\n            print('WARNING: adapt_to_chs: the length of input x is not equal to the required number of channels n.')\n            x = [np.mean(x)] * n\n        else:\n            assert len(x) == n\n    return x\n\n", "CodeBleu_score": 37.215908708288175}, {"code": "def adapt_to_chs(x, n)\n\tif not isinstance(x, (tuple, list)):\n        x = tuple([x]*n)\n        return x\n    else:\n        if len(x) != n:\n            print(f\"Warning: The length of x ({len(x)}) does not match the required number of channels n ({n}).\")\n            print(f\"The mean of x will be used to adapt x to match n.\")\n            x = tuple([np.mean(x)]*n)\n            return x\n        else:\n            assert len(x) == n, f\"The length of x ({len(x)}) does not match the required number of channels n ({n}).\"\n            return x\n", "CodeBleu_score": 39.8462831988319}, {"code": "def adapt_to_chs(x, n)\n\tif not isinstance(x, (tuple, list)):\n        return tuple([x] * n)\n    elif len(x) == n:\n        return x\n    elif len(x) == 1:\n        return tuple([x[0]] * n)\n    else:\n        print(f'WARNING: The number of input channels ({len(x)}) does not match the required number of channels ({n}).", "CodeBleu_score": 26.675410806366806}]}
{"instance_id": "54754b6ba394", "generation_result": [{"code": "def _worker_init(worker_id, worker_seeding='all')\n\tworker_info = torch.utils.data.get_worker_info()\n    if worker_info is None:\n        raise RuntimeError(\"This function can only be called inside a worker process.\")\n    if worker_seeding == 'all':\n        seed = worker_info.seed % 2**32\n        random.seed(seed)\n        torch.manual_seed(seed)\n        np.random.seed(seed)\n    elif isinstance(worker_seeding, str):\n        seed = worker_info.seed % 2**32\n        np.random.seed(seed)\n    elif callable(worker_seeding):\n        worker_seeding(worker_info.id, worker_info.seed)\n    else:\n        raise ValueError(f\"worker_seeding must be either 'all', a callable, or a string, but received {type(worker_seeding)}\")", "CodeBleu_score": 43.13525595063095}, {"code": "def _worker_init(worker_id, worker_seeding='all')\n\timport numpy as np\n    import random\n    import torch\n    worker_info = torch.utils.data.get_worker_info()\n    if worker_info is None:  # single-process data loading, return the seed directly\n        return worker_seeding\n    if worker_seeding == 'all':\n        worker_seeding = worker_info.seed % (2 ** 32)\n    if callable(worker_seeding):\n        worker_seeding = worker_seeding(worker_info)\n    if worker_info.id != worker_id:\n        raise RuntimeError(f'[Worker id {worker_id}] does not match worker info id {worker_info.id}')\n    np.random.seed(worker_seeding)\n    random.seed(worker_seeding)\n    torch.manual_seed(worker_seeding)\n    return worker_seeding\n\n", "CodeBleu_score": 50.303017177940546}, {"code": "def _worker_init(worker_id, worker_seeding='all')\n\tworker_info = torch.utils.data.get_worker_info()\n    if worker_info is None:\n        raise RuntimeError('This function can only be called from within a worker process')\n    if worker_seeding == 'all':\n        seed = worker_info.seed % 2**32\n        random.seed(seed)\n        torch.manual_seed(seed)\n        np.random.seed(seed)\n    elif callable(worker_seeding):\n        worker_seeding(worker_info)\n    elif worker_seeding == 'numpy':\n        seed = worker_info.seed % 2**32\n        np.random.seed(seed)\n    else:\n        raise RuntimeError(f'{worker_seeding} is not a valid value for worker_seeding')\n    if worker_info.id != worker_id:\n        raise RuntimeError(f'The worker ID {worker_id} does not match the worker ID {worker_info.id}')\n\n", "CodeBleu_score": 45.57801960497663}]}
{"instance_id": "81dfdce940db", "generation_result": [{"code": "def _decode_and_center_crop(image_bytes, image_size, resize_method)\n\timage_shape = tf.image.extract_jpeg_shape(image_bytes)\n    image_height = image_shape[0]\n    image_width = image_shape[1]\n    padded_center_crop_size = tf.cast(\n        ((image_size / (image_size + CROP_PADDING)) *\n         tf.cast(tf.minimum(image_height, image_width), tf.float32)),\n        tf.int32)\n    offset_height = ((image_height - padded_center_crop_size) + 1) // 2\n    offset_width = ((image_width - padded_center_crop_size) + 1) // 2\n    crop_window = tf.stack([offset_height, offset_width,\n                            padded_center_crop_size, padded_center_crop_size])\n    image = tf.image.decode_and_crop_jpeg(image_bytes, crop_window, channels=3)\n    image = tf.image.resize_images(\n        image, [image_size, image_size], method=resize_method)\n    return image\n", "CodeBleu_score": 58.712875402420906}, {"code": "def _decode_and_center_crop(image_bytes, image_size, resize_method)\n\t# Decode the image bytes to a tensor.\n    image = tf.io.decode_jpeg(image_bytes, channels=3)\n    # Convert to float32 in [0, 1] range.\n    image = tf.image.convert_image_dtype(image, tf.float32)\n    # Crop the image to the center with padding.\n    shape = tf.cast(tf.shape(image)[:-1], tf.float32)\n    # Calculate the padded center crop size.\n    crop_size = tf.cast(image_size / (image_size + CROP_PADDING) * shape, tf.int32)\n    # Determine the offset for the crop window.\n    offset = (shape - crop_size + 1) / 2\n    offset = tf.cast(offset, tf.int32)\n    # Crop the image to the center with padding.\n    image = tf.image.crop_to_bounding_box(image, offset[0], offset[1], crop_size[0], crop_size[1])\n    # Resize the image to the desired size.\n    image = resize(image, image_size, resize_method)\n    return image\n\n", "CodeBleu_score": 27.922879453214634}, {"code": "def _decode_and_center_crop(image_bytes, image_size, resize_method)\n\t# Decode the image bytes.\n    image = tf.image.decode_jpeg(image_bytes, channels=3)\n    # Determine the shape of the image.\n    image_shape = tf.shape(image)\n    # Calculate the padded center crop size.\n    padded_center_crop_size = tf.cast(\n        ((image_size / (image_size + CROP_PADDING)) *\n         tf.cast(tf.minimum(image_shape[0], image_shape[1]), tf.float32)),\n        tf.int32)\n    # Calculate offset of the crop.\n    offset_height = ((image_shape[0] - padded_center_crop_size) + 1) // 2\n    offset_width = ((image_shape[1] - padded_center_crop_size) + 1) // 2\n    # Crop the image.\n    image = tf.image.crop_to_bounding_box(\n        image, offset_height, offset_width, padded_center_crop_size,\n        padded_center_crop_size)\n    # Resize the image.\n    image = resize_method(image, [image_size, image_size])\n    return image\n\n", "CodeBleu_score": 41.373028225694796}]}
{"instance_id": "cf16a6d37111", "generation_result": [{"code": "def preprocess_for_train(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\timage = _decode_and_random_crop(image_bytes, image_size, interpolation)\n    image = _flip(image)\n    image = tf.reshape(image, [image_size, image_size, 3])\n    image = tf.image.convert_image_dtype(image, dtype=tf.bfloat16 if use_bfloat16 else tf.float32)\n    return image\n\n", "CodeBleu_score": 42.54727842231387}, {"code": "def preprocess_for_train(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\timage = _decode_and_random_crop(image_bytes, image_size, interpolation)\n    image = _flip(image)\n    image = tf.reshape(image, [image_size, image_size, 3])\n    image = tf.image.convert_image_dtype(\n        image, dtype=tf.bfloat16 if use_bfloat16 else tf.float32)\n    return image\n\n", "CodeBleu_score": 46.02317342767556}, {"code": "def preprocess_for_train(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\timage = _decode_and_random_crop(image_bytes, image_size, interpolation)\n    image = _flip(image)\n    image = tf.reshape(image, [image_size, image_size, 3])\n    image = tf.image.convert_image_dtype(image, dtype=tf.bfloat16 if use_bfloat16 else tf.float32)\n    return image\n\n", "CodeBleu_score": 42.54727842231387}]}
{"instance_id": "78b2a3bd2142", "generation_result": [{"code": "def preprocess_for_eval(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\tif interpolation == 'bicubic':\n        resize_method = tf.image.ResizeMethod.BICUBIC\n    elif interpolation == 'bilinear':\n        resize_method = tf.image.ResizeMethod.BILINEAR\n    elif interpolation == 'nearest':\n        resize_method = tf.image.ResizeMethod.NEAREST_NEIGHBOR\n    else:\n        raise ValueError('Invalid interpolation method: {}'.format(interpolation))\n\n    image = _decode_and_center_crop(image_bytes, image_size, resize_method)\n    image = tf.reshape(image, [image_size, image_size, 3])\n    image = tf.image.convert_image_dtype(image, dtype=(tf.bfloat16 if use_bfloat16 else tf.float32))\n\n    return image\n\n", "CodeBleu_score": 61.08319136052693}, {"code": "def preprocess_for_eval(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\tif interpolation == 'bicubic':\n        resize_method = tf.image.ResizeMethod.BICUBIC\n    elif interpolation == 'bilinear':\n        resize_method = tf.image.ResizeMethod.BILINEAR\n    elif interpolation == 'nearest':\n        resize_method = tf.image.ResizeMethod.NEAREST_NEIGHBOR\n    else:\n        raise ValueError('Invalid interpolation method {}'.format(interpolation))\n\n    image = _decode_and_center_crop(image_bytes, image_size, resize_method)\n    image = tf.reshape(image, [image_size, image_size, 3])\n\n    if use_bfloat16:\n        image = tf.cast(image, dtype=tf.bfloat16)\n    else:\n        image = tf.cast(image, dtype=tf.float32)\n\n    image = image * 2.0 / 255.0 - 1.0\n    return image\n\n", "CodeBleu_score": 57.02440463910292}, {"code": "def preprocess_for_eval(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\tif use_bfloat16:\n        dtype = tf.bfloat16\n    else:\n        dtype = tf.float32\n\n    if interpolation == 'bicubic':\n        resize_method = tf.image.ResizeMethod.BICUBIC\n    elif interpolation == 'bilinear':\n        resize_method = tf.image.ResizeMethod.BILINEAR\n    else:\n        resize_method = tf.image.ResizeMethod.AREA\n\n    image = _decode_and_center_crop(image_bytes, image_size, resize_method)\n    image = tf.reshape(image, [image_size, image_size, 3])\n    image = tf.image.convert_image_dtype(image, dtype)\n\n    return image\n\n", "CodeBleu_score": 61.43068673652677}]}
{"instance_id": "8bd4364a9a0e", "generation_result": [{"code": "def evaluate(self, model, eval_dataloader=None, output_file=\"merged\")\n\tif eval_dataloader is None:\n        eval_dataloader = self.eval_dataloader\n    if self.args.n_gpu > 1:\n        model = torch.nn.DataParallel(model)\n    model.eval()\n    all_preds = []\n    all_labels = []\n    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n        with torch.no_grad():\n            inputs = self._get_inputs_dict(batch)\n            outputs = model(**inputs)\n            logits = outputs[0]\n            preds = logits.detach().cpu().numpy()\n            label_ids = inputs[\"labels\"].detach().cpu().numpy()\n        all_preds.append(preds)\n        all_labels.append(label_ids)\n    all_preds = np.concatenate(all_preds, axis=0)\n    all_labels = np.concatenate(all_labels, axis=0)\n    output_eval_file = os.path.join(self.args.output_dir, output_file + \".txt\")\n    with open(output_eval_file, \"w\") as writer:\n        writer.write(json.dumps(all_preds.tolist()))\n    results = self._eval_results(all_preds, all_labels)\n    return results", "CodeBleu_score": 45.810583377484136}, {"code": "def evaluate(self, model, eval_dataloader=None, output_file=\"merged\")\n\tif eval_dataloader is None:\n        eval_dataloader = self.eval_dataloader\n\n    self.model.eval()\n\n    output_file = output_file + \".json\"\n\n    with open(output_file, \"w\") as f:\n        for batch in eval_dataloader:\n            with torch.no_grad():\n                inputs = self._prepare_inputs(batch)\n                outputs = self.model(**inputs)\n                logits = outputs.logits\n                predictions = torch.argmax(logits, dim=-1)\n                predictions = predictions.tolist()\n                # print(predictions)\n                # print(batch['labels'])\n                for i in range(len(predictions)):\n                    f.write(json.dumps({'id': batch['id'][i], 'label': predictions[i]}))\n                    f.write('", "CodeBleu_score": 45.18475263366448}, {"code": "def evaluate(self, model, eval_dataloader=None, output_file=\"merged\")\n\tif eval_dataloader is None:\n        eval_dataloader = self.eval_dataloader\n\n    if self.args.n_gpu > 1:\n        model = torch.nn.DataParallel(model)\n\n    model.eval()\n\n    # Eval!\n    logger.info(\"***** Running evaluation {} *****\".format(prefix))\n    logger.info(\"  Num examples = %d\", len(eval_dataloader))\n    logger.info(\"  Batch size = %d\", eval_batch_size)\n\n    all_results = []\n    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n        model.eval()\n        batch = tuple(t.to(self.args.device) for t in batch)\n\n        with torch.no_grad():\n            inputs = {\"input_ids\": batch[0], \"attention_mask\": batch[1], \"labels\": batch[3]}\n            if self.args.model_type != \"distilbert\":\n                inputs[\"token_type_ids\"] = (\n                    batch[2] if self.args.model_type in [\"bert\", \"xlnet\", \"albert\"] else None\n                )  # XLM, DistilBERT, RoBERTa, and XLM-RoBERTa don't use segment_ids\n            outputs = model(**inputs)\n            tmp_eval_loss, logits = outputs[:2]\n\n            eval_loss += tmp_eval_loss.mean().item()\n        nb_eval_steps += 1\n\n        if preds is None:\n            preds = logits.detach().cpu().numpy()\n            out_label_ids = inputs[\"labels\"].detach().cpu().numpy()\n        else:\n            preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n            out_label_ids = np.append(out_label_ids, inputs[\"labels\"].detach().cpu().numpy(), axis=0)\n\n    eval_loss = eval_loss / nb_eval_steps\n    preds = np.argmax(preds", "CodeBleu_score": 45.16094751879704}]}
{"instance_id": "22d682c958c0", "generation_result": [{"code": " def print_computed_metrics(self, metrics)\n\tprint(f\"Rank1: {metrics['rank1']:.4f}\")\n    print(f\"Rank5: {metrics['rank5']:.4f}\")\n    print(f\"Mean IoU: {metrics['mean_iou']:.4f}\")\n\n\n", "CodeBleu_score": 18.473473460250307}, {"code": " def print_computed_metrics(self, metrics)\n\tprint(\"Rank1: {:.4f}\".format(metrics['rank1']))\n    print(\"Rank5: {:.4f}\".format(metrics['rank5']))\n    print(\"miou: {:.4f}\".format(metrics['miou']))", "CodeBleu_score": 24.1526354655816}, {"code": " def print_computed_metrics(self, metrics)\n\tprint('Rank1: {:.4f}'.format(metrics['Rank@1']))\n    print('Rank5: {:.4f}'.format(metrics['Rank@5']))\n    print('Mean IoU: {:.4f}'.format(metrics['Mean IoU']))", "CodeBleu_score": 23.500097808808878}]}
{"instance_id": "39c1a50b7e7d", "generation_result": [{"code": "def _eval_predictions(self, segments, data)\n\trank1 = []\n    rank5 = []\n    iou = []\n    for seg, gt in zip(segments, data):\n        rank = self._rank(seg, gt)\n        iou_ = self._iou(seg, gt)\n        if rank <= 1:\n            rank1.append(1)\n        else:\n            rank1.append(0)\n        if rank <= 5:\n            rank5.append(1)\n        else:\n            rank5.append(0)\n        iou.append(iou_)\n    return np.mean(rank1), np.mean(rank5), np.mean(iou)\n\n", "CodeBleu_score": 24.22365065516904}, {"code": "def _eval_predictions(self, segments, data)\n\tsegments = sorted(segments, key=lambda x: x[1], reverse=True)\n    ranks = []\n    ious = []\n    for segment in segments:\n        ious.append(self._iou(segment, data))\n        ranks.append(self._rank(segments, data))\n    return np.mean(ranks[:1]), np.mean(ranks[:5]), np.mean(ious)\n\n", "CodeBleu_score": 17.11824462772447}, {"code": "def _eval_predictions(self, segments, data)\n\tranks = []\n    ious = []\n    for segment, gt in zip(segments, data):\n        if segment:\n            ious.append(max([self._iou(pred, gt) for pred in segment]))\n            ranks.append(min([self._rank(segment, gt) for pred in segment]))\n        else:\n            ious.append(0)\n            ranks.append(len(gt)+1)\n    return np.mean(ranks == 1), np.mean(ranks <= 5), np.mean(ious)\n\n", "CodeBleu_score": 23.45890888367192}]}
{"instance_id": "8ccf84f8d034", "generation_result": [{"code": "def predict_loop(self, model, eval_dataloader, output_file=None)\n\tscores = []\n    model.eval()\n    model = model.to(self.ctx)\n    for batch in eval_dataloader:\n        batch = self.to_ctx(batch)\n        with torch.no_grad():\n            output = model(**batch)\n        scores.append(self.process_batch(output))\n    scores = self.finalize(scores)\n    if output_file is not None:\n        self.save_results(scores, output_file)\n    return scores\n\n", "CodeBleu_score": 48.80832089714474}, {"code": "def predict_loop(self, model, eval_dataloader, output_file=None)\n\tscores = []\n    model.eval()\n    model.to(self.ctx)\n    for batch in eval_dataloader:\n        batch = self.to_ctx(batch)\n        with torch.no_grad():\n            scores.append(model(**batch))\n    scores = self.process_scores(scores)\n    if output_file is not None:\n        self.finalize(output_file)\n    return scores\n\n", "CodeBleu_score": 42.945633678880945}, {"code": "def predict_loop(self, model, eval_dataloader, output_file=None)\n\tscores = []\n    model.eval()\n    model.to(0)\n    for batch in eval_dataloader:\n        batch = self.to_ctx(batch)\n        with torch.no_grad():\n            scores.append(model(batch))\n    scores = self.process_scores(scores)\n    if output_file is not None:\n        self.finalize(output_file)\n    return scores\n\n", "CodeBleu_score": 42.31060296574596}]}
{"instance_id": "e68529ad727d", "generation_result": [{"code": "def _aggregate_scores(self, scores)\n\tif len(scores) != 2:\n        raise ValueError(\"Expected 2 sets of scores, got {}\".format(len(scores)))\n    if scores[0].shape[0] != scores[1].shape[0]:\n        raise ValueError(\"Expected the same number of videos and texts, got {} and {}\".format(scores[0].shape[0], scores[1].shape[0]))\n    self.full_scores = None\n    return np.concatenate(scores, axis=0)\n\n\n", "CodeBleu_score": 26.837804567551267}, {"code": "def _aggregate_scores(self, scores)\n\tassert len(scores) == 2, \"There should be exactly two sets of scores, but there are {}\".format(len(scores))\n    self.full_scores = None\n    return torch.cat(scores, 0).dot(torch.cat(scores, 0).T)\n\n", "CodeBleu_score": 23.544070798818115}, {"code": "def _aggregate_scores(self, scores)\n\tassert len(scores) == 2, \"There are {} sets of scores, but there should be exactly two.\".format(len(scores))\n    self.full_scores = None\n    return torch.cat(scores[0], dim=0).dot(torch.cat(scores[1], dim=0).T)\n\n\n", "CodeBleu_score": 31.9886507687665}]}
{"instance_id": "e4e1861fc357", "generation_result": [{"code": "def finalize(self, Y_pred, Y_true, output_file=None)\n\t# convert to numpy arrays\n    Y_pred = np.array(Y_pred)\n    Y_true = np.array(Y_true)\n\n    # check for prediction errors\n    errors = np.where(Y_pred != Y_true)[0]\n    if len(errors) > 0:\n        print(\"Prediction errors:\")\n        for i in range(min(10, len(errors))):\n            print(f\"  {i}: pred={Y_pred[errors[i]]}, true={Y_true[errors[i]]}\")\n\n    # save predictions if output file is specified\n    if output_file is not None:\n        with open(output_file, \"wb\") as f:\n            pickle.dump({\"Y_pred\": Y_pred, \"Y_true\": Y_true}, f)\n\n    return {\"Y_pred\": Y_pred, \"Y_true\": Y_true}", "CodeBleu_score": 36.80013195763854}, {"code": "def finalize(self, Y_pred, Y_true, output_file=None)\n\t# convert to numpy arrays\n    Y_pred = np.array(Y_pred)\n    Y_true = np.array(Y_true)\n\n    # check for prediction errors\n    if np.sum(Y_pred != Y_true) > 0:\n        print(\"Prediction errors found:\")\n        for i in range(len(Y_pred)):\n            if Y_pred[i] != Y_true[i]:\n                print(\"Prediction error:\", Y_pred[i], \"!=\", Y_true[i])\n                print(\"Sample:\", self.X_test[i])\n                print()\n\n    # save predictions and true labels\n    if output_file is not None:\n        with open(output_file, \"wb\") as f:\n            pickle.dump({\"Y_pred\": Y_pred, \"Y_true\": Y_true}, f)\n\n    return {\"Y_pred\": Y_pred, \"Y_true\": Y_true}", "CodeBleu_score": 39.362189948671485}, {"code": "def finalize(self, Y_pred, Y_true, output_file=None)\n\tY_pred = np.array(Y_pred)\n    Y_true = np.array(Y_true)\n\n    if Y_pred.shape != Y_true.shape:\n        raise ValueError(\n            \"The shape of the predicted labels does not match the shape of the true labels.\"\n        )\n\n    if output_file is not None:\n        with open(output_file, \"wb\") as f:\n            pickle.dump({\"Y_pred\": Y_pred, \"Y_true\": Y_true}, f)\n\n    return {\"Y_pred\": Y_pred, \"Y_true\": Y_true}", "CodeBleu_score": 28.511693147452593}]}
{"instance_id": "7df9ab9a4920", "generation_result": [{"code": "def reduce_metrics(logging_outputs) -> None\n\tloss_sum = sum(log.get(\"loss\", 0) for log in logging_outputs)\n    nll_loss_sum = sum(log.get(\"nll_loss\", 0) for log in logging_outputs)\n    ntokens = sum(log.get(\"ntokens\", 0) for log in logging_outputs)\n    sample_size = sum(log.get(\"sample_size\", 0) for log in logging_outputs)\n    nsentences = sum(log.get(\"nsentences\", 0) for log in logging_outputs)\n\n    metrics.log_scalar(\n        \"loss\", loss_sum / sample_size / math.log(2), sample_size, round=3\n    )\n    metrics.log_scalar(\n        \"nll_loss\", nll_loss_sum / ntokens / math.log(2), ntokens, round=3\n    )\n    metrics.log_derived(\n        \"ppl\", lambda meters: utils.get_perplexity(meters[\"nll_loss\"].avg)\n    )\n\n    metrics.log_scalar(\"ntokens\", ntokens)\n    metrics.log_scalar(\"nsentences\", nsentences)\n    metrics.log_scalar(\"sample_size\", sample_size)\n\n    c_errors = sum(log.get(\"c_errors\", 0) for log in logging_outputs)\n    metrics.log_scalar(\"_c_errors\", c_errors)\n    c_total = sum(log.get(\"c_total\", 0) for log in logging_outputs)\n    metrics.log_scalar(\"_c_total\", c_total)\n    w_errors = sum(log.get(\"w_errors\", 0) for log in logging_outputs)\n    metrics.log_scalar(\"_w_errors\", w_errors)\n    wv_errors = sum(log.get(\"wv_errors\", 0) for log in logging_outputs)\n    metrics.log_scalar(\"_wv_errors\", wv_errors)\n    w_total = sum(log.get(\"w_", "CodeBleu_score": 45.743370132791725}, {"code": "def reduce_metrics(logging_outputs) -> None\n\t\"\"\"\n    The function reduces metrics by calculating the average loss from logging outputs.\n    It sums the loss values and sample sizes from the logs, then logs the average loss as a scalar with three decimal places.\n    \"\"\"\n    \"\"\"\n    The function reduces metrics by calculating the average loss from logging outputs.\n    It sums the loss values and sample sizes from the logs, then logs the average loss as a scalar with three decimal places.\n    \"\"\"\n    \"\"\"\n    The function reduces metrics by calculating the average loss from logging outputs.\n    It sums the loss values and sample sizes from the logs, then logs the average loss as a scalar with three decimal places.\n    \"\"\"\n    \"\"\"\n    The function reduces metrics by calculating the average loss from logging outputs.\n    It sums the loss values and sample sizes from the logs, then logs the average loss as a scalar with three decimal places.\n    \"\"\"\n    \"\"\"\n    The function reduces metrics by calculating the average loss from logging outputs.\n    It sums the loss values and sample sizes from the logs, then logs the average loss as a scalar with three decimal places.\n    \"\"\"\n    \"\"\"\n    The function reduces metrics by calculating the average loss from logging outputs.\n    It sums the loss values and sample sizes from the logs, then logs the average loss as a scalar with three decimal places.\n    \"\"\"\n    \"\"\"\n    The function reduces metrics by calculating the average loss from logging outputs.\n    It sums the loss values and sample sizes from the logs, then logs the average loss as a scalar with three decimal places.\n    \"\"\"\n    \"\"\"\n    The function reduces metrics by calculating the average loss from logging outputs.\n    It sums the loss values and sample sizes from the logs, then logs the average loss as a scalar with three decimal places.\n    \"\"\"\n    \"\"\"\n    The function reduces metrics by calculating the average loss from logging outputs.\n    It sums the loss values and sample sizes from the logs, then logs the average loss as a scalar with three decimal places.\n    \"\"\"\n    \"\"\"\n    The function reduces metrics by calculating the average loss from logging outputs.\n    It sums the loss values and sample sizes from the logs, then logs the average loss as a scalar with three decimal places.\n    \"\"\"\n    \"\"\"\n    The function reduces metrics by calculating the average loss from logging outputs.\n    It sums the loss values and sample sizes from the logs,", "CodeBleu_score": 4.488632537209555}, {"code": "def reduce_metrics(logging_outputs) -> None\n\t\"\"\"\n    The function reduces metrics by calculating the average loss from logging outputs.\n    It sums the loss values and sample sizes from the logs, then logs the average loss as a scalar with three decimal places.\n    \"\"\"\n    loss_sum = sum(log.get(\"loss\", 0) for log in logging_outputs)\n    sample_size = sum(log.get(\"sample_size\", 0) for log in logging_outputs)\n    metrics.log_scalar(\"loss\", loss_sum / sample_size, sample_size, round=3)\n\n", "CodeBleu_score": 51.34295438674948}]}
{"instance_id": "7f9c326a96f7", "generation_result": [{"code": "def _mm_on_the_fly(\n        self,\n        cmasks,\n        vmasks,\n        attention_mask\n    )\n\tif attention_mask is None:\n        attention_mask = self._mm_attention_mask(cmasks, vmasks)\n    token_type_ids = self._mm_token_type_ids(cmasks, vmasks)\n    return attention_mask, token_type_ids\n\n", "CodeBleu_score": 25.648352106296535}, {"code": "def _mm_on_the_fly(\n        self,\n        cmasks,\n        vmasks,\n        attention_mask\n    )\n\tif attention_mask is None:\n        attention_mask = self._mm_attention_mask(cmasks, vmasks)\n    token_type_ids = torch.cat([cmasks[:, :1], vmasks, cmasks[:, 1:]], dim=1)\n    return attention_mask, token_type_ids\n\n", "CodeBleu_score": 31.973912006696033}, {"code": "def _mm_on_the_fly(\n        self,\n        cmasks,\n        vmasks,\n        attention_mask\n    )\n\tif attention_mask is None:\n        attention_mask = self._mm_attention_mask(cmasks, vmasks)\n\n    token_type_ids = torch.cat([cmasks[:, :1], vmasks, cmasks[:, 1:]], dim=1)\n    return attention_mask, token_type_ids\n\n", "CodeBleu_score": 31.973912006696033}]}
{"instance_id": "fa99d2a148a5", "generation_result": [{"code": "def prepare_inputs_for_generation(\n        self,\n        input_ids,\n        input_video_embeds,\n        attention_mask=None,\n        token_type_ids=None,\n        **model_kwargs\n    )\n\t# set up input_ids\n    input_ids = input_ids.clone()\n    input_ids = input_ids.view(-1, input_ids.shape[-1])\n\n    # set up input_video_embeds\n    input_video_embeds = input_video_embeds.clone()\n    input_video_embeds = input_video_embeds.view(-1, input_video_embeds.shape[-2], input_video_embeds.shape[-1])\n\n    # set up attention_mask\n    if attention_mask is not None:\n        attention_mask = attention_mask.clone()\n        attention_mask = attention_mask.view(-1, attention_mask.shape[-1])\n        attention_mask = attention_mask[:, :input_ids.shape[1] + input_video_embeds.shape[1]]\n    else:\n        attention_mask = input_ids.new_ones((input_ids.shape[0], input_ids.shape[1] + input_video_embeds.shape[1]))\n\n    # set up token_type_ids\n    if token_type_ids is not None:\n        token_type_ids = token_type_ids.clone()\n        token_type_ids = token_type_ids.view(-1, token_type_ids.shape[-1])\n        token_type_ids = token_type_ids[:, :input_ids.shape[1] + input_video_embeds.shape[1]]\n    else:\n        token_type_ids = input_ids.new_zeros((input_ids.shape[0], input_ids.shape[1] + input_video_embeds.shape[1]))\n\n    return {\n        \"input_ids\": input_ids,\n        \"input_video_embeds\": input_video_embeds,\n        \"attention_mask\": attention_mask,\n        \"token_type_ids\": token_type_ids,\n    }\n\n", "CodeBleu_score": 49.12295441472423}, {"code": "def prepare_inputs_for_generation(\n        self,\n        input_ids,\n        input_video_embeds,\n        attention_mask=None,\n        token_type_ids=None,\n        **model_kwargs\n    )\n\tinput_ids = self.adjust_input_ids_for_generation(input_ids)\n    input_video_embeds = self.adjust_input_video_embeds_for_generation(input_video_embeds)\n    attention_mask = self.adjust_attention_mask_for_generation(attention_mask, input_ids, input_video_embeds)\n    token_type_ids = self.adjust_token_type_ids_for_generation(token_type_ids, input_ids, input_video_embeds)\n    return {\n        \"input_ids\": input_ids,\n        \"input_video_embeds\": input_video_embeds,\n        \"attention_mask\": attention_mask,\n        \"token_type_ids\": token_type_ids,\n    }\n\n\n", "CodeBleu_score": 29.912550049366466}, {"code": "def prepare_inputs_for_generation(\n        self,\n        input_ids,\n        input_video_embeds,\n        attention_mask=None,\n        token_type_ids=None,\n        **model_kwargs\n    )\n\t# Input type checking for clearer error\n    assert isinstance(input_ids, (list, tuple)), f\"Input prompt should be a list or tuple, but is {type(input_ids)}\"\n    assert isinstance(input_video_embeds, (list, tuple)), f\"Input prompt should be a list or tuple, but is {type(input_video_embeds)}\"\n\n    # Adjust the sequence length to match the combined length of input_ids and input_video_embeds\n    seq_length = len(input_ids) + len(input_video_embeds)\n\n    # Modify the attention_mask and token_type_ids to fit the new sequence length\n    if attention_mask is not None:\n        attention_mask = [1] * len(input_ids) + [1] * len(input_video_embeds)\n        if len(attention_mask) < seq_length:\n            attention_mask.extend([0] * (seq_length - len(attention_mask)))\n        attention_mask = attention_mask[:seq_length]\n    if token_type_ids is not None:\n        token_type_ids = [0] * len(input_ids) + [1] * len(input_video_embeds)\n        if len(token_type_ids) < seq_length:\n            token_type_ids.extend([0] * (seq_length - len(token_type_ids)))\n        token_type_ids = token_type_ids[:seq_length]\n\n    # Update the model_kwargs\n    model_kwargs[\"attention_mask\"] = attention_mask\n    model_kwargs[\"token_type_ids\"] = token_type_ids\n\n    return {\n        \"input_ids\": input_ids,\n        \"input_video_embeds\": input_video_embeds,\n        \"attention_mask\": attention_mask,\n        \"token_type_ids\": token_type_ids,\n    }\n\n\n", "CodeBleu_score": 45.290959178685796}]}
{"instance_id": "b1b0d25fe620", "generation_result": [{"code": "def _init_pipeline(self, cfg: ConfigType) -> Compose\n\tif 'meta_keys' in cfg:\n        cfg['meta_keys'] = [\n            x for x in cfg['meta_keys'] if x != 'img_id'\n        ]\n    transform_idx = self._get_transform_idx(cfg, 'LoadImageFromFile')\n    if transform_idx == -1:\n        raise ValueError(\n            'Could not find the LoadImageFromFile transform in the pipeline'\n        )\n    cfg[transform_idx]['type'] = 'mmdet.InferencerLoader'\n    return Compose(cfg)\n\n", "CodeBleu_score": 37.07679852122169}, {"code": "def _init_pipeline(self, cfg: ConfigType) -> Compose\n\tif 'meta_keys' in cfg and 'img_id' in cfg['meta_keys']:\n        cfg['meta_keys'].remove('img_id')\n    infer_idx = self._get_transform_idx(cfg, 'LoadImageFromFile')\n    if infer_idx == -1:\n        raise ValueError('The pipeline must contain a LoadImageFromFile transform.')\n    cfg[infer_idx]['type'] = 'mmdet.InferencerLoader'\n    return Compose(cfg)\n\n", "CodeBleu_score": 31.489642679699276}, {"code": "def _init_pipeline(self, cfg: ConfigType) -> Compose\n\tpipeline = Compose(cfg)\n    if 'img_id' in pipeline.meta_keys:\n        pipeline.meta_keys.remove('img_id')\n    transform_idx = self._get_transform_idx(pipeline.transforms, 'LoadImageFromFile')\n    if transform_idx == -1:\n        raise ValueError('LoadImageFromFile transform not found in the pipeline')\n    pipeline.transforms[transform_idx]['type'] = 'mmdet.InferencerLoader'\n    return pipeline\n\n", "CodeBleu_score": 32.717785563277964}]}
{"instance_id": "7967962856eb", "generation_result": [{"code": "def _inputs_to_list(self, inputs: InputsType) -> list\n\tif isinstance(inputs, str):\n        if os.path.isdir(inputs):\n            inputs = list_files(inputs, extensions=['.jpg', '.png', '.jpeg'])\n        else:\n            inputs = [inputs]\n    elif isinstance(inputs, (np.ndarray, pd.Series)):\n        inputs = list(inputs)\n    elif not isinstance(inputs, (list, tuple)):\n        inputs = [inputs]\n    return inputs\n", "CodeBleu_score": 47.64175934098355}, {"code": "def _inputs_to_list(self, inputs: InputsType) -> list\n\tif isinstance(inputs, str):\n        if os.path.isdir(inputs):\n            inputs = list(glob.glob(os.path.join(inputs, '*')))\n        else:\n            inputs = [inputs]\n    elif not isinstance(inputs, (list, tuple)):\n        inputs = [inputs]\n    return inputs\n\n", "CodeBleu_score": 42.26783792653417}, {"code": "def _inputs_to_list(self, inputs: InputsType) -> list\n\tif isinstance(inputs, str):\n        if os.path.isdir(inputs):\n            inputs = [os.path.join(inputs, f) for f in os.listdir(inputs) if os.path.isfile(os.path.join(inputs, f))]\n        else:\n            inputs = [inputs]\n    elif not isinstance(inputs, (list, tuple)):\n        inputs = [inputs]\n    return inputs\n\n", "CodeBleu_score": 43.9241088755001}]}
{"instance_id": "74f727f06fa0", "generation_result": [{"code": "def _get_chunk_data(self, inputs: Iterable, chunk_size: int)\n\tchunk = []\n    for input in inputs:\n        if isinstance(input, dict) and (\"img\" in input or \"img_path\" in input):\n            chunk.append(self.pipeline(input))\n        else:\n            chunk.append(input)\n        if len(chunk) == chunk_size:\n            yield chunk\n            chunk = []\n    if len(chunk) > 0:\n        yield chunk", "CodeBleu_score": 25.11772954601242}, {"code": "def _get_chunk_data(self, inputs: Iterable, chunk_size: int)\n\tchunk = []\n    for input in inputs:\n        if isinstance(input, dict):\n            if \"img\" in input:\n                chunk.append(self.pipeline(input))\n            elif \"img_path\" in input:\n                chunk.append(self.pipeline({\"img_path\": input[\"img_path\"]}))\n            else:\n                raise ValueError(f\"Input dictionary must contain either an 'img' or 'img_path' key, but got {list(input.keys())}\")\n        else:\n            chunk.append(self.pipeline({\"img\": input}))\n        if len(chunk) == chunk_size:\n            yield chunk\n            chunk = []\n    if len(chunk) > 0:\n        yield chunk\n\n", "CodeBleu_score": 32.86141507041403}, {"code": "def _get_chunk_data(self, inputs: Iterable, chunk_size: int)\n\tchunk = []\n    for input in inputs:\n        if isinstance(input, dict) and ('img' in input or 'img_path' in input):\n            input = self(input)\n        chunk.append(input)\n        if len(chunk) == chunk_size:\n            yield chunk\n            chunk = []\n    if len(chunk) > 0:\n        yield chunk\n\n\n", "CodeBleu_score": 25.0239376994908}]}
{"instance_id": "a43ab51afa59", "generation_result": [{"code": "def postprocess(\n        self,\n        preds: PredType,\n        visualization: Optional[List[np.ndarray]] = None,\n        return_datasamples: bool = False,\n        print_result: bool = False,\n        no_save_pred: bool = False,\n        pred_out_dir: str = '',\n        **kwargs,\n    ) -> Dict\n\t# Initialize the output directory if predictions are not to be saved\n    if not no_save_pred and pred_out_dir != '':\n        mmcv.mkdir_or_exist(pred_out_dir)\n        mmcv.mkdir_or_exist(osp.join(pred_out_dir, 'preds'))\n\n    # Prepare a result dictionary\n    results = dict()\n    results['preds'] = []\n    if visualization is not None:\n        results['visualization'] = visualization\n\n    # Convert predictions to dictionaries and append them to results\n    if not return_datasamples:\n        for pred in preds:\n            results['preds'].append(self.pred2dict(pred, pred_out_dir))\n    else:\n        if pred_out_dir != '':\n            warnings.warn('Saving datasamples is not supported.')\n        results['datasamples'] = preds\n\n    # Print the results if required\n    if print_result:\n        self.print_results(results)\n\n    return results\n\n", "CodeBleu_score": 52.87522316476628}, {"code": "def postprocess(\n        self,\n        preds: PredType,\n        visualization: Optional[List[np.ndarray]] = None,\n        return_datasamples: bool = False,\n        print_result: bool = False,\n        no_save_pred: bool = False,\n        pred_out_dir: str = '',\n        **kwargs,\n    ) -> Dict\n\tif pred_out_dir != '' and return_datasamples:\n        warnings.warn(\n            'Saving datasamples is not supported. Please set '\n            'return_datasamples to False.')\n\n    if not no_save_pred:\n        mmcv.mkdir_or_exist(pred_out_dir)\n        mmcv.mkdir_or_exist(osp.join(pred_out_dir, 'preds'))\n\n    results = {}\n    if return_datasamples:\n        results['datasamples'] = []\n    else:\n        results['predictions'] = []\n\n    for pred in preds:\n        if return_datasamples:\n            datasample = self.pred2datasample(pred, visualization)\n            results['datasamples'].append(datasample)\n        else:\n            result = self.pred2dict(pred, pred_out_dir)\n            results['predictions'].append(result)\n\n    if print_result:\n        self.print_results(results)\n\n    return results\n\n", "CodeBleu_score": 50.61627898871431}, {"code": "def postprocess(\n        self,\n        preds: PredType,\n        visualization: Optional[List[np.ndarray]] = None,\n        return_datasamples: bool = False,\n        print_result: bool = False,\n        no_save_pred: bool = False,\n        pred_out_dir: str = '',\n        **kwargs,\n    ) -> Dict\n\tif not no_save_pred and pred_out_dir != '':\n        mmcv.mkdir_or_exist(pred_out_dir)\n        mmcv.mkdir_or_exist(osp.join(pred_out_dir, 'preds'))\n\n    results = {}\n\n    if return_datasamples:\n        if not no_save_pred and pred_out_dir != '':\n            warnings.warn('Saving datasamples is not supported.')\n        results['datasamples'] = preds\n    else:\n        for i, pred in enumerate(preds):\n            if not no_save_pred and pred_out_dir != '':\n                if 'img_path' in pred:\n                    img_path = osp.basename(pred.img_path)\n                    img_path = osp.splitext(img_path)[0]\n                    out_img_path = osp.join(pred_out_dir, 'preds',\n                                            img_path + '_panoptic_seg.png')\n                    out_json_path = osp.join(pred_out_dir, 'preds',\n                                             img_path + '.json')\n                else:\n                    out_img_path = osp.join(\n                        pred_out_dir, 'preds',\n                        f'{self.num_predicted_imgs}_panoptic_seg.png')\n                    out_json_path = osp.join(pred_out_dir, 'preds',\n                                             f'{self.num_predicted_imgs}.json')\n                    self.num_predicted_imgs += 1\n\n            result = {}\n            if 'pred_instances' in pred:\n                masks = pred.pred_instances.get('masks')\n                pred_instances = pred.pred_instances.numpy()\n                result = {\n                    'labels': pred_instances.labels.tolist(),\n                    'scores': pred_instances.scores.tolist()\n                }\n                if 'bboxes' in pred_instances:\n                    result['bboxes'] = pred_", "CodeBleu_score": 35.90192892914993}]}
{"instance_id": "a3c2561f199c", "generation_result": [{"code": "def build_test_pipeline(cfg: ConfigType) -> ConfigType\n\ttest_transforms = []\n    if cfg.DATASET.TEST_TRANSFORMS:\n        test_transforms.append(cfg.DATASET.TEST_TRANSFORMS[0])\n        test_transforms[0].transforms = [test_transforms[0].transforms[0]]\n        test_transforms.append(cfg.DATASET.TEST_TRANSFORMS[-1])\n        test_transforms = Compose(test_transforms)\n    else:\n        test_transforms = None\n    return test_transforms\n\n", "CodeBleu_score": 26.60510437917733}, {"code": "def build_test_pipeline(cfg: ConfigType) -> ConfigType\n\tdataset_pipeline = cfg.data.dataset.pipeline\n    test_pipeline = [dataset_pipeline[0].copy()]\n    test_pipeline[0].transforms = [test_pipeline[0].transforms[0]]\n    test_pipeline.append(dataset_pipeline[-1])\n    test_pipeline = Compose(test_pipeline)\n    return test_pipeline\n\n", "CodeBleu_score": 16.56916013880988}, {"code": "def build_test_pipeline(cfg: ConfigType) -> ConfigType\n\tdataset_pipeline = cfg.dataset.pipeline\n    test_pipeline = []\n    # copy the first transform in the dataset pipeline\n    first_transform = dataset_pipeline[0].copy()\n    # modify the first transform to include only the 'Resize' transform\n    first_transform['transforms'] = [first_transform['transforms'][0]]\n    # copy the last transform in the dataset pipeline\n    last_transform = dataset_pipeline[-1].copy()\n    # combine the modified first and copied last transforms into a new test pipeline using Compose\n    test_pipeline = Compose([first_transform, last_transform])\n    return test_pipeline", "CodeBleu_score": 26.857020392344822}]}
{"instance_id": "48a358ab5a38", "generation_result": [{"code": "def inference_mot(model: nn.Module, img: np.ndarray, frame_id: int,\n                  video_len: int) -> SampleList\n\tfrom mmtrack.models import build_model\n\n    # prepare the input data\n    data = dict(img=[img], frame_id=[frame_id], original_shape=[img.shape[:2]],\n                img_id=[frame_id], video_len=[video_len])\n\n    # build the test pipeline\n    test_pipeline = build_test_pipeline(model.cfg)\n\n    # prepare the data through the test pipeline\n    data = test_pipeline(data)\n\n    # forward the model\n    with torch.no_grad():\n        model.eval()\n        if next(model.parameters()).is_cuda:\n            # scatter to specified GPU\n            data = scatter(data, [model.device])[0]\n        else:\n            # remove the \"RoIPool\" module in CPU mode\n            model.roi_head.track_head.refine_head.remove_modules(\n                ['refine_head.roi_pool'])\n        result = model(return_loss=False, rescale=True, **data)\n\n    return result\n\n", "CodeBleu_score": 41.18370433195365}, {"code": "def inference_mot(model: nn.Module, img: np.ndarray, frame_id: int,\n                  video_len: int) -> SampleList\n\t# prepare data\n    data = {'image': img, 'frame_id': frame_id, 'original_shape': img.shape,\n            'image_id': frame_id, 'video_len': video_len}\n    data = SampleList([data])\n\n    # prepare test_pipeline\n    test_pipeline = build_test_pipeline(cfg=model.cfg)\n    data = test_pipeline(data)\n\n    # prepare model\n    if next(model.parameters()).is_cuda:\n        # use GPU\n        model.eval()\n        with torch.no_grad():\n            data = model(data)\n    else:\n        # use CPU\n        model.eval()\n        with torch.no_grad():\n            for m in model.modules():\n                if isinstance(m, (RoIPool, RoIAlign)):\n                    raise TypeError('CPU inference with RoIPool/RoIAlign is not supported.')\n            data = model(data)\n\n    return data\n\n", "CodeBleu_score": 42.340860954357154}, {"code": "def inference_mot(model: nn.Module, img: np.ndarray, frame_id: int,\n                  video_len: int) -> SampleList\n\t# prepare data\n    data = dict(\n        img=img,\n        frame_id=frame_id,\n        ori_shape=img.shape,\n        img_id=frame_id,\n        video_len=video_len,\n    )\n    # build test pipeline\n    test_pipeline = build_test_pipeline(model.cfg)\n    # build data holder\n    data_holder = SampleList()\n    # process data through test pipeline\n    data_holder = test_pipeline(data_holder, data)\n    # forward the model\n    if next(model.parameters()).is_cuda:\n        # put data on gpu\n        data_holder = data_holder.to('cuda')\n        # forward the model\n        with torch.no_grad():\n            model_outputs = model(data_holder)\n    else:\n        # put data on cpu\n        data_holder = data_holder.to('cpu')\n        # forward the model\n        with torch.no_grad():\n            model_outputs = model(data_holder)\n    # return the inference result\n    return model_outputs\n\n", "CodeBleu_score": 37.57799815231497}]}
{"instance_id": "6c4ea2ce1787", "generation_result": [{"code": "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\n\twhile True:\n        total_length = len(tokens_a) + len(tokens_b)\n        if total_length <= max_num_tokens:\n            break\n\n        trunc_tokens = tokens_a if len(tokens_a) > len(tokens_b) else tokens_b\n        assert len(trunc_tokens) >= 1\n\n        # We want to sometimes truncate from the front and sometimes from the\n        # back to add more randomness and avoid biases.\n        if rng.random() < 0.5:\n            del trunc_tokens[0]\n        else:\n            trunc_tokens.pop()\n\n", "CodeBleu_score": 74.87891258057613}, {"code": "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\n\twhile True:\n        total_length = len(tokens_a) + len(tokens_b)\n        if total_length <= max_num_tokens:\n            break\n\n        trunc_tokens = tokens_a if len(tokens_a) > len(tokens_b) else tokens_b\n        assert len(trunc_tokens) >= 1\n\n        # We want to sometimes truncate from the front and sometimes from the\n        # back to add more randomness and avoid biases.\n        if rng.random() < 0.5:\n            del trunc_tokens[0]\n        else:\n            trunc_tokens.pop()\n\n", "CodeBleu_score": 74.87891258057613}, {"code": "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\n\twhile True:\n        total_length = len(tokens_a) + len(tokens_b)\n        if total_length <= max_num_tokens:\n            break\n\n        trunc_tokens = tokens_a if len(tokens_a) > len(tokens_b) else tokens_b\n        assert len(trunc_tokens) >= 1\n\n        # We want to sometimes truncate from the front and sometimes from the\n        # back to add more randomness and avoid biases.\n        if rng.random() < 0.5:\n            del trunc_tokens[0]\n        else:\n            trunc_tokens.pop()", "CodeBleu_score": 74.87891258057613}]}
{"instance_id": "007181b6729d", "generation_result": [{"code": "def get_activation(activation_string)\n\tif not isinstance(activation_string, str):\n        return activation_string\n    if activation_string == '':\n        return None\n    if activation_string == 'relu':\n        return tf.nn.relu\n    elif activation_string == 'tanh':\n        return tf.nn.tanh\n    elif activation_string == 'sigmoid':\n        return tf.nn.sigmoid\n    elif activation_string == 'elu':\n        return tf.nn.elu\n    elif activation_string == 'selu':\n        return tf.nn.selu\n    elif activation_string == 'softplus':\n        return tf.nn.softplus\n    elif activation_string == 'softmax':\n        return tf.nn.softmax\n    elif activation_string == 'linear':\n        return tf.identity\n    else:\n        raise ValueError('Unknown activation function: ' + activation_string)", "CodeBleu_score": 34.91938674785457}, {"code": "def get_activation(activation_string)\n\tif not isinstance(activation_string, str):\n        return activation_string\n    if activation_string == '':\n        return None\n    if activation_string == 'relu':\n        return tf.nn.relu\n    if activation_string == 'tanh':\n        return tf.nn.tanh\n    if activation_string == 'sigmoid':\n        return tf.nn.sigmoid\n    if activation_string == 'softmax':\n        return tf.nn.softmax\n    if activation_string == 'identity':\n        return tf.identity\n    if activation_string == 'elu':\n        return tf.nn.elu\n    if activation_string == 'selu':\n        return tf.nn.selu\n    if activation_string == 'softplus':\n        return tf.nn.softplus\n    if activation_string == 'softsign':\n        return tf.nn.softsign\n    if activation_string == 'exponential':\n        return tf.exp\n    if activation_string == 'log':\n        return tf.log\n    if activation_string == 'sqrt':\n        return tf.sqrt\n    if activation_string == 'abs':\n        return tf.abs\n    if activation_string == 'square':\n        return tf.square\n    if activation_string == 'sign':\n        return tf.sign\n    if activation_string == 'round':\n        return tf.round\n    if activation_string == 'ceil':\n        return tf.ceil\n    if activation_string == 'floor':\n        return tf.floor\n    if activation_string == 'cos':\n        return tf.cos\n    if activation_string == 'sin':\n        return tf.sin\n    if activation_string == 'tan':\n        return tf.tan\n    if activation_string == 'acos':\n        return tf.acos\n    if activation_string == 'asin':\n        return tf.asin\n    if activation_string == 'atan':\n        return tf.atan\n    if activation_string == 'cosh':\n        return tf.cosh\n    if activation_string == 'sinh':\n        return tf.sinh\n    if activation_string == 'tanh':\n        return tf.tanh", "CodeBleu_score": 27.785042360384978}, {"code": "def get_activation(activation_string)\n\tif not isinstance(activation_string, str):\n        return activation_string\n    if activation_string == '':\n        return None\n    elif activation_string == 'relu':\n        return tf.nn.relu\n    elif activation_string == 'elu':\n        return tf.nn.elu\n    elif activation_string == 'tanh':\n        return tf.nn.tanh\n    elif activation_string == 'sigmoid':\n        return tf.nn.sigmoid\n    elif activation_string == 'softmax':\n        return tf.nn.softmax\n    else:\n        raise ValueError('Unsupported activation function: {}'.format(activation_string))\n\n", "CodeBleu_score": 34.15711351922178}]}
{"instance_id": "d5d6e0315899", "generation_result": [{"code": "def dropout(input_tensor, dropout_prob)\n\tif dropout_prob is None or dropout_prob == 0.0:\n        return input_tensor\n\n    output = tf.nn.dropout(input_tensor, 1.0 - dropout_prob)\n    return output", "CodeBleu_score": 87.68572576748926}, {"code": "def dropout(input_tensor, dropout_prob)\n\tif dropout_prob is None or dropout_prob == 0.0:\n        return input_tensor\n\n    output = tf.nn.dropout(input_tensor, 1.0 - dropout_prob)\n    return output\n\n", "CodeBleu_score": 87.68572576748926}, {"code": "def dropout(input_tensor, dropout_prob)\n\tif dropout_prob is None or dropout_prob == 0.0:\n        return input_tensor\n\n    output = tf.nn.dropout(input_tensor, 1.0 - dropout_prob)\n    return output", "CodeBleu_score": 87.68572576748926}]}
{"instance_id": "828917b9ac53", "generation_result": [{"code": "def create_attention_mask_from_input_mask(from_tensor, to_mask)\n\tfrom_shape = get_shape_list(from_tensor, expected_rank=[2,3])\n    batch_size = from_shape[0]\n    from_seq_length = from_shape[1]\n\n    to_shape = get_shape_list(to_mask, expected_rank=2)\n    to_seq_length = to_shape[1]\n\n    to_mask = tf.cast(tf.reshape(to_mask, (batch_size, 1, to_seq_length)), tf.float32)\n    # We don't assume that `from_tensor` is a mask (although it could be). We\n    # don't actually care if we attend *from* padding tokens (only *to* padding)\n    # tokens so we create a tensor of all ones.\n    #\n    # `broadcast_ones` = [batch_size, from_seq_length, 1]\n    broadcast_ones = tf.ones(shape=[batch_size, from_seq_length, 1], dtype=tf.float32)\n\n    # Here we broadcast along two dimensions to create the mask.\n    mask = broadcast_ones * to_mask\n\n    return mask\n\n", "CodeBleu_score": 54.99828429488135}, {"code": "def create_attention_mask_from_input_mask(from_tensor, to_mask)\n\tfrom_shape = get_shape_list(from_tensor, expected_rank=[2, 3])\n    batch_size = from_shape[0]\n    from_seq_length = from_shape[1]\n\n    to_shape = get_shape_list(to_mask, expected_rank=2)\n    to_seq_length = to_shape[1]\n\n    to_mask = tf.cast(\n        tf.reshape(to_mask, (batch_size, 1, to_seq_length)),\n        dtype=from_tensor.dtype)\n\n    # We don't assume that `from_tensor` is a mask (although it could be). We\n    # don't actually care if we attend *from* padding tokens (only *to* padding)\n    # tokens so we create a tensor of all ones.\n    #\n    # `broadcast_ones` = [batch_size, from_seq_length, 1]\n    broadcast_ones = tf.ones(\n        shape=[batch_size, from_seq_length, 1], dtype=from_tensor.dtype)\n\n    # Here we broadcast along two dimensions to create the mask.\n    mask = broadcast_ones * to_mask\n\n    return mask\n\n", "CodeBleu_score": 52.92327489967123}, {"code": "def create_attention_mask_from_input_mask(from_tensor, to_mask)\n\tfrom_shape = get_shape_list(from_tensor, expected_rank=[2, 3])\n    batch_size = from_shape[0]\n    from_seq_length = from_shape[1]\n\n    to_shape = get_shape_list(to_mask, expected_rank=2)\n    to_seq_length = to_shape[1]\n\n    to_mask = tf.cast(tf.reshape(to_mask, [batch_size, 1, to_seq_length]), tf.float32)\n\n    # We don't assume that `from_tensor` is a mask (although it could be). We\n    # don't actually care if we attend *from* padding tokens (only *to* padding)\n    # tokens so we create a tensor of all ones.\n    #\n    # `broadcast_ones` = [batch_size, from_seq_length, 1]\n    broadcast_ones = tf.ones(shape=[batch_size, from_seq_length, 1], dtype=tf.float32)\n\n    # Here we broadcast along two dimensions to create the mask.\n    mask = broadcast_ones * to_mask\n\n    return mask\n\n", "CodeBleu_score": 60.05374192462243}]}
{"instance_id": "d7798e41a723", "generation_result": [{"code": "def transpose_for_scores(input_tensor, batch_size, num_attention_heads,\n                           seq_length, width)\n\tinput_tensor = tf.reshape(\n        input_tensor, [batch_size, seq_length, num_attention_heads, width])\n\n    return tf.transpose(input_tensor, [0, 2, 1, 3])\n\n", "CodeBleu_score": 49.526466148446104}, {"code": "def transpose_for_scores(input_tensor, batch_size, num_attention_heads,\n                           seq_length, width)\n\toutput_tensor = tf.reshape(\n        input_tensor, [batch_size, seq_length, num_attention_heads, width])\n\n    output_tensor = tf.transpose(output_tensor, [0, 2, 1, 3])\n    return output_tensor\n\n", "CodeBleu_score": 60.207631454348444}, {"code": "def transpose_for_scores(input_tensor, batch_size, num_attention_heads,\n                           seq_length, width)\n\tif seq_length != input_tensor.shape[1]:\n        raise ValueError(\n            \"The seq_length (\" + str(seq_length) + \") must match the \"\n            \"dimension 1 of the input tensor (\" + str(input_tensor.shape[1]) +\n            \")\")\n\n    if width != input_tensor.shape[2]:\n        raise ValueError(\n            \"The width (\" + str(width) + \") must match the \"\n            \"dimension 2 of the input tensor (\" + str(input_tensor.shape[2]) +\n            \")\")\n\n    output_tensor = tf.reshape(\n        input_tensor, [batch_size, seq_length, num_attention_heads, width])\n\n    output_tensor = tf.transpose(output_tensor, [0, 2, 1, 3])\n    return output_tensor\n\n", "CodeBleu_score": 74.01171243762165}]}
{"instance_id": "711d466ace09", "generation_result": [{"code": "def reshape_to_matrix(input_tensor)\n\tif len(input_tensor.shape) < 2:\n        raise ValueError(\"Input tensor must have rank 2 or higher.\")\n    elif len(input_tensor.shape) == 2:\n        return input_tensor\n    else:\n        return tf.reshape(input_tensor, [-1, input_tensor.shape[-1]])\n", "CodeBleu_score": 33.13881092717666}, {"code": "def reshape_to_matrix(input_tensor)\n\tif len(input_tensor.shape) < 2:\n        raise ValueError(\n            \"Input tensor must have rank 2 or higher, but it has rank {}\".format(\n                len(input_tensor.shape)\n            )\n        )\n    elif len(input_tensor.shape) == 2:\n        return input_tensor\n    else:\n        return tf.reshape(input_tensor, [-1, input_tensor.shape[-1]])\n\n", "CodeBleu_score": 32.10674488220463}, {"code": "def reshape_to_matrix(input_tensor)\n\tif len(input_tensor.shape) < 2:\n        raise ValueError('The input tensor must have rank 2 or higher.')\n\n    if len(input_tensor.shape) == 2:\n        return input_tensor\n\n    else:\n        width = input_tensor.shape[-1]\n        return tf.reshape(input_tensor, [-1, width])\n\n", "CodeBleu_score": 42.27890538349559}]}
{"instance_id": "7adcd08f531e", "generation_result": [{"code": "def get_train_examples(self, data_dir)\n\texamples = []\n    tf.logging.info(\"LOOKING AT {}\".format(os.path.join(data_dir, \"train.tsv\")))\n    with tf.gfile.Open(os.path.join(data_dir, \"train.tsv\"), \"r\") as f:\n        reader = csv.reader(f, delimiter=\"\\t\", quotechar=None)\n        header = next(reader)\n        for i, line in enumerate(reader):\n            text_a = line[3]\n            text_b = line[4]\n            label = line[0]\n            if label == \"contradictory\":\n                label = \"contradiction\"\n            examples.append(\n                InputExample(guid=i, text_a=text_a, text_b=text_b, label=label))\n    return examples\n\n", "CodeBleu_score": 37.53712234126609}, {"code": "def get_train_examples(self, data_dir)\n\ttrain_data = pd.read_csv(os.path.join(data_dir, \"train.tsv\"), sep=\"\\t\", header=None, names=[\"label\", \"text_a\", \"text_b\"])\n    train_examples = []\n    for (i, row) in train_data.iterrows():\n        text_a = tokenization.convert_to_unicode(row[\"text_a\"])\n        text_b = tokenization.convert_to_unicode(row[\"text_b\"])\n        label = tokenization.convert_to_unicode(row[\"label\"])\n        if label == \"contradictory\":\n            label = \"contradiction\"\n        train_examples.append(\n            InputExample(guid=str(i), text_a=text_a, text_b=text_b, label=label))\n    return train_examples", "CodeBleu_score": 31.538023033023016}, {"code": "def get_train_examples(self, data_dir)\n\ttrain_file = os.path.join(data_dir, \"train.tsv\")\n    train_examples = self._create_examples(\n        self._read_tsv(train_file), \"train\")\n    return train_examples\n\n", "CodeBleu_score": 10.568603228927968}]}
{"instance_id": "26a0bb96dab9", "generation_result": [{"code": "def metric_fn(per_example_loss, label_ids, logits, is_real_example)\n\tpredictions = tf.argmax(input=logits, axis=-1, output_type=tf.int32)\n    accuracy = tf.metrics.accuracy(\n        labels=label_ids, predictions=predictions, weights=is_real_example)\n    loss = tf.metrics.mean(values=per_example_loss, weights=is_real_example)\n    return {\n        \"eval_accuracy\": accuracy,\n        \"eval_loss\": loss,\n    }\n\n", "CodeBleu_score": 62.35452659374221}, {"code": "def metric_fn(per_example_loss, label_ids, logits, is_real_example)\n\tpredictions = tf.argmax(input=logits, axis=-1, output_type=tf.int32)\n    accuracy = tf.metrics.accuracy(\n        labels=label_ids, predictions=predictions, weights=is_real_example)\n    loss = tf.metrics.mean(values=per_example_loss, weights=is_real_example)\n    return {\n        'eval_accuracy': accuracy,\n        'eval_loss': loss,\n    }\n\n", "CodeBleu_score": 56.7415239245729}, {"code": "def metric_fn(per_example_loss, label_ids, logits, is_real_example)\n\tpredictions = tf.argmax(logits, axis=-1, output_type=tf.int32)\n    accuracy = tf.metrics.accuracy(\n        labels=label_ids,\n        predictions=predictions,\n        weights=is_real_example)\n    loss = tf.metrics.mean(\n        values=per_example_loss,\n        weights=is_real_example)\n    return {\n        'eval_accuracy': accuracy,\n        'eval_loss': loss,\n    }", "CodeBleu_score": 54.44676904536541}]}
{"instance_id": "6154e4d28509", "generation_result": [{"code": "def convert_examples_to_features(examples, label_list, max_seq_length,\n                                 tokenizer)\n\tfeatures = []\n  for (ex_index, example) in enumerate(examples):\n    if ex_index % 10000 == 0:\n      tf.logging.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n\n    feature = convert_single_example(ex_index, example, label_list,\n                                     max_seq_length, tokenizer)\n\n    features.append(feature)\n  return features\n\n", "CodeBleu_score": 84.62164893589323}, {"code": "def convert_examples_to_features(examples, label_list, max_seq_length,\n                                 tokenizer)\n\t\"\"\"Convert a set of `InputExample`s to a list of `InputFeatures`.\"\"\"\n\n  features = []\n  for (ex_index, example) in enumerate(examples):\n    if ex_index % 10000 == 0:\n      tf.logging.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n\n    feature = convert_single_example(ex_index, example, label_list,\n                                     max_seq_length, tokenizer)\n\n    features.append(feature)\n  return features\n\n", "CodeBleu_score": 84.7980470126739}, {"code": "def convert_examples_to_features(examples, label_list, max_seq_length,\n                                 tokenizer)\n\tfeatures = []\n  for (ex_index, example) in enumerate(examples):\n    if ex_index % 10000 == 0:\n      tf.logging.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n\n    feature = convert_single_example(ex_index, example, label_list,\n                                     max_seq_length, tokenizer)\n\n    features.append(feature)\n  return features\n\n", "CodeBleu_score": 84.62164893589323}]}
{"instance_id": "e09c0db71ebb", "generation_result": [{"code": "def get_next_sentence_output(bert_config, input_tensor, labels)\n\t# Input Tensor: [batch_size, seq_length, hidden_size]\n    # Label Tensor: [batch_size, 1]\n    next_sentence_labels = tf.reshape(labels, [-1])\n    # Output Tensor: [batch_size, 2]\n    output_weights = tf.get_variable(\n        \"cls/seq_relationship/output_weights\",\n        shape=[2, bert_config.hidden_size],\n        initializer=modeling.create_initializer(bert_config.initializer_range))\n    output_bias = tf.get_variable(\n        \"cls/seq_relationship/output_bias\",\n        shape=[2],\n        initializer=tf.zeros_initializer())\n\n    logits = tf.matmul(input_tensor, output_weights, transpose_b=True)\n    logits = tf.nn.bias_add(logits, output_bias)\n    log_probs = tf.nn.log_softmax(logits, axis=-1)\n    labels = tf.one_hot(next_sentence_labels, depth=2, dtype=tf.float32)\n    per_example_loss = -tf.reduce_sum(labels * log_probs, axis=-1)\n    loss = tf.reduce_mean(per_example_loss)\n    return (loss, per_example_loss, log_probs)\n\n", "CodeBleu_score": 53.724471902317795}, {"code": "def get_next_sentence_output(bert_config, input_tensor, labels)\n\t# Input Tensor Shape: [batch_size, seq_length, hidden_size]\n    # Output Tensor Shape: [batch_size, 2]\n    input_tensor = tf.layers.dense(\n        input_tensor,\n        units=bert_config.hidden_size,\n        activation=modeling.get_activation(bert_config.hidden_act),\n        kernel_initializer=modeling.create_initializer(\n            bert_config.initializer_range))\n    input_tensor = modeling.layer_norm(input_tensor)\n    output_weights = tf.get_variable(\n        \"cls/seq_relationship/output_weights\",\n        shape=[2, bert_config.hidden_size],\n        initializer=modeling.create_initializer(bert_config.initializer_range))\n    output_bias = tf.get_variable(\n        \"cls/seq_relationship/output_bias\",\n        shape=[2],\n        initializer=tf.zeros_initializer())\n\n    logits = tf.matmul(input_tensor, output_weights, transpose_b=True)\n    logits = tf.nn.bias_add(logits, output_bias)\n    log_probs = tf.nn.log_softmax(logits, axis=-1)\n    labels = tf.reshape(labels, [-1])\n    one_hot_labels = tf.one_hot(labels, depth=2, dtype=tf.float32)\n    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n    loss = tf.reduce_mean(per_example_loss)\n    return (loss, per_example_loss, log_probs)\n\n", "CodeBleu_score": 54.00396898083669}, {"code": "def get_next_sentence_output(bert_config, input_tensor, labels)\n\t# Input Tensor: [batch_size, seq_length, hidden_size]\n    # Label Tensor: [batch_size, 2]\n    # Output Tensor: [batch_size, 2]\n    with tf.variable_scope(\"cls/seq_relationship\"):\n        output_weights = tf.get_variable(\n            \"output_weights\",\n            shape=[2, bert_config.hidden_size],\n            initializer=modeling.create_initializer(bert_config.initializer_range))\n        output_bias = tf.get_variable(\n            \"output_bias\", shape=[2], initializer=tf.zeros_initializer())\n\n        logits = tf.matmul(input_tensor, output_weights, transpose_b=True)\n        logits = tf.nn.bias_add(logits, output_bias)\n        log_probs = tf.nn.log_softmax(logits, axis=-1)\n        labels = tf.reshape(labels, [-1])\n        one_hot_labels = tf.one_hot(labels, depth=2, dtype=tf.float32)\n        per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n        loss = tf.reduce_mean(per_example_loss)\n        return (loss, per_example_loss, log_probs)\n\n", "CodeBleu_score": 86.94900271179796}]}
{"instance_id": "1f66b020f485", "generation_result": [{"code": "def gather_indexes(sequence_tensor, positions)\n\tsequence_shape = get_shape_list(sequence_tensor, expected_rank=3)\n    batch_size = sequence_shape[0]\n    seq_length = sequence_shape[1]\n    width = sequence_shape[2]\n\n    flat_offsets = tf.reshape(tf.range(0, batch_size, dtype=tf.int32) * seq_length, [-1, 1])\n    flat_positions = tf.reshape(positions + flat_offsets, [-1])\n    flat_sequence_tensor = tf.reshape(sequence_tensor, [batch_size * seq_length, width])\n    output_tensor = tf.gather(flat_sequence_tensor, flat_positions)\n    return output_tensor\n\n", "CodeBleu_score": 53.089401390469625}, {"code": "def gather_indexes(sequence_tensor, positions)\n\tsequence_shape = get_shape_list(sequence_tensor, expected_rank=3)\n    batch_size = sequence_shape[0]\n    seq_length = sequence_shape[1]\n    width = sequence_shape[2]\n\n    flat_offsets = tf.reshape(\n        tf.range(0, batch_size, dtype=tf.int32) * seq_length, [-1, 1])\n    flat_positions = tf.reshape(positions + flat_offsets, [-1])\n    flat_sequence_tensor = tf.reshape(sequence_tensor,\n                                      [batch_size * seq_length, width])\n    output_tensor = tf.gather(flat_sequence_tensor, flat_positions)\n    return output_tensor\n\n", "CodeBleu_score": 56.45491338727393}, {"code": "def gather_indexes(sequence_tensor, positions)\n\tsequence_shape = modeling.get_shape_list(sequence_tensor, expected_rank=3)\n    batch_size = sequence_shape[0]\n    seq_length = sequence_shape[1]\n    width = sequence_shape[2]\n\n    flat_offsets = tf.reshape(tf.range(0, batch_size, dtype=tf.int32) * seq_length, [-1, 1])\n    flat_positions = tf.reshape(positions + flat_offsets, [-1])\n    output_tensor = tf.gather(tf.reshape(sequence_tensor, [batch_size * seq_length, width]), flat_positions)\n    return output_tensor\n\n", "CodeBleu_score": 47.14339859790238}]}
{"instance_id": "956bffee9a33", "generation_result": [{"code": "def _decode_record(record, name_to_features)\n\texample = tf.io.parse_single_example(record, name_to_features)\n\n    # tf.Example only supports tf.int64, but the TPU only supports tf.int32.\n    # So cast all int64 to int32.\n    for name in list(example.keys()):\n        t = example[name]\n        if t.dtype == tf.int64:\n            t = tf.cast(t, tf.int32)\n        example[name] = t\n\n    return example\n", "CodeBleu_score": 59.080951090574594}, {"code": "def _decode_record(record, name_to_features)\n\texample = tf.io.parse_single_example(record, name_to_features)\n\n    # tf.Example only supports tf.int64, but the TPU only supports tf.int32.\n    # So cast all int64 to int32.\n    for name in list(example.keys()):\n        t = example[name]\n        if t.dtype == tf.int64:\n            t = tf.cast(t, tf.int32)\n        example[name] = t\n\n    return example\n\n", "CodeBleu_score": 59.080951090574594}, {"code": "def _decode_record(record, name_to_features)\n\texample = tf.io.parse_single_example(record, name_to_features)\n\n    # tf.Example only supports tf.int64, but the TPU only supports tf.int32.\n    # So cast all int64 to int32.\n    for name in list(example.keys()):\n        t = example[name]\n        if t.dtype == tf.int64:\n            t = tf.cast(t, tf.int32)\n        example[name] = t\n\n    return example\n\n", "CodeBleu_score": 59.080951090574594}]}
{"instance_id": "6b97f354e641", "generation_result": [{"code": "def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer,\n                         orig_answer_text)\n\ttok_answer_text = \" \".join(tokenizer.tokenize(orig_answer_text))\n\n    for new_start in range(input_start, input_end + 1):\n        for new_end in range(input_end, new_start - 1, -1):\n            text_span = \" \".join(doc_tokens[new_start:(new_end + 1)])\n            if text_span == tok_answer_text:\n                return new_start, new_end\n\n    return input_start, input_end\n\n", "CodeBleu_score": 69.18002743421648}, {"code": "def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer,\n                         orig_answer_text)\n\ttok_answer_text = \" \".join(tokenizer.tokenize(orig_answer_text))\n\n    for new_start in range(input_start, input_end + 1):\n        for new_end in range(input_end, new_start - 1, -1):\n            text_span = \" \".join(doc_tokens[new_start:(new_end + 1)])\n            if text_span == tok_answer_text:\n                return new_start, new_end\n\n    return input_start, input_end\n\n", "CodeBleu_score": 69.18002743421648}, {"code": "def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer,\n                         orig_answer_text)\n\ttok_answer_text = \" \".join(tokenizer.tokenize(orig_answer_text))\n\n    for new_start in range(input_start, input_end + 1):\n        for new_end in range(input_end, new_start - 1, -1):\n            text_span = \" \".join(doc_tokens[new_start:(new_end + 1)])\n            if text_span == tok_answer_text:\n                return new_start, new_end\n\n    return input_start, input_end\n\n", "CodeBleu_score": 69.18002743421648}]}
{"instance_id": "aba334b92b34", "generation_result": [{"code": "def _check_is_max_context(doc_spans, cur_span_index, position)\n\tbest_score = None\n    best_span_index = None\n    for (span_index, doc_span) in enumerate(doc_spans):\n        end = doc_span.start + doc_span.length - 1\n        if position < doc_span.start:\n            continue\n        if position > end:\n            continue\n        num_left_context = position - doc_span.start\n        num_right_context = end - position\n        score = min(num_left_context, num_right_context) + 0.01 * doc_span.length\n        if best_score is None or score > best_score:\n            best_score = score\n            best_span_index = span_index\n\n    return cur_span_index == best_span_index\n\n", "CodeBleu_score": 78.7167688253658}, {"code": "def _check_is_max_context(doc_spans, cur_span_index, position)\n\tbest_score = None\n    best_span_index = None\n    for (span_index, doc_span) in enumerate(doc_spans):\n        end = doc_span.start + doc_span.length - 1\n        if position < doc_span.start:\n            continue\n        if position > end:\n            continue\n        num_left_context = position - doc_span.start\n        num_right_context = end - position\n        score = min(num_left_context, num_right_context) + 0.01 * doc_span.length\n        if best_score is None or score > best_score:\n            best_score = score\n            best_span_index = span_index\n\n    return cur_span_index == best_span_index\n\n", "CodeBleu_score": 78.7167688253658}, {"code": "def _check_is_max_context(doc_spans, cur_span_index, position)\n\tbest_score = None\n    best_span_index = None\n    for (span_index, doc_span) in enumerate(doc_spans):\n        end = doc_span.start + doc_span.length - 1\n        if position < doc_span.start:\n            continue\n        if position > end:\n            continue\n        num_left_context = position - doc_span.start\n        num_right_context = end - position\n        score = min(num_left_context, num_right_context) + 0.01 * doc_span.length\n        if best_score is None or score > best_score:\n            best_score = score\n            best_span_index = span_index\n\n    return cur_span_index == best_span_index\n\n", "CodeBleu_score": 78.7167688253658}]}
{"instance_id": "cf92ad509f6a", "generation_result": [{"code": "def load_vocab(vocab_file)\n\tvocab = collections.OrderedDict()\n    index = 0\n    with tf.gfile.GFile(vocab_file, \"r\") as reader:\n        while True:\n            token = convert_to_unicode(reader.readline())\n            if not token:\n                break\n            token = token.strip()\n            vocab[token] = index\n            index += 1\n    return vocab\n\n", "CodeBleu_score": 74.95325416376971}, {"code": "def load_vocab(vocab_file)\n\tvocab = OrderedDict()\n    index = 0\n    with open(vocab_file, \"r\", encoding=\"utf-8\") as reader:\n        while True:\n            token = convert_to_unicode(reader.readline())\n            if not token:\n                break\n            token = token.strip()\n            vocab[token] = index\n            index += 1\n    return vocab\n\n", "CodeBleu_score": 64.72616120033337}, {"code": "def load_vocab(vocab_file)\n\tvocab = collections.OrderedDict()\n    index = 0\n    with tf.gfile.GFile(vocab_file, \"r\") as reader:\n        while True:\n            token = convert_to_unicode(reader.readline())\n            if not token:\n                break\n            token = token.strip()\n            vocab[token] = index\n            index += 1\n    return vocab\n\n", "CodeBleu_score": 74.95325416376971}]}
{"instance_id": "6c865803ac12", "generation_result": [{"code": "def convert_by_vocab(vocab, items)\n\toutput = []\n    for item in items:\n        output.append(vocab[item])\n    return output\n\n", "CodeBleu_score": 79.88651544154007}, {"code": "def convert_by_vocab(vocab, items)\n\toutput = []\n    for item in items:\n        output.append(vocab[item])\n    return output\n\n", "CodeBleu_score": 79.88651544154007}, {"code": "def convert_by_vocab(vocab, items)\n\toutput = []\n    for item in items:\n        output.append(vocab[item])\n    return output\n\n", "CodeBleu_score": 79.88651544154007}]}
{"instance_id": "e93b9200dc4a", "generation_result": [{"code": "def tokenize(self, text)\n\tsplit_tokens = []\n    for token in self.basic_tokenizer.tokenize(text):\n        for sub_token in self.wordpiece_tokenizer.tokenize(token):\n            split_tokens.append(sub_token)\n    return split_tokens", "CodeBleu_score": 88.22929475931163}, {"code": "def tokenize(self, text)\n\ttokens = self.basic_tokenizer.tokenize(text)\n    output_tokens = []\n    for token in tokens:\n        for sub_token in self.wordpiece_tokenizer.tokenize(token):\n            output_tokens.append(sub_token)\n    return output_tokens\n\n\n", "CodeBleu_score": 47.83202002337206}, {"code": "def tokenize(self, text)\n\tsplit_tokens = []\n    for token in self.basic_tokenizer.tokenize(text):\n        for sub_token in self.wordpiece_tokenizer.tokenize(token):\n            split_tokens.append(sub_token)\n\n    return split_tokens\n\n", "CodeBleu_score": 88.22929475931163}]}
{"instance_id": "2ff6f6db53c9", "generation_result": [{"code": "def safe_load(path, offset, duration, sample_rate, dtype)\n\ttry:\n        print(f'Loading {path}...')\n        audio, sr = librosa.load(path, offset=offset, duration=duration, sr=sample_rate)\n        audio = audio.astype(dtype)\n        print(f'Loaded {path} successfully.')\n        return audio, False\n    except Exception as e:\n        print(f'Error loading {path}: {e}')\n        return -1.0, True\n\n", "CodeBleu_score": 32.82092664264187}, {"code": "def safe_load(path, offset, duration, sample_rate, dtype)\n\ttry:\n        data = load(path, offset, duration, sample_rate, dtype)\n        return data, False\n    except Exception as e:\n        print(f'Error loading audio data from {path}')\n        print(f'Error: {e}')\n        return -1.0, True\n\n", "CodeBleu_score": 19.099679360378136}, {"code": "def safe_load(path, offset, duration, sample_rate, dtype)\n\ttry:\n        print(\"Loading file: \" + path)\n        data, _ = librosa.load(path, offset=offset, duration=duration, sr=sample_rate)\n        print(\"Successfully loaded file: \" + path)\n        return data, False\n    except Exception as e:\n        print(\"Error loading file: \" + path)\n        print(e)\n        return np.full(duration * sample_rate, -1.0), True\n\n", "CodeBleu_score": 34.217426809304285}]}
{"instance_id": "12fbe1f03d8f", "generation_result": [{"code": "def to_n_channels(waveform: tf.Tensor, n_channels: int) -> tf.Tensor\n\tif waveform.shape[1] == n_channels:\n        return waveform\n    elif waveform.shape[1] < n_channels:\n        return tf.tile(waveform, [1, n_channels // waveform.shape[1]])[:, :n_channels]\n    else:\n        return waveform[:, :n_channels]\n\n", "CodeBleu_score": 48.63754053616156}, {"code": "def to_n_channels(waveform: tf.Tensor, n_channels: int) -> tf.Tensor\n\tif n_channels <= 0:\n        raise ValueError(\"n_channels must be positive.\")\n    if waveform.shape[1] == n_channels:\n        return waveform\n    elif waveform.shape[1] > n_channels:\n        return waveform[:, :n_channels]\n    else:\n        tiled_waveform = tf.tile(waveform, [1, n_channels // waveform.shape[1] + 1])\n        return tiled_waveform[:, :n_channels]\n\n", "CodeBleu_score": 41.77991768488793}, {"code": "def to_n_channels(waveform: tf.Tensor, n_channels: int) -> tf.Tensor\n\tif n_channels > waveform.shape[-1]:\n        waveform = tf.tile(waveform, [1, n_channels // waveform.shape[-1]])\n    waveform = waveform[..., :n_channels]\n    return waveform\n\n", "CodeBleu_score": 44.60601904856553}]}
{"instance_id": "5e215f029b2e", "generation_result": [{"code": "def spectrogram_to_db_uint(\n    spectrogram: tf.Tensor, db_range: float = 100.0, **kwargs\n) -> tf.Tensor\n\tspectrogram = gain_to_db(spectrogram, **kwargs)\n    spectrogram = tf.clip_by_value(spectrogram, 0, db_range)\n    spectrogram = tf.cast(\n        (spectrogram / db_range) * 255.0, dtype=tf.uint8, name=\"spectrogram_to_db_uint\"\n    )\n    return spectrogram", "CodeBleu_score": 41.058656164379904}, {"code": "def spectrogram_to_db_uint(\n    spectrogram: tf.Tensor, db_range: float = 100.0, **kwargs\n) -> tf.Tensor\n\tdb_spectrogram = gain_to_db(spectrogram, **kwargs)\n    db_spectrogram = db_spectrogram - tf.math.reduce_max(db_spectrogram)\n    db_spectrogram = tf.clip_by_value(db_spectrogram, -db_range, 0.0)\n    db_spectrogram = (db_spectrogram + db_range) / (2.0 * db_range)\n    db_spectrogram = tf.cast(db_spectrogram * 255.0, tf.uint8)\n    return db_spectrogram\n\n", "CodeBleu_score": 42.18371388720623}, {"code": "def spectrogram_to_db_uint(\n    spectrogram: tf.Tensor, db_range: float = 100.0, **kwargs\n) -> tf.Tensor\n\tspectrogram = gain_to_db(spectrogram, **kwargs)\n    spectrogram = tf.clip_by_value(spectrogram, 0.0, db_range)\n    spectrogram = tf.cast(spectrogram, tf.uint8)\n    return spectrogram\n\n", "CodeBleu_score": 35.84339064161738}]}
{"instance_id": "b65c0968dae7", "generation_result": [{"code": "def compute_spectrogram_tf(\n    waveform: tf.Tensor,\n    frame_length: int = 2048,\n    frame_step: int = 512,\n    spec_exponent: float = 1.0,\n    window_exponent: float = 1.0,\n) -> tf.Tensor\n\t# Apply Hann window function\n    hann_window = tf.signal.hann_window(frame_length, periodic=True)\n    hann_window = tf.math.pow(hann_window, window_exponent)\n\n    # Perform STFT\n    stft = tf.signal.stft(waveform, frame_length, frame_step, hann_window)\n    stft = tf.math.pow(tf.math.abs(stft), spec_exponent)\n\n    # Transpose the resulting tensor\n    stft = tf.transpose(stft)\n\n    return stft\n\n", "CodeBleu_score": 36.54121676410628}, {"code": "def compute_spectrogram_tf(\n    waveform: tf.Tensor,\n    frame_length: int = 2048,\n    frame_step: int = 512,\n    spec_exponent: float = 1.0,\n    window_exponent: float = 1.0,\n) -> tf.Tensor\n\twindow_size = frame_length\n    window = tf.signal.hann_window(window_size, periodic=True) ** window_exponent\n    stft = tf.signal.stft(\n        waveform,\n        frame_length=frame_length,\n        frame_step=frame_step,\n        fft_length=frame_length,\n        window_fn=lambda frame_length, dtype: window,\n        pad_end=True,\n    )\n    spectrogram = tf.math.pow(tf.math.abs(stft), spec_exponent)\n    spectrogram = tf.transpose(spectrogram, perm=[1, 0, 2])\n    return spectrogram\n\n", "CodeBleu_score": 39.9222679830928}, {"code": "def compute_spectrogram_tf(\n    waveform: tf.Tensor,\n    frame_length: int = 2048,\n    frame_step: int = 512,\n    spec_exponent: float = 1.0,\n    window_exponent: float = 1.0,\n) -> tf.Tensor\n\t# Compute the spectrogram\n    spectrogram = tf.signal.stft(\n        waveform,\n        frame_length=frame_length,\n        frame_step=frame_step,\n        fft_length=frame_length,\n        window_fn=tf.signal.hann_window(frame_length, periodic=False) ** window_exponent,\n    )\n    # Compute the absolute value of the complex spectrogram\n    spectrogram = tf.abs(spectrogram)\n    # Raise the absolute value to a specified exponent\n    spectrogram = spectrogram ** spec_exponent\n    # Transpose the spectrogram\n    spectrogram = tf.transpose(spectrogram)\n    return spectrogram\n\n", "CodeBleu_score": 37.03694691643034}]}
{"instance_id": "a4686140a634", "generation_result": [{"code": "def time_stretch(\n    spectrogram: tf.Tensor,\n    factor: float = 1.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\tif factor == 1.0:\n        return spectrogram\n    new_time_dim = tf.cast(tf.shape(spectrogram)[1] * factor, tf.int32)\n    spectrogram = tf.image.resize(\n        spectrogram, [tf.shape(spectrogram)[0], new_time_dim], method=method\n    )\n    if new_time_dim > tf.shape(spectrogram)[1]:\n        spectrogram = spectrogram[:, : tf.shape(spectrogram)[1]]\n    else:\n        spectrogram = tf.pad(spectrogram, [[0, 0], [0, new_time_dim - tf.shape(spectrogram)[1]]])\n    return spectrogram\n\n\n", "CodeBleu_score": 43.91768559641891}, {"code": "def time_stretch(\n    spectrogram: tf.Tensor,\n    factor: float = 1.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\tif factor == 1.0:\n        return spectrogram\n    if factor <= 0.0:\n        raise ValueError(\"Factor must be a positive number.\")\n    if method not in [\n        tf.image.ResizeMethod.BILINEAR,\n        tf.image.ResizeMethod.NEAREST_NEIGHBOR,\n        tf.image.ResizeMethod.BICUBIC,\n        tf.image.ResizeMethod.AREA,\n    ]:\n        raise ValueError(\"Unknown interpolation method.\")\n    # get the shape of the spectrogram\n    _, time_dim, _ = spectrogram.shape\n    # calculate the new time dimension\n    new_time_dim = tf.cast(time_dim * factor, dtype=tf.int32)\n    # resize the spectrogram\n    spectrogram = tf.image.resize(\n        spectrogram,\n        [tf.shape(spectrogram)[0], new_time_dim],\n        method=method,\n        preserve_aspect_ratio=False,\n        antialias=True,\n    )\n    # crop or pad the spectrogram to match the original time dimension\n    if new_time_dim > time_dim:\n        spectrogram = spectrogram[:, :time_dim, :]\n    else:\n        spectrogram = tf.pad(\n            spectrogram,\n            [[0, 0], [0, time_dim - new_time_dim], [0, 0]],\n            mode=\"CONSTANT\",\n            constant_values=0.0,\n        )\n    return spectrogram\n\n", "CodeBleu_score": 39.75638961468049}, {"code": "def time_stretch(\n    spectrogram: tf.Tensor,\n    factor: float = 1.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\tif factor == 1.0:\n        return spectrogram\n\n    time_dim = spectrogram.shape[1]\n    new_time_dim = int(time_dim * factor)\n\n    # resize spectrogram\n    spectrogram = tf.image.resize(\n        spectrogram, (spectrogram.shape[0], new_time_dim), method=method\n    )\n\n    # crop or pad spectrogram to original dimensions\n    if new_time_dim < time_dim:\n        # crop\n        spectrogram = spectrogram[:, :time_dim]\n    elif new_time_dim > time_dim:\n        # pad\n        spectrogram = tf.pad(\n            spectrogram, [[0, 0], [0, time_dim - new_time_dim]], constant_values=0.0\n        )\n\n    return spectrogram\n\n\n", "CodeBleu_score": 36.614905292274734}]}
{"instance_id": "6dd01101d4ae", "generation_result": [{"code": "def pitch_shift(\n    spectrogram: tf.Tensor,\n    semitone_shift: float = 0.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t# Calculate the resizing factor\n    resize_factor = 2 ** (semitone_shift / 12)\n\n    # Resize the spectrogram\n    spectrogram = tf.image.resize(spectrogram, [spectrogram.shape[0], spectrogram.shape[1] * resize_factor], method=method)\n\n    # Pad the resized spectrogram to maintain the original dimensions\n    spectrogram = tf.pad(spectrogram, [[0, 0], [0, int(spectrogram.shape[1] - spectrogram.shape[1] * resize_factor)]])\n\n    return spectrogram\n\n", "CodeBleu_score": 27.3094573874187}, {"code": "def pitch_shift(\n    spectrogram: tf.Tensor,\n    semitone_shift: float = 0.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t# calculate the resizing factor\n    factor = 2 ** (semitone_shift / 12)\n\n    # resize the spectrogram\n    resized_spectrogram = tf.image.resize(\n        spectrogram,\n        [spectrogram.shape[0], int(spectrogram.shape[1] * factor)],\n        method=method,\n    )\n\n    # pad the resized spectrogram to maintain the original dimensions\n    shifted_spectrogram = tf.pad(\n        resized_spectrogram,\n        [[0, 0], [0, spectrogram.shape[1] - resized_spectrogram.shape[1]]],\n        mode=\"constant\",\n    )\n\n    return shifted_spectrogram", "CodeBleu_score": 30.73735021603545}, {"code": "def pitch_shift(\n    spectrogram: tf.Tensor,\n    semitone_shift: float = 0.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t# Calculate the resizing factor\n    resize_factor = 2.0**(semitone_shift / 12.0)\n\n    # Resize the spectrogram along the frequency axis\n    spectrogram_shifted = tf.image.resize(\n        spectrogram,\n        size=[spectrogram.shape[0], int(spectrogram.shape[1] * resize_factor)],\n        method=method,\n    )\n\n    # Pad the resized spectrogram to maintain the original dimensions\n    spectrogram_shifted = tf.pad(\n        spectrogram_shifted,\n        [[0, 0], [0, spectrogram.shape[1] - spectrogram_shifted.shape[1]]],\n    )\n\n    return spectrogram_shifted\n\n", "CodeBleu_score": 27.8289533237275}]}
{"instance_id": "612f0826e56a", "generation_result": [{"code": "def _get_conv_activation_layer(params: Dict) -> Any\n\tif params['conv_activation'] == 'relu':\n        return nn.ReLU()\n    elif params['conv_activation'] == 'elu':\n        return nn.ELU()\n    else:\n        return nn.LeakyReLU(negative_slope=0.2)\n\n", "CodeBleu_score": 32.974245437362946}, {"code": "def _get_conv_activation_layer(params: Dict) -> Any\n\tif params[\"conv_activation\"] == \"ReLU\":\n        return nn.ReLU()\n    elif params[\"conv_activation\"] == \"ELU\":\n        return nn.ELU()\n    elif params[\"conv_activation\"] == \"LeakyReLU\":\n        return nn.LeakyReLU(0.2)\n    else:\n        raise ValueError(f\"Invalid conv_activation parameter: {params['conv_activation']}\")\n\n", "CodeBleu_score": 33.90023500915043}, {"code": "def _get_conv_activation_layer(params: Dict) -> Any\n\tif \"conv_activation\" in params:\n        if params[\"conv_activation\"] == \"ReLU\":\n            return nn.ReLU()\n        elif params[\"conv_activation\"] == \"ELU\":\n            return nn.ELU()\n        elif params[\"conv_activation\"] == \"LeakyReLU\":\n            return nn.LeakyReLU(negative_slope=0.2)\n        else:\n            raise ValueError(f\"Unknown conv_activation parameter: {params['conv_activation']}\")\n    else:\n        return nn.LeakyReLU(negative_slope=0.2)\n\n", "CodeBleu_score": 34.495267016861895}]}
{"instance_id": "2f8b5a2fc0f2", "generation_result": [{"code": "def softmax_unet(\n    input_tensor: tf.Tensor, instruments: Iterable[str], params: Dict = {}\n) -> Dict\n\tlogging.info(\"Apply softmax unet\")\n    output_dict = {}\n    for instrument in instruments:\n        output_dict[instrument] = apply_unet(\n            input_tensor,\n            output_name=instrument,\n            params=params,\n            output_mask_logit=False,\n        )\n    return output_dict\n\n", "CodeBleu_score": 23.2715246925677}, {"code": "def softmax_unet(\n    input_tensor: tf.Tensor, instruments: Iterable[str], params: Dict = {}\n) -> Dict\n\tlogging.info(\"Apply softmax unet\")\n    output_dict = {}\n    for instrument in instruments:\n        output_dict[instrument] = apply_unet(\n            input_tensor,\n            output_name=f\"output_{instrument}\",\n            params=params,\n            output_mask_logit=False,\n        )\n    # Combine the logit masks using a softmax function along a specified axis.\n    softmax_axis = params.get(\"softmax_axis\", 1)\n    softmax_output = Softmax(axis=softmax_axis)(\n        Concatenate(axis=softmax_axis)(list(output_dict.values()))\n    )\n    # Create an output dictionary where each entry is a spectrogram for an instrument, obtained by multiplying the corresponding mask with the input tensor.\n    for instrument in instruments:\n        output_dict[instrument] = Multiply()([softmax_output, output_dict[instrument]])\n    return output_dict", "CodeBleu_score": 40.29925801058671}, {"code": "def softmax_unet(\n    input_tensor: tf.Tensor, instruments: Iterable[str], params: Dict = {}\n) -> Dict\n\tlogging.info(\"Apply softmax U-net\")\n    output_dict = {}\n    for instrument in instruments:\n        output_dict[instrument] = apply_unet(\n            input_tensor, output_name=instrument, params=params, output_mask_logit=True\n        )\n    output_tensor = tf.stack(list(output_dict.values()), axis=-1)\n    output_tensor = tf.nn.softmax(output_tensor, axis=-1)\n    output_dict = {}\n    for i, instrument in enumerate(instruments):\n        output_dict[instrument] = Multiply(name=instrument)([output_tensor[..., i], input_tensor])\n    return output_dict", "CodeBleu_score": 48.069265066255504}]}
{"instance_id": "400b19ce8520", "generation_result": [{"code": "def compute_file_checksum(path)\n\timport hashlib\n    sha256 = hashlib.sha256()\n    with open(path, 'rb') as f:\n        while True:\n            data = f.read(65536)\n            if not data:\n                break\n            sha256.update(data)\n    return sha256.hexdigest()\n", "CodeBleu_score": 37.17333346538682}, {"code": "def compute_file_checksum(path)\n\timport hashlib\n    sha256_hash = hashlib.sha256()\n    with open(path, 'rb') as f:\n        # Read and update hash string value in blocks of 4K\n        for byte_block in iter(lambda: f.read(4096), b\"\"):\n            sha256_hash.update(byte_block)\n        return sha256_hash.hexdigest()\n\n", "CodeBleu_score": 48.413232016136085}, {"code": "def compute_file_checksum(path)\n\timport hashlib\n    sha256_hash = hashlib.sha256()\n    with open(path, \"rb\") as f:\n        # Read and update hash string value in blocks of 4K\n        for byte_block in iter(lambda: f.read(4096), b\"\"):\n            sha256_hash.update(byte_block)\n        return sha256_hash.hexdigest()\n\n\n", "CodeBleu_score": 52.66957618272677}]}
{"instance_id": "5f2e9c6e15f1", "generation_result": [{"code": "def checksum(self, name: str) -> str\n\turl = f\"{self.host}/{self.repository}/{self.release}/{self.checksum_index}\"\n    response = requests.get(url)\n    if response.status_code != 200:\n        raise ValueError(f\"Request to {url} failed with status code {response.status_code}\")\n    index = response.json()\n    if name not in index:\n        raise ValueError(f\"Model {name} not found in {url}\")\n    return index[name]\n\n", "CodeBleu_score": 38.782019970523315}, {"code": "def checksum(self, name: str) -> str\n\turl = f\"{self.host}/{self.repository}/{self.release}/{self.checksum_index}\"\n    response = requests.get(url)\n    if response.status_code != 200:\n        raise ValueError(f\"Request to {url} failed with status code {response.status_code}\")\n    checksum_index = response.json()\n    if name not in checksum_index:\n        raise ValueError(f\"Model {name} not found in checksum index.\")\n    return checksum_index[name]\n\n", "CodeBleu_score": 37.02270491669112}, {"code": "def checksum(self, name: str) -> str\n\turl = self.host + self.repository + self.release_path + self.checksum_index\n    response = requests.get(url)\n    if response.status_code == 200:\n        checksum_index = response.json()\n        if name in checksum_index:\n            return checksum_index[name]\n        else:\n            raise ValueError(\"Model name not found in checksum index.\")\n    else:\n        raise ValueError(\"Checksum index not found.\")\n\n", "CodeBleu_score": 36.18909501945885}]}
{"instance_id": "9312c8b6032e", "generation_result": [{"code": "def download(self, name: str, path: str) -> None:\n\turl: str = \"/\".join(\n        (\n            self._host,\n            self._repository,\n            self.RELEASE_PATH,\n            self._release,\n            self.ARCHIVE_PATH,\n            name,\n        )\n    )\n    response: httpx.Response = httpx.get(url, stream=True)\n    response.raise_for_status()\n    with tempfile.NamedTemporaryFile(delete=False) as file:\n        with tqdm(\n            total=int(response.headers[\"Content-Length\"]),\n            unit=\"B\",\n            unit_scale=True,\n            unit_divisor=1024,\n        ) as progress:\n            for chunk in response.iter_bytes():\n                file.write(chunk)\n                progress.update(len(chunk))\n    checksum: str = compute_file_checksum(file.name)\n    if checksum != self.checksum(name):\n        raise RuntimeError(\n            f\"Checksum {checksum} does not match expected value {self.checksum(name)}\"\n        )\n    with tarfile.open(file.name, \"r:gz\") as archive:\n        archive.extractall(path)\n    os.remove(file.name)\n    logger.info(f\"Downloaded and extracted {name}\")\n\n", "CodeBleu_score": 49.49303419342025}, {"code": "def download(self, name: str, path: str) -> None:\n\turl: str = \"/\".join(\n        (\n            self._host,\n            self._repository,\n            self.RELEASE_PATH,\n            self._release,\n            self.ARCHIVE_PATH,\n            name,\n        )\n    )\n    response: httpx.Response = httpx.get(url, follow_redirects=True)\n    response.raise_for_status()\n    with tempfile.NamedTemporaryFile(suffix=\".zip\") as temporary_file:\n        with open(temporary_file.name, \"wb\") as stream:\n            for chunk in response.iter_bytes():\n                stream.write(chunk)\n        checksum: str = compute_file_checksum(temporary_file.name)\n        expected_checksum: str = self.checksum(name)\n        if checksum != expected_checksum:\n            raise ValueError(\n                f\"Checksum {checksum} does not match expected checksum {expected_checksum}\"\n            )\n        with zipfile.ZipFile(temporary_file.name) as archive:\n            archive.extractall(path)", "CodeBleu_score": 46.13113031499893}, {"code": "def download(self, name: str, path: str) -> None:\n\turl: str = \"/\".join(\n        (\n            self._host,\n            self._repository,\n            self.RELEASE_PATH,\n            self._release,\n            name + \".tar.gz\",\n        )\n    )\n    self._log.info(\"Downloading %s from %s\", name, url)\n    response: httpx.Response = httpx.get(url, stream=True)\n    response.raise_for_status()\n    with open(path, \"wb\") as stream:\n        for chunk in response.iter_bytes():\n            stream.write(chunk)\n    self._log.info(\"Downloaded %s\", name)\n    checksum: str = compute_file_checksum(path)\n    if checksum != self.checksum(name):\n        raise ValueError(f\"Checksum mismatch for model {name}\")\n    self._log.info(\"Checksum for %s is valid\", name)\n    with tarfile.open(path, \"r:gz\") as stream:\n        stream.extractall(path=path[:-7])\n    os.remove(path)\n    self._log.info(\"Extracted %s\", name)\n\n", "CodeBleu_score": 46.874955589429575}]}
{"instance_id": "ee9493f37d77", "generation_result": [{"code": "def load_configuration(descriptor: str) -> Dict\n\tpass\n\n", "CodeBleu_score": 3.115942028985507}, {"code": "def load_configuration(descriptor: str) -> Dict\n\tpass\n\n\n\n\n", "CodeBleu_score": 3.115942028985507}, {"code": "def load_configuration(descriptor: str) -> Dict\n\tpass\n\n", "CodeBleu_score": 3.115942028985507}]}
{"instance_id": "546134b67fbc", "generation_result": [{"code": "def sync_apply(\n    tensor_dict: Dict[str, tf.Tensor], func: Callable, concat_axis: int = 1\n) -> Dict[str, tf.Tensor]\n\tif concat_axis not in [0, 1]:\n        raise ValueError(f\"The concat_axis must be either 0 or 1, not {concat_axis}\")\n\n    concat_tensor = tf.concat(list(tensor_dict.values()), axis=concat_axis)\n    processed_tensor = func(concat_tensor)\n\n    if concat_axis == 0:\n        return {k: v for k, v in zip(tensor_dict.keys(), tf.split(processed_tensor, len(tensor_dict)))}\n    elif concat_axis == 1:\n        return {k: v for k, v in zip(tensor_dict.keys(), tf.split(processed_tensor, tf.shape(concat_tensor)[0], axis=0))}\n\n", "CodeBleu_score": 42.30777624748014}, {"code": "def sync_apply(\n    tensor_dict: Dict[str, tf.Tensor], func: Callable, concat_axis: int = 1\n) -> Dict[str, tf.Tensor]\n\tif concat_axis not in [0, 1]:\n        raise ValueError(\n            f\"The concat_axis must be either 0 or 1, but got {concat_axis}.\"\n        )\n    concat_tensor = tf.concat(list(tensor_dict.values()), axis=concat_axis)\n    processed_tensor = func(concat_tensor)\n    if concat_axis == 0:\n        split_tensor = tf.split(processed_tensor, len(tensor_dict), axis=0)\n    else:\n        split_tensor = tf.split(processed_tensor, len(tensor_dict), axis=1)\n    processed_tensor_dict = dict(zip(tensor_dict.keys(), split_tensor))\n    return processed_tensor_dict\n\n", "CodeBleu_score": 41.787609343612495}, {"code": "def sync_apply(\n    tensor_dict: Dict[str, tf.Tensor], func: Callable, concat_axis: int = 1\n) -> Dict[str, tf.Tensor]\n\tif concat_axis not in [0, 1]:\n        raise ValueError(\n            f\"concat_axis must be either 0 or 1, but got {concat_axis} instead.\"\n        )\n    tensor_dict_concat = tf.concat(list(tensor_dict.values()), axis=concat_axis)\n    tensor_dict_processed_concat = func(tensor_dict_concat)\n    if concat_axis == 0:\n        tensor_dict_processed = {\n            key: value\n            for key, value in zip(\n                tensor_dict.keys(), tf.split(tensor_dict_processed_concat, len(tensor_dict))\n            )\n        }\n    elif concat_axis == 1:\n        tensor_dict_processed = {\n            key: value\n            for key, value in zip(\n                tensor_dict.keys(),\n                tf.split(tensor_dict_processed_concat, [value.shape[1] for value in tensor_dict.values()], axis=1),\n            )\n        }\n    return tensor_dict_processed\n\n", "CodeBleu_score": 42.855681428430096}]}
{"instance_id": "df48e78d46d2", "generation_result": [{"code": "def from_float32_to_uint8(\n    tensor: tf.Tensor,\n    tensor_key: str = \"tensor\",\n    min_key: str = \"min\",\n    max_key: str = \"max\",\n) -> tf.Tensor\n\tmin_value = tf.reduce_min(tensor)\n    max_value = tf.reduce_max(tensor)\n    scaled_tensor = tf.cast(\n        tf.math.round(((tensor - min_value) / (max_value - min_value)) * 255),\n        tf.uint8,\n    )\n    return {\n        tensor_key: scaled_tensor,\n        min_key: min_value,\n        max_key: max_value,\n    }\n\n", "CodeBleu_score": 44.420448424118476}, {"code": "def from_float32_to_uint8(\n    tensor: tf.Tensor,\n    tensor_key: str = \"tensor\",\n    min_key: str = \"min\",\n    max_key: str = \"max\",\n) -> tf.Tensor\n\ttensor_min = tf.reduce_min(tensor)\n    tensor_max = tf.reduce_max(tensor)\n    tensor_scaled = (tensor - tensor_min) * 255.0 / (tensor_max - tensor_min)\n    tensor_scaled = tf.cast(tensor_scaled, tf.uint8)\n    return {\n        tensor_key: tensor_scaled,\n        min_key: tensor_min,\n        max_key: tensor_max,\n    }\n\n", "CodeBleu_score": 62.32245851129106}, {"code": "def from_float32_to_uint8(\n    tensor: tf.Tensor,\n    tensor_key: str = \"tensor\",\n    min_key: str = \"min\",\n    max_key: str = \"max\",\n) -> tf.Tensor\n\ttensor_min = tf.reduce_min(tensor)\n    tensor_max = tf.reduce_max(tensor)\n    tensor = (tensor - tensor_min) / (tensor_max - tensor_min)\n    tensor = tf.cast(tensor * 255, tf.uint8)\n    return {\n        tensor_key: tensor,\n        min_key: tensor_min,\n        max_key: tensor_max,\n    }\n\n", "CodeBleu_score": 65.58620930861916}]}
{"instance_id": "7f479e5de492", "generation_result": [{"code": "def pad_and_partition(tensor: tf.Tensor, segment_len: int) -> tf.Tensor\n\tif segment_len < 1:\n        raise ValueError(\"segment_len must be a positive integer\")\n    padding = tf.math.mod(tf.shape(tensor)[0], segment_len)\n    padded = tf.pad(tensor, [[0, padding]])\n    return tf.reshape(padded, [-1, segment_len] + tensor.shape[1:])\n\n", "CodeBleu_score": 36.8691475104421}, {"code": "def pad_and_partition(tensor: tf.Tensor, segment_len: int) -> tf.Tensor\n\tpad_len = segment_len - tf.shape(tensor)[0] % segment_len\n    padded = tf.pad(tensor, [[0, pad_len], [0, 0]])\n    return tf.reshape(padded, [-1, segment_len] + padded.shape[1:])\n\n", "CodeBleu_score": 21.96638300367871}, {"code": "def pad_and_partition(tensor: tf.Tensor, segment_len: int) -> tf.Tensor\n\tpad_len = tf.cast(tf.math.ceil(tf.cast(tf.shape(tensor)[0], tf.float32) / segment_len) * segment_len, tf.int32) - tf.shape(tensor)[0]\n    padded_tensor = tf.pad(tensor, [[0, pad_len], [0, 0]])\n    return tf.reshape(padded_tensor, [-1, segment_len, tf.shape(tensor)[1]])\n\n", "CodeBleu_score": 28.8700450658919}]}
{"instance_id": "1789fae7253f", "generation_result": [{"code": "def pad_and_reshape(instr_spec, frame_length, F) -> Any\n\t# calculate the number of extra rows needed for padding\n    extra_rows = frame_length - instr_spec.shape[1] % frame_length\n    # create an extension of zeros\n    extension = np.zeros((instr_spec.shape[0], extra_rows, instr_spec.shape[2]))\n    # concatenate the extension to the original spectrogram along the frequency axis\n    instr_spec = np.concatenate((instr_spec, extension), axis=1)\n    # reshape the extended spectrogram by merging the first two dimensions\n    instr_spec = instr_spec.reshape((-1, frame_length, F))\n    return instr_spec\n\n\n", "CodeBleu_score": 15.801823535489998}, {"code": "def pad_and_reshape(instr_spec, frame_length, F) -> Any\n\t# Calculate the number of extra rows needed for padding\n    num_rows_needed = frame_length - instr_spec.shape[1] % frame_length\n    # Create an extension of zeros\n    extension = np.zeros((instr_spec.shape[0], num_rows_needed, instr_spec.shape[2]))\n    # Concatenate the extension to the original spectrogram along the frequency axis\n    instr_spec_extended = np.concatenate((instr_spec, extension), axis=1)\n    # Reshape the extended spectrogram by merging the first two dimensions\n    instr_spec_reshaped = instr_spec_extended.reshape(instr_spec_extended.shape[0] * instr_spec_extended.shape[1], instr_spec_extended.shape[2])\n    return instr_spec_reshaped\n\n", "CodeBleu_score": 19.52582644458713}, {"code": "def pad_and_reshape(instr_spec, frame_length, F) -> Any\n\t# The number of extra rows needed for padding\n    num_rows_to_pad = int(frame_length) - int(instr_spec.shape[1])\n    # Create an extension of zeros\n    extension = tf.zeros([instr_spec.shape[0], num_rows_to_pad, instr_spec.shape[2]])\n    # Concatenate the extension to the original spectrogram along the frequency axis\n    instr_spec = tf.concat([instr_spec, extension], axis=1)\n    # Reshape the extended spectrogram by merging the first two dimensions\n    instr_spec = tf.reshape(instr_spec, [instr_spec.shape[0] * instr_spec.shape[1], instr_spec.shape[2]])\n    return instr_spec\n\n", "CodeBleu_score": 20.03128138750393}]}
{"instance_id": "8a4f4658d97c", "generation_result": [{"code": "def dataset_from_csv(csv_path: str, **kwargs) -> Any\n\timport pandas as pd\n    import tensorflow as tf\n    from tensorflow.python.data.ops import dataset_ops\n    from tensorflow.python.framework import ops\n    from tensorflow.python.framework import tensor_shape\n    from tensorflow.python.framework import tensor_util\n    from tensorflow.python.ops import gen_dataset_ops\n    from tensorflow.python.util import compat\n    from tensorflow.python.util import nest\n\n    def _is_valid_dataset_or_tensors(tensors):\n        if isinstance(tensors, dataset_ops.DatasetV2):\n            return True\n        if not isinstance(tensors, (tuple, list)):\n            return False\n        if not tensors:\n            return True\n        return all(\n            isinstance(t, ops.Tensor) or isinstance(t, dataset_ops.DatasetV2)\n            for t in tensors)\n\n    def _make_tensor_slices_from_lists(lists):\n        \"\"\"Make tensor slices from lists of numpy arrays.\"\"\"\n        max_num_elements = max(len(l) for l in lists)\n        if max_num_elements == 0:\n            raise ValueError(\"Cannot create a tensor slice from an empty list.\")\n        # pylint: disable=g-long-lambda\n        # pylint: disable=protected-access\n        slices = [\n            ops.convert_to_tensor(l, name=\"component_%d\" % i)._to_slice(\n                i, max_num_elements)\n            for i, l in enumerate(lists)\n        ]\n        # pylint: enable=g-long-lambda\n        # pylint: enable=protected-access\n        return slices\n\n    def _make_tensor_slices_from_tensors(tensors):\n        \"\"\"Make tensor slices from tensors.\"\"\"\n        slices = []\n        for i, t in enumerate(nest.flatten(tensors)):\n            if not isinstance(t, ops.Tensor):\n                raise TypeError(\n                    \"Expected a component to be a Tensor, got %s instead.\" % t)\n            slices.append(", "CodeBleu_score": 30.139376542559855}, {"code": "def dataset_from_csv(csv_path: str, **kwargs) -> Any\n\timport pandas as pd\n    import tensorflow as tf\n    df = pd.read_csv(csv_path, **kwargs)\n    dataset = tf.data.Dataset.from_tensor_slices(dict(df))\n    return dataset\n\n", "CodeBleu_score": 36.77082911962843}, {"code": "def dataset_from_csv(csv_path: str, **kwargs) -> Any\n\timport pandas as pd\n    import tensorflow as tf\n\n    df = pd.read_csv(csv_path, **kwargs)\n    dataset = tf.data.Dataset.from_tensor_slices(dict(df))\n    return dataset\n\n", "CodeBleu_score": 36.77082911962843}]}
{"instance_id": "9a16e81b182f", "generation_result": [{"code": "def check_tensor_shape(tensor_tf: tf.Tensor, target_shape: Any) -> bool\n\tresult = True\n    for i, target_length in enumerate(target_shape):\n        if target_length is not None:\n            result = result and (tensor_tf.shape[i] == target_length)\n    return result\n\n", "CodeBleu_score": 40.760602303141056}, {"code": "def check_tensor_shape(tensor_tf: tf.Tensor, target_shape: Any) -> bool\n\tresult = True\n    for i in range(len(target_shape)):\n        if target_shape[i] is not None:\n            result = result and (tensor_tf.shape[i] == target_shape[i])\n    return result\n\n", "CodeBleu_score": 30.66561259123293}, {"code": "def check_tensor_shape(tensor_tf: tf.Tensor, target_shape: Any) -> bool\n\tresult = True\n    for i, target_dim in enumerate(target_shape):\n        if target_dim is not None:\n            result = result and (tensor_tf.shape[i] == target_dim)\n    return result\n\n", "CodeBleu_score": 32.60206359062759}]}
{"instance_id": "49d5d3084b3a", "generation_result": [{"code": "def get_training_dataset(\n    audio_params: Dict, audio_adapter: AudioAdapter, audio_path: str\n) -> Any\n\t# Initialize dataset builder.\n    builder = DatasetBuilder(audio_params, audio_adapter)\n    # Build dataset.\n    dataset = builder.build(\n        csv_path=train_csv,\n        batch_size=batch_size,\n        shuffle=True,\n        convert_to_uint=True,\n        random_data_augmentation=True,\n        random_time_crop=True,\n        infinite_generator=True,\n        cache_directory=cache_directory,\n        wait_for_cache=False,\n        num_parallel_calls=4,\n        n_chunks_per_song=n_chunks_per_song,\n    )\n    return dataset\n\n", "CodeBleu_score": 29.13907756051581}, {"code": "def get_training_dataset(\n    audio_params: Dict, audio_adapter: AudioAdapter, audio_path: str\n) -> Any\n\t# Initialize dataset builder.\n    builder = DatasetBuilder(audio_params, audio_adapter, audio_path)\n    # Build dataset.\n    return builder.build(\n        csv_path=TRAIN_CSV_PATH,\n        batch_size=BATCH_SIZE,\n        shuffle=True,\n        convert_to_uint=True,\n        random_data_augmentation=True,\n        random_time_crop=True,\n        infinite_generator=True,\n        cache_directory=CACHE_DIRECTORY,\n        wait_for_cache=False,\n        num_parallel_calls=4,\n        n_chunks_per_song=2,\n    )\n\n", "CodeBleu_score": 27.636270407942444}, {"code": "def get_training_dataset(\n    audio_params: Dict, audio_adapter: AudioAdapter, audio_path: str\n) -> Any\n\tbuilder = DatasetBuilder(audio_params, audio_adapter, audio_path)\n    dataset = builder.build(\n        csv_path=train_csv,\n        batch_size=batch_size,\n        shuffle=True,\n        convert_to_uint=True,\n        random_data_augmentation=True,\n        random_time_crop=True,\n        infinite_generator=True,\n        cache_directory=cache_directory,\n        wait_for_cache=False,\n        num_parallel_calls=4,\n        n_chunks_per_song=n_chunks_per_song,\n    )\n    return dataset\n\n", "CodeBleu_score": 30.360040232074603}]}
{"instance_id": "fe8bbbba06e3", "generation_result": [{"code": "def instruments(self) -> Any\n\tif not self._instrument_builders:\n        for instrument in self._instruments:\n            self._instrument_builders[instrument.name] = InstrumentDatasetBuilder(\n                instrument, self._dataset_id, self._dataset_path\n            )\n    for instrument in self._instrument_builders.values():\n        yield instrument\n\n", "CodeBleu_score": 42.71493161819494}, {"code": "def instruments(self) -> Any\n\tif not hasattr(self, '_instruments'):\n        self._instruments = {}\n        for instrument in self.instruments:\n            self._instruments[instrument] = self.dataset_builder(instrument)\n    return self._instruments.values()\n\n", "CodeBleu_score": 24.87441334233093}, {"code": "def instruments(self) -> Any\n\tif not self._instruments:\n        self._instruments = []\n        for instrument in self._instruments_list:\n            self._instruments.append(InstrumentDatasetBuilder(instrument))\n    for instrument in self._instruments:\n        yield instrument\n\n", "CodeBleu_score": 56.079555796040104}]}
{"instance_id": "438b57fd3413", "generation_result": [{"code": "def cache(self, dataset: Any, cache: Optional[str], wait: bool) -> Any\n\treturn CacheDataset.cache(self, dataset, cache, wait)\n\n", "CodeBleu_score": 16.005166105342127}, {"code": "def cache(self, dataset: Any, cache: Optional[str], wait: bool) -> Any\n\tif cache is not None:\n        if wait:\n            while not exists(f\"{cache}.index\"):\n                logger.info(f\"Cache not available, wait {self.WAIT_PERIOD}\")\n                time.sleep(self.WAIT_PERIOD)\n        cache_path = os.path.split(cache)[0]\n        os.makedirs(cache_path, exist_ok=True)\n        return dataset.cache(cache)\n    return dataset", "CodeBleu_score": 93.3254942392973}, {"code": "def cache(self, dataset: Any, cache: Optional[str], wait: bool) -> Any\n\treturn Cache(self).cache(dataset, cache, wait)\n\n", "CodeBleu_score": 15.43977096366399}]}
{"instance_id": "8838a3a09a66", "generation_result": [{"code": "def create_estimator(params: Dict, MWF: bool) -> tf.Tensor\n\tmodel_provider = ModelProvider(params[\"model_dir\"])\n    model_provider.update_parameters(params)\n\n    session_config = tf.ConfigProto()\n    session_config.gpu_options.allow_growth = True\n    run_config = tf.estimator.RunConfig(\n        model_dir=model_provider.model_dir,\n        session_config=session_config,\n        save_summary_steps=params[\"save_summary_steps\"],\n        save_checkpoints_steps=params[\"save_checkpoints_steps\"],\n        keep_checkpoint_max=params[\"keep_checkpoint_max\"],\n        log_step_count_steps=params[\"log_step_count_steps\"],\n    )\n\n    if MWF:\n        model_function = model_provider.get_model_function()\n    else:\n        model_function = model_provider.get_model_function(\n            model_dir=model_provider.model_dir\n        )\n\n    estimator = tf.estimator.Estimator(\n        model_fn=model_function,\n        config=run_config,\n        params=params,\n    )\n\n    return estimator\n\n", "CodeBleu_score": 36.04548155478039}, {"code": "def create_estimator(params: Dict, MWF: bool) -> tf.Tensor\n\tmodel_provider = ModelProvider(params, MWF)\n    model_dir = model_provider.get_model_dir()\n    params = model_provider.get_params()\n    session_config = tf.ConfigProto()\n    session_config.gpu_options.allow_growth = True\n    run_config = tf.estimator.RunConfig().replace(\n        session_config=session_config,\n        model_dir=model_dir,\n        save_summary_steps=params['save_summary_steps'],\n        save_checkpoints_steps=params['save_checkpoints_steps'],\n        keep_checkpoint_max=params['keep_checkpoint_max'],\n    )\n    return tf.estimator.Estimator(\n        model_fn=model_provider.get_model_fn(),\n        params=params,\n        config=run_config,\n    )\n\n", "CodeBleu_score": 30.15315430115015}, {"code": "def create_estimator(params: Dict, MWF: bool) -> tf.Tensor\n\tmodel_provider = ModelProvider(params[\"model_dir\"])\n    model_dir = model_provider.get_model_dir()\n    params = model_provider.update_parameters(params)\n\n    session_config = tf.ConfigProto()\n    session_config.gpu_options.allow_growth = True\n\n    run_config = tf.estimator.RunConfig(\n        model_dir=model_dir,\n        save_summary_steps=params[\"save_summary_steps\"],\n        save_checkpoints_steps=params[\"save_checkpoints_steps\"],\n        log_step_count_steps=params[\"log_step_count_steps\"],\n        session_config=session_config)\n\n    estimator = tf.estimator.Estimator(\n        model_fn=model_fn,\n        model_dir=model_dir,\n        params=params,\n        config=run_config)\n\n    return estimator\n\n", "CodeBleu_score": 34.83245797428815}]}
{"instance_id": "779c812bf2cb", "generation_result": [{"code": "def _separate_tensorflow(\n        self, waveform: np.ndarray, audio_descriptor: AudioDescriptor\n    ) -> Dict\n\tif waveform.ndim == 1:\n        waveform = np.repeat(waveform[np.newaxis], 2, axis=0)\n    self._data_generator.update_data(\n        {\"waveform\": waveform, \"audio_id\": audio_descriptor.identifier}\n    )\n    prediction = next(self._get_prediction_generator())\n    prediction.pop(\"audio_id\")\n    return prediction\n\n", "CodeBleu_score": 54.884933518376535}, {"code": "def _separate_tensorflow(\n        self, waveform: np.ndarray, audio_descriptor: AudioDescriptor\n    ) -> Dict\n\tif waveform.shape[1] == 1:\n        waveform = np.repeat(waveform, 2, axis=1)\n\n    prediction_generator = self._get_prediction_generator()\n    self.update_data(\n        {\"waveform\": waveform, \"audio_id\": audio_descriptor.identifier}\n    )\n    prediction = next(prediction_generator)\n    del prediction[\"audio_id\"]\n    return prediction", "CodeBleu_score": 60.17003943464464}, {"code": "def _separate_tensorflow(\n        self, waveform: np.ndarray, audio_descriptor: AudioDescriptor\n    ) -> Dict\n\tif waveform.ndim == 1:\n        waveform = np.expand_dims(waveform, axis=0)\n\n    if waveform.ndim == 2:\n        waveform = np.expand_dims(waveform, axis=0)\n\n    prediction_generator = self._get_prediction_generator()\n    self.update_data(\n        {\n            \"waveform\": waveform,\n            \"audio_id\": audio_descriptor.unique_id,\n        }\n    )\n    prediction = next(prediction_generator)\n    prediction.pop(\"audio_id\")\n    return prediction\n\n", "CodeBleu_score": 66.28044169509594}]}
{"instance_id": "cf580cd2ed93", "generation_result": [{"code": "def compute_backbone_shapes(config, image_shape)\n\tif callable(config.BACKBONE):\n        return config.COMPUTE_BACKBONE_SHAPE(image_shape)\n    # Currently supports ResNet only.\n    assert config.BACKBONE in [\"resnet50\", \"resnet101\"]\n    return np.array(\n        [[int(math.ceil(image_shape[0] / stride)),\n          int(math.ceil(image_shape[1] / stride))]\n         for stride in config.BACKBONE_STRIDES])\n\n\n", "CodeBleu_score": 94.44046099762427}, {"code": "def compute_backbone_shapes(config, image_shape)\n\tif callable(config.BACKBONE):\n        return config.COMPUTE_BACKBONE_SHAPE(image_shape)\n    # Currently supports ResNet only.\n    assert config.BACKBONE in [\"resnet50\", \"resnet101\"]\n    return np.array(\n        [[int(math.ceil(image_shape[0] / stride)),\n          int(math.ceil(image_shape[1] / stride))]\n         for stride in config.BACKBONE_STRIDES])\n", "CodeBleu_score": 94.44046099762427}, {"code": "def compute_backbone_shapes(config, image_shape)\n\tif callable(config.BACKBONE):\n        return config.COMPUTE_BACKBONE_SHAPE(image_shape)\n    # Currently supports ResNet only.\n    assert config.BACKBONE in [\"resnet50\", \"resnet101\"]\n    return np.array(\n        [[int(math.ceil(image_shape[0] / stride)),\n          int(math.ceil(image_shape[1] / stride))]\n         for stride in config.BACKBONE_STRIDES])\n\n\n", "CodeBleu_score": 94.44046099762427}]}
{"instance_id": "ebd61a2e59ca", "generation_result": [{"code": "def clip_boxes_graph(boxes, window)\n\t# Split the window and box coordinates into individual components\n    wy1, wx1, wy2, wx2 = tf.split(window, 4)\n    y1, x1, y2, x2 = tf.split(boxes, 4, axis=1)\n\n    # Clip the coordinates\n    y1 = tf.maximum(tf.minimum(y1, wy2), wy1)\n    x1 = tf.maximum(tf.minimum(x1, wx2), wx1)\n    y2 = tf.maximum(tf.minimum(y2, wy2), wy1)\n    x2 = tf.maximum(tf.minimum(x2, wx2), wx1)\n\n    # Concatenate the clipped coordinates back into a tensor of boxes\n    clipped_boxes = tf.concat([y1, x1, y2, x2], axis=1, name=\"clipped_boxes\")\n    # Set the shape of the clipped boxes to be that of the input boxes\n    clipped_boxes.set_shape(boxes.get_shape())\n\n    return clipped_boxes\n\n", "CodeBleu_score": 54.42308288006128}, {"code": "def clip_boxes_graph(boxes, window)\n\t# split up the window and boxes coordinates\n    wy1, wx1, wy2, wx2 = tf.split(window, 4)\n    y1, x1, y2, x2 = tf.split(boxes, 4, axis=1)\n\n    # clip the y's\n    y1 = tf.maximum(tf.minimum(y1, wy2), wy1)\n    y2 = tf.maximum(tf.minimum(y2, wy2), wy1)\n\n    # clip the x's\n    x1 = tf.maximum(tf.minimum(x1, wx2), wx1)\n    x2 = tf.maximum(tf.minimum(x2, wx2), wx1)\n\n    # return a fixed shape\n    return tf.concat([y1, x1, y2, x2], axis=1, name=\"clipped_boxes\")\n\n", "CodeBleu_score": 43.98430425116976}, {"code": "def clip_boxes_graph(boxes, window)\n\t# Split window and boxes coordinates into individual components\n    wy1, wx1, wy2, wx2 = tf.split(window, 4)\n    y1, x1, y2, x2 = tf.split(boxes, 4, axis=1)\n\n    # Clip coordinates\n    y1 = tf.maximum(tf.minimum(y1, wy2), wy1)\n    x1 = tf.maximum(tf.minimum(x1, wx2), wx1)\n    y2 = tf.maximum(tf.minimum(y2, wy2), wy1)\n    x2 = tf.maximum(tf.minimum(x2, wx2), wx1)\n\n    # Concatenate coordinates back into a tensor\n    clipped_boxes = tf.concat([y1, x1, y2, x2], axis=1)\n\n    # Return clipped boxes with fixed shape\n    clipped_boxes = tf.reshape(clipped_boxes, boxes.shape)\n    return clipped_boxes\n\n", "CodeBleu_score": 47.58854776109748}]}
{"instance_id": "8e0411587a70", "generation_result": [{"code": "def rpn_graph(feature_map, anchors_per_location, anchor_stride)\n\t# Apply a shared convolutional layer to the input feature map\n    shared_layer = mx.symbol.Convolution(data=feature_map, kernel=(3, 3), pad=(1, 1),\n                                         num_filter=512, name=\"rpn_conv_3x3\")\n    shared_layer = mx.symbol.Activation(data=shared_layer, act_type=\"relu\", name=\"rpn_relu\")\n\n    # Apply two convolutional layers to the shared layer, one for the class scores and another for the bounding box predictions\n    num_anchors = len(anchors_per_location)\n\n    cls_score = mx.symbol.Convolution(data=shared_layer, kernel=(1, 1), pad=(0, 0),\n                                      num_filter=2 * num_anchors, name=\"rpn_cls_score\")\n    cls_score_reshape = mx.symbol.Reshape(data=cls_score, shape=(0, 2, -1, 0), name=\"rpn_cls_score_reshape\")\n    cls_prob = mx.symbol.SoftmaxActivation(data=cls_score_reshape, mode=\"channel\", name=\"rpn_cls_prob\")\n    cls_prob_reshape = mx.symbol.Reshape(data=cls_prob, shape=(0, 2 * num_anchors, -1, 0), name='rpn_cls_prob_reshape')\n\n    bbox_pred = mx.symbol.Convolution(data=shared_layer, kernel=(1, 1), pad=(0, 0),\n                                      num_filter=4 * num_anchors, name=\"rpn_bbox_pred\")\n    bbox_pred_reshape = mx.symbol.Reshape(data=bbox_pred, shape=(0, 0, -1, 0), name='rpn_bbox_pred_reshape')\n\n    # Proposal layer\n    rois = mx.symbol.contrib.Proposal(cls_prob=cls_prob_reshape, bbox_pred=bbox_pred_", "CodeBleu_score": 22.622411411627404}, {"code": "def rpn_graph(feature_map, anchors_per_location, anchor_stride)\n\t# Apply a shared convolutional layer to the feature map.\n    shared_layer = Conv2D(512, (3, 3), padding='same', activation='relu', kernel_initializer='normal', name='rpn_conv_shared')(feature_map)\n\n    # Apply two convolutional layers to the shared layer.\n    x = Conv2D(2 * anchors_per_location, (1, 1), padding='valid', activation='linear', kernel_initializer='zero', name='rpn_class_raw')(shared_layer)\n    x = Reshape((-1, 2), name='rpn_class_logits')(x)\n    rpn_class_logits = Activation('softmax', name='rpn_class_logits')(x)\n\n    x = Conv2D(anchors_per_location * 4, (1, 1), padding='valid', activation='linear', kernel_initializer='zero', name='rpn_bbox_pred')(shared_layer)\n    x = Reshape((-1, 4), name='rpn_bbox')(x)\n    rpn_bbox = Activation('linear', name='rpn_bbox')(x)\n\n    return [rpn_class_logits, rpn_bbox]\n\n", "CodeBleu_score": 40.683971940701355}, {"code": "def rpn_graph(feature_map, anchors_per_location, anchor_stride)\n\t# Shared convolutional layer\n    shared = Conv2D(512, (3, 3), padding=\"same\", activation=\"relu\", kernel_initializer=\"normal\", name=\"rpn_conv_shared\")(feature_map)\n\n    # Class scores\n    x = Conv2D(2 * anchors_per_location, (1, 1), padding=\"valid\", activation=\"linear\", kernel_initializer=\"zero\", name=\"rpn_out_class\")(shared)\n    x = Reshape((-1, 2), name=\"classification\")(x)\n    x = Activation(\"softmax\", name=\"classification_softmax\")(x)\n    # Bounding box regression\n    y = Conv2D(2 * anchors_per_location, (1, 1), padding=\"valid\", activation=\"linear\", kernel_initializer=\"zero\", name=\"rpn_out_regress\")(shared)\n    y = Reshape((-1, 4), name=\"regression\")(y)\n\n    return [x, y, shared]\n\n\n", "CodeBleu_score": 33.71852431315422}]}
{"instance_id": "70777c9261d8", "generation_result": [{"code": "def smooth_l1_loss(y_true, y_pred)\n\tdiff = K.abs(y_true - y_pred)\n    less_than_one = K.cast(K.less(diff, 1.0), \"float32\")\n    loss = (less_than_one * 0.5 * diff**2) + (1 - less_than_one) * (diff - 0.5)\n    return loss\n\n", "CodeBleu_score": 60.28930735456834}, {"code": "def smooth_l1_loss(y_true, y_pred)\n\tdiff = K.abs(y_true - y_pred)\n    less_than_one = K.cast(K.less(diff, 1.0), \"float32\")\n    loss = (less_than_one * 0.5 * diff**2) + (1 - less_than_one) * (diff - 0.5)\n    return loss\n\n", "CodeBleu_score": 60.28930735456834}, {"code": "def smooth_l1_loss(y_true, y_pred)\n\tabs_diff = tf.abs(y_true - y_pred)\n    squared_diff = tf.square(y_true - y_pred)\n    mask = tf.cast(tf.less(abs_diff, 1.0), tf.float32)\n    squared_mask = tf.square(mask)\n    squared_loss = 0.5 * squared_mask * squared_diff\n    abs_loss = (1 - squared_mask) * (abs_diff - 0.5)\n    loss = squared_loss + abs_loss\n    return loss\n\n", "CodeBleu_score": 28.52619052537961}]}
{"instance_id": "0e747b604087", "generation_result": [{"code": "def build_model(x_train, num_classes)\n\t# Reset the TensorFlow graph\n    tf.reset_default_graph()\n\n    # Define the input layer\n    X_input = tf.placeholder(tf.float32, shape=[None, x_train.shape[1], x_train.shape[2], x_train.shape[3]])\n\n    # Define the first convolutional layer\n    W1 = tf.get_variable(\"W1\", [3, 3, 3, 32], initializer=tf.contrib.layers.xavier_initializer())\n    b1 = tf.Variable(tf.random_normal([32]))\n    layer1 = tf.add(tf.nn.conv2d(X_input, W1, strides=[1, 1, 1, 1], padding='SAME'), b1)\n    layer1 = tf.nn.relu(layer1)\n\n    # Define the first max-pooling layer\n    layer1 = tf.nn.max_pool(layer1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n\n    # Define the second convolutional layer\n    W2 = tf.get_variable(\"W2\", [3, 3, 32, 64], initializer=tf.contrib.layers.xavier_initializer())\n    b2 = tf.Variable(tf.random_normal([64]))\n    layer2 = tf.add(tf.nn.conv2d(layer1, W2, strides=[1, 1, 1, 1], padding='SAME'), b2)\n    layer2 = tf.nn.relu(layer2)\n\n    # Define the second max-pooling layer\n    layer2 = tf.nn.max_pool(layer2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n\n    # Flatten the output of the previous layer\n    layer2_flat = tf.reshape(layer2, [-1, 5 * 5 * 64])\n\n    # Define the first dense layer\n    W3 = tf", "CodeBleu_score": 22.03888998839104}, {"code": "def build_model(x_train, num_classes)\n\t# Reset the TensorFlow graph\n    tf.reset_default_graph()\n\n    # Define the input layer\n    X = tf.placeholder(tf.float32, shape=(None, 64, 64, 3))\n\n    # Define the first convolutional layer\n    W1 = tf.get_variable(\"W1\", shape=(3, 3, 3, 16), initializer=tf.contrib.layers.xavier_initializer())\n    b1 = tf.get_variable(\"b1\", shape=(16), initializer=tf.zeros_initializer())\n    conv1 = tf.nn.conv2d(X, W1, strides=[1, 1, 1, 1], padding=\"VALID\")\n    h1 = tf.nn.relu(conv1 + b1)\n\n    # Define the max-pooling layer\n    h1_pool = tf.nn.max_pool(h1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"VALID\")\n\n    # Define the second convolutional layer\n    W2 = tf.get_variable(\"W2\", shape=(3, 3, 16, 32), initializer=tf.contrib.layers.xavier_initializer())\n    b2 = tf.get_variable(\"b2\", shape=(32), initializer=tf.zeros_initializer())\n    conv2 = tf.nn.conv2d(h1_pool, W2, strides=[1, 1, 1, 1], padding=\"VALID\")\n    h2 = tf.nn.relu(conv2 + b2)\n\n    # Define the max-pooling layer\n    h2_pool = tf.nn.max_pool(h2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"VALID\")\n\n    # Flatten the output of the previous layer\n    h2_pool_flat = tf.reshape(h2_pool, [-1, 16 * 16 * 32])\n\n    # Define the first", "CodeBleu_score": 31.833707490151188}, {"code": "def build_model(x_train, num_classes)\n\t# Reset the TensorFlow graph\n    tf.reset_default_graph()\n\n    # Define the input layer\n    X_input = tf.placeholder(tf.float32, shape=[None, x_train.shape[1], x_train.shape[2], x_train.shape[3]])\n\n    # Define the model\n    X = tf.layers.conv2d(X_input, filters=32, kernel_size=[3, 3], strides=[1, 1], padding='same', activation=tf.nn.relu)\n    X = tf.layers.conv2d(X, filters=32, kernel_size=[3, 3], strides=[1, 1], padding='same', activation=tf.nn.relu)\n    X = tf.layers.max_pooling2d(X, pool_size=[2, 2], strides=2)\n    X = tf.layers.flatten(X)\n    X = tf.layers.dense(X, units=100, activation=tf.nn.relu)\n    X = tf.layers.dense(X, units=num_classes, activation=tf.nn.softmax)\n\n    # Define the model output\n    model = tf.estimator.Estimator(model_fn=tf.estimator.EstimatorSpec(mode=None, predictions=X))\n\n    return model", "CodeBleu_score": 29.507642251803823}]}
{"instance_id": "813e3e2844f4", "generation_result": [{"code": "def extract_bboxes(mask)\n\t# initialize array to store bboxes\n    bboxes = np.zeros((mask.shape[0], 4), dtype=np.int32)\n    # loop over mask layers\n    for i in range(mask.shape[0]):\n        # find horizontal and vertical indices where mask is present\n        horizontal_indices = np.where(np.any(mask[i, :, :], axis=0))[0]\n        vertical_indices = np.where(np.any(mask[i, :, :], axis=1))[0]\n        # find bounding box coordinates based on indices\n        if horizontal_indices.shape[0]:\n            x1, x2 = horizontal_indices[[0, -1]]\n            y1, y2 = vertical_indices[[0, -1]]\n            # store coordinates in array\n            bboxes[i] = x1, y1, x2, y2\n    return bboxes\n\n", "CodeBleu_score": 29.435846325493557}, {"code": "def extract_bboxes(mask)\n\t# initialize array to store the bboxes\n    bboxes = np.zeros((mask.shape[0], 4), dtype=np.int32)\n\n    # loop through the mask layers\n    for i in range(mask.shape[0]):\n        # determine the horizontal and vertical indices where the mask is present\n        horizontal_indices = np.where(np.any(mask[i, :, :], axis=0))[0]\n        vertical_indices = np.where(np.any(mask[i, :, :], axis=1))[0]\n\n        # if there is no mask, set the bbox to zeros\n        if horizontal_indices.shape[0] == 0 or vertical_indices.shape[0] == 0:\n            bboxes[i, :] = 0\n        else:\n            # determine the coordinates of the bounding box based on the indices\n            x1 = horizontal_indices[0]\n            x2 = horizontal_indices[-1]\n            y1 = vertical_indices[0]\n            y2 = vertical_indices[-1]\n\n            # store the bounding box in the array\n            bboxes[i, :] = x1, y1, x2, y2\n\n    return bboxes\n\n", "CodeBleu_score": 30.03965002285115}, {"code": "def extract_bboxes(mask)\n\t# initialize array to store bounding boxes\n    bboxes = np.zeros((mask.shape[0], 4), dtype=np.int32)\n    # loop over mask layers\n    for i in range(mask.shape[0]):\n        # determine horizontal and vertical indices where mask is present\n        # sum up the number of mask pixels for each row and column\n        row_sum = np.sum(mask[i], axis=0)\n        col_sum = np.sum(mask[i], axis=1)\n        # find indices where mask is present\n        row_indices = np.where(row_sum > 0)[0]\n        col_indices = np.where(col_sum > 0)[0]\n        # determine bounding box coordinates\n        if row_indices.shape[0] > 0 and col_indices.shape[0] > 0:\n            x1, x2 = row_indices[[0, -1]]\n            y1, y2 = col_indices[[0, -1]]\n            # store coordinates in array\n            bboxes[i] = x1, y1, x2, y2\n    return bboxes\n\n", "CodeBleu_score": 27.509918793045646}]}
{"instance_id": "7ad5d430d6ed", "generation_result": [{"code": "def compute_iou(box, boxes, box_area, boxes_area)\n\t# Calculate intersection areas\n    y1 = np.maximum(box[0], boxes[:, 0])\n    y2 = np.minimum(box[2], boxes[:, 2])\n    x1 = np.maximum(box[1], boxes[:, 1])\n    x2 = np.minimum(box[3], boxes[:, 3])\n    intersection = np.maximum(y2 - y1, 0) * np.maximum(x2 - x1, 0)\n    # Compute the union areas\n    union = box_area + boxes_area[:] - intersection[:]\n    # Calculate IoU\n    iou = intersection / union\n    # print(iou)\n    return iou\n\n", "CodeBleu_score": 56.12768883353916}, {"code": "def compute_iou(box, boxes, box_area, boxes_area)\n\t# Calculate the intersection areas\n    x1 = np.maximum(box[0], boxes[:, 0])\n    y1 = np.maximum(box[1], boxes[:, 1])\n    x2 = np.minimum(box[2], boxes[:, 2])\n    y2 = np.minimum(box[3], boxes[:, 3])\n    intersection = np.maximum(0, x2 - x1) * np.maximum(0, y2 - y1)\n\n    # Calculate the union areas\n    union = box_area + boxes_area[:] - intersection[:]\n\n    # Calculate the IoU\n    iou = intersection / union\n\n    return iou\n\n", "CodeBleu_score": 49.41974852090001}, {"code": "def compute_iou(box, boxes, box_area, boxes_area)\n\t# Calculate the intersection areas\n    intersection_left_right = np.maximum(boxes[:, 0], box[0])\n    intersection_top_bottom = np.maximum(boxes[:, 1], box[1])\n    intersection_right_left = np.minimum(boxes[:, 2], box[2])\n    intersection_bottom_top = np.minimum(boxes[:, 3], box[3])\n    intersection_widths = np.maximum(intersection_right_left - intersection_left_right, 0)\n    intersection_heights = np.maximum(intersection_bottom_top - intersection_top_bottom, 0)\n    intersections = intersection_widths * intersection_heights\n    # Calculate the union areas\n    unions = boxes_area + box_area - intersections\n    # Calculate the IoU\n    iou = intersections / unions\n    return iou\n\n", "CodeBleu_score": 21.308424063747516}]}
{"instance_id": "15941b852859", "generation_result": [{"code": "def compute_overlaps(boxes1, boxes2)\n\t# Areas of anchors and GT boxes\n    area1 = (boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1])\n    area2 = (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1])\n\n    # Compute overlaps to generate matrix [boxes1 count, boxes2 count]\n    # Each cell contains the IoU value.\n    overlaps = np.zeros((boxes1.shape[0], boxes2.shape[0]))\n    for i in range(overlaps.shape[1]):\n        box2 = boxes2[i]\n        overlaps[:, i] = compute_iou(box2, boxes1, area2[i], area1)\n    return overlaps\n\n", "CodeBleu_score": 62.2853282032944}, {"code": "def compute_overlaps(boxes1, boxes2)\n\t# if there are no boxes, return an empty result\n    if boxes1.shape[0] == 0 or boxes2.shape[0] == 0:\n        return np.zeros((boxes1.shape[0], boxes2.shape[0]))\n\n    # if the sets of boxes are the same, then the overlap is 1 (both boxes are the same)\n    if boxes1 is boxes2:\n        return np.ones((boxes1.shape[0], boxes2.shape[0]))\n\n    # compute areas for each box in both sets\n    areas1 = (boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1])\n    areas2 = (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1])\n\n    # compute overlaps between boxes in two sets\n    overlaps = np.zeros((boxes1.shape[0], boxes2.shape[0]))\n    for i in range(boxes1.shape[0]):\n        box1 = boxes1[i, :]\n        box1_area = areas1[i]\n\n        # calculate IoU\n        # iou = compute_iou(box1, boxes2, box1_area, areas2)\n        # overlaps[i, :] = iou\n        # print(box1)\n        # print(boxes2)\n        # print(box1_area)\n        # print(areas2)\n        # print(overlaps[i, :])\n\n        # find the smallest and largest (x, y) coordinates for the intersection of the two boxes\n        x1 = max(box1[1], boxes2[:, 1])\n        y1 = max(box1[0], boxes2[:, 0])\n        x2 = min(box1[3], boxes2[:, 3])\n        y2 = min(box1[2], boxes2[:, 2])\n\n        # compute the area of intersection rectangle\n        inter_area = max(0, x2 - x1) * max(0, y2 -", "CodeBleu_score": 56.53118361956967}, {"code": "def compute_overlaps(boxes1, boxes2)\n\t# if there are no boxes, return an empty result\n    if len(boxes1) == 0 or len(boxes2) == 0:\n        return np.zeros((len(boxes1), len(boxes2)))\n\n    # if the sets of boxes are the same, then the overlaps are 1 if the boxes are the same\n    if boxes1 is boxes2:\n        # 100% overlap\n        return np.ones((len(boxes1), len(boxes2)))\n\n    # compute the area of each box in both sets of boxes\n    area1 = (boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1])\n    area2 = (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1])\n\n    # compute the intersection areas\n    overlaps = np.zeros((len(boxes1), len(boxes2)))\n    for i in range(overlaps.shape[1]):\n        box2 = boxes2[i]\n        overlaps[:, i] = compute_iou(box2, boxes1, area2[i], area1)\n    return overlaps\n\n", "CodeBleu_score": 71.48472541542318}]}
{"instance_id": "597533722ce3", "generation_result": [{"code": "def load_image(self, image_id):\n\timage = Image.open(self.images[image_id]['path'])\n    image = image.convert('RGB')\n    image = np.array(image)\n    if image.shape[2] == 4:\n        image = image[:, :, :3]\n    return image\n", "CodeBleu_score": 52.13208056675693}, {"code": "def load_image(self, image_id):\n\t# load image\n    image = skimage.io.imread(self.image_info[image_id][\"path\"])\n    # if has an alpha channel, remove it for consistency\n    if image.shape[-1] == 4:\n        image = image[..., :3]\n    # if grayscale. Convert to RGB for consistency.\n    if image.ndim != 3:\n        image = skimage.color.gray2rgb(image)\n    # if has an alpha channel, remove it for consistency\n    if image.shape[-1] == 4:\n        image = image[..., :3]\n    return image", "CodeBleu_score": 87.45142381234082}, {"code": "def load_image(self, image_id):\n\timage_info = self.image_info[image_id]\n    image = skimage.io.imread(image_info[\"path\"])\n    if image.ndim != 3:\n        image = skimage.color.gray2rgb(image)\n    if image.shape[-1] == 4:\n        image = image[..., :3]\n    return image", "CodeBleu_score": 82.40633394685716}]}
{"instance_id": "9b852997ca05", "generation_result": [{"code": "def display_images(images, titles=None, cols=4, cmap=None, norm=None,\n                   interpolation=None)\n\ttitles = titles if titles is not None else ['Image (%d)' % i for i in range(1, len(images) + 1)]\n    rows = len(images) // cols + 1\n    plt.figure(figsize=(14, 14 * rows // cols))\n    i = 1\n    for image, title in zip(images, titles):\n        plt.subplot(rows, cols, i)\n        plt.title(title, fontsize=9)\n        plt.axis('off')\n        plt.imshow(image, cmap=cmap, norm=norm, interpolation=interpolation)\n        i += 1\n    plt.show()", "CodeBleu_score": 69.92441757896728}, {"code": "def display_images(images, titles=None, cols=4, cmap=None, norm=None,\n                   interpolation=None)\n\ttitles = titles if titles is not None else [\"Image (%d)\" % i for i in range(1, len(images) + 1)]\n    rows = len(images) // cols + 1\n    plt.figure(figsize=(14, 14 * rows // cols))\n    i = 1\n    for image, title in zip(images, titles):\n        plt.subplot(rows, cols, i)\n        plt.title(title, fontsize=9)\n        plt.axis('off')\n        plt.imshow(image, cmap=cmap, norm=norm, interpolation=interpolation)\n        i += 1\n    plt.show()\n\n", "CodeBleu_score": 69.9943504412815}, {"code": "def display_images(images, titles=None, cols=4, cmap=None, norm=None,\n                   interpolation=None)\n\ttitles = titles if titles is not None else [\"Image (%d)\" % i for i in range(1, len(images) + 1)]\n    rows = len(images) // cols + 1\n    plt.figure(figsize=(14, 14 * rows // cols))\n    i = 1\n    for image, title in zip(images, titles):\n        plt.subplot(rows, cols, i)\n        plt.title(title, fontsize=9)\n        plt.axis('off')\n        plt.imshow(image, cmap=cmap, norm=norm, interpolation=interpolation)\n        i += 1\n    plt.show()\n\n", "CodeBleu_score": 69.9943504412815}]}
