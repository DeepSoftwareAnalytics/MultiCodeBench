{"instance_id": "1f9f4d85e825", "generation_result": [{"code": "def load_model_from_config(self, half_attention)\n\t\n    if self.cache_path.exists():\n        if self.cache_path.is_file():\n            try:\n                with open(self.cache_path, \"rb\") as f:\n                    cached_model = pickle.load(f)\n                    if cached_model[\"hash\"] == self.hash:\n                        self.model = cached_model[\"model\"]\n                        return cached_model\n                    else:\n                        raise ValueError(\"Cached model hash does not match\")\n            except Exception as e:\n                logger.warning(f\"Error loading cached model: {e}\")\n\n    model = load_model_from_config_internal(self.model_path, self.config_path, self.device, half_attention)\n    if self.cache_path is not None:\n        with open(self.cache_path, \"wb\") as f:\n            pickle.dump({\"hash\": self.hash, \"model\": model}, f)\n    return {\"hash\": self.hash, \"model\": model}", "CodeBleu_score": 27.370307284756766}, {"code": "def load_model_from_config(self, half_attention)\n\t\n    if (\n        self.config.model_path\n        and os.path.exists(self.config.model_path)\n        and self.config.model_path.endswith(\".safetensors\")\n    ):\n        if self.config.use_cache and os.path.exists(\n            os.path.join(\n                self.config.cache_dir,\n                os.path.basename(self.config.model_path).replace(\".safetensors\", \".pt\"),\n            )\n        ):\n            self.logger.info(\n                f\"Loading model from cache: {os.path.join(self.config.cache_dir, os.path.basename(self.config.model_path).replace('.safetensors', '.pt'))}\"\n            )\n            model = torch.load(\n                os.path.join(\n                    self.config.cache_dir,\n                    os.path.basename(self.config.model_path).replace(\".safetensors\", \".pt\"),\n                )\n            )\n        else:\n            self.logger.info(f\"Loading model from: {self.config.model_path}\")\n            model = safetensors.torch.load_file(\n                self.config.model_path,\n                device=self.config.device,\n                safe_device_mem_realloc=True,\n            )\n            if self.config.use_cache:\n                self.logger.info(\n                    f\"Caching model: {os.path.join(self.config.cache_dir, os.path.basename(self.config.model_path).replace('.safetensors', '.pt'))}\"\n                )\n                torch.save(\n                    model,\n                    os.path.join(\n                        self.config.cache_dir,\n                        os.path.basename(self.config.model_path).replace(\".safetensors\", \".pt\"),\n                    ),\n                )\n    else:\n        self.logger.info(f\"Loading model from: {self.config.model_path}\")\n        model = torch.load(\n            self.config.model_path,\n           ", "CodeBleu_score": 28.091552101284158}, {"code": "def load_model_from_config(self, half_attention)\n\t\n    if self.cache_path and os.path.exists(self.cache_path):\n        try:\n            with open(self.cache_path, \"rb\") as f:\n                cached_model = pickle.load(f)\n                if cached_model[\"config\"] == self.config and cached_model[\"half_attention\"] == half_attention:\n                    return cached_model[\"model\"]\n        except:\n            pass\n\n    model = self.load_model_from_path(self.path, half_attention)\n    model.eval()\n    model.to(self.device)\n    if self.cache_path:\n        with open(self.cache_path, \"wb\") as f:\n            pickle.dump({\"config\": self.config, \"half_attention\": half_attention, \"model\": model}, f)\n    return model\n", "CodeBleu_score": 21.287176192610357}]}
{"instance_id": "8dc955007aa2", "generation_result": [{"code": "def convsample_ddim(model, cond, steps, shape, eta=1.0, callback=None, normals_sequence=None,\n                    mask=None, x0=None, quantize_x0=False, temperature=1., score_corrector=None,\n                    corrector_kwargs=None, x_t=None\n                    )\n\t\n    batch_size = shape[0]\n    print(f'sampling {batch_size * steps} steps for {batch_size} samples')\n    model.to(model.device)\n    if isinstance(shape, int):\n        shape = (shape,)\n    print(f'shape: {shape}, eta: {eta}, steps: {steps}')\n    if steps % model.num_timesteps == 0:\n        raise ValueError(f'`steps` must be divisible by model num_timesteps, but got `steps`: {steps} and `model.num_timesteps`: {model.num_timesteps}.')\n\n    if normals_sequence is not None:\n        normal_samples = normal_samples.to(model.device)\n        normal_samples = normal_samples.to(torch.float32)\n\n    if not isinstance(shape, tuple):\n        raise TypeError('`shape` should be a tuple, but got ', type(shape))\n\n    if not isinstance(eta, float) and not isinstance(eta, int) and not isinstance(eta, torch.Tensor):\n        raise TypeError('`eta` should be a float or tensor, but got ', type(eta))\n    if isinstance(eta, float):\n        eta = torch.tensor([eta], device=model.device)\n\n    if not (isinstance(x_t, torch.Tensor) or x_t is None):\n        raise ValueError('`x_t` must be a tensor or None, but got ', type(x_t))\n\n    if isinstance(eta, torch.Tensor) and (eta.shape[0] != 1 and eta.shape[0] != batch_size):\n        raise ValueError('eta must have a shape of (1,) or (batch_size,), but got ', eta.shape)\n\n    if isinstance(x_t, torch.Tensor) and (x_t.shape[0] != 1 and x_t.shape[0] != batch_size):\n        raise ValueError('`x_t` must have a shape of (1,) or (batch_", "CodeBleu_score": 29.147459840908173}, {"code": "def convsample_ddim(model, cond, steps, shape, eta=1.0, callback=None, normals_sequence=None,\n                    mask=None, x0=None, quantize_x0=False, temperature=1., score_corrector=None,\n                    corrector_kwargs=None, x_t=None\n                    )\n\t\n\n    ## This function performs conditional sampling using the DDIM (Denoising Diffusion Implicit Models) sampler. It initializes the sampler with the provided model, extracts the batch size from the shape parameter, and prints the eta and steps values. The function then calls the sample method of the DDIMSampler instance, passing several parameters such as steps, batch size, shape, conditioning, eta, and other optional arguments like callback, normals_sequence, mask, x0, quantize_x0, temperature, score_corrector, corrector_kwargs, and x_t. Finally, it returns the generated samples and intermediate values.\n\n    # Initialize the sampler with the model and the number of steps\n    sampler = DDIMSampler(model)\n    batch_size = shape[0]\n    print(f\"sampling {batch_size * steps} steps for {batch_size} samples\")\n\n    # Print the eta and steps values\n    print(f\"eta: {eta}, steps: {steps}\")\n\n    # Call the sample method of the DDIMSampler instance\n    # Pass several parameters such as steps, batch size, shape, conditioning, eta, and other optional arguments like callback, normals_sequence, mask, x0, quantize_x0, temperature, score_corrector, corrector_kwargs, and x_t\n    return sampler.sample(\n        S=steps,\n        batch_size=batch_size,\n        shape=shape,\n        conditioning=cond,\n        eta=eta,\n        callback=callback,\n        normals_sequence=normals_sequence,\n        mask=mask,\n        x0=x0,\n        quantize_x0=quantize_x0,\n        temperature=temperature,\n        score_corrector=score_corrector,\n        corrector_kwargs=corrector_kwargs,\n        x_T=x_t\n    )\n\n", "CodeBleu_score": 42.53742587830681}, {"code": "def convsample_ddim(model, cond, steps, shape, eta=1.0, callback=None, normals_sequence=None,\n                    mask=None, x0=None, quantize_x0=False, temperature=1., score_corrector=None,\n                    corrector_kwargs=None, x_t=None\n                    )\n\t\n\n    batch_size = shape[0]\n    print(f\"sampling {batch_size * steps} steps for {batch_size} samples\")\n\n    if normals_sequence is None:\n        normals_sequence = [torch.randn(shape).to(model.device) for _ in range(steps)]\n\n    if not isinstance(normals_sequence, list):\n        normals_sequence = [normals_sequence]\n\n    # setup model input\n    if isinstance(cond, tuple):\n        cond = cond[0]\n\n    shape_cond = [batch_size] + list(cond.shape[1:])\n    cond = cond.to(model.device).repeat(1, *((steps,) + (1,) * (len(shape) - 1)))\n    shape_x0 = [batch_size] + list(x0.shape[1:])\n    x0 = x0.to(model.device).repeat(1, *((steps,) + (1,) * (len(shape) - 1)))\n\n    # setup model kwargs\n    model_kwargs = {}\n    if mask is not None:\n        mask = mask.to(model.device).repeat(1, *((steps,) + (1,) * (len(shape) - 1)))\n        model_kwargs['mask'] = mask\n\n    if quantize_x0:\n        x_q = model.quantize(x0)\n        if mask is not None:\n            model_kwargs['mask'] = mask\n        model_kwargs['x_q'] = x_q\n\n    if score_corrector is not None:\n        assert isinstance(score_corrector, str)\n        if not hasattr(model, score_corrector):\n            raise AttributeError(f\"{model.__class__.__name__} has no score corrector named '{score_corrector}'\")\n        score_corrector = getattr(model, score_corrector)\n\n    if corrector_kwargs is None:\n        corrector_kwargs = {}\n\n    # initialize sampler\n    sampler = DDIMSampler(model)\n\n    # call sampler\n    samples, inter", "CodeBleu_score": 32.964088759363065}]}
{"instance_id": "9bdcd9081480", "generation_result": [{"code": "def ema_scope(self, context=None)\n\t\n    pass\n\n", "CodeBleu_score": 3.425925925925926}, {"code": "def ema_scope(self, context=None)\n\t\n    pass\n\n", "CodeBleu_score": 3.425925925925926}, {"code": "def ema_scope(self, context=None)\n\t\n    pass\n\n", "CodeBleu_score": 3.425925925925926}]}
{"instance_id": "14ab009e1c5c", "generation_result": [{"code": "def init_from_ckpt(self, path, ignore_keys=None)\n\t\n    state_dict = torch.load(path, map_location=\"cpu\")\n    if ignore_keys is not None:\n        state_dict = {\n            k: v\n            for k, v in state_dict.items()\n            if not any(ignore_key in k for ignore_key in ignore_keys)\n        }\n    self.load_state_dict(state_dict, strict=False)\n    missing_keys = set(self.state_dict().keys()) - set(state_dict.keys())\n    unexpected_keys = set(state_dict.keys()) - set(self.state_dict().keys())\n    if len(missing_keys) > 0:\n        logger.warning(\n            f\"Missing key(s) in state_dict: {missing_keys}. \"\n            \"Will try to use compatible keys instead.\"\n        )\n    if len(unexpected_keys) > 0:\n        logger.warning(\n            f\"Unexpected key(s) in state_dict: {unexpected_keys}. \"\n            \"The keys will not be used during initialization.\"\n        )\n\n\n\n\n", "CodeBleu_score": 30.871769389832338}, {"code": "def init_from_ckpt(self, path, ignore_keys=None)\n\t\n    state_dict = torch.load(path, map_location=\"cpu\")\n    if ignore_keys is not None:\n        for key in ignore_keys:\n            if key in state_dict:\n                del state_dict[key]\n    self.load_state_dict(state_dict, strict=False)\n    missing_keys = []\n    unexpected_keys = []\n    for name, param in self.named_parameters():\n        if name not in state_dict:\n            missing_keys.append(name)\n        elif param.shape != state_dict[name].shape:\n            unexpected_keys.append(name)\n    if len(missing_keys) > 0:\n        print(\"Missing key(s) in state_dict: {}\".format(missing_keys))\n    if len(unexpected_keys) > 0:\n        print(\"Unexpected key(s) in state_dict: {}\".format(unexpected_keys))\n\n", "CodeBleu_score": 31.156214597028757}, {"code": "def init_from_ckpt(self, path, ignore_keys=None)\n\t\n    state_dict = torch.load(path, map_location=\"cpu\")\n    if ignore_keys is not None:\n        for key in ignore_keys:\n            del state_dict[key]\n    self.load_state_dict(state_dict)\n    logger.info(f\"Loaded model weights from {path}\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 15.638321349136167}]}
{"instance_id": "754176e27b81", "generation_result": [{"code": "def forward(self, input, return_pred_indices=False)\n\t\n    quant, emb_loss, info = self.encode(input)\n    dec = self.decode(quant)\n    return dec, emb_loss, info\n\n", "CodeBleu_score": 27.0673125097895}, {"code": "def forward(self, input, return_pred_indices=False)\n\t\n    quant, diff, _ = self.encode(input)\n    dec = self.decode(quant)\n    if return_pred_indices:\n        return dec, diff, quant.argmax(dim=1)\n    else:\n        return dec, diff\n\n", "CodeBleu_score": 47.36647066506015}, {"code": "def forward(self, input, return_pred_indices=False)\n\t\n    quant, diff, _ = self.encode(input)\n    dec = self.decode(quant)\n    if return_pred_indices:\n        return dec, diff, self.get_pred_indices(quant)\n    return dec, diff\n\n", "CodeBleu_score": 55.268730147079026}]}
{"instance_id": "443df6fdaf0d", "generation_result": [{"code": "def get_input(self, batch, k)\n\t\n    x = batch[k]\n    if x.ndim == 2:\n        x = x[:, None]\n    if x.ndim == 3:\n        x = x[:, None, :, :]\n    if x.ndim == 4:\n        x = x.permute(0, 3, 1, 2)\n    if self.batch_resize:\n        x = x.to(torch.float32)\n        x = torch.nn.functional.interpolate(x, size=(self.training_step, self.training_step), mode='bicubic', align_corners=False)\n    x = x.contiguous()\n    x = x.detach()\n    return x", "CodeBleu_score": 40.81197490390265}, {"code": "def get_input(self, batch, k)\n\t\n    if batch.get(k) is None:\n        raise RuntimeError('No data found for key {}'.format(k))\n    x = batch[k]\n    if x.dim() == 2:\n        x = x.unsqueeze(1)\n    if x.dim() == 3:\n        x = x.unsqueeze(1)\n    if x.dim() == 4:\n        x = x.permute(0, 3, 1, 2)\n    if x.dim() != 4:\n        raise RuntimeError('Invalid number of dimensions for input: {}'.format(x.dim()))\n    if self.batch_resize:\n        x = self.batch_resize_function(x, self.batch_resize_range, self.training_step)\n    x = x.contiguous().float()\n    if self.device is not None:\n        x = x.to(self.device)\n    x = x.detach()\n    return x", "CodeBleu_score": 43.53592097570066}, {"code": "def get_input(self, batch, k)\n\t\n    input = batch[k]\n    if self.batch_size == 1:\n        input = input.unsqueeze(0)\n    if input.dim() == 3:\n        input = input.unsqueeze(0)\n    if input.dim() == 5:\n        input = input.squeeze(0)\n    if input.dim() == 4:\n        input = input.permute(0, 2, 3, 1)\n    input = input.contiguous().float()\n    if self.batch_resize:\n        if (self.training):\n            step = self.model.step\n            if (step < self.batch_resize_start):\n                step = self.batch_resize_start\n            if (step > self.batch_resize_end):\n                step = self.batch_resize_end\n            scale = ((step - self.batch_resize_start) / (self.batch_resize_end - self.batch_resize_start))\n            w = int(((input.size(2) * scale) + 0.5))\n            h = int(((input.size(1) * scale) + 0.5))\n            input = torch.nn.functional.interpolate(input, size=(h, w), mode='bicubic', align_corners=False)\n    input = input.detach()\n    return input\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 36.701943186391475}]}
{"instance_id": "953a2632fc15", "generation_result": [{"code": "def training_step(self, batch, batch_idx, optimizer_idx)\n\t\n    x = self.get_input(batch, \"image\")\n    preds = self.forward(x)\n    if optimizer_idx == 0:\n        loss = self.autoencoding_loss(preds, x)\n    else:\n        loss = self.discriminator_loss(preds, x)\n    self.log(\"train/loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n    return loss\n\n", "CodeBleu_score": 19.341746037589473}, {"code": "def training_step(self, batch, batch_idx, optimizer_idx)\n\t\n    x = self.get_input(batch, \"image\")\n    pred = self.forward(x)\n    loss_autoencoding = self.loss_autoencoding(pred, x)\n    if self.hparams.use_discriminator:\n        loss_discriminator = self.loss_discriminator(pred)\n        loss = loss_autoencoding + loss_discriminator\n        self.log(\"loss/loss_autoencoding\", loss_autoencoding, prog_bar=True)\n        self.log(\"loss/loss_discriminator\", loss_discriminator, prog_bar=True)\n    else:\n        loss = loss_autoencoding\n        self.log(\"loss/loss_autoencoding\", loss_autoencoding, prog_bar=True)\n    self.log(\"loss/total\", loss, prog_bar=True)\n    return loss\n\n", "CodeBleu_score": 24.781128970326126}, {"code": "def training_step(self, batch, batch_idx, optimizer_idx)\n\t\n    x = self.get_input(batch, \"image\")\n    y = self.get_input(batch, \"mask\")\n    pred = self.forward(x)\n    if optimizer_idx == 0:\n        loss = self.autoencoder_loss(pred, y)\n        self.log(\"autoencoder_loss\", loss, prog_bar=True, logger=True)\n    elif optimizer_idx == 1:\n        loss = self.discriminator_loss(pred, y)\n        self.log(\"discriminator_loss\", loss, prog_bar=True, logger=True)\n    return loss\n\n", "CodeBleu_score": 24.64121526282481}]}
{"instance_id": "b449b5e8b77f", "generation_result": [{"code": "def log_images(self, batch, only_inputs=False, plot_ema=False, **kwargs)\n\t\n    x = self.get_input(batch, self.image_key)\n    if plot_ema:\n        if not hasattr(self, \"ema_model\"):\n            raise ValueError(\"EMA model not found.\")\n        x_ema = self.ema_model.get_input(batch, self.image_key)\n    if self.image_key == \"segmentation\":\n        x = self.to_rgb(x)\n        if plot_ema:\n            x_ema = self.to_rgb(x_ema)\n    if not only_inputs:\n        x_rec = self.model(x)\n        if plot_ema:\n            x_rec_ema = self.ema_model(x_ema)\n        self.logger.experiment.add_images(\"input\", x, self.global_step, dataformats=\"NCHW\")\n        self.logger.experiment.add_images(\"reconstruction\", x_rec, self.global_step, dataformats=\"NCHW\")\n        if plot_ema:\n            self.logger.experiment.add_images(\"reconstruction_ema\", x_rec_ema, self.global_step, dataformats=\"NCHW\")\n    else:\n        self.logger.experiment.add_images(\"input\", x, self.global_step, dataformats=\"NCHW\")\n\n", "CodeBleu_score": 31.855652136518707}, {"code": "def log_images(self, batch, only_inputs=False, plot_ema=False, **kwargs)\n\t\n    # process inputs\n    x = self.get_input(batch, self.image_key)\n    # log inputs\n    self.logger.experiment.add_images(f\"{self.image_key}_inputs\", x, self.global_step)\n    # log reconstructions\n    if not only_inputs:\n        with torch.no_grad():\n            z = self.encode(x)\n            if plot_ema:\n                if self.ema_model is not None:\n                    x_recon = self.ema_model.decode(z)\n                    self.logger.experiment.add_images(f\"{self.image_key}_recon_ema\", x_recon, self.global_step)\n            x_recon = self.decode(z)\n            self.logger.experiment.add_images(f\"{self.image_key}_recon\", x_recon, self.global_step)\n            # colorize segmentation maps\n            if self.image_key == \"segmentation\":\n                x_recon = self.to_rgb(x_recon)\n                self.logger.experiment.add_images(f\"{self.image_key}_recon_rgb\", x_recon, self.global_step)\n\n", "CodeBleu_score": 31.491677829475762}, {"code": "def log_images(self, batch, only_inputs=False, plot_ema=False, **kwargs)\n\t\n    if self.global_step % self.log_every_n_steps == 0:\n        if only_inputs:\n            log_inputs(self, batch)\n        else:\n            log_inputs(self, batch)\n            log_reconstructions(self, batch, plot_ema=plot_ema)\n\n", "CodeBleu_score": 7.3432238264865255}]}
{"instance_id": "540d089e3481", "generation_result": [{"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss_type == 'l1':\n        if mean:\n            return torch.mean(torch.abs(pred - target))\n        else:\n            return torch.abs(pred - target)\n    elif self.loss_type == 'l2':\n        if mean:\n            return torch.mean((pred - target) ** 2)\n        else:\n            return (pred - target) ** 2\n    else:\n        raise ValueError(\"Unknown loss type: {}\".format(self.loss_type))", "CodeBleu_score": 33.99461313903018}, {"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss_type == 'l1':\n        loss = F.l1_loss(pred, target, reduction='none')\n        if mean:\n            loss = loss.mean()\n        else:\n            loss = loss.sum()\n    elif self.loss_type == 'l2':\n        loss = F.mse_loss(pred, target, reduction='none')\n        if mean:\n            loss = loss.mean()\n        else:\n            loss = loss.sum()\n    else:\n        raise ValueError(f\"Unknown loss type: {self.loss_type}\")\n    return loss", "CodeBleu_score": 54.76177457240263}, {"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss_type == 'l1':\n        if mean:\n            return torch.mean(torch.abs(pred - target))\n        else:\n            return torch.abs(pred - target)\n    elif self.loss_type == 'l2':\n        if mean:\n            return torch.mean((pred - target) ** 2)\n        else:\n            return (pred - target) ** 2\n    else:\n        raise ValueError(\"Unknown loss type: {}\".format(self.loss_type))\n\n\n\n\n", "CodeBleu_score": 33.99461313903018}]}
{"instance_id": "63a08283666f", "generation_result": [{"code": "def forward(self, x, c, *args, **kwargs)\n\t\n    if self.cond_stage_model is not None:\n        c = self.get_learned_conditioning(c)\n\n    if self.training:\n        t = torch.randint(0, self.num_timesteps, (x.shape[0],), device=x.device)\n    else:\n        t = torch.full((x.shape[0],), self.num_timesteps - 1, device=x.device, dtype=torch.long)\n\n    if self.cond_stage_model is not None:\n        x = self.q_sample(x, t=t)\n\n    if self.cond_stage_model is not None:\n        x = self.cond_stage_model.uncond_transform(x, c)\n\n    if self.cond_stage_model is not None:\n        x = self.cond_stage_model.transform(x, c)\n\n    if self.cond_stage_model is not None:\n        x = self.cond_stage_model.postprocess(x)\n\n    model_kwargs = {}\n    if 'model_kwargs' in kwargs:\n        model_kwargs = kwargs['model_kwargs']\n\n    if self.cond_stage_model is not None:\n        model_kwargs.update({'cond': c})\n\n    if self.cond_stage_model is not None:\n        model_kwargs.update({'uncond': self.cond_stage_model.get_learned_conditioning(0)})\n\n    loss, loss_dict = self.p_losses(x, t, **model_kwargs)\n\n    return loss, loss_dict\n\n", "CodeBleu_score": 46.80590287366632}, {"code": "def forward(self, x, c, *args, **kwargs)\n\t\n\n    if self.cond_stage_model is not None:\n        c = self.get_learned_conditioning(c)\n    if self.use_timesteps:\n        t = torch.randint(0, self.num_timesteps, (x.shape[0],), device=x.device)\n    else:\n        t = torch.zeros((x.shape[0],), device=x.device, dtype=torch.long)\n    if self.parameterization == 'x0':\n        x_input = self.q_sample(x, t=t)\n    elif self.parameterization == 'eps':\n        x_input = x\n    else:\n        raise ValueError(f'{self.parameterization} not supported')\n\n    model_kwargs = dict(x=x_input, t=t, c=c)\n    if self.model_type == 'ddpm':\n        model_kwargs = {k: v[None, ...] for k, v in model_kwargs.items()}\n    elif self.model_type == 'vqvae':\n        model_kwargs = {k: v for k, v in model_kwargs.items()}\n    elif self.model_type == 'vqvae2':\n        model_kwargs = {k: v for k, v in model_kwargs.items()}\n    elif self.model_type == 'vqvae3':\n        model_kwargs = {k: v for k, v in model_kwargs.items()}\n    elif self.model_type == 'vqvae4':\n        model_kwargs = {k: v for k, v in model_kwargs.items()}\n    elif self.model_type == 'vqvae5':\n        model_kwargs = {k: v for k, v in model_kwargs.items()}\n    elif self.model_type == 'vqvae6':\n        model_kwargs = {k: v for k, v in model_kwargs.items()}\n    elif self.model_type == 'vqvae7':\n        model_kwargs = {k: v for k, v in model_kwargs.items()}\n    elif self", "CodeBleu_score": 43.81368931304358}, {"code": "def forward(self, x, c, *args, **kwargs)\n\t\n    c = self.get_learned_conditioning(c)\n    t = self.sample_timesteps(x.shape)\n\n    if self.cond_stage_model_kwargs is not None:\n        model_kwargs = dict(self.cond_stage_model_kwargs)\n        model_kwargs.update(kwargs)\n    else:\n        model_kwargs = kwargs\n\n    if self.model_type == 'ddpm':\n        x_in = self.q_sample(x, t=t)\n        loss, loss_dict = self.p_losses(x_in, t=t)\n    elif self.model_type == 'ddim':\n        x_in = self.q_sample(x, t=t)\n        x_recon = self.p_mean_variance(x_in, t=t)\n        loss, loss_dict = self.p_losses(x_recon, x, t=t)\n    elif self.model_type == 'vqvae':\n        x_in = self.q_sample(x, t=t)\n        x_recon, vqvae_losses = self.p_losses(x_in, t=t)\n        loss, loss_dict = self.p_losses(x_recon, x, t=t)\n        loss_dict.update(vqvae_losses)\n    else:\n        raise NotImplementedError(f\"model_type {self.model_type} not yet supported\")\n\n    return loss, loss_dict\n\n", "CodeBleu_score": 28.163358611139305}]}
{"instance_id": "4627e1e8ed62", "generation_result": [{"code": "def get_codebook_entry(self, indices, shape)\n\t\n    if self.codebook_dim == 0:\n        return torch.zeros(indices.shape[0], *shape, device=indices.device)\n    if self.codebook_dim == 1:\n        if self.remap_to_all:\n            indices = self.unmap_to_all(indices)\n        return self.codebook[indices]\n    else:\n        if self.remap_to_all:\n            indices = self.unmap_to_all(indices)\n        return self.codebook[indices]\n\n", "CodeBleu_score": 31.339733105849373}, {"code": "def get_codebook_entry(self, indices, shape)\n\t\n    if self.codebook_dim > 0:\n        shape = [shape[0], -1] + [1] * (len(shape) - 1)\n        indices = indices.view(shape)\n        indices = self.unmap_to_all(indices)\n        shape = [-1] + list(shape[1:])\n        indices = indices.view(shape)\n    return self.codebook[indices]\n\n", "CodeBleu_score": 43.00652701830656}, {"code": "def get_codebook_entry(self, indices, shape)\n\t\n    if self.re_embed > self.used.shape[0]:  # extra token\n        indices[indices >= self.used.shape[0]] = 0  # simply set to zero\n    if self.remap_to_all:\n        indices = self.unmap_to_all(indices)\n    z = self.codebook[indices]\n    z = z.permute(0, 3, 1, 2)\n    z = z.reshape(shape)\n    return z\n\n", "CodeBleu_score": 41.09691590784756}]}
{"instance_id": "c55d014827ef", "generation_result": [{"code": "def factorization(dimension: int, factor:int=-1) -> tuple[int, int]\n\t\n    if dimension <= 0:\n        raise ValueError(\"dimension must be positive\")\n    if factor > 0 and dimension % factor == 0:\n        return factor, dimension // factor\n    if factor < 0:\n        for m in range(1, dimension + 1):\n            if dimension % m == 0:\n                n = dimension // m\n                if m <= n and n <= abs(factor):\n                    return m, n\n    raise ValueError(f\"dimension {dimension} cannot be factored\")\n\n\n\n", "CodeBleu_score": 37.0270855544359}, {"code": "def factorization(dimension: int, factor:int=-1) -> tuple[int, int]\n\t\n    if factor > 0 and dimension % factor == 0:\n        return factor, dimension // factor\n    if factor < 0:\n        for i in range(2, int(dimension**0.5)+1):\n            if dimension % i == 0:\n                m, n = factorization(dimension // i, factor)\n                if m <= n and m <= abs(factor):\n                    return m, n\n    return 1, dimension\n\n\n", "CodeBleu_score": 34.1993142485693}, {"code": "def factorization(dimension: int, factor:int=-1) -> tuple[int, int]\n\t\n    if factor > 0 and dimension % factor == 0:\n        return factor, dimension // factor\n    elif factor > 0:\n        return factorization(dimension, factor - 1)\n    else:\n        m, n = 1, dimension\n        while m < n:\n            m += 1\n            if dimension % m == 0:\n                n = dimension // m\n        return m, n\n\n\n", "CodeBleu_score": 38.01140504703271}]}
{"instance_id": "5d613ec26036", "generation_result": [{"code": "def apply_weight_decompose(self, updown, orig_weight)\n\t\n    scale_factor = self.scale_factor\n    if updown == \"up\":\n        weight = orig_weight.data\n        weight_up = weight\n        weight_down = torch.zeros_like(weight)\n    elif updown == \"down\":\n        weight = orig_weight.data\n        weight_up = torch.zeros_like(weight)\n        weight_down = weight\n    else:\n        raise ValueError(\"Invalid value for updown: {}\".format(updown))\n    weight = torch.cat((weight_up, weight_down), 0)\n    weight = weight.to(self.device)\n    weight = weight.to(orig_weight.dtype)\n    weight_norm = weight.norm(p=2, dim=1, keepdim=True)\n    weight_norm = torch.max(weight_norm, torch.tensor(1e-12).to(weight.device).to(weight.dtype))\n    weight = weight / weight_norm\n    weight = weight * scale_factor\n    weight = weight.to(orig_weight.dtype)\n    weight = weight.to(orig_weight.device)\n    weight_combined = weight[:, :orig_weight.size(1)] - orig_weight\n    return weight_combined\n\n\n", "CodeBleu_score": 36.5365400094756}, {"code": "def apply_weight_decompose(self, updown, orig_weight)\n\t\n    if self.weight_decompose_scale_factor == 1:\n        return orig_weight\n    if updown == \"up\":\n        if self.weight_decompose_type == \"add\":\n            combined_weight = (orig_weight + self.weight)\n        elif self.weight_decompose_type == \"sub\":\n            combined_weight = (orig_weight - self.weight)\n        elif self.weight_decompose_type == \"mul\":\n            combined_weight = (orig_weight * self.weight)\n        elif self.weight_decompose_type == \"div\":\n            combined_weight = (orig_weight / self.weight)\n        elif self.weight_decompose_type == \"pow\":\n            combined_weight = (orig_weight ** self.weight)\n        else:\n            raise ValueError(f\"Invalid weight_decompose_type: {self.weight_decompose_type}\")\n    elif updown == \"down\":\n        if self.weight_decompose_type == \"add\":\n            combined_weight = (self.weight + orig_weight)\n        elif self.weight_decompose_type == \"sub\":\n            combined_weight = (self.weight - orig_weight)\n        elif self.weight_decompose_type == \"mul\":\n            combined_weight = (self.weight * orig_weight)\n        elif self.weight_decompose_type == \"div\":\n            combined_weight = (self.weight / orig_weight)\n        elif self.weight_decompose_type == \"pow\":\n            combined_weight = (self.weight ** orig_weight)\n        else:\n            raise ValueError(f\"Invalid weight_decompose_type: {self.weight_decompose_type}\")\n    else:\n        raise ValueError(f\"Invalid updown: {updown}\")\n    combined_weight = combined_weight.to(orig_weight.device).type(orig_weight.dtype)\n    combined_weight = combined_weight / combined_weight.norm()\n    combined_weight = combined_weight * self.weight_decompose_scale_factor\n    if updown == \"up", "CodeBleu_score": 17.713392016880082}, {"code": "def apply_weight_decompose(self, updown, orig_weight)\n\t\n    self.weight = torch.nn.Parameter(self.weight.to(orig_weight.device).type(orig_weight.dtype))\n    self.orig_weight = orig_weight.to(self.weight.device).type(self.weight.dtype)\n    if updown:\n        self.weight.data = (self.weight.data + self.orig_weight.data).clamp(0, 1)\n    else:\n        self.weight.data = (self.weight.data - self.orig_weight.data).clamp(0, 1)\n    self.weight.data = self.weight.data / self.weight.data.norm()\n    self.weight.data = self.weight.data * self.scale\n    self.weight.data = self.weight.data - self.orig_weight.data", "CodeBleu_score": 22.11690589047419}]}
{"instance_id": "c73d0d1cd3e5", "generation_result": [{"code": "def finalize_updown(self, updown, orig_weight, output_shape, ex_bias=None)\n\t\n    if ex_bias is not None:\n        updown = updown + ex_bias\n\n    if updown.numel() == orig_weight.numel():\n        updown = updown.reshape(orig_weight.shape)\n    else:\n        updown = updown.reshape(output_shape)\n\n    if self.bias is not None:\n        updown = updown * self.bias\n\n    if self.scale is not None:\n        updown = updown * self.scale\n\n    if self.apply_weight_decompose:\n        updown = self.apply_weight_decompose(updown, orig_weight)\n\n    return updown, ex_bias\n\n", "CodeBleu_score": 52.6007689845025}, {"code": "def finalize_updown(self, updown, orig_weight, output_shape, ex_bias=None)\n\t\n    if ex_bias is not None:\n        ex_bias = ex_bias.to(updown.dtype)\n        updown = updown + ex_bias\n    if self.bias is not None:\n        updown = updown + self.bias\n    updown = updown.reshape(output_shape)\n    if ex_bias is not None:\n        ex_bias = ex_bias.reshape(output_shape)\n    if self.scale is not None:\n        updown = updown * self.scale\n    if self.apply_weight_decompose:\n        updown = self.apply_weight_decompose(updown, orig_weight)\n    return updown, ex_bias\n\n", "CodeBleu_score": 48.87285785024031}, {"code": "def finalize_updown(self, updown, orig_weight, output_shape, ex_bias=None)\n\t\n    # Match the device/dtype\n    orig_weight = orig_weight.to(updown.dtype)\n    updown = updown.to(orig_weight.device)\n\n    # Adjust the bias\n    if ex_bias is not None:\n        ex_bias = ex_bias.to(updown.dtype)\n        updown += ex_bias\n\n    # Reshape updown\n    updown = updown.reshape(output_shape)\n\n    # Reshape updown to match the original weight's shape\n    if updown.numel() == orig_weight.numel():\n        updown = updown.reshape(orig_weight.shape)\n\n    # Apply a multiplier to the bias\n    if self.multiplier is not None:\n        updown *= self.multiplier\n\n    # Apply a weight decomposition\n    if self.apply_weight_decompose:\n        updown = self.apply_weight_decompose(updown, orig_weight)\n\n    # Apply a scaling factor\n    if self.scale is not None:\n        updown *= self.scale\n\n    # Return the scaled and adjusted updown tensor along with the modified additional bias\n    return updown, ex_bias\n\n", "CodeBleu_score": 56.238761317050965}]}
{"instance_id": "c6a41665c62c", "generation_result": [{"code": "def calc_updown(self, orig_weight)\n\t\n    orig_weight = orig_weight.to(self.weight.device)\n    ex_bias = None\n    if self.bias is not None:\n        ex_bias = self.bias.to(orig_weight.device)\n\n    updown = self.weight.to(orig_weight.device)\n    output_shape = updown.shape\n    if len(output_shape) == 4:\n        updown = updown.reshape(output_shape[0], output_shape[1] * output_shape[2] * output_shape[3])\n    updown = self.calc_updown(updown, orig_weight)\n    updown, ex_bias = self.finalize_updown(updown, orig_weight, output_shape, ex_bias)\n    return updown, ex_bias\n\n", "CodeBleu_score": 54.36422374507857}, {"code": "def calc_updown(self, orig_weight)\n\t\n    weight = self.weight.to(orig_weight.device, dtype=orig_weight.dtype)\n    bias = self.bias\n    if bias is not None:\n        bias = bias.to(orig_weight.device, dtype=orig_weight.dtype)\n\n    if self.dora_scale is not None:\n        weight = self.apply_weight_decompose(weight, orig_weight)\n\n    if self.finalize_updown is not None:\n        weight, bias = self.finalize_updown(weight, orig_weight, bias)\n\n    return weight, bias\n\n", "CodeBleu_score": 37.93444598448522}, {"code": "def calc_updown(self, orig_weight)\n\t\n    weight = self.weight.to(orig_weight.device, dtype=orig_weight.dtype)\n    bias = self.bias.to(orig_weight.device, dtype=weight.dtype) if self.bias is not None else None\n    ex_bias = self.ex_bias.to(orig_weight.device, dtype=weight.dtype) if self.ex_bias is not None else None\n    updown, ex_bias = self.finalize_updown(weight, orig_weight, bias, ex_bias)\n    self.ex_bias = ex_bias\n    return updown\n\n", "CodeBleu_score": 43.132995547476064}]}
{"instance_id": "1bb9e5d94af8", "generation_result": [{"code": "def purge_networks_from_memory()\n\t\n    pass\n\n", "CodeBleu_score": 0.8620689668435204}, {"code": "def purge_networks_from_memory()\n\t\n    pass\n\n", "CodeBleu_score": 0.8620689668435204}, {"code": "def purge_networks_from_memory()\n\t\n    pass\n\n", "CodeBleu_score": 0.8620689668435204}]}
{"instance_id": "42b48ebf4d07", "generation_result": [{"code": "def network_restore_weights_from_backup(self: Union[torch.nn.Conv2d, torch.nn.Linear, torch.nn.GroupNorm, torch.nn.LayerNorm, torch.nn.MultiheadAttention]):\n\t\n    if hasattr(self, \"backup_weights\"):\n        self.weight.data = self.backup_weights\n    if hasattr(self, \"backup_biases\"):\n        self.bias.data = self.backup_biases\n    if isinstance(self, torch.nn.MultiheadAttention):\n        if hasattr(self, \"backup_projection_weights\"):\n            self.q_proj.weight.data = self.backup_projection_weights\n            self.k_proj.weight.data = self.backup_projection_weights\n            self.v_proj.weight.data = self.backup_projection_weights\n        if hasattr(self, \"backup_projection_biases\"):\n            self.q_proj.bias.data = self.backup_projection_biases\n            self.k_proj.bias.data = self.backup_projection_biases\n            self.v_proj.bias.data = self.backup_projection_biases\n    if hasattr(self, \"backup_biases\") and self.bias is None:\n        self.bias = None\n\n\n\n", "CodeBleu_score": 36.390090867585734}, {"code": "def network_restore_weights_from_backup(self: Union[torch.nn.Conv2d, torch.nn.Linear, torch.nn.GroupNorm, torch.nn.LayerNorm, torch.nn.MultiheadAttention]):\n\t\n    if hasattr(self, \"backup_weights\"):\n        self.weight.data = self.backup_weights.data\n        if hasattr(self, \"backup_bias\"):\n            self.bias.data = self.backup_bias.data\n        else:\n            self.bias = None\n    elif hasattr(self, \"backup_projection_weights\"):\n        self.projection.weight.data = self.backup_projection_weights.data\n        if hasattr(self, \"backup_projection_bias\"):\n            self.projection.bias.data = self.backup_projection_bias.data\n        else:\n            self.projection.bias = None\n    else:\n        raise ValueError(\"No backup weights found for layer\")", "CodeBleu_score": 35.29446302704884}, {"code": "def network_restore_weights_from_backup(self: Union[torch.nn.Conv2d, torch.nn.Linear, torch.nn.GroupNorm, torch.nn.LayerNorm, torch.nn.MultiheadAttention]):\n\t\n    if self.weight_backup is not None:\n        self.weight.data = self.weight_backup\n    if self.bias_backup is not None:\n        self.bias.data = self.bias_backup\n    if isinstance(self, torch.nn.MultiheadAttention):\n        if self.q_proj_backup is not None:\n            self.q_proj.data = self.q_proj_backup\n        if self.k_proj_backup is not None:\n            self.k_proj.data = self.k_proj_backup\n        if self.v_proj_backup is not None:\n            self.v_proj.data = self.v_proj_backup\n        if self.out_proj_backup is not None:\n            self.out_proj.data = self.out_proj_backup\n        if self.in_proj_backup is not None:\n            self.in_proj.data = self.in_proj_backup\n    if self.weight_backup is None and self.bias_backup is None:\n        self.bias = None\n    return self\n\n\n\n", "CodeBleu_score": 36.74107539731182}]}
{"instance_id": "c8fcd357131a", "generation_result": [{"code": "def network_forward(org_module, input, original_forward)\n\t\n    if len(org_module.sd_module.networks) == 0:\n        return original_forward(input)\n\n    input = input.to(org_module.sd_module.device)\n    org_module.network_restore_weights_from_backup()\n    output = original_forward(input)\n    org_module.network_reset_cached_weight()\n    for module in org_module.sd_module.networks:\n        if module.sd_module.name in org_module.network_current_names:\n            continue\n        if module.sd_module.name in org_module.sd_module.current_layer:\n            output = module.forward(input, output)\n            org_module.network_current_names += (module.sd_module.name,)\n    return output\n\n", "CodeBleu_score": 42.746964579483645}, {"code": "def network_forward(org_module, input, original_forward)\n\t\n    if not hasattr(org_module, \"sd_module\"):\n        return original_forward(input)\n\n    if org_module.sd_module.network is None:\n        return original_forward(input)\n\n    if not org_module.sd_module.network.loaded:\n        return original_forward(input)\n\n    if not org_module.sd_module.network.loaded_networks:\n        return original_forward(input)\n\n    if not org_module.sd_module.network.loaded_networks[org_module.sd_module.network.current_name]:\n        return original_forward(input)\n\n    input = input.to(org_module.sd_module.network.device)\n    org_module.network_restore_weights_from_backup()\n    output = original_forward(input)\n\n    for name, network in org_module.sd_module.network.loaded_networks.items():\n        if name == org_module.sd_module.network.current_name:\n            output = network.forward(output)\n\n    org_module.network_reset_cached_weight()\n    return output\n\n", "CodeBleu_score": 33.67178639173962}, {"code": "def network_forward(org_module, input, original_forward)\n\t\n    if len(org_module.sd_module.loaded_networks) == 0:\n        return original_forward(org_module, input)\n\n    input = input.to(org_module.device)\n\n    org_module.network_restore_weights_from_backup()\n    org_module.network_reset_cached_weight()\n\n    output = original_forward(org_module, input)\n\n    for network in org_module.sd_module.loaded_networks:\n        if network.layer_name == org_module.name:\n            output = network.forward(org_module, input, output)\n\n    return output\n\n", "CodeBleu_score": 41.9474852589475}]}
{"instance_id": "1070211f5c11", "generation_result": [{"code": "def trainables(self)\n\t\n    trainables = []\n    for layer in self.layers:\n        if isinstance(layer, torch.nn.Linear) or isinstance(layer, torch.nn.LayerNorm):\n            trainables.extend(layer.parameters())\n    return trainables\n\n\n", "CodeBleu_score": 39.43275059085417}, {"code": "def trainables(self)\n\t\n    return [param for layer in self.layers for param in layer.parameters() if isinstance(layer, (torch.nn.Linear, torch.nn.LayerNorm))]\n\n\n", "CodeBleu_score": 18.764551609128798}, {"code": "def trainables(self)\n\t\n    return [p for p in self.parameters() if p.requires_grad]\n\n\n", "CodeBleu_score": 6.452447410697191}]}
{"instance_id": "0c3ba62c96a7", "generation_result": [{"code": "def load_hypernetworks(names, multipliers=None)\n\t\n    if multipliers is None:\n        multipliers = [1] * len(names)\n\n    hypernetworks = []\n\n    for name, multiplier in zip(names, multipliers):\n        hypernetwork = load_hypernetwork(name)\n\n        if hypernetwork is None:\n            return None\n\n        hypernetwork.set_multiplier(multiplier)\n        hypernetworks.append(hypernetwork)\n\n    shared.hypernetworks = hypernetworks\n\n    return hypernetworks\n\n", "CodeBleu_score": 29.39976325404788}, {"code": "def load_hypernetworks(names, multipliers=None)\n\t\n    if multipliers is None:\n        multipliers = [1.0] * len(names)\n\n    hypernetworks = []\n\n    for i, name in enumerate(names):\n        hypernetwork = load_hypernetwork(name)\n\n        if hypernetwork is not None:\n            hypernetwork.set_multiplier(multipliers[i])\n            hypernetworks.append(hypernetwork)\n\n    shared.hypernetworks = hypernetworks\n\n", "CodeBleu_score": 31.584498934652576}, {"code": "def load_hypernetworks(names, multipliers=None)\n\t\n    hypernetworks = []\n\n    for name, multiplier in zip(names, multipliers):\n        hypernetwork = load_hypernetwork(name)\n\n        if hypernetwork is None:\n            continue\n\n        hypernetwork.set_multiplier(multiplier)\n        hypernetworks.append(hypernetwork)\n\n    shared.hypernetworks = hypernetworks\n\n", "CodeBleu_score": 26.867831830438337}]}
{"instance_id": "bea3772ed532", "generation_result": [{"code": "\ndef apply_hypernetworks(hypernetworks, context, layer=None)\n\t\n    context_k = context\n    context_v = context\n    for hypernetwork in hypernetworks:\n        context_k, context_v = apply_single_hypernetwork(hypernetwork, context_k, context_v, layer=layer)\n    return context_k, context_v", "CodeBleu_score": 66.82264135585403}, {"code": "\ndef apply_hypernetworks(hypernetworks, context, layer=None)\n\t\n    context_k, context_v = context, context\n\n    for hypernetwork in hypernetworks:\n        context_k, context_v = apply_single_hypernetwork(hypernetwork, context_k, context_v, layer=layer)\n\n    return context_k, context_v\n\n", "CodeBleu_score": 56.435688315169486}, {"code": "\ndef apply_hypernetworks(hypernetworks, context, layer=None)\n\t\n    context_k, context_v = context\n    for hypernetwork in hypernetworks:\n        context_k, context_v = apply_single_hypernetwork(hypernetwork, context_k, context_v, layer)\n    return context_k, context_v\n\n", "CodeBleu_score": 70.49488199289185}]}
{"instance_id": "922efa3e092f", "generation_result": [{"code": "def attention_CrossAttention_forward(self, x, context=None, mask=None, **kwargs)\n\t\n    if context is None:\n        context = x\n\n    q = self.query(x)\n    k = self.key(context)\n    v = self.value(context)\n\n    if self.hypernetworks is not None:\n        k, v = apply_hypernetworks(self.hypernetworks, k, v, self.layer)\n\n    q = self.reshape_heads_to_batch_dim(q)\n    k = self.reshape_heads_to_batch_dim(k)\n    v = self.reshape_heads_to_batch_dim(v)\n\n    # compute scaled dot-product attention\n    attn = torch.einsum('b i d, b j d -> b i j', q, k) * self.scale\n    if mask is not None:\n        attn = attn.masked_fill(mask == 0, float('-inf'))\n\n    attn = torch.softmax(attn, dim=-1)\n    attn = self.attn_drop(attn)\n\n    # output projection\n    x = torch.einsum('b i j, b j d -> b i d', attn, v)\n    x = self.proj(x)\n    x = self.proj_drop(x)\n\n    return x\n\n", "CodeBleu_score": 44.92806542697071}, {"code": "def attention_CrossAttention_forward(self, x, context=None, mask=None, **kwargs)\n\t\n    query = self.query(x)\n    key = self.key(context)\n    value = self.value(context)\n\n    if self.hypernetworks:\n        key, value = apply_hypernetworks(self.hypernetworks, key, value)\n\n    query = query.reshape(query.shape[0], query.shape[1], self.num_heads, -1)\n    key = key.reshape(key.shape[0], key.shape[1], self.num_heads, -1)\n    value = value.reshape(value.shape[0], value.shape[1], self.num_heads, -1)\n\n    query = query.permute(0, 2, 1, 3)\n    key = key.permute(0, 2, 1, 3)\n    value = value.permute(0, 2, 1, 3)\n\n    query = query * (self.scale ** (1 / self.num_heads))\n\n    attention_scores = torch.matmul(query, key.transpose(-2, -1))\n\n    if mask is not None:\n        attention_scores = attention_scores.masked_fill(mask == 0, -1e9)\n\n    attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n\n    attention_probs = self.dropout(attention_probs)\n\n    context = torch.matmul(attention_probs, value)\n\n    context = context.permute(0, 2, 1, 3).reshape(context.shape[0], context.shape[1], -1)\n\n    output = self.out(context)\n\n    return output\n\n", "CodeBleu_score": 36.114892693321025}, {"code": "def attention_CrossAttention_forward(self, x, context=None, mask=None, **kwargs)\n\t\n    if context is None:\n        context = x\n\n    # Project query, key, and value\n    query = self.query_proj(x)\n    key = self.key_proj(context)\n    value = self.value_proj(context)\n\n    # Apply hypernetworks to context\n    if self.use_hypernetworks:\n        key, value = apply_hypernetworks(self.hypernetworks, key, value, layer=self.layer)\n\n    # Reshape for multi-head attention\n    query = query.reshape(query.shape[0], query.shape[1], self.heads, self.head_dim)\n    key = key.reshape(key.shape[0], key.shape[1], self.heads, self.head_dim)\n    value = value.reshape(value.shape[0], value.shape[1], self.heads, self.head_dim)\n\n    # Compute scaled dot-product attention\n    attention_scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.head_dim)\n    if mask is not None:\n        attention_scores = attention_scores.masked_fill(mask == 0, float('-inf'))\n    attention_probs = F.softmax(attention_scores, dim=-1)\n    attention_output = torch.matmul(attention_probs, value)\n\n    # Reshape back to original shape\n    attention_output = attention_output.reshape(attention_output.shape[0], attention_output.shape[1], -1)\n\n    # Pass through final linear layer\n    attention_output = self.out_proj(attention_output)\n\n    return attention_output\n\n", "CodeBleu_score": 37.35936829110809}]}
{"instance_id": "8b7455a03a0e", "generation_result": [{"code": "def save_hypernetwork(hypernetwork, checkpoint, hypernetwork_name, filename)\n\t\n    original_hypernetwork_attributes = hypernetwork.__dict__.copy()\n    hypernetwork.sd_checkpoint = checkpoint\n    hypernetwork.sd_checkpoint_name = hypernetwork_name\n    try:\n        hypernetwork.save(filename)\n    except Exception as e:\n        hypernetwork.__dict__ = original_hypernetwork_attributes\n        raise e\n    hypernetwork.__dict__ = original_hypernetwork_attributes\n\n", "CodeBleu_score": 26.56065382902606}, {"code": "def save_hypernetwork(hypernetwork, checkpoint, hypernetwork_name, filename)\n\t\n    hypernetwork_backup = deepcopy(hypernetwork)\n    hypernetwork.sd_checkpoint = checkpoint\n    hypernetwork.sd_checkpoint_name = hypernetwork_name\n    try:\n        hypernetwork.save(filename)\n    except Exception as e:\n        hypernetwork = hypernetwork_backup\n        raise e\n    hypernetwork = hypernetwork_backup\n", "CodeBleu_score": 20.2620561421489}, {"code": "def save_hypernetwork(hypernetwork, checkpoint, hypernetwork_name, filename)\n\t\n    hypernetwork_orig_attrs = {}\n    for attr in hypernetwork.__dict__:\n        hypernetwork_orig_attrs[attr] = getattr(hypernetwork, attr)\n\n    hypernetwork.sd_checkpoint = checkpoint\n    hypernetwork.sd_checkpoint_name = hypernetwork_name\n    try:\n        hypernetwork.save(filename)\n    except Exception as e:\n        for attr, value in hypernetwork_orig_attrs.items():\n            setattr(hypernetwork, attr, value)\n        raise e\n    finally:\n        for attr, value in hypernetwork_orig_attrs.items():\n            setattr(hypernetwork, attr, value)\n", "CodeBleu_score": 33.17383370875276}]}
{"instance_id": "3175f835140f", "generation_result": [{"code": "def ema_scope(self, context=None)\n\t\n    pass\n\n", "CodeBleu_score": 3.051106025934401}, {"code": "def ema_scope(self, context=None)\n\t\n    pass\n\n", "CodeBleu_score": 3.051106025934401}, {"code": "def ema_scope(self, context=None)\n\t\n    pass\n\n", "CodeBleu_score": 3.051106025934401}]}
{"instance_id": "a7b5d5cd519a", "generation_result": [{"code": "def p_mean_variance(self, x, t, clip_denoised: bool)\n\t\n    model_output = self.model(x, t)\n    if self.model_var_type in [\"learned\", \"learned_range\"]:\n        learned_variance = self.get_learned_variance(x, t)\n        model_variance, model_log_variance = learned_variance, torch.log(learned_variance)\n    else:\n        model_variance, model_log_variance = (\n            self.get_model_variance(x, t=t, **self.model_kwargs),\n            self.get_model_log_variance(x, t=t, **self.model_kwargs),\n        )\n    if self.rescale_timesteps:\n        t = t.float()\n        # this formula is from the original paper appendix C\n        model_variance, model_log_variance = model_variance * (t[..., None] ** 2), model_log_variance * t\n\n    if self.model_var_type in [\"fixedlarge\", \"fixedsmall\"]:\n        model_log_variance = model_log_variance.expand(x.shape)\n        model_variance = torch.exp(model_log_variance)\n\n    if self.rescale_learned_sigma:\n        model_variance, model_log_variance = model_variance * self.rescaled_variance, model_log_variance * self.rescaled_variance_clipped\n\n    if self.rescale_timesteps:\n        t = t.float()\n        # this formula is from the original paper appendix C\n        model_variance, model_log_variance = model_variance * (t[..., None] ** 2), model_log_variance * t\n\n    if self.rescale_learned_sigma:\n        model_variance, model_log_variance = model_variance * self.rescaled_variance, model_log_variance * self.rescaled_variance_clipped\n\n    if self.rescale_timesteps:\n        t = t.float()\n        # this formula is from", "CodeBleu_score": 31.101977034494876}, {"code": "def p_mean_variance(self, x, t, clip_denoised: bool)\n\t\n    model_output = self.model(x, t)\n    if self.model_var_type in [\"learned\", \"learned_range\"]:\n        h = self.get_model_var(x, t)\n        model_var = torch.tensor(np.maximum(h, 1e-30))\n    elif self.model_var_type == \"fixedlarge\":\n        model_var = self.get_model_var(x, t)\n    elif self.model_var_type == \"fixedsmall\":\n        model_var = self.get_model_var(x, t)\n    else:\n        raise NotImplementedError(self.model_var_type)\n    posterior_mean, posterior_variance, posterior_log_variance = self.q_posterior(x_start=x, x_t=model_output, t=t)\n    if clip_denoised:\n        x_recon = self.predict_start_from_noise(x_t=model_output, t=t, noise=model_output)\n        posterior_mean = torch.clamp(posterior_mean, min=-1, max=1)\n    else:\n        x_recon = model_output\n    return posterior_mean, posterior_variance, posterior_log_variance\n\n", "CodeBleu_score": 46.85355239029031}, {"code": "def p_mean_variance(self, x, t, clip_denoised: bool)\n\t\n    model_output = self.predict_start_from_noise(x, t=t, noise=torch.zeros_like(x))\n    if clip_denoised:\n        return model_output, torch.zeros_like(x), torch.zeros_like(x)\n    else:\n        posterior_mean, posterior_variance, posterior_log_variance = self.q_posterior(\n            x_start=x, x_t=model_output, t=t\n        )\n        return posterior_mean, posterior_variance, posterior_log_variance\n\n", "CodeBleu_score": 43.306529731322605}]}
{"instance_id": "a8652fa78473", "generation_result": [{"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss_type == 'l1':\n        if mean:\n            return torch.mean(torch.abs(pred - target))\n        else:\n            return torch.abs(pred - target)\n    elif self.loss_type == 'l2':\n        if mean:\n            return torch.mean((pred - target) ** 2)\n        else:\n            return (pred - target) ** 2\n    else:\n        raise ValueError('Unknown loss type: {}'.format(self.loss_type))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 33.96004812577799}, {"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss_type == \"l1\":\n        loss = torch.abs(pred - target)\n        if mean:\n            loss = torch.mean(loss)\n        return loss\n    elif self.loss_type == \"l2\":\n        loss = torch.nn.MSELoss(reduction=\"none\")(pred, target)\n        if mean:\n            loss = torch.mean(loss)\n        return loss\n    else:\n        raise ValueError(\"Unknown loss type: {}\".format(self.loss_type))", "CodeBleu_score": 48.11472544964835}, {"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss_type == \"l1\":\n        loss = torch.abs(pred - target)\n        if mean:\n            return loss.mean()\n        else:\n            return loss\n    elif self.loss_type == \"l2\":\n        loss = torch.pow(pred - target, 2)\n        if mean:\n            return loss.mean()\n        else:\n            return loss\n    else:\n        raise ValueError(\"Unknown loss type: {}\".format(self.loss_type))", "CodeBleu_score": 39.097945037049584}]}
{"instance_id": "d68e4f061349", "generation_result": [{"code": "def _prior_bpd(self, x_start)\n\t\n    batch_size = x_start.shape[0]\n    t = torch.ones((batch_size,), device=x_start.device, dtype=torch.long)  # Define t tensor of ones\n    mean, _, log_variance = self.q_mean_variance(x_start, t)\n    return 0.5 * mean.pow(2).sum(dim=(1, 2, 3)) / self.num_channels_x + \\\n           0.5 * log_variance.sum(dim=(1, 2, 3))\n\n", "CodeBleu_score": 30.544975496271576}, {"code": "def _prior_bpd(self, x_start)\n\t\n    batch_size = x_start.shape[0]\n    t = torch.tensor([self.num_timesteps - 1] * batch_size, device=x_start.device, dtype=torch.long)\n    mean, log_variance = self.q_mean_variance(x_start, t)\n    return -0.5 * (log_variance.sum(dim=list(range(1, len(log_variance.shape)))) + torch.numel(x_start) * math.log(2 * math.pi))\n\n", "CodeBleu_score": 37.280108857727114}, {"code": "def _prior_bpd(self, x_start)\n\t\n    batch_size = x_start.shape[0]\n    t = torch.ones((batch_size,), device=x_start.device, dtype=torch.long)\n    _, _, log_variance = self.q_mean_variance(x_start, t)\n    return -0.5 * (log_variance.sum(dim=1) + np.log(2 * np.pi))\n\n", "CodeBleu_score": 26.645958046873208}]}
{"instance_id": "c695d93fdf2d", "generation_result": [{"code": "def make_cache(subsection: str) -> diskcache.Cache\n\t\n    return diskcache.Cache(\n        cache_dir=f\"{os.environ['CACHE_DIR']}/{subsection}\",\n        size_limit=4e9,\n        disk_min_file_size=256e3,\n    )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 47.12477001112312}, {"code": "def make_cache(subsection: str) -> diskcache.Cache\n\t\n    cache_dir = os.path.join(os.path.expanduser(\"~\"), \".cache\", \"openai_cache\")\n    os.makedirs(cache_dir, exist_ok=True)\n    cache_path = os.path.join(cache_dir, subsection)\n    return diskcache.Cache(\n        cache_path,\n        size_limit=4 * 1024 * 1024 * 1024,\n        disk_min_file_size=256 * 1024,\n        disk_cache=True,\n    )\n\n", "CodeBleu_score": 52.463277458980095}, {"code": "def make_cache(subsection: str) -> diskcache.Cache\n\t\n    cache_dir = os.path.join(os.getcwd(), \"cache\")\n    if not os.path.exists(cache_dir):\n        os.makedirs(cache_dir)\n    cache = diskcache.Cache(\n        os.path.join(cache_dir, subsection),\n        size_limit=4e9,\n        disk_min_file_size=256e3,\n    )\n    return cache\n\n", "CodeBleu_score": 53.131976924254985}]}
{"instance_id": "ed4c543f0d38", "generation_result": [{"code": "def cache(subsection)\n\t\n    cache_obj = caches.get(subsection)\n    if cache_obj is None:\n        with cache_lock:\n            if not os.path.exists(cache_dir):\n                os.makedirs(cache_dir)\n            if not os.path.isfile(cache_filename):\n                convert_old_cached_data()\n            cache_obj = caches.get(subsection)\n            if cache_obj is None:\n                cache_obj = make_cache(subsection)\n                caches[subsection] = cache_obj\n    return cache_obj\n\n", "CodeBleu_score": 67.25921240957332}, {"code": "def cache(subsection)\n\t\n    cache_obj = caches.get(subsection)\n    if cache_obj is None:\n        with cache_lock:\n            if subsection not in caches:\n                convert_old_cached_data()\n                cache_obj = caches.get(subsection)\n                if cache_obj is None:\n                    cache_obj = make_cache(subsection)\n                    caches[subsection] = cache_obj\n    return cache_obj\n\n", "CodeBleu_score": 58.83158865227974}, {"code": "def cache(subsection)\n\t\n    cache_obj = caches.get(subsection)\n    if cache_obj is None:\n        with cache_lock:\n            if not os.path.exists(cache_dir):\n                os.makedirs(cache_dir)\n            if not os.path.isfile(cache_filename):\n                convert_old_cached_data()\n            cache_obj = caches.get(subsection)\n            if cache_obj is None:\n                cache_obj = make_cache(subsection)\n                caches[subsection] = cache_obj\n    return cache_obj", "CodeBleu_score": 67.25921240957332}]}
{"instance_id": "ccf5ea2d9fc1", "generation_result": [{"code": "def cached_data_for_file(subsection, title, filename, func)\n\t\n\n    cache_obj = cache(subsection)\n    mtime = os.stat(filename).st_mtime\n    cached = cache_obj.get(title)\n    if cached:\n        if cached[\"mtime\"] < mtime:\n            cache_obj.delete(title)\n            cached = None\n    if not cached:\n        cached = func()\n        cache_obj[title] = {\"mtime\": mtime, \"data\": cached}\n        dump_cache()\n    return cached[\"data\"]\n\n", "CodeBleu_score": 32.15847775808723}, {"code": "def cached_data_for_file(subsection, title, filename, func)\n\t\n\n    cache_obj = cache(subsection)\n\n    mtime = os.stat(filename).st_mtime\n\n    if title in cache_obj:\n        if cache_obj[title][\"mtime\"] > mtime:\n            del cache_obj[title]\n\n    if title not in cache_obj:\n        cache_obj[title] = {\"mtime\": mtime, \"data\": func()}\n        dump_cache()\n\n    return cache_obj[title][\"data\"]\n\n", "CodeBleu_score": 22.29185497255425}, {"code": "def cached_data_for_file(subsection, title, filename, func)\n\t\n\n    cache_obj = cache(subsection)\n    mtime = os.path.getmtime(filename)\n    if title in cache_obj:\n        cached_mtime = cache_obj[title][\"mtime\"]\n        if cached_mtime < mtime:\n            cache_obj.pop(title)\n\n    if title not in cache_obj:\n        data = func()\n        cache_obj[title] = {\"mtime\": mtime, \"data\": data}\n        dump_cache()\n        return data\n\n    return cache_obj[title][\"data\"]\n\n", "CodeBleu_score": 32.22264502490313}]}
{"instance_id": "e01a2d4f0ac2", "generation_result": [{"code": "def report(message: str, *, exc_info: bool = False) -> None\n\t\n    record_exception()\n    print(f\"*** {message}\", file=sys.stderr)\n    if exc_info:\n        traceback.print_exc(file=sys.stderr)\n        print(\"---\", file=sys.stderr)", "CodeBleu_score": 40.35401246483239}, {"code": "def report(message: str, *, exc_info: bool = False) -> None\n\t\n    record_exception()\n    print(f\"*** {message}\", file=sys.stderr)\n    if exc_info:\n        print(f\"--- {''.join(exception_records[-1])}\", file=sys.stderr)", "CodeBleu_score": 32.88897634754063}, {"code": "def report(message: str, *, exc_info: bool = False) -> None\n\t\n    record_exception()\n    for line in message.splitlines():\n        print(\"***\", line, file=sys.stderr)\n    if exc_info:\n        traceback.print_exc(file=sys.stderr)\n        print(\"---\", file=sys.stderr)\n\n", "CodeBleu_score": 63.65335006274419}]}
{"instance_id": "125fb9d2320b", "generation_result": [{"code": "def extract_original_prompts(style: PromptStyle, prompt, negative_prompt)\n\t\n\n    if style.prompt == \"\" and style.negative_prompt == \"\":\n        return False, prompt, negative_prompt\n\n    style_text = style.prompt\n    if style_text != \"\":\n        is_extracted, prompt = extract_style_text_from_prompt(style_text, prompt)\n        if not is_extracted:\n            return False, prompt, negative_prompt\n\n    style_text = style.negative_prompt\n    if style_text != \"\":\n        is_extracted, negative_prompt = extract_style_text_from_prompt(style_text, negative_prompt)\n        if not is_extracted:\n            return False, prompt, negative_prompt\n\n    return True, prompt, negative_prompt\n\n", "CodeBleu_score": 56.46668528943757}, {"code": "def extract_original_prompts(style: PromptStyle, prompt, negative_prompt)\n\t\n\n    if style.prompt == \"\" and style.negative_prompt == \"\":\n        return False, prompt, negative_prompt\n\n    positive_extracted, positive_prompt = extract_style_text_from_prompt(style.prompt, prompt)\n    negative_extracted, negative_prompt = extract_style_text_from_prompt(style.negative_prompt, negative_prompt)\n\n    if not positive_extracted or not negative_extracted:\n        return False, prompt, negative_prompt\n\n    return True, positive_prompt, negative_prompt\n\n", "CodeBleu_score": 60.96746702579501}, {"code": "def extract_original_prompts(style: PromptStyle, prompt, negative_prompt)\n\t\n\n    if style.prompt == \"\" and style.negative_prompt == \"\":\n        return False, prompt, negative_prompt\n\n    positive_extracted, positive_prompt = extract_style_text_from_prompt(style.prompt, prompt)\n    negative_extracted, negative_prompt = extract_style_text_from_prompt(style.negative_prompt, negative_prompt)\n\n    if positive_extracted and negative_extracted:\n        return True, positive_prompt, negative_prompt\n\n    return False, prompt, negative_prompt\n\n", "CodeBleu_score": 55.19615277225692}]}
{"instance_id": "0c5f729fbce2", "generation_result": [{"code": "def random_partial(self, count, n_frames)\n\t\n    if self.utterances is None:\n        self._load_utterances()\n\n    utterances = self.utterance_cycler.sample(count)\n\n    a = [(u,) + u.random_partial(n_frames) for u in utterances]\n\n    return a\n\n", "CodeBleu_score": 94.20475781398312}, {"code": "def random_partial(self, count, n_frames)\n\t\n    if self.utterances is None:\n        self._load_utterances()\n\n    utterances = self.utterance_cycler.sample(count)\n\n    a = [(u,) + u.random_partial(n_frames) for u in utterances]\n\n    return a\n\n", "CodeBleu_score": 94.20475781398312}, {"code": "def random_partial(self, count, n_frames)\n\t\n    if self.utterances is None:\n        self._load_utterances()\n\n    utterances = self.utterance_cycler.sample(count)\n\n    a = [(u,) + u.random_partial(n_frames) for u in utterances]\n\n    return a\n\n", "CodeBleu_score": 94.20475781398312}]}
{"instance_id": "31e8b27b8f94", "generation_result": [{"code": "def normalize_volume(wav, target_dBFS, increase_only=False, decrease_only=False)\n\t\n    if increase_only and decrease_only:\n        raise ValueError(\"Both increase_only and decrease_only cannot be True.\")\n    dBFS_change = target_dBFS - 10 * np.log10(np.mean(wav ** 2))\n    if dBFS_change < 0 and increase_only:\n        return wav\n    elif dBFS_change > 0 and decrease_only:\n        return wav\n    return wav * 10 ** (dBFS_change / 20)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 71.71474370817255}, {"code": "def normalize_volume(wav, target_dBFS, increase_only=False, decrease_only=False)\n\t\n    if increase_only and decrease_only:\n        raise ValueError(\"Both increase_only and decrease_only cannot be set to True.\")\n    dBFS_change = target_dBFS - wav.dBFS\n    if (dBFS_change < 0 and increase_only) or (dBFS_change > 0 and decrease_only):\n        return wav\n    return wav.apply_gain(dBFS_change)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 55.876291776903045}, {"code": "def normalize_volume(wav, target_dBFS, increase_only=False, decrease_only=False)\n\t\n    if increase_only and decrease_only:\n        raise ValueError(\"Both increase_only and decrease_only cannot be set to True\")\n    dBFS_change = target_dBFS - _db_level(wav)\n    if dBFS_change < 0 and increase_only or dBFS_change > 0 and decrease_only:\n        return wav\n    return wav * 10 ** (dBFS_change / 20)\n\n", "CodeBleu_score": 50.8661748758997}]}
{"instance_id": "2353831e9033", "generation_result": [{"code": "def load_model(weights_fpath: Path, device=None)\n\t\n    global _model\n    _model = SpeakerEncoder(device=device)\n    checkpoint = torch.load(weights_fpath, map_location='cpu')\n    _model.load_state_dict(checkpoint['model_state'])\n    _model.eval()\n    print('Loaded pretrained model from epoch %d' % checkpoint['epoch'])\n\n\n", "CodeBleu_score": 18.467313316119345}, {"code": "def load_model(weights_fpath: Path, device=None)\n\t\n    global _model\n    if _model is None:\n        _model = SpeakerEncoder(device=device)\n    _model.eval()\n    _model.load_state_dict(torch.load(weights_fpath, map_location=device))\n    print(\"Loaded pretrained model from '{}'\".format(weights_fpath))\n\n", "CodeBleu_score": 23.191271284953718}, {"code": "def load_model(weights_fpath: Path, device=None)\n\t\n    global _model\n    if _model is None:\n        _model = SpeakerEncoder(device=device)\n        checkpoint = torch.load(weights_fpath)\n        _model.load_state_dict(checkpoint[\"model_state\"])\n        _model.eval()\n        print(\"Loaded encoder \\\"%s\\\" trained to step %d\" % (weights_fpath.name, checkpoint[\"step\"]))\n    return _model\n\n", "CodeBleu_score": 46.1589116369392}]}
{"instance_id": "3231ad480199", "generation_result": [{"code": "def embed_frames_batch(frames_batch)\n\t\n    if not _model:\n        raise Exception('Model not loaded. Call load_model() first.')\n    frames_batch = torch.from_numpy(frames_batch).to(_device)\n    embeds = _model.forward(frames_batch).detach().cpu().numpy()\n    return embeds\n\n", "CodeBleu_score": 66.52415190071316}, {"code": "def embed_frames_batch(frames_batch)\n\t\n    if _model is None:\n        raise Exception(\"Model not loaded\")\n    frames_batch = torch.from_numpy(frames_batch).to(_device)\n    embeds = _model.forward(frames_batch).detach().cpu().numpy()\n    return embeds\n\n", "CodeBleu_score": 69.54748106382162}, {"code": "def embed_frames_batch(frames_batch)\n\t\n    if _model is None:\n        raise Exception(\"Model not loaded\")\n    frames_batch = torch.from_numpy(frames_batch).to(_device)\n    embeds = _model.forward(frames_batch).detach().cpu().numpy()\n    return embeds\n\n", "CodeBleu_score": 69.54748106382162}]}
{"instance_id": "117a41716c8d", "generation_result": [{"code": "def compute_partial_slices(n_samples, partial_utterance_n_frames=partials_n_frames,\n                           min_pad_coverage=0.75, overlap=0.5)\n\t\n    assert 0 <= min_pad_coverage <= 1\n    assert 0 < overlap < 1\n\n    frame_step = int(np.ceil(partial_utterance_n_frames * (1 - overlap)))\n    n_frames_total = frame_step * (n_samples // frame_step)\n    n_pad = (n_samples - n_frames_total)\n\n    # Pad\n    assert 0 <= n_pad < frame_step\n    if n_pad > 0:\n        mel_pad = np.tile(mel_spectrogram[:, 0:1], (1, n_pad))\n        spec_pad = np.tile(spectrogram[:, 0:1], (1, n_pad))\n        wav_pad = np.tile(wav[0:1], (n_pad,))\n    else:\n        mel_pad, spec_pad, wav_pad = None, None, None\n\n    # Compute slices\n    mel_slices, spec_slices, wav_slices = [], [], []\n    mel_curr_end, spec_curr_end, wav_curr_end = 0, 0, 0\n    while mel_curr_end < mel_spectrogram.shape[1]:\n        # Trim mel input if necessary\n        mel_curr_start = mel_curr_end\n        mel_curr_end = min(mel_curr_start + partial_utterance_n_frames, mel_spectrogram.shape[1])\n        mel_slices += [(mel_curr_start, mel_curr_end)]\n        spec_curr_start = mel_curr_start\n        spec_curr_end = min(spec_curr_start + partial_utterance_n_frames, spectrogram.shape[1])\n        spec_slices += [(spec_curr_start, spec_curr_end)]\n        wav_curr_start = spec_curr_start\n        wav_curr_end = min(wav_curr_start + partial_utterance_n_frames, wav.", "CodeBleu_score": 32.42896570326196}, {"code": "def compute_partial_slices(n_samples, partial_utterance_n_frames=partials_n_frames,\n                           min_pad_coverage=0.75, overlap=0.5)\n\t\n    assert 0 <= overlap <= 1, 'Invalid value for overlap'\n    assert 0 <= min_pad_coverage <= 1, 'Invalid value for min_pad_coverage'\n    assert partial_utterance_n_frames > 0, 'Invalid value for partial_utterance_n_frames'\n\n    # Compute how many frames will result in (N + partial_utterance_n_frames) frames after padding\n    total_frames = int(np.ceil(n_samples / hparams.hop_size))\n    frame_step = max(int(hparams.hop_size / (1 - overlap)), 1)\n    n_frames = int(frame_step * np.ceil(total_frames / frame_step))\n\n    # Compute required padding\n    pad = (n_frames - total_frames) if total_frames < n_frames else 0\n    pad_left = pad // 2\n    pad_right = pad - pad_left\n\n    # Split padding into left, right and both\n    pad_left_effect = pad_left if pad_left == pad_right else pad_left + 1\n    pad_right_effect = pad - pad_left_effect\n    pad_both_effect = pad if pad_left < pad_right else pad + 1\n\n    # Split into wav and mel slices\n    wav_slices, mel_slices = [], []\n    mel_frames = hparams.n_mel_channels * hparams.mel_window_step / hparams.hop_size\n    for i in range(0, total_frames - partial_utterance_n_frames + 1, frame_step):\n        mel_start = int(i / frame_step)\n        mel_end = int((i + partial_utterance_n_frames * mel_frames) / frame_step) + 1\n        wav_start = int(i + pad_left_effect)\n        wav_end = int((i + pad_right_effect + partial_utterance_n_frames * mel_frames) / frame_step) + 1\n        wav_slices += [(wav_start, wav_end)]\n        mel_slices += [", "CodeBleu_score": 33.4906558563989}, {"code": "def compute_partial_slices(n_samples, partial_utterance_n_frames=partials_n_frames,\n                           min_pad_coverage=0.75, overlap=0.5)\n\t\n\n    # Calculate the number of frames and frame step based on the sampling rate and overlap\n    frame_step = int(np.round(partial_utterance_n_frames * (1 - overlap)))\n    n_frames_total = partial_utterance_n_frames\n    while frame_step < (n_samples - n_frames_total):\n        n_frames_total += frame_step\n\n    # Compute the start and end sample for the current partial utterance\n    start_sample = (n_frames_total - partial_utterance_n_frames) * hparams.hop_size\n    end_sample = start_sample + n_frames_total * hparams.hop_size\n\n    # Compute mel and wav slices\n    mel_slices, wav_slices = [], []\n    mel_frames = range(0, n_frames_total, frame_step)\n    for mel_start, mel_end in zip(mel_frames[:-1], mel_frames[1:]):\n        wav_start = start_sample + mel_start * hparams.hop_size\n        wav_end = start_sample + mel_end * hparams.hop_size\n        mel_slices.append((mel_start, mel_end))\n        wav_slices.append((wav_start, wav_end))\n\n    # Ensure we have at least one frame\n    assert len(mel_slices) >= 1, \"Failed to compute any mel slices\"\n    assert len(wav_slices) >= 1, \"Failed to compute any wav slices\"\n\n    # Ensure all frame lengths are multiples of hop_size\n    frames_diff = np.array(wav_slices)[:, 1] - np.array(wav_slices)[:, 0]\n    assert (frames_diff % hparams.hop_size == 0).all(), \\\n        \"Computed frame lengths are not multiples of hop_size\"\n\n    # Add the last partial utterance if its size is less than the overlap size\n    last_wav_end = wav_slices[-1][1]\n    if last_wav_", "CodeBleu_score": 32.05479974421653}]}
{"instance_id": "34fe5c864f1c", "generation_result": [{"code": "def plot_embedding_as_heatmap(embed, ax=None, title=\"\", shape=None, color_range=(0, 0.30))\n\t\n    import matplotlib.pyplot as plt\n    import numpy as np\n\n    if ax is None:\n        ax = plt.gca()\n    if shape is None:\n        shape = (int(np.sqrt(embed.shape[0])), int(np.sqrt(embed.shape[0])))\n    embed = embed.reshape(shape)\n    im = ax.imshow(embed, cmap=plt.cm.hot)\n    plt.colorbar(im, ax=ax)\n    ax.set_title(title)\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.set_aspect('equal')\n    plt.clim(color_range[0], color_range[1])\n    return ax\n\n\n", "CodeBleu_score": 56.88766489974961}, {"code": "def plot_embedding_as_heatmap(embed, ax=None, title=\"\", shape=None, color_range=(0, 0.30))\n\t\n    if ax is None:\n        ax = plt.gca()\n    if shape is None:\n        shape = (int(np.sqrt(len(embed))), -1)\n    ax.imshow(embed.reshape(*shape), cmap=plt.cm.bwr, vmin=color_range[0], vmax=color_range[1])\n    ax.set_title(title)\n    ax.set_xticks([])\n    ax.set_yticks([])\n    plt.colorbar(ax=ax)\n\n\n", "CodeBleu_score": 41.21543930851907}, {"code": "def plot_embedding_as_heatmap(embed, ax=None, title=\"\", shape=None, color_range=(0, 0.30))\n\t\n    if ax is None:\n        ax = plt.gca()\n    if shape is None:\n        shape = (int(np.sqrt(embed.shape[0])), -1)\n    embed = embed.reshape(shape)\n    im = ax.imshow(embed, cmap=plt.cm.get_cmap('RdBu', 100))\n    plt.colorbar(im, ax=ax)\n    plt.setp(ax, xticks=[], yticks=[])\n    ax.set_title(title)\n    plt.xlim(0, shape[1])\n    plt.ylim(0, shape[0])\n    plt.clim(color_range[0], color_range[1])\n\n\n", "CodeBleu_score": 53.25775836903367}]}
{"instance_id": "92e93d50b62b", "generation_result": [{"code": "def similarity_matrix(self, embeds)\n\t\n    batch_size, num_speakers, num_utterances, _ = embeds.shape\n    centroids_incl = torch.zeros(batch_size, num_speakers, embeds.shape[-1], device=self.device)\n    centroids_excl = torch.zeros(batch_size, num_speakers, embeds.shape[-1], device=self.device)\n    for i in range(num_speakers):\n        centroids_incl[:, i, :] = torch.mean(embeds[:, i, :, :], dim=1)\n        centroids_excl[:, i, :] = torch.mean(torch.cat([embeds[:, :i, :, :], embeds[:, i + 1:, :, :]], dim=1), dim=1)\n    centroids_incl = F.normalize(centroids_incl, dim=-1)\n    centroids_excl = F.normalize(centroids_excl, dim=-1)\n    sim_matrix = torch.zeros(batch_size, num_speakers, num_speakers, device=self.device)\n    mask = torch.ones(num_speakers, num_speakers, dtype=torch.bool, device=self.device)\n    mask = torch.tril(mask, diagonal=-1)\n    for i in range(num_speakers):\n        for j in range(num_speakers):\n            if mask[i, j]:\n                sim_matrix[:, i, j] = torch.sum(embeds[:, i, :, :] * embeds[:, j, :, :], dim=-1)\n                sim_matrix[:, j, i] = sim_matrix[:, i, j]\n    sim_matrix = sim_matrix.clamp(min=-1, max=1)\n    sim_matrix = (sim_matrix + 1) / 2\n    sim_matrix = sim_matrix * self.scale + self.bias\n    return sim_matrix\n\n\n", "CodeBleu_score": 34.09525361671385}, {"code": "def similarity_matrix(self, embeds)\n\t\n    n_speakers = embeds.shape[0]\n    n_utts = embeds.shape[1]\n    centroids_incl = embeds.sum(dim=1) / n_utts\n    centroids_incl = F.normalize(centroids_incl, p=2, dim=1)\n    centroids_excl = embeds.sum(dim=1, keepdim=True)\n    centroids_excl = centroids_excl.repeat(1, n_utts)\n    centroids_excl = centroids_excl.view(n_speakers * n_utts, -1)\n    centroids_excl = centroids_excl[~torch.eye(n_speakers * n_utts, dtype=torch.bool, device=embeds.device)]\n    centroids_excl = centroids_excl.view(n_speakers, n_utts - 1, -1)\n    centroids_excl = centroids_excl.sum(dim=1) / (n_utts - 1)\n    centroids_excl = F.normalize(centroids_excl, p=2, dim=1)\n    sim_matrix = torch.zeros(n_speakers, n_speakers, device=embeds.device)\n    mask = torch.ones(n_speakers, n_speakers, dtype=torch.bool, device=embeds.device)\n    mask = mask.fill_diagonal_(0)\n    for i in range(n_speakers):\n        sim_matrix[i] = torch.matmul(centroids_incl[i], centroids_incl.T)\n        sim_matrix[i] = sim_matrix[i] + torch.matmul(centroids_excl[i], centroids_excl.T)\n        sim_matrix[i] = sim_matrix[i] / 2\n    sim_matrix = sim_matrix * 100\n    sim_matrix = sim_matrix - torch.min(sim_matrix)\n", "CodeBleu_score": 28.2184414132781}, {"code": "def similarity_matrix(self, embeds)\n\t\n    num_speakers = embeds.shape[0]\n    num_utterances = embeds.shape[1]\n    centroids_incl = torch.zeros((num_speakers, embeds.shape[-1]), device=embeds.device)\n    centroids_excl = torch.zeros((num_speakers, embeds.shape[-1]), device=embeds.device)\n    for i in range(num_speakers):\n        centroids_incl[i] = (embeds[i].sum(dim=0) / num_utterances)\n        centroids_incl[i] /= torch.norm(centroids_incl[i])\n        centroids_excl[i] = (embeds[i].sum(dim=0) - embeds[i][i]) / (num_utterances - 1)\n        centroids_excl[i] /= torch.norm(centroids_excl[i])\n    sim_mat = torch.zeros((num_speakers, num_speakers), device=embeds.device)\n    mask = torch.ones((num_speakers, num_speakers), device=embeds.device)\n    for i in range(num_speakers):\n        for j in range(num_speakers):\n            if i == j:\n                mask[i][j] = 0\n            else:\n                sim_mat[i][j] = torch.dot(centroids_incl[i], centroids_excl[j])\n    sim_mat = sim_mat * 100\n    sim_mat = sim_mat + torch.eye(num_speakers, device=embeds.device) * 100\n    sim_mat = sim_mat * mask\n    sim_mat = sim_mat / torch.max(sim_mat)\n    return sim_mat\n\n\n", "CodeBleu_score": 32.57528185052316}]}
{"instance_id": "a6eba54108a1", "generation_result": [{"code": "def loss(self, embeds)\n\t\n    speakers_per_batch, utterances_per_speaker = embeds.shape[:2]\n    # Compute similarity matrix\n    sim_matrix = self.similarity_matrix(embeds)\n    sim_matrix = sim_matrix.reshape((speakers_per_batch, -1))\n\n    # Ground truth labels\n    gt_labels = np.zeros((speakers_per_batch, speakers_per_batch))\n    gt_labels[np.triu_indices(speakers_per_batch, k=1)] = 1\n    gt_labels = gt_labels.reshape((speakers_per_batch, -1))\n\n    # Loss\n    loss = self.loss_fn(sim_matrix, gt_labels)\n\n    # EER\n    with torch.no_grad():\n        pred_labels = np.zeros((speakers_per_batch, speakers_per_batch))\n        pred_labels[np.triu_indices(speakers_per_batch, k=1)] = torch.sigmoid(sim_matrix).detach().cpu().numpy()\n        pred_labels = pred_labels.reshape((speakers_per_batch, -1))\n        fpr, tpr, thresholds = roc_curve(gt_labels.flatten(), pred_labels.flatten())\n        fnr = 1 - tpr\n        eer = fnr[np.nanargmin(np.absolute((fnr - fpr)))]\n\n    return loss, eer\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 45.58724872426691}, {"code": "def loss(self, embeds)\n\t\n    # Determine the number of speakers and utterances per speaker from the shape of the input embeddings\n    speakers_per_batch, utterances_per_speaker = embeds.shape[:2]\n    \n    # Compute the similarity matrix\n    sim_matrix = self.similarity_matrix(embeds)\n    \n    # Reshape the similarity matrix to a 1D vector\n    sim_matrix = sim_matrix.reshape(speakers_per_batch * utterances_per_speaker, -1)\n    \n    # Generate ground truth labels\n    gt_labels = torch.zeros(speakers_per_batch * utterances_per_speaker,\n                            speakers_per_batch * utterances_per_speaker).to(self.loss_device)\n    for i in range(speakers_per_batch):\n        for j in range(utterances_per_speaker):\n            gt_labels[i * utterances_per_speaker + j, i * utterances_per_speaker + j] = 1\n    \n    # Compute the loss\n    loss = self.loss_fn(sim_matrix, gt_labels)\n    \n    # Compute the EER\n    with torch.no_grad():\n        pred_labels = torch.argmax(sim_matrix, dim=1)\n        fpr, tpr, thresholds = roc_curve(gt_labels.reshape(-1).cpu().numpy(), pred_labels.cpu().numpy(), pos_label=1)\n        fnr = 1 - tpr\n        eer = fpr[np.nanargmin(np.absolute((fnr - fpr)))]\n    \n    return loss, eer\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 47.66709578500081}, {"code": "def loss(self, embeds)\n\t\n    speakers_per_batch, utterances_per_speaker = embeds.shape[:2]\n    # Compute the similarity matrix\n    sim_matrix = self.similarity_matrix(embeds)\n    sim_matrix = sim_matrix.reshape(speakers_per_batch, -1)\n    # Generate ground truth labels\n    labels = torch.cat([torch.arange(speakers_per_batch) for _ in range(utterances_per_speaker)], dim=0)\n    # Compute the loss\n    loss = self.loss_fn(sim_matrix, labels)\n    # Compute the EER\n    with torch.no_grad():\n        # Get the predicted labels\n        pred_labels = torch.argmax(sim_matrix, dim=1)\n        # Compute the confusion matrix\n        conf_matrix = torch.zeros(speakers_per_batch, speakers_per_batch).to(self.loss_device)\n        for i in range(speakers_per_batch):\n            for j in range(speakers_per_batch):\n                conf_matrix[i, j] = torch.sum(pred_labels == i)\n        # Compute the false positive rate and false negative rate\n        fpr = conf_matrix.sum(dim=0) / conf_matrix.sum()\n        fnr = conf_matrix.sum(dim=1) / conf_matrix.sum()\n        # Find the threshold where the false positive rate equals the false negative rate\n        eer = torch.where(fpr == fnr)[0][0] / speakers_per_batch\n    return loss, eer\n\n", "CodeBleu_score": 44.20837644021257}]}
{"instance_id": "ac7b74b9d1b1", "generation_result": [{"code": "def log_params(self)\n\t\n    import config\n    import inspect\n    import rich\n    import rich.tree\n    import rich.syntax\n    import rich.console\n    import textwrap\n    from rich.tree import Tree\n    from rich.console import Console\n    from rich.syntax import Syntax\n    from rich import print\n    from rich.panel import Panel\n    import pandas as pd\n    import numpy as np\n    import os\n    import sys\n    import inspect\n    import re\n    import logging\n    import config\n    import os\n    import sys\n    import inspect\n    import re\n    import logging\n    import config\n    import os\n    import sys\n    import inspect\n    import re\n    import logging\n    import config\n    import os\n    import sys\n    import inspect\n    import re\n    import logging\n    import config\n    import os\n    import sys\n    import inspect\n    import re\n    import logging\n    import config\n    import os\n    import sys\n    import inspect\n    import re\n    import logging\n    import config\n    import os\n    import sys\n    import inspect\n    import re\n    import logging\n    import config\n    import os\n    import sys\n    import inspect\n    import re\n    import logging\n    import config\n    import os\n    import sys\n    import inspect\n    import re\n    import logging\n    import config\n    import os\n    import sys\n    import inspect\n    import re\n    import logging\n    import config\n    import os\n    import sys\n    import inspect\n    import re\n    import logging\n    import config\n    import os\n    import sys\n    import inspect\n    import re\n    import logging\n    import config\n    import os\n    import sys\n    import inspect\n    import re\n    import logging\n    import config\n    import os\n    import sys\n    import inspect\n    import re\n    import logging\n    import config\n    import os\n    import sys\n    import inspect\n    import re\n    import logging\n    import config\n    import os\n    import sys\n    import inspect\n    import re\n    import logging\n    import config\n    import os\n    import sys\n    import inspect\n    import re\n    import logging\n    import config\n    import os\n    import sys\n    import inspect\n   ", "CodeBleu_score": 8.916994131895443}, {"code": "def log_params(self)\n\t\n    import config\n    import json\n    from rich.console import Console\n    from rich.table import Table\n    from rich.panel import Panel\n    from rich.markdown import Markdown\n    from rich.text import Text\n    from rich.syntax import Syntax\n    from rich.theme import Theme\n    from rich.padding import Padding\n    from rich.panel import Panel\n    from rich.columns import Columns\n    from rich.markdown import Markdown\n    from rich.text import Text\n    from rich.syntax import Syntax\n    from rich.theme import Theme\n    from rich.padding import Padding\n    from rich.panel import Panel\n    from rich.columns import Columns\n    from rich.markdown import Markdown\n    from rich.text import Text\n    from rich.syntax import Syntax\n    from rich.theme import Theme\n    from rich.padding import Padding\n    from rich.panel import Panel\n    from rich.columns import Columns\n    from rich.markdown import Markdown\n    from rich.text import Text\n    from rich.syntax import Syntax\n    from rich.theme import Theme\n    from rich.padding import Padding\n    from rich.panel import Panel\n    from rich.columns import Columns\n    from rich.markdown import Markdown\n    from rich.text import Text\n    from rich.syntax import Syntax\n    from rich.theme import Theme\n    from rich.padding import Padding\n    from rich.panel import Panel\n    from rich.columns import Columns\n    from rich.markdown import Markdown\n    from rich.text import Text\n    from rich.syntax import Syntax\n    from rich.theme import Theme\n    from rich.padding import Padding\n    from rich.panel import Panel\n    from rich.columns import Columns\n    from rich.markdown import Markdown\n    from rich.text import Text\n    from rich.syntax import Syntax\n    from rich.theme import Theme\n    from rich.padding import Padding\n    from rich.panel import Panel\n    from rich.columns import Columns\n    from rich.markdown import Markdown\n    from rich.text import Text\n    from rich.syntax import Syntax\n    from rich.theme import Theme\n    from rich.padding import Padding\n", "CodeBleu_score": 11.222643235919941}, {"code": "def log_params(self)\n\t\n    import config\n    from rich.console import Console\n    from rich.table import Table\n    from rich.panel import Panel\n    from rich.layout import Layout\n    from rich.padding import Padding\n    from rich.text import Text\n    from rich.syntax import Syntax\n    from rich.box import SQUARE\n    from rich.console import Console\n    from rich.panel import Panel\n    from rich.layout import Layout\n    from rich.padding import Padding\n    from rich.text import Text\n    from rich.syntax import Syntax\n    from rich.box import SQUARE\n    from rich.console import Console\n    from rich.panel import Panel\n    from rich.layout import Layout\n    from rich.padding import Padding\n    from rich.text import Text\n    from rich.syntax import Syntax\n    from rich.box import SQUARE\n    from rich.console import Console\n    from rich.panel import Panel\n    from rich.layout import Layout\n    from rich.padding import Padding\n    from rich.text import Text\n    from rich.syntax import Syntax\n    from rich.box import SQUARE\n    from rich.console import Console\n    from rich.panel import Panel\n    from rich.layout import Layout\n    from rich.padding import Padding\n    from rich.text import Text\n    from rich.syntax import Syntax\n    from rich.box import SQUARE\n    from rich.console import Console\n    from rich.panel import Panel\n    from rich.layout import Layout\n    from rich.padding import Padding\n    from rich.text import Text\n    from rich.syntax import Syntax\n    from rich.box import SQUARE\n    from rich.console import Console\n    from rich.panel import Panel\n    from rich.layout import Layout\n    from rich.padding import Padding\n    from rich.text import Text\n    from rich.syntax import Syntax\n    from rich.box import SQUARE\n    from rich.console import Console\n    from rich.panel import Panel\n    from rich.layout import Layout\n    from rich.padding import Padding\n    from rich.text import Text\n    from rich.syntax import Syntax\n    from rich.box import SQUARE\n    from rich.console import Console\n    from rich", "CodeBleu_score": 11.105402351785497}]}
{"instance_id": "4726b7864386", "generation_result": [{"code": "def draw_projections(self, embeds, utterances_per_speaker, step, out_fpath=None, max_speakers=10)\n\t\n    if self.draw_projections_enabled:\n        if self.draw_projections_interval > 0 and step % self.draw_projections_interval != 0:\n            return\n        if max_speakers > 0:\n            embeds = embeds[:max_speakers]\n            utterances_per_speaker = utterances_per_speaker[:max_speakers]\n        embeds = np.concatenate(embeds)\n        utterances_per_speaker = np.concatenate(utterances_per_speaker)\n        if self.draw_projections_umap_reducer is None:\n            self.draw_projections_umap_reducer = umap.UMAP(n_neighbors=self.draw_projections_umap_n_neighbors, min_dist=self.draw_projections_umap_min_dist, n_components=self.draw_projections_umap_n_components, metric=self.draw_projections_umap_metric)\n        projections = self.draw_projections_umap_reducer.fit_transform(embeds)\n        plt.clf()\n        plt.title(f'step {step}')\n        plt.scatter(projections[:, 0], projections[:, 1], c=utterances_per_speaker, s=5, cmap='hsv')\n        plt.colorbar()\n        if out_fpath is not None:\n            plt.savefig(out_fpath)\n        else:\n            plt.show(block=self.draw_projections_block)\n\n\n", "CodeBleu_score": 39.70138147950719}, {"code": "def draw_projections(self, embeds, utterances_per_speaker, step, out_fpath=None, max_speakers=10)\n\t\n    if self.projections is None:\n        self.projections = {}\n    if self.projections.get(step) is not None:\n        return\n    self.projections[step] = {}\n    for speaker_id, speaker_embeds in embeds.items():\n        if len(speaker_embeds) > max_speakers:\n            speaker_embeds = speaker_embeds[:max_speakers]\n        self.projections[step][speaker_id] = self.umap_projection(speaker_embeds)\n    if out_fpath is not None:\n        self.save_projections(out_fpath)\n    if not self.visualize:\n        return\n    fig = plt.figure(figsize=(10, 10))\n    ax = fig.add_subplot(111)\n    ax.set_title(f'Speaker embeddings at step {step}')\n    for speaker_id, speaker_embeds in embeds.items():\n        if len(speaker_embeds) > max_speakers:\n            continue\n        proj = self.projections[step][speaker_id]\n        ax.scatter(proj[:, 0], proj[:, 1], label=speaker_id)\n    ax.legend()\n    plt.show()\n\n\n", "CodeBleu_score": 35.69934057652357}, {"code": "def draw_projections(self, embeds, utterances_per_speaker, step, out_fpath=None, max_speakers=10)\n\t\n    if not self.draw_projections_enabled:\n        return\n    if len(embeds) > max_speakers:\n        embeds = embeds[:max_speakers]\n    embeds = np.concatenate(embeds, axis=0)\n    embeds = np.reshape(embeds, (-1, embeds.shape[2]))\n    colors = np.tile(np.array(self.colors), (len(embeds) // len(self.colors) + 1, 1))[:len(embeds)]\n    projections = self.umap_reducer.fit_transform(embeds)\n    fig = plt.figure(figsize=(10, 10))\n    ax = fig.add_subplot(111)\n    ax.scatter(projections[:, 0], projections[:, 1], c=colors, s=20)\n    plt.title(f'Step {step}')\n    plt.axis('off')\n    if out_fpath:\n        plt.savefig(out_fpath)\n    else:\n        plt.show()\n    plt.close()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 44.1022857334065}]}
{"instance_id": "274fa50985c0", "generation_result": [{"code": "def add_speaker_embedding(self, x, speaker_embedding)\n\t\n    x_dim = x.dim()\n    speaker_embedding_dim = speaker_embedding.dim()\n    if x_dim == 3:\n        x_dim_name = '(B, T, C)'\n        speaker_embedding_dim_name = '(B, C)'\n        speaker_embedding_dim_index = 1\n    elif x_dim == 4:\n        x_dim_name = '(B, C, H, W)'\n        speaker_embedding_dim_name = '(B, C)'\n        speaker_embedding_dim_index = 1\n    elif x_dim == 5:\n        x_dim_name = '(B, T, C, H, W)'\n        speaker_embedding_dim_name = '(B, C)'\n        speaker_embedding_dim_index = 2\n    else:\n        raise ValueError(f'Unsupported input tensor dimension {x_dim} for adding speaker embedding.')\n    if speaker_embedding_dim != speaker_embedding_dim_index:\n        raise ValueError(f'Unsupported speaker embedding dimension {speaker_embedding_dim} for adding speaker embedding. Expected {speaker_embedding_dim_name}.')\n    x_len = x.size(1)\n    speaker_embedding = speaker_embedding.unsqueeze(1).expand(x.size(0), x_len, -1)\n    speaker_embedding = speaker_embedding.contiguous().view(x.size(0), x_len, -1)\n    speaker_embedding = speaker_embedding.transpose(1, 2)\n    return torch.cat([x, speaker_embedding], dim=-1)", "CodeBleu_score": 32.34867417584755}, {"code": "def add_speaker_embedding(self, x, speaker_embedding)\n\t\n    input_dims = x.shape\n    input_dims = list(input_dims)\n    input_dims.append(1)\n    input_dims = tuple(input_dims)\n    speaker_embedding = tf.expand_dims(speaker_embedding, axis=0)\n    speaker_embedding = tf.repeat(speaker_embedding, repeats=input_dims[0], axis=0)\n    speaker_embedding = tf.reshape(speaker_embedding, input_dims)\n    speaker_embedding = tf.transpose(speaker_embedding, perm=[0, 2, 1])\n    x = tf.concat([x, speaker_embedding], axis=-1)\n    return x\n\n\n", "CodeBleu_score": 18.893136549924446}, {"code": "def add_speaker_embedding(self, x, speaker_embedding)\n\t\n    x_shape = x.shape\n    x_shape_len = len(x_shape)\n    speaker_embedding_shape = speaker_embedding.shape\n    speaker_embedding_shape_len = len(speaker_embedding_shape)\n    if speaker_embedding_shape_len == 1:\n        speaker_embedding = speaker_embedding.unsqueeze(1)\n    if x_shape_len == 2:\n        x = x.unsqueeze(1)\n    elif x_shape_len == 3:\n        x = x.unsqueeze(1)\n        x = x.expand(-1, speaker_embedding_shape[0], -1)\n    elif x_shape_len == 4:\n        x = x.unsqueeze(1)\n        x = x.expand(-1, speaker_embedding_shape[0], -1, -1)\n    elif x_shape_len == 5:\n        x = x.unsqueeze(1)\n        x = x.expand(-1, speaker_embedding_shape[0], -1, -1, -1)\n    elif x_shape_len == 6:\n        x = x.unsqueeze(1)\n        x = x.expand(-1, speaker_embedding_shape[0], -1, -1, -1, -1)\n    elif x_shape_len == 7:\n        x = x.unsqueeze(1)\n        x = x.expand(-1, speaker_embedding_shape[0], -1, -1, -1, -1, -1)\n    elif x_shape_len == 8:\n        x = x.unsqueeze(1)\n        x = x.expand(-1, speaker_embedding_shape[0], -1, -1, -1, -1, -1, -1)\n    elif x_shape_len == 9:\n        x = x.unsqueeze(1)\n        x = x.expand(-1, speaker_embedding_shape[0], -1, -1, -1, -1, -1, -1, -1)\n    elif x_shape_len == 10", "CodeBleu_score": 10.09764858478898}]}
{"instance_id": "e6742ea0fe5e", "generation_result": [{"code": "def init_attention(self, encoder_seq_proj)\n\t\n    device = next(self.parameters()).device\n    batch_size, seq_len, _ = encoder_seq_proj.size()\n    self.cumulative_attn = torch.zeros(batch_size, seq_len, device=device)\n    self.attn = torch.zeros(batch_size, seq_len, device=device)\n\n\n\n", "CodeBleu_score": 24.467603008266043}, {"code": "def init_attention(self, encoder_seq_proj)\n\t\n    device = next(self.parameters()).device\n    batch_size, seq_len = encoder_seq_proj.shape[0], encoder_seq_proj.shape[1]\n    self.cumulative = torch.zeros(batch_size, seq_len).to(device)\n    self.attention = torch.zeros(batch_size, seq_len).to(device)\n\n\n", "CodeBleu_score": 22.986690426727133}, {"code": "def init_attention(self, encoder_seq_proj)\n\t\n    device = next(self.parameters()).device\n    batch_size, seq_len = encoder_seq_proj.size(0), encoder_seq_proj.size(1)\n    self.cum_attn = torch.zeros(batch_size, seq_len, device=device)\n    self.attn = torch.zeros(batch_size, seq_len, device=device)\n\n\n", "CodeBleu_score": 23.306130602278362}]}
{"instance_id": "4e1fc1ed0ed7", "generation_result": [{"code": "def load(self, path, optimizer=None)\n\t\n    # Use device of model params as location for loaded state\n    device = next(self.parameters()).device\n    checkpoint = torch.load(str(path), map_location=device)\n    self.load_state_dict(checkpoint[\"model_state\"])\n\n    if \"optimizer_state\" in checkpoint and optimizer is not None:\n        optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n\n", "CodeBleu_score": 67.98839425729693}, {"code": "def load(self, path, optimizer=None)\n\t\n    # Use device of model params as location for loaded state\n    device = next(self.parameters()).device\n    checkpoint = torch.load(str(path), map_location=device)\n    self.load_state_dict(checkpoint[\"model_state\"])\n\n    if \"optimizer_state\" in checkpoint and optimizer is not None:\n        optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n\n", "CodeBleu_score": 67.98839425729693}, {"code": "def load(self, path, optimizer=None)\n\t\n    # Use device of model params as location for loaded state\n    device = next(self.parameters()).device\n    checkpoint = torch.load(str(path), map_location=device)\n    self.load_state_dict(checkpoint[\"model_state\"])\n\n    if \"optimizer_state\" in checkpoint and optimizer is not None:\n        optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n\n", "CodeBleu_score": 67.98839425729693}]}
{"instance_id": "178e6aa16c0f", "generation_result": [{"code": "def save(self, path, optimizer=None)\n\t\n    if optimizer is not None:\n        torch.save({\n            \"model_state\": self.state_dict(),\n            \"optimizer_state\": optimizer.state_dict(),\n        }, str(path))\n    else:\n        torch.save({\n            \"model_state\": self.state_dict(),\n        }, str(path))\n\n", "CodeBleu_score": 93.4714524643972}, {"code": "def save(self, path, optimizer=None)\n\t\n    if optimizer is not None:\n        torch.save({\n            \"model_state\": self.state_dict(),\n            \"optimizer_state\": optimizer.state_dict(),\n        }, str(path))\n    else:\n        torch.save({\n            \"model_state\": self.state_dict(),\n        }, str(path))\n\n", "CodeBleu_score": 93.4714524643972}, {"code": "def save(self, path, optimizer=None)\n\t\n    if optimizer is not None:\n        torch.save({\n            \"model_state\": self.state_dict(),\n            \"optimizer_state\": optimizer.state_dict(),\n        }, str(path))\n    else:\n        torch.save({\n            \"model_state\": self.state_dict(),\n        }, str(path))\n\n", "CodeBleu_score": 93.4714524643972}]}
{"instance_id": "6528fd80e87b", "generation_result": [{"code": "def english_cleaners(text)\n\t\n    text = convert_to_ascii(text)\n    text = lowercase(text)\n    text = expand_numbers(text)\n    text = expand_abbreviations(text)\n    text = collapse_whitespace(text)\n    return text\n", "CodeBleu_score": 55.88611500274403}, {"code": "def english_cleaners(text)\n\t\n    text = convert_to_ascii(text)\n    text = lowercase(text)\n    text = expand_numbers(text)\n    text = expand_abbreviations(text)\n    text = collapse_whitespace(text)\n    return text\n", "CodeBleu_score": 55.88611500274403}, {"code": "def english_cleaners(text)\n\t\n    text = convert_to_ascii(text)\n    text = lowercase(text)\n    text = expand_numbers(text)\n    text = expand_abbreviations(text)\n    text = collapse_whitespace(text)\n    return text\n", "CodeBleu_score": 55.88611500274403}]}
{"instance_id": "8a4fe08e0985", "generation_result": [{"code": "def text_to_sequence(text, cleaner_names)\n\t\n    sequence = []\n\n    # Check for curly braces and treat their contents as ARPAbet:\n    while \"{\" in text:\n        try:\n            i = text.index(\"{\")\n            # A weird bug in the regex module causes the\n            # first call to findall to return the wrong\n            # thing.\n            if not sequence:\n                text_pieces = re.split(r\"([\\{\\}\\[\\]\\(\\)])\", text[:i])\n            else:\n                text_pieces = re.split(r\"([\\{\\}\\[\\]\\(\\)])\", text)\n            sequence += _symbols_to_sequence(text_pieces[0])\n            sequence += _arpabet_to_sequence(text_pieces[1])\n            text = text[i + len(text_pieces[1]) + 2 :]\n        except:\n            pass\n\n    sequence += _symbols_to_sequence(_clean_text(text, cleaner_names))\n\n    return sequence\n", "CodeBleu_score": 38.812237021016045}, {"code": "def text_to_sequence(text, cleaner_names)\n\t\n    sequence = []\n\n    # Check for curly braces and treat their contents as ARPAbet:\n    while \"{\" in text:\n        try:\n            i = text.index(\"{\")\n            if text.count(\"{\") != text.count(\"}\"):\n                raise ValueError(\"Mismatched brace.\")\n            pattern = re.compile(\"(.*?)(\\\\{.+?\\\\}.*?)\")\n            m = pattern.match(text)\n            pre_curly, curly, post_curly = m.groups()\n            curly_text = re.sub(\"[^A-Z]\", \" \", curly)\n            curly_sequence = _arpabet_to_sequence(curly_text)\n            sequence += _symbols_to_sequence(pre_curly)\n            sequence += curly_sequence\n            sequence += _symbols_to_sequence(post_curly)\n            text = post_curly\n        except:\n            print(\"Error parsing text: \", text)\n            return None\n\n    # Clean symbols using the cleaners\n    text = _clean_text(text, cleaner_names)\n\n    # Extract all symbols from the text\n    sequence += _symbols_to_sequence(text)\n    return sequence\n", "CodeBleu_score": 28.758401170905568}, {"code": "def text_to_sequence(text, cleaner_names)\n\t\n    sequence = []\n\n    # Check for curly braces and treat their contents as ARPAbet:\n    while \"{\" in text:\n        try:\n            i = text.index(\"{\")\n            j = text.index(\"}\")\n            if j <= i:\n                raise ValueError(\"Unmatched brace.\")\n            arpabet = text[i + 1 : j]\n            sequence += _arpabet_to_sequence(arpabet)\n            text = text[:i] + text[j + 1 :]\n        except ValueError:\n            raise ValueError(\"Unmatched brace.\")\n\n    # Process the remaining text:\n    sequence += _symbols_to_sequence(_clean_text(text, cleaner_names))\n\n    return sequence\n", "CodeBleu_score": 36.57514380450155}]}
{"instance_id": "e1c25a37af1f", "generation_result": [{"code": "def sequence_to_text(sequence)\n\t\n    result = ''\n    for symbol_id in sequence:\n        if symbol_id in _arpabet:\n            result += '{%s}' % _arpabet[symbol_id]\n        else:\n            result += _id_to_symbol[symbol_id]\n    return result.replace('}{', ' ')\n\n", "CodeBleu_score": 34.53102794547504}, {"code": "def sequence_to_text(sequence)\n\t\n    result = ''\n    for symbol_id in sequence:\n        if symbol_id in _arpabet:\n            result += _arpabet[symbol_id] + ' '\n        else:\n            result += symbol_id + ' '\n    return result.strip()\n\n\n", "CodeBleu_score": 28.77864092171479}, {"code": "def sequence_to_text(sequence)\n\t\n    result = ''\n    for symbol_id in sequence:\n        if symbol_id in ARPABET:\n            result += '{' + ARPABET[symbol_id] + '} '\n        else:\n            result += ARPABET[symbol_id] + ' '\n    result = result.strip()\n    result = re.sub(r' { }', ' ', result)\n    result = re.sub(r' \\{ ', '{', result)\n    result = re.sub(r' \\} ', '}', result)\n    return result.strip()\n\n\n\n\n", "CodeBleu_score": 33.45688141391723}]}
{"instance_id": "7807cad4b05d", "generation_result": [{"code": "def embed_utterance(fpaths, encoder_model_fpath)\n\t\n    if not encoder.is_loaded():\n        encoder.load_model(encoder_model_fpath)\n\n    # Compute the speaker embedding of the utterance\n    wav_fpath, embed_fpath = fpaths\n    wav = np.load(wav_fpath)\n    wav = encoder.preprocess_wav(wav)\n    embed = encoder.embed_utterance(wav)\n    np.save(embed_fpath, embed, allow_pickle=False)\n\n", "CodeBleu_score": 95.3261531719067}, {"code": "def embed_utterance(fpaths, encoder_model_fpath)\n\t\n    if not encoder.is_loaded():\n        encoder.load_model(encoder_model_fpath)\n\n    # Compute the speaker embedding of the utterance\n    wav_fpath, embed_fpath = fpaths\n    wav = np.load(wav_fpath)\n    wav = encoder.preprocess_wav(wav)\n    embed = encoder.embed_utterance(wav)\n    np.save(embed_fpath, embed, allow_pickle=False)\n\n", "CodeBleu_score": 95.3261531719067}, {"code": "def embed_utterance(fpaths, encoder_model_fpath)\n\t\n    if not encoder.is_loaded():\n        encoder.load_model(encoder_model_fpath)\n\n    # Compute the speaker embedding of the utterance\n    wav_fpath, embed_fpath = fpaths\n    wav = np.load(wav_fpath)\n    wav = encoder.preprocess_wav(wav)\n    embed = encoder.embed_utterance(wav)\n    np.save(embed_fpath, embed, allow_pickle=False)\n\n", "CodeBleu_score": 95.3261531719067}]}
{"instance_id": "9f860273a325", "generation_result": [{"code": "def create_embeddings(synthesizer_root: Path, encoder_model_fpath: Path, n_processes: int)\n\t\n    # Check if the encoder model file exists\n    if not encoder_model_fpath.exists():\n        raise Exception(\"File '%s' with encoder model does not exist\" % encoder_model_fpath)\n\n    # Check if the metadata file exists\n    metadata_fpath = synthesizer_root / \"train.txt\"\n    if not metadata_fpath.exists():\n        raise Exception(\"File '%s' with metadata does not exist\" % metadata_fpath)\n\n    # Check if the output embedding directory exists\n    embedding_dir = synthesizer_root / \"embed\"\n    if not embedding_dir.exists():\n        embedding_dir.mkdir(parents=True)\n\n    # Read metadata\n    metadata = read_metadata(metadata_fpath)\n\n    # Prepare the list of audio files\n    audio_fpaths = [str(synthesizer_root / \"audio\" / m[0]) for m in metadata]\n\n    # Create the embedding generator\n    embedding_generator = EmbeddingGenerator(encoder_model_fpath=encoder_model_fpath)\n\n    # Create a multiprocessing pool and process the audio files\n    embedding_fpaths = [str(embedding_dir / f\"{fpath.split('/')[-1].replace('.wav', '.npy')}\") for fpath in audio_fpaths]\n    with Pool(n_processes) as pool:\n        list(tqdm(pool.imap(embedding_generator.embed_utterance, audio_fpaths), total=len(audio_fpaths), file=sys.stdout))\n\n    # Save the embeddings to numpy arrays\n    for fpath, embed in zip(embedding_fpaths, embedding_generator.embeddings):\n        np.save(fpath, embed, allow_pickle=False)\n\n\n", "CodeBleu_score": 38.74686700615564}, {"code": "def create_embeddings(synthesizer_root: Path, encoder_model_fpath: Path, n_processes: int)\n\t\n    # Verify encoder architecture & checkpoint existence:\n    encoder.load_model(encoder_model_fpath)\n    # Prepare the encoder and synthesizer:\n    encoder.embed_utterance(np.zeros(encoder.sampling_rate))\n    synthesizer = Synthesizer(synthesizer_root)\n    # Preprocess the dataset:\n    metadata = encoder.preprocess_metadata(synthesizer_root, encoder_model_fpath,\n                                           n_processes,\n                                           require_preemphasis=True)\n    # Create the embedding files:\n    embedding_dir = synthesizer_root.joinpath(\"embeddings\")\n    embedding_dir.mkdir(exist_ok=True)\n    # Process the corpus with a pool of workers:\n    with Pool(n_processes) as pool:\n        for i, embedding_fpaths in tqdm(enumerate(pool.imap(partial(\n                encoder.embed_utterance,\n                encoder_model_fpath,\n                synthesizer_root,\n                \"wavs\",\n                \"embeddings\",\n                encoder.sampling_rate,\n                progress_bar=False,\n                embed_utt_fpath=None), metadata)),\n                                       total=len(metadata),\n                                       file=sys.stdout):\n            # Print a message:\n            print(\"Created embeddings for %s\" % (i + 1))\n\n\n\n\n", "CodeBleu_score": 29.25466774495084}, {"code": "def create_embeddings(synthesizer_root: Path, encoder_model_fpath: Path, n_processes: int)\n\t\n    # Verify encoder model path.\n    if not encoder_model_fpath.exists():\n        raise Exception(\"Encoder model file does not exist: %s\" % encoder_model_fpath)\n\n    # Verify output directories.\n    out_dir = synthesizer_root / \"embeds\"\n    out_dir.mkdir(exist_ok=True)\n\n    # Get dataset.\n    metadata_fpath = synthesizer_root / \"train.txt\"\n    metadata = _read_metadata(metadata_fpath)\n\n    # Create the embedding generator.\n    embedding_generator = EmbeddingGenerator(metadata, encoder_model_fpath)\n\n    # Create a multiprocessing pool and generate embeddings.\n    embedding_fpaths_gen = embedding_generator.generate(n_processes)\n\n    # Create a progress bar.\n    total_num_embeddings = len(metadata)\n    progress_bar = tqdm(embedding_fpaths_gen, total=total_num_embeddings)\n\n    # Iterate over embeddings and write them to disk.\n    for embedding_fpath in progress_bar:\n        embedding_fpath.parent.mkdir(exist_ok=True)\n        embedding_fpath.write_bytes(embedding_generator.embedding_data[embedding_fpath.name])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 33.84479579413287}]}
{"instance_id": "9eb0abda14a9", "generation_result": [{"code": "def eval_model(attention, mel_prediction, target_spectrogram, input_seq, step,\n               plot_dir, mel_output_dir, wav_dir, sample_num, loss, hparams)\n\t\n    # Get the predicted mel spectrogram\n    mel_prediction = mel_prediction.detach().cpu().numpy()\n\n    # Get the target mel spectrogram\n    target_spectrogram = target_spectrogram.detach().cpu().numpy()\n\n    # Get the attention plot\n    attention = attention.detach().cpu().numpy()\n\n    # Get the input sequence\n    input_seq = input_seq.detach().cpu().numpy()\n\n    # Convert the input sequence to text\n    input_seq = sequence_to_text(input_seq)\n\n    # Get the current time\n    time = time_string()\n\n    # Save the attention plot\n    save_attention(attention, f\"{plot_dir}/{time}_step{step}_sample{sample_num}_attention\")\n\n    # Save the predicted mel spectrogram\n    np.save(f\"{mel_output_dir}/{time}_step{step}_sample{sample_num}_mel_prediction\", mel_prediction)\n\n    # Save the target mel spectrogram\n    np.save(f\"{mel_output_dir}/{time}_step{step}_sample{sample_num}_mel_target\", target_spectrogram)\n\n    # Save the input sequence\n    with open(f\"{mel_output_dir}/{time}_step{step}_sample{sample_num}_input_sequence.txt\", \"w\") as f:\n        f.write(input_seq)\n\n    # Generate and save the plot comparing the predicted and target mel spectrograms\n    plot_spectrogram(mel_prediction, f\"{plot_dir}/{time}_step{step}_sample{sample_num}_mel_prediction\",\n                     f\"{time} Step {step} Sample {sample_num} Loss {loss:.4f}\", split_title=True,\n                     target_spectrogram=target_spectrogram, max_len=hparams.max_iters)\n\n    # Print the input sequence\n    print(f\"Input Sequence: {input_seq}\")\n\n", "CodeBleu_score": 29.680763191654663}, {"code": "def eval_model(attention, mel_prediction, target_spectrogram, input_seq, step,\n               plot_dir, mel_output_dir, wav_dir, sample_num, loss, hparams)\n\t\n    # Save attention plot\n    save_attention(attention, os.path.join(plot_dir, f\"step_{step}_sample_{sample_num}_attention\"))\n\n    # Save predicted mel spectrogram\n    save_mel_spectrogram(mel_prediction, os.path.join(mel_output_dir, f\"step_{step}_sample_{sample_num}.npy\"))\n\n    # Save reconstructed waveform\n    wav = spectrogram_to_waveform(mel_prediction)\n    save_wav(wav, os.path.join(wav_dir, f\"step_{step}_sample_{sample_num}.wav\"))\n\n    # Save plot comparing predicted and target mel spectrograms\n    plot_spectrogram(mel_prediction, os.path.join(plot_dir, f\"step_{step}_sample_{sample_num}_mel_prediction\"),\n                     title=f\"{hparams.model_type} - Step {step} - Loss: {loss:.4f}\",\n                     split_title=True,\n                     target_spectrogram=target_spectrogram,\n                     max_len=hparams.max_len,\n                     auto_aspect=True)\n\n    # Print input sequence\n    input_seq = input_seq.cpu().numpy()\n    input_seq = input_seq.reshape(input_seq.shape[0], input_seq.shape[1] * input_seq.shape[2])\n    input_seq = input_seq[0]\n    input_seq = input_seq.astype(int)\n    input_seq = input_seq[input_seq != 0]\n    input_seq = input_seq.tolist()\n    input_seq = sequence_to_text(input_seq)\n    print(f\"Input sequence: {input_seq}\")\n\n", "CodeBleu_score": 28.166331524713108}, {"code": "def eval_model(attention, mel_prediction, target_spectrogram, input_seq, step,\n               plot_dir, mel_output_dir, wav_dir, sample_num, loss, hparams)\n\t\n\n    # Save attention plot\n    if attention is not None:\n        save_attention(attention, os.path.join(plot_dir, \"attention\", f\"{sample_num}_step{step}\"))\n\n    # Save predicted mel spectrogram\n    if mel_prediction is not None:\n        save_mel_spectrogram(mel_prediction, os.path.join(mel_output_dir, f\"{sample_num}_step{step}.npy\"))\n\n    # Save reconstructed waveform\n    if mel_prediction is not None:\n        save_waveform(mel_prediction, os.path.join(wav_dir, f\"{sample_num}_step{step}.wav\"), hparams)\n\n    # Generate and save plot comparing predicted and target mel spectrograms\n    if mel_prediction is not None and target_spectrogram is not None:\n        plot_spectrogram(mel_prediction, os.path.join(plot_dir, \"mel\", f\"{sample_num}_step{step}.png\"),\n                         title=f\"{hparams.model_type} | {time_string()} | Step {step} | Loss {loss:.4f}\",\n                         target_spectrogram=target_spectrogram, max_len=hparams.max_iters)\n\n    # Print input sequence\n    print(\"Input:\", sequence_to_text(input_seq))\n\n", "CodeBleu_score": 29.798887226263528}]}
{"instance_id": "6bad4f500286", "generation_result": [{"code": "def num_params(self)\n\t\n    params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n    print(f\"{params * 4 / 1024 / 1024:0.3f}M\")\n\n\n\n", "CodeBleu_score": 11.059104632439109}, {"code": "def num_params(self)\n\t\n    num_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n    num_params = num_params / 1e6\n    print(f\"{num_params:.3f} million parameters\")", "CodeBleu_score": 16.002565254506962}, {"code": "def num_params(self)\n\t\n    total_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n    print(f\"{total_params / 1000000:.3f}M parameters\")\n\n", "CodeBleu_score": 12.839916145414243}]}
{"instance_id": "78b876054bd4", "generation_result": [{"code": "def pad_tensor(self, x, pad, side='both')\n\t\n    if side == 'both':\n        x = torch.cat([torch.zeros(pad, x.shape[1], x.shape[2]).to(x.device), x, torch.zeros(pad, x.shape[1], x.shape[2]).to(x.device)], dim=0)\n    elif side == 'before':\n        x = torch.cat([torch.zeros(pad, x.shape[1], x.shape[2]).to(x.device), x], dim=0)\n    elif side == 'after':\n        x = torch.cat([x, torch.zeros(pad, x.shape[1], x.shape[2]).to(x.device)], dim=0)\n    else:\n        raise ValueError('Invalid side argument. Must be one of \"before\", \"after\", or \"both\".')\n    return x\n\n\n", "CodeBleu_score": 30.34293199306409}, {"code": "def pad_tensor(self, x, pad, side='both')\n\t\n    if pad == 0:\n        return x\n    if side == 'both':\n        x = torch.nn.functional.pad(x, (pad, pad))\n    elif side == 'before':\n        x = torch.nn.functional.pad(x, (pad, 0))\n    elif side == 'after':\n        x = torch.nn.functional.pad(x, (0, pad))\n    else:\n        raise ValueError('side must be one of \"before\", \"after\", or \"both\"')\n    return x\n\n", "CodeBleu_score": 28.09633452562975}, {"code": "def pad_tensor(self, x, pad, side='both')\n\t\n    if side == 'both':\n        return torch.cat([pad_tensor(self, x, pad, side='before'), x, pad_tensor(self, x, pad, side='after')], dim=1)\n    elif side == 'before':\n        return torch.cat([pad_tensor(self, x, pad, side='before'), x], dim=1)\n    elif side == 'after':\n        return torch.cat([x, pad_tensor(self, x, pad, side='after')], dim=1)\n    else:\n        raise ValueError(\"Side must be either 'before', 'after', or 'both'\")\n", "CodeBleu_score": 21.9121790915964}]}
{"instance_id": "a7ed1715a3df", "generation_result": [{"code": "def save_attention(attn, path)\n\t\n    import matplotlib.pyplot as plt\n    plt.figure(figsize=(10, 10))\n    plt.imshow(attn, cmap='Blues', interpolation='nearest')\n    plt.xlabel('Predicted')\n    plt.ylabel('Actual')\n    plt.savefig(path + '.png')\n    plt.close()\n\n\n", "CodeBleu_score": 46.56374134166595}, {"code": "def save_attention(attn, path)\n\t\n    import matplotlib.pyplot as plt\n    fig = plt.figure(figsize=(10, 10))\n    ax = fig.add_subplot(111)\n    cax = ax.matshow(attn, cmap='bone')\n    fig.colorbar(cax)\n    ax.set_xticklabels([''] + ['<sos>'] + [t.lower() for t in source.split(' ')] + ['<eos>'])\n    ax.set_yticklabels([''] + ['<sos>'] + [t.lower() for t in target.split(' ')] + ['<eos>'])\n    ax.xaxis.set_major_locator(plt.MultipleLocator(1))\n    ax.yaxis.set_major_locator(plt.MultipleLocator(1))\n    plt.savefig(path, format='png')\n    plt.close(fig)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 43.24288199218593}, {"code": "def save_attention(attn, path)\n\t\n    import matplotlib.pyplot as plt\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    cax = ax.matshow(attn, cmap='bone')\n    fig.colorbar(cax)\n    plt.savefig(path+'.png', format='png')\n    plt.close()\n\n\n", "CodeBleu_score": 40.67941338117759}]}
{"instance_id": "19375c9f44a2", "generation_result": [{"code": "def plot(array)\n\t\n    import matplotlib.pyplot as plt\n\n    fig = plt.figure(figsize=(30, 5))\n    ax = fig.add_subplot(111)\n    ax.xaxis.label.set_color('grey')\n    ax.yaxis.label.set_color('grey')\n    ax.xaxis.label.set_fontsize(23)\n    ax.yaxis.label.set_fontsize(23)\n    ax.tick_params(axis='x', colors='grey', labelsize=23)\n    ax.tick_params(axis='y', colors='grey', labelsize=23)\n    plt.plot(array)\n\n", "CodeBleu_score": 77.73415655580472}, {"code": "def plot(array)\n\t\n    import matplotlib.pyplot as plt\n\n    fig = plt.figure(figsize=(30, 5))\n    ax = fig.add_subplot(111)\n    ax.xaxis.label.set_color('grey')\n    ax.yaxis.label.set_color('grey')\n    ax.xaxis.label.set_fontsize(23)\n    ax.yaxis.label.set_fontsize(23)\n    ax.tick_params(axis='x', colors='grey', labelsize=23)\n    ax.tick_params(axis='y', colors='grey', labelsize=23)\n    plt.plot(array)", "CodeBleu_score": 77.73415655580472}, {"code": "def plot(array)\n\t\n    import matplotlib.pyplot as plt\n\n    fig = plt.figure(figsize=(30, 5))\n    ax = fig.add_subplot(111)\n    ax.xaxis.label.set_color('grey')\n    ax.yaxis.label.set_color('grey')\n    ax.xaxis.label.set_fontsize(23)\n    ax.yaxis.label.set_fontsize(23)\n    ax.tick_params(axis='x', colors='grey', labelsize=23)\n    ax.tick_params(axis='y', colors='grey', labelsize=23)\n    plt.plot(array)\n\n", "CodeBleu_score": 77.73415655580472}]}
{"instance_id": "ff94c425920c", "generation_result": [{"code": "def write_to_csv(image_name, prediction, confidence)\n\t\n    import csv\n    import os\n\n    # Create a dictionary with the image name, prediction, and confidence\n    data = {'image_name': image_name, 'prediction': prediction, 'confidence': confidence}\n\n    # Open the CSV file in append mode\n    with open('predictions.csv', 'a', newline='') as csvfile:\n        fieldnames = ['image_name', 'prediction', 'confidence']\n        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n\n        # Write the header if the file is new\n        if not os.path.exists('predictions.csv'):\n            writer.writeheader()\n\n        # Write the data as a new row\n        writer.writerow(data)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 48.429597051801125}, {"code": "def write_to_csv(image_name, prediction, confidence)\n\t\n    data = {\n        \"image_name\": image_name,\n        \"prediction\": prediction,\n        \"confidence\": confidence\n    }\n    with open(\"predictions.csv\", \"a\") as f:\n        writer = csv.DictWriter(f, fieldnames=data.keys())\n        if f.tell() == 0:\n            writer.writeheader()\n        writer.writerow(data)\n\n\n\n", "CodeBleu_score": 57.89141303083755}, {"code": "def write_to_csv(image_name, prediction, confidence)\n\t\n    # Create a dictionary with the image name, prediction, and confidence\n    data = {'image_name': image_name, 'prediction': prediction, 'confidence': confidence}\n\n    # Open the CSV file in append mode, creating it if it doesn't exist\n    with open('predictions.csv', 'a', newline='') as csvfile:\n        # Create a CSV writer object\n        writer = csv.DictWriter(csvfile, fieldnames=data.keys())\n\n        # Write the header if the file is new\n        if csvfile.tell() == 0:\n            writer.writeheader()\n\n        # Write the data as a new row\n        writer.writerow(data)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 53.62812396272366}]}
{"instance_id": "436161602009", "generation_result": [{"code": "def export_torchscript(model, im, file, optimize, prefix=colorstr(\"TorchScript:\"))\n\t\n    LOGGER.info(f\"{prefix} Exporting {file}\")\n    model.eval()\n    jit_model = torch.jit.trace(model, im)  # trace model for optimization\n    if optimize:\n        try:\n            jit_model = optimize_for_mobile(jit_model)\n            LOGGER.info(f\"{prefix} Optimized model for mobile use\")\n        except Exception as e:\n            LOGGER.info(f\"{prefix} {e}\")\n    jit_model.save(file)\n    LOGGER.info(f\"{prefix} Model saved to {file}\")\n    return file\n\n", "CodeBleu_score": 31.067377948899477}, {"code": "def export_torchscript(model, im, file, optimize, prefix=colorstr(\"TorchScript:\"))\n\t\n    LOGGER.info(f\"{prefix} Exporting torchscript model to {file}\")\n    model.model[-1].export = True  # set Detect() layer export=True\n    model = model.fuse()  # model.fuse() converts model to torch.jit.ScriptModule with script methods\n    y = model(im)  # dry run\n    LOGGER.info(f\"{prefix} TorchScript model saved to {file}\")\n    if optimize:\n        LOGGER.info(f\"{prefix} Optimizing for mobile...\")\n        model = optimize_for_mobile(model)\n        LOGGER.info(f\"{prefix} TorchScript model optimized for mobile saved to {file}\")\n    else:\n        LOGGER.info(f\"{prefix} TorchScript model saved to {file}\")\n    torch.jit.save(model, file)  # save TorchScript model\n    return file, None\n\n", "CodeBleu_score": 32.47031873877385}, {"code": "def export_torchscript(model, im, file, optimize, prefix=colorstr(\"TorchScript:\"))\n\t\n    if optimize:\n        try:\n            model = optimize_for_mobile(model)\n        except Exception as e:\n            LOGGER.info(f\"{prefix} optimization failed: {e}\")\n            optimize = False\n    else:\n        LOGGER.info(f\"{prefix} optimization skipped\")\n\n    LOGGER.info(f\"{prefix} exporting {model.__class__.__name__}\")\n    LOGGER.info(f\"{prefix} model summary: {summary(model, im.shape)}\")  # print a model summary\n    LOGGER.info(f\"{prefix} tracing with example tensor ({im.shape})\")\n    _ = model(im)  # run once\n    LOGGER.info(f\"{prefix} traced {optimize and 'optimized' or 'standard'} model using {torch.cuda.max_memory_allocated() / 1E9:.03g} GB\")\n\n    # torch.jit.trace(model, im, strict=False)  # trace model\n    model.fuse()  # fuse model for faster inference\n    model.eval()\n\n    # Export the model\n    with torch.no_grad():\n        if optimize:\n            model_trace = torch.jit.trace(model, im)\n            model_trace.save(file)  # torch.jit.save(model_trace, file)\n            suffix = \"optimized\"\n        else:\n            model_script = torch.jit.script(model)  # create TorchScript model\n            model_script.save(file)  # save TorchScript model\n            suffix = \"standard\"\n\n    LOGGER.info(f\"{prefix} export success, saved as {file} ({suffix})\")\n    return file, None\n\n", "CodeBleu_score": 32.27286034447949}]}
{"instance_id": "78335487ac1c", "generation_result": [{"code": "def export_coreml(model, im, file, int8, half, nms, prefix=colorstr(\"CoreML:\")):\n\t\n    import coremltools as ct\n\n    assert (\n        torch.cuda.is_available()\n    ), f\"{prefix} Exporting to CoreML requires a CUDA device. Please enable CUDA or remove --coreml from your command.\"\n    assert (\n        coreml_available\n    ), f\"{prefix} Exporting to CoreML requires the `coremltools` package. Please install with `pip install coremltools` or remove --coreml from your command.\"\n    LOGGER.info(f\"{prefix} Exporting to CoreML\")\n    file = Path(file).with_suffix(\".mlmodel\")\n    LOGGER.info(f\"{prefix} Exporting to CoreML file {file}\")\n\n    if nms:\n        model = iosmodel(model)\n\n    model.eval()\n    traced_model = torch.jit.trace(model, im)\n    traced_model = torch.jit.freeze(traced_model)\n\n    if int8:\n        traced_model = ct.convert(\n            traced_model,\n            inputs=[ct.ImageType(shape=im.shape, scale=1 / 255, name=\"input_1\")],\n            convert_to=\"mlprogram\",\n            compute_precision=\"INT8\",\n            minimum_ios_deployment_target=\"13\",\n        )\n    elif half:\n        traced_model = ct.convert(\n            traced_model,\n            inputs=[ct.ImageType(shape=im.shape, scale=1 / 255, name=\"input_1\")],\n            convert_to=\"mlprogram\",\n            compute_precision=\"FLOAT16\",\n            minimum_ios_deployment_target=\"13\",\n        )\n    else:\n        traced_model = ct.convert(\n            traced_model,\n            inputs=[ct.ImageType(shape=im.shape, scale=1 / 255, name=\"input_1\")],\n            convert_to=\"mlprogram\",\n            minimum_ios_deployment_target=\"13\",\n", "CodeBleu_score": 38.200070032292935}, {"code": "def export_coreml(model, im, file, int8, half, nms, prefix=colorstr(\"CoreML:\")):\n\t\n    import coremltools as ct\n\n    if not coremltools:\n        raise ImportError(\"Please install coremltools: pip install coremltools\")\n\n    print(f\"{prefix} Exporting {file}\")\n    suffix = \".mlmodel\"\n    file = Path(file).with_suffix(suffix)\n\n    # trace\n    model = model.fuse()  # fuse model for faster export\n    model = model.eval()\n    model.model[-1].export = False  # set Detect() layer export=False\n    dummy_input = torch.zeros(1, 3, im.shape[0], im.shape[1]).to(next(model.parameters()).device)  # input_shape(1,3,640,640)\n    model = model.to(torch.float32) if half else model.to(torch.float16) if int8 else model.to(torch.float32)\n    traced_model = torch.jit.trace(model, dummy_input)\n    traced_model = torch.jit.freeze(traced_model)\n\n    # convert\n    try:\n        ct_model = ct.convert(\n            traced_model,\n            inputs=[ct.ImageType(shape=dummy_input.shape, scale=1 / 255.0, name=\"image\")],\n            outputs=[ct.TensorType(shape=(1, 1, -1, -1))],\n            compute_units=ct.ComputeUnit.ALL,\n        )\n    except Exception as e:\n        raise Exception(f\"{prefix} Failed to convert to CoreML: {e}\")\n\n    # quantize\n    if int8:\n        ct_model = ct.utils.convert_neural_network_int8(ct_model)\n\n    # save\n    ct_model.save(file)\n\n    # wrap\n    if nms:\n        ct_model = ct.models.iOSModel(ct_model, ct.models.neural_network.NeuralNetworkMetadata(ct.models", "CodeBleu_score": 41.55616590805319}, {"code": "def export_coreml(model, im, file, int8, half, nms, prefix=colorstr(\"CoreML:\")):\n\t\n    # Check for coremltools\n    try:\n        import coremltools as ct\n    except ImportError:\n        raise ImportError(\"Please install coremltools with `pip install coremltools`.\")\n\n    # Log start of export\n    LOGGER.info(f\"{prefix} Exporting {file}\")\n\n    # Set file suffix to .mlmodel\n    file = file.with_suffix(\".mlmodel\")\n\n    # Wrap model with iOSModel for NMS support\n    if nms:\n        model = iOSModel(model, nms=nms)\n\n    # Trace model\n    LOGGER.info(f\"{prefix} Tracing model\")\n    traced_model = torch.jit.trace(model, im)\n\n    # Convert to CoreML\n    LOGGER.info(f\"{prefix} Converting to CoreML\")\n    coreml_model = ct.convert(traced_model, inputs=[ct.ImageType(name=\"input\", shape=im.shape)])\n\n    # Apply quantization\n    if int8 or half:\n        LOGGER.info(f\"{prefix} Applying quantization\")\n        coreml_model = ct.convert(traced_model, inputs=[ct.ImageType(name=\"input\", shape=im.shape, scale=1 / 255.0, bias=[0, 0, 0])])\n\n    # Save CoreML model\n    LOGGER.info(f\"{prefix} Saving CoreML model\")\n    coreml_model.save(file)\n\n    # Return file path and CoreML model\n    return file, coreml_model\n\n", "CodeBleu_score": 42.25281438184797}]}
{"instance_id": "80233cf71580", "generation_result": [{"code": "def generate_individual(input_ranges, individual_length)\n\t\n    individual = []\n    for i in range(individual_length):\n        individual.append(random.uniform(*input_ranges[i]))\n    return individual\n\n\n", "CodeBleu_score": 45.41230804837097}, {"code": "def generate_individual(input_ranges, individual_length)\n\t\n    individual = []\n    for i in range(individual_length):\n        individual.append(random.uniform(input_ranges[i][0], input_ranges[i][1]))\n    return individual\n\n\n", "CodeBleu_score": 47.630972577458174}, {"code": "def generate_individual(input_ranges, individual_length)\n\t\n    individual = []\n    for i in range(individual_length):\n        individual.append(random.uniform(input_ranges[i][0], input_ranges[i][1]))\n    return individual\n\n\n\n", "CodeBleu_score": 47.630972577458174}]}
{"instance_id": "5ea979cb4a8e", "generation_result": [{"code": "def marginal_log_mean_coeff(self, t)\n\t\n    if self.schedule == \"discrete\":\n        return torch.log(self.alphas[t])\n    elif self.schedule == \"linear\":\n        return torch.log(self.alphas[t]) - 0.5 * (\n            (1 - self.alphas[t]) ** 2 + self.alphas[t + 1] ** 2\n        )\n    elif self.schedule == \"cosine\":\n        return torch.log(self.alphas[t]) + self.s_max - self.s_min\n\n", "CodeBleu_score": 28.699803295869863}, {"code": "def marginal_log_mean_coeff(self, t)\n\t\n    if self.schedule == \"discrete\":\n        return torch.log(self.alphas[t])\n    elif self.schedule == \"linear\":\n        return self.interpolate_fn(\n            t,\n            torch.tensor([0, self.num_timesteps], device=t.device),\n            torch.log(torch.tensor([self.alphas[0], self.alphas[self.num_timesteps]], device=t.device)),\n        )\n    elif self.schedule == \"cosine\":\n        return self.interpolate_fn(\n            t,\n            torch.tensor([0, self.num_timesteps], device=t.device),\n            torch.log(torch.tensor([self.alphas[0], self.alphas[self.num_timesteps]], device=t.device)),\n        )\n\n", "CodeBleu_score": 32.9268643469243}, {"code": "def marginal_log_mean_coeff(self, t)\n\t\n    if self.schedule_type == \"discrete\":\n        return torch.log(self.alphas[t])\n    elif self.schedule_type == \"linear\":\n        return self.interpolate_fn(t, self.timesteps, torch.log(self.alphas))\n    elif self.schedule_type == \"cosine\":\n        return self.interpolate_fn(t, self.timesteps, torch.log(self.alphas))\n    else:\n        raise ValueError(f\"Unrecognized schedule type: {self.schedule_type}\")\n\n", "CodeBleu_score": 30.029426686682143}]}
{"instance_id": "7da4fae5a278", "generation_result": [{"code": "def inverse_lambda(self, lamb)\n\t\n    if self.schedule == \"linear\":\n        beta_t = torch.gather(self.betas, dim=0, index=lamb)\n        alpha_t = torch.gather(self.alphas_cumprod, dim=0, index=lamb)\n        t = torch.sqrt(1 - alpha_t) * (1 - beta_t) / (1 - alpha_t * beta_t)\n        t = torch.max(torch.zeros_like(t), torch.min(t, self.num_timesteps - 1))\n        return t\n    elif self.schedule == \"quad\":\n        alpha_t = torch.gather(self.alphas_cumprod, dim=0, index=lamb)\n        alpha_t_prev = torch.gather(self.alphas_cumprod, dim=0, index=lamb - 1)\n        return (torch.sqrt(alpha_t) - torch.sqrt(alpha_t_prev)) / (\n            math.sqrt(self.alphas_cumprod[1]) - math.sqrt(self.alphas_cumprod[0])\n        )\n    elif self.schedule == \"sigmoid\":\n        return 2 * torch.sigmoid(lamb * 10 - 5) - 1\n    elif self.schedule == \"cosine\":\n        return torch.acos(1 - 2 * lamb) / (2 * math.pi)\n    elif self.schedule == \"js\":\n        return 1 / (1 + torch.exp(-lamb * 10))\n    elif self.schedule == \"warmup10\":\n        if lamb < 0.1:\n            return 10 * lamb\n        else:\n            return 1 - torch.exp(-5 * (lamb - 0.1))\n    elif self.schedule == \"warmup50\":\n        if lamb < 0.5:\n            return 5 * lamb\n        else:\n            return 1 - torch.exp(-5 * (lamb - 0.5))\n    elif self.schedule == \"warmup", "CodeBleu_score": 27.856970768386013}, {"code": "def inverse_lambda(self, lamb)\n\t\n    if self.schedule == \"linear\":\n        return torch.log(self.beta_min + (self.beta_max - self.beta_min) * (1 - lamb))\n    elif self.schedule == \"quadratic\":\n        return torch.log(self.beta_min + (self.beta_max - self.beta_min) * (1 - lamb ** 2))\n    elif self.schedule == \"sigmoid\":\n        return torch.log(self.beta_min + (self.beta_max - self.beta_min) / (1 + torch.exp(-lamb)))\n    elif self.schedule == \"discrete\":\n        return self.interpolate_fn(lamb, self.log_alpha_cumprod, self.log_alpha_cumprod)\n    else:\n        return torch.acos(1 - lamb) / self.beta_max\n\n", "CodeBleu_score": 19.87496357255331}, {"code": "def inverse_lambda(self, lamb)\n\t\n    if self.schedule == \"linear\":\n        beta_t = self.betas[self.T - lamb.shape[0] : self.T]\n        beta_t = beta_t.flip(dims=(0,))\n        alpha = 1.0 - beta_t\n        alpha_cumprod = torch.cumprod(alpha, dim=0)\n        sqrt_recip_alpha_cumprod = torch.sqrt(1.0 / alpha_cumprod)\n        sqrt_recipm1_alpha_cumprod = torch.sqrt(1.0 / alpha_cumprod - 1)\n        # Equation 12 in the paper\n        c = (1 - alpha_cumprod[0]) / (1 - alpha_cumprod) / sqrt_recipm1_alpha_cumprod\n        c = c.to(lamb.device)\n        # Equation 11 in the paper\n        t = c * lamb\n        t = t.cumsum(dim=0)\n        t = t.flip(dims=(0,))\n        return t\n    elif self.schedule == \"quadratic\":\n        beta_t = self.betas[self.T - lamb.shape[0] : self.T]\n        beta_t = beta_t.flip(dims=(0,))\n        alpha = 1.0 - beta_t\n        alpha_cumprod = torch.cumprod(alpha, dim=0)\n        sqrt_recip_alpha_cumprod = torch.sqrt(1.0 / alpha_cumprod)\n        # Equation 12 in the paper\n        c = (1 - alpha_cumprod[0]) / (1 - alpha_cumprod) / sqrt_recip_alpha_cumprod\n        c = c.to(lamb.device)\n        # Equation 11 in the paper\n        t = c * lamb\n        t = t.cumsum(dim=0)\n        t = t.flip(dims=(0,))\n        return t\n    elif self.schedule == \"cosine\":\n        # Equation", "CodeBleu_score": 30.4311994869257}]}
{"instance_id": "a519c853a2b2", "generation_result": [{"code": "def noise_pred_fn(x, t_continuous, cond=None)\n\t\n    t = expand_dims(t_continuous, x.ndim)\n    if model_type == \"ddpm\":\n        if cond is not None:\n            raise ValueError(\"Conditional model does not support DDPM.\")\n        return model(x, t)\n    elif model_type == \"ddim\":\n        if cond is not None:\n            raise ValueError(\"Conditional model does not support DDIM.\")\n        return model(x, t)\n    elif model_type == \"ddim_cond\":\n        if cond is None:\n            raise ValueError(\"Conditional model requires conditioning.\")\n        return model(x, t, cond)\n    else:\n        raise ValueError(\"Invalid model type.\")\n\n", "CodeBleu_score": 16.57830615160016}, {"code": "def noise_pred_fn(x, t_continuous, cond=None)\n\t\n    # Expand the time input to match the batch size.\n    t_continuous = expand_dims(t_continuous, x.ndim - 1)\n    # Process the input through the model.\n    if model_type == 'ddpm':\n        # For DDPM, the model output is the noise prediction.\n        noise_pred = model(x, t_continuous, cond)\n    elif model_type == 'ddim':\n        # For DDIM, the model output is the mean of the noise prediction.\n        noise_pred = model(x, t_continuous, cond)[0]\n    else:\n        raise ValueError(f'Unknown model type: {model_type}')\n    # Apply the noise schedule to the model output.\n    if noise_schedule_type == 'linear':\n        # For linear noise schedule, the model output is the noise prediction.\n        noise_pred = noise_pred\n    elif noise_schedule_type == 'cosine':\n        # For cosine noise schedule, the model output is the mean of the noise prediction.\n        noise_pred = noise_pred\n    elif noise_schedule_type == 'cosine_linear':\n        # For cosine linear noise schedule, the model output is the mean of the noise prediction.\n        noise_pred = noise_pred\n    elif noise_schedule_type == 'cosine_linear_warmup':\n        # For cosine linear warmup noise schedule, the model output is the mean of the noise prediction.\n        noise_pred = noise_pred\n    elif noise_schedule_type == 'cosine_linear_warmup_linear':\n        # For cosine linear warmup linear noise schedule, the model output is the mean of the noise prediction.\n        noise_pred = noise_pred\n    elif noise_schedule_type == 'cosine_linear_warmup_linear_warmup':\n        # For cosine linear warmup linear warmup noise schedule, the model output is the mean of the noise prediction.\n        noise_pred = noise_pred\n    elif noise_schedule_type == 'cosine_linear_warmup_linear_warmup_linear':\n        # For cosine linear warmup linear warmup linear", "CodeBleu_score": 18.512939826575636}, {"code": "def noise_pred_fn(x, t_continuous, cond=None)\n\t\n    t = expand_dims(t_continuous, dims=x.ndim)\n    if model_type == 'ddpm':\n        return model(x, t, cond=cond)\n    elif model_type == 'ddim':\n        return model(x, t, cond=cond)\n    elif model_type == 'dpm':\n        return model(x, t, cond=cond)\n    else:\n        raise ValueError(f'Unknown model type: {model_type}')\n\n", "CodeBleu_score": 11.156728200037186}]}
{"instance_id": "96a9b7d54823", "generation_result": [{"code": "def dynamic_thresholding_fn(self, x0, t=None)\n\t\n    x0 = x0.reshape(x0.shape[0], -1)\n    x0_abs = x0.abs()\n    q = x0_abs.quantile(self.q, dim=-1, keepdim=True)\n    scaling_factor = (self.q / (1 - self.q)) * q\n    x0 = torch.clamp(x0, -scaling_factor, scaling_factor)\n    x0 = x0 / scaling_factor\n    return x0.reshape(x0.shape[0], *self.shape)\n\n", "CodeBleu_score": 27.721861068175162}, {"code": "def dynamic_thresholding_fn(self, x0, t=None)\n\t\n    if t is None:\n        t = self.t\n    x0 = x0.reshape(-1)\n    q = torch.quantile(torch.abs(x0), t)\n    s = q / self.t\n    return torch.clamp(x0, -s, s).reshape(x0.shape)\n\n", "CodeBleu_score": 28.679574937815733}, {"code": "def dynamic_thresholding_fn(self, x0, t=None)\n\t\n    x0 = x0.reshape(x0.shape[0], -1)\n    x0_abs = torch.abs(x0)\n    x0_abs_mean = torch.mean(x0_abs, dim=0)\n    x0_abs_mean_expanded = expand_dims(x0_abs_mean, dims=x0.ndim)\n    x0_abs_max = torch.max(x0_abs_mean_expanded, x0_abs)\n    x0_abs_max_expanded = expand_dims(x0_abs_max, dims=x0.ndim)\n    x0_scaled = x0 / x0_abs_max_expanded\n    x0_scaled_clamped = torch.clamp(x0_scaled, -1.0, 1.0)\n    x0_scaled_clamped = x0_scaled_clamped.reshape(x0.shape)\n    return x0_scaled_clamped\n\n", "CodeBleu_score": 19.07718656498503}]}
{"instance_id": "59c6ec6d7a6a", "generation_result": [{"code": "def data_prediction_fn(self, x, t)\n\t\n    # Calculate the dynamic thresholding ratio\n    dynamic_thresholding_ratio = self.dynamic_thresholding_ratio(t)\n\n    # Calculate the maximum threshold value\n    max_threshold = self.max_threshold(t)\n\n    # Apply dynamic thresholding\n    x_thresholded = torch.clamp(x, min=-max_threshold, max=max_threshold)\n\n    # Normalize the clamped values\n    x_normalized = x_thresholded / max_threshold\n\n    return x_normalized\n\n", "CodeBleu_score": 7.027202570830761}, {"code": "def data_prediction_fn(self, x, t)\n\t\n    if self.dynamic_thresholding:\n        threshold = self.threshold_fn(t)\n        x = torch.clamp(x, min=-threshold, max=threshold)\n    return x / threshold\n\n", "CodeBleu_score": 12.424928956710248}, {"code": "def data_prediction_fn(self, x, t)\n\t\n    x0 = x\n    x0 = x0.to(self.device)\n    x0 = x0.to(self.dtype)\n    x0 = x0.to(self.device)\n    x0 = x0.to(self.dtype)\n    x0 = x0.to(self.device)\n    x0 = x0.to(self.dtype)\n    x0 = x0.to(self.device)\n    x0 = x0.to(self.dtype)\n    x0 = x0.to(self.device)\n    x0 = x0.to(self.dtype)\n    x0 = x0.to(self.device)\n    x0 = x0.to(self.dtype)\n    x0 = x0.to(self.device)\n    x0 = x0.to(self.dtype)\n    x0 = x0.to(self.device)\n    x0 = x0.to(self.dtype)\n    x0 = x0.to(self.device)\n    x0 = x0.to(self.dtype)\n    x0 = x0.to(self.device)\n    x0 = x0.to(self.dtype)\n    x0 = x0.to(self.device)\n    x0 = x0.to(self.dtype)\n    x0 = x0.to(self.device)\n    x0 = x0.to(self.dtype)\n    x0 = x0.to(self.device)\n    x0 = x0.to(self.dtype)\n    x0 = x0.to(self.device)\n    x0 = x0.to(self.dtype)\n    x0 = x0.to(self.device)\n    x0 = x0.to(self.dtype)\n    x0 = x0.to(self.device)\n    x0 = x0.to(self.dtype)\n    x0 = x0.to(self.device)\n    x0 = x0.to(self.dtype)\n    x0 = x0.to(self.device", "CodeBleu_score": 7.484094807991928}]}
{"instance_id": "65e3618468a2", "generation_result": [{"code": "def predict_eps_sigma(model, input, sigma_in, **kwargs)\n\t\n    sigma_in = sigma_in.reshape(sigma_in.shape[0], 1, sigma_in.shape[1], sigma_in.shape[2])\n    sigma_in = sigma_in.repeat(1, input.shape[1], 1, 1)\n    input = input.reshape(input.shape[0], 1, input.shape[1], input.shape[2])\n    input = input.repeat(1, sigma_in.shape[1], 1, 1)\n    input = input * sigma_in\n    output = model(input, **kwargs)\n    output = output * sigma_in\n    return output\n\n\n", "CodeBleu_score": 34.00528025529415}, {"code": "def predict_eps_sigma(model, input, sigma_in, **kwargs)\n\t\n    sigma_in = sigma_in.reshape(input.shape)\n    input_scaled = input / sigma_in\n    pred = model(input_scaled, **kwargs)\n    pred = pred * sigma_in\n    return pred\n\n\n", "CodeBleu_score": 20.068025398603424}, {"code": "def predict_eps_sigma(model, input, sigma_in, **kwargs)\n\t\n    sigma = sigma_in.reshape(input.shape)\n    input = input.reshape(input.shape)\n    input_scaled = input / sigma\n    output = model(input_scaled, **kwargs)\n    output_scaled = output * sigma\n    return output_scaled\n\n", "CodeBleu_score": 21.87520199378395}]}
{"instance_id": "b35846dfbc66", "generation_result": [{"code": "def cal_intergrand(beta_0, beta_1, taus)\n\t\n    with torch.enable_grad():\n        taus = taus.clone().requires_grad_(True)\n        alpha = t2alpha_fn(beta_0, beta_1, taus)\n        log_alpha = torch.log(alpha)\n        log_alpha.backward(torch.ones_like(log_alpha))\n        integrand = taus.grad * alpha\n    return integrand\n\n", "CodeBleu_score": 39.98718497658801}, {"code": "def cal_intergrand(beta_0, beta_1, taus)\n\t\n    alpha = t2alpha_fn(beta_0, beta_1, taus)\n    log_alpha = torch.log(alpha)\n    dlog_alpha = torch.autograd.grad(log_alpha, taus, create_graph=True)[0]\n    return torch.exp(log_alpha) * dlog_alpha\n\n", "CodeBleu_score": 16.01229642763192}, {"code": "def cal_intergrand(beta_0, beta_1, taus)\n\t\n    with torch.enable_grad():\n        taus = taus.clone().requires_grad_(True)\n        alpha = t2alpha_fn(beta_0, beta_1, taus)\n        log_alpha = torch.log(alpha)\n        dlog_alpha_dtaus = torch.autograd.grad(log_alpha.sum(), taus)[0]\n    return alpha * dlog_alpha_dtaus\n\n", "CodeBleu_score": 42.59349705471699}]}
{"instance_id": "5f3667566a10", "generation_result": [{"code": "def sample_euler_ancestral(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None)\n\t\n    if extra_args is None:\n        extra_args = {}\n    if noise_sampler is None:\n        noise_sampler = default_noise_sampler\n    if disable is None:\n        disable = False\n    sigma_from = sigmas[0]\n    sigma_to = sigmas[1]\n    sigma_down, sigma_up = get_ancestral_step(sigma_from, sigma_to, eta)\n    x_ancestral = x\n    for i, sigma in enumerate(sigmas):\n        if disable:\n            break\n        sigma_next = sigmas[i + 1] if i + 1 < len(sigmas) else None\n        denoised = model(x_ancestral, sigma, **extra_args)\n        d = to_d(x_ancestral, sigma, denoised)\n        x = x_ancestral + d * sigma_up\n        if callback:\n            callback(i, x, d)\n        if sigma_next:\n            sigma_down, sigma_up = get_ancestral_step(sigma, sigma_next, eta)\n            x_ancestral = x + (sigma - sigma_down) * d\n        else:\n            x_ancestral = x\n    if s_noise:\n        x = x + s_noise * noise_sampler(x)\n    return x\n\n", "CodeBleu_score": 35.25896349492344}, {"code": "def sample_euler_ancestral(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None)\n\t\n    if extra_args is None:\n        extra_args = {}\n    if noise_sampler is None:\n        noise_sampler = default_noise_sampler\n    x_next = x\n    for sigma_from, sigma_to in zip(sigmas[:-1], sigmas[1:]):\n        sigma_down, sigma_up = get_ancestral_step(sigma_from, sigma_to, eta)\n        if sigma_up:\n            x_next = x_next + sigma_up * noise_sampler(x_next, sigma_up)\n        denoised = model(x_next, sigma_down, **extra_args)\n        x_next = x_next + to_d(x_next, sigma_down, denoised)\n        if callback:\n            callback({'x_next': x_next, 'sigma': sigma_down})\n    if s_noise:\n        x_next = x_next + s_noise * noise_sampler(x_next, sigma_to)\n    return x_next\n\n", "CodeBleu_score": 32.128054622933135}, {"code": "def sample_euler_ancestral(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None)\n\t\n    if extra_args is None:\n        extra_args = {}\n    if noise_sampler is None:\n        noise_sampler = default_noise_sampler\n    if disable is None:\n        disable = False\n    if s_noise:\n        s_noise = utils.append_dims(s_noise, x.ndim)\n    if disable:\n        return model(x, sigmas, **extra_args)\n    x_next = x\n    for sigma_from, sigma_to in zip(sigmas[:-1], sigmas[1:]):\n        denoised = model(x_next, sigma_from, **extra_args)\n        sigma_down, sigma_up = get_ancestral_step(sigma_from, sigma_to, eta)\n        if sigma_up:\n            noise = noise_sampler(x_next) * sigma_up\n        else:\n            noise = 0.\n        x_next = x_next + to_d(x_next, sigma_from, denoised) * sigma_down + noise\n        if callback:\n            callback(locals())\n    if s_noise:\n        x_next = x_next + noise_sampler(x_next) * s_noise\n    return x_next\n\n", "CodeBleu_score": 33.62301300241916}]}
{"instance_id": "6261ded88df2", "generation_result": [{"code": "def linear_multistep_coeff(order, t, i, j)\n\t\n    if order > j:\n        raise ValueError(\"Order must be less than or equal to j\")\n    def prod(k, j):\n        if k == j:\n            return 1\n        return (t[i] - t[k]) * prod(k + 1, j)\n    return 1 / prod(i, j)\n\n\n", "CodeBleu_score": 17.115041596198438}, {"code": "def linear_multistep_coeff(order, t, i, j)\n\t\n    if order > 2 * i:\n        raise ValueError(\"Order is too high for the step index.\")\n    def product_term(t, i, j):\n        if j == i:\n            return 1\n        elif j == i - 1:\n            return t[i] - t[j]\n        else:\n            return (t[i] - t[j]) * product_term(t, i, j - 1) / (t[i] - t[i - j])\n    return integrate.quad(lambda x: product_term(t, i, j), t[i], t[i + 1])[0]\n\n\n", "CodeBleu_score": 31.910289694217038}, {"code": "def linear_multistep_coeff(order, t, i, j)\n\t\n    if order > 1 and j == i:\n        return 1\n    elif order > 1 and j == i + 1:\n        return -1\n    elif order > 1 and j > i + 1:\n        return 0\n    elif order == 1:\n        return 1\n    else:\n        raise ValueError(\"Order too high for step index\")\n\n\n", "CodeBleu_score": 21.067508780662962}]}
{"instance_id": "4ee025e33ab1", "generation_result": [{"code": "def sample_lcm(model, x, sigmas, extra_args=None, callback=None, disable=None, noise_sampler=None)\n\t\n    if extra_args is None:\n        extra_args = {}\n    if noise_sampler is None:\n        noise_sampler = default_noise_sampler\n    x_t = x\n    for i, sigma in enumerate(sigmas):\n        if disable is not None:\n            disable.set_description(f\"Sampling ({i+1}/{len(sigmas)})\")\n        if callback is not None:\n            callback(x_t, i)\n        if sigma > 0:\n            noise = noise_sampler(x_t, sigma)\n            x_t = model(x_t, sigma, **extra_args) + sigma * noise\n        else:\n            x_t = model(x_t, sigma, **extra_args)\n    return x_t\n\n", "CodeBleu_score": 31.126752057261847}, {"code": "def sample_lcm(model, x, sigmas, extra_args=None, callback=None, disable=None, noise_sampler=None)\n\t\n    if extra_args is None:\n        extra_args = {}\n    if noise_sampler is None:\n        noise_sampler = default_noise_sampler\n\n    for i, sigma in enumerate(sigmas):\n        if disable is not None:\n            disable.set_description(f\"Sampling {sigma:.4f}\")\n        if callback is not None:\n            callback(x, i)\n        x_in = x + noise_sampler(x) * sigma\n        if sigma > 0:\n            x_in = x_in / (sigma ** 2 + 1) ** 0.5\n        x = model(x_in, sigma=sigma, **extra_args)\n    return x", "CodeBleu_score": 31.57033104473646}, {"code": "def sample_lcm(model, x, sigmas, extra_args=None, callback=None, disable=None, noise_sampler=None)\n\t\n    if extra_args is None:\n        extra_args = {}\n    if noise_sampler is None:\n        noise_sampler = default_noise_sampler\n    if disable is None:\n        disable = False\n    x_t = x\n    for i, sigma in enumerate(sigmas):\n        if disable:\n            pass\n        else:\n            callback({\"sigma\": sigma}, i)\n        model_output = model(x_t, sigma=sigma, **extra_args)\n        if isinstance(model_output, tuple):\n            x_t, _ = model_output\n        else:\n            x_t = model_output\n        if sigma > 0:\n            noise = noise_sampler(x_t, sigma)\n            x_t = x_t + sigma * noise\n    return x_t", "CodeBleu_score": 31.218362254279352}]}
{"instance_id": "cb51f91da606", "generation_result": [{"code": "def append_dims(x, target_dims)\n\t\n    x = x.detach()\n    if x.device.type == 'mps':\n        return x.clone().expand(target_dims)\n    else:\n        return x.expand(target_dims)\n\n\n", "CodeBleu_score": 16.360300568697077}, {"code": "def append_dims(x, target_dims)\n\t\n    if x.ndim >= target_dims:\n        return x\n    n_dims = target_dims - x.ndim\n    for _ in range(n_dims):\n        x = x.unsqueeze(-1)\n    if x.device.type == 'mps':\n        return x.detach().clone()\n    return x\n\n\n", "CodeBleu_score": 35.455088173625086}, {"code": "def append_dims(x, target_dims)\n\t\n    x = x.clone()\n    n_dims = len(x.shape)\n    if target_dims <= n_dims:\n        raise ValueError('target_dims must be greater than the current number of dimensions')\n    n_dims_to_append = target_dims - n_dims\n    for i in range(n_dims_to_append):\n        x = x.unsqueeze(-1)\n    if x.device.type == 'mps':\n        return x.detach()\n    else:\n        return x\n\n\n", "CodeBleu_score": 33.90361822973726}]}
{"instance_id": "9fac874a3bc7", "generation_result": [{"code": "def download_file(path, url, digest=None)\n\t\n    import os\n    import hashlib\n    import requests\n    import tqdm\n    import logging\n    logging.basicConfig(level=logging.INFO)\n    logger = logging.getLogger(__name__)\n    logger.setLevel(logging.INFO)\n    logger.info(f\"Downloading {url} to {path}\")\n    os.makedirs(os.path.dirname(path), exist_ok=True)\n    if os.path.exists(path):\n        logger.info(f\"{path} already exists, skipping download\")\n        return path\n    response = requests.get(url, stream=True)\n    total_size_in_bytes = int(response.headers.get('content-length', 0))\n    block_size = 1024\n    progress_bar = tqdm.tqdm(total=total_size_in_bytes, unit='iB', unit_scale=True)\n    with open(path, 'wb') as file:\n        for data in response.iter_content(block_size):\n            progress_bar.update(len(data))\n            file.write(data)\n    progress_bar.close()\n    if total_size_in_bytes != 0 and progress_bar.n != total_size_in_bytes:\n        logger.error(\"ERROR, something went wrong\")\n    if digest is not None:\n        with open(path, 'rb') as file:\n            file_digest = hashlib.sha256(file.read()).hexdigest()\n        if file_digest != digest:\n            logger.error(f\"ERROR, SHA-256 digest of {path} does not match expected value\")\n    logger.info(f\"Downloaded {url} to {path}\")\n    return path\n\n\n", "CodeBleu_score": 43.017539203429024}, {"code": "def download_file(path, url, digest=None)\n\t\n    import os\n    import hashlib\n    import requests\n    import tqdm\n    from requests.adapters import HTTPAdapter\n    from requests.packages.urllib3.util.retry import Retry\n    if digest is not None:\n        digest = digest.lower()\n    if not os.path.exists(path):\n        os.makedirs(path)\n    filename = url.split('/')[-1]\n    file_path = os.path.join(path, filename)\n    if os.path.exists(file_path):\n        if digest is not None:\n            with open(file_path, 'rb') as f:\n                file_digest = hashlib.sha256(f.read()).hexdigest()\n                if file_digest != digest:\n                    raise ValueError(f'File {file_path} has incorrect digest {file_digest}, expected {digest}')\n        return file_path\n    session = requests.Session()\n    retry = Retry(total=5, backoff_factor=0.1, status_forcelist=[500, 502, 503, 504])\n    adapter = HTTPAdapter(max_retries=retry)\n    session.mount('http://', adapter)\n    session.mount('https://', adapter)\n    response = session.get(url, stream=True)\n    total_size_in_bytes = int(response.headers.get('content-length', 0))\n    block_size = 1024\n    progress_bar = tqdm.tqdm(total=total_size_in_bytes, unit='iB', unit_scale=True)\n    with open(file_path, 'wb') as file:\n        for data in response.iter_content(block_size):\n            progress_bar.update(len(data))\n            file.write(data)\n    progress_bar.close()\n    if total_size_in_bytes != 0 and progress_bar.n != total_size_in_bytes:\n        raise ValueError(f'Error downloading {url}')\n    if digest is not None:\n        with open(", "CodeBleu_score": 43.52137973406419}, {"code": "def download_file(path, url, digest=None)\n\t\n    import os\n    import hashlib\n    import requests\n    import tqdm\n    if not os.path.exists(path):\n        os.makedirs(path)\n    if not os.path.exists(path):\n        raise Exception(\"Failed to create directory: {}\".format(path))\n    file_path = os.path.join(path, os.path.basename(url))\n    if not os.path.exists(file_path):\n        response = requests.get(url, stream=True)\n        if response.status_code == 200:\n            total_size_in_bytes = int(response.headers.get('content-length', 0))\n            block_size = 1024\n            progress_bar = tqdm.tqdm(total=total_size_in_bytes, unit='iB', unit_scale=True)\n            with open(file_path, 'wb') as file:\n                for data in response.iter_content(block_size):\n                    progress_bar.update(len(data))\n                    file.write(data)\n            progress_bar.close()\n            if total_size_in_bytes != 0 and progress_bar.n != total_size_in_bytes:\n                raise Exception(\"Error downloading file: {}\".format(url))\n    if digest is not None:\n        with open(file_path, 'rb') as file:\n            file_hash = hashlib.sha256(file.read()).hexdigest()\n        if file_hash != digest:\n            raise Exception(\"File hash does not match: {}\".format(file_path))\n    return file_path\n\n\n", "CodeBleu_score": 41.714265870091936}]}
{"instance_id": "f23165e897ce", "generation_result": [{"code": "def train_mode(model, mode=True)\n\t\n    model.train(mode)\n    return model\n\n", "CodeBleu_score": 3.5228876052714493}, {"code": "def train_mode(model, mode=True)\n\t\n    model.train_mode = mode\n    model.training = mode\n    model.eval()\n    if mode:\n        model.train()\n    else:\n        model.eval()\n    return model\n\n\n", "CodeBleu_score": 24.64194027925052}, {"code": "def train_mode(model, mode=True)\n\t\n    training_modes = []\n    for module in model.modules():\n        if hasattr(module, \"training\"):\n            training_modes.append(module.training)\n            module.train(mode)\n    try:\n        yield\n    finally:\n        for module, mode in zip(model.modules(), training_modes):\n            if hasattr(module, \"training\"):\n                module.train(mode)\n\n\n", "CodeBleu_score": 31.765478648109145}]}
{"instance_id": "c6d59c6cb8e1", "generation_result": [{"code": "def ema_update(model, averaged_model, decay)\n\t\n    averaged_model.load_state_dict(model.state_dict())\n    for param, averaged_param in zip(model.parameters(), averaged_model.parameters()):\n        averaged_param.data.mul_(decay).add_(param.data, alpha=1 - decay)\n    for buffer, averaged_buffer in zip(model.buffers(), averaged_model.buffers()):\n        averaged_buffer.data.copy_(buffer.data)\n\n\n", "CodeBleu_score": 32.638606757462675}, {"code": "def ema_update(model, averaged_model, decay)\n\t\n    averaged_model.load_state_dict(model.state_dict())\n    for param in averaged_model.parameters():\n        param.requires_grad = False\n    for buffer in averaged_model.buffers():\n        buffer.requires_grad = False\n    for param, averaged_param in zip(model.parameters(), averaged_model.parameters()):\n        averaged_param.data.mul_(decay).add_(param.data, alpha=1 - decay)\n    for buffer, averaged_buffer in zip(model.buffers(), averaged_model.buffers()):\n        averaged_buffer.data.copy_(buffer.data)\n    return averaged_model", "CodeBleu_score": 40.9486355988368}, {"code": "def ema_update(model, averaged_model, decay)\n\t\n    averaged_model.eval()\n    with torch.no_grad():\n        for (p_current, p_average) in zip(model.parameters(), averaged_model.parameters()):\n            p_average.mul_(decay).add_(p_current, alpha=(1 - decay))\n        for (b_current, b_average) in zip(model.buffers(), averaged_model.buffers()):\n            b_average.copy_(b_current)\n\n\n", "CodeBleu_score": 28.90433682124327}]}
{"instance_id": "600c39a30798", "generation_result": [{"code": "def rand_log_logistic(shape, loc=0., scale=1., min_value=0., max_value=float('inf'), device='cpu', dtype=torch.float32)\n\t\n    min_value = torch.tensor(min_value, device=device, dtype=dtype)\n    max_value = torch.tensor(max_value, device=device, dtype=dtype)\n    min_cdf = torch.log(min_value - loc) / scale\n    max_cdf = torch.log(max_value - loc) / scale\n    uniform_samples = torch.rand(shape, device=device, dtype=dtype)\n    uniform_samples = uniform_samples * (max_cdf - min_cdf) + min_cdf\n    log_samples = torch.logit(uniform_samples) * scale + loc\n    samples = torch.exp(log_samples)\n    return samples\n\n\n\n\n\n", "CodeBleu_score": 39.03224868731326}, {"code": "def rand_log_logistic(shape, loc=0., scale=1., min_value=0., max_value=float('inf'), device='cpu', dtype=torch.float32)\n\t\n    min_value = torch.tensor(min_value, device=device, dtype=dtype)\n    max_value = torch.tensor(max_value, device=device, dtype=dtype)\n    min_cdf = torch.log(min_value - loc) / scale\n    max_cdf = torch.log(max_value - loc) / scale\n    uniform_samples = torch.rand(shape, device=device, dtype=dtype)\n    logit_samples = torch.log(uniform_samples * (max_cdf - min_cdf) + min_cdf)\n    samples = torch.exp(logit_samples * scale + loc) + min_value\n    return samples\n\n", "CodeBleu_score": 36.071167610469836}, {"code": "def rand_log_logistic(shape, loc=0., scale=1., min_value=0., max_value=float('inf'), device='cpu', dtype=torch.float32)\n\t\n    min_value = torch.as_tensor(min_value, dtype=dtype, device=device)\n    max_value = torch.as_tensor(max_value, dtype=dtype, device=device)\n    min_cdf = log_logistic_cdf(min_value, loc=loc, scale=scale)\n    max_cdf = log_logistic_cdf(max_value, loc=loc, scale=scale)\n    uniform_samples = torch.rand(shape, dtype=dtype, device=device)\n    cdf_inverse = torch.log(uniform_samples / (1 - uniform_samples) + min_cdf / (1 - min_cdf)) - loc\n    scaled_samples = cdf_inverse / scale\n    return scaled_samples\n\n\n", "CodeBleu_score": 30.663571814886637}]}
{"instance_id": "75e2def2467f", "generation_result": [{"code": "def get_activation(activation: Literal[\"elu\", \"snake\", \"none\"], antialias=False, channels=None) -> nn.Module\n\t\n    if activation == \"elu\":\n        return nn.ELU(inplace=True)\n    elif activation == \"snake\":\n        return SnakeBeta(channels=channels)\n    elif activation == \"none\":\n        return nn.Identity()\n    else:\n        raise ValueError(f\"Unknown activation type: {activation}\")\n\n", "CodeBleu_score": 42.20063370374166}, {"code": "def get_activation(activation: Literal[\"elu\", \"snake\", \"none\"], antialias=False, channels=None) -> nn.Module\n\t\n    if activation == \"elu\":\n        return Activation1d(nn.ELU(), antialias)\n    elif activation == \"snake\":\n        return Activation1d(SnakeBeta(channels), antialias)\n    elif activation == \"none\":\n        return Activation1d(nn.Identity(), antialias)\n    else:\n        raise ValueError(\"Invalid activation type: {}\".format(activation))", "CodeBleu_score": 42.2987512927482}, {"code": "def get_activation(activation: Literal[\"elu\", \"snake\", \"none\"], antialias=False, channels=None) -> nn.Module\n\t\n    if activation == \"elu\":\n        return nn.ELU(inplace=True)\n    if activation == \"snake\":\n        assert channels is not None\n        return Activation1d(SnakeBeta(channels))\n    if activation == \"none\":\n        return nn.Identity()\n    raise ValueError(f\"Unknown activation: {activation}\")", "CodeBleu_score": 40.63530472276559}]}
{"instance_id": "ef1714c79e51", "generation_result": [{"code": "def apply_rotary_pos_emb(t, freqs, scale = 1)\n\t\n    if t.dtype in (torch.float16, torch.bfloat16):\n        t = t.float()\n    freqs = freqs.to(t)\n    t_orig = t\n    sin_cached = None\n    cos_cached = None\n    if len(t.shape) > 2:\n        t = t.reshape(-1, t.shape[-1])\n    if len(freqs.shape) > 1:\n        freqs = freqs.reshape(-1)\n    if t.is_cuda:\n        if sin_cached is None:\n            sin_cached = torch.empty((t.shape[0], freqs.shape[0]), device = t.device)\n        if cos_cached is None:\n            cos_cached = torch.empty((t.shape[0], freqs.shape[0]), device = t.device)\n        torch.sin(freqs.to(t.device) * t, out = sin_cached)\n        torch.cos(freqs.to(t.device) * t, out = cos_cached)\n    else:\n        sin_cached = torch.sin(freqs * t)\n        cos_cached = torch.cos(freqs * t)\n    t = torch.cat((cos_cached, sin_cached), dim = -1)\n    if scale is not None:\n        t *= scale\n    if len(t_orig.shape) > 2:\n        t = t.reshape(t_orig.shape)\n    return t\n\n", "CodeBleu_score": 36.86962284411437}, {"code": "def apply_rotary_pos_emb(t, freqs, scale = 1)\n\t\n    if t.dtype in (torch.float16, torch.bfloat16):\n        t = t.float()\n    freqs = freqs.reshape(-1, 1, 1)\n    t_orig = t\n    t = rearrange(t, '... (j d) -> ... j d', j = 2)\n    sin_cos = torch.stack((t[..., 0::2].sin(), t[..., 1::2].cos()), dim = -1)\n    return torch.cat((sin_cos * freqs * scale, t[..., 1::2]), dim = -1)\n\n", "CodeBleu_score": 24.733736610075614}, {"code": "def apply_rotary_pos_emb(t, freqs, scale = 1)\n\t\n    if isinstance(freqs, torch.Tensor):\n        freqs = freqs.to(t.device)\n    if t.dtype in (torch.float16, torch.bfloat16):\n        t = t.float()\n    t_orig = t\n    if t.dim() == 3:\n        t = t.unsqueeze(1)\n    t_shape = t.shape\n    t = t.reshape(-1, t_shape[-1])\n    sin_cached, cos_cached = _sincos(freqs)\n    t = torch.einsum('... d, f d -> ... f d', t, cos_cached)\n    t = torch.stack((-t[..., 1::2], t[..., ::2]), dim = -1)\n    t = t.reshape(t_shape)\n    t = t.to(t_orig.dtype)\n    return t\n\n", "CodeBleu_score": 36.946362844646664}]}
{"instance_id": "a7a5f178c6f6", "generation_result": [{"code": "def idx2vq(self, idx, dim=-1)\n\t\n    embeddings = self.codebook[idx]\n    if dim != -1:\n        embeddings = embeddings.moveaxis(dim, -1)\n    return embeddings\n\n\n", "CodeBleu_score": 31.098443999765756}, {"code": "def idx2vq(self, idx, dim=-1)\n\t\n    if idx.shape[dim] != self.codebook.shape[0]:\n        raise ValueError(f'Invalid index: {idx.shape[dim]} != {self.codebook.shape[0]}')\n    embeddings = self.codebook[idx]\n    if dim != -1:\n        embeddings = embeddings.permute(0, *range(2, embeddings.ndim), 1)\n    return embeddings", "CodeBleu_score": 41.44711566732866}, {"code": "def idx2vq(self, idx, dim=-1)\n\t\n    vq = self.codebook[idx]\n    if dim != -1:\n        vq = vq.permute(dim, *range(dim + 1, vq.ndim))\n    return vq\n\n\n", "CodeBleu_score": 35.40582515173817}]}
{"instance_id": "4104c041fb20", "generation_result": [{"code": "def encode(self, x, quantize=False)\n\t\n    if self.use_checkpoint:\n        return self.forward_checkpoint(x, quantize=quantize)\n    else:\n        return self.forward(x, quantize=quantize)\n\n", "CodeBleu_score": 16.699347992334047}, {"code": "def encode(self, x, quantize=False)\n\t\n    # x = self.pre_conv(x)\n    x = self.conv_blocks(x)\n    x = self.pre_res_blocks(x)\n    x = self.res_blocks(x)\n    x = self.post_res_blocks(x)\n    x = self.post_conv_blocks(x)\n    if quantize:\n        x, (vq_loss, commit_loss), indices = self.forward(x)\n        return x, (vq_loss, commit_loss), indices\n    else:\n        return x\n\n", "CodeBleu_score": 30.021724625243586}, {"code": "def encode(self, x, quantize=False)\n\t\n    if self.blocks[0].get_num_layers() == 0:\n        raise ValueError(\"The first block must have at least one layer.\")\n    if self.blocks[-1].get_num_layers() == 0:\n        raise ValueError(\"The last block must have at least one layer.\")\n    if quantize and not self.quantize:\n        raise ValueError(\"Quantization is not enabled.\")\n    if not quantize and self.quantize:\n        raise ValueError(\"Quantization is enabled but quantize is False.\")\n    if quantize and self.quantize_in_channel != x.size(1):\n        raise ValueError(\"Quantization in channel does not match input channel.\")\n    if quantize and self.quantize_out_channel != self.blocks[-1].out_channel:\n        raise ValueError(\"Quantization out channel does not match output channel.\")\n    if quantize and self.quantize_in_channel != self.blocks[-1].out_channel:\n        raise ValueError(\"Quantization in channel does not match output channel.\")\n\n    quantize_losses = None\n    quantize_indices = None\n    if quantize:\n        quantize_losses = [None] * len(self.blocks)\n        quantize_indices = [None] * len(self.blocks)\n\n    x = self.blocks[0](x)\n    for i, block in enumerate(self.blocks[1:-1]):\n        x = block(x)\n        if quantize:\n            x, quantize_loss, quantize_index = self.blocks[i + 1].forward(x, get_losses=True)\n            quantize_losses[i + 1] = quantize_loss\n            quantize_indices[i + 1] = quantize_index\n    x = self.blocks[-1](x)\n    if quantize:\n        x, quantize_loss, quantize_index = self.blocks[-1].forward(x, get_losses=True)\n        quantize_losses[-1] = quantize_loss\n        quantize_indices[-1] = quantize_index\n    if quantize:\n        return x, quantize_losses,", "CodeBleu_score": 23.771663328817098}]}
{"instance_id": "403c52ba2d3b", "generation_result": [{"code": "def ema_scope(self, context=None)\n\t\n    if context is not None:\n        context.ema_params = [param.clone() for param in self.parameters()]\n    self.store(self.parameters())\n    self.copy_to(self)\n    if context is not None:\n        context.ema_switched = True\n        context.ema_params = None\n    return self\n\n", "CodeBleu_score": 37.14491363294706}, {"code": "def ema_scope(self, context=None)\n\t\n    if context is None:\n        return EMAContextManager(self)\n    else:\n        with EMAContextManager(self) as ema:\n            context(ema)\n\n", "CodeBleu_score": 6.840562789393517}, {"code": "def ema_scope(self, context=None)\n\t\n    if self.ema_enabled:\n        if context is not None:\n            self.store(self.parameters())\n            self.copy_to(context)\n            self.logger.log(f\"EMA weights applied in {context.__class__.__name__}\")\n        else:\n            self.restore(self.parameters())\n            self.logger.log(f\"EMA weights restored\")\n\n", "CodeBleu_score": 35.72891160916399}]}
{"instance_id": "94b7467f98e4", "generation_result": [{"code": "def post_attention(self, attn, x, gate_msa, shift_mlp, scale_mlp, gate_mlp)\n\t\n    x = x + attn\n    x = self.norm1(x)\n    x = self.modulate(x, shift_mlp, scale_mlp)\n    x = self.mlp(x)\n    x = x * gate_mlp.unsqueeze(1)\n    return x\n\n", "CodeBleu_score": 45.76567306839091}, {"code": "def post_attention(self, attn, x, gate_msa, shift_mlp, scale_mlp, gate_mlp)\n\t\n    x = x + attn * gate_msa\n    x = self.norm1(x)\n    x = self.modulate(x, shift_mlp, scale_mlp)\n    x = self.mlp(x)\n    x = x * gate_mlp\n    return x\n\n", "CodeBleu_score": 39.89670504962427}, {"code": "def post_attention(self, attn, x, gate_msa, shift_mlp, scale_mlp, gate_mlp)\n\t\n    x = x + attn @ self.post_attention(x) * gate_msa\n    x = self.modulate(x, shift_mlp, scale_mlp)\n    x = self.mlp(x) * gate_mlp\n    return x\n\n", "CodeBleu_score": 37.414758438331766}]}
{"instance_id": "d9fdc05542a2", "generation_result": [{"code": "def _block_mixing(context, x, context_block, x_block, c)\n\t\n    context_qkv = context_block.pre_attention(context)\n    x_qkv = x_block.pre_attention(x)\n    qkv = (torch.cat([context_qkv[0], x_qkv[0]], dim=1),\n           torch.cat([context_qkv[1], x_qkv[1]], dim=1),\n           torch.cat([context_qkv[2], x_qkv[2]], dim=1))\n    context_q, x_q = torch.chunk(qkv[0], 2, dim=1)\n    context_k, x_k = torch.chunk(qkv[1], 2, dim=1)\n    context_v, x_v = torch.chunk(qkv[2], 2, dim=1)\n    context_qkv = (context_q, context_k, context_v)\n    x_qkv = (x_q, x_k, x_v)\n    context_qkv = optimized_attention(context_qkv, c.num_heads)\n    x_qkv = optimized_attention(x_qkv, c.num_heads)\n    context_q, x_q = torch.chunk(context_qkv[0], 2, dim=1)\n    context_k, x_k = torch.chunk(context_qkv[1], 2, dim=1)\n    context_v, x_v = torch.chunk(context_qkv[2], 2, dim=1)\n    context_qkv = (context_q, context_k, context_v)\n    x_qkv = (x_q, x_k, x_v)\n    context = context_block.post_attention(context_qkv[0])\n    x = x_block.post_attention(x_qkv[0])\n    return context, x\n\n", "CodeBleu_score": 22.15561584964425}, {"code": "def _block_mixing(context, x, context_block, x_block, c)\n\t\n    qkv_context = context_block.pre_attention(context)\n    qkv_x = x_block.pre_attention(x)\n    qkv = torch.cat([qkv_context[0], qkv_x[0]], dim=1)\n    k = torch.cat([qkv_context[1], qkv_x[1]], dim=1)\n    v = torch.cat([qkv_context[2], qkv_x[2]], dim=1)\n    qkv = optimized_attention(qkv, c)\n    context_attention, x_attention = qkv[0].split([context.shape[1], x.shape[1]], dim=1)\n    context = context_block.post_attention(context_attention)\n    x = x_block.post_attention(x_attention)\n    return context, x\n\n", "CodeBleu_score": 20.201882787854057}, {"code": "def _block_mixing(context, x, context_block, x_block, c)\n\t\n    qkv_context = context_block.pre_attention(context)\n    qkv_x = x_block.pre_attention(x)\n    qkv = (torch.cat([qkv_context[0], qkv_x[0]], dim=-1),\n           torch.cat([qkv_context[1], qkv_x[1]], dim=-1),\n           torch.cat([qkv_context[2], qkv_x[2]], dim=-1))\n    attention_results = optimized_attention(qkv, c.num_heads)\n    context_attention, x_attention = torch.split(attention_results, (qkv_context[0].shape[1], qkv_x[0].shape[1]), dim=-2)\n    context = context_block.post_attention(context_attention)\n    x = x_block.post_attention(x_attention)\n    return context, x\n\n", "CodeBleu_score": 21.590767417210206}]}
{"instance_id": "b219bce30160", "generation_result": [{"code": "def get_timestep_embedding(timesteps, embedding_dim)\n\t\n    assert len(timesteps.shape) == 1, 'Timesteps input to get_timestep_embedding() must be a one-dimensional tensor'\n    half_dim = embedding_dim // 2\n    emb = math.log(10000) / (half_dim - 1)\n    emb = torch.exp(torch.arange(half_dim, dtype=torch.float32) * -emb)\n    emb = timesteps[:, None] * emb[None, :]\n    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=-1)\n    if embedding_dim % 2 == 1:\n        emb = torch.nn.functional.pad(emb, (0, 1, 0, 0))\n    return emb\n\n\n", "CodeBleu_score": 73.08549046800447}, {"code": "def get_timestep_embedding(timesteps, embedding_dim)\n\t\n    assert len(timesteps.shape) == 1, \"Timesteps input to embedding function should be one-dimensional.\"\n    half_dim = embedding_dim // 2\n    emb = math.log(10000) / (half_dim - 1)\n    emb = torch.exp(torch.arange(half_dim, dtype=torch.float32) * -emb)\n    emb = timesteps[:, None] * emb[None, :]\n    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=-1)\n    if embedding_dim % 2 == 1:\n        emb = torch.nn.functional.pad(emb, (0, 1, 0, 0))\n    return emb\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 73.61798532365378}, {"code": "def get_timestep_embedding(timesteps, embedding_dim)\n\t\n    assert len(timesteps.shape) == 1, \"Timesteps must be one-dimensional\"\n    half_dim = embedding_dim // 2\n    emb = math.log(10000) / (half_dim - 1)\n    emb = torch.exp(torch.arange(half_dim, dtype=torch.float32) * -emb)\n    emb = timesteps[:, None] * emb[None, :]\n    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n    if embedding_dim % 2 == 1:\n        emb = torch.nn.functional.pad(emb, (0, 1), value=0)\n    return emb\n\n\n", "CodeBleu_score": 74.4228901344594}]}
{"instance_id": "6ae8453c86be", "generation_result": [{"code": "def pytorch_attention(q, k, v)\n\t\n    q = q.reshape(q.shape[0], q.shape[1], -1).permute(0, 2, 1)\n    k = k.reshape(k.shape[0], k.shape[1], -1).permute(0, 2, 1)\n    v = v.reshape(v.shape[0], v.shape[1], -1).permute(0, 2, 1)\n\n    try:\n        r1 = torch.bmm(q, k) * (int(q.shape[-1]) ** (-0.5))\n        r1 = torch.nn.functional.softmax(r1, dim=2)\n        r1 = torch.bmm(r1, v)\n    except model_management.OOM_EXCEPTION as e:\n        logging.warning(\"out of memory error, switching to slice attention\")\n        r1 = slice_attention(q, k, v)\n\n    r1 = r1.permute(0, 2, 1).reshape(q.shape)\n\n    return r1", "CodeBleu_score": 29.924538620888676}, {"code": "def pytorch_attention(q, k, v)\n\t\n    q = q.reshape(q.shape[0], q.shape[1], -1)\n    k = k.reshape(k.shape[0], k.shape[1], -1).permute(0, 2, 1)\n    v = v.reshape(v.shape[0], v.shape[1], -1)\n\n    try:\n        r1 = torch.bmm(q, k)\n        r1 = torch.nn.functional.softmax(r1, dim=2).permute(0,2,1)\n        r1 = torch.bmm(v, r1)\n    except model_management.OOM_EXCEPTION as e:\n        r1 = slice_attention(q, k, v)\n    return r1.reshape(q.shape[0], q.shape[1], -1)\n\n", "CodeBleu_score": 24.03066050716334}, {"code": "def pytorch_attention(q, k, v)\n\t\n    try:\n        q = q.reshape(q.shape[0], q.shape[1], -1)\n        q = q.permute(0, 2, 1)\n\n        k = k.reshape(k.shape[0], k.shape[1], -1)\n        k = k.permute(0, 2, 1)\n\n        v = v.reshape(v.shape[0], v.shape[1], -1)\n\n        scale = (int(q.shape[-1])**(-0.5))\n        attn = torch.bmm(q, k) * scale\n        attn = torch.nn.functional.softmax(attn, dim=2)\n        attn = torch.bmm(attn, v)\n\n        attn = attn.permute(0, 2, 1)\n        attn = attn.reshape(attn.shape[0], attn.shape[1], -1)\n\n        return attn\n    except model_management.OOM_EXCEPTION:\n        logging.warning(\"out of memory error, switching to slice attention\")\n        return slice_attention(q, k, v)", "CodeBleu_score": 33.11863064816319}]}
{"instance_id": "fe4433c987ad", "generation_result": [{"code": "def apply_control(h, control, name)\n\t\n    if control and name:\n        if control in h.controls:\n            h.controls.remove(control)\n            try:\n                h.add_control(control)\n            except Exception:\n                logger.warning(f'Failed to add control {control} to {name}')\n    return h\n\n", "CodeBleu_score": 31.962263222558445}, {"code": "def apply_control(h, control, name)\n\t\n    if control and name in control:\n        c = control.pop(name)\n        if not h.add_control(c):\n            print('Warning: could not add control ' + name + ' to ' + h.name + ' because of ' + c.error_msg)\n    return h\n\n", "CodeBleu_score": 33.2339301377595}, {"code": "def apply_control(h, control, name)\n\t\n    if control and name in control:\n        try:\n            h[name] = control.pop(name)\n        except KeyError:\n            logging.warning(f'Could not set {name} to {h[name]}')\n    return h\n\n", "CodeBleu_score": 30.89004993528009}]}
{"instance_id": "56da4547c101", "generation_result": [{"code": "def get_codebook_entry(self, indices, shape)\n\t\n    # get quantized latent vectors\n    indices = torch.stack(indices, dim=-1)\n    quantized_latents = self.embedding(indices)\n\n    # reshape and permute quantized latents\n    if shape is not None:\n        quantized_latents = quantized_latents.view(shape)\n        quantized_latents = quantized_latents.permute(0, 3, 1, 2)\n\n    return quantized_latents", "CodeBleu_score": 33.40700577522356}, {"code": "def get_codebook_entry(self, indices, shape)\n\t\n    if shape is None:\n        shape = self.input_shape\n    indices = tf.one_hot(indices, shape[0] * shape[1])\n    indices = tf.reshape(indices, [-1, shape[0] * shape[1]])\n    quantized_vectors = tf.matmul(indices, self.embedding.weights[0], transpose_b=True)\n    quantized_vectors = tf.reshape(quantized_vectors, [-1, shape[0], shape[1], self.embedding.embeddings.shape[1]])\n    quantized_vectors = tf.transpose(quantized_vectors, [0, 2, 1, 3])\n    return quantized_vectors", "CodeBleu_score": 36.781403471477724}, {"code": "def get_codebook_entry(self, indices, shape)\n\t\n    indices = torch.stack([indices, torch.zeros_like(indices)], dim=-1).long()\n    shape = list(indices.shape[:-1]) + list(shape)\n    quants = self.quantize(indices).view(shape)\n    return quants\n\n\n", "CodeBleu_score": 18.770869179169317}]}
{"instance_id": "548f8a1fb37d", "generation_result": [{"code": "def make_noise(self)\n\t\n    noise = []\n    noise.append(torch.randn(4, 4).to(self.device))\n    for i in range(3, self.log_size + 3):\n        noise.append(torch.randn(2 ** i, 2 ** i).to(self.device))\n        noise.append(torch.randn(2 ** i, 2 ** i).to(self.device))\n    return noise\n\n\n", "CodeBleu_score": 32.565237243854526}, {"code": "def make_noise(self)\n\t\n    noise = [torch.randn(4, 4, device=self.device)]\n    for i in range(3, self.log_size + 3):\n        noise.append(torch.randn(2 ** i, 2 ** i, device=self.device))\n        noise.append(torch.randn(2 ** i, 2 ** i, device=self.device))\n    return noise\n\n\n", "CodeBleu_score": 32.75158523310504}, {"code": "def make_noise(self)\n\t\n    noise = [torch.randn(4, 4).to(self.device)]\n    for i in range(3, self.log_size + 3):\n        noise.append(torch.randn(2 ** i, 2 ** i).to(self.device))\n        noise.append(torch.randn(2 ** i, 2 ** i).to(self.device))\n    return noise\n\n", "CodeBleu_score": 31.065253658226137}]}
{"instance_id": "42fd805563c6", "generation_result": [{"code": "def mean_latent(self, num_latent)\n\t\n    # Create random latent inputs\n    z = torch.randn(num_latent, self.style_dim)\n    # Process the random latent inputs through the style MLP\n    z = self.style(z)\n    # Compute the mean of the processed latents along the first dimension\n    z = z.mean(dim=0)\n    # Return the mean latent vector\n    return z\n\n\n", "CodeBleu_score": 13.396386213093509}, {"code": "def mean_latent(self, num_latent)\n\t\n    mean_latent = torch.randn(num_latent, self.latent_dim, device=self.device)\n    mean_latent = self.style(mean_latent)\n    mean_latent = mean_latent.mean(dim=0)\n    return mean_latent\n\n\n", "CodeBleu_score": 26.65395718797081}, {"code": "def mean_latent(self, num_latent)\n\t\n    mean_latent = torch.randn(num_latent, self.latent_dim, device=self.device)\n    mean_latent = self.style_mlp(mean_latent)\n    mean_latent = torch.mean(mean_latent, dim=0)\n    return mean_latent\n\n\n", "CodeBleu_score": 27.21278639954996}]}
{"instance_id": "2f485fd924af", "generation_result": [{"code": "def color_jitter(img, shift)\n\t\n    jitter = np.random.uniform(low=-shift, high=shift, size=img.shape)\n    img = img + jitter\n    img = np.clip(img, 0, 1)\n    return img\n\n", "CodeBleu_score": 53.79861725238503}, {"code": "def color_jitter(img, shift)\n\t\n    img = img + torch.rand_like(img) * shift\n    img = torch.clamp(img, 0, 1)\n    return img\n\n\n", "CodeBleu_score": 35.89443189877086}, {"code": "def color_jitter(img, shift)\n\t\n    jitter = np.random.uniform(-shift, shift, size=3)\n    img = img + jitter\n    img = np.clip(img, 0, 1)\n    return img\n\n\n\n", "CodeBleu_score": 55.63164026013943}]}
{"instance_id": "c87ccf1fbf48", "generation_result": [{"code": "def feed_data(self, data)\n\t\n    self.lq = data['lq']\n    if 'gt' in data:\n        self.gt = data['gt']\n    if 'l_eye' in data:\n        self.l_eye = data['l_eye']\n        self.r_eye = data['r_eye']\n        self.mouth = data['mouth']\n\n\n", "CodeBleu_score": 42.90189828846784}, {"code": "def feed_data(self, data)\n\t\n    self.lq = data['lq']\n    if 'gt' in data:\n        self.gt = data['gt']\n    if 'left_eye' in data:\n        self.left_eye = data['left_eye']\n    if 'right_eye' in data:\n        self.right_eye = data['right_eye']\n    if 'mouth' in data:\n        self.mouth = data['mouth']\n\n\n\n", "CodeBleu_score": 45.45133321424938}, {"code": "def feed_data(self, data)\n\t\n    self.lq = data['lq']\n    if 'gt' in data:\n        self.gt = data['gt']\n    if 'left_eye' in data:\n        self.left_eye = data['left_eye']\n    if 'right_eye' in data:\n        self.right_eye = data['right_eye']\n    if 'mouth' in data:\n        self.mouth = data['mouth']\n    self.lq = self.lq.to(self.device)\n    if 'gt' in data:\n        self.gt = self.gt.to(self.device)\n    if 'left_eye' in data:\n        self.left_eye = self.left_eye.to(self.device)\n    if 'right_eye' in data:\n        self.right_eye = self.right_eye.to(self.device)\n    if 'mouth' in data:\n        self.mouth = self.mouth.to(self.device)\n    if self.lq.dim() == 5:\n        self.lq = self.lq.squeeze(1)\n    if self.gt.dim() == 5:\n        self.gt = self.gt.squeeze(1)\n\n\n\n", "CodeBleu_score": 38.95823911632755}]}
{"instance_id": "8f8c7cb13037", "generation_result": [{"code": "def construct_img_pyramid(self)\n\t\n    # Get the ground truth image\n    gt_img = self.gt_img\n    # Initialize the image pyramid list\n    img_pyramid = []\n    # Insert the ground truth image at the beginning of the pyramid list\n    img_pyramid.insert(0, gt_img)\n    # Loop over the number of levels in the pyramid\n    for i in range(self.num_levels - 1):\n        # Downscale the image using bilinear interpolation\n        downscaled_img = cv2.resize(img_pyramid[i], (0, 0), fx=0.5, fy=0.5, interpolation=cv2.INTER_LINEAR)\n        # Insert the downscaled image at the beginning of the pyramid list\n        img_pyramid.insert(0, downscaled_img)\n    # Return the constructed image pyramid\n    return img_pyramid\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 36.57466511181919}, {"code": "def construct_img_pyramid(self)\n\t\n\n    # Initialize the image pyramid\n    img_pyramid = []\n\n    # Insert the ground truth image at the beginning of the pyramid\n    img_pyramid.insert(0, self.img)\n\n    # Initialize the current image\n    current_img = self.img\n\n    # Downscale the image and insert it at the beginning of the pyramid\n    for i in range(self.num_scales - 1):\n        # Downscale the image\n        current_img = cv2.resize(current_img, (0, 0), fx=0.5, fy=0.5)\n\n        # Insert the downscaled image at the beginning of the pyramid\n        img_pyramid.insert(0, current_img)\n\n    # Return the constructed image pyramid\n    return img_pyramid\n\n\n\n\n", "CodeBleu_score": 34.11350122501652}, {"code": "def construct_img_pyramid(self)\n\t\n    img_pyramid = [self.img.copy()]\n    for i in range(self.num_scales):\n        img_pyramid.insert(0, cv2.resize(img_pyramid[0], None, fx=0.5, fy=0.5, interpolation=cv2.INTER_LINEAR))\n    return img_pyramid\n\n\n", "CodeBleu_score": 18.14735165729648}]}
{"instance_id": "51247274d40c", "generation_result": [{"code": "def save(self, epoch, current_iter)\n\t\n    if self.opt.save_epoch_freq > 0 and (epoch % self.opt.save_epoch_freq == 0 or epoch == self.opt.niter):\n        print('saving the model at epoch %d' % epoch)\n        self.save_network(self.netG, 'G', epoch, current_iter)\n        self.save_network(self.netG_ema, 'G_ema', epoch, current_iter)\n        self.save_network(self.netD, 'D', epoch, current_iter)\n        if self.opt.use_face_discriminator:\n            self.save_network(self.netD_left, 'D_left', epoch, current_iter)\n            self.save_network(self.netD_right, 'D_right', epoch, current_iter)\n            self.save_network(self.netD_mouth, 'D_mouth', epoch, current_iter)\n        self.save_training_state(epoch, current_iter)\n\n\n\n", "CodeBleu_score": 50.47766358846746}, {"code": "def save(self, epoch, current_iter)\n\t\n    self.generator.save(epoch, current_iter)\n    self.generator_ema.save(epoch, current_iter)\n    self.discriminator.save(epoch, current_iter)\n    if self.use_face_discriminators:\n        self.face_discriminator_left_eye.save(epoch, current_iter)\n        self.face_discriminator_right_eye.save(epoch, current_iter)\n        self.face_discriminator_mouth.save(epoch, current_iter)\n    self.save_training_state(epoch, current_iter)\n\n\n", "CodeBleu_score": 37.917733598185634}, {"code": "def save(self, epoch, current_iter)\n\t\n    # Save the generator network\n    self.save_network(self.generator, \"generator\", epoch, current_iter)\n\n    # Save the EMA version of the generator network\n    self.save_network(self.ema_generator, \"ema_generator\", epoch, current_iter)\n\n    # Save the discriminator network\n    self.save_network(self.discriminator, \"discriminator\", epoch, current_iter)\n\n    # Save the facial discriminators\n    if self.facial_discriminators:\n        self.save_network(self.facial_discriminators[\"left_eye\"], \"left_eye\", epoch, current_iter)\n        self.save_network(self.facial_discriminators[\"right_eye\"], \"right_eye\", epoch, current_iter)\n        self.save_network(self.facial_discriminators[\"mouth\"], \"mouth\", epoch, current_iter)\n\n    # Save the overall training state\n    self.save_training_state(epoch, current_iter)\n\n\n", "CodeBleu_score": 53.11782241806604}]}
{"instance_id": "afa0dc8128c1", "generation_result": [{"code": "def clean_folder(folder)\n\t\n    import os\n    import shutil\n    for filename in os.listdir(folder):\n        file_path = os.path.join(folder, filename)\n        try:\n            if os.path.isfile(file_path) or os.path.islink(file_path):\n                os.unlink(file_path)\n            elif os.path.isdir(file_path):\n                shutil.rmtree(file_path)\n        except Exception as e:\n            print('Failed to delete %s. Reason: %s' % (file_path, e))\n\n\n", "CodeBleu_score": 64.8786320172984}, {"code": "def clean_folder(folder)\n\t\n    import os\n    import shutil\n    for filename in os.listdir(folder):\n        file_path = os.path.join(folder, filename)\n        try:\n            if os.path.isfile(file_path) or os.path.islink(file_path):\n                os.unlink(file_path)\n            elif os.path.isdir(file_path):\n                shutil.rmtree(file_path)\n        except Exception as e:\n            print('Failed to delete %s. Reason: %s' % (file_path, e))\n\n\n\n\n", "CodeBleu_score": 64.8786320172984}, {"code": "def clean_folder(folder)\n\t\n    import os\n    import shutil\n    for filename in os.listdir(folder):\n        file_path = os.path.join(folder, filename)\n        try:\n            if os.path.isfile(file_path) or os.path.islink(file_path):\n                os.unlink(file_path)\n            elif os.path.isdir(file_path):\n                shutil.rmtree(file_path)\n        except Exception as e:\n            print('Failed to delete %s. Reason: %s' % (file_path, e))\n\n\n", "CodeBleu_score": 64.8786320172984}]}
{"instance_id": "53588b481bdd", "generation_result": [{"code": "def next_batch(self, sample_size=1)\n\t\n    if sample_size > len(self.memory):\n        sample_size = len(self.memory)\n    samples = random.sample(self.memory, sample_size)\n    for sample in samples:\n        self.memory.remove(sample)\n    return samples\n\n\n", "CodeBleu_score": 49.54632377040237}, {"code": "def next_batch(self, sample_size=1)\n\t\n    if sample_size > len(self.buffer):\n        sample_size = len(self.buffer)\n    samples = random.sample(self.buffer, sample_size)\n    self.buffer = [x for x in self.buffer if x not in samples]\n    return samples\n\n\n\n\n", "CodeBleu_score": 71.7161265042053}, {"code": "def next_batch(self, sample_size=1)\n\t\n    if sample_size > len(self.buffer):\n        sample_size = len(self.buffer)\n\n    # Randomly select indices from the list of available experiences\n    indices = np.random.choice(len(self.buffer), size=sample_size, replace=False)\n\n    # Retrieve the corresponding experiences and remove them from the list\n    experiences = [self.buffer[i] for i in indices]\n    self.buffer = np.delete(self.buffer, indices, axis=0)\n\n    # Unpack the experiences and return them as a tuple\n    states, actions, rewards, next_states, dones = zip(*experiences)\n    return np.array(states), np.array(actions), np.array(rewards), np.array(next_states), np.array(dones)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 40.20114845474808}]}
{"instance_id": "6bfa6a4e621c", "generation_result": [{"code": "def find_estimated_top_configs(self)\n\t\n    all_configs = self.all_configs\n    flattened_configs = []\n    for config in all_configs:\n        flattened_config = []\n        for key, value in config.items():\n            if isinstance(value, (int, float)):\n                flattened_config.append(value)\n            elif isinstance(value, dict):\n                for sub_key, sub_value in value.items():\n                    if isinstance(sub_value, (int, float)):\n                        flattened_config.append(sub_value)\n        flattened_configs.append(flattened_config)\n    flattened_configs = np.array(flattened_configs)\n    flattened_configs = np.nan_to_num(flattened_configs)\n    flattened_configs = np.array([np.array(config) for config in flattened_configs])\n    flattened_configs = np.array([config for config in flattened_configs if len(config) > 0])\n    if len(flattened_configs) == 0:\n        return []\n    cost_model = self.cost_model\n    cost_model.fit(flattened_configs)\n    cost_model.predict(flattened_configs)\n    cost_model.get_estimates()\n    cost_model.get_errors()\n    cost_model.get_errors_std()\n    cost_model.get_errors_mean()\n    cost_model.get_errors_median()\n    cost_model.get_errors_max()\n    cost_model.get_errors_min()\n    cost_model.get_errors_range()\n    cost_model.get_errors_iqr()\n    cost_model.get_errors_mad()\n    cost_model.get_errors_var()\n    cost_model.get_errors_std()\n    cost_model.get_errors_sem()\n    cost_model.get_errors_skew()\n    cost_model.get_errors_kurtosis()\n    cost_model.get_errors_cov()\n   ", "CodeBleu_score": 31.065803338535776}, {"code": "def find_estimated_top_configs(self)\n\t\n    # Flatten all configurations in all_configs\n    flattened_configs = [self.flatten_config(config) for config in self.all_configs]\n\n    # Collect numerical values from flattened configurations\n    numerical_values = [float(value) for config in flattened_configs for value in config.values() if isinstance(value, (int, float))]\n\n    # Convert numerical values into a numpy array\n    numerical_array = np.array(numerical_values)\n\n    # Use the cost model to predict estimates for the configurations\n    estimates = self.cost_model.predict(numerical_array)\n\n    # Sort the configurations based on the estimated cost\n    sorted_indices = np.argsort(estimates)\n\n    # Return the indices of the top configurations\n    return sorted_indices[:self.top_configs]", "CodeBleu_score": 22.506473544575623}, {"code": "def find_estimated_top_configs(self)\n\t\n    all_configs = self.get_all_configs()\n    all_configs_flattened = [self.flatten_config(config) for config in all_configs]\n    all_configs_flattened_numerical = [self.convert_config_to_numerical(config) for config in all_configs_flattened]\n    all_configs_flattened_numerical_array = np.array(all_configs_flattened_numerical)\n    all_configs_flattened_numerical_array_sorted = self.sort_configs_by_metric(all_configs_flattened_numerical_array)\n    return all_configs_flattened_numerical_array_sorted\n\n\n", "CodeBleu_score": 18.058106023329927}]}
{"instance_id": "d4cf48413a57", "generation_result": [{"code": "def flatten(d, parent_key='', sep='_')\n\t\n    items = []\n    for k, v in d.items():\n        new_key = parent_key + sep + k if parent_key else k\n        if isinstance(v, collections.abc.MutableMapping):\n            items.extend(flatten(v, new_key, sep=sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)\n\n", "CodeBleu_score": 82.56863782179195}, {"code": "def flatten(d, parent_key='', sep='_')\n\t\n    items = []\n    for k, v in d.items():\n        new_key = parent_key + sep + k if parent_key else k\n        if isinstance(v, collections.abc.MutableMapping):\n            items.extend(flatten(v, new_key, sep=sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)\n\n", "CodeBleu_score": 82.56863782179195}, {"code": "def flatten(d, parent_key='', sep='_')\n\t\n    items = []\n    for k, v in d.items():\n        new_key = parent_key + sep + k if parent_key else k\n        if isinstance(v, collections.abc.MutableMapping):\n            items.extend(flatten(v, new_key, sep=sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)\n\n", "CodeBleu_score": 82.56863782179195}]}
{"instance_id": "c2a514249c25", "generation_result": [{"code": "def run_after_tuning(self)\n\t\n    if self.optimal_cmd is not None:\n        try:\n            subprocess.Popen(self.optimal_cmd, shell=True)\n        except Exception as e:\n            logging.error('Failed to run command after tuning: {}'.format(e))\n            return\n        logging.info('Ran command after tuning: {}'.format(self.optimal_cmd))\n    else:\n        logging.info('No optimal command found after tuning.')", "CodeBleu_score": 33.27998243131995}, {"code": "def run_after_tuning(self)\n\t\n    if self.optimal_cmd:\n        self.logger.info(f\"Running command after tuning: {self.optimal_cmd}\")\n        proc = subprocess.Popen(self.optimal_cmd, shell=True)\n        proc.wait()\n        self.logger.info(\"Command after tuning executed successfully\")\n    else:\n        self.logger.info(\"No optimal command found after tuning\")", "CodeBleu_score": 50.88441818592512}, {"code": "def run_after_tuning(self)\n\t\n    if self.optimal_cmd is not None:\n        self.logger.info(\"Executing optimal command: {}\".format(self.optimal_cmd))\n        self.subprocess_popen(self.optimal_cmd)\n    else:\n        self.logger.info(\"No optimal command found\")\n\n", "CodeBleu_score": 31.944176072657456}]}
{"instance_id": "61802de52df0", "generation_result": [{"code": "def run_job(self, exp: dict, reservations)\n\t\n    try:\n        # Set the master port and result directory for the experiment\n        exp['master_port'] = self.get_master_port()\n        exp['result_dir'] = self.get_result_dir(exp)\n\n        # Update the user arguments based on argument mappings\n        if 'argument_mappings' in exp:\n            for arg_mapping in exp['argument_mappings']:\n                if arg_mapping['arg_name'] in exp['user_args']:\n                    exp['user_args'][arg_mapping['arg_name']] = arg_mapping['value']\n\n        # Create a new thread to run the experiment with the specified script and arguments\n        thread = threading.Thread(target=self.run_experiment, args=(exp,))\n        thread.start()\n\n        # Store the running experiment details with a timestamp in running_experiments\n        self.running_experiments[exp['exp_id']] = {\n            'exp': exp,\n            'thread': thread,\n            'timestamp': datetime.now()\n        }\n\n        # Update the running experiments list in the database\n        self.update_running_experiments()\n\n        # Return the exp_id of the running experiment\n        return exp['exp_id']\n    except Exception as e:\n        raise e\n\n\n", "CodeBleu_score": 30.461814214692062}, {"code": "def run_job(self, exp: dict, reservations)\n\t\n    # Set the master port and result directory for the experiment\n    exp['master_port'] = self.get_free_port()\n    exp['result_dir'] = os.path.join(self.result_dir, exp['name'])\n\n    # Update the user arguments based on argument mappings\n    exp['user_args'] = self.update_user_args(exp['user_args'])\n\n    # Create a new thread to run the experiment\n    thread = threading.Thread(target=self.run_experiment, args=(exp,))\n    thread.start()\n\n    # Store the running experiment details with a timestamp\n    running_experiments[exp['name']] = {\n        'exp': exp,\n        'thread': thread,\n        'start_time': datetime.now()\n    }\n\n\n", "CodeBleu_score": 16.481521267399657}, {"code": "def run_job(self, exp: dict, reservations)\n\t\n    # Set the experiment's master port and result directory\n    exp['master_port'] = self.get_free_port()\n    exp['result_dir'] = self.get_result_dir(exp)\n\n    # Update user arguments based on argument mappings\n    exp['args'] = self.update_user_args(exp['args'])\n\n    # Create a new thread to run the experiment\n    thread = threading.Thread(target=self.run_experiment, args=(exp, reservations))\n    thread.start()\n\n    # Store the running experiment details with a timestamp\n    self.running_experiments[thread.ident] = {\n        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n        'exp': exp\n    }\n\n    # Return the thread identifier\n    return thread.ident\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 16.242489027938976}]}
{"instance_id": "4884e2e9665c", "generation_result": [{"code": "def save_exp_results_to_database(self, message, ranks=None, path=None):\n\t\n    if self.distributed:\n        if self.rank == 0:\n            if ranks is None:\n                ranks = [0]\n            for rank in ranks:\n                message[\"rank\"] = rank\n                self.save_to_json(message, path)\n    else:\n        self.save_to_json(message, path)\n\n\n", "CodeBleu_score": 18.6738931683786}, {"code": "def save_exp_results_to_database(self, message, ranks=None, path=None):\n\t\n    if self.distributed_training_initialized:\n        if self.rank == 0:\n            message = f\"{self.rank}: {message}\"\n            self.save_exp_results_to_file(message, ranks=ranks, path=path)\n    else:\n        message = f\"{self.rank}: {message}\"\n        self.save_exp_results_to_file(message, ranks=ranks, path=path)\n\n\n", "CodeBleu_score": 14.68025374257446}, {"code": "def save_exp_results_to_database(self, message, ranks=None, path=None):\n\t\n    if ranks is None:\n        ranks = [0]\n    if path is None:\n        path = os.path.join(self.log_dir, 'experiments.json')\n    if self.rank in ranks:\n        if self.distributed:\n            message = f'[Rank {self.rank}] {message}'\n        with open(path, 'a') as f:\n            json.dump(message, f)\n            f.write('", "CodeBleu_score": 28.24185786359249}]}
{"instance_id": "0429344c3c4d", "generation_result": [{"code": "def _build_tp_other_layer_map(self, layer_index: int)\n\t\n    if not self.layer_files:\n        raise ValueError(\"No layer files found.\")\n    if layer_index < 0 or layer_index >= len(self.layer_files):\n        raise ValueError(\"Invalid layer index.\")\n    layer_files = self.layer_files[layer_index]\n    layer_prefix = self.layer_prefixes[layer_index]\n    tp_degree = self.tp_degree\n    tp_degree = tp_degree if tp_degree > 0 else 1\n    layer_map = {}\n    for file_name in layer_files:\n        if not file_name.startswith(layer_prefix):\n            continue\n        file_name = file_name[len(layer_prefix):]\n        if not file_name.endswith(\".npy\"):\n            continue\n        partition_index = int(file_name) // tp_degree\n        if partition_index not in layer_map:\n            layer_map[partition_index] = []\n        layer_map[partition_index].append(file_name)\n    return layer_map\n\n\n\n", "CodeBleu_score": 34.16174555014321}, {"code": "def _build_tp_other_layer_map(self, layer_index: int)\n\t\n    if not self.layer_files:\n        raise ValueError(f\"No layer files found for layer index {layer_index}\")\n    if layer_index not in self.layer_files:\n        raise ValueError(f\"Layer index {layer_index} not found in layer files\")\n    layer_files = self.layer_files[layer_index]\n    tp_degree = self.tp_degree\n    layer_prefix = f\"{self.prefix}_layer_{layer_index}\"\n    layer_files = [\n        file for file in layer_files if file.startswith(layer_prefix)\n    ]\n    if not layer_files:\n        raise ValueError(f\"No layer files found for layer index {layer_index}\")\n    partition_map = {}\n    for file in layer_files:\n        partition_index = int(file.split(\"_\")[-1].split(\".\")[0])\n        if partition_index not in partition_map:\n            partition_map[partition_index] = []\n        partition_map[partition_index].append(file)\n    for partition_index in partition_map:\n        partition_map[partition_index] = sorted(\n            partition_map[partition_index], key=lambda x: int(x.split(\"_\")[-1].split(\".\")[0])\n        )\n    return partition_map\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 30.241398001230912}, {"code": "def _build_tp_other_layer_map(self, layer_index: int)\n\t\n    if self.layer_files is None:\n        raise ValueError(\n            \"No layer files found. Please make sure to set the layer_files property before calling this method.\"\n        )\n    if layer_index < 0 or layer_index >= len(self.layer_files):\n        raise ValueError(\n            f\"Invalid layer index {layer_index}. Please provide a valid layer index between 0 and {len(self.layer_files) - 1}.\"\n        )\n    layer_files = self.layer_files[layer_index]\n    tp_degree = self.tp_degree\n    prefix = self.tp_prefix\n    partition_files = self._partition_files(\n        layer_files, tp_degree, prefix, layer_index\n    )\n    return partition_files\n\n\n", "CodeBleu_score": 32.563067958044854}]}
{"instance_id": "1b65ecb9be93", "generation_result": [{"code": "def system_info()\n\t\n    import platform\n    import sys\n    import psutil\n\n    info = dict()\n\n    info['system'] = platform.system()\n    info['architecture'] = platform.architecture()\n    info['version'] = platform.version()\n    info['processor'] = platform.processor()\n    info['python_version'] = sys.version.split()[0]\n    info['ram'] = psutil.virtual_memory().total / 1024 / 1024\n\n    return info\n\n", "CodeBleu_score": 33.24579489667203}, {"code": "def system_info()\n\t\n    import platform\n    import sys\n    import psutil\n\n    info = {}\n    info['system'] = platform.system()\n    info['architecture'] = platform.architecture()\n    info['version'] = platform.version()\n    info['processor'] = platform.processor()\n    info['python_version'] = sys.version.split()[0]\n    info['ram'] = psutil.virtual_memory().total / (1024.0 ** 3)\n    info['cpu'] = psutil.cpu_count()\n\n    return info\n\n", "CodeBleu_score": 32.45714683885885}, {"code": "def system_info()\n\t\n    import platform\n    import sys\n\n    system = platform.system()\n    architecture = platform.architecture()\n    version = platform.version()\n    processor = platform.processor()\n    python_version = sys.version\n\n    system_info = {\n        \"Operating System\": system,\n        \"Architecture\": architecture,\n        \"Version\": version,\n        \"Processor\": processor,\n        \"Python Version\": python_version\n    }\n\n    return system_info\n\n\n\n", "CodeBleu_score": 49.04086644414875}]}
{"instance_id": "969bcc5cf3b5", "generation_result": [{"code": "def cuda_info()\n\t\n    import torch\n    cuda_available = torch.cuda.is_available()\n    cuda_version = torch.version.cuda\n    device_count = torch.cuda.device_count()\n    device_names = torch.cuda.get_device_name(0)\n    for i in range(1, device_count):\n        device_names = device_names + \", \" + torch.cuda.get_device_name(i)\n    return cuda_available, cuda_version, device_count, device_names\n\n\n\n\n", "CodeBleu_score": 46.14723625213124}, {"code": "def cuda_info()\n\t\n    import torch\n    cuda_available = torch.cuda.is_available()\n    device_count = torch.cuda.device_count()\n    device_name = torch.cuda.get_device_name(0)\n    cuda_version = torch.version.cuda\n    return [device_name, cuda_available, device_count, cuda_version]\n\n\n", "CodeBleu_score": 25.97037291603115}, {"code": "def cuda_info()\n\t\n    import torch\n    print(torch.cuda.is_available())\n    print(torch.cuda.device_count())\n    print(torch.cuda.get_device_name(0))\n    print(torch.version.cuda)\n    print(torch.cuda.get_device_properties(0))\n\n", "CodeBleu_score": 35.100033957647774}]}
{"instance_id": "a2ba908a4c70", "generation_result": [{"code": "def compute_encoder_accuracy(dataset_items, encoder_manager)\n\t\n    \n    # Get the class name key and optional class ID to class name mapping from the encoder manager's configuration\n    class_name_key = encoder_manager.get_class_name_key()\n    class_id_to_class_name_mapping = encoder_manager.get_class_id_to_class_name_mapping()\n    \n    # Initialize variables for accuracy calculation\n    class_accuracy_dict = {}\n    total_accurate_predictions = 0\n    total_predictions = 0\n    \n    # Loop through each item in the dataset\n    for item in dataset_items:\n        # Extract the audio file from the item\n        audio_file = item[\"audio_file\"]\n        \n        # Compute the embedding for the audio file\n        embedding = encoder_manager.compute_embedding(audio_file)\n        \n        # Predict the class label if criteria and mappings are available\n        if class_name_key and class_id_to_class_name_mapping:\n            class_id = encoder_manager.predict_class_id(embedding)\n            class_name = class_id_to_class_name_mapping[class_id]\n            true_class_name = item[class_name_key]\n            \n            # Record the accuracy for the class\n            if class_name not in class_accuracy_dict:\n                class_accuracy_dict[class_name] = {\"accurate_predictions\": 0, \"total_predictions\": 0}\n            class_accuracy_dict[class_name][\"total_predictions\"] += 1\n            if class_name == true_class_name:\n                class_accuracy_dict[class_name][\"accurate_predictions\"] += 1\n                total_accurate_predictions += 1\n            total_predictions += 1\n    \n    # Calculate and print the accuracy for each class\n    for class_name, class_accuracy in class_accuracy_dict.items():\n        class_accuracy_percentage = (class_accuracy[\"accurate_predictions\"] / class_accuracy[\"total_predictions\"]) * 100\n        print(f\"Class: {class_name}, Accuracy:", "CodeBleu_score": 29.257140596909036}, {"code": "def compute_encoder_accuracy(dataset_items, encoder_manager)\n\t\n    # Get the class name key and optional class ID to class name mapping\n    class_name_key = encoder_manager.config.class_name_key\n    class_id_to_class_name_mapping = encoder_manager.config.class_id_to_class_name_mapping\n\n    # Initialize counters and dictionaries for accuracy calculation\n    total_items = len(dataset_items)\n    class_accuracy_counts = {}\n    class_accuracy_sums = {}\n\n    # Iterate over each item in the dataset\n    for item in dataset_items:\n        # Extract the audio file from the item\n        audio_file = item['audio_file']\n\n        # Compute the embedding for the audio file\n        embeddings = encoder_manager.compute_embeddings(audio_file)\n\n        # Get the true class name from the item\n        true_class_name = item[class_name_key]\n\n        # Predict the class label if criteria and mappings are available\n        if class_id_to_class_name_mapping:\n            predicted_class_id = encoder_manager.predict_class_id(embeddings)\n            predicted_class_name = class_id_to_class_name_mapping[predicted_class_id]\n        else:\n            predicted_class_name = encoder_manager.predict_class_name(embeddings)\n\n        # Record the accuracy for the true class\n        if true_class_name not in class_accuracy_counts:\n            class_accuracy_counts[true_class_name] = 0\n            class_accuracy_sums[true_class_name] = 0\n        class_accuracy_counts[true_class_name] += 1\n        class_accuracy_sums[true_class_name] += int(true_class_name == predicted_class_name)\n\n    # Calculate and print the accuracy for each class\n    for class_name, accuracy_count in class_accuracy_counts.items():\n        accuracy = class_accuracy_sums[class_name] / accuracy_count\n        print(f\"Class '{class_name}' accuracy: {accuracy:.2f}\")\n", "CodeBleu_score": 20.678219873523773}, {"code": "def compute_encoder_accuracy(dataset_items, encoder_manager)\n\t\n    \n    # Get the class name key and optional class ID to class name mapping from the encoder manager's configuration\n    class_name_key = encoder_manager.config['class_name_key']\n    class_id_to_name_mapping = encoder_manager.config['class_id_to_name_mapping']\n    \n    # Initialize variables to store the accuracy for each class and the total number of items\n    class_accuracy = {}\n    total_items = 0\n    \n    # Loop through each item in the dataset\n    for item in dataset_items:\n        # Get the audio file and true class name from the item\n        audio_file = item['audio_file']\n        true_class_name = item[class_name_key]\n        \n        # Compute the embedding for the audio file\n        embedding = encoder_manager.compute_embedding(audio_file)\n        \n        # Predict the class label if criteria and mappings are available\n        if class_id_to_name_mapping:\n            # Get the predicted label\n            predicted_label = encoder_manager.predict_label(embedding)\n            \n            # Get the true class name from the class ID to class name mapping\n            true_class_name = class_id_to_name_mapping[true_class_name]\n        else:\n            # Get the predicted label\n            predicted_label = encoder_manager.predict_label(embedding)\n        \n        # Increment the total number of items\n        total_items += 1\n        \n        # Update the accuracy for the true class name\n        if true_class_name in class_accuracy:\n            class_accuracy[true_class_name] += 1\n        else:\n            class_accuracy[true_class_name] = 1\n        \n        # Print the predicted and true class names for the item\n        print(f'Predicted: {predicted_label}, True: {true_class_name}')\n    \n    # Print the accuracy for each class\n    print('", "CodeBleu_score": 22.0142899284487}]}
{"instance_id": "e01ec69ad58a", "generation_result": [{"code": "def adjust_path_and_remove_silence(audio_path)\n\t\n    # adjust path\n    out_path = audio_path.replace(input_dir, output_dir)\n\n    # if file already exists and force is not specified, return the output path and a False flag\n    if os.path.exists(out_path) and not force:\n        print(f\"> \u2757 {out_path} already exists and force is not specified, skipping.\")\n        return out_path, False\n\n    # create the necessary directory structure\n    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n\n    # remove silence from the audio\n    out_path, is_speech = remove_silence(\n        model_and_utils, audio_path, out_path, vad_sample_rate=8000, trim_just_beginning_and_end=True, use_cuda=False\n    )\n\n    return out_path, is_speech\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 42.20840256907159}, {"code": "def adjust_path_and_remove_silence(audio_path)\n\t\n    # Get the input and output directory paths from the audio path\n    input_dir, input_file = os.path.split(audio_path)\n    output_dir = input_dir.replace(\"input\", \"output\")\n\n    # Create the output directory if it doesn't exist\n    os.makedirs(output_dir, exist_ok=True)\n\n    # Define the output file path\n    output_file = os.path.join(output_dir, input_file)\n\n    # Check if the output file already exists\n    if os.path.exists(output_file) and not force:\n        return output_file, False\n\n    # Load the VAD model and utils\n    model, get_speech_timestamps, _, collect_chunks = load_vad_model(\n        \"./model/vad_model.pt\", \"./model/vad_utils.pt\", use_cuda=use_cuda\n    )\n\n    # Remove silence from the audio\n    output_path, is_speech = remove_silence(\n        (model, get_speech_timestamps, collect_chunks), audio_path, output_file, use_cuda=use_cuda\n    )\n\n    # Return the output path and speech detection flag\n    return output_path, is_speech\n\n", "CodeBleu_score": 43.44700031586788}, {"code": "def adjust_path_and_remove_silence(audio_path)\n\t\n    # Create output directory path\n    output_path = audio_path.replace(input_dir, output_dir)\n\n    # Check if output file already exists\n    if os.path.exists(output_path) and not force:\n        print(f\"> \u2757 The file {output_path} already exists. Skipping.\")\n        return output_path, False\n\n    # Create directory structure\n    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n\n    # Remove silence from audio\n    output_path, is_speech = remove_silence(\n        model_and_utils, audio_path, output_path, vad_sample_rate, trim_just_beginning_and_end, use_cuda\n    )\n\n    return output_path, is_speech\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 47.00826897153348}]}
{"instance_id": "7e0bdb0feff8", "generation_result": [{"code": "def resample_files(input_dir, output_sr, output_dir=None, file_ext=\"wav\", n_jobs=10)\n\t\n    if output_dir is None:\n        output_dir = input_dir\n    if not os.path.exists(output_dir):\n        shutil.copytree(input_dir, output_dir)\n    audio_files = find_audio_files(output_dir, file_ext)\n    if len(audio_files) == 0:\n        print(\"No audio files found in the specified directory.\")\n        return\n    print(f\"Resampling {len(audio_files)} audio files to {output_sr} Hz...\")\n    with mp.Pool(n_jobs) as pool:\n        for _ in tqdm(pool.imap_unordered(partial(resample_file, output_sr=output_sr, output_dir=output_dir), audio_files), total=len(audio_files)):\n            pass\n    print(\"Resampling complete.\")\n\n", "CodeBleu_score": 40.62835460254907}, {"code": "def resample_files(input_dir, output_sr, output_dir=None, file_ext=\"wav\", n_jobs=10)\n\t\n    import os\n    import shutil\n    import librosa\n    from tqdm import tqdm\n    from joblib import Parallel, delayed\n\n    # Check if the input directory exists\n    if not os.path.exists(input_dir):\n        print(f\"Input directory '{input_dir}' does not exist.\")\n        return\n\n    # Check if the output directory exists, create it if not\n    if output_dir is None:\n        output_dir = input_dir\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    # Copy the input directory to the output directory\n    shutil.copytree(input_dir, output_dir, dirs_exist_ok=True)\n\n    # Recursively find all audio files with the specified extension\n    audio_files = []\n    for root, _, files in os.walk(output_dir):\n        for file in files:\n            if file.endswith(f\".{file_ext}\"):\n                audio_files.append(os.path.join(root, file))\n\n    # Resample the audio files in parallel\n    Parallel(n_jobs=n_jobs)(delayed(resample_file)(file, output_sr) for file in tqdm(audio_files, desc=\"Resampling audio files\"))\n\n    # Print a completion message\n    print(f\"Resampling complete. Audio files resampled to {output_sr} Hz.\")\n\n", "CodeBleu_score": 31.718695870260394}, {"code": "def resample_files(input_dir, output_sr, output_dir=None, file_ext=\"wav\", n_jobs=10)\n\t\n    import os\n    import shutil\n    from tqdm import tqdm\n    from concurrent.futures import ProcessPoolExecutor\n    import librosa\n\n    if output_dir is None:\n        output_dir = input_dir\n    elif not os.path.exists(output_dir):\n        shutil.copytree(input_dir, output_dir)\n\n    def resample_file(input_path, output_path):\n        y, sr = librosa.load(input_path, sr=None)\n        y_resampled = librosa.resample(y, sr, output_sr)\n        librosa.output.write_wav(output_path, y_resampled, output_sr)\n\n    audio_files = []\n    for root, dirs, files in os.walk(input_dir):\n        for file in files:\n            if file.endswith(file_ext):\n                audio_files.append(os.path.join(root, file))\n\n    with ProcessPoolExecutor(max_workers=n_jobs) as executor:\n        futures = []\n        for audio_file in audio_files:\n            input_path = audio_file\n            output_path = audio_file.replace(input_dir, output_dir)\n            futures.append(executor.submit(resample_file, input_path, output_path))\n\n        for future in tqdm(futures):\n            future.result()\n\n    print(f\"Resampling completed. {len(audio_files)} audio files resampled.\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 35.9151929138989}]}
{"instance_id": "a1351834ca57", "generation_result": [{"code": "def evaluation(model, criterion, data_loader, global_step)\n\t\n\n    # Set model to evaluation mode\n    model.eval()\n\n    # Initialize evaluation loss\n    eval_loss = 0\n\n    # Loop over batches\n    for batch_idx, (data, labels) in enumerate(data_loader):\n        # Reshape inputs and labels to group samples of each class\n        data = data.reshape(data.shape[0] * data.shape[1], data.shape[2])\n        labels = labels.reshape(labels.shape[0] * labels.shape[1])\n\n        # Move data and labels to GPU if available\n        if torch.cuda.is_available():\n            data = data.cuda()\n            labels = labels.cuda()\n\n        # Compute model output\n        output = model(data)\n\n        # Compute loss\n        loss = criterion(output, labels)\n\n        # Update evaluation loss\n        eval_loss += loss.item()\n\n    # Compute average evaluation loss\n    eval_loss /= len(data_loader)\n\n    # Log evaluation statistics\n    log_dict = {\n        \"eval_loss\": eval_loss,\n        \"global_step\": global_step,\n    }\n    wandb.log(log_dict)\n\n    # Generate and log UMAP plot\n    embeddings = model.get_embeddings(data)\n    fig = plot_embeddings(embeddings.cpu().detach().numpy(), data.shape[0])\n    wandb.log({\"umap\": wandb.Image(fig)})\n\n    # Return average evaluation loss\n    return eval_loss\n\n", "CodeBleu_score": 32.07837316353216}, {"code": "def evaluation(model, criterion, data_loader, global_step)\n\t\n    model.eval()\n    avg_loss = 0\n    with torch.no_grad():\n        for batch_idx, (data, labels) in enumerate(data_loader):\n            data = data.to(device)\n            labels = labels.to(device)\n            data = data.reshape(\n                data.shape[0] * data.shape[1], data.shape[2], data.shape[3]\n            )\n            labels = labels.reshape(labels.shape[0] * labels.shape[1])\n            output = model(data)\n            loss = criterion(output, labels)\n            avg_loss += loss.item()\n        avg_loss /= len(data_loader)\n        print(\n            f\"Evaluation loss at step {global_step}: {avg_loss:.4f}\",\n            flush=True,\n        )\n        embeddings = output.cpu().detach().numpy()\n        num_classes_in_batch = len(labels.unique())\n        fig = plot_embeddings(embeddings, num_classes_in_batch)\n        writer.add_figure(\"Evaluation/UMAP\", fig, global_step)\n        model.train()\n        return avg_loss\n\n", "CodeBleu_score": 35.05532172090724}, {"code": "def evaluation(model, criterion, data_loader, global_step)\n\t\n    model.eval()\n    eval_loss = 0\n    with torch.no_grad():\n        for batch_idx, (data, labels) in enumerate(data_loader):\n            data = data.squeeze(1).to(device)\n            labels = labels.to(device)\n            output = model(data)\n            loss = criterion(output, labels)\n            eval_loss += loss.item()\n            if batch_idx == len(data_loader) - 1:\n                num_classes_in_batch = labels.shape[0] // num_utter_per_class\n                embeddings = output.cpu().numpy()\n                fig = plot_embeddings(embeddings, num_classes_in_batch)\n                writer.add_figure(\"UMAP\", fig, global_step)\n    eval_loss /= len(data_loader)\n    writer.add_scalar(\"Eval Loss\", eval_loss, global_step)\n    return eval_loss\n\n", "CodeBleu_score": 28.674751475843056}]}
{"instance_id": "565f8ed09916", "generation_result": [{"code": "def register_config(model_name: str) -> Coqpit\n\t\n    config_class_name = to_camel(model_name) + \"Config\"\n    try:\n        config = importlib.import_module(\n            \".\" + config_class_name, package=\"diffusers.models.generation\"\n        ).__dict__[config_class_name]\n    except ModuleNotFoundError:\n        try:\n            config = importlib.import_module(\n                \".\" + config_class_name, package=\"diffusers.models.xtts\"\n            ).__dict__[config_class_name]\n        except ModuleNotFoundError:\n            raise ModuleNotFoundError(\n                f\"Could not find configuration class for model {model_name}\"\n            )\n    return config\n\n", "CodeBleu_score": 33.51948177364329}, {"code": "def register_config(model_name: str) -> Coqpit\n\t\n    config_name = to_camel(model_name)\n    if model_name == \"xtts\":\n        try:\n            from .xtts import XTTSConfig\n\n            return XTTSConfig\n        except ModuleNotFoundError:\n            pass\n    paths = [\n        \"diffusers.models.modeling_utils\",\n        \"diffusers.models.vae\",\n        \"diffusers.models.attention\",\n        \"diffusers.models.text_encoder\",\n        \"diffusers.models.unet\",\n        \"diffusers.models.schedulers\",\n        \"diffusers.models.pipelines\",\n        \"diffusers.models.autoencoder\",\n        \"diffusers.models.cross_attention\",\n        \"diffusers.models.attention\",\n        \"diffusers.models.text_encoder\",\n        \"diffusers.models.unet\",\n        \"diffusers.models.schedulers\",\n        \"diffusers.models.pipelines\",\n        \"diffusers.models.autoencoder\",\n        \"diffusers.models.cross_attention\",\n        \"diffusers.models.attention\",\n        \"diffusers.models.text_encoder\",\n        \"diffusers.models.unet\",\n        \"diffusers.models.schedulers\",\n        \"diffusers.models.pipelines\",\n        \"diffusers.models.autoencoder\",\n        \"diffusers.models.cross_attention\",\n        \"diffusers.models.attention\",\n        \"diffusers.models.text_encoder\",\n        \"diffusers.models.unet\",\n        \"diffusers.models.schedulers\",\n        \"diffusers.models.pipelines\",\n        \"diffusers.models.autoencoder\",\n        \"diffusers.models.cross_attention\",\n        \"diffusers.models.attention\",\n        \"diffusers.models.text_encoder\",\n        \"diffusers.models.unet\",\n        \"diffusers.models.schedulers\",\n        \"diffusers.models.pipelines\",\n        \"diffusers.models.autoencoder\",\n        \"diffusers.models.cross_attention\",\n        \"diffusers.models.attention\",\n        \"diffusers.models.text", "CodeBleu_score": 18.227156148369072}, {"code": "def register_config(model_name: str) -> Coqpit\n\t\n    config_name = to_camel(model_name) + \"Config\"\n    try:\n        config = find_module(\"xtts.models.\" + model_name, config_name)\n    except ModuleNotFoundError:\n        raise ModuleNotFoundError(f\"{config_name} not found\")\n    return config\n\n", "CodeBleu_score": 18.008888325364268}]}
{"instance_id": "0ea7862506a7", "generation_result": [{"code": "def load_config(config_path: str) -> Coqpit\n\t\n    config_dict = read_json_with_comments(config_path)\n    model_name = _process_model_name(config_dict)\n    config_class = register_config(model_name)\n    config = config_class()\n    config.update_from_dict(config_dict)\n    return config\n\n", "CodeBleu_score": 10.33619276767899}, {"code": "def load_config(config_path: str) -> Coqpit\n\t\n    config_dict = read_json_with_comments(config_path)\n    model_name = _process_model_name(config_dict)\n    config_class = register_config(model_name)\n    config = config_class()\n    config.update_from_dict(config_dict)\n    return config\n\n", "CodeBleu_score": 10.33619276767899}, {"code": "def load_config(config_path: str) -> Coqpit\n\t\n    config_dict = read_json_with_comments(config_path)\n    model_name = _process_model_name(config_dict)\n    config_class = register_config(model_name)\n    config = config_class()\n    config.parse_obj(config_dict)\n    return config\n\n", "CodeBleu_score": 10.00620439327954}]}
{"instance_id": "e0db68b63143", "generation_result": [{"code": "def list_files(basePath, validExts=None, contains=None)\n\t\n    for (rootDir, dirNames, filenames) in os.walk(basePath):\n        for filename in filenames:\n            if contains is not None and filename.find(contains) == -1:\n                continue\n            ext = filename[filename.rfind(\".\"):].lower()\n            if validExts is None or ext.endswith(validExts):\n                yield os.path.join(rootDir, filename)\n\n\n", "CodeBleu_score": 70.66131881047349}, {"code": "def list_files(basePath, validExts=None, contains=None)\n\t\n\n    for (rootDir, dirNames, filenames) in os.walk(basePath):\n        for filename in filenames:\n            if contains is not None and filename.find(contains) == -1:\n                continue\n            if validExts is None or filename.endswith(validExts):\n                filePath = os.path.join(rootDir, filename)\n                yield filePath\n\n\n", "CodeBleu_score": 62.895330444769634}, {"code": "def list_files(basePath, validExts=None, contains=None)\n\t\n    for (rootDir, dirNames, filenames) in os.walk(basePath):\n        for filename in filenames:\n            if contains is not None and filename.find(contains) == -1:\n                continue\n            if validExts is None or filename.endswith(validExts):\n                filePath = os.path.join(rootDir, filename)\n                yield filePath\n\n\n", "CodeBleu_score": 62.895330444769634}]}
{"instance_id": "b836fcaf0a27", "generation_result": [{"code": "def load_model(xtts_checkpoint, xtts_config, xtts_vocab)\n\t\n    clear_gpu_cache()\n    if xtts_checkpoint is None:\n        raise ValueError(\"Please provide a checkpoint path.\")\n    if xtts_config is None:\n        raise ValueError(\"Please provide a config path.\")\n    if xtts_vocab is None:\n        raise ValueError(\"Please provide a vocab path.\")\n    print(\"Loading model...\")\n    config = load_config(xtts_config)\n    model = XTTS(config)\n    model.load_checkpoint(\n        config,\n        checkpoint_dir=xtts_checkpoint,\n        vocab_path=xtts_vocab,\n        eval=True,\n        strict=True,\n    )\n    if torch.cuda.is_available():\n        model.cuda()\n    print(\"Model loaded successfully.\")\n    return model\n", "CodeBleu_score": 46.45354616808128}, {"code": "def load_model(xtts_checkpoint, xtts_config, xtts_vocab)\n\t\n    clear_gpu_cache()\n    if xtts_checkpoint is None or xtts_config is None or xtts_vocab is None:\n        return \"Please provide all required paths: xtts_checkpoint, xtts_config, xtts_vocab\"\n    config = XTTSConfig.from_json_file(xtts_config)\n    model = XTTSModel(config)\n    model.load_checkpoint(\n        config,\n        checkpoint_path=xtts_checkpoint,\n        vocab_path=xtts_vocab,\n        eval=True,\n        strict=True,\n        use_deepspeed=False,\n    )\n    if torch.cuda.is_available():\n        model = model.to(torch.device(\"cuda\"))\n    print(\"Model loaded successfully\")\n    return model\n", "CodeBleu_score": 49.25132790404339}, {"code": "def load_model(xtts_checkpoint, xtts_config, xtts_vocab)\n\t\n    # clear the GPU cache\n    clear_gpu_cache()\n\n    # check if required paths are provided\n    if xtts_checkpoint is None or xtts_config is None or xtts_vocab is None:\n        return \"Error: Please provide the paths to the XTTS checkpoint, config, and vocab files.\"\n\n    # load the configuration from the provided path\n    config = XTTSConfig.from_json_file(xtts_config)\n\n    # initialize the XTTS model\n    xtts_model = XTTSModel(config)\n\n    # load the model checkpoint and vocabulary\n    xtts_model.load_checkpoint(\n        config,\n        checkpoint_dir=xtts_checkpoint,\n        vocab_path=xtts_vocab,\n        eval=True,\n        strict=True,\n        use_deepspeed=False,\n    )\n\n    # move the model to the GPU if available\n    if torch.cuda.is_available():\n        xtts_model = xtts_model.cuda()\n\n    # print a message indicating the loading progress\n    print(\"XTTS model loaded successfully.\")\n\n    # return a success message\n    return \"XTTS model loaded successfully.\"\n\n", "CodeBleu_score": 48.94060800065498}]}
{"instance_id": "c5d63aa0df26", "generation_result": [{"code": "def compute_embedding(self, x, num_frames=250, num_eval=10, return_mean=True, l2_norm=True)\n\t\n    x = x.unsqueeze(0)\n    hop_length = self.stft.hop_length\n    if x.shape[-1] < self.stft.n_fft:\n        x = torch.nn.functional.pad(x, (0, self.stft.n_fft - x.shape[-1]))\n    num_frames = min(num_frames, x.shape[-1] // hop_length)\n    offsets = torch.arange(num_frames) * hop_length\n    offsets = torch.cat([offsets, offsets + (x.shape[-1] - offsets[-1])])\n    offsets = offsets.unsqueeze(1).repeat(1, num_eval)\n    offsets = offsets.view(-1)\n    x = torch.nn.functional.unfold(x, (self.stft.n_fft, 1), stride=hop_length).permute(0, 2, 1)\n    x = x[..., :self.stft.n_fft // 2 + 1]\n    x = x[..., :-1]\n    x = x.view(-1, x.shape[-1])\n    x = torch.nn.functional.pad(x, (0, 1))\n    x = x[offsets]\n    x = x.view(num_eval, num_frames, -1)\n    x = x.permute(1, 0, 2)\n    x = x.view(num_frames, num_eval * self.stft.n_fft)\n    x = self.inference(x)\n    if return_mean:\n        x = x.view(num_frames, num_eval, -1)\n        x = x.mean(dim=1)\n    if l2_norm:\n        x = torch.nn.functional.normalize(x, p=2, dim=1)\n    return x\n\n", "CodeBleu_score": 34.02095674808464}, {"code": "def compute_embedding(self, x, num_frames=250, num_eval=10, return_mean=True, l2_norm=True)\n\t\n    # Calculate the hop length based on the number of frames\n    hop_length = int(x.shape[-1] / num_frames)\n    # Adjust the number of frames if the input length is not divisible by the hop length\n    num_frames = int(x.shape[-1] / hop_length)\n    # Calculate the number of offset positions for evaluation\n    num_offsets = num_frames - num_eval\n    # Initialize an empty list to store the embeddings\n    embeddings = []\n    # Loop over the offset positions\n    for offset in range(num_offsets):\n        # Calculate the start and end positions of the frames\n        start = offset * hop_length\n        end = start + num_eval * hop_length\n        # Extract the frames at these positions\n        frames = x[:, start:end]\n        # Concatenate the frames into a batch\n        batch = torch.unsqueeze(frames, 0)\n        # Perform inference to obtain embeddings\n        embedding = self.inference(batch)\n        # Append the embeddings to the list\n        embeddings.append(embedding)\n    # Concatenate the embeddings into a single tensor\n    embeddings = torch.cat(embeddings, dim=0)\n    # Compute the mean of the embeddings if specified\n    if return_mean:\n        embeddings = torch.mean(embeddings, dim=0)\n    # Normalize the embeddings with L2 norm if specified\n    if l2_norm:\n        embeddings = F.normalize(embeddings, p=2, dim=1)\n    # Return the embeddings\n    return embeddings\n\n", "CodeBleu_score": 43.70641316712774}, {"code": "def compute_embedding(self, x, num_frames=250, num_eval=10, return_mean=True, l2_norm=True)\n\t\n\n    # Adjust the number of frames to match the hop length if using torch specifications\n    if self.hop_length is not None:\n        num_frames = int(num_frames * self.hop_length / self.sr)\n\n    # Calculate the number of frames that can be extracted from the input\n    num_frames = min(num_frames, x.shape[0] - self.num_frames)\n\n    # Compute the offset positions for evaluation\n    offsets = torch.arange(0, num_frames, num_frames // num_eval)\n\n    # Extract frames at these offsets and concatenate them into a batch\n    x = torch.cat([x[i:i + self.num_frames] for i in offsets])\n\n    # Perform inference to obtain embeddings\n    embeddings = self.inference(x)\n\n    # Compute the mean of the embeddings and return the result, optionally normalizing them with L2 norm\n    if return_mean:\n        embeddings = embeddings.mean(dim=0)\n        if l2_norm:\n            embeddings = embeddings / torch.norm(embeddings)\n\n    return embeddings\n\n", "CodeBleu_score": 36.29621794740921}]}
{"instance_id": "06f5b0335b26", "generation_result": [{"code": "def create_layer(self, block, planes, blocks, stride=1)\n\t\n    downsample = None\n    if stride != 1 or self.inplanes != planes * block.expansion:\n        downsample = nn.Sequential(\n            conv1x1(self.inplanes, planes * block.expansion, stride),\n            nn.BatchNorm2d(planes * block.expansion),\n        )\n\n    layers = []\n    layers.append(block(self.inplanes, planes, stride, downsample))\n    self.inplanes = planes * block.expansion\n    for _ in range(1, blocks):\n        layers.append(block(self.inplanes, planes))\n\n    return nn.Sequential(*layers)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 75.12404610027407}, {"code": "def create_layer(self, block, planes, blocks, stride=1)\n\t\n    downsample = None\n    if stride != 1 or self.inplanes != planes * block.expansion:\n        downsample = nn.Sequential(\n            nn.Conv2d(self.inplanes, planes * block.expansion,\n                      kernel_size=1, stride=stride, bias=False),\n            nn.BatchNorm2d(planes * block.expansion),\n        )\n\n    layers = []\n    layers.append(block(self.inplanes, planes, stride, downsample))\n    self.inplanes = planes * block.expansion\n    for i in range(1, blocks):\n        layers.append(block(self.inplanes, planes))\n\n    return nn.Sequential(*layers)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 85.75459657510368}, {"code": "def create_layer(self, block, planes, blocks, stride=1)\n\t\n    downsample = None\n    if stride != 1 or self.inplanes != planes * block.expansion:\n        downsample = nn.Sequential(\n            conv1x1(self.inplanes, planes * block.expansion, stride),\n            nn.BatchNorm2d(planes * block.expansion),\n        )\n\n    layers = []\n    layers.append(block(self.inplanes, planes, stride, downsample))\n    self.inplanes = planes * block.expansion\n    for _ in range(1, blocks):\n        layers.append(block(self.inplanes, planes))\n\n    return nn.Sequential(*layers)\n\n\n\n", "CodeBleu_score": 75.12404610027407}]}
{"instance_id": "9ac60597b633", "generation_result": [{"code": "def plot_embeddings(embeddings, num_classes_in_batch)\n\t\n    import matplotlib.pyplot as plt\n    import numpy as np\n    from umap import UMAP\n    # Calculate the number of utterances per class\n    num_utterances_per_class = [np.sum(embeddings[:, 0] == i) for i in range(num_classes_in_batch)]\n    # Limit the number of classes to 10 if necessary\n    if len(num_utterances_per_class) > 10:\n        num_utterances_per_class = num_utterances_per_class[:10]\n        num_classes_in_batch = 10\n    # Transform the embeddings using UMAP\n    umap_transformer = UMAP(n_components=2, random_state=42)\n    umap_embeddings = umap_transformer.fit_transform(embeddings)\n    # Create a scatter plot with colors representing different classes\n    fig, ax = plt.subplots()\n    for i in range(num_classes_in_batch):\n        ax.scatter(umap_embeddings[embeddings[:, 0] == i, 0], umap_embeddings[embeddings[:, 0] == i, 1], label=f\"Class {i} ({num_utterances_per_class[i]} utterances)\", alpha=0.5)\n    ax.legend()\n    ax.set_title(\"UMAP projection\")\n    ax.set_aspect(\"equal\")\n    # Save the plot as an image\n    plt.savefig(\"umap.png\")\n    return fig\n\n\n\n\n\n", "CodeBleu_score": 38.80446832746834}, {"code": "def plot_embeddings(embeddings, num_classes_in_batch)\n\t\n    import umap\n    import matplotlib.pyplot as plt\n    import numpy as np\n    from matplotlib.colors import ListedColormap\n\n    # Calculate the number of utterances per class\n    num_utterances_per_class = np.unique(embeddings.argmax(axis=1), return_counts=True)[1]\n\n    # Limit the number of classes to 10 if necessary\n    if num_utterances_per_class.shape[0] > 10:\n        num_utterances_per_class = num_utterances_per_class[:10]\n        num_classes_in_batch = 10\n\n    # Plot the embeddings\n    fig = plt.figure(figsize=(8, 6))\n    ax = fig.add_subplot(111)\n    ax.set_title(\"UMAP projection\")\n    ax.set_aspect(\"equal\")\n\n    # UMAP transforms the embeddings\n    reducer = umap.UMAP(random_state=42)\n    reduced_embeddings = reducer.fit_transform(embeddings)\n\n    # Create a scatter plot with colors representing different classes\n    cmap = ListedColormap(plt.get_cmap(\"tab10\").colors[:num_classes_in_batch])\n    scatter = ax.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], c=embeddings.argmax(axis=1), cmap=cmap, s=5)\n\n    # Add a legend to the plot\n    legend1 = ax.legend(*scatter.legend_elements(), title=\"Classes\", loc=\"upper left\", bbox_to_anchor=(1, 1))\n    ax.add_artist(legend1)\n\n    # Add the number of utterances per class to the plot\n    legend2 = ax.legend(num_utterances_per_class, [f\"Class {i}\" for i in range(num_classes_in_batch)], title=\"Num. Utterances\", loc=\"upper left\", bbox_to_anchor=(1,", "CodeBleu_score": 43.211327237262545}, {"code": "def plot_embeddings(embeddings, num_classes_in_batch)\n\t\n    import matplotlib.pyplot as plt\n    import numpy as np\n    from sklearn.manifold import TSNE\n    from umap import UMAP\n    import seaborn as sns\n    import os\n\n    # Calculate the number of utterances per class\n    num_utterances_per_class = np.bincount(num_classes_in_batch)\n\n    # Limit the number of classes to 10 if necessary\n    if len(num_utterances_per_class) > 10:\n        num_utterances_per_class = num_utterances_per_class[:10]\n        num_classes_in_batch = num_classes_in_batch[:10]\n\n    # Create a scatter plot using UMAP for dimensionality reduction\n    umap = UMAP(n_components=2, n_neighbors=5, min_dist=0.0, metric='cosine')\n    embeddings_umap = umap.fit_transform(embeddings)\n\n    # Create a scatter plot using UMAP for dimensionality reduction\n    umap = UMAP(n_components=2, n_neighbors=5, min_dist=0.0, metric='cosine')\n    embeddings_umap = umap.fit_transform(embeddings)\n\n    # Create a scatter plot using UMAP for dimensionality reduction\n    umap = UMAP(n_components=2, n_neighbors=5, min_dist=0.0, metric='cosine')\n    embeddings_umap = umap.fit_transform(embeddings)\n\n    # Create a scatter plot using UMAP for dimensionality reduction\n    umap = UMAP(n_components=2, n_neighbors=5, min_dist=0.0, metric='cosine')\n    embeddings_umap = umap.fit_transform(embeddings)\n\n    # Create a scatter plot using UMAP for dimensionality reduction\n    umap = UMAP(n_components=2, n_neighbors=5, min_dist=0.0, metric='cosine')\n    embeddings_umap = umap.fit_", "CodeBleu_score": 24.663902648785008}]}
{"instance_id": "19c2b4af256d", "generation_result": [{"code": "def calc_cosine_sim(self, dvecs, centroids)\n\t\n    cosine_sim = []\n    for spkr in range(dvecs.shape[0]):\n        new_centroids = self.calc_new_centroids(dvecs, centroids, spkr, utt)\n        cos_sim = torch.clamp(\n            torch.matmul(dvecs[spkr, :, :], new_centroids.t()), min=1e-10\n        )\n        cosine_sim.append(cos_sim)\n    cosine_sim = torch.cat(cosine_sim, dim=0)\n    return cosine_sim\n\n", "CodeBleu_score": 28.257586077297997}, {"code": "def calc_cosine_sim(self, dvecs, centroids)\n\t\n    cos_sims = []\n    for spkr in range(dvecs.shape[0]):\n        new_centroids = self.calc_new_centroids(dvecs, centroids, spkr, utt)\n        cos_sim = (dvecs[spkr, :, :] @ new_centroids.T).clamp(min=1e-8)\n        cos_sims.append(cos_sim)\n    cos_sims = torch.cat(cos_sims, dim=0)\n    return cos_sims", "CodeBleu_score": 26.606900387421025}, {"code": "def calc_cosine_sim(self, dvecs, centroids)\n\t\n    cos_sim_mat = []\n    for spkr in range(self.num_speakers):\n        new_centroids = self.calc_new_centroids(dvecs, centroids, spkr, utt)\n        cos_sim_utt = torch.nn.functional.cosine_similarity(\n            dvecs[spkr, utt].unsqueeze(0), new_centroids, dim=1\n        )\n        cos_sim_utt = torch.clamp(cos_sim_utt, min=1e-6)\n        cos_sim_utt = torch.stack(cos_sim_utt)\n        cos_sim_mat.append(cos_sim_utt)\n    cos_sim_mat = torch.cat(cos_sim_mat, dim=0)\n    return cos_sim_mat\n\n", "CodeBleu_score": 33.885626519075686}]}
{"instance_id": "c525b411a1e7", "generation_result": [{"code": " def embed_loss_softmax(self, dvecs, cos_sim_matrix)\n\t\n    losses = []\n    for i in range(dvecs.shape[0]):\n        losses.append(-torch.log_softmax(cos_sim_matrix[i], dim=0))\n    losses = torch.stack(losses)\n    return losses\n\n\n\n\n\n\n\n def embed_loss_softmax(self, dvecs, cos_sim_matrix):\n    \"\"\"The function calculates the embedding loss using softmax.", "CodeBleu_score": 31.644328880325872}, {"code": " def embed_loss_softmax(self, dvecs, cos_sim_matrix)\n\t\n    losses = []\n    for i in range(len(dvecs)):\n        # Compute the negative log softmax of the cosine similarity matrix for the current embedding\n        neg_log_softmax = -torch.log(F.softmax(cos_sim_matrix[i], dim=0))\n        # Append the negative log softmax to the list of losses\n        losses.append(neg_log_softmax)\n    # Stack the losses into a tensor and return it\n    return torch.stack(losses)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 32.88696757109069}, {"code": " def embed_loss_softmax(self, dvecs, cos_sim_matrix)\n\t\n    loss_list = []\n    for i in range(dvecs.shape[0]):\n        loss_list.append(-torch.log(torch.softmax(cos_sim_matrix[i], dim=0)))\n    loss_tensor = torch.stack(loss_list)\n    return loss_tensor\n\n\n\n\n def embed_loss_sigmoid(self, dvecs, cos_sim_matrix):\n    \"\"\"The function calculates the embedding loss using sigmoid.", "CodeBleu_score": 29.65791257490712}]}
{"instance_id": "25ac3945da2e", "generation_result": [{"code": "def embed_loss_contrast(self, dvecs, cos_sim_matrix)\n\t\n    # Calculate the sigmoid of the cosine similarity matrix for each embedding\n    sigmoid_cos_sim_matrix = torch.sigmoid(cos_sim_matrix)\n\n    # Iterate over the embeddings\n    losses = []\n    for i, dvec in enumerate(dvecs):\n        # Exclude the current centroid\n        exclude_centroid = torch.ones_like(sigmoid_cos_sim_matrix[i])\n        exclude_centroid[i] = 0\n\n        # Compute the contrast loss\n        contrast_loss = torch.sum(sigmoid_cos_sim_matrix[i] * exclude_centroid)\n\n        # Append the result to the loss row\n        losses.append(contrast_loss)\n\n    # Stack the losses into a tensor\n    losses = torch.stack(losses)\n\n    return losses\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 18.268391588701437}, {"code": "def embed_loss_contrast(self, dvecs, cos_sim_matrix)\n\t\n    loss_row = []\n    for dvec in dvecs:\n        dvec = dvec.unsqueeze(0)\n        cos_sim_matrix_dvec = cos_sim_matrix.gather(1, dvec)\n        cos_sim_matrix_dvec = cos_sim_matrix_dvec.squeeze(1)\n        dvec_loss = torch.sigmoid(cos_sim_matrix_dvec)\n        dvec_loss = dvec_loss.gather(0, dvec)\n        dvec_loss = dvec_loss.squeeze(0)\n        dvec_loss = -torch.log(dvec_loss + 1e-10)\n        loss_row.append(dvec_loss)\n    loss_row = torch.stack(loss_row)\n    return loss_row\n\n\n", "CodeBleu_score": 27.2798113643304}, {"code": "def embed_loss_contrast(self, dvecs, cos_sim_matrix)\n\t\n    n_centroids = cos_sim_matrix.size(0)\n    cos_sim_matrix = torch.sigmoid(cos_sim_matrix)\n    contrast_loss_row = torch.zeros(n_centroids, device=dvecs.device)\n    for i in range(dvecs.size(0)):\n        contrast_loss_row[i] = -torch.log(cos_sim_matrix[i, i] - torch.sum(cos_sim_matrix[i, :]) + cos_sim_matrix[i, i])\n    return torch.stack(contrast_loss_row)\n\n", "CodeBleu_score": 24.56877923043176}]}
{"instance_id": "878d14ffabd2", "generation_result": [{"code": "def forward(self, x, _label=None)\n\t\n    # ensure the input has at least two utterances per speaker\n    if x.shape[1] < 2:\n        raise ValueError(\n            \"The input must have at least two utterances per speaker. The input has {} utterances per speaker.\".format(\n                x.shape[1]\n            )\n        )\n\n    # compute centroids by averaging the embeddings\n    centroids = x.mean(dim=1)\n\n    # calculate the cosine similarity matrix\n    cos_sim_matrix = self.calc_cosine_sim(x, centroids)\n\n    # scale the cosine similarity matrix and shift it\n    scaled_cos_sim_matrix = self.scale_cos_sim_matrix(cos_sim_matrix)\n    shifted_cos_sim_matrix = self.shift_cos_sim_matrix(scaled_cos_sim_matrix)\n\n    # calculate the embedding loss\n    embedding_loss = self.calc_embedding_loss(shifted_cos_sim_matrix)\n\n    # return the mean loss\n    return embedding_loss.mean()\n\n", "CodeBleu_score": 42.11445141499571}, {"code": "def forward(self, x, _label=None)\n\t\n    # check if input is valid\n    assert x.shape[0] > 1, \"Input must have at least 2 speakers\"\n    assert x.shape[1] > 1, \"Input must have at least 2 utterances per speaker\"\n    # calculate centroids\n    centroids = x.mean(dim=1)\n    # calculate cosine similarity matrix\n    cos_sim_matrix = self.calc_cosine_sim(x, centroids)\n    # calculate shifted and scaled cosine similarity matrix\n    cos_sim_matrix_scaled = cos_sim_matrix - torch.mean(cos_sim_matrix, dim=1, keepdim=True)\n    cos_sim_matrix_scaled = cos_sim_matrix_scaled / torch.std(cos_sim_matrix_scaled, dim=1, keepdim=True)\n    # calculate embedding loss\n    loss = torch.mean(torch.sum(cos_sim_matrix_scaled, dim=1))\n    return loss", "CodeBleu_score": 38.35382878730198}, {"code": "def forward(self, x, _label=None)\n\t\n    if x.shape[1] < 2:\n        raise ValueError(\"Input data must have at least two utterances per speaker.\")\n\n    # Calculate centroids by averaging embeddings\n    centroids = torch.mean(x, dim=1)\n\n    # Calculate cosine similarity matrix\n    cos_sim_matrix = self.calc_cosine_sim(x, centroids)\n\n    # Scale and shift similarity matrix\n    scaled_cos_sim_matrix = (cos_sim_matrix - self.centroid_shift) / self.centroid_scale\n\n    # Calculate embedding loss\n    embedding_loss = torch.mean(\n        torch.sum(\n            torch.clamp(\n                torch.cosh(scaled_cos_sim_matrix),\n                min=self.cosh_clip,\n            ),\n            dim=1,\n        )\n    )\n\n    return embedding_loss", "CodeBleu_score": 40.01291450766233}]}
{"instance_id": "86a364c72caa", "generation_result": [{"code": "def get_voices(extra_voice_dirs: List[str] = [])\n\t\n    voices = {}\n    for voice_dir in [os.path.join(os.path.dirname(__file__), \"voices\"), *extra_voice_dirs]:\n        for dirpath, dirnames, filenames in os.walk(voice_dir):\n            if filenames:\n                voices[os.path.relpath(dirpath, voice_dir)] = [os.path.join(dirpath, filename) for filename in filenames if filename.endswith(\".npz\") or filename.endswith(\".wav\") or filename.endswith(\".mp3\")]\n    return voices\n\n\n", "CodeBleu_score": 26.231323071359924}, {"code": "def get_voices(extra_voice_dirs: List[str] = [])\n\t\n    # Get the path to the current directory\n    current_dir = os.path.dirname(os.path.abspath(__file__))\n\n    # Get a list of all directories in the current directory\n    dirs = [d for d in os.listdir(current_dir) if os.path.isdir(os.path.join(current_dir, d))]\n\n    # Create a dictionary to store the files for each directory\n    files = {}\n\n    # Iterate through each directory\n    for dir in dirs:\n        # Get a list of all files in the directory\n        files[dir] = []\n        for root, dirs, filenames in os.walk(os.path.join(current_dir, dir)):\n            for filename in filenames:\n                # Check if the file is a .npz file\n                if filename.endswith('.npz'):\n                    files[dir].append(os.path.join(root, filename))\n                # Check if the file is a .wav or .mp3 file\n                elif filename.endswith('.wav') or filename.endswith('.mp3'):\n                    files[dir].append(os.path.join(root, filename))\n\n    # Add extra voice dirs\n    for dir in extra_voice_dirs:\n        if not os.path.isdir(dir):\n            raise ValueError(f\"Directory {dir} does not exist\")\n        dir = os.path.basename(dir)\n        files[dir] = []\n        for root, dirs, filenames in os.walk(dir):\n            for filename in filenames:\n                # Check if the file is a .npz file\n                if filename.endswith('.npz'):\n                    files[dir].append(os.path.join(root, filename))\n                # Check if the file is a .wav or .mp3 file\n                elif filename.endswith('.wav') or filename.endswith('.mp3'):\n                    files[dir].append(os.path.join(root, filename))\n\n    return files", "CodeBleu_score": 27.475362458819525}, {"code": "def get_voices(extra_voice_dirs: List[str] = [])\n\t\n    voice_dirs = [\"./voices\"]\n    voice_dirs.extend(extra_voice_dirs)\n    voices = {}\n    for voice_dir in voice_dirs:\n        for root, dirs, files in os.walk(voice_dir):\n            for file in files:\n                if file.endswith(\".npz\"):\n                    voices[root] = voices.get(root, []) + [os.path.join(root, file)]\n                elif file.endswith(\".wav\") or file.endswith(\".mp3\"):\n                    voices[root] = voices.get(root, []) + [os.path.join(root, file)]\n    return voices\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 30.06579462193597}]}
{"instance_id": "02f20c8917e6", "generation_result": [{"code": "def load_voice(model, voice: str, extra_voice_dirs: List[str] = [])\n\t\n    if voice == \"random\":\n        return None, None, None\n    voices = get_voices(extra_voice_dirs)\n    paths = voices[voice]\n    if len(paths) > 1:\n        raise ValueError(f\"Voice {voice} has multiple paths: {paths}\")\n    try:\n        path = voices[voice]\n    except KeyError as e:\n        raise KeyError(f\"Voice {voice} not found in {extra_voice_dirs}\") from e\n    if len(paths) == 1 and paths[0].endswith(\".npz\"):\n        return load_npz(path[0])\n    audio_path = paths[0]\n    output_path = os.path.splitext(audio_path)[0] + \".npz\"\n    generate_voice(audio=audio_path, model=model, output_path=output_path)\n    return load_voice(model, voice, extra_voice_dirs)", "CodeBleu_score": 96.1921767598096}, {"code": "def load_voice(model, voice: str, extra_voice_dirs: List[str] = [])\n\t\n    if voice == \"random\":\n        return None, None, None\n\n    voices = get_voices(extra_voice_dirs)\n    paths = voices[voice]\n\n    # bark only uses a single sample for cloning\n    if len(paths) > 1:\n        raise ValueError(f\"Voice {voice} has multiple paths: {paths}\")\n\n    try:\n        path = voices[voice]\n    except KeyError as e:\n        raise KeyError(f\"Voice {voice} not found in {extra_voice_dirs}\") from e\n\n    if len(paths) == 1 and paths[0].endswith(\".npz\"):\n        return load_npz(path[0])\n\n    audio_path = paths[0]\n    # replace the file extension with .npz\n    output_path = os.path.splitext(audio_path)[0] + \".npz\"\n    generate_voice(audio=audio_path, model=model, output_path=output_path)\n    return load_voice(model, voice, extra_voice_dirs)\n\n", "CodeBleu_score": 96.1921767598096}, {"code": "def load_voice(model, voice: str, extra_voice_dirs: List[str] = [])\n\t\n    if voice == \"random\":\n        return None, None, None\n\n    voices = get_voices(extra_voice_dirs)\n    paths = voices[voice]\n\n    # bark only uses a single sample for cloning\n    if len(paths) > 1:\n        raise ValueError(f\"Voice {voice} has multiple paths: {paths}\")\n\n    try:\n        path = voices[voice]\n    except KeyError as e:\n        raise KeyError(f\"Voice {voice} not found in {extra_voice_dirs}\") from e\n\n    if len(paths) == 1 and paths[0].endswith(\".npz\"):\n        return load_npz(path[0])\n\n    audio_path = paths[0]\n    # replace the file extension with .npz\n    output_path = os.path.splitext(audio_path)[0] + \".npz\"\n    generate_voice(audio=audio_path, model=model, output_path=output_path)\n    return load_voice(model, voice, extra_voice_dirs)\n\n", "CodeBleu_score": 96.1921767598096}]}
{"instance_id": "9458e981389f", "generation_result": [{"code": "def _relative_shift(self, pos_score: torch.Tensor) -> torch.Tensor\n\t\n    batch_size, seq_len, _ = pos_score.size()\n    zeros = pos_score.new_zeros(batch_size, seq_len, 1)\n    padded = torch.cat([zeros, pos_score], dim=-1)\n    padded = padded.view(batch_size, seq_len + 1, -1)\n    pos_score = padded[:, :seq_len, :seq_len]\n    return pos_score\n\n\n\n", "CodeBleu_score": 30.353924831107626}, {"code": "def _relative_shift(self, pos_score: torch.Tensor) -> torch.Tensor\n\t\n    batch_size, seq_len, _ = pos_score.size()\n    zeros = pos_score.new_zeros(batch_size, seq_len, 1)\n    padded = torch.cat([zeros, pos_score], dim=-1)\n    padded = padded.view(batch_size, seq_len + 1, -1)\n    pos_score = padded[:, :seq_len, :seq_len]\n    return pos_score\n\n", "CodeBleu_score": 30.353924831107626}, {"code": "def _relative_shift(self, pos_score: torch.Tensor) -> torch.Tensor\n\t\n    batch_size, seq_length = pos_score.size()\n    zeros = pos_score.new_zeros(batch_size, 1)\n    padded_pos_score = torch.cat([zeros, pos_score], dim=-1)\n    padded_pos_score = padded_pos_score.view(batch_size, seq_length + 1, -1)\n    pos_score_prev = padded_pos_score[:, 1:, :].contiguous()\n    return pos_score_prev", "CodeBleu_score": 37.389214120345414}]}
{"instance_id": "c046d4f00e80", "generation_result": [{"code": "def run_padded_sequence(self, context, lens)\n\t\n    batch_size = context.size(0)\n    max_len = context.size(1)\n    if lens is None:\n        lens = torch.LongTensor([max_len] * batch_size)\n    else:\n        lens = lens.data\n    sorted_lens, indices = torch.sort(lens, descending=True)\n    sorted_context = context[indices]\n    packed_context = nn.utils.rnn.pack_padded_sequence(sorted_context, sorted_lens)\n    packed_output, _ = self.conv_net(packed_context)\n    output, _ = nn.utils.rnn.pad_packed_sequence(packed_output)\n    output = output[indices.sort()[1]]\n    return output\n\n\n", "CodeBleu_score": 32.20129723964146}, {"code": "def run_padded_sequence(self, context, lens)\n\t\n    batch_size = context.size(0)\n    max_len = context.size(1)\n    assert max_len == lens.max().item(), 'the true length of sequences should be provided in sorted (descending) order!'\n    # process each batch element\n    for i in range(batch_size):\n        seq_len = lens[i].item()\n        context_i = context[i, :seq_len, :]\n        context_i = self.conv1(context_i)\n        context_i = self.conv2(context_i)\n        context_i = self.conv3(context_i)\n        context_i = self.conv4(context_i)\n        context_i = self.conv5(context_i)\n        context_i = self.conv6(context_i)\n        context_i = self.conv7(context_i)\n        context_i = self.conv8(context_i)\n        context_i = self.conv9(context_i)\n        context_i = self.conv10(context_i)\n        context[i, :seq_len, :] = context_i\n    # pad to the same length\n    context = nn.utils.rnn.pad_sequence(context.split(lens.tolist()), batch_first=True, padding_value=0)\n    return context", "CodeBleu_score": 29.347313804753917}, {"code": "def run_padded_sequence(self, context, lens)\n\t\n    context_list = []\n    for i in range(len(lens)):\n        context_list.append(context[i][:lens[i]])\n    context_padded = torch.nn.utils.rnn.pad_sequence(context_list, batch_first=True)\n    for layer in self.convolution_layers:\n        context_padded = torch.relu(layer(context_padded))\n        context_padded = torch.nn.Dropout(self.dropout)(context_padded)\n    return context_padded\n\n\n", "CodeBleu_score": 35.28345176810184}]}
{"instance_id": "db2644f3c681", "generation_result": [{"code": "def run_unsorted_inputs(self, fn, context, lens)\n\t\n    lens = np.array(lens)\n    inds = np.argsort(lens)\n    lens = lens[inds]\n    context = context[inds]\n    context = np.array([np.array(x) for x in context])\n    context = np.array([np.array(x) for x in context])\n    context = np.array([np.array(x) for x in context])\n    context = np.array([np.array(x) for x in context])\n    context = np.array([np.array(x) for x in context])\n    context = np.array([np.array(x) for x in context])\n    context = np.array([np.array(x) for x in context])\n    context = np.array([np.array(x) for x in context])\n    context = np.array([np.array(x) for x in context])\n    context = np.array([np.array(x) for x in context])\n    context = np.array([np.array(x) for x in context])\n    context = np.array([np.array(x) for x in context])\n    context = np.array([np.array(x) for x in context])\n    context = np.array([np.array(x) for x in context])\n    context = np.array([np.array(x) for x in context])\n    context = np.array([np.array(x) for x in context])\n    context = np.array([np.array(x) for x in context])\n    context = np.array([np.array(x) for x in context])\n    context = np.array([np.array(x) for x in context])\n    context = np.array([np.array(x) for x in context])\n    context = np.array([np.array(x) for x in context])\n    context = np.array([np.array(x) for x in context])\n    context = np.array([np.array(x) for x in context])\n    context = np.array([np.array(x) for x in context])\n    context = np.array([np.array(x)", "CodeBleu_score": 11.795196829149784}, {"code": "def run_unsorted_inputs(self, fn, context, lens)\n\t\n    lens, indices = torch.sort(lens, descending=True)\n    packed_context = pack(context[indices], lens)\n    packed_output = fn(packed_context)\n    output, _ = unpack(packed_output, lens)\n    return output[indices]\n\n", "CodeBleu_score": 6.152256662483306}, {"code": "def run_unsorted_inputs(self, fn, context, lens)\n\t\n    lens_sorted, lens_indices = torch.sort(lens, descending=True)\n    context_sorted = torch.index_select(context, 0, lens_indices)\n    context_packed = context_sorted.view(context.size(0), -1)\n    context_packed_output = fn(context_packed)\n    context_packed_output_unsorted = torch.index_select(context_packed_output, 0, lens_indices)\n    context_output = context_packed_output_unsorted.view(context.size(0), -1, context_packed_output_unsorted.size(-1))\n    return context_output\n\n\n\n", "CodeBleu_score": 16.152097779237987}]}
{"instance_id": "ced082bffc63", "generation_result": [{"code": "def remove_weight_norm(self)\n\t\n    self.kernel_predictor.remove_weight_norm()\n    parametrize.remove_parametrizations(self.convt_pre[1], \"weight\")\n    for block in self.conv_blocks:\n        parametrize.remove_parametrizations(block[1], \"weight\")\n\n\n", "CodeBleu_score": 70.15353357610886}, {"code": "def remove_weight_norm(self)\n\t\n    self.kernel_predictor.remove_weight_norm()\n    parametrize.remove_parametrizations(self.convt_pre[1], \"weight\")\n    for block in self.conv_blocks:\n        parametrize.remove_parametrizations(block[1], \"weight\")\n\n", "CodeBleu_score": 70.15353357610886}, {"code": "def remove_weight_norm(self)\n\t\n    self.kernel_predictor.remove_weight_norm()\n    parametrize.remove_parametrizations(self.convt_pre[1], \"weight\")\n    for block in self.conv_blocks:\n        parametrize.remove_parametrizations(block[1], \"weight\")\n\n\n", "CodeBleu_score": 70.15353357610886}]}
{"instance_id": "b5a39fd97061", "generation_result": [{"code": "def map_mx_to_torch_model(mx_name)\n\t\n    mx_name = mx_name.lower()\n    if mx_name.startswith('se_'):\n        mx_name = mx_name[3:]\n    if mx_name.startswith('senet_'):\n        mx_name = mx_name[6:]\n    if mx_name.startswith('inceptionv3'):\n        mx_name = 'inception_v3'\n    return 'gluon_' + mx_name\n\n", "CodeBleu_score": 31.890158849065987}, {"code": "def map_mx_to_torch_model(mx_name)\n\t\n    mx_name = mx_name.lower()\n    mx_name = mx_name.replace('se_', '')\n    mx_name = mx_name.replace('senet_', '')\n    mx_name = mx_name.replace('inceptionv3', 'inception')\n    return 'gluon_' + mx_name\n\n\n\n", "CodeBleu_score": 30.322632159108153}, {"code": "def map_mx_to_torch_model(mx_name)\n\t\n    mx_name = mx_name.lower()\n    if 'se_' in mx_name:\n        mx_name = mx_name.replace('se_', 'se-')\n    if 'senet_' in mx_name:\n        mx_name = mx_name.replace('senet_', 'se-')\n    if 'inceptionv3' in mx_name:\n        mx_name = mx_name.replace('inceptionv3', 'inception')\n    return 'gluon_' + mx_name\n\n", "CodeBleu_score": 41.41601109390849}]}
{"instance_id": "7eee5efdbf67", "generation_result": [{"code": "def _num_samples_per_worker(self)\n\t\n    num_samples = self.repeats * self.num_samples\n    if self.training or self.multiple_replicas:\n        num_samples = math.ceil(num_samples / self.num_replicas)\n    if self.batch_size:\n        num_samples = (num_samples + self.batch_size - 1) // self.batch_size * self.batch_size\n    return num_samples\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 30.6330375463936}, {"code": "def _num_samples_per_worker(self)\n\t\n    if self.repeats > 1:\n        num_samples = (self.total_samples // self.repeats)\n        num_samples = math.ceil(num_samples / self.num_workers) * self.num_workers\n    else:\n        num_samples = self.total_samples\n    if self.batch_size is not None:\n        num_samples = (num_samples // self.batch_size) * self.batch_size\n    return num_samples\n\n\n", "CodeBleu_score": 42.58087411717171}, {"code": "def _num_samples_per_worker(self)\n\t\n    num_samples_per_worker = self.repeats * self.total_samples // self.num_workers\n    if self.batch_size is not None:\n        num_samples_per_worker = (num_samples_per_worker + self.batch_size - 1) // self.batch_size * self.batch_size\n    return num_samples_per_worker", "CodeBleu_score": 25.63818338065269}]}
{"instance_id": "a698d65ae190", "generation_result": [{"code": "def find_images_and_targets(\n        folder: str,\n        types: Optional[Union[List, Tuple, Set]] = None,\n        class_to_idx: Optional[Dict] = None,\n        leaf_name_only: bool = True,\n        sort: bool = True\n)\n\t\n    if class_to_idx is None:\n        class_to_idx = {}\n        classes = set()\n        for root, _, fnames in sorted(os.walk(folder, followlinks=True)):\n            for fname in sorted(fnames):\n                if leaf_name_only:\n                    path = os.path.relpath(os.path.join(root, fname), folder)\n                else:\n                    path = os.path.join(root, fname)\n                if types is not None:\n                    ext = os.path.splitext(path)[1]\n                    if ext.lower() not in types:\n                        continue\n                class_name = os.path.basename(os.path.dirname(path) if leaf_name_only else path)\n                if class_name not in classes:\n                    idx = len(classes)\n                    classes.add(class_name)\n                    class_to_idx[class_name] = idx\n\n    items = []\n    for root, _, fnames in sorted(os.walk(folder, followlinks=True)):\n        for fname in sorted(fnames):\n            if leaf_name_only:\n                path = os.path.relpath(os.path.join(root, fname), folder)\n            else:\n                path = os.path.join(root, fname)\n            if types is not None:\n                ext = os.path.splitext(path)[1]\n                if ext.lower() not in types:\n                    continue\n            class_name = os.path.basename(os.path.dirname(path) if leaf_name_only else path)\n            items.append((path, class_to_idx[class_name]))\n\n    if sort:\n        items.sort(key=lambda x: natural_key(x[0]))\n\n    return items, class_to_idx\n\n", "CodeBleu_score": 36.34107671782159}, {"code": "def find_images_and_targets(\n        folder: str,\n        types: Optional[Union[List, Tuple, Set]] = None,\n        class_to_idx: Optional[Dict] = None,\n        leaf_name_only: bool = True,\n        sort: bool = True\n)\n\t\n    if class_to_idx is None:\n        unique_labels = set()\n        for root, _, fnames in sorted(os.walk(folder, followlinks=True)):\n            for fname in sorted(fnames):\n                if leaf_name_only:\n                    path = os.path.join(root, fname)\n                else:\n                    path = root\n                if types is not None:\n                    ext = os.path.splitext(path)[1]\n                    if ext.lower() not in types:\n                        continue\n                target = os.path.basename(root)\n                unique_labels.add(target)\n        # Assign an index to each unique label, starting from 0\n        unique_labels = sorted(unique_labels, key=natural_key)\n        class_to_idx = {cls_name: idx\n                        for idx, cls_name in enumerate(unique_labels)}\n    # Find all the image files, recursively\n    instances = []\n    for root, _, fnames in sorted(os.walk(folder, followlinks=True)):\n        for fname in sorted(fnames):\n            if leaf_name_only:\n                path = os.path.join(root, fname)\n            else:\n                path = root\n            if types is not None:\n                ext = os.path.splitext(path)[1]\n                if ext.lower() not in types:\n                    continue\n            target = os.path.basename(root)\n            item = path, class_to_idx[target]\n            instances.append(item)\n    if sort:\n        instances.sort(key=lambda x: natural_key(x[0]))\n    return instances, class_to_idx", "CodeBleu_score": 39.92509962704731}, {"code": "def find_images_and_targets(\n        folder: str,\n        types: Optional[Union[List, Tuple, Set]] = None,\n        class_to_idx: Optional[Dict] = None,\n        leaf_name_only: bool = True,\n        sort: bool = True\n)\n\t\n    if types is not None:\n        types = set(types)\n    image_paths = []\n    for (dirpath, _, filenames) in os.walk(folder):\n        for filename in filenames:\n            if not leaf_name_only:\n                path = os.path.join(dirpath, filename)\n            else:\n                path = filename\n            ext = os.path.splitext(path)[1]\n            if types is not None and ext.lower() not in types:\n                continue\n            image_paths.append(path)\n    if class_to_idx is None:\n        # building class index\n        idx = 0\n        classes, class_to_idx = [], {}\n        for path in image_paths:\n            folder_name = os.path.basename(os.path.split(path)[0])\n            if folder_name not in classes:\n                classes.append(folder_name)\n                class_to_idx[folder_name] = idx\n                idx += 1\n    else:\n        classes = [None] * len(class_to_idx)\n        for key, value in class_to_idx.items():\n            classes[value] = key\n\n    img_name_to_class_idx = [(img_path, class_to_idx[os.path.basename(os.path.split(img_path)[0])]) for img_path in image_paths]\n\n    if sort:\n        img_name_to_class_idx.sort(key=lambda x: natural_key(x[0]))\n\n    return img_name_to_class_idx, classes, class_to_idx\n\n", "CodeBleu_score": 36.47523875919319}]}
{"instance_id": "e7fe43bdecdc", "generation_result": [{"code": "def pytorch_worker_seed()\n\t\n    worker_info = get_worker_info()\n    if worker_info is not None:\n        # favour the seed already created for pytorch dataloader workers if it exists\n        return worker_info.seed\n    # fallback to wds rank based seed\n    return wds.utils.pytorch_worker_seed()\n\n", "CodeBleu_score": 66.80150511570375}, {"code": "def pytorch_worker_seed()\n\t\n    worker_info = get_worker_info()\n    if worker_info is not None:\n        # favour the seed already created for pytorch dataloader workers if it exists\n        return worker_info.seed\n    # fallback to wds rank based seed\n    return wds.utils.pytorch_worker_seed()\n\n", "CodeBleu_score": 66.80150511570375}, {"code": "def pytorch_worker_seed()\n\t\n    worker_info = get_worker_info()\n    if worker_info is not None:\n        # favour the seed already created for pytorch dataloader workers if it exists\n        return worker_info.seed\n    # fallback to wds rank based seed\n    return wds.utils.pytorch_worker_seed()", "CodeBleu_score": 66.80150511570375}]}
{"instance_id": "0ffc7e7ded25", "generation_result": [{"code": "def solarize_add(img, add, thresh=128, **__)\n\t\n    import numpy as np\n    import cv2\n    if img.mode in [\"L\", \"RGB\"]:\n        lut = np.arange(256)\n        lut[thresh:] += 255 - add\n        return img.point(lut)\n    else:\n        return img\n\n\n", "CodeBleu_score": 26.2403912045036}, {"code": "def solarize_add(img, add, thresh=128, **__)\n\t\n    if img.mode == \"L\":\n        lut = [min(255, i + add) for i in range(256)]\n        img = img.point(lut)\n    elif img.mode == \"RGB\":\n        r, g, b = img.split()\n        lut_r = [min(255, i + add) for i in range(256)]\n        lut_g = [min(255, i + add) for i in range(256)]\n        lut_b = [min(255, i + add) for i in range(256)]\n        r = r.point(lut_r)\n        g = g.point(lut_g)\n        b = b.point(lut_b)\n        img = Image.merge(\"RGB\", (r, g, b))\n    else:\n        return img\n    return img\n\n\n", "CodeBleu_score": 37.77499167062157}, {"code": "def solarize_add(img, add, thresh=128, **__)\n\t\n    if img.mode == 'L':\n        lut = np.arange(256)\n        lut[lut < thresh] = lut[lut < thresh] + add\n        return img.point(lut)\n    elif img.mode == 'RGB':\n        lut = np.arange(256)\n        lut[lut < thresh] = lut[lut < thresh] + add\n        return img.point(lut)\n    else:\n        return img\n\n\n", "CodeBleu_score": 39.42691394867163}]}
{"instance_id": "5a0d29577a21", "generation_result": [{"code": "def _get_weighted_transforms(transforms: Dict)\n\t\n    transform_names, transform_probs = zip(*transforms.items())\n    transform_probs = np.array(transform_probs)\n    transform_probs = transform_probs / np.sum(transform_probs)\n    return transform_names, transform_probs\n\n", "CodeBleu_score": 30.101588966387123}, {"code": "def _get_weighted_transforms(transforms: Dict)\n\t\n    transform_names = list(transforms.keys())\n    transform_probs = np.array(list(transforms.values()))\n    transform_probs = transform_probs / np.sum(transform_probs)\n    return transform_names, transform_probs\n\n", "CodeBleu_score": 24.24806282440839}, {"code": "def _get_weighted_transforms(transforms: Dict)\n\t\n    transform_names = []\n    transform_probs = []\n    for transform_name, transform_prob in transforms.items():\n        transform_names.append(transform_name)\n        transform_probs.append(transform_prob)\n    transform_probs = torch.tensor(transform_probs, dtype=torch.float)\n    transform_probs = transform_probs / torch.sum(transform_probs)\n    return transform_names, transform_probs\n\n\n\n\n", "CodeBleu_score": 32.58012759259466}]}
{"instance_id": "84bc5658c135", "generation_result": [{"code": "def _apply_basic(self, img, mixing_weights, m)\n\t\n    mixed_img = np.zeros(img.shape)\n    for weight in mixing_weights:\n        mixed_img += weight * self._augment_image(img, m)\n    mixed_img = np.clip(mixed_img, 0, 255)\n    mixed_img = mixed_img.astype(np.uint8)\n    return cv2.addWeighted(img, 1 - m, mixed_img, m, 0)\n\n\n", "CodeBleu_score": 19.05717356285456}, {"code": "def _apply_basic(self, img, mixing_weights, m)\n\t\n    mixed_img = np.zeros_like(img)\n    for w in mixing_weights:\n        for _ in range(m):\n            aug_list = self.aug_list\n            random.shuffle(aug_list)\n            for aug in aug_list:\n                img = aug(image=img)[\"image\"]\n            mixed_img += img\n    mixed_img = np.clip(mixed_img / m, 0, 255).astype(np.uint8)\n    mixed_img = Image.fromarray(mixed_img)\n    mixed_img = self.blend(img, mixed_img, self.blend_factor)\n    return mixed_img\n\n\n", "CodeBleu_score": 27.74781679645396}, {"code": "def _apply_basic(self, img, mixing_weights, m)\n\t\n    mixed_img = np.zeros_like(img)\n    for w in mixing_weights:\n        seq = self._augment_sequence(m)\n        mixed_img += seq(image=img)['image'] * w\n    mixed_img = np.clip(mixed_img, 0, 255)\n    mixed_img = Image.fromarray(mixed_img.astype('uint8')).convert('RGB')\n    return self._blend(mixed_img, img, m)", "CodeBleu_score": 23.931795464978656}]}
{"instance_id": "84b5b752ff04", "generation_result": [{"code": "def adapt_to_chs(x, n)\n\t\n    if not isinstance(x, (tuple, list)):\n        x = (x,)\n    if len(x) != n:\n        x = tuple(np.mean(x))\n        logging.warning(f'adapt_to_chs: x is not a tuple or list of length {n}, but {len(x)} instead. x was adapted to a tuple of length {n} by repeating the mean of x {n} times.')\n    assert len(x) == n, f'adapt_to_chs: x is not a tuple or list of length {n}, but {len(x)} instead.'\n    return x\n\n\n", "CodeBleu_score": 39.61442089614038}, {"code": "def adapt_to_chs(x, n)\n\t\n    if not isinstance(x, (tuple, list)):\n        x = (x, ) * n\n    elif len(x) != n:\n        x = np.mean(x, axis=0)\n        x = (x, ) * n\n        logger.warning(f\"Input x has length {len(x)}, but n is {n}. The mean of x is used to adapt x to n channels.\")\n    assert len(x) == n, f\"Input x has length {len(x)}, but n is {n}.\"\n    return x\n\n\n", "CodeBleu_score": 45.278310918631384}, {"code": "def adapt_to_chs(x, n)\n\t\n    if not isinstance(x, (tuple, list)):\n        x = (x,) * n\n    elif len(x) != n:\n        x = tuple(np.mean(x))\n        logging.warning(f\"Input x has length {len(x)}, but n is {n}. Using mean of x.\")\n    assert len(x) == n\n    return x\n\n\n", "CodeBleu_score": 42.07839122729678}]}
{"instance_id": "54754b6ba394", "generation_result": [{"code": "def _worker_init(worker_id, worker_seeding='all')\n\t\n    worker_info = torch.utils.data.get_worker_info()\n    if worker_info is None:\n        return\n    if worker_seeding is None:\n        return\n    worker_id = worker_info.id\n    if worker_seeding == 'all':\n        seed = worker_info.seed\n        random.seed(seed)\n        torch.manual_seed(seed)\n        np.random.seed(seed)\n    elif callable(worker_seeding):\n        seed = worker_seeding(worker_id)\n        random.seed(seed)\n        torch.manual_seed(seed)\n        np.random.seed(seed)\n    else:\n        raise ValueError(f'Invalid worker_seeding: {worker_seeding}')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 48.158466469370424}, {"code": "def _worker_init(worker_id, worker_seeding='all')\n\t\n    worker_info = torch.utils.data.get_worker_info()\n    if worker_info is None:\n        return\n    if worker_id != worker_info.id:\n        raise RuntimeError(f'Worker ID {worker_id} does not match worker info ID {worker_info.id}')\n    if worker_seeding == 'all':\n        seed = worker_info.seed\n        random.seed(seed)\n        torch.manual_seed(seed)\n        np.random.seed(seed)\n    elif callable(worker_seeding):\n        worker_seeding(worker_id)\n    else:\n        raise ValueError(f'Invalid worker_seeding {worker_seeding}')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 39.322190965128364}, {"code": "def _worker_init(worker_id, worker_seeding='all')\n\t\n    worker_info = torch.utils.data.get_worker_info()\n    if worker_info is None:\n        return\n    if worker_seeding == 'all':\n        worker_seed = worker_info.seed\n        np.random.seed(worker_seed)\n        random.seed(worker_seed)\n        torch.manual_seed(worker_seed)\n        torch.cuda.manual_seed(worker_seed)\n        torch.cuda.manual_seed_all(worker_seed)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n    elif callable(worker_seeding):\n        worker_seed = worker_seeding(worker_id)\n        np.random.seed(worker_seed)\n        random.seed(worker_seed)\n        torch.manual_seed(worker_seed)\n        torch.cuda.manual_seed(worker_seed)\n        torch.cuda.manual_seed_all(worker_seed)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n    else:\n        raise ValueError(f'Invalid worker_seeding: {worker_seeding}')\n\n", "CodeBleu_score": 38.48813324299438}]}
{"instance_id": "81dfdce940db", "generation_result": [{"code": "def _decode_and_center_crop(image_bytes, image_size, resize_method)\n\t\n    image_shape = tf.image.extract_jpeg_shape(image_bytes)\n    padded_center_crop_size = tf.cast(\n        ((image_size / (image_size + PADDING)) *\n         tf.cast(tf.minimum(image_shape[0], image_shape[1]), tf.float32)),\n        tf.int32)\n    offset_height = ((image_shape[0] - padded_center_crop_size) + 1) // 2\n    offset_width = ((image_shape[1] - padded_center_crop_size) + 1) // 2\n    crop_window = tf.stack([offset_height, offset_width, padded_center_crop_size, padded_center_crop_size])\n    image = tf.image.decode_and_crop_jpeg(image_bytes, crop_window, channels=3)\n    image = tf.image.resize(image, [image_size, image_size], method=resize_method)\n    return image\n\n", "CodeBleu_score": 49.1534155378859}, {"code": "def _decode_and_center_crop(image_bytes, image_size, resize_method)\n\t\n    image = tf.image.decode_jpeg(image_bytes, channels=3)\n    shape = tf.shape(image)\n    image_height = shape[0]\n    image_width = shape[1]\n    padded_center_crop_size = tf.cast(\n        ((image_size / (image_size + PADDING)) *\n         tf.cast(tf.minimum(image_height, image_width), tf.float32)), tf.int32)\n    offset_height = ((image_height - padded_center_crop_size) + 1) // 2\n    offset_width = ((image_width - padded_center_crop_size) + 1) // 2\n    image = tf.image.crop_to_bounding_box(\n        image, offset_height, offset_width, padded_center_crop_size,\n        padded_center_crop_size)\n    image = resize_method(image, [image_size, image_size])\n    return image\n\n", "CodeBleu_score": 49.89446152235366}, {"code": "def _decode_and_center_crop(image_bytes, image_size, resize_method)\n\t\n    image = tf.io.decode_jpeg(image_bytes, channels=3)\n    image_shape = tf.shape(image)\n    padded_center_crop_size = tf.cast(\n        ((1 + PADDING) * image_size + (PADDING - 1) * image_shape) / image_size,\n        tf.int32,\n    )\n    offset_height = ((image_shape[0] - padded_center_crop_size[0]) + 1) // 2\n    offset_width = ((image_shape[1] - padded_center_crop_size[1]) + 1) // 2\n    image = tf.image.crop_to_bounding_box(\n        image, offset_height, offset_width, padded_center_crop_size[0], padded_center_crop_size[1]\n    )\n    image = tf.image.resize(image, [image_size, image_size], method=resize_method)\n    return image\n\n", "CodeBleu_score": 33.07010781895067}]}
{"instance_id": "cf16a6d37111", "generation_result": [{"code": "def preprocess_for_train(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    resize_method = _interpolation_to_resize_method(interpolation)\n    image = _decode_and_random_crop(image_bytes, image_size, resize_method)\n    image = _flip(image)\n    image = tf.reshape(image, [image_size, image_size, 3])\n    image = tf.image.convert_image_dtype(image, dtype=_dtype(use_bfloat16))\n    return image\n\n", "CodeBleu_score": 39.6611622961734}, {"code": "def preprocess_for_train(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    resize_method = tf.image.ResizeMethod.BICUBIC if (interpolation == 'bicubic') else tf.image.ResizeMethod.BILINEAR\n    image = _decode_and_random_crop(image_bytes, image_size, resize_method)\n    image = _flip(image)\n    image = tf.reshape(image, [image_size, image_size, 3])\n    image = tf.image.convert_image_dtype(image, dtype=tf.bfloat16 if use_bfloat16 else tf.float32)\n    return image\n\n", "CodeBleu_score": 78.29207008064606}, {"code": "def preprocess_for_train(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    with tf.name_scope('preprocess_for_train'):\n        # Decode image bytes to float32\n        image = tf.image.decode_jpeg(image_bytes, channels=3)\n        # Resize image to the target size\n        image = tf.image.resize(image, [image_size, image_size], interpolation=interpolation)\n        # Randomly crop a [height, width] section of the image\n        image = _random_crop(image)\n        # Randomly flip the image horizontally\n        image = _flip(image)\n        # Convert image to the specified dtype\n        if use_bfloat16:\n            image = tf.cast(image, dtype=tf.bfloat16)\n        else:\n            image = tf.cast(image, dtype=tf.float32)\n        return image\n\n", "CodeBleu_score": 47.34349007136689}]}
{"instance_id": "78b2a3bd2142", "generation_result": [{"code": "def preprocess_for_eval(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    if interpolation == 'bicubic':\n        resize_method = tf.image.ResizeMethod.BICUBIC\n    elif interpolation == 'bilinear':\n        resize_method = tf.image.ResizeMethod.BILINEAR\n    elif interpolation == 'nearest':\n        resize_method = tf.image.ResizeMethod.NEAREST_NEIGHBOR\n    else:\n        raise ValueError('Unknown interpolation method: {}'.format(interpolation))\n\n    image = _decode_and_center_crop(image_bytes, image_size, resize_method)\n    image = tf.cast(image, tf.bfloat16 if use_bfloat16 else tf.float32)\n\n    return image\n\n", "CodeBleu_score": 49.378121947050296}, {"code": "def preprocess_for_eval(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    if interpolation == 'bicubic':\n        resize_method = tf.image.ResizeMethod.BICUBIC\n    elif interpolation == 'bilinear':\n        resize_method = tf.image.ResizeMethod.BILINEAR\n    else:\n        raise ValueError('Unknown interpolation method')\n\n    image = _decode_and_center_crop(image_bytes, image_size, resize_method)\n\n    if use_bfloat16:\n        image = tf.image.convert_image_dtype(image, dtype=tf.bfloat16)\n    else:\n        image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n\n    return image\n\n", "CodeBleu_score": 46.8540459876297}, {"code": "def preprocess_for_eval(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    if interpolation == 'bilinear':\n        resize_method = tf.image.ResizeMethod.BILINEAR\n    elif interpolation == 'bicubic':\n        resize_method = tf.image.ResizeMethod.BICUBIC\n    else:\n        raise ValueError('Unknown interpolation method: ' + interpolation)\n\n    image = _decode_and_center_crop(image_bytes, image_size, resize_method)\n    image = tf.cast(image, tf.bfloat16 if use_bfloat16 else tf.float32)\n\n    return image\n\n", "CodeBleu_score": 51.704754680933306}]}
{"instance_id": "8bd4364a9a0e", "generation_result": [{"code": "def evaluate(self, model, eval_dataloader=None, output_file=\"merged\")\n\t\n\n    # Set up the device\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # Set the model to evaluation mode\n    model.eval()\n\n    # Initialize the output file\n    output_file = open(output_file, \"w\")\n\n    # Initialize the evaluation metrics\n    metrics = {\n        \"accuracy\": 0,\n        \"precision\": 0,\n        \"recall\": 0,\n        \"f1\": 0,\n        \"auc\": 0\n    }\n\n    # Initialize the counters\n    total_examples = 0\n    correct_examples = 0\n    true_positives = 0\n    false_positives = 0\n    false_negatives = 0\n\n    # Loop over the evaluation dataloader\n    with torch.no_grad():\n        for batch in eval_dataloader:\n            # Get the input and label from the batch\n            input_ids = batch[\"input_ids\"].to(device)\n            attention_mask = batch[\"attention_mask\"].to(device)\n            labels = batch[\"labels\"].to(device)\n\n            # Get the model outputs\n            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n            logits = outputs.logits\n\n            # Get the predicted labels\n            _, predicted_labels = torch.max(logits, dim=1)\n\n            # Update the counters\n            total_examples += labels.size(0)\n            correct_examples += (predicted_labels == labels).sum().item()\n            true_positives += ((predicted_labels == 1) & (labels == 1)).sum().item()\n            false_positives += ((predicted_labels == 1) & (labels == 0)).sum().item()\n            false_negatives += ((predicted_labels == 0) & (labels == 1)).sum().item()\n\n            # Write the outputs to the output file\n            for i in range(labels.size(0)):\n                output_file.write(\n                    f\"{predicted_labels[i].item()}\\t{labels[i].item()}", "CodeBleu_score": 29.050781810269555}, {"code": "def evaluate(self, model, eval_dataloader=None, output_file=\"merged\")\n\t\n    if eval_dataloader is None:\n        eval_dataloader = self.eval_dataloader\n    predictions = []\n    for batch in tqdm(eval_dataloader):\n        predictions.extend(model.predict(batch))\n    self.save_predictions(predictions, output_file)\n    return self.compute_metrics(predictions)\n\n\n", "CodeBleu_score": 46.98577422245784}, {"code": "def evaluate(self, model, eval_dataloader=None, output_file=\"merged\")\n\t\n    if eval_dataloader is None:\n        eval_dataloader = self.eval_dataloader\n    if eval_dataloader is None:\n        raise ValueError(\"No evaluation dataloader provided\")\n    model.eval()\n    predictions = []\n    for batch in eval_dataloader:\n        with torch.no_grad():\n            outputs = model(**batch)\n        predictions.extend(outputs.logits.argmax(dim=-1).tolist())\n    labels = [label for batch in eval_dataloader for label in batch[\"labels\"]]\n    if len(predictions) != len(labels):\n        raise ValueError(\"Number of predictions does not match number of labels\")\n    with open(output_file, \"w\") as f:\n        for prediction, label in zip(predictions, labels):\n            f.write(str(prediction) + \",\" + str(label) + \"", "CodeBleu_score": 41.504378118549404}]}
{"instance_id": "22d682c958c0", "generation_result": [{"code": " def print_computed_metrics(self, metrics)\n\t\n    rank1 = metrics['rank1']\n    rank5 = metrics['rank5']\n    miou = metrics['miou']\n    print('rank1: %.4f, rank5: %.4f, miou: %.4f' % (rank1, rank5, miou))", "CodeBleu_score": 22.393593396925198}, {"code": " def print_computed_metrics(self, metrics)\n\t\n    rank1 = metrics['rank1']\n    rank5 = metrics['rank5']\n    miou = metrics['miou']\n    print('rank1: {:.4f}, rank5: {:.4f}, miou: {:.4f}'.format(rank1, rank5, miou))\n\n\n\n\n def compute_metrics(self, predictions, labels):\n    \"\"\"The function computes the metrics for rank1, rank5, and miou.", "CodeBleu_score": 29.182774177223724}, {"code": " def print_computed_metrics(self, metrics)\n\t\n    print(f\"Rank1: {metrics['rank1']:.4f}\")\n    print(f\"Rank5: {metrics['rank5']:.4f}\")\n    print(f\"mIoU: {metrics['miou']:.4f}\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 18.579754787867735}]}
{"instance_id": "39c1a50b7e7d", "generation_result": [{"code": "def _eval_predictions(self, segments, data)\n\t\n    rank1, rank5, mean_iou = 0, 0, 0\n    for segment, gt in zip(segments, data):\n        iou = [self._iou(pred, gt) for pred in segment]\n        rank = [self._rank(pred, gt) for pred in segment]\n        rank1 += max(rank)\n        rank5 += sum(rank)\n        mean_iou += sum(iou)\n    rank1 = rank1 / len(segments)\n    rank5 = rank5 / len(segments)\n    mean_iou = mean_iou / len(segments)\n    return rank1, rank5, mean_iou\n\n", "CodeBleu_score": 23.959124627168936}, {"code": "def _eval_predictions(self, segments, data)\n\t\n    rank1 = 0\n    rank5 = 0\n    mean_iou = 0\n    for segment in segments:\n        iou_scores = [self._iou(segment, data_segment) for data_segment in data]\n        rank_scores = [self._rank(segment, data_segment) for data_segment in data]\n        rank1 += max(rank_scores)\n        rank5 += sum(rank_scores)\n        mean_iou += max(iou_scores)\n    rank1 /= len(segments)\n    rank5 /= len(segments)\n    mean_iou /= len(segments)\n    return rank1, rank5, mean_iou\n\n", "CodeBleu_score": 23.462701419222327}, {"code": "def _eval_predictions(self, segments, data)\n\t\n    rank1, rank5, mean_iou = 0, 0, 0\n    for segment, gt in zip(segments, data):\n        iou_list = [self._iou(pred, gt) for pred in segment]\n        rank_list = [self._rank(pred, gt) for pred in segment]\n        rank1 += max(rank_list)\n        rank5 += sum(rank_list[:5])\n        mean_iou += max(iou_list)\n    rank1 /= len(segments)\n    rank5 /= len(segments)\n    mean_iou /= len(segments)\n    return rank1, rank5, mean_iou\n\n", "CodeBleu_score": 23.207770698727845}]}
{"instance_id": "8ccf84f8d034", "generation_result": [{"code": "def predict_loop(self, model, eval_dataloader, output_file=None)\n\t\n    scores = []\n    model.eval()\n    model.to(0)\n    for batch in tqdm(eval_dataloader):\n        batch = self.to_ctx(batch, 0)\n        with torch.no_grad():\n            outputs = model(**batch)\n        outputs.update(batch)\n        outputs = self.process_outputs(outputs)\n        scores.append(outputs)\n    self.finalize(output_file)\n    return scores\n\n", "CodeBleu_score": 46.06855671096451}, {"code": "def predict_loop(self, model, eval_dataloader, output_file=None)\n\t\n    scores = []\n    model.eval()\n    model.to(self.device)\n    for batch in tqdm(eval_dataloader):\n        batch = self.to_ctx(batch, self.device)\n        with torch.no_grad():\n            outputs = model(**batch)\n        outputs.update(batch)\n        outputs = self.process(outputs)\n        scores.append(outputs)\n    self.finalize(output_file)\n    return scores\n\n", "CodeBleu_score": 46.77960376102506}, {"code": "def predict_loop(self, model, eval_dataloader, output_file=None)\n\t\n    scores = []\n    model.eval()\n    model.to(0)\n    for batch in eval_dataloader:\n        batch = self.to_ctx(batch, ctx=0)\n        with torch.no_grad():\n            outputs = model(**batch)\n        outputs = self.update_outputs(outputs, batch)\n        outputs = self.process_outputs(outputs)\n        scores.extend(outputs)\n    self.finalize(output_file)\n    return scores\n\n", "CodeBleu_score": 46.570889570807964}]}
{"instance_id": "e68529ad727d", "generation_result": [{"code": "def _aggregate_scores(self, scores)\n\t\n    assert len(scores) == 2\n    text_scores = scores[0]\n    video_scores = scores[1]\n    full_scores = torch.cat([text_scores, video_scores], dim=1)\n    self.full_scores = None\n    return full_scores\n\n\n", "CodeBleu_score": 39.1520295554526}, {"code": "def _aggregate_scores(self, scores)\n\t\n    assert len(scores) == 2\n    text_scores, video_scores = scores\n    full_scores = torch.cat((text_scores, video_scores), dim=0)\n    self.full_scores = full_scores\n    return torch.mm(text_scores, video_scores.t())\n\n", "CodeBleu_score": 34.70915271138898}, {"code": "def _aggregate_scores(self, scores)\n\t\n    assert len(scores) == 2\n    text_scores = scores[0]\n    video_scores = scores[1]\n    full_scores = torch.cat((text_scores, video_scores), dim=1)\n    self.full_scores = None\n    return torch.mm(text_scores, video_scores.t())", "CodeBleu_score": 39.1852110290545}]}
{"instance_id": "e4e1861fc357", "generation_result": [{"code": "def finalize(self, Y_pred, Y_true, output_file=None)\n\t\n\n    # Concatenate the predicted and true labels\n    Y_pred = np.concatenate(Y_pred)\n    Y_true = np.concatenate(Y_true)\n\n    # Convert the predicted and true labels to numpy arrays\n    Y_pred = np.array(Y_pred)\n    Y_true = np.array(Y_true)\n\n    # Check for prediction errors\n    error_indices = np.where(Y_pred != Y_true)[0]\n    if len(error_indices) > 0:\n        # Print the first 10 samples of the errors\n        print(\"Prediction errors:\")\n        for i in range(min(len(error_indices), 10)):\n            print(f\"Error {i+1}:\")\n            print(f\"Predicted label: {Y_pred[error_indices[i]]}\")\n            print(f\"True label: {Y_true[error_indices[i]]}\")\n\n        # Save the predictions and true labels as a pickle file\n        if output_file is not None:\n            with open(output_file, 'wb') as f:\n                pickle.dump({'Y_pred': Y_pred, 'Y_true': Y_true}, f)\n\n    return {'Y_pred': Y_pred, 'Y_true': Y_true}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 41.74689457713938}, {"code": "def finalize(self, Y_pred, Y_true, output_file=None)\n\t\n    Y_pred = np.concatenate(Y_pred)\n    Y_true = np.concatenate(Y_true)\n\n    if len(Y_pred) != len(Y_true):\n        raise ValueError(\n            f\"Predicted and true labels have different lengths: {len(Y_pred)} and {len(Y_true)}\")\n\n    error_indices = np.where(Y_pred != Y_true)[0]\n    if len(error_indices) > 0:\n        print(\n            f\"Found {len(error_indices)} errors. First 10 indices: {error_indices[:10]}\")\n\n    if output_file:\n        with open(output_file, \"wb\") as f:\n            pickle.dump({\"Y_pred\": Y_pred, \"Y_true\": Y_true}, f)\n\n    return {\"Y_pred\": Y_pred, \"Y_true\": Y_true}", "CodeBleu_score": 36.73546371956328}, {"code": "def finalize(self, Y_pred, Y_true, output_file=None)\n\t\n\n    Y_pred = np.concatenate(Y_pred, axis=0)\n    Y_true = np.concatenate(Y_true, axis=0)\n\n    error_indices = np.where(Y_pred != Y_true)[0]\n    if len(error_indices) > 0:\n        print('\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 6.713850881231259}]}
{"instance_id": "7df9ab9a4920", "generation_result": [{"code": "def reduce_metrics(logging_outputs) -> None\n\t\n    loss_sum = sum(log.get('loss', 0) for log in logging_outputs)\n    sample_size = sum(log.get('sample_size', 0) for log in logging_outputs)\n    loss_sum /= sample_size\n    metrics.log_scalar('loss', loss_sum, sample_size, round=3)\n\n\n", "CodeBleu_score": 51.12247454877268}, {"code": "def reduce_metrics(logging_outputs) -> None\n\t\n    loss_sum = sum(log.get('loss', 0) for log in logging_outputs)\n    sample_size_sum = sum(log.get('sample_size', 0) for log in logging_outputs)\n    logging.info(\n        'loss={:.3f}, sample_size={}'.format(loss_sum / sample_size_sum, sample_size_sum))\n\n", "CodeBleu_score": 40.840946931814074}, {"code": "def reduce_metrics(logging_outputs) -> None\n\t\n    loss_sum = sum(log.get('loss', 0) for log in logging_outputs)\n    sample_size = sum(log.get('sample_size', 0) for log in logging_outputs)\n\n    metrics.log_scalar('loss', loss_sum / sample_size / math.log(2), sample_size, round=3)\n\n\n", "CodeBleu_score": 51.67827394838804}]}
{"instance_id": "7f9c326a96f7", "generation_result": [{"code": "def _mm_on_the_fly(\n        self,\n        cmasks,\n        vmasks,\n        attention_mask\n    )\n\t\n\n    if attention_mask is None:\n        attention_mask = self._mm_attention_mask(cmasks, vmasks)\n\n    # Create token type IDs by concatenating zeros for visual tokens and ones for text tokens.\n    token_type_ids = torch.cat([torch.zeros_like(vmasks), torch.ones_like(cmasks)], dim=1)\n\n    return attention_mask, token_type_ids\n\n", "CodeBleu_score": 33.276029921609336}, {"code": "def _mm_on_the_fly(\n        self,\n        cmasks,\n        vmasks,\n        attention_mask\n    )\n\t\n    if attention_mask is None:\n        attention_mask = self._mm_attention_mask(cmasks, vmasks)\n    token_type_ids = torch.cat([torch.zeros_like(vmasks), torch.ones_like(cmasks)], dim=1)\n    return attention_mask, token_type_ids\n\n", "CodeBleu_score": 33.276029921609336}, {"code": "def _mm_on_the_fly(\n        self,\n        cmasks,\n        vmasks,\n        attention_mask\n    )\n\t\n    if attention_mask is None:\n        attention_mask = self._mm_attention_mask(cmasks, vmasks)\n    token_type_ids = torch.cat(\n        [\n            torch.zeros_like(vmasks),\n            torch.ones_like(cmasks),\n        ],\n        dim=1,\n    )\n    return attention_mask, token_type_ids\n\n", "CodeBleu_score": 41.364574302810546}]}
{"instance_id": "fa99d2a148a5", "generation_result": [{"code": "def prepare_inputs_for_generation(\n        self,\n        input_ids,\n        input_video_embeds,\n        attention_mask=None,\n        token_type_ids=None,\n        **model_kwargs\n    )\n\t\n    # adjust sequence length to match combined length of input_ids and input_video_embeds\n    input_ids = input_ids[:, :-input_video_embeds.shape[1]]\n    attention_mask = attention_mask[:, :-input_video_embeds.shape[1]]\n    token_type_ids = token_type_ids[:, :-input_video_embeds.shape[1]]\n    # modify attention_mask and token_type_ids to fit the new sequence length\n    attention_mask = torch.cat([attention_mask, torch.ones((input_ids.shape[0], input_video_embeds.shape[1]), dtype=torch.long)], dim=1)\n    token_type_ids = torch.cat([token_type_ids, torch.ones((input_ids.shape[0], input_video_embeds.shape[1]), dtype=torch.long)], dim=1)\n    # add input_video_embeds to the end of input_ids\n    input_ids = torch.cat([input_ids, input_video_embeds], dim=1)\n    return {\n        \"input_ids\": input_ids,\n        \"attention_mask\": attention_mask,\n        \"token_type_ids\": token_type_ids,\n    }", "CodeBleu_score": 39.23258853913899}, {"code": "def prepare_inputs_for_generation(\n        self,\n        input_ids,\n        input_video_embeds,\n        attention_mask=None,\n        token_type_ids=None,\n        **model_kwargs\n    )\n\t\n    # Adjust sequence length to match the combined length of input_ids and input_video_embeds\n    input_ids = input_ids.reshape(-1)\n    input_video_embeds = input_video_embeds.reshape(-1, input_video_embeds.shape[-1])\n    combined_length = input_ids.shape[0] + input_video_embeds.shape[0]\n    input_ids = input_ids[:combined_length]\n    input_video_embeds = input_video_embeds[:combined_length]\n\n    # Modify attention_mask and token_type_ids to fit the new sequence length\n    if attention_mask is not None:\n        attention_mask = attention_mask.reshape(-1)\n        attention_mask = attention_mask[:combined_length]\n    if token_type_ids is not None:\n        token_type_ids = token_type_ids.reshape(-1)\n        token_type_ids = token_type_ids[:combined_length]\n\n    # Return the updated inputs as a dictionary\n    return {\n        \"input_ids\": input_ids,\n        \"input_video_embeds\": input_video_embeds,\n        \"attention_mask\": attention_mask,\n        \"token_type_ids\": token_type_ids,\n        **model_kwargs,\n    }\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 46.489788911816824}, {"code": "def prepare_inputs_for_generation(\n        self,\n        input_ids,\n        input_video_embeds,\n        attention_mask=None,\n        token_type_ids=None,\n        **model_kwargs\n    )\n\t\n    input_ids = input_ids.to(self.device)\n    input_video_embeds = input_video_embeds.to(self.device)\n    if attention_mask is not None:\n        attention_mask = attention_mask.to(self.device)\n    if token_type_ids is not None:\n        token_type_ids = token_type_ids.to(self.device)\n    if attention_mask is None:\n        attention_mask = torch.ones_like(input_ids, device=self.device)\n    if token_type_ids is None:\n        token_type_ids = torch.zeros_like(input_ids, device=self.device)\n    if self.model_type == 'text-video':\n        input_ids = input_ids.unsqueeze(0)\n        input_video_embeds = input_video_embeds.unsqueeze(0)\n        attention_mask = attention_mask.unsqueeze(0)\n        token_type_ids = token_type_ids.unsqueeze(0)\n    return {\n        'input_ids': input_ids,\n        'input_video_embeds': input_video_embeds,\n        'attention_mask': attention_mask,\n        'token_type_ids': token_type_ids,\n        **model_kwargs\n    }\n\n\n", "CodeBleu_score": 39.85511873460159}]}
{"instance_id": "b1b0d25fe620", "generation_result": [{"code": "def _init_pipeline(self, cfg: ConfigType) -> Compose\n\t\n    pipeline_cfg = cfg['pipeline']\n    if 'img_id' in pipeline_cfg[0]['meta_keys']:\n        pipeline_cfg[0]['meta_keys'].remove('img_id')\n    load_image_idx = self._get_transform_idx(pipeline_cfg, 'LoadImageFromFile')\n    if load_image_idx == -1:\n        raise ValueError('LoadImageFromFile transform not found in the '\n                         'pipeline')\n    pipeline_cfg[load_image_idx]['type'] = 'mmdet.InferencerLoader'\n    return Compose(pipeline_cfg)\n\n", "CodeBleu_score": 38.906421798570165}, {"code": "def _init_pipeline(self, cfg: ConfigType) -> Compose\n\t\n    pipeline_cfg = cfg['pipeline']\n    if 'img_id' in pipeline_cfg[0]['meta_keys']:\n        pipeline_cfg[0]['meta_keys'].remove('img_id')\n    load_image_idx = self._get_transform_idx(pipeline_cfg, 'LoadImageFromFile')\n    if load_image_idx == -1:\n        raise ValueError('LoadImageFromFile transform not found in pipeline')\n    pipeline_cfg[load_image_idx]['type'] = 'mmdet.InferencerLoader'\n    return Compose(pipeline_cfg)\n\n", "CodeBleu_score": 38.22889204964367}, {"code": "def _init_pipeline(self, cfg: ConfigType) -> Compose\n\t\n    if 'meta_keys' in cfg and 'img_id' in cfg['meta_keys']:\n        cfg['meta_keys'].remove('img_id')\n    load_image_from_file_idx = self._get_transform_idx(cfg, 'LoadImageFromFile')\n    if load_image_from_file_idx == -1:\n        raise ValueError('LoadImageFromFile is not found in the pipeline')\n    cfg[load_image_from_file_idx]['type'] = 'mmdet.InferencerLoader'\n    return Compose(cfg)\n\n", "CodeBleu_score": 36.84454859763449}]}
{"instance_id": "7967962856eb", "generation_result": [{"code": "def _inputs_to_list(self, inputs: InputsType) -> list\n\t\n    if isinstance(inputs, (str, Path)):\n        inputs = [inputs]\n    if not isinstance(inputs, (list, tuple)):\n        inputs = [inputs]\n    return inputs\n\n\n", "CodeBleu_score": 23.452859736745207}, {"code": "def _inputs_to_list(self, inputs: InputsType) -> list\n\t\n    if isinstance(inputs, (str, Path)):\n        inputs = list(Path(inputs).glob('*'))\n    if not isinstance(inputs, (list, tuple)):\n        inputs = [inputs]\n    return inputs\n\n\n", "CodeBleu_score": 33.98472589571415}, {"code": "def _inputs_to_list(self, inputs: InputsType) -> list\n\t\n    if isinstance(inputs, str):\n        inputs = self._list_images_in_directory(inputs)\n    if not isinstance(inputs, (list, tuple)):\n        inputs = [inputs]\n    return inputs\n\n\n\n\n", "CodeBleu_score": 31.776631986936838}]}
{"instance_id": "74f727f06fa0", "generation_result": [{"code": "def _get_chunk_data(self, inputs: Iterable, chunk_size: int)\n\t\n    if chunk_size < 1:\n        raise ValueError(f'chunk_size must be greater than 0, got {chunk_size}')\n    inputs_iter = iter(inputs)\n    try:\n        while True:\n            chunk = []\n            for _ in range(chunk_size):\n                try:\n                    input_data = next(inputs_iter)\n                except StopIteration:\n                    break\n                if isinstance(input_data, dict):\n                    if 'img' in input_data:\n                        input_data = input_data['img']\n                    elif 'img_path' in input_data:\n                        input_data = self.load_image(input_data['img_path'])\n                    else:\n                        raise ValueError(f\"Invalid input data: {input_data}\")\n                chunk.append(input_data)\n            if chunk:\n                yield self.pipeline(chunk)\n            else:\n                break\n    except GeneratorExit:\n        pass\n\n\n", "CodeBleu_score": 48.02349187612261}, {"code": "def _get_chunk_data(self, inputs: Iterable, chunk_size: int)\n\t\n    if chunk_size <= 0:\n        raise ValueError('chunk_size must be greater than 0')\n    chunk = []\n    for input in inputs:\n        if isinstance(input, dict) and ('img' in input or 'img_path' in input):\n            input = self.pipeline(input)\n        chunk.append(input)\n        if len(chunk) == chunk_size:\n            yield chunk\n            chunk = []\n    if chunk:\n        yield chunk\n\n\n", "CodeBleu_score": 25.91645414902436}, {"code": "def _get_chunk_data(self, inputs: Iterable, chunk_size: int)\n\t\n    inputs = iter(inputs)\n    chunk = []\n    try:\n        while True:\n            for _ in range(chunk_size):\n                input_ = next(inputs)\n                if isinstance(input_, dict):\n                    if 'img' in input_:\n                        chunk.append(self.pipeline(input_['img']))\n                    elif 'img_path' in input_:\n                        chunk.append(self.pipeline(input_['img_path']))\n                    else:\n                        raise ValueError('Invalid input dictionary format.')\n                else:\n                    chunk.append(self.pipeline(input_))\n            yield chunk\n            chunk = []\n    except StopIteration:\n        if chunk:\n            yield chunk\n\n\n", "CodeBleu_score": 48.506612114390684}]}
{"instance_id": "a43ab51afa59", "generation_result": [{"code": "def postprocess(\n        self,\n        preds: PredType,\n        visualization: Optional[List[np.ndarray]] = None,\n        return_datasamples: bool = False,\n        print_result: bool = False,\n        no_save_pred: bool = False,\n        pred_out_dir: str = '',\n        **kwargs,\n    ) -> Dict\n\t\n    if not no_save_pred and pred_out_dir == '':\n        pred_out_dir = osp.join(self.work_dir, 'preds')\n        mmengine.mkdir_or_exist(pred_out_dir)\n    if return_datasamples and pred_out_dir != '':\n        warnings.warn(\n            'Saving datasamples is not supported when return_datasamples is '\n            'True.')\n\n    result = {}\n    if not return_datasamples:\n        result['predictions'] = []\n    if visualization is not None:\n        result['visualization'] = []\n\n    if isinstance(preds, list):\n        for data_sample in preds:\n            if return_datasamples:\n                result['predictions'].append(data_sample)\n            else:\n                result['predictions'].append(\n                    self.pred2dict(data_sample, pred_out_dir))\n    else:\n        if return_datasamples:\n            result['predictions'].append(preds)\n        else:\n            result['predictions'].append(\n                self.pred2dict(preds, pred_out_dir))\n\n    if visualization is not None:\n        for vis in visualization:\n            result['visualization'].append(vis)\n\n    if print_result:\n        print_dict = {}\n        if 'predictions' in result:\n            print_dict['predictions'] = result['predictions']\n        if 'visualization' in result:\n            print_dict['visualization'] = result['visualization']\n        print(json.dumps(print_dict, indent=4))\n\n    return result", "CodeBleu_score": 40.87021668075528}, {"code": "def postprocess(\n        self,\n        preds: PredType,\n        visualization: Optional[List[np.ndarray]] = None,\n        return_datasamples: bool = False,\n        print_result: bool = False,\n        no_save_pred: bool = False,\n        pred_out_dir: str = '',\n        **kwargs,\n    ) -> Dict\n\t\n    if visualization is not None:\n        if pred_out_dir != '':\n            warnings.warn(\n                'The visualization is only saved in the output directory, '\n                'and the prediction results are not saved.')\n        if return_datasamples:\n            warnings.warn(\n                'The visualization is only saved in the output directory, '\n                'and the prediction results are not saved.')\n\n    if pred_out_dir != '' and return_datasamples:\n        warnings.warn(\n            'The visualization is only saved in the output directory, '\n            'and the prediction results are not saved.')\n\n    if pred_out_dir != '' and no_save_pred:\n        warnings.warn(\n            'The visualization is only saved in the output directory, '\n            'and the prediction results are not saved.')\n\n    if pred_out_dir != '':\n        mmengine.mkdir_or_exist(osp.join(pred_out_dir, 'preds'))\n\n    if return_datasamples and not no_save_pred:\n        warnings.warn(\n            'The visualization is only saved in the output directory, '\n            'and the prediction results are not saved.')\n\n    if return_datasamples:\n        warnings.warn(\n            'The visualization is only saved in the output directory, '\n            'and the prediction results are not saved.')\n\n    if return_datasamples:\n        warnings.warn(\n            'The visualization is only saved in the output directory, '\n            'and the prediction results are not saved.')\n\n    result = {}\n    if visualization is not None:\n        result['visualization'] = visualization\n\n    if return_datasamples:\n        result['datasamples'] = []\n\n    for pred in preds:\n        result_dict = self.pred2dict(pred, pred_out_dir)\n        if return_datasamples:\n            result['datasamples'].append(result_dict)\n        else:\n            result.update(result_dict)\n\n    if print_result:\n        self.print_result(result)\n\n    return result\n\n", "CodeBleu_score": 43.604193063699384}, {"code": "def postprocess(\n        self,\n        preds: PredType,\n        visualization: Optional[List[np.ndarray]] = None,\n        return_datasamples: bool = False,\n        print_result: bool = False,\n        no_save_pred: bool = False,\n        pred_out_dir: str = '',\n        **kwargs,\n    ) -> Dict\n\t\n    if not no_save_pred and pred_out_dir == '':\n        pred_out_dir = osp.join(self.work_dir, 'preds')\n        mmengine.mkdir_or_exist(pred_out_dir)\n\n    if return_datasamples:\n        warnings.warn(\n            'Save datasamples is not supported in this method. '\n            'Please use `save_preds` instead.')\n\n    result = dict(predictions=[])\n\n    if visualization is not None:\n        result['visualization'] = visualization\n\n    for data_sample in preds:\n        if not no_save_pred:\n            pred_dict = self.pred2dict(data_sample, pred_out_dir)\n        else:\n            pred_dict = self.pred2dict(data_sample)\n\n        result['predictions'].append(pred_dict)\n\n    if print_result:\n        self.print_result(result)\n\n    return result\n\n", "CodeBleu_score": 46.90385945041371}]}
{"instance_id": "a3c2561f199c", "generation_result": [{"code": "def build_test_pipeline(cfg: ConfigType) -> ConfigType\n\t\n    test_pipeline = copy.deepcopy(cfg.data.test.pipeline)\n    test_pipeline[0] = dict(type='Resize', **test_pipeline[0]['resize_cfg'])\n    test_pipeline.pop()\n    test_pipeline = [test_pipeline[0], *cfg.data.test.pipeline[-1:]]\n    test_pipeline = Compose(test_pipeline)\n    return test_pipeline\n\n\n\n\n", "CodeBleu_score": 21.879426325727515}, {"code": "def build_test_pipeline(cfg: ConfigType) -> ConfigType\n\t\n    cfg = copy.deepcopy(cfg)\n    transforms = cfg.dataset.pipeline\n    transforms[0] = transforms[0].copy()\n    transforms[0].transforms = [transforms[0].transforms[0]]\n    transforms[-1] = transforms[-1].copy()\n    transforms = [transforms[0], transforms[-1]]\n    test_pipeline = transforms.copy()\n    test_pipeline = Compose(test_pipeline)\n    return test_pipeline\n\n\n", "CodeBleu_score": 24.291059946116462}, {"code": "def build_test_pipeline(cfg: ConfigType) -> ConfigType\n\t\n    test_pipeline = copy.deepcopy(cfg.dataset.pipeline)\n    test_pipeline[0] = dict(type='Resize', **test_pipeline[0]['resize_cfg'])\n    test_pipeline[-1] = dict(type='Collect', **test_pipeline[-1]['collect_cfg'])\n    test_pipeline = [test_pipeline[0], test_pipeline[-1]]\n    cfg.test_pipeline = test_pipeline\n    return cfg\n\n\n", "CodeBleu_score": 20.99004606424997}]}
{"instance_id": "48a358ab5a38", "generation_result": [{"code": "def inference_mot(model: nn.Module, img: np.ndarray, frame_id: int,\n                  video_len: int) -> SampleList\n\t\n\n    # prepare input data\n    data = dict(\n        img=img,\n        frame_id=frame_id,\n        ori_shape=img.shape,\n        img_id=0,\n        video_len=video_len)\n\n    # prepare test pipeline\n    test_pipeline = model.cfg.test_pipeline\n    test_pipeline = build_test_pipeline(test_pipeline)\n    data = test_pipeline(data)\n    data = data.to('cpu')\n\n    # ensure no unsupported modules like RoIPool are used\n    if not model.cfg.get('_allow_unknown_models', True):\n        with pytest.raises(TypeError):\n            model(**data)\n\n    # forward without gradient computation\n    with torch.no_grad():\n        result = model(**data)\n\n    return result", "CodeBleu_score": 30.875681266490453}, {"code": "def inference_mot(model: nn.Module, img: np.ndarray, frame_id: int,\n                  video_len: int) -> SampleList\n\t\n\n    # prepare input data\n    data = {\n        'img': img,\n        'frame_id': frame_id,\n        'ori_shape': img.shape,\n        'img_id': 0,\n        'video_len': video_len\n    }\n\n    # process data through test pipeline\n    data = model.test_pipeline(data)\n\n    # ensure no unsupported modules like RoIPool are used\n    if model.device == 'cpu':\n        model.roi_head.roi_head.mask_roi_extractor.module.__class__ = \\\n            MaskRoIExtractor\n        model.roi_head.roi_head.mask_head.module.__class__ = MaskHead\n\n    # perform inference\n    with torch.no_grad():\n        result = model(return_loss=False, rescale=True, **data)\n\n    return result\n\n", "CodeBleu_score": 29.540264244506638}, {"code": "def inference_mot(model: nn.Module, img: np.ndarray, frame_id: int,\n                  video_len: int) -> SampleList\n\t\n    img_metas = {\n        'img': img,\n        'frame_id': frame_id,\n        'original_shape': img.shape,\n        'img_id': frame_id,\n        'video_len': video_len\n    }\n    test_pipeline = build_test_pipeline(model.cfg)\n    data = dict(img_metas=img_metas)\n    data = test_pipeline(data)\n    data = data.data\n    with torch.no_grad():\n        if model.device == 'cpu':\n            model = model.cpu()\n            model.forward = model.forward_dummy\n        result = model(return_loss=False, rescale=True, **data)\n    return result\n\n", "CodeBleu_score": 34.02272202792119}]}
{"instance_id": "6c4ea2ce1787", "generation_result": [{"code": "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\n\t\n    while True:\n        total_length = len(tokens_a) + len(tokens_b)\n        if total_length <= max_num_tokens:\n            break\n        trunc_tokens = tokens_a if len(tokens_a) > len(tokens_b) else tokens_b\n        assert len(trunc_tokens) >= 1\n        # We want to sometimes truncate from the front and sometimes from the\n        # back to add more randomness and avoid biases.\n        if rng.random() < 0.5:\n            del trunc_tokens[0]\n        else:\n            trunc_tokens.pop()\n\n", "CodeBleu_score": 79.96334663787925}, {"code": "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\n\t\n    while True:\n        total_length = len(tokens_a) + len(tokens_b)\n        if total_length <= max_num_tokens:\n            break\n        trunc_tokens = tokens_a if len(tokens_a) > len(tokens_b) else tokens_b\n        assert len(trunc_tokens) >= 1\n        # We want to sometimes truncate from the front and sometimes from the\n        # back to add more randomness and avoid biases.\n        if rng.random() < 0.5:\n            del trunc_tokens[0]\n        else:\n            trunc_tokens.pop()\n\n", "CodeBleu_score": 79.96334663787925}, {"code": "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\n\t\n    while True:\n        total_length = len(tokens_a) + len(tokens_b)\n        if total_length <= max_num_tokens:\n            break\n        trunc_tokens = tokens_a if len(tokens_a) > len(tokens_b) else tokens_b\n        assert len(trunc_tokens) >= 1\n        # We want to sometimes truncate from the front and sometimes from the\n        # back to add more randomness and avoid biases.\n        if rng.random() < 0.5:\n            del trunc_tokens[0]\n        else:\n            trunc_tokens.pop()\n\n", "CodeBleu_score": 79.96334663787925}]}
{"instance_id": "007181b6729d", "generation_result": [{"code": "def get_activation(activation_string)\n\t\n    if not isinstance(activation_string, str):\n        return activation_string\n    if activation_string == '':\n        return None\n    if activation_string == 'relu':\n        return torch.nn.ReLU()\n    if activation_string == 'sigmoid':\n        return torch.nn.Sigmoid()\n    if activation_string == 'tanh':\n        return torch.nn.Tanh()\n    if activation_string == 'softmax':\n        return torch.nn.Softmax(dim=1)\n    if activation_string == 'softplus':\n        return torch.nn.Softplus()\n    if activation_string == 'leaky_relu':\n        return torch.nn.LeakyReLU()\n    if activation_string == 'prelu':\n        return torch.nn.PReLU()\n    if activation_string == 'selu':\n        return torch.nn.SELU()\n    if activation_string == 'elu':\n        return torch.nn.ELU()\n    if activation_string == 'gelu':\n        return torch.nn.GELU()\n    if activation_string == 'glu':\n        return torch.nn.GLU()\n    if activation_string == 'hardshrink':\n        return torch.nn.Hardshrink()\n    if activation_string == 'hardtanh':\n        return torch.nn.Hardtanh()\n    if activation_string == 'hardsigmoid':\n        return torch.nn.Hardsigmoid()\n    if activation_string == 'hardswish':\n        return torch.nn.Hardswish()\n    if activation_string == 'logsigmoid':\n        return torch.nn.LogSigmoid()\n    if activation_string == 'softshrink':\n        return torch.nn.Softshrink()\n    if activation_string == 'softsign':\n        return torch.nn.Softsign()\n    if activation_string == 'tanhshrink':\n        return torch.nn.Tanhshrink()\n    if activation_string == 'threshold':\n        return tor", "CodeBleu_score": 24.682944527604246}, {"code": "def get_activation(activation_string)\n\t\n    if not isinstance(activation_string, str):\n        return activation_string\n    if not activation_string:\n        return None\n    if activation_string == 'relu':\n        return tf.nn.relu\n    if activation_string == 'tanh':\n        return tf.nn.tanh\n    if activation_string == 'sigmoid':\n        return tf.nn.sigmoid\n    if activation_string == 'softmax':\n        return tf.nn.softmax\n    if activation_string == 'linear':\n        return linear\n    raise ValueError('Unsupported activation: ' + activation_string)\n\n\n", "CodeBleu_score": 45.86164979902197}, {"code": "def get_activation(activation_string)\n\t\n    if isinstance(activation_string, str):\n        if activation_string == \"\":\n            return None\n        elif activation_string == \"relu\":\n            return relu\n        elif activation_string == \"sigmoid\":\n            return sigmoid\n        elif activation_string == \"softmax\":\n            return softmax\n        else:\n            raise ValueError(f\"Activation function {activation_string} not supported\")\n    else:\n        return activation_string\n\n", "CodeBleu_score": 26.270521831138073}]}
{"instance_id": "d5d6e0315899", "generation_result": [{"code": "def dropout(input_tensor, dropout_prob)\n\t\n    if dropout_prob is None or dropout_prob == 0.0:\n        return input_tensor\n    output = tf.nn.dropout(input_tensor, 1.0 - dropout_prob)\n    return output", "CodeBleu_score": 87.51451468041077}, {"code": "def dropout(input_tensor, dropout_prob)\n\t\n    if dropout_prob is None or dropout_prob == 0.0:\n        return input_tensor\n    output = tf.nn.dropout(input_tensor, 1.0 - dropout_prob)\n    return output\n\n", "CodeBleu_score": 87.51451468041077}, {"code": "def dropout(input_tensor, dropout_prob)\n\t\n    if dropout_prob is None or dropout_prob == 0.0:\n        return input_tensor\n    output = tf.nn.dropout(input_tensor, 1.0 - dropout_prob)\n    return output", "CodeBleu_score": 87.51451468041077}]}
{"instance_id": "828917b9ac53", "generation_result": [{"code": "def create_attention_mask_from_input_mask(from_tensor, to_mask)\n\t\n    from_shape = get_shape_list(from_tensor)\n    to_shape = get_shape_list(to_mask)\n\n    batch_size = from_shape[0]\n    from_seq_length = from_shape[1]\n\n    to_seq_length = to_shape[1]\n\n    to_mask = tf.cast(\n        tf.reshape(to_mask, [batch_size, 1, to_seq_length]), tf.float32)\n\n    broadcast_ones = tf.ones(shape=[batch_size, from_seq_length, to_seq_length])\n\n    broadcast_mask = broadcast_ones * to_mask\n\n    return broadcast_mask\n\n", "CodeBleu_score": 44.22793440620368}, {"code": "def create_attention_mask_from_input_mask(from_tensor, to_mask)\n\t\n    from_shape = get_shape_list(from_tensor)\n    to_shape = get_shape_list(to_mask)\n\n    batch_size = from_shape[0]\n    from_seq_length = from_shape[1]\n    to_seq_length = to_shape[1]\n\n    to_mask = tf.cast(\n        tf.reshape(to_mask, [batch_size, 1, to_seq_length]), tf.float32)\n\n    return to_mask\n\n", "CodeBleu_score": 30.48134642560808}, {"code": "def create_attention_mask_from_input_mask(from_tensor, to_mask)\n\t\n    from_shape = get_shape_list(from_tensor, expected_rank=3)\n    to_shape = get_shape_list(to_mask, expected_rank=2)\n\n    batch_size = from_shape[0]\n    from_seq_length = from_shape[1]\n\n    to_seq_length = to_shape[1]\n\n    to_mask = tf.cast(\n        tf.reshape(to_mask, (batch_size, 1, to_seq_length)), tf.float32)\n\n    # We don't assume that `from_tensor` is a mask (although it could be).\n    # `from_mask_cond` = If `from_tensor` is a mask (dtype of `from_tensor` is\n    # tf.float32), then set True, else set False.\n    from_mask_cond = tf.cast(\n        tf.equal(tf.dtypes.cast(from_tensor, tf.float32), 0), tf.float32)\n    # `attention_mask` = [batch_size, from_seq_length, to_seq_length]\n    attention_mask = to_mask * from_mask_cond\n\n    return attention_mask\n\n", "CodeBleu_score": 39.252267726936715}]}
{"instance_id": "d7798e41a723", "generation_result": [{"code": "def transpose_for_scores(input_tensor, batch_size, num_attention_heads,\n                           seq_length, width)\n\t\n    input_tensor = tf.reshape(input_tensor, [batch_size, seq_length, num_attention_heads, width])\n    input_tensor = tf.transpose(input_tensor, [0, 2, 1, 3])\n    return input_tensor\n\n\n", "CodeBleu_score": 42.61845112285781}, {"code": "def transpose_for_scores(input_tensor, batch_size, num_attention_heads,\n                           seq_length, width)\n\t\n    input_tensor = tf.reshape(input_tensor, [batch_size, seq_length, num_attention_heads, width])\n    input_tensor = tf.transpose(input_tensor, [0, 2, 1, 3])\n    return input_tensor\n\n\n\n", "CodeBleu_score": 42.61845112285781}, {"code": "def transpose_for_scores(input_tensor, batch_size, num_attention_heads,\n                           seq_length, width)\n\t\n    input_tensor = tf.reshape(input_tensor, [batch_size, seq_length, num_attention_heads, width])\n    input_tensor = tf.transpose(input_tensor, [0, 2, 1, 3])\n    return input_tensor\n\n\n", "CodeBleu_score": 42.61845112285781}]}
{"instance_id": "711d466ace09", "generation_result": [{"code": "def reshape_to_matrix(input_tensor)\n\t\n    tensor_rank = input_tensor.shape.rank\n    if tensor_rank < 2:\n        raise ValueError(f\"Input tensor must have rank at least 2, but got rank {tensor_rank}.\")\n    if tensor_rank == 2:\n        return input_tensor\n    width = int(input_tensor.shape[-1])\n    return tf.reshape(input_tensor, [-1, width])\n\n\n\n", "CodeBleu_score": 45.59839924968377}, {"code": "def reshape_to_matrix(input_tensor)\n\t\n    input_shape = input_tensor.shape\n    if len(input_shape) < 2:\n        raise ValueError(\"Input tensor must have at least 2 dimensions.\")\n    if len(input_shape) == 2:\n        return input_tensor\n    width = input_shape[-1]\n    return tf.reshape(input_tensor, [-1, width])\n\n\n", "CodeBleu_score": 43.28052460340125}, {"code": "def reshape_to_matrix(input_tensor)\n\t\n    input_tensor_shape = input_tensor.shape\n    if len(input_tensor_shape) < 2:\n        raise ValueError(\"Input tensor must have rank at least 2.\")\n    if len(input_tensor_shape) == 2:\n        return input_tensor\n    width = input_tensor_shape[-1]\n    return tf.reshape(input_tensor, [-1, width])\n\n\n", "CodeBleu_score": 44.066580303326155}]}
{"instance_id": "7adcd08f531e", "generation_result": [{"code": "def get_train_examples(self, data_dir)\n\t\n    examples = []\n    with open(os.path.join(data_dir, \"train.tsv\"), \"r\") as f:\n        reader = csv.reader(f, delimiter=\"\\t\", quotechar=None)\n        next(reader)\n        for line in reader:\n            guid = line[0]\n            text_a = convert_to_unicode(line[1])\n            text_b = convert_to_unicode(line[2])\n            label = convert_to_unicode(line[3])\n            if label == \"contradictory\":\n                label = \"contradiction\"\n            examples.append(\n                InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label)\n            )\n    return examples", "CodeBleu_score": 36.72535236128919}, {"code": "def get_train_examples(self, data_dir)\n\t\n    lines = self._read_data(data_dir)\n\n    examples = []\n    for (i, line) in enumerate(lines):\n        if i == 0:\n            continue\n        guid = \"%s-%s\" % (\"train\", i)\n        text_a = convert_to_unicode(line[3])\n        text_b = convert_to_unicode(line[4])\n        label = convert_to_unicode(line[0])\n        if label == \"contradictory\":\n            label = \"contradiction\"\n        examples.append(\n            InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n    return examples\n\n", "CodeBleu_score": 41.44085870817793}, {"code": "def get_train_examples(self, data_dir)\n\t\n    train_examples = []\n    with open(os.path.join(data_dir, \"train.tsv\"), \"r\") as f:\n        reader = csv.reader(f, delimiter=\"\\t\", quotechar=None)\n        next(reader)\n        for line in reader:\n            text_a = tokenization.convert_to_unicode(line[1])\n            text_b = tokenization.convert_to_unicode(line[2])\n            label = tokenization.convert_to_unicode(line[3])\n            if label == \"contradictory\":\n                label = \"contradiction\"\n            train_examples.append(\n                InputExample(\n                    guid=line[0], text_a=text_a, text_b=text_b, label=label\n                )\n            )\n    return train_examples\n\n", "CodeBleu_score": 38.52960637322418}]}
{"instance_id": "26a0bb96dab9", "generation_result": [{"code": "def metric_fn(per_example_loss, label_ids, logits, is_real_example)\n\t\n    predictions = logits.argmax(dim=-1)\n    accuracy = (predictions == label_ids).float().mean()\n    loss = per_example_loss.mean()\n    return {\n        \"accuracy\": accuracy,\n        \"loss\": loss,\n    }\n\n", "CodeBleu_score": 21.281448580699497}, {"code": "def metric_fn(per_example_loss, label_ids, logits, is_real_example)\n\t\n    predictions = logits.argmax(dim=-1)\n    accuracy = (predictions == label_ids).float().mean()\n    loss = per_example_loss.mean()\n    return {\n        \"accuracy\": accuracy,\n        \"loss\": loss,\n    }\n\n", "CodeBleu_score": 21.281448580699497}, {"code": "def metric_fn(per_example_loss, label_ids, logits, is_real_example)\n\t\n    predictions = np.argmax(logits, axis=1)\n    true_labels = np.argmax(label_ids, axis=1)\n    is_real_example = np.array(is_real_example)\n    accuracy = np.mean((predictions == true_labels) * is_real_example)\n    loss = np.mean(per_example_loss * is_real_example)\n    return {\"accuracy\": accuracy, \"loss\": loss}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 28.019530889515664}]}
{"instance_id": "6154e4d28509", "generation_result": [{"code": "def convert_examples_to_features(examples, label_list, max_seq_length,\n                                 tokenizer)\n\t\n\n    label_map = {}\n    for (i, label) in enumerate(label_list):\n        label_map[label] = i\n\n    features = []\n    for (ex_index, example) in enumerate(examples):\n        if ex_index % 10000 == 0:\n            tf.logging.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n        feature = convert_single_example(ex_index, example, label_list, max_seq_length, tokenizer)\n        features.append(feature)\n    return features\n\n", "CodeBleu_score": 78.40144201781517}, {"code": "def convert_examples_to_features(examples, label_list, max_seq_length,\n                                 tokenizer)\n\t\n    features = []\n    for (ex_index, example) in enumerate(examples):\n        if ex_index % 10000 == 0:\n            tf.logging.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n        feature = convert_single_example(ex_index, example, label_list, max_seq_length, tokenizer)\n        features.append(feature)\n    return features\n\n", "CodeBleu_score": 81.97484429335587}, {"code": "def convert_examples_to_features(examples, label_list, max_seq_length,\n                                 tokenizer)\n\t\n    label_map = {}\n    for (i, label) in enumerate(label_list):\n        label_map[label] = i\n\n    features = []\n    for (ex_index, example) in enumerate(examples):\n        if ex_index % 10000 == 0:\n            tf.logging.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n        feature = convert_single_example(ex_index, example, label_list, max_seq_length, tokenizer)\n        features.append(feature)\n    return features\n\n", "CodeBleu_score": 78.40144201781517}]}
{"instance_id": "e09c0db71ebb", "generation_result": [{"code": "def get_next_sentence_output(bert_config, input_tensor, labels)\n\t\n    output_weights = tf.get_variable(\n        \"output_weights\", [2, bert_config.hidden_size],\n        initializer=tf.truncated_normal_initializer(stddev=0.02))\n    output_bias = tf.get_variable(\n        \"output_bias\", [2], initializer=tf.zeros_initializer())\n\n    with tf.variable_scope(\"loss\"):\n        logits = tf.matmul(input_tensor, output_weights, transpose_b=True)\n        logits = tf.nn.bias_add(logits, output_bias)\n        log_probs = tf.nn.log_softmax(logits, axis=-1)\n        one_hot_labels = tf.one_hot(labels, depth=2, dtype=tf.float32)\n        per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n        loss = tf.reduce_mean(per_example_loss)\n    return (loss, per_example_loss, log_probs)\n\n", "CodeBleu_score": 63.41930033141031}, {"code": "def get_next_sentence_output(bert_config, input_tensor, labels)\n\t\n    # The embedding for [SEP] is the same as the embedding for [CLS].\n    sep_embedding = tf.get_variable(\n        name=\"cls_embeddings\",\n        shape=[1, bert_config.hidden_size],\n        initializer=tf.zeros_initializer())\n\n    # We avoid using `tf.gather` on the `token_type_embeddings` matrix of shape [2, bert_config.hidden_size],\n    # so that the model does not need to pad masks.\n    # However, `tf.gather` on the `outputs` tensor of shape [batch_size, seq_length, bert_config.hidden_size]\n    # requires the second dimension of `indices` to be static.\n    # As a result, we add a statically defined padding to the `token_type_embeddings` on device during training.\n    cls_embeddings = tf.concat([sep_embedding, sep_embedding], axis=0)\n    cls_embeddings = tf.concat([cls_embeddings, sep_embedding], axis=0)\n    cls_embeddings = tf.concat([cls_embeddings, sep_embedding], axis=0)\n    cls_embeddings = tf.concat([cls_embeddings, sep_embedding], axis=0)\n    cls_embeddings = tf.concat([cls_embeddings, sep_embedding], axis=0)\n    cls_embeddings = tf.concat([cls_embeddings, sep_embedding], axis=0)\n    cls_embeddings = tf.concat([cls_embeddings, sep_embedding], axis=0)\n    cls_embeddings = tf.concat([cls_embeddings, sep_embedding], axis=0)\n    cls_embeddings = tf.concat([cls_embeddings, sep_embedding], axis=0)\n    cls_embeddings = tf.concat([cls_embeddings, sep_embedding], axis=0)\n    cls_embeddings = tf.concat([cls_embeddings, sep_embedding], axis=0)\n    cls_embed", "CodeBleu_score": 18.231107587158103}, {"code": "def get_next_sentence_output(bert_config, input_tensor, labels)\n\t\n    output_weights = tf.get_variable(\n        \"output_weights\", [bert_config.hidden_size, 2],\n        initializer=modeling.create_initializer(bert_config.initializer_range))\n    output_bias = tf.get_variable(\n        \"output_bias\", [2], initializer=tf.zeros_initializer())\n\n    seq_relationship_score = tf.matmul(input_tensor, output_weights, transpose_b=True)\n    seq_relationship_score = tf.nn.bias_add(seq_relationship_score, output_bias)\n    seq_relationship_score_flattened = tf.reshape(seq_relationship_score, [-1, 2])\n    logits = seq_relationship_score_flattened\n    labels = tf.reshape(labels, [-1])\n    one_hot_labels = tf.one_hot(labels, depth=2, dtype=tf.float32)\n    per_example_loss = -tf.reduce_sum(one_hot_labels * tf.nn.log_softmax(logits), axis=-1)\n    loss = tf.reduce_mean(per_example_loss)\n    return (loss, per_example_loss, logits)", "CodeBleu_score": 43.509950896482046}]}
{"instance_id": "1f66b020f485", "generation_result": [{"code": "def gather_indexes(sequence_tensor, positions)\n\t\n    sequence_shape = sequence_tensor.get_shape()\n    flat_offsets = tf.range(tf.shape(sequence_tensor)[0]) * tf.shape(sequence_tensor)[1]\n    flat_positions = tf.reshape(flat_offsets + positions, [-1])\n    output_tensor = tf.gather(tf.reshape(sequence_tensor, [-1]), flat_positions)\n    return tf.reshape(output_tensor, [tf.shape(positions)[0], tf.shape(positions)[1], sequence_shape[2]])\n\n", "CodeBleu_score": 30.075642223816267}, {"code": "def gather_indexes(sequence_tensor, positions)\n\t\n    sequence_shape = sequence_tensor.get_shape()\n    flat_offsets = tf.range(tf.shape(sequence_tensor)[0]) * tf.shape(sequence_tensor)[1]\n    batch_positions = tf.reshape(flat_offsets + positions, [-1])\n    output_tensor = tf.gather(tf.reshape(sequence_tensor, [-1]), batch_positions)\n    return output_tensor\n\n", "CodeBleu_score": 26.111982148031167}, {"code": "def gather_indexes(sequence_tensor, positions)\n\t\n    #sequence_tensor: [batch_size, seq_len, embedding_size]\n    #positions: [batch_size, seq_len]\n    #output: [batch_size, seq_len, embedding_size]\n    flattened_tensor = tf.reshape(sequence_tensor, [-1, sequence_tensor.shape[-1]])\n    #flattened_tensor: [batch_size*seq_len, embedding_size]\n    flattened_positions = tf.reshape(positions, [-1])\n    #flattened_positions: [batch_size*seq_len]\n    output_tensor = tf.gather(flattened_tensor, flattened_positions)\n    #output_tensor: [batch_size*seq_len, embedding_size]\n    output_tensor = tf.reshape(output_tensor, [sequence_tensor.shape[0], sequence_tensor.shape[1], sequence_tensor.shape[2]])\n    #output_tensor: [batch_size, seq_len, embedding_size]\n    return output_tensor\n\n", "CodeBleu_score": 19.077183012097453}]}
{"instance_id": "956bffee9a33", "generation_result": [{"code": "def _decode_record(record, name_to_features)\n\t\n    example = tf.io.parse_single_example(record, name_to_features)\n    for name in list(example.keys()):\n        t = example[name]\n        if t.dtype == tf.int64:\n            t = tf.cast(t, tf.int32)\n        example[name] = t\n    return example\n\n", "CodeBleu_score": 59.0051892348668}, {"code": "def _decode_record(record, name_to_features)\n\t\n    example = tf.io.parse_single_example(record, name_to_features)\n    for name in list(example.keys()):\n        t = example[name]\n        if t.dtype == tf.int64:\n            t = tf.cast(t, tf.int32)\n        example[name] = t\n    return example\n\n", "CodeBleu_score": 59.0051892348668}, {"code": "def _decode_record(record, name_to_features)\n\t\n    example = tf.io.parse_single_example(record, name_to_features)\n    for name in list(example.keys()):\n        t = example[name]\n        if t.dtype == tf.int64:\n            t = tf.cast(t, tf.int32)\n        example[name] = t\n    return example\n\n\n", "CodeBleu_score": 59.0051892348668}]}
{"instance_id": "6b97f354e641", "generation_result": [{"code": "def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer,\n                         orig_answer_text)\n\t\n    start_position = input_start\n    end_position = input_end\n    orig_doc_len = len(doc_tokens)\n    while start_position < end_position:\n        start_chars = doc_tokens[start_position]\n        end_chars = doc_tokens[end_position - 1]\n        if orig_answer_text[0] == start_chars and orig_answer_text[-1] == end_chars:\n            return start_position, end_position\n        if start_chars in PUNCTUATION:\n            start_position += 1\n        if end_chars in PUNCTUATION:\n            end_position -= 1\n    return (start_position, end_position)\n\n", "CodeBleu_score": 18.776993755221294}, {"code": "def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer,\n                         orig_answer_text)\n\t\n    # Tokenize the original answer text.\n    orig_answer_tokens = tokenizer.tokenize(orig_answer_text)\n\n    # Find the start position of the first token of the original answer text.\n    for (i, token) in enumerate(doc_tokens):\n        if token == orig_answer_tokens[0]:\n            start_position = i\n            break\n\n    # Find the end position of the last token of the original answer text.\n    for (i, token) in enumerate(doc_tokens):\n        if token == orig_answer_tokens[-1]:\n            end_position = i\n            break\n\n    # Check if the answer is out of the span.\n    if start_position >= len(doc_tokens) or end_position >= len(doc_tokens):\n        return (input_start, input_end)\n\n    # If the answer is in the span, return the start and end position indices.\n    if start_position <= input_start and end_position >= input_end:\n        return (input_start, input_end)\n\n    # Otherwise, find the best answer span by considering the span before and after the answer span.\n    best_start = -1\n    best_end = -1\n    for (start_position, end_position) in [(input_start, input_end), (input_start, input_end - 1), (input_start + 1, input_end), (input_start + 1, input_end - 1)]:\n        if start_position >= len(doc_tokens) or end_position >= len(doc_tokens) or start_position < 0 or end_position < 0:\n            continue\n        with_start_end = doc_tokens[start_position: end_position + 1]\n        if len(with_start_end) < 1 or len(with_start_end) > len(orig_answer_tokens):\n            continue\n        if with_start_end == orig_answer_tokens:\n            return (start_position, end_position)\n        if start_position >= best_start and end_position >= best_end:\n            best_start = start_position\n            best_", "CodeBleu_score": 27.262560025033487}, {"code": "def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer,\n                         orig_answer_text)\n\t\n    # tokenizer.tokenize: def tokenize(self, text):\n    #     split_tokens = []\n    #     for token in self.basic_tokenizer.tokenize(text):\n    #       for sub_token in self.wordpiece_tokenizer.tokenize(token):\n    #         split_tokens.append(sub_token)\n\n    #     return split_tokens\n    # doc_tokens = tokenizer.tokenize(orig_answer_text)\n\n    # The SQuAD annotations are character based. We first project them to\n    # whitespace-tokenized words. But then after WordPiece tokenization, we may\n    # end up with *either* in the *middle* of a word or to the *right* of a\n    # word. If this is the case, this function tries to split the word into two\n    # pieces: one matching to the front of the original word and one matching\n    # to the back.\n\n    # For example:\n    #\n    #   text = [I, 've', gone, shopping .]\n    #   annotated_answer = [I've, gone shopping]\n    #\n    # The annotated answer will be matched earlier.\n    #\n    #   text = [I, 've', gone, shopping .]\n    #   annotated_answer = [I've, gone, shopping]\n    #\n    # The annotated answer will be matched later.\n    #\n    # If the original answer is not in clean tokenizable words, this function\n    # will return None.\n    if len(orig_answer_text) == 0:\n        return (input_start, input_end)\n    start_position = _find_start_position(doc_tokens, orig_answer_text)\n    end_position = _find_end_position(doc_tokens, orig_answer_text)\n    if start_position is None or end_position is None:\n        return (input_start, input_end)\n    # We could go further by considering cases such as \"EU \".\n    if not _is_whitespace(doc_tokens[start_position - 1]):\n        start_position = start_position -", "CodeBleu_score": 26.292546020801637}]}
{"instance_id": "aba334b92b34", "generation_result": [{"code": "def _check_is_max_context(doc_spans, cur_span_index, position)\n\t\n    best_score = None\n    for span_index in range(cur_span_index + 1, len(doc_spans)):\n        start_tokens_to_end_tokens = doc_spans[span_index].start_position - position\n        end_tokens_to_end_tokens = doc_spans[span_index].end_position - position\n        if start_tokens_to_end_tokens < 0 or end_tokens_to_end_tokens < 0:\n            continue\n        num_left_context = min(start_tokens_to_end_tokens, doc_spans[span_index].start_position - doc_spans[span_index - 1].end_position)\n        num_right_context = min(end_tokens_to_end_tokens, doc_spans[span_index + 1].start_position - doc_spans[span_index].end_position)\n        score = (num_left_context + num_right_context) / doc_spans[span_index].length\n        if best_score is None or score > best_score:\n            best_score = score\n            best_span_index = span_index\n    return cur_span_index == best_span_index\n\n\n", "CodeBleu_score": 39.69671523169044}, {"code": "def _check_is_max_context(doc_spans, cur_span_index, position)\n\t\n    max_score = cur_span_index\n    candidate_index = -1\n    for (i, span) in enumerate(doc_spans):\n        end = span.start + span.length - 1\n        if position < span.start:\n            continue\n        if position > end:\n            continue\n        num_left_context = position - span.start\n        num_right_context = end - position\n        score = (num_left_context + num_right_context) / span.length\n        if score > max_score:\n            max_score = score\n            candidate_index = i\n    return candidate_index == cur_span_index\n\n", "CodeBleu_score": 57.87336585365562}, {"code": "def _check_is_max_context(doc_spans, cur_span_index, position)\n\t\n    best_score = -1000000\n    best_span_index = -1\n    for (span_index, span) in enumerate(doc_spans):\n        end = span.start + span.length - 1\n        if position < span.start:\n            continue\n        if position > end:\n            continue\n        num_left_context = position - span.start\n        num_right_context = end - position\n        score = (num_left_context + num_right_context) / span.length\n        if score > best_score:\n            best_score = score\n            best_span_index = span_index\n    return cur_span_index == best_span_index\n\n", "CodeBleu_score": 63.7079123823843}]}
{"instance_id": "cf92ad509f6a", "generation_result": [{"code": "def load_vocab(vocab_file)\n\t\n    vocab = collections.OrderedDict()\n    index = 0\n    with tf.gfile.GFile(vocab_file, \"r\") as reader:\n        while True:\n            token = convert_to_unicode(reader.readline())\n            if not token:\n                break\n            token = token.strip()\n            vocab[token] = index\n            index += 1\n    return vocab\n\n", "CodeBleu_score": 72.75116632898921}, {"code": "def load_vocab(vocab_file)\n\t\n    vocab = collections.OrderedDict()\n    index = 0\n    with tf.gfile.GFile(vocab_file, \"r\") as reader:\n        while True:\n            token = convert_to_unicode(reader.readline())\n            if not token:\n                break\n            token = token.strip()\n            vocab[token] = index\n            index += 1\n    return vocab\n\n", "CodeBleu_score": 72.75116632898921}, {"code": "def load_vocab(vocab_file)\n\t\n    vocab = collections.OrderedDict()\n    index = 0\n    with tf.gfile.GFile(vocab_file, \"r\") as reader:\n        while True:\n            token = convert_to_unicode(reader.readline())\n            if not token:\n                break\n            token = token.strip()\n            vocab[token] = index\n            index += 1\n    return vocab\n\n", "CodeBleu_score": 72.75116632898921}]}
{"instance_id": "6c865803ac12", "generation_result": [{"code": "def convert_by_vocab(vocab, items)\n\t\n    return [vocab[item] for item in items]\n\n", "CodeBleu_score": 18.3448522644182}, {"code": "def convert_by_vocab(vocab, items)\n\t\n    return [vocab[item] for item in items]\n\n", "CodeBleu_score": 18.3448522644182}, {"code": "def convert_by_vocab(vocab, items)\n\t\n    output = []\n    for item in items:\n        output.append(vocab[item])\n    return output\n\n\n", "CodeBleu_score": 79.63749907165396}]}
{"instance_id": "e93b9200dc4a", "generation_result": [{"code": "def tokenize(self, text)\n\t\n    tokens = self.basic_tokenizer.tokenize(text)\n    result = []\n    for token in tokens:\n        if token in self.vocab:\n            result.append(token)\n        else:\n            result.extend(self.wordpiece_tokenizer.tokenize(token))\n    return result\n\n\n", "CodeBleu_score": 43.79719088698059}, {"code": "def tokenize(self, text)\n\t\n    tokens = self.basic_tokenizer.tokenize(text)\n    for token in tokens:\n        if token not in self.vocab:\n            tokens = self.wordpiece_tokenizer.tokenize(token)\n            for sub_token in tokens:\n                if sub_token not in self.vocab:\n                    self.vocab[sub_token] = len(self.vocab)\n        else:\n            tokens = [token]\n    return tokens\n\n\n", "CodeBleu_score": 28.06821419906894}, {"code": "def tokenize(self, text)\n\t\n    tokens = []\n    for token in self.basic_tokenizer.tokenize(text):\n        for sub_token in self.wordpiece_tokenizer.tokenize(token):\n            tokens.append(sub_token)\n    return tokens\n\n\n", "CodeBleu_score": 80.44897730916172}]}
{"instance_id": "2ff6f6db53c9", "generation_result": [{"code": "def safe_load(path, offset, duration, sample_rate, dtype)\n\t\n    try:\n        audio_data = librosa.load(path, offset=offset, duration=duration, sr=sample_rate, dtype=dtype)[0]\n        return audio_data, False\n    except Exception as e:\n        print(f\"Error loading audio data from {path}: {e}\")\n        return -1.0, True\n\n\n", "CodeBleu_score": 30.885363712593104}, {"code": "def safe_load(path, offset, duration, sample_rate, dtype)\n\t\n    try:\n        audio_data = load(path, offset, duration, sample_rate, dtype)\n        return audio_data, False\n    except Exception as e:\n        print(f\"Error loading audio data from {path}: {e}\")\n        return -1.0, True\n\n\n", "CodeBleu_score": 18.47362905537141}, {"code": "def safe_load(path, offset, duration, sample_rate, dtype)\n\t\n    try:\n        audio_data = load(path, offset, duration, sample_rate, dtype)\n        return audio_data, False\n    except Exception as e:\n        logging.error(f\"Error loading audio data from {path}: {e}\")\n        return -1.0, True\n\n", "CodeBleu_score": 23.08562016024619}]}
{"instance_id": "12fbe1f03d8f", "generation_result": [{"code": "def to_n_channels(waveform: tf.Tensor, n_channels: int) -> tf.Tensor\n\t\n    if waveform.shape[-1] == n_channels:\n        return waveform\n    if waveform.shape[-1] > n_channels:\n        return waveform[..., :n_channels]\n    return tf.tile(waveform[..., None], [1] * (waveform.shape.rank - 1) + [n_channels])[..., :n_channels]\n\n\n", "CodeBleu_score": 44.17589292707203}, {"code": "def to_n_channels(waveform: tf.Tensor, n_channels: int) -> tf.Tensor\n\t\n    if waveform.shape[-1] == n_channels:\n        return waveform\n    elif waveform.shape[-1] > n_channels:\n        return waveform[..., :n_channels]\n    else:\n        return tf.tile(waveform, [1] * (waveform.shape.rank - 1) + [n_channels // waveform.shape[-1] + 1])[..., :n_channels]\n\n\n", "CodeBleu_score": 43.0813182560414}, {"code": "def to_n_channels(waveform: tf.Tensor, n_channels: int) -> tf.Tensor\n\t\n    waveform_shape = waveform.shape\n    waveform_rank = len(waveform_shape)\n    if waveform_rank == 1:\n        waveform = tf.reshape(waveform, [1, waveform_shape[0]])\n    elif waveform_rank == 2:\n        waveform = tf.reshape(waveform, waveform_shape)\n    else:\n        raise ValueError(\"The waveform tensor must have rank 1 or 2.\")\n    waveform_channels = waveform_shape[0]\n    if waveform_channels == n_channels:\n        return waveform\n    elif waveform_channels > n_channels:\n        return waveform[:n_channels, :]\n    else:\n        waveform = tf.tile(waveform, [n_channels // waveform_channels, 1])\n        if n_channels % waveform_channels != 0:\n            waveform = tf.concat([waveform, waveform[:n_channels % waveform_channels, :]], axis=0)\n        return waveform\n\n\n", "CodeBleu_score": 39.91982937549026}]}
{"instance_id": "5e215f029b2e", "generation_result": [{"code": "def spectrogram_to_db_uint(\n    spectrogram: tf.Tensor, db_range: float = 100.0, **kwargs\n) -> tf.Tensor\n\t\n    db_spectrogram = gain_to_db(spectrogram)\n    max_db = tf.reduce_max(db_spectrogram)\n    clipped_db_spectrogram = tf.clip_by_value(db_spectrogram, max_db - db_range, max_db)\n    scaled_db_spectrogram = (\n        (clipped_db_spectrogram - (max_db - db_range))\n        / (db_range)\n        * 255\n    )\n    return tf.cast(scaled_db_spectrogram, tf.uint8)", "CodeBleu_score": 42.06276786350347}, {"code": "def spectrogram_to_db_uint(\n    spectrogram: tf.Tensor, db_range: float = 100.0, **kwargs\n) -> tf.Tensor\n\t\n    spectrogram = gain_to_db(spectrogram)\n    max_db = tf.reduce_max(spectrogram)\n    spectrogram = tf.clip_by_value(spectrogram, max_db - db_range, max_db)\n    spectrogram = tf.cast(\n        tf.math.round((spectrogram - max_db + db_range) / db_range * 255), tf.uint8\n    )\n    return spectrogram\n\n", "CodeBleu_score": 44.42492664760505}, {"code": "def spectrogram_to_db_uint(\n    spectrogram: tf.Tensor, db_range: float = 100.0, **kwargs\n) -> tf.Tensor\n\t\n    db_spectrogram = gain_to_db(spectrogram)\n    max_db = tf.reduce_max(db_spectrogram)\n    clipped_db_spectrogram = tf.clip_by_value(db_spectrogram, max_db - db_range, max_db)\n    scaled_uint8_spectrogram = tf.cast(\n        (clipped_db_spectrogram - (max_db - db_range))\n        / (db_range / 255.0),\n        tf.uint8,\n    )\n    return scaled_uint8_spectrogram", "CodeBleu_score": 41.7130739036248}]}
{"instance_id": "b65c0968dae7", "generation_result": [{"code": "def compute_spectrogram_tf(\n    waveform: tf.Tensor,\n    frame_length: int = 2048,\n    frame_step: int = 512,\n    spec_exponent: float = 1.0,\n    window_exponent: float = 1.0,\n) -> tf.Tensor\n\t\n    window = tf.signal.hann_window(frame_length, dtype=waveform.dtype) ** window_exponent\n    stft = tf.signal.stft(waveform, frame_length, frame_step, window, pad_end=True)\n    spectrogram = tf.abs(stft) ** spec_exponent\n    return tf.transpose(spectrogram, [2, 0, 1])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 35.04742108774419}, {"code": "def compute_spectrogram_tf(\n    waveform: tf.Tensor,\n    frame_length: int = 2048,\n    frame_step: int = 512,\n    spec_exponent: float = 1.0,\n    window_exponent: float = 1.0,\n) -> tf.Tensor\n\t\n    # Compute the spectrogram using STFT\n    spectrogram = tf.abs(\n        tf.signal.stft(\n            waveform,\n            frame_length=frame_length,\n            frame_step=frame_step,\n            window_fn=lambda x: tf.math.pow(tf.signal.hann_window(x, periodic=True), window_exponent),\n        )\n    )\n\n    # Transpose the spectrogram\n    spectrogram = tf.transpose(spectrogram, perm=[1, 0, 2])\n\n    # Raise the spectrogram to a specified exponent\n    spectrogram = tf.math.pow(spectrogram, spec_exponent)\n\n    return spectrogram\n\n\n\n", "CodeBleu_score": 39.873404652623144}, {"code": "def compute_spectrogram_tf(\n    waveform: tf.Tensor,\n    frame_length: int = 2048,\n    frame_step: int = 512,\n    spec_exponent: float = 1.0,\n    window_exponent: float = 1.0,\n) -> tf.Tensor\n\t\n    # Compute the spectrogram using STFT\n    spectrogram = tf.abs(\n        tf.signal.stft(\n            waveform,\n            frame_length=frame_length,\n            frame_step=frame_step,\n            window_fn=lambda frame_length: tf.math.pow(tf.signal.hann_window(frame_length), window_exponent),\n        )\n    )\n\n    # Transpose the spectrogram\n    spectrogram = tf.transpose(spectrogram, perm=[1, 0, 2])\n\n    # Raise the absolute value to a specified exponent\n    spectrogram = tf.math.pow(spectrogram, spec_exponent)\n\n    return spectrogram\n\n\n\n", "CodeBleu_score": 39.27075746656785}]}
{"instance_id": "a4686140a634", "generation_result": [{"code": "def time_stretch(\n    spectrogram: tf.Tensor,\n    factor: float = 1.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n    # Get the original shape of the spectrogram\n    original_shape = tf.shape(spectrogram)\n\n    # Calculate the new time dimension\n    new_time = tf.cast(original_shape[0], tf.float32) * factor\n\n    # Resize the spectrogram using the specified interpolation method\n    resized_spectrogram = tf.image.resize(\n        spectrogram,\n        size=(new_time, original_shape[1]),\n        method=method,\n    )\n\n    # Adjust the resized spectrogram to the original dimensions\n    if new_time > original_shape[0]:\n        # Crop the spectrogram to the original dimensions\n        crop_size = (new_time - original_shape[0], 0)\n        resized_spectrogram = tf.image.crop_to_bounding_box(\n            resized_spectrogram,\n            offset_height=0,\n            offset_width=0,\n            target_height=original_shape[0],\n            target_width=original_shape[1],\n        )\n    else:\n        # Pad the spectrogram to the original dimensions\n        pad_size = (original_shape[0] - new_time, 0)\n        resized_spectrogram = tf.image.pad_to_bounding_box(\n            resized_spectrogram,\n            offset_height=0,\n            offset_width=0,\n            target_height=original_shape[0],\n            target_width=original_shape[1],\n        )\n\n    return resized_spectrogram\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 35.85345782156022}, {"code": "def time_stretch(\n    spectrogram: tf.Tensor,\n    factor: float = 1.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n    if factor < 1.0:\n        raise ValueError(f\"Factor must be greater than or equal to 1.0, got {factor}\")\n    if factor == 1.0:\n        return spectrogram\n    spectrogram_shape = tf.shape(spectrogram)\n    time_dim = spectrogram_shape[-2]\n    new_time_dim = tf.cast(time_dim * factor, tf.int32)\n    new_time_dim = tf.maximum(new_time_dim, 1)\n    new_time_dim = tf.minimum(new_time_dim, spectrogram_shape[-2])\n    new_time_dim = tf.cast(new_time_dim, tf.float32)\n    new_time_dim = tf.cast(new_time_dim, tf.int32)\n    new_time_dim = tf.cast(new_time_dim, tf.float32)\n    new_time_dim = tf.cast(new_time_dim, tf.int32)\n    resized_spectrogram = tf.image.resize(\n        spectrogram,\n        [spectrogram_shape[0], new_time_dim],\n        method=method,\n        preserve_aspect_ratio=False,\n    )\n    if new_time_dim > time_dim:\n        pad_widths = [[0, 0], [0, 0], [0, 0], [0, new_time_dim - time_dim]]\n        resized_spectrogram = tf.pad(resized_spectrogram, pad_widths, mode=\"CONSTANT\")\n    else:\n        start_index = tf.cast((new_time_dim - time_dim) / 2, tf.int32)\n        end_index = start_index + time_dim\n        resized_spectrogram = resized_spectrogram[:, :, :, start_index:end_index]\n    return resized_spectrogram\n\n\n\n\n", "CodeBleu_score": 37.903517739142465}, {"code": "def time_stretch(\n    spectrogram: tf.Tensor,\n    factor: float = 1.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n    time_dim = spectrogram.shape[0]\n    new_time_dim = int(time_dim * factor)\n    spectrogram = tf.image.resize(\n        spectrogram,\n        [new_time_dim, spectrogram.shape[1]],\n        method=method,\n        antialias=True,\n    )\n    if new_time_dim > time_dim:\n        spectrogram = tf.image.crop_to_bounding_box(\n            spectrogram, 0, 0, time_dim, spectrogram.shape[1]\n        )\n    else:\n        spectrogram = tf.image.pad_to_bounding_box(\n            spectrogram, 0, 0, time_dim, spectrogram.shape[1]\n        )\n    return spectrogram\n\n\n", "CodeBleu_score": 32.70592837924223}]}
{"instance_id": "6dd01101d4ae", "generation_result": [{"code": "def pitch_shift(\n    spectrogram: tf.Tensor,\n    semitone_shift: float = 0.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n    # Calculate the resizing factor based on the semitone shift\n    resizing_factor = tf.constant(2.0 ** (semitone_shift / 12.0))\n\n    # Resize the spectrogram along the frequency axis using the specified method\n    resized_spectrogram = tf.image.resize(\n        spectrogram,\n        [spectrogram.shape[0], tf.cast(spectrogram.shape[1] * resizing_factor, tf.int32)],\n        method=method,\n    )\n\n    # Pad the resized spectrogram to maintain the original dimensions\n    padded_spectrogram = tf.image.pad_to_bounding_box(\n        resized_spectrogram, 0, 0, spectrogram.shape[0], spectrogram.shape[1]\n    )\n\n    return padded_spectrogram\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 29.084623958632577}, {"code": "def pitch_shift(\n    spectrogram: tf.Tensor,\n    semitone_shift: float = 0.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n    # Calculate the resizing factor based on the semitone shift\n    resizing_factor = 2 ** (semitone_shift / 12)\n\n    # Resize the spectrogram along the frequency axis using the specified method\n    resized_spectrogram = tf.image.resize(\n        spectrogram,\n        (spectrogram.shape[0], int(spectrogram.shape[1] * resizing_factor)),\n        method=method,\n    )\n\n    # Pad the resized spectrogram to maintain the original dimensions\n    padded_spectrogram = tf.pad(\n        resized_spectrogram,\n        [[0, 0], [0, spectrogram.shape[1] - resized_spectrogram.shape[1]]],\n        mode=\"CONSTANT\",\n    )\n\n    return padded_spectrogram\n\n\n", "CodeBleu_score": 30.499898386493896}, {"code": "def pitch_shift(\n    spectrogram: tf.Tensor,\n    semitone_shift: float = 0.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n    resizing_factor = tf.math.exp(semitone_shift * np.log(2) / 12)\n    resized_spectrogram = tf.image.resize(\n        spectrogram,\n        [spectrogram.shape[0], tf.cast(spectrogram.shape[1] * resizing_factor, tf.int32)],\n        method=method,\n    )\n    pad_width = [[0, 0], [0, spectrogram.shape[1] - resized_spectrogram.shape[1]]]\n    return tf.pad(resized_spectrogram, pad_width, mode=\"CONSTANT\")\n\n\n", "CodeBleu_score": 31.130916229464916}]}
{"instance_id": "612f0826e56a", "generation_result": [{"code": "def _get_conv_activation_layer(params: Dict) -> Any\n\t\n    if params[\"conv_activation\"] == \"relu\":\n        return nn.ReLU()\n    elif params[\"conv_activation\"] == \"elu\":\n        return nn.ELU()\n    elif params[\"conv_activation\"] == \"leakyrelu\":\n        return nn.LeakyReLU(negative_slope=0.2)\n    else:\n        raise ValueError(f\"Invalid value for conv_activation: {params['conv_activation']}\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 29.599626610726155}, {"code": "def _get_conv_activation_layer(params: Dict) -> Any\n\t\n    conv_activation = params.get(\"conv_activation\", \"relu\")\n    if conv_activation == \"relu\":\n        return nn.ReLU()\n    elif conv_activation == \"elu\":\n        return nn.ELU()\n    elif conv_activation == \"leaky_relu\":\n        return nn.LeakyReLU(negative_slope=0.2)\n    else:\n        raise ValueError(f\"Invalid activation layer: {conv_activation}\")\n\n", "CodeBleu_score": 33.73088591737927}, {"code": "def _get_conv_activation_layer(params: Dict) -> Any\n\t\n    conv_activation = params.get(\"conv_activation\", \"relu\")\n    if conv_activation == \"relu\":\n        return nn.ReLU()\n    elif conv_activation == \"elu\":\n        return nn.ELU()\n    elif conv_activation == \"leaky_relu\":\n        return nn.LeakyReLU(negative_slope=0.2)\n    else:\n        raise ValueError(f\"Invalid activation layer: {conv_activation}\")\n\n\n\n", "CodeBleu_score": 33.73088591737927}]}
{"instance_id": "2f8b5a2fc0f2", "generation_result": [{"code": "def softmax_unet(\n    input_tensor: tf.Tensor, instruments: Iterable[str], params: Dict = {}\n) -> Dict\n\t\n    logging.info(\"Apply softmax unet\")\n    outputs = {}\n    for instrument in instruments:\n        unet = apply_unet(input_tensor, output_name=instrument, params=params)\n        outputs[instrument] = unet\n    logit_masks = {instrument: outputs[instrument][..., 1] for instrument in instruments}\n    logit_masks = tf.stack(list(logit_masks.values()), axis=-1)\n    logit_masks = tf.math.softmax(logit_masks, axis=-1)\n    outputs = {\n        instrument: tf.math.multiply(input_tensor, logit_masks[..., i])\n        for i, instrument in enumerate(instruments)\n    }\n    return outputs", "CodeBleu_score": 40.668842511370215}, {"code": "def softmax_unet(\n    input_tensor: tf.Tensor, instruments: Iterable[str], params: Dict = {}\n) -> Dict\n\t\n    # Apply unet for each instrument.\n    output_dict = {}\n    for instrument in instruments:\n        output_dict[instrument] = apply_unet(\n            input_tensor,\n            output_name=instrument,\n            params=params,\n            output_mask_logit=True,\n        )\n    # Softmax.\n    output_dict[\"softmax\"] = Softmax(axis=-1)(\n        tf.stack(list(output_dict.values()), axis=-1)\n    )\n    #\n    # Create output dict.\n    output_dict_spectrogram = {}\n    for instrument in instruments:\n        output_dict_spectrogram[instrument] = Multiply(name=instrument)(\n            [input_tensor, output_dict[\"softmax\"][..., instruments.index(instrument)]]\n        )\n    return output_dict_spectrogram\n\n", "CodeBleu_score": 44.945482703002064}, {"code": "def softmax_unet(\n    input_tensor: tf.Tensor, instruments: Iterable[str], params: Dict = {}\n) -> Dict\n\t\n    outputs = {}\n    for instrument in instruments:\n        logit_mask = apply_unet(input_tensor, instrument, params, output_mask_logit=True)\n        outputs[instrument] = logit_mask\n    #\n    #\n    #\n    softmax_outputs = {}\n    for instrument in instruments:\n        softmax_outputs[instrument] = tf.nn.softmax(outputs[instrument], axis=-1)\n    #\n    #\n    #\n    spectrograms = {}\n    for instrument in instruments:\n        spectrograms[instrument] = Multiply(name=instrument)(\n            [input_tensor, softmax_outputs[instrument]]\n        )\n    return spectrograms\n\n", "CodeBleu_score": 37.104068949354996}]}
{"instance_id": "400b19ce8520", "generation_result": [{"code": "def compute_file_checksum(path)\n\t\n    import hashlib\n    sha256_hash = hashlib.sha256()\n    with open(path, \"rb\") as f:\n        for byte_block in iter(lambda: f.read(4096), b\"\"):\n            sha256_hash.update(byte_block)\n    return sha256_hash.hexdigest()\n\n\n", "CodeBleu_score": 51.074864338551386}, {"code": "def compute_file_checksum(path)\n\t\n    import hashlib\n    sha256 = hashlib.sha256()\n    with open(path, 'rb') as f:\n        while True:\n            data = f.read(1024)\n            if not data:\n                break\n            sha256.update(data)\n    return sha256.hexdigest()\n\n\n", "CodeBleu_score": 36.11594747815128}, {"code": "def compute_file_checksum(path)\n\t\n    hash_obj = hashlib.sha256()\n    with open(path, 'rb') as f:\n        for chunk in iter(lambda: f.read(4096), b''):\n            hash_obj.update(chunk)\n    return hash_obj.hexdigest()\n\n", "CodeBleu_score": 53.26429613270075}]}
{"instance_id": "5f2e9c6e15f1", "generation_result": [{"code": "def checksum(self, name: str) -> str\n\t\n    url = f\"{self.host}/{self.repository}/{self.release}/{name}.json\"\n    response = requests.get(url)\n    if response.status_code == 200:\n        index = response.json()\n        if name in index:\n            return index[name]\n        else:\n            raise ValueError(f\"Model {name} not found in index\")\n    else:\n        raise ValueError(f\"Failed to retrieve index: {response.status_code} {response.reason}\")", "CodeBleu_score": 37.698394985671236}, {"code": "def checksum(self, name: str) -> str\n\t\n    url = f\"{self.host}/{self.repository}/{self.release}/{name}\"\n    response = requests.get(url)\n    if response.status_code == 200:\n        index = response.json()\n        if name in index:\n            return index[name]\n        else:\n            raise ValueError(f\"Model {name} not found in index\")\n    else:\n        raise ValueError(f\"Error retrieving index: {response.status_code}\")", "CodeBleu_score": 37.62977105307085}, {"code": "def checksum(self, name: str) -> str\n\t\n    url = f\"{self.host}/{self.repository}/{self.release_path}/{name}\"\n    response = requests.get(url)\n    if response.status_code != 200:\n        raise ValueError(f\"Failed to retrieve checksum for {name}\")\n    index = response.json()\n    if name not in index:\n        raise ValueError(f\"Model {name} not found in index\")\n    return index[name]\n\n\n\n\n", "CodeBleu_score": 41.85068379554575}]}
{"instance_id": "9312c8b6032e", "generation_result": [{"code": "def download(self, name: str, path: str) -> None:\n\t\n    url: str = \"/\".join(\n        (\n            self._host,\n            self._repository,\n            self.RELEASE_PATH,\n            self._release,\n            self.ARCHIVE_INDEX,\n        )\n    )\n    response: httpx.Response = httpx.get(url)\n    response.raise_for_status()\n    index: Dict = response.json()\n    if name not in index:\n        raise ValueError(f\"No archive for model {name}\")\n    url: str = \"/\".join((self._host, index[name]))\n    logger.info(f\"Downloading {name}\")\n    response: httpx.Response = httpx.get(url)\n    response.raise_for_status()\n    with tempfile.NamedTemporaryFile() as f:\n        for chunk in response.iter_bytes():\n            f.write(chunk)\n        f.flush()\n        checksum: str = self.checksum(name)\n        if checksum != compute_file_checksum(f.name):\n            raise ValueError(f\"Checksum mismatch for {name}\")\n        with tarfile.open(f.name, \"r:gz\") as tar:\n            tar.extractall(path)\n    logger.info(f\"Extracted {name}\")\n\n", "CodeBleu_score": 45.86014221090943}, {"code": "def download(self, name: str, path: str) -> None:\n\t\n    url: str = \"/\".join(\n        (\n            self._host,\n            self._repository,\n            self.RELEASE_PATH,\n            self._release,\n            self.MODEL_PATH,\n            name,\n        )\n    )\n    logger.info(f\"Downloading {name} from {url}\")\n    with httpx.stream(\n        \"GET\",\n        url,\n        headers={\"Accept-Encoding\": \"gzip, deflate\"},\n        follow_redirects=True,\n    ) as response:\n        response.raise_for_status()\n        with open(path, \"wb\") as stream:\n            for chunk in response.iter_raw():\n                stream.write(chunk)\n    logger.info(f\"Downloaded {name} to {path}\")\n    if self.checksum(name) != compute_file_checksum(path):\n        raise ValueError(f\"Checksum mismatch for {name}\")\n    logger.info(f\"Extracting {name} to {path}\")\n    with tarfile.open(path, \"r:gz\") as tar:\n        tar.extractall(path=path)\n    logger.info(f\"Extracted {name} to {path}\")\n    os.remove(path)\n\n", "CodeBleu_score": 48.59221072644394}, {"code": "def download(self, name: str, path: str) -> None:\n\t\n    url: str = \"/\".join(\n        (\n            self._host,\n            self._repository,\n            self.RELEASE_PATH,\n            self._release,\n            self.ARCHIVE_PATH,\n            name,\n        )\n    )\n    self._logger.info(f\"Downloading {name} from {url}\")\n    response: httpx.Response = httpx.get(url, stream=True)\n    response.raise_for_status()\n    with NamedTemporaryFile() as file:\n        for chunk in response.iter_bytes():\n            file.write(chunk)\n        checksum: str = self.checksum(name)\n        file_checksum: str = compute_file_checksum(file.name)\n        if checksum != file_checksum:\n            raise ValueError(f\"Invalid checksum for {name}\")\n        self._logger.info(f\"Extracting {name} to {path}\")\n        with tarfile.open(file.name, \"r:gz\") as archive:\n            archive.extractall(path)\n    self._logger.info(f\"Extracted {name}\")\n\n", "CodeBleu_score": 46.03444074305795}]}
{"instance_id": "ee9493f37d77", "generation_result": [{"code": "def load_configuration(descriptor: str) -> Dict\n\t\n    if descriptor.startswith('embedded:'):\n        name = descriptor[len('embedded:'):]\n        return json.loads(resources.read_text(f'{__package__}.resources', name))\n    elif os.path.isfile(descriptor):\n        with open(descriptor, 'r') as file:\n            return json.load(file)\n    else:\n        raise FileNotFoundError(f'Configuration file not found: {descriptor}')\n\n\n\n\n", "CodeBleu_score": 36.942785128599965}, {"code": "def load_configuration(descriptor: str) -> Dict\n\t\n    if descriptor.startswith(\"embedded:\"):\n        name = descriptor.split(\":\")[1]\n        resource_path = f\"resources/config/{name}.json\"\n        if not os.path.exists(resource_path):\n            raise FileNotFoundError(f\"Embedded configuration '{name}' not found at '{resource_path}'\")\n        with open(resource_path, \"r\") as file:\n            return json.load(file)\n    elif os.path.exists(descriptor):\n        with open(descriptor, \"r\") as file:\n            return json.load(file)\n    else:\n        raise FileNotFoundError(f\"Configuration file '{descriptor}' not found\")\n\n\n", "CodeBleu_score": 51.47064409622121}, {"code": "def load_configuration(descriptor: str) -> Dict\n\t\n    if descriptor.startswith(\"embedded:\"):\n        name = descriptor.replace(\"embedded:\", \"\")\n        resource = importlib.resources.read_text(f\"{__package__}.resources\", name)\n        return json.loads(resource)\n    if os.path.isfile(descriptor):\n        with open(descriptor, \"r\") as file:\n            return json.load(file)\n    raise Exception(f\"Configuration not found: {descriptor}\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 42.69395901670918}]}
{"instance_id": "546134b67fbc", "generation_result": [{"code": "def sync_apply(\n    tensor_dict: Dict[str, tf.Tensor], func: Callable, concat_axis: int = 1\n) -> Dict[str, tf.Tensor]\n\t\n    if concat_axis == 0:\n        concat_tensor = tf.concat(list(tensor_dict.values()), axis=0)\n        result_tensor = func(concat_tensor)\n        result_dict = {\n            key: value\n            for key, value in zip(\n                tensor_dict.keys(), tf.split(result_tensor, len(tensor_dict))\n            )\n        }\n    elif concat_axis == 1:\n        concat_tensor = tf.concat(list(tensor_dict.values()), axis=1)\n        result_tensor = func(concat_tensor)\n        result_dict = {\n            key: value\n            for key, value in zip(\n                tensor_dict.keys(), tf.split(result_tensor, len(tensor_dict), axis=1)\n            )\n        }\n    else:\n        raise ValueError(\"Invalid concatenation axis\")\n    return result_dict\n\n", "CodeBleu_score": 45.594002433298954}, {"code": "def sync_apply(\n    tensor_dict: Dict[str, tf.Tensor], func: Callable, concat_axis: int = 1\n) -> Dict[str, tf.Tensor]\n\t\n    if concat_axis not in [0, 1]:\n        raise ValueError(\"concat_axis must be either 0 or 1\")\n    if concat_axis == 0:\n        tensors = [tensor_dict[k] for k in sorted(tensor_dict.keys())]\n        concat_tensor = tf.concat(tensors, axis=concat_axis)\n        processed_tensor = func(concat_tensor)\n        processed_tensors = tf.split(processed_tensor, len(tensor_dict), axis=concat_axis)\n        processed_dict = {\n            k: processed_tensors[i]\n            for i, k in enumerate(sorted(tensor_dict.keys()))\n        }\n    elif concat_axis == 1:\n        tensors = [tensor_dict[k] for k in sorted(tensor_dict.keys())]\n        concat_tensor = tf.concat(tensors, axis=concat_axis)\n        processed_tensor = func(concat_tensor)\n        processed_tensors = tf.split(processed_tensor, len(tensor_dict), axis=concat_axis)\n        processed_dict = {\n            k: processed_tensors[i]\n            for i, k in enumerate(sorted(tensor_dict.keys()))\n        }\n    return processed_dict\n\n", "CodeBleu_score": 43.26340794373119}, {"code": "def sync_apply(\n    tensor_dict: Dict[str, tf.Tensor], func: Callable, concat_axis: int = 1\n) -> Dict[str, tf.Tensor]\n\t\n    if concat_axis not in [0, 1]:\n        raise ValueError(\"concat_axis must be either 0 or 1\")\n    concat_tensor = tf.concat(list(tensor_dict.values()), axis=concat_axis)\n    processed_tensor = func(concat_tensor)\n    if concat_axis == 0:\n        processed_dict = {\n            key: tensor\n            for key, tensor in zip(\n                tensor_dict.keys(),\n                tf.split(processed_tensor, list(tensor_dict.values()), axis=0),\n            )\n        }\n    else:\n        processed_dict = {\n            key: tensor\n            for key, tensor in zip(\n                tensor_dict.keys(),\n                tf.split(processed_tensor, list(tensor_dict.values()), axis=1),\n            )\n        }\n    return processed_dict\n\n", "CodeBleu_score": 46.381629519519215}]}
{"instance_id": "df48e78d46d2", "generation_result": [{"code": "def from_float32_to_uint8(\n    tensor: tf.Tensor,\n    tensor_key: str = \"tensor\",\n    min_key: str = \"min\",\n    max_key: str = \"max\",\n) -> tf.Tensor\n\t\n    min_value = tf.reduce_min(tensor)\n    max_value = tf.reduce_max(tensor)\n    return {\n        tensor_key: tf.cast(\n            tf.round(\n                tf.clip_by_value(\n                    (tensor - min_value) / (max_value - min_value), 0, 1\n                )\n                * 255\n            ),\n            tf.uint8,\n        ),\n        min_key: min_value,\n        max_key: max_value,\n    }\n\n", "CodeBleu_score": 45.78755817509574}, {"code": "def from_float32_to_uint8(\n    tensor: tf.Tensor,\n    tensor_key: str = \"tensor\",\n    min_key: str = \"min\",\n    max_key: str = \"max\",\n) -> tf.Tensor\n\t\n    min_value = tf.reduce_min(tensor)\n    max_value = tf.reduce_max(tensor)\n    tensor = (tensor - min_value) / (max_value - min_value)\n    tensor = tf.cast(tensor * 255.0, tf.uint8)\n    return {tensor_key: tensor, min_key: min_value, max_key: max_value}\n\n\n", "CodeBleu_score": 47.202142731066075}, {"code": "def from_float32_to_uint8(\n    tensor: tf.Tensor,\n    tensor_key: str = \"tensor\",\n    min_key: str = \"min\",\n    max_key: str = \"max\",\n) -> tf.Tensor\n\t\n    min_value = tf.reduce_min(tensor)\n    max_value = tf.reduce_max(tensor)\n    tensor = (tensor - min_value) / (max_value - min_value)\n    tensor = tf.cast(tensor * 255.0, tf.uint8)\n    return {tensor_key: tensor, min_key: min_value, max_key: max_value}\n\n\n", "CodeBleu_score": 47.202142731066075}]}
{"instance_id": "7f479e5de492", "generation_result": [{"code": "def pad_and_partition(tensor: tf.Tensor, segment_len: int) -> tf.Tensor\n\t\n    padding = segment_len - tf.shape(tensor)[-1] % segment_len\n    padded_tensor = tf.pad(tensor, [[0, 0], [0, padding]])\n    return tf.reshape(padded_tensor, [-1, segment_len])\n\n\n", "CodeBleu_score": 18.714210337454652}, {"code": "def pad_and_partition(tensor: tf.Tensor, segment_len: int) -> tf.Tensor\n\t\n    tensor_length = tf.shape(tensor)[0]\n    padding_length = segment_len - (tensor_length % segment_len)\n    padded_tensor = tf.pad(tensor, [[0, padding_length]], constant_values=0)\n    return tf.reshape(padded_tensor, [-1, segment_len])\n\n\n", "CodeBleu_score": 21.940258279094245}, {"code": "def pad_and_partition(tensor: tf.Tensor, segment_len: int) -> tf.Tensor\n\t\n    pad_len = tf.cast(segment_len - tf.math.mod(tf.shape(tensor)[0], segment_len), tf.int32)\n    padded_tensor = tf.pad(tensor, [[0, pad_len], [0, 0]])\n    reshaped_tensor = tf.reshape(padded_tensor, [-1, segment_len, tf.shape(tensor)[-1]])\n    return reshaped_tensor", "CodeBleu_score": 29.01759072267326}]}
{"instance_id": "1789fae7253f", "generation_result": [{"code": "def pad_and_reshape(instr_spec, frame_length, F) -> Any\n\t\n    # Calculate the number of extra rows needed for padding\n    num_extra_rows = (frame_length // 2 + 1) * 2 - F\n\n    # Create an extension of zeros with the required number of extra rows\n    zeros_extension = tf.zeros((num_extra_rows, instr_spec.shape[1]), dtype=tf.float32)\n\n    # Concatenate the extension to the original spectrogram along the frequency axis\n    extended_spec = tf.concat([instr_spec, zeros_extension], axis=0)\n\n    # Reshape the extended spectrogram by merging the first two dimensions\n    processed_spec = tf.reshape(extended_spec, (-1, F * 2))\n\n    return processed_spec\n\n\n", "CodeBleu_score": 18.187216766830115}, {"code": "def pad_and_reshape(instr_spec, frame_length, F) -> Any\n\t\n    pad_width = frame_length - instr_spec.shape[0]\n    pad_width_left = pad_width // 2\n    pad_width_right = pad_width - pad_width_left\n    instr_spec_padded = np.pad(instr_spec, ((pad_width_left, pad_width_right), (0, 0)), 'constant')\n    instr_spec_padded = instr_spec_padded.reshape((-1, F))\n    return instr_spec_padded\n\n\n", "CodeBleu_score": 12.798903062820715}, {"code": "def pad_and_reshape(instr_spec, frame_length, F) -> Any\n\t\n    (batch_size, n_frames, n_freq) = instr_spec.shape\n    n_pad = F.ceil_div(frame_length, 2)\n    zeros = F.zeros((batch_size, n_pad, n_freq), dtype=instr_spec.dtype)\n    instr_spec = F.concatenate((zeros, instr_spec, zeros), axis=1)\n    return instr_spec.reshape((batch_size, n_frames + 2 * n_pad, n_freq))\n\n\n\n", "CodeBleu_score": 13.565403634233853}]}
{"instance_id": "8a4f4658d97c", "generation_result": [{"code": "def dataset_from_csv(csv_path: str, **kwargs) -> Any\n\t\n    import pandas as pd\n    import tensorflow as tf\n    df = pd.read_csv(csv_path, **kwargs)\n    ds = tf.data.Dataset.from_tensor_slices((df.values[:, :-1], df.values[:, -1]))\n    return ds\n\n\n", "CodeBleu_score": 32.53310572679816}, {"code": "def dataset_from_csv(csv_path: str, **kwargs) -> Any\n\t\n    df = pd.read_csv(csv_path)\n    dataset = tf.data.Dataset.from_tensor_slices((dict(df), df.pop('label')))\n    return dataset\n\n", "CodeBleu_score": 27.413735842737662}, {"code": "def dataset_from_csv(csv_path: str, **kwargs) -> Any\n\t\n    df = pd.read_csv(csv_path, **kwargs)\n    return tf.data.Dataset.from_tensor_slices(dict(df))\n\n\n", "CodeBleu_score": 27.127655353502654}]}
{"instance_id": "9a16e81b182f", "generation_result": [{"code": "def check_tensor_shape(tensor_tf: tf.Tensor, target_shape: Any) -> bool\n\t\n    result = True\n    for i, length in enumerate(target_shape):\n        result = result and (tensor_tf.shape[i] == length)\n    return result\n\n\n", "CodeBleu_score": 27.80117491800572}, {"code": "def check_tensor_shape(tensor_tf: tf.Tensor, target_shape: Any) -> bool\n\t\n    result = True\n    for dim, length in enumerate(target_shape):\n        result = result and tf.equal(tensor_tf.shape[dim], length)\n    return result\n\n\n", "CodeBleu_score": 29.68253540007751}, {"code": "def check_tensor_shape(tensor_tf: tf.Tensor, target_shape: Any) -> bool\n\t\n    result = True\n    for dim in target_shape:\n        result = result and (tensor_tf.shape[dim] == target_shape[dim])\n    return result\n\n\n", "CodeBleu_score": 24.801222808227376}]}
{"instance_id": "49d5d3084b3a", "generation_result": [{"code": "def get_training_dataset(\n    audio_params: Dict, audio_adapter: AudioAdapter, audio_path: str\n) -> Any\n\t\n    # Initialize dataset builder\n    builder = DatasetBuilder(\n        audio_params[\"chunk_duration\"],\n        audio_params[\"random_seed\"],\n        audio_adapter.instruments,\n    )\n    # Build dataset\n    return builder.build(\n        audio_params[\"train_csv\"],\n        audio_params[\"batch_size\"],\n        audio_params[\"shuffle\"],\n        audio_params[\"convert_to_uint\"],\n        audio_params[\"random_data_augmentation\"],\n        audio_params[\"random_time_crop\"],\n        audio_params[\"infinite_generator\"],\n        audio_path,\n        audio_params[\"wait_for_cache\"],\n        audio_params[\"num_parallel_calls\"],\n        audio_params[\"n_chunks_per_song\"],\n    )\n\n", "CodeBleu_score": 37.33241274640902}, {"code": "def get_training_dataset(\n    audio_params: Dict, audio_adapter: AudioAdapter, audio_path: str\n) -> Any\n\t\n    train_csv = os.path.join(audio_path, \"train.csv\")\n    dataset_builder = DatasetBuilder(\n        chunk_duration=audio_params[\"chunk_duration\"],\n        random_seed=audio_params[\"random_seed\"],\n    )\n    dataset = dataset_builder.build(\n        csv_path=train_csv,\n        batch_size=audio_params[\"batch_size\"],\n        n_chunks_per_song=audio_params[\"n_chunks_per_song\"],\n        random_data_augmentation=audio_params[\"random_data_augmentation\"],\n        random_time_crop=audio_params[\"random_time_crop\"],\n        cache_directory=audio_params[\"cache_directory\"],\n        wait_for_cache=audio_params[\"wait_for_cache\"],\n        num_parallel_calls=audio_params[\"num_parallel_calls\"],\n    )\n    return dataset\n\n", "CodeBleu_score": 32.06103627858552}, {"code": "def get_training_dataset(\n    audio_params: Dict, audio_adapter: AudioAdapter, audio_path: str\n) -> Any\n\t\n    builder = DatasetBuilder(\n        chunk_duration=audio_params[\"chunk_duration\"],\n        random_seed=audio_params[\"random_seed\"],\n    )\n    builder.add_instrument(audio_adapter)\n    return builder.build(\n        csv_path=audio_params[\"train_csv\"],\n        cache_directory=audio_params[\"cache_directory\"],\n        batch_size=audio_params[\"batch_size\"],\n        n_chunks_per_song=audio_params[\"n_chunks_per_song\"],\n        random_data_augmentation=audio_params[\"random_data_augmentation\"],\n        random_time_crop=audio_params[\"random_time_crop\"],\n    )\n\n", "CodeBleu_score": 34.83934277806724}]}
{"instance_id": "fe8bbbba06e3", "generation_result": [{"code": "def instruments(self) -> Any\n\t\n    if self.instrument_builders:\n        for builder in self.instrument_builders:\n            yield builder\n    else:\n        for instrument in self.instruments:\n            self.instrument_builders.append(InstrumentDatasetBuilder(instrument))\n        for builder in self.instrument_builders:\n            yield builder\n\n\n\n\n", "CodeBleu_score": 39.55221988856018}, {"code": "def instruments(self) -> Any\n\t\n    if not self.instrument_builders:\n        for instrument in self.instruments:\n            self.instrument_builders.append(InstrumentDatasetBuilder(instrument))\n    for builder in self.instrument_builders:\n        yield builder\n\n\n", "CodeBleu_score": 36.51889704297139}, {"code": "def instruments(self) -> Any\n\t\n    if self._instruments is None:\n        self._instruments = {}\n        for instrument in self._instrument_names:\n            self._instruments[instrument] = InstrumentDatasetBuilder(\n                instrument,\n                self._data_dir,\n                self._instrument_metadata,\n                self._instrument_metadata_file_path,\n                self._instrument_metadata_file_name,\n                self._instrument_metadata_file_extension,\n                self._instrument_metadata_file_delimiter,\n                self._instrument_metadata_file_quotechar,\n                self._instrument_metadata_file_escapechar,\n                self._instrument_metadata_file_encoding,\n                self._instrument_metadata_file_skiprows,\n                self._instrument_metadata_file_skipfooter,\n                self._instrument_metadata_file_skipinitialspace,\n                self._instrument_metadata_file_decimal,\n                self._instrument_metadata_file_thousands,\n                self._instrument_metadata_file_true_values,\n                self._instrument_metadata_file_false_values,\n                self._instrument_metadata_file_na_values,\n                self._instrument_metadata_file_comment,\n                self._instrument_metadata_file_index_col,\n                self._instrument_metadata_file_header,\n                self._instrument_metadata_file_names,\n                self._instrument_metadata_file_dtypes,\n                self._instrument_metadata_file_converters,\n                self._instrument_metadata_file_true_values,\n                self._instrument_metadata_file_false_values,\n                self._instrument_metadata_file_na_values,\n                self._instrument_metadata_file_comment,\n                self._instrument_metadata_file_index_col,\n                self._instrument_metadata_file_header,\n                self._instrument_metadata_file_names,\n                self._instrument_metadata_file_dtypes,\n                self._instrument_metadata_file_converters,\n                self._instrument_metadata_file_true_values,\n                self", "CodeBleu_score": 12.727030613840723}]}
{"instance_id": "438b57fd3413", "generation_result": [{"code": "def cache(self, dataset: Any, cache: Optional[str], wait: bool) -> Any\n\t\n    if cache is not None:\n        if wait:\n            while not exists(f\"{cache}.index\"):\n                logger.info(f\"Cache not available, wait {self.WAIT_PERIOD}\")\n                time.sleep(self.WAIT_PERIOD)\n        cache_path = os.path.split(cache)[0]\n        os.makedirs(cache_path, exist_ok=True)\n        return dataset.cache(cache)\n    return dataset", "CodeBleu_score": 93.65648279369884}, {"code": "def cache(self, dataset: Any, cache: Optional[str], wait: bool) -> Any\n\t\n    if cache is not None:\n        if wait:\n            while not exists(f\"{cache}.index\"):\n                logger.info(f\"Cache not available, wait {self.WAIT_PERIOD}\")\n                time.sleep(self.WAIT_PERIOD)\n        cache_path = os.path.split(cache)[0]\n        os.makedirs(cache_path, exist_ok=True)\n        return dataset.cache(cache)\n    return dataset", "CodeBleu_score": 93.65648279369884}, {"code": "def cache(self, dataset: Any, cache: Optional[str], wait: bool) -> Any\n\t\n    if cache is not None:\n        if wait:\n            while not exists(f\"{cache}.index\"):\n                logger.info(f\"Cache not available, wait {self.WAIT_PERIOD}\")\n                time.sleep(self.WAIT_PERIOD)\n        cache_path = os.path.split(cache)[0]\n        os.makedirs(cache_path, exist_ok=True)\n        return dataset.cache(cache)\n    return dataset\n\n", "CodeBleu_score": 93.65648279369884}]}
{"instance_id": "8838a3a09a66", "generation_result": [{"code": "def create_estimator(params: Dict, MWF: bool) -> tf.Tensor\n\t\n    model_dir = ModelProvider.get_model_dir(params)\n    params = ModelProvider.get_params(params)\n    session_config = tf.ConfigProto()\n    session_config.gpu_options.allow_growth = True\n    session_config.gpu_options.per_process_gpu_memory_fraction = 0.7\n    run_config = tf.estimator.RunConfig(session_config=session_config)\n    estimator = tf.estimator.Estimator(model_fn=model_fn, model_dir=model_dir, params=params, config=run_config)\n    return estimator\n\n\n", "CodeBleu_score": 34.681387877815745}, {"code": "def create_estimator(params: Dict, MWF: bool) -> tf.Tensor\n\t\n    model_dir = ModelProvider.get_model_dir(params)\n    params = update_params(params, model_dir)\n    config = tf.compat.v1.ConfigProto()\n    config.gpu_options.allow_growth = True\n    config.gpu_options.per_process_gpu_memory_fraction = 0.9\n    session_config = tf.compat.v1.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n    session_config.gpu_options.allow_growth = True\n    run_config = tf.estimator.RunConfig(session_config=session_config)\n    estimator = tf.estimator.Estimator(model_fn=model_fn, model_dir=model_dir, params=params, config=run_config)\n    return estimator\n\n\n", "CodeBleu_score": 31.493722396571783}, {"code": "def create_estimator(params: Dict, MWF: bool) -> tf.Tensor\n\t\n    model_dir = ModelProvider.get_model_dir(MWF)\n    params.update({'model_dir': model_dir})\n    session_config = tf.ConfigProto()\n    session_config.gpu_options.allow_growth = True\n    run_config = tf.estimator.RunConfig(session_config=session_config)\n    estimator = tf.estimator.Estimator(model_fn=model_fn, params=params, config=run_config, model_dir=model_dir)\n    return estimator\n\n\n", "CodeBleu_score": 30.92117147181672}]}
{"instance_id": "779c812bf2cb", "generation_result": [{"code": "def _separate_tensorflow(\n        self, waveform: np.ndarray, audio_descriptor: AudioDescriptor\n    ) -> Dict\n\t\n    waveform = _ensure_stereo(waveform)\n    prediction_generator = self._get_prediction_generator()\n    self.update_data({\"waveform\": waveform, \"audio_descriptor\": audio_descriptor})\n    prediction = next(prediction_generator)\n    del prediction[\"audio_id\"]\n    return prediction\n\n", "CodeBleu_score": 40.6652413389983}, {"code": "def _separate_tensorflow(\n        self, waveform: np.ndarray, audio_descriptor: AudioDescriptor\n    ) -> Dict\n\t\n    waveform = _ensure_stereo(waveform)\n    prediction_generator = self._get_prediction_generator()\n    self.update_data({\"waveform\": waveform, \"audio_descriptor\": audio_descriptor})\n    prediction = next(prediction_generator)\n    prediction.pop(\"audio_id\")\n    return prediction\n\n", "CodeBleu_score": 46.96671506551698}, {"code": "def _separate_tensorflow(\n        self, waveform: np.ndarray, audio_descriptor: AudioDescriptor\n    ) -> Dict\n\t\n    waveform = self._ensure_stereo(waveform)\n    prediction_generator = self._get_prediction_generator()\n    self.update_data({\"waveform\": waveform, \"audio_descriptor\": audio_descriptor})\n    prediction = next(prediction_generator)\n    prediction.pop(\"audio_id\")\n    return prediction\n\n", "CodeBleu_score": 47.13858376850019}]}
{"instance_id": "cf580cd2ed93", "generation_result": [{"code": "def compute_backbone_shapes(config, image_shape)\n\t\n    if callable(config.BACKBONE):\n        return config.COMPUTE_BACKBONE_SHAPE(image_shape)\n\n    if config.BACKBONE == \"resnet50\":\n        return np.array(\n            [[int(math.ceil(image_shape[0] / stride)),\n              int(math.ceil(image_shape[1] / stride))]\n             for stride in config.BACKBONE_STRIDES])\n\n    if config.BACKBONE == \"resnet101\":\n        return np.array(\n            [[int(math.ceil(image_shape[0] / stride)),\n              int(math.ceil(image_shape[1] / stride))]\n             for stride in config.BACKBONE_STRIDES])\n\n    raise ValueError(\"Backbone '{}' not supported.\".format(config.BACKBONE))\n\n\n", "CodeBleu_score": 70.80383446175789}, {"code": "def compute_backbone_shapes(config, image_shape)\n\t\n    if callable(config.BACKBONE):\n        return config.COMPUTE_BACKBONE_SHAPE(image_shape)\n    else:\n        assert config.BACKBONE in [\"resnet50\", \"resnet101\"], \"Backbone '{}' not implemented\".format(config.BACKBONE)\n        return {\n            \"resnet50\":  [\n                (32, 32, 64),\n                (64, 64, 128),\n                (128, 128, 256),\n                (256, 256, 512),\n                (512, 512, 1024),\n            ],\n            \"resnet101\": [\n                (32, 32, 64),\n                (64, 64, 128),\n                (128, 128, 256),\n                (256, 256, 512),\n                (512, 512, 1024),\n            ],\n        }[config.BACKBONE][:config.BACKBONE_STRIDES.count(1)]\n\n\n", "CodeBleu_score": 37.209916997667825}, {"code": "def compute_backbone_shapes(config, image_shape)\n\t\n    if callable(config.BACKBONE):\n        return config.COMPUTE_BACKBONE_SHAPE(image_shape)\n    else:\n        assert config.BACKBONE in [\"resnet50\", \"resnet101\"]\n        return {\n            \"resnet50\": compute_backbone_shapes_resnet50(image_shape),\n            \"resnet101\": compute_backbone_shapes_resnet101(image_shape),\n        }[config.BACKBONE]\n\n", "CodeBleu_score": 43.313377348031835}]}
{"instance_id": "ebd61a2e59ca", "generation_result": [{"code": "def clip_boxes_graph(boxes, window)\n\t\n    box_components = tf.split(value=boxes, num_or_size_splits=4, axis=1)\n    window_components = tf.split(value=window, num_or_size_splits=4, axis=1)\n    clipped_box_components = []\n    for b, w in zip(box_components, window_components):\n        # w[0] is the width of the window, w[1] is the height of the window.\n        b = tf.maximum(tf.minimum(b, w[0]), 0.0)\n        clipped_box_components.append(b)\n    clipped_box = tf.concat(values=clipped_box_components, axis=1)\n    clipped_box.set_shape((clipped_box.shape[0], 4))\n    return clipped_box\n\n\n", "CodeBleu_score": 23.203716041455795}, {"code": "def clip_boxes_graph(boxes, window)\n\t\n    # split window into (y1, x1, y2, x2)\n    wy1, wx1, wy2, wx2 = tf.split(window, 4)\n    wy1 = tf.expand_dims(wy1, 1)\n    wx1 = tf.expand_dims(wx1, 1)\n    wy2 = tf.expand_dims(wy2, 1)\n    wx2 = tf.expand_dims(wx2, 1)\n    # split boxes into (y1, x1, y2, x2)\n    y1, x1, y2, x2 = tf.split(boxes, 4, axis=1)\n    # clip\n    y1 = tf.maximum(tf.minimum(y1, wy2), wy1)\n    x1 = tf.maximum(tf.minimum(x1, wx2), wx1)\n    y2 = tf.maximum(tf.minimum(y2, wy2), wy1)\n    x2 = tf.maximum(tf.minimum(x2, wx2), wx1)\n    clipped = tf.concat([y1, x1, y2, x2], axis=1, name=\"clipped_boxes\")\n    # clipped.set_shape((clipped.shape[0], 4))\n    return clipped\n\n", "CodeBleu_score": 46.83055281260489}, {"code": "def clip_boxes_graph(boxes, window)\n\t\n    # split\n    wy1, wx1, wy2, wx2 = tf.split(window, 4)\n    y1, x1, y2, x2 = tf.split(boxes, 4)\n    y1 = clip_to_window(y1, wy1)\n    x1 = clip_to_window(x1, wx1)\n    y2 = clip_to_window(y2, wy2)\n    x2 = clip_to_window(x2, wx2)\n    clipped = tf.concat([y1, x1, y2, x2], 1)\n    clipped.set_shape((None, 4))\n    return clipped\n\n\n", "CodeBleu_score": 29.789626974736482}]}
{"instance_id": "8e0411587a70", "generation_result": [{"code": "def rpn_graph(feature_map, anchors_per_location, anchor_stride)\n\t\n    # Shared convolutional layer for the class and box predictions\n    conv_class = Conv2D(anchors_per_location * 2, (1, 1), activation='sigmoid', name='rpn_conv_class')(feature_map)\n    conv_box = Conv2D(anchors_per_location * 4, (1, 1), activation='linear', name='rpn_conv_box')(feature_map)\n    # Reshape the class and box predictions to obtain class probabilities and bounding box predictions\n    class_logits = Reshape((-1, 2), name=\"rpn_class_logits\")(conv_class)\n    class_probs = Activation(\"softmax\", name=\"rpn_class_probs\")(class_logits)\n    box_deltas = Reshape((-1, 4), name=\"rpn_box_deltas\")(conv_box)\n    return class_logits, class_probs, box_deltas\n\n\n", "CodeBleu_score": 18.21038381390159}, {"code": "def rpn_graph(feature_map, anchors_per_location, anchor_stride)\n\t\n    # The shared convolutional layer is used to extract features from the feature map.\n    # The output of the shared convolutional layer is passed through two convolutional layers for class scores and bounding box predictions.\n    # The class scores are reshaped and passed through a softmax activation to obtain class probabilities.\n    # The bounding box predictions are also reshaped.\n    # The function returns the class logits, class probabilities, and bounding box predictions.\n    # The shape of the feature map is (batch_size, height, width, channels).\n    # The number of anchors per location is specified by the anchors_per_location argument.\n    # The anchor stride is specified by the anchor_stride argument.\n    # The number of filters for the shared convolutional layer is 512.\n    # The kernel size for the shared convolutional layer is (3, 3).\n    # The stride for the shared convolutional layer is (1, 1).\n    # The padding for the shared convolutional layer is 'same'.\n    # The activation for the shared convolutional layer is 'relu'.\n    # The number of filters for the class scores layer is 2 * anchors_per_location.\n    # The kernel size for the class scores layer is (1, 1).\n    # The stride for the class scores layer is (1, 1).\n    # The padding for the class scores layer is 'valid'.\n    # The activation for the class scores layer is 'linear'.\n    # The number of filters for the bounding box predictions layer is 4 * anchors_per_location.\n    # The kernel size for the bounding box predictions layer is (1, 1).\n    # The stride for the bounding box predictions layer is (1, 1).\n    # The padding for the bounding box predictions layer is 'valid'.\n    # The activation for the bounding box predictions layer is 'linear'.\n    # The function returns the class logits, class probabilities, and bounding box predictions.\n    # The class logits are the output of the class scores layer.\n    # The class probabilities are the softmax of the class logits.\n    # The bounding box predictions are the output of the bounding box predictions layer.\n    # The shape of the class", "CodeBleu_score": 0.8163911766399816}, {"code": "def rpn_graph(feature_map, anchors_per_location, anchor_stride)\n\t\n    # Feature map shape: [batch_size, num_channels, feature_map_height, feature_map_width]\n    # Anchors per location: number of anchors per grid cell\n    # Anchor stride: stride between anchors\n    num_anchors_per_location = len(anchors_per_location)\n    num_channels = feature_map.get_shape()[1].value\n    num_classes = 2\n    # Shared convolutional layer\n    shared_conv = tf.keras.layers.Conv2D(num_anchors_per_location * num_classes, kernel_size=3, strides=anchor_stride, activation='linear', padding='same', name='rpn_conv_shared')\n    shared_conv_output = shared_conv(feature_map)\n    # Class scores\n    class_scores = tf.keras.layers.Conv2D(num_anchors_per_location * num_classes, kernel_size=1, activation='linear', padding='same', name='rpn_class_raw')(shared_conv_output)\n    class_scores_output = class_scores\n    # Reshape class scores to [batch_size, num_anchors_per_location, num_classes]\n    class_scores_output = tf.reshape(class_scores_output, [tf.shape(class_scores_output)[0], -1, num_classes])\n    # Softmax activation for class scores\n    class_scores_output = tf.nn.softmax(class_scores_output, axis=-1)\n    # Bounding box predictions\n    bbox_pred = tf.keras.layers.Conv2D(num_anchors_per_location * 4, kernel_size=1, activation='linear', padding='same', name='rpn_bbox_pred')(shared_conv_output)\n    bbox_pred_output = bbox_pred\n    # Reshape bounding box predictions to [batch_size, num_anchors_per_location, 4]\n    bbox_pred_output = tf.", "CodeBleu_score": 36.01089851296346}]}
{"instance_id": "70777c9261d8", "generation_result": [{"code": "def smooth_l1_loss(y_true, y_pred)\n\t\n    absolute_difference = K.abs(y_true - y_pred)\n    square_difference = 0.5 * (y_true - y_pred) ** 2\n    l1_loss = tf.where(K.less(absolute_difference, 1.0), square_difference, absolute_difference - 0.5)\n    return K.mean(l1_loss)\n\n\n", "CodeBleu_score": 33.47034045358029}, {"code": "def smooth_l1_loss(y_true, y_pred)\n\t\n    absolute_difference = K.abs(y_true - y_pred)\n    squared_difference = K.square(absolute_difference)\n    small_difference_mask = K.cast(K.less(absolute_difference, 1.0), 'float32')\n    large_difference_mask = K.cast(K.greater_equal(absolute_difference, 1.0), 'float32')\n    small_difference_loss = small_difference_mask * 0.5 * squared_difference\n    large_difference_loss = large_difference_mask * (absolute_difference - 0.5)\n    return K.sum(small_difference_loss + large_difference_loss)\n\n\n", "CodeBleu_score": 27.447973330688097}, {"code": "def smooth_l1_loss(y_true, y_pred)\n\t\n    absolute_difference = tf.abs(y_true - y_pred)\n    small_difference = 0.5 * (tf.square(absolute_difference) / 1.0)\n    large_difference = absolute_difference - 0.5\n    return tf.where(tf.less(absolute_difference, 1.0), small_difference, large_difference)\n\n\n\n", "CodeBleu_score": 19.461213463471104}]}
{"instance_id": "0e747b604087", "generation_result": [{"code": "def build_model(x_train, num_classes)\n\t\n    # Reset the TensorFlow graph\n    tf.reset_default_graph()\n\n    # Define the input layer\n    x = tf.placeholder(tf.float32, [None, x_train.shape[1], x_train.shape[2], x_train.shape[3]])\n\n    # Define the first convolutional layer\n    conv1 = tf.layers.conv2d(inputs=x, filters=32, kernel_size=[3, 3], padding=\"same\", activation=tf.nn.relu)\n\n    # Define the second convolutional layer\n    conv2 = tf.layers.conv2d(inputs=conv1, filters=64, kernel_size=[3, 3], padding=\"same\", activation=tf.nn.relu)\n\n    # Define the max-pooling layer\n    pool = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n\n    # Flatten the output of the max-pooling layer\n    flatten = tf.layers.flatten(pool)\n\n    # Define the dense layer with ReLU activation\n    dense = tf.layers.dense(inputs=flatten, units=128, activation=tf.nn.relu)\n\n    # Define the output layer with softmax activation\n    logits = tf.layers.dense(inputs=dense, units=num_classes)\n\n    # Define the loss function and optimizer\n    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_train, logits=logits))\n    optimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss)\n\n    # Define the accuracy metric\n    correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(y_train, 1))\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\n", "CodeBleu_score": 22.797169054662998}, {"code": "def build_model(x_train, num_classes)\n\t\n    model = tf.keras.Sequential([\n        tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=x_train.shape[1:]),\n        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n        tf.keras.layers.MaxPooling2D((2, 2)),\n        tf.keras.layers.Flatten(),\n        tf.keras.layers.Dense(64, activation='relu'),\n        tf.keras.layers.Dense(num_classes, activation='softmax')\n    ])\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n    return model\n\n\n\n\n", "CodeBleu_score": 27.793266085740225}, {"code": "def build_model(x_train, num_classes)\n\t\n    model = tf.keras.Sequential([\n        tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=x_train.shape[1:]),\n        tf.keras.layers.MaxPooling2D((2, 2)),\n        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n        tf.keras.layers.MaxPooling2D((2, 2)),\n        tf.keras.layers.Flatten(),\n        tf.keras.layers.Dense(64, activation='relu'),\n        tf.keras.layers.Dense(num_classes, activation='softmax')\n    ])\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n    return model\n\n\n", "CodeBleu_score": 28.596530350321416}]}
{"instance_id": "813e3e2844f4", "generation_result": [{"code": "def extract_bboxes(mask)\n\t\n    bboxes = []\n    for i in range(mask.shape[0]):\n        y, x = np.where(mask[i])\n        if len(y) == 0:\n            bboxes.append([0, 0, 0, 0])\n        else:\n            bboxes.append([np.min(x), np.min(y), np.max(x), np.max(y)])\n    return np.array(bboxes, dtype=np.int32)\n\n\n", "CodeBleu_score": 15.404709476281209}, {"code": "def extract_bboxes(mask)\n\t\n    \n    boxes = [np.zeros([4], dtype=np.int32) for i in range(mask.shape[-1])]\n    for i in range(mask.shape[-1]):\n        m = mask[:, :, i]\n        horizontal_indicies = np.where(np.any(m, axis=0))[0]\n        vertical_indicies = np.where(np.any(m, axis=1))[0]\n        if horizontal_indicies.shape[0]:\n            x1, x2 = horizontal_indicies[[0, -1]]\n            y1, y2 = vertical_indicies[[0, -1]]\n            x2 += 1\n            y2 += 1\n        else:\n            x1, x2, y1, y2 = 0, 0, 0, 0\n        boxes[i][0] = x1\n        boxes[i][1] = y1\n        boxes[i][2] = x2\n        boxes[i][3] = y2\n    return boxes\n\n\n", "CodeBleu_score": 52.22808053623661}, {"code": "def extract_bboxes(mask)\n\t\n    bboxes = []\n    for i in range(mask.shape[0]):\n        indices = np.where(mask[i] != 0)\n        if len(indices[0]) == 0:\n            bboxes.append([0, 0, 0, 0])\n        else:\n            bboxes.append([np.min(indices[1]), np.min(indices[0]), np.max(indices[1]), np.max(indices[0])])\n    return bboxes\n\n\n", "CodeBleu_score": 15.455959910004058}]}
{"instance_id": "7ad5d430d6ed", "generation_result": [{"code": "def compute_iou(box, boxes, box_area, boxes_area)\n\t\n    y1 = np.maximum(box[0], boxes[:, 0])\n    y2 = np.minimum(box[2], boxes[:, 2])\n    x1 = np.maximum(box[1], boxes[:, 1])\n    x2 = np.minimum(box[3], boxes[:, 3])\n    intersection = np.maximum(x2 - x1, 0) * np.maximum(y2 - y1, 0)\n    union = box_area + boxes_area[:] - intersection[:]\n    iou = intersection / union\n    return iou\n\n", "CodeBleu_score": 58.663259083089194}, {"code": "def compute_iou(box, boxes, box_area, boxes_area)\n\t\n    y1 = np.maximum(box[0], boxes[:, 0])\n    y2 = np.minimum(box[2], boxes[:, 2])\n    x1 = np.maximum(box[1], boxes[:, 1])\n    x2 = np.minimum(box[3], boxes[:, 3])\n    intersection = np.maximum(x2 - x1, 0) * np.maximum(y2 - y1, 0)\n    union = box_area + boxes_area[:] - intersection[:]\n    iou = intersection / union\n    return iou\n\n\n", "CodeBleu_score": 58.663259083089194}, {"code": "def compute_iou(box, boxes, box_area, boxes_area)\n\t\n    y1 = np.maximum(box[0], boxes[:, 0])\n    y2 = np.minimum(box[2], boxes[:, 2])\n    x1 = np.maximum(box[1], boxes[:, 1])\n    x2 = np.minimum(box[3], boxes[:, 3])\n    intersection = np.maximum(x2 - x1, 0) * np.maximum(y2 - y1, 0)\n    union = box_area + boxes_area[:] - intersection[:]\n    iou = intersection / union\n    return iou\n\n\n\n\n", "CodeBleu_score": 58.663259083089194}]}
{"instance_id": "15941b852859", "generation_result": [{"code": "def compute_overlaps(boxes1, boxes2)\n\t\n    # If either box is empty, return empty result\n    if boxes1.shape[0] == 0 or boxes2.shape[0] == 0:\n        return np.zeros((boxes1.shape[0], boxes2.shape[0]))\n    # Compute intersections\n    ixmin = np.maximum(boxes1[:, 1], boxes2[:, 1])\n    iymin = np.maximum(boxes1[:, 0], boxes2[:, 0])\n    ixmax = np.minimum(boxes1[:, 3], boxes2[:, 3])\n    iymax = np.minimum(boxes1[:, 2], boxes2[:, 2])\n    iw = np.maximum(ixmax - ixmin, 0.)\n    ih = np.maximum(iymax - iymin, 0.)\n    inters = iw * ih\n    # Compute unions\n    uni = ((boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1]) +\n           (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1]) -\n           inters)\n    overlaps = inters / uni\n    return overlaps\n\n", "CodeBleu_score": 49.68883368750151}, {"code": "def compute_overlaps(boxes1, boxes2)\n\t\n    # If either box is empty, return empty result\n    if boxes1.shape[0] == 0 or boxes2.shape[0] == 0:\n        return np.zeros((boxes1.shape[0], boxes2.shape[0]))\n    # Compute intersections\n    ixmin = np.maximum(boxes1[:, 1], boxes2[:, 1])\n    iymin = np.maximum(boxes1[:, 0], boxes2[:, 0])\n    ixmax = np.minimum(boxes1[:, 3], boxes2[:, 3])\n    iymax = np.minimum(boxes1[:, 2], boxes2[:, 2])\n    iw = np.maximum(ixmax - ixmin, 0)\n    ih = np.maximum(iymax - iymin, 0)\n    inters = iw * ih\n    # Compute unions\n    uni = ((boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1]) +\n           (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1]) -\n           inters)\n    overlaps = inters / uni\n    return overlaps\n\n", "CodeBleu_score": 49.724265723691964}, {"code": "def compute_overlaps(boxes1, boxes2)\n\t\n    # If either box is empty, return empty result\n    if boxes1.shape[0] == 0 or boxes2.shape[0] == 0:\n        return np.zeros((boxes1.shape[0], boxes2.shape[0]))\n    # Compute intersections\n    ixmin = np.maximum(boxes1[:, 1], boxes2[:, 1])\n    iymin = np.maximum(boxes1[:, 0], boxes2[:, 0])\n    ixmax = np.minimum(boxes1[:, 3], boxes2[:, 3])\n    iymax = np.minimum(boxes1[:, 2], boxes2[:, 2])\n    iw = np.maximum(ixmax - ixmin + 1., 0.)\n    ih = np.maximum(iymax - iymin + 1., 0.)\n    inters = iw * ih\n    # Calculate the areas\n    areas1 = (boxes1[:, 2] - boxes1[:, 0] + 1.) * (boxes1[:, 3] - boxes1[:, 1] + 1.)\n    areas2 = (boxes2[:, 2] - boxes2[:, 0] + 1.) * (boxes2[:, 3] - boxes2[:, 1] + 1.)\n    # compute the intersection over union\n    uni = (areas1 + areas2) - inters\n    overlaps = inters / uni\n    return overlaps\n\n", "CodeBleu_score": 43.656819163954516}]}
{"instance_id": "597533722ce3", "generation_result": [{"code": "def load_image(self, image_id):\n\t\n    image_path = self.image_info[image_id]['path']\n    image = cv2.imread(image_path)\n    if image.shape[2] == 4:\n        image = image[:, :, :3]\n    return image\n\n\n", "CodeBleu_score": 44.64811573260388}, {"code": "def load_image(self, image_id):\n\t\n    path = self.image_info[image_id][\"path\"]\n    image = skimage.io.imread(path)\n    image = image.astype(np.uint8)\n    if image.shape[-1] == 4:\n        image = image[:, :, :3]\n    return image\n\n\n", "CodeBleu_score": 60.906162183188904}, {"code": "def load_image(self, image_id):\n\t\n    image = Image.open(self.image_info[image_id][\"path\"])\n    image.load()\n    image = image.convert(\"RGB\")\n    if image.mode == \"RGBA\":\n        image = image.convert(\"RGB\")\n    return image\n\n\n", "CodeBleu_score": 37.77944009143852}]}
{"instance_id": "9b852997ca05", "generation_result": [{"code": "def display_images(images, titles=None, cols=4, cmap=None, norm=None,\n                   interpolation=None)\n\t\n\n    if titles is None:\n        titles = ['Image (%d)' % i for i in range(1, len(images) + 1)]\n    if len(images) != len(titles):\n        raise ValueError('Number of titles does not match number of images.')\n\n    rows = int(np.ceil(len(images) / cols))\n    position = range(1, len(images) + 1)\n\n    plt.figure()\n    for n, (image, title) in enumerate(zip(images, titles)):\n        plt.subplot(rows, cols, position[n])\n        plt.imshow(image, cmap=cmap, norm=norm, interpolation=interpolation)\n        plt.title(title)\n        plt.axis('off')\n    plt.tight_layout()\n    plt.show()\n\n\n", "CodeBleu_score": 40.89034475304283}, {"code": "def display_images(images, titles=None, cols=4, cmap=None, norm=None,\n                   interpolation=None)\n\t\n    rows = len(images) // cols + 1\n    plt.figure(figsize=(14, 14 * rows // cols))\n    i = 1\n    for image, title in zip(images, titles):\n        plt.subplot(rows, cols, i)\n        plt.title(title if title else f\"Image {i}\")\n        plt.imshow(image, cmap=cmap, norm=norm, interpolation=interpolation)\n        plt.axis('off')\n        i += 1\n    plt.tight_layout()\n    plt.show()\n\n", "CodeBleu_score": 56.880364926866235}, {"code": "def display_images(images, titles=None, cols=4, cmap=None, norm=None,\n                   interpolation=None)\n\t\n    assert ((titles is None) or (len(images) == len(titles)))\n    n_images = len(images)\n    if titles is None: titles = ['Image (%d)' % i for i in range(1, n_images + 1)]\n    fig = plt.figure()\n    for n, (image, title) in enumerate(zip(images, titles)):\n        a = fig.add_subplot(cols, np.ceil(n_images / float(cols)), n + 1)\n        if image.ndim == 2:\n            plt.gray()\n        plt.imshow(image, cmap=cmap, norm=norm, interpolation=interpolation)\n        a.set_title(title)\n    fig.set_size_inches(np.array(fig.get_size_inches()) * n_images)\n    plt.show()\n\n\n", "CodeBleu_score": 36.76492283051003}]}
