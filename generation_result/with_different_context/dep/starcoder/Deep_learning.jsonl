{"instance_id": "1f9f4d85e825", "generation_result": [{"code": "def load_model_from_config(self, half_attention)\n\t\n\n    # Load the model from cache if available\n    cache_path = os.path.join(self.cache_dir, self.cache_name)\n    if os.path.exists(cache_path) and os.path.isfile(cache_path):\n        try:\n            self.logger.info(f\"Loading model from cache {cache_path}...\")\n            model = torch.load(cache_path, map_location=self.device)\n            if self.half_precision:\n                model.half()\n            model.eval()\n            self.logger.info(\"Model loaded from cache.\")\n            return model\n        except Exception as e:\n            self.logger.info(f\"Error loading model from cache: {e}\")\n\n    # Load the model from the specified path\n    model = torch.load(self.model_path, map_location=self.device)\n    if self.half_precision:\n        model.half()\n    model.eval()\n\n    # Move the model to the appropriate device\n    model.to(self.device)\n\n    # Save the model to cache for future use\n    if self.cache_dir:\n        if not os.path.exists(self.cache_dir):\n            os.makedirs(self.cache_dir)\n        torch.save(model, cache_path)\n\n    return model\n\n", "CodeBleu_score": 30.194345233223572}, {"code": "def load_model_from_config(self, half_attention)\n\t\n\n    # Load the model from a cache if it is available and valid\n    if self.config.use_cache and self.config.cache_dir:\n        cached_model = self.load_model_from_cache()\n        if cached_model:\n            return cached_model\n\n    # Load the model from a file\n    model = self.load_model_from_file(half_attention)\n\n    # Configure the model\n    model = self.configure_model(model)\n\n    # Move the model to the appropriate device\n    model = self.move_model_to_device(model)\n\n    # Ensure the model is in evaluation mode\n    model = self.ensure_model_is_eval(model)\n\n    # Cache the model for future use\n    if self.config.use_cache and self.config.cache_dir:\n        self.cache_model(model)\n\n    return model\n\n", "CodeBleu_score": 17.27040109055533}, {"code": "def load_model_from_config(self, half_attention)\n\t\n    if self.use_cache:\n        # Check if a cached model is available and valid\n        cached_model_path = self.get_cached_model_path()\n        if os.path.exists(cached_model_path):\n            if self.verbose:\n                print(\"Loading model from cache:\", cached_model_path)\n            return self.load_model_from_path(cached_model_path, half_attention)\n\n    # Load the model from the specified path\n    model = self.load_model_from_path(self.model_path, half_attention)\n\n    # Cache the model for future use\n    if self.use_cache:\n        if self.verbose:\n            print(\"Caching model:\", cached_model_path)\n        self.cache_model(model)\n\n    return model\n\n", "CodeBleu_score": 19.181526738858643}]}
{"instance_id": "8dc955007aa2", "generation_result": [{"code": "def convsample_ddim(model, cond, steps, shape, eta=1.0, callback=None, normals_sequence=None,\n                    mask=None, x0=None, quantize_x0=False, temperature=1., score_corrector=None,\n                    corrector_kwargs=None, x_t=None\n                    )\n\t\n\n    # Extract batch size from shape\n    batch_size = shape[0]\n\n    # Print eta and steps\n    print(f\"eta = {eta}, steps = {steps}\")\n\n    # Create DDIM sampler\n    sampler = DDIMSampler(model, batch_size)\n\n    # Sample\n    samples, intermediates = sampler.sample(steps, shape, cond, eta=eta, callback=callback,\n                                            normals_sequence=normals_sequence, mask=mask, x0=x0,\n                                            quantize_x0=quantize_x0, temperature=temperature,\n                                            score_corrector=score_corrector,\n                                            corrector_kwargs=corrector_kwargs, x_t=x_t)\n\n    # Return samples and intermediates\n    return samples, intermediates\n\n", "CodeBleu_score": 54.260721091184564}, {"code": "def convsample_ddim(model, cond, steps, shape, eta=1.0, callback=None, normals_sequence=None,\n                    mask=None, x0=None, quantize_x0=False, temperature=1., score_corrector=None,\n                    corrector_kwargs=None, x_t=None\n                    )\n\t\n\n    batch_size = shape[0]\n    print(\"batch_size =\", batch_size)\n    print(\"eta =\", eta)\n    print(\"steps =\", steps)\n    sampler = DDIMSampler(model)\n    return sampler.sample(steps, batch_size, shape, cond, eta, callback=callback,\n                          normals_sequence=normals_sequence, mask=mask, x0=x0,\n                          quantize_x0=quantize_x0, temperature=temperature,\n                          score_corrector=score_corrector, corrector_kwargs=corrector_kwargs, x_t=x_t)\n\n", "CodeBleu_score": 41.19360387719423}, {"code": "def convsample_ddim(model, cond, steps, shape, eta=1.0, callback=None, normals_sequence=None,\n                    mask=None, x0=None, quantize_x0=False, temperature=1., score_corrector=None,\n                    corrector_kwargs=None, x_t=None\n                    )\n\t\n\n    # Extract the batch size from the shape parameter\n    batch_size = shape[0]\n\n    # Print the eta and steps values\n    print(f'eta: {eta}, steps: {steps}')\n\n    # Initialize the DDIM sampler\n    ddim_sampler = DDIMSampler(model)\n\n    # Sample from the DDIM sampler\n    samples, intermediates = ddim_sampler.sample(steps, batch_size, shape, cond, eta=eta, callback=callback,\n                                                normals_sequence=normals_sequence, mask=mask, x0=x0,\n                                                quantize_x0=quantize_x0, temperature=temperature,\n                                                score_corrector=score_corrector,\n                                                corrector_kwargs=corrector_kwargs, x_t=x_t)\n\n    # Return the generated samples and intermediate values\n    return samples, intermediates\n\n", "CodeBleu_score": 59.72722733633513}]}
{"instance_id": "9bdcd9081480", "generation_result": [{"code": "def ema_scope(self, context=None)\n\t\n    if context is None:\n        context = self.context\n    if self.ema_enabled:\n        if context is not None:\n            context.print(\"EMA: Saving weights...\")\n        self.save_weights()\n        if context is not None:\n            context.print(\"EMA: Restoring weights...\")\n        self.restore_weights()\n        if context is not None:\n            context.print(\"EMA: Weights restored.\")\n    else:\n        if context is not None:\n            context.print(\"EMA: EMA is disabled.\")\n\n", "CodeBleu_score": 43.60192949381726}, {"code": "def ema_scope(self, context=None)\n\t\n    if context is None:\n        context = self.context\n    if self.ema_enabled:\n        if context is not None:\n            context.print('Restoring weights from EMA')\n        self.ema_restore()\n    else:\n        if context is not None:\n            context.print('Saving weights for EMA')\n        self.ema_save()\n\n", "CodeBleu_score": 37.680820076750024}, {"code": "def ema_scope(self, context=None)\n\t\n    return _pywrap_tensorflow_internal.MetaGraphDef_ema_scope(self, context)", "CodeBleu_score": 4.7895960512840405}]}
{"instance_id": "14ab009e1c5c", "generation_result": [{"code": "def init_from_ckpt(self, path, ignore_keys=None)\n\t\n    if not os.path.exists(path):\n        raise RuntimeError(\"Checkpoint '{}' not found\".format(path))\n    checkpoint = torch.load(path, map_location=\"cpu\")\n    # get state_dict from checkpoint\n    if isinstance(checkpoint, dict) and \"state_dict\" in checkpoint:\n        state_dict = checkpoint[\"state_dict\"]\n    else:\n        state_dict = checkpoint\n    # strip prefix of state_dict\n    if list(state_dict.keys())[0].startswith(\"module.\"):\n        state_dict = {k[7:]: v for k, v in state_dict.items()}\n    # delete ignored params\n    if ignore_keys is not None:\n        for k in ignore_keys:\n            del state_dict[k]\n    # load state_dict\n    self.load_state_dict(state_dict, strict=False)\n    # release memory\n    del checkpoint\n    torch.cuda.empty_cache()\n", "CodeBleu_score": 34.61572883136345}, {"code": "def init_from_ckpt(self, path, ignore_keys=None)\n\t\n    self.load_state_dict(torch.load(path, map_location=torch.device('cpu')), strict=False)\n    if ignore_keys is not None:\n        for k in ignore_keys:\n            self.state_dict().pop(k)\n    missing_keys, unexpected_keys = self.load_state_dict(torch.load(path, map_location=torch.device('cpu')), strict=False)\n    if len(missing_keys) > 0:\n        print('Missing Keys: {}'.format(missing_keys))\n    if len(unexpected_keys) > 0:\n        print('Unexpected Keys: {}'.format(unexpected_keys))\n    return self\n\n", "CodeBleu_score": 31.6903776070104}, {"code": "def init_from_ckpt(self, path, ignore_keys=None)\n\t\n    if not os.path.exists(path):\n        raise ValueError(\"Checkpoint '{}' does not exist\".format(path))\n    checkpoint = torch.load(path, map_location=\"cpu\")\n    # get state_dict from checkpoint\n    if isinstance(checkpoint, dict) and \"state_dict\" in checkpoint:\n        state_dict = checkpoint[\"state_dict\"]\n    else:\n        raise RuntimeError(\n            \"No state_dict found in checkpoint file {}\".format(path)\n        )\n    # strip prefix of state_dict\n    if list(state_dict.keys())[0].startswith(\"module.\"):\n        state_dict = {k[7:]: v for k, v in state_dict.items()}\n    # ignore_keys\n    if ignore_keys is not None:\n        for k in ignore_keys:\n            state_dict.pop(k, None)\n    # load state_dict\n    self.load_state_dict(state_dict, strict=False)\n    # release memory\n    del checkpoint\n    torch.cuda.empty_cache()\n", "CodeBleu_score": 34.234266864883736}]}
{"instance_id": "754176e27b81", "generation_result": [{"code": "def forward(self, input, return_pred_indices=False)\n\t\n    quant, diff, info = self.encode(input)\n    dec = self.decode(quant)\n    if return_pred_indices:\n        return dec, diff, info, quant\n    else:\n        return dec, diff, info\n\n", "CodeBleu_score": 45.08419697173648}, {"code": "def forward(self, input, return_pred_indices=False)\n\t\n    quant, diff, info = self.encode(input)\n    dec = self.decode(quant)\n    if return_pred_indices:\n        return dec, diff, info, quant\n    return dec, diff, info\n", "CodeBleu_score": 48.83788397469108}, {"code": "def forward(self, input, return_pred_indices=False)\n\t\n    quant, diff, info = self.encode(input)\n    dec = self.decode(quant)\n    if return_pred_indices:\n        return dec, diff, info, quant\n    return dec, diff, info\n", "CodeBleu_score": 48.83788397469108}]}
{"instance_id": "443df6fdaf0d", "generation_result": [{"code": "def get_input(self, batch, k)\n\t\n    input = batch[k]\n    if len(input.shape) == 3:\n        input = input.unsqueeze(0)\n    if self.resize:\n        input = self.bicubic_resize(input)\n    input = input.permute(0, 3, 1, 2).contiguous().float()\n    input = input.to(self.device)\n    return input\n\n", "CodeBleu_score": 21.06765771893744}, {"code": "def get_input(self, batch, k)\n\t\n    input = batch[k]\n    if len(input.shape) == 3:\n        input = input.unsqueeze(0)\n    if self.resize:\n        size = random.randint(self.resize_range[0], self.resize_range[1])\n        input = F.interpolate(input, size=(size, size), mode='bicubic', align_corners=False)\n    input = input.permute(0, 3, 1, 2)\n    input = input.to(memory_format=torch.contiguous_format).float()\n    return input.detach()\n\n", "CodeBleu_score": 35.38938482572471}, {"code": "def get_input(self, batch, k)\n\t\n    if self.resize:\n        if self.training:\n            x = F.interpolate(batch[k], size=(self.resize_range[0], self.resize_range[1]), mode='bicubic', align_corners=False)\n        else:\n            x = F.interpolate(batch[k], size=(self.resize_range[2], self.resize_range[3]), mode='bicubic', align_corners=False)\n    else:\n        x = batch[k]\n    x = x.permute(0, 3, 1, 2)\n    x = x.to(torch.float32).contiguous()\n    x.requires_grad = False\n    return x\n\n", "CodeBleu_score": 39.44753433971751}]}
{"instance_id": "953a2632fc15", "generation_result": [{"code": "def training_step(self, batch, batch_idx, optimizer_idx)\n\t\n    # get data\n    x = self.get_input(batch, \"img\")\n    y = self.get_input(batch, \"seg\")\n    x_rec, y_rec, z, z_rec, y_logits, x_logits = self.forward(x, y)\n\n    # compute losses\n    ae_loss = self.get_ae_loss(x, x_rec, y, y_rec, z, z_rec)\n    if self.discriminator is not None:\n        disc_loss = self.get_disc_loss(x, x_logits, y, y_logits)\n        loss = ae_loss + disc_loss\n    else:\n        loss = ae_loss\n\n    # log metrics\n    self.log(\"train_loss\", loss)\n    self.log(\"train_ae_loss\", ae_loss)\n    if self.discriminator is not None:\n        self.log(\"train_disc_loss\", disc_loss)\n\n    return loss\n", "CodeBleu_score": 25.070248098027886}, {"code": "def training_step(self, batch, batch_idx, optimizer_idx)\n\t\n    x = self.get_input(batch, \"img\")\n    x2 = self.get_input(batch, \"img2\")\n    x3 = self.get_input(batch, \"img3\")\n    x4 = self.get_input(batch, \"img4\")\n    x5 = self.get_input(batch, \"img5\")\n    x6 = self.get_input(batch, \"img6\")\n    x7 = self.get_input(batch, \"img7\")\n    x8 = self.get_input(batch, \"img8\")\n    x9 = self.get_input(batch, \"img9\")\n    x10 = self.get_input(batch, \"img10\")\n    x11 = self.get_input(batch, \"img11\")\n    x12 = self.get_input(batch, \"img12\")\n    x13 = self.get_input(batch, \"img13\")\n    x14 = self.get_input(batch, \"img14\")\n    x15 = self.get_input(batch, \"img15\")\n    x16 = self.get_input(batch, \"img16\")\n    x17 = self.get_input(batch, \"img17\")\n    x18 = self.get_input(batch, \"img18\")\n    x19 = self.get_input(batch, \"img19\")\n    x20 = self.get_input(batch, \"img20\")\n    x21 = self.get_input(batch, \"img21\")\n    x22 = self.get_input(batch, \"img22\")\n    x23 = self.get_input(batch, \"img23\")\n    x24 = self.get_input(batch, \"img24\")\n    x25 = self.get_input(batch, \"img25\")\n    x26 = self.get_input(batch, \"img26\")\n    x27 = self.get_input(batch, \"img27\")\n    x28 = self.get_input(batch, \"img28\")\n    x29 = self.get_input(batch, \"img29\")\n    x30 = self.get_input", "CodeBleu_score": 18.44119506356484}, {"code": "def training_step(self, batch, batch_idx, optimizer_idx)\n\t\n    if self.global_step == 0:\n        self.log_hyperparams()\n    x = self.get_input(batch, \"x\")\n    y = self.get_input(batch, \"y\")\n    y_hat = self(x)\n    ae_loss = self.loss_function(y_hat, y)\n    if optimizer_idx == 0:\n        self.log(\"train_ae_loss\", ae_loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n        return ae_loss\n    elif optimizer_idx == 1:\n        disc_loss = self.loss_function(self.discriminator(y_hat), self.discriminator(y))\n        self.log(\"train_disc_loss\", disc_loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n        return disc_loss\n", "CodeBleu_score": 34.40006367019998}]}
{"instance_id": "b449b5e8b77f", "generation_result": [{"code": "def log_images(self, batch, only_inputs=False, plot_ema=False, **kwargs)\n\t\n    if self.global_step % self.log_freq != 0:\n        return\n    if self.global_step == 0:\n        self.logger.info(f\"Logging {self.image_key} at {self.global_step}\")\n    if self.image_key == \"segmentation\":\n        if not hasattr(self, \"colorize\"):\n            self.register_buffer(\"colorize\", torch.randn(3, self.get_input(batch, self.image_key).shape[1], 1, 1).to(self.device))\n        self.logger.experiment.add_image(f\"{self.image_key}_inputs\", self.to_rgb(self.get_input(batch, self.image_key)), self.global_step, dataformats=\"HWC\")\n    else:\n        self.logger.experiment.add_image(f\"{self.image_key}_inputs\", self.get_input(batch, self.image_key), self.global_step, dataformats=\"HWC\")\n    if not only_inputs:\n        if plot_ema:\n            self.model.eval()\n            with torch.no_grad():\n                x = self.get_input(batch, self.image_key)\n                x_rec = self.model(x)\n                if self.image_key == \"segmentation\":\n                    x_rec = self.to_rgb(x_rec)\n                self.logger.experiment.add_image(f\"{self.image_key}_reconstruction_ema\", x_rec, self.global_step, dataformats=\"HWC\")\n            self.model.train()\n        else:\n            x_rec = self.model(self.get_input(batch, self.image_key))\n            if self.image_key == \"segmentation\":\n                x_rec = self.to_rgb(x_rec)\n            self.logger.experiment.add_image(f\"{self.image_key}_reconstruction\", x_rec, self.global_step, dataformats=\"HWC\")\n    self.logger.experiment.flush()\n", "CodeBleu_score": 30.426368396255466}, {"code": "def log_images(self, batch, only_inputs=False, plot_ema=False, **kwargs)\n\t\n    if not hasattr(self, \"logger\"):\n        return\n    if self.global_step % self.log_every != 0:\n        return\n    if self.global_step == 0:\n        # log hyperparameters\n        self.logger.log_hyperparams(self.hparams)\n        # log model graph\n        self.logger.log_graph(self, batch)\n    if only_inputs:\n        # log inputs\n        self.logger.experiment.add_image(f\"{self.image_key}_inputs\", self.to_rgb(self.get_input(batch, self.image_key)), self.global_step)\n    else:\n        # log reconstructions\n        recons = self.to_rgb(self.get_input(batch, self.reconstruction_key))\n        self.logger.experiment.add_image(f\"{self.image_key}_reconstructions\", recons, self.global_step)\n        if plot_ema:\n            # log EMA reconstructions\n            ema_recons = self.to_rgb(self.ema_model.get_input(batch, self.reconstruction_key))\n            self.logger.experiment.add_image(f\"{self.image_key}_ema_reconstructions\", ema_recons, self.global_step)\n    # log sample images\n    if self.global_step == 0:\n        self.sample_images(batch)\n    # log gradients\n    if self.global_step % self.grad_log_every == 0:\n        for name, param in self.named_parameters():\n            self.logger.experiment.add_histogram(name, param, self.global_step)\n            if param.grad is not None:\n                self.logger.experiment.add_histogram(f\"{name}.grad\", param.grad, self.global_step)\n    # log model weights\n    if self.global_step % self.model_log_every == 0:\n        self.logger.experiment.add_histogram(\"model_weights\", self.model.state_dict(), self.global_step)\n    # log EMA model weights\n    if plot_ema and self.global_step % self.ema_model_log_every == 0:\n        self.logger.experiment.add_histogram(\"ema_model_weights\", self.ema_model.state_dict(), self", "CodeBleu_score": 26.352799977558472}, {"code": "def log_images(self, batch, only_inputs=False, plot_ema=False, **kwargs)\n\t\n    if self.global_step % self.log_frequency != 0:\n        return\n    if only_inputs:\n        self.log_image_grid(batch, \"inputs\", **kwargs)\n    else:\n        self.log_image_grid(batch, \"inputs\", **kwargs)\n        self.log_image_grid(batch, \"reconstructions\", **kwargs)\n        if plot_ema and hasattr(self, \"get_ema_weights\"):\n            self.log_image_grid(batch, \"reconstructions\", weights=self.get_ema_weights(), **kwargs)", "CodeBleu_score": 19.494072736436095}]}
{"instance_id": "540d089e3481", "generation_result": [{"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss_type == 'l1':\n        loss = torch.abs(pred - target)\n    elif self.loss_type == 'l2':\n        loss = (pred - target) ** 2\n    else:\n        raise ValueError(\"Unknown loss type. Please use either 'l1' or 'l2'.\")\n    if mean:\n        return torch.mean(loss)\n    else:\n        return loss\n", "CodeBleu_score": 35.80015083547347}, {"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss_type == 'l1':\n        loss = torch.abs(pred - target).mean() if mean else torch.abs(pred - target).sum()\n    elif self.loss_type == 'l2':\n        loss = ((pred - target) ** 2).mean() if mean else ((pred - target) ** 2).sum()\n    else:\n        raise ValueError(f\"Unknown loss type '{self.loss_type}'.\")\n    return loss\n\n", "CodeBleu_score": 42.70153987512298}, {"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss_type == \"l1\":\n        if mean:\n            return torch.mean(torch.abs(pred - target))\n        else:\n            return torch.sum(torch.abs(pred - target))\n    elif self.loss_type == \"l2\":\n        if mean:\n            return torch.mean((pred - target) ** 2)\n        else:\n            return torch.sum((pred - target) ** 2)\n    else:\n        raise ValueError(\"Unknown loss type. Please use either 'l1' or 'l2'.\")\n\n", "CodeBleu_score": 31.564414302935212}]}
{"instance_id": "63a08283666f", "generation_result": [{"code": "def forward(self, x, c, *args, **kwargs)\n\t\n    assert x.shape[0] == c.shape[0]\n    assert x.ndim == 4\n    assert c.ndim == 2\n\n    if self.training:\n        t = torch.randint(0, self.num_timesteps, (x.shape[0],), device=x.device)\n    else:\n        t = torch.randint(0, self.num_timesteps, (x.shape[0],), device=x.device).item()\n\n    if self.learned_conditioning:\n        c = self.get_learned_conditioning(c)\n\n    x = self.q_sample(x, t)\n    x = self.model(x, t, c, *args, **kwargs)\n    return x\n", "CodeBleu_score": 50.56664927391405}, {"code": "def forward(self, x, c, *args, **kwargs)\n\t\n    if self.training:\n        t = torch.randint(0, self.num_timesteps, (x.shape[0],), device=x.device)\n    else:\n        t = self.num_timesteps - 1\n    if self.get_learned_conditioning:\n        c = self.get_learned_conditioning(c)\n    x = self.q_sample(x, t)\n    return self.p_losses(x, t, *args, **kwargs)\n", "CodeBleu_score": 39.565008475136366}, {"code": "def forward(self, x, c, *args, **kwargs)\n\t\n    if self.training:\n        self.num_timesteps = c.shape[1]\n        self.cur_timestep = random.randint(0, self.num_timesteps - 1)\n\n    t = torch.full((x.shape[0],), self.cur_timestep, device=x.device, dtype=torch.long)\n\n    if self.cond_stage_forward is not None:\n        c = self.get_learned_conditioning(c)\n\n    return self.p_losses(x, t, c)\n", "CodeBleu_score": 35.87722846041462}]}
{"instance_id": "4627e1e8ed62", "generation_result": [{"code": "def get_codebook_entry(self, indices, shape)\n\t\n    if self.map_to_all:\n        indices = self.map_to_all(indices)\n    if self.unmap_to_all:\n        indices = self.unmap_to_all(indices)\n    if self.permute:\n        indices = self.permute(indices)\n    return self.codebook(indices).reshape(shape)\n\n", "CodeBleu_score": 30.22943349719977}, {"code": "def get_codebook_entry(self, indices, shape)\n\t\n    if self.map_to_all:\n        indices = self.map_to_all(indices)\n    if self.unmap_to_all:\n        indices = self.unmap_to_all(indices)\n    if self.transpose_latent:\n        indices = indices.permute(0, 2, 3, 1)\n    if self.reshape_latent:\n        indices = indices.reshape(shape)\n    return self.codebook(indices)\n\n", "CodeBleu_score": 38.58396637503697}, {"code": "def get_codebook_entry(self, indices, shape)\n\t\n    if self.re_embed > 0:\n        indices = self.map_to_all(indices)\n    if self.permute_soft:\n        indices = self.permute_soft(indices)\n    return self.embed_code(indices).reshape(shape)\n", "CodeBleu_score": 24.9293409037872}]}
{"instance_id": "c55d014827ef", "generation_result": [{"code": "def factorization(dimension: int, factor:int=-1) -> tuple[int, int]\n\t\n    if dimension < 1:\n        raise ValueError(\"Dimension must be positive.\")\n    if factor > 0:\n        if dimension % factor == 0:\n            return factor, dimension//factor\n        else:\n            raise ValueError(\"Factor must divide the dimension.\")\n    elif factor < 0:\n        factor = abs(factor)\n        for i in range(1, factor+1):\n            if dimension % i == 0:\n                return i, dimension//i\n    else:\n        raise ValueError(\"Factor must be positive or negative.\")\n\n", "CodeBleu_score": 28.598662886034347}, {"code": "def factorization(dimension: int, factor:int=-1) -> tuple[int, int]\n\t\n    if factor > 0 and factor <= dimension:\n        return factor, dimension//factor\n    elif factor < 0:\n        factor = abs(factor)\n        m = 1\n        n = dimension\n        while m*n < factor:\n            m += 1\n        return m, n\n    else:\n        raise ValueError(\"Factor must be a positive integer or -1\")\n\n", "CodeBleu_score": 30.180856134823607}, {"code": "def factorization(dimension: int, factor:int=-1) -> tuple[int, int]\n\t\n    if factor > 0:\n        if factor > dimension:\n            raise ValueError(\"Factor must be less than or equal to dimension.\")\n        m = factor\n        n = dimension // factor\n    elif factor < 0:\n        m = 1\n        n = dimension\n        while m * n != dimension:\n            if m * n > abs(factor):\n                m += 1\n            elif m * n < abs(factor):\n                n -= 1\n            else:\n                break\n        if m * n != dimension:\n            raise ValueError(\"No factorization exists for the given dimension and factor.\")\n    else:\n        m = 1\n        n = dimension\n    return m, n\n\n", "CodeBleu_score": 41.74421574166882}]}
{"instance_id": "5d613ec26036", "generation_result": [{"code": "def apply_weight_decompose(self, updown, orig_weight)\n\t\n    if updown == \"up\":\n        weight = self.weight\n        if weight.shape != orig_weight.shape:\n            weight = torch.nn.functional.interpolate(weight, size=orig_weight.shape[2:], mode=\"bilinear\", align_corners=True)\n        weight = torch.nn.functional.normalize(weight, p=2, dim=1)\n        weight = weight * self.scale_factor\n        weight = weight - orig_weight\n        return weight\n    elif updown == \"down\":\n        weight = orig_weight + self.weight\n        weight = torch.nn.functional.normalize(weight, p=2, dim=1)\n        weight = weight * self.scale_factor\n        return weight\n\n", "CodeBleu_score": 32.11183557762697}, {"code": "def apply_weight_decompose(self, updown, orig_weight)\n\t\n    if updown == 'up':\n        if orig_weight.shape == self.weight.shape:\n            return orig_weight - self.weight\n        else:\n            raise ValueError('The shape of the original weight is not equal to the shape of the current weight.')\n    elif updown == 'down':\n        if orig_weight.shape == self.weight.shape:\n            return self.weight - orig_weight\n        else:\n            raise ValueError('The shape of the original weight is not equal to the shape of the current weight.')\n    else:\n        raise ValueError('The updown argument must be either \"up\" or \"down\".')\n\n", "CodeBleu_score": 18.46488316877249}, {"code": "def apply_weight_decompose(self, updown, orig_weight)\n\t\n    device = orig_weight.device\n    dtype = orig_weight.dtype\n    orig_weight = orig_weight.to(self.weight.device)\n    orig_weight = orig_weight.to(self.weight.dtype)\n    if updown == \"up\":\n        weight = orig_weight + self.weight\n    elif updown == \"down\":\n        weight = orig_weight - self.weight\n    weight = weight / torch.norm(weight)\n    weight = weight * self.scale\n    weight = weight - orig_weight\n    weight = weight.to(device)\n    weight = weight.to(dtype)\n    return weight\n\n", "CodeBleu_score": 24.794555190281088}]}
{"instance_id": "c73d0d1cd3e5", "generation_result": [{"code": "def finalize_updown(self, updown, orig_weight, output_shape, ex_bias=None)\n\t\n    # Match the device/dtype\n    orig_weight = orig_weight.to(updown.dtype)\n    updown = updown.to(orig_weight.device)\n\n    # Adjust the bias\n    if ex_bias is not None:\n        ex_bias = ex_bias.to(updown.dtype)\n        ex_bias = ex_bias * self.multiplier\n\n    # Adjust the shape\n    if updown.shape[1:] != output_shape:\n        updown = updown.reshape(updown.shape[0], *output_shape)\n\n    # Adjust the updown tensor with the bias\n    if ex_bias is not None:\n        updown = updown + ex_bias\n\n    # Adjust the shape\n    if updown.shape[1:] != orig_weight.shape:\n        updown = updown.reshape(updown.shape[0], *orig_weight.shape)\n\n    # Apply the weight decomposition\n    if self.apply_weight_decompose:\n        updown = self.apply_weight_decompose(updown, orig_weight)\n\n    return updown\n", "CodeBleu_score": 49.786517391674955}, {"code": "def finalize_updown(self, updown, orig_weight, output_shape, ex_bias=None)\n\t\n    # Adjust the updown tensor with the bias and reshape it according to the output shape\n    updown = updown + self.bias.to(device=updown.device, dtype=updown.dtype)\n    updown = updown.reshape(output_shape)\n\n    # If the original and updown weights have the same number of elements, it reshapes updown to match the original weight's shape\n    if orig_weight.numel() == updown.numel():\n        updown = updown.reshape(orig_weight.shape)\n\n    # If an additional bias is provided, it applies a multiplier to it\n    if ex_bias is not None:\n        ex_bias = ex_bias * self.bias_multiplier\n\n    # If a scaling factor is defined, it applies a weight decomposition to updown\n    if self.scale is not None:\n        updown = self.apply_weight_decompose(updown, orig_weight)\n\n    return updown, ex_bias\n", "CodeBleu_score": 51.53126691007026}, {"code": "def finalize_updown(self, updown, orig_weight, output_shape, ex_bias=None)\n\t\n    # Apply bias\n    if ex_bias is not None:\n        updown = updown + ex_bias\n\n    # Reshape\n    updown = updown.reshape(output_shape)\n\n    # Reshape if necessary\n    if orig_weight.numel() != updown.numel():\n        updown = updown.reshape(orig_weight.shape)\n\n    # Apply multiplier\n    if self.multiplier is not None:\n        updown = updown * self.multiplier\n\n    # Apply weight decomposition\n    if self.apply_weight_decompose:\n        updown = self.apply_weight_decompose(updown, orig_weight)\n\n    # Return\n    return updown\n", "CodeBleu_score": 47.102283524086154}]}
{"instance_id": "c6a41665c62c", "generation_result": [{"code": "def calc_updown(self, orig_weight)\n\t\n    weight = self.weight.to(orig_weight.device, dtype=orig_weight.dtype)\n    if self.bias is not None:\n        bias = self.bias.to(orig_weight.device, dtype=orig_weight.dtype)\n    else:\n        bias = None\n\n    if self.updown_func is None:\n        return self.finalize_updown(weight, orig_weight, self.output_shape, bias)\n    else:\n        return self.updown_func(weight, orig_weight, self.output_shape, bias)\n\n", "CodeBleu_score": 40.41612749964649}, {"code": "def calc_updown(self, orig_weight)\n\t\n\n    # If the original weight tensor is not on the same device as the current weight tensor, then transfer the current weight tensor and optional bias to the device of the original weight tensor.\n    if orig_weight.device != self.weight.device:\n        self.to(device=orig_weight.device)\n\n    # If the current weight tensor is not on the same device as the original weight tensor, then transfer the current weight tensor and optional bias to the device of the original weight tensor.\n    if self.weight.device != orig_weight.device:\n        self.weight.to(device=orig_weight.device)\n\n    # If the optional bias is not None, then transfer the optional bias to the device of the original weight tensor.\n    if self.bias is not None:\n        self.bias.to(device=orig_weight.device)\n\n    # If the current weight tensor is not on the same device as the original weight tensor, then transfer the current weight tensor and optional bias to the device of the original weight tensor.\n    if self.weight.device != orig_weight.device:\n        self.weight.to(device=orig_weight.device)\n\n    # If the optional bias is not None, then transfer the optional bias to the device of the original weight tensor.\n    if self.bias is not None:\n        self.bias.to(device=orig_weight.device)\n\n    # If the current weight tensor is not on the same device as the original weight tensor, then transfer the current weight tensor and optional bias to the device of the original weight tensor.\n    if self.weight.device != orig_weight.device:\n        self.weight.to(device=orig_weight.device)\n\n    # If the optional bias is not None, then transfer the optional bias to the device of the original weight tensor.\n    if self.bias is not None:\n        self.bias.to(device=orig_weight.device)\n\n    # If the current weight tensor is not on the same device as the original weight tensor, then transfer the current weight tensor and optional bias to the device of the original weight tensor.\n    if self.weight.device != orig_weight.device:\n        self.weight.to(device=orig_weight.device)\n\n    # If the optional bias is not None, then transfer the optional bias to the device of the original weight tensor.\n    if self.bias is not None:\n        self.bias.to(", "CodeBleu_score": 26.390331862723844}, {"code": "def calc_updown(self, orig_weight)\n\t\n    if self.dora_weight_decompose:\n        updown, ex_bias = self.weight_decompose(orig_weight)\n    else:\n        updown = self.weight.to(orig_weight.device)\n        ex_bias = None\n\n    if self.dora_updown_bias:\n        updown, ex_bias = self.finalize_updown(updown, orig_weight, self.output_shape, ex_bias)\n\n    return updown, ex_bias\n\n", "CodeBleu_score": 45.71531214740264}]}
{"instance_id": "1bb9e5d94af8", "generation_result": [{"code": "def purge_networks_from_memory()\n\t\n    global network_count\n    while network_count > network_limit:\n        for network in list(networks):\n            del networks[network]\n        gc.collect()\n        network_count = len(networks)\n\n", "CodeBleu_score": 31.55419043929818}, {"code": "def purge_networks_from_memory()\n\t\n    global _network_count\n    if _network_count > _network_count_limit:\n        for i in range(_network_count - _network_count_limit):\n            del _networks[_networks.keys()[0]]\n        gc.collect()\n        _network_count = _network_count_limit\n\n", "CodeBleu_score": 25.09527386317355}, {"code": "def purge_networks_from_memory()\n\t\n    global __networks__\n    while len(__networks__) > __NETWORK_LIMIT__:\n        del __networks__[0]\n    gc.collect()\n\n", "CodeBleu_score": 22.97643715443753}]}
{"instance_id": "42b48ebf4d07", "generation_result": [{"code": "def network_restore_weights_from_backup(self: Union[torch.nn.Conv2d, torch.nn.Linear, torch.nn.GroupNorm, torch.nn.LayerNorm, torch.nn.MultiheadAttention]):\n\t\n    if self.weight_backup is not None:\n        self.weight.data = self.weight_backup.data\n    if self.bias_backup is not None:\n        self.bias.data = self.bias_backup.data\n    if isinstance(self, torch.nn.MultiheadAttention):\n        self.out_proj.weight.data = self.out_proj_weight_backup.data\n        self.out_proj.bias.data = self.out_proj_bias_backup.data\n\n", "CodeBleu_score": 25.96208783819775}, {"code": "def network_restore_weights_from_backup(self: Union[torch.nn.Conv2d, torch.nn.Linear, torch.nn.GroupNorm, torch.nn.LayerNorm, torch.nn.MultiheadAttention]):\n\t\n    if self.has_backup():\n        self.weight.data = self.weight_backup\n        if self.bias is not None:\n            self.bias.data = self.bias_backup\n        if isinstance(self, torch.nn.MultiheadAttention):\n            self.in_proj_weight.data = self.in_proj_weight_backup\n            self.in_proj_bias.data = self.in_proj_bias_backup\n    else:\n        print(\"WARNING: no backup weights found for layer {}\".format(self))\n\n", "CodeBleu_score": 26.680651087673656}, {"code": "def network_restore_weights_from_backup(self: Union[torch.nn.Conv2d, torch.nn.Linear, torch.nn.GroupNorm, torch.nn.LayerNorm, torch.nn.MultiheadAttention]):\n\t\n    if self.weight.device != self.bias.device:\n        raise RuntimeError(\"The weights and biases of the layer are stored on different devices. This is not supported.\")\n    if self.weight.device == torch.device(\"cpu\"):\n        raise RuntimeError(\"The weights and biases of the layer are stored on the CPU. This is not supported.\")\n    if not hasattr(self, \"backup_weight\"):\n        raise RuntimeError(\"The layer does not have a backup weight.\")\n    if not hasattr(self, \"backup_bias\"):\n        raise RuntimeError(\"The layer does not have a backup bias.\")\n    if self.backup_weight.shape != self.weight.shape:\n        raise RuntimeError(\"The shape of the backup weight does not match the shape of the current weight.\")\n    if self.backup_bias.shape != self.bias.shape:\n        raise RuntimeError(\"The shape of the backup bias does not match the shape of the current bias.\")\n    if isinstance(self, torch.nn.MultiheadAttention):\n        self.in_proj_weight.data = self.backup_in_proj_weight\n        self.in_proj_bias.data = self.backup_in_proj_bias\n        self.out_proj.weight.data = self.backup_out_proj_weight\n        self.out_proj.bias.data = self.backup_out_proj_bias\n    else:\n        self.weight.data = self.backup_weight\n        self.bias.data = self.backup_bias\n    delattr(self, \"backup_weight\")\n    delattr(self, \"backup_bias\")\n    if hasattr(self, \"backup_in_proj_weight\"):\n        delattr(self, \"backup_in_proj_weight\")\n        delattr(self, \"backup_in_proj_bias\")\n        delattr(self, \"backup_out_proj_weight\")\n        delattr(self, \"backup_out_proj_bias\")\n\n", "CodeBleu_score": 30.399348411445377}]}
{"instance_id": "c8fcd357131a", "generation_result": [{"code": "def network_forward(org_module, input, original_forward)\n\t\n    if not hasattr(org_module, \"network_current_names\"):\n        return original_forward(org_module, input)\n\n    current_names = org_module.network_current_names\n    weights_backup = org_module.network_weights_backup\n    bias_backup = org_module.network_bias_backup\n\n    # Cast input to correct device\n    if input.device != org_module.weight.device:\n        input = input.to(org_module.weight.device)\n\n    # Restore weights from backup\n    if weights_backup is not None:\n        if isinstance(org_module, torch.nn.MultiheadAttention):\n            org_module.in_proj_weight.copy_(weights_backup[0])\n            org_module.out_proj.weight.copy_(weights_backup[1])\n        else:\n            org_module.weight.copy_(weights_backup)\n\n    if bias_backup is not None:\n        if isinstance(org_module, torch.nn.MultiheadAttention):\n            org_module.out_proj.bias.copy_(bias_backup)\n        else:\n            org_module.bias.copy_(bias_backup)\n    else:\n        if isinstance(org_module, torch.nn.MultiheadAttention):\n            org_module.out_proj.bias = None\n        else:\n            org_module.bias = None\n\n    # Reset cached weight\n    org_module.network_current_names = ()\n    org_module.network_weights_backup = None\n    org_module.network_bias_backup = None\n\n    # Call original forward\n    output = original_forward(org_module, input)\n\n    # Apply loaded networks\n    for network in org_module.network_loaded:\n        if network.name in current_names:\n            updown, ex_bias = network.calc_updown(org_module.sd_module.weight)\n            output = network.ops(output, weight=updown, bias=ex_bias, **network.extra_kwargs)\n\n    return output\n", "CodeBleu_score": 31.10444636245182}, {"code": "def network_forward(org_module, input, original_forward)\n\t\n    if not hasattr(org_module, \"network_loaded_networks\"):\n        return original_forward(org_module, input)\n\n    if not hasattr(org_module, \"network_current_names\"):\n        org_module.network_current_names = ()\n\n    if not hasattr(org_module, \"network_weights_backup\"):\n        org_module.network_weights_backup = None\n\n    if not hasattr(org_module, \"network_bias_backup\"):\n        org_module.network_bias_backup = None\n\n    if not hasattr(org_module, \"network_updown\"):\n        org_module.network_updown = None\n\n    if not hasattr(org_module, \"network_ex_bias\"):\n        org_module.network_ex_bias = None\n\n    if not hasattr(org_module, \"network_ops\"):\n        org_module.network_ops = None\n\n    if not hasattr(org_module, \"extra_kwargs\"):\n        org_module.extra_kwargs = {}\n\n    if not hasattr(org_module, \"network_loaded_networks\"):\n        org_module.network_loaded_networks = {}\n\n    if not hasattr(org_module, \"network_loaded_weights\"):\n        org_module.network_loaded_weights = {}\n\n    if not hasattr(org_module, \"network_loaded_bias\"):\n        org_module.network_loaded_bias = {}\n\n    if not hasattr(org_module, \"network_loaded_updown\"):\n        org_module.network_loaded_updown = {}\n\n    if not hasattr(org_module, \"network_loaded_ex_bias\"):\n        org_module.network_loaded_ex_bias = {}\n\n    if not hasattr(org_module, \"network_loaded_ops\"):\n        org_module.network_loaded_ops = {}\n\n    if not hasattr(org_module, \"network_loaded_extra_kwargs\"):\n        org_module.network_loaded_extra_kwargs = {}\n\n    if not hasattr(org_module, \"network_loaded_names\"):\n        org_module.network_loaded_names = {}\n\n    if not hasattr(org_module, \"network_loaded_types\"):\n        org_module.network_loaded_types = {}\n\n    if not hasattr(org_module, \"network_loaded_weights_backup\"):\n        org_module.network_loaded_weights_backup = {}\n\n    if not hasattr", "CodeBleu_score": 17.742268395695326}, {"code": "def network_forward(org_module, input, original_forward)\n\t\n    if not hasattr(org_module, \"network_current_names\"):\n        org_module.network_current_names = ()\n    if not hasattr(org_module, \"network_weights_backup\"):\n        org_module.network_weights_backup = None\n    if not hasattr(org_module, \"network_bias_backup\"):\n        org_module.network_bias_backup = None\n    if not hasattr(org_module, \"network_restore_weights_from_backup\"):\n        org_module.network_restore_weights_from_backup = network_restore_weights_from_backup\n    if not hasattr(org_module, \"network_reset_cached_weight\"):\n        org_module.network_reset_cached_weight = network_reset_cached_weight\n    if not hasattr(org_module, \"network_names_and_weights\"):\n        org_module.network_names_and_weights = ()\n\n    if not hasattr(org_module, \"network_updown_cache\"):\n        org_module.network_updown_cache = {}\n\n    if not hasattr(org_module, \"network_ex_bias_cache\"):\n        org_module.network_ex_bias_cache = {}\n\n    if not hasattr(org_module, \"network_weight_cache\"):\n        org_module.network_weight_cache = {}\n\n    if not hasattr(org_module, \"network_bias_cache\"):\n        org_module.network_bias_cache = {}\n\n    if not hasattr(org_module, \"network_weight_backup_cache\"):\n        org_module.network_weight_backup_cache = {}\n\n    if not hasattr(org_module, \"network_bias_backup_cache\"):\n        org_module.network_bias_backup_cache = {}\n\n    if not hasattr(org_module, \"network_weight_cache_keys\"):\n        org_module.network_weight_cache_keys = {}\n\n    if not hasattr(org_module, \"network_bias_cache_keys\"):\n        org_module.network_bias_cache_keys = {}\n\n    if not hasattr(org_module, \"network_weight_backup_cache_keys\"):\n        org_module.network_weight_backup_cache_keys = {}\n\n    if not hasattr(org_module, \"network_bias_backup_cache_keys\"):\n        org_module.network_bias_backup_cache", "CodeBleu_score": 14.876062677484814}]}
{"instance_id": "1070211f5c11", "generation_result": [{"code": "def trainables(self)\n\t\n    trainables = []\n    for layer in self.modules():\n        if isinstance(layer, torch.nn.Linear) or isinstance(layer, torch.nn.LayerNorm):\n            trainables.extend(layer.parameters())\n    return trainables\n\n", "CodeBleu_score": 39.04546486508601}, {"code": "def trainables(self)\n\t\n    trainables = []\n    for layer in self.modules():\n        if isinstance(layer, torch.nn.Linear) or isinstance(layer, torch.nn.LayerNorm):\n            trainables.append(layer.weight)\n            if layer.bias is not None:\n                trainables.append(layer.bias)\n    return trainables\n\n", "CodeBleu_score": 38.948620086230825}, {"code": "def trainables(self)\n\t\n    trainable_params = []\n    for name, param in self.named_parameters():\n        if param.requires_grad:\n            trainable_params.append(param)\n    return trainable_params\n\n", "CodeBleu_score": 23.684524588475384}]}
{"instance_id": "0c3ba62c96a7", "generation_result": [{"code": "def load_hypernetworks(names, multipliers=None)\n\t\n    shared.hypernetworks.clear()\n\n    if multipliers is not None:\n        if not isinstance(multipliers, dict):\n            raise TypeError(\"multipliers must be a dictionary\")\n\n        for name, multiplier in multipliers.items():\n            if name not in names:\n                raise ValueError(f\"name {name} not found in names\")\n\n            hypernetwork = load_hypernetwork(name)\n\n            if hypernetwork is not None:\n                hypernetwork.set_multiplier(multiplier)\n\n            shared.hypernetworks.append(hypernetwork)\n\n    else:\n        for name in names:\n            hypernetwork = load_hypernetwork(name)\n\n            if hypernetwork is not None:\n                shared.hypernetworks.append(hypernetwork)\n\n    return shared.hypernetworks\n\n", "CodeBleu_score": 36.295380956167755}, {"code": "def load_hypernetworks(names, multipliers=None)\n\t\n    if multipliers is None:\n        multipliers = [1.0] * len(names)\n\n    hypernetworks = []\n\n    for name, multiplier in zip(names, multipliers):\n        hypernetwork = load_hypernetwork(name)\n\n        if hypernetwork is None:\n            continue\n\n        hypernetwork.set_multiplier(multiplier)\n        hypernetworks.append(hypernetwork)\n\n    shared.hypernetworks.clear()\n    shared.hypernetworks.extend(hypernetworks)\n\n", "CodeBleu_score": 36.30615014180212}, {"code": "def load_hypernetworks(names, multipliers=None)\n\t\n\n    if multipliers is None:\n        multipliers = {}\n\n    hypernetworks = []\n\n    for name in names:\n        hypernetwork = load_hypernetwork(name)\n\n        if hypernetwork is not None:\n            if name in multipliers:\n                hypernetwork.set_multiplier(multipliers[name])\n\n            hypernetworks.append(hypernetwork)\n\n    shared.hypernetworks.clear()\n    shared.hypernetworks.update({hypernetwork.name: hypernetwork.path for hypernetwork in hypernetworks})\n    return hypernetworks\n\n", "CodeBleu_score": 41.99533562429912}]}
{"instance_id": "bea3772ed532", "generation_result": [{"code": "\ndef apply_hypernetworks(hypernetworks, context, layer=None)\n\t\n    context_k, context_v = context, context\n\n    for hypernetwork in hypernetworks:\n        context_k, context_v = apply_single_hypernetwork(hypernetwork, context_k, context_v, layer=layer)\n\n    return context_k, context_v\n\n\n", "CodeBleu_score": 56.435688315169486}, {"code": "\ndef apply_hypernetworks(hypernetworks, context, layer=None)\n\t\n    context_k = context\n    context_v = context\n\n    for hypernetwork in hypernetworks:\n        context_k, context_v = apply_single_hypernetwork(hypernetwork, context_k, context_v, layer=layer)\n\n    return context_k, context_v\n\n\n", "CodeBleu_score": 63.822641355854024}, {"code": "\ndef apply_hypernetworks(hypernetworks, context, layer=None)\n\t\n\n    context_k = context\n    context_v = context\n    for hypernetwork in hypernetworks:\n        context_k, context_v = apply_single_hypernetwork(hypernetwork, context_k, context_v, layer=layer)\n    return context_k, context_v\n\n\n", "CodeBleu_score": 63.822641355854024}]}
{"instance_id": "922efa3e092f", "generation_result": [{"code": "def attention_CrossAttention_forward(self, x, context=None, mask=None, **kwargs)\n\t\n    if context is None:\n        context = x\n\n    if self.apply_hypernetworks:\n        context = self.apply_hypernetworks(self.hypernetworks, context)\n\n    # Apply the query, key, and value projections to the input and context.\n    query = self.query_projection(x)\n    key = self.key_projection(context)\n    value = self.value_projection(context)\n\n    # Reshape the tensors for multi-head attention.\n    batch_size, num_heads, seq_len, head_dim = self.reshape_for_multihead_attention(query, key, value)\n\n    # Compute the scaled dot-product attention.\n    attention_scores = self.scaled_dot_product_attention(query, key, value, mask, batch_size, num_heads, seq_len, head_dim)\n\n    # Compute the attention output.\n    attention_output = self.compute_attention_output(attention_scores, value, batch_size, num_heads, seq_len, head_dim)\n\n    # Reshape the output.\n    attention_output = self.reshape_output(attention_output, batch_size, seq_len)\n\n    # Apply the final linear layer.\n    attention_output = self.output_projection(attention_output)\n\n    return attention_output\n\n", "CodeBleu_score": 25.792680681533408}, {"code": "def attention_CrossAttention_forward(self, x, context=None, mask=None, **kwargs)\n\t\n    # x: (batch_size, seq_len, input_dim)\n    # context: (batch_size, seq_len, input_dim)\n    # mask: (batch_size, seq_len)\n    if context is None:\n        context = x\n\n    # Project the query, key, and value.\n    # query: (batch_size, seq_len, num_heads * head_dim)\n    # key: (batch_size, seq_len, num_heads * head_dim)\n    # value: (batch_size, seq_len, num_heads * head_dim)\n    query = self.query_net(x)\n    key = self.key_net(context)\n    value = self.value_net(context)\n\n    # Apply hypernetworks to the context.\n    # context_k: (batch_size, seq_len, num_heads * head_dim)\n    # context_v: (batch_size, seq_len, num_heads * head_dim)\n    context_k, context_v = apply_hypernetworks(self.hypernetworks, context, layer=self.layer)\n\n    # Reshape for multi-head attention.\n    # query: (batch_size * num_heads, seq_len, head_dim)\n    # key: (batch_size * num_heads, seq_len, head_dim)\n    # value: (batch_size * num_heads, seq_len, head_dim)\n    # context_k: (batch_size * num_heads, seq_len, head_dim)\n    # context_v: (batch_size * num_heads, seq_len, head_dim)\n    query = query.view(query.shape[0], query.shape[1], self.num_heads, self.head_dim).transpose(1, 2)\n    key = key.view(key.shape[0], key.shape[1], self.num_heads, self.head_dim).transpose(1, 2)\n    value = value.view(value.shape[0], value.shape[1], self.num_heads, self.head_dim).transpose(1, 2)\n    context_k = context_k.view(context_k.shape[0], context_k.shape[1], self.num_heads,", "CodeBleu_score": 28.65808710442359}, {"code": "def attention_CrossAttention_forward(self, x, context=None, mask=None, **kwargs)\n\t\n    # Compute query, key, and value projections\n    if context is None:\n        context = x\n    q, k, v = self.apply_hypernetworks(self.hypernetworks, context)\n\n    # Reshape tensors for multi-head attention\n    q = self.reshape_for_multihead_attention(q)\n    k = self.reshape_for_multihead_attention(k)\n    v = self.reshape_for_multihead_attention(v)\n\n    # Apply mask to attention scores\n    if mask is not None:\n        mask = mask.unsqueeze(1)\n\n    # Apply multi-head attention\n    out = self.multihead_attention(q, k, v, mask=mask, **kwargs)\n\n    # Reshape back to original shape\n    out = self.reshape_back_from_multihead_attention(out)\n\n    # Apply final linear layer\n    out = self.linear_out(out)\n\n    return out\n\n", "CodeBleu_score": 27.218116527443225}]}
{"instance_id": "8b7455a03a0e", "generation_result": [{"code": "def save_hypernetwork(hypernetwork, checkpoint, hypernetwork_name, filename)\n\t\n    original_checkpoint = hypernetwork.checkpoint\n    original_checkpoint_name = hypernetwork.checkpoint_name\n    original_optimizer_name = hypernetwork.optimizer_name\n    original_optimizer_state_dict = hypernetwork.optimizer_state_dict\n    original_step = hypernetwork.step\n\n    hypernetwork.checkpoint = checkpoint\n    hypernetwork.checkpoint_name = hypernetwork_name\n    hypernetwork.optimizer_name = None\n    hypernetwork.optimizer_state_dict = None\n    hypernetwork.step = 0\n\n    try:\n        hypernetwork.save(filename)\n    except Exception as e:\n        hypernetwork.checkpoint = original_checkpoint\n        hypernetwork.checkpoint_name = original_checkpoint_name\n        hypernetwork.optimizer_name = original_optimizer_name\n        hypernetwork.optimizer_state_dict = original_optimizer_state_dict\n        hypernetwork.step = original_step\n\n        raise e\n\n", "CodeBleu_score": 36.807246188134734}, {"code": "def save_hypernetwork(hypernetwork, checkpoint, hypernetwork_name, filename)\n\t\n    if hypernetwork.sd_checkpoint is None:\n        hypernetwork.sd_checkpoint = checkpoint\n        hypernetwork.sd_checkpoint_name = hypernetwork_name\n    try:\n        hypernetwork.save(filename)\n    except Exception as e:\n        hypernetwork.sd_checkpoint = None\n        hypernetwork.sd_checkpoint_name = None\n        raise e\n\n", "CodeBleu_score": 24.687682935066153}, {"code": "def save_hypernetwork(hypernetwork, checkpoint, hypernetwork_name, filename)\n\t\n    original_sd_checkpoint = hypernetwork.sd_checkpoint\n    original_sd_checkpoint_name = hypernetwork.sd_checkpoint_name\n    original_optional_info = hypernetwork.optional_info\n\n    hypernetwork.sd_checkpoint = checkpoint\n    hypernetwork.sd_checkpoint_name = hypernetwork_name\n    hypernetwork.optional_info = None\n\n    try:\n        hypernetwork.save(filename)\n    except Exception as e:\n        hypernetwork.sd_checkpoint = original_sd_checkpoint\n        hypernetwork.sd_checkpoint_name = original_sd_checkpoint_name\n        hypernetwork.optional_info = original_optional_info\n        raise e\n\n    hypernetwork.sd_checkpoint = original_sd_checkpoint\n    hypernetwork.sd_checkpoint_name = original_sd_checkpoint_name\n    hypernetwork.optional_info = original_optional_info\n\n", "CodeBleu_score": 37.427632540601216}]}
{"instance_id": "3175f835140f", "generation_result": [{"code": "def ema_scope(self, context=None)\n\t\n    context = context or self._context\n    if context.ema_enabled:\n        return ema_scope_context(context)\n    else:\n        return no_ema_scope_context(context)\n", "CodeBleu_score": 11.451112695901799}, {"code": "def ema_scope(self, context=None)\n\t\n    context = context if context else get_current_context()\n    return _execute.execute(b\"EmaScope\", 0, inputs=[], attrs=[b\"enable\", enable], ctx=context)\n", "CodeBleu_score": 8.13181677350271}, {"code": "def ema_scope(self, context=None)\n\t\n    _context = context if context else _context\n    _inputs_flat = []\n    _attrs = (\"T\", _op.get_attr(\"T\"), \"decay\",\n              _op.get_attr(\"decay\"), \"num_updates\",\n              _op.get_attr(\"num_updates\"))\n    _execute.record_gradient(\n        \"EmaScope\", _inputs_flat, _attrs, _result)\n    _result, = _result\n    return _result\n\n", "CodeBleu_score": 33.259296816632386}]}
{"instance_id": "a7b5d5cd519a", "generation_result": [{"code": "def p_mean_variance(self, x, t, clip_denoised: bool)\n\t\n    x_recon = self.predict_start_from_noise(x, t, self.denoise_fn(x, t))\n    if clip_denoised:\n        x_recon = torch.clamp(x_recon, -1.0, 1.0)\n    model_mean, posterior_variance, posterior_log_variance = self.q_posterior(x_start=x_recon, x_t=x, t=t)\n    return model_mean, posterior_variance, posterior_log_variance\n", "CodeBleu_score": 36.67940160957738}, {"code": "def p_mean_variance(self, x, t, clip_denoised: bool)\n\t\n    x_recon = self.predict_start_from_noise(x, t=t, noise=self.denoise_fn(x, t=t)) if self.denoise_fn is not None else self.predict_start_from_noise(x, t=t, noise=0)\n    if clip_denoised:\n        x_recon = torch.clamp(x_recon, -1.0, 1.0)\n    model_mean, posterior_variance, posterior_log_variance = self.q_posterior(x_start=x_recon, x_t=x, t=t)\n    return model_mean, posterior_variance, posterior_log_variance\n\n", "CodeBleu_score": 49.36019083031626}, {"code": "def p_mean_variance(self, x, t, clip_denoised: bool)\n\t\n    x_recon = self.predict_start_from_noise(x, t, self.denoise_fn(x, t))\n    if clip_denoised:\n        x_recon = torch.clamp(x_recon, -1.0, 1.0)\n    model_mean, posterior_variance, posterior_log_variance = self.q_posterior(x_start=x_recon, x_t=x, t=t)\n    return model_mean, posterior_variance, posterior_log_variance\n\n", "CodeBleu_score": 36.67940160957738}]}
{"instance_id": "a8652fa78473", "generation_result": [{"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss_type == 'l1':\n        loss = torch.abs(pred - target)\n        if mean:\n            loss = torch.mean(loss)\n    elif self.loss_type == 'l2':\n        loss = torch.pow(pred - target, 2)\n        if mean:\n            loss = torch.mean(loss)\n    else:\n        raise ValueError(f'Unknown loss type: {self.loss_type}')\n    return loss\n\n", "CodeBleu_score": 45.846983402382904}, {"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss_type == 'l1':\n        loss = torch.abs(pred - target)\n        if mean:\n            loss = torch.mean(loss)\n    elif self.loss_type == 'l2':\n        loss = torch.pow(pred - target, 2)\n        if mean:\n            loss = torch.mean(loss)\n    else:\n        raise ValueError('Unknown loss type')\n    return loss\n\n", "CodeBleu_score": 47.20120379693241}, {"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss_type == 'l1':\n        loss = torch.abs(pred - target)\n        if mean:\n            loss = torch.mean(loss)\n    elif self.loss_type == 'l2':\n        loss = torch.pow(pred - target, 2)\n        if mean:\n            loss = torch.mean(loss)\n    else:\n        raise ValueError(f'Unknown loss type {self.loss_type}')\n    return loss\n", "CodeBleu_score": 46.02902109631335}]}
{"instance_id": "d68e4f061349", "generation_result": [{"code": "def _prior_bpd(self, x_start)\n\t\n    batch_size = x_start.shape[0]\n    t = torch.full((batch_size,), self.num_timesteps - 1, device=x_start.device, dtype=torch.long)\n    return self.kl_divergence(x_start, t).sum() / (math.log(2) * batch_size)\n\n", "CodeBleu_score": 22.511917911960712}, {"code": "def _prior_bpd(self, x_start)\n\t\n    batch_size = x_start.shape[0]\n    t = torch.full((batch_size,), self.num_timesteps - 1, device=x_start.device, dtype=torch.long)\n    mean, variance, log_variance = self.q_mean_variance(x_start=x_start, t=t)\n    kl = -0.5 * torch.sum(1 + log_variance - mean.pow(2) - variance)\n    return kl / math.log(2.0)\n\n", "CodeBleu_score": 34.33621828231938}, {"code": "def _prior_bpd(self, x_start)\n\t\n    batch_size = x_start.shape[0]\n    t = torch.full((batch_size,), self.num_timesteps - 1, device=x_start.device, dtype=torch.long)\n    mean, variance, log_variance = self.q_mean_variance(x_start=x_start, t=t)\n    return torch.sum(self.normal_kl(mean, log_variance, variance)) / (math.log(2) * batch_size)\n\n", "CodeBleu_score": 31.81413300273287}]}
{"instance_id": "c695d93fdf2d", "generation_result": [{"code": "def make_cache(subsection: str) -> diskcache.Cache\n\t\n    cache_dir = os.path.join(CACHE_DIR, subsection)\n    cache = diskcache.Cache(cache_dir, size_limit=4_000_000_000, disk=diskcache.JSONDisk)\n    cache.set_eviction_policy(\"least-recently-stored\")\n    return cache\n\n", "CodeBleu_score": 45.07987800658507}, {"code": "def make_cache(subsection: str) -> diskcache.Cache\n\t\n    cache_dir = os.path.join(CACHE_DIR, subsection)\n    return diskcache.Cache(cache_dir, size_limit=4 * 1024 ** 3, disk_min_file_size=256)\n\n", "CodeBleu_score": 48.633067929123996}, {"code": "def make_cache(subsection: str) -> diskcache.Cache\n\t\n    cache_dir = os.path.join(CACHE_DIR, subsection)\n    cache = diskcache.Cache(cache_dir, size_limit=4 * 1024 ** 3, disk_min_file_size=256)\n    return cache\n\n", "CodeBleu_score": 49.468367346176585}]}
{"instance_id": "ed4c543f0d38", "generation_result": [{"code": "def cache(subsection)\n\t\n    with cache_lock:\n        if subsection not in caches:\n            if not os.path.exists(cache_dir):\n                os.makedirs(cache_dir)\n            if not os.path.exists(cache_filename):\n                convert_old_cached_data()\n            if subsection not in caches:\n                caches[subsection] = make_cache(subsection)\n        return caches[subsection]\n\n", "CodeBleu_score": 43.367708861453416}, {"code": "def cache(subsection)\n\t\n    global caches\n    global cache_lock\n    with cache_lock:\n        if subsection not in caches:\n            if not os.path.isdir(cache_dir):\n                os.makedirs(cache_dir)\n            if not os.path.isfile(cache_filename):\n                convert_old_cached_data()\n            if subsection not in caches:\n                caches[subsection] = make_cache(subsection)\n        return caches[subsection]\n\n", "CodeBleu_score": 44.791499218298725}, {"code": "def cache(subsection)\n\t\n    global caches\n    global cache_lock\n    global cache_dir\n    global cache_filename\n    global convert_old_cached_data\n\n    with cache_lock:\n        if subsection not in caches:\n            if not os.path.isdir(cache_dir):\n                os.makedirs(cache_dir)\n\n            if not os.path.isfile(cache_filename):\n                convert_old_cached_data()\n\n            if subsection not in caches:\n                caches[subsection] = make_cache(subsection)\n\n        return caches[subsection]\n\n", "CodeBleu_score": 42.27061420666705}]}
{"instance_id": "ccf5ea2d9fc1", "generation_result": [{"code": "def cached_data_for_file(subsection, title, filename, func)\n\t\n\n    cache_obj = cache(subsection)\n    mtime = os.path.getmtime(filename)\n    cache_data = cache_obj.get(title)\n    if cache_data and mtime <= cache_data[\"mtime\"]:\n        return cache_data[\"data\"]\n    else:\n        data = func()\n        cache_obj.set(title, {\"data\": data, \"mtime\": mtime})\n        dump_cache()\n        return data\n\n", "CodeBleu_score": 19.684382761938686}, {"code": "def cached_data_for_file(subsection, title, filename, func)\n\t\n\n    cache_obj = cache(subsection)\n    mtime = os.path.getmtime(filename)\n    if title in cache_obj:\n        entry = cache_obj[title]\n        if entry['mtime'] >= mtime:\n            return entry['data']\n\n    data = func()\n    cache_obj[title] = {'mtime': mtime, 'data': data}\n    dump_cache()\n    return data\n\n", "CodeBleu_score": 25.597265794426498}, {"code": "def cached_data_for_file(subsection, title, filename, func)\n\t\n\n    cache_obj = cache(subsection)\n    mtime = os.path.getmtime(filename)\n    data = cache_obj.get(title, None)\n    if data:\n        if mtime > data['mtime']:\n            cache_obj.delete(title)\n            data = None\n\n    if not data:\n        data = func()\n        cache_obj.set(title, {'data': data, 'mtime': mtime})\n        dump_cache()\n\n    return data['data']\n\n", "CodeBleu_score": 30.96094503345474}]}
{"instance_id": "e01a2d4f0ac2", "generation_result": [{"code": "def report(message: str, *, exc_info: bool = False) -> None\n\t\n    record_exception()\n    print(\"***\", message, file=sys.stderr)\n    if exc_info:\n        print(\"---\", file=sys.stderr)\n        for line in exception_records[-1].splitlines():\n            print(\"    \" + line, file=sys.stderr)\n\n", "CodeBleu_score": 50.71599309108053}, {"code": "def report(message: str, *, exc_info: bool = False) -> None\n\t\n    record_exception()\n    print(\"***\", message, file=sys.stderr)\n    if exc_info:\n        print(\"---\", file=sys.stderr)\n        for line in exception_records[-1].splitlines():\n            print(\"    \" + line, file=sys.stderr)\n\n", "CodeBleu_score": 50.71599309108053}, {"code": "def report(message: str, *, exc_info: bool = False) -> None\n\t\n    record_exception()\n    print(\"***\", file=sys.stderr)\n    print(message, file=sys.stderr)\n    if exc_info:\n        print(\"---\", file=sys.stderr)\n        for line in exception_records[-1].splitlines():\n            print(\"    \" + line, file=sys.stderr)\n\n", "CodeBleu_score": 47.04793774064488}]}
{"instance_id": "125fb9d2320b", "generation_result": [{"code": "def extract_original_prompts(style: PromptStyle, prompt, negative_prompt)\n\t\n\n    if style.positive_prompt == \"\" and style.negative_prompt == \"\":\n        return False, prompt, negative_prompt\n\n    if style.positive_prompt != \"\":\n        extracted_positive_prompt = extract_style_text_from_prompt(style.positive_prompt, prompt)\n        if extracted_positive_prompt[0] == False:\n            return False, prompt, negative_prompt\n        prompt = extracted_positive_prompt[1]\n\n    if style.negative_prompt != \"\":\n        extracted_negative_prompt = extract_style_text_from_prompt(style.negative_prompt, negative_prompt)\n        if extracted_negative_prompt[0] == False:\n            return False, prompt, negative_prompt\n        negative_prompt = extracted_negative_prompt[1]\n\n    return True, prompt, negative_prompt", "CodeBleu_score": 54.64218190305701}, {"code": "def extract_original_prompts(style: PromptStyle, prompt, negative_prompt)\n\t\n\n    if style.positive_prompt == \"\" and style.negative_prompt == \"\":\n        return False, prompt, negative_prompt\n\n    if style.positive_prompt != \"\":\n        success, prompt = extract_style_text_from_prompt(style.positive_prompt, prompt)\n        if not success:\n            return False, prompt, negative_prompt\n\n    if style.negative_prompt != \"\":\n        success, negative_prompt = extract_style_text_from_prompt(style.negative_prompt, negative_prompt)\n        if not success:\n            return False, prompt, negative_prompt\n\n    return True, prompt, negative_prompt", "CodeBleu_score": 64.47792138264063}, {"code": "def extract_original_prompts(style: PromptStyle, prompt, negative_prompt)\n\t\n\n    if style.prompt == \"\" and style.negative_prompt == \"\":\n        return False, prompt, negative_prompt\n\n    if style.prompt != \"\":\n        success, prompt = extract_style_text_from_prompt(style.prompt, prompt)\n        if not success:\n            return False, prompt, negative_prompt\n\n    if style.negative_prompt != \"\":\n        success, negative_prompt = extract_style_text_from_prompt(style.negative_prompt, negative_prompt)\n        if not success:\n            return False, prompt, negative_prompt\n\n    return True, prompt, negative_prompt", "CodeBleu_score": 66.59386943845635}]}
{"instance_id": "0c5f729fbce2", "generation_result": [{"code": "def random_partial(self, count, n_frames)\n\t\n    if self.utterances is None:\n        self._load_utterances()\n\n    utterances = self.utterance_cycler.sample(count)\n\n    a = [(u,) + u.random_partial(n_frames) for u in utterances]\n\n    return a\n", "CodeBleu_score": 94.20475781398312}, {"code": "def random_partial(self, count, n_frames)\n\t\n    if self.utterances is None:\n        self._load_utterances()\n\n    utterances = self.utterance_cycler.sample(count)\n\n    a = [(u,) + u.random_partial(n_frames) for u in utterances]\n\n    return a\n\n", "CodeBleu_score": 94.20475781398312}, {"code": "def random_partial(self, count, n_frames)\n\t\n    if self.utterances is None:\n        self._load_utterances()\n\n    utterances = self.utterance_cycler.sample(count)\n\n    a = [(u,) + u.random_partial(n_frames) for u in utterances]\n\n    return a\n", "CodeBleu_score": 94.20475781398312}]}
{"instance_id": "31e8b27b8f94", "generation_result": [{"code": "def normalize_volume(wav, target_dBFS, increase_only=False, decrease_only=False)\n\t\n    if increase_only and decrease_only:\n        raise ValueError(\"Both increase only and decrease only are set\")\n    dBFS_change = target_dBFS - wav_dBFS(wav)\n    if (dBFS_change < 0 and increase_only) or (dBFS_change > 0 and decrease_only):\n        return wav\n    return wav * (10 ** (dBFS_change / 20))\n\n", "CodeBleu_score": 70.94666423617527}, {"code": "def normalize_volume(wav, target_dBFS, increase_only=False, decrease_only=False)\n\t\n    if increase_only and decrease_only:\n        raise ValueError(\"Both increase only and decrease only are set\")\n    dBFS_change = target_dBFS - 10 * np.log10(np.mean(wav ** 2))\n    if (dBFS_change < 0 and increase_only) or (dBFS_change > 0 and decrease_only):\n        return wav\n    return wav * (10 ** (dBFS_change / 20))\n\n", "CodeBleu_score": 96.28790819755086}, {"code": "def normalize_volume(wav, target_dBFS, increase_only=False, decrease_only=False)\n\t\n    if increase_only and decrease_only:\n        raise ValueError(\"Both increase only and decrease only are set\")\n    dBFS_change = target_dBFS - 10 * np.log10(np.mean(wav ** 2))\n    if (dBFS_change < 0 and increase_only) or (dBFS_change > 0 and decrease_only):\n        return wav\n    return wav * (10 ** (dBFS_change / 20))\n\n", "CodeBleu_score": 96.28790819755086}]}
{"instance_id": "2353831e9033", "generation_result": [{"code": "def load_model(weights_fpath: Path, device=None)\n\t\n    global _model\n\n    if device is None:\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    _model = SpeakerEncoder(device=device)\n    _model.load_state_dict(torch.load(weights_fpath))\n    _model.eval()\n    print(f\"Loaded model: {weights_fpath} trained to step {get_step(_model)}.\")\n\n", "CodeBleu_score": 42.02031730655098}, {"code": "def load_model(weights_fpath: Path, device=None)\n\t\n    global _model\n    device = device or \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    _model = SpeakerEncoder(device=device)\n    _model.load_state_dict(torch.load(weights_fpath))\n    _model.eval()\n    print(f\"Loaded model from step {_model.step} on {device}.\")\n\n", "CodeBleu_score": 26.23235831804298}, {"code": "def load_model(weights_fpath: Path, device=None)\n\t\n    global _model\n    if device is None:\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    checkpoint = torch.load(weights_fpath, map_location=device)\n    _model = SpeakerEncoder(**checkpoint[\"model_kwargs\"])\n    _model.load_state_dict(checkpoint[\"state_dict\"])\n    _model.to(device)\n    _model.eval()\n    print(\"Loaded model: step {}, loss {}\".format(checkpoint[\"step\"], checkpoint[\"loss\"]))\n\n", "CodeBleu_score": 53.780641643255436}]}
{"instance_id": "3231ad480199", "generation_result": [{"code": "def embed_frames_batch(frames_batch)\n\t\n    if _model is None:\n        raise Exception(\"Model not loaded! Load the model by calling the function load_model() first.\")\n\n    # Convert the NumPy array to a PyTorch tensor\n    frames_batch_tensor = torch.from_numpy(frames_batch).to(_device)\n\n    # Transfer the tensor to the GPU if one is used, else to the CPU\n    frames_batch_tensor = frames_batch_tensor.to(_device)\n\n    # Embed the utterance\n    with torch.no_grad():\n        embeds = _model.forward(frames_batch_tensor)\n\n    # Detach the result from the computation graph\n    embeds = embeds.detach().cpu().numpy()\n\n    return embeds\n\n", "CodeBleu_score": 59.009890887964744}, {"code": "def embed_frames_batch(frames_batch)\n\t\n    if _model is None:\n        raise Exception('Model is not loaded!')\n    frames_batch = torch.tensor(frames_batch, device=_device).unsqueeze(0)\n    with torch.no_grad():\n        embeds = _model.forward(frames_batch).detach().cpu().numpy()[0]\n    return embeds\n\n", "CodeBleu_score": 61.64261684471588}, {"code": "def embed_frames_batch(frames_batch)\n\t\n    if _model is None:\n        raise Exception(\"The model is not loaded. Please load the model by calling the load_model() function.\")\n    frames_batch = torch.tensor(frames_batch).to(_device)\n    with torch.no_grad():\n        embeds = _model.forward(frames_batch).cpu().numpy()\n    return embeds\n", "CodeBleu_score": 59.83720350995645}]}
{"instance_id": "117a41716c8d", "generation_result": [{"code": "def compute_partial_slices(n_samples, partial_utterance_n_frames=partials_n_frames,\n                           min_pad_coverage=0.75, overlap=0.5)\n\t\n    assert 0 <= overlap < 1, \"overlap must be in the range [0, 1)\"\n    assert 0 < min_pad_coverage <= 1, \"min_pad_coverage must be in the range (0, 1]\"\n    frame_step = int(n_samples * (1 - overlap))\n    assert frame_step > 0, \"overlap must be smaller than 1 - duration of a single frame\"\n    n_frames = int(np.ceil(float(n_samples - partial_utterance_n_frames + frame_step) / frame_step))\n    assert n_frames > 0, \"audio file must be longer than the size of a partial utterance\"\n    # Compute mel slice indices, there will be (n_frames + 1) indices\n    mel_start = [int(i * frame_step) for i in range(n_frames + 1)]\n    # Compute corresponding wav slice indices, there will be n_frames indices\n    wav_start = [int(i * partial_utterance_n_frames) for i in range(n_frames)]\n    # Compute coverage for each partial utterance\n    coverage = [min(float(mel_start[i + 1] - mel_start[i]) / partial_utterance_n_frames, 1.0) for i in range(n_frames)]\n    # Compute the number of padding frames at the end\n    n_pad = [int(max(0, partial_utterance_n_frames - (wav_start[i + 1] - wav_start[i]))) for i in range(n_frames - 1)]\n    # Compute mel lengths, add one for the last frame\n    mel_lengths = [mel_start[i + 1] - mel_start[i] for i in range(n_frames)]\n    mel_lengths = [mel_lengths[i] + n_pad[i] for i in range(n_frames)]\n    # Compute wav lengths, add one for the last frame\n    wav_lengths = [wav_start[i + 1] - wav_start[i] for i in range(n_frames)]\n    wav_lengths = [wav_lengths[i] + n_pad[i] for i in range(n_frames)]\n    # Adjust wav_start and wav_lengths\n   ", "CodeBleu_score": 32.520193602556404}, {"code": "def compute_partial_slices(n_samples, partial_utterance_n_frames=partials_n_frames,\n                           min_pad_coverage=0.75, overlap=0.5)\n\t\n\n    assert 0.0 <= min_pad_coverage <= 1.0, \"min_pad_coverage must be between 0 and 1\"\n    assert 0.0 <= overlap < 1.0, \"overlap must be between 0 and 1\"\n    assert partial_utterance_n_frames > 0, \"partial_utterance_n_frames must be greater than 0\"\n\n    frame_step = int(np.ceil(n_samples * (1.0 - overlap)))  # Frame step\n    assert frame_step > 0, \"overlap is too big, there is only 1 frame\"\n    n_frames = int(np.ceil(\n        float(n_samples - partial_utterance_n_frames) / frame_step)) + 1  # Number of frames\n\n    # Compute mel slices\n    mel_slices = []\n    for i in range(n_frames):\n        mel_start = int(i * frame_step)\n        mel_end = min(mel_start + partial_utterance_n_frames, n_samples)\n        mel_slices.append([mel_start, mel_end])\n\n    # Compute wav slices\n    wav_slices = []\n    for i in range(len(mel_slices)):\n        wav_start = mel_slices[i][0]\n        wav_end = mel_slices[i][1]\n        wav_slices.append([wav_start, wav_end])\n\n    # Evaluate padding\n    last_wav_slice = wav_slices[-1]\n    coverage = (last_wav_slice[1] - last_wav_slice[0]) / float(n_samples)\n    if coverage < min_pad_coverage and len(wav_slices) > 1:\n        del wav_slices[-1]\n        del mel_slices[-1]\n\n    return wav_slices, mel_slices\n\n", "CodeBleu_score": 34.569640735866884}, {"code": "def compute_partial_slices(n_samples, partial_utterance_n_frames=partials_n_frames,\n                           min_pad_coverage=0.75, overlap=0.5)\n\t\n    assert 0 <= min_pad_coverage <= 1, \"min_pad_coverage should be between 0 and 1\"\n    assert 0 < overlap < 1, \"overlap should be between 0 and 1\"\n\n    # Compute the number of frames, and the frame step\n    n_frames = int(np.ceil(n_samples / sampling_rate * partial_utterance_n_frames))\n    frame_step = int(np.ceil(n_frames * (1. - overlap)))\n\n    # Compute the number of slices\n    n_slices = int(np.ceil((n_frames - partial_utterance_n_frames) / frame_step))\n\n    # Compute partial wav and mel slices\n    wav_slices = []\n    mel_slices = []\n    for i in range(n_slices):\n        mel_range = np.array([i * frame_step, (i + 1) * frame_step])\n        wav_range = mel_range * (sampling_rate / partial_utterance_n_frames)\n        mel_slices.append(slice(*mel_range))\n        wav_slices.append(slice(*wav_range))\n\n    # Evaluate coverage\n    wav_coverage = (wav_slices[-1].stop - wav_slices[-1].start) / n_samples\n    if wav_coverage < min_pad_coverage and len(mel_slices) > 1:\n        mel_slices = mel_slices[:-1]\n        wav_slices = wav_slices[:-1]\n\n    # Return slices\n    return wav_slices, mel_slices\n\n", "CodeBleu_score": 48.63689347536591}]}
{"instance_id": "34fe5c864f1c", "generation_result": [{"code": "def plot_embedding_as_heatmap(embed, ax=None, title=\"\", shape=None, color_range=(0, 0.30))\n\t\n    if ax is None:\n        ax = plt.gca()\n    if shape is None:\n        height = int(np.ceil(np.sqrt(len(embed))))\n        shape = (height, height)\n    embed = embed.reshape(shape)\n    ax.imshow(embed, cmap=plt.cm.Blues, interpolation=\"nearest\")\n    ax.set_title(title)\n    ax.set_xticks([])\n    ax.set_yticks([])\n    plt.colorbar(fraction=0.046, pad=0.04, ticks=np.linspace(color_range[0], color_range[1], 10))\n\n", "CodeBleu_score": 51.53008428950645}, {"code": "def plot_embedding_as_heatmap(embed, ax=None, title=\"\", shape=None, color_range=(0, 0.30))\n\t\n    if ax is None:\n        ax = plt.gca()\n    if shape is None:\n        height = int(np.sqrt(embed.shape[0]))\n        shape = (height, height)\n    embed = embed.reshape(shape)\n    ax.imshow(embed, cmap=\"Reds\", vmin=color_range[0], vmax=color_range[1])\n    ax.set_title(title)\n    ax.set_yticks([])\n    ax.set_xticks([])\n    plt.colorbar(fraction=0.046, pad=0.04)\n\n", "CodeBleu_score": 50.17411262376862}, {"code": "def plot_embedding_as_heatmap(embed, ax=None, title=\"\", shape=None, color_range=(0, 0.30))\n\t\n    if ax is None:\n        ax = plt.gca()\n\n    if shape is None:\n        height = int(np.sqrt(len(embed)))\n        shape = (height, height)\n\n    embed = embed.reshape(shape)\n    im = ax.imshow(embed, cmap=\"Blues\")\n    cbar = ax.figure.colorbar(im, ax=ax)\n    cbar.ax.set_ylabel(\"Similarity\", rotation=-90, va=\"bottom\")\n    cbar.set_ticks(np.linspace(color_range[0], color_range[1], 10))\n    cbar.set_ticklabels([\"{:.2f}\".format(i) for i in np.linspace(color_range[0], color_range[1], 10)])\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.set_title(title)\n    plt.show()\n\n", "CodeBleu_score": 62.224504258388656}]}
{"instance_id": "92e93d50b62b", "generation_result": [{"code": "def similarity_matrix(self, embeds)\n\t\n    # Get batch size and number of utterances per speaker\n    batch_size, num_utterances = embeds.shape[:2]\n\n    # Initialize similarity matrix\n    sim_mat = torch.zeros((batch_size, batch_size), device=embeds.device)\n    mask = torch.ones((batch_size, batch_size), device=embeds.device)\n\n    # Calculate centroids\n    centroids_incl = torch.mean(embeds, dim=1)\n    centroids_incl = F.normalize(centroids_incl, dim=1)\n\n    centroids_excl = torch.mean(embeds, dim=1)\n    centroids_excl = F.normalize(centroids_excl, dim=1)\n\n    # Calculate similarity scores\n    sim_mat = torch.matmul(embeds, centroids_incl.T)\n\n    # Update similarity matrix\n    sim_mat = sim_mat * mask\n\n    # Scale and bias similarity matrix\n    sim_mat = sim_mat * self.scale + self.bias\n\n    return sim_mat\n\n", "CodeBleu_score": 21.251376416721847}, {"code": "def similarity_matrix(self, embeds)\n\t\n    # Determine the number of speakers per batch and the number of utterances per speaker from the shape of the input embeddings.\n    n_speakers = embeds.shape[0]\n    n_utterances = embeds.shape[1]\n\n    # Initialize a similarity matrix with zeros on the specified device and a mask matrix to exclude self-similarity.\n    sim_mat = torch.zeros(n_speakers, n_speakers, device=self.device)\n    mask_mat = torch.ones(n_speakers, n_speakers, device=self.device) - torch.eye(n_speakers, device=self.device)\n\n    # For each speaker, compute the similarity scores using the respective centroids and update the similarity matrix accordingly.\n    for speaker in range(n_speakers):\n        centroids_incl = torch.mean(embeds[speaker], dim=0)\n        centroids_incl = F.normalize(centroids_incl, p=2, dim=0)\n\n        centroids_excl = torch.mean(embeds[speaker][:, 1:], dim=0)\n        centroids_excl = F.normalize(centroids_excl, p=2, dim=0)\n\n        sim_mat[speaker] = torch.matmul(embeds[speaker], centroids_incl.unsqueeze(1))\n        sim_mat[speaker] += torch.matmul(embeds[speaker], centroids_excl.unsqueeze(1))\n        sim_mat[speaker] /= n_utterances\n\n    # Scale and bias the similarity matrix before returning it.\n    sim_mat = sim_mat * self.scale + self.bias\n    sim_mat = sim_mat * mask_mat\n\n    return sim_mat\n\n", "CodeBleu_score": 31.315128204096986}, {"code": "def similarity_matrix(self, embeds)\n\t\n    # Get the number of speakers per batch and utterances per speaker\n    num_speakers = embeds.shape[0]\n    num_utterances = embeds.shape[1]\n\n    # Initialize a similarity matrix on the specified device\n    similarity_matrix = torch.zeros(num_speakers, num_speakers, device=self.device)\n\n    # Initialize a mask matrix to exclude self-similarity\n    mask_matrix = torch.ones(num_speakers, num_speakers, device=self.device)\n    mask_matrix = torch.tril(mask_matrix)\n\n    # Compute the centroids\n    centroids_incl = torch.mean(embeds, dim=1)\n    centroids_incl = F.normalize(centroids_incl, p=2, dim=1)\n    centroids_excl = torch.mean(embeds, dim=1)\n    centroids_excl = F.normalize(centroids_excl, p=2, dim=1)\n\n    # Compute the similarity scores\n    for speaker in range(num_speakers):\n        for utterance in range(num_utterances):\n            similarity_matrix[speaker] += torch.cosine_similarity(embeds[speaker, utterance].unsqueeze(0), centroids_incl, dim=1)\n            similarity_matrix[speaker] -= torch.cosine_similarity(embeds[speaker, utterance].unsqueeze(0), centroids_excl, dim=1)\n\n    # Scale and bias the similarity matrix\n    similarity_matrix = similarity_matrix / num_utterances\n    similarity_matrix = similarity_matrix * self.scale\n    similarity_matrix = similarity_matrix + self.bias\n\n    # Apply the mask matrix\n    similarity_matrix = similarity_matrix * mask_matrix\n\n    return similarity_matrix\n\n", "CodeBleu_score": 27.928814647685403}]}
{"instance_id": "a6eba54108a1", "generation_result": [{"code": "def loss(self, embeds)\n\t\n    speakers_per_batch, utterances_per_speaker = embeds.shape[:2]\n    sim_matrix = self.similarity_matrix(embeds)\n    sim_matrix = sim_matrix.view(-1, speakers_per_batch)\n\n    labels = np.repeat(range(speakers_per_batch), utterances_per_speaker)\n    labels = torch.LongTensor(labels).to(self.loss_device)\n\n    loss = F.cross_entropy(sim_matrix, labels)\n    with torch.no_grad():\n        _, predicted_labels = torch.max(sim_matrix, 1)\n        eer = self.calculate_eer(predicted_labels.cpu().numpy(), labels.cpu().numpy())\n    return loss, eer\n\n", "CodeBleu_score": 36.875570689698925}, {"code": "def loss(self, embeds)\n\t\n    speakers_per_batch, utterances_per_speaker = embeds.shape[:2]\n    sim_matrix = self.similarity_matrix(embeds)\n    sim_matrix = sim_matrix.view(-1, speakers_per_batch)\n\n    labels = torch.tensor([i // utterances_per_speaker for i in range(utterances_per_speaker * speakers_per_batch)]).to(self.loss_device)\n    loss_fn = nn.CrossEntropyLoss()\n    loss = loss_fn(sim_matrix, labels)\n    with torch.no_grad():\n        _, predicted_labels = torch.max(sim_matrix, 1)\n        eer = compute_eer(predicted_labels.cpu().numpy(), labels.cpu().numpy())[0]\n    return loss, eer\n", "CodeBleu_score": 37.97852096671209}, {"code": "def loss(self, embeds)\n\t\n    speakers_per_batch, utterances_per_speaker = embeds.shape[:2]\n    sim_matrix = self.similarity_matrix(embeds)\n    sim_matrix = sim_matrix.view(-1, speakers_per_batch)\n\n    labels = torch.tensor(np.repeat(range(speakers_per_batch), utterances_per_speaker),\n                          dtype=torch.long).to(self.loss_device)\n    loss = self.loss_fn(sim_matrix, labels)\n\n    with torch.no_grad():\n        _, preds = torch.max(sim_matrix, dim=1)\n        eer = (self.fpr(preds, labels) + self.fnr(preds, labels)) / 2\n    return loss, eer\n\n", "CodeBleu_score": 37.26419733569196}]}
{"instance_id": "ac7b74b9d1b1", "generation_result": [{"code": "def log_params(self)\n\t\n    if self.verbose:\n        self.params_log = f\"Model Parameters:", "CodeBleu_score": 3.295862582677788}, {"code": "def log_params(self)\n\t\n    if not self.disable_logging:\n        from .logging import log_params\n        log_params(self)\n\n", "CodeBleu_score": 6.180169509256279}, {"code": "def log_params(self)\n\t\n    if self.log_params:\n        param_string = \"Model Parameters:", "CodeBleu_score": 0.5043566304162515}]}
{"instance_id": "4726b7864386", "generation_result": [{"code": "def draw_projections(self, embeds, utterances_per_speaker, step, out_fpath=None, max_speakers=10)\n\t\n    if self.verbose:\n        print('Drawing projections...')\n    if out_fpath is not None:\n        if self.verbose:\n            print('Saving projections to', out_fpath)\n    if self.verbose:\n        print('Drawing projections...')\n    if out_fpath is not None:\n        if self.verbose:\n            print('Saving projections to', out_fpath)\n    if self.verbose:\n        print('Drawing projections...')\n    if out_fpath is not None:\n        if self.verbose:\n            print('Saving projections to', out_fpath)\n    if self.verbose:\n        print('Drawing projections...')\n    if out_fpath is not None:\n        if self.verbose:\n            print('Saving projections to', out_fpath)\n    if self.verbose:\n        print('Drawing projections...')\n    if out_fpath is not None:\n        if self.verbose:\n            print('Saving projections to', out_fpath)\n    if self.verbose:\n        print('Drawing projections...')\n    if out_fpath is not None:\n        if self.verbose:\n            print('Saving projections to', out_fpath)\n    if self.verbose:\n        print('Drawing projections...')\n    if out_fpath is not None:\n        if self.verbose:\n            print('Saving projections to', out_fpath)\n    if self.verbose:\n        print('Drawing projections...')\n    if out_fpath is not None:\n        if self.verbose:\n            print('Saving projections to', out_fpath)\n    if self.verbose:\n        print('Drawing projections...')\n    if out_fpath is not None:\n        if self.verbose:\n            print('Saving projections to', out_fpath)\n    if self.verbose:\n        print('Drawing projections...')\n    if out_fpath is not None:\n        if self.verbose:\n            print('Saving projections to', out_fpath)\n    if self.verbose:\n        print('Drawing projections...')\n    if out_fpath is not None:\n        if self.verbose:\n            print('Saving projections to', out_fpath)\n    if self.verbose:\n        print('Drawing projections...')\n    if out_fpath is not None:\n        if self.verbose:\n            print('Saving projections to', out_fpath)\n    if self.verbose:\n        print", "CodeBleu_score": 14.505517738262375}, {"code": "def draw_projections(self, embeds, utterances_per_speaker, step, out_fpath=None, max_speakers=10)\n\t\n    # Get the number of speakers to visualize\n    num_speakers = min(max_speakers, len(embeds))\n    # Get the utterances per speaker\n    utterances_per_speaker = min(utterances_per_speaker, len(embeds[0]))\n    # Flatten the embeddings\n    embeds = [embed.flatten() for embed in embeds]\n    # Get the speaker IDs\n    speaker_ids = [i for i in range(len(embeds))]\n    # Get the colors\n    colors = [self.speaker_id_to_color[i] for i in speaker_ids]\n    # Create the UMAP object\n    umap_obj = umap.UMAP(n_neighbors=15, min_dist=0.1, metric=\"cosine\")\n    # Embed the speakers\n    embeds = umap_obj.fit_transform(embeds)\n    # Get the embedding dimensions\n    embed_dim = len(embeds[0])\n    # Create the figure\n    fig = plt.figure(figsize=(10, 10))\n    # Create the axes\n    ax = fig.add_subplot(1, 1, 1)\n    # Draw the points\n    ax.scatter(embeds[:, 0], embeds[:, 1], c=colors, s=10, alpha=0.5)\n    # Set the title\n    ax.set_title(f\"Speaker projections at step {step}\")\n    # Set the axes labels\n    ax.set_xlabel(\"UMAP dim 1\")\n    ax.set_ylabel(\"UMAP dim 2\")\n    # Set the axes limits\n    ax.set_xlim(embeds[:, 0].min() - 0.1, embeds[:, 0].max() + 0.1)\n    ax.set_ylim(embeds[:, 1].min() - 0.1, embeds[:, 1].max() + 0.1)\n    # Set the axes aspect ratio\n    ax.set_aspect(\"equal\")\n    # Set the ticks\n    ax.set_xticks([])\n    ax.set_yticks([])\n    # Draw the utterances\n    for i in range(num_speakers):\n        # Get the utterance IDs\n        utterance_ids = np.random.choice(\n            len(embeds[i]), utterances_per_speaker, replace=False\n       ", "CodeBleu_score": 30.109089776523845}, {"code": "def draw_projections(self, embeds, utterances_per_speaker, step, out_fpath=None, max_speakers=10)\n\t\n    if out_fpath is not None:\n        out_fpath = Path(out_fpath)\n        out_fpath.parent.mkdir(parents=True, exist_ok=True)\n    if step == 0:\n        self.logger.info(\"Drawing projections...\")\n    if not self.config.get(\"visualization\", {}).get(\"project_embeddings\", False):\n        self.logger.info(\"Skipping projections...\")\n        return\n    if embeds is None:\n        self.logger.info(\"No embeddings to project.\")\n        return\n    embeds = embeds.cpu().numpy()\n    embeds = embeds.reshape(embeds.shape[0], -1)\n    embeds = embeds[:utterances_per_speaker * max_speakers]\n    speakers = np.repeat(np.arange(max_speakers), utterances_per_speaker)\n    colors = np.array(\n        [\n            self.config[\"data\"][\"speaker_colors\"][i]\n            for i in speakers\n        ]\n    )\n    reducer = umap.UMAP(\n        n_components=2,\n        n_neighbors=20,\n        min_dist=0.0,\n        metric=\"cosine\",\n        random_state=42,\n    )\n    embeds = reducer.fit_transform(embeds)\n    fig, ax = plt.subplots()\n    ax.scatter(embeds[:, 0], embeds[:, 1], c=colors, s=0.1, alpha=0.25)\n    plt.title(f\"Step {step}\")\n    if out_fpath is None:\n        plt.show()\n    else:\n        plt.savefig(out_fpath)\n        plt.close()\n\n", "CodeBleu_score": 45.29596242166415}]}
{"instance_id": "274fa50985c0", "generation_result": [{"code": "def add_speaker_embedding(self, x, speaker_embedding)\n\t\n    # Save the dimensions of the input tensor\n    old_shape = x.shape\n    # Determine the appropriate dimension index for the speaker embedding\n    speaker_embedding_dim_index = len(old_shape) - 1\n    # Repeat the speaker embedding to match the length of the input text\n    repeats = [1 for _ in range(speaker_embedding_dim_index)]\n    repeats.append(old_shape[speaker_embedding_dim_index])\n    speaker_embedding = tf.repeat(speaker_embedding, repeats=repeats)\n    # Reshape and transpose the speaker embedding\n    speaker_embedding = tf.reshape(speaker_embedding, shape=[-1, old_shape[speaker_embedding_dim_index]])\n    speaker_embedding = tf.transpose(speaker_embedding, perm=[1, 0])\n    # Concatenate the speaker embedding with the input tensor\n    x = tf.concat([x, speaker_embedding], axis=len(old_shape) - 1)\n    return x\n\n", "CodeBleu_score": 17.953348450143398}, {"code": "def add_speaker_embedding(self, x, speaker_embedding)\n\t\n    # Save input tensor dimensions\n    original_shape = x.shape\n    # Get speaker embedding dimension index\n    speaker_embedding_dim = self.get_speaker_embedding_dim()\n    # Repeat the speaker embedding to match the length of the input text\n    repeated_speaker_embedding = tf.repeat(speaker_embedding, tf.shape(x)[1], axis=1)\n    # Reshape the speaker embedding to match the input tensor\n    repeated_speaker_embedding = tf.reshape(repeated_speaker_embedding, (tf.shape(x)[0], tf.shape(x)[1], speaker_embedding_dim))\n    # Transpose the speaker embedding to match the input tensor\n    repeated_speaker_embedding = tf.transpose(repeated_speaker_embedding, perm=[0, 2, 1])\n    # Concatenate the speaker embedding with the input tensor\n    x = tf.concat([x, repeated_speaker_embedding], axis=2)\n    # Return the result\n    return x, original_shape\n\n", "CodeBleu_score": 20.442108154338566}, {"code": "def add_speaker_embedding(self, x, speaker_embedding)\n\t\n    input_shape = tf.shape(x)\n    batch_size = input_shape[0]\n    seq_length = input_shape[1]\n    width = input_shape[2]\n\n    # The embedding should be broadcast along the width dimension.\n    # [batch_size, seq_length, 1]\n    speaker_1d = tf.expand_dims(speaker_embedding, 2)\n\n    # [batch_size, seq_length, width]\n    speaker_broadcast = tf.tile(speaker_1d, [1, 1, width])\n\n    # [batch_size, seq_length, width + speaker_embedding_size]\n    appended_speaker = tf.concat([x, speaker_broadcast], axis=2)\n\n    return appended_speaker\n\n", "CodeBleu_score": 13.844095466338846}]}
{"instance_id": "e6742ea0fe5e", "generation_result": [{"code": "def init_attention(self, encoder_seq_proj)\n\t\n    self.device = next(self.parameters()).device\n    self.encoder_seq_proj_shape = encoder_seq_proj.shape\n    self.attention_cum = torch.zeros(self.encoder_seq_proj_shape[0], self.encoder_seq_proj_shape[1]).to(self.device)\n    self.attention = torch.zeros(self.encoder_seq_proj_shape[0], self.encoder_seq_proj_shape[1]).to(self.device)\n\n", "CodeBleu_score": 16.46332809947026}, {"code": "def init_attention(self, encoder_seq_proj)\n\t\n    self.device = next(self.parameters()).device\n    self.cum_att_weights = torch.zeros(\n        (self.batch_size, self.seq_len), device=self.device\n    )\n    self.att_weights = torch.zeros(\n        (self.batch_size, self.seq_len), device=self.device\n    )\n    self.encoder_seq_proj = encoder_seq_proj\n\n", "CodeBleu_score": 24.64584974926968}, {"code": "def init_attention(self, encoder_seq_proj)\n\t\n    self.device = next(self.parameters()).device\n    self.enc_seq_proj_shape = encoder_seq_proj.shape\n    self.cumulative = torch.zeros((self.enc_seq_proj_shape[0], self.enc_seq_proj_shape[1], self.enc_seq_proj_shape[2]), device=self.device)\n    self.attention = torch.zeros((self.enc_seq_proj_shape[0], self.enc_seq_proj_shape[1], self.enc_seq_proj_shape[2]), device=self.device)\n\n", "CodeBleu_score": 21.951473263622397}]}
{"instance_id": "4e1fc1ed0ed7", "generation_result": [{"code": "def load(self, path, optimizer=None)\n\t\n    # Use device of model params as location for loaded state\n    device = next(self.parameters()).device\n    checkpoint = torch.load(str(path), map_location=device)\n    self.load_state_dict(checkpoint[\"model_state\"])\n\n    if \"optimizer_state\" in checkpoint and optimizer is not None:\n        optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n\n", "CodeBleu_score": 67.98839425729693}, {"code": "def load(self, path, optimizer=None)\n\t\n    # Use device of model params as location for loaded state\n    device = next(self.parameters()).device\n    checkpoint = torch.load(str(path), map_location=device)\n    self.load_state_dict(checkpoint[\"model_state\"])\n\n    if \"optimizer_state\" in checkpoint and optimizer is not None:\n        optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n\n", "CodeBleu_score": 67.98839425729693}, {"code": "def load(self, path, optimizer=None)\n\t\n    # Use device of model params as location for loaded state\n    device = next(self.parameters()).device\n    checkpoint = torch.load(str(path), map_location=device)\n    self.load_state_dict(checkpoint[\"model_state\"])\n\n    if \"optimizer_state\" in checkpoint and optimizer is not None:\n        optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n\n", "CodeBleu_score": 67.98839425729693}]}
{"instance_id": "178e6aa16c0f", "generation_result": [{"code": "def save(self, path, optimizer=None)\n\t\n    if optimizer is not None:\n        torch.save({\n            \"model_state\": self.state_dict(),\n            \"optimizer_state\": optimizer.state_dict(),\n        }, str(path))\n    else:\n        torch.save({\n            \"model_state\": self.state_dict(),\n        }, str(path))\n", "CodeBleu_score": 93.4714524643972}, {"code": "def save(self, path, optimizer=None)\n\t\n    if optimizer is not None:\n        torch.save({\n            \"model_state\": self.state_dict(),\n            \"optimizer_state\": optimizer.state_dict(),\n        }, str(path))\n    else:\n        torch.save({\n            \"model_state\": self.state_dict(),\n        }, str(path))\n", "CodeBleu_score": 93.4714524643972}, {"code": "def save(self, path, optimizer=None)\n\t\n    if optimizer is not None:\n        torch.save({\n            \"model_state\": self.state_dict(),\n            \"optimizer_state\": optimizer.state_dict(),\n        }, str(path))\n    else:\n        torch.save({\n            \"model_state\": self.state_dict(),\n        }, str(path))\n", "CodeBleu_score": 93.4714524643972}]}
{"instance_id": "6528fd80e87b", "generation_result": [{"code": "def english_cleaners(text)\n\t\n    text = convert_to_ascii(text)\n    text = lowercase(text)\n    text = expand_numbers(text)\n    text = expand_abbreviations(text)\n    text = collapse_whitespace(text)\n    return text\n", "CodeBleu_score": 55.88611500274403}, {"code": "def english_cleaners(text)\n\t\n    text = convert_to_ascii(text)\n    text = lowercase(text)\n    text = expand_numbers(text)\n    text = expand_abbreviations(text)\n    text = collapse_whitespace(text)\n    return text\n", "CodeBleu_score": 55.88611500274403}, {"code": "def english_cleaners(text)\n\t\n    text = convert_to_ascii(text)\n    text = lowercase(text)\n    text = expand_numbers(text)\n    text = expand_abbreviations(text)\n    text = collapse_whitespace(text)\n    return text\n\n", "CodeBleu_score": 55.88611500274403}]}
{"instance_id": "8a4fe08e0985", "generation_result": [{"code": "def text_to_sequence(text, cleaner_names)\n\t\n    sequence = []\n    # Check for curly braces and treat their contents as ARPAbet:\n    while len(text):\n        m = _curly_re.match(text)\n        if not m:\n            sequence += _symbols_to_sequence(_clean_text(text, cleaner_names))\n            break\n        sequence += _symbols_to_sequence(_clean_text(m.group(1), cleaner_names))\n        sequence += _arpabet_to_sequence(m.group(2))\n        text = m.group(3)\n    # Append EOS token\n    sequence.append(_symbol_to_id[\"~\"])\n    return sequence", "CodeBleu_score": 81.11096495877042}, {"code": "def text_to_sequence(text, cleaner_names)\n\t\n    sequence = []\n    # Check for curly braces and treat their contents as ARPAbet:\n    while len(text):\n        m = _curly_re.match(text)\n        if not m:\n            sequence += _symbols_to_sequence(_clean_text(text, cleaner_names))\n            break\n        sequence += _symbols_to_sequence(_clean_text(m.group(1), cleaner_names))\n        sequence += _arpabet_to_sequence(m.group(2))\n        text = m.group(3)\n    # Append EOS token\n    sequence.append(_symbol_to_id[\"~\"])\n    return sequence\n\n", "CodeBleu_score": 81.11096495877042}, {"code": "def text_to_sequence(text, cleaner_names)\n\t\n    sequence = []\n    if cleaner_names:\n        text = _clean_text(text, cleaner_names)\n    sequence += _symbols_to_sequence(text.split(\" \"))\n    return sequence\n\n", "CodeBleu_score": 15.204688157916685}]}
{"instance_id": "e1c25a37af1f", "generation_result": [{"code": "def sequence_to_text(sequence)\n\t\n    result = ''\n    for symbol_id in sequence:\n        if symbol_id in _symbol_to_id:\n            s = _id_to_symbol[symbol_id]\n            # Enclose ARPAbet back in curly braces:\n            if len(s) > 1 and s[0] == '@':\n                s = '{%s}' % s[1:]\n            result += s\n    return result.replace('}{', ' ')\n\n", "CodeBleu_score": 67.86166028998515}, {"code": "def sequence_to_text(sequence)\n\t\n    result = ''\n    for symbol_id in sequence:\n        if symbol_id in _symbol_to_id:\n            s = _id_to_symbol[symbol_id]\n            # Enclose ARPAbet back in curly braces:\n            if len(s) > 1 and s[0] == '@':\n                s = '{%s}' % s[1:]\n            result += s\n    return result.replace('}{', ' ')\n\n", "CodeBleu_score": 67.86166028998515}, {"code": "def sequence_to_text(sequence)\n\t\n    result = ''\n    for symbol_id in sequence:\n        if symbol_id in _symbol_to_id:\n            symbol = _id_to_symbol[symbol_id]\n            # Enclose ARPAbet back in curly braces:\n            if len(symbol) > 1 and symbol[0] == '@':\n                symbol = '{%s}' % symbol[1:]\n            # Concatenate the symbols:\n            result += symbol\n    return result.replace('}{', ' ')\n\n", "CodeBleu_score": 57.211523569120025}]}
{"instance_id": "7807cad4b05d", "generation_result": [{"code": "def embed_utterance(fpaths, encoder_model_fpath)\n\t\n    if not encoder.is_loaded():\n        encoder.load_model(encoder_model_fpath)\n\n    # Compute the speaker embedding of the utterance\n    wav_fpath, embed_fpath = fpaths\n    wav = np.load(wav_fpath)\n    wav = encoder.preprocess_wav(wav)\n    embed = encoder.embed_utterance(wav)\n    np.save(embed_fpath, embed, allow_pickle=False)\n\n", "CodeBleu_score": 95.3261531719067}, {"code": "def embed_utterance(fpaths, encoder_model_fpath)\n\t\n    if not encoder.is_loaded():\n        encoder.load_model(encoder_model_fpath)\n\n    # Compute the speaker embedding of the utterance\n    wav_fpath, embed_fpath = fpaths\n    wav = np.load(wav_fpath)\n    wav = encoder.preprocess_wav(wav)\n    embed = encoder.embed_utterance(wav)\n    np.save(embed_fpath, embed, allow_pickle=False)\n\n", "CodeBleu_score": 95.3261531719067}, {"code": "def embed_utterance(fpaths, encoder_model_fpath)\n\t\n    if not encoder.is_loaded():\n        encoder.load_model(encoder_model_fpath)\n\n    # Compute the speaker embedding of the utterance\n    wav_fpath, embed_fpath = fpaths\n    wav = np.load(wav_fpath)\n    wav = encoder.preprocess_wav(wav)\n    embed = encoder.embed_utterance(wav)\n    np.save(embed_fpath, embed, allow_pickle=False)\n\n", "CodeBleu_score": 95.3261531719067}]}
{"instance_id": "9f860273a325", "generation_result": [{"code": "def create_embeddings(synthesizer_root: Path, encoder_model_fpath: Path, n_processes: int)\n\t\n\n    # Verify that the synthesizer_root directory exists\n    assert synthesizer_root.exists() and synthesizer_root.is_dir(), \"Synthesizer root directory does not exist\"\n\n    # Verify that the encoder model file exists\n    assert encoder_model_fpath.exists() and encoder_model_fpath.is_file(), \"Encoder model file does not exist\"\n\n    # Create a directory to store the embeddings\n    embedding_dir = synthesizer_root.joinpath(\"embeds\")\n    embedding_dir.mkdir(exist_ok=True)\n\n    # Create a metadata file for the embeddings\n    metadata_fpath = embedding_dir.joinpath(\"metadata.csv\")\n    metadata_fpath.touch(exist_ok=True)\n\n    # Read the metadata file\n    metadata = pd.read_csv(metadata_fpath)\n\n    # Get the audio paths\n    audio_paths = metadata[\"path\"]\n\n    # Get the embedding paths\n    embedding_paths = [embedding_dir.joinpath(f\"embed-{os.path.basename(audio_path)}.npy\") for audio_path in audio_paths]\n\n    # Remove the embedding paths that already exist\n    embedding_paths = [embedding_path for embedding_path in embedding_paths if not embedding_path.exists()]\n\n    # Get the number of audio files to process\n    n_files = len(audio_paths)\n\n    # Create a progress bar\n    progress_bar = tqdm(total=n_files)\n\n    # Create a pool of workers to process the audio files in parallel\n    with ProcessPoolExecutor(max_workers=n_processes) as pool:\n        # Process the files in parallel\n        for _ in pool.map(process_file, zip(audio_paths, embedding_paths, repeat(encoder_model_fpath))):\n            # Update the progress bar\n            progress_bar.update(1)\n\n    # Close the progress bar\n    progress_bar.close()\n\n    # Print a newline\n    print()\n\n    # Print a success message\n    print(f\"Embeddings created at {embedding_dir}\")\n\n", "CodeBleu_score": 40.74211545906719}, {"code": "def create_embeddings(synthesizer_root: Path, encoder_model_fpath: Path, n_processes: int)\n\t\n    # Verify that the encoder model file exists\n    if not encoder_model_fpath.exists():\n        raise FileNotFoundError(\"Could not find encoder model at %s\" % encoder_model_fpath)\n\n    # Create a directory to store the embeddings\n    encoder_model_name = encoder_model_fpath.stem\n    encoder_model_dir = synthesizer_root.joinpath(\"encoder\", encoder_model_name)\n    os.makedirs(encoder_model_dir, exist_ok=True)\n\n    # Create a metadata file\n    metadata_fpath = encoder_model_dir.joinpath(\"metadata.csv\")\n    if not metadata_fpath.exists():\n        raise FileNotFoundError(\"Could not find metadata at %s\" % metadata_fpath)\n\n    # Read the metadata file\n    metadata = pd.read_csv(metadata_fpath, encoding=\"utf-8\")\n\n    # Prepare the file paths to the audio files\n    wav_dir = synthesizer_root.joinpath(\"audio\")\n    mel_dir = encoder_model_dir.joinpath(\"mel\")\n    wav_files = [wav_dir.joinpath(fname) for fname in metadata[\"wav_file\"]]\n    mel_files = [mel_dir.joinpath(fname.with_suffix(\".npy\")) for fname in metadata[\"wav_file\"]]\n\n    # Create the output directories\n    os.makedirs(mel_dir, exist_ok=True)\n\n    # Progress bar\n    progress_bar = tqdm(total=len(metadata), desc=\"Creating embeddings\")\n\n    # Multiprocessing setup\n    manager = mp.Manager()\n    return_dict = manager.dict()\n    jobs = []\n\n    # Start the jobs\n    for i, (wav_fpath, mel_fpath) in enumerate(zip(wav_files, mel_files)):\n        # Skip existing files\n        if mel_fpath.exists():\n            continue\n\n        # Add the job to the queue\n        p = mp.Process(\n            target=partial(_process_utterance, wav_fpath, mel_fpath, encoder_model_fpath, return_dict)\n        )\n        jobs.append(p)\n        p.start()\n\n        # Run the jobs in parallel\n        while True:\n            running = any(job.is_alive() for job in jobs)\n            if not running:\n                break\n            time.sleep(1)\n\n        # Update the progress bar\n        progress_bar", "CodeBleu_score": 38.8826550503865}, {"code": "def create_embeddings(synthesizer_root: Path, encoder_model_fpath: Path, n_processes: int)\n\t\n\n    # Verify that the encoder model file exists\n    if not encoder_model_fpath.exists():\n        raise FileNotFoundError(f'Encoder model file does not exist: {encoder_model_fpath}')\n\n    # Create a directory to store the embeddings\n    embed_dir = synthesizer_root.joinpath('embeds')\n    embed_dir.mkdir(exist_ok=True)\n\n    # Create a metadata file that maps text to embedding file paths\n    embed_metadata_fpath = synthesizer_root.joinpath('embed_metadata.txt')\n    metadata = []\n\n    # Gather the input audio files\n    search_pattern = synthesizer_root.joinpath('train.txt')\n    search_results = list(search_pattern.parent.glob(search_pattern.name))\n    assert len(search_results) > 0, f'Could not find any audio files to embed in \"{search_pattern.parent.resolve()}\".'\n    print(f'Found {len(search_results)} audio files in \"{search_pattern.parent.resolve()}\".')\n\n    # Read metadata from each audio file\n    for wav_path in tqdm(search_results, desc='Reading metadata', unit='file'):\n        # Read the metadata\n        text = get_metadata(wav_path)\n        # Create the embedding file path\n        embed_fpath = embed_dir.joinpath(f'{wav_path.stem}.npy')\n        # Add metadata to the list\n        metadata.append((text, str(embed_fpath)))\n\n    # Write the metadata to disk\n    with open(embed_metadata_fpath, 'w', encoding='utf8') as f:\n        for m in metadata:\n            f.write(f'{m[0]}\\t{m[1]}", "CodeBleu_score": 38.39441660106644}]}
{"instance_id": "9eb0abda14a9", "generation_result": [{"code": "def eval_model(attention, mel_prediction, target_spectrogram, input_seq, step,\n               plot_dir, mel_output_dir, wav_dir, sample_num, loss, hparams)\n\t\n    # Save attention plot\n    save_attention(attention, os.path.join(plot_dir, \"attention_step{}_sample{}\".format(step, sample_num)))\n\n    # Save predicted mel spectrogram to disk\n    plot_spectrogram(mel_prediction, os.path.join(mel_output_dir, \"mel_step{}_sample{}.png\".format(step, sample_num)),\n                     title=\"{}_step{}_sample{}\".format(hparams.model, step, sample_num),\n                     split_title=True, target_spectrogram=target_spectrogram,\n                     max_len=target_spectrogram.size // hparams.num_mels)\n\n    # Save target and predicted mel-spectrogram plot to disk\n    plot_spectrogram(mel_prediction, os.path.join(plot_dir, \"mel_step{}_sample{}.png\".format(step, sample_num)),\n                     title=\"{}_step{}_sample{}\".format(hparams.model, step, sample_num),\n                     split_title=True, target_spectrogram=target_spectrogram,\n                     max_len=target_spectrogram.size // hparams.num_mels, auto_aspect=True)\n\n    # Save predicted audio (reconstructed waveform) to disk\n    audio.save_wav(wav_dir, mel_prediction, hparams.sample_rate,\n                   \"{}_step{}_sample{}.wav\".format(hparams.model, step, sample_num))\n\n    # Save plot of predicted vs target mel-spectrograms to disk\n    plot_spectrogram(mel_prediction, os.path.join(plot_dir, \"mel_step{}_sample{}.png\".format(step, sample_num)),\n                     title=\"{}, {}, step={}, loss={:.5f}\".format(time_string(), hparams.model, step, loss),\n                     split_title=True, target_spectrogram=target_spectrogram,\n                     max_len=target_spectrogram.size // hparams.num_mels)\n\n    # Print input sequence to disk\n    with open(os.path.join(plot_dir, \"input_step{}.txt\".format(step)), \"w\") as f:\n        f.write(sequence_to_text(input_seq))\n", "CodeBleu_score": 42.61897731288224}, {"code": "def eval_model(attention, mel_prediction, target_spectrogram, input_seq, step,\n               plot_dir, mel_output_dir, wav_dir, sample_num, loss, hparams)\n\t\n\n    # save predicted mel spectrogram to disk (debug)\n    mel_filename = f\"{step}-predicted-mel-spectrogram.npy\"\n    np.save(os.path.join(mel_output_dir, mel_filename), mel_prediction, allow_pickle=False)\n\n    # save griffin lim inverted wav for debug (mel -> wav)\n    wav = audio.inv_mel_spectrogram(mel_prediction.T, hparams)\n    wav_filename = f\"{step}-predicted-audio.wav\"\n    audio.save_wav(wav, os.path.join(wav_dir, wav_filename), sr=hparams.sample_rate)\n\n    # save alignment, mel target, and predicted, input, and attention spectrogram (control purposes)\n    plot_filename = f\"{step}-step-mel-alignment.png\"\n    save_attention(attention, os.path.join(plot_dir, plot_filename))\n\n    # save target mel spectrogram to disk (control purposes)\n    mel_filename = f\"{step}-target-mel-spectrogram.npy\"\n    np.save(os.path.join(mel_output_dir, mel_filename), target_spectrogram, allow_pickle=False)\n\n    # save griffin lim inverted wav for control purposes (mel -> wav)\n    wav = audio.inv_mel_spectrogram(target_spectrogram.T, hparams)\n    wav_filename = f\"{step}-target-audio.wav\"\n    audio.save_wav(wav, os.path.join(wav_dir, wav_filename), sr=hparams.sample_rate)\n\n    # save griffin lim inverted wav for debug (mel -> wav)\n    wav = audio.inv_mel_spectrogram(mel_prediction.T, hparams)\n    wav_filename = f\"{step}-predicted-audio.wav\"\n    audio.save_wav(wav, os.path.join(wav_dir, wav_filename), sr=hparams.sample_rate)\n\n    # save plot of target vs predicted mel-spectrogram\n    plot_spectrogram(\n        mel_prediction,\n        os.path.join(plot_dir, f\"{step}-mel-prediction-step-{step}.png\"),\n        title=f\"Model: {hparams.model_type}, Step: {step}, Loss: {loss:.5f", "CodeBleu_score": 37.12555652946526}, {"code": "def eval_model(attention, mel_prediction, target_spectrogram, input_seq, step,\n               plot_dir, mel_output_dir, wav_dir, sample_num, loss, hparams)\n\t\n    # Save attention plot\n    save_attention(attention, os.path.join(plot_dir, \"attention_step{}_sample{}\".format(step, sample_num)))\n\n    # Save predicted mel spectrogram to disk\n    mel_filename = \"mel-prediction-step{}_sample{}.npy\".format(step, sample_num)\n    np.save(os.path.join(mel_output_dir, mel_filename), mel_prediction, allow_pickle=False)\n\n    # Save griffin lim inverted waveform to disk\n    wav_filename = \"step{}_sample{}.wav\".format(step, sample_num)\n    wav_path = os.path.join(wav_dir, wav_filename)\n    # save_wav(wav_path, wav, sr)\n\n    # Save real and predicted mel-spectrogram plot to disk\n    plot_filename = \"step{}_sample{}.png\".format(step, sample_num)\n    plot_spectrogram(mel_prediction, os.path.join(plot_dir, plot_filename),\n                     title=\"{}, {}, step={}, loss={:.5f}\".format(hparams.model, time_string(), step, loss),\n                     split_title=True)\n\n    # Print some inputs\n    print(\"Input at step {}:\".format(step))\n    print(sequence_to_text(input_seq))\n", "CodeBleu_score": 38.727038476647834}]}
{"instance_id": "6bad4f500286", "generation_result": [{"code": "def num_params(self)\n\t\n    model_parameters = filter(lambda p: p.requires_grad, self.parameters())\n    params = sum([np.prod(p.size()) for p in model_parameters])\n    print(f'The model has {params/1e6:.3f}M trainable parameters')\n\n", "CodeBleu_score": 37.87401177359528}, {"code": "def num_params(self)\n\t\n    trainable_params = filter(lambda p: p.requires_grad, self.parameters())\n    num_params = sum([np.prod(p.size()) for p in trainable_params])\n    print('The number of trainable parameters in the model is: %.3f million' % (num_params / 1e6))\n\n", "CodeBleu_score": 37.18228470063189}, {"code": "def num_params(self)\n\t\n    model_parameters = filter(lambda p: p.requires_grad, self.parameters())\n    params = sum([np.prod(p.size()) for p in model_parameters])\n    print('Trainable parameters: %.3fM' % (params / 1e6))\n\n", "CodeBleu_score": 39.44487323594}]}
{"instance_id": "78b876054bd4", "generation_result": [{"code": "def pad_tensor(self, x, pad, side='both')\n\t\n    if side == 'before':\n        new_size = (x.size(0) + pad, x.size(1), x.size(2))\n    elif side == 'after':\n        new_size = (x.size(0) + pad, x.size(1), x.size(2))\n    elif side == 'both':\n        new_size = (x.size(0) + pad * 2, x.size(1), x.size(2))\n    else:\n        raise ValueError(\"The specified side must be either 'before', 'after', or 'both'.\")\n    padded_tensor = torch.zeros(new_size)\n    if x.is_cuda:\n        padded_tensor = padded_tensor.cuda()\n    if side == 'before':\n        padded_tensor[pad:, :, :] = x\n    elif side == 'after':\n        padded_tensor[:-pad, :, :] = x\n    elif side == 'both':\n        padded_tensor[pad:-pad, :, :] = x\n    return padded_tensor\n\n", "CodeBleu_score": 32.75007142841908}, {"code": "def pad_tensor(self, x, pad, side='both')\n\t\n\n    # Check if the input tensor is on the CPU or GPU\n    if x.is_cuda:\n        # If it is on the GPU, then create a new padded tensor on the GPU\n        padded = torch.cuda.FloatTensor(x.size(0), x.size(1), pad).fill_(0)\n    else:\n        # If it is on the CPU, then create a new padded tensor on the CPU\n        padded = torch.FloatTensor(x.size(0), x.size(1), pad).fill_(0)\n\n    # If the side is 'before', then place the original tensor before the padded tensor\n    if side == 'before':\n        padded[:, :, :x.size(2)] = x\n    # If the side is 'after', then place the original tensor after the padded tensor\n    elif side == 'after':\n        padded[:, :, pad:pad+x.size(2)] = x\n    # If the side is 'both', then place the original tensor before and after the padded tensor\n    elif side == 'both':\n        padded[:, :, :x.size(2)] = x\n        padded[:, :, pad:pad+x.size(2)] = x\n    else:\n        raise ValueError('The side argument must be either \"before\", \"after\", or \"both\".')\n\n    # Return the padded tensor\n    return padded\n\n", "CodeBleu_score": 33.84720961568671}, {"code": "def pad_tensor(self, x, pad, side='both')\n\t\n    # determine new size\n    if side == 'both':\n        new_size = x.size(0) + pad[0] + pad[1], x.size(1)\n    elif side == 'before':\n        new_size = x.size(0) + pad[0], x.size(1)\n    elif side == 'after':\n        new_size = x.size(0) + pad[1], x.size(1)\n    else:\n        raise ValueError('Invalid side value. Must be one of \"before\", \"after\", or \"both\".')\n\n    # create new tensor\n    if x.is_cuda:\n        new_tensor = torch.cuda.FloatTensor(new_size).fill_(0)\n    else:\n        new_tensor = torch.FloatTensor(new_size).fill_(0)\n\n    # place original tensor into new tensor\n    if side == 'both':\n        new_tensor[pad[0]:new_tensor.size(0) - pad[1], :] = x\n    elif side == 'before':\n        new_tensor[pad[0]:, :] = x\n    elif side == 'after':\n        new_tensor[:new_tensor.size(0) - pad[1], :] = x\n\n    return new_tensor\n\n", "CodeBleu_score": 31.802226019984936}]}
{"instance_id": "a7ed1715a3df", "generation_result": [{"code": "def save_attention(attn, path)\n\t\n    plt.figure()\n    plt.imshow(attn, cmap='gray')\n    plt.xticks(np.arange(0, attn.shape[0], 1))\n    plt.yticks(np.arange(0, attn.shape[1], 1))\n    plt.colorbar()\n    plt.savefig(path + '.png')\n    plt.close()\n\n", "CodeBleu_score": 32.53263932811308}, {"code": "def save_attention(attn, path)\n\t\n    plt.figure(figsize=(10, 10))\n    plt.imshow(attn, cmap='hot', interpolation='nearest')\n    plt.colorbar()\n    plt.savefig(path, bbox_inches='tight')\n    plt.close()\n\n", "CodeBleu_score": 37.65264040251108}, {"code": "def save_attention(attn, path)\n\t\n    plt.matshow(attn)\n    plt.colorbar()\n    plt.savefig(path)\n    plt.close()\n\n", "CodeBleu_score": 10.8985642810349}]}
{"instance_id": "19375c9f44a2", "generation_result": [{"code": "def plot(array)\n\t\n    import matplotlib.pyplot as plt\n\n    fig = plt.figure(figsize=(30, 5))\n    ax = fig.add_subplot(111)\n    ax.xaxis.label.set_color('grey')\n    ax.yaxis.label.set_color('grey')\n    ax.xaxis.label.set_fontsize(23)\n    ax.yaxis.label.set_fontsize(23)\n    ax.tick_params(axis='x', colors='grey', labelsize=23)\n    ax.tick_params(axis='y', colors='grey', labelsize=23)\n    plt.plot(array)\n\n", "CodeBleu_score": 77.73415655580472}, {"code": "def plot(array)\n\t\n    import matplotlib.pyplot as plt\n\n    fig = plt.figure(figsize=(30, 5))\n    ax = fig.add_subplot(111)\n    ax.xaxis.label.set_color('grey')\n    ax.yaxis.label.set_color('grey')\n    ax.xaxis.label.set_fontsize(23)\n    ax.yaxis.label.set_fontsize(23)\n    ax.tick_params(axis='x', colors='grey', labelsize=23)\n    ax.tick_params(axis='y', colors='grey', labelsize=23)\n    plt.plot(array)\n\n", "CodeBleu_score": 77.73415655580472}, {"code": "def plot(array)\n\t\n    import matplotlib.pyplot as plt\n\n    fig = plt.figure(figsize=(30, 5))\n    ax = fig.add_subplot(111)\n    ax.xaxis.label.set_color('grey')\n    ax.yaxis.label.set_color('grey')\n    ax.xaxis.label.set_fontsize(23)\n    ax.yaxis.label.set_fontsize(23)\n    ax.tick_params(axis='x', colors='grey', labelsize=23)\n    ax.tick_params(axis='y', colors='grey', labelsize=23)\n    plt.plot(array)\n\n", "CodeBleu_score": 77.73415655580472}]}
{"instance_id": "ff94c425920c", "generation_result": [{"code": "def write_to_csv(image_name, prediction, confidence)\n\t\n    with open(CSV_PATH, \"a\", newline=\"\") as csv_file:\n        fieldnames = [\"image\", \"prediction\", \"confidence\"]\n        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n        if csv_file.tell() == 0:\n            writer.writeheader()\n        writer.writerow({\"image\": image_name, \"prediction\": prediction, \"confidence\": confidence})\n\n", "CodeBleu_score": 46.73297253523018}, {"code": "def write_to_csv(image_name, prediction, confidence)\n\t\n    data = {'image': image_name, 'prediction': prediction, 'confidence': confidence}\n    with open('predictions.csv', 'a') as f:\n        writer = csv.DictWriter(f, data.keys())\n        if f.tell() == 0:\n            writer.writeheader()\n        writer.writerow(data)\n\n", "CodeBleu_score": 51.84429294108388}, {"code": "def write_to_csv(image_name, prediction, confidence)\n\t\n    with open('predictions.csv', 'a', newline='') as csvfile:\n        writer = csv.DictWriter(csvfile, fieldnames=['image', 'prediction', 'confidence'])\n        if csvfile.tell() == 0:\n            writer.writeheader()\n        writer.writerow({'image': image_name, 'prediction': prediction, 'confidence': confidence})\n\n", "CodeBleu_score": 46.13111496081659}]}
{"instance_id": "436161602009", "generation_result": [{"code": "def export_torchscript(model, im, file, optimize, prefix=colorstr(\"TorchScript:\"))\n\t\n    f = file.split(\".\")\n    if f[-1] != \"pt\":\n        f = \".\".join(f[:-1] + [\"pt\"])\n    else:\n        f = \".\".join(f)\n    ts_model = torch.jit.trace(model, im, strict=False)\n    if optimize:\n        ts_model = torch.jit.optimize_for_mobile(ts_model)\n    ts_model.save(f)\n    LOGGER.info(\n        \"%s Saved %s TorchScript model to %s\",\n        prefix,\n        \"optimized \" if optimize else \"\",\n        f,\n    )\n    return f, None\n\n", "CodeBleu_score": 41.202892409307154}, {"code": "def export_torchscript(model, im, file, optimize, prefix=colorstr(\"TorchScript:\"))\n\t\n    LOGGER.info(f\"", "CodeBleu_score": 6.951754385967379}, {"code": "def export_torchscript(model, im, file, optimize, prefix=colorstr(\"TorchScript:\"))\n\t\n    f = file.split(\".\")\n    if f[1] != \"pt\":\n        f = f[0] + \".pt\"\n    else:\n        f = f[0]\n    f = Path(f)\n    if f.exists():\n        LOGGER.warning(\n            f\"File {f} already exists. Existing models will be overwritten.\"\n        )\n    LOGGER.info(f\"{prefix} starting export with torch {torch.__version__}...\")\n    LOGGER.info(f\"{prefix} tracing model...\")\n    example = torch.rand(1, 3, *im.shape[1:])\n    traced_script_module = torch.jit.trace(model, example)\n    if optimize:\n        LOGGER.info(f\"{prefix} optimizing and saving the model...\")\n        traced_script_module = optimize_for_mobile(traced_script_module)\n        traced_script_module._save_for_lite_interpreter(str(f))\n    else:\n        LOGGER.info(f\"{prefix} saving the model...\")\n        traced_script_module.save(str(f))\n    LOGGER.info(f\"{prefix} export complete. Model saved to {f}\")\n    return f, None\n\n", "CodeBleu_score": 44.227677058039795}]}
{"instance_id": "78335487ac1c", "generation_result": [{"code": "def export_coreml(model, im, file, int8, half, nms, prefix=colorstr(\"CoreML:\")):\n\t\n    if not coremltools_found:\n        LOGGER.info(\n            f\"{prefix} {colorstr('red')}\"\n            \"CoreML export requires 'coremltools' to be installed. Run `pip install coremltools` and try again.\"\n        )\n        return None, None\n\n    LOGGER.info(f\"{prefix} starting export with torch {torch.__version__}...\")\n    start_time = time.time()\n\n    # set file suffix and default export function\n    file_suffix = \".mlmodel\"\n    export_func = coremltools.convert\n\n    # wrap model for NMS support\n    if nms:\n        from coremltools.models.neural_network import quantization_utils\n\n        class_labels = [\"__background__\"]\n        if isinstance(model, Detect):\n            class_labels += model.names\n        model = iOSModel(model, class_labels)\n        #LOGGER.info(f\"{prefix} wrapping model with class_labels {class_labels}\")\n\n    # trace model, convert to CoreML, and apply quantization if requested\n    try:\n        mlmodel = export_func(model, im, file=file, int8_quantization_mode=int8, is_bgr=False, image_input_names=IMAGE_INPUT_NAMES, image_scale=1/255.0, class_labels=class_labels)\n    except Exception as e:\n        LOGGER.info(f\"{prefix} {colorstr('red')}error: {e}\")\n        return None, None\n\n    # save CoreML model\n    if file:\n        mlmodel.save(file)\n        LOGGER.info(f\"{prefix} saved CoreML model at '{colorstr('cyan')}{file}{colorstr('end')}'\")\n    else:\n        tmp = tempfile.NamedTemporaryFile(suffix=file_suffix, delete=False)\n        tmp.close()\n        mlmodel.save(tmp.name)\n        LOGGER.info(f\"{prefix} saved CoreML model at '{colorstr('cyan')}{tmp.name}{colorstr('end')}'\")\n\n    # return file path and CoreML model\n    return tmp.name, mlmodel\n\n", "CodeBleu_score": 36.33370402004399}, {"code": "def export_coreml(model, im, file, int8, half, nms, prefix=colorstr(\"CoreML:\")):\n\t\n    if not is_coreml_installed():\n        raise ImportError(\n            \"CoreML export requires `coremltools` which is not installed yet,\"\n            \" install it with `pip install coremltools`.\"\n        )\n    if half:\n        assert torch.cuda.is_available(), \"Half precision only supported on CUDA devices.\"\n        model.half()\n    if int8:\n        assert torch.cuda.is_available(), \"INT8 quantization only supported on CUDA devices.\"\n        model.eval()\n        model = torch.quantization.quantize_dynamic(model, {torch.nn.Conv2d, torch.nn.Linear}, dtype=torch.qint8)\n    if nms:\n        assert torch.cuda.is_available(), \"NMS only supported on CUDA devices.\"\n        model = iOSModel(model)\n    traced_model = torch.jit.trace(model, im)\n    traced_model.save(file)\n    return file\n\n", "CodeBleu_score": 30.727083092205532}, {"code": "def export_coreml(model, im, file, int8, half, nms, prefix=colorstr(\"CoreML:\")):\n\t\n    try:\n        import coremltools as ct\n    except ImportError as e:\n        print(\n            \"export_coreml(): the library coremltools is not installed, install it with `pip install coremltools`\")\n        exit()\n    if half:\n        model = model.half()  # to FP16\n    if int8:\n        model = model.eval()\n        model = torch_utils.quantize_model(model)  # to INT8\n    if nms:\n        model = iOSModel(model)  # wrap for NMS\n    traced_model = torch.jit.trace(model, im)  # create a torch.jit.ScriptModule via tracing\n    mlmodel = ct.convert(traced_model, inputs=[\n        ct.ImageType(name=\"image\", shape=im.shape, scale=1 / 255.0, bias=[0, 0, 0])\n    ])  # CoreML\n    suffix = \".mlmodel\"\n    if half:\n        suffix = \"_fp16\" + suffix\n    if int8:\n        suffix = \"_int8\" + suffix\n    if nms:\n        suffix = \"_ios\" + suffix\n    file = Path(increment_path(Path(file).parent, suffix=suffix))\n    mlmodel.save(file)\n    LOGGER.info(f\"{prefix} exported {colorstr('bold', file)}\")\n    return file, mlmodel\n\n", "CodeBleu_score": 46.651805546644276}]}
{"instance_id": "80233cf71580", "generation_result": [{"code": "def generate_individual(input_ranges, individual_length)\n\t\n    individual = []\n    for i in range(individual_length):\n        individual.append(random.uniform(input_ranges[i][0], input_ranges[i][1]))\n    return individual\n\n", "CodeBleu_score": 47.630972577458174}, {"code": "def generate_individual(input_ranges, individual_length)\n\t\n    individual = []\n    for i in range(individual_length):\n        individual.append(random.uniform(input_ranges[0], input_ranges[1]))\n    return individual\n\n", "CodeBleu_score": 45.11328150959641}, {"code": "def generate_individual(input_ranges, individual_length)\n\t\n    individual = []\n    for i in range(individual_length):\n        individual.append(random.uniform(input_ranges[0], input_ranges[1]))\n    return individual\n\n", "CodeBleu_score": 45.11328150959641}]}
{"instance_id": "5ea979cb4a8e", "generation_result": [{"code": "def marginal_log_mean_coeff(self, t)\n\t\n    if self.schedule_type == \"discrete\":\n        return torch.log(self.alpha_t_list[t])\n    elif self.schedule_type == \"linear\":\n        return (\n            torch.log(self.alpha_t_list[t])\n            + torch.log(self.alpha_t_list[t + 1])\n            + (self.alpha_t_list[t + 1] - self.alpha_t_list[t])\n            / (self.T - 1)\n            * (t - self.t_list[t])\n        )\n    elif self.schedule_type == \"cosine\":\n        return (\n            torch.log(self.alpha_t_list[t])\n            + torch.log(self.alpha_t_list[t + 1])\n            + (self.alpha_t_list[t + 1] - self.alpha_t_list[t])\n            / 2\n            * (1 - torch.cos(math.pi * (t - self.t_list[t]) / (self.T - 1)))\n        )\n    else:\n        raise ValueError(\"Invalid schedule type: %s\" % self.schedule_type)\n\n", "CodeBleu_score": 29.093959911134338}, {"code": "def marginal_log_mean_coeff(self, t)\n\t\n    if self.schedule_type == \"discrete\":\n        return torch.log(self.alpha_schedule[t])\n    elif self.schedule_type == \"linear\":\n        return torch.log(self.alpha_schedule[t] + (1 - self.alpha_schedule[t]) * (t / self.T))\n    elif self.schedule_type == \"cosine\":\n        return torch.log(self.alpha_schedule[t] + (1 - self.alpha_schedule[t]) * (0.5 + 0.5 * np.cos(np.pi * t / self.T)))\n    else:\n        raise NotImplementedError\n\n", "CodeBleu_score": 30.666939326959742}, {"code": "def marginal_log_mean_coeff(self, t)\n\t\n    if self.schedule_type == \"discrete\":\n        return self.log_alpha[t]\n    elif self.schedule_type == \"linear\":\n        return self.log_alpha[0] + (t - self.t_min) * self.log_alpha_diff\n    elif self.schedule_type == \"cosine\":\n        return self.log_alpha[0] + (1 + math.cos(math.pi * (t - self.t_min) / (self.t_max - self.t_min))) / 2 * self.log_alpha_diff\n    else:\n        raise ValueError(\"Unknown schedule type.\")\n\n", "CodeBleu_score": 31.28115971730714}]}
{"instance_id": "7da4fae5a278", "generation_result": [{"code": "def inverse_lambda(self, lamb)\n\t\n    if self.schedule == \"linear\":\n        t = (lamb - self.beta_min) / (self.beta_max - self.beta_min)\n        return t\n    elif self.schedule == \"discrete\":\n        xp = self.log_alpha_discrete\n        yp = self.t_discrete\n        return interpolate_fn(lamb, xp, yp)\n    elif self.schedule == \"log_alpha\":\n        return torch.acos(1 - 2 * lamb) / np.pi\n    else:\n        raise NotImplementedError(\"Unknown schedule: {}\".format(self.schedule))\n\n", "CodeBleu_score": 18.354992819790862}, {"code": "def inverse_lambda(self, lamb)\n\t\n    if self.schedule_type == \"linear\":\n        return self.beta * lamb\n    elif self.schedule_type == \"discrete\":\n        return torch.exp(interpolate_fn(lamb, self.log_alpha, self.t))\n    elif self.schedule_type == \"logarithmic\":\n        return torch.acos(torch.exp(-lamb))\n    elif self.schedule_type == \"exponential\":\n        return torch.log(torch.exp(lamb) - 1)\n    else:\n        raise NotImplementedError\n", "CodeBleu_score": 14.533571637575141}, {"code": "def inverse_lambda(self, lamb)\n\t\n    if self.schedule == \"linear\":\n        # calculate the label t using beta values\n        beta_t = torch.gather(self.beta_values, dim=1, index=self.t.long().unsqueeze(1)).squeeze(1)\n        t = self.t + beta_t * lamb\n    elif self.schedule == \"discrete\":\n        # calculate the label t using a piecewise linear function\n        log_alpha = interpolate_fn(self.t, self.log_alpha_values, self.t_values)\n        t = torch.acos(torch.exp(-lamb * torch.exp(log_alpha))) / self.pi\n    elif self.schedule == \"logarithmic\":\n        # calculate the label t using a piecewise linear function\n        log_alpha = interpolate_fn(self.t, self.log_alpha_values, self.t_values)\n        t = torch.acos(torch.exp(-lamb * torch.exp(log_alpha))) / self.pi\n    else:\n        raise NotImplementedError\n    return t\n\n", "CodeBleu_score": 28.77358015080574}]}
{"instance_id": "a519c853a2b2", "generation_result": [{"code": "def noise_pred_fn(x, t_continuous, cond=None)\n\t\n    if cond is None:\n        cond = torch.tensor([0.])\n    if len(cond.shape) == 1:\n        cond = cond.unsqueeze(0)\n    if len(x.shape) == 1:\n        x = x.unsqueeze(0)\n    if len(t_continuous.shape) == 1:\n        t_continuous = t_continuous.unsqueeze(0)\n    if len(cond.shape) == 2:\n        cond = cond.unsqueeze(2)\n    if len(x.shape) == 2:\n        x = x.unsqueeze(2)\n    if len(t_continuous.shape) == 2:\n        t_continuous = t_continuous.unsqueeze(2)\n    if x.shape[0] != cond.shape[0] or x.shape[0] != t_continuous.shape[0]:\n        raise ValueError(\"Batch size of x, cond and t_continuous must be the same.\")\n    if x.shape[0] > 1:\n        if x.shape[1] != cond.shape[1] or x.shape[1] != t_continuous.shape[1]:\n            raise ValueError(\"Batch size of x, cond and t_continuous must be the same.\")\n    if x.shape[2] != cond.shape[2] or x.shape[2] != t_continuous.shape[2]:\n        raise ValueError(\"Batch size of x, cond and t_continuous must be the same.\")\n    if x.shape[2] > 1:\n        if x.shape[3] != cond.shape[3] or x.shape[3] != t_continuous.shape[3]:\n            raise ValueError(\"Batch size of x, cond and t_continuous must be the same.\")\n    if x.shape[2] > 2:\n        if x.shape[4] != cond.shape[4] or x.shape[4] != t_continuous.shape[4]:\n            raise ValueError(\"Batch size of x, cond and t_continuous must be the same.\")\n    if x.shape[2] > 3:\n        if x.shape[5] != cond.shape[5] or x.shape[5] != t_continuous.shape[5]:\n            raise ValueError(\"Batch size of x, cond and t_continuous must be the same.\")\n    if x.shape[2] > ", "CodeBleu_score": 18.2725281084822}, {"code": "def noise_pred_fn(x, t_continuous, cond=None)\n\t\n    if cond is None:\n        cond = {}\n    if 'model_type' not in cond:\n        cond['model_type'] = 'simple'\n    if cond['model_type'] == 'simple':\n        # The input is expanded to match the batch size.\n        x = expand_dims(x, 1)\n        # The input is processed through the model.\n        out = model(x, t_continuous, cond)\n        # The output is processed using noise schedule parameters.\n        out = noise_schedule.process_noise_pred(out, t_continuous)\n    elif cond['model_type'] == 'simple_with_cond':\n        # The input is expanded to match the batch size.\n        x = expand_dims(x, 1)\n        # The input is processed through the model.\n        out = model(x, t_continuous, cond)\n        # The output is processed using noise schedule parameters.\n        out = noise_schedule.process_noise_pred(out, t_continuous)\n    else:\n        raise ValueError(f\"Unknown model type: {cond['model_type']}\")\n    return out\n\n", "CodeBleu_score": 19.335234479686612}, {"code": "def noise_pred_fn(x, t_continuous, cond=None)\n\t\n    if model_type == 'linear':\n        return noise_schedule.linear_transform(x)\n    elif model_type == 'conv':\n        return noise_schedule.conv_transform(x)\n    elif model_type == 'conv_cond':\n        return noise_schedule.conv_cond_transform(x, cond)\n    elif model_type == 'conv_cond_linear':\n        return noise_schedule.conv_cond_linear_transform(x, cond)\n    elif model_type == 'conv_cond_linear_shared':\n        return noise_schedule.conv_cond_linear_shared_transform(x, cond)\n    elif model_type == 'conv_cond_linear_shared_marginal':\n        return noise_schedule.conv_cond_linear_shared_marginal_transform(x, cond, t_continuous)\n    else:\n        raise ValueError('Unknown model type: %s' % model_type)", "CodeBleu_score": 22.911463934148372}]}
{"instance_id": "96a9b7d54823", "generation_result": [{"code": "def dynamic_thresholding_fn(self, x0, t=None)\n\t\n    if t is None:\n        t = self.threshold\n    if self.quantile is None:\n        q = 0.99\n    else:\n        q = self.quantile\n    x0 = x0.reshape(-1)\n    x0 = x0[torch.abs(x0) > 0]\n    if len(x0) == 0:\n        return x0\n    else:\n        q_th = torch.quantile(x0, q)\n        scale = t / q_th\n        x0 = torch.clamp(x0, min=0)\n        x0 = x0 * scale\n        return x0\n\n", "CodeBleu_score": 31.1542416326964}, {"code": "def dynamic_thresholding_fn(self, x0, t=None)\n\t\n    if t is None:\n        t = self.threshold\n    x0 = x0.view(x0.shape[0], -1)\n    x0_abs = torch.abs(x0)\n    x0_abs_quantile = torch.quantile(x0_abs, self.quantile, dim=1)\n    x0_abs_quantile = expand_dims(x0_abs_quantile, 1)\n    x0_abs_quantile = expand_dims(x0_abs_quantile, 1)\n    x0_abs_quantile = expand_dims(x0_abs_quantile, 1)\n    x0_abs_quantile = expand_dims(x0_abs_quantile, 1)\n    x0_abs_quantile = expand_dims(x0_abs_quantile, 1)\n    scaling_factor = t / x0_abs_quantile\n    x0_scaled = x0 * scaling_factor\n    x0_clamped = torch.clamp(x0_scaled, -t, t)\n    return x0_clamped.view(x0.shape)\n", "CodeBleu_score": 33.28892875835048}, {"code": "def dynamic_thresholding_fn(self, x0, t=None)\n\t\n    if t is None:\n        t = self.t\n    x0_shape = x0.size()\n    x0_flatten = x0.view(-1)\n    x0_abs_flatten = torch.abs(x0_flatten)\n    x0_abs_flatten_sorted, _ = torch.sort(x0_abs_flatten, descending=True)\n    threshold = x0_abs_flatten_sorted[int(round(t * x0_abs_flatten_sorted.numel()))]\n    scaling_factor = threshold / self.eps\n    x0_clamped = torch.clamp(x0_flatten, -threshold, threshold)\n    x0_clamped = x0_clamped * scaling_factor\n    return x0_clamped.view(x0_shape)\n\n", "CodeBleu_score": 36.63996290868161}]}
{"instance_id": "59c6ec6d7a6a", "generation_result": [{"code": "def data_prediction_fn(self, x, t)\n\t\n    return self.normalize(torch.clamp(x, min=0., max=self.marginal_std(t)))", "CodeBleu_score": 8.41682786476769}, {"code": "def data_prediction_fn(self, x, t)\n\t\n    threshold = self.max_threshold * self.dynamic_thresholding_ratio(t)\n    x = torch.clamp(x, min=0., max=threshold)\n    x = x / threshold\n    return x", "CodeBleu_score": 10.828455218979723}, {"code": "def data_prediction_fn(self, x, t)\n\t\n    x0 = x\n    x = torch.clamp(x, 0., self.max_threshold)\n    x = x / self.max_threshold\n    x = x * self.dynamic_thresholding_ratio\n    return x", "CodeBleu_score": 3.952432564362171}]}
{"instance_id": "65e3618468a2", "generation_result": [{"code": "def predict_eps_sigma(model, input, sigma_in, **kwargs)\n\t\n    sigma_in = np.array(sigma_in)\n    sigma_in = np.reshape(sigma_in, (1, sigma_in.shape[0]))\n    input = np.array(input)\n    input = np.reshape(input, (input.shape[0], 1))\n    input = input / sigma_in\n    prediction = model.predict(input, **kwargs)\n    prediction = prediction * sigma_in\n    return prediction\n\n", "CodeBleu_score": 25.711768339328977}, {"code": "def predict_eps_sigma(model, input, sigma_in, **kwargs)\n\t\n    # Reshape sigma to match the dimensions of the input\n    sigma = sigma_in.reshape(input.shape)\n    # Scale the input\n    input_scaled = input / sigma\n    # Compute the prediction\n    prediction = model(input_scaled, **kwargs)\n    # Adjust the prediction\n    prediction_scaled = prediction * sigma\n    return prediction_scaled\n\n", "CodeBleu_score": 19.92269164030501}, {"code": "def predict_eps_sigma(model, input, sigma_in, **kwargs)\n\t\n    # Reshape sigma to match the dimensions of the input\n    sigma_in = sigma_in.reshape(input.shape)\n    # Scale the input\n    input_scaled = input / sigma_in\n    # Compute the prediction\n    prediction = model(input_scaled, **kwargs)\n    # Adjust the prediction\n    prediction = prediction * sigma_in\n    # Return the adjusted prediction\n    return prediction\n\n", "CodeBleu_score": 19.958879854531247}]}
{"instance_id": "b35846dfbc66", "generation_result": [{"code": "def cal_intergrand(beta_0, beta_1, taus)\n\t\n    with torch.enable_grad():\n        taus_clone = taus.clone().requires_grad_(True)\n        alpha = t2alpha_fn(beta_0, beta_1, taus_clone)\n        log_alpha = torch.log(alpha)\n        log_alpha.backward(torch.ones_like(log_alpha))\n        integrand = (taus_clone.grad * alpha).sum(dim=-1)\n    return integrand\n\n", "CodeBleu_score": 41.900313274809264}, {"code": "def cal_intergrand(beta_0, beta_1, taus)\n\t\n    with torch.enable_grad():\n        taus = taus.clone().requires_grad_(True)\n        alpha = t2alpha_fn(beta_0, beta_1, taus)\n        log_alpha = torch.log(alpha)\n        grad = torch.autograd.grad(log_alpha, taus, grad_outputs=torch.ones_like(taus))[0]\n    return grad\n\n", "CodeBleu_score": 41.21951271421616}, {"code": "def cal_intergrand(beta_0, beta_1, taus)\n\t\n    with torch.enable_grad():\n        taus = taus.clone().requires_grad_(True)\n        alpha = t2alpha_fn(beta_0, beta_1, taus)\n        log_alpha = torch.log(alpha)\n        log_alpha.backward(torch.ones_like(log_alpha))\n        grad_taus = taus.grad\n        grad_taus.zero_()\n        intergrand = grad_taus * t2alpha_fn(beta_0, beta_1, taus)\n    return intergrand\n\n", "CodeBleu_score": 44.96952734870791}]}
{"instance_id": "5f3667566a10", "generation_result": [{"code": "def sample_euler_ancestral(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None)\n\t\n    if extra_args is None:\n        extra_args = {}\n    if noise_sampler is None:\n        noise_sampler = default_noise_sampler\n    if disable is None:\n        disable = []\n    x = x.clone().detach()\n    denoised = model(x, **extra_args)\n    for sigma in sigmas:\n        sigma_down, sigma_up = get_ancestral_step(denoised.std(), sigma, eta)\n        if sigma_down > 0:\n            denoised = denoised + noise_sampler(x, sigma_down) * s_noise\n        if 'grad' not in disable:\n            denoised = model(denoised, **extra_args)\n        if 'grad' not in disable:\n            grad = to_d(x, sigma_up, denoised)\n            x = x + sigma_up * grad\n        if 'callback' not in disable and callback is not None:\n            callback(x, denoised, sigma)\n        if 'noise' not in disable:\n            x = x + sigma * noise_sampler(x, sigma) * s_noise\n    return x\n", "CodeBleu_score": 35.72884632697766}, {"code": "def sample_euler_ancestral(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None)\n\t\n    if extra_args is None:\n        extra_args = {}\n    if noise_sampler is None:\n        noise_sampler = default_noise_sampler(x)\n    if disable is None:\n        disable = []\n    x = x.clone().detach()\n    x.requires_grad_(True)\n    for sigma_from, sigma_to in zip(sigmas[:-1], sigmas[1:]):\n        if sigma_to == 0:\n            continue\n        denoised = model(x, sigma_to, **extra_args)\n        sigma_down, sigma_up = get_ancestral_step(sigma_from, sigma_to, eta=eta)\n        if sigma_up:\n            x += to_d(x, sigma_up, denoised)\n        if sigma_down:\n            x -= to_d(x, sigma_down, denoised)\n        if sigma_up or sigma_down:\n            x += noise_sampler(s_noise * sigma_down, s_noise * sigma_up)\n        if callback:\n            callback(x, sigma_from, sigma_to, denoised, disable=disable)\n    return x\n", "CodeBleu_score": 36.660272696296715}, {"code": "def sample_euler_ancestral(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None)\n\t\n    if extra_args is None:\n        extra_args = []\n    if noise_sampler is None:\n        noise_sampler = default_noise_sampler\n    if disable is None:\n        disable = []\n    if not isinstance(sigmas, torch.Tensor):\n        sigmas = torch.tensor(sigmas)\n    if not isinstance(x, torch.Tensor):\n        x = torch.tensor(x)\n    if not isinstance(extra_args, list):\n        extra_args = [extra_args]\n    if not isinstance(disable, list):\n        disable = [disable]\n    x = x.to(sigmas.device)\n    extra_args = [arg.to(sigmas.device) if isinstance(arg, torch.Tensor) else arg for arg in extra_args]\n    if not isinstance(s_noise, torch.Tensor):\n        s_noise = torch.tensor(s_noise)\n    s_noise = s_noise.to(sigmas.device)\n    with torch.no_grad():\n        for sigma in sigmas:\n            if sigma == 0.:\n                continue\n            if callback is not None:\n                callback(x, sigma)\n            if \"Denoising\" not in disable:\n                denoised = model(x, sigma, *extra_args)\n            if \"Ancestral\" not in disable:\n                sigma_down, sigma_up = get_ancestral_step(sigma, s_noise, eta=eta)\n                noise = noise_sampler(x)(sigma_down, sigma_up)\n                if \"Denoising\" not in disable:\n                    x = x + to_d(x, sigma, denoised) * sigma_up + noise\n                else:\n                    x = x + noise\n            if \"Denoising\" not in disable:\n                x = denoised\n    if \"Denoising\" not in disable:\n        x = x + s_noise * torch.randn_like(x)\n    return x.cpu().numpy()\n\n", "CodeBleu_score": 30.603641293405698}]}
{"instance_id": "6261ded88df2", "generation_result": [{"code": "def linear_multistep_coeff(order, t, i, j)\n\t\n    if order > i:\n        raise ValueError(\"Order is too high for the step index.\")\n    elif order == i:\n        return 1\n    else:\n        def product_term(k):\n            if k == i:\n                return 0\n            else:\n                return t[i+1-k] / (t[i+1] - t[i+1-k])\n        return integrate.quad(product_term, i, j)[0]\n\n", "CodeBleu_score": 27.476603888268965}, {"code": "def linear_multistep_coeff(order, t, i, j)\n\t\n    if order < 0:\n        raise ValueError(\"Order must be non-negative.\")\n    elif order > i:\n        raise ValueError(\"Order must be less than or equal to the step index.\")\n    elif order == 0:\n        return 1\n    else:\n        def nested_function(t):\n            return (t - t[i]) / (t[i + 1] - t[i]) * nested_function(t[1:i])\n        return integrate.quad(nested_function, t[i], t[i + 1])[0]\n\n", "CodeBleu_score": 27.13759485962249}, {"code": "def linear_multistep_coeff(order, t, i, j)\n\t\n    def prod_term(t, i, j):\n        \"\"\"The function computes the product term for the coefficient, excluding the j-th term.\"\"\"\n        if j == 0:\n            return 1\n        elif j == i:\n            return t[i] - t[i-1]\n        else:\n            return t[i] - t[i-1] * prod_term(t, i-1, j-1) / prod_term(t, i-1, j)\n    if order < 0:\n        raise ValueError(\"Order must be non-negative.\")\n    elif order == 0:\n        return 1\n    elif order == 1:\n        return (t[i+1] - t[i]) / (t[i+1] - t[i-1])\n    else:\n        return integrate.quad(lambda t: prod_term(t, i, order), t[i], t[i+1])[0] / prod_term(t, i, order)\n\n", "CodeBleu_score": 25.374814242877168}]}
{"instance_id": "4ee025e33ab1", "generation_result": [{"code": "def sample_lcm(model, x, sigmas, extra_args=None, callback=None, disable=None, noise_sampler=None)\n\t\n    # Initialize extra arguments\n    if extra_args is None:\n        extra_args = {}\n\n    # Initialize noise sampler\n    if noise_sampler is None:\n        noise_sampler = default_noise_sampler\n\n    # Initialize disable\n    if disable is None:\n        disable = []\n\n    # Initialize sigma\n    sigma = sigmas[0]\n\n    # Initialize x_next\n    x_next = x\n\n    # Iterate over sigmas\n    for sigma_next in sigmas[1:]:\n        # Update x_next\n        x_next = model(x_next, sigma, **extra_args)\n\n        # Update x_next with denoised values\n        for name, module in x_next.named_modules():\n            if not any(name.startswith(d) for d in disable):\n                if hasattr(module, 'weight'):\n                    x_next[name] = module.weight + noise_sampler(x_next[name], sigma_next) * sigma_next\n\n        # Execute callback\n        if callback is not None:\n            callback(x_next, sigma, sigma_next)\n\n        # Update sigma\n        sigma = sigma_next\n\n    return x_next\n\n", "CodeBleu_score": 35.59284279028249}, {"code": "def sample_lcm(model, x, sigmas, extra_args=None, callback=None, disable=None, noise_sampler=None)\n\t\n    if extra_args is None:\n        extra_args = {}\n    if noise_sampler is None:\n        noise_sampler = default_noise_sampler(x)\n    if disable is None:\n        disable = []\n    if not isinstance(disable, list):\n        disable = [disable]\n    if 'denoise' not in disable:\n        disable.append('denoise')\n    if 'noise_scale' not in disable:\n        disable.append('noise_scale')\n    if 'noise_scale_next' not in disable:\n        disable.append('noise_scale_next')\n    if 'noise_scale_next_prev' not in disable:\n        disable.append('noise_scale_next_prev')\n    if 'noise_scale_next_prev_prev' not in disable:\n        disable.append('noise_scale_next_prev_prev')\n    if 'noise_scale_next_prev_prev_prev' not in disable:\n        disable.append('noise_scale_next_prev_prev_prev')\n    if 'noise_scale_next_prev_prev_prev_prev' not in disable:\n        disable.append('noise_scale_next_prev_prev_prev_prev')\n    if 'noise_scale_next_prev_prev_prev_prev_prev' not in disable:\n        disable.append('noise_scale_next_prev_prev_prev_prev_prev')\n    if 'noise_scale_next_prev_prev_prev_prev_prev_prev' not in disable:\n        disable.append('noise_scale_next_prev_prev_prev_prev_prev_prev')\n    if 'noise_scale_next_prev_prev_prev_prev_prev_prev_prev' not in disable:\n        disable.append('noise_scale_next_prev_prev_prev_prev_prev_prev_prev')\n    if 'noise_scale_next_prev_prev_prev_prev_prev_prev_prev_prev' not in disable:\n        disable.append('noise_scale_next_prev_prev_prev_prev_prev_prev_prev_prev')\n    if 'noise_scale_next_prev_prev_prev_prev_prev_prev_prev_prev_prev' not in disable:\n        disable.append('noise_scale_next_prev_prev_prev_prev_prev_prev_", "CodeBleu_score": 18.657322613428253}, {"code": "def sample_lcm(model, x, sigmas, extra_args=None, callback=None, disable=None, noise_sampler=None)\n\t\n    # Initialize extra arguments\n    if extra_args is None:\n        extra_args = {}\n\n    # Initialize noise sampler\n    if noise_sampler is None:\n        noise_sampler = default_noise_sampler\n\n    # Initialize disable\n    if disable is None:\n        disable = []\n\n    # Initialize input\n    x_cur = x.clone()\n\n    # Iterate over sigmas\n    for i, sigma in enumerate(sigmas):\n        # Execute callback\n        if callback is not None:\n            callback(x_cur, i, len(sigmas))\n\n        # Apply model\n        x_cur = model(x_cur, **extra_args)\n\n        # Denoise\n        x_cur = x_cur + noise_sampler(x_cur, sigma)\n\n        # Apply noise scaling\n        if i < len(sigmas) - 1 and sigmas[i + 1] > 0:\n            x_cur = x_cur / sigmas[i + 1]\n\n    # Return final value\n    return x_cur\n\n", "CodeBleu_score": 33.796641710370544}]}
{"instance_id": "cb51f91da606", "generation_result": [{"code": "def append_dims(x, target_dims)\n\t\n    x_dims = len(x.shape)\n    diff = target_dims - x_dims\n    if diff < 0:\n        raise ValueError(f\"The target dimensions ({target_dims}) are fewer than the current dimensions ({x_dims})\")\n    if diff == 0:\n        return x.clone().detach() if x.device.type == \"mps\" else x\n    return x.expand(*x.shape, *[1] * diff)\n\n", "CodeBleu_score": 31.1655975584493}, {"code": "def append_dims(x, target_dims)\n\t\n    x_dims = len(x.shape)\n    if x_dims == target_dims:\n        return x\n    elif x_dims > target_dims:\n        raise ValueError(f'The target dimensions ({target_dims}) must be greater than the current dimensions ({x_dims}).')\n    else:\n        if x.device.type == 'mps':\n            return x.detach().expand(*x.shape, *[1]*(target_dims-x_dims))\n        else:\n            return x.expand(*x.shape, *[1]*(target_dims-x_dims))\n\n", "CodeBleu_score": 22.802358464731267}, {"code": "def append_dims(x, target_dims)\n\t\n    if x.dim() == target_dims:\n        return x\n    elif x.dim() > target_dims:\n        raise ValueError(f\"The tensor has too many dimensions ({x.dim()}) for the target dimensions ({target_dims}).\")\n    else:\n        return x.unsqueeze(tuple(range(x.dim(), target_dims))).clone().detach()\n\n", "CodeBleu_score": 21.528759309028203}]}
{"instance_id": "9fac874a3bc7", "generation_result": [{"code": "def download_file(path, url, digest=None)\n\t\n    path = pathlib.Path(path)\n    path.parent.mkdir(parents=True, exist_ok=True)\n    if path.exists() and (digest is None or hash_file(path) == digest):\n        return path\n    else:\n        print(\"Downloading\", url, \"to\", path)\n        urllib.request.urlretrieve(url, path)\n        if digest is not None and hash_file(path) != digest:\n            raise RuntimeError(\"Downloaded file hash does not match expected hash\")\n        return path\n\n", "CodeBleu_score": 33.68879846281098}, {"code": "def download_file(path, url, digest=None)\n\t\n    if not os.path.exists(path):\n        os.makedirs(path)\n    if not os.path.exists(path):\n        raise FileNotFoundError(f\"The target directory {path} does not exist.\")\n    if not os.path.isfile(path):\n        raise FileNotFoundError(f\"The target path {path} is not a file.\")\n    if digest is not None:\n        if not isinstance(digest, str):\n            raise TypeError(f\"The provided digest must be a string, not {type(digest)}.\")\n        if not len(digest) == 64:\n            raise ValueError(f\"The provided digest must be 64 characters long, not {len(digest)}.\")\n        if not all(c in string.hexdigits for c in digest):\n            raise ValueError(f\"The provided digest must be composed of hexadecimal characters, not {digest}.\")\n    if not os.path.exists(path):\n        print(f\"Downloading {url} to {path}.\")\n        urllib.request.urlretrieve(url, path)\n    if digest is not None:\n        if not file_hash(path) == digest:\n            raise ValueError(f\"The downloaded file at {path} has an invalid SHA-256 hash.\")\n    return path\n\n", "CodeBleu_score": 26.928342973261643}, {"code": "def download_file(path, url, digest=None)\n\t\n    if not os.path.exists(path):\n        os.makedirs(os.path.dirname(path), exist_ok=True)\n        with urllib.request.urlopen(url) as response, open(path, \"wb\") as out_file:\n            shutil.copyfileobj(response, out_file)\n        if digest is not None:\n            if not verify_file_hash(path, digest):\n                raise RuntimeError(\n                    f\"Hash mismatch for {path}, expected {digest}, got {file_hash(path)}\"\n                )\n    return path\n\n", "CodeBleu_score": 44.551644929787756}]}
{"instance_id": "f23165e897ce", "generation_result": [{"code": "def train_mode(model, mode=True)\n\t\n\n    # Save current training modes of all modules\n    training_modes = []\n    for module in model.modules():\n        training_modes.append(module.training)\n\n    # Set model to training mode\n    model.train(mode)\n\n    try:\n        yield\n    finally:\n        # Restore training modes of all modules\n        for module, training_mode in zip(model.modules(), training_modes):\n            module.train(training_mode)\n\n", "CodeBleu_score": 35.50659159372793}, {"code": "def train_mode(model, mode=True)\n\t\n    if mode:\n        training_modes = {}\n        for module in model.modules():\n            training_modes[module] = module.training\n        for module in training_modes:\n            module.train(mode)\n        yield\n    else:\n        for module in model.modules():\n            module.train(training_modes[module])\n\n", "CodeBleu_score": 32.977278883645646}, {"code": "def train_mode(model, mode=True)\n\t\n    prev_modes = {}\n    for name, module in model.named_modules():\n        prev_modes[name] = module.training\n    try:\n        model.train(mode)\n        yield\n    finally:\n        for name, module in model.named_modules():\n            module.training = prev_modes[name]\n\n", "CodeBleu_score": 34.945835071273514}]}
{"instance_id": "c6d59c6cb8e1", "generation_result": [{"code": "def ema_update(model, averaged_model, decay)\n\t\n    for current_params, averaged_params in zip(model.parameters(), averaged_model.parameters()):\n        current_params.data.mul_(decay).add_(averaged_params.data, alpha=1 - decay)\n\n    for current_buffer, averaged_buffer in zip(model.buffers(), averaged_model.buffers()):\n        current_buffer.data.copy_(averaged_buffer.data)\n\n", "CodeBleu_score": 30.14073651009257}, {"code": "def ema_update(model, averaged_model, decay)\n\t\n    for ema_param, param in zip(averaged_model.parameters(), model.parameters()):\n        ema_param.data.mul_(decay).add_(param.data, alpha=1 - decay)\n\n    for ema_buffer, buffer in zip(averaged_model.buffers(), model.buffers()):\n        ema_buffer.data.copy_(buffer.data)\n\n", "CodeBleu_score": 27.833169153262077}, {"code": "def ema_update(model, averaged_model, decay)\n\t\n    for current_param, averaged_param in zip(model.parameters(), averaged_model.parameters()):\n        if current_param.dtype != torch.float64:\n            current_param = current_param.float()\n        if averaged_param.dtype != torch.float64:\n            averaged_param = averaged_param.float()\n        current_param_state = dict(current_param.state_dict())\n        for k, v in current_param_state.items():\n            if v is not None:\n                v = v.to(dtype=torch.float64)\n            if averaged_param.dtype != v.dtype:\n                v = v.to(dtype=averaged_param.dtype)\n            v_t = decay * averaged_param[k] + (1 - decay) * v\n            averaged_param[k] = v_t.to(averaged_param[k].dtype)\n    for current_buffer, averaged_buffer in zip(model.buffers(), averaged_model.buffers()):\n        if current_buffer.dtype != torch.float64:\n            current_buffer = current_buffer.float()\n        if averaged_buffer.dtype != torch.float64:\n            averaged_buffer = averaged_buffer.float()\n        current_buffer_state = dict(current_buffer.state_dict())\n        for k, v in current_buffer_state.items():\n            if v is not None:\n                v = v.to(dtype=torch.float64)\n            if averaged_buffer.dtype != v.dtype:\n                v = v.to(dtype=averaged_buffer.dtype)\n            v_t = decay * averaged_buffer[k] + (1 - decay) * v\n            averaged_buffer[k] = v_t.to(averaged_buffer[k].dtype)\n\n", "CodeBleu_score": 36.17575829968532}]}
{"instance_id": "600c39a30798", "generation_result": [{"code": "def rand_log_logistic(shape, loc=0., scale=1., min_value=0., max_value=float('inf'), device='cpu', dtype=torch.float32)\n\t\n    # Convert min and max values to tensors\n    min_value = torch.tensor(min_value, device=device, dtype=dtype)\n    max_value = torch.tensor(max_value, device=device, dtype=dtype)\n    # Compute CDF values\n    min_cdf = logit(min_value)\n    max_cdf = logit(max_value)\n    # Generate uniform random values\n    rand_uniform = torch.rand(shape, device=device, dtype=dtype)\n    # Transform uniform random values to samples\n    rand_log_logistic = torch.exp(torch.log(rand_uniform) * (max_cdf - min_cdf) + min_cdf)\n    # Scale samples\n    rand_log_logistic = rand_log_logistic * scale + loc\n    return rand_log_logistic\n\n", "CodeBleu_score": 35.33254692089544}, {"code": "def rand_log_logistic(shape, loc=0., scale=1., min_value=0., max_value=float('inf'), device='cpu', dtype=torch.float32)\n\t\n    min_value = torch.tensor(min_value, device=device, dtype=dtype)\n    max_value = torch.tensor(max_value, device=device, dtype=dtype)\n    cdf_min = torch.sigmoid(min_value - loc)\n    cdf_max = torch.sigmoid(max_value - loc)\n    u = torch.rand(shape, device=device, dtype=dtype)\n    samples = torch.log(u * (cdf_max - cdf_min) + cdf_min) + loc\n    return samples\n\n", "CodeBleu_score": 32.34616169679027}, {"code": "def rand_log_logistic(shape, loc=0., scale=1., min_value=0., max_value=float('inf'), device='cpu', dtype=torch.float32)\n\t\n    if min_value > max_value:\n        raise ValueError('The minimum value must be less than or equal to the maximum value.')\n    min_value = torch.tensor(min_value, device=device, dtype=dtype)\n    max_value = torch.tensor(max_value, device=device, dtype=dtype)\n    min_cdf = logit(cdf(min_value, loc=loc, scale=scale))\n    max_cdf = logit(cdf(max_value, loc=loc, scale=scale))\n    cdf_range = max_cdf - min_cdf\n    uniform_samples = torch.rand(shape, device=device, dtype=dtype)\n    samples = torch.exp(logit(uniform_samples * cdf_range + min_cdf)) * scale + loc\n    return samples\n\n", "CodeBleu_score": 39.2288712760393}]}
{"instance_id": "75e2def2467f", "generation_result": [{"code": "def get_activation(activation: Literal[\"elu\", \"snake\", \"none\"], antialias=False, channels=None) -> nn.Module\n\t\n    if activation == \"elu\":\n        if antialias:\n            return nn.Sequential(\n                Activation1d(nn.ELU()),\n                GaussianBlur1d(channels, 3, 1.5),\n            )\n        else:\n            return nn.ELU()\n    elif activation == \"snake\":\n        return SnakeBeta(channels)\n    elif activation == \"none\":\n        return nn.Identity()\n    else:\n        raise ValueError(f\"Invalid activation type: {activation}\")\n\n", "CodeBleu_score": 42.723675926323864}, {"code": "def get_activation(activation: Literal[\"elu\", \"snake\", \"none\"], antialias=False, channels=None) -> nn.Module\n\t\n    if activation == \"elu\":\n        activation = nn.ELU()\n    elif activation == \"snake\":\n        activation = SnakeBeta(channels)\n    elif activation == \"none\":\n        activation = nn.Identity()\n    else:\n        raise ValueError(f\"Unknown activation type: {activation}\")\n    if antialias:\n        activation = Activation1d(activation)\n    return activation\n\n", "CodeBleu_score": 61.77557278257606}, {"code": "def get_activation(activation: Literal[\"elu\", \"snake\", \"none\"], antialias=False, channels=None) -> nn.Module\n\t\n    if activation == \"elu\":\n        act = nn.ELU()\n    elif activation == \"snake\":\n        act = SnakeBeta(channels)\n    elif activation == \"none\":\n        act = nn.Identity()\n    else:\n        raise ValueError(\"Activation type must be one of: 'elu', 'snake', or 'none'.\")\n    if antialias:\n        act = Activation1d(act)\n    return act\n\n", "CodeBleu_score": 67.84417724388547}]}
{"instance_id": "ef1714c79e51", "generation_result": [{"code": "def apply_rotary_pos_emb(t, freqs, scale = 1)\n\t\n    t = t.to(torch.float32) if t.dtype == torch.float16 else t\n    t = rearrange(t, '... (j d) -> ... j d', j = 2)\n    freqs = rearrange(freqs, '... (j d) -> ... j d', j = 2)\n    freqs = freqs * scale\n    freqs = freqs.to(torch.float32) if freqs.dtype == torch.float16 else freqs\n    freqs = torch.stack((-freqs, freqs), dim = -2)\n    return torch.cat((t, t), dim = -2) * freqs\n\n", "CodeBleu_score": 16.90131342257467}, {"code": "def apply_rotary_pos_emb(t, freqs, scale = 1)\n\t\n    if not isinstance(freqs, torch.Tensor):\n        freqs = torch.tensor(freqs)\n    if t.dtype != torch.float32:\n        t = t.to(torch.float32)\n    t, freqs = map(lambda t: rearrange(t, '... -> () ...'), (t, freqs))\n    t, freqs = map(lambda t: t.to(t.device), (t, freqs))\n    half = t.shape[-1] // 2\n    t1, t2 = t[..., :half], t[..., half:]\n    sinu = (t1 * freqs).sin()\n    cosu = (t1 * freqs).cos()\n    sinus, cosus = map(lambda t: rearrange(t, 'i -> () i () ()'), (sinu, cosu))\n    t = torch.cat((sinus, cosus), dim = -1)\n    t = rearrange(t, '() i j k -> i (j k)')\n    t = apply_rotary_emb(t, scale = scale)\n    t = rearrange(t, 'i (j k) -> () i j k', j = 2)\n    sin, cos = t.unbind(dim = -3)\n    sin, cos = map(lambda t: rearrange(t, 'i j -> i () j ()'), (sin, cos))\n    t = torch.cat((sin, cos), dim = -2)\n    t = rearrange(t, '() ... d -> ... d')\n    t = torch.cat((t1, t2), dim = -1)\n    return t\n\n", "CodeBleu_score": 39.32019637726774}, {"code": "def apply_rotary_pos_emb(t, freqs, scale = 1)\n\t\n    if t.dtype != torch.float32:\n        t = t.to(torch.float32)\n    t = rearrange(t, '... (j d) -> ... j d', j = 2)\n    freqs = rearrange(freqs, '... (j d) -> ... j d', j = 2)\n    freqs = freqs * scale\n    t, t_pass = t[..., :1], t[..., 1:]\n    t = (t * freqs.cos()) + (rotate_half(t) * freqs.sin())\n    return torch.cat((t, t_pass), dim = -2)\n\n", "CodeBleu_score": 31.891007951805417}]}
{"instance_id": "a7a5f178c6f6", "generation_result": [{"code": "def idx2vq(self, idx, dim=-1)\n\t\n    return self.vq_embeddings(idx).movedim(1, dim)\n\n", "CodeBleu_score": 18.332692797594518}, {"code": "def idx2vq(self, idx, dim=-1)\n\t\n    return torch.index_select(self.vq_embedding, dim, idx)\n\n", "CodeBleu_score": 18.379391324462553}, {"code": "def idx2vq(self, idx, dim=-1)\n\t\n    return self.vq_layer.idx2vq(idx, dim)\n\n", "CodeBleu_score": 15.91911961373264}]}
{"instance_id": "4104c041fb20", "generation_result": [{"code": "def encode(self, x, quantize=False)\n\t\n    for module in self.children():\n        x = module(x)\n    if quantize:\n        z_q_x, z_e_x, indices, loss = self.quantize(x)\n        return z_q_x, z_e_x, indices, loss\n    else:\n        return x\n\n", "CodeBleu_score": 32.53898019001259}, {"code": "def encode(self, x, quantize=False)\n\t\n    x = self.conv1(x)\n    x = self.bn1(x)\n    x = self.relu(x)\n    x = self.maxpool(x)\n\n    x = self.layer1(x)\n    x = self.layer2(x)\n    x = self.layer3(x)\n    x = self.layer4(x)\n\n    x = self.avgpool(x)\n    x = torch.flatten(x, 1)\n\n    x = self.fc(x)\n    if quantize:\n        z_q_x, (vq_loss, commit_loss), indices = self.quantize(x)\n        return z_q_x, z_q_x, indices, vq_loss, commit_loss\n    return x\n\n", "CodeBleu_score": 31.66706869863155}, {"code": "def encode(self, x, quantize=False)\n\t\n    x = self.encoder(x)\n    if quantize:\n        z_q_x, loss, indices = self.quantize(x)\n        return z_q_x, x, indices, loss\n    return x\n", "CodeBleu_score": 20.554364142956803}]}
{"instance_id": "403c52ba2d3b", "generation_result": [{"code": "def ema_scope(self, context=None)\n\t\n    if not hasattr(self, 'ema'):\n        self.ema = EMA(self)\n    if context is None:\n        return self.ema.store(self.parameters())\n    else:\n        return context(self.ema)\n", "CodeBleu_score": 24.77356371594577}, {"code": "def ema_scope(self, context=None)\n\t\n    if context is None:\n        context = Context()\n    context.store(self.parameters())\n    context.copy_to(self)\n    yield\n    context.restore(self.parameters())\n    if context.logger is not None:\n        context.logger.info(\"Restored parameters.\")\n\n", "CodeBleu_score": 32.78557903752546}, {"code": "def ema_scope(self, context=None)\n\t\n    if context is None:\n        context = self.ema\n    with context:\n        yield\n", "CodeBleu_score": 14.153464128538623}]}
{"instance_id": "94b7467f98e4", "generation_result": [{"code": "def post_attention(self, attn, x, gate_msa, shift_mlp, scale_mlp, gate_mlp)\n\t\n    x = x + attn * gate_msa\n    x = self.norm(x)\n    x = modulate(x, shift_mlp, scale_mlp)\n    x = self.mlp(x)\n    return x * gate_mlp\n\n", "CodeBleu_score": 36.09765227147497}, {"code": "def post_attention(self, attn, x, gate_msa, shift_mlp, scale_mlp, gate_mlp)\n\t\n    assert not self.pre_only\n    x = self.proj(x)\n    x = self.proj_drop(x)\n    x = x + attn * gate_msa\n    x = x * (1 + scale_mlp(x)) + shift_mlp(x)\n    x = x * gate_mlp(x)\n    return x\n", "CodeBleu_score": 47.54714348576873}, {"code": "def post_attention(self, attn, x, gate_msa, shift_mlp, scale_mlp, gate_mlp)\n\t\n    x = x + gate_msa.unsqueeze(1) * attn\n    x = self.norm(x)\n    x = modulate(x, shift_mlp, scale_mlp)\n    x = self.mlp(x)\n    return x * gate_mlp.unsqueeze(1)\n", "CodeBleu_score": 46.420758000961094}]}
{"instance_id": "d9fdc05542a2", "generation_result": [{"code": "def _block_mixing(context, x, context_block, x_block, c)\n\t\n    context_qkv = context_block.pre_attention(context)\n    x_qkv = x_block.pre_attention(x)\n    if c.context_block.pre_norm:\n        context = context_block.pre_norm(context)\n        x = x_block.pre_norm(x)\n    context_qkv = torch.cat(context_qkv, dim=2)\n    x_qkv = torch.cat(x_qkv, dim=2)\n    context_qkv = optimized_attention(context_qkv, c.context_block.num_heads)\n    x_qkv = optimized_attention(x_qkv, c.x_block.num_heads)\n    context_qkv = torch.split(context_qkv, context_qkv.shape[2] // 3, dim=2)\n    x_qkv = torch.split(x_qkv, x_qkv.shape[2] // 3, dim=2)\n    if c.context_block.post_norm:\n        context = context_block.post_norm(context)\n        x = x_block.post_norm(x)\n    context = context_block.post_attention(context)\n    x = x_block.post_attention(x)\n    return (context, x)\n", "CodeBleu_score": 22.42001691320745}, {"code": "def _block_mixing(context, x, context_block, x_block, c)\n\t\n    context_qkv = context_block.pre_attention(context)\n    x_qkv = x_block.pre_attention(x)\n    qkv = torch.cat((context_qkv[0], x_qkv[0]), dim=2)\n    context_qkv_2, x_qkv_2 = optimized_attention(qkv, c.num_heads), torch.cat((context_qkv[1], x_qkv[1]), dim=2)\n    context_qkv_2, x_qkv_2 = context_qkv_2.reshape(context_qkv_2.shape[0], context_qkv_2.shape[1], c.num_heads, -1), x_qkv_2.reshape(x_qkv_2.shape[0], x_qkv_2.shape[1], c.num_heads, -1)\n    context_qkv_2, x_qkv_2 = context_qkv_2.permute(0, 2, 1, 3), x_qkv_2.permute(0, 2, 1, 3)\n    context_qkv_2, x_qkv_2 = torch.cat((context_qkv_2[0], context_qkv_2[1]), dim=2), torch.cat((x_qkv_2[0], x_qkv_2[1]), dim=2)\n    context_qkv_2, x_qkv_2 = split_qkv(context_qkv_2, c.head_dim), split_qkv(x_qkv_2, c.head_dim)\n    context, x = context_block.post_attention(context_qkv_2), x_block.post_attention(x_qkv_2)\n    return (context, x)\n", "CodeBleu_score": 19.3414233391841}, {"code": "def _block_mixing(context, x, context_block, x_block, c)\n\t\n    if context_block.pre_only:\n        context = context_block.pre_attention(context)\n        x = x_block.pre_attention(x)\n        qkv = (context, x)\n    else:\n        context = context_block.pre_attention(context)\n        x = x_block.pre_attention(x)\n        qkv = (context, x)\n        qkv = optimized_attention(qkv, context_block.num_heads)\n        qkv = (qkv[0].reshape(context.shape), qkv[1].reshape(x.shape))\n    if context_block.pre_only:\n        context = context_block.post_attention(context)\n        x = x_block.post_attention(x)\n    else:\n        qkv = (qkv[0].reshape(context.shape), qkv[1].reshape(x.shape))\n        qkv = (qkv[0] + c, qkv[1])\n        context = context_block.post_attention(qkv[0])\n        x = x_block.post_attention(qkv[1])\n    return context, x\n", "CodeBleu_score": 32.559841003977446}]}
{"instance_id": "b219bce30160", "generation_result": [{"code": "def get_timestep_embedding(timesteps, embedding_dim)\n\t\n    assert len(timesteps.shape) == 1  # and timesteps.dtype == tf.int32\n    half_dim = embedding_dim // 2\n    emb = math.log(10000) / (half_dim - 1)\n    emb = tf.exp(tf.range(half_dim, dtype=tf.float32) * -emb)\n    emb = tf.cast(timesteps, dtype=tf.float32)[:, None] * emb[None, :]\n    emb = tf.concat([tf.sin(emb), tf.cos(emb)], axis=1)\n    if embedding_dim % 2 == 1:\n        emb = tf.pad(emb, [[0, 0], [0, 1]])\n    assert emb.shape == [timesteps.shape[0], embedding_dim]\n    return emb\n\n", "CodeBleu_score": 69.24672013994488}, {"code": "def get_timestep_embedding(timesteps, embedding_dim)\n\t\n    assert len(timesteps.shape) == 1  # and timesteps.dtype == tf.int32\n    half_dim = embedding_dim // 2\n    emb = math.log(10000) / (half_dim - 1)\n    emb = tf.exp(tf.range(half_dim, dtype=tf.float32) * -emb)\n    emb = tf.cast(timesteps, dtype=tf.float32)[:, None] * emb[None, :]\n    emb = tf.concat([tf.sin(emb), tf.cos(emb)], axis=1)\n    if embedding_dim % 2 == 1:\n        emb = tf.pad(emb, [[0, 0], [0, 1]])\n    assert emb.shape == [timesteps.shape[0], embedding_dim]\n    return emb\n\n", "CodeBleu_score": 69.24672013994488}, {"code": "def get_timestep_embedding(timesteps, embedding_dim)\n\t\n    assert len(timesteps.shape) == 1  # and timesteps.dtype == tf.int32\n    half_dim = embedding_dim // 2\n    emb = math.log(10000) / (half_dim - 1)\n    emb = torch.exp(torch.arange(half_dim, dtype=torch.float32) * -emb)\n    emb = torch.arange(timesteps.shape[1], dtype=torch.float32).unsqueeze(1) * emb.unsqueeze(0)\n    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n    if embedding_dim % 2 == 1:  # zero pad\n        emb = F.pad(emb, (0,1,0,0))\n    assert emb.shape == (timesteps.shape[1], embedding_dim)\n    return emb\n\n", "CodeBleu_score": 71.66653631466102}]}
{"instance_id": "6ae8453c86be", "generation_result": [{"code": "def pytorch_attention(q, k, v)\n\t\n    try:\n        return torch_attention(q, k, v)\n    except model_management.OOM_EXCEPTION:\n        return slice_attention(q, k, v)\n\n", "CodeBleu_score": 5.09721800980786}, {"code": "def pytorch_attention(q, k, v)\n\t\n\n    q = q.permute(0, 2, 1, 3)\n    k = k.permute(0, 2, 3, 1)\n    v = v.permute(0, 2, 1, 3)\n\n    # Reshape q, k, v for attention computation\n    q = q.reshape(q.shape[0], q.shape[1], q.shape[2] * q.shape[3])\n    k = k.reshape(k.shape[0], k.shape[1], k.shape[2] * k.shape[3])\n    v = v.reshape(v.shape[0], v.shape[1], v.shape[2] * v.shape[3])\n\n    # Compute attention\n    r1 = torch.bmm(q, k)\n    scale = (int(q.shape[-1])**(-0.5))\n    r1 = r1 * scale\n\n    r2 = torch.nn.functional.softmax(r1, dim=2).permute(0,2,1)\n    del r1\n\n    r3 = torch.bmm(v, r2)\n    del r2\n\n    # Reshape r3 back to the original dimensions\n    r3 = r3.reshape(r3.shape[0], r3.shape[1], q.shape[2], q.shape[3])\n    r3 = r3.permute(0, 2, 1, 3)\n\n    return r3\n\n", "CodeBleu_score": 25.6865860883968}, {"code": "def pytorch_attention(q, k, v)\n\t\n    try:\n        q = q.permute(0, 2, 1, 3)\n        k = k.permute(0, 2, 1, 3)\n        v = v.permute(0, 2, 1, 3)\n\n        scale = (int(q.shape[-1])**(-0.5))\n\n        mem_free_total = model_management.get_free_memory(q.device)\n\n        gb = 1024 ** 3\n        tensor_size = q.shape[0] * q.shape[1] * k.shape[2] * q.element_size()\n        modifier = 3 if q.element_size() == 2 else 2.5\n        mem_required = tensor_size * modifier\n        steps = 1\n\n        if mem_required > mem_free_total:\n            steps = 2**(math.ceil(math.log(mem_required / mem_free_total, 2)))\n\n        while True:\n            try:\n                slice_size = q.shape[1] // steps if (q.shape[1] % steps) == 0 else q.shape[1]\n                for i in range(0, q.shape[1], slice_size):\n                    end = i + slice_size\n                    s1 = torch.bmm(q[:, i:end], k) * scale\n\n                    s2 = torch.nn.functional.softmax(s1, dim=2).permute(0,2,1)\n                    del s1\n\n                    r1 = torch.bmm(v, s2)\n                    del s2\n                    if i == 0:\n                        r = r1\n                    else:\n                        r = torch.cat((r, r1), dim=1)\n                break\n            except model_management.OOM_EXCEPTION as e:\n                model_management.soft_empty_cache(True)\n                steps *= 2\n                if steps > 128:\n                    raise e\n                logging.warning(\"out of memory error, increasing steps and trying again {}\".format(steps))\n\n        r = r.permute(0, 2, 1, 3)\n        return r\n\n    except Exception as e:\n        logging.error(\"Exception in attention computation: {}\".format(e))\n        raise e\n\n", "CodeBleu_score": 22.98600951848101}]}
{"instance_id": "fe4433c987ad", "generation_result": [{"code": "def apply_control(h, control, name)\n\t\n    if control in controls and name in controls[control]:\n        try:\n            h = h + controls[control][name]\n        except:\n            logger.warning(\"Failed to apply control adjustment to %s with control %s and name %s.\" % (h, control, name))\n    return h\n\n", "CodeBleu_score": 35.47091595360382}, {"code": "def apply_control(h, control, name)\n\t\n    if control in control_adjustments and name in control_adjustments[control]:\n        adjustment = control_adjustments[control][name]\n        try:\n            h = h + adjustment\n        except:\n            logging.warning(\"Control adjustment failed for {} {}: {}\".format(control, name, adjustment))\n    return h\n\n", "CodeBleu_score": 33.69510611743799}, {"code": "def apply_control(h, control, name)\n\t\n    if control and name:\n        if control[name]:\n            try:\n                h += control[name]\n            except:\n                logging.warning(\n                    f\"Could not apply control adjustment {name} to {h}.\"\n                )\n        control.pop(name, None)\n    return h\n\n", "CodeBleu_score": 36.069461827324}]}
{"instance_id": "56da4547c101", "generation_result": [{"code": "def get_codebook_entry(self, indices, shape)\n\t\n    if self.verbose:\n        print(\"Getting codebook entry...\")\n    if indices.ndim == 1:\n        indices = np.expand_dims(indices, axis=0)\n    if shape is not None:\n        if self.verbose:\n            print(\"Reshaping codebook entry...\")\n        indices = np.reshape(indices, shape)\n        indices = np.transpose(indices, (0, 3, 1, 2))\n    if self.verbose:\n        print(\"Getting codebook entry...Done\")\n    return indices\n\n", "CodeBleu_score": 31.041206526170896}, {"code": "def get_codebook_entry(self, indices, shape)\n\t\n    indices = tf.cast(indices, tf.int32)\n    one_hot = tf.one_hot(indices, self.codebook_size)\n    quantized = tf.matmul(one_hot, self.codebook)\n    if shape is not None:\n        quantized = tf.reshape(quantized, shape)\n        quantized = tf.transpose(quantized, [0, 3, 1, 2])\n    return quantized\n\n", "CodeBleu_score": 31.430693705480444}, {"code": "def get_codebook_entry(self, indices, shape)\n\t\n    if self.codebook is None:\n        raise RuntimeError(\"The codebook is not yet initialized.\")\n    indices = tf.cast(indices, tf.int32)\n    one_hot_indices = tf.one_hot(indices, self.codebook.shape[0])\n    quantized_vectors = tf.matmul(one_hot_indices, self.codebook)\n    if shape is not None:\n        quantized_vectors = tf.reshape(quantized_vectors, shape)\n        quantized_vectors = tf.transpose(quantized_vectors, [0, 3, 1, 2])\n    return quantized_vectors\n\n", "CodeBleu_score": 37.31731455395831}]}
{"instance_id": "548f8a1fb37d", "generation_result": [{"code": "def make_noise(self)\n\t\n    # make a list of noise tensors\n    noise = []\n    # initialize the list with a noise tensor of size 4x4 on the appropriate device\n    noise.append(torch.zeros(4, 4).to(self.device))\n    # for each size from 2^3 to 2^log_size, append two noise tensors of the corresponding size to the list\n    for i in range(3, self.log_size + 1):\n        for _ in range(2):\n            noise.append(torch.randn(2 ** i, 2 ** i).to(self.device))\n    return noise\n\n", "CodeBleu_score": 39.26400475079219}, {"code": "def make_noise(self)\n\t\n    # Initialize the list with a noise tensor of size 4x4 on the appropriate device\n    noise_list = [torch.zeros(4, 4, device=self.device)]\n    # For each size from 2^3 to 2^log_size, it appends two noise tensors of the corresponding size to the list\n    for size in range(3, self.log_size + 1):\n        for _ in range(2):\n            noise = torch.randn(size, size, device=self.device)\n            noise = torch.mm(noise, noise.t())\n            noise_list.append(noise)\n    return noise_list\n\n", "CodeBleu_score": 39.75921816671765}, {"code": "def make_noise(self)\n\t\n    noise = []\n    noise.append(torch.randn(4, 4, device=self.device))\n    for i in range(self.log_size - 3):\n        size = 4 * 2 ** (i + 3)\n        noise.append(torch.randn(size, size, device=self.device))\n    return noise\n\n", "CodeBleu_score": 28.643411094723042}]}
{"instance_id": "42fd805563c6", "generation_result": [{"code": "def mean_latent(self, num_latent)\n\t\n    latent_in = torch.randn(num_latent, self.latent_dim, device=self.device)\n    latent_out = self.style(latent_in).mean(dim=0, keepdim=True)\n    return latent_out\n\n", "CodeBleu_score": 33.202092191533914}, {"code": "def mean_latent(self, num_latent)\n\t\n    mean_style = self.mean_style(torch.randn(num_latent, self.latent_dim).cuda())\n    return mean_style\n\n", "CodeBleu_score": 16.026751992940362}, {"code": "def mean_latent(self, num_latent)\n\t\n    latent_in = tf.random.normal([num_latent, self.latent_dim])\n    latent_out = self.style(latent_in)\n    latent_out = latent_out.mean(axis=0, keepdims=True)\n    return latent_out\n\n", "CodeBleu_score": 25.257796113710505}]}
{"instance_id": "2f485fd924af", "generation_result": [{"code": "def color_jitter(img, shift)\n\t\n    img = img.astype(np.float32)\n    if shift is None:\n        shift = np.random.uniform(-0.1, 0.1, 3)\n    shift = np.array(shift)\n    img = img + shift\n    img = np.clip(img, 0, 1)\n    return img\n\n", "CodeBleu_score": 55.94547448894088}, {"code": "def color_jitter(img, shift)\n\t\n    # The function takes a numpy array representing an image and a shift value.\n    # It randomly generates a jitter value for each RGB channel and adds these values to the image.\n    # The function then clips the result to ensure pixel values remain between 0 and 1.\n    # The function returns the jittered image.\n    # The function can be used to generate data augmentation.\n    # The function can be used to generate data augmentation.\n    # The function can be used to generate data augmentation.\n    # The function can be used to generate data augmentation.\n    # The function can be used to generate data augmentation.\n    # The function can be used to generate data augmentation.\n    # The function can be used to generate data augmentation.\n    # The function can be used to generate data augmentation.\n    # The function can be used to generate data augmentation.\n    # The function can be used to generate data augmentation.\n    # The function can be used to generate data augmentation.\n    # The function can be used to generate data augmentation.\n    # The function can be used to generate data augmentation.\n    # The function can be used to generate data augmentation.\n    # The function can be used to generate data augmentation.\n    # The function can be used to generate data augmentation.\n    # The function can be used to generate data augmentation.\n    # The function can be used to generate data augmentation.\n    # The function can be used to generate data augmentation.\n    # The function can be used to generate data augmentation.\n    # The function can be used to generate data augmentation.\n    # The function can be used to generate data augmentation.\n    # The function can be used to generate data augmentation.\n    # The function can be used to generate data augmentation.\n    # The function can be used to generate data augmentation.\n    # The function can be used to generate data augmentation.\n    # The function can be used to generate data augmentation.\n    # The function can be used to generate data augmentation.\n    # The function can be used to generate data augmentation.\n    # The function can be used to generate data augmentation.\n    # The function can be used to generate data augmentation.\n    # The function can be used to generate data augmentation.\n    # The function can be used to generate data augmentation.\n    # The function can be used to generate data augmentation.\n   ", "CodeBleu_score": 2.393018018018018}, {"code": "def color_jitter(img, shift)\n\t\n    # Convert image to HSV color space\n    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n\n    # Generate random jitter values for each RGB channel\n    jitter_val = np.random.uniform(-shift, shift, 3)\n\n    # Add jitter values to the image\n    hsv[:, :, 0] = (hsv[:, :, 0] + jitter_val[0]) % 180\n    hsv[:, :, 1] = np.clip(hsv[:, :, 1] + jitter_val[1], 0, 255)\n    hsv[:, :, 2] = np.clip(hsv[:, :, 2] + jitter_val[2], 0, 255)\n\n    # Convert back to BGR color space and return jittered image\n    return cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n\n", "CodeBleu_score": 22.073515372370274}]}
{"instance_id": "c87ccf1fbf48", "generation_result": [{"code": "def feed_data(self, data)\n\t\n    self.lq = data['lq'].to(self.device)\n    if 'gt' in data:\n        self.gt = data['gt'].to(self.device)\n    if 'left_eye' in data:\n        self.left_eye = data['left_eye'].to(self.device)\n    if 'right_eye' in data:\n        self.right_eye = data['right_eye'].to(self.device)\n    if 'mouth' in data:\n        self.mouth = data['mouth'].to(self.device)\n\n", "CodeBleu_score": 56.77114126112309}, {"code": "def feed_data(self, data)\n\t\n    if data.get('lq', None) is not None:\n        self.lq = data['lq'].to(self.device)\n    if data.get('gt', None) is not None:\n        self.gt = data['gt'].to(self.device)\n    if data.get('left_eye', None) is not None:\n        self.left_eye = data['left_eye'].to(self.device)\n    if data.get('right_eye', None) is not None:\n        self.right_eye = data['right_eye'].to(self.device)\n    if data.get('mouth', None) is not None:\n        self.mouth = data['mouth'].to(self.device)\n\n", "CodeBleu_score": 41.13030882759597}, {"code": "def feed_data(self, data)\n\t\n    if data.get('lq', None) is not None:\n        self.lq = data['lq'].to(self.device)\n    if data.get('gt', None) is not None:\n        self.gt = data['gt'].to(self.device)\n    if data.get('lq_path', None) is not None:\n        self.lq_path = data['lq_path']\n    if data.get('gt_path', None) is not None:\n        self.gt_path = data['gt_path']\n    if data.get('left_eye', None) is not None:\n        self.left_eye = data['left_eye'].to(self.device)\n    if data.get('right_eye', None) is not None:\n        self.right_eye = data['right_eye'].to(self.device)\n    if data.get('mouth', None) is not None:\n        self.mouth = data['mouth'].to(self.device)\n    if data.get('left_eye_path', None) is not None:\n        self.left_eye_path = data['left_eye_path']\n    if data.get('right_eye_path', None) is not None:\n        self.right_eye_path = data['right_eye_path']\n    if data.get('mouth_path', None) is not None:\n        self.mouth_path = data['mouth_path']\n\n", "CodeBleu_score": 38.73428360316032}]}
{"instance_id": "8f8c7cb13037", "generation_result": [{"code": "def construct_img_pyramid(self)\n\t\n    img_pyramid = []\n    img_pyramid.append(self.gt_img)\n    for i in range(self.num_scales):\n        img_pyramid.append(\n            tf.image.resize(img_pyramid[i], (img_pyramid[i].shape[1] // 2, img_pyramid[i].shape[2] // 2),\n                            method=tf.image.ResizeMethod.BILINEAR))\n    return img_pyramid\n\n", "CodeBleu_score": 19.68353121956827}, {"code": "def construct_img_pyramid(self)\n\t\n    img_pyramid = []\n    img_pyramid.append(self.img_true)\n    for i in range(self.num_scales):\n        img_pyramid.append(cv2.resize(img_pyramid[i], (0, 0), fx=0.5, fy=0.5, interpolation=cv2.INTER_LINEAR))\n    return img_pyramid\n\n", "CodeBleu_score": 19.778595396852687}, {"code": "def construct_img_pyramid(self)\n\t\n    img_pyramid = [self.img]\n    for i in range(self.n_scales):\n        img_pyramid.insert(0, cv2.resize(img_pyramid[0], (int(self.img.shape[1]/(2**i)), int(self.img.shape[0]/(2**i))), interpolation = cv2.INTER_LINEAR))\n    return img_pyramid\n\n", "CodeBleu_score": 20.53758424425305}]}
{"instance_id": "51247274d40c", "generation_result": [{"code": "def save(self, epoch, current_iter)\n\t\n    print('Saving the model...')\n    state = {\n        'epoch': epoch,\n        'iter': current_iter,\n        'gen_state_dict': self.gen.state_dict(),\n        'gen_ema_state_dict': self.gen_ema.state_dict(),\n        'dis_state_dict': self.dis.state_dict()\n    }\n    if self.use_facial_disc:\n        state['dis_left_state_dict'] = self.dis_left.state_dict()\n        state['dis_right_state_dict'] = self.dis_right.state_dict()\n        state['dis_mouth_state_dict'] = self.dis_mouth.state_dict()\n    torch.save(state, os.path.join(self.checkpoint_dir, 'checkpoint.pt'))\n    print('Model saved.')\n\n", "CodeBleu_score": 27.18867248901698}, {"code": "def save(self, epoch, current_iter)\n\t\n    save_filename = f'checkpoint_{epoch+1}_{current_iter}.pth'\n    save_path = os.path.join(self.model_dir, save_filename)\n    torch.save({\n        'epoch': epoch,\n        'current_iter': current_iter,\n        'generator_state_dict': self.generator.state_dict(),\n        'generator_ema_state_dict': self.generator_ema.state_dict(),\n        'discriminator_state_dict': self.discriminator.state_dict(),\n        'discriminator_left_state_dict': self.discriminator_left.state_dict(),\n        'discriminator_right_state_dict': self.discriminator_right.state_dict(),\n        'discriminator_mouth_state_dict': self.discriminator_mouth.state_dict(),\n        'optimizer_state_dict': self.optimizer.state_dict(),\n        'scheduler_state_dict': self.scheduler.state_dict(),\n        'best_fid': self.best_fid,\n        'best_fid_checkpoint': self.best_fid_checkpoint,\n        'best_fid_epoch': self.best_fid_epoch,\n        'best_fid_iter': self.best_fid_iter,\n        'best_fid_checkpoint_path': self.best_fid_checkpoint_path,\n        'best_fid_ema_checkpoint_path': self.best_fid_ema_checkpoint_path,\n        'best_fid_ema': self.best_fid_ema,\n        'best_fid_ema_checkpoint': self.best_fid_ema_checkpoint,\n        'best_fid_ema_epoch': self.best_fid_ema_epoch,\n        'best_fid_ema_iter': self.best_fid_ema_iter,\n        'best_fid_ema_checkpoint_path': self.best_fid_ema_checkpoint_path,\n        'best_fid_ema_checkpoint_path': self.best_fid_ema_checkpoint_path,\n        'best_fid_ema_checkpoint_path': self.best_fid_ema_checkpoint_path,\n        'best_fid_ema_checkpoint_path': self.best_fid_ema_checkpoint_path,\n        'best_fid_ema_checkpoint_path': self.best_fid_ema_checkpoint", "CodeBleu_score": 26.290748609787535}, {"code": "def save(self, epoch, current_iter)\n\t\n    # Save generators, discriminators, and optimizers\n    save_models(self, epoch, current_iter)\n    # Save training state\n    save_training_state(self, epoch, current_iter)\n\n", "CodeBleu_score": 12.464647220305181}]}
{"instance_id": "afa0dc8128c1", "generation_result": [{"code": "def clean_folder(folder)\n\t\n    for item in os.listdir(folder):\n        item_path = os.path.join(folder, item)\n        try:\n            if os.path.isfile(item_path):\n                os.unlink(item_path)\n            elif os.path.isdir(item_path):\n                shutil.rmtree(item_path)\n        except Exception as e:\n            print(\"Failed to delete %s. Reason: %s\" % (item_path, e))\n\n", "CodeBleu_score": 45.49565118578772}, {"code": "def clean_folder(folder)\n\t\n    for item in os.listdir(folder):\n        item_path = os.path.join(folder, item)\n        try:\n            if os.path.isfile(item_path):\n                os.unlink(item_path)\n            else:\n                shutil.rmtree(item_path)\n        except Exception as e:\n            print('Failed to delete %s. Reason: %s' % (item_path, e))\n\n", "CodeBleu_score": 43.3729490296683}, {"code": "def clean_folder(folder)\n\t\n    for item in os.listdir(folder):\n        item_path = os.path.join(folder, item)\n        try:\n            if os.path.isfile(item_path):\n                os.unlink(item_path)\n            elif os.path.isdir(item_path):\n                shutil.rmtree(item_path)\n        except Exception as e:\n            print(\"Failed to delete %s. Reason: %s\" % (item_path, e))\n\n", "CodeBleu_score": 45.49565118578772}]}
{"instance_id": "53588b481bdd", "generation_result": [{"code": "def next_batch(self, sample_size=1)\n\t\n    batch = []\n    for _ in range(sample_size):\n        if len(self.memory) > 0:\n            index = np.random.choice(range(len(self.memory)))\n            batch.append(self.memory.pop(index))\n        else:\n            print('Error: No more experiences to sample.')\n            break\n    return batch\n\n", "CodeBleu_score": 28.374301305314464}, {"code": "def next_batch(self, sample_size=1)\n\t\n    batch = []\n    if sample_size > len(self.memory):\n        sample_size = len(self.memory)\n    batch = random.sample(self.memory, sample_size)\n    for experience in batch:\n        self.memory.remove(experience)\n    return batch\n\n", "CodeBleu_score": 51.09007999739647}, {"code": "def next_batch(self, sample_size=1)\n\t\n    # sample_size = min(sample_size, len(self.memory))\n    sample_size = min(sample_size, len(self.memory))\n    batch = random.sample(self.memory, sample_size)\n    for experience in batch:\n        self.memory.remove(experience)\n    return batch\n\n", "CodeBleu_score": 26.55362674297525}]}
{"instance_id": "6bfa6a4e621c", "generation_result": [{"code": "def find_estimated_top_configs(self)\n\t\n    all_configs = self.all_configs\n    cost_model = self.cost_model\n    metric = self.metric\n    top_configs_indices = []\n    for config in all_configs:\n        config_list = []\n        for key in config.keys():\n            config_list.append(config[key])\n        config_array = np.array(config_list)\n        estimated_cost = cost_model.predict(config_array.reshape(1, -1))\n        estimated_cost = estimated_cost[0]\n        if metric == 'min':\n            estimated_cost = -estimated_cost\n        top_configs_indices.append(estimated_cost)\n    top_configs_indices = np.argsort(top_configs_indices)\n    top_configs_indices = list(top_configs_indices)\n    top_configs_indices.reverse()\n    top_configs_indices = top_configs_indices[:self.num_top_configs]\n    return top_configs_indices\n\n", "CodeBleu_score": 36.64704597778677}, {"code": "def find_estimated_top_configs(self)\n\t\n    if self.cost_model is None:\n        raise ValueError(\"Please specify a cost model before finding the estimated top configurations.\")\n    all_configs_flattened = [config for config_list in self.all_configs for config in config_list]\n    all_configs_flattened = [config for config in all_configs_flattened if config.is_valid]\n    all_configs_flattened = [config.flatten() for config in all_configs_flattened]\n    all_configs_flattened = np.array(all_configs_flattened)\n    all_configs_flattened = self.cost_model.predict(all_configs_flattened)\n    if self.metric == \"min\":\n        top_indices = np.argsort(all_configs_flattened)[:self.n_top]\n    elif self.metric == \"max\":\n        top_indices = np.argsort(-all_configs_flattened)[:self.n_top]\n    else:\n        raise ValueError(\"Please specify a valid metric: 'min' or 'max'.\")\n    return top_indices\n\n", "CodeBleu_score": 29.459546601178616}, {"code": "def find_estimated_top_configs(self)\n\t\n    all_configs = self.all_configs\n    configs_flattened = [config.flatten() for config in all_configs]\n    configs_flattened = np.array(configs_flattened)\n    costs = self.cost_model.predict(configs_flattened)\n    if self.metric == \"time\":\n        costs = costs / 1000\n    elif self.metric == \"flops\":\n        costs = costs / 1000000000\n    elif self.metric == \"memory\":\n        costs = costs / 1000000\n    top_indices = np.argsort(costs)[-self.num_configs :][::-1]\n    return top_indices\n\n", "CodeBleu_score": 24.41256967961453}]}
{"instance_id": "d4cf48413a57", "generation_result": [{"code": "def flatten(d, parent_key='', sep='_')\n\t\n    items = []\n    for k, v in d.items():\n        new_key = parent_key + sep + k if parent_key else k\n        if isinstance(v, collections.abc.MutableMapping):\n            items.extend(flatten(v, new_key, sep=sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)\n\n", "CodeBleu_score": 81.92761218076632}, {"code": "def flatten(d, parent_key='', sep='_')\n\t\n    items = []\n    for k, v in d.items():\n        new_key = parent_key + sep + k if parent_key else k\n        if isinstance(v, collections.abc.MutableMapping):\n            items.extend(flatten(v, new_key, sep=sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)\n\n", "CodeBleu_score": 81.92761218076632}, {"code": "def flatten(d, parent_key='', sep='_')\n\t\n    items = []\n    for k, v in d.items():\n        new_key = parent_key + sep + k if parent_key else k\n        if isinstance(v, collections.abc.MutableMapping):\n            items.extend(flatten(v, new_key, sep=sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)\n\n", "CodeBleu_score": 81.92761218076632}]}
{"instance_id": "c2a514249c25", "generation_result": [{"code": "def run_after_tuning(self)\n\t\n\n    if self.optimal_cmd:\n        self.logger.info(\"Running optimal command: %s\", self.optimal_cmd)\n        self.optimal_cmd_process = subprocess.Popen(self.optimal_cmd, shell=True)\n        self.optimal_cmd_process.wait()\n        self.logger.info(\"Optimal command finished\")\n    else:\n        self.logger.info(\"No optimal command found\")\n", "CodeBleu_score": 46.16010788253623}, {"code": "def run_after_tuning(self)\n\t\n    if self.optimal_cmd:\n        try:\n            subprocess.Popen(self.optimal_cmd, shell=True).wait()\n            self.logger.info(\"Successfully ran command: \" + self.optimal_cmd)\n        except Exception as e:\n            self.logger.error(\n                \"Error: \" + str(e) + \" while running command: \" + self.optimal_cmd\n            )\n    else:\n        self.logger.info(\n            \"No optimal command found. Skipping command execution after tuning.\"\n        )\n", "CodeBleu_score": 28.94311296118134}, {"code": "def run_after_tuning(self)\n\t\n    if self.optimal_cmd:\n        subprocess.Popen(self.optimal_cmd)\n        self.logger.info(f'Optimal command found: {self.optimal_cmd}')\n    else:\n        self.logger.info(f'No optimal command found.')\n", "CodeBleu_score": 28.301548362888052}]}
{"instance_id": "61802de52df0", "generation_result": [{"code": "def run_job(self, exp: dict, reservations)\n\t\n    # Set the master port and result directory for the experiment\n    exp['master_port'] = self.master_port\n    exp['result_dir'] = self.result_dir\n\n    # Update the user arguments based on the argument mappings\n    for arg_mapping in exp['arg_mappings']:\n        # Get the user argument's name and value\n        user_arg_name = arg_mapping['user_arg_name']\n        user_arg_value = arg_mapping['user_arg_value']\n        # Get the experiment's argument's name and value\n        exp_arg_name = arg_mapping['exp_arg_name']\n        exp_arg_value = arg_mapping['exp_arg_value']\n        # Update the experiment's argument value\n        exp[exp_arg_name] = exp_arg_value\n        # Update the user argument value\n        if user_arg_value != '':\n            self.user_args[user_arg_name] = user_arg_value\n\n    # Create a new thread to run the experiment and start it\n    thread = threading.Thread(target=run_experiment, args=(exp, reservations))\n    thread.start()\n    # Add the experiment's details to the running experiments\n    self.running_experiments[exp['exp_id']] = {\n        'exp_id': exp['exp_id'],\n        'exp_name': exp['exp_name'],\n        'exp_type': exp['exp_type'],\n        'exp_script': exp['exp_script'],\n        'exp_args': exp['exp_args'],\n        'exp_status': 'running',\n        'exp_start_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n        'exp_end_time': '',\n        'exp_duration': '',\n        'exp_result': '',\n        'exp_result_dir': exp['result_dir'],\n        'exp_master_port': exp['master_port'],\n        'exp_reservation_id': reservations,\n        'exp_reservation_status': ''\n    }\n\n", "CodeBleu_score": 27.989351773186016}, {"code": "def run_job(self, exp: dict, reservations)\n\t\n    # Set the experiment's master port and result directory\n    exp['master_port'] = self.master_port\n    exp['result_dir'] = self.result_dir\n\n    # Update user arguments based on argument mappings\n    exp['args'] = self.update_arguments(exp['args'])\n\n    # Start the experiment in a new thread\n    exp_thread = Thread(target=self.run_experiment, args=(exp, reservations))\n    exp_thread.start()\n\n    # Add the experiment to the running experiments\n    self.running_experiments.append(exp)\n    self.running_experiments[-1]['timestamp'] = datetime.datetime.now()\n\n    # Update the experiment queue\n    self.update_queue()\n\n", "CodeBleu_score": 13.90605306013684}, {"code": "def run_job(self, exp: dict, reservations)\n\t\n    # update experiment parameters\n    exp['master_port'] = self.master_port\n    exp['result_dir'] = self.result_dir\n    exp['reservations'] = reservations\n    exp['experiment_name'] = exp['experiment_name']\n    exp['experiment_description'] = exp['experiment_description']\n    exp['experiment_author'] = exp['experiment_author']\n    exp['experiment_date'] = exp['experiment_date']\n    exp['experiment_version'] = exp['experiment_version']\n    exp['experiment_tags'] = exp['experiment_tags']\n    exp['experiment_parameters'] = exp['experiment_parameters']\n    exp['experiment_resources'] = exp['experiment_resources']\n    exp['experiment_script'] = exp['experiment_script']\n    exp['experiment_arguments'] = exp['experiment_arguments']\n    exp['experiment_reservations'] = exp['experiment_reservations']\n    exp['experiment_status'] = 'running'\n    exp['experiment_start_time'] = datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")\n    exp['experiment_end_time'] = None\n    exp['experiment_log_file'] = None\n    exp['experiment_log_file_path'] = None\n    exp['experiment_pid'] = None\n    exp['experiment_process'] = None\n    exp['experiment_output'] = None\n    exp['experiment_output_path'] = None\n    exp['experiment_error'] = None\n    exp['experiment_error_path'] = None\n    exp['experiment_exit_code'] = None\n    exp['experiment_duration'] = None\n    exp['experiment_duration_seconds'] = None\n    exp['experiment_duration_minutes'] = None\n    exp['experiment_duration_hours'] = None\n    exp['experiment_duration_days'] = None\n    exp['experiment_duration_weeks'] = None\n    exp['experiment_duration_months'] = None\n    exp['experiment_duration_years'] = None\n    exp['experiment_duration_microseconds'] = None\n    exp['experiment_duration_milliseconds'] = None\n    exp['experiment_duration_centiseconds'] = None\n    exp['experiment_duration_deciseconds'] = None\n    exp['experiment_duration_picoseconds'] = None\n    exp['experiment_duration_femtoseconds'] = None\n    exp['experiment_duration_attoseconds']", "CodeBleu_score": 9.386355395947318}]}
{"instance_id": "4884e2e9665c", "generation_result": [{"code": "def save_exp_results_to_database(self, message, ranks=None, path=None):\n\t\n    if self.distributed_training:\n        if self.rank == 0:\n            self.save_exp_results_to_database(message, ranks, path)\n        else:\n            pass\n    else:\n        self.save_exp_results_to_database(message, ranks, path)\n\n", "CodeBleu_score": 13.057587112047269}, {"code": "def save_exp_results_to_database(self, message, ranks=None, path=None):\n\t\n    if self.distributed:\n        if self.rank == 0:\n            message[\"rank\"] = 0\n        else:\n            message[\"rank\"] = 1\n    else:\n        message[\"rank\"] = 0\n    if ranks is not None:\n        if message[\"rank\"] in ranks:\n            if self.distributed:\n                if self.rank == 0:\n                    if path is not None:\n                        with open(path, \"a\") as f:\n                            json.dump(message, f)\n                            f.write(\"", "CodeBleu_score": 25.490786449915642}, {"code": "def save_exp_results_to_database(self, message, ranks=None, path=None):\n\t\n    if self.distributed_training and self.rank == 0:\n        self.save_exp_results_to_database(message, ranks, path)\n    else:\n        if self.rank == 0:\n            self.save_exp_results_to_database(message, ranks, path)\n\n", "CodeBleu_score": 12.339074671284948}]}
{"instance_id": "0429344c3c4d", "generation_result": [{"code": "def _build_tp_other_layer_map(self, layer_index: int)\n\t\n    # get all files in the directory\n    files = os.listdir(self.path)\n    # get all files with the given prefix\n    prefix = self.get_prefix(layer_index)\n    layer_files = [f for f in files if f.startswith(prefix)]\n    # check if there are any files\n    if len(layer_files) == 0:\n        raise ValueError(\"There are no files for the layer index {}.\".format(layer_index))\n    # check if the layer index is valid\n    if layer_index >= self.layer_count:\n        raise ValueError(\"The layer index {} is invalid. There are only {} layers.\".format(layer_index, self.layer_count))\n    # create a dictionary mapping partition indices to their corresponding file lists\n    partition_map = {}\n    # the number of files in each partition\n    partition_size = int(len(layer_files) / self.tp_degree)\n    # the number of partitions with one additional file\n    additional_partitions = len(layer_files) % self.tp_degree\n    # iterate over all partitions\n    for i in range(self.tp_degree):\n        # the start index of the current partition\n        start_index = i * partition_size\n        # the end index of the current partition\n        end_index = start_index + partition_size\n        # if there is an additional partition, add one to the end index\n        if i < additional_partitions:\n            end_index += 1\n        # create a list of files for the current partition\n        partition_files = layer_files[start_index:end_index]\n        # add the partition index to the partition map\n        partition_map[i] = partition_files\n    return partition_map\n\n", "CodeBleu_score": 30.358700520718706}, {"code": "def _build_tp_other_layer_map(self, layer_index: int)\n\t\n    # Check if there are any layer files\n    if not os.path.exists(self.layer_files_path):\n        raise ValueError(\"No layer files found in the specified layer files directory.\")\n\n    # Check if the layer index is valid\n    if layer_index < 0 or layer_index >= self.num_layers:\n        raise ValueError(f\"Invalid layer index: {layer_index}.\")\n\n    # Retrieve files with the given layer index\n    layer_files = [f for f in os.listdir(self.layer_files_path) if f.startswith(f\"{layer_index}_\")]\n\n    # Check if there are any layer files for the given layer index\n    if len(layer_files) == 0:\n        raise ValueError(f\"No layer files found for layer index {layer_index}.\")\n\n    # Partition the layer files based on tp_degree\n    partition_indices = range(self.tp_degree)\n    partitioned_layer_files = [[] for _ in partition_indices]\n    for layer_file in layer_files:\n        partition_index = int(layer_file.split(\"_\")[1])\n        partitioned_layer_files[partition_index].append(layer_file)\n\n    # Create a dictionary mapping partition indices to their corresponding file lists\n    return {partition_index: partitioned_layer_files[partition_index] for partition_index in partition_indices}\n\n", "CodeBleu_score": 30.734931956328342}, {"code": "def _build_tp_other_layer_map(self, layer_index: int)\n\t\n    # Check if there are layer files\n    if len(self.layer_files) == 0:\n        raise Exception(f\"No layer files found for layer {layer_index}\")\n\n    # Check if the layer index is valid\n    if layer_index < 0 or layer_index >= self.layer_count:\n        raise Exception(f\"Layer index {layer_index} is invalid\")\n\n    # Get the layer files for the given layer index\n    layer_files = self.layer_files[layer_index]\n\n    # Get the prefix for the given layer index\n    prefix = self.prefix_list[layer_index]\n\n    # Get the number of partitions for the given layer index\n    tp_degree = self.tp_degree_list[layer_index]\n\n    # Partition the layer files based on tp_degree\n    partitioned_layer_files = [layer_files[i : i + tp_degree] for i in range(0, len(layer_files), tp_degree)]\n\n    # Create the dictionary mapping partition indices to their corresponding file lists\n    partition_map = dict(enumerate(partitioned_layer_files))\n\n    return partition_map\n\n", "CodeBleu_score": 36.2436154871526}]}
{"instance_id": "1b65ecb9be93", "generation_result": [{"code": "def system_info()\n\t\n    info = {\n        'os': platform.system(),\n        'arch': platform.architecture(),\n        'version': platform.version(),\n        'processor': platform.processor(),\n        'python_version': platform.python_version()\n    }\n    return info\n\n", "CodeBleu_score": 52.051676942566175}, {"code": "def system_info()\n\t\n    sys_info = {}\n    sys_info[\"os\"] = platform.system()\n    sys_info[\"arch\"] = platform.machine()\n    sys_info[\"version\"] = platform.version()\n    sys_info[\"processor\"] = platform.processor()\n    sys_info[\"python\"] = platform.python_version()\n    return sys_info\n\n", "CodeBleu_score": 41.6944761073079}, {"code": "def system_info()\n\t\n    system_info = {}\n    system_info['Operating System'] = platform.system()\n    system_info['Architecture'] = platform.machine()\n    system_info['Version'] = platform.version()\n    system_info['Processor'] = platform.processor()\n    system_info['Python Version'] = platform.python_version()\n    return system_info\n\n", "CodeBleu_score": 39.260030691334656}]}
{"instance_id": "969bcc5cf3b5", "generation_result": [{"code": "def cuda_info()\n\t\n    if torch.cuda.is_available():\n        device_count = torch.cuda.device_count()\n        gpu_names = [torch.cuda.get_device_name(i) for i in range(device_count)]\n        cuda_version = torch.version.cuda\n        return gpu_names, cuda_version\n    else:\n        return None\n\n", "CodeBleu_score": 52.2517476266126}, {"code": "def cuda_info()\n\t\n    try:\n        import cupy as cp\n        import cupyx.scipy as cpx\n        import cupyx.scipy.fft as cpxfft\n        import cupyx.scipy.ndimage as cpxndi\n        import cupyx.scipy.ndimage.filters as cpxndi_filters\n        import cupyx.scipy.ndimage.interpolation as cpxndi_interp\n        import cupyx.scipy.ndimage.measurements as cpxndi_meas\n        import cupyx.scipy.ndimage.morphology as cpxndi_morph\n        import cupyx.scipy.ndimage.filters as cpxndi_filters\n        import cupyx.scipy.ndimage.interpolation as cpxndi_interp\n        import cupyx.scipy.ndimage.measurements as cpxndi_meas\n        import cupyx.scipy.ndimage.morphology as cpxndi_morph\n        import cupyx.scipy.ndimage.filters as cpxndi_filters\n        import cupyx.scipy.ndimage.interpolation as cpxndi_interp\n        import cupyx.scipy.ndimage.measurements as cpxndi_meas\n        import cupyx.scipy.ndimage.morphology as cpxndi_morph\n        import cupyx.scipy.ndimage.filters as cpxndi_filters\n        import cupyx.scipy.ndimage.interpolation as cpxndi_interp\n        import cupyx.scipy.ndimage.measurements as cpxndi_meas\n        import cupyx.scipy.ndimage.morphology as cpxndi_morph\n        import cupyx.scipy.ndimage.filters as cpxndi_filters\n        import cupyx.scipy.ndimage.interpolation as cpxndi_interp\n        import cupyx.scipy.ndimage.measurements as cpxndi_meas\n        import cupyx.scipy.ndimage.morphology as cpxndi_morph\n        import cupyx.scipy.ndimage.filters as cpxndi_filters\n        import cupyx.scipy.ndimage.interpolation as cpxndi_interp\n        import cupyx.scipy.ndimage.measurements as cpxndi_meas\n        import cupyx.scipy.ndimage.morphology as cpxn", "CodeBleu_score": 23.301869100029236}, {"code": "def cuda_info()\n\t\n    import torch\n    import subprocess\n    import os\n    from torch.utils.cpp_extension import CUDA_HOME\n    from .utils import get_torch_version\n\n    if not torch.cuda.is_available():\n        return \"CUDA is not available.\"\n\n    if CUDA_HOME is None:\n        return \"CUDA_HOME is not available.\"\n\n    try:\n        nvcc_path = os.path.join(CUDA_HOME, \"bin/nvcc\")\n        nvcc_version = subprocess.check_output([nvcc_path, \"--version\"]).decode(\"utf-8\")\n        torch_version = get_torch_version()\n        return f\"CUDA is available.", "CodeBleu_score": 39.18078718241106}]}
{"instance_id": "a2ba908a4c70", "generation_result": [{"code": "def compute_encoder_accuracy(dataset_items, encoder_manager)\n\t\n    encoder_accuracy = {}\n    class_name_key = encoder_manager.config.get('class_name_key')\n    class_id_to_name_mapping = encoder_manager.config.get('class_id_to_name_mapping')\n    if class_id_to_name_mapping is not None:\n        class_id_to_name_mapping = {int(k): v for k, v in class_id_to_name_mapping.items()}\n    for item in dataset_items:\n        audio_file = item['audio_file']\n        embedding = encoder_manager.embed_audio(audio_file)\n        predicted_label = encoder_manager.predict(embedding)\n        true_label = item[class_name_key]\n        if class_id_to_name_mapping is not None:\n            true_label = class_id_to_name_mapping[int(true_label)]\n        if true_label not in encoder_accuracy:\n            encoder_accuracy[true_label] = {'true_positives': 0, 'false_positives': 0, 'false_negatives': 0}\n        if predicted_label == true_label:\n            encoder_accuracy[true_label]['true_positives'] += 1\n        else:\n            if true_label not in encoder_accuracy:\n                encoder_accuracy[true_label] = {'true_positives': 0, 'false_positives': 0, 'false_negatives': 0}\n            encoder_accuracy[true_label]['false_positives'] += 1\n            encoder_accuracy[true_label]['false_negatives'] += 1\n    for class_name in encoder_accuracy:\n        true_positives = encoder_accuracy[class_name]['true_positives']\n        false_positives = encoder_accuracy[class_name]['false_positives']\n        false_negatives = encoder_accuracy[class_name]['false_negatives']\n        accuracy = (true_positives + 0.5 * false_negatives) / (true_positives + false_positives + false_negatives)\n        print(f'Accuracy for class {class_name}: {accuracy:.2f}')\n    total_true_positives = 0\n    total_false_positives = 0\n    total_false_negatives = ", "CodeBleu_score": 33.04850866967302}, {"code": "def compute_encoder_accuracy(dataset_items, encoder_manager)\n\t\n    encoder_accuracy = {}\n    encoder_class_id_to_name = encoder_manager.config.get(\"mappings\", {}).get(\"class_id_to_name\", {})\n    for dataset_item in dataset_items:\n        audio_file = dataset_item.get(\"audio\")\n        if audio_file is None:\n            continue\n        embedding = encoder_manager.embed_audio(audio_file)\n        class_name = dataset_item.get(\"class_name\")\n        if class_name is None:\n            continue\n        predicted_class_name = encoder_manager.predict(embedding, class_id_to_name=encoder_class_id_to_name)\n        if class_name not in encoder_accuracy:\n            encoder_accuracy[class_name] = 0\n        if predicted_class_name == class_name:\n            encoder_accuracy[class_name] += 1\n    total_accuracy = 0\n    for class_name, accuracy in encoder_accuracy.items():\n        total_accuracy += accuracy\n        print(f\"{class_name}: {accuracy/len(dataset_items):.2f}\")\n    print(f\"Average accuracy: {total_accuracy/len(dataset_items):.2f}\")\n\n", "CodeBleu_score": 27.2562168941083}, {"code": "def compute_encoder_accuracy(dataset_items, encoder_manager)\n\t\n    class_name_key = encoder_manager.get_config().get(\"class_name_key\", None)\n    class_id_to_name_mapping = encoder_manager.get_config().get(\n        \"class_id_to_name_mapping\", None\n    )\n    if class_name_key is None:\n        raise ValueError(\n            \"The encoder manager must have a class_name_key set to compute the accuracy.\"\n        )\n    if class_id_to_name_mapping is None:\n        raise ValueError(\n            \"The encoder manager must have a class_id_to_name_mapping set to compute the accuracy.\"\n        )\n    class_names = list(class_id_to_name_mapping.values())\n    class_accuracies = {name: 0.0 for name in class_names}\n    total_count = 0\n    for dataset_item in dataset_items:\n        audio_file_path = dataset_item.get_path()\n        embedding = encoder_manager.embed_audio(audio_file_path)\n        predicted_class_id = encoder_manager.predict(embedding)\n        true_class_name = dataset_item.get_metadata().get(class_name_key)\n        if true_class_name is None:\n            continue\n        predicted_class_name = class_id_to_name_mapping.get(predicted_class_id)\n        if predicted_class_name is None:\n            continue\n        class_accuracies[true_class_name] += 1.0\n        if true_class_name == predicted_class_name:\n            class_accuracies[true_class_name] += 0.5\n        total_count += 1\n    for class_name in class_names:\n        class_accuracies[class_name] /= total_count\n    print(\"Encoder accuracy:\")\n    for class_name, accuracy in class_accuracies.items():\n        print(f\"{class_name}: {accuracy}\")\n    print(f\"Average: {sum(class_accuracies.values()) / len(class_accuracies)}\")\n\n", "CodeBleu_score": 32.708127059398315}]}
{"instance_id": "e01ec69ad58a", "generation_result": [{"code": "def adjust_path_and_remove_silence(audio_path)\n\t\n    # get the output directory path\n    out_path = audio_path.replace(INPUT_DIR, OUTPUT_DIR)\n    out_dir = os.path.dirname(out_path)\n    os.makedirs(out_dir, exist_ok=True)\n\n    # if the file already exists and force is not specified, return the output path and a False flag\n    if os.path.exists(out_path) and not FORCE:\n        print(f\"> The file {out_path} already exists and force is not specified, skipping it\")\n        return out_path, False\n\n    # remove silence from the audio\n    out_path, is_speech = remove_silence(\n        model_and_utils, audio_path, out_path, vad_sample_rate=8000, trim_just_beginning_and_end=True, use_cuda=False\n    )\n    return out_path, is_speech\n\n", "CodeBleu_score": 35.346761058368834}, {"code": "def adjust_path_and_remove_silence(audio_path)\n\t\n\n    # get the output path\n    out_path = audio_path.replace(INPUT_DIR, OUTPUT_DIR)\n\n    # if the file already exists, return the output path and a False flag\n    if os.path.exists(out_path) and not FORCE:\n        print(f\"> \u2757 The file {out_path} already exists, skipping\")\n        return out_path, False\n\n    # create the necessary directory structure\n    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n\n    # remove silence from the audio\n    out_path, is_speech = remove_silence(\n        model_and_utils, audio_path, out_path, vad_sample_rate=VAD_SAMPLE_RATE, trim_just_beginning_and_end=TRIM_JUST_BEGINNING_AND_END, use_cuda=USE_CUDA\n    )\n\n    return out_path, is_speech\n\n", "CodeBleu_score": 40.93295507728454}, {"code": "def adjust_path_and_remove_silence(audio_path)\n\t\n    # get the output path\n    out_path = audio_path.replace(args.input_dir, args.output_dir)\n\n    # if the output path exists and force is not specified, return the output path and a False flag\n    if os.path.exists(out_path) and not args.force:\n        print(f\"> \u2757 {out_path} already exists, skipping\")\n        return out_path, False\n\n    # create the necessary directory structure\n    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n\n    # remove silence from the audio\n    model_and_utils = load_vad_model()\n    out_path, is_speech = remove_silence(\n        model_and_utils, audio_path, out_path, vad_sample_rate=args.vad_sample_rate, use_cuda=args.use_cuda\n    )\n\n    # return the output path and the flag indicating if speech was detected\n    return out_path, is_speech\n\n", "CodeBleu_score": 46.019292567864134}]}
{"instance_id": "7e0bdb0feff8", "generation_result": [{"code": "def resample_files(input_dir, output_sr, output_dir=None, file_ext=\"wav\", n_jobs=10)\n\t\n    if output_dir is not None:\n        shutil.copytree(input_dir, output_dir)\n\n    wav_files = glob.glob(os.path.join(input_dir, \"**\", \"*.\" + file_ext), recursive=True)\n    resample_files_parallel(wav_files, output_sr, n_jobs=n_jobs)\n\n", "CodeBleu_score": 19.932904820341996}, {"code": "def resample_files(input_dir, output_sr, output_dir=None, file_ext=\"wav\", n_jobs=10)\n\t\n    if output_dir is None:\n        output_dir = input_dir\n    else:\n        copy_tree(input_dir, output_dir)\n    audio_files = find_files(input_dir, file_ext)\n    print(\"Resampling {} files from {} to {} Hz\".format(len(audio_files), input_dir, output_sr))\n    resample_files_parallel(audio_files, output_sr, output_dir, n_jobs)\n    print(\"Done\")\n\n", "CodeBleu_score": 22.369876920422467}, {"code": "def resample_files(input_dir, output_sr, output_dir=None, file_ext=\"wav\", n_jobs=10)\n\t\n    if output_dir is not None:\n        shutil.copytree(input_dir, output_dir)\n        input_dir = output_dir\n    files = find_files(input_dir, file_ext)\n    resample_files_parallel(files, output_sr, n_jobs)\n    print(\"Resampling complete.\")\n\n", "CodeBleu_score": 19.48678564128462}]}
{"instance_id": "a1351834ca57", "generation_result": [{"code": "def evaluation(model, criterion, data_loader, global_step)\n\t\n    model.eval()\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    loss_sum = 0\n    num_batches = 0\n    embeddings = []\n    with torch.no_grad():\n        for batch in data_loader:\n            inputs, labels = batch\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n            labels = labels.reshape(-1, 1)\n            labels = torch.cat([labels for _ in range(inputs.shape[1])], dim=1)\n            embeddings.append(model(inputs))\n            loss = criterion(model(inputs), labels)\n            loss_sum += loss.item()\n            num_batches += 1\n\n    embeddings = torch.cat(embeddings, dim=1).cpu().numpy()\n    fig = plot_embeddings(embeddings, num_classes_in_batch=10)\n    fig.savefig(f\"umap_step_{global_step}\")\n    fig.clf()\n    plt.close(fig)\n\n    average_loss = loss_sum / num_batches\n    model.train()\n    return average_loss\n\n", "CodeBleu_score": 32.1991492137093}, {"code": "def evaluation(model, criterion, data_loader, global_step)\n\t\n    model.eval()\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    losses = []\n    with torch.no_grad():\n        for i, (inputs, labels) in enumerate(data_loader):\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            losses.append(loss.item())\n            if i == 0:\n                plot_embeddings(outputs, labels.shape[0])\n    loss = np.mean(losses)\n    logging.info(\n        \"Eval loss: {:.4f} at step {}\".format(loss, global_step)\n    )\n    return loss\n\n", "CodeBleu_score": 23.51348956912679}, {"code": "def evaluation(model, criterion, data_loader, global_step)\n\t\n\n    model.eval()\n    with torch.no_grad():\n        loss_sum = 0.0\n        num_batches = 0\n        for batch in data_loader:\n            # get the inputs and labels\n            inputs, labels = batch\n            # reshape to group samples of each class\n            inputs = inputs.view(-1, inputs.shape[2], inputs.shape[3])\n            labels = labels.view(-1)\n            # dispatch data to GPU if available\n            if torch.cuda.is_available():\n                inputs = inputs.cuda()\n                labels = labels.cuda()\n            # forward pass\n            outputs = model(inputs)\n            # compute loss\n            loss = criterion(outputs, labels)\n            loss_sum += loss.item()\n            num_batches += 1\n        # compute average loss\n        loss_avg = loss_sum / num_batches\n        # log average loss\n        logger.log_scalar(\n            \"eval_loss\",\n            loss_avg,\n            global_step,\n        )\n        # log UMAP plot\n        if global_step % 100 == 0:\n            plot_embeddings(\n                model.get_embeddings(),\n                model.num_classes_in_batch,\n            )\n        # log statistics\n        logger.log_scalar(\n            \"eval_accuracy\",\n            accuracy(model, data_loader),\n            global_step,\n        )\n        logger.log_scalar(\n            \"eval_precision\",\n            precision(model, data_loader),\n            global_step,\n        )\n        logger.log_scalar(\n            \"eval_recall\",\n            recall(model, data_loader),\n            global_step,\n        )\n        logger.log_scalar(\n            \"eval_f1\",\n            f1(model, data_loader),\n            global_step,\n        )\n    return loss_avg\n\n", "CodeBleu_score": 35.13446248492645}]}
{"instance_id": "565f8ed09916", "generation_result": [{"code": "def register_config(model_name: str) -> Coqpit\n\t\n    try:\n        if model_name.lower() == \"xtts\":\n            from coqpit.configs.tts.xtts_config import XTTSConfig\n\n            return XTTSConfig\n        else:\n            config_class = find_module(\"coqpit.configs\", model_name)\n            return config_class\n    except ModuleNotFoundError:\n        raise ModuleNotFoundError(\n            f\"The configuration class for {model_name} was not found.", "CodeBleu_score": 29.906473232518906}, {"code": "def register_config(model_name: str) -> Coqpit\n\t\n    if model_name == \"xtts\":\n        config_class_name = \"XttsConfig\"\n    else:\n        config_class_name = to_camel(model_name) + \"Config\"\n    for path in config_paths:\n        try:\n            config_class = find_module(path, model_name)\n            return config_class\n        except ModuleNotFoundError:\n            pass\n    raise ModuleNotFoundError(f\"Could not find a configuration class for {model_name} in any of the following paths: {config_paths}\")\n\n", "CodeBleu_score": 36.344217259380116}, {"code": "def register_config(model_name: str) -> Coqpit\n\t\n    config_class_name = to_camel(model_name)\n    if model_name == \"xtts\":\n        config_class_name += \"Config\"\n    try:\n        config_class = getattr(importlib.import_module(\"coqpit.configs\"), config_class_name)\n    except ModuleNotFoundError:\n        for path in config_paths:\n            try:\n                config_class = find_module(path, model_name)\n                break\n            except ModuleNotFoundError:\n                continue\n        else:\n            raise ModuleNotFoundError(\n                f\"The configuration class for {model_name} was not found. Please make sure you have a configuration class for the model in the paths {config_paths}.\"\n            )\n    return config_class\n", "CodeBleu_score": 34.75765285646714}]}
{"instance_id": "0ea7862506a7", "generation_result": [{"code": "def load_config(config_path: str) -> Coqpit\n\t\n    with fsspec.open(config_path, \"r\", encoding=\"utf-8\") as f:\n        input_str = f.read()\n    if config_path.endswith(\".json\"):\n        config_dict = read_json_with_comments(config_path)\n    elif config_path.endswith(\".yaml\"):\n        config_dict = yaml.safe_load(input_str)\n    elif config_path.endswith(\".yml\"):\n        config_dict = yaml.safe_load(input_str)\n    else:\n        raise ValueError(f\" [!] Unknown file type {config_path}\")\n    model_name = _process_model_name(config_dict)\n    config_class = register_config(model_name)\n    config = config_class()\n    config.update(config_dict)\n    return config\n\n", "CodeBleu_score": 48.89412334447275}, {"code": "def load_config(config_path: str) -> Coqpit\n\t\n    if config_path.endswith(\".yaml\"):\n        config_dict = read_yaml(config_path)\n    elif config_path.endswith(\".json\"):\n        config_dict = read_json(config_path)\n    else:\n        raise ValueError(f\" [!] Unknown config file format: {config_path}\")\n    model_name = _process_model_name(config_dict)\n    config_class = register_config(model_name)\n    config = config_class()\n    config.load_config(config_dict)\n    return config\n\n", "CodeBleu_score": 34.10256851424126}, {"code": "def load_config(config_path: str) -> Coqpit\n\t\n    config_dict = read_config(config_path)\n    model_name = _process_model_name(config_dict)\n    config_class = register_config(model_name)\n    config = config_class()\n    config.update_from_dict(config_dict)\n    return config\n", "CodeBleu_score": 9.748336422228249}]}
{"instance_id": "e0db68b63143", "generation_result": [{"code": "def list_files(basePath, validExts=None, contains=None)\n\t\n\n    # loop over the directory structure\n    for (rootDir, dirNames, filenames) in os.walk(basePath):\n        # loop over the filenames in the current directory\n        for filename in filenames:\n            # if the contains string is not none and the filename does not contain\n            # the supplied string, then ignore the file\n            if contains is not None and filename.find(contains) == -1:\n                continue\n\n            # determine the file extension of the current file\n            ext = filename[filename.rfind(\".\"):].lower()\n\n            # check to see if the file is an image and should be processed\n            if validExts is None or ext.endswith(validExts):\n                # construct the path to the image and yield it\n                imagePath = os.path.join(rootDir, filename)\n                yield imagePath\n\n", "CodeBleu_score": 78.4028354587536}, {"code": "def list_files(basePath, validExts=None, contains=None)\n\t\n    # loop over the directory structure\n    for (rootDir, dirNames, filenames) in os.walk(basePath):\n        # loop over the filenames in the current directory\n        for filename in filenames:\n            # if the contains string is not none and the filename does not contain\n            # the supplied string, then ignore the file\n            if contains is not None and filename.find(contains) == -1:\n                continue\n\n            # determine the file extension of the current file\n            ext = filename[filename.rfind(\".\"):].lower()\n\n            # check to see if the file is an image and should be processed\n            if validExts is None or ext.endswith(validExts):\n                # construct the path to the image and yield it\n                imagePath = os.path.join(rootDir, filename)\n                yield imagePath\n\n", "CodeBleu_score": 78.4028354587536}, {"code": "def list_files(basePath, validExts=None, contains=None)\n\t\n    # loop over the directory structure\n    for (rootDir, dirNames, filenames) in os.walk(basePath):\n        # loop over the filenames in the current directory\n        for filename in filenames:\n            # if the contains string is not none and the filename does not contain\n            # the supplied string, then ignore the file\n            if contains is not None and filename.find(contains) == -1:\n                continue\n\n            # determine the file extension of the current file\n            ext = filename[filename.rfind(\".\"):].lower()\n\n            # check to see if the file is an image and should be processed\n            if validExts is None or ext.endswith(validExts):\n                # construct the path to the image and yield it\n                imagePath = os.path.join(rootDir, filename)\n                yield imagePath\n\n", "CodeBleu_score": 78.4028354587536}]}
{"instance_id": "b836fcaf0a27", "generation_result": [{"code": "def load_model(xtts_checkpoint, xtts_config, xtts_vocab)\n\t\n    if clear_gpu_cache:\n        clear_gpu_cache()\n    if xtts_checkpoint is None or xtts_config is None or xtts_vocab is None:\n        print(\"Error: One or more paths to XTTS model files are missing.\")\n        return\n    if not os.path.exists(xtts_checkpoint):\n        print(\"Error: The provided XTTS checkpoint path does not exist.\")\n        return\n    if not os.path.exists(xtts_config):\n        print(\"Error: The provided XTTS config path does not exist.\")\n        return\n    if not os.path.exists(xtts_vocab):\n        print(\"Error: The provided XTTS vocab path does not exist.\")\n        return\n    print(\"Loading XTTS model...\")\n    config = XTTS_MODEL.load_config(xtts_config)\n    model = XTTS_MODEL(config)\n    model.load_checkpoint(config, checkpoint_path=xtts_checkpoint, vocab_path=xtts_vocab)\n    if torch.cuda.is_available():\n        model.cuda()\n    print(\"XTTS model loaded successfully.\")\n    return model\n", "CodeBleu_score": 47.99129391279917}, {"code": "def load_model(xtts_checkpoint, xtts_config, xtts_vocab)\n\t\n    if clear_gpu_cache:\n        clear_gpu_cache()\n    if xtts_checkpoint is None or xtts_config is None or xtts_vocab is None:\n        print(\"Error: Checkpoint, config, and vocab paths must be provided.\")\n        return\n    else:\n        print(\"Loading XTTS model...\")\n    config = load_config(xtts_config)\n    model = XTTS(config)\n    model.load_checkpoint(config, checkpoint_path=xtts_checkpoint, vocab_path=xtts_vocab)\n    if torch.cuda.is_available():\n        model.cuda()\n    print(\"XTTS model loaded.\")\n    return model\n\n", "CodeBleu_score": 50.25904979625167}, {"code": "def load_model(xtts_checkpoint, xtts_config, xtts_vocab)\n\t\n    if clear_gpu_cache:\n        clear_gpu_cache()\n    if xtts_checkpoint is None:\n        print(\"Error: No checkpoint path provided.\")\n        return\n    if xtts_config is None:\n        print(\"Error: No configuration path provided.\")\n        return\n    if xtts_vocab is None:\n        print(\"Error: No vocabulary path provided.\")\n        return\n    print(\"Loading XTTS model...\")\n    config = load_config(xtts_config)\n    model = XTTS(config)\n    model.load_checkpoint(config, checkpoint_path=xtts_checkpoint, vocab_path=xtts_vocab)\n    if torch.cuda.is_available():\n        model.cuda()\n    print(\"XTTS model loaded.\")\n    return model\n", "CodeBleu_score": 50.54514996854911}]}
{"instance_id": "c5d63aa0df26", "generation_result": [{"code": "def compute_embedding(self, x, num_frames=250, num_eval=10, return_mean=True, l2_norm=True)\n\t\n    if self.spec_type == 'torch':\n        num_frames = int(np.ceil(num_frames / self.hop_length))\n    num_frames = min(num_frames, x.shape[1])\n    eval_offset = (x.shape[1] - num_frames) // num_eval\n    embeddings = []\n    for i in range(num_eval):\n        start = i * eval_offset\n        end = start + num_frames\n        if self.spec_type == 'torch':\n            frames = x[:, start:end]\n        else:\n            frames = x[start:end]\n        embedding = self.inference(frames)\n        embeddings.append(embedding)\n    embeddings = torch.stack(embeddings, dim=0)\n    if return_mean:\n        embeddings = torch.mean(embeddings, dim=0)\n        if l2_norm:\n            embeddings = embeddings / torch.norm(embeddings, p=2)\n    return embeddings\n\n", "CodeBleu_score": 43.71320360335355}, {"code": "def compute_embedding(self, x, num_frames=250, num_eval=10, return_mean=True, l2_norm=True)\n\t\n    if self.torch_spec:\n        num_frames = int(np.ceil(num_frames / self.hop_length))\n    num_frames = min(num_frames, x.shape[1])\n    offset = x.shape[1] - num_frames\n    offset_range = np.linspace(0, offset, num_eval)\n    embeddings = []\n    for i in offset_range:\n        frames = x[:, int(i) : int(i) + num_frames]\n        if self.torch_spec:\n            frames = torch.from_numpy(frames).unsqueeze(0).to(self.device)\n        else:\n            frames = torch.from_numpy(frames).unsqueeze(0)\n        embedding = self.inference(frames)\n        embeddings.append(embedding)\n    embeddings = torch.stack(embeddings)\n    if return_mean:\n        embeddings = embeddings.mean(dim=0)\n        if l2_norm:\n            embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)\n        return embeddings\n    else:\n        return embeddings\n", "CodeBleu_score": 39.389587473190794}, {"code": "def compute_embedding(self, x, num_frames=250, num_eval=10, return_mean=True, l2_norm=True)\n\t\n    if self.torch_spec:\n        num_frames = int(np.ceil(num_frames / self.hop_length))\n        num_eval = int(np.ceil(num_eval / self.hop_length))\n    num_frames = min(num_frames, x.shape[1])\n    if x.shape[1] < num_frames + num_eval:\n        num_frames = x.shape[1] - num_eval\n    eval_offset = np.linspace(0, x.shape[1] - num_frames, num_eval, dtype=int)\n    eval_frames = [x[:, offset : offset + num_frames] for offset in eval_offset]\n    eval_frames = torch.stack(eval_frames, dim=0)\n    with torch.no_grad():\n        embeddings = self.inference(eval_frames)\n    if l2_norm:\n        embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)\n    if return_mean:\n        embeddings = torch.mean(embeddings, dim=0)\n    return embeddings\n\n", "CodeBleu_score": 43.82587019747729}]}
{"instance_id": "06f5b0335b26", "generation_result": [{"code": "def create_layer(self, block, planes, blocks, stride=1)\n\t\n    downsample = None\n    if stride != 1 or self.inplanes != planes * block.expansion:\n        downsample = nn.Sequential(\n            nn.Conv2d(self.inplanes, planes * block.expansion,\n                      kernel_size=1, stride=stride, bias=False),\n            nn.BatchNorm2d(planes * block.expansion),\n        )\n\n    layers = []\n    layers.append(block(self.inplanes, planes, stride, downsample))\n    self.inplanes = planes * block.expansion\n    for i in range(1, blocks):\n        layers.append(block(self.inplanes, planes))\n\n    return nn.Sequential(*layers)\n\n", "CodeBleu_score": 85.18641475692186}, {"code": "def create_layer(self, block, planes, blocks, stride=1)\n\t\n    downsample = None\n    if stride != 1 or self.inplanes != planes * block.expansion:\n        downsample = nn.Sequential(\n            nn.Conv2d(self.inplanes, planes * block.expansion,\n                      kernel_size=1, stride=stride, bias=False),\n            nn.BatchNorm2d(planes * block.expansion),\n        )\n\n    layers = []\n    layers.append(block(self.inplanes, planes, stride, downsample))\n    self.inplanes = planes * block.expansion\n    for i in range(1, blocks):\n        layers.append(block(self.inplanes, planes))\n\n    return nn.Sequential(*layers)\n\n", "CodeBleu_score": 85.18641475692186}, {"code": "def create_layer(self, block, planes, blocks, stride=1)\n\t\n    downsample = None\n    if stride != 1 or self.inplanes != planes * block.expansion:\n        downsample = nn.Sequential(\n            nn.Conv2d(self.inplanes, planes * block.expansion, kernel_size=1, stride=stride, bias=False),\n            nn.BatchNorm2d(planes * block.expansion),\n        )\n\n    layers = []\n    layers.append(block(self.inplanes, planes, stride, downsample))\n    self.inplanes = planes * block.expansion\n    for i in range(1, blocks):\n        layers.append(block(self.inplanes, planes))\n\n    return nn.Sequential(*layers)\n\n", "CodeBleu_score": 86.18602391417932}]}
{"instance_id": "9ac60597b633", "generation_result": [{"code": "def plot_embeddings(embeddings, num_classes_in_batch)\n\t\n    num_classes = min(num_classes_in_batch, 10)\n    num_utterances_per_class = num_classes_in_batch // num_classes\n    fig = plt.figure(figsize=(12, 12))\n    ax = fig.add_subplot(1, 1, 1)\n    ax.set_title(\"UMAP projection\", fontsize=24)\n    ax.axis(\"off\")\n    for i in range(num_classes):\n        ax.scatter(\n            embeddings[i * num_utterances_per_class : (i + 1) * num_utterances_per_class, 0],\n            embeddings[i * num_utterances_per_class : (i + 1) * num_utterances_per_class, 1],\n            s=10,\n            c=np.random.rand(3,),\n        )\n    plt.savefig(\"umap.png\")\n    return fig\n\n", "CodeBleu_score": 25.344203717733325}, {"code": "def plot_embeddings(embeddings, num_classes_in_batch)\n\t\n    num_classes = min(num_classes_in_batch, 10)\n    num_utterances = np.array([len(np.where(embeddings[:, 0] == i)[0]) for i in range(num_classes)])\n    num_utterances = num_utterances / np.sum(num_utterances)\n\n    reducer = umap.UMAP()\n    embeddings = reducer.fit_transform(embeddings)\n\n    fig = plt.figure(figsize=(10, 10))\n    plt.scatter(embeddings[:, 0], embeddings[:, 1], s=num_utterances * 1000, c=np.arange(num_classes), cmap='Spectral')\n    plt.gca().set_aspect('equal', 'datalim')\n    plt.title('UMAP projection')\n    plt.savefig('umap')\n    return fig\n\n", "CodeBleu_score": 28.918213475747628}, {"code": "def plot_embeddings(embeddings, num_classes_in_batch)\n\t\n    num_classes = min(num_classes_in_batch, 10)\n    num_utterances_per_class = [len(np.where(embeddings[i, 0] == 1)[0]) for i in range(num_classes)]\n    num_utterances_per_class = np.array(num_utterances_per_class)\n    num_utterances_per_class = num_utterances_per_class / num_utterances_per_class.sum()\n    num_utterances_per_class = num_utterances_per_class * 100\n    num_utterances_per_class = num_utterances_per_class.astype(int)\n\n    umap_embeddings = umap.UMAP().fit_transform(embeddings)\n    fig = plt.figure(figsize=(12, 12))\n    ax = fig.add_subplot(1, 1, 1)\n    ax.set_title(\"UMAP projection\")\n    ax.set_aspect(\"equal\")\n    for i in range(num_classes):\n        plt.scatter(\n            umap_embeddings[i, 0], umap_embeddings[i, 1], label=f\"Class {i + 1} ({num_utterances_per_class[i]}%)\"\n        )\n    plt.legend(loc=\"upper left\", bbox_to_anchor=(1.05, 1), borderaxespad=0)\n    plt.savefig(\"umap\")\n    return fig\n\n", "CodeBleu_score": 25.98659764253142}]}
{"instance_id": "19c2b4af256d", "generation_result": [{"code": "def calc_cosine_sim(self, dvecs, centroids)\n\t\n    cosine_sim = []\n    for spkr in range(len(dvecs)):\n        new_centroids = self.calc_new_centroids(dvecs, centroids, spkr, 0)\n        sim = torch.clamp(torch.mm(dvecs[spkr, 0], new_centroids.t()), min=0)\n        for utt in range(1, len(dvecs[spkr])):\n            new_centroids = self.calc_new_centroids(dvecs, centroids, spkr, utt)\n            sim = torch.cat(\n                (\n                    sim,\n                    torch.clamp(\n                        torch.mm(dvecs[spkr, utt], new_centroids.t()), min=0\n                    ),\n                )\n            )\n        cosine_sim.append(sim)\n    return torch.stack(cosine_sim)\n\n", "CodeBleu_score": 38.43678414101429}, {"code": "def calc_cosine_sim(self, dvecs, centroids)\n\t\n    cos_sim = []\n    for spkr in range(self.num_speakers):\n        cos_sim_spkr = []\n        for utt in range(self.num_utts_per_spkr):\n            new_centroids = self.calc_new_centroids(dvecs, centroids, spkr, utt)\n            cos_sim_utt = torch.clamp(\n                torch.mm(dvecs[spkr, utt].unsqueeze(0), new_centroids.t())\n                / torch.norm(dvecs[spkr, utt])\n                / torch.norm(new_centroids, dim=1),\n                min=-1.0,\n                max=1.0,\n            )\n            cos_sim_spkr.append(cos_sim_utt)\n        cos_sim.append(torch.cat(cos_sim_spkr))\n    return torch.stack(cos_sim)\n\n", "CodeBleu_score": 42.02094862833294}, {"code": "def calc_cosine_sim(self, dvecs, centroids)\n\t\n    sim_matrix = []\n    for spkr in range(self.num_spkrs):\n        new_centroids = self.calc_new_centroids(dvecs, centroids, spkr)\n        sim_matrix.append([])\n        for utt in range(self.num_utts_per_spkr):\n            sim = torch.clamp(\n                torch.sum(dvecs[spkr, utt] * new_centroids, dim=0)\n                / (\n                    torch.norm(dvecs[spkr, utt], 2, dim=0)\n                    * torch.norm(new_centroids, 2, dim=0)\n                ),\n                min=-1.0,\n            )\n            sim_matrix[spkr].append(sim)\n    sim_matrix = torch.cat(sim_matrix)\n    return sim_matrix\n\n", "CodeBleu_score": 35.02816960504638}]}
{"instance_id": "c525b411a1e7", "generation_result": [{"code": " def embed_loss_softmax(self, dvecs, cos_sim_matrix)\n\t\n    losses = []\n    for i in range(len(dvecs)):\n        loss = F.cross_entropy(cos_sim_matrix[i], torch.arange(self.num_classes), reduction='none')\n        losses.append(loss)\n    losses = torch.stack(losses)\n    return losses\n", "CodeBleu_score": 34.572006444850544}, {"code": " def embed_loss_softmax(self, dvecs, cos_sim_matrix)\n\t\n    # Calculate the loss for each embedding\n    loss_per_embedding = -torch.nn.functional.log_softmax(cos_sim_matrix, dim=1)[:, self.target_idx]\n    # Stack the losses for each embedding into a tensor\n    loss_per_embedding = torch.stack(loss_per_embedding, dim=0)\n    # Return the final tensor of losses\n    return loss_per_embedding\n\n def embed_loss_contrastive(self, dvecs, cos_sim_matrix):\n    \"\"\"The function calculates the embedding loss using contrastive loss.", "CodeBleu_score": 15.866982989667106}, {"code": " def embed_loss_softmax(self, dvecs, cos_sim_matrix)\n\t\n    losses = []\n    for i in range(len(dvecs)):\n        row = cos_sim_matrix[i]\n        loss = -torch.log_softmax(row, dim=0)\n        losses.append(loss)\n    losses = torch.stack(losses)\n    return losses\n", "CodeBleu_score": 37.83769451003395}]}
{"instance_id": "25ac3945da2e", "generation_result": [{"code": "def embed_loss_contrast(self, dvecs, cos_sim_matrix)\n\t\n    # Get the number of embeddings\n    num_embeddings = dvecs.shape[0]\n    # Create a tensor of zeros\n    loss_row = torch.zeros(num_embeddings).cuda()\n    # Iterate over the embeddings\n    for i in range(num_embeddings):\n        # Get the cosine similarity matrix\n        cos_sim = cos_sim_matrix[i]\n        # Get the cosine similarity of the current embedding with all other embeddings\n        cos_sim_others = cos_sim[1:]\n        # Get the cosine similarity of the current embedding with the closest centroid\n        cos_sim_closest = cos_sim[0]\n        # Compute the sigmoid of the cosine similarity matrix\n        sigmoid = torch.sigmoid(cos_sim_others)\n        # Compute the contrast loss\n        loss = torch.sum(sigmoid) - cos_sim_closest\n        # Append the loss to the loss row\n        loss_row[i] = loss\n    # Stack the loss row into a tensor\n    loss_row = loss_row.unsqueeze(0)\n    # Return the loss row\n    return loss_row\n\n", "CodeBleu_score": 27.594560689805796}, {"code": "def embed_loss_contrast(self, dvecs, cos_sim_matrix)\n\t\n    # Calculate the sigmoid of the cosine similarity matrix.\n    sigmoid_cos_sim_matrix = torch.sigmoid(cos_sim_matrix)\n\n    # Initialize the loss tensor.\n    loss = torch.empty(dvecs.shape[0], device=self.device)\n\n    # Iterate over the embeddings.\n    for i in range(dvecs.shape[0]):\n        # Exclude the current centroid.\n        sigmoid_cos_sim_matrix_excl_centroid = torch.cat((sigmoid_cos_sim_matrix[:i, :], sigmoid_cos_sim_matrix[(i + 1):, :]), dim=0)\n\n        # Calculate the contrast loss for the current embedding.\n        loss[i] = -torch.mean(torch.log(sigmoid_cos_sim_matrix[i, i] / torch.sum(sigmoid_cos_sim_matrix_excl_centroid, dim=0)))\n\n    # Return the loss tensor.\n    return loss\n\n", "CodeBleu_score": 29.05750632478552}, {"code": "def embed_loss_contrast(self, dvecs, cos_sim_matrix)\n\t\n    loss = torch.empty((dvecs.shape[0], dvecs.shape[1]))\n    for i in range(dvecs.shape[0]):\n        for j in range(dvecs.shape[1]):\n            # Exclude the current centroid\n            cos_sim_matrix[i, j, self.centroid_idx[i]] = 0\n            # Calculate the sigmoid of the cosine similarity matrix\n            sigmoid_cos_sim_matrix = torch.sigmoid(cos_sim_matrix[i])\n            # Calculate the contrast loss\n            loss[i, j] = torch.sum(sigmoid_cos_sim_matrix)\n    return loss\n\n", "CodeBleu_score": 22.89522512457019}]}
{"instance_id": "878d14ffabd2", "generation_result": [{"code": "def forward(self, x, _label=None)\n\t\n    # make sure the input is at least two utterances per speaker\n    if x.shape[1] < 2:\n        raise ValueError(\"GE2E loss requires at least two utterances per speaker.\")\n    # calculate the centroids by averaging the embeddings\n    centroids = self.calc_centroids(x)\n    # calculate the cosine similarity matrix\n    cos_sim_matrix = self.calc_cosine_sim(x, centroids)\n    # scale and shift the cosine similarity matrix\n    scaled_sim_matrix = self.scale * (cos_sim_matrix - self.shift)\n    # compute the embedding loss\n    embedding_loss = torch.sum(scaled_sim_matrix, dim=2)\n    # average the embedding loss over speakers and utterances\n    if _label is not None:\n        return embedding_loss[_label]\n    return embedding_loss.mean()\n", "CodeBleu_score": 43.537743479812804}, {"code": "def forward(self, x, _label=None)\n\t\n    # x: (num_speakers, num_utts_per_speaker, dvec_feats)\n    # label: (num_speakers,)\n    # centroids: (num_speakers, dvec_feats)\n    # cos_sim_matrix: (num_speakers, num_utts_per_speaker, num_speakers)\n    # loss: (1,)\n    # centroids: (num_speakers, dvec_feats)\n    # cos_sim_matrix: (num_speakers, num_utts_per_speaker, num_speakers)\n    # loss: (1,)\n    x = x.transpose(1, 2)\n    if self.centroids is None:\n        self.centroids = x.mean(dim=1)\n    cos_sim_matrix = self.calc_cosine_sim(x, self.centroids)\n    loss = self.calc_embedding_loss(cos_sim_matrix)\n    return loss\n", "CodeBleu_score": 35.19335998458304}, {"code": "def forward(self, x, _label=None)\n\t\n    # ensure the input has at least two utterances per speaker\n    if x.shape[1] < 2:\n        raise ValueError(\n            \"The GE2E loss function requires at least two utterances per speaker.\"\n        )\n    # calculate the centroids by averaging the embeddings\n    centroids = self.calc_centroids(x)\n    # calculate the cosine similarity matrix\n    cos_sim_matrix = self.calc_cosine_sim(x, centroids)\n    # scale and shift the cosine similarity matrix\n    scaled_sim_matrix = self.scale * (cos_sim_matrix - self.shift)\n    # calculate the embedding loss\n    if _label is not None:\n        embedding_loss = self.embedding_loss(scaled_sim_matrix, _label)\n    else:\n        embedding_loss = self.embedding_loss(scaled_sim_matrix)\n    # average the embedding loss over speakers and utterances\n    return embedding_loss.mean()\n\n", "CodeBleu_score": 42.20860533720672}]}
{"instance_id": "86a364c72caa", "generation_result": [{"code": "def get_voices(extra_voice_dirs: List[str] = [])\n\t\n    voice_dirs = [os.path.join(os.path.dirname(__file__), \"voices\")]\n    voice_dirs.extend(extra_voice_dirs)\n    voices = {}\n    for voice_dir in voice_dirs:\n        if os.path.isdir(voice_dir):\n            for voice_subdir in os.listdir(voice_dir):\n                if os.path.isdir(os.path.join(voice_dir, voice_subdir)):\n                    voice_files = []\n                    for voice_file in os.listdir(os.path.join(voice_dir, voice_subdir)):\n                        if voice_file.endswith(\".npz\"):\n                            voice_files.append(os.path.join(voice_dir, voice_subdir, voice_file))\n                        elif voice_file.endswith(\".wav\") or voice_file.endswith(\".mp3\"):\n                            voice_files.append(os.path.join(voice_dir, voice_subdir, voice_file))\n                    if len(voice_files) > 0:\n                        voices[voice_subdir] = voice_files\n    return voices\n\n", "CodeBleu_score": 25.15932376314372}, {"code": "def get_voices(extra_voice_dirs: List[str] = [])\n\t\n    voice_files = {}\n    for voice_dir in voice_dirs + extra_voice_dirs:\n        voice_files[voice_dir] = []\n        for (dirpath, dirnames, filenames) in os.walk(voice_dir):\n            for filename in filenames:\n                if filename.endswith('.npz'):\n                    voice_files[voice_dir].append(os.path.join(dirpath, filename))\n                elif filename.endswith('.wav') or filename.endswith('.mp3'):\n                    voice_files[voice_dir].append(os.path.join(dirpath, filename))\n    return voice_files\n\n", "CodeBleu_score": 24.41361663489592}, {"code": "def get_voices(extra_voice_dirs: List[str] = [])\n\t\n    voice_files = {}\n    for voice_dir in VOICE_DIRS + extra_voice_dirs:\n        voice_files[voice_dir] = []\n        for root, _, files in os.walk(voice_dir):\n            for file in files:\n                if file.endswith(\".npz\"):\n                    voice_files[voice_dir].append(os.path.join(root, file))\n                elif file.endswith(\".wav\") or file.endswith(\".mp3\"):\n                    voice_files[voice_dir].append(os.path.join(root, file))\n    return voice_files\n\n", "CodeBleu_score": 26.92744776560525}]}
{"instance_id": "02f20c8917e6", "generation_result": [{"code": "def load_voice(model, voice: str, extra_voice_dirs: List[str] = [])\n\t\n    if voice == \"random\":\n        return None, None, None\n\n    voices = get_voices(extra_voice_dirs)\n    paths = voices[voice]\n\n    # bark only uses a single sample for cloning\n    if len(paths) > 1:\n        raise ValueError(f\"Voice {voice} has multiple paths: {paths}\")\n\n    try:\n        path = voices[voice]\n    except KeyError as e:\n        raise KeyError(f\"Voice {voice} not found in {extra_voice_dirs}\") from e\n\n    if len(paths) == 1 and paths[0].endswith(\".npz\"):\n        return load_npz(path[0])\n\n    audio_path = paths[0]\n    # replace the file extension with .npz\n    output_path = os.path.splitext(audio_path)[0] + \".npz\"\n    generate_voice(audio=audio_path, model=model, output_path=output_path)\n    return load_voice(model, voice, extra_voice_dirs)\n\n", "CodeBleu_score": 96.1921767598096}, {"code": "def load_voice(model, voice: str, extra_voice_dirs: List[str] = [])\n\t\n    if voice == \"random\":\n        return None, None, None\n\n    voices = get_voices(extra_voice_dirs)\n    paths = voices[voice]\n\n    # bark only uses a single sample for cloning\n    if len(paths) > 1:\n        raise ValueError(f\"Voice {voice} has multiple paths: {paths}\")\n\n    try:\n        path = voices[voice]\n    except KeyError as e:\n        raise KeyError(f\"Voice {voice} not found in {extra_voice_dirs}\") from e\n\n    if len(paths) == 1 and paths[0].endswith(\".npz\"):\n        return load_npz(path[0])\n\n    audio_path = paths[0]\n    # replace the file extension with .npz\n    output_path = os.path.splitext(audio_path)[0] + \".npz\"\n    generate_voice(audio=audio_path, model=model, output_path=output_path)\n    return load_voice(model, voice, extra_voice_dirs)\n", "CodeBleu_score": 96.1921767598096}, {"code": "def load_voice(model, voice: str, extra_voice_dirs: List[str] = [])\n\t\n    if voice == \"random\":\n        return None, None, None\n\n    voices = get_voices(extra_voice_dirs)\n    paths = voices[voice]\n\n    # bark only uses a single sample for cloning\n    if len(paths) > 1:\n        raise ValueError(f\"Voice {voice} has multiple paths: {paths}\")\n\n    try:\n        path = voices[voice]\n    except KeyError as e:\n        raise KeyError(f\"Voice {voice} not found in {extra_voice_dirs}\") from e\n\n    if len(paths) == 1 and paths[0].endswith(\".npz\"):\n        return load_npz(path[0])\n\n    audio_path = paths[0]\n    # replace the file extension with .npz\n    output_path = os.path.splitext(audio_path)[0] + \".npz\"\n    generate_voice(audio=audio_path, model=model, output_path=output_path)\n    return load_voice(model, voice, extra_voice_dirs)\n", "CodeBleu_score": 96.1921767598096}]}
{"instance_id": "9458e981389f", "generation_result": [{"code": "def _relative_shift(self, pos_score: torch.Tensor) -> torch.Tensor\n\t\n    batch_size, seq_len, _ = pos_score.size()\n    zero_tensor = torch.zeros(batch_size, seq_len, 1).to(pos_score.device)\n    pos_score = torch.cat([zero_tensor, pos_score], dim=2)\n    pos_score = pos_score.view(batch_size, seq_len * 2 + 1)\n    pos_score = pos_score[:, 1:seq_len + 1]\n    return pos_score\n\n", "CodeBleu_score": 26.512077452950656}, {"code": "def _relative_shift(self, pos_score: torch.Tensor) -> torch.Tensor\n\t\n    # Extract tensor's dimensions\n    batch_size, seq_len, _ = pos_score.size()\n\n    # Create a zero tensor with an additional dimension\n    zero_tensor = torch.zeros((batch_size, seq_len, 1), device=pos_score.device)\n\n    # Concatenate the zero tensor with the original position score tensor along the last dimension\n    pos_score = torch.cat((zero_tensor, pos_score), dim=2)\n\n    # Reshape and slice the concatenated tensor to achieve the relative shift\n    pos_score = pos_score.view((batch_size, seq_len + 1, seq_len))[:, 1:, :]\n\n    return pos_score\n\n", "CodeBleu_score": 25.915512594079935}, {"code": "def _relative_shift(self, pos_score: torch.Tensor) -> torch.Tensor\n\t\n    # Extract the tensor's dimensions.\n    batch_size, num_heads, seq_length = pos_score.size()\n    # Create a zero tensor with an additional dimension.\n    zero_tensor = torch.zeros((batch_size, num_heads, seq_length, 1), device=self.device)\n    # Concatenate the zero tensor with the original position score tensor along the last dimension.\n    pos_score = torch.cat((zero_tensor, pos_score), dim=-1)\n    # Reshape the concatenated tensor.\n    pos_score = pos_score.view(batch_size, num_heads, seq_length + 1, seq_length)\n    # Slice the tensor to achieve the relative shift.\n    pos_score = pos_score[:, :, 1:].view_as(pos_score)\n    return pos_score\n\n", "CodeBleu_score": 39.90561081170545}]}
{"instance_id": "c046d4f00e80", "generation_result": [{"code": "def run_padded_sequence(self, context, lens)\n\t\n    # Extract the context up to the specified length\n    context = context[:, :lens.max()]\n    # Apply the convolutional layers\n    context = self.conv(context)\n    # Store the results\n    self.context = context\n    # Pad the results to create a consistent batch size\n    context = pad_sequence(context, batch_first=True, padding_value=0)\n    return context\n\n", "CodeBleu_score": 16.32539482764614}, {"code": "def run_padded_sequence(self, context, lens)\n\t\n    # Extract the context up to the specified length\n    context = context[:, : lens[0], :]\n    # Apply the convolutional layers\n    for i in range(self.num_layers):\n        # Apply the convolutional layer\n        context = self.conv_layers[i](context)\n        # Apply the ReLU activation\n        context = F.relu(context)\n        # Apply dropout\n        context = F.dropout(context, p=self.dropout, training=self.training)\n    # Pad the sequence to create a consistent batch size\n    context = pad_sequence(context, batch_first=True)\n    # Return the padded sequence\n    return context\n\n", "CodeBleu_score": 26.136726486798644}, {"code": "def run_padded_sequence(self, context, lens)\n\t\n\n    # Create a padded sequence\n    padded_sequence = torch.nn.utils.rnn.pad_sequence(context, batch_first=True)\n\n    # Apply convolutional layers\n    for conv in self.convolutional_layers:\n        # Apply convolutional layer\n        padded_sequence = conv(padded_sequence)\n\n        # Apply ReLU activation\n        padded_sequence = torch.nn.functional.relu(padded_sequence)\n\n        # Apply dropout\n        padded_sequence = torch.nn.functional.dropout(padded_sequence, p=self.dropout_rate, training=self.training)\n\n    # Pad the sequence\n    padded_sequence = torch.nn.utils.rnn.pad_sequence(padded_sequence, batch_first=True)\n\n    # Return the padded sequence\n    return padded_sequence\n\n", "CodeBleu_score": 19.547031979917197}]}
{"instance_id": "db2644f3c681", "generation_result": [{"code": "def run_unsorted_inputs(self, fn, context, lens)\n\t\n    lens, indices = torch.sort(lens, descending=True)\n    context = context.index_select(0, indices)\n    context = pack_padded_sequence(context, lens, batch_first=True)\n    context, _ = fn(context)\n    context, _ = pad_packed_sequence(context, batch_first=True)\n    context = context.index_select(0, indices)\n    return context\n\n", "CodeBleu_score": 14.52310572258885}, {"code": "def run_unsorted_inputs(self, fn, context, lens)\n\t\n    lens, indices = torch.sort(lens, descending=True)\n    context = pack_padded_sequence(context[indices], lens, batch_first=True)\n    context, _ = fn(context)\n    context, _ = pad_packed_sequence(context, batch_first=True)\n    return context[torch.argsort(indices)]\n\n", "CodeBleu_score": 12.665032426049303}, {"code": "def run_unsorted_inputs(self, fn, context, lens)\n\t\n    if len(lens) == 0:\n        return context\n    sorted_lens, sorted_indices = torch.sort(lens, descending=True)\n    sorted_context = pack_padded_sequence(context[sorted_indices], sorted_lens, batch_first=True)\n    sorted_context, _ = fn(sorted_context)\n    sorted_context, _ = pad_packed_sequence(sorted_context, batch_first=True)\n    unsorted_context = sorted_context[torch.argsort(sorted_indices)]\n    return unsorted_context\n\n", "CodeBleu_score": 29.053842881893893}]}
{"instance_id": "ced082bffc63", "generation_result": [{"code": "def remove_weight_norm(self)\n\t\n    self.kernel_predictor.remove_weight_norm()\n    parametrize.remove_parametrizations(self.convt_pre[1], \"weight\")\n    for block in self.conv_blocks:\n        parametrize.remove_parametrizations(block[1], \"weight\")\n", "CodeBleu_score": 70.15353357610886}, {"code": "def remove_weight_norm(self)\n\t\n    self.kernel_predictor.remove_weight_norm()\n    parametrize.remove_parametrizations(self.convt_pre[1], \"weight\")\n    for block in self.conv_blocks:\n        parametrize.remove_parametrizations(block[1], \"weight\")\n", "CodeBleu_score": 70.15353357610886}, {"code": "def remove_weight_norm(self)\n\t\n    self.kernel_predictor.remove_weight_norm()\n    parametrize.remove_parametrizations(self.convt_pre[1], \"weight\")\n    for block in self.conv_blocks:\n        parametrize.remove_parametrizations(block[1], \"weight\")\n", "CodeBleu_score": 70.15353357610886}]}
{"instance_id": "b5a39fd97061", "generation_result": [{"code": "def map_mx_to_torch_model(mx_name)\n\t\n    # Convert to lowercase\n    mx_name = mx_name.lower()\n\n    # Replace 'se_' with 'senet_'\n    mx_name = mx_name.replace('se_', 'senet_')\n\n    # Replace 'inceptionv3' with 'inception_v3'\n    mx_name = mx_name.replace('inceptionv3', 'inception_v3')\n\n    # Prefix with 'gluon_'\n    return 'gluon_' + mx_name\n\n", "CodeBleu_score": 25.44052296016685}, {"code": "def map_mx_to_torch_model(mx_name)\n\t\n    mx_name = mx_name.lower()\n    mx_name = mx_name.replace('se_', 'senet_')\n    mx_name = mx_name.replace('inceptionv3', 'inception_v3')\n    return 'gluon_' + mx_name\n\n", "CodeBleu_score": 25.44052296016685}, {"code": "def map_mx_to_torch_model(mx_name)\n\t\n    # Convert input name to lowercase\n    mx_name = mx_name.lower()\n    # Apply specific replacements\n    mx_name = mx_name.replace('se_', 'senet_')\n    mx_name = mx_name.replace('inceptionv3', 'inception_v3')\n    # Prefix name with 'gluon_'\n    mx_name = 'gluon_' + mx_name\n    return mx_name\n\n", "CodeBleu_score": 29.59624453822981}]}
{"instance_id": "7eee5efdbf67", "generation_result": [{"code": "def _num_samples_per_worker(self)\n\t\n    num_samples = self.num_samples\n    num_replicas = self.num_replicas\n    num_workers = self.num_workers\n    batch_size = self.batch_size\n\n    if num_workers is None:\n        num_workers = 1\n\n    if num_replicas is None:\n        num_replicas = 1\n\n    if num_samples is None:\n        raise ValueError(\"The number of samples must be specified.\")\n\n    if num_samples < 0:\n        raise ValueError(\"The number of samples must be positive.\")\n\n    if num_replicas > num_workers:\n        raise ValueError(\"The number of replicas must be less than or equal to the number of workers.\")\n\n    if batch_size is not None:\n        if batch_size < 0:\n            raise ValueError(\"The batch size must be positive.\")\n\n        num_samples = int(math.ceil(num_samples / batch_size)) * batch_size\n\n    if num_workers > num_samples:\n        raise ValueError(\"The number of workers must be less than or equal to the number of samples.\")\n\n    if num_workers > 1:\n        num_samples = int(math.ceil(num_samples / num_workers)) * num_workers\n\n    return num_samples\n\n", "CodeBleu_score": 27.316767288978095}, {"code": "def _num_samples_per_worker(self)\n\t\n    # Get the number of samples for each replica\n    num_samples = self.num_samples_per_replica\n    if self.num_replicas > 1:\n        # If multiple replicas are used, round up the number of samples\n        num_samples = math.ceil(num_samples / self.num_replicas)\n    if self.batch_size is not None:\n        # Adjust the number of samples to be a multiple of the batch size\n        num_samples = math.ceil(num_samples / self.batch_size) * self.batch_size\n    return num_samples\n\n", "CodeBleu_score": 37.988483302595235}, {"code": "def _num_samples_per_worker(self)\n\t\n    if self.num_replicas > 1:\n        if self.num_workers > self.num_replicas:\n            print(\n                \"WARNING: The number of workers should be less than the number of replicas.\"\n            )\n        if self.num_workers > self.num_samples:\n            print(\n                \"WARNING: The number of workers should be less than the number of samples.\"\n            )\n        num_samples_per_worker = math.ceil(\n            self.num_samples / self.num_workers / self.num_replicas\n        )\n    else:\n        if self.num_workers > self.num_samples:\n            print(\n                \"WARNING: The number of workers should be less than the number of samples.\"\n            )\n        num_samples_per_worker = math.ceil(self.num_samples / self.num_workers)\n    if self.batch_size is not None:\n        num_samples_per_worker = math.ceil(\n            num_samples_per_worker / self.batch_size\n        ) * self.batch_size\n    return num_samples_per_worker\n\n", "CodeBleu_score": 32.893610222445524}]}
{"instance_id": "a698d65ae190", "generation_result": [{"code": "def find_images_and_targets(\n        folder: str,\n        types: Optional[Union[List, Tuple, Set]] = None,\n        class_to_idx: Optional[Dict] = None,\n        leaf_name_only: bool = True,\n        sort: bool = True\n)\n\t\n    if types is None:\n        types = IMG_EXTENSIONS\n    if class_to_idx is None:\n        class_to_idx = {}\n    imgs = []\n    targets = []\n    for root, _, fnames in sorted(os.walk(folder, followlinks=True)):\n        for fname in sorted(fnames):\n            if fname.lower().endswith(types):\n                if leaf_name_only:\n                    path = os.path.join(root, fname)\n                    target = path.split(os.path.sep)[-2]\n                else:\n                    path = os.path.join(root, fname)\n                    target = path.split(os.path.sep)[-1]\n                if target not in class_to_idx:\n                    class_to_idx[target] = len(class_to_idx)\n                imgs.append(path)\n                targets.append(class_to_idx[target])\n    if sort:\n        indices = np.argsort(imgs)\n        imgs = np.array(imgs)[indices]\n        targets = np.array(targets)[indices]\n    return imgs, targets, class_to_idx\n\n", "CodeBleu_score": 36.514933368193894}, {"code": "def find_images_and_targets(\n        folder: str,\n        types: Optional[Union[List, Tuple, Set]] = None,\n        class_to_idx: Optional[Dict] = None,\n        leaf_name_only: bool = True,\n        sort: bool = True\n)\n\t\n    if types is None:\n        types = ('jpg', 'jpeg', 'png', 'bmp')\n    if class_to_idx is None:\n        class_to_idx = {}\n    images = []\n    dir = Path(folder)\n    for target in sorted(dir.iterdir(), key=natural_key):\n        if target.is_dir():\n            for root, _, fnames in sorted(os.walk(target)):\n                for fname in sorted(fnames):\n                    if fname.lower().endswith(types):\n                        path = os.path.join(root, fname) if leaf_name_only else os.path.relpath(os.path.join(root, fname), dir)\n                        item = (path, class_to_idx.setdefault(target.name, len(class_to_idx)))\n                        images.append(item)\n    if sort:\n        images.sort(key=lambda x: natural_key(x[0]))\n    return images, class_to_idx\n\n", "CodeBleu_score": 34.82633832996236}, {"code": "def find_images_and_targets(\n        folder: str,\n        types: Optional[Union[List, Tuple, Set]] = None,\n        class_to_idx: Optional[Dict] = None,\n        leaf_name_only: bool = True,\n        sort: bool = True\n)\n\t\n    if types is None:\n        types = ('.jpg', '.jpeg', '.png', '.ppm', '.bmp', '.pgm', '.tif', '.tiff', '.webp')\n    if class_to_idx is None:\n        class_to_idx = {}\n    images = []\n    dir = os.path.expanduser(folder)\n    for target in sorted(os.listdir(dir)):\n        d = os.path.join(dir, target)\n        if not os.path.isdir(d):\n            continue\n        for root, _, fnames in sorted(os.walk(d)):\n            for fname in sorted(fnames):\n                if fname.lower().endswith(types):\n                    path = os.path.join(root, fname) if leaf_name_only else os.path.relpath(os.path.join(root, fname), start=dir)\n                    item = (path, class_to_idx.setdefault(target, len(class_to_idx)))\n                    images.append(item)\n    if sort:\n        images = sorted(images, key=lambda k: natural_key(k[0]))\n    return images, class_to_idx\n\n", "CodeBleu_score": 40.418156767103916}]}
{"instance_id": "e7fe43bdecdc", "generation_result": [{"code": "def pytorch_worker_seed()\n\t\n    return _pytorch_worker_seed()\n\n", "CodeBleu_score": 2.761889457221136}, {"code": "def pytorch_worker_seed()\n\t\n    return _pytorch_worker_seed()\n\n", "CodeBleu_score": 2.761889457221136}, {"code": "def pytorch_worker_seed()\n\t\n    return _get_pytorch_worker_seed()\n\n", "CodeBleu_score": 3.219438749892223}]}
{"instance_id": "0ffc7e7ded25", "generation_result": [{"code": "def solarize_add(img, add, thresh=128, **__)\n\t\n    if img.mode != 'L' and img.mode != 'RGB':\n        return img\n    lut = []\n    for i in range(256):\n        if i < thresh:\n            lut.append(min(255, add + i))\n        else:\n            lut.append(i)\n    if img.mode == 'RGB':\n        return img.point(lut)\n    if img.mode == 'L':\n        return img.point(lut, 'L')\n\n", "CodeBleu_score": 62.30607320809639}, {"code": "def solarize_add(img, add, thresh=128, **__)\n\t\n    return ImageOps.solarize(img, thresh) + add\n\n", "CodeBleu_score": 4.537674568464032}, {"code": "def solarize_add(img, add, thresh=128, **__)\n\t\n    return cv2.LUT(img, np.clip(add + img, 0, 255) if img.dtype == np.uint8 else add + img)\n\n", "CodeBleu_score": 8.016774852369771}]}
{"instance_id": "5a0d29577a21", "generation_result": [{"code": "def _get_weighted_transforms(transforms: Dict)\n\t\n    transform_names = list(transforms.keys())\n    transform_probs = list(transforms.values())\n    transform_probs = [p / sum(transform_probs) for p in transform_probs]\n    return transform_names, transform_probs\n\n", "CodeBleu_score": 20.698957173702425}, {"code": "def _get_weighted_transforms(transforms: Dict)\n\t\n    transform_names = []\n    transform_weights = []\n    for transform, weight in transforms.items():\n        transform_names.append(transform)\n        transform_weights.append(weight)\n    transform_weights = np.array(transform_weights)\n    transform_weights = transform_weights / np.sum(transform_weights)\n    return transform_names, transform_weights\n\n", "CodeBleu_score": 33.260088849491424}, {"code": "def _get_weighted_transforms(transforms: Dict)\n\t\n    transform_names = list(transforms.keys())\n    transform_probs = list(transforms.values())\n    transform_probs = np.array(transform_probs)\n    transform_probs = transform_probs / np.sum(transform_probs)\n    return transform_names, transform_probs\n\n", "CodeBleu_score": 20.3734651568263}]}
{"instance_id": "84bc5658c135", "generation_result": [{"code": "def _apply_basic(self, img, mixing_weights, m)\n\t\n    if m == 0:\n        return img\n    elif m == 1:\n        return self.apply(img, 0)\n    else:\n        mixed_image = np.zeros_like(img)\n        for i in range(m):\n            aug = self.to_deterministic()\n            img_aug = aug.augment_image(img)\n            w = mixing_weights[i]\n            mixed_image += w * img_aug\n        return np.clip(mixed_image, 0, 255).astype(np.uint8)\n\n", "CodeBleu_score": 23.695851432946686}, {"code": "def _apply_basic(self, img, mixing_weights, m)\n\t\n    # img = img.copy()\n    # for i in range(m):\n    #     aug = self.get_random_aug(img)\n    #     img = aug.apply(img)\n    # return img\n    img = img.copy()\n    for i in range(m):\n        aug = self.get_random_aug(img)\n        img = aug.apply(img)\n    return img\n\n", "CodeBleu_score": 10.065751632032066}, {"code": "def _apply_basic(self, img, mixing_weights, m)\n\t\n    # Initialize an empty array for the mixed image\n    mixed_image = np.zeros_like(img)\n    # Iterate through given mixing weights\n    for i in range(len(mixing_weights)):\n        # Apply a random sequence of operations to the image\n        aug_img = apply_augmentations(self, img, m)\n        # Accumulate the results\n        mixed_image += aug_img * mixing_weights[i]\n    # Clip the image to valid pixel values\n    mixed_image = np.clip(mixed_image, 0, 255)\n    # Convert the image back to an RGB image\n    mixed_image = Image.fromarray(mixed_image.astype(np.uint8)).convert('RGB')\n    # Blend the original image with the mixed image\n    blended = Image.blend(img, mixed_image, self.blending_factor)\n    return blended\n\n", "CodeBleu_score": 25.379759635750503}]}
{"instance_id": "84b5b752ff04", "generation_result": [{"code": "def adapt_to_chs(x, n)\n\t\n    if not isinstance(x, (tuple, list)):\n        x = (x,) * n\n    else:\n        if len(x) == 1:\n            x = x * n\n        else:\n            if len(x) != n:\n                print(\n                    f\"WARNING: The input {x} has {len(x)} channels, but {n} are required. The mean of {x} will be used.\"\n                )\n                x = (np.mean(x),) * n\n    assert len(x) == n\n    return x\n\n", "CodeBleu_score": 38.76984254590538}, {"code": "def adapt_to_chs(x, n)\n\t\n    if not isinstance(x, (tuple, list)):\n        x = (x,) * n\n        return x\n    elif len(x) == n:\n        return x\n    elif len(x) != n:\n        mean = sum(x) / len(x)\n        x = (mean,) * n\n        warnings.warn(f\"Length of input {x} does not match required number of channels {n}.", "CodeBleu_score": 43.60848286882439}, {"code": "def adapt_to_chs(x, n)\n\t\n    if isinstance(x, tuple) or isinstance(x, list):\n        if len(x) == n:\n            assert len(x) == n\n        elif len(x) == 1:\n            x = x * n\n            assert len(x) == n\n        else:\n            warnings.warn('The length of the input tuple is not equal to the required number of channels n. The tuple is repeated to match the required number of channels.')\n            x = x * n\n            assert len(x) == n\n    else:\n        x = (x,) * n\n        assert len(x) == n\n    return x\n\n", "CodeBleu_score": 29.90024626570605}]}
{"instance_id": "54754b6ba394", "generation_result": [{"code": "def _worker_init(worker_id, worker_seeding='all')\n\t\n    worker_info = torch.utils.data.get_worker_info()\n    if worker_info is None:\n        raise RuntimeError('Cannot initialize a worker without worker_info')\n    if worker_info.id != worker_id:\n        raise RuntimeError('Expected worker_id {}, but got {}'.format(worker_id, worker_info.id))\n    if callable(worker_seeding):\n        worker_seeding(worker_info.seed)\n    elif worker_seeding == 'all':\n        np.random.seed(worker_info.seed)\n        torch.manual_seed(worker_info.seed)\n        torch.cuda.manual_seed(worker_info.seed)\n        torch.cuda.manual_seed_all(worker_info.seed)\n    else:\n        raise RuntimeError('Unknown seeding method: {}'.format(worker_seeding))\n\n", "CodeBleu_score": 32.79395244611233}, {"code": "def _worker_init(worker_id, worker_seeding='all')\n\t\n    worker_info = torch.utils.data.get_worker_info()\n    if worker_info is None:\n        raise RuntimeError(\"No worker information available. Please use torch.utils.data.DataLoader(..., num_workers=N, worker_init_fn=worker_init_fn) to set the worker_init_fn function.\")\n    worker_id = worker_info.id\n    if worker_seeding is None:\n        pass\n    elif callable(worker_seeding):\n        worker_seeding(worker_id)\n    elif isinstance(worker_seeding, str):\n        np.random.seed(worker_info.seed % (2**32 - 1))\n    else:\n        raise ValueError(\"The worker_seeding parameter must be a callable or a string.\")\n\n", "CodeBleu_score": 35.410974014631805}, {"code": "def _worker_init(worker_id, worker_seeding='all')\n\t\n    worker_info = torch.utils.data.get_worker_info()\n    if worker_info is None:\n        raise ValueError(\"This function should only be called in a worker.\")\n    if worker_id != worker_info.id:\n        raise ValueError(\"The worker ID is not the same as the worker info.\")\n\n    if callable(worker_seeding):\n        worker_seeding(worker_info.seed)\n    elif worker_seeding == 'all':\n        seed = worker_info.seed\n        np.random.seed(seed)\n        torch.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n    else:\n        raise ValueError(\"The worker seeding is not callable or 'all'.\")\n\n", "CodeBleu_score": 36.4868372800797}]}
{"instance_id": "81dfdce940db", "generation_result": [{"code": "def _decode_and_center_crop(image_bytes, image_size, resize_method)\n\t\n    # 1. Extract the image shape and calculate the padded center crop size\n    shape = tf.image.extract_jpeg_shape(image_bytes)\n    image_height = shape[0]\n    image_width = shape[1]\n\n    padded_center_crop_size = tf.cast(\n        ((image_size / (image_size + CROP_PADDING)) *\n         tf.cast(tf.minimum(image_height, image_width), tf.float32)), tf.int32)\n\n    # 2. Determine the offset for the center crop\n    offset_height = ((image_height - padded_center_crop_size) + 1) // 2\n    offset_width = ((image_width - padded_center_crop_size) + 1) // 2\n\n    # 3. Crop the center of the image\n    image = tf.image.decode_and_crop_jpeg(\n        image_bytes, [offset_height, offset_width, padded_center_crop_size, padded_center_crop_size], channels=3)\n\n    # 4. Resize the image to the desired size\n    image = tf.image.resize([image], [image_size, image_size], method=resize_method)[0]\n\n    return image\n\n", "CodeBleu_score": 54.49574160827052}, {"code": "def _decode_and_center_crop(image_bytes, image_size, resize_method)\n\t\n    # Decode the image\n    image = tf.image.decode_jpeg(image_bytes, channels=3)\n    # Get the image shape\n    image_shape = tf.cast(tf.shape(image)[:-1], tf.float32)\n    # Calculate the padded center crop size\n    padded_center_crop_size = tf.cast(\n        ((image_size / (image_size + CROP_PADDING)) *\n         tf.reduce_min(image_shape)), tf.int32)\n\n    # Calculate the offset for the center crop\n    offset_height = ((image_shape[0] - padded_center_crop_size) + 1) // 2\n    offset_width = ((image_shape[1] - padded_center_crop_size) + 1) // 2\n    # Crop the image\n    image = tf.image.crop_to_bounding_box(\n        image, offset_height, offset_width, padded_center_crop_size, padded_center_crop_size)\n    # Resize the image\n    image = tf.image.resize([image], [image_size, image_size],\n                            method=resize_method)[0]\n    return image\n\n", "CodeBleu_score": 43.82039650728444}, {"code": "def _decode_and_center_crop(image_bytes, image_size, resize_method)\n\t\n    # 1. Extract the image shape\n    shape = tf.image.extract_jpeg_shape(image_bytes)\n    # 2. Calculate the padded center crop size\n    padded_center_crop_size = tf.cast(\n        ((image_size / (image_size + CROP_PADDING)) *\n        tf.cast(tf.minimum(shape[0], shape[1]), tf.float32)), tf.int32)\n\n    # 3. Determine the offset for the crop window\n    offset_height = ((shape[0] - padded_center_crop_size) + 1) // 2\n    offset_width = ((shape[1] - padded_center_crop_size) + 1) // 2\n    # 4. Crop the image\n    image = tf.image.decode_and_crop_jpeg(image_bytes, [offset_height, offset_width, padded_center_crop_size, padded_center_crop_size], channels=3)\n    # 5. Resize the image\n    image = tf.image.resize([image], [image_size, image_size], method=resize_method)[0]\n    return image\n\n", "CodeBleu_score": 43.43638664633475}]}
{"instance_id": "cf16a6d37111", "generation_result": [{"code": "def preprocess_for_train(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    if use_bfloat16:\n        image = tf.image.convert_image_dtype(image_bytes, dtype=tf.bfloat16)\n    else:\n        image = tf.image.convert_image_dtype(image_bytes, dtype=tf.float32)\n    image = _decode_and_random_crop(image, image_size, interpolation)\n    image = _flip(image)\n    image = tf.reshape(image, [image_size, image_size, 3])\n    return image\n\n", "CodeBleu_score": 55.55299115404806}, {"code": "def preprocess_for_train(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    image = _decode_and_random_crop(image_bytes, image_size, interpolation)\n    image = _flip(image)\n    image = tf.reshape(image, [image_size, image_size, 3])\n    if use_bfloat16:\n        image = tf.image.convert_image_dtype(image, dtype=tf.bfloat16)\n    else:\n        image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n    return image\n\n", "CodeBleu_score": 45.62482957011018}, {"code": "def preprocess_for_train(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    if use_bfloat16:\n        image = tf.image.decode_and_crop_jpeg(image_bytes, _crop_shape(image_size), channels=3)\n        image = _flip(image)\n        image = tf.reshape(image, [_image_size, _image_size, 3])\n        image = tf.image.convert_image_dtype(image, dtype=tf.bfloat16)\n    else:\n        image = _decode_and_random_crop(image_bytes, image_size, interpolation)\n        image = _flip(image)\n        image = tf.reshape(image, [_image_size, _image_size, 3])\n        image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n    return image\n", "CodeBleu_score": 53.98156424728434}]}
{"instance_id": "78b2a3bd2142", "generation_result": [{"code": "def preprocess_for_eval(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    if interpolation == 'bicubic':\n        resize_method = tf.image.ResizeMethod.BICUBIC\n    else:\n        resize_method = tf.image.ResizeMethod.BILINEAR\n\n    image = _decode_and_center_crop(image_bytes, image_size, resize_method)\n\n    image = tf.reshape(image, [image_size, image_size, 3])\n\n    if use_bfloat16:\n        image = tf.cast(image, tf.bfloat16)\n    else:\n        image = tf.cast(image, tf.float32)\n\n    return image\n\n", "CodeBleu_score": 66.25569399035093}, {"code": "def preprocess_for_eval(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    if interpolation == 'bicubic':\n        resize_method = tf.image.ResizeMethod.BICUBIC\n    else:\n        resize_method = tf.image.ResizeMethod.BILINEAR\n\n    image = _decode_and_center_crop(image_bytes, image_size, resize_method)\n    image = tf.reshape(image, [image_size, image_size, 3])\n    image = tf.image.convert_image_dtype(image, dtype=tf.bfloat16 if use_bfloat16 else tf.float32)\n\n    return image\n\n", "CodeBleu_score": 78.83007151473656}, {"code": "def preprocess_for_eval(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    if interpolation == 'bicubic':\n        resize_method = tf.image.ResizeMethod.BICUBIC\n    elif interpolation == 'bilinear':\n        resize_method = tf.image.ResizeMethod.BILINEAR\n    else:\n        resize_method = tf.image.ResizeMethod.NEAREST_NEIGHBOR\n\n    image = _decode_and_center_crop(image_bytes, image_size, resize_method)\n    image = tf.reshape(image, [image_size, image_size, 3])\n    image = tf.image.convert_image_dtype(image, dtype=tf.bfloat16 if use_bfloat16 else tf.float32)\n\n    return image\n\n", "CodeBleu_score": 69.63579119187676}]}
{"instance_id": "8bd4364a9a0e", "generation_result": [{"code": "def evaluate(self, model, eval_dataloader=None, output_file=\"merged\")\n\t\n\n    # Set up the default evaluation dataloader if none is provided\n    if eval_dataloader is None:\n        eval_dataloader = self.get_eval_dataloader()\n\n    # Get the model predictions\n    predictions, labels, metrics = self.prediction_loop(model, eval_dataloader)\n\n    # Save the model predictions to a file\n    self.save_predictions(predictions, labels, output_file)\n\n    return metrics\n\n", "CodeBleu_score": 48.57570028950414}, {"code": "def evaluate(self, model, eval_dataloader=None, output_file=\"merged\")\n\t\n    if eval_dataloader is None:\n        eval_dataloader = self.eval_dataloader\n    if not isinstance(eval_dataloader, DataLoader):\n        raise ValueError(\"eval_dataloader is not a torch.utils.data.DataLoader object\")\n    eval_dataloader.collate_fn = self.data_collator\n    model.eval()\n    self.control = self.callback_handler.on_evaluate(self.args, self.state, self.control)\n    # if output_file is None:\n    #     output_file = os.path.join(self.args.output_dir, f\"eval_results_{self.args.max_seq_length}.txt\")\n    # else:\n    #     output_file = os.path.join(self.args.output_dir, output_file)\n    # if os.path.isfile(output_file) and os.stat(output_file).st_size > 0:\n    #     self.logger.info(f\"Loading evaluation results from {output_file}\")\n    #     with open(output_file, \"r\") as f:\n    #         results = json.load(f)\n    # else:\n    #     self.logger.info(f\"Evaluating with {len(eval_dataset)} samples\")\n    #     if output_file is not None:\n    #         os.makedirs(os.path.dirname(output_file), exist_ok=True)\n    #     else:\n    #         output_file = os.path.join(self.args.output_dir, f\"eval_results_{self.args.max_seq_length}.txt\")\n    #     with open(output_file, \"w\") as writer:\n    #         writer.write(\"***** Eval results *****", "CodeBleu_score": 47.61593713477279}, {"code": "def evaluate(self, model, eval_dataloader=None, output_file=\"merged\")\n\t\n    self.log.info(f\"Starting evaluation\")\n    if eval_dataloader is None:\n        eval_dataloader = self.eval_dataloader\n    self.compute_metrics = self.compute_metrics_fn(self.eval_metric_name)\n    self.eval_metric = self.compute_metrics()\n    self.eval_dataset = eval_dataloader.dataset\n    self.eval_dataloader = eval_dataloader\n    self.eval_examples = self.eval_dataset.examples\n    self.eval_features = self.eval_dataset.features\n    self.eval_batch_size = self.eval_dataloader.batch_size\n    self.eval_steps = self.eval_steps_fn(self.eval_batch_size)\n    self.eval_loss = 0.0\n    self.model = model\n    self.model.eval()\n    self.eval_start_time = time.time()\n    self.log.info(f\"  Num examples = {self.eval_dataset.num_examples}\")\n    self.log.info(f\"  Batch size = {self.eval_batch_size}\")\n    self.log.info(f\"  Num steps = {self.eval_steps}\")\n    self.eval_file = os.path.join(self.output_dir, f\"eval_{output_file}.txt\")\n    self.eval_writer = open(self.eval_file, \"w\")\n    self.eval_features = self.eval_dataset.features\n    self.eval_dataloader = eval_dataloader\n    self.eval_examples = self.eval_dataset.examples\n    self.eval_features = self.eval_dataset.features\n    self.eval_batch_size = self.eval_dataloader.batch_size\n    self.eval_steps = self.eval_steps_fn(self.eval_batch_size)\n    self.eval_loss = 0.0\n    self.log.info(f\"***** Running evaluation *****\")\n    self.log.info(f\"  Num examples = {self.eval_dataset.num_examples}\")\n    self.log.info(f\"  Batch size = {self.eval_batch_size}\")\n    self.log.info(f\"  Num steps = {self.eval_steps}\")\n    self.eval_loss = 0.0\n    self.nb_eval_steps = ", "CodeBleu_score": 38.70808367556068}]}
{"instance_id": "22d682c958c0", "generation_result": [{"code": " def print_computed_metrics(self, metrics)\n\t\n    print('Rank1: {:.4f}'.format(metrics['rank1']))\n    print('Rank5: {:.4f}'.format(metrics['rank5']))\n    print('mIoU: {:.4f}'.format(metrics['miou']))\n\n", "CodeBleu_score": 23.703594977822583}, {"code": " def print_computed_metrics(self, metrics)\n\t\n    print(\"Rank1: {:.4f}, Rank5: {:.4f}, MIOU: {:.4f}\".format(metrics['rank1'], metrics['rank5'], metrics['miou']))", "CodeBleu_score": 19.65525961659555}, {"code": " def print_computed_metrics(self, metrics)\n\t\n    rank1 = metrics['rank1']\n    rank5 = metrics['rank5']\n    miou = metrics['miou']\n    print('Rank@1: {0:.4f}", "CodeBleu_score": 16.77495796998422}]}
{"instance_id": "39c1a50b7e7d", "generation_result": [{"code": "def _eval_predictions(self, segments, data)\n\t\n    ious = []\n    ranks = []\n    for segment, data in zip(segments, data):\n        iou = 0\n        rank = 0\n        for gt in data:\n            iou += self._iou(segment, gt)\n            rank += self._rank(segment, gt)\n        ious.append(iou/len(data))\n        ranks.append(rank/len(data))\n    return np.mean(ranks), np.mean(ious)\n", "CodeBleu_score": 21.74859918375611}, {"code": "def _eval_predictions(self, segments, data)\n\t\n    ious = []\n    ranks = []\n    for segment, data in zip(segments, data):\n        iou = []\n        rank = []\n        for gt in data:\n            iou.append(self._iou(segment, gt))\n            rank.append(self._rank(segment, gt))\n        ious.append(np.mean(iou))\n        ranks.append(np.mean(rank))\n    return np.mean(ranks), np.mean(ious)\n", "CodeBleu_score": 22.02538337629017}, {"code": "def _eval_predictions(self, segments, data)\n\t\n    rank1 = 0\n    rank5 = 0\n    iou = 0\n    for seg, gt in zip(segments, data):\n        iou += self._iou(seg, gt)\n        rank1 += self._rank(seg, gt) == 1\n        rank5 += self._rank(seg, gt) <= 5\n    return rank1/len(segments), rank5/len(segments), iou/len(segments)\n", "CodeBleu_score": 18.658391516936224}]}
{"instance_id": "8ccf84f8d034", "generation_result": [{"code": "def predict_loop(self, model, eval_dataloader, output_file=None)\n\t\n    model.eval()\n    model.to(self.to_ctx)\n    scores = []\n    for batch in tqdm(eval_dataloader):\n        batch = self.to_ctx(batch, self.to_ctx)\n        with torch.no_grad():\n            output = model(**batch)\n        scores.append(output)\n    if output_file is not None:\n        self.finalize(scores, output_file)\n    return scores\n\n", "CodeBleu_score": 34.36826160283669}, {"code": "def predict_loop(self, model, eval_dataloader, output_file=None)\n\t\n    self.scores = []\n    model.eval()\n    model.to(self.to_ctx(model))\n    for batch in eval_dataloader:\n        batch = self.to_ctx(batch)\n        with torch.no_grad():\n            output = model(**batch)\n        self.update(batch, output)\n    self.finalize(output_file)\n    return self.scores\n\n", "CodeBleu_score": 40.94136243719483}, {"code": "def predict_loop(self, model, eval_dataloader, output_file=None)\n\t\n    # Initialize the list for scores\n    scores = []\n    # Set the model to evaluation mode\n    model.eval()\n    # Transfer the model to GPU 0\n    model = model.to(self.to_ctx(model, ctx=0))\n    # Iterate over the dataloader\n    for i, batch in enumerate(eval_dataloader):\n        # Transfer the data to the appropriate context\n        batch = self.to_ctx(batch, ctx=0)\n        # Make predictions without computing gradients\n        with torch.no_grad():\n            # Make predictions\n            output = model(**batch)\n        # Update the outputs with the input data\n        output = self.update_outputs(output, batch)\n        # Process the outputs further\n        output = self.finalize(output)\n        # Update the list for scores\n        scores.append(output)\n    # Finalize and return the results\n    return scores\n\n", "CodeBleu_score": 49.76117572878758}]}
{"instance_id": "e68529ad727d", "generation_result": [{"code": "def _aggregate_scores(self, scores)\n\t\n    assert len(scores) == 2, \"There must be exactly two sets of scores.\"\n    scores = torch.cat(scores, dim=0)\n    self.full_scores = None\n    return scores.dot(self.text_hidden_state.squeeze(0)).squeeze(0)\n\n", "CodeBleu_score": 30.42480269772772}, {"code": "def _aggregate_scores(self, scores)\n\t\n    assert len(scores) == 2, \"There should be exactly two sets of scores.\"\n    self.full_scores = np.concatenate(scores, axis=0)\n    self.full_scores = np.squeeze(self.full_scores)\n    self.full_scores = np.squeeze(self.full_scores)\n    return np.dot(self.full_scores[0], self.full_scores[1])\n\n", "CodeBleu_score": 33.21737363575551}, {"code": "def _aggregate_scores(self, scores)\n\t\n    assert len(scores) == 2, 'There should be exactly two sets of scores'\n    video_scores, text_scores = scores\n    video_scores = torch.cat(video_scores, dim=0)\n    text_scores = torch.cat(text_scores, dim=0)\n    self.full_scores = None\n    return torch.dot(text_scores, video_scores)\n\n", "CodeBleu_score": 30.810046803828595}]}
{"instance_id": "e4e1861fc357", "generation_result": [{"code": "def finalize(self, Y_pred, Y_true, output_file=None)\n\t\n    Y_pred = np.array(Y_pred)\n    Y_true = np.array(Y_true)\n    if Y_pred.shape != Y_true.shape:\n        print(\"Error: The shape of the predicted and true labels must be the same.\")\n        return\n    if len(Y_pred.shape) > 1 and Y_pred.shape[1] > 1:\n        print(\"Warning: The shape of the predicted and true labels is not a vector. Only the first column will be used.\")\n        Y_pred = Y_pred[:,0]\n        Y_true = Y_true[:,0]\n    if len(Y_pred.shape) > 1 and Y_pred.shape[1] > 1:\n        print(\"Warning: The shape of the predicted and true labels is not a vector. Only the first column will be used.\")\n        Y_pred = Y_pred[:,0]\n        Y_true = Y_true[:,0]\n    if output_file is not None:\n        with open(output_file, 'wb') as f:\n            pickle.dump({\"Y_pred\": Y_pred, \"Y_true\": Y_true}, f)\n    print(\"Number of samples: %d\" % len(Y_pred))\n    print(\"Number of errors: %d\" % (Y_pred != Y_true).sum())\n    if (Y_pred != Y_true).sum() > 0:\n        print(\"Samples of errors:\")\n        for i in range(len(Y_pred)):\n            if Y_pred[i] != Y_true[i]:\n                print(\"Predicted: %s\\tTrue: %s\" % (Y_pred[i], Y_true[i]))\n    return {\"Y_pred\": Y_pred, \"Y_true\": Y_true}\n\n", "CodeBleu_score": 41.44779406020939}, {"code": "def finalize(self, Y_pred, Y_true, output_file=None)\n\t\n    Y_pred = np.array(Y_pred)\n    Y_true = np.array(Y_true)\n    if Y_pred.shape[0] != Y_true.shape[0]:\n        raise ValueError(\n            \"Number of predictions and true labels must be equal. Got {} and {}.\".format(\n                Y_pred.shape[0], Y_true.shape[0]\n            )\n        )\n    if len(Y_pred.shape) != 2:\n        raise ValueError(\n            \"Predictions must be a 2-dimensional array. Got {}-dimensional array.\".format(\n                len(Y_pred.shape)\n            )\n        )\n    if len(Y_true.shape) != 2:\n        raise ValueError(\n            \"True labels must be a 2-dimensional array. Got {}-dimensional array.\".format(\n                len(Y_true.shape)\n            )\n        )\n    if Y_pred.shape[1] != Y_true.shape[1]:\n        raise ValueError(\n            \"Number of predictions and true labels must be equal. Got {} and {}.\".format(\n                Y_pred.shape[1], Y_true.shape[1]\n            )\n        )\n    if Y_pred.shape[1] == 1:\n        Y_pred = np.squeeze(Y_pred, axis=1)\n        Y_true = np.squeeze(Y_true, axis=1)\n    if Y_pred.shape[1] != 1 and Y_pred.shape[1] != 2:\n        raise ValueError(\n            \"Predictions must be a 1- or 2-dimensional array. Got {}-dimensional array.\".format(\n                Y_pred.shape[1]\n            )\n        )\n    if Y_true.shape[1] != 1 and Y_true.shape[1] != 2:\n        raise ValueError(\n            \"True labels must be a 1- or 2-dimensional array. Got {}-dimensional array.\".format(\n                Y_true.shape[1]\n            )\n        )\n    if Y_pred.shape[1] == 1:\n        Y_pred = np.round(Y_pred)\n        Y_true = np.round(Y_true)\n    if Y_pred.shape[1] == 2:\n        Y_pred = np.argmax(Y_pred, axis=1", "CodeBleu_score": 27.382902385491903}, {"code": "def finalize(self, Y_pred, Y_true, output_file=None)\n\t\n\n    # Check if predictions are correct\n    check_prediction(self.predictions, Y_pred)\n\n    # Convert predictions and true labels to numpy arrays\n    Y_pred = np.array(Y_pred)\n    Y_true = np.array(Y_true)\n\n    # Print samples of prediction errors\n    print_errors(self.predictions, Y_pred, Y_true)\n\n    # Save predictions and true labels\n    if output_file:\n        save_predictions(Y_pred, Y_true, output_file)\n\n    # Return predictions and true labels\n    return {'Y_pred': Y_pred, 'Y_true': Y_true}\n\n", "CodeBleu_score": 15.91786146554619}]}
{"instance_id": "7df9ab9a4920", "generation_result": [{"code": "def reduce_metrics(logging_outputs) -> None\n\t\n    loss_sum = sum(log.get(\"loss\", 0) for log in logging_outputs)\n    sample_size = sum(log.get(\"sample_size\", 0) for log in logging_outputs)\n\n    metrics.log_scalar(\n        \"loss\", loss_sum / sample_size, sample_size, round=3\n    )  # This is the average loss from all the batches\n\n", "CodeBleu_score": 53.20220322545547}, {"code": "def reduce_metrics(logging_outputs) -> None\n\t\n    loss_sum = sum(log.get('loss', 0) for log in logging_outputs)\n    sample_size = sum(log.get('sample_size', 0) for log in logging_outputs)\n    metrics.log_scalar('loss', loss_sum / sample_size, sample_size, round=3)\n\n", "CodeBleu_score": 52.7910982560264}, {"code": "def reduce_metrics(logging_outputs) -> None\n\t\n    loss_sum = sum(log.get(\"loss\", 0) for log in logging_outputs)\n    sample_size = sum(log.get(\"sample_size\", 0) for log in logging_outputs)\n\n    metrics.log_scalar(\n        \"loss\", loss_sum / sample_size, sample_size, round=3\n    )  # , round=3 for 3 decimal places\n\n", "CodeBleu_score": 54.21672107432893}]}
{"instance_id": "7f9c326a96f7", "generation_result": [{"code": "def _mm_on_the_fly(\n        self,\n        cmasks,\n        vmasks,\n        attention_mask\n    )\n\t\n\n    if attention_mask is None:\n        attention_mask = self._mm_attention_mask(cmasks, vmasks)\n    token_type_ids = torch.cat([torch.zeros_like(cmasks), torch.ones_like(vmasks)], dim=1)\n    return attention_mask, token_type_ids\n\n", "CodeBleu_score": 33.276029921609336}, {"code": "def _mm_on_the_fly(\n        self,\n        cmasks,\n        vmasks,\n        attention_mask\n    )\n\t\n    # generate an attention mask if attention_mask is not provided.\n    if attention_mask is None:\n        attention_mask = self._mm_attention_mask(cmasks, vmasks)\n\n    # construct token type IDs by concatenating zeros for visual tokens and ones for text tokens.\n    token_type_ids = torch.cat([\n        torch.zeros_like(vmasks),\n        torch.ones_like(cmasks),\n    ], dim=1)\n\n    return attention_mask, token_type_ids\n\n", "CodeBleu_score": 34.67907732514585}, {"code": "def _mm_on_the_fly(\n        self,\n        cmasks,\n        vmasks,\n        attention_mask\n    )\n\t\n    if attention_mask is None:\n        attention_mask = self._mm_attention_mask(cmasks, vmasks)\n    token_type_ids = torch.cat([cmasks, vmasks], dim=1)\n    return attention_mask, token_type_ids\n\n", "CodeBleu_score": 28.779517304926504}]}
{"instance_id": "fa99d2a148a5", "generation_result": [{"code": "def prepare_inputs_for_generation(\n        self,\n        input_ids,\n        input_video_embeds,\n        attention_mask=None,\n        token_type_ids=None,\n        **model_kwargs\n    )\n\t\n    # Adjust sequence length\n    if attention_mask is not None and input_ids is not None:\n        # Find out how many video tokens are at the end\n        num_video_tokens = input_video_embeds.shape[1]\n        num_video_tokens_at_end = input_ids.shape[1] - num_video_tokens\n        # Adjust attention_mask\n        attention_mask = attention_mask[:, :-num_video_tokens_at_end]\n        # Adjust token_type_ids\n        if token_type_ids is not None:\n            token_type_ids = token_type_ids[:, :-num_video_tokens_at_end]\n    # Return\n    return {\n        'input_ids': input_ids,\n        'input_video_embeds': input_video_embeds,\n        'attention_mask': attention_mask,\n        'token_type_ids': token_type_ids,\n        **model_kwargs,\n    }\n\n", "CodeBleu_score": 42.62858476760161}, {"code": "def prepare_inputs_for_generation(\n        self,\n        input_ids,\n        input_video_embeds,\n        attention_mask=None,\n        token_type_ids=None,\n        **model_kwargs\n    )\n\t\n\n    # Check if the input_ids is a list\n    if isinstance(input_ids, list):\n        input_ids = torch.cat(input_ids, dim=0)\n\n    # Check if the input_video_embeds is a list\n    if isinstance(input_video_embeds, list):\n        input_video_embeds = torch.cat(input_video_embeds, dim=0)\n\n    # Check if the attention_mask is a list\n    if isinstance(attention_mask, list):\n        attention_mask = torch.cat(attention_mask, dim=0)\n\n    # Check if the token_type_ids is a list\n    if isinstance(token_type_ids, list):\n        token_type_ids = torch.cat(token_type_ids, dim=0)\n\n    # Check if the input_ids is a Tensor\n    if isinstance(input_ids, torch.Tensor):\n        input_ids = input_ids.to(self.device)\n\n    # Check if the input_video_embeds is a Tensor\n    if isinstance(input_video_embeds, torch.Tensor):\n        input_video_embeds = input_video_embeds.to(self.device)\n\n    # Check if the attention_mask is a Tensor\n    if isinstance(attention_mask, torch.Tensor):\n        attention_mask = attention_mask.to(self.device)\n\n    # Check if the token_type_ids is a Tensor\n    if isinstance(token_type_ids, torch.Tensor):\n        token_type_ids = token_type_ids.to(self.device)\n\n    # Get the combined sequence length\n    combined_sequence_length = input_ids.size(1) + input_video_embeds.size(1)\n\n    # Adjust the sequence length to match the combined length of input_ids and input_video_embeds\n    if input_ids.size(1) > input_video_embeds.size(1):\n        input_ids = input_ids[:, :input_video_embeds.size(1)]\n        attention_mask = attention_mask[:, :input_video_embeds.size(1)]\n        token_type_ids = token_type_ids[:, :input_video_embeds.size(1)]\n    else:\n        input_video_embeds = input_video_embeds[:, :input_ids.", "CodeBleu_score": 31.866470449475425}, {"code": "def prepare_inputs_for_generation(\n        self,\n        input_ids,\n        input_video_embeds,\n        attention_mask=None,\n        token_type_ids=None,\n        **model_kwargs\n    )\n\t\n\n    # get the combined sequence length\n    combined_length = input_ids.shape[-1] + input_video_embeds.shape[-2]\n\n    # get the attention mask\n    if attention_mask is None:\n        attention_mask = torch.ones(input_ids.shape, dtype=torch.long, device=input_ids.device)\n    else:\n        attention_mask = attention_mask.to(input_ids.device)\n\n    # get the token type ids\n    if token_type_ids is None:\n        token_type_ids = torch.zeros(input_ids.shape, dtype=torch.long, device=input_ids.device)\n    else:\n        token_type_ids = token_type_ids.to(input_ids.device)\n\n    # adjust the sequence length to match the combined length\n    if input_ids.shape[-1] > combined_length:\n        input_ids = input_ids[:, :combined_length]\n        attention_mask = attention_mask[:, :combined_length]\n        token_type_ids = token_type_ids[:, :combined_length]\n    else:\n        padding_length = combined_length - input_ids.shape[-1]\n        input_ids = torch.cat([input_ids, torch.zeros((input_ids.shape[0], padding_length), dtype=torch.long, device=input_ids.device)], dim=-1)\n        attention_mask = torch.cat([attention_mask, torch.ones((attention_mask.shape[0], padding_length), dtype=torch.long, device=attention_mask.device)], dim=-1)\n        token_type_ids = torch.cat([token_type_ids, torch.ones((token_type_ids.shape[0], padding_length), dtype=torch.long, device=token_type_ids.device)], dim=-1)\n\n    # adjust the video sequence length to match the combined length\n    if input_video_embeds.shape[-2] > combined_length:\n        input_video_embeds = input_video_embeds[:, :combined_length]\n    else:\n        padding_length = combined_length - input_video_embeds.shape[-2]\n        input_video_embeds = torch.cat([input_video_embeds, torch.zeros((input_video_embeds.shape", "CodeBleu_score": 35.26908726062881}]}
{"instance_id": "b1b0d25fe620", "generation_result": [{"code": "def _init_pipeline(self, cfg: ConfigType) -> Compose\n\t\n    if cfg is None:\n        cfg = []\n    assert isinstance(cfg, list), 'pipeline must be a list'\n    cfg = copy.deepcopy(cfg)\n\n    load_image_to_file_transform_idx = self._get_transform_idx(\n        cfg, ('LoadImageFromFile', mmcv.parallel.CollateFn))\n    assert load_image_to_file_transform_idx != -1, 'LoadImageFromFile is not in the pipeline'\n\n    cfg[load_image_to_file_transform_idx]['type'] = 'mmdet.InferencerLoader'\n    cfg[load_image_to_file_transform_idx]['img_id'] = 'img_id'\n\n    cfg = Compose(cfg)\n    return cfg\n\n", "CodeBleu_score": 35.63519800139642}, {"code": "def _init_pipeline(self, cfg: ConfigType) -> Compose\n\t\n    cfg = cfg.copy()\n    if 'img_id' in cfg.data.test.pipeline[0]['meta_keys']:\n        cfg.data.test.pipeline[0]['meta_keys'].remove('img_id')\n    load_transform_idx = self._get_transform_idx(cfg.data.test.pipeline, 'LoadImageFromFile')\n    if load_transform_idx == -1:\n        raise RuntimeError('Cannot find \"LoadImageFromFile\" transform in the test pipeline.')\n    cfg.data.test.pipeline[load_transform_idx] = {\n        'type': 'mmdet.InferencerLoader',\n        'transforms': cfg.data.test.pipeline[load_transform_idx]['transforms']\n    }\n    return Compose(cfg.data.test.pipeline)\n\n", "CodeBleu_score": 39.559668767760826}, {"code": "def _init_pipeline(self, cfg: ConfigType) -> Compose\n\t\n    pipeline_cfg = cfg.copy()\n    if 'meta_keys' in pipeline_cfg:\n        meta_keys = pipeline_cfg['meta_keys']\n        if 'img_id' in meta_keys:\n            meta_keys.remove('img_id')\n    else:\n        meta_keys = None\n    transform_idx = self._get_transform_idx(pipeline_cfg, 'LoadImageFromFile')\n    if transform_idx == -1:\n        raise RuntimeError('Cannot find \"LoadImageFromFile\" in the test pipeline.')\n    pipeline_cfg[transform_idx] = {\n        'type': 'mmdet.InferencerLoader',\n        'img_norm_cfg': pipeline_cfg[transform_idx]['img_norm_cfg'],\n        'to_float32': pipeline_cfg[transform_idx]['to_float32'],\n        'color_type': pipeline_cfg[transform_idx]['color_type'],\n        'meta_keys': meta_keys,\n    }\n    return Compose(pipeline_cfg)\n\n", "CodeBleu_score": 42.73413346632311}]}
{"instance_id": "7967962856eb", "generation_result": [{"code": "def _inputs_to_list(self, inputs: InputsType) -> list\n\t\n    if isinstance(inputs, str):\n        if os.path.isdir(inputs):\n            inputs = [os.path.join(inputs, file) for file in os.listdir(inputs)]\n        else:\n            raise TypeError(\"The input must be a string representing a directory or a list of image file paths.\")\n    if not isinstance(inputs, (list, tuple)):\n        inputs = [inputs]\n    return inputs\n\n", "CodeBleu_score": 44.587866176158}, {"code": "def _inputs_to_list(self, inputs: InputsType) -> list\n\t\n    if isinstance(inputs, str):\n        if os.path.isdir(inputs):\n            inputs = os.listdir(inputs)\n        else:\n            inputs = [inputs]\n    if not isinstance(inputs, (list, tuple)):\n        inputs = [inputs]\n    return inputs\n\n", "CodeBleu_score": 36.202097898148125}, {"code": "def _inputs_to_list(self, inputs: InputsType) -> list\n\t\n    if isinstance(inputs, str):\n        if os.path.isdir(inputs):\n            inputs = sorted(glob.glob(os.path.join(inputs, \"*\")))\n        else:\n            inputs = [inputs]\n    if not isinstance(inputs, (list, tuple)):\n        inputs = [inputs]\n    return inputs\n\n", "CodeBleu_score": 43.32653635658586}]}
{"instance_id": "74f727f06fa0", "generation_result": [{"code": "def _get_chunk_data(self, inputs: Iterable, chunk_size: int)\n\t\n    for i in range(0, len(inputs), chunk_size):\n        chunk = inputs[i : i + chunk_size]\n        chunk = [self.pipeline(d) for d in chunk]\n        yield chunk\n\n", "CodeBleu_score": 14.975996520799967}, {"code": "def _get_chunk_data(self, inputs: Iterable, chunk_size: int)\n\t\n    # Check if the input is a dictionary containing an 'img' or 'img_path' key and process it with the pipeline function\n    if isinstance(inputs, dict):\n        if 'img' in inputs:\n            yield self.pipeline(inputs['img'])\n        elif 'img_path' in inputs:\n            yield self.pipeline(Image.open(inputs['img_path']))\n    else:\n        # If the input is not a dictionary, it is assumed that it is an Image object and processed with the pipeline function\n        yield self.pipeline(inputs)\n\n    # If there are remaining inputs after the last chunk, yield the final chunk\n    if len(inputs) % chunk_size != 0:\n        yield self.pipeline(inputs[-(len(inputs) % chunk_size):])\n\n", "CodeBleu_score": 19.777805720584034}, {"code": "def _get_chunk_data(self, inputs: Iterable, chunk_size: int)\n\t\n    chunk = []\n    for input in inputs:\n        if isinstance(input, dict):\n            if 'img' in input:\n                chunk.append(self.pipeline(input['img']))\n            elif 'img_path' in input:\n                img = cv2.imread(input['img_path'])\n                chunk.append(self.pipeline(img))\n            else:\n                raise ValueError(\n                    'Input dictionary must contain an \\'img\\' or \\'img_path\\' key')\n        else:\n            chunk.append(self.pipeline(input))\n        if len(chunk) == chunk_size:\n            yield chunk\n            chunk = []\n    if len(chunk) > 0:\n        yield chunk\n\n", "CodeBleu_score": 37.84300041062246}]}
{"instance_id": "a43ab51afa59", "generation_result": [{"code": "def postprocess(\n        self,\n        preds: PredType,\n        visualization: Optional[List[np.ndarray]] = None,\n        return_datasamples: bool = False,\n        print_result: bool = False,\n        no_save_pred: bool = False,\n        pred_out_dir: str = '',\n        **kwargs,\n    ) -> Dict\n\t\n    if not no_save_pred and pred_out_dir == '':\n        warnings.warn(\n            'The prediction results are not saved because pred_out_dir is not specified.'\n        )\n\n    if return_datasamples and pred_out_dir != '':\n        warnings.warn(\n            'Saving datasamples is not supported, pred_out_dir will be ignored.'\n        )\n\n    if not return_datasamples:\n        if visualization is not None:\n            preds['visualization'] = visualization\n\n        if print_result:\n            print(json.dumps(preds, indent=2))\n\n        return preds\n\n    # Return datasamples\n    datasamples = []\n    for pred in preds:\n        data_sample = DetDataSample()\n        if 'pred_instances' in pred:\n            data_sample.pred_instances = Instances(\n                pred['pred_instances']['bboxes'],\n                pred['pred_instances']['labels'],\n                pred['pred_instances']['scores'],\n                pred['pred_instances'].get('masks', None),\n            )\n        if 'pred_panoptic_seg' in pred:\n            panoptic_seg = pred['pred_panoptic_seg']\n            if 'panoptic_seg_path' in pred:\n                panoptic_seg_path = pred['panoptic_seg_path']\n            else:\n                panoptic_seg_path = None\n            data_sample.pred_panoptic_seg = PanopticSegmentation(\n                panoptic_seg, panoptic_seg_path)\n        datasamples.append(data_sample)\n\n    if not no_save_pred:\n        mmengine.mkdir_or_exist(pred_out_dir)\n        for i, data_sample in enumerate(datasamples):\n            data_sample.pred2dict(\n                data_sample, pred_out_dir=pred_out_dir)\n\n    return datasamples\n\n", "CodeBleu_score": 41.98130931674054}, {"code": "def postprocess(\n        self,\n        preds: PredType,\n        visualization: Optional[List[np.ndarray]] = None,\n        return_datasamples: bool = False,\n        print_result: bool = False,\n        no_save_pred: bool = False,\n        pred_out_dir: str = '',\n        **kwargs,\n    ) -> Dict\n\t\n\n    if not no_save_pred and pred_out_dir != '':\n        mmcv.mkdir_or_exist(pred_out_dir)\n        mmcv.mkdir_or_exist(osp.join(pred_out_dir, 'preds'))\n\n    if not return_datasamples and pred_out_dir != '':\n        warnings.warn(\n            'Saving datasamples is not supported when return_datasamples is False. '\n            'Predictions will be saved instead.'\n        )\n\n    if return_datasamples:\n        result = []\n    else:\n        result = {}\n\n    if visualization is not None:\n        result['visualization'] = visualization\n\n    if isinstance(preds, list):\n        for pred in preds:\n            if return_datasamples:\n                result.append(self.pred2dict(pred, pred_out_dir))\n            else:\n                result.update(self.pred2dict(pred, pred_out_dir))\n    else:\n        if return_datasamples:\n            result.append(self.pred2dict(preds, pred_out_dir))\n        else:\n            result.update(self.pred2dict(preds, pred_out_dir))\n\n    if print_result:\n        print(result)\n\n    return result\n", "CodeBleu_score": 50.030830358524746}, {"code": "def postprocess(\n        self,\n        preds: PredType,\n        visualization: Optional[List[np.ndarray]] = None,\n        return_datasamples: bool = False,\n        print_result: bool = False,\n        no_save_pred: bool = False,\n        pred_out_dir: str = '',\n        **kwargs,\n    ) -> Dict\n\t\n    # Initialize output directory\n    if not no_save_pred and pred_out_dir != '':\n        mmcv.mkdir_or_exist(pred_out_dir)\n        mmcv.mkdir_or_exist(osp.join(pred_out_dir, 'preds'))\n        if visualization is not None:\n            mmcv.mkdir_or_exist(osp.join(pred_out_dir, 'visuals'))\n\n    # Initialize results\n    results = {}\n\n    # Convert predictions to dictionaries\n    if return_datasamples:\n        if pred_out_dir != '':\n            warnings.warn(\n                'Saving datasamples is not supported when return_datasamples is True.'\n            )\n        results['datasamples'] = []\n        for pred in preds:\n            results['datasamples'].append(self.pred2dict(pred, pred_out_dir))\n    else:\n        results['preds'] = []\n        for pred in preds:\n            results['preds'].append(self.pred2dict(pred, pred_out_dir))\n\n    # Add visualizations\n    if visualization is not None:\n        results['visuals'] = visualization\n\n    # Print results\n    if print_result:\n        print_log(self.format_results(results, **kwargs), logger=get_root_logger())\n\n    return results\n\n", "CodeBleu_score": 50.246044677942734}]}
{"instance_id": "a3c2561f199c", "generation_result": [{"code": "def build_test_pipeline(cfg: ConfigType) -> ConfigType\n\t\n    # copy first transform in dataset pipeline\n    first_transform = cfg['dataset']['pipeline'][0]\n    # modify first transform to include only Resize transform\n    first_transform['transforms'] = [\n        transform for transform in first_transform['transforms'] if transform['type'] == 'Resize'\n    ]\n    # copy last transform in dataset pipeline\n    last_transform = cfg['dataset']['pipeline'][-1]\n    # combine modified first and copied last transforms into a new test pipeline\n    cfg['test_pipeline'] = [first_transform, last_transform]\n    return cfg\n\n", "CodeBleu_score": 27.13065697395497}, {"code": "def build_test_pipeline(cfg: ConfigType) -> ConfigType\n\t\n    dataset_pipeline = cfg[\"dataset_pipeline\"]\n    first_transform = dataset_pipeline[0]\n    last_transform = dataset_pipeline[-1]\n    first_transform_copy = copy.deepcopy(first_transform)\n    first_transform_copy[\"transforms\"] = [first_transform_copy[\"transforms\"][0]]\n    test_pipeline = copy.deepcopy(last_transform)\n    test_pipeline[\"transforms\"] = [first_transform_copy, *test_pipeline[\"transforms\"]]\n    test_pipeline[\"transforms\"] = Compose(test_pipeline[\"transforms\"])\n    return test_pipeline\n\n", "CodeBleu_score": 24.610841766804935}, {"code": "def build_test_pipeline(cfg: ConfigType) -> ConfigType\n\t\n    cfg = copy.deepcopy(cfg)\n    cfg[\"dataset\"][\"transforms\"][0] = {\"Resize\": {\"height\": 224, \"width\": 224}}\n    cfg[\"dataset\"][\"transforms\"][-1] = {\"Resize\": {\"height\": 224, \"width\": 224}}\n    cfg[\"dataset\"][\"transforms\"] = Compose(cfg[\"dataset\"][\"transforms\"])\n    return cfg\n\n", "CodeBleu_score": 16.19236794303161}]}
{"instance_id": "48a358ab5a38", "generation_result": [{"code": "def inference_mot(model: nn.Module, img: np.ndarray, frame_id: int,\n                  video_len: int) -> SampleList\n\t\n    cfg = model.cfg\n    device = next(model.parameters()).device\n    data = {'img': img, 'frame_id': frame_id, 'original_shape': img.shape,\n            'img_id': np.array([0]), 'video_len': video_len}\n    data = test_pipeline(data)\n    data = collate([data], samples_per_gpu=1)\n    if next(model.parameters()).is_cuda:\n        # scatter to specified GPU\n        data = scatter(data, [device])[0]\n    else:\n        for m in model.modules():\n            assert not isinstance(\n                m, RoIPool\n            ), 'CPU inference with RoIPool is not supported currently.'\n    with torch.no_grad():\n        result = model(return_loss=False, rescale=True, **data)\n    return result\n\n", "CodeBleu_score": 54.38367103179631}, {"code": "def inference_mot(model: nn.Module, img: np.ndarray, frame_id: int,\n                  video_len: int) -> SampleList\n\t\n    cfg = model.cfg\n    device = next(model.parameters()).device\n    if device == torch.device('cpu'):\n        assert not cfg.get('bbox_head',\n                           None) or cfg.bbox_head.get('bbox_coder', None) is not None, \\\n            'CPU inference with RoIHead requires a bbox_coder in the config.'\n        assert not cfg.get('mask_head',\n                           None) or cfg.mask_head.get('mask_coder', None) is not None, \\\n            'CPU inference with MaskHead requires a mask_coder in the config.'\n\n    # build the data pipeline\n    test_pipeline = build_test_pipeline(cfg)\n    data = dict(img=img, frame_id=frame_id, video_len=video_len)\n    data = test_pipeline(data)\n    data = collate([data], samples_per_gpu=1)\n    if next(model.parameters()).is_cuda:\n        # scatter to specified GPU\n        data = scatter(data, [device])[0]\n    else:\n        for m in model.modules():\n            assert not isinstance(\n                m, RoIAlign), 'CPU inference with RoIAlign is not supported currently.'\n\n    # forward the model\n    with torch.no_grad():\n        result = model(return_loss=False, rescale=True, **data)\n    return result\n\n", "CodeBleu_score": 55.782827650383915}, {"code": "def inference_mot(model: nn.Module, img: np.ndarray, frame_id: int,\n                  video_len: int) -> SampleList\n\t\n    cfg = model.cfg\n    device = next(model.parameters()).device\n    if device == torch.device('cpu'):\n        cfg = cfg.copy()\n        cfg.model.backbone.pretrained = None\n        cfg.model.neck.in_channels = [\n            x.channels for x in cfg.model.backbone.out_channels\n        ]\n        cfg.model.neck.out_channels = cfg.model.neck.num_outs\n        cfg.model.bbox_head.in_channels = cfg.model.neck.out_channels\n    else:\n        # register hooks for feature extraction\n        model.backbone.register_forward_hook(get_activation('backbone'))\n\n    cfg.data.test.test_mode = True\n    cfg.data.test.ann_file = 'data/mot/annotations/half-train_cocoformat.json'\n    cfg.data.test.img_prefix = 'data/mot/train'\n    cfg.data.test.data_root = 'data/mot/'\n    cfg.data.test.pipeline = build_test_pipeline(cfg)\n    cfg.data.test.start_index = frame_id\n    cfg.data.test.end_index = frame_id + video_len\n    cfg.data.test.ann_file = 'data/mot/annotations/half-val_cocoformat.json'\n    cfg.data.test.img_prefix = 'data/mot/train'\n    cfg.data.test.data_root = 'data/mot/'\n    cfg.data.test.pipeline = build_test_pipeline(cfg)\n    cfg.data.test.start_index = frame_id\n    cfg.data.test.end_index = frame_id + video_len\n    data_loader = build_dataloader(\n        cfg.data.test,\n        imgs=None,\n        workers_per_gpu=1,\n        dist=False,\n        shuffle=False)\n\n    # build the model and load checkpoint\n    model = build_detector(cfg.model, test_cfg=cfg.get('test_cfg'))\n    load_checkpoint(model, args.checkpoint, map_location='cpu')\n    model.eval()\n\n    # prepare data\n    data = dict(img_info=dict(filename=img), img_prefix=None)\n    data = data_loader.dataset.prepare_train_img(", "CodeBleu_score": 27.7099546294594}]}
{"instance_id": "6c4ea2ce1787", "generation_result": [{"code": "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\n\t\n    while True:\n        total_length = len(tokens_a) + len(tokens_b)\n        if total_length <= max_num_tokens:\n            break\n\n        trunc_tokens = tokens_a if len(tokens_a) > len(tokens_b) else tokens_b\n        assert len(trunc_tokens) >= 1\n\n        # We want to sometimes truncate from the front and sometimes from the\n        # back to add more randomness and avoid biases.\n        if rng.random() < 0.5:\n            del trunc_tokens[0]\n        else:\n            trunc_tokens.pop()\n\n", "CodeBleu_score": 78.49275840258514}, {"code": "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\n\t\n    while True:\n        total_length = len(tokens_a) + len(tokens_b)\n        if total_length <= max_num_tokens:\n            break\n        if len(tokens_a) > len(tokens_b):\n            trunc_tokens = tokens_a\n            assert len(trunc_tokens) >= 1\n        else:\n            trunc_tokens = tokens_b\n            assert len(trunc_tokens) >= 1\n\n        # We want to sometimes truncate from the front and sometimes from the\n        # back to add more randomness and avoid biases.\n        if rng.random() < 0.5:\n            del trunc_tokens[0]\n        else:\n            trunc_tokens.pop()\n\n", "CodeBleu_score": 64.38276661675066}, {"code": "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\n\t\n    while True:\n        total_length = len(tokens_a) + len(tokens_b)\n        if total_length <= max_num_tokens:\n            break\n\n        trunc_tokens = tokens_a if len(tokens_a) > len(tokens_b) else tokens_b\n        assert len(trunc_tokens) >= 1\n\n        # We want to sometimes truncate from the front and sometimes from the\n        # back to add more randomness and avoid biases.\n        if rng.random() < 0.5:\n            del trunc_tokens[0]\n        else:\n            trunc_tokens.pop()\n\n", "CodeBleu_score": 78.49275840258514}]}
{"instance_id": "007181b6729d", "generation_result": [{"code": "def get_activation(activation_string)\n\t\n    if not isinstance(activation_string, str):\n        return activation_string\n    if not activation_string:\n        return None\n    act = activation_string.lower()\n    if act == \"linear\":\n        return linear\n    if act == \"relu\":\n        return relu\n    if act == \"elu\":\n        return elu\n    if act == \"leaky_relu\":\n        return leaky_relu\n    if act == \"selu\":\n        return selu\n    if act == \"softplus\":\n        return softplus\n    if act == \"softsign\":\n        return softsign\n    if act == \"tanh\":\n        return tanh\n    if act == \"sigmoid\":\n        return sigmoid\n    if act == \"hard_sigmoid\":\n        return hard_sigmoid\n    if act == \"exponential\":\n        return exponential\n    if act == \"softmax\":\n        return softmax\n    if act == \"softplus_softmax\":\n        return softplus_softmax\n    if act == \"swish\":\n        return swish\n    if act == \"mish\":\n        return mish\n    if act == \"gelu\":\n        return gelu\n    if act == \"hard_swish\":\n        return hard_swish\n    if act == \"hard_mish\":\n        return hard_mish\n    if act == \"log_sigmoid\":\n        return log_sigmoid\n    if act == \"log_softmax\":\n        return log_softmax\n    if act == \"softmax2d\":\n        return softmax2d\n    if act == \"log_softmax2d\":\n        return log_softmax2d\n    if act == \"logit\":\n        return logit\n    if act == \"sin\":\n        return sin\n    if act == \"cos\":\n        return cos\n    if act == \"tan\":\n        return tan\n    if act == \"sinh\":\n        return sinh\n    if act == \"cosh\":\n        return cosh\n    if act == \"tanh\":\n        return tanh\n    if act == \"asinh\":\n        return asinh\n    if act == \"acosh\":\n        return acosh\n    if act == \"atanh\":\n        return atanh\n    if act == \"relu6\":\n        return relu6\n    if act == \"hard_tanh\":\n        return hard_tanh\n    if act == \"hard_swish\":\n        return hard_swish\n    if act == \"hard_mish\":\n        return hard_mish\n    if act == \"log_sigmoid\":\n        return", "CodeBleu_score": 56.04642182024655}, {"code": "def get_activation(activation_string)\n\t\n    if not isinstance(activation_string, str):\n        return activation_string\n    if not activation_string:\n        return None\n    if activation_string == \"relu\":\n        return tf.nn.relu\n    if activation_string == \"elu\":\n        return tf.nn.elu\n    if activation_string == \"selu\":\n        return tf.nn.selu\n    if activation_string == \"sigmoid\":\n        return tf.nn.sigmoid\n    if activation_string == \"tanh\":\n        return tf.nn.tanh\n    if activation_string == \"softmax\":\n        return tf.nn.softmax\n    if activation_string == \"leaky_relu\":\n        return tf.nn.leaky_relu\n    if activation_string == \"softplus\":\n        return tf.nn.softplus\n    raise ValueError(\"Unsupported activation function: %s\" % activation_string)\n\n", "CodeBleu_score": 44.44116018652836}, {"code": "def get_activation(activation_string)\n\t\n    if not isinstance(activation_string, str):\n        return activation_string\n    if not activation_string:\n        return None\n    act = activation_string.lower()\n    if act == \"linear\":\n        return linear\n    elif act == \"relu\":\n        return relu\n    elif act == \"elu\":\n        return elu\n    elif act == \"selu\":\n        return selu\n    elif act == \"sigmoid\":\n        return sigmoid\n    elif act == \"tanh\":\n        return tanh\n    elif act == \"softmax\":\n        return softmax\n    elif act == \"softplus\":\n        return softplus\n    elif act == \"softsign\":\n        return softsign\n    elif act == \"hard_sigmoid\":\n        return hard_sigmoid\n    elif act == \"exponential\":\n        return exponential\n    elif act == \"linear_plus_abs\":\n        return linear_plus_abs\n    elif act == \"linear_plus_abs_squared\":\n        return linear_plus_abs_squared\n    elif act == \"linear_plus_abs_cubic\":\n        return linear_plus_abs_cubic\n    elif act == \"linear_plus_abs_quartic\":\n        return linear_plus_abs_quartic\n    elif act == \"linear_plus_abs_quintic\":\n        return linear_plus_abs_quintic\n    elif act == \"linear_plus_abs_sextic\":\n        return linear_plus_abs_sextic\n    elif act == \"linear_plus_abs_septic\":\n        return linear_plus_abs_septic\n    elif act == \"linear_plus_abs_octic\":\n        return linear_plus_abs_octic\n    elif act == \"linear_plus_abs_nonic\":\n        return linear_plus_abs_nonic\n    elif act == \"linear_plus_abs_decic\":\n        return linear_plus_abs_decic\n    elif act == \"linear_plus_abs_undecic\":\n        return linear_plus_abs_undecic\n    elif act == \"linear_plus_abs_duodecic\":\n        return linear_plus_abs_duodecic\n    elif act == \"linear_plus_abs_tredecic\":\n        return linear_plus_abs_tredecic\n    elif act == \"linear_plus_abs_quattuordecic\":\n        return linear_plus_abs_quattuordecic\n    elif act", "CodeBleu_score": 53.707636780307844}]}
{"instance_id": "d5d6e0315899", "generation_result": [{"code": "def dropout(input_tensor, dropout_prob)\n\t\n    if dropout_prob is None or dropout_prob == 0.0:\n        return input_tensor\n    else:\n        return tf.nn.dropout(input_tensor, dropout_prob)\n", "CodeBleu_score": 53.31243274886709}, {"code": "def dropout(input_tensor, dropout_prob)\n\t\n    return tf.nn.dropout(input_tensor, 1.0 - dropout_prob)\n\n", "CodeBleu_score": 17.092029675696523}, {"code": "def dropout(input_tensor, dropout_prob)\n\t\n    if dropout_prob is None or dropout_prob == 0.0:\n        return input_tensor\n    return tf.nn.dropout(input_tensor, 1.0 - dropout_prob)\n\n", "CodeBleu_score": 60.78403505639335}]}
{"instance_id": "828917b9ac53", "generation_result": [{"code": "def create_attention_mask_from_input_mask(from_tensor, to_mask)\n\t\n    from_shape = get_shape_list(from_tensor, expected_rank=[2, 3])\n    batch_size = from_shape[0]\n    from_seq_length = from_shape[1]\n\n    to_shape = get_shape_list(to_mask, expected_rank=2)\n    to_seq_length = to_shape[1]\n\n    to_mask = tf.cast(\n        tf.reshape(to_mask, [batch_size, 1, to_seq_length]), tf.float32)\n\n    # We don't assume that `from_tensor` is a mask (although it could be). We\n    # don't actually care if we attend *from* padding tokens (only *to* padding)\n    # tokens so we create a tensor of all ones.\n    #\n    # `broadcast_ones` = [batch_size, from_seq_length, 1]\n    broadcast_ones = tf.ones(\n        shape=[batch_size, from_seq_length, 1], dtype=tf.float32)\n\n    # Here we broadcast along two dimensions to create the mask.\n    mask = broadcast_ones * to_mask\n\n    return mask", "CodeBleu_score": 64.38636645230761}, {"code": "def create_attention_mask_from_input_mask(from_tensor, to_mask)\n\t\n    from_shape = get_shape_list(from_tensor, expected_rank=[2, 3])\n    batch_size = from_shape[0]\n    from_seq_length = from_shape[1]\n\n    to_shape = get_shape_list(to_mask, expected_rank=2)\n    to_seq_length = to_shape[1]\n\n    to_mask = tf.cast(\n        tf.reshape(to_mask, [batch_size, 1, to_seq_length]), tf.float32)\n\n    # We don't assume that `from_tensor` is a mask (although it could be). We\n    # don't actually care if we attend *from* padding tokens (only *to* padding)\n    # tokens so we create a tensor of all ones.\n    #\n    # `broadcast_ones` = [batch_size, from_seq_length, 1]\n    broadcast_ones = tf.ones(\n        shape=[batch_size, from_seq_length, 1], dtype=tf.float32)\n\n    # Here we broadcast along two dimensions to create the mask.\n    mask = broadcast_ones * to_mask\n\n    return mask\n\n", "CodeBleu_score": 64.38636645230761}, {"code": "def create_attention_mask_from_input_mask(from_tensor, to_mask)\n\t\n    from_shape = get_shape_list(from_tensor, expected_rank=[2, 3])\n    batch_size = from_shape[0]\n    from_seq_length = from_shape[1]\n\n    to_shape = get_shape_list(to_mask, expected_rank=2)\n    to_seq_length = to_shape[1]\n\n    to_mask = tf.cast(\n        tf.reshape(to_mask, [batch_size, 1, to_seq_length]), tf.float32)\n\n    # We don't assume that `from_tensor` is a mask (although it could be). We\n    # don't actually care if we attend *from* padding tokens (only *to* padding)\n    # tokens so we create a tensor of all ones.\n    #\n    # `broadcast_ones` = [batch_size, from_seq_length, 1]\n    broadcast_ones = tf.ones(\n        shape=[batch_size, from_seq_length, 1], dtype=tf.float32)\n\n    # Here we broadcast along two dimensions to create the mask.\n    mask = broadcast_ones * to_mask\n\n    return mask\n\n", "CodeBleu_score": 64.38636645230761}]}
{"instance_id": "d7798e41a723", "generation_result": [{"code": "def transpose_for_scores(input_tensor, batch_size, num_attention_heads,\n                           seq_length, width)\n\t\n    output_tensor = tf.reshape(input_tensor, [batch_size, seq_length, num_attention_heads, width])\n    output_tensor = tf.transpose(output_tensor, [0, 2, 1, 3])\n    return output_tensor\n\n", "CodeBleu_score": 54.22065594588168}, {"code": "def transpose_for_scores(input_tensor, batch_size, num_attention_heads,\n                           seq_length, width)\n\t\n    output_tensor = tf.reshape(input_tensor, [batch_size, seq_length, num_attention_heads, width])\n    output_tensor = tf.transpose(output_tensor, [0, 2, 1, 3])\n    return output_tensor\n\n", "CodeBleu_score": 54.22065594588168}, {"code": "def transpose_for_scores(input_tensor, batch_size, num_attention_heads,\n                           seq_length, width)\n\t\n\n    output_tensor = tf.reshape(input_tensor, [batch_size, seq_length, num_attention_heads, width])\n\n    output_tensor = tf.transpose(output_tensor, [0, 2, 1, 3])\n\n    return output_tensor\n\n", "CodeBleu_score": 54.22065594588168}]}
{"instance_id": "711d466ace09", "generation_result": [{"code": "def reshape_to_matrix(input_tensor)\n\t\n    ndims = input_tensor.shape.ndims\n    if ndims < 2:\n        raise ValueError(\"Input tensor must have at least rank 2. Shape = %s\" %\n                         (input_tensor.shape))\n    if ndims == 2:\n        return input_tensor\n\n    width = input_tensor.shape[-1]\n    output_tensor = tf.reshape(input_tensor, [-1, width])\n    return output_tensor\n\n", "CodeBleu_score": 83.6520561341743}, {"code": "def reshape_to_matrix(input_tensor)\n\t\n    ndims = input_tensor.shape.ndims\n    if ndims < 2:\n        raise ValueError(\"Input tensor must have at least rank 2. Shape = %s\" %\n                         (input_tensor.shape))\n    if ndims == 2:\n        return input_tensor\n\n    width = input_tensor.shape[-1]\n    output_tensor = tf.reshape(input_tensor, [-1, width])\n    return output_tensor\n\n", "CodeBleu_score": 83.6520561341743}, {"code": "def reshape_to_matrix(input_tensor)\n\t\n    ndims = input_tensor.shape.ndims\n    if ndims < 2:\n        raise ValueError(\"Input tensor must have at least rank 2. Shape = %s\" % (input_tensor.shape))\n    if ndims == 2:\n        return input_tensor\n\n    width = input_tensor.shape[-1]\n    output_tensor = tf.reshape(input_tensor, [-1, width])\n    return output_tensor\n\n", "CodeBleu_score": 84.88208944563895}]}
{"instance_id": "7adcd08f531e", "generation_result": [{"code": "def get_train_examples(self, data_dir)\n\t\n    train_file = os.path.join(data_dir, \"train.tsv\")\n    return self._create_examples(self._read_tsv(train_file), \"train\")\n", "CodeBleu_score": 6.209409432138726}, {"code": "def get_train_examples(self, data_dir)\n\t\n    train_file = os.path.join(data_dir, \"train.tsv\")\n    with tf.gfile.Open(train_file, \"r\") as f:\n        reader = csv.reader(f, delimiter=\"\\t\", quotechar=None)\n        examples = []\n        for line in reader:\n            if len(line) != 2:\n                continue\n            label = convert_to_unicode(line[1])\n            if label == \"contradictory\":\n                label = \"contradiction\"\n            text_a = convert_to_unicode(line[0])\n            examples.append(\n                InputExample(guid=None, text_a=text_a, text_b=None, label=label))\n        return examples\n", "CodeBleu_score": 36.39277015376863}, {"code": "def get_train_examples(self, data_dir)\n\t\n    train_path = os.path.join(data_dir, \"train.tsv\")\n    if not os.path.exists(train_path):\n        raise ValueError(\"Training file does not exist at %s\" % (train_path))\n    with tf.gfile.Open(train_path) as f:\n        reader = csv.reader(f, delimiter=\"\\t\")\n        examples = []\n        for i, line in enumerate(reader):\n            if i == 0:\n                continue\n            guid = \"%s-%s\" % (\"train\", i)\n            text_a = tokenization.convert_to_unicode(line[3])\n            text_b = tokenization.convert_to_unicode(line[4])\n            label = tokenization.convert_to_unicode(line[5])\n            if label == \"contradictory\":\n                label = \"contradiction\"\n            examples.append(InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n        return examples\n", "CodeBleu_score": 52.137654672458986}]}
{"instance_id": "26a0bb96dab9", "generation_result": [{"code": "def metric_fn(per_example_loss, label_ids, logits, is_real_example)\n\t\n    predictions = tf.argmax(logits, axis=-1, output_type=tf.int32)\n    accuracy = tf.metrics.accuracy(\n        labels=label_ids, predictions=predictions, weights=is_real_example)\n    loss = tf.metrics.mean(\n        values=per_example_loss, weights=is_real_example)\n    return {\n        \"eval_accuracy\": accuracy,\n        \"eval_loss\": loss,\n    }\n\n", "CodeBleu_score": 61.73755888184837}, {"code": "def metric_fn(per_example_loss, label_ids, logits, is_real_example)\n\t\n    logits = tf.reshape(logits, (-1, logits.shape[-1]))\n    label_ids = tf.reshape(label_ids, (-1,))\n    is_real_example = tf.reshape(is_real_example, (-1,))\n\n    # For real examples, calculate the classification accuracy and the mean loss by taking the mean of the loss of the predicted class.\n    # For the other examples, the loss is zero.\n    per_example_loss = tf.reshape(per_example_loss, (-1,))\n    loss = tf.reduce_mean(\n        input_tensor=per_example_loss * is_real_example)\n    logits = tf.cast(logits, tf.float32)\n    predictions = tf.argmax(input=logits, axis=-1, output_type=tf.int32)\n    accuracy = tf.reduce_mean(\n        input_tensor=tf.cast(tf.equal(predictions, label_ids), tf.float32) * is_real_example)\n\n    # Calculate the average loss over all examples.\n    total_loss = tf.reduce_sum(input_tensor=per_example_loss)\n\n    # Calculate the total accuracy.\n    total_accuracy = tf.reduce_sum(input_tensor=tf.cast(tf.equal(predictions, label_ids), tf.float32))\n\n    return {\n        \"eval_accuracy\": total_accuracy / tf.cast(tf.reduce_sum(input_tensor=is_real_example), tf.float32),\n        \"eval_loss\": total_loss / tf.cast(tf.reduce_sum(input_tensor=is_real_example), tf.float32)\n    }\n\n", "CodeBleu_score": 38.32199785950315}, {"code": "def metric_fn(per_example_loss, label_ids, logits, is_real_example)\n\t\n    predictions = tf.argmax(logits, axis=-1, output_type=tf.int32)\n    accuracy = tf.metrics.accuracy(\n        labels=label_ids, predictions=predictions, weights=is_real_example)\n    loss = tf.metrics.mean(values=per_example_loss, weights=is_real_example)\n    return {\n        \"eval_accuracy\": accuracy,\n        \"eval_loss\": loss,\n    }\n\n", "CodeBleu_score": 65.47238279114057}]}
{"instance_id": "6154e4d28509", "generation_result": [{"code": "def convert_examples_to_features(examples, label_list, max_seq_length,\n                                 tokenizer)\n\t\n\n    features = []\n    for (ex_index, example) in enumerate(examples):\n        if ex_index % 10000 == 0:\n            tf.logging.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n\n        feature = convert_single_example(ex_index, example, label_list,\n                                         max_seq_length, tokenizer)\n\n        features.append(feature)\n    return features\n\n", "CodeBleu_score": 82.83594654369867}, {"code": "def convert_examples_to_features(examples, label_list, max_seq_length,\n                                 tokenizer)\n\t\n    features = []\n    for (ex_index, example) in enumerate(examples):\n        if ex_index % 10000 == 0:\n            tf.logging.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n\n        feature = convert_single_example(ex_index, example, label_list,\n                                         max_seq_length, tokenizer)\n\n        features.append(feature)\n    return features\n\n", "CodeBleu_score": 82.83594654369867}, {"code": "def convert_examples_to_features(examples, label_list, max_seq_length,\n                                 tokenizer)\n\t\n    features = []\n    for (ex_index, example) in enumerate(examples):\n        if ex_index % 10000 == 0:\n            tf.logging.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n\n        feature = convert_single_example(ex_index, example, label_list,\n                                         max_seq_length, tokenizer)\n\n        features.append(feature)\n    return features\n\n", "CodeBleu_score": 82.83594654369867}]}
{"instance_id": "e09c0db71ebb", "generation_result": [{"code": "def get_next_sentence_output(bert_config, input_tensor, labels)\n\t\n    # Simple binary classification. Note that 0 is \"next sentence\" and 1 is \"random sentence\". This weight matrix is not used after pre-training.\n    with tf.variable_scope(\"cls/seq_relationship\"):\n        output_weights = tf.get_variable(\n            \"output_weights\",\n            shape=[2, bert_config.hidden_size],\n            initializer=create_initializer(bert_config.initializer_range))\n        output_bias = tf.get_variable(\n            \"output_bias\", shape=[2], initializer=tf.zeros_initializer())\n\n    logits = tf.matmul(input_tensor, output_weights, transpose_b=True)\n    logits = tf.nn.bias_add(logits, output_bias)\n    log_probs = tf.nn.log_softmax(logits, axis=-1)\n\n    labels = tf.reshape(labels, [-1])\n    one_hot_labels = tf.one_hot(labels, depth=2, dtype=tf.float32)\n\n    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n    loss = tf.reduce_mean(per_example_loss)\n    return (loss, per_example_loss, log_probs)\n\n", "CodeBleu_score": 84.68685365041817}, {"code": "def get_next_sentence_output(bert_config, input_tensor, labels)\n\t\n    input_shape = modeling.get_shape_list(input_tensor, expected_rank=3)\n    batch_size = input_shape[0]\n    seq_length = input_shape[1]\n    width = input_shape[2]\n\n    output_weights = tf.get_variable(\n        \"cls/squad/output_weights\", [2, width],\n        initializer=modeling.create_initializer(bert_config.initializer_range))\n    output_bias = tf.get_variable(\n        \"cls/squad/output_bias\", [2], initializer=tf.zeros_initializer())\n\n    logits = tf.matmul(input_tensor, output_weights, transpose_b=True)\n    logits = tf.nn.bias_add(logits, output_bias)\n    logits = tf.reshape(logits, [batch_size, seq_length, 2])\n    logits = tf.transpose(logits, [2, 0, 1])\n\n    unstacked_logits = tf.unstack(logits, axis=0)\n\n    (start_logits, end_logits) = (unstacked_logits[0], unstacked_logits[1])\n\n    def compute_loss(logits, labels):\n        \"\"\"The function computes the per-example loss for the start and end positions.", "CodeBleu_score": 25.749937628767185}, {"code": "def get_next_sentence_output(bert_config, input_tensor, labels)\n\t\n    # input_tensor: [batch_size, seq_length, hidden_size]\n    # labels: [batch_size, seq_length]\n    input_shape = get_shape_list(input_tensor, expected_rank=3)\n    batch_size = input_shape[0]\n    seq_length = input_shape[1]\n    hidden_size = input_shape[2]\n\n    output_weights = tf.get_variable(\n        \"cls/squad/output_weights\", [2, hidden_size],\n        initializer=create_initializer(bert_config.initializer_range))\n    output_bias = tf.get_variable(\n        \"cls/squad/output_bias\", [2], initializer=tf.zeros_initializer())\n\n    logits = tf.matmul(tf.reshape(input_tensor, [-1, hidden_size]), output_weights, transpose_b=True)\n    logits = tf.nn.bias_add(logits, output_bias)\n    logits = tf.reshape(logits, [batch_size, seq_length, 2])\n    logits = tf.transpose(logits, [2, 0, 1])\n\n    logits = tf.reshape(logits, [-1, logits.shape[-1]])\n    logits = tf.nn.log_softmax(logits, axis=-1)\n    logits = tf.reshape(logits, [2, batch_size, seq_length])\n    logits = tf.transpose(logits, [1, 2, 0])\n\n    logits_masked = tf.boolean_mask(logits, tf.cast(tf.reshape(labels, [-1]), tf.bool))\n    labels_masked = tf.boolean_mask(tf.reshape(labels, [-1]), tf.cast(tf.reshape(labels, [-1]), tf.bool))\n    per_example_loss = -tf.reduce_sum(tf.cast(labels_masked, tf.float32) * logits_masked, axis=-1)\n    loss = tf.reduce_mean(per_example_loss)\n\n    return (loss, per_example_loss, logits)\n\n", "CodeBleu_score": 33.40655947624443}]}
{"instance_id": "1f66b020f485", "generation_result": [{"code": "def gather_indexes(sequence_tensor, positions)\n\t\n    sequence_shape = modeling.get_shape_list(sequence_tensor, expected_rank=3)\n    batch_size = sequence_shape[0]\n    seq_length = sequence_shape[1]\n    width = sequence_shape[2]\n\n    flat_offsets = tf.reshape(tf.range(0, batch_size, dtype=tf.int32) * seq_length, [-1, 1])\n    flat_positions = tf.reshape(positions + flat_offsets, [-1])\n    flat_sequence_tensor = tf.reshape(sequence_tensor, [batch_size * seq_length, width])\n    output_tensor = tf.gather(flat_sequence_tensor, flat_positions)\n    return output_tensor\n\n", "CodeBleu_score": 55.18468812590758}, {"code": "def gather_indexes(sequence_tensor, positions)\n\t\n\n    # Get the shape of the sequence tensor\n    sequence_length = get_shape_list(sequence_tensor, expected_rank=3)[1]\n    batch_size = get_shape_list(sequence_tensor, expected_rank=3)[0]\n\n    # Calculate the offsets\n    offsets = tf.range(0, batch_size) * sequence_length\n    flat_offsets = tf.reshape(offsets, [-1, 1])\n    flat_positions = tf.reshape(positions + flat_offsets, [-1])\n    flat_sequence_tensor = tf.reshape(sequence_tensor,\n                                      [batch_size * sequence_length, -1])\n    output_tensor = tf.gather(flat_sequence_tensor, flat_positions)\n\n    return output_tensor\n\n", "CodeBleu_score": 38.19771187668416}, {"code": "def gather_indexes(sequence_tensor, positions)\n\t\n    sequence_shape = modeling.get_shape_list(sequence_tensor, expected_rank=3)\n    batch_size = sequence_shape[0]\n    seq_length = sequence_shape[1]\n    width = sequence_shape[2]\n\n    flat_offsets = tf.reshape(tf.range(0, batch_size, dtype=tf.int32) * seq_length, [-1, 1])\n    flat_positions = tf.reshape(positions + flat_offsets, [-1])\n    flat_sequence_tensor = tf.reshape(sequence_tensor,\n                                      [batch_size * seq_length, width])\n    output_tensor = tf.gather(flat_sequence_tensor, flat_positions)\n    return output_tensor\n\n", "CodeBleu_score": 55.954854694995305}]}
{"instance_id": "956bffee9a33", "generation_result": [{"code": "def _decode_record(record, name_to_features)\n\t\n    example = tf.io.parse_single_example(record, name_to_features)\n\n    # tf.Example only supports tf.int64, but the TPU only supports tf.int32.\n    # So cast all int64 to int32.\n    for name in list(example.keys()):\n        t = example[name]\n        if t.dtype == tf.int64:\n            t = tf.cast(t, tf.int32)\n        example[name] = t\n\n    return example\n\n", "CodeBleu_score": 59.0051892348668}, {"code": "def _decode_record(record, name_to_features)\n\t\n    example = tf.io.parse_single_example(record, name_to_features)\n    for name in list(example.keys()):\n        t = example[name]\n        if t.dtype == tf.int64:\n            t = tf.cast(t, tf.int32)\n        example[name] = t\n    return example\n\n", "CodeBleu_score": 59.0051892348668}, {"code": "def _decode_record(record, name_to_features)\n\t\n    example = tf.io.parse_single_example(record, name_to_features)\n\n    # tf.Example only supports tf.int64, but the TPU only supports tf.int32.\n    # So cast all int64 to int32.\n    for name in list(example.keys()):\n        t = example[name]\n        if t.dtype == tf.int64:\n            t = tf.cast(t, tf.int32)\n        example[name] = t\n\n    return example\n\n", "CodeBleu_score": 59.0051892348668}]}
{"instance_id": "6b97f354e641", "generation_result": [{"code": "def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer,\n                         orig_answer_text)\n\t\n    tok_answer_text = \" \".join(tokenizer.tokenize(orig_answer_text))\n\n    for new_start in range(input_start, input_end + 1):\n        for new_end in range(input_end, new_start - 1, -1):\n            text_span = \" \".join(doc_tokens[new_start:(new_end + 1)])\n            if text_span == tok_answer_text:\n                return (new_start, new_end)\n\n    return (input_start, input_end)\n\n", "CodeBleu_score": 78.30625722805418}, {"code": "def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer,\n                         orig_answer_text)\n\t\n    tok_answer_text = \" \".join(tokenizer.tokenize(orig_answer_text))\n\n    for new_start in range(input_start, input_end + 1):\n        for new_end in range(input_end, new_start - 1, -1):\n            text_span = \" \".join(doc_tokens[new_start:(new_end + 1)])\n            if text_span == tok_answer_text:\n                return (new_start, new_end)\n\n    return (input_start, input_end)\n\n", "CodeBleu_score": 78.30625722805418}, {"code": "def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer,\n                         orig_answer_text)\n\t\n    tok_answer_text = \" \".join(tokenizer.tokenize(orig_answer_text))\n\n    for new_start in range(input_start, input_end + 1):\n        for new_end in range(input_end, new_start - 1, -1):\n            text_span = \" \".join(doc_tokens[new_start:(new_end + 1)])\n            if text_span == tok_answer_text:\n                return (new_start, new_end)\n\n    return (input_start, input_end)\n\n", "CodeBleu_score": 78.30625722805418}]}
{"instance_id": "aba334b92b34", "generation_result": [{"code": "def _check_is_max_context(doc_spans, cur_span_index, position)\n\t\n    best_score = None\n    best_span_index = None\n    for (span_index, doc_span) in enumerate(doc_spans):\n        end = doc_span.start + doc_span.length - 1\n        if position < doc_span.start:\n            continue\n        if position > end:\n            continue\n        num_left_context = position - doc_span.start\n        num_right_context = end - position\n        score = min(num_left_context, num_right_context) + 0.01 * doc_span.length\n        if best_score is None or score > best_score:\n            best_score = score\n            best_span_index = span_index\n\n    return cur_span_index == best_span_index\n\n", "CodeBleu_score": 77.80029854234776}, {"code": "def _check_is_max_context(doc_spans, cur_span_index, position)\n\t\n    best_score = None\n    best_span_index = None\n    for (span_index, doc_span) in enumerate(doc_spans):\n        end = doc_span.start + doc_span.length - 1\n        if position < doc_span.start:\n            continue\n        if position > end:\n            continue\n        num_left_context = position - doc_span.start\n        num_right_context = end - position\n        score = min(num_left_context, num_right_context) + 0.01 * doc_span.length\n        if best_score is None or score > best_score:\n            best_score = score\n            best_span_index = span_index\n\n    return cur_span_index == best_span_index\n\n", "CodeBleu_score": 77.80029854234776}, {"code": "def _check_is_max_context(doc_spans, cur_span_index, position)\n\t\n    best_score = None\n    best_span_index = None\n    for (span_index, doc_span) in enumerate(doc_spans):\n        end = doc_span.start + doc_span.length - 1\n        if position < doc_span.start:\n            continue\n        if position > end:\n            continue\n        num_left_context = position - doc_span.start\n        num_right_context = end - position\n        score = min(num_left_context, num_right_context) + 0.01 * doc_span.length\n        if best_score is None or score > best_score:\n            best_score = score\n            best_span_index = span_index\n\n    return cur_span_index == best_span_index\n\n", "CodeBleu_score": 77.80029854234776}]}
{"instance_id": "cf92ad509f6a", "generation_result": [{"code": "def load_vocab(vocab_file)\n\t\n    vocab = collections.OrderedDict()\n    index = 0\n    with open(vocab_file, \"r\", encoding=\"utf-8\") as reader:\n        while True:\n            token = convert_to_unicode(reader.readline())\n            if not token:\n                break\n            token = token.strip()\n            vocab[token] = index\n            index += 1\n    return vocab\n\n", "CodeBleu_score": 66.58135914758769}, {"code": "def load_vocab(vocab_file)\n\t\n    vocab = collections.OrderedDict()\n    index = 0\n    with tf.gfile.GFile(vocab_file, \"r\") as reader:\n        while True:\n            token = convert_to_unicode(reader.readline())\n            if not token:\n                break\n            token = token.strip()\n            vocab[token] = index\n            index += 1\n    return vocab\n\n", "CodeBleu_score": 74.89402347184635}, {"code": "def load_vocab(vocab_file)\n\t\n    vocab = collections.OrderedDict()\n    index = 0\n    with tf.gfile.GFile(vocab_file, \"r\") as reader:\n        while True:\n            token = convert_to_unicode(reader.readline())\n            if not token:\n                break\n            token = token.strip()\n            vocab[token] = index\n            index += 1\n    return vocab\n\n", "CodeBleu_score": 74.89402347184635}]}
{"instance_id": "6c865803ac12", "generation_result": [{"code": "def convert_by_vocab(vocab, items)\n\t\n    output = []\n    for item in items:\n        output.append(vocab[item])\n    return output\n\n", "CodeBleu_score": 79.63749907165396}, {"code": "def convert_by_vocab(vocab, items)\n\t\n    output = []\n    for item in items:\n        output.append(vocab[item])\n    return output\n\n", "CodeBleu_score": 79.63749907165396}, {"code": "def convert_by_vocab(vocab, items)\n\t\n    output = []\n    for item in items:\n        output.append(vocab.get(item, UNK_ID))\n    return output\n\n", "CodeBleu_score": 59.42791785154598}]}
{"instance_id": "e93b9200dc4a", "generation_result": [{"code": "def tokenize(self, text)\n\t\n    text = convert_to_unicode(text)\n    text = self.basic_tokenizer.tokenize(text)\n    text = self.wordpiece_tokenizer.tokenize(text)\n    return text\n\n", "CodeBleu_score": 19.7783917281408}, {"code": "def tokenize(self, text)\n\t\n    # first, tokenize text using basic tokenizer\n    tokens = self.basic_tokenizer.tokenize(text)\n    # then, tokenize each token using wordpiece tokenizer\n    output_tokens = []\n    for token in tokens:\n        output_tokens.extend(self.wordpiece_tokenizer.tokenize(token))\n    return output_tokens\n\n", "CodeBleu_score": 41.31203424290428}, {"code": "def tokenize(self, text)\n\t\n    text = convert_to_unicode(text)\n    text = self.basic_tokenizer.tokenize(text)\n    text = [self.wordpiece_tokenizer.tokenize(token) for token in text]\n    text = [item for sublist in text for item in sublist]\n    return text\n\n", "CodeBleu_score": 34.970468204217354}]}
{"instance_id": "2ff6f6db53c9", "generation_result": [{"code": "def safe_load(path, offset, duration, sample_rate, dtype)\n\t\n    try:\n        audio, _ = librosa.load(path, offset=offset, duration=duration, sr=sample_rate, mono=True, dtype=dtype)\n        return audio, False\n    except Exception as e:\n        logging.exception(e)\n        return -1.0, True\n\n", "CodeBleu_score": 27.34832819578249}, {"code": "def safe_load(path, offset, duration, sample_rate, dtype)\n\t\n    try:\n        print(f\"Loading {path} from {offset} to {offset + duration}...\")\n        audio_data, _ = librosa.load(path, offset=offset, duration=duration, sr=sample_rate, dtype=dtype)\n        return audio_data, False\n    except Exception as e:\n        print(f\"Error loading {path}: {e}\")\n        return -1.0, True\n\n", "CodeBleu_score": 37.261194482118185}, {"code": "def safe_load(path, offset, duration, sample_rate, dtype)\n\t\n    try:\n        audio_data, _ = sf.read(path, start=offset, stop=offset + duration, dtype=dtype, samplerate=sample_rate)\n        logging.info(f\"Loaded audio data from {path} at offset {offset} with duration {duration} and sample rate {sample_rate}.\")\n        return audio_data, False\n    except Exception as e:\n        logging.error(f\"Error loading audio data from {path} at offset {offset} with duration {duration} and sample rate {sample_rate}.\")\n        logging.error(e)\n        return -1.0, True\n\n", "CodeBleu_score": 37.567727069176}]}
{"instance_id": "12fbe1f03d8f", "generation_result": [{"code": "def to_n_channels(waveform: tf.Tensor, n_channels: int) -> tf.Tensor\n\t\n    if n_channels < 1:\n        raise ValueError(\"The number of channels must be at least 1.\")\n    elif n_channels == 1:\n        return tf.expand_dims(waveform, axis=-1)\n    else:\n        if waveform.shape[-1] >= n_channels:\n            return waveform[..., :n_channels]\n        else:\n            return tf.tile(waveform, (1, 1, n_channels))\n\n", "CodeBleu_score": 43.1248353553935}, {"code": "def to_n_channels(waveform: tf.Tensor, n_channels: int) -> tf.Tensor\n\t\n    if n_channels < 1:\n        raise ValueError(f\"The number of channels must be greater than 0, but got {n_channels}.\")\n\n    if n_channels == 1:\n        return tf.expand_dims(tf.reduce_mean(waveform, axis=-1), axis=-1)\n\n    if n_channels == waveform.shape[-1]:\n        return waveform\n\n    if waveform.shape[-1] > n_channels:\n        return waveform[..., :n_channels]\n\n    return tf.tile(waveform, [1, 1, n_channels])\n\n", "CodeBleu_score": 41.972337040711274}, {"code": "def to_n_channels(waveform: tf.Tensor, n_channels: int) -> tf.Tensor\n\t\n    if n_channels == waveform.shape[1]:\n        return waveform\n    elif n_channels > waveform.shape[1]:\n        return tf.tile(waveform, [1, n_channels // waveform.shape[1], 1])[:, :n_channels, :]\n    else:\n        return waveform[:, :n_channels, :]\n\n", "CodeBleu_score": 47.09323052887244}]}
{"instance_id": "5e215f029b2e", "generation_result": [{"code": "def spectrogram_to_db_uint(\n    spectrogram: tf.Tensor, db_range: float = 100.0, **kwargs\n) -> tf.Tensor\n\t\n    spectrogram = gain_to_db(spectrogram, **kwargs)\n    max_value = tf.reduce_max(spectrogram)\n    max_value = tf.maximum(max_value, 0.0)\n    max_value = tf.minimum(max_value, db_range)\n    return tf.cast((db_range - max_value) / max_value * spectrogram + max_value, tf.uint8)\n\n", "CodeBleu_score": 42.44542087661466}, {"code": "def spectrogram_to_db_uint(\n    spectrogram: tf.Tensor, db_range: float = 100.0, **kwargs\n) -> tf.Tensor\n\t\n    db_spectrogram = gain_to_db(spectrogram, **kwargs)\n    db_max = tf.math.reduce_max(db_spectrogram)\n    db_spectrogram = tf.clip_by_value(db_spectrogram, -db_range, db_max - db_range)\n    return tf.cast((db_spectrogram + db_range) / (2 * db_range), tf.uint8)\n\n", "CodeBleu_score": 42.87067681374484}, {"code": "def spectrogram_to_db_uint(\n    spectrogram: tf.Tensor, db_range: float = 100.0, **kwargs\n) -> tf.Tensor\n\t\n    spectrogram_db = gain_to_db(spectrogram, **kwargs)\n    spectrogram_db_max = tf.reduce_max(spectrogram_db, axis=[-3, -2], keepdims=True)\n    spectrogram_db_range = tf.clip_by_value(\n        spectrogram_db - spectrogram_db_max, -db_range, db_range\n    )\n    return tf.cast(\n        tf.expand_dims(tf.expand_dims(spectrogram_db_range, -1), -1) * 255.0 / db_range,\n        tf.uint8,\n    )\n\n", "CodeBleu_score": 41.08656850537533}]}
{"instance_id": "b65c0968dae7", "generation_result": [{"code": "def compute_spectrogram_tf(\n    waveform: tf.Tensor,\n    frame_length: int = 2048,\n    frame_step: int = 512,\n    spec_exponent: float = 1.0,\n    window_exponent: float = 1.0,\n) -> tf.Tensor\n\t\n    # compute the STFT\n    stft = tf.signal.stft(\n        waveform,\n        frame_length=frame_length,\n        frame_step=frame_step,\n        window_fn=lambda frame_length, dtype: tf.signal.hann_window(frame_length, periodic=True, dtype=dtype) ** window_exponent,\n    )\n    # transpose the STFT\n    stft = tf.transpose(stft, perm=[1, 2, 0])\n    # compute the magnitude spectrogram\n    spectrogram = tf.abs(stft) ** spec_exponent\n    return spectrogram\n\n", "CodeBleu_score": 42.23139465286893}, {"code": "def compute_spectrogram_tf(\n    waveform: tf.Tensor,\n    frame_length: int = 2048,\n    frame_step: int = 512,\n    spec_exponent: float = 1.0,\n    window_exponent: float = 1.0,\n) -> tf.Tensor\n\t\n    window_length = int(frame_length * 2 ** window_exponent)\n    window_step = int(frame_step * 2 ** window_exponent)\n\n    window = tf.signal.hann_window(window_length, periodic=True)\n\n    stft = tf.signal.stft(\n        waveform,\n        frame_length=frame_length,\n        frame_step=frame_step,\n        fft_length=frame_length,\n        window_fn=lambda _, l: window,\n    )\n\n    spectrogram = tf.abs(stft) ** spec_exponent\n\n    return tf.transpose(spectrogram, perm=[1, 2, 0])\n\n", "CodeBleu_score": 39.614240944685065}, {"code": "def compute_spectrogram_tf(\n    waveform: tf.Tensor,\n    frame_length: int = 2048,\n    frame_step: int = 512,\n    spec_exponent: float = 1.0,\n    window_exponent: float = 1.0,\n) -> tf.Tensor\n\t\n    window_fn = tf.signal.hann_window\n    window_fn = functools.partial(window_fn, periodic=True)\n    window_fn = functools.partial(window_fn, alpha=window_exponent)\n    stft = tf.signal.stft(\n        waveform,\n        frame_length=frame_length,\n        frame_step=frame_step,\n        window_fn=window_fn,\n    )\n    stft = tf.transpose(stft, perm=(1, 2, 0))\n    stft = tf.abs(stft)\n    stft = tf.pow(stft, spec_exponent)\n    return stft\n\n", "CodeBleu_score": 38.096886533197114}]}
{"instance_id": "a4686140a634", "generation_result": [{"code": "def time_stretch(\n    spectrogram: tf.Tensor,\n    factor: float = 1.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n    if factor == 1.0:\n        return spectrogram\n    time_steps = tf.shape(spectrogram)[1]\n    new_time_steps = tf.cast(tf.math.ceil(time_steps * factor), tf.int32)\n    new_spectrogram = tf.image.resize(\n        spectrogram,\n        [tf.shape(spectrogram)[0], new_time_steps],\n        method=method,\n    )\n    if factor > 1.0:\n        return new_spectrogram[:, :time_steps, :]\n    else:\n        return tf.pad(\n            new_spectrogram,\n            [[0, 0], [0, time_steps - new_time_steps], [0, 0]],\n        )\n\n", "CodeBleu_score": 43.74712681701675}, {"code": "def time_stretch(\n    spectrogram: tf.Tensor,\n    factor: float = 1.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n    # calculate new time dimension\n    new_time_dim = tf.cast(\n        tf.math.round(tf.shape(spectrogram)[1] * factor), tf.int32\n    )\n\n    # resize spectrogram\n    resized_spectrogram = tf.image.resize(\n        spectrogram, [tf.shape(spectrogram)[0], new_time_dim], method=method\n    )\n\n    # crop/pad spectrogram to original dimensions\n    if factor > 1.0:\n        cropped_spectrogram = resized_spectrogram[:, : tf.shape(spectrogram)[1]]\n    else:\n        cropped_spectrogram = tf.image.pad_to_bounding_box(\n            resized_spectrogram,\n            0,\n            0,\n            tf.shape(spectrogram)[0],\n            tf.shape(spectrogram)[1],\n        )\n\n    return cropped_spectrogram\n\n", "CodeBleu_score": 38.162702507469085}, {"code": "def time_stretch(\n    spectrogram: tf.Tensor,\n    factor: float = 1.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n    if factor == 1.0:\n        return spectrogram\n\n    # Calculate the new time dimension\n    new_time_dim = tf.cast(\n        tf.round(tf.cast(spectrogram.shape[1], tf.float32) * factor), tf.int32\n    )\n\n    # Resize the spectrogram\n    resized_spectrogram = tf.image.resize(\n        spectrogram, [spectrogram.shape[0], new_time_dim], method=method\n    )\n\n    # Adjust the resized spectrogram to the original dimensions\n    if new_time_dim > spectrogram.shape[1]:\n        resized_spectrogram = resized_spectrogram[:, : spectrogram.shape[1]]\n    else:\n        resized_spectrogram = tf.pad(\n            resized_spectrogram,\n            [[0, 0], [0, spectrogram.shape[1] - new_time_dim]],\n            mode=\"CONSTANT\",\n            constant_values=0.0,\n        )\n\n    return resized_spectrogram\n\n", "CodeBleu_score": 39.344991737643035}]}
{"instance_id": "6dd01101d4ae", "generation_result": [{"code": "def pitch_shift(\n    spectrogram: tf.Tensor,\n    semitone_shift: float = 0.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n\n    # Calculate the resizing factor\n    factor = 2.0 ** (semitone_shift / 12.0)\n\n    # Resize the spectrogram along the frequency axis\n    resized = tf.image.resize(\n        spectrogram,\n        [tf.shape(spectrogram)[0], tf.cast(tf.shape(spectrogram)[1] * factor, tf.int32)],\n        method=method,\n    )\n\n    # Pad the resized spectrogram to maintain the original dimensions\n    padded = tf.pad(\n        resized,\n        [[0, 0], [0, tf.shape(spectrogram)[1] - tf.shape(resized)[1]]],\n        constant_values=0.0,\n    )\n\n    return padded\n\n", "CodeBleu_score": 35.381618307235506}, {"code": "def pitch_shift(\n    spectrogram: tf.Tensor,\n    semitone_shift: float = 0.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n    # Calculate the resizing factor\n    factor = 2 ** semitone_shift\n\n    # Resize the spectrogram\n    resized_spectrogram = tf.image.resize(\n        spectrogram,\n        size=tf.cast(tf.shape(spectrogram)[1:3] * factor, tf.int32),\n        method=method,\n    )\n\n    # Pad the resized spectrogram to maintain the original dimensions\n    padded_spectrogram = tf.image.pad_to_bounding_box(\n        resized_spectrogram,\n        offset_height=0,\n        offset_width=0,\n        target_height=tf.shape(spectrogram)[0],\n        target_width=tf.shape(spectrogram)[1],\n    )\n\n    return padded_spectrogram\n\n", "CodeBleu_score": 30.007653990449057}, {"code": "def pitch_shift(\n    spectrogram: tf.Tensor,\n    semitone_shift: float = 0.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n\n    # Calculating the resizing factor\n    factor = tf.math.exp(semitone_shift * tf.math.log(2) / 12)\n\n    # Resizing the spectrogram\n    resized_spectrogram = tf.image.resize(\n        spectrogram,\n        size=[\n            tf.math.ceil(spectrogram.shape[0] * factor),\n            spectrogram.shape[1],\n        ],\n        method=method,\n    )\n\n    # Padding the resized spectrogram to maintain the original dimensions\n    padded_spectrogram = tf.pad(\n        resized_spectrogram,\n        paddings=[\n            [\n                tf.math.floor(\n                    (spectrogram.shape[0] * factor - resized_spectrogram.shape[0]) / 2\n                ),\n                tf.math.floor(\n                    (spectrogram.shape[0] * factor - resized_spectrogram.shape[0]) / 2\n                ),\n            ],\n            [0, 0],\n        ],\n    )\n\n    return padded_spectrogram\n\n", "CodeBleu_score": 23.944260436605873}]}
{"instance_id": "612f0826e56a", "generation_result": [{"code": "def _get_conv_activation_layer(params: Dict) -> Any\n\t\n    if params[\"conv_activation\"] == \"relu\":\n        return nn.ReLU()\n    elif params[\"conv_activation\"] == \"elu\":\n        return nn.ELU()\n    elif params[\"conv_activation\"] == \"leaky_relu\":\n        return nn.LeakyReLU(negative_slope=0.2)\n    else:\n        raise ValueError(f\"The provided convolutional activation layer {params['conv_activation']} is not supported.\")\n\n", "CodeBleu_score": 28.648668768613362}, {"code": "def _get_conv_activation_layer(params: Dict) -> Any\n\t\n    if params[\"conv_activation\"] == \"ReLU\":\n        return nn.ReLU()\n    elif params[\"conv_activation\"] == \"ELU\":\n        return nn.ELU()\n    elif params[\"conv_activation\"] == \"LeakyReLU\":\n        return nn.LeakyReLU(negative_slope=0.2)\n    else:\n        raise ValueError(\n            f\"The value of the 'conv_activation' parameter is not supported. Please choose one of the following: 'ReLU', 'ELU', 'LeakyReLU'.\"\n        )\n\n", "CodeBleu_score": 32.36044827245834}, {"code": "def _get_conv_activation_layer(params: Dict) -> Any\n\t\n    conv_activation = params.get(\"conv_activation\", \"relu\")\n    if conv_activation == \"relu\":\n        return nn.ReLU()\n    elif conv_activation == \"elu\":\n        return nn.ELU()\n    elif conv_activation == \"leaky_relu\":\n        return nn.LeakyReLU(negative_slope=0.2)\n    else:\n        raise ValueError(f\"Unknown activation function: {conv_activation}\")\n\n", "CodeBleu_score": 33.73568360479261}]}
{"instance_id": "2f8b5a2fc0f2", "generation_result": [{"code": "def softmax_unet(\n    input_tensor: tf.Tensor, instruments: Iterable[str], params: Dict = {}\n) -> Dict\n\t\n    output_dict = {}\n    for instrument in instruments:\n        output_dict[instrument] = apply_unet(\n            input_tensor, output_name=instrument, params=params\n        )\n    return output_dict\n\n", "CodeBleu_score": 18.26797141800781}, {"code": "def softmax_unet(\n    input_tensor: tf.Tensor, instruments: Iterable[str], params: Dict = {}\n) -> Dict\n\t\n    output_dict = {}\n    for instrument in instruments:\n        output_name = f\"{instrument}_mask\"\n        output_dict[instrument] = apply_unet(\n            input_tensor, output_name=output_name, params=params\n        )\n    output_dict = {\n        instrument: Multiply(name=instrument)([output_dict[instrument], input_tensor])\n        for instrument in instruments\n    }\n    return output_dict\n\n", "CodeBleu_score": 36.24970161418841}, {"code": "def softmax_unet(\n    input_tensor: tf.Tensor, instruments: Iterable[str], params: Dict = {}\n) -> Dict\n\t\n    output_mask_logit = params.get(\"output_mask_logit\", False)\n    output_name = params.get(\"output_name\", \"output\")\n    output_dict = {}\n    for instrument in instruments:\n        output_dict[instrument] = apply_unet(\n            input_tensor,\n            output_name=f\"{output_name}_{instrument}\",\n            params=params,\n            output_mask_logit=output_mask_logit,\n        )\n    output_tensor = tf.stack(list(output_dict.values()), axis=-1)\n    if output_mask_logit:\n        return output_tensor\n    output_tensor = tf.reduce_sum(output_tensor, axis=-1)\n    output_tensor = tf.math.log(output_tensor + 1e-10)\n    output_tensor = tf.nn.softmax(output_tensor, axis=-1)\n    output_dict = {}\n    for instrument, mask in zip(instruments, output_tensor):\n        output_dict[instrument] = Multiply(name=f\"{output_name}_{instrument}\")(\n            [mask, input_tensor]\n        )\n    return output_dict\n\n", "CodeBleu_score": 39.220579316910865}]}
{"instance_id": "400b19ce8520", "generation_result": [{"code": "def compute_file_checksum(path)\n\t\n", "CodeBleu_score": 0.7142857142857143}, {"code": "def compute_file_checksum(path)\n\t\n", "CodeBleu_score": 0.7142857142857143}, {"code": "def compute_file_checksum(path)\n\t\n", "CodeBleu_score": 0.7142857142857143}]}
{"instance_id": "5f2e9c6e15f1", "generation_result": [{"code": "def checksum(self, name: str) -> str\n\t\n    checksum_url = self.host + self.repository + self.release_path + self.checksum_index\n    response = requests.get(checksum_url)\n    if response.status_code != 200:\n        raise ValueError(\n            \"Request to URL \" + checksum_url + \" failed with status code \" + str(response.status_code))\n    index = response.json()\n    if name not in index:\n        raise ValueError(\"Model \" + name + \" not found in checksum index.\")\n    return index[name]\n\n", "CodeBleu_score": 40.003213327732325}, {"code": "def checksum(self, name: str) -> str\n\t\n    url = self.host + self.repository + self.release + self.checksum_index\n    response = requests.get(url)\n    if response.status_code != 200:\n        raise ConnectionError(\"HTTP GET request failed with status code \" + str(response.status_code))\n    data = response.json()\n    if name not in data:\n        raise ValueError(\"Model name not found in checksum index\")\n    return data[name]\n\n", "CodeBleu_score": 36.31830130116753}, {"code": "def checksum(self, name: str) -> str\n\t\n    url = f\"{self.host}{self.repository}/{self.release}/{self.checksum_index}\"\n    response = requests.get(url)\n    if response.status_code != 200:\n        raise ValueError(f\"Error: {response.status_code}\")\n    index = response.json()\n    if name not in index:\n        raise ValueError(f\"Error: {name} not found in index.\")\n    return index[name]\n\n", "CodeBleu_score": 38.68790013820191}]}
{"instance_id": "9312c8b6032e", "generation_result": [{"code": "def download(self, name: str, path: str) -> None:\n\t\n    url: str = \"/\".join(\n        (\n            self._host,\n            self._repository,\n            self.RELEASE_PATH,\n            self._release,\n            self.MODELS_PATH,\n            name,\n        )\n    )\n    checksum: str = self.checksum(name)\n    logger.info(f\"Downloading model {name} from {url}\")\n    with tempfile.NamedTemporaryFile(delete=False) as temporary_file:\n        with httpx.stream(\"GET\", url) as response:\n            response.raise_for_status()\n            for chunk in response.iter_bytes():\n                temporary_file.write(chunk)\n    logger.info(f\"Validating checksum for model {name}\")\n    if self.compute_file_checksum(temporary_file.name) != checksum:\n        logger.error(f\"Checksum for model {name} is invalid\")\n        os.remove(temporary_file.name)\n        raise RuntimeError(f\"Checksum for model {name} is invalid\")\n    logger.info(f\"Extracting model {name} to {path}\")\n    with tarfile.open(temporary_file.name) as archive:\n        archive.extractall(path)\n    os.remove(temporary_file.name)\n    logger.info(f\"Model {name} extracted to {path}\")\n", "CodeBleu_score": 49.77166173116533}, {"code": "def download(self, name: str, path: str) -> None:\n\t\n    url: str = \"/\".join(\n        (\n            self._host,\n            self._repository,\n            self.RELEASE_PATH,\n            self._release,\n            self.MODEL_PATH,\n            name,\n        )\n    )\n    checksum: str = self.checksum(name)\n    logger.info(f\"Downloading model {name} from {url}...\")\n    with tempfile.NamedTemporaryFile(\"wb\", delete=False) as archive:\n        with httpx.stream(\"GET\", url) as response:\n            response.raise_for_status()\n            for chunk in response.iter_bytes():\n                archive.write(chunk)\n    logger.info(\"Download complete.\")\n    logger.info(f\"Validating checksum for {name}...\")\n    if self.compute_file_checksum(archive.name) != checksum:\n        raise RuntimeError(f\"Checksum for {name} is invalid.\")\n    logger.info(\"Checksum is valid.\")\n    logger.info(f\"Extracting {name} to {path}...\")\n    with tarfile.open(archive.name) as tar:\n        tar.extractall(path)\n    logger.info(\"Extraction complete.\")\n    os.remove(archive.name)\n\n", "CodeBleu_score": 48.6561302317298}, {"code": "def download(self, name: str, path: str) -> None:\n\t\n    url: str = \"/\".join(\n        (\n            self._host,\n            self._repository,\n            self.RELEASE_PATH,\n            self._release,\n            self.ARCHIVE_PATH,\n            name,\n        )\n    )\n    checksum: str = self.checksum(name)\n    with tempfile.NamedTemporaryFile() as stream:\n        self._logger.info(f\"Downloading {url} to {stream.name}\")\n        with httpx.stream(\"GET\", url) as response:\n            response.raise_for_status()\n            for data in response.iter_bytes():\n                stream.write(data)\n        self._logger.info(f\"Validating checksum for {stream.name}\")\n        if checksum != self.compute_file_checksum(stream.name):\n            raise RuntimeError(f\"Checksum mismatch for {stream.name}\")\n        self._logger.info(f\"Extracting {stream.name} to {path}\")\n        with tarfile.open(stream.name, \"r:gz\") as archive:\n            archive.extractall(path)\n        self._logger.info(f\"Removing {stream.name}\")\n        os.remove(stream.name)\n", "CodeBleu_score": 45.594357555677135}]}
{"instance_id": "ee9493f37d77", "generation_result": [{"code": "def load_configuration(descriptor: str) -> Dict\n\t\n    if descriptor.startswith(\"embedded:\"):\n        name = descriptor[len(\"embedded:\") :]\n        return json.loads(pkgutil.get_data(__name__, \"configurations/\" + name + \".json\"))\n    else:\n        if os.path.exists(descriptor):\n            with open(descriptor, \"r\") as f:\n                return json.load(f)\n        else:\n            raise FileNotFoundError(f\"Configuration file {descriptor} does not exist.\")\n\n", "CodeBleu_score": 35.63738729017563}, {"code": "def load_configuration(descriptor: str) -> Dict\n\t\n    if descriptor.startswith(\"config://\"):\n        config_name = descriptor[len(\"config://\") :]\n        return load_embedded_configuration(config_name)\n    else:\n        return load_file_configuration(descriptor)\n\n", "CodeBleu_score": 17.708436991333564}, {"code": "def load_configuration(descriptor: str) -> Dict\n\t\n    if descriptor.startswith('{'):\n        return json.loads(descriptor)\n    if descriptor.startswith('embedded:'):\n        configuration_name = descriptor[len('embedded:'):]\n        if configuration_name not in resources.contents():\n            raise Exception(f'Embedded configuration {configuration_name} not found')\n        return json.loads(resources.read_text(configuration_name))\n    if not os.path.exists(descriptor):\n        raise Exception(f'Configuration file {descriptor} not found')\n    with open(descriptor, 'r') as f:\n        return json.load(f)\n\n", "CodeBleu_score": 50.51488007047264}]}
{"instance_id": "546134b67fbc", "generation_result": [{"code": "def sync_apply(\n    tensor_dict: Dict[str, tf.Tensor], func: Callable, concat_axis: int = 1\n) -> Dict[str, tf.Tensor]\n\t\n    if concat_axis not in [0, 1]:\n        raise ValueError(\n            f\"concat_axis must be either 0 or 1, but is {concat_axis}.\"\n        )\n    concat_tensors = tf.concat(\n        [tensor_dict[key] for key in tensor_dict.keys()], concat_axis\n    )\n    processed_concat_tensor = func(concat_tensors)\n    processed_tensors = tf.split(\n        processed_concat_tensor,\n        [tensor_dict[key].shape[concat_axis] for key in tensor_dict.keys()],\n        concat_axis,\n    )\n    return dict(zip(tensor_dict.keys(), processed_tensors))\n\n", "CodeBleu_score": 38.00269314862887}, {"code": "def sync_apply(\n    tensor_dict: Dict[str, tf.Tensor], func: Callable, concat_axis: int = 1\n) -> Dict[str, tf.Tensor]\n\t\n    if concat_axis not in [0, 1]:\n        raise ValueError(\"concat_axis must be either 0 or 1\")\n    if concat_axis == 0:\n        tensor_list = [tf.expand_dims(tensor, 0) for tensor in tensor_dict.values()]\n    else:\n        tensor_list = list(tensor_dict.values())\n    concatenated_tensor = tf.concat(tensor_list, concat_axis)\n    processed_tensor = func(concatenated_tensor)\n    if concat_axis == 0:\n        processed_tensor_list = tf.split(processed_tensor, len(tensor_list), 0)\n    else:\n        processed_tensor_list = tf.split(processed_tensor, len(tensor_list), 1)\n    processed_tensor_dict = {\n        name: tensor for name, tensor in zip(tensor_dict.keys(), processed_tensor_list)\n    }\n    return processed_tensor_dict\n\n", "CodeBleu_score": 48.821856184428384}, {"code": "def sync_apply(\n    tensor_dict: Dict[str, tf.Tensor], func: Callable, concat_axis: int = 1\n) -> Dict[str, tf.Tensor]\n\t\n    if concat_axis not in [0, 1]:\n        raise ValueError(\"concat_axis must be either 0 or 1.\")\n    if concat_axis == 0:\n        concat_func = tf.concat\n    else:\n        concat_func = tf.stack\n    concat_tensor = concat_func(list(tensor_dict.values()), axis=concat_axis)\n    processed_tensor = func(concat_tensor)\n    if concat_axis == 0:\n        split_func = tf.split\n    else:\n        split_func = tf.unstack\n    processed_list = split_func(processed_tensor, num_or_size_splits=len(tensor_dict))\n    return dict(zip(tensor_dict.keys(), processed_list))\n\n", "CodeBleu_score": 41.310343441785726}]}
{"instance_id": "df48e78d46d2", "generation_result": [{"code": "def from_float32_to_uint8(\n    tensor: tf.Tensor,\n    tensor_key: str = \"tensor\",\n    min_key: str = \"min\",\n    max_key: str = \"max\",\n) -> tf.Tensor\n\t\n\n    # Get the minimum and maximum values of the tensor\n    min_value = tf.reduce_min(tensor)\n    max_value = tf.reduce_max(tensor)\n\n    # Scale the tensor values to the uint8 range (0-255)\n    tensor = tf.divide(tf.subtract(tensor, min_value), tf.subtract(max_value, min_value))\n    tensor = tf.multiply(tensor, 255)\n    tensor = tf.cast(tensor, tf.uint8)\n\n    # Return the scaled tensor and the original minimum and maximum values\n    return {\n        tensor_key: tensor,\n        min_key: min_value,\n        max_key: max_value,\n    }\n\n", "CodeBleu_score": 43.4419312818602}, {"code": "def from_float32_to_uint8(\n    tensor: tf.Tensor,\n    tensor_key: str = \"tensor\",\n    min_key: str = \"min\",\n    max_key: str = \"max\",\n) -> tf.Tensor\n\t\n    with tf.name_scope(name=\"from_float32_to_uint8\"):\n        min_value = tf.reduce_min(input_tensor=tensor)\n        max_value = tf.reduce_max(input_tensor=tensor)\n        scale = tf.divide(x=max_value - min_value, y=tf.constant(255.0, dtype=tf.float32))\n        tensor = tf.cast(x=tensor, dtype=tf.float32)\n        tensor = tf.subtract(x=tensor, y=min_value)\n        tensor = tf.divide(x=tensor, y=scale)\n        tensor = tf.cast(x=tensor, dtype=tf.uint8)\n        return {tensor_key: tensor, min_key: min_value, max_key: max_value}\n\n", "CodeBleu_score": 44.751098480647755}, {"code": "def from_float32_to_uint8(\n    tensor: tf.Tensor,\n    tensor_key: str = \"tensor\",\n    min_key: str = \"min\",\n    max_key: str = \"max\",\n) -> tf.Tensor\n\t\n    min_value = tf.reduce_min(tensor)\n    max_value = tf.reduce_max(tensor)\n    tensor_normalized = tf.divide(tf.subtract(tensor, min_value), tf.subtract(max_value, min_value))\n    tensor_normalized = tf.multiply(tensor_normalized, 255)\n    tensor_normalized = tf.cast(tensor_normalized, tf.uint8)\n    return {\n        tensor_key: tensor_normalized,\n        min_key: min_value,\n        max_key: max_value,\n    }\n\n", "CodeBleu_score": 41.268781093730325}]}
{"instance_id": "7f479e5de492", "generation_result": [{"code": "def pad_and_partition(tensor: tf.Tensor, segment_len: int) -> tf.Tensor\n\t\n    pad_len = segment_len - (tensor.shape[0] % segment_len)\n    padded = tf.pad(tensor, [[0, pad_len], [0, 0]])\n    return tf.reshape(padded, [tensor.shape[0] // segment_len, segment_len, tensor.shape[1]])\n\n", "CodeBleu_score": 21.57711329276569}, {"code": "def pad_and_partition(tensor: tf.Tensor, segment_len: int) -> tf.Tensor\n\t\n    # Calculate the padding needed to make the tensor length a multiple of the segment length\n    pad_len = tf.math.floormod(tf.shape(tensor)[0], segment_len)\n    # Pad the tensor\n    padded = tf.pad(tensor, [[0, pad_len], [0, 0]])\n    # Reshape the tensor into a new tensor with the specified segment length\n    return tf.reshape(padded, [-1, segment_len])\n\n", "CodeBleu_score": 21.746818620235842}, {"code": "def pad_and_partition(tensor: tf.Tensor, segment_len: int) -> tf.Tensor\n\t\n    # Calculate the number of padding elements needed to make the tensor length a multiple of the segment length\n    pad_len = tf.math.mod(tf.shape(tensor)[0], segment_len)\n    # Pad the tensor with zeros\n    padded_tensor = tf.pad(tensor, [[0, pad_len], [0, 0]])\n    # Reshape the tensor into segments of the specified length\n    return tf.reshape(padded_tensor, [-1, segment_len])\n\n", "CodeBleu_score": 19.813980680338588}]}
{"instance_id": "1789fae7253f", "generation_result": [{"code": "def pad_and_reshape(instr_spec, frame_length, F) -> Any\n\t\n    pad_size = frame_length - (instr_spec.shape[1] % frame_length)\n    if pad_size == frame_length:\n        pad_size = 0\n    pad_spec = torch.zeros((instr_spec.shape[0], pad_size, instr_spec.shape[2]))\n    padded_spec = torch.cat((instr_spec, pad_spec), 1)\n    return padded_spec.reshape((padded_spec.shape[0] * padded_spec.shape[1], F))\n\n", "CodeBleu_score": 20.377444988418734}, {"code": "def pad_and_reshape(instr_spec, frame_length, F) -> Any\n\t\n    pad_size = frame_length - (instr_spec.shape[1] % frame_length)\n    if pad_size != frame_length:\n        pad_width = ((0, 0), (0, pad_size))\n        instr_spec = np.pad(instr_spec, pad_width, 'constant')\n    return np.reshape(instr_spec, (instr_spec.shape[0] * instr_spec.shape[1] // frame_length, frame_length, F))\n\n", "CodeBleu_score": 17.098988212112747}, {"code": "def pad_and_reshape(instr_spec, frame_length, F) -> Any\n\t\n    pad_size = frame_length - (instr_spec.shape[-1] % frame_length)\n    if pad_size != 0:\n        pad = torch.zeros(instr_spec.shape[0], pad_size, instr_spec.shape[2])\n        instr_spec = torch.cat((instr_spec, pad), dim=1)\n    return instr_spec.view(instr_spec.shape[0], -1, F)\n\n", "CodeBleu_score": 18.514398434453128}]}
{"instance_id": "8a4f4658d97c", "generation_result": [{"code": "def dataset_from_csv(csv_path: str, **kwargs) -> Any\n\t\n    df = pd.read_csv(csv_path)\n    return tf.data.Dataset.from_tensor_slices((df[kwargs[\"columns\"][0]], df[kwargs[\"columns\"][1]]))\n\n", "CodeBleu_score": 22.75913027220249}, {"code": "def dataset_from_csv(csv_path: str, **kwargs) -> Any\n\t\n    df = pd.read_csv(csv_path, **kwargs)\n    return tf.data.Dataset.from_tensor_slices((dict(df)))\n\n", "CodeBleu_score": 27.26343108616177}, {"code": "def dataset_from_csv(csv_path: str, **kwargs) -> Any\n\t\n    # Read the CSV file into a pandas DataFrame\n    df = pd.read_csv(csv_path, **kwargs)\n\n    # Convert the DataFrame columns into a TensorFlow dataset by slicing the tensor values for each column\n    return tf.data.Dataset.from_tensor_slices((df[df.columns[:-1]].values, df[df.columns[-1]].values))\n\n", "CodeBleu_score": 34.10015351594897}]}
{"instance_id": "9a16e81b182f", "generation_result": [{"code": "def check_tensor_shape(tensor_tf: tf.Tensor, target_shape: Any) -> bool\n\t\n    result = True\n    for i, dim in enumerate(target_shape):\n        if dim is not None:\n            result = result and (tensor_tf.shape[i] == dim)\n    return result\n\n", "CodeBleu_score": 30.692801589977275}, {"code": "def check_tensor_shape(tensor_tf: tf.Tensor, target_shape: Any) -> bool\n\t\n    result = True\n    for i, dim in enumerate(target_shape):\n        if dim is not None:\n            result = result and (tensor_tf.shape[i] == dim)\n    return result\n\n", "CodeBleu_score": 30.692801589977275}, {"code": "def check_tensor_shape(tensor_tf: tf.Tensor, target_shape: Any) -> bool\n\t\n    result = True\n    for i, dim in enumerate(target_shape):\n        if dim is not None:\n            result = result and (tensor_tf.shape[i] == dim)\n    return result\n\n", "CodeBleu_score": 30.692801589977275}]}
{"instance_id": "49d5d3084b3a", "generation_result": [{"code": "def get_training_dataset(\n    audio_params: Dict, audio_adapter: AudioAdapter, audio_path: str\n) -> Any\n\t\n    builder = DatasetBuilder(\n        chunk_duration=audio_params[\"chunk_duration\"],\n        random_seed=audio_params[\"random_seed\"],\n        instruments=[\n            Instrument(\n                instrument_name=instrument_name,\n                audio_adapter=audio_adapter,\n                audio_path=audio_path,\n            )\n            for instrument_name in audio_params[\"instruments\"]\n        ],\n    )\n    return builder.build(\n        csv_path=audio_params[\"train_csv\"],\n        batch_size=audio_params[\"batch_size\"],\n        shuffle=audio_params[\"shuffle\"],\n        convert_to_uint=audio_params[\"convert_to_uint\"],\n        random_data_augmentation=audio_params[\"random_data_augmentation\"],\n        random_time_crop=audio_params[\"random_time_crop\"],\n        infinite_generator=audio_params[\"infinite_generator\"],\n        cache_directory=audio_params[\"cache_directory\"],\n        wait_for_cache=audio_params[\"wait_for_cache\"],\n        num_parallel_calls=audio_params[\"num_parallel_calls\"],\n        n_chunks_per_song=audio_params[\"n_chunks_per_song\"],\n    )\n\n", "CodeBleu_score": 30.57568335327775}, {"code": "def get_training_dataset(\n    audio_params: Dict, audio_adapter: AudioAdapter, audio_path: str\n) -> Any\n\t\n    builder = DatasetBuilder(\n        chunk_duration=audio_params[\"chunk_duration\"],\n        random_seed=audio_params[\"random_seed\"],\n        instruments=[\n            Instrument(\n                name=\"vocals\",\n                audio_adapter=audio_adapter,\n                audio_path=audio_path,\n                instrument_type=\"vocals\",\n            ),\n            Instrument(\n                name=\"drums\",\n                audio_adapter=audio_adapter,\n                audio_path=audio_path,\n                instrument_type=\"drums\",\n            ),\n            Instrument(\n                name=\"bass\",\n                audio_adapter=audio_adapter,\n                audio_path=audio_path,\n                instrument_type=\"bass\",\n            ),\n            Instrument(\n                name=\"other\",\n                audio_adapter=audio_adapter,\n                audio_path=audio_path,\n                instrument_type=\"other\",\n            ),\n        ],\n    )\n    dataset = builder.build(\n        csv_path=audio_params[\"train_csv\"],\n        batch_size=audio_params[\"batch_size\"],\n        convert_to_uint=audio_params[\"convert_to_uint\"],\n        random_data_augmentation=audio_params[\"random_data_augmentation\"],\n        random_time_crop=audio_params[\"random_time_crop\"],\n        infinite_generator=True,\n        cache_directory=audio_params[\"cache_directory\"],\n        wait_for_cache=audio_params[\"wait_for_cache\"],\n        num_parallel_calls=audio_params[\"num_parallel_calls\"],\n        n_chunks_per_song=audio_params[\"n_chunks_per_song\"],\n    )\n    return dataset\n\n", "CodeBleu_score": 31.45827884024185}, {"code": "def get_training_dataset(\n    audio_params: Dict, audio_adapter: AudioAdapter, audio_path: str\n) -> Any\n\t\n    builder = DatasetBuilder(\n        chunk_duration=audio_params[\"chunk_duration\"],\n        random_seed=audio_params[\"random_seed\"],\n        instruments=audio_params[\"instruments\"],\n    )\n    return builder.build(\n        csv_path=audio_path,\n        batch_size=audio_params[\"batch_size\"],\n        cache_directory=audio_params[\"cache_directory\"],\n        wait_for_cache=audio_params[\"wait_for_cache\"],\n        random_data_augmentation=audio_params[\"random_data_augmentation\"],\n        random_time_crop=audio_params[\"random_time_crop\"],\n        n_chunks_per_song=audio_params[\"n_chunks_per_song\"],\n    )\n\n", "CodeBleu_score": 34.306585698773965}]}
{"instance_id": "fe8bbbba06e3", "generation_result": [{"code": "def instruments(self) -> Any\n\t\n    if self._instruments is not None:\n        for instrument in self._instruments:\n            yield instrument\n    else:\n        self._instruments = []\n        for instrument in self.config[\"instruments\"]:\n            self._instruments.append(\n                InstrumentDatasetBuilder(\n                    self.config[\"instruments\"][instrument],\n                    self.config[\"market\"],\n                    self.config[\"start_time\"],\n                    self.config[\"end_time\"],\n                    self.config[\"data_frequency\"],\n                    self.config[\"output_file_type\"],\n                    self.config[\"output_size\"],\n                    self.config[\"csv_path\"],\n                    self.config[\"sql_path\"],\n                    self.config[\"api_key\"],\n                    self.config[\"api_secret\"],\n                    self.config[\"api_passphrase\"],\n                    self.config[\"api_url\"],\n                    self.config[\"api_version\"],\n                    self.config[\"api_timeout\"],\n                    self.config[\"api_max_retries\"],\n                    self.config[\"api_retry_codes\"],\n                    self.config[\"api_retry_delay\"],\n                    self.config[\"api_retry_backoff\"],\n                    self.config[\"api_verify\"],\n                    self.config[\"api_session\"],\n                    self.config[\"api_logging\"],\n                    self.config[\"api_throttling\"],\n                    self.config[\"api_throttling_weight\"],\n                    self.config[\"api_throttling_rate\"],\n                    self.config[\"api_throttling_concurrent\"],\n                    self.config[\"api_throttling_retry\"],\n                    self.config[\"api_throttling_backoff\"],\n                    self.config[\"api_throttling_expire\"],\n                    self.config[\"api_throttling_expire_duration\"],\n                    self.config[\"api_throttling_expire_duration_unit\"],\n                    self.config[\"api_throttling_max_tokens\"],\n                    self.config[\"api_throttling_tokens\"],\n                    self.config[\"api_throttling_tokens_duration\"],\n                    self.config[\"api_throttling_tokens_duration_unit\"],\n                    self.config[\"api_throttling_lock\"],\n                    self.config[\"api_throttling_lock_timeout\"],\n                    self.config[\"api_throttling_lock_timeout_unit\"],\n                    self.config[\"api_throttling_lock_retry\"],\n                    self.config[\"api_throttling_lock_retry_delay\"],\n                    self.config[\"api_th", "CodeBleu_score": 28.289624569943573}, {"code": "def instruments(self) -> Any\n\t\n    return self._instruments\n\n", "CodeBleu_score": 0.8939435868598695}, {"code": "def instruments(self) -> Any\n\t\n    if self.instrument_builders is None:\n        self.instrument_builders = []\n        for instrument in self.instruments:\n            self.instrument_builders.append(InstrumentDatasetBuilder(instrument, self.exchange_name, self.start_date, self.end_date, self.frequency, self.timezone, self.currency, self.fill_method, self.order_mapping, self.asset_mapping, self.asset_name_mappings, self.asset_id_mappings, self.asset_type_mappings, self.asset_db_ids, self.asset_db_name_mappings, self.asset_db_id_mappings, self.asset_db_type_mappings, self.asset_db_exchange_mappings, self.asset_db_country_mappings, self.asset_db_currency_mappings, self.asset_db_commodity_mappings, self.asset_db_industry_mappings, self.asset_db_category_mappings, self.asset_db_region_mappings, self.asset_db_sector_mappings, self.asset_db_fund_mappings, self.asset_db_market_mappings, self.asset_db_quote_type_mappings, self.asset_db_quoting_style_mappings, self.asset_db_tick_size_mappings, self.asset_db_min_tick_size_mappings, self.asset_db_option_type_mappings, self.asset_db_underlying_asset_id_mappings, self.asset_db_strike_price_mappings, self.asset_db_expiration_date_mappings, self.asset_db_multiplier_mappings, self.asset_db_underlying_asset_mappings, self.asset_db_option_style_mappings, self.asset_db_option_root_symbol_mappings, self.asset_db_option_expiration_date_mappings, self.asset_db_option_strike_price_mappings, self.asset_db_option_contract_type_mappings, self.asset_db_option_currency_mappings, self.asset_db_future_root_symbol_mappings, self.asset_db_future_contract_type_mappings, self.asset_db_future_expiration_date_mappings, self.asset_db_future_tick_size_mappings,", "CodeBleu_score": 21.335885366240756}]}
{"instance_id": "438b57fd3413", "generation_result": [{"code": "def cache(self, dataset: Any, cache: Optional[str], wait: bool) -> Any\n\t\n    return self.cache_dataset(dataset, cache, wait)\n\n", "CodeBleu_score": 14.556785063025519}, {"code": "def cache(self, dataset: Any, cache: Optional[str], wait: bool) -> Any\n\t\n    if cache is not None:\n        if wait:\n            while not exists(f\"{cache}.index\"):\n                logger.info(f\"Cache not available, wait {self.WAIT_PERIOD}\")\n                time.sleep(self.WAIT_PERIOD)\n        cache_path = os.path.split(cache)[0]\n        os.makedirs(cache_path, exist_ok=True)\n        return dataset.cache(cache)\n    return dataset\n\n", "CodeBleu_score": 93.65648279369884}, {"code": "def cache(self, dataset: Any, cache: Optional[str], wait: bool) -> Any\n\t\n    if cache is not None:\n        if wait:\n            while not exists(f\"{cache}.index\"):\n                logger.info(f\"Cache not available, wait {self.WAIT_PERIOD}\")\n                time.sleep(self.WAIT_PERIOD)\n        cache_path = os.path.split(cache)[0]\n        os.makedirs(cache_path, exist_ok=True)\n        return dataset.cache(cache)\n    return dataset\n\n", "CodeBleu_score": 93.65648279369884}]}
{"instance_id": "8838a3a09a66", "generation_result": [{"code": "def create_estimator(params: Dict, MWF: bool) -> tf.Tensor\n\t\n    # Load model directory\n    model_provider = ModelProvider(params[\"model_dir\"])\n    # Update parameters\n    params.update(model_provider.get_params())\n    # Set up session configuration\n    session_config = tf.ConfigProto()\n    # Limit GPU memory usage\n    if params[\"gpu_memory_fraction\"] is not None:\n        session_config.gpu_options.per_process_gpu_memory_fraction = params[\"gpu_memory_fraction\"]\n    # Create RunConfig\n    run_config = tf.estimator.RunConfig(\n        model_dir=params[\"model_dir\"],\n        session_config=session_config,\n        save_summary_steps=params[\"save_summary_steps\"],\n        save_checkpoints_steps=params[\"save_checkpoints_steps\"],\n        log_step_count_steps=params[\"log_step_count_steps\"],\n        keep_checkpoint_max=params[\"keep_checkpoint_max\"],\n    )\n    # Initialize and return estimator\n    estimator = tf.estimator.Estimator(\n        model_fn=model_provider.get_model_fn(MWF),\n        model_dir=params[\"model_dir\"],\n        params=params,\n        config=run_config,\n    )\n    return estimator\n\n", "CodeBleu_score": 43.87787657212618}, {"code": "def create_estimator(params: Dict, MWF: bool) -> tf.Tensor\n\t\n    model_provider = ModelProvider(params)\n    model_provider.load_model_from_dir()\n    params = model_provider.update_params(params)\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    config.gpu_options.per_process_gpu_memory_fraction = params[\"gpu_memory_fraction\"]\n    config.log_device_placement = False\n    run_config = tf.estimator.RunConfig().replace(\n        session_config=config, save_checkpoints_secs=None, save_checkpoints_steps=None\n    )\n    estimator = tf.estimator.Estimator(\n        model_fn=model_provider.get_model_function(),\n        model_dir=params[\"model_dir\"],\n        params=params,\n        config=run_config,\n    )\n    return estimator\n\n", "CodeBleu_score": 34.753024302900535}, {"code": "def create_estimator(params: Dict, MWF: bool) -> tf.Tensor\n\t\n    # Load the model directory\n    model_provider = ModelProvider(params)\n    model_dir = model_provider.get_model_dir()\n\n    # Update the parameters\n    params[\"model_dir\"] = model_dir\n    params[\"MWF\"] = MWF\n\n    # Setup the session configuration\n    session_config = tf.ConfigProto()\n    session_config.gpu_options.allow_growth = True\n\n    # Create a RunConfig with the session configuration\n    run_config = tf.estimator.RunConfig().replace(session_config=session_config)\n\n    # Initialize and return a TensorFlow estimator\n    estimator = tf.estimator.Estimator(\n        model_fn=model_provider.get_model_fn(),\n        model_dir=model_dir,\n        params=params,\n        config=run_config,\n    )\n    return estimator\n\n", "CodeBleu_score": 40.83513502489026}]}
{"instance_id": "779c812bf2cb", "generation_result": [{"code": "def _separate_tensorflow(\n        self, waveform: np.ndarray, audio_descriptor: AudioDescriptor\n    ) -> Dict\n\t\n    if len(waveform.shape) > 1 and waveform.shape[1] > 1:\n        waveform = waveform.mean(axis=1)\n    self._data_generator.update_data((waveform, audio_descriptor.audio_id))\n    prediction = next(self._get_prediction_generator)\n    prediction.pop(\"audio_id\")\n    return prediction\n\n", "CodeBleu_score": 45.07452061832995}, {"code": "def _separate_tensorflow(\n        self, waveform: np.ndarray, audio_descriptor: AudioDescriptor\n    ) -> Dict\n\t\n    if waveform.ndim != 2 or waveform.shape[1] != 2:\n        raise ValueError(\n            \"Input waveform should be a 2D array with shape (n_samples, 2)\"\n        )\n\n    self._update_data(\n        {\"waveform\": waveform, \"audio_id\": audio_descriptor.audio_id}\n    )\n    prediction_generator = self._get_prediction_generator()\n    prediction = next(prediction_generator)\n    return {k: v for k, v in prediction.items() if k != \"audio_id\"}\n", "CodeBleu_score": 52.22922648943782}, {"code": "def _separate_tensorflow(\n        self, waveform: np.ndarray, audio_descriptor: AudioDescriptor\n    ) -> Dict\n\t\n    if len(waveform.shape) > 1 and waveform.shape[1] > 1:\n        waveform = np.mean(waveform, axis=1)\n    if len(waveform.shape) == 1:\n        waveform = np.expand_dims(waveform, axis=0)\n    self._update_data(\n        {\"waveform\": waveform, \"audio_id\": audio_descriptor.audio_id}\n    )\n    prediction = next(self._get_prediction_generator())\n    prediction.pop(\"audio_id\", None)\n    return prediction\n", "CodeBleu_score": 48.41846212748694}]}
{"instance_id": "cf580cd2ed93", "generation_result": [{"code": "def compute_backbone_shapes(config, image_shape)\n\t\n    if callable(config.BACKBONE):\n        return config.COMPUTE_BACKBONE_SHAPE(image_shape)\n\n    assert config.BACKBONE in [\"resnet50\", \"resnet101\"], \"backbone is not resnet50 or resnet101\"\n    return np.array(\n        [[int(math.ceil(image_shape[0] / stride)),\n            int(math.ceil(image_shape[1] / stride))]\n            for stride in config.BACKBONE_STRIDES])\n\n", "CodeBleu_score": 88.67982951914676}, {"code": "def compute_backbone_shapes(config, image_shape)\n\t\n    if callable(config.BACKBONE):\n        return config.COMPUTE_BACKBONE_SHAPE(image_shape)\n\n    assert config.BACKBONE in [\"resnet50\", \"resnet101\"], \"backbone is not resnet50 or resnet101\"\n    return np.array(\n        [[int(math.ceil(image_shape[0] / stride)), int(math.ceil(image_shape[1] / stride))] for stride in config.BACKBONE_STRIDES]\n    )\n\n", "CodeBleu_score": 87.19476552695762}, {"code": "def compute_backbone_shapes(config, image_shape)\n\t\n    if callable(config.BACKBONE):\n        return config.COMPUTE_BACKBONE_SHAPE(image_shape)\n\n    assert config.BACKBONE in [\"resnet50\", \"resnet101\"], \"backbone is not resnet50 or resnet101\"\n    return np.array(\n        [[int(math.ceil(image_shape[0] / stride)), int(math.ceil(image_shape[1] / stride))] for stride in config.BACKBONE_STRIDES])\n\n", "CodeBleu_score": 88.35928973797768}]}
{"instance_id": "ebd61a2e59ca", "generation_result": [{"code": "def clip_boxes_graph(boxes, window)\n\t\n    # Split\n    wy1, wx1, wy2, wx2 = tf.split(window, 4)\n    y1, x1, y2, x2 = tf.split(boxes, 4, axis=2)\n\n    # Clip\n    y1 = tf.maximum(tf.minimum(y1, wy2), wy1)\n    x1 = tf.maximum(tf.minimum(x1, wx2), wx1)\n    y2 = tf.maximum(tf.minimum(y2, wy2), wy1)\n    x2 = tf.maximum(tf.minimum(x2, wx2), wx1)\n    clipped = tf.concat([y1, x1, y2, x2], axis=2, name=\"clipped_boxes\")\n    clipped.set_shape((clipped.shape[0], clipped.shape[1], 4))\n    return clipped\n\n", "CodeBleu_score": 55.60362680082691}, {"code": "def clip_boxes_graph(boxes, window)\n\t\n    # Split\n    wy1, wx1, wy2, wx2 = tf.split(window, 4)\n    y1, x1, y2, x2 = tf.split(boxes, 4, axis=1)\n    # Clip\n    y1 = tf.maximum(tf.minimum(y1, wy2), wy1)\n    x1 = tf.maximum(tf.minimum(x1, wx2), wx1)\n    y2 = tf.maximum(tf.minimum(y2, wy2), wy1)\n    x2 = tf.maximum(tf.minimum(x2, wx2), wx1)\n    clipped = tf.concat([y1, x1, y2, x2], axis=1)\n    clipped.set_shape((clipped.shape[0], 4))\n    return clipped\n\n", "CodeBleu_score": 56.01484333396114}, {"code": "def clip_boxes_graph(boxes, window)\n\t\n    # Split the window into coordinates\n    wy1, wx1, wy2, wx2 = tf.split(window, 4)\n    y1, x1, y2, x2 = tf.split(boxes, 4, axis=2)\n    # Clip\n    y1 = tf.maximum(tf.minimum(y1, wy2), wy1)\n    x1 = tf.maximum(tf.minimum(x1, wx2), wx1)\n    y2 = tf.maximum(tf.minimum(y2, wy2), wy1)\n    x2 = tf.maximum(tf.minimum(x2, wx2), wx1)\n    clipped = tf.concat([y1, x1, y2, x2], axis=2)\n    clipped.set_shape((clipped.shape[0], clipped.shape[1], 4))\n    return clipped\n\n", "CodeBleu_score": 54.67274953842913}]}
{"instance_id": "8e0411587a70", "generation_result": [{"code": "def rpn_graph(feature_map, anchors_per_location, anchor_stride)\n\t\n\n    # Shared convolutional base of the RPN\n    shared = Conv2D(512, (3, 3), padding='same', activation='relu', kernel_initializer='normal', name='rpn_conv_shared')(feature_map)\n\n    # Anchor Score. [batch, height, width, anchors per location * 2].\n    x = Conv2D(2 * anchors_per_location, (1, 1), padding='valid', activation='linear', kernel_initializer='zero', name='rpn_class_raw')(shared)\n\n    # Reshape to [batch, anchors, 2]\n    rpn_class_logits = Lambda(lambda t: tf.reshape(t, [tf.shape(t)[0], -1, 2]))(x)\n\n    # Softmax on last dimension of BG/FG.\n    rpn_probs = Activation('softmax', name='rpn_class_xxx')(rpn_class_logits)\n\n    # Bounding box refinement. [batch, H, W, anchors per location * depth]\n    # where depth is [x, y, log(w), log(h)]\n    x = Conv2D(anchors_per_location * 4, (1, 1), padding='valid', activation='linear', kernel_initializer='zero', name='rpn_bbox_pred')(shared)\n\n    # Reshape to [batch, anchors, 4]\n    rpn_bbox = Lambda(lambda t: tf.reshape(t, [tf.shape(t)[0], -1, 4]))(x)\n\n    return [rpn_class_logits, rpn_probs, rpn_bbox]\n\n", "CodeBleu_score": 61.109779464242784}, {"code": "def rpn_graph(feature_map, anchors_per_location, anchor_stride)\n\t\n    # Shared convolutional base of the RPN\n    shared = Conv2D(512, (3, 3), padding='same', activation='relu', strides=anchor_stride, name='rpn_conv_shared')(feature_map)\n\n    # Anchor Score. [batch, height, width, anchors per location * 2].\n    x = Conv2D(2 * anchors_per_location, (1, 1), padding='valid', activation='linear', name='rpn_class_raw')(shared)\n\n    # Reshape to [batch, anchors, 2]\n    rpn_class_logits = Lambda(lambda t: tf.reshape(t, [tf.shape(t)[0], -1, 2]))(x)\n\n    # Softmax on last dimension of BG/FG.\n    rpn_probs = Activation(\"softmax\", name=\"rpn_class_xxx\")(rpn_class_logits)\n\n    # Bounding box refinement. [batch, H, W, anchors per location * depth]\n    # where depth is [x, y, log(w), log(h)]\n    x = Conv2D(anchors_per_location * 4, (1, 1), padding=\"valid\", activation='linear', name='rpn_bbox_pred')(shared)\n\n    # Reshape to [batch, anchors, 4]\n    rpn_bbox = Lambda(lambda t: tf.reshape(t, [tf.shape(t)[0], -1, 4]))(x)\n\n    return [rpn_class_logits, rpn_probs, rpn_bbox]\n\n", "CodeBleu_score": 66.44609526445541}, {"code": "def rpn_graph(feature_map, anchors_per_location, anchor_stride)\n\t\n    shared = Conv2D(512, (3, 3), padding='same', activation='relu', kernel_initializer='normal', name='rpn_conv_shared')(feature_map)\n    x = Conv2D(512, (3, 3), padding='same', activation='relu', kernel_initializer='normal', name='rpn_class_raw')(shared)\n    rpn_class_logits = Reshape((-1, 2))(x)\n    rpn_probs = Activation(\"softmax\", name=\"rpn_class_xxx\")(rpn_class_logits)\n    x = Conv2D(512, (3, 3), padding='same', activation='relu', kernel_initializer='normal', name='rpn_bbox_pred')(shared)\n    rpn_bbox_pred = Reshape((-1, 4))(x)\n    return [rpn_class_logits, rpn_probs, rpn_bbox_pred]\n\n", "CodeBleu_score": 36.331801139845425}]}
{"instance_id": "70777c9261d8", "generation_result": [{"code": "def smooth_l1_loss(y_true, y_pred)\n\t\n    abs_diff = tf.abs(y_true - y_pred)\n    sqr_diff = 0.5 * (y_true - y_pred) ** 2\n    l1_loss = tf.where(abs_diff < 1.0, sqr_diff, abs_diff - 0.5)\n    return tf.reduce_sum(l1_loss)\n\n", "CodeBleu_score": 26.093252129576406}, {"code": "def smooth_l1_loss(y_true, y_pred)\n\t\n    mask = K.abs(y_true - y_pred) < 1\n    loss = 0.5 * K.square(y_true - y_pred)\n    loss = loss * mask + (K.abs(y_true - y_pred) - 0.5) * (1 - mask)\n    return K.sum(loss)\n\n", "CodeBleu_score": 34.00086158337067}, {"code": "def smooth_l1_loss(y_true, y_pred)\n\t\n    absolute_difference = K.abs(y_true - y_pred)\n    mask = K.cast(K.less(absolute_difference, 1.0), \"float32\")\n    return (\n        mask * 0.5 * K.square(absolute_difference)\n        + (1 - mask) * (absolute_difference - 0.5)\n    )\n\n", "CodeBleu_score": 30.782769605389205}]}
{"instance_id": "0e747b604087", "generation_result": [{"code": "def build_model(x_train, num_classes)\n\t\n    # Reset the TensorFlow graph\n    tf.reset_default_graph()\n\n    # Define the input layer\n    with tf.name_scope('input'):\n        x = tf.placeholder(tf.float32, shape=[None, x_train.shape[1], x_train.shape[2], x_train.shape[3]], name='x')\n        y = tf.placeholder(tf.float32, shape=[None, num_classes], name='y')\n\n    # Define the first convolutional layer\n    with tf.name_scope('conv1'):\n        W_conv1 = weight_variable([5, 5, 3, 32], name='W_conv1')\n        b_conv1 = bias_variable([32], name='b_conv1')\n        h_conv1 = tf.nn.relu(conv2d(x, W_conv1) + b_conv1)\n        h_pool1 = max_pool_2x2(h_conv1)\n\n    # Define the second convolutional layer\n    with tf.name_scope('conv2'):\n        W_conv2 = weight_variable([5, 5, 32, 64], name='W_conv2')\n        b_conv2 = bias_variable([64], name='b_conv2')\n        h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n        h_pool2 = max_pool_2x2(h_conv2)\n\n    # Define the densely connected layer\n    with tf.name_scope('fc1'):\n        W_fc1 = weight_variable([7 * 7 * 64, 1024], name='W_fc1')\n        b_fc1 = bias_variable([1024], name='b_fc1')\n        h_pool2_flat = tf.reshape(h_pool2, [-1, 7 * 7 * 64])\n        h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n\n    # Define the output layer\n    with tf.name_scope('output'):\n        W_fc2 = weight_variable([1024, num_classes], name='W", "CodeBleu_score": 30.915292375617476}, {"code": "def build_model(x_train, num_classes)\n\t\n\n    tf.reset_default_graph()\n    x = tf.placeholder(tf.float32, [None, x_train.shape[1], x_train.shape[2], 1])\n    y = tf.placeholder(tf.int32, [None])\n    keep_prob = tf.placeholder(tf.float32)\n\n    conv1 = tf.layers.conv2d(inputs=x, filters=32, kernel_size=[5, 5], padding=\"same\", activation=tf.nn.relu)\n    pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\n\n    conv2 = tf.layers.conv2d(inputs=pool1, filters=64, kernel_size=[5, 5], padding=\"same\", activation=tf.nn.relu)\n    pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n\n    pool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 64])\n    dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)\n    dropout = tf.layers.dropout(inputs=dense, rate=keep_prob)\n    logits = tf.layers.dense(inputs=dropout, units=num_classes)\n    y_pred = tf.nn.softmax(logits)\n    loss = tf.losses.sparse_softmax_cross_entropy(labels=y, logits=logits)\n    optimizer = tf.train.AdamOptimizer()\n    train_op = optimizer.minimize(loss)\n    correct_pred = tf.equal(tf.argmax(y_pred, 1), tf.cast(y, tf.int64))\n    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n    return train_op, loss, accuracy, x, y, keep_prob\n\n", "CodeBleu_score": 23.289864200734122}, {"code": "def build_model(x_train, num_classes)\n\t\n    tf.reset_default_graph()\n    x = tf.keras.layers.Input(shape=x_train.shape[1:])\n\n    # Convolutional layer 1\n    conv1 = tf.keras.layers.Conv2D(filters=32, kernel_size=3, strides=1, padding='same', activation='relu')(x)\n    conv1 = tf.keras.layers.BatchNormalization()(conv1)\n    conv1 = tf.keras.layers.Conv2D(filters=32, kernel_size=3, strides=1, padding='same', activation='relu')(conv1)\n    conv1 = tf.keras.layers.BatchNormalization()(conv1)\n    pool1 = tf.keras.layers.MaxPool2D(pool_size=2, strides=2, padding='same')(conv1)\n\n    # Convolutional layer 2\n    conv2 = tf.keras.layers.Conv2D(filters=64, kernel_size=3, strides=1, padding='same', activation='relu')(pool1)\n    conv2 = tf.keras.layers.BatchNormalization()(conv2)\n    conv2 = tf.keras.layers.Conv2D(filters=64, kernel_size=3, strides=1, padding='same', activation='relu')(conv2)\n    conv2 = tf.keras.layers.BatchNormalization()(conv2)\n    pool2 = tf.keras.layers.MaxPool2D(pool_size=2, strides=2, padding='same')(conv2)\n\n    # Dense layer\n    flat = tf.keras.layers.Flatten()(pool2)\n    dense = tf.keras.layers.Dense(units=128, activation='relu')(flat)\n    dense = tf.keras.layers.Dropout(rate=0.5)(dense)\n    logits = tf.keras.layers.Dense(units=num_classes, activation='softmax')(dense)\n\n    model = tf.keras.Model(inputs=x, outputs=logits)\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n    return model\n\n", "CodeBleu_score": 22.73302844190735}]}
{"instance_id": "813e3e2844f4", "generation_result": [{"code": "def extract_bboxes(mask)\n\t\n    bboxes = []\n    for i in range(mask.shape[2]):\n        m = mask[:, :, i]\n        # Bounding box.\n        horizontal_indicies = np.where(np.any(m, axis=0))[0]\n        vertical_indicies = np.where(np.any(m, axis=1))[0]\n        if horizontal_indicies.shape[0]:\n            x1, x2 = horizontal_indicies[[0, -1]]\n            y1, y2 = vertical_indicies[[0, -1]]\n            # x2 and y2 should not be part of the box. Increment by 1.\n            x2 += 1\n            y2 += 1\n        else:\n            # No mask for this layer. Might happen due to\n            # resizing or cropping. Set bbox to zeros\n            x1, x2, y1, y2 = 0, 0, 0, 0\n        bboxes.append([x1, x2, y1, y2])\n    return np.array(bboxes).astype(np.int32)\n\n", "CodeBleu_score": 64.70551253000752}, {"code": "def extract_bboxes(mask)\n\t\n    bboxes = []\n    for i in range(mask.shape[2]):\n        m = mask[:, :, i]\n        horizontal_indicies = np.where(np.any(m, axis=0))[0]\n        vertical_indicies = np.where(np.any(m, axis=1))[0]\n        if horizontal_indicies.shape[0]:\n            x1, x2 = horizontal_indicies[[0, -1]]\n            y1, y2 = vertical_indicies[[0, -1]]\n            # x2 and y2 should not be part of the box. Increment by 1.\n            x2 += 1\n            y2 += 1\n        else:\n            # No mask for this layer, therefore set bbox to zeros\n            x1, x2, y1, y2 = 0, 0, 0, 0\n        bboxes.append([x1, x2, y1, y2])\n    return bboxes\n\n", "CodeBleu_score": 64.69936300047173}, {"code": "def extract_bboxes(mask)\n\t\n    mask = np.asarray(mask)\n    if len(mask.shape) == 3:\n        mask = np.squeeze(mask)\n    # Bounding boxes\n    num_masks = mask.shape[-1]\n    bboxes = np.zeros([num_masks, 4], dtype=np.int32)\n    for i in range(num_masks):\n        m = mask[:, :, i]\n        # Bounding box.\n        horizontal_indicies = np.where(np.any(m, axis=0))[0]\n        vertical_indicies = np.where(np.any(m, axis=1))[0]\n        if horizontal_indicies.shape[0]:\n            x1, x2 = horizontal_indicies[[0, -1]]\n            y1, y2 = vertical_indicies[[0, -1]]\n            # x2 and y2 should not be part of the box. Increment by 1.\n            x2 += 1\n            y2 += 1\n        else:\n            # No mask for this instance. Might happen due to\n            # resizing or cropping. Set bbox to zeros\n            x1, x2, y1, y2 = 0, 0, 0, 0\n        bboxes[i] = np.array([y1, x1, y2, x2])\n    return bboxes.astype(np.int32)\n\n", "CodeBleu_score": 72.67427525538554}]}
{"instance_id": "7ad5d430d6ed", "generation_result": [{"code": "def compute_iou(box, boxes, box_area, boxes_area)\n\t\n    # Calculate intersection areas\n    x1 = np.maximum(box[0], boxes[:, 0])\n    x2 = np.minimum(box[2], boxes[:, 2])\n    y1 = np.maximum(box[1], boxes[:, 1])\n    y2 = np.minimum(box[3], boxes[:, 3])\n    intersection = np.maximum(x2 - x1, 0) * np.maximum(y2 - y1, 0)\n    union = box_area + boxes_area[:] - intersection[:]\n    iou = intersection / union\n    return iou\n\n", "CodeBleu_score": 54.70904949138438}, {"code": "def compute_iou(box, boxes, box_area, boxes_area)\n\t\n    # compute intersection areas\n    y1 = np.maximum(box[0], boxes[:, 0])\n    y2 = np.minimum(box[2], boxes[:, 2])\n    x1 = np.maximum(box[1], boxes[:, 1])\n    x2 = np.minimum(box[3], boxes[:, 3])\n    intersection = np.maximum(x2 - x1, 0) * np.maximum(y2 - y1, 0)\n    union = box_area + boxes_area[:] - intersection[:]\n    iou = intersection / union\n    return iou\n\n", "CodeBleu_score": 58.663259083089194}, {"code": "def compute_iou(box, boxes, box_area, boxes_area)\n\t\n    intersection = minimum(box[2], boxes[:, 2]) * minimum(box[3], boxes[:, 3])\n    union = box_area + boxes_area[:] - intersection[:]\n    iou = intersection / union\n    return iou\n\n", "CodeBleu_score": 36.69864899094223}]}
{"instance_id": "15941b852859", "generation_result": [{"code": "def compute_overlaps(boxes1, boxes2)\n\t\n    # Areas of anchors and GT boxes\n    area1 = (boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1])\n    area2 = (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1])\n\n    # Compute overlaps to generate matrix [boxes1 count, boxes2 count]\n    # Each cell contains the IoU value.\n    overlaps = np.zeros((boxes1.shape[0], boxes2.shape[0]))\n    for i in range(overlaps.shape[1]):\n        box2 = boxes2[i]\n        overlaps[:, i] = compute_iou(box2, boxes1, area2[i], area1)\n    return overlaps\n\n", "CodeBleu_score": 62.2853282032944}, {"code": "def compute_overlaps(boxes1, boxes2)\n\t\n    # Areas of anchors and GT boxes\n    area1 = (boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1])\n    area2 = (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1])\n\n    # Compute overlaps to generate matrix [boxes1 count, boxes2 count]\n    # Each cell contains the IoU value.\n    overlaps = np.zeros((boxes1.shape[0], boxes2.shape[0]))\n    for i in range(overlaps.shape[1]):\n        box2 = boxes2[i]\n        overlaps[:, i] = compute_iou(box2, boxes1, area2[i], area1)\n    return overlaps\n\n", "CodeBleu_score": 62.2853282032944}, {"code": "def compute_overlaps(boxes1, boxes2)\n\t\n    # Areas of anchors and GT boxes\n    area1 = (boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1])\n    area2 = (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1])\n\n    # Compute overlaps to generate matrix [boxes1 count, boxes2 count]\n    # Each cell contains the IoU value.\n    overlaps = np.zeros((boxes1.shape[0], boxes2.shape[0]))\n    for i in range(overlaps.shape[1]):\n        box2 = boxes2[i]\n        overlaps[:, i] = compute_iou(box2, boxes1, area2[i], area1)\n    return overlaps\n\n", "CodeBleu_score": 62.2853282032944}]}
{"instance_id": "597533722ce3", "generation_result": [{"code": "def load_image(self, image_id):\n\t\n    # Load image\n    image = skimage.io.imread(self.image_info[image_id]['path'])\n    # If grayscale. Convert to RGB for consistency.\n    if image.ndim != 3:\n        image = skimage.color.gray2rgb(image)\n    # If has an alpha channel, remove it for consistency\n    if image.shape[-1] == 4:\n        image = image[..., :3]\n    return image\n\n", "CodeBleu_score": 100.0}, {"code": "def load_image(self, image_id):\n\t\n    # Load image\n    image = skimage.io.imread(self.image_info[image_id]['path'])\n    # If grayscale. Convert to RGB for consistency.\n    if image.ndim != 3:\n        image = skimage.color.gray2rgb(image)\n    # If has an alpha channel, remove it for consistency\n    if image.shape[-1] == 4:\n        image = image[..., :3]\n    return image\n\n", "CodeBleu_score": 100.0}, {"code": "def load_image(self, image_id):\n\t\n    # Load the image\n    image = skimage.io.imread(self.image_info[image_id]['path'])\n    # If grayscale. Convert to RGB for consistency.\n    if image.ndim != 3:\n        image = skimage.color.gray2rgb(image)\n    # If has an alpha channel, remove it for consistency\n    if image.shape[-1] == 4:\n        image = image[..., :3]\n    return image\n\n", "CodeBleu_score": 100.0}]}
{"instance_id": "9b852997ca05", "generation_result": [{"code": "def display_images(images, titles=None, cols=4, cmap=None, norm=None,\n                   interpolation=None)\n\t\n    if titles is None:\n        titles = ['Image (%d)' % i for i in range(1, len(images) + 1)]\n    rows = len(images) // cols + 1\n    plt.figure(figsize=(14, 14 * rows // cols))\n    i = 1\n    for image, title in zip(images, titles):\n        plt.subplot(rows, cols, i)\n        plt.title(title, fontsize=9)\n        plt.axis('off')\n        plt.imshow(image, cmap=cmap, norm=norm, interpolation=interpolation)\n        i += 1\n    plt.show()\n\n", "CodeBleu_score": 74.59094578616218}, {"code": "def display_images(images, titles=None, cols=4, cmap=None, norm=None,\n                   interpolation=None)\n\t\n    plt.ioff()\n    n_images = len(images)\n    if titles is None:\n        titles = ['Image (%d)' % i for i in range(1, n_images + 1)]\n    fig = plt.figure()\n    for n, (image, title) in enumerate(zip(images, titles)):\n        a = fig.add_subplot(cols, np.ceil(n_images / float(cols)), n + 1)\n        if image.ndim == 2:\n            plt.gray()\n        plt.imshow(image, cmap=cmap, norm=norm, interpolation=interpolation)\n        a.set_title(title)\n    fig.set_size_inches(np.array(fig.get_size_inches()) * n_images)\n    plt.show()\n\n", "CodeBleu_score": 37.088850633777156}, {"code": "def display_images(images, titles=None, cols=4, cmap=None, norm=None,\n                   interpolation=None)\n\t\n    if type(images) is not list:\n        images = [images]\n    n_images = len(images)\n    if titles is None:\n        titles = ['Image (%d)' % i for i in range(1, n_images + 1)]\n    fig = plt.figure()\n    for n, (image, title) in enumerate(zip(images, titles)):\n        a = fig.add_subplot(cols, np.ceil(n_images / float(cols)), n + 1)\n        if image.ndim == 2:\n            plt.gray()\n        plt.imshow(image, cmap=cmap, norm=norm, interpolation=interpolation)\n        a.set_title(title)\n    fig.set_size_inches(np.array(fig.get_size_inches()) * n_images)\n    plt.show()\n\n", "CodeBleu_score": 39.50370076687163}]}
