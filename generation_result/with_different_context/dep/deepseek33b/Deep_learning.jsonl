{"instance_id": "1f9f4d85e825", "generation_result": [{"code": "def load_model_from_config(self, half_attention)\n\t\n\n    # Load the model from the cache if it is available and valid\n    cached_model_path = self.get_cached_model_path()\n    if cached_model_path is not None:\n        try:\n            model = load_model(cached_model_path, self.device, half_attention, self.max_batch_size, self.max_image_size, self.max_vram_usage_in_gb)\n            self.logger.info(f\"Loaded model from cache: {cached_model_path}\")\n            return model\n        except Exception as e:\n            self.logger.warning(f\"Failed to load model from cache: {cached_model_path}. Error: {e}\")\n\n    # Load the model from the specified path\n    model_path = self.model_path\n    if model_path is None:\n        raise ValueError(\"Model path is not set.\")\n\n    if model_path.endswith(\".safetensors\"):\n        # Load the model from a safetensors file\n        model = load_model_from_safetensors(model_path, self.device, half_attention, self.max_batch_size, self.max_image_size, self.max_vram_usage_in_gb)\n    else:\n        # Load the model from a PyTorch-compatible file format\n        model = load_model_from_file(model_path, self.device, half_attention, self.max_batch_size, self.max_image_size, self.max_vram_usage_in_gb)\n\n    # Configure the model using settings from the YAML file\n    model = configure_model(model, self.model_config)\n\n    # Move the model to the appropriate device\n    model.to(self.device)\n\n    # Optionally, use half-precision attention\n    if half_attention:\n        model = model.half()\n\n    # Optionally, optimize the memory format of the model\n    if self.optimize_memory_format:\n        model = optimize_memory_format(model)\n\n    # Put the model in evaluation mode\n    model.eval()\n\n   ", "CodeBleu_score": 36.848070338788055}, {"code": "def load_model_from_config(self, half_attention)\n\t\n    # Check if a cached model is available and valid\n    if self.use_cache and os.path.exists(self.cache_path):\n        # Load the cached model\n        model = torch.load(self.cache_path)\n        # Check if the cached model is valid\n        if self.validate_model(model):\n            # Move the model to the appropriate device\n            model = model.to(self.device)\n            # Set the model to evaluation mode\n            model.eval()\n            # Return the loaded model\n            return {\"model\": model}\n\n    # Load the model from the specified path\n    if self.model_path.endswith(\".safetensors\"):\n        # Load the model from a safetensors file\n        model = safe_open(self.model_path, framework=\"pt\")\n    else:\n        # Load the model from a PyTorch-compatible file format\n        model = torch.load(self.model_path)\n\n    # Configure the model using settings from a YAML file\n    self.configure_model(model)\n\n    # Move the model to the appropriate device\n    model = model.to(self.device)\n\n    # Set the model to evaluation mode\n    model.eval()\n\n    # Optionally cache the loaded model for future use\n    if self.use_cache:\n        torch.save(model, self.cache_path)\n\n    # Return the loaded model\n    return {\"model\": model}\n\n", "CodeBleu_score": 25.962973237471154}, {"code": "def load_model_from_config(self, half_attention)\n\t\n\n    # Check if the model is already cached and valid\n    if self.cache_model:\n        cached_model = self.load_cached_model(half_attention)\n        if cached_model:\n            return cached_model\n\n    # Load the model from the specified path\n    if self.model_path.endswith(\".safetensors\"):\n        model = load_file(self.model_path, device=self.device)\n    else:\n        model = load_file(self.model_path, device=self.device, load_in_8bit=half_attention)\n\n    # Configure the model using settings from the YAML file\n    model_config = load_yaml(self.model_config_path)\n    model_config = model_config.get(\"model\", {})\n    for key, value in model_config.items():\n        setattr(model, key, value)\n\n    # Move the model to the appropriate device\n    model.to(self.device)\n\n    # Optionally enable half-precision attention\n    if half_attention:\n        model.enable_attention_slicing()\n\n    # Optionally optimize the memory format of the model\n    if self.optimize_memory_format:\n        model.half().cuda()\n\n    # Ensure the model is in evaluation mode\n    model.eval()\n\n    # Cache the loaded model for future use\n    if self.cache_model:\n        self.cache_loaded_model(model)\n\n    # Return the loaded model\n    return {\"model\": model}\n\n", "CodeBleu_score": 31.883276622371948}]}
{"instance_id": "8dc955007aa2", "generation_result": [{"code": "def convsample_ddim(model, cond, steps, shape, eta=1.0, callback=None, normals_sequence=None,\n                    mask=None, x0=None, quantize_x0=False, temperature=1., score_corrector=None,\n                    corrector_kwargs=None, x_t=None\n                    )\n\t\n    ddim = DDIMSampler(model)\n    bs = shape[0]\n    print(f'eta {eta}, steps {steps}')\n    samples, intermediates = ddim.sample(steps, batch_size=bs, shape=shape,\n                                          conditioning=cond, eta=eta, callback=callback,\n                                          normals_sequence=normals_sequence, mask=mask, x0=x0,\n                                          quantize_x0=quantize_x0, temperature=temperature,\n                                          score_corrector=score_corrector, corrector_kwargs=corrector_kwargs, x_t=x_t)\n    return samples, intermediates\n\n\n", "CodeBleu_score": 67.33042073446873}, {"code": "def convsample_ddim(model, cond, steps, shape, eta=1.0, callback=None, normals_sequence=None,\n                    mask=None, x0=None, quantize_x0=False, temperature=1., score_corrector=None,\n                    corrector_kwargs=None, x_t=None\n                    )\n\t\n    from guided_diffusion.guided_diffusion import DDIMSampler\n    bs = shape[0]\n    print(f'eta: {eta} steps: {steps}')\n    sampler = DDIMSampler(model)\n    return sampler.sample(steps, batch_size=bs, shape=shape, conditioning=cond, eta=eta, callback=callback,\n                           normals_sequence=normals_sequence, mask=mask, x0=x0, quantize_x0=quantize_x0,\n                           temperature=temperature, score_corrector=score_corrector, corrector_kwargs=corrector_kwargs,\n                           x_T=x_t)\n\n\n\n", "CodeBleu_score": 47.716751187167496}, {"code": "def convsample_ddim(model, cond, steps, shape, eta=1.0, callback=None, normals_sequence=None,\n                    mask=None, x0=None, quantize_x0=False, temperature=1., score_corrector=None,\n                    corrector_kwargs=None, x_t=None\n                    )\n\t\n    ddim = DDIMSampler(model)\n    bs = shape[0]\n    print(f\"Beginning sampling with eta {eta} and {steps} steps\")\n    samples, intermediates = ddim.sample(steps, bs, shape,\n                                          conditioning=cond,\n                                          eta=eta, callback=callback,\n                                          normals_sequence=normals_sequence,\n                                          mask=mask, x0=x0,\n                                          quantize_x0=quantize_x0,\n                                          temperature=temperature,\n                                          score_corrector=score_corrector,\n                                          corrector_kwargs=corrector_kwargs,\n                                          x_T=x_t)\n    return samples, intermediates\n\n", "CodeBleu_score": 55.63102358309293}]}
{"instance_id": "9bdcd9081480", "generation_result": [{"code": "def ema_scope(self, context=None)\n\t\n    if context is not None:\n        print(f\"{context}: switching to EMA weights\")\n    self.apply_ema_weights()\n    if context is not None:\n        print(f\"{context}: restored model weights from EMA\")\n    yield\n    if context is not None:\n        print(f\"{context}: switching to non-EMA weights\")\n    self.apply_model_weights()\n    if context is not None:\n        print(f\"{context}: restored model weights from non-EMA weights\")\n\n\n\n", "CodeBleu_score": 49.9839038365695}, {"code": "def ema_scope(self, context=None)\n\t\n    if self.ema_weights is None:\n        self.ema_weights = self.get_weights()\n    if context is not None:\n        print(f\"{context}: Switching to EMA weights\")\n    self.set_weights(self.ema_weights)\n    if context is not None:\n        print(f\"{context}: Restoring original weights\")\n    self.set_weights(self.original_weights)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 49.68018991435775}, {"code": "def ema_scope(self, context=None)\n\t\n    if self.ema_weights is None:\n        return self\n    if context is not None:\n        print(f\"{context} - Switching to EMA weights\")\n    self.ema_weights.restore()\n    if context is not None:\n        print(f\"{context} - Restored original weights\")\n    return self\n\n", "CodeBleu_score": 45.04363965870112}]}
{"instance_id": "14ab009e1c5c", "generation_result": [{"code": "def init_from_ckpt(self, path, ignore_keys=None)\n\t\n    state_dict = torch.load(path, map_location=\"cpu\")[\"state_dict\"]\n    if ignore_keys:\n        for k in list(state_dict.keys()):\n            if any([k.startswith(i) for i in ignore_keys]):\n                del state_dict[k]\n    self.load_state_dict(state_dict, strict=False)\n    print(\"Loaded state_dict from\", path)\n    missing_keys, unexpected_keys = self.load_state_dict(state_dict, strict=False)\n    if missing_keys:\n        print(\"missing keys:\")\n        print(missing_keys)\n    if unexpected_keys:\n        print(\"unexpected keys:\")\n        print(unexpected_keys)", "CodeBleu_score": 36.88663506841986}, {"code": "def init_from_ckpt(self, path, ignore_keys=None)\n\t\n    state_dict = torch.load(path, map_location='cpu')\n    if ignore_keys:\n        for key in ignore_keys:\n            state_dict.pop(key)\n    self.load_state_dict(state_dict, strict=False)\n    del state_dict\n    print(f'Loaded {path}')\n\n\n\n", "CodeBleu_score": 18.236899800430972}, {"code": "def init_from_ckpt(self, path, ignore_keys=None)\n\t\n    if ignore_keys is None:\n        ignore_keys = []\n    state_dict = torch.load(path, map_location='cpu')\n    for k in list(state_dict.keys()):\n        if k in ignore_keys:\n            del state_dict[k]\n    self.load_state_dict(state_dict, strict=False)\n    print('Loaded checkpoint from {}'.format(path))\n    missing_keys, unexpected_keys = self.load_state_dict(state_dict, strict=False)\n    if len(missing_keys) > 0:\n        print('Missing keys: {}'.format(missing_keys))\n    if len(unexpected_keys) > 0:\n        print('Unexpected keys: {}'.format(unexpected_keys))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 41.1177780983618}]}
{"instance_id": "754176e27b81", "generation_result": [{"code": "def forward(self, input, return_pred_indices=False)\n\t\n    quant, diff, idxs = self.encode(input)\n    dec = self.decode(quant)\n    if return_pred_indices:\n        return dec, diff, idxs\n    return dec, diff\n\n", "CodeBleu_score": 53.91721711229367}, {"code": "def forward(self, input, return_pred_indices=False)\n\t\n    quant, diff, idxs = self.encode(input)\n    dec = self.decode(quant)\n    if return_pred_indices:\n        return dec, diff, idxs\n    return dec, diff\n\n", "CodeBleu_score": 53.91721711229367}, {"code": "def forward(self, input, return_pred_indices=False)\n\t\n    quant, diff, _ = self.encode(input)\n    dec = self.decode(quant)\n    if return_pred_indices:\n        return dec, diff, torch.argmax(F.softmax(dec, dim=1), dim=1)\n    return dec, diff\n\n", "CodeBleu_score": 49.2190349031619}]}
{"instance_id": "443df6fdaf0d", "generation_result": [{"code": "def get_input(self, batch, k)\n\t\n    input_tensor = batch[k]\n    while len(input_tensor.shape) < 4:\n        input_tensor = input_tensor[:, :, :, None]\n    input_tensor = input_tensor.permute(0, 2, 3, 1)\n    input_tensor = input_tensor.contiguous()\n    if input_tensor.shape[-1] == 1:\n        input_tensor = input_tensor[:, :, :, 0]\n    if self.batch_resizing:\n        input_tensor = resize_fn(input_tensor, oshape=input_tensor.shape[1:3], ishape=input_tensor.shape[1:3], xishape=(self.cur_step - self.resizing_steps // 2, self.cur_step + self.resizing_steps // 2), yishape=(self.cur_step - self.resizing_steps // 2, self.cur_step + self.resizing_steps // 2))\n    return input_tensor.detach()\n\n", "CodeBleu_score": 29.46611340067348}, {"code": "def get_input(self, batch, k)\n\t\n    if isinstance(batch, dict):\n        x = batch[k]\n    else:\n        x = batch\n    if x.dim() == 3:\n        x = x[..., None]\n    if x.size(2) != self.image_size[0] or x.size(3) != self.image_size[1]:\n        x = F.interpolate(x, size=self.image_size, mode=\"bicubic\", align_corners=False)\n    x = x.permute(0, 2, 3, 1).contiguous()\n    if self.resize_n_patch:\n        x = self.random_resize_crop(x)\n    x = x.detach()\n    return x\n\n", "CodeBleu_score": 43.022342446612484}, {"code": "def get_input(self, batch, k)\n\t\n    if isinstance(batch, dict):\n        input_batch = batch[k]\n    else:\n        input_batch = batch\n    if len(input_batch.shape) == 3:\n        input_batch = input_batch[None, ...]\n    if input_batch.shape[2] < 8 or input_batch.shape[3] < 8:\n        logger.warning(f'Input image is too small {input_batch.shape}. Use bicubic instead')\n        input_batch = torch.nn.functional.interpolate(input_batch, size=(input_batch.shape[2] * 4, input_batch.shape[3] * 4), mode='bicubic', align_corners=False)\n    input_batch = input_batch.permute(0, 3, 1, 2).contiguous()\n    if self.training and self.batch_resizing:\n        resize_amount = self.resize_amounts[self.cur_step]\n        input_batch = torch.nn.functional.interpolate(input_batch, size=(int(input_batch.shape[2] * (1 + resize_amount)), int(input_batch.shape[3] * (1 + resize_amount))), mode='bicubic', align_corners=False)\n    return input_batch.detach()\n\n", "CodeBleu_score": 38.71125932197723}]}
{"instance_id": "953a2632fc15", "generation_result": [{"code": "def training_step(self, batch, batch_idx, optimizer_idx)\n\t\n    x = self.get_input(batch, self.image_key)\n    xrec, posterior = self.p_losses(x, return_pred=True)\n    if optimizer_idx == 0:\n        # autoencode\n        aeloss, log_dict_ae = self.ae_loss(x, xrec, posterior)\n        self.log(\"aeloss\", aeloss, prog_bar=True, logger=True, on_step=True, on_epoch=True)\n        return aeloss\n    if optimizer_idx == 1:\n        # update discriminator\n        discloss, log_dict_disc = self.dis_loss(x, xrec, posterior)\n        self.log(\"discloss\", discloss, prog_bar=True, logger=True, on_step=True, on_epoch=True)\n        return discloss\n\n", "CodeBleu_score": 39.54184818235845}, {"code": "def training_step(self, batch, batch_idx, optimizer_idx)\n\t\n    x = self.get_input(batch, self.image_key)\n    with autocast(enabled=self.fp16):\n        if optimizer_idx == 0:\n            # autoencode\n            xrec, qloss = self(x, return_q_loss=True)\n            aeloss, log_dict_ae = self.autoencoder_loss(x, xrec, qloss)\n            output = aeloss\n            log_dict = log_dict_ae\n        elif optimizer_idx == 1:\n            # update discriminator\n            dis_loss, dis_loss_dict = self.discriminator_loss(x)\n            output = dis_loss\n            log_dict = dis_loss_dict\n        else:\n            raise ValueError(f\"Unexpected optimizer index: {optimizer_idx}\")\n\n    if self.use_ema:\n        with torch.no_grad():\n            decay = self.ema_decay\n            if self.global_step < 1000:\n                decay = 0.99\n            self.model_ema.ema_model.update(self.model.parameters(), decay=decay)\n\n    if isinstance(output, dict):\n        log_dict.update(**output)\n    else:\n        log_dict[\"loss\"] = output\n\n    self.log_dict(log_dict, prog_bar=True, logger=True, on_step=True, on_epoch=True)\n    return output\n\n", "CodeBleu_score": 39.77160330424236}, {"code": "def training_step(self, batch, batch_idx, optimizer_idx)\n\t\n    x = self.get_input(batch, self.image_key)\n    xrec, qloss = self.autoencode(x)\n    if optimizer_idx == 0:\n        # autoencode\n        aeloss, log_dict_ae = self.loss(qloss, x, xrec, optimizer_idx, self.global_step,\n                                        last_layer=self.get_last_layer(), split=\"train\")\n        self.log(\"aeloss\", aeloss, prog_bar=True, logger=True, on_step=True, on_epoch=True)\n        self.log_dict(log_dict_ae, prog_bar=False, logger=True, on_step=True, on_epoch=False)\n        return aeloss\n\n    if optimizer_idx == 1:\n        # update discriminator\n        dis_loss, log_dict_dis = self.loss(qloss, x, xrec, optimizer_idx, self.global_step,\n                                            last_layer=self.get_last_layer(), split=\"train\")\n        self.log(\"dis_loss\", dis_loss, prog_bar=True, logger=True, on_step=True, on_epoch=True)\n        self.log_dict(log_dict_dis, prog_bar=False, logger=True, on_step=True, on_epoch=False)\n        return dis_loss\n\n", "CodeBleu_score": 59.62335409241185}]}
{"instance_id": "b449b5e8b77f", "generation_result": [{"code": "def log_images(self, batch, only_inputs=False, plot_ema=False, **kwargs)\n\t\n    inputs = self.get_input(batch, self.image_key)\n    reconstructions = self.get_last_output()\n    if only_inputs:\n        images_to_log = inputs\n    else:\n        images_to_log = torch.stack([inputs, reconstructions], dim=1).flatten(0,1)\n    if self.ema_start is not None:\n        if plot_ema:\n            ema_reconstructions = self.ema_model.get_reconstruction(inputs)\n            images_to_log = torch.cat([images_to_log, ema_reconstructions], dim=0)\n        if self.global_step < self.ema_start:\n            images_to_log = images_to_log[:images_to_log.shape[0]//2]\n    if images_to_log.shape[1] == 3:\n        images_to_log = self.to_rgb(images_to_log)\n    log_dict = {f\"images/{k}\": v for k, v in enumerate(images_to_log)}\n    self.logger.log_image(key=\"sample\", images=log_dict, global_step=self.global_step)\n", "CodeBleu_score": 34.727380183859566}, {"code": "def log_images(self, batch, only_inputs=False, plot_ema=False, **kwargs)\n\t\n    x = self.get_input(batch, self.image_key)\n    if self.ema_start is not None:\n        if plot_ema:\n            x_ema = self.get_input(batch, self.image_key)\n            x_ema = self.ema_model.get_reconstruction(x_ema, sampling=True, return_dict=False)[0]\n            x_ema = self.to_rgb(x_ema)\n            x_ema = x_ema.clamp(-1, 1)\n            x_ema = (x_ema + 1.) / 2.\n            x_ema = x_ema.cpu().numpy()\n            x_ema = np.transpose(x_ema, (0, 2, 3, 1))\n            x_ema = np.clip(x_ema, 0, 1)\n            x_ema = x_ema * 255\n            x_ema = x_ema.astype(np.uint8)\n            x_ema = np.squeeze(x_ema)\n            self.log_images(x_ema, name=\"%s_ema\" % self.image_key, step=self.global_step)\n        if only_inputs:\n            return\n    x = self.to_rgb(x)\n    x = x.clamp(-1, 1)\n    x = (x + 1.) / 2.\n    x = x.cpu().numpy()\n    x = np.transpose(x, (0, 2, 3, 1))\n    x = np.clip(x, 0, 1)\n    x = x * 255\n    x = x.astype(np.uint8)\n    x = np.squeeze(x)\n    self.log_images(x, name=self.image_key, step=self.global_step)\n\n", "CodeBleu_score": 34.15481381921091}, {"code": "def log_images(self, batch, only_inputs=False, plot_ema=False, **kwargs)\n\t\n    log = dict()\n\n    def process(x):\n        \"\"\"Process input data for logging.\"\"\"\n        if len(x.shape) > 4:\n            x = x.squeeze(1)\n        if x.shape[1] > 3:\n            # for multi-channel images, assume the 1st channel is\n            # meant to be ignored for colorization\n            x = x[:, 1:]\n        if x.shape[1] == 3:\n            # for single-channel images, apply colorization\n            x = self.to_rgb(x)\n        return x\n\n    x = self.get_input(batch, self.image_key).clamp(-1., 1.)\n    x = process(x)\n    if only_inputs:\n        log[\"inputs\"] = x\n    else:\n        log[\"reconstructions\"] = x\n\n    if plot_ema:\n        with self.ema.average_parameters():\n            xrec, _ = self.forward(**batch)\n            log[\"reconstructions_ema\"] = process(xrec)\n\n    return log", "CodeBleu_score": 52.99835752803346}]}
{"instance_id": "540d089e3481", "generation_result": [{"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss_type == 'l1':\n        loss = F.l1_loss(pred, target, reduction='none')\n        if mean:\n            loss = loss.mean()\n    elif self.loss_type == 'l2':\n        loss = F.mse_loss(pred, target, reduction='none')\n        if mean:\n            loss = loss.mean()\n    else:\n        raise NotImplementedError(\n            'Loss type {} is not recognized.'.format(self.loss_type))\n    return loss\n\n", "CodeBleu_score": 57.15991922081077}, {"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss_type == 'l1':\n        loss = F.l1_loss(pred, target, reduction='none')\n        if mean:\n            loss = loss.mean()\n    elif self.loss_type == 'l2':\n        loss = F.mse_loss(pred, target, reduction='none')\n        if mean:\n            loss = loss.mean()\n    else:\n        raise Exception(\"unknown loss type\")\n    return loss", "CodeBleu_score": 56.684682498327945}, {"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss_type == 'l1':\n        loss = F.l1_loss(pred, target, reduction='none')\n        if mean:\n            loss = loss.mean()\n    elif self.loss_type == 'l2':\n        loss = F.mse_loss(pred, target, reduction='none')\n        if mean:\n            loss = loss.mean()\n    else:\n        raise Exception(\"unknown loss type\")\n    return loss", "CodeBleu_score": 56.684682498327945}]}
{"instance_id": "63a08283666f", "generation_result": [{"code": "def forward(self, x, c, *args, **kwargs)\n\t\n    b, c, h, w = x.shape\n    t = torch.randint(0, self.num_timesteps, (b,), device=x.device).long()\n\n    if self.cond_stage_model is not None:\n        c = self.get_learned_conditioning(c)\n\n    if self.model.conditioning_key is not None:\n        assert c is not None\n        c = dict(c=c)\n\n    if self.model.conditioning_key is None:\n        x_cond = x\n    else:\n        x_cond = torch.cat([x, c['c']], dim=1)\n\n    model_output = self.model(x_cond, t)\n    if self.model.parameterization == \"x0\":\n        output = model_output\n    elif self.model.parameterization == \"eps\":\n        output = model_output + (x - self.sqrt_one_minus_alphas_cumprod[t] * noise) / self.sqrt_alphas_cumprod[t]\n    else:\n        raise ValueError(f\"Unknown parameterization {self.model.parameterization}\")\n\n    if self.loss_type == 'kl':\n        losses = self.p_losses(model_output, x, t, c)\n    elif self.loss_type == 'mse':\n        losses = self.mse_loss(model_output, x)\n    else:\n        raise ValueError(f\"Unknown loss type {self.loss_type}\")\n\n    return output, losses\n\n\n\n", "CodeBleu_score": 49.4373552057929}, {"code": "def forward(self, x, c, *args, **kwargs)\n\t\n    b, c_, h, w = x.shape\n    t = torch.randint(0, self.num_timesteps, (b,), device=x.device).long()\n\n    # if conditioning on class labels\n    if c is not None:\n        assert c.shape == (b, self.num_classes), \"class labels must have shape (batch_size, num_classes)\"\n        cond = self.get_learned_conditioning(c)\n    else:\n        cond = None\n\n    return self.p_losses(x, t, cond)\n\n", "CodeBleu_score": 30.522283488530817}, {"code": "def forward(self, x, c, *args, **kwargs)\n\t\n    if isinstance(c, int):\n        c = torch.ones(x.shape[0], device=x.device) * c\n    t = torch.randint(0, self.num_timesteps, (x.shape[0],), device=x.device).long()\n    if self.cond_stage_model is not None:\n        c = self.get_learned_conditioning(c)\n    return self.p_losses(x, t, *args, c=c, **kwargs)\n", "CodeBleu_score": 44.037779121185146}]}
{"instance_id": "4627e1e8ed62", "generation_result": [{"code": "def get_codebook_entry(self, indices, shape)\n\t\n    # shape spec [batch, height, width, channel]\n    if self.remap is not None:\n        indices = self.unmap_to_all(indices)\n    # get quantized latent vectors\n    z_q = self.embedding(indices)\n\n    if shape is not None:\n        z_q = z_q.view(shape)\n        # reshape back to match original input shape\n        z_q = z_q.permute(0, 3, 1, 2).contiguous()\n\n    return z_q\n\n\n", "CodeBleu_score": 61.21094770993108}, {"code": "def get_codebook_entry(self, indices, shape)\n\t\n    # shape spec [batch, height, width, channel]\n    if self.remap_used is not None:\n        indices = indices.clone()\n        indices[indices >= 0] = self.remap_used[indices[indices >= 0]]\n    # gather\n    z = self.embedding(indices)\n\n    if shape is not None:\n        z = z.view([z.shape[0]] + list(shape))\n    return z\n", "CodeBleu_score": 48.62621793265719}, {"code": "def get_codebook_entry(self, indices, shape)\n\t\n    # shape spec [batch, height, width, channel]\n    if self.remap_used is not None:\n        indices = self.unmap_to_used(indices)\n    if self.permuter is not None:\n        indices = self.permuter(indices)\n    bhwc = [indices.shape[0], *shape]\n    return self.codebook(indices).view(bhwc)\n\n\n", "CodeBleu_score": 36.9776664273717}]}
{"instance_id": "c55d014827ef", "generation_result": [{"code": "def factorization(dimension: int, factor:int=-1) -> tuple[int, int]\n\t\n    if factor > 0 and dimension % factor == 0:\n        return factor, dimension // factor\n    else:\n        if factor > 0:\n            print(f\"The factor {factor} does not divide the dimension {dimension}. The factor will be ignored.\")\n        m = dimension // 2\n        n = dimension // m\n        while m * n != dimension or m > n or (factor > 0 and m > factor):\n            m -= 1\n            n = dimension // m\n        return m, n\n\n\n\n\n", "CodeBleu_score": 38.188482126041244}, {"code": "def factorization(dimension: int, factor:int=-1) -> tuple[int, int]\n\t\n    if factor > 0 and dimension % factor == 0:\n        return factor, dimension // factor\n    else:\n        m = dimension // 2\n        n = dimension - m\n        while m > n or (factor > 0 and m > factor):\n            m -= 1\n            n += 1\n        return m, n\n\n", "CodeBleu_score": 27.984586815287148}, {"code": "def factorization(dimension: int, factor:int=-1) -> tuple[int, int]\n\t\n    if factor > 0 and dimension % factor == 0:\n        return factor, dimension // factor\n    else:\n        if factor < 0:\n            factor = dimension\n        for i in range(1, factor):\n            if dimension % i == 0:\n                j = dimension // i\n                if i <= j and i * j == dimension:\n                    return i, j\n    return dimension, 1\n\n\n\n\n\n", "CodeBleu_score": 33.391933940793145}]}
{"instance_id": "5d613ec26036", "generation_result": [{"code": "def apply_weight_decompose(self, updown, orig_weight)\n\t\n    (orig_weight, updown) = (orig_weight.to(self.device), updown.to(self.device))\n    if (orig_weight.dtype != updown.dtype):\n        updown = updown.to(orig_weight.dtype)\n    (orig_weight, updown) = (orig_weight.cpu(), updown.cpu())\n    updown = (updown * self.scale)\n    (orig_weight, updown) = (orig_weight.type(torch.float64), updown.type(torch.float64))\n    combined_weight = (orig_weight + updown)\n    max_weight = torch.max(torch.abs(combined_weight))\n    combined_weight = (combined_weight / max_weight)\n    adjusted_weight = (combined_weight - orig_weight)\n    return adjusted_weight", "CodeBleu_score": 31.425328848455525}, {"code": "def apply_weight_decompose(self, updown, orig_weight)\n\t\n    (orig_weight, updown) = (orig_weight.to(self.device), updown.to(self.device))\n    if (orig_weight.dtype != updown.dtype):\n        updown = updown.to(orig_weight.dtype)\n    (orig_weight, updown) = (orig_weight.cpu(), updown.cpu())\n    (orig_weight, updown) = (orig_weight.float(), updown.float())\n    (orig_weight, updown) = (orig_weight.numpy(), updown.numpy())\n    (orig_weight, updown) = (torch.from_numpy(orig_weight), torch.from_numpy(updown))\n    (orig_weight, updown) = (orig_weight.to(self.device), updown.to(self.device))\n    updown = ((updown * self.scale) + orig_weight)\n    updown = (updown - orig_weight)\n    return updown", "CodeBleu_score": 32.082771173248915}, {"code": "def apply_weight_decompose(self, updown, orig_weight)\n\t\n    if self.args.decompose_weights == \"\":\n        return updown\n    else:\n        device = updown.weight.device\n        dtype = updown.weight.dtype\n        updown.weight = torch.tensor(\n            updown.weight, device=device, dtype=dtype\n        )\n        updown.weight = (\n            updown.weight\n            - self.args.decompose_weights\n            * orig_weight\n            / torch.max(torch.abs(orig_weight))\n            * torch.max(torch.abs(updown.weight))\n            * self.args.scale_factor\n        )\n        return updown\n\n", "CodeBleu_score": 29.254196781844737}]}
{"instance_id": "c73d0d1cd3e5", "generation_result": [{"code": "def finalize_updown(self, updown, orig_weight, output_shape, ex_bias=None)\n\t\n    # Match the device/dtype\n    orig_weight = orig_weight.to(updown.dtype)\n    if ex_bias is not None:\n        ex_bias = ex_bias.to(orig_weight.device, dtype=updown.dtype)\n\n    # Apply bias\n    if ex_bias is not None:\n        updown = updown + ex_bias\n\n    # Reshape\n    updown = updown.reshape(output_shape)\n\n    # Reshape to original weight if same number of elements\n    if updown.numel() == orig_weight.numel():\n        updown = updown.reshape(orig_weight.shape)\n\n    # Apply multiplier\n    if self.multiplier is not None:\n        updown = updown * self.multiplier\n\n    # Apply weight decomposition\n    if self.calc_scale() != 1.0:\n        updown = self.apply_weight_decompose(updown, orig_weight)\n\n    return updown, ex_bias\n\n", "CodeBleu_score": 58.23703988214762}, {"code": "def finalize_updown(self, updown, orig_weight, output_shape, ex_bias=None)\n\t\n    updown = updown + ex_bias\n    updown = updown.reshape(output_shape)\n    if orig_weight.numel() == updown.numel():\n        updown = updown.reshape(orig_weight.shape)\n    if ex_bias is not None:\n        ex_bias = ex_bias * self.multiplier\n    if self.scale is not None:\n        updown = self.apply_weight_decompose(updown, orig_weight)\n    return updown, ex_bias\n\n", "CodeBleu_score": 46.43856098997228}, {"code": "def finalize_updown(self, updown, orig_weight, output_shape, ex_bias=None)\n\t\n    # Apply bias\n    if ex_bias is not None:\n        updown = updown + ex_bias\n\n    # Reshape updown to match output shape\n    updown = updown.reshape(output_shape)\n\n    # If orig_weight and updown have the same number of elements, reshape updown to match orig_weight\n    if orig_weight.numel() == updown.numel():\n        updown = updown.reshape(orig_weight.shape)\n\n    # Apply multiplier to additional bias\n    if ex_bias is not None:\n        ex_bias = ex_bias * self.multiplier()\n\n    # Apply weight decomposition\n    if self.sd_key in self.network.sd_config.weight_decomposition:\n        updown = self.apply_weight_decompose(updown, orig_weight)\n\n    # Apply scaling factor\n    updown = updown * self.calc_scale()\n\n    return updown, ex_bias\n\n", "CodeBleu_score": 55.7217842663069}]}
{"instance_id": "c6a41665c62c", "generation_result": [{"code": "def calc_updown(self, orig_weight)\n\t\n    device = orig_weight.device\n    dtype = orig_weight.dtype\n    if self.bias is not None:\n        bias = self.bias.to(device, dtype=dtype)\n    else:\n        bias = None\n    if self.weight.size().numel() == orig_weight.size().numel():\n        updown = self.weight.reshape(orig_weight.shape)\n    else:\n        updown = self.weight\n    if self.dora_scale is not None:\n        updown = self.apply_weight_decompose(updown, orig_weight)\n    return self.finalize_updown(updown, orig_weight, orig_weight.shape, bias)\n\n", "CodeBleu_score": 47.094361988767865}, {"code": "def calc_updown(self, orig_weight)\n\t\n    updown = self.weight.to(orig_weight.device, dtype=orig_weight.dtype)\n    output_shape = self.output_shape\n    if self.bias is not None:\n        ex_bias = self.bias.to(orig_weight.device, dtype=orig_weight.dtype)\n    else:\n        ex_bias = None\n\n    updown, ex_bias = self.finalize_updown(updown, orig_weight, output_shape, ex_bias)\n\n    return updown, ex_bias\n\n", "CodeBleu_score": 57.34650283985317}, {"code": "def calc_updown(self, orig_weight)\n\t\n    updown = self.weight.to(orig_weight.device, dtype=orig_weight.dtype)\n    ex_bias = self.bias\n    if self.bias is not None:\n        ex_bias = ex_bias.to(orig_weight.device, dtype=orig_weight.dtype)\n    return self.finalize_updown(updown, orig_weight, orig_weight.shape, ex_bias)\n\n", "CodeBleu_score": 46.5335699278741}]}
{"instance_id": "1bb9e5d94af8", "generation_result": [{"code": "def purge_networks_from_memory()\n\t\n    networks = []\n    limit = 1000000\n    count = 0\n    for network in networks:\n        count += 1\n        if count > limit:\n            del network\n            gc.collect()\n            count = 0", "CodeBleu_score": 15.845560674253473}, {"code": "def purge_networks_from_memory()\n\t\n    import gc\n    import sys\n    from .. import settings\n    from .. import network\n    from .. import utils\n    from .. import exceptions\n    if len(network.NETWORKS) > settings.MAX_NETWORKS_IN_MEMORY:\n        while len(network.NETWORKS) > settings.MAX_NETWORKS_IN_MEMORY:\n            try:\n                network.NETWORKS.popitem()\n            except KeyError:\n                pass\n        gc.collect()\n        utils.log_string(\n            'purged networks from memory, number of networks now: ' + str(len(network.NETWORKS)))\n\n\n", "CodeBleu_score": 23.652771140350684}, {"code": "def purge_networks_from_memory()\n\t\n    pass\n", "CodeBleu_score": 0.8620689668435204}]}
{"instance_id": "42b48ebf4d07", "generation_result": [{"code": "def network_restore_weights_from_backup(self: Union[torch.nn.Conv2d, torch.nn.Linear, torch.nn.GroupNorm, torch.nn.LayerNorm, torch.nn.MultiheadAttention]):\n\t\n    if hasattr(self, \"backup_weights\"):\n        self.weight = torch.nn.Parameter(self.backup_weights)\n    if hasattr(self, \"backup_biases\"):\n        self.bias = torch.nn.Parameter(self.backup_biases)\n    if hasattr(self, \"backup_running_mean\"):\n        self.running_mean = self.backup_running_mean\n    if hasattr(self, \"backup_running_var\"):\n        self.running_var = self.backup_running_var\n    if hasattr(self, \"backup_num_batches_tracked\"):\n        self.num_batches_tracked = self.backup_num_batches_tracked\n    if hasattr(self, \"backup_projection_weights\"):\n        self.projection_weights = torch.nn.Parameter(self.backup_projection_weights)\n    if hasattr(self, \"backup_projection_biases\"):\n        self.projection_biases = torch.nn.Parameter(self.backup_projection_biases)\n    if hasattr(self, \"backup_projection_running_mean\"):\n        self.projection_running_mean = self.backup_projection_running_mean\n    if hasattr(self, \"backup_projection_running_var\"):\n        self.projection_running_var = self.backup_projection_running_var\n    if hasattr(self, \"backup_projection_num_batches_tracked\"):\n        self.projection_num_batches_tracked = self.backup_projection_num_batches_tracked\n    if hasattr(self, \"backup_bias\"):\n        self.bias = torch.nn.Parameter(self.backup_bias)\n    else:\n        self.bias = None\n\n\n\n", "CodeBleu_score": 32.29406332841057}, {"code": "def network_restore_weights_from_backup(self: Union[torch.nn.Conv2d, torch.nn.Linear, torch.nn.GroupNorm, torch.nn.LayerNorm, torch.nn.MultiheadAttention]):\n\t\n    if self.backup is not None:\n        self.weight.copy_(self.backup['weight'])\n        if self.bias is not None:\n            self.bias.copy_(self.backup['bias'])\n        else:\n            self.bias = None\n        if isinstance(self, torch.nn.MultiheadAttention):\n            self.q_proj.weight.copy_(self.backup['q_proj.weight'])\n            self.k_proj.weight.copy_(self.backup['k_proj.weight'])\n            self.v_proj.weight.copy_(self.backup['v_proj.weight'])\n            self.q_proj.bias.copy_(self.backup['q_proj.bias'])\n            self.k_proj.bias.copy_(self.backup['k_proj.bias'])\n            self.v_proj.bias.copy_(self.backup['v_proj.bias'])\n        self.backup = None\n\n\n\n", "CodeBleu_score": 40.16697765128306}, {"code": "def network_restore_weights_from_backup(self: Union[torch.nn.Conv2d, torch.nn.Linear, torch.nn.GroupNorm, torch.nn.LayerNorm, torch.nn.MultiheadAttention]):\n\t\n    if hasattr(self, 'weight_backup'):\n        self.weight = self.weight_backup\n    if hasattr(self, 'bias_backup'):\n        self.bias = self.bias_backup\n    if isinstance(self, torch.nn.MultiheadAttention):\n        if hasattr(self, 'proj_weight_backup'):\n            self.proj_weight = self.proj_weight_backup\n        if hasattr(self, 'proj_bias_backup'):\n            self.proj_bias = self.proj_bias_backup\n\n\n\n", "CodeBleu_score": 27.287866308538472}]}
{"instance_id": "c8fcd357131a", "generation_result": [{"code": "def network_forward(org_module, input, original_forward)\n\t\n    if not hasattr(org_module, \"network_weights_backup\"):\n        return original_forward(input)\n\n    input = input.to(org_module.device)\n    org_module.network_restore_weights_from_backup()\n\n    out = original_forward(input)\n\n    for network in network_loaded:\n        if network.matches_layer(org_module):\n            out = network.forward(out, input)\n\n    return out\n\n", "CodeBleu_score": 33.04471897963353}, {"code": "def network_forward(org_module, input, original_forward)\n\t\n    if not hasattr(org_module, \"sd_module\"):\n        return original_forward(input)\n\n    input = input.to(org_module.sd_module.weight.device)\n    org_module.network_restore_weights_from_backup()\n    org_module.network_reset_cached_weight()\n\n    output = original_forward(input)\n    for network in networks.networks:\n        if org_module.sd_module.weight.shape == network.sd_module.weight.shape:\n            output = network.forward(input, output)\n\n    return output\n\n", "CodeBleu_score": 37.839742819435486}, {"code": "def network_forward(org_module, input, original_forward)\n\t\n    if not hasattr(org_module, \"network_weights_backup\"):\n        return original_forward(input)\n\n    input = input.to(org_module.device)\n    org_module.network_restore_weights_from_backup()\n\n    output = original_forward(input)\n    for network_name in org_module.loaded_networks:\n        network = org_module.loaded_networks[network_name]\n        if network.network_current_names and org_module.name in network.network_current_names:\n            output = network.forward(input, output)\n    return output\n\n", "CodeBleu_score": 39.89071715381275}]}
{"instance_id": "1070211f5c11", "generation_result": [{"code": "def trainables(self)\n\t\n    trainable_params = []\n    for name, param in self.named_parameters():\n        if param.requires_grad:\n            if 'weight' in name and isinstance(param, torch.nn.Parameter):\n                trainable_params.append(param)\n            elif 'bias' in name and isinstance(param, torch.nn.Parameter):\n                trainable_params.append(param)\n    return trainable_params\n\n\n\n", "CodeBleu_score": 27.699725565645462}, {"code": "def trainables(self)\n\t\n    trainable_params = []\n    for layer in self.layers:\n        if isinstance(layer, torch.nn.Linear) or isinstance(layer, torch.nn.LayerNorm):\n            trainable_params.extend(layer.parameters())\n    return trainable_params\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 41.80660416514515}, {"code": "def trainables(self)\n\t\n    trainable_params = []\n    for name, param in self.named_parameters():\n        if param.requires_grad:\n            trainable_params.append((name, param))\n    return trainable_params\n\n\n\n", "CodeBleu_score": 25.638209418862935}]}
{"instance_id": "0c3ba62c96a7", "generation_result": [{"code": "def load_hypernetworks(names, multipliers=None)\n\t\n    if multipliers is None:\n        multipliers = [1] * len(names)\n\n    loaded_hypernetworks = []\n\n    for name, multiplier in zip(names, multipliers):\n        loaded_hypernetwork = load_hypernetwork(name)\n\n        if loaded_hypernetwork is None:\n            continue\n\n        loaded_hypernetwork.set_multiplier(multiplier)\n        loaded_hypernetworks.append(loaded_hypernetwork)\n\n    shared.loaded_hypernetworks.clear()\n    shared.loaded_hypernetworks.extend(loaded_hypernetworks)\n\n", "CodeBleu_score": 35.76217233291579}, {"code": "def load_hypernetworks(names, multipliers=None)\n\t\n    if multipliers is None:\n        multipliers = [1] * len(names)\n\n    if len(names) != len(multipliers):\n        print(f\"Error: The number of hypernetwork names ({len(names)}) does not match the number of multipliers ({len(multipliers)}).\")\n        return\n\n    shared.hypernetworks = {}\n    shared.loaded_hypernetworks = []\n\n    for name, multiplier in zip(names, multipliers):\n        hypernetwork = load_hypernetwork(name)\n\n        if hypernetwork is None:\n            continue\n\n        hypernetwork.set_multiplier(multiplier)\n        shared.loaded_hypernetworks.append(hypernetwork)\n\n    return shared.loaded_hypernetworks\n\n", "CodeBleu_score": 41.209055420039086}, {"code": "def load_hypernetworks(names, multipliers=None)\n\t\n    if multipliers is None:\n        multipliers = [1.0] * len(names)\n    loaded_hypernetworks.clear()\n    for name, multiplier in zip(names, multipliers):\n        hypernetwork = load_hypernetwork(name)\n        if hypernetwork is not None:\n            hypernetwork.set_multiplier(multiplier)\n            loaded_hypernetworks.append(hypernetwork)\n\n", "CodeBleu_score": 25.546500614362106}]}
{"instance_id": "bea3772ed532", "generation_result": [{"code": "\ndef apply_hypernetworks(hypernetworks, context, layer=None)\n\t\n    context_k = context\n    context_v = context\n    for hypernetwork in hypernetworks:\n        context_k, context_v = apply_single_hypernetwork(hypernetwork, context_k, context_v, layer)\n    return context_k, context_v", "CodeBleu_score": 79.04946886953897}, {"code": "\ndef apply_hypernetworks(hypernetworks, context, layer=None)\n\t\n    context_k = context\n    context_v = context\n    for hypernetwork in hypernetworks:\n        context_k, context_v = apply_single_hypernetwork(hypernetwork, context_k, context_v, layer)\n    return context_k, context_v\n\n\n", "CodeBleu_score": 79.04946886953897}, {"code": "\ndef apply_hypernetworks(hypernetworks, context, layer=None)\n\t\n    context_k = context_v = context\n    for hypernetwork in hypernetworks:\n        context_k, context_v = apply_single_hypernetwork(hypernetwork, context_k, context_v, layer=layer)\n    return context_k, context_v\n\n\n", "CodeBleu_score": 60.71057509388896}]}
{"instance_id": "922efa3e092f", "generation_result": [{"code": "def attention_CrossAttention_forward(self, x, context=None, mask=None, **kwargs)\n\t\n    h = self.heads\n\n    q = self.to_q(x)\n    context = context if context is not None else x\n    k = self.to_k(context)\n    v = self.to_v(context)\n    context, k, v = apply_hypernetworks(self.hypernetworks, context)\n\n    b, c, *spatial = k.shape\n    q = q.reshape(b, c, -1)\n    k = k.reshape(b, c, -1)\n    v = v.reshape(b, c, -1)\n    q = q.permute(0, 2, 1)\n    k = k.permute(0, 2, 1)\n    v = v.permute(0, 2, 1)\n\n    q = q * self.scale\n\n    sim = einsum(\"b i d, b j d -> b i j\", q, k)\n\n    if exists(mask):\n        mask = default(mask, lambda: torch.ones((b, h)).to(context))\n        max_neg_value = -torch.finfo(sim.dtype).max\n        mask = repeat(mask, \"b j -> (b h) () j\", h=h)\n        sim = sim.masked_fill(~mask, max_neg_value)\n\n    # attention, what we cannot get enough of\n    attn = sim.softmax(dim=-1)\n    out = einsum(\"b i j, b j d -> b i d\", attn, v)\n\n    out = out.permute(0, 2, 1)\n    out = out.reshape(b, c, *spatial)\n    return self.to_out(out)\n\n", "CodeBleu_score": 53.38518178761266}, {"code": "def attention_CrossAttention_forward(self, x, context=None, mask=None, **kwargs)\n\t\n    h = self.heads\n\n    q = self.to_q(x)\n\n    context = default(context, x)\n\n    k = self.to_k(context) * self.scale\n    v = self.to_v(context)\n\n    q, k, v = map(lambda t: rearrange(t, \"b n (h d) -> (b h) n d\", h=h), (q, k, v))\n\n    if exists(self.hypernetworks):\n        k, v = apply_hypernetworks(self.hypernetworks, context)\n\n    sim = einsum(\"b i d, b j d -> b i j\", q, k) * self.scale\n\n    if exists(mask):\n        mask = F.pad(mask, (self.window_size - 1, 0), value=True)\n        mask = rearrange(mask, \"b j -> b 1 1 j\")\n        sim = sim.masked_fill(~mask, -torch.finfo(sim.dtype).max)\n\n    attn = sim.softmax(dim=-1)\n    attn = self.attn_dropout(attn)\n\n    out = einsum(\"b i j, b j d -> b i d\", attn, v)\n    out = rearrange(out, \"(b h) n d -> b n (h d)\", h=h)\n    return self.to_out(out)\n\n", "CodeBleu_score": 62.11155743996374}, {"code": "def attention_CrossAttention_forward(self, x, context=None, mask=None, **kwargs)\n\t\n    h = self.heads\n    q = self.to_q(x)\n    context = default(context, x)\n    k = self.to_k(context) * self.scale\n    v = self.to_v(context)\n    (q, k, v) = map(lambda t: rearrange(t, 'b n (h d) -> (b h) n d', h=h), (q, k, v))\n    (context_k, context_v) = apply_hypernetworks(self.hypernetworks, context, layer=self.layer)\n    context_k = self.to_k(context_k) * self.scale\n    context_v = self.to_v(context_v)\n    context_k = rearrange(context_k, 'b n (h d) -> (b h) n d', h=h)\n    context_v = rearrange(context_v, 'b n (h d) -> (b h) n d', h=h)\n    (k, v) = map(lambda t: torch.cat((t, context_k), dim=1), (k, v))\n    (q, context_v) = map(lambda t: torch.cat((t, context_v), dim=1), (q, context_v))\n    sim = einsum('b i d, b j d -> b i j', q, k)\n    if exists(self.bias):\n        (b, c, *_) = q.shape\n        self.bias = self.bias.reshape(b, c, 1, 1)\n        sim = sim + self.bias\n    if exists(mask):\n        mask = F.pad(mask, (self.mask_padding, 0), value=True)\n        (attn_mask, mask_value) = expand_mask(mask, 'b ... -> (b h) ... ()', h=h)\n        sim = sim.masked_fill(~attn_mask, mask_value)\n    attn = sim.softmax(dim=-1)\n    attn = self.attn_dropout(attn", "CodeBleu_score": 45.51095884925193}]}
{"instance_id": "8b7455a03a0e", "generation_result": [{"code": "def save_hypernetwork(hypernetwork, checkpoint, hypernetwork_name, filename)\n\t\n    hypernetwork_name = hypernetwork_name or hypernetwork.name\n    filename = filename or hypernetwork.filename\n    old_checkpoint = hypernetwork.checkpoint\n    old_name = hypernetwork.name\n    old_filename = hypernetwork.filename\n    old_step = hypernetwork.step\n    old_layer_structure = hypernetwork.layer_structure\n    old_activation_func = hypernetwork.activation_func\n    old_add_layer_norm = hypernetwork.add_layer_norm\n    old_weight_init = hypernetwork.weight_init\n    old_sd_checkpoint = hypernetwork.sd_checkpoint\n    old_sd_checkpoint_name = hypernetwork.sd_checkpoint_name\n    old_activate_output = hypernetwork.activate_output\n    old_use_dropout = hypernetwork.use_dropout\n    old_dropout_structure = hypernetwork.dropout_structure\n    old_last_layer_dropout = hypernetwork.last_layer_dropout\n    old_optional_info = hypernetwork.optional_info\n\n    hypernetwork.checkpoint = checkpoint\n    hypernetwork.name = hypernetwork_name\n    hypernetwork.filename = filename\n    hypernetwork.step = checkpoint.get('step', 0)\n    hypernetwork.layer_structure = checkpoint.get('layer_structure', [128, 128])\n    hypernetwork.activation_func = checkpoint.get('activation_func', 'quick_gelu')\n    hypernetwork.add_layer_norm = checkpoint.get('is_layer_norm', False)\n    hypernetwork.weight_init = checkpoint.get('weight_initialization', 'normal')\n    hypernetwork.sd_checkpoint = checkpoint.get('sd_checkpoint', None)\n    hypernetwork.sd_checkpoint_name = checkpoint.get('sd_checkpoint_name', None)\n    hypernetwork.activate_output = checkpoint.get('activate_output', True)\n    hypernetwork.use_dropout = checkpoint.get('use_dropout', False)\n    hypernetwork.dropout_structure = checkpoint.get('dropout_structure', [0.0", "CodeBleu_score": 29.453274611637426}, {"code": "def save_hypernetwork(hypernetwork, checkpoint, hypernetwork_name, filename)\n\t\n    # Backup the original attributes\n    original_name = hypernetwork.name\n    original_step = hypernetwork.step\n    original_sd_checkpoint = hypernetwork.sd_checkpoint\n    original_sd_checkpoint_name = hypernetwork.sd_checkpoint_name\n    original_activate_output = hypernetwork.activate_output\n    original_use_dropout = hypernetwork.use_dropout\n    original_dropout_structure = hypernetwork.dropout_structure\n    original_last_layer_dropout = hypernetwork.last_layer_dropout\n    original_optional_info = hypernetwork.optional_info\n\n    # Update the hypernetwork with the new checkpoint data\n    hypernetwork.name = hypernetwork_name\n    hypernetwork.step = checkpoint['step']\n    hypernetwork.sd_checkpoint = checkpoint['sd_checkpoint']\n    hypernetwork.sd_checkpoint_name = checkpoint['sd_checkpoint_name']\n    hypernetwork.activate_output = checkpoint['activate_output']\n    hypernetwork.use_dropout = checkpoint['use_dropout']\n    hypernetwork.dropout_structure = checkpoint['dropout_structure']\n    hypernetwork.last_layer_dropout = checkpoint['last_layer_dropout']\n    hypernetwork.optional_info = checkpoint['optional_info']\n\n    try:\n        # Attempt to save the hypernetwork\n        hypernetwork.save(filename)\n    except Exception as e:\n        # If an error occurs, restore the original attributes before re-raising the exception\n        hypernetwork.name = original_name\n        hypernetwork.step = original_step\n        hypernetwork.sd_checkpoint = original_sd_checkpoint\n        hypernetwork.sd_checkpoint_name = original_sd_checkpoint_name\n        hypernetwork.activate_output = original_activate_output\n        hypernetwork.use_dropout = original_use_dropout\n        hypernetwork.dropout_structure = original_dropout_structure\n        hypernetwork.last_layer_dropout = original_last_layer_dropout\n        hypernetwork.optional_info = original_optional_info\n        raise e\n\n    # Restore", "CodeBleu_score": 24.49881354057421}, {"code": "def save_hypernetwork(hypernetwork, checkpoint, hypernetwork_name, filename)\n\t\n    # Back up the original attributes\n    original_name = hypernetwork.name\n    original_step = hypernetwork.step\n    original_sd_checkpoint = hypernetwork.sd_checkpoint\n    original_sd_checkpoint_name = hypernetwork.sd_checkpoint_name\n    original_activate_output = hypernetwork.activate_output\n    original_use_dropout = hypernetwork.use_dropout\n    original_dropout_structure = hypernetwork.dropout_structure\n    original_last_layer_dropout = hypernetwork.last_layer_dropout\n    original_optional_info = hypernetwork.optional_info\n\n    # Update the hypernetwork with the new checkpoint data\n    hypernetwork.name = hypernetwork_name\n    hypernetwork.step = checkpoint['step']\n    hypernetwork.sd_checkpoint = checkpoint['sd_checkpoint']\n    hypernetwork.sd_checkpoint_name = checkpoint['sd_checkpoint_name']\n    hypernetwork.activate_output = checkpoint['activate_output']\n    hypernetwork.use_dropout = checkpoint['use_dropout']\n    hypernetwork.dropout_structure = checkpoint['dropout_structure']\n    hypernetwork.last_layer_dropout = checkpoint['last_layer_dropout']\n    hypernetwork.optional_info = checkpoint['optional_info']\n\n    try:\n        # Attempt to save the hypernetwork\n        hypernetwork.save(filename)\n    except Exception as e:\n        # Restore the original attributes if an error occurs\n        hypernetwork.name = original_name\n        hypernetwork.step = original_step\n        hypernetwork.sd_checkpoint = original_sd_checkpoint\n        hypernetwork.sd_checkpoint_name = original_sd_checkpoint_name\n        hypernetwork.activate_output = original_activate_output\n        hypernetwork.use_dropout = original_use_dropout\n        hypernetwork.dropout_structure = original_dropout_structure\n        hypernetwork.last_layer_dropout = original_last_layer_dropout\n        hypernetwork.optional_info = original_optional_info\n\n        # Re-raise the exception\n        raise e\n\n\n", "CodeBleu_score": 24.49881354057421}]}
{"instance_id": "3175f835140f", "generation_result": [{"code": "def ema_scope(self, context=None)\n\t\n    if not self.ema_enabled:\n        return context\n    if context is None:\n        context = {}\n    self.store_ema_weights()\n    self.copy_ema_weights_to_model()\n    if self.ema_verbose:\n        print(\"Using EMA weights for evaluation\")\n    yield context\n    self.restore_model_weights()\n    if self.ema_verbose:\n        print(\"Restored model weights\")\n\n", "CodeBleu_score": 36.360701786468354}, {"code": "def ema_scope(self, context=None)\n\t\n    import torch\n    from copy import deepcopy\n\n    if context is None:\n        context = locals()\n    context['ema'] = getattr(self, 'ema', None)\n    if context['ema'] is None:\n        return\n    if context['ema'].status == 'off':\n        return\n    if context['ema'].training:\n        return\n    model = context['model']\n    ema_model = context['ema'].ema_model\n    ema_model.eval()\n    device = next(model.parameters()).device\n    ema_state_dict = deepcopy(ema_model.state_dict())\n    model_state_dict = deepcopy(model.state_dict())\n    ema_state_dict = {k: v.to(device) if v.is_floating_point() else v.long().to(device) for k, v in ema_state_dict.items()}\n    model.load_state_dict(ema_state_dict)\n    if context['ema'].log:\n        context['ema'].log.info(f\"Switched to EMA weights.\")\n    try:\n        yield\n    finally:\n        model.load_state_dict(model_state_dict)\n        if context['ema'].log:\n            context['ema'].log.info(f\"Restored model weights.\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 23.25507920471989}, {"code": "def ema_scope(self, context=None)\n\t\n    if context is None:\n        context = {}\n    if self.ema_enabled:\n        context[\"old_weights\"] = self.get_weights()\n        self.set_weights(self.ema_weights)\n        if self.ema_log_restore:\n            print(\"Restoring EMA weights\")\n    yield\n    if self.ema_enabled:\n        self.set_weights(context[\"old_weights\"])\n        if self.ema_log_restore:\n            print(\"Restored original weights\")\n\n", "CodeBleu_score": 34.81591795425237}]}
{"instance_id": "a7b5d5cd519a", "generation_result": [{"code": "def p_mean_variance(self, x, t, clip_denoised: bool)\n\t\n    model_output = self.model(x, t)\n    if self.model_var_type == 'learned':\n        assert model_output.shape == x.shape\n        model_log_variance = model_output\n        model_variance = torch.exp(model_log_variance)\n    else:\n        model_variance, model_log_variance = {\n            # For fixedlarge, we set the initial (log-)variance according to the\n            # variance type.\n            'fixed_large': (\n                np.append(self.sigma_max, self.sigma_min),\n                np.append(np.log(self.sigma_max), np.log(self.sigma_min))\n            ),\n            'fixed_small': (\n                self.sigma_min * np.ones(shape=x.shape[0]),\n                np.log(np.full(shape=x.shape[0], fill_value=self.sigma_min))\n            ),\n        }[self.model_var_type]\n        model_variance = extract_into_tensor(model_variance, t, x.shape)\n        model_log_variance = extract_into_tensor(model_log_variance, t, x.shape)\n\n    def process_xstart(x):\n        if clip_denoised:\n            return x.clamp(-1, 1)\n        return x\n\n    if self.model_mean_type == 'xstart':\n        pred_xstart = process_xstart(self.predict_start_from_noise(x_t=x, t=t, noise=model_output))\n        model_mean = pred_xstart\n    elif self.model_mean_type == 'xprev':\n        pred_xstart = process_xstart(self.predict_start_from_noise(x_t=self.q_posterior_mean_variance(x_start=x_t, x_t=x, t=t)[0], t=t, noise=model_output))\n        model_mean = pred_xstart\n    elif self.model_mean_type == 'epsilon':\n        model_mean = model_output", "CodeBleu_score": 36.99269520780807}, {"code": "def p_mean_variance(self, x, t, clip_denoised: bool)\n\t\n    # x_recon = self.predict_start_from_noise(x, t=t, noise=model_output)\n    model_mean, posterior_variance, posterior_log_variance = self.q_posterior(x_start=x, x_t=x, t=t)\n    if clip_denoised:\n        model_mean = torch.clamp(model_mean, -1, 1)\n    return model_mean, posterior_variance, posterior_log_variance\n\n", "CodeBleu_score": 22.808771227192214}, {"code": "def p_mean_variance(self, x, t, clip_denoised: bool)\n\t\n    batch, device = x.shape[0], x.device\n    noise_level = torch.FloatTensor([self.sqrt_recip_alphas_cumprod[t]]).to(device)\n    # Model output\n    model_output = self.model(x, noise_level)\n    # Reconstruct input\n    if self.model_var_type in ['learned', 'learned_range']:\n        pred_original_sample = self.predict_start_from_noise(x, t=t, noise=model_output)\n        model_output = self.predict_start_from_noise(x, t=t, noise=model_output)\n    else:\n        pred_original_sample = self.predict_start_from_noise(x, t=t, noise=torch.zeros_like(x))\n    # Clip\n    if clip_denoised:\n        model_output = torch.clamp(model_output, -1., 1.)\n    # Model mean\n    model_mean, posterior_variance, posterior_log_variance = self.q_posterior(x_start=pred_original_sample, x_t=x, t=t)\n    return model_mean, posterior_variance, posterior_log_variance\n\n", "CodeBleu_score": 48.85381243858675}]}
{"instance_id": "a8652fa78473", "generation_result": [{"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss_type == 'l1':\n        loss = F.l1_loss(pred, target, reduction='none')\n        if mean:\n            loss = loss.mean()\n    elif self.loss_type == 'l2':\n        loss = F.mse_loss(pred, target, reduction='none')\n        if not mean:\n            loss = loss.sum()\n    else:\n        raise NotImplementedError('loss type [%s] is not found' % self.loss_type)\n    return loss\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 54.697585323888276}, {"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss_type == 'l1':\n        loss = F.l1_loss(pred, target, reduction='none')\n        if mean:\n            loss = loss.mean()\n    elif self.loss_type == 'l2':\n        loss = F.mse_loss(pred, target, reduction='none')\n        if not mean:\n            loss = loss.sum()\n    else:\n        raise NotImplementedError(self.loss_type)\n    return loss\n\n\n", "CodeBleu_score": 53.86968263770287}, {"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss_type == 'l1':\n        loss = F.l1_loss(pred, target, reduction='none')\n        if mean:\n            loss = loss.mean()\n    elif self.loss_type == 'l2':\n        loss = F.mse_loss(pred, target, reduction='none')\n        if not mean:\n            loss = loss.sum()\n    else:\n        raise NotImplementedError('Loss type {} is not supported.'.format(self.loss_type))\n    return loss\n\n", "CodeBleu_score": 54.87249852237937}]}
{"instance_id": "d68e4f061349", "generation_result": [{"code": "def _prior_bpd(self, x_start)\n\t\n    batch_size = x_start.shape[0]\n    t = torch.tensor([self.num_timesteps - 1] * batch_size, device=x_start.device)\n    mean, _, log_variance = self.q_mean_variance(x_start, t)\n    return self.kl_normal(mean, log_variance) / np.log(2.0)\n\n", "CodeBleu_score": 40.00659388019202}, {"code": "def _prior_bpd(self, x_start)\n\t\n    batch_size = x_start.shape[0]\n    t = torch.tensor([self.num_timesteps - 1] * batch_size, device=x_start.device)\n    mean, _, log_variance = self.q_mean_variance(x_start=x_start, t=t)\n    return mean, log_variance\n\n", "CodeBleu_score": 36.39424180202961}, {"code": "def _prior_bpd(self, x_start)\n\t\n    batch_size = x_start.shape[0]\n    t = torch.tensor([self.num_timesteps - 1] * batch_size, device=x_start.device)\n    mean, _, log_var = self.q_mean_variance(x_start, t)\n    return normal_kl(mean, log_var) / np.log(2.0)\n\n", "CodeBleu_score": 39.011553638378935}]}
{"instance_id": "c695d93fdf2d", "generation_result": [{"code": "def make_cache(subsection: str) -> diskcache.Cache\n\t\n    cache_dir = os.path.join(os.path.expanduser(\"~\"), \".cache\", \"sleap\", subsection)\n    cache = diskcache.Cache(cache_dir, size_limit=int(4e9), eviction_policy=\"least-recently-used\")\n    cache.settings[\"sqlite_min_file_size\"] = 256000\n    return cache\n\n\n\n", "CodeBleu_score": 44.33248207180044}, {"code": "def make_cache(subsection: str) -> diskcache.Cache\n\t\n    cache = diskcache.Cache(\n        directory=os.path.join(CACHE_DIR, subsection),\n        size_limit=4 * 1024 * 1024 * 1024,\n        disk_min_file_size=256 * 1024,\n        disk_pickle_protocol=pickle.HIGHEST_PROTOCOL,\n    )\n    return cache\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 55.6355115406151}, {"code": "def make_cache(subsection: str) -> diskcache.Cache\n\t\n    return diskcache.Cache(os.path.join(CACHE_DIR, subsection), size_limit=4e9, disk_min_file_size=256000)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 45.34906543827416}]}
{"instance_id": "ed4c543f0d38", "generation_result": [{"code": "def cache(subsection)\n\t\n    cache_obj = caches.get(subsection)\n    if cache_obj is None:\n        with cache_lock:\n            if not os.path.exists(cache_dir):\n                os.makedirs(cache_dir)\n            if not os.path.isfile(cache_filename):\n                convert_old_cached_data()\n\n            cache_obj = caches.get(subsection)\n            if cache_obj is None:\n                cache_obj = make_cache(subsection)\n                caches[subsection] = cache_obj\n\n    return cache_obj\n\n", "CodeBleu_score": 67.25921240957332}, {"code": "def cache(subsection)\n\t\n    cache_obj = caches.get(subsection)\n    if cache_obj is None:\n        with cache_lock:\n            if not os.path.isdir(cache_dir):\n                os.makedirs(cache_dir)\n            if not os.path.isfile(cache_filename):\n                convert_old_cached_data()\n\n            cache_obj = caches.get(subsection)\n            if cache_obj is None:\n                cache_obj = make_cache(subsection)\n                caches[subsection] = cache_obj\n\n    return cache_obj\n", "CodeBleu_score": 67.00625368564643}, {"code": "def cache(subsection)\n\t\n    global cache_lock\n    cache_obj = caches.get(subsection)\n    if cache_obj is None:\n        with cache_lock:\n            if not os.path.isdir(cache_dir):\n                os.mkdir(cache_dir)\n            if not os.path.isfile(cache_filename):\n                convert_old_cached_data()\n            cache_obj = caches.get(subsection)\n            if cache_obj is None:\n                cache_obj = make_cache(subsection)\n                caches[subsection] = cache_obj\n    return cache_obj\n\n", "CodeBleu_score": 64.55070617239383}]}
{"instance_id": "ccf5ea2d9fc1", "generation_result": [{"code": "def cached_data_for_file(subsection, title, filename, func)\n\t\n    cache_obj = cache(subsection)\n    mtime = os.path.getmtime(filename)\n    if title in cache_obj:\n        cached_mtime, cached_data = cache_obj[title]\n        if mtime > cached_mtime:\n            cache_obj.invalidate(title)\n    if title not in cache_obj:\n        data = func()\n        cache_obj[title] = (mtime, data)\n        dump_cache()\n    else:\n        _, data = cache_obj[title]\n    return data\n\n", "CodeBleu_score": 26.758128166737627}, {"code": "def cached_data_for_file(subsection, title, filename, func)\n\t\n\n    cache_obj = cache(subsection)\n    mtime = os.path.getmtime(filename)\n    cached_data = cache_obj.get(title)\n    if cached_data:\n        cached_mtime, cached_data = cached_data\n        if mtime > cached_mtime:\n            cache_obj.invalidate(title)\n            cached_data = None\n\n    if not cached_data:\n        cached_data = func()\n        cache_obj.set(title, (mtime, cached_data), expire=None)\n        dump_cache()\n\n    return cached_data\n\n", "CodeBleu_score": 28.534968638134888}, {"code": "def cached_data_for_file(subsection, title, filename, func)\n\t\n\n    cache_obj = cache(subsection)\n\n    mtime = os.path.getmtime(filename)\n    if title in cache_obj:\n        cached_mtime, cached_data = cache_obj[title]\n        if mtime > cached_mtime:\n            del cache_obj[title]\n    if title not in cache_obj:\n        data = func()\n        cache_obj[title] = (mtime, data)\n        dump_cache()\n        return data\n    else:\n        return cache_obj[title][1]\n\n", "CodeBleu_score": 24.111280393217715}]}
{"instance_id": "e01a2d4f0ac2", "generation_result": [{"code": "def report(message: str, *, exc_info: bool = False) -> None\n\t\n    record_exception()\n    for line in message.splitlines():\n        print(f\"*** {line}\", file=sys.stderr)\n\n    if exc_info:\n        for line in format_exception(*sys.exc_info()):\n            print(f\"    {line}\", file=sys.stderr)\n\n    print(\"---\", file=sys.stderr)\n\n", "CodeBleu_score": 53.74698908895211}, {"code": "def report(message: str, *, exc_info: bool = False) -> None\n\t\n    record_exception()\n\n    for line in message.splitlines():\n        print(\"*** \" + line, file=sys.stderr)\n\n    if exc_info:\n        for line in format_exception(sys.exc_info()):\n            print(\"    \" + line, file=sys.stderr)\n\n    print(\"---\", file=sys.stderr)\n\n", "CodeBleu_score": 57.18069774057047}, {"code": "def report(message: str, *, exc_info: bool = False) -> None\n\t\n\n    record_exception()\n\n    for line in message.splitlines():\n        print(\"***\", line, file=sys.stderr)\n\n    if exc_info:\n        print(\"---\", file=sys.stderr)\n        for line in format_exception(sys.exc_info()):\n            print(\"    \" + line, file=sys.stderr)\n\n", "CodeBleu_score": 63.850506984829025}]}
{"instance_id": "125fb9d2320b", "generation_result": [{"code": "def extract_original_prompts(style: PromptStyle, prompt, negative_prompt)\n\t\n\n    if not style.prompt and not style.negative_prompt:\n        return False, prompt, negative_prompt\n\n    extracted_positive, positive_prompt = extract_style_text_from_prompt(style.prompt, prompt)\n    extracted_negative, negative_prompt = extract_style_text_from_prompt(style.negative_prompt, negative_prompt)\n\n    if not extracted_positive or not extracted_negative:\n        return False, prompt, negative_prompt\n\n    return True, positive_prompt, negative_prompt\n\n", "CodeBleu_score": 68.50601693143966}, {"code": "def extract_original_prompts(style: PromptStyle, prompt, negative_prompt)\n\t\n\n    if not style.prompt and not style.negative_prompt:\n        return False, prompt, negative_prompt\n\n    extracted_positive, positive_prompt = extract_style_text_from_prompt(style.prompt, prompt)\n    extracted_negative, negative_prompt = extract_style_text_from_prompt(style.negative_prompt, negative_prompt)\n\n    if not extracted_positive or not extracted_negative:\n        return False, prompt, negative_prompt\n\n    return True, positive_prompt, negative_prompt\n\n", "CodeBleu_score": 68.50601693143966}, {"code": "def extract_original_prompts(style: PromptStyle, prompt, negative_prompt)\n\t\n\n    if style.prompt == \"\" and style.negative_prompt == \"\":\n        return False, prompt, negative_prompt\n\n    extracted_positive, prompt = extract_style_text_from_prompt(style.prompt, prompt)\n    extracted_negative, negative_prompt = extract_style_text_from_prompt(style.negative_prompt, negative_prompt)\n\n    if not extracted_positive or not extracted_negative:\n        return False, prompt, negative_prompt\n\n    return True, prompt, negative_prompt\n\n", "CodeBleu_score": 61.936431057895525}]}
{"instance_id": "0c5f729fbce2", "generation_result": [{"code": "def random_partial(self, count, n_frames)\n\t\n\n    if self.utterances is None:\n        self._load_utterances()\n\n    utterances = self.utterance_cycler.sample(count)\n\n    a = [(u,) + u.random_partial(n_frames) for u in utterances]\n\n    return a\n\n", "CodeBleu_score": 94.20475781398312}, {"code": "def random_partial(self, count, n_frames)\n\t\n\n    if self.utterances is None:\n        self._load_utterances()\n\n    utterances = self.utterance_cycler.sample(count)\n\n    a = [(u,) + u.random_partial(n_frames) for u in utterances]\n\n    return a\n\n", "CodeBleu_score": 94.20475781398312}, {"code": "def random_partial(self, count, n_frames)\n\t\n    if self.utterances is None:\n        self._load_utterances()\n\n    utterances = self.utterance_cycler.sample(count)\n\n    a = [(u,) + u.random_partial(n_frames) for u in utterances]\n\n    return a\n\n", "CodeBleu_score": 94.20475781398312}]}
{"instance_id": "31e8b27b8f94", "generation_result": [{"code": "def normalize_volume(wav, target_dBFS, increase_only=False, decrease_only=False)\n\t\n    if increase_only and decrease_only:\n        raise ValueError(\"Both increase only and decrease only are set\")\n    dBFS_change = target_dBFS - 10 * np.log10(np.mean(wav ** 2))\n    if (dBFS_change < 0) and increase_only:\n        return wav\n    if (dBFS_change > 0) and decrease_only:\n        return wav\n    return wav * (10 ** (dBFS_change / 20))\n\n", "CodeBleu_score": 84.8381782725433}, {"code": "def normalize_volume(wav, target_dBFS, increase_only=False, decrease_only=False)\n\t\n    if increase_only and decrease_only:\n        raise ValueError(\"Both increase only and decrease only are set\")\n    dBFS_change = target_dBFS - 10 * np.log10(np.mean(wav ** 2))\n    if (dBFS_change < 0) and increase_only:\n        return wav\n    if (dBFS_change > 0) and decrease_only:\n        return wav\n    return wav * (10 ** (dBFS_change / 20))\n\n", "CodeBleu_score": 84.8381782725433}, {"code": "def normalize_volume(wav, target_dBFS, increase_only=False, decrease_only=False)\n\t\n    if increase_only and decrease_only:\n        raise ValueError(\"Both increase only and decrease only are set\")\n    dBFS_change = target_dBFS - 10 * np.log10(np.mean(wav ** 2))\n    if (dBFS_change < 0) and increase_only:\n        return wav\n    if (dBFS_change > 0) and decrease_only:\n        return wav\n    return wav * (10 ** (dBFS_change / 20))\n\n", "CodeBleu_score": 84.8381782725433}]}
{"instance_id": "2353831e9033", "generation_result": [{"code": "def load_model(weights_fpath: Path, device=None)\n\t\n    global _model\n\n    if device is None:\n        _model_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    elif isinstance(device, str):\n        _model_device = torch.device(device)\n    else:\n        _model_device = device\n    print(\"Running a model on device: {}\".format(_model_device))\n\n    _model = SpeakerEncoder(_model_device, torch.device(\"cpu\"))\n    checkpoint = torch.load(weights_fpath, map_location=\"cpu\")\n    _model.load_state_dict(checkpoint[\"model_state\"])\n    _model.eval()\n    print(\"Loaded model \\\"%s\\\" trained to step %d\" % (weights_fpath, checkpoint[\"step\"]))\n\n", "CodeBleu_score": 68.75935649609461}, {"code": "def load_model(weights_fpath: Path, device=None)\n\t\n    global _model\n\n    if device is None:\n        _device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    elif isinstance(device, str):\n        _device = torch.device(device)\n    _model = SpeakerEncoder(_device, torch.device(\"cpu\"))\n    checkpoint = torch.load(weights_fpath, map_location=\"cpu\")\n    _model.load_state_dict(checkpoint[\"model_state\"])\n    _model.eval()\n    print(\"Loaded encoder \\\"%s\\\" trained to step %d\" % (weights_fpath, checkpoint[\"step\"]))\n\n\n\n\n", "CodeBleu_score": 80.86907711357529}, {"code": "def load_model(weights_fpath: Path, device=None)\n\t\n    global _model\n    if device is None:\n        _model_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    elif isinstance(device, str):\n        _model_device = torch.device(device)\n    else:\n        _model_device = device\n    _model = SpeakerEncoder(_model_device, torch.device(\"cpu\"))\n    checkpoint = torch.load(weights_fpath, map_location=\"cpu\")\n    _model.load_state_dict(checkpoint[\"model_state\"])\n    _model.eval()\n    print(f\"Loaded encoder \\\"{weights_fpath.name}\\\" trained to step {checkpoint['step']}\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 67.83704619286101}]}
{"instance_id": "3231ad480199", "generation_result": [{"code": "def embed_frames_batch(frames_batch)\n\t\n    if _model is None:\n        raise Exception(\"Model was not loaded.\")\n\n    frames = torch.from_numpy(frames_batch).to(_device)\n    embed = _model.forward(frames).detach().cpu().numpy()\n\n    return embed\n\n", "CodeBleu_score": 83.27192644053888}, {"code": "def embed_frames_batch(frames_batch)\n\t\n    if _model is None:\n        raise Exception(\"Model was not loaded. Call load_model() before inference.\")\n\n    frames = torch.from_numpy(frames_batch).to(_device)\n    embed = _model(frames).detach().cpu().numpy()\n\n    return embed\n\n", "CodeBleu_score": 82.35633439652212}, {"code": "def embed_frames_batch(frames_batch)\n\t\n    if _model is None:\n        raise Exception(\"Model was not loaded.\")\n\n    frames = torch.from_numpy(frames_batch).to(_device)\n    embed = _model.forward(frames).detach().cpu().numpy()\n\n    return embed\n\n", "CodeBleu_score": 83.27192644053888}]}
{"instance_id": "117a41716c8d", "generation_result": [{"code": "def compute_partial_slices(n_samples, partial_utterance_n_frames=partials_n_frames,\n                           min_pad_coverage=0.75, overlap=0.5)\n\t\n    assert 0 < min_pad_coverage <= 1\n    assert 0 <= overlap <= 1\n    pad_samples = int((partial_utterance_n_frames - overlap) * step)\n    mel_samples = partial_utterance_n_frames\n    hop_samples = int((1 - overlap) * step)\n    slices = []\n    start_idx = 0\n    while start_idx < n_samples:\n        end_idx = start_idx + mel_samples\n        mel_diff = end_idx - start_idx\n        if end_idx >= n_samples:\n            shift = n_samples - end_idx\n            end_idx = n_samples\n            start_idx = start_idx + shift\n        slice_ = (start_idx, end_idx)\n        mel_diff = end_idx - start_idx\n        if mel_diff % step != 0:\n            raise ValueError(f\"mel diff {mel_diff} is not multiple of `step` {step}\")\n        mel_frames = mel_diff // step\n        slices.append(slice_)\n        start_idx = end_idx + pad_samples\n    while len(slices) > 0 and slices[-1][1] == n_samples:\n        slices.pop()\n    return slices\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 27.162939925588}, {"code": "def compute_partial_slices(n_samples, partial_utterance_n_frames=partials_n_frames,\n                           min_pad_coverage=0.75, overlap=0.5)\n\t\n    assert 0 < min_pad_coverage <= 1\n    assert 0 <= overlap <= 1\n    frames = partial_utterance_n_frames\n    min_pad_coverage = int(min_pad_coverage * frames)\n\n    # Compute the slices\n    wav_slices, mel_slices = [], []\n    steps = max(1, int(frames * (1 - overlap)))\n    for i in range(0, n_samples, steps):\n        mel_range = np.array([i, i + frames])\n        wav_range = mel_range * config.audio['hop_length']\n        mel_slices.append(slice(*mel_range))\n        wav_slices.append(slice(*wav_range))\n\n    # Evaluate whether extra padding is warranted or not\n    last_wav_range = wav_slices[-1]\n    coverage = (n_samples - last_wav_range.start) / (last_wav_range.stop - last_wav_range.start)\n    if coverage < min_pad_coverage and len(mel_slices) > 1:\n        mel_slices = mel_slices[:-1]\n        wav_slices = wav_slices[:-1]\n\n    return wav_slices, mel_slices\n\n", "CodeBleu_score": 60.292899298666455}, {"code": "def compute_partial_slices(n_samples, partial_utterance_n_frames=partials_n_frames,\n                           min_pad_coverage=0.75, overlap=0.5)\n\t\n    assert 0 < min_pad_coverage <= 1\n    assert 0 <= overlap <= 1\n    frames = mel_slices(n_samples, rate=partial_utterance_n_frames,\n                        hop_length=int((1 - overlap) * partial_utterance_n_frames))\n    mel_slices = []\n    wav_slices = []\n    steps = max(1, int(partial_utterance_n_frames * (1 - overlap)))\n    for i in range(0, len(frames), steps):\n        mel_range = np.array([i, i + partial_utterance_n_frames])\n        wav_range = mel_range * int(rate / partial_utterance_n_frames)\n        mel_slices.append(slice(*mel_range))\n        wav_slices.append(slice(*wav_range))\n    last_wav_range = wav_slices[-1]\n    coverage = (n_samples - last_wav_range.start) / (last_wav_range.stop - last_wav_range.start)\n    if coverage < min_pad_coverage and len(mel_slices) > 1:\n        mel_slices = mel_slices[:-1]\n        wav_slices = wav_slices[:-1]\n    return wav_slices, mel_slices\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 61.33621863859723}]}
{"instance_id": "34fe5c864f1c", "generation_result": [{"code": "def plot_embedding_as_heatmap(embed, ax=None, title=\"\", shape=None, color_range=(0, 0.30))\n\t\n    if ax is None:\n        ax = plt.gca()\n    if shape is None:\n        height = int(np.sqrt(embed.shape[0]))\n        shape = (height, -1)\n    embed = embed.reshape(shape)\n    cmap = cm.RdBu\n    im = ax.imshow(embed, aspect=\"auto\", cmap=cmap, clim=color_range)\n    divider = make_axes_locatable(ax)\n    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.1)\n    plt.colorbar(im, cax=cax)\n    cax.set_ylabel(\"Embedding value\", labelpad=15, rotation=270)\n    ax.set_title(title, fontsize=15)\n    ax.set_xticks([])\n    ax.set_yticks([])\n\n", "CodeBleu_score": 63.509010040757076}, {"code": "def plot_embedding_as_heatmap(embed, ax=None, title=\"\", shape=None, color_range=(0, 0.30))\n\t\n    if ax is None:\n        ax = plt.gca()\n\n    if shape is None:\n        height = int(np.sqrt(embed.shape[0]))\n        shape = (height, height)\n\n    embed = embed.reshape(shape)\n    cmap = plt.cm.get_cmap(\"bwr\")\n    heatmap = ax.imshow(embed, cmap=cmap, vmin=color_range[0], vmax=color_range[1])\n    ax.set_title(title)\n    ax.set_xticks([])\n    ax.set_yticks([])\n    plt.colorbar(heatmap, ax=ax)\n\n\n\n\n\n", "CodeBleu_score": 59.9293478308736}, {"code": "def plot_embedding_as_heatmap(embed, ax=None, title=\"\", shape=None, color_range=(0, 0.30))\n\t\n    if ax is None:\n        ax = plt.gca()\n\n    if shape is None:\n        height = int(np.sqrt(embed.shape[0]))\n        embed = embed.reshape(height, -1)\n\n    cmap = plt.cm.get_cmap()\n    im = ax.imshow(embed, cmap=cmap)\n    plt.colorbar(im, ax=ax)\n    plt.clim(color_range[0], color_range[1])\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.set_title(title)\n\n\n\n", "CodeBleu_score": 53.85140823237611}]}
{"instance_id": "92e93d50b62b", "generation_result": [{"code": "def similarity_matrix(self, embeds)\n\t\n    speakers_per_batch, utterances_per_speaker = embeds.shape\n\n    # Compute the embeddings of the centroids, excluding the utterances of the same speaker.\n    centroids_incl = torch.mean(embeds, dim=1, keepdim=True)\n    centroids_excl = (torch.sum(embeds, dim=1, keepdim=True) - embeds) / (utterances_per_speaker - 1)\n\n    # In this case, centroids_excl would be the same as centroids_incl if we excluded the utterance of the speaker itself.\n    centroids = torch.cat([centroids_incl, centroids_excl], dim=1)\n\n    # Normalize the centroids, such that they lie on the unit sphere.\n    centroids = F.normalize(centroids, p=2, dim=2)\n\n    # Calculate similarities.\n    similarity_matrix = torch.zeros((speakers_per_batch, utterances_per_speaker, speakers_per_batch), device=self.device)\n    mask_matrix = torch.ones_like(similarity_matrix)\n    mask_matrix.fill_diagonal_(0)\n\n    for speaker_index in range(speakers_per_batch):\n        n_idx = speaker_index * utterances_per_speaker\n        m_idx = (speaker_index + 1) * utterances_per_speaker\n\n        for centroid_index, centroid in enumerate(centroids[speaker_index]):\n            similarity_matrix[speaker_index, :, centroid_index] = self.similarity_function(embeds[speaker_index], centroid)\n\n    # The similarity matrix is masked to exclude self-similarity.\n    similarity_matrix = similarity_matrix * mask_matrix\n\n    # Add a softmax to each row.\n    masked_similarity_matrix = similarity_matrix + mask_matrix * -1e30\n    softmax_similarity_matrix = F.softmax(masked_similarity_matrix, dim=2)\n\n   ", "CodeBleu_score": 33.22520506530594}, {"code": "def similarity_matrix(self, embeds)\n\t\n    speakers_per_batch, utterances_per_speaker = embeds.shape\n    embeds_mask = torch.eye(speakers_per_batch, dtype=torch.bool, device=self.device)\n    embeds_mask = embeds_mask.repeat([1, utterances_per_speaker])\n    embeds_mask = embeds_mask.reshape([speakers_per_batch * utterances_per_speaker, speakers_per_batch])\n    embeds_mask = embeds_mask.fliplr(dims=[1])\n    centroids_incl = torch.mean(embeds, 1, keepdim=True)\n    centroids_excl = torch.mean(embeds.masked_select(embeds_mask).view(speakers_per_batch, speakers_per_batch - 1, embeds.shape[-1]), 1, keepdim=True)\n    centroids_excl = centroids_excl.masked_fill(torch.isnan(centroids_excl), 0)\n    centroids = torch.cat([centroids_incl, centroids_excl], 1).transpose(0, 1)\n    centroids = F.normalize(centroids, p=2, dim=1)\n    similarity_matrix = self.similarity_weight * torch.matmul(embeds, centroids)\n    similarity_matrix = similarity_matrix + self.similarity_bias\n    similarity_matrix = similarity_matrix.view([self.num_speakers_per_batch, self.num_utterances_per_speaker, self.num_speakers])\n    similarity_matrix = similarity_matrix.masked_fill(embeds_mask, -np.inf)\n    similarity_matrix = F.softmax(similarity_matrix, dim=2)\n    similarity_matrix = similarity_matrix.masked_fill(embeds_mask, 0)\n    return similarity_matrix\n\n\n\n", "CodeBleu_score": 22.760676321550843}, {"code": "def similarity_matrix(self, embeds)\n\t\n    speakers_per_batch, utterances_per_speaker = embeds.shape\n    embeds_flat = embeds.view(speakers_per_batch * utterances_per_speaker, -1)\n    centroids_incl = torch.mean(embeds_flat, dim=1, keepdim=True)\n    centroids_excl = torch.mean(embeds_flat, dim=1, keepdim=True)\n    centroids_incl = centroids_incl / torch.norm(centroids_incl, dim=1, keepdim=True)\n    centroids_excl = centroids_excl / torch.norm(centroids_excl, dim=1, keepdim=True)\n    similarity_matrix = torch.zeros(speakers_per_batch * utterances_per_speaker, speakers_per_batch * utterances_per_speaker, device=embeds.device)\n    mask_matrix = 1 - torch.eye(speakers_per_batch * utterances_per_speaker, device=embeds.device)\n    for speaker_index in range(speakers_per_batch):\n        for utterance_index in range(utterances_per_speaker):\n            centroids_incl_idx = torch.arange(speakers_per_batch * utterances_per_speaker)\n            centroids_excl_idx = torch.cat((torch.arange(0, speaker_index * utterances_per_speaker), torch.arange((speaker_index + 1) * utterances_per_speaker, speakers_per_batch * utterances_per_speaker)))\n            similarity_matrix[speaker_index * utterances_per_speaker + utterance_index, centroids_incl_idx] = torch.einsum(\"i,ji->j\", embeds_flat[speaker_index * utterances_per_speaker + utterance_index].detach(), centroids_incl).detach()\n            similarity_matrix[speaker_index * utterances_per_speaker + utterance_index, centroids_excl_idx]", "CodeBleu_score": 27.100495647702882}]}
{"instance_id": "a6eba54108a1", "generation_result": [{"code": "def loss(self, embeds)\n\t\n    speakers_per_batch, utterances_per_speaker = embeds.shape[:2]\n\n    # Similarity matrix. The columns correspond to utterances of a speaker and the rows are\n    # the speaker embeddings.\n    sim_matrix = self.similarity_matrix(embeds)\n    sim_matrix = sim_matrix.reshape((speakers_per_batch * utterances_per_speaker,\n                                     speakers_per_batch))\n    ground_truth = np.repeat(np.arange(speakers_per_batch), utterances_per_speaker)\n    ground_truth = torch.LongTensor(ground_truth.cuda())\n\n    # Compute the loss\n    loss = self.loss_fn(sim_matrix, ground_truth)\n\n    # EER\n    with torch.no_grad():\n        inv_argmax = lambda i: torch.argmax(i - 100 * torch.eye(i.shape[0]).cuda(), dim=1)\n        tp = 0\n        fp = 0\n        fn = 0\n        tn = 0\n        for j in range(speakers_per_batch):\n            for i in [inv_argmax(row) for row in sim_matrix[j * utterances_per_speaker: (j + 1) * utterances_per_speaker]]:\n                if i == j:\n                    tp += 1\n                else:\n                    fp += 1\n            for i in [inv_argmax(column) for column in sim_matrix.t()[j * utterances_per_speaker: (j + 1) * utterances_per_speaker]]:\n                if i == j:\n                    fn += 1\n                else:\n                    tn += 1\n        eer = (1.0 * fp + fn) / (tp + fp + tn + fn)\n\n    return loss, eer\n\n", "CodeBleu_score": 49.816315790003024}, {"code": "def loss(self, embeds)\n\t\n    speakers_per_batch, utterances_per_speaker = embeds.shape[:2]\n    \n    # Similarity matrix. The columns correspond to utterances of a speaker and the rows are\n    # the speaker embeddings.\n    sim_matrix = self.similarity_matrix(embeds)\n    sim_matrix = sim_matrix.reshape((speakers_per_batch * utterances_per_speaker,\n                                    speakers_per_batch))\n    ground_truth = np.repeat(np.arange(speakers_per_batch), utterances_per_speaker)\n    target = torch.from_numpy(ground_truth).long().to(self.loss_device)\n    predictions = F.log_softmax(sim_matrix, dim=1)\n    loss = F.nll_loss(predictions, target)\n    \n    # EER\n    with torch.no_grad():\n        ### Derive the EER from the loss. EER is the point on the DET curve\n        ### where the false negative rate (misses) are equal to the false positive rate (false alarm)\n        fnrs, fprs, thresholds = roc_curve(ground_truth, predictions[:, 1])\n        eer = fnrs[np.nanargmin(np.absolute((fnrs - fprs)))]\n        \n    return loss, eer\n\n", "CodeBleu_score": 52.64099100002356}, {"code": "def loss(self, embeds)\n\t\n    speakers_per_batch, utterances_per_speaker = embeds.shape[:2]\n\n    # Similarity matrix. The columns correspond to utterances of a speaker and the rows are\n    # the speakers, excluding the speaker itself.\n    sim_matrix = self.similarity_matrix(embeds)\n    sim_matrix = sim_matrix.reshape((speakers_per_batch * utterances_per_speaker,\n                                     speakers_per_batch))\n    ground_truth = np.repeat(np.arange(speakers_per_batch), utterances_per_speaker)\n    target = torch.from_numpy(ground_truth).long().to(self.loss_device)\n\n    # Calculating loss and EER\n    loss = self.loss_fn(sim_matrix, target)\n\n    with torch.no_grad():\n        # EER\n        inv_argmax = lambda i: np.eye(1, speakers_per_batch, i, dtype=np.int)[0]\n        labels = np.array([inv_argmax(i) for i in ground_truth])\n        preds = sim_matrix.detach().cpu().numpy()\n\n        # Snippet from https://yangcha.github.io/EER-ROC/\n        fpr, tpr, thresholds = roc_curve(labels.flatten(), preds.flatten())\n        eer = brentq(lambda x: 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n\n    return loss, eer\n\n", "CodeBleu_score": 86.97199701035129}]}
{"instance_id": "ac7b74b9d1b1", "generation_result": [{"code": "def log_params(self)\n\t\n    # Import parameter configurations\n    from .parameters import model_params, data_params\n\n    # Initialize the parameter string\n    param_str = \"\"\n\n    # Iterate through model parameters\n    for key, value in model_params.items():\n        # Add the parameter name and value to the string\n        param_str += f\"{key}: {value}", "CodeBleu_score": 5.010935244457773}, {"code": "def log_params(self)\n\t\n    import inspect\n    import os\n    import sys\n\n    if not self.disable_viz_log:\n        from . import visualize\n\n        model_params = self.model.named_parameters()\n        data_params = self.data.named_parameters()\n        params = list(model_params) + list(data_params)\n        params_str = \"\".join(\n            [\n                f\"{name}: {value}", "CodeBleu_score": 15.544198701375725}, {"code": "def log_params(self)\n\t\n    import yaml\n\n    # Import parameter configurations\n    with open(\"configs/model_config.yaml\", \"r\") as f:\n        model_config = yaml.safe_load(f)\n    with open(\"configs/data_config.yaml\", \"r\") as f:\n        data_config = yaml.safe_load(f)\n\n    # Create a formatted string for the model parameters\n    model_params = \"Model Parameters:", "CodeBleu_score": 9.524767336694682}]}
{"instance_id": "4726b7864386", "generation_result": [{"code": "def draw_projections(self, embeds, utterances_per_speaker, step, out_fpath=None, max_speakers=10)\n\t\n    embeds = embeds[:max_speakers * utterances_per_speaker]\n    utterances_per_speaker = min(utterances_per_speaker, len(embeds) // max_speakers)\n    labels = np.expand_dims(np.arange(max_speakers), axis=1)\n    labels = np.repeat(labels, utterances_per_speaker, axis=1).flatten()[:len(embeds)]\n    reducer = umap.UMAP()\n    projected = reducer.fit_transform(embeds)\n    plt.figure(figsize=(10, 10))\n    plt.scatter(projected[:, 0], projected[:, 1], c=labels, s=10, alpha=0.5)\n    plt.title(f\"Step {step}\")\n    plt.gca().set_aspect(\"equal\", \"datalim\")\n    plt.tight_layout()\n    if out_fpath is not None:\n        plt.savefig(out_fpath)\n    else:\n        plt.show()\n    plt.close()\n\n\n", "CodeBleu_score": 50.6873252673646}, {"code": "def draw_projections(self, embeds, utterances_per_speaker, step, out_fpath=None, max_speakers=10)\n\t\n    if self.disable_umap:\n        return\n    if embeds.shape[0] > max_speakers:\n        embeds = embeds[:max_speakers, :]\n        utterances_per_speaker = utterances_per_speaker[:max_speakers]\n    embeds = embeds.reshape(embeds.shape[0] * embeds.shape[1], -1)\n    n_speakers = embeds.shape[0]\n    utterances_per_speaker = np.repeat(utterances_per_speaker, utterances_per_speaker)\n    colors = [plt.cm.inferno_r(i / float(n_speakers)) for i in range(n_speakers)]\n    reducer = umap.UMAP()\n    projected = reducer.fit_transform(embeds)\n    plt.figure(figsize=(12, 9))\n    plt.scatter(projected[:, 0], projected[:, 1], c=colors, s=utterances_per_speaker * 100, alpha=0.5)\n    plt.title(f\"{self.run_id} Step {step}\")\n    plt.gca().set_aspect(\"equal\", \"datalim\")\n    plt.tight_layout()\n    if out_fpath is not None:\n        plt.savefig(out_fpath, bbox_inches=\"tight\", pad_inches=0)\n    else:\n        plt.show()\n    plt.close()\n\n\n", "CodeBleu_score": 51.08852408678022}, {"code": "def draw_projections(self, embeds, utterances_per_speaker, step, out_fpath=None, max_speakers=10)\n\t\n    if self.use_umap:\n        import umap\n        mapper = umap.UMAP().fit(embeds)\n        embeds = mapper.transform(embeds)\n    embeds = embeds[:max_speakers]\n    embeds = embeds.reshape((embeds.shape[0] * embeds.shape[1], -1))\n    speakers = np.repeat(np.arange(len(utterances_per_speaker)), utterances_per_speaker[:len(utterances_per_speaker)], 0)\n    colors = [plt.cm.tab20(i) for i in speakers]\n    if out_fpath is None:\n        if not self.update_progress_bar:\n            return\n        self.progress_bar.update(step)\n        self.progress_bar.set_postfix(message=\"UMAP projections computed. Close figure to continue\")\n        plt.scatter(embeds[:, 0], embeds[:, 1], c=colors)\n        plt.title(f\"UMAP projections (step {step})\")\n        plt.draw()\n        plt.pause(0.001)\n        view()\n    else:\n        plt.scatter(embeds[:, 0], embeds[:, 1], c=colors)\n        plt.title(f\"UMAP projections (step {step})\")\n        plt.savefig(out_fpath)\n        plt.close()\n\n\n", "CodeBleu_score": 46.27891889612251}]}
{"instance_id": "274fa50985c0", "generation_result": [{"code": "def add_speaker_embedding(self, x, speaker_embedding)\n\t\n    batch_size, seq_len, _ = x.size()\n    x = torch.cat([x, speaker_embedding.repeat(1, seq_len, 1)], dim=-1)\n    return x\n\n", "CodeBleu_score": 9.670212960568785}, {"code": "def add_speaker_embedding(self, x, speaker_embedding)\n\t\n    x_shape = tf.shape(x)\n    b = x_shape[0]\n    t = x_shape[1]\n    e = speaker_embedding.shape[1]\n    speaker_embedding_e = tf.broadcast_to(speaker_embedding, [b, t, e])\n    speaker_embedding_e = tf.reshape(speaker_embedding_e, [b, t, e])\n    return tf.concat([x, speaker_embedding_e], 2)\n\n\n\n", "CodeBleu_score": 15.679550113042115}, {"code": "def add_speaker_embedding(self, x, speaker_embedding)\n\t\n    x_shape = tf.shape(x)\n    batch_size, length, _ = x_shape[0], x_shape[1], x_shape[2]\n    speaker_embedding_shape = tf.shape(speaker_embedding)\n    speaker_embedding_dim = speaker_embedding_shape[1]\n    speaker_embedding_tiled = tf.tile(tf.reshape(speaker_embedding, [batch_size, 1, speaker_embedding_dim]), [1, length, 1])\n    return tf.concat([x, speaker_embedding_tiled], axis=-1)\n\n", "CodeBleu_score": 16.80985994558014}]}
{"instance_id": "e6742ea0fe5e", "generation_result": [{"code": "def init_attention(self, encoder_seq_proj)\n\t\n    device = next(self.parameters()).device\n    batch_size, seq_len, _ = encoder_seq_proj.shape\n    self.cumulative = torch.zeros(\n        (batch_size, seq_len), device=device)\n    self.attention = torch.zeros(\n        (batch_size, seq_len), device=device)\n\n\n", "CodeBleu_score": 25.65167357765114}, {"code": "def init_attention(self, encoder_seq_proj)\n\t\n    device = next(self.parameters()).device\n    self.cumulative = torch.zeros(encoder_seq_proj.size(), device=device)\n    self.attention = torch.zeros(encoder_seq_proj.size(), device=device)\n\n", "CodeBleu_score": 24.42925401945782}, {"code": "def init_attention(self, encoder_seq_proj)\n\t\n    self.device = next(self.parameters()).device\n    self.batch_size, self.seq_len = encoder_seq_proj.shape[:2]\n    self.cumulative = torch.zeros(\n        self.batch_size, self.seq_len, device=self.device)\n    self.attention = torch.zeros(\n        self.batch_size, self.seq_len, device=self.device)\n\n", "CodeBleu_score": 29.076910240577913}]}
{"instance_id": "4e1fc1ed0ed7", "generation_result": [{"code": "def load(self, path, optimizer=None)\n\t\n    # Use device of model params as location for loaded state\n    device = next(self.parameters()).device\n    checkpoint = torch.load(str(path), map_location=device)\n    self.load_state_dict(checkpoint[\"model_state\"])\n\n    if \"optimizer_state\" in checkpoint and optimizer is not None:\n        optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n\n", "CodeBleu_score": 67.98839425729693}, {"code": "def load(self, path, optimizer=None)\n\t\n    # Use device of model params as location for loaded state\n    device = next(self.parameters()).device\n    checkpoint = torch.load(str(path), map_location=device)\n    self.load_state_dict(checkpoint[\"model_state\"])\n\n    if \"optimizer_state\" in checkpoint and optimizer is not None:\n        optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n\n", "CodeBleu_score": 67.98839425729693}, {"code": "def load(self, path, optimizer=None)\n\t\n    # Use device of model params as location for loaded state\n    device = next(self.parameters()).device\n    checkpoint = torch.load(str(path), map_location=device)\n    self.load_state_dict(checkpoint[\"model_state\"])\n\n    if \"optimizer_state\" in checkpoint and optimizer is not None:\n        optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n\n", "CodeBleu_score": 67.98839425729693}]}
{"instance_id": "178e6aa16c0f", "generation_result": [{"code": "def save(self, path, optimizer=None)\n\t\n    if optimizer is not None:\n        torch.save({\n            \"model_state\": self.state_dict(),\n            \"optimizer_state\": optimizer.state_dict(),\n        }, str(path))\n    else:\n        torch.save({\n            \"model_state\": self.state_dict(),\n        }, str(path))\n\n", "CodeBleu_score": 93.4714524643972}, {"code": "def save(self, path, optimizer=None)\n\t\n    if optimizer is not None:\n        torch.save({\n            \"model_state\": self.state_dict(),\n            \"optimizer_state\": optimizer.state_dict(),\n        }, str(path))\n    else:\n        torch.save({\n            \"model_state\": self.state_dict(),\n        }, str(path))\n\n", "CodeBleu_score": 93.4714524643972}, {"code": "def save(self, path, optimizer=None)\n\t\n    if optimizer is not None:\n        torch.save({\n            \"model_state\": self.state_dict(),\n            \"optimizer_state\": optimizer.state_dict(),\n        }, str(path))\n    else:\n        torch.save({\n            \"model_state\": self.state_dict(),\n        }, str(path))\n\n", "CodeBleu_score": 93.4714524643972}]}
{"instance_id": "6528fd80e87b", "generation_result": [{"code": "def english_cleaners(text)\n\t\n    text = convert_to_ascii(text)\n    text = lowercase(text)\n    text = expand_numbers(text)\n    text = expand_abbreviations(text)\n    text = collapse_whitespace(text)\n    return text\n\n", "CodeBleu_score": 55.88611500274403}, {"code": "def english_cleaners(text)\n\t\n    text = convert_to_ascii(text)\n    text = lowercase(text)\n    text = expand_numbers(text)\n    text = expand_abbreviations(text)\n    text = collapse_whitespace(text)\n    return text\n\n", "CodeBleu_score": 55.88611500274403}, {"code": "def english_cleaners(text)\n\t\n    text = convert_to_ascii(text)\n    text = lowercase(text)\n    text = expand_numbers(text)\n    text = expand_abbreviations(text)\n    text = collapse_whitespace(text)\n    return text\n\n", "CodeBleu_score": 55.88611500274403}]}
{"instance_id": "8a4fe08e0985", "generation_result": [{"code": "def text_to_sequence(text, cleaner_names)\n\t\n    sequence = []\n\n    # Check for curly braces and treat their contents as ARPAbet:\n    while len(text):\n        if text[0] == \"{\":\n            try:\n                end_idx = text.index(\"}\")\n            except ValueError:\n                raise Exception(\"Missing closing curly brace\")\n            sequence += _arpabet_to_sequence(text[1:end_idx])\n            text = text[end_idx + 1 :]\n            continue\n        else:\n            sequence += _symbols_to_sequence(_clean_text(text, cleaner_names))\n            break\n\n    # Append EOS token\n    sequence.append(_symbol_to_id[\"~\"])\n    return sequence\n\n", "CodeBleu_score": 43.5936401928612}, {"code": "def text_to_sequence(text, cleaner_names)\n\t\n    sequence = []\n\n    # Check for curly braces and treat their contents as ARPAbet:\n    while len(text):\n        m = _curly_re.match(text)\n        if not m:\n            sequence += _symbols_to_sequence(_clean_text(text, cleaner_names))\n            break\n        sequence += _symbols_to_sequence(_clean_text(m.group(1), cleaner_names))\n        sequence += _arpabet_to_sequence(m.group(2))\n        text = m.group(3)\n\n    # Append EOS token\n    sequence.append(_symbol_to_id[\"~\"])\n    return sequence\n\n", "CodeBleu_score": 82.3821513994484}, {"code": "def text_to_sequence(text, cleaner_names)\n\t\n    sequence = []\n    while len(text):\n        m = _curly_re.match(text)\n        if not m:\n            sequence += _symbols_to_sequence(_clean_text(text, cleaner_names))\n            break\n        sequence += _symbols_to_sequence(_clean_text(m.group(1), cleaner_names))\n        sequence += _arpabet_to_sequence(m.group(2))\n        text = m.group(3)\n    sequence.append(_symbol_to_id[\"~\"])  # append eos token\n    return sequence\n\n", "CodeBleu_score": 81.18281269556827}]}
{"instance_id": "e1c25a37af1f", "generation_result": [{"code": "def sequence_to_text(sequence)\n\t\n    result = \"\"\n    for symbol_id in sequence:\n        if symbol_id in _id_to_symbol:\n            s = _id_to_symbol[symbol_id]\n            # Enclose ARPAbet back in curly braces:\n            if len(s) > 1:\n                s = \"{%s}\" % s\n            result += s\n    result = result.replace(\"}{\", \" \")\n    return result\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 58.983780361766115}, {"code": "def sequence_to_text(sequence)\n\t\n    text = \"\"\n    for symbol_id in sequence:\n        if symbol_id in _id_to_symbol:\n            s = _id_to_symbol[symbol_id]\n            if len(s) > 1:\n                s = \"{%s}\" % s\n            text += s\n        else:\n            text += \"{%d}\" % symbol_id\n    text = text.replace(\"}{\", \" \")\n    return text\n\n", "CodeBleu_score": 53.483844703445826}, {"code": "def sequence_to_text(sequence)\n\t\n    result = \"\"\n    for symbol_id in sequence:\n        if symbol_id in _id_to_symbol:\n            s = _id_to_symbol[symbol_id]\n            result += \"{\" + s + \"}\" if _should_keep_symbols(s) else s\n        else:\n            continue\n    result = result.replace(\"}{\", \" \")\n    return result\n\n", "CodeBleu_score": 45.38199329172331}]}
{"instance_id": "7807cad4b05d", "generation_result": [{"code": "def embed_utterance(fpaths, encoder_model_fpath)\n\t\n    if not encoder.is_loaded():\n        encoder.load_model(encoder_model_fpath)\n\n    # Compute the speaker embedding of the utterance\n    wav_fpath, embed_fpath = fpaths\n    wav = np.load(wav_fpath)\n    wav = encoder.preprocess_wav(wav)\n    embed = encoder.embed_utterance(wav)\n    np.save(embed_fpath, embed, allow_pickle=False)\n\n", "CodeBleu_score": 95.3261531719067}, {"code": "def embed_utterance(fpaths, encoder_model_fpath)\n\t\n    if not encoder.is_loaded():\n        encoder.load_model(encoder_model_fpath)\n\n    # Compute the speaker embedding of the utterance\n    wav_fpath, embed_fpath = fpaths\n    wav = np.load(wav_fpath)\n    wav = encoder.preprocess_wav(wav)\n    embed = encoder.embed_utterance(wav)\n    np.save(embed_fpath, embed, allow_pickle=False)\n\n", "CodeBleu_score": 95.3261531719067}, {"code": "def embed_utterance(fpaths, encoder_model_fpath)\n\t\n    if not encoder.is_loaded():\n        encoder.load_model(encoder_model_fpath)\n\n    # Compute the speaker embedding of the utterance\n    wav_fpath, embed_fpath = fpaths\n    wav = np.load(wav_fpath)\n    wav = encoder.preprocess_wav(wav)\n    embed = encoder.embed_utterance(wav)\n    np.save(embed_fpath, embed, allow_pickle=False)\n\n", "CodeBleu_score": 95.3261531719067}]}
{"instance_id": "9f860273a325", "generation_result": [{"code": "def create_embeddings(synthesizer_root: Path, encoder_model_fpath: Path, n_processes: int)\n\t\n\n    print(\"Creating embeddings...\")\n    # Verify the existence of the required directories and files\n    if not encoder_model_fpath.exists():\n        raise Exception(\n            f\"Couldn't find encoder at {encoder_model_fpath}. Download it and place it in the models folder.\"\n        )\n    encoder.load_model(encoder_model_fpath)\n\n    # Read the metadata file and get the list of audio files\n    metadata_fpath = synthesizer_root.joinpath(\"train.json\")\n    assert metadata_fpath.exists()\n    metadata = Metadata.from_file(metadata_fpath)\n    wav_fpaths = metadata.files\n    assert len(wav_fpaths) > 0\n\n    # Create the output directories for the embeddings and the train file\n    syn_dir = synthesizer_root.joinpath(\"SV2TTS\")\n    syn_dir.mkdir(exist_ok=True)\n    train_dir = synthesizer_root.joinpath(\"SV2TTS/train_data\")\n    train_dir.mkdir(exist_ok=True)\n\n    # Create a metadata file where the paths to the audio files are stored\n    train_metadata_fpath = train_dir.joinpath(\"train.json\")\n    train_metadata_fpath.write_text(\n        json.dumps(\n            [\n                {\n                    \"audio_file\": str(Path(wav_fpath).with_suffix(\".npy\")),\n                    \"text\": metadata.items[wav_fpath][\"text\"],\n                    \"original_wav\": wav_fpath,\n                }\n                for wav_fpath in wav_fpaths\n            ],\n            indent=2,\n        )\n    )\n\n    # Generate the speaker embeddings\n    print(\"Generating speaker embeddings...\")\n    create_embeddings_func = partial(\n        create_embedding, encoder_model_fpath=encoder_model_fpath\n    )\n    job = Pool(n_processes).imap(create_embeddings_func, wav_fpaths)\n    list(tqdm", "CodeBleu_score": 39.8954357787826}, {"code": "def create_embeddings(synthesizer_root: Path, encoder_model_fpath: Path, n_processes: int)\n\t\n    print(\"Creating embeddings...\")\n    # Verify the existence of required directories and files\n    if not encoder_model_fpath.exists():\n        print(\n            \"Error: Couldn't find encoder model. Download pretrained models from https://github.com/CorentinJ/Real-Time-Voice-Cloning/\")\n        exit(-1)\n    if not synthesizer_root.exists():\n        os.makedirs(synthesizer_root)\n    # Read the metadata file\n    metadata_fpath = synthesizer_root.joinpath(\"train.json\")\n    print(\"Loading metadata at %s...\" % metadata_fpath)\n    with metadata_fpath.open(\"r\") as metadata_file:\n        metadata = json.load(metadata_file)\n        mel_frames = sum([int(meta[\"mel_frames\"]) for meta in metadata])\n        timesteps = sum([int(meta[\"timesteps\"]) for meta in metadata])\n        sample_rate = metadata[0][\"sample_rate\"]\n        hours = timesteps / sample_rate / 3600\n        print(\"The dataset consists of %d utterances, %d mel frames, %d audio timesteps (%.2f hours).\" %\n              (len(metadata), mel_frames, timesteps, hours))\n        print(\"Max input length (text chars): %d\" %\n              max(len(m[\"text\"]) for m in metadata))\n        print(\"Max mel frames length: %d\" % max(int(m[\"mel_frames\"]) for m in metadata))\n        print(\"Max audio timesteps length: %d\" % max(int(m[\"timesteps\"]) for m in metadata))\n    # Create the dataset\n    embed_dir = synthesizer_root.joinpath(\"embeds\")\n    meta_file_train = synthesizer_root.joinpath(\"train.json\")\n    # Create the directory if it doesn't exist\n    embed_dir.mkdir(exist_ok=True)\n    # Read the metadata file\n    with meta_file_train.open(\"r\") as metadata_file:\n        metadata = json.load(metadata_file)\n   ", "CodeBleu_score": 41.96932289247363}, {"code": "def create_embeddings(synthesizer_root: Path, encoder_model_fpath: Path, n_processes: int)\n\t\n    print(\"Creating embeddings...\")\n    synthesizer_root.joinpath(\"encoder/saved_models\").mkdir(exist_ok=True, parents=True)\n    assert encoder_model_fpath.exists()\n    embed_dir = synthesizer_root.joinpath(\"embeds\")\n    embed_dir.mkdir(exist_ok=True, parents=True)\n\n    with encoder_model_fpath.open(\"rb\") as f:\n        encoder_model = pickle.load(f)\n    filenames = [\n        x for x in metadata_to_list(synthesizer_root.joinpath(\"train.txt\")) if is_valid_file(x, synthesizer_root)\n    ]\n\n    embed_names = [embed_dir.joinpath(Path(f).stem + \".npy\") for f in filenames]\n\n    # Gather the input wave filepath and the target output embed filepath\n    filepaths = [(synthesizer_root.joinpath(\"wavs\", f), embed_dir.joinpath(f)) for f in filenames]\n\n    # TODO: improve on the multiprocessing, it's terrible. Disk I/O is the bottleneck here.\n    # Embed the utterances in separate threads\n    func = partial(embed_utterance, encoder_model=encoder_model)\n    job = Pool(n_processes).imap(func, filepaths)\n    list(tqdm(job, \"Embedding\", len(filenames), unit=\"utterances\"))\n\n", "CodeBleu_score": 51.47579075558022}]}
{"instance_id": "9eb0abda14a9", "generation_result": [{"code": "def eval_model(attention, mel_prediction, target_spectrogram, input_seq, step,\n               plot_dir, mel_output_dir, wav_dir, sample_num, loss, hparams)\n\t\n    # Save attention plot\n    save_attention(attention, os.path.join(plot_dir, \"att-step{:09d}.png\".format(step)))\n    # Save predicted mel spectrogram\n    np.save(os.path.join(mel_output_dir, \"mel-step{:09d}.npy\".format(step)), mel_prediction)\n    # Save reconstructed waveform\n    wav = audio.inv_mel_spectrogram(mel_prediction.T, hparams)\n    audio.save_wav(wav, os.path.join(wav_dir, \"step{:09d}_{}.wav\".format(step, sample_num)))\n    # Save plot comparing the predicted and target mel spectrograms\n    plot_spectrogram(pred_spectrogram=mel_prediction, path=os.path.join(plot_dir, \"mel-step{:09d}_{}.png\".format(step, sample_num)),\n                     target_spectrogram=target_spectrogram,\n                     title=\"{} Step {:09d} Loss={:.6f}\".format(hparams.model, step, loss),\n                     split_title=True,\n                     max_len=target_spectrogram.shape[0] // hparams.hop_size)\n    # Print input sequence at current step\n    print(\"Input: {}\".format(sequence_to_text(input_seq)))\n\n", "CodeBleu_score": 41.74635143104041}, {"code": "def eval_model(attention, mel_prediction, target_spectrogram, input_seq, step,\n               plot_dir, mel_output_dir, wav_dir, sample_num, loss, hparams)\n\t\n    # Save attention plot\n    save_attention(attention, f\"{plot_dir}/att-{sample_num:06d}-{step}.png\")\n\n    # Save predicted mel spectrogram\n    np.save(f\"{mel_output_dir}/mel-{sample_num:06d}-{step}.npy\", mel_prediction)\n    plot_spectrogram(mel_prediction, f\"{plot_dir}/mel-{sample_num:06d}-{step}.png\",\n                     title=f\"{hparams.builder}-{time_string()} Step {step} Loss {loss}\",\n                     target_spectrogram=target_spectrogram, max_len=mel_prediction.shape[0],\n                     split_title=True)\n\n    # Save reconstructed waveform\n    wav = audio.inv_mel_spectrogram(mel_prediction.T, hparams)\n    audio.save_wav(wav, f\"{wav_dir}/wav-{sample_num:06d}-{step}.wav\", hparams)\n\n    # Print input sequence\n    print(f\"{sample_num} {sequence_to_text(input_seq)}\")\n\n", "CodeBleu_score": 28.855705550951793}, {"code": "def eval_model(attention, mel_prediction, target_spectrogram, input_seq, step,\n               plot_dir, mel_output_dir, wav_dir, sample_num, loss, hparams)\n\t\n    # Saving attention\n    save_attention(attention, os.path.join(plot_dir, \"attn-step{:09d}.png\".format(step)))\n    # Saving mel-spectrogram\n    np.save(os.path.join(mel_output_dir, \"mel-step{:09d}.npy\".format(step)), mel_prediction)\n    # Saving wav\n    wav = audio.inv_mel_spectrogram(mel_prediction.T)\n    audio.save_wav(wav, os.path.join(wav_dir, \"step{:09d}_{}.wav\".format(step, sample_num)))\n    # Saving plot\n    plot_spectrogram(mel_prediction, os.path.join(plot_dir, \"mel-step{:09d}_{}.png\".format(step, sample_num)),\n                     title=\"{} Step {:09d} Loss {:.6f}\".format(hparams.model, step, loss),\n                     target_spectrogram=target_spectrogram,\n                     max_len=mel_prediction.shape[0] // hparams.hop_size,\n                     auto_aspect=False)\n    # Printing input sequence\n    print(\"{} > {}\".format(step, sequence_to_text(input_seq)))\n\n", "CodeBleu_score": 39.07130390045695}]}
{"instance_id": "6bad4f500286", "generation_result": [{"code": "def num_params(self)\n\t\n    model_params = [\n        p for p in self.model.parameters() if p.requires_grad\n    ]\n    return sum(p.numel() for p in model_params) / 1e6", "CodeBleu_score": 19.332215823568085}, {"code": "def num_params(self)\n\t\n    model_parameters = filter(lambda p: p.requires_grad, self.parameters())\n    params = sum([np.prod(p.size()) for p in model_parameters])\n    print(\"Trainable Parameters: {} Million\".format(round(params/1000000, 3)))\n\n\n", "CodeBleu_score": 37.985501365539335}, {"code": "def num_params(self)\n\t\n    model_parameters = filter(lambda p: p.requires_grad, self.parameters())\n    params = sum([np.prod(p.size()) for p in model_parameters])\n    print(\"Number of trainable parameters: {} M\".format(round(params/1e6, 3)))", "CodeBleu_score": 36.98919820839961}]}
{"instance_id": "78b876054bd4", "generation_result": [{"code": "def pad_tensor(self, x, pad, side='both')\n\t\n    if side == 'both':\n        x_new = torch.zeros((x.shape[0], x.shape[1] + 2 * pad, x.shape[2]))\n        if torch.cuda.is_available():\n            x_new = x_new.cuda()\n        x_new[:, pad:-pad, :] = x\n    elif side == 'after':\n        x_new = torch.zeros((x.shape[0], x.shape[1] + pad, x.shape[2]))\n        if torch.cuda.is_available():\n            x_new = x_new.cuda()\n        x_new[:, :-pad, :] = x\n    elif side == 'before':\n        x_new = torch.zeros((x.shape[0], x.shape[1] + pad, x.shape[2]))\n        if torch.cuda.is_available():\n            x_new = x_new.cuda()\n        x_new[:, pad:, :] = x\n    return x_new\n\n\n\n\n\n", "CodeBleu_score": 38.77326189040038}, {"code": "def pad_tensor(self, x, pad, side='both')\n\t\n    if x is None:\n        return None\n    if pad == 0:\n        return x\n    if side == 'before':\n        new_size = (x.shape[0] + pad,) + x.shape[1:]\n        if x.is_cuda:\n            new_x = torch.zeros(new_size, device=x.device)\n        else:\n            new_x = torch.zeros(new_size)\n        new_x[pad:, ...] = x\n    elif side == 'after':\n        new_size = (x.shape[0] + pad,) + x.shape[1:]\n        if x.is_cuda:\n            new_x = torch.zeros(new_size, device=x.device)\n        else:\n            new_x = torch.zeros(new_size)\n        new_x[:-pad, ...] = x\n    else:\n        new_size = (x.shape[0] + 2 * pad,) + x.shape[1:]\n        if x.is_cuda:\n            new_x = torch.zeros(new_size, device=x.device)\n        else:\n            new_x = torch.zeros(new_size)\n        new_x[pad:-pad, ...] = x\n    return new_x\n\n", "CodeBleu_score": 28.398749305159154}, {"code": "def pad_tensor(self, x, pad, side='both')\n\t\n    if side == 'before':\n        x = torch.cat((torch.zeros((x.shape[0], pad, x.shape[2])).to(x.device), x), dim=1)\n    elif side == 'after':\n        x = torch.cat((x, torch.zeros((x.shape[0], pad, x.shape[2])).to(x.device)), dim=1)\n    elif side == 'both':\n        x = torch.cat((torch.zeros((x.shape[0], pad, x.shape[2])).to(x.device), x, torch.zeros((x.shape[0], pad, x.shape[2])).to(x.device)), dim=1)\n    return x\n\n", "CodeBleu_score": 31.737890486714825}]}
{"instance_id": "a7ed1715a3df", "generation_result": [{"code": "def save_attention(attn, path)\n\t\n    import matplotlib.pyplot as plt\n    fig = plt.figure(figsize=(12, 6))\n    plt.imshow(attn, aspect='auto', origin='lower', interpolation='none')\n    fig.savefig(f'{path}.png', bbox_inches='tight')\n    plt.close(fig)", "CodeBleu_score": 56.630036372554216}, {"code": "def save_attention(attn, path)\n\t\n    import matplotlib.pyplot as plt\n    fig = plt.figure(figsize=(12, 6))\n    plt.imshow(attn.T, interpolation='nearest', aspect='auto')\n    fig.savefig(f'{path}.png', bbox_inches='tight')\n    plt.close(fig)", "CodeBleu_score": 62.46286889331226}, {"code": "def save_attention(attn, path)\n\t\n    import matplotlib.pyplot as plt\n    fig = plt.figure(figsize=(12, 6))\n    plt.imshow(attn.T, interpolation='nearest', aspect='auto')\n    fig.savefig(f'{path}.png', dpi=fig.dpi)\n    plt.close(fig)\n\n\n", "CodeBleu_score": 63.14697515177593}]}
{"instance_id": "19375c9f44a2", "generation_result": [{"code": "def plot(array)\n\t\n    import matplotlib.pyplot as plt\n\n    fig = plt.figure(figsize=(30, 5))\n    ax = fig.add_subplot(111)\n    ax.xaxis.label.set_color('grey')\n    ax.yaxis.label.set_color('grey')\n    ax.xaxis.label.set_fontsize(23)\n    ax.yaxis.label.set_fontsize(23)\n    ax.tick_params(axis='x', colors='grey', labelsize=23)\n    ax.tick_params(axis='y', colors='grey', labelsize=23)\n    plt.plot(array)\n\n", "CodeBleu_score": 77.73415655580472}, {"code": "def plot(array)\n\t\n    import matplotlib.pyplot as plt\n\n    fig = plt.figure(figsize=(30, 5))\n    ax = fig.add_subplot(111)\n    ax.xaxis.label.set_color(\"grey\")\n    ax.yaxis.label.set_color(\"grey\")\n    ax.xaxis.label.set_fontsize(23)\n    ax.yaxis.label.set_fontsize(23)\n    ax.tick_params(axis=\"x\", colors=\"grey\", labelsize=23)\n    ax.tick_params(axis=\"y\", colors=\"grey\", labelsize=23)\n    plt.plot(array)\n\n", "CodeBleu_score": 64.9396240586664}, {"code": "def plot(array)\n\t\n    import matplotlib.pyplot as plt\n\n    fig = plt.figure(figsize=(30, 5))\n    ax = fig.add_subplot(111)\n    ax.xaxis.label.set_color('grey')\n    ax.yaxis.label.set_color('grey')\n    ax.xaxis.label.set_fontsize(23)\n    ax.yaxis.label.set_fontsize(23)\n    ax.tick_params(axis='x', colors='grey', labelsize=23)\n    ax.tick_params(axis='y', colors='grey', labelsize=23)\n    plt.plot(array)\n\n", "CodeBleu_score": 77.73415655580472}]}
{"instance_id": "ff94c425920c", "generation_result": [{"code": "def write_to_csv(image_name, prediction, confidence)\n\t\n    import csv\n    import os\n\n    # Create a dictionary with the image name, prediction, and confidence\n    data = {'image_name': image_name, 'prediction': prediction, 'confidence': confidence}\n\n    # Open the CSV file in append mode\n    with open('predictions.csv', 'a', newline='') as csvfile:\n        # Create a CSV writer object\n        writer = csv.DictWriter(csvfile, fieldnames=['image_name', 'prediction', 'confidence'])\n\n        # Write the header if the file is new\n        if os.path.getsize('predictions.csv') == 0:\n            writer.writeheader()\n\n        # Write the data as a new row\n        writer.writerow(data)\n\n\n", "CodeBleu_score": 49.82695480710393}, {"code": "def write_to_csv(image_name, prediction, confidence)\n\t\n    import csv\n    import os\n    import pandas as pd\n\n    # Create a dictionary with the image name, prediction, and confidence\n    data = {'image_name': image_name, 'prediction': prediction, 'confidence': confidence}\n\n    # Open the CSV file in append mode\n    with open('predictions.csv', 'a', newline='') as csvfile:\n        # Create a CSV writer object\n        writer = csv.DictWriter(csvfile, fieldnames=['image_name', 'prediction', 'confidence'])\n\n        # Write the header if the file is new\n        if csvfile.tell() == 0:\n            writer.writeheader()\n\n        # Write the data as a new row\n        writer.writerow(data)\n\n", "CodeBleu_score": 49.55921396196648}, {"code": "def write_to_csv(image_name, prediction, confidence)\n\t\n    import csv\n    import os\n    # Create a dictionary with the image name, prediction, and confidence\n    data = {'image_name': image_name, 'prediction': prediction, 'confidence': confidence}\n    # Open the CSV file in append mode\n    with open('predictions.csv', 'a', newline='') as csvfile:\n        # Create a CSV writer object\n        writer = csv.DictWriter(csvfile, fieldnames=['image_name', 'prediction', 'confidence'])\n        # Write the header if the file is new\n        if csvfile.tell() == 0:\n            writer.writeheader()\n        # Write the data as a new row\n        writer.writerow(data)\n\n", "CodeBleu_score": 50.43344560658952}]}
{"instance_id": "436161602009", "generation_result": [{"code": "def export_torchscript(model, im, file, optimize, prefix=colorstr(\"TorchScript:\"))\n\t\n    try:\n        import torch\n\n        LOGGER.info(f\"", "CodeBleu_score": 10.027696614025022}, {"code": "def export_torchscript(model, im, file, optimize, prefix=colorstr(\"TorchScript:\"))\n\t\n    try:\n        LOGGER.info(f\"", "CodeBleu_score": 9.912281340461456}, {"code": "def export_torchscript(model, im, file, optimize, prefix=colorstr(\"TorchScript:\"))\n\t\n    try:\n        LOGGER.info(f\"{prefix} starting export with torch {torch.__version__}...\")\n        f = file.with_suffix(\".torchscript.pt\")  # filename\n        if optimize:\n            LOGGER.info(f\"{prefix} optimizing with {optimize}...\")\n            torch.jit.save(torch.jit.optimize_for_mobile(torch.jit.trace(model, im)), f)\n        else:\n            LOGGER.info(f\"{prefix} saving model...\")\n            torch.jit.save(torch.jit.trace(model, im), f)\n        LOGGER.info(f\"{prefix} export success, saved as {f}\")\n        return f, None\n    except Exception as e:\n        LOGGER.warning(f\"{prefix} export failure: {e}\")\n        return f, e\n", "CodeBleu_score": 38.83451436689913}]}
{"instance_id": "78335487ac1c", "generation_result": [{"code": "def export_coreml(model, im, file, int8, half, nms, prefix=colorstr(\"CoreML:\")):\n\t\n    try:\n        import coremltools as ct\n\n        LOGGER.info(f\"{prefix} starting export with coremltools {ct.__version__}...\")\n        f = file.with_suffix(\".mlmodel\")  # filename\n        model.fuse()  # fuse Conv2d + BatchNorm2d layers\n        model.to(device).eval()  # update model\n        for k, m in model.named_modules():\n            if isinstance(m, Conv):  # assign export-friendly activations\n                if isinstance(m.act, nn.Hardswish):\n                    m.act = Hardswish()\n                elif isinstance(m.act, nn.SiLU):\n                    m.act = Swish()\n        model.model[-1].export = True  # set Detect() layer export=True\n        _ = model(im)  # dry run\n        mlmodel = ct.convert(\n            model,\n            inputs=[\n                ct.ImageType(name=\"images\", shape=im.shape, scale=1 / 255.0, bias=[0, 0, 0])\n            ],\n        )\n        if int8:\n            mlmodel = quantize_weights(mlmodel, nbits=8)\n        if half:\n            mlmodel = ct.convert(mlmodel, convert_to=\"neuron\")\n        if nms:\n            mlmodel = wrap_nms(mlmodel)\n        mlmodel.save(f)\n        LOGGER.info(f\"{prefix} export success, saved as {f} ({file_size(f):.1f} MB)\")\n        return f, mlmodel\n    except Exception as e:\n        LOGGER.warning(f\"{prefix} export failure: {e}\")\n\n", "CodeBleu_score": 42.03740981205058}, {"code": "def export_coreml(model, im, file, int8, half, nms, prefix=colorstr(\"CoreML:\")):\n\t\n    try:\n        import coremltools as ct\n\n        LOGGER.info(f\"{prefix} starting export with coremltools {ct.__version__}...\")\n        f = file.with_suffix(\".mlmodel\")  # filename\n        model.eval()  # CoreML export\n        for k, m in model.named_modules():\n            if isinstance(m, Detect):  # export Detect() layer\n                m.inplace = False\n                m.onnx_dynamic = False  # ONNX Scatter not supported\n                m.export = True  # CoreML Detect() layer export\n\n        model = model.cpu()\n        model.model[-1].concat = False  # concatenate tensors for export\n        LOGGER.info(f\"{prefix} exporting jit model with {torch.cuda.device_count()} GPUs...\")\n        model = torch.jit.trace(model, im)\n        LOGGER.info(f\"{prefix} optimizing jit model...\")\n        model = torch.jit.optimize_for_inference(model)\n\n        LOGGER.info(f\"{prefix} exporting CoreML model to {f}...\")\n        example_output = model(im) if isinstance(im, torch.Tensor) else model(im[0][None])\n        proto = (\n            ct.convert(\n                model,\n                inputs=[ct.ImageType(name=\"images\", shape=im.shape, scale=1 / 255.0, bias=[0, 0, 0])],\n                outputs=[\n                    ct.ImageType(name=\"output\", shape=example_output.shape, scale=1 / 255.0, bias=[0, 0, 0]),\n                    ct.ArrayType(name=\"boxes\", shape=example_output.shape[:2]),\n                    ct.ArrayType(name=\"scores\", shape=example_output.shape[:2]),\n                    ct.ArrayType(name=\"labels\", shape=example_output.shape[:2]),\n                ],\n            )\n            if nms\n            else", "CodeBleu_score": 39.79427113292888}, {"code": "def export_coreml(model, im, file, int8, half, nms, prefix=colorstr(\"CoreML:\")):\n\t\n    try:\n        import coremltools as ct\n\n        LOGGER.info(f\"{prefix} starting export with coremltools {ct.__version__}...\")\n        f = file.with_suffix(\".mlmodel\")  # file name\n        model.fuse()  # fuse model layers\n        model.model[-1].export = True  # set Detect() layer export=True\n        im = im.to(torch.float32)  # CoreML quantization requires full-precision\n        model = model.to(torch.float32)\n        mlmodel = ct.convert(\n            model,\n            inputs=[ct.ImageType(name=\"images\", shape=im.shape, scale=1 / 255.0, bias=[0, 0, 0])],\n        )\n        if nms:  # CoreML NMS\n            mlmodel = ct.models.neural_network.mlprogram.run_flexnets_optimization_pass(mlmodel, \"MILNMS\")\n        if int8 and mlmodel.get_spec().target_needs_default_cpu_ios_backend():\n            LOGGER.info(f\"{prefix} applying CoreML INT8 quantization...\")\n            mlmodel = ct.models.neural_network.quantization.quantize_weights(mlmodel, nbits=8)\n        if half:\n            LOGGER.info(f\"{prefix} applying CoreML FP16 quantization...\")\n            mlmodel = mlmodel.convert(precision=ct.precision.16bit)\n        mlmodel.save(f)\n        LOGGER.info(f\"{prefix} export success \u2705\")\n        return f, mlmodel\n    except Exception as e:\n        LOGGER.info(f\"{prefix} export failure \u274c\")\n        LOGGER.info(f\"{prefix} {e}\")\n        return None, None\n\n", "CodeBleu_score": 46.064456517788535}]}
{"instance_id": "80233cf71580", "generation_result": [{"code": "def generate_individual(input_ranges, individual_length)\n\t\n    individual = []\n    for i in range(individual_length):\n        individual.append(\n            random.uniform(input_ranges[i][0], input_ranges[i][1]))\n    return individual\n\n", "CodeBleu_score": 48.68218168541654}, {"code": "def generate_individual(input_ranges, individual_length)\n\t\n    individual = []\n    for i in range(individual_length):\n        individual.append(\n            random.uniform(input_ranges[i][0], input_ranges[i][1]))\n    return individual\n\n", "CodeBleu_score": 48.68218168541654}, {"code": "def generate_individual(input_ranges, individual_length)\n\t\n    individual = []\n    for i in range(individual_length):\n        individual.append(random.uniform(input_ranges[i][0], input_ranges[i][1]))\n    return individual\n\n\n\n", "CodeBleu_score": 47.630972577458174}]}
{"instance_id": "5ea979cb4a8e", "generation_result": [{"code": "def marginal_log_mean_coeff(self, t)\n\t\n    if self.schedule_type == \"discrete\":\n        return torch.log(self.alpha_s[t])\n    elif self.schedule_type == \"linear\":\n        return torch.log(self.alpha_s[0]) + 0.5 * t * (\n            torch.log(self.alpha_s[1]) - torch.log(self.alpha_s[0])\n        )\n    elif self.schedule_type == \"cosine\":\n        return torch.log(self.alpha_s[0]) - torch.cos(\n            (t + 0.008) / (1 + self.T) * pi / 2\n        ) * 0.5 * torch.log(self.alpha_s[0] / self.alpha_s[-1])\n    else:\n        raise ValueError(\"Invalid schedule type\")\n\n", "CodeBleu_score": 31.816567285640694}, {"code": "def marginal_log_mean_coeff(self, t)\n\t\n    if self.schedule == \"discrete\":\n        log_alpha_t = torch.log(self.alpha_s[t])\n    elif self.schedule == \"linear\":\n        log_alpha_t = torch.log(self.alpha_s[t])\n    elif self.schedule == \"cosine\":\n        log_alpha_t = torch.log(\n            interpolate_fn(\n                torch.tensor([t], device=self.device, dtype=torch.float32),\n                torch.tensor([0, self.num_timesteps], device=self.device, dtype=torch.float32),\n                torch.tensor([self.log_alpha_s[0], self.log_alpha_s[-1]], device=self.device, dtype=torch.float32),\n            )\n        )\n    return log_alpha_t\n\n", "CodeBleu_score": 40.82975363258633}, {"code": "def marginal_log_mean_coeff(self, t)\n\t\n    if self.schedule_type == \"discrete\":\n        return torch.log(self.alpha_t[t])\n    elif self.schedule_type == \"linear\":\n        return torch.log(self.alpha_t[t]) - torch.log(self.alpha_t[t - 1])\n    elif self.schedule_type == \"cosine\":\n        return torch.log(self.alpha_t[t]) - torch.log(self.alpha_t[t - 1]) + torch.log(\n            self.alpha_bar[t - 1] / self.alpha_bar[t]\n        )\n    else:\n        raise NotImplementedError\n\n", "CodeBleu_score": 29.338746667230442}]}
{"instance_id": "7da4fae5a278", "generation_result": [{"code": "def inverse_lambda(self, lamb)\n\t\n    if self.schedule == \"linear\":\n        return (torch.sqrt(self.beta_0 ** 2 + 2 * (lamb - self.log_snr_max) * (self.beta_1 - self.beta_0)) - self.beta_0) / (self.beta_1 - self.beta_0) * self.T\n    elif self.schedule == \"quadratic\":\n        return (torch.sqrt(self.beta_0 ** 2 + 2 * (lamb - self.log_snr_max) * (self.beta_1 - self.beta_0)) - self.beta_0) / (self.beta_1 - self.beta_0) * self.T\n    elif self.schedule == \"sigmoid\":\n        return (torch.sqrt(self.beta_0 ** 2 + 2 * (lamb - self.log_snr_max) * (self.beta_1 - self.beta_0)) - self.beta_0) / (self.beta_1 - self.beta_0) * self.T\n    elif self.schedule == \"vp\":\n        return (torch.sqrt(self.beta_0 ** 2 + 2 * (lamb - self.log_snr_max) * (self.beta_1 - self.beta_0)) - self.beta_0) / (self.beta_1 - self.beta_0) * self.T\n    elif self.schedule == \"vp-fast\":\n        return (torch.sqrt(self.beta_0 ** 2 + 2 * (lamb - self.log_snr_max) * (self.beta_1 - self.beta_0)) - self.beta_0) / (self.beta_1 - self.beta_0) * self.T\n    elif self.schedule == \"vp-slow\":\n        return (torch.sqrt(self.beta_0 ** 2 + 2 * (lamb - self.log_snr_max) * (self.beta_1 - self.beta_0)) - self.beta_0) / (self.beta_1 - self.beta_0) * self.T\n    elif", "CodeBleu_score": 21.72789577338918}, {"code": "def inverse_lambda(self, lamb)\n\t\n    if self.schedule == \"linear\":\n        t = (lamb - self.beta_0) / (self.beta_1 - self.beta_0)\n    elif self.schedule == \"quad\":\n        t = ((lamb ** 2) - self.beta_0) / (self.beta_1 ** 2 - self.beta_0)\n    elif self.schedule == \"sigmoid\":\n        t = torch.log(lamb - self.beta_0) - torch.log(self.beta_1 - lamb)\n        t = t / self.sigmoid_scale\n    elif self.schedule == \"step\":\n        t = torch.floor(lamb)\n    else:\n        log_alpha = self.log_alpha_fn(lamb)\n        t = torch.arccos(1 - 2 * log_alpha) / torch.pi\n    return t\n\n", "CodeBleu_score": 28.634656838350242}, {"code": "def inverse_lambda(self, lamb)\n\t\n    if self.schedule == \"linear\":\n        beta_start = self.beta_start\n        beta_end = self.beta_end\n        t = (torch.sqrt(beta_start ** 2 + 4 * lamb * beta_start) - beta_start) / (2 * lamb)\n    elif self.schedule == \"quad\":\n        beta_start = self.beta_start\n        beta_end = self.beta_end\n        t = (torch.sqrt(beta_start ** 2 + 4 * lamb * beta_start) - beta_start) / (2 * lamb)\n        t = t ** 2\n    elif self.schedule == \"sigmoid\":\n        lamb = self.sigmoid(lamb)\n        log_alpha = -1.6094379124341003 + 1.6094379124341003 * lamb\n        t = self.inverse_sigmoid(log_alpha)\n    elif self.schedule == \"cosine\":\n        lamb = self.cosine(lamb)\n        log_alpha = -3.2188758248682006 + 3.2188758248682006 * lamb\n        t = self.inverse_cosine(log_alpha)\n    else:\n        raise ValueError(f\"Schedule {self.schedule} not recognized.\")\n    return t\n\n", "CodeBleu_score": 28.779292619480252}]}
{"instance_id": "a519c853a2b2", "generation_result": [{"code": "def noise_pred_fn(x, t_continuous, cond=None)\n\t\n    t_continuous = expand_dims(t_continuous, x.ndim)\n    if cond is not None:\n        cond = expand_dims(cond, x.ndim)\n    model_output = model(x, t_continuous, cond=cond)\n    if model_type == 'score':\n        return model_output\n    elif model_type == 'epsilon':\n        return (x - model_output) / extract(sqrt_alphas_cumprod, t_continuous, x.shape)\n    elif model_type == 'fixed_large':\n        return model_output\n    elif model_type == 'fixed_small':\n        return model_output\n    else:\n        raise ValueError(model_type)\n\n", "CodeBleu_score": 21.63612168497291}, {"code": "def noise_pred_fn(x, t_continuous, cond=None)\n\t\n    # Expand the time input to match the batch size\n    t_expanded = expand_dims(t_continuous, x.ndim)\n\n    # Apply the model to the input and time\n    if model_type == 'eps':\n        model_output = model(x, t_expanded)\n    elif model_type == 'v':\n        model_output = model(x, t_expanded)\n        model_output = noise_schedule.marginal_std(t_continuous) * model_output\n    else:\n        raise ValueError(f\"Unknown model type: {model_type}\")\n\n    # Apply the condition if provided\n    if cond is not None:\n        model_output += cond\n\n    # Apply the bias if provided\n    if model_output_type == 'x0':\n        model_output = model_output - noise_schedule.marginal_std(t_continuous) * x\n    elif model_output_type == 'eps':\n        model_output = model_output\n    else:\n        raise ValueError(f\"Unknown model output type: {model_output_type}\")\n\n    return model_output\n\n", "CodeBleu_score": 26.646678049444105}, {"code": "def noise_pred_fn(x, t_continuous, cond=None)\n\t\n    t_continuous = expand_dims(t_continuous, x.ndim)\n    if cond is not None:\n        cond = expand_dims(cond, x.ndim)\n\n    if isinstance(model, torch.nn.Module):\n        return model(x, t_continuous, cond)\n    elif isinstance(model, torch.nn.ModuleList):\n        if cond is not None:\n            cond = expand_dims(cond, x.ndim)\n        return model[0](x, t_continuous, cond)\n    elif isinstance(model, torch.nn.ModuleDict):\n        if cond is not None:\n            cond = expand_dims(cond, x.ndim)\n        return model['model'](x, t_continuous, cond)\n    elif callable(model):\n        return model(x, t_continuous, cond)\n    else:\n        raise ValueError(f\"The model type {type(model)} is not supported.\")\n\n", "CodeBleu_score": 19.998635005435617}]}
{"instance_id": "96a9b7d54823", "generation_result": [{"code": "def dynamic_thresholding_fn(self, x0, t=None)\n\t\n    x0 = x0.view(-1)\n    x0_abs = x0.abs()\n    x0_abs_quantile = x0_abs.quantile(self.quantile)\n    x0_abs_quantile = expand_dims(x0_abs_quantile, len(x0.shape))\n    x0_abs_scaled = x0_abs / x0_abs_quantile\n    x0_abs_clamped = x0_abs_scaled.clamp(max=1.0)\n    x0_abs_clamped = x0_abs_clamped * x0_abs_quantile\n    x0_clamped = x0.sign() * x0_abs_clamped\n    x0_clamped = x0_clamped.view(x0.shape)\n    return x0_clamped\n\n", "CodeBleu_score": 16.660839418746974}, {"code": "def dynamic_thresholding_fn(self, x0, t=None)\n\t\n    if t is None:\n        t = self.t\n    x0 = x0.view(-1)\n    x0_abs = x0.abs()\n    x0_abs_quantile = x0_abs.quantile(t)\n    scale = 1.0 / (1.0 - t)\n    x0_abs_clamp = x0_abs.clamp(max=x0_abs_quantile)\n    x0_abs_clamp_scale = x0_abs_clamp * scale\n    x0_abs_clamp_scale_reshape = x0_abs_clamp_scale.view(x0.shape)\n    return x0_abs_clamp_scale_reshape\n\n", "CodeBleu_score": 31.31768279217248}, {"code": "def dynamic_thresholding_fn(self, x0, t=None)\n\t\n    if t is None:\n        t = self.t\n    x0 = x0.view(-1)\n    x0_abs = x0.abs()\n    x0_abs_sorted = x0_abs.sort().values\n    x0_abs_quantile = x0_abs_sorted[int(t * x0_abs_sorted.numel())]\n    scale = x0_abs_quantile / 2\n    x0_clamped = x0_abs.clamp(min=scale)\n    x0_scaled = x0 / x0_clamped * scale\n    return x0_scaled\n\n", "CodeBleu_score": 30.182037303488002}]}
{"instance_id": "59c6ec6d7a6a", "generation_result": [{"code": "def data_prediction_fn(self, x, t)\n\t\n    # Calculate the threshold value based on the dynamic thresholding ratio and maximum threshold value\n    threshold = self.dynamic_threshold_ratio * self.max_threshold\n\n    # Clamp the input tensor values within the threshold\n    x = torch.clamp(x, min=-threshold, max=threshold)\n\n    # Normalize the clamped values\n    x = x / threshold\n\n    return x\n", "CodeBleu_score": 9.87447104792731}, {"code": "def data_prediction_fn(self, x, t)\n\t\n    x0 = self.model(x, t)\n    dynamic_threshold_ratio = self.dynamic_threshold_ratio\n    max_threshold = self.max_threshold\n    threshold = dynamic_threshold_ratio * max_threshold\n    x0 = torch.clamp(x0, -threshold, threshold)\n    x0 = x0 / max_threshold\n    return x0\n\n", "CodeBleu_score": 14.496313824544432}, {"code": "def data_prediction_fn(self, x, t)\n\t\n    threshold = self.dynamic_thresholding_ratio * self.max_threshold\n    x0 = x[:, 0]\n    x0 = torch.clamp(x0, min=-threshold, max=threshold)\n    x0 = x0 / threshold\n    return x0\n", "CodeBleu_score": 11.970883018724376}]}
{"instance_id": "65e3618468a2", "generation_result": [{"code": "def predict_eps_sigma(model, input, sigma_in, **kwargs)\n\t\n    sigma_in = sigma_in.reshape(input.shape)\n    scaled_input = input * sigma_in\n    prediction = model(scaled_input, **kwargs)\n    return prediction * sigma_in\n\n", "CodeBleu_score": 24.243709493255057}, {"code": "def predict_eps_sigma(model, input, sigma_in, **kwargs)\n\t\n    sigma_in = sigma_in.reshape((1, 1, 1, -1))\n    scaled_input = input / sigma_in\n    prediction = model(scaled_input, **kwargs)\n    adjusted_prediction = prediction * sigma_in\n    return adjusted_prediction", "CodeBleu_score": 19.333456862544875}, {"code": "def predict_eps_sigma(model, input, sigma_in, **kwargs)\n\t\n    sigma = sigma_in.reshape(input.shape[0], 1, 1, 1)\n    input = input * sigma\n    prediction = model(input, **kwargs)\n    return prediction * sigma + input\n\n\n", "CodeBleu_score": 27.67985786163225}]}
{"instance_id": "b35846dfbc66", "generation_result": [{"code": "def cal_intergrand(beta_0, beta_1, taus)\n\t\n    alpha = t2alpha_fn(beta_0, beta_1, taus)\n    log_alpha = torch.log(alpha)\n    log_alpha.backward(torch.ones_like(log_alpha), retain_graph=True)\n    grad_log_alpha = taus.grad\n    taus.grad = None\n    return alpha * grad_log_alpha\n\n", "CodeBleu_score": 18.74893813304664}, {"code": "def cal_intergrand(beta_0, beta_1, taus)\n\t\n    alpha = t2alpha_fn(beta_0, beta_1, taus)\n    log_alpha = torch.log(alpha)\n    log_alpha.backward(gradient=torch.ones_like(log_alpha))\n    grad_log_alpha = taus.grad\n    taus.grad = None\n    return (alpha * grad_log_alpha).detach()\n\n", "CodeBleu_score": 22.774391847947285}, {"code": "def cal_intergrand(beta_0, beta_1, taus)\n\t\n    alpha = t2alpha_fn(beta_0, beta_1, taus)\n    log_alpha = torch.log(alpha)\n    log_alpha.backward(gradient=torch.ones_like(log_alpha))\n    grad = taus.grad\n    return (log_alpha - grad).detach()\n\n", "CodeBleu_score": 19.70033795077086}]}
{"instance_id": "5f3667566a10", "generation_result": [{"code": "def sample_euler_ancestral(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None)\n\t\n    extra_args = {} if extra_args is None else extra_args\n    s_in = x.new_ones([x.shape[0]])\n    sigmas_forward = torch.cat([sigmas[0:1], sigmas])\n    if noise_sampler is None:\n        noise_sampler = default_noise_sampler\n    if callback is None:\n        callback = lambda x, i, sigma: x\n    for i in trange(len(sigmas), disable=disable):\n        with torch.enable_grad():\n            x = x.detach().requires_grad_()\n            d = to_d(x, sigmas[i], model(x, sigma_from=sigmas_forward[i], sigma_to=sigmas_forward[i + 1], extra_args=extra_args))\n            grad = (d * sigmas_forward[i + 1] / sigmas_forward[i] + noise_sampler(x, sigma_from=sigmas_forward[i], sigma_to=sigmas_forward[i + 1]) * s_noise)\n            dt = sigmas_forward[i + 1] - sigmas_forward[i]\n            x = x + grad * dt\n        x = callback(x, i, sigmas[i])\n    return x\n\n", "CodeBleu_score": 46.38530062423787}, {"code": "def sample_euler_ancestral(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None)\n\t\n    extra_args = {} if extra_args is None else extra_args\n    s_in = x.new_ones([x.shape[0]])\n    if noise_sampler is None:\n        noise_sampler = default_noise_sampler\n    if callback is None:\n        callback = lambda x, i, sigma, s_noise, s_in, **kwargs: x\n    for i in trange(len(sigmas) - 1, disable=disable):\n        with torch.enable_grad():\n            x = x.detach().requires_grad_()\n            d = to_d(x, sigmas[i], model(x, sigmas[i] * s_in, **extra_args))\n            denoised = model(x, sigmas[i] * s_in, **extra_args)\n            d = d * sigmas[i + 1] / sigmas[i]\n            dt = sigmas[i + 1] - sigmas[i]\n            x = x + d * dt\n            x = callback(x, i, sigmas[i], s_noise, s_in, denoised=denoised, **extra_args)\n            sigma_down, sigma_up = get_ancestral_step(sigmas[i], sigmas[i + 1], eta=eta)\n            mask = 1 - (sigma_down / sigmas[i + 1])\n            mask = mask.cumprod(dim=0)\n            mask = mask.roll(1, 0)\n            mask[0] = 1.\n            s_in = 1. / mask\n            s_in = s_in * s_noise\n            x = x + noise_sampler(x, sigma_up) * s_in\n    return x\n\n", "CodeBleu_score": 52.4879035518338}, {"code": "def sample_euler_ancestral(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None)\n\t\n    extra_args = {} if extra_args is None else extra_args\n    s_in = x.new_ones([x.shape[0]])\n    if noise_sampler is None:\n        noise_sampler = default_noise_sampler\n    if callback is None:\n        callback = lambda x, sigma, **kwargs: x\n    with torch.enable_grad():\n        x = x.detach().requires_grad_()\n    for sigma in tqdm(sigmas, disable=disable):\n        sigma_next = sigma_to(sigma, eta)\n        sigma_down, sigma_up = get_ancestral_step(sigma, sigma_next, eta)\n        denoised = model(x, sigma, **extra_args)\n        d = to_d(x, sigma, denoised)\n        if sigma_down == 0:\n            x = x + d * sigma_down\n        else:\n            x = x + (d + noise_to_sigma(x, sigma_down, noise_sampler)) * sigma_down\n        x = x + noise_to_sigma(x, sigma_up, noise_sampler) * sigma_up\n        x = callback(x, sigma)\n    return x", "CodeBleu_score": 43.30405255097036}]}
{"instance_id": "6261ded88df2", "generation_result": [{"code": "def linear_multistep_coeff(order, t, i, j)\n\t\n    if order > i:\n        raise ValueError(\"Order is too high for the step index.\")\n\n    def product_term(t):\n        result = 1\n        for k in range(order):\n            if k != j:\n                result *= (t - t[i + k]) / (t[i + j] - t[i + k])\n        return result\n\n    integral, _ = quad(product_term, t[i], t[i + 1])\n    return integral\n\n", "CodeBleu_score": 55.185167703856244}, {"code": "def linear_multistep_coeff(order, t, i, j)\n\t\n    if order < j:\n        raise ValueError(\n            f\"Order {order} must be at least {j} for the linear multistep method.\"\n        )\n\n    def product_term(tau):\n        prod = 1\n        for k in range(j):\n            prod *= (tau - t[i + k]) / (t[i + j] - t[i + k])\n        return prod\n\n    integral, _ = quad(product_term, t[i], t[i + 1])\n    return integral\n\n", "CodeBleu_score": 47.883879674876326}, {"code": "def linear_multistep_coeff(order, t, i, j)\n\t\n    def product_term(t):\n        product = 1\n        for k in range(order):\n            if k != j:\n                product *= (t - t[i+k]) / (t[i+j] - t[i+k])\n        return product\n\n    if j >= order:\n        raise ValueError(\"Order is too high for the step index.\")\n\n    return integrate.quad(product_term, t[i], t[i+1])[0]\n\n\n\n\n\n", "CodeBleu_score": 47.383315825751396}]}
{"instance_id": "4ee025e33ab1", "generation_result": [{"code": "def sample_lcm(model, x, sigmas, extra_args=None, callback=None, disable=None, noise_sampler=None)\n\t\n    extra_args = {} if extra_args is None else extra_args\n    s_in = x.new_ones([x.shape[0]])\n    sigma_fn = noise_sampler or default_noise_sampler\n    with torch.enable_grad():\n        for sigma in sigmas:\n            eps = sigma_fn(x, sigma)\n            eps = eps * s_in\n            denoised = model(x, sigma * s_in, **extra_args)\n            denoised = x + (denoised - x) * (sigma / (sigma_next if sigma_next > 0 else sigma))\n            x = denoised + eps * (sigma_next - sigma)\n            if callback is not None:\n                callback({'x': x, 'i': i, 'sigma': sigma, 'sigma_hat': sigma_hat, 'denoised': denoised})\n            s_in = sigma_next\n    return x\n\n", "CodeBleu_score": 44.714671638708495}, {"code": "def sample_lcm(model, x, sigmas, extra_args=None, callback=None, disable=None, noise_sampler=None)\n\t\n    if extra_args is None:\n        extra_args = {}\n    if noise_sampler is None:\n        noise_sampler = default_noise_sampler\n    s_in = x\n    for sigma in sigmas:\n        sigma_next = sigmas[min(sigmas.index(sigma) + 1, len(sigmas) - 1)]\n        model_fn = lambda x, sigma: model(x, sigma, **extra_args)\n        s_out = (\n            diffusion_step(\n                model_fn,\n                s_in,\n                sigma,\n                sigma_next,\n                noise_sampler,\n            )\n            if not disable\n            else model_fn(s_in, sigma)\n        )\n        s_in = s_out\n        if callback is not None:\n            callback({\"x\": x, \"i\": i, \"sigma\": sigma, \"sigma_hat\": sigmas[i], \"denoised\": s_out})\n    return s_out\n\n", "CodeBleu_score": 37.3535062030102}, {"code": "def sample_lcm(model, x, sigmas, extra_args=None, callback=None, disable=None, noise_sampler=None)\n\t\n    extra_args = {} if extra_args is None else extra_args\n    s_in = x.new_ones([x.shape[0]])\n    sigma_fn = noise_sampler or default_noise_sampler\n    with torch.enable_grad():\n        with model.ema_scope():\n            for sigma in tqdm(sigmas, disable=disable):\n                sigma_next = sigma_fn(x, sigma)\n                sigma_next = sigma_next.to(x.device)\n                sigma_next = sigma_next.flatten()\n                sigma_next = sigma_next.repeat(4)\n                sigma_next[0] = 0\n                sigma_next[1] = 0\n                sigma_next[2] = 0\n                sigma_next[3] = 0\n                denoised = model(x, sigma_next, **extra_args)\n                if callback is not None:\n                    callback({'x': x, 'i': i, 'sigma': sigma, 'sigma_next': sigma_next, 'denoised': denoised})\n                if sigma_next != 0:\n                    x = x + (sigma_next / s_in) * (denoised - x)\n                else:\n                    x = denoised\n    return x\n\n", "CodeBleu_score": 40.83421121713147}]}
{"instance_id": "cb51f91da606", "generation_result": [{"code": "def append_dims(x, target_dims)\n\t\n    dims_to_append = target_dims - x.ndim\n    if dims_to_append < 0:\n        raise ValueError(\n            f\"Invalid target dimensions. The tensor has {x.ndim} dimensions, but the target dimensions are {target_dims}.\"\n        )\n    for _ in range(dims_to_append):\n        x = torch.unsqueeze(x, -1)\n    if x.device.type == \"mps\":\n        x = x.detach().clone()\n    return x", "CodeBleu_score": 52.25395485999888}, {"code": "def append_dims(x, target_dims)\n\t\n    dims_to_append = target_dims - x.ndim\n    if dims_to_append < 0:\n        raise ValueError(\n            f\"Invalid target dimensions. The tensor already has {x.ndim} dimensions, but the target dimensions are {target_dims}.\"\n        )\n    for _ in range(dims_to_append):\n        x = torch.unsqueeze(x, dim=-1)\n    if x.device.type == \"mps\":\n        return x.detach().clone()\n    else:\n        return x\n\n", "CodeBleu_score": 49.131471827683015}, {"code": "def append_dims(x, target_dims)\n\t\n    dims_to_append = target_dims - x.ndim\n    if dims_to_append < 0:\n        raise ValueError(f\"Target dimensions ({target_dims}) cannot be fewer than current dimensions ({x.ndim}).\")\n    for _ in range(dims_to_append):\n        x = torch.unsqueeze(x, dim=-1)\n    return x.detach().clone() if str(x.device) == 'mps' else x\n\n\n", "CodeBleu_score": 45.838430141316884}]}
{"instance_id": "9fac874a3bc7", "generation_result": [{"code": "def download_file(path, url, digest=None)\n\t\n    import os\n    import hashlib\n    import urllib.request\n    from tqdm import tqdm\n\n    def calculate_sha256(file_path):\n        sha256_hash = hashlib.sha256()\n        with open(file_path, \"rb\") as f:\n            for byte_block in iter(lambda: f.read(4096), b\"\"):\n                sha256_hash.update(byte_block)\n        return sha256_hash.hexdigest()\n\n    def download_progress_bar(url, file_path):\n        with urllib.request.urlopen(url) as response:\n            total_size = int(response.headers.get(\"Content-Length\", 0))\n            block_size = 1024\n            progress_bar = tqdm(total=total_size, unit=\"iB\", unit_scale=True)\n            with open(file_path, \"wb\") as f:\n                while True:\n                    buffer = response.read(block_size)\n                    if not buffer:\n                        break\n                    f.write(buffer)\n                    progress_bar.update(len(buffer))\n            progress_bar.close()\n\n    def check_sha256(file_path, expected_hash):\n        actual_hash = calculate_sha256(file_path)\n        if actual_hash != expected_hash:\n            raise ValueError(\n                f\"SHA-256 hash mismatch for {file_path}.", "CodeBleu_score": 35.53204701521339}, {"code": "def download_file(path, url, digest=None)\n\t\n    import os\n    import urllib.request\n    import hashlib\n\n    # Ensure the target directory exists\n    os.makedirs(os.path.dirname(path), exist_ok=True)\n\n    # Download the file if it does not exist\n    if not os.path.exists(path):\n        print(f\"Downloading {url} to {path}\")\n        urllib.request.urlretrieve(url, path)\n    else:\n        print(f\"{path} already exists, skipping download\")\n\n    # Check the file's hash if a digest was provided\n    if digest is not None:\n        # Calculate the file's hash\n        with open(path, \"rb\") as f:\n            file_hash = hashlib.sha256(f.read()).hexdigest()\n\n        # Compare the file's hash to the provided digest\n        if file_hash != digest:\n            raise ValueError(f\"File hash {file_hash} does not match expected hash {digest}\")\n        else:\n            print(f\"File hash {file_hash} matches expected hash {digest}\")\n\n    return path\n\n\n\n\n\n", "CodeBleu_score": 45.39075265009077}, {"code": "def download_file(path, url, digest=None)\n\t\n    import os\n    import urllib.request\n    import hashlib\n\n    def sha256_checksum(filename, block_size=65536):\n        \"\"\"The function calculates the SHA-256 hash of a given file and returns it as a hexadecimal string.\"\"\"\n        sha256 = hashlib.sha256()\n        with open(filename, 'rb') as f:\n            for block in iter(lambda: f.read(block_size), b''):\n                sha256.update(block)\n        return sha256.hexdigest()\n\n    def check_file(file_path, expected_hash):\n        \"\"\"The function checks the SHA-256 hash of a given file against an expected hash, raising an error if they do not match.\"\"\"\n        actual_hash = sha256_checksum(file_path)\n        if actual_hash != expected_hash:\n            raise ValueError('Unexpected hash value: {} (expected: {})'.format(actual_hash, expected_hash))\n\n    if not os.path.exists(path):\n        os.makedirs(os.path.dirname(path), exist_ok=True)\n        urllib.request.urlretrieve(url, path)\n\n    if digest is not None:\n        check_file(path, digest)\n\n    return path\n\n", "CodeBleu_score": 41.529327390488355}]}
{"instance_id": "f23165e897ce", "generation_result": [{"code": "def train_mode(model, mode=True)\n\t\n    modes = [module.training for module in model.modules()]\n    model.train(mode)\n    try:\n        yield\n    finally:\n        for i, module in enumerate(model.modules()):\n            module.training = modes[i]\n\n", "CodeBleu_score": 60.88475146305245}, {"code": "def train_mode(model, mode=True)\n\t\n    modes = [module.training for module in model.modules()]\n    model.train(mode)\n    try:\n        yield\n    finally:\n        for module, mode in zip(model.modules(), modes):\n            module.training = mode\n\n\n\n", "CodeBleu_score": 45.17133808214328}, {"code": "def train_mode(model, mode=True)\n\t\n    modes = [module.training for module in model.modules()]\n    try:\n        yield model.train(mode)\n    finally:\n        for module, mode in zip(model.modules(), modes):\n            module.training = mode", "CodeBleu_score": 57.436110533580745}]}
{"instance_id": "c6d59c6cb8e1", "generation_result": [{"code": "def ema_update(model, averaged_model, decay)\n\t\n    with torch.no_grad():\n        for ema_param, param in zip(averaged_model.parameters(), model.parameters()):\n            ema_param.data.mul_(decay).add_(param.data, alpha=1 - decay)\n        for ema_buffer, buffer in zip(averaged_model.buffers(), model.buffers()):\n            ema_buffer.data.copy_(buffer.data)\n\n\n", "CodeBleu_score": 30.31540772000797}, {"code": "def ema_update(model, averaged_model, decay)\n\t\n    with torch.no_grad():\n        for ema_v, model_v in zip(averaged_model.parameters(), model.parameters()):\n            if ema_v.requires_grad:\n                ema_v.data.mul_(decay).add_(model_v.data, alpha=1 - decay)\n        for ema_v, model_v in zip(averaged_model.buffers(), model.buffers()):\n            if ema_v.requires_grad:\n                ema_v.data.copy_(model_v.data)\n\n", "CodeBleu_score": 31.88073724720467}, {"code": "def ema_update(model, averaged_model, decay)\n\t\n    model_dict = model.state_dict()\n    averaged_model_dict = averaged_model.state_dict()\n    for k, v in model_dict.items():\n        if v.dtype in (torch.float16, torch.float32, torch.float64):\n            averaged_model_dict[k] = decay * averaged_model_dict[k] + (1 - decay) * v\n        else:\n            averaged_model_dict[k] = v\n    averaged_model.load_state_dict(averaged_model_dict)\n\n\n\n", "CodeBleu_score": 21.651711403517787}]}
{"instance_id": "600c39a30798", "generation_result": [{"code": "def rand_log_logistic(shape, loc=0., scale=1., min_value=0., max_value=float('inf'), device='cpu', dtype=torch.float32)\n\t\n    min_value = torch.tensor(min_value, dtype=dtype, device=device)\n    max_value = torch.tensor(max_value, dtype=dtype, device=device)\n    u = torch.rand(shape, dtype=dtype, device=device)\n    a = (min_value - loc) / scale\n    b = (max_value - loc) / scale\n    return loc + scale * torch.log(u / (1 - u))\n\n", "CodeBleu_score": 26.47703486046233}, {"code": "def rand_log_logistic(shape, loc=0., scale=1., min_value=0., max_value=float('inf'), device='cpu', dtype=torch.float32)\n\t\n    min_value = torch.as_tensor(min_value, device=device, dtype=dtype)\n    max_value = torch.as_tensor(max_value, device=device, dtype=dtype)\n    min_value_cdf = log_logistic.cdf(min_value, loc, scale)\n    max_value_cdf = log_logistic.cdf(max_value, loc, scale)\n    u = torch.rand(shape, device=device, dtype=dtype)\n    u = u * (max_value_cdf - min_value_cdf) + min_value_cdf\n    v = logit(u)\n    x = scale * torch.exp(v) + loc\n    return x\n\n", "CodeBleu_score": 38.06961757542727}, {"code": "def rand_log_logistic(shape, loc=0., scale=1., min_value=0., max_value=float('inf'), device='cpu', dtype=torch.float32)\n\t\n    min_value = torch.tensor(min_value, device=device, dtype=dtype)\n    max_value = torch.tensor(max_value, device=device, dtype=dtype)\n    min_cdf = torch.sigmoid((min_value - loc) / scale)\n    max_cdf = torch.sigmoid((max_value - loc) / scale)\n    u = torch.rand(shape, device=device, dtype=dtype)\n    u = u * (max_cdf - min_cdf) + min_cdf\n    return torch.exp(torch.log(u / (1 - u)) * scale + loc)\n\n", "CodeBleu_score": 42.40029954593068}]}
{"instance_id": "75e2def2467f", "generation_result": [{"code": "def get_activation(activation: Literal[\"elu\", \"snake\", \"none\"], antialias=False, channels=None) -> nn.Module\n\t\n    if activation == \"elu\":\n        return nn.ELU()\n    elif activation == \"snake\":\n        return SnakeBeta(channels, antialias=antialias)\n    elif activation == \"none\":\n        return nn.Identity()\n    else:\n        raise ValueError(f\"Activation {activation} not implemented\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 39.1577377281018}, {"code": "def get_activation(activation: Literal[\"elu\", \"snake\", \"none\"], antialias=False, channels=None) -> nn.Module\n\t\n    if activation == \"elu\":\n        return nn.ELU()\n    elif activation == \"snake\":\n        assert channels is not None, \"Specify channel width.\"\n        return SnakeBeta(channels, antialias=antialias)\n    elif activation == \"none\":\n        return nn.Identity()\n    else:\n        raise ValueError(f\"Activation {activation} not implemented.\")\n\n", "CodeBleu_score": 39.70186059609253}, {"code": "def get_activation(activation: Literal[\"elu\", \"snake\", \"none\"], antialias=False, channels=None) -> nn.Module\n\t\n    if activation == \"elu\":\n        return nn.ELU()\n    elif activation == \"snake\":\n        return SnakeBeta(channels=channels, antialias=antialias)\n    elif activation == \"none\":\n        return nn.Identity()\n    else:\n        raise ValueError(f\"Activation {activation} not implemented\")\n\n", "CodeBleu_score": 39.328317521327705}]}
{"instance_id": "ef1714c79e51", "generation_result": [{"code": "def apply_rotary_pos_emb(t, freqs, scale = 1)\n\t\n    if freqs.shape[-1] != t.shape[-1]:\n        raise ValueError(f'The number of frequencies ({freqs.shape[-1]}) must match the number of elements in the tensor ({t.shape[-1]}).')\n    t, t_pass = t[..., :freqs.shape[-1] * 2], t[..., freqs.shape[-1] * 2:]\n    t, t_pass = map(lambda t: t.float(), (t, t_pass))\n    t = rearrange(t, '... (j d) -> ... j d', j = 2)\n    t = apply_rotary_emb(t, freqs, scale = scale)\n    t = rearrange(t, '... j d -> ... (j d)')\n    return torch.cat((t, t_pass), dim = -1)\n\n", "CodeBleu_score": 31.672751680915013}, {"code": "def apply_rotary_pos_emb(t, freqs, scale = 1)\n\t\n    if freqs.ndim == 1:\n        freqs = rearrange(freqs, '... -> ... 1')\n\n    t, t_pass = t[..., :freqs.shape[-2] * 2, :], t[..., freqs.shape[-2] * 2:, :]\n\n    t, t_pass = map(lambda t: t.to(freqs, non_blocking = True), (t, t_pass))\n\n    t, t_pass = map(lambda t: (t * scale).to(torch.float32), (t, t_pass))\n\n    t = rearrange(t, '... (d r) -> ... d r', r = 2)\n    t = apply_rotary_emb(t, freqs)\n    t = rearrange(t, '... d r -> ... (d r)')\n\n    t = torch.cat((t, t_pass), dim = -2)\n    return t\n\n", "CodeBleu_score": 36.902651851637934}, {"code": "def apply_rotary_pos_emb(t, freqs, scale = 1)\n\t\n    if freqs.shape[-1] != t.shape[-1]:\n        raise ValueError(f'The number of frequencies ({freqs.shape[-1]}) must match the number of elements in the tensor ({t.shape[-1]}).')\n    if freqs.ndim == 1:\n        freqs = freqs.unsqueeze(0)\n    t, t_pass = t[..., :freqs.shape[-1] * 2], t[..., freqs.shape[-1] * 2:]\n    t, t_pass = map(lambda t: t.view(*t.shape[:-1], -1, 2), (t, t_pass))\n    t = rearrange(t, '... s (j d) -> ... (s j) d', j = 2)\n    t = apply_rotary_emb(t, freqs, scale = scale)\n    t = rearrange(t, '... (s j) d -> ... s (j d)', j = 2)\n    return torch.cat((t, t_pass), dim = -1)\n\n", "CodeBleu_score": 34.96883802462737}]}
{"instance_id": "a7a5f178c6f6", "generation_result": [{"code": "def idx2vq(self, idx, dim=-1)\n\t\n    embed = self.codebook(idx)\n    if dim != -1:\n        embed = embed.moveaxis(dim, -1)\n    return embed\n\n", "CodeBleu_score": 39.14758913766859}, {"code": "def idx2vq(self, idx, dim=-1)\n\t\n    return self.idx2vq(idx, dim=-1)\n\n\n", "CodeBleu_score": 13.964505353001211}, {"code": "def idx2vq(self, idx, dim=-1)\n\t\n    vq = self.codebook.embedding(idx)\n    if dim != -1:\n        vq = vq.moveaxis(dim, -1)\n    return vq", "CodeBleu_score": 34.478187260987845}]}
{"instance_id": "4104c041fb20", "generation_result": [{"code": "def encode(self, x, quantize=False)\n\t\n    x_enc = self.encoder(x)\n    if quantize:\n        x_enc, (vq_loss, commit_loss), indices = self.vq(x_enc)\n        return x_enc, vq_loss, commit_loss, indices\n    return x_enc\n\n", "CodeBleu_score": 25.443184991296846}, {"code": "def encode(self, x, quantize=False)\n\t\n    if quantize:\n        return self.encode_quantized(x)\n    z_q_x = x\n    for layer in self.layers:\n        z_q_x = layer(z_q_x)\n    return z_q_x\n\n", "CodeBleu_score": 22.928396818813226}, {"code": "def encode(self, x, quantize=False)\n\t\n    if quantize:\n        return self.encode_quantize(x)\n    else:\n        return self.encode_no_quantize(x)\n\n", "CodeBleu_score": 16.273566741082547}]}
{"instance_id": "403c52ba2d3b", "generation_result": [{"code": "def ema_scope(self, context=None)\n\t\n    if context is None:\n        context = nullcontext()\n    with context:\n        self.store(self.model.parameters())\n        self.copy_to(self.model)\n        if self.log_info:\n            self.logger.info(\"Switched to EMA weights\")\n    try:\n        yield\n    finally:\n        self.restore(self.model.parameters())\n        if self.log_info:\n            self.logger.info(\"Restored original parameters\")", "CodeBleu_score": 44.83145311801133}, {"code": "def ema_scope(self, context=None)\n\t\n    if context is None:\n        context = self.ema_context\n    if context == \"eval\":\n        self.store(self.model.parameters())\n        self.copy_to(self.model)\n    elif context == \"train\":\n        self.copy_to(self.model)\n        self.restore(self.model.parameters())\n    else:\n        raise ValueError(f\"Invalid context: {context}\")\n    yield\n    if context == \"eval\":\n        self.restore(self.model.parameters())\n    elif context == \"train\":\n        self.copy_to(self.model)\n    else:\n        raise ValueError(f\"Invalid context: {context}\")\n\n", "CodeBleu_score": 42.960921981905855}, {"code": "def ema_scope(self, context=None)\n\t\n    self.store(self.model.parameters())\n    self.copy_to(self.model)\n    if context:\n        self.logger.info(f\"{context} | Using Exponential Moving Average weights\")\n    try:\n        yield\n    finally:\n        self.restore(self.model.parameters())\n        if context:\n            self.logger.info(f\"{context} | Restored original parameters\")\n\n", "CodeBleu_score": 47.84903972349237}]}
{"instance_id": "94b7467f98e4", "generation_result": [{"code": "def post_attention(self, attn, x, gate_msa, shift_mlp, scale_mlp, gate_mlp)\n\t\n    x = x + attn * gate_msa\n    x = self.norm_mlp(x)\n    x = modulate(x, shift_mlp, scale_mlp)\n    x = self.mlp(x) * gate_mlp\n    return x\n\n", "CodeBleu_score": 37.557812845264614}, {"code": "def post_attention(self, attn, x, gate_msa, shift_mlp, scale_mlp, gate_mlp)\n\t\n    x = x + attn * gate_msa\n    x = self.norm1(x)\n    x = modulate(x, shift_mlp, scale_mlp)\n    x = self.mlp(self.norm2(x), gate_mlp)\n    return x\n\n", "CodeBleu_score": 41.04041906160383}, {"code": "def post_attention(self, attn, x, gate_msa, shift_mlp, scale_mlp, gate_mlp)\n\t\n    x = x + attn * gate_msa\n    x = self.norm1(x)\n    x = modulate(x, shift_mlp, scale_mlp)\n    x = self.mlp(x) * gate_mlp\n    return x\n\n", "CodeBleu_score": 37.3749078252831}]}
{"instance_id": "d9fdc05542a2", "generation_result": [{"code": "def _block_mixing(context, x, context_block, x_block, c)\n\t\n    context_qkv, context_intermediate = context_block.pre_attention(context)\n    x_qkv, x_intermediate = x_block.pre_attention(x)\n    qkv = torch.cat([context_qkv, x_qkv], dim=1)\n    qkv = optimized_attention(qkv, c)\n    context_qkv, x_qkv = torch.split(qkv, [context.shape[1], x.shape[1]], dim=1)\n    context = context_block.post_attention(context_qkv)\n    x = x_block.post_attention(x_qkv)\n    return context, x\n\n", "CodeBleu_score": 19.330870846534612}, {"code": "def _block_mixing(context, x, context_block, x_block, c)\n\t\n    context_qkv, context_intermediate = context_block.pre_attention(context)\n    x_qkv, x_intermediate = x_block.pre_attention(x)\n    qkv = torch.cat([context_qkv, x_qkv], dim=1)\n    attention_result = optimized_attention(qkv, c.num_heads)\n    context_result, x_result = torch.split(attention_result, [context_qkv.shape[1], x_qkv.shape[1]], dim=1)\n    if context_block.pre_only:\n        context = context_result\n    else:\n        context = context_block.post_attention(context_result)\n    if x_block.pre_only:\n        x = x_result\n    else:\n        x = x_block.post_attention(x_result)\n    return context, x\n\n", "CodeBleu_score": 25.88264994708429}, {"code": "def _block_mixing(context, x, context_block, x_block, c)\n\t\n    qkv_context = context_block.pre_attention(context)\n    qkv_x = x_block.pre_attention(x)\n    qkv = torch.cat((qkv_context, qkv_x), dim=1)\n    qkv = optimized_attention(qkv, c)\n    qkv_context, qkv_x = torch.split(qkv, [qkv_context.shape[1], qkv_x.shape[1]], dim=1)\n    if context_block.pre_only:\n        return context_block.post_attention(qkv_context), x_block.post_attention(qkv_x)\n    else:\n        return context_block.post_attention(context + qkv_context), x_block.post_attention(x + qkv_x)\n\n", "CodeBleu_score": 20.09727928857305}]}
{"instance_id": "b219bce30160", "generation_result": [{"code": "def get_timestep_embedding(timesteps, embedding_dim)\n\t\n    assert len(timesteps.shape) == 1, \"Timesteps should be a 1d-array\"\n    half_dim = embedding_dim // 2\n    emb = math.log(10000) / (half_dim - 1)\n    emb = torch.exp(torch.arange(half_dim, dtype=torch.float32) * -emb)\n    emb = timesteps[:, None] * emb[None, :]\n    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n    if embedding_dim % 2 == 1:  # zero pad\n        emb = torch.nn.functional.pad(emb, (0, 1, 0, 0))\n    return emb\n\n", "CodeBleu_score": 77.2112794103032}, {"code": "def get_timestep_embedding(timesteps, embedding_dim)\n\t\n    assert len(timesteps.shape) == 1\n\n    half_dim = embedding_dim // 2\n    emb = math.log(10000) / (half_dim - 1)\n    emb = torch.exp(torch.arange(half_dim, dtype=torch.float32) * -emb)\n    emb = timesteps[:, None] * emb[None, :]\n    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n    if embedding_dim % 2 == 1:  # zero pad\n        emb = torch.nn.functional.pad(emb, (0, 1, 0, 0))\n    return emb\n\n\n\n", "CodeBleu_score": 77.99718186620508}, {"code": "def get_timestep_embedding(timesteps, embedding_dim)\n\t\n    assert len(timesteps.shape) == 1\n    half_dim = embedding_dim // 2\n    emb = math.log(10000) / (half_dim - 1)\n    emb = torch.exp(torch.arange(half_dim, dtype=torch.float32) * -emb)\n    emb = timesteps[:, None] * emb[None, :]\n    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n    if embedding_dim % 2 == 1:  # zero pad\n        emb = torch.nn.functional.pad(emb, (0, 1, 0, 0))\n    return emb\n\n", "CodeBleu_score": 77.99718186620508}]}
{"instance_id": "6ae8453c86be", "generation_result": [{"code": "def pytorch_attention(q, k, v)\n\t\n    q = q.reshape(q.shape[0], q.shape[1], q.shape[2] * q.shape[3]).transpose(1, 2)\n    k = k.reshape(k.shape[0], k.shape[1], k.shape[2] * k.shape[3]).transpose(1, 2)\n    v = v.reshape(v.shape[0], v.shape[1], v.shape[2] * v.shape[3]).transpose(1, 2)\n    scale = (int(q.shape[-1])**(-0.5))\n\n    try:\n        s1 = torch.bmm(q, k) * scale\n        s2 = torch.nn.functional.softmax(s1, dim=2).permute(0,2,1)\n        del s1\n        r1 = torch.bmm(v, s2)\n        del s2\n    except model_management.OOM_EXCEPTION as e:\n        logging.warning(\"out of memory error, falling back to slice attention\")\n        r1 = slice_attention(q, k, v)\n\n    r1 = r1.transpose(1, 2).reshape(q.shape[0], q.shape[2], q.shape[3], v.shape[2])\n    return r1\n\n", "CodeBleu_score": 37.642957922392895}, {"code": "def pytorch_attention(q, k, v)\n\t\n    q = q.permute(0, 2, 1)\n    k = k.permute(0, 2, 1)\n    v = v.permute(0, 2, 1)\n    q = q.view(q.shape[0], q.shape[1], q.shape[2] // q.shape[0]).permute(0, 2, 1)\n    k = k.view(k.shape[0], k.shape[1], k.shape[2] // k.shape[0]).permute(0, 2, 1)\n    v = v.view(v.shape[0], v.shape[1], v.shape[2] // v.shape[0]).permute(0, 2, 1)\n\n    r1 = torch.zeros_like(k, device=q.device)\n    scale = (int(q.shape[-1])**(-0.5))\n\n    try:\n        s1 = torch.bmm(q, k) * scale\n        s2 = torch.nn.functional.softmax(s1, dim=2).permute(0,2,1)\n        del s1\n\n        r1 = torch.bmm(v, s2)\n        del s2\n    except model_management.OOM_EXCEPTION as e:\n        logging.warning(\"out of memory error, switching to slice attention\")\n        r1 = slice_attention(q, k, v)\n\n    r1 = r1.permute(0, 2, 1).reshape(q.shape[0], q.shape[2], r1.shape[1])\n    return r1\n\n", "CodeBleu_score": 32.29588061292025}, {"code": "def pytorch_attention(q, k, v)\n\t\n    q = q.reshape(q.shape[0], q.shape[1], q.shape[2]*q.shape[3]).transpose(1, 2)\n    k = k.reshape(k.shape[0], k.shape[1], k.shape[2]*k.shape[3]).transpose(1, 2)\n    v = v.reshape(v.shape[0], v.shape[1], v.shape[2]*v.shape[3]).transpose(1, 2)\n\n    scale = (int(q.shape[-1])**(-0.5))\n    try:\n        r1 = torch.bmm(q, k) * scale\n    except model_management.OOM_EXCEPTION as e:\n        logging.warning(\"out of memory error, falling back to slice attention\")\n        r1 = slice_attention(q, k, v)\n\n    s2 = torch.nn.functional.softmax(r1, dim=2).permute(0,2,1)\n    del r1\n\n    r2 = torch.bmm(v, s2)\n    del s2\n\n    r2 = r2.transpose(1, 2).reshape(r2.shape[0], r2.shape[1], r2.shape[2]//q.shape[1], q.shape[1])\n\n    return r2\n\n", "CodeBleu_score": 39.2819274841292}]}
{"instance_id": "fe4433c987ad", "generation_result": [{"code": "def apply_control(h, control, name)\n\t\n    if control and name:\n        try:\n            h.add_control(control, name)\n        except Exception as e:\n            logger.warning(f\"Failed to add control {name} to h: {e}\")\n    return h", "CodeBleu_score": 22.596887855452042}, {"code": "def apply_control(h, control, name)\n\t\n    if control and name:\n        try:\n            control_value = control.pop(name)\n            h.add_control(control_value)\n        except Exception as e:\n            print(f\"Warning: Failed to add control {name} with value {control_value}: {e}\")\n    return h", "CodeBleu_score": 32.74942013527568}, {"code": "def apply_control(h, control, name)\n\t\n    if control and name:\n        control_name = control.pop(0)\n        if control_name == name:\n            try:\n                h = h.copy(deep=True)\n                h.data[name] = control.pop(0)\n            except Exception as e:\n                print(f\"Failed to apply control {control_name} to {name}: {e}\")\n    return h\n\n", "CodeBleu_score": 35.97302884975188}]}
{"instance_id": "56da4547c101", "generation_result": [{"code": "def get_codebook_entry(self, indices, shape)\n\t\n    one_hot = F.one_hot(indices, self.num_embeddings).float()\n    z_q = torch.matmul(one_hot, self.embedding.weight)\n    if shape is not None:\n        z_q = z_q.view(shape)\n        z_q = z_q.permute(0, 3, 1, 2).contiguous()\n    return z_q\n\n", "CodeBleu_score": 53.12351780542423}, {"code": "def get_codebook_entry(self, indices, shape)\n\t\n    codebook_entry = self.embedding(indices)\n    if shape is not None:\n        codebook_entry = codebook_entry.view(shape).permute(0, 3, 1, 2)\n    return codebook_entry\n\n\n\n", "CodeBleu_score": 26.276995731138207}, {"code": "def get_codebook_entry(self, indices, shape)\n\t\n    if indices is None:\n        return None\n    if self.use_embedding_features is not None:\n        # add an offset to indices for the conditional mask\n        # so that original codebook indices start at len(self.use_embedding_features)\n        offset = len(self.use_embedding_features)\n        indices = ((indices.long() + offset) *\n                   (self.mask[self.use_embedding_features].long()))\n    # get quantized latent vectors\n    z_q = self.embedding(indices)\n    if shape is not None:\n        z_q = z_q.view(shape).permute(0, 3, 1, 2).contiguous()\n    return z_q\n\n", "CodeBleu_score": 53.88961777631687}]}
{"instance_id": "548f8a1fb37d", "generation_result": [{"code": "def make_noise(self)\n\t\n    noise_list = [th.randn(4, 4, device=self.device)]\n    for i in range(3, int(self.log_size) + 1):\n        noise_list.append(th.randn(2 ** i, 2 ** i, device=self.device))\n        noise_list.append(th.randn(2 ** i, 2 ** i, device=self.device))\n    return noise_list\n\n", "CodeBleu_score": 29.630259039076307}, {"code": "def make_noise(self)\n\t\n    noise_list = [torch.randn(4, 4, device=self.device)]\n    for i in range(3, self.log_size + 1):\n        noise_list.append(torch.randn(2 ** i, 2 ** i, device=self.device))\n        noise_list.append(torch.randn(2 ** i, 2 ** i, device=self.device))\n    return noise_list\n\n", "CodeBleu_score": 33.42800291962016}, {"code": "def make_noise(self)\n\t\n    noise_list = [th.randn(4, 4, device=self.device)]\n    for i in range(3, int(self.log_size) + 1):\n        noise_list.append(th.randn(2 ** i, 2 ** i, device=self.device))\n        noise_list.append(th.randn(2 ** i, 2 ** i, device=self.device))\n    return noise_list\n\n", "CodeBleu_score": 29.630259039076307}]}
{"instance_id": "42fd805563c6", "generation_result": [{"code": "def mean_latent(self, num_latent)\n\t\n    return torch.mean(self.style(torch.randn(num_latent, self.style_dim)), dim=0)\n\n\n\n", "CodeBleu_score": 12.752711541627141}, {"code": "def mean_latent(self, num_latent)\n\t\n    return self.style(torch.randn(num_latent, self.style_dim))\n\n\n\n", "CodeBleu_score": 10.362883133507655}, {"code": "def mean_latent(self, num_latent)\n\t\n    if self.w_avg is not None:\n        return self.w_avg\n    else:\n        z_samples = np.random.randn(num_latent, self.z_dim)\n        w_samples = self.style(z_samples)\n        w_samples = w_samples[0].repeat(self.num_ws, 1)\n        self.w_avg = nn.Parameter(w_samples.detach().mean(0))\n        return self.w_avg\n\n", "CodeBleu_score": 37.71655585949738}]}
{"instance_id": "2f485fd924af", "generation_result": [{"code": "def color_jitter(img, shift)\n\t\n    w, h, c = img.shape\n    img = img.reshape(w * h, c)\n    img = img + shift * (np.random.rand(w * h, c) - 0.5)\n    img = np.clip(img, 0, 1)\n    img = img.reshape(w, h, c)\n    return img\n\n", "CodeBleu_score": 43.17414230177717}, {"code": "def color_jitter(img, shift)\n\t\n    jitter = tf.random.uniform([], -shift, shift)\n    img = tf.add(img, jitter)\n    img = tf.clip_by_value(img, 0, 1)\n    return img\n\n\n\n", "CodeBleu_score": 42.37874637438025}, {"code": "def color_jitter(img, shift)\n\t\n    img = tf.image.random_brightness(img, shift)\n    img = tf.image.random_saturation(img, shift, shift)\n    img = tf.image.random_hue(img, shift)\n    img = tf.image.random_contrast(img, 1-shift, 1+shift)\n    return tf.clip_by_value(img, 0, 1)\n", "CodeBleu_score": 16.254212325460017}]}
{"instance_id": "c87ccf1fbf48", "generation_result": [{"code": "def feed_data(self, data)\n\t\n    self.lq = data[\"lq\"]\n    if \"gt\" in data:\n        self.gt = data[\"gt\"]\n    if \"facial_components\" in data:\n        self.facial_components = data[\"facial_components\"]\n        self.left_eye = data[\"facial_components\"][\"left_eye\"]\n        self.right_eye = data[\"facial_components\"][\"right_eye\"]\n        self.mouth = data[\"facial_components\"][\"mouth\"]\n\n", "CodeBleu_score": 37.29532554897194}, {"code": "def feed_data(self, data)\n\t\n    self.lq = data[\"lq\"]\n    if \"gt\" in data:\n        self.gt = data[\"gt\"]\n    if \"facial_components\" in data:\n        self.facial_components = data[\"facial_components\"]\n        self.left_eye = self.facial_components[\"left_eye\"]\n        self.right_eye = self.facial_components[\"right_eye\"]\n        self.mouth = self.facial_components[\"mouth\"]\n\n", "CodeBleu_score": 37.26328792816622}, {"code": "def feed_data(self, data)\n\t\n    self.lq = data[\"lq\"]\n    if \"gt\" in data:\n        self.gt = data[\"gt\"]\n    if \"face\" in data:\n        self.face = data[\"face\"]\n        self.left_eye = data[\"face\"][:, 0:2]\n        self.right_eye = data[\"face\"][:, 2:4]\n        self.mouth = data[\"face\"][:, 4:6]", "CodeBleu_score": 40.63932013545003}]}
{"instance_id": "8f8c7cb13037", "generation_result": [{"code": "def construct_img_pyramid(self)\n\t\n    img_pyramid = []\n    img_pyramid.insert(0, self.gt_img)\n    for i in range(self.num_downscales):\n        img_pyramid.insert(0, self.downscale_img(img_pyramid[0]))\n    return img_pyramid\n\n", "CodeBleu_score": 20.52075873998345}, {"code": "def construct_img_pyramid(self)\n\t\n    img_pyramid = []\n    current_img = self.gt_image\n    for i in range(self.num_downscales):\n        current_img = cv2.resize(current_img, (0, 0), fx=0.5, fy=0.5, interpolation=cv2.INTER_LINEAR)\n        img_pyramid.insert(0, current_img)\n    return img_pyramid\n\n\n\n", "CodeBleu_score": 33.19771995515464}, {"code": "def construct_img_pyramid(self)\n\t\n    img_pyramid = []\n    img_pyramid.append(self.gt_img)\n    for i in range(self.pyramid_levels):\n        img_pyramid.insert(0, self.downscale_img(img_pyramid[0]))\n    return img_pyramid\n\n", "CodeBleu_score": 18.197749352026033}]}
{"instance_id": "51247274d40c", "generation_result": [{"code": "def save(self, epoch, current_iter)\n\t\n    state = {\n        \"G_ema\": self.G_ema.state_dict(),\n        \"G\": self.G.state_dict(),\n        \"D\": self.D.state_dict(),\n        \"optimizer\": self.optimizer.state_dict(),\n        \"epoch\": epoch,\n        \"iter\": current_iter,\n    }\n    if self.args.facial_discriminators:\n        state[\"D_left_eye\"] = self.D_left_eye.state_dict()\n        state[\"D_right_eye\"] = self.D_right_eye.state_dict()\n        state[\"D_mouth\"] = self.D_mouth.state_dict()\n    torch.save(state, os.path.join(self.args.save_dir, \"model_state.pth\"))\n    torch.save(self.G.state_dict(), os.path.join(self.args.save_dir, \"G.pth\"))\n    torch.save(self.G_ema.state_dict(), os.path.join(self.args.save_dir, \"G_ema.pth\"))\n    torch.save(self.D.state_dict(), os.path.join(self.args.save_dir, \"D.pth\"))\n    if self.args.facial_discriminators:\n        torch.save(\n            self.D_left_eye.state_dict(),\n            os.path.join(self.args.save_dir, \"D_left_eye.pth\"),\n        )\n        torch.save(\n            self.D_right_eye.state_dict(),\n            os.path.join(self.args.save_dir, \"D_right_eye.pth\"),\n        )\n        torch.save(\n            self.D_mouth.state_dict(),\n            os.path.join(self.args.save_dir, \"D_mouth.pth\"),\n        )\n\n", "CodeBleu_score": 31.816605222380684}, {"code": "def save(self, epoch, current_iter)\n\t\n    state = {\n        \"epoch\": epoch,\n        \"iter\": current_iter,\n        \"generator\": self.generator.state_dict(),\n        \"generator_ema\": self.generator_ema.state_dict(),\n        \"discriminator\": self.discriminator.state_dict(),\n    }\n    if self.facial_discriminator:\n        state[\"facial_discriminator\"] = self.facial_discriminator.state_dict()\n    if self.facial_discriminator_left_eye:\n        state[\"facial_discriminator_left_eye\"] = self.facial_discriminator_left_eye.state_dict()\n    if self.facial_discriminator_right_eye:\n        state[\"facial_discriminator_right_eye\"] = self.facial_discriminator_right_eye.state_dict()\n    if self.facial_discriminator_mouth:\n        state[\"facial_discriminator_mouth\"] = self.facial_discriminator_mouth.state_dict()\n    torch.save(state, self.save_dir / \"latest.pth\")\n    torch.save(state, self.save_dir / f\"epoch_{epoch}_iter_{current_iter}.pth\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 29.81762976972004}, {"code": "def save(self, epoch, current_iter)\n\t\n    self.save_network(self.netG, \"G\", epoch, current_iter)\n    self.save_network(self.netG_ema, \"G_ema\", epoch, current_iter)\n    self.save_network(self.netD, \"D\", epoch, current_iter)\n    if self.opt.use_facial_discriminator:\n        self.save_network(self.netD_left_eye, \"D_left_eye\", epoch, current_iter)\n        self.save_network(self.netD_right_eye, \"D_right_eye\", epoch, current_iter)\n        self.save_network(self.netD_mouth, \"D_mouth\", epoch, current_iter)\n    self.save_training_state(epoch, current_iter)\n\n", "CodeBleu_score": 54.52255788225791}]}
{"instance_id": "afa0dc8128c1", "generation_result": [{"code": "def clean_folder(folder)\n\t\n    import os\n    import shutil\n    for filename in os.listdir(folder):\n        file_path = os.path.join(folder, filename)\n        try:\n            if os.path.isfile(file_path) or os.path.islink(file_path):\n                os.unlink(file_path)\n            elif os.path.isdir(file_path):\n                shutil.rmtree(file_path)\n        except Exception as e:\n            print('Failed to delete %s. Reason: %s' % (file_path, e))\n\n\n", "CodeBleu_score": 64.8786320172984}, {"code": "def clean_folder(folder)\n\t\n    import os\n    import shutil\n    for filename in os.listdir(folder):\n        file_path = os.path.join(folder, filename)\n        try:\n            if os.path.isfile(file_path) or os.path.islink(file_path):\n                os.unlink(file_path)\n            elif os.path.isdir(file_path):\n                shutil.rmtree(file_path)\n        except Exception as e:\n            print('Failed to delete %s. Reason: %s' % (file_path, e))\n\n", "CodeBleu_score": 64.8786320172984}, {"code": "def clean_folder(folder)\n\t\n    import os\n    import shutil\n    for filename in os.listdir(folder):\n        file_path = os.path.join(folder, filename)\n        try:\n            if os.path.isfile(file_path) or os.path.islink(file_path):\n                os.unlink(file_path)\n            elif os.path.isdir(file_path):\n                shutil.rmtree(file_path)\n        except Exception as e:\n            print('Failed to delete %s. Reason: %s' % (file_path, e))\n\n", "CodeBleu_score": 64.8786320172984}]}
{"instance_id": "53588b481bdd", "generation_result": [{"code": "def next_batch(self, sample_size=1)\n\t\n    if sample_size > len(self.memory):\n        sample_size = len(self.memory)\n    batch = random.sample(self.memory, sample_size)\n    for experience in batch:\n        self.memory.remove(experience)\n    return batch\n\n\n\n", "CodeBleu_score": 49.02659354588814}, {"code": "def next_batch(self, sample_size=1)\n\t\n    batch = []\n    ids = []\n    # adjusting the sample size if it exceeds the available experiences\n    sample_size = min(sample_size, len(self.buffer))\n    # randomly selecting the specified number of samples\n    batch = random.sample(self.buffer, sample_size)\n    # removing the sampled experiences from the original list\n    for exp in batch:\n        if exp in self.buffer:\n            self.buffer.remove(exp)\n    return batch\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 29.305585164452918}, {"code": "def next_batch(self, sample_size=1)\n\t\n    if sample_size > len(self.experiences):\n        sample_size = len(self.experiences)\n    samples = random.sample(self.experiences, sample_size)\n    for sample in samples:\n        self.experiences.remove(sample)\n    return samples\n\n\n\n", "CodeBleu_score": 50.54757130595764}]}
{"instance_id": "6bfa6a4e621c", "generation_result": [{"code": "def find_estimated_top_configs(self)\n\t\n    flattened_configs = [self.flatten_config(config) for config in self.all_configs]\n    numerical_values = [self.convert_to_numerical(config) for config in flattened_configs]\n    numpy_array = np.array(numerical_values)\n    predictions = self.cost_model.predict(numpy_array)\n    sorted_indices = np.argsort(predictions)\n    if self.metric == \"cost\":\n        sorted_indices = sorted_indices[::-1]\n    return sorted_indices[: self.num_configs]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 22.995722209598917}, {"code": "def find_estimated_top_configs(self)\n\t\n    flattened_configs = [\n        {k: v for k, v in flatten_dict(config).items() if isinstance(v, (int, float))}\n        for config in self.all_configs\n    ]\n    numerical_configs = np.array([list(config.values()) for config in flattened_configs])\n\n    estimated_top_configs = self.cost_model.predict(numerical_configs)\n    if self.metric == \"time\":\n        estimated_top_configs = estimated_top_configs.argsort()\n    elif self.metric == \"energy\":\n        estimated_top_configs = estimated_top_configs.argsort()[::-1]\n    else:\n        raise ValueError(f\"Invalid metric: {self.metric}\")\n\n    return estimated_top_configs\n\n", "CodeBleu_score": 28.496699893837242}, {"code": "def find_estimated_top_configs(self)\n\t\n    flattened_configs = [list(config.values()) for config in self.all_configs]\n    X = np.array(flattened_configs)\n    y_pred = self.cost_model.predict(X)\n    if self.metric == \"time\":\n        top_configs = np.argsort(y_pred)[: self.num_configs]\n    else:\n        top_configs = np.argsort(y_pred)[-self.num_configs :]\n    return top_configs\n\n\n\n", "CodeBleu_score": 21.055327267333087}]}
{"instance_id": "d4cf48413a57", "generation_result": [{"code": "def flatten(d, parent_key='', sep='_')\n\t\n    items = []\n    for k, v in d.items():\n        new_key = parent_key + sep + k if parent_key else k\n        if isinstance(v, collections.abc.MutableMapping):\n            items.extend(flatten(v, new_key, sep=sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)\n\n", "CodeBleu_score": 83.85068910384324}, {"code": "def flatten(d, parent_key='', sep='_')\n\t\n    items = []\n    for k, v in d.items():\n        new_key = parent_key + sep + k if parent_key else k\n        if isinstance(v, collections.abc.MutableMapping):\n            items.extend(flatten(v, new_key, sep=sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)\n\n", "CodeBleu_score": 83.85068910384324}, {"code": "def flatten(d, parent_key='', sep='_')\n\t\n    items = []\n    for k, v in d.items():\n        new_key = parent_key + sep + k if parent_key else k\n        if isinstance(v, collections.abc.MutableMapping):\n            items.extend(flatten(v, new_key, sep=sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)\n\n", "CodeBleu_score": 83.85068910384324}]}
{"instance_id": "c2a514249c25", "generation_result": [{"code": "def run_after_tuning(self)\n\t\n    if self.optimal_cmd is not None:\n        cmd = self.optimal_cmd.split()\n        process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        stdout, stderr = process.communicate()\n        if process.returncode == 0:\n            self.logger.info(\"Successfully executed the optimal command: %s\", self.optimal_cmd)\n        else:\n            self.logger.error(\"Failed to execute the optimal command: %s\", self.optimal_cmd)\n            self.logger.error(\"Error message: %s\", stderr.decode())\n    else:\n        self.logger.info(\"Autotuning did not find an optimal configuration.\")", "CodeBleu_score": 38.28118282987095}, {"code": "def run_after_tuning(self)\n\t\n    if self.optimal_cmd is not None:\n        proc = subprocess.Popen(self.optimal_cmd, shell=True)\n        proc.wait()\n        self.logger.info(\"Successfully ran optimal command: %s\", self.optimal_cmd)\n    else:\n        self.logger.info(\"Autotuning did not find an optimal configuration.\")", "CodeBleu_score": 50.214872377332}, {"code": "def run_after_tuning(self)\n\t\n    if self.optimal_cmd != None:\n        cmd = self.optimal_cmd.split()\n        process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        stdout, stderr = process.communicate()\n        if process.returncode == 0:\n            self.logger.info(f\"Successfully executed command: {self.optimal_cmd}\")\n        else:\n            self.logger.error(f\"Error executing command: {self.optimal_cmd}\")\n            self.logger.error(f\"stdout: {stdout}\")\n            self.logger.error(f\"stderr: {stderr}\")\n    else:\n        self.logger.info(\"Autotuning did not find an optimal configuration.\")", "CodeBleu_score": 40.74183987110928}]}
{"instance_id": "61802de52df0", "generation_result": [{"code": "def run_job(self, exp: dict, reservations)\n\t\n    experiment_id = exp[\"id\"]\n    exp[\"master_port\"] = self.get_free_port()\n    exp[\"result_dir\"] = self.result_dir\n    exp[\"user_args\"] = self.update_user_args(exp[\"user_args\"])\n    exp[\"user_args\"][\"experiment_id\"] = experiment_id\n    self.running_experiments[experiment_id] = {\n        \"exp\": exp,\n        \"reservations\": reservations,\n        \"timestamp\": datetime.now(),\n    }\n    self.logger.info(f\"Running experiment {experiment_id}\")\n    self.logger.info(f\"Experiment {experiment_id} arguments: {exp['user_args']}\")\n    thread = Thread(target=self.run_experiment, args=(exp,))\n    thread.start()\n\n", "CodeBleu_score": 26.359131834768274}, {"code": "def run_job(self, exp: dict, reservations)\n\t\n    self.exp = exp\n    self.reservations = reservations\n    self.exp[\"master_port\"] = self.get_free_port()\n    self.exp[\"result_dir\"] = self.get_result_dir()\n    self.update_user_args()\n    self.thread = threading.Thread(\n        target=self.run_experiment,\n        args=(self.exp[\"script\"], self.exp[\"args\"]),\n        kwargs={\"master_port\": self.exp[\"master_port\"]},\n    )\n    self.thread.start()\n    self.running_experiments[self.exp[\"timestamp\"]] = self.exp\n\n", "CodeBleu_score": 18.554258164127187}, {"code": "def run_job(self, exp: dict, reservations)\n\t\n    exp_id = exp['id']\n    exp_name = exp['name']\n    exp_script = exp['script']\n    exp_args = exp['args']\n    exp_result_dir = exp['result_dir']\n    exp_master_port = exp['master_port']\n    exp_arg_mappings = exp['arg_mappings']\n    exp_arg_mappings = exp_arg_mappings if exp_arg_mappings else {}\n    exp_arg_mappings = {k: v for k, v in exp_arg_mappings.items() if v}\n    exp_arg_mappings = {k: v for k, v in exp_arg_mappings.items() if v}\n    exp_arg_mappings = {k: v for k, v in exp_arg_mappings.items() if v}\n    exp_arg_mappings = {k: v for k, v in exp_arg_mappings.items() if v}\n    exp_arg_mappings = {k: v for k, v in exp_arg_mappings.items() if v}\n    exp_arg_mappings = {k: v for k, v in exp_arg_mappings.items() if v}\n    exp_arg_mappings = {k: v for k, v in exp_arg_mappings.items() if v}\n    exp_arg_mappings = {k: v for k, v in exp_arg_mappings.items() if v}\n    exp_arg_mappings = {k: v for k, v in exp_arg_mappings.items() if v}\n    exp_arg_mappings = {k: v for k, v in exp_arg_mappings.items() if v}\n    exp_arg_mappings = {k: v for k, v in exp_arg_mappings.items() if v}\n    exp_arg_mappings = {k: v for k, v in exp_arg_mappings.items() if v}\n    exp_arg_mappings = {k: v for k, v in exp_arg_mappings.items() if v}\n    exp_arg_mappings = {k: v for", "CodeBleu_score": 15.756737586413477}]}
{"instance_id": "4884e2e9665c", "generation_result": [{"code": "def save_exp_results_to_database(self, message, ranks=None, path=None):\n\t\n    if self.distributed_training_initialized:\n        if ranks is None:\n            ranks = [self.rank]\n        for rank in ranks:\n            if rank == 0:\n                self.log_to_file(message, path)\n    else:\n        self.log_to_file(message, path)\n\n\n\n", "CodeBleu_score": 17.553032230212395}, {"code": "def save_exp_results_to_database(self, message, ranks=None, path=None):\n\t\n    if self.distributed_training_initialized:\n        if self.rank == 0:\n            if path is None:\n                path = self.logging_path\n            if ranks is None:\n                ranks = self.ranks\n            message = json.dumps({'message': message, 'ranks': ranks})\n            with open(path, 'a') as f:\n                f.write(message + '", "CodeBleu_score": 26.339086407662656}, {"code": "def save_exp_results_to_database(self, message, ranks=None, path=None):\n\t\n    if self.distributed:\n        if self.rank == 0:\n            self.logging_permission = True\n    else:\n        self.logging_permission = True\n\n    if self.logging_permission:\n        if ranks is not None:\n            ranks = [self.rank]\n        else:\n            ranks = []\n        message = {\"message\": message, \"ranks\": ranks}\n        if path is None:\n            path = self.config[\"log_path\"]\n        with open(path, \"a\") as f:\n            json.dump(message, f)\n            f.write(\"", "CodeBleu_score": 28.147521670351583}]}
{"instance_id": "0429344c3c4d", "generation_result": [{"code": "def _build_tp_other_layer_map(self, layer_index: int)\n\t\n    if not self.layer_files:\n        return None\n    if layer_index < 0 or layer_index >= len(self.layer_files):\n        raise ValueError(\"Invalid layer index\")\n    prefix = f\"{self.prefix}_{layer_index}_\"\n    partition_files = defaultdict(list)\n    for file in self.layer_files:\n        if file.startswith(prefix):\n            partition = int(file.split(\"_\")[-1])\n            partition_files[partition].append(file)\n    return partition_files\n\n\n\n", "CodeBleu_score": 35.44293465816482}, {"code": "def _build_tp_other_layer_map(self, layer_index: int)\n\t\n    if not self.layer_files:\n        raise Exception(\"No layer files found\")\n    if layer_index < 0 or layer_index >= len(self.layer_files):\n        raise Exception(\"Invalid layer index\")\n    layer_prefix = f\"layer_{layer_index}_\"\n    layer_files = [\n        file for file in self.layer_files if file.startswith(layer_prefix)]\n    partition_map = {}\n    for file in layer_files:\n        partition_index = int(file.split(\"_\")[-1].split(\".\")[0])\n        if partition_index not in partition_map:\n            partition_map[partition_index] = []\n        partition_map[partition_index].append(file)\n    return partition_map\n\n", "CodeBleu_score": 35.982643884287945}, {"code": "def _build_tp_other_layer_map(self, layer_index: int)\n\t\n    if not self.layer_files:\n        raise ValueError(\"No layer files found.\")\n    if layer_index < 0 or layer_index >= len(self.layer_files):\n        raise ValueError(\"Invalid layer index.\")\n    prefix = f\"layer_{layer_index}_\"\n    partition_map = {}\n    for file in self.layer_files:\n        if file.startswith(prefix):\n            tp_degree = int(file.split(\"_\")[2])\n            if tp_degree not in partition_map:\n                partition_map[tp_degree] = []\n            partition_map[tp_degree].append(file)\n    return partition_map\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 33.73566988185699}]}
{"instance_id": "1b65ecb9be93", "generation_result": [{"code": "def system_info()\n\t\n    import platform\n    import psutil\n    import os\n    import subprocess\n\n    # Get system information\n    system = platform.system()\n    architecture = platform.architecture()[0]\n    version = platform.version()\n    processor = platform.processor()\n    python_version = platform.python_version()\n\n    # Get disk usage information\n    disk_usage = psutil.disk_usage('/')\n    total_disk_space = disk_usage.total // (1024**3)\n    used_disk_space = disk_usage.used // (1024**3)\n    free_disk_space = disk_usage.free // (1024**3)\n\n    # Get memory usage information\n    memory_usage = psutil.virtual_memory()\n    total_memory = memory_usage.total // (1024**3)\n    available_memory = memory_usage.available // (1024**3)\n    used_memory = memory_usage.used // (1024**3)\n    free_memory = memory_usage.free // (1024**3)\n\n    # Get CPU usage information\n    cpu_usage = psutil.cpu_percent(interval=1)\n\n    # Get GPU information (if available)\n    gpu_info = None\n    try:\n        nvidia_smi_output = subprocess.check_output(['nvidia-smi', '--query-gpu=name,memory.total,driver_version,memory.used', '--format=csv'])\n        gpu_info = nvidia_smi_output.decode('utf-8').strip().split('", "CodeBleu_score": 46.33785770181918}, {"code": "def system_info()\n\t\n    import platform\n    import psutil\n    import os\n    import sys\n    import subprocess\n\n    # Get the operating system information\n    os_info = platform.system()\n    os_release = platform.release()\n    os_version = platform.version()\n    os_architecture = platform.machine()\n\n    # Get the processor information\n    processor = platform.processor()\n\n    # Get the Python version\n    python_version = sys.version.split('", "CodeBleu_score": 49.8363827655664}, {"code": "def system_info()\n\t\n    import platform\n    import psutil\n    import os\n    import sys\n\n    # Get system information\n    system = platform.system()\n    architecture = platform.architecture()[0]\n    version = platform.version()\n    processor = platform.processor()\n    python_version = sys.version\n\n    # Get memory information\n    memory = psutil.virtual_memory()\n    total_memory = memory.total\n    available_memory = memory.available\n\n    # Get disk information\n    disk = psutil.disk_usage('/')\n    total_disk_space = disk.total\n    free_disk_space = disk.free\n\n    # Get CPU information\n    cpu_count = psutil.cpu_count()\n    cpu_percent = psutil.cpu_percent()\n\n    # Get current working directory\n    cwd = os.getcwd()\n\n    # Get current user\n    user = os.getlogin()\n\n    # Get environment variables\n    env_vars = os.environ\n\n    # Get network information\n    network = psutil.net_if_addrs()\n\n    # Get process information\n    processes = psutil.process_iter(['pid', 'name', 'username', 'status', 'memory_info'])\n\n    # Get information about the current process\n    current_process = psutil.Process()\n    current_process_info = {\n        'pid': current_process.pid,\n        'name': current_process.name(),\n        'username': current_process.username(),\n        'status': current_process.status(),\n        'memory_info': current_process.memory_info(),\n    }\n\n    # Get information about the parent process\n    parent_process = psutil.Process(current_process.ppid())\n    parent_process_info = {\n        'pid': parent_process.pid,\n        'name': parent_process.name(),\n        'username': parent_process.username(),\n        'status': parent_process.status(),\n        'memory_info': parent_process.memory_info(),\n    }\n\n    # Get information about the child processes\n    child_processes = current_process.children()\n    child_processes_info = []", "CodeBleu_score": 44.68688960239655}]}
{"instance_id": "969bcc5cf3b5", "generation_result": [{"code": "def cuda_info()\n\t\n    import torch\n    cuda_is_available = torch.cuda.is_available()\n    cuda_version = torch.version.cuda\n    device_name = torch.cuda.get_device_name(0)\n    device_count = torch.cuda.device_count()\n    print(\"CUDA is available:\", cuda_is_available)\n    print(\"CUDA version:\", cuda_version)\n    print(\"Device name:\", device_name)\n    print(\"Device count:\", device_count)\n    return cuda_is_available, cuda_version, device_name, device_count\n\n\n\n\n", "CodeBleu_score": 32.19449463534614}, {"code": "def cuda_info()\n\t\n    import torch\n    if torch.cuda.is_available():\n        print(\"CUDA is available\")\n        print(\"CUDA version:\", torch.version.cuda)\n        print(\"Number of GPUs available:\", torch.cuda.device_count())\n        for i in range(torch.cuda.device_count()):\n            print(\"GPU device name:\", torch.cuda.get_device_name(i))\n    else:\n        print(\"CUDA is not available\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 49.462611681242805}, {"code": "def cuda_info()\n\t\n    import torch\n    device_count = torch.cuda.device_count()\n    device_names = []\n    for i in range(device_count):\n        device_names.append(torch.cuda.get_device_name(i))\n    cuda_available = torch.cuda.is_available()\n    cuda_version = torch.version.cuda\n    return device_names, cuda_available, cuda_version\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 47.36525435277335}]}
{"instance_id": "a2ba908a4c70", "generation_result": [{"code": "def compute_encoder_accuracy(dataset_items, encoder_manager)\n\t\n    class_name_key = encoder_manager.config.class_name_key\n    class_id_to_class_name = encoder_manager.config.class_id_to_class_name\n    num_items = len(dataset_items)\n    num_correct = 0\n    accuracy_per_class = {}\n    for item in dataset_items:\n        audio_file = item.audio_file\n        true_class_name = item.metadata[class_name_key]\n        true_class_id = item.metadata.get(\"class_id\", None)\n        embedding = encoder_manager.compute_embedding(audio_file)\n        predicted_class_name = encoder_manager.predict_class(embedding)\n        if true_class_name == predicted_class_name:\n            num_correct += 1\n        if true_class_name in accuracy_per_class:\n            accuracy_per_class[true_class_name][\"num_correct\"] += 1\n            accuracy_per_class[true_class_name][\"num_total\"] += 1\n        else:\n            accuracy_per_class[true_class_name] = {\n                \"num_correct\": 1 if true_class_name == predicted_class_name else 0,\n                \"num_total\": 1,\n            }\n        if true_class_id is not None and class_id_to_class_name is not None:\n            true_class_name = class_id_to_class_name[true_class_id]\n            if true_class_name in accuracy_per_class:\n                accuracy_per_class[true_class_name][\"num_correct\"] += 1\n                accuracy_per_class[true_class_name][\"num_total\"] += 1\n            else:\n                accuracy_per_class[true_class_name] = {\n                    \"num_correct\": 1 if true_class_name == predicted_class_name else 0,\n                    \"num_total\": 1,\n                }\n    for class_name, stats in accuracy_per_class.items():\n        accuracy = stats[\"num_correct\"] / stats[\"num_total\"]\n        print(f\"Accuracy", "CodeBleu_score": 27.78298387046762}, {"code": "def compute_encoder_accuracy(dataset_items, encoder_manager)\n\t\n    import torch\n    from tqdm import tqdm\n    from collections import defaultdict\n    import numpy as np\n\n    class_name_key = encoder_manager.config.class_name_key\n    class_id_to_class_name_mapping = encoder_manager.config.class_id_to_class_name_mapping\n\n    if class_name_key is None:\n        raise ValueError(\"class_name_key is not defined in the encoder manager's configuration.\")\n\n    if class_id_to_class_name_mapping is None:\n        raise ValueError(\"class_id_to_class_name_mapping is not defined in the encoder manager's configuration.\")\n\n    class_accuracy = defaultdict(list)\n    total_accuracy = []\n\n    for item in tqdm(dataset_items):\n        audio_file = item['audio_file']\n        true_class_name = item[class_name_key]\n        class_id = item.get('class_id')\n\n        if class_id is not None:\n            true_class_name = class_id_to_class_name_mapping.get(class_id, true_class_name)\n\n        with torch.no_grad():\n            audio_features = encoder_manager.ap.load(audio_file)\n            embedding = encoder_manager.model.get_embedding(audio_features)\n            predicted_class_name = encoder_manager.model.classify_embedding(embedding)\n\n        if class_id is not None:\n            true_class_name = class_id_to_class_name_mapping.get(class_id, true_class_name)\n\n        accuracy = int(predicted_class_name == true_class_name)\n        class_accuracy[true_class_name].append(accuracy)\n        total_accuracy.append(accuracy)\n\n    for class_name, accuracies in class_accuracy.items():\n        class_accuracy[class_name] = np.mean(accuracies)\n\n    average_accuracy = np.mean(total_accuracy)\n\n    print(\"Class Accuracy:\")\n    for", "CodeBleu_score": 37.93159890260186}, {"code": "def compute_encoder_accuracy(dataset_items, encoder_manager)\n\t\n    import numpy as np\n    from tqdm import tqdm\n    from sklearn.metrics import accuracy_score\n\n    # Get the class name key and optional class ID to class name mapping from the encoder manager's configuration\n    class_name_key = encoder_manager.config.get(\"class_name_key\", \"class_name\")\n    class_id_to_class_name = encoder_manager.config.get(\"class_id_to_class_name\", None)\n\n    # Initialize variables to store the accuracy for each class and the total number of samples\n    class_accuracies = {}\n    total_samples = 0\n\n    # Iterate over each item in the dataset\n    for item in tqdm(dataset_items):\n        # Get the audio file and the true class name\n        audio_file = item[\"audio_file\"]\n        true_class_name = item[class_name_key]\n\n        # If a class ID to class name mapping is available, use it to convert the class name to a class ID\n        if class_id_to_class_name is not None:\n            true_class_id = list(class_id_to_class_name.keys())[\n                list(class_id_to_class_name.values()).index(true_class_name)\n            ]\n        else:\n            true_class_id = true_class_name\n\n        # Compute the embedding for the audio file\n        embedding = encoder_manager.compute_embedding(audio_file)\n\n        # Predict the class label if criteria and mappings are available\n        if (\n            encoder_manager.criteria is not None\n            and encoder_manager.criteria.class_name_to_id_mapping is not None\n        ):\n            predicted_class_id = encoder_manager.predict_class_id(embedding)\n            predicted_class_name = encoder_manager.predict_class_name(embedding)\n        else:\n            predicted_class_id = None\n            predicted_class_name = None\n\n        # Increment the total number of samples\n        total_samples += 1\n\n        # If the true class name is not in the class accuracies dictionary, add it with", "CodeBleu_score": 32.68037852280262}]}
{"instance_id": "e01ec69ad58a", "generation_result": [{"code": "def adjust_path_and_remove_silence(audio_path)\n\t\n\n    # adjust the path to the output directory\n    out_path = audio_path.replace(input_dir, output_dir)\n\n    # if the file already exists and force is not specified, return the output path and a False flag\n    if os.path.exists(out_path) and not force:\n        return out_path, False\n\n    # create the necessary directory structure for the output file\n    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n\n    # remove silence from the audio file and save the processed audio\n    out_path, is_speech = remove_silence(\n        model_and_utils,\n        audio_path,\n        out_path,\n        vad_sample_rate=8000,\n        trim_just_beginning_and_end=True,\n        use_cuda=False,\n    )\n\n    # return the output path and a flag indicating if speech was detected\n    return out_path, is_speech\n\n", "CodeBleu_score": 43.38560147343027}, {"code": "def adjust_path_and_remove_silence(audio_path)\n\t\n    # adjust output path\n    out_path = audio_path.replace(input_dir, output_dir)\n\n    # if file already exists, skip it\n    if os.path.exists(out_path):\n        if not force:\n            return out_path, False\n\n    # create the necessary directory structure\n    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n\n    # remove silence\n    out_path, is_speech = remove_silence(\n        model_and_utils, audio_path, out_path, vad_sample_rate, trim_just_beginning_and_end, use_cuda\n    )\n    return out_path, is_speech\n\n", "CodeBleu_score": 37.5975872996181}, {"code": "def adjust_path_and_remove_silence(audio_path)\n\t\n    # adjust output path\n    out_path = audio_path.replace(INPUT_DIR, OUTPUT_DIR)\n\n    # if file already exists and force is not specified, return output path and False flag\n    if os.path.exists(out_path) and not FORCE:\n        return out_path, False\n\n    # create necessary directory structure\n    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n\n    # remove silence from audio using specified parameters\n    out_path, is_speech = remove_silence(\n        model_and_utils, audio_path, out_path, vad_sample_rate=VAD_SAMPLE_RATE, trim_just_beginning_and_end=TRIM_JUST_BEGINNING_AND_END, use_cuda=USE_CUDA\n    )\n\n    # return output path and flag indicating if speech was detected\n    return out_path, is_speech\n\n", "CodeBleu_score": 40.00424740152609}]}
{"instance_id": "7e0bdb0feff8", "generation_result": [{"code": "def resample_files(input_dir, output_sr, output_dir=None, file_ext=\"wav\", n_jobs=10)\n\t\n\n    from pathlib import Path\n    from tqdm.auto import tqdm\n    from joblib import Parallel, delayed\n    import librosa\n    import soundfile as sf\n\n    if output_dir is not None:\n        output_dir = Path(output_dir)\n        output_dir.mkdir(exist_ok=True, parents=True)\n        input_dir = Path(input_dir)\n        for path in input_dir.rglob(\"*\"):\n            if path.is_dir():\n                (output_dir / path.relative_to(input_dir)).mkdir(exist_ok=True, parents=True)\n\n    def resample_file(path):\n        path = Path(path)\n        if output_dir is not None:\n            output_path = output_dir / path.relative_to(input_dir)\n        else:\n            output_path = path\n        y, sr = librosa.load(path, sr=None)\n        y_resampled = librosa.resample(y, sr, output_sr)\n        sf.write(output_path, y_resampled, output_sr)\n\n    input_dir = Path(input_dir)\n    files = list(input_dir.rglob(f\"*.{file_ext}\"))\n    with tqdm(total=len(files)) as pbar:\n        Parallel(n_jobs=n_jobs)(delayed(resample_file)(path) for path in files)\n        pbar.update(len(files))\n\n    print(\"Resampling completed successfully.\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 35.09390747092465}, {"code": "def resample_files(input_dir, output_sr, output_dir=None, file_ext=\"wav\", n_jobs=10)\n\t\n    import os\n    import librosa\n    import soundfile as sf\n    from joblib import Parallel, delayed\n    from tqdm import tqdm\n    from shutil import copytree\n\n    # Copy input directory to output directory if provided\n    if output_dir:\n        copytree(input_dir, output_dir)\n        input_dir = output_dir\n\n    # Recursively find all audio files with the specified extension\n    audio_files = []\n    for root, _, files in os.walk(input_dir):\n        for file in files:\n            if file.endswith(file_ext):\n                audio_files.append(os.path.join(root, file))\n\n    # Define function to resample audio file\n    def resample_audio(audio_file):\n        y, sr = librosa.load(audio_file, sr=None)\n        y_resampled = librosa.resample(y, sr, output_sr)\n        sf.write(audio_file, y_resampled, output_sr)\n\n    # Resample audio files in parallel using specified number of jobs\n    with tqdm(total=len(audio_files)) as pbar:\n        Parallel(n_jobs=n_jobs)(delayed(resample_audio)(audio_file) for audio_file in audio_files)\n        pbar.update(1)\n\n    # Print completion message\n    print(f\"Resampled {len(audio_files)} audio files to {output_sr} Hz.\")\n\n\n\n\n", "CodeBleu_score": 40.65299960168707}, {"code": "def resample_files(input_dir, output_sr, output_dir=None, file_ext=\"wav\", n_jobs=10)\n\t\n\n    import os\n    import shutil\n    from tqdm import tqdm\n    from pydub import AudioSegment\n    from joblib import Parallel, delayed\n\n    if output_dir is not None:\n        if os.path.exists(output_dir):\n            shutil.rmtree(output_dir)\n        shutil.copytree(input_dir, output_dir)\n    else:\n        output_dir = input_dir\n\n    audio_files = []\n    for root, _, files in os.walk(output_dir):\n        for file in files:\n            if file.endswith(file_ext):\n                audio_files.append(os.path.join(root, file))\n\n    def resample_file(file):\n        audio = AudioSegment.from_file(file)\n        audio = audio.set_frame_rate(output_sr)\n        audio.export(file, format=file_ext)\n\n    with tqdm(total=len(audio_files), desc=\"Resampling audio files\") as pbar:\n        def update(*a):\n            pbar.update()\n\n        Parallel(n_jobs=n_jobs)(delayed(resample_file)(file) for file in audio_files)\n\n    print(\"Resampling complete.\")\n\n", "CodeBleu_score": 36.81962230475457}]}
{"instance_id": "a1351834ca57", "generation_result": [{"code": "def evaluation(model, criterion, data_loader, global_step)\n\t\n    model.eval()\n    eval_loss = 0.0\n    num_classes_in_batch = data_loader.batch_size // data_loader.num_utter_per_class\n    with torch.no_grad():\n        for batch_idx, (inputs, labels) in enumerate(data_loader):\n            inputs = inputs.reshape(\n                [\n                    num_classes_in_batch,\n                    data_loader.num_utter_per_class,\n                    inputs.shape[-1],\n                ]\n            )\n            labels = labels.reshape(\n                [\n                    num_classes_in_batch,\n                    data_loader.num_utter_per_class,\n                ]\n            )\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            eval_loss += loss.item()\n\n        eval_loss /= len(data_loader)\n        writer.add_scalar(\"Loss/eval\", eval_loss, global_step)\n\n        fig = plot_embeddings(outputs, num_classes_in_batch)\n        writer.add_figure(\"UMAP\", fig, global_step)\n\n    return eval_loss\n\n", "CodeBleu_score": 35.98918869643202}, {"code": "def evaluation(model, criterion, data_loader, global_step)\n\t\n    model.eval()\n    total_loss = 0\n    total_accuracy = 0\n    total_num = 0\n    for batch_idx, (inputs, labels) in enumerate(data_loader):\n        with torch.no_grad():\n            inputs = inputs.reshape(\n                num_classes_in_batch, num_utter_per_class, inputs.shape[1]\n            )\n            labels = labels.reshape(\n                num_classes_in_batch, num_utter_per_class, labels.shape[1]\n            )\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n            embeddings = model(inputs)\n            embeddings = embeddings.reshape(\n                embeddings.shape[0] * embeddings.shape[1], embeddings.shape[2]\n            )\n            labels = labels.reshape(\n                labels.shape[0] * num_utter_per_class, labels.shape[2]\n            )\n            loss = criterion(embeddings, labels)\n            total_loss += loss.item()\n            total_accuracy += accuracy(embeddings, labels)\n            total_num += embeddings.shape[0]\n\n    avg_loss = total_loss / (batch_idx + 1)\n    avg_accuracy = total_accuracy / total_num\n    writer.add_scalar(\"Loss/test\", avg_loss, global_step)\n    writer.add_scalar(\"Accuracy/test\", avg_accuracy, global_step)\n\n    fig = plot_embeddings(embeddings, num_classes_in_batch)\n    writer.add_figure(\"UMAP\", fig, global_step=global_step)\n    return avg_loss\n\n", "CodeBleu_score": 36.11572817312244}, {"code": "def evaluation(model, criterion, data_loader, global_step)\n\t\n\n    model.eval()\n    eval_loss = 0.0\n    num_batches = 0\n\n    with torch.no_grad():\n        for batch_idx, (inputs, labels) in enumerate(data_loader):\n            inputs = inputs.reshape(\n                (num_classes_in_batch, num_utter_per_class, inputs.shape[1])\n            )\n            labels = labels.reshape(\n                (num_classes_in_batch, num_utter_per_class, labels.shape[1])\n            )\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            eval_loss += loss.item()\n            num_batches += 1\n\n    eval_loss /= num_batches\n    writer.add_scalar(\"Loss/eval\", eval_loss, global_step)\n\n    embeddings = outputs.detach().cpu().numpy()\n    fig = plot_embeddings(embeddings, num_classes_in_batch)\n    writer.add_figure(\"UMAP\", fig, global_step)\n\n    return eval_loss\n\n", "CodeBleu_score": 36.29963960843787}]}
{"instance_id": "565f8ed09916", "generation_result": [{"code": "def register_config(model_name: str) -> Coqpit\n\t\n    config_class = None\n    config_name = model_name + \"_config\"\n    config_class_name = to_camel(config_name)\n    if model_name == \"xtts\":\n        config_class = importlib.import_module(\"xtts.configs.\" + config_name).XTTSConfig\n    else:\n        try:\n            config_class = importlib.import_module(\"TTS.tts.configs.\" + config_name).BaseTTSConfig\n        except ModuleNotFoundError:\n            try:\n                config_class = importlib.import_module(\"TTS.vocoder.configs.\" + config_name).BaseVocoderConfig\n            except ModuleNotFoundError:\n                try:\n                    config_class = importlib.import_module(\"TTS.tts.configs.\" + config_name).BaseTTSConfig\n                except ModuleNotFoundError:\n                    pass\n    if config_class is None:\n        raise ModuleNotFoundError(f\" [!] Config for {model_name} cannot be found.\")\n    return config_class\n\n", "CodeBleu_score": 49.81342063258323}, {"code": "def register_config(model_name: str) -> Coqpit\n\t\n    if model_name == \"xtts\":\n        config_class = \"XttsConfig\"\n    else:\n        config_class = to_camel(model_name + \"_config\")\n\n    config_class_found = False\n    paths = [\n        \"TTS.tts.configs\",\n        \"TTS.vocoder.configs\",\n        \"TTS.encoder.configs\",\n        \"TTS.config\",\n    ]\n    for path in paths:\n        try:\n            config_class_found = find_module(path, config_class)\n        except ModuleNotFoundError:\n            pass\n    if not config_class_found:\n        raise ModuleNotFoundError(\n            f\" [!] Config for {model_name} cannot be found in {','.join(paths)}.\"\n        )\n    return config_class_found\n\n", "CodeBleu_score": 47.603394670343725}, {"code": "def register_config(model_name: str) -> Coqpit\n\t\n    model_name = model_name.lower()\n    config_class_name = to_camel(model_name)\n    config_class = None\n    if model_name == \"xtts\":\n        config_class = XTTSConfig\n    if config_class is None:\n        config_class = find_module(\n            \"TTS.config.shared_configs\", config_class_name + \"Config\"\n        )\n        if config_class is None:\n            config_class = find_module(\"TTS.config\", config_class_name + \"Config\")\n    if config_class is None:\n        raise ModuleNotFoundError(\n            f\" [!] Config for {model_name} cannot be found. \n Config file for {model_name} should be placed in TTS/config/shared_configs or TTS/config. \n If it is a new model you can base your config file on the template config at TTS/config/shared_configs/model_template.yaml\"\n        )\n    return config_class\n\n", "CodeBleu_score": 40.03687597832598}]}
{"instance_id": "0ea7862506a7", "generation_result": [{"code": "def load_config(config_path: str) -> Coqpit\n\t\n    config_dict = {}\n    ext = os.path.splitext(config_path)[1]\n    if ext.lower() in (\".yaml\", \".yml\"):\n        with fsspec.open(config_path, \"r\", encoding=\"utf-8\") as yaml_file:\n            config_dict = yaml.load(yaml_file, Loader=yaml.Loader)\n    elif ext.lower() in (\".json\"):\n        config_dict = read_json_with_comments(config_path)\n    else:\n        raise TypeError(f\" [!] Unknown config file type. Use .yaml or .json file formats. {config_path}\")\n\n    model_name = _process_model_name(config_dict)\n    config_class = register_config(model_name)\n    config = config_class()\n    config.model = model_name\n    config.from_dict(config_dict)\n    return config\n", "CodeBleu_score": 56.32444102153546}, {"code": "def load_config(config_path: str) -> Coqpit\n\t\n    config_dict = {}\n    ext = config_path.split(\".\")[-1]\n    if ext.lower() == \"yaml\":\n        config_dict = read_yaml(config_path)\n    elif ext.lower() == \"json\":\n        config_dict = read_json_with_comments(config_path)\n    else:\n        raise TypeError(f\" [!] Unknown config file type. Use .yaml or .json file formats. {config_path}\")\n\n    model_name = _process_model_name(config_dict)\n    config_class = register_config(model_name)\n    config = config_class()\n    config.from_dict(config_dict)\n    return config\n", "CodeBleu_score": 42.33682040016231}, {"code": "def load_config(config_path: str) -> Coqpit\n\t\n    config_dict = {}\n    ext = config_path.split(\".\")[-1]\n    if ext.lower() in [\"yaml\", \"yml\"]:\n        config_dict = read_yaml(config_path)\n    elif ext.lower() == \"json\":\n        config_dict = read_json_with_comments(config_path)\n    else:\n        raise ValueError(f\" [!] Unknown config type. Only yaml and json are supported. {config_path}\")\n\n    model_name = _process_model_name(config_dict)\n    config_class = register_config(model_name)\n    config = config_class()\n    config.from_dict(config_dict)\n    return config\n", "CodeBleu_score": 40.798284130178956}]}
{"instance_id": "e0db68b63143", "generation_result": [{"code": "def list_files(basePath, validExts=None, contains=None)\n\t\n    # loop over the directory structure\n    for (rootDir, dirNames, filenames) in os.walk(basePath):\n        # loop over the filenames in the current directory\n        for filename in filenames:\n            # if the contains string is not none and the filename does not contain\n            # the supplied string, then ignore the file\n            if contains is not None and filename.find(contains) == -1:\n                continue\n\n            # determine the file extension of the current file\n            ext = filename[filename.rfind(\".\"):].lower()\n\n            # check to see if the file is an image and should be processed\n            if validExts is None or ext.endswith(validExts):\n                # construct the path to the image and yield it\n                imagePath = os.path.join(rootDir, filename)\n                yield imagePath\n\n\n", "CodeBleu_score": 77.47690953282769}, {"code": "def list_files(basePath, validExts=None, contains=None)\n\t\n\n    # loop over the directory structure\n    for (rootDir, dirNames, filenames) in os.walk(basePath):\n        # loop over the filenames in the current directory\n        for filename in filenames:\n            # if the contains string is not none and the filename does not contain\n            # the supplied string, then ignore the file\n            if contains is not None and filename.find(contains) == -1:\n                continue\n\n            # determine the file extension of the current file\n            ext = filename[filename.rfind(\".\"):].lower()\n\n            # check to see if the file is an image and should be processed\n            if validExts is None or ext.endswith(validExts):\n                # construct the path to the image and yield it\n                imagePath = os.path.join(rootDir, filename)\n                yield imagePath\n\n\n\n", "CodeBleu_score": 77.47690953282769}, {"code": "def list_files(basePath, validExts=None, contains=None)\n\t\n\n    # Loop over the files in the directory structure\n    for (rootDir, dirNames, filenames) in os.walk(basePath):\n        # Loop over the filenames in the current directory\n        for filename in filenames:\n            # If the contains string is not none and the filename does not contain\n            # the supplied string, then ignore the file\n            if contains is not None and filename.find(contains) == -1:\n                continue\n\n            # Determine the file extension of the current file\n            ext = filename[filename.rfind(\".\"):].lower()\n\n            # Check to see if the file is an image and should be processed\n            if validExts is None or ext.endswith(validExts):\n                # Construct the path to the image and yield it\n                imagePath = os.path.join(rootDir, filename)\n                yield imagePath\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 77.47690953282769}]}
{"instance_id": "b836fcaf0a27", "generation_result": [{"code": "def load_model(xtts_checkpoint, xtts_config, xtts_vocab)\n\t\n    clear_gpu_cache()\n    if xtts_checkpoint is None or xtts_config is None or xtts_vocab is None:\n        return \"Error: Please provide all required paths for the XTTS model.\"\n\n    config = load_config(xtts_config)\n    model = XTTSModel(config)\n    model.load_checkpoint(config, checkpoint_path=xtts_checkpoint, vocab_path=xtts_vocab)\n    if torch.cuda.is_available():\n        model.cuda()\n    print(\"XTTS Model loaded successfully.\")\n    return \"XTTS Model loaded successfully.\"\n\n", "CodeBleu_score": 46.661306586325814}, {"code": "def load_model(xtts_checkpoint, xtts_config, xtts_vocab)\n\t\n    clear_gpu_cache()\n    if xtts_checkpoint is None or xtts_config is None or xtts_vocab is None:\n        return \"Error: XTTS checkpoint, config, and vocab paths are required\"\n    config = load_config(xtts_config)\n    model = XTTSModel(config)\n    model.load_checkpoint(config, checkpoint_path=xtts_checkpoint, vocab_path=xtts_vocab)\n    if torch.cuda.is_available():\n        model.cuda()\n    print(\"XTTS model loaded successfully\")\n    return \"XTTS model loaded successfully\"\n\n", "CodeBleu_score": 47.16431030167411}, {"code": "def load_model(xtts_checkpoint, xtts_config, xtts_vocab)\n\t\n    clear_gpu_cache()\n    if xtts_checkpoint is None or xtts_config is None or xtts_vocab is None:\n        return \"Error: xtts_checkpoint, xtts_config, and xtts_vocab are required\"\n    config = load_config(xtts_config)\n    model = XTTSModel(config)\n    model.load_checkpoint(config, xtts_checkpoint, xtts_vocab)\n    if torch.cuda.is_available():\n        model.cuda()\n    print(\"XTTS model loaded successfully\")\n    return \"XTTS model loaded successfully\"\n", "CodeBleu_score": 47.22431894041759}]}
{"instance_id": "c5d63aa0df26", "generation_result": [{"code": "def compute_embedding(self, x, num_frames=250, num_eval=10, return_mean=True, l2_norm=True)\n\t\n    # Adjust number of frames based on the hop length\n    if num_frames is not None:\n        num_frames = num_frames if self.hop_length is None else int(\n            num_frames * self.hop_length / self.n_fft)\n\n    # Ensure the number of frames does not exceed the input length\n    num_frames = min(num_frames, x.shape[-1])\n\n    # Calculate offset positions for evaluation\n    step = int(num_frames / num_eval)\n    offsets = range(0, x.shape[-1] - num_frames + 1, step)\n\n    # Extract frames at the offset positions and concatenate them into a batch\n    frames = torch.cat([x[..., i:i + num_frames] for i in offsets], dim=0)\n\n    # Perform inference to obtain embeddings\n    embeddings = self.inference(frames)\n\n    # Compute the mean of the embeddings\n    if return_mean:\n        embeddings = torch.mean(embeddings, dim=0)\n\n    # Normalize the embeddings with L2 norm\n    if l2_norm:\n        embeddings = torch.nn.functional.normalize(embeddings, dim=0)\n\n    return embeddings\n\n", "CodeBleu_score": 43.73683337955192}, {"code": "def compute_embedding(self, x, num_frames=250, num_eval=10, return_mean=True, l2_norm=True)\n\t\n    if isinstance(self.model, torch.nn.Module):\n        hop_length = self.model.segment_length // self.model.hop_length\n    else:\n        hop_length = 1\n    if num_frames > x.shape[1]:\n        num_frames = x.shape[1]\n    offsets = np.linspace(0, x.shape[1] - num_frames, num=num_eval)\n    frames = []\n    for offset in offsets:\n        offset = int(offset)\n        frames.append(x[:, offset * hop_length : (offset + num_frames) * hop_length])\n    frames = np.vstack(frames)\n    embeddings = self.inference(frames)\n    if return_mean:\n        embeddings = torch.mean(embeddings, axis=0)\n        if l2_norm:\n            embeddings = embeddings / torch.norm(embeddings)\n    return embeddings\n\n", "CodeBleu_score": 49.30207913104322}, {"code": "def compute_embedding(self, x, num_frames=250, num_eval=10, return_mean=True, l2_norm=True)\n\t\n    hop_length = int(self.sample_rate * self.hop_length)\n    if num_frames is None:\n        num_frames = math.ceil(len(x) / hop_length)\n    else:\n        num_frames = min(num_frames, math.ceil(len(x) / hop_length))\n    offsets = np.linspace(0, len(x) - num_frames * hop_length, num_eval)\n    frames = np.array([x[int(offset) : int(offset) + num_frames * hop_length] for offset in offsets])\n    frames = torch.tensor(frames, dtype=torch.float32)\n    embeddings = self.inference(frames)\n    if return_mean:\n        embeddings = torch.mean(embeddings, dim=0)\n        if l2_norm:\n            embeddings = torch.nn.functional.normalize(embeddings, dim=0)\n    return embeddings\n\n", "CodeBleu_score": 43.49787735003474}]}
{"instance_id": "06f5b0335b26", "generation_result": [{"code": "def create_layer(self, block, planes, blocks, stride=1)\n\t\n    downsample = None\n    if stride != 1 or self.inplanes != planes * block.expansion:\n        downsample = nn.Sequential(\n            nn.Conv2d(self.inplanes, planes * block.expansion,\n                      kernel_size=1, stride=stride, bias=False),\n            nn.BatchNorm2d(planes * block.expansion),\n        )\n\n    layers = []\n    layers.append(block(self.inplanes, planes, stride, downsample))\n    self.inplanes = planes * block.expansion\n    for i in range(1, blocks):\n        layers.append(block(self.inplanes, planes))\n\n    return nn.Sequential(*layers)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 85.18641475692186}, {"code": "def create_layer(self, block, planes, blocks, stride=1)\n\t\n    downsample = None\n    if stride != 1 or self.inplanes != planes * block.expansion:\n        downsample = nn.Sequential(\n            nn.Conv2d(self.inplanes, planes * block.expansion,\n                      kernel_size=1, stride=stride, bias=False),\n            nn.BatchNorm2d(planes * block.expansion),\n        )\n\n    layers = []\n    layers.append(block(self.inplanes, planes, stride, downsample))\n    self.inplanes = planes * block.expansion\n    for i in range(1, blocks):\n        layers.append(block(self.inplanes, planes))\n\n    return nn.Sequential(*layers)\n\n\n\n", "CodeBleu_score": 85.18641475692186}, {"code": "def create_layer(self, block, planes, blocks, stride=1)\n\t\n    downsample = None\n    if stride != 1 or self.inplanes != planes * block.expansion:\n        downsample = nn.Sequential(\n            nn.Conv2d(self.inplanes, planes * block.expansion,\n                      kernel_size=1, stride=stride, bias=False),\n            nn.BatchNorm2d(planes * block.expansion),\n        )\n\n    layers = []\n    layers.append(block(self.inplanes, planes, stride, downsample))\n    self.inplanes = planes * block.expansion\n    for i in range(1, blocks):\n        layers.append(block(self.inplanes, planes))\n\n    return nn.Sequential(*layers)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 85.18641475692186}]}
{"instance_id": "9ac60597b633", "generation_result": [{"code": "def plot_embeddings(embeddings, num_classes_in_batch)\n\t\n    import umap\n    import matplotlib.pyplot as plt\n    import numpy as np\n    import torch\n\n    # Calculate the number of utterances per class\n    num_utterances_per_class = [torch.sum(num_classes_in_batch == i).item() for i in range(torch.max(num_classes_in_batch).item() + 1)]\n\n    # Limit the number of classes to 10 if necessary\n    if len(num_utterances_per_class) > 10:\n        num_utterances_per_class = num_utterances_per_class[:10]\n\n    # Calculate the cumulative sum of utterances per class\n    cumulative_utterances_per_class = np.cumsum(num_utterances_per_class)\n\n    # Reduce the dimensionality of the embeddings using UMAP\n    reducer = umap.UMAP()\n    reduced_embeddings = reducer.fit_transform(embeddings)\n\n    # Create a scatter plot of the reduced embeddings\n    plt.figure(figsize=(10, 8))\n    plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], c=num_classes_in_batch, cmap='tab10')\n    plt.title(\"UMAP projection\")\n    plt.axis('equal')\n    plt.savefig(\"umap\")\n\n    # Return the figure\n    return plt.gcf()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 39.66699338467895}, {"code": "def plot_embeddings(embeddings, num_classes_in_batch)\n\t\n    # Calculate the number of utterances per class\n    num_utterances_per_class = [np.sum(num_classes_in_batch == i) for i in range(10)]\n    # Limit the number of classes to 10 if necessary\n    if len(num_utterances_per_class) > 10:\n        num_utterances_per_class = num_utterances_per_class[:10]\n    # UMAP transforms the embeddings\n    reducer = umap.UMAP()\n    embedding = reducer.fit_transform(embeddings)\n    # Create a scatter plot with colors representing different classes\n    plt.figure(figsize=(10, 8))\n    plt.scatter(embedding[:, 0], embedding[:, 1], c=num_classes_in_batch, cmap='tab10')\n    plt.gca().set_aspect('equal', 'datalim')\n    plt.colorbar(boundaries=np.arange(11)-0.5).set_ticks(np.arange(10))\n    plt.title('UMAP projection')\n    plt.savefig(\"umap\")\n    # Return the plot as a figure\n    return plt.gcf()\n\n\n\n\n", "CodeBleu_score": 35.008378838733925}, {"code": "def plot_embeddings(embeddings, num_classes_in_batch)\n\t\n    # Calculate the number of utterances per class\n    num_utterances_per_class = [sum(embeddings[:, 1] == i) for i in range(num_classes_in_batch)]\n\n    # Limit the number of classes to 10\n    num_classes_to_plot = min(10, num_classes_in_batch)\n\n    # Get the indices of the top classes\n    top_class_indices = sorted(range(len(num_utterances_per_class)), key=lambda i: num_utterances_per_class[i], reverse=True)[:num_classes_to_plot]\n\n    # Filter the embeddings to include only the top classes\n    filtered_embeddings = embeddings[np.isin(embeddings[:, 1], top_class_indices)]\n\n    # Perform dimensionality reduction using UMAP\n    reducer = umap.UMAP()\n    embedding = reducer.fit_transform(filtered_embeddings[:, 0])\n\n    # Create a scatter plot with colors representing different classes\n    plt.figure(figsize=(10, 10))\n    scatter = plt.scatter(embedding[:, 0], embedding[:, 1], c=filtered_embeddings[:, 1], cmap='tab10')\n    plt.gca().set_aspect('equal', 'datalim')\n    plt.colorbar(scatter)\n    plt.title('UMAP projection')\n\n    # Save the plot as an image\n    plt.savefig('umap.png')\n\n    # Return the figure\n    return plt.gcf()\n\n\n\n", "CodeBleu_score": 40.418359874721865}]}
{"instance_id": "19c2b4af256d", "generation_result": [{"code": "def calc_cosine_sim(self, dvecs, centroids)\n\t\n    num_spkrs, num_utts = dvecs.size()[0], dvecs.size()[1]\n    cos_sim_matrix = []\n    for spkr in range(num_spkrs):\n        cos_sim_row = []\n        for utt in range(num_utts):\n            # Calculate new centroids excluding the current utterance\n            new_centroids = self.calc_new_centroids(dvecs, centroids, spkr, utt)\n\n            # Compute cosine similarity with new centroids\n            cos_sim = F.cosine_similarity(\n                dvecs[spkr, utt].repeat(num_spkrs, 1), new_centroids\n            )\n            # Clamp the values to avoid small values\n            cos_sim = torch.clamp(cos_sim, 1e-7, 1 - 1e-7)\n\n            # Concatenate the cosine similarity values and stack them\n            cos_sim_row.append(cos_sim)\n        cos_sim_row = torch.cat(cos_sim_row, 0)\n        cos_sim_matrix.append(cos_sim_row)\n    cos_sim_matrix = torch.stack(cos_sim_matrix, 0)\n    return cos_sim_matrix", "CodeBleu_score": 36.38262628762205}, {"code": "def calc_cosine_sim(self, dvecs, centroids)\n\t\n    num_spkrs, num_utts = dvecs.shape[:2]\n    cos_sim_matrix = []\n    for spkr, speaker_dvecs in enumerate(dvecs):\n        cos_sim_row = []\n        for utt, ref_dvec in enumerate(speaker_dvecs):\n            centroids_wo_ref = self.calc_new_centroids(dvecs, centroids, spkr, utt)\n            cos_sim = torch.clamp(\n                torch.mm(ref_dvec.unsqueeze(1), centroids_wo_ref.transpose(1, 0))\n                / (torch.norm(ref_dvec) * torch.norm(centroids_wo_ref, dim=1)),\n                0.0,\n                1.0,\n            )\n            cos_sim_row.append(cos_sim)\n        cos_sim_row = torch.cat(cos_sim_row, dim=1)\n        cos_sim_matrix.append(cos_sim_row)\n    cos_sim_matrix = torch.cat(cos_sim_matrix)\n    return cos_sim_matrix\n\n", "CodeBleu_score": 46.718740149526624}, {"code": "def calc_cosine_sim(self, dvecs, centroids)\n\t\n    cos_sim_matrix = []\n    for spkr, speaker in enumerate(dvecs):\n        spkr_cos_sim = []\n        for utt, utterance in enumerate(speaker):\n            if spkr == len(dvecs) - 1:\n                break\n            new_centroids = self.calc_new_centroids(dvecs, centroids, spkr, utt)\n            cos_sim = torch.clamp(\n                torch.mm(\n                    utterance.unsqueeze(1).transpose(0, 1),\n                    new_centroids.transpose(0, 1),\n                ),\n                1e-5,\n            )\n            spkr_cos_sim.append(cos_sim)\n        spkr_cos_sim = torch.cat(spkr_cos_sim, dim=1)\n        cos_sim_matrix.append(spkr_cos_sim)\n    return torch.cat(cos_sim_matrix)", "CodeBleu_score": 56.64110358230443}]}
{"instance_id": "c525b411a1e7", "generation_result": [{"code": " def embed_loss_softmax(self, dvecs, cos_sim_matrix)\n\t\n    N = dvecs.size(0)\n    main_loss = dvecs.new(N, N).fill_(0)\n    nloss = dvecs.new(N, N).fill_(0)\n    for j in range(N):\n        dvec_j = dvecs[j].unsqueeze(0)\n        ref_dvecs = torch.cat((dvecs[:j], dvecs[j + 1:]), dim=0)\n        dvecs_except_j = ref_dvecs.repeat(j, 1)\n        if j != N - 1:\n            dvecs_except_j = torch.cat((dvecs_except_j, ref_dvecs), dim=0)\n        cos_sim_matrix_j = cos_sim_matrix[j, :]\n        cos_sim_matrix_j = cos_sim_matrix_j.unsqueeze(0)\n        cos_sim_matrix_j_large = cos_sim_matrix_j.repeat(N, 1)\n        cos_sim_diff = cos_sim_matrix_j_large - cos_sim_matrix_j.transpose(0, 1)\n        cos_sim_matrix_j_final = cos_sim_diff + cos_sim_matrix_j\n        nloss[j, :] = -F.log_softmax(cos_sim_matrix_j_final, dim=0).diag()\n    main_loss = nloss.sum(0)\n    return main_loss\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 19.65975703703974}, {"code": " def embed_loss_softmax(self, dvecs, cos_sim_matrix)\n\t\n    N = dvecs.size(0)\n    main_loss = dvecs.new(N, N).fill_(0)\n    nloss = dvecs.new(N, N).fill_(0)\n    for j in range(N):\n        nloss[j] = -F.log_softmax(cos_sim_matrix[j], 0)\n    for i in range(N):\n        main_loss[i] = nloss[:, i]\n    return main_loss\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 33.990349405993655}, {"code": " def embed_loss_softmax(self, dvecs, cos_sim_matrix)\n\t\n    N = dvecs.size(0)\n    main_loss = dvecs.new(N, N).fill_(0)\n    nloss = dvecs.new(N, N).fill_(0)\n    for j in range(N):\n        dvec_j = dvecs[j].unsqueeze(1)\n        x = torch.sum(dvec_j * dvecs, dim=2)\n        x = x.unsqueeze(1)\n        sub_loss = -F.log_softmax(x, dim=0)\n        main_loss[:, j] = sub_loss[:, 0]\n        nloss[:, j] = sub_loss[:, 0]\n    return main_loss, nloss\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 22.589195254999044}]}
{"instance_id": "25ac3945da2e", "generation_result": [{"code": "def embed_loss_contrast(self, dvecs, cos_sim_matrix)\n\t\n    loss_row = []\n    for i in range(dvecs.size()[0]):\n        excl_cent = [x for x in range(dvecs.size()[0]) if x != i]\n        centroids_sigmoids = torch.sigmoid(cos_sim_matrix[i, excl_cent])\n        loss_row.append(\n            sum(-torch.log(centroids_sigmoids) -\n                torch.log(1 - centroids_sigmoids)))\n    return torch.stack(loss_row)\n\n", "CodeBleu_score": 31.56089092767269}, {"code": "def embed_loss_contrast(self, dvecs, cos_sim_matrix)\n\t\n    loss_row = []\n    for i in range(len(dvecs)):\n        centroids_sigmoids = torch.sigmoid(cos_sim_matrix[i, :])\n        excl_centroids_sigmoids = torch.cat((centroids_sigmoids[:i], centroids_sigmoids[i + 1:]))\n        loss_row.append(1. - torch.sigmoid(cos_sim_matrix[i, i]) + torch.max(excl_centroids_sigmoids))\n    return torch.stack(loss_row)\n\n", "CodeBleu_score": 39.582193426256254}, {"code": "def embed_loss_contrast(self, dvecs, cos_sim_matrix)\n\t\n    loss_rows = []\n    for i, dvec_A in enumerate(dvecs):\n        contrast_this_row = torch.cat(\n            [dvecs[j] for j in range(len(dvecs)) if j != i]\n        )\n        contrast_product = torch.matmul(contrast_this_row, dvec_A)\n        contrast_losses_row = torch.log(\n            1 + torch.exp(contrast_product * self.cos_scale)\n        )\n        loss_rows.append(contrast_losses_row)\n    loss = torch.cat(loss_rows)\n    return loss\n\n", "CodeBleu_score": 31.40569878285795}]}
{"instance_id": "878d14ffabd2", "generation_result": [{"code": "def forward(self, x, _label=None)\n\t\n    (num_speakers, num_utts, dvec_feats) = x.shape\n    if num_speakers < 2:\n        raise ValueError(\n            f\"{self.__class__.__name__} requires more than 2 speaker embeddings, but received {num_speakers}.\"\n        )\n    if num_utts < 2:\n        raise ValueError(\n            f\"{self.__class__.__name__} requires each speaker to have more than 2 utterances, but received {num_utts}.\"\n        )\n    if _label is None:\n        label = torch.arange(num_speakers)\n    else:\n        label = _label\n    if self.device is not None:\n        label = label.to(self.device)\n\n    # Calculate centroids\n    centroids = self.get_centroids(x)\n\n    # Calculate the cosine similarity matrix\n    cos_sim_matrix = self.calc_cosine_sim(x, centroids)\n\n    # Scale cosine similarity matrix\n    cos_sim_matrix *= self.sim_scale\n\n    # Shift cosine similarity matrix\n    cos_sim_matrix += self.sim_bias\n\n    # Calculate the speaker embedding loss\n    loss = self.speaker_embedding_loss(cos_sim_matrix, label)\n\n    return loss.mean()", "CodeBleu_score": 39.65766530966843}, {"code": "def forward(self, x, _label=None)\n\t\n    (num_spks, num_utts, dvec_feats) = x.shape\n    if num_spks < 2:\n        return 0\n    if num_utts < 2:\n        return 0\n    centroids = torch.mean(x, dim=1)\n    # x = x.view(-1, dvec_feats)\n    cos_sim_matrix = self.calc_cosine_sim(x, centroids)\n    torch.clamp(self.w * cos_sim_matrix + self.b, 1e-6, 1 - 1e-6)\n    loss = torch.mean(-torch.log(cos_sim_matrix))\n    return loss", "CodeBleu_score": 48.084952134612244}, {"code": "def forward(self, x, _label=None)\n\t\n    (num_speakers, num_utts_per_speaker, dvec_feats) = x.shape\n    if num_speakers < 2:\n        raise ValueError(\n            f\"{self.__class__.__name__} requires more than 1 speaker per batch. Received batch with \"\n            f\"{num_speakers} speakers, each with {num_utts_per_speaker} utterances.\"\n        )\n    if num_utts_per_speaker < 2:\n        raise ValueError(\n            f\"{self.__class__.__name__} requires each speaker to have more than 1 utterance. Received batch \"\n            f\"with {num_speakers} speakers, each with {num_utts_per_speaker} utterances.\"\n        )\n    if _label is None:\n        _label = torch.arange(num_speakers).repeat_interleave(num_utts_per_speaker)\n        _label = _label.to(x.device)\n    centroids = self.calc_centroids(x)\n    cos_sim_matrix = self.calc_cosine_sim(x, centroids)\n    L = self.embed_loss(cos_sim_matrix, _label)\n    return L.mean(), cos_sim_matrix", "CodeBleu_score": 37.89052165821867}]}
{"instance_id": "86a364c72caa", "generation_result": [{"code": "def get_voices(extra_voice_dirs: List[str] = [])\n\t\n    extra_voice_dirs.append(\"./voices\")\n    voices = {}\n\n    for extra_voice_dir in extra_voice_dirs:\n        for subdir, dirs, files in os.walk(extra_voice_dir):\n            subdir_name = os.path.basename(subdir)\n            if subdir_name not in voices:\n                voices[subdir_name] = []\n\n            npz_files = [os.path.join(subdir, f) for f in files if f.endswith(\".npz\")]\n            wav_files = [os.path.join(subdir, f) for f in files if f.endswith(\".wav\")]\n            mp3_files = [os.path.join(subdir, f) for f in files if f.endswith(\".mp3\")]\n\n            if npz_files:\n                voices[subdir_name].extend(npz_files)\n            elif wav_files or mp3_files:\n                voices[subdir_name].extend(wav_files + mp3_files)\n\n    return voices\n\n", "CodeBleu_score": 30.10854125248519}, {"code": "def get_voices(extra_voice_dirs: List[str] = [])\n\t\n    extra_voice_dirs.append(os.path.join(script_path, \"voices\"))\n    voices = {}\n    for extra_voice_dir in extra_voice_dirs:\n        for voice_dir, subdirs, voice_files in os.walk(extra_voice_dir):\n            if not voice_files:\n                continue\n            if not any(f.endswith(\".npz\") for f in voice_files):\n                voice_files = [f for f in voice_files if f.endswith(\".wav\") or f.endswith(\".mp3\")]\n            voice_dir_name = os.path.basename(voice_dir)\n            voices[voice_dir_name] = [os.path.join(voice_dir, f) for f in voice_files]\n    return voices\n\n\n\n", "CodeBleu_score": 27.67590536545141}, {"code": "def get_voices(extra_voice_dirs: List[str] = [])\n\t\n    extra_voice_dirs.append(\"./voices\")\n    voices = {}\n    for extra_voice_dir in extra_voice_dirs:\n        for subdir, dirs, files in os.walk(extra_voice_dir):\n            for file in files:\n                if file.endswith(\".npz\"):\n                    if subdir not in voices:\n                        voices[subdir] = []\n                    voices[subdir].append(os.path.join(subdir, file))\n\n        for subdir, dirs, files in os.walk(extra_voice_dir):\n            if subdir not in voices:\n                voices[subdir] = []\n            for file in files:\n                if file.endswith(\".wav\") or file.endswith(\".mp3\"):\n                    voices[subdir].append(os.path.join(subdir, file))\n    return voices\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 26.134113850415698}]}
{"instance_id": "02f20c8917e6", "generation_result": [{"code": "def load_voice(model, voice: str, extra_voice_dirs: List[str] = [])\n\t\n    if voice == \"random\":\n        return None, None, None\n\n    voices = get_voices(extra_voice_dirs)\n    paths = voices[voice]\n\n    # bark only uses a single sample for cloning\n    if len(paths) > 1:\n        raise ValueError(f\"Voice {voice} has multiple paths: {paths}\")\n\n    try:\n        path = voices[voice]\n    except KeyError as e:\n        raise KeyError(f\"Voice {voice} not found in {extra_voice_dirs}\") from e\n\n    if len(paths) == 1 and paths[0].endswith(\".npz\"):\n        return load_npz(path[0])\n\n    audio_path = paths[0]\n    # replace the file extension with .npz\n    output_path = os.path.splitext(audio_path)[0] + \".npz\"\n    generate_voice(audio=audio_path, model=model, output_path=output_path)\n    return load_voice(model, voice, extra_voice_dirs)\n\n", "CodeBleu_score": 96.1921767598096}, {"code": "def load_voice(model, voice: str, extra_voice_dirs: List[str] = [])\n\t\n    if voice == \"random\":\n        return None, None, None\n\n    voices = get_voices(extra_voice_dirs)\n    paths = voices[voice]\n\n    # bark only uses a single sample for cloning\n    if len(paths) > 1:\n        raise ValueError(f\"Voice {voice} has multiple paths: {paths}\")\n\n    try:\n        path = voices[voice]\n    except KeyError as e:\n        raise KeyError(f\"Voice {voice} not found in {extra_voice_dirs}\") from e\n\n    if len(paths) == 1 and paths[0].endswith(\".npz\"):\n        return load_npz(path[0])\n\n    audio_path = paths[0]\n    # replace the file extension with .npz\n    output_path = os.path.splitext(audio_path)[0] + \".npz\"\n    generate_voice(audio=audio_path, model=model, output_path=output_path)\n    return load_voice(model, voice, extra_voice_dirs)\n", "CodeBleu_score": 96.1921767598096}, {"code": "def load_voice(model, voice: str, extra_voice_dirs: List[str] = [])\n\t\n    if voice == \"random\":\n        return None, None, None\n\n    voices = get_voices(extra_voice_dirs)\n    paths = voices[voice]\n\n    # bark only uses a single sample for cloning\n    if len(paths) > 1:\n        raise ValueError(f\"Voice {voice} has multiple paths: {paths}\")\n\n    try:\n        path = voices[voice]\n    except KeyError as e:\n        raise KeyError(f\"Voice {voice} not found in {extra_voice_dirs}\") from e\n\n    if len(paths) == 1 and paths[0].endswith(\".npz\"):\n        return load_npz(path[0])\n\n    audio_path = paths[0]\n    # replace the file extension with .npz\n    output_path = os.path.splitext(audio_path)[0] + \".npz\"\n    generate_voice(audio=audio_path, model=model, output_path=output_path)\n    return load_voice(model, voice, extra_voice_dirs)\n\n", "CodeBleu_score": 96.1921767598096}]}
{"instance_id": "9458e981389f", "generation_result": [{"code": "def _relative_shift(self, pos_score: torch.Tensor) -> torch.Tensor\n\t\n    batch_size, seq_len, _ = pos_score.size()\n    zeros = pos_score.new_zeros(batch_size, seq_len, 1)\n    padded_pos_score = torch.cat([zeros, pos_score], dim=-1)\n    padded_pos_score = padded_pos_score.view(batch_size, seq_len + 1, seq_len + 1)\n    pos_score = padded_pos_score[:, 1:].view_as(pos_score)\n    return pos_score\n\n", "CodeBleu_score": 40.78440469425969}, {"code": "def _relative_shift(self, pos_score: torch.Tensor) -> torch.Tensor\n\t\n    batch_size, num_heads, seq_len1, seq_len2 = pos_score.size()\n    zeros = pos_score.new_zeros(batch_size, num_heads, seq_len1, 1)\n    padded_pos_score = torch.cat([zeros, pos_score], dim=-1)\n    padded_pos_score = padded_pos_score.view(\n        batch_size, num_heads, seq_len2 + 1, seq_len1)\n    pos_score = padded_pos_score[:, :, 1:].view_as(pos_score)\n    return pos_score\n\n", "CodeBleu_score": 47.2487815016042}, {"code": "def _relative_shift(self, pos_score: torch.Tensor) -> torch.Tensor\n\t\n    batch_size, num_heads, seq_length1, seq_length2 = pos_score.size()\n    zeros = pos_score.new_zeros(batch_size, num_heads, seq_length1, 1)\n    padded_pos_score = torch.cat([zeros, pos_score], dim=-1)\n    padded_pos_score = padded_pos_score.view(batch_size, num_heads, seq_length2 + 1, seq_length1)\n    pos_score = padded_pos_score[:, :, 1:].view_as(pos_score)\n    return pos_score\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 57.08735369887231}]}
{"instance_id": "c046d4f00e80", "generation_result": [{"code": "def run_padded_sequence(self, context, lens)\n\t\n    batch_size = context.size(0)\n    context_lens = context.size(1)\n    context = context.transpose(1, 2).contiguous()\n    conv_in = context.view(batch_size * context_lens, 1, -1)\n    conv_in = self.dropout(conv_in)\n    conv_out = self.conv_layers(conv_in)\n    conv_out = conv_out.view(batch_size, context_lens, -1)\n    conv_out = conv_out.transpose(1, 2).contiguous()\n    conv_out = self.dropout(conv_out)\n    conv_out = self.pad_sequence(conv_out, lens)\n    return conv_out\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 17.130231098513978}, {"code": "def run_padded_sequence(self, context, lens)\n\t\n    padded_sequence = []\n    for i in range(len(context)):\n        context_i = context[i][:lens[i]]\n        context_i = self.conv_layers(context_i)\n        padded_sequence.append(context_i)\n    padded_sequence = torch.nn.utils.rnn.pad_sequence(padded_sequence, batch_first=True)\n    return padded_sequence\n\n\n\n\n\n", "CodeBleu_score": 27.772011246394896}, {"code": "def run_padded_sequence(self, context, lens)\n\t\n    return [self.run_padded_sequence_single(context[i][:lens[i]], lens[i]) for i in range(len(context))]\n\n", "CodeBleu_score": 7.337813248417879}]}
{"instance_id": "db2644f3c681", "generation_result": [{"code": "def run_unsorted_inputs(self, fn, context, lens)\n\t\n    # sort by length\n    # sort by length\n    lens, indices = torch.sort(lens, descending=True)\n    # indices = indices.to(self.device)\n    # lens = lens.to(self.device)\n    seq_tensor = pack_sequence(context)\n\n    # apply fn\n    # apply fn\n    packed_seq_tensor = fn(seq_tensor, lens)\n    seq_tensor = pad_packed_sequence(packed_seq_tensor)\n\n    # unsort\n    # unsort\n    _, unsorted_indices = torch.sort(indices)\n    # unsorted_indices = unsorted_indices.to(self.device)\n    seq_tensor = seq_tensor[:, unsorted_indices]\n\n    return seq_tensor.transpose(0, 1)", "CodeBleu_score": 10.812935693944786}, {"code": "def run_unsorted_inputs(self, fn, context, lens)\n\t\n    lens, ids = lens.sort(descending=True)\n    context = context[ids]\n    context = pack_sequence(context)\n    context = fn(context)\n    context, _ = pad_packed_sequence(context)\n    context = context[ids]\n    return context\n\n", "CodeBleu_score": 8.949903711100282}, {"code": "def run_unsorted_inputs(self, fn, context, lens)\n\t\n    lens, ids = lens.sort(descending=True)\n    context = context.index_select(0, ids)\n    unsorted = fn(context, lens)\n    _, inv_ids = ids.sort()\n    return unsorted.index_select(0, inv_ids)\n\n", "CodeBleu_score": 6.117032516080484}]}
{"instance_id": "ced082bffc63", "generation_result": [{"code": "def remove_weight_norm(self)\n\t\n    self.kernel_predictor.remove_weight_norm()\n    parametrize.remove_parametrizations(self.convt_pre[1], \"weight\")\n    for block in self.conv_blocks:\n        parametrize.remove_parametrizations(block[1], \"weight\")", "CodeBleu_score": 70.15353357610886}, {"code": "def remove_weight_norm(self)\n\t\n    self.kernel_predictor.remove_weight_norm()\n    parametrize.remove_parametrizations(self.convt_pre[1], \"weight\")\n    for block in self.conv_blocks:\n        parametrize.remove_parametrizations(block[1], \"weight\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 70.15353357610886}, {"code": "def remove_weight_norm(self)\n\t\n    self.kernel_predictor.remove_weight_norm()\n    parametrize.remove_parametrizations(self.convt_pre[1], \"weight\")\n    for block in self.conv_blocks:\n        parametrize.remove_parametrizations(block[1], \"weight\")\n\n\n", "CodeBleu_score": 70.15353357610886}]}
{"instance_id": "b5a39fd97061", "generation_result": [{"code": "def map_mx_to_torch_model(mx_name)\n\t\n    name = mx_name.lower()\n    name = name.replace('se_', 'se')\n    name = name.replace('senet_', 'senet')\n    name = name.replace('inceptionv3', 'inception_v3')\n    pt_name = 'gluon_' + name\n    return pt_name\n\n", "CodeBleu_score": 34.3619024072173}, {"code": "def map_mx_to_torch_model(mx_name)\n\t\n    name = mx_name.lower()\n    name = name.replace('se_', 'se')\n    name = name.replace('senet_', 'senet')\n    name = name.replace('inceptionv3', 'inception_v3')\n    name = 'gluon_' + name\n    return name\n\n\n\n\n", "CodeBleu_score": 33.97539232331951}, {"code": "def map_mx_to_torch_model(mx_name)\n\t\n    name = mx_name.lower()\n    name = name.replace('se_', 'se')\n    name = name.replace('senet_', 'senet')\n    name = name.replace('inceptionv3', 'inception_v3')\n    pt_name = 'gluon_' + name\n    return pt_name\n\n", "CodeBleu_score": 34.3619024072173}]}
{"instance_id": "7eee5efdbf67", "generation_result": [{"code": "def _num_samples_per_worker(self)\n\t\n    if self.repeats:\n        num_samples_per_worker = math.ceil(self.total_samples / self.num_workers)\n    else:\n        num_samples_per_worker = math.ceil(self.total_samples / self.num_replicas)\n    if self.train:\n        num_samples_per_worker = math.ceil(num_samples_per_worker / self.batch_size) * self.batch_size\n    return num_samples_per_worker", "CodeBleu_score": 39.91385959970849}, {"code": "def _num_samples_per_worker(self)\n\t\n    num_samples = self.repeats * self.total_samples\n    num_samples_per_worker = int(math.ceil(num_samples / self.num_workers))\n    if self.batch_size:\n        num_samples_per_worker = int(math.ceil(num_samples_per_worker / self.batch_size)) * self.batch_size\n    return num_samples_per_worker\n\n", "CodeBleu_score": 31.41990654752169}, {"code": "def _num_samples_per_worker(self)\n\t\n    num_samples_per_worker = int(math.ceil(self.repeats * self.total_samples / self.num_workers))\n    if self.training or self.num_workers > 1:\n        num_samples_per_worker = int(math.ceil(num_samples_per_worker / self.repeats))\n    if self.batch_size is not None:\n        num_samples_per_worker = int(math.ceil(num_samples_per_worker / self.batch_size)) * self.batch_size\n    return num_samples_per_worker\n\n\n\n", "CodeBleu_score": 39.19221273650606}]}
{"instance_id": "a698d65ae190", "generation_result": [{"code": "def find_images_and_targets(\n        folder: str,\n        types: Optional[Union[List, Tuple, Set]] = None,\n        class_to_idx: Optional[Dict] = None,\n        leaf_name_only: bool = True,\n        sort: bool = True\n)\n\t\n    if types is None:\n        types = IMG_EXTENSIONS\n    if class_to_idx is None:\n        # build class_to_idx based on the structure of the input folder\n        classes = [d.name for d in os.scandir(folder) if d.is_dir()]\n        classes.sort()\n        class_to_idx = {cls_name: i for i, cls_name in enumerate(classes)}\n    images_and_targets = get_images_and_targets(folder, class_to_idx, types, leaf_name_only, sort)\n    return images_and_targets, class_to_idx\n\n", "CodeBleu_score": 20.50492746334901}, {"code": "def find_images_and_targets(\n        folder: str,\n        types: Optional[Union[List, Tuple, Set]] = None,\n        class_to_idx: Optional[Dict] = None,\n        leaf_name_only: bool = True,\n        sort: bool = True\n)\n\t\n    if class_to_idx is None:\n        # find the class folders in a dataset\n        classes = [d.name for d in os.scandir(folder) if d.is_dir()]\n        classes.sort()\n        class_to_idx = {cls_name: i for i, cls_name in enumerate(classes)}\n    return make_dataset(folder, class_to_idx, extensions=types, is_valid_file=None, leaf_name_only=leaf_name_only, sort=sort)\n\n", "CodeBleu_score": 16.72030124873532}, {"code": "def find_images_and_targets(\n        folder: str,\n        types: Optional[Union[List, Tuple, Set]] = None,\n        class_to_idx: Optional[Dict] = None,\n        leaf_name_only: bool = True,\n        sort: bool = True\n)\n\t\n    if types is None:\n        types = IMG_EXTENSIONS\n    if class_to_idx is None:\n        class_to_idx = dict()\n    images_and_targets = list()\n    for root, _, fnames in sorted(os.walk(folder, followlinks=True)):\n        for fname in sorted(fnames):\n            if is_valid_file(fname, types):\n                path = os.path.join(root, fname)\n                # the item is a file\n                if not leaf_name_only:\n                    # use full path\n                    item = path\n                else:\n                    # use file name without extension\n                    item = fname.split(\".\")[0]\n                target = natural_key(os.path.split(root)[-1])[0]\n                if target not in class_to_idx:\n                    class_to_idx[target] = len(class_to_idx)\n                images_and_targets.append((item, class_to_idx[target]))\n\n    if sort:\n        images_and_targets = sorted(images_and_targets, key=lambda k: natural_key(k[0]))\n\n    return images_and_targets, class_to_idx\n\n", "CodeBleu_score": 40.90914535250643}]}
{"instance_id": "e7fe43bdecdc", "generation_result": [{"code": "def pytorch_worker_seed()\n\t\n    worker_info = get_worker_info()\n    if worker_info is not None:\n        # favour the seed already created for pytorch dataloader workers if it exists\n        return worker_info.seed\n    # fallback to wds rank based seed\n    return wds.utils.pytorch_worker_seed()\n\n", "CodeBleu_score": 66.80150511570375}, {"code": "def pytorch_worker_seed()\n\t\n    worker_info = get_worker_info()\n    if worker_info is not None:\n        return worker_info.seed\n    return wds.utils.pytorch_worker_seed()\n\n", "CodeBleu_score": 66.80150511570375}, {"code": "def pytorch_worker_seed()\n\t\n    worker_info = get_worker_info()\n    if worker_info is not None:\n        # favour the seed already created for pytorch dataloader workers if it exists\n        return worker_info.seed\n    # fallback to wds rank based seed\n    return wds.utils.pytorch_worker_seed()\n\n", "CodeBleu_score": 66.80150511570375}]}
{"instance_id": "0ffc7e7ded25", "generation_result": [{"code": "def solarize_add(img, add, thresh=128, **__)\n\t\n    if img.mode in [\"L\", \"RGB\"]:\n        lut = [0] * 256\n        for i in range(256):\n            lut[i] = min(i + add, 255) if i < thresh else i\n        return img.point(lut)\n    else:\n        return img\n\n", "CodeBleu_score": 34.35683914590195}, {"code": "def solarize_add(img, add, thresh=128, **__)\n\t\n    if img.mode == \"L\":\n        return img.point(lambda x: x + add if x < thresh else x)\n    elif img.mode == \"RGB\":\n        return ImageOps.solarize(img, thresh=thresh).point(lambda x: x + add)\n    return img\n\n", "CodeBleu_score": 25.694963758394497}, {"code": "def solarize_add(img, add, thresh=128, **__)\n\t\n    if img.mode in (\"L\", \"RGB\"):\n        lookup_table = [\n            (0 if i < thresh else table) for i in range(256)\n        ]\n        if img.mode == \"L\":\n            img = img.point(lookup_table)\n        else:\n            img = img.convert(\"RGB\")\n            r, g, b = img.split()\n            r = r.point(lookup_table)\n            g = g.point(lookup_table)\n            b = b.point(lookup_table)\n            img = Image.merge(\"RGB\", (r, g, b))\n        return img\n    else:\n        return img\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 30.118684090172152}]}
{"instance_id": "5a0d29577a21", "generation_result": [{"code": "def _get_weighted_transforms(transforms: Dict)\n\t\n    transform_names = list(transforms.keys())\n    transform_probs = list(transforms.values())\n    transform_probs = np.array(transform_probs) / sum(transform_probs)\n    return transform_names, transform_probs\n\n", "CodeBleu_score": 20.318371327924858}, {"code": "def _get_weighted_transforms(transforms: Dict)\n\t\n    transform_names, probs = zip(*transforms.items())\n    probs = np.array(probs)\n    probs = probs / probs.sum()\n    return transform_names, probs\n\n", "CodeBleu_score": 44.23941626472621}, {"code": "def _get_weighted_transforms(transforms: Dict)\n\t\n    transform_names, probabilities = zip(*transforms.items())\n    probabilities = np.array(probabilities)\n    probabilities = probabilities / probabilities.sum()\n    return transform_names, probabilities\n\n", "CodeBleu_score": 26.585001877184617}]}
{"instance_id": "84bc5658c135", "generation_result": [{"code": "def _apply_basic(self, img, mixing_weights, m)\n\t\n    mixed = np.empty_like(img)\n    for weight in mixing_weights:\n        mixed += weight * self._apply_augment_sequence(img.copy(), m)\n    mixed = np.clip(mixed, 0, 255).astype(np.uint8)\n    cv2.addWeighted(img, 1 - self.mixed.blend_factor, mixed, self.mixed.blend_factor, 0, mixed)\n    return mixed\n\n", "CodeBleu_score": 22.320354953253307}, {"code": "def _apply_basic(self, img, mixing_weights, m)\n\t\n    mixed = np.zeros_like(img)\n    for (weight, op) in zip(mixing_weights, m):\n        mixed += weight * op(img)\n    mixed = np.clip(mixed, 0, 255).astype(np.uint8)\n    return cv2.addWeighted(img, 1 - self.mixed.blend_factor, mixed, self.mixed.blend_factor, 0)", "CodeBleu_score": 19.978804399039202}, {"code": "def _apply_basic(self, img, mixing_weights, m)\n\t\n    mixed = np.zeros_like(img)\n    for weight in mixing_weights:\n        mixed += weight * self._apply_sequence(img, m)\n    mixed = self._clip(mixed)\n    mixed = Image.fromarray(np.uint8(mixed))\n    return Image.blend(img, mixed, self.blend_factor)\n\n", "CodeBleu_score": 18.262260695241434}]}
{"instance_id": "84b5b752ff04", "generation_result": [{"code": "def adapt_to_chs(x, n)\n\t\n    if not isinstance(x, (tuple, list)):\n        x = (x,) * n\n    elif len(x) != n:\n        x = (sum(x) / len(x),) * n\n        print(f\"Warning: The length of the input tuple x ({len(x)}) does not match the required number of channels n ({n}). The mean of x is used to repeat it {n} times.\")\n    assert len(x) == n, f\"Error: The length of the input tuple x ({len(x)}) does not match the required number of channels n ({n}).\"\n    return x\n\n\n\n", "CodeBleu_score": 41.20145619541261}, {"code": "def adapt_to_chs(x, n)\n\t\n    if not isinstance(x, (tuple, list)):\n        x = (x,) * n\n    elif len(x) != n:\n        x = tuple([sum(x) / len(x)] * n)\n        print(f'Warning: x is not a tuple or list of length {n}. It will be averaged and repeated {n} times.')\n    else:\n        assert len(x) == n\n    return x\n\n", "CodeBleu_score": 43.268792398553735}, {"code": "def adapt_to_chs(x, n)\n\t\n    if not isinstance(x, (tuple, list)):\n        x = (x,) * n\n    elif len(x) != n:\n        x = tuple([sum(x) / len(x)]) * n\n    assert len(x) == n, f\"Input x must have {n} elements.\"\n    return x\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 40.109330508771365}]}
{"instance_id": "54754b6ba394", "generation_result": [{"code": "def _worker_init(worker_id, worker_seeding='all')\n\t\n    worker_info = torch.utils.data.get_worker_info()\n    if worker_info is None:\n        return\n    if worker_id != worker_info.id:\n        return\n    if isinstance(worker_seeding, str):\n        if worker_seeding == 'all':\n            seeding = (worker_info.seed % (2 ** 32 - 1)) + worker_info.id\n            random.seed(seeding)\n            np.random.seed(seeding)\n            torch.manual_seed(seeding)\n            torch.cuda.manual_seed_all(seeding)\n        else:\n            np.random.seed(worker_seeding)\n    else:\n        worker_seeding(worker_info.seed)\n\n\n\n", "CodeBleu_score": 51.7230164871165}, {"code": "def _worker_init(worker_id, worker_seeding='all')\n\t\n    worker_info = torch.utils.data.get_worker_info()\n    if worker_info is None:\n        return\n    if isinstance(worker_seeding, str):\n        if worker_seeding == 'all':\n            seed = worker_info.seed % (2 ** 32 - 1)\n            np.random.seed(seed)\n            random.seed(seed)\n            torch.manual_seed(seed)\n        else:\n            raise ValueError(f\"Expected worker_seeding to be 'all' or a callable function, but got {worker_seeding}\")\n    elif callable(worker_seeding):\n        worker_seeding(worker_info.seed)\n    else:\n        raise ValueError(f\"Expected worker_seeding to be 'all' or a callable function, but got {worker_seeding}\")\n\n", "CodeBleu_score": 46.013459041437244}, {"code": "def _worker_init(worker_id, worker_seeding='all')\n\t\n    worker_info = torch.utils.data.get_worker_info()\n    if worker_info is None:\n        return\n    worker_id = worker_info.id\n    if isinstance(worker_seeding, str):\n        assert worker_seeding in ('all', 'random', 'torch', 'numpy')\n        worker_seeding = [worker_seeding]\n    for seeding in worker_seeding:\n        if seeding == 'all':\n            seed = worker_info.seed % (2 ** 32 - 1)\n            random.seed(seed)\n            torch.manual_seed(seed)\n            np.random.seed(seed)\n        elif seeding == 'random':\n            seed = worker_info.seed % (2 ** 32 - 1)\n            random.seed(seed)\n        elif seeding == 'torch':\n            seed = worker_info.seed % (2 ** 32 - 1)\n            torch.manual_seed(seed)\n        elif seeding == 'numpy':\n            seed = worker_info.seed % (2 ** 32 - 1)\n            np.random.seed(seed)\n        else:\n            raise Exception('Unknown seeding option')", "CodeBleu_score": 40.17686842521688}]}
{"instance_id": "81dfdce940db", "generation_result": [{"code": "def _decode_and_center_crop(image_bytes, image_size, resize_method)\n\t\n    shape = tf.image.extract_jpeg_shape(image_bytes)\n    image_height = shape[0]\n    image_width = shape[1]\n    padded_center_crop_size = tf.cast(\n        ((image_size / (image_size + CROP_PADDING)) * tf.cast(tf.maximum(image_height, image_width), tf.float32)), tf.int32)\n    offset_height = ((image_height - padded_center_crop_size) + 1) // 2\n    offset_width = ((image_width - padded_center_crop_size) + 1) // 2\n    crop_window = tf.stack([offset_height, offset_width, padded_center_crop_size, padded_center_crop_size])\n    image = tf.image.decode_and_crop_jpeg(image_bytes, crop_window, channels=3)\n    image = tf.image.resize(image, [image_size, image_size], method=resize_method)\n    return image\n\n", "CodeBleu_score": 59.10215344895222}, {"code": "def _decode_and_center_crop(image_bytes, image_size, resize_method)\n\t\n    shape = tf.image.extract_jpeg_shape(image_bytes)\n    image_height = shape[0]\n    image_width = shape[1]\n    padded_center_crop_size = tf.cast(\n        ((image_size / (image_size + CROP_PADDING)) * tf.cast(tf.maximum(image_height, image_width), tf.float32)),\n        tf.int32,\n    )\n    offset_height = ((image_height - padded_center_crop_size) + 1) // 2\n    offset_width = ((image_width - padded_center_crop_size) + 1) // 2\n    crop_window = tf.stack(\n        [offset_height, offset_width, padded_center_crop_size, padded_center_crop_size]\n    )\n    image = tf.image.decode_and_crop_jpeg(image_bytes, crop_window, channels=3)\n    image = _resize_image(image, image_size, resize_method)\n    return image\n\n", "CodeBleu_score": 55.429265966823806}, {"code": "def _decode_and_center_crop(image_bytes, image_size, resize_method)\n\t\n    shape = tf.image.extract_jpeg_shape(image_bytes)\n    image_height = shape[0]\n    image_width = shape[1]\n    padded_center_crop_size = tf.cast(\n        ((image_size / (image_size + CROP_PADDING)) * tf.cast(tf.maximum(image_height, image_width), tf.float32)), tf.int32\n    )\n    offset_height = ((image_height - padded_center_crop_size) + 1) // 2\n    offset_width = ((image_width - padded_center_crop_size) + 1) // 2\n    crop_window = tf.stack([offset_height, offset_width, padded_center_crop_size, padded_center_crop_size])\n    image = tf.image.decode_and_crop_jpeg(image_bytes, crop_window, channels=3)\n    image = resize_method(image, [image_size, image_size])\n    return image\n\n", "CodeBleu_score": 57.178806710923304}]}
{"instance_id": "cf16a6d37111", "generation_result": [{"code": "def preprocess_for_train(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    if interpolation == 'bicubic':\n        interpolation = tf.image.ResizeMethod.BICUBIC\n    elif interpolation == 'area':\n        interpolation = tf.image.ResizeMethod.AREA\n    elif interpolation == 'nearest':\n        interpolation = tf.image.ResizeMethod.NEAREST_NEIGHBOR\n    else:\n        raise ValueError('Invalid interpolation method: {}'.format(interpolation))\n\n    image = _decode_and_random_crop(image_bytes, image_size, interpolation)\n    image = _flip(image)\n    image = tf.reshape(image, [image_size, image_size, 3])\n    image = tf.clip_by_value(image, 0.0, 255.0)\n    if use_bfloat16:\n        image = tf.cast(image, tf.bfloat16)\n    else:\n        image = tf.cast(image, tf.float32)\n    return image\n\n", "CodeBleu_score": 59.88645670238467}, {"code": "def preprocess_for_train(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    if interpolation == 'bicubic':\n        interpolation = tf.image.ResizeMethod.BICUBIC\n    elif interpolation == 'area':\n        interpolation = tf.image.ResizeMethod.AREA\n    elif interpolation == 'bilinear':\n        interpolation = tf.image.ResizeMethod.BILINEAR\n    else:\n        raise ValueError('Unknown interpolation method: {}'.format(interpolation))\n\n    image = _decode_and_random_crop(image_bytes, image_size, interpolation)\n    image = _flip(image)\n\n    image = tf.reshape(image, [image_size, image_size, 3])\n    image = tf.clip_by_value(image, 0.0, 255.0)\n    image = tf.cast(image, dtype=tf.bfloat16 if use_bfloat16 else tf.float32)\n    return image\n\n", "CodeBleu_score": 66.70765718272509}, {"code": "def preprocess_for_train(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    image = _decode_and_random_crop(image_bytes, image_size, interpolation)\n    image = _flip(image)\n\n    image = tf.reshape(image, [image_size, image_size, 3])\n    image = tf.image.convert_image_dtype(\n        image, dtype=tf.bfloat16 if use_bfloat16 else tf.float32)\n\n    return image\n\n", "CodeBleu_score": 46.37171642737686}]}
{"instance_id": "78b2a3bd2142", "generation_result": [{"code": "def preprocess_for_eval(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    resize_method = get_resize_method(interpolation)\n    image = _decode_and_center_crop(image_bytes, image_size, resize_method)\n    image = tf.reshape(image, [image_size, image_size, 3])\n    image = tf.image.convert_image_dtype(image, dtype=tf.bfloat16 if use_bfloat16 else tf.float32)\n    return image\n\n", "CodeBleu_score": 48.26234331526798}, {"code": "def preprocess_for_eval(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    resize_method = get_resize_method(interpolation)\n    image = _decode_and_center_crop(image_bytes, image_size, resize_method)\n    image = tf.reshape(image, [image_size, image_size, 3])\n    image = tf.image.convert_image_dtype(image, dtype=tf.bfloat16 if use_bfloat16 else tf.float32)\n    return image\n\n", "CodeBleu_score": 48.26234331526798}, {"code": "def preprocess_for_eval(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    resize_method = get_resize_method(interpolation)\n    image = _decode_and_center_crop(image_bytes, image_size, resize_method)\n    image = tf.reshape(image, [image_size, image_size, 3])\n    image = tf.image.convert_image_dtype(image, dtype=tf.bfloat16 if use_bfloat16 else tf.float32)\n    return image\n\n", "CodeBleu_score": 48.26234331526798}]}
{"instance_id": "8bd4364a9a0e", "generation_result": [{"code": "def evaluate(self, model, eval_dataloader=None, output_file=\"merged\")\n\t\n    if eval_dataloader is None:\n        eval_dataloader = self.eval_dataloader\n    self.model = model\n    self.model.eval()\n    self.model.to(self.device)\n    all_preds = []\n    all_labels = []\n    all_ids = []\n    with torch.no_grad():\n        for batch in tqdm(eval_dataloader):\n            batch = {k: v.to(self.device) for k, v in batch.items()}\n            outputs = self.model(**batch)\n            logits = outputs.logits\n            preds = torch.argmax(logits, dim=-1)\n            labels = batch[\"labels\"]\n            ids = batch[\"ids\"]\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n            all_ids.extend(ids.cpu().numpy())\n    if self.args.local_rank in [-1, 0]:\n        with open(os.path.join(self.args.output_dir, \"predictions.txt\"), \"w\") as writer:\n            for ids, preds, labels in zip(all_ids, all_preds, all_labels):\n                writer.write(str(ids) + \"\\t\" + str(preds) + \"\\t\" + str(labels) + \"", "CodeBleu_score": 43.70668528594442}, {"code": "def evaluate(self, model, eval_dataloader=None, output_file=\"merged\")\n\t\n    if not eval_dataloader:\n        eval_dataloader = self.eval_dataloader\n    self.model = model\n    self.model.eval()\n    self.model.to(self.device)\n    all_labels = []\n    all_logits = []\n    for batch in tqdm(eval_dataloader):\n        with torch.no_grad():\n            labels = batch.pop(\"labels\")\n            input_ids = batch[\"input_ids\"].to(self.device)\n            attention_mask = batch[\"attention_mask\"].to(self.device)\n            outputs = self.model(input_ids, attention_mask=attention_mask)\n            all_logits.append(outputs.logits)\n            all_labels.append(labels)\n    all_logits = torch.cat(all_logits, dim=0).detach().cpu().numpy()\n    all_labels = torch.cat(all_labels, dim=0).detach().cpu().numpy()\n    if self.config[\"problem_type\"] == \"multi_label_classification\":\n        all_predictions = np.where(all_logits > 0, 1, 0)\n    else:\n        all_predictions = np.argmax(all_logits, axis=1)\n    metric = load_metric(self.config[\"metric\"])\n    metric_output = metric.compute(\n        predictions=all_predictions, references=all_labels)\n    return metric_output\n\n\n\n\n\n\n", "CodeBleu_score": 33.15016933767946}, {"code": "def evaluate(self, model, eval_dataloader=None, output_file=\"merged\")\n\t\n    if eval_dataloader is None:\n        eval_dataloader = self.eval_dataloader\n    if self.training_args.output_dir is not None:\n        if not os.path.exists(self.training_args.output_dir):\n            os.makedirs(self.training_args.output_dir)\n        output_file = os.path.join(self.training_args.output_dir, output_file)\n    model.eval()\n    model.to(self.device)\n    with torch.no_grad():\n        for batch in tqdm(eval_dataloader):\n            batch = {k: v.to(self.device) for k, v in batch.items()}\n            outputs = model(**batch)\n            start_logits = outputs.start_logits\n            end_logits = outputs.end_logits\n            start_logits = start_logits.detach().cpu().numpy()\n            end_logits = end_logits.detach().cpu().numpy()\n            for i in range(len(start_logits)):\n                start_logits[i] = np.exp(start_logits[i]) / np.sum(np.exp(start_logits[i]))\n                end_logits[i] = np.exp(end_logits[i]) / np.sum(np.exp(end_logits[i]))\n            start_pred = np.argmax(start_logits, axis=1)\n            end_pred = np.argmax(end_logits, axis=1)\n            for i in range(len(start_pred)):\n                if start_pred[i] > end_pred[i]:\n                    start_pred[i] = end_pred[i]\n            for i in range(len(start_pred)):\n                if start_pred[i] == end_pred[i]:\n                    start_pred[i] = end_pred[i] - 1\n            start_pred = start_pred.tolist()\n            end_pred = end_pred.tolist()\n            for i in range(", "CodeBleu_score": 44.34513744288411}]}
{"instance_id": "22d682c958c0", "generation_result": [{"code": " def print_computed_metrics(self, metrics)\n\t\n    rank1 = metrics[\"rank1\"]\n    rank5 = metrics[\"rank5\"]\n    miou = metrics[\"miou\"]\n    print(\"Rank@1: {:.4f}\".format(rank1))\n    print(\"Rank@5: {:.4f}\".format(rank5))\n    print(\"mIoU: {:.4f}\".format(miou))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 37.29336163436595}, {"code": " def print_computed_metrics(self, metrics)\n\t\n    print('Rank@1: {:.4f}'.format(metrics['rank1']))\n    print('Rank@5: {:.4f}'.format(metrics['rank5']))\n    print('mIoU: {:.4f}'.format(metrics['miou']))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 24.162951722557334}, {"code": " def print_computed_metrics(self, metrics)\n\t\n    print(\"Rank@1: {:.4f}\".format(metrics[\"rank1\"]))\n    print(\"Rank@5: {:.4f}\".format(metrics[\"rank5\"]))\n    print(\"mIoU: {:.4f}\".format(metrics[\"miou\"]))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 25.884161475728828}]}
{"instance_id": "39c1a50b7e7d", "generation_result": [{"code": "def _eval_predictions(self, segments, data)\n\t\n\n    # Calculate IoUs and ranks for each segment-data pair\n    ious = []\n    ranks = []\n    for seg in segments:\n        for d in data:\n            iou = self._iou(seg, d)\n            rank = self._rank(segments, d)\n            ious.append(iou)\n            ranks.append(rank)\n\n    # Calculate average IoU and rank for top predictions\n    avg_iou = sum(ious) / len(ious)\n    avg_rank = sum(ranks) / len(ranks)\n\n    # Calculate percentage of segments with rank 1 and rank 5\n    rank1 = sum([1 for rank in ranks if rank == 1]) / len(ranks)\n    rank5 = sum([1 for rank in ranks if rank <= 5]) / len(ranks)\n\n    # Calculate mean IoU across all segments\n    mean_iou = sum(ious) / len(ious)\n\n    return rank1, rank5, mean_iou", "CodeBleu_score": 29.33940585306719}, {"code": "def _eval_predictions(self, segments, data)\n\t\n    ious = []\n    ranks = []\n    for segment in segments:\n        iou = self._iou(segment, data)\n        ious.append(iou)\n        rank = self._rank(segments, segment)\n        ranks.append(rank)\n    ious = sorted(ious, reverse=True)\n    ranks = sorted(ranks)\n    rank1 = 100.0 * len([x for x in ranks if x == 1]) / len(ranks)\n    rank5 = 100.0 * len([x for x in ranks if x == 5]) / len(ranks)\n    mean_iou = 100.0 * sum(ious) / len(ious)\n    return rank1, rank5, mean_iou", "CodeBleu_score": 28.437330021533292}, {"code": "def _eval_predictions(self, segments, data)\n\t\n    # Calculate IoUs and ranks for each segment-data pair\n    ious = []\n    ranks = []\n    for segment in segments:\n        for datum in data:\n            iou = self._iou(segment, datum)\n            rank = self._rank(segments, datum)\n            ious.append(iou)\n            ranks.append(rank)\n\n    # Average the top values for each segment and data\n    ious = np.array(ious).reshape(-1, len(data))\n    ranks = np.array(ranks).reshape(-1, len(data))\n    ious = np.mean(np.sort(ious, axis=1)[:, :5], axis=1)\n    ranks = np.mean(np.sort(ranks, axis=1)[:, :5], axis=1)\n\n    # Calculate the percentage of segments with rank 1 and rank 5\n    rank1 = np.mean(ranks == 1)\n    rank5 = np.mean(ranks <= 5)\n\n    # Calculate the mean IoU across all segments\n    mean_iou = np.mean(ious)\n\n    return rank1, rank5, mean_iou", "CodeBleu_score": 38.29983699180587}]}
{"instance_id": "8ccf84f8d034", "generation_result": [{"code": "def predict_loop(self, model, eval_dataloader, output_file=None)\n\t\n    scores = []\n    model.eval()\n    model.to(0)\n    for data in tqdm(eval_dataloader):\n        data = self.to_ctx(data, ctx=0)\n        with torch.no_grad():\n            outputs = model(**data)\n            scores += outputs\n    results = self.process_outputs(scores)\n    results = self.to_ctx(results, ctx=0)\n    results = self.postprocess(results)\n    if output_file is not None:\n        self.save_results(results, output_file)\n    self.finalize(output_file)\n    return results", "CodeBleu_score": 45.08294477890346}, {"code": "def predict_loop(self, model, eval_dataloader, output_file=None)\n\t\n    scores = []\n    model.eval()\n    model.to(0)\n    for batch in eval_dataloader:\n        batch = self.to_ctx(batch, ctx=0)\n        with torch.no_grad():\n            outputs = model(**batch)\n        scores.extend(outputs.cpu().detach().numpy())\n    results = self.process_outputs(scores)\n    if output_file:\n        self.save_results(results, output_file)\n    return results\n\n", "CodeBleu_score": 46.034824640086114}, {"code": "def predict_loop(self, model, eval_dataloader, output_file=None)\n\t\n    scores = []\n    model.eval()\n    model.to(0)\n    for batch in tqdm(eval_dataloader):\n        batch = self.to_ctx(batch, ctx=0)\n        with torch.no_grad():\n            outputs = model(**batch)\n        outputs = self.to_ctx(outputs, ctx=-1)\n        scores.append(outputs)\n    results = self.process_scores(scores)\n    self.finalize(output_file)\n    return results\n\n", "CodeBleu_score": 46.07853563298939}]}
{"instance_id": "e68529ad727d", "generation_result": [{"code": "def _aggregate_scores(self, scores)\n\t\n    assert len(scores) == 2\n    text_scores, video_scores = scores\n    full_scores = torch.cat((text_scores, video_scores), dim=1)\n    self.full_scores = None\n    return torch.matmul(text_scores, video_scores.t())\n\n", "CodeBleu_score": 35.4101707445168}, {"code": "def _aggregate_scores(self, scores)\n\t\n    assert len(scores) == 2\n    self.full_scores = None\n    return scores[0].dot(scores[1].T)", "CodeBleu_score": 19.176475507865657}, {"code": "def _aggregate_scores(self, scores)\n\t\n    assert len(scores) == 2\n    text_scores = scores[0]\n    video_scores = scores[1]\n    full_scores = torch.cat([text_scores, video_scores], dim=0)\n    self.full_scores = None\n    return torch.matmul(text_scores, video_scores.t())\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 39.85492310041501}]}
{"instance_id": "e4e1861fc357", "generation_result": [{"code": "def finalize(self, Y_pred, Y_true, output_file=None)\n\t\n    # Concatenate predictions and true labels\n    Y_pred = np.concatenate(Y_pred, axis=0)\n    Y_true = np.concatenate(Y_true, axis=0)\n    # Convert to numpy arrays\n    Y_pred = np.array(Y_pred)\n    Y_true = np.array(Y_true)\n    # Check for prediction errors\n    errors = Y_pred != Y_true\n    if np.any(errors):\n        print(\"Prediction errors:\")\n        for i in range(len(Y_pred)):\n            if errors[i]:\n                print(f\"Sample {i}: predicted {Y_pred[i]}, true {Y_true[i]}\")\n    # Save predictions and true labels to pickle file\n    if output_file is not None:\n        with open(output_file, \"wb\") as f:\n            pickle.dump({\"Y_pred\": Y_pred, \"Y_true\": Y_true}, f)\n    # Return predictions and true labels\n    return {\"Y_pred\": Y_pred, \"Y_true\": Y_true}\n\n\n\n", "CodeBleu_score": 42.24579904604339}, {"code": "def finalize(self, Y_pred, Y_true, output_file=None)\n\t\n    Y_pred = np.concatenate(Y_pred, axis=0)\n    Y_true = np.concatenate(Y_true, axis=0)\n    if Y_pred.shape != Y_true.shape:\n        raise ValueError(\"Predictions and true labels have different shapes\")\n    if np.sum(Y_pred == Y_true) != Y_pred.shape[0]:\n        print(\"Prediction errors:\")\n        for i in range(Y_pred.shape[0]):\n            if Y_pred[i] != Y_true[i]:\n                print(f\"Sample {i}: Predicted {Y_pred[i]}, True {Y_true[i]}\")\n    if output_file is not None:\n        with open(output_file, \"wb\") as f:\n            pickle.dump({\"Y_pred\": Y_pred, \"Y_true\": Y_true}, f)\n    return {\"Y_pred\": Y_pred, \"Y_true\": Y_true}", "CodeBleu_score": 38.64498836532596}, {"code": "def finalize(self, Y_pred, Y_true, output_file=None)\n\t\n    Y_pred = np.concatenate(Y_pred)\n    Y_true = np.concatenate(Y_true)\n    assert Y_pred.shape == Y_true.shape\n    error_indices = np.where(Y_pred != Y_true)[0]\n    print(f\"Prediction errors: {len(error_indices)}/{len(Y_pred)}\")\n    if len(error_indices) > 0:\n        print(f\"Samples of prediction errors:\")\n        for i in error_indices[:10]:\n            print(f\"Predicted: {Y_pred[i]}, True: {Y_true[i]}\")\n    result = {\"Y_pred\": Y_pred, \"Y_true\": Y_true}\n    if output_file is not None:\n        with open(output_file, \"wb\") as f:\n            pickle.dump(result, f)\n    return result\n\n", "CodeBleu_score": 37.53999279509023}]}
{"instance_id": "7df9ab9a4920", "generation_result": [{"code": "def reduce_metrics(logging_outputs) -> None\n\t\n    loss_sum = sum(log.get(\"loss\", 0) for log in logging_outputs)\n    sample_size = sum(log.get(\"sample_size\", 0) for log in logging_outputs)\n\n    metrics.log_scalar(\"loss\", loss_sum / sample_size, sample_size, round=3)", "CodeBleu_score": 62.429646714521894}, {"code": "def reduce_metrics(logging_outputs) -> None\n\t\n    loss_sum = sum(log.get(\"loss\", 0) for log in logging_outputs)\n    sample_size = sum(log.get(\"sample_size\", 0) for log in logging_outputs)\n\n    metrics.log_scalar(\"loss\", loss_sum / sample_size, sample_size, round=3)\n\n", "CodeBleu_score": 62.429646714521894}, {"code": "def reduce_metrics(logging_outputs) -> None\n\t\n    loss_sum = sum(log.get(\"loss\", 0) for log in logging_outputs)\n    sample_size = sum(log.get(\"sample_size\", 0) for log in logging_outputs)\n\n    metrics.log_scalar(\"loss\", loss_sum / sample_size, sample_size, round=3)\n\n", "CodeBleu_score": 62.429646714521894}]}
{"instance_id": "7f9c326a96f7", "generation_result": [{"code": "def _mm_on_the_fly(\n        self,\n        cmasks,\n        vmasks,\n        attention_mask\n    )\n\t\n    if attention_mask is None:\n        attention_mask = self._mm_attention_mask(cmasks, vmasks)\n    else:\n        # make sure the attention mask is the same as the expected shape\n        assert attention_mask.size() == (\n            cmasks.size(0),\n            self.num_hidden_layers + 1,\n            cmasks.size(1) + vmasks.size(1),\n            cmasks.size(1) + vmasks.size(1),\n        ), \"{}, {}, {}, {}\".format(\n            str(attention_mask.size()),\n            str(cmasks.size(0)),\n            str(self.num_hidden_layers + 1),\n            str(cmasks.size(1) + vmasks.size(1)),\n        )\n\n    token_type_ids = torch.cat(\n        [torch.zeros_like(cmasks), torch.ones_like(vmasks)], dim=-1\n    )\n    return attention_mask, token_type_ids\n\n", "CodeBleu_score": 47.79662357356657}, {"code": "def _mm_on_the_fly(\n        self,\n        cmasks,\n        vmasks,\n        attention_mask\n    )\n\t\n    if attention_mask is None:\n        attention_mask = self._mm_attention_mask(cmasks, vmasks)\n    token_type_ids = torch.cat([torch.zeros_like(cmasks), torch.ones_like(vmasks)], dim=-1)\n    return attention_mask, token_type_ids\n\n", "CodeBleu_score": 33.04421730860111}, {"code": "def _mm_on_the_fly(\n        self,\n        cmasks,\n        vmasks,\n        attention_mask\n    )\n\t\n    # create attention mask if not provided\n    if attention_mask is None:\n        attention_mask = torch.cat([cmasks[:, :1], vmasks, cmasks[:, 1:]], dim=1)\n\n    # create token type IDs\n    token_type_ids = torch.cat([torch.zeros_like(cmasks), torch.ones_like(vmasks)], dim=1)\n\n    return attention_mask, token_type_ids\n", "CodeBleu_score": 28.998768836460727}]}
{"instance_id": "fa99d2a148a5", "generation_result": [{"code": "def prepare_inputs_for_generation(\n        self,\n        input_ids,\n        input_video_embeds,\n        attention_mask=None,\n        token_type_ids=None,\n        **model_kwargs\n    )\n\t\n    # Adjust sequence length to match the combined length of input_ids and input_video_embeds\n    combined_length = input_ids.size(1) + input_video_embeds.size(1)\n    model_kwargs[\"sequence_length\"] = combined_length\n\n    # Modify attention_mask and token_type_ids to fit the new sequence length\n    if attention_mask is not None:\n        attention_mask = attention_mask.narrow(1, 0, combined_length)\n    if token_type_ids is not None:\n        token_type_ids = token_type_ids.narrow(1, 0, combined_length)\n\n    return {\n        \"input_ids\": input_ids,\n        \"input_video_embeds\": input_video_embeds,\n        \"attention_mask\": attention_mask,\n        \"token_type_ids\": token_type_ids,\n        **model_kwargs\n    }\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 45.3323979910897}, {"code": "def prepare_inputs_for_generation(\n        self,\n        input_ids,\n        input_video_embeds,\n        attention_mask=None,\n        token_type_ids=None,\n        **model_kwargs\n    )\n\t\n    # adjust the sequence length to match the combined length of input_ids and input_video_embeds\n    combined_seq_length = input_ids.shape[1] + input_video_embeds.shape[1]\n    model_inputs = self.prepare_inputs_for_generation(\n        input_ids,\n        attention_mask=attention_mask,\n        token_type_ids=token_type_ids,\n        **model_kwargs,\n    )\n    # modify the attention_mask and token_type_ids to fit the new sequence length\n    if \"attention_mask\" in model_inputs:\n        attention_mask = model_inputs[\"attention_mask\"]\n        attention_mask = torch.cat(\n            [\n                attention_mask,\n                torch.zeros(\n                    (attention_mask.shape[0], combined_seq_length - attention_mask.shape[1]),\n                    dtype=attention_mask.dtype,\n                    device=attention_mask.device,\n                ),\n            ],\n            dim=1,\n        )\n        model_inputs[\"attention_mask\"] = attention_mask\n    if \"token_type_ids\" in model_inputs:\n        token_type_ids = model_inputs[\"token_type_ids\"]\n        token_type_ids = torch.cat(\n            [\n                token_type_ids,\n                torch.zeros(\n                    (token_type_ids.shape[0], combined_seq_length - token_type_ids.shape[1]),\n                    dtype=token_type_ids.dtype,\n                    device=token_type_ids.device,\n                ),\n            ],\n            dim=1,\n        )\n        model_inputs[\"token_type_ids\"] = token_type_ids\n    return model_inputs\n\n\n\n", "CodeBleu_score": 26.13844936760763}, {"code": "def prepare_inputs_for_generation(\n        self,\n        input_ids,\n        input_video_embeds,\n        attention_mask=None,\n        token_type_ids=None,\n        **model_kwargs\n    )\n\t\n\n    # Adjust the sequence length to match the combined length of input_ids and input_video_embeds\n    max_length = self.config.max_length\n    if max_length is not None and max_length > 0:\n        total_length = input_ids.shape[1] + input_video_embeds.shape[1]\n        if total_length > max_length:\n            input_ids = input_ids[:, :max_length-input_video_embeds.shape[1]]\n            attention_mask = attention_mask[:, :max_length-input_video_embeds.shape[1]]\n            if token_type_ids is not None:\n                token_type_ids = token_type_ids[:, :max_length-input_video_embeds.shape[1]]\n\n    # Concatenate input_ids and input_video_embeds\n    input_ids = torch.cat([input_ids, input_video_embeds], dim=1)\n\n    # Modify the attention_mask and token_type_ids to fit the new sequence length\n    if attention_mask is not None:\n        attention_mask = torch.cat([attention_mask, torch.ones_like(input_video_embeds)], dim=1)\n    if token_type_ids is not None:\n        token_type_ids = torch.cat([token_type_ids, torch.zeros_like(input_video_embeds)], dim=1)\n\n    # Return the updated input_ids, input_video_embeds, attention_mask, and token_type_ids\n    return {\n        \"input_ids\": input_ids,\n        \"attention_mask\": attention_mask,\n        \"token_type_ids\": token_type_ids,\n        **model_kwargs\n    }\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 42.23476325040002}]}
{"instance_id": "b1b0d25fe620", "generation_result": [{"code": "def _init_pipeline(self, cfg: ConfigType) -> Compose\n\t\n    # Adjust the pipeline config by removing the 'img_id' key from meta_keys\n    for transform in cfg:\n        if 'meta_keys' in transform and 'img_id' in transform['meta_keys']:\n            transform['meta_keys'].remove('img_id')\n\n    # Find the index of the LoadImageFromFile transform\n    load_image_idx = self._get_transform_idx(cfg, 'LoadImageFromFile')\n    if load_image_idx == -1:\n        raise ValueError('LoadImageFromFile transform not found in pipeline')\n\n    # Update the transform type to mmdet.InferencerLoader\n    cfg[load_image_idx]['type'] = 'mmdet.InferencerLoader'\n\n    # Wrap the pipeline config in a Compose object\n    pipeline = Compose(cfg)\n    return pipeline\n\n", "CodeBleu_score": 36.09141018564028}, {"code": "def _init_pipeline(self, cfg: ConfigType) -> Compose\n\t\n    # Adjust the pipeline config to remove the 'img_id' key from meta_keys\n    for transform in cfg:\n        if 'meta_keys' in transform:\n            if 'img_id' in transform['meta_keys']:\n                transform['meta_keys'].remove('img_id')\n    # Find the index of the 'LoadImageFromFile' transform\n    idx = self._get_transform_idx(cfg, LoadImageFromFile)\n    if idx == -1:\n        raise ValueError('Cannot find `LoadImageFromFile` in the pipeline')\n    # Update the transform type to 'mmdet.InferencerLoader'\n    cfg[idx]['type'] = 'mmdet.InferencerLoader'\n    return Compose(cfg)\n\n", "CodeBleu_score": 33.71648098475633}, {"code": "def _init_pipeline(self, cfg: ConfigType) -> Compose\n\t\n    # Adjust pipeline config\n    cfg = copy.deepcopy(cfg)\n    for transform in cfg:\n        if 'meta_keys' in transform:\n            if 'img_id' in transform['meta_keys']:\n                transform['meta_keys'].remove('img_id')\n    # Find the index of LoadImageFromFile\n    idx = self._get_transform_idx(cfg, 'LoadImageFromFile')\n    if idx == -1:\n        raise ValueError('LoadImageFromFile is not found in the pipeline.')\n    # Update the type of transform\n    cfg[idx]['type'] = 'mmdet.InferencerLoader'\n    return Compose(cfg)\n\n", "CodeBleu_score": 38.382315544414844}]}
{"instance_id": "7967962856eb", "generation_result": [{"code": "def _inputs_to_list(self, inputs: InputsType) -> list\n\t\n    if isinstance(inputs, str):\n        if os.path.isdir(inputs):\n            inputs = [os.path.join(inputs, f) for f in os.listdir(inputs)]\n        else:\n            inputs = [inputs]\n    elif not isinstance(inputs, (list, tuple)):\n        inputs = [inputs]\n    return inputs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 40.28657590586616}, {"code": "def _inputs_to_list(self, inputs: InputsType) -> list\n\t\n    if isinstance(inputs, str):\n        if os.path.isdir(inputs):\n            inputs = [os.path.join(inputs, f) for f in os.listdir(inputs) if f.endswith((\".png\", \".jpg\", \".jpeg\", \".bmp\", \".gif\"))]\n        else:\n            inputs = [inputs]\n    elif not isinstance(inputs, (list, tuple)):\n        inputs = [inputs]\n    return inputs\n\n", "CodeBleu_score": 44.238418972850305}, {"code": "def _inputs_to_list(self, inputs: InputsType) -> list\n\t\n    if isinstance(inputs, str):\n        if os.path.isdir(inputs):\n            inputs = [os.path.join(inputs, f) for f in os.listdir(inputs) if f.endswith((\".png\", \".jpg\", \".jpeg\", \".bmp\", \".gif\"))]\n        else:\n            inputs = [inputs]\n    elif not isinstance(inputs, (list, tuple)):\n        inputs = [inputs]\n    return inputs\n\n", "CodeBleu_score": 44.238418972850305}]}
{"instance_id": "74f727f06fa0", "generation_result": [{"code": "def _get_chunk_data(self, inputs: Iterable, chunk_size: int)\n\t\n    chunk_data = []\n    for input in inputs:\n        if isinstance(input, dict):\n            if 'img' in input:\n                data = self.pipeline(input['img'])\n            elif 'img_path' in input:\n                data = self.pipeline(input['img_path'])\n            else:\n                raise ValueError('Input dictionary must contain either \"img\" or \"img_path\" key.')\n        else:\n            data = self.pipeline(input)\n        chunk_data.append(data)\n        if len(chunk_data) == chunk_size:\n            yield chunk_data\n            chunk_data = []\n    if chunk_data:\n        yield chunk_data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 39.731714447409736}, {"code": "def _get_chunk_data(self, inputs: Iterable, chunk_size: int)\n\t\n    chunk_data = []\n    for input_data in inputs:\n        if isinstance(input_data, dict):\n            if \"img\" in input_data:\n                input_data = self.pipeline(input_data[\"img\"])\n            elif \"img_path\" in input_data:\n                input_data = self.pipeline(input_data[\"img_path\"])\n        chunk_data.append(input_data)\n        if len(chunk_data) == chunk_size:\n            yield chunk_data\n            chunk_data = []\n    if chunk_data:\n        yield chunk_data\n\n", "CodeBleu_score": 34.4384022349812}, {"code": "def _get_chunk_data(self, inputs: Iterable, chunk_size: int)\n\t\n    chunk = []\n    for input_ in inputs:\n        if isinstance(input_, dict) and \"img\" in input_:\n            input_ = self.pipeline(input_)\n        elif isinstance(input_, dict) and \"img_path\" in input_:\n            input_ = self.pipeline(input_)\n        chunk.append(input_)\n        if len(chunk) == chunk_size:\n            yield chunk\n            chunk = []\n    if chunk:\n        yield chunk\n\n\n", "CodeBleu_score": 26.57020850363136}]}
{"instance_id": "a43ab51afa59", "generation_result": [{"code": "def postprocess(\n        self,\n        preds: PredType,\n        visualization: Optional[List[np.ndarray]] = None,\n        return_datasamples: bool = False,\n        print_result: bool = False,\n        no_save_pred: bool = False,\n        pred_out_dir: str = '',\n        **kwargs,\n    ) -> Dict\n\t\n    # TODO: Support saving datasamples\n    if pred_out_dir != '' and return_datasamples:\n        warnings.warn('Saving datasamples is not supported, please set '\n                      '`return_datasamples=False`')\n\n    if return_datasamples:\n        return preds\n\n    if no_save_pred:\n        pred_out_dir = ''\n\n    result = []\n    for pred_data_sample in preds:\n        result.append(self.pred2dict(pred_data_sample, pred_out_dir))\n\n    if visualization is not None:\n        result.extend(visualization)\n\n    if print_result:\n        print_log(result, logger='visualize')\n\n    return result", "CodeBleu_score": 43.88146031750853}, {"code": "def postprocess(\n        self,\n        preds: PredType,\n        visualization: Optional[List[np.ndarray]] = None,\n        return_datasamples: bool = False,\n        print_result: bool = False,\n        no_save_pred: bool = False,\n        pred_out_dir: str = '',\n        **kwargs,\n    ) -> Dict\n\t\n    if no_save_pred:\n        pred_out_dir = ''\n\n    if pred_out_dir != '':\n        mmengine.mkdir_or_exist(pred_out_dir)\n        mmengine.mkdir_or_exist(osp.join(pred_out_dir, 'preds'))\n\n    if return_datasamples:\n        if pred_out_dir != '':\n            warnings.warn(\n                'Saving datasamples is not supported when '\n                'return_datasamples is True.')\n        return preds\n\n    result = []\n    for pred in preds:\n        if isinstance(pred, DetDataSample):\n            pred = self.pred2dict(pred, pred_out_dir)\n        result.append(pred)\n\n    if visualization is not None:\n        for i, vis in enumerate(visualization):\n            result[i]['visualization'] = vis\n\n    if print_result:\n        print(json.dumps(result, indent=4))\n\n    return result\n\n", "CodeBleu_score": 50.37071426752699}, {"code": "def postprocess(\n        self,\n        preds: PredType,\n        visualization: Optional[List[np.ndarray]] = None,\n        return_datasamples: bool = False,\n        print_result: bool = False,\n        no_save_pred: bool = False,\n        pred_out_dir: str = '',\n        **kwargs,\n    ) -> Dict\n\t\n    if no_save_pred and pred_out_dir != '':\n        warnings.warn(\n            'The `no_save_pred` is True, but `pred_out_dir` is not empty. '\n            'The predictions will not be saved.')\n\n    if pred_out_dir != '' and not no_save_pred:\n        os.makedirs(pred_out_dir, exist_ok=True)\n        os.makedirs(osp.join(pred_out_dir, 'preds'), exist_ok=True)\n\n    if return_datasamples:\n        if pred_out_dir != '':\n            warnings.warn(\n                '`return_datasamples` is True, but `pred_out_dir` is not empty. '\n                'The predictions will not be saved.')\n        return preds\n\n    results = []\n    for pred in preds:\n        results.append(self.pred2dict(pred, pred_out_dir))\n\n    if visualization is not None:\n        for i, vis in enumerate(visualization):\n            results[i]['visualization'] = vis\n\n    if print_result:\n        print_log(results, logger='visualizer')\n\n    return results", "CodeBleu_score": 47.47649461802723}]}
{"instance_id": "a3c2561f199c", "generation_result": [{"code": "def build_test_pipeline(cfg: ConfigType) -> ConfigType\n\t\n    test_pipeline = cfg.data.test.pipeline\n    test_pipeline[0] = cfg.data.test.pipeline[0].replace('RandomResizedCrop', 'Resize')\n    test_pipeline[0] = cfg.data.test.pipeline[0].replace('(224, 224)', '(256, 256)')\n    test_pipeline[0] = cfg.data.test.pipeline[0].replace('interpolation=bicubic', 'interpolation=bilinear')\n    test_pipeline[-1] = cfg.data.test.pipeline[-1]\n    test_pipeline = Compose(test_pipeline)\n    return test_pipeline\n\n", "CodeBleu_score": 25.674425255814466}, {"code": "def build_test_pipeline(cfg: ConfigType) -> ConfigType\n\t\n    transforms = cfg.data.test.dataset.pipeline\n    transforms[0] = dict(type='Resize', img_scale=(1333, 800), keep_ratio=True)\n    transforms.append(dict(type='DefaultFormatBundle'))\n    transforms.append(dict(type='Collect', keys=['img']))\n    test_pipeline = Compose(transforms)\n    return test_pipeline\n\n\n\n", "CodeBleu_score": 20.82211034556134}, {"code": "def build_test_pipeline(cfg: ConfigType) -> ConfigType\n\t\n    test_pipeline = cfg.data.test.pipeline\n    test_pipeline[0] = cfg.data.test.pipeline[0].replace(type=\"Resize\")\n    test_pipeline[-1] = cfg.data.test.pipeline[-1]\n    test_pipeline = Compose(test_pipeline)\n    return test_pipeline\n\n", "CodeBleu_score": 21.87279819325011}]}
{"instance_id": "48a358ab5a38", "generation_result": [{"code": "def inference_mot(model: nn.Module, img: np.ndarray, frame_id: int,\n                  video_len: int) -> SampleList\n\t\n    data = dict(\n        img=img,\n        frame_id=frame_id,\n        gt_bboxes=[],\n        gt_labels=[],\n        gt_bboxes_ignore=[],\n        gt_instance_ids=[],\n        gt_match_indices=[],\n        video_len=video_len,\n        img_id=0,\n        img_shape=img.shape,\n        ori_shape=img.shape)\n    data = test_pipeline(data)\n    data = collate([data])\n    if next(model.parameters()).is_cuda:\n        # scatter to specified GPU\n        data = scatter(data, [device])[0]\n    else:\n        # this is a workaround to avoid the bug of MMDataParallel\n        data['img_metas'] = data['img_metas'][0].data\n        data['img'] = data['img'][0].data\n    # forward the model\n    with torch.no_grad():\n        result = model(return_loss=False, rescale=True, **data)\n    return result\n\n", "CodeBleu_score": 39.99824287258958}, {"code": "def inference_mot(model: nn.Module, img: np.ndarray, frame_id: int,\n                  video_len: int) -> SampleList\n\t\n    data = {\n        'img': img,\n        'frame_id': frame_id,\n        'imgs_shape': img.shape,\n        'img_id': 0,\n        'video_len': video_len\n    }\n    test_pipeline = build_test_pipeline(model.cfg)\n    data = test_pipeline(data)\n    if next(model.parameters()).is_cuda:\n        # scatter to specified GPU\n        data = scatter(data, [device])[0]\n    else:\n        # this is a workaround to avoid the bug of MMDataParallel\n        data['img_metas'] = data['img_metas'][0].data\n    # forward the model\n    with torch.no_grad():\n        result = model(return_loss=False, rescale=True, **data)\n    return result\n\n", "CodeBleu_score": 39.4399163251989}, {"code": "def inference_mot(model: nn.Module, img: np.ndarray, frame_id: int,\n                  video_len: int) -> SampleList\n\t\n    data = dict(img=img, frame_id=frame_id, ori_shape=img.shape, img_id=0,\n                video_len=video_len)\n    test_pipeline = build_test_pipeline(model.cfg)\n    data = test_pipeline(data)\n    if next(model.parameters()).is_cuda:\n        # scatter to specified GPU\n        data = scatter(data, [device])[0]\n    elif torch.cuda.is_available():\n        # scatter to GPU 0\n        data = scatter(data, [device])[0]\n    else:\n        # just get the stuff to CPU\n        data = scatter(data, ['cpu'])[0]\n\n    # forward the model\n    with torch.no_grad():\n        result = model(return_loss=False, rescale=True, **data)\n    return result\n\n", "CodeBleu_score": 37.32452230646848}]}
{"instance_id": "6c4ea2ce1787", "generation_result": [{"code": "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\n\t\n    while True:\n        total_length = len(tokens_a) + len(tokens_b)\n        if total_length <= max_num_tokens:\n            break\n        trunc_tokens = tokens_a if len(tokens_a) > len(tokens_b) else tokens_b\n        assert len(trunc_tokens) >= 1\n        if rng.random() < 0.5:\n            del trunc_tokens[0]\n        else:\n            trunc_tokens.pop()", "CodeBleu_score": 77.75746428493808}, {"code": "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\n\t\n    while True:\n        total_length = len(tokens_a) + len(tokens_b)\n        if total_length <= max_num_tokens:\n            break\n        trunc_tokens = tokens_a if len(tokens_a) > len(tokens_b) else tokens_b\n        assert len(trunc_tokens) >= 1\n        if rng.random() < 0.5:\n            del trunc_tokens[0]\n        else:\n            trunc_tokens.pop()\n    return tokens_a, tokens_b\n\n", "CodeBleu_score": 76.77635508499546}, {"code": "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\n\t\n    while True:\n        total_length = len(tokens_a) + len(tokens_b)\n        if total_length <= max_num_tokens:\n            break\n        trunc_tokens = tokens_a if len(tokens_a) > len(tokens_b) else tokens_b\n        assert len(trunc_tokens) >= 1\n        if rng.random() < 0.5:\n            del trunc_tokens[0]\n        else:\n            trunc_tokens.pop()\n\n", "CodeBleu_score": 77.75746428493808}]}
{"instance_id": "007181b6729d", "generation_result": [{"code": "def get_activation(activation_string)\n\t\n    if activation_string is None:\n        return None\n    if isinstance(activation_string, str):\n        if not activation_string:\n            return None\n        act = activation_string.lower()\n        if act == \"linear\":\n            return None\n        if act == \"relu\":\n            return tf.keras.layers.ReLU()\n        if act == \"gelu\":\n            return tf.keras.layers.GELU()\n        if act == \"tanh\":\n            return tf.keras.layers.Activation(\"tanh\")\n        if act == \"gated-gelu\":\n            return GatedGELU()\n        if act == \"quick-gelu\":\n            return QuickGELU()\n        if act == \"gated-gelu-new\":\n            return GatedGeluNew()\n        if act == \"gated-gelu-fast\":\n            return GatedGeluFast()\n        if act == \"gated-gelu-1.5\":\n            return GatedGelu1p5()\n        if act == \"gated-gelu-1.5-fast\":\n            return GatedGelu1p5Fast()\n        if act == \"gated-gelu-1.5-new\":\n            return GatedGelu1p5New()\n        raise KeyError(f\"function {activation_string} not found in activation layers\")\n    return activation_string\n\n", "CodeBleu_score": 47.48294570874697}, {"code": "def get_activation(activation_string)\n\t\n    if activation_string == \"relu\":\n        return tf.nn.relu\n    elif activation_string == \"tanh\":\n        return tf.nn.tanh\n    elif activation_string == \"sigmoid\":\n        return tf.nn.sigmoid\n    elif activation_string == \"softmax\":\n        return tf.nn.softmax\n    elif activation_string == \"leaky_relu\":\n        return tf.nn.leaky_relu\n    elif activation_string == \"elu\":\n        return tf.nn.elu\n    elif activation_string == \"selu\":\n        return tf.nn.selu\n    elif activation_string == \"softplus\":\n        return tf.nn.softplus\n    elif activation_string == \"softsign\":\n        return tf.nn.softsign\n    elif activation_string == \"exponential\":\n        return tf.nn.exponential\n    elif activation_string == \"linear\":\n        return None\n    else:\n        raise ValueError(f\"Unsupported activation function: {activation_string}\")\n\n\n", "CodeBleu_score": 30.38422274971282}, {"code": "def get_activation(activation_string)\n\t\n    if activation_string is None:\n        return None\n    if isinstance(activation_string, str):\n        if activation_string == 'relu':\n            return tf.nn.relu\n        elif activation_string == 'sigmoid':\n            return tf.nn.sigmoid\n        elif activation_string == 'tanh':\n            return tf.nn.tanh\n        else:\n            raise ValueError('Unsupported activation: %s' % activation_string)\n    elif callable(activation_string):\n        return activation_string\n    else:\n        raise ValueError('The activation argument should be a callable function or a string in {\"relu\", \"tanh\", \"sigmoid\"}, passed %s' % (activation_string,))\n\n", "CodeBleu_score": 36.39465525196365}]}
{"instance_id": "d5d6e0315899", "generation_result": [{"code": "def dropout(input_tensor, dropout_prob)\n\t\n    if dropout_prob is None or dropout_prob == 0.0:\n        return input_tensor\n    else:\n        output = tf.nn.dropout(input_tensor, 1.0 - dropout_prob)\n        return output\n\n", "CodeBleu_score": 82.89511352687609}, {"code": "def dropout(input_tensor, dropout_prob)\n\t\n    if dropout_prob is None or dropout_prob == 0.0:\n        return input_tensor\n    output = tf.nn.dropout(input_tensor, 1.0 - dropout_prob)\n    return output\n\n", "CodeBleu_score": 87.51451468041077}, {"code": "def dropout(input_tensor, dropout_prob)\n\t\n    if dropout_prob is None or dropout_prob == 0.0:\n        return input_tensor\n\n    output = tf.nn.dropout(input_tensor, 1.0 - dropout_prob)\n    return output", "CodeBleu_score": 87.51451468041077}]}
{"instance_id": "828917b9ac53", "generation_result": [{"code": "def create_attention_mask_from_input_mask(from_tensor, to_mask)\n\t\n    from_shape = get_shape_list(from_tensor, expected_rank=[2, 3])\n    batch_size = from_shape[0]\n    from_seq_length = from_shape[1]\n\n    to_shape = get_shape_list(to_mask, expected_rank=2)\n    to_seq_length = to_shape[1]\n\n    to_mask = tf.cast(\n        tf.reshape(to_mask, [batch_size, 1, to_seq_length]), tf.float32\n    )\n\n    # We don't assume that `from_tensor` is a mask (although it could be).\n    #\n    # `from_tensor` may actually be the output of embedding for padding words.\n    # Shape: [batch_size, from_seq_length, embedding_size].\n    from_mask = tf.ones([batch_size, from_seq_length, 1], tf.float32)\n\n    # We don't assume that `to_mask` is a mask.\n    #\n    # It could be a pad mask for encoder sequence with its corresponding\n    # key value states as an attention mask.\n    # Shape: [batch_size, 1, to_seq_length]\n    to_mask = tf.cast(\n        tf.reshape(to_mask, [batch_size, 1, to_seq_length]), tf.float32\n    )\n\n    return to_mask * from_mask\n\n", "CodeBleu_score": 53.30683637866078}, {"code": "def create_attention_mask_from_input_mask(from_tensor, to_mask)\n\t\n    from_shape = get_shape_list(from_tensor, expected_rank=[2, 3])\n    batch_size = from_shape[0]\n    from_seq_length = from_shape[1]\n\n    to_shape = get_shape_list(to_mask, expected_rank=2)\n    to_seq_length = to_shape[1]\n\n    to_mask = tf.cast(\n        tf.reshape(to_mask, [batch_size, 1, to_seq_length]), tf.float32\n    )\n\n    # We don't assume that `from_tensor` is a mask (although it could be).\n    #\n    # `from_tensor` might be a real input tensor, or come from a hidden state\n    # undergoing attention and we need to run the simulation many times.\n    #\n    # If the representation is a mask (i.e., zeroes and ones), then we use\n    # use use minimum value and add the attention mask.\n    #\n    # This is a small hack to allow attending to a masked position and retain\n    # the `0.0` value, which in models with scalar values would correspond to a\n    # zero, but in models with vector representations it would correspond to a\n    # vector of zeros.\n    #\n    # This hack works for both cases, but it creates a representation that is\n    # not easily interpretable.\n    #\n    # In the case where the representation is a vector (like `to_tensor` below)\n    # and we need to fill up the last `from_seq_length` with something, we can\n    # do so using `broadcast_ones`. In this case, the last parameter (`pad_dims`)\n    # is set to 1 to broadcast only the last dimension.\n    to_mask_broadcast = tf.reshape(\n        to_mask,\n        (batch_size, 1, to_seq_length, 1))\n    to_mask_broadcast = tf.cast(\n        to_mask_broadcast, tf.float32)\n\n    to_mask_broadcast = tf.mult", "CodeBleu_score": 47.73552869533106}, {"code": "def create_attention_mask_from_input_mask(from_tensor, to_mask)\n\t\n    from_shape = get_shape_list(from_tensor, expected_rank=[2, 3])\n    batch_size = from_shape[0]\n    from_seq_length = from_shape[1]\n\n    to_shape = get_shape_list(to_mask, expected_rank=2)\n    to_seq_length = to_shape[1]\n\n    to_mask = tf.cast(\n        tf.reshape(to_mask, [batch_size, 1, to_seq_length]), tf.float32\n    )\n\n    # We don't assume that `from_tensor` is a mask (although it could be).\n    #\n    # `from_tensor` might be a normal tensor, or a mask.\n    #\n    # 1. If `from_tensor` is a normal tensor, it is still a mask.\n    # Assume we are doing sequence to sequence.\n    # Its mask should still be an all-ones mask.\n    #\n    # 2. If `from_tensor` is a mask:\n    # From TF doc:\n    # If `from_tensor` is a mask then `axis` should be known statically.\n    if len(from_shape) == 3:\n        broadcast_ones = tf.ones(\n            shape=[batch_size, from_seq_length, 1], dtype=tf.float32\n        )\n    else:\n        broadcast_ones = tf.ones(\n            shape=[batch_size, 1, from_seq_length], dtype=tf.float32\n        )\n\n    # Here we broadcast along two dimensions to create the mask.\n    mask = broadcast_ones * to_mask\n\n    return mask\n\n", "CodeBleu_score": 55.99291775238687}]}
{"instance_id": "d7798e41a723", "generation_result": [{"code": "def transpose_for_scores(input_tensor, batch_size, num_attention_heads,\n                           seq_length, width)\n\t\n    output_tensor = tf.reshape(\n        input_tensor, [batch_size, seq_length, num_attention_heads, width])\n\n    output_tensor = tf.transpose(output_tensor, [0, 2, 1, 3])\n    return output_tensor\n\n", "CodeBleu_score": 59.99845781533536}, {"code": "def transpose_for_scores(input_tensor, batch_size, num_attention_heads,\n                           seq_length, width)\n\t\n    output_tensor = tf.reshape(\n        input_tensor, [batch_size, seq_length, num_attention_heads, width])\n\n    output_tensor = tf.transpose(output_tensor, [0, 2, 1, 3])\n    return output_tensor\n\n\n\n\n", "CodeBleu_score": 59.99845781533536}, {"code": "def transpose_for_scores(input_tensor, batch_size, num_attention_heads,\n                           seq_length, width)\n\t\n    output_tensor = tf.reshape(\n        input_tensor,\n        [batch_size, seq_length, num_attention_heads, width])\n\n    output_tensor = tf.transpose(output_tensor, [0, 2, 1, 3])\n    return output_tensor\n\n\n\n\n", "CodeBleu_score": 58.86672543766436}]}
{"instance_id": "711d466ace09", "generation_result": [{"code": "def reshape_to_matrix(input_tensor)\n\t\n    if len(input_tensor.shape) < 2:\n        raise ValueError(\n            \"Input tensor must have at least 2 dimensions, but got shape {}\".format(\n                input_tensor.shape\n            )\n        )\n    if len(input_tensor.shape) == 2:\n        return input_tensor\n    else:\n        width = input_tensor.shape[-1]\n        return tf.reshape(input_tensor, [-1, width])\n\n", "CodeBleu_score": 44.96218278747626}, {"code": "def reshape_to_matrix(input_tensor)\n\t\n    if len(input_tensor.shape) < 2:\n        raise ValueError(\"Input tensor must have at least 2 dimensions.\")\n    if len(input_tensor.shape) == 2:\n        return input_tensor\n    width = input_tensor.shape[-1]\n    return tf.reshape(input_tensor, [-1, width])\n\n", "CodeBleu_score": 48.28026085708082}, {"code": "def reshape_to_matrix(input_tensor)\n\t\n    if len(input_tensor.shape) < 2:\n        raise ValueError(\"Input tensor must have at least 2 dimensions\")\n    if len(input_tensor.shape) == 2:\n        return input_tensor\n    width = input_tensor.shape[-1]\n    return tf.reshape(input_tensor, [-1, width])\n\n\n\n", "CodeBleu_score": 48.26442560851442}]}
{"instance_id": "7adcd08f531e", "generation_result": [{"code": "def get_train_examples(self, data_dir)\n\t\n    return self._create_examples(\n        self._read_tsv(os.path.join(data_dir, \"train.tsv\")), \"train\")\n\n", "CodeBleu_score": 6.324149162613836}, {"code": "def get_train_examples(self, data_dir)\n\t\n    return self._create_examples(\n        self._read_tsv(os.path.join(data_dir, \"train.tsv\")), \"train\")\n", "CodeBleu_score": 6.324149162613836}, {"code": "def get_train_examples(self, data_dir)\n\t\n    return self._create_examples(\n        self._read_tsv(os.path.join(data_dir, \"train.tsv\")), \"train\")\n\n", "CodeBleu_score": 6.324149162613836}]}
{"instance_id": "26a0bb96dab9", "generation_result": [{"code": "def metric_fn(per_example_loss, label_ids, logits, is_real_example)\n\t\n\n    predictions = tf.argmax(logits, axis=-1, output_type=tf.int32)\n    accuracy = tf.cast(tf.equal(predictions, label_ids), dtype=tf.float32)\n    accuracy = tf.reduce_sum(accuracy * is_real_example) / tf.reduce_sum(is_real_example)\n    loss = tf.reduce_sum(per_example_loss * is_real_example) / tf.reduce_sum(is_real_example)\n    return {\"eval_accuracy\": accuracy, \"eval_loss\": loss}\n\n", "CodeBleu_score": 40.66933326447678}, {"code": "def metric_fn(per_example_loss, label_ids, logits, is_real_example)\n\t\n\n    predictions = tf.argmax(logits, axis=-1)\n    accuracy = tf.metrics.accuracy(\n        labels=label_ids, predictions=predictions, weights=is_real_example\n    )\n    mean_loss = tf.metrics.mean(values=per_example_loss, weights=is_real_example)\n    return {\"eval_accuracy\": accuracy, \"eval_loss\": mean_loss}\n\n", "CodeBleu_score": 44.27040825228}, {"code": "def metric_fn(per_example_loss, label_ids, logits, is_real_example)\n\t\n    predictions = tf.argmax(logits, axis=-1)\n    accuracy = tf.metrics.accuracy(\n        labels=label_ids, predictions=predictions, weights=is_real_example\n    )\n    loss = tf.metrics.mean(values=per_example_loss, weights=is_real_example)\n    return {\"eval_accuracy\": accuracy, \"eval_loss\": loss}\n\n", "CodeBleu_score": 45.73488057868157}]}
{"instance_id": "6154e4d28509", "generation_result": [{"code": "def convert_examples_to_features(examples, label_list, max_seq_length,\n                                 tokenizer)\n\t\n    features = []\n    for (ex_index, example) in enumerate(examples):\n        if ex_index % 10000 == 0:\n            tf.logging.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n\n        feature = convert_single_example(ex_index, example, label_list,\n                                         max_seq_length, tokenizer)\n\n        features.append(feature)\n    return features\n\n", "CodeBleu_score": 83.41734189253587}, {"code": "def convert_examples_to_features(examples, label_list, max_seq_length,\n                                 tokenizer)\n\t\n    \"\"\"Convert a set of `InputExample`s to a list of `InputFeatures`.\"\"\"\n\n    features = []\n    for (ex_index, example) in enumerate(examples):\n        if ex_index % 10000 == 0:\n            tf.logging.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n\n        feature = convert_single_example(ex_index, example, label_list,\n                                         max_seq_length, tokenizer)\n\n        features.append(feature)\n    return features\n\n", "CodeBleu_score": 83.41734189253587}, {"code": "def convert_examples_to_features(examples, label_list, max_seq_length,\n                                 tokenizer)\n\t\n\n    features = []\n    for (ex_index, example) in enumerate(examples):\n        if ex_index % 10000 == 0:\n            tf.logging.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n\n        feature = convert_single_example(ex_index, example, label_list,\n                                         max_seq_length, tokenizer)\n\n        features.append(feature)\n    return features\n\n", "CodeBleu_score": 83.41734189253587}]}
{"instance_id": "e09c0db71ebb", "generation_result": [{"code": "def get_next_sentence_output(bert_config, input_tensor, labels)\n\t\n    output_weights = tf.get_variable(\n        \"output_weights\", [bert_config.vocab_size, bert_config.hidden_size],\n        initializer=create_initializer(bert_config.initializer_range))\n    output_bias = tf.get_variable(\n        \"output_bias\", [bert_config.vocab_size],\n        initializer=tf.zeros_initializer())\n\n    logits = tf.matmul(input_tensor, output_weights, transpose_b=True)\n    logits = tf.nn.bias_add(logits, output_bias)\n    log_probs = tf.nn.log_softmax(logits, axis=-1)\n\n    labels = tf.reshape(labels, [-1])\n    one_hot_labels = tf.one_hot(labels, depth=bert_config.vocab_size, dtype=tf.float32)\n\n    per_example_loss = -tf.reduce_sum(log_probs * one_hot_labels, axis=-1)\n    loss = tf.reduce_mean(per_example_loss)\n\n    return loss, per_example_loss, log_probs\n\n", "CodeBleu_score": 46.73505377188981}, {"code": "def get_next_sentence_output(bert_config, input_tensor, labels)\n\t\n    with tf.variable_scope(\"cls/seq_relationship\"):\n        output_weights = tf.get_variable(\n            \"output_weights\",\n            shape=[2, bert_config.hidden_size],\n            initializer=create_initializer(bert_config.initializer_range))\n        output_bias = tf.get_variable(\n            \"output_bias\", shape=[2], initializer=tf.zeros_initializer())\n\n        logits = tf.matmul(input_tensor, output_weights, transpose_b=True)\n        logits = tf.nn.bias_add(logits, output_bias)\n        log_probs = tf.nn.log_softmax(logits, axis=-1)\n        labels = tf.reshape(labels, [-1])\n        one_hot_labels = tf.one_hot(labels, depth=2, dtype=tf.float32)\n        per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n        loss = tf.reduce_mean(per_example_loss)\n        return loss, per_example_loss, log_probs\n\n", "CodeBleu_score": 81.77676007088661}, {"code": "def get_next_sentence_output(bert_config, input_tensor, labels)\n\t\n    output_weights = tf.get_variable(\n        \"output_weights\", [bert_config.vocab_size, bert_config.hidden_size],\n        initializer=modeling.create_initializer(bert_config.initializer_range))\n    output_bias = tf.get_variable(\n        \"output_bias\", [bert_config.vocab_size],\n        initializer=tf.zeros_initializer())\n\n    logits = tf.matmul(input_tensor, output_weights, transpose_b=True)\n    logits = tf.nn.bias_add(logits, output_bias)\n    log_probs = tf.nn.log_softmax(logits, axis=-1)\n\n    labels = tf.reshape(labels, [-1])\n    one_hot_labels = tf.one_hot(labels, depth=bert_config.vocab_size, dtype=tf.float32)\n\n    per_example_loss = -tf.reduce_sum(log_probs * one_hot_labels, axis=-1)\n    loss = tf.reduce_mean(per_example_loss)\n\n    return loss, per_example_loss, log_probs\n\n", "CodeBleu_score": 47.947423413533116}]}
{"instance_id": "1f66b020f485", "generation_result": [{"code": "def gather_indexes(sequence_tensor, positions)\n\t\n    sequence_shape = modeling.get_shape_list(sequence_tensor, expected_rank=3)\n    batch_size = sequence_shape[0]\n    seq_length = sequence_shape[1]\n    width = sequence_shape[2]\n\n    flat_offsets = tf.reshape(\n        tf.range(0, batch_size, dtype=tf.int32) * seq_length, [-1, 1]\n    )\n    flat_positions = tf.reshape(positions + flat_offsets, [-1])\n    flat_sequence_tensor = tf.reshape(sequence_tensor, [batch_size * seq_length, width])\n    output_tensor = tf.gather(flat_sequence_tensor, flat_positions)\n    return output_tensor\n\n", "CodeBleu_score": 56.34905172926283}, {"code": "def gather_indexes(sequence_tensor, positions)\n\t\n    sequence_shape = modeling.get_shape_list(sequence_tensor, expected_rank=3)\n    batch_size = sequence_shape[0]\n    seq_length = sequence_shape[1]\n    width = sequence_shape[2]\n\n    flat_offsets = tf.reshape(\n        tf.range(0, batch_size, dtype=tf.int32) * seq_length, [-1, 1]\n    )\n    flat_positions = tf.reshape(positions + flat_offsets, [-1])\n    flat_sequence_tensor = tf.reshape(sequence_tensor, [batch_size * seq_length, width])\n    output_tensor = tf.gather(flat_sequence_tensor, flat_positions)\n\n    return output_tensor\n\n", "CodeBleu_score": 56.34905172926283}, {"code": "def gather_indexes(sequence_tensor, positions)\n\t\n    sequence_shape = modeling.get_shape_list(sequence_tensor, expected_rank=3)\n    batch_size = sequence_shape[0]\n    seq_length = sequence_shape[1]\n    width = sequence_shape[2]\n\n    flat_offsets = tf.reshape(\n        tf.range(0, batch_size, dtype=tf.int32) * seq_length, [-1, 1]\n    )\n    flat_positions = tf.reshape(positions + flat_offsets, [-1])\n    flat_sequence_tensor = tf.reshape(sequence_tensor, [batch_size * seq_length, width])\n    output_tensor = tf.gather(flat_sequence_tensor, flat_positions)\n    return output_tensor\n\n", "CodeBleu_score": 56.34905172926283}]}
{"instance_id": "956bffee9a33", "generation_result": [{"code": "def _decode_record(record, name_to_features)\n\t\n    example = tf.io.parse_single_example(record, name_to_features)\n    for name in list(example.keys()):\n        t = example[name]\n        if t.dtype == tf.int64:\n            t = tf.cast(t, tf.int32)\n        example[name] = t\n    return example\n\n", "CodeBleu_score": 60.19566542534298}, {"code": "def _decode_record(record, name_to_features)\n\t\n    example = tf.io.parse_single_example(record, name_to_features)\n    for name in list(example.keys()):\n        t = example[name]\n        if t.dtype == tf.int64:\n            t = tf.cast(t, tf.int32)\n        example[name] = t\n    return example\n\n", "CodeBleu_score": 60.19566542534298}, {"code": "def _decode_record(record, name_to_features)\n\t\n    example = tf.io.parse_single_example(record, name_to_features)\n    for name in list(example.keys()):\n        t = example[name]\n        if t.dtype == tf.int64:\n            t = tf.cast(t, tf.int32)\n        example[name] = t\n    return example\n\n", "CodeBleu_score": 60.19566542534298}]}
{"instance_id": "6b97f354e641", "generation_result": [{"code": "def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer,\n                         orig_answer_text)\n\t\n    tok_text = \" \".join(doc_tokens)\n\n    for new_start in range(input_start, input_end + 1):\n      for new_end in range(input_end, new_start - 1, -1):\n        text_span = \" \".join(doc_tokens[new_start:(new_end + 1)])\n        if text_span == orig_answer_text:\n          return (new_start, new_end)\n\n    return (input_start, input_end)\n\n", "CodeBleu_score": 72.21769421887572}, {"code": "def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer,\n                         orig_answer_text)\n\t\n    tok_text = \" \".join(doc_tokens)\n\n    # Tokenize orig_answer_text\n    tok_tokens = tokenizer.tokenize(orig_answer_text)\n    for new_start in range(input_start, input_end + 1):\n        for new_end in range(input_end, new_start - 1, -1):\n            text_span = \" \".join(doc_tokens[new_start:(new_end + 1)])\n            if text_span == orig_answer_text:\n                return (new_start, new_end)\n\n    return (input_start, input_end)\n\n", "CodeBleu_score": 76.73449518572171}, {"code": "def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer,\n                         orig_answer_text)\n\t\n    tok_text = \" \".join(doc_tokens)\n\n    # Iterate through possible new start and end positions\n    for new_start in range(input_start, input_end + 1):\n        for new_end in range(input_end, new_start - 1, -1):\n            text_span = \" \".join(doc_tokens[new_start:(new_end + 1)])\n            if text_span == orig_answer_text:\n                return (new_start, new_end)\n\n    return (input_start, input_end)\n\n", "CodeBleu_score": 72.25303531634142}]}
{"instance_id": "aba334b92b34", "generation_result": [{"code": "def _check_is_max_context(doc_spans, cur_span_index, position)\n\t\n    best_score = None\n    best_span_index = None\n    for (span_index, doc_span) in enumerate(doc_spans):\n        end = doc_span.start + doc_span.length - 1\n        if position < doc_span.start:\n            continue\n        if position > end:\n            continue\n        num_left_context = position - doc_span.start\n        num_right_context = end - position\n        score = min(num_left_context, num_right_context) + 0.01 * doc_span.length\n        if best_score is None or score > best_score:\n            best_score = score\n            best_span_index = span_index\n    return cur_span_index == best_span_index\n\n", "CodeBleu_score": 78.23889503357583}, {"code": "def _check_is_max_context(doc_spans, cur_span_index, position)\n\t\n    best_score = None\n    best_span_index = None\n    for (span_index, doc_span) in enumerate(doc_spans):\n        end = doc_span.start + doc_span.length - 1\n        if position < doc_span.start:\n            continue\n        if position > end:\n            continue\n        num_left_context = position - doc_span.start\n        num_right_context = end - position\n        score = min(num_left_context, num_right_context) + 0.01 * doc_span.length\n        if best_score is None or score > best_score:\n            best_score = score\n            best_span_index = span_index\n    return cur_span_index == best_span_index\n\n", "CodeBleu_score": 78.23889503357583}, {"code": "def _check_is_max_context(doc_spans, cur_span_index, position)\n\t\n    best_score = None\n    best_span_index = None\n    for (span_index, doc_span) in enumerate(doc_spans):\n        end = doc_span.start + doc_span.length - 1\n        if position < doc_span.start:\n            continue\n        if position > end:\n            continue\n        num_left_context = position - doc_span.start\n        num_right_context = end - position\n        score = min(num_left_context, num_right_context) + 0.01 * doc_span.length\n        if best_score is None or score > best_score:\n            best_score = score\n            best_span_index = span_index\n    return cur_span_index == best_span_index\n\n", "CodeBleu_score": 78.23889503357583}]}
{"instance_id": "cf92ad509f6a", "generation_result": [{"code": "def load_vocab(vocab_file)\n\t\n    vocab = collections.OrderedDict()\n    index = 0\n    with open(vocab_file, \"r\", encoding=\"utf-8\") as reader:\n        while True:\n            token = convert_to_unicode(reader.readline())\n            if not token:\n                break\n            token = token.strip()\n            vocab[token] = index\n            index += 1\n    return vocab\n\n", "CodeBleu_score": 65.15278771901627}, {"code": "def load_vocab(vocab_file)\n\t\n    vocab = collections.OrderedDict()\n    index = 0\n    with open(vocab_file, \"r\", encoding=\"utf-8\") as reader:\n        while True:\n            token = convert_to_unicode(reader.readline())\n            if not token:\n                break\n            token = token.strip()\n            vocab[token] = index\n            index += 1\n    return vocab\n\n", "CodeBleu_score": 65.15278771901627}, {"code": "def load_vocab(vocab_file)\n\t\n    vocab = collections.OrderedDict()\n    index = 0\n    with tf.gfile.GFile(vocab_file, \"r\") as reader:\n        while True:\n            token = convert_to_unicode(reader.readline())\n            if not token:\n                break\n            token = token.strip()\n            vocab[token] = index\n            index += 1\n    return vocab\n\n", "CodeBleu_score": 73.46545204327492}]}
{"instance_id": "6c865803ac12", "generation_result": [{"code": "def convert_by_vocab(vocab, items)\n\t\n    output = []\n    for item in items:\n        output.append(vocab[item])\n    return output\n\n", "CodeBleu_score": 79.63749907165396}, {"code": "def convert_by_vocab(vocab, items)\n\t\n    output = []\n    for item in items:\n        output.append(vocab[item])\n    return output\n\n", "CodeBleu_score": 79.63749907165396}, {"code": "def convert_by_vocab(vocab, items)\n\t\n    output = []\n    for item in items:\n        output.append(vocab[item])\n    return output\n\n", "CodeBleu_score": 79.63749907165396}]}
{"instance_id": "e93b9200dc4a", "generation_result": [{"code": "def tokenize(self, text)\n\t\n    tokens = []\n    for token in self.basic_tokenizer.tokenize(text, never_split=self.all_special_tokens):\n        for sub_token in self.wordpiece_tokenizer.tokenize(token):\n            tokens.append(sub_token)\n    return tokens\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 59.16801169011795}, {"code": "def tokenize(self, text)\n\t\n    tokens = self.basic_tokenizer.tokenize(text, never_split=self.all_special_tokens)\n    _tokens = []\n    for token in tokens:\n        if token in self.vocab:\n            _tokens.append(token)\n        else:\n            _tokens += self.wordpiece_tokenizer.tokenize(token)\n    return _tokens\n\n\n", "CodeBleu_score": 42.89382786130255}, {"code": "def tokenize(self, text)\n\t\n    tokens = self.basic_tokenizer.tokenize(text, never_split=self.all_special_tokens)\n    _tokens = []\n    for token in tokens:\n        if token in self.vocab:\n            _tokens.append(token)\n        else:\n            _tokens += self.wordpiece_tokenizer.tokenize(token)\n    return _tokens\n\n", "CodeBleu_score": 42.89382786130255}]}
{"instance_id": "2ff6f6db53c9", "generation_result": [{"code": "def safe_load(path, offset, duration, sample_rate, dtype)\n\t\n    try:\n        audio = load_audio(path, offset, duration, sample_rate, dtype)\n        return audio, False\n    except Exception as e:\n        logging.error(f\"Error loading audio from {path}: {e}\")\n        return -1.0, True\n\n", "CodeBleu_score": 22.448939006112553}, {"code": "def safe_load(path, offset, duration, sample_rate, dtype)\n\t\n    try:\n        audio = load(path, offset, duration, sample_rate, dtype)\n        return audio, False\n    except Exception as e:\n        logging.error(f\"Error loading audio file {path}: {e}\")\n        return -1.0, True\n\n\n\n", "CodeBleu_score": 22.07509922834857}, {"code": "def safe_load(path, offset, duration, sample_rate, dtype)\n\t\n    try:\n        audio_data, _ = librosa.load(\n            path,\n            offset=offset,\n            duration=duration,\n            sr=sample_rate,\n            dtype=dtype,\n        )\n        return audio_data, False\n    except Exception as e:\n        logger.exception(e)\n        return -1.0, True\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 32.11391139952467}]}
{"instance_id": "12fbe1f03d8f", "generation_result": [{"code": "def to_n_channels(waveform: tf.Tensor, n_channels: int) -> tf.Tensor\n\t\n    if waveform.shape.rank == 1:\n        waveform = tf.expand_dims(waveform, axis=-1)\n    if waveform.shape.rank == 2:\n        waveform = tf.expand_dims(waveform, axis=0)\n    if waveform.shape.rank == 3:\n        waveform = tf.expand_dims(waveform, axis=0)\n    if waveform.shape.rank == 4:\n        waveform = tf.expand_dims(waveform, axis=0)\n    if waveform.shape.rank == 5:\n        waveform = tf.expand_dims(waveform, axis=0)\n    if waveform.shape.rank == 6:\n        waveform = tf.expand_dims(waveform, axis=0)\n    if waveform.shape.rank == 7:\n        waveform = tf.expand_dims(waveform, axis=0)\n    if waveform.shape.rank == 8:\n        waveform = tf.expand_dims(waveform, axis=0)\n    if waveform.shape.rank == 9:\n        waveform = tf.expand_dims(waveform, axis=0)\n    if waveform.shape.rank == 10:\n        waveform = tf.expand_dims(waveform, axis=0)\n    if waveform.shape.rank == 11:\n        waveform = tf.expand_dims(waveform, axis=0)\n    if waveform.shape.rank == 12:\n        waveform = tf.expand_dims(waveform, axis=0)\n    if waveform.shape.rank == 13:\n        waveform = tf.expand_dims(waveform, axis=0)\n    if waveform.shape.rank == 14:\n        waveform = tf.expand_dims(waveform, axis=0)\n    if waveform.shape.rank == 15:\n        waveform = tf.expand_dims(waveform, axis=0)\n    if waveform.shape.rank == 16:\n        waveform = tf.expand_dims", "CodeBleu_score": 34.60655079139458}, {"code": "def to_n_channels(waveform: tf.Tensor, n_channels: int) -> tf.Tensor\n\t\n    if waveform.shape[-1] >= n_channels:\n        return waveform[..., :n_channels]\n    else:\n        waveform = tf.tile(waveform, [1, n_channels // waveform.shape[-1] + 1])\n        return waveform[..., :n_channels]\n\n", "CodeBleu_score": 47.29136339226846}, {"code": "def to_n_channels(waveform: tf.Tensor, n_channels: int) -> tf.Tensor\n\t\n    if waveform.shape[-1] >= n_channels:\n        return waveform[..., :n_channels]\n    else:\n        return tf.tile(waveform, [1, n_channels // waveform.shape[-1] + 1])[..., :n_channels]\n\n\n\n", "CodeBleu_score": 38.10655171826537}]}
{"instance_id": "5e215f029b2e", "generation_result": [{"code": "def spectrogram_to_db_uint(\n    spectrogram: tf.Tensor, db_range: float = 100.0, **kwargs\n) -> tf.Tensor\n\t\n    db_spectrogram = gain_to_db(spectrogram, **kwargs)\n    max_db = tf.reduce_max(db_spectrogram)\n    clipped_db_spectrogram = tf.clip_by_value(\n        db_spectrogram,\n        clip_value_min=max_db - db_range,\n        clip_value_max=max_db,\n    )\n    return tf.cast(\n        (clipped_db_spectrogram - max_db + db_range) / db_range * 255, tf.uint8\n    )\n\n", "CodeBleu_score": 43.25321406241024}, {"code": "def spectrogram_to_db_uint(\n    spectrogram: tf.Tensor, db_range: float = 100.0, **kwargs\n) -> tf.Tensor\n\t\n    db_spectrogram = gain_to_db(spectrogram, **kwargs)\n    max_db = tf.reduce_max(db_spectrogram)\n    clipped_db = tf.clip_by_value(db_spectrogram, max_db - db_range, max_db)\n    return tf.cast(255 * (clipped_db / db_range), tf.uint8)\n\n", "CodeBleu_score": 41.53659291295859}, {"code": "def spectrogram_to_db_uint(\n    spectrogram: tf.Tensor, db_range: float = 100.0, **kwargs\n) -> tf.Tensor\n\t\n    db_spectrogram = gain_to_db(spectrogram, **kwargs)\n    db_spectrogram = tf.clip_by_value(\n        db_spectrogram,\n        clip_value_min=-db_range,\n        clip_value_max=tf.reduce_max(db_spectrogram),\n    )\n    db_spectrogram += db_range\n    db_spectrogram *= 255.0 / db_range\n    return tf.cast(db_spectrogram, tf.uint8)\n\n", "CodeBleu_score": 43.47991123510357}]}
{"instance_id": "b65c0968dae7", "generation_result": [{"code": "def compute_spectrogram_tf(\n    waveform: tf.Tensor,\n    frame_length: int = 2048,\n    frame_step: int = 512,\n    spec_exponent: float = 1.0,\n    window_exponent: float = 1.0,\n) -> tf.Tensor\n\t\n    stft = tf.signal.stft(\n        waveform,\n        frame_length=frame_length,\n        frame_step=frame_step,\n        fft_length=frame_length,\n        window_fn=tf.signal.hann_window,\n        pad_end=True,\n    )\n    spectrogram = tf.transpose(tf.abs(stft) ** spec_exponent)\n    return spectrogram\n\n", "CodeBleu_score": 36.95543924946334}, {"code": "def compute_spectrogram_tf(\n    waveform: tf.Tensor,\n    frame_length: int = 2048,\n    frame_step: int = 512,\n    spec_exponent: float = 1.0,\n    window_exponent: float = 1.0,\n) -> tf.Tensor\n\t\n\n    # Apply the Hann window function raised to a specified exponent\n    window = tf.signal.hann_window(frame_length, periodic=True) ** window_exponent\n\n    # Perform the STFT with defined frame length and step\n    stft = tf.signal.stft(\n        waveform,\n        frame_length=frame_length,\n        frame_step=frame_step,\n        fft_length=frame_length,\n        window_fn=window,\n        pad_end=True,\n    )\n\n    # Transpose the spectrogram tensor\n    spectrogram = tf.transpose(tf.abs(stft) ** spec_exponent)\n\n    return spectrogram\n\n", "CodeBleu_score": 39.84983149981273}, {"code": "def compute_spectrogram_tf(\n    waveform: tf.Tensor,\n    frame_length: int = 2048,\n    frame_step: int = 512,\n    spec_exponent: float = 1.0,\n    window_exponent: float = 1.0,\n) -> tf.Tensor\n\t\n    # Apply a Hann window raised to a power.\n    window = tf.signal.hann_window(frame_length, periodic=True)\n    window = tf.math.pow(window, window_exponent)\n    # Extract the STFT.\n    stft = tf.signal.stft(\n        waveform,\n        frame_length=frame_length,\n        frame_step=frame_step,\n        fft_length=frame_length,\n        window_fn=window,\n        pad_end=True,\n    )\n    # Obtain the magnitude of the STFT.\n    spectrogram = tf.abs(stft)\n    # Raise the magnitude to the power specified.\n    spectrogram = tf.math.pow(spectrogram, spec_exponent)\n    # Setting DC component to a small value.\n    spectrogram = tf.maximum(spectrogram, 1e-12)\n    # Transpose the spectrogram.\n    spectrogram = tf.transpose(spectrogram, [1, 0])\n    return spectrogram\n\n\n\n", "CodeBleu_score": 37.28070054693544}]}
{"instance_id": "a4686140a634", "generation_result": [{"code": "def time_stretch(\n    spectrogram: tf.Tensor,\n    factor: float = 1.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n    # Get the original shape of the spectrogram\n    original_shape = tf.shape(spectrogram)\n\n    # Calculate the new time dimension\n    new_time_dim = tf.cast(\n        tf.cast(original_shape[1], tf.float32) * factor, tf.int32\n    )\n\n    # Resize the spectrogram using the specified interpolation method\n    resized_spectrogram = tf.image.resize(\n        spectrogram, [original_shape[0], new_time_dim], method=method\n    )\n\n    # Adjust the resized spectrogram to the original dimensions\n    # by cropping or padding\n    if factor > 1.0:\n        # Crop the spectrogram\n        cropped_spectrogram = resized_spectrogram[\n            :, :original_shape[1]\n        ]\n        return cropped_spectrogram\n    elif factor < 1.0:\n        # Pad the spectrogram\n        padded_spectrogram = tf.pad(\n            resized_spectrogram,\n            [[0, 0], [0, original_shape[1] - new_time_dim]],\n            \"CONSTANT\",\n        )\n        return padded_spectrogram\n    else:\n        return resized_spectrogram", "CodeBleu_score": 36.82457266774681}, {"code": "def time_stretch(\n    spectrogram: tf.Tensor,\n    factor: float = 1.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n\n    # Get the original shape of the spectrogram\n    original_shape = tf.shape(spectrogram)\n\n    # Calculate the new time dimension\n    new_time_dim = tf.cast(\n        tf.cast(original_shape[1], tf.float32) * factor, tf.int32\n    )\n\n    # Resize the spectrogram using the specified interpolation method\n    resized_spectrogram = tf.image.resize(\n        spectrogram, [original_shape[0], new_time_dim], method=method\n    )\n\n    # Adjust the resized spectrogram to the original dimensions\n    if factor < 1.0:\n        # Crop the spectrogram\n        cropped_spectrogram = resized_spectrogram[\n            :, :original_shape[1]\n        ]\n        return cropped_spectrogram\n    elif factor > 1.0:\n        # Pad the spectrogram with zeros\n        padded_spectrogram = tf.pad(\n            resized_spectrogram,\n            [[0, 0], [0, original_shape[1] - new_time_dim]],\n            \"CONSTANT\",\n        )\n        return padded_spectrogram\n    else:\n        # No change needed\n        return resized_spectrogram\n\n\n\n\n", "CodeBleu_score": 36.82457266774681}, {"code": "def time_stretch(\n    spectrogram: tf.Tensor,\n    factor: float = 1.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n    time_dim = tf.shape(spectrogram)[1]\n    new_time_dim = tf.cast(tf.cast(time_dim, tf.float32) * factor, tf.int32)\n    resized_spectrogram = tf.image.resize(\n        spectrogram, [new_time_dim, tf.shape(spectrogram)[2]], method=method\n    )\n    if factor < 1.0:\n        cropped_spectrogram = resized_spectrogram[\n            :, :time_dim, :\n        ]  # Crop the resized spectrogram to the original time dimension\n    else:\n        padding = tf.maximum(time_dim - new_time_dim, 0)\n        padded_spectrogram = tf.pad(\n            resized_spectrogram, [[0, 0], [0, padding], [0, 0]]\n        )  # Pad the resized spectrogram to the original time dimension\n        cropped_spectrogram = padded_spectrogram[\n            :, :time_dim, :\n        ]  # Crop the padded spectrogram to the original time dimension\n    return cropped_spectrogram\n\n", "CodeBleu_score": 40.427704938586594}]}
{"instance_id": "6dd01101d4ae", "generation_result": [{"code": "def pitch_shift(\n    spectrogram: tf.Tensor,\n    semitone_shift: float = 0.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n    # Calculate the resizing factor based on the semitone shift\n    resizing_factor = 2.0 ** (semitone_shift / 12.0)\n\n    # Resize the spectrogram along the frequency axis\n    resized_spectrogram = tf.image.resize(\n        spectrogram,\n        [int(spectrogram.shape[1] * resizing_factor), spectrogram.shape[2]],\n        method=method,\n    )\n\n    # Pad the resized spectrogram to maintain the original dimensions\n    padded_spectrogram = tf.image.pad_to_bounding_box(\n        resized_spectrogram,\n        0,\n        0,\n        spectrogram.shape[1],\n        spectrogram.shape[2],\n    )\n\n    return padded_spectrogram\n\n\n", "CodeBleu_score": 27.671297095740805}, {"code": "def pitch_shift(\n    spectrogram: tf.Tensor,\n    semitone_shift: float = 0.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n    semitone_shift = tf.cast(semitone_shift, tf.float32)\n    semitone_shift = tf.clip_by_value(semitone_shift, -12.0, 12.0)\n    factor = 2.0 ** (semitone_shift / 12.0)\n    factor = tf.where(tf.math.is_nan(factor), 1.0, factor)\n    new_height = tf.cast(\n        tf.cast(tf.shape(spectrogram)[1], tf.float32) * factor, tf.int32\n    )\n    spectrogram = tf.image.resize(\n        spectrogram, [new_height, tf.shape(spectrogram)[2]], method=method\n    )\n    spectrogram = tf.image.resize_with_crop_or_pad(\n        spectrogram, tf.shape(spectrogram)[1], tf.shape(spectrogram)[2]\n    )\n    return spectrogram\n\n", "CodeBleu_score": 32.606265719344314}, {"code": "def pitch_shift(\n    spectrogram: tf.Tensor,\n    semitone_shift: float = 0.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n    # Calculate the resizing factor\n    resizing_factor = 2 ** (semitone_shift / 12)\n\n    # Resize the spectrogram along the frequency axis\n    resized_spectrogram = tf.image.resize(\n        spectrogram,\n        [tf.shape(spectrogram)[1], int(tf.shape(spectrogram)[2] * resizing_factor)],\n        method=method,\n    )\n\n    # Pad the resized spectrogram to maintain the original dimensions\n    padded_spectrogram = tf.image.pad_to_bounding_box(\n        resized_spectrogram,\n        0,\n        0,\n        tf.shape(spectrogram)[1],\n        tf.shape(spectrogram)[2],\n    )\n\n    return padded_spectrogram\n\n\n\n\n", "CodeBleu_score": 30.847640798488733}]}
{"instance_id": "612f0826e56a", "generation_result": [{"code": "def _get_conv_activation_layer(params: Dict) -> Any\n\t\n    if params[\"conv_activation\"] == \"relu\":\n        return nn.ReLU()\n    elif params[\"conv_activation\"] == \"elu\":\n        return nn.ELU()\n    else:\n        return nn.LeakyReLU(negative_slope=0.2)\n\n", "CodeBleu_score": 34.13471863812688}, {"code": "def _get_conv_activation_layer(params: Dict) -> Any\n\t\n    conv_activation = params.get(\"conv_activation\", \"ReLU\")\n    if conv_activation == \"ReLU\":\n        return nn.ReLU()\n    elif conv_activation == \"ELU\":\n        return nn.ELU()\n    elif conv_activation == \"LeakyReLU\":\n        return nn.LeakyReLU(negative_slope=0.2)\n    else:\n        raise ValueError(f\"Invalid conv_activation: {conv_activation}\")\n\n\n\n", "CodeBleu_score": 42.620950946616695}, {"code": "def _get_conv_activation_layer(params: Dict) -> Any\n\t\n    activation_name = params.get(\"conv_activation\", \"ReLU\")\n    if activation_name == \"ReLU\":\n        return nn.ReLU()\n    elif activation_name == \"ELU\":\n        return nn.ELU()\n    elif activation_name == \"LeakyReLU\":\n        return nn.LeakyReLU(0.2)\n    else:\n        raise ValueError(f\"Unsupported activation: {activation_name}\")\n\n\n\n", "CodeBleu_score": 34.66874030259108}]}
{"instance_id": "2f8b5a2fc0f2", "generation_result": [{"code": "def softmax_unet(\n    input_tensor: tf.Tensor, instruments: Iterable[str], params: Dict = {}\n) -> Dict\n\t\n    logging.info(\"Apply softmax unet\")\n    # Initialize the output dictionary with the input tensor.\n    outputs = {\"mix\": input_tensor}\n    # Initialize an empty list to collect the logit masks.\n    logit_masks = []\n    # Iterate over the instruments.\n    for instrument in instruments:\n        # Apply the U-Net model to the input tensor for the current instrument.\n        logit_mask = apply_unet(input_tensor, instrument, params)\n        # Collect the logit mask.\n        logit_masks.append(logit_mask)\n    # Combine the logit masks using a softmax function along the channel axis.\n    mask = Softmax(axis=-1)(tf.stack(logit_masks, axis=-1))\n    # Iterate over the instruments.\n    for idx, instrument in enumerate(instruments):\n        # Create a spectrogram for the current instrument by multiplying the corresponding mask with the input tensor.\n        outputs[instrument] = Multiply(name=instrument)([mask[..., idx], input_tensor])\n    # Return the dictionary of instrument spectrograms.\n    return outputs\n\n", "CodeBleu_score": 32.03845293044255}, {"code": "def softmax_unet(\n    input_tensor: tf.Tensor, instruments: Iterable[str], params: Dict = {}\n) -> Dict\n\t\n    logging.info(f\"Apply softmax unet\")\n    logit_masks = {}\n    for instrument in instruments:\n        logit_masks[instrument] = apply_unet(\n            input_tensor, instrument, params, output_mask_logit=True\n        )\n    mask = Softmax(axis=-1)(Concatenate(axis=-1)(list(logit_masks.values())))\n    return {\n        instrument: Multiply(name=instrument)([mask, input_tensor])\n        for instrument in instruments\n    }\n\n", "CodeBleu_score": 33.91682754092149}, {"code": "def softmax_unet(\n    input_tensor: tf.Tensor, instruments: Iterable[str], params: Dict = {}\n) -> Dict\n\t\n    logging.info(\"Apply softmax unet\")\n    instrument_logit_masks = {\n        instrument: apply_unet(input_tensor, instrument, params)\n        for instrument in instruments\n    }\n    instrument_masks = {\n        instrument: Softmax(axis=-1, name=f\"{instrument}_mask\")(logit_mask)\n        for instrument, logit_mask in instrument_logit_masks.items()\n    }\n    return {\n        instrument: Multiply(name=instrument)([input_tensor, mask])\n        for instrument, mask in instrument_masks.items()\n    }\n\n", "CodeBleu_score": 27.55396384879947}]}
{"instance_id": "400b19ce8520", "generation_result": [{"code": "def compute_file_checksum(path)\n\t\n    import hashlib\n\n    sha256_hash = hashlib.sha256()\n    with open(path, \"rb\") as f:\n        for byte_block in iter(lambda: f.read(4096), b\"\"):\n            sha256_hash.update(byte_block)\n        return sha256_hash.hexdigest()\n\n", "CodeBleu_score": 53.392094209182495}, {"code": "def compute_file_checksum(path)\n\t\n    import hashlib\n\n    # Initialize the hash object\n    sha256_hash = hashlib.sha256()\n\n    # Read the file in chunks and update the hash\n    with open(path, \"rb\") as f:\n        while True:\n            chunk = f.read(4096)\n            if not chunk:\n                break\n            sha256_hash.update(chunk)\n\n    # Return the hexadecimal representation of the computed hash\n    return sha256_hash.hexdigest()\n\n", "CodeBleu_score": 37.75169198682076}, {"code": "def compute_file_checksum(path)\n\t\n    import hashlib\n    sha256_hash = hashlib.sha256()\n    with open(path, \"rb\") as f:\n        for byte_block in iter(lambda: f.read(4096), b\"\"):\n            sha256_hash.update(byte_block)\n        return sha256_hash.hexdigest()\n\n", "CodeBleu_score": 53.392094209182495}]}
{"instance_id": "5f2e9c6e15f1", "generation_result": [{"code": "def checksum(self, name: str) -> str\n\t\n    url = f\"{self.host}/{self.repository}/{self.release_path}/{self.checksum_index}\"\n    response = requests.get(url)\n    if response.status_code != 200:\n        raise ValueError(f\"Failed to retrieve checksum index: {response.status_code}\")\n    index = response.json()\n    if name not in index:\n        raise ValueError(f\"Model '{name}' not found in checksum index\")\n    return index[name]\n\n\n\n\n\n", "CodeBleu_score": 39.348146568787044}, {"code": "def checksum(self, name: str) -> str\n\t\n    url = f\"{self.host}/{self.repository}/{self.release_path}/checksum.json\"\n    response = requests.get(url)\n    if response.status_code == 200:\n        index = response.json()\n        if name in index:\n            return index[name]\n        else:\n            raise ValueError(f\"Model '{name}' not found in checksum index.\")\n    else:\n        raise ValueError(f\"Failed to retrieve checksum index from {url}\")\n\n\n\n\n", "CodeBleu_score": 39.33809935727185}, {"code": "def checksum(self, name: str) -> str\n\t\n    url = f\"{self.host}/{self.repository}/{self.release_path}/{self.checksum_index}\"\n    response = requests.get(url)\n    if response.status_code == 200:\n        index = response.json()\n        if name in index:\n            return index[name]\n    raise ValueError(f\"Model {name} not found in index\")\n\n", "CodeBleu_score": 34.74784531957354}]}
{"instance_id": "9312c8b6032e", "generation_result": [{"code": "def download(self, name: str, path: str) -> None:\n\t\n    url: str = \"/\".join(\n        (\n            self._host,\n            self._repository,\n            self.RELEASE_PATH,\n            self._release,\n            self.MODELS_PATH,\n            name,\n        )\n    )\n    self._logger.info(f\"Downloading model {name} from {url}\")\n    with httpx.stream(\"GET\", url, http2=True) as response:\n        response.raise_for_status()\n        with tempfile.NamedTemporaryFile(delete=False) as stream:\n            for chunk in response.iter_bytes():\n                stream.write(chunk)\n        if self.checksum(name) != compute_file_checksum(stream.name):\n            raise ValueError(f\"Checksum mismatch for model {name}\")\n        self._logger.info(f\"Extracting model {name} to {path}\")\n        with zipfile.ZipFile(stream.name, \"r\") as archive:\n            archive.extractall(path)\n        os.remove(stream.name)\n    self._logger.info(f\"Model {name} extracted to {path}\")", "CodeBleu_score": 46.98846818531269}, {"code": "def download(self, name: str, path: str) -> None:\n\t\n    url: str = \"/\".join(\n        (\n            self._host,\n            self._repository,\n            self.RELEASE_PATH,\n            self._release,\n            self.MODELS_PATH,\n            name,\n        )\n    )\n    self._logger.info(\"Downloading model archive from %s\", url)\n    response: httpx.Response = httpx.get(url, follow_redirects=True)\n    response.raise_for_status()\n    with tempfile.NamedTemporaryFile() as file:\n        file.write(response.content)\n        file.flush()\n        file.seek(0)\n        self._logger.info(\"Validating checksum for model %s\", name)\n        checksum: str = self.checksum(name)\n        if checksum != compute_file_checksum(file.name):\n            raise ValueError(f\"Invalid checksum for model {name}\")\n        self._logger.info(\"Extracting model archive to %s\", path)\n        with tarfile.open(fileobj=file) as archive:\n            archive.extractall(path)\n        self._logger.info(\"Extraction completed\")\n\n", "CodeBleu_score": 46.801263038401785}, {"code": "def download(self, name: str, path: str) -> None:\n\t\n    url: str = \"/\".join(\n        (\n            self._host,\n            self._repository,\n            self.RELEASE_PATH,\n            self._release,\n            self.MODELS_PATH,\n            name,\n        )\n    )\n    self._logger.info(f\"Downloading model {name} from {url}\")\n    with httpx.stream(\"GET\", url) as response:\n        response.raise_for_status()\n        path = Path(path)\n        path.mkdir(parents=True, exist_ok=True)\n        temporary: Path = path / f\"{name}.tmp\"\n        with temporary.open(\"wb\") as stream:\n            for chunk in response.iter_bytes():\n                stream.write(chunk)\n        checksum: str = self.checksum(name)\n        if compute_file_checksum(temporary) != checksum:\n            raise ValueError(f\"Checksum mismatch for model {name}\")\n        with temporary.open(\"rb\") as stream:\n            with tarfile.open(fileobj=stream, mode=\"r:gz\") as archive:\n                archive.extractall(path)\n        temporary.unlink()\n        self._logger.info(f\"Extracted model {name} to {path}\")\n\n", "CodeBleu_score": 51.192162210710556}]}
{"instance_id": "ee9493f37d77", "generation_result": [{"code": "def load_configuration(descriptor: str) -> Dict\n\t\n    if descriptor.startswith(\"embedded://\"):\n        name = descriptor.split(\"embedded://\")[1]\n        try:\n            content = pkgutil.get_data(__name__, f\"resources/{name}.json\")\n            return json.loads(content)\n        except FileNotFoundError:\n            raise ValueError(f\"Embedded configuration '{name}' not found.\")\n    else:\n        if not os.path.exists(descriptor):\n            raise ValueError(f\"Configuration file '{descriptor}' not found.\")\n        with open(descriptor, \"r\") as f:\n            return json.load(f)\n\n\n\n", "CodeBleu_score": 49.154899460101085}, {"code": "def load_configuration(descriptor: str) -> Dict\n\t\n    if descriptor.startswith(\"embedded://\"):\n        # Extract the embedded configuration name\n        config_name = descriptor.split(\"embedded://\")[1]\n        # Read the JSON content from the embedded resource\n        config_json = pkgutil.get_data(\"my_package\", f\"configs/{config_name}.json\")\n        if config_json is None:\n            raise ValueError(f\"Embedded configuration '{config_name}' not found.\")\n        config = json.loads(config_json)\n    else:\n        # Check if the file exists\n        if not os.path.exists(descriptor):\n            raise ValueError(f\"Configuration file '{descriptor}' not found.\")\n        # Read the JSON content from the file\n        with open(descriptor, \"r\") as f:\n            config = json.load(f)\n    return config\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 45.846172210375094}, {"code": "def load_configuration(descriptor: str) -> Dict\n\t\n    if descriptor.startswith(\"embedded://\"):\n        name = descriptor.split(\"embedded://\")[1]\n        try:\n            return json.loads(pkgutil.get_data(__name__, f\"configurations/{name}.json\"))\n        except FileNotFoundError:\n            raise ValueError(f\"Embedded configuration '{name}' not found.\")\n    else:\n        if not os.path.isfile(descriptor):\n            raise ValueError(f\"File '{descriptor}' not found.\")\n        with open(descriptor, \"r\") as f:\n            return json.load(f)\n\n", "CodeBleu_score": 48.22012984244691}]}
{"instance_id": "546134b67fbc", "generation_result": [{"code": "def sync_apply(\n    tensor_dict: Dict[str, tf.Tensor], func: Callable, concat_axis: int = 1\n) -> Dict[str, tf.Tensor]\n\t\n    assert concat_axis in [0, 1], \"concat_axis must be either 0 or 1.\"\n    tensor_list = list(tensor_dict.values())\n    if concat_axis == 0:\n        tensor = tf.concat(tensor_list, axis=concat_axis)\n    else:\n        tensor = tf.stack(tensor_list, axis=concat_axis)\n    processed_tensor = func(tensor)\n    if concat_axis == 0:\n        processed_tensor_list = tf.split(processed_tensor, len(tensor_list), axis=concat_axis)\n    else:\n        processed_tensor_list = tf.unstack(processed_tensor, axis=concat_axis)\n    processed_tensor_dict = {}\n    for i, key in enumerate(tensor_dict.keys()):\n        processed_tensor_dict[key] = processed_tensor_list[i]\n    return processed_tensor_dict\n\n", "CodeBleu_score": 43.46221597053342}, {"code": "def sync_apply(\n    tensor_dict: Dict[str, tf.Tensor], func: Callable, concat_axis: int = 1\n) -> Dict[str, tf.Tensor]\n\t\n    if concat_axis not in [0, 1]:\n        raise ValueError(\n            f\"concat_axis must be either 0 or 1, but got {concat_axis} instead.\"\n        )\n    tensor_list = list(tensor_dict.values())\n    if concat_axis == 0:\n        tensor = tf.concat(tensor_list, axis=concat_axis)\n    else:\n        tensor = tf.stack(tensor_list, axis=concat_axis)\n    processed_tensor = func(tensor)\n    if concat_axis == 0:\n        processed_tensor_list = tf.split(processed_tensor, len(tensor_list), concat_axis)\n    else:\n        processed_tensor_list = tf.unstack(processed_tensor, axis=concat_axis)\n    return {\n        key: tensor\n        for key, tensor in zip(tensor_dict.keys(), processed_tensor_list)\n    }\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 47.58133261541462}, {"code": "def sync_apply(\n    tensor_dict: Dict[str, tf.Tensor], func: Callable, concat_axis: int = 1\n) -> Dict[str, tf.Tensor]\n\t\n    if concat_axis not in [0, 1]:\n        raise ValueError(\n            f\"concat_axis must be either 0 or 1, but got {concat_axis}.\"\n        )\n\n    tensor_list = list(tensor_dict.values())\n    if concat_axis == 0:\n        tensor = tf.concat(tensor_list, axis=concat_axis)\n        processed_tensor = func(tensor)\n        processed_tensor_list = tf.split(processed_tensor, len(tensor_list), axis=0)\n    else:\n        tensor = tf.concat(tensor_list, axis=concat_axis)\n        processed_tensor = func(tensor)\n        processed_tensor_list = tf.split(processed_tensor, len(tensor_list), axis=1)\n\n    processed_tensor_dict = {}\n    for i, key in enumerate(tensor_dict.keys()):\n        processed_tensor_dict[key] = processed_tensor_list[i]\n\n    return processed_tensor_dict\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 47.330181632337144}]}
{"instance_id": "df48e78d46d2", "generation_result": [{"code": "def from_float32_to_uint8(\n    tensor: tf.Tensor,\n    tensor_key: str = \"tensor\",\n    min_key: str = \"min\",\n    max_key: str = \"max\",\n) -> tf.Tensor\n\t\n\n    # Get the minimum and maximum values of the tensor\n    min_value = tf.reduce_min(tensor)\n    max_value = tf.reduce_max(tensor)\n\n    # Scale the tensor values to the uint8 range\n    tensor = tf.cast(tensor, tf.float32)\n    tensor = (tensor - min_value) / (max_value - min_value)\n    tensor = tf.cast(tensor * 255, tf.uint8)\n\n    # Return the scaled tensor and the original minimum and maximum values\n    return {tensor_key: tensor, min_key: min_value, max_key: max_value}\n\n", "CodeBleu_score": 49.21164030304975}, {"code": "def from_float32_to_uint8(\n    tensor: tf.Tensor,\n    tensor_key: str = \"tensor\",\n    min_key: str = \"min\",\n    max_key: str = \"max\",\n) -> tf.Tensor\n\t\n    min = tf.reduce_min(tensor)\n    max = tf.reduce_max(tensor)\n    scaled_tensor = tf.cast((tensor - min) / (max - min) * 255, tf.uint8)\n    return {tensor_key: scaled_tensor, min_key: min, max_key: max}\n\n", "CodeBleu_score": 38.24653524883072}, {"code": "def from_float32_to_uint8(\n    tensor: tf.Tensor,\n    tensor_key: str = \"tensor\",\n    min_key: str = \"min\",\n    max_key: str = \"max\",\n) -> tf.Tensor\n\t\n    if tensor.dtype != tf.float32:\n        raise ValueError(\n            f\"The tensor must be of type tf.float32, but got {tensor.dtype}.\"\n        )\n    if tensor.ndim != 3:\n        raise ValueError(\n            f\"The tensor must be 3D, but got {tensor.ndim} dimensions.\"\n        )\n    if tensor.shape[2] != 3:\n        raise ValueError(\n            f\"The tensor must have 3 channels, but got {tensor.shape[2]} channels.\"\n        )\n    min_value = tf.reduce_min(tensor)\n    max_value = tf.reduce_max(tensor)\n    scaled_tensor = tf.cast(\n        (tensor - min_value) / (max_value - min_value) * 255, tf.uint8\n    )\n    return {tensor_key: scaled_tensor, min_key: min_value, max_key: max_value}\n\n", "CodeBleu_score": 47.12537234071274}]}
{"instance_id": "7f479e5de492", "generation_result": [{"code": "def pad_and_partition(tensor: tf.Tensor, segment_len: int) -> tf.Tensor\n\t\n    padding_len = (-len(tensor)) % segment_len\n    if padding_len > 0:\n        padding = tf.zeros([padding_len], tf.int64)\n        tensor = tf.concat([tensor, padding], 0)\n    total_segments = len(tensor) // segment_len\n    tensor = tf.reshape(tensor, [total_segments, segment_len])\n    return tensor\n\n\n", "CodeBleu_score": 31.607247016579322}, {"code": "def pad_and_partition(tensor: tf.Tensor, segment_len: int) -> tf.Tensor\n\t\n    padding_len = (-len(tensor)) % segment_len\n    padded = tf.pad(tensor, [[0, padding_len]])\n    split = tf.reshape(padded, [-1, segment_len])\n    return split\n\n", "CodeBleu_score": 17.391688632647746}, {"code": "def pad_and_partition(tensor: tf.Tensor, segment_len: int) -> tf.Tensor\n\t\n    padding_len = (-len(tensor)) % segment_len\n    padded = tf.pad(tensor, [[0, padding_len]])\n    split = tf.reshape(padded, [-1, segment_len])\n    return split\n\n", "CodeBleu_score": 17.391688632647746}]}
{"instance_id": "1789fae7253f", "generation_result": [{"code": "def pad_and_reshape(instr_spec, frame_length, F) -> Any\n\t\n    n_frames = instr_spec.shape[1]\n    n_padding = (frame_length - n_frames % frame_length) % frame_length\n    extension = np.zeros((instr_spec.shape[0], n_padding))\n    padded = np.concatenate((instr_spec, extension), axis=1)\n    n_grid_cells = padded.shape[1] // frame_length\n    reshaped = padded.reshape((F, n_grid_cells, frame_length))\n    return reshaped\n\n", "CodeBleu_score": 16.612981241112657}, {"code": "def pad_and_reshape(instr_spec, frame_length, F) -> Any\n\t\n    num_extra_rows = frame_length - (instr_spec.shape[0] % frame_length)\n    extension = np.zeros((num_extra_rows, instr_spec.shape[1]))\n    padded_instr_spec = np.concatenate((instr_spec, extension), axis=0)\n    padded_instr_spec = padded_instr_spec.reshape((-1, F))\n    return padded_instr_spec\n\n", "CodeBleu_score": 15.829725367936534}, {"code": "def pad_and_reshape(instr_spec, frame_length, F) -> Any\n\t\n    pad_length = frame_length - F\n    extension = torch.zeros((pad_length, instr_spec.shape[2]))\n    extended_spec = torch.cat((instr_spec, extension), 0)\n    processed_spec = extended_spec.reshape((extended_spec.shape[0] * extended_spec.shape[1], extended_spec.shape[2]))\n    return processed_spec\n\n", "CodeBleu_score": 16.924974303315913}]}
{"instance_id": "8a4f4658d97c", "generation_result": [{"code": "def dataset_from_csv(csv_path: str, **kwargs) -> Any\n\t\n    df = pd.read_csv(csv_path)\n    ds = tf.data.Dataset.from_tensor_slices(dict(df))\n    return ds\n\n", "CodeBleu_score": 21.529355723608496}, {"code": "def dataset_from_csv(csv_path: str, **kwargs) -> Any\n\t\n    df = pd.read_csv(csv_path)\n    df.drop(columns=[\"Unnamed: 0\"], inplace=True)\n    df.dropna(inplace=True)\n    df.reset_index(inplace=True)\n    df.drop(columns=[\"index\"], inplace=True)\n    df.drop(columns=[\"Unnamed: 0\"], inplace=True)\n    df.reset_index(inplace=True)\n    df.drop(columns=[\"index\"], inplace=True)\n    df.drop(columns=[\"Unnamed: 0\"], inplace=True)\n    df.reset_index(inplace=True)\n    df.drop(columns=[\"index\"], inplace=True)\n    df.drop(columns=[\"Unnamed: 0\"], inplace=True)\n    df.reset_index(inplace=True)\n    df.drop(columns=[\"index\"], inplace=True)\n    df.drop(columns=[\"Unnamed: 0\"], inplace=True)\n    df.reset_index(inplace=True)\n    df.drop(columns=[\"index\"], inplace=True)\n    df.drop(columns=[\"Unnamed: 0\"], inplace=True)\n    df.reset_index(inplace=True)\n    df.drop(columns=[\"index\"], inplace=True)\n    df.drop(columns=[\"Unnamed: 0\"], inplace=True)\n    df.reset_index(inplace=True)\n    df.drop(columns=[\"index\"], inplace=True)\n    df.drop(columns=[\"Unnamed: 0\"], inplace=True)\n    df.reset_index(inplace=True)\n    df.drop(columns=[\"index\"], inplace=True)\n    df.drop(columns=[\"Unnamed: 0\"], inplace=True)\n    df.reset_index(inplace=True)\n    df.drop(columns=[\"index\"], inplace=True)\n    df.drop(columns=[\"Unnamed: 0\"], inplace=True)\n    df.reset_index(inplace=True)\n    df.drop(columns=[\"index\"],", "CodeBleu_score": 12.946845849389865}, {"code": "def dataset_from_csv(csv_path: str, **kwargs) -> Any\n\t\n    df = pd.read_csv(csv_path)\n    df = df.sample(frac=1).reset_index(drop=True)\n    ds = tf.data.Dataset.from_tensor_slices((dict(df)))\n    return ds\n\n", "CodeBleu_score": 24.958755859892378}]}
{"instance_id": "9a16e81b182f", "generation_result": [{"code": "def check_tensor_shape(tensor_tf: tf.Tensor, target_shape: Any) -> bool\n\t\n    result = True\n    for i, length in enumerate(target_shape):\n        if length is not None:\n            result &= tensor_tf.shape[i] == length\n    return result\n\n", "CodeBleu_score": 30.88674131158809}, {"code": "def check_tensor_shape(tensor_tf: tf.Tensor, target_shape: Any) -> bool\n\t\n    result = True\n    for i, length in enumerate(target_shape):\n        if length is not None:\n            result &= (tf.shape(tensor_tf)[i] == length)\n    return result\n\n\n\n", "CodeBleu_score": 35.26451857476895}, {"code": "def check_tensor_shape(tensor_tf: tf.Tensor, target_shape: Any) -> bool\n\t\n    result = True\n    for dim, length in enumerate(target_shape):\n        if length is not None:\n            result &= tensor_tf.shape[dim] == length\n    return result\n\n", "CodeBleu_score": 30.69623101381621}]}
{"instance_id": "49d5d3084b3a", "generation_result": [{"code": "def get_training_dataset(\n    audio_params: Dict, audio_adapter: AudioAdapter, audio_path: str\n) -> Any\n\t\n    # Initialize the DatasetBuilder with audio parameters and a random seed\n    builder = DatasetBuilder(\n        chunk_duration=audio_params[\"chunk_duration\"], random_seed=0\n    )\n    # Build the training dataset\n    return builder.build(\n        csv_path=audio_adapter.train_csv_path(audio_path),\n        cache_directory=audio_adapter.cache_directory(audio_path),\n        batch_size=audio_params[\"batch_size\"],\n        n_chunks_per_song=audio_params[\"n_chunks_per_song\"],\n        random_data_augmentation=audio_params[\"random_data_augmentation\"],\n        wait_for_cache=audio_params[\"wait_for_cache\"],\n    )\n\n", "CodeBleu_score": 38.64829417600769}, {"code": "def get_training_dataset(\n    audio_params: Dict, audio_adapter: AudioAdapter, audio_path: str\n) -> Any\n\t\n    # Initialize DatasetBuilder.\n    builder = DatasetBuilder(\n        chunk_duration=audio_params[\"chunk_duration\"], random_seed=audio_params[\"random_seed\"]\n    )\n    # Build training dataset.\n    return builder.build(\n        csv_path=os.path.join(audio_path, \"train.csv\"),\n        cache_directory=os.path.join(audio_path, \"cache\"),\n        batch_size=audio_params[\"batch_size\"],\n        n_chunks_per_song=audio_params[\"n_chunks_per_song\"],\n        random_data_augmentation=audio_params[\"random_data_augmentation\"],\n        num_parallel_calls=audio_params[\"num_parallel_calls\"],\n        wait_for_cache=audio_params[\"wait_for_cache\"],\n    )\n\n", "CodeBleu_score": 41.29285011434349}, {"code": "def get_training_dataset(\n    audio_params: Dict, audio_adapter: AudioAdapter, audio_path: str\n) -> Any\n\t\n    builder = DatasetBuilder(\n        audio_params=audio_params,\n        audio_adapter=audio_adapter,\n        chunk_duration=11.88,\n        random_seed=42,\n    )\n    train_csv = os.path.join(audio_path, \"train.csv\")\n    cache_directory = os.path.join(audio_path, \"cache\")\n    dataset = builder.build(\n        csv_path=train_csv,\n        cache_directory=cache_directory,\n        batch_size=8,\n        n_chunks_per_song=2,\n        random_data_augmentation=True,\n        wait_for_cache=True,\n    )\n    return dataset\n\n", "CodeBleu_score": 36.205646438962376}]}
{"instance_id": "fe8bbbba06e3", "generation_result": [{"code": "def instruments(self) -> Any\n\t\n    if not self._instrument_builders:\n        for instrument in self._instruments:\n            self._instrument_builders.append(InstrumentDatasetBuilder(instrument, self._client, self._instrument_data_service, self._instrument_data_service_v2))\n    for builder in self._instrument_builders:\n        yield builder\n\n", "CodeBleu_score": 49.66144350262053}, {"code": "def instruments(self) -> Any\n\t\n    if self._instruments is None:\n        self._instruments = {\n            \"S&P 500\": InstrumentDatasetBuilder(\n                \"S&P 500\", \"SP500\", \"SP500\", \"SP500\", \"SP500\", \"SP500\"\n            ),\n            \"NASDAQ 100\": InstrumentDatasetBuilder(\n                \"NASDAQ 100\", \"NDX\", \"NDX\", \"NDX\", \"NDX\", \"NDX\"\n            ),\n            \"Russell 2000\": InstrumentDatasetBuilder(\n                \"Russell 2000\", \"RUT\", \"RUT\", \"RUT\", \"RUT\", \"RUT\"\n            ),\n            \"Dow Jones\": InstrumentDatasetBuilder(\n                \"Dow Jones\", \"DJI\", \"DJI\", \"DJI\", \"DJI\", \"DJI\"\n            ),\n            \"Nikkei 225\": InstrumentDatasetBuilder(\n                \"Nikkei 225\", \"N225\", \"N225\", \"N225\", \"N225\", \"N225\"\n            ),\n            \"S&P/TSX Composite Index\": InstrumentDatasetBuilder(\n                \"S&P/TSX Composite Index\", \"SPTSX\", \"SPTSX\", \"SPTSX\", \"SPTSX\", \"SPTSX\"\n            ),\n            \"S&P GSCI Composite Index\": InstrumentDatasetBuilder(\n                \"S&P GSCI Composite Index\", \"GSCI\", \"GSCI\", \"GSCI\", \"GSCI\", \"GSCI\"\n            ),\n            \"S&P GSCI Composite Index (Capped)\": InstrumentDatasetBuilder(\n                \"S&P GSCI Composite Index (Capped)\",\n                \"GSCICAP\",\n                \"GSCICAP\",\n                \"GSCICAP\",\n                \"GSCICAP\",\n                \"GSCICAP\",\n            ),\n            \"FTSE 100\": InstrumentDataset", "CodeBleu_score": 19.017232258483396}, {"code": "def instruments(self) -> Any\n\t\n    if not self._instruments:\n        for instrument in self.config.instruments:\n            self._instruments[instrument] = InstrumentDatasetBuilder(instrument, self.config)\n    for instrument in self._instruments:\n        yield self._instruments[instrument]", "CodeBleu_score": 35.55895422226432}]}
{"instance_id": "438b57fd3413", "generation_result": [{"code": "def cache(self, dataset: Any, cache: Optional[str], wait: bool) -> Any\n\t\n    if cache is not None:\n        if wait:\n            while not exists(f\"{cache}.index\"):\n                logger.info(f\"Cache not available, wait {self.WAIT_PERIOD}\")\n                time.sleep(self.WAIT_PERIOD)\n        cache_path = os.path.split(cache)[0]\n        os.makedirs(cache_path, exist_ok=True)\n        return dataset.cache(cache)\n    return dataset\n\n", "CodeBleu_score": 93.65648279369884}, {"code": "def cache(self, dataset: Any, cache: Optional[str], wait: bool) -> Any\n\t\n    if cache is not None:\n        if wait:\n            while not exists(f\"{cache}.index\"):\n                logger.info(f\"Cache not available, wait {self.WAIT_PERIOD}\")\n                time.sleep(self.WAIT_PERIOD)\n        cache_path = os.path.split(cache)[0]\n        os.makedirs(cache_path, exist_ok=True)\n        return dataset.cache(cache)\n    return dataset", "CodeBleu_score": 93.65648279369884}, {"code": "def cache(self, dataset: Any, cache: Optional[str], wait: bool) -> Any\n\t\n    if cache is not None:\n        if wait:\n            while not exists(f\"{cache}.index\"):\n                logger.info(f\"Cache not available, wait {self.WAIT_PERIOD}\")\n                time.sleep(self.WAIT_PERIOD)\n        cache_path = os.path.split(cache)[0]\n        os.makedirs(cache_path, exist_ok=True)\n        return dataset.cache(cache)\n    return dataset", "CodeBleu_score": 93.65648279369884}]}
{"instance_id": "8838a3a09a66", "generation_result": [{"code": "def create_estimator(params: Dict, MWF: bool) -> tf.Tensor\n\t\n    model_dir = ModelProvider.get_model_dir(params)\n    params.update({\"model_dir\": model_dir})\n    session_config = tf.ConfigProto()\n    session_config.gpu_options.allow_growth = True\n    run_config = tf.estimator.RunConfig(\n        save_summary_steps=params[\"save_summary_steps\"],\n        keep_checkpoint_max=params[\"keep_checkpoint_max\"],\n        save_checkpoints_steps=params[\"save_checkpoints_steps\"],\n        log_step_count_steps=params[\"log_step_count_steps\"],\n        session_config=session_config,\n    )\n    return tf.estimator.Estimator(\n        model_fn=model_fn,\n        model_dir=model_dir,\n        params=params,\n        config=run_config,\n        warm_start_from=MWF,\n    )\n\n", "CodeBleu_score": 28.34480400398031}, {"code": "def create_estimator(params: Dict, MWF: bool) -> tf.Tensor\n\t\n    model_dir = ModelProvider.get_model_dir(params)\n    params.update({\"model_dir\": model_dir})\n    session_config = tf.ConfigProto()\n    session_config.gpu_options.allow_growth = True\n    run_config = tf.estimator.RunConfig(\n        model_dir=model_dir,\n        save_summary_steps=params[\"save_summary_steps\"],\n        save_checkpoints_steps=params[\"save_checkpoints_steps\"],\n        keep_checkpoint_max=params[\"keep_checkpoint_max\"],\n        log_step_count_steps=params[\"log_step_count_steps\"],\n        session_config=session_config,\n    )\n    return tf.estimator.Estimator(\n        model_fn=model_fn,\n        model_dir=model_dir,\n        params=params,\n        config=run_config,\n        warm_start_from=MWF,\n    )\n\n", "CodeBleu_score": 28.437344292834098}, {"code": "def create_estimator(params: Dict, MWF: bool) -> tf.Tensor\n\t\n\n    model_dir = params['model_dir']\n    model_dir = ModelProvider.get_model_dir(model_dir)\n    params['model_dir'] = model_dir\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    config.gpu_options.per_process_gpu_memory_fraction = 0.45\n    session_config = tf.compat.v1.ConfigProto(device_count={'GPU': 0})\n    run_config = tf.estimator.RunConfig(\n        model_dir=model_dir,\n        save_summary_steps=params['save_summary_steps'],\n        save_checkpoints_steps=params['save_checkpoints_steps'],\n        keep_checkpoint_max=params['keep_checkpoint_max'],\n        log_step_count_steps=params['log_step_count_steps'],\n        session_config=session_config\n    )\n\n    if MWF:\n        model_fn = model_fn_estimator\n    else:\n        model_fn = model_fn_estimator_not_distributed\n\n    return tf.estimator.Estimator(\n        model_fn=model_fn,\n        params=params,\n        config=run_config\n    )\n\n", "CodeBleu_score": 29.43483625818529}]}
{"instance_id": "779c812bf2cb", "generation_result": [{"code": "def _separate_tensorflow(\n        self, waveform: np.ndarray, audio_descriptor: AudioDescriptor\n    ) -> Dict\n\t\n    waveform = self._ensure_stereo(waveform)\n    prediction_generator = self._get_prediction_generator()\n    self._data_generator.update_data(\n        {\"waveform\": waveform, \"audio_id\": audio_descriptor.audio_id}\n    )\n    prediction = next(prediction_generator)\n    prediction.pop(\"audio_id\")\n    return prediction\n\n", "CodeBleu_score": 55.94886029262578}, {"code": "def _separate_tensorflow(\n        self, waveform: np.ndarray, audio_descriptor: AudioDescriptor\n    ) -> Dict\n\t\n    if waveform.shape[1] == 1:\n        waveform = np.repeat(waveform, 2, axis=1)\n    prediction_generator = self._get_prediction_generator()\n    self._data_generator = lambda: (\n        {\"waveform\": waveform, \"audio_id\": audio_descriptor.audio_id},\n    )\n    prediction = next(prediction_generator)\n    del prediction[\"audio_id\"]\n    return prediction\n\n", "CodeBleu_score": 59.68941618603294}, {"code": "def _separate_tensorflow(\n        self, waveform: np.ndarray, audio_descriptor: AudioDescriptor\n    ) -> Dict\n\t\n        waveform = self._ensure_stereo(waveform)\n        prediction_generator = self._get_prediction_generator()\n        self._data_generator.update_data(\n            {\"waveform\": waveform, \"audio_id\": audio_descriptor.audio_id}\n        )\n        prediction = next(prediction_generator)\n        del prediction[\"audio_id\"]\n        return prediction\n\n", "CodeBleu_score": 51.55143183663067}]}
{"instance_id": "cf580cd2ed93", "generation_result": [{"code": "def compute_backbone_shapes(config, image_shape)\n\t\n    if callable(config.BACKBONE):\n        return config.COMPUTE_BACKBONE_SHAPE(image_shape)\n\n    # Currently supports ResNet only\n    assert config.BACKBONE in [\"resnet50\", \"resnet101\"]\n    return np.array(\n        [[int(math.ceil(image_shape[0] / stride)),\n            int(math.ceil(image_shape[1] / stride))]\n            for stride in config.BACKBONE_STRIDES])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 95.198357853076}, {"code": "def compute_backbone_shapes(config, image_shape)\n\t\n    if callable(config.BACKBONE):\n        return config.COMPUTE_BACKBONE_SHAPE(image_shape)\n\n    # Currently supports ResNet only\n    if config.BACKBONE in [\"resnet50\", \"resnet101\"]:\n        return np.array(\n            [[int(math.ceil(image_shape[0] / stride)),\n              int(math.ceil(image_shape[1] / stride))]\n             for stride in config.BACKBONE_STRIDES])\n\n    raise NotImplementedError(config.BACKBONE)\n\n", "CodeBleu_score": 81.9634042405937}, {"code": "def compute_backbone_shapes(config, image_shape)\n\t\n    if callable(config.BACKBONE):\n        return config.COMPUTE_BACKBONE_SHAPE(image_shape)\n\n    # Currently supports ResNet only\n    if \"resnet\" in config.BACKBONE:\n        return np.array(\n            [[image_shape[0] // stride, image_shape[1] // stride]\n             for stride in config.BACKBONE_STRIDES])\n\n    assert(\n        False, \"compute_backbone_shapes not yet supported for the selected backbone\")\n\n", "CodeBleu_score": 53.79976409371625}]}
{"instance_id": "ebd61a2e59ca", "generation_result": [{"code": "def clip_boxes_graph(boxes, window)\n\t\n    with tf.name_scope('clip_boxes'):\n        boxes = tf.transpose(boxes)\n        window = tf.transpose(window)\n        d = tf.cast(tf.floor(window[0]), tf.int32)\n        ymin = tf.maximum(d[0], boxes[0])\n        xmin = tf.maximum(d[1], boxes[1])\n        ymax = tf.minimum(window[0], boxes[2])\n        xmax = tf.minimum(window[1], boxes[3])\n        clipped = tf.concat([ymin, xmin, ymax, xmax], axis=0)\n        clipped = tf.transpose(clipped)\n        return clipped\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 30.834718245289327}, {"code": "def clip_boxes_graph(boxes, window)\n\t\n    with tf.name_scope('clip_to_window'):\n        boxes = tf.transpose(boxes)\n        window = tf.transpose(window)\n        d = tf.cast(tf.maximum(window - boxes[:, 0], 0), tf.int32)\n        boxes[:, 0] += d[:, 0]\n        d = tf.cast(tf.maximum(window - boxes[:, 2], 0), tf.int32)\n        boxes[:, 2] -= d[:, 1]\n        d = tf.cast(tf.maximum(boxes[:, 0] - window[:, 0], 0), tf.int32)\n        boxes[:, 0] -= d[:, 0]\n        d = tf.cast(tf.maximum(boxes[:, 2] - window[:, 2], 0), tf.int32)\n        boxes[:, 2] += d[:, 1]\n        d = tf.cast(tf.maximum(window - boxes[:, 1], 0), tf.int32)\n        boxes[:, 1] += d[:, 0]\n        d = tf.cast(tf.maximum(window - boxes[:, 3], 0), tf.int32)\n        boxes[:, 3] -= d[:, 1]\n        d = tf.cast(tf.maximum(boxes[:, 1] - window[:, 1], 0), tf.int32)\n        boxes[:, 1] -= d[:, 0]\n        d = tf.cast(tf.maximum(boxes[:, 3] - window[:, 3], 0), tf.int32)\n        boxes[:, 3] += d[:, 1]\n        return tf.transpose(boxes)\n\n\n\n", "CodeBleu_score": 24.922764746608784}, {"code": "def clip_boxes_graph(boxes, window)\n\t\n    with tf.name_scope('clip_to_window'):\n        boxes = tf.transpose(boxes)\n        window = tf.transpose(window)\n        d = tf.cast(tf.maximum(window - boxes[:, 0], 0), tf.int32)\n        boxes[:, 0] += d[:, 0]\n        boxes[:, 2] += d[:, 0]\n        d = tf.cast(tf.maximum(window - boxes[:, 1], 0), tf.int32)\n        boxes[:, 1] += d[:, 1]\n        boxes[:, 3] += d[:, 1]\n        return tf.transpose(boxes)\n\n\n\n", "CodeBleu_score": 23.879476223088997}]}
{"instance_id": "8e0411587a70", "generation_result": [{"code": "def rpn_graph(feature_map, anchors_per_location, anchor_stride)\n\t\n    shared = tf.keras.layers.Conv2D(512, kernel_size=3, strides=anchor_stride,\n                                    padding=\"same\", activation=\"relu\",\n                                    kernel_initializer=tf.keras.initializers.RandomNormal(stddev=0.02))(feature_map)\n\n    x = tf.keras.layers.Conv2D(len(anchors_per_location) * 3, kernel_size=1, padding=\"valid\",\n                               kernel_initializer=tf.keras.initializers.RandomNormal(stddev=0.02))(shared)\n\n    box_scores = tf.keras.layers.Reshape((-1, 3))(x)\n    box_scores = tf.keras.layers.Softmax(axis=-1)(box_scores)\n\n    x = tf.keras.layers.Conv2D(len(anchors_per_location) * 4, kernel_size=1, padding=\"valid\",\n                               kernel_initializer=tf.keras.initializers.RandomNormal(stddev=0.02))(shared)\n\n    box_delta = tf.keras.layers.Reshape((-1, 4))(x)\n\n    return box_scores, box_delta\n\n\n\n\n", "CodeBleu_score": 23.980991588002695}, {"code": "def rpn_graph(feature_map, anchors_per_location, anchor_stride)\n\t\n    shared = KL.Conv2D(512, (3, 3), padding='same', activation='relu',\n                       strides=anchor_stride,\n                       name='rpn_conv_shared')(feature_map)\n\n    x = KL.Conv2D(2 * anchors_per_location, (1, 1), padding='valid',\n                  activation='linear', name='rpn_class_raw')(shared)\n\n    rpn_class_logits = KL.Lambda(\n        lambda t: tf.reshape(t, [tf.shape(t)[0], -1, 2]))(x)\n\n    rpn_probs = KL.Activation(\n        \"softmax\", name=\"rpn_class_xxx\")(rpn_class_logits)\n\n    x = KL.Conv2D(anchors_per_location * 4, (1, 1), padding=\"valid\",\n                  activation='linear', name='rpn_bbox_pred')(shared)\n\n    rpn_bbox = KL.Lambda(lambda t: tf.reshape(t, [tf.shape(t)[0], -1, 4]))(x)\n\n    return [rpn_class_logits, rpn_probs, rpn_bbox]\n\n", "CodeBleu_score": 85.99714523366548}, {"code": "def rpn_graph(feature_map, anchors_per_location, anchor_stride)\n\t\n    shared = tf.keras.layers.Conv2D(512, kernel_size=3, strides=anchor_stride,\n                                    padding=\"same\", activation=\"relu\",\n                                    kernel_initializer='random_uniform',\n                                    bias_initializer='zeros')(feature_map)\n\n    x = tf.keras.layers.Conv2D(2 * anchors_per_location, kernel_size=1, padding=\"valid\",\n                               kernel_initializer='random_uniform',\n                               bias_initializer='zeros')(shared)\n    x = tf.keras.layers.Reshape((-1, 2))(x)\n    x = tf.keras.layers.Activation(\"softmax\")(x)\n    class_logits = tf.keras.layers.Reshape((-1, 2))(x)\n\n    x = tf.keras.layers.Conv2D(4 * anchors_per_location, kernel_size=1, padding=\"valid\",\n                               kernel_initializer='random_uniform',\n                               bias_initializer='zeros')(shared)\n    x = tf.keras.layers.Reshape((-1, 4))(x)\n    box_predictions = tf.keras.layers.Activation(\"linear\")(x)\n\n    return class_logits, box_predictions\n\n", "CodeBleu_score": 33.55610671673128}]}
{"instance_id": "70777c9261d8", "generation_result": [{"code": "def smooth_l1_loss(y_true, y_pred)\n\t\n    diff = tf.abs(y_true - y_pred)\n    mask = tf.cast(diff < 1, tf.float32)\n    loss = (diff ** 2) * mask + (diff - 0.5) * (1 - mask)\n    return tf.reduce_mean(loss)\n\n\n\n\n", "CodeBleu_score": 25.419759155273102}, {"code": "def smooth_l1_loss(y_true, y_pred)\n\t\n    absolute_difference = tf.abs(y_true - y_pred)\n    mask = tf.cast(absolute_difference < 1, tf.float32)\n    loss = (absolute_difference ** 2) * mask + (absolute_difference - 0.5) * (1 - mask)\n    return tf.reduce_mean(loss)\n\n\n\n", "CodeBleu_score": 21.92191133513069}, {"code": "def smooth_l1_loss(y_true, y_pred)\n\t\n    abs_diff = tf.abs(y_true - y_pred)\n    mask = tf.less(abs_diff, 1.0)\n    squared_loss = tf.square(abs_diff)\n    linear_loss = abs_diff - 0.5\n    loss = tf.where(mask, squared_loss, linear_loss)\n    return tf.reduce_mean(loss)\n\n\n\n", "CodeBleu_score": 21.094840478114556}]}
{"instance_id": "0e747b604087", "generation_result": [{"code": "def build_model(x_train, num_classes)\n\t\n    import tensorflow as tf\n    from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n    from tensorflow.keras.models import Sequential\n\n    # Reset the TensorFlow graph\n    tf.keras.backend.clear_session()\n\n    # Define the input layer\n    input_shape = x_train.shape[1:]\n\n    # Build the model\n    model = Sequential()\n    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n    model.add(Conv2D(32, (3, 3), activation='relu'))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Flatten())\n    model.add(Dense(64, activation='relu'))\n    model.add(Dense(num_classes, activation='softmax'))\n\n    # Compile the model\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n    return model\n\n", "CodeBleu_score": 28.520354149292054}, {"code": "def build_model(x_train, num_classes)\n\t\n    import tensorflow as tf\n    from tensorflow.keras import layers\n\n    # Reset the TensorFlow graph\n    tf.keras.backend.clear_session()\n\n    # Define the input layer\n    input_layer = layers.Input(shape=x_train.shape[1:])\n\n    # Add convolutional layers with ReLU activation\n    x = layers.Conv2D(32, (3, 3), activation='relu')(input_layer)\n    x = layers.Conv2D(64, (3, 3), activation='relu')(x)\n\n    # Add max-pooling layer\n    x = layers.MaxPooling2D((2, 2))(x)\n\n    # Flatten the output\n    x = layers.Flatten()(x)\n\n    # Add dense layers with ReLU activation\n    x = layers.Dense(128, activation='relu')(x)\n\n    # Add dense layer with softmax activation for classification\n    output_layer = layers.Dense(num_classes, activation='softmax')(x)\n\n    # Create the model\n    model = tf.keras.Model(inputs=input_layer, outputs=output_layer)\n\n    # Compile the model\n    model.compile(optimizer='adam',\n                  loss='categorical_crossentropy',\n                  metrics=['accuracy'])\n\n    return model\n\n", "CodeBleu_score": 42.982329461467636}, {"code": "def build_model(x_train, num_classes)\n\t\n    import tensorflow as tf\n    from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n    from tensorflow.keras.models import Sequential\n\n    # Reset the TensorFlow graph\n    tf.keras.backend.clear_session()\n\n    # Define the input layer with the shape of the training data\n    input_shape = x_train.shape[1:]\n\n    # Define the model architecture\n    model = Sequential()\n    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n    model.add(Conv2D(64, (3, 3), activation='relu'))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Flatten())\n    model.add(Dense(128, activation='relu'))\n    model.add(Dense(num_classes, activation='softmax'))\n\n    # Compile the model\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n    return model\n\n", "CodeBleu_score": 28.733742743136997}]}
{"instance_id": "813e3e2844f4", "generation_result": [{"code": "def extract_bboxes(mask)\n\t\n    boxes = [None] * mask.shape[-1]\n    for i in range(mask.shape[-1]):\n        m = mask[..., i]\n        horizontal_indicies = np.where(np.any(m, axis=0))[0]\n        vertical_indicies = np.where(np.any(m, axis=1))[0]\n        if horizontal_indicies.shape[0]:\n            x1, x2 = horizontal_indicies[[0, -1]]\n            y1, y2 = vertical_indicies[[0, -1]]\n            boxes[i] = np.array([y1, x1, y2, x2])\n        else:\n            boxes[i] = np.zeros((4,))\n    return boxes\n\n\n\n", "CodeBleu_score": 56.0563471188669}, {"code": "def extract_bboxes(mask)\n\t\n    bboxes = []\n    for m in mask:\n        if m.sum() == 0:\n            bboxes.append([0, 0, 0, 0])\n        else:\n            horizontal = m.sum(0) != 0\n            vertical = m.sum(1) != 0\n            left = max(horizontal.nonzero().min().item() - 1, 0)\n            top = max(vertical.nonzero().min().item() - 1, 0)\n            right = min(horizontal.nonzero().max().item() + 1, m.shape[1])\n            bottom = min(vertical.nonzero().max().item() + 1, m.shape[0])\n            bboxes.append([left, top, right, bottom])\n    return bboxes\n\n\n\n", "CodeBleu_score": 24.53022146975365}, {"code": "def extract_bboxes(mask)\n\t\n    boxes = np.zeros([mask.shape[-1], 4], dtype=np.int32)\n    for m in range(mask.shape[-1]):\n        # If there are true values in mask tensor, save the coordinates of the box\n        msk = mask[:, :, m]\n        horizontal_indicies = np.where(np.any(msk, axis=0))[0]\n        vertical_indicies = np.where(np.any(msk, axis=1))[0]\n        if horizontal_indicies.shape[0]:\n            x1, x2 = horizontal_indicies[[0, -1]]\n            y1, y2 = vertical_indicies[[0, -1]]\n            # x2 and y2 should not be part of the box. Increment by 1.\n            x2 += 1\n            y2 += 1\n        else:\n            # No mask for this instance. Might happen due to\n            # resizing or cropping. Set bbox to zeros\n            x1, x2, y1, y2 = 0, 0, 0, 0\n        boxes[m] = np.array([x1, y1, x2, y2])\n    return boxes.astype(np.int32)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 69.46097208408891}]}
{"instance_id": "7ad5d430d6ed", "generation_result": [{"code": "def compute_iou(box, boxes, box_area, boxes_area)\n\t\n    y1 = np.maximum(box[0], boxes[:, 0])\n    y2 = np.minimum(box[2], boxes[:, 2])\n    x1 = np.maximum(box[1], boxes[:, 1])\n    x2 = np.minimum(box[3], boxes[:, 3])\n    intersection = np.maximum(x2 - x1, 0) * np.maximum(y2 - y1, 0)\n    union = box_area + boxes_area[:] - intersection[:]\n    iou = intersection / union\n    return iou\n\n", "CodeBleu_score": 58.663259083089194}, {"code": "def compute_iou(box, boxes, box_area, boxes_area)\n\t\n    y1 = np.maximum(box[0], boxes[:, 0])\n    y2 = np.minimum(box[2], boxes[:, 2])\n    x1 = np.maximum(box[1], boxes[:, 1])\n    x2 = np.minimum(box[3], boxes[:, 3])\n    intersection = np.maximum(x2 - x1, 0) * np.maximum(y2 - y1, 0)\n    union = box_area + boxes_area[:] - intersection[:]\n    iou = intersection / union\n    return iou\n\n\n\n\n", "CodeBleu_score": 58.663259083089194}, {"code": "def compute_iou(box, boxes, box_area, boxes_area)\n\t\n    y1 = np.maximum(box[0], boxes[:, 0])\n    y2 = np.minimum(box[2], boxes[:, 2])\n    x1 = np.maximum(box[1], boxes[:, 1])\n    x2 = np.minimum(box[3], boxes[:, 3])\n    intersection = np.maximum(x2 - x1, 0) * np.maximum(y2 - y1, 0)\n    union = box_area + boxes_area[:] - intersection[:]\n    iou = intersection / union\n    return iou\n\n", "CodeBleu_score": 58.663259083089194}]}
{"instance_id": "15941b852859", "generation_result": [{"code": "def compute_overlaps(boxes1, boxes2)\n\t\n    # compute overlaps\n    # generate box areas\n    area1 = (boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1])\n    area2 = (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1])\n\n    # initialize overlap matrix\n    overlaps = np.zeros((boxes1.shape[0], boxes2.shape[0]))\n\n    # compute overlaps\n    for i in range(overlaps.shape[1]):\n        box2 = boxes2[i]\n        overlaps[:, i] = compute_iou(box2, boxes1, area2[i], area1)\n    return overlaps\n\n", "CodeBleu_score": 62.2853282032944}, {"code": "def compute_overlaps(boxes1, boxes2)\n\t\n    # compute overlaps\n    # generate box areas\n    area1 = (boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1])\n    area2 = (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1])\n\n    # initialize overlap matrix\n    overlaps = np.zeros((boxes1.shape[0], boxes2.shape[0]))\n\n    # compute overlap between boxes\n    for i in range(overlaps.shape[1]):\n        box2 = boxes2[i]\n        overlaps[:, i] = compute_iou(box2, boxes1, area2[i], area1)\n    return overlaps\n\n", "CodeBleu_score": 62.2853282032944}, {"code": "def compute_overlaps(boxes1, boxes2)\n\t\n    # compute overlaps\n    # generate boxes areas\n    area1 = (boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1])\n    area2 = (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1])\n\n    # initialize overlap matrix\n    overlaps = np.zeros((boxes1.shape[0], boxes2.shape[0]))\n\n    # iterate through each box in the second set\n    for i in range(boxes2.shape[0]):\n        overlaps[:, i] = compute_iou(boxes2[i], boxes1, area2[i], area1)\n\n    return overlaps\n\n", "CodeBleu_score": 55.70923231107078}]}
{"instance_id": "597533722ce3", "generation_result": [{"code": "def load_image(self, image_id):\n\t\n    image_path = self.image_info[image_id][\"path\"]\n    image = Image.open(image_path)\n    image = image.convert(\"RGB\")\n    image = np.asarray(image)\n    if image.shape[-1] == 4:\n        image = image[..., :-1]\n    return image\n\n\n\n\n\n", "CodeBleu_score": 58.72833925284871}, {"code": "def load_image(self, image_id):\n\t\n    image_info = self.image_info[image_id]\n    image = imageio.imread(image_info['path'])\n    if image.shape[-1] == 4:\n        image = image[..., :-1]\n    if image.shape[-1] == 1:\n        image = image[..., -1]\n    return image\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 56.188090456858866}, {"code": "def load_image(self, image_id):\n\t\n    info = self.image_info[image_id]\n    image = cv2.imread(info['path'])\n    if image.shape[-1] == 4:\n        image = image[:, :, :3]\n    if image.shape[-1] != 3:\n        image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n    return image\n", "CodeBleu_score": 59.502266340293154}]}
{"instance_id": "9b852997ca05", "generation_result": [{"code": "def display_images(images, titles=None, cols=4, cmap=None, norm=None,\n                   interpolation=None)\n\t\n    import matplotlib.pyplot as plt\n    import math\n\n    # Set default titles if none are provided\n    if titles is None:\n        titles = ['Image (%d)' % i for i in range(1, len(images) + 1)]\n\n    # Calculate the number of rows needed based on the number of images and columns\n    n_images = len(images)\n    if titles is not None:\n        assert len(images) == len(titles)\n    n_cols = cols\n    n_rows = math.ceil(n_images / n_cols)\n\n    # Create a figure\n    plt.figure()\n\n    # Iterate over the images and titles, displaying each image in a subplot with its corresponding title\n    for n, (image, title) in enumerate(zip(images, titles)):\n        plt.subplot(n_rows, n_cols, n + 1)\n        plt.imshow(image, cmap=cmap, norm=norm, interpolation=interpolation)\n        plt.title(title)\n        plt.xticks([])\n        plt.yticks([])\n\n    # Show the figure\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 42.22224351605176}, {"code": "def display_images(images, titles=None, cols=4, cmap=None, norm=None,\n                   interpolation=None)\n\t\n    import math\n    import matplotlib.pyplot as plt\n    assert ((titles is None) or (len(images) == len(titles)))\n    n_images = len(images)\n    if titles is None: titles = ['Image (%d)' % i for i in range(1,n_images + 1)]\n    fig = plt.figure()\n    for n, (image, title) in enumerate(zip(images, titles)):\n        a = fig.add_subplot(cols, math.ceil(n_images/float(cols)), n + 1)\n        if image.ndim == 2:\n            plt.gray()\n        plt.imshow(image, cmap = cmap, norm = norm, interpolation = interpolation)\n        a.set_title(title)\n    fig.set_size_inches(np.array(fig.get_size_inches()) * n_images)\n    plt.show()\n\n", "CodeBleu_score": 36.51832537723916}, {"code": "def display_images(images, titles=None, cols=4, cmap=None, norm=None,\n                   interpolation=None)\n\t\n    assert((titles is None)or (len(images) == len(titles)))\n    n_images = len(images)\n    if titles is None: titles = ['Image (%d)' % i for i in range(1,n_images + 1)]\n    fig = plt.figure()\n    for n, (image, title) in enumerate(zip(images, titles)):\n        a = fig.add_subplot(cols, np.ceil(n_images/float(cols)), n + 1)\n        if image.ndim == 2:\n            plt.gray()\n        plt.imshow(image, cmap = cmap, norm = norm, interpolation = interpolation)\n        a.set_title(title)\n    fig.set_size_inches(np.array(fig.get_size_inches()) * n_images)\n    plt.show()\n\n\n\n", "CodeBleu_score": 36.072204304114905}]}
