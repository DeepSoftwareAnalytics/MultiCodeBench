{"instance_id": "1f9f4d85e825", "generation_result": [{"code": "def load_model_from_config(self, half_attention)\n\t\n    if self.model is None:\n        if self.model_cache_path is not None and os.path.exists(self.model_cache_path):\n            self.model = torch.load(self.model_cache_path, map_location=\"cpu\")\n        else:\n            if self.model_path.endswith(\".safetensors\"):\n                self.model = load_model_from_file(self.model_path, device=\"cpu\")\n            else:\n                self.model = torch.load(self.model_path, map_location=\"cpu\")\n\n        if self.model_config_path is not None and os.path.exists(self.model_config_path):\n            with open(self.model_config_path, \"r\") as f:\n                config = yaml.safe_load(f)\n            if config[\"model_type\"] == \"vae\":\n                self.model.vae = AutoencoderKL.from_pretrained(self.model_path, subfolder=\"vae\")\n                self.model.vae.to(self.device)\n                self.model.vae.eval()\n                self.model.vae.requires_grad_(False)\n            self.model.to(self.device)\n            self.model.eval()\n            if half_attention:\n                self.model.half()\n            if self.optimize_memory:\n                self.model.set_memory_format(torch.channels_last)\n            if self.model_cache_path is not None:\n                torch.save(self.model, self.model_cache_path)\n    return {\"model\": self.model}\n\n", "CodeBleu_score": 35.27906394222949}, {"code": "def load_model_from_config(self, half_attention)\n\t\n    if self.config_file is None:\n        raise ValueError(\"You need to specify a config file.\")\n\n    if self.cache_dir is None:\n        self.cache_dir = os.path.join(os.path.expanduser(\"~\"), \".cache\", \"llama.cpp\")\n    os.makedirs(self.cache_dir, exist_ok=True)\n\n    cache_path = os.path.join(self.cache_dir, os.path.basename(self.config_file))\n    if os.path.exists(cache_path):\n        try:\n            model = torch.jit.load(cache_path)\n            return {\"model\": model}\n        except Exception as e:\n            print(f\"Invalid cache file {cache_path}, re-loading model from config file: {e}\")\n\n    with open(self.config_file, \"r\") as f:\n        config = yaml.safe_load(f)\n\n    if \"path\" not in config:\n        raise ValueError(\"You need to specify a 'path' in the config file.\")\n\n    model_path = config[\"path\"]\n\n    if not os.path.exists(model_path):\n        raise ValueError(f\"Model path {model_path} does not exist.\")\n\n    if model_path.endswith(\".safetensors\"):\n        model = load_model_from_safetensors(model_path)\n    else:\n        model = torch.jit.load(model_path)\n\n    if half_attention:\n        model.half_attention()\n\n    if self.memory_format is not None:\n        model.set_memory_format(self.memory_format)\n\n    device = torch.device(self.device)\n    model.to(device)\n    model.eval()\n\n    if self.cache_dir is not None:\n        try:\n            torch.jit.save(model, cache_path)\n        except Exception as e:\n            print(f\"Failed to cache model to {cache_path}: {e}\")\n\n    return {\"model\": model}\n\n", "CodeBleu_score": 39.106824260751424}, {"code": "def load_model_from_config(self, half_attention)\n\t\n    if self.config.cache_dir is not None:\n        cached_model_path = os.path.join(self.config.cache_dir, \"cached_model.safetensors\")\n        if os.path.exists(cached_model_path):\n            try:\n                model = torch.load(cached_model_path)\n                if model.device!= self.device:\n                    model = model.to(self.device)\n                if model.dtype!= torch.float16:\n                    model = model.half()\n                if self.config.memory_format == \"channels_last\":\n                    model = model.to(memory_format=torch.channels_last)\n                if self.config.half_attention:\n                    model.set_half_precision_attention()\n                return {\"model\": model}\n            except Exception:\n                pass\n    model = load_model(self.config.model_path, self.device, self.config.memory_format, self.config.half_attention)\n    if self.config.cache_dir is not None:\n        torch.save(model, cached_model_path)\n    return {\"model\": model}\n\n", "CodeBleu_score": 39.42578492964973}]}
{"instance_id": "8dc955007aa2", "generation_result": [{"code": "def convsample_ddim(model, cond, steps, shape, eta=1.0, callback=None, normals_sequence=None,\n                    mask=None, x0=None, quantize_x0=False, temperature=1., score_corrector=None,\n                    corrector_kwargs=None, x_t=None\n                    )\n\t\n    if x0 is None:\n        x0 = torch.randn(shape)\n    if quantize_x0:\n        x0 = x0.round()\n    if x_t is None:\n        x_t = x0\n    if normals_sequence is None:\n        normals_sequence = [torch.randn(shape) for _ in range(steps)]\n    if score_corrector is None:\n        score_corrector = lambda x, t, _: 0.\n    if corrector_kwargs is None:\n        corrector_kwargs = {}\n    sampler = DDIMSampler(model)\n    return sampler.sample(\n        steps, batch_size=shape[0], shape=shape[1:], cond=cond, eta=eta,\n        callback=callback, normals_sequence=normals_sequence, mask=mask, x0=x0,\n        quantize_x0=quantize_x0, temperature=temperature, score_corrector=score_corrector,\n        corrector_kwargs=corrector_kwargs, x_t=x_t\n    )\n\n", "CodeBleu_score": 45.6904979348515}, {"code": "def convsample_ddim(model, cond, steps, shape, eta=1.0, callback=None, normals_sequence=None,\n                    mask=None, x0=None, quantize_x0=False, temperature=1., score_corrector=None,\n                    corrector_kwargs=None, x_t=None\n                    )\n\t\n    if x_t is None:\n        x_t = model.sample_x_0(shape)\n    if quantize_x0:\n        x_t = model.quantize(x_t)\n    if x0 is not None:\n        x_t = x0\n    sampler = DDIMSampler(model)\n    return sampler.sample(\n        steps,\n        batch_size=shape[0],\n        shape=shape[1:],\n        cond=cond,\n        eta=eta,\n        callback=callback,\n        normals_sequence=normals_sequence,\n        mask=mask,\n        x_t=x_t,\n        temperature=temperature,\n        score_corrector=score_corrector,\n        corrector_kwargs=corrector_kwargs,\n    )\n\n", "CodeBleu_score": 43.815126862778854}, {"code": "def convsample_ddim(model, cond, steps, shape, eta=1.0, callback=None, normals_sequence=None,\n                    mask=None, x0=None, quantize_x0=False, temperature=1., score_corrector=None,\n                    corrector_kwargs=None, x_t=None\n                    )\n\t\n    sampler = DDIMSampler(model)\n    batch_size = shape[0]\n    print('eta', eta,'steps', steps)\n    return sampler.sample(\n        steps=steps,\n        batch_size=batch_size,\n        shape=shape,\n        cond=cond,\n        eta=eta,\n        callback=callback,\n        normals_sequence=normals_sequence,\n        mask=mask,\n        x0=x0,\n        quantize_x0=quantize_x0,\n        temperature=temperature,\n        score_corrector=score_corrector,\n        corrector_kwargs=corrector_kwargs,\n        x_t=x_t,\n    )\n\n", "CodeBleu_score": 38.1995501801142}]}
{"instance_id": "9bdcd9081480", "generation_result": [{"code": "def ema_scope(self, context=None)\n\t\n    if not self.ema_enabled:\n        return\n    if context is None:\n        context = self.context\n    if self.ema_scope_enabled:\n        return\n    self.ema_scope_enabled = True\n    if context is not None:\n        context.print(\"Switching to EMA weights.\")\n    self.set_weights(self.get_ema_weights())\n    if context is not None:\n        context.print(\"Restored original weights.\")\n    self.ema_scope_enabled = False\n\n", "CodeBleu_score": 42.412733741931355}, {"code": "def ema_scope(self, context=None)\n\t\n    if self.ema_decay is not None:\n        if context is not None:\n            print(\"Switching to EMA weights\")\n        self.model.set_weights(self.ema.get_weights())\n        if context is not None:\n            print(\"Restored original weights\")\n\n", "CodeBleu_score": 34.714953834302285}, {"code": "def ema_scope(self, context=None)\n\t\n    if self.ema_weights is not None:\n        if context is not None:\n            context.log_info('Switching to EMA weights')\n        self.set_weights(self.ema_weights)\n        if context is not None:\n            context.log_info('Switched to EMA weights')\n    try:\n        yield\n    finally:\n        if self.ema_weights is not None:\n            if context is not None:\n                context.log_info('Restoring original weights')\n            self.set_weights(self.weights)\n            if context is not None:\n                context.log_info('Restored original weights')\n\n", "CodeBleu_score": 48.95598966347484}]}
{"instance_id": "14ab009e1c5c", "generation_result": [{"code": "def init_from_ckpt(self, path, ignore_keys=None)\n\t\n    if ignore_keys is None:\n        ignore_keys = []\n    # Load the state dictionary from the checkpoint\n    checkpoint = torch.load(path)\n    # Get the state dictionary from the checkpoint\n    state_dict = checkpoint[\"state_dict\"]\n    # Delete any keys that match the ignore_keys list\n    for key in list(state_dict.keys()):\n        if any(ignore_key in key for ignore_key in ignore_keys):\n            del state_dict[key]\n    # Update the model's state with the loaded state dictionary\n    self.load_state_dict(state_dict, strict=False)\n    # Print out any missing or unexpected keys\n    if hasattr(self, \"missing_keys\"):\n        print(\"Missing keys:\", self.missing_keys)\n    if hasattr(self, \"unexpected_keys\"):\n        print(\"Unexpected keys:\", self.unexpected_keys)\n\n", "CodeBleu_score": 32.79622883116835}, {"code": "def init_from_ckpt(self, path, ignore_keys=None)\n\t\n    if ignore_keys is None:\n        ignore_keys = []\n    state_dict = torch.load(path, map_location=torch.device(\"cpu\"))\n    state_dict = {k: v for k, v in state_dict.items() if k not in ignore_keys}\n    self.load_state_dict(state_dict, strict=False)\n    missing_keys = set(self.state_dict().keys()) - set(state_dict.keys())\n    unexpected_keys = set(state_dict.keys()) - set(self.state_dict().keys())\n    if len(missing_keys) > 0:\n        print(\"Missing keys: {}\".format(missing_keys))\n    if len(unexpected_keys) > 0:\n        print(\"Unexpected keys: {}\".format(unexpected_keys))\n\n", "CodeBleu_score": 40.772109698459175}, {"code": "def init_from_ckpt(self, path, ignore_keys=None)\n\t\n    state_dict = torch.load(path, map_location=torch.device('cpu'))\n    if ignore_keys is not None:\n        for key in ignore_keys:\n            if key in state_dict:\n                del state_dict[key]\n    self.load_state_dict(state_dict, strict=False)\n    missing_keys, unexpected_keys = self.state_dict(), state_dict\n    print('Missing keys:', missing_keys)\n    print('Unexpected keys:', unexpected_keys)\n\n", "CodeBleu_score": 26.451845393577827}]}
{"instance_id": "754176e27b81", "generation_result": [{"code": "def forward(self, input, return_pred_indices=False)\n\t\n    quant, emb_loss, info = self.encode(input)\n    if return_pred_indices:\n        quant = self.quantize.sample(quant)\n    output = self.decode(quant)\n    diff = (input - output).abs().mean()\n    return output, diff, emb_loss, info\n", "CodeBleu_score": 24.637812280764887}, {"code": "def forward(self, input, return_pred_indices=False)\n\t\n    quant, emb_loss, info = self.encode(input)\n    pred = self.decode(quant)\n    diff = torch.abs(input - pred)\n    if return_pred_indices:\n        return pred, diff, info['pred_indices']\n    else:\n        return pred, diff\n", "CodeBleu_score": 27.644709775047655}, {"code": "def forward(self, input, return_pred_indices=False)\n\t\n    if self.training:\n        with torch.no_grad():\n            quant, emb_loss, info = self.encode(input)\n            pred = self.decode(quant)\n            diff = (input - pred).abs().sum(dim=1)\n            if return_pred_indices:\n                return pred, diff, info[\"pred_indices\"]\n            else:\n                return pred, diff\n    else:\n        quant, emb_loss, info = self.encode(input)\n        pred = self.decode(quant)\n        diff = (input - pred).abs().sum(dim=1)\n        if return_pred_indices:\n            return pred, diff, info[\"pred_indices\"]\n        else:\n            return pred, diff\n\n", "CodeBleu_score": 36.30417306514449}]}
{"instance_id": "443df6fdaf0d", "generation_result": [{"code": "def get_input(self, batch, k)\n\t\n    input = batch[k]\n    if input.dim() == 2:\n        input = input.unsqueeze(0)\n    input = input.permute(0, 3, 1, 2)\n    if self.resize_range:\n        input = F.interpolate(input, scale_factor=self.resize_range[0] + self.resize_range[1] * (self.step - 1) / self.num_steps, mode='bicubic', align_corners=True)\n    input = input.to(self.device, dtype=torch.float)\n    input = input.detach()\n    return input\n\n", "CodeBleu_score": 33.86773927094373}, {"code": "def get_input(self, batch, k)\n\t\n    if k not in batch:\n        raise ValueError(f\"Key {k} not in batch\")\n    x = batch[k]\n    if x.dim() == 2:\n        x = x.unsqueeze(1)\n    if x.dim()!= 4:\n        raise ValueError(f\"Expected 4D tensor, got {x.dim()}D\")\n    x = x.permute(0, 3, 1, 2)\n    if self.resize:\n        step = self.steps[self.step]\n        x = F.interpolate(x, size=step, mode=\"bicubic\", align_corners=False)\n    x = x.contiguous().float()\n    x = x.detach()\n    return x\n\n", "CodeBleu_score": 39.799572148763254}, {"code": "def get_input(self, batch, k)\n\t\n    input_data = batch[k]\n    if input_data.dim() == 2:\n        input_data = input_data.unsqueeze(0)\n    if input_data.dim() == 3:\n        input_data = input_data.unsqueeze(1)\n    input_data = input_data.permute(0, 1, 3, 2)\n    input_data = input_data.contiguous().float()\n    if self.batch_resizing:\n        input_data = F.interpolate(input_data, size=self.batch_resizing_range[self.batch_resizing_step], mode='bicubic', align_corners=False)\n    input_data = input_data.detach()\n    return input_data\n\n", "CodeBleu_score": 32.05136376179269}]}
{"instance_id": "953a2632fc15", "generation_result": [{"code": "def training_step(self, batch, batch_idx, optimizer_idx)\n\t        # get input data\n        x = self.get_input(batch, \"x\")\n        if self.use_x_pred:\n            x_pred = self.get_input(batch, \"x_pred\")\n        else:\n            x_pred = None\n        # get predictions\n        if self.use_x_pred:\n            pred = self.model(x, x_pred)\n        else:\n            pred = self.model(x)\n        # compute losses\n        if optimizer_idx == 0:\n            # autoencoding loss\n            loss = self.loss_fn(x, pred)\n        else:\n            # discriminator loss\n            loss = self.loss_fn(x, x_pred, pred)\n        # log metrics\n        self.log(\"train_loss\", loss, prog_bar=True)\n        if optimizer_idx == 0:\n            self.log(\"train_psnr\", self.psnr(x, pred), prog_bar=True)\n        return loss", "CodeBleu_score": 31.743259570187004}, {"code": "def training_step(self, batch, batch_idx, optimizer_idx)\n\t        x = self.get_input(batch, \"image\")\n        predictions = self.forward(x)\n        if optimizer_idx == 0:\n            loss = self.loss_function(predictions, x)\n            self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n            return loss\n        elif optimizer_idx == 1:\n            loss = self.discriminator_loss(predictions)\n            self.log(\"train_loss_discriminator\", loss, on_step=True, on_epoch=True, prog_bar=True)\n            return loss", "CodeBleu_score": 28.486288247899346}, {"code": "def training_step(self, batch, batch_idx, optimizer_idx)\n\t        x = self.get_input(batch, 0)\n        x_hat = self.forward(x)\n        if optimizer_idx == 0:\n            # Autoencoder loss\n            loss = self.loss_fn(x_hat, x)\n            self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n            return loss\n        elif optimizer_idx == 1:\n            # Discriminator loss\n            x_hat = self.get_input(batch, 1)\n            x_hat = self.forward(x_hat)\n            x = self.get_input(batch, 2)\n            x = self.forward(x)\n            loss = self.discriminator_loss_fn(x_hat, x)\n            self.log(\"train_disc_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n            return loss", "CodeBleu_score": 33.92879751092744}]}
{"instance_id": "b449b5e8b77f", "generation_result": [{"code": "def log_images(self, batch, only_inputs=False, plot_ema=False, **kwargs)\n\t        # process input data\n        x = self.get_input(batch, self.image_key)\n        if self.image_key == \"segmentation\":\n            x = self.to_rgb(x)\n        x = self.process_input(x)\n        # log inputs\n        if not only_inputs:\n            # log reconstructions\n            with torch.no_grad():\n                if plot_ema:\n                    # use EMA weights\n                    self.eval_ema()\n                    x_recon = self.forward(x, **kwargs)\n                    self.train()\n                else:\n                    x_recon = self.forward(x, **kwargs)\n            x_recon = self.process_output(x_recon)\n            if self.image_key == \"segmentation\":\n                x_recon = self.to_rgb(x_recon)\n            self.logger.log_image(x, x_recon, step=self.global_step, **kwargs)\n        else:\n            self.logger.log_image(x, step=self.global_step, **kwargs)", "CodeBleu_score": 32.82579701283538}, {"code": "def log_images(self, batch, only_inputs=False, plot_ema=False, **kwargs)\n\t        if not only_inputs:\n            if plot_ema:\n                self.log_ema_reconstructions(batch, **kwargs)\n            else:\n                self.log_reconstructions(batch, **kwargs)\n        if self.image_key == \"segmentation\":\n            self.log_segmentation(batch, **kwargs)\n        elif self.image_key == \"rgb\":\n            self.log_rgb(batch, **kwargs)\n        elif self.image_key == \"gray\":\n            self.log_gray(batch, **kwargs)\n        elif self.image_key == \"depth\":\n            self.log_depth(batch, **kwargs)\n        elif self.image_key == \"inpainting\":\n            self.log_inpainting(batch, **kwargs)\n        elif self.image_key == \"super_resolution\":\n            self.log_super_resolution(batch, **kwargs)\n        elif self.image_key == \"text_to_image\":\n            self.log_text_to_image(batch, **kwargs)\n        else:\n            self.log_inputs(batch, **kwargs)", "CodeBleu_score": 17.942161242649096}, {"code": "def log_images(self, batch, only_inputs=False, plot_ema=False, **kwargs)\n\t        if plot_ema:\n            assert self.ema_model is not None, \"You must have an EMA model to log EMA images.\"\n        if self.image_key is None:\n            return\n        if only_inputs:\n            if self.image_key in batch:\n                x = batch[self.image_key]\n            else:\n                x = batch[\"inputs\"]\n        else:\n            x = self.get_input(batch, self.image_key)\n        if self.image_key == \"segmentation\":\n            x = self.to_rgb(x)\n        if plot_ema:\n            with torch.no_grad():\n                ema_x = self.get_input(batch, self.image_key)\n                if self.image_key == \"segmentation\":\n                    ema_x = self.to_rgb(ema_x)\n                ema_x = self.ema_model(ema_x)\n        if self.image_key == \"segmentation\":\n            x = self.to_rgb(x)\n        if plot_ema:\n            with torch.no_grad():\n                ema_x = self.get_input(batch, self.image_key)\n                if self.image_key == \"segmentation\":\n                    ema_x = self.to_rgb(ema_x)\n                ema_x = self.ema_model(ema_x)\n        if self.image_key == \"segmentation\":\n            x = self.to_rgb(x)\n        if plot_ema:\n            with torch.no_grad():\n                ema_x = self.get_input(batch, self.image_key)\n                if self.image_key == \"segmentation\":\n                    ema_x = self.to_rgb(ema_x)\n                ema_x = self.ema_model(ema_x)\n        if self.image_key == \"segmentation\":\n            x = self.to_rgb(x)\n        if plot_ema:\n            with torch.no_grad():\n                ema_x = self.get_input(batch, self.image_key)\n                if self.image_key == \"segmentation\":\n                    ema_x = self.to_rgb(ema_x)\n                ema_x = self.ema_model(ema_x)\n        if self.image_key == \"segmentation\":\n            x = self.", "CodeBleu_score": 33.381267165555506}]}
{"instance_id": "540d089e3481", "generation_result": [{"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss_type == 'l1':\n        loss = F.l1_loss(pred, target, reduction='mean')\n    elif self.loss_type == 'l2':\n        loss = F.mse_loss(pred, target, reduction='mean')\n    else:\n        raise NotImplementedError('loss type {} not implemented.'.format(self.loss_type))\n    return loss\n\n", "CodeBleu_score": 36.74017393593002}, {"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss_type == 'l1':\n        loss = nn.L1Loss()\n        if mean:\n            return loss(pred, target).mean()\n        else:\n            return loss(pred, target)\n    elif self.loss_type == 'l2':\n        loss = nn.MSELoss()\n        if mean:\n            return loss(pred, target).mean()\n        else:\n            return loss(pred, target)\n    else:\n        raise ValueError('Loss type {} not supported'.format(self.loss_type))\n\n", "CodeBleu_score": 43.66707105115649}, {"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss_type == 'l1':\n        loss = nn.L1Loss()\n        if mean:\n            return loss(pred, target).mean()\n        else:\n            return loss(pred, target)\n    elif self.loss_type == 'l2':\n        loss = nn.MSELoss()\n        if mean:\n            return loss(pred, target).mean()\n        else:\n            return loss(pred, target)\n    else:\n        raise ValueError(f\"loss_type must be either 'l1' or 'l2', not {self.loss_type}\")\n\n", "CodeBleu_score": 43.215083118160315}]}
{"instance_id": "63a08283666f", "generation_result": [{"code": "def forward(self, x, c, *args, **kwargs)\n\t\n        if self.training:\n            t = self.get_timesteps(x.shape[0], device=x.device).long()\n        else:\n            t = self.get_timesteps(1, device=x.device).long()\n\n        if self.learn_sigma:\n            sigma = self.sigma_model(x, t)\n            loss_sigma = self.loss_sigma(sigma)\n            if self.training:\n                self.log(f\"train/loss_sigma\", loss_sigma, on_step=True, on_epoch=False)\n            else:\n                self.log(f\"val/loss_sigma\", loss_sigma, on_step=True, on_epoch=False)\n\n        if self.learn_t:\n            t_emb = self.time_emb(t)\n            if self.learn_t_in_cond:\n                c = torch.cat([c, t_emb], dim=1)\n            else:\n                x = torch.cat([x, t_emb.expand(x.shape[0], -1, -1, -1)], dim=1)\n\n        if self.learn_c:\n            c = self.get_learned_conditioning(c)\n\n        if self.conditioning_type == \"concat\":\n            x = torch.cat([x, c.expand(x.shape[0], -1, -1, -1)], dim=1)\n        elif self.conditioning_type == \"add\":\n            x = x + c.expand(x.shape[0], -1, -1, -1)\n        elif self.conditioning_type == \"none\":\n            pass\n        else:\n            raise NotImplementedError(f\"Conditioning type {self.conditioning_type} not yet supported\")\n\n        if self.learn_sigma:\n            loss, loss_dict = self.p_losses(x, t, sigma)\n        else:\n            loss, loss_dict = self.p_losses(x, t)\n\n        if self.training:\n            for k, v in loss_dict.items():\n                self.log(k, v, on_step=True, on_epoch=False)\n        else:\n            for k, v in loss_dict.items():\n                self.log(k, v, on_step=True, on_epoch=False)\n\n        return loss\n", "CodeBleu_score": 38.386058010466996}, {"code": "def forward(self, x, c, *args, **kwargs)\n\t\n        t = self.sample_timesteps(x.shape[0])\n        if self.get_learned_conditioning is not None:\n            c = self.get_learned_conditioning(c)\n        if self.cond_stage_forward is not None:\n            x = self.cond_stage_model(x, c)\n        loss, loss_dict = self.p_losses(x, t, *args, **kwargs)\n        return loss, loss_dict\n", "CodeBleu_score": 21.87625720581381}, {"code": "def forward(self, x, c, *args, **kwargs)\n\t\n        if self.training:\n            t = self.get_timesteps(x.shape[0], device=x.device)\n        else:\n            t = self.get_timesteps(1, device=x.device)\n\n        if self.get_learned_conditioning is not None:\n            c = self.get_learned_conditioning(c)\n\n        x = self.p_losses(x, t, c=c, *args, **kwargs)\n\n        return x\n", "CodeBleu_score": 36.467182403345866}]}
{"instance_id": "4627e1e8ed62", "generation_result": [{"code": "def get_codebook_entry(self, indices, shape)\n\t\n    indices = indices.view(-1)\n    if self.use_ema:\n        latent = self.ema_codebook.get_codebook_entry(indices)\n    else:\n        latent = self.codebook.get_codebook_entry(indices)\n    latent = latent.view(shape)\n    if self.permutations is not None:\n        latent = latent.permute(self.permutations)\n    return latent\n", "CodeBleu_score": 35.817635651005105}, {"code": "def get_codebook_entry(self, indices, shape)\n\t\n    indices = indices.reshape(shape)\n    if self.re_embed > self.used.shape[0]:  # extra token\n        indices[indices >= self.used.shape[0]] = 0  # simply set to zero\n    if self.unmap_to_all:\n        indices = self.unmap_to_all(indices)\n    return self.used[indices].reshape(shape)\n\n", "CodeBleu_score": 32.87747570491006}, {"code": "def get_codebook_entry(self, indices, shape)\n\t\n    indices = self.unmap_to_all(indices)\n    return self.quantize.embed_code(indices).permute(0, 2, 1).reshape(shape)\n\n", "CodeBleu_score": 8.790879569369313}]}
{"instance_id": "c55d014827ef", "generation_result": [{"code": "def factorization(dimension: int, factor:int=-1) -> tuple[int, int]\n\t\n    if factor > 0:\n        if dimension % factor == 0:\n            return (factor, dimension // factor)\n    else:\n        for i in range(int(sqrt(dimension)), 0, -1):\n            if dimension % i == 0:\n                return (i, dimension // i)\n    return (dimension, 1)\n\n", "CodeBleu_score": 17.36345678597515}, {"code": "def factorization(dimension: int, factor:int=-1) -> tuple[int, int]\n\t\n    if factor > 0 and dimension % factor == 0:\n        return factor, dimension // factor\n    elif factor < 0:\n        factor = -factor\n    m = 1\n    n = dimension\n    while m <= n and m <= factor:\n        if dimension % m == 0:\n            n = dimension // m\n            if n <= m:\n                return m, n\n        m += 1\n    return m - 1, n\n\n", "CodeBleu_score": 38.5798486576397}, {"code": "def factorization(dimension: int, factor:int=-1) -> tuple[int, int]\n\t\n    if factor > 0 and dimension % factor == 0:\n        return factor, dimension // factor\n    if factor < 0:\n        factor = -factor\n    for m in range(1, factor + 1):\n        if dimension % m == 0:\n            n = dimension // m\n            if m <= n:\n                return m, n\n    return 1, dimension\n\n", "CodeBleu_score": 33.560657163594485}]}
{"instance_id": "5d613ec26036", "generation_result": [{"code": "def apply_weight_decompose(self, updown, orig_weight)\n\t\n    if updown == \"up\":\n        weight = orig_weight + self.weight\n    else:\n        weight = orig_weight - self.weight\n    weight = weight.div(weight.norm(p=2, dim=1, keepdim=True).expand_as(weight) + 1e-12)\n    weight = weight * self.scale\n    return weight\n\n", "CodeBleu_score": 16.420859118415866}, {"code": "def apply_weight_decompose(self, updown, orig_weight)\n\t\n    if updown == 'up':\n        if self.weight_decompose_type == 'add':\n            new_weight = orig_weight + self.weight_decompose_scale * self.weight_decompose_tensor\n        elif self.weight_decompose_type =='mul':\n            new_weight = orig_weight * self.weight_decompose_scale * self.weight_decompose_tensor\n        else:\n            raise NotImplementedError\n        new_weight = new_weight / torch.norm(new_weight)\n        new_weight = self.weight_decompose_scale * new_weight\n        return new_weight\n    elif updown == 'down':\n        if self.weight_decompose_type == 'add':\n            new_weight = orig_weight - self.weight_decompose_scale * self.weight_decompose_tensor\n        elif self.weight_decompose_type =='mul':\n            new_weight = orig_weight / (self.weight_decompose_scale * self.weight_decompose_tensor)\n        else:\n            raise NotImplementedError\n        return new_weight\n    else:\n        raise NotImplementedError\n\n", "CodeBleu_score": 17.559909550443425}, {"code": "def apply_weight_decompose(self, updown, orig_weight)\n\t\n    if updown == \"up\":\n        if self.weight_decompose:\n            combined_weight = orig_weight + self.weight_decompose_scale * self.weight_decompose_tensor\n            combined_weight = combined_weight / combined_weight.norm()\n            combined_weight = combined_weight * self.weight_decompose_scale\n            weight = combined_weight - orig_weight\n        else:\n            weight = self.weight_decompose_scale * self.weight_decompose_tensor\n    else:\n        weight = orig_weight\n    return weight\n\n", "CodeBleu_score": 21.206652532361165}]}
{"instance_id": "c73d0d1cd3e5", "generation_result": [{"code": "def finalize_updown(self, updown, orig_weight, output_shape, ex_bias=None)\n\t if ex_bias is not None:\n        updown += ex_bias\n    updown = updown.reshape(output_shape)\n    if updown.numel() == orig_weight.numel():\n        updown = updown.reshape(orig_weight.shape)\n    if ex_bias is not None:\n        ex_bias = ex_bias * self.multiplier\n    if self.scale is not None:\n        updown = self.apply_weight_decompose(updown, orig_weight)\n    return updown, ex_bias\n", "CodeBleu_score": 48.49044164941797}, {"code": "def finalize_updown(self, updown, orig_weight, output_shape, ex_bias=None)\n\t updown = updown + ex_bias\n    updown = updown.reshape(output_shape)\n    if updown.numel() == orig_weight.numel():\n        updown = updown.reshape(orig_weight.shape)\n    if ex_bias is not None:\n        ex_bias = ex_bias * self.multiplier\n    if self.scale is not None:\n        updown = self.apply_weight_decompose(updown, orig_weight)\n    return updown, ex_bias\n", "CodeBleu_score": 46.21513744772067}, {"code": "def finalize_updown(self, updown, orig_weight, output_shape, ex_bias=None)\n\t if ex_bias is not None:\n        updown = updown + ex_bias\n    updown = updown.reshape(output_shape)\n\n    if updown.numel() == orig_weight.numel():\n        updown = updown.reshape(orig_weight.shape)\n\n    if ex_bias is not None:\n        ex_bias = ex_bias * self.multiplier\n\n    if self.scale is not None:\n        updown = self.apply_weight_decompose(updown, orig_weight)\n\n    return updown, ex_bias\n", "CodeBleu_score": 50.08841100474385}]}
{"instance_id": "c6a41665c62c", "generation_result": [{"code": "def calc_updown(self, orig_weight)\n\t\n    if self.bias is not None:\n        ex_bias = self.bias.to(orig_weight.device, dtype=self.bias.dtype)\n    else:\n        ex_bias = None\n    if self.dora_scale is not None:\n        orig_weight = self.apply_weight_decompose(orig_weight, orig_weight)\n    updown = self.calc_updown(orig_weight)\n    return self.finalize_updown(updown, orig_weight, orig_weight.shape, ex_bias=ex_bias)\n\n", "CodeBleu_score": 47.76717135032862}, {"code": "def calc_updown(self, orig_weight)\n\t\n    updown = self.calc_updown_core(orig_weight)\n    if self.bias is not None:\n        ex_bias = self.bias.to(orig_weight.device, dtype=updown.dtype)\n    else:\n        ex_bias = None\n    updown, ex_bias = self.finalize_updown(updown, orig_weight, orig_weight.shape, ex_bias)\n    return updown, ex_bias\n\n", "CodeBleu_score": 47.30001690148262}, {"code": "def calc_updown(self, orig_weight)\n\t\n    weight = self.weight.to(orig_weight.device)\n    if self.bias is not None:\n        bias = self.bias.to(orig_weight.device)\n    else:\n        bias = None\n    output_shape = weight.shape\n    if len(output_shape) == 4:\n        weight = weight.reshape(-1, weight.shape[3])\n    updown = self.calc_updown(weight, orig_weight)\n    return self.finalize_updown(updown, orig_weight, output_shape, bias)\n\n", "CodeBleu_score": 53.68298833437301}]}
{"instance_id": "1bb9e5d94af8", "generation_result": [{"code": "def purge_networks_from_memory()\n\t\n    if len(networks) > network_memory_limit:\n        while len(networks) > network_memory_limit:\n            del networks[0]\n        gc.collect()\n\n", "CodeBleu_score": 27.730766557442255}, {"code": "def purge_networks_from_memory()\n\t\n    global networks_in_memory\n    if len(networks_in_memory) > network_count_limit:\n        while len(networks_in_memory) > network_count_limit:\n            networks_in_memory.popitem(last=False)\n        gc.collect()\n\n", "CodeBleu_score": 36.60114632541769}, {"code": "def purge_networks_from_memory()\n\t\n    global networks\n    if len(networks) > NETWORKS_MEMORY_LIMIT:\n        networks = networks[len(networks) - NETWORKS_MEMORY_LIMIT:]\n        gc.collect()\n\n", "CodeBleu_score": 27.855251908644696}]}
{"instance_id": "42b48ebf4d07", "generation_result": [{"code": "def network_restore_weights_from_backup(self: Union[torch.nn.Conv2d, torch.nn.Linear, torch.nn.GroupNorm, torch.nn.LayerNorm, torch.nn.MultiheadAttention]):\n\t\n    if hasattr(self, 'weight_backup') and hasattr(self, 'bias_backup'):\n        if self.weight_backup is not None:\n            self.weight.data = self.weight_backup\n        if self.bias_backup is not None:\n            self.bias.data = self.bias_backup\n        if isinstance(self, torch.nn.MultiheadAttention):\n            if hasattr(self, 'in_proj_weight_backup') and hasattr(self, 'in_proj_bias_backup'):\n                if self.in_proj_weight_backup is not None:\n                    self.in_proj_weight.data = self.in_proj_weight_backup\n                if self.in_proj_bias_backup is not None:\n                    self.in_proj_bias.data = self.in_proj_bias_backup\n            if hasattr(self, 'out_proj_weight_backup') and hasattr(self, 'out_proj_bias_backup'):\n                if self.out_proj_weight_backup is not None:\n                    self.out_proj_weight.data = self.out_proj_weight_backup\n                if self.out_proj_bias_backup is not None:\n                    self.out_proj_bias.data = self.out_proj_bias_backup\n\n", "CodeBleu_score": 35.57907363834536}, {"code": "def network_restore_weights_from_backup(self: Union[torch.nn.Conv2d, torch.nn.Linear, torch.nn.GroupNorm, torch.nn.LayerNorm, torch.nn.MultiheadAttention]):\n\t\n    if self.weight_backup is not None:\n        self.weight.data = self.weight_backup.clone()\n    if self.bias_backup is not None:\n        self.bias.data = self.bias_backup.clone()\n    if isinstance(self, torch.nn.MultiheadAttention):\n        if self.in_proj_weight_backup is not None:\n            self.in_proj_weight.data = self.in_proj_weight_backup.clone()\n        if self.in_proj_bias_backup is not None:\n            self.in_proj_bias.data = self.in_proj_bias_backup.clone()\n\n", "CodeBleu_score": 30.792298298372188}, {"code": "def network_restore_weights_from_backup(self: Union[torch.nn.Conv2d, torch.nn.Linear, torch.nn.GroupNorm, torch.nn.LayerNorm, torch.nn.MultiheadAttention]):\n\t\n    if self.backup_weights is not None and self.backup_biases is not None:\n        if isinstance(self, torch.nn.MultiheadAttention):\n            self.in_proj_weight.data.copy_(self.backup_weights[\"in_proj_weight\"])\n            self.in_proj_bias.data.copy_(self.backup_biases[\"in_proj_bias\"])\n            self.out_proj.weight.data.copy_(self.backup_weights[\"out_proj_weight\"])\n            self.out_proj.bias.data.copy_(self.backup_biases[\"out_proj_bias\"])\n        else:\n            self.weight.data.copy_(self.backup_weights)\n            if self.bias is not None:\n                self.bias.data.copy_(self.backup_biases)\n    else:\n        print(f\"No backup weights or biases found for layer {self}\")\n\n", "CodeBleu_score": 39.134283686939504}]}
{"instance_id": "c8fcd357131a", "generation_result": [{"code": "def network_forward(org_module, input, original_forward)\n\t\n    if not hasattr(org_module, \"networks\"):\n        return original_forward(org_module, input)\n    input = input.to(device=org_module.networks[0].device)\n    org_module.network_restore_weights_from_backup()\n    org_module.network_reset_cached_weight()\n    output = original_forward(org_module, input)\n    for network in org_module.networks:\n        if network.current_layer == org_module.current_layer:\n            output = network.forward(output)\n    return output\n", "CodeBleu_score": 43.7360597573937}, {"code": "def network_forward(org_module, input, original_forward)\n\t\n    if not hasattr(org_module, \"networks\"):\n        return original_forward(org_module, input)\n\n    if not isinstance(input, torch.Tensor):\n        return original_forward(org_module, input)\n\n    input = input.to(device=org_module.weight.device)\n\n    if isinstance(org_module, torch.nn.MultiheadAttention):\n        org_module.network_restore_weights_from_backup()\n        org_module.network_reset_cached_weight()\n        output = original_forward(org_module, input[0], input[1], input[2], input[3], input[4], input[5])\n    else:\n        org_module.network_restore_weights_from_backup()\n        org_module.network_reset_cached_weight()\n        output = original_forward(org_module, input)\n\n    for network in org_module.networks:\n        if network.network_current_names == org_module.network_current_names:\n            if isinstance(org_module, torch.nn.MultiheadAttention):\n                output = network(output[0], output[1], output[2], output[3], output[4], output[5])\n            else:\n                output = network(output)\n\n    return output\n", "CodeBleu_score": 39.74031557927448}, {"code": "def network_forward(org_module, input, original_forward)\n\t\n    if not hasattr(org_module, \"network_forward_called\"):\n        org_module.network_forward_called = False\n    if not org_module.network_forward_called:\n        org_module.network_forward_called = True\n        org_module.network_reset_cached_weight()\n    if not hasattr(org_module, \"network_weights_backup\"):\n        org_module.network_weights_backup = None\n    if not hasattr(org_module, \"network_bias_backup\"):\n        org_module.network_bias_backup = None\n    if not hasattr(org_module, \"network_current_names\"):\n        org_module.network_current_names = ()\n    if not hasattr(org_module, \"network_weights_backup\"):\n        org_module.network_weights_backup = None\n    if not hasattr(org_module, \"network_bias_backup\"):\n        org_module.network_bias_backup = None\n    if not hasattr(org_module, \"network_current_names\"):\n        org_module.network_current_names = ()\n    if not hasattr(org_module, \"network_weights_backup\"):\n        org_module.network_weights_backup = None\n    if not hasattr(org_module, \"network_bias_backup\"):\n        org_module.network_bias_backup = None\n    if not hasattr(org_module, \"network_current_names\"):\n        org_module.network_current_names = ()\n    if not hasattr(org_module, \"network_weights_backup\"):\n        org_module.network_weights_backup = None\n    if not hasattr(org_module, \"network_bias_backup\"):\n        org_module.network_bias_backup = None\n    if not hasattr(org_module, \"network_current_names\"):\n        org_module.network_current_names = ()\n    if not hasattr(org_module, \"network_weights_backup\"):\n        org_module.network_weights_backup = None\n    if not hasattr(org_module, \"network_bias_backup\"):\n        org_module.network_bias_backup = None\n    if not hasattr(org_module, \"network_current_names\"):\n        org_module.network_current_names = ()\n    if not hasattr(org_module, \"network_weights_backup\"):\n        org_module.network_weights_backup", "CodeBleu_score": 14.699702513035195}]}
{"instance_id": "1070211f5c11", "generation_result": [{"code": "def trainables(self)\n\t\n    trainables = []\n    for layer in self.layers:\n        if isinstance(layer, torch.nn.Linear) or isinstance(layer, torch.nn.LayerNorm):\n            trainables.append(layer.weight)\n            trainables.append(layer.bias)\n    return trainables\n\n", "CodeBleu_score": 39.34944059769795}, {"code": "def trainables(self)\n\t\n    trainables = []\n    for layer in self.model:\n        if isinstance(layer, torch.nn.Linear) or isinstance(layer, torch.nn.LayerNorm):\n            trainables.append(layer.weight)\n            trainables.append(layer.bias)\n    return trainables\n\n", "CodeBleu_score": 39.0377101819515}, {"code": "def trainables(self)\n\t\n    trainables = []\n    for layer in self.model.modules():\n        if isinstance(layer, torch.nn.Linear) or isinstance(layer, torch.nn.LayerNorm):\n            trainables.append(layer.weight)\n            trainables.append(layer.bias)\n    return trainables\n\n", "CodeBleu_score": 35.8100357829207}]}
{"instance_id": "0c3ba62c96a7", "generation_result": [{"code": "def load_hypernetworks(names, multipliers=None)\n\t\n    loaded = []\n\n    for name in names:\n        hypernetwork = shared.hypernetworks.get(name, None)\n\n        if hypernetwork is not None:\n            loaded.append(hypernetwork)\n            continue\n\n        hypernetwork = load_hypernetwork(name)\n\n        if hypernetwork is None:\n            continue\n\n        if multipliers is not None:\n            multiplier = multipliers.get(name, None)\n\n            if multiplier is not None:\n                hypernetwork.set_multiplier(multiplier)\n\n        loaded.append(hypernetwork)\n\n    shared.hypernetworks.clear()\n\n    for hypernetwork in loaded:\n        shared.hypernetworks[hypernetwork.name] = hypernetwork\n\n    return loaded\n\n", "CodeBleu_score": 46.1665262225411}, {"code": "def load_hypernetworks(names, multipliers=None)\n\t\n    shared.hypernetworks.clear()\n\n    for name in names:\n        hypernetwork = load_hypernetwork(name)\n\n        if hypernetwork is not None:\n            if multipliers is not None:\n                hypernetwork.set_multiplier(multipliers[name])\n\n            shared.hypernetworks.append(hypernetwork)\n\n", "CodeBleu_score": 25.44051628073918}, {"code": "def load_hypernetworks(names, multipliers=None)\n\t\n    if multipliers is None:\n        multipliers = [1.0] * len(names)\n\n    shared.hypernetworks = {}\n\n    for name, multiplier in zip(names, multipliers):\n        hypernetwork = load_hypernetwork(name)\n\n        if hypernetwork is not None:\n            hypernetwork.set_multiplier(multiplier)\n            shared.hypernetworks[name] = hypernetwork\n", "CodeBleu_score": 27.3222331830624}]}
{"instance_id": "bea3772ed532", "generation_result": [{"code": "\ndef apply_hypernetworks(hypernetworks, context, layer=None)\n\t\n    context_k = context\n    context_v = context\n    for hypernetwork in hypernetworks:\n        context_k, context_v = apply_single_hypernetwork(hypernetwork, context_k, context_v, layer)\n    return context_k, context_v\n\n", "CodeBleu_score": 77.04946886953897}, {"code": "\ndef apply_hypernetworks(hypernetworks, context, layer=None)\n\t\n    context_k, context_v = context, context\n    for hypernetwork in hypernetworks:\n        context_k, context_v = apply_single_hypernetwork(hypernetwork, context_k, context_v, layer=layer)\n    return context_k, context_v", "CodeBleu_score": 57.435688315169486}, {"code": "\ndef apply_hypernetworks(hypernetworks, context, layer=None)\n\t\n    context_k = context\n    context_v = context\n\n    for hypernetwork in hypernetworks:\n        context_k, context_v = apply_single_hypernetwork(hypernetwork, context_k, context_v, layer)\n\n    return context_k, context_v\n\n", "CodeBleu_score": 77.04946886953897}]}
{"instance_id": "922efa3e092f", "generation_result": [{"code": "def attention_CrossAttention_forward(self, x, context=None, mask=None, **kwargs)\n\t\n    batch_size = x.size(0)\n    head_dim = self.head_dim\n    if self.is_decoder:\n        assert context is not None, \"context must be provided for cross-attention\"\n        assert batch_size == context.size(0), \"the batch size of x and context must match\"\n        assert head_dim * self.num_heads == context.size(-1), \"the last dimension of context must be equal to head_dim * num_heads\"\n\n    if self.query_proj is not None:\n        query = self.query_proj(x)\n    else:\n        query = x\n    query = query.view(batch_size, -1, self.num_heads, head_dim).transpose(1, 2)\n    if self.key_proj is not None:\n        key = self.key_proj(x)\n    else:\n        key = x\n    key = key.view(batch_size, -1, self.num_heads, head_dim).transpose(1, 2)\n    if self.value_proj is not None:\n        value = self.value_proj(x)\n    else:\n        value = x\n    value = value.view(batch_size, -1, self.num_heads, head_dim).transpose(1, 2)\n\n    if self.is_decoder:\n        context_k, context_v = apply_hypernetworks(self.hypernetworks, context, layer=self.layer)\n        context_k = context_k.view(batch_size, -1, self.num_heads, head_dim).transpose(1, 2)\n        context_v = context_v.view(batch_size, -1, self.num_heads, head_dim).transpose(1, 2)\n        key = torch.cat((key, context_k), dim=1)\n        value = torch.cat((value, context_v), dim=1)\n\n    scores = torch.matmul(query, key.transpose(-2, -1))\n    if self.scale_attn_by_inverse_layer_idx:\n        scores = scores / math.sqrt(float(self.layer + 1))\n    else:\n        scores = scores / math.sqrt(float(self.head_dim))\n    if mask is not None:\n        scores = scores.masked_fill(", "CodeBleu_score": 32.61859406929341}, {"code": "def attention_CrossAttention_forward(self, x, context=None, mask=None, **kwargs)\n\t\n    # apply input projection\n    query = self.query_proj(x)\n    key = self.key_proj(x)\n    value = self.value_proj(x)\n\n    # apply hypernetworks\n    if self.hypernetworks is not None:\n        context_k, context_v = apply_hypernetworks(self.hypernetworks, context, layer=self.layer)\n        key = torch.cat([key, context_k], dim=-1)\n        value = torch.cat([value, context_v], dim=-1)\n\n    # reshape for multi-head attention\n    batch_size = query.size(0)\n    query = query.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n    key = key.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n    value = value.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n\n    # scaled dot-product attention\n    attn_scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.head_dim)\n    if mask is not None:\n        attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n    attn_weights = F.softmax(attn_scores, dim=-1)\n    attn_output = torch.matmul(attn_weights, value)\n\n    # reshape back\n    attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads * self.head_dim)\n\n    # apply output projection\n    attn_output = self.output_proj(attn_output)\n\n    return attn_output\n\n", "CodeBleu_score": 35.22265554480742}, {"code": "def attention_CrossAttention_forward(self, x, context=None, mask=None, **kwargs)\n\t\n    # Input projection\n    query = self.query_proj(x)\n    key = self.key_proj(x)\n    value = self.value_proj(x)\n\n    # Context projection\n    if context is None:\n        context = x\n\n    context_k, context_v = apply_hypernetworks(self.hypernetworks, context, layer=self.layer)\n\n    # Reshape for multi-head attention\n    query = self.reshape_to_batches(query)\n    key = self.reshape_to_batches(key)\n    value = self.reshape_to_batches(value)\n\n    context_k = self.reshape_to_batches(context_k)\n    context_v = self.reshape_to_batches(context_v)\n\n    # Attention\n    attn_scores = torch.matmul(query, key.transpose(-2, -1))\n    attn_scores = attn_scores / math.sqrt(query.size(-1))\n\n    if mask is not None:\n        mask = mask.unsqueeze(1)\n        attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n\n    attn_probs = F.softmax(attn_scores, dim=-1)\n    attn_probs = self.dropout(attn_probs)\n\n    attn_output = torch.matmul(attn_probs, value)\n    attn_output = self.reshape_from_batches(attn_output)\n\n    # Output projection\n    attn_output = self.out_proj(attn_output)\n    attn_output = self.dropout(attn_output)\n\n    return attn_output\n\n", "CodeBleu_score": 35.51471648684679}]}
{"instance_id": "8b7455a03a0e", "generation_result": [{"code": "def save_hypernetwork(hypernetwork, checkpoint, hypernetwork_name, filename)\n\t    if hypernetwork.sd_checkpoint!= checkpoint:\n        original_sd_checkpoint = hypernetwork.sd_checkpoint\n        original_sd_checkpoint_name = hypernetwork.sd_checkpoint_name\n        hypernetwork.sd_checkpoint = checkpoint\n        hypernetwork.sd_checkpoint_name = hypernetwork_name\n        try:\n            hypernetwork.save(filename)\n        except Exception as e:\n            hypernetwork.sd_checkpoint = original_sd_checkpoint\n            hypernetwork.sd_checkpoint_name = original_sd_checkpoint_name\n            raise e\n        else:\n            hypernetwork.sd_checkpoint = original_sd_checkpoint\n            hypernetwork.sd_checkpoint_name = original_sd_checkpoint_name\n\n", "CodeBleu_score": 41.07771268423723}, {"code": "def save_hypernetwork(hypernetwork, checkpoint, hypernetwork_name, filename)\n\t    original_checkpoint = hypernetwork.checkpoint\n    original_checkpoint_name = hypernetwork.checkpoint_name\n    original_checkpoint_dir = hypernetwork.checkpoint_dir\n    original_checkpoint_file = hypernetwork.checkpoint_file\n    original_checkpoint_file_name = hypernetwork.checkpoint_file_name\n    original_sd_checkpoint = hypernetwork.sd_checkpoint\n    original_sd_checkpoint_name = hypernetwork.sd_checkpoint_name\n    original_sd_checkpoint_dir = hypernetwork.sd_checkpoint_dir\n    original_sd_checkpoint_file = hypernetwork.sd_checkpoint_file\n    original_sd_checkpoint_file_name = hypernetwork.sd_checkpoint_file_name\n    original_checkpoint_file_name_list = hypernetwork.checkpoint_file_name_list\n    original_checkpoint_file_list = hypernetwork.checkpoint_file_list\n    original_sd_checkpoint_file_name_list = hypernetwork.sd_checkpoint_file_name_list\n    original_sd_checkpoint_file_list = hypernetwork.sd_checkpoint_file_list\n\n    hypernetwork.checkpoint = checkpoint\n    hypernetwork.checkpoint_name = hypernetwork_name\n    hypernetwork.checkpoint_dir = shared.opts.checkpoint_dir\n    hypernetwork.checkpoint_file = shared.opts.checkpoint_file\n    hypernetwork.checkpoint_file_name = shared.opts.checkpoint_file_name\n    hypernetwork.sd_checkpoint = shared.opts.sd_checkpoint\n    hypernetwork.sd_checkpoint_name = shared.opts.sd_checkpoint_name\n    hypernetwork.sd_checkpoint_dir = shared.opts.sd_checkpoint_dir\n    hypernetwork.sd_checkpoint_file = shared.opts.sd_checkpoint_file\n    hypernetwork.sd_checkpoint_file_name = shared.opts.sd_checkpoint_file_name\n    hypernetwork.checkpoint_file_name_list = shared.opts.checkpoint_file_name_list\n    hypernetwork.checkpoint_file_list = shared.opts.checkpoint_file_list\n    hypernetwork.sd_checkpoint_file_name_list = shared.opts.sd_checkpoint_file_name_list\n    hypernetwork.sd_checkpoint_file_list = shared.opts.sd_checkpoint_file_list\n\n    try:\n        hypernetwork.save(filename)\n    except Exception as e:\n        hypernetwork.checkpoint =", "CodeBleu_score": 26.203341995365275}, {"code": "def save_hypernetwork(hypernetwork, checkpoint, hypernetwork_name, filename)\n\t    original_name = hypernetwork.name\n    original_checkpoint = hypernetwork.sd_checkpoint\n    original_checkpoint_name = hypernetwork.sd_checkpoint_name\n\n    hypernetwork.name = hypernetwork_name\n    hypernetwork.sd_checkpoint = checkpoint\n    hypernetwork.sd_checkpoint_name = filename\n\n    try:\n        hypernetwork.save(filename)\n    except Exception as e:\n        hypernetwork.name = original_name\n        hypernetwork.sd_checkpoint = original_checkpoint\n        hypernetwork.sd_checkpoint_name = original_checkpoint_name\n        raise e\n\n    hypernetwork.name = original_name\n    hypernetwork.sd_checkpoint = original_checkpoint\n    hypernetwork.sd_checkpoint_name = original_checkpoint_name", "CodeBleu_score": 40.115536673610926}]}
{"instance_id": "3175f835140f", "generation_result": [{"code": "def ema_scope(self, context=None)\n\t\n    if self.ema_enabled:\n        if self.ema_use_locking:\n            self.model.lock()\n        self.model.store_weights()\n        self.model.load_weights(self.ema_weights)\n        if self.log_ema:\n            self.log(\"EMA weights used\")\n        yield\n        self.model.restore_weights()\n        if self.log_ema:\n            self.log(\"EMA weights restored\")\n        if self.ema_use_locking:\n            self.model.unlock()\n    else:\n        yield\n\n", "CodeBleu_score": 41.88690817723263}, {"code": "def ema_scope(self, context=None)\n\t\n    if self.ema_enabled:\n        if self.ema_log:\n            self.logger.info(\"Switching to EMA weights\")\n        self.store_model_params()\n        self.load_ema_params()\n    yield\n    if self.ema_enabled:\n        if self.ema_log:\n            self.logger.info(\"Restoring original weights\")\n        self.load_model_params()\n\n", "CodeBleu_score": 36.654401171422855}, {"code": "def ema_scope(self, context=None)\n\t\n    if self.ema_enable:\n        if self.ema_enable_log:\n            self.logger.info(\"Switching to EMA weights.\")\n        self.ema_switch()\n        yield\n        if self.ema_enable_log:\n            self.logger.info(\"Switching back to original weights.\")\n        self.ema_switch()\n    else:\n        yield\n\n", "CodeBleu_score": 34.31505920246654}]}
{"instance_id": "a7b5d5cd519a", "generation_result": [{"code": "def p_mean_variance(self, x, t, clip_denoised: bool)\n\t\n    model_output = self.model(x, t)\n    if self.use_clipped_model_output:\n        model_output = torch.clamp(model_output, -1, 1)\n    if self.learn_sigma:\n        model_mean, model_log_variance = model_output.chunk(2, dim=-1)\n    else:\n        model_mean = model_output\n        model_log_variance = None\n    if clip_denoised:\n        model_mean = model_mean.clamp(-1, 1)\n    if self.learn_sigma:\n        posterior_log_variance = model_log_variance - 2 * (t * self.log_alpha_bar).float()\n        posterior_variance = torch.exp(posterior_log_variance)\n    else:\n        posterior_variance = None\n        posterior_log_variance = None\n    return model_mean, posterior_variance, posterior_log_variance\n", "CodeBleu_score": 36.297074792774666}, {"code": "def p_mean_variance(self, x, t, clip_denoised: bool)\n\t\n    model_output = self.model(x, t)\n    if self.config.noise_schedule_type == NoiseScheduleType.LINEAR:\n        sqrt_alpha_prod = extract_into_tensor(self.sqrt_alphas_cumprod, t, x.shape)\n        sqrt_one_minus_alpha_prod = extract_into_tensor(self.sqrt_one_minus_alphas_cumprod, t, x.shape)\n        posterior_variance = model_output[:, 0].pow(2)\n        posterior_log_variance_clipped = model_output[:, 1:2].clamp(self.config.log_variance_clip_range[0], self.config.log_variance_clip_range[1])\n        posterior_mean = (\n                sqrt_one_minus_alpha_prod * x +\n                sqrt_alpha_prod * (model_output[:, 2:] / posterior_variance.sqrt())\n        )\n        if clip_denoised:\n            posterior_mean.clamp_(-1.0, 1.0)\n    elif self.config.noise_schedule_type == NoiseScheduleType.COSINE:\n        posterior_log_variance_clipped = model_output[:, 0:1].clamp(self.config.log_variance_clip_range[0], self.config.log_variance_clip_range[1])\n        posterior_mean = model_output[:, 1:]\n        if clip_denoised:\n            posterior_mean.clamp_(-1.0, 1.0)\n        posterior_variance = posterior_log_variance_clipped.exp()\n    else:\n        raise ValueError(f\"Noise schedule type {self.config.noise_schedule_type} not supported.\")\n    return posterior_mean, posterior_variance, posterior_log_variance_clipped\n\n", "CodeBleu_score": 32.369727681703075}, {"code": "def p_mean_variance(self, x, t, clip_denoised: bool)\n\t\n    model_output = self.unet(x, t)\n    if self.config.condition_on_x_prev:\n        model_output_prev = self.unet(x, t - 1)\n        model_output = model_output + model_output_prev\n    if self.config.use_scale_shift_norm:\n        model_output = self.conv_out(model_output)\n        eps, log_variance = model_output.chunk(2, dim=-3)\n        eps = self.conv_eps(eps)\n        log_variance = self.conv_log_variance(log_variance)\n        log_variance = torch.clamp(log_variance, max=self.max_logvar)\n    else:\n        eps, log_variance = model_output.chunk(2, dim=-3)\n        eps = self.conv_eps(eps)\n        log_variance = self.conv_log_variance(log_variance)\n        log_variance = torch.clamp(log_variance, max=self.max_logvar)\n    if clip_denoised:\n        x_recon = x - extract_into_tensor(self.sqrt_recip_alphas_cumprod, t, x.shape) * eps\n    else:\n        x_recon = extract_into_tensor(self.sqrt_recip_alphas_cumprod, t, x.shape) * eps\n    x_recon = x_recon.detach()\n    model_mean = (x_recon / extract_into_tensor(self.sqrt_alphas_cumprod, t, x.shape)).detach()\n    posterior_variance = extract_into_tensor(self.posterior_variance, t, x.shape)\n    posterior_log_variance_clipped = extract_into_tensor(self.posterior_log_variance_clipped, t, x.shape)\n    return model_mean, posterior_variance, posterior_log_variance_clipped\n", "CodeBleu_score": 29.336858726494896}]}
{"instance_id": "a8652fa78473", "generation_result": [{"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss_type == 'l1':\n        loss = torch.abs(pred - target)\n        if mean:\n            loss = torch.mean(loss)\n    elif self.loss_type == 'l2':\n        loss = F.mse_loss(pred, target, reduction='none')\n        if not mean:\n            loss = torch.mean(loss)\n    else:\n        raise ValueError('Unsupported loss type: {}'.format(self.loss_type))\n    return loss\n\n", "CodeBleu_score": 49.87301883996081}, {"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss_type == 'l1':\n        loss = torch.abs(pred - target)\n        if mean:\n            loss = loss.mean()\n    elif self.loss_type == 'l2':\n        loss = F.mse_loss(pred, target, reduction='none')\n        if not mean:\n            loss = loss.mean(0)\n    else:\n        raise ValueError('Unsupported loss type: {}'.format(self.loss_type))\n    return loss\n\n", "CodeBleu_score": 54.34521765749406}, {"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss == 'l1':\n        loss = torch.abs(pred - target)\n        if mean:\n            loss = loss.mean()\n    elif self.loss == 'l2':\n        loss = F.mse_loss(pred, target, reduction='none')\n        if not mean:\n            loss = loss.sum()\n    else:\n        raise ValueError('Loss type not supported.')\n    return loss\n\n", "CodeBleu_score": 51.19331120241573}]}
{"instance_id": "d68e4f061349", "generation_result": [{"code": "def _prior_bpd(self, x_start)\n\t    batch_size = x_start.shape[0]\n    t = torch.ones([batch_size], device=x_start.device, dtype=x_start.dtype) * (self.num_timesteps - 1)\n    mean, log_variance = self.q_mean_variance(x_start, t)\n    kl = -0.5 * (log_variance - math.log(math.pi * 2) - 1)\n    kl = kl.sum(dim=-1)\n    return torch.mean(kl) / math.log(2)\n\n", "CodeBleu_score": 30.278369173087754}, {"code": "def _prior_bpd(self, x_start)\n\t    batch_size = x_start.shape[0]\n    t = torch.full((batch_size,), self.num_timesteps - 1, device=x_start.device, dtype=x_start.dtype)\n    mean, _, log_variance = self.q_mean_variance(x_start, t)\n    return (log_normal_cdf(-mean / (log_variance.exp() * math.sqrt(2))) - log_variance / 2).mean() / math.log(2)\n\n", "CodeBleu_score": 30.78277451783159}, {"code": "def _prior_bpd(self, x_start)\n\t    batch_size = x_start.shape[0]\n    t = torch.full((batch_size,), self.num_timesteps - 1, device=x_start.device)\n    mean, _, log_variance = self.q_mean_variance(x_start, t)\n    kl = -0.5 * (log_variance - math.log(2 * math.pi) - 1)\n    kl = kl.sum(dim=(1, 2, 3))\n    return torch.mean(kl) / math.log(2)\n\n", "CodeBleu_score": 31.685559149039115}]}
{"instance_id": "c695d93fdf2d", "generation_result": [{"code": "def make_cache(subsection: str) -> diskcache.Cache\n\t\n    cache_dir = os.path.join(os.path.dirname(__file__), \"cache\")\n    os.makedirs(cache_dir, exist_ok=True)\n    return diskcache.Cache(\n        os.path.join(cache_dir, subsection),\n        size_limit=4 * 1024 ** 3,\n        disk_min_file_size=256 * 1024,\n    )\n\n", "CodeBleu_score": 63.78726043433337}, {"code": "def make_cache(subsection: str) -> diskcache.Cache\n\t\n    cache_dir = Path(CACHE_DIR)\n    cache_dir.mkdir(exist_ok=True)\n    return diskcache.Cache(cache_dir / subsection, size_limit=4 * 1024 ** 3, disk_min_file_size=256 * 1024)\n\n", "CodeBleu_score": 49.49306991564272}, {"code": "def make_cache(subsection: str) -> diskcache.Cache\n\t\n    cache_dir = Path(CACHE_DIR)\n    cache_dir.mkdir(parents=True, exist_ok=True)\n    return diskcache.Cache(\n        cache_dir / subsection,\n        size_limit=4 * 1024 * 1024 * 1024,\n        disk_min_file_size=256 * 1024,\n    )\n\n", "CodeBleu_score": 52.135594499333116}]}
{"instance_id": "ed4c543f0d38", "generation_result": [{"code": "def cache(subsection)\n\t\n    cache_obj = caches.get(subsection)\n    if cache_obj is None:\n        with cache_lock:\n            if not os.path.exists(cache_dir):\n                os.makedirs(cache_dir)\n            if not os.path.isfile(cache_filename):\n                convert_old_cached_data()\n            cache_obj = caches.get(subsection)\n            if cache_obj is None:\n                cache_obj = make_cache(subsection)\n                caches[subsection] = cache_obj\n    return cache_obj\n\n", "CodeBleu_score": 67.25921240957332}, {"code": "def cache(subsection)\n\t\n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            cache_obj = caches.get(subsection)\n            if cache_obj is None:\n                with cache_lock:\n                    if not os.path.exists(cache_dir):\n                        os.makedirs(cache_dir)\n                    if not os.path.isfile(cache_filename):\n                        convert_old_cached_data()\n\n                    cache_obj = caches.get(subsection)\n                    if cache_obj is None:\n                        cache_obj = make_cache(subsection)\n                        caches[subsection] = cache_obj\n            return func(*args, cache=cache_obj, **kwargs)\n        return wrapper\n    return decorator\n\n", "CodeBleu_score": 60.091808076041886}, {"code": "def cache(subsection)\n\t\n    cache_obj = caches.get(subsection)\n    if cache_obj is None:\n        with cache_lock:\n            if not os.path.isdir(cache_dir):\n                os.mkdir(cache_dir)\n            if not os.path.isfile(cache_filename):\n                convert_old_cached_data()\n\n            cache_obj = caches.get(subsection)\n            if cache_obj is None:\n                cache_obj = make_cache(subsection)\n                caches[subsection] = cache_obj\n    return cache_obj\n", "CodeBleu_score": 67.10085685143339}]}
{"instance_id": "ccf5ea2d9fc1", "generation_result": [{"code": "def cached_data_for_file(subsection, title, filename, func)\n\t\n\n    cache_obj = cache(subsection)\n    mtime = os.path.getmtime(filename)\n    cached_data = cache_obj.get(title)\n    if cached_data and cached_data.get(\"mtime\") >= mtime:\n        return cached_data[\"data\"]\n    else:\n        data = func()\n        cache_obj[title] = {\"mtime\": mtime, \"data\": data}\n        dump_cache()\n        return data\n", "CodeBleu_score": 24.713103239211833}, {"code": "def cached_data_for_file(subsection, title, filename, func)\n\t\n\n    cache_obj = cache(subsection)\n    mtime = os.stat(filename).st_mtime\n    cached_data = cache_obj.get(title)\n    if cached_data and cached_data.get('mtime') >= mtime:\n        return cached_data['data']\n\n    data = func()\n    cache_obj[title] = {'mtime': mtime, 'data': data}\n    dump_cache()\n    return data\n", "CodeBleu_score": 23.072167286762905}, {"code": "def cached_data_for_file(subsection, title, filename, func)\n\t\n\n    cache = cache(subsection)\n    mtime = os.path.getmtime(filename)\n    cached_data = cache.get(title)\n    if cached_data:\n        cached_mtime = cached_data.get(\"mtime\")\n        if cached_mtime >= mtime:\n            return cached_data[\"data\"]\n    data = func()\n    cache[title] = {\"mtime\": mtime, \"data\": data}\n    dump_cache()\n    return data\n\n", "CodeBleu_score": 29.72846486546656}]}
{"instance_id": "e01a2d4f0ac2", "generation_result": [{"code": "def report(message: str, *, exc_info: bool = False) -> None\n\t\n    record_exception()\n    print(\"***\", message, file=sys.stderr)\n    if exc_info:\n        print(\"    \" + (\"\n    \".join(exception_records[-1])), file=sys.stderr)\n        print(\"---\", file=sys.stderr)\n\n", "CodeBleu_score": 45.32775034020295}, {"code": "def report(message: str, *, exc_info: bool = False) -> None\n\t\n    record_exception()\n    for line in message.splitlines():\n        print(f\"*** {line}\", file=sys.stderr)\n    if exc_info:\n        for line in traceback.format_exc().splitlines():\n            print(f\"    {line}\", file=sys.stderr)\n        print(\"---\", file=sys.stderr)\n\n", "CodeBleu_score": 56.18109894329668}, {"code": "def report(message: str, *, exc_info: bool = False) -> None\n\t\n    record_exception()\n    print(f\"*** {message}\", file=sys.stderr)\n    if exc_info:\n        for line in exception_records[-1].splitlines():\n            print(f\"    {line}\", file=sys.stderr)\n        print(\"---\", file=sys.stderr)\n\n", "CodeBleu_score": 48.08269094676451}]}
{"instance_id": "125fb9d2320b", "generation_result": [{"code": "def extract_original_prompts(style: PromptStyle, prompt, negative_prompt)\n\t\n\n    if style.prompt == \"\" and style.negative_prompt == \"\":\n        return False, prompt, negative_prompt\n\n    success, prompt = extract_style_text_from_prompt(style.prompt, prompt)\n    if not success:\n        return False, prompt, negative_prompt\n\n    success, negative_prompt = extract_style_text_from_prompt(style.negative_prompt, negative_prompt)\n    if not success:\n        return False, prompt, negative_prompt\n\n    return True, prompt, negative_prompt", "CodeBleu_score": 70.36703151473317}, {"code": "def extract_original_prompts(style: PromptStyle, prompt, negative_prompt)\n\t\n\n    if style.prompt == \"\" and style.negative_prompt == \"\":\n        return False, prompt, negative_prompt\n\n    prompt_match, prompt = extract_style_text_from_prompt(style.prompt, prompt)\n    negative_prompt_match, negative_prompt = extract_style_text_from_prompt(style.negative_prompt, negative_prompt)\n\n    if not prompt_match or not negative_prompt_match:\n        return False, prompt, negative_prompt\n\n    return True, prompt, negative_prompt", "CodeBleu_score": 59.76287587688691}, {"code": "def extract_original_prompts(style: PromptStyle, prompt, negative_prompt)\n\t\n\n    if style.prompt == '' and style.negative_prompt == '':\n        return False, prompt, negative_prompt\n\n    success, prompt = extract_style_text_from_prompt(style.prompt, prompt)\n    if not success:\n        return False, prompt, negative_prompt\n\n    success, negative_prompt = extract_style_text_from_prompt(style.negative_prompt, negative_prompt)\n    if not success:\n        return False, prompt, negative_prompt\n\n    return True, prompt, negative_prompt\n", "CodeBleu_score": 70.36703151473317}]}
{"instance_id": "0c5f729fbce2", "generation_result": [{"code": "def random_partial(self, count, n_frames)\n\t\n    if self.utterances is None:\n        self._load_utterances()\n\n    utterances = self.utterance_cycler.sample(count)\n\n    a = [(u,) + u.random_partial(n_frames) for u in utterances]\n\n    return a\n", "CodeBleu_score": 94.20475781398312}, {"code": "def random_partial(self, count, n_frames)\n\t\n    if self.utterances is None:\n        self._load_utterances()\n\n    utterances = self.utterance_cycler.sample(count)\n\n    a = [(u,) + u.random_partial(n_frames) for u in utterances]\n\n    return a\n", "CodeBleu_score": 94.20475781398312}, {"code": "def random_partial(self, count, n_frames)\n\t\n    if self.utterances is None:\n        self._load_utterances()\n\n    utterances = self.utterance_cycler.sample(count)\n\n    a = [(u,) + u.random_partial(n_frames) for u in utterances]\n\n    return a\n", "CodeBleu_score": 94.20475781398312}]}
{"instance_id": "31e8b27b8f94", "generation_result": [{"code": "def normalize_volume(wav, target_dBFS, increase_only=False, decrease_only=False)\n\t\n    if increase_only and decrease_only:\n        raise ValueError(\"Cannot set both increase_only and decrease_only to True\")\n\n    dBFS_change = target_dBFS - audioop.dB(wav, 2)\n    if dBFS_change < 0 and increase_only or dBFS_change > 0 and decrease_only:\n        return wav\n    return audioop.mul(wav, 2, int(round(math.pow(10.0, dBFS_change / 10.0))))\n\n", "CodeBleu_score": 43.18179099961563}, {"code": "def normalize_volume(wav, target_dBFS, increase_only=False, decrease_only=False)\n\t\n    if increase_only and decrease_only:\n        raise ValueError(\"Cannot set both increase_only and decrease_only to True\")\n\n    dBFS_change = target_dBFS - audioop.dB(audioop.max(wav, 2), 2)\n    if dBFS_change < 0 and increase_only or dBFS_change > 0 and decrease_only:\n        return wav\n\n    return audioop.mul(wav, 2, int(10 ** (dBFS_change / 10)))\n\n", "CodeBleu_score": 50.51045034410932}, {"code": "def normalize_volume(wav, target_dBFS, increase_only=False, decrease_only=False)\n\t\n    if increase_only and decrease_only:\n        raise ValueError(\"Cannot set both increase_only and decrease_only to True\")\n\n    dBFS_change = target_dBFS - audioop.dB(wav, 2)\n    if dBFS_change < 0 and increase_only or dBFS_change > 0 and decrease_only:\n        return wav\n    return audioop.mul(wav, 2, int(10 ** (dBFS_change / 10)))\n\n", "CodeBleu_score": 47.48065494426961}]}
{"instance_id": "2353831e9033", "generation_result": [{"code": "def load_model(weights_fpath: Path, device=None)\n\t\n    global _model\n\n    if device is None:\n        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    device = torch.device(device)\n    _model = SpeakerEncoder(device=device)\n    _model.load_state_dict(torch.load(weights_fpath, map_location=device))\n    _model.eval()\n    print(f\"Loaded model from {weights_fpath} trained for {get_steps(weights_fpath)} steps\")\n\n", "CodeBleu_score": 40.652112013293284}, {"code": "def load_model(weights_fpath: Path, device=None)\n\t\n    global _model\n    if device is None:\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    _model = SpeakerEncoder(device=device)\n    _model.load_state_dict(torch.load(weights_fpath, map_location=device))\n    _model.eval()\n    print(f\"Loaded model from {weights_fpath} trained for {get_num_training_steps(weights_fpath)} steps\")\n\n", "CodeBleu_score": 42.410086181589804}, {"code": "def load_model(weights_fpath: Path, device=None)\n\t\n    global _model\n\n    if device is None:\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    _model = SpeakerEncoder(device)\n    _model.load_state_dict(torch.load(weights_fpath, map_location=device))\n    _model.eval()\n    print(f\"Loaded model from {weights_fpath} trained to step {get_step_from_path(weights_fpath)}\")\n\n", "CodeBleu_score": 41.66703364520685}]}
{"instance_id": "3231ad480199", "generation_result": [{"code": "def embed_frames_batch(frames_batch)\n\t\n    # Check if the model is loaded\n    if _model is None:\n        raise Exception(\"No model loaded!\")\n\n    # Convert the input to a tensor and transfer it to the appropriate device\n    frames_batch = torch.from_numpy(frames_batch).to(_device)\n\n    # Perform a forward pass through the model to compute the embeddings\n    with torch.no_grad():\n        embeddings = _model.forward(frames_batch)\n\n    # Detach the result from the computation graph and move it to the CPU\n    embeddings = embeddings.detach().cpu()\n\n    # Convert the embeddings back to a NumPy array and return them\n    return embeddings.numpy()\n", "CodeBleu_score": 58.886209999608056}, {"code": "def embed_frames_batch(frames_batch)\n\t\n    if not _model:\n        raise Exception(\"Model not loaded\")\n\n    frames_batch = torch.from_numpy(frames_batch).to(_device)\n    with torch.no_grad():\n        embeds = _model.forward(frames_batch.unsqueeze(0))\n        embeds = embeds.detach().cpu().numpy()\n    return embeds\n\n", "CodeBleu_score": 54.99157984999879}, {"code": "def embed_frames_batch(frames_batch)\n\t\n    if not _model:\n        raise Exception(\"Model not loaded\")\n\n    frames_batch_tensor = torch.from_numpy(frames_batch).to(_device)\n    with torch.no_grad():\n        embeddings = _model.forward(frames_batch_tensor).detach().cpu().numpy()\n    return embeddings\n", "CodeBleu_score": 61.88731502695535}]}
{"instance_id": "117a41716c8d", "generation_result": [{"code": "def compute_partial_slices(n_samples, partial_utterance_n_frames=partials_n_frames,\n                           min_pad_coverage=0.75, overlap=0.5)\n\t\n    assert 0 < overlap < 1, f\"overlap must be between 0 and 1, got {overlap}\"\n    assert 0 < min_pad_coverage < 1, f\"min_pad_coverage must be between 0 and 1, got {min_pad_coverage}\"\n\n    # Number of frames and frame step\n    n_frames = partial_utterance_n_frames\n    frame_step = int(n_frames * (1 - overlap))\n\n    # Compute wav and mel slices\n    wav_slices = []\n    mel_slices = []\n    start = 0\n    while start + n_frames <= n_samples:\n        wav_slices.append(slice(start, start + n_frames))\n        mel_slices.append(slice(start // hop_length, (start + n_frames) // hop_length))\n        start += frame_step\n\n    # Evaluate need for extra padding\n    coverage = (n_samples - start) / n_frames\n    if coverage < min_pad_coverage and len(wav_slices) > 1:\n        wav_slices.pop()\n        mel_slices.pop()\n\n    return wav_slices, mel_slices\n\n", "CodeBleu_score": 28.181025351715387}, {"code": "def compute_partial_slices(n_samples, partial_utterance_n_frames=partials_n_frames,\n                           min_pad_coverage=0.75, overlap=0.5)\n\t\n    assert 0 <= min_pad_coverage <= 1, f\"min_pad_coverage must be in [0, 1]. Got {min_pad_coverage}\"\n    assert 0 <= overlap <= 1, f\"overlap must be in [0, 1]. Got {overlap}\"\n    frame_step = partial_utterance_n_frames - int(partial_utterance_n_frames * overlap)\n    n_frames = (n_samples - partial_utterance_n_frames) // frame_step + 1\n    wav_slices = []\n    mel_slices = []\n    for i in range(n_frames):\n        start = i * frame_step\n        end = start + partial_utterance_n_frames\n        wav_slices.append(start)\n        mel_slices.append(start)\n    if n_frames > 1:\n        last_slice_coverage = (n_samples - wav_slices[-1]) / partial_utterance_n_frames\n        if last_slice_coverage < min_pad_coverage:\n            wav_slices = wav_slices[:-1]\n            mel_slices = mel_slices[:-1]\n    return wav_slices, mel_slices\n\n", "CodeBleu_score": 33.880607228123736}, {"code": "def compute_partial_slices(n_samples, partial_utterance_n_frames=partials_n_frames,\n                           min_pad_coverage=0.75, overlap=0.5)\n\t\n    assert 0 < min_pad_coverage <= 1,'min_pad_coverage must be in range (0, 1]'\n    assert 0 < overlap <= 1, 'overlap must be in range (0, 1]'\n\n    # Compute the number of frames and frame step\n    n_frames = int(np.ceil(n_samples / partial_utterance_n_frames))\n    frame_step = int(np.ceil(partial_utterance_n_frames * (1 - overlap)))\n\n    # Compute wav and mel slices\n    wav_slices = []\n    mel_slices = []\n    for i in range(n_frames):\n        start = i * frame_step\n        end = start + partial_utterance_n_frames\n        wav_slices.append(start)\n        mel_slices.append(end)\n\n    # Evaluate the need for extra padding\n    coverage = (mel_slices[-1] - wav_slices[-1]) / partial_utterance_n_frames\n    if coverage < min_pad_coverage and len(wav_slices) > 1:\n        wav_slices = wav_slices[:-1]\n        mel_slices = mel_slices[:-1]\n\n    return wav_slices, mel_slices\n\n", "CodeBleu_score": 38.19860183132759}]}
{"instance_id": "34fe5c864f1c", "generation_result": [{"code": "def plot_embedding_as_heatmap(embed, ax=None, title=\"\", shape=None, color_range=(0, 0.30))\n\t\n    if ax is None:\n        ax = plt.gca()\n    if shape is None:\n        shape = (int(np.sqrt(len(embed))), int(np.sqrt(len(embed))))\n    ax.imshow(embed.reshape(*shape), cmap=\"magma\", vmin=color_range[0], vmax=color_range[1])\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.set_title(title)\n\n", "CodeBleu_score": 38.51912506987429}, {"code": "def plot_embedding_as_heatmap(embed, ax=None, title=\"\", shape=None, color_range=(0, 0.30))\n\t\n    if ax is None:\n        ax = plt.gca()\n    if shape is None:\n        shape = (int(np.sqrt(len(embed))),) * 2\n    embed = embed.reshape(shape)\n    ax.imshow(embed, cmap=\"viridis\", vmin=color_range[0], vmax=color_range[1])\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.set_title(title)\n    plt.colorbar()\n\n", "CodeBleu_score": 45.00134730768687}, {"code": "def plot_embedding_as_heatmap(embed, ax=None, title=\"\", shape=None, color_range=(0, 0.30))\n\t\n    if ax is None:\n        ax = plt.gca()\n    if shape is None:\n        shape = (int(np.sqrt(len(embed))), int(np.sqrt(len(embed))))\n    embed = embed.reshape(shape)\n    im = ax.imshow(embed, cmap=\"coolwarm\", vmin=color_range[0], vmax=color_range[1])\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.set_title(title)\n    plt.colorbar(im)\n\n", "CodeBleu_score": 51.15687610975848}]}
{"instance_id": "92e93d50b62b", "generation_result": [{"code": "def similarity_matrix(self, embeds)\n\t\n    num_speakers, num_utterances, embedding_size = embeds.shape\n    centroids_incl = torch.zeros(num_speakers, embedding_size, device=self.device)\n    centroids_excl = torch.zeros(num_speakers, embedding_size, device=self.device)\n    for i in range(num_speakers):\n        centroids_incl[i] = torch.nn.functional.normalize(\n            torch.mean(embeds[i], dim=0, keepdim=True), dim=-1\n        )\n        centroids_excl[i] = torch.nn.functional.normalize(\n            torch.mean(torch.cat([embeds[i, :i], embeds[i, i + 1 :]], dim=0), dim=0),\n            dim=-1,\n        )\n    similarity_matrix = torch.zeros(\n        num_speakers, num_speakers, device=self.device\n    ).float()\n    mask = torch.ones(num_speakers, num_speakers, device=self.device).float()\n    for i in range(num_speakers):\n        similarity_matrix[i] = torch.matmul(embeds[i], centroids_incl[i].t())\n        mask[i, i] = 0\n    similarity_matrix = similarity_matrix * mask\n    similarity_matrix = self.scale * similarity_matrix + self.bias\n    return similarity_matrix\n\n", "CodeBleu_score": 32.93826252950892}, {"code": "def similarity_matrix(self, embeds)\n\t\n    num_speakers_per_batch = embeds.size(0)\n    num_utterances_per_speaker = embeds.size(1)\n    centroids_incl = embeds.mean(dim=1, keepdim=True)\n    centroids_incl = F.normalize(centroids_incl, p=2, dim=-1)\n    centroids_excl = embeds.sum(dim=1, keepdim=True) - embeds\n    centroids_excl = centroids_excl / (num_utterances_per_speaker - 1)\n    centroids_excl = F.normalize(centroids_excl, p=2, dim=-1)\n    sim_matrix = torch.zeros(\n        num_speakers_per_batch, num_speakers_per_batch, device=embeds.device\n    )\n    mask = torch.eye(num_speakers_per_batch, device=embeds.device)\n    for i in range(num_speakers_per_batch):\n        sim_matrix[i] = (embeds[i] * centroids_incl[i]).sum(-1)\n        sim_matrix[i] += (embeds[i] * centroids_excl[i]).sum(-1)\n        sim_matrix[i] *= 0.5\n        sim_matrix[i] = self.scale * sim_matrix[i] + self.bias\n        sim_matrix[i] = sim_matrix[i] * (1 - mask[i])\n    return sim_matrix\n\n", "CodeBleu_score": 31.408943272663727}, {"code": "def similarity_matrix(self, embeds)\n\t\n    num_spks = embeds.shape[0]\n    num_utts = embeds.shape[1]\n    centroids_incl = torch.zeros(num_spks, embeds.shape[-1], device=embeds.device)\n    centroids_excl = torch.zeros(num_spks, embeds.shape[-1], device=embeds.device)\n    for i in range(num_spks):\n        centroids_incl[i] = torch.nn.functional.normalize(\n            torch.mean(embeds[i], dim=0), dim=-1\n        )\n        centroids_excl[i] = torch.nn.functional.normalize(\n            torch.mean(torch.cat([embeds[i, :i], embeds[i, i + 1 :]], dim=0), dim=0),\n            dim=-1,\n        )\n    sim_mat = torch.zeros(num_spks, num_spks, device=embeds.device)\n    mask = torch.ones(num_spks, num_spks, device=embeds.device)\n    for i in range(num_spks):\n        sim_mat[i] = torch.matmul(embeds[i], centroids_incl[i].unsqueeze(-1))\n        sim_mat[i] += torch.matmul(embeds[i], centroids_excl[i].unsqueeze(-1))\n        sim_mat[i] /= 2\n        mask[i, i] = 0\n    sim_mat *= self.scale\n    sim_mat += self.bias\n    return sim_mat, mask\n\n", "CodeBleu_score": 30.20023694782364}]}
{"instance_id": "a6eba54108a1", "generation_result": [{"code": "def loss(self, embeds)\n\t\n    # Compute similarity matrix\n    sim_matrix = self.similarity_matrix(embeds)\n    # Reshape similarity matrix\n    speakers_per_batch, utterances_per_speaker = embeds.shape[:2]\n    sim_matrix = sim_matrix.view(speakers_per_batch, utterances_per_speaker,\n                                 speakers_per_batch, utterances_per_speaker)\n    # Generate labels\n    labels = torch.zeros(speakers_per_batch, utterances_per_speaker,\n                         speakers_per_batch, utterances_per_speaker)\n    for i in range(speakers_per_batch):\n        labels[i, :, i, :] = 1\n    labels = labels.to(self.loss_device)\n    # Calculate loss\n    loss = self.loss_function(sim_matrix, labels)\n    # Calculate EER\n    # Flatten similarity matrix\n    sim_matrix = sim_matrix.view(speakers_per_batch, utterances_per_speaker * speakers_per_batch,\n                                 utterances_per_speaker)\n    # Generate labels\n    labels = torch.zeros(speakers_per_batch, utterances_per_speaker * speakers_per_batch)\n    for i in range(speakers_per_batch):\n        labels[i, i * utterances_per_speaker:(i + 1) * utterances_per_speaker] = 1\n    labels = labels.to(self.loss_device)\n    # Calculate EER\n    eer = self.calculate_eer(sim_matrix, labels)\n    return loss, eer\n\n", "CodeBleu_score": 31.446199519853725}, {"code": "def loss(self, embeds)\n\t\n    speakers_per_batch, utterances_per_speaker = embeds.shape[:2]\n    sim_matrix = self.similarity_matrix(embeds)\n    sim_matrix = sim_matrix.reshape(speakers_per_batch, utterances_per_speaker * (speakers_per_batch - 1))\n\n    # Generate labels\n    labels = torch.zeros(speakers_per_batch, utterances_per_speaker * (speakers_per_batch - 1)).to(self.loss_device)\n    for i in range(speakers_per_batch):\n        labels[i, i * utterances_per_speaker: (i + 1) * utterances_per_speaker] = 1\n\n    # Calculate loss\n    loss = self.loss_function(sim_matrix, labels)\n\n    # Calculate EER\n    labels = labels.detach().cpu().numpy()\n    sim_matrix = sim_matrix.detach().cpu().numpy()\n    fpr, tpr, thresholds = roc_curve(labels, sim_matrix)\n    eer = fpr[np.nanargmin(np.abs(fpr - (1 - tpr)))]\n\n    return loss, eer\n\n", "CodeBleu_score": 45.228995883270926}, {"code": "def loss(self, embeds)\n\t\n    speakers_per_batch, utterances_per_speaker = embeds.shape[:2]\n    sim_matrix = self.similarity_matrix(embeds)\n    sim_matrix = sim_matrix.reshape(speakers_per_batch * utterances_per_speaker, -1)\n    labels = torch.zeros(speakers_per_batch * utterances_per_speaker).to(self.loss_device)\n    for i in range(speakers_per_batch):\n        labels[i * utterances_per_speaker:(i + 1) * utterances_per_speaker] = i\n    loss = self.loss_fn(sim_matrix, labels)\n    loss = loss * self.loss_weight + self.loss_bias\n    eer = self.calculate_eer(sim_matrix, labels)\n    return loss, eer\n\n", "CodeBleu_score": 31.39211615210363}]}
{"instance_id": "ac7b74b9d1b1", "generation_result": [{"code": "def log_params(self)\n\t\n    if self.log_params is False:\n        return\n    params = []\n    params.append(\"Model Parameters\")\n    params.append(\"----------------\")\n    params.append(\"model_name: \" + self.model_name)\n    params.append(\"model_type: \" + self.model_type)\n    params.append(\"model_params: \" + str(self.model_params))\n    params.append(\"model_fit_params: \" + str(self.model_fit_params))\n    params.append(\"model_fit_params_last: \" + str(self.model_fit_params_last))\n    params.append(\"model_fit_params_all: \" + str(self.model_fit_params_all))\n    params.append(\"model_fit_params_all_last: \" + str(self.model_fit_params_all_last))\n    params.append(\"model_fit_params_all_last_last: \" + str(self.model_fit_params_all_last_last))\n    params.append(\"model_fit_params_all_last_last_last: \" + str(self.model_fit_params_all_last_last_last))\n    params.append(\"model_fit_params_all_last_last_last_last: \" + str(self.model_fit_params_all_last_last_last_last))\n    params.append(\"model_fit_params_all_last_last_last_last_last: \" + str(self.model_fit_params_all_last_last_last_last_last))\n    params.append(\"model_fit_params_all_last_last_last_last_last_last: \" + str(self.model_fit_params_all_last_last_last_last_last_last))\n    params.append(\"model_fit_params_all_last_last_last_last_last_last_last: \" + str(self.model_fit_params_all_last_last_last_last_last_last_last))\n    params.append(\"model_fit_params_all_last_last_last_last_last_last_last_last: \" + str(self.model_fit_params_all_last_last_last_last", "CodeBleu_score": 12.377958571950074}, {"code": "def log_params(self)\n\t\n    if not self.disable_logging:\n        try:\n            import matplotlib.pyplot as plt\n        except:\n            print(\"matplotlib is not installed.  Please install it to use this feature.\")\n            return\n        param_list = []\n        param_list.append(\"Model Parameters:", "CodeBleu_score": 9.172759518500795}, {"code": "def log_params(self)\n\t\n    if not self.disable_logging:\n        # Get parameter configurations\n        param_configs = self.get_param_configs()\n\n        # Iterate through the model and data parameters\n        param_string = \"\"\n        for param_name in self.model_params:\n            param_string += f\"{param_name} = {self.model_params[param_name]}", "CodeBleu_score": 7.166928647158683}]}
{"instance_id": "4726b7864386", "generation_result": [{"code": "def draw_projections(self, embeds, utterances_per_speaker, step, out_fpath=None, max_speakers=10)\n\t\n\n    if len(embeds) > max_speakers:\n        embeds = embeds[:max_speakers]\n        utterances_per_speaker = utterances_per_speaker[:max_speakers]\n\n    embeds = embeds.reshape(-1, embeds.shape[-1])\n\n    speakers = np.repeat(np.arange(len(utterances_per_speaker)), utterances_per_speaker)\n\n    if self.config[\"umap_config\"] is not None:\n        reducer = umap.UMAP(**self.config[\"umap_config\"])\n        embeds = reducer.fit_transform(embeds)\n\n    if self.config[\"plot_config\"] is not None:\n        plt.figure(figsize=(10, 10))\n        plt.title(f\"Step: {step}\")\n        plt.scatter(embeds[:, 0], embeds[:, 1], c=speakers, cmap=\"tab20\")\n        plt.colorbar()\n        plt.tight_layout()\n\n        if out_fpath is None:\n            plt.show()\n        else:\n            plt.savefig(out_fpath)\n            plt.close()", "CodeBleu_score": 42.41063755595529}, {"code": "def draw_projections(self, embeds, utterances_per_speaker, step, out_fpath=None, max_speakers=10)\n\t\n    if not self.config.umap_projections:\n        return\n    if self.config.num_speakers > max_speakers:\n        print(f\"Too many speakers ({self.config.num_speakers}) to draw projections. Limiting to {max_speakers}.\")\n        self.config.num_speakers = max_speakers\n    if self.config.num_speakers == 1:\n        print(\"Only one speaker. Not drawing projections.\")\n        return\n    if not self.config.draw_projections:\n        return\n    if self.config.draw_projections_every_step:\n        if step % self.config.draw_projections_every_step!= 0:\n            return\n    if self.config.draw_projections_every_epoch:\n        if step % self.config.draw_projections_every_epoch!= 0:\n            return\n    if self.config.draw_projections_every_epoch:\n        if step % self.config.draw_projections_every_epoch!= 0:\n            return\n    if self.config.draw_projections_every_epoch:\n        if step % self.config.draw_projections_every_epoch!= 0:\n            return\n    if self.config.draw_projections_every_epoch:\n        if step % self.config.draw_projections_every_epoch!= 0:\n            return\n    if self.config.draw_projections_every_epoch:\n        if step % self.config.draw_projections_every_epoch!= 0:\n            return\n    if self.config.draw_projections_every_epoch:\n        if step % self.config.draw_projections_every_epoch!= 0:\n            return\n    if self.config.draw_projections_every_epoch:\n        if step % self.config.draw_projections_every_epoch!= 0:\n            return\n    if self.config.draw_projections_every_epoch:\n        if step % self.config.draw_projections_every_epoch!= 0:\n            return\n    if self.config.draw_projections_every_epoch:\n        if step % self.config.draw_projections_every_epoch!= 0:\n            return\n    if self.config.draw_projections_every_epoch:\n        if step % self.", "CodeBleu_score": 14.296987611090234}, {"code": "def draw_projections(self, embeds, utterances_per_speaker, step, out_fpath=None, max_speakers=10)\n\t\n    # Limit the number of speakers\n    if len(embeds) > max_speakers:\n        embeds = embeds[:max_speakers]\n        utterances_per_speaker = utterances_per_speaker[:max_speakers]\n    # Flatten the embeddings\n    embeds = embeds.reshape(-1, embeds.shape[-1])\n    # Assign colors based on speaker identity\n    colors = [self.color_map[i] for i in range(len(embeds))]\n    # UMAP reduces the dimensionality of the embeddings\n    reducer = umap.UMAP(random_state=42)\n    embedding = reducer.fit_transform(embeds)\n    # Plot the embeddings\n    plt.figure(figsize=(8, 8))\n    plt.scatter(embedding[:, 0], embedding[:, 1], c=colors)\n    # Add labels to the plot\n    for i in range(len(embedding)):\n        plt.text(\n            embedding[i, 0],\n            embedding[i, 1],\n            str(i),\n            fontsize=12,\n            color=colors[i],\n            horizontalalignment=\"center\",\n            verticalalignment=\"center\",\n        )\n    # Add a title to the plot\n    plt.title(f\"Step {step}\")\n    # Display or save the plot\n    if out_fpath:\n        plt.savefig(out_fpath)\n    else:\n        plt.show()\n\n", "CodeBleu_score": 42.02674060811116}]}
{"instance_id": "274fa50985c0", "generation_result": [{"code": "def add_speaker_embedding(self, x, speaker_embedding)\n\t\n    input_shape = x.shape\n    dim_names = [x.shape[i].name for i in range(len(input_shape))]\n    speaker_dim_idx = dim_names.index(\"timesteps\")\n    speaker_embedding = tf.repeat(\n        speaker_embedding, repeats=input_shape[speaker_dim_idx], axis=0\n    )\n    speaker_embedding = tf.reshape(\n        speaker_embedding, shape=(-1, self.speaker_embedding_size)\n    )\n    speaker_embedding = tf.transpose(speaker_embedding, perm=[1, 0])\n    x = tf.concat([x, speaker_embedding], axis=-1)\n    return x\n\n", "CodeBleu_score": 32.877028250693954}, {"code": "def add_speaker_embedding(self, x, speaker_embedding)\n\t\n    input_shape = x.shape.as_list()\n    dim_index = len(input_shape) - 1\n    speaker_embedding = tf.expand_dims(speaker_embedding, axis=0)\n    speaker_embedding = tf.tile(speaker_embedding, [input_shape[0], 1])\n    speaker_embedding = tf.reshape(speaker_embedding, [input_shape[0], 1, -1])\n    speaker_embedding = tf.transpose(speaker_embedding, [0, 2, 1])\n    x = tf.concat([x, speaker_embedding], axis=dim_index)\n    return x\n\n", "CodeBleu_score": 31.32459572953349}, {"code": "def add_speaker_embedding(self, x, speaker_embedding)\n\t\n    # Save the dimensions of the input tensor as human-readable names\n    batch_size, seq_len, _ = x.shape\n    # Determine the appropriate dimension index for the speaker embedding\n    speaker_dim_idx = self.speaker_embedding.shape.rank - 1\n    # Repeat the speaker embedding to match the length of the input text\n    speaker_embedding = tf.tile(self.speaker_embedding, [batch_size, seq_len, 1])\n    # Reshape and transpose the speaker embedding\n    speaker_embedding = tf.reshape(\n        speaker_embedding, [batch_size, seq_len, self.speaker_embedding.shape[-1]]\n    )\n    speaker_embedding = tf.transpose(speaker_embedding, [0, 2, 1])\n    # Concatenate the speaker embedding with the input tensor along the last dimension\n    x = tf.concat([x, speaker_embedding], axis=-1)\n    return x\n\n", "CodeBleu_score": 33.82402538879554}]}
{"instance_id": "e6742ea0fe5e", "generation_result": [{"code": "def init_attention(self, encoder_seq_proj)\n\t\n    self.device = self.encoder_seq_proj.weight.device\n    self.batch_size = encoder_seq_proj.size(0)\n    self.seq_len = encoder_seq_proj.size(1)\n    self.cum_attention = torch.zeros(self.batch_size, self.seq_len, device=self.device)\n    self.attention = torch.zeros(self.batch_size, self.seq_len, device=self.device)\n\n", "CodeBleu_score": 23.462457721205936}, {"code": "def init_attention(self, encoder_seq_proj)\n\t\n    self.device = self.parameters().__next__().device\n    self.batch_size = encoder_seq_proj.shape[0]\n    self.seq_len = encoder_seq_proj.shape[1]\n    self.cumulative_attention = torch.zeros(self.batch_size, self.seq_len, device=self.device)\n    self.attention = torch.zeros(self.batch_size, self.seq_len, device=self.device)\n\n", "CodeBleu_score": 24.32843348614446}, {"code": "def init_attention(self, encoder_seq_proj)\n\t\n    self.device = next(self.parameters()).device\n    self.batch_size = encoder_seq_proj.size(0)\n    self.seq_len = encoder_seq_proj.size(1)\n    self.cumulative = torch.zeros(self.batch_size, self.seq_len, device=self.device)\n    self.attention = torch.zeros(self.batch_size, self.seq_len, device=self.device)\n\n", "CodeBleu_score": 25.999060516472124}]}
{"instance_id": "4e1fc1ed0ed7", "generation_result": [{"code": "def load(self, path, optimizer=None)\n\t\n    device = next(self.parameters()).device\n    checkpoint = torch.load(str(path), map_location=device)\n    self.load_state_dict(checkpoint[\"model_state\"])\n\n    if \"optimizer_state\" in checkpoint and optimizer is not None:\n        optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n\n", "CodeBleu_score": 67.98839425729693}, {"code": "def load(self, path, optimizer=None)\n\t\n    device = next(self.parameters()).device\n    checkpoint = torch.load(str(path), map_location=device)\n    self.load_state_dict(checkpoint[\"model_state\"])\n\n    if \"optimizer_state\" in checkpoint and optimizer is not None:\n        optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n\n", "CodeBleu_score": 67.98839425729693}, {"code": "def load(self, path, optimizer=None)\n\t\n    # Use device of model params as location for loaded state\n    device = next(self.parameters()).device\n    checkpoint = torch.load(str(path), map_location=device)\n    self.load_state_dict(checkpoint[\"model_state\"])\n\n    if \"optimizer_state\" in checkpoint and optimizer is not None:\n        optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n\n", "CodeBleu_score": 67.98839425729693}]}
{"instance_id": "178e6aa16c0f", "generation_result": [{"code": "def save(self, path, optimizer=None)\n\t\n    if optimizer is not None:\n        torch.save({\n            \"model_state\": self.state_dict(),\n            \"optimizer_state\": optimizer.state_dict(),\n        }, str(path))\n    else:\n        torch.save({\n            \"model_state\": self.state_dict(),\n        }, str(path))\n", "CodeBleu_score": 93.4714524643972}, {"code": "def save(self, path, optimizer=None)\n\t\n    if optimizer is not None:\n        torch.save({\n            \"model_state\": self.state_dict(),\n            \"optimizer_state\": optimizer.state_dict(),\n        }, str(path))\n    else:\n        torch.save({\n            \"model_state\": self.state_dict(),\n        }, str(path))\n", "CodeBleu_score": 93.4714524643972}, {"code": "def save(self, path, optimizer=None)\n\t\n    if optimizer is not None:\n        torch.save({\n            \"model_state\": self.state_dict(),\n            \"optimizer_state\": optimizer.state_dict(),\n        }, str(path))\n    else:\n        torch.save({\n            \"model_state\": self.state_dict(),\n        }, str(path))", "CodeBleu_score": 93.4714524643972}]}
{"instance_id": "6528fd80e87b", "generation_result": [{"code": "def english_cleaners(text)\n\t\n    text = convert_to_ascii(text)\n    text = lowercase(text)\n    text = expand_numbers(text)\n    text = expand_abbreviations(text)\n    text = collapse_whitespace(text)\n    return text", "CodeBleu_score": 55.88611500274403}, {"code": "def english_cleaners(text)\n\t\n    text = convert_to_ascii(text)\n    text = lowercase(text)\n    text = expand_numbers(text)\n    text = expand_abbreviations(text)\n    text = collapse_whitespace(text)\n    return text\n\n", "CodeBleu_score": 55.88611500274403}, {"code": "def english_cleaners(text)\n\t\n    text = convert_to_ascii(text)\n    text = lowercase(text)\n    text = expand_numbers(text)\n    text = expand_abbreviations(text)\n    text = collapse_whitespace(text)\n    return text\n\n", "CodeBleu_score": 55.88611500274403}]}
{"instance_id": "8a4fe08e0985", "generation_result": [{"code": "def text_to_sequence(text, cleaner_names)\n\t\n    sequence = []\n    # Check for curly braces and treat their contents as ARPAbet:\n    while len(text):\n        m = _curly_re.match(text)\n        if not m:\n            sequence += _symbols_to_sequence(_clean_text(text, cleaner_names))\n            break\n        sequence += _arpabet_to_sequence(m.group(1))\n        text = m.group(2)\n    return sequence + [_eos_id]\n\n", "CodeBleu_score": 57.213824034460735}, {"code": "def text_to_sequence(text, cleaner_names)\n\t\n    text = _clean_text(text, cleaner_names)\n    sequences = []\n    for line in text.split(\"", "CodeBleu_score": 5.9366391506203895}, {"code": "def text_to_sequence(text, cleaner_names)\n\t\n    sequence = []\n    # Check for curly braces and treat their contents as ARPAbet notation if present.\n    if len(text) == 0:\n        return sequence\n    text = text.lower()\n    text = re.sub(\"[^{}]\".format(cleaners.valid_arpabet()), \"\", text)\n    arpabet_text = re.findall(\"{([^}]+)}\", text)\n    for i in range(len(arpabet_text)):\n        arpabet_text[i] = _arpabet_to_sequence(arpabet_text[i])\n    start_new_sequence = True\n    # Add a space to the beginning of the text to simplify regex matching\n    text = \" \" + text\n    while len(text):\n        m = re.match(r\"\\s*([^\\s{}]+)\", text)\n        # No text before the special symbol\n        if not m:\n            raise Exception(\n                \"Special symbol in the wrong place in text: \" + text + \" at \" + str(i)\n            )\n        sequence += _clean_text(m.group(1), cleaner_names)\n        text = text[m.end():]\n        m = re.match(r\"\\s*{\\s*([^}]+)\\s*}\", text)\n        if m:\n            text = text[m.end():]\n            # Add ARPAbet sequence to the end\n            sequence += arpabet_text[0]\n            arpabet_text = arpabet_text[1:]\n        m = re.match(r\"\\s*}\\s*\", text)\n        if m:\n            text = text[m.end():]\n            if start_new_sequence:\n                sequence += _clean_text(\" \", cleaner_names)\n            start_new_sequence = True\n        else:\n            start_new_sequence = False\n    # Add end-of-sequence token\n    sequence += _clean_text(\"~\", cleaner_names)\n    return sequence\n\n", "CodeBleu_score": 33.1797573939556}]}
{"instance_id": "e1c25a37af1f", "generation_result": [{"code": "def sequence_to_text(sequence)\n\t\n    result = \"\"\n    for symbol_id in sequence:\n        if symbol_id!= 0:\n            result += f\"{{{symbols[symbol_id]}}}\"\n        else:\n            result += \" \"\n    result = result.replace(\"} {\", \" \")\n    return result\n\n", "CodeBleu_score": 31.4792011432639}, {"code": "def sequence_to_text(sequence)\n\t\n    result = ''\n    for id in sequence:\n        if id!= 0:\n            result += dictionary[id] +''\n    return result\n\n", "CodeBleu_score": 14.938457096261828}, {"code": "def sequence_to_text(sequence)\n\t\n    result = \"\"\n    for symbol in sequence:\n        if symbol in arpabet_dict:\n            result += \"{\" + arpabet_dict[symbol] + \"}\"\n        else:\n            result += symbol\n    result = re.sub(r'\\{([^\\}]+)\\}\\{([^\\}]+)\\}', r'{\\1 \\2}', result)\n    return result\n\n", "CodeBleu_score": 32.22760610229994}]}
{"instance_id": "7807cad4b05d", "generation_result": [{"code": "def embed_utterance(fpaths, encoder_model_fpath)\n\t\n    if not encoder.is_loaded():\n        encoder.load_model(encoder_model_fpath)\n\n    # Compute the speaker embedding of the utterance\n    wav_fpath, embed_fpath = fpaths\n    wav = np.load(wav_fpath)\n    wav = encoder.preprocess_wav(wav)\n    embed = encoder.embed_utterance(wav)\n    np.save(embed_fpath, embed, allow_pickle=False)\n\n", "CodeBleu_score": 95.3261531719067}, {"code": "def embed_utterance(fpaths, encoder_model_fpath)\n\t\n    if not encoder.is_loaded():\n        encoder.load_model(encoder_model_fpath)\n\n    # Compute the speaker embedding of the utterance\n    wav_fpath, embed_fpath = fpaths\n    wav = np.load(wav_fpath)\n    wav = encoder.preprocess_wav(wav)\n    embed = encoder.embed_utterance(wav)\n    np.save(embed_fpath, embed, allow_pickle=False)\n\n", "CodeBleu_score": 95.3261531719067}, {"code": "def embed_utterance(fpaths, encoder_model_fpath)\n\t\n    if not encoder.is_loaded():\n        encoder.load_model(encoder_model_fpath)\n\n    # Compute the speaker embedding of the utterance\n    wav_fpath, embed_fpath = fpaths\n    wav = np.load(wav_fpath)\n    wav = encoder.preprocess_wav(wav)\n    embed = encoder.embed_utterance(wav)\n    np.save(embed_fpath, embed, allow_pickle=False)\n\n", "CodeBleu_score": 95.3261531719067}]}
{"instance_id": "9f860273a325", "generation_result": [{"code": "def create_embeddings(synthesizer_root: Path, encoder_model_fpath: Path, n_processes: int)\n\t\n    # Check if the synthesizer root directory exists\n    if not synthesizer_root.exists():\n        raise FileNotFoundError(f\"Synthesizer root directory not found: {synthesizer_root}\")\n\n    # Check if the encoder model file exists\n    if not encoder_model_fpath.exists():\n        raise FileNotFoundError(f\"Encoder model file not found: {encoder_model_fpath}\")\n\n    # Load the encoder model\n    encoder = load_model(encoder_model_fpath)\n\n    # Read the metadata file\n    metadata_fpath = synthesizer_root / \"train.txt\"\n    if not metadata_fpath.exists():\n        raise FileNotFoundError(f\"Metadata file not found: {metadata_fpath}\")\n    metadata = pd.read_csv(metadata_fpath, sep=\"|\")\n\n    # Prepare the file paths for audio and embeddings\n    audio_dir = synthesizer_root / \"wavs\"\n    embeddings_dir = synthesizer_root / \"embeddings\"\n    embeddings_dir.mkdir(parents=True, exist_ok=True)\n    audio_fpaths = [audio_dir / f\"{row['wav_id']}.wav\" for _, row in metadata.iterrows()]\n    embeddings_fpaths = [embeddings_dir / f\"{row['wav_id']}.npy\" for _, row in metadata.iterrows()]\n\n    # Create a multiprocessing pool\n    with multiprocessing.Pool(n_processes) as pool:\n        # Create a progress bar\n        pbar = tqdm(total=len(audio_fpaths))\n\n        # Define a callback function to update the progress bar\n        def update_progress(result):\n            pbar.update(1)\n\n        # Process the audio files in parallel, generating embeddings\n        for audio_fpath, embeddings_fpath in zip(audio_fpaths, embeddings_fpaths):\n            pool.apply_async(\n                generate_embedding,\n                args=(audio_fpath, embeddings_fpath, encoder),\n                callback=update_progress,\n            )\n\n        # Wait for all tasks to finish\n        pool.close()\n        pool.join()\n\n", "CodeBleu_score": 31.8508445053258}, {"code": "def create_embeddings(synthesizer_root: Path, encoder_model_fpath: Path, n_processes: int)\n\t\n    # Create the output directory if it doesn't exist\n    os.makedirs(synthesizer_root, exist_ok=True)\n    # Load the encoder model\n    encoder = Encoder(encoder_model_fpath)\n    # Load the metadata\n    metadata_fpath = synthesizer_root / \"metadata.csv\"\n    metadata_df = pd.read_csv(metadata_fpath)\n    # Prepare file paths for audio and embeddings\n    audio_fpaths = [synthesizer_root / row[\"id\"] for _, row in metadata_df.iterrows()]\n    embedding_fpaths = [synthesizer_root / f\"{row['id']}.npy\" for _, row in metadata_df.iterrows()]\n    # Create a multiprocessing pool\n    pool = mp.Pool(n_processes)\n    # Create a progress bar\n    progress_bar = tqdm(total=len(audio_fpaths), desc=\"Creating embeddings\", position=0, leave=True)\n    # Define a function to create embeddings for a single audio file\n    def create_embedding(audio_fpath, embedding_fpath):\n        # Load the audio file\n        audio, _ = librosa.load(audio_fpath, sr=16000)\n        # Generate the embedding\n        embedding = encoder.embed_utterance(audio)\n        # Save the embedding to a file\n        np.save(embedding_fpath, embedding)\n        # Update the progress bar\n        progress_bar.update(1)\n    # Create embeddings for all audio files in parallel\n    _ = pool.starmap(create_embedding, zip(audio_fpaths, embedding_fpaths))\n    # Close the progress bar\n    progress_bar.close()\n    # Close the multiprocessing pool\n    pool.close()\n\n", "CodeBleu_score": 33.19255247989553}, {"code": "def create_embeddings(synthesizer_root: Path, encoder_model_fpath: Path, n_processes: int)\n\t\n    # Define the paths to the required directories and files\n    audio_dir = synthesizer_root / \"audio\"\n    embeddings_dir = synthesizer_root / \"embeddings\"\n    metadata_fpath = synthesizer_root / \"metadata.csv\"\n\n    # Verify the existence of required directories and files\n    if not audio_dir.exists():\n        raise FileNotFoundError(f\"Audio directory not found: {audio_dir}\")\n    if not embeddings_dir.exists():\n        raise FileNotFoundError(f\"Embeddings directory not found: {embeddings_dir}\")\n    if not metadata_fpath.exists():\n        raise FileNotFoundError(f\"Metadata file not found: {metadata_fpath}\")\n\n    # Read the metadata file and prepare file paths for audio and embeddings\n    metadata = pd.read_csv(metadata_fpath)\n    audio_fpaths = [audio_dir / f\"{row['id']}.wav\" for _, row in metadata.iterrows()]\n    embeddings_fpaths = [embeddings_dir / f\"{row['id']}.npy\" for _, row in metadata.iterrows()]\n\n    # Create a multiprocessing pool to process the audio files in parallel\n    with multiprocessing.Pool(n_processes) as pool:\n        # Define a function to generate embeddings for a single audio file\n        def generate_embedding(audio_fpath, embeddings_fpath, encoder_model_fpath):\n            # Load the audio file\n            audio, sample_rate = librosa.load(audio_fpath, sr=22050)\n\n            # Generate the embedding using the specified encoder model\n            embedding = encoder.embed_utterance(encoder_model_fpath, audio)\n\n            # Save the embedding as a NumPy array\n            np.save(embeddings_fpath, embedding)\n\n        # Track progress using a progress bar\n        progress_bar = tqdm(total=len(audio_fpaths), desc=\"Generating embeddings\")\n\n        # Process the audio files in parallel, generating embeddings using a specified encoder model\n        for _ in pool.imap_unordered(generate_embedding, audio_fpaths, embeddings_fpaths, repeat(encoder_model_fpath)):\n            progress_bar.update(1)\n\n        # Close the progress bar\n        progress_bar.close()\n\n", "CodeBleu_score": 32.992652232764534}]}
{"instance_id": "9eb0abda14a9", "generation_result": [{"code": "def eval_model(attention, mel_prediction, target_spectrogram, input_seq, step,\n               plot_dir, mel_output_dir, wav_dir, sample_num, loss, hparams)\n\t\n    if not os.path.exists(plot_dir):\n        os.makedirs(plot_dir)\n    if not os.path.exists(mel_output_dir):\n        os.makedirs(mel_output_dir)\n    if not os.path.exists(wav_dir):\n        os.makedirs(wav_dir)\n\n    # Save attention plot\n    if attention is not None:\n        save_attention(attention, os.path.join(plot_dir, \"step_{}_sample_{}\".format(step, sample_num)))\n\n    # Save predicted mel spectrogram\n    plot_spectrogram(mel_prediction.T, os.path.join(mel_output_dir, \"step_{}_sample_{}\".format(step, sample_num)),\n                     title=\"Mel Spectrogram, Step {}, Loss {:.4f}\".format(step, loss),\n                     target_spectrogram=target_spectrogram.T, max_len=hparams.max_iters)\n\n    # Save reconstructed wav\n    wav = spectrogram2wav(mel_prediction.T)\n    write_wav(wav, os.path.join(wav_dir, \"step_{}_sample_{}\".format(step, sample_num)),\n              hparams.sample_rate)\n\n    # Print input sequence\n    print(\"Step {}, Loss {:.4f}: {}\".format(step, loss, sequence_to_text(input_seq)))\n\n", "CodeBleu_score": 38.30177324706767}, {"code": "def eval_model(attention, mel_prediction, target_spectrogram, input_seq, step,\n               plot_dir, mel_output_dir, wav_dir, sample_num, loss, hparams)\n\t\n    # plot and save attention\n    if attention is not None:\n        plot_attention(attention, os.path.join(plot_dir, \"attention_step_{}.png\".format(step)))\n\n    # save predicted mel spectrogram\n    plot_spectrogram(mel_prediction, os.path.join(mel_output_dir, \"mel_step_{}.png\".format(step)),\n                     title=\"Predicted Mel-Spectrogram\", split_title=True, max_len=hparams.max_iters)\n\n    # save reconstructed wav\n    audio = inv_mel_spectrogram(mel_prediction.T, hparams)\n    audio_filename = os.path.join(wav_dir, \"reconstructed_step_{}.wav\".format(step))\n    save_wav(audio, audio_filename)\n\n    # plot and save predicted vs. target mel spectrogram\n    plot_spectrogram(mel_prediction, os.path.join(plot_dir, \"predicted_step_{}.png\".format(step)),\n                     title=\"Predicted vs. Target Mel-Spectrogram\", target_spectrogram=target_spectrogram,\n                     split_title=True, max_len=hparams.max_iters)\n\n    # print input sequence\n    print(\"Input: {}\".format(sequence_to_text(input_seq)))\n\n", "CodeBleu_score": 38.88640624005198}, {"code": "def eval_model(attention, mel_prediction, target_spectrogram, input_seq, step,\n               plot_dir, mel_output_dir, wav_dir, sample_num, loss, hparams)\n\t\n    # Save attention plot\n    save_attention(attention, os.path.join(plot_dir, \"attention_step_{}_sample_{}\".format(step, sample_num)))\n    # Save predicted mel spectrogram\n    save_spectrogram(mel_prediction, os.path.join(mel_output_dir, \"mel_step_{}_sample_{}\".format(step, sample_num)))\n    # Save reconstructed waveform\n    save_wav(wav_from_spectrogram(mel_prediction), os.path.join(wav_dir, \"reconstructed_step_{}_sample_{}\".format(step, sample_num)))\n    # Save predicted vs. target mel spectrogram\n    plot_spectrogram(mel_prediction, os.path.join(plot_dir, \"predicted_step_{}_sample_{}\".format(step, sample_num)),\n                     title=\"Predicted vs. Target Mel Spectrogram", "CodeBleu_score": 26.575444610737815}]}
{"instance_id": "6bad4f500286", "generation_result": [{"code": "def num_params(self)\n\t\n    total_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n    print(f\"Total number of trainable parameters: {total_params / 1000000:.3f}M\")\n\n", "CodeBleu_score": 14.969496047282767}, {"code": "def num_params(self)\n\t\n    total_params = 0\n    for param in self.parameters():\n        if param.requires_grad:\n            num_params = torch.prod(torch.tensor(param.shape))\n            total_params += num_params\n    return print(f\"There are {total_params} parameters in the model.\")\n\n", "CodeBleu_score": 24.650394455750035}, {"code": "def num_params(self)\n\t\n    return sum(p.numel() for p in self.parameters() if p.requires_grad) / 1e6\n\n", "CodeBleu_score": 5.637300874658717}]}
{"instance_id": "78b876054bd4", "generation_result": [{"code": "def pad_tensor(self, x, pad, side='both')\n\t\n    if side == 'both':\n        return torch.cat([pad_tensor(x, pad, side='before'), pad_tensor(x, pad, side='after')], dim=1)\n    elif side == 'after':\n        return torch.cat([x, torch.zeros(x.shape[0], pad).to(x.device)], dim=1)\n    elif side == 'before':\n        return torch.cat([torch.zeros(x.shape[0], pad).to(x.device), x], dim=1)\n    else:\n        raise ValueError('Side must be either \"before\", \"after\", or \"both\".')\n\n", "CodeBleu_score": 23.758677959839385}, {"code": "def pad_tensor(self, x, pad, side='both')\n\t\n    if pad == 0:\n        return x\n    if side == 'both':\n        pad_left = pad_right = pad\n    elif side == 'before':\n        pad_left = pad\n        pad_right = 0\n    elif side == 'after':\n        pad_left = 0\n        pad_right = pad\n    else:\n        raise ValueError('Side must be one of \"before\", \"after\", or \"both\".')\n    return torch.nn.functional.pad(x, (pad_left, pad_right), 'constant', 0)\n\n", "CodeBleu_score": 24.39501146726699}, {"code": "def pad_tensor(self, x, pad, side='both')\n\t\n    if side == 'before':\n        padded_tensor = torch.zeros(x.size(0), pad, x.size(2)).fill_(self.pad_value)\n        padded_tensor = torch.cat((padded_tensor, x), dim=1)\n    elif side == 'after':\n        padded_tensor = torch.zeros(x.size(0), pad, x.size(2)).fill_(self.pad_value)\n        padded_tensor = torch.cat((x, padded_tensor), dim=1)\n    elif side == 'both':\n        padded_tensor = torch.zeros(x.size(0), pad, x.size(2)).fill_(self.pad_value)\n        padded_tensor = torch.cat((padded_tensor, x, padded_tensor), dim=1)\n    else:\n        raise ValueError('Side must be either \"before\", \"after\", or \"both\"')\n    return padded_tensor\n\n", "CodeBleu_score": 29.97315101201665}]}
{"instance_id": "a7ed1715a3df", "generation_result": [{"code": "def save_attention(attn, path)\n\t    import matplotlib.pyplot as plt\n    fig = plt.figure(figsize=(10, 10))\n    ax = fig.add_subplot(111)\n    cax = ax.matshow(attn, cmap='bone')\n    fig.colorbar(cax)\n    plt.savefig(path + '.png')\n    plt.close(fig)\n\n", "CodeBleu_score": 51.93000263799167}, {"code": "def save_attention(attn, path)\n\t    import matplotlib.pyplot as plt\n    fig = plt.figure(figsize=(10, 10))\n    plt.imshow(attn, cmap='gray')\n    plt.savefig(path + '.png')\n    plt.close(fig)\n\n", "CodeBleu_score": 49.26511547816859}, {"code": "def save_attention(attn, path)\n\t    import matplotlib.pyplot as plt\n    fig = plt.figure(figsize=(10, 10))\n    ax = fig.add_subplot(111)\n    cax = ax.matshow(attn, cmap='bone')\n    fig.colorbar(cax)\n    plt.savefig(path)\n    plt.close(fig)\n\n", "CodeBleu_score": 47.037029442722144}]}
{"instance_id": "19375c9f44a2", "generation_result": [{"code": "def plot(array)\n\t\n    import matplotlib.pyplot as plt\n\n    fig = plt.figure(figsize=(30, 5))\n    ax = fig.add_subplot(111)\n    ax.xaxis.label.set_color('grey')\n    ax.yaxis.label.set_color('grey')\n    ax.xaxis.label.set_fontsize(23)\n    ax.yaxis.label.set_fontsize(23)\n    ax.tick_params(axis='x', colors='grey', labelsize=23)\n    ax.tick_params(axis='y', colors='grey', labelsize=23)\n    plt.plot(array)\n\n", "CodeBleu_score": 77.73415655580472}, {"code": "def plot(array)\n\t\n    import matplotlib.pyplot as plt\n\n    fig = plt.figure(figsize=(30, 5))\n    ax = fig.add_subplot(111)\n    ax.xaxis.label.set_color('grey')\n    ax.yaxis.label.set_color('grey')\n    ax.xaxis.label.set_fontsize(23)\n    ax.yaxis.label.set_fontsize(23)\n    ax.tick_params(axis='x', colors='grey', labelsize=23)\n    ax.tick_params(axis='y', colors='grey', labelsize=23)\n    plt.plot(array)\n\n", "CodeBleu_score": 77.73415655580472}, {"code": "def plot(array)\n\t\n    import matplotlib.pyplot as plt\n\n    fig = plt.figure(figsize=(30, 5))\n    ax = fig.add_subplot(111)\n    ax.xaxis.label.set_color('grey')\n    ax.yaxis.label.set_color('grey')\n    ax.xaxis.label.set_fontsize(23)\n    ax.yaxis.label.set_fontsize(23)\n    ax.tick_params(axis='x', colors='grey', labelsize=23)\n    ax.tick_params(axis='y', colors='grey', labelsize=23)\n    plt.plot(array)\n\n", "CodeBleu_score": 77.73415655580472}]}
{"instance_id": "ff94c425920c", "generation_result": [{"code": "def write_to_csv(image_name, prediction, confidence)\n\t\n    # create a dictionary with the image name, prediction, and confidence\n    data = {\"image_name\": image_name, \"prediction\": prediction, \"confidence\": confidence}\n    # open the CSV file in append mode\n    with open(\"predictions.csv\", \"a\") as csvfile:\n        # write the header if the file is new\n        if os.stat(\"predictions.csv\").st_size == 0:\n            fieldnames = [\"image_name\", \"prediction\", \"confidence\"]\n            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n            writer.writeheader()\n        # write the data as a new row\n        writer.writerow(data)\n\n", "CodeBleu_score": 46.88847294229503}, {"code": "def write_to_csv(image_name, prediction, confidence)\n\t\n    data = {\"image_name\": image_name, \"prediction\": prediction, \"confidence\": confidence}\n    with open(\"predictions.csv\", \"a\") as f:\n        writer = csv.DictWriter(f, fieldnames=data.keys())\n        if not os.path.isfile(\"predictions.csv\"):\n            writer.writeheader()\n        writer.writerow(data)\n\n", "CodeBleu_score": 62.77432452258221}, {"code": "def write_to_csv(image_name, prediction, confidence)\n\t\n    data = {\n        \"image_name\": image_name,\n        \"prediction\": prediction,\n        \"confidence\": confidence,\n    }\n    with open(\"predictions.csv\", \"a\") as csvfile:\n        writer = csv.DictWriter(csvfile, fieldnames=data.keys())\n        if os.stat(\"predictions.csv\").st_size == 0:\n            writer.writeheader()\n        writer.writerow(data)\n\n", "CodeBleu_score": 50.65773684659066}]}
{"instance_id": "436161602009", "generation_result": [{"code": "def export_torchscript(model, im, file, optimize, prefix=colorstr(\"TorchScript:\"))\n\t\n    # Export model to TorchScript\n    if not file.endswith(\".pt\"):\n        file += \".pt\"\n    LOGGER.info(f\"{prefix} Exporting {file}...\")\n    try:\n        if optimize:\n            # https://pytorch.org/docs/stable/jit.html#torch.jit.trace\n            LOGGER.info(f\"{prefix} Optimizing TorchScript model...\")\n            model.fuse()\n            model.eval()\n            model.model[-1].export = True  # onnx export\n            with torch.no_grad():\n                traced_script_module = torch.jit.trace(model, im)  # trace model\n            traced_script_module.save(file)  # save model\n            LOGGER.info(f\"{prefix} Optimized TorchScript model saved as {file}\")\n        else:\n            # https://pytorch.org/docs/stable/jit.html#torch.jit.script\n            LOGGER.info(f\"{prefix} Converting model to TorchScript...\")\n            script_model = torch.jit.script(model)  # convert to TorchScript\n            script_model.save(file)  # save model\n            LOGGER.info(f\"{prefix} TorchScript model saved as {file}\")\n    except Exception as e:\n        LOGGER.info(f\"{prefix} TorchScript export failed! {e}\")\n        return None\n    # Save model details\n    with open(file.replace(\".pt\", \"_info.txt\"), \"w\") as f:\n        f.write(str(model))\n    return file\n\n", "CodeBleu_score": 30.28370761882607}, {"code": "def export_torchscript(model, im, file, optimize, prefix=colorstr(\"TorchScript:\"))\n\t\n    from torch.jit import trace_module\n\n    # Log\n    LOGGER.info(f\"{prefix} Exporting model to TorchScript format...\")\n    LOGGER.info(f\"{prefix} Input image size: {im.shape}\")\n\n    # Trace model\n    LOGGER.info(f\"{prefix} Tracing model...\")\n    model.eval()\n    model.model[-1].export = True\n    model.model[-1].names = model.names\n    model.model[-1].stride = model.stride\n    model.model[-1].nc = model.nc\n    model.model[-1].class_weights = model.class_weights\n    model.model[-1].anchors = model.anchors\n    model.model[-1].anchor_grid = model.anchor_grid\n    model.model[-1].anchor_grid_F = model.anchor_grid_F\n    model.model[-1].anchor_vec = model.anchor_vec\n    model.model[-1].anchor_wh = model.anchor_wh\n    model.model[-1].anchor_area = model.anchor_area\n    model.model[-1].anchor_n = model.anchor_n\n    model.model[-1].grid_x = model.grid_x\n    model.model[-1].grid_y = model.grid_y\n    model.model[-1].grid_F = model.grid_F\n    model.model[-1].ng = model.ng\n    model.model[-1].nx = model.nx\n    model.model[-1].ny = model.ny\n    model.model[-1].no = model.no\n    model.model[-1].na = model.na\n    model.model[-1].nc = model.nc\n    model.model[-1].nl = model.nl\n    model.model[-1].na = model.na\n    model.model[-1].anchor_bias = model.anchor_bias\n    model.model[-1].anchor_bias_F = model.anchor_bias_F\n    model.model[-1].anchor_bias_T = model.anchor_bias_T\n    model.model[-1].anchor_target = model.anchor_target\n    model.model[-1].anchor_target_F = model.anchor_target_F\n    model.model[-1].anchor_target_T = model.anchor_target_T\n    model.model[-1].output = model.output", "CodeBleu_score": 25.08774305666328}, {"code": "def export_torchscript(model, im, file, optimize, prefix=colorstr(\"TorchScript:\"))\n\t\n    if not torch.cuda.is_available():\n        LOGGER.warning(f\"{prefix} CUDA is not available, falling back to CPU.\")\n    model.to(torch.device(\"cpu\"))\n    model.eval()\n    if optimize:\n        model = torch.jit.optimize_for_mobile(model)\n    torch.jit.save(model, file)\n    LOGGER.info(f\"{prefix} Exported model to {file}.\")\n    return file, None\n\n", "CodeBleu_score": 25.393464590863644}]}
{"instance_id": "78335487ac1c", "generation_result": [{"code": "def export_coreml(model, im, file, int8, half, nms, prefix=colorstr(\"CoreML:\")):\n\t    check_requirements([\"coremltools\"])\n    LOGGER.info(f\"{prefix} Exporting model to CoreML format...\")\n    if nms:\n        model = iOSModel(model, im.shape[1:], int8, half, nms)\n    model.eval()\n    if isinstance(im, torch.Tensor):\n        im = im.numpy()\n    elif isinstance(im, PIL.Image.Image):\n        im = np.array(im)\n    with torch.no_grad():\n        model.forward(torch.from_numpy(im).unsqueeze(0))\n    if isinstance(model, iOSModel):\n        model.model.eval()\n    model.model.eval()\n    torch.onnx.export(\n        model.model,\n        torch.randn(1, 3, *im.shape[1:]),\n        file,\n        export_params=True,\n        verbose=False,\n        opset_version=11,\n        input_names=[\"image\"],\n        output_names=[\"classes\", \"boxes\"],\n        dynamic_axes={\"image\": {0: \"batch_size\"}, \"classes\": {0: \"batch_size\"}, \"boxes\": {0: \"batch_size\"}},\n    )\n    coreml_model = coremltools.models.MLModel(file)\n    if int8:\n        coreml_model = coremltools.models.quantization.quantize_weights(coreml_model, nbits=8, mode=\"linear\")\n    if half:\n        coreml_model = coremltools.models.quantization.quantize_weights(coreml_model, nbits=16, mode=\"linear\")\n    coreml_model.save(file)\n    LOGGER.info(f\"{prefix} Done exporting model to CoreML format.\")\n    return file, coreml_model\n\n", "CodeBleu_score": 38.11116266390913}, {"code": "def export_coreml(model, im, file, int8, half, nms, prefix=colorstr(\"CoreML:\")):\n\t    # Checks\n    check_requirements(\n        [\"coremltools\", \"torch\"],\n        [f\"Run '{sys.executable} -m pip install coremltools torch' to install.\"],\n    )\n    LOGGER.info(f\"{prefix} Exporting model to CoreML format...\")\n    # NMS wrapper\n    if nms:\n        LOGGER.info(f\"{prefix} Adding NMS wrapper to model...\")\n        model = iOSModel(model, im, nc=model.nc)\n    # Trace\n    LOGGER.info(f\"{prefix} Tracing model...\")\n    model.eval()\n    model.model.fuse()\n    model.model.eval()\n    x = torch.zeros(1, 3, im.shape[0], im.shape[1]).to(model.device)\n    model_traced = torch.jit.trace(model.model[0], x)\n    # Convert to CoreML\n    LOGGER.info(f\"{prefix} Converting to CoreML format...\")\n    model_coreml = ct.convert(model_traced, inputs=[ct.ImageType(name=\"input_1\", shape=x.shape)])\n    # Quantize\n    if int8:\n        if not ct.utils.is_macos_version(\">=12.0\") and not ct.utils.is_macos_version(\"<11.0\"):\n            LOGGER.info(f\"{prefix} Quantizing model...\")\n            model_coreml = ct.utils.quantization.quantize_weights(model_coreml, mode=\"linear\")\n        else:\n            LOGGER.warning(\n                f\"{prefix} Quantization not supported on this version of macOS, skipping...\"\n            )\n    # Half precision\n    if half:\n        if ct.utils.is_macos_version(\">=12.0\"):\n            LOGGER.info(f\"{prefix} Converting to half precision...\")\n            model_coreml = ct.utils.convert_neural_network_weights_to_fp16(model_coreml)\n        else:\n            LOGGER.warning(\n                f\"{prefix} Half precision not supported on this version of macOS, skipping...\"\n            )\n    # Save\n    file = Path(file).with_suffix(\".mlmodel\")\n    LOGGER.info(f\"{prefix} Saving model to {file}...\")\n    model_coreml.save(file)\n    LOGGER.info(f\"{prefix} Done! Exported model to {file}.", "CodeBleu_score": 40.331993113164316}, {"code": "def export_coreml(model, im, file, int8, half, nms, prefix=colorstr(\"CoreML:\")):\n\t    # Check for coremltools\n    check_requirements([\"coremltools\"])\n    # Log start\n    LOGGER.info(f\"{prefix} Starting export with onnx to CoreML conversion...\")\n    # Trace model\n    model.model.fuse()  # fuse\n    model.model.eval()  # eval\n    model.model.model[-1].export = True  # set export flag\n    img = torch_utils.img_size_to_torch(im.size)  # image size\n    img = torch.zeros((1, 3) + img)  # image tensor\n    img = img.to(next(model.parameters()).device)  # to device\n    model.model(img)  # run once\n    # Convert to CoreML\n    model.model = torch2coreml(model.model, img, img, onnx_coreml=True)  # to CoreML\n    if model.model is None:\n        return None, None\n    # Apply quantization\n    if int8:\n        model.model = quantization.quantize_weights(model.model, mode=\"int8\", rounding_mode=\"nearest\")  # quantize\n        model.model.input_description[\"input_1\"] = quantization.InputTensorSettings(\n            scale=1 / 255.0, bias=-1, border={2, 2, 2, 2}\n        )  # input settings\n    elif half:\n        model.model = quantization.quantize_weights(model.model, mode=\"fp16\")  # quantize\n    # Wrap with iOSModel\n    if nms:\n        model.model = iOSModel(model.model, nms_iou=0.45, nms_conf=0.25, nms_type=\"greedy\")  # wrap with iOSModel\n    # Save model\n    file = file.replace(\".onnx\", \".mlmodel\")  # suffix\n    model.model.save(file)  # save\n    LOGGER.info(f\"{prefix} Exported model to {file}.\")\n    return file, model.model  # return file and model\n\n", "CodeBleu_score": 37.69413081556553}]}
{"instance_id": "80233cf71580", "generation_result": [{"code": "def generate_individual(input_ranges, individual_length)\n\t\n    individual = []\n    for i in range(individual_length):\n        individual.append(random.uniform(input_ranges[0], input_ranges[1]))\n    return individual\n\n", "CodeBleu_score": 45.11328150959641}, {"code": "def generate_individual(input_ranges, individual_length)\n\t\n    individual = []\n    for i in range(individual_length):\n        individual.append(random.uniform(input_ranges[0], input_ranges[1]))\n    return individual\n\n", "CodeBleu_score": 45.11328150959641}, {"code": "def generate_individual(input_ranges, individual_length)\n\t\n    individual = []\n    for i in range(individual_length):\n        individual.append(random.uniform(input_ranges[i][0], input_ranges[i][1]))\n    return individual\n\n", "CodeBleu_score": 47.630972577458174}]}
{"instance_id": "5ea979cb4a8e", "generation_result": [{"code": "def marginal_log_mean_coeff(self, t)\n\t\n    if self.schedule == 'discrete':\n        return torch.log(self.alpha_t_bar)\n    elif self.schedule == 'linear':\n        return torch.log(self.alpha_t_bar - (self.alpha_t_bar - self.alpha_min_bar) * t / self.t_max)\n    elif self.schedule == 'cosine':\n        return torch.log(self.alpha_min_bar + 0.5 * (self.alpha_t_bar - self.alpha_min_bar) * (1 + torch.cos(math.pi * t / self.t_max)))\n    else:\n        raise ValueError(f'Unknown schedule {self.schedule}')\n\n", "CodeBleu_score": 38.69911792416784}, {"code": "def marginal_log_mean_coeff(self, t)\n\t\n    if self.schedule_type == 'discrete':\n        return torch.log(self.interpolate_fn(t, self.schedule_t, self.schedule_alpha))\n    if self.schedule_type == 'linear':\n        return self.schedule_alpha_start + (self.schedule_alpha_end - self.schedule_alpha_start) * t / self.schedule_t_end\n    if self.schedule_type == 'cosine':\n        t_diff = t - self.schedule_t_start\n        t_diff_scaled = t_diff / (self.schedule_t_end - self.schedule_t_start) * np.pi\n        return self.schedule_alpha_start + (self.schedule_alpha_end - self.schedule_alpha_start) * (1.0 + torch.cos(t_diff_scaled)) / 2.0\n\n", "CodeBleu_score": 37.49445591555312}, {"code": "def marginal_log_mean_coeff(self, t)\n\t\n    if self.schedule == 'discrete':\n        return torch.log(torch.clamp(self.interpolate_fn(t, self.t_values, self.alpha_values), min=1e-06))\n    if self.schedule == 'linear':\n        return torch.log(torch.clamp(self.alpha_min + (self.alpha_max - self.alpha_min) * t / self.T, min=1e-06))\n    if self.schedule == 'cosine':\n        return torch.log(torch.clamp(self.alpha_min + (self.alpha_max - self.alpha_min) * 0.5 * (1 + torch.cos(torch.pi * t / self.T)), min=1e-06))\n    raise ValueError(f\"Schedule type {self.schedule} is not supported.\")\n\n", "CodeBleu_score": 38.538144458628}]}
{"instance_id": "7da4fae5a278", "generation_result": [{"code": "def inverse_lambda(self, lamb)\n\t\n    if self.schedule == \"linear\":\n        if self.beta_schedule == \"squaredcos_cap_v2\":\n            t = torch.acos(1 - 2 * lamb) / (math.pi * self.beta_final)\n        elif self.beta_schedule == \"squaredcos\":\n            t = torch.acos(1 - 2 * lamb) / (math.pi * self.beta_final)\n        else:\n            t = lamb / self.beta_final\n        return t\n    elif self.schedule == \"discrete\":\n        log_alpha = self.log_alpha_0 - lamb\n        t = torch.argmin(torch.abs(log_alpha.unsqueeze(1) - self.log_alphas_cumprod), dim=1).float()\n        return t\n    else:\n        log_alpha = self.log_alpha_0 - lamb\n        t = (torch.acos(torch.exp(log_alpha)) - self.min_log_alpha) / (self.max_log_alpha - self.min_log_alpha)\n        return t\n\n", "CodeBleu_score": 32.427097321150846}, {"code": "def inverse_lambda(self, lamb)\n\t\n    if self.schedule == \"linear\":\n        beta = self.beta_schedule(lamb)\n        t = torch.sqrt(1 - beta) / (lamb * torch.sqrt(beta))\n    elif self.schedule == \"discrete\":\n        log_alpha = self.log_alpha_schedule(lamb)\n        log_alpha_0 = self.log_alpha_schedule(0)\n        log_alpha_T = self.log_alpha_schedule(self.num_diffusion_timesteps)\n        t = (log_alpha - log_alpha_0) / (log_alpha_T - log_alpha_0) * self.num_diffusion_timesteps\n    else:\n        alpha = self.alpha_schedule(lamb)\n        t = torch.acos(torch.sqrt(alpha)) / math.pi * self.num_diffusion_timesteps\n    return t\n\n", "CodeBleu_score": 26.325385357323945}, {"code": "def inverse_lambda(self, lamb)\n\t\n    if self.schedule == \"linear\":\n        temp = (1 - self.beta_start) * lamb + self.beta_start * self.beta_end\n        t = torch.sqrt(temp / self.beta_end)\n    elif self.schedule == \"discrete\":\n        log_alpha = 2 * lamb - torch.log(torch.expm1(lamb))\n        t = torch.acos(log_alpha / math.log(self.num_diffusion_time_steps)) / math.pi\n    else:\n        log_alpha = 2 * lamb - torch.log(torch.expm1(lamb))\n        t = torch.acos(log_alpha / math.log(self.num_diffusion_time_steps)) / math.pi\n    return t\n\n", "CodeBleu_score": 25.141047563636437}]}
{"instance_id": "a519c853a2b2", "generation_result": [{"code": "def noise_pred_fn(x, t_continuous, cond=None)\n\tt_continuous = expand_dims(t_continuous, x.dim())\n    if model_type == \"ddpm\":\n        return noise_schedule.marginal_alpha(t_continuous) * x + noise_schedule.marginal_std(t_continuous) * noise_model(x, t_continuous, cond)\n    elif model_type == \"ddim\":\n        return noise_schedule.marginal_alpha(t_continuous) * x + noise_schedule.marginal_std(t_continuous) * noise_model(x, t_continuous, cond)\n    elif model_type == \"dpm\":\n        return noise_schedule.marginal_alpha(t_continuous) * x + noise_schedule.marginal_std(t_continuous) * noise_model(x, t_continuous, cond)\n    elif model_type == \"dpm_2\":\n        return noise_schedule.marginal_alpha(t_continuous) * x + noise_schedule.marginal_std(t_continuous) * noise_model(x, t_continuous, cond)\n    elif model_type == \"dpm_2\":\n        return noise_schedule.marginal_alpha(t_continuous) * x + noise_schedule.marginal_std(t_continuous) * noise_model(x, t_continuous, cond)\n    elif model_type == \"dpm_2\":\n        return noise_schedule.marginal_alpha(t_continuous) * x + noise_schedule.marginal_std(t_continuous) * noise_model(x, t_continuous, cond)\n    elif model_type == \"dpm_2\":\n        return noise_schedule.marginal_alpha(t_continuous) * x + noise_schedule.marginal_std(t_continuous) * noise_model(x, t_continuous, cond)\n    elif model_type == \"dpm_2\":\n        return noise_schedule.marginal_alpha(t_continuous) * x + noise_schedule.marginal_std(t_continuous) * noise_model(x, t_continuous, cond)\n    elif model_type == \"dpm_2\":\n        return noise_schedule.marginal_alpha(t_continuous) * x + noise_schedule.marginal_std(t_continuous) * noise_model(x, t_continuous", "CodeBleu_score": 21.95377152835423}, {"code": "def noise_pred_fn(x, t_continuous, cond=None)\n\tt = expand_dims(t_continuous, x.ndim)\n    if model_type == \"unet\":\n        return noise_pred_fn_unet(x, t, cond)\n    elif model_type == \"resunet\":\n        return noise_pred_fn_resunet(x, t, cond)\n    else:\n        raise NotImplementedError(f\"Model type {model_type} is not supported.\")", "CodeBleu_score": 7.082750679891303}, {"code": "def noise_pred_fn(x, t_continuous, cond=None)\n\tt_continuous = expand_dims(t_continuous, x.ndim)\n    if cond is not None:\n        cond = expand_dims(cond, x.ndim)\n        pred = model(x, t_continuous, cond)\n    else:\n        pred = model(x, t_continuous)\n    if model_type == 'unet':\n        pred = noise_schedule.marginal_alpha(t_continuous) * pred\n    elif model_type =='resnet':\n        pred = noise_schedule.marginal_alpha(t_continuous) * pred + noise_schedule.marginal_std(t_continuous) * x\n    return pred", "CodeBleu_score": 24.50433561654114}]}
{"instance_id": "96a9b7d54823", "generation_result": [{"code": "def dynamic_thresholding_fn(self, x0, t=None)\n\t    if t is None:\n        t = self.threshold\n    x0 = x0.reshape(x0.shape[0], -1)\n    x0_abs = torch.abs(x0)\n    if self.dynamic_threshold:\n        if self.dynamic_threshold == 'percentile':\n            x0_abs_threshold = torch.quantile(x0_abs, t, dim=1, keepdim=True)\n        elif self.dynamic_threshold =='mean':\n            x0_abs_threshold = torch.mean(x0_abs, dim=1, keepdim=True)\n        else:\n            raise ValueError('Invalid dynamic thresholding method')\n    else:\n        x0_abs_threshold = torch.tensor(t).to(x0.device).reshape(1, 1)\n    x0_abs_threshold = expand_dims(x0_abs_threshold, x0.ndim - 1)\n    x0_abs_threshold = x0_abs_threshold.repeat(x0.shape[0], *[1]*(x0.ndim - 1))\n    x0_abs_threshold = torch.max(x0_abs_threshold, torch.tensor(1e-10).to(x0.device))\n    x0_abs_threshold = x0_abs_threshold.reshape(x0.shape)\n    x0 = torch.clamp(x0, -x0_abs_threshold, x0_abs_threshold)\n    x0 = x0 / x0_abs_threshold\n    x0 = x0.reshape(x0.shape)\n    return x0\n\n", "CodeBleu_score": 40.63990384924843}, {"code": "def dynamic_thresholding_fn(self, x0, t=None)\n\t    x0 = x0.abs()\n    x0 = x0.view(x0.shape[0], -1)\n    x0, _ = torch.sort(x0, dim=1)\n    x0 = x0.view(x0.shape[0], *([1]*(len(x0.shape) - 1)))\n    if t is None:\n        t = self.threshold\n    x0 = x0[:, int(x0.shape[1] * t) - 1]\n    x0 = x0.unsqueeze(1)\n    x0 = x0.unsqueeze(1)\n    x0 = x0.unsqueeze(1)\n    x0 = x0.unsqueeze(1)\n    x0 = x0.expand_dims(len(x0.shape))\n    x0 = x0.expand_dims(len(x0.shape))\n    x0 = x0.expand_dims(len(x0.shape))\n    x0 = x0.expand_dims(len(x0.shape))\n    x0 = x0.expand_dims(len(x0.shape))\n    x0 = x0.expand_dims(len(x0.shape))\n    x0 = x0.expand_dims(len(x0.shape))\n    x0 = x0.expand_dims(len(x0.shape))\n    x0 = x0.expand_dims(len(x0.shape))\n    x0 = x0.expand_dims(len(x0.shape))\n    x0 = x0.expand_dims(len(x0.shape))\n    x0 = x0.expand_dims(len(x0.shape))\n    x0 = x0.expand_dims(len(x0.shape))\n    x0 = x0.expand_dims(len(x0.shape))\n    x0 = x0.expand_dims(len(x0.shape))\n    x0 = x0.expand_dims(len(x0.shape))\n    x0 = x0.expand_dims(len(x0.shape))\n    x0 = x0.expand_dims(len(x0.shape))\n    x0 = x0.expand_dims(len(x0.shape))\n    x0 = x0.expand_dims(len(x", "CodeBleu_score": 21.70300407069273}, {"code": "def dynamic_thresholding_fn(self, x0, t=None)\n\t    x0 = x0.view(x0.size(0), -1)\n    x0 = x0.abs()\n    x0 = x0.sort(dim=1)[0]\n    x0 = x0[:, int(x0.size(1) * self.quantile)]\n    x0 = x0.unsqueeze(1).unsqueeze(1)\n    x0 = x0.expand_as(x0)\n    x0 = x0.view(x0.size(0), -1)\n    x0 = x0.unsqueeze(1)\n    x0 = x0.expand_as(x0)\n    x0 = x0.view(x0.size(0), -1)\n    x0 = x0.unsqueeze(1)\n    x0 = x0.expand_as(x0)\n    x0 = x0.view(x0.size(0), -1)\n    x0 = x0.unsqueeze(1)\n    x0 = x0.expand_as(x0)\n    x0 = x0.view(x0.size(0), -1)\n    x0 = x0.unsqueeze(1)\n    x0 = x0.expand_as(x0)\n    x0 = x0.view(x0.size(0), -1)\n    x0 = x0.unsqueeze(1)\n    x0 = x0.expand_as(x0)\n    x0 = x0.view(x0.size(0), -1)\n    x0 = x0.unsqueeze(1)\n    x0 = x0.expand_as(x0)\n    x0 = x0.view(x0.size(0), -1)\n    x0 = x0.unsqueeze(1)\n    x0 = x0.expand_as(x0)\n    x0 = x0.view(x0.size(0), -1)\n    x0 = x0.unsqueeze(1)\n    x0 = x0.expand_as(x0)\n    x0 = x0.view(x0.size(0), -1)\n    x0 = x0.unsqueeze(1)\n    x0 = x0.expand_as(x0)\n    x0 = x0.view(x0.size(0), -1)\n    x0", "CodeBleu_score": 11.64800411246417}]}
{"instance_id": "59c6ec6d7a6a", "generation_result": [{"code": "def data_prediction_fn(self, x, t)\n\t  # Compute the threshold value\n    threshold = self.dynamic_thresholding_ratio * self.dynamic_thresholding_max\n    # Clamp the input tensor values within the threshold\n    x = torch.clamp(x, min=0.0, max=threshold)\n    # Normalize the clamped values\n    x = x / threshold\n    return x\n", "CodeBleu_score": 9.524330612304311}, {"code": "def data_prediction_fn(self, x, t)\n\t  # apply dynamic thresholding\n    threshold = self.dynamic_thresholding_ratio * self.max_threshold\n    x = torch.where(x > threshold, x, 0.)\n    # normalize\n    x = x / threshold\n    return x\n", "CodeBleu_score": 8.229170530007897}, {"code": "def data_prediction_fn(self, x, t)\n\t  threshold = self.threshold_ratio * self.max_threshold\n    x = torch.clamp(x, min=-threshold, max=threshold)\n    return x / threshold\n", "CodeBleu_score": 9.176508852933972}]}
{"instance_id": "65e3618468a2", "generation_result": [{"code": "def predict_eps_sigma(model, input, sigma_in, **kwargs)\n\t\n    sigma = sigma_in.reshape(input.shape)\n    input_scaled = input / sigma\n    pred = model.predict(input_scaled, **kwargs)\n    pred = pred * sigma\n    return pred\n\n", "CodeBleu_score": 19.48923060015173}, {"code": "def predict_eps_sigma(model, input, sigma_in, **kwargs)\n\t\n    sigma = sigma_in.reshape(input.shape)\n    scaled_input = (input - model.mean) / model.std\n    prediction = model.predict(scaled_input, **kwargs)\n    return prediction + sigma\n\n", "CodeBleu_score": 20.461462834596958}, {"code": "def predict_eps_sigma(model, input, sigma_in, **kwargs)\n\t\n    sigma_in = sigma_in.reshape(input.shape)\n    input_scaled = scale_input(input)\n    prediction = model.predict(input_scaled, **kwargs)\n    return prediction + sigma_in * input_scaled\n\n", "CodeBleu_score": 19.825379637086797}]}
{"instance_id": "b35846dfbc66", "generation_result": [{"code": "def cal_intergrand(beta_0, beta_1, taus)\n\t\n    with torch.enable_grad():\n        taus_clone = taus.clone()\n        taus_clone.requires_grad_(True)\n        alpha = t2alpha_fn(beta_0, beta_1, taus_clone)\n        log_alpha = torch.log(alpha)\n        log_alpha.backward(torch.ones_like(log_alpha))\n        dlog_alpha_dtaus = taus_clone.grad\n        taus_clone.grad.zero_()\n        integrand = alpha * dlog_alpha_dtaus\n    return integrand\n\n", "CodeBleu_score": 45.90917948379485}, {"code": "def cal_intergrand(beta_0, beta_1, taus)\n\t\n    # compute alpha and its logarithm\n    alpha = t2alpha_fn(beta_0, beta_1, taus)\n    log_alpha = torch.log(alpha)\n\n    # perform backpropagation to obtain the gradient of log_alpha with respect to taus\n    log_alpha.backward(torch.ones_like(log_alpha), retain_graph=True)\n\n    # calculate the integrand using the gradient of log_alpha and the alpha values\n    integrand = alpha * taus.grad\n\n    # reset the gradients of taus to zero\n    taus.grad.zero_()\n\n    return integrand\n\n", "CodeBleu_score": 24.781202417897397}, {"code": "def cal_intergrand(beta_0, beta_1, taus)\n\t\n    taus = taus.clone().requires_grad_()\n    alpha = t2alpha_fn(beta_0, beta_1, taus)\n    log_alpha = torch.log(alpha)\n    log_alpha.backward(gradient=torch.ones_like(log_alpha), retain_graph=True)\n    integrand = alpha * taus.grad\n    return integrand\n\n", "CodeBleu_score": 26.196878512280698}]}
{"instance_id": "5f3667566a10", "generation_result": [{"code": "def sample_euler_ancestral(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None)\n\t\n    if extra_args is None:\n        extra_args = {}\n    if noise_sampler is None:\n        noise_sampler = default_noise_sampler\n    if disable is None:\n        disable = []\n    x = x.clone()\n    if 'x_init' not in extra_args:\n        extra_args['x_init'] = x.clone()\n    if'sigma_init' not in extra_args:\n        extra_args['sigma_init'] = sigmas[0]\n    if'sigma_prev' not in extra_args:\n        extra_args['sigma_prev'] = sigmas[0]\n    for i, sigma in enumerate(sigmas):\n        if i == 0:\n            continue\n        denoised = model(x, sigmas[i - 1], **extra_args)\n        if 'denoised' not in extra_args:\n            extra_args['denoised'] = denoised\n        if 'denoised_prev' not in extra_args:\n            extra_args['denoised_prev'] = denoised\n        if'sigma_prev' not in extra_args:\n            extra_args['sigma_prev'] = sigmas[i - 1]\n        sigma_down, sigma_up = get_ancestral_step(sigmas[i - 1], sigma, eta)\n        if'sigma_down' not in extra_args:\n            extra_args['sigma_down'] = sigma_down\n        if'sigma_up' not in extra_args:\n            extra_args['sigma_up'] = sigma_up\n        if'sigma' not in extra_args:\n            extra_args['sigma'] = sigma\n        if'sigma_next' not in extra_args:\n            extra_args['sigma_next'] = sigma\n        if's_noise' not in extra_args:\n            extra_args['s_noise'] = s_noise\n        if 'noise' not in extra_args:\n            extra_args['noise'] = noise_sampler(sigma, sigma_down)\n        if 'noise_prev' not in extra_args:\n            extra_args['noise_prev'] = noise_sampler(sigma_down, sigma_up)\n        if callback is not None and i not in disable:\n            callback(x, denoised, sigma, sigma_down, sigma_up, **extra_args)\n        x = x +", "CodeBleu_score": 29.324325913824467}, {"code": "def sample_euler_ancestral(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None)\n\t\n    if extra_args is None:\n        extra_args = model.get_default_extra_args()\n    if noise_sampler is None:\n        noise_sampler = default_noise_sampler(x)\n    for sigma_from, sigma_to in zip(sigmas, sigmas[1:]):\n        denoised = model(x, sigmas=(sigma_from, sigma_to), extra_args=extra_args, disable=disable)\n        sigma_down, sigma_up = get_ancestral_step(sigma_from, sigma_to, eta=eta)\n        d = to_d(x, sigma_down, denoised)\n        x = x - sigma_down ** 2 * d\n        if callback is not None:\n            callback(x, sigma_down)\n        if sigma_up > 0:\n            x = x + sigma_up * noise_sampler(sigma_up, sigma_to)\n    if s_noise > 0:\n        x = x + s_noise * noise_sampler(s_noise, 0)\n    return x\n\n", "CodeBleu_score": 38.54173555759503}, {"code": "def sample_euler_ancestral(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None)\n\t\n    if extra_args is None:\n        extra_args = model.get_default_extra_args()\n    if noise_sampler is None:\n        noise_sampler = model.get_default_noise_sampler()\n    for i, (sigma, sigma_next) in enumerate(zip(sigmas[:-1], sigmas[1:])):\n        denoised = model.denoise(x, sigma, extra_args=extra_args, disable=disable)\n        sigma_down, sigma_up = get_ancestral_step(sigma, sigma_next, eta=eta)\n        x = x - to_d(x, sigma, denoised) * utils.append_dims(sigma, x.ndim)\n        if callback is not None:\n            callback(x, sigma, i)\n        if sigma_up > 0:\n            x = x + noise_sampler(sigma, sigma_up) * s_noise\n    return x + noise_sampler(sigmas[-1], 0) * s_noise\n\n", "CodeBleu_score": 35.169841859035046}]}
{"instance_id": "6261ded88df2", "generation_result": [{"code": "def linear_multistep_coeff(order, t, i, j)\n\t\n    if order > i:\n        raise ValueError(\"The order is too high for the step index.\")\n    def product_term(k):\n        \"\"\"The function calculates the product term for the coefficient, excluding the j-th term.", "CodeBleu_score": 14.496535534634166}, {"code": "def linear_multistep_coeff(order, t, i, j)\n\t\n    if order > i:\n        raise ValueError('The order is too high for the step index.')\n    def product_term(x):\n        \"\"\"The function computes the product term for the coefficient, excluding the j-th term.\"\"\"\n        result = 1\n        for k in range(i):\n            if k!= j:\n                result *= x - t[k]\n        return result\n    return integrate.quad(product_term, t[i], t[i+1])[0]\n\n", "CodeBleu_score": 32.65636436648328}, {"code": "def linear_multistep_coeff(order, t, i, j)\n\t\n    if order > i:\n        raise ValueError(\"The order is too high for the step index.\")\n    def product_term(x):\n        \"\"\"The function computes the product term for the coefficient, excluding the j-th term.\"\"\"\n        prod = 1\n        for k in range(i+1):\n            if k!= j:\n                prod *= (x - t[k])\n        return prod\n    return integrate.quad(product_term, t[i], t[i+1])[0]\n\n", "CodeBleu_score": 33.54090970040778}]}
{"instance_id": "4ee025e33ab1", "generation_result": [{"code": "def sample_lcm(model, x, sigmas, extra_args=None, callback=None, disable=None, noise_sampler=None)\n\t\n    if extra_args is None:\n        extra_args = model.get_default_extra_args()\n    if noise_sampler is None:\n        noise_sampler = model.get_default_noise_sampler()\n    if disable is None:\n        disable = []\n    if not isinstance(sigmas, list):\n        sigmas = [sigmas]\n    if not isinstance(extra_args, list):\n        extra_args = [extra_args]\n    if not isinstance(disable, list):\n        disable = [disable]\n    if len(extra_args) == 1:\n        extra_args = extra_args * len(sigmas)\n    if len(disable) == 1:\n        disable = disable * len(sigmas)\n    x_prev = x.clone()\n    sigma_prev = 0.0\n    for i, (sigma, extra_arg, disable_item) in enumerate(zip(sigmas, extra_args, disable)):\n        if callback is not None:\n            callback(x, i, sigma, x_prev, sigma_prev)\n        if sigma > 0.0:\n            noise = noise_sampler(sigma, sigma_prev)\n            x = x + noise\n        x = model(x, sigma, extra_arg, disable=disable_item)\n        x_prev = x.clone()\n        sigma_prev = sigma\n    return x\n\n", "CodeBleu_score": 33.0267186184675}, {"code": "def sample_lcm(model, x, sigmas, extra_args=None, callback=None, disable=None, noise_sampler=None)\n\t\n    if extra_args is None:\n        extra_args = {}\n    if noise_sampler is None:\n        noise_sampler = default_noise_sampler\n    if disable is None:\n        disable = {}\n    if not isinstance(sigmas, list):\n        sigmas = [sigmas]\n    for i, sigma in enumerate(sigmas):\n        if i == 0:\n            x_prev = x\n        else:\n            x_prev = x_cur\n        if i + 1 < len(sigmas):\n            sigma_next = sigmas[i + 1]\n        else:\n            sigma_next = 0.0\n        if sigma > 0.0:\n            noise = noise_sampler(x_prev, sigma_next)\n            x_cur = x_prev + noise\n        else:\n            x_cur = x_prev\n        x_cur = model(x_cur, sigma, **extra_args)\n        if sigma_next > 0.0:\n            x_cur = x_cur * (1 - sigma_next / sigma) + x_prev * (sigma_next / sigma)\n        if callback is not None:\n            callback(x_cur, i, sigma, sigma_next)\n    return x_cur\n\n", "CodeBleu_score": 29.867824464341}, {"code": "def sample_lcm(model, x, sigmas, extra_args=None, callback=None, disable=None, noise_sampler=None)\n\t\n    if extra_args is None:\n        extra_args = model.default_extra_args()\n    if noise_sampler is None:\n        noise_sampler = model.default_noise_sampler()\n    if disable is None:\n        disable = model.default_disable()\n    for i, sigma in enumerate(sigmas):\n        with torch.no_grad():\n            if i == 0:\n                x_out = model(x, sigma, extra_args=extra_args, disable=disable)\n            else:\n                x_out = model(x, sigma, extra_args=extra_args, disable=disable)\n                x_out = x_out + (x_out - x) * torch.exp(-sigma ** 2 / (2 * sigmas[i - 1] ** 2))\n        if callback is not None:\n            callback(x_out, i)\n        if sigmas[i] > 0:\n            x = x_out + noise_sampler(sigmas[i], sigmas[i + 1]) * sigmas[i]\n    return x_out\n\n", "CodeBleu_score": 36.076986674335785}]}
{"instance_id": "cb51f91da606", "generation_result": [{"code": "def append_dims(x, target_dims)\n\t    if x.ndim < target_dims:\n        x = x.expand(*x.shape, *((1,) * (target_dims - x.ndim)))\n    if x.ndim > target_dims:\n        raise ValueError(\n            f\"The number of dimensions of the input tensor ({x.ndim}) is greater than the target number of dimensions ({target_dims}).\"\n        )\n    return x.detach() if x.device.type == \"mps\" else x\n\n", "CodeBleu_score": 42.7417123935942}, {"code": "def append_dims(x, target_dims)\n\t    if len(x.shape) >= target_dims:\n        raise ValueError(f\"Target dimensions ({target_dims}) are fewer than the current dimensions ({len(x.shape)})\")\n    for _ in range(target_dims - len(x.shape)):\n        x = x.unsqueeze(-1)\n    if x.device.type == \"mps\":\n        return x.detach().clone()\n    return x\n\n", "CodeBleu_score": 38.06094737124267}, {"code": "def append_dims(x, target_dims)\n\t    if len(x.shape) >= target_dims:\n        raise ValueError(\"The target dimensions are fewer than the current dimensions.\")\n    return x.expand(x.shape + (1,) * (target_dims - len(x.shape))).detach() if x.device.type == \"mps\" else x.expand(x.shape + (1,) * (target_dims - len(x.shape)))\n\n", "CodeBleu_score": 28.412311458215555}]}
{"instance_id": "9fac874a3bc7", "generation_result": [{"code": "def download_file(path, url, digest=None)\n\t\n    if os.path.exists(path):\n        return path\n    if not os.path.exists(os.path.dirname(path)):\n        os.makedirs(os.path.dirname(path))\n    with open(path, \"wb\") as f:\n        print(\"Downloading %s to %s\" % (url, path))\n        response = requests.get(url, stream=True)\n        total = response.headers.get(\"content-length\")\n        if total is None:\n            f.write(response.content)\n        else:\n            downloaded = 0\n            total = int(total)\n            for data in response.iter_content(chunk_size=max(int(total / 1000), 1024 * 1024)):\n                downloaded += len(data)\n                f.write(data)\n                done = int(50 * downloaded / total)\n                sys.stdout.write(\"\\r[%s%s]\" % (\"#\" * done, \" \" * (50 - done)))\n                sys.stdout.flush()\n    sys.stdout.write(\"", "CodeBleu_score": 39.22890366130333}, {"code": "def download_file(path, url, digest=None)\n\t\n    if not os.path.exists(path):\n        os.makedirs(os.path.dirname(path), exist_ok=True)\n        print(\"Downloading %s to %s\" % (url, path))\n        r = requests.get(url, stream=True)\n        with open(path, \"wb\") as f:\n            for chunk in r.iter_content(chunk_size=1024):\n                if chunk:\n                    f.write(chunk)\n    if digest:\n        sha256 = hashlib.sha256()\n        with open(path, \"rb\") as f:\n            while True:\n                data = f.read(65536)\n                if not data:\n                    break\n                sha256.update(data)\n        if sha256.hexdigest()!= digest:\n            raise ValueError(\"File %s has an invalid hash (%s), expected %s\" % (path, sha256.hexdigest(), digest))\n    return path\n\n", "CodeBleu_score": 35.70016162028927}, {"code": "def download_file(path, url, digest=None)\n\t\n    if not os.path.exists(path):\n        os.makedirs(path)\n    file_path = os.path.join(path, url.split(\"/\")[-1])\n    if not os.path.exists(file_path):\n        with open(file_path, \"wb\") as f:\n            print(f\"Downloading {url}...\")\n            response = requests.get(url, stream=True)\n            total = response.headers.get(\"content-length\")\n\n            if total is None:\n                f.write(response.content)\n            else:\n                downloaded = 0\n                total = int(total)\n                for data in response.iter_content(\n                    chunk_size=max(int(total / 1000), 1024 * 1024)\n                ):\n                    downloaded += len(data)\n                    f.write(data)\n                    done = int(50 * downloaded / total)\n                    sys.stdout.write(\"\\r[{}{}]\".format(\"\u2588\" * done, \".\" * (50 - done)))\n                    sys.stdout.flush()\n    if digest:\n        print(f\"Checking hash of {file_path}...\")\n        if not check_hash(file_path, digest):\n            raise ValueError(f\"Hash of {file_path} does not match the expected value.\")\n    return file_path\n\n", "CodeBleu_score": 38.23216793064105}]}
{"instance_id": "f23165e897ce", "generation_result": [{"code": "def train_mode(model, mode=True)\n\t\n    if mode:\n        model.train()\n    else:\n        model.eval()\n    return model\n\n", "CodeBleu_score": 14.524436844388436}, {"code": "def train_mode(model, mode=True)\n\t\n    if mode:\n        model.train()\n    else:\n        model.eval()\n    return model\n\n", "CodeBleu_score": 14.524436844388436}, {"code": "def train_mode(model, mode=True)\n\t\n    if not isinstance(model, nn.Module):\n        raise TypeError(\"model must be a nn.Module, but got {}\".format(type(model)))\n    if not isinstance(mode, bool):\n        raise TypeError(\"mode must be a bool, but got {}\".format(type(mode)))\n    current_mode = model.training\n    model.train(mode)\n    yield\n    model.train(current_mode)\n\n", "CodeBleu_score": 21.48617225935501}]}
{"instance_id": "c6d59c6cb8e1", "generation_result": [{"code": "def ema_update(model, averaged_model, decay)\n\t\n    # Ensure both models have matching parameters and buffers\n    for param, avg_param in zip(model.parameters(), averaged_model.parameters()):\n        assert param.size() == avg_param.size(), \"Parameter sizes do not match\"\n    for buffer, avg_buffer in zip(model.buffers(), averaged_model.buffers()):\n        assert buffer.size() == avg_buffer.size(), \"Buffer sizes do not match\"\n    # Update the averaged model parameters using a weighted average based on the decay factor\n    for param, avg_param in zip(model.parameters(), averaged_model.parameters()):\n        avg_param.data.mul_(decay).add_(param.data, alpha=1 - decay)\n    # Copy the current model's buffers to the averaged model\n    for buffer, avg_buffer in zip(model.buffers(), averaged_model.buffers()):\n        avg_buffer.data.copy_(buffer.data)\n\n", "CodeBleu_score": 38.754973419390936}, {"code": "def ema_update(model, averaged_model, decay)\n\t\n    for param, avg_param in zip(model.parameters(), averaged_model.parameters()):\n        avg_param.data.mul_(decay).add_(param.data, alpha=1 - decay)\n    for buffer, avg_buffer in zip(model.buffers(), averaged_model.buffers()):\n        avg_buffer.data.copy_(buffer.data)\n\n", "CodeBleu_score": 27.49313936795724}, {"code": "def ema_update(model, averaged_model, decay)\n\t\n    for param, avg_param in zip(model.parameters(), averaged_model.parameters()):\n        avg_param.data.mul_(decay).add_(param.data, alpha=1 - decay)\n    for buffer, avg_buffer in zip(model.buffers(), averaged_model.buffers()):\n        avg_buffer.data.copy_(buffer.data)\n\n", "CodeBleu_score": 27.49313936795724}]}
{"instance_id": "600c39a30798", "generation_result": [{"code": "def rand_log_logistic(shape, loc=0., scale=1., min_value=0., max_value=float('inf'), device='cpu', dtype=torch.float32)\n\t\n    min_value = torch.tensor(min_value, device=device, dtype=dtype)\n    max_value = torch.tensor(max_value, device=device, dtype=dtype)\n    min_value_cdf = torch.sigmoid(log_logistic_cdf(min_value, loc, scale))\n    max_value_cdf = torch.sigmoid(log_logistic_cdf(max_value, loc, scale))\n    u = min_value_cdf + (max_value_cdf - min_value_cdf) * torch.rand(shape, device=device, dtype=dtype)\n    x = torch.sigmoid(torch.log(u / (1 - u)))\n    return loc + scale * torch.log(x / (1 - x))\n\n", "CodeBleu_score": 31.516006951290183}, {"code": "def rand_log_logistic(shape, loc=0., scale=1., min_value=0., max_value=float('inf'), device='cpu', dtype=torch.float32)\n\t\n    min_value = torch.tensor(min_value, device=device, dtype=dtype)\n    max_value = torch.tensor(max_value, device=device, dtype=dtype)\n    min_cdf = log_logistic_cdf(min_value, loc, scale)\n    max_cdf = log_logistic_cdf(max_value, loc, scale)\n    uniform_samples = torch.empty(shape, device=device, dtype=dtype).uniform_(min_cdf, max_cdf)\n    return log_logistic_ppf(uniform_samples, loc, scale)\n\n", "CodeBleu_score": 29.97751777623721}, {"code": "def rand_log_logistic(shape, loc=0., scale=1., min_value=0., max_value=float('inf'), device='cpu', dtype=torch.float32)\n\t\n    min_value = torch.as_tensor(min_value, device=device, dtype=dtype)\n    max_value = torch.as_tensor(max_value, device=device, dtype=dtype)\n    min_cdf = log_logistic_cdf(min_value, loc, scale)\n    max_cdf = log_logistic_cdf(max_value, loc, scale)\n    u = torch.rand(shape, device=device, dtype=dtype)\n    u = (max_cdf - min_cdf) * u + min_cdf\n    return log_logistic_ppf(u, loc, scale)\n\n", "CodeBleu_score": 39.501148420112}]}
{"instance_id": "75e2def2467f", "generation_result": [{"code": "def get_activation(activation: Literal[\"elu\", \"snake\", \"none\"], antialias=False, channels=None) -> nn.Module\n\t\n    if activation == \"elu\":\n        return nn.ELU(inplace=True)\n    elif activation == \"snake\":\n        return SnakeBeta(channels=channels)\n    elif activation == \"none\":\n        return nn.Identity()\n    else:\n        raise ValueError(f\"Unknown activation type: {activation}\")\n\n", "CodeBleu_score": 42.20063370374166}, {"code": "def get_activation(activation: Literal[\"elu\", \"snake\", \"none\"], antialias=False, channels=None) -> nn.Module\n\t\n    if activation == \"elu\":\n        return nn.ELU(inplace=True)\n    elif activation == \"snake\":\n        return SnakeBeta(channels)\n    elif activation == \"none\":\n        return nn.Identity()\n    else:\n        raise ValueError(f\"Invalid activation type: {activation}\")\n\n", "CodeBleu_score": 42.78412116496468}, {"code": "def get_activation(activation: Literal[\"elu\", \"snake\", \"none\"], antialias=False, channels=None) -> nn.Module\n\t\n    if activation == \"elu\":\n        return nn.ELU(inplace=True)\n    elif activation == \"snake\":\n        return SnakeBeta(channels)\n    elif activation == \"none\":\n        return nn.Identity()\n    else:\n        raise ValueError(f\"Invalid activation type: {activation}\")\n\n", "CodeBleu_score": 42.78412116496468}]}
{"instance_id": "ef1714c79e51", "generation_result": [{"code": "def apply_rotary_pos_emb(t, freqs, scale = 1)\n\t    t = t.to(freqs.dtype)\n    t = rearrange(t, '... (d j) ->... d j', j = 2)\n    freqs = freqs.to(t)\n    freqs = freqs.unsqueeze(-2)\n    sin, cos = map(partial(torch.einsum, '... d j,... f ->... (d f) j'), ('... i j,... j ->... i j',)... i j,... j ->... i j'), (freqs, t))\n    return torch.cat((sin, cos), dim = -1)\n\n", "CodeBleu_score": 16.038976825093716}, {"code": "def apply_rotary_pos_emb(t, freqs, scale = 1)\n\t    t = t.to(freqs.dtype)\n    t_shape = t.shape\n    freqs = freqs.view(1, -1, 1)\n    t = t.view(*t_shape[:-1], -1, 2)\n    t_rot = rotate_half(t)\n    t = torch.cat([t, t_rot], dim = -1)\n    t = t * freqs * scale\n    t = rotate_half(t)\n    return t.view(*t_shape)\n\n", "CodeBleu_score": 19.310551721182684}, {"code": "def apply_rotary_pos_emb(t, freqs, scale = 1)\n\t    t = t.to(freqs.dtype)\n    t = rearrange(t, '... d ->... 1 d')\n    freqs = freqs.to(t.dtype)\n    freqs = rearrange(freqs, 'd -> 1 1 d')\n    t = torch.cat((t, t), dim = -1)\n    t = t * freqs * scale\n    t = rotate_half(t)\n    return rearrange(t, '... (d j) ->... d j', j = 2)\n\n", "CodeBleu_score": 23.668572314740825}]}
{"instance_id": "a7a5f178c6f6", "generation_result": [{"code": "def idx2vq(self, idx, dim=-1)\n\t\n    return self.codebook(idx).permute(0, 2, 1).contiguous()\n\n", "CodeBleu_score": 17.58738302822947}, {"code": "def idx2vq(self, idx, dim=-1)\n\t\n    return self.codebook(idx).transpose(dim, -1)\n\n", "CodeBleu_score": 16.840795329970103}, {"code": "def idx2vq(self, idx, dim=-1)\n\t\n    return self.codebook(idx).transpose(dim, -1)\n\n", "CodeBleu_score": 16.840795329970103}]}
{"instance_id": "4104c041fb20", "generation_result": [{"code": "def encode(self, x, quantize=False)\n\t   if quantize:\n        z_e_x, losses, indices = self.forward(x, get_losses=True)\n        return z_e_x, (losses[0] + losses[1]), indices\n    else:\n        z_e_x = self.forward(x, get_losses=False)\n        return z_e_x\n\n", "CodeBleu_score": 30.17100838599657}, {"code": "def encode(self, x, quantize=False)\n\t   if quantize:\n        z, loss, indices = self.vq(x)\n        return z, loss, indices\n    else:\n        return x\n\n", "CodeBleu_score": 21.400080636741496}, {"code": "def encode(self, x, quantize=False)\n\t   if quantize:\n        z_e_x, (vq_loss, commit_loss), indices = self.forward(x, get_losses=True)\n        return z_e_x, (vq_loss, commit_loss), indices\n    else:\n        return self.forward(x, get_losses=False)\n\n", "CodeBleu_score": 32.66637009356892}]}
{"instance_id": "403c52ba2d3b", "generation_result": [{"code": "def ema_scope(self, context=None)\n\t\n    if self.use_ema:\n        self.shadow_model.store(self.model.parameters())\n        self.shadow_model.copy_to(self.model)\n        if context:\n            context.log_info(\"EMA enabled. Switched to EMA weights.\")\n    yield\n    if self.use_ema:\n        self.shadow_model.restore(self.model.parameters())\n        if context:\n            context.log_info(\"Switched to original weights.\")\n\n", "CodeBleu_score": 50.54111463111213}, {"code": "def ema_scope(self, context=None)\n\t\n    if not self.use_ema:\n        yield\n        return\n    if not context:\n        context = lambda: None\n    self.store(self.parameters())\n    self.copy_to(self)\n    context()\n    self.restore(self.parameters())\n\n", "CodeBleu_score": 32.19396897520785}, {"code": "def ema_scope(self, context=None)\n\t\n    if self.ema_config is None:\n        yield\n        return\n    if context is None:\n        context = DummyContextManager()\n    with context:\n        self.shadow.store(self.parameters())\n        self.shadow.copy_to(self)\n        yield\n    with context:\n        self.shadow.restore(self.parameters())\n", "CodeBleu_score": 43.475974720498186}]}
{"instance_id": "94b7467f98e4", "generation_result": [{"code": "def post_attention(self, attn, x, gate_msa, shift_mlp, scale_mlp, gate_mlp)\n\t\n    x = x + attn * gate_msa\n    x = self.norm(x)\n    x = modulate(x, shift_mlp, scale_mlp)\n    x = self.mlp(x) * gate_mlp\n    return x\n", "CodeBleu_score": 37.30464450232957}, {"code": "def post_attention(self, attn, x, gate_msa, shift_mlp, scale_mlp, gate_mlp)\n\t\n    x = x + attn * gate_msa\n    x = self.norm(x)\n    x = modulate(x, shift_mlp, scale_mlp)\n    x = self.mlp(x) * gate_mlp\n    return x\n", "CodeBleu_score": 37.30464450232957}, {"code": "def post_attention(self, attn, x, gate_msa, shift_mlp, scale_mlp, gate_mlp)\n\t\n    x = x + attn * gate_msa\n    x = self.norm(x)\n    x = modulate(x, shift_mlp, scale_mlp)\n    x = self.mlp(x) * gate_mlp\n    return x\n", "CodeBleu_score": 37.30464450232957}]}
{"instance_id": "d9fdc05542a2", "generation_result": [{"code": "def _block_mixing(context, x, context_block, x_block, c)\n\t\n    qkv_c, intermediates_c = context_block.pre_attention(context)\n    qkv_x, intermediates_x = x_block.pre_attention(x)\n    qkv = torch.cat([qkv_c, qkv_x], dim=1)\n    qkv = optimized_attention(qkv, c.num_heads)\n    qkv_c, qkv_x = torch.split(qkv, [qkv_c.shape[1], qkv_x.shape[1]], dim=1)\n    context = context_block.post_attention(qkv_c)\n    if context_block.pre_only:\n        x = x_block.post_attention(qkv_x)\n    else:\n        x = x_block.post_attention(torch.cat([qkv_x, x], dim=1))\n    return context, x, intermediates_c, intermediates_x\n", "CodeBleu_score": 23.911473523465034}, {"code": "def _block_mixing(context, x, context_block, x_block, c)\n\t\n    qkv_context, qkv_x = context_block.pre_attention(context), x_block.pre_attention(x)\n    qkv = torch.cat([qkv_context, qkv_x], dim=1)\n    qkv = optimized_attention(qkv, c.num_heads)\n    context, x = split_qkv(qkv, c.head_dim, 2)\n    if context_block.pre_only:\n        context = context_block.post_attention(context)\n    else:\n        context, x = context_block.post_attention(context), x_block.post_attention(x)\n    return context, x\n", "CodeBleu_score": 22.41614622658556}, {"code": "def _block_mixing(context, x, context_block, x_block, c)\n\t\n    qkv = context_block.pre_attention(context)\n    qkv = qkv + x_block.pre_attention(x)\n    qkv = torch.cat(qkv, dim=-1)\n    qkv = optimized_attention(qkv, c)\n    qkv = split_qkv(qkv, c)\n    context, x = context_block.post_attention(qkv[0]), x_block.post_attention(qkv[1])\n    return context, x\n", "CodeBleu_score": 11.021145585901374}]}
{"instance_id": "b219bce30160", "generation_result": [{"code": "def get_timestep_embedding(timesteps, embedding_dim)\n\t\n    assert embedding_dim % 2 == 0, \"embedding_dim must be even\"\n    assert len(timesteps.shape) == 1, \"timesteps must be one-dimensional\"\n    half_dim = embedding_dim // 2\n    emb = math.log(10000) / (half_dim - 1)\n    emb = torch.exp(torch.arange(half_dim, dtype=torch.float) * -emb)\n    emb = timesteps.float()[:, None] * emb[None, :]\n    emb = torch.cat((emb.sin(), emb.cos()), dim=1)\n    if embedding_dim % 2 == 1:\n        emb = F.pad(emb, (0, 1), \"constant\", 0)\n    return emb\n\n", "CodeBleu_score": 68.88810964622718}, {"code": "def get_timestep_embedding(timesteps, embedding_dim)\n\t\n    if timesteps.ndim!= 1:\n        raise ValueError(\n            f\"The input timesteps must be one-dimensional, but got {timesteps.ndim} dimensions.\"\n        )\n    num_timesteps = len(timesteps)\n    half_dim = embedding_dim // 2\n    emb = np.log(10000) * (timesteps[:, None] // (10000**np.arange(half_dim)[None, :]))\n    emb = np.concatenate([np.sin(emb), np.cos(emb)], axis=1)\n    if embedding_dim % 2 == 1:\n        emb = np.pad(emb, [[0, 0], [0, 1]])\n    return emb\n\n", "CodeBleu_score": 49.17170308254105}, {"code": "def get_timestep_embedding(timesteps, embedding_dim)\n\t\n    # Ensure the timesteps input is one-dimensional\n    if len(timesteps.shape) > 1:\n        raise ValueError(\n            \"The timesteps input must be one-dimensional, but has shape {}\".format(\n                timesteps.shape\n            )\n        )\n    # Calculate the positional embedding\n    embedding = np.zeros((timesteps.shape[0], embedding_dim))\n    for dim in range(0, embedding_dim, 2):\n        embedding[:, dim] = np.sin(timesteps * (10000 ** (dim / embedding_dim)))\n        embedding[:, dim + 1] = np.cos(timesteps * (10000 ** (dim / embedding_dim)))\n    # Scale the embedding by a logarithmic factor\n    embedding *= np.log(timesteps.shape[0])\n    # If the embedding dimension is odd, zero-pad the embedding\n    if embedding_dim % 2 == 1:\n        embedding = np.concatenate(\n            (embedding, np.zeros((timesteps.shape[0], 1))), axis=1\n        )\n    return embedding\n\n", "CodeBleu_score": 29.55819000878969}]}
{"instance_id": "6ae8453c86be", "generation_result": [{"code": "def pytorch_attention(q, k, v)\n\t\n    try:\n        scale = (int(q.shape[-1])**(-0.5))\n        r1 = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=0.0, is_causal=False, average_attn_weights=False, scale=scale)\n        return r1\n    except model_management.OOM_EXCEPTION:\n        logging.warning(\"out of memory error, switching to slice attention\")\n        return slice_attention(q, k, v)\n\n", "CodeBleu_score": 27.971143075695164}, {"code": "def pytorch_attention(q, k, v)\n\t\n    try:\n        scale = (int(q.shape[-1])**(-0.5))\n        r1 = torch.bmm(q, k) * scale\n        r2 = torch.nn.functional.softmax(r1, dim=2).permute(0,2,1)\n        r3 = torch.bmm(v, r2)\n        return r3\n    except model_management.OOM_EXCEPTION:\n        logging.warning(\"out of memory error, using slice attention\")\n        return slice_attention(q, k, v)\n\n", "CodeBleu_score": 22.399408477229326}, {"code": "def pytorch_attention(q, k, v)\n\t\n\n    try:\n        scale = (int(q.shape[-1])**(-0.5))\n        q = q.reshape(q.shape[0], q.shape[1], -1).permute(0, 2, 1)\n        k = k.reshape(k.shape[0], k.shape[1], -1).permute(0, 2, 1)\n        v = v.reshape(v.shape[0], v.shape[1], -1).permute(0, 2, 1)\n\n        r1 = torch.bmm(q, k) * scale\n        r2 = torch.nn.functional.softmax(r1, dim=2).permute(0,2,1)\n        r3 = torch.bmm(v, r2)\n        r3 = r3.permute(0, 2, 1).reshape(r3.shape[0], r3.shape[1], -1)\n        return r3\n    except model_management.OOM_EXCEPTION as e:\n        logging.warning(\"out of memory error, switching to slice attention\")\n        return slice_attention(q, k, v)\n\n", "CodeBleu_score": 33.8255103757197}]}
{"instance_id": "fe4433c987ad", "generation_result": [{"code": "def apply_control(h, control, name)\n\t\n    if control and name and name in control:\n        try:\n            h.add_control(control[name])\n        except Exception as e:\n            logging.warning(f\"Failed to add control {name} to h: {e}\")\n        control.pop(name)\n    return h\n\n", "CodeBleu_score": 30.44266070625319}, {"code": "def apply_control(h, control, name)\n\t\n    if control and name and name in control:\n        try:\n            h.add_control(control.pop(name))\n        except Exception as e:\n            logger.warning(f\"Failed to add {name} control to header: {e}\")\n    return h\n\n", "CodeBleu_score": 26.85965939851784}, {"code": "def apply_control(h, control, name)\n\t\n    if control and name:\n        control_to_apply = control.pop(0)\n        try:\n            h.add_control(name, control_to_apply)\n        except Exception as e:\n            logging.warning(\"Could not add control {} to {}: {}\".format(control_to_apply, name, e))\n    return h\n\n", "CodeBleu_score": 39.00841965681957}]}
{"instance_id": "56da4547c101", "generation_result": [{"code": "def get_codebook_entry(self, indices, shape)\n\t\n    indices = tf.cast(indices, tf.int32)\n    depth = self.embedding_weights.shape[0]\n    one_hot_indices = tf.one_hot(indices, depth, axis=-1)\n    quantized_vectors = tf.matmul(one_hot_indices, self.embedding_weights)\n    if shape is not None:\n        quantized_vectors = tf.reshape(quantized_vectors, shape)\n        quantized_vectors = tf.transpose(quantized_vectors, perm=[0, 2, 1])\n    return quantized_vectors\n\n", "CodeBleu_score": 30.641255150106662}, {"code": "def get_codebook_entry(self, indices, shape)\n\t\n    indices = tf.cast(indices, tf.int32)\n    indices = tf.reshape(indices, [-1])\n    one_hot = tf.one_hot(indices, self.embedding_dim)\n    quantized = tf.matmul(one_hot, self.embedding)\n    if shape is not None:\n        quantized = tf.reshape(quantized, shape)\n        quantized = tf.transpose(quantized, perm=[0, 2, 1])\n    return quantized\n\n", "CodeBleu_score": 31.093303627976667}, {"code": "def get_codebook_entry(self, indices, shape)\n\t\n    indices = tf.cast(indices, tf.int32)\n    one_hot = tf.one_hot(indices, self.num_embeddings)\n    quantized = tf.matmul(one_hot, self.embedding_weights)\n    if shape is not None:\n        quantized = tf.reshape(quantized, shape)\n        quantized = tf.transpose(quantized, [0, 3, 1, 2])\n    return quantized\n\n", "CodeBleu_score": 32.62079289018981}]}
{"instance_id": "548f8a1fb37d", "generation_result": [{"code": "def make_noise(self)\n\t\n    noise = [torch.randn(4, 4, device=self.device)]\n    for i in range(3, self.log_size + 1):\n        noise.append(torch.randn(4 * 2 ** (i - 1), 4 * 2 ** (i - 1), device=self.device))\n        noise.append(torch.randn(4 * 2 ** (i - 1), 4 * 2 ** (i - 1), device=self.device))\n    return noise\n\n", "CodeBleu_score": 30.832544745686818}, {"code": "def make_noise(self)\n\t\n    device = self.device\n    log_size = int(math.log(self.image_size, 2))\n    noise = torch.randn(1, 1, 4, 4, device=device)\n    noises = [noise]\n    for i in range(3, log_size):\n        for _ in range(2):\n            noises.append(torch.randn(1, 1, 2 ** i, 2 ** i, device=device))\n    return noises\n\n", "CodeBleu_score": 50.05770009978582}, {"code": "def make_noise(self)\n\t\n    noise = [torch.randn(1, 4, 4, 4, device=self.device)]\n    for i in range(3, self.log_size):\n        noise.append(torch.randn(1, 4 * 2 ** (i - 3), 2 ** (i - 3), 2 ** (i - 3), device=self.device))\n        noise.append(torch.randn(1, 4 * 2 ** (i - 3), 2 ** (i - 3), 2 ** (i - 3), device=self.device))\n    return noise\n\n", "CodeBleu_score": 26.86124319484217}]}
{"instance_id": "42fd805563c6", "generation_result": [{"code": "def mean_latent(self, num_latent)\n\t\n    latent_in = torch.randn(num_latent, self.latent_size, device=self.device)\n    latent_out = self.style(latent_in)\n    latent_mean = latent_out.mean(0)\n    return latent_mean\n\n", "CodeBleu_score": 30.10104660147502}, {"code": "def mean_latent(self, num_latent)\n\t\n    latent_in = torch.randn(num_latent, self.latent_size, device=self.device)\n    latent_out = self.style(latent_in)\n    mean_latent = latent_out.mean(0)\n    return mean_latent\n\n", "CodeBleu_score": 30.067416318864964}, {"code": "def mean_latent(self, num_latent)\n\t\n    latent_in = torch.randn(num_latent, self.latent_size, device=self.device)\n    latent_out = self.style(latent_in)\n    mean_latent = latent_out.mean(0)\n    return mean_latent\n\n", "CodeBleu_score": 30.067416318864964}]}
{"instance_id": "2f485fd924af", "generation_result": [{"code": "def color_jitter(img, shift)\n\t\n    # Randomly shift the RGB values within the specified range\n    jitter = torch.zeros_like(img)\n    jitter[:, 0, :, :] = torch.randint(-shift, shift + 1, size=(1, 1, 1)) / 255.0\n    jitter[:, 1, :, :] = torch.randint(-shift, shift + 1, size=(1, 1, 1)) / 255.0\n    jitter[:, 2, :, :] = torch.randint(-shift, shift + 1, size=(1, 1, 1)) / 255.0\n    # Add the jitter to the image and clip the result\n    img = torch.clamp(img + jitter, 0, 1)\n    return img\n\n", "CodeBleu_score": 22.858981345819995}, {"code": "def color_jitter(img, shift)\n\t\n    r = random.randint(-shift, shift)\n    g = random.randint(-shift, shift)\n    b = random.randint(-shift, shift)\n    jittered_img = img + torch.tensor([r, g, b]).float()\n    jittered_img = torch.clamp(jittered_img, 0, 1)\n    return jittered_img\n\n", "CodeBleu_score": 39.29644295761035}, {"code": "def color_jitter(img, shift)\n\t\n    img = img.copy()\n    for channel in range(3):\n        shift_value = np.random.uniform(-shift, shift)\n        img[:, :, channel] += shift_value\n        img[:, :, channel] = np.clip(img[:, :, channel], 0, 1)\n    return img\n\n", "CodeBleu_score": 36.33659413005685}]}
{"instance_id": "c87ccf1fbf48", "generation_result": [{"code": "def feed_data(self, data)\n\t\n    self.lq = data['lq'].to(self.device)\n    if 'gt' in data:\n        self.gt = data['gt'].to(self.device)\n    if 'left_eye' in data:\n        self.left_eye = data['left_eye']\n    if 'right_eye' in data:\n        self.right_eye = data['right_eye']\n    if'mouth' in data:\n        self.mouth = data['mouth']\n\n", "CodeBleu_score": 59.899511487904114}, {"code": "def feed_data(self, data)\n\t\n    self.lq = data['lq'].to(self.device)\n    if 'gt' in data:\n        self.gt = data['gt'].to(self.device)\n    if 'left_eye' in data:\n        self.left_eye = data['left_eye'].to(self.device)\n    if 'right_eye' in data:\n        self.right_eye = data['right_eye'].to(self.device)\n    if'mouth' in data:\n        self.mouth = data['mouth'].to(self.device)\n\n", "CodeBleu_score": 56.81975192363866}, {"code": "def feed_data(self, data)\n\t\n    self.lq = data['lq'].to(self.device)\n    if 'gt' in data:\n        self.gt = data['gt'].to(self.device)\n    if 'facial_components' in data:\n        self.left_eye = data['facial_components']['left_eye'].to(self.device)\n        self.right_eye = data['facial_components']['right_eye'].to(self.device)\n        self.mouth = data['facial_components']['mouth'].to(self.device)\n\n", "CodeBleu_score": 55.28326156469985}]}
{"instance_id": "8f8c7cb13037", "generation_result": [{"code": "def construct_img_pyramid(self)\n\t\n    pyramid = []\n    img = self.gt_img\n    for i in range(self.pyramid_levels):\n        pyramid.insert(0, img)\n        img = cv2.resize(img, (0, 0), fx=0.5, fy=0.5, interpolation=cv2.INTER_LINEAR)\n    return pyramid\n\n", "CodeBleu_score": 31.87809365507514}, {"code": "def construct_img_pyramid(self)\n\t\n    img = self.img_gt\n    img_pyramid = [img]\n    for i in range(self.n_scales - 1):\n        img = cv2.resize(img, (0, 0), fx=0.5, fy=0.5, interpolation=cv2.INTER_LINEAR)\n        img_pyramid.insert(0, img)\n    return img_pyramid\n\n", "CodeBleu_score": 32.385337797708544}, {"code": "def construct_img_pyramid(self)\n\t\n    img_pyramid = []\n    img_pyramid.append(self.gt)\n    for i in range(self.num_scales - 1):\n        img_pyramid.insert(0, cv2.resize(img_pyramid[0], None, fx=0.5, fy=0.5, interpolation=cv2.INTER_LINEAR))\n    return img_pyramid\n\n", "CodeBleu_score": 20.066178935873186}]}
{"instance_id": "51247274d40c", "generation_result": [{"code": "def save(self, epoch, current_iter)\n\t\n    if self.config.save_model:\n        if self.config.save_state:\n            torch.save(self.generator.state_dict(), self.config.save_path + \"/generator_\" + str(epoch) + \"_\" + str(current_iter) + \".pt\")\n            torch.save(self.ema_generator.state_dict(), self.config.save_path + \"/ema_generator_\" + str(epoch) + \"_\" + str(current_iter) + \".pt\")\n            torch.save(self.discriminator.state_dict(), self.config.save_path + \"/discriminator_\" + str(epoch) + \"_\" + str(current_iter) + \".pt\")\n            if self.config.facial_discriminators:\n                torch.save(self.left_eye_discriminator.state_dict(), self.config.save_path + \"/left_eye_discriminator_\" + str(epoch) + \"_\" + str(current_iter) + \".pt\")\n                torch.save(self.right_eye_discriminator.state_dict(), self.config.save_path + \"/right_eye_discriminator_\" + str(epoch) + \"_\" + str(current_iter) + \".pt\")\n                torch.save(self.mouth_discriminator.state_dict(), self.config.save_path + \"/mouth_discriminator_\" + str(epoch) + \"_\" + str(current_iter) + \".pt\")\n            torch.save(self.state, self.config.save_path + \"/state_\" + str(epoch) + \"_\" + str(current_iter) + \".pt\")\n        else:\n            torch.save(self.generator.state_dict(), self.config.save_path + \"/generator.pt\")\n            torch.save(self.ema_generator.state_dict(), self.config.save_path + \"/ema_generator.pt\")\n            torch.save(self.discriminator.state_dict(), self.config.save_path + \"/discriminator.pt\")\n            if self.config.facial_discriminators:\n                torch.save(self.left_eye_discriminator.state_dict(), self.config.save_path + \"/left_eye_discriminator.pt\")\n                torch.save(self.right_eye_discriminator.state_dict(), self", "CodeBleu_score": 38.404354500584645}, {"code": "def save(self, epoch, current_iter)\n\t\n    if self.opt.save_discriminators:\n        if self.opt.facial_discriminators:\n            torch.save(self.netD_left.state_dict(), os.path.join(self.opt.outf, \"netD_left_epoch_%d_iter_%d.pth\" % (epoch, current_iter)))\n            torch.save(self.netD_right.state_dict(), os.path.join(self.opt.outf, \"netD_right_epoch_%d_iter_%d.pth\" % (epoch, current_iter)))\n            torch.save(self.netD_mouth.state_dict(), os.path.join(self.opt.outf, \"netD_mouth_epoch_%d_iter_%d.pth\" % (epoch, current_iter)))\n        torch.save(self.netD.state_dict(), os.path.join(self.opt.outf, \"netD_epoch_%d_iter_%d.pth\" % (epoch, current_iter)))\n    torch.save(self.netG.state_dict(), os.path.join(self.opt.outf, \"netG_epoch_%d_iter_%d.pth\" % (epoch, current_iter)))\n    torch.save(self.netG_ema.state_dict(), os.path.join(self.opt.outf, \"netG_ema_epoch_%d_iter_%d.pth\" % (epoch, current_iter)))\n    torch.save(self.state, os.path.join(self.opt.outf, \"training_state_epoch_%d_iter_%d.pth\" % (epoch, current_iter)))\n\n", "CodeBleu_score": 38.21846203491046}, {"code": "def save(self, epoch, current_iter)\n\t\n    # Save the model\n    print(\"Saving the model...\")\n    torch.save(self.G.state_dict(), os.path.join(self.save_path, \"G_epoch_%d_iter_%d.pth\" % (epoch, current_iter)))\n    torch.save(self.G_ema.state_dict(), os.path.join(self.save_path, \"G_ema_epoch_%d_iter_%d.pth\" % (epoch, current_iter)))\n    torch.save(self.D.state_dict(), os.path.join(self.save_path, \"D_epoch_%d_iter_%d.pth\" % (epoch, current_iter)))\n    if self.use_facial_discriminators:\n        torch.save(self.D_left_eye.state_dict(), os.path.join(self.save_path, \"D_left_eye_epoch_%d_iter_%d.pth\" % (epoch, current_iter)))\n        torch.save(self.D_right_eye.state_dict(), os.path.join(self.save_path, \"D_right_eye_epoch_%d_iter_%d.pth\" % (epoch, current_iter)))\n        torch.save(self.D_mouth.state_dict(), os.path.join(self.save_path, \"D_mouth_epoch_%d_iter_%d.pth\" % (epoch, current_iter)))\n    torch.save(self.state, os.path.join(self.save_path, \"state_epoch_%d_iter_%d.pth\" % (epoch, current_iter)))\n    print(\"Model saved.\")\n\n", "CodeBleu_score": 38.25191815611346}]}
{"instance_id": "afa0dc8128c1", "generation_result": [{"code": "def clean_folder(folder)\n\t\n    for file in os.listdir(folder):\n        file_path = os.path.join(folder, file)\n        try:\n            if os.path.isfile(file_path):\n                os.unlink(file_path)\n            elif os.path.isdir(file_path):\n                shutil.rmtree(file_path)\n        except Exception as e:\n            print(f\"Failed to delete {file_path}. Reason: {e}\")\n\n", "CodeBleu_score": 63.01377748668477}, {"code": "def clean_folder(folder)\n\t\n    for item in os.listdir(folder):\n        path = os.path.join(folder, item)\n        try:\n            if os.path.isfile(path) or os.path.islink(path):\n                os.unlink(path)\n            elif os.path.isdir(path):\n                shutil.rmtree(path)\n        except Exception as e:\n            print('Failed to delete %s. Reason: %s' % (path, e))\n\n", "CodeBleu_score": 50.89590611390399}, {"code": "def clean_folder(folder)\n\t\n    for file in os.listdir(folder):\n        file_path = os.path.join(folder, file)\n        try:\n            if os.path.isfile(file_path) or os.path.islink(file_path):\n                os.unlink(file_path)\n            elif os.path.isdir(file_path):\n                shutil.rmtree(file_path)\n        except Exception as e:\n            print('Failed to delete %s. Reason: %s' % (file_path, e))\n\n", "CodeBleu_score": 65.82798113105332}]}
{"instance_id": "53588b481bdd", "generation_result": [{"code": "def next_batch(self, sample_size=1)\n\t\n    if sample_size > len(self.experiences):\n        sample_size = len(self.experiences)\n    sample_indices = np.random.choice(len(self.experiences), sample_size)\n    batch = [self.experiences[i] for i in sample_indices]\n    self.experiences = [self.experiences[i] for i in range(len(self.experiences)) if i not in sample_indices]\n    return batch\n\n", "CodeBleu_score": 47.93322334907434}, {"code": "def next_batch(self, sample_size=1)\n\t\n\n    if sample_size > len(self.experiences):\n        sample_size = len(self.experiences)\n\n    samples = random.sample(self.experiences, sample_size)\n    self.experiences = [exp for exp in self.experiences if exp not in samples]\n\n    return samples\n\n", "CodeBleu_score": 65.59319547320264}, {"code": "def next_batch(self, sample_size=1)\n\t\n    if sample_size > len(self.experiences):\n        sample_size = len(self.experiences)\n    sample = random.sample(self.experiences, sample_size)\n    self.experiences = [item for item in self.experiences if item not in sample]\n    return sample\n\n", "CodeBleu_score": 65.59319547320264}]}
{"instance_id": "6bfa6a4e621c", "generation_result": [{"code": "def find_estimated_top_configs(self)\n\t\n    # Flatten all_configs\n    flat_configs = []\n    for config in self.all_configs:\n        flat_configs.append(flatten_dict(config))\n\n    # Convert to numpy array\n    flat_configs = np.array(flat_configs)\n\n    # Convert to floats\n    flat_configs = flat_configs.astype(float)\n\n    # Predict cost\n    cost = self.cost_model.predict(flat_configs)\n\n    # Sort based on metric\n    if self.metric == \"min\":\n        sorted_indices = np.argsort(cost)\n    elif self.metric == \"max\":\n        sorted_indices = np.argsort(-cost)\n\n    return sorted_indices\n\n", "CodeBleu_score": 28.514666592787425}, {"code": "def find_estimated_top_configs(self)\n\t\n    # Flatten each configuration in all_configs\n    flat_configs = []\n    for config in self.all_configs:\n        flat_configs.append(flatten_config(config))\n    # Convert the flattened configurations into a numpy array\n    X = np.array(flat_configs)\n    # Predict estimates for the configurations using the cost model\n    y_pred = self.cost_model.predict(X)\n    # Sort the configurations based on the specified metric\n    if self.metric == \"min\":\n        sorted_indices = np.argsort(y_pred)\n    elif self.metric == \"max\":\n        sorted_indices = np.argsort(-y_pred)\n    else:\n        raise ValueError(\"Invalid metric specified.\")\n    # Return the indices of the top configurations\n    return sorted_indices[:self.top_k]\n\n", "CodeBleu_score": 26.645094548214765}, {"code": "def find_estimated_top_configs(self)\n\t\n    # Flatten each configuration in all_configs\n    flattened_configs = [flatten_config(config) for config in self.all_configs]\n    # Collect numerical values from flattened configurations\n    numerical_values = [\n        [value for value in config.values() if isinstance(value, (int, float))]\n        for config in flattened_configs\n    ]\n    # Convert numerical values to a numpy array\n    numerical_values = np.array(numerical_values)\n    # Predict estimates for the numerical values using the cost model\n    estimates = self.cost_model.predict(numerical_values)\n    # Sort the indices of the configurations based on the estimates\n    sorted_indices = np.argsort(estimates)\n    # Return the indices of the top configurations\n    return sorted_indices[-self.top_k:]\n\n", "CodeBleu_score": 22.423715025398096}]}
{"instance_id": "d4cf48413a57", "generation_result": [{"code": "def flatten(d, parent_key='', sep='_')\n\t\n    items = []\n    for k, v in d.items():\n        new_key = parent_key + sep + k if parent_key else k\n        if isinstance(v, collections.abc.MutableMapping):\n            items.extend(flatten(v, new_key, sep=sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)\n\n", "CodeBleu_score": 83.2096634628176}, {"code": "def flatten(d, parent_key='', sep='_')\n\t\n    items = []\n    for k, v in d.items():\n        new_key = parent_key + sep + k if parent_key else k\n        if isinstance(v, collections.abc.MutableMapping):\n            items.extend(flatten(v, new_key, sep=sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)\n\n", "CodeBleu_score": 83.2096634628176}, {"code": "def flatten(d, parent_key='', sep='_')\n\t\n    items = []\n    for k, v in d.items():\n        new_key = parent_key + sep + k if parent_key else k\n        if isinstance(v, collections.abc.MutableMapping):\n            items.extend(flatten(v, new_key, sep=sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)\n\n", "CodeBleu_score": 83.2096634628176}]}
{"instance_id": "c2a514249c25", "generation_result": [{"code": "def run_after_tuning(self)\n\t\n    if self.optimal_cmd:\n        subprocess.Popen(self.optimal_cmd, shell=True).wait()\n        self.logger.log_success(f'Successfully ran command: {self.optimal_cmd}')\n    else:\n        self.logger.log_info('Autotuning did not find an optimal configuration')\n\n", "CodeBleu_score": 29.602070098545575}, {"code": "def run_after_tuning(self)\n\t\n    if self.optimal_cmd is not None:\n        with open(self.log_file, \"a\") as f:\n            f.write(\"Running after tuning: \" + self.optimal_cmd + \"", "CodeBleu_score": 22.123520220663295}, {"code": "def run_after_tuning(self)\n\t\n    if self.optimal_cmd is not None:\n        print(\"Running optimal command: \", self.optimal_cmd)\n        p = subprocess.Popen(self.optimal_cmd, shell=True)\n        p.wait()\n        print(\"Success!\")\n    else:\n        print(\"No optimal command found.\")\n\n", "CodeBleu_score": 47.45429512814617}]}
{"instance_id": "61802de52df0", "generation_result": [{"code": "def run_job(self, exp: dict, reservations)\n\t\n    exp_id = exp['id']\n    exp_name = exp['name']\n    exp_script = exp['script']\n    exp_args = exp['args']\n    exp_port = exp['port']\n    exp_result_dir = exp['result_dir']\n    exp_user_args = exp['user_args']\n    exp_arg_map = exp['arg_map']\n    exp_arg_map_dict = exp['arg_map_dict']\n    exp_arg_map_dict_reversed = exp['arg_map_dict_reversed']\n    exp_user_arg_map = exp['user_arg_map']\n    exp_user_arg_map_dict = exp['user_arg_map_dict']\n    exp_user_arg_map_dict_reversed = exp['user_arg_map_dict_reversed']\n    exp_user_arg_map_dict_reversed_list = exp['user_arg_map_dict_reversed_list']\n    exp_user_arg_map_dict_reversed_list_type = exp['user_arg_map_dict_reversed_list_type']\n    exp_user_arg_map_dict_reversed_list_type_default = exp['user_arg_map_dict_reversed_list_type_default']\n    exp_user_arg_map_dict_reversed_list_type_default_list = exp['user_arg_map_dict_reversed_list_type_default_list']\n    exp_user_arg_map_dict_reversed_list_type_default_list_type = exp['user_arg_map_dict_reversed_list_type_default_list_type']\n    exp_user_arg_map_dict_reversed_list_type_default_list_type_default = exp['user_arg_map_dict_reversed_list_type_default_list_type_default']\n    exp_user_arg_map_dict_reversed_list_type_default_list_type_default_list = exp['user_arg_map_dict_reversed_list_type_default_list_type_default_list']\n    exp_user_arg_map_dict_reversed_list_type_default_list_type_default_list_type = exp['user_arg_map_dict_reversed", "CodeBleu_score": 8.478688242590724}, {"code": "def run_job(self, exp: dict, reservations)\n\t\n    exp_id = exp['exp_id']\n    exp_script = exp['script']\n    exp_args = exp['args']\n    exp_args_mapping = exp['args_mapping']\n    exp_master_port = exp['master_port']\n    exp_result_dir = exp['result_dir']\n    exp_result_dir_mapping = exp['result_dir_mapping']\n    exp_log_dir = exp['log_dir']\n    exp_log_dir_mapping = exp['log_dir_mapping']\n    exp_name = exp['name']\n    exp_name_mapping = exp['name_mapping']\n    exp_args_dict = {}\n    exp_args_dict['master_port'] = exp_master_port\n    exp_args_dict['result_dir'] = exp_result_dir\n    exp_args_dict['log_dir'] = exp_log_dir\n    exp_args_dict['name'] = exp_name\n    for arg in exp_args_mapping:\n        exp_args_dict[arg] = exp_args[arg]\n    for arg in exp_result_dir_mapping:\n        exp_args_dict[arg] = exp_result_dir\n    for arg in exp_log_dir_mapping:\n        exp_args_dict[arg] = exp_log_dir\n    for arg in exp_name_mapping:\n        exp_args_dict[arg] = exp_name\n    exp_args_str =''.join(map(lambda x: '--' + x +'' + str(exp_args_dict[x]), exp_args_dict))\n    print(exp_args_str)\n    exp_cmd = f'python {exp_script} {exp_args_str}'\n    exp_thread = threading.Thread(target=run_exp_thread, args=(self, exp_id, exp_cmd))\n    exp_thread.start()\n    self.running_experiments[exp_id] = {\n        'thread': exp_thread,\n       'script': exp_script,\n        'args': exp_args,\n        'args_mapping': exp_args_mapping,\n       'master_port': exp_master_port,\n       'result_dir': exp_result_dir,\n       'result_dir_mapping': exp_result_dir_mapping,\n        'log_dir':", "CodeBleu_score": 24.88113170815001}, {"code": "def run_job(self, exp: dict, reservations)\n\t\n\n    # set experiment master port\n    exp[\"master_port\"] = self.master_port\n    self.master_port += 1\n\n    # set experiment result directory\n    exp[\"result_dir\"] = self.result_dir\n\n    # update user arguments based on argument mappings\n    if \"argument_mappings\" in exp:\n        for arg_mapping in exp[\"argument_mappings\"]:\n            if \"user_arg\" in arg_mapping and \"exp_arg\" in arg_mapping:\n                exp[\"user_args\"][arg_mapping[\"user_arg\"]] = exp[arg_mapping[\"exp_arg\"]]\n\n    # create a new thread to run the experiment\n    thread = threading.Thread(target=self.run_experiment, args=(exp, reservations))\n    thread.start()\n\n    # store the running experiment details with a timestamp\n    self.running_experiments[exp[\"name\"]] = {\"thread\": thread, \"start_time\": time.time()}\n\n", "CodeBleu_score": 25.294606692353188}]}
{"instance_id": "4884e2e9665c", "generation_result": [{"code": "def save_exp_results_to_database(self, message, ranks=None, path=None):\n\t\n    if self.distributed_training_initialized:\n        if self.distributed_training_initialized:\n            message = f\"{self.rank} {message}\"\n    if self.logging_permitted:\n        with open(path, \"a\") as f:\n            f.write(json.dumps(message))\n            f.write(\"", "CodeBleu_score": 19.19835912066914}, {"code": "def save_exp_results_to_database(self, message, ranks=None, path=None):\n\t\n\n    if self.rank == 0:\n        if self.is_distributed and self.rank!= 0:\n            message = \"Rank {}: {}\".format(self.rank, message)\n        if self.is_distributed and self.rank == 0:\n            message = \"Rank {}: {}\".format(self.rank, message)\n        if self.is_distributed and self.rank == 0:\n            message = \"Rank {}: {}\".format(self.rank, message)\n        if self.is_distributed and self.rank == 0:\n            message = \"Rank {}: {}\".format(self.rank, message)\n        if self.is_distributed and self.rank == 0:\n            message = \"Rank {}: {}\".format(self.rank, message)\n        if self.is_distributed and self.rank == 0:\n            message = \"Rank {}: {}\".format(self.rank, message)\n        if self.is_distributed and self.rank == 0:\n            message = \"Rank {}: {}\".format(self.rank, message)\n        if self.is_distributed and self.rank == 0:\n            message = \"Rank {}: {}\".format(self.rank, message)\n        if self.is_distributed and self.rank == 0:\n            message = \"Rank {}: {}\".format(self.rank, message)\n        if self.is_distributed and self.rank == 0:\n            message = \"Rank {}: {}\".format(self.rank, message)\n        if self.is_distributed and self.rank == 0:\n            message = \"Rank {}: {}\".format(self.rank, message)\n        if self.is_distributed and self.rank == 0:\n            message = \"Rank {}: {}\".format(self.rank, message)\n        if self.is_distributed and self.rank == 0:\n            message = \"Rank {}: {}\".format(self.rank, message)\n        if self.is_distributed and self.rank == 0:\n            message = \"Rank {}: {}\".format(self.rank, message)\n        if self.is_distributed and self.rank == 0:\n            message = \"Rank {}: {}\".format(self.rank, message)\n        if self.is_distributed and self.rank == 0:\n            message = \"Rank {}: {}\".format(self.rank, message)\n        if self.is_distributed", "CodeBleu_score": 14.413776728664782}, {"code": "def save_exp_results_to_database(self, message, ranks=None, path=None):\n\t\n    if self.is_distributed_initialized:\n        ranks = ranks or self.get_global_ranks()\n        message = f\"rank {ranks[0]}: {message}\"\n    if self.is_logging_permitted:\n        if path is None:\n            path = self.log_path\n        with open(path, \"a\") as file:\n            file.write(message + \"", "CodeBleu_score": 25.587956808198076}]}
{"instance_id": "0429344c3c4d", "generation_result": [{"code": "def _build_tp_other_layer_map(self, layer_index: int)\n\t\n    if not self.layer_files:\n        return None\n    if layer_index < 0 or layer_index >= len(self.layer_files):\n        return None\n    prefix = self.layer_files[layer_index]\n    layer_files = [f for f in self.layer_files if f.startswith(prefix)]\n    layer_files.sort()\n    layer_files_map = {}\n    for i in range(self.tp_degree):\n        layer_files_map[i] = []\n    for i, f in enumerate(layer_files):\n        layer_files_map[i % self.tp_degree].append(f)\n    return layer_files_map\n\n", "CodeBleu_score": 39.89505868443953}, {"code": "def _build_tp_other_layer_map(self, layer_index: int)\n\t\n    if not self.layer_files:\n        self.logger.error(\"No layer files found.\")\n        raise ValueError(\"No layer files found.\")\n    if layer_index < 0 or layer_index >= len(self.layer_files):\n        self.logger.error(f\"Invalid layer index: {layer_index}.\")\n        raise ValueError(f\"Invalid layer index: {layer_index}.\")\n    layer_files = self.layer_files[layer_index]\n    layer_prefix = os.path.splitext(os.path.basename(layer_files[0]))[0]\n    layer_files = [f for f in layer_files if f.startswith(layer_prefix)]\n    if not layer_files:\n        self.logger.error(f\"No layer files found for layer index {layer_index}.\")\n        raise ValueError(f\"No layer files found for layer index {layer_index}.\")\n    layer_files = sorted(layer_files)\n    layer_map = {}\n    for file in layer_files:\n        partition_index = int(file.split(\"_\")[-1].split(\".\")[0])\n        if partition_index not in layer_map:\n            layer_map[partition_index] = []\n        layer_map[partition_index].append(file)\n    return layer_map\n\n", "CodeBleu_score": 36.682988546085014}, {"code": "def _build_tp_other_layer_map(self, layer_index: int)\n\t\n    # Check if there are any layer files\n    if not self.layer_files:\n        raise ValueError(f\"No layer files found for layer {layer_index}.\")\n\n    # Verify the layer index\n    if layer_index < 0 or layer_index >= len(self.layer_files):\n        raise ValueError(f\"Invalid layer index {layer_index}.\")\n\n    # Retrieve files with a specific prefix for the given layer\n    layer_prefix = self.layer_files[layer_index].split(\".\")[0]\n    layer_files = [\n        file for file in self.layer_files if file.startswith(layer_prefix)\n    ]\n\n    # Partition these files based on tp_degree\n    layer_files_partitioned = [\n        layer_files[i::self.tp_degree] for i in range(self.tp_degree)\n    ]\n\n    # Create and return a dictionary mapping partition indices to their corresponding file lists\n    return {\n        i: layer_files_partitioned[i] for i in range(self.tp_degree)\n    }\n\n", "CodeBleu_score": 36.15678398562322}]}
{"instance_id": "1b65ecb9be93", "generation_result": [{"code": "def system_info()\n\t\n    sys_info = {\n        \"Operating System\": platform.system(),\n        \"Architecture\": platform.architecture()[0],\n        \"Version\": platform.version(),\n        \"Processor\": platform.processor(),\n        \"Python Version\": platform.python_version(),\n    }\n    return sys_info\n\n", "CodeBleu_score": 51.29317400202436}, {"code": "def system_info()\n\t\n    sys_info = {}\n    sys_info[\"operating_system\"] = platform.system()\n    sys_info[\"architecture\"] = platform.architecture()[0]\n    sys_info[\"version\"] = platform.version()\n    sys_info[\"processor\"] = platform.processor()\n    sys_info[\"python_version\"] = platform.python_version()\n    return sys_info\n\n", "CodeBleu_score": 41.524310760438354}, {"code": "def system_info()\n\t\n    return {\n        \"os\": platform.system(),\n        \"arch\": platform.machine(),\n        \"version\": platform.version(),\n        \"processor\": platform.processor(),\n        \"python\": platform.python_version(),\n    }\n\n", "CodeBleu_score": 81.55701036135778}]}
{"instance_id": "969bcc5cf3b5", "generation_result": [{"code": "def cuda_info()\n\t\n    try:\n        import cupy\n    except ImportError:\n        return None, None, None\n    return cupy.cuda.get_device_name(0), cupy.cuda.is_available(), cupy.cuda.runtime.runtimeGetVersion()\n\n", "CodeBleu_score": 30.07233146690057}, {"code": "def cuda_info()\n\t\n    cuda_info_dict = {}\n    cuda_info_dict[\"cuda_available\"] = torch.cuda.is_available()\n    cuda_info_dict[\"cuda_version\"] = torch.version.cuda\n    cuda_info_dict[\"gpu_device_names\"] = [\n        torch.cuda.get_device_name(i) for i in range(torch.cuda.device_count())\n    ]\n    return cuda_info_dict\n\n", "CodeBleu_score": 49.19335097265577}, {"code": "def cuda_info()\n\t\n    cuda_available = torch.cuda.is_available()\n    cuda_version = torch.version.cuda\n    device_names = [torch.cuda.get_device_name(i) for i in range(torch.cuda.device_count())]\n    return device_names, cuda_available, cuda_version\n\n", "CodeBleu_score": 45.87352074370546}]}
{"instance_id": "a2ba908a4c70", "generation_result": [{"code": "def compute_encoder_accuracy(dataset_items, encoder_manager)\n\t\n    class_name_key = encoder_manager.config.get(\"class_name_key\")\n    class_id_to_name_mapping = encoder_manager.config.get(\"class_id_to_name_mapping\")\n    class_name_to_id_mapping = {v: k for k, v in class_id_to_name_mapping.items()}\n    class_name_to_accuracy = {}\n    for item in dataset_items:\n        audio_file = item[\"audio_file\"]\n        class_name = item[class_name_key]\n        class_id = class_name_to_id_mapping.get(class_name)\n        if class_id is None:\n            continue\n        embedding = encoder_manager.compute_embedding(audio_file)\n        predicted_class_id = encoder_manager.predict_class_id(embedding)\n        if predicted_class_id is None:\n            continue\n        predicted_class_name = class_id_to_name_mapping.get(predicted_class_id)\n        if predicted_class_name is None:\n            continue\n        if predicted_class_name!= class_name:\n            continue\n        class_name_to_accuracy[class_name] = class_name_to_accuracy.get(class_name, 0) + 1\n    total_count = len(dataset_items)\n    for class_name, count in class_name_to_accuracy.items():\n        accuracy = count / total_count * 100\n        print(f\"Accuracy for class '{class_name}': {accuracy:.2f}%\")\n    average_accuracy = sum(class_name_to_accuracy.values()) / total_count * 100\n    print(f\"Average accuracy across all classes: {average_accuracy:.2f}%\")\n\n", "CodeBleu_score": 34.46922778792474}, {"code": "def compute_encoder_accuracy(dataset_items, encoder_manager)\n\t\n    # Extract the class name key and optional class ID to class name mapping from the encoder manager's configuration\n    class_name_key = encoder_manager.config.get(\"class_name_key\", \"class_name\")\n    class_id_to_name = encoder_manager.config.get(\"class_id_to_name\")\n\n    # Initialize the accuracy dictionary to store the accuracy for each class\n    accuracy_dict = {}\n\n    # Iterate through each item in the dataset\n    for item in dataset_items:\n        # Extract the audio file and the true class name\n        audio_file = item[\"audio_file\"]\n        true_class_name = item[class_name_key]\n\n        # Compute the embedding for the audio file\n        embedding = encoder_manager.get_embedding(audio_file)\n\n        # Predict the class label if criteria and mappings are available\n        if encoder_manager.config.get(\"class_name_criteria\") and class_id_to_name:\n            # Predict the class label using the criteria and mappings\n            predicted_class_id = encoder_manager.predict_class_id(embedding)\n            predicted_class_name = class_id_to_name.get(predicted_class_id)\n        else:\n            # If criteria and mappings are not available, predict the class label using the encoder manager's predict function\n            predicted_class_name = encoder_manager.predict(embedding)\n\n        # Record the accuracy for the true class name and the predicted class name\n        if true_class_name not in accuracy_dict:\n            accuracy_dict[true_class_name] = {\"total\": 0, \"correct\": 0}\n        accuracy_dict[true_class_name][\"total\"] += 1\n        if true_class_name == predicted_class_name:\n            accuracy_dict[true_class_name][\"correct\"] += 1\n\n    # Calculate the accuracy for each class\n    for class_name, accuracy_data in accuracy_dict.items():\n        accuracy = accuracy_data[\"correct\"] / accuracy_data[\"total\"]\n        print(f\"Accuracy for class {class_name}: {accuracy:.4f}\")\n\n    # Calculate the average accuracy across all classes\n    total_accuracy = sum(accuracy_dict[class_name][\"correct\"] for class_name in accuracy_dict) / sum(\n        accuracy_dict[class_name][\"total\"] for class_name in accuracy_dict", "CodeBleu_score": 31.115448037672756}, {"code": "def compute_encoder_accuracy(dataset_items, encoder_manager)\n\t\n    # Retrieve class name key and optional class ID to class name mapping from the encoder manager's configuration\n    class_name_key = encoder_manager.config.get(\"class_name_key\", None)\n    class_id_to_name = encoder_manager.config.get(\"class_id_to_name\", None)\n\n    # Initialize the accuracy dictionary\n    accuracy_dict = {}\n\n    # Iterate over the dataset items\n    for item in dataset_items:\n        # Extract the audio file\n        audio_file = item[\"audio_file\"]\n\n        # Compute the embedding\n        embedding = encoder_manager.compute_embedding(audio_file)\n\n        # Predict the class label if criteria and mappings are available\n        if class_name_key and class_id_to_name:\n            # Retrieve the class name from the item\n            class_name = item[class_name_key]\n\n            # Retrieve the class ID from the class name\n            class_id = encoder_manager.class_name_to_id(class_name)\n\n            # Predict the class label\n            predicted_class_id = encoder_manager.predict_class(embedding)\n\n            # Check if the predicted class ID is valid\n            if predicted_class_id is not None:\n                # Retrieve the predicted class name\n                predicted_class_name = class_id_to_name[predicted_class_id]\n\n                # Check if the predicted class name is valid\n                if predicted_class_name is not None:\n                    # Update the accuracy dictionary\n                    if class_name in accuracy_dict:\n                        accuracy_dict[class_name][\"total\"] += 1\n                        if predicted_class_name == class_name:\n                            accuracy_dict[class_name][\"correct\"] += 1\n                    else:\n                        accuracy_dict[class_name] = {\"total\": 1, \"correct\": 1 if predicted_class_name == class_name else 0}\n\n    # Calculate and print the accuracy for each class and the average accuracy across all classes\n    total_correct = 0\n    total_total = 0\n    for class_name, accuracy_info in accuracy_dict.items():\n        total = accuracy_info[\"total\"]\n        correct = accuracy_info[\"correct\"]\n        accuracy = correct / total\n        print(f\"Accuracy for {class_name}: {accuracy:.2f} ({correct}/{total})\")\n        total_correct += correct\n        total_total += total\n    average_", "CodeBleu_score": 34.82838923482948}]}
{"instance_id": "e01ec69ad58a", "generation_result": [{"code": "def adjust_path_and_remove_silence(audio_path)\n\t   # check if the file already exists and force is not specified\n    if os.path.exists(audio_path) and not force:\n        print(f\"> \u2757 {audio_path} already exists. Skipping...\")\n        return audio_path, False\n\n    # get the output path by replacing the input directory path with the output directory path\n    out_path = audio_path.replace(input_dir, output_dir)\n\n    # create the necessary directory structure\n    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n\n    # remove silence from the audio using specified parameters\n    out_path, is_speech = remove_silence(\n        model_and_utils, audio_path, out_path, vad_sample_rate=vad_sample_rate, use_cuda=use_cuda\n    )\n\n    return out_path, is_speech\n\n", "CodeBleu_score": 40.81438051763503}, {"code": "def adjust_path_and_remove_silence(audio_path)\n\t   # get the VAD model and utils functions\n    model, get_speech_timestamps, _, collect_chunks = model_and_utils\n\n    # read ground truth wav and resample the audio for the VAD\n    try:\n        wav, gt_sample_rate = read_audio(audio_path)\n    except:\n        print(f\"> \u2757 Failed to read {audio_path}\")\n        return None, False\n\n    # if needed, resample the audio for the VAD model\n    if gt_sample_rate!= vad_sample_rate:\n        wav_vad = resample_wav(wav, gt_sample_rate, vad_sample_rate)\n    else:\n        wav_vad = wav\n\n    if use_cuda:\n        wav_vad = wav_vad.cuda()\n\n    # get speech timestamps from full audio file\n    speech_timestamps = get_speech_timestamps(wav_vad, model, sampling_rate=vad_sample_rate, window_size_samples=768)\n\n    # map the current speech_timestamps to the sample rate of the ground truth audio\n    new_speech_timestamps = map_timestamps_to_new_sr(\n        vad_sample_rate, gt_sample_rate, speech_timestamps, trim_just_beginning_and_end\n    )\n\n    # if have speech timestamps else save the wav\n    if new_speech_timestamps:\n        wav = collect_chunks(new_speech_timestamps, wav)\n        is_speech = True\n    else:\n        print(f\"> The file {audio_path} probably does not have speech please check it!!\")\n        is_speech = False\n\n    # save\n    torchaudio.save(out_path, wav[None, :], gt_sample_rate)\n    return out_path, is_speech\n\n", "CodeBleu_score": 30.075445448677772}, {"code": "def adjust_path_and_remove_silence(audio_path)\n\t   # get the VAD model and utils functions\n    model, get_speech_timestamps, _, collect_chunks = model_and_utils\n\n    # read ground truth wav and resample the audio for the VAD\n    try:\n        wav, gt_sample_rate = read_audio(audio_path)\n    except:\n        print(f\"> \u2757 Failed to read {audio_path}\")\n        return None, False\n\n    # if needed, resample the audio for the VAD model\n    if gt_sample_rate!= vad_sample_rate:\n        wav_vad = resample_wav(wav, gt_sample_rate, vad_sample_rate)\n    else:\n        wav_vad = wav\n\n    if use_cuda:\n        wav_vad = wav_vad.cuda()\n\n    # get speech timestamps from full audio file\n    speech_timestamps = get_speech_timestamps(wav_vad, model, sampling_rate=vad_sample_rate, window_size_samples=768)\n\n    # map the current speech_timestamps to the sample rate of the ground truth audio\n    new_speech_timestamps = map_timestamps_to_new_sr(\n        vad_sample_rate, gt_sample_rate, speech_timestamps, trim_just_beginning_and_end\n    )\n\n    # if have speech timestamps else save the wav\n    if new_speech_timestamps:\n        wav = collect_chunks(new_speech_timestamps, wav)\n        is_speech = True\n    else:\n        print(f\"> The file {audio_path} probably does not have speech please check it!!\")\n        is_speech = False\n\n    # save\n    torchaudio.save(out_path, wav[None, :], gt_sample_rate)\n    return out_path, is_speech\n\n\n", "CodeBleu_score": 30.075445448677772}]}
{"instance_id": "7e0bdb0feff8", "generation_result": [{"code": "def resample_files(input_dir, output_sr, output_dir=None, file_ext=\"wav\", n_jobs=10)\n\t\n    if output_dir is None:\n        output_dir = input_dir\n    else:\n        shutil.copytree(input_dir, output_dir)\n    files = glob.glob(f\"{output_dir}/**/*{file_ext}\", recursive=True)\n    Parallel(n_jobs=n_jobs)(\n        delayed(resample_file)(file, output_sr) for file in tqdm(files)\n    )\n    print(\"Done!\")\n\n", "CodeBleu_score": 23.525108535188725}, {"code": "def resample_files(input_dir, output_sr, output_dir=None, file_ext=\"wav\", n_jobs=10)\n\t\n    # create output directory if not provided\n    if not output_dir:\n        output_dir = os.path.join(input_dir, f\"resampled_{output_sr}\")\n    # copy input directory to output directory\n    shutil.copytree(input_dir, output_dir)\n    # recursively find all audio files in the output directory\n    audio_files = glob.glob(os.path.join(output_dir, \"**\", f\"*.{file_ext}\"), recursive=True)\n    # resample audio files in parallel\n    with Pool(n_jobs) as pool:\n        with tqdm(total=len(audio_files), desc=\"Resampling\") as pbar:\n            for _ in pool.imap_unordered(partial(resample_file, output_sr=output_sr), audio_files):\n                pbar.update()\n    # print completion message\n    print(f\"Resampled audio files to {output_sr} Hz in {output_dir}\")\n\n", "CodeBleu_score": 43.210499989303095}, {"code": "def resample_files(input_dir, output_sr, output_dir=None, file_ext=\"wav\", n_jobs=10)\n\t\n    if not output_dir:\n        output_dir = input_dir\n    copy_directory(input_dir, output_dir)\n    files = find_files(output_dir, file_ext=file_ext)\n    print(f\"Resampling {len(files)} files to {output_sr} Hz...\")\n    Parallel(n_jobs=n_jobs)(delayed(resample_file)(file, output_sr) for file in tqdm(files))\n    print(\"Done!\")\n\n", "CodeBleu_score": 26.12487536239912}]}
{"instance_id": "a1351834ca57", "generation_result": [{"code": "def evaluation(model, criterion, data_loader, global_step)\n\t\n    model.eval()\n    eval_loss = 0.0\n    with torch.no_grad():\n        for i, (inputs, labels) in enumerate(data_loader):\n            if i == 0:\n                # if the first batch, save the embeddings for UMAP plot\n                embeddings = model.get_embeddings(inputs.to(device))\n                num_classes_in_batch = len(np.unique(labels.numpy()))\n            else:\n                # if not the first batch, get the embeddings and concatenate them to the previous ones\n                embeddings = torch.cat(\n                    (\n                        embeddings,\n                        model.get_embeddings(inputs.to(device)),\n                    )\n                )\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            eval_loss += loss.item()\n        eval_loss = eval_loss / len(data_loader)\n        print(f\"Eval Loss: {eval_loss}\")\n        # plot the UMAP of the embeddings from the last batch\n        plot_embeddings(embeddings, num_classes_in_batch)\n        # log the evaluation statistics\n        wandb.log({\"eval_loss\": eval_loss}, step=global_step)\n    return eval_loss\n\n", "CodeBleu_score": 34.0440181407422}, {"code": "def evaluation(model, criterion, data_loader, global_step)\n\t\n    # switch to evaluation mode\n    model.eval()\n\n    # initialize variables\n    eval_loss = 0.0\n    embeddings = []\n    labels = []\n\n    # loop over batches\n    for batch_idx, (inputs, targets) in enumerate(data_loader):\n        # reshape inputs and labels\n        inputs = inputs.reshape(-1, inputs.shape[-1])\n        targets = targets.reshape(-1)\n\n        # dispatch to GPU if available\n        if torch.cuda.is_available():\n            inputs = inputs.cuda()\n            targets = targets.cuda()\n\n        # compute embeddings\n        with torch.no_grad():\n            _, embeddings_batch = model(inputs)\n\n        # reshape embeddings and labels\n        embeddings_batch = embeddings_batch.reshape(-1, embeddings_batch.shape[-1])\n        targets = targets.reshape(-1)\n\n        # append embeddings and labels to lists\n        embeddings.append(embeddings_batch.detach().cpu())\n        labels.append(targets.detach().cpu())\n\n        # compute loss\n        loss = criterion(embeddings_batch, targets)\n\n        # update evaluation loss\n        eval_loss += loss.item()\n\n    # average evaluation loss\n    eval_loss /= len(data_loader)\n\n    # reshape embeddings and labels\n    embeddings = torch.cat(embeddings, dim=0)\n    labels = torch.cat(labels, dim=0)\n\n    # plot embeddings\n    plot_embeddings(embeddings, len(np.unique(labels)))\n\n    # log evaluation statistics\n    wandb.log({\"eval_loss\": eval_loss}, step=global_step)\n    wandb.log({\"eval_loss_per_class\": eval_loss / len(np.unique(labels))}, step=global_step)\n\n    return eval_loss\n\n", "CodeBleu_score": 35.92687321887636}, {"code": "def evaluation(model, criterion, data_loader, global_step)\n\t\n    model.eval()\n    total_loss = 0\n    with torch.no_grad():\n        for batch_idx, (inputs, labels) in enumerate(data_loader):\n            inputs = inputs.reshape(-1, inputs.shape[-1])\n            labels = labels.reshape(-1)\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            total_loss += loss.item()\n            if batch_idx == 0:\n                embeddings = outputs.cpu().numpy()\n                num_classes_in_batch = labels.unique().shape[0]\n            elif batch_idx == 1:\n                embeddings = np.concatenate((embeddings, outputs.cpu().numpy()), axis=0)\n                num_classes_in_batch = labels.unique().shape[0]\n            else:\n                embeddings = np.concatenate((embeddings, outputs.cpu().numpy()), axis=0)\n                num_classes_in_batch = labels.unique().shape[0]\n        average_loss = total_loss / len(data_loader)\n        wandb.log({\"val_loss\": average_loss, \"global_step\": global_step})\n        fig = plot_embeddings(embeddings, num_classes_in_batch)\n        wandb.log({\"umap\": wandb.Image(fig)}, commit=False)\n        return average_loss\n\n", "CodeBleu_score": 38.64920868247413}]}
{"instance_id": "565f8ed09916", "generation_result": [{"code": "def register_config(model_name: str) -> Coqpit\n\t\n    if model_name == \"xtts\":\n        from coqpit.config.xtts_config import XttsConfig\n        return XttsConfig\n    config_path = \"coqpit.config.\"\n    config_name = to_snake(model_name) + \"_config\"\n    try:\n        config_class = find_module(config_path, config_name)\n    except ModuleNotFoundError:\n        config_class = find_module(config_path, \"default_config\")\n    return config_class\n\n", "CodeBleu_score": 36.5386687118151}, {"code": "def register_config(model_name: str) -> Coqpit\n\t\n    model_name = model_name.lower()\n    class_name = to_camel(model_name)\n    if model_name == \"xtts\":\n        module_name = \"xtts.config\"\n    else:\n        module_name = \"coqpit.config\"\n    try:\n        module = importlib.import_module(module_name)\n    except ModuleNotFoundError:\n        module = importlib.import_module(\"coqpit.config\")\n    try:\n        return getattr(module, class_name)\n    except AttributeError:\n        raise ModuleNotFoundError(f\"No configuration class found for model {model_name}\")\n\n", "CodeBleu_score": 33.82469189834365}, {"code": "def register_config(model_name: str) -> Coqpit\n\t\n    model_name = model_name.lower()\n    if model_name == \"xtts\":\n        from. import xtts_config\n        return xtts_config.XttsConfig\n    else:\n        try:\n            module_name = \"coqpit.configs.\" + model_name + \"_config\"\n            module = importlib.import_module(module_name)\n            class_name = to_camel(model_name) + \"Config\"\n            return getattr(module, class_name)\n        except ModuleNotFoundError:\n            for path in CONFIG_PATHS:\n                try:\n                    return find_module(path, model_name)\n                except ModuleNotFoundError:\n                    pass\n            raise ModuleNotFoundError(f\"Could not find configuration class for model {model_name}\")\n\n", "CodeBleu_score": 34.870157955827175}]}
{"instance_id": "0ea7862506a7", "generation_result": [{"code": "def load_config(config_path: str) -> Coqpit\n\t\n    config_dict = None\n    if config_path.endswith(\".json\"):\n        config_dict = read_json_with_comments(config_path)\n    elif config_path.endswith(\".yml\"):\n        config_dict = read_yaml(config_path)\n    else:\n        raise ValueError(f\" [!] {config_path} is not a valid configuration file.\")\n    model_name = _process_model_name(config_dict)\n    config_class = register_config(model_name)\n    config = config_class()\n    config.update_from_dict(config_dict)\n    return config", "CodeBleu_score": 33.79131050397458}, {"code": "def load_config(config_path: str) -> Coqpit\n\t\n    config_dict = None\n    if config_path.endswith(\".json\"):\n        config_dict = read_json_with_comments(config_path)\n    elif config_path.endswith(\".yaml\"):\n        with open(config_path, \"r\") as f:\n            config_dict = yaml.safe_load(f)\n    else:\n        raise ValueError(f\" [!] Unknown config file extension: {config_path}\")\n    model_name = _process_model_name(config_dict)\n    config_class = register_config(model_name)\n    config = config_class()\n    config.update_from_dict(config_dict)\n    return config", "CodeBleu_score": 41.762461826670304}, {"code": "def load_config(config_path: str) -> Coqpit\n\t\n    config_dict = None\n    if config_path.endswith(\".yaml\") or config_path.endswith(\".yml\"):\n        config_dict = read_yaml(config_path)\n    elif config_path.endswith(\".json\"):\n        config_dict = read_json_with_comments(config_path)\n    else:\n        raise ValueError(f\" [!] Config file extension is not supported. It should be either.yaml or.json. Given: {config_path}\")\n    model_name = _process_model_name(config_dict)\n    config_class = register_config(model_name)\n    config = config_class()\n    config.update_from_dict(config_dict)\n    return config\n", "CodeBleu_score": 37.15231227983583}]}
{"instance_id": "e0db68b63143", "generation_result": [{"code": "def list_files(basePath, validExts=None, contains=None)\n\t\n    for (dirpath, dirnames, filenames) in os.walk(basePath):\n        for filename in filenames:\n            if contains is not None and filename.find(contains) == -1:\n                continue\n\n            ext = filename[filename.rfind(\".\"):].lower()\n\n            if validExts is None or ext.endswith(validExts):\n                imagePath = os.path.join(dirpath, filename).replace(\" \", \"\\\\ \")\n                yield imagePath\n\n", "CodeBleu_score": 70.61019089422095}, {"code": "def list_files(basePath, validExts=None, contains=None)\n\t\n    for (dirpath, dirnames, filenames) in os.walk(basePath):\n        for filename in filenames:\n            if contains is not None and filename.find(contains) == -1:\n                continue\n\n            ext = filename[filename.rfind(\".\"):].lower()\n\n            if validExts is None or ext.endswith(validExts):\n                # construct the full path to the file and yield it\n                fullPath = os.path.join(dirpath, filename).replace(\"\\\\\", \"/\")\n                yield fullPath\n\n", "CodeBleu_score": 70.76881156807715}, {"code": "def list_files(basePath, validExts=None, contains=None)\n\t\n    for (dirpath, dirnames, filenames) in os.walk(basePath):\n        for filename in filenames:\n            if contains is not None and filename.find(contains) == -1:\n                continue\n\n            ext = filename[filename.rfind(\".\"):].lower()\n\n            if validExts is None or ext.endswith(validExts):\n                # construct the full path to the file and yield it\n                fullPath = os.path.join(dirpath, filename)\n                yield fullPath\n\n", "CodeBleu_score": 73.57576361848155}]}
{"instance_id": "b836fcaf0a27", "generation_result": [{"code": "def load_model(xtts_checkpoint, xtts_config, xtts_vocab)\n\t\n\n    # clear the GPU cache\n    clear_gpu_cache()\n\n    # check if the required paths are provided\n    if xtts_checkpoint is None or xtts_config is None or xtts_vocab is None:\n        return \"Please provide the paths to the XTTS checkpoint, config, and vocab.\"\n\n    # load the configuration from the provided path\n    with open(xtts_config, \"r\") as f:\n        config = json.load(f)\n\n    # initialize the XTTS model\n    model = XTTSModel(config)\n\n    # load the model checkpoint and vocabulary\n    model.load_checkpoint(config, checkpoint_path=xtts_checkpoint, vocab_path=xtts_vocab)\n\n    # move the model to the GPU if available\n    if torch.cuda.is_available():\n        model.cuda()\n\n    print(\"XTTS model loaded successfully!\")\n    return model\n\n", "CodeBleu_score": 49.34492359503619}, {"code": "def load_model(xtts_checkpoint, xtts_config, xtts_vocab)\n\t\n    clear_gpu_cache()\n    if xtts_checkpoint is None:\n        raise ValueError(\"Please provide a checkpoint path for XTTS.\")\n    if xtts_config is None:\n        raise ValueError(\"Please provide a config path for XTTS.\")\n    if xtts_vocab is None:\n        raise ValueError(\"Please provide a vocab path for XTTS.\")\n\n    config = load_config(xtts_config)\n    model = XTTS_MODEL(config)\n    model.load_checkpoint(\n        config=config,\n        checkpoint_path=xtts_checkpoint,\n        vocab_path=xtts_vocab,\n        eval=True,\n        strict=True,\n    )\n    if torch.cuda.is_available():\n        model.cuda()\n    print(\"XTTS model loaded.\")\n    return model\n\n", "CodeBleu_score": 48.03995788949677}, {"code": "def load_model(xtts_checkpoint, xtts_config, xtts_vocab)\n\t\n    clear_gpu_cache()\n    if xtts_checkpoint is None or xtts_config is None or xtts_vocab is None:\n        return \"Error: Missing checkpoint, config, or vocab path.\"\n    config = json.load(open(xtts_config))\n    model = XTTS_MODEL(config)\n    model.load_checkpoint(config, checkpoint_path=xtts_checkpoint, vocab_path=xtts_vocab)\n    if torch.cuda.is_available():\n        model.cuda()\n    print(\"Loaded XTTS model from checkpoint:\", xtts_checkpoint)\n    return model\n\n", "CodeBleu_score": 45.50532170936113}]}
{"instance_id": "c5d63aa0df26", "generation_result": [{"code": "def compute_embedding(self, x, num_frames=250, num_eval=10, return_mean=True, l2_norm=True)\n\t\n    hop_length = self.hop_length\n    if self.torch_spec:\n        hop_length = self.hop_length // self.sr\n    x_length = x.shape[0]\n    if num_frames > x_length:\n        num_frames = x_length\n    num_frames = num_frames - num_frames % hop_length\n    num_offset = num_eval * hop_length\n    offset_list = list(range(num_offset))\n    offset_list = [i for i in offset_list if i + num_frames <= x_length]\n    if len(offset_list) == 0:\n        offset_list = [0]\n    embedding_list = []\n    for offset in offset_list:\n        x_batch = x[offset : offset + num_frames]\n        x_batch = x_batch.unsqueeze(0)\n        embedding = self.model(x_batch)\n        embedding = embedding.squeeze(0)\n        embedding_list.append(embedding)\n    embedding = torch.stack(embedding_list)\n    if return_mean:\n        embedding = torch.mean(embedding, dim=0)\n        if l2_norm:\n            embedding = embedding / torch.norm(embedding)\n    return embedding\n\n", "CodeBleu_score": 41.31291207116081}, {"code": "def compute_embedding(self, x, num_frames=250, num_eval=10, return_mean=True, l2_norm=True)\n\t\n    if self.hop_length is not None:\n        num_frames = int(num_frames * self.hop_length)\n    if num_frames > x.shape[1]:\n        num_frames = x.shape[1]\n    offset_positions = np.linspace(0, x.shape[1] - num_frames, num_eval).astype(int)\n    embeddings = []\n    for offset in offset_positions:\n        batch = x[:, offset : offset + num_frames]\n        batch = batch.unsqueeze(0)\n        embedding = self.inference(batch)\n        embeddings.append(embedding)\n    embeddings = torch.stack(embeddings)\n    if return_mean:\n        embeddings = torch.mean(embeddings, dim=0)\n        if l2_norm:\n            embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=0)\n    return embeddings\n\n", "CodeBleu_score": 47.79910133942731}, {"code": "def compute_embedding(self, x, num_frames=250, num_eval=10, return_mean=True, l2_norm=True)\n\t\n    # Ensure that the number of frames does not exceed the input length\n    num_frames = min(num_frames, x.shape[1])\n    # Calculate the offset positions for evaluation\n    offsets = np.linspace(0, x.shape[1] - num_frames, num_eval, dtype=int)\n    # Extract frames at these offsets and concatenate them into a batch\n    batch = []\n    for offset in offsets:\n        batch.append(x[:, offset : offset + num_frames])\n    batch = torch.cat(batch, dim=0)\n    # Perform inference to obtain embeddings\n    embeddings = self.inference(batch)\n    # Compute the mean of the embeddings and return the result, optionally normalizing them with L2 norm\n    if return_mean:\n        embeddings = torch.mean(embeddings, dim=0)\n        if l2_norm:\n            embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=0)\n    return embeddings\n\n", "CodeBleu_score": 36.84656229726187}]}
{"instance_id": "06f5b0335b26", "generation_result": [{"code": "def create_layer(self, block, planes, blocks, stride=1)\n\t\n    downsample = None\n    if stride!= 1 or self.inplanes!= planes * block.expansion:\n        downsample = nn.Sequential(\n            nn.Conv2d(\n                self.inplanes,\n                planes * block.expansion,\n                kernel_size=1,\n                stride=stride,\n                bias=False,\n            ),\n            nn.BatchNorm2d(planes * block.expansion),\n        )\n    layers = []\n    layers.append(block(self.inplanes, planes, stride, downsample))\n    self.inplanes = planes * block.expansion\n    for i in range(1, blocks):\n        layers.append(block(self.inplanes, planes))\n    return nn.Sequential(*layers)\n\n", "CodeBleu_score": 80.82383622378171}, {"code": "def create_layer(self, block, planes, blocks, stride=1)\n\t\n    downsample = None\n    if stride!= 1 or self.inplanes!= planes * block.expansion:\n        downsample = nn.Sequential(\n            nn.Conv2d(self.inplanes, planes * block.expansion,\n                      kernel_size=1, stride=stride, bias=False),\n            nn.BatchNorm2d(planes * block.expansion),\n        )\n\n    layers = []\n    layers.append(block(self.inplanes, planes, stride, downsample))\n    self.inplanes = planes * block.expansion\n    for i in range(1, blocks):\n        layers.append(block(self.inplanes, planes))\n\n    return nn.Sequential(*layers)\n\n", "CodeBleu_score": 85.54080921158837}, {"code": "def create_layer(self, block, planes, blocks, stride=1)\n\t\n    downsample = None\n    if stride!= 1 or self.inplanes!= planes * block.expansion:\n        downsample = nn.Sequential(\n            nn.Conv2d(\n                self.inplanes,\n                planes * block.expansion,\n                kernel_size=1,\n                stride=stride,\n                bias=False,\n            ),\n            nn.BatchNorm2d(planes * block.expansion),\n        )\n    layers = []\n    layers.append(block(self.inplanes, planes, stride, downsample))\n    self.inplanes = planes * block.expansion\n    for i in range(1, blocks):\n        layers.append(block(self.inplanes, planes))\n    return nn.Sequential(*layers)\n\n", "CodeBleu_score": 80.82383622378171}]}
{"instance_id": "9ac60597b633", "generation_result": [{"code": "def plot_embeddings(embeddings, num_classes_in_batch)\n\t\n    num_classes = embeddings.shape[0]\n    if num_classes_in_batch > 10:\n        num_classes = 10\n    num_per_class = int(num_classes_in_batch / num_classes)\n    colors = [\"tab:blue\", \"tab:orange\", \"tab:green\", \"tab:red\", \"tab:purple\", \"tab:brown\", \"tab:pink\", \"tab:gray\", \"tab:olive\", \"tab:cyan\"]\n    fig, ax = plt.subplots()\n    for i in range(num_classes):\n        start = i * num_per_class\n        end = start + num_per_class\n        x = embeddings[i, start:end, 0]\n        y = embeddings[i, start:end, 1]\n        ax.scatter(x, y, c=colors[i], label=f\"Class {i}\", s=10)\n    ax.legend()\n    ax.set_title(\"UMAP projection\")\n    ax.set_aspect(\"equal\")\n    fig.savefig(\"umap.png\")\n    return fig\n\n", "CodeBleu_score": 32.476833847841796}, {"code": "def plot_embeddings(embeddings, num_classes_in_batch)\n\t\n    # UMAP\n    umap_embeddings = umap.UMAP(\n        n_neighbors=10,\n        min_dist=0.0,\n        metric=\"cosine\",\n        random_state=42,\n    ).fit_transform(embeddings)\n    # Plot\n    fig = plt.figure(figsize=(12, 12))\n    plt.scatter(\n        umap_embeddings[:, 0],\n        umap_embeddings[:, 1],\n        c=labels,\n        cmap=\"Spectral\",\n        s=0.1,\n    )\n    plt.title(\"UMAP projection\")\n    plt.gca().set_aspect(\"equal\", \"datalim\")\n    plt.colorbar(boundaries=np.arange(num_classes_in_batch) - 0.5).set_ticks(\n        np.arange(num_classes_in_batch)\n    )\n    plt.savefig(\"umap.png\")\n    plt.show()\n    return fig\n\n", "CodeBleu_score": 30.256891283910576}, {"code": "def plot_embeddings(embeddings, num_classes_in_batch)\n\t\n\n    # calculate the number of utterances per class\n    num_utterances_per_class = np.sum(num_classes_in_batch, axis=0)\n\n    # limit the number of classes to 10 if necessary\n    if len(num_utterances_per_class) > 10:\n        num_utterances_per_class = num_utterances_per_class[:10]\n        embeddings = embeddings[:10]\n\n    # calculate the total number of utterances\n    num_utterances = np.sum(num_utterances_per_class)\n\n    # calculate the cumulative sum of utterances\n    cumulative_sum = np.cumsum(num_utterances_per_class)\n\n    # create a list of colors\n    colors = [\"red\", \"blue\", \"green\", \"orange\", \"purple\", \"brown\", \"pink\", \"gray\", \"olive\", \"cyan\"]\n\n    # create a figure and axis\n    fig, ax = plt.subplots()\n\n    # calculate UMAP for embeddings\n    umap_embeddings = umap.UMAP().fit_transform(embeddings)\n\n    # plot the embeddings\n    for i, num_utterances_class in enumerate(num_utterances_per_class):\n        start_index = cumulative_sum[i] - num_utterances_class\n        end_index = cumulative_sum[i]\n        ax.scatter(\n            umap_embeddings[start_index:end_index, 0],\n            umap_embeddings[start_index:end_index, 1],\n            c=colors[i],\n            label=f\"Class {i}\",\n        )\n\n    # add a legend\n    ax.legend()\n\n    # set the title\n    ax.set_title(\"UMAP projection\")\n\n    # adjust the aspect ratio\n    ax.set_aspect(\"equal\")\n\n    # save the figure\n    fig.savefig(\"umap.png\")\n\n    # return the figure\n    return fig\n\n", "CodeBleu_score": 34.38994686792704}]}
{"instance_id": "19c2b4af256d", "generation_result": [{"code": "def calc_cosine_sim(self, dvecs, centroids)\n\t\n    cosine_sim = []\n    for spkr in range(self.num_speakers):\n        for utt in range(self.num_utts):\n            new_centroids = self.calc_new_centroids(dvecs, centroids, spkr, utt)\n            sim = torch.nn.functional.cosine_similarity(\n                dvecs[spkr, utt], new_centroids, dim=0\n            )\n            sim = torch.clamp(sim, min=1e-05)\n            cosine_sim.append(sim)\n    cosine_sim = torch.stack(cosine_sim)\n    return cosine_sim\n\n", "CodeBleu_score": 31.750101305379186}, {"code": "def calc_cosine_sim(self, dvecs, centroids)\n\t\n    cos_sim = []\n    for spkr in range(self.num_spkrs):\n        for utt in range(self.num_utts):\n            new_centroids = self.calc_new_centroids(dvecs, centroids, spkr, utt)\n            sim = torch.nn.functional.cosine_similarity(\n                dvecs[spkr, utt].unsqueeze(0), new_centroids, dim=1\n            )\n            sim = torch.clamp(sim, min=1e-12)\n            cos_sim.append(sim)\n    cos_sim = torch.cat(cos_sim)\n    cos_sim = cos_sim.reshape(self.num_spkrs, self.num_utts, self.num_spkrs)\n    return cos_sim\n\n", "CodeBleu_score": 34.327696455855694}, {"code": "def calc_cosine_sim(self, dvecs, centroids)\n\t\n    sim_mat = []\n    for spkr in range(dvecs.shape[0]):\n        for utt in range(dvecs.shape[1]):\n            new_centroids = self.calc_new_centroids(dvecs, centroids, spkr, utt)\n            sim = torch.cosine_similarity(dvecs[spkr, utt], new_centroids, dim=1)\n            sim = torch.clamp(sim, min=1e-06)\n            sim_mat.append(sim)\n    sim_mat = torch.stack(sim_mat)\n    return sim_mat\n\n", "CodeBleu_score": 31.19651358441243}]}
{"instance_id": "c525b411a1e7", "generation_result": [{"code": " def embed_loss_softmax(self, dvecs, cos_sim_matrix)\n\t\n    losses = []\n    for i, dvec in enumerate(dvecs):\n        dvec = dvec.unsqueeze(0)\n        cos_sim_matrix = cos_sim_matrix.unsqueeze(0)\n        neg_log_softmax = F.log_softmax(-cos_sim_matrix, dim=2)\n        neg_log_softmax = neg_log_softmax.squeeze(0)\n        neg_log_softmax_i = neg_log_softmax[i]\n        losses.append(neg_log_softmax_i)\n    losses = torch.stack(losses)\n    return losses\n\n\n def embed_loss_softmax_batch(self, dvecs, cos_sim_matrix):\n    \"\"\"The function calculates the embedding loss using softmax for a batch of embeddings.", "CodeBleu_score": 26.27445113737643}, {"code": " def embed_loss_softmax(self, dvecs, cos_sim_matrix)\n\t\n    losses = []\n    for dvec in dvecs:\n        # cos_sim_matrix = torch.matmul(dvec, dvecs.T)\n        # cos_sim_matrix = torch.matmul(dvec, dvecs.T)\n        # cos_sim_matrix = torch.matmul(dvec, dvecs.T)\n        # cos_sim_matrix = torch.matmul(dvec, dvecs.T)\n        # cos_sim_matrix = torch.matmul(dvec, dvecs.T)\n        # cos_sim_matrix = torch.matmul(dvec, dvecs.T)\n        # cos_sim_matrix = torch.matmul(dvec, dvecs.T)\n        # cos_sim_matrix = torch.matmul(dvec, dvecs.T)\n        # cos_sim_matrix = torch.matmul(dvec, dvecs.T)\n        # cos_sim_matrix = torch.matmul(dvec, dvecs.T)\n        # cos_sim_matrix = torch.matmul(dvec, dvecs.T)\n        # cos_sim_matrix = torch.matmul(dvec, dvecs.T)\n        # cos_sim_matrix = torch.matmul(dvec, dvecs.T)\n        # cos_sim_matrix = torch.matmul(dvec, dvecs.T)\n        # cos_sim_matrix = torch.matmul(dvec, dvecs.T)\n        # cos_sim_matrix = torch.matmul(dvec, dvecs.T)\n        # cos_sim_matrix = torch.matmul(dvec, dvecs.T)\n        # cos_sim_matrix = torch.matmul(dvec, dvecs.T)\n        # cos_sim_matrix = torch.matmul(dvec, dvecs.T)\n        # cos_sim_matrix = torch.matmul(dvec, dvecs.T)\n        # cos_sim_matrix = torch.matmul(dvec, dvecs.T)\n        # cos_sim_matrix = torch.matmul(dvec, dvecs.T)\n        # cos_sim_matrix = torch.matmul(dvec, dvecs.T)\n        # cos_sim_matrix = torch.matmul(dvec, dvecs.T)\n        # cos_sim_matrix = torch.matmul(dvec, dvecs.T)\n        # cos_sim_matrix = torch.matmul(dvec, dvecs.T)\n        # cos_sim_", "CodeBleu_score": 3.3439636761234626}, {"code": " def embed_loss_softmax(self, dvecs, cos_sim_matrix)\n\t\n    losses = []\n    for i in range(dvecs.shape[0]):\n        # Compute the negative log softmax of the cosine similarity matrix for each embedding\n        neg_log_softmax = -torch.log_softmax(cos_sim_matrix[i], dim=1)\n        # Stack the losses for each row into tensors\n        losses.append(neg_log_softmax)\n    # Return the final stacked tensor of losses\n    return torch.stack(losses)\n", "CodeBleu_score": 31.59577450484538}]}
{"instance_id": "25ac3945da2e", "generation_result": [{"code": "def embed_loss_contrast(self, dvecs, cos_sim_matrix)\n\t\n    loss_row = []\n    for i in range(len(dvecs)):\n        # exclude the current centroid\n        cos_sim_matrix[i, i] = 0\n        # compute the sigmoid of the cosine similarity matrix\n        sigmoid_matrix = torch.sigmoid(cos_sim_matrix)\n        # compute the contrast loss\n        contrast_loss = torch.log(sigmoid_matrix[i, :]).sum()\n        loss_row.append(contrast_loss)\n    # stack the losses for each row into tensors\n    loss_row = torch.stack(loss_row)\n    # return the final stacked tensor of losses\n    return loss_row\n\n", "CodeBleu_score": 28.306758689423955}, {"code": "def embed_loss_contrast(self, dvecs, cos_sim_matrix)\n\t\n    loss_row = []\n    for i in range(self.batch_size):\n        # Compute the sigmoid of the cosine similarity matrix for each embedding\n        sigmoid_matrix = torch.sigmoid(cos_sim_matrix[i])\n        # For each embedding, exclude the current centroid\n        sigmoid_matrix[i, self.centroids[i]] = 0\n        # Compute the contrast loss\n        contrast_loss = -torch.log(sigmoid_matrix[i, self.labels[i]] / torch.sum(sigmoid_matrix[i]))\n        # Append the result to the loss row\n        loss_row.append(contrast_loss)\n    # Stack the losses for each row into tensors\n    loss_tensor = torch.stack(loss_row)\n    # Return the final stacked tensor of losses\n    return loss_tensor\n\n", "CodeBleu_score": 29.5257546970262}, {"code": "def embed_loss_contrast(self, dvecs, cos_sim_matrix)\n\t\n    losses = []\n    for i, dvec in enumerate(dvecs):\n        dvec = dvec.unsqueeze(0)\n        cos_sim_matrix_i = cos_sim_matrix[i, :]\n        cos_sim_matrix_i = cos_sim_matrix_i.unsqueeze(0)\n        cos_sim_matrix_i = torch.cat([cos_sim_matrix_i, dvec], dim=1)\n        sigmoid_matrix_i = torch.sigmoid(cos_sim_matrix_i)\n        contrast_loss_i = torch.mean(-torch.log(sigmoid_matrix_i[:, 0] + 1e-10))\n        losses.append(contrast_loss_i)\n    losses = torch.stack(losses)\n    return losses\n\n", "CodeBleu_score": 29.04619191566174}]}
{"instance_id": "878d14ffabd2", "generation_result": [{"code": "def forward(self, x, _label=None)\n\t\n    if self.training:\n        assert (\n            x.shape[1] > 1\n        ), \"GE2E loss requires at least 2 utterances per speaker.\"\n        centroids = self.calc_centroids(x)\n        cos_sim_matrix = self.calc_cosine_sim(x, centroids)\n        loss = self.calc_loss(cos_sim_matrix)\n        return loss.mean()\n    else:\n        return x\n\n", "CodeBleu_score": 43.08782102100867}, {"code": "def forward(self, x, _label=None)\n\t\n    if x.size(0) < 2:\n        raise ValueError(\"GE2E loss requires at least 2 speakers\")\n    if x.size(1) < 2:\n        raise ValueError(\"GE2E loss requires at least 2 utterances per speaker\")\n\n    # compute centroids for each speaker\n    centroids = self.calc_centroids(x)\n\n    # compute cosine similarity matrix\n    cos_sim_matrix = self.calc_cosine_sim(x, centroids)\n\n    # scale and shift cosine similarity matrix\n    cos_sim_matrix = self.scale * cos_sim_matrix + self.shift\n\n    # compute embedding loss\n    loss = torch.mean(self.loss_fn(cos_sim_matrix))\n\n    return loss", "CodeBleu_score": 44.09757560810704}, {"code": "def forward(self, x, _label=None)\n\t\n    if x.shape[1] < 2:\n        raise ValueError(\"GE2E loss requires at least 2 utterances per speaker!\")\n    centroids = self.calc_centroids(x)\n    cos_sim_matrix = self.calc_cosine_sim(x, centroids)\n    loss = self.calc_loss(cos_sim_matrix)\n    return loss.mean()\n\n", "CodeBleu_score": 45.11910768242892}]}
{"instance_id": "86a364c72caa", "generation_result": [{"code": "def get_voices(extra_voice_dirs: List[str] = [])\n\toices = {}\n    voice_dirs = [os.path.join(os.path.dirname(os.path.abspath(__file__)), \"voices\")] + extra_voice_dirs\n    for voice_dir in voice_dirs:\n        for root, _, files in os.walk(voice_dir):\n            if len(files) == 0:\n                continue\n            voice_name = os.path.basename(root)\n            if voice_name in voices:\n                raise RuntimeError(f\"Duplicate voice name: {voice_name}\")\n            voices[voice_name] = []\n            for file in files:\n                if file.endswith(\".npz\"):\n                    voices[voice_name].append(os.path.join(root, file))\n                elif file.endswith(\".wav\"):\n                    voices[voice_name].append(os.path.join(root, file))\n                elif file.endswith(\".mp3\"):\n                    voices[voice_name].append(os.path.join(root, file))\n    return voices\n\n", "CodeBleu_score": 27.73940156084855}, {"code": "def get_voices(extra_voice_dirs: List[str] = [])\n\toice_dirs = [os.path.join(os.path.dirname(__file__), \"voices\")] + extra_voice_dirs\n    voices = {}\n    for voice_dir in voice_dirs:\n        for root, dirs, files in os.walk(voice_dir):\n            for file in files:\n                if file.endswith(\".npz\"):\n                    voice_name = os.path.basename(root)\n                    if voice_name not in voices:\n                        voices[voice_name] = []\n                    voices[voice_name].append(os.path.join(root, file))\n                elif file.endswith(\".wav\") or file.endswith(\".mp3\"):\n                    voice_name = os.path.basename(root)\n                    if voice_name not in voices:\n                        voices[voice_name] = []\n                    voices[voice_name].append(os.path.join(root, file))\n    return voices\n\n", "CodeBleu_score": 28.018774838503052}, {"code": "def get_voices(extra_voice_dirs: List[str] = [])\n\toices = {}\n    for voice_dir in extra_voice_dirs:\n        voices[voice_dir] = []\n    for voice_dir in [VOICES_DIR] + extra_voice_dirs:\n        for root, _, files in os.walk(voice_dir):\n            for file in files:\n                if file.endswith(\".npz\"):\n                    voices[root] = voices.get(root, []) + [os.path.join(root, file)]\n                elif file.endswith(\".wav\"):\n                    voices[root] = voices.get(root, []) + [os.path.join(root, file)]\n                elif file.endswith(\".mp3\"):\n                    voices[root] = voices.get(root, []) + [os.path.join(root, file)]\n    return voices\n\n", "CodeBleu_score": 29.0218252773232}]}
{"instance_id": "02f20c8917e6", "generation_result": [{"code": "def load_voice(model, voice: str, extra_voice_dirs: List[str] = [])\n\t  if voice == \"random\":\n        return None, None, None\n\n    voices = get_voices(extra_voice_dirs)\n    paths = voices[voice]\n\n    # bark only uses a single sample for cloning\n    if len(paths) > 1:\n        raise ValueError(f\"Voice {voice} has multiple paths: {paths}\")\n\n    try:\n        path = voices[voice]\n    except KeyError as e:\n        raise KeyError(f\"Voice {voice} not found in {extra_voice_dirs}\") from e\n\n    if len(paths) == 1 and paths[0].endswith(\".npz\"):\n        return load_npz(path[0])\n\n    audio_path = paths[0]\n    # replace the file extension with.npz\n    output_path = os.path.splitext(audio_path)[0] + \".npz\"\n    generate_voice(audio=audio_path, model=model, output_path=output_path)\n    return load_voice(model, voice, extra_voice_dirs)\n\n", "CodeBleu_score": 96.09328500721692}, {"code": "def load_voice(model, voice: str, extra_voice_dirs: List[str] = [])\n\t  if voice == \"random\":\n        return None, None, None\n\n    voices = get_voices(extra_voice_dirs)\n    paths = voices[voice]\n\n    # bark only uses a single sample for cloning\n    if len(paths) > 1:\n        raise ValueError(f\"Voice {voice} has multiple paths: {paths}\")\n\n    try:\n        path = voices[voice]\n    except KeyError as e:\n        raise KeyError(f\"Voice {voice} not found in {extra_voice_dirs}\") from e\n\n    if len(paths) == 1 and paths[0].endswith(\".npz\"):\n        return load_npz(path[0])\n\n    audio_path = paths[0]\n    # replace the file extension with.npz\n    output_path = os.path.splitext(audio_path)[0] + \".npz\"\n    generate_voice(audio=audio_path, model=model, output_path=output_path)\n    return load_voice(model, voice, extra_voice_dirs)\n\n", "CodeBleu_score": 96.09328500721692}, {"code": "def load_voice(model, voice: str, extra_voice_dirs: List[str] = [])\n\t  if voice == \"random\":\n        return None, None, None\n\n    voices = get_voices(extra_voice_dirs)\n    paths = voices[voice]\n\n    # bark only uses a single sample for cloning\n    if len(paths) > 1:\n        raise ValueError(f\"Voice {voice} has multiple paths: {paths}\")\n\n    try:\n        path = voices[voice]\n    except KeyError as e:\n        raise KeyError(f\"Voice {voice} not found in {extra_voice_dirs}\") from e\n\n    if len(paths) == 1 and paths[0].endswith(\".npz\"):\n        return load_npz(path[0])\n\n    audio_path = paths[0]\n    # replace the file extension with.npz\n    output_path = os.path.splitext(audio_path)[0] + \".npz\"\n    generate_voice(audio=audio_path, model=model, output_path=output_path)\n    return load_voice(model, voice, extra_voice_dirs)\n\n", "CodeBleu_score": 96.09328500721692}]}
{"instance_id": "9458e981389f", "generation_result": [{"code": "def _relative_shift(self, pos_score: torch.Tensor) -> torch.Tensor\n\t\n    dim = pos_score.shape\n    zero_tensor = torch.zeros(dim[0], dim[1], 1, device=pos_score.device)\n    pos_score = torch.cat([zero_tensor, pos_score], dim=2)\n    pos_score = pos_score.reshape(dim[0], dim[1], dim[2], dim[3] + 1)[:, :, :, 1:]\n    return pos_score\n\n", "CodeBleu_score": 21.160078532906862}, {"code": "def _relative_shift(self, pos_score: torch.Tensor) -> torch.Tensor\n\t\n    batch_size, num_heads, seq_len, head_dim = pos_score.shape\n    pos_score = pos_score.reshape(batch_size, num_heads, seq_len, 1, head_dim)\n    pos_score = torch.cat([torch.zeros_like(pos_score), pos_score], dim=-2)\n    pos_score = pos_score.reshape(batch_size, num_heads, seq_len, -1)\n    pos_score = pos_score[:, :, :, :-head_dim]\n    return pos_score\n\n", "CodeBleu_score": 28.235407865779194}, {"code": "def _relative_shift(self, pos_score: torch.Tensor) -> torch.Tensor\n\t\n    dim = pos_score.shape\n    zero = torch.zeros((dim[0], dim[1], 1), dtype=pos_score.dtype, device=pos_score.device)\n    pos_score = torch.cat((zero, pos_score), dim=2)\n    pos_score = pos_score.reshape((dim[0], dim[1], dim[2], dim[3] + 1))\n    pos_score = pos_score[:, :, :, 1:]\n    return pos_score\n\n", "CodeBleu_score": 23.311856323178958}]}
{"instance_id": "c046d4f00e80", "generation_result": [{"code": "def run_padded_sequence(self, context, lens)\n\t\n    # Extract the context up to the specified length\n    context = context[:, :lens.max()]\n\n    # Apply the convolutional layers to the context\n    for layer in self.convolutions:\n        context = layer(context)\n        context = F.relu(context)\n        context = self.dropout(context)\n\n    # Pad the sequence to create a consistent batch size\n    context = pad_sequence(context, batch_first=True)\n\n    return context\n\n", "CodeBleu_score": 23.051546324779977}, {"code": "def run_padded_sequence(self, context, lens)\n\t\n    # Extract the context up to the specified length\n    context = context[:, :max(lens)]\n    # Apply the convolutional layers\n    for layer in self.conv_layers:\n        context = layer(context)\n        context = F.relu(context)\n        context = self.dropout(context)\n    # Store the results\n    context = context.transpose(0, 1)\n    # Pad the context to create a consistent batch size\n    context = pad_sequence(context, batch_first=True)\n    return context\n\n", "CodeBleu_score": 24.69449416989774}, {"code": "def run_padded_sequence(self, context, lens)\n\t\n    # Extract the context up to the specified length\n    context = context[:, :max(lens)]\n    # Apply the convolutional layers and store the results\n    conv_out = self.conv_layers(context)\n    # Pad the processed contexts to create a consistent batch size\n    padded_conv_out = torch.nn.utils.rnn.pad_sequence(conv_out, batch_first=True)\n    return padded_conv_out\n\n", "CodeBleu_score": 16.747401120587647}]}
{"instance_id": "db2644f3c681", "generation_result": [{"code": "def run_unsorted_inputs(self, fn, context, lens)\n\t\n    lengths, indices = torch.sort(lens, descending=True)\n    context = context[indices]\n    context = fn(context)\n    context = context[torch.argsort(indices)]\n    return context\n\n", "CodeBleu_score": 8.478558879796982}, {"code": "def run_unsorted_inputs(self, fn, context, lens)\n\t\n    lens, indices = torch.sort(lens, descending=True)\n    context = context[indices]\n    context = fn(context)\n    context = context[torch.argsort(indices)]\n    return context\n\n", "CodeBleu_score": 8.394367229609703}, {"code": "def run_unsorted_inputs(self, fn, context, lens)\n\t\n    lens, indices = torch.sort(lens, 0, descending=True)\n    context = context[indices]\n    context = fn(context)\n    context = context[torch.argsort(indices)]\n    return context\n\n", "CodeBleu_score": 8.010611083677013}]}
{"instance_id": "ced082bffc63", "generation_result": [{"code": "def remove_weight_norm(self)\n\t\n    self.kernel_predictor.remove_weight_norm()\n    parametrize.remove_parametrizations(self.convt_pre[1], \"weight\")\n    for block in self.conv_blocks:\n        parametrize.remove_parametrizations(block[1], \"weight\")\n", "CodeBleu_score": 70.15353357610886}, {"code": "def remove_weight_norm(self)\n\t\n    self.kernel_predictor.remove_weight_norm()\n    parametrize.remove_parametrizations(self.convt_pre[1], \"weight\")\n    for block in self.conv_blocks:\n        parametrize.remove_parametrizations(block[1], \"weight\")\n\n", "CodeBleu_score": 70.15353357610886}, {"code": "def remove_weight_norm(self)\n\t\n    self.kernel_predictor.remove_weight_norm()\n    parametrize.remove_parametrizations(self.convt_pre[1], \"weight\")\n    for block in self.conv_blocks:\n        parametrize.remove_parametrizations(block[1], \"weight\")\n", "CodeBleu_score": 70.15353357610886}]}
{"instance_id": "b5a39fd97061", "generation_result": [{"code": "def map_mx_to_torch_model(mx_name)\n\t   mx_name = mx_name.lower()\n    if mx_name.startswith(\"se_\"):\n        mx_name = \"gluon_se_\" + mx_name[3:]\n    elif mx_name.startswith(\"senet_\"):\n        mx_name = \"gluon_senet_\" + mx_name[6:]\n    elif mx_name == \"inceptionv3\":\n        mx_name = \"gluon_inceptionv3\"\n    return \"gluon_\" + mx_name\n\n", "CodeBleu_score": 32.53553978855405}, {"code": "def map_mx_to_torch_model(mx_name)\n\t   mx_name = mx_name.lower()\n    if mx_name.startswith('se_'):\n        mx_name = 'gluon_se_' + mx_name[3:]\n    elif mx_name.startswith('senet_'):\n        mx_name = 'gluon_se_' + mx_name[6:]\n    elif mx_name == 'inceptionv3':\n        mx_name = 'gluon_inceptionv3'\n    return 'gluon_' + mx_name\n\n", "CodeBleu_score": 33.1359736779814}, {"code": "def map_mx_to_torch_model(mx_name)\n\t   mx_name = mx_name.lower()\n    if mx_name.startswith(\"se_\"):\n        mx_name = mx_name.replace(\"se_\", \"gluon_se_\")\n    elif mx_name.startswith(\"senet_\"):\n        mx_name = mx_name.replace(\"senet_\", \"gluon_se_\")\n    elif mx_name.startswith(\"inceptionv3\"):\n        mx_name = \"gluon_inceptionv3\"\n    else:\n        mx_name = \"gluon_\" + mx_name\n    return mx_name\n\n", "CodeBleu_score": 43.3694613937988}]}
{"instance_id": "7eee5efdbf67", "generation_result": [{"code": "def _num_samples_per_worker(self)\n\t\n    # Compute the initial number of samples per worker based on the repeats, total samples, and the number of workers or replicas.\n    samples_per_worker = self.total_samples * self.repeats // self.num_workers\n    if self.training or self.num_replicas > 1:\n        # Round up the number of samples if training or multiple replicas are used.\n        samples_per_worker = -(-samples_per_worker // self.num_replicas)\n    if self.batch_size is not None:\n        # Adjust the number of samples to be a multiple of the batch size if a batch size is specified during training.\n        samples_per_worker = (samples_per_worker // self.batch_size) * self.batch_size\n    return samples_per_worker\n\n", "CodeBleu_score": 36.32478679910225}, {"code": "def _num_samples_per_worker(self)\n\t\n    if self._num_samples_per_worker is None:\n        if self._num_replicas > 1:\n            self._num_samples_per_worker = int(\n                math.ceil(self._num_samples / self._num_replicas)\n            )\n        else:\n            self._num_samples_per_worker = self._num_samples\n        if self._batch_size is not None:\n            self._num_samples_per_worker = int(\n                math.ceil(self._num_samples_per_worker / self._batch_size)\n            ) * self._batch_size\n    return self._num_samples_per_worker\n\n", "CodeBleu_score": 32.58224762730125}, {"code": "def _num_samples_per_worker(self)\n\t\n    # Calculate the initial number of samples per worker\n    if self.num_workers == 1:\n        samples_per_worker = self.num_samples\n    else:\n        samples_per_worker = self.num_samples // self.num_workers\n        if self.num_samples % self.num_workers > 0:\n            samples_per_worker += 1\n    # Adjust the number of samples per worker if training\n    if self.training:\n        if self.num_replicas > 1:\n            samples_per_worker = int(math.ceil(samples_per_worker * self.num_replicas))\n        if self.batch_size is not None:\n            samples_per_worker = int(math.ceil(samples_per_worker / self.batch_size)) * self.batch_size\n    return int(samples_per_worker)\n\n", "CodeBleu_score": 40.43192942591022}]}
{"instance_id": "a698d65ae190", "generation_result": [{"code": "def find_images_and_targets(\n        folder: str,\n        types: Optional[Union[List, Tuple, Set]] = None,\n        class_to_idx: Optional[Dict] = None,\n        leaf_name_only: bool = True,\n        sort: bool = True\n)\n\t\n    if types is None:\n        types = ['jpg', 'jpeg', 'png', 'bmp', 'tif', 'tiff']\n    if not isinstance(types, (list, tuple, set)):\n        types = [types]\n\n    if class_to_idx is None:\n        class_to_idx = {}\n\n    # find all images\n    all_images = []\n    for path, subdirs, files in os.walk(folder):\n        for name in files:\n            if os.path.splitext(name)[-1][1:].lower() in types:\n                all_images += [os.path.join(path, name)]\n\n    # find 'natural' order for images\n    if sort:\n        all_images = sorted(all_images, key=natural_key)\n\n    # find all targets\n    all_targets = []\n    for image_path in all_images:\n        target = os.path.basename(os.path.dirname(image_path))\n        if leaf_name_only:\n            target = os.path.basename(target)\n        if target not in class_to_idx:\n            class_to_idx[target] = len(class_to_idx)\n        all_targets += [class_to_idx[target]]\n\n    # create dataset\n    dataset = list(zip(all_images, all_targets))\n\n    return dataset, class_to_idx\n\n", "CodeBleu_score": 38.96762843865499}, {"code": "def find_images_and_targets(\n        folder: str,\n        types: Optional[Union[List, Tuple, Set]] = None,\n        class_to_idx: Optional[Dict] = None,\n        leaf_name_only: bool = True,\n        sort: bool = True\n)\n\t\n    if types is None:\n        types = ['.jpg', '.jpeg', '.png', '.ppm', '.bmp', '.pgm', '.tif']\n    if not isinstance(types, (list, tuple, set)):\n        raise TypeError(\"types must be a list, tuple, or set\")\n    if class_to_idx is None:\n        class_to_idx = {}\n    if not isinstance(class_to_idx, dict):\n        raise TypeError(\"class_to_idx must be a dict\")\n    if not isinstance(leaf_name_only, bool):\n        raise TypeError(\"leaf_name_only must be a bool\")\n    if not isinstance(sort, bool):\n        raise TypeError(\"sort must be a bool\")\n\n    # Find all images from the folder\n    images = []\n    for (dirpath, dirnames, filenames) in os.walk(folder):\n        for filename in filenames:\n            if not filename.lower().endswith(tuple(types)):\n                continue\n            if leaf_name_only:\n                image_path = os.path.join(dirpath, filename)\n            else:\n                image_path = os.path.join(dirpath, filename)\n            images.append(image_path)\n    if len(images) == 0:\n        raise FileNotFoundError(\"No images found in the folder\")\n\n    # Find all labels from the folder\n    labels = []\n    for (dirpath, dirnames, filenames) in os.walk(folder):\n        for filename in filenames:\n            if not filename.lower().endswith(tuple(types)):\n                continue\n            label = dirpath.replace(folder, '').lstrip(os.path.sep)\n            labels.append(label)\n    if len(labels) == 0:\n        raise FileNotFoundError(\"No labels found in the folder\")\n\n    # Create class_to_idx mapping if not provided\n    if len(class_to_idx) == 0:\n        class_names = sorted(set(labels), key=natural_key)\n        class_to_idx = {class_name: i for i, class_name in enumerate(class_names)}\n\n    # Pair each image with its label index\n    targets = [class_to_idx[label] for label in labels]\n\n    # Sort if needed\n    if sort:\n        images, targets = zip(*sorted(zip(images, targets)))\n\n    return list(images), list(targets), class_to_idx", "CodeBleu_score": 37.415305839092625}, {"code": "def find_images_and_targets(\n        folder: str,\n        types: Optional[Union[List, Tuple, Set]] = None,\n        class_to_idx: Optional[Dict] = None,\n        leaf_name_only: bool = True,\n        sort: bool = True\n)\n\t\n    # if types is None:\n    #     types = ['.jpg', '.jpeg', '.png', '.ppm', '.bmp', '.pgm', '.tif']\n    if types is None:\n        types = ['.jpg', '.jpeg', '.png']\n\n    if not isinstance(types, (list, tuple, set)):\n        raise TypeError('types must be a list, tuple or set')\n\n    if not isinstance(folder, str):\n        raise TypeError('folder must be a string')\n\n    if not os.path.isdir(folder):\n        raise ValueError('folder must be an existing directory')\n\n    if class_to_idx is None:\n        class_to_idx = {}\n\n    images = []\n    for path, subdirs, files in os.walk(folder):\n        for name in files:\n            if not name.lower().endswith(tuple(types)):\n                continue\n\n            if leaf_name_only:\n                label = os.path.basename(path)\n            else:\n                label = path\n\n            if label not in class_to_idx:\n                class_to_idx[label] = len(class_to_idx)\n\n            images.append(os.path.join(path, name))\n\n    if sort:\n        images.sort(key=natural_key)\n\n    return images, [class_to_idx[os.path.basename(os.path.dirname(i))] for i in images], class_to_idx\n\n", "CodeBleu_score": 35.61958725900689}]}
{"instance_id": "e7fe43bdecdc", "generation_result": [{"code": "def pytorch_worker_seed()\n\t\n    worker_info = get_worker_info()\n    if worker_info is not None:\n        # favour the seed already created for pytorch dataloader workers if it exists\n        return worker_info.seed\n    # fallback to wds rank based seed\n    return wds.utils.pytorch_worker_seed()\n\n", "CodeBleu_score": 66.80150511570375}, {"code": "def pytorch_worker_seed()\n\t\n    worker_info = get_worker_info()\n    if worker_info is not None:\n        # favour the seed already created for pytorch dataloader workers if it exists\n        return worker_info.seed\n    # fallback to wds rank based seed\n    return wds.utils.pytorch_worker_seed()\n\n", "CodeBleu_score": 66.80150511570375}, {"code": "def pytorch_worker_seed()\n\t\n    worker_info = get_worker_info()\n    if worker_info is not None:\n        # favour the seed already created for pytorch dataloader workers if it exists\n        return worker_info.seed\n    # fallback to wds rank based seed\n    return wds.utils.pytorch_worker_seed()\n\n", "CodeBleu_score": 66.80150511570375}]}
{"instance_id": "0ffc7e7ded25", "generation_result": [{"code": "def solarize_add(img, add, thresh=128, **__)\n\t\n    if img.mode == \"L\":\n        return solarize_add_lut(img, add, thresh)\n    elif img.mode == \"RGB\":\n        return solarize_add_lut(img, add, thresh).convert(\"RGB\")\n    else:\n        return img\n\n", "CodeBleu_score": 20.719926478291107}, {"code": "def solarize_add(img, add, thresh=128, **__)\n\t\n    if img.mode not in (\"L\", \"RGB\"):\n        return img\n    if thresh < 0 or thresh > 255:\n        return img\n    if add < 0 or add > 255:\n        return img\n    if img.mode == \"L\":\n        return img.point(lambda x: x + add if x < thresh else x)\n    else:\n        return img.convert(\"RGB\").point(lambda x: x + add if x < thresh else x).convert(img.mode)\n\n", "CodeBleu_score": 31.844632434149712}, {"code": "def solarize_add(img, add, thresh=128, **__)\n\t\n    if img.mode in (\"L\", \"RGB\"):\n        if img.mode == \"L\":\n            return img.point(lambda i: i + add if i < thresh else i)\n        return img.point(lambda r, g, b: (r + add if r < thresh else r, g + add if g < thresh else g, b + add if b < thresh else b))\n    return img\n\n", "CodeBleu_score": 37.62124112454924}]}
{"instance_id": "5a0d29577a21", "generation_result": [{"code": "def _get_weighted_transforms(transforms: Dict)\n\t\n    transforms_list = list(transforms.keys())\n    weights = list(transforms.values())\n    weights = [float(i) / sum(weights) for i in weights]\n    return transforms_list, weights\n\n", "CodeBleu_score": 20.098356032330784}, {"code": "def _get_weighted_transforms(transforms: Dict)\n\t\n    transforms_names = list(transforms.keys())\n    transforms_probs = list(transforms.values())\n    transforms_probs = [prob / sum(transforms_probs) for prob in transforms_probs]\n    return transforms_names, transforms_probs\n\n", "CodeBleu_score": 20.625674433006047}, {"code": "def _get_weighted_transforms(transforms: Dict)\n\t\n    transforms_list = list(transforms.keys())\n    probs_list = list(transforms.values())\n    probs_list = [float(i) / sum(probs_list) for i in probs_list]\n    return transforms_list, probs_list\n\n", "CodeBleu_score": 22.045753647412734}]}
{"instance_id": "84bc5658c135", "generation_result": [{"code": "def _apply_basic(self, img, mixing_weights, m)\n\t\n    mixed_img = np.zeros(img.shape, dtype=img.dtype)\n    for w in mixing_weights:\n        if m.p_affine > 0:\n            img = _apply_affine(self, img, m)\n        if m.p_flip > 0:\n            img = _apply_flip(self, img, m)\n        if m.p_crop > 0:\n            img = _apply_crop(self, img, m)\n        if m.p_brightness > 0:\n            img = _apply_brightness(self, img, m)\n        if m.p_contrast > 0:\n            img = _apply_contrast(self, img, m)\n        if m.p_blur > 0:\n            img = _apply_blur(self, img, m)\n        if m.p_noise > 0:\n            img = _apply_noise(self, img, m)\n        mixed_img += img * w\n    mixed_img = np.clip(mixed_img, 0, 255).astype(img.dtype)\n    return Image.fromarray(mixed_img), mixed_img\n\n", "CodeBleu_score": 27.41296026626757}, {"code": "def _apply_basic(self, img, mixing_weights, m)\n\t\n    mixed_img = np.zeros(img.shape, dtype=img.dtype)\n    for weight in mixing_weights:\n        seq = self.to_deterministic()\n        seq_img = seq.augment_image(img)\n        mixed_img = np.add(mixed_img, seq_img, dtype=np.float32)\n    mixed_img = np.clip(mixed_img, 0, 255)\n    mixed_img = mixed_img.astype(img.dtype)\n    return (1 - m) * img + m * mixed_img\n\n", "CodeBleu_score": 25.270358153772932}, {"code": "def _apply_basic(self, img, mixing_weights, m)\n\t\n    mixed_img = np.zeros(img.shape, dtype=np.float32)\n    for w in mixing_weights:\n        # apply random sequence of operations to the image\n        img = _apply_random_sequence(self, img)\n        # accumulate the results\n        mixed_img += w * img\n    # clip to valid pixel values\n    mixed_img = np.clip(mixed_img, 0, 255)\n    # convert back to an image\n    mixed_img = mixed_img.astype(np.uint8)\n    # blend with the original image\n    mixed_img = cv2.addWeighted(mixed_img, m, img, 1 - m, 0)\n    return mixed_img\n\n", "CodeBleu_score": 21.7209831783377}]}
{"instance_id": "84b5b752ff04", "generation_result": [{"code": "def adapt_to_chs(x, n)\n\t\n    if not isinstance(x, (tuple, list)):\n        x = (x,) * n\n    elif len(x)!= n:\n        x = (np.mean(x),) * n\n        print(f\"The length of x ({len(x)}) is not equal to n ({n}). The mean of x ({np.mean(x)}) is repeated {n} times.\")\n    assert len(x) == n\n    return x\n\n", "CodeBleu_score": 40.37361603574787}, {"code": "def adapt_to_chs(x, n)\n\t\n    if not isinstance(x, (tuple, list)):\n        x = (x,) * n\n    elif len(x)!= n:\n        x = tuple(sum(x) / len(x) for _ in range(n))\n        print(f\"Warning: x was a tuple or list but its length is not equal to n. The mean of x was calculated and repeated n times.\")\n    assert len(x) == n, f\"The length of x is not equal to n.\"\n    return x\n\n", "CodeBleu_score": 43.599053353428644}, {"code": "def adapt_to_chs(x, n)\n\t\n    if not isinstance(x, (tuple, list)):\n        x = (x,) * n\n    elif len(x)!= n:\n        logger.warning(\n            f\"The length of x is {len(x)} but n is {n}. Replacing x with a tuple of length n.\"\n        )\n        x = (sum(x) / len(x),) * n\n    assert len(x) == n\n    return x\n\n", "CodeBleu_score": 40.96446891134324}]}
{"instance_id": "54754b6ba394", "generation_result": [{"code": "def _worker_init(worker_id, worker_seeding='all')\n\t\n    worker_info = torch.utils.data.get_worker_info()\n    if worker_info is None:\n        raise ValueError(\"A worker seeding has been provided but the DataLoader is not being run in a \"\n                         \"multiprocessing setting. Make sure to set the num_workers argument of \"\n                         \"DataLoader to a non-zero value.\")\n\n    if callable(worker_seeding):\n        # Generate a seed based on the worker id and the provided seeding callable\n        seed = worker_seeding(worker_id, worker_info.seed)\n        np.random.seed(seed)\n        random.seed(seed)\n        torch.manual_seed(seed)\n    elif isinstance(worker_seeding, str):\n        # Set the seed based on the worker info seed\n        if worker_seeding == 'all':\n            np.random.seed(worker_info.seed)\n        else:\n            raise ValueError(f\"worker_seeding must be 'all' or a callable, got {worker_seeding}.\")\n    else:\n        raise ValueError(f\"worker_seeding must be 'all' or a callable, got {worker_seeding}.\")\n\n", "CodeBleu_score": 39.98145302859596}, {"code": "def _worker_init(worker_id, worker_seeding='all')\n\t\n    worker_info = get_worker_info()\n    if worker_info is None:\n        return\n    assert worker_info.id == worker_id, f'Worker ID {worker_info.id} does not match the expected worker ID {worker_id}.'\n    if callable(worker_seeding):\n        worker_seeding(worker_info)\n    elif worker_seeding == 'all':\n        np.random.seed(worker_info.seed)\n    else:\n        raise ValueError(f'worker_seeding must be a callable or \"all\", but got {worker_seeding}.')\n\n", "CodeBleu_score": 31.40262352245504}, {"code": "def _worker_init(worker_id, worker_seeding='all')\n\t\n    worker_info = torch.utils.data.get_worker_info()\n    if worker_info is None:\n        raise ValueError('worker_init_fn cannot be used in the absence of a worker_info')\n    if worker_id is None:\n        worker_id = worker_info.id\n    if not isinstance(worker_id, int):\n        raise ValueError('worker_id must be an integer')\n    if not isinstance(worker_seeding, (int, str, Callable)):\n        raise ValueError('worker_seeding must be an integer, string, or callable')\n    if isinstance(worker_seeding, int):\n        torch.manual_seed(worker_seeding)\n        random.seed(worker_seeding)\n        np.random.seed(worker_seeding)\n    elif isinstance(worker_seeding, str):\n        if worker_seeding == 'all':\n            torch.manual_seed(worker_info.seed)\n            random.seed(worker_info.seed)\n            np.random.seed(worker_info.seed)\n        else:\n            raise ValueError('worker_seeding must be \"all\"')\n    elif callable(worker_seeding):\n        worker_seeding(worker_id)\n    else:\n        raise ValueError('worker_seeding must be an integer, string, or callable')\n\n", "CodeBleu_score": 35.68158030451771}]}
{"instance_id": "81dfdce940db", "generation_result": [{"code": "def _decode_and_center_crop(image_bytes, image_size, resize_method)\n\t\n    # Decode image\n    image = tf.io.decode_jpeg(image_bytes, channels=3)\n    # Calculate padded center crop size\n    shape = tf.shape(image)\n    height, width = shape[0], shape[1]\n    padded_center_crop_size = tf.cast(\n        tf.cast(tf.minimum(height, width), tf.float32) / (224.0 / 225.0), tf.int32\n    )\n    offset_height = ((height - padded_center_crop_size) + 1) // 2\n    offset_width = ((width - padded_center_crop_size) + 1) // 2\n    # Crop image\n    image = tf.image.crop_to_bounding_box(\n        image, offset_height, offset_width, padded_center_crop_size, padded_center_crop_size\n    )\n    # Resize image\n    image = tf.image.resize(image, (image_size, image_size), method=resize_method)\n    return image\n\n", "CodeBleu_score": 36.29116025472847}, {"code": "def _decode_and_center_crop(image_bytes, image_size, resize_method)\n\t\n    image_shape = tf.image.extract_jpeg_shape(image_bytes)\n    image_height = image_shape[0]\n    image_width = image_shape[1]\n    padded_center_crop_size = tf.cast(\n        (\n            (image_size / resize_method) * ((image_height + image_width) / 2),\n        ),\n        tf.int32,\n    )\n    offset_height = ((image_height - padded_center_crop_size) + 1) // 2\n    offset_width = ((image_width - padded_center_crop_size) + 1) // 2\n    crop_window = tf.stack([offset_height, offset_width, padded_center_crop_size, padded_center_crop_size])\n    image = tf.image.decode_and_crop_jpeg(image_bytes, crop_window, channels=3)\n    return tf.image.resize(image, [image_size, image_size], method=resize_method)\n\n", "CodeBleu_score": 45.80050756157416}, {"code": "def _decode_and_center_crop(image_bytes, image_size, resize_method)\n\t\n    image = tf.image.decode_jpeg(image_bytes, channels=3)\n    image = tf.image.convert_image_dtype(image, tf.float32)\n    image_shape = tf.shape(image)\n    image_height = image_shape[0]\n    image_width = image_shape[1]\n    padded_center_crop_size = tf.cast(\n        tf.reduce_min([image_size * (1 + _PADDING_CONSTANT), image_height, image_width]), tf.int32)\n    offset_height = ((image_height - padded_center_crop_size) + 1) // 2\n    offset_width = ((image_width - padded_center_crop_size) + 1) // 2\n    image = tf.image.crop_to_bounding_box(image, offset_height, offset_width, padded_center_crop_size,\n                                          padded_center_crop_size)\n    image = tf.image.resize(image, [image_size, image_size], method=resize_method)\n    return image\n\n", "CodeBleu_score": 39.63407722814402}]}
{"instance_id": "cf16a6d37111", "generation_result": [{"code": "def preprocess_for_train(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    image = _decode_and_random_crop(image_bytes, image_size, interpolation)\n    image = _flip(image)\n    image.set_shape([image_size, image_size, 3])\n    image = tf.cast(image, dtype=tf.bfloat16 if use_bfloat16 else tf.float32)\n    return image\n\n", "CodeBleu_score": 34.75415654446066}, {"code": "def preprocess_for_train(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    image = _decode_and_random_crop(image_bytes, image_size, interpolation)\n    image = _flip(image)\n    image.set_shape([image_size, image_size, 3])\n    image = tf.cast(image, dtype=tf.float32)\n    if use_bfloat16:\n        image = tf.cast(image, dtype=tf.bfloat16)\n    return image\n", "CodeBleu_score": 34.163510281556256}, {"code": "def preprocess_for_train(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    image = _decode_and_random_crop(image_bytes, image_size, interpolation)\n    image = _flip(image)\n    image = tf.reshape(image, [image_size, image_size, 3])\n    image = tf.cast(image, dtype=tf.bfloat16 if use_bfloat16 else tf.float32)\n    return image\n\n", "CodeBleu_score": 40.92077124969757}]}
{"instance_id": "78b2a3bd2142", "generation_result": [{"code": "def preprocess_for_eval(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    if use_bfloat16:\n        image = _decode_and_center_crop(image_bytes, image_size,\n                                        tf.image.ResizeMethod.BILINEAR)\n        image = tf.image.convert_image_dtype(image, dtype=tf.bfloat16)\n    else:\n        image = _decode_and_center_crop(image_bytes, image_size,\n                                        tf.image.ResizeMethod.BILINEAR)\n        image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n    return image\n\n", "CodeBleu_score": 41.94345106198105}, {"code": "def preprocess_for_eval(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    image = _decode_and_center_crop(image_bytes, image_size, resize_method=interpolation)\n    if use_bfloat16:\n        image = tf.cast(image, tf.bfloat16)\n    else:\n        image = tf.cast(image, tf.float32)\n    return image\n\n", "CodeBleu_score": 30.00443425686969}, {"code": "def preprocess_for_eval(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    image = _decode_and_center_crop(image_bytes, image_size, resize_method=interpolation)\n    image = tf.reshape(image, [image_size, image_size, 3])\n    if use_bfloat16:\n        image = tf.cast(image, tf.bfloat16)\n    else:\n        image = tf.cast(image, tf.float32)\n    return image\n\n", "CodeBleu_score": 38.502519510392695}]}
{"instance_id": "8bd4364a9a0e", "generation_result": [{"code": "def evaluate(self, model, eval_dataloader=None, output_file=\"merged\")\n\t\n    if eval_dataloader is None:\n        eval_dataloader = self.eval_dataloader\n\n    # Initialize the prediction and label lists\n    predictions, labels = [], []\n\n    # Put the model in evaluation mode\n    model.eval()\n\n    # For each batch in the evaluation dataloader\n    for batch in eval_dataloader:\n        # Move the batch to the device\n        batch = tuple(t.to(self.device) for t in batch)\n\n        # Unpack the inputs from the dataloader\n        b_input_ids, b_input_mask, b_labels = batch\n\n        # Telling the model not to compute or store gradients, saving memory and speeding up prediction\n        with torch.no_grad():\n            # Forward pass, calculate logit predictions\n            logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n\n        # Move logits and labels to CPU\n        logits = logits.detach().cpu().numpy()\n        label_ids = b_labels.to(\"cpu\").numpy()\n\n        # Store predictions and true labels\n        predictions.append(logits)\n        labels.append(label_ids)\n\n    # Flatten the predictions and true values for evaluation\n    predictions = np.concatenate(predictions, axis=0)\n    true_labels = np.concatenate(labels, axis=0)\n\n    # Return the evaluation results\n    return self.compute_metrics(predictions, true_labels)\n\n", "CodeBleu_score": 38.13743526849302}, {"code": "def evaluate(self, model, eval_dataloader=None, output_file=\"merged\")\n\t\n    if eval_dataloader is None:\n        eval_dataloader = self.eval_dataloader\n    if self.model_type == \"classifier\":\n        self.model.eval()\n        preds = []\n        for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n            batch = tuple(t.to(self.device) for t in batch)\n            input_ids, input_mask, segment_ids, label_ids = batch\n            with torch.no_grad():\n                logits = self.model(input_ids, segment_ids, input_mask, labels=None)\n            preds.append(logits.detach().cpu().numpy())\n        preds = np.concatenate(preds, axis=0)\n        preds = np.argmax(preds, axis=1)\n        labels = eval_dataloader.dataset.tensors[1].numpy()\n        result = compute_metrics(preds, labels)\n        return result\n    elif self.model_type == \"tagger\":\n        self.model.eval()\n        preds = []\n        for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n            batch = tuple(t.to(self.device) for t in batch)\n            input_ids, input_mask, segment_ids, label_ids = batch\n            with torch.no_grad():\n                logits = self.model(input_ids, segment_ids, input_mask, labels=None)\n            preds.append(logits.detach().cpu().numpy())\n        preds = np.concatenate(preds, axis=0)\n        preds = np.argmax(preds, axis=2)\n        labels = eval_dataloader.dataset.tensors[1].numpy()\n        result = compute_metrics(preds, labels)\n        return result\n    elif self.model_type == \"ner\":\n        self.model.eval()\n        preds = []\n        for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n            batch = tuple(t.to(self.device) for t in batch)\n            input_ids, input_mask, segment_ids, label_ids = batch\n            with torch.no_grad():\n                logits = self.model(input_ids, segment_ids, input_mask, labels=None)\n            preds.append(logits.detach().cpu().numpy())\n        preds = np.concatenate(preds, axis=0)\n        preds = np.argmax(preds, axis=2)\n        labels = eval_dataloader", "CodeBleu_score": 39.06632090828379}, {"code": "def evaluate(self, model, eval_dataloader=None, output_file=\"merged\")\n\t\n    if eval_dataloader is None:\n        eval_dataloader = self.eval_dataloader\n    if self.task_type == \"regression\":\n        return self.regression_eval(model, eval_dataloader, output_file)\n    elif self.task_type == \"classification\":\n        return self.classification_eval(model, eval_dataloader, output_file)\n    else:\n        raise ValueError(f\"Task type {self.task_type} not supported\")\n\n", "CodeBleu_score": 42.69941738169487}]}
{"instance_id": "22d682c958c0", "generation_result": [{"code": " def print_computed_metrics(self, metrics)\n\t\n    print(f\"Rank1: {metrics['rank1']:.4f}\")\n    print(f\"Rank5: {metrics['rank5']:.4f}\")\n    print(f\"mIoU: {metrics['miou']:.4f}\")\n\n\n def print_elapsed_time(self, start_time, end_time):\n    \"\"\"The function prints the elapsed time between the start and end times in hours, minutes, and seconds.\"\"\"\n    elapsed_time = end_time - start_time\n    hours, remainder = divmod(elapsed_time.seconds, 3600)\n    minutes, seconds = divmod(remainder, 60)\n    print(f\"Elapsed time: {hours:02d}:{minutes:02d}:{seconds:02d}\")\n\n\n def print_metrics(self, metrics):\n    \"\"\"The function prints the computed metrics for rank1, rank5, and miou.", "CodeBleu_score": 25.52664435885384}, {"code": " def print_computed_metrics(self, metrics)\n\t\n    print(f\"rank1: {metrics['rank1'] * 100:.4f}\")\n    print(f\"rank5: {metrics['rank5'] * 100:.4f}\")\n    print(f\"miou: {metrics['miou'] * 100:.4f}\")\n", "CodeBleu_score": 20.102852267145305}, {"code": " def print_computed_metrics(self, metrics)\n\t\n    print(\"Rank1: {:.4f}\".format(metrics[\"rank1\"]))\n    print(\"Rank5: {:.4f}\".format(metrics[\"rank5\"]))\n    print(\"mIoU: {:.4f}\".format(metrics[\"miou\"]))\n\n", "CodeBleu_score": 25.437147343981938}]}
{"instance_id": "39c1a50b7e7d", "generation_result": [{"code": "def _eval_predictions(self, segments, data)\n\t\n    ious = []\n    ranks = []\n    for segment, gt in zip(segments, data):\n        ious.append(self._iou(segment, gt))\n        ranks.append(self._rank(segment, gt))\n    rank1 = sum(1 for rank in ranks if rank == 1)/len(ranks)\n    rank5 = sum(1 for rank in ranks if rank <= 5)/len(ranks)\n    mean_iou = sum(ious)/len(ious)\n    return rank1, rank5, mean_iou\n", "CodeBleu_score": 23.637740868194935}, {"code": "def _eval_predictions(self, segments, data)\n\t\n    rank1 = 0\n    rank5 = 0\n    iou = 0\n    for i in range(len(segments)):\n        segment = segments[i]\n        segment = segment.split()\n        segment = [int(segment[i]) for i in range(len(segment))]\n        segment = tuple(segment)\n        gt = data[i]\n        gt = gt.split()\n        gt = [int(gt[i]) for i in range(len(gt))]\n        gt = tuple(gt)\n        iou += self._iou(segment, gt)\n        rank = self._rank(segment, gt)\n        if rank == 1:\n            rank1 += 1\n        if rank <= 5:\n            rank5 += 1\n    rank1 = rank1/len(segments)\n    rank5 = rank5/len(segments)\n    iou = iou/len(segments)\n    return rank1, rank5, iou\n\n", "CodeBleu_score": 29.22538568754407}, {"code": "def _eval_predictions(self, segments, data)\n\t\n    ious = []\n    ranks = []\n    for segment, gt in zip(segments, data):\n        iou = self._iou(segment, gt)\n        rank = self._rank(segment, gt)\n        ious.append(iou)\n        ranks.append(rank)\n    rank1 = sum(1 for rank in ranks if rank == 1)/len(ranks)\n    rank5 = sum(1 for rank in ranks if rank <= 5)/len(ranks)\n    mean_iou = sum(ious)/len(ious)\n    return rank1, rank5, mean_iou\n\n", "CodeBleu_score": 26.729007719501702}]}
{"instance_id": "8ccf84f8d034", "generation_result": [{"code": "def predict_loop(self, model, eval_dataloader, output_file=None)\n\t\n    scores = []\n    model.eval()\n    model = self.to_ctx(model)\n    with torch.no_grad():\n        for batch in tqdm(eval_dataloader):\n            batch = self.to_ctx(batch)\n            outputs = model(batch)\n            scores.append(outputs)\n    scores = self.finalize(scores)\n    if output_file is not None:\n        with open(output_file, \"w\") as f:\n            f.write(scores)\n    return scores\n\n", "CodeBleu_score": 43.30508414215926}, {"code": "def predict_loop(self, model, eval_dataloader, output_file=None)\n\t\n    scores = []\n    model.eval()\n    model = self.to_ctx(model, ctx=0)\n    with torch.no_grad():\n        for batch in eval_dataloader:\n            batch = self.to_ctx(batch, ctx=0)\n            outputs = model(**batch)\n            scores.append(outputs)\n        scores = self.finalize(scores, output_file)\n    return scores\n\n", "CodeBleu_score": 47.887399061790916}, {"code": "def predict_loop(self, model, eval_dataloader, output_file=None)\n\t\n    scores = []\n    model.eval()\n    model = self.to_ctx(model, ctx=0)\n    for i, batch in enumerate(eval_dataloader):\n        batch = self.to_ctx(batch, ctx=0)\n        with torch.no_grad():\n            output = model(**batch)\n        output[\"input\"] = batch\n        scores.append(output)\n    if output_file is not None:\n        self.finalize(scores, output_file)\n    return scores\n\n", "CodeBleu_score": 47.6889608160114}]}
{"instance_id": "e68529ad727d", "generation_result": [{"code": "def _aggregate_scores(self, scores)\n\t\n    assert len(scores) == 2\n    scores = [torch.cat(score, dim=1) for score in scores]\n    self.full_scores = None\n    return torch.bmm(scores[0].unsqueeze(1), scores[1].unsqueeze(2)).squeeze(2).squeeze(1)\n\n", "CodeBleu_score": 34.69685582361301}, {"code": "def _aggregate_scores(self, scores)\n\t\n    assert len(scores) == 2\n    scores = [torch.cat(score, dim=1) for score in scores]\n    self.full_scores = None\n    return torch.bmm(scores[0].unsqueeze(1), scores[1].unsqueeze(2)).squeeze(2).squeeze(1)\n\n", "CodeBleu_score": 34.69685582361301}, {"code": "def _aggregate_scores(self, scores)\n\t\n    assert len(scores) == 2\n    for i in range(len(scores)):\n        scores[i] = torch.cat(scores[i], dim=1)\n    self.full_scores = None\n    return torch.matmul(scores[0], scores[1].transpose(1, 0))\n\n", "CodeBleu_score": 36.388564329516015}]}
{"instance_id": "e4e1861fc357", "generation_result": [{"code": "def finalize(self, Y_pred, Y_true, output_file=None)\n\t\n    # Concatenate predictions and true labels\n    Y_pred = np.concatenate(Y_pred, axis=0)\n    Y_true = np.concatenate(Y_true, axis=0)\n    # Check for prediction errors\n    Y_pred_max = np.argmax(Y_pred, axis=1)\n    Y_true_max = np.argmax(Y_true, axis=1)\n    Y_pred_max_wrong = Y_pred_max[Y_pred_max!= Y_true_max]\n    Y_true_max_wrong = Y_true_max[Y_pred_max!= Y_true_max]\n    if len(Y_pred_max_wrong) > 0:\n        print(\"Prediction errors:\")\n        for i in range(len(Y_pred_max_wrong)):\n            print(\"Predicted: %s, True: %s\" % (Y_pred_max_wrong[i], Y_true_max_wrong[i]))\n    # Save predictions and true labels\n    if output_file is not None:\n        with open(output_file, \"wb\") as f:\n            pickle.dump({\"Y_pred\": Y_pred, \"Y_true\": Y_true}, f)\n    return {\"Y_pred\": Y_pred, \"Y_true\": Y_true}\n\n", "CodeBleu_score": 42.95150682778214}, {"code": "def finalize(self, Y_pred, Y_true, output_file=None)\n\t\n    if self.is_multilabel:\n        Y_pred = np.array(Y_pred)\n        Y_true = np.array(Y_true)\n        if Y_pred.shape[0]!= Y_true.shape[0]:\n            raise ValueError(\"The number of predictions and true labels does not match.\")\n        if Y_pred.shape[1]!= Y_true.shape[1]:\n            raise ValueError(\"The number of predicted and true labels does not match.\")\n        if Y_pred.shape[2]!= Y_true.shape[2]:\n            raise ValueError(\"The number of predicted and true labels does not match.\")\n        Y_pred = Y_pred.reshape((Y_pred.shape[0], Y_pred.shape[1] * Y_pred.shape[2]))\n        Y_true = Y_true.reshape((Y_true.shape[0], Y_true.shape[1] * Y_true.shape[2]))\n    else:\n        Y_pred = np.array(Y_pred)\n        Y_true = np.array(Y_true)\n        if Y_pred.shape[0]!= Y_true.shape[0]:\n            raise ValueError(\"The number of predictions and true labels does not match.\")\n        if Y_pred.shape[1]!= Y_true.shape[1]:\n            raise ValueError(\"The number of predicted and true labels does not match.\")\n        Y_pred = Y_pred.reshape((Y_pred.shape[0], Y_pred.shape[1]))\n        Y_true = Y_true.reshape((Y_true.shape[0], Y_true.shape[1]))\n    if self.is_multilabel:\n        if self.is_binary:\n            Y_pred = (Y_pred > 0.5).astype(int)\n        else:\n            Y_pred = np.argmax(Y_pred, axis=1)\n    else:\n        Y_pred = np.argmax(Y_pred, axis=1)\n    if self.is_multilabel:\n        Y_true = np.argmax(Y_true, axis=1)\n    else:\n        Y_true = np.argmax(Y_true, axis=1)\n    if output_file is not None:\n        with open(output_file, \"wb\") as f:\n            pickle.dump", "CodeBleu_score": 32.46201188559916}, {"code": "def finalize(self, Y_pred, Y_true, output_file=None)\n\t\n    # check for prediction errors\n    Y_pred = np.concatenate(Y_pred, axis=0)\n    Y_true = np.concatenate(Y_true, axis=0)\n    if len(Y_pred)!= len(Y_true):\n        raise ValueError(\"The number of predictions does not match the number of true labels.\")\n    # print samples of prediction errors\n    if len(Y_pred)!= len(Y_true):\n        print(\"Samples of prediction errors:\")\n        for i in range(len(Y_pred)):\n            if Y_pred[i]!= Y_true[i]:\n                print(\"Predicted: {}, True: {}\".format(Y_pred[i], Y_true[i]))\n    # save predictions and true labels as a pickle file\n    if output_file:\n        with open(output_file, \"wb\") as f:\n            pickle.dump({\"Y_pred\": Y_pred, \"Y_true\": Y_true}, f)\n    # return predictions and true labels\n    return {\"Y_pred\": Y_pred, \"Y_true\": Y_true}\n\n", "CodeBleu_score": 39.191833897659514}]}
{"instance_id": "7df9ab9a4920", "generation_result": [{"code": "def reduce_metrics(logging_outputs) -> None\n\t\n    loss_sum = sum(log.get(\"loss\", 0) for log in logging_outputs)\n    sample_size = sum(log.get(\"sample_size\", 0) for log in logging_outputs)\n    metrics.log_scalar(\"loss\", loss_sum / sample_size, sample_size)\n\n", "CodeBleu_score": 62.06001936124095}, {"code": "def reduce_metrics(logging_outputs) -> None\n\t\n    for key in list(logging_outputs[0].keys()):\n        if key.startswith(\"loss\"):\n            loss_sum = sum(log.get(key, 0) for log in logging_outputs)\n            sample_size = sum(log.get(\"sample_size\", 0) for log in logging_outputs)\n            metrics.log_scalar(key, loss_sum / sample_size, sample_size)\n\n", "CodeBleu_score": 64.42507858816296}, {"code": "def reduce_metrics(logging_outputs) -> None\n\t\n    loss_sum = sum(log.get(\"loss\", 0) for log in logging_outputs)\n    sample_size = sum(log.get(\"sample_size\", 0) for log in logging_outputs)\n    metrics.log_scalar(\"loss\", loss_sum / sample_size, sample_size, round=3)\n\n", "CodeBleu_score": 62.429646714521894}]}
{"instance_id": "7f9c326a96f7", "generation_result": [{"code": "def _mm_on_the_fly(\n        self,\n        cmasks,\n        vmasks,\n        attention_mask\n    )\n\t\n    if attention_mask is None:\n        attention_mask = self._mm_attention_mask(cmasks, vmasks)\n    token_type_ids = torch.cat([torch.zeros_like(vmasks), torch.ones_like(cmasks)], dim=1)\n    return attention_mask, token_type_ids\n\n", "CodeBleu_score": 33.276029921609336}, {"code": "def _mm_on_the_fly(\n        self,\n        cmasks,\n        vmasks,\n        attention_mask\n    )\n\t\n    # mm_mask = torch.cat([cmasks[:, :1], vmasks, cmasks[:, 1:]], dim=1)\n    mm_mask = self._mm_attention_mask(cmasks, vmasks)\n    if attention_mask is not None:\n        mm_mask = attention_mask * mm_mask\n    # mm_token_type_ids = torch.cat([torch.zeros_like(cmasks), torch.ones_like(vmasks), torch.zeros_like(cmasks)], dim=1)\n    mm_token_type_ids = self._mm_token_type_ids(cmasks, vmasks)\n    return mm_mask, mm_token_type_ids\n\n", "CodeBleu_score": 11.113189050636803}, {"code": "def _mm_on_the_fly(\n        self,\n        cmasks,\n        vmasks,\n        attention_mask\n    )\n\t\n    if attention_mask is None:\n        attention_mask = self._mm_attention_mask(cmasks, vmasks)\n    token_type_ids = torch.cat(\n        [torch.zeros_like(vmasks), torch.ones_like(cmasks)], dim=1\n    )\n    return attention_mask, token_type_ids\n\n", "CodeBleu_score": 35.59766362655816}]}
{"instance_id": "fa99d2a148a5", "generation_result": [{"code": "def prepare_inputs_for_generation(\n        self,\n        input_ids,\n        input_video_embeds,\n        attention_mask=None,\n        token_type_ids=None,\n        **model_kwargs\n    )\n\t\n    # Prepare inputs for text generation\n    if attention_mask is None:\n        attention_mask = input_ids.ne(self.config.pad_token_id)\n    if token_type_ids is None:\n        token_type_ids = torch.zeros_like(input_ids)\n\n    # Prepare inputs for video generation\n    input_video_embeds = input_video_embeds.unsqueeze(0)\n    video_seq_len = input_video_embeds.shape[1]\n\n    # Adjust sequence length to match the combined length of input_ids and input_video_embeds\n    combined_seq_len = input_ids.shape[1] + video_seq_len\n    if combined_seq_len > self.config.max_position_embeddings:\n        input_ids = input_ids[:, : self.config.max_position_embeddings - video_seq_len]\n        attention_mask = attention_mask[:, : self.config.max_position_embeddings - video_seq_len]\n        token_type_ids = token_type_ids[:, : self.config.max_position_embeddings - video_seq_len]\n\n    # Pad input_video_embeds to match the combined sequence length\n    input_video_embeds = F.pad(input_video_embeds, (0, 0, 0, combined_seq_len - video_seq_len))\n\n    # Prepare inputs for text and video generation\n    input_ids = torch.cat([input_ids, torch.zeros(input_video_embeds.shape[1], dtype=torch.long, device=input_ids.device)], dim=0)\n    attention_mask = torch.cat([attention_mask, torch.zeros(input_video_embeds.shape[1], dtype=torch.bool, device=attention_mask.device)], dim=0)\n    token_type_ids = torch.cat([token_type_ids, torch.ones(input_video_embeds.shape[1], dtype=torch.long, device=token_type_ids.device)], dim=0)\n\n    # Prepare inputs for text and video generation\n    inputs = {\n        \"input_ids\": input_ids,\n        \"attention_mask\": attention_mask,\n        \"token_type_ids\": token_type_ids,\n        \"input_video_embeds", "CodeBleu_score": 34.04111112432605}, {"code": "def prepare_inputs_for_generation(\n        self,\n        input_ids,\n        input_video_embeds,\n        attention_mask=None,\n        token_type_ids=None,\n        **model_kwargs\n    )\n\t\n    # Prepare inputs for text generation models\n    if attention_mask is not None:\n        attention_mask = attention_mask.to(input_ids.device)\n    if token_type_ids is not None:\n        token_type_ids = token_type_ids.to(input_ids.device)\n    # Prepare inputs for video generation models\n    if input_video_embeds is not None:\n        input_video_embeds = input_video_embeds.to(input_ids.device)\n\n    # Adjust sequence length to match the combined length of input_ids and input_video_embeds\n    sequence_length = input_ids.size(1) + input_video_embeds.size(1)\n    if sequence_length > self.max_position_embeddings:\n        raise ValueError(\n            f\"The combined sequence length of input_ids and input_video_embeds exceeds the maximum sequence length of the model ({self.max_position_embeddings}).\"\n        )\n\n    # Modify attention_mask and token_type_ids to fit the new sequence length\n    if attention_mask is not None:\n        attention_mask = torch.cat(\n            [attention_mask, torch.ones(attention_mask.size(0), input_video_embeds.size(1)).to(attention_mask.device)],\n            dim=1\n        )\n    if token_type_ids is not None:\n        token_type_ids = torch.cat(\n            [token_type_ids, torch.ones(token_type_ids.size(0), input_video_embeds.size(1)).to(token_type_ids.device)],\n            dim=1\n        )\n\n    return {\n        \"input_ids\": input_ids,\n        \"input_video_embeds\": input_video_embeds,\n        \"attention_mask\": attention_mask,\n        \"token_type_ids\": token_type_ids,\n        **model_kwargs\n    }\n\n", "CodeBleu_score": 41.665304971591766}, {"code": "def prepare_inputs_for_generation(\n        self,\n        input_ids,\n        input_video_embeds,\n        attention_mask=None,\n        token_type_ids=None,\n        **model_kwargs\n    )\n\t\n    if attention_mask is None:\n        attention_mask = input_ids.new_ones(input_ids.shape)\n    if token_type_ids is None:\n        token_type_ids = input_ids.new_zeros(input_ids.shape)\n    # Concatenate input_ids and input_video_embeds\n    combined_input_ids = torch.cat([input_ids, input_video_embeds], dim=1)\n    combined_attention_mask = torch.cat([attention_mask, attention_mask.new_ones(input_video_embeds.shape)], dim=1)\n    combined_token_type_ids = torch.cat([token_type_ids, token_type_ids.new_zeros(input_video_embeds.shape)], dim=1)\n    # Adjust the sequence length to match the combined length\n    combined_input_ids = combined_input_ids[:, :self.config.max_position_embeddings]\n    combined_attention_mask = combined_attention_mask[:, :self.config.max_position_embeddings]\n    combined_token_type_ids = combined_token_type_ids[:, :self.config.max_position_embeddings]\n    # Return the updated input_ids, input_video_embeds, attention_mask, and token_type_ids\n    return {\n        \"input_ids\": combined_input_ids,\n        \"input_video_embeds\": input_video_embeds,\n        \"attention_mask\": combined_attention_mask,\n        \"token_type_ids\": combined_token_type_ids,\n        **model_kwargs,\n    }\n\n", "CodeBleu_score": 34.574173151240686}]}
{"instance_id": "b1b0d25fe620", "generation_result": [{"code": "def _init_pipeline(self, cfg: ConfigType) -> Compose\n\t    cfg = copy.deepcopy(cfg)\n    if 'img_id' in cfg.meta_keys:\n        cfg.meta_keys.remove('img_id')\n    load_img_idx = self._get_transform_idx(cfg, 'LoadImageFromFile')\n    if load_img_idx < 0:\n        raise ValueError(\n            'LoadImageFromFile transform not found in the pipeline.')\n    cfg[load_img_idx]['type'] ='mmdet.InferencerLoader'\n    return Compose(cfg)", "CodeBleu_score": 32.435418541587765}, {"code": "def _init_pipeline(self, cfg: ConfigType) -> Compose\n\t    if'meta_keys' in cfg[0]:\n        cfg[0]['meta_keys'].remove('img_id')\n    loader_idx = self._get_transform_idx(cfg, 'LoadImageFromFile')\n    if loader_idx == -1:\n        raise ValueError(\n            'LoadImageFromFile is not found in the pipeline, please check your config.'\n        )\n    cfg[loader_idx]['type'] ='mmdet.InferencerLoader'\n    return Compose(cfg)\n\n", "CodeBleu_score": 37.72699516661374}, {"code": "def _init_pipeline(self, cfg: ConfigType) -> Compose\n\t    if 'img_id' in cfg.meta_keys:\n        cfg.meta_keys.remove('img_id')\n    load_img_idx = self._get_transform_idx(cfg, 'LoadImageFromFile')\n    if load_img_idx == -1:\n        raise ValueError(\n            'LoadImageFromFile is not found in the test pipeline.')\n    cfg[load_img_idx]['type'] ='mmdet.InferencerLoader'\n    return Compose(cfg)\n\n", "CodeBleu_score": 39.69758432651808}]}
{"instance_id": "7967962856eb", "generation_result": [{"code": "def _inputs_to_list(self, inputs: InputsType) -> list\n\t\n    if isinstance(inputs, str):\n        inputs = list(Path(inputs).rglob(\"*.png\"))\n        inputs = [str(path) for path in inputs]\n    elif not isinstance(inputs, (list, tuple)):\n        inputs = [inputs]\n    return inputs\n\n", "CodeBleu_score": 38.04973321906883}, {"code": "def _inputs_to_list(self, inputs: InputsType) -> list\n\t\n    if isinstance(inputs, str):\n        if os.path.isdir(inputs):\n            inputs = [os.path.join(inputs, file) for file in os.listdir(inputs) if file.endswith(self.image_ext)]\n        else:\n            inputs = [inputs]\n    if not isinstance(inputs, (list, tuple)):\n        inputs = [inputs]\n    return inputs\n\n", "CodeBleu_score": 44.06992733174002}, {"code": "def _inputs_to_list(self, inputs: InputsType) -> list\n\t\n    if isinstance(inputs, str):\n        inputs = [os.path.join(inputs, file) for file in os.listdir(inputs) if file.endswith((\".jpg\", \".png\", \".jpeg\"))]\n    elif not isinstance(inputs, (list, tuple)):\n        inputs = [inputs]\n    return inputs\n\n", "CodeBleu_score": 38.72788339919753}]}
{"instance_id": "74f727f06fa0", "generation_result": [{"code": "def _get_chunk_data(self, inputs: Iterable, chunk_size: int)\n\t\n    chunk = []\n    for input in inputs:\n        if isinstance(input, dict) and ('img' in input or 'img_path' in input):\n            input = self.pipeline(input)\n        chunk.append(input)\n        if len(chunk) == chunk_size:\n            yield chunk\n            chunk = []\n    if chunk:\n        yield chunk\n\n", "CodeBleu_score": 23.59926429365173}, {"code": "def _get_chunk_data(self, inputs: Iterable, chunk_size: int)\n\t\n    inputs = list(inputs)\n    num_inputs = len(inputs)\n    chunk_size = min(chunk_size, num_inputs)\n    for i in range(0, num_inputs, chunk_size):\n        chunk = inputs[i:i + chunk_size]\n        chunk_data = []\n        for input in chunk:\n            if isinstance(input, dict):\n                if 'img' in input:\n                    input = self.pipeline(input)\n                elif 'img_path' in input:\n                    input = self.pipeline(input)\n            chunk_data.append(input)\n        yield chunk_data\n\n", "CodeBleu_score": 35.43279358854108}, {"code": "def _get_chunk_data(self, inputs: Iterable, chunk_size: int)\n\t\n    chunk = []\n    for input_data in inputs:\n        if isinstance(input_data, dict) and ('img' in input_data or 'img_path' in input_data):\n            input_data = self.pipeline(input_data)\n        chunk.append(input_data)\n        if len(chunk) == chunk_size:\n            yield chunk\n            chunk = []\n    if chunk:\n        yield chunk\n\n", "CodeBleu_score": 24.39247960504641}]}
{"instance_id": "a43ab51afa59", "generation_result": [{"code": "def postprocess(\n        self,\n        preds: PredType,\n        visualization: Optional[List[np.ndarray]] = None,\n        return_datasamples: bool = False,\n        print_result: bool = False,\n        no_save_pred: bool = False,\n        pred_out_dir: str = '',\n        **kwargs,\n    ) -> Dict\n\t    if not no_save_pred:\n        if pred_out_dir == '':\n            pred_out_dir = osp.join(self.work_dir, 'pred')\n            mmengine.mkdir_or_exist(pred_out_dir)\n        elif return_datasamples:\n            warnings.warn(\n                'Saving datasamples is not supported when '\n               'return_datasamples is True. The predictions will not be '\n               'saved.')\n    if return_datasamples:\n        results = []\n        for i, data_sample in enumerate(preds):\n            data_sample = self.pred2dict(data_sample, pred_out_dir)\n            results.append(data_sample)\n    else:\n        results = []\n        for i, data_sample in enumerate(preds):\n            data_sample = self.pred2dict(data_sample)\n            results.append(data_sample)\n    if visualization is not None:\n        results.extend(visualization)\n    if print_result:\n        print_log(results, logger='root')\n    return results", "CodeBleu_score": 46.36786633645281}, {"code": "def postprocess(\n        self,\n        preds: PredType,\n        visualization: Optional[List[np.ndarray]] = None,\n        return_datasamples: bool = False,\n        print_result: bool = False,\n        no_save_pred: bool = False,\n        pred_out_dir: str = '',\n        **kwargs,\n    ) -> Dict\n\t    if pred_out_dir!= '' and return_datasamples:\n        warnings.warn(\n            'Saving datasamples is not supported. Please set return_datasamples to False or set pred_out_dir to empty.'\n        )\n    if pred_out_dir == '' and not no_save_pred:\n        pred_out_dir = osp.join(self.work_dir, 'pred_results')\n        if not osp.exists(pred_out_dir):\n            os.makedirs(pred_out_dir)\n\n    if return_datasamples:\n        results = preds\n    else:\n        results = []\n        for pred in preds:\n            if isinstance(pred, dict):\n                result = self.pred2dict(pred, pred_out_dir)\n            else:\n                result = self.pred2dict(pred.data_sample, pred_out_dir)\n            results.append(result)\n\n    if visualization is not None:\n        results.append(visualization)\n\n    if print_result:\n        print_log(results, logger='root')\n\n    return results\n\n", "CodeBleu_score": 51.22783116844285}, {"code": "def postprocess(\n        self,\n        preds: PredType,\n        visualization: Optional[List[np.ndarray]] = None,\n        return_datasamples: bool = False,\n        print_result: bool = False,\n        no_save_pred: bool = False,\n        pred_out_dir: str = '',\n        **kwargs,\n    ) -> Dict\n\t    if not no_save_pred:\n        if pred_out_dir == '':\n            pred_out_dir = self.work_dir\n        mmengine.mkdir_or_exist(pred_out_dir)\n    else:\n        pred_out_dir = ''\n    if return_datasamples:\n        if pred_out_dir!= '':\n            warnings.warn('Saving datasamples is not supported, '\n                         'return_datasamples is set to False.')\n        return_datasamples = False\n    result = dict()\n    if return_datasamples:\n        result['datasamples'] = []\n    if visualization is not None:\n        result['visualization'] = visualization\n    if return_datasamples:\n        for data_sample in preds:\n            result['datasamples'].append(self.pred2dict(data_sample, pred_out_dir))\n    else:\n        for data_sample in preds:\n            result.update(self.pred2dict(data_sample, pred_out_dir))\n    if print_result:\n        print(result)\n    return result", "CodeBleu_score": 45.362996980552616}]}
{"instance_id": "a3c2561f199c", "generation_result": [{"code": "def build_test_pipeline(cfg: ConfigType) -> ConfigType\n\t\n    dataset_pipeline = cfg[\"dataset\"][\"pipeline\"]\n    test_pipeline = deepcopy(cfg[\"dataset\"][\"pipeline\"])\n    test_pipeline[0][\"transforms\"] = [transform for transform in dataset_pipeline[0][\"transforms\"] if transform[\"type\"] == \"Resize\"]\n    test_pipeline[-1] = deepcopy(dataset_pipeline[-1])\n    test_pipeline[-1][\"transforms\"].insert(0, test_pipeline[0][\"transforms\"][0])\n    test_pipeline[0][\"transforms\"] = []\n    test_pipeline = Compose(test_pipeline)\n    return test_pipeline\n\n", "CodeBleu_score": 27.25964817745702}, {"code": "def build_test_pipeline(cfg: ConfigType) -> ConfigType\n\t\n    test_pipeline = deepcopy(cfg.dataset.pipeline)\n    test_pipeline[0] = dict(type=\"Resize\", scale=cfg.data.test.img_scale)\n    test_pipeline.append(dict(type=\"Collect\", keys=[\"img\"], meta_keys=[]))\n    cfg.data.test.pipeline = test_pipeline\n    return cfg", "CodeBleu_score": 19.591896752098144}, {"code": "def build_test_pipeline(cfg: ConfigType) -> ConfigType\n\t\n    dataset_pipeline = cfg[\"dataset\"][\"pipeline\"]\n    test_pipeline = deepcopy(dataset_pipeline)\n    test_pipeline[0] = deepcopy(dataset_pipeline[0])\n    test_pipeline[0][\"transforms\"] = [\n        transform for transform in dataset_pipeline[0][\"transforms\"] if transform[\"name\"] == \"Resize\"\n    ]\n    test_pipeline.append(deepcopy(dataset_pipeline[-1]))\n    test_pipeline[-1][\"transforms\"] = [\n        transform for transform in dataset_pipeline[-1][\"transforms\"] if transform[\"name\"] == \"Compose\"\n    ]\n    return test_pipeline\n\n", "CodeBleu_score": 31.580605048698995}]}
{"instance_id": "48a358ab5a38", "generation_result": [{"code": "def inference_mot(model: nn.Module, img: np.ndarray, frame_id: int,\n                  video_len: int) -> SampleList\n\t\n    # prepare input data\n    data = dict(\n        img=img,\n        frame_id=frame_id,\n        ori_shape=img.shape,\n        img_id=0,\n        video_len=video_len)\n    data = [data]\n\n    # build the pipeline\n    cfg = model.cfg\n    test_pipeline = build_test_pipeline(cfg)\n    data = test_pipeline(data)\n\n    # check if model is on cpu\n    if next(model.parameters()).device!= torch.device('cpu'):\n        model = model.cpu()\n        model.eval()\n\n    # perform forward pass without gradient computation\n    with torch.no_grad():\n        results = model.forward(return_loss=False, **data[0])\n\n    return results\n\n", "CodeBleu_score": 37.104225847613655}, {"code": "def inference_mot(model: nn.Module, img: np.ndarray, frame_id: int,\n                  video_len: int) -> SampleList\n\t\n    # prepare data\n    data = dict(\n        img=img,\n        frame_id=frame_id,\n        ori_shape=img.shape[:2],\n        img_id=frame_id,\n        video_len=video_len)\n\n    # build the pipeline\n    test_pipeline = build_test_pipeline(model.cfg)\n    data = test_pipeline(data)\n\n    # put the data into a list, and send it to a device\n    data = [data]\n    if next(model.parameters()).device!= torch.device('cpu'):\n        data = track_utils.to_device(data, 'cuda')\n\n    # forward the model\n    with torch.no_grad():\n        result = model.forward_test(data, **model.test_cfg)\n\n    return result\n\n", "CodeBleu_score": 35.057594088332614}, {"code": "def inference_mot(model: nn.Module, img: np.ndarray, frame_id: int,\n                  video_len: int) -> SampleList\n\t\n    # prepare data\n    data = dict(\n        img=img,\n        frame_id=frame_id,\n        ori_shape=img.shape[:2],\n        img_id=frame_id,\n        video_len=video_len)\n    data = model.cfg.test_dataloader.dataset.pipeline(data)\n\n    # forward the model\n    with torch.no_grad():\n        results = model(return_loss=False, rescale=True, **data)\n\n    return results\n\n", "CodeBleu_score": 23.400400005711365}]}
{"instance_id": "6c4ea2ce1787", "generation_result": [{"code": "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\n\t\n    while True:\n        total_length = len(tokens_a) + len(tokens_b)\n        if total_length <= max_num_tokens:\n            break\n\n        length_diff = total_length - max_num_tokens\n        if length_diff > len(tokens_a):\n            tokens_a = tokens_a[length_diff:]\n        elif length_diff > len(tokens_b):\n            tokens_b = tokens_b[:length_diff]\n        else:\n            if rng.random() < 0.5:\n                tokens_a = tokens_a[length_diff:]\n            else:\n                tokens_b = tokens_b[:length_diff]\n\n", "CodeBleu_score": 44.62245398066635}, {"code": "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\n\t\n    while True:\n        total_length = len(tokens_a) + len(tokens_b)\n        if total_length <= max_num_tokens:\n            break\n\n        if len(tokens_a) > len(tokens_b):\n            if rng.random() < 0.5:\n                del tokens_a[0]\n            else:\n                del tokens_a[-1]\n        else:\n            if rng.random() < 0.5:\n                del tokens_b[0]\n            else:\n                del tokens_b[-1]\n\n", "CodeBleu_score": 50.02038513481243}, {"code": "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\n\t\n    while True:\n        total_length = len(tokens_a) + len(tokens_b)\n        if total_length <= max_num_tokens:\n            break\n\n        if len(tokens_a) > len(tokens_b):\n            if rng.random() < 0.5:\n                tokens_a.pop(0)\n            else:\n                tokens_a.pop()\n        else:\n            if rng.random() < 0.5:\n                tokens_b.pop(0)\n            else:\n                tokens_b.pop()\n\n", "CodeBleu_score": 49.029529139177065}]}
{"instance_id": "007181b6729d", "generation_result": [{"code": "def get_activation(activation_string)\n\t\n    if not isinstance(activation_string, str):\n        return activation_string\n    if not activation_string:\n        return None\n    activation_string = activation_string.lower()\n    if activation_string == \"relu\":\n        return tf.nn.relu\n    elif activation_string == \"softmax\":\n        return tf.nn.softmax\n    elif activation_string == \"sigmoid\":\n        return tf.nn.sigmoid\n    else:\n        raise ValueError(f\"Unsupported activation: {activation_string}\")\n\n", "CodeBleu_score": 59.87826628283545}, {"code": "def get_activation(activation_string)\n\t\n    if not isinstance(activation_string, str):\n        return activation_string\n    if not activation_string:\n        return None\n    act = activation_string.lower()\n    if act =='relu':\n        return tf.nn.relu\n    elif act == 'tanh':\n        return tf.nn.tanh\n    elif act =='sigmoid':\n        return tf.nn.sigmoid\n    else:\n        raise ValueError('Unsupported activation: %s' % act)\n\n", "CodeBleu_score": 72.27197149319441}, {"code": "def get_activation(activation_string)\n\t\n    if isinstance(activation_string, str):\n        if activation_string == \"\":\n            return None\n        elif activation_string == \"relu\":\n            return tf.nn.relu\n        elif activation_string == \"elu\":\n            return tf.nn.elu\n        elif activation_string == \"leaky_relu\":\n            return tf.nn.leaky_relu\n        elif activation_string == \"selu\":\n            return tf.nn.selu\n        elif activation_string == \"sigmoid\":\n            return tf.nn.sigmoid\n        elif activation_string == \"tanh\":\n            return tf.nn.tanh\n        else:\n            raise ValueError(\"Unknown activation function: {}\".format(activation_string))\n    else:\n        return activation_string\n\n", "CodeBleu_score": 30.281776237006703}]}
{"instance_id": "d5d6e0315899", "generation_result": [{"code": "def dropout(input_tensor, dropout_prob)\n\t\n    if dropout_prob is None or dropout_prob == 0.0:\n        return input_tensor\n    output = tf.nn.dropout(input_tensor, 1.0 - dropout_prob)\n    return output\n\n", "CodeBleu_score": 87.51451468041077}, {"code": "def dropout(input_tensor, dropout_prob)\n\t\n    if dropout_prob is None or dropout_prob == 0.0:\n        return input_tensor\n    output = tf.nn.dropout(input_tensor, 1.0 - dropout_prob)\n    return output\n\n", "CodeBleu_score": 87.51451468041077}, {"code": "def dropout(input_tensor, dropout_prob)\n\t\n    if dropout_prob is None or dropout_prob == 0.0:\n        return input_tensor\n    output = tf.nn.dropout(input_tensor, 1.0 - dropout_prob)\n    return output\n", "CodeBleu_score": 87.51451468041077}]}
{"instance_id": "828917b9ac53", "generation_result": [{"code": "def create_attention_mask_from_input_mask(from_tensor, to_mask)\n\t\n    from_shape = get_shape_list(from_tensor, expected_rank=[2, 3])\n    batch_size = from_shape[0]\n    from_seq_length = from_shape[1]\n\n    to_shape = get_shape_list(to_mask, expected_rank=2)\n    to_seq_length = to_shape[1]\n\n    to_mask = tf.cast(\n        tf.reshape(to_mask, [batch_size, 1, to_seq_length]), tf.float32)\n\n    # We don't assume that `from_tensor` is a mask (although it could be). We\n    # don't actually care if we attend *from* padding tokens (only *to* padding)\n    # tokens so we create a tensor of all ones.\n    #\n    # `broadcast_ones` = [batch_size, from_seq_length, 1]\n    broadcast_ones = tf.ones(\n        shape=[batch_size, from_seq_length, 1], dtype=tf.float32)\n\n    # Here we broadcast along two dimensions to create the mask.\n    mask = broadcast_ones * to_mask\n\n    return mask", "CodeBleu_score": 64.09901013046853}, {"code": "def create_attention_mask_from_input_mask(from_tensor, to_mask)\n\t\n    from_shape = get_shape_list(from_tensor, expected_rank=[2, 3])\n    batch_size = from_shape[0]\n    from_seq_length = from_shape[1]\n\n    to_shape = get_shape_list(to_mask, expected_rank=2)\n    to_seq_length = to_shape[1]\n\n    to_mask = tf.cast(\n        tf.reshape(to_mask, [batch_size, 1, to_seq_length]), tf.float32)\n    broadcast_ones = tf.ones(\n        shape=[batch_size, from_seq_length, 1], dtype=tf.float32)\n    mask = broadcast_ones * to_mask\n\n    return mask\n", "CodeBleu_score": 64.09901013046853}, {"code": "def create_attention_mask_from_input_mask(from_tensor, to_mask)\n\t\n    from_shape = get_shape_list(from_tensor, expected_rank=[2, 3])\n    batch_size = from_shape[0]\n    from_seq_length = from_shape[1]\n\n    to_shape = get_shape_list(to_mask, expected_rank=2)\n    to_seq_length = to_shape[1]\n\n    to_mask = tf.cast(\n        tf.reshape(to_mask, [batch_size, 1, to_seq_length]), tf.float32)\n\n    # We don't assume that `from_tensor` is a mask (although it could be). We\n    # don't actually care if we attend *from* padding tokens (only *to* padding)\n    # tokens so we create a tensor of all ones.\n    #\n    # `broadcast_ones` = [batch_size, from_seq_length, 1]\n    broadcast_ones = tf.ones(\n        shape=[batch_size, from_seq_length, 1], dtype=tf.float32)\n\n    # Here we broadcast along two dimensions to create the mask.\n    mask = broadcast_ones * to_mask\n\n    return mask\n", "CodeBleu_score": 64.09901013046853}]}
{"instance_id": "d7798e41a723", "generation_result": [{"code": "def transpose_for_scores(input_tensor, batch_size, num_attention_heads,\n                           seq_length, width)\n\t\n    output_tensor = tf.reshape(input_tensor, [batch_size, seq_length, num_attention_heads, width])\n    output_tensor = tf.transpose(output_tensor, [0, 2, 1, 3])\n    return output_tensor\n\n", "CodeBleu_score": 54.22065594588168}, {"code": "def transpose_for_scores(input_tensor, batch_size, num_attention_heads,\n                           seq_length, width)\n\t\n    output_tensor = tf.reshape(\n        input_tensor, [batch_size, seq_length, num_attention_heads, width])\n\n    output_tensor = tf.transpose(output_tensor, [0, 2, 1, 3])\n\n    return output_tensor\n\n", "CodeBleu_score": 59.99845781533536}, {"code": "def transpose_for_scores(input_tensor, batch_size, num_attention_heads,\n                           seq_length, width)\n\t\n    output_tensor = tf.reshape(\n        input_tensor, [batch_size, seq_length, num_attention_heads, width])\n\n    output_tensor = tf.transpose(output_tensor, [0, 2, 1, 3])\n    return output_tensor\n\n", "CodeBleu_score": 59.99845781533536}]}
{"instance_id": "711d466ace09", "generation_result": [{"code": "def reshape_to_matrix(input_tensor)\n\t\n    input_tensor = tf.convert_to_tensor(input_tensor)\n    if input_tensor.get_shape().ndims < 2:\n        raise ValueError(\"The input tensor should have rank 2 or higher.\")\n    if input_tensor.get_shape().ndims == 2:\n        return input_tensor\n    return tf.reshape(input_tensor, [-1, input_tensor.get_shape().dims[-1].value])\n\n", "CodeBleu_score": 31.75426415760581}, {"code": "def reshape_to_matrix(input_tensor)\n\t\n    if len(input_tensor.shape) < 2:\n        raise ValueError(\"The input tensor must have at least 2 dimensions.\")\n    elif len(input_tensor.shape) == 2:\n        return input_tensor\n    else:\n        return tf.reshape(input_tensor, [-1, input_tensor.shape[-1]])\n\n", "CodeBleu_score": 32.92279761766094}, {"code": "def reshape_to_matrix(input_tensor)\n\t\n    if len(input_tensor.shape) < 2:\n        raise ValueError('Input tensor must have at least 2 dimensions')\n    if len(input_tensor.shape) == 2:\n        return input_tensor\n    return tf.reshape(input_tensor, [-1, input_tensor.shape[-1]])\n\n", "CodeBleu_score": 32.760970356653296}]}
{"instance_id": "7adcd08f531e", "generation_result": [{"code": "def get_train_examples(self, data_dir)\n\t\n    return self._create_examples(\n        self._read_tsv(os.path.join(data_dir, \"train.tsv\")), \"train\")", "CodeBleu_score": 6.324149162613836}, {"code": "def get_train_examples(self, data_dir)\n\t\n    return self._create_examples(\n        self._read_tsv(os.path.join(data_dir, \"train.tsv\")), \"train\")\n", "CodeBleu_score": 6.324149162613836}, {"code": "def get_train_examples(self, data_dir)\n\t\n    return self._create_examples(\n        self._read_tsv(os.path.join(data_dir, \"train.tsv\")), \"train\")\n", "CodeBleu_score": 6.324149162613836}]}
{"instance_id": "26a0bb96dab9", "generation_result": [{"code": "def metric_fn(per_example_loss, label_ids, logits, is_real_example)\n\t\n    predictions = tf.argmax(logits, axis=-1, output_type=tf.int32)\n    accuracy = tf.metrics.accuracy(\n        labels=label_ids, predictions=predictions, weights=is_real_example\n    )\n    loss = tf.metrics.mean(values=per_example_loss, weights=is_real_example)\n    return {\n        \"eval_accuracy\": accuracy,\n        \"eval_loss\": loss,\n    }\n\n", "CodeBleu_score": 62.41740489696974}, {"code": "def metric_fn(per_example_loss, label_ids, logits, is_real_example)\n\t\n    predictions = tf.argmax(logits, axis=-1, output_type=tf.int32)\n    accuracy = tf.metrics.accuracy(\n        labels=label_ids, predictions=predictions, weights=is_real_example\n    )\n    loss = tf.metrics.mean(values=per_example_loss, weights=is_real_example)\n    return {\"eval_accuracy\": accuracy, \"eval_loss\": loss}\n\n", "CodeBleu_score": 56.84338615000034}, {"code": "def metric_fn(per_example_loss, label_ids, logits, is_real_example)\n\t\n    predictions = tf.argmax(logits, axis=-1, output_type=tf.int32)\n    accuracy = tf.metrics.accuracy(\n        labels=label_ids, predictions=predictions, weights=is_real_example\n    )\n    loss = tf.metrics.mean(values=per_example_loss, weights=is_real_example)\n    return {\"eval_accuracy\": accuracy, \"eval_loss\": loss}\n\n", "CodeBleu_score": 56.84338615000034}]}
{"instance_id": "6154e4d28509", "generation_result": [{"code": "def convert_examples_to_features(examples, label_list, max_seq_length,\n                                 tokenizer)\n\t\"\"\"Convert a set of `InputExample`s to a list of `InputFeatures`.\"\"\"\n\n  features = []\n  for (ex_index, example) in enumerate(examples):\n    if ex_index % 10000 == 0:\n      tf.logging.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n\n    feature = convert_single_example(ex_index, example, label_list,\n                                     max_seq_length, tokenizer)\n\n    features.append(feature)\n  return features\n\n", "CodeBleu_score": 83.05386096616228}, {"code": "def convert_examples_to_features(examples, label_list, max_seq_length,\n                                 tokenizer)\n\t\"\"\"Convert a set of `InputExample`s to a list of `InputFeatures`.\"\"\"\n\n  features = []\n  for (ex_index, example) in enumerate(examples):\n    if ex_index % 10000 == 0:\n      tf.logging.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n\n    feature = convert_single_example(ex_index, example, label_list,\n                                     max_seq_length, tokenizer)\n\n    features.append(feature)\n  return features\n\n", "CodeBleu_score": 83.05386096616228}, {"code": "def convert_examples_to_features(examples, label_list, max_seq_length,\n                                 tokenizer)\n\tfeatures = []\n  for (ex_index, example) in enumerate(examples):\n    if ex_index % 10000 == 0:\n      tf.logging.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n\n    feature = convert_single_example(ex_index, example, label_list,\n                                     max_seq_length, tokenizer)\n\n    features.append(feature)\n  return features\n\n", "CodeBleu_score": 82.87746288938162}]}
{"instance_id": "e09c0db71ebb", "generation_result": [{"code": "def get_next_sentence_output(bert_config, input_tensor, labels)\n\t\n    with tf.variable_scope(\"cls/seq_relationship\"):\n        output_weights = tf.get_variable(\n            \"output_weights\",\n            shape=[2, bert_config.hidden_size],\n            initializer=modeling.create_initializer(bert_config.initializer_range))\n        output_bias = tf.get_variable(\n            \"output_bias\", shape=[2], initializer=tf.zeros_initializer())\n\n    logits = tf.matmul(input_tensor, output_weights, transpose_b=True)\n    logits = tf.nn.bias_add(logits, output_bias)\n    log_probs = tf.nn.log_softmax(logits, axis=-1)\n\n    labels = tf.reshape(labels, [-1])\n    one_hot_labels = tf.one_hot(labels, depth=2, dtype=tf.float32)\n\n    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n    loss = tf.reduce_mean(per_example_loss)\n    return (loss, per_example_loss, log_probs)\n\n", "CodeBleu_score": 87.87003311020517}, {"code": "def get_next_sentence_output(bert_config, input_tensor, labels)\n\t\n    with tf.variable_scope(\"cls/seq_relationship\"):\n        output_weights = tf.get_variable(\n            \"output_weights\",\n            shape=[2, bert_config.hidden_size],\n            initializer=modeling.create_initializer(bert_config.initializer_range))\n        output_bias = tf.get_variable(\n            \"output_bias\", shape=[2], initializer=tf.zeros_initializer())\n\n        logits = tf.matmul(input_tensor, output_weights, transpose_b=True)\n        logits = tf.nn.bias_add(logits, output_bias)\n        log_probs = tf.nn.log_softmax(logits, axis=-1)\n\n        labels = tf.reshape(labels, [-1])\n        one_hot_labels = tf.one_hot(labels, depth=2, dtype=tf.float32)\n\n        per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n        loss = tf.reduce_mean(per_example_loss)\n        return (loss, per_example_loss, log_probs)\n\n", "CodeBleu_score": 86.94900271179796}, {"code": "def get_next_sentence_output(bert_config, input_tensor, labels)\n\t\n    with tf.variable_scope(\"cls/seq_relationship\"):\n        output_weights = tf.get_variable(\n            \"output_weights\",\n            shape=[2, bert_config.hidden_size],\n            initializer=modeling.create_initializer(bert_config.initializer_range))\n        output_bias = tf.get_variable(\n            \"output_bias\", shape=[2], initializer=tf.zeros_initializer())\n\n        logits = tf.matmul(input_tensor, output_weights, transpose_b=True)\n        logits = tf.nn.bias_add(logits, output_bias)\n        log_probs = tf.nn.log_softmax(logits, axis=-1)\n\n        labels = tf.reshape(labels, [-1])\n        one_hot_labels = tf.one_hot(labels, depth=2, dtype=tf.float32)\n\n        per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n        loss = tf.reduce_mean(per_example_loss)\n        return (loss, per_example_loss, log_probs)\n\n", "CodeBleu_score": 86.94900271179796}]}
{"instance_id": "1f66b020f485", "generation_result": [{"code": "def gather_indexes(sequence_tensor, positions)\n\t\n    # sequence_tensor: [batch_size, seq_length, dim]\n    # positions: [batch_size, num_positions]\n    # output: [batch_size, num_positions, dim]\n    sequence_flat = tf.reshape(sequence_tensor, [-1, tf.shape(sequence_tensor)[-1]])\n    flat_offsets = tf.reshape(tf.range(0, tf.shape(sequence_tensor)[0], dtype=tf.int32) * tf.shape(sequence_tensor)[1], [-1, 1])\n    flat_positions = tf.reshape(positions + flat_offsets, [-1])\n    output_flat = tf.gather(sequence_flat, flat_positions)\n    output = tf.reshape(output_flat, tf.shape(positions) + [int(sequence_tensor.shape[-1])])\n    return output\n\n", "CodeBleu_score": 28.739745687547284}, {"code": "def gather_indexes(sequence_tensor, positions)\n\t\n    # sequence_tensor: [batch_size, sequence_length,...]\n    # positions: [batch_size,...]\n    # output_tensor: [batch_size,...]\n    sequence_flat = tf.reshape(sequence_tensor, [-1, tf.shape(sequence_tensor)[-1]])\n    flat_offsets = tf.reshape(tf.range(tf.shape(sequence_tensor)[0]) * tf.shape(sequence_tensor)[1], [-1, 1])\n    gather_indices = tf.reshape(positions + flat_offsets, [-1])\n    output_tensor = tf.gather(sequence_flat, gather_indices)\n    return output_tensor\n", "CodeBleu_score": 22.34329232052142}, {"code": "def gather_indexes(sequence_tensor, positions)\n\t\n    \"\"\"Gathers the vectors at the specific positions over a minibatch.\"\"\"\n    sequence_shape = modeling.get_shape_list(sequence_tensor, expected_rank=3)\n    batch_size = sequence_shape[0]\n    seq_length = sequence_shape[1]\n    width = sequence_shape[2]\n\n    flat_offsets = tf.reshape(\n        tf.range(0, batch_size, dtype=tf.int32) * seq_length, [-1, 1])\n    flat_positions = tf.reshape(positions + flat_offsets, [-1])\n    flat_sequence_tensor = tf.reshape(sequence_tensor,\n                                      [batch_size * seq_length, width])\n    output_tensor = tf.gather(flat_sequence_tensor, flat_positions)\n    return output_tensor\n\n", "CodeBleu_score": 57.853066637478335}]}
{"instance_id": "956bffee9a33", "generation_result": [{"code": "def _decode_record(record, name_to_features)\n\t\n    example = tf.io.parse_single_example(record, name_to_features)\n    for name in list(example.keys()):\n        t = example[name]\n        if t.dtype == tf.int64:\n            t = tf.cast(t, tf.int32)\n        example[name] = t\n    return example\n\n", "CodeBleu_score": 59.0051892348668}, {"code": "def _decode_record(record, name_to_features)\n\t\n    example = tf.io.parse_single_example(record, name_to_features)\n    for name in list(example.keys()):\n        t = example[name]\n        if t.dtype == tf.int64:\n            t = tf.cast(t, tf.int32)\n        example[name] = t\n    return example\n\n", "CodeBleu_score": 59.0051892348668}, {"code": "def _decode_record(record, name_to_features)\n\t\n    example = tf.io.parse_single_example(record, name_to_features)\n    for name in list(example.keys()):\n        t = example[name]\n        if t.dtype == tf.int64:\n            t = tf.cast(t, tf.int32)\n        example[name] = t\n    return example\n\n", "CodeBleu_score": 59.0051892348668}]}
{"instance_id": "6b97f354e641", "generation_result": [{"code": "def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer,\n                         orig_answer_text)\n\t\n    tok_answer_text = tokenizer.tokenize(orig_answer_text)\n    for new_start in range(input_start, input_end + 1):\n        for new_end in range(input_end, new_start - 1, -1):\n            text_span = \" \".join(doc_tokens[new_start:(new_end + 1)])\n            if text_span == \"\".join(tok_answer_text):\n                return (new_start, new_end)\n\n    return (input_start, input_end)\n\n", "CodeBleu_score": 69.00394732612052}, {"code": "def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer,\n                         orig_answer_text)\n\t\n    tok_answer_text = tokenizer.tokenize(orig_answer_text)\n    for new_start in range(input_start, input_end + 1):\n        for new_end in range(input_end, new_start - 1, -1):\n            text_span = \" \".join(doc_tokens[new_start:(new_end + 1)])\n            if text_span == \"\".join(tok_answer_text):\n                return (new_start, new_end)\n\n    return (input_start, input_end)\n\n", "CodeBleu_score": 69.00394732612052}, {"code": "def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer,\n                         orig_answer_text)\n\t\n    tok_answer_text = tokenizer.tokenize(orig_answer_text)\n    for new_start in range(input_start, input_end + 1):\n        for new_end in range(input_end, new_start - 1, -1):\n            text_span = \" \".join(doc_tokens[new_start:(new_end + 1)])\n            if text_span == \"\".join(tok_answer_text):\n                return (new_start, new_end)\n\n    return (input_start, input_end)\n\n", "CodeBleu_score": 69.00394732612052}]}
{"instance_id": "aba334b92b34", "generation_result": [{"code": "def _check_is_max_context(doc_spans, cur_span_index, position)\n\t\n    best_score = None\n    best_span_index = None\n    for (span_index, doc_span) in enumerate(doc_spans):\n        end = doc_span.start + doc_span.length - 1\n        if position < doc_span.start:\n            continue\n        if position > end:\n            continue\n        num_left_context = position - doc_span.start\n        num_right_context = end - position\n        score = min(num_left_context, num_right_context) + 0.01 * doc_span.length\n        if best_score is None or score > best_score:\n            best_score = score\n            best_span_index = span_index\n    return cur_span_index == best_span_index\n\n", "CodeBleu_score": 77.36170205111968}, {"code": "def _check_is_max_context(doc_spans, cur_span_index, position)\n\t\n    best_score = None\n    best_span_index = None\n    for (span_index, doc_span) in enumerate(doc_spans):\n        end = doc_span.start + doc_span.length - 1\n        if position < doc_span.start:\n            continue\n        if position > end:\n            continue\n        num_left_context = position - doc_span.start\n        num_right_context = end - position\n        score = min(num_left_context, num_right_context) + 0.01 * doc_span.length\n        if best_score is None or score > best_score:\n            best_score = score\n            best_span_index = span_index\n    return cur_span_index == best_span_index\n\n", "CodeBleu_score": 77.36170205111968}, {"code": "def _check_is_max_context(doc_spans, cur_span_index, position)\n\t\n    best_score = None\n    best_span_index = None\n    for (span_index, doc_span) in enumerate(doc_spans):\n        end = doc_span.start + doc_span.length - 1\n        if position < doc_span.start:\n            continue\n        if position > end:\n            continue\n        num_left_context = position - doc_span.start\n        num_right_context = end - position\n        score = min(num_left_context, num_right_context) + 0.01 * doc_span.length\n        if best_score is None or score > best_score:\n            best_score = score\n            best_span_index = span_index\n    return cur_span_index == best_span_index\n\n", "CodeBleu_score": 77.36170205111968}]}
{"instance_id": "cf92ad509f6a", "generation_result": [{"code": "def load_vocab(vocab_file)\n\t\n    vocab = collections.OrderedDict()\n    index = 0\n    with open(vocab_file, \"r\", encoding=\"utf-8\") as reader:\n        while True:\n            token = convert_to_unicode(reader.readline())\n            if not token:\n                break\n            token = token.strip()\n            vocab[token] = index\n            index += 1\n    return vocab\n\n", "CodeBleu_score": 67.2956448618734}, {"code": "def load_vocab(vocab_file)\n\t\n    vocab = collections.OrderedDict()\n    index = 0\n    with open(vocab_file, \"r\", encoding=\"utf-8\") as reader:\n        while True:\n            token = convert_to_unicode(reader.readline())\n            if not token:\n                break\n            token = token.strip()\n            vocab[token] = index\n            index += 1\n    return vocab\n\n", "CodeBleu_score": 67.2956448618734}, {"code": "def load_vocab(vocab_file)\n\t\n    vocab = collections.OrderedDict()\n    index = 0\n    with open(vocab_file, \"r\", encoding=\"utf-8\") as reader:\n        while True:\n            token = convert_to_unicode(reader.readline())\n            if not token:\n                break\n            token = token.strip()\n            vocab[token] = index\n            index += 1\n    return vocab\n\n", "CodeBleu_score": 67.2956448618734}]}
{"instance_id": "6c865803ac12", "generation_result": [{"code": "def convert_by_vocab(vocab, items)\n\t\n    output = []\n    for item in items:\n        output.append(vocab[item])\n    return output\n\n", "CodeBleu_score": 79.63749907165396}, {"code": "def convert_by_vocab(vocab, items)\n\t\n    output = []\n    for item in items:\n        output.append(vocab[item])\n    return output\n\n", "CodeBleu_score": 79.63749907165396}, {"code": "def convert_by_vocab(vocab, items)\n\t\n    output = []\n    for item in items:\n        output.append(vocab[item])\n    return output\n\n", "CodeBleu_score": 79.63749907165396}]}
{"instance_id": "e93b9200dc4a", "generation_result": [{"code": "def tokenize(self, text)\n\t\n    tokenized_text = []\n    for token in self.tokenizer.basic_tokenizer.tokenize(text):\n        for sub_token in self.tokenizer.wordpiece_tokenizer.tokenize(token):\n            tokenized_text.append(sub_token)\n    return tokenized_text\n\n", "CodeBleu_score": 54.2673643428779}, {"code": "def tokenize(self, text)\n\t\n    tokens = self.basic_tokenizer.tokenize(text)\n    return self.wordpiece_tokenizer.tokenize(\" \".join(tokens))\n\n", "CodeBleu_score": 18.176977416125304}, {"code": "def tokenize(self, text)\n\t\n    tokens = []\n    for token in self.basic_tokenizer.tokenize(text):\n        tokens.extend(self.wordpiece_tokenizer.tokenize(token))\n    return tokens\n", "CodeBleu_score": 52.08741589116925}]}
{"instance_id": "2ff6f6db53c9", "generation_result": [{"code": "def safe_load(path, offset, duration, sample_rate, dtype)\n\t\n    try:\n        data, sr = librosa.load(path, offset=offset, duration=duration, sr=sample_rate)\n        data = data.astype(dtype)\n        return data, False\n    except Exception as e:\n        print(f\"Failed to load {path} from {offset} to {offset + duration} seconds with sample rate {sample_rate}.\")\n        print(e)\n        return -1.0, True\n\n", "CodeBleu_score": 37.062622318590414}, {"code": "def safe_load(path, offset, duration, sample_rate, dtype)\n\t\n    try:\n        audio, _ = librosa.load(path, sr=sample_rate, offset=offset, duration=duration)\n        audio = audio.astype(dtype)\n        return audio, False\n    except Exception as e:\n        logging.error(f\"Failed to load audio: {path} | {offset} | {duration} | {sample_rate} | {dtype}\")\n        logging.error(e)\n        return -1.0, True\n\n", "CodeBleu_score": 33.432517737409704}, {"code": "def safe_load(path, offset, duration, sample_rate, dtype)\n\t\n    try:\n        # Load audio data from the specified file path\n        audio_data, _ = librosa.load(path, offset=offset, duration=duration, sr=sample_rate)\n\n        # Check if the audio data is empty\n        if audio_data.size == 0:\n            # Log warning message\n            logger.warning(f\"Empty audio data from {path}.\")\n\n            # Return default value with error flag\n            return -1.0, True\n\n        # Convert audio data to the specified dtype\n        audio_data = audio_data.astype(dtype)\n\n        # Return audio data with no error\n        return audio_data, False\n    except Exception as e:\n        # Log error message\n        logger.error(f\"Failed to load audio data from {path}.\", exc_info=True)\n\n        # Return default value with error flag\n        return -1.0, True\n\n", "CodeBleu_score": 36.20365403942999}]}
{"instance_id": "12fbe1f03d8f", "generation_result": [{"code": "def to_n_channels(waveform: tf.Tensor, n_channels: int) -> tf.Tensor\n\t\n    if waveform.shape[-1] < n_channels:\n        waveform = tf.tile(waveform, [1, n_channels // waveform.shape[-1]])\n    return waveform[..., :n_channels]\n\n", "CodeBleu_score": 44.10632157302486}, {"code": "def to_n_channels(waveform: tf.Tensor, n_channels: int) -> tf.Tensor\n\t\n    if waveform.shape[1] >= n_channels:\n        return waveform[:, :n_channels]\n    else:\n        return tf.tile(waveform, [1, n_channels // waveform.shape[1]])[:, :n_channels]\n\n", "CodeBleu_score": 40.988233950674235}, {"code": "def to_n_channels(waveform: tf.Tensor, n_channels: int) -> tf.Tensor\n\t\n    n_channels = int(n_channels)\n    if n_channels < 1:\n        raise ValueError(\"n_channels must be greater than or equal to 1\")\n    if waveform.shape[1] == n_channels:\n        return waveform\n    if waveform.shape[1] > n_channels:\n        return waveform[:, :n_channels]\n    return tf.tile(waveform, (1, n_channels // waveform.shape[1]))[:, :n_channels]\n\n", "CodeBleu_score": 44.09144172316742}]}
{"instance_id": "5e215f029b2e", "generation_result": [{"code": "def spectrogram_to_db_uint(\n    spectrogram: tf.Tensor, db_range: float = 100.0, **kwargs\n) -> tf.Tensor\n\t\n    spectrogram = gain_to_db(spectrogram, **kwargs)\n    max_db = tf.reduce_max(spectrogram)\n    spectrogram = tf.clip_by_value(spectrogram, max_db - db_range, max_db)\n    spectrogram = tf.cast(spectrogram, tf.uint8)\n    return spectrogram\n\n", "CodeBleu_score": 43.825133432023}, {"code": "def spectrogram_to_db_uint(\n    spectrogram: tf.Tensor, db_range: float = 100.0, **kwargs\n) -> tf.Tensor\n\t\n    spectrogram_db = gain_to_db(spectrogram, **kwargs)\n    max_db = tf.math.reduce_max(spectrogram_db)\n    spectrogram_db = spectrogram_db - max_db + db_range\n    spectrogram_db = tf.clip_by_value(spectrogram_db, 0.0, db_range)\n    spectrogram_db = tf.cast(spectrogram_db, tf.uint8)\n    return spectrogram_db\n\n", "CodeBleu_score": 43.50478123122321}, {"code": "def spectrogram_to_db_uint(\n    spectrogram: tf.Tensor, db_range: float = 100.0, **kwargs\n) -> tf.Tensor\n\t\n    # Convert the spectrogram to dB\n    db = gain_to_db(spectrogram, **kwargs)\n    # Find the maximum dB value\n    max_db = tf.reduce_max(db)\n    # Clip the dB spectrogram to a specified range below the maximum\n    clipped_db = tf.clip_by_value(db, max_db - db_range, max_db)\n    # Convert the clipped dB spectrogram to uint8 format\n    uint8_spectrogram = tf.cast(\n        (clipped_db - (max_db - db_range)) / db_range * 255.0, tf.uint8\n    )\n    return uint8_spectrogram\n\n", "CodeBleu_score": 42.65128584458164}]}
{"instance_id": "b65c0968dae7", "generation_result": [{"code": "def compute_spectrogram_tf(\n    waveform: tf.Tensor,\n    frame_length: int = 2048,\n    frame_step: int = 512,\n    spec_exponent: float = 1.0,\n    window_exponent: float = 1.0,\n) -> tf.Tensor\n\t\n    # Apply Hann window function raised to the specified exponent\n    window = tf.signal.hann_window(frame_length, periodic=True)\n    window = tf.pow(window, window_exponent)\n\n    # Compute STFT with the specified frame length and step\n    stft = tf.signal.stft(\n        waveform,\n        frame_length=frame_length,\n        frame_step=frame_step,\n        fft_length=frame_length,\n        window_fn=lambda x, y: window,\n        pad_end=False,\n    )\n\n    # Transpose the STFT tensor and compute the absolute value\n    stft = tf.transpose(stft)\n    stft = tf.abs(stft)\n\n    # Raise the absolute value to the specified exponent\n    stft = tf.pow(stft, spec_exponent)\n\n    return stft\n\n", "CodeBleu_score": 41.073063092036946}, {"code": "def compute_spectrogram_tf(\n    waveform: tf.Tensor,\n    frame_length: int = 2048,\n    frame_step: int = 512,\n    spec_exponent: float = 1.0,\n    window_exponent: float = 1.0,\n) -> tf.Tensor\n\t\n    # Apply Hann window function\n    hann_window = tf.signal.hann_window(\n        frame_length, periodic=True, dtype=tf.dtypes.float32\n    )\n    hann_window = tf.pow(hann_window, window_exponent)\n    # Compute STFT\n    stft = tf.signal.stft(\n        waveform,\n        frame_length=frame_length,\n        frame_step=frame_step,\n        fft_length=frame_length,\n        window_fn=lambda x, y: hann_window,\n        pad_end=False,\n    )\n    # Transpose and take absolute value\n    spectrogram = tf.abs(tf.transpose(stft))\n    # Apply exponent\n    spectrogram = tf.pow(spectrogram, spec_exponent)\n    return spectrogram\n\n", "CodeBleu_score": 41.212241283691434}, {"code": "def compute_spectrogram_tf(\n    waveform: tf.Tensor,\n    frame_length: int = 2048,\n    frame_step: int = 512,\n    spec_exponent: float = 1.0,\n    window_exponent: float = 1.0,\n) -> tf.Tensor\n\t\n    # Get the window function and apply it to the waveform\n    window_function = tf.signal.hann_window(frame_length, periodic=True, dtype=tf.dtypes.float32)\n    window_function = tf.pow(window_function, window_exponent)\n    waveform = tf.multiply(waveform, window_function)\n    # Compute the STFT of the waveform\n    stft = tf.signal.stft(\n        waveform,\n        frame_length=frame_length,\n        frame_step=frame_step,\n        fft_length=frame_length,\n        window_fn=None,\n        pad_end=False,\n    )\n    # Transpose the STFT tensor and take the absolute value\n    stft = tf.transpose(stft)\n    stft = tf.abs(stft)\n    # Apply the spectrogram exponent\n    stft = tf.pow(stft, spec_exponent)\n    return stft\n\n", "CodeBleu_score": 37.93139817048547}]}
{"instance_id": "a4686140a634", "generation_result": [{"code": "def time_stretch(\n    spectrogram: tf.Tensor,\n    factor: float = 1.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n    time_dim = spectrogram.shape[1]\n    new_time_dim = int(time_dim * factor)\n    spectrogram = tf.image.resize(\n        spectrogram, (spectrogram.shape[0], new_time_dim), method=method\n    )\n    if new_time_dim > time_dim:\n        spectrogram = spectrogram[:, :time_dim, :]\n    else:\n        spectrogram = tf.pad(\n            spectrogram, [[0, 0], [0, time_dim - new_time_dim], [0, 0]]\n        )\n    return spectrogram\n\n", "CodeBleu_score": 34.84942720209563}, {"code": "def time_stretch(\n    spectrogram: tf.Tensor,\n    factor: float = 1.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n    time_dim = tf.shape(spectrogram)[1]\n    new_time_dim = tf.cast(tf.round(tf.cast(time_dim, tf.float32) * factor), tf.int32)\n    spectrogram = tf.expand_dims(spectrogram, axis=0)\n    spectrogram = tf.image.resize(\n        spectrogram, [tf.shape(spectrogram)[1], new_time_dim], method=method\n    )\n    spectrogram = tf.squeeze(spectrogram, axis=0)\n    if new_time_dim > time_dim:\n        spectrogram = spectrogram[:, :time_dim]\n    else:\n        spectrogram = tf.pad(\n            spectrogram, [[0, 0], [0, time_dim - new_time_dim]], constant_values=0.0\n        )\n    return spectrogram\n\n", "CodeBleu_score": 41.9227471539931}, {"code": "def time_stretch(\n    spectrogram: tf.Tensor,\n    factor: float = 1.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n    time_dim = tf.shape(spectrogram)[-1]\n    new_time_dim = tf.cast(time_dim * factor, tf.int32)\n    spectrogram = tf.image.resize(\n        spectrogram, size=(spectrogram.shape[0], new_time_dim), method=method\n    )\n    if factor > 1:\n        spectrogram = spectrogram[:, :time_dim]\n    else:\n        spectrogram = tf.pad(\n            spectrogram, [[0, 0], [0, time_dim - new_time_dim]], constant_values=0\n        )\n    return spectrogram\n\n", "CodeBleu_score": 37.334610684357294}]}
{"instance_id": "6dd01101d4ae", "generation_result": [{"code": "def pitch_shift(\n    spectrogram: tf.Tensor,\n    semitone_shift: float = 0.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n    # Calculate the resizing factor based on the semitone shift\n    factor = 2.0 ** (semitone_shift / 12.0)\n    # Resize the spectrogram along the frequency axis using the specified method\n    resized_spectrogram = tf.image.resize(\n        spectrogram,\n        size=(tf.shape(spectrogram)[0], tf.cast(tf.shape(spectrogram)[1] * factor, tf.int32)),\n        method=method,\n    )\n    # Pad the resized spectrogram to maintain the original dimensions\n    paddings = tf.constant([[0, 0], [0, tf.shape(spectrogram)[1] - tf.shape(resized_spectrogram)[1]]])\n    pitch_shifted_spectrogram = tf.pad(resized_spectrogram, paddings, constant_values=0.0)\n    return pitch_shifted_spectrogram\n\n", "CodeBleu_score": 35.15882863119175}, {"code": "def pitch_shift(\n    spectrogram: tf.Tensor,\n    semitone_shift: float = 0.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n    if semitone_shift == 0.0:\n        return spectrogram\n    semitone_shift = tf.cast(semitone_shift, tf.float32)\n    freq_shift_factor = tf.exp(tf.math.log(2.0) * semitone_shift / 12.0)\n    spectrogram_shape = tf.shape(spectrogram)\n    freq_axis_length = tf.cast(spectrogram_shape[1], tf.float32)\n    resized_freq_axis_length = tf.cast(freq_axis_length * freq_shift_factor, tf.int32)\n    resized_spectrogram = tf.image.resize(\n        spectrogram,\n        [spectrogram_shape[0], resized_freq_axis_length],\n        method=method,\n    )\n    pad_width = spectrogram_shape[1] - resized_freq_axis_length\n    pad_width = tf.cast(pad_width / 2, tf.int32)\n    resized_spectrogram = tf.pad(\n        resized_spectrogram, [[0, 0], [pad_width, pad_width], [0, 0]]\n    )\n    return resized_spectrogram\n\n", "CodeBleu_score": 38.02948227023284}, {"code": "def pitch_shift(\n    spectrogram: tf.Tensor,\n    semitone_shift: float = 0.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n    if semitone_shift == 0.0:\n        return spectrogram\n\n    # Calculate the resizing factor based on the semitone shift\n    resize_factor = 2.0 ** (semitone_shift / 12.0)\n\n    # Resize the spectrogram along the frequency axis using the specified method\n    resized_spectrogram = tf.image.resize(\n        spectrogram,\n        [\n            tf.cast(tf.shape(spectrogram)[0] * resize_factor, tf.int32),\n            tf.shape(spectrogram)[1],\n        ],\n        method=method,\n    )\n\n    # Pad the resized spectrogram to maintain the original dimensions\n    padding_width = tf.cast(\n        tf.shape(spectrogram)[0] - tf.shape(resized_spectrogram)[0], tf.int32\n    )\n    padded_spectrogram = tf.pad(\n        resized_spectrogram, [[0, padding_width], [0, 0]], constant_values=0.0\n    )\n\n    return padded_spectrogram\n\n", "CodeBleu_score": 41.91325719049195}]}
{"instance_id": "612f0826e56a", "generation_result": [{"code": "def _get_conv_activation_layer(params: Dict) -> Any\n\t\n    if params[\"conv_activation\"] == \"relu\":\n        return nn.ReLU()\n    elif params[\"conv_activation\"] == \"elu\":\n        return nn.ELU()\n    elif params[\"conv_activation\"] == \"leaky_relu\":\n        return nn.LeakyReLU(negative_slope=0.2)\n    else:\n        raise ValueError(\n            f\"Invalid value for 'conv_activation' parameter: {params['conv_activation']}\"\n        )\n\n", "CodeBleu_score": 30.121320392765373}, {"code": "def _get_conv_activation_layer(params: Dict) -> Any\n\t\n    if params[\"conv_activation\"] == \"relu\":\n        return tf.keras.layers.ReLU()\n    elif params[\"conv_activation\"] == \"elu\":\n        return tf.keras.layers.ELU()\n    elif params[\"conv_activation\"] == \"leakyrelu\":\n        return tf.keras.layers.LeakyReLU(alpha=0.2)\n    else:\n        raise ValueError(\n            \"The provided activation function is not supported. Please choose between'relu', 'elu', and 'leakyrelu'.\"\n        )\n\n", "CodeBleu_score": 32.9076342099083}, {"code": "def _get_conv_activation_layer(params: Dict) -> Any\n\t\n    if params[\"conv_activation\"] == \"relu\":\n        return nn.ReLU()\n    elif params[\"conv_activation\"] == \"elu\":\n        return nn.ELU()\n    else:\n        return nn.LeakyReLU(negative_slope=0.2)\n\n", "CodeBleu_score": 34.13471863812688}]}
{"instance_id": "2f8b5a2fc0f2", "generation_result": [{"code": "def softmax_unet(\n    input_tensor: tf.Tensor, instruments: Iterable[str], params: Dict = {}\n) -> Dict\n\t\n    logging.info(\"Apply softmax unet\")\n    output_masks = {}\n    for instrument in instruments:\n        output_masks[instrument] = apply_unet(\n            input_tensor,\n            output_name=instrument,\n            params=params,\n            output_mask_logit=True,\n        )\n    #\n    #\n    output = {}\n    for instrument in instruments:\n        output[instrument] = Multiply(name=instrument)([input_tensor, output_masks[instrument]])\n    return output\n\n", "CodeBleu_score": 34.24786737690176}, {"code": "def softmax_unet(\n    input_tensor: tf.Tensor, instruments: Iterable[str], params: Dict = {}\n) -> Dict\n\t\n    instrument_spectrograms = {}\n    for instrument in instruments:\n        output_name = instrument + \"_output\"\n        output = apply_unet(input_tensor, output_name, params)\n        instrument_spectrograms[instrument] = Multiply(name=output_name)(\n            [output, input_tensor]\n        )\n    # Combine logit masks using softmax along the axis specified in params.\n    logit_masks = [\n        instrument_spectrograms[instrument]\n        for instrument in instrument_spectrograms.keys()\n    ]\n    logit_masks = Concatenate(axis=3)(logit_masks)\n    softmax_axis = params.get(\"softmax_axis\", 3)\n    softmax_masks = Softmax(axis=softmax_axis)(logit_masks)\n    # Multiply softmax masks with input tensor to get spectrograms.\n    for i, instrument in enumerate(instruments):\n        instrument_spectrograms[instrument] = Multiply(\n            name=instrument + \"_output\"\n        )([softmax_masks[:, :, :, i : i + 1], input_tensor])\n    return instrument_spectrograms\n\n", "CodeBleu_score": 43.005508981417776}, {"code": "def softmax_unet(\n    input_tensor: tf.Tensor, instruments: Iterable[str], params: Dict = {}\n) -> Dict\n\t\n    logging.info(\"Apply softmax unet\")\n    # Apply U-Net for each instrument.\n    logits = {}\n    for instrument in instruments:\n        logits[instrument] = apply_unet(input_tensor, instrument, params)\n    # Softmax combination.\n    softmax_logits = {}\n    for instrument in instruments:\n        softmax_logits[instrument] = Activation(\"softmax\")(logits[instrument])\n    # Output spectrograms.\n    spectrograms = {}\n    for instrument in instruments:\n        spectrograms[instrument] = Multiply()([softmax_logits[instrument], input_tensor])\n    return spectrograms\n\n", "CodeBleu_score": 29.898472582955588}]}
{"instance_id": "400b19ce8520", "generation_result": [{"code": "def compute_file_checksum(path)\n\t\n    hash_sha256 = hashlib.sha256()\n    with open(path, \"rb\") as f:\n        for chunk in iter(lambda: f.read(4096), b\"\"):\n            hash_sha256.update(chunk)\n    return hash_sha256.hexdigest()\n\n", "CodeBleu_score": 56.4841622322711}, {"code": "def compute_file_checksum(path)\n\t\n    hash_sha256 = hashlib.sha256()\n    with open(path, \"rb\") as f:\n        for chunk in iter(lambda: f.read(4096), b\"\"):\n            hash_sha256.update(chunk)\n    return hash_sha256.hexdigest()\n\n", "CodeBleu_score": 56.4841622322711}, {"code": "def compute_file_checksum(path)\n\t\n    hash_sha256 = hashlib.sha256()\n    with open(path, \"rb\") as f:\n        for chunk in iter(lambda: f.read(4096), b\"\"):\n            hash_sha256.update(chunk)\n    return hash_sha256.hexdigest()\n\n", "CodeBleu_score": 56.4841622322711}]}
{"instance_id": "5f2e9c6e15f1", "generation_result": [{"code": "def checksum(self, name: str) -> str\n\t\n    url = self.url_for(\n        \"checksums\",\n        host=self.host,\n        repository=self.repository,\n        release=self.release,\n        index=\"checksums.json\",\n    )\n    response = requests.get(url)\n    response.raise_for_status()\n    index = response.json()\n    if name not in index:\n        raise ValueError(f\"Model {name} not found in checksum index\")\n    return index[name]\n\n", "CodeBleu_score": 39.679984203850346}, {"code": "def checksum(self, name: str) -> str\n\t\n    url = f\"{self.host}/{self.repository}/{self.release}/index/models.json\"\n    response = requests.get(url)\n    if response.status_code!= 200:\n        raise ValueError(f\"Failed to retrieve index from {url}\")\n    index = response.json()\n    if name not in index:\n        raise ValueError(f\"Model {name} not found in index\")\n    return index[name]\n\n", "CodeBleu_score": 40.77810773513913}, {"code": "def checksum(self, name: str) -> str\n\t\n    url = self._host + self._repository + self._release_path + self._checksum_index\n    response = requests.get(url)\n    if response.status_code!= 200:\n        raise ValueError(\n            f\"Could not get checksum index from {url}. Response code: {response.status_code}\"\n        )\n    index = response.json()\n    if name not in index:\n        raise ValueError(f\"Could not find checksum for {name}\")\n    return index[name]\n\n", "CodeBleu_score": 45.39182140712771}]}
{"instance_id": "9312c8b6032e", "generation_result": [{"code": "def download(self, name: str, path: str) -> None:\n\t\n    url: str = \"/\".join(\n        (self._host, self._repository, self.RELEASE_PATH, self._release, name)\n    )\n    logger.info(f\"Downloading {name} from {url}\")\n    with httpx.stream(\"GET\", url) as response:\n        response.raise_for_status()\n        with tempfile.NamedTemporaryFile() as stream:\n            for chunk in response.iter_bytes():\n                stream.write(chunk)\n            stream.flush()\n            expected_checksum: str = self.checksum(name)\n            actual_checksum: str = compute_file_checksum(stream.name)\n            if expected_checksum!= actual_checksum:\n                raise RuntimeError(\n                    f\"Checksum mismatch for {name}. Expected {expected_checksum}, got {actual_checksum}\"\n                )\n            with zipfile.ZipFile(stream.name, \"r\") as archive:\n                archive.extractall(path)\n    logger.info(f\"Extracted {name} to {path}\")\n\n", "CodeBleu_score": 52.013950769453146}, {"code": "def download(self, name: str, path: str) -> None:\n\t\n    url: str = \"/\".join(\n        (\n            self._host,\n            self._repository,\n            self.RELEASE_PATH,\n            self._release,\n            name,\n        )\n    )\n    logger.info(f\"Downloading model {name} from {url}\")\n    response: httpx.Response = httpx.get(url, http2=True)\n    response.raise_for_status()\n    with tempfile.NamedTemporaryFile(\n        mode=\"wb\", suffix=\".tar.gz\", delete=False\n    ) as stream:\n        for chunk in response.iter_bytes():\n            stream.write(chunk)\n        stream.flush()\n        checksum: str = compute_file_checksum(stream.name)\n        if checksum!= self.checksum(name):\n            raise ValueError(f\"Checksum mismatch for model {name}\")\n        with tarfile.open(stream.name) as archive:\n            archive.extractall(path)\n        logger.info(f\"Extracted model {name} to {path}\")\n        os.remove(stream.name)\n", "CodeBleu_score": 47.484502073287516}, {"code": "def download(self, name: str, path: str) -> None:\n\t\n    url: str = \"/\".join(\n        (\n            self._host,\n            self._repository,\n            self.RELEASE_PATH,\n            self._release,\n            name,\n        )\n    )\n    logger.info(f\"Downloading {name} from {url}\")\n    with httpx.stream(\n        \"GET\",\n        url,\n        headers={\"Accept-Encoding\": \"gzip\"},\n        follow_redirects=True,\n    ) as stream:\n        stream.raise_for_status()\n        with tempfile.NamedTemporaryFile() as file:\n            for chunk in stream.iter_bytes():\n                file.write(chunk)\n            file.flush()\n            checksum: str = self.compute_file_checksum(file.name)\n            expected_checksum: str = self.checksum(name)\n            if checksum!= expected_checksum:\n                raise ValueError(\n                    f\"Checksum mismatch for {name}: {checksum}!= {expected_checksum}\"\n                )\n            with tarfile.open(file.name) as archive:\n                archive.extractall(path)\n            logger.info(f\"Extracted {name} to {path}\")\n\n", "CodeBleu_score": 46.78863979861067}]}
{"instance_id": "ee9493f37d77", "generation_result": [{"code": "def load_configuration(descriptor: str) -> Dict\n\t\n    if descriptor.startswith('embedded:'):\n        name = descriptor.split(':')[1]\n        resource = pkg_resources.resource_string(__name__, 'config/{}.json'.format(name))\n        return json.loads(resource)\n    else:\n        if not os.path.isfile(descriptor):\n            raise Exception('The configuration file {} does not exist'.format(descriptor))\n        with open(descriptor, 'r') as f:\n            return json.load(f)\n\n", "CodeBleu_score": 39.19993000523922}, {"code": "def load_configuration(descriptor: str) -> Dict\n\t\n    if descriptor.startswith(\"embedded:\"):\n        name = descriptor.split(\":\")[1]\n        resource_path = f\"configurations/{name}.json\"\n        try:\n            with open(resource_path) as resource:\n                return json.load(resource)\n        except FileNotFoundError:\n            raise ValueError(f\"Embedded configuration {name} not found.\")\n    elif os.path.isfile(descriptor):\n        try:\n            with open(descriptor) as file:\n                return json.load(file)\n        except FileNotFoundError:\n            raise ValueError(f\"Configuration file {descriptor} not found.\")\n    else:\n        raise ValueError(f\"Invalid configuration descriptor {descriptor}.\")\n\n", "CodeBleu_score": 46.152544817118844}, {"code": "def load_configuration(descriptor: str) -> Dict\n\t\n    if descriptor.startswith(\"embedded:\"):\n        name = descriptor.replace(\"embedded:\", \"\")\n        try:\n            with open(f\"./embedded_configs/{name}.json\") as f:\n                return json.load(f)\n        except FileNotFoundError:\n            raise ValueError(f\"Embedded configuration '{name}' not found.\")\n    elif os.path.isfile(descriptor):\n        try:\n            with open(descriptor) as f:\n                return json.load(f)\n        except FileNotFoundError:\n            raise ValueError(f\"Configuration file '{descriptor}' not found.\")\n    else:\n        raise ValueError(f\"Invalid configuration descriptor '{descriptor}'.\")\n\n", "CodeBleu_score": 49.58808324587369}]}
{"instance_id": "546134b67fbc", "generation_result": [{"code": "def sync_apply(\n    tensor_dict: Dict[str, tf.Tensor], func: Callable, concat_axis: int = 1\n) -> Dict[str, tf.Tensor]\n\t\n    if concat_axis not in [0, 1]:\n        raise ValueError(\n            f\"concat_axis must be either 0 or 1, not {concat_axis}.\"\n        )\n    tensor_list = []\n    for _, tensor in tensor_dict.items():\n        tensor_list.append(tensor)\n    tensor_concat = tf.concat(tensor_list, axis=concat_axis)\n    processed_tensor = func(tensor_concat)\n    processed_tensor_dict = {}\n    start = 0\n    for key, tensor in tensor_dict.items():\n        if concat_axis == 0:\n            end = start + tensor.shape[1]\n        else:\n            end = start + tensor.shape[0]\n        processed_tensor_dict[key] = processed_tensor[start:end]\n        start = end\n    return processed_tensor_dict\n\n", "CodeBleu_score": 42.48278846181079}, {"code": "def sync_apply(\n    tensor_dict: Dict[str, tf.Tensor], func: Callable, concat_axis: int = 1\n) -> Dict[str, tf.Tensor]\n\t\n    if concat_axis not in [0, 1]:\n        raise ValueError(\n            \"The concatenation axis must be either 0 or 1, but got {}.\".format(concat_axis)\n        )\n    if concat_axis == 0:\n        concat_axis = -1\n    tensor_list = [v for _, v in tensor_dict.items()]\n    concat_tensor = tf.concat(tensor_list, axis=concat_axis)\n    processed_tensor = func(concat_tensor)\n    processed_tensor_list = tf.split(processed_tensor, len(tensor_list), axis=concat_axis)\n    processed_tensor_dict = {\n        k: v for k, v in zip(tensor_dict.keys(), processed_tensor_list)\n    }\n    return processed_tensor_dict\n\n", "CodeBleu_score": 42.578288532585184}, {"code": "def sync_apply(\n    tensor_dict: Dict[str, tf.Tensor], func: Callable, concat_axis: int = 1\n) -> Dict[str, tf.Tensor]\n\t\n    if concat_axis not in [0, 1]:\n        raise ValueError(\"concat_axis must be either 0 or 1\")\n    tensor_list = []\n    for key, tensor in tensor_dict.items():\n        tensor_list.append(tensor)\n    tensor_concat = tf.concat(tensor_list, axis=concat_axis)\n    tensor_concat = func(tensor_concat)\n    tensor_dict = {}\n    for i, key in enumerate(tensor_dict.keys()):\n        tensor_dict[key] = tensor_concat[i]\n    return tensor_dict\n\n", "CodeBleu_score": 35.59373109870676}]}
{"instance_id": "df48e78d46d2", "generation_result": [{"code": "def from_float32_to_uint8(\n    tensor: tf.Tensor,\n    tensor_key: str = \"tensor\",\n    min_key: str = \"min\",\n    max_key: str = \"max\",\n) -> tf.Tensor\n\t\n    min_value = tf.math.reduce_min(tensor)\n    max_value = tf.math.reduce_max(tensor)\n    tensor = tf.cast(\n        tf.math.round((tensor - min_value) * 255 / (max_value - min_value)), tf.uint8\n    )\n    return {\n        tensor_key: tensor,\n        min_key: min_value,\n        max_key: max_value,\n    }\n\n", "CodeBleu_score": 44.60046567995533}, {"code": "def from_float32_to_uint8(\n    tensor: tf.Tensor,\n    tensor_key: str = \"tensor\",\n    min_key: str = \"min\",\n    max_key: str = \"max\",\n) -> tf.Tensor\n\t\n    min_value = tf.math.reduce_min(tensor)\n    max_value = tf.math.reduce_max(tensor)\n    tensor = tf.cast(\n        (tensor - min_value) * (255 / (max_value - min_value)), tf.uint8\n    )\n    return {\n        tensor_key: tensor,\n        min_key: min_value,\n        max_key: max_value,\n    }\n\n", "CodeBleu_score": 44.93621377433592}, {"code": "def from_float32_to_uint8(\n    tensor: tf.Tensor,\n    tensor_key: str = \"tensor\",\n    min_key: str = \"min\",\n    max_key: str = \"max\",\n) -> tf.Tensor\n\t\n    # Get the minimum and maximum values of the tensor\n    min_value = tf.reduce_min(tensor)\n    max_value = tf.reduce_max(tensor)\n\n    # Scale the tensor values to the uint8 range (0-255)\n    scaled_tensor = tf.cast(\n        tf.clip_by_value(\n            (tensor - min_value) / (max_value - min_value), 0.0, 1.0\n        )\n        * 255,\n        tf.uint8,\n    )\n\n    # Return a dictionary containing the scaled uint8 tensor, the original minimum value, and the original maximum value\n    return {\n        tensor_key: scaled_tensor,\n        min_key: min_value,\n        max_key: max_value,\n    }\n\n", "CodeBleu_score": 43.32389689957097}]}
{"instance_id": "7f479e5de492", "generation_result": [{"code": "def pad_and_partition(tensor: tf.Tensor, segment_len: int) -> tf.Tensor\n\t\n    tensor_len = tf.shape(tensor)[0]\n    padding_len = segment_len - (tensor_len % segment_len)\n    padding = tf.zeros([padding_len, tensor.shape[1]], dtype=tensor.dtype)\n    padded_tensor = tf.concat([tensor, padding], axis=0)\n    num_segments = tf.shape(padded_tensor)[0] // segment_len\n    reshaped_tensor = tf.reshape(padded_tensor, [num_segments, segment_len, tensor.shape[1]])\n    return reshaped_tensor\n\n", "CodeBleu_score": 30.54622810068086}, {"code": "def pad_and_partition(tensor: tf.Tensor, segment_len: int) -> tf.Tensor\n\t\n    tensor_len = tf.shape(tensor)[0]\n    pad_len = segment_len - tf.math.floormod(tensor_len, segment_len)\n    pad_tensor = tf.pad(tensor, [[0, pad_len], [0, 0]])\n    return tf.reshape(pad_tensor, [-1, segment_len, tf.shape(tensor)[1]])\n\n", "CodeBleu_score": 26.691546722994612}, {"code": "def pad_and_partition(tensor: tf.Tensor, segment_len: int) -> tf.Tensor\n\t\n    tensor_len = tf.shape(tensor)[0]\n    padding_needed = segment_len - (tensor_len % segment_len)\n    padding = tf.constant([[0, padding_needed], [0, 0]])\n    padded_tensor = tf.pad(tensor, padding)\n    segments = tf.reshape(padded_tensor, [-1, segment_len, tensor.shape[1]])\n    return segments\n\n", "CodeBleu_score": 26.487031704631043}]}
{"instance_id": "1789fae7253f", "generation_result": [{"code": "def pad_and_reshape(instr_spec, frame_length, F) -> Any\n\t\n    # Calculate the number of extra rows needed for padding\n    num_extra_rows = frame_length - (instr_spec.shape[1] % frame_length)\n    # Create an extension of zeros\n    extra_rows = torch.zeros((instr_spec.shape[0], num_extra_rows, instr_spec.shape[2]))\n    # Concatenate the extension to the original spectrogram along the frequency axis\n    instr_spec = torch.cat((instr_spec, extra_rows), dim=1)\n    # Reshape the extended spectrogram by merging the first two dimensions\n    instr_spec = instr_spec.reshape(-1, instr_spec.shape[2])\n    return instr_spec\n\n", "CodeBleu_score": 15.581735318030654}, {"code": "def pad_and_reshape(instr_spec, frame_length, F) -> Any\n\t\n    # Calculate the number of extra rows needed for padding\n    extra_rows = (frame_length - F.shape[0] % frame_length) % frame_length\n    # Create an extension of zeros\n    extension = torch.zeros(extra_rows, F.shape[1], dtype=F.dtype, device=F.device)\n    # Concatenate the extension to the original spectrogram along the frequency axis\n    F = torch.cat([F, extension], dim=0)\n    # Reshape the extended spectrogram by merging the first two dimensions\n    F = F.reshape(-1, frame_length, F.shape[1])\n    return F\n\n", "CodeBleu_score": 14.150143201095858}, {"code": "def pad_and_reshape(instr_spec, frame_length, F) -> Any\n\t\n    # Get the number of extra rows needed for padding\n    num_extra_rows = frame_length - (instr_spec.shape[1] % frame_length)\n\n    # Create an extension of zeros\n    extra_rows = torch.zeros(instr_spec.shape[0], num_extra_rows, F)\n\n    # Concatenate the extension to the original spectrogram along the frequency axis\n    instr_spec = torch.cat((instr_spec, extra_rows), dim=1)\n\n    # Reshape the extended spectrogram by merging the first two dimensions\n    instr_spec = instr_spec.reshape(-1, instr_spec.shape[2])\n\n    return instr_spec\n\n", "CodeBleu_score": 14.76737881405906}]}
{"instance_id": "8a4f4658d97c", "generation_result": [{"code": "def dataset_from_csv(csv_path: str, **kwargs) -> Any\n\t\n    import pandas as pd\n    import tensorflow as tf\n\n    df = pd.read_csv(csv_path, **kwargs)\n    return tf.data.Dataset.from_tensor_slices(dict(df))\n\n", "CodeBleu_score": 31.40320167404485}, {"code": "def dataset_from_csv(csv_path: str, **kwargs) -> Any\n\t\n    # Read the CSV file into a pandas DataFrame\n    df = pd.read_csv(csv_path, **kwargs)\n\n    # Convert the DataFrame columns into a TensorFlow dataset\n    dataset = tf.data.Dataset.from_tensor_slices(df)\n\n    return dataset\n\n", "CodeBleu_score": 37.1683611224408}, {"code": "def dataset_from_csv(csv_path: str, **kwargs) -> Any\n\t\n    df = pd.read_csv(csv_path)\n    dataset = tf.data.Dataset.from_tensor_slices(dict(df))\n    return dataset\n\n", "CodeBleu_score": 24.4711468031792}]}
{"instance_id": "9a16e81b182f", "generation_result": [{"code": "def check_tensor_shape(tensor_tf: tf.Tensor, target_shape: Any) -> bool\n\t\n    result = True\n    for i, dim in enumerate(target_shape):\n        if dim is not None:\n            result = result and (tensor_tf.shape[i] == dim)\n    return result\n\n", "CodeBleu_score": 31.182997668408653}, {"code": "def check_tensor_shape(tensor_tf: tf.Tensor, target_shape: Any) -> bool\n\t\n    result = True\n    for i, dim in enumerate(target_shape):\n        if dim is not None:\n            result = result and tf.equal(tf.shape(tensor_tf)[i], dim)\n    return result\n\n", "CodeBleu_score": 37.25007750647915}, {"code": "def check_tensor_shape(tensor_tf: tf.Tensor, target_shape: Any) -> bool\n\t\n    result = True\n    for dim, length in enumerate(target_shape):\n        if length is not None:\n            result = result and (tensor_tf.shape[dim] == length)\n    return result\n\n", "CodeBleu_score": 32.61913248656539}]}
{"instance_id": "49d5d3084b3a", "generation_result": [{"code": "def get_training_dataset(\n    audio_params: Dict, audio_adapter: AudioAdapter, audio_path: str\n) -> Any\n\tchunk_duration = audio_params[\"chunk_duration\"]\n    random_seed = audio_params[\"random_seed\"]\n    builder = DatasetBuilder(\n        audio_adapter,\n        chunk_duration=chunk_duration,\n        random_seed=random_seed,\n    )\n    train_csv = os.path.join(audio_path, \"train.csv\")\n    cache_directory = os.path.join(audio_path, \"cache\")\n    dataset = builder.build(\n        train_csv,\n        batch_size=audio_params[\"batch_size\"],\n        n_chunks_per_song=audio_params[\"n_chunks_per_song\"],\n        random_data_augmentation=audio_params[\"random_data_augmentation\"],\n        cache_directory=cache_directory,\n        wait_for_cache=audio_params[\"wait_for_cache\"],\n    )\n    return dataset\n\n", "CodeBleu_score": 43.09309876508208}, {"code": "def get_training_dataset(\n    audio_params: Dict, audio_adapter: AudioAdapter, audio_path: str\n) -> Any\n\tchunk_duration = audio_params[\"chunk_duration\"]\n    random_seed = audio_params[\"random_seed\"]\n    builder = DatasetBuilder(\n        audio_adapter=audio_adapter,\n        chunk_duration=chunk_duration,\n        random_seed=random_seed,\n    )\n    train_csv_path = os.path.join(audio_path, \"train.csv\")\n    cache_directory = os.path.join(audio_path, \"cache\")\n    batch_size = audio_params[\"batch_size\"]\n    n_chunks_per_song = audio_params[\"n_chunks_per_song\"]\n    random_data_augmentation = audio_params[\"random_data_augmentation\"]\n    random_time_crop = audio_params[\"random_time_crop\"]\n    dataset = builder.build(\n        csv_path=train_csv_path,\n        batch_size=batch_size,\n        shuffle=True,\n        convert_to_uint=True,\n        random_data_augmentation=random_data_augmentation,\n        random_time_crop=random_time_crop,\n        infinite_generator=True,\n        cache_directory=cache_directory,\n        wait_for_cache=False,\n        num_parallel_calls=4,\n        n_chunks_per_song=n_chunks_per_song,\n    )\n    return dataset\n\n", "CodeBleu_score": 36.34221688012238}, {"code": "def get_training_dataset(\n    audio_params: Dict, audio_adapter: AudioAdapter, audio_path: str\n) -> Any\n\t# Build training dataset\n    chunk_duration = audio_params[\"chunk_duration\"]\n    random_seed = audio_params[\"random_seed\"]\n    dataset_builder = DatasetBuilder(\n        audio_adapter, chunk_duration, random_seed=random_seed\n    )\n    train_dataset = dataset_builder.build(\n        csv_path=audio_path + \"train_dataset.csv\",\n        batch_size=audio_params[\"batch_size\"],\n        shuffle=True,\n        random_data_augmentation=True,\n        random_time_crop=True,\n        cache_directory=audio_path + \"cache/train/\",\n        n_chunks_per_song=audio_params[\"n_chunks_per_song\"],\n    )\n    return train_dataset\n\n", "CodeBleu_score": 40.64355783180067}]}
{"instance_id": "fe8bbbba06e3", "generation_result": [{"code": "def instruments(self) -> Any\n\t\n    if not self._instrument_builders:\n        for instrument in self._instruments:\n            self._instrument_builders.append(InstrumentDatasetBuilder(instrument))\n    for builder in self._instrument_builders:\n        yield builder\n\n", "CodeBleu_score": 44.01804900829681}, {"code": "def instruments(self) -> Any\n\t\n    if not self._instrument_builders:\n        for instrument in self._instruments:\n            self._instrument_builders.append(InstrumentDatasetBuilder(instrument))\n    for builder in self._instrument_builders:\n        yield builder\n\n", "CodeBleu_score": 44.01804900829681}, {"code": "def instruments(self) -> Any\n\t\n    if not self._instrument_builders:\n        for instrument in self._instruments:\n            self._instrument_builders.append(InstrumentDatasetBuilder(instrument))\n    for instrument_builder in self._instrument_builders:\n        yield instrument_builder\n\n", "CodeBleu_score": 40.92387456420741}]}
{"instance_id": "438b57fd3413", "generation_result": [{"code": "def cache(self, dataset: Any, cache: Optional[str], wait: bool) -> Any\n\t\n    if cache is not None:\n        if wait:\n            while not exists(f\"{cache}.index\"):\n                logger.info(f\"Cache not available, wait {self.WAIT_PERIOD}\")\n                time.sleep(self.WAIT_PERIOD)\n        cache_path = os.path.split(cache)[0]\n        os.makedirs(cache_path, exist_ok=True)\n        return dataset.cache(cache)\n    return dataset\n\n", "CodeBleu_score": 93.65648279369884}, {"code": "def cache(self, dataset: Any, cache: Optional[str], wait: bool) -> Any\n\t\n    if cache is not None:\n        if wait:\n            while not exists(f\"{cache}.index\"):\n                logger.info(f\"Cache not available, wait {self.WAIT_PERIOD}\")\n                time.sleep(self.WAIT_PERIOD)\n        cache_path = os.path.split(cache)[0]\n        os.makedirs(cache_path, exist_ok=True)\n        return dataset.cache(cache)\n    return dataset\n\n", "CodeBleu_score": 93.65648279369884}, {"code": "def cache(self, dataset: Any, cache: Optional[str], wait: bool) -> Any\n\t\n    if cache is not None:\n        if wait:\n            while not exists(f\"{cache}.index\"):\n                logger.info(f\"Cache not available, wait {self.WAIT_PERIOD}\")\n                time.sleep(self.WAIT_PERIOD)\n        cache_path = os.path.split(cache)[0]\n        os.makedirs(cache_path, exist_ok=True)\n        return dataset.cache(cache)\n    return dataset", "CodeBleu_score": 93.65648279369884}]}
{"instance_id": "8838a3a09a66", "generation_result": [{"code": "def create_estimator(params: Dict, MWF: bool) -> tf.Tensor\n\t\n    model_provider = ModelProvider(params[\"model_name\"])\n    model_provider.update_parameters(params)\n    model_provider.set_model_dir(params[\"model_dir\"])\n    session_config = tf.ConfigProto()\n    session_config.gpu_options.allow_growth = True\n    run_config = tf.estimator.RunConfig(session_config=session_config)\n    estimator = tf.estimator.Estimator(\n        model_fn=model_provider.get_model_fn(),\n        model_dir=params[\"model_dir\"],\n        params=params,\n        config=run_config,\n    )\n    if MWF:\n        estimator = tf.contrib.estimator.add_metrics(\n            estimator, model_provider.get_metrics()\n        )\n    return estimator\n\n", "CodeBleu_score": 40.22721908495986}, {"code": "def create_estimator(params: Dict, MWF: bool) -> tf.Tensor\n\t\n    model_dir = params[\"model_dir\"]\n    model_provider = ModelProvider(params)\n    model_provider.update_parameters()\n    session_config = tf.ConfigProto()\n    session_config.gpu_options.allow_growth = True\n    session_config.allow_soft_placement = True\n    session_config.log_device_placement = False\n    run_config = tf.estimator.RunConfig(\n        model_dir=model_dir,\n        session_config=session_config,\n        save_checkpoints_steps=params[\"save_checkpoints_steps\"],\n        save_summary_steps=params[\"save_summary_steps\"],\n        keep_checkpoint_max=params[\"keep_checkpoint_max\"],\n        log_step_count_steps=params[\"log_step_count_steps\"],\n    )\n    estimator = tf.estimator.Estimator(\n        model_fn=model_fn,\n        model_dir=model_dir,\n        params=params,\n        config=run_config,\n    )\n    return estimator\n\n", "CodeBleu_score": 30.9855780006648}, {"code": "def create_estimator(params: Dict, MWF: bool) -> tf.Tensor\n\t\n    model_provider = ModelProvider(params[\"model_name\"])\n    model_provider.update_params(params)\n    session_config = tf.ConfigProto()\n    session_config.gpu_options.allow_growth = True\n    session_config.gpu_options.per_process_gpu_memory_fraction = 0.9\n    run_config = tf.estimator.RunConfig(session_config=session_config)\n    estimator = tf.estimator.Estimator(\n        model_fn=model_provider.model_fn,\n        model_dir=params[\"model_dir\"],\n        params=params,\n        config=run_config,\n    )\n    return estimator\n\n", "CodeBleu_score": 38.71480079809172}]}
{"instance_id": "779c812bf2cb", "generation_result": [{"code": "def _separate_tensorflow(\n        self, waveform: np.ndarray, audio_descriptor: AudioDescriptor\n    ) -> Dict\n\t\n    waveform = ensure_stereo(waveform)\n    self._data_generator.update_data(\n        {\"waveform\": waveform, \"audio_id\": audio_descriptor.audio_id}\n    )\n    prediction = next(self._get_prediction_generator())\n    prediction.pop(\"audio_id\")\n    return prediction\n", "CodeBleu_score": 43.253402205943445}, {"code": "def _separate_tensorflow(\n        self, waveform: np.ndarray, audio_descriptor: AudioDescriptor\n    ) -> Dict\n\t\n    assert waveform.ndim == 2, \"Waveform must be stereo\"\n\n    prediction = next(self._get_prediction_generator())\n    prediction.pop(\"audio_id\")\n    return prediction\n", "CodeBleu_score": 26.349516804310564}, {"code": "def _separate_tensorflow(\n        self, waveform: np.ndarray, audio_descriptor: AudioDescriptor\n    ) -> Dict\n\t\n    if waveform.ndim!= 2:\n        raise ValueError(\n            \"Waveform should be a 2-dimensional array, but got {}-dimensional array.\".format(\n                waveform.ndim\n            )\n        )\n\n    prediction = next(self._get_prediction_generator())\n    prediction.pop(\"audio_id\")\n    return prediction\n", "CodeBleu_score": 41.36885393499961}]}
{"instance_id": "cf580cd2ed93", "generation_result": [{"code": "def compute_backbone_shapes(config, image_shape)\n\t\n    if callable(config.BACKBONE):\n        return config.COMPUTE_BACKBONE_SHAPE(image_shape)\n    assert config.BACKBONE in [\"resnet50\", \"resnet101\"]\n    return np.array(\n        [\n            [\n                int(np.ceil(image_shape[0] / stride)),\n                int(np.ceil(image_shape[1] / stride)),\n            ]\n            for stride in config.BACKBONE_STRIDES\n        ]\n    )\n\n", "CodeBleu_score": 79.53916287162126}, {"code": "def compute_backbone_shapes(config, image_shape)\n\t\n    if callable(config.BACKBONE):\n        return config.COMPUTE_BACKBONE_SHAPE(image_shape)\n    assert config.BACKBONE in [\"resnet50\", \"resnet101\"]\n    return np.array(\n        [\n            tuple(int(math.ceil(image_shape[0] / stride)) for stride in config.BACKBONE_STRIDES),\n            tuple(int(math.ceil(image_shape[1] / stride)) for stride in config.BACKBONE_STRIDES),\n        ]\n    )\n\n", "CodeBleu_score": 76.22060222489522}, {"code": "def compute_backbone_shapes(config, image_shape)\n\t\n    if callable(config.BACKBONE):\n        return config.COMPUTE_BACKBONE_SHAPE(image_shape)\n\n    assert config.BACKBONE in [\"resnet50\", \"resnet101\"]\n    return np.array(\n        [\n            [\n                int(np.ceil(image_shape[0] / stride)),\n                int(np.ceil(image_shape[1] / stride)),\n            ]\n            for stride in config.BACKBONE_STRIDES\n        ]\n    )\n\n", "CodeBleu_score": 79.53916287162126}]}
{"instance_id": "ebd61a2e59ca", "generation_result": [{"code": "def clip_boxes_graph(boxes, window)\n\t\n    # Split the boxes into their x and y coordinates\n    x1, y1, x2, y2 = tf.split(boxes, 4, axis=1)\n    # Split the window into its x and y coordinates\n    wx1, wy1, wx2, wy2 = tf.split(window, 4, axis=1)\n    # Clip the x and y coordinates of the boxes to lie within the window\n    x1 = tf.maximum(tf.minimum(x1, wx2), wx1)\n    x2 = tf.maximum(tf.minimum(x2, wx2), wx1)\n    y1 = tf.maximum(tf.minimum(y1, wy2), wy1)\n    y2 = tf.maximum(tf.minimum(y2, wy2), wy1)\n    # Concatenate the clipped coordinates back into a tensor\n    clipped_boxes = tf.concat([x1, y1, x2, y2], axis=1)\n    # Reshape the clipped boxes tensor to match the original shape\n    clipped_boxes = tf.reshape(clipped_boxes, tf.shape(boxes))\n    return clipped_boxes\n\n", "CodeBleu_score": 35.70970683809846}, {"code": "def clip_boxes_graph(boxes, window)\n\t\n    # Split the box coordinates into individual components\n    x1, y1, x2, y2 = tf.split(boxes, 4, axis=1)\n    # Split the window coordinates into individual components\n    w1, h1, w2, h2 = tf.split(window, 4, axis=1)\n    # Clip the coordinates to lie within the window\n    x1 = tf.maximum(x1, w1)\n    y1 = tf.maximum(y1, h1)\n    x2 = tf.minimum(x2, w2)\n    y2 = tf.minimum(y2, h2)\n    # Concatenate the clipped coordinates back into a tensor\n    clipped_boxes = tf.concat([x1, y1, x2, y2], axis=1)\n    # Reshape the clipped coordinates to match the input shape\n    clipped_boxes = tf.reshape(clipped_boxes, tf.shape(boxes))\n    return clipped_boxes\n\n", "CodeBleu_score": 23.80211804348869}, {"code": "def clip_boxes_graph(boxes, window)\n\t\n    xmin, ymin, xmax, ymax = tf.unstack(boxes, axis=1)\n    xmin = tf.maximum(tf.minimum(xmin, window[2]), window[0])\n    xmax = tf.maximum(tf.minimum(xmax, window[3]), window[1])\n    ymin = tf.maximum(tf.minimum(ymin, window[2]), window[0])\n    ymax = tf.maximum(tf.minimum(ymax, window[3]), window[1])\n    clipped = tf.stack([xmin, ymin, xmax, ymax], axis=1)\n    return clipped\n\n", "CodeBleu_score": 16.899192874982276}]}
{"instance_id": "8e0411587a70", "generation_result": [{"code": "def rpn_graph(feature_map, anchors_per_location, anchor_stride)\n\t\n    # Get the number of anchors per location\n    num_anchors = len(anchors_per_location)\n\n    # Define a shared convolutional layer with 512 filters, kernel size 3, and stride 1\n    shared = KL.Conv2D(512, (3, 3), padding=\"same\", activation=\"relu\", strides=anchor_stride, name=\"rpn_conv_shared\")(feature_map)\n\n    # Define two separate convolutional layers for class scores and bounding box predictions\n    x = KL.Conv2D(2 * num_anchors, (1, 1), padding=\"valid\", activation=\"linear\", name=\"rpn_class_raw\")(shared)\n    rpn_class_logits = KL.Lambda(lambda t: tf.reshape(t, [tf.shape(t)[0], -1, 2]))(x)\n    rpn_probs = KL.Activation(\"softmax\", name=\"rpn_class_xxx\")(rpn_class_logits)\n\n    # Reshape the bounding box predictions\n    x = KL.Conv2D(4 * num_anchors, (1, 1), padding=\"valid\", activation=\"linear\", name=\"rpn_bbox_pred\")(shared)\n    rpn_bbox = KL.Lambda(lambda t: tf.reshape(t, [tf.shape(t)[0], -1, 4]))(x)\n\n    return [rpn_class_logits, rpn_probs, rpn_bbox]\n\n", "CodeBleu_score": 47.49209588408726}, {"code": "def rpn_graph(feature_map, anchors_per_location, anchor_stride)\n\t\n    # Get the number of anchor ratios\n    num_anchors = len(anchors_per_location)\n    # Define a shared convolutional layer with 512 filters, 3x3 kernel, and ReLU activation\n    shared = KL.Conv2D(512, (3, 3), padding=\"same\", activation=\"relu\", strides=anchor_stride)(feature_map)\n    # Define two separate convolutional layers for class scores and bounding box predictions\n    x = KL.Conv2D(2 * num_anchors, (1, 1), padding=\"valid\", activation=\"linear\", strides=(1, 1))(shared)\n    rpn_class_logits = KL.Lambda(lambda t: tf.reshape(t, [tf.shape(t)[0], -1, 2]))(x)\n    rpn_probs = KL.Activation(\"softmax\", name=\"rpn_class\")(rpn_class_logits)\n    x = KL.Conv2D(4 * num_anchors, (1, 1), padding=\"valid\", activation=\"linear\", strides=(1, 1))(shared)\n    rpn_bbox = KL.Lambda(lambda t: tf.reshape(t, [tf.shape(t)[0], -1, 4]))(x)\n    return [rpn_class_logits, rpn_probs, rpn_bbox]\n\n", "CodeBleu_score": 46.55414607421687}, {"code": "def rpn_graph(feature_map, anchors_per_location, anchor_stride)\n\t\n    # Get the number of anchor ratios\n    num_anchors = len(anchors_per_location)\n\n    # Define a shared convolutional layer with 512 filters of size 3x3 and ReLU activation\n    shared = KL.Conv2D(512, (3, 3), padding=\"same\", activation=\"relu\", strides=anchor_stride, name=\"rpn_conv_shared\")(feature_map)\n\n    # Define two separate convolutional layers for class scores and bounding box predictions\n    x = KL.Conv2D(2 * num_anchors, (1, 1), padding=\"valid\", activation=\"linear\", name=\"rpn_class_raw\")(shared)\n    rpn_class_logits = KL.Lambda(lambda t: tf.reshape(t, [tf.shape(t)[0], -1, 2]))(x)\n    rpn_probs = KL.Activation(\"softmax\", name=\"rpn_class_xxx\")(rpn_class_logits)\n\n    # Reshape the bounding box predictions\n    x = KL.Conv2D(4 * num_anchors, (1, 1), padding=\"valid\", activation=\"linear\", name=\"rpn_bbox_pred\")(shared)\n    rpn_bbox = KL.Lambda(lambda t: tf.reshape(t, [tf.shape(t)[0], -1, 4]))(x)\n\n    return [rpn_class_logits, rpn_probs, rpn_bbox]\n\n", "CodeBleu_score": 47.49209588408726}]}
{"instance_id": "70777c9261d8", "generation_result": [{"code": "def smooth_l1_loss(y_true, y_pred)\n\t\n    diff = K.abs(y_true - y_pred)\n    less_than_one = K.cast(K.less(diff, 1.0), \"float32\")\n    loss = (less_than_one * 0.5 * diff**2) + (1 - less_than_one) * (diff - 0.5)\n    return loss\n\n", "CodeBleu_score": 60.928513203413225}, {"code": "def smooth_l1_loss(y_true, y_pred)\n\t\n    diff = K.abs(y_true - y_pred)\n    less_than_one = K.cast(K.less(diff, 1.0), \"float32\")\n    loss = (less_than_one * 0.5 * diff**2) + (1 - less_than_one) * (diff - 0.5)\n    return loss\n\n", "CodeBleu_score": 60.928513203413225}, {"code": "def smooth_l1_loss(y_true, y_pred)\n\t\n    diff = K.abs(y_true - y_pred)\n    less_than_one = K.cast(K.less(diff, 1.0), \"float32\")\n    loss = (less_than_one * 0.5 * diff**2) + (1 - less_than_one) * (diff - 0.5)\n    return loss\n\n", "CodeBleu_score": 60.928513203413225}]}
{"instance_id": "0e747b604087", "generation_result": [{"code": "def build_model(x_train, num_classes)\n\t\n    model = keras.Sequential()\n    model.add(layers.Conv2D(32, (3, 3), activation=\"relu\", input_shape=x_train.shape[1:]))\n    model.add(layers.Conv2D(32, (3, 3), activation=\"relu\"))\n    model.add(layers.MaxPooling2D((2, 2)))\n    model.add(layers.Flatten())\n    model.add(layers.Dense(128, activation=\"relu\"))\n    model.add(layers.Dense(num_classes, activation=\"softmax\"))\n    model.compile(\n        loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n    )\n    return model\n\n", "CodeBleu_score": 22.49917142836048}, {"code": "def build_model(x_train, num_classes)\n\t\n    # Reset the graph\n    tf.keras.backend.clear_session()\n    # Define the input layer\n    inputs = tf.keras.layers.Input(shape=x_train.shape[1:])\n    # Convolutional layers\n    x = tf.keras.layers.Conv2D(32, (3, 3), activation=\"relu\")(inputs)\n    x = tf.keras.layers.Conv2D(32, (3, 3), activation=\"relu\")(x)\n    x = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\n    # Flatten and dense layers\n    x = tf.keras.layers.Flatten()(x)\n    x = tf.keras.layers.Dense(64, activation=\"relu\")(x)\n    outputs = tf.keras.layers.Dense(num_classes, activation=\"softmax\")(x)\n    # Define the model\n    model = tf.keras.models.Model(inputs=inputs, outputs=outputs)\n    # Compile the model\n    model.compile(\n        optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n    )\n    return model\n\n", "CodeBleu_score": 27.581519652892318}, {"code": "def build_model(x_train, num_classes)\n\t\n    model = tf.keras.models.Sequential()\n    model.add(\n        tf.keras.layers.Conv2D(\n            32, (3, 3), activation=\"relu\", input_shape=x_train.shape[1:]\n        )\n    )\n    model.add(tf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\"))\n    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n    model.add(tf.keras.layers.Flatten())\n    model.add(tf.keras.layers.Dense(128, activation=\"relu\"))\n    model.add(tf.keras.layers.Dense(num_classes, activation=\"softmax\"))\n    model.compile(\n        loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n    )\n    return model\n\n", "CodeBleu_score": 25.148890336161443}]}
{"instance_id": "813e3e2844f4", "generation_result": [{"code": "def extract_bboxes(mask)\n\t\n    boxes = []\n    for i in range(mask.shape[-1]):\n        m = mask[:, :, i]\n        horizontal_indicies = np.where(np.any(m, axis=0))[0]\n        vertical_indicies = np.where(np.any(m, axis=1))[0]\n        if horizontal_indicies.shape[0]:\n            x1, x2 = horizontal_indicies[[0, -1]]\n            y1, y2 = vertical_indicies[[0, -1]]\n            x2 += 1\n            y2 += 1\n        else:\n            x1, x2, y1, y2 = 0, 0, 0, 0\n        boxes.append([y1, x1, y2, x2])\n    return np.array(boxes)\n\n", "CodeBleu_score": 69.04407594280562}, {"code": "def extract_bboxes(mask)\n\t\n    boxes = np.zeros([mask.shape[-1], 4], dtype=np.int32)\n    for i in range(mask.shape[-1]):\n        m = mask[:, :, i]\n        # Bounding box.\n        horizontal_indicies = np.where(np.any(m, axis=0))[0]\n        vertical_indicies = np.where(np.any(m, axis=1))[0]\n        if horizontal_indicies.shape[0]:\n            x1, x2 = horizontal_indicies[[0, -1]]\n            y1, y2 = vertical_indicies[[0, -1]]\n            # x2 and y2 should not be part of the box. Increment by 1.\n            x2 += 1\n            y2 += 1\n        else:\n            # No mask for this instance. Might happen due to\n            # resizing or cropping. Set bbox to zeros\n            x1, x2, y1, y2 = 0, 0, 0, 0\n        boxes[i] = np.array([y1, x1, y2, x2])\n    return boxes.astype(np.int32)\n\n", "CodeBleu_score": 80.52847651211997}, {"code": "def extract_bboxes(mask)\n\t\n    boxes = np.zeros([mask.shape[-1], 4], dtype=np.int32)\n    for i in range(mask.shape[-1]):\n        m = mask[:, :, i]\n        # Bounding box.\n        horizontal_indicies = np.where(np.any(m, axis=0))[0]\n        vertical_indicies = np.where(np.any(m, axis=1))[0]\n        if horizontal_indicies.shape[0]:\n            x1, x2 = horizontal_indicies[[0, -1]]\n            y1, y2 = vertical_indicies[[0, -1]]\n            # x2 and y2 should not be part of the box. Increment by 1.\n            x2 += 1\n            y2 += 1\n        else:\n            # No mask for this instance. Might happen due to\n            # resizing or cropping. Set bbox to zeros\n            x1, x2, y1, y2 = 0, 0, 0, 0\n        boxes[i] = np.array([y1, x1, y2, x2])\n    return boxes.astype(np.int32)\n\n", "CodeBleu_score": 80.52847651211997}]}
{"instance_id": "7ad5d430d6ed", "generation_result": [{"code": "def compute_iou(box, boxes, box_area, boxes_area)\n\t\n    # Calculate intersection areas\n    x1 = np.maximum(box[0], boxes[:, 0])\n    y1 = np.maximum(box[1], boxes[:, 1])\n    x2 = np.minimum(box[2], boxes[:, 2])\n    y2 = np.minimum(box[3], boxes[:, 3])\n    intersection = np.maximum(x2 - x1, 0) * np.maximum(y2 - y1, 0)\n    union = box_area + boxes_area[:] - intersection[:]\n    iou = intersection / union\n    return iou\n\n", "CodeBleu_score": 55.49021365037661}, {"code": "def compute_iou(box, boxes, box_area, boxes_area)\n\t\n    # 1. Calculate intersection areas\n    y1 = np.maximum(box[0], boxes[:, 0])\n    y2 = np.minimum(box[2], boxes[:, 2])\n    x1 = np.maximum(box[1], boxes[:, 1])\n    x2 = np.minimum(box[3], boxes[:, 3])\n    intersection = np.maximum(x2 - x1, 0) * np.maximum(y2 - y1, 0)\n    # 2. Calculate union areas\n    union = box_area + boxes_area[:] - intersection[:]\n    # 3. Compute IoU\n    iou = intersection / union\n    return iou\n\n", "CodeBleu_score": 58.663259083089194}, {"code": "def compute_iou(box, boxes, box_area, boxes_area)\n\t\n    # Calculate the intersection areas\n    y1 = np.maximum(box[0], boxes[:, 0])\n    y2 = np.minimum(box[2], boxes[:, 2])\n    x1 = np.maximum(box[1], boxes[:, 1])\n    x2 = np.minimum(box[3], boxes[:, 3])\n    intersection = np.maximum(x2 - x1, 0) * np.maximum(y2 - y1, 0)\n    union = box_area + boxes_area[:] - intersection[:]\n    iou = intersection / union\n    return iou\n\n", "CodeBleu_score": 58.663259083089194}]}
{"instance_id": "15941b852859", "generation_result": [{"code": "def compute_overlaps(boxes1, boxes2)\n\t\n    # Areas of anchors and GT boxes\n    area1 = (boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1])\n    area2 = (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1])\n\n    # Compute overlaps to generate matrix [boxes1 count, boxes2 count]\n    # Each cell contains the IoU value.\n    overlaps = np.zeros((boxes1.shape[0], boxes2.shape[0]))\n    for i in range(overlaps.shape[1]):\n        box2 = boxes2[i]\n        overlaps[:, i] = compute_iou(box2, boxes1, area2[i], area1)\n    return overlaps\n\n", "CodeBleu_score": 62.2853282032944}, {"code": "def compute_overlaps(boxes1, boxes2)\n\t\n    # Areas of anchors and GT boxes\n    area1 = (boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1])\n    area2 = (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1])\n    # Compute overlaps to generate matrix [boxes1 count, boxes2 count]\n    # Each cell contains the IoU value.\n    overlaps = np.zeros((boxes1.shape[0], boxes2.shape[0]))\n    for i in range(overlaps.shape[1]):\n        box2 = boxes2[i]\n        overlaps[:, i] = compute_iou(box2, boxes1, area2[i], area1)\n    return overlaps\n\n", "CodeBleu_score": 62.2853282032944}, {"code": "def compute_overlaps(boxes1, boxes2)\n\t\n    # Areas of anchors and GT boxes\n    area1 = (boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1])\n    area2 = (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1])\n\n    # Compute overlaps to generate matrix [boxes1 count, boxes2 count]\n    # Each cell contains the IoU value.\n    overlaps = np.zeros((boxes1.shape[0], boxes2.shape[0]))\n    for i in range(overlaps.shape[1]):\n        box2 = boxes2[i]\n        overlaps[:, i] = compute_iou(box2, boxes1, area2[i], area1)\n    return overlaps\n\n", "CodeBleu_score": 62.2853282032944}]}
{"instance_id": "597533722ce3", "generation_result": [{"code": "def load_image(self, image_id):\n\t\n    info = self.image_info[image_id]\n    image = cv2.imread(info[\"image_path\"])\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    if image.shape[2] == 4:\n        image = image[..., :3]\n    return image\n\n", "CodeBleu_score": 59.77126361333836}, {"code": "def load_image(self, image_id):\n\t\n    image_info = self.image_info[image_id]\n    image_path = image_info['path']\n    image = cv2.imread(image_path)\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    if image.shape[2] == 4:\n        image = image[..., :3]\n    return image\n\n", "CodeBleu_score": 57.08802030753202}, {"code": "def load_image(self, image_id):\n\t\n    info = self.image_info[image_id]\n    path = info[\"path\"]\n    image = cv2.imread(path)\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    if image.shape[2] == 4:\n        image = image[..., :3]\n    return image\n\n", "CodeBleu_score": 57.24220927648083}]}
{"instance_id": "9b852997ca05", "generation_result": [{"code": "def display_images(images, titles=None, cols=4, cmap=None, norm=None,\n                   interpolation=None)\n\t\n    titles = titles or [\"\"] * len(images)\n    rows = math.ceil(len(images) / cols)\n    height_ratio = 1.2 * (rows/cols) * (0.5 if type(images[0]) is not np.ndarray else 1)\n    plt.figure(figsize=(11, 11 * height_ratio))\n    for i in range(len(images)):\n        plt.subplot(rows, cols, i + 1)\n        plt.grid(False)\n        plt.axis(\"off\")\n        plt.title(titles[i])\n        plt.imshow(images[i], cmap=cmap, norm=norm, interpolation=interpolation)\n    plt.tight_layout()\n    plt.show()\n\n", "CodeBleu_score": 35.80656975636637}, {"code": "def display_images(images, titles=None, cols=4, cmap=None, norm=None,\n                   interpolation=None)\n\t\n    titles = titles or [\"\"] * len(images)\n    rows = math.ceil(len(images) / cols)\n    height_ratio = 1.2 * (rows/cols) * (0.5 if type(images[0]) is not np.ndarray else 1)\n    plt.figure(figsize=(11, 11 * height_ratio))\n    for i in range(len(images)):\n        plt.subplot(rows, cols, i + 1)\n        plt.axis(\"off\")\n        plt.title(titles[i])\n        plt.imshow(images[i], cmap=cmap, norm=norm, interpolation=interpolation)\n    plt.tight_layout()\n    plt.show()\n\n", "CodeBleu_score": 36.292502565202426}, {"code": "def display_images(images, titles=None, cols=4, cmap=None, norm=None,\n                   interpolation=None)\n\t\n    titles = titles if titles is not None else [\"\"] * len(images)\n    rows = math.ceil(len(images) / cols)\n    height_ratio = 1.2 * (rows/cols) * (0.5 if type(images[0]) is not np.ndarray else 1)\n    plt.figure(figsize=(11, 11 * height_ratio))\n    for i, (image, title) in enumerate(zip(images, titles)):\n        plt.subplot(rows, cols, i + 1)\n        plt.imshow(image, cmap=cmap, norm=norm, interpolation=interpolation)\n        plt.title(title, fontsize=9)\n        plt.xticks([])\n        plt.yticks([])\n    plt.tight_layout(pad=0, h_pad=0, w_pad=0)\n    plt.show()\n\n", "CodeBleu_score": 45.81157703079005}]}
