{"instance_id": "1f9f4d85e825", "generation_result": [{"code": "def load_model_from_config(self, half_attention)\n\t\n\n    # Load model from cache if available and valid\n    model_cache_path = self.cache_dir / \"model\"\n    if model_cache_path.exists() and model_cache_path.is_file() and model_cache_path.stat().st_size > 0:\n        try:\n            model_state_dict = torch.load(model_cache_path)\n            model = instantiate_from_config(self.model_config, model_state_dict)\n            model.eval()\n            if half_attention:\n                model.half()\n            model.to(self.device)\n            model.load_state_dict(model_state_dict)\n            model.eval()\n            return {\"model\": model}\n        except:\n            pass\n\n    # Load model from path\n    model_path = self.model_path\n    if model_path.endswith(\".safetensors\"):\n        model = instantiate_from_config(self.model_config)\n        model.eval()\n        if half_attention:\n            model.half()\n        model.to(self.device)\n        model.load_state_dict(torch.load(model_path, map_location=self.device))\n        model.eval()\n        return {\"model\": model}\n    else:\n        model = torch.load(model_path, map_location=self.device)\n        model.eval()\n        if half_attention:\n            model.half()\n        model.to(self.device)\n        return {\"model\": model}\n\n", "CodeBleu_score": 33.08295436906011}, {"code": "def load_model_from_config(self, half_attention)\n\t\n    # Load model from cache if available and valid\n    if self.cache_dir is not None:\n        cache_path = os.path.join(self.cache_dir, self.name)\n        if os.path.exists(cache_path) and self.model_cache_valid(cache_path):\n            logger.info(f\"Loading model from cache at {cache_path}\")\n            model = torch.load(cache_path, map_location=self.device)\n            if self.half_attention and model.half() != half_attention:\n                logger.warning(f\"Warning: model was loaded from cache but half-attention setting is different ({model.half()} vs {half_attention}).\")\n            if self.memory_optimized_format and model.to(memory_format=torch.channels_last) != self.memory_optimized_format:\n                logger.warning(f\"Warning: model was loaded from cache but memory-optimized-format setting is different ({model.to(memory_format=torch.channels_last)} vs {self.memory_optimized_format}).\")\n            model.eval()\n            return model\n    # Load model from path\n    model_path = os.path.join(self.model_path, self.name)\n    logger.info(f\"Loading model from {model_path}\")\n    model = instantiate_from_config(self.config)\n    if self.half_attention and model.half() != half_attention:\n        logger.warning(f\"Warning: model was loaded from path but half-attention setting is different ({model.half()} vs {half_attention}).\")\n    if self.memory_optimized_format and model.to(memory_format=torch.channels_last) != self.memory_optimized_format:\n        logger.warning(f\"Warning: model was loaded from path but memory-optimized-format setting is different ({model.to(memory_format=torch.channels_last)} vs {self.memory_optimized_format}).\")\n    model.load_state_dict(load_file(model_path), strict=False)\n    model.to(self.device)\n    model.eval", "CodeBleu_score": 31.40950245066264}, {"code": "def load_model_from_config(self, half_attention)\n\t\n    config_file = self.config_file\n    config_cache_file = self.config_cache_file\n    model_name = self.model_name\n    model_type = self.model_type\n    cache_dir = self.cache_dir\n    model_path = self.model_path\n    use_fast_tokenizer = self.use_fast_tokenizer\n    from_tf = self.from_tf\n    cache_model_type = self.cache_model_type\n    cache_model_name = self.cache_model_name\n    cache_dir = self.cache_dir\n    from_pt = self.from_pt\n    half_attention = half_attention\n    verbose = self.verbose\n    device = self.device\n    version = self.version\n    use_auth_token = self.use_auth_token\n    from_pipeline = self.from_pipeline\n    from_transformers = self.from_transformers\n    from_flax = self.from_flax\n    revision = self.revision\n    mirror = self.mirror\n    use_cdn = self.use_cdn\n    force_download = self.force_download\n    resume_download = self.resume_download\n    proxies = self.proxies\n    use_auth_token = self.use_auth_token\n    local_files_only = self.local_files_only\n    use_legacy_model_type = self.use_legacy_model_type\n    from_flax = self.from_flax\n    from_pt = self.from_pt\n    from_tf = self.from_tf\n    from_pipeline = self.from_pipeline\n    from_transformers = self.from_transformers\n    from_flax = self.from_flax\n    from_pt = self.from_pt\n    from_tf = self.from_tf\n    from_pipeline = self.from_pipeline\n    from_transformers = self.from_transformers\n    from_flax = self.from_flax\n    from_pt = self.from_pt\n    from_tf = self.from_tf\n    from_pip", "CodeBleu_score": 11.663686135740086}]}
{"instance_id": "8dc955007aa2", "generation_result": [{"code": "def convsample_ddim(model, cond, steps, shape, eta=1.0, callback=None, normals_sequence=None,\n                    mask=None, x0=None, quantize_x0=False, temperature=1., score_corrector=None,\n                    corrector_kwargs=None, x_t=None\n                    )\n\t\n\n    # Get the batch size from the shape parameter\n    batch_size = shape[0]\n\n    # Print the eta and steps values\n    print(f'Sampling with eta = {eta} and {steps} steps...')\n\n    # Initialize the sampler\n    sampler = DDIMSampler(model, cond, steps, batch_size, eta, callback, normals_sequence,\n                          mask, x0, quantize_x0, temperature, score_corrector, corrector_kwargs, x_t)\n\n    # Sample the data\n    samples, log_probs, log_det_jacobians, losses = sampler.sample(shape, cond, eta, steps,\n                                                                   callback, normals_sequence,\n                                                                   mask, x0, quantize_x0,\n                                                                   temperature, score_corrector,\n                                                                   corrector_kwargs, x_t)\n\n    # Return the samples, intermediate values, and losses\n    return samples, log_probs, log_det_jacobians, losses\n\n", "CodeBleu_score": 29.790492469767344}, {"code": "def convsample_ddim(model, cond, steps, shape, eta=1.0, callback=None, normals_sequence=None,\n                    mask=None, x0=None, quantize_x0=False, temperature=1., score_corrector=None,\n                    corrector_kwargs=None, x_t=None\n                    )\n\t\n    # This function performs conditional sampling using the DDIM (Denoising Diffusion Implicit Models) sampler. It initializes the sampler with the provided model, extracts the batch size from the shape parameter, and prints the eta and steps values. The function then calls the sample method of the DDIMSampler instance, passing several parameters such as steps, batch size, shape, conditioning, eta, and other optional arguments like callback, normals_sequence, mask, x0, quantize_x0, temperature, score_corrector, corrector_kwargs, and x_t. Finally, it returns the generated samples and intermediate values.\n    sampler = DDIMSampler(model)\n    batch_size = shape[0]\n    print(f\"eta: {eta}, steps: {steps}\")\n    samples, log_probs, diffs, x_t = sampler.sample(\n        steps, batch_size, shape, cond, eta, callback, normals_sequence, mask, x0, quantize_x0, temperature, score_corrector, corrector_kwargs, x_t\n    )\n    return samples, log_probs, diffs, x_t\n\n", "CodeBleu_score": 25.640325519531913}, {"code": "def convsample_ddim(model, cond, steps, shape, eta=1.0, callback=None, normals_sequence=None,\n                    mask=None, x0=None, quantize_x0=False, temperature=1., score_corrector=None,\n                    corrector_kwargs=None, x_t=None\n                    )\n\t\n    batch_size = shape[0]\n    print(f'eta: {eta}')\n    print(f'steps: {steps}')\n    sampler = DDIMSampler(model, cond, steps, batch_size, eta, callback, normals_sequence, mask, x0, quantize_x0, temperature, score_corrector, corrector_kwargs, x_t)\n    samples, log_prob = sampler.sample(shape, cond)\n    return samples, log_prob\n\n", "CodeBleu_score": 22.12772204510845}]}
{"instance_id": "9bdcd9081480", "generation_result": [{"code": "def ema_scope(self, context=None)\n\t\n    if self.ema_enabled:\n        if context is not None:\n            print(f\"{context}: Switching to EMA weights...\")\n        self.ema_weights = self.ema_weights.copy_to(self.device)\n        self.ema_weights.restore()\n        if context is not None:\n            print(f\"{context}: Restoring original weights...\")\n        self.ema_weights.restore()\n    else:\n        if context is not None:\n            print(f\"{context}: EMA weights are not enabled.\")\n", "CodeBleu_score": 59.24778318097792}, {"code": "def ema_scope(self, context=None)\n\t\n    if context is not None:\n        context.print(f\"Switching to EMA weights\")\n    self.ema_weights = self.ema_weights.copy()\n    self.ema_weights.restore()\n    if context is not None:\n        context.print(f\"Restoring original weights\")\n    self.ema_weights.store()\n\n", "CodeBleu_score": 43.70974808954091}, {"code": "def ema_scope(self, context=None)\n\t\n    if self.ema_enabled:\n        if context is not None:\n            print(f\"{context}: Switching to EMA weights\")\n        self.ema_enabled = False\n        self.ema_weights = self.parameters()\n        for param, ema_param in zip(self.parameters(), self.ema_weights):\n            ema_param.data.copy_(param.data)\n        if context is not None:\n            print(f\"{context}: Restoring original weights\")\n        self.ema_enabled = True\n\n", "CodeBleu_score": 52.772379068629945}]}
{"instance_id": "14ab009e1c5c", "generation_result": [{"code": "def init_from_ckpt(self, path, ignore_keys=None)\n\t\n    sd = torch.load(path)\n    if ignore_keys is not None:\n        for k in ignore_keys:\n            if k in sd:\n                del sd[k]\n    self.load_state_dict(sd, strict=False)\n    missing_keys = []\n    unexpected_keys = []\n    error_msgs = []\n    # copy state_dict so _load_from_state_dict can modify it\n    metadata = getattr(self.state_dict(), \"_metadata\", None)\n    state_dict = self.state_dict().copy()\n    for k, v in sd.items():\n        if k not in state_dict:\n            missing_keys.append(k)\n        elif v.shape != state_dict[k].shape:\n            unexpected_keys.append(k)\n        else:\n            state_dict[k] = v\n    self.load_state_dict(state_dict, strict=False)\n    if len(unexpected_keys) > 0:\n        error_msgs.insert(\n            0, \"Unexpected key(s) in state_dict: {}. \".format(\n                \", \".join('\"{}\"'.format(k) for k in unexpected_keys)))\n    if len(missing_keys) > 0:\n        error_msgs.insert(\n            0, \"Missing key(s) in state_dict: {}. \".format(\n                \", \".join('\"{}\"'.format(k) for k in missing_keys)))\n    if len(error_msgs) > 0:\n        error_msgs.insert(\n            0, \"Error(s) in loading state_dict for {}:", "CodeBleu_score": 30.044095073731437}, {"code": "def init_from_ckpt(self, path, ignore_keys=None)\n\t\n    sd = torch.load(path, map_location=torch.device('cpu'))\n    if ignore_keys is not None:\n        for k in ignore_keys:\n            if k in sd:\n                del sd[k]\n    missing_keys = []\n    unexpected_keys = []\n    self.load_state_dict(sd, strict=False)\n    for k in self.state_dict().keys():\n        if k not in sd:\n            missing_keys.append(k)\n        elif sd[k].shape != self.state_dict()[k].shape:\n            unexpected_keys.append(k)\n    print(f'missing_keys: {missing_keys}')\n    print(f'unexpected_keys: {unexpected_keys}')\n    return missing_keys, unexpected_keys\n\n", "CodeBleu_score": 37.282618952394074}, {"code": "def init_from_ckpt(self, path, ignore_keys=None)\n\t\n    sd = torch.load(path, map_location=lambda storage, loc: storage)['model']\n    if ignore_keys is not None:\n        for k in ignore_keys:\n            sd.pop(k, None)\n    self.load_state_dict(sd, strict=False)\n    missing_keys, unexpected_keys = self.load_state_dict(sd, strict=False)\n    print('missing_keys: {}", "CodeBleu_score": 22.202425794545654}]}
{"instance_id": "754176e27b81", "generation_result": [{"code": "def forward(self, input, return_pred_indices=False)\n\t\n    # Encoding\n    self.encode(input)\n    # Decoding\n    decoded = self.decode()\n    # Difference metric\n    difference = self.difference_metric(decoded)\n    # Prediction indices\n    if return_pred_indices:\n        return decoded, difference, self.pred_indices\n    else:\n        return decoded, difference\n\n", "CodeBleu_score": 26.7252933067901}, {"code": "def forward(self, input, return_pred_indices=False)\n\t\n    enc_output = self.encode(input)\n    dec_output = self.decode(enc_output)\n    diff = torch.mean(torch.abs(input - dec_output))\n    if return_pred_indices:\n        return dec_output, diff, self.pred_indices\n    return dec_output, diff\n\n", "CodeBleu_score": 28.917435542280845}, {"code": "def forward(self, input, return_pred_indices=False)\n\t\n    # Encode the input\n    encoded_input = self.encode(input)\n    # Decode the input\n    decoded_output = self.decode(encoded_input)\n    # Calculate the difference metric\n    difference_metric = self.difference_metric(input, decoded_output)\n    # Return the decoded output and the difference metric\n    if return_pred_indices:\n        return decoded_output, difference_metric, self.predict_indices(input)\n    else:\n        return decoded_output, difference_metric\n\n", "CodeBleu_score": 28.27634079951025}]}
{"instance_id": "443df6fdaf0d", "generation_result": [{"code": "def get_input(self, batch, k)\n\t\n    x = batch[k]\n    if self.resize_batch:\n        x = F.interpolate(x, size=(self.batch_size, self.batch_size), mode='bicubic')\n    x = x.permute(0, 3, 1, 2)\n    x = x.contiguous().float().detach()\n    return x\n\n", "CodeBleu_score": 19.007919078193478}, {"code": "def get_input(self, batch, k)\n\t\n    x = batch[k]\n    if len(x.shape) == 2:\n        x = x.unsqueeze(0)\n    elif len(x.shape) == 3:\n        x = x.unsqueeze(0)\n    elif len(x.shape) == 4:\n        x = x.permute(0, 3, 1, 2)\n    else:\n        raise ValueError(f\"Unsupported input shape {x.shape}.\")\n    x = x.float()\n    x = x.contiguous()\n    if self.resize:\n        x = F.interpolate(x, size=self.get_new_size(k), mode=\"bicubic\", align_corners=False)\n    x = x.detach()\n    return x\n\n", "CodeBleu_score": 40.41482867482676}, {"code": "def get_input(self, batch, k)\n\t\n    x = batch[k]\n    if len(x.shape) == 2:\n        x = x.unsqueeze(0)\n    if len(x.shape) == 3:\n        x = x.unsqueeze(0)\n    if len(x.shape) == 4:\n        x = x.permute(0, 3, 1, 2)\n    if len(x.shape) == 5:\n        x = x.permute(0, 4, 1, 2, 3)\n    x = x.contiguous()\n    if self.resize_batch:\n        x = F.interpolate(x, size=self.get_size(k), mode='bicubic', align_corners=True)\n    x = x.detach()\n    return x\n\n", "CodeBleu_score": 40.338608115893926}]}
{"instance_id": "953a2632fc15", "generation_result": [{"code": "def training_step(self, batch, batch_idx, optimizer_idx)\n\t\n    x, y = batch\n    x = x.to(self.device)\n    y = y.to(self.device)\n    y_hat = self(x)\n    if optimizer_idx == 0:\n        loss = self.loss(y_hat, y)\n        self.log_dict(\n            {\n                \"train_loss\": loss,\n                \"train_recon_loss\": loss,\n                \"train_kl_loss\": 0,\n            }\n        )\n    else:\n        loss = self.loss(y_hat, y)\n        self.log_dict(\n            {\n                \"train_loss\": loss,\n                \"train_recon_loss\": loss,\n                \"train_kl_loss\": loss,\n            }\n        )\n    return loss\n\n", "CodeBleu_score": 21.46247721383414}, {"code": "def training_step(self, batch, batch_idx, optimizer_idx)\n\t\n    x, y = batch\n    x = x.to(self.device)\n    y = y.to(self.device)\n    y_hat = self(x)\n    loss = 0\n    if optimizer_idx == 0:\n        loss = self.loss(y_hat, y)\n    else:\n        loss = self.loss(y_hat, y)\n    self.log_dict({\"train_loss\": loss})\n    return loss\n", "CodeBleu_score": 13.598315136866892}, {"code": "def training_step(self, batch, batch_idx, optimizer_idx)\n\t\n    x, y = batch\n    x = x.to(self.device)\n    y = y.to(self.device)\n    y_hat = self.forward(x)\n    loss = self.loss(y_hat, y)\n    self.log_dict({'train_loss': loss})\n    return loss\n\n", "CodeBleu_score": 5.6995455173138145}]}
{"instance_id": "b449b5e8b77f", "generation_result": [{"code": "def log_images(self, batch, only_inputs=False, plot_ema=False, **kwargs)\n\t\n    if self.ema_scope:\n        with tf.variable_scope(self.ema_scope, reuse=tf.AUTO_REUSE):\n            ema = tf.train.ExponentialMovingAverage(self.ema_decay)\n            variables_to_restore = ema.variables_to_restore()\n            ema_vars = {var.op.name: var for var in tf.global_variables() if var.op.name in variables_to_restore}\n            ema_assign_op = ema.apply(ema_vars.keys())\n            self.ema_assign_op = ema_assign_op\n    else:\n        ema_assign_op = None\n\n    def get_images(x):\n        if self.input_channels == 1:\n            x = tf.expand_dims(x, axis=-1)\n        x = tf.clip_by_value(x, 0, 1)\n        x = tf.cast(x * 255, tf.uint8)\n        return x\n\n    def get_reconstructions(x, ema_assign_op):\n        if self.ema_scope and ema_assign_op is not None:\n            with tf.control_dependencies([ema_assign_op]):\n                reconstructions = self.to_rgb(self.get_output(x, **kwargs))\n        else:\n            reconstructions = self.to_rgb(self.get_output(x, **kwargs))\n        return reconstructions\n\n    def get_inputs(x):\n        return get_images(x)\n\n    def get_reconstructions_and_inputs(x, ema_assign_op):\n        reconstructions = get_reconstructions(x, ema_assign_op)\n        inputs = get_images(x)\n        return reconstructions, inputs\n\n    def get_reconstructions_and_inputs_ema(x):\n        reconstructions, inputs = get_reconstructions_and_inputs(x, ema_assign_op)\n        return reconstructions, inputs\n\n    if only_inputs:\n", "CodeBleu_score": 25.184247489945477}, {"code": "def log_images(self, batch, only_inputs=False, plot_ema=False, **kwargs)\n\t\n    self.ema_scope = plot_ema\n    if self.ema_scope:\n        self.ema.apply()\n    self.get_input(batch)\n    self.to_rgb()\n    self.to(self.device)\n    if not only_inputs:\n        self.reconstruct()\n        self.to_rgb()\n        self.to(self.device)\n    self.log_inputs(**kwargs)\n    if not only_inputs:\n        self.log_reconstructions(**kwargs)\n    if self.ema_scope:\n        self.ema.restore()\n\n", "CodeBleu_score": 22.873887499134067}, {"code": "def log_images(self, batch, only_inputs=False, plot_ema=False, **kwargs)\n\t\n    if not only_inputs:\n        if plot_ema:\n            self.ema_scope(self.to_rgb, **kwargs)\n        else:\n            self.to_rgb(**kwargs)\n    self.get_input(batch, only_inputs=only_inputs)\n    self.get_reconstruction(batch, only_inputs=only_inputs, plot_ema=plot_ema, **kwargs)\n    self.get_input_reconstruction(batch, only_inputs=only_inputs, plot_ema=plot_ema, **kwargs)\n\n", "CodeBleu_score": 17.044338054474323}]}
{"instance_id": "540d089e3481", "generation_result": [{"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss_type == 'l1':\n        if mean:\n            return loss.mean(abs(pred - target))\n        else:\n            return loss.mean(abs(pred - target))\n    elif self.loss_type == 'l2':\n        if mean:\n            return loss.mean(loss.square(pred - target))\n        else:\n            return loss.mean(loss.square(pred - target))\n    else:\n        raise NotImplementedError('Unknown loss type {}'.format(self.loss_type))\n\n", "CodeBleu_score": 34.81499637465713}, {"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss_type == 'l1':\n        loss = self.abs(pred - target)\n        if mean:\n            loss = self.mean(loss)\n    elif self.loss_type == 'l2':\n        loss = self.mean(self.square(pred - target))\n    else:\n        raise NotImplementedError('Unknown loss type: {}'.format(self.loss_type))\n    return loss\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 40.64805617662715}, {"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss_type == 'l1':\n        loss = loss.mean if mean else loss\n        return loss\n    elif self.loss_type == 'l2':\n        loss = loss.mean if mean else loss\n        return loss\n    else:\n        raise NotImplementedError\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 28.301657975327508}]}
{"instance_id": "63a08283666f", "generation_result": [{"code": "def forward(self, x, c, *args, **kwargs)\n\t\n    # randomly select time steps\n    if self.training:\n        if self.q_sample:\n            t = torch.randint(self.num_timesteps, (x.size(0),), device=x.device)\n        else:\n            t = torch.arange(self.num_timesteps, device=x.device)\n        x = x[:, t]\n        c = c[:, t]\n\n    # apply learned conditioning\n    if self.get_learned_conditioning:\n        c = self.learned_conditioning(c)\n\n    # process inputs\n    if self.conditioning_type == 'binary':\n        x = self.input_layer(x)\n        c = self.conditioning_layer(c)\n    elif self.conditioning_type == 'binary_binary':\n        x = self.input_layer(x)\n        c = self.conditioning_layer(c)\n    elif self.conditioning_type == 'binary_binary_binary':\n        x = self.input_layer(x)\n        c = self.conditioning_layer(c)\n    elif self.conditioning_type == 'binary_binary_binary_binary':\n        x = self.input_layer(x)\n        c = self.conditioning_layer(c)\n    elif self.conditioning_type == 'binary_binary_binary_binary_binary':\n        x = self.input_layer(x)\n        c = self.conditioning_layer(c)\n    elif self.conditioning_type == 'binary_binary_binary_binary_binary_binary':\n        x = self.input_layer(x)\n        c = self.conditioning_layer(c)\n    elif self.conditioning_type == 'binary_binary_binary_binary_binary_binary_binary':\n        x = self.input_layer(x)\n        c = self.conditioning_layer(c)\n    elif self.conditioning_type == 'binary_binary_binary_binary_binary_binary_binary_binary':\n        x = self.input_layer(x)\n        c = self.conditioning_layer(c)\n    elif self.conditioning_type ==", "CodeBleu_score": 28.78092003504188}, {"code": "def forward(self, x, c, *args, **kwargs)\n\t\n    if self.training:\n        # Sample time steps\n        if self.sample_steps:\n            t = torch.randint(0, self.num_steps, (x.size(0), 1), device=x.device)\n            x = x[torch.arange(x.size(0)), t]\n            c = c[torch.arange(c.size(0)), t]\n\n        # Apply learned conditioning\n        if self.conditioning_type == \"learned\":\n            c = self.get_learned_conditioning(c)\n\n        # Apply conditioning requirements\n        if self.conditioning_type == \"embed\":\n            x = self.embedding(x)\n        elif self.conditioning_type == \"conv\":\n            x = self.conv(x)\n        elif self.conditioning_type == \"lstm\":\n            x, _ = self.lstm(x)\n        elif self.conditioning_type == \"gru\":\n            x, _ = self.gru(x)\n\n    # Compute loss\n    y_hat = self.model(x, c, *args, **kwargs)\n    if self.training:\n        y = x[torch.arange(x.size(0)), t]\n        loss = self.loss(y_hat, y)\n        if self.sample_steps:\n            return loss, t\n        else:\n            return loss\n    else:\n        return y_hat\n\n", "CodeBleu_score": 42.39695808552415}, {"code": "def forward(self, x, c, *args, **kwargs)\n\t\n    if self.conditioning_type == \"learned\":\n        c = self.get_learned_conditioning(c)\n    if self.conditioning_type == \"time\":\n        c = self.get_time_conditioning(c)\n    if self.conditioning_type == \"learned_time\":\n        c = self.get_learned_time_conditioning(c)\n    if self.conditioning_type == \"learned_time_v2\":\n        c = self.get_learned_time_conditioning_v2(c)\n    if self.conditioning_type == \"learned_time_v3\":\n        c = self.get_learned_time_conditioning_v3(c)\n    if self.conditioning_type == \"learned_time_v4\":\n        c = self.get_learned_time_conditioning_v4(c)\n    if self.conditioning_type == \"learned_time_v5\":\n        c = self.get_learned_time_conditioning_v5(c)\n    if self.conditioning_type == \"learned_time_v6\":\n        c = self.get_learned_time_conditioning_v6(c)\n    if self.conditioning_type == \"learned_time_v7\":\n        c = self.get_learned_time_conditioning_v7(c)\n    if self.conditioning_type == \"learned_time_v8\":\n        c = self.get_learned_time_conditioning_v8(c)\n    if self.conditioning_type == \"learned_time_v9\":\n        c = self.get_learned_time_conditioning_v9(c)\n    if self.conditioning_type == \"learned_time_v10\":\n        c = self.get_learned_time_conditioning_v10(c)\n    if self.conditioning_type == \"learned_time_v11\":\n        c = self.get_learned_time_conditioning_v11(c)\n    if self.conditioning_type == \"learned_time_", "CodeBleu_score": 27.70587333356451}]}
{"instance_id": "4627e1e8ed62", "generation_result": [{"code": "def get_codebook_entry(self, indices, shape)\n\t\n    indices = indices.view(-1, 1)\n    if self.remap_indices:\n        indices = self.remap_indices(indices)\n    z_q = self.embedding(indices)\n    z_q = z_q.view(shape)\n    if self.permute_latents:\n        z_q = z_q.permute(0, 2, 3, 1)\n    return z_q\n\n", "CodeBleu_score": 39.42363592366339}, {"code": "def get_codebook_entry(self, indices, shape)\n\t\n    if self.remap_indices:\n        indices = self.remap_indices(indices)\n    z_q = self.codebook.index_select(0, indices.view(-1)).view(shape)\n    if self.permute_latents:\n        z_q = z_q.permute(self.permute_latents)\n    if self.permute_codebook:\n        z_q = z_q.permute(self.permute_codebook)\n    return z_q\n\n", "CodeBleu_score": 42.789443360975675}, {"code": "def get_codebook_entry(self, indices, shape)\n\t\n    # remap indices if necessary\n    if self.remap_indices:\n        indices = self.remap_indices(indices)\n    # get the latent vectors from the codebook\n    z_q = self.codebook(indices)\n    # reshape and permute the latent vectors to match the input shape\n    z_q = z_q.view(indices.shape[0], *shape)\n    z_q = z_q.permute(0, 2, 3, 1)\n    return z_q\n\n", "CodeBleu_score": 36.5984397496209}]}
{"instance_id": "c55d014827ef", "generation_result": [{"code": "def factorization(dimension: int, factor:int=-1) -> tuple[int, int]\n\t\n    if factor > 0:\n        if dimension % factor == 0:\n            return (factor, dimension // factor)\n        else:\n            return (dimension, 1)\n    else:\n        if factor == -1:\n            factor = dimension\n        while True:\n            if dimension % factor == 0:\n                if dimension // factor <= factor:\n                    return (factor, dimension // factor)\n                else:\n                    factor += 1\n            else:\n                factor += 1\n\n", "CodeBleu_score": 31.502750316778684}, {"code": "def factorization(dimension: int, factor:int=-1) -> tuple[int, int]\n\t\n    if factor < 0:\n        factor = 1\n    if factor > 0:\n        if dimension % factor == 0:\n            return (factor, dimension // factor)\n        else:\n            for i in range(1, dimension):\n                if dimension % i == 0:\n                    return (i, dimension // i)\n    else:\n        for i in range(1, dimension):\n            if dimension % i == 0:\n                return (i, dimension // i)\n\n", "CodeBleu_score": 28.275532085487843}, {"code": "def factorization(dimension: int, factor:int=-1) -> tuple[int, int]\n\t\n    if factor > 0:\n        if dimension % factor == 0:\n            return factor, dimension // factor\n        else:\n            raise ValueError(\"The factor does not divide the dimension.\")\n    else:\n        if factor == -1:\n            factor = dimension\n        if dimension % factor == 0:\n            return factor, dimension // factor\n        else:\n            while True:\n                factor += 1\n                if dimension % factor == 0:\n                    return factor, dimension // factor\n\n", "CodeBleu_score": 29.581594977120616}]}
{"instance_id": "5d613ec26036", "generation_result": [{"code": "def apply_weight_decompose(self, updown, orig_weight)\n\t\n    if updown:\n        merged_scale1 = orig_weight * self.scale_factor\n    else:\n        merged_scale1 = orig_weight * self.scale_factor\n    merged_scale1 = merged_scale1.to(self.device)\n    merged_scale1 = merged_scale1.type(self.dtype)\n    merged_scale2 = merged_scale1.transpose(0, 1)\n    merged_scale3 = merged_scale2.reshape(merged_scale2.shape[0], -1)\n    merged_scale4 = merged_scale3.norm(dim=1)\n    merged_scale5 = merged_scale4.reshape(merged_scale4.shape[0], 1)\n    merged_scale6 = merged_scale5.transpose(0, 1)\n    merged_scale7 = merged_scale6.reshape(merged_scale6.shape[0], -1)\n    merged_scale8 = merged_scale7.norm(dim=1)\n    merged_scale9 = merged_scale8.reshape(merged_scale8.shape[0], 1)\n    merged_scale10 = merged_scale9.transpose(0, 1)\n    merged_scale11 = merged_scale10.reshape(merged_scale10.shape[0], -1)\n    merged_scale12 = merged_scale11.norm(dim=1)\n    merged_scale13 = merged_scale12.reshape(merged_scale12.shape[0], 1)\n    merged_scale14 = merged_scale13.transpose(0, 1)\n    merged_scale15 = merged_scale14.reshape(merged_scale14.shape[0], -1)\n    merged_scale16 = merged_scale15.norm(dim=1)\n    merged_scale17 = merged_scale16.reshape(merged_scale16.shape[0], 1)\n    merged_scale18 = merged_scale17.transpose(0, 1)\n    merged_scale19 = merged_scale18.reshape", "CodeBleu_score": 35.9045307095766}, {"code": "def apply_weight_decompose(self, updown, orig_weight)\n\t\n    #print(\"apply_weight_decompose\")\n    #print(updown)\n    #print(orig_weight)\n    #print(self.merged_scale1)\n    #print(self.merged_scale2)\n    #print(self.merged_scale3)\n    #print(self.merged_scale4)\n    #print(self.merged_scale5)\n    #print(self.merged_scale6)\n    #print(self.merged_scale7)\n    #print(self.merged_scale8)\n    #print(self.merged_scale9)\n    #print(self.merged_scale10)\n    #print(self.merged_scale11)\n    #print(self.merged_scale12)\n    #print(self.merged_scale13)\n    #print(self.merged_scale14)\n    #print(self.merged_scale15)\n    #print(self.merged_scale16)\n    #print(self.merged_scale17)\n    #print(self.merged_scale18)\n    #print(self.merged_scale19)\n    #print(self.merged_scale20)\n    #print(self.merged_scale21)\n    #print(self.merged_scale22)\n    #print(self.merged_scale23)\n    #print(self.merged_scale24)\n    #print(self.merged_scale25)\n    #print(self.merged_scale26)\n    #print(self.merged_scale27)\n    #print(self.merged_scale28)\n    #print(self.merged_scale29)\n    #print(self.merged_scale30)\n    #print(self.merged_scale31)\n    #print(self.merged_scale32)\n    #print(self.merged_scale33)\n    #print(self.merged_scale34)\n    #print(self.merged_scale35)\n   ", "CodeBleu_score": 1.049465240641711}, {"code": "def apply_weight_decompose(self, updown, orig_weight)\n\t\n    # 1. Ensure that all tensors are on the same device and have the same data type\n    device = orig_weight.device\n    dtype = orig_weight.dtype\n    updown = updown.to(device=device, dtype=dtype)\n    orig_weight = orig_weight.to(device=device, dtype=dtype)\n\n    # 2. Decompose the weight\n    # 2.1. Transpose the weight\n    transpose = updown.transpose(-2, -1)\n\n    # 2.2. Reshape the weight\n    reshape = transpose.reshape(orig_weight.shape[0], orig_weight.shape[1], -1)\n\n    # 2.3. Merge the weight and the scale1\n    merged_scale1 = torch.cat((orig_weight, reshape), dim=2)\n\n    # 3. Normalize the weight\n    norm = merged_scale1 / merged_scale1.norm(dim=2, keepdim=True)\n\n    # 4. Scale the weight\n    scaled = norm * self.scale1\n\n    # 5. Adjust the weight\n    adjusted_weight = scaled - orig_weight\n    return adjusted_weight\n\n", "CodeBleu_score": 20.703342166441757}]}
{"instance_id": "c73d0d1cd3e5", "generation_result": [{"code": "def finalize_updown(self, updown, orig_weight, output_shape, ex_bias=None)\n\t\n    if updown.size == orig_weight.size:\n        updown = updown.reshape(orig_weight.shape)\n    else:\n        updown = updown.reshape(output_shape)\n    if ex_bias is not None:\n        ex_bias *= self.multiplier\n    return updown, ex_bias\n\n", "CodeBleu_score": 23.70570405103406}, {"code": "def finalize_updown(self, updown, orig_weight, output_shape, ex_bias=None)\n\t\n    if updown.numel() == orig_weight.numel():\n        updown = updown.reshape(orig_weight.size())\n    else:\n        updown = updown.reshape(output_shape)\n    if ex_bias is not None:\n        ex_bias = ex_bias * self.multiplier\n    if self.calc_scale:\n        updown = updown * self.scale\n    if self.decompose_weight:\n        updown = self.apply_weight_decompose(updown)\n    return updown, ex_bias\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 42.09248408463417}, {"code": "def finalize_updown(self, updown, orig_weight, output_shape, ex_bias=None)\n\t\n    if ex_bias is None:\n        ex_bias = torch.zeros(updown.size(0)).to(updown.device)\n    updown = updown + ex_bias\n    if updown.size(0) == orig_weight.size(0):\n        updown = updown.reshape(output_shape)\n    else:\n        updown = updown.reshape(orig_weight.size())\n    if self.multiplier is not None:\n        ex_bias = ex_bias * self.multiplier\n    if self.calc_scale:\n        updown = updown * self.scale\n    if self.decompose_weight:\n        updown = updown.mul(self.decompose_weight)\n    return updown, ex_bias\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 49.853824735182386}]}
{"instance_id": "c6a41665c62c", "generation_result": [{"code": "def calc_updown(self, orig_weight)\n\t\n    if self.use_bias:\n        self.bias = self.bias.to(orig_weight.device)\n    return self.finalize_updown(orig_weight, self.weight, self.bias)\n\n", "CodeBleu_score": 27.654048135487518}, {"code": "def calc_updown(self, orig_weight)\n\t\n\n    # transfer the current weight tensor and optional bias to the device of the original weight tensor\n    weight = self.weight.to(orig_weight.device)\n    bias = self.bias.to(orig_weight.device) if self.bias is not None else None\n\n    # call another function to finalize the updated weights\n    return self.finalize_updown(orig_weight, weight, bias)\n\n", "CodeBleu_score": 25.249047676744617}, {"code": "def calc_updown(self, orig_weight)\n\t\n    if orig_weight.is_cuda:\n        orig_weight = orig_weight.cpu()\n        if self.bias is not None:\n            self.bias = self.bias.cpu()\n    return self.finalize_updown(orig_weight, self.weight, self.bias)\n\n", "CodeBleu_score": 31.517621854719042}]}
{"instance_id": "1bb9e5d94af8", "generation_result": [{"code": "def purge_networks_from_memory()\n\t\n    global networks_in_memory\n    global networks_in_memory_limit\n    while len(networks_in_memory) > networks_in_memory_limit:\n        networks_in_memory.pop()\n    gc.collect()\n\n", "CodeBleu_score": 35.24803048583419}, {"code": "def purge_networks_from_memory()\n\t\n    while len(networks_in_memory) > max_networks_in_memory:\n        networks_in_memory.pop()\n        gc.collect()\n\n", "CodeBleu_score": 17.998697065332312}, {"code": "def purge_networks_from_memory()\n\t\n    if len(networks_in_memory) > MAX_NETWORKS_IN_MEMORY:\n        while len(networks_in_memory) > MAX_NETWORKS_IN_MEMORY:\n            networks_in_memory.pop()\n        torch.cuda.empty_cache()\n        gc.collect()\n\n", "CodeBleu_score": 35.120339641570034}]}
{"instance_id": "42b48ebf4d07", "generation_result": [{"code": "def network_restore_weights_from_backup(self: Union[torch.nn.Conv2d, torch.nn.Linear, torch.nn.GroupNorm, torch.nn.LayerNorm, torch.nn.MultiheadAttention]):\n\t\n    if self.weight.shape == self.backup_weight.shape:\n        self.weight = self.backup_weight\n    if self.bias is not None and self.backup_bias is not None:\n        if self.backup_bias.shape == self.bias.shape:\n            self.bias = self.backup_bias\n        else:\n            self.bias = None\n    if isinstance(self, torch.nn.MultiheadAttention):\n        if self.in_proj_weight.shape == self.backup_in_proj_weight.shape:\n            self.in_proj_weight = self.backup_in_proj_weight\n        if self.in_proj_bias is not None and self.backup_in_proj_bias is not None:\n            if self.backup_in_proj_bias.shape == self.in_proj_bias.shape:\n                self.in_proj_bias = self.backup_in_proj_bias\n            else:\n                self.in_proj_bias = None\n        if self.out_proj.weight.shape == self.backup_out_proj_weight.shape:\n            self.out_proj.weight = self.backup_out_proj_weight\n        if self.out_proj.bias is not None and self.backup_out_proj_bias is not None:\n            if self.backup_out_proj_bias.shape == self.out_proj.bias.shape:\n                self.out_proj.bias = self.backup_out_proj_bias\n            else:\n                self.out_proj.bias = None\n\n", "CodeBleu_score": 41.06911121303212}, {"code": "def network_restore_weights_from_backup(self: Union[torch.nn.Conv2d, torch.nn.Linear, torch.nn.GroupNorm, torch.nn.LayerNorm, torch.nn.MultiheadAttention]):\n\t\n    if self.weight.shape == self.backup_weight.shape:\n        self.weight = self.backup_weight\n    if self.bias is not None and self.backup_bias is not None:\n        self.bias = self.backup_bias\n    if isinstance(self, torch.nn.MultiheadAttention):\n        self.in_proj_weight = self.backup_in_proj_weight\n        self.in_proj_bias = self.backup_in_proj_bias\n        self.out_proj.weight = self.backup_out_proj_weight\n        self.out_proj.bias = self.backup_out_proj_bias\n\n", "CodeBleu_score": 27.91855782074967}, {"code": "def network_restore_weights_from_backup(self: Union[torch.nn.Conv2d, torch.nn.Linear, torch.nn.GroupNorm, torch.nn.LayerNorm, torch.nn.MultiheadAttention]):\n\t\n    if self.weight.shape != self.backup_weight.shape:\n        raise Exception(f\"Weights of {self} do not match the shape of the backup weights.\")\n    if self.bias is not None and self.bias.shape != self.backup_bias.shape:\n        raise Exception(f\"Biases of {self} do not match the shape of the backup biases.\")\n    if self.backup_bias is not None:\n        self.bias = self.backup_bias\n    if self.backup_weight is not None:\n        self.weight = self.backup_weight\n    if isinstance(self, torch.nn.MultiheadAttention):\n        if self.in_proj_weight.shape != self.backup_in_proj_weight.shape:\n            raise Exception(f\"Projection weights of {self} do not match the shape of the backup projection weights.\")\n        if self.in_proj_bias is not None and self.in_proj_bias.shape != self.backup_in_proj_bias.shape:\n            raise Exception(f\"Projection biases of {self} do not match the shape of the backup projection biases.\")\n        if self.backup_in_proj_bias is not None:\n            self.in_proj_bias = self.backup_in_proj_bias\n        if self.backup_in_proj_weight is not None:\n            self.in_proj_weight = self.backup_in_proj_weight\n    if isinstance(self, torch.nn.LayerNorm):\n        if self.weight.shape != self.backup_weight.shape:\n            raise Exception(f\"Weights of {self} do not match the shape of the backup weights.\")\n        if self.bias is not None and self.bias.shape != self.backup_bias.shape:\n            raise Exception(f\"Biases of {self} do not match the shape of the backup biases.\")\n        if self.backup_bias is not None:\n            self.bias = self.backup_bias\n        if self.backup_weight is not None:\n           ", "CodeBleu_score": 34.62053121158547}]}
{"instance_id": "c8fcd357131a", "generation_result": [{"code": "def network_forward(org_module, input, original_forward)\n\t\n    if not isinstance(input, torch.Tensor):\n        input = torch.tensor(input, device=org_module.device)\n    input = devices.cond_cast_unet(input, org_module.device)\n    network_reset_cached_weight(org_module)\n    network_restore_weights_from_backup(org_module)\n    output = original_forward(org_module, input)\n    for network in networks.loaded_networks:\n        if network.name == org_module.name:\n            output = network_apply_network(network, output)\n    return output\n\n", "CodeBleu_score": 41.614430275087194}, {"code": "def network_forward(org_module, input, original_forward)\n\t\n    # Check if any networks are loaded\n    if len(networks) == 0:\n        # If not, just run the original forward method\n        return original_forward(input)\n    # If networks are present, check if the module is in the loaded network\n    if org_module in networks:\n        # If it is, get the loaded network\n        loaded_network = networks[org_module]\n        # Get the network's attributes\n        network_attr = getattr(loaded_network, 'attr', None)\n        # Get the network's reset cached weight method\n        network_reset_cached_weight = getattr(loaded_network, 'reset_cached_weight', None)\n        # Get the network's forward method\n        network_forward = getattr(loaded_network, 'forward', None)\n        # If any of the network attributes are missing, raise an exception\n        if network_attr is None or network_reset_cached_weight is None or network_forward is None:\n            raise Exception('Network ' + str(loaded_network) + ' is missing an attribute.')\n        # Get the network's device\n        network_device = getattr(loaded_network, 'device', None)\n        # If the network's device is not set, raise an exception\n        if network_device is None:\n            raise Exception('Network ' + str(loaded_network) + ' is missing a device.')\n        # If the network's device is not set to the current device, raise an exception\n        if network_device != devices.current_device():\n            raise Exception('Network ' + str(loaded_network) + ' is on a different device than the current one.')\n        # Cast the input to the correct device\n        input = devices.cond_cast_unet(input, network_device)\n        # Reset the cached weight\n        network_reset_cached_weight()\n        # If the network's attributes are not set, raise an exception\n        if network_attr is None:\n            raise Exception('Network ' + str(loaded_network) + ' is missing an attribute.')\n        # If the network's attributes are not set, raise an exception\n        if network_attr['input_shape'] is None:\n            raise Exception", "CodeBleu_score": 31.796775358126705}, {"code": "def network_forward(org_module, input, original_forward)\n\t\n    if not hasattr(org_module, \"networks\"):\n        return original_forward(input)\n    if not hasattr(org_module, \"networks_backup\"):\n        org_module.networks_backup = {}\n    if not hasattr(org_module, \"networks_cached_weight\"):\n        org_module.networks_cached_weight = {}\n    if not hasattr(org_module, \"networks_cached_weight_backup\"):\n        org_module.networks_cached_weight_backup = {}\n    if not hasattr(org_module, \"networks_cached_weight_backup_backup\"):\n        org_module.networks_cached_weight_backup_backup = {}\n    if not hasattr(org_module, \"networks_cached_weight_backup_backup_backup\"):\n        org_module.networks_cached_weight_backup_backup_backup = {}\n    if not hasattr(org_module, \"networks_cached_weight_backup_backup_backup_backup\"):\n        org_module.networks_cached_weight_backup_backup_backup_backup = {}\n    if not hasattr(org_module, \"networks_cached_weight_backup_backup_backup_backup_backup\"):\n        org_module.networks_cached_weight_backup_backup_backup_backup_backup = {}\n    if not hasattr(org_module, \"networks_cached_weight_backup_backup_backup_backup_backup_backup\"):\n        org_module.networks_cached_weight_backup_backup_backup_backup_backup_backup = {}\n    if not hasattr(org_module, \"networks_cached_weight_backup_backup_backup_backup_backup_backup_backup\"):\n        org_module.networks_cached_weight_backup_backup_backup_backup_backup_backup_backup =", "CodeBleu_score": 18.01964451825346}]}
{"instance_id": "1070211f5c11", "generation_result": [{"code": "def trainables(self)\n\t\n    trainables = []\n    for name, param in self.named_parameters():\n        if param.requires_grad:\n            if isinstance(param, torch.nn.Linear):\n                trainables.append(param)\n            elif isinstance(param, torch.nn.LayerNorm):\n                trainables.append(param.weight)\n    return trainables\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 28.97758908256148}, {"code": "def trainables(self)\n\t\n    trainables = []\n    for name, param in self.named_parameters():\n        if param.requires_grad:\n            if isinstance(param, torch.nn.Linear) or isinstance(param, torch.nn.LayerNorm):\n                trainables.append(param)\n    return trainables\n", "CodeBleu_score": 30.94566567912863}, {"code": "def trainables(self)\n\t\n\n    trainables = []\n    for name, param in self.named_parameters():\n        if 'weight' in name or 'bias' in name:\n            trainables.append(param)\n    return trainables\n\n\n\n", "CodeBleu_score": 25.392464132106284}]}
{"instance_id": "0c3ba62c96a7", "generation_result": [{"code": "def load_hypernetworks(names, multipliers=None)\n\t\n    global already_loaded\n    global hypernetworks\n    global multiplier\n    global multipliers\n    global name\n    global names\n    global network\n    global network_list\n    global network_name\n    global networks\n    global network_names\n    global network_names_list\n    global network_names_list_loaded\n    global network_names_list_loaded_names\n    global network_names_list_loaded_names_multipliers\n    global network_names_list_loaded_names_multipliers_networks\n    global network_names_list_loaded_names_multipliers_networks_names\n    global network_names_list_loaded_names_multipliers_networks_names_multipliers\n    global network_names_list_loaded_names_multipliers_networks_names_multipliers_networks\n    global network_names_list_loaded_names_multipliers_networks_names_multipliers_networks_names\n    global network_names_list_loaded_names_multipliers_networks_names_multipliers_networks_names_multipliers\n    global network_names_list_loaded_names_multipliers_networks_names_multipliers_networks_names_multipliers_networks\n    global network_names_list_loaded_names_multipliers_networks_names_multipliers_networks_names_multipliers_networks_names\n    global network_names_list_loaded_names_multipliers_networks_names_multipliers_networks_names_multipliers_networks_names_multipliers\n    global network_names_list_loaded_names_multipliers_networks_names_multipliers_networks_names_multipliers_networks_names_multipliers_networks\n    global network_names_list_loaded_names_multipliers_networks_names_multipliers_networks_names_multipliers_networks_names_multipliers_networks_names\n    global network_names_list_loaded_names_multipliers_networks_names_multipliers_networks_names_multipliers_networks_names_multipliers_networks_names_multipliers\n    global network_names_list_loaded_names_multipliers_networks_names_multipliers", "CodeBleu_score": 6.333906831897385}, {"code": "def load_hypernetworks(names, multipliers=None)\n\t\n    if multipliers is None:\n        multipliers = [1] * len(names)\n    if len(names) != len(multipliers):\n        raise ValueError(\"The number of hypernetwork names must equal the number of multipliers.\")\n    for name, multiplier in zip(names, multipliers):\n        if name in already_loaded.get():\n            hypernetwork.set_multiplier(name, multiplier)\n        else:\n            hypernetwork.load(name)\n            hypernetwork.set_multiplier(name, multiplier)\n        append(name)\n\n", "CodeBleu_score": 24.16131373100228}, {"code": "def load_hypernetworks(names, multipliers=None)\n\t\n    global already_loaded\n    global hypernetworks\n    if multipliers is None:\n        multipliers = [1] * len(names)\n    already_loaded.clear()\n    for name, multiplier in zip(names, multipliers):\n        if name not in already_loaded:\n            if name in hypernetworks:\n                hypernetwork = hypernetworks[name]\n            else:\n                hypernetwork = Hypernetwork(name)\n            hypernetwork.set_multiplier(multiplier)\n            hypernetworks.append(hypernetwork)\n            already_loaded.append(name)\n\n", "CodeBleu_score": 28.856080681182483}]}
{"instance_id": "bea3772ed532", "generation_result": [{"code": "\ndef apply_hypernetworks(hypernetworks, context, layer=None)\n\t\n\n    # Initialize context variables\n    context_k = context\n    context_v = context\n\n    # Apply each hypernetwork to the context variables\n    for hypernetwork in hypernetworks:\n        context_k, context_v = apply_single_hypernetwork(hypernetwork, context_k, context_v, layer)\n\n    # Return the modified context variables\n    return context_k, context_v\n\n\n", "CodeBleu_score": 76.04946886953897}, {"code": "\ndef apply_hypernetworks(hypernetworks, context, layer=None)\n\t\n    context_k, context_v = context, context\n    for hypernetwork in hypernetworks:\n        context_k, context_v = apply_single_hypernetwork(hypernetwork, context_k, context_v, layer)\n    return context_k, context_v\n\n\n", "CodeBleu_score": 68.4402416707376}, {"code": "\ndef apply_hypernetworks(hypernetworks, context, layer=None)\n\t\n    \n    context_k = context\n    context_v = context\n    \n    for hypernetwork in hypernetworks:\n        context_k, context_v = apply_single_hypernetwork(hypernetwork, context_k, context_v, layer=layer)\n    \n    return context_k, context_v\n\n\n", "CodeBleu_score": 66.82264135585403}]}
{"instance_id": "922efa3e092f", "generation_result": [{"code": "def attention_CrossAttention_forward(self, x, context=None, mask=None, **kwargs)\n\t\n    # compute query, key, and value projections\n    query = self.to_q(x)\n    key = self.to_k(context)\n    value = self.to_v(context)\n    # apply hypernetworks\n    key = self.to_k_hyper(key)\n    value = self.to_v_hyper(value)\n    # compute attention scores\n    scores = einsum('bhqd,bhd->bhqd', query, key)\n    # apply mask to attention scores\n    if mask is not None:\n        scores = sim.masked_fill_(scores, mask, -9e15)\n    # compute attention weights\n    attention = sim.softmax(scores, dim=-1)\n    # compute attention output\n    output = einsum('bhqd,bhd->bqd', attention, value)\n    # reshape back\n    output = self.to_out(output)\n    # apply final linear layer\n    output = self.out_lin(output)\n    return output\n\n", "CodeBleu_score": 28.13704798921956}, {"code": "def attention_CrossAttention_forward(self, x, context=None, mask=None, **kwargs)\n\t\n    # Perform the forward pass of the cross-attention mechanism\n    # Compute query, key, and value projections\n    query = self.to_q(x)\n    key = self.to_k(context)\n    value = self.to_v(context)\n\n    # Apply hypernetworks\n    query = apply_hypernetworks(query, self.hypernetworks_q)\n    key = apply_hypernetworks(key, self.hypernetworks_k)\n    value = apply_hypernetworks(value, self.hypernetworks_v)\n\n    # Reshape tensors for multi-head attention\n    query = query.view(query.size(0), self.num_heads, self.head_size)\n    key = key.view(key.size(0), self.num_heads, self.head_size)\n    value = value.view(value.size(0), self.num_heads, self.head_size)\n\n    # Calculate attention scores\n    attention_scores = torch.einsum('bhqd,bkhd->bhqk', query, key)\n    attention_scores = attention_scores / math.sqrt(self.head_size)\n\n    # Apply mask to attention scores\n    if mask is not None:\n        attention_scores = masked_fill_tensor(attention_scores, mask, -1e8)\n\n    # Calculate attention weights\n    attention_weights = sim.softmax(attention_scores, dim=-1)\n\n    # Calculate attention output\n    attention_output = torch.einsum('bhqk,bkhd->bhqd', attention_weights, value)\n\n    # Reshape attention output back to the original input shape\n    attention_output = attention_output.contiguous().view(query.size(0), -1, self.head_size * self.num_heads)\n\n    # Pass through a final linear layer to get the output\n    attention_output = self.to_out(attention_output)\n\n    # Return the attention output and attention weights\n    return attention_output, attention_weights\n\n", "CodeBleu_score": 31.09682443880988}, {"code": "def attention_CrossAttention_forward(self, x, context=None, mask=None, **kwargs)\n\t\n    # x: (batch_size, input_len, d_model)\n    # context: (batch_size, context_len, d_model)\n    # mask: (batch_size, input_len, context_len)\n    # output: (batch_size, input_len, d_model)\n    # attention: (batch_size, input_len, context_len)\n    # weights: (batch_size, input_len, context_len)\n    # context_mask: (batch_size, context_len)\n    # context_mask_reshaped: (batch_size, context_len, 1)\n    # context_mask_expanded: (batch_size, context_len, context_len)\n    # query_reshaped: (batch_size, input_len, num_heads, d_head)\n    # query: (batch_size, input_len, d_model)\n    # key_reshaped: (batch_size, context_len, num_heads, d_head)\n    # key: (batch_size, context_len, d_model)\n    # value_reshaped: (batch_size, context_len, num_heads, d_head)\n    # value: (batch_size, context_len, d_model)\n    # attention_reshaped: (batch_size, input_len, num_heads, context_len)\n    # attention: (batch_size, input_len, context_len)\n    # attention_reshaped_sum: (batch_size, input_len, num_heads, context_len)\n    # attention_sum: (batch_size, input_len, context_len)\n    # attention_sum_reshaped: (batch_size, input_len, context_len, num_heads, d_head)\n    # attention_sum_linear: (batch_size, input_len, context_len, d_model)\n    # output_linear: (batch_size, input_len, d_model)\n    # output: (batch_size, input_len, d_model)\n    # context_mask_reshaped: (batch_size, context_len, 1)\n", "CodeBleu_score": 1.9834764933446987}]}
{"instance_id": "8b7455a03a0e", "generation_result": [{"code": "def save_hypernetwork(hypernetwork, checkpoint, hypernetwork_name, filename)\n\t\n\n    # Back up the hypernetwork's attributes\n    hypernetwork_attributes = {}\n    for attribute in dir(hypernetwork):\n        if not attribute.startswith(\"__\") and not callable(getattr(hypernetwork, attribute)):\n            hypernetwork_attributes[attribute] = getattr(hypernetwork, attribute)\n\n    # Update the hypernetwork with the new checkpoint data\n    setattr(hypernetwork, hypernetwork_name, checkpoint)\n\n    # Save the hypernetwork\n    try:\n        hypernetwork.save(filename)\n    except Exception as e:\n        # Restore the hypernetwork's attributes\n        for attribute in dir(hypernetwork):\n            if not attribute.startswith(\"__\") and not callable(getattr(hypernetwork, attribute)):\n                setattr(hypernetwork, attribute, hypernetwork_attributes[attribute])\n\n        # Reraise the exception\n        raise e\n\n", "CodeBleu_score": 28.97289089704674}, {"code": "def save_hypernetwork(hypernetwork, checkpoint, hypernetwork_name, filename)\n\t\n    hypernetwork_attrs = {attr: getattr(hypernetwork, attr) for attr in dir(hypernetwork) if not attr.startswith('_')}\n    hypernetwork_attrs['checkpoint'] = checkpoint\n    hypernetwork_attrs['hypernetwork_name'] = hypernetwork_name\n    hypernetwork_attrs['filename'] = filename\n    for attr in hypernetwork_attrs:\n        setattr(hypernetwork, attr, hypernetwork_attrs[attr])\n    try:\n        hypernetwork.save()\n    except:\n        for attr in hypernetwork_attrs:\n            setattr(hypernetwork, attr, hypernetwork_attrs[attr])\n        raise\n    for attr in hypernetwork_attrs:\n        setattr(hypernetwork, attr, hypernetwork_attrs[attr])\n\n", "CodeBleu_score": 28.356177935144217}, {"code": "def save_hypernetwork(hypernetwork, checkpoint, hypernetwork_name, filename)\n\t\n    # Save the hypernetwork's attributes\n    hypernetwork_attributes = dict(hypernetwork.__dict__)\n    # Update the hypernetwork's attributes with the checkpoint information\n    hypernetwork.__dict__ = checkpoint\n    # Save the hypernetwork\n    try:\n        hypernetwork.save(filename)\n    except:\n        # Restore the hypernetwork's attributes\n        hypernetwork.__dict__ = hypernetwork_attributes\n        # Re-raise the exception\n        raise\n    # Restore the hypernetwork's attributes\n    hypernetwork.__dict__ = hypernetwork_attributes\n\n", "CodeBleu_score": 21.393474792229753}]}
{"instance_id": "3175f835140f", "generation_result": [{"code": "def ema_scope(self, context=None)\n\t\n    if context is None:\n        context = {}\n    context[\"ema\"] = self.ema\n    context[\"ema_enabled\"] = self.ema_enabled\n    context[\"ema_initialized\"] = self.ema_initialized\n    context[\"ema_weights\"] = self.ema_weights\n    context[\"ema_weights_names\"] = self.ema_weights_names\n    context[\"ema_weights_names_prefix\"] = self.ema_weights_names_prefix\n    context[\"ema_weights_names_suffix\"] = self.ema_weights_names_suffix\n    context[\"ema_weights_names_regex\"] = self.ema_weights_names_regex\n    context[\"ema_weights_names_regex_pattern\"] = self.ema_weights_names_regex_pattern\n    context[\"ema_weights_names_regex_flags\"] = self.ema_weights_names_regex_flags\n    context[\"ema_weights_names_regex_group\"] = self.ema_weights_names_regex_group\n    context[\"ema_weights_names_regex_group_index\"] = self.ema_weights_names_regex_group_index\n    context[\"ema_weights_names_regex_group_name\"] = self.ema_weights_names_regex_group_name\n    context[\"ema_weights_names_regex_group_names\"] = self.ema_weights_names_regex_group_names\n    context[\"ema_weights_names_regex_groups\"] = self.ema_weights_names_regex_groups\n    context[\"ema_weights_names_regex_groups_count\"] = self.ema_weights_names_regex_groups_count\n    context[\"ema_weights_names_regex_groups_indices\"] = self.ema_weights_names_regex_groups_indices\n    context[\"ema_weights_names_regex_groups_names\"] = self.ema_weights_names_regex_groups_names\n    context[\"ema_weights_names_regex_groups_offsets\"] = self.ema_weights_names_regex_groups_offsets\n    context[\"ema_weights", "CodeBleu_score": 19.321647521346197}, {"code": "def ema_scope(self, context=None)\n\t\n    # TODO: add docstring\n    if context is None:\n        context = {}\n    if context.get('ema_enabled', False):\n        # TODO: add docstring\n        if context.get('ema_scope_depth', 0) == 0:\n            # TODO: add docstring\n            context['ema_scope_depth'] = 1\n            context['ema_scope_last_restore'] = context['ema_scope_last_store'] = None\n            context['ema_scope_last_store'] = context['ema_scope_last_restore'] = None\n            context['ema_scope_last_store_params'] = context['ema_scope_last_restore_params'] = None\n            context['ema_scope_last_store_ema_params'] = context['ema_scope_last_restore_ema_params'] = None\n            context['ema_scope_last_store_ema_params_dict'] = context['ema_scope_last_restore_ema_params_dict'] = None\n            context['ema_scope_last_store_ema_params_dict_ema'] = context['ema_scope_last_restore_ema_params_dict_ema'] = None\n            context['ema_scope_last_store_ema_params_dict_ema_ema'] = context['ema_scope_last_restore_ema_params_dict_ema_ema'] = None\n            context['ema_scope_last_store_ema_params_dict_ema_ema_ema'] = context['ema_scope_last_restore_ema_params_dict_ema_ema_ema'] = None\n            context['ema_scope_last_store_ema_params_dict_ema_ema_ema_ema'] = context['ema_scope_last_restore_ema_params_dict_ema_ema_ema_ema'] = None\n            context['ema_scope_last_store_ema_params_dict_ema_ema_ema_ema_ema'] = context['ema_scope_last_restore_ema_params_dict_ema_ema_ema_ema_ema'] = None\n            context['ema_scope_last_store_ema_params_dict_ema_ema_ema_ema_", "CodeBleu_score": 19.987682906243343}, {"code": "def ema_scope(self, context=None)\n\t\n    if context is None:\n        context = {}\n\n    if context.get('ema_scope_enabled', False):\n        raise Exception(\"ema_scope() cannot be nested.\")\n\n    context['ema_scope_enabled'] = True\n    context['ema_scope_original_weights'] = {}\n    context['ema_scope_ema_weights'] = {}\n    context['ema_scope_ema_weights_copied'] = False\n\n    if self.ema is not None:\n        for name, param in self.ema.named_parameters():\n            if param.requires_grad:\n                context['ema_scope_original_weights'][name] = param.data.clone()\n                context['ema_scope_ema_weights'][name] = self.ema.state[name]['momentum_buffer'].clone()\n\n        if len(context['ema_scope_original_weights']) > 0:\n            self.ema.update()\n            context['ema_scope_ema_weights_copied'] = True\n\n    try:\n        yield\n    finally:\n        if context['ema_scope_ema_weights_copied']:\n            for name, param in self.ema.named_parameters():\n                if param.requires_grad:\n                    param.data.copy_(context['ema_scope_original_weights'][name])\n\n        context['ema_scope_enabled'] = False\n\n", "CodeBleu_score": 24.723068632870294}]}
{"instance_id": "a7b5d5cd519a", "generation_result": [{"code": "def p_mean_variance(self, x, t, clip_denoised: bool)\n\t\n    x_recon = self.model(x, t, self.context, self.num_timesteps, self.q_posterior)\n    if self.predict_start_from_noise:\n        x_recon = x_recon + self.q_posterior.sample()\n    if clip_denoised:\n        x_recon = x_recon.clamp_(-1, 1)\n    return x_recon, self.q_posterior.variance, self.q_posterior.log_variance\n\n", "CodeBleu_score": 29.75979963290996}, {"code": "def p_mean_variance(self, x, t, clip_denoised: bool)\n\t\n    # Get the model output for the input and time step\n    x_model = self.model(x, t, self.context, self.mask)\n    # Reconstruct the input either directly or from predicted noise\n    if self.predict_start_from_noise:\n        x_recon = self.q_posterior.sample(x_model)\n    else:\n        x_recon = x_model\n    # Clip the reconstructed input to a range of -1 to 1\n    if clip_denoised:\n        x_recon = x_recon.clamp_(-1, 1)\n    # Calculate and return the model mean, posterior variance, and posterior log variance\n    return x_model, self.q_posterior.variance.exp(), (x_recon - x_model).pow(2).mul(0.5).log_()\n\n", "CodeBleu_score": 30.37102770242978}, {"code": "def p_mean_variance(self, x, t, clip_denoised: bool)\n\t\n    # Get the model output for the input and time step\n    x_hat = self.model(x, t, deterministic=True)\n    # Reconstruct the input either directly or from predicted noise\n    if self.predict_start_from_noise:\n        x_recon = x_hat + self.q_posterior.sample(clip_denoised=clip_denoised)\n    else:\n        x_recon = x_hat\n    # Calculate and return the model mean, posterior variance, and posterior log variance\n    return x_hat, self.q_posterior.variance, self.q_posterior.log_variance\n\n", "CodeBleu_score": 26.571978879325}]}
{"instance_id": "a8652fa78473", "generation_result": [{"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss_type == 'l1':\n        loss = torch.abs(pred - target)\n        if mean:\n            loss = loss.mean()\n        return loss\n    elif self.loss_type == 'l2':\n        loss = torch.nn.MSELoss(reduction='none')(pred, target)\n        if mean:\n            loss = loss.mean()\n        return loss\n    else:\n        raise NotImplementedError\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 55.80803146357861}, {"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss_type == 'l1':\n        loss = torch.abs(pred - target)\n        if mean:\n            loss = loss.mean()\n        return loss\n    elif self.loss_type == 'l2':\n        loss = F.mse_loss(pred, target, reduction='none')\n        if mean:\n            loss = loss.mean()\n        return loss\n    else:\n        raise NotImplementedError\n\n", "CodeBleu_score": 54.472077957209365}, {"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss_type == 'l1':\n        loss = torch.abs(pred - target)\n        if mean:\n            loss = loss.mean()\n        return loss\n    elif self.loss_type == 'l2':\n        loss = (pred - target) ** 2\n        if not mean:\n            return loss\n        return loss.mean()\n    else:\n        raise NotImplementedError\n\n", "CodeBleu_score": 44.77232491092874}]}
{"instance_id": "d68e4f061349", "generation_result": [{"code": "def _prior_bpd(self, x_start)\n\t\n    t = torch.tensor(self.seq_len, dtype=torch.int64)\n    t = t.to(x_start.device)\n    t = t.unsqueeze(0)\n    t = t.expand_as(x_start)\n    t = t.view(-1)\n    mean_flat, log_var_flat = self.q_mean_variance(x_start, t)\n    kl = normal_kl(mean_flat, log_var_flat, torch.zeros_like(mean_flat), torch.ones_like(log_var_flat))\n    kl = kl.view(-1, self.seq_len)\n    kl = kl.sum(dim=1)\n    kl = kl.mean()\n    return kl\n\n", "CodeBleu_score": 22.573410299309653}, {"code": "def _prior_bpd(self, x_start)\n\t\n    t = x_start.size(1) - 1\n    mean_flat, variance_flat = self.q_mean_variance(x_start, t)\n    mean_flat = mean_flat.view(-1, self.latent_dim)\n    variance_flat = variance_flat.view(-1, self.latent_dim)\n    kl = normal_kl(mean_flat, variance_flat, torch.zeros_like(mean_flat), torch.ones_like(variance_flat))\n    kl = torch.sum(kl, dim=1)\n    return torch.mean(kl) / np.log(2)\n\n", "CodeBleu_score": 37.020985230862216}, {"code": "def _prior_bpd(self, x_start)\n\t\n    batch_size = x_start.shape[0]\n    t = torch.tensor(self.num_timesteps - 1, dtype=torch.float32, device=x_start.device)\n    q_mean_variance = self.q_mean_variance(x_start, t)\n    mean_flat = q_mean_variance[0].view(batch_size, -1)\n    log_variance_flat = q_mean_variance[1].view(batch_size, -1)\n    normal_kl = normal_kl_diag(mean_flat, log_variance_flat)\n    return normal_kl.mean() / np.log(2)\n\n", "CodeBleu_score": 29.625293037120926}]}
{"instance_id": "c695d93fdf2d", "generation_result": [{"code": "def make_cache(subsection: str) -> diskcache.Cache\n\t\n    return diskcache.Cache(\n        f\"{os.path.dirname(os.path.realpath(__file__))}/cache/{subsection}\",\n        size_limit=4 * 1024 * 1024 * 1024,\n        min_file_size=256 * 1024,\n    )\n\n", "CodeBleu_score": 53.81067704803848}, {"code": "def make_cache(subsection: str) -> diskcache.Cache\n\t\n\n    cache_dir = os.path.join(os.path.expanduser('~'), '.cache', 'cogs', 'cogs.cogs.utils.cache')\n\n    if not os.path.exists(cache_dir):\n        os.makedirs(cache_dir)\n\n    cache = diskcache.Cache(os.path.join(cache_dir, subsection))\n    cache.max_size = 4 * 1024 ** 3\n    cache.min_file_size = 256 * 1024\n\n    return cache\n\n", "CodeBleu_score": 48.20318956554367}, {"code": "def make_cache(subsection: str) -> diskcache.Cache\n\t\n    cache_path = os.path.join(CACHE_DIR, subsection)\n    cache = diskcache.Cache(\n        cache_path,\n        size_limit=4 * 1024 * 1024 * 1024,\n        min_file_size=256 * 1024,\n    )\n    return cache\n\n", "CodeBleu_score": 54.35173828681652}]}
{"instance_id": "ed4c543f0d38", "generation_result": [{"code": "def cache(subsection)\n\t\n    global caches\n    if caches.get(subsection) is None:\n        with cache_lock:\n            if not os.path.exists(cache_dir):\n                os.makedirs(cache_dir)\n            if not isfile(cache_filename):\n                convert_old_cached_data()\n            if caches.get(subsection) is None:\n                caches[subsection] = make_cache(subsection)\n    return caches[subsection]\n\n", "CodeBleu_score": 47.301538261140976}, {"code": "def cache(subsection)\n\t\n    global caches\n    global cache_lock\n    global cachefile\n    global cachepath\n    global cachename\n    global cachetype\n    global cachetag\n    global cachemode\n    global cachetime\n    global cachelock\n    global cachetimelock\n    global cachelock_timeout\n    global cachelock_sleep\n    global cachelock_retry\n    global cachelock_retry_sleep\n    global cachelock_retry_timeout\n    global cachelock_retry_max\n    global cachelock_retry_max_sleep\n    global cachelock_retry_max_timeout\n    global cachelock_retry_max_retry\n    global cachelock_retry_max_retry_sleep\n    global cachelock_retry_max_retry_timeout\n    global cachelock_retry_max_retry_max\n    global cachelock_retry_max_retry_max_sleep\n    global cachelock_retry_max_retry_max_timeout\n    global cachelock_retry_max_retry_max_retry\n    global cachelock_retry_max_retry_max_retry_sleep\n    global cachelock_retry_max_retry_max_retry_timeout\n    global cachelock_retry_max_retry_max_retry_max\n    global cachelock_retry_max_retry_max_retry_max_sleep\n    global cachelock_retry_max_retry_max_retry_max_timeout\n    global cachelock_retry_max_retry_max_retry_max_retry\n    global cachelock_retry_max_retry_max_retry_max_retry_sleep\n    global cachelock_retry_max_retry_max_retry_max_retry_timeout\n    global cachelock_retry_max_retry_max_retry_max_retry_max\n   ", "CodeBleu_score": 3.949231092211783}, {"code": "def cache(subsection)\n\t\n\n    # Check if the cache directory exists and if the cache filename is a valid file. If these conditions are not met, it calls convert_old_cached_data() to convert legacy cached data.\n    if not isfile(cachedir + subsection + '.json'):\n        convert_old_cached_data(subsection)\n\n    # Re-check the caches dictionary for the subsection. If the cache object still does not exist, it creates a new cache object using make_cache(subsection) and stores it in the caches dictionary.\n    if subsection not in caches:\n        caches[subsection] = make_cache(subsection)\n\n    # Return the retrieved or newly created cache object.\n    return caches[subsection]\n\n", "CodeBleu_score": 21.797655582037617}]}
{"instance_id": "ccf5ea2d9fc1", "generation_result": [{"code": "def cached_data_for_file(subsection, title, filename, func)\n\t\n    existing_cache = cache(subsection)\n    mtime = getmtime(filename)\n    if title in existing_cache:\n        if mtime > existing_cache[title].get(\"mtime\"):\n            del existing_cache[title]\n    if title not in existing_cache:\n        data = func()\n        existing_cache[title] = {\"mtime\": mtime, \"data\": data}\n        dump_cache(subsection, existing_cache)\n        return data\n    else:\n        return existing_cache[title][\"data\"]\n\n", "CodeBleu_score": 29.29345996777888}, {"code": "def cached_data_for_file(subsection, title, filename, func)\n\t\n    cache = cache(subsection)\n    mtime = getmtime(filename)\n    if title in cache:\n        if cache[title][0] >= mtime:\n            return cache[title][1]\n    data = func()\n    cache[title] = (mtime, data)\n    dump_cache(subsection, cache)\n    return data\n\n", "CodeBleu_score": 16.85805167258468}, {"code": "def cached_data_for_file(subsection, title, filename, func)\n\t\n    cache = cache(subsection)\n    mtime = getmtime(filename)\n    entry = cache.get(title)\n    if entry:\n        if mtime > entry.get('mtime'):\n            entry.clear()\n        else:\n            return entry.get('data')\n    data = func()\n    entry = {'data': data, 'mtime': mtime}\n    cache[title] = entry\n    dump_cache(subsection, cache)\n    return data\n\n", "CodeBleu_score": 31.17304917969267}]}
{"instance_id": "e01a2d4f0ac2", "generation_result": [{"code": "def report(message: str, *, exc_info: bool = False) -> None\n\t\n\n    if exc_info:\n        record_exception()\n        traceback.print_exc()\n        print(\"---\")\n\n    print(\"***\", message, file=sys.stderr)\n\n", "CodeBleu_score": 35.985311964913414}, {"code": "def report(message: str, *, exc_info: bool = False) -> None\n\t\n    record_exception()\n    if exc_info:\n        print(message, file=sys.stderr)\n        traceback.print_exc(file=sys.stderr)\n        print(\"---\", file=sys.stderr)\n    else:\n        print(\"***\", message, file=sys.stderr)\n\n", "CodeBleu_score": 44.720280122916336}, {"code": "def report(message: str, *, exc_info: bool = False) -> None\n\t\n    record_exception()\n    print(message, file=sys.stderr)\n    if exc_info:\n        print(textwrap.indent(traceback.format_exc(), \"    \"), file=sys.stderr)\n    print(\"---\", file=sys.stderr)\n\n", "CodeBleu_score": 58.20228784832826}]}
{"instance_id": "125fb9d2320b", "generation_result": [{"code": "def extract_original_prompts(style: PromptStyle, prompt, negative_prompt)\n\t\n\n    if style.prompt == \"\" and style.negative_prompt == \"\":\n        return False, prompt, negative_prompt\n\n    if style.prompt != \"\":\n        prompt = extract_style_text_from_prompt(style.prompt, prompt)\n        if prompt == \"\":\n            return False, prompt, negative_prompt\n\n    if style.negative_prompt != \"\":\n        negative_prompt = extract_style_text_from_prompt(style.negative_prompt, negative_prompt)\n        if negative_prompt == \"\":\n            return False, prompt, negative_prompt\n\n    return True, prompt, negative_prompt\n\n", "CodeBleu_score": 57.96963086158667}, {"code": "def extract_original_prompts(style: PromptStyle, prompt, negative_prompt)\n\t\n    if not prompt and not negative_prompt:\n        return False, prompt, negative_prompt\n\n    if not style.prompt and not style.negative_prompt:\n        return False, prompt, negative_prompt\n\n    try:\n        positive_prompt = extract_style_text_from_prompt(style.prompt, prompt)\n    except Exception:\n        return False, prompt, negative_prompt\n\n    try:\n        negative_prompt = extract_style_text_from_prompt(style.negative_prompt, negative_prompt)\n    except Exception:\n        return False, prompt, negative_prompt\n\n    return True, positive_prompt, negative_prompt\n\n", "CodeBleu_score": 68.99975338512951}, {"code": "def extract_original_prompts(style: PromptStyle, prompt, negative_prompt)\n\t\n    if not prompt and not negative_prompt:\n        return False, prompt, negative_prompt\n    positive_prompt = extract_style_text_from_prompt(style, prompt)\n    if not positive_prompt:\n        return False, prompt, negative_prompt\n    negative_prompt = extract_style_text_from_prompt(style, negative_prompt)\n    if not negative_prompt:\n        return False, prompt, negative_prompt\n    return True, positive_prompt, negative_prompt\n\n", "CodeBleu_score": 53.26223684370199}]}
{"instance_id": "0c5f729fbce2", "generation_result": [{"code": "def random_partial(self, count, n_frames)\n\t\n    if not self.utterances:\n        self._load_utterances()\n    utterances = self.utterance_cycler.sample(count)\n    partials = []\n    for utt in utterances:\n        partials.append((utt, u.random_partial(utt, n_frames)))\n    return partials\n\n", "CodeBleu_score": 48.341488479250074}, {"code": "def random_partial(self, count, n_frames)\n\t\n    if not self.utterances:\n        self._load_utterances()\n    utterances = self.utterance_cycler.sample(count)\n    return [(u, u.random_partial(n_frames)) for u in utterances]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 55.89879214825755}, {"code": "def random_partial(self, count, n_frames)\n\t\n\n    if self.utterances is None:\n        self._load_utterances()\n\n    utterances = self.utterance_cycler.sample(count)\n    partials = [u.random_partial(n_frames) for u in utterances]\n    return [(u, p) for u, p in zip(utterances, partials)]\n\n", "CodeBleu_score": 67.08142862120138}]}
{"instance_id": "31e8b27b8f94", "generation_result": [{"code": "def normalize_volume(wav, target_dBFS, increase_only=False, decrease_only=False)\n\t\n    if increase_only and decrease_only:\n        raise ValueError(\"Cannot set increase_only and decrease_only to True at the same time.\")\n    wav_dBFS = 20 * np.log10(np.max(np.abs(wav)))\n    dBFS_change = target_dBFS - wav_dBFS\n    if (dBFS_change < 0 and increase_only) or (dBFS_change > 0 and decrease_only):\n        return wav\n    return wav * (10 ** (dBFS_change / 20))\n\n", "CodeBleu_score": 70.83341732088005}, {"code": "def normalize_volume(wav, target_dBFS, increase_only=False, decrease_only=False)\n\t\n    if increase_only and decrease_only:\n        raise ValueError(\"Cannot set increase_only and decrease_only to True at the same time.\")\n    if increase_only and decrease_only:\n        raise ValueError(\"Cannot set increase_only and decrease_only to True at the same time.\")\n    change_in_dBFS = target_dBFS - np.max(wav)\n    if change_in_dBFS < 0 and increase_only:\n        return wav\n    elif change_in_dBFS > 0 and decrease_only:\n        return wav\n    else:\n        return wav * (10 ** (change_in_dBFS / 20))\n\n", "CodeBleu_score": 48.28602828476319}, {"code": "def normalize_volume(wav, target_dBFS, increase_only=False, decrease_only=False)\n\t\n    if increase_only and decrease_only:\n        raise ValueError(\"Cannot set increase_only and decrease_only to True at the same time.\")\n    wav = np.array(wav)\n    rms = np.sqrt(np.mean(wav**2))\n    change_in_dBFS = target_dBFS - 20 * np.log10(rms)\n    if (change_in_dBFS < 0 and increase_only) or (change_in_dBFS > 0 and decrease_only):\n        return wav\n    return wav * (10**(change_in_dBFS / 20))\n\n", "CodeBleu_score": 64.685690314121}]}
{"instance_id": "2353831e9033", "generation_result": [{"code": "def load_model(weights_fpath: Path, device=None)\n\t\n    if device is None:\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Loading model from {weights_fpath} to {device}\")\n    _model = SpeakerEncoder().to(device)\n    _model.load_state_dict(torch.load(weights_fpath, map_location=device))\n    _model.eval()\n    print(f\"Loaded model trained to step {_model.step} to {device}\")\n    return _model\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 44.644644360653}, {"code": "def load_model(weights_fpath: Path, device=None)\n\t\n    if device is None:\n        if torch.cuda.is_available():\n            device = torch.device(\"cuda\")\n        else:\n            device = torch.device(\"cpu\")\n    _model = SpeakerEncoder().to(device)\n    _model.load_state_dict(torch.load(weights_fpath))\n    _model.eval()\n    print(f\"Loaded model from {weights_fpath} trained to step {_model.step} on {device}\")\n    return _model\n\n", "CodeBleu_score": 39.99278678789121}, {"code": "def load_model(weights_fpath: Path, device=None)\n\t\n    if device is None:\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    _model = SpeakerEncoder(device=device)\n    _model.load_state_dict(torch.load(weights_fpath))\n    _model.to(device)\n    _model.eval()\n    print(f\"Loaded model {weights_fpath.stem} trained to step {_model.step} from {weights_fpath}\")\n    return _model\n\n", "CodeBleu_score": 42.7886789453619}]}
{"instance_id": "3231ad480199", "generation_result": [{"code": "def embed_frames_batch(frames_batch)\n\t\n    if not _model:\n        raise Exception('Model not loaded.')\n    frames_batch_tensor = torch.from_numpy(frames_batch).to(_device)\n    embeddings = _model.forward(frames_batch_tensor).detach().cpu().numpy()\n    return embeddings\n\n", "CodeBleu_score": 62.36776301183753}, {"code": "def embed_frames_batch(frames_batch)\n\t\n    if _model is None:\n        raise Exception(\"Model not loaded!\")\n    frames_batch = torch.from_numpy(frames_batch).to(_device)\n    embeddings = _model.forward(frames_batch)\n    embeddings = embeddings.detach().cpu().numpy()\n    return embeddings\n\n", "CodeBleu_score": 65.10990897243778}, {"code": "def embed_frames_batch(frames_batch)\n\t\n    if not _model:\n        raise Exception(\"Model is not loaded.\")\n    frames_batch = torch.from_numpy(frames_batch).to(_device)\n    embeddings = _model.forward(frames_batch)\n    embeddings = embeddings.detach().cpu().numpy()\n    return embeddings\n\n", "CodeBleu_score": 56.99136797565616}]}
{"instance_id": "117a41716c8d", "generation_result": [{"code": "def compute_partial_slices(n_samples, partial_utterance_n_frames=partials_n_frames,\n                           min_pad_coverage=0.75, overlap=0.5)\n\t\n    assert partial_utterance_n_frames > 0, \"partial_utterance_n_frames must be greater than 0\"\n    assert min_pad_coverage > 0, \"min_pad_coverage must be greater than 0\"\n    assert min_pad_coverage < 1, \"min_pad_coverage must be less than 1\"\n    assert overlap > 0, \"overlap must be greater than 0\"\n    assert overlap < 1, \"overlap must be less than 1\"\n    assert n_samples > 0, \"n_samples must be greater than 0\"\n    n_frames = int(np.ceil(n_samples / sampling_rate))\n    frame_step = int(np.round(sampling_rate * overlap))\n    wav_slices = []\n    mel_slices = []\n    for i in range(0, n_frames - partial_utterance_n_frames + 1, frame_step):\n        wav_slices.append(slice(i, i + partial_utterance_n_frames))\n        mel_slices.append(slice(i * hop_size, (i + partial_utterance_n_frames) * hop_size))\n    if len(wav_slices) == 1:\n        return wav_slices, mel_slices\n    else:\n        last_slice = wav_slices[-1]\n        last_mel_slice = mel_slices[-1]\n        if (last_slice.stop - last_slice.start) / sampling_rate < min_pad_coverage:\n            wav_slices = wav_slices[:-1]\n            mel_slices = mel_slices[:-1]\n        return wav_slices, mel_slices\n\n", "CodeBleu_score": 35.28345519000563}, {"code": "def compute_partial_slices(n_samples, partial_utterance_n_frames=partials_n_frames,\n                           min_pad_coverage=0.75, overlap=0.5)\n\t\n    assert partial_utterance_n_frames >= 1, \"partial_utterance_n_frames must be >= 1\"\n    assert min_pad_coverage >= 0.0, \"min_pad_coverage must be >= 0\"\n    assert min_pad_coverage <= 1.0, \"min_pad_coverage must be <= 1\"\n    assert overlap >= 0.0, \"overlap must be >= 0\"\n    assert overlap <= 1.0, \"overlap must be <= 1\"\n    # Compute the number of frames and the frame step\n    n_frames = int(np.ceil(n_samples / (partial_utterance_n_frames * sampling_rate)))\n    frame_step = int(np.round(partial_utterance_n_frames * sampling_rate * (1 - overlap)))\n    # Compute the slices\n    wav_slices = []\n    mel_slices = []\n    for i in range(n_frames):\n        wav_slices.append(slice(i * frame_step, (i + 1) * frame_step))\n        mel_slices.append(slice(i * frame_step, (i + 1) * frame_step))\n    # Check if we need to pad the last slice\n    if len(wav_slices) > 1:\n        last_slice = wav_slices[-1]\n        last_mel_slice = mel_slices[-1]\n        if (last_slice.stop - last_slice.start) / sampling_rate < min_pad_coverage:\n            # Remove the last slice\n            wav_slices = wav_slices[:-1]\n            mel_slices = mel_slices[:-1]\n    return wav_slices, mel_slices\n\n", "CodeBleu_score": 36.040000204830804}, {"code": "def compute_partial_slices(n_samples, partial_utterance_n_frames=partials_n_frames,\n                           min_pad_coverage=0.75, overlap=0.5)\n\t\n    assert partial_utterance_n_frames >= 1\n    assert min_pad_coverage >= 0\n    assert min_pad_coverage <= 1\n    assert overlap >= 0\n    assert overlap <= 1\n    n_frames = int(np.ceil(n_samples / float(partial_utterance_n_frames)))\n    frame_step = int(np.round(partial_utterance_n_frames * overlap))\n    wav_slices = []\n    mel_slices = []\n    for i in range(n_frames):\n        wav_slices.append(slice(i * partial_utterance_n_frames, (i + 1) * partial_utterance_n_frames))\n        mel_slices.append(slice(i * (partial_utterance_n_frames - frame_step), (i + 1) * (partial_utterance_n_frames - frame_step)))\n    coverage = 1 - (wav_slices[-1].stop - wav_slices[-1].start) / float(n_samples)\n    if coverage < min_pad_coverage and len(wav_slices) > 1:\n        wav_slices = wav_slices[:-1]\n        mel_slices = mel_slices[:-1]\n    return wav_slices, mel_slices\n\n", "CodeBleu_score": 37.890045708188666}]}
{"instance_id": "34fe5c864f1c", "generation_result": [{"code": "def plot_embedding_as_heatmap(embed, ax=None, title=\"\", shape=None, color_range=(0, 0.30))\n\t\n    if ax is None:\n        ax = plt.gca()\n    if shape is None:\n        shape = int(np.sqrt(embed.shape[0]))\n    embed = embed.reshape(shape, shape)\n    sm = cm.ScalarMappable(cmap=cm.get_cmap(\"RdBu\"))\n    sm.set_clim(color_range)\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.set_title(title)\n    ax.imshow(embed, cmap=cm.get_cmap(\"RdBu\"))\n    plt.colorbar(sm)\n\n", "CodeBleu_score": 48.91453309877451}, {"code": "def plot_embedding_as_heatmap(embed, ax=None, title=\"\", shape=None, color_range=(0, 0.30))\n\t\n    if ax is None:\n        ax = plt.gca()\n    if shape is None:\n        shape = (int(np.sqrt(len(embed))), int(np.sqrt(len(embed))))\n    embed = embed.reshape(shape)\n    sm = plt.cm.ScalarMappable(cmap=plt.get_cmap('RdBu'))\n    sm.set_clim(*color_range)\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.set_title(title)\n    ax.imshow(embed, cmap=plt.get_cmap('RdBu'), interpolation='nearest')\n    cbar = plt.colorbar(sm)\n    cbar.ax.set_ylabel('')\n    cbar.ax.set_yticklabels([])\n    return ax\n\n", "CodeBleu_score": 56.76473763346582}, {"code": "def plot_embedding_as_heatmap(embed, ax=None, title=\"\", shape=None, color_range=(0, 0.30))\n\t\n    if ax is None:\n        ax = plt.gca()\n    if shape is None:\n        shape = (int(np.sqrt(len(embed))), int(np.sqrt(len(embed))))\n    embed = embed.reshape(shape)\n    sm = cm.ScalarMappable(cmap=cm.get_cmap(\"RdBu\"), norm=plt.Normalize(color_range[0], color_range[1]))\n    ax.imshow(embed, interpolation=\"nearest\", cmap=sm.cmap)\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.set_title(title)\n    sm.set_clim(color_range[0], color_range[1])\n    plt.colorbar(sm, ax=ax)\n\n", "CodeBleu_score": 59.40828104777981}]}
{"instance_id": "92e93d50b62b", "generation_result": [{"code": "def similarity_matrix(self, embeds)\n\t\n    # number of speakers per batch\n    num_speakers = embeds.shape[0]\n    # number of utterances per speaker\n    num_utts = embeds.shape[1]\n    # initialize similarity matrix\n    sim_matrix = torch.zeros(num_speakers, num_speakers).to(embeds.device)\n    # initialize mask matrix\n    mask_matrix = torch.eye(num_speakers).to(embeds.device)\n    # compute centroids\n    centroids_incl = torch.mean(embeds, dim=1)\n    centroids_excl = embeds.clone()\n    centroids_excl[:, 0] = torch.mean(embeds[:, 1:], dim=1)\n    # for each speaker\n    for i in range(num_speakers):\n        # compute similarity scores\n        sim_scores_incl = torch.sum(torch.div(torch.pow(torch.sub(centroids_incl, embeds[i]), 2), self.sigma_sq), dim=1)\n        sim_scores_excl = torch.sum(torch.div(torch.pow(torch.sub(centroids_excl[i], embeds[i]), 2), self.sigma_sq), dim=1)\n        # update similarity matrix\n        sim_matrix = sim_matrix.masked_fill(mask_matrix[i], sim_scores_incl)\n        sim_matrix = sim_matrix.masked_fill(mask_matrix[i, :i], sim_scores_excl)\n        sim_matrix = sim_matrix.masked_fill(mask_matrix[i, i + 1:], sim_scores_excl)\n    # scale and bias similarity matrix\n    sim_matrix = torch.div(torch.add(torch.mul(sim_matrix, self.scale), self.bias), self.scale)\n    return sim_matrix\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 31.972459385249337}, {"code": "def similarity_matrix(self, embeds)\n\t\n    n_speakers = embeds.shape[0]\n    n_utts = embeds.shape[1]\n    centroids_incl = torch.zeros(n_speakers, self.n_clusters, embeds.shape[-1], device=embeds.device)\n    centroids_excl = torch.zeros(n_speakers, self.n_clusters, embeds.shape[-1], device=embeds.device)\n    mask = torch.eye(n_speakers, device=embeds.device).bool()\n    for i in range(n_speakers):\n        for j in range(self.n_clusters):\n            centroids_incl[i, j] = torch.mean(embeds[i, :, :], dim=0)\n            centroids_excl[i, j] = torch.mean(embeds[i, :, :] - embeds[i, j, :], dim=0)\n    sim_matrix = torch.zeros(n_speakers, n_speakers, device=embeds.device)\n    for i in range(n_speakers):\n        for j in range(n_speakers):\n            if mask[i, j]:\n                sim_matrix[i, j] = torch.sum(torch.abs(embeds[i, :, :] - centroids_incl[j])) / torch.norm(centroids_incl[j])\n                sim_matrix[i, j] += torch.sum(torch.abs(embeds[i, :, :] - centroids_excl[j])) / torch.norm(centroids_excl[j])\n    sim_matrix = (sim_matrix - sim_matrix.min()) / (sim_matrix.max() - sim_matrix.min())\n    sim_matrix = sim_matrix * self.scale + self.bias\n    return sim_matrix\n\n", "CodeBleu_score": 33.80896632432619}, {"code": "def similarity_matrix(self, embeds)\n\t\n    # Calculate the number of speakers and utterances per speaker\n    speakers, utterances = embeds.shape[0], embeds.shape[1]\n    # Initialize a similarity matrix with zeros on the specified device\n    sim_matrix = torch.zeros(speakers, speakers, device=embeds.device)\n    # Initialize a mask matrix to exclude self-similarity\n    mask = torch.eye(speakers, dtype=torch.bool, device=embeds.device)\n    # Compute centroids for each speaker\n    centroids_incl = embeds.mean(dim=1, keepdim=True)\n    centroids_excl = embeds.clone()\n    centroids_excl[:, 0] = 0\n    centroids_excl = centroids_excl.mean(dim=1, keepdim=True)\n    # Compute the similarity scores for each speaker\n    for i in range(speakers):\n        for j in range(speakers):\n            # Compute the similarity score for the current speaker pair\n            sim_score = torch.mean(torch.norm(centroids_incl[i] - embeds[j], dim=1))\n            # Update the similarity matrix\n            sim_matrix[i][j] = sim_score\n            # Update the mask matrix\n            mask[i][j] = False\n    # Scale and bias the similarity matrix\n    sim_matrix = (sim_matrix - sim_matrix.min()) / (sim_matrix.max() - sim_matrix.min())\n    sim_matrix = sim_matrix * 0.5 + 0.5\n    # Return the similarity matrix\n    return sim_matrix, mask\n\n", "CodeBleu_score": 29.53842463810587}]}
{"instance_id": "a6eba54108a1", "generation_result": [{"code": "def loss(self, embeds)\n\t\n    # determine number of speakers and utterances per speaker\n    n_speakers = embeds.shape[0]\n    n_utterances = embeds.shape[1]\n    # compute similarity matrix\n    sim_matrix = self.similarity_matrix(embeds)\n    sim_matrix = sim_matrix.reshape(n_speakers, n_utterances, n_utterances)\n    sim_matrix = sim_matrix.detach()\n    # compute ground truth labels\n    labels = torch.from_numpy(np.eye(n_utterances, n_utterances)).float()\n    labels = labels.to(self.device)\n    # compute loss\n    loss = self.loss_fn(sim_matrix, labels)\n    # compute EER\n    preds = inv_argmax(sim_matrix, dim=1)\n    preds = preds.cpu().numpy()\n    labels = labels.cpu().numpy()\n    fpr, tpr, thresholds = roc_curve(labels, preds)\n    eer = brentq(lambda x : 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n    return loss, eer\n\n\n", "CodeBleu_score": 26.85960703876663}, {"code": "def loss(self, embeds)\n\t\n    num_speakers = embeds.shape[0]\n    num_utterances = embeds.shape[1]\n    # Calculate similarity matrix\n    sim_matrix = torch.from_numpy(self.similarity_matrix).to(embeds.device)\n    sim_matrix = sim_matrix.reshape(num_speakers, num_utterances, num_speakers)\n    sim_matrix = sim_matrix.detach()\n    # Generate ground truth labels\n    labels = torch.from_numpy(np.eye(num_speakers)).to(embeds.device)\n    labels = labels.repeat(num_utterances, 1, 1)\n    # Calculate loss\n    loss = self.loss_fn(embeds, labels, sim_matrix)\n    # Calculate EER\n    preds = inv_argmax(sim_matrix, dim=2)\n    preds = preds.flatten()\n    labels = labels.flatten()\n    fpr, tpr, thresholds = roc_curve(labels.cpu(), preds.cpu())\n    eer = brentq(lambda x: 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n    return loss, eer\n\n", "CodeBleu_score": 28.65595411986403}, {"code": "def loss(self, embeds)\n\t\n    # Get the number of speakers and utterances per speaker from the shape of the input embeddings\n    num_speakers, num_utterances = embeds.shape[0], embeds.shape[1]\n\n    # Compute the similarity matrix\n    sim_matrix = self.similarity_matrix(embeds)\n\n    # Reshape the similarity matrix\n    sim_matrix = sim_matrix.reshape(num_speakers, num_utterances, num_utterances)\n\n    # Generate ground truth labels\n    labels = torch.from_numpy(np.eye(num_utterances)).long()\n\n    # Calculate the loss\n    loss = self.loss_fn(sim_matrix, labels)\n\n    # Calculate the Equal Error Rate (EER)\n    preds = inv_argmax(sim_matrix, axis=2)\n    fpr, tpr, thresholds = roc_curve(labels.flatten(), preds.flatten())\n    eer = brentq(lambda x: 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n\n    return loss, eer\n\n", "CodeBleu_score": 25.503821939559018}]}
{"instance_id": "ac7b74b9d1b1", "generation_result": [{"code": "def log_params(self)\n\t\n    if not self.disable_params:\n        # import parameter configurations\n        with open(os.path.join(self.path, \"configs\", \"params.json\"), \"r\") as f:\n            params = json.load(f)\n        # create string of parameter names and values\n        param_str = \"\"\n        for param in self.params:\n            param_str += f\"{params[param]}: {self.params[param]}", "CodeBleu_score": 12.52824657930811}, {"code": "def log_params(self)\n\t\n    if not self.disable_params:\n        if self.params is None:\n            self.params = {\n                \"model\": [],\n                \"data\": [],\n                \"other\": []\n            }\n        for k, v in self.model.named_parameters():\n            if v.requires_grad:\n                self.params[\"model\"].append((k, v.data.cpu().numpy()))\n        for k, v in self.data.named_parameters():\n            if v.requires_grad:\n                self.params[\"data\"].append((k, v.data.cpu().numpy()))\n        for k, v in self.named_parameters():\n            if v.requires_grad:\n                self.params[\"other\"].append((k, v.data.cpu().numpy()))\n        self.log_params_text()\n\n", "CodeBleu_score": 19.69732143867755}, {"code": "def log_params(self)\n\t\n    if not self.disable_params_logging:\n        # import parameter configurations\n        with open(os.path.join(os.path.dirname(os.path.abspath(__file__)), \"configurations/parameters.json\")) as f:\n            params = json.load(f)\n\n        # create a string of the model parameters\n        model_params = \"\"\n        for param in params[\"model\"]:\n            model_params += param + \": \" + str(getattr(self, param)) + \"", "CodeBleu_score": 14.740655873617403}]}
{"instance_id": "4726b7864386", "generation_result": [{"code": "def draw_projections(self, embeds, utterances_per_speaker, step, out_fpath=None, max_speakers=10)\n\t\n    if self.speaker_embedding_visualization_disabled:\n        return\n    if embeds.shape[0] > max_speakers:\n        embeds = embeds[:max_speakers]\n    embeds = embeds.reshape(embeds.shape[0], -1)\n    embeds = self.reducer.fit_transform(embeds)\n    fig = plt.figure()\n    plt.clf()\n    plt.title(f\"Step {step}\")\n    plt.scatter(embeds[:, 0], embeds[:, 1], c=utterances_per_speaker.astype(int), cmap=\"tab10\")\n    plt.gca().set_aspect(\"equal\")\n    plt.xlim([-1.5, 1.5])\n    plt.ylim([-1.5, 1.5])\n    plt.xticks(np.arange(-1.5, 1.5, step=0.5))\n    plt.yticks(np.arange(-1.5, 1.5, step=0.5))\n    if out_fpath is not None:\n        plt.savefig(out_fpath)\n    else:\n        plt.show()\n\n", "CodeBleu_score": 39.2643928730516}, {"code": "def draw_projections(self, embeds, utterances_per_speaker, step, out_fpath=None, max_speakers=10)\n\t\n    if self.disable_visualizations:\n        return\n    if embeds.shape[0] > max_speakers:\n        embeds = embeds[:max_speakers]\n    embeds = embeds.reshape(embeds.shape[0], -1)\n    reducer = umap.UMAP()\n    projections = reducer.fit_transform(embeds)\n    if self.current_step == 0:\n        self.projections = projections\n        self.speaker_colors = np.random.rand(embeds.shape[0])\n    else:\n        self.projections = np.vstack((self.projections, projections))\n        self.speaker_colors = np.vstack((self.speaker_colors, np.random.rand(embeds.shape[0])))\n    plt.clf()\n    plt.title(\"Step {}\".format(step))\n    plt.scatter(projections[:, 0], projections[:, 1], c=self.speaker_colors, cmap=\"tab20\")\n    plt.gca().set_aspect(\"equal\")\n    plt.xticks(np.arange(0, 1, 0.1))\n    plt.yticks(np.arange(0, 1, 0.1))\n    plt.xlabel(\"UMAP 1\")\n    plt.ylabel(\"UMAP 2\")\n    plt.savefig(out_fpath) if out_fpath else None\n    plt.pause(0.001)\n\n", "CodeBleu_score": 42.07895437515001}, {"code": "def draw_projections(self, embeds, utterances_per_speaker, step, out_fpath=None, max_speakers=10)\n\t\n    if embeds.shape[0] > max_speakers:\n        embeds = embeds[:max_speakers]\n    embeds = embeds.reshape((-1, embeds.shape[-1]))\n    reducer = umap.UMAP(random_state=42)\n    projections = reducer.fit_transform(embeds)\n    plt.clf()\n    plt.title(f\"Step {step}\")\n    plt.scatter(projections[:, 0], projections[:, 1], c=utterances_per_speaker, cmap=\"rainbow\", s=10)\n    plt.gca().set_aspect(\"equal\")\n    plt.xlim(np.arange(-1.1, 1.1, 0.05))\n    plt.ylim(np.arange(-1.1, 1.1, 0.05))\n    plt.xticks(np.arange(-1, 1, 0.5))\n    plt.yticks(np.arange(-1, 1, 0.5))\n    if not self.disable_visualizations:\n        plt.draw()\n        plt.pause(0.0001)\n    if out_fpath is not None:\n        plt.savefig(out_fpath, dpi=300)\n\n", "CodeBleu_score": 31.011313043674377}]}
{"instance_id": "274fa50985c0", "generation_result": [{"code": "def add_speaker_embedding(self, x, speaker_embedding)\n\t\n    # Save dimensions\n    x_dims = x.size()\n    speaker_embedding_dims = speaker_embedding.size()\n    # Determine index\n    if len(x_dims) == 3:\n        index = 1\n    elif len(x_dims) == 2:\n        index = 0\n    else:\n        raise ValueError(\"Unsupported input dimension.\")\n    # Repeat\n    speaker_embedding = speaker_embedding.repeat_interleave(x_dims[index], dim=0)\n    # Reshape and transpose\n    speaker_embedding = speaker_embedding.reshape(speaker_embedding_dims[0], -1).transpose(0, 1)\n    # Concatenate\n    x = torch.cat((x, speaker_embedding), dim=-1)\n    return x\n\n", "CodeBleu_score": 35.494814978819186}, {"code": "def add_speaker_embedding(self, x, speaker_embedding)\n\t\n    x_dims = [x.size(i) for i in range(x.dim())]\n    speaker_dims = [speaker_embedding.size(i) for i in range(speaker_embedding.dim())]\n    if len(speaker_dims) == 2:\n        speaker_dims = speaker_dims[1:]\n    speaker_dim_index = len(x_dims) - len(speaker_dims)\n    if speaker_dim_index < 0:\n        raise ValueError(f\"The speaker embedding has {len(speaker_dims)} dimensions, but the input tensor has only {len(x_dims)} dimensions.\")\n    if any(x_dims[speaker_dim_index:] != speaker_dims):\n        raise ValueError(f\"The speaker embedding has dimensions {speaker_dims}, but the input tensor has dimensions {x_dims} starting from the {speaker_dim_index}th dimension.\")\n    if x.dim() == 2:\n        speaker_embedding = speaker_embedding.unsqueeze(1)\n    elif x.dim() > 2:\n        speaker_embedding = speaker_embedding.repeat_interleave(repeats=x.size(1), dim=1)\n    speaker_embedding = speaker_embedding.reshape(speaker_embedding.size(0), *[1] * (x.dim() - 2), *speaker_embedding.size()[1:])\n    speaker_embedding = speaker_embedding.transpose(0, 1)\n    return torch.cat((x, speaker_embedding), dim=-1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 33.33883380653479}, {"code": "def add_speaker_embedding(self, x, speaker_embedding)\n\t\n    x_size = x.size()\n    x_dim_names = ['batch', 'seq', 'text']\n    speaker_embedding_dim_names = ['speaker', 'embedding']\n    speaker_embedding_dim_index = x_dim_names.index(speaker_embedding_dim_names[0])\n    speaker_embedding_dim_size = x_size[speaker_embedding_dim_index]\n    speaker_embedding_dim_size_repeated = speaker_embedding_dim_size * x_size[1]\n    speaker_embedding = speaker_embedding.repeat_interleave(x_size[1], dim=speaker_embedding_dim_index)\n    speaker_embedding = speaker_embedding.reshape(speaker_embedding_dim_size_repeated, -1)\n    speaker_embedding = speaker_embedding.transpose(0, 1)\n    x = torch.cat([x, speaker_embedding], dim=-1)\n    return x\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 14.55370842706014}]}
{"instance_id": "e6742ea0fe5e", "generation_result": [{"code": "def init_attention(self, encoder_seq_proj)\n\t\n    self.device = self.parameters().__next__().device\n    self.cum_attention = torch.zeros(encoder_seq_proj.size(0), encoder_seq_proj.size(1)).to(self.device)\n    self.attention = torch.zeros(encoder_seq_proj.size(0), encoder_seq_proj.size(1)).to(self.device)\n\n", "CodeBleu_score": 19.338282523301313}, {"code": "def init_attention(self, encoder_seq_proj)\n\t\n    self.device = self.parameters().__next__().device\n    self.batch_size = encoder_seq_proj.size(0)\n    self.seq_len = encoder_seq_proj.size(1)\n    self.cum_attn = torch.zeros(self.batch_size, self.seq_len, device=self.device)\n    self.attn = torch.zeros(self.batch_size, self.seq_len, device=self.device)\n\n", "CodeBleu_score": 24.08904814712331}, {"code": "def init_attention(self, encoder_seq_proj)\n\t\n    self.device = self.parameters().__next__().device\n    batch_size, seq_len, _ = encoder_seq_proj.size()\n    self.cum_attn = torch.zeros(batch_size, seq_len, seq_len).to(self.device)\n    self.attn = torch.zeros(batch_size, seq_len, seq_len).to(self.device)\n\n", "CodeBleu_score": 20.50678820288516}]}
{"instance_id": "4e1fc1ed0ed7", "generation_result": [{"code": "def load(self, path, optimizer=None)\n\t\n    if isinstance(path, str):\n        path = Path(path)\n    if path.exists():\n        checkpoint = torch.load(str(path), map_location=lambda storage, loc: storage)\n        self.load_state_dict(checkpoint['state_dict'])\n        if optimizer is not None:\n            optimizer.load_state_dict(checkpoint['optimizer'])\n        return checkpoint\n    else:\n        raise FileNotFoundError\n\n\n", "CodeBleu_score": 50.54201679627637}, {"code": "def load(self, path, optimizer=None)\n\t\n    checkpoint = torch.load(path, map_location=self.device)\n    self.load_state_dict(checkpoint['model_state_dict'])\n    if optimizer is not None:\n        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n    return checkpoint\n\n", "CodeBleu_score": 35.196841678405654}, {"code": "def load(self, path, optimizer=None)\n\t\n    path = Path(path)\n    checkpoint = torch.load(path, map_location=self.device)\n    self.load_state_dict(checkpoint['state_dict'])\n    if optimizer and 'optimizer' in checkpoint:\n        optimizer.load_state_dict(checkpoint['optimizer'])\n    return checkpoint\n\n", "CodeBleu_score": 37.94621069031126}]}
{"instance_id": "178e6aa16c0f", "generation_result": [{"code": "def save(self, path, optimizer=None)\n\t\n    if isinstance(path, Path):\n        path = str(path)\n    if optimizer is not None:\n        torch.save({\"state_dict\": self.state_dict(), \"optimizer\": optimizer.state_dict()}, path)\n    else:\n        torch.save(self.state_dict(), path)\n\n", "CodeBleu_score": 59.34303726578557}, {"code": "def save(self, path, optimizer=None)\n\t\n    if isinstance(path, Path):\n        path = str(path)\n    os.makedirs(os.path.dirname(path), exist_ok=True)\n    state_dict = self.state_dict()\n    if optimizer is not None:\n        optimizer_state_dict = optimizer.state_dict()\n        torch.save({\n            'state_dict': state_dict,\n            'optimizer': optimizer_state_dict\n        }, path)\n    else:\n        torch.save(state_dict, path)\n\n", "CodeBleu_score": 54.30978517884704}, {"code": "def save(self, path, optimizer=None)\n\t\n    torch.save({\n        'state_dict': self.state_dict(),\n        'optimizer': optimizer.state_dict() if optimizer else None\n    }, path)\n\n", "CodeBleu_score": 46.02584944451304}]}
{"instance_id": "6528fd80e87b", "generation_result": [{"code": "def english_cleaners(text)\n\t\n    text = convert_to_ascii(text)\n    text = lowercase(text)\n    text = expand_numbers(text)\n    text = expand_abbreviations(text)\n    text = collapse_whitespace(text)\n    return text\n", "CodeBleu_score": 55.88611500274403}, {"code": "def english_cleaners(text)\n\t\n    text = convert_to_ascii(text)\n    text = lowercase(text)\n    text = expand_numbers(text)\n    text = expand_abbreviations(text)\n    text = collapse_whitespace(text)\n    return text\n\n", "CodeBleu_score": 55.88611500274403}, {"code": "def english_cleaners(text)\n\t\n    text = collapse_whitespace(text)\n    text = lowercase(text)\n    text = expand_numbers(text)\n    text = expand_abbreviations(text)\n    text = convert_to_ascii(text)\n    return text\n\n", "CodeBleu_score": 50.663927467982894}]}
{"instance_id": "8a4fe08e0985", "generation_result": [{"code": "def text_to_sequence(text, cleaner_names)\n\t\n    sequence = []\n    for char in text:\n        if 65 <= ord(char) <= 90:\n            sequence += _arpabet_to_sequence(char)\n        else:\n            sequence += _symbols_to_sequence(char, cleaner_names)\n    sequence.append(_symbols_to_sequence(\"<EOS>\", cleaner_names))\n    return sequence\n\n", "CodeBleu_score": 25.21780606578674}, {"code": "def text_to_sequence(text, cleaner_names)\n\t\n    sequence = []\n    # Check for curly braces and treat their contents as ARPAbet:\n    while len(text):\n        m = _curly_re.match(text)\n        if not m:\n            sequence += _symbols_to_sequence(_clean_text(text, cleaner_names))\n            break\n        sequence += _symbols_to_sequence(_clean_text(m.group(1), cleaner_names))\n        sequence += _arpabet_to_sequence(m.group(3))\n        text = m.group(2) + text[m.end():]\n    # Append EOS token\n    sequence.append(_symbols_to_sequence(_clean_text(u'~', cleaner_names)))\n    return sequence\n\n", "CodeBleu_score": 68.81035984987771}, {"code": "def text_to_sequence(text, cleaner_names)\n\t\n    sequence = []\n    # Check for curly braces and treat their contents as ARPAbet:\n    while len(text):\n        m = _curly_re.match(text)\n        if not m:\n            sequence += _clean_text(text, cleaner_names)\n            break\n        sequence += _clean_text(m.group(1), cleaner_names)\n        sequence += _arpabet_to_sequence(m.group(3))\n        text = m.group(2)\n    # Append EOS token\n    sequence.append(_symbols_to_sequence[_EOS])\n    return sequence\n\n", "CodeBleu_score": 56.33692247622991}]}
{"instance_id": "e1c25a37af1f", "generation_result": [{"code": "def sequence_to_text(sequence)\n\t\n    result = ''\n    for symbol_id in sequence:\n        symbol = symbols[symbol_id]\n        if symbol == ' ':\n            result += ' '\n        elif symbol == '{' or symbol == '}':\n            result += ' ' + symbol + ' '\n        else:\n            result += symbol\n    return result.replace('}{', ' ')\n\n", "CodeBleu_score": 30.321271077522844}, {"code": "def sequence_to_text(sequence)\n\t\n    result = ''\n    for symbol_id in sequence:\n        if symbol_id == 0:\n            result += ' '\n        else:\n            symbol = id_to_symbol[symbol_id]\n            result += symbol\n    return result\n\n", "CodeBleu_score": 30.953043039034412}, {"code": "def sequence_to_text(sequence)\n\t\n    result = ''\n    for symbol_id in sequence:\n        if symbol_id in _symbols_to_text:\n            s = _symbols_to_text[symbol_id]\n            # Enclose ARPAbet back in curly braces:\n            if len(s) > 1 and s[0] == '{' and s[-1] == '}':\n                s = '{%s}' % s[1:-1]\n            result += s\n    return result.replace('}{', ' ')\n\n", "CodeBleu_score": 58.78978418736972}]}
{"instance_id": "7807cad4b05d", "generation_result": [{"code": "def embed_utterance(fpaths, encoder_model_fpath)\n\t\n    encoder = get_encoder(encoder_model_fpath)\n    wav = encoder.preprocess_wav(fpaths)\n    embedding = encoder.embed_utterance(wav)\n    np.save(fpaths, embedding)\n    return embedding\n\n", "CodeBleu_score": 18.29356484814597}, {"code": "def embed_utterance(fpaths, encoder_model_fpath)\n\t\n    if not encoder.is_loaded():\n        encoder.load_model(encoder_model_fpath)\n    wav = encoder.preprocess_wav(fpaths[0])\n    embedding = encoder.embed_utterance(wav)\n    np.save(fpaths[1], embedding)\n    return embedding\n\n", "CodeBleu_score": 51.87674044877978}, {"code": "def embed_utterance(fpaths, encoder_model_fpath)\n\t\n    if not encoder.is_loaded():\n        encoder.load_model(encoder_model_fpath)\n    wav = encoder.preprocess_wav(fpaths)\n    embedding = encoder.embed_utterance(wav)\n    np.save(fpaths, embedding)\n\n", "CodeBleu_score": 48.09864860302132}]}
{"instance_id": "9f860273a325", "generation_result": [{"code": "def create_embeddings(synthesizer_root: Path, encoder_model_fpath: Path, n_processes: int)\n\t\n\n    # Check if the required directories and files exist\n    assert synthesizer_root.exists()\n    assert encoder_model_fpath.exists()\n\n    # Create the output directory if it doesn't exist\n    embed_dir = synthesizer_root.joinpath(\"embeds\")\n    if not embed_dir.exists():\n        embed_dir.mkdir()\n\n    # Read the metadata file\n    metadata_fpath = synthesizer_root.joinpath(\"metadata.csv\")\n    assert metadata_fpath.exists()\n    metadata = pd.read_csv(metadata_fpath, index_col=0)\n\n    # Create a list of file paths for audio files\n    wav_dir = synthesizer_root.joinpath(\"wavs\")\n    assert wav_dir.exists()\n    wav_fpaths = list(wav_dir.rglob(\"*.wav\"))\n\n    # Create a list of file paths for embeddings\n    embed_fpaths = [embed_dir.joinpath(f\"{fpath.stem}.npy\") for fpath in wav_fpaths]\n\n    # Create a progress bar\n    pbar = tqdm(total=len(wav_fpaths), desc=\"Creating embeddings\")\n\n    # Create a multiprocessing pool\n    pool = Pool(n_processes)\n\n    # Create a partial function to generate embeddings\n    generate_embedding = partial(generate_embedding_from_file, encoder_model_fpath=encoder_model_fpath)\n\n    # Generate embeddings in parallel\n    for embed_fpath, embed in zip(embed_fpaths, pool.imap(generate_embedding, wav_fpaths)):\n        np.save(embed_fpath, embed)\n        pbar.update(1)\n\n    # Close the multiprocessing pool\n    pool.close()\n\n    # Update the progress bar\n    pbar.update(len(wav_fpaths) - pbar.n)\n    pbar.close()\n\n    # Update the metadata\n    metadata[\"embed_fpath\"] = embed_fpaths\n    metadata.to_", "CodeBleu_score": 45.69475687711856}, {"code": "def create_embeddings(synthesizer_root: Path, encoder_model_fpath: Path, n_processes: int)\n\t\n    # Check if the required directories and files exist.\n    if not synthesizer_root.exists():\n        raise RuntimeError(f\"Synthesizer root directory {synthesizer_root} does not exist. Please create it.\")\n    if not encoder_model_fpath.exists():\n        raise RuntimeError(f\"Encoder model file {encoder_model_fpath} does not exist. Please download it.\")\n    if not synthesizer_root.joinpath(\"audio\").exists():\n        raise RuntimeError(f\"Audio directory {synthesizer_root.joinpath('audio')} does not exist. Please create it.\")\n    if not synthesizer_root.joinpath(\"metadata.csv\").exists():\n        raise RuntimeError(f\"Metadata file {synthesizer_root.joinpath('metadata.csv')} does not exist. Please create it.\")\n    if not synthesizer_root.joinpath(\"embeddings\").exists():\n        synthesizer_root.joinpath(\"embeddings\").mkdir()\n    # Read metadata and prepare file paths for audio and embeddings.\n    metadata_fpath = synthesizer_root.joinpath(\"metadata.csv\")\n    with metadata_fpath.open(\"r\") as f:\n        metadata = [line.split() for line in f]\n    wav_dir = synthesizer_root.joinpath(\"audio\")\n    embed_dir = synthesizer_root.joinpath(\"embeddings\")\n    # Create a multiprocessing pool.\n    pool = Pool(n_processes)\n    # Create a progress bar.\n    pbar = tqdm(total=len(metadata))\n    # Process the audio files in parallel.\n    for _ in pool.imap(partial(create_embedding, encoder_model_fpath=encoder_model_fpath, wav_dir=wav_dir, embed_dir=embed_dir), metadata):\n        pbar.update(1)\n    # Close the pool and display the progress bar.\n    pool.close()\n    pbar.close()\n\n", "CodeBleu_score": 40.60975013901811}, {"code": "def create_embeddings(synthesizer_root: Path, encoder_model_fpath: Path, n_processes: int)\n\t\n\n    # Check existence of required directories and files.\n    if not synthesizer_root.exists():\n        raise FileNotFoundError(f\"The synthesizer root directory {synthesizer_root} does not exist.\")\n    if not encoder_model_fpath.exists():\n        raise FileNotFoundError(f\"The encoder model file {encoder_model_fpath} does not exist.\")\n    wav_dir = synthesizer_root.joinpath(\"wavs\")\n    if not wav_dir.exists():\n        raise FileNotFoundError(f\"The wavs directory {wav_dir} does not exist.\")\n    metadata_fpath = synthesizer_root.joinpath(\"train.txt\")\n    if not metadata_fpath.exists():\n        raise FileNotFoundError(f\"The metadata file {metadata_fpath} does not exist.\")\n    embed_dir = synthesizer_root.joinpath(\"embeds\")\n    if embed_dir.exists():\n        raise FileExistsError(f\"The embeds directory {embed_dir} already exists.\")\n\n    # Read metadata and prepare file paths for audio and embeddings.\n    with metadata_fpath.open(\"r\") as metadata_file:\n        metadata = metadata_file.readlines()\n    wav_fpaths = [wav_dir.joinpath(line.split()[0]) for line in metadata]\n    embed_fpaths = [embed_dir.joinpath(line.split()[0].replace(\".wav\", \".npy\")) for line in metadata]\n\n    # Create the embeds directory.\n    embed_dir.mkdir()\n\n    # Create the encoder model.\n    encoder = Encoder(encoder_model_fpath)\n\n    # Create a progress bar.\n    pbar = tqdm(total=len(metadata))\n\n    # Create a multiprocessing pool.\n    pool = Pool(n_processes)\n\n    # Process the audio files in parallel.\n    for _ in pool.imap_unordered(partial(process_audio_file, encoder=encoder), zip(wav_fpaths, embed_fpaths)):\n        pbar.update()\n\n    # Close", "CodeBleu_score": 46.89712444243144}]}
{"instance_id": "9eb0abda14a9", "generation_result": [{"code": "def eval_model(attention, mel_prediction, target_spectrogram, input_seq, step,\n               plot_dir, mel_output_dir, wav_dir, sample_num, loss, hparams)\n\t\n    # Save the attention plot\n    attention_plot = plot_attention(attention, target_spectrogram,\n                                     input_seq, step, plot_dir, hparams)\n\n    # Save the predicted mel spectrogram plot\n    mel_output_filename = 'mel-prediction-step-{}.png'.format(step)\n    mel_output_path = mel_output_dir.joinpath(mel_output_filename)\n    plot.plot_spectrogram(mel_prediction,\n                          mel_output_path,\n                          title='Predicted Mel Spectrogram',\n                          target_spectrogram=target_spectrogram,\n                          max_len=hparams.max_iters)\n\n    # Save the predicted mel spectrogram\n    mel_output_filename = 'mel-prediction-step-{}.npy'.format(step)\n    mel_output_path = mel_output_dir.joinpath(mel_output_filename)\n    np.save(mel_output_path, mel_prediction.T, allow_pickle=False)\n\n    # Save the predicted wav\n    wav_output_filename = 'audio-prediction-step-{}.wav'.format(step)\n    wav_output_path = wav_dir.joinpath(wav_output_filename)\n    audio.inv_mel_spectrogram(mel_prediction.T, hparams)\n    audio.save_wav(wav_output_path, mel_prediction.T, hparams)\n\n    # Save the ground truth mel spectrogram plot\n    mel_output_filename = 'mel-ground-truth-step-{}.png'.format(step)\n    mel_output_path = mel_output_dir.joinpath(mel_output_filename)\n    plot.plot_spectrogram(target_spectrogram,\n                          mel_output_path,\n                          title='Ground Truth Mel Spectrogram',\n                          target_spectrogram=target_spectrogram,\n                          max_len=hparams.max_iters)\n\n    # Save the sequence to text\n    seq_filename = 'seq-step-", "CodeBleu_score": 36.381682768393105}, {"code": "def eval_model(attention, mel_prediction, target_spectrogram, input_seq, step,\n               plot_dir, mel_output_dir, wav_dir, sample_num, loss, hparams)\n\t\n    # Get the model type\n    model_type = 'Tacotron' if hparams.tacotron_teacher_forcing else 'Tacotron-2'\n    # Create the directory for the current time if it doesn't exist\n    time_string = time.strftime('%Y-%m-%d-%H-%M', time.gmtime(time.time()))\n    plot_dir = plot_dir.joinpath(time_string)\n    plot_dir.mkdir(exist_ok=True)\n    # Save the attention plot\n    attention_plot.save_attention(attention, model_type, step, plot_dir, hparams)\n    # Save the predicted mel spectrogram to disk (debug)\n    if hparams.predict_linear:\n        mel_filename = 'mel-prediction-step-{}.npy'.format(step)\n        mel_prediction.T.tofile(mel_output_dir.joinpath(mel_filename).as_posix())\n    # Save the predicted mel spectrogram to disk (debug)\n    mel_filename = 'mel-prediction-step-{}.npy'.format(step)\n    mel_prediction.T.tofile(mel_output_dir.joinpath(mel_filename).as_posix())\n    # Save the predicted mel spectrogram to disk (debug)\n    mel_filename = 'mel-prediction-step-{}.npy'.format(step)\n    mel_prediction.T.tofile(mel_output_dir.joinpath(mel_filename).as_posix())\n    # Save the predicted mel spectrogram to disk (debug)\n    mel_filename = 'mel-prediction-step-{}.npy'.format(step)\n    mel_prediction.T.tofile(mel_output_dir.joinpath(mel_filename).as_posix())\n    # Save the predicted mel spectrogram to disk (debug)\n    mel_filename = 'mel-prediction-step-{}.npy'.format(step)\n    mel_prediction.T.tofile(mel_output_dir.joinpath(mel_filename).as_posix())\n    # Save the predicted", "CodeBleu_score": 32.33414426659015}, {"code": "def eval_model(attention, mel_prediction, target_spectrogram, input_seq, step,\n               plot_dir, mel_output_dir, wav_dir, sample_num, loss, hparams)\n\t\n    # Plot the alignment\n    plot_alignment(attention,\n                   os.path.join(plot_dir, 'step-{}-alignment.png'.format(step)),\n                   target_spectrogram,\n                   input_seq,\n                   hparams)\n\n    # Plot the spectrogram\n    plot_spectrogram(mel_prediction,\n                     os.path.join(mel_output_dir, 'step-{}.png'.format(step)),\n                     target_spectrogram,\n                     step,\n                     hparams)\n\n    # Save the predicted mel spectrogram\n    mel_filename = 'step-{}-mel-spectrogram.npy'.format(step)\n    mel_prediction_path = mel_output_dir.joinpath(mel_filename)\n    np.save(mel_prediction_path, mel_prediction.T, allow_pickle=False)\n\n    # Save the predicted mel spectrogram\n    mel_filename = 'step-{}-mel-spectrogram.npy'.format(step)\n    mel_prediction_path = mel_output_dir.joinpath(mel_filename)\n    np.save(mel_prediction_path, mel_prediction.T, allow_pickle=False)\n\n    # Save the predicted linear spectrogram\n    wav_prediction = audio.inv_mel_spectrogram(mel_prediction.T, hparams)\n    wav_prediction_path = wav_dir.joinpath('step-{}.wav'.format(step))\n    audio.save_wav(wav_prediction, wav_prediction_path, hparams.sample_rate)\n\n    # Save the attention plot\n    alignment_path = plot_dir.joinpath('step-{}-attention.png'.format(step))\n    plot_attention(attention, alignment_path, target_spectrogram,\n                    input_seq, hparams)\n\n    # Save the predicted mel spectrogram plot\n    mel_target_path = plot_dir.joinpath('step-{}-mel-target.png'.format(step))\n    plot_spectrogram(", "CodeBleu_score": 39.230710380917316}]}
{"instance_id": "6bad4f500286", "generation_result": [{"code": "def num_params(self)\n\t\n    num_params = sum([np.prod(p.size()) for p in self.parameters() if p.requires_grad])\n    num_params = num_params / 1000000\n    print('Trainable parameters: %.3f M' % num_params)\n\n", "CodeBleu_score": 34.02327195667307}, {"code": "def num_params(self)\n\t\n    num_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n    num_params = num_params / 1000000\n    print('Total number of trainable parameters: %.3fM' % num_params)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 19.773458664456843}, {"code": "def num_params(self)\n\t\n    num_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n    print(f\"Number of trainable parameters: {num_params/1e6:.3f}M\")\n\n", "CodeBleu_score": 13.79178903190411}]}
{"instance_id": "78b876054bd4", "generation_result": [{"code": "def pad_tensor(self, x, pad, side='both')\n\t\n    if side == 'before':\n        new_size = x.size(0) + pad\n        padded = torch.zeros(new_size, *x.size()[1:]).type(x.type())\n        padded[:x.size(0)] = x\n        return padded\n    elif side == 'after':\n        new_size = x.size(0) + pad\n        padded = torch.zeros(new_size, *x.size()[1:]).type(x.type())\n        padded[pad:] = x\n        return padded\n    elif side == 'both':\n        new_size = x.size(0) + 2 * pad\n        padded = torch.zeros(new_size, *x.size()[1:]).type(x.type())\n        padded[pad:x.size(0)+pad] = x\n        return padded\n\n", "CodeBleu_score": 41.50011717683918}, {"code": "def pad_tensor(self, x, pad, side='both')\n\t\n    if pad == 0:\n        return x\n    if side == 'before':\n        pad_width = (0, pad)\n    elif side == 'after':\n        pad_width = (pad, 0)\n    elif side == 'both':\n        pad_width = (pad, pad)\n    else:\n        raise ValueError(\"side must be either 'before', 'after', or 'both'\")\n    new_size = x.size()[:-1] + (x.size()[-1] + pad_width[0] + pad_width[1],)\n    if torch.cuda.is_available():\n        padded = torch.cuda.FloatTensor(new_size).fill_(0)\n    else:\n        padded = torch.FloatTensor(new_size).fill_(0)\n    if pad_width[0] > 0:\n        padded[:, :, :pad_width[0]] = x\n        padded[:, :, -pad_width[0]:] = x\n    elif pad_width[0] < 0:\n        padded[:, :, pad_width[0]:] = x\n    if pad_width[1] > 0:\n        padded[:, :, :, :pad_width[1]] = x\n        padded[:, :, :, -pad_width[1]:] = x\n    elif pad_width[1] < 0:\n        padded[:, :, :, pad_width[1]:] = x\n    return padded\n\n", "CodeBleu_score": 33.280954516429496}, {"code": "def pad_tensor(self, x, pad, side='both')\n\t\n    assert side in ['before', 'after', 'both'], \"side must be either 'before', 'after', or 'both'\"\n    if side == 'before':\n        new_size = x.size()[1] + pad\n        new_tensor = torch.zeros(x.size()[0], new_size)\n        if torch.cuda.is_available():\n            new_tensor = new_tensor.cuda()\n        new_tensor[:, :x.size()[1]] = x\n        return new_tensor\n    elif side == 'after':\n        new_size = x.size()[1] + pad\n        new_tensor = torch.zeros(x.size()[0], new_size)\n        if torch.cuda.is_available():\n            new_tensor = new_tensor.cuda()\n        new_tensor[:, pad:] = x\n        return new_tensor\n    else:\n        new_size = x.size()[1] + 2 * pad\n        new_tensor = torch.zeros(x.size()[0], new_size)\n        if torch.cuda.is_available():\n            new_tensor = new_tensor.cuda()\n        new_tensor[:, pad:x.size()[1] + pad] = x\n        return new_tensor\n\n", "CodeBleu_score": 32.477050290036615}]}
{"instance_id": "a7ed1715a3df", "generation_result": [{"code": "def save_attention(attn, path)\n\t\n    import matplotlib.pyplot as plt\n    fig = plt.figure(figsize=(10, 10))\n    ax = fig.add_subplot(1, 1, 1)\n    cax = ax.matshow(attn, cmap='bone')\n    fig.colorbar(cax)\n    ax.set_xticklabels([''] + ['Word ' + str(i) for i in range(attn.shape[1])], rotation=90)\n    ax.set_yticklabels([''] + ['Word ' + str(i) for i in range(attn.shape[0])])\n    plt.tight_layout()\n    plt.savefig(path)\n    plt.close()\n\n", "CodeBleu_score": 45.83022387512785}, {"code": "def save_attention(attn, path)\n\t\n    import matplotlib.pyplot as plt\n    fig = plt.figure(figsize=(10, 6))\n    ax = fig.add_subplot(111)\n    cax = ax.matshow(attn, cmap='bone')\n    fig.colorbar(cax)\n    plt.xlabel('Decoder timestep')\n    plt.ylabel('Encoder timestep')\n    plt.tight_layout()\n    plt.savefig(path)\n    plt.close()\n\n", "CodeBleu_score": 48.838623482199075}, {"code": "def save_attention(attn, path)\n\t\n    import matplotlib.pyplot as plt\n    fig = plt.figure(figsize=(10, 10))\n    ax = fig.add_subplot(1, 1, 1)\n    ax.matshow(attn, cmap='viridis')\n    ax.set_xticklabels([''] + model.input_ids[1:], fontsize=16, rotation=90)\n    ax.set_yticklabels([''] + model.input_ids[1:], fontsize=16)\n    fig.savefig(path)\n    plt.close()\n\n", "CodeBleu_score": 48.20378808232201}]}
{"instance_id": "19375c9f44a2", "generation_result": [{"code": "def plot(array)\n\t\n\n    # Create a figure with a specified size\n    fig = plt.figure(figsize=(10, 10))\n\n    # Add a subplot\n    ax = fig.add_subplot(1, 1, 1)\n\n    # Customize the axis labels and tick parameters with specific colors and font sizes\n    ax.set_xlabel(\"x\", color=\"black\", fontsize=20)\n    ax.set_ylabel(\"y\", color=\"black\", fontsize=20)\n    ax.tick_params(axis='x', colors='black', labelsize=20)\n    ax.tick_params(axis='y', colors='black', labelsize=20)\n\n    # Plot the array on the customized subplot\n    plt.plot(array, color=\"black\", linewidth=1)\n\n    # Show the figure\n    plt.show()\n\n", "CodeBleu_score": 33.824917271098435}, {"code": "def plot(array)\n\t\n    fig = plt.figure(figsize=(10, 5))\n    ax = fig.add_subplot(111)\n    ax.tick_params(axis='both', which='major', labelsize=16, colors='black')\n    ax.tick_params(axis='both', which='minor', labelsize=16, colors='black')\n    ax.set_xlabel('x', fontsize=16, color='black')\n    ax.set_ylabel('y', fontsize=16, color='black')\n    ax.plot(array, color='black')\n    plt.show()\n\n", "CodeBleu_score": 28.380745602004914}, {"code": "def plot(array)\n\t\n    fig = plt.figure(figsize=(12, 12))\n    ax = fig.add_subplot(111)\n    ax.tick_params(axis='x', colors='black', labelsize=20)\n    ax.tick_params(axis='y', colors='black', labelsize=20)\n    ax.spines['top'].set_color('black')\n    ax.spines['bottom'].set_color('black')\n    ax.spines['left'].set_color('black')\n    ax.spines['right'].set_color('black')\n    ax.xaxis.label.set_color('black')\n    ax.yaxis.label.set_color('black')\n    plt.plot(array)\n    plt.show()\n\n", "CodeBleu_score": 38.859613490969494}]}
{"instance_id": "ff94c425920c", "generation_result": [{"code": "def write_to_csv(image_name, prediction, confidence)\n\t\n\n    data = {\"image_name\": image_name, \"prediction\": prediction, \"confidence\": confidence}\n    csv_path = \"data/predictions.csv\"\n\n    if csv_path.is_file():\n        with open(csv_path, \"a\", newline=\"\") as f:\n            writer = csv.DictWriter(f, fieldnames=data.keys())\n            if csv_path.stat().st_size == 0:\n                writer.writeheader()\n            writer.writerow(data)\n    else:\n        with open(csv_path, \"w\", newline=\"\") as f:\n            writer = csv.DictWriter(f, fieldnames=data.keys())\n            writer.writeheader()\n            writer.writerow(data)\n\n", "CodeBleu_score": 56.39685828353049}, {"code": "def write_to_csv(image_name, prediction, confidence)\n\t\n    data = {\n        'Image Name': image_name,\n        'Prediction': prediction,\n        'Confidence': confidence\n    }\n    csv_path = Path(f'{image_name}.csv')\n    if csv_path.is_file():\n        with open(csv_path, 'a') as csv_file:\n            writer = csv.DictWriter(csv_file, fieldnames=data.keys())\n            writer.writerow(data)\n    else:\n        with open(csv_path, 'w') as csv_file:\n            writer = csv.DictWriter(csv_file, fieldnames=data.keys())\n            writer.writeheader()\n            writer.writerow(data)\n\n", "CodeBleu_score": 45.77329934108387}, {"code": "def write_to_csv(image_name, prediction, confidence)\n\t\n\n    # Create a dictionary with the image name, prediction, and confidence\n    data = {\n        'Image': image_name,\n        'Prediction': prediction,\n        'Confidence': confidence\n    }\n\n    # Open the CSV file in append mode\n    with open(csv_path, 'a') as csv_file:\n\n        # Create a DictWriter object\n        writer = csv.DictWriter(csv_file, fieldnames=data.keys())\n\n        # Write the header if the file is new\n        if csv_path.is_file():\n            writer.writeheader()\n\n        # Write the data as a new row\n        writer.writerow(data)\n\n", "CodeBleu_score": 52.46201156876479}]}
{"instance_id": "436161602009", "generation_result": [{"code": "def export_torchscript(model, im, file, optimize, prefix=colorstr(\"TorchScript:\"))\n\t\n    LOGGER.info(f\"{prefix} Exporting TorchScript...\")\n    torch.jit.script(model)\n    ts = torch.jit.trace(model, im)\n    ts.save(file)\n    if optimize:\n        ts = optimize_for_mobile(ts)\n        ts.save(file.with_suffix(\".optimized\"))\n    LOGGER.info(f\"{prefix} Exported TorchScript to {file}.\")\n    return file\n\n", "CodeBleu_score": 28.442964359066032}, {"code": "def export_torchscript(model, im, file, optimize, prefix=colorstr(\"TorchScript:\"))\n\t\n    LOGGER.info(f\"{prefix} exporting model to TorchScript format\")\n    torch.jit.trace(model, im)\n    if optimize:\n        LOGGER.info(f\"{prefix} optimizing model for mobile use\")\n        model = optimize_for_mobile(model)\n    model_details = {\"model_name\": model.module_name if hasattr(model, \"module_name\") else model.__class__.__name__, \"model_hash\": model.module_hash if hasattr(model, \"module_hash\") else model.__class__.__name__, \"model_details\": model.details if hasattr(model, \"details\") else None}\n    extra_files = {\"model_details.json\": json.dumps(model_details)}\n    ts = torch.jit.script(model)\n    ts.save(file, _extra_files=extra_files)\n    LOGGER.info(f\"{prefix} exported model to {file}\")\n    return file, None\n\n", "CodeBleu_score": 43.29642892696146}, {"code": "def export_torchscript(model, im, file, optimize, prefix=colorstr(\"TorchScript:\"))\n\t\n    LOGGER.info(f\"{prefix} Exporting model to TorchScript format...\")\n    if not isinstance(model, torch.nn.Module):\n        raise ValueError(\"The model must be a PyTorch model.\")\n    if not isinstance(im, torch.Tensor):\n        raise ValueError(\"The input must be a PyTorch tensor.\")\n    if not isinstance(file, str):\n        raise ValueError(\"The output file must be a string.\")\n    if not isinstance(optimize, bool):\n        raise ValueError(\"The optimization flag must be a boolean.\")\n    ts = torch.jit.trace(model, im)\n    if optimize:\n        ts = optimize_for_mobile(ts)\n    ts.save(file)\n    _save_for_lite_interpreter(model, file, prefix)\n    return file\n\n", "CodeBleu_score": 31.87606400594389}]}
{"instance_id": "78335487ac1c", "generation_result": [{"code": "def export_coreml(model, im, file, int8, half, nms, prefix=colorstr(\"CoreML:\")):\n\t\n    try:\n        import coremltools as ct\n    except:\n        raise ModuleNotFoundError(\"CoreML export requires coremltools. Please install it before using CoreML export.\")\n    if nms:\n        model = iOSModel(model)\n    LOGGER.info(f\"{prefix} exporting model to CoreML format...\")\n    file = file.with_suffix(\".mlmodel\")\n    ct_model = ct.convert(model, inputs=[ct.ImageType(name=\"image\", shape=im.shape, scale=1 / 255.0, bias=[0, 0, 0])])\n    if int8:\n        if half:\n            ct_model.save(file, use_float16=True, use_float_weights=True)\n        else:\n            ct_model.save(file, use_float16=False, use_float_weights=True)\n    else:\n        if half:\n            ct_model.save(file, use_float16=True, use_float_weights=False)\n        else:\n            ct_model.save(file, use_float16=False, use_float_weights=False)\n    LOGGER.info(f\"{prefix} exported model to CoreML format: {file}\")\n    return file, ct_model\n\n", "CodeBleu_score": 39.42255119060201}, {"code": "def export_coreml(model, im, file, int8, half, nms, prefix=colorstr(\"CoreML:\")):\n\t\n    LOGGER.info(f\"{prefix} exporting to CoreML format\")\n    check_requirements([\"coremltools\"])\n    file = file.with_suffix(\".mlmodel\")\n    if nms:\n        model = iOSModel(model)\n    model.eval()\n    model.cpu()\n    im = im.unsqueeze(0)\n    with trace(model, im):\n        ct_model = convert(model, im, file, int8, half)\n    if int8:\n        quantize_weights(ct_model)\n    ct_model.save(file)\n    return file, ct_model\n\n", "CodeBleu_score": 25.37114923078757}, {"code": "def export_coreml(model, im, file, int8, half, nms, prefix=colorstr(\"CoreML:\")):\n\t\n    LOGGER.info(f\"{prefix} exporting model to CoreML format\")\n    if not file.endswith(\".mlmodel\"):\n        file = file.with_suffix(\".mlmodel\")\n    if not file.exists():\n        file.parent.mkdir(parents=True, exist_ok=True)\n    if int8:\n        LOGGER.info(f\"{prefix} quantizing model\")\n        model = quantize_weights(model)\n    if half:\n        LOGGER.info(f\"{prefix} converting model to half precision\")\n        model = convert(model, inplace=True)\n    if nms:\n        LOGGER.info(f\"{prefix} wrapping model with iOSModel for NMS support\")\n        model = iOSModel(model)\n    trace(model, (im,), verbose=False)\n    model = ct_model.save(model, file)\n    return file, model\n\n", "CodeBleu_score": 28.39964454239957}]}
{"instance_id": "80233cf71580", "generation_result": [{"code": "def generate_individual(input_ranges, individual_length)\n\t\n    individual = []\n    for i in range(individual_length):\n        individual.append(random.uniform(input_ranges[i][0], input_ranges[i][1]))\n    return individual\n\n", "CodeBleu_score": 47.630972577458174}, {"code": "def generate_individual(input_ranges, individual_length)\n\t\n    individual = []\n    for _ in range(individual_length):\n        individual.append(random.uniform(input_ranges[0], input_ranges[1]))\n    return individual\n\n", "CodeBleu_score": 39.41234669340793}, {"code": "def generate_individual(input_ranges, individual_length)\n\t\n    individual = []\n    for i in range(individual_length):\n        individual.append(random.uniform(input_ranges[i][0], input_ranges[i][1]))\n    return individual\n\n", "CodeBleu_score": 47.630972577458174}]}
{"instance_id": "5ea979cb4a8e", "generation_result": [{"code": "def marginal_log_mean_coeff(self, t)\n\t\n    if self.schedule_type == 'discrete':\n        return self.log_alpha_fn(t)\n    elif self.schedule_type == 'linear':\n        return self.log_alpha_fn(t) + torch.log(1.0 - t) - torch.log(1.0 - self.max_t)\n    elif self.schedule_type == 'cosine':\n        return self.log_alpha_fn(t) - torch.log(1.0 + torch.cos(math.pi * t))\n    else:\n        raise ValueError('Unknown schedule type: {}'.format(self.schedule_type))\n\n", "CodeBleu_score": 32.183883010271984}, {"code": "def marginal_log_mean_coeff(self, t)\n\t\n    if self.schedule_type == 'discrete':\n        return self.log_alpha_fn(t)\n    elif self.schedule_type == 'linear':\n        return self.log_alpha_fn(t) - self.log_alpha_fn(self.t_start)\n    elif self.schedule_type == 'cosine':\n        return self.log_alpha_fn(t) - self.log_alpha_fn(self.t_start) - self.log_alpha_fn(self.t_end)\n    else:\n        raise ValueError(\"schedule_type must be either 'discrete', 'linear', or 'cosine'\")\n\n", "CodeBleu_score": 32.74563902024988}, {"code": "def marginal_log_mean_coeff(self, t)\n\t\n    if self.schedule_type == 'discrete':\n        return torch.log(self.alpha_t[t])\n    elif self.schedule_type == 'linear':\n        return torch.log(self.alpha_t[t] + (1 - self.alpha_t[t]) * t)\n    elif self.schedule_type == 'cosine':\n        return torch.log(self.alpha_t[t] + (1 - self.alpha_t[t]) * (1 - torch.cos(t * math.pi)))\n    else:\n        raise ValueError('schedule_type must be either \\'discrete\\', \\'linear\\', or \\'cosine\\'.')\n\n", "CodeBleu_score": 30.065685717591528}]}
{"instance_id": "7da4fae5a278", "generation_result": [{"code": "def inverse_lambda(self, lamb)\n\t\n    if self.schedule == \"linear\":\n        beta = self.beta\n        t = beta * lamb + self.temp\n        return t\n    elif self.schedule == \"discrete\":\n        log_alpha = self.log_alpha\n        log_alpha_interp = interpolate_fn(lamb, log_alpha)\n        t = torch.arccos(torch.exp(log_alpha_interp))\n        return t\n    else:\n        log_alpha = self.log_alpha\n        log_alpha_interp = interpolate_fn(lamb, log_alpha)\n        t = torch.arccos(torch.exp(log_alpha_interp))\n        return t\n\n", "CodeBleu_score": 22.58509553504852}, {"code": "def inverse_lambda(self, lamb)\n\t\n    if self.schedule == \"linear\":\n        return self.beta * lamb\n    elif self.schedule == \"discrete\":\n        log_alpha = self.interpolate_fn(lamb)\n        return torch.arccos(log_alpha)\n    else:\n        log_alpha = self.interpolate_fn(lamb)\n        return torch.arccos(log_alpha)\n\n", "CodeBleu_score": 12.54701293548155}, {"code": "def inverse_lambda(self, lamb)\n\t\n    if self.schedule == 'linear':\n        t = self.beta * lamb\n    elif self.schedule == 'discrete':\n        log_alpha = self.log_alpha.reshape(1, -1)\n        t = interpolate_fn(lamb, log_alpha)\n    else:\n        log_alpha = torch.logaddexp(self.log_alpha, -self.log_alpha)\n        t = torch.arccos(lamb * torch.exp(log_alpha)) / self.alpha_scale\n    return t\n\n", "CodeBleu_score": 21.77083320859287}]}
{"instance_id": "a519c853a2b2", "generation_result": [{"code": "def noise_pred_fn(x, t_continuous, cond=None)\n\t\n    if cond is None:\n        cond = t_continuous\n    t_continuous = t_continuous.expand(x.shape[0], *t_continuous.shape[1:])\n    x = x.reshape(x.shape[0], *x.shape[1:])\n    model_output = model(x, t_continuous, cond)\n    if model_type == \"gaussian\":\n        return model_output\n    elif model_type == \"gaussian_mixture\":\n        return model_output\n    elif model_type == \"gaussian_mixture_with_uncertainty\":\n        return model_output\n    elif model_type == \"gaussian_mixture_with_uncertainty_and_conditional\":\n        return model_output\n    elif model_type == \"gaussian_mixture_with_uncertainty_and_conditional_and_time\":\n        return model_output\n    elif model_type == \"gaussian_mixture_with_uncertainty_and_conditional_and_time_and_noise\":\n        return model_output\n    elif model_type == \"gaussian_mixture_with_uncertainty_and_conditional_and_time_and_noise_and_noise_schedule\":\n        return model_output\n    elif model_type == \"gaussian_mixture_with_uncertainty_and_conditional_and_time_and_noise_and_noise_schedule_and_time\":\n        return model_output\n    elif model_type == \"gaussian_mixture_with_uncertainty_and_conditional_and_time_and_noise_and_noise_schedule_and_time_and_noise\":\n        return model_output\n    elif model_type == \"gaussian_mixture_with_uncertainty_and_conditional_and_time_and_noise_and_noise_schedule_and_time_and_noise_and_conditional\":\n        return model_output\n    elif model_type == \"gaussian_mixture_with_uncertainty_and_conditional_and_time_and_noise_and_noise", "CodeBleu_score": 24.618338496888363}, {"code": "def noise_pred_fn(x, t_continuous, cond=None)\n\t\n    t_continuous = expand_dims(t_continuous, 0)\n    x = model(x, t_continuous, cond)\n    if model_type == \"gaussian\":\n        return x\n    elif model_type == \"gaussian_mixture\":\n        return x[:, :, :, :, 0]\n    elif model_type == \"gaussian_mixture_full\":\n        return x[:, :, :, :, 0]\n    elif model_type == \"gaussian_mixture_full_marginal\":\n        return x[:, :, :, :, 0]\n    elif model_type == \"gaussian_mixture_full_marginal_alpha\":\n        return x[:, :, :, :, 0]\n    else:\n        raise ValueError(\n            f\"Unknown model type: {model_type}. Valid model types are: gaussian, gaussian_mixture, gaussian_mixture_full, gaussian_mixture_full_marginal, gaussian_mixture_full_marginal_alpha\"\n        )\n\n", "CodeBleu_score": 17.69368650027699}, {"code": "def noise_pred_fn(x, t_continuous, cond=None)\n\t\n    if cond is None:\n        cond = {}\n    if t_continuous is None:\n        t_continuous = get_model_input_time(x, cond=cond)\n    x = expand_dims(x, t_continuous)\n    x = model(x, t_continuous, cond=cond)\n    x = t_continuous.reshape(x)\n    x = x.dim(0)\n    return x\n\n", "CodeBleu_score": 16.324346236761066}]}
{"instance_id": "96a9b7d54823", "generation_result": [{"code": "def dynamic_thresholding_fn(self, x0, t=None)\n\t\n    x0 = torch.abs(x0)\n    x0 = torch.expand_dims(x0, -1)\n    x0 = torch.to(x0, dtype=torch.float32)\n    x0 = torch.ones_like(x0)\n    x0 = torch.reshape(x0, (x0.shape[0], 1))\n    x0 = torch.maximum(x0, x0)\n    x0 = torch.clamp(x0, min=1e-5)\n    if t is None:\n        t = torch.quantile(x0, 0.999, dim=1, keepdim=True)\n        t = torch.reshape(t, (t.shape[0], 1))\n    x0 = torch.clamp(x0, max=t)\n    x0 = torch.div(x0, t)\n    return x0\n\n", "CodeBleu_score": 32.56787865435547}, {"code": "def dynamic_thresholding_fn(self, x0, t=None)\n\t\n    x0_abs = torch.abs(x0)\n    x0_abs_quantile = torch.quantile(x0_abs, self.quantile)\n    x0_abs_quantile = torch.unsqueeze(x0_abs_quantile, x0.dim())\n    x0_abs_quantile = x0_abs_quantile.expand_dims(x0.dim())\n    x0_abs_quantile = torch.ones_like(x0_abs_quantile) * x0_abs_quantile\n    x0_abs_quantile = torch.clamp(x0_abs_quantile, min=1e-12)\n    x0_abs_quantile = x0_abs_quantile.reshape(x0.shape)\n    x0_abs_quantile = torch.maximum(x0_abs, x0_abs_quantile)\n    x0_abs_quantile = x0_abs_quantile.reshape(x0.shape)\n    x0_abs_quantile = torch.clamp(x0_abs_quantile, min=1e-12)\n    x0_abs_quantile = x0_abs_quantile.reshape(x0.shape)\n    x0_abs_quantile = torch.ones_like(x0_abs_quantile) * x0_abs_quantile\n    x0_abs_quantile = torch.clamp(x0_abs_quantile, min=1e-12)\n    x0_abs_quantile = x0_abs_quantile.reshape(x0.shape)\n    x0_abs_quantile = torch.maximum(x0_abs, x0_abs_quantile)\n    x0_abs_quantile = x0_abs_quantile.reshape(x0.shape)\n    x0_abs_quantile = torch.clamp(x0_abs_quantile, min=1e-12)\n    x0_abs_quantile = x0_abs_quantile.reshape(x0.shape)\n    x0", "CodeBleu_score": 22.01321930952665}, {"code": "def dynamic_thresholding_fn(self, x0, t=None)\n\t\n    if t is None:\n        t = torch.quantile(torch.abs(x0), 0.99)\n    else:\n        t = t\n    s = torch.ones_like(x0)\n    s = torch.maximum(s, t)\n    s = torch.clamp(s, 1e-6)\n    return x0 / s\n\n", "CodeBleu_score": 26.423479275879714}]}
{"instance_id": "59c6ec6d7a6a", "generation_result": [{"code": "def data_prediction_fn(self, x, t)\n\t\n    x = self.noise_prediction_fn(x)\n    x = torch.clamp(x, min=-self.max_threshold, max=self.max_threshold)\n    x = x / self.max_threshold\n    return x\n\n", "CodeBleu_score": 8.712535715900566}, {"code": "def data_prediction_fn(self, x, t)\n\t\n    x = self.noise_prediction_fn(x, t)\n    return x\n\n", "CodeBleu_score": 2.562319023595779}, {"code": "def data_prediction_fn(self, x, t)\n\t\n    #print(x.shape)\n    #print(t.shape)\n    x_norm = self.normalization_fn(x)\n    x_norm_clamped = torch.clamp(x_norm, min=self.threshold_min, max=self.threshold_max)\n    x_norm_clamped_scaled = (x_norm_clamped - self.threshold_min) / (self.threshold_max - self.threshold_min)\n    #print(x_norm_clamped_scaled.shape)\n    return x_norm_clamped_scaled\n\n", "CodeBleu_score": 10.855006092541784}]}
{"instance_id": "65e3618468a2", "generation_result": [{"code": "def predict_eps_sigma(model, input, sigma_in, **kwargs)\n\t\n    sigma = sigma_in.view(input.shape)\n    scaled_input = input * sigma\n    scaled_input = scaled_input.view(1, *scaled_input.shape)\n    return model(scaled_input, **kwargs)\n\n", "CodeBleu_score": 29.420860928270827}, {"code": "def predict_eps_sigma(model, input, sigma_in, **kwargs)\n\t\n    sigma_in = sigma_in.view(input.shape[0], 1, 1, 1)\n    input = input * sigma_in\n    return model(input, **kwargs)\n\n", "CodeBleu_score": 22.165219637053823}, {"code": "def predict_eps_sigma(model, input, sigma_in, **kwargs)\n\t\n    sigma_in = sigma_in.view(input.shape[0], 1, 1, 1)\n    input = input.view(input.shape[0], 1, input.shape[1], input.shape[2])\n    input_scaled = input / sigma_in\n    return model(input_scaled, **kwargs)\n\n", "CodeBleu_score": 32.453856103803076}]}
{"instance_id": "b35846dfbc66", "generation_result": [{"code": "def cal_intergrand(beta_0, beta_1, taus)\n\t\n    with torch.enable_grad():\n        taus_ = taus.clone().requires_grad_()\n        alpha = torch.sqrt(beta_0 + beta_1 * taus_)\n        log_alpha = alpha.log()\n        log_alpha.sum().backward()\n        integrand = torch.sqrt(beta_0 + beta_1 * taus_) * taus_.grad\n    return integrand\n\n", "CodeBleu_score": 37.39330903608529}, {"code": "def cal_intergrand(beta_0, beta_1, taus)\n\t\n    with torch.enable_grad():\n        taus = taus.clone().requires_grad_(True)\n        alpha = beta_0 + beta_1 * taus\n        log_alpha = alpha.log()\n        backward(log_alpha)\n        integrand = torch.sqrt(alpha) * log_alpha\n        return integrand\n\n", "CodeBleu_score": 33.581106187172374}, {"code": "def cal_intergrand(beta_0, beta_1, taus)\n\t\n    with torch.enable_grad():\n        taus = taus.clone().requires_grad_(True)\n        alpha = torch.sqrt(beta_0 / (beta_1 * taus))\n        log_alpha = alpha.log()\n        log_alpha.sum().backward()\n        grad_taus = taus.grad\n        taus.grad = None\n    return grad_taus * alpha\n\n", "CodeBleu_score": 38.86160191640871}]}
{"instance_id": "5f3667566a10", "generation_result": [{"code": "def sample_euler_ancestral(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None)\n\t\n    if extra_args is None:\n        extra_args = []\n    if noise_sampler is None:\n        noise_sampler = get_default_noise_sampler(model, x, s_noise, sigmas, eta, extra_args, disable)\n    for i, sigma in enumerate(sigmas):\n        if callback is not None:\n            callback(model, x, sigma, i, sigmas, extra_args)\n        x = get_ancestral_step(model, x, sigma, extra_args)\n        x = x + sigma * noise_sampler(x, sigma, i, sigmas, extra_args)\n    return x\n\n", "CodeBleu_score": 20.809319866374988}, {"code": "def sample_euler_ancestral(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None)\n\t\n    if extra_args is None:\n        extra_args = {}\n    if noise_sampler is None:\n        noise_sampler = default_noise_sampler\n    if disable is None:\n        disable = []\n    if callback is None:\n        callback = lambda x, s: None\n    if s_noise == 0.:\n        s_noise = 1e-10\n    if eta == 0.:\n        eta = 1e-10\n    sigma = sigmas[0]\n    x_new = x\n    for sigma in sigmas:\n        if 'sigma' in disable:\n            sigma = 0.\n        x_new = get_ancestral_step(model, x_new, sigma, eta, s_noise, noise_sampler, extra_args)\n        callback(x_new, sigma)\n    if 'noise' in disable:\n        return x_new\n    return noise_sampler(x_new, s_noise)\n\n", "CodeBleu_score": 23.9197452435358}, {"code": "def sample_euler_ancestral(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None)\n\t\n    if extra_args is None:\n        extra_args = {}\n    if noise_sampler is None:\n        noise_sampler = default_noise_sampler\n    if disable is None:\n        disable = []\n    if callback is None:\n        callback = lambda x, y: None\n    if eta < 0.:\n        raise ValueError(\"eta must be non-negative\")\n    if s_noise < 0.:\n        raise ValueError(\"s_noise must be non-negative\")\n    if eta == 0.:\n        if s_noise == 0.:\n            return x\n        else:\n            return x + s_noise * noise_sampler(x.shape)\n    if s_noise == 0.:\n        return x + eta * sigmas * model(x, **extra_args)\n    return sample_euler_ancestral(model, x + eta * sigmas * model(x, **extra_args), sigmas, extra_args, callback, disable, eta, s_noise, noise_sampler)\n\n", "CodeBleu_score": 26.155397105551952}]}
{"instance_id": "6261ded88df2", "generation_result": [{"code": "def linear_multistep_coeff(order, t, i, j)\n\t\n    if order > i:\n        raise ValueError(\"The order of the multistep method must be less than or equal to the step index.\")\n    if order == 0:\n        return 1\n    else:\n        def product(k):\n            if k == 0:\n                return 1\n            else:\n                return product(k - 1) * (j - k + 1)\n        return product(order) * t[i + 1] - product(order - 1) * t[i]\n\n", "CodeBleu_score": 26.348988057656953}, {"code": "def linear_multistep_coeff(order, t, i, j)\n\t\n    if order > i:\n        raise ValueError('The order of the multistep method is too high for the step index.')\n    def product(t):\n        return (t[i] - t[j]) / (t[i+1] - t[j])\n    return integrate.quad(product, t[j], t[i+1])[0]\n\n", "CodeBleu_score": 21.91177635778503}, {"code": "def linear_multistep_coeff(order, t, i, j)\n\t\n    if order > 1:\n        def nested_function(t):\n            return t**order * (1 - t)**(order - 1)\n        return integrate.quad(nested_function, i, j)[0]\n    else:\n        return 0\n\n", "CodeBleu_score": 14.95349901331004}]}
{"instance_id": "4ee025e33ab1", "generation_result": [{"code": "def sample_lcm(model, x, sigmas, extra_args=None, callback=None, disable=None, noise_sampler=None)\n\t\n    if extra_args is None:\n        extra_args = {}\n    if noise_sampler is None:\n        noise_sampler = default_noise_sampler\n    if disable is None:\n        disable = []\n    if callback is None:\n        callback = default_callback\n    if callback is not None:\n        callback(0, x, sigmas)\n    for i, sigma in enumerate(sigmas):\n        if callback is not None:\n            callback(i, x, sigmas)\n        if sigma > 0:\n            x = x.new_ones(x.shape).mul(sigma)\n        x = model(x, sigmas[i], extra_args)\n        if callback is not None:\n            callback(i, x, sigmas)\n    return x\n\n", "CodeBleu_score": 33.537128289544704}, {"code": "def sample_lcm(model, x, sigmas, extra_args=None, callback=None, disable=None, noise_sampler=None)\n\t\n    if extra_args is None:\n        extra_args = {}\n    if noise_sampler is None:\n        noise_sampler = default_noise_sampler\n    if callback is None:\n        callback = default_callback\n    if disable is None:\n        disable = []\n    if len(disable) == 0:\n        disable = []\n    if len(disable) == 1 and disable[0] == 'all':\n        disable = ['lcm']\n    if 'lcm' not in disable:\n        if 'lcm' in extra_args:\n            lcm_sigmas = extra_args['lcm']\n        else:\n            lcm_sigmas = sigmas\n        for sigma in lcm_sigmas:\n            if sigma > 0:\n                x = x.new_ones(x.size())\n                x = x * sigma\n            x = model(x, **extra_args)\n            if callback is not None:\n                callback(x)\n    return x\n\n", "CodeBleu_score": 33.97871361648669}, {"code": "def sample_lcm(model, x, sigmas, extra_args=None, callback=None, disable=None, noise_sampler=None)\n\t\n    if extra_args is None:\n        extra_args = {}\n    if noise_sampler is None:\n        noise_sampler = default_noise_sampler\n    if disable is None:\n        disable = []\n    if callback is None:\n        callback = lambda *args: None\n    x = x.new_ones(x.shape) if x.dim() == 0 else x\n    for sigma in sigmas:\n        if sigma > 0:\n            x = x * noise_sampler(sigma)\n        x = x + model(x, sigmas[-1], **extra_args)\n        if callback is not None:\n            callback(x, sigmas[-1], extra_args)\n    return x\n\n", "CodeBleu_score": 34.591426271359396}]}
{"instance_id": "cb51f91da606", "generation_result": [{"code": "def append_dims(x, target_dims)\n\t\n    if x.dim() >= target_dims:\n        raise ValueError(\"The target dimensions must be greater than the current dimensions.\")\n    dims_to_append = target_dims - x.dim()\n    expanded = x.expand(tuple(list(x.shape) + [1] * dims_to_append))\n    return expanded.detach() if x.is_mps else expanded\n\n", "CodeBleu_score": 40.449915812519436}, {"code": "def append_dims(x, target_dims)\n\t\n    if x.dim() >= target_dims:\n        raise ValueError(\"The target dimensions must be greater than the current dimensions.\")\n    else:\n        dims_to_append = target_dims - x.dim()\n        expanded = x.unsqueeze(-1)\n        for _ in range(dims_to_append):\n            expanded = expanded.unsqueeze(-1)\n        return expanded.detach() if x.device.type == \"mps\" else expanded\n\n", "CodeBleu_score": 39.65009486686627}, {"code": "def append_dims(x, target_dims)\n\t\n    if x.dim() >= target_dims:\n        raise ValueError(\"Target dimensions should be greater than the current dimensions.\")\n    num_dims_to_append = target_dims - x.dim()\n    expanded = x.expand(tuple(x.shape) + (1,) * num_dims_to_append)\n    return expanded.detach() if expanded.device.type == \"mps\" else expanded\n\n", "CodeBleu_score": 40.498844271319314}]}
{"instance_id": "9fac874a3bc7", "generation_result": [{"code": "def download_file(path, url, digest=None)\n\t\n    path = Path(path)\n    if not path.exists():\n        path.parent.mkdir(parents=True, exist_ok=True)\n        with urlopen(url) as response, open(path, 'wb') as f:\n            shutil.copyfileobj(response, f)\n    if digest is not None:\n        with open(path, 'rb') as f:\n            file_hash = hashlib.sha256(f.read()).hexdigest()\n        if file_hash != digest:\n            raise OSError(f'File hash mismatch: {path}')\n    return path\n\n", "CodeBleu_score": 62.529324255113394}, {"code": "def download_file(path, url, digest=None)\n\t\n    path = Path(path)\n    if not path.exists():\n        path.parent.mkdir(parents=True, exist_ok=True)\n        with urlopen(url) as response, path.open(\"wb\") as file:\n            shutil.copyfileobj(response, file)\n    if digest is not None:\n        if not verify_file(path, digest):\n            raise OSError(\"File verification failed\")\n    return path\n\n", "CodeBleu_score": 38.87307667475681}, {"code": "def download_file(path, url, digest=None)\n\t\n    if not path.exists():\n        path.parent.mkdir(parents=True, exist_ok=True)\n        with urlopen(url) as response, open(path, \"wb\") as out_file:\n            shutil.copyfileobj(response, out_file)\n    if digest is not None:\n        with open(path, \"rb\") as f:\n            actual_digest = hashlib.sha256(f.read()).hexdigest()\n        if actual_digest != digest:\n            raise OSError(f\"File at {path} has incorrect hash: {actual_digest} != {digest}\")\n    return path\n\n", "CodeBleu_score": 55.11588541572115}]}
{"instance_id": "f23165e897ce", "generation_result": [{"code": "def train_mode(model, mode=True)\n\t\n    return model.train(mode)\n\n", "CodeBleu_score": 3.2785437034092797}, {"code": "def train_mode(model, mode=True)\n\t\n    return model.train(mode)\n\n", "CodeBleu_score": 3.2785437034092797}, {"code": "def train_mode(model, mode=True)\n\t\n    model.train(mode)\n    for module in model.modules():\n        if hasattr(module, 'training'):\n            module.training = mode\n\n", "CodeBleu_score": 22.256902076701675}]}
{"instance_id": "c6d59c6cb8e1", "generation_result": [{"code": "def ema_update(model, averaged_model, decay)\n\t\n    for name, param in model.named_parameters():\n        averaged_param = averaged_model.named_parameters()[name]\n        averaged_param.data.mul_(decay).add_(1 - decay, param.data)\n    for name, buf in model.named_buffers():\n        averaged_buf = averaged_model.named_buffers()[name]\n        averaged_buf.data.mul_(decay).add_(1 - decay, buf.data)\n\n", "CodeBleu_score": 38.70524765042905}, {"code": "def ema_update(model, averaged_model, decay)\n\t\n    model_params = dict(model.named_parameters())\n    averaged_params = dict(averaged_model.named_parameters())\n    model_buffers = dict(model.named_buffers())\n    averaged_buffers = dict(averaged_model.named_buffers())\n    for name in model_params:\n        if name in averaged_params:\n            averaged_params[name].data.copy_(\n                averaged_params[name].data * decay + model_params[name].data * (1.0 - decay)\n            )\n        else:\n            averaged_params[name] = model_params[name]\n    for name in model_buffers:\n        if name in averaged_buffers:\n            averaged_buffers[name].copy_(model_buffers[name])\n        else:\n            averaged_buffers[name] = model_buffers[name]\n    averaged_model.load_state_dict(averaged_params)\n    averaged_model.load_state_dict(averaged_buffers)\n\n", "CodeBleu_score": 36.62984839146156}, {"code": "def ema_update(model, averaged_model, decay)\n\t\n    model_params = dict(model.named_parameters())\n    averaged_params = dict(averaged_model.named_parameters())\n    averaged_buffers = dict(averaged_model.named_buffers())\n    model_buffers = dict(model.named_buffers())\n    for name in model_params.keys():\n        averaged_params[name].data.copy_(\n            averaged_params[name].data * decay + model_params[name].data * (1.0 - decay)\n        )\n    for name in model_buffers.keys():\n        averaged_buffers[name].copy_(model_buffers[name])\n\n", "CodeBleu_score": 45.06787611171417}]}
{"instance_id": "600c39a30798", "generation_result": [{"code": "def rand_log_logistic(shape, loc=0., scale=1., min_value=0., max_value=float('inf'), device='cpu', dtype=torch.float32)\n\t\n    min_value = torch.as_tensor(min_value, device=device, dtype=dtype)\n    max_value = torch.as_tensor(max_value, device=device, dtype=dtype)\n    u = torch.rand(shape, device=device, dtype=dtype)\n    logit_min = min_value.log()\n    logit_max = max_value.log()\n    logit_u = logit(u)\n    logit_u = logit_u * (logit_max - logit_min) + logit_min\n    return exp(logit_u) * scale + loc\n\n", "CodeBleu_score": 35.02022426355326}, {"code": "def rand_log_logistic(shape, loc=0., scale=1., min_value=0., max_value=float('inf'), device='cpu', dtype=torch.float32)\n\t\n    min_value = torch.as_tensor(min_value, dtype=dtype, device=device)\n    max_value = torch.as_tensor(max_value, dtype=dtype, device=device)\n    u = torch.rand(shape, device=device, dtype=dtype)\n    logit_min = min_value.log()\n    logit_max = max_value.log()\n    logit_u = logit(u)\n    logit_u = logit_u * (logit_max - logit_min) + logit_min\n    return scale * exp(logit_u) + loc\n\n", "CodeBleu_score": 32.88347231899405}, {"code": "def rand_log_logistic(shape, loc=0., scale=1., min_value=0., max_value=float('inf'), device='cpu', dtype=torch.float32)\n\t\n    min_value = torch.as_tensor(min_value, device=device, dtype=dtype)\n    max_value = torch.as_tensor(max_value, device=device, dtype=dtype)\n    min_value_cdf = min_value.log().logit()\n    max_value_cdf = max_value.log().logit()\n    uniform_samples = torch.rand(shape, device=device, dtype=dtype)\n    log_samples = (max_value_cdf - min_value_cdf) * uniform_samples + min_value_cdf\n    samples = log_samples.exp().logit()\n    return samples * scale + loc\n\n", "CodeBleu_score": 40.72494820893613}]}
{"instance_id": "75e2def2467f", "generation_result": [{"code": "def get_activation(activation: Literal[\"elu\", \"snake\", \"none\"], antialias=False, channels=None) -> nn.Module\n\t\n    if activation == \"elu\":\n        if antialias:\n            return Activation1d(ELU())\n        return ELU()\n    elif activation == \"snake\":\n        if antialias:\n            return Activation1d(SnakeBeta(channels))\n        return SnakeBeta(channels)\n    elif activation == \"none\":\n        if antialias:\n            return Activation1d(Identity())\n        return Identity()\n    else:\n        raise ValueError(f\"{activation} is not a valid activation type. Valid types are 'elu', 'snake', and 'none'.\")\n\n", "CodeBleu_score": 39.51727583530757}, {"code": "def get_activation(activation: Literal[\"elu\", \"snake\", \"none\"], antialias=False, channels=None) -> nn.Module\n\t\n    if activation == \"elu\":\n        return nn.ELU()\n    elif activation == \"snake\":\n        return SnakeBeta(channels)\n    elif activation == \"none\":\n        return nn.Identity()\n    else:\n        raise ValueError(\"Activation type not recognized.\")\n\n", "CodeBleu_score": 39.17943399279716}, {"code": "def get_activation(activation: Literal[\"elu\", \"snake\", \"none\"], antialias=False, channels=None) -> nn.Module\n\t\n    if activation == \"elu\":\n        return ELU()\n    elif activation == \"snake\":\n        return SnakeBeta(channels=channels)\n    elif activation == \"none\":\n        return Identity()\n    else:\n        raise ValueError(\"The activation type must be either 'elu', 'snake', or 'none'.\")\n\n", "CodeBleu_score": 36.319935560478456}]}
{"instance_id": "ef1714c79e51", "generation_result": [{"code": "def apply_rotary_pos_emb(t, freqs, scale = 1)\n\t\n    t = t.to(dtype = torch.float32 if t.dtype == torch.float16 else t.dtype)\n    freqs = freqs.to(dtype = torch.float32 if freqs.dtype == torch.float16 else freqs.dtype)\n    freqs = freqs.sin() * scale\n    t_unrotated, t = t.split([int(t.shape[1] / 2), int(t.shape[1] / 2)], dim = 1)\n    t = t.to(dtype = torch.float32 if t.dtype == torch.float16 else t.dtype)\n    t = torch.cat([t_unrotated, t * freqs], dim = 1)\n    return t\n", "CodeBleu_score": 26.150401733617684}, {"code": "def apply_rotary_pos_emb(t, freqs, scale = 1)\n\t\n    if t.is_cuda:\n        t = t.to(dtype = torch.float32)\n    else:\n        t = t.to(dtype = torch.float32)\n    freqs = freqs.to(dtype = torch.float32)\n    t_unrotated = t.clone()\n    t = t.to(dtype = torch.float32)\n    t_unrotated = t_unrotated.to(dtype = torch.float32)\n    t_unrotated = t_unrotated.reshape(-1, 2 * t.shape[1])\n    t = t.reshape(-1, 2 * t.shape[1])\n    t[:, 1::2] = t[:, 1::2] * scale.reshape(-1, 1)\n    t[:, ::2] = t[:, ::2] * scale.reshape(-1, 1)\n    t[:, 1::2] = t[:, 1::2] + freqs.sin()\n    t[:, ::2] = t[:, ::2] + freqs.cos()\n    t_unrotated = t_unrotated.reshape(t.shape)\n    t = t.reshape(t.shape)\n    return torch.cat((t_unrotated, t), dim = 1)\n", "CodeBleu_score": 31.702316746473002}, {"code": "def apply_rotary_pos_emb(t, freqs, scale = 1)\n\t\n    t = t.to(torch.float32)\n    freqs = freqs.to(torch.float32)\n    if t.dim() == 1:\n        t = t.unsqueeze(0)\n        freqs = freqs.unsqueeze(0)\n    if t.dim() == 3:\n        t = t.unsqueeze(1)\n        freqs = freqs.unsqueeze(1)\n    if t.dim() == 4:\n        t = t.transpose(1, 2)\n        freqs = freqs.transpose(1, 2)\n    if t.dim() > 4:\n        raise ValueError(\"t cannot have more than 4 dimensions\")\n    if freqs.dim() != t.dim():\n        raise ValueError(\"freqs must have the same number of dimensions as t\")\n    if freqs.shape[-1] != t.shape[-1]:\n        raise ValueError(\"freqs must have the same number of channels as t\")\n    if freqs.shape[0] != t.shape[0]:\n        freqs = freqs.transpose(0, 1)\n    t_unrotated = t\n    t = t.reshape(t.shape[0], t.shape[1], t.shape[2] * t.shape[3])\n    freqs = freqs.reshape(freqs.shape[0], freqs.shape[1], freqs.shape[2] * freqs.shape[3])\n    t = torch.cat((t.sin(), t.cos()), dim = -1)\n    freqs = torch.cat((freqs.sin(), freqs.cos()), dim = -1)\n    t = t * freqs * scale\n    t = t.reshape(t_unrotated.shape)\n    return t, t_unrotated\n\n", "CodeBleu_score": 32.15646544452822}]}
{"instance_id": "a7a5f178c6f6", "generation_result": [{"code": "def idx2vq(self, idx, dim=-1)\n\t\n    return self.codebook[idx].movedim(dim, -1)\n\n", "CodeBleu_score": 17.038320327172993}, {"code": "def idx2vq(self, idx, dim=-1)\n\t\n    if dim != -1:\n        idx = idx.movedim(dim, -1)\n    return self.codebook[idx]\n\n", "CodeBleu_score": 42.274537091243914}, {"code": "def idx2vq(self, idx, dim=-1)\n\t\n    return self.codebook.index_select(dim, idx).movedim(dim, -1)\n\n", "CodeBleu_score": 20.724301952363884}]}
{"instance_id": "4104c041fb20", "generation_result": [{"code": "def encode(self, x, quantize=False)\n\t\n    if self.training:\n        return self.encode_train(x)\n    else:\n        return self.encode_eval(x)\n\n", "CodeBleu_score": 15.163449822779345}, {"code": "def encode(self, x, quantize=False)\n\t\n    x = self.in_block(x)\n    x = self.down_blocks(x)\n    x = self.up_blocks(x)\n    x = self.out_block(x)\n    if quantize:\n        x, indices = self.quantize(x)\n        return x, indices\n    return x\n\n", "CodeBleu_score": 31.017039798712055}, {"code": "def encode(self, x, quantize=False)\n\t\n    x = self.in_block(x)\n    x = self.down_blocks(x)\n    x = self.middle_blocks(x)\n    x = self.up_blocks(x)\n    x = self.out_block(x)\n    if quantize:\n        x, indices = self.quantize(x)\n        return x, indices\n    return x\n\n", "CodeBleu_score": 31.761611831667906}]}
{"instance_id": "403c52ba2d3b", "generation_result": [{"code": "def ema_scope(self, context=None)\n\t\n    if context is None:\n        context = self\n    # Save the current parameters\n    original_parameters = copy.deepcopy(self.parameters)\n    # Apply the EMA weights\n    self.apply_ema_weights()\n    # Log the switch\n    if context is self:\n        logpy.info(\"Applied EMA weights\")\n    # Return a context manager that restores the original parameters\n    return lambda: context.set_parameters(original_parameters)\n\n", "CodeBleu_score": 28.91957456728281}, {"code": "def ema_scope(self, context=None)\n\t\n    if context is not None:\n        context.log_info('EMA scope entered')\n    if not self.parameters.ema_enabled:\n        if context is not None:\n            context.log_info('EMA not enabled, skipping scope')\n        return\n    if self.parameters.ema_enabled and self.parameters.ema_weights is None:\n        raise ValueError('EMA weights not loaded')\n    if self.parameters.ema_enabled and self.parameters.ema_weights is not None:\n        if context is not None:\n            context.log_info('Replacing parameters with EMA weights')\n        self.parameters.replace_with_ema_weights()\n        if context is not None:\n            context.log_info('EMA scope exited')\n\n", "CodeBleu_score": 47.148741920251325}, {"code": "def ema_scope(self, context=None)\n\t\n    if context is not None:\n        context.log(\"Using EMA weights.\")\n    self.parameters = self.parameters.copy()\n    for name, param in self.parameters.items():\n        if \"weight\" in name:\n            self.parameters[name] = param.detach().clone()\n            self.parameters[name].requires_grad = False\n            self.parameters[name].data = param.data.clone()\n            self.parameters[name].data.mul_(0)\n            self.parameters[name].data.add_(self.ema_weights[name].data)\n\n", "CodeBleu_score": 39.63811802753974}]}
{"instance_id": "94b7467f98e4", "generation_result": [{"code": "def post_attention(self, attn, x, gate_msa, shift_mlp, scale_mlp, gate_mlp)\n\t\n    x = x + attn * gate_msa\n    x = self.norm1(x)\n    x = self.mlp(x) * gate_mlp\n    return x\n\n", "CodeBleu_score": 28.427849008342033}, {"code": "def post_attention(self, attn, x, gate_msa, shift_mlp, scale_mlp, gate_mlp)\n\t\n    return (\n        x\n        + attn\n        * (\n            self.gate_msa\n            * (\n                self.shift_mlp(x)\n                + self.scale_mlp(x)\n                + self.mlp(x)\n                * self.gate_mlp\n            )\n        )\n    )\n\n", "CodeBleu_score": 24.898508061723145}, {"code": "def post_attention(self, attn, x, gate_msa, shift_mlp, scale_mlp, gate_mlp)\n\t\n    x = x + attn * gate_msa\n    x = x.norm(dim=-1, keepdim=True)\n    x = x * self.mod_scale\n    x = x + self.shift(x)\n    x = x * self.scale(x)\n    x = x + self.mlp(x) * gate_mlp\n    return x\n\n", "CodeBleu_score": 45.67255483934215}]}
{"instance_id": "d9fdc05542a2", "generation_result": [{"code": "def _block_mixing(context, x, context_block, x_block, c)\n\t\n    # x_block.pre_attention\n    qkv_x = x_block.pre_attention(x)\n    # context_block.pre_attention\n    qkv_context = context_block.pre_attention(context)\n    # tuple\n    qkv_x, qkv_context = qkv_x[0], qkv_context[0]\n    # o.append\n    qkv_x = qkv_x.permute(0, 2, 1)\n    # context_block.pre_attention\n    qkv_context = qkv_context.permute(0, 2, 1)\n    # torch.cat\n    qkv_context = torch.cat((qkv_context, qkv_x), dim=1)\n    # optimized_attention\n    qkv_context = optimized_attention(qkv_context, c)\n    # x_block.post_attention\n    qkv_context = qkv_context.permute(0, 2, 1)\n    # context_block.post_attention\n    qkv_context = context_block.post_attention(qkv_context)\n    # range\n    qkv_context = qkv_context.permute(0, 2, 1)\n    # x_block.post_attention\n    qkv_x = x_block.post_attention(qkv_x)\n    # context_block.post_attention\n    qkv_x = context_block.post_attention(qkv_x)\n    # tuple\n    return (qkv_x, qkv_context)\n\n", "CodeBleu_score": 18.221440188962294}, {"code": "def _block_mixing(context, x, context_block, x_block, c)\n\t\n    c.append(context)\n    c.append(x)\n    q = x_block.pre_attention(x)\n    k = context_block.pre_attention(context)\n    v = context_block.pre_attention(context)\n    qkv = torch.cat((q, k, v), dim=1)\n    qkv = optimized_attention(qkv, c)\n    qkv = qkv.view(-1, 3, qkv.shape[-1])\n    q, k, v = qkv.split(q.shape[-1], dim=1)\n    q = x_block.post_attention(q)\n    k = context_block.post_attention(k)\n    v = context_block.post_attention(v)\n    x = x + q\n    context = context + k\n    return context, x\n\n", "CodeBleu_score": 18.657688769608704}, {"code": "def _block_mixing(context, x, context_block, x_block, c)\n\t\n    c = c.clone()\n    c.requires_grad = False\n    q_context, k_context, v_context = context_block.pre_attention(context)\n    q_x, k_x, v_x = x_block.pre_attention(x)\n    qkv_context = torch.cat([q_context, k_context, v_context], dim=2)\n    qkv_x = torch.cat([q_x, k_x, v_x], dim=2)\n    qkv_context, qkv_x = optimized_attention(qkv_context, qkv_x, c)\n    q_context, k_context, v_context = qkv_context.split([q_context.size(2), k_context.size(2), v_context.size(2)], dim=2)\n    q_x, k_x, v_x = qkv_x.split([q_x.size(2), k_x.size(2), v_x.size(2)], dim=2)\n    context = context_block.post_attention(q_context, k_context, v_context)\n    x = x_block.post_attention(q_x, k_x, v_x)\n    return context, x\n\n", "CodeBleu_score": 31.835714458212095}]}
{"instance_id": "b219bce30160", "generation_result": [{"code": "def get_timestep_embedding(timesteps, embedding_dim)\n\t\n    assert len(timesteps.shape) == 1\n    half_dim = embedding_dim // 2\n    timestep_range = torch.arange(timesteps.max() + 1)\n    enc = timestep_range.float()\n    # enc = timestep_range.float().unsqueeze(0)\n    enc = timestep_range.float().unsqueeze(1)\n    # enc = timestep_range.float().unsqueeze(0).unsqueeze(1)\n    # enc = timestep_range.float().unsqueeze(0).unsqueeze(1).unsqueeze(2)\n    # print(enc.shape)\n    freq_seq = 1. / (10000 ** (torch.arange(half_dim, dtype=torch.float) / embedding_dim))\n    # print(freq_seq.shape)\n    # print(enc.shape)\n    # print(freq_seq.shape)\n    # print(enc.shape)\n    # print(enc.shape)\n    emb = torch.cat([torch.sin(enc * freq_seq), torch.cos(enc * freq_seq)], dim=1)\n    # print(emb.shape)\n    if embedding_dim % 2 == 1:\n        # print(emb.shape)\n        # print(torch.zeros(emb.shape[0], 1).to(emb.device).float().shape)\n        # print(torch.zeros(emb.shape[0], 1).to(emb.device).float().shape)\n        # print(torch.zeros(emb.shape[0], 1).to(emb.device).float().shape)\n        # print(torch.zeros(emb.shape[0], 1).to(emb.device).float().shape)\n        # print(torch.zeros(emb.shape[0], 1).to(emb.device).float().shape)\n        # print(torch.zeros(emb.shape[0], 1).to(emb.device).float().shape)", "CodeBleu_score": 45.02215474112057}, {"code": "def get_timestep_embedding(timesteps, embedding_dim)\n\t\n    assert len(timesteps.shape) == 1\n    half_dim = embedding_dim // 2\n    timestep_range = torch.arange(timesteps.shape[0], dtype=torch.float32)\n    dim_range = torch.arange(embedding_dim, dtype=torch.float32)\n    dim_range = torch.pow(torch.tensor(10000.0), 2 * (dim_range // 2) / embedding_dim)\n    pos_x = torch.sin(timestep_range[None, :] / dim_range[None, :])\n    pos_y = torch.cos(timestep_range[None, :] / dim_range[None, :])\n    pos_x = torch.reshape(pos_x, [timesteps.shape[0], 1, embedding_dim])\n    pos_y = torch.reshape(pos_y, [timesteps.shape[0], 1, embedding_dim])\n    pos_x = torch.reshape(pos_x, [timesteps.shape[0], embedding_dim])\n    pos_y = torch.reshape(pos_y, [timesteps.shape[0], embedding_dim])\n    pos_x = torch.reshape(pos_x, [timesteps.shape[0], 1, embedding_dim])\n    pos_y = torch.reshape(pos_y, [timesteps.shape[0], 1, embedding_dim])\n    pos_x = torch.reshape(pos_x, [timesteps.shape[0], embedding_dim])\n    pos_y = torch.reshape(pos_y, [timesteps.shape[0], embedding_dim])\n    pos_x = pos_x.to(device=timesteps.device)\n    pos_y = pos_y.to(device=timesteps.device)\n    if embedding_dim % 2 == 1:\n        pos_x = torch.cat([pos_x, torch.zeros([timesteps.shape[0], 1,", "CodeBleu_score": 41.24731983189476}, {"code": "def get_timestep_embedding(timesteps, embedding_dim)\n\t\n    assert len(timesteps.shape) == 1\n    half_dim = embedding_dim // 2\n    timestep_range = torch.arange(timesteps.shape[0], dtype=torch.float32)\n    enc = timestep_range[None, :] / (timesteps[:, None] + half_dim)\n    # Apply sin/cos to even and odd parts separately\n    emb = torch.cat([torch.sin(enc), torch.cos(enc)], dim=1)\n    if embedding_dim % 2 == 1:\n        # Zero pad\n        emb = torch.cat([emb, torch.zeros(emb.shape[0], 1, device=emb.device)], dim=1)\n    return emb\n\n", "CodeBleu_score": 52.867852724409246}]}
{"instance_id": "6ae8453c86be", "generation_result": [{"code": "def pytorch_attention(q, k, v)\n\t\n    try:\n        q = q.view(q.size(0), q.size(1), q.size(2) * q.size(3)).transpose(1, 2)\n        k = k.view(k.size(0), k.size(1), k.size(2) * k.size(3)).transpose(1, 2)\n        v = v.view(v.size(0), v.size(1), v.size(2) * v.size(3)).transpose(1, 2)\n        attention = scaled_dot_product_attention(q, k, v)\n        attention = attention.transpose(1, 2).contiguous().view(attention.size(0), attention.size(1), v.size(2), v.size(3))\n        return attention\n    except RuntimeError:\n        logging.warning(\"OOM: Switching to slice attention\")\n        return slice_attention(q, k, v)\n\n", "CodeBleu_score": 32.214611051054256}, {"code": "def pytorch_attention(q, k, v)\n\t\n    try:\n        # Reshape and transpose to prepare tensors for attention computation.\n        q = q.contiguous().view(q.size(0), -1, q.size(-1)).transpose(1, 2)\n        k = k.contiguous().view(k.size(0), -1, k.size(-1)).transpose(1, 2)\n        v = v.contiguous().view(v.size(0), -1, v.size(-1)).transpose(1, 2)\n        # Compute attention.\n        attn = torch.bmm(q, k.transpose(1, 2))\n        attn = attn / math.sqrt(k.size(-1))\n        attn = F.softmax(attn, dim=-1)\n        attn = F.dropout(attn, p=0.1, training=self.training)\n        # Compute attention output.\n        output = torch.bmm(attn, v)\n        # Reshape and transpose back to the original dimensions.\n        output = output.transpose(1, 2).contiguous().view(q.size(0), -1, v.size(-1))\n        return output\n    except RuntimeError:\n        # Switch to slice-attention if OOM exception occurs.\n        logging.warning(\"OOM exception caught. Switching to slice-attention.\")\n        return slice_attention(q, k, v)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 35.36630357061673}, {"code": "def pytorch_attention(q, k, v)\n\t\n    # Reshape and transpose the tensors to prepare them for attention computation\n    q = q.contiguous().view(-1, q.size(-1)).transpose(0, 1)\n    k = k.contiguous().view(-1, k.size(-1)).transpose(0, 1)\n    v = v.contiguous().view(-1, v.size(-1)).transpose(0, 1)\n    # Compute the attention output\n    out = scaled_dot_product_attention(q, k, v)\n    # Reshape and transpose the attention output back to the original dimensions\n    out = out.transpose(0, 1).contiguous().view(q.size(0), -1, out.size(-1))\n    return out\n\n", "CodeBleu_score": 23.07284570257589}]}
{"instance_id": "fe4433c987ad", "generation_result": [{"code": "def apply_control(h, control, name)\n\t\n    if control and name:\n        if name in control:\n            try:\n                h = h.apply_control(control[name])\n            except:\n                logging.warning(\"Could not apply control to %s\" % name)\n    return h\n\n", "CodeBleu_score": 38.988818247299136}, {"code": "def apply_control(h, control, name)\n\t\n    if control and name:\n        if name in control:\n            try:\n                h += control[name]\n            except Exception as e:\n                logging.warning(f\"Failed to apply control {name} to h: {e}\")\n    return h\n\n", "CodeBleu_score": 36.39426323471627}, {"code": "def apply_control(h, control, name)\n\t\n    if control is not None and len(control) > 0:\n        try:\n            h = h.apply_control(control, name)\n        except Exception as e:\n            logging.warning(f\"{name} control failed to apply: {e}\")\n    return h\n\n", "CodeBleu_score": 33.06528488241557}]}
{"instance_id": "56da4547c101", "generation_result": [{"code": "def get_codebook_entry(self, indices, shape)\n\t\n    min_encodings = torch.zeros(indices.shape[0], self.num_embeddings, device=indices.device)\n    min_encodings.scatter_(1, indices.unsqueeze(1), 1)\n    min_encodings = min_encodings.view(shape)\n    min_encodings = min_encodings.permute(0, 2, 3, 1)\n    z_q = torch.matmul(min_encodings, self.embeddings)\n    z_q = z_q.view(shape)\n    return z_q\n\n", "CodeBleu_score": 24.110620175398363}, {"code": "def get_codebook_entry(self, indices, shape)\n\t\n    min_encodings = torch.zeros(indices.size(0), self.num_embeddings, device=indices.device)\n    min_encodings.scatter_(1, indices.unsqueeze(1), 1)\n    min_encodings = min_encodings.view(shape)\n    z_q = torch.matmul(min_encodings, self.embedding.weight.transpose(0, 1))\n    z_q = z_q.permute(0, 2, 3, 1).contiguous()\n    return z_q\n\n", "CodeBleu_score": 29.379690604991737}, {"code": "def get_codebook_entry(self, indices, shape)\n\t\n    min_encodings = torch.zeros(indices.shape + (self.num_embeddings,), dtype=torch.float32, device=indices.device)\n    min_encodings.scatter_(dim=-1, index=indices.unsqueeze(-1), value=1)\n    min_encodings = min_encodings.view(shape + (self.num_embeddings,)).permute(0, 2, 1)\n    quantized = torch.matmul(min_encodings.float(), self.embedding.weight.transpose(0, 1))\n    return quantized\n\n", "CodeBleu_score": 29.010549268197572}]}
{"instance_id": "548f8a1fb37d", "generation_result": [{"code": "def make_noise(self)\n\t\n    self.noises = []\n    self.noises.append(torch.randn(4, 4, device=self.device))\n    for log_size in range(3, self.log_size + 1):\n        self.noises.append(torch.randn(2 ** log_size, 2 ** log_size, device=self.device))\n        self.noises.append(torch.randn(2 ** log_size, 2 ** log_size, device=self.device))\n    return self.noises\n\n", "CodeBleu_score": 28.41966660707065}, {"code": "def make_noise(self)\n\t\n    self.noises = []\n    self.noises.append(torch.randn(4, 4, device=self.device))\n    for log_size in range(3, self.log_size + 1):\n        size = 2 ** log_size\n        self.noises.append(torch.randn(size, size, device=self.device))\n        self.noises.append(torch.randn(size, size, device=self.device))\n    return self.noises\n\n", "CodeBleu_score": 30.535491816344063}, {"code": "def make_noise(self)\n\t\n    # TODO: Write your code here\n    noises = torch.randn(4, 4, device=self.device)\n    noises.requires_grad = True\n    noises.retain_grad()\n    for log_size in range(3, self.log_size + 1):\n        noises.append(torch.randn(2 ** log_size, 2 ** log_size, device=self.device))\n        noises.append(torch.randn(2 ** log_size, 2 ** log_size, device=self.device))\n    return noises\n\n", "CodeBleu_score": 34.03937093806042}]}
{"instance_id": "42fd805563c6", "generation_result": [{"code": "def mean_latent(self, num_latent)\n\t\n    latent_input = torch.randn(num_latent, self.style_mlp.in_features)\n    latent_input = latent_input.to(self.device)\n    latent_output = self.style_mlp(latent_input)\n    latent_output = latent_output.to(self.device)\n    mean_latent = torch.mean(latent_output, dim=0)\n    return mean_latent\n\n", "CodeBleu_score": 21.699955489260212}, {"code": "def mean_latent(self, num_latent)\n\t\n    latent_inputs = torch.randn(num_latent, self.latent_dim)\n    latent_outputs = self.style_mlp(latent_inputs)\n    mean_latent = torch.mean(latent_outputs, dim=0)\n    return mean_latent\n\n", "CodeBleu_score": 21.23766524661599}, {"code": "def mean_latent(self, num_latent)\n\t\n    # Generate random latent inputs\n    latent_inputs = torch.randn(num_latent, self.latent_dim, device=self.device)\n    # Process latent inputs through style MLP\n    latent_outputs = self.style_mlp(latent_inputs)\n    # Compute mean of processed latents\n    return torch.mean(latent_outputs, dim=0)\n\n", "CodeBleu_score": 29.014433961214635}]}
{"instance_id": "2f485fd924af", "generation_result": [{"code": "def color_jitter(img, shift)\n\t\n    shift = np.array(shift, dtype=np.float32)\n    img = img.astype(np.float32)\n    img = img + np.random.uniform(-shift, shift, img.shape)\n    return np.clip(img, 0, 1)\n\n", "CodeBleu_score": 40.014777374725746}, {"code": "def color_jitter(img, shift)\n\t\n    shift_r = shift * np.random.uniform(-1, 1)\n    shift_g = shift * np.random.uniform(-1, 1)\n    shift_b = shift * np.random.uniform(-1, 1)\n    img = img.astype(np.float32)\n    img[:, :, 0] += shift_r\n    img[:, :, 1] += shift_g\n    img[:, :, 2] += shift_b\n    img = np.clip(img, 0, 1)\n    return img\n\n", "CodeBleu_score": 45.78254012310921}, {"code": "def color_jitter(img, shift)\n\t\n    if shift == 0:\n        return img\n    assert isinstance(img, np.ndarray)\n    assert isinstance(shift, (int, float))\n    assert img.ndim == 3\n    assert 0 <= shift <= 1\n\n    # Generate random jitter values for each RGB channel\n    r_jitter = np.random.uniform(-shift, shift, size=img.shape[0])\n    g_jitter = np.random.uniform(-shift, shift, size=img.shape[0])\n    b_jitter = np.random.uniform(-shift, shift, size=img.shape[0])\n\n    # Add the jitter values to the image\n    img += np.stack((r_jitter, g_jitter, b_jitter), axis=-1)\n\n    # Clip the result to ensure pixel values remain between 0 and 1\n    return np.clip(img, 0, 1)\n\n", "CodeBleu_score": 35.738831947487334}]}
{"instance_id": "c87ccf1fbf48", "generation_result": [{"code": "def feed_data(self, data)\n\t\n    if data is None:\n        return\n    if 'lq' in data:\n        self.lq = data['lq']\n    if 'gt' in data:\n        self.gt = data['gt']\n    if 'eyes' in data:\n        self.eyes = data['eyes']\n    if 'mouth' in data:\n        self.mouth = data['mouth']\n    if 'face_shape' in data:\n        self.face_shape = data['face_shape']\n\n", "CodeBleu_score": 41.76950543430662}, {"code": "def feed_data(self, data)\n\t\n    if len(data) == 2:\n        self.lq = data[0]\n        self.gt = data[1]\n    else:\n        self.lq = data\n    if len(self.lq.shape) == 3:\n        self.lq = self.lq[0]\n    if len(self.gt.shape) == 3:\n        self.gt = self.gt[0]\n    if self.lq.shape[1] == 2:\n        self.lq = self.lq[:, :, 0:1]\n    if self.gt.shape[1] == 2:\n        self.gt = self.gt[:, :, 0:1]\n    if self.lq.shape[1] == 3:\n        self.lq = self.lq[:, :, 0:3]\n    if self.gt.shape[1] == 3:\n        self.gt = self.gt[:, :, 0:3]\n    if self.lq.shape[1] == 4:\n        self.lq = self.lq[:, :, 0:4]\n    if self.gt.shape[1] == 4:\n        self.gt = self.gt[:, :, 0:4]\n    if self.lq.shape[1] == 5:\n        self.lq = self.lq[:, :, 0:5]\n    if self.gt.shape[1] == 5:\n        self.gt = self.gt[:, :, 0:5]\n    if self.lq.shape[1] == 6:\n        self.lq = self.lq[:, :, 0:6]\n    if self.gt.shape[1] == 6:\n        self.gt = self.gt[:, :, 0:6]\n    if self.lq.shape[1] == 7:\n        self.lq = self.lq[:, :, 0:7]\n    if self.gt.shape[1] == 7:\n        self.gt = self.gt[:, :, 0", "CodeBleu_score": 16.507569668601878}, {"code": "def feed_data(self, data)\n\t\n    if self.lq_key in data:\n        self.lq = data[self.lq_key]\n    if self.gt_key in data:\n        self.gt = data[self.gt_key]\n    if self.left_eye_key in data:\n        self.left_eye = data[self.left_eye_key]\n    if self.right_eye_key in data:\n        self.right_eye = data[self.right_eye_key]\n    if self.mouth_key in data:\n        self.mouth = data[self.mouth_key]\n    if self.face_key in data:\n        self.face = data[self.face_key]\n\n", "CodeBleu_score": 25.104071912574916}]}
{"instance_id": "8f8c7cb13037", "generation_result": [{"code": "def construct_img_pyramid(self)\n\t\n    img_pyramid = []\n    img_pyramid.append(self.gt)\n    for i in range(self.n_levels - 1):\n        img_pyramid.insert(0, F.interpolate(img_pyramid[0], scale_factor=0.5, mode='bilinear', align_corners=True))\n    return img_pyramid\n", "CodeBleu_score": 26.051889826794998}, {"code": "def construct_img_pyramid(self)\n\t\n    img_pyramid = []\n    img_pyramid.append(self.img_gt)\n    for i in range(self.num_scales - 1):\n        img_pyramid.insert(0, F.interpolate(img_pyramid[0], scale_factor=0.5, mode='bilinear', align_corners=True))\n    return img_pyramid\n\n", "CodeBleu_score": 26.005963592012666}, {"code": "def construct_img_pyramid(self)\n\t\n    img_pyramid = []\n    img_pyramid.append(self.gt)\n    for i in range(self.num_scales):\n        img_pyramid.insert(0, F.interpolate(img_pyramid[0], scale_factor=0.5, mode='bilinear', align_corners=False))\n    return img_pyramid\n", "CodeBleu_score": 27.187704389806633}]}
{"instance_id": "51247274d40c", "generation_result": [{"code": "def save(self, epoch, current_iter)\n\t\n    if self.save_network:\n        self.save_network_state(epoch, current_iter)\n    if self.save_training_state:\n        self.save_training_state(epoch, current_iter)\n\n", "CodeBleu_score": 24.25990281142223}, {"code": "def save(self, epoch, current_iter)\n\t\n    if self.save_model_interval is not None and (epoch + 1) % self.save_model_interval == 0:\n        self.save_network(self.netG, 'G', epoch, current_iter)\n        self.save_network(self.netG_ema, 'G_ema', epoch, current_iter)\n        self.save_network(self.netD, 'D', epoch, current_iter)\n        if self.opt.lambda_identity > 0:\n            self.save_network(self.netD_identity, 'D_identity', epoch, current_iter)\n        if self.opt.lambda_reconstruct_face > 0:\n            self.save_network(self.netD_reconstruct_face, 'D_reconstruct_face', epoch, current_iter)\n        if self.opt.lambda_reconstruct_eye > 0:\n            self.save_network(self.netD_reconstruct_eye, 'D_reconstruct_eye', epoch, current_iter)\n        if self.opt.lambda_reconstruct_mouth > 0:\n            self.save_network(self.netD_reconstruct_mouth, 'D_reconstruct_mouth', epoch, current_iter)\n        if self.opt.lambda_reconstruct_nose > 0:\n            self.save_network(self.netD_reconstruct_nose, 'D_reconstruct_nose', epoch, current_iter)\n        if self.opt.lambda_reconstruct_ear > 0:\n            self.save_network(self.netD_reconstruct_ear, 'D_reconstruct_ear', epoch, current_iter)\n        if self.opt.lambda_reconstruct_shoulder > 0:\n            self.save_network(self.netD_reconstruct_shoulder, 'D_reconstruct_shoulder', epoch, current_iter)\n        if self.opt.lambda_reconstruct_elbow > 0:\n            self.save_network(self.netD_reconstruct_elbow, 'D_reconstruct_elbow', epoch, current_iter)\n        if", "CodeBleu_score": 40.7248900582363}, {"code": "def save(self, epoch, current_iter)\n\t\n    self.save_network(self.G_ema, 'G_ema', epoch, current_iter)\n    self.save_network(self.G, 'G', epoch, current_iter)\n    self.save_network(self.D, 'D', epoch, current_iter)\n    if self.discriminator_type == 'wgan-gp':\n        self.save_network(self.D_A, 'D_A', epoch, current_iter)\n        self.save_network(self.D_B, 'D_B', epoch, current_iter)\n        if self.discriminator_deep_supervision:\n            self.save_network(self.D_C, 'D_C', epoch, current_iter)\n            self.save_network(self.D_D, 'D_D', epoch, current_iter)\n            self.save_network(self.D_E, 'D_E', epoch, current_iter)\n            self.save_network(self.D_F, 'D_F', epoch, current_iter)\n            self.save_network(self.D_G, 'D_G', epoch, current_iter)\n            self.save_network(self.D_H, 'D_H', epoch, current_iter)\n            self.save_network(self.D_I, 'D_I', epoch, current_iter)\n            self.save_network(self.D_J, 'D_J', epoch, current_iter)\n            self.save_network(self.D_K, 'D_K', epoch, current_iter)\n            self.save_network(self.D_L, 'D_L', epoch, current_iter)\n            self.save_network(self.D_M, 'D_M', epoch, current_iter)\n            self.save_network(self.D_N, 'D_N', epoch, current_iter)\n            self.save_network(self.D_O, 'D_O', epoch, current_iter)\n            self.save_network(self.D_P, 'D_P", "CodeBleu_score": 39.949908765058886}]}
{"instance_id": "afa0dc8128c1", "generation_result": [{"code": "def clean_folder(folder)\n\t\n    for item in os.listdir(folder):\n        file_path = os.path.join(folder, item)\n        if os.path.isfile(file_path):\n            try:\n                os.unlink(file_path)\n            except Exception as e:\n                print('Failed to delete %s. Reason: %s' % (file_path, e))\n        elif os.path.isdir(file_path):\n            try:\n                shutil.rmtree(file_path)\n            except Exception as e:\n                print('Failed to delete %s. Reason: %s' % (file_path, e))\n        elif os.path.islink(file_path):\n            try:\n                os.unlink(file_path)\n            except Exception as e:\n                print('Failed to delete %s. Reason: %s' % (file_path, e))\n\n", "CodeBleu_score": 43.0395392742464}, {"code": "def clean_folder(folder)\n\t\n    for item in os.listdir(folder):\n        path = os.path.join(folder, item)\n        try:\n            if os.path.isfile(path):\n                os.unlink(path)\n            elif os.path.islink(path):\n                os.unlink(path)\n            elif os.path.isdir(path):\n                shutil.rmtree(path)\n        except Exception as e:\n            print(\"Failed to delete %s. Reason: %s\" % (path, e))\n\n", "CodeBleu_score": 45.80396871096698}, {"code": "def clean_folder(folder)\n\t\n    for item in os.listdir(folder):\n        item_path = os.path.join(folder, item)\n        if os.path.isfile(item_path):\n            try:\n                os.unlink(item_path)\n            except OSError as e:\n                print(\"Error when deleting\", item_path, e)\n        elif os.path.islink(item_path):\n            try:\n                os.unlink(item_path)\n            except OSError as e:\n                print(\"Error when deleting\", item_path, e)\n        elif os.path.isdir(item_path):\n            try:\n                shutil.rmtree(item_path)\n            except OSError as e:\n                print(\"Error when deleting\", item_path, e)\n\n", "CodeBleu_score": 35.81280261228527}]}
{"instance_id": "53588b481bdd", "generation_result": [{"code": "def next_batch(self, sample_size=1)\n\t\n    sample_size = min(sample_size, len(self.experiences))\n    return random.sample(self.experiences, sample_size)\n\n", "CodeBleu_score": 17.061865289772015}, {"code": "def next_batch(self, sample_size=1)\n\t\n    if sample_size > len(self.memory):\n        sample_size = len(self.memory)\n    batch = random.sample(self.memory, sample_size)\n    self.memory.remove(batch[0])\n    return batch\n\n", "CodeBleu_score": 46.52652081747675}, {"code": "def next_batch(self, sample_size=1)\n\t\n    if sample_size > len(self.experience):\n        sample_size = len(self.experience)\n    batch = random.sample(self.experience, sample_size)\n    self.experience = [x for x in self.experience if x not in batch]\n    return batch\n\n", "CodeBleu_score": 71.81037875425677}]}
{"instance_id": "6bfa6a4e621c", "generation_result": [{"code": "def find_estimated_top_configs(self)\n\t\n    flattened_ds_config = flatten(self.all_configs)\n    feature_val = []\n    for key, val in flattened_ds_config.items():\n        if isinstance(val, np.ndarray):\n            feature_val.append(val.flatten())\n        else:\n            feature_val.append(val)\n    feature_val = np.array(feature_val)\n    if self.cost_model is None:\n        raise ValueError(\"No cost model has been specified.\")\n    if self.metric is None:\n        raise ValueError(\"No metric has been specified.\")\n    if self.metric not in self.cost_model.metrics:\n        raise ValueError(\"The specified metric is not in the cost model's metrics.\")\n    if self.metric not in self.cost_model.costs:\n        raise ValueError(\"The specified metric is not in the cost model's costs.\")\n    if self.metric not in self.cost_model.predict:\n        raise ValueError(\"The specified metric is not in the cost model's predict function.\")\n    predict = self.cost_model.predict[self.metric]\n    configs = predict(feature_val)\n    if self.metric == \"cost\":\n        configs = -configs\n    configs = np.argsort(configs)\n    return configs\n\n", "CodeBleu_score": 25.03747951203336}, {"code": "def find_estimated_top_configs(self)\n\t\n    flattened_ds_config = self.flatten_configs()\n    feature_val = []\n    for config in flattened_ds_config.values():\n        feature_val.append(config)\n    feature_val = np.array(feature_val)\n    cost_model = self.cost_model\n    if cost_model is None:\n        raise ValueError(\"No cost model specified. Please specify a cost model to use the function find_estimated_top_configs.\")\n    if not isinstance(cost_model, CostModel):\n        raise ValueError(\"The cost model specified is not a CostModel object. Please specify a CostModel object to use the function find_estimated_top_configs.\")\n    predict = cost_model.predict\n    configs = self.all_configs\n    if len(configs) == 0:\n        raise ValueError(\"The number of configurations is 0. Please specify a cost model to use the function find_estimated_top_configs.\")\n    if not isinstance(configs[0], Config):\n        raise ValueError(\"The configurations specified are not Config objects. Please specify Config objects to use the function find_estimated_top_configs.\")\n    if self.metric is None:\n        raise ValueError(\"No metric specified. Please specify a metric to use the function find_estimated_top_configs.\")\n    if not isinstance(self.metric, Metric):\n        raise ValueError(\"The metric specified is not a Metric object. Please specify a Metric object to use the function find_estimated_top_configs.\")\n    if self.metric.direction is None:\n        raise ValueError(\"No direction specified for the metric. Please specify a direction to use the function find_estimated_top_configs.\")\n    if not isinstance(self.metric.direction, Direction):\n        raise ValueError(\"The direction specified for the metric is not a Direction object. Please specify a Direction object to use the function find_estimated_top_configs.\")\n    if self.metric.direction == Direction.MAXIMIZE:\n        indices = np.argsort(predict(feature_val))\n    else:\n        indices = np.argsort", "CodeBleu_score": 19.153819365874288}, {"code": "def find_estimated_top_configs(self)\n\t\n    flattened_ds_config = flatten(self.ds_config)\n    feature_val = []\n    for key, value in flattened_ds_config.items():\n        if isinstance(value, str):\n            continue\n        feature_val.append(value)\n    feature_val = np.array(feature_val)\n    if self.cost_model is None:\n        raise ValueError(\n            \"Cost model is not defined. Please define a cost model to find estimated top configurations.\"\n        )\n    if self.cost_model.predict is None:\n        raise ValueError(\n            \"Cost model does not have a predict method. Please define a cost model with a predict method to find estimated top configurations.\"\n        )\n    configs = []\n    for config in self.all_configs:\n        configs.append(flatten(config))\n    configs = np.array(configs)\n    if len(configs) == 0:\n        raise ValueError(\n            \"No configurations are found. Please define configurations to find estimated top configurations.\"\n        )\n    if len(feature_val) == 0:\n        raise ValueError(\n            \"No numerical values are found. Please define numerical values to find estimated top configurations.\"\n        )\n    if len(configs) != len(feature_val):\n        raise ValueError(\n            \"The number of configurations and the number of numerical values are not equal. Please define the same number of configurations and numerical values to find estimated top configurations.\"\n        )\n    estimated_costs = self.cost_model.predict(feature_val)\n    if len(estimated_costs) == 0:\n        raise ValueError(\n            \"No estimated costs are found. Please define a cost model with a predict method to find estimated top configurations.\"\n        )\n    if len(estimated_costs) != len(configs):\n        raise ValueError(\n            \"The number of estimated costs and the number of configurations are not equal. Please define the same number of estimated costs and configurations to find estimated top configurations.\"\n        )\n    sorted_indices = np.argsort(estimated_costs)\n    sorted_estimated_costs = estimated_costs[sorted_", "CodeBleu_score": 22.173603155675764}]}
{"instance_id": "d4cf48413a57", "generation_result": [{"code": "def flatten(d, parent_key='', sep='_')\n\t\n    items = []\n    for k, v in d.items():\n        new_key = parent_key + sep + k if parent_key else k\n        if isinstance(v, collections.MutableMapping):\n            items.extend(flatten(v, new_key, sep=sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)\n\n", "CodeBleu_score": 78.2184919550726}, {"code": "def flatten(d, parent_key='', sep='_')\n\t\n    items = []\n    for k, v in d.items():\n        new_key = parent_key + sep + k if parent_key else k\n        if isinstance(v, MutableMapping):\n            items.extend(flatten(v, new_key, sep=sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)\n\n", "CodeBleu_score": 76.47722435430262}, {"code": "def flatten(d, parent_key='', sep='_')\n\t\n    items = []\n    for k, v in d.items():\n        new_key = parent_key + sep + k if parent_key else k\n        if isinstance(v, collections.MutableMapping):\n            items.extend(flatten(v, new_key, sep=sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)\n\n\n", "CodeBleu_score": 78.2184919550726}]}
{"instance_id": "c2a514249c25", "generation_result": [{"code": "def run_after_tuning(self)\n\t\n    if self.optimal_cmd:\n        logger.info(\"Running command: %s\", self.optimal_cmd)\n        result = subprocess.Popen(self.optimal_cmd, shell=True)\n        result.wait()\n        logger.info(\"Command %s completed successfully.\", self.optimal_cmd)\n    else:\n        logger.info(\"AutoTuning did not find an optimal configuration.\")\n\n", "CodeBleu_score": 50.81391733119629}, {"code": "def run_after_tuning(self)\n\t\n    if self.optimal_cmd:\n        logger.info(\"Running after tuning command: \" + self.optimal_cmd)\n        result = subprocess.Popen(self.optimal_cmd, shell=True)\n        result.wait()\n        logger.info(\"Successfully ran after tuning command: \" + self.optimal_cmd)\n    else:\n        logger.info(\"Autotuning did not find an optimal command.\")\n\n", "CodeBleu_score": 49.843831939238655}, {"code": "def run_after_tuning(self)\n\t\n    logger.info(\"Running after tuning\")\n    if self.optimal_cmd is not None:\n        try:\n            result = subprocess.Popen(self.optimal_cmd, shell=True)\n            result.wait()\n            logger.info(\"After tuning, the command completed successfully\")\n        except subprocess.CalledProcessError as e:\n            logger.info(\"After tuning, the command returned an error\")\n            logger.info(e)\n    else:\n        logger.info(\"After tuning, the command was not found\")\n\n", "CodeBleu_score": 44.98921940137072}]}
{"instance_id": "61802de52df0", "generation_result": [{"code": "def run_job(self, exp: dict, reservations)\n\t\n    self.log.info(f\"Running experiment {exp['name']}\")\n    exp['master_port'] = self.get_master_port()\n    exp['result_dir'] = self.get_result_dir(exp['name'])\n    self.update_user_args(exp, reservations)\n    t = threading.Thread(target=self.run_experiment, args=(exp,))\n    t.start()\n    self.running_experiments[time.time()] = exp\n\n", "CodeBleu_score": 19.024066951905773}, {"code": "def run_job(self, exp: dict, reservations)\n\t\n    # TODO: use a lock to prevent concurrent access to running_experiments\n    # TODO: use a lock to prevent concurrent access to running_experiments\n    # TODO: use a lock to prevent concurrent access to running_experiments\n    # TODO: use a lock to prevent concurrent access to running_experiments\n    # TODO: use a lock to prevent concurrent access to running_experiments\n    # TODO: use a lock to prevent concurrent access to running_experiments\n    # TODO: use a lock to prevent concurrent access to running_experiments\n    # TODO: use a lock to prevent concurrent access to running_experiments\n    # TODO: use a lock to prevent concurrent access to running_experiments\n    # TODO: use a lock to prevent concurrent access to running_experiments\n    # TODO: use a lock to prevent concurrent access to running_experiments\n    # TODO: use a lock to prevent concurrent access to running_experiments\n    # TODO: use a lock to prevent concurrent access to running_experiments\n    # TODO: use a lock to prevent concurrent access to running_experiments\n    # TODO: use a lock to prevent concurrent access to running_experiments\n    # TODO: use a lock to prevent concurrent access to running_experiments\n    # TODO: use a lock to prevent concurrent access to running_experiments\n    # TODO: use a lock to prevent concurrent access to running_experiments\n    # TODO: use a lock to prevent concurrent access to running_experiments\n    # TODO: use a lock to prevent concurrent access to running_experiments\n    # TODO: use a lock to prevent concurrent access to running_experiments\n    # TODO: use a lock to prevent concurrent access to running_experiments\n    # TODO: use a lock to prevent concurrent access to running_experiments\n    # TODO: use a lock to prevent concurrent access to running_experiments\n    # TODO: use a lock to prevent concurrent access to running_experiments\n    # TODO: use a lock to prevent concurrent access to running_experiments\n    # TODO: use a lock to prevent concurrent access to running_experiments\n    # TODO: use a lock to prevent concurrent access to running_experiments\n    # TODO: use a lock", "CodeBleu_score": 1.8529411764705885}, {"code": "def run_job(self, exp: dict, reservations)\n\t\n    if self.is_running():\n        raise Exception(\"A job is already running\")\n\n    # set master port\n    exp[\"master_port\"] = self.get_master_port()\n\n    # set result directory\n    exp[\"result_dir\"] = self.get_result_dir()\n\n    # update user arguments\n    user_args = exp[\"user_args\"]\n    for arg_mapping in exp[\"arg_mappings\"]:\n        if arg_mapping[\"key\"] in user_args:\n            user_args[user_args.index(arg_mapping[\"key\"])] = arg_mapping[\"value\"]\n\n    # start experiment in a new thread\n    t = threading.Thread(target=self.run_experiment, args=(exp, reservations))\n    t.start()\n\n    # store experiment details\n    self.running_experiments.append({\n        \"experiment\": exp,\n        \"thread\": t,\n        \"start_time\": time.time()\n    })\n\n", "CodeBleu_score": 28.02909330663213}]}
{"instance_id": "4884e2e9665c", "generation_result": [{"code": "def save_exp_results_to_database(self, message, ranks=None, path=None):\n\t\n    if self.dist.is_initialized() and self.dist.get_rank() != 0:\n        return\n    if ranks is None:\n        ranks = [self.dist.get_rank()]\n    if path is None:\n        path = self.outfile.write\n    message = self.logger.debug(message)\n    message = {\n        \"rank\": ranks,\n        \"message\": message\n    }\n    path.write(json.dumps(message))\n\n", "CodeBleu_score": 27.17831290987679}, {"code": "def save_exp_results_to_database(self, message, ranks=None, path=None):\n\t\n    if dist.is_initialized() and dist.get_rank() != 0:\n        if ranks is None:\n            ranks = []\n        if path is None:\n            path = self.log_path\n        message = f'{dist.get_rank()}: {message}'\n        ranks.append(dist.get_rank())\n        with open(path, 'a') as outfile:\n            json.dump({message}, outfile, indent=4)\n    else:\n        if ranks is None:\n            ranks = []\n        if path is None:\n            path = self.log_path\n        with open(path, 'a') as outfile:\n            json.dump({message}, outfile, indent=4)\n\n", "CodeBleu_score": 30.621308568570775}, {"code": "def save_exp_results_to_database(self, message, ranks=None, path=None):\n\t\n    if dist.is_initialized():\n        if dist.get_rank() == 0:\n            if message != \"\":\n                if ranks is None:\n                    ranks = dist.get_rank()\n                else:\n                    ranks = [rank for rank in ranks]\n                message = ranks + [message]\n                with open(path, \"a\") as outfile:\n                    json.dump(message, outfile)\n                    outfile.write(\"", "CodeBleu_score": 32.29221221367364}]}
{"instance_id": "0429344c3c4d", "generation_result": [{"code": "def _build_tp_other_layer_map(self, layer_index: int)\n\t\n    layer_files = self.get_files_with_prefix(f\"{self.tp_prefix}_{layer_index}\")\n    if len(layer_files) == 0:\n        return None\n    if layer_index != self.tp_degree:\n        return None\n    tp_other_layer_map = {}\n    for tp_index, layer_file in enumerate(layer_files):\n        tp_other_layer_map[tp_index] = self.partition_data(layer_file, tp_index)\n    return tp_other_layer_map\n\n", "CodeBleu_score": 33.20073706809518}, {"code": "def _build_tp_other_layer_map(self, layer_index: int)\n\t\n    if not self.has_layer_files:\n        raise ValueError(\"There are no layer files.\")\n    if layer_index < 0 or layer_index >= self.tp_degree:\n        raise ValueError(f\"The layer index must be between 0 and {self.tp_degree - 1}.\")\n    layer_files = self.get_files_with_prefix(f\"{self.layer_prefix}{layer_index}\")\n    if len(layer_files) == 0:\n        raise ValueError(f\"There are no files with the prefix {self.layer_prefix}{layer_index}.\")\n    return {\n        partition_index: layer_files[partition_index::self.tp_degree]\n        for partition_index in range(self.tp_degree)\n    }\n\n", "CodeBleu_score": 30.9399995657896}, {"code": "def _build_tp_other_layer_map(self, layer_index: int)\n\t\n    if self.layer_files[layer_index]:\n        if layer_index != self.layer_index:\n            raise ValueError(\"Layer index does not match the layer index of the data map.\")\n        else:\n            # get the partition indices\n            partition_indices = list(self.partition_map.keys())\n            # get the files for the given layer\n            layer_files = self.get_files_with_prefix(layer_index)\n            # partition the files\n            partition_data = self.partition_data(layer_files, self.tp_degree)\n            # create a map for the partition indices\n            tp_other_layer_map = {}\n            # loop over the partition indices\n            for partition_index in partition_indices:\n                # get the files for the partition index\n                partition_files = partition_data[partition_index]\n                # add the files to the map\n                tp_other_layer_map[partition_index] = partition_files\n            # return the map\n            return tp_other_layer_map\n    else:\n        raise ValueError(\"No layer files found.\")\n\n", "CodeBleu_score": 34.18065086615496}]}
{"instance_id": "1b65ecb9be93", "generation_result": [{"code": "def system_info()\n\t\n    system_dict = {'OS': platform.system(),\n                    'Architecture': platform.architecture(),\n                    'Version': platform.version(),\n                    'Processor': platform.processor(),\n                    'Python Version': platform.python_version()}\n    return system_dict\n\n", "CodeBleu_score": 48.398300694902744}, {"code": "def system_info()\n\t\n    system_info_dict = {\n        \"Operating System\": platform.system(),\n        \"Architecture\": platform.architecture(),\n        \"Version\": platform.version(),\n        \"Processor\": platform.processor(),\n        \"Python Version\": platform.python_version()\n    }\n    return system_info_dict\n\n", "CodeBleu_score": 50.73421455865964}, {"code": "def system_info()\n\t\n    system_info = {\n        \"Operating System\": platform.system(),\n        \"Architecture\": platform.architecture(),\n        \"Version\": platform.version(),\n        \"Processor\": platform.processor(),\n        \"Python Version\": platform.python_version()\n    }\n    return system_info\n\n", "CodeBleu_score": 51.39546124963006}]}
{"instance_id": "969bcc5cf3b5", "generation_result": [{"code": "def cuda_info()\n\t\n    from torch import cuda\n    import platform\n    import torch\n    import os\n    import sys\n    import subprocess\n\n    # get the number of GPUs\n    device_count = cuda.device_count()\n\n    # get the name of each GPU\n    device_name = []\n    for i in range(device_count):\n        device_name.append(cuda.get_device_name(i))\n\n    # get the availability of CUDA\n    is_available = cuda.is_available()\n\n    # get the CUDA version\n    cuda_version = cuda.version()\n\n    # get the CUDA architecture\n    cuda_architecture = cuda.get_arch_list()\n\n    # get the CUDA driver version\n    cuda_driver_version = cuda.get_driver_version()\n\n    # get the CUDA runtime version\n    cuda_runtime_version = cuda.runtime_version()\n\n    # get the CUDA capabilities\n    cuda_capabilities = cuda.get_device_capability(0)\n\n    # get the CUDA compute capabilities\n    cuda_compute_capabilities = cuda.get_device_properties(0)\n\n    # get the CUDA device properties\n    cuda_device_properties = cuda.get_device_properties(0)\n\n    # get the CUDA device properties\n    cuda_device_properties = cuda.get_device_properties(0)\n\n    # get the CUDA device properties\n    cuda_device_properties = cuda.get_device_properties(0)\n\n    # get the CUDA device properties\n    cuda_device_properties = cuda.get_device_properties(0)\n\n    # get the CUDA device properties\n    cuda_device_properties = cuda.get_device_properties(0)\n\n    # get the CUDA device properties\n    cuda_device_properties = cuda.get_device_properties(0)\n\n    # get the CUDA device properties\n    cuda_device_properties = cuda.get_device_properties(0)\n\n    #", "CodeBleu_score": 28.90689248000492}, {"code": "def cuda_info()\n\t\n    return get_device_name(), is_available(), range()\n\n", "CodeBleu_score": 2.811190476837308}, {"code": "def cuda_info()\n\t\n    device_list = []\n    for i in range(get_device_count()):\n        device_list.append(get_device_name(i))\n    cuda_available = is_available()\n    cuda_version = get_version()\n    return device_list, cuda_available, cuda_version\n\n", "CodeBleu_score": 32.12032899627976}]}
{"instance_id": "a2ba908a4c70", "generation_result": [{"code": "def compute_encoder_accuracy(dataset_items, encoder_manager)\n\t\n    class_acc_dict = {}\n    for item in tqdm(dataset_items):\n        try:\n            audio_file = item.getattr(\"audio_file\")\n        except AttributeError:\n            audio_file = item.getattr(\"path\")\n        embedding = encoder_manager.compute_embedding_from_clip(audio_file)\n        try:\n            predicted_class_id = encoder_manager.predict_class_id(embedding)\n            predicted_class_name = encoder_manager.class_id_to_class_name[predicted_class_id]\n        except RuntimeError:\n            print(\"Encoder failed to compute embedding for audio file: {}\".format(audio_file))\n            continue\n        try:\n            true_class_name = item.getattr(\"class_name\")\n        except AttributeError:\n            true_class_name = item.getattr(\"label\")\n        if true_class_name in encoder_manager.class_name_to_class_id:\n            true_class_id = encoder_manager.class_name_to_class_id[true_class_name]\n        else:\n            true_class_id = None\n        if predicted_class_name in encoder_manager.class_name_to_class_id:\n            predicted_class_id = encoder_manager.class_name_to_class_id[predicted_class_name]\n        else:\n            predicted_class_id = None\n        if true_class_id is not None and predicted_class_id is not None:\n            if true_class_id == predicted_class_id:\n                class_acc_dict[true_class_id] = class_acc_dict.get(true_class_id, 0) + 1\n            else:\n                class_acc_dict[true_class_id] = class_acc_dict.get(true_class_id, 0)\n                class_acc_dict[predicted_class_id] = class_acc_dict.get(predicted_class_id, 0)\n    for class_id, class_acc in class_acc_dict.items():\n        print(\"Class ID: {}", "CodeBleu_score": 35.06859853423654}, {"code": "def compute_encoder_accuracy(dataset_items, encoder_manager)\n\t\n    class_acc_dict = {}\n    for item in tqdm(dataset_items):\n        audio_file = item[0]\n        true_class = item[1]\n        true_class_name = item[2]\n        true_class_id = item[3]\n        embedding = encoder_manager.compute_embedding_from_clip(audio_file)\n        if embedding is None:\n            continue\n        prediction = encoder_manager.predict_class_from_embedding(embedding)\n        if prediction is None:\n            continue\n        if prediction not in class_acc_dict:\n            class_acc_dict[prediction] = 0\n        if true_class_name in encoder_manager.class_name_to_id_mapping:\n            if encoder_manager.class_name_to_id_mapping[true_class_name] == prediction:\n                class_acc_dict[prediction] += 1\n    print(\"Encoder accuracy\")\n    for class_name, count in class_acc_dict.items():\n        print(f\"{class_name}: {count / len(dataset_items):.2f}\")\n    print(f\"Average: {sum(class_acc_dict.values()) / len(dataset_items):.2f}\")\n\n", "CodeBleu_score": 27.991950574834473}, {"code": "def compute_encoder_accuracy(dataset_items, encoder_manager)\n\t\n    class_acc_dict = {}\n    for item in tqdm(dataset_items, desc='Computing encoder accuracy'):\n        try:\n            audio_file = item['audio_file']\n            class_name = item['class_name']\n            class_id = item['class_id']\n        except KeyError:\n            audio_file = item['audio_file']\n            class_name = None\n            class_id = None\n        embedding = encoder_manager.compute_embedding_from_clip(audio_file)\n        class_label = encoder_manager.predict_class(embedding)\n        if class_name is not None:\n            if class_name not in class_acc_dict:\n                class_acc_dict[class_name] = 0\n            if class_name == class_label:\n                class_acc_dict[class_name] += 1\n        if class_id is not None:\n            if class_id not in class_acc_dict:\n                class_acc_dict[class_id] = 0\n            if class_id == class_label:\n                class_acc_dict[class_id] += 1\n    class_acc_dict = dict(sorted(class_acc_dict.items(), key=lambda x: x[1], reverse=True))\n    print(f'Encoder accuracy:')\n    for class_name, class_acc in class_acc_dict.items():\n        print(f'{class_name}: {class_acc / len(dataset_items) * 100:.2f}%')\n    print(f'Average accuracy: {sum(class_acc_dict.values()) / len(dataset_items) * 100:.2f}%')\n    return class_acc_dict\n\n", "CodeBleu_score": 37.46137362559365}]}
{"instance_id": "e01ec69ad58a", "generation_result": [{"code": "def adjust_path_and_remove_silence(audio_path)\n\t\n    # If the file already exists and force is not specified, it returns the output path and a False flag.\n    if not force and os.path.exists(audio_path):\n        return audio_path, False\n    # Otherwise, it creates the necessary directory structure, removes silence from the audio using specified parameters, and saves the processed audio.\n    else:\n        # It replaces the input directory path with the output directory path to determine the new file location.\n        audio_path = audio_path.replace(input_dir, output_dir)\n        # It creates the necessary directory structure.\n        os.makedirs(os.path.dirname(audio_path), exist_ok=True)\n        # It removes silence from the audio using specified parameters.\n        audio_path = remove_silence(audio_path, silence_threshold=-50, silence_duration_threshold=0.5, silence_chunk_duration=0.1)\n        # It saves the processed audio.\n        audio_path = join(output_dir, audio_path.split(sep=os.sep)[-1])\n        return audio_path, True\n\n", "CodeBleu_score": 40.36468586425174}, {"code": "def adjust_path_and_remove_silence(audio_path)\n\t\n    audio_path = audio_path.replace(\"\\\\\", \"/\")\n    audio_path = audio_path.replace(\"//\", \"/\")\n    audio_path = audio_path.replace(\"\\\\\\\\\", \"/\")\n    audio_path = audio_path.replace(\"//\", \"/\")\n    audio_path = audio_path.replace(\"\\\\\\\\\", \"/\")\n    audio_path = audio_path.replace(\"\\\\\\\\\\\\\", \"/\")\n    audio_path = audio_path.replace(\"//\\\\\", \"/\")\n    audio_path = audio_path.replace(\"\\\\\\\\\\\\\\\\\", \"/\")\n    audio_path = audio_path.replace(\"//\\\\\\\\\", \"/\")\n    audio_path = audio_path.replace(\"\\\\\\\\\\\\\\\\\\\\\", \"/\")\n    audio_path = audio_path.replace(\"//\\\\\\\\\\\\\", \"/\")\n    audio_path = audio_path.replace(\"\\\\\\\\\\\\\\\\\\\\\\\\\", \"/\")\n    audio_path = audio_path.replace(\"//\\\\\\\\\\\\\\\\\", \"/\")\n    audio_path = audio_path.replace(\"\\\\\\\\\\\\\\\\\\\\\\\\\\\\\", \"/\")\n    audio_path = audio_path.replace(\"//\\\\\\\\\\\\\\\\\\\\\", \"/\")\n    audio_path = audio_path.replace(\"\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\", \"/\")\n    audio_path = audio_path.replace(\"//\\\\\\\\\\\\\\\\\\\\\\\\\", \"/\")\n    audio_path = audio_path.replace(\"\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\", \"/\")\n    audio_path = audio_path.replace(\"//\\\\\\\\\\\\\\\\\\\\\\\\\\\\\", \"/\")\n    audio_path = audio_path.replace(\"\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\", \"/\")\n    audio_path = audio_path.replace(\"//\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\", \"/\")\n    audio_path = audio_path.replace(\"\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\", \"/\")\n    audio_path = audio_path.replace(\"//\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\", \"/\")\n    audio_path = audio_path.replace(\"\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\", \"/\")\n    audio_path = audio_path.replace(\"//\\\\\\\\\\\\", "CodeBleu_score": 18.632230311450176}, {"code": "def adjust_path_and_remove_silence(audio_path)\n\t\n    audio_path = str(audio_path)\n    audio_path = audio_path.replace(\"\\\\\", \"/\")\n    if audio_path.startswith(\"file://\"):\n        audio_path = audio_path.replace(\"file://\", \"\")\n    audio_path = Path(audio_path)\n    audio_path = audio_path.parent\n    audio_path = audio_path.joinpath(audio_path.name.split(\".\")[0] + \"_no_silence.wav\")\n    audio_path = str(audio_path)\n    if audio_path.startswith(\"file://\"):\n        audio_path = audio_path.replace(\"file://\", \"\")\n    audio_path = Path(audio_path)\n    if audio_path.exists():\n        return audio_path, False\n    audio_path = audio_path.parent\n    if not audio_path.exists():\n        audio_path.mkdir(parents=True, exist_ok=True)\n    audio_path = audio_path.joinpath(audio_path.name.split(\".\")[0] + \"_no_silence.wav\")\n    audio_path = str(audio_path)\n    if audio_path.startswith(\"file://\"):\n        audio_path = audio_path.replace(\"file://\", \"\")\n    audio_path = Path(audio_path)\n    if audio_path.exists():\n        return audio_path, False\n    audio_path = audio_path.parent\n    if not audio_path.exists():\n        audio_path.mkdir(parents=True, exist_ok=True)\n    audio_path = audio_path.joinpath(audio_path.name.split(\".\")[0] + \"_no_silence.wav\")\n    audio_path = str(audio_path)\n    if audio_path.startswith(\"file://\"):\n        audio_path = audio_path.replace(\"file://\", \"\")\n    audio_path = Path(audio_path)\n    if audio_path.exists():\n        return audio_path, False\n    audio_path = audio_path.parent\n    if not audio_path.exists():\n        audio_path.mkdir(par", "CodeBleu_score": 33.08586883607324}]}
{"instance_id": "7e0bdb0feff8", "generation_result": [{"code": "def resample_files(input_dir, output_sr, output_dir=None, file_ext=\"wav\", n_jobs=10)\n\t\n    if output_dir is None:\n        output_dir = input_dir\n    else:\n        shutil.copytree(input_dir, output_dir)\n    files = glob.glob(os.path.join(output_dir, \"**\", \"*.\" + file_ext), recursive=True)\n    pbar = tqdm(total=len(files))\n    p = Pool(n_jobs)\n    p.imap_unordered(resample_file, zip(files, pbar))\n    p.close()\n    p.join()\n    print(\"Done resampling all files.\")\n\n", "CodeBleu_score": 35.025312563012854}, {"code": "def resample_files(input_dir, output_sr, output_dir=None, file_ext=\"wav\", n_jobs=10)\n\t\n    if output_dir is None:\n        output_dir = input_dir\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    if not os.path.exists(input_dir):\n        raise ValueError(\"Input directory does not exist.\")\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    if not os.path.exists(output_dir):\n        raise ValueError(\"Output directory does not exist.\")\n    if not os.path.isdir(input_dir):\n        raise ValueError(\"Input is not a directory.\")\n    if not os.path.isdir(output_dir):\n        raise ValueError(\"Output is not a directory.\")\n    if not file_ext.startswith(\".\"):\n        file_ext = \".\" + file_ext\n    if not file_ext.endswith(\"*\"):\n        file_ext = file_ext + \"*\"\n    files = glob.glob(os.path.join(input_dir, file_ext))\n    if len(files) == 0:\n        raise ValueError(\"No files found.\")\n    with Pool(n_jobs) as pool:\n        tqdm.tqdm.write(\"Resampling files...\")\n        pool.imap_unordered(resample_file, zip(files, repeat(output_sr), repeat(output_dir)))\n    tqdm.tqdm.write(\"Resampling complete.\")\n    tqdm.tqdm.write(\"Copying files...\")\n    copytree(input_dir, output_dir)\n    tqdm.tqdm.write(\"Copying complete.\")\n\n", "CodeBleu_score": 32.75566481485319}, {"code": "def resample_files(input_dir, output_sr, output_dir=None, file_ext=\"wav\", n_jobs=10)\n\t\n    if output_dir is None:\n        output_dir = input_dir\n    else:\n        copytree(input_dir, output_dir)\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    files = glob.glob(os.path.join(input_dir, \"**/*.\" + file_ext), recursive=True)\n    pbar = tqdm(total=len(files), desc=\"Resampling files\")\n    with Pool(n_jobs) as pool:\n        pool.starmap(resample_file, zip(files, pbar))\n    print(\"Resampling complete!\")\n\n", "CodeBleu_score": 37.713514883513014}]}
{"instance_id": "a1351834ca57", "generation_result": [{"code": "def evaluation(model, criterion, data_loader, global_step)\n\t\n    model.eval()\n    eval_loss = 0\n    eval_correct = 0\n    eval_total = 0\n    eval_labels = []\n    eval_embeddings = []\n    for batch_idx, (inputs, labels) in enumerate(data_loader):\n        inputs = inputs.view(inputs.shape[0], -1)\n        inputs = inputs.cuda() if torch.cuda.is_available() else inputs\n        labels = labels.view(labels.shape[0], -1)\n        labels = labels.cuda() if torch.cuda.is_available() else labels\n        with torch.no_grad():\n            outputs = model(inputs)\n            outputs = outputs.view(outputs.shape[0], -1)\n            loss = criterion(outputs, labels)\n            eval_loss += loss.item()\n            eval_correct += (outputs.argmax(1) == labels).sum().item()\n            eval_total += labels.size(0)\n            eval_labels.append(labels.cpu().numpy())\n            eval_embeddings.append(outputs.cpu().numpy())\n    eval_loss /= len(data_loader)\n    eval_accuracy = eval_correct / eval_total\n    dashboard_logger.eval_stats(\n        global_step, eval_loss, eval_accuracy, len(data_loader)\n    )\n    eval_labels = np.concatenate(eval_labels)\n    eval_embeddings = np.concatenate(eval_embeddings)\n    eval_figures = plot_embeddings(eval_embeddings, eval_labels)\n    dashboard_logger.eval_figures(global_step, eval_figures)\n    return eval_loss\n\n", "CodeBleu_score": 35.557919071367024}, {"code": "def evaluation(model, criterion, data_loader, global_step)\n\t\n    model.eval()\n    eval_loss = 0\n    eval_batches = 0\n    embeddings = []\n    labels = []\n    with torch.no_grad():\n        for batch in data_loader:\n            inputs, labels_ = batch\n            inputs = inputs.view(-1, *inputs.shape[-3:])\n            labels = labels_.view(-1)\n            inputs = inputs.cuda() if torch.cuda.is_available() else inputs\n            labels = labels.cuda() if torch.cuda.is_available() else labels\n            outputs = model(inputs)\n            outputs = outputs.view(-1, *outputs.shape[-3:])\n            loss = criterion(outputs, labels)\n            eval_loss += loss.item()\n            eval_batches += 1\n            embeddings.append(outputs.detach().cpu())\n            labels.append(labels.detach().cpu())\n        eval_loss /= eval_batches\n        embeddings = torch.cat(embeddings)\n        labels = torch.cat(labels)\n        eval_figures = plot_embeddings(embeddings, labels, global_step)\n        dashboard_logger.eval_stats(eval_loss, eval_batches, global_step)\n        dashboard_logger.eval_figures(eval_figures, global_step)\n        return eval_loss\n\n", "CodeBleu_score": 38.40053571774462}, {"code": "def evaluation(model, criterion, data_loader, global_step)\n\t\n    model.eval()\n    eval_loss = 0.0\n    eval_correct = 0.0\n    eval_total = 0.0\n    eval_labels = []\n    eval_preds = []\n    eval_embeddings = []\n    for i, (inputs, labels) in enumerate(data_loader):\n        inputs = inputs.view(inputs.size(0), -1)\n        labels = labels.view(labels.size(0))\n        if torch.cuda.is_available():\n            inputs = inputs.cuda()\n            labels = labels.cuda()\n        with torch.no_grad():\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            eval_loss += loss.item()\n            _, preds = torch.max(outputs, 1)\n            eval_correct += torch.sum(preds == labels.data)\n            eval_total += labels.size(0)\n            eval_labels.extend(labels.cpu().numpy())\n            eval_preds.extend(preds.cpu().numpy())\n            eval_embeddings.extend(outputs.cpu().numpy())\n    eval_loss /= len(data_loader)\n    eval_accuracy = eval_correct / eval_total\n    eval_stats = {\n        \"eval_loss\": eval_loss,\n        \"eval_accuracy\": eval_accuracy,\n        \"eval_precision\": precision_score(eval_labels, eval_preds, average=\"micro\"),\n        \"eval_recall\": recall_score(eval_labels, eval_preds, average=\"micro\"),\n        \"eval_f1\": f1_score(eval_labels, eval_preds, average=\"micro\"),\n    }\n    dashboard_logger.eval_stats(global_step, eval_stats)\n    eval_figures = {\n        \"eval_embeddings\": plot_embeddings(eval_embeddings, labels=eval_labels),\n    }\n    dashboard_logger.eval_figures(global_step, eval_figures)\n    return eval_loss\n\n", "CodeBleu_score": 37.79295864610576}]}
{"instance_id": "565f8ed09916", "generation_result": [{"code": "def register_config(model_name: str) -> Coqpit\n\t\n    config_class_name = model_name + \"Config\"\n    if model_name == \"xtts\":\n        config_class_name = \"XttsConfig\"\n    try:\n        config_class = importlib.import_module(\"models.\" + model_name + \".config.\" + config_class_name)\n    except ModuleNotFoundError:\n        try:\n            config_class = importlib.import_module(\"models.\" + model_name + \".config.\" + config_class_name)\n        except ModuleNotFoundError:\n            try:\n                config_class = importlib.import_module(\"models.\" + model_name + \".config.\" + config_class_name)\n            except ModuleNotFoundError:\n                try:\n                    config_class = importlib.import_module(\"models.\" + model_name + \".config.\" + config_class_name)\n                except ModuleNotFoundError:\n                    try:\n                        config_class = importlib.import_module(\"models.\" + model_name + \".config.\" + config_class_name)\n                    except ModuleNotFoundError:\n                        try:\n                            config_class = importlib.import_module(\"models.\" + model_name + \".config.\" + config_class_name)\n                        except ModuleNotFoundError:\n                            raise ModuleNotFoundError(\"The configuration class for \" + model_name + \" could not be found.\")\n    return config_class\n\n", "CodeBleu_score": 26.842702740295564}, {"code": "def register_config(model_name: str) -> Coqpit\n\t\n    config_name = f'{model_name.capitalize()}Config'\n    if model_name == 'xtts':\n        config_name = 'XttsConfig'\n    try:\n        config_module = importlib.import_module(f'{model_name}.config')\n        config_class = getattr(config_module, config_name)\n    except ModuleNotFoundError:\n        try:\n            config_module = importlib.import_module(f'{model_name}.config')\n            config_class = getattr(config_module, config_name)\n        except ModuleNotFoundError:\n            try:\n                config_module = importlib.import_module(f'{model_name}.config.{model_name}')\n                config_class = getattr(config_module, config_name)\n            except ModuleNotFoundError:\n                try:\n                    config_module = importlib.import_module(f'{model_name}.config.{model_name}')\n                    config_class = getattr(config_module, config_name)\n                except ModuleNotFoundError:\n                    try:\n                        config_module = importlib.import_module(f'{model_name}.config.{model_name}')\n                        config_class = getattr(config_module, config_name)\n                    except ModuleNotFoundError:\n                        try:\n                            config_module = importlib.import_module(f'{model_name}.config.{model_name}')\n                            config_class = getattr(config_module, config_name)\n                        except ModuleNotFoundError:\n                            try:\n                                config_module = importlib.import_module(f'{model_name}.config.{model_name}')\n                                config_class = getattr(config_module, config_name)\n                            except ModuleNotFoundError:\n                                try:\n                                    config_module = importlib.import_module(f'{model_name}.config.{model_name}')\n                                    config_class = getattr(config_module, config_name)\n                                except ModuleNotFoundError:\n                                    try:\n                                        config_module =", "CodeBleu_score": 24.16036555350895}, {"code": "def register_config(model_name: str) -> Coqpit\n\t\n    if model_name == \"xtts\":\n        from .configs.xtts_config import XttsConfig\n\n        return XttsConfig\n    else:\n        config_class_name = \"\".join([model_name, \"Config\"])\n        try:\n            config_module = importlib.import_module(\n                \".\" + config_class_name, \"coqpit.configs\"\n            )\n            config_class = getattr(config_module, config_class_name)\n            return config_class\n        except ModuleNotFoundError:\n            raise ModuleNotFoundError(\n                f\"Configuration class for {model_name} model is not found. \"\n                \"Please make sure that the configuration class is defined \"\n                \"and the file is located in the configs folder.\"\n            )\n\n", "CodeBleu_score": 39.02968075232398}]}
{"instance_id": "0ea7862506a7", "generation_result": [{"code": "def load_config(config_path: str) -> Coqpit\n\t\n    config_path = str(config_path)\n    _, ext = splitext(config_path)\n    if ext == \".yaml\":\n        with open(config_path, \"r\") as f:\n            config_dict = yaml.safe_load(f)\n    elif ext == \".json\":\n        with open(config_path, \"r\") as f:\n            config_dict = json.load(f)\n    else:\n        raise TypeError(f\"Unknown file type: {ext}\")\n    model_name = _process_model_name(config_dict)\n    config_class = register_config(model_name)\n    config = config_class.from_dict(config_dict)\n    return config\n\n", "CodeBleu_score": 43.542425307981226}, {"code": "def load_config(config_path: str) -> Coqpit\n\t\n    config_dict = read_config(config_path)\n    model_name = _process_model_name(config_dict[\"model_name\"])\n    config_class = register_config(model_name)\n    config = config_class.from_dict(config_dict)\n    return config\n\n", "CodeBleu_score": 8.537292772690636}, {"code": "def load_config(config_path: str) -> Coqpit\n\t\n    config_path = Path(config_path)\n    if not config_path.exists():\n        raise FileNotFoundError(f\"Configuration file {config_path} not found.\")\n    if config_path.suffix == \".yaml\":\n        config_dict = read_yaml_with_comments(config_path)\n    elif config_path.suffix == \".json\":\n        config_dict = read_json_with_comments(config_path)\n    else:\n        raise TypeError(f\"Unknown file type: {config_path.suffix}\")\n    model_name = _process_model_name(config_dict)\n    config_class = register_config(model_name)\n    config = config_class.from_dict(config_dict)\n    return config\n\n", "CodeBleu_score": 36.90662016026933}]}
{"instance_id": "e0db68b63143", "generation_result": [{"code": "def list_files(basePath, validExts=None, contains=None)\n\t\n    # Walk through the directory structure\n    for root, dirs, files in os.walk(basePath):\n        # Process files\n        for name in files:\n            # Check if the file extension is valid\n            if validExts is not None and not name.lower().endswith(validExts):\n                continue\n            # Check if the file name contains the string\n            if contains is not None and contains not in name.lower():\n                continue\n            # Construct the full file path and yield it\n            yield os.path.join(root, name)\n\n", "CodeBleu_score": 37.436946421238886}, {"code": "def list_files(basePath, validExts=None, contains=None)\n\t\n    for root, dirs, files in os.walk(basePath):\n        for name in files:\n            if validExts is not None:\n                if not any(name.endswith(x) for x in validExts):\n                    continue\n            if contains is not None:\n                if contains not in name.lower():\n                    continue\n            yield os.path.join(root, name)\n\n", "CodeBleu_score": 38.26823696205929}, {"code": "def list_files(basePath, validExts=None, contains=None)\n\t\n\n    # if the validExts parameter is not provided, set it to an empty list\n    if validExts is None:\n        validExts = []\n\n    # if the contains parameter is not provided, set it to an empty string\n    if contains is None:\n        contains = \"\"\n\n    # if the basePath does not exist, raise an exception\n    if not os.path.exists(basePath):\n        raise Exception(\"basePath does not exist\")\n\n    # if the basePath is not a directory, raise an exception\n    if not os.path.isdir(basePath):\n        raise Exception(\"basePath is not a directory\")\n\n    # walk through the directory structure\n    for root, dirs, files in os.walk(basePath):\n        # process each file\n        for name in files:\n            # skip files that do not contain the 'contains' string in their names\n            if contains != \"\" and contains not in name.lower():\n                continue\n\n            # get the full file path\n            filePath = os.path.join(root, name)\n\n            # get the file extension\n            _, ext = os.path.splitext(filePath)\n\n            # if the file extension is in the valid extensions, process the file\n            if ext.lower() in validExts:\n                yield filePath\n\n", "CodeBleu_score": 37.90399815196353}]}
{"instance_id": "b836fcaf0a27", "generation_result": [{"code": "def load_model(xtts_checkpoint, xtts_config, xtts_vocab)\n\t\n    clear_gpu_cache()\n    if not xtts_checkpoint or not xtts_config or not xtts_vocab:\n        raise ValueError(\"XTTS checkpoint, config, and vocab paths are required.\")\n    config = XttsConfig.load_json(xtts_config)\n    Xtts.init_from_config(config)\n    XTTS_MODEL.load_checkpoint(xtts_checkpoint)\n    XTTS_MODEL.load_vocabulary(xtts_vocab)\n    if torch.cuda.is_available():\n        XTTS_MODEL.cuda()\n    print(\"Model loaded successfully.\")\n\n", "CodeBleu_score": 50.09053716556748}, {"code": "def load_model(xtts_checkpoint, xtts_config, xtts_vocab)\n\t\n    clear_gpu_cache()\n    if not xtts_checkpoint:\n        print(\"XTTS checkpoint path not provided. Please provide the path to the checkpoint file.\")\n        return\n    if not xtts_config:\n        print(\"XTTS config path not provided. Please provide the path to the config file.\")\n        return\n    if not xtts_vocab:\n        print(\"XTTS vocab path not provided. Please provide the path to the vocab file.\")\n        return\n    print(\"Loading XTTS configuration...\")\n    config = XttsConfig.load_json(xtts_config)\n    print(\"Initializing XTTS model...\")\n    XTTS_MODEL = Xtts.init_from_config(config)\n    print(\"Loading XTTS vocabulary...\")\n    XTTS_MODEL.load_vocabulary(xtts_vocab)\n    print(\"Loading XTTS checkpoint...\")\n    XTTS_MODEL.load_checkpoint(xtts_checkpoint)\n    if torch.cuda.is_available():\n        print(\"Moving model to GPU...\")\n        XTTS_MODEL.cuda()\n    print(\"XTTS model loaded successfully.\")\n\n", "CodeBleu_score": 52.52090968815026}, {"code": "def load_model(xtts_checkpoint, xtts_config, xtts_vocab)\n\t\n    clear_gpu_cache()\n    if not os.path.exists(xtts_checkpoint):\n        raise Exception(\"XTTS checkpoint not found at: \" + xtts_checkpoint)\n    if not os.path.exists(xtts_config):\n        raise Exception(\"XTTS config not found at: \" + xtts_config)\n    if not os.path.exists(xtts_vocab):\n        raise Exception(\"XTTS vocab not found at: \" + xtts_vocab)\n    config = XttsConfig.load_json(xtts_config)\n    Xtts.init_from_config(config)\n    XTTS_MODEL.cuda()\n    if not XTTS_MODEL.is_available():\n        raise Exception(\"CUDA not available\")\n    XTTS_MODEL.load_checkpoint(xtts_checkpoint)\n    XTTS_MODEL.load_vocab(xtts_vocab)\n    print(\"XTTS model loaded successfully\")\n\n", "CodeBleu_score": 49.40612337767594}]}
{"instance_id": "c5d63aa0df26", "generation_result": [{"code": "def compute_embedding(self, x, num_frames=250, num_eval=10, return_mean=True, l2_norm=True)\n\t\n    # Compute the number of frames to use\n    if self.torch_specifications:\n        num_frames = int(num_frames * self.hop_length)\n    if num_frames > x.shape[1]:\n        num_frames = x.shape[1]\n    # Compute the offsets for evaluation\n    offset_positions = np.linspace(0, x.shape[1] - num_frames, num_eval, endpoint=False)\n    # Extract frames\n    frames_batch = []\n    for offset in offset_positions:\n        frames_batch.append(x[:, int(offset):int(offset + num_frames)])\n    # Concatenate frames into a batch\n    frames_batch = torch.cat(frames_batch, dim=0)\n    # Perform inference\n    embeddings = self.inference(frames_batch)\n    # Compute the mean of the embeddings\n    if return_mean:\n        embeddings = torch.mean(embeddings, dim=0)\n    # Normalize with L2 norm\n    if l2_norm:\n        embeddings = torch.nn.functional.normalize(embeddings, dim=1)\n    # Return the embeddings\n    return embeddings\n\n", "CodeBleu_score": 52.179929059007115}, {"code": "def compute_embedding(self, x, num_frames=250, num_eval=10, return_mean=True, l2_norm=True)\n\t\n    # if x.shape[1] < num_frames:\n    #     num_frames = x.shape[1]\n    # if num_frames < 250:\n    #     num_frames = 250\n    # if num_frames % 2 != 0:\n    #     num_frames = num_frames - 1\n    # num_frames = int(num_frames)\n    # if num_frames > 250:\n    #     num_frames = 250\n    # num_frames = 250\n    # num_frames = 100\n    # num_frames = 250\n    # num_frames = 100\n    # num_frames = 250\n    # num_frames = 250\n    # num_frames = 250\n    # num_frames = 250\n    # num_frames = 250\n    # num_frames = 250\n    # num_frames = 250\n    # num_frames = 250\n    # num_frames = 250\n    # num_frames = 250\n    # num_frames = 250\n    # num_frames = 250\n    # num_frames = 250\n    # num_frames = 250\n    # num_frames = 250\n    # num_frames = 250\n    # num_frames = 250\n    # num_frames = 250\n    # num_frames = 250\n    # num_frames = 250\n    # num_frames = 250\n    # num_frames = 250\n    # num_frames = 250\n    # num_frames = 250\n    # num_frames = 250\n    # num_frames = 250\n    # num_frames = 250\n    # num_frames = 250\n    # num_frames = 250\n    # num_frames = 250\n    # num_frames = 250\n    # num_frames =", "CodeBleu_score": 3.814953483095076}, {"code": "def compute_embedding(self, x, num_frames=250, num_eval=10, return_mean=True, l2_norm=True)\n\t\n    if self.use_torch:\n        num_frames = int(num_frames / self.hop_length)\n    num_frames = min(num_frames, x.shape[1])\n    offset_positions = np.linspace(0, x.shape[1] - num_frames, num_eval, endpoint=False)\n    frames_batch = []\n    for offset in offset_positions:\n        frames_batch.append(x[:, int(offset):int(offset + num_frames)])\n    frames_batch = torch.cat(frames_batch, dim=0).unsqueeze(0)\n    embeddings = self.inference(frames_batch)\n    if return_mean:\n        embeddings = torch.mean(embeddings, dim=1)\n    if l2_norm:\n        embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)\n    return embeddings\n\n", "CodeBleu_score": 50.58849089721068}]}
{"instance_id": "06f5b0335b26", "generation_result": [{"code": "def create_layer(self, block, planes, blocks, stride=1)\n\t\n    downsample = None\n    if stride != 1 or self.inplanes != planes * block.expansion:\n        downsample = nn.Sequential(\n            nn.Conv2d(self.inplanes, planes * block.expansion,\n                      kernel_size=1, stride=stride, bias=False),\n            nn.BatchNorm2d(planes * block.expansion),\n        )\n\n    layers = []\n    layers.append(block(self.inplanes, planes, stride, downsample))\n    self.inplanes = planes * block.expansion\n    for i in range(1, blocks):\n        layers.append(block(self.inplanes, planes))\n\n    return nn.Sequential(*layers)\n\n", "CodeBleu_score": 85.18641475692186}, {"code": "def create_layer(self, block, planes, blocks, stride=1)\n\t\n    downsample = None\n    if stride != 1 or self.inplanes != planes * block.expansion:\n        downsample = nn.Sequential(\n            nn.Conv2d(self.inplanes, planes * block.expansion,\n                      kernel_size=1, stride=stride, bias=False),\n            nn.BatchNorm2d(planes * block.expansion),\n        )\n\n    layers = []\n    layers.append(block(self.inplanes, planes, stride, downsample))\n    self.inplanes = planes * block.expansion\n    for i in range(1, blocks):\n        layers.append(block(self.inplanes, planes))\n\n    return nn.Sequential(*layers)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 85.18641475692186}, {"code": "def create_layer(self, block, planes, blocks, stride=1)\n\t\n    downsample = None\n    if stride != 1 or self.inplanes != planes * block.expansion:\n        downsample = nn.Sequential(\n            nn.Conv2d(self.inplanes, planes * block.expansion,\n                      kernel_size=1, stride=stride, bias=False),\n            nn.BatchNorm2d(planes * block.expansion),\n        )\n\n    layers = []\n    layers.append(block(self.inplanes, planes, stride, downsample))\n    self.inplanes = planes * block.expansion\n    for i in range(1, blocks):\n        layers.append(block(self.inplanes, planes))\n\n    return nn.Sequential(*layers)\n\n", "CodeBleu_score": 85.18641475692186}]}
{"instance_id": "9ac60597b633", "generation_result": [{"code": "def plot_embeddings(embeddings, num_classes_in_batch)\n\t\n    # Calculate the number of utterances per class\n    num_utterances_per_class = np.zeros(num_classes_in_batch)\n    for i in range(num_classes_in_batch):\n        num_utterances_per_class[i] = embeddings[i].shape[0]\n    # Limit the number of classes to 10 if necessary\n    if num_classes_in_batch > 10:\n        num_classes_to_plot = 10\n        num_utterances_per_class = num_utterances_per_class[:10]\n    else:\n        num_classes_to_plot = num_classes_in_batch\n    # Create a UMAP model\n    model = umap.UMAP(random_state=42)\n    # Transform the embeddings\n    embeddings_transformed = model.fit_transform(embeddings)\n    # Create a figure\n    fig, ax = plt.subplots(figsize=(10, 10))\n    # Set the aspect ratio\n    set_aspect(ax, 1)\n    # Create a scatter plot\n    ax.scatter(embeddings_transformed[:, 0], embeddings_transformed[:, 1], c=np.repeat(np.arange(num_classes_to_plot), num_utterances_per_class), cmap='tab10', s=0.5)\n    # Set the title\n    plt.title('UMAP projection')\n    # Save the figure\n    plt.savefig('umap.png')\n    # Return the figure\n    return fig\n\n", "CodeBleu_score": 43.072255273446636}, {"code": "def plot_embeddings(embeddings, num_classes_in_batch)\n\t\n    if num_classes_in_batch > 10:\n        num_classes_in_batch = 10\n    num_utterances_per_class = int(len(embeddings) / num_classes_in_batch)\n    embeddings_reduced = umap.UMAP().fit_transform(embeddings)\n    fig, ax = plt.subplots(figsize=(12, 12))\n    plt.title(\"UMAP projection\")\n    plt.tight_layout()\n    set_aspect(ax, 1)\n    colors = np.repeat(np.arange(num_classes_in_batch), num_utterances_per_class)\n    ax.scatter(embeddings_reduced[:, 0], embeddings_reduced[:, 1], c=colors, cmap=\"tab10\")\n    plt.gca().set_aspect(\"equal\", adjustable=\"box\")\n    plt.savefig(\"umap.png\")\n    return fig\n\n", "CodeBleu_score": 47.17852602787932}, {"code": "def plot_embeddings(embeddings, num_classes_in_batch)\n\t\n    # Calculate the number of utterances per class\n    num_utterances_per_class = np.array([len(embeddings[i]) for i in range(num_classes_in_batch)])\n    # Limit the number of classes to 10 if necessary\n    if num_utterances_per_class.max() > 10:\n        num_utterances_per_class = num_utterances_per_class.argsort()[::-1]\n        num_utterances_per_class = num_utterances_per_class[:10]\n    # Create the UMAP transformer\n    reducer = umap.UMAP(n_neighbors=10, min_dist=0.1, metric='cosine', random_state=42)\n    # Transform the embeddings\n    transformed_embeddings = reducer.fit_transform(embeddings)\n    # Create a scatter plot\n    fig, ax = plt.subplots(figsize=(10, 10))\n    ax.set_title('UMAP projection')\n    ax.set_aspect('equal')\n    # Create a color palette for the classes\n    color_palette = sns.color_palette('hls', num_classes_in_batch)\n    # Scatter plot the embeddings\n    for i in range(num_classes_in_batch):\n        ax.scatter(transformed_embeddings[i][:, 0], transformed_embeddings[i][:, 1], color=color_palette[i], label=i)\n    # Set the axis limits\n    ax.set_xlim([-1.1, 1.1])\n    ax.set_ylim([-1.1, 1.1])\n    # Set the labels\n    ax.set_xticks(np.arange(-1, 1.1, 0.2))\n    ax.set_yticks(np.arange(-1, 1.1, 0.2))\n    ax.set_xticklabels(np.arange(-1, 1.1, 0.2))\n    ax.set_yticklabels(", "CodeBleu_score": 41.258538337562214}]}
{"instance_id": "19c2b4af256d", "generation_result": [{"code": "def calc_cosine_sim(self, dvecs, centroids)\n\t\n    cs_row = []\n    for i in range(len(centroids)):\n        dvecs_ex = dvecs[i][1:]\n        centroids_ex = centroids[i][1:]\n        cs_row.append(torch.mm(dvecs_ex, centroids_ex.transpose(0, 1)))\n    cs_row = torch.stack(cs_row, 0)\n    cs_row = cs_row.transpose(0, 1)\n    cs_row = cs_row.clamp(min=1e-8)\n    cs_row = torch.div(cs_row, torch.norm(cs_row, dim=1).unsqueeze(1))\n    cs_row = torch.mm(cs_row, cs_row.transpose(0, 1))\n    cs_row = cs_row.clamp(min=1e-8)\n    cs_row = torch.div(cs_row, torch.norm(cs_row, dim=1).unsqueeze(1))\n    return cs_row\n\n", "CodeBleu_score": 33.77420856622512}, {"code": "def calc_cosine_sim(self, dvecs, centroids)\n\t\n    cs_row = []\n    for i in range(dvecs.shape[0]):\n        # print(\"i\", i)\n        dvec = dvecs[i].unsqueeze(0)\n        # print(\"dvec\", dvec)\n        # print(\"centroids\", centroids)\n        # print(\"centroids.shape\", centroids.shape)\n        # print(\"dvec.shape\", dvec.shape)\n        # print(\"dvecs.shape\", dvecs.shape)\n        # print(\"dvecs[i].shape\", dvecs[i].shape)\n        # print(\"dvecs[i].shape\", dvecs[i].shape)\n        # print(\"dvecs[i].shape\", dvecs[i].shape)\n        # print(\"dvecs[i].shape\", dvecs[i].shape)\n        # print(\"dvecs[i].shape\", dvecs[i].shape)\n        # print(\"dvecs[i].shape\", dvecs[i].shape)\n        # print(\"dvecs[i].shape\", dvecs[i].shape)\n        # print(\"dvecs[i].shape\", dvecs[i].shape)\n        # print(\"dvecs[i].shape\", dvecs[i].shape)\n        # print(\"dvecs[i].shape\", dvecs[i].shape)\n        # print(\"dvecs[i].shape\", dvecs[i].shape)\n        # print(\"dvecs[i].shape\", dvecs[i].shape)\n        # print(\"dvecs[i].shape\", dvecs[i].shape)\n        # print(\"dvecs[i].shape\", dvecs[i].shape)\n        # print(\"dvecs[i].shape\", dvecs[i].shape)\n        # print(\"dvecs[i].shape\", dvecs[i].shape)\n        # print(\"dvecs[i].shape\", dvecs[i].shape)\n        # print(\"dvecs[i].shape\", dvecs[i].shape)\n        # print(\"dvecs[i].", "CodeBleu_score": 8.712676583906262}, {"code": "def calc_cosine_sim(self, dvecs, centroids)\n\t\n    cos_sim_matrix = []\n    for speaker_id in range(self.num_speakers):\n        cs_row = []\n        for utt_id in range(self.num_utts):\n            if utt_id == speaker_id:\n                continue\n            dvec = dvecs[speaker_id][utt_id]\n            new_centroids = self.calc_new_centroids(dvecs, centroids, speaker_id, utt_id)\n            cs = torch.mm(dvec.unsqueeze(0), new_centroids.transpose(0, 1))\n            cs = torch.clamp(cs, min=self.eps)\n            cs_row.append(cs)\n        cs_row = torch.stack(cs_row)\n        cos_sim_matrix.append(cs_row)\n    cos_sim_matrix = torch.cat(cos_sim_matrix, dim=0)\n    return cos_sim_matrix\n", "CodeBleu_score": 47.72444936645434}]}
{"instance_id": "c525b411a1e7", "generation_result": [{"code": " def embed_loss_softmax(self, dvecs, cos_sim_matrix)\n\t\n    cos_sim_matrix = cos_sim_matrix.float()\n    dvecs = dvecs.float()\n    # Iterate over the embeddings\n    losses = []\n    for i in range(dvecs.shape[0]):\n        # Compute the negative log softmax of the cosine similarity matrix for the current embedding\n        loss = -F.log_softmax(cos_sim_matrix[i], dim=0)\n        # Add the losses to the list\n        losses.append(loss)\n    # Stack the losses into a tensor\n    losses = torch.stack(losses, dim=0)\n    # Return the stacked tensor\n    return losses\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 26.22866877430805}, {"code": " def embed_loss_softmax(self, dvecs, cos_sim_matrix)\n\t\n    #print(\"embed_loss_softmax: \", dvecs.size(), cos_sim_matrix.size())\n    #print(\"embed_loss_softmax: \", dvecs.size(), cos_sim_matrix.size())\n    #print(\"embed_loss_softmax: \", dvecs.size(), cos_sim_matrix.size())\n    #print(\"embed_loss_softmax: \", dvecs.size(), cos_sim_matrix.size())\n    #print(\"embed_loss_softmax: \", dvecs.size(), cos_sim_matrix.size())\n    #print(\"embed_loss_softmax: \", dvecs.size(), cos_sim_matrix.size())\n    #print(\"embed_loss_softmax: \", dvecs.size(), cos_sim_matrix.size())\n    #print(\"embed_loss_softmax: \", dvecs.size(), cos_sim_matrix.size())\n    #print(\"embed_loss_softmax: \", dvecs.size(), cos_sim_matrix.size())\n    #print(\"embed_loss_softmax: \", dvecs.size(), cos_sim_matrix.size())\n    #print(\"embed_loss_softmax: \", dvecs.size(), cos_sim_matrix.size())\n    #print(\"embed_loss_softmax: \", dvecs.size(), cos_sim_matrix.size())\n    #print(\"embed_loss_softmax: \", dvecs.size(), cos_sim_matrix.size())\n    #print(\"embed_loss_softmax: \", dvecs.size(), cos_sim_matrix.size())\n    #print(\"embed_loss_softmax: \", dvecs.size(), cos_sim_matrix.size())\n    #print(\"embed_loss_softmax: \", dvecs.size(), cos_sim_matrix.size())\n    #print(\"embed_loss_softmax: \", dvecs.size(), cos_sim_matrix.size())\n    #print(\"embed_loss_softmax: \", dvecs.size(), cos_sim_matrix.size())\n    #print(\"embed_loss_softmax: \", dvecs.size(), cos_sim_matrix.size", "CodeBleu_score": 2.26010101010101}, {"code": " def embed_loss_softmax(self, dvecs, cos_sim_matrix)\n\t\n    losses = []\n    for i in range(dvecs.shape[0]):\n        loss = -F.log_softmax(cos_sim_matrix[i], dim=0)\n        losses.append(loss)\n    losses = torch.stack(losses)\n    return losses\n\n def embed_loss_softmax_batch(self, dvecs, cos_sim_matrix):\n    \"\"\"The function calculates the embedding loss using softmax.", "CodeBleu_score": 32.60548200958085}]}
{"instance_id": "25ac3945da2e", "generation_result": [{"code": "def embed_loss_contrast(self, dvecs, cos_sim_matrix)\n\t\n    L = []\n    for i in range(dvecs.size(0)):\n        L_row = []\n        for j in range(dvecs.size(0)):\n            if i == j:\n                L_row.append(0)\n            else:\n                L_row.append(self.contrast_loss(dvecs[i], dvecs[j], cos_sim_matrix[i, j]))\n        L.append(torch.stack(L_row))\n    return torch.stack(L)\n\n", "CodeBleu_score": 34.0748155466718}, {"code": "def embed_loss_contrast(self, dvecs, cos_sim_matrix)\n\t\n    losses = []\n    for i in range(dvecs.shape[0]):\n        row_losses = []\n        for j in range(dvecs.shape[1]):\n            if j == i:\n                continue\n            else:\n                row_losses.append(torch.sigmoid(cos_sim_matrix[i, j]))\n        losses.append(torch.stack(row_losses))\n    return torch.stack(losses)\n\n", "CodeBleu_score": 33.878024937989146}, {"code": "def embed_loss_contrast(self, dvecs, cos_sim_matrix)\n\t\n    losses = []\n    for i in range(dvecs.shape[0]):\n        row = []\n        for j in range(dvecs.shape[0]):\n            if i != j:\n                row.append(torch.sigmoid(cos_sim_matrix[i, j]))\n        losses.append(torch.stack(row))\n    return torch.stack(losses)\n", "CodeBleu_score": 29.56901224600483}]}
{"instance_id": "878d14ffabd2", "generation_result": [{"code": "def forward(self, x, _label=None)\n\t\n    if x.size(1) < 2:\n        raise ValueError(\"The number of utterances per speaker should be at least two.\")\n    embeddings = self.embedding(x)\n    embeddings = embeddings.transpose(1, 2)\n    embeddings = embeddings.contiguous()\n    centroids = self.calc_centroids(embeddings)\n    sim_matrix = self.calc_cosine_sim(embeddings, centroids)\n    sim_matrix = self.scale_sim_matrix(sim_matrix)\n    sim_matrix = self.shift_sim_matrix(sim_matrix)\n    sim_matrix = self.calc_loss(sim_matrix)\n    sim_matrix = self.embed_loss(sim_matrix)\n    sim_matrix = sim_matrix.mean()\n    return sim_matrix\n\n", "CodeBleu_score": 37.62154564080422}, {"code": "def forward(self, x, _label=None)\n\t\n    # x: (num_speakers, num_utts_per_speaker, dvec_feats)\n    # _label: (num_speakers,)\n    if _label is not None:\n        assert x.size(0) == _label.size(0)\n    if x.size(1) < 2:\n        raise ValueError('GE2E requires at least two utterances per speaker.')\n    # x: (num_speakers, num_utts_per_speaker, dvec_feats)\n    # _label: (num_speakers,)\n    x = self.embed_layer(x)\n    # x: (num_speakers, num_utts_per_speaker, dvec_feats)\n    # _label: (num_speakers,)\n    x = self.embed_dropout(x)\n    # x: (num_speakers, num_utts_per_speaker, dvec_feats)\n    # _label: (num_speakers,)\n    x = self.embed_norm(x)\n    # x: (num_speakers, num_utts_per_speaker, dvec_feats)\n    # _label: (num_speakers,)\n    x = x.view(x.size(0), -1)\n    # x: (num_speakers, num_utts_per_speaker * dvec_feats)\n    # _label: (num_speakers,)\n    x = self.calc_centroids(x)\n    # x: (num_speakers, dvec_feats)\n    # _label: (num_speakers,)\n    x = self.calc_cosine_sim(x)\n    # x: (num_speakers, num_speakers)\n    # _label: (num_speakers,)\n    x = self.scale_shift(x)\n    # x: (num_speakers, num_speakers)\n    # _label: (num_speakers,)\n    x = self.embed_loss(x)\n    # x: (num_speakers,)\n    # _label: (", "CodeBleu_score": 30.077876232425893}, {"code": "def forward(self, x, _label=None)\n\t\n    x = x.view(x.size(0), -1, x.size(-1))\n    x_mean = torch.mean(x, dim=1)\n    x_centroid = torch.mean(x_mean, dim=0)\n    x_centroid = x_centroid.unsqueeze(0)\n    x_centroid = x_centroid.expand_as(x)\n    x_sim = self.calc_cosine_sim(x, x_centroid)\n    x_sim = torch.clamp(x_sim, min=self.min_val, max=self.max_val)\n    x_sim = self.scale_sim(x_sim)\n    x_loss = self.embed_loss(x_sim)\n    x_loss = torch.mean(x_loss)\n    return x_loss\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 35.52361032129866}]}
{"instance_id": "86a364c72caa", "generation_result": [{"code": "def get_voices(extra_voice_dirs: List[str] = [])\n\t\n    # List of all directories to search for voices\n    dirs = [\n        'voices',\n        'voices_en',\n        'voices_en_us',\n        'voices_en_us_slt',\n        'voices_en_us_awb',\n        'voices_en_us_ksp',\n        'voices_en_us_rms',\n        'voices_en_us_jmk',\n        'voices_en_us_slt_awb',\n        'voices_en_us_slt_ksp',\n        'voices_en_us_slt_rms',\n        'voices_en_us_slt_jmk',\n        'voices_en_us_awb_ksp',\n        'voices_en_us_awb_rms',\n        'voices_en_us_awb_jmk',\n        'voices_en_us_ksp_rms',\n        'voices_en_us_ksp_jmk',\n        'voices_en_us_rms_jmk',\n        'voices_en_us_slt_awb_ksp',\n        'voices_en_us_slt_awb_rms',\n        'voices_en_us_slt_awb_jmk',\n        'voices_en_us_slt_ksp_rms',\n        'voices_en_us_slt_ksp_jmk',\n        'voices_en_us_slt_rms_jmk',\n        'voices_en_us_awb_ksp_rms',\n        'voices_en_us_awb_ksp_jmk',\n        'voices_en_us_awb_rms_jmk',\n        'voices_en_us_ksp_rms_jmk',\n        'voices_en_us_slt_awb_ksp_rms',\n        'voices_en_us_slt_awb_ksp_jmk',\n        'voices_en_us_slt", "CodeBleu_score": 9.284409334498358}, {"code": "def get_voices(extra_voice_dirs: List[str] = [])\n\t\n    # TODO: Add support for .mp3 files\n    # TODO: Add support for .wav files\n    # TODO: Add support for .flac files\n    # TODO: Add support for .ogg files\n    # TODO: Add support for .m4a files\n    # TODO: Add support for .opus files\n    # TODO: Add support for .aac files\n    # TODO: Add support for .wma files\n    # TODO: Add support for .mka files\n    # TODO: Add support for .mkv files\n    # TODO: Add support for .mp4 files\n    # TODO: Add support for .mov files\n    # TODO: Add support for .avi files\n    # TODO: Add support for .flv files\n    # TODO: Add support for .swf files\n    # TODO: Add support for .3gp files\n    # TODO: Add support for .webm files\n    # TODO: Add support for .wmv files\n    # TODO: Add support for .vob files\n    # TODO: Add support for .m2v files\n    # TODO: Add support for .mp2 files\n    # TODO: Add support for .mp1 files\n    # TODO: Add support for .m1v files\n    # TODO: Add support for .m2a files\n    # TODO: Add support for .m4v files\n    # TODO: Add support for .mpg files\n    # TODO: Add support for .mpeg files\n    # TODO: Add support for .mpv files\n    # TODO: Add support for .qt files\n    # TODO: Add support for .rm files\n    # TODO: Add support for .rmvb files\n    # TODO: Add support for .srt files\n    # TODO: Add support for .vob files\n    # TODO: Add support for .yuv files\n    # TODO: Add support for .yuv files\n    # TODO: Add support for .yuv files\n    # TODO: Add support for .yuv files\n    # TODO: Add support for .yuv files\n    # TODO: Add support for .yuv files\n    # TODO: Add support for .yuv files\n    # TODO: Add support for .yuv files\n    # TODO: Add support for .yuv files\n    # TODO: Add support for", "CodeBleu_score": 3.9473684210526314}, {"code": "def get_voices(extra_voice_dirs: List[str] = [])\n\t\n    voice_dirs = [\n        \"data/voices/\",\n        \"data/voices/female\",\n        \"data/voices/male\",\n        \"data/voices/female/extra\",\n        \"data/voices/male/extra\",\n    ]\n    voice_dirs = [os.path.join(voice_dir, subdir) for voice_dir in voice_dirs for subdir in os.listdir(voice_dir)]\n    voice_dirs = [os.path.join(voice_dir, subdir) for voice_dir in voice_dirs for subdir in os.listdir(voice_dir)]\n    voice_dirs = [os.path.join(voice_dir, subdir) for voice_dir in voice_dirs for subdir in os.listdir(voice_dir)]\n    voice_dirs = [os.path.join(voice_dir, subdir) for voice_dir in voice_dirs for subdir in os.listdir(voice_dir)]\n    voice_dirs = [os.path.join(voice_dir, subdir) for voice_dir in voice_dirs for subdir in os.listdir(voice_dir)]\n    voice_dirs = [os.path.join(voice_dir, subdir) for voice_dir in voice_dirs for subdir in os.listdir(voice_dir)]\n    voice_dirs = [os.path.join(voice_dir, subdir) for voice_dir in voice_dirs for subdir in os.listdir(voice_dir)]\n    voice_dirs = [os.path.join(voice_dir, subdir) for voice_dir in voice_dirs for subdir in os.listdir(voice_dir)]\n    voice_dirs = [os.path.join(voice_dir, subdir) for voice_dir in voice_dirs for subdir in os.listdir(voice_dir)]\n    voice_dirs = [os.path.join(voice_dir, subdir) for voice_dir in voice_dirs for subdir in os.listdir(voice_dir)]\n    voice_dirs = [os.path.join(", "CodeBleu_score": 16.030562323451022}]}
{"instance_id": "02f20c8917e6", "generation_result": [{"code": "def load_voice(model, voice: str, extra_voice_dirs: List[str] = [])\n\t\n    if voice == \"random\":\n        return None, None, None, None\n    voice_paths = get_voices(model, voice, extra_voice_dirs)\n    if len(voice_paths) != 1:\n        raise ValueError(f\"Expected 1 voice path for {voice}, got {len(voice_paths)}\")\n    voice_path = voice_paths[0]\n    if voice_path.endswith(\".npz\"):\n        return load_npz(voice_path)\n    else:\n        return generate_voice(voice_path, model)\n\n", "CodeBleu_score": 33.020089167249225}, {"code": "def load_voice(model, voice: str, extra_voice_dirs: List[str] = [])\n\t\n    if voice == \"random\":\n        return None, None, None, None\n    voice_paths = get_voices(model, voice, extra_voice_dirs)\n    if len(voice_paths) > 1:\n        raise ValueError(f\"Multiple voices found for {model}/{voice}\")\n    if len(voice_paths) == 0:\n        raise KeyError(f\"No voice found for {model}/{voice}\")\n    voice_path = voice_paths[0]\n    if voice_path.endswith(\".npz\"):\n        return load_npz(voice_path)\n    else:\n        return generate_voice(model, voice_path)\n\n", "CodeBleu_score": 38.41644096814718}, {"code": "def load_voice(model, voice: str, extra_voice_dirs: List[str] = [])\n\t\n    if voice == \"random\":\n        return None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None", "CodeBleu_score": 10.173333113998137}]}
{"instance_id": "9458e981389f", "generation_result": [{"code": "def _relative_shift(self, pos_score: torch.Tensor) -> torch.Tensor\n\t\n    pos_score_shape = pos_score.size()\n    pos_score_shape_list = list(pos_score_shape)\n    pos_score_shape_list.insert(0, 1)\n    pos_score_shape_list.insert(0, 1)\n    pos_score_shape_list.insert(0, 1)\n    pos_score_shape_list.insert(0, 1)\n    pos_score_shape_list.insert(0, 1)\n    pos_score_shape_list.insert(0, 1)\n    pos_score_shape_list.insert(0, 1)\n    pos_score_shape_list.insert(0, 1)\n    pos_score_shape_list.insert(0, 1)\n    pos_score_shape_list.insert(0, 1)\n    pos_score_shape_list.insert(0, 1)\n    pos_score_shape_list.insert(0, 1)\n    pos_score_shape_list.insert(0, 1)\n    pos_score_shape_list.insert(0, 1)\n    pos_score_shape_list.insert(0, 1)\n    pos_score_shape_list.insert(0, 1)\n    pos_score_shape_list.insert(0, 1)\n    pos_score_shape_list.insert(0, 1)\n    pos_score_shape_list.insert(0, 1)\n    pos_score_shape_list.insert(0, 1)\n    pos_score_shape_list.insert(0, 1)\n    pos_score_shape_list.insert(0, 1)\n    pos_score_shape_list.insert(0, 1)\n    pos_score_shape_list.insert(0, 1)\n    pos_score_shape_list.insert(0, 1)\n    pos_score_shape_list.insert(0, 1)\n    pos_score_shape_list.insert(0, 1)\n    pos_score_shape_list.insert(0, 1)\n    pos_", "CodeBleu_score": 13.20965963148803}, {"code": "def _relative_shift(self, pos_score: torch.Tensor) -> torch.Tensor\n\t\n    pos_score_dim = pos_score.dim()\n    pos_score_size = pos_score.size()\n    pos_score_shape = pos_score.shape\n    zero_tensor = torch.zeros(pos_score_size[:-1] + (pos_score_size[-1] + 1,), device=pos_score.device, dtype=pos_score.dtype)\n    pos_score_with_zero = torch.cat([zero_tensor, pos_score], dim=-1)\n    pos_score_with_zero_view = pos_score_with_zero.view(pos_score_shape[0], pos_score_shape[1], pos_score_shape[2] + 1)\n    pos_score_shifted = pos_score_with_zero_view[:, :, 1:]\n    return pos_score_shifted\n\n", "CodeBleu_score": 26.87393460627414}, {"code": "def _relative_shift(self, pos_score: torch.Tensor) -> torch.Tensor\n\t\n    # pos_score: [batch_size, num_heads, num_queries, num_keys]\n    batch_size, num_heads, num_queries, num_keys = pos_score.size()\n    # pos_score_pad: [batch_size, num_heads, num_queries + num_keys - 1, num_keys]\n    pos_score_pad = torch.cat(\n        [pos_score, torch.zeros(batch_size, num_heads, num_queries, 1, device=pos_score.device, dtype=pos_score.dtype)], dim=3\n    )\n    # pos_score_pad: [batch_size, num_heads, num_queries + num_keys - 1, num_keys + 1]\n    pos_score_pad = pos_score_pad.view(batch_size, num_heads, num_queries + num_keys - 1, num_keys + 1)\n    # pos_score_pad: [batch_size, num_heads, num_queries + num_keys - 1, num_keys]\n    pos_score_pad = pos_score_pad[:, :, :, :-1]\n    # pos_score_pad: [batch_size, num_heads, num_queries + num_keys - 1, num_keys - 1]\n    pos_score_pad = pos_score_pad[:, :, :, 1:]\n    # pos_score_pad: [batch_size, num_heads, num_queries, num_keys]\n    return pos_score_pad\n\n", "CodeBleu_score": 33.09546458142775}]}
{"instance_id": "c046d4f00e80", "generation_result": [{"code": "def run_padded_sequence(self, context, lens)\n\t\n    # 1. Extract the context up to the specified length\n    context = context[:, :lens.max()]\n    # 2. Apply the convolutional layers\n    context = self.conv(context)\n    context = self.relu(context)\n    context = self.dropout(context)\n    # 3. Pad the contexts\n    context = torch.nn.utils.rnn.pad_sequence(context, batch_first=True, padding_value=0)\n    return context\n\n", "CodeBleu_score": 23.6891213828296}, {"code": "def run_padded_sequence(self, context, lens)\n\t\n    context_embedded = self.context_embedding(context)\n    context_embedded = self.dropout(context_embedded)\n    context_embedded = self.context_conv(context_embedded)\n    context_embedded = self.dropout(context_embedded)\n    context_embedded = self.context_relu(context_embedded)\n    context_embedded = self.dropout(context_embedded)\n    context_embedded = self.context_conv2(context_embedded)\n    context_embedded = self.dropout(context_embedded)\n    context_embedded = self.context_relu(context_embedded)\n    context_embedded = self.dropout(context_embedded)\n    context_embedded = self.context_conv3(context_embedded)\n    context_embedded = self.dropout(context_embedded)\n    context_embedded = self.context_relu(context_embedded)\n    context_embedded = self.dropout(context_embedded)\n    context_embedded = self.context_conv4(context_embedded)\n    context_embedded = self.dropout(context_embedded)\n    context_embedded = self.context_relu(context_embedded)\n    context_embedded = self.dropout(context_embedded)\n    context_embedded = self.context_conv5(context_embedded)\n    context_embedded = self.dropout(context_embedded)\n    context_embedded = self.context_relu(context_embedded)\n    context_embedded = self.dropout(context_embedded)\n    context_embedded = self.context_conv6(context_embedded)\n    context_embedded = self.dropout(context_embedded)\n    context_embedded = self.context_relu(context_embedded)\n    context_embedded = self.dropout(context_embedded)\n    context_embedded = self.context_conv7(context_embedded)\n    context_embedded = self.dropout(context_embedded)\n    context_embedded = self.context_rel", "CodeBleu_score": 6.780805905220716}, {"code": "def run_padded_sequence(self, context, lens)\n\t\n    context_embedded = []\n    for i in range(len(context)):\n        context_embedded.append(self.embedding(context[i]))\n    context_embedded = torch.stack(context_embedded, dim=0)\n    context_embedded = context_embedded.permute(1, 0, 2)\n    context_embedded = self.dropout(context_embedded)\n    context_embedded = self.conv(context_embedded)\n    context_embedded = self.relu(context_embedded)\n    context_embedded = self.dropout(context_embedded)\n    context_embedded = self.conv(context_embedded)\n    context_embedded = self.relu(context_embedded)\n    context_embedded = self.dropout(context_embedded)\n    context_embedded = self.conv(context_embedded)\n    context_embedded = self.relu(context_embedded)\n    context_embedded = self.dropout(context_embedded)\n    context_embedded = self.conv(context_embedded)\n    context_embedded = self.relu(context_embedded)\n    context_embedded = self.dropout(context_embedded)\n    context_embedded = self.conv(context_embedded)\n    context_embedded = self.relu(context_embedded)\n    context_embedded = self.dropout(context_embedded)\n    context_embedded = self.conv(context_embedded)\n    context_embedded = self.relu(context_embedded)\n    context_embedded = self.dropout(context_embedded)\n    context_embedded = self.conv(context_embedded)\n    context_embedded = self.relu(context_embedded)\n    context_embedded = self.dropout(context_embedded)\n    context_embedded = self.conv(context_embedded)\n    context_embedded = self.relu(context_embedded)\n    context_embedded = self.dropout(context_embedded)\n    context_embedded = self.conv(context_embed", "CodeBleu_score": 19.830171061348285}]}
{"instance_id": "db2644f3c681", "generation_result": [{"code": "def run_unsorted_inputs(self, fn, context, lens)\n\t\n    lens_sorted, indices_sorted = lens.sort(0, descending=True)\n    context_packed = pack_padded_sequence(context, lens_sorted, batch_first=True)\n    context_sorted, _ = fn(context_packed)\n    context_sorted, _ = pad_packed_sequence(context_sorted, batch_first=True, total_length=lens_sorted.max())\n    context_sorted = context_sorted[indices_sorted]\n    return context_sorted\n\n", "CodeBleu_score": 19.477226703189643}, {"code": "def run_unsorted_inputs(self, fn, context, lens)\n\t\n    lens_sorted, indices_sorted = torch.sort(lens, descending=True)\n    context_sorted = pack_padded_sequence(context, lens_sorted, batch_first=True)\n    context_sorted = fn(context_sorted)\n    context_sorted, _ = pad_packed_sequence(context_sorted, batch_first=True)\n    context_sorted = context_sorted[indices_sorted]\n    return context_sorted\n\n", "CodeBleu_score": 14.787346918470485}, {"code": "def run_unsorted_inputs(self, fn, context, lens)\n\t\n    if not isinstance(lens, torch.Tensor):\n        lens = torch.LongTensor(lens)\n    lens, perm_idx = lens.sort(0, descending=True)\n    ctx = pack_padded_sequence(context, lens, batch_first=True)\n    ctx = fn(ctx)\n    ctx, _ = pad_packed_sequence(ctx, batch_first=True)\n    ctx = ctx[perm_idx]\n    return ctx\n", "CodeBleu_score": 23.617420471604934}]}
{"instance_id": "ced082bffc63", "generation_result": [{"code": "def remove_weight_norm(self)\n\t\n    self.kernel_predictor.remove_weight_norm()\n    self.convt_pre.remove_parametrizations()\n    for i in range(len(self.conv_blocks)):\n        self.conv_blocks[i].remove_weight_norm()\n\n", "CodeBleu_score": 34.143166291613284}, {"code": "def remove_weight_norm(self)\n\t\n    self.kernel_predictor.remove_weight_norm()\n    self.convt_pre.remove_parametrizations()\n    for i in range(len(self.conv_blocks)):\n        self.conv_blocks[i].remove_parametrizations()\n\n", "CodeBleu_score": 35.4986463241055}, {"code": "def remove_weight_norm(self)\n\t\n    self.kernel_predictor.remove_weight_norm()\n    self.convt_pre.remove_parametrizations()\n    for block in self.conv_blocks:\n        block.remove_parametrizations()\n\n", "CodeBleu_score": 36.39504072468004}]}
{"instance_id": "b5a39fd97061", "generation_result": [{"code": "def map_mx_to_torch_model(mx_name)\n\t\n    mx_name = mx_name.lower()\n    torch_name = mx_name\n    torch_name = torch_name.replace('se_', 'se_resnext')\n    torch_name = torch_name.replace('senet_', 'senet')\n    torch_name = torch_name.replace('inceptionv3', 'inception_v3')\n    torch_name = torch_name.replace('_v1', '')\n    torch_name = torch_name.replace('_v2', '')\n    torch_name = torch_name.replace('_v3', '')\n    torch_name = torch_name.replace('_v4', '')\n    torch_name = torch_name.replace('_v5', '')\n    torch_name = torch_name.replace('_v6', '')\n    torch_name = torch_name.replace('_v7', '')\n    torch_name = torch_name.replace('_v8', '')\n    torch_name = torch_name.replace('_v9', '')\n    torch_name = torch_name.replace('_v10', '')\n    torch_name = torch_name.replace('_v11', '')\n    torch_name = torch_name.replace('_v12', '')\n    torch_name = torch_name.replace('_v13', '')\n    torch_name = torch_name.replace('_v14', '')\n    torch_name = torch_name.replace('_v15', '')\n    torch_name = torch_name.replace('_v16', '')\n    torch_name = torch_name.replace('_v17', '')\n    torch_name = torch_name.replace('_v18', '')\n    torch_name = torch_name.replace('_v19', '')\n    torch_name = torch_name.replace('_v20', '')\n    torch_name = torch_name.replace('_v21', '')\n    torch_", "CodeBleu_score": 42.07850576241049}, {"code": "def map_mx_to_torch_model(mx_name)\n\t\n    torch_name = mx_name.lower()\n    torch_name = torch_name.replace('se_', 'se')\n    torch_name = torch_name.replace('senet_', 'senet')\n    torch_name = torch_name.replace('inceptionv3', 'inception_v3')\n    return 'gluon_' + torch_name\n\n", "CodeBleu_score": 40.26548954275221}, {"code": "def map_mx_to_torch_model(mx_name)\n\t\n    torch_name = mx_name.lower()\n    torch_name = torch_name.replace('se_', 'se_resnet')\n    torch_name = torch_name.replace('senet_', 'se_resnet')\n    torch_name = torch_name.replace('inceptionv3', 'inception_v3')\n    torch_name = 'gluon_' + torch_name\n    return torch_name\n\n", "CodeBleu_score": 49.95039892311691}]}
{"instance_id": "7eee5efdbf67", "generation_result": [{"code": "def _num_samples_per_worker(self)\n\t\n    num_samples = self.num_samples\n    num_workers = self.num_workers\n    batch_size = self.batch_size\n    if batch_size is None:\n        if num_workers is None:\n            return num_samples\n        else:\n            return math.ceil(num_samples / num_workers)\n    else:\n        if num_workers is None:\n            return num_samples\n        else:\n            return math.ceil(num_samples / num_workers) * batch_size\n\n", "CodeBleu_score": 26.83600499098539}, {"code": "def _num_samples_per_worker(self)\n\t\n    if self.repeats is None:\n        return None\n    if self.total_samples is None:\n        return None\n    if self.num_workers is None:\n        return None\n    num_samples_per_worker = self.total_samples // self.num_workers\n    if self.repeats > 1:\n        num_samples_per_worker = math.ceil(num_samples_per_worker)\n    if self.batch_size is not None:\n        num_samples_per_worker = num_samples_per_worker // self.batch_size * self.batch_size\n    return num_samples_per_worker\n\n", "CodeBleu_score": 37.75020637363188}, {"code": "def _num_samples_per_worker(self)\n\t\n    if self.repeat_count is not None:\n        num_samples = self.repeat_count\n    else:\n        num_samples = self.num_samples\n    if self.batch_size is not None:\n        num_samples = int(math.ceil(num_samples / self.batch_size)) * self.batch_size\n    if self.num_replicas is not None:\n        num_samples = int(math.ceil(num_samples / self.num_replicas)) * self.num_replicas\n    return num_samples\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 39.66800302032897}]}
{"instance_id": "a698d65ae190", "generation_result": [{"code": "def find_images_and_targets(\n        folder: str,\n        types: Optional[Union[List, Tuple, Set]] = None,\n        class_to_idx: Optional[Dict] = None,\n        leaf_name_only: bool = True,\n        sort: bool = True\n)\n\t\n    if types is None:\n        types = ('.jpg', '.jpeg', '.png', '.tif', '.tiff', '.bmp')\n    if class_to_idx is None:\n        class_to_idx = {}\n    if isinstance(types, str):\n        types = [types]\n    if isinstance(types, list):\n        types = set(types)\n    if isinstance(types, tuple):\n        types = set(types)\n    if isinstance(types, set):\n        pass\n    else:\n        raise ValueError(f'types must be either a string, a list, a tuple or a set, not {type(types)}.')\n    filenames = []\n    labels = []\n    if leaf_name_only:\n        for root, dirs, files in os.walk(folder):\n            for file in files:\n                ext = splitext(file)[1]\n                if ext.lower() in types:\n                    filenames.append(relpath(join(root, file), folder))\n                    labels.append(root.split(sep)[-1])\n    else:\n        for root, dirs, files in os.walk(folder):\n            for file in files:\n                ext = splitext(file)[1]\n                if ext.lower() in types:\n                    filenames.append(relpath(join(root, file), folder))\n                    labels.append(relpath(join(root, file), folder).split(sep)[-1])\n    if sort:\n        filenames = sorted(filenames)\n        labels = sorted(labels)\n    if class_to_idx is None:\n        class_to_idx = {label: idx for idx, label in enumerate(sorted(set(labels)))}\n    else:\n        for label in labels:\n            if label not in class_to_idx:\n                class_to_idx[label] = len(class_to_idx)\n    return filenames, labels, class_to_idx\n\n", "CodeBleu_score": 34.576740198966604}, {"code": "def find_images_and_targets(\n        folder: str,\n        types: Optional[Union[List, Tuple, Set]] = None,\n        class_to_idx: Optional[Dict] = None,\n        leaf_name_only: bool = True,\n        sort: bool = True\n)\n\t\n    if not os.path.isdir(folder):\n        raise NotADirectoryError(f'The folder {folder} does not exist.')\n    if not types:\n        types = ['.jpg', '.jpeg', '.png', '.tif', '.tiff']\n    if not class_to_idx:\n        class_to_idx = {}\n    filenames = []\n    labels = []\n    for root, dirs, files in os.walk(folder):\n        if leaf_name_only:\n            rel_path = os.path.basename(root)\n        else:\n            rel_path = os.path.relpath(root, folder)\n        if rel_path.startswith('.'):\n            continue\n        for f in files:\n            if splitext(f)[1].lower() in types:\n                filenames.append(os.path.join(rel_path, f))\n                labels.append(rel_path)\n    if sort:\n        filenames, labels = zip(*sorted(zip(filenames, labels), key=natural_key))\n    for label in set(labels):\n        if label not in class_to_idx:\n            class_to_idx[label] = len(class_to_idx)\n    return filenames, labels, class_to_idx\n\n", "CodeBleu_score": 41.655404185694586}, {"code": "def find_images_and_targets(\n        folder: str,\n        types: Optional[Union[List, Tuple, Set]] = None,\n        class_to_idx: Optional[Dict] = None,\n        leaf_name_only: bool = True,\n        sort: bool = True\n)\n\t\n    filenames = []\n    labels = []\n    if types is None:\n        types = get_img_extensions()\n    if class_to_idx is None:\n        class_to_idx = {}\n    for root, dirs, files in os.walk(folder):\n        if leaf_name_only:\n            rel_path = os.path.basename(root)\n        else:\n            rel_path = os.path.relpath(root, folder)\n        for f in files:\n            ext = splitext(f)[1].lower()\n            if ext.lower() in types:\n                filenames.append(os.path.join(rel_path, f))\n                labels.append(rel_path)\n    if sort:\n        filenames, labels = zip(*sorted(zip(filenames, labels), key=natural_key))\n    labels = list(set(labels))\n    if len(labels) > 0:\n        if class_to_idx is None:\n            for i, label in enumerate(labels):\n                class_to_idx[label] = i\n    return filenames, labels, class_to_idx\n\n", "CodeBleu_score": 39.86912791686244}]}
{"instance_id": "e7fe43bdecdc", "generation_result": [{"code": "def pytorch_worker_seed()\n\t\n    worker_info = get_worker_info()\n    if worker_info is not None:\n        worker_seed = worker_info.seed\n    else:\n        worker_seed = int.from_bytes(os.urandom(4), byteorder=\"little\")\n    return worker_seed\n\n", "CodeBleu_score": 48.442329751514166}, {"code": "def pytorch_worker_seed()\n\t\n    worker_info = get_worker_info()\n    if worker_info is not None:\n        worker_seed = worker_info.seed\n    else:\n        worker_seed = wds.rank()\n    return worker_seed\n\n", "CodeBleu_score": 54.424195406576416}, {"code": "def pytorch_worker_seed()\n\t\n    worker_info = get_worker_info()\n    if worker_info is not None:\n        worker_seed = worker_info.seed\n    else:\n        worker_seed = int(os.getenv(\"PYTORCH_WORKER_SEED\", \"0\"))\n    return worker_seed\n\n", "CodeBleu_score": 48.73674428189846}]}
{"instance_id": "0ffc7e7ded25", "generation_result": [{"code": "def solarize_add(img, add, thresh=128, **__)\n\t\n    if img.mode == \"L\":\n        lut = []\n        for i in range(256):\n            if i < thresh:\n                lut.append(min(255, i + add))\n            else:\n                lut.append(i)\n        return img.point(lut)\n    else:\n        return img\n\n", "CodeBleu_score": 59.27860748247185}, {"code": "def solarize_add(img, add, thresh=128, **__)\n\t\n    if img.mode == \"L\":\n        lut = []\n        for i in range(256):\n            if i < thresh:\n                lut.append(min(255, i + add))\n            else:\n                lut.append(i)\n        return img.point(lut)\n    else:\n        return img\n\n", "CodeBleu_score": 59.27860748247185}, {"code": "def solarize_add(img, add, thresh=128, **__)\n\t\n    return _solarize_add(img, add, thresh)\n", "CodeBleu_score": 4.499840316075399}]}
{"instance_id": "5a0d29577a21", "generation_result": [{"code": "def _get_weighted_transforms(transforms: Dict)\n\t\n    transform_names = []\n    transform_probs = []\n    for transform, prob in transforms.items():\n        transform_names.append(transform)\n        transform_probs.append(prob)\n    transform_probs = np.array(transform_probs)\n    transform_probs = transform_probs / np.sum(transform_probs)\n    return transform_names, transform_probs\n\n", "CodeBleu_score": 37.21265282786524}, {"code": "def _get_weighted_transforms(transforms: Dict)\n\t\n    transform_names = []\n    probabilities = []\n    for transform, prob in transforms.items():\n        transform_names.append(transform)\n        probabilities.append(prob)\n    transform_names = tuple(transform_names)\n    probabilities = tuple(probabilities)\n    probabilities = np.array(probabilities)\n    probabilities = probabilities / np.sum(probabilities)\n    return transform_names, probabilities\n\n", "CodeBleu_score": 34.781202403856696}, {"code": "def _get_weighted_transforms(transforms: Dict)\n\t\n    transforms_names = list(transforms.keys())\n    transforms_probs = list(transforms.values())\n    transforms_probs = np.array(transforms_probs) / np.sum(transforms_probs)\n    return transforms_names, transforms_probs\n\n", "CodeBleu_score": 21.139889103628697}]}
{"instance_id": "84bc5658c135", "generation_result": [{"code": "def _apply_basic(self, img, mixing_weights, m)\n\t\n    mixed = np.zeros(img.shape, dtype=img.dtype)\n    for i in range(m):\n        op = choice(self.operations)\n        mixed = mixed + op(img, self.p) * mixing_weights[i]\n    mixed = np.clip(mixed, 0, 255)\n    mixed = mixed.astype(img.dtype)\n    mixed = Image.blend(img, Image.fromarray(mixed), self.blending_factor)\n    return mixed\n\n", "CodeBleu_score": 25.05786762274432}, {"code": "def _apply_basic(self, img, mixing_weights, m)\n\t\n    mixed = np.zeros(img.shape, dtype=img.dtype)\n    for i in range(len(mixing_weights)):\n        op = choice(m)\n        mixed += op(img, randint(0, 100)) * mixing_weights[i]\n    mixed = np.clip(mixed, 0, 255)\n    mixed = mixed.astype(img.dtype)\n    return Image.blend(img, Image.fromarray(mixed), 0.5)\n\n", "CodeBleu_score": 23.966182910552916}, {"code": "def _apply_basic(self, img, mixing_weights, m)\n\t\n    mixed = np.zeros(img.shape, dtype=img.dtype)\n    for i in range(len(mixing_weights)):\n        mixed += mixing_weights[i] * m(img)\n    mixed = np.clip(mixed, 0, 255)\n    mixed = np.asarray(mixed, dtype=img.dtype)\n    mixed = Image.fromarray(mixed)\n    mixed = Image.blend(img, mixed, self.blending_factor)\n    return mixed\n\n", "CodeBleu_score": 26.243768531637084}]}
{"instance_id": "84b5b752ff04", "generation_result": [{"code": "def adapt_to_chs(x, n)\n\t\n    if not isinstance(x, (list, tuple)):\n        x = (x,) * n\n    else:\n        assert len(x) == n, \"Input x must be a tuple or list of length n.\"\n    return x\n\n", "CodeBleu_score": 23.977516229166657}, {"code": "def adapt_to_chs(x, n)\n\t\n    if not isinstance(x, (tuple, list)):\n        x = (x,) * n\n    elif len(x) != n:\n        _logger.warning('The input x is a tuple or list of length {}, but the required number of channels is {}. The input will be adapted to match the required number of channels.'.format(len(x), n))\n        if len(x) == 1:\n            x = (x[0],) * n\n        else:\n            x = tuple(np.repeat(np.mean(x, axis=0), n))\n    else:\n        assert len(x) == n\n    return x\n\n", "CodeBleu_score": 48.90427183654979}, {"code": "def adapt_to_chs(x, n)\n\t\n    if not isinstance(x, (list, tuple)):\n        x = repeat(x, n)\n    else:\n        if len(x) != n:\n            _logger.warning(f\"Input x is a tuple or list of length {len(x)}, but required number of channels is {n}. Averaging the input and repeating it {n} times.\")\n            x = repeat(np.mean(x), n)\n        else:\n            assert len(x) == n, f\"Input x is a tuple or list of length {len(x)}, but required number of channels is {n}.\"\n    return x\n\n", "CodeBleu_score": 36.62486348652767}]}
{"instance_id": "54754b6ba394", "generation_result": [{"code": "def _worker_init(worker_id, worker_seeding='all')\n\t\n    worker_info = get_worker_info()\n    if worker_info is None:\n        raise RuntimeError('worker_init: worker_info is None')\n    worker_id = worker_info.id\n    if worker_id is None:\n        raise RuntimeError('worker_init: worker_id is None')\n    if worker_id == 0:\n        return\n    if worker_seeding is None:\n        raise RuntimeError('worker_init: worker_seeding is None')\n    if worker_seeding == 'all':\n        random.seed(worker_info.seed)\n        torch.manual_seed(worker_info.seed)\n        np.random.seed(worker_info.seed)\n    elif isinstance(worker_seeding, int):\n        np.random.seed(worker_seeding)\n    elif callable(worker_seeding):\n        worker_seeding(worker_id)\n    else:\n        raise RuntimeError('worker_init: worker_seeding is not a callable or a string')\n\n", "CodeBleu_score": 34.29904002301622}, {"code": "def _worker_init(worker_id, worker_seeding='all')\n\t\n    worker_info = get_worker_info()\n    if worker_info is None:\n        raise RuntimeError(\"worker_info is None, please use DataLoader with num_workers > 0\")\n    worker_id = worker_info.id\n    if worker_id is None:\n        raise RuntimeError(\"worker_id is None, please use DataLoader with num_workers > 0\")\n    if worker_seeding == 'all':\n        random.seed(worker_info.seed)\n        torch.manual_seed(worker_info.seed)\n        np.random.seed(worker_info.seed)\n    elif callable(worker_seeding):\n        worker_seeding(worker_info.seed)\n    elif isinstance(worker_seeding, str):\n        if worker_seeding == 'numpy':\n            np.random.seed(worker_info.seed)\n        elif worker_seeding == 'torch':\n            torch.manual_seed(worker_info.seed)\n        else:\n            raise ValueError(\"worker_seeding must be 'all', 'numpy', 'torch', or a callable\")\n    else:\n        raise ValueError(\"worker_seeding must be 'all', 'numpy', 'torch', or a callable\")\n\n", "CodeBleu_score": 32.39598801887445}, {"code": "def _worker_init(worker_id, worker_seeding='all')\n\t\n    worker_info = get_worker_info()\n    if worker_info is None:\n        raise RuntimeError('The function get_worker_info() must be called before calling _worker_init().')\n    if worker_id is None:\n        raise RuntimeError('The worker_id must be provided to the function _worker_init().')\n    if worker_info.id != worker_id:\n        raise RuntimeError('The worker_id provided to the function _worker_init() does not match the worker_id provided to the function get_worker_info().')\n    if worker_seeding is None:\n        raise RuntimeError('The worker_seeding must be provided to the function _worker_init().')\n    if worker_seeding == 'all':\n        random.seed(worker_info.seed)\n        torch.manual_seed(worker_info.seed)\n        np.random.seed(worker_info.seed)\n    elif callable(worker_seeding):\n        worker_seeding(worker_info.seed)\n    elif isinstance(worker_seeding, str):\n        if worker_seeding == 'all':\n            np.random.seed(worker_info.seed)\n        else:\n            raise RuntimeError('The worker_seeding must be a callable function or a string \\'all\\' for setting the numpy seed.')\n    else:\n        raise RuntimeError('The worker_seeding must be a callable function or a string \\'all\\' for setting the numpy seed.')\n\n", "CodeBleu_score": 30.0799507302789}]}
{"instance_id": "81dfdce940db", "generation_result": [{"code": "def _decode_and_center_crop(image_bytes, image_size, resize_method)\n\t\n    shape = extract_jpeg_shape(image_bytes)\n    image_height = shape[0]\n    image_width = shape[1]\n    window_height = image_size\n    window_width = image_size\n    offset_height = ((image_height - window_height) + 1) // 2\n    offset_width = ((image_width - window_width) + 1) // 2\n    image = decode_jpeg(image_bytes, channels=3)\n    image = tf.image.crop_to_bounding_box(image, offset_height, offset_width, window_height, window_width)\n    image = resize(image, image_size, resize_method)\n    return image\n\n", "CodeBleu_score": 27.522855502409683}, {"code": "def _decode_and_center_crop(image_bytes, image_size, resize_method)\n\t\n    shape = extract_jpeg_shape(image_bytes)\n    image_height = shape[0]\n    image_width = shape[1]\n    padded_center_crop_size = _get_padded_center_crop_size(image_height, image_width, image_size)\n    offset_height = _get_offset(image_height, padded_center_crop_size)\n    offset_width = _get_offset(image_width, padded_center_crop_size)\n    image = decode_jpeg(image_bytes, channels=3)\n    image = tf.image.crop_to_bounding_box(image, offset_height, offset_width, padded_center_crop_size, padded_center_crop_size)\n    image = resize(image, padded_center_crop_size, resize_method)\n    return image\n\n", "CodeBleu_score": 25.48364756242105}, {"code": "def _decode_and_center_crop(image_bytes, image_size, resize_method)\n\t\n    shape = extract_jpeg_shape(image_bytes)\n    image_height = shape[0]\n    image_width = shape[1]\n    image_channels = shape[2]\n    # Calculate the padded crop size.\n    crop_height = tf.cast(image_size, tf.float32)\n    crop_width = tf.cast(image_size, tf.float32)\n    offset_height = ((image_height - crop_height) + 1) // 2\n    offset_width = ((image_width - crop_width) + 1) // 2\n    # Crop the image to the specified size with a centered crop window.\n    cropped_image = tf.image.decode_and_crop_jpeg(\n        image_bytes,\n        offset_height=offset_height,\n        offset_width=offset_width,\n        target_height=crop_height,\n        target_width=crop_width,\n        channels=image_channels)\n    # Resize the image to the specified size.\n    resized_image = tf.compat.v1.image.resize(\n        cropped_image,\n        [image_size, image_size],\n        method=resize_method)\n    return resized_image\n\n", "CodeBleu_score": 34.167605777360386}]}
{"instance_id": "cf16a6d37111", "generation_result": [{"code": "def preprocess_for_train(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    image = tf.image.decode_jpeg(image_bytes, channels=3)\n    image = _decode_and_random_crop(image, image_size, interpolation)\n    image = _flip(image)\n    image = tf.reshape(image, [image_size, image_size, 3])\n    image = tf.image.convert_image_dtype(image, dtype=tf.bfloat16 if use_bfloat16 else tf.float32)\n    return image\n\n", "CodeBleu_score": 44.321745268848325}, {"code": "def preprocess_for_train(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    # Decode the image.\n    image = tf.image.decode_jpeg(image_bytes, channels=3)\n    # Randomly crop the image.\n    image = _decode_and_random_crop(image, image_size, interpolation)\n    # Flip the image horizontally.\n    image = _flip(image)\n    # Convert the image from RGB to BGR.\n    image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n    # Reshape the image.\n    image = tf.reshape(image, [image_size, image_size, 3])\n    # Convert the image from float32 to bfloat16.\n    if use_bfloat16:\n        image = tf.cast(image, dtype=tf.bfloat16)\n    return image\n\n", "CodeBleu_score": 41.98223307469367}, {"code": "def preprocess_for_train(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    image = tf.image.decode_jpeg(image_bytes, channels=3)\n    image = _decode_and_random_crop(image, image_size, interpolation)\n    image = _flip(image)\n    image = tf.reshape(image, [image_size, image_size, 3])\n    image = tf.image.convert_image_dtype(image, dtype=tf.bfloat16 if use_bfloat16 else tf.float32)\n    return image\n\n", "CodeBleu_score": 44.321745268848325}]}
{"instance_id": "78b2a3bd2142", "generation_result": [{"code": "def preprocess_for_eval(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    image = tf.image.decode_image(image_bytes, channels=3)\n    image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n    image = _decode_and_center_crop(image, image_size)\n    image = tf.image.resize(image, [image_size, image_size], method=interpolation)\n    image = tf.image.convert_image_dtype(image, dtype=tf.bfloat16 if use_bfloat16 else tf.float32)\n    return image\n\n", "CodeBleu_score": 41.3292195944825}, {"code": "def preprocess_for_eval(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    image = tf.image.decode_image(image_bytes, channels=3)\n    image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n    image = _decode_and_center_crop(image, image_size)\n    image = tf.image.resize(image, [image_size, image_size], method=interpolation)\n    if use_bfloat16:\n        image = tf.image.convert_image_dtype(image, dtype=tf.bfloat16)\n    else:\n        image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n    return image\n\n", "CodeBleu_score": 36.7461544126752}, {"code": "def preprocess_for_eval(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    image = tf.image.decode_image(image_bytes, channels=3)\n    image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n    image = tf.image.resize(image, [image_size, image_size], method=interpolation)\n    image = tf.image.convert_image_dtype(image, dtype=tf.bfloat16 if use_bfloat16 else tf.float32)\n    return image\n\n", "CodeBleu_score": 38.435265855784415}]}
{"instance_id": "8bd4364a9a0e", "generation_result": [{"code": "def evaluate(self, model, eval_dataloader=None, output_file=\"merged\")\n\t\n    if eval_dataloader is None:\n        eval_dataloader = self.eval_dataloader\n    if eval_dataloader is None:\n        raise ValueError(\"No evaluation dataloader found.\")\n    self.compute_metrics = self.compute_metrics_fn(self.task_name)\n    self.predict_loop(model, eval_dataloader, output_file)\n    return self.compute_metrics(self.preds, self.label_ids)\n\n", "CodeBleu_score": 50.95381905396613}, {"code": "def evaluate(self, model, eval_dataloader=None, output_file=\"merged\")\n\t\n\n    if eval_dataloader is None:\n        eval_dataloader = self.eval_dataloader\n\n    # Predict\n    self.predict_loop(model, eval_dataloader, output_file)\n\n    # Compute metrics\n    metrics = self.compute_metrics(eval_dataloader, output_file)\n\n    # Delete predictions\n    if os.path.exists(output_file):\n        os.remove(output_file)\n\n    return metrics\n\n", "CodeBleu_score": 55.71507753535072}, {"code": "def evaluate(self, model, eval_dataloader=None, output_file=\"merged\")\n\t\n    if eval_dataloader is None:\n        eval_dataloader = self.eval_dataloader\n    self.compute_metrics = self.compute_metrics.get_function()\n    self.predict_loop = self.predict_loop.get_function()\n    self.predict_loop(model, eval_dataloader, output_file)\n    return self.compute_metrics(output_file)\n\n", "CodeBleu_score": 55.245801979663824}]}
{"instance_id": "22d682c958c0", "generation_result": [{"code": " def print_computed_metrics(self, metrics)\n\t\n    print(\"Rank 1: {:.4f}\".format(metrics[\"rank1\"]))\n    print(\"Rank 5: {:.4f}\".format(metrics[\"rank5\"]))\n    print(\"mIoU: {:.4f}\".format(metrics[\"miou\"]))\n\n", "CodeBleu_score": 25.36505627327303}, {"code": " def print_computed_metrics(self, metrics)\n\t\n    print(\"Rank1: {:.4f}\".format(metrics['rank1']))\n    print(\"Rank5: {:.4f}\".format(metrics['rank5']))\n    print(\"mIoU: {:.4f}\".format(metrics['miou']))\n\n\n def print_computed_metrics(self, metrics):\n    \"\"\"The function prints the computed metrics for rank1, rank5, and miou.", "CodeBleu_score": 28.50297235508867}, {"code": " def print_computed_metrics(self, metrics)\n\t\n    print('Rank1: {:.4f}'.format(metrics['rank1']))\n    print('Rank5: {:.4f}'.format(metrics['rank5']))\n    print('mIoU: {:.4f}'.format(metrics['miou']))\n\n\n def print_computed_metrics(self, metrics):\n    \"\"\"The function prints the computed metrics for rank1, rank5, and miou.", "CodeBleu_score": 28.154066563163106}]}
{"instance_id": "39c1a50b7e7d", "generation_result": [{"code": "def _eval_predictions(self, segments, data)\n\t\n    average_iou = []\n    average_ranks = []\n    for segment, data in zip(segments, data):\n        iou = self._iou(segment, data)\n        rank = self._rank(iou)\n        average_iou.append(iou)\n        average_ranks.append(rank)\n    return np.mean(average_iou), np.mean(average_ranks)\n\n", "CodeBleu_score": 20.3612616293748}, {"code": "def _eval_predictions(self, segments, data)\n\t\n    average_iou = []\n    average_ranks = []\n    for segment, data_ in zip(segments, data):\n        ious = []\n        for pred_segment, pred_data in zip(segment, data_):\n            ious.append(self._iou(pred_segment, pred_data))\n        average_iou.append(np.mean(ious))\n        ranks = []\n        for pred_segment, pred_data in zip(segment, data_):\n            ranks.append(self._rank(pred_segment, pred_data))\n        average_ranks.append(np.mean(ranks))\n    return np.mean(average_ranks), np.mean(average_iou)\n\n", "CodeBleu_score": 28.89910218182697}, {"code": "def _eval_predictions(self, segments, data)\n\t\n    num_segments = len(segments)\n    num_data = len(data)\n    average_iou = []\n    average_ranks = []\n    for segment, data_point in zip(segments, data):\n        iou = self._iou(segment, data_point)\n        average_iou.append(iou)\n    average_iou = np.array(average_iou)\n    average_iou = np.sort(average_iou)\n    average_iou = np.mean(average_iou)\n    for segment, data_point in zip(segments, data):\n        iou = self._iou(segment, data_point)\n        average_ranks.append(np.sum(average_iou <= iou))\n    average_ranks = np.array(average_ranks)\n    average_ranks = np.sort(average_ranks)\n    average_ranks = np.mean(average_ranks)\n    rank1 = np.sum(average_ranks <= 1) / float(num_segments)\n    rank5 = np.sum(average_ranks <= 5) / float(num_segments)\n    return rank1, rank5, average_iou\n\n", "CodeBleu_score": 29.926756395603366}]}
{"instance_id": "8ccf84f8d034", "generation_result": [{"code": "def predict_loop(self, model, eval_dataloader, output_file=None)\n\t\n    model.eval()\n    model.to(self.device)\n    outputs = {}\n    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n        batch = self.to_ctx(batch)\n        with torch.no_grad():\n            outputs = self.predict_step(model, batch, outputs)\n    outputs = self.finalize(outputs)\n    if output_file is not None:\n        self.to_file(outputs, output_file)\n    return outputs\n\n", "CodeBleu_score": 39.09576378843844}, {"code": "def predict_loop(self, model, eval_dataloader, output_file=None)\n\t\n    model.eval()\n    model = model.to(self.device)\n    outputs = {}\n    for batch in tqdm(eval_dataloader):\n        batch = {k: v.to(self.device) for k, v in batch.items()}\n        with torch.no_grad():\n            outputs = self.to_ctx(model, batch, outputs)\n    outputs = self.finalize(outputs)\n    if output_file is not None:\n        self.to_file(outputs, output_file)\n    return outputs\n\n", "CodeBleu_score": 40.24394390534746}, {"code": "def predict_loop(self, model, eval_dataloader, output_file=None)\n\t\n    model.eval()\n    model.to(self.device)\n    outputs = {}\n    for batch in tqdm(eval_dataloader, desc=\"Prediction\"):\n        with torch.no_grad():\n            batch = {k: v.to(self.device) for k, v in batch.items()}\n            outputs = self.update_outputs(outputs, batch)\n    outputs = self.finalize(outputs)\n    if output_file:\n        self.to_ctx(outputs, output_file)\n    return outputs\n\n", "CodeBleu_score": 38.680350928587146}]}
{"instance_id": "e68529ad727d", "generation_result": [{"code": "def _aggregate_scores(self, scores)\n\t\n    assert len(scores) == 2\n    video_scores = np.concatenate(scores[0])\n    text_scores = np.concatenate(scores[1])\n    self.full_scores = None\n    return np.matmul(text_scores, video_scores)\n\n", "CodeBleu_score": 45.44671053934117}, {"code": "def _aggregate_scores(self, scores)\n\t\n    assert len(scores) == 2, \"Exactly two sets of scores are required.\"\n    assert len(scores[0]) == len(scores[1]), \"The number of scores in each set must be equal.\"\n    self.full_scores = np.concatenate(scores, axis=0)\n    self.full_scores = np.matmul(self.full_scores[:, :self.text_dim], self.full_scores[:, self.text_dim:])\n    self.full_scores = self.full_scores.reshape(-1, 1)\n    return self.full_scores\n\n", "CodeBleu_score": 33.098475436732585}, {"code": "def _aggregate_scores(self, scores)\n\t\n    assert len(scores) == 2\n    scores = [np.concatenate(score) for score in scores]\n    self.full_scores = None\n    return np.matmul(scores[0], scores[1])\n\n", "CodeBleu_score": 32.57403859593268}]}
{"instance_id": "e4e1861fc357", "generation_result": [{"code": "def finalize(self, Y_pred, Y_true, output_file=None)\n\t\n    if not self.is_finalized:\n        self.is_finalized = True\n        if self.verbose:\n            print('Finalizing predictions...')\n        Y_pred = torch.cat(Y_pred, dim=0)\n        Y_true = torch.cat(Y_true, dim=0)\n        Y_pred = Y_pred.cpu().numpy()\n        Y_true = Y_true.cpu().numpy()\n        if output_file is not None:\n            if self.verbose:\n                print('Saving predictions and true labels to file...')\n            with open(output_file, 'wb') as f:\n                pickle.dump((Y_pred, Y_true), f)\n        if self.verbose:\n            print('Prediction errors:')\n            for i in range(min(5, len(Y_pred))):\n                if Y_pred[i] != Y_true[i]:\n                    print(f'Prediction error: {Y_pred[i]} != {Y_true[i]}')\n        return {'Y_pred': Y_pred, 'Y_true': Y_true}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 42.757628128989126}, {"code": "def finalize(self, Y_pred, Y_true, output_file=None)\n\t\n    Y_pred = np.array(Y_pred)\n    Y_true = np.array(Y_true)\n    if output_file is not None:\n        pickle.dump([Y_pred, Y_true], open(output_file, 'wb'))\n    if len(Y_pred) != len(Y_true):\n        raise ValueError('Predictions and true labels must have the same length')\n    if len(Y_pred.shape) != 1:\n        raise ValueError('Predictions must be a one-dimensional array')\n    if len(Y_true.shape) != 1:\n        raise ValueError('True labels must be a one-dimensional array')\n    if Y_pred.shape[0] != Y_true.shape[0]:\n        raise ValueError('Predictions and true labels must have the same length')\n    if not set(Y_pred).issubset(set(Y_true)):\n        raise ValueError('Predictions must be a subset of the true labels')\n    errors = np.where(Y_pred != Y_true)[0]\n    if len(errors) > 0:\n        print('There are {} prediction errors.'.format(len(errors)))\n        print('Predictions and true labels are saved in pickle file {}.'.format(output_file))\n        print('Predictions are saved in the key \"pred\" and true labels are saved in the key \"true\".')\n        print('The following samples contain errors:')\n        for i in range(len(errors)):\n            print('Sample {}: Prediction {} != True label {}'.format(i, Y_pred[errors[i]], Y_true[errors[i]]))\n    return {'pred': Y_pred, 'true': Y_true}\n\n", "CodeBleu_score": 34.62419312149458}, {"code": "def finalize(self, Y_pred, Y_true, output_file=None)\n\t\n    Y_pred = torch.cat(Y_pred, dim=0).numpy()\n    Y_true = torch.cat(Y_true, dim=0).numpy()\n    if output_file is not None:\n        with open(output_file, \"wb\") as f:\n            pickle.dump((Y_pred, Y_true), f)\n    if self.check_errors:\n        error_samples = []\n        for i in range(Y_pred.shape[0]):\n            if Y_pred[i] != Y_true[i]:\n                error_samples.append(i)\n        if len(error_samples) > 0:\n            print(\"Error samples:\")\n            for i in error_samples:\n                print(f\"Sample {i}:\")\n                print(f\"Predicted label: {Y_pred[i]}\")\n                print(f\"True label: {Y_true[i]}\")\n    return {\"pred\": Y_pred, \"true\": Y_true}\n\n", "CodeBleu_score": 46.8105343252297}]}
{"instance_id": "7df9ab9a4920", "generation_result": [{"code": "def reduce_metrics(logging_outputs) -> None\n\t\n    # sum over number of samples in batch\n    logging_outputs = {k: sum(v) for k, v in logging_outputs.items()}\n    # average the loss\n    metrics.log_scalar('loss', logging_outputs.get('loss', 0) / logging_outputs.get('sample_size', 1))\n    metrics.log_scalar('sample_size', logging_outputs.get('sample_size', 1))\n    # logging_outputs['loss'] = logging_outputs['loss'] / logging_outputs['sample_size']\n    # logging_outputs['ntokens'] = logging_outputs['ntokens'] / logging_outputs['nsentences']\n    # logging_outputs['nsentences'] = 1\n    # logging_outputs['sample_size'] = 1\n    # logging_outputs['ntokens'] = 1\n\n", "CodeBleu_score": 27.653651343678128}, {"code": "def reduce_metrics(logging_outputs) -> None\n\t\n    # logging outputs is a dictionary\n    # logging outputs is a list of dictionaries\n    # logging outputs is a list of lists\n    # logging outputs is a list of lists of lists\n    # logging outputs is a list of lists of lists of lists\n    # logging outputs is a list of lists of lists of lists of lists\n    # logging outputs is a list of lists of lists of lists of lists of lists\n    # logging outputs is a list of lists of lists of lists of lists of lists of lists\n    # logging outputs is a list of lists of lists of lists of lists of lists of lists of lists\n    # logging outputs is a list of lists of lists of lists of lists of lists of lists of lists of lists\n    # logging outputs is a list of lists of lists of lists of lists of lists of lists of lists of lists of lists\n    # logging outputs is a list of lists of lists of lists of lists of lists of lists of lists of lists of lists of lists\n    # logging outputs is a list of lists of lists of lists of lists of lists of lists of lists of lists of lists of lists of lists\n    # logging outputs is a list of lists of lists of lists of lists of lists of lists of lists of lists of lists of lists of lists of lists\n    # logging outputs is a list of lists of lists of lists of lists of lists of lists of lists of lists of lists of lists of lists of lists of lists\n    # logging outputs is a list of lists of lists of lists of lists of lists of lists of lists of lists of lists of lists of lists of lists of lists of lists\n    # logging outputs is a list of lists of lists of lists of lists of lists of lists of lists of lists of lists of lists of lists of lists of lists of lists of lists\n    # logging outputs is a list of lists of lists of lists of lists of lists of lists of lists of lists of lists of lists of lists of lists of lists of lists of lists of lists\n    # logging outputs is a list of lists of lists of lists of lists of lists of lists of lists of lists of lists of lists of lists of lists of lists of lists of lists of lists of lists\n    # logging outputs is a list of lists of lists of lists of lists of lists of lists of lists of lists of lists of lists of lists of lists of lists of lists of lists of lists of lists of lists\n    # logging outputs is a", "CodeBleu_score": 0.8333333333333334}, {"code": "def reduce_metrics(logging_outputs) -> None\n\t\n    # TODO: This function is currently unused.\n    # TODO: This function should be moved to the metrics.py file.\n    # TODO: This function should be rewritten to use the new metrics.py file.\n    # TODO: This function should be rewritten to use the new metrics.py file.\n    # TODO: This function should be rewritten to use the new metrics.py file.\n    # TODO: This function should be rewritten to use the new metrics.py file.\n    # TODO: This function should be rewritten to use the new metrics.py file.\n    # TODO: This function should be rewritten to use the new metrics.py file.\n    # TODO: This function should be rewritten to use the new metrics.py file.\n    # TODO: This function should be rewritten to use the new metrics.py file.\n    # TODO: This function should be rewritten to use the new metrics.py file.\n    # TODO: This function should be rewritten to use the new metrics.py file.\n    # TODO: This function should be rewritten to use the new metrics.py file.\n    # TODO: This function should be rewritten to use the new metrics.py file.\n    # TODO: This function should be rewritten to use the new metrics.py file.\n    # TODO: This function should be rewritten to use the new metrics.py file.\n    # TODO: This function should be rewritten to use the new metrics.py file.\n    # TODO: This function should be rewritten to use the new metrics.py file.\n    # TODO: This function should be rewritten to use the new metrics.py file.\n    # TODO: This function should be rewritten to use the new metrics.py file.\n    # TODO: This function should be rewritten to use the new metrics.py file.\n    # TODO: This function should be rewritten to use the new metrics.py file.\n    # TODO: This function should be rewritten to use the new metrics.py file.\n    # TODO: This function should be rewritten to use the new metrics.py file.\n    # TODO: This function should be rewritten to use the new metrics.py file.\n    # TODO: This function should be rewritten to use the new metrics.py file.\n    #", "CodeBleu_score": 0.8333333333333334}]}
{"instance_id": "7f9c326a96f7", "generation_result": [{"code": "def _mm_on_the_fly(\n        self,\n        cmasks,\n        vmasks,\n        attention_mask\n    )\n\t\n    # Generate an attention mask and token type IDs.\n    if attention_mask is None:\n        attention_mask = self._mm_attention_mask(cmasks, vmasks)\n    token_type_ids = torch.cat([torch.zeros_like(cmasks), torch.ones_like(vmasks)], dim=-1)\n    return attention_mask, token_type_ids\n\n", "CodeBleu_score": 33.04421730860111}, {"code": "def _mm_on_the_fly(\n        self,\n        cmasks,\n        vmasks,\n        attention_mask\n    )\n\t\n    if attention_mask is None:\n        attention_mask = torch.zeros(cmasks.size(0), cmasks.size(1), dtype=torch.float32, device=cmasks.device)\n        attention_mask[:, :cmasks.size(1)] = cmasks.float()\n        attention_mask[:, cmasks.size(1):] = vmasks.float()\n    attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n    attention_mask = attention_mask.to(dtype=self.dtype)\n    attention_mask = (1.0 - attention_mask) * -10000.0\n    attention_mask = attention_mask.expand(\n        attention_mask.size(0),\n        attention_mask.size(1),\n        self.config.hidden_size\n    )\n    attention_mask = attention_mask.to(dtype=self.dtype)\n    vmasks = vmasks.unsqueeze(1).unsqueeze(2)\n    vmasks = vmasks.to(dtype=self.dtype)\n    vmasks = (1.0 - vmasks) * -10000.0\n    vmasks = vmasks.expand(\n        vmasks.size(0),\n        vmasks.size(1),\n        self.config.hidden_size\n    )\n    vmasks = vmasks.to(dtype=self.dtype)\n    token_type_ids = torch.cat([torch.zeros_like(cmasks), torch.ones_like(vmasks)], dim=1)\n    return attention_mask, token_type_ids\n\n", "CodeBleu_score": 40.9205107686101}, {"code": "def _mm_on_the_fly(\n        self,\n        cmasks,\n        vmasks,\n        attention_mask\n    )\n\t\n    if attention_mask is None:\n        attention_mask = torch.zeros(cmasks.size(0), cmasks.size(1), vmasks.size(1)).to(cmasks.device)\n        attention_mask[:, :, 0] = 1\n        attention_mask = attention_mask.unsqueeze(1)\n        attention_mask = attention_mask.unsqueeze(1)\n        attention_mask = attention_mask * cmasks.unsqueeze(2) * vmasks.unsqueeze(1)\n        attention_mask = attention_mask.view(cmasks.size(0), cmasks.size(1), vmasks.size(1))\n        attention_mask = (1.0 - attention_mask) * -100000.0\n    attention_mask = attention_mask.unsqueeze(1)\n    attention_mask = attention_mask.unsqueeze(1)\n    token_type_ids = torch.cat([torch.zeros(cmasks.size(0), cmasks.size(1), vmasks.size(1)).to(cmasks.device), torch.ones(cmasks.size(0), cmasks.size(1), vmasks.size(1)).to(cmasks.device)], dim=1)\n    return attention_mask, token_type_ids\n\n", "CodeBleu_score": 41.77404833272196}]}
{"instance_id": "fa99d2a148a5", "generation_result": [{"code": "def prepare_inputs_for_generation(\n        self,\n        input_ids,\n        input_video_embeds,\n        attention_mask=None,\n        token_type_ids=None,\n        **model_kwargs\n    )\n\t\n    # input_ids = input_ids.view(-1, input_ids.size(-1))\n    # input_video_embeds = input_video_embeds.view(-1, input_video_embeds.size(-1))\n    # attention_mask = attention_mask.view(-1, attention_mask.size(-1))\n    # token_type_ids = token_type_ids.view(-1, token_type_ids.size(-1))\n    # if token_type_ids is not None:\n    #     assert token_type_ids.size() == attention_mask.size()\n    # if token_type_ids is None:\n    #     assert len(input_ids) == len(input_video_embeds)\n    # else:\n    #     assert len(input_ids) == len(input_video_embeds) == len(token_type_ids)\n    #     assert token_type_ids.max().item() < self.config.type_vocab_size\n    # model_inputs = {}\n    # model_inputs[\"input_ids\"] = input_ids\n    # model_inputs[\"input_video_embeds\"] = input_video_embeds\n    # if attention_mask is not None:\n    #     model_inputs[\"attention_mask\"] = attention_mask\n    # if token_type_ids is not None:\n    #     model_inputs[\"token_type_ids\"] = token_type_ids\n    # return model_inputs\n    input_ids = input_ids.view(-1, input_ids.size(-1))\n    input_video_embeds = input_video_embeds.view(-1, input_video_embeds.size(-1))\n    attention_mask = attention_mask.view(-1, attention_mask.size(-1))\n    token_type_ids = token_type_ids.view(-1, token_type_ids.size(-1))\n    if token_type_ids is not None:\n        assert token_type_ids.size() == attention_mask.size()\n    if token_type_ids is None:\n        assert len(input_ids) == len(input_video", "CodeBleu_score": 33.418492669941564}, {"code": "def prepare_inputs_for_generation(\n        self,\n        input_ids,\n        input_video_embeds,\n        attention_mask=None,\n        token_type_ids=None,\n        **model_kwargs\n    )\n\t\n    # input_ids = torch.tensor(input_ids, dtype=torch.long, device=self.device)\n    # input_video_embeds = torch.tensor(input_video_embeds, dtype=torch.float, device=self.device)\n    # attention_mask = torch.tensor(attention_mask, dtype=torch.long, device=self.device)\n    # token_type_ids = torch.tensor(token_type_ids, dtype=torch.long, device=self.device)\n    # print(input_ids.size)\n    # print(input_video_embeds.size)\n    # print(attention_mask.size)\n    # print(token_type_ids.size)\n    # print(input_ids.size)\n    # print(input_video_embeds.size)\n    # print(attention_mask.size)\n    # print(token_type_ids.size)\n    # print(input_ids.size)\n    # print(input_video_embeds.size)\n    # print(attention_mask.size)\n    # print(token_type_ids.size)\n    # print(input_ids.size)\n    # print(input_video_embeds.size)\n    # print(attention_mask.size)\n    # print(token_type_ids.size)\n    # print(input_ids.size)\n    # print(input_video_embeds.size)\n    # print(attention_mask.size)\n    # print(token_type_ids.size)\n    # print(input_ids.size)\n    # print(input_video_embeds.size)\n    # print(attention_mask.size)\n    # print(token_type_ids.size)\n    # print(input_ids.size)\n    # print(input_video_embeds.size)\n    # print(attention_mask.size)\n    # print(token_type_ids.size)\n    # print(input_ids.size)\n    # print(input_video_embeds.size)\n    # print(attention_mask.", "CodeBleu_score": 5.36986301369863}, {"code": "def prepare_inputs_for_generation(\n        self,\n        input_ids,\n        input_video_embeds,\n        attention_mask=None,\n        token_type_ids=None,\n        **model_kwargs\n    )\n\t\n    # input_ids = input_ids.view(-1, input_ids.size(-1))\n    # input_video_embeds = input_video_embeds.view(-1, input_video_embeds.size(-1))\n    # attention_mask = attention_mask.view(-1, attention_mask.size(-1))\n    # token_type_ids = token_type_ids.view(-1, token_type_ids.size(-1))\n    # if attention_mask is None:\n    #     attention_mask = torch.ones_like(input_ids)\n    # if token_type_ids is None:\n    #     token_type_ids = torch.zeros_like(input_ids)\n    # if input_ids.size() != input_video_embeds.size():\n    #     raise ValueError(\n    #         \"input_ids and input_video_embeds have different sizes: {} and {}\".format(\n    #             input_ids.size(), input_video_embeds.size()\n    #         )\n    #     )\n    # input_ids = input_ids[:, : self.config.max_position_embeddings]\n    # input_video_embeds = input_video_embeds[:, : self.config.max_position_embeddings]\n    # attention_mask = attention_mask[:, : self.config.max_position_embeddings]\n    # token_type_ids = token_type_ids[:, : self.config.max_position_embeddings]\n    # input_ids = input_ids[:, -input_video_embeds.size(1):]\n    # input_video_embeds = input_video_embeds[:, -input_video_embeds.size(1):]\n    # attention_mask = attention_mask[:, -input_video_embeds.size(1):]\n    # token_type_ids = token_type_ids[:, -input_video_embeds.size(1):]\n    # return {\n    #     \"input_ids\": input_ids,\n    #     \"input_video_embeds\": input_video_embeds,\n    #", "CodeBleu_score": 5.36986301369863}]}
{"instance_id": "b1b0d25fe620", "generation_result": [{"code": "def _init_pipeline(self, cfg: ConfigType) -> Compose\n\t\n    cfg = deepcopy(cfg)\n    meta_keys = cfg.get('meta_keys', ('img_id', 'img_info', 'img_prefix'))\n    if 'img_id' in meta_keys:\n        meta_keys.remove('img_id')\n    cfg['meta_keys'] = meta_keys\n    pipeline = [t for t in cfg['test_pipeline'] if t['type'] != 'LoadImageFromFile']\n    pipeline.insert(0, dict(type='mmdet.InferencerLoader'))\n    self._get_transform_idx(pipeline)\n    pipeline = Compose(pipeline)\n    return pipeline\n\n", "CodeBleu_score": 26.552994982724186}, {"code": "def _init_pipeline(self, cfg: ConfigType) -> Compose\n\t\n    pipeline = cfg.data.test.pipeline\n    meta_keys = pipeline[0]['meta_keys']\n    if 'img_id' in meta_keys:\n        meta_keys.remove('img_id')\n    pipeline[0]['meta_keys'] = meta_keys\n    pipeline[0]['type'] = 'mmdet.InferencerLoader'\n    pipeline[0]['kwargs']['img_id_key'] = 'img_id'\n    pipeline[0]['kwargs']['img_id_list_key'] = 'img_id_list'\n    pipeline[0]['kwargs']['img_id_list_len_key'] = 'img_id_list_len'\n    pipeline[0]['kwargs']['img_id_list_max_len_key'] = 'img_id_list_max_len'\n    pipeline[0]['kwargs']['img_id_list_len_key'] = 'img_id_list_len'\n    pipeline[0]['kwargs']['img_id_list_max_len_key'] = 'img_id_list_max_len'\n    pipeline[0]['kwargs']['img_id_list_len_key'] = 'img_id_list_len'\n    pipeline[0]['kwargs']['img_id_list_max_len_key'] = 'img_id_list_max_len'\n    pipeline[0]['kwargs']['img_id_list_len_key'] = 'img_id_list_len'\n    pipeline[0]['kwargs']['img_id_list_max_len_key'] = 'img_id_list_max_len'\n    pipeline[0]['kwargs']['img_id_list_len_key'] = 'img_id_list_len'\n    pipeline[0]['kwargs']['img_id_list_max_len_key'] = 'img_id_list_max_len'\n    pipeline[0]['kwargs']['img_id_list_len_key'] = 'img_id_list_len'\n    pipeline[0]['kwargs']['img_id_list_max_len_key'] = 'img_id_list_max_len'\n    pipeline[0]['kwargs']['img_id_list_len_key'] = 'img_id", "CodeBleu_score": 17.373859657315638}, {"code": "def _init_pipeline(self, cfg: ConfigType) -> Compose\n\t\n    pipeline = cfg.data.test.pipeline\n    # remove img_id from meta_keys\n    if 'img_id' in pipeline[0]['meta_keys']:\n        pipeline[0]['meta_keys'].remove('img_id')\n    # update transform type\n    for i, transform in enumerate(pipeline):\n        if transform['type'] == 'LoadImageFromFile':\n            pipeline[i]['type'] = 'mmdet.InferencerLoader'\n    return Compose(pipeline)\n\n", "CodeBleu_score": 22.648407542324232}]}
{"instance_id": "7967962856eb", "generation_result": [{"code": "def _inputs_to_list(self, inputs: InputsType) -> list\n\t\n    if isinstance(inputs, str):\n        if isdir(inputs):\n            inputs = list_dir_or_file(inputs)\n        else:\n            inputs = [inputs]\n    elif not isinstance(inputs, (list, tuple)):\n        inputs = [inputs]\n    return inputs\n\n", "CodeBleu_score": 37.27664845802001}, {"code": "def _inputs_to_list(self, inputs: InputsType) -> list\n\t\n    if isinstance(inputs, str):\n        if isdir(inputs):\n            inputs = list_dir_or_file(inputs)\n        else:\n            inputs = [inputs]\n    elif not isinstance(inputs, (list, tuple)):\n        inputs = [inputs]\n    return inputs\n\n", "CodeBleu_score": 37.27664845802001}, {"code": "def _inputs_to_list(self, inputs: InputsType) -> list\n\t\n    if isinstance(inputs, str):\n        if isdir(inputs):\n            inputs = join_path(inputs, '*')\n            inputs = glob(inputs)\n        else:\n            inputs = [inputs]\n    elif not isinstance(inputs, (list, tuple)):\n        inputs = [inputs]\n    return inputs\n\n", "CodeBleu_score": 43.545103195398234}]}
{"instance_id": "74f727f06fa0", "generation_result": [{"code": "def _get_chunk_data(self, inputs: Iterable, chunk_size: int)\n\t\n    chunk_data = []\n    for i, input_data in enumerate(inputs):\n        if isinstance(input_data, dict):\n            if 'img' in input_data:\n                input_data = input_data['img']\n            elif 'img_path' in input_data:\n                input_data = self.pipeline(input_data['img_path'])\n        if i % chunk_size == 0:\n            if i > 0:\n                yield chunk_data\n                chunk_data = []\n        chunk_data.append(input_data)\n    if len(chunk_data) > 0:\n        yield chunk_data\n\n", "CodeBleu_score": 38.4376778924801}, {"code": "def _get_chunk_data(self, inputs: Iterable, chunk_size: int)\n\t\n    chunk_data = []\n    for input_ in inputs:\n        if isinstance(input_, dict):\n            if \"img\" in input_:\n                input_ = input_[\"img\"]\n            elif \"img_path\" in input_:\n                input_ = input_[\"img_path\"]\n        chunk_data.append(input_)\n        if len(chunk_data) == chunk_size:\n            yield self.pipeline(chunk_data)\n            chunk_data = []\n    if chunk_data:\n        yield self.pipeline(chunk_data)\n\n", "CodeBleu_score": 31.11181966214512}, {"code": "def _get_chunk_data(self, inputs: Iterable, chunk_size: int)\n\t\n    chunk_data = []\n    for input_data in inputs:\n        if isinstance(input_data, dict) and ('img' in input_data or 'img_path' in input_data):\n            input_data = self.pipeline(input_data)\n        for chunk in chunk(input_data, chunk_size):\n            chunk_data.append(chunk)\n        if len(chunk_data) == 0:\n            continue\n        yield chunk_data\n        chunk_data = []\n    if len(chunk_data) > 0:\n        yield chunk_data\n\n", "CodeBleu_score": 30.71673198047274}]}
{"instance_id": "a43ab51afa59", "generation_result": [{"code": "def postprocess(\n        self,\n        preds: PredType,\n        visualization: Optional[List[np.ndarray]] = None,\n        return_datasamples: bool = False,\n        print_result: bool = False,\n        no_save_pred: bool = False,\n        pred_out_dir: str = '',\n        **kwargs,\n    ) -> Dict\n\t\n    if not no_save_pred:\n        if not pred_out_dir:\n            pred_out_dir = self.pred_out_dir\n        if not os.path.exists(pred_out_dir):\n            os.makedirs(pred_out_dir)\n        if return_datasamples:\n            warnings.warn('Saving datasamples is not supported.')\n    if not return_datasamples:\n        preds = self.pred2dict(preds)\n    result = {'preds': preds}\n    if visualization:\n        result['visualization'] = visualization\n    if print_result:\n        print(result)\n    return result\n\n", "CodeBleu_score": 45.678551220262555}, {"code": "def postprocess(\n        self,\n        preds: PredType,\n        visualization: Optional[List[np.ndarray]] = None,\n        return_datasamples: bool = False,\n        print_result: bool = False,\n        no_save_pred: bool = False,\n        pred_out_dir: str = '',\n        **kwargs,\n    ) -> Dict\n\t\n    if pred_out_dir and not return_datasamples:\n        warnings.warn('Saving predictions is not supported if return_datasamples is False.')\n    if not return_datasamples:\n        if no_save_pred:\n            pred2dict = self.pred2dict\n        else:\n            pred2dict = self.pred2dict_save\n        results = []\n        for i, pred in enumerate(preds):\n            results.append(pred2dict(pred, i, visualization[i], **kwargs))\n        return results\n    else:\n        if pred_out_dir:\n            os.makedirs(pred_out_dir, exist_ok=True)\n            for i, pred in enumerate(preds):\n                pred2dict(pred, i, visualization[i], pred_out_dir, **kwargs)\n        if print_result:\n            for i, pred in enumerate(preds):\n                print(pred2dict(pred, i, visualization[i], **kwargs))\n        return None\n\n", "CodeBleu_score": 42.80583013143448}, {"code": "def postprocess(\n        self,\n        preds: PredType,\n        visualization: Optional[List[np.ndarray]] = None,\n        return_datasamples: bool = False,\n        print_result: bool = False,\n        no_save_pred: bool = False,\n        pred_out_dir: str = '',\n        **kwargs,\n    ) -> Dict\n\t\n    if pred_out_dir and not no_save_pred:\n        if not os.path.exists(pred_out_dir):\n            os.makedirs(pred_out_dir)\n        else:\n            warnings.warn(f'Prediction output directory already exists: {pred_out_dir}')\n    if not return_datasamples and not no_save_pred:\n        if pred_out_dir:\n            warnings.warn('Saving predictions is not supported when return_datasamples is False.')\n        if print_result:\n            warnings.warn('Printing predictions is not supported when return_datasamples is False.')\n    if not return_datasamples and not no_save_pred and not pred_out_dir:\n        warnings.warn('No prediction output directory was specified and predictions are not to be saved.')\n    results = []\n    if print_result:\n        print('Predictions:')\n    for i, pred in enumerate(preds):\n        if return_datasamples:\n            results.append(pred)\n        else:\n            results.append(self.pred2dict(pred))\n        if visualization:\n            if print_result:\n                print(f'{i+1}: {pred[\"label\"]}')\n            if not no_save_pred:\n                if pred_out_dir:\n                    warnings.warn('Saving predictions is not supported when return_datasamples is True.')\n                if print_result:\n                    warnings.warn('Printing predictions is not supported when return_datasamples is True.')\n        if print_result:\n            print(pred)\n    if print_result:\n        print()\n    if visualization:\n        for i, visualization_sample in enumerate(visualization):\n            if return_datasamples:\n                results.append(visualization_sample)\n            else:\n                results.append(self.pred2dict(visualization_sample))\n            if print_result:\n                print(f'{i+1}: {visualization_sample[\"label\"]}')\n            if print_result:\n                print(visualization_sample)\n    if print_result:\n        print()\n    return results\n\n", "CodeBleu_score": 38.17934593088131}]}
{"instance_id": "a3c2561f199c", "generation_result": [{"code": "def build_test_pipeline(cfg: ConfigType) -> ConfigType\n\t\n    test_pipeline = copy.deepcopy(cfg.data.test.pipeline)\n    resize_transform = copy.deepcopy(test_pipeline[0])\n    resize_transform.pop('keys')\n    resize_transform.pop('keep_ratio')\n    resize_transform.pop('ratio_range')\n    resize_transform.pop('scale_range')\n    resize_transform.pop('size_divisor')\n    resize_transform.pop('interpolation')\n    resize_transform.update(dict(type='Resize', img_scale=cfg.data.test.img_scale, keep_ratio=False))\n    test_pipeline.insert(0, resize_transform)\n    test_pipeline.append(dict(type='ImageToTensor', keys=['img']))\n    test_pipeline.append(dict(type='Collect', keys=['img']))\n    return test_pipeline\n\n", "CodeBleu_score": 26.601831485990342}, {"code": "def build_test_pipeline(cfg: ConfigType) -> ConfigType\n\t\n    # copy the first transform in the pipeline\n    first_transform = copy.deepcopy(cfg.dataset_type.pipeline[0])\n    # modify the first transform to include only the 'Resize' transform\n    first_transform = first_transform.transforms[0]\n    # copy the last transform in the pipeline\n    last_transform = copy.deepcopy(cfg.dataset_type.pipeline[-1])\n    # combine the modified first and copied last transforms into a new test pipeline\n    test_pipeline = Compose([first_transform, last_transform])\n    # return the constructed test pipeline\n    return test_pipeline\n\n", "CodeBleu_score": 23.199581047421404}, {"code": "def build_test_pipeline(cfg: ConfigType) -> ConfigType\n\t\n    test_transform = copy.deepcopy(cfg.data.test_transform)\n    test_transform[0] = test_transform[0][0]\n    test_transform.append(test_transform[-1])\n    test_transform = Compose(test_transform)\n    cfg.data.test_transform = test_transform\n    return cfg\n\n", "CodeBleu_score": 17.82524840198077}]}
{"instance_id": "48a358ab5a38", "generation_result": [{"code": "def inference_mot(model: nn.Module, img: np.ndarray, frame_id: int,\n                  video_len: int) -> SampleList\n\t\n    # prepare input data\n    input_data = {\"img\": img, \"frame_id\": frame_id, \"video_len\": video_len}\n    input_data = {k: v.astype(np.float32) for k, v in input_data.items()}\n    input_data = {k: torch.from_numpy(v) for k, v in input_data.items()}\n    input_data = SampleList(input_data)\n    # build test pipeline\n    test_pipeline = build_test_pipeline(model.cfg.data.test)\n    # build the model\n    model = model.eval()\n    # process input data\n    with torch.no_grad():\n        # check if the model is on the CPU\n        if next(model.parameters()).is_cuda:\n            # check if the model contains unsupported modules\n            if isinstance(model, torch.nn.DataParallel):\n                model = model.module\n            for module in model.modules():\n                if isinstance(module, RoIPool):\n                    raise NotImplementedError(\"RoIPool is not supported on the GPU\")\n            # run the test pipeline\n            input_data = test_pipeline(input_data)\n            # run the model\n            model_output = model(input_data)\n        else:\n            # check if the model contains unsupported modules\n            if isinstance(model, torch.nn.DataParallel):\n                model = model.module\n            for module in model.modules():\n                if isinstance(module, RoIPool):\n                    raise NotImplementedError(\"RoIPool is not supported on the CPU\")\n            # run the test pipeline\n            input_data = test_pipeline(input_data)\n            # run the model\n            model_output = model(input_data)\n    # return the inference result\n    return model_output\n\n", "CodeBleu_score": 34.07317625310872}, {"code": "def inference_mot(model: nn.Module, img: np.ndarray, frame_id: int,\n                  video_len: int) -> SampleList\n\t\n    # prepare input data\n    input_data = {\n        \"image\": img,\n        \"frame_id\": frame_id,\n        \"video_len\": video_len,\n        \"image_id\": frame_id,\n        \"original_shape\": img.shape[:2]  # H, W\n    }\n\n    # build test pipeline\n    test_pipeline = build_test_pipeline(cfg=model.test_cfg, dataset=model.dataset)\n\n    # check if the model is on the CPU\n    if next(model.parameters()).device.type == \"cpu\":\n        # if so, make sure no unsupported modules are used\n        unsupported_modules = [\n            \"RoIPool\",\n            \"RoIAlign\",\n            \"RoIFeatureTransform\",\n            \"DeformConv\",\n            \"DeformConvPack\",\n            \"ModulatedDeformConv\",\n            \"ModulatedDeformConvPack\",\n            \"SigmoidFocalLoss\",\n            \"MaskedConv\",\n            \"ContextBlock\",\n            \"PAA\",\n            \"PAConv\",\n            \"PointRend\",\n            \"PointRend+DeformConv\",\n            \"PointRend+DeformConvPack\",\n            \"PointRend+ModulatedDeformConv\",\n            \"PointRend+ModulatedDeformConvPack\"\n        ]\n        for module in unsupported_modules:\n            assert module not in model.module_str, \"{} is not supported on CPU.\".format(module)\n\n    # process input data through the test pipeline\n    data = test_pipeline(input_data)\n\n    # forward pass\n    with torch.no_grad():\n        result = model.test_step(data, model)\n\n    return result\n\n", "CodeBleu_score": 34.8296484296072}, {"code": "def inference_mot(model: nn.Module, img: np.ndarray, frame_id: int,\n                  video_len: int) -> SampleList\n\t\n    # pylint: disable=arguments-differ\n    if not isinstance(img, torch.Tensor):\n        img = torch.from_numpy(img.transpose(2, 0, 1))\n\n    data = {\n        \"img\": img,\n        \"frame_id\": frame_id,\n        \"video_len\": video_len,\n        \"img_shape\": img.shape,\n        \"img_id\": 0,\n        \"video_id\": 0\n    }\n\n    data = {k: torch.as_tensor(v) for k, v in data.items()}\n\n    data = {k: v.to(device=next(model.parameters()).device) for k, v in data.items()}\n\n    if not isinstance(model, torch.nn.Module):\n        raise TypeError(\"model must be an instance of torch.nn.Module\")\n\n    if not isinstance(img, torch.Tensor):\n        raise TypeError(\"img must be a torch.Tensor\")\n\n    if not isinstance(frame_id, int):\n        raise TypeError(\"frame_id must be an int\")\n\n    if not isinstance(video_len, int):\n        raise TypeError(\"video_len must be an int\")\n\n    if not isinstance(img_shape, tuple):\n        raise TypeError(\"img_shape must be a tuple\")\n\n    if not isinstance(img_id, int):\n        raise TypeError(\"img_id must be an int\")\n\n    if not isinstance(video_id, int):\n        raise TypeError(\"video_id must be an int\")\n\n    if not isinstance(img, torch.Tensor):\n        raise TypeError(\"img must be a torch.Tensor\")\n\n    if not isinstance(frame_id, int):\n        raise TypeError(\"frame_id must be an int\")\n\n    if not isinstance(video_len, int):\n        raise TypeError(\"video_len must be an int\")\n\n    if not isinstance(img_shape, tuple):\n        raise TypeError(\"img_shape must be a tuple\")\n\n    if not isinstance(img_id, int):\n        raise TypeError(\"img_id", "CodeBleu_score": 35.06499543682978}]}
{"instance_id": "6c4ea2ce1787", "generation_result": [{"code": "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\n\t\n    while True:\n        total_length = len(tokens_a) + len(tokens_b)\n        if total_length <= max_num_tokens:\n            break\n        trunc_tokens = tokens_a if len(tokens_a) > len(tokens_b) else tokens_b\n        assert len(trunc_tokens) >= 1\n        if rng.random() < 0.5:\n            del trunc_tokens[0]\n        else:\n            trunc_tokens.pop()\n", "CodeBleu_score": 79.2280525202322}, {"code": "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\n\t\n    while True:\n        total_length = len(tokens_a) + len(tokens_b)\n        if total_length <= max_num_tokens:\n            break\n\n        trunc_tokens = tokens_a if len(tokens_a) > len(tokens_b) else tokens_b\n        assert len(trunc_tokens) >= 1\n\n        if rng.random() < 0.5:\n            del trunc_tokens[0]\n        else:\n            trunc_tokens.pop()\n", "CodeBleu_score": 79.2280525202322}, {"code": "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\n\t\n    while True:\n        total_length = len(tokens_a) + len(tokens_b)\n        if total_length <= max_num_tokens:\n            break\n\n        trunc_tokens = tokens_a if len(tokens_a) > len(tokens_b) else tokens_b\n        assert len(trunc_tokens) >= 1\n\n        # We want to sometimes truncate from the front and sometimes from the\n        # back to add more randomness and avoid biases.\n        if rng.random() < 0.5:\n            del trunc_tokens[0]\n        else:\n            trunc_tokens.pop()\n\n", "CodeBleu_score": 79.2280525202322}]}
{"instance_id": "007181b6729d", "generation_result": [{"code": "def get_activation(activation_string)\n\t\n    if not isinstance(activation_string, str):\n        return activation_string\n    if not activation_string.lower():\n        return None\n    if activation_string.lower() == 'relu':\n        return tf.nn.relu\n    if activation_string.lower() == 'sigmoid':\n        return tf.nn.sigmoid\n    if activation_string.lower() == 'tanh':\n        return tf.nn.tanh\n    if activation_string.lower() == 'softplus':\n        return tf.nn.softplus\n    if activation_string.lower() == 'softsign':\n        return tf.nn.softsign\n    if activation_string.lower() == 'elu':\n        return tf.nn.elu\n    if activation_string.lower() == 'selu':\n        return tf.nn.selu\n    if activation_string.lower() == 'softmax':\n        return tf.nn.softmax\n    if activation_string.lower() == 'leakyrelu':\n        return tf.nn.leaky_relu\n    if activation_string.lower() == 'prelu':\n        return tf.nn.parametric_relu\n    if activation_string.lower() == 'crelu':\n        return tf.nn.crelu\n    if activation_string.lower() == 'softimax':\n        return tf.nn.softmax\n    if activation_string.lower() == 'softmax2d':\n        return tf.nn.softmax\n    if activation_string.lower() == 'softmax3d':\n        return tf.nn.softmax\n    if activation_string.lower() == 'softmax4d':\n        return tf.nn.softmax\n    if activation_string.lower() == 'softmax5d':\n        return tf.nn.softmax\n    if activation_string.lower() == 'softmax6d':\n        return tf.nn.softmax\n    if activation_string.lower() == 'softmax7d':\n        return tf.nn.softmax\n    if activation_string.lower() == 'softmax8d':\n        return tf.nn.softmax\n    if activation_string.lower() == 'softmax", "CodeBleu_score": 29.46394107388759}, {"code": "def get_activation(activation_string)\n\t\n    if not isinstance(activation_string, str):\n        return activation_string\n    if not activation_string:\n        return None\n    if activation_string.lower() == 'relu':\n        return tf.nn.relu\n    if activation_string.lower() == 'leakyrelu':\n        return tf.nn.leaky_relu\n    if activation_string.lower() == 'elu':\n        return tf.nn.elu\n    if activation_string.lower() == 'selu':\n        return tf.nn.selu\n    if activation_string.lower() == 'softplus':\n        return tf.nn.softplus\n    if activation_string.lower() == 'softsign':\n        return tf.nn.softsign\n    if activation_string.lower() == 'sigmoid':\n        return tf.nn.sigmoid\n    if activation_string.lower() == 'tanh':\n        return tf.nn.tanh\n    if activation_string.lower() == 'hard_sigmoid':\n        return tf.nn.hard_sigmoid\n    if activation_string.lower() == 'linear':\n        return tf.identity\n    raise ValueError('Unsupported activation function: ' + str(activation_string))\n\n", "CodeBleu_score": 39.3478641659911}, {"code": "def get_activation(activation_string)\n\t\n    if not isinstance(activation_string, str):\n        return activation_string\n    if not activation_string:\n        return None\n    if activation_string.lower() == 'relu':\n        return tf.nn.relu\n    elif activation_string.lower() == 'softmax':\n        return tf.nn.softmax\n    elif activation_string.lower() == 'sigmoid':\n        return tf.nn.sigmoid\n    elif activation_string.lower() == 'tanh':\n        return tf.nn.tanh\n    elif activation_string.lower() == 'elu':\n        return tf.nn.elu\n    elif activation_string.lower() == 'selu':\n        return tf.nn.selu\n    elif activation_string.lower() == 'softplus':\n        return tf.nn.softplus\n    elif activation_string.lower() == 'softsign':\n        return tf.nn.softsign\n    elif activation_string.lower() == 'logsigmoid':\n        return tf.nn.log_sigmoid\n    elif activation_string.lower() == 'exponential':\n        return tf.nn.exponential\n    elif activation_string.lower() == 'hard_sigmoid':\n        return tf.nn.hard_sigmoid\n    elif activation_string.lower() == 'swish':\n        return tf.nn.swish\n    elif activation_string.lower() == 'mish':\n        return tf.nn.mish\n    elif activation_string.lower() == 'gelu':\n        return tf.nn.gelu\n    elif activation_string.lower() == 'log_softmax':\n        return tf.nn.log_softmax\n    elif activation_string.lower() == 'soft_sign':\n        return tf.nn.soft_sign\n    elif activation_string.lower() == 'soft_plus':\n        return tf.nn.soft_plus\n    elif activation_string.lower() == 'soft_max':\n        return tf.nn.soft_max\n    elif activation_string.lower() == 'relu6':\n        return tf.nn.relu6\n    elif activation_string.lower() == 'relu_no_", "CodeBleu_score": 29.810730066074232}]}
{"instance_id": "d5d6e0315899", "generation_result": [{"code": "def dropout(input_tensor, dropout_prob)\n\t\n    if dropout_prob is None or dropout_prob == 0.0:\n        return input_tensor\n    else:\n        return tf.nn.dropout(input_tensor, dropout_prob)\n\n", "CodeBleu_score": 53.31243274886709}, {"code": "def dropout(input_tensor, dropout_prob)\n\t\n    if dropout_prob is None or dropout_prob == 0.0:\n        return input_tensor\n    else:\n        return tf.nn.dropout(input_tensor, dropout_prob)\n\n", "CodeBleu_score": 53.31243274886709}, {"code": "def dropout(input_tensor, dropout_prob)\n\t\n    if dropout_prob is None or dropout_prob == 0:\n        return input_tensor\n    else:\n        return tf.nn.dropout(input_tensor, dropout_prob)\n\n", "CodeBleu_score": 47.64493389882693}]}
{"instance_id": "828917b9ac53", "generation_result": [{"code": "def create_attention_mask_from_input_mask(from_tensor, to_mask)\n\t\n    shape_list = get_shape_list(from_tensor, expected_rank=[2, 3])\n    mask = tf.cast(tf.reshape(to_mask, [shape_list[1]]), dtype=from_tensor.dtype)\n    attention_mask = (1.0 - mask) * -10000.0\n    return attention_mask\n\n", "CodeBleu_score": 18.392719972182505}, {"code": "def create_attention_mask_from_input_mask(from_tensor, to_mask)\n\t\n    from_shape = get_shape_list(from_tensor, expected_rank=2)\n    to_shape = get_shape_list(to_mask, expected_rank=2)\n\n    if len(from_shape) != len(to_shape):\n        raise ValueError(\"The rank of `from_tensor` must match the rank of `to_mask`.\")\n\n    if len(from_shape) == 1:\n        raise ValueError(\"`from_tensor` must have rank 2.\")\n    if len(to_shape) == 1:\n        raise ValueError(\"`to_mask` must have rank 2.\")\n\n    to_mask = tf.cast(tf.reshape(to_mask, [-1]), tf.float32)\n\n    attention_mask = (tf.ones(from_shape) * to_mask)\n\n    return attention_mask\n\n", "CodeBleu_score": 27.96814955659937}, {"code": "def create_attention_mask_from_input_mask(from_tensor, to_mask)\n\t\n    from_shape = get_shape_list(from_tensor, expected_rank=2)\n    to_shape = get_shape_list(to_mask, expected_rank=2)\n    if len(from_shape) != len(to_shape):\n        raise ValueError(\"The rank of `from_tensor` must match the rank of `to_mask`.\")\n    if len(from_shape) == 1:\n        raise ValueError(\"The rank of `from_tensor` must be at least 1.\")\n    if (from_shape is None) or (to_shape is None):\n        raise ValueError(\"The shape of `from_tensor` and `to_mask` must be fully known.\")\n    # We do not support attention mask for the last token yet.\n    if to_mask is None:\n        attention_mask = None\n    else:\n        to_mask = tf.cast(tf.reshape(to_mask, [-1]), tf.float32)\n        attention_mask = (\n            tf.ones([from_shape[0], to_shape[1]], dtype=tf.float32) * to_mask\n        )\n    return attention_mask\n\n", "CodeBleu_score": 31.171871317342504}]}
{"instance_id": "d7798e41a723", "generation_result": [{"code": "def transpose_for_scores(input_tensor, batch_size, num_attention_heads,\n                           seq_length, width)\n\t\n    input_tensor = tf.reshape(\n        input_tensor, [batch_size, seq_length, num_attention_heads, width])\n    input_tensor = tf.transpose(input_tensor, [0, 2, 1, 3])\n    return input_tensor\n\n", "CodeBleu_score": 47.75921652627385}, {"code": "def transpose_for_scores(input_tensor, batch_size, num_attention_heads,\n                           seq_length, width)\n\t\n    input_tensor = tf.reshape(\n        input_tensor, [batch_size, seq_length, num_attention_heads, width])\n    input_tensor = tf.transpose(input_tensor, [0, 2, 1, 3])\n    return input_tensor\n\n", "CodeBleu_score": 47.75921652627385}, {"code": "def transpose_for_scores(input_tensor, batch_size, num_attention_heads,\n                           seq_length, width)\n\t\n    # Reshape the tensor to [batch_size, seq_length, num_attention_heads, width]\n    input_tensor = tf.reshape(\n        input_tensor, [batch_size, seq_length, num_attention_heads, width])\n    # Transpose the tensor to [batch_size, num_attention_heads, seq_length, width]\n    input_tensor = tf.transpose(input_tensor, [0, 2, 1, 3])\n    return input_tensor\n\n", "CodeBleu_score": 47.75921652627385}]}
{"instance_id": "711d466ace09", "generation_result": [{"code": "def reshape_to_matrix(input_tensor)\n\t\n    input_shape = input_tensor.get_shape()\n    if input_shape.ndims is not None:\n        if input_shape.ndims < 2:\n            raise ValueError(\"Reshape to matrix is possible only for tensors of rank 2 or higher. Input tensor has rank: %s\" % input_shape.ndims)\n        if input_shape.ndims == 2:\n            return input_tensor\n    else:\n        raise ValueError(\"Cannot infer rank of input tensor. Input has unknown rank.\")\n    width = input_shape[-1]\n    return tf.reshape(input_tensor, [-1, width])\n\n", "CodeBleu_score": 32.520307247622156}, {"code": "def reshape_to_matrix(input_tensor)\n\t\n    input_shape = input_tensor.get_shape().as_list()\n    if len(input_shape) < 2:\n        raise ValueError(\"Reshape to matrix is possible only for tensors of rank 2 or higher. Input tensor's rank: %d\" % len(input_shape))\n    if len(input_shape) == 2:\n        return input_tensor\n    else:\n        width = input_shape[-1]\n        return tf.reshape(input_tensor, [-1, width])\n\n", "CodeBleu_score": 35.329622086458926}, {"code": "def reshape_to_matrix(input_tensor)\n\t\n    input_shape = input_tensor.get_shape().as_list()\n    if len(input_shape) < 2:\n        raise ValueError(\"Input tensor must have rank of at least 2.\")\n    width = input_shape[-1]\n    output_tensor = tf.reshape(input_tensor, [-1, width])\n    return output_tensor\n\n", "CodeBleu_score": 50.06790139087771}]}
{"instance_id": "7adcd08f531e", "generation_result": [{"code": "def get_train_examples(self, data_dir)\n\t\n    examples = []\n    for (i, line) in enumerate(self._read_tsv(os.path.join(data_dir, \"train.tsv\"))):\n        guid = \"%s-%s\" % (self.name, i)\n        text_a = tokenization.convert_to_unicode(line[3])\n        text_b = tokenization.convert_to_unicode(line[4])\n        label = tokenization.convert_to_unicode(line[0])\n        if label == \"contradictory\":\n            label = \"contradiction\"\n        examples.append(\n            InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n    return examples\n\n", "CodeBleu_score": 48.3306486083808}, {"code": "def get_train_examples(self, data_dir)\n\t\n    examples = []\n    for (i, line) in enumerate(self._read_tsv(os.path.join(data_dir, \"train.tsv\"))):\n        if i == 0:\n            continue\n        guid = \"%s-%s\" % (set_type, line[0])\n        text_a = tokenization.convert_to_unicode(line[8])\n        text_b = tokenization.convert_to_unicode(line[9])\n        label = tokenization.convert_to_unicode(line[-1])\n        if label == \"contradictory\":\n            label = \"contradiction\"\n        examples.append(\n            InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n    return examples\n\n", "CodeBleu_score": 57.45983173312202}, {"code": "def get_train_examples(self, data_dir)\n\t\n    examples = []\n    for (i, line) in enumerate(self._read_tsv(os.path.join(data_dir, \"train.tsv\"))):\n        if i == 0:\n            continue\n        guid = \"%s-%s\" % (set_type, i)\n        text_a = tokenization.convert_to_unicode(line[3])\n        text_b = None\n        if set_type == \"train\":\n            label = tokenization.convert_to_unicode(line[1])\n            if label == \"contradictory\":\n                label = \"contradiction\"\n            examples.append(\n                InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n        else:\n            label = tokenization.convert_to_unicode(line[1])\n            examples.append(\n                InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n    return examples\n\n", "CodeBleu_score": 51.11961635682762}]}
{"instance_id": "26a0bb96dab9", "generation_result": [{"code": "def metric_fn(per_example_loss, label_ids, logits, is_real_example)\n\t\n    logits = tf.cast(logits, tf.float32)\n    label_ids = tf.cast(label_ids, tf.float32)\n    per_example_loss = tf.cast(per_example_loss, tf.float32)\n    is_real_example = tf.cast(is_real_example, tf.float32)\n    loss = tf.reduce_sum(per_example_loss * is_real_example) / tf.reduce_sum(is_real_example)\n    probabilities = tf.nn.softmax(logits, axis=-1)\n    predictions = tf.argmax(probabilities, axis=-1, output_type=tf.int32)\n    accuracy = tf.reduce_sum(tf.cast(tf.equal(predictions, label_ids), tf.float32) * is_real_example) / tf.reduce_sum(is_real_example)\n    return {\n        'eval_accuracy': accuracy,\n        'eval_loss': loss,\n    }\n\n", "CodeBleu_score": 31.948629608374517}, {"code": "def metric_fn(per_example_loss, label_ids, logits, is_real_example)\n\t\n    predictions = tf.argmax(logits, axis=-1, output_type=tf.int32)\n    accuracy = tf.metrics.accuracy(\n        labels=label_ids, predictions=predictions, weights=is_real_example)\n    mean_loss = tf.metrics.mean(values=per_example_loss, weights=is_real_example)\n    return {\n        \"eval_accuracy\": accuracy,\n        \"eval_loss\": mean_loss,\n    }\n\n", "CodeBleu_score": 59.49834653602287}, {"code": "def metric_fn(per_example_loss, label_ids, logits, is_real_example)\n\t\n    predictions = tf.argmax(logits, axis=-1, output_type=tf.int32)\n    accuracy = tf.reduce_sum(tf.cast(tf.equal(predictions, label_ids), tf.float32))\n    mask = tf.cast(is_real_example, dtype=tf.float32)\n    accuracy = tf.reduce_sum(accuracy * mask)\n    accuracy = tf.reduce_sum(accuracy)\n    mask = tf.cast(is_real_example, dtype=tf.float32)\n    per_example_loss = tf.reduce_sum(per_example_loss * mask)\n    loss = tf.reduce_sum(per_example_loss)\n    return {\n        \"eval_accuracy\": accuracy,\n        \"eval_loss\": loss,\n    }\n\n", "CodeBleu_score": 43.99120997836138}]}
{"instance_id": "6154e4d28509", "generation_result": [{"code": "def convert_examples_to_features(examples, label_list, max_seq_length,\n                                 tokenizer)\n\t\n    # Convert the examples to features\n    features = []\n    for (ex_index, example) in enumerate(examples):\n        if ex_index % 10000 == 0:\n            logger.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n\n        feature = convert_single_example(ex_index, example, label_list,\n                                         max_seq_length, tokenizer)\n        features.append(feature)\n    return features\n\n", "CodeBleu_score": 74.9165505721602}, {"code": "def convert_examples_to_features(examples, label_list, max_seq_length,\n                                 tokenizer)\n\t\n    # TODO: Implement\n    pass\n\n", "CodeBleu_score": 4.20265780730897}, {"code": "def convert_examples_to_features(examples, label_list, max_seq_length,\n                                 tokenizer)\n\t\n    # Convert the examples to features.\n    features = []\n    for (ex_index, example) in enumerate(examples):\n        if ex_index % 10000 == 0:\n            logger.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n\n        feature = convert_single_example(ex_index, example, label_list,\n                                         max_seq_length, tokenizer)\n\n        features.append(feature)\n    return features\n\n", "CodeBleu_score": 74.9165505721602}]}
{"instance_id": "e09c0db71ebb", "generation_result": [{"code": "def get_next_sentence_output(bert_config, input_tensor, labels)\n\t\n    input_tensor = gather_indexes(input_tensor, labels)\n    with tf.variable_scope(\"cls/seq_relationship\"):\n        output_weights = tf.get_variable(\n            \"output_weights\",\n            shape=[bert_config.hidden_size, 2],\n            initializer=tf.truncated_normal_initializer(stddev=0.02))\n        output_bias = tf.get_variable(\n            \"output_bias\", shape=[2], initializer=tf.zeros_initializer())\n    logits = tf.matmul(input_tensor, output_weights, transpose_b=True)\n    logits = tf.nn.bias_add(logits, output_bias)\n    log_probs = tf.nn.log_softmax(logits, axis=-1)\n    labels = tf.reshape(labels, [-1])\n    one_hot_labels = tf.one_hot(labels, depth=2, dtype=tf.float32)\n    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n    loss = tf.reduce_mean(per_example_loss)\n    return (loss, per_example_loss, log_probs)\n\n", "CodeBleu_score": 82.81364437608522}, {"code": "def get_next_sentence_output(bert_config, input_tensor, labels)\n\t\n    input_tensor = tf.reshape(input_tensor, [-1, bert_config.hidden_size])\n    output_weights = tf.get_variable(\n        \"output_weights\", [2, bert_config.hidden_size],\n        initializer=tf.truncated_normal_initializer(stddev=0.02))\n    output_bias = tf.get_variable(\n        \"output_bias\", [2], initializer=tf.zeros_initializer())\n    logits = tf.matmul(input_tensor, output_weights, transpose_b=True)\n    logits = tf.nn.bias_add(logits, output_bias)\n    log_probs = tf.nn.log_softmax(logits, axis=-1)\n    labels = tf.reshape(labels, [-1])\n    one_hot_labels = tf.one_hot(labels, depth=2, dtype=tf.float32)\n    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n    loss = tf.reduce_mean(per_example_loss)\n    return (loss, per_example_loss, log_probs)\n\n", "CodeBleu_score": 51.89716976519668}, {"code": "def get_next_sentence_output(bert_config, input_tensor, labels)\n\t\n    input_tensor = gather_indexes(input_tensor, labels)\n    with tf.variable_scope(\"cls/seq_relationship\"):\n        output_weights = tf.get_variable(\n            \"output_weights\",\n            shape=[2, bert_config.hidden_size],\n            initializer=tf.truncated_normal_initializer(stddev=0.02))\n        output_bias = tf.get_variable(\n            \"output_bias\", shape=[2], initializer=tf.zeros_initializer())\n    logits = tf.matmul(input_tensor, output_weights, transpose_b=True)\n    logits = tf.nn.bias_add(logits, output_bias)\n    log_probs = tf.nn.log_softmax(logits, axis=-1)\n    labels = tf.reshape(labels, [-1])\n    one_hot_labels = tf.one_hot(labels, depth=2, dtype=tf.float32)\n    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n    loss = tf.reduce_mean(per_example_loss)\n    return (loss, per_example_loss, log_probs)\n\n", "CodeBleu_score": 83.46307167572988}]}
{"instance_id": "1f66b020f485", "generation_result": [{"code": "def gather_indexes(sequence_tensor, positions)\n\t\n    sequence_shape = tf.shape(sequence_tensor)\n    batch_size = sequence_shape[0]\n    seq_length = sequence_shape[1]\n    flat_offsets = tf.range(batch_size) * seq_length\n    flat_positions = tf.reshape(positions + flat_offsets, [-1])\n    flat_sequence_tensor = tf.reshape(sequence_tensor, [batch_size * seq_length])\n    output_tensor = tf.gather(flat_sequence_tensor, flat_positions)\n    return output_tensor\n\n", "CodeBleu_score": 46.450872506110166}, {"code": "def gather_indexes(sequence_tensor, positions)\n\t\n    sequence_shape = tf.shape(sequence_tensor)\n    batch_size = sequence_shape[0]\n    seq_length = sequence_shape[1]\n    width = sequence_shape[2]\n    flat_offsets = tf.range(batch_size) * seq_length\n    flat_positions = tf.reshape(positions + flat_offsets, [-1])\n    flat_sequence_tensor = tf.reshape(sequence_tensor, [batch_size * seq_length, width])\n    output_tensor = tf.gather(flat_sequence_tensor, flat_positions)\n    return output_tensor\n\n", "CodeBleu_score": 55.815359188039714}, {"code": "def gather_indexes(sequence_tensor, positions)\n\t\n    sequence_shape = tf.shape(sequence_tensor)\n    batch_size = sequence_shape[0]\n    seq_length = sequence_shape[1]\n    width = sequence_shape[2]\n    flat_offsets = tf.range(batch_size) * seq_length\n    flat_positions = tf.reshape(positions + flat_offsets, [-1])\n    flat_sequence_tensor = tf.reshape(sequence_tensor, [batch_size * seq_length, width])\n    output_tensor = tf.gather(flat_sequence_tensor, flat_positions)\n    return output_tensor\n\n", "CodeBleu_score": 55.815359188039714}]}
{"instance_id": "956bffee9a33", "generation_result": [{"code": "def _decode_record(record, name_to_features)\n\t\n    example = tf.parse_single_example(record, name_to_features)\n    for name in list(example.keys()):\n        t = example[name]\n        if t.dtype == tf.int64:\n            t = tf.to_int32(t)\n        example[name] = t\n    return example\n\n", "CodeBleu_score": 71.32095143473128}, {"code": "def _decode_record(record, name_to_features)\n\t\n    example = tf.parse_single_example(record, name_to_features)\n    for name in list(example.keys()):\n        t = example[name]\n        if t.dtype == tf.int64:\n            t = tf.to_int32(t)\n        example[name] = t\n    return example\n\n", "CodeBleu_score": 71.32095143473128}, {"code": "def _decode_record(record, name_to_features)\n\t\n    example = tf.parse_single_example(record, name_to_features)\n    # tf.Example only supports tf.int64, but the TPU only supports tf.int32.\n    # So cast all int64 to int32.\n    for name in list(example.keys()):\n        t = example[name]\n        if t.dtype == tf.int64:\n            t = tf.to_int32(t)\n        example[name] = t\n    return example\n\n", "CodeBleu_score": 71.32095143473128}]}
{"instance_id": "6b97f354e641", "generation_result": [{"code": "def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer,\n                         orig_answer_text)\n\t\n    # The SQuAD annotations are character based. We first project them to\n    # whitespace-tokenized words. But then after WordPiece tokenization, we can\n    # often find a \"better match\". For example:\n    #\n    #   Question: What year was John Smith born?\n    #   Context: The leader was John Smith (1895-1943).\n    #   Answer: 1895\n    #\n    # The original whitespace-tokenized answer will be \"(1895-1943).\". However\n    # after tokenization, our tokens will be \"( 1895 - 1943 ) .\". So we can match\n    # the exact answer, 1895.\n    #\n    # However, this is not always possible. Consider the following:\n    #\n    #   Question: What country is the top exporter of electornics?\n    #   Context: The Japanese electronics industry is the lagest in the world.\n    #   Answer: Japan\n    #\n    # In this case, the annotator chose \"Japan\" as a character sub-span of\n    # the word \"Japanese\". Since our WordPiece tokenizer does not split\n    # \"Japanese\", we just use \"Japanese\" as the annotation. This is fairly rare\n    # in SQuAD, but does happen.\n    tok_answer_text = \" \".join(tokenizer.tokenize(orig_answer_text))\n\n    for new_start in range(input_start, input_end + 1):\n        for new_end in range(input_end, new_start - 1, -1):\n            text_span = \" \".join(doc_tokens[new_start:(new_end + 1)])\n            if text_span == tok_answer_text:\n                return new_start, new_end\n\n    return input_start, input_end\n\n", "CodeBleu_score": 70.32408211324}, {"code": "def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer,\n                         orig_answer_text)\n\t\n    # The SQuAD annotations are character based. We first project them to\n    # whitespace-tokenized words. But then after WordPiece tokenization, we can\n    # often find a \"better match\". For example:\n    #\n    #   Question: What year was John Smith born?\n    #   Context: The leader was John Smith (1895-1943).\n    #   Answer: 1895\n    #\n    # The original whitespace-tokenized answer will be \"(1895-1943).\". However\n    # after tokenization, our tokens will be \"( 1895 - 1943 ) .\". So we can match\n    # the exact answer, 1895.\n    #\n    # However, this is not always possible. Consider the following:\n    #\n    #   Question: What country is the top exporter of electornics?\n    #   Context: The Japanese electronics industry is the lagest in the world.\n    #   Answer: Japan\n    #\n    # In this case, the annotator chose \"Japan\" as a character sub-span of\n    # the word \"Japanese\". Since our WordPiece tokenizer does not split\n    # \"Japanese\", we just use \"Japanese\" as the annotation. This is fairly rare\n    # in SQuAD, but does happen.\n    tok_answer_text = \" \".join(tokenizer.tokenize(orig_answer_text))\n\n    for new_start in range(input_start, input_end + 1):\n        for new_end in range(input_end, new_start - 1, -1):\n            text_span = \" \".join(doc_tokens[new_start:(new_end + 1)])\n            if text_span == tok_answer_text:\n                return new_start, new_end\n\n    return input_start, input_end\n\n", "CodeBleu_score": 70.32408211324}, {"code": "def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer,\n                         orig_answer_text)\n\t\n    # The SQuAD annotations are character based. We first project them to\n    # whitespace-tokenized words. But then after WordPiece tokenization, we can\n    # often find a \"better match\". For example:\n    #\n    #   Question: What year was John Smith born?\n    #   Context: The leader was John Smith (1895-1943).\n    #   Answer: 1895\n    #\n    # The original whitespace-tokenized answer will be \"(1895-1943).\". However\n    # after tokenization, our tokens will be \"( 1895 - 1943 ) .\". So we can match\n    # the exact answer, 1895.\n    #\n    # However, this is not always possible. Consider the following:\n    #\n    #   Question: What country is the top exporter of electornics?\n    #   Context: The Japanese electronics industry is the lagest in the world.\n    #   Answer: Japan\n    #\n    # In this case, the annotator chose \"Japan\" as a character sub-span of\n    # the word \"Japanese\". Since our WordPiece tokenizer does not split\n    # \"Japanese\", we just use \"Japanese\" as the annotation. This is fairly rare\n    # in SQuAD, but does happen.\n    tok_answer_text = \" \".join(tokenizer.tokenize(orig_answer_text))\n\n    for new_start in range(input_start, input_end + 1):\n        for new_end in range(input_end, new_start - 1, -1):\n            text_span = \" \".join(doc_tokens[new_start:(new_end + 1)])\n            if text_span == tok_answer_text:\n                return new_start, new_end\n\n    return input_start, input_end\n\n", "CodeBleu_score": 70.32408211324}]}
{"instance_id": "aba334b92b34", "generation_result": [{"code": "def _check_is_max_context(doc_spans, cur_span_index, position)\n\t\n    # Maximum context is defined as the context that has the highest\n    # probability of occurring.\n    max_score = None\n    max_span_index = None\n    for (span_index, doc_span) in enumerate(doc_spans):\n        # We can approximate the max score for the current span\n        # by looking at the score of the previous span.\n        if span_index == 0:\n            continue\n        if not doc_span.is_impossible:\n            prev_score = doc_spans[span_index - 1].score\n            score = prev_score + min(1, doc_span.length - position) + \\\n                    (position + doc_span.length) * \\\n                    (1 - prev_score)\n            if max_score is None or score > max_score:\n                max_score = score\n                max_span_index = span_index\n\n    return max_span_index == cur_span_index\n\n", "CodeBleu_score": 40.50790781160361}, {"code": "def _check_is_max_context(doc_spans, cur_span_index, position)\n\t\n    # When there is exactly one span for a document, we get `doc_spans == 1` and `cur_span_index == 0`.\n    # When there are multiple document spans, we iterate over them and find the one with the highest score.\n    best_score = None\n    best_span_index = None\n    for (span_index, doc_span) in enumerate(doc_spans):\n        # For each span, we compute the context score.\n        # The context score is based on the fraction of tokens in the span that precede and follow the token at position.\n        # We compute the left and right context fraction by dividing the number of tokens in the span by the length of the span.\n        # We then compute the score for the span by taking the product of the left and right context fraction.\n        # The score is then the sum of the score for each token in the span.\n        span_score = 0.0\n        num_left_context = 0.0\n        num_right_context = 0.0\n        for token in doc_span.tokens:\n            if token.token_number >= position:\n                num_left_context += 1.0\n            else:\n                num_right_context += 1.0\n            span_score += min(num_left_context / doc_span.length, num_right_context / doc_span.length)\n        # If the span has a higher score than the previous highest score, we update the previous highest score and the span index.\n        if best_score is None or span_score > best_score:\n            best_score = span_score\n            best_span_index = span_index\n        # If the current span is the best one, we return True.\n    return cur_span_index == best_span_index\n\n", "CodeBleu_score": 46.7346249057407}, {"code": "def _check_is_max_context(doc_spans, cur_span_index, position)\n\t\n    max_score = None\n    max_span_index = None\n    for (span_index, doc_span) in enumerate(doc_spans):\n        end = doc_span[\"start\"] + doc_span[\"length\"] - 1\n        if position < doc_span[\"start\"]:\n            continue\n        if position > end:\n            continue\n        num_left_context = position - doc_span[\"start\"]\n        num_right_context = end - position\n        score = min(num_left_context, num_right_context) + 0.01 * doc_span[\"length\"]\n        if max_score is None or score > max_score:\n            max_score = score\n            max_span_index = span_index\n\n    return cur_span_index == max_span_index\n\n", "CodeBleu_score": 59.9202399831669}]}
{"instance_id": "cf92ad509f6a", "generation_result": [{"code": "def load_vocab(vocab_file)\n\t\n    vocab = collections.OrderedDict()\n    index = 0\n    with codecs.open(vocab_file, \"r\", \"utf-8\") as reader:\n        while True:\n            token = convert_to_unicode(reader.readline())\n            if not token:\n                break\n            token = token.strip()\n            vocab[token] = index\n            index += 1\n    return vocab\n\n", "CodeBleu_score": 66.71430380881995}, {"code": "def load_vocab(vocab_file)\n\t\n    vocab = collections.OrderedDict()\n    index = 0\n    with codecs.open(vocab_file, 'r', encoding='utf-8') as reader:\n        while True:\n            token = convert_to_unicode(reader.readline())\n            if not token:\n                break\n            token = token.strip()\n            vocab[token] = index\n            index += 1\n    return vocab\n\n", "CodeBleu_score": 66.12132296081141}, {"code": "def load_vocab(vocab_file)\n\t\n    vocab = collections.OrderedDict()\n    index = 0\n    with codecs.open(vocab_file, \"r\", \"utf-8\") as reader:\n        while True:\n            token = convert_to_unicode(reader.readline())\n            if not token:\n                break\n            token = token.strip()\n            vocab[token] = index\n            index += 1\n    return vocab\n", "CodeBleu_score": 66.71430380881995}]}
{"instance_id": "6c865803ac12", "generation_result": [{"code": "def convert_by_vocab(vocab, items)\n\t\n    return [vocab[item] for item in items]\n\n", "CodeBleu_score": 18.3448522644182}, {"code": "def convert_by_vocab(vocab, items)\n\t\n    output = []\n    for item in items:\n        output.append(vocab[item])\n    return output\n\n", "CodeBleu_score": 79.63749907165396}, {"code": "def convert_by_vocab(vocab, items)\n\t\n    output = []\n    for item in items:\n        output.append(vocab[item])\n    return output\n\n", "CodeBleu_score": 79.63749907165396}]}
{"instance_id": "e93b9200dc4a", "generation_result": [{"code": "def tokenize(self, text)\n\t\n    split_tokens = []\n    for token in self.basic_tokenizer.tokenize(text):\n        for sub_token in self.wordpiece_tokenizer.tokenize(token):\n            split_tokens.append(sub_token)\n    return split_tokens\n\n", "CodeBleu_score": 88.10762355418231}, {"code": "def tokenize(self, text)\n\t\n    split_tokens = []\n    for token in self.basic_tokenizer.tokenize(text):\n        for sub_token in self.wordpiece_tokenizer.tokenize(token):\n            split_tokens.append(sub_token)\n    return split_tokens\n\n", "CodeBleu_score": 88.10762355418231}, {"code": "def tokenize(self, text)\n\t\n    split_tokens = []\n    for token in self.basic_tokenizer.tokenize(text):\n        for sub_token in self.wordpiece_tokenizer.tokenize(token):\n            split_tokens.append(sub_token)\n    return split_tokens\n\n\n", "CodeBleu_score": 88.10762355418231}]}
{"instance_id": "2ff6f6db53c9", "generation_result": [{"code": "def safe_load(path, offset, duration, sample_rate, dtype)\n\t\n    logger.info(\"Loading audio data from %s\", path)\n    try:\n        audio_data = self.load(path, offset, duration, sample_rate, dtype)\n    except Exception as e:\n        logger.exception(e)\n        audio_data = -1.0\n        error = True\n    else:\n        error = False\n    return audio_data, error\n\n", "CodeBleu_score": 29.132400965992428}, {"code": "def safe_load(path, offset, duration, sample_rate, dtype)\n\t\n    logger.info(\"Loading audio data from file: %s\", path)\n    try:\n        data = self.load(path, offset, duration, sample_rate, dtype)\n        logger.info(\"Audio data loaded successfully.\")\n        return data, False\n    except Exception as e:\n        logger.exception(\"Failed to load audio data from file: %s\", path)\n        logger.error(\"Error: %s\", e)\n        return -1.0, True\n\n", "CodeBleu_score": 34.10283177077859}, {"code": "def safe_load(path, offset, duration, sample_rate, dtype)\n\t\n    try:\n        logger.info(f\"Loading audio data from {path} at offset {offset} for {duration} seconds with sample rate {sample_rate} and dtype {dtype}\")\n        audio_data = self.load(path, offset, duration, sample_rate, dtype)\n        return audio_data, False\n    except Exception as e:\n        logger.exception(f\"Exception occurred when loading audio data from {path} at offset {offset} for {duration} seconds with sample rate {sample_rate} and dtype {dtype}\")\n        return -1.0, True\n\n", "CodeBleu_score": 32.19640960681201}]}
{"instance_id": "12fbe1f03d8f", "generation_result": [{"code": "def to_n_channels(waveform: tf.Tensor, n_channels: int) -> tf.Tensor\n\t\n    waveform_shape = tf.shape(waveform)\n    if waveform_shape[-1] >= n_channels:\n        return tf.slice(waveform, [0, 0, 0], [-1, -1, n_channels])\n    else:\n        return tf.tile(waveform, [1, 1, n_channels])\n\n", "CodeBleu_score": 40.572555164900066}, {"code": "def to_n_channels(waveform: tf.Tensor, n_channels: int) -> tf.Tensor\n\t\n    if n_channels == 1:\n        return tf.squeeze(waveform, axis=-1)\n    elif n_channels == waveform.shape[-1]:\n        return waveform\n    elif n_channels > waveform.shape[-1]:\n        return tf.tile(waveform, [1, 1, n_channels])\n    elif n_channels < waveform.shape[-1]:\n        return tf.squeeze(waveform, axis=-1)\n\n", "CodeBleu_score": 44.54659757989971}, {"code": "def to_n_channels(waveform: tf.Tensor, n_channels: int) -> tf.Tensor\n\t\n    if n_channels < 1:\n        raise ValueError(\"The number of channels must be greater than 0.\")\n    if n_channels == 1:\n        return tf.squeeze(waveform, axis=-1)\n    shape = tf.shape(waveform)\n    if shape[-1] == n_channels:\n        return waveform\n    elif shape[-1] > n_channels:\n        return tf.squeeze(tf.slice(waveform, [0, 0], [shape[0], n_channels]), axis=-1)\n    else:\n        return tf.tile(tf.expand_dims(waveform, axis=-1), [1, 1, n_channels])\n\n", "CodeBleu_score": 42.72282536162309}]}
{"instance_id": "5e215f029b2e", "generation_result": [{"code": "def spectrogram_to_db_uint(\n    spectrogram: tf.Tensor, db_range: float = 100.0, **kwargs\n) -> tf.Tensor\n\t\n    # Convert the spectrogram to dB\n    dB_spectrogram = gain_to_db(spectrogram, **kwargs)\n\n    # Find the maximum dB value\n    max_dB = tf.reduce_max(dB_spectrogram)\n\n    # Clip the dB spectrogram to the specified range below the maximum\n    clipped_dB_spectrogram = tf.maximum(dB_spectrogram - max_dB + db_range, 0.0)\n\n    # Convert the clipped dB spectrogram from float32 to uint8 format\n    clipped_dB_spectrogram = tf.cast(\n        tf.round(tf.multiply(clipped_dB_spectrogram, 255.0 / db_range)), tf.uint8\n    )\n\n    return clipped_dB_spectrogram\n\n", "CodeBleu_score": 43.708513209722796}, {"code": "def spectrogram_to_db_uint(\n    spectrogram: tf.Tensor, db_range: float = 100.0, **kwargs\n) -> tf.Tensor\n\t\n    spectrogram_db = gain_to_db(spectrogram, **kwargs)\n    max_db_value = tf.reduce_max(spectrogram_db)\n    spectrogram_db = tf.maximum(spectrogram_db, max_db_value - db_range)\n    return from_float32_to_uint8(spectrogram_db)\n\n", "CodeBleu_score": 44.133585989569774}, {"code": "def spectrogram_to_db_uint(\n    spectrogram: tf.Tensor, db_range: float = 100.0, **kwargs\n) -> tf.Tensor\n\t\n    spectrogram_db = gain_to_db(spectrogram, **kwargs)\n    max_db = tf.reduce_max(spectrogram_db)\n    spectrogram_db = tf.clip_by_value(spectrogram_db, -db_range, max_db)\n    spectrogram_db = tf.maximum(spectrogram_db, -db_range)\n    spectrogram_db = tf.cast(spectrogram_db, tf.uint8)\n    return spectrogram_db\n\n", "CodeBleu_score": 42.92390254654641}]}
{"instance_id": "b65c0968dae7", "generation_result": [{"code": "def compute_spectrogram_tf(\n    waveform: tf.Tensor,\n    frame_length: int = 2048,\n    frame_step: int = 512,\n    spec_exponent: float = 1.0,\n    window_exponent: float = 1.0,\n) -> tf.Tensor\n\t\n    # TODO: Add support for window_exponent != 1.0\n    window_length = frame_length\n    window_step = frame_step\n    window = tf.signal.hann_window(window_length)\n    window = tf.pow(window, window_exponent)\n    stfts = tf.signal.stft(waveform, frame_length=frame_length, frame_step=frame_step, fft_length=frame_length, window_fn=lambda: window)\n    spectrogram = tf.abs(stfts)\n    spectrogram = tf.pow(spectrogram, spec_exponent)\n    spectrogram = tf.transpose(spectrogram, perm=[0, 2, 1])\n    return spectrogram\n\n", "CodeBleu_score": 36.717184657897626}, {"code": "def compute_spectrogram_tf(\n    waveform: tf.Tensor,\n    frame_length: int = 2048,\n    frame_step: int = 512,\n    spec_exponent: float = 1.0,\n    window_exponent: float = 1.0,\n) -> tf.Tensor\n\t\n    # Compute the spectrogram.\n    # waveform = tf.convert_to_tensor(waveform)\n    # waveform = tf.cast(waveform, tf.float32)\n    window_length = frame_length\n    window_length = tf.cast(window_length, tf.float32)\n    window = tf.abs(tf.signal.hann_window(window_length))\n    window = tf.cast(window, tf.float32)\n    window = tf.pow(window, window_exponent)\n    window = tf.expand_dims(window, axis=-1)\n    window = tf.tile(window, [1, frame_length])\n    stfts = tf.abs(tf.signal.stft(waveform, frame_length, frame_step, window_fn=window))\n    stfts = tf.transpose(stfts, [0, 2, 1])\n    stfts = tf.pow(stfts, spec_exponent)\n    return stfts\n\n", "CodeBleu_score": 37.269317846998426}, {"code": "def compute_spectrogram_tf(\n    waveform: tf.Tensor,\n    frame_length: int = 2048,\n    frame_step: int = 512,\n    spec_exponent: float = 1.0,\n    window_exponent: float = 1.0,\n) -> tf.Tensor\n\t\n    window_length = frame_length\n    window_length_half = window_length // 2\n    window_length_half_float = tf.cast(window_length_half, dtype=tf.float32)\n    window_length_float = tf.cast(window_length, dtype=tf.float32)\n    window = tf.cast(\n        tf.math.abs(\n            tf.signal.hann_window(window_length, dtype=tf.float32) ** window_exponent\n        ),\n        dtype=tf.float32,\n    )\n    window = tf.pad(window, [[0, window_length_half]])\n    window = tf.slice(window, [window_length_half], [window_length])\n    window = tf.concat(\n        [window, tf.zeros(window_length_half_float, dtype=tf.float32)], axis=0\n    )\n    window = tf.cast(window, dtype=tf.complex64)\n    window = tf.reshape(window, [1, window_length])\n    window = tf.tile(window, [tf.shape(waveform)[0], 1])\n    windowed_waveform = waveform * window\n    stfts = tf.signal.stft(\n        windowed_waveform,\n        frame_length=frame_length,\n        frame_step=frame_step,\n        fft_length=frame_length,\n    )\n    stfts = tf.abs(stfts)\n    stfts = tf.pow(stfts, spec_exponent)\n    stfts = tf.transpose(stfts, perm=[0, 2, 1])\n    return stfts\n\n", "CodeBleu_score": 37.049968641992585}]}
{"instance_id": "a4686140a634", "generation_result": [{"code": "def time_stretch(\n    spectrogram: tf.Tensor,\n    factor: float = 1.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n    # TODO: Implement the function\n    # TODO: Test the function\n    # TODO: Document the function\n    # TODO: Add unit tests\n    # TODO: Add integration tests\n    # TODO: Add documentation\n    # TODO: Add type hints\n    # TODO: Add return\n\n", "CodeBleu_score": 15.230481727574752}, {"code": "def time_stretch(\n    spectrogram: tf.Tensor,\n    factor: float = 1.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n    # Get shape of spectrogram\n    shape = tf.shape(spectrogram)\n    # Calculate new time dimension\n    new_time_dim = tf.cast(tf.cast(shape[1], tf.float32) * factor, tf.int32)\n    # Resize spectrogram\n    resized_spectrogram = tf.image.resize(\n        spectrogram,\n        [shape[0], new_time_dim],\n        method=method,\n        antialias=True,\n    )\n    # Adjust dimensions to original shape\n    resized_spectrogram = resize_image_with_crop_or_pad(\n        resized_spectrogram,\n        shape[0],\n        shape[1],\n    )\n    return resized_spectrogram\n\n", "CodeBleu_score": 36.10991550166297}, {"code": "def time_stretch(\n    spectrogram: tf.Tensor,\n    factor: float = 1.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n    # Calculate the new time dimension\n    time_dim = tf.cast(tf.shape(spectrogram)[1] * factor, tf.int32)\n\n    # Resize the spectrogram\n    spectrogram = tf.image.resize(\n        spectrogram,\n        [tf.shape(spectrogram)[0], time_dim],\n        method=method,\n    )\n\n    # Adjust the spectrogram to the original dimensions\n    spectrogram = resize_image_with_crop_or_pad(\n        spectrogram,\n        tf.shape(spectrogram)[0],\n        tf.shape(spectrogram)[1],\n        tf.shape(spectrogram)[0],\n        tf.shape(spectrogram)[1],\n    )\n\n    return spectrogram\n\n", "CodeBleu_score": 36.058597283547364}]}
{"instance_id": "6dd01101d4ae", "generation_result": [{"code": "def pitch_shift(\n    spectrogram: tf.Tensor,\n    semitone_shift: float = 0.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n\n    # Calculate the resizing factor\n    resizing_factor = 2 ** (semitone_shift / 12)\n\n    # Resize the spectrogram along the frequency axis\n    resized_spectrogram = tf.image.resize(\n        spectrogram,\n        tf.cast(\n            tf.shape(spectrogram)[:-1],\n            dtype=tf.int32,\n        ),\n        method=method,\n        antialias=True,\n    )\n\n    # Padding the resized spectrogram to maintain the original dimensions\n    padding_size = tf.cast(\n        tf.math.ceil(\n            tf.math.log2(resizing_factor)\n        ),\n        dtype=tf.int32,\n    )\n    padded_spectrogram = tf.pad(\n        resized_spectrogram,\n        tf.constant(\n            [[0, 0], [0, 0], [padding_size, padding_size]],\n            dtype=tf.int32,\n        ),\n        mode=\"CONSTANT\",\n    )\n\n    return padded_spectrogram\n\n", "CodeBleu_score": 31.43487578204419}, {"code": "def pitch_shift(\n    spectrogram: tf.Tensor,\n    semitone_shift: float = 0.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n    # Calculate the resizing factor\n    resizing_factor = 2.0 ** (semitone_shift / 12.0)\n    # Calculate the new size\n    new_size = tf.cast(tf.shape(spectrogram)[-2] * resizing_factor, tf.int32)\n    # Resize the spectrogram\n    spectrogram = tf.image.resize(\n        spectrogram, [new_size, tf.shape(spectrogram)[-1]], method=method\n    )\n    # Pad the resized spectrogram\n    spectrogram = tf.pad(\n        spectrogram,\n        paddings=[[0, 0], [0, tf.shape(spectrogram)[-2] - tf.shape(spectrogram)[-1]]],\n    )\n    return spectrogram\n\n", "CodeBleu_score": 34.259737250900024}, {"code": "def pitch_shift(\n    spectrogram: tf.Tensor,\n    semitone_shift: float = 0.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n    # Calculate the resizing factor\n    resizing_factor = 2 ** (semitone_shift / 12)\n    # Resize the spectrogram\n    resized_spectrogram = tf.image.resize(\n        spectrogram,\n        tf.shape(spectrogram)[:2] * resizing_factor,\n        method=method,\n    )\n    # Pad the resized spectrogram\n    padded_resized_spectrogram = tf.pad(\n        resized_spectrogram,\n        tf.constant([[0, 0], [0, 0], [0, 1], [0, 0]]),\n        constant_values=0,\n    )\n    return padded_resized_spectrogram\n\n", "CodeBleu_score": 32.00482202657335}]}
{"instance_id": "612f0826e56a", "generation_result": [{"code": "def _get_conv_activation_layer(params: Dict) -> Any\n\t\n    if params.get(\"conv_activation\") == \"ReLU\":\n        return nn.ReLU()\n    elif params.get(\"conv_activation\") == \"ELU\":\n        return nn.ELU()\n    elif params.get(\"conv_activation\") == \"LeakyReLU\":\n        return nn.LeakyReLU(0.2)\n    else:\n        return nn.ReLU()\n\n", "CodeBleu_score": 40.99222635914421}, {"code": "def _get_conv_activation_layer(params: Dict) -> Any\n\t\n    if params.get(\"conv_activation\") == \"ReLU\":\n        return nn.ReLU()\n    elif params.get(\"conv_activation\") == \"ELU\":\n        return nn.ELU()\n    elif params.get(\"conv_activation\") == \"LeakyReLU\":\n        return nn.LeakyReLU(0.2)\n    else:\n        return nn.ReLU()\n\n", "CodeBleu_score": 40.99222635914421}, {"code": "def _get_conv_activation_layer(params: Dict) -> Any\n\t\n    if params.get(\"conv_activation\") == \"ReLU\":\n        return nn.ReLU()\n    elif params.get(\"conv_activation\") == \"ELU\":\n        return nn.ELU()\n    elif params.get(\"conv_activation\") == \"LeakyReLU\":\n        return nn.LeakyReLU(negative_slope=0.2)\n    else:\n        return nn.ReLU()\n\n", "CodeBleu_score": 39.48424703401421}]}
{"instance_id": "2f8b5a2fc0f2", "generation_result": [{"code": "def softmax_unet(\n    input_tensor: tf.Tensor, instruments: Iterable[str], params: Dict = {}\n) -> Dict\n\t\n    logit_mask_list = []\n    for instrument in instruments:\n        logit_mask = apply_unet(input_tensor, instrument, params)\n        logit_mask_list.append(logit_mask)\n    softmax_mask = tf.stack(logit_mask_list, axis=0)\n    softmax_mask = tf.nn.softmax(softmax_mask, axis=1)\n    output_dict = {}\n    for instrument, logit_mask in zip(instruments, logit_mask_list):\n        output_dict[instrument] = tf.multiply(input_tensor, logit_mask)\n    return output_dict\n\n", "CodeBleu_score": 32.372070345122374}, {"code": "def softmax_unet(\n    input_tensor: tf.Tensor, instruments: Iterable[str], params: Dict = {}\n) -> Dict\n\t\n\n    logit_mask_list = []\n    for instrument in instruments:\n        logit_mask = apply_unet(\n            input_tensor,\n            instrument,\n            params,\n            apply_softmax=True,\n            softmax_axis=0,\n        )\n        logit_mask_list.append(logit_mask)\n\n    softmax_mask = tf.stack(logit_mask_list, axis=0)\n    softmax_mask = tf.math.softmax(softmax_mask, axis=0)\n\n    spectrograms = {}\n    for i, instrument in enumerate(instruments):\n        spectrograms[instrument] = tf.multiply(input_tensor, softmax_mask[i])\n\n    return spectrograms\n\n", "CodeBleu_score": 39.817647681158284}, {"code": "def softmax_unet(\n    input_tensor: tf.Tensor, instruments: Iterable[str], params: Dict = {}\n) -> Dict\n\t\n\n    logit_mask_list = []\n\n    for instrument in instruments:\n        logit_mask = apply_unet(input_tensor, instrument, params)\n        logit_mask_list.append(logit_mask)\n\n    logit_mask_list = tf.stack(logit_mask_list, axis=0)\n\n    logit_mask_list = softmax(logit_mask_list, axis=0)\n\n    output_dict = {}\n\n    for instrument, logit_mask in zip(instruments, logit_mask_list):\n        output_dict[instrument] = multiply(input_tensor, logit_mask)\n\n    return output_dict\n\n", "CodeBleu_score": 32.32426160265589}]}
{"instance_id": "400b19ce8520", "generation_result": [{"code": "def compute_file_checksum(path)\n\t\n    sha256 = hashlib.sha256()\n    with open(path, \"rb\") as f:\n        for chunk in iter(lambda: f.read(4096), b\"\"):\n            sha256.update(chunk)\n    return sha256.hexdigest()\n\n", "CodeBleu_score": 61.71149493320378}, {"code": "def compute_file_checksum(path)\n\t\n    # Initialize a SHA-256 hash object\n    sha256_hash = hashlib.sha256()\n    # Open the file for reading in binary mode\n    with open(path, \"rb\") as file:\n        # Read the file in chunks\n        for chunk in iter(lambda: file.read(4096), b\"\"):\n            # Update the hash object with the chunk\n            sha256_hash.update(chunk)\n    # Return the hexadecimal representation of the hash\n    return sha256_hash.hexdigest()\n\n", "CodeBleu_score": 56.84150002750917}, {"code": "def compute_file_checksum(path)\n\t\n    sha256_hash = hashlib.sha256()\n    with open(path, 'rb') as f:\n        for chunk in iter(lambda: f.read(4096), b\"\"):\n            sha256_hash.update(chunk)\n    return sha256_hash.hexdigest()\n\n", "CodeBleu_score": 52.523643700929966}]}
{"instance_id": "5f2e9c6e15f1", "generation_result": [{"code": "def checksum(self, name: str) -> str\n\t\n    url = f\"{self.host}/{self.repository}/{self.release_path}/{self.checksum_index}\"\n    response = httpx.get(url)\n    response.raise_for_status()\n    index = response.json()\n    if name not in index:\n        raise ValueError(f\"Model name {name} is not found in the checksum index.\")\n    return index[name]\n\n", "CodeBleu_score": 37.038455211262104}, {"code": "def checksum(self, name: str) -> str\n\t\n    url = f\"{self.host}/{self.repository}/{self.release_path}/{self.checksum_index}\"\n    response = httpx.get(url)\n    response.raise_for_status()\n    data = response.json()\n    if name not in data:\n        raise ValueError(f\"Model {name} not found.\")\n    return data[name]\n\n", "CodeBleu_score": 33.484792878192806}, {"code": "def checksum(self, name: str) -> str\n\t\n    url = self._host + self._repository + self._release_path + self._checksum_index\n    response = httpx.get(url)\n    response.raise_for_status()\n    response_json = response.json()\n    if name not in response_json:\n        raise ValueError(f\"Model name {name} not found in checksum index.\")\n    return response_json[name]\n\n", "CodeBleu_score": 34.711713894861916}]}
{"instance_id": "9312c8b6032e", "generation_result": [{"code": "def download(self, name: str, path: str) -> None:\n\t\n    logger.info(f\"Downloading {name} model to {path}\")\n    url = self._get_url(name)\n    stream = client.stream(\"GET\", url, headers=self.headers)\n    stream.raise_for_status()\n    with NamedTemporaryFile() as temp_file:\n        for chunk in stream.iter_raw():\n            temp_file.write(chunk)\n        temp_file.flush()\n        self.checksum = self._get_checksum(temp_file.name)\n        self.checksum == self._get_expected_checksum(name) or raise IOError(\"Checksum mismatch\")\n        tar.open(temp_file.name).extractall(path)\n    logger.info(f\"Finished downloading {name} model\")\n\n", "CodeBleu_score": 32.71082737867334}, {"code": "def download(self, name: str, path: str) -> None:\n\t\n    url = self.get_download_url(name)\n    logger.info(f\"Downloading {name} from {url}\")\n    with httpx.Client(http2=True) as client:\n        with client.stream(\"GET\", url) as response:\n            response.raise_for_status()\n            with NamedTemporaryFile() as file:\n                for chunk in response.iter_raw():\n                    file.write(chunk)\n                file.flush()\n                self.checksum(file.name)\n                with tarfile.open(file.name) as tar:\n                    tar.extractall(path=path)\n                os.unlink(file.name)\n    logger.info(f\"Extracted {name} to {path}\")\n\n", "CodeBleu_score": 38.92953063644922}, {"code": "def download(self, name: str, path: str) -> None:\n\t\n    url = self._get_url(name)\n    logger.info(f\"Downloading {name} from {url}\")\n    with tempfile.NamedTemporaryFile() as temp_file:\n        with httpx.Client(http2=True) as client:\n            stream = client.stream(\"GET\", url)\n            for chunk in stream:\n                temp_file.write(chunk)\n        compute_file_checksum(temp_file.name)\n        with tarfile.open(temp_file.name, \"r:gz\") as tar:\n            tar.extractall(path)\n        os.unlink(temp_file.name)\n    logger.info(f\"Extracted {name} to {path}\")\n\n", "CodeBleu_score": 31.26394464543053}]}
{"instance_id": "ee9493f37d77", "generation_result": [{"code": "def load_configuration(descriptor: str) -> Dict\n\t\n    if descriptor.startswith('resource:'):\n        name = descriptor[9:]\n        try:\n            with loader.open_text(name) as f:\n                return json.load(f)\n        except FileNotFoundError:\n            raise SpleeterError(f'Resource {name} not found.')\n    elif loader.is_resource(descriptor):\n        raise SpleeterError('Embedded configuration not supported.')\n    elif loader.exists(descriptor):\n        with open(descriptor) as f:\n            return json.load(f)\n    else:\n        raise SpleeterError(f'File {descriptor} not found.')\n\n", "CodeBleu_score": 49.1390753898325}, {"code": "def load_configuration(descriptor: str) -> Dict\n\t\n    if descriptor.startswith(\"embedded:\"):\n        name = descriptor[9:]\n        with loader.open_text(\"embedded\", name) as f:\n            return json.load(f)\n    elif loader.is_resource(descriptor):\n        with loader.open_text(descriptor) as f:\n            return json.load(f)\n    else:\n        if not os.path.exists(descriptor):\n            raise SpleeterError(\"Configuration file not found: %s\" % descriptor)\n        with open(descriptor) as f:\n            return json.load(f)\n\n", "CodeBleu_score": 43.92987531654483}, {"code": "def load_configuration(descriptor: str) -> Dict\n\t\n    if descriptor.startswith(\"embedded:\"):\n        name = descriptor[len(\"embedded:\"):]\n        if loader.is_resource(name):\n            return json.load(loader.open_text(name))\n        raise SpleeterError(f\"Embedded configuration {name} not found\")\n    if loader.is_resource(descriptor):\n        return json.load(loader.open_text(descriptor))\n    if loader.exists(descriptor):\n        with open(descriptor) as f:\n            return json.load(f)\n    raise SpleeterError(f\"Configuration file {descriptor} not found\")\n\n", "CodeBleu_score": 57.2567714308256}]}
{"instance_id": "546134b67fbc", "generation_result": [{"code": "def sync_apply(\n    tensor_dict: Dict[str, tf.Tensor], func: Callable, concat_axis: int = 1\n) -> Dict[str, tf.Tensor]\n\t\n    if concat_axis not in [0, 1]:\n        raise NotImplementedError(\n            \"The function can only concatenate along the 0th or 1st axis.\"\n        )\n    tensor_list = list(tensor_dict.values())\n    if concat_axis == 0:\n        tensor_concat = tf.concat(tensor_list, axis=concat_axis)\n    else:\n        tensor_concat = tf.concat(tensor_list, axis=concat_axis)\n    tensor_processed = func(tensor_concat)\n    if concat_axis == 0:\n        tensor_dict_processed = {\n            key: tensor_processed[i] for i, key in enumerate(tensor_dict.keys())\n        }\n    else:\n        tensor_dict_processed = {\n            key: tensor_processed[i]\n            for i, key in enumerate(tensor_dict.keys())\n        }\n    return tensor_dict_processed\n\n", "CodeBleu_score": 48.88202082366332}, {"code": "def sync_apply(\n    tensor_dict: Dict[str, tf.Tensor], func: Callable, concat_axis: int = 1\n) -> Dict[str, tf.Tensor]\n\t\n    if concat_axis not in (0, 1):\n        raise NotImplementedError(\n            f\"The concatenation axis must be either 0 or 1, not {concat_axis}.\"\n        )\n    if concat_axis == 1:\n        tensor_dict = {\n            key: tf.transpose(tensor_dict[key], [1, 0, 2])\n            for key in tensor_dict.keys()\n        }\n    tensor_list = [tensor_dict[key] for key in tensor_dict.keys()]\n    tensor = tf.concat(tensor_list, axis=concat_axis)\n    tensor = func(tensor)\n    if concat_axis == 1:\n        tensor = tf.transpose(tensor, [1, 0, 2])\n    tensor_dict = {}\n    for key in tensor_dict.keys():\n        tensor_dict[key] = tf.split(tensor, tensor_dict[key].shape[0], axis=concat_axis)\n    return tensor_dict\n\n", "CodeBleu_score": 43.16057509077819}, {"code": "def sync_apply(\n    tensor_dict: Dict[str, tf.Tensor], func: Callable, concat_axis: int = 1\n) -> Dict[str, tf.Tensor]\n\t\n    if concat_axis not in [0, 1]:\n        raise NotImplementedError\n\n    def _sync_apply(tensor_dict, func, concat_axis):\n        if concat_axis == 0:\n            return {\n                key: func(tf.concat(list(tensor_dict.values()), axis=concat_axis))\n                for key in tensor_dict.keys()\n            }\n        else:\n            return {\n                key: func(\n                    tf.concat(\n                        [tf.expand_dims(tensor, axis=0) for tensor in tensor_dict.values()],\n                        axis=concat_axis,\n                    )\n                )\n                for key in tensor_dict.keys()\n            }\n\n    return _sync_apply(tensor_dict, func, concat_axis)\n\n", "CodeBleu_score": 36.81809313757943}]}
{"instance_id": "df48e78d46d2", "generation_result": [{"code": "def from_float32_to_uint8(\n    tensor: tf.Tensor,\n    tensor_key: str = \"tensor\",\n    min_key: str = \"min\",\n    max_key: str = \"max\",\n) -> tf.Tensor\n\t\n    min_value = tf.reduce_min(tensor)\n    max_value = tf.reduce_max(tensor)\n    scaled_tensor = tf.cast(\n        tf.cast(\n            tf.clip_by_value(tensor, min_value, max_value), tf.float32\n        )\n        * 255,\n        tf.uint8,\n    )\n    return {\n        tensor_key: scaled_tensor,\n        min_key: min_value,\n        max_key: max_value,\n    }\n\n", "CodeBleu_score": 41.89092598441748}, {"code": "def from_float32_to_uint8(\n    tensor: tf.Tensor,\n    tensor_key: str = \"tensor\",\n    min_key: str = \"min\",\n    max_key: str = \"max\",\n) -> tf.Tensor\n\t\n    min_value = tf.reduce_min(tensor)\n    max_value = tf.reduce_max(tensor)\n    scaled_tensor = tf.cast(\n        tf.round(\n            (tf.cast(tensor, tf.float32) - min_value)\n            * (255.0 / (max_value - min_value))\n        ),\n        tf.uint8,\n    )\n    return {\n        tensor_key: scaled_tensor,\n        min_key: min_value,\n        max_key: max_value,\n    }\n\n", "CodeBleu_score": 43.372898757280375}, {"code": "def from_float32_to_uint8(\n    tensor: tf.Tensor,\n    tensor_key: str = \"tensor\",\n    min_key: str = \"min\",\n    max_key: str = \"max\",\n) -> tf.Tensor\n\t\n    min_value = tf.reduce_min(tensor)\n    max_value = tf.reduce_max(tensor)\n    scaled_tensor = tf.cast(\n        tf.round(\n            (tf.cast(tensor, tf.float32) - min_value)\n            * (255.0 / (max_value - min_value))\n        ),\n        tf.uint8,\n    )\n    return {\n        tensor_key: scaled_tensor,\n        min_key: min_value,\n        max_key: max_value,\n    }\n\n", "CodeBleu_score": 43.372898757280375}]}
{"instance_id": "7f479e5de492", "generation_result": [{"code": "def pad_and_partition(tensor: tf.Tensor, segment_len: int) -> tf.Tensor\n\t\n    # Calculate the padding needed to make the tensor length a multiple of the segment length\n    pad_len = tf.math.floormod(tf.shape(tensor)[0], segment_len)\n    pad_len = tf.cond(tf.equal(pad_len, 0), lambda: tf.constant(segment_len), lambda: tf.constant(segment_len - pad_len))\n    # Pad the tensor\n    padded_tensor = tf.pad(tensor, [[0, pad_len], [0, 0]])\n    # Reshape the tensor into segments\n    return tf.reshape(padded_tensor, [-1, segment_len])\n\n", "CodeBleu_score": 30.01865661843008}, {"code": "def pad_and_partition(tensor: tf.Tensor, segment_len: int) -> tf.Tensor\n\t\n    # Calculate the number of segments\n    num_segments = tf.math.ceil(tf.shape(tensor)[0] / segment_len)\n    # Calculate the padding needed to make the tensor length a multiple of the segment length\n    padding = tf.math.floormod(num_segments * segment_len - tf.shape(tensor)[0], segment_len)\n    # Pad the tensor\n    padded_tensor = tf.pad(tensor, [[0, padding]])\n    # Reshape the tensor\n    reshaped_tensor = tf.reshape(padded_tensor, [num_segments, segment_len])\n    # Return the reshaped tensor\n    return reshaped_tensor\n\n", "CodeBleu_score": 31.36667796212252}, {"code": "def pad_and_partition(tensor: tf.Tensor, segment_len: int) -> tf.Tensor\n\t\n    # Calculate the padding needed to make the tensor length a multiple of the segment length\n    padding = tf.math.floormod(tf.shape(tensor)[0], segment_len)\n    # Pad the tensor\n    tensor = tf.pad(tensor, [[0, padding]])\n    # Reshape the tensor into a new tensor with the specified segment length\n    tensor = tf.reshape(tensor, [tf.shape(tensor)[0] // segment_len, segment_len])\n    return tensor\n\n", "CodeBleu_score": 25.287771468360436}]}
{"instance_id": "1789fae7253f", "generation_result": [{"code": "def pad_and_reshape(instr_spec, frame_length, F) -> Any\n\t\n    num_rows_to_pad = frame_length - instr_spec.shape[0]\n    num_cols_to_pad = frame_length - instr_spec.shape[1]\n    instr_spec_padded = tf.pad(instr_spec, [[0, num_rows_to_pad], [0, num_cols_to_pad]])\n    instr_spec_padded = tf.reshape(instr_spec_padded, [-1, frame_length * frame_length])\n    instr_spec_padded = tf.tile(instr_spec_padded, [F, 1])\n    instr_spec_padded = tf.reshape(instr_spec_padded, [F, frame_length, frame_length])\n    return instr_spec_padded\n\n", "CodeBleu_score": 17.750236964574718}, {"code": "def pad_and_reshape(instr_spec, frame_length, F) -> Any\n\t\n    num_rows_to_pad = frame_length - instr_spec.shape[0]\n    num_cols_to_pad = frame_length - instr_spec.shape[1]\n    instr_spec_padded = tf.pad(instr_spec, [[0, num_rows_to_pad], [0, num_cols_to_pad]])\n    instr_spec_extended = tf.tile(instr_spec_padded, [1, 1, F])\n    instr_spec_reshaped = tf.reshape(instr_spec_extended, [frame_length * F, frame_length])\n    return instr_spec_reshaped\n\n", "CodeBleu_score": 16.422748513402034}, {"code": "def pad_and_reshape(instr_spec, frame_length, F) -> Any\n\t\n    pad_rows = tf.math.ceil(frame_length / instr_spec.shape[0])\n    pad_rows = tf.cast(pad_rows, tf.int32)\n    pad_rows = tf.math.ceil(pad_rows)\n    pad_rows = tf.cast(pad_rows, tf.int32)\n    pad_rows = tf.math.ceil(pad_rows)\n    pad_rows = tf.cast(pad_rows, tf.int32)\n    pad_rows = tf.math.ceil(pad_rows)\n    pad_rows = tf.cast(pad_rows, tf.int32)\n    pad_rows = tf.math.ceil(pad_rows)\n    pad_rows = tf.cast(pad_rows, tf.int32)\n    pad_rows = tf.math.ceil(pad_rows)\n    pad_rows = tf.cast(pad_rows, tf.int32)\n    pad_rows = tf.math.ceil(pad_rows)\n    pad_rows = tf.cast(pad_rows, tf.int32)\n    pad_rows = tf.math.ceil(pad_rows)\n    pad_rows = tf.cast(pad_rows, tf.int32)\n    pad_rows = tf.math.ceil(pad_rows)\n    pad_rows = tf.cast(pad_rows, tf.int32)\n    pad_rows = tf.math.ceil(pad_rows)\n    pad_rows = tf.cast(pad_rows, tf.int32)\n    pad_rows = tf.math.ceil(pad_rows)\n    pad_rows = tf.cast(pad_rows, tf.int32)\n    pad_rows = tf.math.ceil(pad_rows)\n    pad_rows = tf.cast(pad_rows, tf.int32)\n    pad_rows = tf.math.ceil(pad_rows)\n    pad_rows = tf.cast(pad_rows, tf.int32)\n    pad_rows = tf.math.ceil(pad_rows)\n    pad_rows = tf.cast(pad_rows, tf.int32", "CodeBleu_score": 11.339351407722704}]}
{"instance_id": "8a4f4658d97c", "generation_result": [{"code": "def dataset_from_csv(csv_path: str, **kwargs) -> Any\n\t\n    df = pd.read_csv(csv_path, **kwargs)\n    return from_tensor_slices(dict(df))\n\n", "CodeBleu_score": 23.327444845428477}, {"code": "def dataset_from_csv(csv_path: str, **kwargs) -> Any\n\t\n    df = pd.read_csv(csv_path, **kwargs)\n    return tf.data.Dataset.from_tensor_slices(dict(df))\n\n", "CodeBleu_score": 27.127655353502654}, {"code": "def dataset_from_csv(csv_path: str, **kwargs) -> Any\n\t\n    df = pd.read_csv(csv_path, **kwargs)\n    return from_tensor_slices(dict(zip(df.columns, df.to_numpy())))\n\n", "CodeBleu_score": 27.574466621221728}]}
{"instance_id": "9a16e81b182f", "generation_result": [{"code": "def check_tensor_shape(tensor_tf: tf.Tensor, target_shape: Any) -> bool\n\t\n    result = True\n    for target_dim, tensor_dim in zip(target_shape, tensor_tf.shape):\n        if target_dim is not None and target_dim != tensor_dim:\n            result = tf.logical_and(result, tf.equal(tensor_dim, target_dim))\n    return tf.logical_and(result, tf.equal(tf.shape(tensor_tf), target_shape))\n\n", "CodeBleu_score": 37.01316919440294}, {"code": "def check_tensor_shape(tensor_tf: tf.Tensor, target_shape: Any) -> bool\n\t\n    result = True\n    for target_dim_length, tensor_dim_length in zip(target_shape, tensor_tf.shape):\n        if target_dim_length is not None:\n            result = tf.logical_and(result, tf.equal(tensor_dim_length, target_dim_length))\n    return result\n\n", "CodeBleu_score": 35.09177107180591}, {"code": "def check_tensor_shape(tensor_tf: tf.Tensor, target_shape: Any) -> bool\n\t\n    result = True\n    for dim_index, dim_length in enumerate(target_shape):\n        if dim_length is not None:\n            result = tf.logical_and(result, tf.equal(tf.shape(tensor_tf)[dim_index], dim_length))\n    return result\n\n", "CodeBleu_score": 43.53889663038512}]}
{"instance_id": "49d5d3084b3a", "generation_result": [{"code": "def get_training_dataset(\n    audio_params: Dict, audio_adapter: AudioAdapter, audio_path: str\n) -> Any\n\t\n    dataset_builder = DatasetBuilder(\n        audio_params, audio_adapter, audio_path, \"train\", random_seed=0\n    )\n    dataset_builder.build(\n        train_csv=\"train.csv\",\n        cache_dir=\"cache\",\n        batch_size=8,\n        num_chunks_per_song=10,\n        augment_options=[\n            \"pitch_shift\",\n            \"time_stretch\",\n            \"speed\",\n            \"tempo\",\n            \"reverb\",\n            \"noise\",\n            \"harmonic_distortion\",\n            \"gain\",\n        ],\n        cache_options=[\"cache_audio\", \"cache_features\"],\n    )\n    return dataset_builder.dataset\n\n", "CodeBleu_score": 36.42695696080992}, {"code": "def get_training_dataset(\n    audio_params: Dict, audio_adapter: AudioAdapter, audio_path: str\n) -> Any\n\t\n    dataset_builder = DatasetBuilder(\n        audio_params=audio_params,\n        audio_adapter=audio_adapter,\n        audio_path=audio_path,\n        chunk_duration=audio_params[\"chunk_duration\"],\n        random_seed=audio_params[\"random_seed\"],\n    )\n    return dataset_builder.build(\n        train_csv=audio_params[\"train_csv\"],\n        cache_dir=audio_params[\"cache_dir\"],\n        batch_size=audio_params[\"batch_size\"],\n        num_chunks_per_song=audio_params[\"num_chunks_per_song\"],\n        augment=audio_params[\"augment\"],\n        cache=audio_params[\"cache\"],\n    )\n\n", "CodeBleu_score": 33.50810144914551}, {"code": "def get_training_dataset(\n    audio_params: Dict, audio_adapter: AudioAdapter, audio_path: str\n) -> Any\n\t\n    builder = DatasetBuilder(\n        audio_params=audio_params,\n        audio_adapter=audio_adapter,\n        chunk_duration=audio_params[\"chunk_duration\"],\n        random_seed=audio_params[\"random_seed\"],\n    )\n    builder.build(\n        train_csv=audio_path + \"/train.csv\",\n        cache_dir=audio_path + \"/cache\",\n        batch_size=audio_params[\"batch_size\"],\n        num_chunks_per_song=audio_params[\"num_chunks_per_song\"],\n        augment=audio_params[\"augment\"],\n        cache=audio_params[\"cache\"],\n    )\n    return builder.dataset\n\n", "CodeBleu_score": 35.85913253212647}]}
{"instance_id": "fe8bbbba06e3", "generation_result": [{"code": "def instruments(self) -> Any\n\t\n    if not self._instrument_dataset_builders:\n        for instrument in self._instruments:\n            self._instrument_dataset_builders.append(\n                InstrumentDatasetBuilder(instrument, self._start_date, self._end_date, self._data_frequency))\n    for instrument_dataset_builder in self._instrument_dataset_builders:\n        yield instrument_dataset_builder\n\n", "CodeBleu_score": 42.4515287088025}, {"code": "def instruments(self) -> Any\n\t\n    if self._instrument_dataset_builders is None:\n        self._instrument_dataset_builders = []\n        for instrument in self._instruments:\n            builder = InstrumentDatasetBuilder(self, instrument)\n            self._instrument_dataset_builders.append(builder)\n    for builder in self._instrument_dataset_builders:\n        yield builder\n\n", "CodeBleu_score": 55.64255380448454}, {"code": "def instruments(self) -> Any\n\t\n    if self.instrument_dataset_builders is None:\n        self.instrument_dataset_builders = []\n        for instrument in self.instruments:\n            instrument_dataset_builder = InstrumentDatasetBuilder(instrument, self.data_directory, self.data_files)\n            self.instrument_dataset_builders.append(instrument_dataset_builder)\n    for instrument_dataset_builder in self.instrument_dataset_builders:\n        yield instrument_dataset_builder\n\n", "CodeBleu_score": 42.63069846565507}]}
{"instance_id": "438b57fd3413", "generation_result": [{"code": "def cache(self, dataset: Any, cache: Optional[str], wait: bool) -> Any\n\t\n    if not cache:\n        return dataset\n    cache = Path(cache)\n    cache.mkdir(parents=True, exist_ok=True)\n    cache_path = cache / f\"{self.name}.cache\"\n    if cache_path.exists():\n        logger.info(f\"Cache found for {self.name}, loading...\")\n        dataset = self.load_cache(cache_path)\n        return dataset\n    if wait:\n        while not cache_path.exists():\n            logger.info(f\"Waiting for cache to be created for {self.name}...\")\n            time.sleep(1)\n    logger.info(f\"Caching {self.name}...\")\n    self.save_cache(dataset, cache_path)\n    return dataset\n\n", "CodeBleu_score": 44.36293494576996}, {"code": "def cache(self, dataset: Any, cache: Optional[str], wait: bool) -> Any\n\t\n    if cache is not None:\n        cache = Path(cache)\n        if cache.is_dir():\n            if wait:\n                while not (cache / \"index.json\").exists():\n                    logger.info(f\"Waiting for cache index file to be created at {cache}\")\n                    time.sleep(1)\n            return self.cache_dataset(dataset, cache)\n        else:\n            os.makedirs(cache, exist_ok=True)\n            return self.cache_dataset(dataset, cache)\n    else:\n        return dataset\n\n", "CodeBleu_score": 46.24943919017827}, {"code": "def cache(self, dataset: Any, cache: Optional[str], wait: bool) -> Any\n\t\n    if cache is None:\n        return dataset\n    else:\n        if not os.path.exists(cache):\n            os.makedirs(cache)\n        cache_path = os.path.join(cache, \"dataset.json\")\n        if not os.path.exists(cache_path):\n            logger.info(\"Caching dataset to %s\", cache)\n            if wait:\n                while not os.path.exists(cache_path):\n                    logger.info(\"Waiting for cache to be ready\")\n                    time.sleep(1)\n            self.cache_dataset(dataset, cache)\n        else:\n            logger.info(\"Loading cached dataset from %s\", cache)\n        return self.load_cached_dataset(cache)\n\n", "CodeBleu_score": 44.8186773544579}]}
{"instance_id": "8838a3a09a66", "generation_result": [{"code": "def create_estimator(params: Dict, MWF: bool) -> tf.Tensor\n\t\n    # Load the model directory using a ModelProvider\n    model_provider = ModelProvider(params)\n    model_dir = model_provider.get()\n    # Update the parameters\n    params = model_provider.update(params)\n    # Set up the session configuration to limit GPU memory usage\n    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.333)\n    session_conf = tf.ConfigProto(gpu_options=gpu_options, log_device_placement=False)\n    # Create a RunConfig with this session configuration\n    run_config = tf.estimator.RunConfig(session_config=session_conf)\n    # Initialize and return the TensorFlow estimator\n    if MWF:\n        return tf.estimator.Estimator(model_fn=model_provider.model_fn, model_dir=model_dir, params=params, config=run_config)\n    else:\n        return tf.estimator.Estimator(model_fn=model_provider.model_fn, params=params, config=run_config)\n\n", "CodeBleu_score": 30.038858486235764}, {"code": "def create_estimator(params: Dict, MWF: bool) -> tf.Tensor\n\t\n\n    # Create a model provider\n    provider = ModelProvider(params['model_dir'], params['model_name'])\n\n    # Load the model directory\n    provider.get()\n\n    # Update the parameters\n    params = update_parameters(params, MWF)\n\n    # Set up the session configuration\n    config = tf.ConfigProto()\n    config.gpu_options.per_process_gpu_memory_fraction = 0.3\n\n    # Create a run configuration\n    run_config = RunConfig(session_config=config)\n\n    # Initialize and return the estimator\n    return tf.estimator.Estimator(model_fn=model_fn, model_dir=params['model_dir'], params=params, config=run_config)\n\n", "CodeBleu_score": 35.06458353062216}, {"code": "def create_estimator(params: Dict, MWF: bool) -> tf.Tensor\n\t\n    # Load model directory\n    model_dir = ModelProvider.default(params['model_dir'], MWF)\n    # Update parameters\n    params['model_dir'] = model_dir\n    # Set up session configuration\n    session_config = tf.ConfigProto()\n    session_config.gpu_options.per_process_gpu_memory_fraction = 0.5\n    # Set up RunConfig\n    run_config = tf.estimator.RunConfig(session_config=session_config)\n    # Initialize and return TensorFlow estimator\n    return tf.estimator.Estimator(model_fn=model_fn, model_dir=model_dir, params=params, config=run_config)\n\n", "CodeBleu_score": 36.069362963331095}]}
{"instance_id": "779c812bf2cb", "generation_result": [{"code": "def _separate_tensorflow(\n        self, waveform: np.ndarray, audio_descriptor: AudioDescriptor\n    ) -> Dict\n\t\n    if not audio_descriptor.is_stereo:\n        waveform = self._to_stereo(waveform)\n    prediction_generator = self._get_prediction_generator(waveform, audio_descriptor)\n    prediction = next(prediction_generator)\n    prediction = self._separate_tensorflow_prediction(prediction)\n    return prediction\n\n", "CodeBleu_score": 51.07071077633088}, {"code": "def _separate_tensorflow(\n        self, waveform: np.ndarray, audio_descriptor: AudioDescriptor\n    ) -> Dict\n\t\n    # Ensure the waveform is stereo\n    waveform = to_stereo(waveform)\n\n    # Obtain a prediction generator\n    prediction_generator = self._get_prediction_generator(\n        waveform, audio_descriptor\n    )\n\n    # Update the data generator with the waveform and audio descriptor\n    self._update_data(waveform, audio_descriptor)\n\n    # Generate a prediction\n    prediction = next(prediction_generator)\n\n    # Remove the \"audio_id\" key from the prediction before returning it\n    return prediction.pop(\"audio_id\")\n\n", "CodeBleu_score": 43.50576226540448}, {"code": "def _separate_tensorflow(\n        self, waveform: np.ndarray, audio_descriptor: AudioDescriptor\n    ) -> Dict\n\t\n    waveform = self._to_stereo(waveform)\n    prediction_generator = self._get_prediction_generator(waveform, audio_descriptor)\n    prediction = self._predict(prediction_generator)\n    return prediction\n\n", "CodeBleu_score": 30.54146197747305}]}
{"instance_id": "cf580cd2ed93", "generation_result": [{"code": "def compute_backbone_shapes(config, image_shape)\n\t\n    \n    if callable(config.BACKBONE):\n        return config.COMPUTE_BACKBONE_SHAPE(image_shape)\n    else:\n        assert config.BACKBONE in [\"resnet50\", \"resnet101\"], \"backbone must be either resnet50 or resnet101\"\n        return compute_resnet_backbone_shapes(config, image_shape)\n\n", "CodeBleu_score": 42.19817242864939}, {"code": "def compute_backbone_shapes(config, image_shape)\n\t\n    if callable(config.BACKBONE):\n        return config.COMPUTE_BACKBONE_SHAPE(image_shape)\n    else:\n        assert config.BACKBONE in [\"resnet50\", \"resnet101\"], \"Backbone {} not supported\".format(config.BACKBONE)\n        import torchvision.models as models\n        import torch.nn as nn\n        backbone = models.__dict__[config.BACKBONE]()\n        if len(backbone.layers) == 4:\n            layer_strides_and_filters = [\n                (backbone.layers[0].conv1.stride, backbone.layers[0].conv1.out_channels),\n                (backbone.layers[1].conv2.stride, backbone.layers[1].conv2.out_channels),\n                (backbone.layers[2].conv3.stride, backbone.layers[2].conv3.out_channels),\n            ]\n        elif len(backbone.layers) == 5:\n            layer_strides_and_filters = [\n                (backbone.layers[0].conv1.stride, backbone.layers[0].conv1.out_channels),\n                (backbone.layers[1].conv2.stride, backbone.layers[1].conv2.out_channels),\n                (backbone.layers[2].conv3.stride, backbone.layers[2].conv3.out_channels),\n                (backbone.layers[3].conv4.stride, backbone.layers[3].conv4.out_channels),\n            ]\n        else:\n            raise ValueError(\"Backbone {} has unexpected number of layers\".format(config.BACKBONE))\n        backbone_shapes = []\n        for i, layer_strides_and_filters in enumerate(layer_strides_and_filters):\n            stride, filters = layer_strides_and_filters\n            if i == 0:\n                shape = np.ceil(image_shape / stride)\n            else:\n                shape = np.ceil(shape / stride)\n            backbone_", "CodeBleu_score": 43.49770748746107}, {"code": "def compute_backbone_shapes(config, image_shape)\n\t\n    if callable(config.BACKBONE):\n        return config.COMPUTE_BACKBONE_SHAPE(image_shape)\n    else:\n        assert config.BACKBONE in [\"resnet50\", \"resnet101\"], \"backbone must be either resnet50 or resnet101\"\n        return np.array(\n            [[int(math.ceil(image_shape[0] / stride)), int(math.ceil(image_shape[1] / stride))] for stride in config.BACKBONE_STRIDES]\n        )\n\n", "CodeBleu_score": 84.29837813158969}]}
{"instance_id": "ebd61a2e59ca", "generation_result": [{"code": "def clip_boxes_graph(boxes, window)\n\t\n    # Clip the boxes to the window boundaries.\n    # The window boundaries are represented as a 2-D tensor of shape [1, 4].\n    # The boxes are represented as a 2-D tensor of shape [num_boxes, 4].\n    # The window boundaries are in the form of [[y_min, x_min, y_max, x_max]].\n    # The boxes are in the form of [[y1, x1, y2, x2], [y2, x2, y3, x3], ...].\n    window = tf.reshape(window, [1, 4])\n    boxes = tf.reshape(boxes, [-1, 4])\n    boxes = tf.cast(boxes, tf.float32)\n    window = tf.cast(window, tf.float32)\n    y_min = tf.maximum(boxes[:, 0], window[0, 0])\n    x_min = tf.maximum(boxes[:, 1], window[0, 1])\n    y_max = tf.minimum(boxes[:, 2], window[0, 2])\n    x_max = tf.minimum(boxes[:, 3], window[0, 3])\n    clipped = tf.concat([y_min, x_min, y_max, x_max], axis=1)\n    clipped.set_shape([None, 4])\n    return clipped\n\n", "CodeBleu_score": 16.437396440777533}, {"code": "def clip_boxes_graph(boxes, window)\n\t\n    x1, y1, x2, y2 = tf.split(boxes, 4, axis=1)\n    w_window, h_window = tf.split(window, 2, axis=1)\n    x1 = tf.maximum(x1, 0)\n    y1 = tf.maximum(y1, 0)\n    x2 = tf.minimum(x2, w_window)\n    y2 = tf.minimum(y2, h_window)\n    clipped = tf.concat([x1, y1, x2, y2], 1)\n    clipped.set_shape((clipped.shape[0], 4))\n    return clipped\n\n", "CodeBleu_score": 29.336387433422278}, {"code": "def clip_boxes_graph(boxes, window)\n\t\n    with tf.name_scope('clip_boxes_graph'):\n        x1, y1, x2, y2 = tf.split(boxes, 4, axis=1)\n        w_window, h_window = tf.split(window, 2)\n        x1_c = tf.maximum(0., x1)\n        y1_c = tf.maximum(0., y1)\n        x2_c = tf.minimum(x2, w_window)\n        y2_c = tf.minimum(y2, h_window)\n        boxes_c = tf.concat([x1_c, y1_c, x2_c, y2_c], axis=1)\n        return boxes_c\n\n", "CodeBleu_score": 37.08960644623414}]}
{"instance_id": "8e0411587a70", "generation_result": [{"code": "def rpn_graph(feature_map, anchors_per_location, anchor_stride)\n\t\n    # The shared convolutional layer is applied to the feature map.\n    shared_convolutional_layer = KL.Conv2D(512, (3, 3), activation='relu', padding='same', name='rpn_conv_3x3')(feature_map)\n\n    # The first and second branches are applied to the shared convolutional layer.\n    # The first branch applies a 1x1 convolutional layer and a 3x3 convolutional layer.\n    # The second branch applies a 1x1 convolutional layer and a 3x3 convolutional layer.\n    first_branch = KL.Conv2D(512, (1, 1), activation='relu', padding='same', name='rpn_bias_shared')(shared_convolutional_layer)\n    second_branch = KL.Conv2D(256, (3, 3), activation='relu', padding='same', name='rpn_conv_5x5')(shared_convolutional_layer)\n\n    # The first and second branches are concatenated.\n    # The concatenation is flattened into a 2D tensor.\n    # The flattened tensor is reshaped into a 3D tensor.\n    merged_tensor = KL.concatenate([first_branch, second_branch], axis=3)\n    merged_tensor = KL.reshape(merged_tensor, (-1, 2*512))\n    merged_tensor = KL.reshape(merged_tensor, (-1, 2, 512))\n\n    # The merged tensor is split into two branches.\n    # The first branch applies a 1x1 convolutional layer to the first branch.\n    # The second branch applies a 1x1 convolutional layer to the second branch.\n    first_branch = KL.Conv2D(256, (1, 1), activation='relu', padding='same', name='rpn_cls_score')(merged_tensor[0])\n    second_branch = KL.Conv2D(256, (1, 1), activation='relu', padding='same', name='rpn_", "CodeBleu_score": 37.002057202912134}, {"code": "def rpn_graph(feature_map, anchors_per_location, anchor_stride)\n\t\n    with tf.variable_scope('rpn'):\n        # shared convolutional layer for class scores and bounding box predictions\n        rpn_conv = KL.Conv2D(512, (3, 3), activation='relu', kernel_initializer='normal', padding='same', name='conv_rpn')(feature_map)\n        # class scores\n        rpn_class_logits = KL.Conv2D(anchors_per_location * 2, (1, 1), activation=None, kernel_initializer='normal', padding='same', name='conv_class')(rpn_conv)\n        rpn_class_logits = tf.reshape(rpn_class_logits, [-1, 2])\n        rpn_class_logits = tf.reshape(rpn_class_logits, [-1, 2])\n        rpn_class_probs = KL.Activation('softmax', name='class_probs')(rpn_class_logits)\n        # bounding box predictions\n        rpn_bbox = KL.Conv2D(anchors_per_location * 4, (1, 1), activation=None, kernel_initializer='normal', padding='same', name='conv_bbox')(rpn_conv)\n        rpn_bbox = tf.reshape(rpn_bbox, [-1, 4])\n        # reshape the predictions to [batch_size, H, W, anchors_per_location * (class_score + class_bbox_delta)]\n        rpn_class_logits = tf.reshape(rpn_class_logits, [-1, tf.shape(feature_map)[1], tf.shape(feature_map)[2], anchors_per_location * 2])\n        rpn_bbox = tf.reshape(rpn_bbox, [-1, tf.shape(feature_map)[1], tf.shape(feature_map)[2], anchors_per_location * 4])\n        return rpn_class_logits, rpn_class_probs, rpn_bbox\n\n", "CodeBleu_score": 46.00930119996099}, {"code": "def rpn_graph(feature_map, anchors_per_location, anchor_stride)\n\t\n    with tf.variable_scope('rpn'):\n        # shared convolutional layer for all\n        shared_conv = KL.Conv2D(512, (3, 3), strides=(1, 1), padding='same', name='shared_conv')(feature_map)\n        shared_conv = KL.BatchNormalization(axis=3, name='shared_conv_bn')(shared_conv)\n        shared_conv = KL.Activation('relu', name='shared_conv_relu')(shared_conv)\n\n        # class prediction convolutional layer\n        class_conv = KL.Conv2D(anchors_per_location * 2, (1, 1), strides=(1, 1), padding='same', name='class_conv')(shared_conv)\n        class_conv = KL.BatchNormalization(axis=3, name='class_conv_bn')(class_conv)\n        class_conv = KL.Activation('relu', name='class_conv_relu')(class_conv)\n\n        # bounding box prediction convolutional layer\n        box_conv = KL.Conv2D(anchors_per_location * 4, (1, 1), strides=(1, 1), padding='same', name='box_conv')(shared_conv)\n        box_conv = KL.BatchNormalization(axis=3, name='box_conv_bn')(box_conv)\n        box_conv = KL.Activation('relu', name='box_conv_relu')(box_conv)\n\n        # reshape class predictions\n        class_logits = tf.reshape(class_conv, [-1, 2])\n\n        # reshape box predictions\n        box_predictions = tf.reshape(box_conv, [-1, 4])\n\n        # reshape anchors\n        anchors = tf.reshape(anchors_per_location, [-1, 4])\n\n        # calculate anchor scores\n        anchor_scores = tf.reduce_sum(tf.multiply(class_logits, tf.reshape(anchors, [-1, 2])), axis=1)\n", "CodeBleu_score": 44.67417504855759}]}
{"instance_id": "70777c9261d8", "generation_result": [{"code": "def smooth_l1_loss(y_true, y_pred)\n\t\n    abs_diff = K.abs(y_true - y_pred)\n    abs_diff_lt_1 = K.cast(K.less(abs_diff, 1), K.floatx())\n    smooth_l1_loss = abs_diff_lt_1 * 0.5 * (abs_diff ** 2) + (1 - abs_diff_lt_1) * (abs_diff - 0.5)\n    return smooth_l1_loss\n\n", "CodeBleu_score": 32.94085266231866}, {"code": "def smooth_l1_loss(y_true, y_pred)\n\t\n    return K.mean(K.switch(K.abs(y_true - y_pred) < 1, 0.5 * K.square(y_true - y_pred), K.abs(y_true - y_pred) - 0.5), axis=-1)\n\n", "CodeBleu_score": 19.157253050781918}, {"code": "def smooth_l1_loss(y_true, y_pred)\n\t\n    abs_diff = K.abs(y_true - y_pred)\n    abs_diff_less_one = K.cast(K.less(abs_diff, 1), K.floatx())\n    squared_loss = 0.5 * (abs_diff ** 2)\n    linear_loss = abs_diff - 0.5\n    return K.sum(squared_loss * abs_diff_less_one) + K.sum(linear_loss * (1 - abs_diff_less_one))\n\n", "CodeBleu_score": 27.6022421881019}]}
{"instance_id": "0e747b604087", "generation_result": [{"code": "def build_model(x_train, num_classes)\n\t\n    tf.reset_default_graph()\n    x = KL.Input(shape=x_train.shape[1:])\n    x = KL.Conv2D(32, (3, 3), activation='relu')(x)\n    x = KL.Conv2D(32, (3, 3), activation='relu')(x)\n    x = KL.MaxPooling2D(pool_size=(2, 2))(x)\n    x = KL.Flatten()(x)\n    x = KL.Dense(128, activation='relu')(x)\n    x = KL.Dense(num_classes, activation='softmax')(x)\n    model = KM.Model(inputs=x, outputs=x)\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model\n\n", "CodeBleu_score": 34.55767034089401}, {"code": "def build_model(x_train, num_classes)\n\t\n    tf.reset_default_graph()\n    input_layer = KL.Input(shape=x_train.shape[1:])\n    conv_layer = KL.Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(input_layer)\n    conv_layer = KL.Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(conv_layer)\n    pool_layer = KL.MaxPooling2D(pool_size=(2, 2))(conv_layer)\n    flatten_layer = KL.Flatten()(pool_layer)\n    dense_layer = KL.Dense(units=128, activation='relu')(flatten_layer)\n    output_layer = KL.Dense(units=num_classes, activation='softmax')(dense_layer)\n    model = KM.Model(inputs=input_layer, outputs=output_layer)\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n    return model\n\n", "CodeBleu_score": 28.360549893203164}, {"code": "def build_model(x_train, num_classes)\n\t\n\n    tf.reset_default_graph()\n\n    # define input layer\n    x = KL.Input(shape=x_train.shape[1:])\n\n    # define first convolutional layer\n    x = KL.Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(x)\n\n    # define second convolutional layer\n    x = KL.Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(x)\n\n    # define max-pooling layer\n    x = KL.MaxPooling2D(pool_size=(2, 2))(x)\n\n    # flatten output of max-pooling layer\n    x = KL.Flatten()(x)\n\n    # define dense layer\n    x = KL.Dense(units=64, activation='relu')(x)\n\n    # define output layer\n    x = KL.Dense(units=num_classes, activation='softmax')(x)\n\n    # define model\n    model = KM.Model(inputs=x, outputs=x)\n\n    # compile model\n    model.compile(optimizer='adam',\n                  loss='categorical_crossentropy',\n                  metrics=['accuracy'])\n\n    return model\n\n", "CodeBleu_score": 33.144200077710956}]}
{"instance_id": "813e3e2844f4", "generation_result": [{"code": "def extract_bboxes(mask)\n\t\n    bboxes = np.zeros((mask.shape[0], 4), dtype=np.int32)\n    for i in range(mask.shape[0]):\n        m = mask[i, :, :, None]\n        idx = np.where(m)\n        bboxes[i] = np.array([idx[1].min(), idx[0].min(), idx[1].max(), idx[0].max()])\n    return bboxes\n\n", "CodeBleu_score": 13.311893785488657}, {"code": "def extract_bboxes(mask)\n\t\n    mask = np.where(mask == 1, 1, 0)\n    bboxes = np.zeros((mask.shape[0], 4), dtype=np.int)\n    for i in range(mask.shape[0]):\n        if np.any(mask[i]):\n            horizontal = np.where(mask[i])[0]\n            vertical = np.where(mask[i])[1]\n            bboxes[i, 0] = horizontal[0]\n            bboxes[i, 1] = vertical[0]\n            bboxes[i, 2] = horizontal[-1]\n            bboxes[i, 3] = vertical[-1]\n        else:\n            bboxes[i, :] = 0\n    return bboxes\n\n", "CodeBleu_score": 27.7491963460111}, {"code": "def extract_bboxes(mask)\n\t\n    bboxes = np.zeros((mask.shape[0], 4), dtype=int)\n    for i in range(mask.shape[0]):\n        m = mask[i, :, :, None]\n        y, x = np.where(m)\n        if not np.any(y) and not np.any(x):\n            continue\n        ymin, ymax = y.min(), y.max()\n        xmin, xmax = x.min(), x.max()\n        bboxes[i] = np.array([xmin, ymin, xmax, ymax])\n    return bboxes\n\n", "CodeBleu_score": 22.084608526829765}]}
{"instance_id": "7ad5d430d6ed", "generation_result": [{"code": "def compute_iou(box, boxes, box_area, boxes_area)\n\t\n    # determine the (x, y)-coordinates of the intersection rectangle\n    x = np.maximum(box[0], boxes[:, 0])\n    y = np.maximum(box[1], boxes[:, 1])\n    x2 = np.minimum(box[2], boxes[:, 2])\n    y2 = np.minimum(box[3], boxes[:, 3])\n\n    # compute the area of intersection rectangle\n    inter_area = np.maximum((x2 - x), 0) * np.maximum((y2 - y), 0)\n\n    # compute the area of the union\n    union_area = box_area + boxes_area[:] - inter_area\n\n    # compute the IoU\n    iou = inter_area / union_area\n\n    return iou\n\n", "CodeBleu_score": 37.62142599574332}, {"code": "def compute_iou(box, boxes, box_area, boxes_area)\n\t\n    # Determine the (x, y)-coordinates of the intersection rectangle\n    x = max(box[0], boxes[:, 0])\n    y = max(box[1], boxes[:, 1])\n    w = min(box[2], boxes[:, 2]) - x + 1\n    h = min(box[3], boxes[:, 3]) - y + 1\n\n    if w < 0 or h < 0:\n        return 0.0\n\n    # Compute the area of intersection rectangle\n    intersection_area = w * h\n\n    # Compute the area of both the prediction and ground-truth\n    # rectangles\n    box_area = box_area\n    boxes_area = boxes_area\n\n    # Compute the IoU\n    iou = intersection_area / (box_area + boxes_area - intersection_area)\n\n    return iou\n\n", "CodeBleu_score": 33.47528089341118}, {"code": "def compute_iou(box, boxes, box_area, boxes_area)\n\t\n    # determine the (x, y)-coordinates of the intersection rectangle\n    x1 = np.maximum(box[0], boxes[:, 0])\n    y1 = np.maximum(box[1], boxes[:, 1])\n    x2 = np.minimum(box[2], boxes[:, 2])\n    y2 = np.minimum(box[3], boxes[:, 3])\n\n    # compute the area of intersection rectangle\n    intersection_area = np.maximum(x2 - x1 + 1, 0) * np.maximum(y2 - y1 + 1, 0)\n\n    # compute the area of both the prediction and ground-truth rectangles\n    box_area = box_area\n    boxes_area = boxes_area\n\n    # compute the IoU\n    iou = intersection_area / (box_area + boxes_area - intersection_area)\n\n    # return the intersection over union value\n    return iou\n\n", "CodeBleu_score": 35.833871491631406}]}
{"instance_id": "15941b852859", "generation_result": [{"code": "def compute_overlaps(boxes1, boxes2)\n\t\n    # Calculate areas of anchors and GT boxes\n    area1 = (boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1])\n    area2 = (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1])\n\n    # Initialize overlaps matrix\n    overlaps = np.zeros((boxes1.shape[0], boxes2.shape[0]))\n\n    # Compute overlaps\n    for j in range(overlaps.shape[1]):\n        # Find intersections\n        min_xy = np.maximum(boxes1[:, None, :2], boxes2[j, :2])\n        max_xy = np.minimum(boxes1[:, None, 2:], boxes2[j, 2:])\n        inter = np.maximum(0, max_xy - min_xy)\n        inter = inter[:, :, 0] * inter[:, :, 1]\n        # Find areas of each intersection\n        area_inter = inter.prod(axis=2)\n        # Find areas of each anchor and GT box\n        area_anchors = (area1[:, None]).prod(axis=1)\n        area_gts = (area2).prod(axis=1)\n        # Compute the IoU\n        union = area_anchors[:, None] + area_gts - area_inter\n        iou = area_inter / union\n        # Store the overlap\n        overlaps[:, j] = iou\n    return overlaps\n\n", "CodeBleu_score": 49.13200666559723}, {"code": "def compute_overlaps(boxes1, boxes2)\n\t\n    # Calculate areas of anchors and GT boxes\n    box_area = (boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1])\n    area = (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1])\n\n    # Compute overlaps to generate matrix [boxes1 count] x [boxes2 count]\n    # Each cell contains the IoU value.\n    overlaps = np.zeros((boxes1.shape[0], boxes2.shape[0]))\n    for i in range(overlaps.shape[1]):\n        box2 = boxes2[i]\n        overlaps[:, i] = compute_iou(box_area, box2[0], box2[2], box2[1], box2[3], area)\n    return overlaps\n\n", "CodeBleu_score": 54.61166464764455}, {"code": "def compute_overlaps(boxes1, boxes2)\n\t\n    # Calculate areas of anchors and GT boxes\n    box_area = (boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1])\n    area = (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1])\n\n    # Compute overlaps to generate matrix [boxes1 count, boxes2 count]\n    # Each cell contains the IoU value.\n    overlaps = np.zeros((boxes1.shape[0], boxes2.shape[0]))\n    for i in range(overlaps.shape[1]):\n        box2 = boxes2[i]\n        overlaps[:, i] = compute_iou(box_area, box2[0], box2[2], box2[1], box2[3], area)\n    return overlaps\n\n", "CodeBleu_score": 54.61166464764455}]}
{"instance_id": "597533722ce3", "generation_result": [{"code": "def load_image(self, image_id):\n\t\n    image = self.image_info[image_id]['image']\n    if image.ndim != 3:\n        image = image[..., :3]\n    return image\n\n", "CodeBleu_score": 44.4248015980931}, {"code": "def load_image(self, image_id):\n\t\n    image = self.image_info[image_id]['image']\n    if image.ndim != 3:\n        image = image[..., :3]\n    return image\n\n", "CodeBleu_score": 44.4248015980931}, {"code": "def load_image(self, image_id):\n\t\n    image_info = self.image_info[image_id]\n    image = skimage.io.imread(image_info[\"path\"])\n    if image.ndim != 3:\n        image = skimage.color.gray2rgb(image)\n    if image.shape[-1] == 4:\n        image = image[..., :3]\n    image = skimage.img_as_float32(image)\n    return image\n\n", "CodeBleu_score": 77.06672692364141}]}
{"instance_id": "9b852997ca05", "generation_result": [{"code": "def display_images(images, titles=None, cols=4, cmap=None, norm=None,\n                   interpolation=None)\n\t\n    if titles is None: titles = [\"\" for _ in range(len(images))]\n    rows = len(images) // cols + 1\n    fig, axes = plt.subplots(rows, cols, figsize=(15, 10))\n    fig.tight_layout()\n    for i in range(len(images)):\n        ax = axes[i // cols, i % cols]\n        ax.imshow(images[i], cmap=cmap, norm=norm, interpolation=interpolation)\n        ax.set_title(titles[i])\n    plt.show()\n\n", "CodeBleu_score": 36.670516833470664}, {"code": "def display_images(images, titles=None, cols=4, cmap=None, norm=None,\n                   interpolation=None)\n\t\n    assert len(images) == len(titles)\n    n_rows = math.ceil(len(images) / cols)\n    if titles is None:\n        titles = [\"Image (%d)\" % i for i in range(1, len(images) + 1)]\n    fig, axes = plt.subplots(n_rows, cols, figsize=(10, 10))\n    fig.subplots_adjust(hspace=0.3, wspace=0.05)\n    for i, (image, title) in enumerate(zip(images, titles)):\n        ax = axes[i // cols, i % cols]\n        ax.imshow(image, cmap=cmap, norm=norm, interpolation=interpolation)\n        ax.set_title(title)\n        ax.axis(\"off\")\n    plt.show()\n\n", "CodeBleu_score": 37.85960860385228}, {"code": "def display_images(images, titles=None, cols=4, cmap=None, norm=None,\n                   interpolation=None)\n\t\n    assert ((titles is None) or (len(images) == len(titles)))\n    n_images = len(images)\n    if titles is None: titles = ['Image (%d)' % i for i in range(1, n_images + 1)]\n    figsize = (1.05 * cols, 1.05 * math.ceil(n_images / float(cols)))\n    fig, axes = plt.subplots(figsize=figsize)\n    fig.subplots_adjust(hspace=0.05, wspace=0.05)\n    for i, (image, title) in enumerate(zip(images, titles)):\n        ax = axes[i]\n        ax.imshow(image, cmap=cmap, norm=norm, interpolation=interpolation)\n        ax.set_title(title)\n        ax.axis('off')\n    fig.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 38.421141638085835}]}
