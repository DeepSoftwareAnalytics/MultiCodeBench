{"instance_id": "1f9f4d85e825", "generation_result": [{"code": "def load_model_from_config(self, half_attention)\n\t\n    model_config = self.config.model\n    model_path = self.config.model_path\n    model_type = model_config.type\n    model_name = model_config.name\n    model_cache_path = model_config.cache_path\n    model_cache_size = model_config.cache_size\n    model_cache_name = f\"{model_name}.pt\"\n    model_cache_path = (\n        model_cache_path / model_cache_name\n        if model_cache_path is not None\n        else None\n    )\n    if model_cache_path is not None and model_cache_path.is_file():\n        if model_cache_size is not None:\n            if model_cache_path.stat().st_size > model_cache_size:\n                logger.info(\n                    f\"Cached model is too large, loading from {model_path} instead.\"\n                )\n                model_cache_path = None\n        else:\n            logger.info(\n                f\"Loading cached model from {model_cache_path} instead of {model_path}.\"\n            )\n    if model_cache_path is not None and model_cache_path.is_file():\n        model = torch.load(model_cache_path)\n    else:\n        model = instantiate_from_config(model_config)\n        model.load_state_dict(\n            torch.load(\n                model_path, map_location=torch.device(\"cpu\")\n            ),\n            strict=False,\n        )\n        if model_cache_path is not None:\n            logger.info(f\"Caching model to {model_cache_path}.\")\n            torch.save(model, model_cache_path)\n    model.eval()\n    if half_attention:\n        model.half()\n    model.to(self.device)\n    if self.memory_format == \"channels_last\":\n        model.to(memory_format=torch.channels_last)\n    return model\n", "CodeBleu_score": 30.7731699307937}, {"code": "def load_model_from_config(self, half_attention)\n\t\n    if self.config.get(\"cache_dir\", None) is not None:\n        # Check if a cached model is available and valid\n        cached_model_path = self.get_cached_model_path()\n        if cached_model_path is not None:\n            print(f\"Loading cached model from {cached_model_path}\")\n            model = self.load_cached_model(cached_model_path, half_attention)\n            return model\n\n    # Load the model from the specified path\n    model = self.load_model_from_path(half_attention)\n\n    # Cache the model for future use\n    if self.config.get(\"cache_dir\", None) is not None:\n        print(f\"Caching model to {cached_model_path}\")\n        self.cache_model(model, cached_model_path)\n\n    return model\n\n", "CodeBleu_score": 20.03531391074137}, {"code": "def load_model_from_config(self, half_attention)\n\t\n    # Load from cache if available\n    cache_path = self.get_cache_path()\n    if cache_path.exists():\n        if self.config.get(\"cache_size\", 0) > 0:\n            # Check cache size\n            cache_size = sum(f.stat().st_size for f in cache_path.glob(\"*.pt\"))\n            if cache_size > self.config[\"cache_size\"]:\n                print(\n                    f\"Cache size of {cache_size / 1024**2:.2f} MB exceeds specified limit of {self.config['cache_size'] / 1024**2:.2f} MB. Skipping cache.\"\n                )\n                cache_path.rmdir()\n            else:\n                # Load from cache\n                print(f\"Loading model from cache at {cache_path}\")\n                model = self.load_from_cache(cache_path)\n                return model\n\n    # Load model from config\n    model = instantiate_from_config(self.config[\"model\"])\n    if half_attention:\n        model.half()\n    model.to(self.device)\n    model.eval()\n\n    # Save to cache\n    if self.config.get(\"cache_size\", 0) > 0:\n        print(f\"Saving model to cache at {cache_path}\")\n        self.save_to_cache(model, cache_path)\n\n    return model\n\n", "CodeBleu_score": 31.27497869517476}]}
{"instance_id": "8dc955007aa2", "generation_result": [{"code": "def convsample_ddim(model, cond, steps, shape, eta=1.0, callback=None, normals_sequence=None,\n                    mask=None, x0=None, quantize_x0=False, temperature=1., score_corrector=None,\n                    corrector_kwargs=None, x_t=None\n                    )\n\t\n    # Initialize the DDIM sampler.\n    ddim = DDIMSampler(model, eta=eta, score_corrector=score_corrector, corrector_kwargs=corrector_kwargs)\n    # Extract the batch size from the shape parameter.\n    batch_size = shape[0]\n    # Print the eta and steps values.\n    print(\"DDIM eta = {}\".format(eta))\n    print(\"DDIM steps = {}\".format(steps))\n    # Call the sample method of the DDIMSampler instance.\n    samples, intermediates = ddim.sample(steps, batch_size, shape, cond, callback=callback,\n                                        normals_sequence=normals_sequence, mask=mask, x0=x0,\n                                        quantize_x0=quantize_x0, temperature=temperature, x_t=x_t)\n    # Return the generated samples and intermediate values.\n    return samples, intermediates\n", "CodeBleu_score": 45.98811585114589}, {"code": "def convsample_ddim(model, cond, steps, shape, eta=1.0, callback=None, normals_sequence=None,\n                    mask=None, x0=None, quantize_x0=False, temperature=1., score_corrector=None,\n                    corrector_kwargs=None, x_t=None\n                    )\n\t\n\n    # Extract the batch size from the shape parameter\n    batch_size = shape[0]\n\n    # Print the eta and steps values\n    print(f'eta = {eta}')\n    print(f'steps = {steps}')\n\n    # Initialize the DDIM sampler\n    ddim = DDIMSampler(model, cond, batch_size=batch_size, shape=shape)\n\n    # Sample\n    samples = ddim.sample(steps, eta=eta, callback=callback, normals_sequence=normals_sequence,\n                          mask=mask, x0=x0, quantize_x0=quantize_x0, temperature=temperature,\n                          score_corrector=score_corrector, corrector_kwargs=corrector_kwargs, x_t=x_t)\n\n    return samples, ddim.intermediates\n\n", "CodeBleu_score": 46.229863383359934}, {"code": "def convsample_ddim(model, cond, steps, shape, eta=1.0, callback=None, normals_sequence=None,\n                    mask=None, x0=None, quantize_x0=False, temperature=1., score_corrector=None,\n                    corrector_kwargs=None, x_t=None\n                    )\n\t\n\n    batch_size = shape[0]\n    print('batch_size', batch_size)\n    print('eta', eta)\n    print('steps', steps)\n\n    ddim = DDIMSampler(model, batch_size=batch_size, shape=shape, temperature=temperature)\n    samples = ddim.sample(steps, callback=callback, normals_sequence=normals_sequence,\n                          conditioning=cond, eta=eta, mask=mask, x0=x0, quantize_x0=quantize_x0,\n                          score_corrector=score_corrector, corrector_kwargs=corrector_kwargs, x_t=x_t)\n    return samples, ddim\n\n", "CodeBleu_score": 48.4117503935878}]}
{"instance_id": "9bdcd9081480", "generation_result": [{"code": "def ema_scope(self, context=None)\n\t\n    if context is None:\n        context = self.context\n    if self.ema_enabled:\n        if context is not None:\n            context.print(\"Using EMA weights\")\n        self.ema_restore()\n    else:\n        if context is not None:\n            context.print(\"Not using EMA weights\")\n        self.ema_store()\n\n", "CodeBleu_score": 38.07026775707793}, {"code": "def ema_scope(self, context=None)\n\t\n    if context is None:\n        context = self\n    if self.ema_enabled:\n        if self.ema_context is not context:\n            context.parameters.append(self.ema_context.parameters.pop())\n            context.parameters.append(self.ema_context.parameters.pop())\n            context.parameters.append(self.ema_context.parameters.pop())\n            context.parameters.append(self.ema_context.parameters.pop())\n            context.parameters.append(self.ema_context.parameters.pop())\n            context.parameters.append(self.ema_context.parameters.pop())\n            context.parameters.append(self.ema_context.parameters.pop())\n            context.parameters.append(self.ema_context.parameters.pop())\n            context.parameters.append(self.ema_context.parameters.pop())\n            context.parameters.append(self.ema_context.parameters.pop())\n            context.parameters.append(self.ema_context.parameters.pop())\n            context.parameters.append(self.ema_context.parameters.pop())\n            context.parameters.append(self.ema_context.parameters.pop())\n            context.parameters.append(self.ema_context.parameters.pop())\n            context.parameters.append(self.ema_context.parameters.pop())\n            context.parameters.append(self.ema_context.parameters.pop())\n            context.parameters.append(self.ema_context.parameters.pop())\n            context.parameters.append(self.ema_context.parameters.pop())\n            context.parameters.append(self.ema_context.parameters.pop())\n            context.parameters.append(self.ema_context.parameters.pop())\n            context.parameters.append(self.ema_context.parameters.pop())\n            context.parameters.append(self.ema_context.parameters.pop())\n            context.parameters.append(self.ema_context.parameters.pop())\n            context.parameters.append(self.ema_context.parameters.pop())\n            context.parameters.append(self.ema_context.parameters.pop())\n            context.parameters.append(self.ema_context.parameters.pop())\n            context.parameters.append(self.ema_context.parameters.pop())\n            context.parameters.append(self.ema_context.parameters.pop())\n            context.parameters.append(", "CodeBleu_score": 26.700671580769868}, {"code": "def ema_scope(self, context=None)\n\t\n    if context is None:\n        context = self\n    if self.ema_enabled:\n        context.print('Using EMA weights')\n        context.parameters.weights = self.ema_weights\n    else:\n        context.print('Not using EMA weights')\n        self.ema_weights = self.parameters.weights\n\n", "CodeBleu_score": 25.20200794091058}]}
{"instance_id": "14ab009e1c5c", "generation_result": [{"code": "def init_from_ckpt(self, path, ignore_keys=None)\n\t\n    if ignore_keys is None:\n        ignore_keys = []\n    # Load the state dict from the checkpoint.\n    sd = torch.load(path, map_location=\"cpu\")\n    # Delete ignored keys.\n    for k in list(sd.keys()):\n        for ik in ignore_keys:\n            if k.startswith(ik):\n                del sd[k]\n    # Load the new state dict.\n    self.load_state_dict(sd, strict=False)\n    # Print out the differences between the old and new state dicts.\n    added_keys = set(self.state_dict().keys()) - set(sd.keys())\n    removed_keys = set(sd.keys()) - set(self.state_dict().keys())\n    modified_keys = [k for k in sd.keys() if self.state_dict()[k] != sd[k]]\n    msg = \"Missing keys: {}", "CodeBleu_score": 37.93372170729005}, {"code": "def init_from_ckpt(self, path, ignore_keys=None)\n\t\n    if ignore_keys is None:\n        ignore_keys = []\n    sd = torch.load(path, map_location=\"cpu\")\n    # delete ignored params\n    for k in list(sd.keys()):\n        for ik in ignore_keys:\n            if k.startswith(ik):\n                del sd[k]\n    # load state dict\n    self.load_state_dict(sd, strict=False)\n    # show unexpected params\n    missing_keys, unexpected_keys = self.load_state_dict(sd, strict=False)\n    if len(missing_keys) > 0:\n        print(\"Missing Keys: {}\".format(missing_keys))\n    if len(unexpected_keys) > 0:\n        print(\"Unexpected Keys: {}\".format(unexpected_keys))\n", "CodeBleu_score": 44.75563872858986}, {"code": "def init_from_ckpt(self, path, ignore_keys=None)\n\t\n    if ignore_keys is None:\n        ignore_keys = []\n    ckpt = torch.load(path, map_location=\"cpu\")\n    sd = ckpt[\"state_dict\"]\n    # delete ignored params\n    for k in list(sd.keys()):\n        for ik in ignore_keys:\n            if k.startswith(ik):\n                del sd[k]\n    # load state_dict\n    self.load_state_dict(sd, strict=False)\n    # show unexpected params\n    missing_keys, unexpected_keys = self.load_state_dict(sd, strict=False)\n    if len(missing_keys) > 0:\n        print(\"Missing Keys: {}\".format(missing_keys))\n    if len(unexpected_keys) > 0:\n        print(\"Unexpected Keys: {}\".format(unexpected_keys))\n    return self\n", "CodeBleu_score": 48.99617233648924}]}
{"instance_id": "754176e27b81", "generation_result": [{"code": "def forward(self, input, return_pred_indices=False)\n\t\n    encoded_input = self.encode(input)\n    decoded_output = self.decode(encoded_input)\n    if return_pred_indices:\n        pred_indices = self.get_pred_indices(decoded_output)\n        return decoded_output, self.get_diff_metric(input, decoded_output), pred_indices\n    else:\n        return decoded_output, self.get_diff_metric(input, decoded_output)\n\n", "CodeBleu_score": 29.01636745858197}, {"code": "def forward(self, input, return_pred_indices=False)\n\t\n    if return_pred_indices:\n        return self.encode(input), self.decode(input), self.difference(input), self.pred_indices\n    else:\n        return self.encode(input), self.decode(input), self.difference(input)\n", "CodeBleu_score": 30.493414961910027}, {"code": "def forward(self, input, return_pred_indices=False)\n\t\n    encoded = self.encode(input)\n    decoded = self.decode(encoded)\n    if return_pred_indices:\n        return decoded, self.difference_metric(input, decoded), self.pred_indices\n    else:\n        return decoded, self.difference_metric(input, decoded)\n", "CodeBleu_score": 26.185252737071963}]}
{"instance_id": "443df6fdaf0d", "generation_result": [{"code": "def get_input(self, batch, k)\n\t\n    x = batch[k]\n    if x.dim() == 3:\n        x = x.unsqueeze(0)\n    if self.resize:\n        x = F.interpolate(x, size=self.resize, mode='bicubic')\n    x = x.permute(0, 3, 1, 2)\n    x = x.detach()\n    return x\n\n", "CodeBleu_score": 25.6999239132501}, {"code": "def get_input(self, batch, k)\n\t\n    x = batch[k]\n    if len(x.shape) == 3:\n        x = x.unsqueeze(0)\n    x = x.permute(0, 3, 1, 2)\n    if self.resize:\n        x = F.interpolate(x, size=self.resize_range[0], mode='bicubic')\n    x = x.detach()\n    return x\n\n", "CodeBleu_score": 29.30700619730146}, {"code": "def get_input(self, batch, k)\n\t\n    x = batch[k]\n    if len(x.shape) == 3:\n        x = x.unsqueeze(0)\n    if self.resize:\n        x = F.interpolate(x, size=self.resize_range[0], mode='bicubic')\n    x = x.permute(0, 3, 1, 2)\n    x = x.to(torch.float32).contiguous()\n    return x.detach()\n", "CodeBleu_score": 30.501173013956734}]}
{"instance_id": "953a2632fc15", "generation_result": [{"code": "def training_step(self, batch, batch_idx, optimizer_idx)\n\t\n    # Get data\n    x, y = batch\n    # Get predictions\n    y_hat = self(x)\n    # Compute losses\n    if optimizer_idx == 0:\n        loss = self.loss(y_hat, y)\n        self.log('train_loss', loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n        return loss\n    elif optimizer_idx == 1:\n        loss = self.loss(y_hat, y)\n        self.log('train_loss', loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n        return loss\n", "CodeBleu_score": 14.195893923004324}, {"code": "def training_step(self, batch, batch_idx, optimizer_idx)\n\t\n    input_data = batch[0]\n    predictions = self.forward(input_data)\n    loss = None\n    if optimizer_idx == 0:\n        loss = self.loss(predictions, input_data)\n    elif optimizer_idx == 1:\n        loss = self.loss(predictions, input_data)\n    self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n    return loss\n\n", "CodeBleu_score": 18.556115272855862}, {"code": "def training_step(self, batch, batch_idx, optimizer_idx)\n\t\n    x, y = batch\n    x_hat = self.forward(x)\n    if optimizer_idx == 0:\n        loss = self.loss(x, x_hat)\n        self.log(\"train_loss\", loss)\n        return loss\n    else:\n        disc_loss = self.loss(x, x_hat)\n        self.log(\"disc_loss\", disc_loss)\n        return disc_loss\n\n", "CodeBleu_score": 13.946420023859844}]}
{"instance_id": "b449b5e8b77f", "generation_result": [{"code": "def log_images(self, batch, only_inputs=False, plot_ema=False, **kwargs)\n\t\n    def process_image(img):\n        if self.data_type == 'image_array':\n            if img.shape[2] == 1:\n                img = img.repeat(3, 2)\n            elif img.shape[2] == 2:\n                img = img.repeat(3, 1)\n            elif img.shape[2] == 4:\n                img = img.repeat(1, 2)\n            elif img.shape[2] == 3:\n                img = img.transpose(2, 0, 1)\n            elif img.shape[2] == 5:\n                img = img[:, :, :3].transpose(2, 0, 1)\n            elif img.shape[2] == 6:\n                img = img[:, :, :3].transpose(2, 0, 1)\n            elif img.shape[2] == 7:\n                img = img[:, :, :3].transpose(2, 0, 1)\n            elif img.shape[2] == 8:\n                img = img[:, :, :3].transpose(2, 0, 1)\n            elif img.shape[2] == 9:\n                img = img[:, :, :3].transpose(2, 0, 1)\n            elif img.shape[2] == 11:\n                img = img[:, :, :3].transpose(2, 0, 1)\n            elif img.shape[2] == 12:\n                img = img[:, :, :3].transpose(2, 0, 1)\n            elif img.shape[2] == 14:\n                img = img[:, :, :3].transpose(2, 0, 1)\n            elif img.shape[2] == 15:\n                img = img[:, :, :3].transpose(2, 0, 1)\n            elif img.shape[2] == 16:\n                img = img[:, :, :3].transpose(2, 0, 1)\n            elif img.shape[2] == 18:\n                img = img[:, :, :3].transpose(2, 0, 1)\n            elif img.shape[2] == 20:\n                img = img[:, :, :3].transpose(2, 0, 1)\n            elif img.shape[2] == 21:", "CodeBleu_score": 17.818592154476733}, {"code": "def log_images(self, batch, only_inputs=False, plot_ema=False, **kwargs)\n\t\n    if not self.training:\n        return\n    if plot_ema:\n        assert self.ema is not None, \"EMA model is not available; cannot log EMA images.\"\n    # get batch\n    x, y = batch[\"img\"], batch[\"label\"]\n    # move to device\n    x = x.to(self.device)\n    y = y.to(self.device)\n    # get predictions\n    with torch.no_grad():\n        if plot_ema:\n            y_hat, y_hat_ema = self.ema(x)\n        else:\n            y_hat = self.net(x)\n    # log images\n    if only_inputs:\n        self.log_image(\"inputs\", x, **kwargs)\n    else:\n        self.log_image(\"inputs\", x, **kwargs)\n        self.log_image(\"reconstructions\", y_hat, **kwargs)\n        if plot_ema:\n            self.log_image(\"ema_reconstructions\", y_hat_ema, **kwargs)\n\n", "CodeBleu_score": 38.109416627290315}, {"code": "def log_images(self, batch, only_inputs=False, plot_ema=False, **kwargs)\n\t\n    if not self.log_images:\n        return\n    if self.global_step is None:\n        return\n    if self.global_step % self.image_interval != 0:\n        return\n    if self.global_step == 0:\n        return\n    if self.ema_decay is None:\n        ema_weights = None\n    else:\n        ema_weights = self.ema_model.get_weights()\n    if self.use_ema and not plot_ema:\n        self.ema_model.set_weights(self.model.get_weights())\n    if self.use_ema and plot_ema:\n        self.ema_model.set_weights(self.ema_model.get_weights())\n    if self.input_images is None:\n        self.input_images = self.get_input_images(batch, only_inputs)\n    if self.reconstructions is None:\n        self.reconstructions = self.get_reconstructions(batch, only_inputs)\n    if self.use_ema and not plot_ema:\n        self.ema_model.set_weights(ema_weights)\n    if self.use_ema and plot_ema:\n        self.ema_model.set_weights(ema_weights)\n    if self.use_ema and not plot_ema:\n        self.reconstructions_ema = self.get_reconstructions(batch, only_inputs)\n    if self.use_ema and plot_ema:\n        self.reconstructions_ema = self.get_reconstructions(batch, only_inputs)\n    if self.input_images.shape[1] == 1:\n        self.input_images = tf.tile(self.input_images, [1, 3, 1, 1])\n        self.reconstructions = tf.tile(self.reconstructions, [1, 3, 1, 1])\n        if self.use_ema and not plot_ema:\n            self.reconstructions_ema = tf.tile(self.reconstructions_ema, [1, 3, 1, 1])\n        if self.use_ema and plot_ema:\n            self.reconstructions_ema = tf.tile(self.reconstructions_ema, [1, 3, 1, 1])\n    if self.input_images.shape[1] == 3:", "CodeBleu_score": 24.077010903925284}]}
{"instance_id": "540d089e3481", "generation_result": [{"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss_type == 'l1':\n        loss = F.l1_loss(pred, target, reduction='mean')\n        if mean:\n            loss = loss.mean()\n        return loss\n    elif self.loss_type == 'l2':\n        loss = F.mse_loss(pred, target, reduction='mean')\n        if mean:\n            loss = loss.mean()\n        return loss\n    else:\n        raise NotImplementedError\n", "CodeBleu_score": 53.06019449693282}, {"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss_type == 'l1':\n        loss = torch.abs(pred - target)\n        if mean:\n            loss = loss.mean()\n    elif self.loss_type == 'l2':\n        loss = (pred - target)**2\n        if mean:\n            loss = loss.mean()\n        else:\n            loss = loss.sum()\n    else:\n        raise NotImplementedError\n    return loss\n", "CodeBleu_score": 51.60710457552793}, {"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss_type == 'l1':\n        loss = torch.abs(pred - target)\n        if mean:\n            loss = loss.mean()\n    elif self.loss_type == 'l2':\n        loss = (pred - target) ** 2\n        if mean:\n            loss = loss.mean()\n        else:\n            loss = loss.sum()\n    else:\n        raise NotImplementedError\n    return loss\n", "CodeBleu_score": 50.737571082517164}]}
{"instance_id": "63a08283666f", "generation_result": [{"code": "def forward(self, x, c, *args, **kwargs)\n\t\n    # If we are conditioning on a learned embedding, we need to get the learned embedding.\n    if self.learned_conditioning:\n        c = self.get_learned_conditioning(c)\n    # If we are conditioning on a learned embedding, we need to get the learned embedding.\n    if self.learned_conditioning:\n        c = self.get_learned_conditioning(c)\n    # If we are conditioning on a learned embedding, we need to get the learned embedding.\n    if self.learned_conditioning:\n        c = self.get_learned_conditioning(c)\n    # If we are conditioning on a learned embedding, we need to get the learned embedding.\n    if self.learned_conditioning:\n        c = self.get_learned_conditioning(c)\n    # If we are conditioning on a learned embedding, we need to get the learned embedding.\n    if self.learned_conditioning:\n        c = self.get_learned_conditioning(c)\n    # If we are conditioning on a learned embedding, we need to get the learned embedding.\n    if self.learned_conditioning:\n        c = self.get_learned_conditioning(c)\n    # If we are conditioning on a learned embedding, we need to get the learned embedding.\n    if self.learned_conditioning:\n        c = self.get_learned_conditioning(c)\n    # If we are conditioning on a learned embedding, we need to get the learned embedding.\n    if self.learned_conditioning:\n        c = self.get_learned_conditioning(c)\n    # If we are conditioning on a learned embedding, we need to get the learned embedding.\n    if self.learned_conditioning:\n        c = self.get_learned_conditioning(c)\n    # If we are conditioning on a learned embedding, we need to get the learned embedding.\n    if self.learned_conditioning:\n        c = self.get_learned_conditioning(c)\n    # If we are conditioning on a learned embedding, we need to get the learned embedding.\n    if self.learned_conditioning:\n        c = self.get_learned_conditioning(c)\n    # If we are conditioning on a learned embedding, we need to get the", "CodeBleu_score": 21.636326468621306}, {"code": "def forward(self, x, c, *args, **kwargs)\n\t\n    # If we're using learned conditioning, we'll need to process it.\n    if self.learned_conditioning:\n        c = self.get_learned_conditioning(c)\n\n    # If we're using a time-dependent conditioning, we'll need to process it.\n    if self.time_dependent_conditioning:\n        c = self.get_time_dependent_conditioning(c)\n\n    # If we're using a time-dependent conditioning, we'll need to process it.\n    if self.time_dependent_conditioning:\n        c = self.get_time_dependent_conditioning(c)\n\n    # If we're using a time-dependent conditioning, we'll need to process it.\n    if self.time_dependent_conditioning:\n        c = self.get_time_dependent_conditioning(c)\n\n    # If we're using a time-dependent conditioning, we'll need to process it.\n    if self.time_dependent_conditioning:\n        c = self.get_time_dependent_conditioning(c)\n\n    # If we're using a time-dependent conditioning, we'll need to process it.\n    if self.time_dependent_conditioning:\n        c = self.get_time_dependent_conditioning(c)\n\n    # If we're using a time-dependent conditioning, we'll need to process it.\n    if self.time_dependent_conditioning:\n        c = self.get_time_dependent_conditioning(c)\n\n    # If we're using a time-dependent conditioning, we'll need to process it.\n    if self.time_dependent_conditioning:\n        c = self.get_time_dependent_conditioning(c)\n\n    # If we're using a time-dependent conditioning, we'll need to process it.\n    if self.time_dependent_conditioning:\n        c = self.get_time_dependent_conditioning(c)\n\n    # If we're using a time-dependent conditioning, we'll need to process it.\n    if self.time_dependent_conditioning:\n        c = self.get_time_dependent_conditioning(c)\n\n    # If we're using a time-dependent conditioning, we'll need to process it.\n    if self.time_dependent_conditioning:\n        c = self.get_time_dependent_conditioning(c)", "CodeBleu_score": 21.154628327785712}, {"code": "def forward(self, x, c, *args, **kwargs)\n\t\n    # Get the time steps to use\n    t = self.get_time_steps(x.shape[0], self.num_timesteps)\n    # Get the conditioning\n    c = self.get_learned_conditioning(c)\n    # Process the inputs\n    x = self.process_inputs(x, t)\n    # Compute the loss\n    loss = self.compute_loss(x, c, t)\n    # Return the loss\n    return loss\n", "CodeBleu_score": 9.018325439118314}]}
{"instance_id": "4627e1e8ed62", "generation_result": [{"code": "def get_codebook_entry(self, indices, shape)\n\t\n    if self.map_to_all:\n        indices = self.map_to_all_indices(indices)\n    z_q = self.embedding(indices.contiguous().view(-1)).view(indices.shape + shape)\n    if self.unmap_to_all:\n        z_q = self.unmap_to_all(z_q)\n    z_q = z_q.permute(0, 3, 1, 2)\n    return z_q\n", "CodeBleu_score": 48.527564244226745}, {"code": "def get_codebook_entry(self, indices, shape)\n\t\n    if self.map_to_all:\n        indices = self.map_to_all_indices(indices)\n    z_q = self.embedding(indices.contiguous())\n    z_q = z_q.view(shape)\n    if self.unmap_to_all:\n        z_q = self.unmap_to_all(z_q)\n    z_q = z_q.permute(0, 3, 1, 2)\n    return z_q\n\n", "CodeBleu_score": 45.39119591977608}, {"code": "def get_codebook_entry(self, indices, shape)\n\t\n    if self.map_to_all:\n        indices = self.unmap_to_all(indices)\n    z_q = self.embedding(indices)\n    z_q = z_q.view(shape)\n    if self.map_to_all:\n        z_q = z_q.permute(0, 3, 1, 2)\n    return z_q.contiguous()\n\n", "CodeBleu_score": 39.79726319514705}]}
{"instance_id": "c55d014827ef", "generation_result": [{"code": "def factorization(dimension: int, factor:int=-1) -> tuple[int, int]\n\t\n    if factor > 0:\n        if factor > dimension:\n            raise ValueError(f\"The factor must be less than or equal to the dimension, but {factor} > {dimension}.\")\n        else:\n            return factor, dimension // factor\n    elif factor < 0:\n        if abs(factor) > dimension:\n            raise ValueError(f\"The factor must be less than or equal to the dimension, but {abs(factor)} > {dimension}.\")\n        else:\n            for i in range(1, dimension // 2 + 1):\n                if dimension % i == 0:\n                    if abs(i + dimension // i) <= abs(factor):\n                        return i, dimension // i\n            raise ValueError(f\"No factorization of the dimension {dimension} could be found that does not exceed the specified factor.\")\n    else:\n        raise ValueError(f\"The factor must be non-zero, but {factor} is zero.\")\n", "CodeBleu_score": 28.413082181746223}, {"code": "def factorization(dimension: int, factor:int=-1) -> tuple[int, int]\n\t\n    if factor > 0:\n        if factor > dimension:\n            raise ValueError(f\"The specified factor {factor} cannot be greater than the dimension {dimension}.\")\n        elif dimension % factor == 0:\n            return factor, dimension // factor\n        else:\n            return dimension, 1\n    else:\n        if factor < -dimension:\n            raise ValueError(f\"The specified factor {factor} cannot be less than the dimension {dimension}.\")\n        elif factor < 0:\n            factor = -factor\n            while factor > 1:\n                if dimension % factor == 0:\n                    return factor, dimension // factor\n                factor -= 1\n        return dimension, 1\n\n", "CodeBleu_score": 30.481295613164882}, {"code": "def factorization(dimension: int, factor:int=-1) -> tuple[int, int]\n\t\n    if dimension < 1:\n        raise ValueError(\"Dimension must be a positive integer.\")\n    if factor > 0:\n        if dimension % factor == 0:\n            return factor, dimension//factor\n    if factor < 0:\n        factor = 2\n        while factor <= dimension:\n            if dimension % factor == 0:\n                return factor, dimension//factor\n            factor += 1\n    return factor, dimension//factor\n", "CodeBleu_score": 29.75907755592391}]}
{"instance_id": "5d613ec26036", "generation_result": [{"code": "def apply_weight_decompose(self, updown, orig_weight)\n\t\n    if updown.dtype != orig_weight.dtype:\n        raise ValueError(f\"The data types of the input weight ({updown.dtype}) and the original weight ({orig_weight.dtype}) must be the same.\")\n    if updown.device != orig_weight.device:\n        raise ValueError(f\"The devices of the input weight ({updown.device}) and the original weight ({orig_weight.device}) must be the same.\")\n    if updown.ndim != orig_weight.ndim:\n        raise ValueError(f\"The number of dimensions of the input weight ({updown.ndim}) and the original weight ({orig_weight.ndim}) must be the same.\")\n    if updown.shape != orig_weight.shape:\n        raise ValueError(f\"The shapes of the input weight ({updown.shape}) and the original weight ({orig_weight.shape}) must be the same.\")\n    if updown.ndim == 2:\n        if updown.shape[0] != updown.shape[1]:\n            raise ValueError(f\"The shapes of the input weight ({updown.shape}) and the original weight ({orig_weight.shape}) must be square matrices.\")\n    if updown.ndim == 4:\n        if updown.shape[1] != updown.shape[2]:\n            raise ValueError(f\"The shapes of the input weight ({updown.shape}) and the original weight ({orig_weight.shape}) must be square matrices.\")\n        if updown.shape[1] != updown.shape[3]:\n            raise ValueError(f\"The shapes of the input weight ({updown.shape}) and the original weight ({orig_weight.shape}) must be square matrices.\")\n    if updown.ndim == 3:\n        if updown.shape[0] != updown.shape[1]:\n            raise ValueError(f\"The shapes of the input weight ({updown.shape}) and the original weight ({orig_weight.shape}) must be square matrices.\")\n    if updown.ndim == 1:\n        if updown.shape[0] != 1:\n            raise ValueError(f\"The shapes of the input weight ({updown.shape}) and the original weight ({orig_weight.shape}) must be vectors.\")\n    if updown.ndim > 4:\n        raise ValueError(f\"The number of dimensions of the input weight ({updown.ndim}) must", "CodeBleu_score": 11.531090945431988}, {"code": "def apply_weight_decompose(self, updown, orig_weight)\n\t\n    if updown.device != orig_weight.device:\n        raise RuntimeError(\"Input weight and original weight must be on the same device.\")\n    if updown.dtype != orig_weight.dtype:\n        raise RuntimeError(\"Input weight and original weight must have the same data type.\")\n    if updown.shape != orig_weight.shape:\n        raise RuntimeError(\"Input weight and original weight must have the same shape.\")\n    if updown.requires_grad:\n        raise RuntimeError(\"Input weight must be a leaf tensor.\")\n    if orig_weight.requires_grad:\n        raise RuntimeError(\"Original weight must be a leaf tensor.\")\n    if updown.ndim != 2:\n        raise RuntimeError(\"Input weight must be a 2D tensor.\")\n    if orig_weight.ndim != 2:\n        raise RuntimeError(\"Original weight must be a 2D tensor.\")\n    if updown.shape[0] != orig_weight.shape[0]:\n        raise RuntimeError(\"Input weight and original weight must have the same number of rows.\")\n    if updown.shape[1] != orig_weight.shape[1]:\n        raise RuntimeError(\"Input weight and original weight must have the same number of columns.\")\n    if updown.shape[0] != updown.shape[1]:\n        raise RuntimeError(\"Input weight must be square.\")\n    if orig_weight.shape[0] != orig_weight.shape[1]:\n        raise RuntimeError(\"Original weight must be square.\")\n    if updown.shape[0] % 2 != 0:\n        raise RuntimeError(\"Input weight must have an even number of rows.\")\n    if orig_weight.shape[0] % 2 != 0:\n        raise RuntimeError(\"Original weight must have an even number of rows.\")\n    if updown.shape[0] != orig_weight.shape[0]:\n        raise RuntimeError(\"Input weight and original weight must have the same number of rows.\")\n    if updown.shape[1] != orig_weight.shape[1]:\n        raise RuntimeError(\"Input weight and original weight must have the same number of columns.\")\n    if updown.shape[0] != updown.shape[1]:\n        raise RuntimeError(\"Input weight must be square.\")\n    if orig_weight.shape[0] != orig_weight.shape[1]:\n        raise RuntimeError(\"Original weight must be square.\")\n    if updown.shape[0] % 2 != 0:\n        raise RuntimeError(\"Input", "CodeBleu_score": 11.667124854775434}, {"code": "def apply_weight_decompose(self, updown, orig_weight)\n\t\n    # Ensure that all tensors are on the same device and have the same data type before performing these operations\n    device = updown.device\n    dtype = updown.dtype\n    updown = updown.to(device=device, dtype=dtype)\n    orig_weight = orig_weight.to(device=device, dtype=dtype)\n    # Transpose the original weight\n    orig_weight = orig_weight.transpose(0, 1)\n    # Reshape the original weight\n    orig_weight = orig_weight.reshape(orig_weight.shape[0], -1)\n    # Merge the original weight and the input weight\n    merged_weight = torch.cat([orig_weight, updown], dim=0)\n    # Normalize the merged weight\n    norm = torch.norm(merged_weight, dim=0)\n    norm = norm.reshape(1, -1)\n    norm = torch.max(norm, torch.tensor([1e-8], device=device, dtype=dtype))\n    merged_weight = merged_weight / norm\n    # Scale the merged weight\n    merged_weight = merged_weight * self.scale_factor\n    # Calculate the final adjusted weight\n    adjusted_weight = merged_weight - orig_weight\n    # Reshape the adjusted weight\n    adjusted_weight = adjusted_weight.reshape(updown.shape[0], updown.shape[1], -1)\n    # Transpose the adjusted weight\n    adjusted_weight = adjusted_weight.transpose(0, 1)\n    return adjusted_weight\n", "CodeBleu_score": 18.46122146774206}]}
{"instance_id": "c73d0d1cd3e5", "generation_result": [{"code": "def finalize_updown(self, updown, orig_weight, output_shape, ex_bias=None)\n\t\n    if self.bias:\n        updown = updown + self.bias\n    updown = updown.reshape(output_shape)\n    if self.bias and ex_bias is not None:\n        ex_bias = ex_bias + self.bias\n    if self.bias and self.multiplier != 1:\n        ex_bias = ex_bias * self.multiplier\n    if self.scale is not None:\n        updown = self.apply_weight_decompose(updown, orig_weight)\n    return updown, ex_bias\n", "CodeBleu_score": 39.451662320728495}, {"code": "def finalize_updown(self, updown, orig_weight, output_shape, ex_bias=None)\n\t\n    if ex_bias is not None:\n        ex_bias = ex_bias.reshape(1, -1, 1, 1)\n        ex_bias = ex_bias.repeat(1, 1, *output_shape[2:])\n        updown = updown + ex_bias\n    if self.updown_type == \"upscale\":\n        updown = updown.reshape(output_shape)\n    elif self.updown_type == \"downscale\":\n        updown = updown.reshape(orig_weight.shape)\n    else:\n        raise ValueError(f\"Unknown updown type {self.updown_type}\")\n    if orig_weight.numel() == updown.numel():\n        updown = updown.reshape(orig_weight.shape)\n    if self.multiplier is not None:\n        updown = updown * self.multiplier\n    if self.calc_scale:\n        return updown, ex_bias\n    else:\n        return updown, None\n", "CodeBleu_score": 50.84601424908888}, {"code": "def finalize_updown(self, updown, orig_weight, output_shape, ex_bias=None)\n\t\n    if ex_bias is not None:\n        updown = updown + ex_bias\n    updown = updown.reshape(output_shape)\n    if orig_weight.numel() == updown.numel():\n        updown = updown.reshape(orig_weight.shape)\n    if self.multiplier is not None:\n        updown = self.multiplier * updown\n    if self.calc_scale:\n        updown = self.apply_weight_decompose(updown)\n    return updown\n\n", "CodeBleu_score": 43.043936541789996}]}
{"instance_id": "c6a41665c62c", "generation_result": [{"code": "def calc_updown(self, orig_weight)\n\t\n    if orig_weight.is_cuda:\n        return self.calc_updown_cuda(orig_weight)\n    else:\n        return self.calc_updown_cpu(orig_weight)\n", "CodeBleu_score": 14.423930849474372}, {"code": "def calc_updown(self, orig_weight)\n\t\n    if orig_weight.is_cuda:\n        if orig_weight.device == self.weight.device:\n            return self.finalize_updown(orig_weight, self.weight)\n        else:\n            with torch.cuda.device(orig_weight.device):\n                updown_weight = orig_weight.new(orig_weight.size())\n                updown_weight.copy_(orig_weight)\n            return self.finalize_updown(updown_weight, self.weight)\n    else:\n        updown_weight = orig_weight.new(orig_weight.size())\n        updown_weight.copy_(orig_weight)\n        return self.finalize_updown(updown_weight, self.weight)\n\n", "CodeBleu_score": 34.05324012850493}, {"code": "def calc_updown(self, orig_weight)\n\t\n    if orig_weight.device != self.weight.device:\n        self.weight = self.weight.to(orig_weight.device)\n        if self.bias is not None:\n            self.bias = self.bias.to(orig_weight.device)\n    return self.finalize_updown(orig_weight)\n\n", "CodeBleu_score": 42.893393237823076}]}
{"instance_id": "1bb9e5d94af8", "generation_result": [{"code": "def purge_networks_from_memory()\n\t\n    global networks_in_memory\n    while len(networks_in_memory) > MAX_NETWORKS_IN_MEMORY:\n        networks_in_memory.pop()\n        gc.collect()\n\n", "CodeBleu_score": 32.1493548073695}, {"code": "def purge_networks_from_memory()\n\t\n    global networks_in_memory\n    while len(networks_in_memory) > NETWORK_MEMORY_LIMIT:\n        networks_in_memory.pop(0)\n        gc.collect()\n", "CodeBleu_score": 32.13798071626869}, {"code": "def purge_networks_from_memory()\n\t\n    global networks_in_memory\n    while len(networks_in_memory) > 10:\n        networks_in_memory.popitem()\n        gc.collect()\n\n", "CodeBleu_score": 31.028506493465958}]}
{"instance_id": "42b48ebf4d07", "generation_result": [{"code": "def network_restore_weights_from_backup(self: Union[torch.nn.Conv2d, torch.nn.Linear, torch.nn.GroupNorm, torch.nn.LayerNorm, torch.nn.MultiheadAttention]):\n\t\n    # Check if backup weights and biases exist\n    if self.weight_backup is not None:\n        # If so, copy them back to the corresponding attributes\n        self.weight = self.weight_backup.clone()\n        if self.bias_backup is not None:\n            self.bias = self.bias_backup.clone()\n        else:\n            self.bias = None\n    else:\n        # If not, print a warning\n        print(f\"WARNING: No backup weights for {self.__class__.__name__} layer {self.name}.\")\n\n", "CodeBleu_score": 24.68682635315612}, {"code": "def network_restore_weights_from_backup(self: Union[torch.nn.Conv2d, torch.nn.Linear, torch.nn.GroupNorm, torch.nn.LayerNorm, torch.nn.MultiheadAttention]):\n\t\n    if not hasattr(self, 'backup_weights'):\n        raise ValueError(f'The layer {self} does not have a backup of weights.')\n    if not hasattr(self, 'backup_bias'):\n        self.bias = None\n    else:\n        self.bias = self.backup_bias\n    if isinstance(self, torch.nn.Conv2d) or isinstance(self, torch.nn.Linear):\n        self.weight = self.backup_weights\n    elif isinstance(self, torch.nn.GroupNorm) or isinstance(self, torch.nn.LayerNorm):\n        self.weight = self.backup_weights\n        self.bias = self.backup_bias\n    elif isinstance(self, torch.nn.MultiheadAttention):\n        self.in_proj_weight = self.backup_weights[:self.in_proj_weight.numel()]\n        self.in_proj_bias = self.backup_bias[:self.in_proj_bias.numel()]\n        self.out_proj.weight = self.backup_weights[self.in_proj_weight.numel():]\n        self.out_proj.bias = self.backup_bias[self.in_proj_bias.numel():]\n    else:\n        raise ValueError(f'The layer {self} is not supported.')\n\n", "CodeBleu_score": 40.41398042870197}, {"code": "def network_restore_weights_from_backup(self: Union[torch.nn.Conv2d, torch.nn.Linear, torch.nn.GroupNorm, torch.nn.LayerNorm, torch.nn.MultiheadAttention]):\n\t\n    if hasattr(self, \"weight_backup\"):\n        self.weight.data = self.weight_backup.data.clone()\n    if hasattr(self, \"bias_backup\"):\n        self.bias.data = self.bias_backup.data.clone()\n    if isinstance(self, torch.nn.MultiheadAttention):\n        if hasattr(self, \"in_proj_weight_backup\"):\n            self.in_proj_weight.data = self.in_proj_weight_backup.data.clone()\n        if hasattr(self, \"in_proj_bias_backup\"):\n            self.in_proj_bias.data = self.in_proj_bias_backup.data.clone()\n\n", "CodeBleu_score": 30.925479656642047}]}
{"instance_id": "c8fcd357131a", "generation_result": [{"code": "def network_forward(org_module, input, original_forward)\n\t\n    # Check if the module has a _loaded_networks attribute, and if so, use it to check if any loaded networks match the current layer\n    if hasattr(org_module, '_loaded_networks'):\n        # Get the current layer name\n        layer_name = org_module.__class__.__name__\n        # Iterate through the loaded networks\n        for loaded_network in org_module._loaded_networks:\n            # Check if the loaded network is for the current layer\n            if loaded_network['layer_name'] == layer_name:\n                # Get the device to cast the input to\n                device = loaded_network['device']\n                # Cast the input to the appropriate device\n                input = input.to(device)\n                # Restore the module's weights from backup\n                network_restore_weights_from_backup(loaded_network['network'], org_module)\n                # Reset the module's cached weights\n                network_reset_cached_weight(org_module)\n                # Call the original forward method\n                output = original_forward(org_module, input)\n                # Apply the loaded network to the output\n                output = loaded_network['network'](output)\n                # Return the output\n                return output\n    # If no loaded networks match the current layer, return the original forward method's output\n    return original_forward(org_module, input)\n", "CodeBleu_score": 37.849686813377545}, {"code": "def network_forward(org_module, input, original_forward)\n\t\n    if not _network_loaded():\n        return original_forward(input)\n    device = input.device\n    input = input.to(_network_device())\n    network_restore_weights_from_backup(org_module)\n    network_reset_cached_weight(org_module)\n    output = original_forward(input)\n    for net in _network_loaded():\n        if net.layer == org_module:\n            output = net(output)\n    return output.to(device)\n", "CodeBleu_score": 31.807690786542953}, {"code": "def network_forward(org_module, input, original_forward)\n\t\n    if not hasattr(org_module, 'networks'):\n        return original_forward(input)\n    devices = org_module.networks\n    device = input.device\n    if device not in devices:\n        return original_forward(input)\n    networks = devices[device]\n    if not networks:\n        return original_forward(input)\n    if hasattr(org_module, 'network_backup'):\n        network_restore_weights_from_backup(org_module)\n    else:\n        network_reset_cached_weight(org_module)\n    output = original_forward(input)\n    for network in networks:\n        if network.layer == org_module:\n            output = network(output)\n    return output\n", "CodeBleu_score": 37.39693158980494}]}
{"instance_id": "1070211f5c11", "generation_result": [{"code": "def trainables(self)\n\t\n    trainables = []\n    for module in self.modules():\n        if isinstance(module, (nn.Linear, nn.LayerNorm)):\n            trainables.extend(list(module.parameters()))\n    return trainables\n\n", "CodeBleu_score": 28.530259457956053}, {"code": "def trainables(self)\n\t\n    trainable_params = []\n    for name, param in self.named_parameters():\n        if 'weight' in name and 'layer_norm' not in name:\n            trainable_params.append(param)\n        elif 'bias' in name:\n            trainable_params.append(param)\n    return trainable_params\n\n", "CodeBleu_score": 24.470934038337646}, {"code": "def trainables(self)\n\t\n    return [p for p in self.parameters() if p.requires_grad]\n\n", "CodeBleu_score": 6.452447410697191}]}
{"instance_id": "0c3ba62c96a7", "generation_result": [{"code": "def load_hypernetworks(names, multipliers=None)\n\t\n    global already_loaded\n    global hypernetworks\n    already_loaded = []\n    hypernetworks = []\n    for name in names:\n        hypernetwork = load_hypernetwork(name)\n        if multipliers is not None:\n            hypernetwork.set_multiplier(multipliers[names.index(name)])\n        already_loaded.append(name)\n        hypernetworks.append(hypernetwork)\n    return hypernetworks\n\n", "CodeBleu_score": 28.350710438030045}, {"code": "def load_hypernetworks(names, multipliers=None)\n\t\n    global already_loaded\n    if already_loaded is None:\n        already_loaded = []\n    for name in names:\n        if name not in already_loaded:\n            already_loaded.append(name)\n            hypernetwork = load_hypernetwork(name)\n            if multipliers is not None:\n                hypernetwork.set_multiplier(multipliers[name])\n            hypernetworks.append(hypernetwork)\n    return hypernetworks\n", "CodeBleu_score": 30.950330528551074}, {"code": "def load_hypernetworks(names, multipliers=None)\n\t\n    global already_loaded\n    if already_loaded is None:\n        already_loaded = []\n    else:\n        already_loaded.clear()\n    if multipliers is None:\n        multipliers = [1.0] * len(names)\n    for name, multiplier in zip(names, multipliers):\n        if name in already_loaded:\n            hypernetwork = already_loaded[name]\n        else:\n            hypernetwork = load_hypernetwork(name)\n            already_loaded[name] = hypernetwork\n        hypernetwork.set_multiplier(multiplier)\n    already_loaded.extend(hypernetworks)\n\n", "CodeBleu_score": 32.223559540892886}]}
{"instance_id": "bea3772ed532", "generation_result": [{"code": "\ndef apply_hypernetworks(hypernetworks, context, layer=None)\n\t\n    context_k = context\n    context_v = context\n    for hypernetwork in hypernetworks:\n        context_k, context_v = apply_single_hypernetwork(hypernetwork, context_k, context_v, layer=layer)\n    return context_k, context_v\n\n\n", "CodeBleu_score": 64.82264135585403}, {"code": "\ndef apply_hypernetworks(hypernetworks, context, layer=None)\n\t\n    context_k, context_v = context, context\n    for hn in hypernetworks:\n        context_k, context_v = apply_single_hypernetwork(hn, context_k, context_v, layer)\n    return context_k, context_v\n\n\n", "CodeBleu_score": 59.292318895742234}, {"code": "\ndef apply_hypernetworks(hypernetworks, context, layer=None)\n\t\n    context_k, context_v = context\n    for hn in hypernetworks:\n        context_k, context_v = apply_single_hypernetwork(hn, context_k, context_v, layer)\n    return context_k, context_v\n\n", "CodeBleu_score": 60.101028025938895}]}
{"instance_id": "922efa3e092f", "generation_result": [{"code": "def attention_CrossAttention_forward(self, x, context=None, mask=None, **kwargs)\n\t\n    if context is None:\n        context = x\n    # Project the query, key, and value.\n    q, k, v = self.to_q(x), self.to_k(context), self.to_v(context)\n    # Apply the hypernetworks to the context.\n    k_proj = self.to_k_proj(context)\n    v_proj = self.to_v_proj(context)\n    # Reshape the tensors for multi-head attention.\n    q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=self.heads), (q, k, v))\n    k_proj, v_proj = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=self.heads), (k_proj, v_proj))\n    # Calculate the attention and apply the mask (if provided).\n    attn = sim.masked_softmax(sim.einsum('b h i d, b h j d -> b h i j', q, k_proj), mask=mask)\n    out = sim.einsum('b h i j, b h j d -> b h i d', attn, v_proj)\n    # Reshape the output and apply the final linear layer.\n    out = rearrange(out, 'b h n d -> b n (h d)')\n    return self.to_out(out)\n", "CodeBleu_score": 42.85143106382656}, {"code": "def attention_CrossAttention_forward(self, x, context=None, mask=None, **kwargs)\n\t\n    # Extract some shapes and compute the number of heads\n    batch_size, n_q, d_q = x.shape\n    n_k = context.shape[1]\n    d_k = context.shape[2]\n    n_heads = self.n_heads\n    d_v = self.d_v\n    d_out = self.d_out\n    n_out = self.n_out\n    d_model = self.d_model\n    d_in = self.d_in\n    d_in_context = self.d_in_context\n    d_in_q = self.d_in_q\n    d_in_k = self.d_in_k\n    d_in_v = self.d_in_v\n    d_in_out = self.d_in_out\n    d_in_context_out = self.d_in_context_out\n    d_in_q_out = self.d_in_q_out\n    d_in_k_out = self.d_in_k_out\n    d_in_v_out = self.d_in_v_out\n    d_in_q_context_out = self.d_in_q_context_out\n    d_in_k_context_out = self.d_in_k_context_out\n    d_in_v_context_out = self.d_in_v_context_out\n    d_in_q_k_out = self.d_in_q_k_out\n    d_in_q_v_out = self.d_in_q_v_out\n    d_in_k_v_out = self.d_in_k_v_out\n    d_in_q_k_context_out = self.d_in_q_k_context_out\n    d_in_q_v_context_out = self.d_in_q_v_context_out\n    d_in_k_v_context_out = self.d_in_k_v_context_out\n    d_in_q_k_v_out = self.d_in_q_k_v_out\n    d_in_q_k_v_context_out = self.d_in_q_k_", "CodeBleu_score": 9.04847928603104}, {"code": "def attention_CrossAttention_forward(self, x, context=None, mask=None, **kwargs)\n\t\n    if context is None:\n        context = x\n\n    # Compute query, key, and value tensors from the input and context.\n    q, k, v = self.project_qkv(x, context)\n\n    # Apply hypernetworks to the context.\n    k_proj = self.to_k(context)\n    v_proj = self.to_v(context)\n\n    # Reshape tensors for multi-head attention.\n    q = self.reshape_q(q)\n    k = self.reshape_k(k_proj)\n    v = self.reshape_v(v_proj)\n\n    # Calculate scaled dot-product attention.\n    attn = self.attn(q, k, v, mask=mask)\n\n    # Reshape the output and apply a final linear layer.\n    out = self.to_out(attn)\n\n    return out\n\n", "CodeBleu_score": 20.676167104701488}]}
{"instance_id": "8b7455a03a0e", "generation_result": [{"code": "def save_hypernetwork(hypernetwork, checkpoint, hypernetwork_name, filename)\n\t\n    # Backup original attributes\n    original_attributes = dict()\n    original_attributes['hypernetwork_name'] = hypernetwork.hypernetwork_name\n    original_attributes['filename'] = hypernetwork.filename\n    original_attributes['hypernetwork_state_dict'] = hypernetwork.state_dict()\n    original_attributes['hypernetwork_optimizer_state_dict'] = hypernetwork.optimizer.state_dict()\n    original_attributes['hypernetwork_loss_function'] = hypernetwork.loss_function\n    original_attributes['hypernetwork_loss_function_state_dict'] = hypernetwork.loss_function.state_dict()\n    original_attributes['hypernetwork_loss_function_optimizer_state_dict'] = hypernetwork.loss_function.optimizer.state_dict()\n    original_attributes['hypernetwork_is_cuda'] = hypernetwork.is_cuda\n    original_attributes['hypernetwork_device'] = hypernetwork.device\n    original_attributes['hypernetwork_dtype'] = hypernetwork.dtype\n    original_attributes['hypernetwork_parallel_mode'] = hypernetwork.parallel_mode\n    original_attributes['hypernetwork_forward_requires_grad'] = hypernetwork.forward_requires_grad\n    original_attributes['hypernetwork_is_double_precision'] = hypernetwork.is_double_precision\n    original_attributes['hypernetwork_is_verbose'] = hypernetwork.is_verbose\n    original_attributes['hypernetwork_is_quiet'] = hypernetwork.is_quiet\n    original_attributes['hypernetwork_is_debug'] = hypernetwork.is_debug\n    original_attributes['hypernetwork_is_backup'] = hypernetwork.is_backup\n    original_attributes['hypernetwork_is_plotting'] = hypernetwork.is_plotting\n    original_attributes['hypernetwork_is_saving'] = hypernetwork.is_saving\n    original_attributes['hypernetwork_is_loading'] = hypernetwork.is_loading\n    original_attributes['hypernetwork_is_resetting'] = hypernetwork.is_resetting\n    original_attributes['hypernetwork_is_initializing'] = hypernetwork.is_initializing\n    original_attributes['hypernetwork_is_recurrent'] = hypernetwork.is_recurrent\n    original_attributes['hypernetwork_is_pruning'] = hypernetwork.is_pruning\n    original_attributes['hypernetwork_is_growing'] =", "CodeBleu_score": 12.182971735107484}, {"code": "def save_hypernetwork(hypernetwork, checkpoint, hypernetwork_name, filename)\n\t\n    # Save a backup of the original attributes\n    checkpoint_backup = {}\n    for attr in hypernetwork.checkpoint_attrs:\n        checkpoint_backup[attr] = getattr(hypernetwork, attr)\n\n    # Update the hypernetwork with the checkpoint data\n    for attr in hypernetwork.checkpoint_attrs:\n        setattr(hypernetwork, attr, checkpoint[attr])\n\n    # Save the hypernetwork\n    try:\n        hypernetwork.save(filename)\n    except Exception as e:\n        # Restore the original attributes\n        for attr in hypernetwork.checkpoint_attrs:\n            setattr(hypernetwork, attr, checkpoint_backup[attr])\n\n        # Re-raise the exception\n        raise e\n\n", "CodeBleu_score": 25.107386236813078}, {"code": "def save_hypernetwork(hypernetwork, checkpoint, hypernetwork_name, filename)\n\t\n    try:\n        hypernetwork_state = hypernetwork.state_dict()\n        hypernetwork.load_state_dict(checkpoint['hypernetwork_state_dict'])\n        hypernetwork.save(filename)\n        hypernetwork.load_state_dict(hypernetwork_state)\n    except Exception as e:\n        hypernetwork.load_state_dict(hypernetwork_state)\n        raise e\n\n", "CodeBleu_score": 22.71579746448163}]}
{"instance_id": "3175f835140f", "generation_result": [{"code": "def ema_scope(self, context=None)\n\t\n    if context is None:\n        context = get_current_context()\n    if context.ema_enabled:\n        if context.ema_model is None:\n            context.ema_model = context.model.copy()\n        context.model.load_state_dict(context.ema_model.state_dict())\n    yield\n    if context.ema_enabled:\n        if context.ema_model is not None:\n            context.ema_model.load_state_dict(context.model.state_dict())\n            context.ema_model = None\n        if context.ema_log:\n            print(\"EMA weights restored.\")\n\n", "CodeBleu_score": 38.40052384800523}, {"code": "def ema_scope(self, context=None)\n\t\n    if context is None:\n        context = self._context\n    if context.ema_enabled:\n        # Store the original weights\n        original_weights = self.get_weights()\n        # Copy EMA weights to the model\n        self.set_weights(context.ema_weights)\n        # Log the copy operation\n        if context.ema_logging:\n            self.log_ema_copy()\n        # Execute the code block\n        yield\n        # Restore the original weights\n        self.set_weights(original_weights)\n        # Log the restoration\n        if context.ema_logging:\n            self.log_ema_restore()\n    else:\n        yield\n\n", "CodeBleu_score": 35.64012074661787}, {"code": "def ema_scope(self, context=None)\n\t\n    if context is None:\n        context = get_context()\n    if context.ema_enabled:\n        # store current weights\n        if context.ema_store_weights:\n            context.ema_store_weights(context, self)\n        # copy EMA weights to the model\n        context.ema_copy_to(context, self)\n        # log the action\n        if context.ema_log_action:\n            context.ema_log_action(context, self, 'ema_scope')\n    # execute the code block\n    yield\n    # restore the original weights\n    if context.ema_enabled:\n        context.ema_restore(context, self)\n        # log the restoration\n        if context.ema_log_action:\n            context.ema_log_action(context, self, 'ema_scope', 'restore')\n\n", "CodeBleu_score": 32.04102091425106}]}
{"instance_id": "a7b5d5cd519a", "generation_result": [{"code": "def p_mean_variance(self, x, t, clip_denoised: bool)\n\t\n    # get model output for time step t\n    model_output = self.model(x, t)\n\n    # reconstruct input from model output\n    if self.predict_start_from_noise:\n        x_recon = self.reconstruct_from_noise(model_output)\n    else:\n        x_recon = self.reconstruct(model_output)\n\n    # clip the reconstructed input to [-1, 1]\n    if clip_denoised:\n        x_recon = x_recon.clamp_(-1.0, 1.0)\n\n    # get mean and variance of posterior distribution\n    mean, log_variance = self.q_posterior(model_output)\n    variance = log_variance.exp()\n\n    return x_recon, mean, variance, log_variance\n\n", "CodeBleu_score": 39.87983196750592}, {"code": "def p_mean_variance(self, x, t, clip_denoised: bool)\n\t\n    x_recon = self.model(x, t)\n    if self.predict_start_from_noise:\n        x_recon = self.q_posterior.sample(x_recon.size())\n    if clip_denoised:\n        x_recon = x_recon.clamp_(-1, 1)\n    model_mean, posterior_variance = self.q_posterior(x_recon, t)\n    return model_mean, posterior_variance, posterior_variance.log()\n", "CodeBleu_score": 38.55740109442362}, {"code": "def p_mean_variance(self, x, t, clip_denoised: bool)\n\t\n    # get model output\n    output = self.model(x, t)\n    # get predicted noise\n    if self.predict_start_from_noise:\n        predicted_noise = self.q_posterior(output, t, reverse=True)\n    else:\n        predicted_noise = 0\n    # reconstruct the input from the model output\n    x_recon = self.model.reverse(output, t, noise=predicted_noise)\n    # clip the reconstructed input to [-1, 1]\n    if clip_denoised:\n        x_recon = x_recon.clamp_(-1, 1)\n    # get the posterior distribution for the input\n    p_x = self.q_posterior(x_recon, t)\n    # return the mean, variance, and log variance\n    return p_x.mean, p_x.variance, p_x.log_variance\n", "CodeBleu_score": 42.5633049263748}]}
{"instance_id": "a8652fa78473", "generation_result": [{"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss_type == 'l1':\n        if mean:\n            return torch.mean(torch.abs(pred - target))\n        else:\n            return torch.abs(pred - target)\n    elif self.loss_type == 'l2':\n        if mean:\n            return torch.mean((pred - target) ** 2)\n        else:\n            return (pred - target) ** 2\n    else:\n        raise NotImplementedError\n\n", "CodeBleu_score": 33.16912388958236}, {"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss_type == 'l1':\n        if mean:\n            return torch.abs(pred - target).mean()\n        else:\n            return torch.abs(pred - target)\n    elif self.loss_type == 'l2':\n        if mean:\n            return F.mse_loss(pred, target)\n        else:\n            return F.mse_loss(pred, target, reduction='none')\n    else:\n        raise NotImplementedError\n\n", "CodeBleu_score": 37.94268928617853}, {"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss_type == 'l1':\n        loss = torch.abs(pred - target)\n        if mean:\n            loss = loss.mean()\n    elif self.loss_type == 'l2':\n        loss = (pred - target) ** 2\n        if mean:\n            loss = loss.mean()\n        else:\n            loss = loss.sum()\n    else:\n        raise NotImplementedError\n    return loss\n", "CodeBleu_score": 50.737571082517164}]}
{"instance_id": "d68e4f061349", "generation_result": [{"code": "def _prior_bpd(self, x_start)\n\t\n    batch_size = x_start.shape[0]\n    t = torch.tensor(self.num_timesteps - 1).repeat(batch_size)\n    mean, log_variance = self.q_mean_variance(x_start, t)\n    kl_divergence = normal_kl(mean, log_variance)\n    return kl_divergence.sum() / np.log(2.0)\n", "CodeBleu_score": 23.722323604517868}, {"code": "def _prior_bpd(self, x_start)\n\t\n    batch_size = x_start.shape[0]\n    t = torch.ones(batch_size, device=x_start.device, dtype=torch.long) * (self.T - 1)\n    mean, log_var = self.q_mean_variance(x_start, t)\n    kl = self.normal_kl(mean, log_var)\n    return kl.mean() / np.log(2.0)\n\n", "CodeBleu_score": 29.34320384657151}, {"code": "def _prior_bpd(self, x_start)\n\t\n    batch_size = x_start.shape[0]\n    t = torch.tensor(self.num_timesteps - 1).repeat(batch_size).to(self.device)\n    q_mean, q_log_variance = self.q_mean_variance(x_start, t)\n    prior_mean = torch.zeros_like(q_mean)\n    prior_log_variance = torch.zeros_like(q_log_variance)\n    kl = normal_kl(q_mean, q_log_variance, prior_mean, prior_log_variance)\n    return torch.mean(kl / torch.log(torch.tensor(2.0)))\n", "CodeBleu_score": 28.814808585971146}]}
{"instance_id": "c695d93fdf2d", "generation_result": [{"code": "def make_cache(subsection: str) -> diskcache.Cache\n\t\n    cache_dir = Path.cwd() / \"cache\" / subsection\n    cache_dir.mkdir(parents=True, exist_ok=True)\n    return diskcache.Cache(\n        cache_dir, size_limit=4 * 1024 ** 3, disk_min_file_size=256 * 1024\n    )\n\n", "CodeBleu_score": 52.07147882863686}, {"code": "def make_cache(subsection: str) -> diskcache.Cache\n\t\n    cache_path = os.path.join(cache_dir, subsection)\n    return diskcache.Cache(cache_path, size_limit=4 * 1024 ** 3, disk_min_file_size=256)\n\n", "CodeBleu_score": 48.841999094535446}, {"code": "def make_cache(subsection: str) -> diskcache.Cache\n\t\n    cache_dir = os.path.join(CACHE_DIR, subsection)\n    return diskcache.Cache(cache_dir, size_limit=4 * 1024 ** 3, disk_min_file_size=256 * 1024)\n\n", "CodeBleu_score": 51.44856529505344}]}
{"instance_id": "ed4c543f0d38", "generation_result": [{"code": "def cache(subsection)\n\t\n    global caches, cache_lock\n    with cache_lock:\n        if subsection in caches:\n            return caches[subsection]\n        else:\n            if not isdir(cache_dir):\n                make_cache(subsection)\n            elif not isfile(cache_filename):\n                convert_old_cached_data()\n            if subsection in caches:\n                return caches[subsection]\n            else:\n                caches[subsection] = make_cache(subsection)\n                return caches[subsection]\n\n", "CodeBleu_score": 33.69844216153157}, {"code": "def cache(subsection)\n\t\n    # Check if the cache object already exists in the global caches dictionary.\n    if subsection in caches:\n        return caches[subsection]\n\n    # Acquire a lock to ensure thread-safety.\n    with cache_lock:\n        # Check if the cache object already exists in the global caches dictionary.\n        if subsection in caches:\n            return caches[subsection]\n\n        # Check if the cache directory exists and if the cache filename is a valid file.\n        if not isdir(cache_dir):\n            # Create the cache directory if it does not exist.\n            makedirs(cache_dir)\n        elif not isfile(cache_file):\n            # Convert legacy cached data.\n            convert_old_cached_data()\n\n        # Re-check if the cache object already exists in the global caches dictionary.\n        if subsection in caches:\n            return caches[subsection]\n\n        # Create a new cache object.\n        caches[subsection] = make_cache(subsection)\n\n        return caches[subsection]\n\n", "CodeBleu_score": 35.922751745564184}, {"code": "def cache(subsection)\n\t\n    cache_lock.acquire()\n    if subsection in caches:\n        cache_lock.release()\n        return caches[subsection]\n    if not isdir(cache_dir):\n        makedirs(cache_dir)\n    if not isfile(cache_filename):\n        convert_old_cached_data()\n    if subsection in caches:\n        cache_lock.release()\n        return caches[subsection]\n    caches[subsection] = make_cache(subsection)\n    cache_lock.release()\n    return caches[subsection]\n\n", "CodeBleu_score": 36.68804570962066}]}
{"instance_id": "ccf5ea2d9fc1", "generation_result": [{"code": "def cached_data_for_file(subsection, title, filename, func)\n\t_dict.\"\"\"\n    cache = cache(subsection)\n    if title in cache:\n        entry = cache[title]\n        mtime = getmtime(filename)\n        if mtime > entry.get('mtime', 0):\n            del cache[title]\n    if title not in cache:\n        data_dict = func()\n        cache[title] = {\n            'mtime': mtime,\n            'data': data_dict,\n        }\n        dump_cache(cache)\n    return cache[title]['data']\n", "CodeBleu_score": 35.04002047216993}, {"code": "def cached_data_for_file(subsection, title, filename, func)\n\t_dict.\"\"\"\n    cache = cache(subsection)\n    entry = cache.get(title)\n    mtime = getmtime(filename)\n    if entry is not None:\n        if entry.get('mtime') is not None and entry.get('mtime') >= mtime:\n            return entry.get('data')\n    data = func()\n    entry = {\n        'mtime': mtime,\n        'data': data\n    }\n    cache[title] = entry\n    dump_cache(cache)\n    return data\n", "CodeBleu_score": 37.42513985563986}, {"code": "def cached_data_for_file(subsection, title, filename, func)\n\t_dict.\"\"\"\n    cache_dict = cache(subsection)\n    if title in cache_dict:\n        entry = cache_dict[title]\n        mtime = getmtime(filename)\n        if entry.get('mtime', 0) >= mtime:\n            return entry['data']\n    data = func()\n    cache_dict[title] = {\n        'data': data,\n        'mtime': getmtime(filename),\n    }\n    dump_cache(subsection, cache_dict)\n    return data\n", "CodeBleu_score": 34.70655520229101}]}
{"instance_id": "e01a2d4f0ac2", "generation_result": [{"code": "def report(message: str, *, exc_info: bool = False) -> None\n\t\n    record_exception(exc_info=exc_info)\n    print(\"***\", file=sys.stderr)\n    for line in message.splitlines():\n        print(line, file=sys.stderr)\n    if exc_info:\n        print(\"---\", file=sys.stderr)\n        print(textwrap.indent(traceback.format_exc(), \"    \"), file=sys.stderr)\n\n", "CodeBleu_score": 58.06366578132657}, {"code": "def report(message: str, *, exc_info: bool = False) -> None\n\t\n    record_exception()\n    print(\"***\", file=sys.stderr)\n    for line in message.splitlines():\n        print(\"*** \" + line, file=sys.stderr)\n    if exc_info:\n        print(\"***\", file=sys.stderr)\n        print(textwrap.indent(traceback.format_exc(), \"    \"), file=sys.stderr)\n        print(\"***\", file=sys.stderr)\n\n", "CodeBleu_score": 59.084648547537896}, {"code": "def report(message: str, *, exc_info: bool = False) -> None\n\t\n    record_exception(exc_info=exc_info)\n    for line in message.splitlines():\n        print(\"*** \" + line, file=sys.stderr)\n    if exc_info:\n        print(textwrap.indent(traceback.format_exc(), \"    \"), file=sys.stderr)\n        print(\"---\", file=sys.stderr)\n\n", "CodeBleu_score": 69.85132840281992}]}
{"instance_id": "125fb9d2320b", "generation_result": [{"code": "def extract_original_prompts(style: PromptStyle, prompt, negative_prompt)\n\t\n    if style.positive_prompt == \"\" and style.negative_prompt == \"\":\n        return False, prompt, negative_prompt\n    positive_prompt = extract_style_text_from_prompt(style.positive_prompt, prompt)\n    if positive_prompt == False:\n        return False, prompt, negative_prompt\n    negative_prompt = extract_style_text_from_prompt(style.negative_prompt, negative_prompt)\n    if negative_prompt == False:\n        return False, prompt, negative_prompt\n    return True, positive_prompt, negative_prompt\n", "CodeBleu_score": 58.451945788308436}, {"code": "def extract_original_prompts(style: PromptStyle, prompt, negative_prompt)\n\t\n\n    if style.prompt == \"\" and style.negative_prompt == \"\":\n        return False, prompt, negative_prompt\n\n    try:\n        positive_prompt = extract_style_text_from_prompt(style, prompt)\n    except:\n        return False, prompt, negative_prompt\n\n    try:\n        negative_prompt = extract_style_text_from_prompt(style, negative_prompt)\n    except:\n        return False, prompt, negative_prompt\n\n    return True, positive_prompt, negative_prompt\n\n", "CodeBleu_score": 50.59355254635619}, {"code": "def extract_original_prompts(style: PromptStyle, prompt, negative_prompt)\n\t\n    if style.prompt == \"\" or style.negative_prompt == \"\":\n        return False, prompt, negative_prompt\n    try:\n        positive_prompt = extract_style_text_from_prompt(style.prompt, prompt)\n    except:\n        return False, prompt, negative_prompt\n    try:\n        negative_prompt = extract_style_text_from_prompt(style.negative_prompt, negative_prompt)\n    except:\n        return False, prompt, negative_prompt\n    return True, positive_prompt, negative_prompt\n\n", "CodeBleu_score": 55.778343636784314}]}
{"instance_id": "0c5f729fbce2", "generation_result": [{"code": "def random_partial(self, count, n_frames)\n\t\n    if not self.utterances:\n        self._load_utterances()\n    utterances = self.utterance_cycler.sample(count)\n    return [(u, u.random_partial(n_frames)) for u in utterances]\n\n", "CodeBleu_score": 55.89879214825755}, {"code": "def random_partial(self, count, n_frames)\n\t\n    if not self.utterances:\n        self._load_utterances()\n    utterances = self.utterance_cycler.sample(count)\n    return [(u, u.random_partial(n_frames)) for u in utterances]\n", "CodeBleu_score": 55.89879214825755}, {"code": "def random_partial(self, count, n_frames)\n\t\n    if not self.utterances:\n        self._load_utterances()\n    return [(u, u.random_partial(n_frames)) for u in self.utterance_cycler.sample(count)]\n", "CodeBleu_score": 42.93472535513073}]}
{"instance_id": "31e8b27b8f94", "generation_result": [{"code": "def normalize_volume(wav, target_dBFS, increase_only=False, decrease_only=False)\n\t\n    if increase_only and decrease_only:\n        raise ValueError(\"Both increase only and decrease only are set.\")\n    dBFS_change = target_dBFS - 10 * np.log10(np.mean(wav ** 2))\n    if (dBFS_change < 0 and increase_only) or (dBFS_change > 0 and decrease_only):\n        return wav\n    return wav * (10 ** (dBFS_change / 20))\n\n", "CodeBleu_score": 94.76607381503017}, {"code": "def normalize_volume(wav, target_dBFS, increase_only=False, decrease_only=False)\n\t\n    if increase_only and decrease_only:\n        raise ValueError(\"Both increase only and decrease only are set\")\n    RMS = np.sqrt(np.mean(wav ** 2))\n    change_in_dBFS = target_dBFS - 10 * np.log10(RMS)\n    if (change_in_dBFS < 0 and increase_only) or (change_in_dBFS > 0 and decrease_only):\n        return wav\n    return wav * (10 ** (change_in_dBFS / 20))\n\n", "CodeBleu_score": 75.34841795640982}, {"code": "def normalize_volume(wav, target_dBFS, increase_only=False, decrease_only=False)\n\t\n    if increase_only and decrease_only:\n        raise ValueError(\"Both increase only and decrease only are set\")\n    dBFS_change = target_dBFS - 10 * np.log10(np.mean(wav ** 2))\n    if (dBFS_change < 0 and increase_only) or (dBFS_change > 0 and decrease_only):\n        return wav\n    return wav * (10 ** (dBFS_change / 20))\n\n", "CodeBleu_score": 96.28790819755086}]}
{"instance_id": "2353831e9033", "generation_result": [{"code": "def load_model(weights_fpath: Path, device=None)\n\t\n    if device is None:\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Loading model on device {device}.\")\n    _model = SpeakerEncoder()\n    _model.to(device)\n    _model.eval()\n    _model.load_state_dict(torch.load(weights_fpath, map_location=device))\n    print(f\"Loaded model {weights_fpath}.\")\n    return _model\n\n", "CodeBleu_score": 40.42292782798252}, {"code": "def load_model(weights_fpath: Path, device=None)\n\t\n    global _model\n    if device is None:\n        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    _model = SpeakerEncoder(device)\n    _model.load_state_dict(torch.load(weights_fpath))\n    _model.eval()\n    print(f'Loaded model: {weights_fpath}')\n    print(f'Model training step: {_model.step}')\n\n", "CodeBleu_score": 37.76592794653417}, {"code": "def load_model(weights_fpath: Path, device=None)\n\t\n    global _model\n    if device is None:\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    _model = SpeakerEncoder(device=device)\n    _model.load_state_dict(torch.load(weights_fpath))\n    _model.eval()\n    print(f\"Loaded model: {_model}", "CodeBleu_score": 38.78979553784198}]}
{"instance_id": "3231ad480199", "generation_result": [{"code": "def embed_frames_batch(frames_batch)\n\t\n    if _model is None:\n        raise Exception(\"The model is not loaded. You need to load the model first using the function load_model().\")\n    frames_batch_tensor = torch.from_numpy(frames_batch)\n    frames_batch_tensor = frames_batch_tensor.to(_device)\n    with torch.no_grad():\n        embeddings = _model.forward(frames_batch_tensor)\n    embeddings = embeddings.detach()\n    embeddings = embeddings.cpu()\n    embeddings = embeddings.numpy()\n    return embeddings\n", "CodeBleu_score": 49.8045260250713}, {"code": "def embed_frames_batch(frames_batch)\n\t\n    if _model is None:\n        raise Exception(\"Model not loaded.\")\n    frames_batch = torch.from_numpy(frames_batch)\n    frames_batch = frames_batch.to(_device)\n    with torch.no_grad():\n        embeddings = _model.forward(frames_batch).detach().cpu().numpy()\n    return embeddings\n", "CodeBleu_score": 66.48449736699217}, {"code": "def embed_frames_batch(frames_batch)\n\t\n    if _model is None:\n        raise Exception('Model not loaded')\n    frames_batch = torch.from_numpy(frames_batch).to(_device)\n    with torch.no_grad():\n        embeddings = _model(frames_batch).detach().cpu().numpy()\n    return embeddings\n\n", "CodeBleu_score": 61.04790110772982}]}
{"instance_id": "117a41716c8d", "generation_result": [{"code": "def compute_partial_slices(n_samples, partial_utterance_n_frames=partials_n_frames,\n                           min_pad_coverage=0.75, overlap=0.5)\n\t\n    assert 0.0 < overlap < 1.0, \"overlap must be between 0.0 and 1.0\"\n    assert 0.0 < min_pad_coverage < 1.0, \"min_pad_coverage must be between 0.0 and 1.0\"\n    assert 0 < partial_utterance_n_frames <= n_samples, \"partial utterance n_frames must be greater than 0 and less than or equal to n_samples\"\n    # Compute number of frames and frame step\n    n_frames = int(np.ceil(partial_utterance_n_frames))\n    frame_step = int(np.ceil(partial_utterance_n_frames - overlap * partial_utterance_n_frames))\n    # Compute mel and wav slices\n    wav_slices = []\n    mel_slices = []\n    for mel_start in range(0, n_samples - n_frames + 1, frame_step):\n        mel_end = mel_start + n_frames\n        wav_start = int(np.floor(mel_start / hop_length))\n        wav_end = int(np.ceil(mel_end / hop_length))\n        # Ensure coverage is at least min_pad_coverage\n        pad_len = max(0, wav_end - wav_slices[-1][1]) if len(wav_slices) > 0 else 0\n        wav_pad_coverage = (wav_end - wav_start + pad_len) / (wav_end - wav_start)\n        mel_pad_coverage = (mel_end - mel_start + pad_len) / (mel_end - mel_start)\n        assert mel_pad_coverage >= min_pad_coverage, \"Coverage below min_pad_coverage\"\n        # Append slices\n        wav_slices.append([wav_start, wav_end])\n        mel_slices.append([mel_start, mel_end])\n    # Remove last slice if coverage is below min_pad_coverage\n    if len(wav_slices) > 1 and wav_pad_coverage < min_pad_coverage:\n        wav_slices.pop(-1)\n        mel_slices.pop(-1)\n    return wav_slices, mel_slices\n\n", "CodeBleu_score": 32.27402060010383}, {"code": "def compute_partial_slices(n_samples, partial_utterance_n_frames=partials_n_frames,\n                           min_pad_coverage=0.75, overlap=0.5)\n\t\n    assert 0 < overlap < 1, \"Overlap must be between 0 and 1!\"\n    assert 0 < min_pad_coverage < 1, \"Minimum pad coverage must be between 0 and 1!\"\n    assert 0 < partial_utterance_n_frames < n_samples, \"Partial utterance length must be less than total utterance length!\"\n    frame_step = int(np.ceil(partial_utterance_n_frames * (1.0 - overlap)))\n    n_frames = int(np.ceil((n_samples - partial_utterance_n_frames) / frame_step)) + 1\n    wav_slices = []\n    mel_slices = []\n    for i in range(n_frames):\n        mel_range = np.array([i * frame_step, i * frame_step + partial_utterance_n_frames])\n        wav_range = mel_range * audio_processing.hparams.hop_size\n        wav_slices.append(slice(*wav_range.astype(int)))\n        mel_slices.append(slice(*mel_range.astype(int)))\n    last_wav_range = wav_slices[-1]\n    last_mel_range = mel_slices[-1]\n    last_wav_coverage = (last_wav_range.stop - last_wav_range.start) / float(n_samples)\n    last_mel_coverage = (last_mel_range.stop - last_mel_range.start) / float(partial_utterance_n_frames)\n    assert 0 <= last_wav_coverage <= 1, \"Wav coverage must be between 0 and 1!\"\n    assert 0 <= last_mel_coverage <= 1, \"Mel coverage must be between 0 and 1!\"\n    if last_wav_coverage < min_pad_coverage and len(wav_slices) > 1:\n        wav_slices = wav_slices[:-1]\n        mel_slices = mel_slices[:-1]\n    return wav_slices, mel_slices\n", "CodeBleu_score": 42.056142711658126}, {"code": "def compute_partial_slices(n_samples, partial_utterance_n_frames=partials_n_frames,\n                           min_pad_coverage=0.75, overlap=0.5)\n\t\n    assert 0.0 <= min_pad_coverage <= 1.0, \"min_pad_coverage must be between 0.0 and 1.0\"\n    assert 0.0 < overlap < 1.0, \"overlap must be between 0.0 and 1.0\"\n    assert n_samples > 0, \"n_samples must be greater than 0\"\n    assert partial_utterance_n_frames > 0, \"partial_utterance_n_frames must be greater than 0\"\n    frame_step = int(np.ceil(n_samples * (1.0 - overlap)))  # Frame step\n    assert frame_step > 0, \"frame_step must be greater than 0\"\n    n_frames = int(np.ceil(1.0 * n_samples / frame_step))  # Number of frames\n    assert n_frames > 0, \"n_frames must be greater than 0\"\n    assert n_frames >= partial_utterance_n_frames, \"n_frames must be greater than partial_utterance_n_frames\"\n    # Compute the coverage for partial utterances\n    coverage = (n_frames - partial_utterance_n_frames + 1) * frame_step / n_samples\n    # If coverage is less than min_pad_coverage, we need to add padding\n    if coverage < min_pad_coverage:\n        # Compute the number of padding frames\n        pad = int(np.ceil((min_pad_coverage * n_frames - frame_step * (n_frames - partial_utterance_n_frames)) / (1.0 - overlap)))\n        # If coverage is less than min_pad_coverage, it is because pad is 0\n        if pad == 0:\n            # If coverage is less than min_pad_coverage and pad is 0, it is because frame_step is too big\n            # We need to reduce frame_step\n            frame_step = int(np.ceil(n_samples * (1.0 - min_pad_coverage)))\n            # Recompute the number of frames and the coverage\n            n_frames = int(np.ceil(1.0 * n_samples / frame_step))\n            coverage = (n_frames - partial_utterance_n_frames + 1) * frame_step / n_samples\n           ", "CodeBleu_score": 22.883657058471087}]}
{"instance_id": "34fe5c864f1c", "generation_result": [{"code": "def plot_embedding_as_heatmap(embed, ax=None, title=\"\", shape=None, color_range=(0, 0.30))\n\t\n    if ax is None:\n        ax = plt.gca()\n    if shape is None:\n        height = int(np.sqrt(len(embed)))\n        shape = (height, height)\n    embed = embed.reshape(shape)\n    cmap = cm.get_cmap('Reds')\n    norm = Normalize(vmin=color_range[0], vmax=color_range[1])\n    sm = cm.ScalarMappable(cmap=cmap, norm=norm)\n    sm.set_array([])\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.set_title(title)\n    ax.imshow(embed, cmap=cmap, norm=norm)\n    sm.set_clim(color_range)\n    plt.colorbar(sm)\n    return ax\n\n", "CodeBleu_score": 65.75254016339565}, {"code": "def plot_embedding_as_heatmap(embed, ax=None, title=\"\", shape=None, color_range=(0, 0.30))\n\t\n    if ax is None:\n        ax = plt.gca()\n    if shape is None:\n        height = int(np.sqrt(len(embed)))\n        shape = (height, height)\n    embed = embed.reshape(shape)\n    ax.imshow(embed, cmap=cm.get_cmap(\"Reds\"), vmin=color_range[0], vmax=color_range[1])\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.set_title(title)\n    cbar = plt.colorbar(cm.ScalarMappable(cmap=cm.get_cmap(\"Reds\")), ax=ax)\n    cbar.ax.tick_params(labelsize=14)\n    plt.show()\n    return ax\n", "CodeBleu_score": 62.51364362591464}, {"code": "def plot_embedding_as_heatmap(embed, ax=None, title=\"\", shape=None, color_range=(0, 0.30))\n\t\n    if ax is None:\n        ax = plt.gca()\n    if shape is None:\n        shape = int(np.sqrt(len(embed)))\n        embed = embed.reshape(shape, shape)\n    sm = cm.ScalarMappable(cmap=cm.get_cmap(\"RdBu_r\"))\n    sm.set_clim(color_range[0], color_range[1])\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.imshow(embed, cmap=sm.get_cmap(), clim=color_range)\n    ax.set_title(title)\n    cbar = plt.colorbar(sm)\n    cbar.set_ticks(np.linspace(color_range[0], color_range[1], 5))\n    plt.show()\n", "CodeBleu_score": 56.59930315689655}]}
{"instance_id": "92e93d50b62b", "generation_result": [{"code": "def similarity_matrix(self, embeds)\n\t\n    # Get the number of speakers and utterances per speaker\n    num_speakers, num_utts = embeds.shape[:2]\n\n    # Initialize the similarity matrix and the mask matrix\n    sim_mat = torch.zeros((num_speakers, num_speakers), device=embeds.device)\n    mask_mat = torch.ones((num_speakers, num_speakers), device=embeds.device)\n\n    # Compute the centroids\n    centroids_incl = torch.mean(embeds, dim=1)\n    centroids_incl = centroids_incl / torch.norm(centroids_incl, dim=1, keepdim=True)\n    centroids_excl = torch.sum(embeds, dim=1)\n    centroids_excl = centroids_excl / (num_utts - 1)\n    centroids_excl = centroids_excl / torch.norm(centroids_excl, dim=1, keepdim=True)\n\n    # Compute the similarity matrix\n    for i in range(num_speakers):\n        sim_mat[i] = torch.cosine_similarity(embeds[i], centroids_incl, dim=1)\n        sim_mat[i] += torch.cosine_similarity(embeds[i], centroids_excl, dim=1)\n        mask_mat[i, i] = 0\n\n    # Scale and bias the similarity matrix\n    sim_mat = sim_mat * self.scale + self.bias\n    sim_mat = sim_mat * mask_mat\n\n    return sim_mat\n\n", "CodeBleu_score": 35.71559703687717}, {"code": "def similarity_matrix(self, embeds)\n\t\n    # Determine the number of speakers and utterances per speaker from the shape of the input embeddings\n    num_speakers, num_utts = embeds.shape[0], embeds.shape[1]\n\n    # Initialize a similarity matrix with zeros on the specified device and a mask matrix to exclude self-similarity\n    sim_mat = torch.zeros(num_speakers, num_speakers, device=embeds.device)\n    mask = torch.ones(num_speakers, num_speakers, device=embeds.device)\n    mask = mask.fill_diagonal_(0)\n\n    # Compute the centroids for the embeddings\n    centroids_incl = torch.mean(embeds, dim=1)\n    centroids_incl = centroids_incl / torch.norm(centroids_incl, dim=1).unsqueeze(1)\n    centroids_excl = torch.zeros(embeds.shape[0], embeds.shape[2], device=embeds.device)\n    for i in range(embeds.shape[0]):\n        centroids_excl[i] = torch.mean(embeds[i][embeds[i] != 0], dim=0)\n    centroids_excl = centroids_excl / torch.norm(centroids_excl, dim=1).unsqueeze(1)\n\n    # Compute the similarity matrix for each speaker\n    for i in range(num_speakers):\n        sim_mat[i] = torch.matmul(embeds[i], centroids_incl.T)\n        sim_mat[i] += torch.matmul(embeds[i], centroids_excl[i].unsqueeze(0).T)\n        sim_mat[i] /= 2\n        sim_mat[i] -= 1\n        sim_mat[i] *= mask[i]\n\n    # Scale and bias the similarity matrix\n    sim_mat = torch.exp(self.alpha * sim_mat)\n    sim_mat = sim_mat / torch.sum(sim_mat, dim=1).unsqueeze(1)\n    return sim_mat\n\n", "CodeBleu_score": 30.509342068152534}, {"code": "def similarity_matrix(self, embeds)\n\t\n    batch_size, num_speakers, num_utterances, embed_dim = embeds.shape\n    centroids_incl = torch.zeros(batch_size, num_speakers, embed_dim).to(embeds.device)\n    centroids_excl = torch.zeros(batch_size, num_speakers, embed_dim).to(embeds.device)\n    mask = torch.ones(batch_size, num_speakers, num_speakers).to(embeds.device)\n    for i in range(batch_size):\n        for j in range(num_speakers):\n            # Calculate the number of utterances per speaker\n            num_uttrs = (embeds[i, j] != 0).sum(dim=0)\n            # Calculate the mean utterance embedding per speaker\n            centroids_incl[i, j] = torch.sum(embeds[i, j], dim=0) / num_uttrs\n            # Calculate the mean utterance embedding per speaker excluding the current speaker\n            centroids_excl[i, j] = torch.sum(embeds[i, j, :num_uttrs-1], dim=0) / (num_uttrs-1)\n            # Calculate the similarity scores per speaker\n            sim_scores = 1 - torch.norm(centroids_incl[i, j] - embeds[i, j], dim=1) / torch.norm(centroids_excl[i, j] - embeds[i, j, :num_uttrs-1], dim=1)\n            # Update the similarity matrix\n            mask[i, j, :num_speakers-1] = 0\n            sim_matrix = torch.zeros(num_speakers, num_speakers).to(embeds.device)\n            sim_matrix[j, :num_speakers-1] = sim_scores\n            sim_matrix[:num_speakers-1, j] = sim_scores\n            sim_matrix[j, j] = 0\n            sim_matrix[:num_speakers-1, :num_speakers-1] = sim_matrix[:num_speakers-1, :num_speakers-1] * mask[i]\n            sim_matrix = sim_matrix + torch.eye(num_speakers).to(embeds.device) *", "CodeBleu_score": 29.03132556045498}]}
{"instance_id": "a6eba54108a1", "generation_result": [{"code": "def loss(self, embeds)\n\t\n    # Get the number of speakers and utterances per speaker from the shape of the input embeddings.\n    num_speakers, num_utts = embeds.shape[0], embeds.shape[1]\n    # Compute the similarity matrix.\n    sim_matrix = self.similarity_matrix(embeds)\n    # Reshape the similarity matrix.\n    sim_matrix = sim_matrix.reshape(num_speakers * num_utts, num_speakers * num_utts)\n    # Generate the ground truth labels.\n    labels = np.repeat(np.arange(num_speakers), num_utts)\n    # Calculate the loss.\n    loss = self.loss_fn(sim_matrix, torch.from_numpy(labels))\n    # Compute the EER.\n    eer = self.compute_eer(sim_matrix.detach().cpu().numpy(), labels)\n    return loss, eer\n", "CodeBleu_score": 10.92014226558246}, {"code": "def loss(self, embeds)\n\t\n    num_speakers, num_utts_per_speaker = embeds.shape[:2]\n    sim_matrix = self.similarity_matrix(embeds)\n    sim_matrix = sim_matrix.reshape(num_speakers, num_utts_per_speaker, -1)\n    sim_matrix = sim_matrix.detach().cpu().numpy()\n\n    # Generate the ground truth labels\n    labels = np.repeat(np.arange(num_speakers), num_utts_per_speaker)\n\n    # Calculate the loss\n    loss = self.loss_fn(torch.from_numpy(sim_matrix), torch.from_numpy(labels))\n\n    # Calculate the Equal Error Rate (EER)\n    fpr, tpr, thresholds = roc_curve(labels, sim_matrix.flatten())\n    eer = brentq(lambda x: 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n\n    return loss, eer\n", "CodeBleu_score": 45.86460162282975}, {"code": "def loss(self, embeds)\n\t\n    # Calculate the number of speakers and utterances per speaker\n    num_speakers, num_uttrs = embeds.shape[:2]\n\n    # Calculate the similarity matrix\n    sim_matrix = self.similarity_matrix(embeds)\n\n    # Reshape the similarity matrix\n    sim_matrix = sim_matrix.reshape(num_speakers, num_uttrs, num_uttrs)\n\n    # Generate the ground truth labels\n    labels = np.repeat(np.arange(num_speakers), num_uttrs)\n\n    # Calculate the loss\n    loss = self.loss_fn(sim_matrix, torch.from_numpy(labels).to(self.device))\n\n    # Calculate the Equal Error Rate (EER)\n    with torch.no_grad():\n        # Calculate the ROC curve\n        fpr, tpr, thresholds = roc_curve(labels, sim_matrix.detach().cpu().numpy().flatten())\n\n        # Find the threshold where the false positive rate equals the false negative rate\n        eer = brentq(lambda x : 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n\n    return loss, eer\n", "CodeBleu_score": 45.04260624995792}]}
{"instance_id": "ac7b74b9d1b1", "generation_result": [{"code": "def log_params(self)\n\t\n    if not self.disable_params:\n        import inspect\n        import pandas as pd\n        from IPython.display import display\n        from .visualizations import show_text\n        params = inspect.getmembers(self, lambda a: not(inspect.isroutine(a)))\n        params = [a for a in params if not(a[0].startswith('__') and a[0].endswith('__'))]\n        params = [a for a in params if not(a[0].startswith('_'))]\n        params = [a for a in params if not(a[0].endswith('_'))]\n        params = [a for a in params if not(a[0].endswith('__'))]\n        params = [a for a in params if not(a[0].startswith('__'))]\n        params = [a for a in params if not(a[0].startswith('self'))]\n        params = [a for a in params if not(a[0].startswith('disable_params'))]\n        params = [a for a in params if not(a[0].startswith('disable_param_logging'))]\n        params = [a for a in params if not(a[0].startswith('disable_data_logging'))]\n        params = [a for a in params if not(a[0].startswith('disable_model_logging'))]\n        params = [a for a in params if not(a[0].startswith('disable_training_logging'))]\n        params = [a for a in params if not(a[0].startswith('disable_prediction_logging'))]\n        params = [a for a in params if not(a[0].startswith('disable_evaluation_logging'))]\n        params = [a for a in params if not(a[0].startswith('disable_all_logging'))]\n        params = [a for a in params if not(a[0].startswith('disable_logging'))]\n        params = [a for a in params if not(a[0].startswith('enable_params'))]\n        params = [a for a in params if not(a[0].startswith('enable_param_logging'))]\n        params = [a for a in params if not(a[0].startswith('enable_data_logging'))]\n        params = [a for a in params if not(a[0].startswith('enable_model_logging'))]\n        params = [a", "CodeBleu_score": 19.681252619476226}, {"code": "def log_params(self)\n\t\n    if self.log_params:\n        # Import parameter configurations\n        params = self.params\n        # Create a formatted string of model and data parameters\n        params_str = 'Model Parameters:", "CodeBleu_score": 3.3445471153665194}, {"code": "def log_params(self)\n\t\n    if self.params['display_params']:\n        from IPython.display import display, Markdown\n        display(Markdown(\"### Parameters\"))\n        params = \"\"\n        for key, value in self.params.items():\n            params += \"**{}**: {}", "CodeBleu_score": 12.296392857031357}]}
{"instance_id": "4726b7864386", "generation_result": [{"code": "def draw_projections(self, embeds, utterances_per_speaker, step, out_fpath=None, max_speakers=10)\n\t\n    if out_fpath is not None:\n        plt.ioff()\n    if embeds.shape[0] > max_speakers:\n        embeds = embeds[:max_speakers]\n    embeds = embeds.reshape(embeds.shape[0], -1)\n    reducer = umap.UMAP()\n    embeds = reducer.fit_transform(embeds)\n    colors = np.repeat(np.arange(embeds.shape[0]), utterances_per_speaker)\n    plt.scatter(embeds[:, 0], embeds[:, 1], c=colors, cmap=\"Spectral\", s=1)\n    plt.gca().set_aspect(\"equal\", \"datalim\")\n    plt.title(f\"Step {step}\")\n    if out_fpath is not None:\n        plt.savefig(out_fpath)\n        plt.close()\n    else:\n        plt.draw()\n        plt.pause(0.001)\n\n", "CodeBleu_score": 42.73892218845269}, {"code": "def draw_projections(self, embeds, utterances_per_speaker, step, out_fpath=None, max_speakers=10)\n\t\n    if self.verbose:\n        print(\"Drawing projections...\")\n\n    # Limit the number of speakers\n    if len(embeds) > max_speakers:\n        embeds = embeds[:max_speakers]\n\n    # Flatten the embeddings\n    flat_embeds = np.array([embed for embeds_per_speaker in embeds for embed in embeds_per_speaker])\n\n    # Assign colors based on speaker identity\n    colors = [self.speaker_ids[i % len(self.speaker_ids)] for i in range(len(flat_embeds))]\n\n    # Perform UMAP projection\n    reducer = umap.UMAP()\n    projected = reducer.fit_transform(flat_embeds)\n\n    # Draw the plot\n    plt.scatter(projected[:, 0], projected[:, 1], c=colors, s=1)\n    plt.gca().set_aspect('equal', 'datalim')\n    plt.title(f\"Step {step}: {len(embeds)} Speakers\")\n    plt.draw()\n    plt.pause(0.001)\n\n    # Save the plot\n    if out_fpath is not None:\n        plt.savefig(out_fpath)\n        if self.verbose:\n            print(f\"Saved projections to {out_fpath}\")\n\n    # Clear the plot\n    plt.clf()\n\n", "CodeBleu_score": 49.49422138111266}, {"code": "def draw_projections(self, embeds, utterances_per_speaker, step, out_fpath=None, max_speakers=10)\n\t\n    # Limit the number of speakers to visualize\n    if len(embeds) > max_speakers:\n        embeds = embeds[:max_speakers]\n\n    # Flatten the embeddings\n    flat_embeds = np.array([embed.flatten() for embed in embeds])\n\n    # Create a UMAP reducer\n    reducer = umap.UMAP()\n\n    # Reduce the dimensionality of the embeddings\n    reduced_embeds = reducer.fit_transform(flat_embeds)\n\n    # Assign colors based on speaker identity\n    colors = [self.speaker_id_to_color[speaker_id] for speaker_id in self.speaker_ids[:len(embeds)]]\n\n    # Create a plot\n    fig, ax = plt.subplots(figsize=(12, 12))\n\n    # Draw the projections\n    plt.scatter(reduced_embeds[:, 0], reduced_embeds[:, 1], c=colors)\n\n    # Set the aspect ratio to be equal\n    ax.set_aspect('equal', 'datalim')\n\n    # Set the plot title\n    plt.title(f\"Step {step}: {len(embeds)} Speakers\")\n\n    # Save the plot to a file\n    if out_fpath is not None:\n        plt.savefig(out_fpath)\n\n    # Clear the plot\n    plt.clf()\n\n    # Show the plot\n    plt.show()\n", "CodeBleu_score": 47.39456941040229}]}
{"instance_id": "274fa50985c0", "generation_result": [{"code": "def add_speaker_embedding(self, x, speaker_embedding)\n\t\n    # save input tensor dimensions\n    input_shape = x.shape\n    input_dim = len(input_shape)\n    # determine appropriate dimension index for speaker embedding\n    if input_dim == 2:\n        speaker_embedding_dim = 0\n    elif input_dim == 3:\n        speaker_embedding_dim = 1\n    else:\n        raise ValueError(\n            \"Input tensor must have 2 or 3 dimensions, but tensor has {} dimensions.\".format(\n                input_dim\n            )\n        )\n    # repeat speaker embedding to match the length of the input text\n    repeat_count = x.shape[speaker_embedding_dim]\n    speaker_embedding = speaker_embedding.repeat_interleave(repeat_count, dim=0)\n    # reshape and transpose speaker embedding\n    speaker_embedding = speaker_embedding.reshape(\n        speaker_embedding.shape[0], speaker_embedding.shape[2]\n    )\n    speaker_embedding = speaker_embedding.transpose(0, 1)\n    # concatenate speaker embedding with the input tensor\n    x = torch.cat([x, speaker_embedding], dim=input_dim)\n    return x\n\n", "CodeBleu_score": 32.38396545234208}, {"code": "def add_speaker_embedding(self, x, speaker_embedding)\n\t\n    if speaker_embedding is None:\n        return x\n    x_dim = x.dim()\n    speaker_embedding_dim = speaker_embedding.dim()\n    if x_dim != 2:\n        raise RuntimeError(\"The input tensor has to have two dimensions.\")\n    if speaker_embedding_dim == 1:\n        speaker_embedding = speaker_embedding.unsqueeze(0)\n    elif speaker_embedding_dim > 2:\n        raise RuntimeError(\"The speaker embedding has to have one or two dimensions.\")\n    batch_size = x.size(0)\n    speaker_embedding = speaker_embedding.repeat_interleave(batch_size, dim=0)\n    speaker_embedding = speaker_embedding.reshape(batch_size, -1)\n    speaker_embedding = speaker_embedding.transpose(0, 1)\n    x = torch.cat((x, speaker_embedding), dim=x_dim - 1)\n    return x\n", "CodeBleu_score": 35.95330831623366}, {"code": "def add_speaker_embedding(self, x, speaker_embedding)\n\t\n    # Save input tensor dimensions\n    batch_size, seq_len, hidden_size = x.size()\n    # Determine speaker embedding dimension index\n    speaker_embedding_dim = hidden_size\n    if self.config.n_speakers > 1:\n        speaker_embedding_dim += self.config.n_speakers - 1\n    # Repeat the speaker embedding to match the length of the input text\n    speaker_embedding = speaker_embedding.unsqueeze(1).repeat(1, seq_len, 1)\n    # Reshape and transpose the speaker embedding\n    speaker_embedding = speaker_embedding.reshape(batch_size * seq_len, speaker_embedding_dim)\n    speaker_embedding = speaker_embedding.transpose(0, 1)\n    # Concatenate the speaker embedding with the input tensor\n    x = torch.cat((x, speaker_embedding), dim=1)\n    return x\n", "CodeBleu_score": 35.23851748659279}]}
{"instance_id": "e6742ea0fe5e", "generation_result": [{"code": "def init_attention(self, encoder_seq_proj)\n\t\n    self.device = next(self.parameters()).device\n    self.encoder_seq_proj_shape = encoder_seq_proj.size()\n    self.cumulative = torch.zeros(self.encoder_seq_proj_shape[0], self.encoder_seq_proj_shape[1], self.encoder_seq_proj_shape[2]).to(self.device)\n    self.attention = torch.zeros(self.encoder_seq_proj_shape[0], self.encoder_seq_proj_shape[1], self.encoder_seq_proj_shape[2]).to(self.device)\n", "CodeBleu_score": 19.23895260819238}, {"code": "def init_attention(self, encoder_seq_proj)\n\t\n    self.device = next(self.parameters()).device\n    self.batch_size = encoder_seq_proj.size(0)\n    self.seq_len = encoder_seq_proj.size(1)\n    self.cumulative_attentions = torch.zeros(self.batch_size, self.seq_len, device=self.device)\n    self.attentions = torch.zeros(self.batch_size, self.seq_len, device=self.device)\n\n", "CodeBleu_score": 25.044562520680063}, {"code": "def init_attention(self, encoder_seq_proj)\n\t\n    self.device = self.parameters[0].device\n    self.encoder_seq_proj_shape = encoder_seq_proj.size()\n    self.cumulative_attention = torch.zeros(self.encoder_seq_proj_shape[0], self.encoder_seq_proj_shape[1], device=self.device)\n    self.attention = torch.zeros(self.encoder_seq_proj_shape[0], self.encoder_seq_proj_shape[1], device=self.device)\n\n", "CodeBleu_score": 17.760179011986878}]}
{"instance_id": "4e1fc1ed0ed7", "generation_result": [{"code": "def load(self, path, optimizer=None)\n\t\n    if not os.path.isfile(path):\n        raise FileNotFoundError(f\"File {path} not found\")\n    checkpoint = torch.load(path, map_location=self.device)\n    self.load_state_dict(checkpoint[\"state_dict\"])\n    if optimizer is not None:\n        optimizer.load_state_dict(checkpoint[\"optimizer\"])\n    return checkpoint\n", "CodeBleu_score": 46.021123232075766}, {"code": "def load(self, path, optimizer=None)\n\t\n    if not os.path.isfile(path):\n        raise FileNotFoundError(f\"The file {path} does not exist\")\n    checkpoint = torch.load(path)\n    self.load_state_dict(checkpoint[\"state_dict\"])\n    if optimizer is not None:\n        optimizer.load_state_dict(checkpoint[\"optimizer\"])\n    self.to(self.device)\n", "CodeBleu_score": 42.96596522468565}, {"code": "def load(self, path, optimizer=None)\n\t\n    checkpoint = torch.load(path, map_location=self.device)\n    self.load_state_dict(checkpoint['model_state_dict'])\n    if optimizer:\n        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n\n", "CodeBleu_score": 28.42176354288886}]}
{"instance_id": "178e6aa16c0f", "generation_result": [{"code": "def save(self, path, optimizer=None)\n\t\n    if optimizer is not None:\n        torch.save({\n            'state_dict': self.state_dict(),\n            'optimizer': optimizer.state_dict(),\n        }, path)\n    else:\n        torch.save(self.state_dict(), path)\n", "CodeBleu_score": 63.11340099925056}, {"code": "def save(self, path, optimizer=None)\n\t\n    if optimizer is not None:\n        torch.save({\n            'model_state_dict': self.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict()\n        }, path)\n    else:\n        torch.save(self.state_dict(), path)\n", "CodeBleu_score": 63.81618480133533}, {"code": "def save(self, path, optimizer=None)\n\t\n    if optimizer is not None:\n        torch.save({\n            'model_state_dict': self.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n        }, path)\n    else:\n        torch.save({\n            'model_state_dict': self.state_dict(),\n        }, path)\n\n", "CodeBleu_score": 69.29600967425841}]}
{"instance_id": "6528fd80e87b", "generation_result": [{"code": "def english_cleaners(text)\n\t\n    text = convert_to_ascii(text)\n    text = lowercase(text)\n    text = expand_numbers(text)\n    text = expand_abbreviations(text)\n    text = collapse_whitespace(text)\n    return text\n", "CodeBleu_score": 55.88611500274403}, {"code": "def english_cleaners(text)\n\t\n    text = convert_to_ascii(text)\n    text = lowercase(text)\n    text = expand_numbers(text)\n    text = expand_abbreviations(text)\n    text = collapse_whitespace(text)\n    return text\n", "CodeBleu_score": 55.88611500274403}, {"code": "def english_cleaners(text)\n\t\n    text = convert_to_ascii(text)\n    text = lowercase(text)\n    text = expand_numbers(text)\n    text = expand_abbreviations(text)\n    text = collapse_whitespace(text)\n    return text\n\n", "CodeBleu_score": 55.88611500274403}]}
{"instance_id": "8a4fe08e0985", "generation_result": [{"code": "def text_to_sequence(text, cleaner_names)\n\t\n    sequence = []\n    # Check for curly braces and treat their contents as ARPAbet:\n    while len(text):\n        m = _curly_re.match(text)\n        if not m:\n            sequence += _symbols_to_sequence(_clean_text(text, cleaner_names))\n            break\n        sequence += _symbols_to_sequence(_clean_text(m.group(1), cleaner_names))\n        sequence += _arpabet_to_sequence(m.group(2))\n        text = m.group(3)\n    # Append EOS token\n    sequence.append(_symbol_to_id['~'])\n    return sequence\n\n", "CodeBleu_score": 78.05719062473496}, {"code": "def text_to_sequence(text, cleaner_names)\n\t\n    sequence = []\n    # Check for curly braces and treat their contents as ARPAbet:\n    while len(text):\n        m = _curly_re.match(text)\n        if not m:\n            sequence += _symbols_to_sequence(_clean_text(text, cleaner_names))\n            break\n        sequence += _symbols_to_sequence(_clean_text(m.group(1), cleaner_names))\n        sequence += _arpabet_to_sequence(m.group(2))\n        text = m.group(3)\n    # Append EOS token\n    sequence.append(_symbol_to_id[text_to_sequence.eos_token])\n    return sequence\n\n", "CodeBleu_score": 74.46374797655515}, {"code": "def text_to_sequence(text, cleaner_names)\n\t\n    sequence = []\n    # Check for curly braces and treat their contents as ARPAbet:\n    while len(text):\n        m = _curly_re.match(text)\n        if not m:\n            sequence += _symbols_to_sequence(_clean_text(text, cleaner_names))\n            break\n        sequence += _symbols_to_sequence(_clean_text(m.group(1), cleaner_names))\n        sequence += _arpabet_to_sequence(m.group(2))\n        text = m.group(3)\n    # Append EOS token\n    sequence.append(_symbol_to_id['~'])\n    return sequence\n\n", "CodeBleu_score": 78.05719062473496}]}
{"instance_id": "e1c25a37af1f", "generation_result": [{"code": "def sequence_to_text(sequence)\n\t\n    result = ''\n    for symbol_id in sequence:\n        if symbol_id in _symbol_to_id:\n            s = _id_to_symbol[symbol_id]\n            # Enclose ARPAbet back in curly braces:\n            if len(s) > 1 and s[0] == '@':\n                s = '{%s}' % s[1:]\n            result += s\n    return result.replace('}{', ' ')\n\n", "CodeBleu_score": 66.81999362331848}, {"code": "def sequence_to_text(sequence)\n\t\n    result = ''\n    for symbol_id in sequence:\n        if symbol_id in _id_to_symbol:\n            s = _id_to_symbol[symbol_id]\n            # Enclose ARPAbet back in curly braces:\n            if len(s) > 1 and s[0] == '@':\n                s = '{%s}' % s[1:]\n            result += s\n    return result.replace('}{', ' ')\n\n", "CodeBleu_score": 69.32718912956324}, {"code": "def sequence_to_text(sequence)\n\t\n    result = ''\n    for symbol_id in sequence:\n        if symbol_id in _symbol_to_id:\n            s = _id_to_symbol[symbol_id]\n            # Enclose ARPAbet back in curly braces:\n            if len(s) > 1 and s[0] == '@':\n                s = '{%s}' % s[1:]\n            result += s\n    return result.replace('}{', ' ')\n\n", "CodeBleu_score": 66.81999362331848}]}
{"instance_id": "7807cad4b05d", "generation_result": [{"code": "def embed_utterance(fpaths, encoder_model_fpath)\n\t\n    if not encoder.is_loaded():\n        encoder.load_model(encoder_model_fpath)\n\n    for fpath in fpaths:\n        wav = encoder.preprocess_wav(fpath)\n        embed = encoder.embed_utterance(wav)\n        fname = os.path.basename(fpath).split(\".\")[0]\n        np.save(os.path.join(embed_dir, fname + \".npy\"), embed)\n\n", "CodeBleu_score": 43.043082444726444}, {"code": "def embed_utterance(fpaths, encoder_model_fpath)\n\t\n    if not encoder.is_loaded():\n        encoder.load_model(encoder_model_fpath)\n\n    for fpath in fpaths:\n        wav = encoder.preprocess_wav(fpath)\n        embed = encoder.embed_utterance(wav)\n        fname = os.path.basename(fpath)\n        np.save(os.path.join(os.path.dirname(fpath), fname.split(\".\")[0]), embed, allow_pickle=False)\n\n", "CodeBleu_score": 44.00656855367935}, {"code": "def embed_utterance(fpaths, encoder_model_fpath)\n\t\n    # Load the encoder model if it is not already loaded\n    if not encoder.is_loaded():\n        encoder.load_model(encoder_model_fpath)\n\n    # Load and preprocess the waveform\n    wav = encoder.preprocess_wav(fpaths[0])\n\n    # Compute the speaker embedding\n    embed = encoder.embed_utterance(wav)\n\n    # Save the embedding\n    np.save(fpaths[1], embed, allow_pickle=False)\n\n", "CodeBleu_score": 54.514070421817884}]}
{"instance_id": "9f860273a325", "generation_result": [{"code": "def create_embeddings(synthesizer_root: Path, encoder_model_fpath: Path, n_processes: int)\n\t\n    print(\"Preparing the encoder, the synthesizer and the vocoder...\")\n    encoder.load_model(encoder_model_fpath)\n    print(\"Loaded encoder \\\"%s\\\" trained to step %d\" % (encoder_model_fpath, encoder.steps))\n    synthesizer = Synthesizer(synthesizer_root.joinpath(\"taco_pretrained\"))\n    vocoder = inference.Vocoder(synthesizer_root.joinpath(\"saved_models/vocoder\"))\n    print(\"Loaded synthesizer \\\"%s\\\" trained to step %d\" % (synthesizer_root, synthesizer.steps))\n    print(\"Loaded vocoder \\\"%s\\\" trained to step %d\" % (synthesizer_root, vocoder.steps))\n    print(\"Preparing the encoder, the synthesizer and the vocoder... Done\")\n    print(\"Preparing the encoder, the synthesizer and the vocoder... Done\")\n    print(\"Preparing the encoder, the synthesizer and the vocoder... Done\")\n    print(\"Preparing the encoder, the synthesizer and the vocoder... Done\")\n    print(\"Preparing the encoder, the synthesizer and the vocoder... Done\")\n    print(\"Preparing the encoder, the synthesizer and the vocoder... Done\")\n    print(\"Preparing the encoder, the synthesizer and the vocoder... Done\")\n    print(\"Preparing the encoder, the synthesizer and the vocoder... Done\")\n    print(\"Preparing the encoder, the synthesizer and the vocoder... Done\")\n    print(\"Preparing the encoder, the synthesizer and the vocoder... Done\")\n    print(\"Preparing the encoder, the synthesizer and the vocoder... Done\")\n    print(\"Preparing the encoder, the synthesizer and the vocoder... Done\")\n    print(\"Preparing the encoder, the synthesizer and the vocoder... Done\")\n    print(\"Preparing the encoder, the synthesizer and the vocoder... Done\")\n    print(\"Preparing the encoder, the synthesizer and the vocoder... Done\")\n    print(\"Preparing the encoder, the synthesizer and the vocoder... Done\")\n    print(\"Preparing the encoder, the synthesizer and the vocoder... Done\")\n    print(\"Preparing the encoder, the synthesizer and the vocoder... Done\")\n    print(\"Preparing the", "CodeBleu_score": 19.9399015440951}, {"code": "def create_embeddings(synthesizer_root: Path, encoder_model_fpath: Path, n_processes: int)\n\t\n    metadata_fpath = synthesizer_root.joinpath(\"train.txt\")\n    wav_dir = synthesizer_root.joinpath(\"audio\")\n    embed_dir = synthesizer_root.joinpath(\"embeds\")\n    embed_dir.mkdir(exist_ok=True)\n    assert metadata_fpath.exists() and metadata_fpath.is_file(), \"train.txt file is missing\"\n    assert wav_dir.exists() and wav_dir.is_dir(), \"audio directory is missing\"\n    print(\"Generating embeddings...\")\n    with metadata_fpath.open(\"r\", encoding=\"utf-8\") as metadata_file:\n        metadata = [line.split(\"|\") for line in metadata_file]\n    wav_fpaths = [wav_dir.joinpath(m[0] + \".wav\") for m in metadata]\n    embed_fpaths = [embed_dir.joinpath(m[0] + \".npy\") for m in metadata]\n    assert len(wav_fpaths) == len(\n        embed_fpaths\n    ), \"number of wav files does not match number of embedding files\"\n    print(f\"Found {len(wav_fpaths)} audio files\")\n    if n_processes <= 1:\n        for wav_fpath, embed_fpath in tqdm(zip(wav_fpaths, embed_fpaths), total=len(wav_fpaths)):\n            create_embedding(wav_fpath, embed_fpath, encoder_model_fpath)\n    else:\n        pool = Pool(n_processes)\n        list(\n            tqdm(\n                pool.imap(partial(create_embedding, encoder_model_fpath=encoder_model_fpath), wav_fpaths),\n                total=len(wav_fpaths),\n            )\n        )\n    print(\"Embeddings generated!\")\n\n", "CodeBleu_score": 54.92016153192898}, {"code": "def create_embeddings(synthesizer_root: Path, encoder_model_fpath: Path, n_processes: int)\n\t\n    assert encoder_model_fpath.exists(), f\"Encoder model '{encoder_model_fpath}' does not exist.\"\n    encoder.load_model(encoder_model_fpath)\n    metadata_fpath = synthesizer_root.joinpath(\"train.txt\")\n    assert metadata_fpath.exists(), f\"Metadata '{metadata_fpath}' does not exist. Run synthesizer_preprocess_audio first.\"\n    wav_dir = synthesizer_root.joinpath(\"audio\")\n    assert wav_dir.exists(), f\"Audio directory '{wav_dir}' does not exist. Run synthesizer_preprocess_audio first.\"\n    embed_dir = synthesizer_root.joinpath(\"embeds\")\n    if not embed_dir.exists():\n        embed_dir.mkdir()\n    metadata = [line.split(\"|\") for line in metadata_fpath.read_text().splitlines()[1:]]\n    wav_fpaths = [wav_dir.joinpath(m[0] + \".wav\") for m in metadata]\n    embed_fpaths = [embed_dir.joinpath(m[0] + \".npy\") for m in metadata]\n    print(\"Creating embeddings...\")\n    _embed_fpaths = partial(_embed_fpath, encoder_model_fpath=encoder_model_fpath)\n    with Pool(n_processes) as pool:\n        list(tqdm(pool.imap(_embed_fpaths, zip(wav_fpaths, embed_fpaths)), total=len(metadata)))\n\n", "CodeBleu_score": 50.347734681637355}]}
{"instance_id": "9eb0abda14a9", "generation_result": [{"code": "def eval_model(attention, mel_prediction, target_spectrogram, input_seq, step,\n               plot_dir, mel_output_dir, wav_dir, sample_num, loss, hparams)\n\t\n    # Save attention plot\n    attention_plot_name = 'attention_step{}_sample_{}'.format(step, sample_num)\n    save_attention(attention, attention_plot_name, plot_dir)\n\n    # Save predicted mel spectrogram to disk (log scale)\n    mel_output_filename = 'mel-prediction-step{}_sample_{}.npy'.format(step, sample_num)\n    np.save(str(mel_output_dir.joinpath(mel_output_filename)), mel_prediction.T, allow_pickle=False)\n\n    # Save griffin lim inverted wav for debug\n    wav = audio.inv_mel_spectrogram(mel_prediction.T, hparams)\n    wav_filename = 'step{}-wave-from-mel_sample_{}.wav'.format(step, sample_num)\n    audio.save_wav(wav, wav_dir.joinpath(wav_filename), sr=hparams.sample_rate)\n\n    # Save real and predicted mel-spectrogram plot to disk\n    spec_filename = 'step{}_sample_{}_spec.png'.format(step, sample_num)\n    plot_spectrogram(mel_prediction, str(plot_dir.joinpath(spec_filename)), title='Predicted Mel-Spectrogram')\n    spec_filename = 'step{}_sample_{}_spec_target.png'.format(step, sample_num)\n    plot_spectrogram(target_spectrogram, str(plot_dir.joinpath(spec_filename)), title='Target Mel-Spectrogram')\n\n    # Save plot of predicted vs target mel-spectrogram to disk\n    plot_filename = 'step{}_sample_{}_mel_plot.png'.format(step, sample_num)\n    plot_spectrogram(mel_prediction, str(plot_dir.joinpath(plot_filename)), target_spectrogram=target_spectrogram,\n                     title='Predicted vs Target Mel-Spectrogram, Loss=%.3f' % loss)\n\n    # Print input sequence to disk\n    sequence_filename = 'step{}_sample_{}_input_seq.txt'.format(step, sample_num)\n    with open(str(plot_dir.joinpath(sequence_filename)), 'w') as file:\n        file.write(sequence_to_text(input_seq))\n\n", "CodeBleu_score": 45.55947774989365}, {"code": "def eval_model(attention, mel_prediction, target_spectrogram, input_seq, step,\n               plot_dir, mel_output_dir, wav_dir, sample_num, loss, hparams)\n\t\n    # Save the predicted mel spectrogram to disk (debug)\n    mel_output_filename = \"mel-prediction-step-{}-sample-{}.npy\".format(step, sample_num)\n    np.save(str(mel_output_dir.joinpath(mel_output_filename)), mel_prediction, allow_pickle=False)\n\n    # Save the attention plot to disk\n    attention_plot_filename = \"attention_step_{}_sample_{}.png\".format(step, sample_num)\n    save_attention(attention, str(plot_dir.joinpath(attention_plot_filename)))\n\n    # Save the predicted wav (generated from mel-spectrogram) to disk (for audio)\n    predicted_wav = audio.inv_mel_spectrogram(mel_prediction.T, hparams)\n    predicted_wav_filename = \"predicted-waveform-step-{}-sample-{}.wav\".format(step, sample_num)\n    audio.save_wav(predicted_wav, str(wav_dir.joinpath(predicted_wav_filename)), sr=hparams.sample_rate)\n\n    # Save the target wav to disk (for audio)\n    target_wav = audio.inv_mel_spectrogram(target_spectrogram.T, hparams)\n    target_wav_filename = \"target-waveform-step-{}-sample-{}.wav\".format(step, sample_num)\n    audio.save_wav(target_wav, str(wav_dir.joinpath(target_wav_filename)), sr=hparams.sample_rate)\n\n    # Save the mel-spectrogram plot to disk (control purposes)\n    spec_filename = \"step-{}-mel-spectrogram-sample-{}.png\".format(step, sample_num)\n    plot_spectrogram(mel_prediction, str(plot_dir.joinpath(spec_filename)), title=\"{}, {}, step={}, loss={:.5f}\".format(time_string(), hparams.model_type, step, loss))\n\n    # Save the mel-spectrogram plot to disk (control purposes)\n    spec_filename = \"step-{}-mel-target-spectrogram-sample-{}.png\".format(step, sample_num)\n    plot_spectrogram(target_spectrogram, str(plot_dir.joinpath(spec_filename)), title=\"{}, {}, step={}, loss={", "CodeBleu_score": 45.044249655010944}, {"code": "def eval_model(attention, mel_prediction, target_spectrogram, input_seq, step,\n               plot_dir, mel_output_dir, wav_dir, sample_num, loss, hparams)\n\t\n    # Save attention plot\n    save_attention(attention, input_seq, step, plot_dir)\n    # Save predicted mel spectrogram to disk (plot and save png)\n    mel_filename = \"mel-prediction-%d-%d.npy\" % (step, sample_num)\n    np.save(mel_output_dir.joinpath(mel_filename), mel_prediction, allow_pickle=False)\n    plot_spectrogram(mel_prediction, step, plot_dir, \"Predicted Mel-Spectrogram\")\n    # Save griffin lim inverted wav for debug (plot and save wav)\n    wav = audio.inv_mel_spectrogram(mel_prediction.T, hparams)\n    wav_filename = \"step-%d-wave-from-mel-%d.wav\" % (step, sample_num)\n    audio.save_wav(wav, wav_dir.joinpath(wav_filename), sr=hparams.sample_rate)\n    # Save real and predicted mel-spectrogram plot to disk (plot and save png)\n    plot_filename = \"step-%d-mel-spectrogram-prediction-vs-target-%d.png\" % (step, sample_num)\n    plot_spectrogram(target_spectrogram, step, plot_dir, \"Target Mel-Spectrogram\",\n                     filename=plot_filename, target_spectrogram=mel_prediction)\n    # Print some values\n    print(\"eval loss for global step %d: %f\" % (step, loss))\n    print(\"Input at step %d: %s\" % (step, sequence_to_text(input_seq)))\n\n", "CodeBleu_score": 38.29930195899505}]}
{"instance_id": "6bad4f500286", "generation_result": [{"code": "def num_params(self)\n\t\n    p = filter(lambda p: p.requires_grad, self.parameters())\n    print(sum(map(lambda x: x.numel(), p))/1000000)\n", "CodeBleu_score": 21.738150843342503}, {"code": "def num_params(self)\n\t\n    p = filter(lambda p: p.requires_grad, self.parameters())\n    print('The model has {} Million parameters'.format(round(sum(p.numel() for p in p) / 1000000, 3)))\n\n", "CodeBleu_score": 29.446570362875523}, {"code": "def num_params(self)\n\t\n    print(f\"Number of trainable parameters in the model: {round(sum(np.prod(p.size()) for p in self.parameters() if p.requires_grad()), 3)} M\")\n", "CodeBleu_score": 13.684843550388765}]}
{"instance_id": "78b876054bd4", "generation_result": [{"code": "def pad_tensor(self, x, pad, side='both')\n\t\n    if side == 'both':\n        new_length = x.size(1) + pad * 2\n    elif side == 'before':\n        new_length = x.size(1) + pad\n    elif side == 'after':\n        new_length = x.size(1) + pad\n    else:\n        raise Exception(\"side must be one of 'before', 'after', or 'both'.\")\n    if x.is_cuda:\n        padded = torch.cuda.FloatTensor(x.size(0), new_length, x.size(2)).fill_(0)\n    else:\n        padded = torch.zeros(x.size(0), new_length, x.size(2))\n    if side == 'both':\n        padded[:, pad:pad + x.size(1), :] = x\n    elif side == 'before':\n        padded[:, pad:pad + x.size(1), :] = x\n    elif side == 'after':\n        padded[:, :x.size(1), :] = x\n    return padded\n\n", "CodeBleu_score": 39.390814241437546}, {"code": "def pad_tensor(self, x, pad, side='both')\n\t\n    # Determine new tensor dimensions\n    if side == 'both':\n        new_dim = x.size(0) + pad * 2\n    elif side == 'before':\n        new_dim = x.size(0) + pad\n    elif side == 'after':\n        new_dim = x.size(0) + pad\n    else:\n        raise ValueError(\"side must be one of 'before', 'after', or 'both'\")\n\n    # Create the padded tensor\n    if self.is_available:\n        padded_tensor = torch.zeros(new_dim, x.size(1), x.size(2), x.size(3), device=self.device)\n    else:\n        padded_tensor = torch.zeros(new_dim, x.size(1), x.size(2), x.size(3))\n\n    # Place the original tensor in the padded tensor\n    if side == 'both':\n        padded_tensor[pad:pad + x.size(0)] = x\n    elif side == 'before':\n        padded_tensor[pad:] = x\n    elif side == 'after':\n        padded_tensor[:x.size(0)] = x\n    else:\n        raise ValueError(\"side must be one of 'before', 'after', or 'both'\")\n\n    return padded_tensor\n", "CodeBleu_score": 32.71900690644583}, {"code": "def pad_tensor(self, x, pad, side='both')\n\t\n    if side == 'both':\n        pad = (pad, pad)\n    elif side == 'before':\n        pad = (pad, 0)\n    elif side == 'after':\n        pad = (0, pad)\n    else:\n        raise Exception(\"side must be one of 'both', 'before', or 'after'\")\n\n    if self.is_available():\n        x_padded = torch.zeros(x.size(0), x.size(1), x.size(2) + pad[0] + pad[1], x.size(3)).cuda()\n    else:\n        x_padded = torch.zeros(x.size(0), x.size(1), x.size(2) + pad[0] + pad[1], x.size(3))\n\n    for i in range(x.size(0)):\n        for j in range(x.size(1)):\n            x_padded[i, j] = torch.nn.functional.pad(x[i, j], (pad[1], pad[1], pad[0], pad[0]), 'constant', 0)\n\n    return x_padded\n\n", "CodeBleu_score": 32.19363457820972}]}
{"instance_id": "a7ed1715a3df", "generation_result": [{"code": "def save_attention(attn, path)\n\t\n    import matplotlib.pyplot as plt\n    fig = plt.figure(figsize=(10, 10))\n    ax = fig.add_subplot(1, 1, 1)\n    ax.matshow(attn, cmap='viridis')\n    fontdict = {'fontsize': 14}\n    ax.set_xticklabels([''] + ['<sos>'] + [t.lower() for t in sentence] + ['<eos>'], fontdict=fontdict, rotation=90)\n    ax.set_yticklabels([''] + sentence + ['<eos>'], fontdict=fontdict)\n    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n    plt.savefig(path, format='png')\n    plt.close()\n\n", "CodeBleu_score": 43.35736906320429}, {"code": "def save_attention(attn, path)\n\t\n    import matplotlib.pyplot as plt\n    fig = plt.figure(figsize=(10, 10))\n    plt.imshow(attn, cmap='gray')\n    plt.xticks(range(len(attn)), attn.columns, fontsize=14)\n    plt.yticks(range(len(attn)), attn.index, fontsize=14)\n    plt.colorbar()\n    plt.savefig(path, bbox_inches='tight')\n    plt.close(fig)\n\n", "CodeBleu_score": 45.20459049546891}, {"code": "def save_attention(attn, path)\n\t\n    import matplotlib.pyplot as plt\n    fig = plt.figure(figsize=(10,10))\n    ax = fig.add_subplot(1,1,1)\n    cax = ax.matshow(attn, cmap='bone')\n    fig.colorbar(cax)\n    ax.set_xticklabels(['']+['<sos>']+[t.lower() for t in sentence]+['<eos>'], rotation=90)\n    ax.set_yticklabels(['']+[t.lower() for t in sentence]+['<eos>'])\n    plt.show()\n    fig.savefig(path)\n    plt.close()\n", "CodeBleu_score": 45.486187743958936}]}
{"instance_id": "19375c9f44a2", "generation_result": [{"code": "def plot(array)\n\t\n    fig = plt.figure(figsize=(10, 10))\n    ax = fig.add_subplot(111)\n    ax.tick_params(axis='both', colors='black', labelsize=15)\n    plt.plot(array, color='red')\n    plt.xlabel('x', fontsize=20)\n    plt.ylabel('y', fontsize=20)\n    plt.show()\n\n", "CodeBleu_score": 20.431302215047616}, {"code": "def plot(array)\n\t\n    fig = plt.figure(figsize=(10, 5))\n    ax = fig.add_subplot(111)\n    ax.tick_params(axis='both', colors='red', labelsize=16)\n    plt.plot(array, color='red')\n    plt.xlabel('Time (s)', color='red', fontsize=16)\n    plt.ylabel('Amplitude', color='red', fontsize=16)\n    plt.show()\n", "CodeBleu_score": 23.254819914297183}, {"code": "def plot(array)\n\t\n    fig = plt.figure(figsize=(10, 5))\n    ax = fig.add_subplot(111)\n    ax.tick_params(axis='both', which='major', labelsize=16, colors='black')\n    ax.tick_params(axis='both', which='minor', labelsize=16, colors='black')\n    plt.plot(array)\n    set_color(ax)\n\n", "CodeBleu_score": 25.908321633688207}]}
{"instance_id": "ff94c425920c", "generation_result": [{"code": "def write_to_csv(image_name, prediction, confidence)\n\t\n    data = {\n        \"image\": image_name,\n        \"prediction\": prediction,\n        \"confidence\": confidence,\n    }\n    csv_path = Path(\"predictions.csv\")\n    if csv_path.is_file():\n        open_mode = \"a\"\n    else:\n        open_mode = \"w\"\n        with open(csv_path, \"w\") as csv_file:\n            csv_writer = csv.DictWriter(csv_file, fieldnames=data.keys())\n            csv_writer.writeheader()\n    with open(csv_path, open_mode) as csv_file:\n        csv_writer = csv.DictWriter(csv_file, fieldnames=data.keys())\n        csv_writer.writerow(data)\n\n", "CodeBleu_score": 47.43085126513221}, {"code": "def write_to_csv(image_name, prediction, confidence)\n\t\n    data = {\"Image\": image_name, \"Prediction\": prediction, \"Confidence\": confidence}\n    csv_path = Path(\"predictions.csv\")\n    if csv_path.is_file():\n        mode = \"a\"\n    else:\n        mode = \"w\"\n        with open(csv_path, mode) as csv_file:\n            writer = csv.DictWriter(csv_file, fieldnames=data.keys())\n            writer.writeheader()\n    with open(csv_path, mode) as csv_file:\n        writer = csv.DictWriter(csv_file, fieldnames=data.keys())\n        writer.writerow(data)\n\n", "CodeBleu_score": 53.839166661151914}, {"code": "def write_to_csv(image_name, prediction, confidence)\n\t\n    # Create the data dictionary\n    data = {\n        'image': image_name,\n        'prediction': prediction,\n        'confidence': confidence\n    }\n    # Open the CSV file in append mode\n    with open(csv_path, 'a') as csv_file:\n        # Create a DictWriter object\n        writer = csv.DictWriter(csv_file, fieldnames=data.keys())\n        # If the CSV file is new, write the header\n        if not csv_path.is_file():\n            writer.writeheader()\n        # Write the data as a new row\n        writer.writerow(data)\n", "CodeBleu_score": 58.98528051353592}]}
{"instance_id": "436161602009", "generation_result": [{"code": "def export_torchscript(model, im, file, optimize, prefix=colorstr(\"TorchScript:\"))\n\t\n    LOGGER.info(f\"{prefix} starting export with torch {torch.__version__}...\")\n    ts = torch.jit.trace(model, im)\n    if optimize:\n        LOGGER.info(f\"{prefix} optimizing for mobile...\")\n        ts = ts.optimize_for_mobile()\n    LOGGER.info(f\"{prefix} saving...\")\n    _save_for_lite_interpreter(ts, file)\n    LOGGER.info(f\"{prefix} done. ({file})\")\n    return file, None\n\n", "CodeBleu_score": 31.533090538511054}, {"code": "def export_torchscript(model, im, file, optimize, prefix=colorstr(\"TorchScript:\"))\n\t\n    LOGGER.info(f\"{prefix} starting export with torch {torch.__version__}...\")\n    ts = torch.jit.trace(model, im)\n    if optimize:\n        ts = optimize_for_mobile(ts)\n    ts.save(file)\n    LOGGER.info(f\"{prefix} export success, saved as {file}\")\n    return file, None\n\n", "CodeBleu_score": 29.980244516893713}, {"code": "def export_torchscript(model, im, file, optimize, prefix=colorstr(\"TorchScript:\"))\n\t\n    LOGGER.info(f\"{prefix} starting export with torch {torch.__version__}...\")\n    f = file.with_suffix(\".torchscript.pt\")\n    ts = torch.jit.trace(model, im)\n    if optimize:\n        ts = optimize_for_mobile(ts)\n    ts.save(f)\n    LOGGER.info(f\"{prefix} export success, saved as {f}\")\n    return f, None\n\n", "CodeBleu_score": 37.23648243839801}]}
{"instance_id": "78335487ac1c", "generation_result": [{"code": "def export_coreml(model, im, file, int8, half, nms, prefix=colorstr(\"CoreML:\")):\n\t\n    if not ct:\n        LOGGER.error(\"CoreML export requires `coremltools` to be installed. Run `pip install coremltools` to install it.\")\n        sys.exit(1)\n    if nms:\n        model = iOSModel(model)\n    LOGGER.info(\"%s Exporting model to CoreML format at '%s'\", prefix, file)\n    model.eval()\n    traced_model = trace(model, im)\n    if int8:\n        if half:\n            LOGGER.warning(\"%s CoreML does not support FP16 models, converting to FP32\", prefix)\n        traced_model = quantize_weights(traced_model)\n    mlmodel = ct.convert(traced_model, inputs=[ct.ImageType(name=\"images\", shape=im.shape, scale=1/255.0, bias=[0,0,0])])\n    file = file.with_suffix(\".mlmodel\")\n    mlmodel.save(file)\n    LOGGER.info(\"%s Successfully exported CoreML model at '%s'\", prefix, file)\n    return file, mlmodel\n\n", "CodeBleu_score": 37.838333749339284}, {"code": "def export_coreml(model, im, file, int8, half, nms, prefix=colorstr(\"CoreML:\")):\n\t\n    check_requirements(\"coremltools\")\n    LOGGER.info(\"%s Exporting model to CoreML format at '%s'\", prefix, file)\n    if nms:\n        model = iOSModel(model)\n    traced_model = trace(model, im, half)\n    ct_model = ct.convert(traced_model, inputs=[ct.ImageType(name=\"images\", shape=im.shape, scale=1/255.0, bias=[0,0,0])])\n    if int8:\n        ct_model = quantize_weights(ct_model, nbits=8)\n    ct_model.save(file)\n    return file, ct_model\n\n", "CodeBleu_score": 29.791099657578723}, {"code": "def export_coreml(model, im, file, int8, half, nms, prefix=colorstr(\"CoreML:\")):\n\t\n    if not check_requirements(\"coremltools\"):\n        return\n    LOGGER.info(f\"{prefix} starting export with torch {torch.__version__}...\")\n    file = file.with_suffix(\".mlmodel\")\n    if nms:\n        model = iOSModel(model)\n    model.eval()\n    example = torch.rand(1, 3, im, im)\n    traced_model = torch.jit.trace(model, example)\n    if int8:\n        if not traced_model.graph.findNode(\"quantized::conv2d\"):\n            LOGGER.warning(f\"{prefix} quantization not possible for model, no quantized::conv2d nodes found\")\n            int8 = False\n        else:\n            try:\n                from coremltools.converters.mil.frontend.torch.quantization_utils import (\n                    quantize_weights,\n                )\n            except ImportError:\n                LOGGER.warning(f\"{prefix} quantization not possible for model, coremltools version is too low\")\n                int8 = False\n    if int8:\n        quantize_weights(traced_model, nbits=8)\n    if half:\n        traced_model = traced_model.half()\n    mlmodel = ct.convert(\n        traced_model,\n        inputs=[ct.ImageType(name=\"input_1\", shape=example.shape)],\n        convert_to=\"mlprogram\",\n    )\n    mlmodel.save(file)\n    LOGGER.info(f\"{prefix} export success, saved as {file}\")\n    return file, mlmodel\n\n", "CodeBleu_score": 42.31645865022139}]}
{"instance_id": "80233cf71580", "generation_result": [{"code": "def generate_individual(input_ranges, individual_length)\n\t\n    individual = []\n    for i in range(individual_length):\n        individual.append(random.uniform(input_ranges[i][0], input_ranges[i][1]))\n    return individual\n\n", "CodeBleu_score": 47.630972577458174}, {"code": "def generate_individual(input_ranges, individual_length)\n\t\n    individual = []\n    for i in range(individual_length):\n        individual.append(random.uniform(input_ranges[0], input_ranges[1]))\n    return individual\n", "CodeBleu_score": 45.11328150959641}, {"code": "def generate_individual(input_ranges, individual_length)\n\t\n    individual = []\n    for i in range(individual_length):\n        individual.append(random.uniform(input_ranges[i][0], input_ranges[i][1]))\n    return individual\n\n", "CodeBleu_score": 47.630972577458174}]}
{"instance_id": "5ea979cb4a8e", "generation_result": [{"code": "def marginal_log_mean_coeff(self, t)\n\t_\n    Args:\n        t: the continuous-time label t.\n    Returns:\n        The log(alpha_t) value.\n    \"\"\"\n    if self.schedule_type == 'discrete':\n        return self.log_alpha_fn(t)\n    elif self.schedule_type == 'linear':\n        return self.log_alpha_fn(t)\n    elif self.schedule_type == 'cosine':\n        return self.log_alpha_fn(t)\n    else:\n        raise ValueError('schedule_type must be one of \\'discrete\\', \\'linear\\', or \\'cosine\\'')\n\n", "CodeBleu_score": 19.152622326743728}, {"code": "def marginal_log_mean_coeff(self, t)\n\t_\n    Args:\n        t (torch.Tensor): continuous-time label.\n    Returns:\n        torch.Tensor: log(alpha_t)\n    \"\"\"\n    if self.schedule_type == 'discrete':\n        return torch.log(self.alpha_0) + (torch.log(self.alpha_1) - torch.log(self.alpha_0)) * (t / self.T)\n    elif self.schedule_type == 'linear':\n        return torch.log(self.alpha_0) + (torch.log(self.alpha_1) - torch.log(self.alpha_0)) * (t / self.T)\n    elif self.schedule_type == 'cosine':\n        return self.log_alpha_fn(t)\n    else:\n        raise ValueError('Unknown schedule type: {}'.format(self.schedule_type))\n", "CodeBleu_score": 10.711635172561962}, {"code": "def marginal_log_mean_coeff(self, t)\n\t_\n    \"\"\"\n    if self.schedule_type == 'discrete':\n        return self.log_alpha_fn(t)\n    elif self.schedule_type == 'linear':\n        return self.log_alpha_fn(t)\n    elif self.schedule_type == 'cosine':\n        return self.log_alpha_fn(t)\n    else:\n        raise ValueError('Unknown schedule type: %s' % self.schedule_type)", "CodeBleu_score": 1.182071031236628}]}
{"instance_id": "7da4fae5a278", "generation_result": [{"code": "def inverse_lambda(self, lamb)\n\t\n    if self.schedule == 'linear':\n        t = (lamb - self.beta_min) / (self.beta_max - self.beta_min)\n    elif self.schedule == 'discrete':\n        t = self.interpolate_fn(self.log_alpha, lamb)\n    else:\n        t = self.t_fn(self.log_alpha, lamb)\n    return t\n\n", "CodeBleu_score": 15.302202638913839}, {"code": "def inverse_lambda(self, lamb)\n\t\n    if self.schedule_type == 'linear':\n        return self.beta * lamb\n    elif self.schedule_type == 'discrete':\n        return self.interpolate_fn(self.log_alpha, lamb)\n    else:\n        return torch.arccos(1 - 2 * lamb) / self.log_alpha\n\n", "CodeBleu_score": 9.081314650324842}, {"code": "def inverse_lambda(self, lamb)\n\t\n    if self.schedule == \"linear\":\n        beta_0 = self.beta_0\n        beta_T = self.beta_T\n        t = (np.log(2) - lamb) / (np.log(2) - beta_0) * (beta_T - beta_0) + beta_0\n        return t\n    elif self.schedule == \"discrete\":\n        log_alpha = self.log_alpha\n        t = interpolate_fn(lamb, log_alpha)\n        return t\n    else:\n        log_alpha = self.log_alpha\n        t = arccos_fn(lamb, log_alpha)\n        return t\n\n", "CodeBleu_score": 20.203543472484327}]}
{"instance_id": "a519c853a2b2", "generation_result": [{"code": "def noise_pred_fn(x, t_continuous, cond=None)\n\t\n    if t_continuous.dim() == 1:\n        t_continuous = t_continuous.expand(x.shape[0], -1)\n    elif t_continuous.dim() == 2:\n        t_continuous = t_continuous.reshape(x.shape[0], -1)\n    else:\n        raise ValueError(\"t_continuous must have 1 or 2 dimensions\")\n    if cond is None:\n        return noise_schedule(model(x, t_continuous))\n    else:\n        return noise_schedule(model(x, t_continuous, cond))\n", "CodeBleu_score": 20.28220442657192}, {"code": "def noise_pred_fn(x, t_continuous, cond=None)\n\t\n    t_continuous = expand_dims(t_continuous, axis=0)\n    t_continuous = t_continuous.expand(x.dim()[0], -1)\n    t_continuous = t_continuous.reshape(-1, 1)\n    x_t = torch.cat((x, t_continuous), dim=1)\n    if model_type == 'linear':\n        return noise_schedule.marginal_alpha * x_t\n    elif model_type == 'nonlinear':\n        return noise_schedule.marginal_alpha * torch.sin(x_t) + noise_schedule.marginal_std\n    else:\n        raise NotImplementedError\n\n", "CodeBleu_score": 18.505461281892792}, {"code": "def noise_pred_fn(x, t_continuous, cond=None)\n\t\n    if model_type == \"linear\":\n        t_continuous = expand_dims(t_continuous, 1)\n        t_continuous = t_continuous.expand(x.shape[0], 1, x.shape[1])\n        t_continuous = t_continuous.reshape(x.shape[0] * x.shape[1], 1)\n        x = torch.cat((x, t_continuous), 1)\n        x = model(x)\n        x = x.reshape(x.shape[0], x.shape[1])\n        x = noise_schedule.marginal_std * x\n    elif model_type == \"rnn\":\n        x = torch.cat((x, t_continuous), 1)\n        x = model(x)\n        x = x.reshape(x.shape[0], x.shape[1])\n        x = noise_schedule.marginal_std * x\n    elif model_type == \"lstm\":\n        x = torch.cat((x, t_continuous), 1)\n        x = model(x)\n        x = x.reshape(x.shape[0], x.shape[1])\n        x = noise_schedule.marginal_std * x\n    elif model_type == \"gru\":\n        x = torch.cat((x, t_continuous), 1)\n        x = model(x)\n        x = x.reshape(x.shape[0], x.shape[1])\n        x = noise_schedule.marginal_std * x\n    return x\n", "CodeBleu_score": 37.552428609406995}]}
{"instance_id": "96a9b7d54823", "generation_result": [{"code": "def dynamic_thresholding_fn(self, x0, t=None)\n\t\n    if t is None:\n        t = 0.99\n    x0 = x0.reshape(-1)\n    quantile = torch.quantile(torch.abs(x0), t)\n    scale = 1 / quantile\n    x0 = torch.clamp(x0, min=-scale, max=scale)\n    x0 = x0 * scale\n    return x0.reshape(x0.shape[0], x0.shape[1], x0.shape[2], 1)\n", "CodeBleu_score": 33.11342944614724}, {"code": "def dynamic_thresholding_fn(self, x0, t=None)\n\t\n    # Reshape x0 to compute the quantile of absolute values\n    x0_abs = torch.abs(x0).reshape(x0.shape[0], -1)\n    q = torch.quantile(x0_abs, self.quantile, dim=1)\n    # Compute the scaling factor\n    scaling_factor = torch.clamp(q, min=self.min_val)\n    # Clamp x0 values based on the scaling factor\n    x0_clamped = torch.clamp(x0, min=-scaling_factor, max=scaling_factor)\n    # Scale x0 and return\n    return x0_clamped * self.scale_factor\n", "CodeBleu_score": 23.846092672938582}, {"code": "def dynamic_thresholding_fn(self, x0, t=None)\n\t\n    if t is None:\n        t = self.threshold\n    x0_abs = torch.abs(x0)\n    q = torch.quantile(x0_abs, self.quantile)\n    scaling_factor = t / q\n    x0_clamped = torch.clamp(x0, min=-t, max=t)\n    x0_scaled = x0_clamped * scaling_factor\n    return x0_scaled\n", "CodeBleu_score": 30.86045895920662}]}
{"instance_id": "59c6ec6d7a6a", "generation_result": [{"code": "def data_prediction_fn(self, x, t)\n\t\n    threshold = self.noise_prediction_fn(x, t)\n    clamped_x = torch.clamp(x, min=0, max=threshold)\n    normalized_x = clamped_x / threshold\n    return normalized_x\n", "CodeBleu_score": 4.763510178956413}, {"code": "def data_prediction_fn(self, x, t)\n\t\n    # Calculate the dynamic threshold value\n    threshold = self.noise_prediction_fn(x, t)\n    # Clamp the input tensor values\n    x = torch.clamp(x, 0, threshold)\n    # Normalize the clamped values\n    x = x / threshold\n    return x\n", "CodeBleu_score": 3.6405888710947285}, {"code": "def data_prediction_fn(self, x, t)\n\t\n    # x0 = x.clone()\n    x = x.clone()\n    if self.threshold_type == 'dynamic':\n        # threshold_value = self.threshold_value * (1 + self.threshold_ratio * t)\n        threshold_value = self.threshold_value * (1 + self.threshold_ratio * t)\n    else:\n        threshold_value = self.threshold_value\n    # x = torch.clamp(x, 0, threshold_value)\n    x = torch.clamp(x, 0, threshold_value)\n    # x = x / threshold_value\n    x = x / threshold_value\n    return x\n", "CodeBleu_score": 13.74109604214665}]}
{"instance_id": "65e3618468a2", "generation_result": [{"code": "def predict_eps_sigma(model, input, sigma_in, **kwargs)\n\t\n    sigma_in = sigma_in.view(sigma_in.shape[0], 1, 1, 1)\n    input_scaled = input * sigma_in\n    output = model(input_scaled, **kwargs)\n    output = output * sigma_in\n    return output\n\n", "CodeBleu_score": 27.039034348156143}, {"code": "def predict_eps_sigma(model, input, sigma_in, **kwargs)\n\t\n    sigma_in = sigma_in.view(input.shape)\n    input_scaled = input * sigma_in\n    output = model(input_scaled, **kwargs)\n    return output * sigma_in\n\n", "CodeBleu_score": 25.018263125444268}, {"code": "def predict_eps_sigma(model, input, sigma_in, **kwargs)\n\t\n    sigma_in = sigma_in.view(sigma_in.shape[0], 1, 1, 1)\n    input_scaled = input * sigma_in\n    return model(input_scaled, **kwargs)\n\n", "CodeBleu_score": 20.92958067714501}]}
{"instance_id": "b35846dfbc66", "generation_result": [{"code": "def cal_intergrand(beta_0, beta_1, taus)\n\t\n    with torch.inference_mode():\n        alpha = torch.sqrt(beta_0) * taus\n        log_alpha = torch.log(alpha)\n        log_alpha.sum().backward()\n        t2alpha_fn = torch.autograd.grad(log_alpha, taus, torch.ones_like(log_alpha))[0]\n        t2alpha_fn = t2alpha_fn / (beta_0 * beta_1)\n    return t2alpha_fn\n", "CodeBleu_score": 34.017032671353924}, {"code": "def cal_intergrand(beta_0, beta_1, taus)\n\t\n    with torch.inference_mode():\n        alpha = beta_0.clone()\n        log_alpha = taus.log() + alpha\n        log_alpha.sum().backward()\n        taus.requires_grad_(True)\n        torch.enable_grad()\n        beta_1.clone()\n        return torch.sqrt(1 + taus) * log_alpha.grad\n\n", "CodeBleu_score": 33.50890905207442}, {"code": "def cal_intergrand(beta_0, beta_1, taus)\n\t\n    with torch.inference_mode():\n        alpha = beta_0.clone()\n        log_alpha = alpha.log()\n        alpha.backward(taus)\n        t2alpha_fn = taus * 2 * alpha\n        return torch.sqrt(t2alpha_fn) * log_alpha\n\n", "CodeBleu_score": 26.467498752984593}]}
{"instance_id": "5f3667566a10", "generation_result": [{"code": "def sample_euler_ancestral(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None)\n\t\n    if extra_args is None:\n        extra_args = []\n    if noise_sampler is None:\n        noise_sampler = default_noise_sampler\n    if callback is None:\n        callback = lambda x: None\n    if disable is None:\n        disable = False\n    x = x.clone()\n    x.requires_grad_(True)\n    x = x.to(device)\n    for sigma in trange(sigmas, desc='Ancestral sampling', disable=disable):\n        x_noise = noise_sampler(x, sigma)\n        x_noise = x_noise.to(device)\n        step = get_ancestral_step(model, x_noise, extra_args, eta=eta, s_noise=s_noise)\n        x = x + step\n        callback(x)\n    if sigma != 0:\n        x = x + noise_sampler(x, sigma)\n    return x\n\n", "CodeBleu_score": 30.46536276641677}, {"code": "def sample_euler_ancestral(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None)\n\t\n    if extra_args is None:\n        extra_args = {}\n    if callback is None:\n        callback = lambda x: None\n    if noise_sampler is None:\n        noise_sampler = default_noise_sampler\n    if disable is None:\n        disable = []\n    if len(x.shape) == 1:\n        x = x.reshape(1, -1)\n    if x.shape[0] != len(sigmas):\n        raise ValueError(\n            \"The number of sigmas must be equal to the number of samples.\")\n    x = x.clone()\n    x.requires_grad = True\n    x = x.to(dtype=torch.float64)\n    if s_noise != 1.:\n        x = noise_sampler(x, s_noise)\n    for i in trange(len(sigmas), desc=\"Ancestral sampling\", disable=disable):\n        sigma = sigmas[i]\n        callback(x)\n        if sigma != 0.:\n            x = denoise(model, x, sigma, **extra_args)\n        ancestral_step = get_ancestral_step(\n            model, x, sigma, **extra_args) * eta\n        x = x + ancestral_step\n        if s_noise != 1.:\n            x = noise_sampler(x, s_noise)\n    if s_noise != 1.:\n        x = x * s_noise\n    return x.to(dtype=torch.float32)\n", "CodeBleu_score": 33.100982034660596}, {"code": "def sample_euler_ancestral(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None)\n\t\n    if extra_args is None:\n        extra_args = []\n    if noise_sampler is None:\n        noise_sampler = default_noise_sampler\n    if disable is None:\n        disable = []\n    x = x.clone()\n    x.requires_grad_(True)\n    for sigma in trange(len(sigmas), desc=\"Sampling\", disable=disable):\n        sigma = sigmas[sigma]\n        x_denoised = denoise_fn(x, sigma, model, extra_args)\n        ancestral_step = get_ancestral_step(x, x_denoised, sigma, model, extra_args)\n        x = x + eta * ancestral_step\n        if callback is not None:\n            callback(x, sigma, extra_args)\n        if s_noise > 0:\n            x += s_noise * noise_sampler(x)\n    return x.detach()\n", "CodeBleu_score": 36.713509128403196}]}
{"instance_id": "6261ded88df2", "generation_result": [{"code": "def linear_multistep_coeff(order, t, i, j)\n\t\n    if order == 1:\n        return 1.0\n    elif order == 2:\n        return 1.0 - t\n    elif order == 3:\n        if i == 0:\n            return 0.5 * (1.0 - t) * (1.0 - 2.0 * t)\n        elif i == 1:\n            return 0.5 * (1.0 + t) * (1.0 - 2.0 * t)\n        else:\n            raise ValueError(\"The order is too high for the step index.\")\n    elif order == 4:\n        if i == 0:\n            return (1.0 - t) * (1.0 - 2.0 * t) * (1.0 - 3.0 * t)\n        elif i == 1:\n            return (1.0 + t) * (1.0 - 2.0 * t) * (1.0 - 3.0 * t)\n        elif i == 2:\n            return (1.0 + t) * (1.0 + t) * (1.0 - 3.0 * t)\n        else:\n            raise ValueError(\"The order is too high for the step index.\")\n    elif order == 5:\n        if i == 0:\n            return 0.5 * (1.0 - t) * (1.0 - 2.0 * t) * (1.0 - 3.0 * t) * (1.0 - 4.0 * t)\n        elif i == 1:\n            return 0.5 * (1.0 + t) * (1.0 - 2.0 * t) * (1.0 - 3.0 * t) * (1.0 - 4.0 * t)\n        elif i == 2:\n            return 0.5 * (1.0 + t) * (1.0 + t) * (1.0 - 3.0 * t) * (1.0 - 4.0 * t)\n        elif i == 3:\n            return 0.5 * (1.0 + t) * (1.0 + t) * (1.0 + t) * (1.0 - 4.0 * t)\n        else:\n            raise ValueError(\"The order is too high for the step", "CodeBleu_score": 12.660143793056383}, {"code": "def linear_multistep_coeff(order, t, i, j)\n\t\n    if order < 0:\n        raise ValueError(\"Order must be non-negative.\")\n    if i < 0 or i > j:\n        raise ValueError(\"Indices i and j must satisfy i <= j.\")\n    if order > j - i:\n        raise ValueError(\"Order must be less than or equal to j - i.\")\n\n    def product_term(k):\n        \"\"\"The nested function computes the product term for the coefficient, excluding the j-th term.\"\"\"\n        if k == j:\n            return 0\n        return (t[k + 1] - t[i]) / (t[k + 1] - t[k])\n\n    return integrate.quad(product_term, i, j)[0]\n\n", "CodeBleu_score": 26.448719065302985}, {"code": "def linear_multistep_coeff(order, t, i, j)\n\t\n    if order < 1:\n        raise ValueError(\"Order must be greater than 0\")\n    if order > i + 1:\n        raise ValueError(\"Order must be less than or equal to the step index\")\n    if t[i] >= t[j]:\n        raise ValueError(\"Time step index i must be less than the time step index j\")\n    if j == len(t) - 1:\n        return 0\n    if order == 1:\n        return (t[j] - t[i]) / (t[j + 1] - t[i])\n    else:\n        return integrate.quad(lambda s: linear_multistep_coeff(order - 1, t, i, j) * (t[j] - t[i]) / (t[j + 1] - t[i]), t[i], t[j])[0]\n\n", "CodeBleu_score": 23.07434661832947}]}
{"instance_id": "4ee025e33ab1", "generation_result": [{"code": "def sample_lcm(model, x, sigmas, extra_args=None, callback=None, disable=None, noise_sampler=None)\n\t\n    if extra_args is None:\n        extra_args = {}\n    if noise_sampler is None:\n        noise_sampler = default_noise_sampler\n    if callback is None:\n        callback = lambda x: x\n    if disable is None:\n        disable = []\n    if len(disable) > 0:\n        for d in disable:\n            if d not in ['noise_scaling']:\n                raise ValueError(\"Unknown disable option: %s\" % d)\n    if 'noise_scaling' not in disable:\n        if 'noise_scaling' in extra_args:\n            noise_scaling = extra_args['noise_scaling']\n        else:\n            noise_scaling = 1.0\n    else:\n        noise_scaling = 1.0\n    for i in trange(len(sigmas)):\n        x = callback(x)\n        if sigmas[i] > 0:\n            x = model(x, **extra_args)\n            if 'noise_scaling' not in disable:\n                x = noise_scaling * noise_sampler(x, sigmas[i])\n        else:\n            x = model(x, **extra_args)\n    return x.new_ones(x.shape)\n", "CodeBleu_score": 31.89094364223854}, {"code": "def sample_lcm(model, x, sigmas, extra_args=None, callback=None, disable=None, noise_sampler=None)\n\t\n    if extra_args is None:\n        extra_args = {}\n    if noise_sampler is None:\n        noise_sampler = default_noise_sampler(model, x)\n    if disable is None:\n        disable = []\n    for k, v in extra_args.items():\n        if k not in disable:\n            setattr(model, k, v)\n    for i in trange(len(sigmas), desc='LCM sampling'):\n        if callback is not None:\n            callback(model, x, i)\n        x = model(x)\n        x = noise_sampler(x, sigmas[i])\n        if sigmas[i] > 0:\n            x = x * (1 + sigmas[i] * noise_scaling)\n    return x\n\n", "CodeBleu_score": 34.60888919032108}, {"code": "def sample_lcm(model, x, sigmas, extra_args=None, callback=None, disable=None, noise_sampler=None)\n\t\n    if extra_args is None:\n        extra_args = {}\n    if noise_sampler is None:\n        noise_sampler = default_noise_sampler\n    if disable is None:\n        disable = []\n    for arg in [\"x\", \"sigmas\"]:\n        if arg in disable:\n            continue\n        assert arg in extra_args, f\"missing {arg} in extra_args\"\n    assert len(sigmas) > 0, \"sigmas must be non-empty\"\n    assert len(x) == len(sigmas[0]), \"x and sigmas must have the same length\"\n    x = x.clone()\n    for sigma in sigmas:\n        x = model(x, **extra_args)\n        if callback is not None:\n            callback(x)\n        if sigma > 0:\n            x = x + noise_sampler(x, sigma) * noise_scaling\n    return x\n", "CodeBleu_score": 35.16015143576827}]}
{"instance_id": "cb51f91da606", "generation_result": [{"code": "def append_dims(x, target_dims)\n\t\n    if x.dim() == target_dims:\n        return x.detach().clone()\n    if x.dim() > target_dims:\n        raise ValueError(\n            f\"The tensor has more dimensions than the target dimensions. The tensor has {x.dim()} dimensions, but the target dimensions are {target_dims}.\")\n    if x.device.type == 'mps':\n        return x.detach().clone()\n    return x.expand(*x.shape, *[1] * (target_dims - x.dim()))\n\n", "CodeBleu_score": 27.679522990249726}, {"code": "def append_dims(x, target_dims)\n\t\n    if x.dim() == target_dims:\n        return x\n    elif x.dim() > target_dims:\n        raise ValueError(\"The number of dimensions of the input tensor is larger than the target number of dimensions.\")\n    else:\n        if x.is_mps():\n            return x.clone().detach().expand(x.size() + (1,)*(target_dims - x.dim()))\n        else:\n            return x.expand(x.size() + (1,)*(target_dims - x.dim()))\n\n", "CodeBleu_score": 21.34576584000274}, {"code": "def append_dims(x, target_dims)\n\t\n    if x.dim() == target_dims:\n        return x\n    elif x.dim() > target_dims:\n        raise ValueError(\n            f\"The tensor has too many dimensions. The tensor has {x.dim()} dimensions, but the target dimensions are {target_dims}.\"\n        )\n    else:\n        dims_to_add = target_dims - x.dim()\n        if x.device.type == \"mps\":\n            return x.detach().reshape(x.shape + (1,) * dims_to_add)\n        else:\n            return x.reshape(x.shape + (1,) * dims_to_add)\n\n", "CodeBleu_score": 30.192316931432085}]}
{"instance_id": "9fac874a3bc7", "generation_result": [{"code": "def download_file(path, url, digest=None)\n\t\n    if not path.exists():\n        path.mkdir()\n    if not (path / url.split(\"/\")[-1]).exists():\n        with urlopen(url) as response:\n            with (path / url.split(\"/\")[-1]).open(\"wb\") as file:\n                shutil.copyfileobj(response, file)\n    if digest is not None:\n        with (path / url.split(\"/\")[-1]).open(\"rb\") as file:\n            if hashlib.sha256(file.read()).hexdigest() != digest:\n                raise OSError(f\"The downloaded file {path / url.split('/')[-1]} has an invalid SHA-256 hash.\")\n    return path / url.split(\"/\")[-1]\n\n", "CodeBleu_score": 36.65736325451959}, {"code": "def download_file(path, url, digest=None)\n\t\n    if not os.path.exists(path):\n        os.makedirs(path)\n    if not os.path.exists(os.path.join(path, os.path.basename(url))):\n        with urlopen(url) as response:\n            with open(os.path.join(path, os.path.basename(url)), \"wb\") as outfile:\n                shutil.copyfileobj(response, outfile)\n    if digest is not None:\n        if not hash_file(os.path.join(path, os.path.basename(url)), \"sha256\") == digest:\n            raise OSError(\"Hash of downloaded file does not match provided digest.\")\n    return os.path.join(path, os.path.basename(url))\n\n", "CodeBleu_score": 38.67681720115762}, {"code": "def download_file(path, url, digest=None)\n\t\n    if not path.exists():\n        path.mkdir(parents=True)\n    if not path.is_dir():\n        raise ValueError(\"path must be a directory\")\n    if not isinstance(url, str):\n        raise TypeError(\"url must be a string\")\n    if not isinstance(digest, str) and digest is not None:\n        raise TypeError(\"digest must be a string or None\")\n    if not url.startswith(\"http\"):\n        raise ValueError(\"url must start with http\")\n    if not url.endswith(\"/\"):\n        url += \"/\"\n    filename = url.split(\"/\")[-1]\n    filepath = path / filename\n    if filepath.exists():\n        if digest is not None:\n            with open(filepath, \"rb\") as f:\n                actual_digest = hashlib.sha256(f.read()).hexdigest()\n            if actual_digest != digest:\n                raise RuntimeError(f\"expected {digest} but got {actual_digest}\")\n    else:\n        with urlopen(url + filename) as response, open(filepath, \"wb\") as f:\n            shutil.copyfileobj(response, f)\n    return filepath\n\n", "CodeBleu_score": 44.22931733860111}]}
{"instance_id": "f23165e897ce", "generation_result": [{"code": "def train_mode(model, mode=True)\n\t\n    if mode:\n        training_modes = [module.training for module in model.modules()]\n    else:\n        training_modes = [False] * len(model.modules())\n    model.train(mode)\n    yield\n    model.train(training_modes[0])\n\n", "CodeBleu_score": 51.145607078408126}, {"code": "def train_mode(model, mode=True)\n\t\n    if mode:\n        model.train()\n    else:\n        model.eval()\n    original_mode = []\n    for module in model.modules():\n        original_mode.append(module.training)\n        module.train(mode)\n    yield\n    for module, mode in zip(model.modules(), original_mode):\n        module.train(mode)\n\n", "CodeBleu_score": 37.893764082457324}, {"code": "def train_mode(model, mode=True)\n\t\n    prev_states = []\n    for module in model.modules():\n        prev_states.append(module.training)\n        module.train(mode)\n    yield\n    for module, state in zip(model.modules(), prev_states):\n        module.train(state)\n\n", "CodeBleu_score": 37.44678923793591}]}
{"instance_id": "c6d59c6cb8e1", "generation_result": [{"code": "def ema_update(model, averaged_model, decay)\n\t\n    with torch.no_grad():\n        # Use the \"dict.update()\" function to update the \"averaged_model\" parameters and buffers\n        # with the \"model\" parameters and buffers\n        for model_params, averaged_params in zip(model.parameters(), averaged_model.parameters()):\n            averaged_params.data.mul_(decay).add_(model_params.data, alpha=1 - decay)\n\n        for model_buffers, averaged_buffers in zip(model.buffers(), averaged_model.buffers()):\n            averaged_buffers.data.copy_(model_buffers.data)\n\n", "CodeBleu_score": 33.804587894252414}, {"code": "def ema_update(model, averaged_model, decay)\n\t\n    with torch.no_grad():\n        model_buffers = dict(model.named_buffers())\n        averaged_buffers = dict(averaged_model.named_buffers())\n        for k, v in model_buffers.items():\n            if k not in averaged_buffers:\n                continue\n            v_ = averaged_buffers[k]\n            v_ *= decay\n            v_ += (1.0 - decay) * v\n            averaged_buffers[k] = v_\n        for k, v in model.named_parameters():\n            if k not in averaged_model.named_parameters():\n                continue\n            v_ = averaged_model.named_parameters()[k]\n            v_ *= decay\n            v_ += (1.0 - decay) * v\n            averaged_model.named_parameters()[k] = v_\n\n", "CodeBleu_score": 49.45070434333957}, {"code": "def ema_update(model, averaged_model, decay)\n\t\n    for model_param, avg_param in zip(model.parameters(), averaged_model.parameters()):\n        avg_param.data.mul_(decay).add_(1 - decay, model_param.data)\n    for model_buffer, avg_buffer in zip(model.buffers(), averaged_model.buffers()):\n        avg_buffer.data.copy_(model_buffer.data)\n\n", "CodeBleu_score": 28.420819770409487}]}
{"instance_id": "600c39a30798", "generation_result": [{"code": "def rand_log_logistic(shape, loc=0., scale=1., min_value=0., max_value=float('inf'), device='cpu', dtype=torch.float32)\n\t\n    min_value = torch.as_tensor(min_value, device=device, dtype=dtype)\n    max_value = torch.as_tensor(max_value, device=device, dtype=dtype)\n    cdf_min = torch.sigmoid(min_value.log() - loc.log() - scale.log())\n    cdf_max = torch.sigmoid(max_value.log() - loc.log() - scale.log())\n    u = torch.rand(shape, device=device, dtype=dtype)\n    u = u * (cdf_max - cdf_min) + cdf_min\n    return loc + scale.logit(u).exp()\n\n", "CodeBleu_score": 41.61049107055007}, {"code": "def rand_log_logistic(shape, loc=0., scale=1., min_value=0., max_value=float('inf'), device='cpu', dtype=torch.float32)\n\t\n    if min_value > max_value:\n        raise ValueError(\"The minimum value must be less than or equal to the maximum value.\")\n    if min_value == max_value:\n        return torch.full(shape, min_value, device=device, dtype=dtype)\n    min_value = torch.as_tensor(min_value, device=device, dtype=dtype)\n    max_value = torch.as_tensor(max_value, device=device, dtype=dtype)\n    min_cdf = min_value.log().sigmoid()\n    max_cdf = max_value.log().sigmoid()\n    cdf_range = max_cdf - min_cdf\n    u = torch.rand(shape, device=device, dtype=dtype) * cdf_range + min_cdf\n    u = u.logit() * scale + loc\n    return u\n", "CodeBleu_score": 47.77446381174573}, {"code": "def rand_log_logistic(shape, loc=0., scale=1., min_value=0., max_value=float('inf'), device='cpu', dtype=torch.float32)\n\t\n    min_value = torch.as_tensor(min_value, device=device, dtype=dtype)\n    max_value = torch.as_tensor(max_value, device=device, dtype=dtype)\n    u = torch.rand(shape, device=device, dtype=dtype)\n    cdf_min = min_value.log().sub(loc).div(scale).exp()\n    cdf_max = max_value.log().sub(loc).div(scale).exp()\n    return u.mul(cdf_max.sub(cdf_min)).add(cdf_min).logit().mul(scale).add(loc).clamp(min_value, max_value)\n", "CodeBleu_score": 40.25791620838031}]}
{"instance_id": "75e2def2467f", "generation_result": [{"code": "def get_activation(activation: Literal[\"elu\", \"snake\", \"none\"], antialias=False, channels=None) -> nn.Module\n\t\n    if activation == \"elu\":\n        activation = ELU()\n    elif activation == \"snake\":\n        activation = SnakeBeta(channels)\n    elif activation == \"none\":\n        activation = Identity()\n    else:\n        raise ValueError(f\"Activation type {activation} not supported.\")\n    if antialias:\n        activation = Activation1d(activation)\n    return activation\n", "CodeBleu_score": 55.87340860734221}, {"code": "def get_activation(activation: Literal[\"elu\", \"snake\", \"none\"], antialias=False, channels=None) -> nn.Module\n\t\n    if activation == \"elu\":\n        if antialias:\n            return nn.Sequential(\n                Activation1d(ELU(alpha=1.0), channels=channels),\n                nn.AvgPool1d(kernel_size=2, stride=1)\n            )\n        return ELU(alpha=1.0)\n    elif activation == \"snake\":\n        if antialias:\n            return nn.Sequential(\n                Activation1d(SnakeBeta(channels=channels), channels=channels),\n                nn.AvgPool1d(kernel_size=2, stride=1)\n            )\n        return SnakeBeta(channels=channels)\n    elif activation == \"none\":\n        return Identity()\n    else:\n        raise ValueError(f\"Activation type {activation} is not supported.\")\n", "CodeBleu_score": 38.8126801024085}, {"code": "def get_activation(activation: Literal[\"elu\", \"snake\", \"none\"], antialias=False, channels=None) -> nn.Module\n\t\n    if activation == \"elu\":\n        return ELU(antialias=antialias)\n    elif activation == \"snake\":\n        return SnakeBeta(channels=channels)\n    elif activation == \"none\":\n        return Identity()\n    else:\n        raise ValueError(f\"Activation type {activation} not recognized\")\n", "CodeBleu_score": 38.01757221227034}]}
{"instance_id": "ef1714c79e51", "generation_result": [{"code": "def apply_rotary_pos_emb(t, freqs, scale = 1)\n\t\n    if t.dtype != torch.float32:\n        t = t.to(torch.float32)\n    t_unrotated = t\n    freqs = freqs.to(t.device)\n    freqs = freqs.unsqueeze(-1)\n    freqs = freqs.unsqueeze(-1)\n    t = t.unsqueeze(-2)\n    t = t * freqs\n    t = t.sin()\n    t = t * scale\n    t = t.squeeze(-2)\n    t = torch.cat([t, t_unrotated], dim = -1)\n    return t, t_unrotated\n\n", "CodeBleu_score": 25.72120324854711}, {"code": "def apply_rotary_pos_emb(t, freqs, scale = 1)\n\t\n    if t.dtype != torch.float32:\n        t = t.float()\n    if freqs.dtype != torch.float32:\n        freqs = freqs.float()\n    t_unrotated = t\n    t = rearrange(t, '... (d j) -> ... d j', j = 2)\n    freqs = rearrange(freqs, '... (d j) -> ... d j', j = 2)\n    freqs = scale * freqs\n    freqs = torch.stack((-freqs[..., 1], freqs[..., 0]), dim = -1)\n    t = t * freqs.cos() + t_unrotated * freqs.sin()\n    t = rearrange(t, '... d j -> ... (d j)')\n    return torch.cat((t_unrotated, t), dim = -1)\n\n", "CodeBleu_score": 33.9528939790932}, {"code": "def apply_rotary_pos_emb(t, freqs, scale = 1)\n\t\n    if t.dtype == torch.float16:\n        t = t.to(torch.float32)\n    if freqs.dtype == torch.float16:\n        freqs = freqs.to(torch.float32)\n    # rearrange\n    t_unrotated, t = t[:, :1], t[:, 1:]\n    # rotate\n    freqs = freqs * scale\n    freqs = torch.stack([freqs.sin(), freqs.cos()], dim=-1)\n    freqs = freqs.view(1, 1, -1, 2)\n    t = torch.cat([t.unsqueeze(1), t.unsqueeze(1)], dim=1)\n    t = torch.matmul(t, freqs).squeeze(1)\n    # rearrange\n    t = torch.cat([t_unrotated, t], dim=1)\n    return t\n", "CodeBleu_score": 35.42382534457307}]}
{"instance_id": "a7a5f178c6f6", "generation_result": [{"code": "def idx2vq(self, idx, dim=-1)\n\t\n    return self.codebook(idx, dim=dim)\n\n", "CodeBleu_score": 16.59312750823775}, {"code": "def idx2vq(self, idx, dim=-1)\n\t\n    return self.codebook(idx).movedim(1, dim)\n\n", "CodeBleu_score": 18.88410605975309}, {"code": "def idx2vq(self, idx, dim=-1)\n\t\n    return self.codebook(idx).movedim(1, dim)\n\n", "CodeBleu_score": 18.88410605975309}]}
{"instance_id": "4104c041fb20", "generation_result": [{"code": "def encode(self, x, quantize=False)\n\t\n    quantized_encoding = None\n    quantized_indices = None\n    quantized_loss = None\n    encoding = x\n    for block in self.down_blocks:\n        encoding = block(encoding)\n        if quantize:\n            quantized_encoding, quantized_indices, quantized_loss = self.quantizer(encoding)\n    if quantize:\n        return quantized_encoding, encoding, quantized_indices, quantized_loss\n    else:\n        return encoding\n\n", "CodeBleu_score": 26.19447655700085}, {"code": "def encode(self, x, quantize=False)\n\t\n    if quantize:\n        z = self.forward(x)\n        z_q, z_orig, z_indices, z_loss = self.vq_layer(z)\n        return z_q, z_orig, z_indices, z_loss\n    else:\n        return self.forward(x)\n", "CodeBleu_score": 32.400559410041865}, {"code": "def encode(self, x, quantize=False)\n\t\n    quantized_encoding = None\n    quantized_encoding_indices = None\n    quantized_encoding_loss = None\n    if quantize:\n        quantized_encoding, quantized_encoding_indices, quantized_encoding_loss = self.quantizer(x)\n    else:\n        quantized_encoding = x\n    for block in self.down_blocks:\n        quantized_encoding = block(quantized_encoding)\n    return quantized_encoding, quantized_encoding_indices, quantized_encoding_loss\n", "CodeBleu_score": 24.667990999207635}]}
{"instance_id": "403c52ba2d3b", "generation_result": [{"code": "def ema_scope(self, context=None)\n\t\n    if self.ema_decay is None:\n        yield\n        return\n    if context is None:\n        context = {}\n    parameters = self.parameters\n    if \"ema_parameters\" not in context:\n        context[\"ema_parameters\"] = parameters\n    else:\n        for name, parameter in parameters.items():\n            ema_parameter = context[\"ema_parameters\"][name]\n            if parameter.shape != ema_parameter.shape:\n                raise ValueError(\n                    \"Parameter {} has a different shape in the new graph\".format(name)\n                )\n            ema_parameter.data = self.ema_decay * parameter.data + (\n                1 - self.ema_decay\n            ) * ema_parameter.data\n    yield\n    parameters = self.parameters\n    for name, parameter in parameters.items():\n        ema_parameter = context[\"ema_parameters\"][name]\n        parameter.data = ema_parameter.data\n    if self.ema_log:\n        logpy.info(\"Restored parameters to EMA weights\")\n\n", "CodeBleu_score": 35.54390977741348}, {"code": "def ema_scope(self, context=None)\n\t\n    if context is None:\n        yield\n    else:\n        if self.parameters is not None:\n            original = self.parameters.copy()\n            self.parameters = context.ema_weights(self.parameters)\n            yield\n            self.parameters = original\n        else:\n            yield\n\n", "CodeBleu_score": 30.384512207448477}, {"code": "def ema_scope(self, context=None)\n\t\n    if context is None:\n        context = self.parameters\n    else:\n        context = copy_to(context, self.parameters)\n    if self.ema_decay > 0.0:\n        if self.ema_log:\n            logpy.info(\n                \"Exponential Moving Average (EMA) is applied to the parameters\"\n            )\n        for name, value in self.parameters.items():\n            if name not in context:\n                continue\n            ema_value = context[name]\n            context[name] = (1.0 - self.ema_decay) * ema_value + self.ema_decay * value\n    yield context\n    if self.ema_decay > 0.0:\n        if self.ema_log:\n            logpy.info(\n                \"Exponential Moving Average (EMA) is restored to the parameters\"\n            )\n        for name, value in context.items():\n            if name not in self.parameters:\n                continue\n            self.parameters[name] = value\n\n", "CodeBleu_score": 34.91297910374767}]}
{"instance_id": "94b7467f98e4", "generation_result": [{"code": "def post_attention(self, attn, x, gate_msa, shift_mlp, scale_mlp, gate_mlp)\n\t\n    x = x + attn * gate_msa\n    x = self.norm2(x)\n    x = self.modulate(x, shift_mlp, scale_mlp)\n    x = self.mlp(x)\n    x = x * gate_mlp\n    return x\n\n", "CodeBleu_score": 40.166779550822454}, {"code": "def post_attention(self, attn, x, gate_msa, shift_mlp, scale_mlp, gate_mlp)\n\t\n    x = x + gate_msa * attn\n    x = self.norm2(x)\n    x = modulate(x, shift_mlp, scale_mlp)\n    x = self.mlp(x)\n    x = gate_mlp * x\n    return x\n\n", "CodeBleu_score": 38.56817526858224}, {"code": "def post_attention(self, attn, x, gate_msa, shift_mlp, scale_mlp, gate_mlp)\n\t\n    x = x + attn * gate_msa\n    x = self.norm2(x)\n    x = self.modulate(x, shift_mlp, scale_mlp)\n    x = self.mlp(x)\n    return x * gate_mlp\n", "CodeBleu_score": 37.4824462150072}]}
{"instance_id": "d9fdc05542a2", "generation_result": [{"code": "def _block_mixing(context, x, context_block, x_block, c)\n\t\n    context_block.pre_attention(context)\n    x_block.pre_attention(x)\n    c = torch.cat([c, context_block.intermediate_query(context), x_block.intermediate_query(x)], dim=1)\n    c = optimized_attention(c)\n    c = torch.split(c, [c.shape[1] // 3, c.shape[1] // 3, c.shape[1] // 3], dim=1)\n    context = context_block.post_attention(context, c[0])\n    x = x_block.post_attention(x, c[1])\n    return context, x\n\n", "CodeBleu_score": 18.53150573017541}, {"code": "def _block_mixing(context, x, context_block, x_block, c)\n\t\n    # Pre-attention\n    context_block.pre_attention(context)\n    x_block.pre_attention(x)\n    # Attention\n    qkv = torch.cat([context_block.query(context), x_block.query(x), context_block.key(context), x_block.key(x), context_block.value(context), x_block.value(x)], dim=1)\n    if c.optimized_attention:\n        qkv = optimized_attention(qkv, context_block.attention_mask(context), c.block_size)\n    else:\n        qkv = attention(qkv, context_block.attention_mask(context), c.block_size)\n    # Split\n    q, k, v = torch.split(qkv, [context_block.q_dim, context_block.k_dim, context_block.v_dim], dim=1)\n    # Post-attention\n    context_block.post_attention(context, q, k, v)\n    x_block.post_attention(x, q, k, v)\n    return context, x\n\n", "CodeBleu_score": 27.798409993451013}, {"code": "def _block_mixing(context, x, context_block, x_block, c)\n\t\n    # Process context through pre-attention block.\n    context = context_block.pre_attention(context)\n    # Process x through pre-attention block.\n    x = x_block.pre_attention(x)\n    # Concatenate context and x.\n    x = torch.cat((context, x), dim=1)\n    # Apply optimized attention.\n    x = optimized_attention(x, c)\n    # Split x back into context and x.\n    context, x = torch.split(x, [context.shape[1], x.shape[1] - context.shape[1]], dim=1)\n    # Process context through post-attention block.\n    context = context_block.post_attention(context)\n    # Process x through post-attention block.\n    x = x_block.post_attention(x)\n    return context, x\n\n", "CodeBleu_score": 16.58922760919754}]}
{"instance_id": "b219bce30160", "generation_result": [{"code": "def get_timestep_embedding(timesteps, embedding_dim)\n\t\n    assert len(timesteps.shape) == 1  # and timesteps.dtype == tf.int32\n    half_dim = embedding_dim // 2\n    emb = math.log(10000) / (half_dim - 1)\n    emb = torch.exp(torch.arange(half_dim, dtype=torch.float32) * -emb)\n    emb = timesteps.float()[:, None] * emb[None, :]\n    emb = torch.cat([torch.sin(emb), torch.cos(emb)], axis=1)\n    if embedding_dim % 2 == 1:  # zero pad\n        emb = torch.nn.functional.pad(emb, (0,1,0,0))\n    assert emb.shape == (timesteps.shape[0], embedding_dim)\n    return emb\n", "CodeBleu_score": 82.29950976005253}, {"code": "def get_timestep_embedding(timesteps, embedding_dim)\n\t\n    assert len(timesteps.shape) == 1  # and timesteps.dtype == tf.int32\n    half_dim = embedding_dim // 2\n    emb = math.log(10000) / (half_dim - 1)\n    emb = torch.exp(torch.arange(half_dim, dtype=torch.float32) * -emb)\n    emb = timesteps.float()[:, None] * emb[None, :]\n    emb = torch.cat([torch.sin(emb), torch.cos(emb)], axis=1)\n    if embedding_dim % 2 == 1:  # zero pad\n        emb = torch.nn.functional.pad(emb, (0, 1, 0, 0))\n    assert emb.shape == [timesteps.shape[0], embedding_dim]\n    return emb\n\n", "CodeBleu_score": 80.05289943128734}, {"code": "def get_timestep_embedding(timesteps, embedding_dim)\n\t\n    assert len(timesteps.shape) == 1  # and timesteps.dtype == tf.int32\n    half_dim = embedding_dim // 2\n    emb = math.log(10000) / (half_dim - 1)\n    emb = torch.exp(torch.arange(half_dim, dtype=torch.float32) * -emb)\n    emb = timesteps.float()[:, None] * emb[None, :]\n    emb = torch.cat([torch.sin(emb), torch.cos(emb)], axis=1)\n    if embedding_dim % 2 == 1:  # zero pad\n        emb = torch.nn.functional.pad(emb, (0, 1, 0, 0))\n    assert emb.shape == (timesteps.shape[0], embedding_dim)\n    return emb\n", "CodeBleu_score": 80.79553721789459}]}
{"instance_id": "6ae8453c86be", "generation_result": [{"code": "def pytorch_attention(q, k, v)\n\t\n    try:\n        return scaled_dot_product_attention(q, k, v)\n    except RuntimeError:\n        return slice_attention(q, k, v)\n", "CodeBleu_score": 2.2207262921060935}, {"code": "def pytorch_attention(q, k, v)\n\t\n    try:\n        out = scaled_dot_product_attention(q, k, v)\n        return out.contiguous().view(out.size(0), -1, out.size(3))\n    except RuntimeError:\n        return slice_attention(q, k, v)\n\n", "CodeBleu_score": 9.139791696810157}, {"code": "def pytorch_attention(q, k, v)\n\t\n    try:\n        return scaled_dot_product_attention(q, k, v)\n    except RuntimeError as e:\n        if 'out of memory' in str(e):\n            logging.warning('PyTorch OOM exception occurred. Switching to slice attention.')\n            return slice_attention(q, k, v)\n        else:\n            raise e\n\n", "CodeBleu_score": 13.184596966285397}]}
{"instance_id": "fe4433c987ad", "generation_result": [{"code": "def apply_control(h, control, name)\n\t\n    if control in control_list and name in control_list[control]:\n        try:\n            h.add_control(control_list[control].pop(name))\n        except:\n            logging.warning(\"Control \" + control + \" with name \" + name + \" could not be added to h.\")\n    return h\n\n", "CodeBleu_score": 29.650205005911683}, {"code": "def apply_control(h, control, name)\n\t\n    if control and name and name in control and control[name]:\n        try:\n            h = h + control[name]\n        except:\n            logging.warning(f\"Control {name} could not be applied to {h}.\")\n        control.pop(name)\n    return h\n\n", "CodeBleu_score": 37.33856209535475}, {"code": "def apply_control(h, control, name)\n\t\n    if control in control_list and name in control_list[control]:\n        try:\n            h += control_list[control][name]\n        except:\n            logging.warning(\"Control \" + control + \" could not be applied to \" + name)\n    return h\n\n", "CodeBleu_score": 31.247665776587763}]}
{"instance_id": "56da4547c101", "generation_result": [{"code": "def get_codebook_entry(self, indices, shape)\n\t\n    indices = indices.unsqueeze(-1).to(torch.int64)\n    shape = torch.tensor(shape).to(torch.int64)\n    shape = shape.flip(-1)\n    one_hot = torch.zeros(indices.shape[0], self.num_embeddings, *shape).to(indices.device)\n    one_hot.scatter_(-1, indices, 1)\n    z_q = torch.matmul(one_hot, self.embedding.weight).permute(0, 3, 1, 2).contiguous()\n    return z_q\n", "CodeBleu_score": 25.245570825076253}, {"code": "def get_codebook_entry(self, indices, shape)\n\t\n    indices = indices.unsqueeze(-1).float()\n    z_q = torch.matmul(indices, self.embedding.weight).view(shape)\n    return z_q\n", "CodeBleu_score": 8.131027929728853}, {"code": "def get_codebook_entry(self, indices, shape)\n\t\n    indices = indices.unsqueeze(-1).to(self.embedding.weight.device)\n    shape = torch.Size(shape) if shape is not None else None\n    quantized_vectors = F.embedding(indices, self.embedding.weight)\n    if shape is not None:\n        quantized_vectors = quantized_vectors.view(shape)\n        quantized_vectors = quantized_vectors.permute(0, 3, 1, 2)\n    return quantized_vectors\n\n", "CodeBleu_score": 23.10584116123382}]}
{"instance_id": "548f8a1fb37d", "generation_result": [{"code": "def make_noise(self)\n\t\n    if self.noise is None:\n        self.noise = []\n        for size in range(3, self.log_size + 1):\n            for i in range(2):\n                tensor = self.make_noise_tensor(size)\n                self.noise.append(tensor)\n    return self.noise\n\n", "CodeBleu_score": 36.03119607811325}, {"code": "def make_noise(self)\n\t\n    if self.log_size > 3:\n        size_list = [2 ** i for i in range(3, self.log_size + 1)]\n        noise = [torch.randn(1, 1, size, size, device=self.device) for size in size_list]\n    else:\n        size_list = [2 ** i for i in range(3, self.log_size + 1)]\n        noise = [torch.randn(1, 1, 4, 4, device=self.device)]\n    return noise\n\n", "CodeBleu_score": 44.26210208662115}, {"code": "def make_noise(self)\n\t\n    size = 4\n    noises = [torch.randn(size, size, 1, 1, device=self.device)]\n    while size < 2 ** self.log_size:\n        size *= 2\n        noises.append(torch.randn(size, size, 1, 1, device=self.device))\n        noises.append(torch.randn(size, size, 1, 1, device=self.device))\n    return noises\n\n", "CodeBleu_score": 30.642941143589574}]}
{"instance_id": "42fd805563c6", "generation_result": [{"code": "def mean_latent(self, num_latent)\n\t\n    latent_in = torch.randn(num_latent, self.latent_dim, device=self.device)\n    latent_out = self.style_mlp(latent_in)\n    latent_out = latent_out.mean(dim=0, keepdim=True)\n    return latent_out\n\n", "CodeBleu_score": 32.76576020199861}, {"code": "def mean_latent(self, num_latent)\n\t\n    latent_in = torch.randn(num_latent, self.latent_dim, device=self.device)\n    latent_out = self.style_mlp(latent_in)\n    latent_out = latent_out.mean(dim=0, keepdim=True)\n    return latent_out\n\n", "CodeBleu_score": 32.76576020199861}, {"code": "def mean_latent(self, num_latent)\n\t\n    mean_style = self.style_mean(torch.randn(num_latent, self.style_dim))\n    return mean_style\n", "CodeBleu_score": 15.300376583730651}]}
{"instance_id": "2f485fd924af", "generation_result": [{"code": "def color_jitter(img, shift)\n\t\n    # Convert the image to HSV\n    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n\n    # Generate random jitter values\n    jitter = np.random.uniform(-shift, shift, 3)\n\n    # Add the jitter values to the image\n    hsv[:, :, 2] = np.clip(hsv[:, :, 2] + jitter, 0, 255)\n\n    # Convert the image back to BGR\n    img = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n\n    return img\n\n", "CodeBleu_score": 21.852330568043605}, {"code": "def color_jitter(img, shift)\n\t\n    jittered_image = np.zeros_like(img)\n    for i in range(3):\n        jittered_image[:, :, i] = np.clip(img[:, :, i] + np.random.uniform(-shift, shift), 0, 1)\n    return jittered_image\n\n", "CodeBleu_score": 38.56785846958042}, {"code": "def color_jitter(img, shift)\n\t\n    img = img.astype(np.float32)\n    # Randomly shift the values of the R, G, and B channels.\n    img[:, :, 0] += shift[0]\n    img[:, :, 1] += shift[1]\n    img[:, :, 2] += shift[2]\n    # Clip the pixel values to ensure they remain in the range 0 to 1.\n    img = np.clip(img, 0, 1)\n    return img\n\n", "CodeBleu_score": 29.084233324437804}]}
{"instance_id": "c87ccf1fbf48", "generation_result": [{"code": "def feed_data(self, data)\n\t\n    self.lq = data['lq'].to(self.device)\n    if 'gt' in data:\n        self.gt = data['gt'].to(self.device)\n    if 'facial_component_locations' in data:\n        self.facial_component_locations = data['facial_component_locations'].to(self.device)\n\n", "CodeBleu_score": 52.49127141876648}, {"code": "def feed_data(self, data)\n\t\n    self.lq = data['lq'].to(self.device)\n    if 'gt' in data:\n        self.gt = data['gt'].to(self.device)\n    if 'left_eye' in data:\n        self.left_eye = data['left_eye'].to(self.device)\n    if 'right_eye' in data:\n        self.right_eye = data['right_eye'].to(self.device)\n    if 'mouth' in data:\n        self.mouth = data['mouth'].to(self.device)\n\n", "CodeBleu_score": 56.77114126112309}, {"code": "def feed_data(self, data)\n\t\n    if self.opt['data_type'] == 'img':\n        self.lq = data['lq'].to(self.device)\n        if 'gt' in data:\n            self.gt = data['gt'].to(self.device)\n    elif self.opt['data_type'] == 'lmdb':\n        self.var_L = data['LQs'].to(self.device)\n        if 'GT' in data:\n            self.real_H = data['GT'].to(self.device)\n\n    # if self.opt['use_face_contour']:\n    #     self.left_eye = data['left_eye'].to(self.device)\n    #     self.right_eye = data['right_eye'].to(self.device)\n    #     self.mouth = data['mouth'].to(self.device)\n\n", "CodeBleu_score": 54.095089434776696}]}
{"instance_id": "8f8c7cb13037", "generation_result": [{"code": "def construct_img_pyramid(self)\n\t\n    self.img_pyramid = [self.img_gt]\n    for i in range(self.num_scales):\n        self.img_pyramid.insert(0, F.interpolate(self.img_pyramid[0], scale_factor=1/2, mode='bilinear'))\n    return self.img_pyramid\n\n", "CodeBleu_score": 22.057601810657417}, {"code": "def construct_img_pyramid(self)\n\t\n    img_pyramid = [self.img_gt]\n    for i in range(self.num_scales - 1):\n        img_pyramid.insert(0, F.interpolate(img_pyramid[0], scale_factor=1 / 2, mode='bilinear'))\n    return img_pyramid\n\n", "CodeBleu_score": 21.04764786777189}, {"code": "def construct_img_pyramid(self)\n\t\n    # Create an empty list for the image pyramid\n    img_pyramid = []\n    # Append the ground truth image to the image pyramid\n    img_pyramid.append(self.img_gt)\n    # Create a copy of the ground truth image\n    img_tmp = self.img_gt.copy()\n    # Loop over the number of pyramid levels\n    for i in range(self.num_pyramid_levels):\n        # Downscale the image using bilinear interpolation\n        img_tmp = F.interpolate(img_tmp, scale_factor=1 / 2, mode='bilinear', align_corners=False)\n        # Append the downscaled image to the image pyramid\n        img_pyramid.insert(0, img_tmp)\n    # Return the image pyramid\n    return img_pyramid\n\n", "CodeBleu_score": 34.69893466241813}]}
{"instance_id": "51247274d40c", "generation_result": [{"code": "def save(self, epoch, current_iter)\n\t\n    save_network(self.netG, 'G', epoch, current_iter, self.opt)\n    save_network(self.netG_EMA, 'GEMA', epoch, current_iter, self.opt)\n    save_network(self.netD, 'D', epoch, current_iter, self.opt)\n    if self.opt.use_facial_disc:\n        save_network(self.netD_left, 'D_left', epoch, current_iter, self.opt)\n        save_network(self.netD_right, 'D_right', epoch, current_iter, self.opt)\n        save_network(self.netD_mouth, 'D_mouth', epoch, current_iter, self.opt)\n    save_training_state(self.opt, epoch, current_iter)\n\n", "CodeBleu_score": 46.17900373243227}, {"code": "def save(self, epoch, current_iter)\n\t\n    # Save the generator, EMA generator, and discriminator\n    torch.save(self.generator.state_dict(), os.path.join(self.model_dir, 'generator.pth'))\n    torch.save(self.generator_ema.state_dict(), os.path.join(self.model_dir, 'generator_ema.pth'))\n    torch.save(self.discriminator.state_dict(), os.path.join(self.model_dir, 'discriminator.pth'))\n    # Save the facial discriminators\n    if self.use_facial_disc:\n        torch.save(self.facial_disc_left_eye.state_dict(), os.path.join(self.model_dir, 'facial_disc_left_eye.pth'))\n        torch.save(self.facial_disc_right_eye.state_dict(), os.path.join(self.model_dir, 'facial_disc_right_eye.pth'))\n        torch.save(self.facial_disc_mouth.state_dict(), os.path.join(self.model_dir, 'facial_disc_mouth.pth'))\n    # Save the training state\n    torch.save({'epoch': epoch,\n                'iteration': current_iter,\n                'generator_loss': self.generator_loss,\n                'discriminator_loss': self.discriminator_loss,\n                'generator_optimizer': self.generator_optimizer.state_dict(),\n                'discriminator_optimizer': self.discriminator_optimizer.state_dict(),\n                'latent_avg': self.latent_avg,\n                'latent_std': self.latent_std,\n                'config': self.config,\n                'step': self.step,\n                'total_steps': self.total_steps,\n                'use_ema': self.use_ema,\n                'ema_decay': self.ema_decay,\n                'ema_generator_state': self.ema_generator_state,\n                'ema_discriminator_state': self.ema_discriminator_state,\n                'ema_facial_disc_left_eye_state': self.ema_facial_disc_left_eye_state,\n                'ema_facial_disc_right_eye_state': self.ema_facial_disc_right_eye_state,\n               ", "CodeBleu_score": 33.26251497154236}, {"code": "def save(self, epoch, current_iter)\n\t\n    if self.save_training_state:\n        self.save_training_state(epoch, current_iter)\n    if self.save_network:\n        self.save_network(epoch, current_iter)\n\n", "CodeBleu_score": 24.02746136015004}]}
{"instance_id": "afa0dc8128c1", "generation_result": [{"code": "def clean_folder(folder)\n\t\n    if not os.path.isdir(folder):\n        print(\"The specified folder does not exist.\")\n        return\n    for item in os.listdir(folder):\n        item_path = os.path.join(folder, item)\n        if os.path.isfile(item_path):\n            try:\n                os.unlink(item_path)\n            except Exception as e:\n                print(\"Error deleting file {}: {}\".format(item_path, e))\n        elif os.path.islink(item_path):\n            try:\n                os.unlink(item_path)\n            except Exception as e:\n                print(\"Error deleting link {}: {}\".format(item_path, e))\n        elif os.path.isdir(item_path):\n            try:\n                shutil.rmtree(item_path)\n            except Exception as e:\n                print(\"Error deleting folder {}: {}\".format(item_path, e))\n\n", "CodeBleu_score": 34.81653538371434}, {"code": "def clean_folder(folder)\n\t\n    for item in os.listdir(folder):\n        path = os.path.join(folder, item)\n        if os.path.isfile(path):\n            try:\n                os.unlink(path)\n            except Exception as e:\n                print('Failed to delete %s. Reason: %s' % (path, e))\n        elif os.path.isdir(path):\n            try:\n                shutil.rmtree(path)\n            except Exception as e:\n                print('Failed to delete %s. Reason: %s' % (path, e))\n", "CodeBleu_score": 45.44023108738992}, {"code": "def clean_folder(folder)\n\t\n    for item in os.listdir(folder):\n        path = os.path.join(folder, item)\n        try:\n            if os.path.isfile(path):\n                os.unlink(path)\n            elif os.path.isdir(path):\n                shutil.rmtree(path)\n        except Exception as e:\n            print('Failed to delete %s. Reason: %s' % (path, e))\n", "CodeBleu_score": 47.84937828114967}]}
{"instance_id": "53588b481bdd", "generation_result": [{"code": "def next_batch(self, sample_size=1)\n\t\n    if sample_size > len(self.memory):\n        sample_size = len(self.memory)\n    return random.sample(self.memory, sample_size)\n", "CodeBleu_score": 32.782246025742744}, {"code": "def next_batch(self, sample_size=1)\n\t\n    batch = []\n    while len(batch) < sample_size:\n        if len(self.memory) > 0:\n            batch.append(self.memory.pop())\n        else:\n            break\n    return batch\n\n", "CodeBleu_score": 23.853932497620004}, {"code": "def next_batch(self, sample_size=1)\n\t\n    if sample_size > len(self.memory):\n        sample_size = len(self.memory)\n    batch = random.sample(self.memory, sample_size)\n    for experience in batch:\n        self.memory.remove(experience)\n    return batch\n", "CodeBleu_score": 49.02659354588814}]}
{"instance_id": "6bfa6a4e621c", "generation_result": [{"code": "def find_estimated_top_configs(self)\n\t\n    flattened_ds_config = self.flatten_ds_config(self.ds_config)\n    flattened_ds_config = flattened_ds_config.values()\n    flattened_ds_config = list(flattened_ds_config)\n    flattened_ds_config = np.array(flattened_ds_config)\n    estimated_costs = self.cost_model.predict(flattened_ds_config)\n    estimated_costs = np.argsort(estimated_costs)\n    return estimated_costs\n", "CodeBleu_score": 11.414162835289376}, {"code": "def find_estimated_top_configs(self)\n\t\n    flattened_ds_config = self.flatten_configs()\n    ds_config_values = np.array(list(flattened_ds_config.values()))\n    ds_config_values = ds_config_values.reshape(1, -1)\n    estimated_costs = self.cost_model.predict(ds_config_values)\n    estimated_costs = estimated_costs.flatten()\n    estimated_costs_sorted_indices = np.argsort(estimated_costs)\n    return estimated_costs_sorted_indices\n\n", "CodeBleu_score": 15.842881145473548}, {"code": "def find_estimated_top_configs(self)\n\t\n    # flatten the configs\n    flattened_configs = [config.flatten() for config in self.all_configs]\n    # collect the numerical values\n    numerical_values = [\n        [\n            config[feature]\n            for config in flattened_configs\n            if feature in config\n        ]\n        for feature in self.ds_config.keys()\n    ]\n    # convert to numpy array\n    numerical_values = np.array(numerical_values)\n    # predict the estimated cost for each configuration\n    estimated_costs = self.cost_model.predict(numerical_values)\n    # sort the configurations based on the specified metric\n    if self.metric == 'cost':\n        sorted_indices = np.argsort(estimated_costs)\n    elif self.metric == 'time':\n        sorted_indices = np.argsort(self.time_per_config)\n    elif self.metric == 'accuracy':\n        sorted_indices = np.argsort(self.accuracy_per_config)\n    else:\n        raise ValueError(f'Invalid metric: {self.metric}')\n    # return the indices of the top configurations\n    return sorted_indices[-self.num_configs:]\n\n", "CodeBleu_score": 24.62387313609586}]}
{"instance_id": "d4cf48413a57", "generation_result": [{"code": "def flatten(d, parent_key='', sep='_')\n\t\n    items = []\n    for k, v in d.items():\n        new_key = parent_key + sep + k if parent_key else k\n        if isinstance(v, collections.MutableMapping):\n            items.extend(flatten(v, new_key, sep=sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)\n", "CodeBleu_score": 76.29541503199566}, {"code": "def flatten(d, parent_key='', sep='_')\n\t\n    items = []\n    for k, v in d.items():\n        new_key = parent_key + sep + k if parent_key else k\n        if isinstance(v, dict):\n            items.extend(flatten(v, new_key, sep=sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)\n", "CodeBleu_score": 73.40656824096638}, {"code": "def flatten(d, parent_key='', sep='_')\n\t\n    items = []\n    for k, v in d.items():\n        new_key = parent_key + sep + k if parent_key else k\n        if isinstance(v, collections.MutableMapping):\n            items.extend(flatten(v, new_key, sep=sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)\n\n", "CodeBleu_score": 76.29541503199566}]}
{"instance_id": "c2a514249c25", "generation_result": [{"code": "def run_after_tuning(self)\n\t\n    if self.optimal_cmd:\n        logger.info(\"Running command after tuning...\")\n        result = subprocess.Popen(self.optimal_cmd, shell=True)\n        result.wait()\n        logger.info(\"Command after tuning completed.\")\n    else:\n        logger.info(\"No optimal command found.\")\n", "CodeBleu_score": 51.84188151963496}, {"code": "def run_after_tuning(self)\n\t\n    if self.optimal_cmd:\n        logger.info(\n            \"Optimal command found, running command: {}\".format(self.optimal_cmd)\n        )\n        result = subprocess.Popen(self.optimal_cmd, shell=True)\n        result.wait()\n        logger.info(\"Command finished running\")\n    else:\n        logger.info(\"No optimal command found\")\n\n", "CodeBleu_score": 52.013854678497395}, {"code": "def run_after_tuning(self)\n\t\n    if self.optimal_cmd:\n        logger.info(\"Running command after autotuning: %s\", self.optimal_cmd)\n        result = subprocess.Popen(self.optimal_cmd, shell=True)\n        result.wait()\n        logger.info(\"Command after autotuning completed.\")\n    else:\n        logger.info(\"No optimal command found.\")\n", "CodeBleu_score": 52.56234614251392}]}
{"instance_id": "61802de52df0", "generation_result": [{"code": "def run_job(self, exp: dict, reservations)\n\t\n    # Set the master port\n    self.master_port = exp['master_port']\n    # Set the result directory\n    self.result_dir = exp['result_dir']\n    # Update user arguments based on argument mappings\n    user_args = self.update_user_args(exp['user_args'], reservations)\n    # Get the script and arguments\n    script = exp['script']\n    args = exp['args']\n    # Create a new thread to run the experiment\n    t = threading.Thread(target=self.run_experiment, args=(script, args, user_args))\n    # Start the thread\n    t.start()\n    # Store the experiment's details\n    self.running_experiments[exp['id']] = {\n        'script': script,\n        'args': args,\n        'user_args': user_args,\n        'thread': t,\n        'start_time': time.time()\n    }\n    # Print the experiment's details\n    print('Experiment {} has started with script {} and arguments {}'.format(exp['id'], script, args))\n\n", "CodeBleu_score": 23.15618216155167}, {"code": "def run_job(self, exp: dict, reservations)\n\t\n    exp_id = exp[\"exp_id\"]\n    exp_name = exp[\"exp_name\"]\n    exp_dir = exp[\"exp_dir\"]\n    exp_script = exp[\"exp_script\"]\n    exp_args = exp[\"exp_args\"]\n    exp_user_args = exp[\"exp_user_args\"]\n    exp_user_arg_map = exp[\"exp_user_arg_map\"]\n    exp_user_arg_vals = exp[\"exp_user_arg_vals\"]\n    exp_user_arg_map_vals = exp[\"exp_user_arg_map_vals\"]\n    exp_user_arg_map_types = exp[\"exp_user_arg_map_types\"]\n    exp_user_arg_map_choices = exp[\"exp_user_arg_map_choices\"]\n    exp_user_arg_map_descs = exp[\"exp_user_arg_map_descs\"]\n    exp_user_arg_map_req = exp[\"exp_user_arg_map_req\"]\n    exp_user_arg_map_req_vals = exp[\"exp_user_arg_map_req_vals\"]\n    exp_user_arg_map_req_types = exp[\"exp_user_arg_map_req_types\"]\n    exp_user_arg_map_req_choices = exp[\"exp_user_arg_map_req_choices\"]\n    exp_user_arg_map_req_descs = exp[\"exp_user_arg_map_req_descs\"]\n    exp_user_arg_map_req_vals_str = exp[\"exp_user_arg_map_req_vals_str\"]\n    exp_user_arg_map_req_vals_str_list = exp[\"exp_user_arg_map_req_vals_str_list\"]\n    exp_user_arg_map_req_vals_list = exp[\"exp_user_arg_map_req_vals_list\"]\n    exp_user_arg_map_req_vals_list_str = exp[\"exp_user_arg_map_req_vals_list_str\"]\n    exp_user_arg_map_req_vals_list_str_list = exp[\"exp_user_arg_map_req_vals_list_str_list\"]\n    exp_user_arg", "CodeBleu_score": 9.57335501520811}, {"code": "def run_job(self, exp: dict, reservations)\n\t\n    # Get experiment details\n    exp_name = exp.get(\"name\")\n    script = exp.get(\"script\")\n    args = exp.get(\"args\")\n    env = exp.get(\"env\")\n    # Set experiment parameters\n    self.set_experiment_params(exp)\n    # Update user arguments based on argument mappings\n    self.update_user_args(args)\n    # Set master port\n    self.set_master_port()\n    # Set result directory\n    self.set_result_dir()\n    # Create new thread\n    t = threading.Thread(target=self.run_exp, args=(script, self.user_args, env))\n    # Start thread\n    t.start()\n    # Store experiment details\n    self.running_experiments[exp_name] = {\n        \"time\": time.time(),\n        \"thread\": t,\n        \"args\": self.user_args,\n        \"env\": env\n    }\n\n", "CodeBleu_score": 25.35099630730403}]}
{"instance_id": "4884e2e9665c", "generation_result": [{"code": "def save_exp_results_to_database(self, message, ranks=None, path=None):\n\t\n    if dist.is_initialized() and ranks is not None:\n        if dist.get_rank() in ranks:\n            message['rank'] = dist.get_rank()\n            message['world_size'] = dist.get_world_size()\n            message['time'] = time.time()\n            message = json.dumps(message)\n            if path is not None:\n                with open(path, 'a') as outfile:\n                    outfile.write(message + '", "CodeBleu_score": 35.28127642574559}, {"code": "def save_exp_results_to_database(self, message, ranks=None, path=None):\n\t\n    if not dist.is_initialized():\n        ranks = None\n        path = None\n    if ranks is not None and dist.get_rank() in ranks:\n        message[\"rank\"] = dist.get_rank()\n        with open(path, \"a\") as outfile:\n            json.dump(message, outfile)\n            outfile.write(\"", "CodeBleu_score": 27.10154299230645}, {"code": "def save_exp_results_to_database(self, message, ranks=None, path=None):\n\t\n    if not dist.is_initialized() or dist.get_rank() == 0:\n        if ranks is None:\n            ranks = [0]\n        if path is None:\n            path = self.save_path\n        message[\"rank\"] = dist.get_rank()\n        with open(path, \"a\") as outfile:\n            json.dump(message, outfile)\n            outfile.write(\"", "CodeBleu_score": 31.87664864118669}]}
{"instance_id": "0429344c3c4d", "generation_result": [{"code": "def _build_tp_other_layer_map(self, layer_index: int)\n\t\n    if not self.layer_files:\n        raise ValueError(\"No layer files found.\")\n    if layer_index >= len(self.layer_files):\n        raise ValueError(\n            f\"Layer index {layer_index} is greater than the number of layers.\"\n        )\n    layer_files = self.layer_files[layer_index]\n    layer_files = get_files_with_prefix(layer_files, self.prefix)\n    layer_files = partition_data(layer_files, self.tp_degree)\n    return {i: layer_files[i] for i in range(self.tp_degree)}\n\n", "CodeBleu_score": 39.80036911071658}, {"code": "def _build_tp_other_layer_map(self, layer_index: int)\n\t\n    if self.tp_degree == 1:\n        return None\n    layer_files = self._get_files_with_prefix(self.tp_layer_prefix, layer_index)\n    if len(layer_files) == 0:\n        return None\n    partition_data = self.partition_data(layer_files, self.tp_degree)\n    return {i: partition_data[i] for i in range(self.tp_degree)}\n\n", "CodeBleu_score": 35.85940875904105}, {"code": "def _build_tp_other_layer_map(self, layer_index: int)\n\t\n    if not self.has_layers:\n        raise ValueError(f\"No layer files found in the directory {self.directory}.\")\n    if layer_index >= self.num_layers:\n        raise ValueError(f\"Layer index {layer_index} is out of bounds. There are only {self.num_layers} layers.\")\n    layer_files = get_files_with_prefix(self.directory, f\"layer{layer_index}_\")\n    if not layer_files:\n        raise ValueError(f\"No layer files found in the directory {self.directory}.\")\n    partitions = partition_data(layer_files, self.tp_degree)\n    return {i: partitions[i] for i in range(len(partitions))}\n\n", "CodeBleu_score": 35.015919940907054}]}
{"instance_id": "1b65ecb9be93", "generation_result": [{"code": "def system_info()\n\t\n    return {\n        'platform': platform.platform(),\n        'platform.python_version': platform.python_version(),\n        'platform.version': platform.version(),\n        'platform.architecture': platform.architecture(),\n        'platform.system': platform.system(),\n        'platform.processor': platform.processor()\n    }\n\n", "CodeBleu_score": 62.56002721795673}, {"code": "def system_info()\n\t\n    info = {'os': platform.system(), 'arch': platform.architecture()[0], 'version': platform.version(), 'processor': platform.processor(), 'python_version': platform.python_version()}\n    return info\n\n", "CodeBleu_score": 47.30900102137762}, {"code": "def system_info()\n\t\n    return {\n        \"Operating System\": platform.system(),\n        \"Architecture\": platform.architecture(),\n        \"Version\": platform.version(),\n        \"Processor\": platform.processor(),\n        \"Python Version\": platform.python_version(),\n    }\n\n", "CodeBleu_score": 69.65854801721277}]}
{"instance_id": "969bcc5cf3b5", "generation_result": [{"code": "def cuda_info()\n\t\n    print(\"CUDA Version: \" + str(torch.version.cuda))\n    print(\"CUDA is available: \" + str(torch.cuda.is_available()))\n    print(\"Number of CUDA devices: \" + str(torch.cuda.device_count()))\n    print(\"Device Name: \" + torch.cuda.get_device_name(0))\n    print(\"Device Name: \" + torch.cuda.get_device_name(1))\n    print(\"Device Name: \" + torch.cuda.get_device_name(2))\n    print(\"Device Name: \" + torch.cuda.get_device_name(3))\n    print(\"Device Name: \" + torch.cuda.get_device_name(4))\n    print(\"Device Name: \" + torch.cuda.get_device_name(5))\n    print(\"Device Name: \" + torch.cuda.get_device_name(6))\n    print(\"Device Name: \" + torch.cuda.get_device_name(7))\n    print(\"Device Name: \" + torch.cuda.get_device_name(8))\n    print(\"Device Name: \" + torch.cuda.get_device_name(9))\n    print(\"Device Name: \" + torch.cuda.get_device_name(10))\n    print(\"Device Name: \" + torch.cuda.get_device_name(11))\n    print(\"Device Name: \" + torch.cuda.get_device_name(12))\n    print(\"Device Name: \" + torch.cuda.get_device_name(13))\n    print(\"Device Name: \" + torch.cuda.get_device_name(14))\n    print(\"Device Name: \" + torch.cuda.get_device_name(15))\n    print(\"Device Name: \" + torch.cuda.get_device_name(16))\n    print(\"Device Name: \" + torch.cuda.get_device_name(17))\n    print(\"Device Name: \" + torch.cuda.get_device_name(18))\n    print(\"Device Name: \" + torch.cuda.get_device_name(19))\n    print(\"Device Name: \" + torch.cuda.get_device_name(20))\n    print(\"Device Name: \" + torch.cuda.get_device_name(21))\n    print(\"Device Name", "CodeBleu_score": 30.708603827245582}, {"code": "def cuda_info()\n\t\n    print(\"CUDA info:\")\n    print(\"CUDA version: \" + str(torch.version.cuda))\n    print(\"CUDA device count: \" + str(torch.cuda.device_count()))\n    print(\"CUDA device name: \" + torch.cuda.get_device_name(0))\n    print(\"CUDA device available: \" + str(torch.cuda.is_available()))\n    print(\"CUDA device range: \" + str(torch.cuda.device_count()))\n\n", "CodeBleu_score": 36.82010219718386}, {"code": "def cuda_info()\n\t\n    print('CUDA is available: ', torch.cuda.is_available())\n    print('CUDA version: ', torch.version.cuda)\n    print('Number of CUDA devices: ', torch.cuda.device_count())\n    print('Name of CUDA devices: ', torch.cuda.get_device_name(0))\n    print('CUDA devices: ', torch.cuda.device_count())\n    print('CUDA devices: ', torch.cuda.device_count())\n    print('CUDA devices: ', torch.cuda.device_count())\n    print('CUDA devices: ', torch.cuda.device_count())\n    print('CUDA devices: ', torch.cuda.device_count())\n    print('CUDA devices: ', torch.cuda.device_count())\n    print('CUDA devices: ', torch.cuda.device_count())\n    print('CUDA devices: ', torch.cuda.device_count())\n    print('CUDA devices: ', torch.cuda.device_count())\n    print('CUDA devices: ', torch.cuda.device_count())\n    print('CUDA devices: ', torch.cuda.device_count())\n    print('CUDA devices: ', torch.cuda.device_count())\n    print('CUDA devices: ', torch.cuda.device_count())\n    print('CUDA devices: ', torch.cuda.device_count())\n    print('CUDA devices: ', torch.cuda.device_count())\n    print('CUDA devices: ', torch.cuda.device_count())\n    print('CUDA devices: ', torch.cuda.device_count())\n    print('CUDA devices: ', torch.cuda.device_count())\n    print('CUDA devices: ', torch.cuda.device_count())\n    print('CUDA devices: ', torch.cuda.device_count())\n    print('CUDA devices: ', torch.cuda.device_count())\n    print('CUDA devices: ', torch.cuda.device_count())\n    print('CUDA devices: ', torch.cuda.device_count())\n    print('CUDA devices: ', torch.cuda.device_count())\n    print('CUDA devices: ', torch.cuda.device_count())\n    print('CUDA devices: ', torch.cuda.device_count())\n    print('CUDA devices: ', torch.cuda.device_count())\n    print('CUDA devices: ', torch.cuda.device_count())\n    print('CUDA devices: ', torch.cuda.device_count())\n    print('CUDA devices: ', torch.cuda", "CodeBleu_score": 25.1505233476508}]}
{"instance_id": "a2ba908a4c70", "generation_result": [{"code": "def compute_encoder_accuracy(dataset_items, encoder_manager)\n\t\n    class_acc_dict = {}\n    for item in tqdm(dataset_items):\n        audio_file = item.get(\"audio_file\")\n        if audio_file is None:\n            raise RuntimeError(\"Audio file not found in dataset item.\")\n        embedding = encoder_manager.compute_embedding_from_clip(audio_file)\n        if embedding is None:\n            raise RuntimeError(\"Unable to compute embedding from audio file.\")\n        if encoder_manager.config.get(\"class_id_to_label_map\") is not None:\n            class_id = item.get(\"class_id\")\n            if class_id is None:\n                raise RuntimeError(\"Class ID not found in dataset item.\")\n            class_name = encoder_manager.config.get(\"class_id_to_label_map\")[class_id]\n        else:\n            class_name = item.get(\"class_name\")\n            if class_name is None:\n                raise RuntimeError(\"Class name not found in dataset item.\")\n        if class_name not in class_acc_dict:\n            class_acc_dict[class_name] = []\n        class_acc_dict[class_name].append(\n            int(encoder_manager.predict(embedding) == class_name)\n        )\n    for class_name, acc_list in class_acc_dict.items():\n        print(f\"{class_name}: {sum(acc_list) / len(acc_list)}\")\n    print(f\"Average accuracy: {sum(list(class_acc_dict.values())) / len(class_acc_dict)}\")\n\n", "CodeBleu_score": 34.95113391645282}, {"code": "def compute_encoder_accuracy(dataset_items, encoder_manager)\n\t\n    class_acc_dict = {}\n    for item in tqdm(dataset_items):\n        audio_file_path = item[\"audio_file_path\"]\n        true_class_name = item[\"class_name\"]\n        if \"class_id\" in item:\n            true_class_id = item[\"class_id\"]\n        else:\n            true_class_id = None\n        try:\n            embedding = encoder_manager.compute_embedding_from_clip(audio_file_path)\n            embedding = embedding.cuda()\n            predicted_class_name = encoder_manager.predict_class_name(embedding)\n            if true_class_name not in class_acc_dict:\n                class_acc_dict[true_class_name] = []\n            if predicted_class_name == true_class_name:\n                class_acc_dict[true_class_name].append(1)\n            else:\n                class_acc_dict[true_class_name].append(0)\n        except RuntimeError:\n            print(f\"RuntimeError: {audio_file_path} is corrupted.\")\n    for class_name, class_acc_list in class_acc_dict.items():\n        accuracy = sum(class_acc_list) / len(class_acc_list)\n        print(f\"{class_name}: {accuracy}\")\n    accuracy = sum([sum(v) for v in class_acc_dict.values()]) / sum([len(v) for v in class_acc_dict.values()])\n    print(f\"Average: {accuracy}\")\n    return accuracy\n", "CodeBleu_score": 34.68326973425772}, {"code": "def compute_encoder_accuracy(dataset_items, encoder_manager)\n\t\n    class_acc_dict = {}\n    for item in tqdm(dataset_items):\n        audio_file = item['audio_filepath']\n        class_name = item['label']\n        class_id = item['label_id']\n        if class_name not in class_acc_dict:\n            class_acc_dict[class_name] = []\n        embedding = encoder_manager.compute_embedding_from_clip(audio_file)\n        if embedding is None:\n            continue\n        if class_id is None:\n            class_id = 0\n        class_id = int(class_id)\n        predicted_class_id = encoder_manager.predict_class_from_embedding(embedding)\n        if predicted_class_id == class_id:\n            class_acc_dict[class_name].append(1)\n        else:\n            class_acc_dict[class_name].append(0)\n    for class_name, acc_list in class_acc_dict.items():\n        acc = sum(acc_list) / len(acc_list)\n        print(f'{class_name}: {acc}')\n    acc_list = []\n    for acc_list_for_class in class_acc_dict.values():\n        acc_list += acc_list_for_class\n    acc = sum(acc_list) / len(acc_list)\n    print(f'Average: {acc}')\n", "CodeBleu_score": 32.969117635857145}]}
{"instance_id": "e01ec69ad58a", "generation_result": [{"code": "def adjust_path_and_remove_silence(audio_path)\n\t\n    # If the file already exists, return the output path and a False flag.\n    if exists(audio_path.replace(input_dir, output_dir)):\n        return audio_path.replace(input_dir, output_dir), False\n    # Otherwise, create the necessary directory structure, remove silence, and save the processed audio.\n    else:\n        # Create the directory structure.\n        if not exists(output_dir):\n            mkdir(output_dir)\n        if not exists(output_dir + '/' + audio_path.split('/')[-2]):\n            mkdir(output_dir + '/' + audio_path.split('/')[-2])\n        # Remove silence from the audio.\n        remove_silence(audio_path, output_dir + '/' + audio_path.split('/')[-2] + '/' + audio_path.split('/')[-1])\n        # Return the output path and a True flag.\n        return audio_path.replace(input_dir, output_dir), True\n", "CodeBleu_score": 23.171108982835552}, {"code": "def adjust_path_and_remove_silence(audio_path)\n\t\n    output_path = audio_path.replace(input_dir, output_dir)\n    if not force and exists(output_path):\n        return output_path, False\n    else:\n        if not exists(output_path.parent):\n            mkdir(output_path.parent)\n        if not force and exists(output_path):\n            remove(output_path)\n        remove_silence(audio_path, output_path, **kwargs)\n        return output_path, True\n\n", "CodeBleu_score": 25.728166058292313}, {"code": "def adjust_path_and_remove_silence(audio_path)\n\t\n    # get the output path\n    output_path = get_output_path(audio_path)\n    # check if the file already exists\n    if output_path.exists() and not force:\n        return output_path, False\n    # create the necessary directory structure\n    output_path.parent.mkdir(parents=True, exist_ok=True)\n    # remove silence from the audio\n    remove_silence(audio_path, output_path, min_silence_len, silence_thresh, keep_silence, seek_step)\n    # return the output path and a flag indicating if speech was detected\n    return output_path, True\n\n", "CodeBleu_score": 24.992627954530004}]}
{"instance_id": "7e0bdb0feff8", "generation_result": [{"code": "def resample_files(input_dir, output_sr, output_dir=None, file_ext=\"wav\", n_jobs=10)\n\t\n    if output_dir is None:\n        output_dir = input_dir + \"_resampled\"\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    else:\n        raise Exception(\"Output directory already exists.\")\n    files = glob.glob(input_dir + \"/**/*.\" + file_ext, recursive=True)\n    if len(files) == 0:\n        raise Exception(\"No files found.\")\n    else:\n        print(\"Found \" + str(len(files)) + \" files.\")\n    print(\"Resampling...\")\n    with Pool(n_jobs) as p:\n        with tqdm(total=len(files)) as pbar:\n            for i, _ in enumerate(p.imap_unordered(resample_file, zip(files, [output_sr] * len(files), [output_dir] * len(files)))):\n                pbar.update()\n    print(\"Done.\")\n\n", "CodeBleu_score": 42.14278945308694}, {"code": "def resample_files(input_dir, output_sr, output_dir=None, file_ext=\"wav\", n_jobs=10)\n\t\n    if output_dir is None:\n        output_dir = input_dir\n    else:\n        copytree(input_dir, output_dir)\n    input_files = glob.glob(os.path.join(input_dir, \"**\", f\"*.{file_ext}\"), recursive=True)\n    resample_files_parallel(input_files, output_sr, output_dir, n_jobs)\n\n", "CodeBleu_score": 22.238058254937233}, {"code": "def resample_files(input_dir, output_sr, output_dir=None, file_ext=\"wav\", n_jobs=10)\n\t\n    if output_dir is not None:\n        copytree(input_dir, output_dir)\n    audio_files = glob.glob(f\"{input_dir}/**/*.{file_ext}\", recursive=True)\n    print(f\"Resampling {len(audio_files)} files...\")\n    with Pool(n_jobs) as p:\n        with tqdm(total=len(audio_files)) as pbar:\n            for i, _ in enumerate(\n                p.imap_unordered(\n                    resample_file,\n                    zip(audio_files, [output_sr] * len(audio_files)),\n                )\n            ):\n                pbar.update()\n    print(\"Done!\")\n\n", "CodeBleu_score": 48.43418106837545}]}
{"instance_id": "a1351834ca57", "generation_result": [{"code": "def evaluation(model, criterion, data_loader, global_step)\n\t\n    model.eval()\n    dashboard_logger.eval_stats.reset()\n    dashboard_logger.eval_figures.reset()\n    with torch.no_grad():\n        for batch_idx, (inputs, labels) in enumerate(data_loader):\n            inputs = inputs.view(inputs.shape[0], -1)\n            labels = labels.cuda()\n            inputs = inputs.cuda()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            dashboard_logger.eval_stats.update(loss.item(), labels.view(-1, 1), outputs.detach())\n            dashboard_logger.eval_figures.update(inputs.view(-1, 1), outputs.detach())\n    dashboard_logger.eval_stats.log(global_step)\n    dashboard_logger.eval_figures.log(global_step)\n    return dashboard_logger.eval_stats.avg_loss\n\n", "CodeBleu_score": 26.231030785617744}, {"code": "def evaluation(model, criterion, data_loader, global_step)\n\t\n    model.eval()\n    total_loss = 0\n    total_correct = 0\n    total_samples = 0\n    for batch_idx, (inputs, labels) in enumerate(data_loader):\n        inputs = inputs.view(inputs.size(0), -1)\n        labels = labels.view(labels.size(0), -1)\n        inputs = inputs.cuda()\n        labels = labels.cuda()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        total_loss += loss.item()\n        _, predicted = torch.max(outputs.data, 1)\n        total_correct += (predicted == labels).sum().item()\n        total_samples += labels.size(0)\n        if batch_idx == len(data_loader) - 1:\n            embeddings = outputs.detach().cpu().numpy()\n            labels = labels.cpu().numpy()\n            plot_embeddings(embeddings, labels, global_step)\n    avg_loss = total_loss / len(data_loader)\n    avg_acc = total_correct / total_samples\n    dashboard_logger.eval_stats(global_step, avg_loss, avg_acc)\n    return avg_loss\n\n", "CodeBleu_score": 35.557940237553055}, {"code": "def evaluation(model, criterion, data_loader, global_step)\n\t\n    model.eval()\n    dashboard_logger.eval_stats = {}\n    dashboard_logger.eval_figures = {}\n    total_loss = 0\n    total_samples = 0\n    with torch.no_grad():\n        for inputs, labels in data_loader:\n            inputs = inputs.view(inputs.shape[0], -1)\n            labels = labels.view(labels.shape[0], -1)\n            inputs = inputs.cuda()\n            labels = labels.cuda()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            total_loss += loss.item() * inputs.shape[0]\n            total_samples += inputs.shape[0]\n            dashboard_logger.eval_stats[global_step] = {\n                \"loss\": total_loss / total_samples\n            }\n            if global_step == 0:\n                dashboard_logger.eval_figures[global_step] = {\n                    \"umap\": umap.UMAP().fit_transform(outputs.cpu())\n                }\n    return total_loss / total_samples\n\n", "CodeBleu_score": 34.355193838756456}]}
{"instance_id": "565f8ed09916", "generation_result": [{"code": "def register_config(model_name: str) -> Coqpit\n\t\n    config_name = model_name.capitalize() + 'Config'\n    config_paths = [\n        'coqpit.configs.' + model_name,\n        'coqpit.configs.xtts',\n        'coqpit.configs.tts',\n        'coqpit.configs.asr',\n        'coqpit.configs.st',\n        'coqpit.configs.mt',\n        'coqpit.configs.tts_models',\n        'coqpit.configs.asr_models',\n        'coqpit.configs.st_models',\n        'coqpit.configs.mt_models',\n        'coqpit.configs.common'\n    ]\n    for config_path in config_paths:\n        try:\n            config_module = importlib.import_module(config_path)\n            config_class = getattr(config_module, config_name)\n            return config_class\n        except ModuleNotFoundError:\n            pass\n    raise ModuleNotFoundError(f'No configuration class found for {model_name} model')\n", "CodeBleu_score": 34.5619005726106}, {"code": "def register_config(model_name: str) -> Coqpit\n\t\n    config_name = model_name.upper() + \"Config\"\n    try:\n        module = _find_module(config_name)\n        config_class = getattr(module, config_name)\n        return config_class\n    except ModuleNotFoundError:\n        if model_name == \"xtts\":\n            config_name = \"XTTSConfig\"\n            module = _find_module(config_name)\n            config_class = getattr(module, config_name)\n            return config_class\n        else:\n            raise ModuleNotFoundError(\n                f\"No configuration class found for {model_name}.\"\n            )\n", "CodeBleu_score": 36.411770890912535}, {"code": "def register_config(model_name: str) -> Coqpit\n\t\n    # Construct the configuration class name\n    config_class_name = model_name.capitalize() + \"Config\"\n    # Check if the model is \"xtts\" to import its specific configuration\n    if model_name == \"xtts\":\n        # Import the configuration class\n        from .configs.xtts import XTTSConfig\n        # Assign the configuration class\n        config_class = XTTSConfig\n    else:\n        # Search through predefined paths to find and assign the configuration class, handling ModuleNotFoundError exceptions\n        for path in config_paths:\n            try:\n                # Import the configuration class\n                config_module = import_module(path)\n                config_class = getattr(config_module, config_class_name)\n                # If the configuration class is found, break the loop\n                break\n            except ModuleNotFoundError:\n                # Continue the loop if the configuration class is not found\n                continue\n        # If the configuration class is not found, raise a ModuleNotFoundError\n        if not config_class:\n            raise ModuleNotFoundError(f\"No configuration class found for model {model_name}.\")\n    # Return the found configuration class\n    return config_class\n\n", "CodeBleu_score": 42.713106768287176}]}
{"instance_id": "0ea7862506a7", "generation_result": [{"code": "def load_config(config_path: str) -> Coqpit\n\t\n    config_path = Path(config_path)\n    with fsspec.open(config_path, \"r\") as f:\n        if config_path.suffix in (\".yaml\", \".yml\"):\n            config_dict = yaml.safe_load(f)\n        elif config_path.suffix == \".json\":\n            config_dict = read_json_with_comments(f)\n        else:\n            raise ValueError(\n                f\"Unknown file type {config_path.suffix} for configuration file {config_path}\"\n            )\n\n    model_name = _process_model_name(config_dict)\n    register_config(model_name)\n    config_class = get_config_class(model_name)\n    config = config_class.from_dict(config_dict)\n    return config\n", "CodeBleu_score": 39.0899437250549}, {"code": "def load_config(config_path: str) -> Coqpit\n\t\n    with fsspec.open(config_path) as f:\n        if config_path.endswith(\".yaml\"):\n            config_dict = yaml.safe_load(f)\n        elif config_path.endswith(\".json\"):\n            config_dict = json.load(f)\n        else:\n            raise TypeError(\n                f\"Unknown file type: {config_path}. Only .yaml and .json are supported.\"\n            )\n    model_name = _process_model_name(config_dict)\n    register_config(model_name)\n    config_class = config_dict.pop(\"config_class\")\n    config = config_class.from_dict(config_dict)\n    return config\n\n", "CodeBleu_score": 38.63531471806458}, {"code": "def load_config(config_path: str) -> Coqpit\n\t\n    config_path = Path(config_path)\n    if not config_path.is_file():\n        raise FileNotFoundError(f\"The configuration file {config_path} was not found.\")\n    with fsspec.open(config_path, \"r\") as f:\n        if config_path.suffix == \".json\":\n            config_dict = read_json_with_comments(f)\n        elif config_path.suffix in (\".yml\", \".yaml\"):\n            config_dict = yaml.safe_load(f)\n        else:\n            raise TypeError(\n                f\"The configuration file {config_path} has an unknown type.\"\n            )\n    model_name = _process_model_name(config_dict)\n    register_config(model_name, config_dict)\n    config_class = config_dict.pop(\"config_class\")\n    config = config_class.from_dict(config_dict)\n    return config\n\n", "CodeBleu_score": 43.64735659978706}]}
{"instance_id": "e0db68b63143", "generation_result": [{"code": "def list_files(basePath, validExts=None, contains=None)\n\t\n    # setup the valid extensions\n    if validExts is None:\n        validExts = []\n\n    # loop over the directory structure\n    for (rootDir, dirNames, filenames) in os.walk(basePath):\n        # loop over the filenames in the current directory\n        for filename in filenames:\n            # if the contains string is not none and the filename does not contain\n            # the supplied string, then ignore the file\n            if contains is not None and filename.find(contains) == -1:\n                continue\n\n            # determine the file extension of the current file\n            ext = filename[filename.rfind(\".\"):].lower()\n\n            # check to see if the file is an image and should be processed\n            if ext.endswith(validExts):\n                # construct the path to the image and yield it\n                imagePath = os.path.join(rootDir, filename)\n                yield imagePath\n", "CodeBleu_score": 72.24999650096078}, {"code": "def list_files(basePath, validExts=None, contains=None)\n\t\n    # setup valid extensions\n    if validExts is None:\n        validExts = []\n    # check if contains is a list\n    if contains is None:\n        contains = []\n    elif type(contains) is str:\n        contains = [contains]\n    # walk the directory structure\n    for (rootDir, dirNames, filenames) in os.walk(basePath):\n        # loop filenames\n        for filename in filenames:\n            # check contains\n            skip = False\n            for check in contains:\n                if check not in filename:\n                    skip = True\n            # check extension\n            ext = filename[filename.rfind('.'):]\n            if skip or (len(validExts) > 0 and ext.lower() not in validExts):\n                continue\n            # build the full path\n            filePath = os.path.join(rootDir, filename)\n            # yield path\n            yield filePath\n", "CodeBleu_score": 50.43148605818003}, {"code": "def list_files(basePath, validExts=None, contains=None)\n\t\n    # setup valid extensions\n    if validExts is None:\n        validExts = []\n    # check if contains is a string and convert to a list\n    if isinstance(contains, str):\n        contains = [contains]\n    # begin walk through directory\n    for (rootDir, dirNames, filenames) in os.walk(basePath):\n        # loop filenames in current directory\n        for filename in filenames:\n            # determine if file should be processed\n            skip = False\n            # check contains\n            if contains is not None:\n                skip = True\n                for check in contains:\n                    if check in filename:\n                        skip = False\n            # check file extension\n            ext = os.path.splitext(filename)[1]\n            if ext.lower() not in validExts:\n                skip = True\n            # construct full filepath\n            if not skip:\n                filePath = os.path.join(rootDir, filename)\n                yield filePath\n\n", "CodeBleu_score": 45.5978348356173}]}
{"instance_id": "b836fcaf0a27", "generation_result": [{"code": "def load_model(xtts_checkpoint, xtts_config, xtts_vocab)\n\t\n    clear_gpu_cache()\n    if xtts_checkpoint is None:\n        print(\"Please provide the path to the XTTS model checkpoint.\")\n        return\n    if xtts_config is None:\n        print(\"Please provide the path to the XTTS model config.\")\n        return\n    if xtts_vocab is None:\n        print(\"Please provide the path to the XTTS model vocabulary.\")\n        return\n    config = XttsConfig.load_json(xtts_config)\n    xtts = Xtts.init_from_config(config)\n    xtts.load_checkpoint(checkpoint=xtts_checkpoint,\n                        model_state_dict=True,\n                        optimizer_state_dict=False)\n    xtts.load_vocab(vocab_path=xtts_vocab)\n    if torch.cuda.is_available():\n        xtts.cuda()\n    print(\"The XTTS model has been loaded successfully.\")\n    return\n\n", "CodeBleu_score": 48.29069676620846}, {"code": "def load_model(xtts_checkpoint, xtts_config, xtts_vocab)\n\t\n    clear_gpu_cache()\n    if not (xtts_checkpoint and xtts_config and xtts_vocab):\n        return \"Error: Missing one or more required paths. Please provide a path to the XTTS checkpoint, configuration, and vocabulary.\"\n    config = XttsConfig.load_json(xtts_config)\n    xtts = Xtts.init_from_config(config)\n    xtts.load_checkpoint(checkpoint_path=xtts_checkpoint,\n                        load_optimizer_states=True)\n    xtts.load_vocab(vocab_path=xtts_vocab)\n    if torch.cuda.is_available():\n        xtts.cuda()\n    print(\"XTTS model successfully loaded.\")\n    return xtts\n\n", "CodeBleu_score": 51.560189931935554}, {"code": "def load_model(xtts_checkpoint, xtts_config, xtts_vocab)\n\t\n    clear_gpu_cache()\n    if xtts_checkpoint is None or xtts_config is None or xtts_vocab is None:\n        print(\"Error: Checkpoint, config, and vocab paths must be specified.\")\n        return\n    config = XttsConfig.load_json(xtts_config)\n    xtts = Xtts.init_from_config(config)\n    xtts.load_checkpoint(xtts_checkpoint)\n    xtts.load_vocab(xtts_vocab)\n    if torch.cuda.is_available():\n        xtts.cuda()\n    print(\"Model loaded successfully.\")\n    return\n\n", "CodeBleu_score": 47.0572159140611}]}
{"instance_id": "c5d63aa0df26", "generation_result": [{"code": "def compute_embedding(self, x, num_frames=250, num_eval=10, return_mean=True, l2_norm=True)\n\t\n    if self.use_torch:\n        num_frames = x.shape[1] * x.shape[2] // self.hop_length\n    if num_frames > x.shape[1]:\n        num_frames = x.shape[1]\n        print(\"WARNING: Number of frames exceeds length of input. Using length of input instead.\")\n    if num_eval > num_frames:\n        num_eval = num_frames\n        print(\"WARNING: Number of evaluation frames exceeds number of frames. Using number of frames instead.\")\n    eval_frames = np.linspace(0, num_frames, num_eval).astype(np.int)\n    offset_frames = np.linspace(0, num_frames - num_eval, num_frames - num_eval).astype(np.int)\n    frames_batch = []\n    for frame in offset_frames:\n        frames_batch.append(x[:, frame:frame + num_eval, :])\n    frames_batch = torch.cat(frames_batch, dim=0)\n    with torch.no_grad():\n        if self.use_cuda:\n            frames_batch = frames_batch.cuda()\n        embeddings = self.inference(frames_batch)\n    if return_mean:\n        embeddings = torch.mean(embeddings, dim=0)\n        if l2_norm:\n            embeddings = F.normalize(embeddings, p=2, dim=1)\n        return embeddings.cpu().numpy()\n    else:\n        if l2_norm:\n            embeddings = F.normalize(embeddings, p=2, dim=1)\n        return embeddings.cpu().numpy(), eval_frames\n\n", "CodeBleu_score": 43.01847632786793}, {"code": "def compute_embedding(self, x, num_frames=250, num_eval=10, return_mean=True, l2_norm=True)\n\t\n    frames_batch = []\n    if self.use_torch:\n        num_frames = int(num_frames / self.hop_length)\n        num_frames = min(num_frames, x.shape[1])\n        for i in range(num_eval):\n            start = i * num_frames\n            end = start + num_frames\n            frames_batch.append(x[:, start:end])\n        frames_batch = torch.cat(frames_batch, dim=0)\n    else:\n        num_frames = min(num_frames, x.shape[1])\n        offsets = np.linspace(0, x.shape[1] - num_frames, num_eval)\n        for offset in offsets:\n            start = int(offset)\n            end = start + num_frames\n            frames_batch.append(x[:, start:end])\n    frames_batch = torch.from_numpy(np.array(frames_batch)).to(self.device)\n    with torch.no_grad():\n        embeddings = self.model(frames_batch)\n    if return_mean:\n        embeddings = torch.mean(embeddings, dim=0)\n        if l2_norm:\n            embeddings = F.normalize(embeddings, p=2, dim=1)\n    return embeddings\n", "CodeBleu_score": 43.35245322058395}, {"code": "def compute_embedding(self, x, num_frames=250, num_eval=10, return_mean=True, l2_norm=True)\n\t\n    if isinstance(x, torch.Tensor):\n        x = x.detach().cpu().numpy()\n    if x.ndim == 1:\n        x = np.expand_dims(x, axis=0)\n    if x.ndim > 2:\n        raise ValueError(\"Input can only have 1 or 2 dimensions.\")\n    if x.shape[1] > 1:\n        raise ValueError(\"Input can only have 1 channel.\")\n    if num_frames > x.shape[0]:\n        raise ValueError(\"Number of frames cannot exceed input length.\")\n    hop_length = self.hop_length if self.torch_spec else num_frames\n    num_frames = x.shape[0] if num_frames is None else num_frames\n    num_eval = min(num_eval, num_frames)\n    offset = np.linspace(0, num_frames - num_eval, num_eval, dtype=int)\n    frames_batch = []\n    for i in range(num_frames):\n        frames_batch.append(x[i : i + hop_length])\n    frames_batch = np.stack(frames_batch, axis=0)\n    if self.torch_spec:\n        frames_batch = torch.from_numpy(frames_batch).to(self.device)\n    else:\n        frames_batch = torch.from_numpy(frames_batch).to(self.device)\n    if self.torch_spec:\n        embeddings = self.inference(frames_batch)\n    else:\n        embeddings = self.inference(frames_batch.unsqueeze(0))\n    if self.torch_spec:\n        embeddings = embeddings.detach().cpu().numpy()\n    else:\n        embeddings = embeddings.detach().squeeze(0).cpu().numpy()\n    embeddings = np.concatenate(embeddings, axis=0)\n    embeddings = embeddings[offset]\n    if return_mean:\n        embeddings = embeddings.mean(axis=0)\n        if l2_norm:\n            embeddings = embeddings / np.linalg.norm(embeddings)\n    return embeddings\n", "CodeBleu_score": 36.61426453667646}]}
{"instance_id": "06f5b0335b26", "generation_result": [{"code": "def create_layer(self, block, planes, blocks, stride=1)\n\t\n    downsample = None\n    if stride != 1 or self.inplanes != planes * block.expansion:\n        downsample = nn.Sequential(\n            nn.Conv2d(self.inplanes, planes * block.expansion,\n                      kernel_size=1, stride=stride, bias=False),\n            nn.BatchNorm2d(planes * block.expansion),\n        )\n\n    layers = []\n    layers.append(block(self.inplanes, planes, stride, downsample))\n    self.inplanes = planes * block.expansion\n    for i in range(1, blocks):\n        layers.append(block(self.inplanes, planes))\n\n    return nn.Sequential(*layers)\n", "CodeBleu_score": 85.47050566601277}, {"code": "def create_layer(self, block, planes, blocks, stride=1)\n\t\n    downsample = None\n    if stride != 1 or self.inplanes != planes * block.expansion:\n        downsample = nn.Sequential(\n            nn.Conv2d(self.inplanes, planes * block.expansion,\n                      kernel_size=1, stride=stride, bias=False),\n            nn.BatchNorm2d(planes * block.expansion),\n        )\n\n    layers = []\n    layers.append(block(self.inplanes, planes, stride, downsample))\n    self.inplanes = planes * block.expansion\n    for i in range(1, blocks):\n        layers.append(block(self.inplanes, planes))\n\n    return nn.Sequential(*layers)\n\n", "CodeBleu_score": 85.47050566601277}, {"code": "def create_layer(self, block, planes, blocks, stride=1)\n\t\n    downsample = None\n    if stride != 1 or self.inplanes != planes * block.expansion:\n        downsample = nn.Sequential(\n            nn.Conv2d(self.inplanes, planes * block.expansion, kernel_size=1, stride=stride, bias=False),\n            nn.BatchNorm2d(planes * block.expansion),\n        )\n    layers = []\n    layers.append(block(self.inplanes, planes, stride, downsample))\n    self.inplanes = planes * block.expansion\n    for i in range(1, blocks):\n        layers.append(block(self.inplanes, planes))\n    return nn.Sequential(*layers)\n", "CodeBleu_score": 86.47011482327022}]}
{"instance_id": "9ac60597b633", "generation_result": [{"code": "def plot_embeddings(embeddings, num_classes_in_batch)\n\t\n    # Calculate the number of utterances per class\n    num_utterances_per_class = np.sum(num_classes_in_batch, axis=0)\n    # Limit the number of classes to 10\n    if num_utterances_per_class.shape[0] > 10:\n        num_utterances_per_class = num_utterances_per_class[:10]\n    # Calculate the number of classes\n    num_classes = num_utterances_per_class.shape[0]\n    # Calculate the number of samples\n    num_samples = num_utterances_per_class.sum()\n    # Calculate the number of samples per class\n    num_samples_per_class = num_utterances_per_class / num_samples\n    # Calculate the number of samples per class\n    num_samples_per_class = num_samples_per_class * num_classes_in_batch.shape[0]\n    # Calculate the number of samples per class\n    num_samples_per_class = num_samples_per_class.astype(int)\n    # Calculate the number of samples per class\n    num_samples_per_class = num_samples_per_class.reshape(num_classes, num_classes_in_batch.shape[0])\n    # Calculate the number of samples per class\n    num_samples_per_class = np.repeat(num_samples_per_class, num_samples_per_class.shape[1], axis=1)\n    # Calculate the number of samples per class\n    num_samples_per_class = num_samples_per_class.reshape(num_classes * num_classes_in_batch.shape[0], num_classes_in_batch.shape[0])\n    # Calculate the number of samples per class\n    num_samples_per_class = np.repeat(num_samples_per_class, num_samples_per_class.shape[1], axis=1)\n    # Calculate the number of samples per class\n    num_samples_per_class = num_samples_per_class.reshape(num_classes * num_classes_in_batch.shape[0], num_classes_in_batch.shape[0])\n    # Calculate the number of samples per class\n    num_samples_per_class = np.repeat(num_samples_per_class", "CodeBleu_score": 23.21207960853926}, {"code": "def plot_embeddings(embeddings, num_classes_in_batch)\n\t\n    if num_classes_in_batch > 10:\n        num_classes_in_batch = 10\n    fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n    ax.set_title(\"UMAP projection\")\n    ax.set_aspect(\"equal\")\n    for i in range(num_classes_in_batch):\n        plt.scatter(embeddings[i, :, 0], embeddings[i, :, 1], label=i)\n    plt.legend(loc=\"upper left\", bbox_to_anchor=(1.05, 1), ncol=1, fontsize=12)\n    plt.tight_layout()\n    plt.savefig(\"umap\")\n    return fig\n\n", "CodeBleu_score": 32.8670671241741}, {"code": "def plot_embeddings(embeddings, num_classes_in_batch)\n\t\n    if num_classes_in_batch > 10:\n        num_classes_in_batch = 10\n    num_utterances_per_class = np.repeat(np.arange(num_classes_in_batch), 10)\n    reducer = umap.UMAP()\n    embeddings_2d = reducer.fit_transform(embeddings)\n    fig, ax = plt.subplots(figsize=(10, 10))\n    ax.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], c=num_utterances_per_class, s=5)\n    ax.set_aspect('equal', 'datalim')\n    plt.title('UMAP projection')\n    plt.tight_layout()\n    plt.savefig('umap')\n    return fig\n", "CodeBleu_score": 44.03907285444992}]}
{"instance_id": "19c2b4af256d", "generation_result": [{"code": "def calc_cosine_sim(self, dvecs, centroids)\n\t\n    new_centroids = []\n    cs_row = []\n    for i, dvec in enumerate(dvecs):\n        centroid_i = centroids[i]\n        for j, dvec_j in enumerate(dvecs):\n            if i == j:\n                continue\n            centroid_j = centroids[j]\n            new_centroid = self.calc_new_centroids(centroid_i, centroid_j, dvec, dvec_j)\n            new_centroids.append(new_centroid)\n            cs_row.append(self.cos_sim(dvec, new_centroid))\n        cs_row.append(self.cos_sim(dvec, centroid_i))\n    cs_row.append(self.cos_sim(dvec, centroid_i))\n    cos_sim_matrix = torch.stack(cs_row)\n    cos_sim_matrix = torch.clamp(cos_sim_matrix, min=0.00001)\n    cos_sim_matrix = torch.cat((cos_sim_matrix, torch.zeros(1, cos_sim_matrix.shape[1])), dim=0)\n    cos_sim_matrix = torch.cat((cos_sim_matrix, torch.ones(1, cos_sim_matrix.shape[1])), dim=0)\n    return cos_sim_matrix, new_centroids\n", "CodeBleu_score": 40.056726883472734}, {"code": "def calc_cosine_sim(self, dvecs, centroids)\n\t\n    cos_sim_matrix = []\n    for i, speaker in enumerate(self.speakers):\n        cs_row = []\n        for j, utterance in enumerate(self.speaker_utterances[speaker]):\n            new_centroids = self.calc_new_centroids(dvecs, centroids, i, j)\n            cs_row.append(torch.clamp(torch.mm(dvecs[utterance].unsqueeze(0), new_centroids.transpose(1,0)), min=1e-7))\n        cos_sim_matrix.append(torch.cat(cs_row))\n    return torch.stack(cos_sim_matrix)\n", "CodeBleu_score": 41.57024888179512}, {"code": "def calc_cosine_sim(self, dvecs, centroids)\n\t\n    cos_sim_matrix = []\n    for i, cs_row in enumerate(centroids):\n        new_centroids = []\n        for j, cs_col in enumerate(centroids):\n            if i == j:\n                new_centroids.append(cs_row)\n            else:\n                new_centroids.append(self.calc_new_centroids(dvecs, cs_row, cs_col))\n        new_centroids = torch.stack(new_centroids)\n        new_centroids = torch.clamp(new_centroids, min=-1, max=1)\n        cos_sim_matrix.append(new_centroids)\n    cos_sim_matrix = torch.cat(cos_sim_matrix)\n    cos_sim_matrix = torch.stack(cos_sim_matrix)\n    return cos_sim_matrix\n", "CodeBleu_score": 38.54244725606727}]}
{"instance_id": "c525b411a1e7", "generation_result": [{"code": " def embed_loss_softmax(self, dvecs, cos_sim_matrix)\n\t\n    loss_L = []\n    for i in range(len(dvecs)):\n        loss_L.append(F.log_softmax(cos_sim_matrix[i], dim=0))\n    loss_L = torch.stack(loss_L)\n    return loss_L\n", "CodeBleu_score": 27.983536569797536}, {"code": " def embed_loss_softmax(self, dvecs, cos_sim_matrix)\n\t\n    #print(\"embedding loss softmax\")\n    #print(dvecs.shape)\n    #print(cos_sim_matrix.shape)\n    #print(cos_sim_matrix)\n    #print(cos_sim_matrix.shape)\n    #print(cos_sim_matrix.unsqueeze(2).shape)\n    #print(cos_sim_matrix.unsqueeze(2))\n    #print(cos_sim_matrix.unsqueeze(2).shape)\n    #print(cos_sim_matrix.unsqueeze(2).shape)\n    #print(cos_sim_matrix.unsqueeze(2).shape)\n    #print(cos_sim_matrix.unsqueeze(2).shape)\n    #print(cos_sim_matrix.unsqueeze(2).shape)\n    #print(cos_sim_matrix.unsqueeze(2).shape)\n    #print(cos_sim_matrix.unsqueeze(2).shape)\n    #print(cos_sim_matrix.unsqueeze(2).shape)\n    #print(cos_sim_matrix.unsqueeze(2).shape)\n    #print(cos_sim_matrix.unsqueeze(2).shape)\n    #print(cos_sim_matrix.unsqueeze(2).shape)\n    #print(cos_sim_matrix.unsqueeze(2).shape)\n    #print(cos_sim_matrix.unsqueeze(2).shape)\n    #print(cos_sim_matrix.unsqueeze(2).shape)\n    #print(cos_sim_matrix.unsqueeze(2).shape)\n    #print(cos_sim_matrix.unsqueeze(2).shape)\n    #print(cos_sim_matrix.unsqueeze(2).shape)\n    #print(cos_sim_matrix.unsqueeze(2).shape)\n    #print(cos_sim_matrix.unsqueeze(2).shape)\n    #print(cos_sim_matrix.unsqueeze(2).shape)\n    #print(cos_sim_matrix.unsqueeze(2).shape)\n    #print(cos_sim_matrix.unsqueeze(2).shape)\n    #print(cos_sim_matrix.unsqueeze(2).shape)\n    #print(cos_sim_matrix.unsqueeze(2).shape)\n    #print(cos_sim_matrix.unsqueeze(2).shape)\n    #print(cos_sim_matrix.unsqueeze(2).shape)\n    #print(cos_sim_matrix.unsqueeze(2).", "CodeBleu_score": 2.26010101010101}, {"code": " def embed_loss_softmax(self, dvecs, cos_sim_matrix)\n\t\n    losses = []\n    for i in range(len(dvecs)):\n        losses.append(F.log_softmax(cos_sim_matrix[i], dim=1))\n    return torch.stack(losses)\n", "CodeBleu_score": 25.21056482627933}]}
{"instance_id": "25ac3945da2e", "generation_result": [{"code": "def embed_loss_contrast(self, dvecs, cos_sim_matrix)\n\t\n    # Initialize the loss row\n    loss_row = []\n    # Iterate over the embeddings\n    for i in range(len(dvecs)):\n        # Get the current centroid\n        centroid = self.centroids[i]\n        # Get the cosine similarity matrix for the current embedding\n        cos_sim_row = cos_sim_matrix[i]\n        # Get the sigmoid of the cosine similarity matrix\n        sigmoid_cos_sim_row = torch.sigmoid(cos_sim_row)\n        # Exclude the current centroid from the cosine similarity matrix\n        cos_sim_row_no_centroid = cos_sim_row.clone()\n        cos_sim_row_no_centroid[i] = 0\n        # Compute the contrast loss\n        loss_row.append(\n            torch.max(\n                torch.tensor(0.0),\n                torch.sum(sigmoid_cos_sim_row * cos_sim_row_no_centroid)\n                - self.margin,\n            )\n        )\n    # Stack the loss row into a tensor\n    loss_row = torch.stack(loss_row)\n    # Return the final stacked tensor of losses\n    return loss_row\n\n", "CodeBleu_score": 31.793426093019296}, {"code": "def embed_loss_contrast(self, dvecs, cos_sim_matrix)\n\t\n    loss_row = []\n    for i in range(len(dvecs)):\n        temp_cos_sim_matrix = cos_sim_matrix[i].clone()\n        temp_cos_sim_matrix[i] = -1000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000", "CodeBleu_score": 11.355591401191871}, {"code": "def embed_loss_contrast(self, dvecs, cos_sim_matrix)\n\t\n    loss_row = []\n    for i in range(self.n_clusters):\n        centroid = self.centroids[i]\n        cos_sim_matrix_row = cos_sim_matrix[i]\n        cos_sim_matrix_row = torch.cat((cos_sim_matrix_row[:i], cos_sim_matrix_row[i+1:]))\n        cos_sim_matrix_row = torch.sigmoid(cos_sim_matrix_row)\n        loss_row.append(torch.max(cos_sim_matrix_row))\n    loss_row = torch.stack(loss_row)\n    return loss_row\n\n", "CodeBleu_score": 35.051971453734446}]}
{"instance_id": "878d14ffabd2", "generation_result": [{"code": "def forward(self, x, _label=None)\n\t\n    # Check if the input is valid\n    assert x.size(0) >= 2, \"The batch size must be at least 2.\"\n    # Calculate the number of speakers and utterances per speaker\n    num_speakers = x.size(0)\n    num_utts_per_speaker = x.size(1)\n    # Calculate the centroids by averaging the embeddings\n    centroids = x.mean(dim=1)\n    # Calculate the cosine similarity matrix\n    cos_sim_matrix = self.calc_cosine_sim(x, centroids)\n    # Scale and shift the cosine similarity matrix\n    scaled_cos_sim_matrix = self.scale_and_shift_matrix(cos_sim_matrix, num_utts_per_speaker)\n    # Calculate the embedding loss\n    embed_loss = self.embed_loss(scaled_cos_sim_matrix)\n    # Average the embedding loss over speakers\n    embed_loss = embed_loss.mean()\n    # Return the mean embedding loss\n    return embed_loss\n\n", "CodeBleu_score": 43.7551452085334}, {"code": "def forward(self, x, _label=None)\n\t\n    # ensure at least two utterances per speaker\n    assert x.size(1) >= 2, 'at least two utterances per speaker'\n    # calculate the centroids by averaging the embeddings\n    centroids = torch.mean(x, dim=1)\n    # calculate the cosine similarity matrix\n    sim_matrix = self.calc_cosine_sim(x, centroids)\n    # scale and shift the similarity matrix\n    scaled_sim_matrix = self.scale * (sim_matrix - self.shift)\n    # get the labels\n    if _label is None:\n        label = torch.arange(x.size(0))\n    else:\n        label = _label\n    # get the loss\n    loss = self.embed_loss(scaled_sim_matrix, label)\n    # return the mean loss\n    return loss.mean()\n", "CodeBleu_score": 44.39969869705591}, {"code": "def forward(self, x, _label=None)\n\t\n    # Check the input tensor dimensions\n    if x.size()[0] < 2:\n        raise ValueError(\"The input tensor must have at least two speakers\")\n    if x.size()[1] < 2:\n        raise ValueError(\"The input tensor must have at least two utterances per speaker\")\n    if x.size()[2] != self.embed_size:\n        raise ValueError(\"The input tensor must have the same embedding size as the model\")\n\n    # Calculate the mean of the input tensor\n    x_mean = torch.mean(x, dim=1)\n\n    # Calculate the squared L2 distance between the input and mean tensors\n    x_sub = x.sub(x_mean.expand_as(x))\n    x_dist_sq = torch.sum(x_sub**2, dim=2)\n\n    # Calculate the squared L2 distance between the mean and mean tensors\n    x_mean_sub = x_mean.expand_as(x) - x_mean.expand_as(x).t()\n    x_mean_dist_sq = torch.sum(x_mean_sub**2, dim=2)\n\n    # Ensure the distance between the mean and mean is zero for numerical stability\n    x_mean_dist_sq = torch.clamp(x_mean_dist_sq, min=1e-12)\n\n    # Calculate the cosine similarity between the input and mean tensors\n    x_cos_sim = self.calc_cosine_sim(x, x_mean)\n\n    # Calculate the masked mean of the cosine similarity matrix\n    x_cos_sim_mean = self.mask_mean(x_cos_sim, dim=1)\n\n    # Calculate the masked mean of the squared L2 distance matrix\n    x_dist_sq_mean = self.mask_mean(x_dist_sq, dim=1)\n\n    # Calculate the masked mean of the squared L2 distance between the mean and mean tensors\n    x_mean_dist_sq_mean = self.mask_mean(x_mean_dist_sq, dim=1)\n\n    # Calculate the embedding loss\n    embed_loss = torch.log(1 + torch.exp(self.embed_loss_factor * (x_cos_sim_mean - self.embed_margin))) + \\\n        torch.log(1 + torch.exp(self.embed_loss_factor * (self.embed", "CodeBleu_score": 37.264542664307584}]}
{"instance_id": "86a364c72caa", "generation_result": [{"code": "def get_voices(extra_voice_dirs: List[str] = [])\n\t\n    voice_dirs = []\n    voice_dirs.extend(extra_voice_dirs)\n    voice_dirs.extend(\n        [\n            os.path.join(os.path.dirname(__file__), \"voices\"),\n            os.path.join(os.path.dirname(__file__), \"voices\", \"en\"),\n        ]\n    )\n    voices = {}\n    for voice_dir in voice_dirs:\n        if not os.path.isdir(voice_dir):\n            continue\n        voice_files = []\n        for voice_file in glob.glob(os.path.join(voice_dir, \"*.npz\")):\n            voice_files.append(voice_file)\n        if not voice_files:\n            for voice_file in glob.glob(os.path.join(voice_dir, \"*.wav\")):\n                voice_files.append(voice_file)\n            for voice_file in glob.glob(os.path.join(voice_dir, \"*.mp3\")):\n                voice_files.append(voice_file)\n        if voice_files:\n            voices[os.path.basename(voice_dir)] = voice_files\n    return voices\n\n", "CodeBleu_score": 25.031259347110584}, {"code": "def get_voices(extra_voice_dirs: List[str] = [])\n\t\n    voice_files = {}\n    for voice_dir in extra_voice_dirs:\n        voice_files[voice_dir] = []\n        for root, dirs, files in os.walk(voice_dir):\n            for file in files:\n                if file.endswith(\".npz\"):\n                    voice_files[voice_dir].append(os.path.join(root, file))\n                elif file.endswith(\".wav\") or file.endswith(\".mp3\"):\n                    voice_files[voice_dir].append(os.path.join(root, file))\n    return voice_files\n\n", "CodeBleu_score": 28.310348693154115}, {"code": "def get_voices(extra_voice_dirs: List[str] = [])\n\t\n    # get the list of directories\n    voice_dirs = list(\n        filter(\n            lambda x: os.path.isdir(x),\n            [\n                os.path.join(os.path.dirname(os.path.abspath(__file__)), \"voices\")\n            ]\n            + extra_voice_dirs,\n        )\n    )\n    # get the list of voice files\n    voice_files = {}\n    for voice_dir in voice_dirs:\n        # get the list of .npz files\n        npz_files = list(\n            filter(\n                lambda x: x.endswith(\".npz\"),\n                glob(os.path.join(voice_dir, \"**/*.npz\"), recursive=True),\n            )\n        )\n        if len(npz_files) > 0:\n            voice_files[voice_dir] = npz_files\n        else:\n            # get the list of .wav files\n            wav_files = list(\n                filter(\n                    lambda x: x.endswith(\".wav\"),\n                    glob(os.path.join(voice_dir, \"**/*.wav\"), recursive=True),\n                )\n            )\n            # get the list of .mp3 files\n            mp3_files = list(\n                filter(\n                    lambda x: x.endswith(\".mp3\"),\n                    glob(os.path.join(voice_dir, \"**/*.mp3\"), recursive=True),\n                )\n            )\n            # combine the .wav and .mp3 files\n            voice_files[voice_dir] = wav_files + mp3_files\n    return voice_files\n\n", "CodeBleu_score": 16.98768655200549}]}
{"instance_id": "02f20c8917e6", "generation_result": [{"code": "def load_voice(model, voice: str, extra_voice_dirs: List[str] = [])\n\t\n    # Get the voice paths\n    voice_paths = get_voices(model, voice, extra_voice_dirs)\n    # Check if there are multiple paths\n    if len(voice_paths) > 1:\n        raise ValueError(\"Multiple voice paths found for {}: {}\".format(voice, voice_paths))\n    # Get the voice path\n    voice_path = voice_paths[0]\n    # Check if the voice path is a .npz file\n    if voice_path.endswith(\".npz\"):\n        return load_npz(voice_path)\n    # Check if the voice path exists\n    if not os.path.exists(voice_path):\n        raise FileNotFoundError(\"Voice file not found: {}\".format(voice_path))\n    # Check if the voice path is a directory\n    if os.path.isdir(voice_path):\n        # Generate the voice\n        generate_voice(model, voice_path)\n        # Recursively load the voice\n        return load_voice(model, voice, extra_voice_dirs)\n    # Load the voice\n    return np.load(voice_path)\n\n", "CodeBleu_score": 34.60527131208341}, {"code": "def load_voice(model, voice: str, extra_voice_dirs: List[str] = [])\n\t\n    voice_paths = get_voices(model, voice, extra_voice_dirs)\n    if len(voice_paths) == 1:\n        voice_path = voice_paths[0]\n        if voice_path.endswith(\".npz\"):\n            return load_npz(voice_path)\n        else:\n            return generate_voice(voice_path)\n    else:\n        raise ValueError(\n            f\"Multiple voice paths for {voice} found in {voice_paths}\")\n\n", "CodeBleu_score": 26.210393250464108}, {"code": "def load_voice(model, voice: str, extra_voice_dirs: List[str] = [])\n\t\n    voice = voice.lower()\n    if voice == \"random\":\n        return None\n    paths = get_voices(model, voice, extra_voice_dirs)\n    if len(paths) > 1:\n        raise ValueError(f\"Multiple voices found for {model} {voice}\")\n    path = paths[0]\n    if path.endswith(\".npz\"):\n        return load_npz(path)\n    if not path.endswith(\".wav\"):\n        raise ValueError(f\"Unsupported file extension for {model} {voice}\")\n    if not os.path.isfile(path):\n        raise ValueError(f\"Voice {model} {voice} not found\")\n    return generate_voice(model, voice, extra_voice_dirs)\n\n", "CodeBleu_score": 42.6431140686892}]}
{"instance_id": "9458e981389f", "generation_result": [{"code": "def _relative_shift(self, pos_score: torch.Tensor) -> torch.Tensor\n\t\n    batch_size, seq_len, _ = pos_score.size()\n    zero_pad = torch.zeros((batch_size, seq_len, 1), device=pos_score.device, dtype=pos_score.dtype)\n    padded_pos_score = torch.cat([zero_pad, pos_score], dim=2)\n    padded_pos_score = padded_pos_score.view(batch_size, seq_len + 1, 2 * seq_len - 1)\n    pos_score = padded_pos_score[:, :seq_len, seq_len - 1:]\n    return pos_score\n\n", "CodeBleu_score": 33.626037046243496}, {"code": "def _relative_shift(self, pos_score: torch.Tensor) -> torch.Tensor\n\t\n    batch_size, seq_len, _ = pos_score.size()\n    zero_pad = torch.zeros((batch_size, seq_len, 1), device=pos_score.device, dtype=pos_score.dtype)\n    padded_pos_score = torch.cat([zero_pad, pos_score], dim=-1)\n    padded_pos_score = padded_pos_score.view(batch_size, seq_len + 1, 2 * seq_len + 1)\n    pos_score = padded_pos_score[:, 1:].view_as(pos_score)\n    return pos_score\n", "CodeBleu_score": 40.084204889679256}, {"code": "def _relative_shift(self, pos_score: torch.Tensor) -> torch.Tensor\n\t\n    batch_size, seq_len, _ = pos_score.size()\n    zeros = torch.zeros((batch_size, seq_len, 1), dtype=pos_score.dtype, device=pos_score.device)\n    padded_pos_score = torch.cat([zeros, pos_score], dim=-1)\n    view = padded_pos_score.view(batch_size, seq_len + 1, 2 * seq_len + 1)\n    shifted_pos_score = view[:, 1:, seq_len:]\n    return shifted_pos_score\n\n", "CodeBleu_score": 39.058434918881936}]}
{"instance_id": "c046d4f00e80", "generation_result": [{"code": "def run_padded_sequence(self, context, lens)\n\t\n    context_embedded = [self.embedding(c) for c in context]\n    context_embedded = torch.stack(context_embedded)\n    context_embedded = torch.transpose(context_embedded, 0, 1)\n    context_embedded = nn.utils.rnn.pad_sequence(context_embedded, batch_first=True)\n    context_embedded = self.dropout(context_embedded)\n    for conv in self.convs:\n        context_embedded = conv(context_embedded)\n    context_embedded = torch.transpose(context_embedded, 0, 1)\n    context_embedded = nn.utils.rnn.pad_sequence(context_embedded, batch_first=True)\n    return context_embedded\n\n", "CodeBleu_score": 32.44303936342824}, {"code": "def run_padded_sequence(self, context, lens)\n\t\n    context_embedded = [self.embedding(context[:, i]) for i in range(context.size(1))]\n    context_embedded = torch.stack(context_embedded, dim=1)\n    context_embedded = self.dropout(context_embedded)\n    context_embedded = context_embedded.transpose(0, 1)\n    context_embedded = pad_sequence(context_embedded, batch_first=True, padding_value=self.pad_index)\n    context_embedded = context_embedded.transpose(0, 1)\n    context_embedded = context_embedded.transpose(1, 2)\n    context_embedded = [F.relu(conv(context_embedded)) for conv in self.convs]\n    context_embedded = [F.max_pool1d(context_embedded[i], context_embedded[i].size(2)).squeeze(2) for i in range(len(context_embedded))]\n    context_embedded = torch.cat(context_embedded, dim=1)\n    context_embedded = context_embedded.transpose(0, 1)\n    context_embedded = context_embedded.transpose(1, 2)\n    context_embedded = [F.relu(conv(context_embedded)) for conv in self.convs]\n    context_embedded = [F.max_pool1d(context_embedded[i], context_embedded[i].size(2)).squeeze(2) for i in range(len(context_embedded))]\n    context_embedded = torch.cat(context_embedded, dim=1)\n    context_embedded = context_embedded.transpose(0, 1)\n    context_embedded = context_embedded.transpose(1, 2)\n    context_embedded = context_embedded.transpose(0, 1)\n    context_embedded = pad_sequence(context_embedded, batch_first=True, padding_value=self.pad_index)\n    context_embedded = context_embedded.transpose(0, 1)\n    context_embedded = context_embedded.transpose(1, 2)\n    return context_embedded\n", "CodeBleu_score": 21.642425638172586}, {"code": "def run_padded_sequence(self, context, lens)\n\t\n    context_embedded = []\n    for i in range(context.size(0)):\n        context_embedded.append(self.embedding(context[i, :lens[i]]))\n    context_embedded = torch.stack(context_embedded)\n    context_embedded = context_embedded.transpose(0, 1)\n    context_embedded = nn.utils.rnn.pad_sequence(context_embedded, batch_first=True)\n    context_embedded = self.dropout(context_embedded)\n    for conv in self.convs:\n        context_embedded = conv(context_embedded)\n        context_embedded = self.relu(context_embedded)\n    return context_embedded\n", "CodeBleu_score": 39.27640036316472}]}
{"instance_id": "db2644f3c681", "generation_result": [{"code": "def run_unsorted_inputs(self, fn, context, lens)\n\t\n    lens_sorted, indices_sorted = torch.sort(lens, descending=True)\n    context_sorted = context.index_select(0, indices_sorted)\n    context_packed = pack_padded_sequence(context_sorted, lens_sorted.cpu(), enforce_sorted=False)\n    context_unpacked, _ = pad_packed_sequence(fn(context_packed), batch_first=True)\n    context_unpacked = context_unpacked.to(context.device)\n    return context_unpacked.index_select(0, indices_sorted)\n", "CodeBleu_score": 16.15151692186715}, {"code": "def run_unsorted_inputs(self, fn, context, lens)\n\t\n    lens_sorted, indices = torch.sort(lens, descending=True)\n    context_sorted = context[indices]\n    context_packed = pack_padded_sequence(context_sorted, lens_sorted, batch_first=True)\n    context_unpacked, lens_unpacked = pad_packed_sequence(fn(context_packed), batch_first=True)\n    context_unpacked = context_unpacked.data\n    context_unsorted = context_unpacked[indices]\n    return context_unsorted\n\n", "CodeBleu_score": 16.06023212119872}, {"code": "def run_unsorted_inputs(self, fn, context, lens)\n\t\n    if lens is None:\n        return fn(context)\n    lens_sorted, indices = torch.sort(lens, descending=True)\n    context_sorted = context.index_select(0, indices)\n    context_packed = pack_padded_sequence(context_sorted, lens_sorted.cpu(), batch_first=True)\n    context_unpacked, _ = pad_packed_sequence(context_packed, batch_first=True)\n    context_unpacked = context_unpacked.to(context.device)\n    results = fn(context_unpacked)\n    if isinstance(results, tuple):\n        results = tuple(result.index_select(0, indices) for result in results)\n    else:\n        results = results.index_select(0, indices)\n    return results\n", "CodeBleu_score": 34.25602118958254}]}
{"instance_id": "ced082bffc63", "generation_result": [{"code": "def remove_weight_norm(self)\n\t\n    if self.kernel_predictor is not None:\n        self.kernel_predictor.remove_weight_norm()\n    if self.convt_pre is not None:\n        self.convt_pre.remove_weight_norm()\n    for block in self.conv_blocks:\n        block.remove_weight_norm()\n\n", "CodeBleu_score": 44.88325198810765}, {"code": "def remove_weight_norm(self)\n\t\n    self.kernel_predictor.remove_weight_norm()\n    self.convt_pre.remove_weight_norm()\n    for block in self.conv_blocks:\n        block.remove_weight_norm()\n\n", "CodeBleu_score": 33.51046275391997}, {"code": "def remove_weight_norm(self)\n\t\n    self.kernel_predictor.remove_weight_norm()\n    self.convt_pre.remove_weight_norm()\n    for block in self.conv_blocks:\n        block.remove_weight_norm()\n    return self\n\n", "CodeBleu_score": 34.175908271870114}]}
{"instance_id": "b5a39fd97061", "generation_result": [{"code": "def map_mx_to_torch_model(mx_name)\n\t\n    mx_name = mx_name.lower()\n    if mx_name.startswith('se_'):\n        mx_name = 'senet_' + mx_name[3:]\n    elif mx_name.startswith('inceptionv3'):\n        mx_name = 'inception_v3'\n    elif mx_name.startswith('inception_v3'):\n        mx_name = 'inception_v3'\n    return 'gluon_' + mx_name\n\n", "CodeBleu_score": 31.660839740230955}, {"code": "def map_mx_to_torch_model(mx_name)\n\t\n    if mx_name.startswith('se_'):\n        return 'gluon_' + mx_name[3:]\n    elif mx_name.startswith('senet_'):\n        return 'gluon_' + mx_name[6:]\n    elif mx_name.startswith('inceptionv3'):\n        return 'gluon_' + mx_name\n    else:\n        return 'gluon_' + mx_name.lower()\n\n", "CodeBleu_score": 26.515252290106282}, {"code": "def map_mx_to_torch_model(mx_name)\n\t\n    if mx_name.startswith('se_'):\n        torch_name = mx_name.replace('se_', 'senet_')\n    elif mx_name.startswith('senet_'):\n        torch_name = mx_name.replace('senet_', 'senet154_')\n    elif mx_name.startswith('inceptionv3'):\n        torch_name = mx_name.replace('inceptionv3', 'inception_v3')\n    else:\n        torch_name = mx_name\n    return 'gluon_' + torch_name.lower()\n\n", "CodeBleu_score": 50.377068755670585}]}
{"instance_id": "7eee5efdbf67", "generation_result": [{"code": "def _num_samples_per_worker(self)\n\t\n    num_samples = self.num_samples\n    if self.num_replicas > 1:\n        num_samples = math.ceil(num_samples / self.num_replicas)\n    if self.batch_size is not None:\n        num_samples = math.ceil(num_samples / self.batch_size)\n    return num_samples\n\n", "CodeBleu_score": 30.833456305058526}, {"code": "def _num_samples_per_worker(self)\n\t\n    num_samples = self.num_samples\n    num_replicas = self.num_replicas\n    num_workers = self.num_workers\n    batch_size = self.batch_size\n    if num_workers is None:\n        num_workers = 1\n    if batch_size is None:\n        batch_size = 1\n    if num_workers > 1:\n        if num_samples % num_workers != 0:\n            num_samples += num_workers - num_samples % num_workers\n    if num_replicas > 1:\n        if num_samples % num_replicas != 0:\n            num_samples += num_replicas - num_samples % num_replicas\n    if batch_size > 1:\n        if num_samples % batch_size != 0:\n            num_samples += batch_size - num_samples % batch_size\n    return num_samples\n\n", "CodeBleu_score": 17.55616437363478}, {"code": "def _num_samples_per_worker(self)\n\t\n    if self.num_replicas > 1:\n        if self.training:\n            # If training, we want to make sure that all replicas process the same number of batches\n            # To do so, we round up the number of samples per worker to the nearest multiple of total_num_samples / num_replicas\n            samples_per_worker = math.ceil(self.total_num_samples / self.num_replicas)\n            # If a batch size is specified, we want to make sure that all workers process the same number of batches\n            # To do so, we adjust the number of samples per worker to be a multiple of the batch size\n            if self.batch_size is not None:\n                samples_per_worker = math.ceil(samples_per_worker / self.batch_size) * self.batch_size\n        else:\n            # If not training, we want to make sure that each worker processes the same number of batches\n            # To do so, we round down the number of samples per worker to the nearest multiple of total_num_samples / num_replicas\n            samples_per_worker = math.floor(self.total_num_samples / self.num_replicas)\n            # If a batch size is specified, we want to make sure that all workers process the same number of batches\n            # To do so, we adjust the number of samples per worker to be a multiple of the batch size\n            if self.batch_size is not None:\n                samples_per_worker = math.floor(samples_per_worker / self.batch_size) * self.batch_size\n    else:\n        samples_per_worker = self.total_num_samples\n    return samples_per_worker\n\n", "CodeBleu_score": 39.63326255248757}]}
{"instance_id": "a698d65ae190", "generation_result": [{"code": "def find_images_and_targets(\n        folder: str,\n        types: Optional[Union[List, Tuple, Set]] = None,\n        class_to_idx: Optional[Dict] = None,\n        leaf_name_only: bool = True,\n        sort: bool = True\n)\n\t\n    if types is None:\n        types = get_img_extensions()\n    if class_to_idx is None:\n        class_to_idx = {}\n    labels = []\n    filenames = []\n    for root, _, fnames in sorted(os.walk(folder)):\n        for fname in sorted(fnames):\n            if fname.lower().endswith(types):\n                path = os.path.join(root, fname)\n                class_name = os.path.relpath(root, folder) if leaf_name_only else root\n                if class_name not in class_to_idx:\n                    class_to_idx[class_name] = len(class_to_idx)\n                labels.append(class_to_idx[class_name])\n                filenames.append(path)\n    if sort:\n        filenames, labels = zip(*sorted(zip(filenames, labels)))\n    return filenames, labels, class_to_idx\n", "CodeBleu_score": 36.27772441608387}, {"code": "def find_images_and_targets(\n        folder: str,\n        types: Optional[Union[List, Tuple, Set]] = None,\n        class_to_idx: Optional[Dict] = None,\n        leaf_name_only: bool = True,\n        sort: bool = True\n)\n\t\n    if types is None:\n        types = get_img_extensions()\n    if class_to_idx is None:\n        class_to_idx = {}\n    filenames = []\n    labels = []\n    for root, subdirs, files in os.walk(folder):\n        for filename in files:\n            if filename.endswith(types):\n                rel_path = os.path.relpath(root, folder)\n                if leaf_name_only:\n                    rel_path = os.path.basename(rel_path)\n                filenames.append(os.path.join(rel_path, filename))\n                label = os.path.basename(rel_path)\n                if label not in class_to_idx:\n                    class_to_idx[label] = len(class_to_idx)\n                labels.append(class_to_idx[label])\n    if sort:\n        filenames, labels = zip(*sorted(zip(filenames, labels)))\n    return filenames, labels, class_to_idx\n\n", "CodeBleu_score": 39.06806151163543}, {"code": "def find_images_and_targets(\n        folder: str,\n        types: Optional[Union[List, Tuple, Set]] = None,\n        class_to_idx: Optional[Dict] = None,\n        leaf_name_only: bool = True,\n        sort: bool = True\n)\n\t\n    if types is None:\n        types = get_img_extensions()\n    else:\n        types = set(types)\n    files = []\n    labels = []\n    for root, _, filenames in os.walk(folder):\n        for filename in filenames:\n            if splitext(filename)[1].lower() in types:\n                rel_path = os.path.relpath(root, folder)\n                if leaf_name_only:\n                    rel_path = os.path.basename(rel_path)\n                files.append(os.path.join(root, filename))\n                labels.append(rel_path)\n    if sort:\n        files, labels = zip(*sorted(zip(files, labels), key=lambda x: natural_key(x[0])))\n    if class_to_idx is None:\n        class_to_idx = {label: idx for idx, label in enumerate(set(labels))}\n    else:\n        class_to_idx = dict(class_to_idx)\n    labels = [class_to_idx[label] for label in labels]\n    return files, labels, class_to_idx\n\n", "CodeBleu_score": 43.77882247289131}]}
{"instance_id": "e7fe43bdecdc", "generation_result": [{"code": "def pytorch_worker_seed()\n\t\n    worker_info = get_worker_info()\n    if worker_info is None:\n        seed = 0\n    else:\n        seed = worker_info.seed\n    return seed\n\n", "CodeBleu_score": 43.11773678451842}, {"code": "def pytorch_worker_seed()\n\t\n    if get_worker_info() is not None:\n        worker_info = get_worker_info()\n        return worker_info.seed % 2**32\n    else:\n        return wds_rank() % 2**32\n\n", "CodeBleu_score": 52.945745014243364}, {"code": "def pytorch_worker_seed()\n\t\n    worker_info = get_worker_info()\n    if worker_info is None:\n        # If the worker_info is not available, use the seed based on the wds rank.\n        seed = int(torch.initial_seed() % (2 ** 32 - 1))\n    else:\n        # If the worker_info is available, use the seed based on the worker id.\n        seed = int(worker_info.id)\n    return seed\n", "CodeBleu_score": 47.05156768692377}]}
{"instance_id": "0ffc7e7ded25", "generation_result": [{"code": "def solarize_add(img, add, thresh=128, **__)\n\t\n    if img.mode == 'L' or img.mode == 'RGB':\n        lut = []\n        for i in range(256):\n            if i < thresh:\n                lut.append(min(255, add + i))\n            else:\n                lut.append(i)\n        if img.mode == 'L':\n            img = img.point(lut)\n        else:\n            img = img.point(lut, 'RGB')\n        return img\n    else:\n        return img\n\n", "CodeBleu_score": 58.19210832769426}, {"code": "def solarize_add(img, add, thresh=128, **__)\n\t\n    return img.point(lambda i: i + add if i < thresh else i)\n\n", "CodeBleu_score": 9.02861057025556}, {"code": "def solarize_add(img, add, thresh=128, **__)\n\t\n    return cv2.add(img, add, dtype=cv2.CV_8U)\n\n", "CodeBleu_score": 7.555299870178941}]}
{"instance_id": "5a0d29577a21", "generation_result": [{"code": "def _get_weighted_transforms(transforms: Dict)\n\t\n    transform_names = list(transforms.keys())\n    transform_probs = [transforms[transform] for transform in transform_names]\n    transform_probs = np.array(transform_probs)\n    transform_probs = transform_probs / np.sum(transform_probs)\n    return transform_names, transform_probs\n\n", "CodeBleu_score": 18.994433488532042}, {"code": "def _get_weighted_transforms(transforms: Dict)\n\t\n    transform_names = []\n    transform_probs = []\n    for key, value in transforms.items():\n        transform_names.append(key)\n        transform_probs.append(value)\n    transform_probs = np.array(transform_probs)\n    transform_probs = transform_probs / np.sum(transform_probs)\n    return transform_names, transform_probs\n\n", "CodeBleu_score": 37.34450475888201}, {"code": "def _get_weighted_transforms(transforms: Dict)\n\t\n    transform_names = list(transforms.keys())\n    transform_probs = [transforms[name] for name in transform_names]\n    transform_probs = np.array(transform_probs)\n    transform_probs = transform_probs / np.sum(transform_probs)\n    return transform_names, transform_probs\n\n", "CodeBleu_score": 19.38486487768144}]}
{"instance_id": "84bc5658c135", "generation_result": [{"code": "def _apply_basic(self, img, mixing_weights, m)\n\t\n    mixed = np.zeros(img.shape, dtype=np.uint8)\n    for i in range(m):\n        seq = np.random.choice(self.augmentations, p=mixing_weights)\n        for op in seq:\n            img = op(img)\n        mixed += img\n    return np.clip(mixed, 0, 255).astype(np.uint8)\n\n", "CodeBleu_score": 21.559888049475695}, {"code": "def _apply_basic(self, img, mixing_weights, m)\n\t\n    if m.shape[0] != len(mixing_weights):\n        raise ValueError(\"The number of mixing weights must be equal to the number of augmentations.\")\n    if m.shape[1] != 3:\n        raise ValueError(\"The mixing matrix must be of shape (n, 3) where n is the number of augmentations.\")\n    if m.shape[2] != img.size[1]:\n        raise ValueError(\"The height of the mixing matrix must be equal to the height of the image.\")\n    if m.shape[3] != img.size[0]:\n        raise ValueError(\"The width of the mixing matrix must be equal to the width of the image.\")\n    if len(img.getbands()) != 3:\n        raise ValueError(\"The image must be RGB.\")\n    if len(mixing_weights) != len(self.augmentations):\n        raise ValueError(\"The number of mixing weights must be equal to the number of augmentations.\")\n    if not all(0 <= w <= 1 for w in mixing_weights):\n        raise ValueError(\"All mixing weights must be between 0 and 1.\")\n    if not all(isinstance(aug, Augmentation) for aug in self.augmentations):\n        raise ValueError(\"All augmentations must be of type Augmentation.\")\n    if not all(isinstance(aug, Augmentation) for aug in self.augmentations):\n        raise ValueError(\"All augmentations must be of type Augmentation.\")\n    if not all(aug.supported_modes == self.augmentations[0].supported_modes for aug in self.augmentations):\n        raise ValueError(\"All augmentations must support the same modes.\")\n    if not all(aug.supported_dtypes == self.augmentations[0].supported_dtypes for aug in self.augmentations):\n        raise ValueError(\"All augmentations must support the same dtypes.\")\n    if not all(aug.supports_heatmaps == self.augmentations[0].supports_heatmaps for aug in self.augmentations):\n        raise ValueError(\"All augmentations must support the same heatmaps.\")\n    if not all(aug.supports_segmentation_maps == self.augmentations[0].supports_segmentation_maps for aug in self.augmentations):\n        raise ValueError(\"All augmentations must support the same segmentation maps.\")\n    if not all(aug.deterministic == self.augmentations[0].deterministic for aug in self.augmentations):", "CodeBleu_score": 19.642384657224866}, {"code": "def _apply_basic(self, img, mixing_weights, m)\n\t\n    mixed = np.zeros(img.shape, dtype=np.uint8)\n    for i in range(m):\n        seq = np.random.choice(self.augmentations, p=mixing_weights)\n        for op in seq:\n            img = op(img)\n        mixed += img\n    mixed = np.clip(mixed, 0, 255)\n    return Image.fromarray(mixed.astype(np.uint8))\n\n", "CodeBleu_score": 25.4224152342558}]}
{"instance_id": "84b5b752ff04", "generation_result": [{"code": "def adapt_to_chs(x, n)\n\t\n    if isinstance(x, (tuple, list)):\n        if len(x) == n:\n            assert len(x) == n\n        elif len(x) == 1:\n            x = x * n\n            _logger.warning(\n                f\"The length of {x} is 1, so it was repeated {n} times to form {x}.\"\n            )\n        else:\n            x = [np.mean(x)] * n\n            _logger.warning(\n                f\"The length of {x} is {len(x)}, so it was averaged and repeated {n} times to form {x}.\"\n            )\n    else:\n        x = (x,) * n\n    return x\n\n", "CodeBleu_score": 33.635679425906076}, {"code": "def adapt_to_chs(x, n)\n\t\n    if isinstance(x, (tuple, list)):\n        if len(x) == n:\n            assert len(x) == n, 'length of x must be equal to n'\n        elif len(x) == 1:\n            x = tuple(x * n)\n        else:\n            _logger.warning(\n                f'length of x ({len(x)}) does not match n ({n}), using mean of x')\n            x = tuple(np.mean(x) * n)\n    else:\n        x = tuple(x for _ in range(n))\n    return x\n\n", "CodeBleu_score": 41.24485049876236}, {"code": "def adapt_to_chs(x, n)\n\t\n    if not isinstance(x, (tuple, list)):\n        x = (x,) * n\n        _logger.warning(\n            \"Input to adapt_to_chs is not a tuple or list. \"\n            \"It is being repeated to form a tuple of length n.\"\n        )\n    elif len(x) != n:\n        x = (np.mean(x),) * n\n        _logger.warning(\n            \"Input to adapt_to_chs is a tuple or list but its length is not equal to n. \"\n            \"The mean of the tuple or list is being repeated to form a tuple of length n.\"\n        )\n    else:\n        assert len(x) == n\n    return x\n\n", "CodeBleu_score": 42.336427792215744}]}
{"instance_id": "54754b6ba394", "generation_result": [{"code": "def _worker_init(worker_id, worker_seeding='all')\n\t\n    worker_info = get_worker_info()\n    if worker_info is None:\n        raise RuntimeError('Cannot seed a child process before the parent process has been spawned.')\n    if worker_id == 0:\n        print('=> Using seed for random, torch, and numpy.')\n    if worker_seeding == 'all':\n        seed = worker_info.seed % (2**32 - 1)\n        random.seed(seed)\n        torch.manual_seed(seed)\n        worker_seeding = seed\n    if isinstance(worker_seeding, int):\n        np.random.seed(worker_seeding)\n    elif callable(worker_seeding):\n        worker_seeding(worker_id)\n    else:\n        raise RuntimeError('worker_seeding must be a callable or an int.')\n\n", "CodeBleu_score": 43.471547565816316}, {"code": "def _worker_init(worker_id, worker_seeding='all')\n\t\n    worker_info = get_worker_info()\n    if worker_info is None:\n        raise RuntimeError('Requires PyTorch >= 1.7.0')\n    wid = worker_info.id\n    if wid != worker_id:\n        raise RuntimeError(f'Worker id {wid} is different from worker_id {worker_id}')\n    if worker_seeding is not None:\n        if callable(worker_seeding):\n            worker_seeding(worker_info.seed)\n        elif worker_seeding == 'all':\n            seed = worker_info.seed\n            random.seed(seed)\n            torch.manual_seed(seed)\n            torch.cuda.manual_seed_all(seed)\n            np.random.seed(seed)\n        else:\n            raise ValueError('worker_seeding must be callable or \"all\"')\n", "CodeBleu_score": 38.640473997298805}, {"code": "def _worker_init(worker_id, worker_seeding='all')\n\t\n    worker_info = get_worker_info()\n    if worker_info is None:\n        raise RuntimeError('Cannot seed torch workers without the worker info context')\n    wid = worker_info.id\n    if wid != worker_id:\n        raise RuntimeError(f'Worker id {wid} is different from the expected worker id {worker_id}')\n    if isinstance(worker_seeding, str):\n        if worker_seeding == 'all':\n            worker_seeding = worker_info.seed\n        else:\n            raise ValueError(f'Unknown worker_seeding value {worker_seeding}')\n    if worker_seeding is not None:\n        worker_seeding(worker_info.seed)\n\n", "CodeBleu_score": 33.0242962028373}]}
{"instance_id": "81dfdce940db", "generation_result": [{"code": "def _decode_and_center_crop(image_bytes, image_size, resize_method)\n\t\n    shape = extract_jpeg_shape(image_bytes)\n    image_height = shape[0]\n    image_width = shape[1]\n    padded_center_crop_size = tf.cast(\n        ((image_size / (image_size + CROP_PADDING)) *\n         tf.cast(tf.minimum(image_height, image_width), tf.float32)), tf.int32)\n\n    offset_height = ((image_height - padded_center_crop_size) + 1) // 2\n    offset_width = ((image_width - padded_center_crop_size) + 1) // 2\n    crop_window = tf.stack([offset_height, offset_width, padded_center_crop_size, padded_center_crop_size])\n    image = tf.image.decode_and_crop_jpeg(image_bytes, crop_window, channels=3)\n    image = tf.image.resize([image], [image_size, image_size], method=resize_method)[0]\n    return image\n\n", "CodeBleu_score": 59.860919648427604}, {"code": "def _decode_and_center_crop(image_bytes, image_size, resize_method)\n\t\n    shape = extract_jpeg_shape(image_bytes)\n    image_height = shape[0]\n    image_width = shape[1]\n\n    padded_center_crop_size = tf.cast(\n        ((image_size / (image_size + CROP_PADDING)) *\n         tf.cast(tf.minimum(image_height, image_width), tf.float32)),\n        tf.int32)\n\n    offset_height = ((image_height - padded_center_crop_size) + 1) // 2\n    offset_width = ((image_width - padded_center_crop_size) + 1) // 2\n    crop_window = tf.stack([offset_height, offset_width,\n                            padded_center_crop_size, padded_center_crop_size])\n    image = decode_and_crop_jpeg(image_bytes, crop_window, channels=3)\n    image = resize(image, image_size, image_size, resize_method)\n    return image\n\n", "CodeBleu_score": 56.574565801771534}, {"code": "def _decode_and_center_crop(image_bytes, image_size, resize_method)\n\t\n    shape = extract_jpeg_shape(image_bytes)\n    image_height = shape[0]\n    image_width = shape[1]\n    padded_center_crop_size = tf.cast(\n        ((image_size / (image_size + CROP_PADDING)) *\n         tf.cast(tf.minimum(image_height, image_width), tf.float32)), tf.int32)\n\n    offset_height = ((image_height - padded_center_crop_size) + 1) // 2\n    offset_width = ((image_width - padded_center_crop_size) + 1) // 2\n    crop_window = tf.stack([offset_height, offset_width, padded_center_crop_size, padded_center_crop_size])\n    image = decode_and_crop_jpeg(image_bytes, crop_window, channels=3)\n    image = resize(image, [image_size, image_size], method=resize_method)\n    return image\n\n", "CodeBleu_score": 55.9940118532954}]}
{"instance_id": "cf16a6d37111", "generation_result": [{"code": "def preprocess_for_train(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    # Decode the jpeg image, end up with pixel values that are in the range [0, 255]\n    image = tf.image.decode_jpeg(image_bytes, channels=3)\n    # Resize the image to the specified height and width\n    image = tf.image.resize(image, [image_size, image_size], method=interpolation)\n    # Randomly crop a [height, width] section of the image.\n    image = _random_crop(image, image_size, image_size)\n    # Randomly flip the image horizontally.\n    image = _flip(image)\n    # Rescale to [-1,1] instead of [0, 1)\n    image = _rescale(image)\n    # Finally, reformat the image for the model, convert to bfloat16 if necessary, and return it\n    if use_bfloat16:\n        image = tf.image.convert_image_dtype(image, dtype=tf.bfloat16)\n    return image\n\n", "CodeBleu_score": 40.19328826530229}, {"code": "def preprocess_for_train(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    image = _decode_and_random_crop(image_bytes, image_size, interpolation)\n    image = _flip(image)\n    image = tf.reshape(image, [image_size, image_size, 3])\n    if use_bfloat16:\n        image = tf.image.convert_image_dtype(image, dtype=tf.bfloat16)\n    else:\n        image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n    return image\n\n", "CodeBleu_score": 45.62482957011018}, {"code": "def preprocess_for_train(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    image = _decode_and_random_crop(image_bytes, image_size, interpolation)\n    image = _flip(image)\n    image = tf.reshape(image, [image_size, image_size, 3])\n    image = _convert_image_dtype(image, tf.bfloat16 if use_bfloat16 else tf.float32)\n    return image\n\n", "CodeBleu_score": 35.74051098880117}]}
{"instance_id": "78b2a3bd2142", "generation_result": [{"code": "def preprocess_for_eval(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    # 1. Decode the image.\n    # 2. Crop the central region of the image with an area containing 87.5% of the original image.\n    # 3. Resize the image to the specified image size.\n    # 4. Rescale to [-1,1] instead of [0, 1)\n    image = _decode_and_center_crop(image_bytes, image_size)\n    # 5. Convert the image to dtype.\n    image = tf.image.convert_image_dtype(image, dtype=tf.bfloat16 if use_bfloat16 else tf.float32)\n    return image\n\n", "CodeBleu_score": 28.848682746336774}, {"code": "def preprocess_for_eval(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    image = _decode_and_center_crop(image_bytes)\n    image = _resize_image(image, image_size, interpolation)\n    image = _convert_image_dtype(image, use_bfloat16)\n    return image\n\n", "CodeBleu_score": 14.835564352554954}, {"code": "def preprocess_for_eval(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    if use_bfloat16:\n        image = tf.image.decode_and_crop_jpeg(image_bytes, _decode_and_center_crop(image_size), channels=3)\n        image = tf.image.resize(image, [image_size, image_size], method=interpolation)\n        image = tf.cast(image, dtype=tf.bfloat16)\n    else:\n        image = tf.image.decode_and_crop_jpeg(image_bytes, _decode_and_center_crop(image_size), channels=3)\n        image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n        image = tf.image.resize(image, [image_size, image_size], method=interpolation)\n    return image\n", "CodeBleu_score": 40.881383385513516}]}
{"instance_id": "8bd4364a9a0e", "generation_result": [{"code": "def evaluate(self, model, eval_dataloader=None, output_file=\"merged\")\n\t\n    if eval_dataloader is None:\n        eval_dataloader = self.eval_dataloader\n    self.compute_metrics = self.compute_metrics_fn\n    eval_loop = self.prediction_loop if self.args.use_legacy_prediction_loop else self.evaluation_loop\n    output_eval_file = os.path.join(self.args.output_dir, output_file)\n    if os.path.isfile(output_eval_file) and os.path.getsize(output_eval_file) > 0:\n        self.logger.info(\"Loading evaluation results from %s\", output_eval_file)\n        metrics = torch.load(output_eval_file)\n    else:\n        metrics = eval_loop(model, eval_dataloader, description=\"Evaluation\")\n        torch.save(metrics, output_eval_file)\n    self.compute_metrics = self.compute_metrics_original\n    return metrics\n\n", "CodeBleu_score": 45.77399668112762}, {"code": "def evaluate(self, model, eval_dataloader=None, output_file=\"merged\")\n\t\n    if eval_dataloader is None:\n        eval_dataloader = self.eval_dataloader\n    output_file = self.args.output_dir / f\"{output_file}.json\"\n    self.predict_and_save_results(model, eval_dataloader, output_file=output_file)\n    return self.compute_metrics(output_file)\n\n", "CodeBleu_score": 51.04835404455115}, {"code": "def evaluate(self, model, eval_dataloader=None, output_file=\"merged\")\n\t\n    if eval_dataloader is None:\n        eval_dataloader = self.eval_dataloader\n    else:\n        self.eval_dataloader = eval_dataloader\n    model.eval()\n    self.compute_metrics = self.compute_metrics_fn(eval_dataloader.dataset)\n    eval_loop = self.prediction_loop if self.args.use_legacy_prediction_loop else self.evaluation_loop\n    output = eval_loop(model, eval_dataloader, description=\"Evaluation\")\n    self.log(output.metrics)\n    self.control = self.callback_handler.on_evaluate(self.args, self.state, self.control, output.metrics)\n    if self.control.should_save:\n        self._save_evalute(output_file)\n    return output.metrics\n\n", "CodeBleu_score": 46.17427993867517}]}
{"instance_id": "22d682c958c0", "generation_result": [{"code": " def print_computed_metrics(self, metrics)\n\t\n    print('Computed metrics:')\n    print('  Rank@1: %.4f' % metrics['rank1'])\n    print('  Rank@5: %.4f' % metrics['rank5'])\n    print('  Mean IoU: %.4f' % metrics['miou'])\n    print('  Class IoU: ')\n    for k, v in metrics['class_iou'].items():\n        print('    %s: %.4f' % (k, v))\n    print('  Class Acc: ')\n    for k, v in metrics['class_acc'].items():\n        print('    %s: %.4f' % (k, v))\n    print('  Pixel Acc: %.4f' % metrics['pixel_acc'])\n    print('  Mean Acc: %.4f' % metrics['mean_acc'])\n    print('  Mean Acc Class: %.4f' % metrics['mean_acc_cls'])\n    print('  Frequency Weighted IoU: %.4f' % metrics['fwiou'])\n", "CodeBleu_score": 23.458157480846882}, {"code": " def print_computed_metrics(self, metrics)\n\t\n    print(\"Rank@1: {:.4f}\".format(metrics[\"rank1\"]))\n    print(\"Rank@5: {:.4f}\".format(metrics[\"rank5\"]))\n    print(\"mIoU: {:.4f}\".format(metrics[\"miou\"]))\n", "CodeBleu_score": 25.884161475728828}, {"code": " def print_computed_metrics(self, metrics)\n\t\n    print('Rank1: {0:.4f}'.format(metrics['rank1']))\n    print('Rank5: {0:.4f}'.format(metrics['rank5']))\n    print('mIoU: {0:.4f}'.format(metrics['miou']))\n", "CodeBleu_score": 23.18659431274208}]}
{"instance_id": "39c1a50b7e7d", "generation_result": [{"code": "def _eval_predictions(self, segments, data)\n\t\n    average_iou = []\n    average_ranks = []\n    for segment, data in zip(segments, data):\n        iou = []\n        rank = []\n        for i in range(len(segment)):\n            iou.append(self._iou(segment[i], data[i]))\n            rank.append(self._rank(segment[i], data[i]))\n        average_iou.append(np.mean(np.array(iou)))\n        average_ranks.append(np.sum(np.sort(np.array(rank))[:5])/len(rank))\n    rank1 = np.mean(np.array(average_ranks) < 1)\n    rank5 = np.mean(np.array(average_ranks) < 5)\n    mean_iou = np.mean(np.array(average_iou))\n    return rank1, rank5, mean_iou\n", "CodeBleu_score": 39.16649192770936}, {"code": "def _eval_predictions(self, segments, data)\n\t\n    average_iou = []\n    average_ranks = []\n    for segment, data in zip(segments, data):\n        ious = []\n        ranks = []\n        for s in segment:\n            ious.append(iou(s, data))\n        for d in data:\n            ranks.append(rank(segment, d))\n        average_iou.append(np.mean(np.array(ious)))\n        average_ranks.append(np.mean(np.array(ranks)))\n    rank1 = np.sum([1 for x in average_ranks if x == 1]) / len(average_ranks)\n    rank5 = np.sum([1 for x in average_ranks if x <= 5]) / len(average_ranks)\n    mean_iou = np.mean(np.array(average_iou))\n    return rank1, rank5, mean_iou\n\n", "CodeBleu_score": 37.77812329113786}, {"code": "def _eval_predictions(self, segments, data)\n\t\n    average_iou = []\n    average_ranks = []\n    for segment, data in zip(segments, data):\n        iou, rank = self._compute_iou_and_rank(segment, data)\n        average_iou.append(iou)\n        average_ranks.append(rank)\n    rank1 = np.mean(np.array(average_ranks) == 1) * 100\n    rank5 = np.mean(np.array(average_ranks) <= 5) * 100\n    mean_iou = np.mean(average_iou)\n    return rank1, rank5, mean_iou\n\n", "CodeBleu_score": 31.11988925490689}]}
{"instance_id": "8ccf84f8d034", "generation_result": [{"code": "def predict_loop(self, model, eval_dataloader, output_file=None)\n\t\n    self.scores = []\n    model.eval()\n    model.to(self.device)\n    torch.no_grad()\n    for batch in eval_dataloader:\n        batch = {k: v.to(self.device) for k, v in batch.items()}\n        outputs = model(**batch)\n        self.update(outputs)\n    self.finalize()\n    if output_file is not None:\n        self.to_ctx(output_file)\n    return self.scores\n\n", "CodeBleu_score": 28.79201031871631}, {"code": "def predict_loop(self, model, eval_dataloader, output_file=None)\n\t\n    scores = []\n    model.eval()\n    model.to(self.device)\n    for step, batch in enumerate(eval_dataloader):\n        batch = self.to_ctx(batch, self.device)\n        with torch.no_grad():\n            outputs = model(**batch)\n        scores.append(outputs)\n    scores = self.finalize(scores)\n    if output_file is not None:\n        self.save_json(scores, output_file)\n    return scores\n", "CodeBleu_score": 45.80419223660848}, {"code": "def predict_loop(self, model, eval_dataloader, output_file=None)\n\t\n    scores = []\n    model.eval()\n    model.to(self.device)\n    torch.no_grad()\n    for batch in eval_dataloader:\n        batch = self.to_ctx(batch)\n        outputs = model(**batch)\n        scores.update(outputs)\n    scores = self.finalize(scores)\n    if output_file is not None:\n        with open(output_file, \"w\") as file:\n            json.dump(scores, file)\n    return scores\n\n", "CodeBleu_score": 41.953383542920555}]}
{"instance_id": "e68529ad727d", "generation_result": [{"code": "def _aggregate_scores(self, scores)\n\t\n    assert len(scores) == 2, \"There should be exactly two sets of scores.\"\n    self.full_scores = np.concatenate(scores, axis=0)\n    self.full_scores = self.full_scores.reshape(self.full_scores.shape[0], -1)\n    return np.matmul(self.full_scores, self.text_model.text_hidden)\n\n", "CodeBleu_score": 33.14639872558689}, {"code": "def _aggregate_scores(self, scores)\n\t\n    assert len(scores) == 2, \"scores should contain exactly two sets of scores\"\n    # concatenate along the first axis\n    self.full_scores = np.concatenate(scores, axis=0)\n    # clear scores\n    self.scores = None\n    # return the dot product of the text and video hidden representations\n    return np.matmul(self.full_scores[:, :self.text_hidden_dim], self.full_scores[:, self.text_hidden_dim:])\n\n", "CodeBleu_score": 31.49041417594807}, {"code": "def _aggregate_scores(self, scores)\n\t\n    assert len(scores) == 2, \"scores must contain exactly two sets of scores\"\n    video_scores = np.concatenate(scores[0], axis=0)\n    text_scores = np.concatenate(scores[1], axis=0)\n    self.full_scores = None\n    return np.matmul(text_scores, video_scores.T)\n\n", "CodeBleu_score": 64.25373472086952}]}
{"instance_id": "e4e1861fc357", "generation_result": [{"code": "def finalize(self, Y_pred, Y_true, output_file=None)\n\t\n    Y_pred = Y_pred.cpu().detach().numpy()\n    Y_true = Y_true.cpu().detach().numpy()\n    assert Y_pred.shape == Y_true.shape, \"Predictions and true labels must have the same shape.\"\n    print(\"Predictions and true labels have the same shape: \" + str(Y_pred.shape))\n    print(\"Prediction errors: \" + str(np.sum(Y_pred != Y_true)))\n    print(\"Samples of prediction errors:\")\n    for i in range(len(Y_pred)):\n        if Y_pred[i] != Y_true[i]:\n            print(\"Predicted: \" + str(Y_pred[i]) + \", True: \" + str(Y_true[i]))\n    if output_file is not None:\n        with open(output_file, \"wb\") as f:\n            pickle.dump({\"Y_pred\": Y_pred, \"Y_true\": Y_true}, f)\n    return {\"Y_pred\": Y_pred, \"Y_true\": Y_true}\n\n", "CodeBleu_score": 40.338297901927824}, {"code": "def finalize(self, Y_pred, Y_true, output_file=None)\n\t\n    Y_pred = Y_pred.cpu().numpy()\n    Y_true = Y_true.cpu().numpy()\n    Y_pred = np.concatenate(Y_pred, axis=0)\n    Y_true = np.concatenate(Y_true, axis=0)\n    if output_file is not None:\n        with open(output_file, 'wb') as f:\n            pickle.dump((Y_pred, Y_true), f)\n    print('Finalizing predictions...')\n    if Y_pred.shape != Y_true.shape:\n        print('ERROR: The number of predicted and true labels differ.')\n        return\n    print('Number of predicted and true labels: %d' % Y_pred.shape[0])\n    print('Number of predicted labels: %d' % np.sum(Y_pred))\n    print('Number of true labels: %d' % np.sum(Y_true))\n    print('Number of correct predictions: %d' % np.sum(Y_pred == Y_true))\n    print('Number of incorrect predictions: %d' % np.sum(Y_pred != Y_true))\n    print('Fraction of correct predictions: %.2f%%' % (100.0 * np.sum(Y_pred == Y_true) / Y_pred.shape[0]))\n    print('Fraction of incorrect predictions: %.2f%%' % (100.0 * np.sum(Y_pred != Y_true) / Y_pred.shape[0]))\n    print('Fraction of missing labels: %.2f%%' % (100.0 * np.sum(Y_true == 0) / Y_true.shape[0]))\n    print('Fraction of erroneous labels: %.2f%%' % (100.0 * np.sum(Y_true == -1) / Y_true.shape[0]))\n    print('Fraction of correct predictions excluding missing labels: %.2f%%' % (100.0 * np.sum(Y_pred[Y_true != 0] == Y_true[Y_true != 0]) / np.sum(Y_true != 0)))\n    print('Fraction of correct predictions excluding erroneous labels: %.2f%%' % (100.0 * np.sum(Y_pred[Y_", "CodeBleu_score": 28.20946470071336}, {"code": "def finalize(self, Y_pred, Y_true, output_file=None)\n\t\n    Y_pred = np.array(Y_pred)\n    Y_true = np.array(Y_true)\n    # check for prediction errors\n    if Y_pred.shape != Y_true.shape:\n        raise ValueError('The shape of the predicted labels does not match the shape of the true labels.')\n    errors = np.where(Y_pred != Y_true)\n    if len(errors[0]) > 0:\n        print('The following samples were predicted incorrectly:')\n        for i in range(len(errors[0])):\n            print('Sample: ', errors[0][i])\n            print('Predicted label: ', Y_pred[errors[0][i]])\n            print('True label: ', Y_true[errors[0][i]])\n            print()\n    # save predictions and true labels\n    if output_file is not None:\n        with open(output_file, 'wb') as f:\n            pickle.dump({'Y_pred': Y_pred, 'Y_true': Y_true}, f)\n    return {'Y_pred': Y_pred, 'Y_true': Y_true}\n\n", "CodeBleu_score": 41.28529773439602}]}
{"instance_id": "7df9ab9a4920", "generation_result": [{"code": "def reduce_metrics(logging_outputs) -> None\n\t\n    loss_sum = sum(log.get(\"loss\", 0) for log in logging_outputs)\n    sample_size = sum(log.get(\"sample_size\", 0) for log in logging_outputs)\n\n    metrics.log_scalar(\n        \"loss\", loss_sum / sample_size / math.log(2), sample_size, round=3\n    )\n\n", "CodeBleu_score": 55.969294100204195}, {"code": "def reduce_metrics(logging_outputs) -> None\n\t\n    loss_sum = sum(log.get('loss', 0) for log in logging_outputs)\n    sample_size = sum(log.get('sample_size', 0) for log in logging_outputs)\n\n    metrics.log_scalar('loss', loss_sum / sample_size, sample_size, round=3)\n\n", "CodeBleu_score": 52.7910982560264}, {"code": "def reduce_metrics(logging_outputs) -> None\n\t\n    loss_sum = sum(log.get(\"loss\", 0) for log in logging_outputs)\n    sample_size = sum(log.get(\"sample_size\", 0) for log in logging_outputs)\n\n    metrics.log_scalar(\n        \"loss\", loss_sum / sample_size, sample_size, round=3\n    )  # The round=3 is for rounding to 3 decimal places.\n\n", "CodeBleu_score": 53.01218273043066}]}
{"instance_id": "7f9c326a96f7", "generation_result": [{"code": "def _mm_on_the_fly(\n        self,\n        cmasks,\n        vmasks,\n        attention_mask\n    )\n\t.\n    \"\"\"\n    if attention_mask is None:\n        attention_mask = torch.cat((cmasks, vmasks), dim=1)\n    token_type_ids = torch.cat((torch.zeros(cmasks.size()), torch.ones(vmasks.size())), dim=1)\n    return attention_mask, token_type_ids\n", "CodeBleu_score": 2.431943651246906}, {"code": "def _mm_on_the_fly(\n        self,\n        cmasks,\n        vmasks,\n        attention_mask\n    )\n\t.\n    \"\"\"\n    if attention_mask is None:\n        attention_mask = torch.cat((cmasks, vmasks), dim=1)\n    token_type_ids = torch.cat((torch.zeros_like(cmasks), torch.ones_like(vmasks)), dim=1)\n    return attention_mask, token_type_ids\n\n", "CodeBleu_score": 2.431943651246906}, {"code": "def _mm_on_the_fly(\n        self,\n        cmasks,\n        vmasks,\n        attention_mask\n    )\n\t.\n    \"\"\"\n    if attention_mask is None:\n        attention_mask = torch.cat((cmasks, vmasks), dim=1)\n    token_type_ids = torch.cat((torch.zeros(cmasks.size(), dtype=torch.long), torch.ones(vmasks.size(), dtype=torch.long)), dim=1)\n    return attention_mask, token_type_ids\n", "CodeBleu_score": 2.431943651246906}]}
{"instance_id": "fa99d2a148a5", "generation_result": [{"code": "def prepare_inputs_for_generation(\n        self,\n        input_ids,\n        input_video_embeds,\n        attention_mask=None,\n        token_type_ids=None,\n        **model_kwargs\n    )\n\t\n    input_ids = input_ids.to(self.device)\n    input_video_embeds = input_video_embeds.to(self.device)\n    if attention_mask is None:\n        attention_mask = torch.ones(input_ids.shape, device=self.device)\n    if token_type_ids is None:\n        token_type_ids = torch.zeros(input_ids.shape, dtype=torch.long, device=self.device)\n\n    # get the combined sequence length\n    combined_length = input_ids.shape[1] + input_video_embeds.shape[1]\n\n    # pad input_ids if necessary\n    if input_ids.shape[1] < combined_length:\n        padding_length = combined_length - input_ids.shape[1]\n        input_ids = torch.nn.functional.pad(input_ids, (0, padding_length), value=self.config.pad_token_id)\n        attention_mask = torch.nn.functional.pad(attention_mask, (0, padding_length), value=0)\n        token_type_ids = torch.nn.functional.pad(token_type_ids, (0, padding_length), value=0)\n\n    # pad input_video_embeds if necessary\n    if input_video_embeds.shape[1] < combined_length:\n        padding_length = combined_length - input_video_embeds.shape[1]\n        input_video_embeds = torch.nn.functional.pad(input_video_embeds, (0, padding_length), value=0)\n\n    # update the attention mask\n    attention_mask = torch.cat((attention_mask, torch.ones((input_video_embeds.shape[0], padding_length), device=self.device)), dim=1)\n\n    # update the token type ids\n    token_type_ids = torch.cat((token_type_ids, torch.ones((input_video_embeds.shape[0], padding_length), dtype=torch.long, device=self.device)), dim=1)\n\n    return {\n        \"input_ids\": input_ids,\n        \"input_video_embeds\": input_video_embeds,\n        \"attention_mask\": attention_mask,\n        \"token_type_ids\": token_type", "CodeBleu_score": 36.48060943346279}, {"code": "def prepare_inputs_for_generation(\n        self,\n        input_ids,\n        input_video_embeds,\n        attention_mask=None,\n        token_type_ids=None,\n        **model_kwargs\n    )\n\t\n    # set correct input_ids length\n    if input_ids is not None and input_video_embeds is not None:\n        input_ids_length = input_ids.shape[-1]\n        input_video_embeds_length = input_video_embeds.shape[-1]\n        total_sequence_length = input_ids_length + input_video_embeds_length\n        if total_sequence_length > self.config.max_position_embeddings:\n            input_ids_length = (total_sequence_length - input_video_embeds_length) // 2\n            input_ids = input_ids[:, :input_ids_length]\n        elif total_sequence_length < self.config.max_position_embeddings:\n            input_video_embeds_length = total_sequence_length - input_ids_length\n            input_video_embeds = input_video_embeds[:, :input_video_embeds_length]\n        # modify attention_mask\n        if attention_mask is not None:\n            attention_mask = attention_mask[:, :, :input_ids_length, :input_video_embeds_length]\n        # modify token_type_ids\n        if token_type_ids is not None:\n            token_type_ids = token_type_ids[:, :input_ids_length, :input_video_embeds_length]\n    # prepare inputs\n    inputs = {\n        \"input_ids\": input_ids,\n        \"input_video_embeds\": input_video_embeds,\n        \"attention_mask\": attention_mask,\n        \"token_type_ids\": token_type_ids,\n        \"decoder_input_ids\": input_ids,\n    }\n    return inputs\n\n", "CodeBleu_score": 47.76484507134507}, {"code": "def prepare_inputs_for_generation(\n        self,\n        input_ids,\n        input_video_embeds,\n        attention_mask=None,\n        token_type_ids=None,\n        **model_kwargs\n    )\n\t\n    # input_ids, input_video_embeds, attention_mask, token_type_ids = self._pad_to_max_length(\n    #     input_ids, input_video_embeds, attention_mask, token_type_ids,\n    #     max_length=self.config.max_length\n    # )\n    # input_ids, input_video_embeds, attention_mask, token_type_ids = self._pad_to_multiple_of(\n    #     input_ids, input_video_embeds, attention_mask, token_type_ids,\n    #     pad_to_multiple_of=self.config.pad_to_multiple_of\n    # )\n    input_ids, input_video_embeds, attention_mask, token_type_ids = self._resize_token_embeddings(\n        input_ids, input_video_embeds, attention_mask, token_type_ids,\n        max_length=self.config.max_length\n    )\n    return {\n        \"input_ids\": input_ids,\n        \"input_video_embeds\": input_video_embeds,\n        \"attention_mask\": attention_mask,\n        \"token_type_ids\": token_type_ids,\n    }\n\n", "CodeBleu_score": 28.692515916312587}]}
{"instance_id": "b1b0d25fe620", "generation_result": [{"code": "def _init_pipeline(self, cfg: ConfigType) -> Compose\n\t\n    cfg = copy.deepcopy(cfg)\n    if 'meta_keys' in cfg:\n        meta_keys = cfg.pop('meta_keys')\n        if 'img_id' in meta_keys:\n            meta_keys.remove('img_id')\n    else:\n        meta_keys = None\n    load_image_from_file_transform_idx = None\n    for idx, transform in enumerate(cfg):\n        if transform['type'] == 'LoadImageFromFile':\n            load_image_from_file_transform_idx = idx\n            transform['type'] = 'mmdet.InferencerLoader'\n            transform['meta_keys'] = meta_keys\n    if load_image_from_file_transform_idx is None:\n        raise ValueError(\n            'The test pipeline must contain a `LoadImageFromFile` transform')\n    return Compose(cfg)\n", "CodeBleu_score": 31.38972011525936}, {"code": "def _init_pipeline(self, cfg: ConfigType) -> Compose\n\t\n    cfg = cfg.copy()\n    if 'img_id' in cfg.data.test.pipeline[0]['meta_keys']:\n        cfg.data.test.pipeline[0]['meta_keys'].remove('img_id')\n    if 'LoadImageFromFile' not in cfg.data.test.pipeline[0]['type']:\n        raise ValueError(\n            f\"The test pipeline must contain a 'LoadImageFromFile' transform. The provided pipeline is: {cfg.data.test.pipeline}\"\n        )\n    transform_idx = None\n    for i, transform in enumerate(cfg.data.test.pipeline):\n        if transform['type'] == 'LoadImageFromFile':\n            transform_idx = i\n    if transform_idx is None:\n        raise ValueError(\n            f\"The test pipeline must contain a 'LoadImageFromFile' transform. The provided pipeline is: {cfg.data.test.pipeline}\"\n        )\n    cfg.data.test.pipeline[transform_idx]['type'] = 'mmdet.InferencerLoader'\n    return Compose(cfg.data.test.pipeline)\n\n", "CodeBleu_score": 27.020667541180245}, {"code": "def _init_pipeline(self, cfg: ConfigType) -> Compose\n\t\n    cfg = cfg.copy()\n    if 'img_id' in cfg.data.test.pipeline[0]['meta_keys']:\n        cfg.data.test.pipeline[0]['meta_keys'].remove('img_id')\n    pipeline = Compose(cfg.data.test.pipeline)\n    transform_idx = None\n    for idx, transform in enumerate(pipeline.transforms):\n        if isinstance(transform, LoadImageFromFile):\n            transform_idx = idx\n            break\n    if transform_idx is None:\n        raise ValueError('Cannot find the index of LoadImageFromFile transform')\n    pipeline.transforms[transform_idx] = InferencerLoader(\n        pipeline.transforms[transform_idx])\n    return pipeline\n\n", "CodeBleu_score": 33.74131607359891}]}
{"instance_id": "7967962856eb", "generation_result": [{"code": "def _inputs_to_list(self, inputs: InputsType) -> list\n\t\n    if isinstance(inputs, str):\n        inputs = list_dir_or_file(inputs)\n    if not isinstance(inputs, (list, tuple)):\n        inputs = [inputs]\n    return inputs\n\n", "CodeBleu_score": 32.234335486847}, {"code": "def _inputs_to_list(self, inputs: InputsType) -> list\n\t\n    if isinstance(inputs, str):\n        inputs = list_dir_or_file(inputs, recursive=True)\n    if not isinstance(inputs, (list, tuple)):\n        inputs = [inputs]\n    return inputs\n\n", "CodeBleu_score": 31.075591754949954}, {"code": "def _inputs_to_list(self, inputs: InputsType) -> list\n\t\n    if isinstance(inputs, str):\n        inputs = list_dir_or_file(inputs, recursive=True)\n    elif not isinstance(inputs, (list, tuple)):\n        inputs = [inputs]\n    return inputs\n\n", "CodeBleu_score": 28.627475277599153}]}
{"instance_id": "74f727f06fa0", "generation_result": [{"code": "def _get_chunk_data(self, inputs: Iterable, chunk_size: int)\n\t\n    if chunk_size <= 0:\n        raise ValueError(\"chunk_size must be greater than 0.\")\n    for input_data in inputs:\n        if isinstance(input_data, dict):\n            if \"img\" in input_data:\n                yield self.pipeline(input_data[\"img\"])\n            elif \"img_path\" in input_data:\n                yield self.pipeline(input_data[\"img_path\"])\n            else:\n                raise ValueError(\"Input data must contain either an 'img' or 'img_path' key.\")\n        else:\n            yield self.pipeline(input_data)\n\n", "CodeBleu_score": 24.142887919526046}, {"code": "def _get_chunk_data(self, inputs: Iterable, chunk_size: int)\n\t\n    chunk_data = []\n    for input in inputs:\n        if isinstance(input, dict):\n            if \"img\" in input:\n                chunk_data.append(self.pipeline(input[\"img\"]))\n            elif \"img_path\" in input:\n                img = cv2.imread(input[\"img_path\"])\n                chunk_data.append(self.pipeline(img))\n        else:\n            chunk_data.append(self.pipeline(input))\n        if len(chunk_data) == chunk_size:\n            yield chunk_data\n            chunk_data = []\n    if len(chunk_data) > 0:\n        yield chunk_data\n\n", "CodeBleu_score": 33.9485290125595}, {"code": "def _get_chunk_data(self, inputs: Iterable, chunk_size: int)\n\t\n    chunk_data = []\n    for i, input in enumerate(inputs):\n        if isinstance(input, dict):\n            if 'img' in input:\n                img = input['img']\n            elif 'img_path' in input:\n                img = cv2.imread(input['img_path'])\n            else:\n                raise KeyError(\"Input dictionary must contain an 'img' or 'img_path' key\")\n        else:\n            img = input\n        chunk_data.append(self.pipeline(img))\n        if (i + 1) % chunk_size == 0:\n            yield chunk_data\n            chunk_data = []\n    if chunk_data:\n        yield chunk_data\n\n", "CodeBleu_score": 44.5477728798757}]}
{"instance_id": "a43ab51afa59", "generation_result": [{"code": "def postprocess(\n        self,\n        preds: PredType,\n        visualization: Optional[List[np.ndarray]] = None,\n        return_datasamples: bool = False,\n        print_result: bool = False,\n        no_save_pred: bool = False,\n        pred_out_dir: str = '',\n        **kwargs,\n    ) -> Dict\n\t\n    if pred_out_dir:\n        os.makedirs(pred_out_dir, exist_ok=True)\n    if not return_datasamples:\n        if not no_save_pred:\n            if not pred_out_dir:\n                warnings.warn(\n                    'Predictions will not be saved because pred_out_dir is empty.'\n                )\n            else:\n                for pred in preds:\n                    pred_path = os.path.join(pred_out_dir, pred['name'] + '.json')\n                    with open(pred_path, 'w') as f:\n                        json.dump(pred, f)\n        results = self.pred2dict(preds)\n    else:\n        if pred_out_dir:\n            warnings.warn(\n                'Predictions will not be saved because return_datasamples is True.'\n            )\n        results = []\n    if visualization:\n        results.append(visualization)\n    if print_result:\n        print(results)\n    return results\n", "CodeBleu_score": 48.06001581677525}, {"code": "def postprocess(\n        self,\n        preds: PredType,\n        visualization: Optional[List[np.ndarray]] = None,\n        return_datasamples: bool = False,\n        print_result: bool = False,\n        no_save_pred: bool = False,\n        pred_out_dir: str = '',\n        **kwargs,\n    ) -> Dict\n\t\n    # Initialize output directory\n    if not no_save_pred and pred_out_dir:\n        os.makedirs(pred_out_dir, exist_ok=True)\n    # Initialize results\n    results = []\n    # Convert predictions to dictionaries\n    if return_datasamples:\n        if pred_out_dir:\n            warnings.warn(\n                'Saving datasamples is not supported when return_datasamples is True.'\n            )\n        results = [self.pred2dict(pred) for pred in preds]\n    # Append predictions to results\n    else:\n        results.extend(preds)\n    # Append visualizations to results\n    if visualization is not None:\n        results.extend(visualization)\n    # Print results\n    if print_result:\n        print(results)\n    # Save predictions\n    if not no_save_pred and pred_out_dir:\n        for i, result in enumerate(results):\n            if isinstance(result, dict):\n                filename = os.path.join(pred_out_dir, f'{i}.json')\n                with open(filename, 'w') as f:\n                    json.dump(result, f, indent=4)\n            else:\n                filename = os.path.join(pred_out_dir, f'{i}.npy')\n                np.save(filename, result)\n    return results\n\n", "CodeBleu_score": 45.097362012398634}, {"code": "def postprocess(\n        self,\n        preds: PredType,\n        visualization: Optional[List[np.ndarray]] = None,\n        return_datasamples: bool = False,\n        print_result: bool = False,\n        no_save_pred: bool = False,\n        pred_out_dir: str = '',\n        **kwargs,\n    ) -> Dict\n\t\n    # Initialize output directory\n    if not no_save_pred and pred_out_dir:\n        if not os.path.exists(pred_out_dir):\n            os.makedirs(pred_out_dir)\n\n    # Prepare result dictionary\n    result = {\n        'pred': [],\n        'visualization': [],\n    }\n\n    # Convert predictions to dictionaries and append them to results\n    if not return_datasamples:\n        for pred in preds:\n            result['pred'].append(self.pred2dict(pred))\n    else:\n        if pred_out_dir:\n            warnings.warn(\n                'Saving datasamples is not supported when return_datasamples is True.'\n            )\n        result['pred'] = preds\n\n    # Add visualization to results\n    if visualization is not None:\n        result['visualization'] = visualization\n\n    # Print results\n    if print_result:\n        self.print_results(result)\n\n    # Save results\n    if pred_out_dir:\n        self.save_results(result, pred_out_dir)\n\n    return result\n\n", "CodeBleu_score": 46.36993293191543}]}
{"instance_id": "a3c2561f199c", "generation_result": [{"code": "def build_test_pipeline(cfg: ConfigType) -> ConfigType\n\t\n    cfg = copy.deepcopy(cfg)\n    cfg.dataset.pipeline = cfg.dataset.pipeline[:1]\n    cfg.dataset.pipeline[0]['type'] = 'Resize'\n    cfg.dataset.pipeline = cfg.dataset.pipeline + cfg.dataset.pipeline[-1:]\n    cfg.dataset.pipeline = Compose(cfg.dataset.pipeline)\n    return cfg\n", "CodeBleu_score": 20.048543209759565}, {"code": "def build_test_pipeline(cfg: ConfigType) -> ConfigType\n\t\n    cfg = copy(cfg)\n    cfg.dataset.pipeline.transforms[0] = dict(Resize=dict(size=(300, 300)))\n    cfg.dataset.pipeline.transforms[-1] = dict(Resize=dict(size=(300, 300)))\n    cfg.dataset.pipeline.transforms = Compose(cfg.dataset.pipeline.transforms)\n    return cfg\n", "CodeBleu_score": 22.30667234963779}, {"code": "def build_test_pipeline(cfg: ConfigType) -> ConfigType\n\t\n    # copy first transform in dataset pipeline\n    test_pipeline = copy.deepcopy(cfg['dataset']['pipeline'][0])\n    # modify first transform in dataset pipeline\n    test_pipeline['type'] = 'Resize'\n    # copy last transform in dataset pipeline\n    test_pipeline_last = copy.deepcopy(cfg['dataset']['pipeline'][-1])\n    # combine modified first and copied last transforms in a new test pipeline\n    test_pipeline = Compose([test_pipeline] + cfg['dataset']['pipeline'][1:-1] + [test_pipeline_last])\n    return test_pipeline\n\n", "CodeBleu_score": 21.396089191079}]}
{"instance_id": "48a358ab5a38", "generation_result": [{"code": "def inference_mot(model: nn.Module, img: np.ndarray, frame_id: int,\n                  video_len: int) -> SampleList\n\t\n    data = dict(img=img, frame_id=frame_id, video_len=video_len)\n    data = build_test_pipeline(model.cfg)(data)\n    if isinstance(model, nn.parallel.DistributedDataParallel):\n        model = model.module\n    if not isinstance(model, nn.Module):\n        raise TypeError(\"Input model type is not a Module subclass\")\n    if isinstance(img, torch.Tensor):\n        img = img.numpy()\n    if not isinstance(img, np.ndarray):\n        raise TypeError(\"Input img type is not a numpy.ndarray\")\n    if not (isinstance(frame_id, int) or isinstance(frame_id, np.ndarray)):\n        raise TypeError(\"Input frame_id type is not a int or numpy.ndarray\")\n    if not (isinstance(video_len, int) or isinstance(video_len, np.ndarray)):\n        raise TypeError(\"Input video_len type is not a int or numpy.ndarray\")\n    if not (img.dtype == np.float32 or img.dtype == np.uint8):\n        raise ValueError(\"Input img must have np.float32 or np.uint8 type\")\n    if not isinstance(frame_id, np.ndarray):\n        frame_id = np.array(frame_id)\n    if not isinstance(video_len, np.ndarray):\n        video_len = np.array(video_len)\n    if img.dtype == np.uint8:\n        img = img.astype(np.float32)\n    if not torch.is_tensor(data[\"img\"]):\n        data[\"img\"] = torch.from_numpy(data[\"img\"])\n    if data[\"img\"].dtype != torch.float32:\n        data[\"img\"] = data[\"img\"].to(torch.float32)\n    if not isinstance(model, nn.Module):\n        raise TypeError(\"Input model type is not a Module subclass\")\n    if not (\n        isinstance(data[\"img\"], torch.Tensor)\n        and isinstance(data[\"frame_id\"], torch.Tensor)\n        and isinstance(data[\"video_len\"], torch.Tensor)\n    ):\n        raise TypeError(\"Input data must contain tensors\")\n    if not data[\"img\"].ndim == 4:\n        raise ValueError(\"Input data must be a BxCxHxW tensor\")\n    if not data[\"img\"].shape[0] == 1:\n       ", "CodeBleu_score": 32.02991860613581}, {"code": "def inference_mot(model: nn.Module, img: np.ndarray, frame_id: int,\n                  video_len: int) -> SampleList\n\t\n    # Prepare the input data\n    data = {\"image\": img, \"frame_id\": frame_id, \"original_shape\": img.shape,\n            \"image_id\": frame_id, \"video_len\": video_len}\n    # Build a test pipeline\n    test_pipeline = build_test_pipeline(cfg.data.test)\n    # Preprocess the data\n    data = test_pipeline(data)\n    # Convert image to float32\n    data[\"image\"] = data[\"image\"].astype(np.float32)\n    # If the model is on CPU, check for unsupported modules\n    if isinstance(model, nn.Module):\n        for module in model.modules():\n            if not isinstance(module, (RoIPool, RoIAlign)):\n                assert not isinstance(module, nn.modules.batchnorm._BatchNorm), \\\n                    \"CPU inference does not support BatchNorm layers!\"\n    # Perform forward pass\n    with torch.no_grad():\n        result = model(return_loss=False, rescale=True, **data)\n    return result\n", "CodeBleu_score": 36.23038291730482}, {"code": "def inference_mot(model: nn.Module, img: np.ndarray, frame_id: int,\n                  video_len: int) -> SampleList\n\t\n    # Prepare the input data\n    data = {\"image\": img, \"frame_id\": frame_id, \"original_shape\": img.shape,\n            \"image_id\": 0, \"video_len\": video_len}\n    # Build the test pipeline\n    test_pipeline = build_test_pipeline(model.cfg)\n    # Preprocess the data\n    data = test_pipeline(data)\n    # Move the data to the model's device\n    data = data.to(model.device)\n    # Perform forward inference\n    with torch.no_grad():\n        if model.device == \"cpu\":\n            model = model.to(device=\"cpu\")\n            model.eval()\n            for module in model.modules():\n                if not isinstance(module, nn.Sequential) and \\\n                        not isinstance(module, nn.ModuleList) and \\\n                        not isinstance(module, nn.ModuleDict) and \\\n                        not (module == model):\n                    if module.__class__.__name__ == \"RoIPool\":\n                        raise NotImplementedError(\n                            \"CPU inference is currently not supported with RoIPool.\"\n                        )\n        return model.inference(data)\n", "CodeBleu_score": 31.3709453841329}]}
{"instance_id": "6c4ea2ce1787", "generation_result": [{"code": "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\n\t\n    while True:\n        total_length = len(tokens_a) + len(tokens_b)\n        if total_length <= max_num_tokens:\n            break\n\n        trunc_tokens = tokens_a if len(tokens_a) > len(tokens_b) else tokens_b\n        assert len(trunc_tokens) >= 1\n\n        # We want to sometimes truncate from the front and sometimes from the\n        # back to add more randomness and avoid biases.\n        if rng.random() < 0.5:\n            del trunc_tokens[0]\n        else:\n            trunc_tokens.pop()\n\n", "CodeBleu_score": 77.02217016729102}, {"code": "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\n\t\n    while True:\n        total_length = len(tokens_a) + len(tokens_b)\n        if total_length <= max_num_tokens:\n            break\n        if len(tokens_a) > len(tokens_b):\n            trunc_tokens = tokens_a\n            assert len(trunc_tokens) >= 1\n        else:\n            trunc_tokens = tokens_b\n            assert len(trunc_tokens) >= 1\n        assert len(trunc_tokens) >= 1\n        # We want to sometimes truncate from the front and sometimes from the\n        # back to add more randomness and avoid biases.\n        if rng.random() < 0.5:\n            del trunc_tokens[0]\n        else:\n            trunc_tokens.pop()\n", "CodeBleu_score": 62.31098828050627}, {"code": "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\n\t\n    while True:\n        total_length = len(tokens_a) + len(tokens_b)\n        if total_length <= max_num_tokens:\n            break\n\n        trunc_tokens = tokens_a if len(tokens_a) > len(tokens_b) else tokens_b\n        assert len(trunc_tokens) >= 1\n\n        # We want to sometimes truncate from the front and sometimes from the\n        # back to add more randomness and avoid biases.\n        if rng.random() < 0.5:\n            del trunc_tokens[0]\n        else:\n            trunc_tokens.pop()\n", "CodeBleu_score": 77.02217016729102}]}
{"instance_id": "007181b6729d", "generation_result": [{"code": "def get_activation(activation_string)\n\t\n    if not isinstance(activation_string, str):\n        return activation_string\n    if not activation_string:\n        return None\n    act = activation_string.lower()\n    if act == 'linear':\n        return linear\n    elif act == 'relu':\n        return relu\n    elif act == 'sigmoid':\n        return sigmoid\n    elif act == 'tanh':\n        return tanh\n    elif act == 'softmax':\n        return softmax\n    else:\n        raise ValueError('Unsupported activation: %s' % act)\n\n", "CodeBleu_score": 70.82923492654677}, {"code": "def get_activation(activation_string)\n\t\n    if not isinstance(activation_string, str):\n        return activation_string\n    if not activation_string:\n        return None\n    act = activation_string.lower()\n    if act == 'linear':\n        return linear\n    elif act == 'relu':\n        return relu\n    elif act == 'sigmoid':\n        return sigmoid\n    elif act == 'tanh':\n        return tanh\n    elif act == 'elu':\n        return elu\n    elif act == 'selu':\n        return selu\n    elif act == 'softplus':\n        return softplus\n    elif act == 'softsign':\n        return softsign\n    elif act == 'hard_sigmoid':\n        return hard_sigmoid\n    elif act == 'exponential':\n        return exponential\n    elif act == 'linear_relu':\n        return linear_relu\n    elif act == 'soft_exponential':\n        return soft_exponential\n    elif act == 'soft_plus':\n        return soft_plus\n    elif act == 'soft_sign':\n        return soft_sign\n    elif act == 'hard_tanh':\n        return hard_tanh\n    elif act == 'logistic':\n        return logistic\n    elif act == 'relu6':\n        return relu6\n    elif act == 'leaky_relu':\n        return leaky_relu\n    elif act == 'prelu':\n        return prelu\n    elif act == 'elu':\n        return elu\n    elif act == 'swish':\n        return swish\n    elif act == 'mish':\n        return mish\n    elif act == 'hard_mish':\n        return hard_mish\n    elif act == 'hard_swish':\n        return hard_swish\n    else:\n        raise ValueError('Unsupported activation: %s' % act)\n", "CodeBleu_score": 58.958139381039544}, {"code": "def get_activation(activation_string)\n\t\n    # First, we check if the input is a string\n    if not isinstance(activation_string, str):\n        return activation_string\n    # If we get this far, we know that the input is a string\n    activation_string = activation_string.lower()\n    # Next, we check if the string is empty\n    if len(activation_string) == 0:\n        return None\n    # Next, we check if the string is a known activation\n    if activation_string in ['linear', 'identity', 'none']:\n        return None\n    elif activation_string in ['relu', 'leakyrelu', 'leaky_relu', 'leaky relu']:\n        return tf.nn.leaky_relu\n    elif activation_string in ['sigmoid', 'logistic', 'log_sigmoid']:\n        return tf.nn.sigmoid\n    elif activation_string in ['tanh', 'tanh_sigmoid']:\n        return tf.nn.tanh\n    elif activation_string in ['softmax', 'log_softmax']:\n        return tf.nn.softmax\n    elif activation_string in ['softplus', 'softplus_sigmoid']:\n        return tf.nn.softplus\n    elif activation_string in ['elu', 'exponential', 'exponential_linear']:\n        return tf.nn.elu\n    elif activation_string in ['selu', 'scaled_elu']:\n        return tf.nn.selu\n    elif activation_string in ['swish', 'swish_sigmoid']:\n        return tf.nn.swish\n    else:\n        raise ValueError('Unsupported activation function: ' + activation_string)\n\n", "CodeBleu_score": 49.712274509573504}]}
{"instance_id": "d5d6e0315899", "generation_result": [{"code": "def dropout(input_tensor, dropout_prob)\n\t\n    if dropout_prob is None or dropout_prob == 0.0:\n        return input_tensor\n    else:\n        return tf.nn.dropout(input_tensor, dropout_prob)\n\n", "CodeBleu_score": 53.31243274886709}, {"code": "def dropout(input_tensor, dropout_prob)\n\t\n    if dropout_prob is not None and dropout_prob > 0.0:\n        return tf.nn.dropout(input_tensor, keep_prob=1.0 - dropout_prob)\n    return input_tensor\n\n", "CodeBleu_score": 43.184690875255406}, {"code": "def dropout(input_tensor, dropout_prob)\n\t\n    if dropout_prob is not None and dropout_prob > 0.0:\n        return tf.nn.dropout(input_tensor, keep_prob=1 - dropout_prob)\n    return input_tensor\n\n", "CodeBleu_score": 41.6000266704443}]}
{"instance_id": "828917b9ac53", "generation_result": [{"code": "def create_attention_mask_from_input_mask(from_tensor, to_mask)\n\t\n    from_shape = get_shape_list(from_tensor, expected_rank=[2, 3])\n    batch_size = from_shape[0]\n    from_seq_length = from_shape[1]\n\n    to_shape = get_shape_list(to_mask, expected_rank=2)\n    to_seq_length = to_shape[1]\n\n    to_mask = tf.cast(\n        tf.reshape(to_mask, [batch_size, 1, to_seq_length]), tf.float32)\n\n    # We don't assume that `from_tensor` is a mask (although it could be). We\n    # don't actually care if we attend *from* padding tokens (only *to* padding)\n    # tokens so we create a tensor of all ones.\n    #\n    # `broadcast_ones` = [batch_size, from_seq_length, 1]\n    broadcast_ones = tf.ones(\n        shape=[batch_size, from_seq_length, 1], dtype=tf.float32)\n\n    # Here we broadcast along two dimensions to create the mask.\n    mask = broadcast_ones * to_mask\n\n    return mask\n\n", "CodeBleu_score": 64.6737227741467}, {"code": "def create_attention_mask_from_input_mask(from_tensor, to_mask)\n\t\n    from_shape = get_shape_list(from_tensor, expected_rank=[2, 3])\n    batch_size = from_shape[0]\n    from_seq_length = from_shape[1]\n\n    to_shape = get_shape_list(to_mask, expected_rank=2)\n    to_seq_length = to_shape[1]\n\n    to_mask = tf.cast(\n        tf.reshape(to_mask, [batch_size, 1, to_seq_length]), tf.float32)\n\n    # We don't assume that `from_tensor` is a mask (although it could be). We\n    # don't actually care if we attend *from* padding tokens (only *to* padding)\n    # tokens so we create a tensor of all ones.\n    #\n    # `broadcast_ones` = [batch_size, from_seq_length, 1]\n    broadcast_ones = tf.ones(\n        shape=[batch_size, from_seq_length, 1], dtype=tf.float32)\n\n    # Here we broadcast along two dimensions to create the mask.\n    mask = broadcast_ones * to_mask\n\n    return mask\n\n", "CodeBleu_score": 64.6737227741467}, {"code": "def create_attention_mask_from_input_mask(from_tensor, to_mask)\n\t\n    from_shape = get_shape_list(from_tensor, expected_rank=[2, 3])\n    batch_size = from_shape[0]\n    from_seq_length = from_shape[1]\n\n    to_shape = get_shape_list(to_mask, expected_rank=2)\n    to_seq_length = to_shape[1]\n\n    to_mask = tf.cast(\n        tf.reshape(to_mask, [batch_size, 1, to_seq_length]), tf.float32)\n\n    # We don't assume that `from_tensor` is a mask (although it could be). We\n    # don't actually care if we attend *from* padding tokens (only *to* padding)\n    # tokens so we create a tensor of all ones.\n    #\n    # `broadcast_ones` = [batch_size, from_seq_length, 1]\n    broadcast_ones = tf.ones(\n        shape=[batch_size, from_seq_length, 1], dtype=tf.float32)\n\n    # Here we broadcast along two dimensions to create the mask.\n    mask = broadcast_ones * to_mask\n\n    return mask\n\n", "CodeBleu_score": 64.6737227741467}]}
{"instance_id": "d7798e41a723", "generation_result": [{"code": "def transpose_for_scores(input_tensor, batch_size, num_attention_heads,\n                           seq_length, width)\n\t.\n\n    Args:\n      input_tensor: input tensor with shape [batch_size, seq_length, num_attention_heads, width]\n      batch_size: int\n      num_attention_heads: int\n      seq_length: int\n      width: int\n\n    Returns:\n      output tensor with shape [batch_size, num_attention_heads, seq_length, width]\n    \"\"\"\n    output_tensor = tf.reshape(input_tensor, [batch_size, seq_length, num_attention_heads, width])\n\n    output_tensor = tf.transpose(output_tensor, [0, 2, 1, 3])\n    return output_tensor\n\n", "CodeBleu_score": 17.261453703779406}, {"code": "def transpose_for_scores(input_tensor, batch_size, num_attention_heads,\n                           seq_length, width)\n\t.\n    \"\"\"\n    output_tensor = tf.reshape(input_tensor, [batch_size, seq_length, num_attention_heads, width])\n\n    output_tensor = tf.transpose(output_tensor, [0, 2, 1, 3])\n    return output_tensor\n\n", "CodeBleu_score": 6.348638191641985}, {"code": "def transpose_for_scores(input_tensor, batch_size, num_attention_heads,\n                           seq_length, width)\n\t.\n    \"\"\"\n    output_tensor = tf.reshape(input_tensor, [batch_size, seq_length, num_attention_heads, width])\n\n    output_tensor = tf.transpose(output_tensor, [0, 2, 1, 3])\n    return output_tensor\n\n", "CodeBleu_score": 6.348638191641985}]}
{"instance_id": "711d466ace09", "generation_result": [{"code": "def reshape_to_matrix(input_tensor)\n\t\n    ndims = input_tensor.shape.ndims\n    if ndims < 2:\n        raise ValueError(\"Input tensor must have at least rank 2. Shape = %s\" %\n                         (input_tensor.shape))\n    if ndims == 2:\n        return input_tensor\n\n    width = input_tensor.shape[-1]\n    output_tensor = tf.reshape(input_tensor, [-1, width])\n    return output_tensor\n\n", "CodeBleu_score": 83.6520561341743}, {"code": "def reshape_to_matrix(input_tensor)\n\t\n    ndims = input_tensor.shape.ndims\n    if ndims < 2:\n        raise ValueError(\"Input tensor must have at least rank 2. Shape = %s\" %\n                         (input_tensor.shape))\n    if ndims == 2:\n        return input_tensor\n\n    width = input_tensor.shape[-1]\n    output_tensor = tf.reshape(input_tensor, [-1, width])\n    return output_tensor\n\n", "CodeBleu_score": 83.6520561341743}, {"code": "def reshape_to_matrix(input_tensor)\n\t\n    ndims = input_tensor.shape.ndims\n    if ndims < 2:\n        raise ValueError(\"Input tensor must have at least rank 2. Shape = %s\" %\n                         (input_tensor.shape))\n    if ndims == 2:\n        return input_tensor\n\n    width = input_tensor.shape[-1]\n    output_tensor = tf.reshape(input_tensor, [-1, width])\n    return output_tensor\n\n", "CodeBleu_score": 83.6520561341743}]}
{"instance_id": "7adcd08f531e", "generation_result": [{"code": "def get_train_examples(self, data_dir)\n\t\n    train_file = os.path.join(data_dir, \"train.tsv\")\n    return self._create_examples(self._read_tsv(train_file), \"train\")\n\n", "CodeBleu_score": 6.209409432138726}, {"code": "def get_train_examples(self, data_dir)\n\t\n    return self._create_examples(self._read_tsv(os.path.join(data_dir, \"train.tsv\")), \"train\")\n\n", "CodeBleu_score": 5.998047197537955}, {"code": "def get_train_examples(self, data_dir)\n\t\n    return self._create_examples(self._read_tsv(os.path.join(data_dir, \"train.tsv\")), \"train\")\n", "CodeBleu_score": 5.998047197537955}]}
{"instance_id": "26a0bb96dab9", "generation_result": [{"code": "def metric_fn(per_example_loss, label_ids, logits, is_real_example)\n\t\n    predictions = tf.argmax(logits, axis=-1, output_type=tf.int32)\n    accuracy = tf.metrics.accuracy(\n        labels=label_ids, predictions=predictions, weights=is_real_example)\n    loss = tf.metrics.mean(\n        values=per_example_loss, weights=is_real_example)\n    return {\n        \"eval_accuracy\": accuracy,\n        \"eval_loss\": loss,\n    }\n", "CodeBleu_score": 61.73755888184837}, {"code": "def metric_fn(per_example_loss, label_ids, logits, is_real_example)\n\t\n    predictions = tf.argmax(logits, axis=-1, output_type=tf.int32)\n    accuracy = tf.metrics.accuracy(\n        labels=label_ids, predictions=predictions, weights=is_real_example)\n    loss = tf.metrics.mean(\n        values=per_example_loss, weights=is_real_example)\n    return {\n        \"eval_accuracy\": accuracy,\n        \"eval_loss\": loss,\n    }\n\n", "CodeBleu_score": 61.73755888184837}, {"code": "def metric_fn(per_example_loss, label_ids, logits, is_real_example)\n\t\n    predictions = tf.argmax(logits, axis=-1, output_type=tf.int32)\n    accuracy = tf.metrics.accuracy(\n        labels=label_ids, predictions=predictions, weights=is_real_example)\n    loss = tf.metrics.mean(values=per_example_loss, weights=is_real_example)\n    return {\n        \"eval_accuracy\": accuracy,\n        \"eval_loss\": loss,\n    }\n\n", "CodeBleu_score": 65.47238279114057}]}
{"instance_id": "6154e4d28509", "generation_result": [{"code": "def convert_examples_to_features(examples, label_list, max_seq_length,\n                                 tokenizer)\n\t\n    \"\"\"The function converts a set of InputExamples to a list of InputFeatures.", "CodeBleu_score": 4.674115508394336}, {"code": "def convert_examples_to_features(examples, label_list, max_seq_length,\n                                 tokenizer)\n\t\n    \"\"\"The function converts a set of InputExamples to a list of InputFeatures.", "CodeBleu_score": 4.674115508394336}, {"code": "def convert_examples_to_features(examples, label_list, max_seq_length,\n                                 tokenizer)\n\t\n    features = []\n    for (ex_index, example) in enumerate(examples):\n        if ex_index % 10000 == 0:\n            tf.logging.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n\n        feature = convert_single_example(ex_index, example, label_list,\n                                         max_seq_length, tokenizer)\n\n        features.append(feature)\n    return features\n\n", "CodeBleu_score": 82.25455119486146}]}
{"instance_id": "e09c0db71ebb", "generation_result": [{"code": "def get_next_sentence_output(bert_config, input_tensor, labels)\n\t\n    # Get the weights and biases for the next sentence prediction.\n    output_weights = tf.get_variable(\n        \"cls/seq_relationship/output_weights\",\n        shape=[2, bert_config.hidden_size],\n        initializer=modeling.create_initializer(bert_config.initializer_range))\n    output_bias = tf.get_variable(\n        \"cls/seq_relationship/output_bias\",\n        shape=[2],\n        initializer=tf.zeros_initializer())\n\n    # Apply one-hot encoding to the input tensor.\n    logits = tf.matmul(input_tensor, output_weights, transpose_b=True)\n    logits = tf.nn.bias_add(logits, output_bias)\n    logits = tf.reshape(logits, [-1, 2])\n\n    # Apply log softmax to compute probabilities.\n    log_probs = tf.nn.log_softmax(logits, axis=-1)\n\n    # Convert labels to one-hot encoding.\n    one_hot_labels = tf.one_hot(labels, depth=2, dtype=tf.float32)\n\n    # The per-example loss is the negative sum of the product of one-hot labels and log probabilities.\n    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n\n    # The mean loss is the average of the per-example loss.\n    loss = tf.reduce_mean(per_example_loss)\n\n    return (loss, per_example_loss, log_probs)\n\n", "CodeBleu_score": 51.22796775827376}, {"code": "def get_next_sentence_output(bert_config, input_tensor, labels)\n\t\n    # Simple binary classification. Note that 0 is \"next sentence\" and 1 is \"random sentence\". This weight matrix is not used after pre-training.\n    with tf.variable_scope(\"cls/seq_relationship\"):\n        output_weights = tf.get_variable(\n            \"output_weights\",\n            shape=[2, bert_config.hidden_size],\n            initializer=modeling.create_initializer(bert_config.initializer_range))\n        output_bias = tf.get_variable(\n            \"output_bias\", shape=[2], initializer=tf.zeros_initializer())\n\n        logits = tf.matmul(input_tensor, output_weights, transpose_b=True)\n        logits = tf.nn.bias_add(logits, output_bias)\n        log_probs = tf.nn.log_softmax(logits, axis=-1)\n\n        labels = tf.reshape(labels, [-1])\n        one_hot_labels = tf.one_hot(labels, depth=2, dtype=tf.float32)\n        per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n        loss = tf.reduce_mean(per_example_loss)\n        return (loss, per_example_loss, log_probs)\n\n", "CodeBleu_score": 86.94900271179796}, {"code": "def get_next_sentence_output(bert_config, input_tensor, labels)\n\t\n    # Simple binary classification. Note that 0 is \"next sentence\" and 1 is\n    # \"random sentence\". This weight matrix is not used after pre-training.\n    with tf.variable_scope(\"cls/seq_relationship\"):\n        output_weights = tf.get_variable(\n            \"output_weights\",\n            shape=[2, bert_config.hidden_size],\n            initializer=modeling.create_initializer(bert_config.initializer_range))\n        output_bias = tf.get_variable(\n            \"output_bias\", shape=[2], initializer=tf.zeros_initializer())\n\n        logits = tf.matmul(input_tensor, output_weights, transpose_b=True)\n        logits = tf.nn.bias_add(logits, output_bias)\n        log_probs = tf.nn.log_softmax(logits, axis=-1)\n\n        labels = tf.reshape(labels, [-1])\n        one_hot_labels = tf.one_hot(labels, depth=2, dtype=tf.float32)\n        per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n        loss = tf.reduce_mean(per_example_loss)\n        return (loss, per_example_loss, log_probs)\n", "CodeBleu_score": 86.94900271179796}]}
{"instance_id": "1f66b020f485", "generation_result": [{"code": "def gather_indexes(sequence_tensor, positions)\n\t\n\n    sequence_shape = modeling.get_shape_list(sequence_tensor, expected_rank=3)\n    batch_size = sequence_shape[0]\n    seq_length = sequence_shape[1]\n    width = sequence_shape[2]\n\n    flat_offsets = tf.reshape(\n        tf.range(0, batch_size, dtype=tf.int32) * seq_length, [-1, 1])\n    flat_positions = tf.reshape(positions + flat_offsets, [-1])\n    flat_sequence_tensor = tf.reshape(sequence_tensor,\n                                      [batch_size * seq_length, width])\n    output_tensor = tf.gather(flat_sequence_tensor, flat_positions)\n    return output_tensor\n\n", "CodeBleu_score": 57.853066637478335}, {"code": "def gather_indexes(sequence_tensor, positions)\n\t\n    sequence_shape = modeling.get_shape_list(sequence_tensor, expected_rank=3)\n    batch_size = sequence_shape[0]\n    seq_length = sequence_shape[1]\n    width = sequence_shape[2]\n\n    flat_offsets = tf.reshape(tf.range(0, batch_size, dtype=tf.int32) * seq_length, [-1, 1])\n    flat_positions = tf.reshape(positions + flat_offsets, [-1])\n    flat_sequence_tensor = tf.reshape(sequence_tensor, [batch_size * seq_length, width])\n    output_tensor = tf.gather(flat_sequence_tensor, flat_positions)\n    return output_tensor\n\n", "CodeBleu_score": 55.18468812590758}, {"code": "def gather_indexes(sequence_tensor, positions)\n\t\n    sequence_shape = modeling.get_shape_list(sequence_tensor, expected_rank=3)\n    batch_size = sequence_shape[0]\n    seq_length = sequence_shape[1]\n    width = sequence_shape[2]\n\n    flat_offsets = tf.reshape(tf.range(0, batch_size, dtype=tf.int32) * seq_length, [-1, 1])\n    flat_positions = tf.reshape(positions + flat_offsets, [-1])\n    flat_sequence_tensor = tf.reshape(sequence_tensor, [batch_size * seq_length, width])\n    output_tensor = tf.gather(flat_sequence_tensor, flat_positions)\n    return output_tensor\n\n", "CodeBleu_score": 55.18468812590758}]}
{"instance_id": "956bffee9a33", "generation_result": [{"code": "def _decode_record(record, name_to_features)\n\t\n    example = tf.parse_single_example(record, name_to_features)\n    for name in list(example.keys()):\n        t = example[name]\n        if t.dtype == tf.int64:\n            t = tf.to_int32(t)\n        example[name] = t\n    return example\n\n", "CodeBleu_score": 71.91618952996937}, {"code": "def _decode_record(record, name_to_features)\n\t\n    example = tf.parse_single_example(record, name_to_features)\n    for name in list(example.keys()):\n        t = example[name]\n        if t.dtype == tf.int64:\n            t = tf.to_int32(t)\n        example[name] = t\n    return example\n\n", "CodeBleu_score": 71.91618952996937}, {"code": "def _decode_record(record, name_to_features)\n\t\n    example = tf.parse_single_example(record, name_to_features)\n    for name in list(example.keys()):\n        t = example[name]\n        if t.dtype == tf.int64:\n            t = tf.to_int32(t)\n        example[name] = t\n    return example\n\n", "CodeBleu_score": 71.91618952996937}]}
{"instance_id": "6b97f354e641", "generation_result": [{"code": "def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer,\n                         orig_answer_text)\n\t\n    tok_answer_text = \" \".join(tokenizer.tokenize(orig_answer_text))\n\n    for new_start in range(input_start, input_end + 1):\n        for new_end in range(input_end, new_start - 1, -1):\n            text_span = \" \".join(doc_tokens[new_start:(new_end + 1)])\n            if text_span == tok_answer_text:\n                return (new_start, new_end)\n\n    return (input_start, input_end)\n\n", "CodeBleu_score": 78.30625722805418}, {"code": "def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer,\n                         orig_answer_text)\n\t\n    tok_answer_text = \" \".join(tokenizer.tokenize(orig_answer_text))\n\n    for new_start in range(input_start, input_end + 1):\n        for new_end in range(input_end, new_start - 1, -1):\n            text_span = \" \".join(doc_tokens[new_start:(new_end + 1)])\n            if text_span == tok_answer_text:\n                return (new_start, new_end)\n\n    return (input_start, input_end)\n\n", "CodeBleu_score": 78.30625722805418}, {"code": "def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer,\n                         orig_answer_text)\n\t\n    tok_answer_text = \" \".join(tokenizer.tokenize(orig_answer_text))\n\n    for new_start in range(input_start, input_end + 1):\n        for new_end in range(input_end, new_start - 1, -1):\n            text_span = \" \".join(doc_tokens[new_start:(new_end + 1)])\n            if text_span == tok_answer_text:\n                return (new_start, new_end)\n\n    return (input_start, input_end)\n\n", "CodeBleu_score": 78.30625722805418}]}
{"instance_id": "aba334b92b34", "generation_result": [{"code": "def _check_is_max_context(doc_spans, cur_span_index, position)\n\t\n    best_score = None\n    best_span_index = None\n    for (span_index, doc_span) in enumerate(doc_spans):\n        end = doc_span.start + doc_span.length - 1\n        if position < doc_span.start:\n            continue\n        if position > end:\n            continue\n        num_left_context = position - doc_span.start\n        num_right_context = end - position\n        score = min(num_left_context, num_right_context) + 0.01 * doc_span.length\n        if best_score is None or score > best_score:\n            best_score = score\n            best_span_index = span_index\n\n    return cur_span_index == best_span_index\n\n", "CodeBleu_score": 78.23889503357583}, {"code": "def _check_is_max_context(doc_spans, cur_span_index, position)\n\t\n    best_score = None\n    best_span_index = None\n    for (span_index, doc_span) in enumerate(doc_spans):\n        end = doc_span.start + doc_span.length - 1\n        if position < doc_span.start:\n            continue\n        if position > end:\n            continue\n        num_left_context = position - doc_span.start\n        num_right_context = end - position\n        score = min(num_left_context, num_right_context) + 0.01 * doc_span.length\n        if best_score is None or score > best_score:\n            best_score = score\n            best_span_index = span_index\n\n    return cur_span_index == best_span_index\n\n", "CodeBleu_score": 78.23889503357583}, {"code": "def _check_is_max_context(doc_spans, cur_span_index, position)\n\t\n    best_score = None\n    best_span_index = None\n    for (span_index, doc_span) in enumerate(doc_spans):\n        end = doc_span.start + doc_span.length - 1\n        if position < doc_span.start:\n            continue\n        if position > end:\n            continue\n        num_left_context = position - doc_span.start\n        num_right_context = end - position\n        score = min(num_left_context, num_right_context) + 0.01 * doc_span.length\n        if best_score is None or score > best_score:\n            best_score = score\n            best_span_index = span_index\n\n    return cur_span_index == best_span_index\n\n", "CodeBleu_score": 78.23889503357583}]}
{"instance_id": "cf92ad509f6a", "generation_result": [{"code": "def load_vocab(vocab_file)\n\t\n    vocab = collections.OrderedDict()\n    index = 0\n    with open(vocab_file, \"r\", encoding=\"utf-8\") as reader:\n        while True:\n            token = convert_to_unicode(reader.readline())\n            if not token:\n                break\n            token = token.strip()\n            vocab[token] = index\n            index += 1\n    return vocab\n\n", "CodeBleu_score": 65.86707343330198}, {"code": "def load_vocab(vocab_file)\n\t\n    vocab = collections.OrderedDict()\n    index = 0\n    with tf.gfile.GFile(vocab_file, \"r\") as reader:\n        while True:\n            token = convert_to_unicode(reader.readline())\n            if not token:\n                break\n            token = token.strip()\n            vocab[token] = index\n            index += 1\n    return vocab\n", "CodeBleu_score": 74.17973775756064}, {"code": "def load_vocab(vocab_file)\n\t\n    vocab = collections.OrderedDict()\n    index = 0\n    with open(vocab_file, \"r\", encoding=\"utf-8\") as reader:\n        while True:\n            token = convert_to_unicode(reader.readline())\n            if not token:\n                break\n            token = token.strip()\n            vocab[token] = index\n            index += 1\n    return vocab\n", "CodeBleu_score": 65.86707343330198}]}
{"instance_id": "6c865803ac12", "generation_result": [{"code": "def convert_by_vocab(vocab, items)\n\t\n    output = []\n    for item in items:\n        output.append(vocab[item])\n    return output\n\n", "CodeBleu_score": 79.63749907165396}, {"code": "def convert_by_vocab(vocab, items)\n\t\n    output = []\n    for item in items:\n        output.append(vocab[item])\n    return output\n\n", "CodeBleu_score": 79.63749907165396}, {"code": "def convert_by_vocab(vocab, items)\n\t\n    output = []\n    for item in items:\n        output.append(vocab.get(item, UNK_ID))\n    return output\n\n", "CodeBleu_score": 59.42791785154598}]}
{"instance_id": "e93b9200dc4a", "generation_result": [{"code": "def tokenize(self, text)\n\t\n    split_tokens = []\n    for token in self.basic_tokenizer.tokenize(text):\n        for sub_token in self.wordpiece_tokenizer.tokenize(token):\n            split_tokens.append(sub_token)\n\n    return split_tokens\n", "CodeBleu_score": 88.10762355418231}, {"code": "def tokenize(self, text)\n\t\n    split_tokens = []\n    for token in self.basic_tokenizer.tokenize(text):\n        for sub_token in self.wordpiece_tokenizer.tokenize(token):\n            split_tokens.append(sub_token)\n\n    return split_tokens\n\n", "CodeBleu_score": 88.10762355418231}, {"code": "def tokenize(self, text)\n\t\n    # Basic tokenizer\n    text = self.basic_tokenizer.tokenize(text)\n    # Wordpiece tokenizer\n    text = self.wordpiece_tokenizer.tokenize(text)\n    return text\n\n", "CodeBleu_score": 16.376586011334997}]}
{"instance_id": "2ff6f6db53c9", "generation_result": [{"code": "def safe_load(path, offset, duration, sample_rate, dtype)\n\t\n    try:\n        logger.info(f\"Loading audio data from {path}...\")\n        audio_data = self.load(path, offset=offset, duration=duration)\n        logger.info(f\"Successfully loaded audio data from {path}.\")\n        return audio_data, False\n    except Exception as e:\n        logger.exception(e)\n        logger.warning(\n            f\"Error loading audio data from {path}.", "CodeBleu_score": 33.967840106561056}, {"code": "def safe_load(path, offset, duration, sample_rate, dtype)\n\t\n    try:\n        logger.info(f\"Loading audio data from {path}...\")\n        data, _ = librosa.load(path, offset=offset, duration=duration, sr=sample_rate, dtype=dtype)\n        logger.info(f\"Loaded audio data from {path} successfully.\")\n        return data, False\n    except Exception as e:\n        logger.exception(f\"Failed to load audio data from {path}.\")\n        return -1.0, True\n\n", "CodeBleu_score": 37.79990258254631}, {"code": "def safe_load(path, offset, duration, sample_rate, dtype)\n\t\n    try:\n        logger.info(f\"Loading audio data from {path}...\")\n        audio_data = self.load(path, offset=offset, duration=duration)\n        logger.info(f\"Loaded audio data from {path}...\")\n        return audio_data, False\n    except Exception as e:\n        logger.exception(f\"Error loading audio data from {path}...\")\n        return -1.0, True\n", "CodeBleu_score": 33.54305239244361}]}
{"instance_id": "12fbe1f03d8f", "generation_result": [{"code": "def to_n_channels(waveform: tf.Tensor, n_channels: int) -> tf.Tensor\n\t\n    if n_channels < 1:\n        raise ValueError(f\"The number of channels must be at least 1, but was {n_channels}.\")\n    if n_channels == 1:\n        return waveform[..., 0:1]\n    elif n_channels > waveform.shape[-1]:\n        return tf.tile(waveform, [1, 1, n_channels])\n    else:\n        return waveform[..., 0:n_channels]\n\n", "CodeBleu_score": 36.57676323656622}, {"code": "def to_n_channels(waveform: tf.Tensor, n_channels: int) -> tf.Tensor\n\t\n    if n_channels < 1:\n        raise ValueError(\"n_channels must be at least 1.\")\n    if n_channels == 1:\n        return tf.squeeze(waveform, axis=-1)\n    n_channels_in = tf.shape(waveform)[-1]\n    if n_channels_in >= n_channels:\n        return tf.slice(waveform, begin=[0, 0], size=[-1, n_channels])\n    else:\n        return tf.tile(waveform, [1, 1, n_channels // n_channels_in + 1])[:, :, :n_channels]\n\n", "CodeBleu_score": 47.639728715676846}, {"code": "def to_n_channels(waveform: tf.Tensor, n_channels: int) -> tf.Tensor\n\t\n    # Check if the waveform has the correct number of channels\n    if n_channels == 1:\n        return tf.squeeze(waveform, axis=-1)\n    elif n_channels == 2:\n        return tf.tile(waveform, [1, 1, 2])\n    elif n_channels == 3:\n        return tf.tile(waveform, [1, 1, 3])\n    elif n_channels == 4:\n        return tf.tile(waveform, [1, 1, 4])\n    else:\n        # If the waveform has more or less channels, trim or tile it to the desired number of channels\n        if tf.shape(waveform)[-1] > n_channels:\n            return tf.slice(waveform, [0, 0, 0], [-1, -1, n_channels])\n        elif tf.shape(waveform)[-1] < n_channels:\n            return tf.tile(waveform, [1, 1, n_channels])\n        else:\n            return waveform\n\n", "CodeBleu_score": 41.8105850086504}]}
{"instance_id": "5e215f029b2e", "generation_result": [{"code": "def spectrogram_to_db_uint(\n    spectrogram: tf.Tensor, db_range: float = 100.0, **kwargs\n) -> tf.Tensor\n\t\n    spectrogram = gain_to_db(spectrogram, **kwargs)\n    db_max = tf.reduce_max(spectrogram)\n    db_range = tf.cast(db_range, spectrogram.dtype)\n    return tf.cast(\n        tf.clip_by_value(spectrogram, -db_range, db_range), tf.uint8\n    )\n\n", "CodeBleu_score": 40.1392291782674}, {"code": "def spectrogram_to_db_uint(\n    spectrogram: tf.Tensor, db_range: float = 100.0, **kwargs\n) -> tf.Tensor\n\t\n    db_spectrogram = gain_to_db(spectrogram, **kwargs)\n    db_max = tf.reduce_max(db_spectrogram)\n    db_min = db_max - db_range\n    db_spectrogram = tf.clip_by_value(db_spectrogram, db_min, db_max)\n    return from_float32_to_uint8(db_spectrogram)\n\n", "CodeBleu_score": 44.700833822742226}, {"code": "def spectrogram_to_db_uint(\n    spectrogram: tf.Tensor, db_range: float = 100.0, **kwargs\n) -> tf.Tensor\n\t\n    return from_float32_to_uint8(\n        tf.clip_by_value(\n            gain_to_db(spectrogram, **kwargs), -db_range, 0.0\n        )\n    )\n\n", "CodeBleu_score": 29.591564068208022}]}
{"instance_id": "b65c0968dae7", "generation_result": [{"code": "def compute_spectrogram_tf(\n    waveform: tf.Tensor,\n    frame_length: int = 2048,\n    frame_step: int = 512,\n    spec_exponent: float = 1.0,\n    window_exponent: float = 1.0,\n) -> tf.Tensor\n\t\n    # Apply Hann window function to the waveform\n    window = hann_window(waveform.shape[-1], window_exponent)\n    windowed_waveform = waveform * window\n\n    # Compute the Short-Time Fourier Transform of the windowed waveform\n    stft_waveform = stft(windowed_waveform, frame_length, frame_step)\n\n    # Transpose the STFT tensor to get the shape (batch_size, time_steps, freq_bins)\n    stft_waveform = tf.transpose(stft_waveform, perm=[0, 2, 1])\n\n    # Get the magnitude of the STFT tensor\n    magnitude_stft_waveform = tf.abs(stft_waveform)\n\n    # Raise the magnitude to the specified exponent\n    magnitude_stft_waveform = tf.pow(magnitude_stft_waveform, spec_exponent)\n\n    return magnitude_stft_waveform\n", "CodeBleu_score": 35.45308850241176}, {"code": "def compute_spectrogram_tf(\n    waveform: tf.Tensor,\n    frame_length: int = 2048,\n    frame_step: int = 512,\n    spec_exponent: float = 1.0,\n    window_exponent: float = 1.0,\n) -> tf.Tensor\n\t\n    # Apply Hann window function to the waveform\n    window = hann_window(waveform.shape[0], window_exponent)\n    windowed_waveform = waveform * window\n\n    # Compute STFT\n    stft_matrix = stft(\n        windowed_waveform, frame_length, frame_step, window_exponent\n    )\n\n    # Transpose the STFT matrix and take the absolute value\n    stft_matrix_t = tf.transpose(stft_matrix)\n    stft_matrix_t_abs = tf.abs(stft_matrix_t)\n\n    # Raise the absolute value to a specified exponent\n    spectrogram = tf.pow(stft_matrix_t_abs, spec_exponent)\n\n    return spectrogram\n\n", "CodeBleu_score": 36.096187108543745}, {"code": "def compute_spectrogram_tf(\n    waveform: tf.Tensor,\n    frame_length: int = 2048,\n    frame_step: int = 512,\n    spec_exponent: float = 1.0,\n    window_exponent: float = 1.0,\n) -> tf.Tensor\n\t\n    window = hann_window(frame_length, window_exponent)\n    stft_matrix = stft(\n        waveform, frame_length, frame_step, fft_length=frame_length, window_fn=window\n    )\n    spectrogram = tf.transpose(tf.abs(stft_matrix) ** spec_exponent)\n    return spectrogram\n\n", "CodeBleu_score": 34.48213221282811}]}
{"instance_id": "a4686140a634", "generation_result": [{"code": "def time_stretch(\n    spectrogram: tf.Tensor,\n    factor: float = 1.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n    original_shape = tf.shape(spectrogram)\n    time_size = original_shape[1]\n    new_time_size = tf.cast(time_size * factor, tf.int32)\n    resized_spectrogram = resize_images(\n        spectrogram,\n        [original_shape[0], new_time_size],\n        method=method,\n    )\n    resized_shape = tf.shape(resized_spectrogram)\n    resized_time_size = resized_shape[1]\n    diff = resized_time_size - time_size\n    if diff > 0:\n        resized_spectrogram = resized_spectrogram[:, :time_size]\n    else:\n        resized_spectrogram = resize_image_with_crop_or_pad(\n            resized_spectrogram,\n            time_size,\n            time_size,\n        )\n    return resized_spectrogram\n", "CodeBleu_score": 32.281253220123084}, {"code": "def time_stretch(\n    spectrogram: tf.Tensor,\n    factor: float = 1.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n    # Calculate the new time dimension\n    new_time_dim = tf.cast(tf.shape(spectrogram)[1] * factor, tf.int32)\n\n    # Resize the spectrogram\n    resized_spectrogram = resize_images(\n        spectrogram,\n        size=[new_time_dim, tf.shape(spectrogram)[2]],\n        method=method,\n    )\n\n    # Adjust the resized spectrogram to the original dimensions\n    resized_spectrogram = resize_image_with_crop_or_pad(\n        resized_spectrogram,\n        target_height=tf.shape(spectrogram)[1],\n        target_width=tf.shape(spectrogram)[2],\n    )\n\n    return resized_spectrogram\n\n", "CodeBleu_score": 34.85678601621684}, {"code": "def time_stretch(\n    spectrogram: tf.Tensor,\n    factor: float = 1.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n    input_shape = tf.shape(spectrogram)\n    input_time_dim = input_shape[1]\n    new_time_dim = tf.cast(input_time_dim * factor, tf.int32)\n    resized_spectrogram = tf.image.resize(\n        spectrogram,\n        [input_shape[0], new_time_dim],\n        method=method,\n    )\n    if new_time_dim > input_time_dim:\n        resized_spectrogram = resize_image_with_crop_or_pad(\n            resized_spectrogram,\n            input_shape[0],\n            input_shape[1],\n        )\n    else:\n        resized_spectrogram = resize_image_with_crop_or_pad(\n            resized_spectrogram,\n            input_shape[0],\n            new_time_dim,\n        )\n    return resized_spectrogram\n\n", "CodeBleu_score": 32.052135321676836}]}
{"instance_id": "6dd01101d4ae", "generation_result": [{"code": "def pitch_shift(\n    spectrogram: tf.Tensor,\n    semitone_shift: float = 0.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n    if semitone_shift == 0.0:\n        return spectrogram\n    num_freq = tf.shape(spectrogram)[-2]\n    freq_factor = 2 ** (semitone_shift / 12)\n    new_num_freq = tf.cast(num_freq * freq_factor, tf.int32)\n    new_num_freq = tf.maximum(new_num_freq, 1)\n    new_num_freq = tf.minimum(new_num_freq, num_freq)\n    new_num_freq = tf.cast(new_num_freq, tf.int32)\n    new_num_freq = tf.expand_dims(new_num_freq, axis=-1)\n    new_num_freq = tf.expand_dims(new_num_freq, axis=-1)\n    new_num_freq = tf.expand_dims(new_num_freq, axis=-1)\n    new_num_freq = tf.cast(new_num_freq, tf.float32)\n    resized_spectrogram = tf.image.resize(\n        spectrogram,\n        size=[new_num_freq, tf.shape(spectrogram)[-1]],\n        method=method,\n    )\n    resized_spectrogram = tf.pad(\n        resized_spectrogram,\n        paddings=[\n            [0, 0],\n            [0, 0],\n            [0, 0],\n            [tf.maximum(0, new_num_freq - num_freq), 0],\n        ],\n        mode=\"CONSTANT\",\n        constant_values=0.0,\n    )\n    return resized_spectrogram\n\n", "CodeBleu_score": 40.24494764413839}, {"code": "def pitch_shift(\n    spectrogram: tf.Tensor,\n    semitone_shift: float = 0.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n    # Calculating the resizing factor\n    factor = 2.0 ** (semitone_shift / 12.0)\n    # Resizing the spectrogram\n    resized_spectrogram = resize_images(\n        spectrogram,\n        size=[int(spectrogram.shape[0] * factor), spectrogram.shape[1]],\n        method=method,\n    )\n    # Padding the resized spectrogram to maintain the original dimensions\n    return tf.pad(\n        resized_spectrogram,\n        paddings=[\n            [\n                int(spectrogram.shape[0] * factor - spectrogram.shape[0]),\n                spectrogram.shape[0] - int(spectrogram.shape[0] * factor),\n            ],\n            [0, 0],\n        ],\n    )\n\n", "CodeBleu_score": 27.77069018205432}, {"code": "def pitch_shift(\n    spectrogram: tf.Tensor,\n    semitone_shift: float = 0.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n    if semitone_shift == 0.0:\n        return spectrogram\n    # Calculating the resizing factor\n    factor = 2 ** (semitone_shift / 12)\n    # Resizing the spectrogram\n    resized_spectrogram = resize_images(\n        spectrogram,\n        size=[\n            tf.cast(tf.math.ceil(tf.shape(spectrogram)[0] * factor), tf.int32),\n            tf.shape(spectrogram)[1],\n        ],\n        method=method,\n    )\n    # Paddings the resized spectrogram to maintain the original dimensions\n    padded_spectrogram = tf.pad(\n        resized_spectrogram,\n        paddings=[\n            [\n                tf.math.floordiv(\n                    tf.cast(tf.shape(spectrogram)[0], tf.int32), 2\n                ) - tf.math.floordiv(tf.shape(resized_spectrogram)[0], 2),\n                tf.math.floordiv(\n                    tf.cast(tf.shape(spectrogram)[0], tf.int32), 2\n                )\n                - tf.math.floordiv(tf.shape(resized_spectrogram)[0], 2),\n            ],\n            [0, 0],\n        ],\n        mode=\"CONSTANT\",\n        constant_values=0.0,\n    )\n    return padded_spectrogram\n\n", "CodeBleu_score": 41.828994061943106}]}
{"instance_id": "612f0826e56a", "generation_result": [{"code": "def _get_conv_activation_layer(params: Dict) -> Any\n\t\n    conv_activation = params.get(\"conv_activation\", \"ReLU\")\n    if conv_activation == \"ReLU\":\n        return ReLU()\n    elif conv_activation == \"ELU\":\n        return ELU()\n    elif conv_activation == \"LeakyReLU\":\n        return LeakyReLU(alpha=0.2)\n    else:\n        raise ValueError(f\"Unknown activation function {conv_activation}\")\n\n", "CodeBleu_score": 50.983492147562494}, {"code": "def _get_conv_activation_layer(params: Dict) -> Any\n\t\n    conv_activation = params.get(\"conv_activation\", \"ReLU\")\n    if conv_activation == \"ReLU\":\n        return nn.ReLU()\n    elif conv_activation == \"ELU\":\n        return nn.ELU()\n    elif conv_activation == \"LeakyReLU\":\n        return nn.LeakyReLU(negative_slope=0.2)\n    else:\n        raise ValueError(f\"Unknown activation function: {conv_activation}\")\n\n", "CodeBleu_score": 42.38926490878458}, {"code": "def _get_conv_activation_layer(params: Dict) -> Any\n\t\n    conv_activation = params.get(\"conv_activation\", \"ReLU\")\n    if conv_activation == \"ReLU\":\n        return nn.ReLU()\n    elif conv_activation == \"ELU\":\n        return nn.ELU()\n    elif conv_activation == \"LeakyReLU\":\n        return nn.LeakyReLU(negative_slope=0.2)\n    else:\n        raise ValueError(\n            \"Unknown activation function: {0}.\".format(conv_activation)\n        )\n\n", "CodeBleu_score": 43.08822893079806}]}
{"instance_id": "2f8b5a2fc0f2", "generation_result": [{"code": "def softmax_unet(\n    input_tensor: tf.Tensor, instruments: Iterable[str], params: Dict = {}\n) -> Dict\n\t\n    logit_masks = []\n    for instrument in instruments:\n        logit_mask = apply_unet(input_tensor, instrument, params)\n        logit_masks.append(logit_mask)\n    logit_mask_list = tf.stack(logit_masks, axis=0)\n    softmax_mask = Softmax(axis=0)(logit_mask_list)\n    output = {}\n    for instrument, mask in zip(instruments, softmax_mask):\n        output[instrument] = Multiply()([input_tensor, mask])\n    return output\n\n", "CodeBleu_score": 29.00095365630862}, {"code": "def softmax_unet(\n    input_tensor: tf.Tensor, instruments: Iterable[str], params: Dict = {}\n) -> Dict\n\t\n    logit_masks = []\n    for instrument in instruments:\n        logit_mask = apply_unet(input_tensor, instrument, params)\n        logit_masks.append(logit_mask)\n    logit_mask_list = tf.stack(logit_masks, axis=0)\n    logit_mask_list = tf.transpose(logit_mask_list, [1, 0, 2, 3])\n    logit_mask_list = tf.unstack(logit_mask_list, axis=0)\n    logit_mask_list = [tf.expand_dims(mask, axis=0) for mask in logit_mask_list]\n    logit_mask_list = [tf.expand_dims(mask, axis=0) for mask in logit_mask_list]\n    logit_mask_list = [tf.expand_dims(mask, axis=0) for mask in logit_mask_list]\n    logit_mask_list = [tf.expand_dims(mask, axis=0) for mask in logit_mask_list]\n    logit_mask_list = [tf.expand_dims(mask, axis=0) for mask in logit_mask_list]\n    logit_mask_list = [tf.expand_dims(mask, axis=0) for mask in logit_mask_list]\n    logit_mask_list = [tf.expand_dims(mask, axis=0) for mask in logit_mask_list]\n    logit_mask_list = [tf.expand_dims(mask, axis=0) for mask in logit_mask_list]\n    logit_mask_list = [tf.expand_dims(mask, axis=0) for mask in logit_mask_list]\n    logit_mask_list = [tf.expand_dims(mask, axis=0) for mask in logit_mask_list]\n    logit_mask_list = [tf.expand_dims(mask, axis=0) for mask in logit_mask_list]\n    logit_mask_list = [tf.expand_dims(mask, axis=0) for mask in logit_mask_list]\n    logit_mask_list = [tf.expand_dims(mask, axis", "CodeBleu_score": 24.18390006642386}, {"code": "def softmax_unet(\n    input_tensor: tf.Tensor, instruments: Iterable[str], params: Dict = {}\n) -> Dict\n\t\n    logit_mask_list = []\n    for instrument in instruments:\n        logit_mask = apply_unet(input_tensor, instrument, params)\n        logit_mask_list.append(logit_mask)\n    output_tensor = tf.nn.softmax(tf.stack(logit_mask_list, axis=0), axis=0)\n    output_dict = {}\n    for instrument, logit_mask in zip(instruments, logit_mask_list):\n        output_dict[instrument] = tf.math.multiply(input_tensor, logit_mask)\n    return output_dict\n\n", "CodeBleu_score": 31.788149821758765}]}
{"instance_id": "400b19ce8520", "generation_result": [{"code": "def compute_file_checksum(path)\n\t\n    with open(path, \"rb\") as f:\n        # Initialize the SHA-256 hash object\n        sha256_hash = hashlib.sha256()\n\n        # Read and update hash in chunks of 4K\n        for byte_block in iter(lambda: f.read(4096), b\"\"):\n            sha256_hash.update(byte_block)\n\n        # Return the hexadecimal representation of the hash\n        return sha256_hash.hexdigest()\n\n", "CodeBleu_score": 58.42785093621599}, {"code": "def compute_file_checksum(path)\n\t\n    sha256 = hashlib.sha256()\n    with open(path, \"rb\") as f:\n        for chunk in iter(lambda: f.read(4096), b\"\"):\n            sha256.update(chunk)\n    return sha256.hexdigest()\n", "CodeBleu_score": 62.67303339474224}, {"code": "def compute_file_checksum(path)\n\t\n    # Create the hash object\n    sha256_hash = hashlib.sha256()\n\n    # Open the file to read it in chunks\n    with open(path, \"rb\") as f:\n        # Read and update hash string value in blocks of 4K\n        for byte_block in iter(lambda: f.read(4096), b\"\"):\n            sha256_hash.update(byte_block)\n\n    # Return the hexadecimal representation of the hash\n    return sha256_hash.hexdigest()\n", "CodeBleu_score": 52.714058032433776}]}
{"instance_id": "5f2e9c6e15f1", "generation_result": [{"code": "def checksum(self, name: str) -> str\n\t\n    url = f\"{self.host}/{self.repository}/{self.release_path}/{self.checksum_index}\"\n    response = httpx.get(url)\n    response.raise_for_status()\n    checksums = response.json()\n    if name not in checksums:\n        raise ValueError(f\"No checksum found for model {name}\")\n    return checksums[name]\n", "CodeBleu_score": 39.681613083765164}, {"code": "def checksum(self, name: str) -> str\n\t\n    url = f\"{self.host}/{self.repository}/{self.release_path}/{self.checksum_index}\"\n    response = httpx.get(url)\n    response.raise_for_status()\n    index = response.json()\n    if name not in index:\n        raise ValueError(f\"Model {name} not found in index\")\n    return index[name]\n", "CodeBleu_score": 36.418427243737774}, {"code": "def checksum(self, name: str) -> str\n\t\n    url = f\"{self.host}/{self.repository}/{self.release_path}/{self.checksum_index}\"\n    response = httpx.get(url)\n    response.raise_for_status()\n    index = response.json()\n    if name not in index:\n        raise ValueError(f\"Model {name} not found in index.\")\n    return index[name]\n", "CodeBleu_score": 36.46374963857735}]}
{"instance_id": "9312c8b6032e", "generation_result": [{"code": "def download(self, name: str, path: str) -> None:\n\t\n    url = self.get_url(name)\n    logger.info(f\"Downloading {name} from {url}\")\n    with client.stream(\"GET\", url) as response:\n        response.raise_for_status()\n        with NamedTemporaryFile() as temp_file:\n            for chunk in response.iter_raw():\n                temp_file.write(chunk)\n            temp_file.flush()\n            logger.info(f\"Validating {name}\")\n            if compute_file_checksum(temp_file.name) != self.checksums[name]:\n                raise IOError(f\"Checksum mismatch for {name}\")\n            logger.info(f\"Extracting {name}\")\n            with tarfile.open(temp_file.name) as tar:\n                tar.extractall(path)\n            os.unlink(temp_file.name)\n\n", "CodeBleu_score": 35.82475864169674}, {"code": "def download(self, name: str, path: str) -> None:\n\t\n    url = self.get_download_url(name)\n    logger.info(f\"Downloading model {name} from {url}\")\n    with client.stream(\"GET\", url) as response:\n        response.raise_for_status()\n        with NamedTemporaryFile() as temp_file:\n            for data in response.iter_raw():\n                temp_file.write(data)\n            temp_file.flush()\n            temp_file.seek(0)\n            checksum = compute_file_checksum(temp_file.name)\n            if checksum != self.checksum(name):\n                raise IOError(\n                    f\"Checksum for {name} does not match expected checksum. \"\n                    f\"The file may be corrupted.\"\n                )\n            logger.info(f\"Checksum for {name} matches expected checksum\")\n            logger.info(f\"Extracting model {name} to {path}\")\n            with tarfile.open(temp_file.name, \"r:gz\") as tar:\n                tar.extractall(path=path)\n            os.unlink(temp_file.name)\n            logger.info(f\"Extraction of model {name} complete\")\n", "CodeBleu_score": 46.05363362051515}, {"code": "def download(self, name: str, path: str) -> None:\n\t\n    url = self.get_download_url(name)\n    logger.info(f\"Downloading {name} from {url}\")\n    with client.stream(\"GET\", url) as response:\n        response.raise_for_status()\n        with NamedTemporaryFile(delete=False) as stream:\n            for chunk in response.iter_raw():\n                stream.write(chunk)\n            checksum = compute_file_checksum(stream.name)\n    if checksum != self.checksum(name):\n        raise IOError(\n            f\"Checksum mismatch for {name}. Expected {self.checksum(name)}, got {checksum}\"\n        )\n    logger.info(f\"Extracting {name} to {path}\")\n    with tarfile.open(stream.name, \"r:gz\") as tar:\n        tar.extractall(path=path)\n    os.unlink(stream.name)\n", "CodeBleu_score": 42.024319790682064}]}
{"instance_id": "ee9493f37d77", "generation_result": [{"code": "def load_configuration(descriptor: str) -> Dict\n\t\n    if descriptor.startswith(\"spleeter:\"):\n        return json.loads(pkgutil.get_data(\"spleeter\", \"/\".join((\"resources\", \"configuration\", descriptor[9:] + \".json\"))).decode(\"utf-8\"))\n    if loader.is_resource(descriptor):\n        return json.loads(loader.get_resource_string(descriptor).decode(\"utf-8\"))\n    if loader.exists(descriptor):\n        with open(descriptor, \"r\") as file:\n            return json.load(file)\n    raise SpleeterError(f\"Configuration file not found: {descriptor}\")\n\n", "CodeBleu_score": 38.27021543962131}, {"code": "def load_configuration(descriptor: str) -> Dict\n\t\n    if descriptor.startswith(\"json://\"):\n        return json.loads(loader.get_resource(descriptor[7:]).decode(\"utf-8\"))\n    elif os.path.exists(descriptor):\n        with open(descriptor, \"r\") as file:\n            return json.load(file)\n    else:\n        raise SpleeterError(\n            \"Configuration file not found: {}\".format(descriptor))\n", "CodeBleu_score": 30.33845306086275}, {"code": "def load_configuration(descriptor: str) -> Dict\n\t\n    if descriptor.startswith(\"resource:\"):\n        name = descriptor[len(\"resource:\") :]\n        if not loader.is_resource(name):\n            raise SpleeterError(\n                f\"Configuration {descriptor} not found in the package resources.\"\n            )\n        return json.load(loader.open_text(name))\n    elif os.path.exists(descriptor):\n        with open(descriptor, \"r\") as stream:\n            return json.load(stream)\n    raise SpleeterError(f\"Configuration {descriptor} not found.\")\n\n", "CodeBleu_score": 54.3339117897087}]}
{"instance_id": "546134b67fbc", "generation_result": [{"code": "def sync_apply(\n    tensor_dict: Dict[str, tf.Tensor], func: Callable, concat_axis: int = 1\n) -> Dict[str, tf.Tensor]\n\t\n    if concat_axis not in [0, 1]:\n        raise ValueError(\n            f\"Concat axis must be either 0 or 1, but got {concat_axis}.\"\n        )\n    concat_axis = concat_axis - 1\n    concat_tensor = tf.concat(list(tensor_dict.values()), concat_axis)\n    processed_tensor = func(concat_tensor)\n    processed_tensor_list = tf.split(\n        processed_tensor,\n        [tensor.shape[concat_axis] for tensor in tensor_dict.values()],\n        concat_axis,\n    )\n    return {\n        key: value\n        for key, value in zip(tensor_dict.keys(), processed_tensor_list)\n    }\n\n", "CodeBleu_score": 43.32565722347518}, {"code": "def sync_apply(\n    tensor_dict: Dict[str, tf.Tensor], func: Callable, concat_axis: int = 1\n) -> Dict[str, tf.Tensor]\n\t\n    concat_axis = 0 if concat_axis == 0 else 1\n    if concat_axis not in [0, 1]:\n        raise NotImplementedError(\n            \"The concatenation axis must be either 0 or 1, \"\n            f\"but got {concat_axis} instead.\"\n        )\n    values = tensor_dict.values()\n    concat_tensor = tf.concat(values, axis=concat_axis)\n    processed_tensor = func(concat_tensor)\n    processed_tensor_list = tf.split(\n        processed_tensor, len(tensor_dict), axis=concat_axis\n    )\n    return dict(zip(tensor_dict.keys(), processed_tensor_list))\n\n", "CodeBleu_score": 38.3478053657187}, {"code": "def sync_apply(\n    tensor_dict: Dict[str, tf.Tensor], func: Callable, concat_axis: int = 1\n) -> Dict[str, tf.Tensor]\n\t\n    if concat_axis not in [0, 1]:\n        raise NotImplementedError\n    tensor_list = list(tensor_dict.values())\n    if concat_axis == 0:\n        tensor_list = tf.concat(tensor_list, axis=concat_axis)\n    else:\n        tensor_list = tf.concat(tensor_list, axis=concat_axis)\n    tensor_list = func(tensor_list)\n    if concat_axis == 0:\n        tensor_list = tf.split(tensor_list, len(tensor_dict.keys()), axis=0)\n    else:\n        tensor_list = tf.split(tensor_list, len(tensor_dict.keys()), axis=1)\n    tensor_dict = {\n        key: tensor_list[idx] for idx, key in enumerate(tensor_dict.keys())\n    }\n    return tensor_dict\n\n", "CodeBleu_score": 44.641843237954994}]}
{"instance_id": "df48e78d46d2", "generation_result": [{"code": "def from_float32_to_uint8(\n    tensor: tf.Tensor,\n    tensor_key: str = \"tensor\",\n    min_key: str = \"min\",\n    max_key: str = \"max\",\n) -> tf.Tensor\n\t\n    # Get the minimum and maximum values of the tensor.\n    min_value = tf.reduce_min(tensor)\n    max_value = tf.reduce_max(tensor)\n\n    # Scale the tensor values to the uint8 range.\n    tensor = tf.cast(\n        tf.round((tensor - min_value) / (max_value - min_value) * 255), tf.uint8\n    )\n\n    return {\n        tensor_key: tensor,\n        min_key: min_value,\n        max_key: max_value,\n    }\n\n", "CodeBleu_score": 44.421534977887596}, {"code": "def from_float32_to_uint8(\n    tensor: tf.Tensor,\n    tensor_key: str = \"tensor\",\n    min_key: str = \"min\",\n    max_key: str = \"max\",\n) -> tf.Tensor\n\t\n    tensor_min = tf.reduce_min(tensor)\n    tensor_max = tf.reduce_max(tensor)\n    tensor_scaled = tf.divide(\n        tf.subtract(tensor, tensor_min), tf.subtract(tensor_max, tensor_min)\n    )\n    tensor_scaled = tf.multiply(tensor_scaled, 255)\n    tensor_scaled = tf.cast(tensor_scaled, tf.uint8)\n    return {tensor_key: tensor_scaled, min_key: tensor_min, max_key: tensor_max}\n\n", "CodeBleu_score": 46.34627428223133}, {"code": "def from_float32_to_uint8(\n    tensor: tf.Tensor,\n    tensor_key: str = \"tensor\",\n    min_key: str = \"min\",\n    max_key: str = \"max\",\n) -> tf.Tensor\n\t\n    tensor_min = tf.reduce_min(tensor)\n    tensor_max = tf.reduce_max(tensor)\n    tensor_scaled = tf.divide(\n        tf.subtract(tensor, tensor_min), tf.subtract(tensor_max, tensor_min)\n    )\n    tensor_scaled = tf.multiply(tensor_scaled, 255)\n    tensor_scaled = tf.cast(tensor_scaled, tf.uint8)\n    return {\n        tensor_key: tensor_scaled,\n        min_key: tensor_min,\n        max_key: tensor_max,\n    }\n\n", "CodeBleu_score": 50.61171258770791}]}
{"instance_id": "7f479e5de492", "generation_result": [{"code": "def pad_and_partition(tensor: tf.Tensor, segment_len: int) -> tf.Tensor\n\t\n    pad_len = segment_len - (tf.shape(tensor)[0] % segment_len)\n    padded_tensor = tf.pad(tensor, [[0, pad_len], [0, 0]])\n    return tf.reshape(padded_tensor, [-1, segment_len, tf.shape(tensor)[1]])\n\n", "CodeBleu_score": 23.13776884750816}, {"code": "def pad_and_partition(tensor: tf.Tensor, segment_len: int) -> tf.Tensor\n\t\n    # Pad the tensor to make its length a multiple of the segment length\n    pad_len = tf.math.floormod(tf.shape(tensor)[0], segment_len)\n    padded_tensor = tf.pad(tensor, [[0, segment_len - pad_len], [0, 0]])\n    # Reshape the tensor into segments of the specified length\n    return tf.reshape(padded_tensor, [-1, segment_len])\n\n", "CodeBleu_score": 21.914527097639326}, {"code": "def pad_and_partition(tensor: tf.Tensor, segment_len: int) -> tf.Tensor\n\t\n    # pad to the nearest multiple of segment_len\n    pad_len = segment_len - (tf.size(tensor) % segment_len)\n    padded_tensor = tf.pad(tensor, [[0, pad_len]])\n    # reshape into segments of segment_len\n    return tf.reshape(padded_tensor, [-1, segment_len])\n\n", "CodeBleu_score": 17.743989157194413}]}
{"instance_id": "1789fae7253f", "generation_result": [{"code": "def pad_and_reshape(instr_spec, frame_length, F) -> Any\n\t\n    # Calculate the number of rows to pad\n    num_rows_to_pad = frame_length - tf.math.mod(tf.shape(instr_spec)[0], frame_length)\n    # Pad the input spectrogram\n    padded_instr_spec = tf.pad(instr_spec, tf.constant([[0, num_rows_to_pad], [0, 0]]))\n    # Calculate the number of columns to pad\n    num_cols_to_pad = frame_length - tf.math.mod(tf.shape(instr_spec)[1], frame_length)\n    # Pad the input spectrogram\n    padded_instr_spec = tf.pad(padded_instr_spec, tf.constant([[0, 0], [0, num_cols_to_pad]]))\n    # Reshape the padded spectrogram\n    reshaped_padded_instr_spec = tf.reshape(padded_instr_spec, [tf.shape(instr_spec)[0] + num_rows_to_pad, -1, F])\n    return reshaped_padded_instr_spec\n\n", "CodeBleu_score": 21.619768743666956}, {"code": "def pad_and_reshape(instr_spec, frame_length, F) -> Any\n\t\n    # Calculate the number of extra rows needed for padding\n    pad_length = frame_length - tf.math.mod(tf.shape(instr_spec)[0], frame_length)\n    # Create an extension of zeros\n    pad_spec = tf.zeros((pad_length, tf.shape(instr_spec)[1]))\n    # Concatenate the extension to the original spectrogram along the frequency axis\n    instr_spec = tf.concat([instr_spec, pad_spec], axis=0)\n    # Reshape the extended spectrogram by merging the first two dimensions\n    instr_spec = tf.reshape(instr_spec, (F, -1, frame_length))\n    return instr_spec\n\n", "CodeBleu_score": 18.363607006257595}, {"code": "def pad_and_reshape(instr_spec, frame_length, F) -> Any\n\t\n    # Calculate the number of extra rows needed for padding\n    pad_size = tf.math.maximum(frame_length - tf.shape(instr_spec)[0], 0)\n    # Create an extension of zeros\n    pad = tf.zeros([pad_size, tf.shape(instr_spec)[1], F])\n    # Concatenate the extension to the original spectrogram\n    instr_spec_padded = tf.concat([instr_spec, pad], 0)\n    # Reshape the extended spectrogram\n    instr_spec_padded = tf.reshape(instr_spec_padded, [tf.shape(instr_spec_padded)[0] * tf.shape(instr_spec_padded)[1], F])\n    return instr_spec_padded\n\n", "CodeBleu_score": 18.04881731856706}]}
{"instance_id": "8a4f4658d97c", "generation_result": [{"code": "def dataset_from_csv(csv_path: str, **kwargs) -> Any\n\t\n    df = read_csv(csv_path, **kwargs)\n    return from_tensor_slices(df.values)\n", "CodeBleu_score": 21.53898778813552}, {"code": "def dataset_from_csv(csv_path: str, **kwargs) -> Any\n\t\n    df = read_csv(csv_path, **kwargs)\n    return from_tensor_slices(tuple(df[col].values for col in df.columns))\n\n", "CodeBleu_score": 29.657369390153455}, {"code": "def dataset_from_csv(csv_path: str, **kwargs) -> Any\n\t\n    df = read_csv(csv_path, **kwargs)\n    return from_tensor_slices((df[col].values for col in df.columns))\n", "CodeBleu_score": 29.353960257390575}]}
{"instance_id": "9a16e81b182f", "generation_result": [{"code": "def check_tensor_shape(tensor_tf: tf.Tensor, target_shape: Any) -> bool\n\t\n    result = True\n    for i, length in enumerate(target_shape):\n        if length is not None:\n            result = tf.logical_and(result, tf.equal(tf.shape(tensor_tf)[i], length))\n    return result\n\n", "CodeBleu_score": 42.62285958199549}, {"code": "def check_tensor_shape(tensor_tf: tf.Tensor, target_shape: Any) -> bool\n\t\n    result = True\n    for dim, length in enumerate(target_shape):\n        if length is not None:\n            result = tf.logical_and(result, tf.equal(tf.shape(tensor_tf)[dim], length))\n    return result\n\n", "CodeBleu_score": 42.28173603768897}, {"code": "def check_tensor_shape(tensor_tf: tf.Tensor, target_shape: Any) -> bool\n\t\n    # initialize boolean result\n    result = True\n    # iterate over target shape dimensions\n    for i, dim in enumerate(target_shape):\n        # check if dimension is specified\n        if dim is not None:\n            # compare tensor's corresponding dimension with target length\n            result = tf.logical_and(result, tf.equal(tf.shape(tensor_tf)[i], dim))\n    return result\n\n", "CodeBleu_score": 40.96549122846467}]}
{"instance_id": "49d5d3084b3a", "generation_result": [{"code": "def get_training_dataset(\n    audio_params: Dict, audio_adapter: AudioAdapter, audio_path: str\n) -> Any\n\t\n    builder = DatasetBuilder(\n        audio_params,\n        audio_adapter,\n        chunk_duration=audio_params[\"chunk_duration\"],\n        random_seed=audio_params[\"random_seed\"],\n    )\n    builder.build(\n        train_csv=audio_params[\"train_csv\"],\n        cache_dir=audio_params[\"cache_dir\"],\n        batch_size=audio_params[\"batch_size\"],\n        num_chunks_per_song=audio_params[\"num_chunks_per_song\"],\n        augment_data=audio_params[\"augment_data\"],\n        cache_data=audio_params[\"cache_data\"],\n    )\n    return builder.get_dataset()\n\n", "CodeBleu_score": 40.18980478329744}, {"code": "def get_training_dataset(\n    audio_params: Dict, audio_adapter: AudioAdapter, audio_path: str\n) -> Any\n\t\n    builder = DatasetBuilder(\n        audio_params,\n        audio_adapter,\n        chunk_duration=audio_params[\"chunk_duration\"],\n        random_seed=audio_params[\"random_seed\"],\n    )\n    builder.build(\n        train_csv=audio_path,\n        cache_dir=audio_params[\"cache_dir\"],\n        batch_size=audio_params[\"batch_size\"],\n        num_chunks_per_song=audio_params[\"num_chunks_per_song\"],\n        augment=audio_params[\"augment\"],\n        cache=audio_params[\"cache\"],\n    )\n    return builder.dataset\n\n", "CodeBleu_score": 39.70308948894844}, {"code": "def get_training_dataset(\n    audio_params: Dict, audio_adapter: AudioAdapter, audio_path: str\n) -> Any\n\t\n    builder = DatasetBuilder(\n        audio_adapter=audio_adapter,\n        audio_params=audio_params,\n        chunk_duration=audio_params[\"chunk_duration\"],\n        random_seed=audio_params[\"random_seed\"],\n    )\n    builder.build(\n        train_csv=audio_path,\n        cache_dir=audio_params[\"cache_dir\"],\n        batch_size=audio_params[\"batch_size\"],\n        num_chunks_per_song=audio_params[\"num_chunks_per_song\"],\n        augment=audio_params[\"augment\"],\n        cache=audio_params[\"cache\"],\n    )\n    return builder.dataset\n\n", "CodeBleu_score": 34.71510737584721}]}
{"instance_id": "fe8bbbba06e3", "generation_result": [{"code": "def instruments(self) -> Any\n\t\n    # Initialize instrument dataset builders if they are not already created\n    if not hasattr(self, \"_instrument_builders\"):\n        self._instrument_builders = [\n            InstrumentDatasetBuilder(instrument=instrument, root_dir=self.root_dir)\n            for instrument in self.instruments\n        ]\n\n    yield from self._instrument_builders\n", "CodeBleu_score": 27.149148319521395}, {"code": "def instruments(self) -> Any\n\t\n    if not hasattr(self, \"instrument_builders\"):\n        self.instrument_builders = []\n        for instrument in self.instruments:\n            self.instrument_builders.append(InstrumentDatasetBuilder(instrument))\n    for instrument_builder in self.instrument_builders:\n        yield instrument_builder\n\n", "CodeBleu_score": 46.965939731050895}, {"code": "def instruments(self) -> Any\n\t\n    if self._instrument_builders is None:\n        self._instrument_builders = []\n        for instrument in self._instruments:\n            self._instrument_builders.append(InstrumentDatasetBuilder(instrument, self._market_data_store))\n    for builder in self._instrument_builders:\n        yield builder\n\n", "CodeBleu_score": 63.05463743190983}]}
{"instance_id": "438b57fd3413", "generation_result": [{"code": "def cache(self, dataset: Any, cache: Optional[str], wait: bool) -> Any\n\t\n    if cache:\n        logger.info(f\"Caching dataset to {cache}\")\n        time.sleep(wait)\n        os.makedirs(os.path.dirname(cache), exist_ok=True)\n        dataset.cache(cache)\n    return dataset\n\n", "CodeBleu_score": 30.783457971613277}, {"code": "def cache(self, dataset: Any, cache: Optional[str], wait: bool) -> Any\n\t\n    if cache:\n        if wait:\n            logger.info(f\"Waiting for the cache index file {cache} to exist...\")\n            while not os.path.exists(cache):\n                time.sleep(10)\n                logger.info(f\"Waiting for the cache index file {cache} to exist...\")\n        logger.info(f\"Caching dataset to {cache}\")\n        os.makedirs(os.path.dirname(cache), exist_ok=True)\n        dataset.cache(cache)\n    return dataset\n\n", "CodeBleu_score": 43.89201981456263}, {"code": "def cache(self, dataset: Any, cache: Optional[str], wait: bool) -> Any\n\t\n    if cache:\n        logger.info(f\"Caching dataset at {cache}\")\n        time.sleep(1)\n        os.makedirs(cache, exist_ok=True)\n        dataset.cache(cache)\n        if wait:\n            while not os.path.exists(f\"{cache}/_SUCCESS\"):\n                logger.info(f\"Cache not found at {cache}, sleeping for 10 seconds\")\n                time.sleep(10)\n    return dataset\n\n", "CodeBleu_score": 49.55921897887255}]}
{"instance_id": "8838a3a09a66", "generation_result": [{"code": "def create_estimator(params: Dict, MWF: bool) -> tf.Tensor\n\t\n    model_provider = ModelProvider(params)\n    model_dir = model_provider.get()\n    params['model_dir'] = model_dir\n    config = ConfigProto()\n    config.gpu_options.allow_growth = True\n    run_config = RunConfig().replace(\n        session_config=config, model_dir=model_dir)\n    estimator = tf.estimator.Estimator(\n        model_fn=model_provider.get_model_fn(MWF), params=params, config=run_config)\n    return estimator\n\n", "CodeBleu_score": 32.76845531709701}, {"code": "def create_estimator(params: Dict, MWF: bool) -> tf.Tensor\n\t\n    # Load model directory\n    model_provider = ModelProvider(params['model_dir'])\n    params = model_provider.update_params(params)\n    # Set up session config\n    session_config = ConfigProto()\n    session_config.gpu_options.allow_growth = True\n    # Set up run config\n    run_config = RunConfig(session_config=session_config)\n    # Initialize and return estimator\n    estimator = tf.estimator.Estimator(\n        model_fn=model_provider.get_model_fn(MWF),\n        model_dir=params['model_dir'],\n        params=params,\n        config=run_config)\n    return estimator\n\n", "CodeBleu_score": 36.782766290904135}, {"code": "def create_estimator(params: Dict, MWF: bool) -> tf.Tensor\n\t\n    model_provider = ModelProvider(params, MWF)\n    params = model_provider.get_params()\n    model_fn = model_provider.get_model_fn()\n    config = ConfigProto()\n    config.gpu_options.allow_growth = True\n    run_config = RunConfig().replace(\n        session_config=config,\n        model_dir=params[\"model_dir\"],\n        save_checkpoints_steps=params[\"save_checkpoints_steps\"],\n        save_summary_steps=params[\"save_summary_steps\"],\n    )\n    estimator = tf.estimator.Estimator(\n        model_fn=model_fn, params=params, config=run_config\n    )\n    return estimator\n\n", "CodeBleu_score": 36.35111191805977}]}
{"instance_id": "779c812bf2cb", "generation_result": [{"code": "def _separate_tensorflow(\n        self, waveform: np.ndarray, audio_descriptor: AudioDescriptor\n    ) -> Dict\n\t\n    if audio_descriptor.channels != 2:\n        waveform = to_stereo(waveform)\n    prediction_generator = self._get_prediction_generator(\n        waveform, audio_descriptor\n    )\n    self._update_data(prediction_generator, waveform, audio_descriptor)\n    prediction = next(prediction_generator)\n    prediction.pop(\"audio_id\")\n    return prediction\n\n", "CodeBleu_score": 65.74441606175185}, {"code": "def _separate_tensorflow(\n        self, waveform: np.ndarray, audio_descriptor: AudioDescriptor\n    ) -> Dict\n\t\n    waveform = _convert_to_stereo(waveform)\n    prediction_generator = self._get_prediction_generator(\n        waveform, audio_descriptor\n    )\n    self.update_data(waveform, audio_descriptor)\n    prediction = next(prediction_generator)\n    prediction.pop(\"audio_id\")\n    return prediction\n\n", "CodeBleu_score": 46.61859604353057}, {"code": "def _separate_tensorflow(\n        self, waveform: np.ndarray, audio_descriptor: AudioDescriptor\n    ) -> Dict\n\t\n    if audio_descriptor.channels != 2:\n        waveform = convert_to_stereo(waveform)\n    self._get_prediction_generator(audio_descriptor)\n    self.prediction_generator.update(waveform, audio_descriptor)\n    prediction = self.prediction_generator.next()\n    prediction.pop(\"audio_id\")\n    return prediction\n\n", "CodeBleu_score": 52.3473167634729}]}
{"instance_id": "cf580cd2ed93", "generation_result": [{"code": "def compute_backbone_shapes(config, image_shape)\n\t\n    if callable(config.COMPUTE_BACKBONE_SHAPE):\n        return config.COMPUTE_BACKBONE_SHAPE(image_shape)\n    # Currently supports ResNet only\n    assert config.BACKBONE in [\"resnet50\", \"resnet101\"]\n    return np.array(\n        [int(math.ceil(image_shape[0] / stride)) for stride in config.BACKBONE_STRIDES]\n    )\n\n", "CodeBleu_score": 60.52210641506749}, {"code": "def compute_backbone_shapes(config, image_shape)\n\t\n    if callable(config.COMPUTE_BACKBONE_SHAPE):\n        return config.COMPUTE_BACKBONE_SHAPE(image_shape)\n    assert config.BACKBONE in [\"resnet50\", \"resnet101\"], \"backbone is not resnet50 or resnet101\"\n    return np.array(\n        [[int(math.ceil(image_shape[0] / stride)), int(math.ceil(image_shape[1] / stride))] for stride in config.BACKBONE_STRIDES])\n\n", "CodeBleu_score": 85.22761323802618}, {"code": "def compute_backbone_shapes(config, image_shape)\n\t\n    if callable(config.BACKBONE):\n        return config.COMPUTE_BACKBONE_SHAPE(image_shape)\n    assert config.BACKBONE in [\"resnet50\", \"resnet101\"], \"backbone is not resnet50 or resnet101\"\n    return np.array(\n        [[int(math.ceil(image_shape[0] / stride)),\n          int(math.ceil(image_shape[1] / stride))]\n         for stride in config.BACKBONE_STRIDES])\n\n", "CodeBleu_score": 88.72488418551895}]}
{"instance_id": "ebd61a2e59ca", "generation_result": [{"code": "def clip_boxes_graph(boxes, window)\n\t\n    # Split\n    wy1, wx1, wy2, wx2 = tf.split(window, 4)\n    y1, x1, y2, x2 = tf.split(boxes, 4, axis=1)\n    # Clip\n    y1 = tf.maximum(tf.minimum(y1, wy2), wy1)\n    x1 = tf.maximum(tf.minimum(x1, wx2), wx1)\n    y2 = tf.maximum(tf.minimum(y2, wy2), wy1)\n    x2 = tf.maximum(tf.minimum(x2, wx2), wx1)\n    clipped = tf.concat([y1, x1, y2, x2], axis=1)\n    clipped.set_shape((clipped.shape[0], 4))\n    return clipped\n\n", "CodeBleu_score": 56.01484333396114}, {"code": "def clip_boxes_graph(boxes, window)\n\t\n    # Split\n    wy1, wx1, wy2, wx2 = tf.split(window, 4)\n    y1, x1, y2, x2 = tf.split(boxes, 4, axis=1)\n    # Clip\n    y1 = tf.maximum(tf.minimum(y1, wy2), wy1)\n    x1 = tf.maximum(tf.minimum(x1, wx2), wx1)\n    y2 = tf.maximum(tf.minimum(y2, wy2), wy1)\n    x2 = tf.maximum(tf.minimum(x2, wx2), wx1)\n    clipped = tf.concat([y1, x1, y2, x2], axis=1, name=\"clipped_boxes\")\n    clipped.set_shape((clipped.shape[0], 4))\n    return clipped\n\n", "CodeBleu_score": 61.634432626045545}, {"code": "def clip_boxes_graph(boxes, window)\n\t\n    # Split\n    # Split the input boxes into upper left, upper right, lower left, and lower right corners.\n    upper_left, upper_right, lower_left, lower_right = tf.split(boxes, 4, axis=1)\n    # Clip\n    # Clip the upper left and lower right corners of the bounding box.\n    # The clip_by_value function takes the tensor we want to clip and the minimum and maximum values as inputs.\n    # The min_coord and max_coord tensors are used to create the clipped coordinates.\n    clip_upper_left = tf.clip_by_value(upper_left, window[0], window[1])\n    clip_lower_right = tf.clip_by_value(lower_right, window[0], window[1])\n    # Concat\n    # Concatenate the clipped coordinates.\n    return tf.concat([clip_upper_left, clip_lower_right], axis=1)\n", "CodeBleu_score": 11.18666625312373}]}
{"instance_id": "8e0411587a70", "generation_result": [{"code": "def rpn_graph(feature_map, anchors_per_location, anchor_stride)\n\t\n\n    # Shared convolutional base of the RPN\n    shared = KL.Conv2D(512, (3, 3), padding='same', activation='relu',\n                       strides=anchor_stride,\n                       name='rpn_conv_shared')(feature_map)\n\n    # Anchor Score. [batch, height, width, anchors per location * 2].\n    x = KL.Conv2D(2 * anchors_per_location, (1, 1), padding='valid',\n                  activation='linear', name='rpn_class_raw')(shared)\n\n    # Reshape to [batch, anchors, 2]\n    rpn_class_logits = KL.Lambda(\n        lambda t: tf.reshape(t, [tf.shape(t)[0], -1, 2]))(x)\n\n    # Softmax on last dimension of BG/FG.\n    rpn_probs = KL.Activation(\n        \"softmax\", name=\"rpn_class_xxx\")(rpn_class_logits)\n\n    # Bounding box refinement. [batch, H, W, anchors per location * depth]\n    # where depth is [x, y, log(w), log(h)]\n    x = KL.Conv2D(anchors_per_location * 4, (1, 1), padding=\"valid\",\n                  activation='linear', name='rpn_bbox_pred')(shared)\n\n    # Reshape to [batch, anchors, 4]\n    rpn_bbox = KL.Lambda(lambda t: tf.reshape(t, [tf.shape(t)[0], -1, 4]))(x)\n\n    return [rpn_class_logits, rpn_probs, rpn_bbox]\n\n", "CodeBleu_score": 85.99714523366548}, {"code": "def rpn_graph(feature_map, anchors_per_location, anchor_stride)\n\t\n    # Shared convolutional base of the RPN\n    shared = KL.Conv2D(512, (3, 3), padding='same', activation='relu',\n                       strides=anchor_stride,\n                       name='rpn_conv_shared')(feature_map)\n\n    # Anchor Score. [batch, height, width, anchors per location * 2].\n    x = KL.Conv2D(2 * anchors_per_location, (1, 1), padding='valid',\n                  activation='linear', name='rpn_class_raw')(shared)\n\n    # Reshape to [batch, anchors, 2]\n    rpn_class_logits = KL.Lambda(\n        lambda t: tf.reshape(t, [tf.shape(t)[0], -1, 2]))(x)\n\n    # Softmax on last dimension of BG/FG.\n    rpn_probs = KL.Activation(\n        \"softmax\", name=\"rpn_class_xxx\")(rpn_class_logits)\n\n    # Bounding box refinement. [batch, H, W, anchors per location * depth]\n    # where depth is [x, y, log(w), log(h)]\n    x = KL.Conv2D(anchors_per_location * 4, (1, 1), padding=\"valid\",\n                  activation='linear', name='rpn_bbox_pred')(shared)\n\n    # Reshape to [batch, anchors, 4]\n    rpn_bbox = KL.Lambda(lambda t: tf.reshape(t, [tf.shape(t)[0], -1, 4]))(x)\n\n    return [rpn_class_logits, rpn_probs, rpn_bbox]\n\n", "CodeBleu_score": 85.99714523366548}, {"code": "def rpn_graph(feature_map, anchors_per_location, anchor_stride)\n\t\n    # Shared convolutional base of the RPN\n    shared = KL.Conv2D(512, (3, 3), padding='same', activation='relu',\n                       strides=anchor_stride,\n                       name='rpn_conv_shared')(feature_map)\n\n    # Anchor Score. [batch, height, width, anchors per location * 2].\n    x = KL.Conv2D(2 * anchors_per_location, (1, 1), padding='valid',\n                  activation='linear', name='rpn_class_raw')(shared)\n\n    # Reshape to [batch, anchors, 2]\n    rpn_class_logits = KL.Lambda(\n        lambda t: tf.reshape(t, [tf.shape(t)[0], -1, 2]))(x)\n\n    # Softmax on last dimension of BG/FG.\n    rpn_probs = KL.Activation(\n        \"softmax\", name=\"rpn_class_xxx\")(rpn_class_logits)\n\n    # Bounding box refinement. [batch, H, W, anchors per location, depth]\n    # where depth is [x, y, log(w), log(h)]\n    x = KL.Conv2D(anchors_per_location * 4, (1, 1), padding=\"valid\",\n                  activation='linear', name='rpn_bbox_pred')(shared)\n\n    # Reshape to [batch, anchors, 4]\n    rpn_bbox = KL.Lambda(lambda t: tf.reshape(t, [tf.shape(t)[0], -1, 4]))(x)\n\n    return [rpn_class_logits, rpn_probs, rpn_bbox]\n\n", "CodeBleu_score": 85.99714523366548}]}
{"instance_id": "70777c9261d8", "generation_result": [{"code": "def smooth_l1_loss(y_true, y_pred)\n\t\n    diff = K.abs(y_true - y_pred)\n    less_than_one = K.cast(K.less(diff, 1.0), \"float32\")\n    loss = (less_than_one * 0.5 * diff**2) + (1 - less_than_one) * (diff - 0.5)\n    return loss\n\n", "CodeBleu_score": 60.928513203413225}, {"code": "def smooth_l1_loss(y_true, y_pred)\n\t\n    diff = K.abs(y_true - y_pred)\n    less_than_one = K.cast(K.less(diff, 1.0), \"float32\")\n    loss = (less_than_one * 0.5 * diff**2) + (1 - less_than_one) * (diff - 0.5)\n    return loss\n\n", "CodeBleu_score": 60.928513203413225}, {"code": "def smooth_l1_loss(y_true, y_pred)\n\t\n    diff = K.abs(y_true - y_pred)\n    less_mask = K.cast(K.less(diff, 1.0), \"float32\")\n    loss = (less_mask * 0.5 * diff**2) + (1 - less_mask) * (diff - 0.5)\n    return K.sum(loss)\n\n", "CodeBleu_score": 47.1007114086757}]}
{"instance_id": "0e747b604087", "generation_result": [{"code": "def build_model(x_train, num_classes)\n\t\n    tf.reset_default_graph()\n    inputs = KL.Input(shape=x_train.shape[1:])\n    x = KL.Conv2D(filters=16, kernel_size=(3, 3), activation='relu')(inputs)\n    x = KL.Conv2D(filters=16, kernel_size=(3, 3), activation='relu')(x)\n    x = KL.MaxPooling2D(pool_size=(2, 2))(x)\n    x = KL.Flatten()(x)\n    x = KL.Dense(units=128, activation='relu')(x)\n    predictions = KL.Dense(units=num_classes, activation='softmax')(x)\n    model = KM.Model(inputs=inputs, outputs=predictions)\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n    return model\n\n", "CodeBleu_score": 32.754040851583206}, {"code": "def build_model(x_train, num_classes)\n\t\n    tf.reset_default_graph()\n    model_input = KL.Input(shape=x_train.shape[1:])\n    x = KL.Conv2D(filters=32, kernel_size=(3, 3), strides=(1, 1), padding='same', activation='relu')(model_input)\n    x = KL.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same')(x)\n    x = KL.Conv2D(filters=64, kernel_size=(3, 3), strides=(1, 1), padding='same', activation='relu')(x)\n    x = KL.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same')(x)\n    x = KL.Flatten()(x)\n    x = KL.Dense(units=128, activation='relu')(x)\n    model_output = KL.Dense(units=num_classes, activation='softmax')(x)\n    model = KM.Model(inputs=model_input, outputs=model_output)\n    model.compile(optimizer=tf.train.AdamOptimizer(), loss='categorical_crossentropy', metrics=['accuracy'])\n    return model\n\n", "CodeBleu_score": 35.47929371239075}, {"code": "def build_model(x_train, num_classes)\n\t\n    tf.reset_default_graph()\n    input_layer = KL.Input(shape=x_train.shape[1:])\n    conv1 = KL.Conv2D(32, (3, 3), activation='relu')(input_layer)\n    conv2 = KL.Conv2D(32, (3, 3), activation='relu')(conv1)\n    max_pool = KL.MaxPooling2D((2, 2))(conv2)\n    flatten = KL.Flatten()(max_pool)\n    dense1 = KL.Dense(128, activation='relu')(flatten)\n    output = KL.Dense(num_classes, activation='softmax')(dense1)\n    model = KM.Model(inputs=input_layer, outputs=output)\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model\n\n", "CodeBleu_score": 29.381773305038227}]}
{"instance_id": "813e3e2844f4", "generation_result": [{"code": "def extract_bboxes(mask)\n\t\n    # Extract bounding boxes from a mask tensor\n    boxes = np.zeros([mask.shape[-1], 4], dtype=np.int32)\n    for i in range(mask.shape[-1]):\n        m = mask[:, :, i]\n        # Bounding box.\n        horizontal_indicies = np.where(np.any(m, axis=0))[0]\n        vertical_indicies = np.where(np.any(m, axis=1))[0]\n        if horizontal_indicies.shape[0]:\n            x1, x2 = horizontal_indicies[[0, -1]]\n            y1, y2 = vertical_indicies[[0, -1]]\n            # x2 and y2 should not be part of the box. Increment by 1.\n            x2 += 1\n            y2 += 1\n        else:\n            # No mask for this layer, therefore bounding box is zeros\n            x1, x2, y1, y2 = 0, 0, 0, 0\n        boxes[i] = np.array([y1, x1, y2, x2])\n    return boxes.astype(np.int32)\n", "CodeBleu_score": 79.08962759125664}, {"code": "def extract_bboxes(mask)\n\t\n    # Bounding boxes\n    bboxes = np.zeros([mask.shape[-1], 4], dtype=np.int32)\n\n    # Loop through layers of mask\n    for i in range(mask.shape[-1]):\n        # Find horizontal and vertical indices where mask is present\n        horizontal_indicies = np.where(mask[:, :, i] != 0)[0]\n        vertical_indicies = np.where(mask[:, :, i] != 0)[1]\n\n        # Find left, top, right, and bottom pixel values\n        if horizontal_indicies.shape[0]:\n            x1, x2 = horizontal_indicies[[0, -1]]\n            y1, y2 = vertical_indicies[[0, -1]]\n            # x2 and y2 should not be part of the box. Increment by 1.\n            x2 += 1\n            y2 += 1\n        else:\n            # No mask for this layer\n            x1, x2, y1, y2 = 0, 0, 0, 0\n        bboxes[i] = np.array([y1, x1, y2, x2])\n    return bboxes.astype(np.int32)\n\n", "CodeBleu_score": 64.723183300235}, {"code": "def extract_bboxes(mask)\n\t\n    boxes = np.zeros([mask.shape[-1], 4], dtype=np.int32)\n    for i in range(mask.shape[-1]):\n        m = mask[:, :, i]\n        # Bounding box.\n        horizontal_indicies = np.where(np.any(m, axis=0))[0]\n        vertical_indicies = np.where(np.any(m, axis=1))[0]\n        if horizontal_indicies.shape[0]:\n            x1, x2 = horizontal_indicies[[0, -1]]\n            y1, y2 = vertical_indicies[[0, -1]]\n            # x2 and y2 should not be part of the box. Increment by 1.\n            x2 += 1\n            y2 += 1\n        else:\n            # No mask for this layer, return zeros.\n            x1, x2, y1, y2 = 0, 0, 0, 0\n        boxes[i] = np.array([y1, x1, y2, x2])\n    return boxes.astype(np.int32)\n\n", "CodeBleu_score": 79.08962759125664}]}
{"instance_id": "7ad5d430d6ed", "generation_result": [{"code": "def compute_iou(box, boxes, box_area, boxes_area)\n\t\n    # Calculate intersection areas\n    y1 = np.maximum(box[0], boxes[:, 0])\n    y2 = np.minimum(box[2], boxes[:, 2])\n    x1 = np.maximum(box[1], boxes[:, 1])\n    x2 = np.minimum(box[3], boxes[:, 3])\n    intersection = np.maximum(x2 - x1, 0) * np.maximum(y2 - y1, 0)\n    union = box_area + boxes_area[:] - intersection[:]\n    iou = intersection / union\n    return iou\n\n", "CodeBleu_score": 58.663259083089194}, {"code": "def compute_iou(box, boxes, box_area, boxes_area)\n\t\n    # Calculate intersection areas\n    y1 = np.maximum(box[0], boxes[:, 0])\n    y2 = np.minimum(box[2], boxes[:, 2])\n    x1 = np.maximum(box[1], boxes[:, 1])\n    x2 = np.minimum(box[3], boxes[:, 3])\n    intersection = np.maximum(x2 - x1, 0) * np.maximum(y2 - y1, 0)\n    union = box_area + boxes_area[:] - intersection[:]\n    iou = intersection / union\n    return iou\n\n", "CodeBleu_score": 58.663259083089194}, {"code": "def compute_iou(box, boxes, box_area, boxes_area)\n\t\n    # Calculate intersection areas\n    y1 = np.maximum(box[0], boxes[:, 0])\n    y2 = np.minimum(box[2], boxes[:, 2])\n    x1 = np.maximum(box[1], boxes[:, 1])\n    x2 = np.minimum(box[3], boxes[:, 3])\n    intersection = np.maximum(x2 - x1, 0) * np.maximum(y2 - y1, 0)\n    union = box_area + boxes_area[:] - intersection[:]\n    iou = intersection / union\n    return iou\n\n", "CodeBleu_score": 58.663259083089194}]}
{"instance_id": "15941b852859", "generation_result": [{"code": "def compute_overlaps(boxes1, boxes2)\n\t\n    # Areas of anchors and GT boxes\n    area1 = (boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1])\n    area2 = (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1])\n\n    # Compute overlaps to generate matrix [boxes1 count, boxes2 count]\n    # Each cell contains the IoU value.\n    overlaps = np.zeros((boxes1.shape[0], boxes2.shape[0]))\n    for i in range(overlaps.shape[1]):\n        box2 = boxes2[i]\n        overlaps[:, i] = compute_iou(box2, boxes1, area2[i], area1)\n    return overlaps\n\n", "CodeBleu_score": 62.2853282032944}, {"code": "def compute_overlaps(boxes1, boxes2)\n\t\n    # 1. Calculate the area of each box in both sets\n    area1 = (boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1])\n    area2 = (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1])\n\n    # 2. Initialize the overlap matrix\n    overlaps = np.zeros((boxes1.shape[0], boxes2.shape[0]))\n\n    # 3. Calculate the IoU of each pair of boxes\n    for i in range(overlaps.shape[1]):\n        box2 = boxes2[i]\n        overlaps[:, i] = compute_iou(box2, boxes1, area2[i], area1)\n\n    return overlaps\n\n", "CodeBleu_score": 62.2853282032944}, {"code": "def compute_overlaps(boxes1, boxes2)\n\t\n    # Areas of anchors and GT boxes\n    area1 = (boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1])\n    area2 = (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1])\n\n    # Compute overlaps to generate matrix [boxes1 count, boxes2 count]\n    # Each cell contains the IoU value.\n    overlaps = np.zeros((boxes1.shape[0], boxes2.shape[0]))\n    for i in range(overlaps.shape[1]):\n        box2 = boxes2[i]\n        overlaps[:, i] = compute_iou(box2, boxes1, area2[i], area1)\n    return overlaps\n\n", "CodeBleu_score": 62.2853282032944}]}
{"instance_id": "597533722ce3", "generation_result": [{"code": "def load_image(self, image_id):\n\t\n    # Load image\n    image = skimage.io.imread(self.image_info[image_id]['path'])\n    # If grayscale. Convert to RGB for consistency.\n    if image.ndim != 3:\n        image = skimage.color.gray2rgb(image)\n    # If has an alpha channel, remove it for consistency\n    if image.shape[-1] == 4:\n        image = image[..., :3]\n    return image\n\n", "CodeBleu_score": 100.0}, {"code": "def load_image(self, image_id):\n\t\n    image_path = self.image_info[image_id]['path']\n    image = skimage.io.imread(image_path)\n    image = gray2rgb(image)\n    if image.shape[-1] == 4:\n        image = image[..., :3]\n    return image\n", "CodeBleu_score": 70.10910610223648}, {"code": "def load_image(self, image_id):\n\t\n    image = self.image_info[image_id]\n    # If grayscale. Convert to RGB for consistency.\n    if image.channels == 1:\n        image = image.convert('RGB')\n    # If has an alpha channel, remove it for consistency\n    if image.mode == 'RGBA':\n        image = image.convert('RGB')\n    return image\n\n", "CodeBleu_score": 32.017120823407424}]}
{"instance_id": "9b852997ca05", "generation_result": [{"code": "def display_images(images, titles=None, cols=4, cmap=None, norm=None,\n                   interpolation=None)\n\t\n    # Set default titles if none are provided\n    if titles is None:\n        titles = ['Image (%d)' % i for i in range(1, len(images) + 1)]\n    # Calculate the number of rows needed\n    rows = len(images) // cols + 1\n    # Create figure and set display parameters\n    fig = plt.figure()\n    fig.set_size_inches(14, 10)\n    for n, (image, title) in enumerate(zip(images, titles)):\n        a = fig.add_subplot(rows, cols, n + 1)\n        if image.ndim == 2:\n            plt.gray()\n        plt.imshow(image, cmap=cmap, norm=norm, interpolation=interpolation)\n        a.set_title(title)\n    plt.show()\n\n", "CodeBleu_score": 47.71542684209098}, {"code": "def display_images(images, titles=None, cols=4, cmap=None, norm=None,\n                   interpolation=None)\n\t\n    # Set default values if none are provided.\n    titles = titles if titles is not None else [\"\"] * len(images)\n    rows = len(images) // cols + 1\n\n    # Create a figure.\n    fig = plt.figure()\n\n    # Adjust layout to make room for titles.\n    fig.subplots_adjust(top=0.85, bottom=0, left=0, right=1, hspace=0.05, wspace=0.05)\n\n    # Iterate over the images and titles.\n    for i, (image, title) in enumerate(zip(images, titles)):\n        a = fig.add_subplot(rows, cols, i + 1)\n        plt.imshow(image, cmap=cmap, norm=norm, interpolation=interpolation)\n        plt.axis(\"off\")\n        a.set_title(title)\n\n    # Show the figure.\n    plt.show()\n\n", "CodeBleu_score": 51.00583699460449}, {"code": "def display_images(images, titles=None, cols=4, cmap=None, norm=None,\n                   interpolation=None)\n\t\n    # Set default values for parameters\n    if titles is None:\n        titles = ['Image (%d)' % i for i in range(1, len(images) + 1)]\n    # Set the number of rows and columns\n    rows = len(images) // cols + 1\n    # Create the figure and set the title\n    fig = plt.figure()\n    fig.suptitle('Images')\n    # Iterate over the images and titles, displaying the images in a subplot\n    for n, (image, title) in enumerate(zip(images, titles)):\n        a = fig.add_subplot(rows, cols, n + 1)\n        if image.ndim == 2:\n            plt.gray()\n        plt.imshow(image, cmap=cmap, norm=norm, interpolation=interpolation)\n        a.set_title(title)\n        plt.axis('off')\n    # Show the figure\n    plt.show()\n", "CodeBleu_score": 48.98395474106812}]}
