{"instance_id": "1f9f4d85e825", "generation_result": [{"code": "def load_model_from_config(self, half_attention)\n\t\n\n    # Load model from cache if available\n    if self.cache_model:\n        self.load_model_from_cache()\n\n    # Load model from path if not loaded from cache\n    if not self.loaded_model:\n        self.load_model_from_path()\n\n    # Configure model\n    self.configure_model()\n\n    # Move model to device\n    self.model.to(self.device)\n\n    # Set model to evaluation mode\n    self.model.eval()\n\n    # Set model to half precision if specified\n    if half_attention:\n        self.model.half()\n\n    # Set memory format\n    self.model.to(memory_format=torch.channels_last)\n\n    # Cache model if not loaded from cache\n    if not self.loaded_model:\n        self.cache_model_state_dict()\n\n    return self.model\n\n", "CodeBleu_score": 13.112353318055481}, {"code": "def load_model_from_config(self, half_attention)\n\t\n    # Check if a cached model is available and valid, loading it if so.\n    # Otherwise, load the model from the specified path.\n    if self.model_path in self.cached_models:\n        if self.cached_models[self.model_path] is not None:\n            print(f\"Loading model from cache: {self.model_path}\")\n            model = self.cached_models[self.model_path]\n            self.model_config = model.config\n        else:\n            print(f\"Cached model not found: {self.model_path}\")\n            model = None\n    else:\n        print(f\"Loading model from path: {self.model_path}\")\n        model = None\n    if model is None:\n        # Load model from file.\n        model = instantiate_from_config(self.model_config)\n        # Handle different file formats.\n        if self.model_path.endswith(\".safetensors\"):\n            model.load_state_dict(torch.load(self.model_path))\n        else:\n            model = load_file(self.model_path)\n        # Configure the model.\n        model.eval()\n        # Move the model to the appropriate device.\n        model = model.to(self.device)\n        # Optionally use half-precision attention.\n        if half_attention:\n            model.half_attention()\n        # Optionally optimize memory format.\n        if self.memory_format:\n            model.to_memory_format(self.memory_format)\n        # Cache the loaded model for future use.\n        self.cached_models[self.model_path] = model\n    return model\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 36.148937259255966}, {"code": "def load_model_from_config(self, half_attention)\n\t\n    model_path = self.model_path\n    model_name = self.model_name\n    model_cache_path = self.model_cache_path\n    model_cache_name = self.model_cache_name\n    model_config_path = self.model_config_path\n    model_config_name = self.model_config_name\n    model_cache_dir = self.model_cache_dir\n    model_config_dir = self.model_config_dir\n    model_cache_file = self.model_cache_file\n    model_config_file = self.model_config_file\n    model_cache_exists = self.model_cache_exists\n    model_config_exists = self.model_config_exists\n    model_cache_valid = self.model_cache_valid\n    model_cache_load = self.model_cache_load\n    model_config_load = self.model_config_load\n    model_load = self.model_load\n    model_device = self.model_device\n    model_config = self.model_config\n    model_cache = self.model_cache\n    model = self.model\n    model_cache_dict = self.model_cache_dict\n    model_config_dict = self.model_config_dict\n    model_dict = self.model_dict\n    model_config_load_dict = self.model_config_load_dict\n    model_load_dict = self.model_load_dict\n    model_cache_load_dict = self.model_cache_load_dict\n    model_config_dict_load = self.model_config_dict_load\n    model_load_dict_load = self.model_load_dict_load\n    model_cache_dict_load = self.model_cache_dict_load\n    model_config_dict_load_dict = self.model_config_dict_load_dict\n    model_load_dict_load_dict = self.model_load_dict_load_dict\n    model_cache_dict_load_dict = self.model_cache_dict_load_dict\n    model_config_dict_load_dict_load = self.model_config_dict_load_", "CodeBleu_score": 8.121835891347697}]}
{"instance_id": "8dc955007aa2", "generation_result": [{"code": "def convsample_ddim(model, cond, steps, shape, eta=1.0, callback=None, normals_sequence=None,\n                    mask=None, x0=None, quantize_x0=False, temperature=1., score_corrector=None,\n                    corrector_kwargs=None, x_t=None\n                    )\n\t\n\n    print(f\"Sampling from eta: {eta}\")\n    print(f\"Number of steps: {steps}\")\n\n    sampler = DDIMSampler(model)\n\n    batch_size = shape[0]\n    shape = shape[1:]\n\n    if x0 is None:\n        x0 = sampler.sample_x0(batch_size, shape, cond, x_t)\n\n    if normals_sequence is None:\n        normals_sequence = sampler.sample_normals(batch_size, shape, eta)\n\n    x, scores = sampler.sample(x0, normals_sequence, cond, steps, eta, callback, mask, x0, quantize_x0, temperature,\n                               score_corrector, corrector_kwargs, x_t)\n\n    return x, scores\n\n", "CodeBleu_score": 45.94624257509502}, {"code": "def convsample_ddim(model, cond, steps, shape, eta=1.0, callback=None, normals_sequence=None,\n                    mask=None, x0=None, quantize_x0=False, temperature=1., score_corrector=None,\n                    corrector_kwargs=None, x_t=None\n                    )\n\t\n    print(f\"Sampling from eta: {eta}, {steps} steps\")\n    sampler = DDIMSampler(model, eta=eta)\n    batch_size = shape[0]\n    if x0 is None:\n        x0 = torch.randn(batch_size, *shape[1:], device=cond.device)\n    else:\n        x0 = x0.to(cond.device)\n    if quantize_x0:\n        x0 = quantize(x0, cond)\n    if normals_sequence is None:\n        normals_sequence = torch.randn(batch_size, *shape[1:], device=cond.device)\n    else:\n        normals_sequence = normals_sequence.to(cond.device)\n    if mask is None:\n        mask = torch.ones(batch_size, *shape[1:], 1, device=cond.device)\n    else:\n        mask = mask.to(cond.device)\n    if x_t is None:\n        x_t = torch.randn(batch_size, *shape[1:], device=cond.device)\n    else:\n        x_t = x_t.to(cond.device)\n    if score_corrector is not None:\n        if corrector_kwargs is None:\n            corrector_kwargs = {}\n        score_corrector = score_corrector(**corrector_kwargs)\n    return sampler.sample(\n        steps,\n        shape,\n        cond=cond,\n        callback=callback,\n        normals_sequence=normals_sequence,\n        mask=mask,\n        x0=x0,\n        temperature=temperature,\n        score_corrector=score_corrector,\n        x_t=x_t\n    )\n\n", "CodeBleu_score": 41.95735649682329}, {"code": "def convsample_ddim(model, cond, steps, shape, eta=1.0, callback=None, normals_sequence=None,\n                    mask=None, x0=None, quantize_x0=False, temperature=1., score_corrector=None,\n                    corrector_kwargs=None, x_t=None\n                    )\n\t\n\n    sampler = DDIMSampler(model, eta=eta)\n    print(f\"Sampling with eta = {eta}\")\n    print(f\"Sampling {steps} steps\")\n\n    batch_size = shape[0]\n    if x0 is None:\n        x0 = torch.randn(batch_size, *shape[1:], device=cond.device)\n    x0 = x0.to(device=cond.device)\n    if quantize_x0:\n        x0 = (x0 * 256).long() / 256.\n    x = sampler.sample(steps, x0, cond, shape, callback=callback, normals_sequence=normals_sequence, mask=mask,\n                       temperature=temperature, score_corrector=score_corrector, corrector_kwargs=corrector_kwargs,\n                       x_t=x_t)\n    return x, sampler.intermediates\n\n", "CodeBleu_score": 52.702746325815916}]}
{"instance_id": "9bdcd9081480", "generation_result": [{"code": "def ema_scope(self, context=None)\n\t\n    if self.ema_enabled:\n        if context:\n            print(\"Using EMA weights\")\n        self.ema_restore()\n        self.ema_store()\n        if context:\n            print(\"Restored original weights\")\n\n", "CodeBleu_score": 24.918127505965586}, {"code": "def ema_scope(self, context=None)\n\t\n    if not self.ema_weights:\n        return\n    if context:\n        print(f\"{context}: Using EMA weights.\")\n    self.model.set_weights(self.ema_weights)\n    yield\n    if context:\n        print(f\"{context}: Restoring original weights.\")\n    self.model.set_weights(self.weights)\n\n", "CodeBleu_score": 40.90188157767412}, {"code": "def ema_scope(self, context=None)\n\t\n\n    if context is not None:\n        print('Using EMA weights.')\n\n    if self.ema_weights is None:\n        self.ema_weights = self.parameters()\n        self.ema_weights = [w.clone().detach() for w in self.ema_weights]\n\n    self.store()\n    self.restore(self.ema_weights)\n\n    yield\n\n    self.store()\n    self.restore()\n\n    if context is not None:\n        print('Using original weights.')\n\n", "CodeBleu_score": 44.551376105749775}]}
{"instance_id": "14ab009e1c5c", "generation_result": [{"code": "def init_from_ckpt(self, path, ignore_keys=None)\n\t\n    sd = torch.load(path)\n    if ignore_keys is not None:\n        for k in list(sd.keys()):\n            if k.startswith(ignore_keys):\n                del sd[k]\n    self.load_state_dict(sd)\n    print(f\"Loaded {path} with {len(sd)} keys:\")\n    print(sd.keys())\n    print(f\"Ignored {len(ignore_keys)} keys:\")\n    print(ignore_keys)\n    print(f\"Missing keys:\")\n    print(self.state_dict().keys() - sd.keys())\n    print(f\"Unexpected keys:\")\n    print(sd.keys() - self.state_dict().keys())\n\n", "CodeBleu_score": 29.690295192489934}, {"code": "def init_from_ckpt(self, path, ignore_keys=None)\n\t\n    if os.path.exists(path):\n        sd = torch.load(path)\n        if ignore_keys is not None:\n            for k in ignore_keys:\n                if k in sd:\n                    del sd[k]\n        self.load_state_dict(sd, strict=False)\n        missing_keys, unexpected_keys = self.load_state_dict(sd, strict=False)\n        if len(missing_keys) > 0:\n            print('Missing keys in state dict: {}'.format(missing_keys))\n        if len(unexpected_keys) > 0:\n            print('Unexpected keys in state dict: {}'.format(unexpected_keys))\n    else:\n        raise FileNotFoundError(f'Could not find checkpoint file at {path}')\n\n", "CodeBleu_score": 37.83776924459499}, {"code": "def init_from_ckpt(self, path, ignore_keys=None)\n\t\n    sd = torch.load(path)\n    if ignore_keys is not None:\n        for key in ignore_keys:\n            if key in sd:\n                del sd[key]\n    missing, unexpected = self.load_state_dict(sd, strict=False)\n    if len(missing) > 0:\n        print('Missing keys in state_dict:')\n        for m in missing:\n            print('\\t' + m)\n    if len(unexpected) > 0:\n        print('Unexpected keys in state_dict:')\n        for u in unexpected:\n            print('\\t' + u)\n\n", "CodeBleu_score": 33.40676520689421}]}
{"instance_id": "754176e27b81", "generation_result": [{"code": "def forward(self, input, return_pred_indices=False)\n\t\n    # Encode input\n    encoded_input = self.encode(input)\n    # Decode encoded input\n    decoded_output = self.decode(encoded_input)\n    # Calculate difference metric\n    diff_metric = self.calc_diff_metric(input, decoded_output)\n    # Return decoded output and difference metric\n    if return_pred_indices:\n        return decoded_output, diff_metric, encoded_input\n    else:\n        return decoded_output, diff_metric\n\n", "CodeBleu_score": 26.123341790371136}, {"code": "def forward(self, input, return_pred_indices=False)\n\t\n    encoded = self.encode(input)\n    decoded = self.decode(encoded)\n    diff = torch.sum((input - decoded) ** 2)\n    if return_pred_indices:\n        return decoded, diff, encoded\n    else:\n        return decoded, diff\n\n", "CodeBleu_score": 27.003017553267807}, {"code": "def forward(self, input, return_pred_indices=False)\n\t\n    if self.training:\n        self.train()\n    else:\n        self.eval()\n    with torch.no_grad():\n        if return_pred_indices:\n            output, pred_indices = self.decode(self.encode(input))\n            return output, pred_indices\n        else:\n            output = self.decode(self.encode(input))\n            return output\n\n", "CodeBleu_score": 27.018667246909757}]}
{"instance_id": "443df6fdaf0d", "generation_result": [{"code": "def get_input(self, batch, k)\n\t\n    x = batch[k]\n    x = x.unsqueeze(0)\n    x = x.permute(0, 3, 1, 2)\n    x = x.float()\n    if self.opt.resize_factors is not None:\n        x = F.interpolate(x, scale_factor=self.opt.resize_factors[self.step], mode='bicubic', align_corners=False)\n    x = x.contiguous()\n    x = x.detach()\n    return x\n\n", "CodeBleu_score": 28.795911854924544}, {"code": "def get_input(self, batch, k)\n\t\n    x = batch[k]\n    x = x.permute(0, 3, 1, 2).float().contiguous()\n    if self.resize_batch:\n        x = F.interpolate(x, size=(self.img_size, self.img_size), mode='bicubic', align_corners=False)\n    x = x.detach()\n    return x\n\n", "CodeBleu_score": 23.0577608182451}, {"code": "def get_input(self, batch, k)\n\t\n    x = batch[k]\n    if x.ndim == 2:\n        x = x[:, np.newaxis, :, np.newaxis]\n    elif x.ndim == 3:\n        x = x.transpose((2, 0, 1))\n    elif x.ndim == 4:\n        x = x.transpose((0, 3, 1, 2))\n    else:\n        raise ValueError('Input must be 2, 3, or 4 dimensions')\n    x = F.interpolate(x, self.size, mode='bicubic', align_corners=False)\n    x = x.contiguous().float()\n    if self.resize:\n        f = self.f_max * self.d_f\n        d = self.d_f * (2 ** self.n_downsample_lr)\n        n = (self.n_training_steps - self.n_skip) * self.d_f\n        x = x[:, :, :, int((f - d * n) // 2) : int((f + d * n) // 2)]\n    x = x.detach()\n    return x\n\n", "CodeBleu_score": 41.297868804237034}]}
{"instance_id": "953a2632fc15", "generation_result": [{"code": "def training_step(self, batch, batch_idx, optimizer_idx)\n\t\n    x, y = batch\n    x_hat, y_hat = self.get_input(x, y)\n    loss = self.loss(x_hat, y_hat)\n    self.log_dict({'loss': loss, 'step': self.global_step})\n    return loss\n\n", "CodeBleu_score": 9.175270712899787}, {"code": "def training_step(self, batch, batch_idx, optimizer_idx)\n\t\n    x, y = batch\n    x_hat, z = self(x)\n    loss_ae = F.mse_loss(x_hat, x)\n    loss_d = self.loss(z, y)\n    loss = loss_ae + loss_d\n    self.log('train_loss_ae', loss_ae)\n    self.log('train_loss_d', loss_d)\n    return loss\n\n", "CodeBleu_score": 6.969032440890954}, {"code": "def training_step(self, batch, batch_idx, optimizer_idx)\n\t\n    x, y = batch\n    x_hat = self.get_input(x)\n    loss = self.loss(x_hat, x)\n    self.log_dict({\"ae_loss\": loss})\n    return loss\n\n", "CodeBleu_score": 4.46040744344363}]}
{"instance_id": "b449b5e8b77f", "generation_result": [{"code": "def log_images(self, batch, only_inputs=False, plot_ema=False, **kwargs)\n\t\n\n    if not self.log_inputs:\n        return\n\n    if only_inputs:\n        self.log_images_inputs(batch, **kwargs)\n    else:\n        self.log_images_inputs_and_reconstructions(batch, plot_ema=plot_ema, **kwargs)\n\n", "CodeBleu_score": 10.492785576184641}, {"code": "def log_images(self, batch, only_inputs=False, plot_ema=False, **kwargs)\n\t\n    if self.ema_scope is not None:\n        ema_weights = self.ema_scope.variables()\n        self.ema_scope.restore_ema_variables()\n    else:\n        ema_weights = None\n    if only_inputs:\n        self.log_inputs(batch, **kwargs)\n    else:\n        self.log_reconstructions(batch, ema_weights=ema_weights, **kwargs)\n    if self.ema_scope is not None:\n        self.ema_scope.restore_variables()\n\n", "CodeBleu_score": 18.415527702078837}, {"code": "def log_images(self, batch, only_inputs=False, plot_ema=False, **kwargs)\n\t\n\n    # Set up EMA model\n    if plot_ema:\n        ema_model = self.ema_scope.data\n    else:\n        ema_model = None\n\n    # Process input data\n    x, x_rgb = self.get_input(batch, self.to_rgb)\n\n    # Log inputs\n    self.logger.experiment.add_images(\"inputs\", x_rgb, self.global_step)\n\n    # Log reconstructions\n    if not only_inputs:\n        self.logger.experiment.add_images(\"reconstructions\", self.to_rgb(self.model(x)), self.global_step)\n        if plot_ema:\n            self.logger.experiment.add_images(\"reconstructions_ema\", self.to_rgb(ema_model(x)), self.global_step)\n\n", "CodeBleu_score": 27.905363119603678}]}
{"instance_id": "540d089e3481", "generation_result": [{"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss_type == 'l1':\n        loss = torch.abs(pred - target)\n        if mean:\n            loss = loss.mean()\n    elif self.loss_type == 'l2':\n        loss = torch.pow(pred - target, 2)\n        if mean:\n            loss = loss.mean()\n    else:\n        raise NotImplementedError(\n            'Loss type {} is not implemented.'.format(self.loss_type))\n    return loss\n\n", "CodeBleu_score": 53.52214940270854}, {"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss_type == 'l1':\n        if mean:\n            return pred.abs().mean()\n        else:\n            return pred.abs()\n    elif self.loss_type == 'l2':\n        return F.mse_loss(pred, target, reduction='mean' if mean else 'none')\n    else:\n        raise NotImplementedError(f\"Unknown loss type {self.loss_type}\")\n\n", "CodeBleu_score": 31.61702947696104}, {"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss_type == 'l1':\n        loss = torch.abs(pred - target)\n        if mean:\n            loss = loss.mean()\n    elif self.loss_type == 'l2':\n        loss = torch.pow(pred - target, 2)\n        if mean:\n            loss = loss.mean()\n    else:\n        raise NotImplementedError(\"Unknown loss type!\")\n    return loss\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 53.62731959487193}]}
{"instance_id": "63a08283666f", "generation_result": [{"code": "def forward(self, x, c, *args, **kwargs)\n\t\n    # Get time steps\n    if self.time_conditioning:\n        t = torch.randint(0, self.time_steps, (x.shape[0],), device=x.device)\n    else:\n        t = torch.zeros(x.shape[0], device=x.device, dtype=torch.long)\n\n    # Apply learned conditioning\n    if self.learned_conditioning:\n        c = self.get_learned_conditioning(c)\n\n    # Process inputs\n    x = self.process_inputs(x, t)\n\n    # Compute loss\n    loss = self.compute_loss(x, c, t)\n\n    return loss\n\n", "CodeBleu_score": 38.90000471888203}, {"code": "def forward(self, x, c, *args, **kwargs)\n\t\n    # Sample time steps\n    if self.time_conditioning:\n        # Sample time steps\n        if self.time_conditioning_range is None:\n            t = torch.randint(0, self.num_timesteps, (x.shape[0],), device=x.device)\n        else:\n            t = torch.randint(self.time_conditioning_range[0], self.time_conditioning_range[1], (x.shape[0],), device=x.device)\n        # Apply learned conditioning\n        if self.learned_conditioning:\n            c = self.get_learned_conditioning(t)\n        # Process inputs\n        if self.time_conditioning_type == 'embed':\n            x = self.time_embed(t)\n        elif self.time_conditioning_type == 'linear':\n            x = self.time_linear(t)\n        elif self.time_conditioning_type == 'sinusoidal':\n            x = self.time_sinusoidal(t)\n        else:\n            raise ValueError(f'Time conditioning type {self.time_conditioning_type} not supported.')\n    else:\n        t = None\n    # Compute loss\n    loss = self.compute_loss(x, c, t, *args, **kwargs)\n    return loss\n\n", "CodeBleu_score": 45.79514718550262}, {"code": "def forward(self, x, c, *args, **kwargs)\n\t\n    # Get conditioning\n    if c is None:\n        c = self.get_learned_conditioning(x)\n    # Get time step information\n    t = self.get_time_info(x)\n    # Get random time step indices\n    indices = self.get_indices(t)\n    # Get random time steps\n    x = self.get_time_steps(x, indices)\n    # Apply conditioning\n    x = self.apply_conditioning(x, c)\n    # Process inputs\n    x = self.process_inputs(x)\n    # Compute loss\n    loss = self.compute_loss(x, *args, **kwargs)\n    return loss\n\n", "CodeBleu_score": 28.818856014099946}]}
{"instance_id": "4627e1e8ed62", "generation_result": [{"code": "def get_codebook_entry(self, indices, shape)\n\t\n    if self.remap is not None:\n        indices = self.remap[indices]\n    z_q = self.embedding(indices).view(shape)\n    if self.permutation is not None:\n        z_q = z_q.permute(self.permutation)\n    return z_q.contiguous()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 38.87392386717522}, {"code": "def get_codebook_entry(self, indices, shape)\n\t\n    if self.remap:\n        indices = self.remap_to_all(indices)\n    z_q = self.embedding(indices).view(shape)\n    if self.permutation is not None:\n        z_q = z_q.permute(self.permutation)\n    return z_q.contiguous()\n\n", "CodeBleu_score": 37.13798371206665}, {"code": "def get_codebook_entry(self, indices, shape)\n\t\n    if self.remap:\n        indices = self.remap_indices(indices)\n    z_q = self.embedding(indices).view(shape)\n    if self.permutation is not None:\n        z_q = z_q.permute(self.permutation)\n    return z_q\n\n", "CodeBleu_score": 36.14681469167768}]}
{"instance_id": "c55d014827ef", "generation_result": [{"code": "def factorization(dimension: int, factor:int=-1) -> tuple[int, int]\n\t\n    if factor > 0 and dimension % factor == 0:\n        return factor, dimension // factor\n    elif factor < 0:\n        for i in range(1, dimension // 2 + 1):\n            if dimension % i == 0:\n                if abs(i - dimension // i) <= abs(factor):\n                    factor = i\n                elif abs(i - dimension // i) == abs(factor) and i > factor:\n                    factor = i\n    else:\n        raise ValueError(\"The factor must be positive.\")\n    return factor, dimension // factor\n\n", "CodeBleu_score": 32.240686835751596}, {"code": "def factorization(dimension: int, factor:int=-1) -> tuple[int, int]\n\t\n    if factor > 0 and dimension % factor == 0:\n        return (factor, dimension // factor)\n    elif factor > 0:\n        raise ValueError(\"The factor must divide the dimension\")\n    elif factor == 0:\n        raise ValueError(\"The factor cannot be zero\")\n    else:\n        if dimension % 2 == 0:\n            return (2, dimension // 2)\n        else:\n            i = 3\n            while i * i <= dimension:\n                if dimension % i == 0:\n                    return (i, dimension // i)\n                i += 2\n            return (dimension, 1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 32.71633234589749}, {"code": "def factorization(dimension: int, factor:int=-1) -> tuple[int, int]\n\t\n    if factor < 0:\n        if dimension % 2 == 0:\n            m = 2\n            n = dimension // 2\n        else:\n            m = 1\n            n = dimension\n        while n > 1:\n            if n % 2 == 0:\n                n //= 2\n            else:\n                n -= 1\n                m += 1\n        if m > n:\n            m, n = n, m\n        return m, n\n    elif dimension % factor == 0:\n        return dimension // factor, factor\n    else:\n        raise ValueError(f\"The factor {factor} does not divide the dimension {dimension}.\")\n\n\n", "CodeBleu_score": 45.564794798613946}]}
{"instance_id": "5d613ec26036", "generation_result": [{"code": "def apply_weight_decompose(self, updown, orig_weight)\n\t\n    if self.device != orig_weight.device:\n        orig_weight = orig_weight.to(self.device)\n    if self.device != updown.device:\n        updown = updown.to(self.device)\n    if self.dtype != orig_weight.dtype:\n        orig_weight = orig_weight.type(self.dtype)\n    if self.dtype != updown.dtype:\n        updown = updown.type(self.dtype)\n    # Combine the input weight with the original weight\n    merged_weight = updown + orig_weight\n    # Normalize the combined weight\n    merged_weight = merged_weight / merged_weight.norm()\n    # Scale the normalized combined weight\n    merged_scale = merged_weight * self.scale\n    # Calculate the adjusted weight by subtracting the original weight from the scaled and normalized combined weight\n    adjusted_weight = merged_scale - orig_weight\n    return adjusted_weight\n\n", "CodeBleu_score": 34.3290381698552}, {"code": "def apply_weight_decompose(self, updown, orig_weight)\n\t\n    # Ensure that all tensors are on the same device and have the same data type\n    if updown.device != orig_weight.device:\n        updown = updown.to(orig_weight.device)\n    if updown.dtype != orig_weight.dtype:\n        updown = updown.type(orig_weight.dtype)\n    # Calculate the adjusted weight\n    merged_scale1 = updown.transpose(0, 1)\n    norm = merged_scale1.norm(dim=1)\n    merged_scale1 = merged_scale1.reshape(merged_scale1.shape[0], 1, merged_scale1.shape[1])\n    merged_scale1 = merged_scale1.to(orig_weight.device)\n    merged_scale1 = merged_scale1.type(orig_weight.dtype)\n    merged_scale1 = merged_scale1.transpose(0, 1)\n    merged_scale1 = merged_scale1.reshape(merged_scale1.shape[0], merged_scale1.shape[1])\n    merged_scale1 = merged_scale1.to(orig_weight.device)\n    merged_scale1 = merged_scale1.type(orig_weight.dtype)\n    merged_scale1 = merged_scale1 / norm\n    merged_scale1 = merged_scale1 * self.scale\n    merged_scale1 = merged_scale1.to(orig_weight.device)\n    merged_scale1 = merged_scale1.type(orig_weight.dtype)\n    merged_scale1 = merged_scale1.reshape(merged_scale1.shape[0], 1, merged_scale1.shape[1])\n    merged_scale1 = merged_scale1.to(orig_weight.device)\n    merged_scale1 = merged_scale1.type(orig_weight.dtype)\n    merged_scale1 = merged_scale1.transpose(0, 1)\n    merged_scale1 = merged_scale1.reshape(merged_scale1.shape[0], merged_scale1.shape[1])\n    merged_scale1 = merged_scale1.to", "CodeBleu_score": 37.33939656443292}, {"code": "def apply_weight_decompose(self, updown, orig_weight)\n\t\n    if updown == \"up\":\n        weight_combined = orig_weight + self.weight_decompose_scale_factor * torch.nn.functional.normalize(\n            orig_weight + self.weight_decompose_scale_factor * self.weight_decompose_scale_factor * updown\n        )\n        weight_adjusted = weight_combined - orig_weight\n    elif updown == \"down\":\n        weight_combined = orig_weight + self.weight_decompose_scale_factor * torch.nn.functional.normalize(\n            orig_weight - self.weight_decompose_scale_factor * self.weight_decompose_scale_factor * updown\n        )\n        weight_adjusted = weight_combined - orig_weight\n    return weight_adjusted\n\n", "CodeBleu_score": 23.84686711350955}]}
{"instance_id": "c73d0d1cd3e5", "generation_result": [{"code": "def finalize_updown(self, updown, orig_weight, output_shape, ex_bias=None)\n\t\n    if ex_bias is not None:\n        updown = updown + ex_bias\n    if updown.numel() == orig_weight.numel():\n        updown = updown.reshape(orig_weight.size())\n    else:\n        updown = updown.reshape(output_shape)\n    if self.calc_scale:\n        updown = self.apply_weight_decompose(updown)\n    return updown, ex_bias\n\n", "CodeBleu_score": 34.74319006996284}, {"code": "def finalize_updown(self, updown, orig_weight, output_shape, ex_bias=None)\n\t\n    if ex_bias is not None:\n        updown = updown + ex_bias\n    if updown.numel() == orig_weight.numel():\n        updown = updown.reshape(orig_weight.size())\n    else:\n        updown = updown.reshape(output_shape)\n    if self.calc_scale:\n        updown = self.apply_weight_decompose(updown, orig_weight)\n    return updown, ex_bias\n\n", "CodeBleu_score": 38.15089402421364}, {"code": "def finalize_updown(self, updown, orig_weight, output_shape, ex_bias=None)\n\t\n    if ex_bias is not None:\n        ex_bias = ex_bias.reshape(output_shape)\n    if updown.numel() == orig_weight.numel():\n        updown = updown.reshape(orig_weight.shape)\n    else:\n        updown = updown.reshape(output_shape)\n    if self.bias_scalar is not None:\n        ex_bias = ex_bias * self.bias_scalar\n    if self.calc_scale:\n        updown = self.apply_weight_decompose(updown, orig_weight)\n    return updown, ex_bias\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 45.884030956586805}]}
{"instance_id": "c6a41665c62c", "generation_result": [{"code": "def calc_updown(self, orig_weight)\n\t\n    weight = self.weight.to(orig_weight.device)\n    if self.bias is not None:\n        bias = self.bias.to(orig_weight.device)\n    else:\n        bias = None\n    return self.finalize_updown(orig_weight, weight, bias)\n\n", "CodeBleu_score": 43.89636664243215}, {"code": "def calc_updown(self, orig_weight)\n\t\n    weight = self.weight.clone()\n    if self.bias is not None:\n        bias = self.bias.clone()\n    else:\n        bias = None\n    if self.weight.device != orig_weight.device:\n        weight = weight.to(orig_weight.device)\n        if bias is not None:\n            bias = bias.to(orig_weight.device)\n    return self.finalize_updown(weight, orig_weight, bias)\n\n", "CodeBleu_score": 38.92837160547063}, {"code": "def calc_updown(self, orig_weight)\n\t\n    # self.finalize_updown(orig_weight, self.weight, self.bias)\n    # return self.weight\n    if self.bias is not None:\n        return self.finalize_updown(orig_weight, self.weight, self.bias)\n    else:\n        return self.finalize_updown(orig_weight, self.weight)\n\n", "CodeBleu_score": 20.384737372253287}]}
{"instance_id": "1bb9e5d94af8", "generation_result": [{"code": "def purge_networks_from_memory()\n\t\n    if len(networks_in_memory) > MAX_NETWORKS_IN_MEMORY:\n        while len(networks_in_memory) > MAX_NETWORKS_IN_MEMORY:\n            networks_in_memory.pop()\n        gc.collect()\n\n", "CodeBleu_score": 33.1572837655859}, {"code": "def purge_networks_from_memory()\n\t\n    global networks_in_memory\n    global networks_in_memory_limit\n    global devices\n    while len(networks_in_memory) > networks_in_memory_limit:\n        networks_in_memory.pop(0)\n    devices.torch_gc\n\n", "CodeBleu_score": 34.81343387288709}, {"code": "def purge_networks_from_memory()\n\t\n    # Get the number of networks in memory\n    networks_in_memory = len(devices.torch_gc.get_objects())\n    # Set the limit of networks in memory\n    networks_in_memory_limit = 100\n    # If the number of networks in memory exceeds the limit\n    if networks_in_memory > networks_in_memory_limit:\n        # Remove networks from memory until the count is within the limit\n        while networks_in_memory > networks_in_memory_limit:\n            # Remove the oldest network from memory\n            devices.torch_gc.pop()\n            # Get the number of networks in memory\n            networks_in_memory = len(devices.torch_gc.get_objects())\n        # Trigger garbage collection to free up memory\n        devices.torch_gc.collect()\n\n", "CodeBleu_score": 30.93824591653175}]}
{"instance_id": "42b48ebf4d07", "generation_result": [{"code": "def network_restore_weights_from_backup(self: Union[torch.nn.Conv2d, torch.nn.Linear, torch.nn.GroupNorm, torch.nn.LayerNorm, torch.nn.MultiheadAttention]):\n\t\n    if hasattr(self, \"weight_backup\"):\n        self.weight = self.weight_backup\n    if hasattr(self, \"bias_backup\"):\n        self.bias = self.bias_backup\n    else:\n        self.bias = None\n    if isinstance(self, torch.nn.MultiheadAttention):\n        if hasattr(self, \"in_proj_weight_backup\"):\n            self.in_proj_weight = self.in_proj_weight_backup\n        if hasattr(self, \"in_proj_bias_backup\"):\n            self.in_proj_bias = self.in_proj_bias_backup\n        else:\n            self.in_proj_bias = None\n\n", "CodeBleu_score": 33.93005480507518}, {"code": "def network_restore_weights_from_backup(self: Union[torch.nn.Conv2d, torch.nn.Linear, torch.nn.GroupNorm, torch.nn.LayerNorm, torch.nn.MultiheadAttention]):\n\t\n    if self.weight.requires_grad:\n        if self.weight.grad is not None:\n            self.weight.grad.data.zero_()\n\n    if isinstance(self, torch.nn.Conv2d):\n        if self.bias is not None:\n            if self.bias.requires_grad:\n                if self.bias.grad is not None:\n                    self.bias.grad.data.zero_()\n\n    if isinstance(self, torch.nn.MultiheadAttention):\n        if self.in_proj_weight.requires_grad:\n            if self.in_proj_weight.grad is not None:\n                self.in_proj_weight.grad.data.zero_()\n\n        if self.in_proj_bias is not None:\n            if self.in_proj_bias.requires_grad:\n                if self.in_proj_bias.grad is not None:\n                    self.in_proj_bias.grad.data.zero_()\n\n    if hasattr(self, 'weight_backup'):\n        self.weight.data.copy_(self.weight_backup.data)\n\n    if hasattr(self, 'bias_backup'):\n        self.bias.data.copy_(self.bias_backup.data)\n\n    if isinstance(self, torch.nn.MultiheadAttention):\n        if hasattr(self, 'in_proj_weight_backup'):\n            self.in_proj_weight.data.copy_(self.in_proj_weight_backup.data)\n\n        if hasattr(self, 'in_proj_bias_backup'):\n            self.in_proj_bias.data.copy_(self.in_proj_bias_backup.data)\n\n", "CodeBleu_score": 45.46728199689296}, {"code": "def network_restore_weights_from_backup(self: Union[torch.nn.Conv2d, torch.nn.Linear, torch.nn.GroupNorm, torch.nn.LayerNorm, torch.nn.MultiheadAttention]):\n\t\n    if self.backup_weights is not None:\n        self.weight.data = self.backup_weights\n    if self.backup_bias is not None:\n        self.bias.data = self.backup_bias\n    else:\n        self.bias = None\n    if isinstance(self, torch.nn.MultiheadAttention):\n        if self.backup_in_proj_weight is not None:\n            self.in_proj_weight.data = self.backup_in_proj_weight\n        if self.backup_in_proj_bias is not None:\n            self.in_proj_bias.data = self.backup_in_proj_bias\n        else:\n            self.in_proj_bias = None\n        if self.backup_out_proj_weight is not None:\n            self.out_proj.weight.data = self.backup_out_proj_weight\n        if self.backup_out_proj_bias is not None:\n            self.out_proj.bias.data = self.backup_out_proj_bias\n        else:\n            self.out_proj.bias = None\n\n", "CodeBleu_score": 40.505565127079464}]}
{"instance_id": "c8fcd357131a", "generation_result": [{"code": "def network_forward(org_module, input, original_forward)\n\t\n    if len(devices.loaded_networks) == 0:\n        return original_forward(input)\n    else:\n        input = input.to(devices.device)\n        network_restore_weights_from_backup(org_module)\n        network_reset_cached_weight(org_module)\n        output = original_forward(input)\n        for network in devices.loaded_networks:\n            if network.layer == org_module:\n                output = network.forward(output)\n        return output\n", "CodeBleu_score": 35.423146652738914}, {"code": "def network_forward(org_module, input, original_forward)\n\t\n    if len(devices.loaded_networks) == 0:\n        return original_forward(input)\n    else:\n        input = input.to(devices.current_device)\n        network_restore_weights_from_backup(org_module)\n        network_reset_cached_weight(org_module)\n        output = original_forward(input)\n        for network in devices.loaded_networks:\n            if network.layer == org_module:\n                output = network.forward(output)\n        return output\n\n", "CodeBleu_score": 35.602149843572185}, {"code": "def network_forward(org_module, input, original_forward)\n\t\n    if len(devices.loaded_networks) == 0:\n        return original_forward(input)\n    else:\n        input = input.to(devices.current_device)\n        module.restore_weights_from_backup(org_module)\n        module.reset_cached_weight(org_module)\n        output = original_forward(input)\n        for loaded_network in devices.loaded_networks:\n            if loaded_network.layer == org_module:\n                output = loaded_network.forward(output)\n        return output\n\n", "CodeBleu_score": 35.09639841405613}]}
{"instance_id": "1070211f5c11", "generation_result": [{"code": "def trainables(self)\n\t\n    trainables = []\n    for module in self.modules():\n        if isinstance(module, torch.nn.Linear) or isinstance(module, torch.nn.LayerNorm):\n            trainables.append(module.weight)\n            if module.bias is not None:\n                trainables.append(module.bias)\n    return trainables\n\n", "CodeBleu_score": 30.96439578985467}, {"code": "def trainables(self)\n\t\n    return [p for p in self.parameters() if p.requires_grad]\n\n", "CodeBleu_score": 6.452447410697191}, {"code": "def trainables(self)\n\t\n    trainables = []\n    for name, layer in self.named_modules():\n        if isinstance(layer, torch.nn.Linear) or isinstance(layer, torch.nn.LayerNorm):\n            trainables.append(layer)\n    return trainables\n\n", "CodeBleu_score": 35.33573961019367}]}
{"instance_id": "0c3ba62c96a7", "generation_result": [{"code": "def load_hypernetworks(names, multipliers=None)\n\t\n    already_loaded.get\n    hypernetwork.set_multiplier\n    enumerate\n    append\n    clear\n    load_hypernetwork\n    already_loaded.set\n\n", "CodeBleu_score": 3.0879935534370366}, {"code": "def load_hypernetworks(names, multipliers=None)\n\t\n    if multipliers is None:\n        multipliers = [None]*len(names)\n    already_loaded.get()\n    hypernetwork.set_multiplier()\n    enumerate()\n    clear()\n    for name, multiplier in zip(names, multipliers):\n        if name in already_loaded:\n            hypernetwork.set_multiplier(multiplier)\n            loaded_hypernetworks.append(hypernetwork)\n        else:\n            load_hypernetwork(name)\n            hypernetwork.set_multiplier(multiplier)\n            loaded_hypernetworks.append(hypernetwork)\n    already_loaded.set(loaded_hypernetworks)\n\n", "CodeBleu_score": 29.026204884608024}, {"code": "def load_hypernetworks(names, multipliers=None)\n\t\n    if not isinstance(names, list):\n        names = [names]\n    if multipliers is None:\n        multipliers = [None] * len(names)\n    if not isinstance(multipliers, list):\n        multipliers = [multipliers]\n    if len(names) != len(multipliers):\n        raise ValueError('Number of hypernetworks and multipliers must be equal.')\n    already_loaded = get_loaded_hypernetworks()\n    clear_loaded_hypernetworks()\n    for i in range(len(names)):\n        if names[i] in already_loaded:\n            loaded_hypernetwork = already_loaded[names[i]]\n        else:\n            loaded_hypernetwork = load_hypernetwork(names[i])\n        if multipliers[i] is not None:\n            loaded_hypernetwork.set_multiplier(multipliers[i])\n        loaded_hypernetwork.append()\n\n", "CodeBleu_score": 28.868452310679316}]}
{"instance_id": "bea3772ed532", "generation_result": [{"code": "\ndef apply_hypernetworks(hypernetworks, context, layer=None)\n\t\n    if layer is not None:\n        context_k = context['k' + str(layer)]\n        context_v = context['v' + str(layer)]\n    else:\n        context_k = context['k']\n        context_v = context['v']\n    for hypernetwork in hypernetworks:\n        context_k, context_v = apply_single_hypernetwork(hypernetwork, context_k, context_v, context)\n    if layer is not None:\n        context['k' + str(layer)] = context_k\n        context['v' + str(layer)] = context_v\n    else:\n        context['k'] = context_k\n        context['v'] = context_v\n    return context_k, context_v\n\n\n", "CodeBleu_score": 50.79802067073052}, {"code": "\ndef apply_hypernetworks(hypernetworks, context, layer=None)\n\t\n    # Initialize context_k and context_v with the input context\n    context_k = context\n    context_v = context\n    # Iteratively apply each hypernetwork to context_k and context_v\n    for hypernetwork in hypernetworks:\n        context_k, context_v = apply_single_hypernetwork(hypernetwork, context_k, context_v, layer)\n    # Return the modified context_k and context_v\n    return context_k, context_v\n\n\n", "CodeBleu_score": 77.04946886953897}, {"code": "\ndef apply_hypernetworks(hypernetworks, context, layer=None)\n\t\n    # Initialize context variables\n    context_k = context\n    context_v = context\n    # Apply each hypernetwork to the context variables\n    for hypernetwork in hypernetworks:\n        context_k, context_v = apply_single_hypernetwork(hypernetwork, context_k, context_v, layer=layer)\n    # Return modified context variables\n    return context_k, context_v\n\n\n\n", "CodeBleu_score": 61.822641355854024}]}
{"instance_id": "922efa3e092f", "generation_result": [{"code": "def attention_CrossAttention_forward(self, x, context=None, mask=None, **kwargs)\n\t\n    # x: [batch_size, length, input_dim]\n    # context: [batch_size, context_length, context_dim]\n    # mask: [batch_size, length, context_length]\n\n    # Apply query, key, and value projections\n    q = self.to_q(x)\n    k = self.to_k(context)\n    v = self.to_v(context)\n\n    # Apply hypernetworks\n    if self.hyper:\n        k, v = apply_hypernetworks(self.hyper, context, k, v)\n\n    # Reshape for multi-head attention\n    q = rearrange(q, 'b l (h d) -> b h l d', h=self.heads)\n    k = rearrange(k, 'b l (h d) -> b h l d', h=self.heads)\n    v = rearrange(v, 'b l (h d) -> b h l d', h=self.heads)\n\n    # Calculate scaled dot-product attention\n    d = q.shape[-1]\n    scale = d ** -0.5\n    scores = torch.einsum('b h i d, b h j d -> b h i j', q, k) * scale\n    if mask is not None:\n        scores = scores.masked_fill_(mask == 0, -1e9)\n    scores = torch.softmax(scores, dim=-1)\n    out = torch.einsum('b h i j, b h j d -> b h i d', scores, v)\n\n    # Reshape back\n    out = rearrange(out, 'b h l d -> b l (h d)')\n\n    # Apply final linear layer\n    out = self.to_out(out)\n    return out\n\n", "CodeBleu_score": 44.782661410137905}, {"code": "def attention_CrossAttention_forward(self, x, context=None, mask=None, **kwargs)\n\t\n    # Apply query, key, and value projections\n    q = self.to_q(x)\n    k = self.to_k(x)\n    v = self.to_v(x)\n    # Apply hypernetworks to the context\n    context = apply_hypernetworks(self.hyper_w_0, self.hyper_b_0, context)\n    # Reshape for multi-head attention\n    q = rearrange(q, 'b n (h d) -> b h n d', h=self.h)\n    k = rearrange(k, 'b n (h d) -> b h n d', h=self.h)\n    v = rearrange(v, 'b n (h d) -> b h n d', h=self.h)\n    context = rearrange(context, 'b n (h d) -> b h n d', h=self.h)\n    # Calculate scaled dot-product attention\n    attn = sim.softmax(sim.einsum('bhid,bhjd->bhij', q, k) / self.scale, dim=-1)\n    # Apply mask\n    if mask is not None:\n        mask = rearrange(mask, 'b n -> b () n ()')\n        attn = attn.masked_fill_(mask, 0)\n    # Calculate final attention output\n    out = sim.einsum('bhij,bhjd->bhid', attn, v)\n    # Reshape back\n    out = rearrange(out, 'b h n d -> b n (h d)')\n    # Pass through final linear layer\n    out = self.to_out(out)\n    return out\n\n", "CodeBleu_score": 43.66800417493063}, {"code": "def attention_CrossAttention_forward(self, x, context=None, mask=None, **kwargs)\n\t\n    # x: [batch_size, length, d_model]\n    # context: [batch_size, length, d_model]\n    # mask: [batch_size, 1, length]\n    # kwargs:\n    #     query_mask: [batch_size, 1, length]\n    #     context_mask: [batch_size, 1, length]\n    #     hyper_params: [batch_size, num_heads, d_model]\n    #     hyper_params_mask: [batch_size, num_heads, length]\n    #     hyper_params_bias: [batch_size, num_heads, length]\n    #     hyper_params_ln: [batch_size, num_heads, d_model]\n    #     hyper_params_ln_mask: [batch_size, num_heads, length]\n    #     hyper_params_ln_bias: [batch_size, num_heads, length]\n    #     hyper_params_q: [batch_size, num_heads, d_model]\n    #     hyper_params_q_mask: [batch_size, num_heads, length]\n    #     hyper_params_q_bias: [batch_size, num_heads, length]\n    #     hyper_params_k: [batch_size, num_heads, d_model]\n    #     hyper_params_k_mask: [batch_size, num_heads, length]\n    #     hyper_params_k_bias: [batch_size, num_heads, length]\n    #     hyper_params_v: [batch_size, num_heads, d_model]\n    #     hyper_params_v_mask: [batch_size, num_heads, length]\n    #     hyper_params_v_bias: [batch_size, num_heads, length]\n    #     hyper_params_out: [batch_size, num_heads, d_model]\n    #     hyper_params_out_mask: [batch_size, num_heads, length]\n    #     hyper_params_out_bias: [batch_", "CodeBleu_score": 1.9834764933446987}]}
{"instance_id": "8b7455a03a0e", "generation_result": [{"code": "def save_hypernetwork(hypernetwork, checkpoint, hypernetwork_name, filename)\n\t\n    try:\n        # Back up the original attributes\n        original_attributes = {\n            \"name\": hypernetwork.name,\n            \"checkpoint\": hypernetwork.checkpoint,\n        }\n\n        # Update the attributes with the new checkpoint information\n        hypernetwork.name = hypernetwork_name\n        hypernetwork.checkpoint = checkpoint\n\n        # Save the hypernetwork\n        hypernetwork.save(filename)\n    except Exception as e:\n        # Restore the original attributes\n        hypernetwork.name = original_attributes[\"name\"]\n        hypernetwork.checkpoint = original_attributes[\"checkpoint\"]\n\n        # Re-raise the exception\n        raise e\n\n", "CodeBleu_score": 33.60357128200058}, {"code": "def save_hypernetwork(hypernetwork, checkpoint, hypernetwork_name, filename)\n\t\n    try:\n        hypernetwork.save(filename, checkpoint, hypernetwork_name)\n    except Exception as e:\n        raise e\n\n", "CodeBleu_score": 11.685417107400573}, {"code": "def save_hypernetwork(hypernetwork, checkpoint, hypernetwork_name, filename)\n\t\n    # Backup the original attributes.\n    hypernetwork_backup = {\n        \"hypernetwork_name\": hypernetwork.hypernetwork_name,\n        \"checkpoint\": hypernetwork.checkpoint,\n        \"checkpoint_dir\": hypernetwork.checkpoint_dir,\n        \"checkpoint_file\": hypernetwork.checkpoint_file,\n        \"checkpoint_path\": hypernetwork.checkpoint_path,\n        \"checkpoint_file_exists\": hypernetwork.checkpoint_file_exists,\n        \"checkpoint_path_exists\": hypernetwork.checkpoint_path_exists,\n        \"checkpoint_path_is_file\": hypernetwork.checkpoint_path_is_file,\n        \"checkpoint_path_is_dir\": hypernetwork.checkpoint_path_is_dir,\n    }\n    # Update the hypernetwork's attributes with the new checkpoint information.\n    hypernetwork.hypernetwork_name = hypernetwork_name\n    hypernetwork.checkpoint = checkpoint\n    hypernetwork.checkpoint_dir = os.path.join(\n        hypernetwork.checkpoint_dir, hypernetwork.hypernetwork_name\n    )\n    hypernetwork.checkpoint_file = os.path.join(\n        hypernetwork.checkpoint_dir, f\"{hypernetwork.hypernetwork_name}.pth\"\n    )\n    hypernetwork.checkpoint_path = os.path.join(\n        hypernetwork.checkpoint_dir,\n        f\"{hypernetwork.hypernetwork_name}_{hypernetwork.checkpoint}.pth\",\n    )\n    hypernetwork.checkpoint_file_exists = os.path.exists(hypernetwork.checkpoint_file)\n    hypernetwork.checkpoint_path_exists = os.path.exists(hypernetwork.checkpoint_path)\n    hypernetwork.checkpoint_path_is_file = os.path.isfile(hypernetwork.checkpoint_path)\n    hypernetwork.checkpoint_path_is_dir = os.path.isdir(hypernetwork.checkpoint_path)\n    # Try to save the hypernetwork.\n    try:\n        torch.save(hypernetwork.state_dict(), filename)\n    # If an error occurs, restore the original attributes.\n    except", "CodeBleu_score": 31.642921421482047}]}
{"instance_id": "3175f835140f", "generation_result": [{"code": "def ema_scope(self, context=None)\n\t\n    if self.ema:\n        if context is None:\n            context = {}\n        context['ema_params'] = self.model.state_dict()\n        self.model.load_state_dict(self.ema.state_dict())\n        if self.verbose:\n            print(f'EMA: Switching to EMA weights')\n    yield context\n    if self.ema:\n        self.model.load_state_dict(context['ema_params'])\n        if self.verbose:\n            print(f'EMA: Restoring original weights')\n\n", "CodeBleu_score": 48.55075968195044}, {"code": "def ema_scope(self, context=None)\n\t\n    if not self.ema:\n        yield\n        return\n\n    if context is None:\n        context = {}\n\n    if self.ema_model is None:\n        self.ema_model = copy_to(self.model, self.device)\n\n    if self.ema_model.training:\n        self.ema_model.eval()\n\n    if self.ema_model.has_parameters:\n        self.ema_model.parameters.store(context)\n        self.ema_model.parameters.restore(self.ema_model.parameters.ema)\n\n    if self.ema_model.has_buffers:\n        self.ema_model.buffers.store(context)\n        self.ema_model.buffers.restore(self.ema_model.buffers.ema)\n\n    if self.ema_model.has_states:\n        self.ema_model.states.store(context)\n        self.ema_model.states.restore(self.ema_model.states.ema)\n\n    try:\n        yield\n    finally:\n        if self.ema_model.has_parameters:\n            self.ema_model.parameters.restore(context)\n\n        if self.ema_model.has_buffers:\n            self.ema_model.buffers.restore(context)\n\n        if self.ema_model.has_states:\n            self.ema_model.states.restore(context)\n", "CodeBleu_score": 33.81974689039806}, {"code": "def ema_scope(self, context=None)\n\t\n    if context is None:\n        context = self.context\n    if context is None:\n        raise ValueError('No context provided')\n    if self.ema_enabled:\n        self.store(context)\n        self.copy_to(context)\n        if self.verbose:\n            print('EMA: Switched to EMA weights')\n    try:\n        yield\n    finally:\n        if self.ema_enabled:\n            self.restore(context)\n            if self.verbose:\n                print('EMA: Restored original weights')\n", "CodeBleu_score": 43.58984592336189}]}
{"instance_id": "a7b5d5cd519a", "generation_result": [{"code": "def p_mean_variance(self, x, t, clip_denoised: bool)\n\t\n    x_recon = self.model(x, t)\n    if self.model_output_size == 2:\n        x_recon = self.predict_start_from_noise(x_recon, t)\n    if clip_denoised:\n        x_recon.clamp_(-1.0, 1.0)\n    model_mean, posterior_variance, posterior_log_variance = self.q_posterior(x, x_recon, t)\n    return model_mean, posterior_variance, posterior_log_variance\n\n", "CodeBleu_score": 43.665821040174166}, {"code": "def p_mean_variance(self, x, t, clip_denoised: bool)\n\t\n    x_recon = self.predict_start_from_noise(x, t)\n    if clip_denoised:\n        x_recon.clamp_(-1.0, 1.0)\n\n    x_in = x_recon if self.model_output_size == 3 else x\n    model_output = self.model(x_in, t)\n    if self.model_output_size == 3:\n        mean, _, variance = model_output\n    else:\n        mean, variance = model_output\n    log_variance = torch.log(variance)\n\n    return mean, log_variance, variance\n\n", "CodeBleu_score": 28.317773146323976}, {"code": "def p_mean_variance(self, x, t, clip_denoised: bool)\n\t\n    x_recon = self.model(x, t)\n    if self.predict_start_from_noise:\n        x_recon = self.q_posterior(x_recon)\n    if clip_denoised:\n        x_recon.clamp_(-1., 1.)\n    model_mean, posterior_variance, posterior_log_variance = self.model.p_mean_variance(x if self.model.input_type == 'x' else x_recon, t, clip_denoised=False)\n    return model_mean, posterior_variance, posterior_log_variance\n\n", "CodeBleu_score": 51.069841596097035}]}
{"instance_id": "a8652fa78473", "generation_result": [{"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss_type == 'l1':\n        loss = torch.abs(pred - target)\n        if mean:\n            loss = loss.mean()\n    elif self.loss_type == 'l2':\n        loss = F.mse_loss(pred, target, reduction='none')\n        if not mean:\n            loss = loss.mean(dim=0)\n    else:\n        raise NotImplementedError('Unknown loss type.')\n    return loss\n", "CodeBleu_score": 56.7099545855707}, {"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss_type == 'l1':\n        loss = torch.abs(pred - target)\n        if mean:\n            loss = loss.mean()\n    elif self.loss_type == 'l2':\n        loss = torch.nn.functional.mse_loss(pred, target, reduction='none')\n        if mean:\n            loss = loss.mean()\n    else:\n        raise NotImplementedError(f'{self.loss_type} loss is not implemented.')\n    return loss\n\n", "CodeBleu_score": 61.33894002844061}, {"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss_type == 'l1':\n        loss = torch.abs(pred - target)\n        if mean:\n            loss = loss.mean()\n    elif self.loss_type == 'l2':\n        loss = torch.nn.functional.mse_loss(pred, target, reduction='none')\n    else:\n        raise NotImplementedError(f\"Unknown loss type: {self.loss_type}\")\n    return loss\n\n", "CodeBleu_score": 57.11982998115121}]}
{"instance_id": "d68e4f061349", "generation_result": [{"code": "def _prior_bpd(self, x_start)\n\t\n    batch_size = x_start.shape[0]\n    t = torch.zeros(batch_size, device=self.device).long() + self.n_timesteps - 1\n    q_mean, q_logvar = self.q_mean_variance(x_start, t)\n    kl = normal_kl(q_mean, q_logvar)\n    return kl.mean() / np.log(2.)\n\n", "CodeBleu_score": 31.937501650993287}, {"code": "def _prior_bpd(self, x_start)\n\t\n    batch_size = x_start.shape[0]\n    t = torch.ones(batch_size, 1, 1, 1, 1) * (self.T - 1)\n    t = t.to(self.device)\n    x_start = x_start.to(self.device)\n    q_mean, q_logvar = self.q_mean_variance(x_start, t)\n    kl = normal_kl(q_mean, q_logvar)\n    return torch.mean(kl) / np.log(2)\n\n", "CodeBleu_score": 25.065072269194154}, {"code": "def _prior_bpd(self, x_start)\n\t\n    batch_size = x_start.shape[0]\n    t = torch.tensor([self.seq_len - 1] * batch_size).to(self.device)\n    q_mean_variance = self.q_mean_variance(x_start, t)\n    q_mean = q_mean_variance[:, :self.z_dim]\n    q_log_variance = q_mean_variance[:, self.z_dim:]\n    return torch.mean(normal_kl(q_mean, q_log_variance)) / np.log(2)\n\n", "CodeBleu_score": 30.113949816234797}]}
{"instance_id": "c695d93fdf2d", "generation_result": [{"code": "def make_cache(subsection: str) -> diskcache.Cache\n\t\n    cache_path = os.path.join(CACHE_DIR, subsection)\n    return diskcache.Cache(cache_path, size_limit=4 * 1024 ** 3, min_file_size=256 * 1024)\n\n", "CodeBleu_score": 50.12777897320036}, {"code": "def make_cache(subsection: str) -> diskcache.Cache\n\t\n    cache_dir = os.path.join(\n        os.path.expanduser(\"~\"),\n        \".cache\",\n        \"covid_data_tracker\",\n        \"covid_data_tracker\",\n        \"cache\",\n    )\n    os.makedirs(cache_dir, exist_ok=True)\n    return diskcache.Cache(\n        os.path.join(cache_dir, subsection),\n        disk_min_file_size=256 * 1024,\n        disk_pickle_protocol=pickle.HIGHEST_PROTOCOL,\n        disk_cache_size=4 * 1024 * 1024 * 1024,\n    )\n\n", "CodeBleu_score": 60.51900271083402}, {"code": "def make_cache(subsection: str) -> diskcache.Cache\n\t\n    return diskcache.Cache(\n        directory=f\"{cache_dir}/{subsection}\",\n        size_limit=4 * 1024 ** 3,\n        min_file_size=256 * 1024,\n    )\n\n", "CodeBleu_score": 51.623902561813125}]}
{"instance_id": "ed4c543f0d38", "generation_result": [{"code": "def cache(subsection)\n\t\n    global caches\n    if subsection in caches:\n        return caches[subsection]\n    else:\n        with cache_lock:\n            if not os.path.isdir(cache_dir):\n                os.makedirs(cache_dir)\n            cache_filename = os.path.join(cache_dir, subsection + '.cache')\n            if not os.path.isfile(cache_filename):\n                convert_old_cached_data(subsection)\n            if subsection not in caches:\n                caches[subsection] = make_cache(subsection)\n            return caches[subsection]\n\n", "CodeBleu_score": 42.503312139932326}, {"code": "def cache(subsection)\n\t\n    global caches\n    global cache_lock\n\n    if subsection in caches:\n        return caches[subsection]\n\n    cache_lock.acquire()\n\n    if not os.path.exists(CACHE_DIR):\n        os.makedirs(CACHE_DIR)\n\n    cache_file = os.path.join(CACHE_DIR, subsection + \".pkl\")\n    if not os.path.isfile(cache_file):\n        convert_old_cached_data(subsection)\n\n    if subsection not in caches:\n        caches[subsection] = make_cache(subsection)\n\n    cache_lock.release()\n    return caches[subsection]\n\n", "CodeBleu_score": 41.23087114839101}, {"code": "def cache(subsection)\n\t\n    global caches\n    global cache_lock\n    cache_object = caches.get(subsection)\n    if cache_object is None:\n        with cache_lock:\n            cache_filename = os.path.join(cache_dir, subsection + '.cache')\n            if not os.path.exists(cache_dir) or not os.path.isfile(cache_filename):\n                convert_old_cached_data(subsection)\n            cache_object = caches.get(subsection)\n            if cache_object is None:\n                cache_object = make_cache(subsection)\n                caches[subsection] = cache_object\n    return cache_object\n\n", "CodeBleu_score": 58.31244495326404}]}
{"instance_id": "ccf5ea2d9fc1", "generation_result": [{"code": "def cached_data_for_file(subsection, title, filename, func)\n\t\n    cache = existing_cache.get(subsection)\n    mtime = getmtime(filename)\n    entry = cache.get(title)\n    if entry:\n        if entry.get('mtime') < mtime:\n            cache.invalidate(title)\n            entry = None\n    if not entry:\n        data = func()\n        entry = {'data': data, 'mtime': mtime}\n        cache.set(title, entry)\n        dump_cache(subsection, cache)\n    return entry.get('data')\n\n", "CodeBleu_score": 38.33310503579847}, {"code": "def cached_data_for_file(subsection, title, filename, func)\n\t\n    cache = existing_cache.get(subsection)\n    mtime = getmtime(filename)\n    entry = cache.get(title)\n    if entry and entry.get('mtime', 0) < mtime:\n        cache.pop(title)\n        dump_cache(subsection, cache)\n        entry = None\n    if not entry:\n        data = func()\n        entry = {'mtime': mtime, 'data': data}\n        cache[title] = entry\n        dump_cache(subsection, cache)\n    return entry['data']\n\n", "CodeBleu_score": 40.23376000745494}, {"code": "def cached_data_for_file(subsection, title, filename, func)\n\t\n    cache = existing_cache.get(subsection)\n    mtime = getmtime(filename)\n    entry = cache.get(title)\n    if entry is not None:\n        if mtime > entry.get(\"mtime\"):\n            cache.invalidate(title)\n            dump_cache(cache)\n            entry = None\n    if entry is None:\n        data = func()\n        entry = {\"mtime\": mtime, \"data\": data}\n        cache.put(title, entry)\n        dump_cache(cache)\n    return entry.get(\"data\")\n\n", "CodeBleu_score": 38.84790591313204}]}
{"instance_id": "e01a2d4f0ac2", "generation_result": [{"code": "def report(message: str, *, exc_info: bool = False) -> None\n\t\n    record_exception()\n    print(\"***\", message, sep=\"", "CodeBleu_score": 13.229294953929745}, {"code": "def report(message: str, *, exc_info: bool = False) -> None\n\t\n    if exc_info:\n        record_exception()\n        traceback.print_exc()\n    print(f\"*** {message}\", file=sys.stderr)\n    if exc_info:\n        print(\"---\", file=sys.stderr)\n\n", "CodeBleu_score": 42.56422233858933}, {"code": "def report(message: str, *, exc_info: bool = False) -> None\n\t\n    record_exception()\n    print(f\"*** {message}\", file=sys.stderr)\n    if exc_info:\n        print(textwrap.indent(traceback.format_exc(), \"    \"), file=sys.stderr)\n        print(\"---\", file=sys.stderr)\n\n", "CodeBleu_score": 59.764601562137884}]}
{"instance_id": "125fb9d2320b", "generation_result": [{"code": "def extract_original_prompts(style: PromptStyle, prompt, negative_prompt)\n\t\n    # If the style's prompt and negative prompt are empty, return False with the original prompts.\n    if style.prompt == \"\" and style.negative_prompt == \"\":\n        return False, prompt, negative_prompt\n\n    # Extract style text from the positive prompt.\n    try:\n        prompt = extract_style_text_from_prompt(style, prompt)\n    except:\n        return False, prompt, negative_prompt\n\n    # Extract style text from the negative prompt.\n    try:\n        negative_prompt = extract_style_text_from_prompt(style, negative_prompt)\n    except:\n        return False, prompt, negative_prompt\n\n    # Return True with the extracted prompts.\n    return True, prompt, negative_prompt\n", "CodeBleu_score": 49.23572684330962}, {"code": "def extract_original_prompts(style: PromptStyle, prompt, negative_prompt)\n\t\n    if style.prompt == \"\" and style.negative_prompt == \"\":\n        return False, prompt, negative_prompt\n    try:\n        prompt = extract_style_text_from_prompt(style, prompt)\n    except:\n        return False, prompt, negative_prompt\n    try:\n        negative_prompt = extract_style_text_from_prompt(style, negative_prompt)\n    except:\n        return False, prompt, negative_prompt\n    return True, prompt, negative_prompt\n\n", "CodeBleu_score": 49.23572684330962}, {"code": "def extract_original_prompts(style: PromptStyle, prompt, negative_prompt)\n\t\n    if style.prompt == '' and style.negative_prompt == '':\n        return False, prompt, negative_prompt\n    positive_prompt = extract_style_text_from_prompt(style, prompt)\n    if positive_prompt is False:\n        return False, prompt, negative_prompt\n    negative_prompt = extract_style_text_from_prompt(style, negative_prompt)\n    if negative_prompt is False:\n        return False, prompt, negative_prompt\n    return True, positive_prompt, negative_prompt\n", "CodeBleu_score": 52.02606488846657}]}
{"instance_id": "0c5f729fbce2", "generation_result": [{"code": "def random_partial(self, count, n_frames)\n\t\n    if self.utterances == None:\n        self._load_utterances()\n    return [(u, u.random_partial(n_frames)) for u in self.utterance_cycler.sample(count)]\n\n\n", "CodeBleu_score": 45.22989823369231}, {"code": "def random_partial(self, count, n_frames)\n\t\n    if not self.utterances:\n        self._load_utterances()\n    utterances = self.utterance_cycler.sample(count)\n    return [(u, u.random_partial(n_frames)) for u in utterances]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 55.89879214825755}, {"code": "def random_partial(self, count, n_frames)\n\t\n    if not self.utterances:\n        self._load_utterances()\n    utterances = self.utterance_cycler.sample(count)\n    return [(u, u.random_partial(n_frames)) for u in utterances]\n\n", "CodeBleu_score": 55.89879214825755}]}
{"instance_id": "31e8b27b8f94", "generation_result": [{"code": "def normalize_volume(wav, target_dBFS, increase_only=False, decrease_only=False)\n\t\n    if increase_only and decrease_only:\n        raise ValueError(\"Both increase_only and decrease_only are set\")\n    change_in_dBFS = target_dBFS - 10 * np.log10(np.mean(wav ** 2))\n    if (change_in_dBFS < 0 and increase_only) or (change_in_dBFS > 0 and decrease_only):\n        return wav\n    return wav * (10 ** (change_in_dBFS / 20))\n", "CodeBleu_score": 84.35634890872163}, {"code": "def normalize_volume(wav, target_dBFS, increase_only=False, decrease_only=False)\n\t\n    if increase_only and decrease_only:\n        raise ValueError(\"Both increase_only and decrease_only are set\")\n    dBFS_change = target_dBFS - 10 * np.log10(np.mean(wav ** 2))\n    if (dBFS_change < 0 and increase_only) or (dBFS_change > 0 and decrease_only):\n        return wav\n    return wav * 10 ** (dBFS_change / 20)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 88.36496466130208}, {"code": "def normalize_volume(wav, target_dBFS, increase_only=False, decrease_only=False)\n\t\n    if increase_only and decrease_only:\n        raise ValueError(\"Both increase_only and decrease_only are set\")\n    dBFS_change = target_dBFS - 10 * np.log10(np.mean(wav ** 2))\n    if (dBFS_change < 0 and increase_only) or (dBFS_change > 0 and decrease_only):\n        return wav\n    return wav * (10 ** (dBFS_change / 20))\n\n", "CodeBleu_score": 92.49715443537325}]}
{"instance_id": "2353831e9033", "generation_result": [{"code": "def load_model(weights_fpath: Path, device=None)\n\t\n    if device is None:\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    global _model\n    _model = SpeakerEncoder(device)\n\n    print(\"Loading weights at {}\".format(weights_fpath))\n    _model.load_state_dict(torch.load(weights_fpath, map_location=device))\n    _model.eval()\n\n    step = _model.step\n    print(\"Loaded step {}\".format(step))\n    return step\n\n", "CodeBleu_score": 43.86268299754163}, {"code": "def load_model(weights_fpath: Path, device=None)\n\t\n    if device is None:\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    global _model\n    _model = SpeakerEncoder(device)\n    checkpoint = torch.load(weights_fpath, map_location=device)\n    _model.load_state_dict(checkpoint[\"model_state\"])\n    _model.eval()\n\n    print(\"Loaded speaker encoder \\\"{}\\\" trained to step {}\".format(\n        weights_fpath, checkpoint[\"step\"]))\n\n", "CodeBleu_score": 53.033408074225704}, {"code": "def load_model(weights_fpath: Path, device=None)\n\t\n    if device is None:\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(\"Using device: {}\".format(device))\n    _model = SpeakerEncoder(device)\n    _model.load_state_dict(torch.load(weights_fpath, map_location=device))\n    _model.eval()\n    print(\"Loaded model from step {}\".format(_model.step))\n    return _model\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 41.38834294496011}]}
{"instance_id": "3231ad480199", "generation_result": [{"code": "def embed_frames_batch(frames_batch)\n\t\n    # Check if the model is loaded\n    if _model is None:\n        raise Exception('Model not loaded.')\n\n    # Convert the input frames from a NumPy array to a PyTorch tensor\n    frames_batch = torch.from_numpy(frames_batch).to(_device)\n\n    # Perform a forward pass through the model to compute the embeddings\n    embeddings = _model.forward(frames_batch).detach().cpu().numpy()\n\n    # Return the embeddings\n    return embeddings\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 68.06526145774792}, {"code": "def embed_frames_batch(frames_batch)\n\t\n    if _model is None:\n        raise Exception('Model not loaded.')\n    frames_batch = torch.from_numpy(frames_batch).to(_device)\n    embeddings = _model.forward(frames_batch).detach().cpu().numpy()\n    return embeddings\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 68.06526145774792}, {"code": "def embed_frames_batch(frames_batch)\n\t\n    if _model is None:\n        raise Exception(\"Model not loaded.\")\n\n    frames_batch_tensor = torch.from_numpy(frames_batch).to(_device)\n    embeddings = _model.forward(frames_batch_tensor).detach().cpu().numpy()\n    return embeddings\n\n", "CodeBleu_score": 74.01414536465555}]}
{"instance_id": "117a41716c8d", "generation_result": [{"code": "def compute_partial_slices(n_samples, partial_utterance_n_frames=partials_n_frames,\n                           min_pad_coverage=0.75, overlap=0.5)\n\t\n\n    \"\"\"\n    Compute necessary partial slices for the audio to be processed.\n\n    Args:\n        n_samples (int): The number of samples in the audio to be processed.\n        partial_utterance_n_frames (int): The number of frames in each partial utterance. (Default value = partials_n_frames)\n        min_pad_coverage (float): The minimum coverage of the last partial utterance in the audio. (Default value = 0.75)\n        overlap (float): The amount of overlap between partial utterances. (Default value = 0.5)\n\n    Returns:\n        (tuple): A tuple containing:\n\n        **wav_slices** (list): A list of tuples representing the start and end points of the wav slices.\n\n        **mel_slices** (list): A list of tuples representing the start and end points of the mel slices.\n    \"\"\"\n    assert 0.0 <= overlap < 1.0, \"Overlap must be between 0 and 1\"\n    assert 0 <= min_pad_coverage <= 1.0, \"Padding Coverage must be between 0 and 1\"\n\n    samples_per_frame = int((sampling_rate / 1000) * partial_utterance_n_frames)\n    frame_step = int(samples_per_frame * (1 - overlap))\n    n_frames = int(np.ceil(float(n_samples) / frame_step))\n\n    wav_slices = [(i * frame_step, (i + 1) * frame_step) for i in range(n_frames)]\n    mel_slices = [(i * frame_step, (i + 1) * frame_step) for i in range(n_frames)]\n\n    last_wav_slice = wav_slices[-1]\n    coverage = float(last_wav_slice[1] - last_wav_slice[0]) / float(n_samples)\n    if coverage < min_pad_coverage and len(wav_slices) > 1:\n        wav_slices = wav_slices[:-1]\n        mel_s", "CodeBleu_score": 37.5191050493599}, {"code": "def compute_partial_slices(n_samples, partial_utterance_n_frames=partials_n_frames,\n                           min_pad_coverage=0.75, overlap=0.5)\n\t\n    assert 0 <= overlap < 1\n    assert 0 < partial_utterance_n_frames\n    assert 0 <= min_pad_coverage <= 1\n\n    frame_step = int(sampling_rate * overlap)\n    n_frames = int(np.ceil(n_samples / frame_step))\n\n    wav_slices = list()\n    mel_slices = list()\n\n    for i in range(max(1, n_frames - 1)):\n        mel_range = np.array([i * frame_step, (i + 1) * frame_step])\n        wav_range = mel_range * sampling_rate / 1000\n        mel_slices.append(mel_range)\n        wav_slices.append(wav_range)\n\n    last_wav_range = wav_slices[-1]\n    coverage = (n_samples - last_wav_range[0]) / (last_wav_range[1] - last_wav_range[0])\n    if coverage < min_pad_coverage and len(mel_slices) > 1:\n        mel_slices = mel_slices[:-1]\n        wav_slices = wav_slices[:-1]\n\n    return wav_slices, mel_slices\n\n", "CodeBleu_score": 48.55298164544005}, {"code": "def compute_partial_slices(n_samples, partial_utterance_n_frames=partials_n_frames,\n                           min_pad_coverage=0.75, overlap=0.5)\n\t\n    assert 0.0 < overlap < 1.0, \"overlap should be between 0 and 1\"\n    assert 0.0 <= min_pad_coverage <= 1.0, \"min_pad_coverage should be between 0 and 1\"\n    assert partial_utterance_n_frames % 2 == 1, \"partial_utterance_n_frames should be odd\"\n\n    frame_step = int((1.0 - overlap) * partial_utterance_n_frames)\n    n_frames = int(np.ceil(n_samples / hparams.hop_length))\n    n_partials = int(np.ceil((n_frames - partial_utterance_n_frames) / frame_step)) + 1\n\n    wav_slices = []\n    mel_slices = []\n\n    for i in range(n_partials):\n        start = i * frame_step\n        end = start + partial_utterance_n_frames\n        wav_slices.append(slice(start, end))\n        mel_slices.append(slice(start, end))\n\n    coverage = wav_slices[-1].stop * hparams.hop_length / n_samples\n    if coverage < min_pad_coverage and len(mel_slices) > 1:\n        wav_slices = wav_slices[:-1]\n        mel_slices = mel_slices[:-1]\n\n    return wav_slices, mel_slices\n\n", "CodeBleu_score": 39.591194257320325}]}
{"instance_id": "34fe5c864f1c", "generation_result": [{"code": "def plot_embedding_as_heatmap(embed, ax=None, title=\"\", shape=None, color_range=(0, 0.30))\n\t\n    # Check if an Axes object is provided\n    if ax is None:\n        ax = plt.gca()\n\n    # If the shape is not specified, calculate it\n    if shape is None:\n        shape = (int(np.sqrt(len(embed))), int(np.sqrt(len(embed))))\n\n    # Reshape the embedding\n    embed = embed.reshape(shape)\n\n    # Set the colormap\n    cmap = cm.get_cmap(\"coolwarm\")\n\n    # Set the limits of the color range\n    sm = cm.ScalarMappable(cmap=cmap)\n    sm.set_clim(vmin=color_range[0], vmax=color_range[1])\n\n    # Plot the embedding as a heatmap\n    ax.imshow(embed, cmap=cmap)\n\n    # Add a color bar\n    cbar = ax.figure.colorbar(sm)\n\n    # Remove the x and y ticks\n    ax.set_xticks([])\n    ax.set_yticks([])\n\n    # Set the title\n    ax.set_title(title)\n\n    return ax\n\n", "CodeBleu_score": 61.75416280630526}, {"code": "def plot_embedding_as_heatmap(embed, ax=None, title=\"\", shape=None, color_range=(0, 0.30))\n\t\n    # Check if an Axes object is provided\n    if ax is None:\n        ax = plt.gca()\n\n    # If the shape of the embedding is not specified, calculate it\n    if shape is None:\n        shape = int(np.sqrt(len(embed)))\n        embed = embed.reshape(shape, shape)\n\n    # Set the colormap\n    cmap = cm.get_cmap(\"viridis\", 256)\n\n    # Set the limits of the colorbar\n    sm = cm.ScalarMappable(cmap=cmap)\n    sm.set_clim(color_range[0], color_range[1])\n\n    # Plot the heatmap\n    ax.set_title(title)\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.imshow(embed, cmap=cmap)\n\n    # Add the colorbar\n    plt.colorbar(sm)\n\n", "CodeBleu_score": 56.87679261401062}, {"code": "def plot_embedding_as_heatmap(embed, ax=None, title=\"\", shape=None, color_range=(0, 0.30))\n\t\n    if ax is None:\n        ax = plt.gca()\n\n    if shape is None:\n        shape = int(np.sqrt(embed.shape[0]))\n        embed = embed.reshape(shape, shape)\n\n    cmap = cm.get_cmap('RdBu')\n    sm = ax.imshow(embed, cmap=cmap, interpolation='nearest')\n    sm.set_clim(color_range)\n    cbar = plt.colorbar(sm)\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.set_title(title)\n    return ax\n\n", "CodeBleu_score": 54.026831730882186}]}
{"instance_id": "92e93d50b62b", "generation_result": [{"code": "def similarity_matrix(self, embeds)\n\t\n    n_speakers = embeds.shape[0]\n    n_utterances = embeds.shape[1]\n    centroids_incl = torch.zeros(n_speakers, n_utterances, self.embedding_size).to(self.device)\n    centroids_excl = torch.zeros(n_speakers, self.embedding_size).to(self.device)\n    for spk_idx in range(n_speakers):\n        for utt_idx in range(n_utterances):\n            centroids_incl[spk_idx, utt_idx, :] = torch.mean(embeds[spk_idx, :utt_idx+1, :], dim=0)\n            centroids_excl[spk_idx, :] = torch.mean(embeds[spk_idx, utt_idx+1:, :], dim=0)\n    centroids_incl = torch.nn.functional.normalize(centroids_incl, p=2, dim=2)\n    centroids_excl = torch.nn.functional.normalize(centroids_excl, p=2, dim=1)\n    sim_matrix = torch.zeros(n_speakers, n_speakers).to(self.device)\n    mask = np.eye(n_speakers) == 0\n    for spk_idx in range(n_speakers):\n        sim_matrix[spk_idx, :] = torch.sum(centroids_incl[spk_idx, :, :] * centroids_excl, dim=1)\n    sim_matrix = sim_matrix * mask\n    sim_matrix = sim_matrix * self.scale + self.bias\n    return sim_matrix\n\n", "CodeBleu_score": 34.92385102934277}, {"code": "def similarity_matrix(self, embeds)\n\t\n    num_speakers = embeds.shape[0]\n    num_utterances = embeds.shape[1]\n\n    centroids_incl = torch.zeros((num_speakers, num_utterances, self.embedding_size), device=self.device)\n    centroids_excl = torch.zeros((num_speakers, self.embedding_size), device=self.device)\n\n    for i in range(num_speakers):\n        centroids_incl[i] = torch.mean(embeds[i], dim=0, keepdim=True)\n        centroids_excl[i] = torch.mean(embeds[i], dim=0)\n\n    centroids_incl = F.normalize(centroids_incl, p=2, dim=2)\n    centroids_excl = F.normalize(centroids_excl, p=2, dim=1)\n\n    sim_matrix = torch.zeros((num_speakers, num_utterances, num_utterances), device=self.device)\n    mask = np.eye(num_utterances) == 1\n\n    for i in range(num_speakers):\n        sim_matrix[i] = torch.matmul(centroids_incl[i], centroids_excl[i].T)\n        sim_matrix[i][mask] = -1e9\n\n    sim_matrix = sim_matrix * self.scale + self.bias\n\n    return sim_matrix\n\n", "CodeBleu_score": 36.00135230169567}, {"code": "def similarity_matrix(self, embeds)\n\t\n    n_speakers = embeds.shape[0]\n    n_utterances = embeds.shape[1]\n    centroids_incl = torch.zeros(n_speakers, n_utterances, self.embedding_dim).to(self.device)\n    centroids_excl = torch.zeros(n_speakers, self.embedding_dim).to(self.device)\n    for i in range(n_speakers):\n        centroids_incl[i] = torch.mean(embeds[i], dim=0)\n        centroids_excl[i] = torch.mean(embeds[i], dim=0)\n    centroids_incl = centroids_incl / torch.norm(centroids_incl, dim=1, keepdim=True)\n    centroids_excl = centroids_excl / torch.norm(centroids_excl, dim=1, keepdim=True)\n    similarity_matrix = torch.zeros(n_speakers, n_speakers).to(self.device)\n    mask = np.eye(n_speakers)\n    for i in range(n_speakers):\n        sim_scores = torch.matmul(centroids_incl[i], centroids_excl.T)\n        similarity_matrix[i] = sim_scores\n    similarity_matrix = similarity_matrix * (1 - torch.Tensor(mask).to(self.device))\n    similarity_matrix = similarity_matrix + torch.Tensor(mask).to(self.device)\n    similarity_matrix = self.scale * similarity_matrix + self.bias\n    return similarity_matrix\n\n", "CodeBleu_score": 35.52601733703785}]}
{"instance_id": "a6eba54108a1", "generation_result": [{"code": "def loss(self, embeds)\n\t\n    # Get the number of speakers and utterances per speaker from the shape of the embeddings\n    num_speakers, num_utterances = embeds.shape[0], embeds.shape[1]\n\n    # Compute the similarity matrix\n    sim_matrix = self.similarity_matrix(embeds)\n\n    # Reshape the similarity matrix\n    sim_matrix = sim_matrix.reshape(num_speakers * num_utterances, num_speakers * num_utterances)\n\n    # Generate ground truth labels\n    labels = torch.from_numpy(np.repeat(np.arange(num_speakers), num_utterances)).long().to(self.device)\n\n    # Compute the loss\n    loss = self.loss_fn(sim_matrix, labels)\n\n    # Compute the EER without backpropagation\n    with torch.no_grad():\n        preds = inv_argmax(sim_matrix.detach())\n        fpr, tpr, thresholds = roc_curve(labels.flatten().cpu().numpy(), preds.flatten().cpu().numpy())\n        eer = brentq(lambda x: 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n        eer = float(eer)\n\n    return loss, eer\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 44.707653506592735}, {"code": "def loss(self, embeds)\n\t\n    # Get the number of speakers and utterances per speaker from the shape of the input embeddings.\n    num_speakers = embeds.shape[0]\n    num_utterances = embeds.shape[1]\n\n    # Compute the similarity matrix.\n    sim_matrix = self.similarity_matrix(embeds)\n    # Reshape the similarity matrix so that it can be used for calculating the loss.\n    sim_matrix = sim_matrix.reshape(num_speakers * num_utterances, num_speakers * num_utterances)\n\n    # Generate the ground truth labels.\n    labels = torch.from_numpy(np.repeat(np.arange(num_speakers), num_utterances)).long().to(self.device)\n\n    # Compute the loss.\n    loss = self.loss_fn(sim_matrix, labels)\n\n    # Compute the Equal Error Rate (EER).\n    with torch.no_grad():\n        # Get the predicted labels.\n        preds = inv_argmax(sim_matrix.detach().cpu().numpy())\n        # Get the true labels.\n        true_labels = labels.detach().cpu().numpy()\n        # Compute the false positive rates and true positive rates for all possible thresholds.\n        fpr, tpr, thresholds = roc_curve(true_labels, preds)\n        # Get the EER by finding the threshold where the false positive rate equals the false negative rate.\n        eer = brentq(lambda x: 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n        # Get the threshold corresponding to the EER.\n        eer_threshold = interp1d(fpr, thresholds)(eer)\n\n    return loss, eer, eer_threshold\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 44.91217058728626}, {"code": "def loss(self, embeds)\n\t\n    # Determine number of speakers and utterances per speaker\n    num_spks = embeds.shape[0]\n    num_utts = embeds.shape[1]\n\n    # Compute similarity matrix\n    sim_matrix = self.similarity_matrix(embeds, embeds)\n    sim_matrix = sim_matrix.reshape(num_spks * num_utts, num_spks * num_utts)\n\n    # Generate ground truth labels\n    labels = torch.from_numpy(np.repeat(np.arange(num_spks), num_utts))\n    labels = torch.cat((labels, labels), 0)\n\n    # Compute loss\n    with torch.no_grad():\n        preds = self.loss_fn(sim_matrix, labels)\n        preds = preds.flatten()\n        labels = labels.flatten()\n        eer = self.compute_eer(preds, labels)\n    loss = self.loss_fn(sim_matrix, labels)\n    return loss, eer\n\n", "CodeBleu_score": 25.412668287446948}]}
{"instance_id": "ac7b74b9d1b1", "generation_result": [{"code": "def log_params(self)\n\t\n    if not self.disable_logging:\n        self.logger.log_params(self.params, self.model_params, self.data_params)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 3.096903815868634}, {"code": "def log_params(self)\n\t\n    if self.log_params:\n        # get model parameters\n        model_params = inspect.signature(self.model).parameters\n        model_params = {key: val.default for key, val in model_params.items() if key != 'self'}\n        # get data parameters\n        data_params = inspect.signature(self.data).parameters\n        data_params = {key: val.default for key, val in data_params.items() if key != 'self'}\n        # format parameters\n        params = []\n        for key, val in {**model_params, **data_params}.items():\n            params.append(f\"{key}: {val}\")\n        params = '", "CodeBleu_score": 20.228098330380345}, {"code": "def log_params(self)\n\t\n\n    if not self.log_parameters:\n        return\n\n    # imports\n    import sys\n    from IPython.display import display, HTML\n\n    # get parameters\n    params = self.get_params()\n    model_params = params[\"model_params\"]\n    data_params = params[\"data_params\"]\n\n    # format string\n    s = \"<h3>Parameters</h3><hr>\"\n    s += \"<h4>Model Parameters</h4>\"\n    s += \"<table>\"\n    s += \"<tr><th>Name</th><th>Value</th></tr>\"\n    for k, v in model_params.items():\n        s += \"<tr><td>{}</td><td>{}</td></tr>\".format(k, v)\n    s += \"</table>\"\n    s += \"<h4>Data Parameters</h4>\"\n    s += \"<table>\"\n    s += \"<tr><th>Name</th><th>Value</th></tr>\"\n    for k, v in data_params.items():\n        s += \"<tr><td>{}</td><td>{}</td></tr>\".format(k, v)\n    s += \"</table>\"\n\n    # display\n    display(HTML(s))\n\n", "CodeBleu_score": 30.742805030601932}]}
{"instance_id": "4726b7864386", "generation_result": [{"code": "def draw_projections(self, embeds, utterances_per_speaker, step, out_fpath=None, max_speakers=10)\n\t\n    # Limit the number of speakers\n    speakers = list(embeds.keys())[:max_speakers]\n    embeds = {k: embeds[k] for k in speakers}\n    utterances_per_speaker = {k: utterances_per_speaker[k] for k in speakers}\n\n    # Flatten the embeddings and reduce dimensionality\n    embeds = np.array([embed for speaker_embeds in embeds.values() for embed in speaker_embeds])\n    reducer = umap.UMAP(n_neighbors=10, min_dist=0.1, metric=\"cosine\")\n    embeds = reducer.fit_transform(embeds)\n\n    # Create the plot\n    fig, ax = plt.subplots(figsize=(16, 10))\n    ax.set_aspect(\"equal\", \"datalim\")\n    ax.set_title(f\"Step {step}\")\n    for speaker, speaker_embeds in embeds.items():\n        # Assign a color to each speaker\n        color = np.random.rand(3)\n        ax.scatter(\n            speaker_embeds[:, 0],\n            speaker_embeds[:, 1],\n            label=speaker,\n            c=color,\n            alpha=0.5,\n            edgecolors=\"none\",\n        )\n        for i, utterance in enumerate(utterances_per_speaker[speaker]):\n            ax.annotate(\n                utterance,\n                (speaker_embeds[i, 0], speaker_embeds[i, 1]),\n                alpha=0.8,\n                color=\"white\",\n            )\n\n    # Draw the legend outside the plot\n    box = ax.get_position()\n    ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])\n    ax.legend(loc=\"center left\", bbox_to_anchor=(1, 0.5), fontsize=16)\n\n    # Update or save the plot\n    if out_fpath is None:\n        plt.show()\n    else:\n        plt", "CodeBleu_score": 38.98926352667005}, {"code": "def draw_projections(self, embeds, utterances_per_speaker, step, out_fpath=None, max_speakers=10)\n\t\n    # Limit the number of speakers\n    if len(embeds) > max_speakers:\n        embeds = embeds[:max_speakers]\n        utterances_per_speaker = utterances_per_speaker[:max_speakers]\n\n    # Flatten the embeddings\n    embeds = np.concatenate(embeds)\n\n    # Assign colors based on speaker identity\n    colors = np.repeat(np.arange(len(embeds)), utterances_per_speaker)\n\n    # Reduce the dimensionality of the embeddings\n    reducer = umap.UMAP()\n    embeds = reducer.fit_transform(embeds)\n\n    # Plot the embeddings\n    fig, ax = plt.subplots(figsize=(10, 10))\n    plt.scatter(embeds[:, 0], embeds[:, 1], c=colors, cmap=\"viridis\", alpha=0.5)\n\n    # Set the aspect ratio to be equal\n    ax.set_aspect(\"equal\")\n\n    # Set the title\n    plt.title(f\"Step: {step}\")\n\n    # Save the plot if a file path is provided\n    if out_fpath:\n        plt.savefig(out_fpath)\n\n    # Clear the current figure\n    plt.clf()\n\n    # Show the plot if enabled\n    if self.visualize:\n        plt.show()\n", "CodeBleu_score": 41.75016870148935}, {"code": "def draw_projections(self, embeds, utterances_per_speaker, step, out_fpath=None, max_speakers=10)\n\t\n    # Limit the number of speakers\n    speakers = list(embeds.keys())[:max_speakers]\n    # Flatten the embeddings and limit to the specified number of speakers\n    embeds_flat = np.array([embed for speaker, embed in embeds.items() if speaker in speakers])\n    # Assign colors based on speaker identity\n    colors = np.repeat(np.arange(len(speakers)), utterances_per_speaker)\n    # Reduce the dimensionality of the embeddings\n    reducer = umap.UMAP(random_state=1)\n    projections = reducer.fit_transform(embeds_flat)\n    # Plot the embeddings\n    fig, ax = plt.subplots()\n    ax.scatter(projections[:, 0], projections[:, 1], c=colors, s=1)\n    ax.set_title(f\"Step {step}\")\n    plt.savefig(out_fpath)\n    plt.clf()\n    plt.close()\n\n", "CodeBleu_score": 36.720916468831355}]}
{"instance_id": "274fa50985c0", "generation_result": [{"code": "def add_speaker_embedding(self, x, speaker_embedding)\n\t\n    # save dimensions of input tensor as human-readable names\n    batch_size, sequence_length, hidden_size = x.size()\n    # determine appropriate dimension index for speaker embedding\n    if self.speaker_embedding_dim == 'sequence':\n        dimension_index = 1\n    elif self.speaker_embedding_dim == 'hidden':\n        dimension_index = 2\n    # repeat speaker embedding to match length of input text\n    speaker_embedding = speaker_embedding.repeat_interleave(sequence_length, dim=0)\n    # reshape and transpose speaker embedding\n    speaker_embedding = speaker_embedding.reshape(batch_size, sequence_length, -1).transpose(1, 2)\n    # concatenate speaker embedding with input tensor along last dimension\n    x = torch.cat([x, speaker_embedding], dim=dimension_index)\n    return x\n\n", "CodeBleu_score": 36.49632753404328}, {"code": "def add_speaker_embedding(self, x, speaker_embedding)\n\t\n    # save the dimensions of the input tensor as human-readable names\n    batch_size, sequence_length, embedding_dim = x.size()\n    # determine the appropriate dimension index for the speaker embedding\n    if self.speaker_embedding_dim == 1:\n        speaker_embedding_dim_index = 1\n    elif self.speaker_embedding_dim == 2:\n        speaker_embedding_dim_index = 2\n    # repeat the speaker embedding to match the length of the input text\n    speaker_embedding = speaker_embedding.repeat_interleave(sequence_length, dim=0)\n    # reshape and transpose the speaker embedding\n    speaker_embedding = speaker_embedding.reshape(batch_size, sequence_length, -1).transpose(1, 2)\n    # concatenate the speaker embedding with the input tensor along the last dimension\n    x = torch.cat([x, speaker_embedding], dim=speaker_embedding_dim_index)\n    return x\n\n", "CodeBleu_score": 20.401978373107692}, {"code": "def add_speaker_embedding(self, x, speaker_embedding)\n\t\n    # Save the dimensions of the input tensor as human-readable names\n    batch_size, sequence_length, embedding_size = x.size()\n    # Determine the appropriate dimension index for the speaker embedding\n    speaker_embedding_dim = 1 if self.speaker_embedding_first else 2\n    # Repeat the speaker embedding to match the length of the input text\n    speaker_embedding = speaker_embedding.repeat_interleave(sequence_length, dim=0)\n    # Reshape and transpose the speaker embedding\n    speaker_embedding = speaker_embedding.reshape(batch_size, sequence_length, -1).transpose(speaker_embedding_dim, 2)\n    # Concatenate the speaker embedding with the input tensor along the last dimension\n    x = torch.cat([x, speaker_embedding], dim=2)\n    # Return the result\n    return x\n\n", "CodeBleu_score": 21.28665678233847}]}
{"instance_id": "e6742ea0fe5e", "generation_result": [{"code": "def init_attention(self, encoder_seq_proj)\n\t\n    self.device = next(self.parameters()).device\n    self.encoder_seq_proj = encoder_seq_proj\n    self.batch_size, self.seq_len, self.num_features = encoder_seq_proj.size()\n    self.cumulative_alpha = torch.zeros(self.batch_size, self.seq_len).to(self.device)\n    self.alpha = torch.zeros(self.batch_size, self.seq_len).to(self.device)\n\n", "CodeBleu_score": 17.713505327566413}, {"code": "def init_attention(self, encoder_seq_proj)\n\t\n    self.device = next(self.parameters()).device\n    self.seq_len = encoder_seq_proj.size(1)\n    self.cumulative_weights = torch.zeros(\n        self.seq_len, device=self.device, dtype=torch.float32\n    )\n    self.attention_weights = torch.zeros(\n        self.seq_len, device=self.device, dtype=torch.float32\n    )\n", "CodeBleu_score": 25.74007343102831}, {"code": "def init_attention(self, encoder_seq_proj)\n\t\n    device = next(self.parameters()).device\n    self.attention_hidden = torch.zeros((encoder_seq_proj.size(0), encoder_seq_proj.size(1), self.hidden_size), device=device)\n    self.attention_cell = torch.zeros((encoder_seq_proj.size(0), encoder_seq_proj.size(1), self.hidden_size), device=device)\n    self.attention_weights = torch.zeros((encoder_seq_proj.size(0), encoder_seq_proj.size(1), encoder_seq_proj.size(2)), device=device)\n    self.attention_weights_cum = torch.zeros((encoder_seq_proj.size(0), encoder_seq_proj.size(1), encoder_seq_proj.size(2)), device=device)\n    self.attention_context = torch.zeros((encoder_seq_proj.size(0), encoder_seq_proj.size(1), encoder_seq_proj.size(2)), device=device)\n    self.attention_weights_cum = self.attention_weights_cum.to(device)\n\n", "CodeBleu_score": 17.01290530032381}]}
{"instance_id": "4e1fc1ed0ed7", "generation_result": [{"code": "def load(self, path, optimizer=None)\n\t\n    path = Path(path)\n    if path.suffix == '.pt':\n        checkpoint = torch.load(path, map_location=self.device)\n    else:\n        checkpoint = torch.load(path)\n    self.load_state_dict(checkpoint['state_dict'])\n    if optimizer is not None:\n        optimizer.load_state_dict(checkpoint['optimizer'])\n    return checkpoint\n\n", "CodeBleu_score": 46.469725008184106}, {"code": "def load(self, path, optimizer=None)\n\t\n    device = next(self.parameters()).device\n    checkpoint = torch.load(path, map_location=device)\n    self.load_state_dict(checkpoint['model_state_dict'])\n    if optimizer is not None:\n        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n    return checkpoint\n\n", "CodeBleu_score": 38.940842827054844}, {"code": "def load(self, path, optimizer=None)\n\t\n    checkpoint = torch.load(path, map_location=self.parameters().__next__().device)\n    self.load_state_dict(checkpoint['model'])\n    if optimizer is not None:\n        optimizer.load_state_dict(checkpoint['optimizer'])\n\n", "CodeBleu_score": 39.35838813522624}]}
{"instance_id": "178e6aa16c0f", "generation_result": [{"code": "def save(self, path, optimizer=None)\n\t\n    torch.save(self.state_dict(), path)\n    if optimizer is not None:\n        torch.save(optimizer.state_dict(), path + \".opt\")\n", "CodeBleu_score": 40.94133051879746}, {"code": "def save(self, path, optimizer=None)\n\t\n    torch.save(self.state_dict(), path)\n    if optimizer:\n        torch.save(optimizer.state_dict(), path.replace(\".pt\", \"_optimizer.pt\"))\n", "CodeBleu_score": 35.89293507318129}, {"code": "def save(self, path, optimizer=None)\n\t\n    torch.save(self.state_dict(), path)\n    if optimizer is not None:\n        torch.save(optimizer.state_dict(), path + '.opt')\n\n", "CodeBleu_score": 40.89826482011996}]}
{"instance_id": "6528fd80e87b", "generation_result": [{"code": "def english_cleaners(text)\n\t\n    text = convert_to_ascii(text)\n    text = lowercase(text)\n    text = expand_numbers(text)\n    text = expand_abbreviations(text)\n    text = collapse_whitespace(text)\n    return text\n\n", "CodeBleu_score": 55.88611500274403}, {"code": "def english_cleaners(text)\n\t\n    text = text.encode('ascii', 'ignore').decode('ascii')\n    text = text.lower()\n    text = expand_abbreviations(text)\n    text = expand_numbers(text)\n    text = collapse_whitespace(text)\n    return text\n\n", "CodeBleu_score": 69.72132722718946}, {"code": "def english_cleaners(text)\n\t\n    text = convert_to_ascii(text)\n    text = lowercase(text)\n    text = expand_numbers(text)\n    text = expand_abbreviations(text)\n    text = collapse_whitespace(text)\n    return text\n\n", "CodeBleu_score": 55.88611500274403}]}
{"instance_id": "8a4fe08e0985", "generation_result": [{"code": "def text_to_sequence(text, cleaner_names)\n\t\n    sequence = []\n    # Check for curly braces and treat their contents as ARPAbet:\n    while len(text):\n        m = _curly_re.match(text)\n        if not m:\n            sequence += _symbols_to_sequence(_clean_text(text, cleaner_names))\n            break\n        sequence += _arpabet_to_sequence(m.group(1))\n        text = m.group(2)\n    # Append EOS token\n    sequence.append(_symbols.index(_eos))\n    return sequence\n", "CodeBleu_score": 59.41736361892953}, {"code": "def text_to_sequence(text, cleaner_names)\n\t\n    sequence = []\n    # Check for curly braces and treat their contents as ARPAbet:\n    while len(text):\n        m = _curly_re.match(text)\n        if not m:\n            sequence += _symbols_to_sequence(_clean_text(text, cleaner_names))\n            break\n        sequence += _arpabet_to_sequence(m.group(1))\n        text = m.group(2)\n    # Append EOS token\n    sequence.append(_symbols.index(_eos))\n    return sequence\n", "CodeBleu_score": 59.41736361892953}, {"code": "def text_to_sequence(text, cleaner_names)\n\t\n    sequence = []\n    # Check for curly braces and treat their contents as ARPAbet:\n    while len(text):\n        m = _curly_re.match(text)\n        if not m:\n            sequence += _symbols_to_sequence(_clean_text(text, cleaner_names))\n            break\n        sequence += _arpabet_to_sequence(m.group(1))\n        text = m.group(2)\n    # Append EOS token\n    sequence.append(_symbols.index(_eos))\n    return sequence\n", "CodeBleu_score": 59.41736361892953}]}
{"instance_id": "e1c25a37af1f", "generation_result": [{"code": "def sequence_to_text(sequence)\n\t\n    result = \"\"\n    for symbol_id in sequence:\n        if symbol_id in _arpabet_to_symbol:\n            result += \"{\" + _arpabet_to_symbol[symbol_id] + \"}\"\n        else:\n            result += _symbol_to_arpabet[symbol_id]\n    result = result.replace(\"}{\", \" \")\n    return result\n\n", "CodeBleu_score": 43.83918950321678}, {"code": "def sequence_to_text(sequence)\n\t\n    result = \"\"\n    for symbol_id in sequence:\n        if symbol_id in _arpabet:\n            result += \"{%s}\" % _arpabet[symbol_id]\n        else:\n            result += _symbols[symbol_id]\n    result = result.replace(\"}{\", \" \")\n    return result\n\n", "CodeBleu_score": 45.288812886399434}, {"code": "def sequence_to_text(sequence)\n\t\n    result = \"\"\n    for symbol_id in sequence:\n        if symbol_id in _symbol_to_id:\n            symbol = _id_to_symbol[symbol_id]\n            if len(symbol) > 1:\n                symbol = \"{%s}\" % symbol\n            result += symbol\n    result = result.replace(\"}{\", \" \")\n    return result.strip()\n\n", "CodeBleu_score": 48.58972541787324}]}
{"instance_id": "7807cad4b05d", "generation_result": [{"code": "def embed_utterance(fpaths, encoder_model_fpath)\n\t\n    if not encoder.is_loaded():\n        encoder.load_model(encoder_model_fpath)\n\n    wav_fpaths = to_absolute_paths(fpaths)\n    for wav_fpath in wav_fpaths:\n        wav = encoder.preprocess_wav(wav_fpath)\n        embed = encoder.embed_utterance(wav)\n        embed_fpath = wav_fpath.replace(\".wav\", \".npy\")\n        np.save(embed_fpath, embed, allow_pickle=False)\n", "CodeBleu_score": 48.66444990688614}, {"code": "def embed_utterance(fpaths, encoder_model_fpath)\n\t\n    if not encoder.is_loaded():\n        encoder.load_model(encoder_model_fpath)\n    preprocessed_wav = encoder.preprocess_wav(fpath)\n    return encoder.embed_utterance(preprocessed_wav)\n\n", "CodeBleu_score": 40.18419993830226}, {"code": "def embed_utterance(fpaths, encoder_model_fpath)\n\t\n    if not encoder.is_loaded():\n        encoder.load_model(encoder_model_fpath)\n\n    wav_fpath, embed_fpath = fpaths\n    wav = encoder.preprocess_wav(wav_fpath)\n    embed = encoder.embed_utterance(wav)\n    np.save(embed_fpath, embed, allow_pickle=False)\n\n", "CodeBleu_score": 76.77045343010442}]}
{"instance_id": "9f860273a325", "generation_result": [{"code": "def create_embeddings(synthesizer_root: Path, encoder_model_fpath: Path, n_processes: int)\n\t\n    wav_dir = synthesizer_root.joinpath(\"audio\")\n    assert wav_dir.exists()\n    embed_dir = synthesizer_root.joinpath(\"embeds\")\n    embed_dir.mkdir(exist_ok=True)\n    metadata_fpath = synthesizer_root.joinpath(\"train.txt\")\n    assert metadata_fpath.exists()\n    with metadata_fpath.open(\"r\") as metadata_file:\n        metadata = [line.split(\"|\") for line in metadata_file]\n        fpaths = [m[0] for m in metadata]\n        text = [m[1] for m in metadata]\n        assert len(fpaths) == len(text)\n    missing_audio_files = [f for f in fpaths if not wav_dir.joinpath(f).exists()]\n    assert len(missing_audio_files) == 0\n    print(\"Loaded metadata for %d audio files\" % len(fpaths))\n    encoder = Encoder(encoder_model_fpath)\n    with Pool(n_processes) as pool:\n        # TODO: fix the type of embed_fpaths\n        embed_fpaths = list(\n            tqdm(\n                pool.imap(\n                    partial(\n                        embed_utterance,\n                        encoder=encoder,\n                        wav_dir=wav_dir,\n                        embed_dir=embed_dir,\n                    ),\n                    fpaths,\n                ),\n                total=len(fpaths),\n            )\n        )\n    return embed_fpaths\n\n", "CodeBleu_score": 56.583211449340844}, {"code": "def create_embeddings(synthesizer_root: Path, encoder_model_fpath: Path, n_processes: int)\n\t\n    ########################################################################################################################\n    # TODO 1: Verify the existence of directories and files.\n    ########################################################################################################################\n    # Check whether the synthesizer_root directory exists.\n    # If not, raise an error.\n    if not synthesizer_root.exists():\n        raise ValueError(\"synthesizer_root does not exist.\")\n\n    # Check whether the encoder_model_fpath file exists.\n    # If not, raise an error.\n    if not encoder_model_fpath.exists():\n        raise ValueError(\"encoder_model_fpath does not exist.\")\n\n    # Check whether the metadata file exists.\n    # If not, raise an error.\n    metadata_fpath = synthesizer_root.joinpath(\"train.txt\")\n    if not metadata_fpath.exists():\n        raise ValueError(\"metadata_fpath does not exist.\")\n\n    # Check whether the wav directory exists.\n    # If not, raise an error.\n    wav_dir = synthesizer_root.joinpath(\"audio\")\n    if not wav_dir.exists():\n        raise ValueError(\"wav_dir does not exist.\")\n\n    # Check whether the embed directory exists.\n    # If not, create it.\n    embed_dir = synthesizer_root.joinpath(\"embeds\")\n    if not embed_dir.exists():\n        embed_dir.mkdir()\n\n    ########################################################################################################################\n    # TODO 2: Prepare file paths for audio and embeddings.\n    ########################################################################################################################\n    # Read metadata file.\n    # The file contains lines of the form \"<file_id>|<text>\".\n    # We only need the file_id part.\n    with metadata_fpath.open(\"r\") as metadata_file:\n        metadata = [line.split(\"|\")[0] for line in metadata_file]\n\n    # Prepare a list of tuples of the form (audio file path, embedding file path).\n    file_paths = [(wav_dir.joinpath(f\"{file_id}.wav\"), embed_dir.join", "CodeBleu_score": 43.383476305298416}, {"code": "def create_embeddings(synthesizer_root: Path, encoder_model_fpath: Path, n_processes: int)\n\t\n    wav_dir = synthesizer_root.joinpath(\"audio\")\n    assert wav_dir.exists()\n    metadata_fpath = synthesizer_root.joinpath(\"train.txt\")\n    assert metadata_fpath.exists()\n    embed_dir = synthesizer_root.joinpath(\"embeds\")\n    embed_dir.mkdir(exist_ok=True)\n    print(\"embed_dir:\", embed_dir)\n\n    # Gather the input audio filepath and the target output embed filepath\n    with metadata_fpath.open(\"r\", encoding=\"utf-8\") as metadata_file:\n        metadata = [line.split(\"|\") for line in metadata_file]\n        fpaths = [wav_dir.joinpath(m[0]) for m in metadata]\n        embed_fpaths = [embed_dir.joinpath(m[0].replace(\".wav\", \".npy\")) for m in metadata]\n\n    # TODO: improve on the multiprocessing, it's terrible. Disk I/O is the bottleneck here.\n    # Embed the utterances in separate threads\n    func = partial(embed_utterance, encoder_model_fpath)\n    job = Pool(n_processes).imap(func, tqdm(zip(fpaths, embed_fpaths)))\n    list(job)\n\n", "CodeBleu_score": 69.06420790585425}]}
{"instance_id": "9eb0abda14a9", "generation_result": [{"code": "def eval_model(attention, mel_prediction, target_spectrogram, input_seq, step,\n               plot_dir, mel_output_dir, wav_dir, sample_num, loss, hparams)\n\t\n    # save attention plot\n    attention_plot = plot_attention(attention, input_seq, hparams)\n    attention_plot.savefig(plot_dir.joinpath('attention_step_{}_{}.png'.format(step, sample_num)))\n    plt.close(attention_plot)\n\n    # save predicted mel spectrogram plot\n    mel_output_plot = plot_spectrogram(mel_prediction, hparams)\n    mel_output_plot.savefig(plot_dir.joinpath('mel_prediction_step_{}_{}.png'.format(step, sample_num)))\n    plt.close(mel_output_plot)\n\n    # save predicted wav file\n    wav = audio.inv_mel_spectrogram(mel_prediction.T, hparams)\n    audio.save_wav(wav, wav_dir.joinpath('step-{}-wave-{}.wav'.format(step, sample_num)), hparams)\n\n    # save reconstructed wav file\n    wav = audio.inv_mel_spectrogram(target_spectrogram.T, hparams)\n    audio.save_wav(wav, wav_dir.joinpath('step-{}-wave-{}-reconstructed.wav'.format(step, sample_num)), hparams)\n\n    # save target mel spectrogram plot\n    target_mel_plot = plot_spectrogram(target_spectrogram, hparams)\n    target_mel_plot.savefig(mel_output_dir.joinpath('step-{}-mel-{}.png'.format(step, sample_num)))\n    plt.close(target_mel_plot)\n\n    # save target mel spectrogram plot with attention\n    target_mel_plot = plot_spectrogram(target_spectrogram, hparams, title='Target Spectrogram (Step {})'.format(step))\n    save_attention(target_mel_plot, attention)\n    target_mel_plot.savefig(mel_output_dir.joinpath('step-{}-mel-{}-attention.png'.format(step, sample_num)))\n    plt.close(target_mel", "CodeBleu_score": 37.831459912337166}, {"code": "def eval_model(attention, mel_prediction, target_spectrogram, input_seq, step,\n               plot_dir, mel_output_dir, wav_dir, sample_num, loss, hparams)\n\t\n    # save the attention plot\n    attention_plot = plot_attention(attention, input_seq, hparams)\n    attention_plot.savefig(plot_dir.joinpath('{}_attention_step{}_{}.png'.format(hparams.model_type, step, sample_num)))\n    plt.clf()\n\n    # save the predicted mel spectrogram\n    mel_output_filename = mel_output_dir.joinpath('{}_mel_prediction_step{}_{}.npy'.format(hparams.model_type, step, sample_num))\n    np.save(mel_output_filename, mel_prediction, allow_pickle=False)\n\n    # save the reconstructed waveform\n    wav_filename = wav_dir.joinpath('{}_step{}_{}.wav'.format(hparams.model_type, step, sample_num))\n    audio.inv_mel_spectrogram(mel_prediction.T, wav_filename)\n\n    # save the target mel spectrogram\n    target_mel_filename = mel_output_dir.joinpath('{}_mel_target_step{}_{}.npy'.format(hparams.model_type, step, sample_num))\n    np.save(target_mel_filename, target_spectrogram, allow_pickle=False)\n\n    # save the comparison plot\n    title = '{}_step{}_{}_loss{:.4f}'.format(hparams.model_type, step, sample_num, loss)\n    plot_spectrogram(mel_prediction, title, target_spectrogram, plot_dir, hparams)\n\n    # print the input sequence\n    print('Input sequence at step {}: {}'.format(step, sequence_to_text(input_seq)))\n\n", "CodeBleu_score": 42.89317943059754}, {"code": "def eval_model(attention, mel_prediction, target_spectrogram, input_seq, step,\n               plot_dir, mel_output_dir, wav_dir, sample_num, loss, hparams)\n\t\n    # save attention\n    attention_path = plot_dir.joinpath(\"step{}_sample{}_attention.png\".format(step, sample_num))\n    save_attention(attention, attention_path)\n\n    # save predicted mel spectrogram\n    mel_prediction_path = mel_output_dir.joinpath(\"step{}_sample{}_predicted_mel.npy\".format(step, sample_num))\n    np.save(mel_prediction_path, mel_prediction.T, allow_pickle=False)\n\n    # save reconstructed wav\n    wav_path = wav_dir.joinpath(\"step{}_sample{}_predicted.wav\".format(step, sample_num))\n    audio.inv_mel_spectrogram(mel_prediction.T, out_path=wav_path)\n\n    # save target mel spectrogram\n    target_path = mel_output_dir.joinpath(\"step{}_sample{}_target_mel.npy\".format(step, sample_num))\n    np.save(target_path, target_spectrogram.T, allow_pickle=False)\n\n    # save comparison plot\n    title = \"{} step {}, loss {:.5f}, sample {}\".format(hparams.model_type, step, loss, sample_num)\n    comparison_path = plot_dir.joinpath(\"step{}_sample{}_comparison.png\".format(step, sample_num))\n    plot_spectrogram(mel_prediction, target_spectrogram, title, comparison_path)\n\n    # print input sequence\n    print(\"\n", "CodeBleu_score": 46.06058101604264}]}
{"instance_id": "6bad4f500286", "generation_result": [{"code": "def num_params(self)\n\t\n    params = [p for p in self.parameters() if p.requires_grad]\n    n_params = sum([np.prod(p.size()) for p in params])\n    print(f\"Number of parameters: {n_params / 1e6:.3f}M\")\n\n\n\n", "CodeBleu_score": 32.19221506672953}, {"code": "def num_params(self)\n\t\n    parameters = filter(lambda p: p.requires_grad, self.parameters())\n    parameters = sum([np.prod(p.size()) for p in parameters])\n    print(\"Trainable Parameters: %.3fM\" % (parameters / 1e6))\n\n", "CodeBleu_score": 47.31560059195279}, {"code": "def num_params(self)\n\t\n    params = filter(lambda p: p.requires_grad, self.parameters())\n    num_params = sum([np.prod(p.size()) for p in params])\n    print(f\"The model has {num_params/1e6:.3f} million trainable parameters\")\n\n", "CodeBleu_score": 37.23625019352076}]}
{"instance_id": "78b876054bd4", "generation_result": [{"code": "def pad_tensor(self, x, pad, side='both')\n\t\n    if side not in ['both', 'before', 'after']:\n        raise ValueError(\"side must be either 'both', 'before', or 'after'\")\n\n    if self.is_cuda and not torch.cuda.is_available():\n        raise ValueError(\"CUDA is not available.\")\n\n    if side == 'both':\n        new_size = x.size()[1] + pad * 2\n    else:\n        new_size = x.size()[1] + pad\n\n    if self.is_cuda:\n        device = torch.device('cuda')\n    else:\n        device = torch.device('cpu')\n\n    # Create a new tensor filled with zeros on either the CPU or GPU, depending on availability.\n    x_padded = torch.zeros(x.size()[0], new_size, x.size()[2], device=device)\n\n    # Place the original tensor into this new padded tensor according to the specified side.\n    if side == 'both':\n        x_padded[:, pad:-pad, :] = x\n    elif side == 'before':\n        x_padded[:, pad:, :] = x\n    elif side == 'after':\n        x_padded[:, :-pad, :] = x\n\n    return x_padded\n\n", "CodeBleu_score": 37.42145039358245}, {"code": "def pad_tensor(self, x, pad, side='both')\n\t\n    if side not in ['before', 'after', 'both']:\n        raise ValueError(f\"side must be 'before', 'after', or 'both', got {side}\")\n    if x.size(1) + pad*2 < 0:\n        raise ValueError(f\"Cannot pad {x.size(1)} by {pad} on the left and right\")\n    if side == 'both':\n        pad_left = pad\n        pad_right = pad\n    elif side == 'after':\n        pad_left = 0\n        pad_right = pad\n    elif side == 'before':\n        pad_left = pad\n        pad_right = 0\n    new_size = (x.size(0), x.size(1) + pad_left + pad_right, x.size(2))\n    device = x.device\n    if device.type == 'cpu':\n        new_x = torch.zeros(new_size)\n    elif device.type == 'cuda':\n        if torch.cuda.is_available():\n            new_x = torch.zeros(new_size).cuda()\n        else:\n            new_x = torch.zeros(new_size)\n    if side == 'both':\n        new_x[:, pad:-pad, :] = x\n    elif side == 'after':\n        new_x[:, :-pad, :] = x\n    elif side == 'before':\n        new_x[:, pad:, :] = x\n    return new_x\n\n", "CodeBleu_score": 34.684677659827365}, {"code": "def pad_tensor(self, x, pad, side='both')\n\t\n    if side == 'both':\n        return torch.cat([torch.zeros(x.size(0), pad).to(x.device), x, torch.zeros(x.size(0), pad).to(x.device)])\n    elif side == 'before':\n        return torch.cat([torch.zeros(x.size(0), pad).to(x.device), x])\n    elif side == 'after':\n        return torch.cat([x, torch.zeros(x.size(0), pad).to(x.device)])\n    else:\n        raise Exception('Invalid side specified for padding. Must be one of: before, after, or both.')\n\n", "CodeBleu_score": 23.795158497603484}]}
{"instance_id": "a7ed1715a3df", "generation_result": [{"code": "def save_attention(attn, path)\n\t\n    import matplotlib.pyplot as plt\n    fig = plt.figure(figsize=(12, 6))\n    plt.imshow(attn.numpy(), cmap='viridis')\n    plt.colorbar()\n    plt.grid(False)\n    plt.ylabel('Head')\n    plt.xlabel('Position')\n    plt.savefig(path, bbox_inches='tight')\n    plt.close(fig)\n\n", "CodeBleu_score": 55.01738509524146}, {"code": "def save_attention(attn, path)\n\t\n    import matplotlib.pyplot as plt\n    fig = plt.figure(figsize=(15, 15))\n    plt.imshow(attn.T, cmap='Blues', vmin=0, vmax=1)\n    plt.colorbar()\n    plt.yticks(range(0, attn.shape[0], 10), range(0, attn.shape[0], 10))\n    plt.xticks(range(0, attn.shape[1], 10), range(0, attn.shape[1], 10))\n    plt.xlabel('Heads')\n    plt.ylabel('Source')\n    plt.savefig(path)\n    plt.close(fig)\n\n", "CodeBleu_score": 45.655719894780034}, {"code": "def save_attention(attn, path)\n\t\n    import matplotlib.pyplot as plt\n    fig = plt.figure(figsize=(12, 6))\n    plt.imshow(attn.numpy(), cmap='viridis')\n    plt.colorbar()\n    plt.grid(False)\n    plt.ylabel('Head')\n    plt.xlabel('Position')\n    plt.savefig(path, bbox_inches='tight', pad_inches=0)\n    plt.close()\n\n", "CodeBleu_score": 51.27488797110089}]}
{"instance_id": "19375c9f44a2", "generation_result": [{"code": "def plot(array)\n\t\n    fig = plt.figure(figsize=(12, 8))\n    ax = fig.add_subplot(111)\n    ax.tick_params(axis='both', labelsize=14)\n    ax.set_xlabel('X', fontsize=16, color='#191970')\n    ax.set_ylabel('Y', fontsize=16, color='#191970')\n    plt.plot(array, color='#191970')\n    plt.show()\n\n", "CodeBleu_score": 21.809880804998798}, {"code": "def plot(array)\n\t\n    fig = plt.figure(figsize=(10, 5))\n    ax = fig.add_subplot(111)\n    ax.tick_params(axis='both', which='major', labelsize=15, labelcolor='#000000')\n    plt.plot(array, color='#000000')\n    plt.show()\n\n", "CodeBleu_score": 18.443970292126664}, {"code": "def plot(array)\n\t\n    fig = plt.figure(figsize=(10, 5))\n    ax = fig.add_subplot(111)\n    ax.tick_params(axis='x', colors='k', labelsize=14)\n    ax.tick_params(axis='y', colors='k', labelsize=14)\n    ax.set_xlabel('Time (s)', color='k', fontsize=14)\n    ax.set_ylabel('Amplitude', color='k', fontsize=14)\n    plt.plot(array, color='k')\n    plt.show()\n\n", "CodeBleu_score": 28.61904352190736}]}
{"instance_id": "ff94c425920c", "generation_result": [{"code": "def write_to_csv(image_name, prediction, confidence)\n\t\n    with open(csv_path, \"a\") as csv_file:\n        csv_writer = csv.DictWriter(csv_file, fieldnames=[\"Image\", \"Prediction\", \"Confidence\"])\n        if csv_file.tell() == 0:\n            csv_writer.writeheader()\n        csv_writer.writerow({\"Image\": image_name, \"Prediction\": prediction, \"Confidence\": confidence})\n\n", "CodeBleu_score": 49.20121457242903}, {"code": "def write_to_csv(image_name, prediction, confidence)\n\t\n    with open(csv_path, 'a') as csvfile:\n        fieldnames = ['image_name', 'prediction', 'confidence']\n        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n        if csvfile.tell() == 0:\n            writer.writeheader()\n        writer.writerow({'image_name': image_name, 'prediction': prediction, 'confidence': confidence})\n\n", "CodeBleu_score": 45.08344726455215}, {"code": "def write_to_csv(image_name, prediction, confidence)\n\t\n    with open(csv_path, 'a') as csvfile:\n        fieldnames = ['image_name', 'prediction', 'confidence']\n        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n        if csvfile.tell() == 0:\n            writer.writeheader()\n        writer.writerow({'image_name': image_name, 'prediction': prediction, 'confidence': confidence})\n\n", "CodeBleu_score": 45.08344726455215}]}
{"instance_id": "436161602009", "generation_result": [{"code": "def export_torchscript(model, im, file, optimize, prefix=colorstr(\"TorchScript:\"))\n\t\n    file = Path(file).with_suffix(\".torchscript.pt\")\n    try:\n        LOGGER.info(f\"{prefix} starting export with torch {torch.__version__}\")\n        f = str(file)  # file path\n        ts = torch.jit.trace(model, im)  # trace\n        extra_files = {\"config.txt\": \"\"}  # extra files\n        if optimize:\n            extra_files.update({\"optimize_for_mobile.txt\": \"\"})\n            ts = optimize_for_mobile(ts)\n        ts.save(f, _extra_files=extra_files)\n        LOGGER.info(f\"{prefix} export success, saved as {f}\")\n    except Exception as e:\n        LOGGER.info(f\"{prefix} export failure: {e}\")\n    finally:\n        return file, None\n\n", "CodeBleu_score": 44.80319974823641}, {"code": "def export_torchscript(model, im, file, optimize, prefix=colorstr(\"TorchScript:\"))\n\t\n    try:\n        LOGGER.info(f\"{prefix} starting export with torch {torch.__version__}\")\n        f = str(file).with_suffix(\".torchscript.pt\")\n        ts = torch.jit.trace(model, im, strict=False)\n        extra_files[\"architecture\"] = str(model.yaml)\n        extra_files[\"config\"] = str(model.cfg)\n        model.model.float()  # always save model as FP32\n        extra_files[\"weight_order\"] = str(model.model.state_dict().keys())  # save order of weights\n        ts.save(f, _extra_files=extra_files)\n        if optimize:  # save mobile-optimized version\n            f2 = str(file).with_suffix(\".torchscript.mobile.pt\")\n            ts.save(f2, _extra_files=extra_files)\n            LOGGER.info(f\"{prefix} saved {f2} ({file_size(f2):.1f} MB)\")\n        LOGGER.info(f\"{prefix} saved {f} ({file_size(f):.1f} MB)\")\n        return f, None\n    except Exception as e:\n        LOGGER.info(f\"{prefix} failed ({e}).\")\n        return None, e\n\n", "CodeBleu_score": 52.14386282068031}, {"code": "def export_torchscript(model, im, file, optimize, prefix=colorstr(\"TorchScript:\"))\n\t\n    try:\n        LOGGER.info(f\"{prefix} starting export with torch {torch.__version__}\")\n        f = str(file).with_suffix(\".torchscript.pt\")\n        ts = torch.jit.trace(model, im, strict=False)\n        extra_files = {\"config.txt\": str(model)}\n        if optimize:\n            LOGGER.info(f\"{prefix} saving for mobile\")\n            ts.save(f, _extra_files=extra_files)\n        else:\n            LOGGER.info(f\"{prefix} saving\")\n            ts.save(f, _extra_files=extra_files)\n        LOGGER.info(f\"{prefix} saved {f}\")\n        return f, None\n    except Exception as e:\n        LOGGER.info(f\"{prefix} failed ({e}), saving source code\")\n        f = str(file).with_suffix(\".torchscript.py\")\n        with open(f, \"w\") as file:\n            file.write(model.export())\n        return None, f\n\n", "CodeBleu_score": 51.265011247498684}]}
{"instance_id": "78335487ac1c", "generation_result": [{"code": "def export_coreml(model, im, file, int8, half, nms, prefix=colorstr(\"CoreML:\")):\n\t\n    if not HAS_COREML:\n        raise RuntimeError(f\"{prefix} coremltools not installed. Run: pip install coremltools\")\n    if not HAS_TORCH:\n        raise RuntimeError(f\"{prefix} PyTorch not installed. Run: pip install torch\")\n    if not HAS_ONNX:\n        raise RuntimeError(f\"{prefix} onnx not installed. Run: pip install onnx\")\n    if not HAS_ONNX_TF:\n        raise RuntimeError(f\"{prefix} onnx-tf not installed. Run: pip install onnx-tf\")\n    if not HAS_TF2ONNX:\n        raise RuntimeError(f\"{prefix} tf2onnx not installed. Run: pip install tf2onnx\")\n    if not HAS_TF:\n        raise RuntimeError(f\"{prefix} TensorFlow not installed. Run: pip install tensorflow\")\n    if not HAS_TF_V2:\n        raise RuntimeError(f\"{prefix} TensorFlow V2 not installed. Run: pip install tensorflow\")\n    if not HAS_TF_V2_ONNX:\n        raise RuntimeError(f\"{prefix} tf2onnx not installed. Run: pip install tf2onnx\")\n    if not HAS_TF_V2_KERAS:\n        raise RuntimeError(f\"{prefix} TensorFlow V2 Keras not installed. Run: pip install tensorflow\")\n    if not HAS_TF_V2_SAVED_MODEL:\n        raise RuntimeError(f\"{prefix} TensorFlow V2 SavedModel not installed. Run: pip install tensorflow\")\n    if not HAS_TF_V2_TFLITE:\n        raise RuntimeError(f\"{prefix} TensorFlow V2 TFLite not installed. Run: pip install tensorflow\")\n    if not HAS_TF_V2_TFLITE_CONVERTER:\n        raise RuntimeError(f\"{prefix} TensorFlow V2 TFLite Converter not installed. Run: pip install tensorflow\")\n    if not HAS_TF_V2_TFLITE_SUPPORT:\n        raise RuntimeError(f\"{", "CodeBleu_score": 15.097028786515077}, {"code": "def export_coreml(model, im, file, int8, half, nms, prefix=colorstr(\"CoreML:\")):\n\t\n    if not check_requirements((\"coremltools\",)):\n        return\n    LOGGER.info(f\"{prefix} exporting with torch {torch.__version__}...\")\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\"ignore\", category=UserWarning)\n        if half:\n            im = im.half()  # for more precision\n        if int8:\n            im = im.div(255).unsqueeze(0)\n        map = model.forward(im).float()\n        if int8:\n            map = map.mul(255).round().clamp(0, 255).squeeze(0).to(torch.uint8)\n        if nms:\n            model = iOSModel(model, nms=nms)\n        try:\n            mlmodel = ct.convert(\n                traced_model = ct.trace(model, im, check_trace=False),\n                convert_to = \"mlprogram\",\n                compute_precision = ct.precision.FLOAT16 if half else ct.precision.FLOAT32,\n                skip_optimization = False,\n                inputs = [ct.TensorType(shape=list(im.shape), dtype=ct.Dtype.from_numpy(im.numpy().dtype))],\n                outputs = [ct.TensorType(shape=list(map.shape), dtype=ct.Dtype.from_numpy(map.numpy().dtype))],\n            )\n        except Exception as e:\n            LOGGER.error(f\"{prefix} export failure: {e}\")\n            return\n        if int8:\n            mlmodel = quantize_weights(mlmodel, 8)\n        mlmodel.save(file)\n        LOGGER.info(f\"{prefix} export success, saved as {file}\")\n        return file, mlmodel\n\n", "CodeBleu_score": 44.74237690623033}, {"code": "def export_coreml(model, im, file, int8, half, nms, prefix=colorstr(\"CoreML:\")):\n\t\n    try:\n        import coremltools as ct\n    except ImportError:\n        raise ImportError('CoreML export requires coremltools. Install with: $ pip install coremltools')\n\n    suffix = '.mlmodel'\n    file = Path(file).with_suffix(suffix)\n    LOGGER.info(f'{prefix} exporting {file}...')\n    ct_model = ct.convert(\n        traced_model(model, im, int8, half, nms),\n        inputs=[ct.ImageType(shape=im.shape, bias=[-x for x in im.mean], scale=im.std)],\n        outputs=[ct.ImageType(shape=(1, 1, 1, 255), bias=[0], scale=1)],\n        classifier_config=ct.ClassifierConfig(class_labels=COCO_CLASSES),\n    )\n    if int8:\n        ct_model = ct_model.quantized()\n    ct_model.save(file)\n    return file, ct_model\n\n", "CodeBleu_score": 38.77395188788258}]}
{"instance_id": "80233cf71580", "generation_result": [{"code": "def generate_individual(input_ranges, individual_length)\n\t\n    individual = []\n    for i in range(individual_length):\n        individual.append(random.uniform(input_ranges[i][0], input_ranges[i][1]))\n    return individual\n\n", "CodeBleu_score": 47.630972577458174}, {"code": "def generate_individual(input_ranges, individual_length)\n\t\n    individual = []\n    for i in range(individual_length):\n        individual.append(random.uniform(input_ranges[i][0], input_ranges[i][1]))\n    return individual\n\n", "CodeBleu_score": 47.630972577458174}, {"code": "def generate_individual(input_ranges, individual_length)\n\t\n\n    individual = []\n    for i in range(individual_length):\n        individual.append(random.uniform(input_ranges[i][0], input_ranges[i][1]))\n\n    return individual\n\n", "CodeBleu_score": 47.630972577458174}]}
{"instance_id": "5ea979cb4a8e", "generation_result": [{"code": "def marginal_log_mean_coeff(self, t)\n\t\n    if self.schedule_type == 'discrete':\n        return torch.log(self.interpolate_fn(self.log_alpha_fn, t))\n    elif self.schedule_type == 'linear':\n        return self.interpolate_fn(self.log_alpha_fn, t)\n    elif self.schedule_type == 'cosine':\n        return torch.log(torch.cos(t * np.pi / 2))\n    else:\n        raise NotImplementedError\n\n", "CodeBleu_score": 27.601639414906227}, {"code": "def marginal_log_mean_coeff(self, t)\n\t\n    if self.schedule_type == 'discrete':\n        if t <= 0:\n            return self.log_alpha_fn(t)\n        else:\n            return torch.log(self.interpolate_fn(t))\n    elif self.schedule_type == 'linear':\n        if t <= 0:\n            return self.log_alpha_fn(t)\n        else:\n            return self.log_alpha_fn(t) + (1 - t) * torch.log(self.interpolate_fn(t))\n    elif self.schedule_type == 'cosine':\n        if t <= 0:\n            return self.log_alpha_fn(t)\n        else:\n            return self.log_alpha_fn(t) + torch.log(self.interpolate_fn(t))\n    else:\n        raise NotImplementedError\n\n", "CodeBleu_score": 31.733487374932913}, {"code": "def marginal_log_mean_coeff(self, t)\n\t\n    if self.schedule_type == 'discrete':\n        # for discrete, we interpolate between the two closest coefficients\n        t_int = int(t)\n        if t_int < 0:\n            raise ValueError('Time t must be greater than 0.')\n        if t_int >= self.T:\n            raise ValueError('Time t must be lower than T.')\n        log_alpha_t = self.log_alpha_fn(t, t_int)\n    elif self.schedule_type == 'linear':\n        # for linear, we use a quadratic function\n        if t < 0:\n            raise ValueError('Time t must be greater than 0.')\n        if t >= self.T:\n            raise ValueError('Time t must be lower than T.')\n        log_alpha_t = self.log_alpha_fn(t, self.T)\n    elif self.schedule_type == 'cosine':\n        # for cosine, we use a cosine-based function\n        if t < 0:\n            raise ValueError('Time t must be greater than 0.')\n        if t > self.T:\n            raise ValueError('Time t must be lower than T.')\n        log_alpha_t = self.log_alpha_fn(t, self.T)\n    else:\n        raise NotImplementedError('Schedule type not implemented.')\n    return log_alpha_t\n\n", "CodeBleu_score": 29.5174636792429}]}
{"instance_id": "7da4fae5a278", "generation_result": [{"code": "def inverse_lambda(self, lamb)\n\t\n    if self.schedule_type == 'linear':\n        # lamb = beta * t + (1 - beta) * T\n        # t = (lamb - (1 - beta) * T) / beta\n        t = (lamb - (1 - self.beta) * self.T) / self.beta\n    elif self.schedule_type == 'discrete':\n        # lamb = log_alpha_t\n        # t = torch.arccos(torch.exp(-lamb)) / np.pi\n        t = interpolate_fn(self.log_alpha, lamb)\n    elif self.schedule_type == 'logarithmic':\n        # lamb = log_alpha_t\n        # t = torch.arccos(torch.exp(-lamb)) / np.pi\n        t = torch.arccos(torch.exp(-lamb)) / np.pi\n    elif self.schedule_type == 'exponential':\n        # lamb = log_alpha_t\n        # t = torch.arccos(torch.exp(-lamb)) / np.pi\n        t = torch.arccos(torch.exp(-lamb)) / np.pi\n    else:\n        raise NotImplementedError\n    return t\n\n", "CodeBleu_score": 18.972835858136484}, {"code": "def inverse_lambda(self, lamb)\n\t\n    if self.schedule == \"linear\":\n        t = lamb / self.beta\n    elif self.schedule == \"discrete\":\n        t = self.interpolate_fn(lamb)\n    else:\n        log_alpha = torch.log(self.alpha)\n        t = torch.arccos(torch.exp(-lamb / log_alpha)) / np.pi\n    return t\n\n", "CodeBleu_score": 15.611053364192049}, {"code": "def inverse_lambda(self, lamb)\n\t\n    if self.schedule_type == \"linear\":\n        # for linear schedule\n        # t = self.beta_values[0] + (self.beta_values[1] - self.beta_values[0]) * (lamb - self.lamb_min) / (self.lamb_max - self.lamb_min)\n        t = self.beta_values[0] + (self.beta_values[1] - self.beta_values[0]) * (lamb - self.lamb_min) / (self.lamb_max - self.lamb_min)\n    elif self.schedule_type == \"discrete\":\n        # for discrete schedule\n        t = interpolate_fn(lamb, self.log_alpha_values, self.beta_values)\n    elif self.schedule_type == \"logarithmic\":\n        # for logarithmic schedule\n        log_alpha = self.lamb_min + (self.lamb_max - self.lamb_min) * (lamb - self.lamb_min) / (self.lamb_max - self.lamb_min)\n        t = torch.arccos(torch.exp(-log_alpha)) / np.pi\n    elif self.schedule_type == \"exponential\":\n        # for exponential schedule\n        log_alpha = self.lamb_min + (self.lamb_max - self.lamb_min) * (lamb - self.lamb_min) / (self.lamb_max - self.lamb_min)\n        t = torch.sqrt(torch.flip(self.beta_values, [0])[0] - torch.exp(-log_alpha))\n    else:\n        raise NotImplementedError(\"Schedule type not implemented.\")\n    return t\n\n", "CodeBleu_score": 30.48850789453757}]}
{"instance_id": "a519c853a2b2", "generation_result": [{"code": "def noise_pred_fn(x, t_continuous, cond=None)\n\t\n    # Expand the time input to match the batch size.\n    t_expanded = t_continuous.expand(x.shape[:-1])\n    # Process the input through the model.\n    if cond is None:\n        model_output = model(x, t_expanded)\n    else:\n        model_output = model(x, t_expanded, cond)\n    # Apply different transformations to the model output depending on the model type.\n    if noise_schedule.marginal_type == \"diagonal\":\n        # The model output is a vector of means and a vector of standard deviations.\n        # We need to return a vector of means and a diagonal matrix of standard deviations.\n        # We can do this by taking the square root of the standard deviations and expanding the dimensions.\n        return model_output[..., 0], noise_schedule.marginal_std * model_output[..., 1:].sqrt().unsqueeze(dim=-1)\n    elif noise_schedule.marginal_type == \"additive\":\n        # The model output is a vector of means and a vector of standard deviations.\n        # We need to return a vector of means and a vector of standard deviations.\n        # We can do this by taking the square root of the standard deviations.\n        return model_output[..., 0], noise_schedule.marginal_std * model_output[..., 1:].sqrt()\n    elif noise_schedule.marginal_type == \"mixed\":\n        # The model output is a vector of means and a vector of standard deviations.\n        # We need to return a vector of means and a vector of standard deviations.\n        # We can do this by taking the square root of the standard deviations.\n        return model_output[..., 0], noise_schedule.marginal_std * model_output[..., 1:].sqrt()\n    elif noise_schedule.marginal_type == \"factor\":\n        # The model output is a vector of means and a vector of standard deviations.\n        # We need to return a vector of means and a vector of standard deviations.\n        # We can do this by taking the square root of the standard", "CodeBleu_score": 22.291316334230814}, {"code": "def noise_pred_fn(x, t_continuous, cond=None)\n\t\n    if cond is None:\n        cond = torch.zeros(x.shape[0], 1).to(x)\n    t_expanded = t_continuous.expand(x.shape[0], -1)\n    t_expanded = t_expanded.reshape(x.shape[0], -1, 1)\n    if x.dim() == 2:\n        x = x.reshape(x.shape[0], 1, x.shape[1])\n    x = torch.cat([x, t_expanded, cond], dim=2)\n    x = model(x)\n    if isinstance(noise_schedule, ConstantNoiseSchedule):\n        return x\n    if isinstance(noise_schedule, LinearNoiseSchedule):\n        return x * noise_schedule.marginal_alpha + noise_schedule.marginal_std\n    if isinstance(noise_schedule, QuadraticNoiseSchedule):\n        return x * noise_schedule.marginal_alpha + noise_schedule.marginal_std * t_expanded\n    if isinstance(noise_schedule, ExponentialNoiseSchedule):\n        return x * noise_schedule.marginal_alpha + noise_schedule.marginal_std * torch.exp(\n            t_expanded * noise_schedule.marginal_beta\n        )\n    if isinstance(noise_schedule, LogNoiseSchedule):\n        return x * noise_schedule.marginal_alpha + noise_schedule.marginal_std * torch.log(\n            t_expanded + 1\n        )\n    raise ValueError(\"Noise schedule not recognized.\")\n\n", "CodeBleu_score": 31.56232569478502}, {"code": "def noise_pred_fn(x, t_continuous, cond=None)\n\t\n    t_expanded = t_continuous.expand(x.shape[:-1])\n    if cond is None:\n        model_input = torch.cat([x, t_expanded], dim=-1)\n    else:\n        model_input = torch.cat([x, t_expanded, cond], dim=-1)\n    noise_pred = model(model_input)\n    if noise_schedule.marginal_alpha is not None:\n        noise_pred = noise_pred.pow(noise_schedule.marginal_alpha)\n    if noise_schedule.marginal_std is not None:\n        noise_pred = noise_pred * noise_schedule.marginal_std\n    return noise_pred\n\n", "CodeBleu_score": 19.959208724748652}]}
{"instance_id": "96a9b7d54823", "generation_result": [{"code": "def dynamic_thresholding_fn(self, x0, t=None)\n\t\n    if t is None:\n        t = self.threshold\n    x0_abs = torch.abs(x0)\n    x0_abs_reshaped = x0_abs.reshape(-1)\n    x0_abs_reshaped_sorted, _ = torch.sort(x0_abs_reshaped)\n    x0_abs_quantile = torch.quantile(x0_abs_reshaped_sorted, t)\n    x0_abs_quantile_expanded_dims = torch.ones_like(x0_abs).reshape(-1) * x0_abs_quantile\n    x0_abs_quantile_expanded_dims = x0_abs_quantile_expanded_dims.reshape(x0.shape)\n    x0_abs_max = torch.maximum(x0_abs, x0_abs_quantile_expanded_dims)\n    x0_abs_max_clamped = torch.clamp(x0_abs_max, 0, 1)\n    x0_abs_max_clamped_scaled = x0_abs_max_clamped / (x0_abs_max_clamped.max())\n    return x0_abs_max_clamped_scaled\n\n", "CodeBleu_score": 35.57064012310228}, {"code": "def dynamic_thresholding_fn(self, x0, t=None)\n\t\n    if t is None:\n        t = self.t\n    x0_shape = x0.shape\n    x0_abs = torch.abs(x0)\n    x0_abs_flat = torch.reshape(x0_abs, [-1])\n    x0_abs_flat_sorted, _ = torch.sort(x0_abs_flat)\n    x0_abs_quantile = torch.quantile(x0_abs_flat_sorted, t)\n    x0_abs_quantile = torch.maximum(x0_abs_quantile, torch.ones_like(x0_abs_quantile))\n    x0_abs_scaled = x0_abs / x0_abs_quantile\n    x0_abs_scaled_clamped = torch.clamp(x0_abs_scaled, 0, 1)\n    x0_scaled_clamped = x0_abs_scaled_clamped * torch.sign(x0)\n    x0_scaled_clamped = torch.reshape(x0_scaled_clamped, x0_shape)\n    return x0_scaled_clamped\n\n", "CodeBleu_score": 33.38339326441784}, {"code": "def dynamic_thresholding_fn(self, x0, t=None)\n\t\n    if t is None:\n        t = 0.99\n    x0_abs = torch.abs(x0)\n    x0_abs_sorted, _ = torch.sort(x0_abs, dim=x0.dim() - 1)\n    x0_abs_sorted_flat = torch.reshape(x0_abs_sorted, [-1, x0_abs_sorted.shape[-1]])\n    x0_abs_sorted_flat_dim0 = x0_abs_sorted_flat.shape[0]\n    index = int(t * x0_abs_sorted_flat_dim0)\n    threshold = x0_abs_sorted_flat[index]\n    scaling_factor = torch.maximum(threshold, torch.ones_like(threshold))\n    x0_scaled = x0 / scaling_factor\n    x0_scaled_clamped = torch.clamp(x0_scaled, -1.0, 1.0)\n    return x0_scaled_clamped\n", "CodeBleu_score": 36.082430891592935}]}
{"instance_id": "59c6ec6d7a6a", "generation_result": [{"code": "def data_prediction_fn(self, x, t)\n\t\n    x_dim = x.dim()\n    x = x.reshape(-1, self.num_channels, self.input_size[0], self.input_size[1])\n    t = t.reshape(-1)\n    x_t = self.noise_prediction_fn(x, t)\n    x_t = x_t.reshape(-1, self.num_channels, self.input_size[0], self.input_size[1])\n    x_t = x_t.mean(dim=1)\n    x_t = x_t.reshape(-1, self.input_size[0], self.input_size[1])\n    x_t = torch.clamp(x_t, 0, self.max_threshold)\n    x_t = x_t.reshape(-1, 1, self.input_size[0], self.input_size[1])\n    x_t = x_t.repeat(1, self.num_channels, 1, 1)\n    x = torch.abs(x - x_t)\n    x = x.reshape(-1, self.num_channels, self.input_size[0], self.input_size[1])\n    x = torch.clamp(x, 0, self.max_threshold)\n    x = x.reshape(-1, self.num_channels, self.input_size[0], self.input_size[1])\n    x = x / self.max_threshold\n    x = x.reshape(-1, self.num_channels, self.input_size[0], self.input_size[1])\n    x = x.reshape(-1, self.num_channels, self.input_size[0], self.input_size[1])\n    return x\n\n", "CodeBleu_score": 27.180679081109176}, {"code": "def data_prediction_fn(self, x, t)\n\t\n    # get the threshold value\n    threshold = self.threshold_fn(x, t)\n    # clamp the input tensor x0 within the threshold\n    x_clamped = torch.clamp(x, min=threshold)\n    # normalize the clamped values\n    x_normalized = self.normalize_fn(x_clamped)\n    return x_normalized\n", "CodeBleu_score": 3.8749958310633015}, {"code": "def data_prediction_fn(self, x, t)\n\t\n    x0 = x\n    x = self.normalize_fn(x)\n    x = x.reshape(x.shape[0], -1)\n    if t == 0:\n        self.threshold_prediction_fn(x, t)\n    x = self.noise_prediction_fn(x, t)\n    x = torch.clamp(x, min=0)\n    x = self.normalize_fn.inverse_transform(x)\n    x = x.reshape(x0.shape)\n    return x\n", "CodeBleu_score": 10.791381633356867}]}
{"instance_id": "65e3618468a2", "generation_result": [{"code": "def predict_eps_sigma(model, input, sigma_in, **kwargs)\n\t\n    sigma_in = sigma_in.view(1, 1, 1, -1).repeat(1, 1, input.shape[2], 1)\n    input = input.view(1, 1, input.shape[2], -1)\n    input = input / sigma_in\n    input = input.view(1, 1, -1)\n    prediction = model(input, **kwargs)\n    prediction = prediction.view(1, -1)\n    prediction = prediction * sigma_in\n    prediction = prediction.view(1, 1, -1)\n    return prediction\n\n", "CodeBleu_score": 29.278185641069033}, {"code": "def predict_eps_sigma(model, input, sigma_in, **kwargs)\n\t\n    sigma = np.repeat(sigma_in, input.shape[0]).reshape(input.shape[0], -1)\n    input = input.reshape(input.shape[0], -1)\n    scaled_input = input / sigma\n    prediction = model.predict(scaled_input, **kwargs)\n    prediction = prediction * sigma\n    return prediction\n\n", "CodeBleu_score": 20.549467992363766}, {"code": "def predict_eps_sigma(model, input, sigma_in, **kwargs)\n\t\n    sigma_in = sigma_in.view(1, -1).repeat(input.shape[0], 1)\n    input = input.view(1, -1).repeat(sigma_in.shape[0], 1)\n    input = input * sigma_in\n    pred = model(input)\n    pred = pred - input\n    return pred\n\n", "CodeBleu_score": 34.851903564315684}]}
{"instance_id": "b35846dfbc66", "generation_result": [{"code": "def cal_intergrand(beta_0, beta_1, taus)\n\t\n    with torch.enable_grad():\n        taus.requires_grad_()\n        alpha = torch.sqrt(beta_0 ** 2 + beta_1 ** 2 * taus ** 2)\n        log_alpha = torch.log(alpha)\n        log_alpha.sum().backward()\n        taus_grad = taus.grad\n        return (beta_1 ** 2 / alpha) * taus_grad\n\n", "CodeBleu_score": 30.924845956728316}, {"code": "def cal_intergrand(beta_0, beta_1, taus)\n\t\n    taus.requires_grad_()\n    with torch.enable_grad():\n        alpha = torch.sqrt(beta_0 ** 2 + beta_1 ** 2 + taus ** 2)\n        log_alpha = alpha.log()\n        log_alpha.sum().backward()\n        taus.requires_grad_(False)\n        return taus.grad * alpha\n\n", "CodeBleu_score": 33.644193334711744}, {"code": "def cal_intergrand(beta_0, beta_1, taus)\n\t\n    taus.requires_grad_()\n    with torch.enable_grad():\n        alpha = torch.sqrt(1 + (beta_1 / beta_0) ** 2 * taus ** 2)\n        log_alpha = alpha.log()\n        log_alpha.sum().backward()\n        taus.requires_grad_(False)\n        return (beta_1 / beta_0) ** 2 * taus * log_alpha.grad\n\n", "CodeBleu_score": 35.53508011965029}]}
{"instance_id": "5f3667566a10", "generation_result": [{"code": "def sample_euler_ancestral(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None)\n\t\n    if extra_args is None:\n        extra_args = {}\n    if noise_sampler is None:\n        noise_sampler = get_default_noise_sampler(model)\n    x_new = x.new_ones(x.shape)\n    if disable is None:\n        disable = []\n    for sigma in sigmas:\n        if 'sigma' not in extra_args:\n            extra_args['sigma'] = sigma\n        x_new = x_new * (1 - eta) + eta * get_ancestral_step(model, x, extra_args, disable=disable)\n        x_new = x_new.detach()\n        x_new.requires_grad = True\n        x = denoise(model, x_new, sigma, extra_args, disable=disable)\n        x = x.detach()\n        x.requires_grad = True\n        if callback is not None:\n            callback(x_new, sigma, **extra_args)\n    if s_noise > 0:\n        x_new = x_new + s_noise * noise_sampler(x_new.shape)\n    return x_new\n\n", "CodeBleu_score": 30.460524977139368}, {"code": "def sample_euler_ancestral(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None)\n\t\n    if extra_args is None:\n        extra_args = {}\n    if noise_sampler is None:\n        noise_sampler = get_default_noise_sampler(model, x, extra_args)\n    if disable is None:\n        disable = []\n    x = x.clone()\n    for sigma in sigmas:\n        if sigma not in disable:\n            x = denoise(model, x, sigma, extra_args)\n            step = get_ancestral_step(model, x, sigma, extra_args)\n            x = x - eta * step\n            if callback is not None:\n                callback(x, sigma)\n    if s_noise > 0:\n        x = x + s_noise * noise_sampler(x)\n    return x\n\n", "CodeBleu_score": 27.77218451632189}, {"code": "def sample_euler_ancestral(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None)\n\t\n\n    # Initialize extra arguments and noise sampler\n    if extra_args is None:\n        extra_args = {}\n    if noise_sampler is None:\n        noise_sampler = get_default_noise_sampler(model, x, extra_args)\n\n    # Iterate over the sigmas\n    for sigma in sigmas:\n        # Denoise the input\n        x = denoise(model, x, sigma=sigma, disable=disable, extra_args=extra_args)\n\n        # Calculate the ancestral step\n        step = get_ancestral_step(model, x, sigma=sigma, disable=disable, extra_args=extra_args)\n\n        # Update the input\n        x = x + eta * step\n\n        # Call the callback function\n        if callback is not None:\n            callback(x, sigma, step, extra_args)\n\n    # Add noise if required\n    if s_noise > 0:\n        x = x + s_noise * noise_sampler(len(x))\n\n    return x\n\n", "CodeBleu_score": 27.105039248305168}]}
{"instance_id": "6261ded88df2", "generation_result": [{"code": "def linear_multistep_coeff(order, t, i, j)\n\t\n    def product(t, j):\n        \"\"\"The function computes the product term for the coefficient, excluding the j-th term.", "CodeBleu_score": 6.858038991920057}, {"code": "def linear_multistep_coeff(order, t, i, j)\n\t\n    def prod(k):\n        \"\"\"The function computes the product term for the coefficient, excluding the j-th term.\"\"\"\n        if k == j:\n            return 1\n        else:\n            return t[k] - t[i]\n\n    if order == 1:\n        return 1\n    elif order == 2:\n        if j == 0:\n            return t[1] - t[0]\n        elif j == 1:\n            return t[0] - t[1]\n        else:\n            raise ValueError(\"The order of the coefficient must be lower than the number of time steps.\")\n    else:\n        return integrate.quad(prod, t[j], t[i])[0]\n\n", "CodeBleu_score": 20.95702081144819}, {"code": "def linear_multistep_coeff(order, t, i, j)\n\t\n    def product_term(t, i, j):\n        \"\"\"The function computes the product term for the coefficient, excluding the j-th term.\"\"\"\n        if i == j:\n            return 1\n        else:\n            return t[i] - t[j]\n    if order == 1:\n        if i == j:\n            return 1\n        else:\n            return 0\n    elif order == 2:\n        if i == j:\n            return 1\n        elif i == j - 1:\n            return 1\n        else:\n            return 0\n    elif order == 3:\n        if i == j:\n            return 1\n        elif i == j - 1:\n            return 1\n        elif i == j - 2:\n            return 1\n        else:\n            return 0\n    elif order == 4:\n        if i == j:\n            return 1\n        elif i == j - 1:\n            return 1\n        elif i == j - 2:\n            return 1\n        elif i == j - 3:\n            return 1\n        else:\n            return 0\n    elif order == 5:\n        if i == j:\n            return 1\n        elif i == j - 1:\n            return 1\n        elif i == j - 2:\n            return 1\n        elif i == j - 3:\n            return 1\n        elif i == j - 4:\n            return 1\n        else:\n            return 0\n    elif order == 6:\n        if i == j:\n            return 1\n        elif i == j - 1:\n            return 1\n        elif i == j - 2:\n            return 1\n        elif i == j - 3:\n            return 1\n        elif i == j - 4:\n            return 1\n        elif i == j - 5:\n            return 1\n        else:\n            return 0\n    elif order == 7:\n        if i == j:\n            return 1\n        elif i == j - 1:\n            return 1\n        elif i == j - 2:\n            return ", "CodeBleu_score": 11.13134658343935}]}
{"instance_id": "4ee025e33ab1", "generation_result": [{"code": "def sample_lcm(model, x, sigmas, extra_args=None, callback=None, disable=None, noise_sampler=None)\n\t\n    if extra_args is None:\n        extra_args = {}\n    if noise_sampler is None:\n        noise_sampler = default_noise_sampler\n    if disable is None:\n        disable = []\n    if callback is None:\n        callback = lambda x, y, z: None\n    for sigma in sigmas:\n        if sigma > 0:\n            if 'trange' not in extra_args:\n                extra_args['trange'] = trange(len(x), desc='Sampling', disable=disable)\n            if 'noise_scaling' not in extra_args:\n                extra_args['noise_scaling'] = 0.03\n            x = model(x, sigma, **extra_args)\n            x = noise_sampler(x, sigma, **extra_args)\n            callback(x, sigma, extra_args)\n        else:\n            x = model(x, sigma, **extra_args)\n            callback(x, sigma, extra_args)\n    return x\n\n", "CodeBleu_score": 32.46407375463797}, {"code": "def sample_lcm(model, x, sigmas, extra_args=None, callback=None, disable=None, noise_sampler=None)\n\t\n    if extra_args is None:\n        extra_args = {}\n    if noise_sampler is None:\n        noise_sampler = default_noise_sampler\n    if disable is None:\n        disable = []\n    if callback is None:\n        callback = lambda **kwargs: None\n    for i, sigma in enumerate(sigmas):\n        if sigma > 0:\n            x = model(x, **extra_args)\n            if 'noise' not in disable:\n                x = noise_sampler(x, sigma=sigma, **extra_args)\n            if 'noise_scale' not in disable:\n                x = noise_scaling(x, sigma=sigma, **extra_args)\n        callback(i=i, x=x, sigma=sigma)\n    return x\n\n", "CodeBleu_score": 34.07518001283215}, {"code": "def sample_lcm(model, x, sigmas, extra_args=None, callback=None, disable=None, noise_sampler=None)\n\t\n    if extra_args is None:\n        extra_args = {}\n    if noise_sampler is None:\n        noise_sampler = default_noise_sampler\n    if disable is None:\n        disable = []\n    if callback is None:\n        callback = lambda x, **kwargs: None\n    for sigma in sigmas:\n        x = model(x, **extra_args)\n        x = noise_sampler(x, sigma, disable=disable)\n        callback(x, **extra_args)\n    return x\n\n", "CodeBleu_score": 24.82210521740342}]}
{"instance_id": "cb51f91da606", "generation_result": [{"code": "def append_dims(x, target_dims)\n\t\n    if len(x.shape) > target_dims:\n        raise ValueError(\n            f\"The target dimensions ({target_dims}) must be greater than or equal to the current dimensions ({len(x.shape)}).\"\n        )\n    elif len(x.shape) == target_dims:\n        return x\n    else:\n        if x.device == torch.device(\"mps\"):\n            return x.expand(x.shape + (1,) * (target_dims - len(x.shape))).detach()\n        else:\n            return x.expand(x.shape + (1,) * (target_dims - len(x.shape)))\n\n", "CodeBleu_score": 21.719956725907725}, {"code": "def append_dims(x, target_dims)\n\t\n    if x.dim() > target_dims:\n        raise ValueError(\"Target dimensions must be greater than the current dimensions.\")\n    elif x.dim() == target_dims:\n        return x\n    else:\n        num_dims_to_append = target_dims - x.dim()\n        return x.expand(*(x.shape + (1,) * num_dims_to_append)).detach().clone()\n\n", "CodeBleu_score": 28.94284940193665}, {"code": "def append_dims(x, target_dims)\n\t\n    if x.dim() < target_dims:\n        raise ValueError(f\"The target dimensions {target_dims} are fewer than the current dimensions {x.dim()}\")\n    elif x.dim() == target_dims:\n        return x\n    else:\n        return x.expand(*([1]*(target_dims-x.dim())+list(x.shape))).clone().detach()\n\n", "CodeBleu_score": 22.00794919005182}]}
{"instance_id": "9fac874a3bc7", "generation_result": [{"code": "def download_file(path, url, digest=None)\n\t\n    if not os.path.exists(path):\n        os.makedirs(path)\n    file_name = os.path.basename(url)\n    file_path = os.path.join(path, file_name)\n    if os.path.exists(file_path):\n        return file_path\n    with urlopen(url) as response, open(file_path, 'wb') as out_file:\n        shutil.copyfileobj(response, out_file)\n    if digest is not None:\n        with open(file_path, 'rb') as f:\n            if hashlib.sha256(f.read()).hexdigest() != digest:\n                raise OSError(f'The file {file_path} failed to download.')\n    return file_path\n", "CodeBleu_score": 48.12418183456153}, {"code": "def download_file(path, url, digest=None)\n\t\n    if not os.path.exists(path):\n        os.makedirs(path)\n    filename = os.path.join(path, os.path.basename(url))\n    if not os.path.exists(filename):\n        print(\"Downloading {}\".format(url))\n        with open(filename, \"wb\") as f:\n            response = urllib.request.urlopen(url)\n            shutil.copyfileobj(response, f)\n    if digest is not None:\n        sha256 = hashlib.sha256()\n        with open(filename, \"rb\") as f:\n            sha256.update(f.read())\n        if sha256.hexdigest() != digest:\n            raise OSError(\"File {} has an SHA-256 hash of {} but expected {}\".format(filename, sha256.hexdigest(), digest))\n    return filename\n\n", "CodeBleu_score": 47.491511827993484}, {"code": "def download_file(path, url, digest=None)\n\t\n    if not os.path.exists(path):\n        os.makedirs(path)\n    file_path = os.path.join(path, url.split('/')[-1])\n    if not os.path.exists(file_path):\n        print('Downloading ' + url)\n        response = urllib.request.urlopen(url)\n        with open(file_path, 'wb') as out:\n            shutil.copyfileobj(response, out)\n    else:\n        print('File {} already exists'.format(file_path))\n    if digest is not None:\n        print('Verifying hash of {}'.format(file_path))\n        with open(file_path, 'rb') as f:\n            d = hashlib.sha256(f.read()).hexdigest()\n        if digest != d:\n            raise OSError('Digest {} does not match that of {}'.format(digest, file_path))\n    return file_path\n\n", "CodeBleu_score": 47.47915342484119}]}
{"instance_id": "f23165e897ce", "generation_result": [{"code": "def train_mode(model, mode=True)\n\t\n    def set_train_mode(module, mode):\n        if (hasattr(module, \"training\") and module.training is not mode):\n            module.training = mode\n\n    if mode:\n        training_modes = {}\n        for module in model.modules():\n            training_modes[module] = module.training\n            set_train_mode(module, True)\n    else:\n        for module in model.modules():\n            set_train_mode(module, False)\n\n    return model\n\n", "CodeBleu_score": 30.19250567757202}, {"code": "def train_mode(model, mode=True)\n\t\n    pass\n\n", "CodeBleu_score": 3.033088238794708}, {"code": "def train_mode(model, mode=True)\n\t\n    if mode:\n        return model.train()\n    else:\n        return model.eval()\n\n", "CodeBleu_score": 13.777709534837959}]}
{"instance_id": "c6d59c6cb8e1", "generation_result": [{"code": "def ema_update(model, averaged_model, decay)\n\t\n    model_buffers = dict(model.named_buffers())\n    averaged_buffers = dict(averaged_model.named_buffers())\n    model_params = dict(model.named_parameters())\n    averaged_params = dict(averaged_model.named_parameters())\n    for name, averaged_param in averaged_params.items():\n        averaged_param.data.mul_(decay).add_(1 - decay, model_params[name].data)\n    for name, averaged_buffer in averaged_buffers.items():\n        averaged_buffer.data.copy_(model_buffers[name].data)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 48.77062501083954}, {"code": "def ema_update(model, averaged_model, decay)\n\t\n    model_buffers = dict(model.named_buffers())\n    averaged_buffers = dict(averaged_model.named_buffers())\n    model_params = dict(model.named_parameters())\n    averaged_params = dict(averaged_model.named_parameters())\n    for name in averaged_params.keys():\n        averaged_params[name].data.mul_(decay).add_(\n            1 - decay, model_params[name].data\n        )\n    for name in averaged_buffers.keys():\n        averaged_buffers[name].data.copy_(model_buffers[name].data)\n\n", "CodeBleu_score": 46.76295323260199}, {"code": "def ema_update(model, averaged_model, decay)\n\t\n    model_buffers = dict(model.named_buffers())\n    averaged_buffers = dict(averaged_model.named_buffers())\n    model_params = dict(model.named_parameters())\n    averaged_params = dict(averaged_model.named_parameters())\n    for name, averaged_param in averaged_params.items():\n        model_param = model_params[name]\n        averaged_param.data.mul_(decay).add_(1 - decay, model_param.data)\n    for name, averaged_buffer in averaged_buffers.items():\n        averaged_buffer.data.copy_(model_buffers[name].data)\n\n", "CodeBleu_score": 51.03124496777872}]}
{"instance_id": "600c39a30798", "generation_result": [{"code": "def rand_log_logistic(shape, loc=0., scale=1., min_value=0., max_value=float('inf'), device='cpu', dtype=torch.float32)\n\t\n    u = torch.rand(shape, device=device, dtype=dtype)\n    min_value = torch.as_tensor(min_value, device=device, dtype=dtype)\n    max_value = torch.as_tensor(max_value, device=device, dtype=dtype)\n    min_value = min_value.log()\n    max_value = max_value.log()\n    u = (max_value - min_value) * u + min_value\n    u = u.logit()\n    return (u * scale) + loc\n\n", "CodeBleu_score": 39.758967706711076}, {"code": "def rand_log_logistic(shape, loc=0., scale=1., min_value=0., max_value=float('inf'), device='cpu', dtype=torch.float32)\n\t\n    u = torch.rand(shape, device=device, dtype=dtype)\n    min_value = torch.as_tensor(min_value, device=device, dtype=dtype)\n    max_value = torch.as_tensor(max_value, device=device, dtype=dtype)\n    log_cdf_min = min_value.log() - loc\n    log_cdf_max = max_value.log() - loc\n    log_uniform = log_cdf_min + u * (log_cdf_max - log_cdf_min)\n    x = log_uniform.exp()\n    return loc + scale * x.logit()\n\n", "CodeBleu_score": 41.01242618568301}, {"code": "def rand_log_logistic(shape, loc=0., scale=1., min_value=0., max_value=float('inf'), device='cpu', dtype=torch.float32)\n\t\n    if not torch.is_tensor(min_value):\n        min_value = torch.as_tensor(min_value, dtype=dtype, device=device)\n    if not torch.is_tensor(max_value):\n        max_value = torch.as_tensor(max_value, dtype=dtype, device=device)\n    u = torch.rand(shape, dtype=dtype, device=device)\n    return torch.exp(torch.log(u) / torch.log(1 - u) * scale + loc) * (max_value - min_value) + min_value\n\n", "CodeBleu_score": 40.444453308799936}]}
{"instance_id": "75e2def2467f", "generation_result": [{"code": "def get_activation(activation: Literal[\"elu\", \"snake\", \"none\"], antialias=False, channels=None) -> nn.Module\n\t\n    if activation == \"elu\":\n        if antialias:\n            return Activation1d(nn.ELU())\n        else:\n            return nn.ELU()\n    elif activation == \"snake\":\n        if antialias:\n            return Activation1d(SnakeBeta(channels))\n        else:\n            return SnakeBeta(channels)\n    elif activation == \"none\":\n        return Identity()\n    else:\n        raise ValueError(f\"Activation type {activation} not recognized.\")\n\n", "CodeBleu_score": 41.650876361274605}, {"code": "def get_activation(activation: Literal[\"elu\", \"snake\", \"none\"], antialias=False, channels=None) -> nn.Module\n\t\n    if activation == \"elu\":\n        act = ELU(alpha=1e-2, inplace=True)\n    elif activation == \"snake\":\n        act = SnakeBeta(channels=channels)\n    elif activation == \"none\":\n        act = Identity()\n    else:\n        raise ValueError(f\"Activation {activation} not supported\")\n\n    if antialias:\n        act = Activation1d(act)\n\n    return act\n\n", "CodeBleu_score": 65.96850173740127}, {"code": "def get_activation(activation: Literal[\"elu\", \"snake\", \"none\"], antialias=False, channels=None) -> nn.Module\n\t\n    if activation == \"elu\":\n        if antialias:\n            return Activation1d(ELU())\n        else:\n            return ELU()\n    elif activation == \"snake\":\n        if antialias:\n            return Activation1d(SnakeBeta(channels))\n        else:\n            return SnakeBeta(channels)\n    elif activation == \"none\":\n        return Identity()\n    else:\n        raise ValueError(f\"Activation {activation} not found.\")\n", "CodeBleu_score": 41.31965482332636}]}
{"instance_id": "ef1714c79e51", "generation_result": [{"code": "def apply_rotary_pos_emb(t, freqs, scale = 1)\n\t\n\n    # Ensure numerical stability\n    if t.dtype != torch.float32:\n        t = t.float()\n\n    # Adjust shapes\n    t = rearrange(t, 'b n d -> b d n')\n    freqs = freqs.unsqueeze(0).unsqueeze(-1)\n\n    # Rotate half of the tensor\n    t_unrotated = t[:, :, :t.shape[-1] // 2]\n    t_rotated = t[:, :, t.shape[-1] // 2:]\n    t_rotated = rotate_half(t_rotated, freqs, scale)\n\n    # Concatenate rotated and unrotated parts\n    t = torch.cat((t_unrotated, t_rotated), dim = -1)\n\n    # Adjust shapes back\n    t = rearrange(t, 'b d n -> b n d')\n    return t\n", "CodeBleu_score": 34.17639967681322}, {"code": "def apply_rotary_pos_emb(t, freqs, scale = 1)\n\t\n    if t.dtype != torch.float32:\n        t = t.float()\n\n    freqs = freqs.to(t.device)\n\n    if len(t.shape) == 2:\n        t = t.unsqueeze(0)\n\n    t_unrotated = t[:, :, :, 0::2]\n    t_rotated = t[:, :, :, 1::2]\n\n    t_rotated = t_rotated * freqs.cos() + t_rotated * freqs.sin()\n\n    return torch.cat([t_unrotated, t_rotated], dim = -1)\n", "CodeBleu_score": 23.238353576787002}, {"code": "def apply_rotary_pos_emb(t, freqs, scale = 1)\n\t\n    t = t.to(dtype = torch.float32)\n    freqs = freqs.to(dtype = torch.float32)\n\n    if len(t.shape) == 3:\n        t = t.unsqueeze(1)\n\n    if len(freqs.shape) == 2:\n        freqs = freqs.unsqueeze(0)\n\n    t_unrotated = t[:, :, :, 0::2]\n    t_rotated = t[:, :, :, 1::2]\n\n    t_rotated = t_rotated * freqs.cos() + t_rotated * freqs.sin()\n    t = torch.cat([t_unrotated, t_rotated], dim = -1)\n\n    return t\n\n", "CodeBleu_score": 29.737096512939722}]}
{"instance_id": "a7a5f178c6f6", "generation_result": [{"code": "def idx2vq(self, idx, dim=-1)\n\t\n    emb = self.codebook(idx)\n    if dim != -1:\n        emb = emb.movedim(-1, dim)\n    return emb\n\n", "CodeBleu_score": 42.71809373052369}, {"code": "def idx2vq(self, idx, dim=-1)\n\t\n    embeddings = self.codebook[idx]\n    if dim != -1:\n        embeddings = embeddings.movedim(-1, dim)\n    return embeddings\n\n", "CodeBleu_score": 34.58885807486271}, {"code": "def idx2vq(self, idx, dim=-1)\n\t\n    embeddings = self.codebook[idx]\n    if dim != -1:\n        embeddings = embeddings.movedim(-1, dim)\n    return embeddings\n\n", "CodeBleu_score": 34.58885807486271}]}
{"instance_id": "4104c041fb20", "generation_result": [{"code": "def encode(self, x, quantize=False)\n\t\n    if quantize:\n        x = self.quantizer(x)\n    for block in self.down_blocks:\n        x = block(x)\n    return x\n\n", "CodeBleu_score": 25.69664141335275}, {"code": "def encode(self, x, quantize=False)\n\t\n    if quantize:\n        return self.quantize(x)\n    else:\n        return self.forward(x)\n\n", "CodeBleu_score": 15.118427054569917}, {"code": "def encode(self, x, quantize=False)\n\t\n    if quantize:\n        quant_x, indices, loss = self.quantizer(x)\n        x = self.down_blocks(quant_x)\n        x = self.in_block(x)\n        return x, quant_x, indices, loss\n    else:\n        x = self.down_blocks(x)\n        x = self.in_block(x)\n        return x\n\n", "CodeBleu_score": 40.34208818657932}]}
{"instance_id": "403c52ba2d3b", "generation_result": [{"code": "def ema_scope(self, context=None)\n\t\n    if context is not None:\n        logpy.info(f\"EMA weights applied in {context}\")\n    self.ema_weights = self.parameters\n    self.parameters = self.ema_weights\n    yield\n    self.parameters = self.ema_weights\n    if context is not None:\n        logpy.info(f\"Restored parameters in {context}\")\n\n", "CodeBleu_score": 41.054800578142945}, {"code": "def ema_scope(self, context=None)\n\t\n    if self.ema_weights is None:\n        raise RuntimeError('EMA weights are not available.')\n    if context is not None:\n        context.log('Switching to EMA weights.')\n    self.store()\n    self.parameters = self.ema_weights\n    if context is not None:\n        context.log('Restoring original parameters.')\n    self.restore()\n\n", "CodeBleu_score": 43.94643673007812}, {"code": "def ema_scope(self, context=None)\n\t\n    if self.ema is None:\n        raise ValueError('EMA is not initialized.')\n    if context is not None:\n        context.log('Using EMA weights.')\n    self.parameters = self.ema.parameters()\n    yield\n    if context is not None:\n        context.log('Restoring original weights.')\n    self.parameters = self.original_parameters\n\n", "CodeBleu_score": 43.90662882983101}]}
{"instance_id": "94b7467f98e4", "generation_result": [{"code": "def post_attention(self, attn, x, gate_msa, shift_mlp, scale_mlp, gate_mlp)\n\t\n    x = x + attn * gate_msa\n    x = self.norm1(x)\n    x = x + self.mlp(x) * gate_mlp\n    return x\n", "CodeBleu_score": 30.94461142038618}, {"code": "def post_attention(self, attn, x, gate_msa, shift_mlp, scale_mlp, gate_mlp)\n\t\n    x = x + attn * gate_msa\n    x = x.transpose(1, 2)\n    x = self.norm1(x)\n    x = x.transpose(1, 2)\n    x = x + self.mlp(x) * gate_mlp\n    return x\n\n", "CodeBleu_score": 39.549337209138926}, {"code": "def post_attention(self, attn, x, gate_msa, shift_mlp, scale_mlp, gate_mlp)\n\t\n    # Add the attention output, scaled by gate_msa, to x.\n    x = x + attn * gate_msa\n    # Normalize x.\n    x = self.norm1(x)\n    # Modulate x using shift_mlp and scale_mlp.\n    x = x + shift_mlp(x)\n    x = x * scale_mlp(x)\n    # Apply an MLP scaled by gate_mlp.\n    return self.norm2(x + self.mlp(x) * gate_mlp)\n\n", "CodeBleu_score": 36.358768298998726}]}
{"instance_id": "d9fdc05542a2", "generation_result": [{"code": "def _block_mixing(context, x, context_block, x_block, c)\n\t\n    # Pre-attention blocks\n    context_intermediates = context_block.pre_attention(context)\n    x_intermediates = x_block.pre_attention(x)\n    # Concatenate qkv tensors\n    context_qkv = context_intermediates[\"qkv\"]\n    x_qkv = x_intermediates[\"qkv\"]\n    qkv_tensor = torch.cat([context_qkv, x_qkv], dim=2)\n    # Apply optimized attention\n    qkv_tensor = qkv_tensor.view(qkv_tensor.shape[0], qkv_tensor.shape[1], c, 3, -1)\n    qkv_tensor = qkv_tensor.permute(0, 3, 1, 4, 2)\n    qkv_tensor = qkv_tensor.reshape(qkv_tensor.shape[0], qkv_tensor.shape[1], qkv_tensor.shape[2], -1)\n    attention_scores = optimized_attention(qkv_tensor[:, 0], qkv_tensor[:, 1], qkv_tensor[:, 2])\n    # Split attention results\n    context_attention_scores = attention_scores[:, :context_qkv.shape[1], :, :]\n    x_attention_scores = attention_scores[:, context_qkv.shape[1]:, :, :]\n    # Post-attention blocks\n    context_post_attention = context_block.post_attention(context_intermediates, context_attention_scores)\n    x_post_attention = x_block.post_attention(x_intermediates, x_attention_scores)\n    return context_post_attention, x_post_attention\n\n", "CodeBleu_score": 21.47655790541767}, {"code": "def _block_mixing(context, x, context_block, x_block, c)\n\t\n\n    # Process context and x through their respective pre-attention blocks\n    context_intermediates = context_block.pre_attention(context)\n    x_intermediates = x_block.pre_attention(x)\n\n    # Concatenate qkv tensors\n    context_qkv = context_intermediates[0]\n    x_qkv = x_intermediates[0]\n    qkv = torch.cat([context_qkv, x_qkv], dim=2)\n\n    # Apply optimized attention\n    attention_output, attention_bias = optimized_attention(qkv, c)\n\n    # Split attention results back into context and x parts\n    context_attention_output = attention_output[:, :, :context_qkv.shape[2]]\n    x_attention_output = attention_output[:, :, context_qkv.shape[2]:]\n\n    # Process context and x through their respective post-attention blocks\n    if context_block.post_attention is not None:\n        context = context_block.post_attention(context_attention_output, context_intermediates[1])\n    else:\n        context = context_attention_output\n    if x_block.post_attention is not None:\n        x = x_block.post_attention(x_attention_output, x_intermediates[1])\n    else:\n        x = x_attention_output\n    return context, x, attention_bias\n\n", "CodeBleu_score": 25.39608912108184}, {"code": "def _block_mixing(context, x, context_block, x_block, c)\n\t\n    # context block pre-attention\n    context_block_pre_attention_intermediates = context_block.pre_attention(context)\n    # x block pre-attention\n    x_block_pre_attention_intermediates = x_block.pre_attention(x)\n\n    # context and x qkv tensors\n    context_qkv = context_block.get_qkv(context_block_pre_attention_intermediates)\n    x_qkv = x_block.get_qkv(x_block_pre_attention_intermediates)\n\n    # context and x attention masks\n    context_attention_mask = context_block.get_attention_mask(context)\n    x_attention_mask = x_block.get_attention_mask(x)\n\n    # optimized attention\n    context_x_attention_output = optimized_attention(\n        q=context_qkv,\n        k=x_qkv,\n        mask=context_attention_mask,\n        context_length=context.shape[1],\n        x_length=x.shape[1],\n        c=c,\n        x_attention_mask=x_attention_mask,\n    )\n\n    # split context and x attention outputs\n    context_attention_output, x_attention_output = torch.split(context_x_attention_output, [context.shape[1], x.shape[1]], dim=1)\n\n    # context block post-attention\n    context_block_post_attention_intermediates = context_block.post_attention(context_attention_output, context_block_pre_attention_intermediates)\n\n    # x block post-attention\n    x_block_post_attention_intermediates = x_block.post_attention(x_attention_output, x_block_pre_attention_intermediates)\n\n    # update context and x\n    context = context_block.update_state(context_block_post_attention_intermediates)\n    x = x_block.update_state(x_block_post_attention_", "CodeBleu_score": 20.57097018589894}]}
{"instance_id": "b219bce30160", "generation_result": [{"code": "def get_timestep_embedding(timesteps, embedding_dim)\n\t\n    assert len(timesteps.shape) == 1, 'timesteps input must be a 1D tensor'\n    assert embedding_dim % 2 == 0, 'embedding dimension must be even'\n    half_dim = embedding_dim // 2\n    emb = math.log(10000) / (half_dim - 1)\n    emb = torch.exp(torch.arange(half_dim, dtype=torch.float32) * -emb)\n    emb = timesteps.float()[:, None] * emb[None, :]\n    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n    if embedding_dim % 2 == 1:\n        emb = torch.nn.functional.pad(emb, (0, 1))\n    return emb\n", "CodeBleu_score": 76.72478848695465}, {"code": "def get_timestep_embedding(timesteps, embedding_dim)\n\t\n    assert len(timesteps.shape) == 1, 'timesteps input must be one-dimensional'\n    assert embedding_dim % 2 == 0, 'embedding dimension must be even'\n    half_dim = embedding_dim // 2\n    emb = math.log(10000) / (half_dim - 1)\n    emb = torch.exp(torch.arange(half_dim, dtype=torch.float32) * -emb)\n    emb = timesteps.float()[:, None] * emb[None, :]\n    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n    if embedding_dim % 2 == 1:\n        emb = pad(emb, (0, 1, 0, 0))\n    return emb\n\n", "CodeBleu_score": 75.18583301928135}, {"code": "def get_timestep_embedding(timesteps, embedding_dim)\n\t\n    assert len(timesteps.shape) == 1, 'timesteps input must be one-dimensional'\n    assert embedding_dim % 2 == 0, 'embedding dimension must be even'\n    half_dim = embedding_dim // 2\n    emb = math.log(10000) / (half_dim - 1)\n    emb = torch.exp(torch.arange(half_dim, dtype=torch.float32) * -emb)\n    emb = timesteps.float()[:, None] * emb[None, :]\n    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n    if embedding_dim % 2 == 1:\n        emb = torch.nn.functional.pad(emb, (0, 1))\n    return emb\n\n", "CodeBleu_score": 76.56335709931248}]}
{"instance_id": "6ae8453c86be", "generation_result": [{"code": "def pytorch_attention(q, k, v)\n\t\n    try:\n        q = q.view(q.shape[0], -1, q.shape[-1])\n        k = k.view(k.shape[0], -1, k.shape[-1])\n        v = v.view(v.shape[0], -1, v.shape[-1])\n        return pytorch_scaled_dot_product_attention(q, k, v)\n    except RuntimeError as e:\n        if \"out of memory\" in str(e):\n            logging.warning(\n                \"OOM exception occurred during attention computation. Switching to slice attention.\"\n            )\n            return slice_attention(q, k, v)\n        else:\n            raise e\n\n", "CodeBleu_score": 25.442959349061805}, {"code": "def pytorch_attention(q, k, v)\n\t\n    try:\n        q = q.transpose(-2, -1)\n        k = k.transpose(-2, -1)\n        v = v.transpose(-2, -1)\n        q = q.contiguous().view(q.shape[0], -1, q.shape[-1])\n        k = k.contiguous().view(k.shape[0], -1, k.shape[-1])\n        v = v.contiguous().view(v.shape[0], -1, v.shape[-1])\n        out = scaled_dot_product_attention(q, k, v)\n        out = out.view(out.shape[0], out.shape[1], q.shape[1], out.shape[2])\n        out = out.transpose(-2, -1)\n        return out\n    except RuntimeError as e:\n        logging.warning(f\"OOM error occurred during attention computation. Switching to slice attention. Error: {e}\")\n        return slice_attention(q, k, v)\n\n", "CodeBleu_score": 30.764967628480655}, {"code": "def pytorch_attention(q, k, v)\n\t\n    try:\n        # reshape and transpose q, k, v\n        q = q.view(q.shape[0], -1, q.shape[-1]).transpose(0, 1)\n        k = k.view(k.shape[0], -1, k.shape[-1]).transpose(0, 1)\n        v = v.view(v.shape[0], -1, v.shape[-1]).transpose(0, 1)\n        # compute attention\n        output = torch.matmul(q, k.transpose(-2, -1))\n        output = output / math.sqrt(k.shape[-1])\n        output = torch.softmax(output, dim=-1)\n        output = torch.matmul(output, v)\n        # reshape output\n        output = output.transpose(0, 1).contiguous().view(v.shape[0], -1, v.shape[-1])\n    except RuntimeError as e:\n        if \"CUDA out of memory\" in str(e):\n            logging.warning(\n                \"CUDA out of memory. Switching to slice attention. This will result in slower training.\"\n            )\n            # reshape and transpose q, k, v\n            q = q.contiguous().view(q.shape[0], -1, q.shape[-1]).transpose(0, 1)\n            k = k.contiguous().view(k.shape[0], -1, k.shape[-1]).transpose(0, 1)\n            v = v.contiguous().view(v.shape[0], -1, v.shape[-1]).transpose(0, 1)\n            # compute attention\n            output = slice_attention(q, k, v)\n            # reshape output\n            output = output.transpose(0, 1).contiguous().view(v.shape[0], -1, v.shape[-1])\n        else:\n            raise e\n    return output\n\n", "CodeBleu_score": 32.20738475864084}]}
{"instance_id": "fe4433c987ad", "generation_result": [{"code": "def apply_control(h, control, name)\n\t\n    if control and name:\n        c = control.pop(name, None)\n        if c:\n            try:\n                h.add_control(c)\n            except Exception as e:\n                logging.warning(f\"Failed to add control {name} to {h.name}: {e}\")\n    return h\n\n", "CodeBleu_score": 37.5683375980994}, {"code": "def apply_control(h, control, name)\n\t\n    if control is not None and name in control:\n        c = control.pop(name)\n        if c is not None and c != \"\":\n            try:\n                h = h.add_control(c)\n            except:\n                logging.warning(\"Could not add control {} to {}\".format(c, name))\n    return h\n\n", "CodeBleu_score": 48.16338746530404}, {"code": "def apply_control(h, control, name)\n\t\n    if control is not None and name is not None:\n        if len(control) > 0:\n            c = control.pop(0)\n            try:\n                h = h.apply(c)\n            except:\n                logging.warning(f'{c} could not be applied to {name}')\n    return h\n\n", "CodeBleu_score": 44.47683017223484}]}
{"instance_id": "56da4547c101", "generation_result": [{"code": "def get_codebook_entry(self, indices, shape)\n\t\n    one_hot = F.one_hot(indices, num_classes=self.embedding.shape[0])\n    z_q = torch.matmul(one_hot.float(), self.embedding.weight.T)\n    if shape is not None:\n        z_q = z_q.view(shape)\n        z_q = z_q.permute(0, 3, 1, 2).contiguous()\n    return z_q\n\n", "CodeBleu_score": 55.36815770438099}, {"code": "def get_codebook_entry(self, indices, shape)\n\t\n    one_hot = F.one_hot(indices, num_classes=self.num_embeddings).type(self.dtype)\n    quantized = torch.matmul(one_hot, self.embedding.weight.t())\n    quantized = quantized.view(shape)\n    quantized = quantized.permute(0, 3, 1, 2).contiguous()\n    return quantized\n\n", "CodeBleu_score": 24.18989206676149}, {"code": "def get_codebook_entry(self, indices, shape)\n\t\n    one_hot = torch.zeros(indices.shape[0], self.num_embeddings, device=self.device).scatter_(1, indices.unsqueeze(1), 1)\n    quantized = torch.matmul(one_hot, self.embedding.weight)\n    if shape is not None:\n        quantized = quantized.view(shape)\n        quantized = quantized.permute(0, 3, 1, 2).contiguous()\n    return quantized\n\n", "CodeBleu_score": 46.39850751651955}]}
{"instance_id": "548f8a1fb37d", "generation_result": [{"code": "def make_noise(self)\n\t\n    noise = torch.randn(4, 4, device=self.device)\n    noise_list = [noise]\n    for i in range(3, self.log_size):\n        size = 2 ** i\n        noise_list.append(torch.randn(size, size, device=self.device))\n        noise_list.append(torch.randn(size, size, device=self.device))\n    return noise_list\n\n", "CodeBleu_score": 31.954942907203794}, {"code": "def make_noise(self)\n\t\n    noise = torch.randn(4, 4, device=self.device)\n    self.noise.append(noise)\n    for i in range(3, self.log_size):\n        self.noise.append(torch.randn(2 ** i, 2 ** i, device=self.device))\n        self.noise.append(torch.randn(2 ** i, 2 ** i, device=self.device))\n    return self.noise\n\n", "CodeBleu_score": 30.226168711681446}, {"code": "def make_noise(self)\n\t\n    # Initialize the list of noise tensors.\n    noises = [torch.randn(4, 4, device=self.device)]\n    # For each size from 2^3 to 2^log_size, append two noise tensors of the corresponding size to the list.\n    for i in range(3, self.log_size):\n        noises.append(torch.randn(2 ** i, 2 ** i, device=self.device))\n        noises.append(torch.randn(2 ** i, 2 ** i, device=self.device))\n    return noises\n\n", "CodeBleu_score": 31.242692009893585}]}
{"instance_id": "42fd805563c6", "generation_result": [{"code": "def mean_latent(self, num_latent)\n\t\n    latent = torch.randn(num_latent, self.latent_dim, device=self.device)\n    latent = self.style_mlp(latent)\n    return torch.mean(latent, dim=0)\n\n", "CodeBleu_score": 28.459814276715576}, {"code": "def mean_latent(self, num_latent)\n\t\n    mean_latent = torch.randn(num_latent, self.latent_dim, device=self.device)\n    mean_latent = self.style_mlp(mean_latent)\n    mean_latent = mean_latent.mean(0)\n    return mean_latent\n\n", "CodeBleu_score": 26.559707321257832}, {"code": "def mean_latent(self, num_latent)\n\t\n    mean_latent = torch.randn(num_latent, self.latent_dim)\n    mean_latent = self.style_mlp(mean_latent)\n    mean_latent = mean_latent.mean(0)\n    return mean_latent\n\n", "CodeBleu_score": 18.07300455399596}]}
{"instance_id": "2f485fd924af", "generation_result": [{"code": "def color_jitter(img, shift)\n\t\n    img = img.astype(np.float32)\n    shift = np.array(shift, dtype=np.float32)\n    r = np.random.uniform(-shift[0], shift[0])\n    g = np.random.uniform(-shift[1], shift[1])\n    b = np.random.uniform(-shift[2], shift[2])\n    img[:, :, 0] = np.clip(img[:, :, 0] + r, 0., 1.)\n    img[:, :, 1] = np.clip(img[:, :, 1] + g, 0., 1.)\n    img[:, :, 2] = np.clip(img[:, :, 2] + b, 0., 1.)\n    return img\n\n", "CodeBleu_score": 25.02249533271257}, {"code": "def color_jitter(img, shift)\n\t\n    jitter = np.array([shift, shift, shift])\n    jitter = np.random.randint(-jitter, jitter, 3)\n    img = img + jitter / 255\n    img = np.clip(img, 0, 1)\n    return img\n\n", "CodeBleu_score": 48.68692359253787}, {"code": "def color_jitter(img, shift)\n\t\n    shift = np.array(shift)\n    img = img.astype(np.float32)\n    img = np.clip(img + np.random.uniform(-shift, shift, 3), 0, 1)\n    return img\n", "CodeBleu_score": 39.367899835064755}]}
{"instance_id": "c87ccf1fbf48", "generation_result": [{"code": "def feed_data(self, data)\n\t\n    self.lq = data['lq'].to(self.device)\n    if 'gt' in data:\n        self.gt = data['gt'].to(self.device)\n    if 'face_locations' in data:\n        self.left_eye = data['face_locations']['left_eye']\n        self.right_eye = data['face_locations']['right_eye']\n        self.mouth = data['face_locations']['mouth']\n\n", "CodeBleu_score": 56.933876915027504}, {"code": "def feed_data(self, data)\n\t\n    self.lq = data['lq'].to(self.device)\n    self.gt = data['gt'].to(self.device) if 'gt' in data else None\n    if 'face_locations' in data:\n        self.left_eye_loc = data['face_locations'][:, :2]\n        self.right_eye_loc = data['face_locations'][:, 2:]\n        self.mouth_loc = data['mouth_locations']\n", "CodeBleu_score": 47.51344612466775}, {"code": "def feed_data(self, data)\n\t\n    self.lq = data['lq'].to(self.device)\n    if 'gt' in data:\n        self.gt = data['gt'].to(self.device)\n    if 'gt_path' in data:\n        self.gt_path = data['gt_path']\n    if 'lq_path' in data:\n        self.lq_path = data['lq_path']\n    if 'key' in data:\n        self.key = data['key']\n    if 'lq_path' in data:\n        self.lq_path = data['lq_path']\n    if 'gt_path' in data:\n        self.gt_path = data['gt_path']\n    if 'key' in data:\n        self.key = data['key']\n    if 'face_landmarks' in data:\n        self.face_landmarks = data['face_landmarks']\n    if 'left_eye' in data:\n        self.left_eye = data['left_eye']\n    if 'right_eye' in data:\n        self.right_eye = data['right_eye']\n    if 'mouth' in data:\n        self.mouth = data['mouth']\n\n", "CodeBleu_score": 48.30552169607759}]}
{"instance_id": "8f8c7cb13037", "generation_result": [{"code": "def construct_img_pyramid(self)\n\t\n\n    img_pyramid = [self.gt]\n    for i in range(self.num_scales):\n        img_pyramid.insert(0, F.interpolate(img_pyramid[0], scale_factor=self.scale_factor, mode='bilinear'))\n    return img_pyramid\n\n", "CodeBleu_score": 21.821205031594022}, {"code": "def construct_img_pyramid(self)\n\t\n    pyramid_gt = []\n    pyramid_gt.insert(0, self.gt)\n    for i in range(self.num_inter_layers):\n        pyramid_gt.insert(0, F.interpolate(pyramid_gt[0], scale_factor=0.5, mode='bilinear', align_corners=False))\n    return pyramid_gt\n\n", "CodeBleu_score": 32.5707135681094}, {"code": "def construct_img_pyramid(self)\n\t\n    pyramid_gt = [self.gt]\n    for i in range(self.n_levels):\n        pyramid_gt.insert(0, F.interpolate(pyramid_gt[0], scale_factor=0.5, mode='bilinear', align_corners=False))\n    return pyramid_gt\n\n", "CodeBleu_score": 27.696981663003772}]}
{"instance_id": "51247274d40c", "generation_result": [{"code": "def save(self, epoch, current_iter)\n\t\n    self.save_network(self.gen_net, \"gen_net\", epoch, current_iter)\n    self.save_network(self.gen_net_ema, \"gen_net_ema\", epoch, current_iter)\n    self.save_network(self.disc_net, \"disc_net\", epoch, current_iter)\n    if self.use_facial_discriminators:\n        self.save_network(self.left_eye_disc_net, \"left_eye_disc_net\", epoch, current_iter)\n        self.save_network(self.right_eye_disc_net, \"right_eye_disc_net\", epoch, current_iter)\n        self.save_network(self.mouth_disc_net, \"mouth_disc_net\", epoch, current_iter)\n    self.save_training_state(epoch, current_iter)\n\n", "CodeBleu_score": 52.36327409097788}, {"code": "def save(self, epoch, current_iter)\n\t\n    # Save the generator network\n    self.save_network(self.G, 'G', epoch, current_iter)\n    # Save the EMA version of the generator\n    self.save_network(self.G_EMA, 'G_EMA', epoch, current_iter)\n    # Save the discriminator network\n    self.save_network(self.D, 'D', epoch, current_iter)\n    # Save the discriminator networks for the eyes\n    if self.eye_disc:\n        self.save_network(self.D_eye_left, 'D_eye_left', epoch, current_iter)\n        self.save_network(self.D_eye_right, 'D_eye_right', epoch, current_iter)\n    # Save the discriminator network for the mouth\n    if self.mouth_disc:\n        self.save_network(self.D_mouth, 'D_mouth', epoch, current_iter)\n    # Save the training state\n    self.save_training_state(epoch, current_iter)\n\n", "CodeBleu_score": 53.69460449271843}, {"code": "def save(self, epoch, current_iter)\n\t\n    if self.save_network:\n        self.save_network_state(epoch, current_iter)\n    if self.save_training_state:\n        self.save_training_state(epoch, current_iter)\n\n", "CodeBleu_score": 24.25990281142223}]}
{"instance_id": "afa0dc8128c1", "generation_result": [{"code": "def clean_folder(folder)\n\t\n    for item in os.listdir(folder):\n        item = os.path.join(folder, item)\n        if os.path.isfile(item):\n            try:\n                os.unlink(item)\n            except Exception as e:\n                print(f\"Error deleting {item}: {e}\")\n        elif os.path.isdir(item):\n            try:\n                shutil.rmtree(item)\n            except Exception as e:\n                print(f\"Error deleting {item}: {e}\")\n        elif os.path.islink(item):\n            try:\n                os.unlink(item)\n            except Exception as e:\n                print(f\"Error deleting {item}: {e}\")\n\n", "CodeBleu_score": 43.6280426137928}, {"code": "def clean_folder(folder)\n\t\n    for item in os.listdir(folder):\n        item_path = os.path.join(folder, item)\n        if os.path.isfile(item_path) or os.path.islink(item_path):\n            try:\n                os.unlink(item_path)\n            except Exception as e:\n                print(f\"Error occurred while deleting {item_path}: {e}\")\n        elif os.path.isdir(item_path):\n            try:\n                shutil.rmtree(item_path)\n            except Exception as e:\n                print(f\"Error occurred while deleting {item_path}: {e}\")\n\n", "CodeBleu_score": 47.756623188047506}, {"code": "def clean_folder(folder)\n\t\n    for item in os.listdir(folder):\n        path = os.path.join(folder, item)\n        try:\n            if os.path.isfile(path):\n                os.unlink(path)\n            elif os.path.isdir(path):\n                shutil.rmtree(path)\n            elif os.path.islink(path):\n                os.unlink(path)\n        except Exception as e:\n            print('Failed to delete %s. Reason: %s' % (path, e))\n\n", "CodeBleu_score": 48.31981965293412}]}
{"instance_id": "53588b481bdd", "generation_result": [{"code": "def next_batch(self, sample_size=1)\n\t\n    if len(self.experiences) < sample_size:\n        sample_size = len(self.experiences)\n    batch = random.sample(self.experiences, sample_size)\n    for _ in range(sample_size):\n        self.experiences.pop(0)\n    return batch\n\n", "CodeBleu_score": 46.52647190089346}, {"code": "def next_batch(self, sample_size=1)\n\t\n    if sample_size > len(self.experiences):\n        sample_size = len(self.experiences)\n\n    batch = random.sample(self.experiences, sample_size)\n    self.experiences = [experience for experience in self.experiences if experience not in batch]\n\n    return batch\n\n", "CodeBleu_score": 64.16417619471308}, {"code": "def next_batch(self, sample_size=1)\n\t\n    if sample_size > len(self.experiences):\n        sample_size = len(self.experiences)\n    sample = random.sample(self.experiences, sample_size)\n    self.experiences = [i for i in self.experiences if i not in sample]\n    return sample\n\n", "CodeBleu_score": 65.67029633081789}]}
{"instance_id": "6bfa6a4e621c", "generation_result": [{"code": "def find_estimated_top_configs(self)\n\t\n    all_configs = self.get_all_configs()\n    all_estimated_costs = self.predict(all_configs)\n    indices = np.argsort(all_estimated_costs)\n    if self.metric == \"cost\":\n        return indices\n    elif self.metric == \"error\":\n        return indices[::-1]\n    else:\n        raise ValueError(\"Metric must be either 'cost' or 'error'.\")\n\n", "CodeBleu_score": 14.201211442239789}, {"code": "def find_estimated_top_configs(self)\n\t\n    # Flatten each configuration in all_configs, collect numerical values, and convert them into a numpy array\n    flattened_ds_config = []\n    for config in self.all_configs:\n        flattened_ds_config.append(self.flatten(config))\n    flattened_ds_config = np.array(flattened_ds_config)\n\n    # Predict estimates for these configurations, and sort them to identify the top configurations\n    estimates = self.cost_model.predict(flattened_ds_config)\n    if self.metric == \"cost\":\n        indices = np.argsort(estimates)[:self.top_k]\n    else:\n        indices = np.argsort(estimates)[-self.top_k :]\n\n    return indices\n\n", "CodeBleu_score": 28.608111205709392}, {"code": "def find_estimated_top_configs(self)\n\t\n    # flatten all configs\n    flattened_ds_config = {}\n    for config in self.all_configs:\n        flattened_ds_config[config.config_id] = flatten(config.get_dictionary())\n\n    # collect numerical values\n    feature_val = []\n    for config_id in flattened_ds_config:\n        feature_val.append(list(flattened_ds_config[config_id].values()))\n    feature_val = np.array(feature_val)\n\n    # predict\n    configs = self.predict(feature_val)\n    configs = configs.tolist()\n\n    # sort\n    if self.metric == 'accuracy':\n        configs.sort(reverse=True)\n    else:\n        configs.sort()\n\n    # return top configs\n    len_configs = len(configs)\n    if self.top_k > len_configs:\n        return configs\n    else:\n        return configs[:self.top_k]\n\n", "CodeBleu_score": 40.11124417865548}]}
{"instance_id": "d4cf48413a57", "generation_result": [{"code": "def flatten(d, parent_key='', sep='_')\n\t\n    items = []\n    for k, v in d.items():\n        new_key = parent_key + sep + k if parent_key else k\n        if isinstance(v, collections.MutableMapping):\n            items.extend(flatten(v, new_key, sep=sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)\n", "CodeBleu_score": 76.93644067302131}, {"code": "def flatten(d, parent_key='', sep='_')\n\t\n    items = []\n    for k, v in d.items():\n        new_key = parent_key + sep + k if parent_key else k\n        if isinstance(v, collections.abc.MutableMapping):\n            items.extend(flatten(v, new_key, sep=sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)\n\n", "CodeBleu_score": 82.56863782179195}, {"code": "def flatten(d, parent_key='', sep='_')\n\t\n    items = []\n    for k, v in d.items():\n        new_key = parent_key + sep + k if parent_key else k\n        if isinstance(v, collections.abc.MutableMapping):\n            items.extend(flatten(v, new_key, sep=sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)\n\n", "CodeBleu_score": 82.56863782179195}]}
{"instance_id": "c2a514249c25", "generation_result": [{"code": "def run_after_tuning(self)\n\t\n    if self.optimal_cmd:\n        logger.info(f\"Autotuning found an optimal command: {self.optimal_cmd}\")\n        result = subprocess.Popen(self.optimal_cmd, shell=True)\n        result.wait()\n    else:\n        logger.info(\"Autotuning did not find an optimal configuration\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 57.56246057134548}, {"code": "def run_after_tuning(self)\n\t\n    if self.optimal_cmd:\n        logger.info(f\"Running the optimal command: {self.optimal_cmd}\")\n        result = subprocess.Popen(self.optimal_cmd, shell=True)\n        result.wait()\n    else:\n        logger.info(\"Autotuning did not find an optimal configuration.\")\n\n", "CodeBleu_score": 57.62444511505236}, {"code": "def run_after_tuning(self)\n\t\n    if self.optimal_cmd:\n        logger.info(\"Running the optimal command: {}\".format(self.optimal_cmd))\n        result = subprocess.Popen(self.optimal_cmd, shell=True)\n        result.wait()\n    else:\n        logger.info(\"Autotuning did not find an optimal configuration\")\n", "CodeBleu_score": 54.04387502295602}]}
{"instance_id": "61802de52df0", "generation_result": [{"code": "def run_job(self, exp: dict, reservations)\n\t\n\n    # set the experiment's master port and result directory\n    exp['master_port'] = self.get_val_by_key(reservations, 'master_port')\n    exp['result_dir'] = self.get_val_by_key(reservations, 'result_dir')\n\n    # update user arguments based on argument mappings\n    for mapping in exp['argument_mappings']:\n        key = mapping['key']\n        val = self.get_val_by_key(reservations, key)\n        if val is not None:\n            exp['user_args'] = self.update_user_args(exp['user_args'], mapping['arg'], val)\n\n    # start the experiment in a new thread\n    t = threading.Thread(target=self.run_exp, args=(exp,))\n    t.start()\n\n    # store the running experiment details with a timestamp\n    running_exp = {\n        'timestamp': time.time(),\n        'exp': exp\n    }\n    self.running_experiments.append(running_exp)\n\n", "CodeBleu_score": 30.526498147760396}, {"code": "def run_job(self, exp: dict, reservations)\n\t\n    exp_id = exp[\"id\"]\n    script = exp[\"script\"]\n    user_args = exp[\"user_args\"]\n    arg_mappings = exp[\"arg_mappings\"]\n    master_port = self.get_master_port(reservations)\n    reservations = self.get_reservations(exp_id)\n    result_dir = self.get_result_dir(exp_id)\n    self.set_val_by_key(exp, \"master_port\", master_port)\n    self.set_val_by_key(exp, \"result_dir\", result_dir)\n    self.update_user_args(user_args, arg_mappings)\n    t = threading.Thread(target=self.run_exp, args=(script, user_args))\n    t.start()\n    self.running_experiments[str(time.time())] = {\n        \"exp_id\": exp_id,\n        \"script\": script,\n        \"user_args\": user_args,\n        \"master_port\": master_port,\n        \"result_dir\": result_dir,\n    }\n\n", "CodeBleu_score": 26.369265154690513}, {"code": "def run_job(self, exp: dict, reservations)\n\t\n\n    # Set the experiment's master port and result directory\n    exp[\"master_port\"] = self.master_port\n    exp[\"result_dir\"] = self.result_dir\n\n    # Update user arguments based on argument mappings\n    if \"user_args\" in exp:\n        user_args = exp[\"user_args\"]\n        for arg in user_args:\n            if \"mapping\" in arg:\n                if arg[\"mapping\"] == \"time\":\n                    arg[\"val\"] = time.time()\n                elif arg[\"mapping\"] == \"reservation\":\n                    if arg[\"reservation_id\"] in reservations:\n                        reservation = reservations[arg[\"reservation_id\"]]\n                        if arg[\"reservation_key\"] in reservation:\n                            arg[\"val\"] = reservation[arg[\"reservation_key\"]]\n                        else:\n                            self.logger.error(\n                                f\"The reservation with id {arg['reservation_id']} does not contain the key {arg['reservation_key']}.\"\n                            )\n                            return\n                    else:\n                        self.logger.error(\n                            f\"The reservation with id {arg['reservation_id']} does not exist.\"\n                        )\n                        return\n            else:\n                arg[\"val\"] = arg[\"default\"]\n\n    # Start the experiment in a new thread\n    thread = threading.Thread(target=self.run_experiment, args=(exp,))\n    thread.start()\n\n    # Store the running experiment details with a timestamp in running_experiments\n    exp[\"timestamp\"] = time.time()\n    self.running_experiments.append(exp)\n\n", "CodeBleu_score": 26.188100468066377}]}
{"instance_id": "4884e2e9665c", "generation_result": [{"code": "def save_exp_results_to_database(self, message, ranks=None, path=None):\n\t\n\n    if dist.is_initialized():\n        if dist.get_rank() == 0:\n            if path is None:\n                path = self.exp_results_path\n            if ranks is None:\n                ranks = [dist.get_rank()]\n            for rank in ranks:\n                message['rank'] = rank\n                with open(path, 'a') as outfile:\n                    json.dump(message, outfile)\n                    outfile.write('", "CodeBleu_score": 35.03237655027933}, {"code": "def save_exp_results_to_database(self, message, ranks=None, path=None):\n\t\n    if dist.is_initialized() and dist.get_rank() == 0:\n        if ranks is not None:\n            for rank in ranks:\n                message[\"rank\"] = rank\n                self.save_exp_results_to_database(message, path=path)\n        else:\n            if path is None:\n                path = self.exp_results_path\n            with open(path, \"a\") as outfile:\n                json.dump(message, outfile)\n                outfile.write(\"", "CodeBleu_score": 31.461087472093517}, {"code": "def save_exp_results_to_database(self, message, ranks=None, path=None):\n\t\n    if dist.is_initialized():\n        if dist.get_rank() in ranks:\n            if path is not None:\n                with open(path, \"a\") as outfile:\n                    message[\"rank\"] = dist.get_rank()\n                    json.dump(message, outfile)\n                    outfile.write(\"", "CodeBleu_score": 26.441730308816226}]}
{"instance_id": "0429344c3c4d", "generation_result": [{"code": "def _build_tp_other_layer_map(self, layer_index: int)\n\t\n    self.logger.debug(f'Building data map for layer index {layer_index}')\n    if not self.layer_files:\n        raise ValueError('No layer files were found.')\n    if layer_index < 0 or layer_index >= len(self.layer_files):\n        raise ValueError(f'Layer index {layer_index} is out of range.')\n    files = get_files_with_prefix(self.layer_files[layer_index], self.tp_degree_prefix)\n    partition_map = partition_data(files, self.tp_degree_prefix)\n    self.logger.debug(f'Data map for layer index {layer_index} was built successfully.')\n    return partition_map\n\n", "CodeBleu_score": 38.01988404176177}, {"code": "def _build_tp_other_layer_map(self, layer_index: int)\n\t\n    layer_files = self._get_files_with_prefix(f\"{self.layer_prefix}{layer_index}\")\n    if not layer_files:\n        raise ValueError(f\"No layer files found for layer index {layer_index}\")\n    if layer_index < 0 or layer_index >= self.num_layers:\n        raise ValueError(f\"Layer index {layer_index} is not in the range [0, {self.num_layers})\")\n    layer_files = self._partition_data(layer_files, self.tp_degree)\n    layer_map = {i: files for i, files in enumerate(layer_files)}\n    return layer_map\n\n", "CodeBleu_score": 38.79018646741245}, {"code": "def _build_tp_other_layer_map(self, layer_index: int)\n\t\n    if len(self.layer_files) == 0:\n        raise ValueError(\"No layer files found.\")\n    if layer_index < 0 or layer_index >= len(self.layer_files):\n        raise ValueError(f\"Layer index {layer_index} out of range.\")\n\n    layer_files = get_files_with_prefix(self.layer_files[layer_index], self.tp_degree_prefix)\n    layer_files_partitioned = partition_data(layer_files, self.tp_degree_prefix)\n\n    return {tp_degree: layer_files for tp_degree, layer_files in enumerate(layer_files_partitioned)}\n\n", "CodeBleu_score": 44.600943261148466}]}
{"instance_id": "1b65ecb9be93", "generation_result": [{"code": "def system_info()\n\t\n    system = platform.system()\n    architecture = platform.architecture()\n    version = platform.version()\n    processor = platform.processor()\n    python_version = platform.python_version()\n\n    system_info = {\n        \"system\": system,\n        \"architecture\": architecture,\n        \"version\": version,\n        \"processor\": processor,\n        \"python_version\": python_version,\n    }\n\n    return system_info\n", "CodeBleu_score": 43.5005932741235}, {"code": "def system_info()\n\t\n    system_info = {\n        \"operating system\": platform.system(),\n        \"architecture\": platform.architecture(),\n        \"version\": platform.version(),\n        \"processor\": platform.processor(),\n        \"python version\": platform.python_version(),\n    }\n    return system_info\n\n", "CodeBleu_score": 64.46598813573664}, {"code": "def system_info()\n\t\n    system_info_dict = {\n        \"operating_system\": platform.system(),\n        \"architecture\": platform.architecture(),\n        \"version\": platform.version(),\n        \"processor\": platform.processor(),\n        \"python_version\": platform.python_version(),\n    }\n    return system_info_dict\n\n", "CodeBleu_score": 63.775301191056286}]}
{"instance_id": "969bcc5cf3b5", "generation_result": [{"code": "def cuda_info()\n\t\n    import pynvml\n    pynvml.nvmlInit()\n    deviceCount = pynvml.nvmlDeviceGetCount()\n    cuda_version = pynvml.nvmlSystemGetCudaDriverVersion()\n    cuda_version = str(cuda_version)\n    cuda_version = cuda_version.split(' ')[0]\n    print(\"CUDA version:\", cuda_version)\n    print(\"Device Count:\", deviceCount)\n    for i in range(deviceCount):\n        handle = pynvml.nvmlDeviceGetHandleByIndex(i)\n        name = pynvml.nvmlDeviceGetName(handle)\n        print(\"Device\", i, \":\", name)\n        print(\"Device\", i, \":\", pynvml.nvmlDeviceGetComputeMode(handle))\n        print(\"Device\", i, \":\", pynvml.nvmlDeviceGetCurrPcieLinkGeneration(handle))\n        print(\"Device\", i, \":\", pynvml.nvmlDeviceGetCurrPcieLinkWidth(handle))\n        print(\"Device\", i, \":\", pynvml.nvmlDeviceGetCurrPcieLinkGeneration(handle))\n        print(\"Device\", i, \":\", pynvml.nvmlDeviceGetCurrPcieLinkWidth(handle))\n        print(\"Device\", i, \":\", pynvml.nvmlDeviceGetCurrPcieLinkGeneration(handle))\n        print(\"Device\", i, \":\", pynvml.nvmlDeviceGetCurrPcieLinkWidth(handle))\n        print(\"Device\", i, \":\", pynvml.nvmlDeviceGetCurrPcieLinkGeneration(handle))\n        print(\"Device\", i, \":\", pynvml.nvmlDeviceGetCurrPcieLinkWidth(handle))\n        print(\"Device\", i, \":\", pynvml.nvmlDeviceGetCurrPcieLinkGeneration(handle))\n        print(\"Device\", i, \":\", pynvml.nvmlDeviceGetCurrPcieLinkWidth(handle))\n        print(\"Device\", i, \":\", pynvml.", "CodeBleu_score": 26.3630184733065}, {"code": "def cuda_info()\n\t\n    import pynvml\n    pynvml.nvmlInit()\n    deviceCount = pynvml.nvmlDeviceGetCount()\n    devices = []\n    for i in range(deviceCount):\n        handle = pynvml.nvmlDeviceGetHandleByIndex(i)\n        name = pynvml.nvmlDeviceGetName(handle)\n        devices.append(name)\n    pynvml.nvmlShutdown()\n    return devices\n\n", "CodeBleu_score": 22.224874029880937}, {"code": "def cuda_info()\n\t\n    import pynvml\n    pynvml.nvmlInit()\n    deviceCount = pynvml.nvmlDeviceGetCount()\n    print(deviceCount)\n    if deviceCount == 0:\n        print(\"There is no device supporting CUDA\")\n    else:\n        for i in range(deviceCount):\n            handle = pynvml.nvmlDeviceGetHandleByIndex(i)\n            print(\"Device\", i, \":\", pynvml.nvmlDeviceGetName(handle))\n            print(\"Device\", i, \"is\",\n                  \"available\" if pynvml.nvmlDeviceGetComputeRunningProcesses(handle) == 0 else \"NOT available\")\n    print(\"CUDA Driver Version:\", pynvml.nvmlSystemGetDriverVersion())\n    print(\"CUDA Runtime Version:\", pynvml.nvmlSystemGetDriverVersion())\n    pynvml.nvmlShutdown()\n\n", "CodeBleu_score": 36.860208455616004}]}
{"instance_id": "a2ba908a4c70", "generation_result": [{"code": "def compute_encoder_accuracy(dataset_items, encoder_manager)\n\t\n    # Get the class name key and optional class ID to class name mapping from the encoder manager's configuration\n    class_name_key = encoder_manager.config[\"class_name_key\"]\n    class_id_to_class_name_mapping = encoder_manager.config.get(\n        \"class_id_to_class_name_mapping\", None\n    )\n\n    # Create a dictionary to record the accuracy for each class\n    class_acc_dict = {}\n\n    # Iterate through the dataset\n    for item in tqdm(dataset_items):\n        # Extract the audio file\n        audio_file = item[\"audio_file\"]\n\n        # Compute the embedding\n        embedding = encoder_manager.compute_embedding_from_clip(audio_file)\n\n        # Predict the class label if criteria and mappings are available\n        if class_name_key in item and class_id_to_class_name_mapping is not None:\n            # Get the class name and class ID from the item\n            class_name = item[class_name_key]\n            class_id = class_id_to_class_name_mapping[class_name]\n\n            # Get the predicted class label\n            predicted_class_label = encoder_manager.inference(embedding)\n\n            # Record the accuracy for the class if it is the first time we see it\n            if class_id not in class_acc_dict:\n                class_acc_dict[class_id] = []\n\n            # Record the accuracy if the predicted class label matches the class ID\n            if predicted_class_label == class_id:\n                class_acc_dict[class_id].append(1)\n            else:\n                class_acc_dict[class_id].append(0)\n\n    # Calculate and print the accuracy for each class\n    for class_id, accuracies in class_acc_dict.items():\n        # Get the class name\n        class_name = class_id_to_class_name_mapping[class_id]\n\n        # Calculate the accuracy\n        acc = sum(accuracies) / len(accuracies)\n\n        # Print the accuracy\n        print(f\"", "CodeBleu_score": 34.04993136859352}, {"code": "def compute_encoder_accuracy(dataset_items, encoder_manager)\n\t\n    # Get the class name key and optional class ID to class name mapping from the encoder manager's configuration\n    class_name_key = encoder_manager.config['class_name_key']\n    class_id_to_class_name_map = encoder_manager.config.get('class_id_to_class_name_map', None)\n    # Initialize the class accuracy dictionary\n    class_acc_dict = {}\n    # Iterate through the dataset\n    for item in tqdm(dataset_items):\n        # Extract the audio file\n        audio_file = item['audio_file']\n        # Compute the embedding\n        embedding = encoder_manager.compute_embedding_from_clip(audio_file)\n        # Predict the class label if criteria and mappings are available\n        if class_name_key in item and class_id_to_class_name_map is not None:\n            # Get the true class name\n            true_class_name = item[class_name_key]\n            # Get the class ID\n            class_id = class_id_to_class_name_map[true_class_name]\n            # Predict the class label\n            class_name = encoder_manager.inference(embedding.cuda().unsqueeze(0))\n            # Convert the class name to a string\n            class_name = str(class_name)\n            # Get the predicted class ID\n            predicted_class_id = int(class_name.split('_')[0])\n            # Get the predicted class name\n            predicted_class_name = class_id_to_class_name_map[predicted_class_id]\n            # Check if the predicted class name is the same as the true class name\n            if predicted_class_name == true_class_name:\n                # If so, update the class accuracy dictionary\n                if true_class_name in class_acc_dict:\n                    class_acc_dict[true_class_name] += 1\n                else:\n                    class_acc_dict[true_class_name] = 1\n    # Calculate the accuracy for each class\n    for key, value in class_acc_dict.items():\n        class_", "CodeBleu_score": 33.64100970036337}, {"code": "def compute_encoder_accuracy(dataset_items, encoder_manager)\n\t\n    class_acc_dict = {}\n    for class_name, class_id in encoder_manager.class_name_to_id_map.items():\n        class_acc_dict[class_name] = {\n            'correct': 0,\n            'total': 0,\n            'class_id': class_id\n        }\n    for item in tqdm(dataset_items):\n        audio_file = item[0]\n        true_class_name = item[1]\n        if true_class_name in encoder_manager.class_name_to_id_map:\n            true_class_id = encoder_manager.class_name_to_id_map[true_class_name]\n            embedding = encoder_manager.compute_embedding_from_clip(audio_file)\n            predicted_class_id = inference(embedding.cuda())\n            predicted_class_name = encoder_manager.class_id_to_name_map[predicted_class_id]\n            class_acc_dict[true_class_name]['total'] += 1\n            if true_class_id == predicted_class_id:\n                class_acc_dict[true_class_name]['correct'] += 1\n    for class_name, acc_dict in class_acc_dict.items():\n        acc_dict['accuracy'] = acc_dict['correct'] / acc_dict['total']\n    class_acc_dict['average_accuracy'] = sum([v['accuracy'] for k, v in class_acc_dict.items() if k != 'average_accuracy']) / len(class_acc_dict)\n    for class_name, acc_dict in class_acc_dict.items():\n        print('Accuracy for class %s: %s' % (class_name, acc_dict['accuracy']))\n    print('Average accuracy: %s' % class_acc_dict['average_accuracy'])\n    return class_acc_dict\n\n", "CodeBleu_score": 33.99733754611536}]}
{"instance_id": "e01ec69ad58a", "generation_result": [{"code": "def adjust_path_and_remove_silence(audio_path)\n\t\n    output_path = audio_path.replace(\n        input_directory, output_directory\n    )\n    if Path(output_path).exists() and not force:\n        return output_path, False\n    Path(output_path).parent.mkdir(parents=True, exist_ok=True)\n    speech_detected = remove_silence(\n        audio_path, output_path, min_silence_len, silence_thresh, seek_step\n    )\n    return output_path, speech_detected\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 26.44949796053431}, {"code": "def adjust_path_and_remove_silence(audio_path)\n\t\n    # Adjust the output path\n    audio_path = audio_path.replace(\n        args.input_dir, args.output_dir\n    )\n\n    # Check if the file already exists\n    if os.path.exists(audio_path) and not args.force:\n        print(f\"{audio_path} already exists. Skipping...\")\n        return audio_path, False\n\n    # Create the necessary directory structure\n    os.makedirs(os.path.dirname(audio_path), exist_ok=True)\n\n    # Remove silence from the audio\n    print(f\"Removing silence from {audio_path}\")\n    audio, speech_detected = remove_silence(\n        audio_path,\n        args.chunk_size,\n        args.chunk_padding,\n        args.chunk_threshold,\n        args.sample_rate,\n        args.frame_size,\n        args.frame_stride,\n        args.frame_padding,\n        args.frame_threshold,\n    )\n\n    # Save the processed audio\n    print(f\"Saving processed audio to {audio_path}\")\n    audio.export(audio_path, format=\"wav\")\n\n    return audio_path, speech_detected\n\n", "CodeBleu_score": 36.30923742452657}, {"code": "def adjust_path_and_remove_silence(audio_path)\n\t\n    output_path = audio_path.replace(input_dir, output_dir)\n    output_path = output_path.replace(input_dir_name, output_dir_name)\n    output_path = output_path.replace(input_dir_name_2, output_dir_name_2)\n    output_path = output_path.replace(input_dir_name_3, output_dir_name_3)\n    output_path = output_path.replace(input_dir_name_4, output_dir_name_4)\n    output_path = output_path.replace(input_dir_name_5, output_dir_name_5)\n    output_path = output_path.replace(input_dir_name_6, output_dir_name_6)\n    output_path = output_path.replace(input_dir_name_7, output_dir_name_7)\n    output_path = output_path.replace(input_dir_name_8, output_dir_name_8)\n    output_path = output_path.replace(input_dir_name_9, output_dir_name_9)\n    output_path = output_path.replace(input_dir_name_10, output_dir_name_10)\n    output_path = output_path.replace(input_dir_name_11, output_dir_name_11)\n    output_path = output_path.replace(input_dir_name_12, output_dir_name_12)\n    output_path = output_path.replace(input_dir_name_13, output_dir_name_13)\n    output_path = output_path.replace(input_dir_name_14, output_dir_name_14)\n    output_path = output_path.replace(input_dir_name_15, output_dir_name_15)\n    output_path = output_path.replace(input_dir_name_16, output_dir_name_16)\n    output_path = output_path.replace(input_dir_name_17, output_dir_name_17)\n    output_", "CodeBleu_score": 15.240629287932503}]}
{"instance_id": "7e0bdb0feff8", "generation_result": [{"code": "def resample_files(input_dir, output_sr, output_dir=None, file_ext=\"wav\", n_jobs=10)\n\t\n    if output_dir is None:\n        output_dir = input_dir\n    copytree(input_dir, output_dir)\n    files = glob.glob(output_dir + \"/**/*.\" + file_ext, recursive=True)\n    print(f\"Found {len(files)} files to resample.\")\n    pbar = tqdm(total=len(files))\n\n    def resample_file(file):\n        resample(file, output_sr)\n        pbar.update(1)\n\n    with Pool(n_jobs) as p:\n        p.imap_unordered(resample_file, files)\n    pbar.close()\n    print(\"Done!\")\n\n", "CodeBleu_score": 39.85963170037732}, {"code": "def resample_files(input_dir, output_sr, output_dir=None, file_ext=\"wav\", n_jobs=10)\n\t\n\n    if output_dir is None:\n        output_dir = input_dir\n\n    # copy input directory to output directory\n    copytree(input_dir, output_dir)\n\n    # get all audio files with specified extension\n    audio_files = glob.glob(output_dir + \"**/*.\" + file_ext, recursive=True)\n\n    # resample files in parallel\n    with Pool(n_jobs) as p:\n        pbar = tqdm(total=len(audio_files))\n        p.imap_unordered(\n            partial(resample_file, output_sr=output_sr),\n            enumerate(audio_files),\n            chunksize=1,\n        )\n        pbar.update(n=len(audio_files))\n    pbar.close()\n    print(f\"Finished resampling {len(audio_files)} files.\")\n\n", "CodeBleu_score": 39.083545324835}, {"code": "def resample_files(input_dir, output_sr, output_dir=None, file_ext=\"wav\", n_jobs=10)\n\t\n    if output_dir is None:\n        output_dir = input_dir\n    copytree(input_dir, output_dir)\n    files = glob.glob(f\"{output_dir}/**/*.{file_ext}\", recursive=True)\n    pbar = tqdm(total=len(files))\n    with Pool(n_jobs) as p:\n        p.imap_unordered(\n            partial(resample_file, output_sr=output_sr), files, chunksize=10\n        )\n        pbar.update(10)\n    pbar.close()\n    print(f\"Resampling complete. {len(files)} files resampled.\")\n\n", "CodeBleu_score": 36.45946078582444}]}
{"instance_id": "a1351834ca57", "generation_result": [{"code": "def evaluation(model, criterion, data_loader, global_step)\n\t\n    model.eval()\n    eval_loss = 0\n    eval_accuracy = 0\n    num_batches = 0\n    labels = torch.tensor([], device=device)\n    embeddings = torch.tensor([], device=device)\n    with torch.no_grad():\n        for batch_idx, (inputs, target) in enumerate(data_loader):\n            num_batches += 1\n            inputs = inputs.reshape(-1, 1, 28, 28)\n            inputs = inputs.view(inputs.size(0), -1)\n            inputs = inputs.cuda()\n            target = target.view(-1)\n            target = target.cuda()\n            outputs = model(inputs)\n            loss = criterion(outputs, target)\n            eval_loss += loss.item()\n            _, predicted = torch.max(outputs.data, 1)\n            eval_accuracy += (predicted == target).sum().item()\n            labels = torch.cat((labels, target), 0)\n            embeddings = torch.cat((embeddings, outputs), 0)\n\n    eval_loss /= num_batches\n    eval_accuracy /= len(data_loader.dataset)\n\n    dashboard_logger.eval_stats(\n        loss=eval_loss,\n        accuracy=eval_accuracy,\n        global_step=global_step,\n    )\n\n    dashboard_logger.eval_figures(\n        embeddings=plot_embeddings(\n            embeddings.cpu().numpy(),\n            labels.cpu().numpy(),\n        ),\n        global_step=global_step,\n    )\n\n    return eval_loss\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 40.14558210341829}, {"code": "def evaluation(model, criterion, data_loader, global_step)\n\t\n    model.eval()\n    eval_loss = 0\n    for batch_idx, (inputs, labels) in enumerate(data_loader):\n        inputs = inputs.reshape(-1, 1, 28, 28)\n        labels = labels.reshape(-1, 1)\n        inputs, labels = inputs.cuda(), labels.cuda()\n        with torch.no_grad():\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            eval_loss += loss.item()\n        if batch_idx == len(data_loader) - 1:\n            embeddings = outputs.detach().view(outputs.size(0), -1).cpu().numpy()\n            labels = labels.view(labels.size(0)).cpu().numpy()\n            dashboard_logger.eval_figures(plot_embeddings(embeddings, labels), global_step)\n    eval_loss /= len(data_loader)\n    dashboard_logger.eval_stats(eval_loss, global_step)\n    model.train()\n    return eval_loss\n", "CodeBleu_score": 35.86248386812397}, {"code": "def evaluation(model, criterion, data_loader, global_step)\n\t\n    model.eval()\n    eval_loss = 0\n    with torch.no_grad():\n        for batch_index, (inputs, labels) in enumerate(data_loader):\n            inputs = inputs.view(-1, 1, 28, 28)\n            inputs = inputs.cuda()\n            labels = labels.cuda()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            eval_loss += loss.item()\n            if batch_index == 0:\n                embeddings = outputs.detach().cpu().numpy()\n                labels = labels.view(-1).cpu().numpy()\n                fig = plot_embeddings(embeddings, labels)\n                dashboard_logger.eval_figures(fig, global_step)\n    eval_loss /= len(data_loader)\n    dashboard_logger.eval_stats(eval_loss, global_step)\n    return eval_loss\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 32.085486585379016}]}
{"instance_id": "565f8ed09916", "generation_result": [{"code": "def register_config(model_name: str) -> Coqpit\n\t\n\n    # construct the configuration class name\n    config_class_name = model_name.title() + \"Config\"\n\n    # check if the model is \"xtts\" to import its specific configuration\n    if model_name == \"xtts\":\n        from xtts.config import XttsConfig\n\n        return XttsConfig\n\n    # search through predefined paths to find and assign the configuration class\n    for path in CONFIG_PATHS:\n        try:\n            config_class = find_module(config_class_name, path)\n            return config_class\n        except ModuleNotFoundError:\n            continue\n\n    # raise a ModuleNotFoundError if the configuration class is not found\n    raise ModuleNotFoundError(f\"Configuration class {config_class_name} not found\")\n\n", "CodeBleu_score": 35.969996700005446}, {"code": "def register_config(model_name: str) -> Coqpit\n\t\n    config_class_name = model_name.title() + 'Config'\n    if model_name == 'xtts':\n        try:\n            from xtts.config import XttsConfig\n            config_class = XttsConfig\n        except ModuleNotFoundError:\n            raise ModuleNotFoundError(f'Cannot find configuration class for model \"{model_name}\"')\n    else:\n        try:\n            from models.config import ModelConfig\n            config_class = ModelConfig\n        except ModuleNotFoundError:\n            try:\n                from xtts.config import XttsConfig\n                config_class = XttsConfig\n            except ModuleNotFoundError:\n                raise ModuleNotFoundError(f'Cannot find configuration class for model \"{model_name}\"')\n    return config_class\n\n", "CodeBleu_score": 32.27656814261428}, {"code": "def register_config(model_name: str) -> Coqpit\n\t\n    if model_name == \"xtts\":\n        config_class_name = \"XTTSConfig\"\n        import xtts.config as config_module\n    else:\n        config_class_name = model_name.capitalize() + \"Config\"\n        config_module = find_module(model_name)\n    config_class = getattr(config_module, config_class_name)\n    return config_class\n\n", "CodeBleu_score": 27.42632423230116}]}
{"instance_id": "0ea7862506a7", "generation_result": [{"code": "def load_config(config_path: str) -> Coqpit\n\t\n    config_dict = {}\n    ext = os.path.splitext(config_path)[1]\n    if ext == '.yaml':\n        with fsspec.open(config_path, 'r') as f:\n            config_dict = yaml.safe_load(f)\n    elif ext == '.json':\n        with fsspec.open(config_path, 'r') as f:\n            config_dict = json.load(f)\n    else:\n        raise TypeError(f\"Unknown file type: {ext}\")\n\n    model_name = _process_model_name(config_dict)\n    config_class = register_config(model_name)\n    config = config_class()\n    config.from_dict(config_dict)\n    return config\n\n", "CodeBleu_score": 49.815558626863506}, {"code": "def load_config(config_path: str) -> Coqpit\n\t\n    config_path = Path(config_path)\n    config_ext = config_path.suffix\n    if config_ext == \".json\":\n        with fsspec.open(config_path, \"r\") as f:\n            config_dict = json.load(f)\n    elif config_ext == \".yaml\":\n        with fsspec.open(config_path, \"r\") as f:\n            config_dict = yaml.safe_load(f)\n    else:\n        raise TypeError(f\"Unknown configuration file extension: {config_ext}\")\n\n    model_name = _process_model_name(config_dict)\n    register_config(model_name)\n    config_class = config.from_dict(config_dict)\n    config_obj = config_class()\n    config_obj.populate(config_dict)\n    return config_obj\n\n", "CodeBleu_score": 41.91004449752691}, {"code": "def load_config(config_path: str) -> Coqpit\n\t\n    ext = splitext(config_path)[-1]\n    if ext == \".yaml\" or ext == \".yml\":\n        with fsspec.open(config_path, \"r\") as f:\n            config_dict = yaml.safe_load(f)\n    elif ext == \".json\":\n        with fsspec.open(config_path, \"r\") as f:\n            config_dict = json.load(f)\n    else:\n        raise TypeError(f\"Unknown file type: {ext}\")\n\n    model_name = _process_model_name(config_dict)\n    register_config(model_name)\n    config_class = config.from_dict(config_dict)\n    config_obj = config_class()\n    config_obj.update(config_dict)\n    return config_obj\n\n", "CodeBleu_score": 44.706661472574375}]}
{"instance_id": "e0db68b63143", "generation_result": [{"code": "def list_files(basePath, validExts=None, contains=None)\n\t\n    for root, dirs, files in os.walk(basePath):\n        for file in files:\n            if contains is not None and contains.lower() not in file.lower():\n                continue\n            if validExts is not None:\n                ext = file[file.rfind('.'):].lower()\n                if ext not in validExts:\n                    continue\n            yield os.path.join(root, file)\n\n", "CodeBleu_score": 45.19688598019096}, {"code": "def list_files(basePath, validExts=None, contains=None)\n\t\n    for root, dirs, files in os.walk(basePath):\n        for file in files:\n            if contains is not None and contains not in file:\n                continue\n            ext = file[file.rfind('.')+1:].lower()\n            if (validExts is None or ext in validExts) and os.path.isfile(os.path.join(root, file)):\n                yield os.path.join(root, file)\n\n", "CodeBleu_score": 41.87331233233682}, {"code": "def list_files(basePath, validExts=None, contains=None)\n\t\n    for root, dirs, files in os.walk(basePath):\n        for filename in files:\n            if contains is not None and filename.find(contains) == -1:\n                continue\n            if validExts is not None:\n                if not any(filename.lower().endswith(ext) for ext in validExts):\n                    continue\n            yield os.path.join(root, filename)\n\n", "CodeBleu_score": 49.86145057159937}]}
{"instance_id": "b836fcaf0a27", "generation_result": [{"code": "def load_model(xtts_checkpoint, xtts_config, xtts_vocab)\n\t\n    clear_gpu_cache()\n    if xtts_checkpoint is None or xtts_config is None or xtts_vocab is None:\n        return \"Please provide the required paths to the model checkpoint, config, and vocabulary.\"\n    print(\"Loading configuration from\", xtts_config)\n    config = XttsConfig.load_json(xtts_config)\n    print(\"Initializing model from configuration\")\n    Xtts = XttsModel.init_from_config(config)\n    print(\"Loading model checkpoint from\", xtts_checkpoint)\n    Xtts.load_checkpoint(xtts_checkpoint)\n    print(\"Loading vocabulary from\", xtts_vocab)\n    Xtts.load_vocab(xtts_vocab)\n    if is_available():\n        print(\"Moving model to GPU\")\n        Xtts.cuda()\n    return \"Model loaded successfully\"\n\n", "CodeBleu_score": 48.56890721622755}, {"code": "def load_model(xtts_checkpoint, xtts_config, xtts_vocab)\n\t\n    clear_gpu_cache()\n    if xtts_checkpoint is None:\n        return 'XTTS model checkpoint path is required.'\n    if xtts_config is None:\n        return 'XTTS model config path is required.'\n    if xtts_vocab is None:\n        return 'XTTS model vocab path is required.'\n    try:\n        config = XttsConfig.load_json(xtts_config)\n        print('XTTS model config loaded.')\n    except:\n        return 'XTTS model config could not be loaded.'\n    try:\n        Xtts.init_from_config(config)\n        print('XTTS model initialized.')\n    except:\n        return 'XTTS model could not be initialized.'\n    try:\n        XTTS_MODEL.load_checkpoint(xtts_checkpoint)\n        print('XTTS model checkpoint loaded.')\n    except:\n        return 'XTTS model checkpoint could not be loaded.'\n    try:\n        XTTS_MODEL.load_vocabulary(xtts_vocab)\n        print('XTTS model vocab loaded.')\n    except:\n        return 'XTTS model vocab could not be loaded.'\n    try:\n        if is_available():\n            XTTS_MODEL.cuda()\n            print('XTTS model moved to GPU.')\n    except:\n        pass\n    return 'XTTS model loaded.'\n", "CodeBleu_score": 39.94629747644849}, {"code": "def load_model(xtts_checkpoint, xtts_config, xtts_vocab)\n\t\n    clear_gpu_cache()\n    if xtts_checkpoint is None or xtts_config is None or xtts_vocab is None:\n        return \"Please provide paths for the XTTS checkpoint, config, and vocab.\"\n    config = XttsConfig.load_json(xtts_config)\n    Xtts.init_from_config(config)\n    XTTS_MODEL.load_checkpoint(xtts_checkpoint, xtts_config, xtts_vocab)\n    if is_available():\n        XTTS_MODEL.cuda()\n    return \"XTTS model loaded.\"\n\n", "CodeBleu_score": 41.13582506414421}]}
{"instance_id": "c5d63aa0df26", "generation_result": [{"code": "def compute_embedding(self, x, num_frames=250, num_eval=10, return_mean=True, l2_norm=True)\n\t\n    x_len = x.shape[1]\n    if self.hop_length is not None:\n        num_frames = int(np.ceil(num_frames * self.hop_length / self.win_length))\n        num_frames = min(num_frames, x_len)\n    num_eval = min(num_eval, x_len)\n    offsets = np.linspace(0, x_len - num_frames, num_eval, endpoint=False)\n    frames_batch = []\n    for offset in offsets:\n        frames_batch.append(x[:, int(offset) : int(offset + num_frames)])\n    frames_batch = torch.cat(frames_batch, dim=0)\n    with torch.no_grad():\n        embeddings = self.inference(frames_batch)\n    if return_mean:\n        embeddings = torch.mean(embeddings, dim=1)\n    if l2_norm:\n        embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)\n    return embeddings\n\n", "CodeBleu_score": 45.0854329597431}, {"code": "def compute_embedding(self, x, num_frames=250, num_eval=10, return_mean=True, l2_norm=True)\n\t\n    if self.hop_size is not None:\n        num_frames = int(np.ceil(num_frames / self.hop_size))\n    if num_frames > x.shape[-1]:\n        num_frames = x.shape[-1]\n    eval_offsets = np.linspace(0, x.shape[-1] - num_frames, num_eval, dtype=int)\n    frames_batch = []\n    for offset in eval_offsets:\n        frames_batch.append(x[..., offset : offset + num_frames])\n    frames_batch = torch.cat(frames_batch, dim=0)\n    embeddings = self.inference(frames_batch)\n    if return_mean:\n        embeddings = torch.mean(embeddings, dim=1)\n    if l2_norm:\n        embeddings = torch.nn.functional.normalize(embeddings, dim=-1)\n    return embeddings\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 50.67495945637447}, {"code": "def compute_embedding(self, x, num_frames=250, num_eval=10, return_mean=True, l2_norm=True)\n\t\n    # Adjust the number of frames based on the hop length\n    if self.hop_length is not None:\n        num_frames = int(np.ceil(num_frames * self.hop_length))\n\n    # Ensure the number of frames does not exceed the input length\n    num_frames = min(num_frames, x.shape[1])\n\n    # Calculate offset positions for evaluation\n    offset_position = np.linspace(0, x.shape[1] - num_frames, num_eval, dtype=int)\n\n    # Extract frames at these offsets\n    frames_batch = []\n    for position in offset_position:\n        frames_batch.append(x[:, position : position + num_frames])\n    frames_batch = torch.cat(frames_batch, dim=0)\n\n    # Perform inference to obtain embeddings\n    embeddings = self.inference(frames_batch)\n\n    # If specified, compute the mean of the embeddings\n    if return_mean:\n        embeddings = torch.mean(embeddings, dim=0, keepdim=True)\n\n    # If specified, normalize the embeddings with L2 norm\n    if l2_norm:\n        embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)\n\n    return embeddings\n\n", "CodeBleu_score": 53.10961643830405}]}
{"instance_id": "06f5b0335b26", "generation_result": [{"code": "def create_layer(self, block, planes, blocks, stride=1)\n\t\n    downsample = None\n    if stride != 1 or self.inplanes != planes * block.expansion:\n        downsample = nn.Sequential(\n            nn.Conv2d(self.inplanes, planes * block.expansion,\n                      kernel_size=1, stride=stride, bias=False),\n            nn.BatchNorm2d(planes * block.expansion),\n        )\n\n    layers = []\n    layers.append(block(self.inplanes, planes, stride, downsample))\n    self.inplanes = planes * block.expansion\n    for i in range(1, blocks):\n        layers.append(block(self.inplanes, planes))\n\n    return nn.Sequential(*layers)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 85.18641475692186}, {"code": "def create_layer(self, block, planes, blocks, stride=1)\n\t\n    downsample = None\n    if stride != 1 or self.inplanes != planes * block.expansion:\n        downsample = nn.Sequential(\n            nn.Conv2d(self.inplanes, planes * block.expansion,\n                      kernel_size=1, stride=stride, bias=False),\n            nn.BatchNorm2d(planes * block.expansion),\n        )\n\n    layers = []\n    layers.append(block(self.inplanes, planes, stride, downsample))\n    self.inplanes = planes * block.expansion\n    for i in range(1, blocks):\n        layers.append(block(self.inplanes, planes))\n\n    return nn.Sequential(*layers)\n\n", "CodeBleu_score": 85.18641475692186}, {"code": "def create_layer(self, block, planes, blocks, stride=1)\n\t\n    downsample = None\n    if stride != 1 or self.inplanes != planes * block.expansion:\n        downsample = nn.Sequential(\n            nn.Conv2d(self.inplanes, planes * block.expansion,\n                      kernel_size=1, stride=stride, bias=False),\n            nn.BatchNorm2d(planes * block.expansion),\n        )\n\n    layers = []\n    layers.append(block(self.inplanes, planes, stride, downsample))\n    self.inplanes = planes * block.expansion\n    for i in range(1, blocks):\n        layers.append(block(self.inplanes, planes))\n\n    return nn.Sequential(*layers)\n\n", "CodeBleu_score": 85.18641475692186}]}
{"instance_id": "9ac60597b633", "generation_result": [{"code": "def plot_embeddings(embeddings, num_classes_in_batch)\n\t\n    num_classes = len(num_classes_in_batch)\n    if num_classes > 10:\n        num_classes_in_batch = num_classes_in_batch[:10]\n        embeddings = embeddings[:10]\n        num_classes = len(num_classes_in_batch)\n    num_utts_per_class = [sum(num_classes_in_batch) // num_classes] * num_classes\n    num_utts_per_class[:(sum(num_classes_in_batch) % num_classes)] = [x + 1 for x in num_utts_per_class[:(sum(num_classes_in_batch) % num_classes)]]\n    embeddings = np.repeat(embeddings, num_utts_per_class, axis=0)\n    reducer = umap.UMAP()\n    projected = reducer.fit_transform(embeddings)\n    fig = plt.figure(figsize=(10, 10))\n    ax = fig.add_subplot(1, 1, 1)\n    ax.scatter(projected[:, 0], projected[:, 1], s=0.1)\n    plt.title(\"UMAP projection\")\n    plt.tight_layout()\n    plt.savefig(\"umap.png\")\n    plt.gca().set_aspect(\"equal\", \"datalim\")\n    plt.show()\n    return fig\n\n", "CodeBleu_score": 48.565510611572385}, {"code": "def plot_embeddings(embeddings, num_classes_in_batch)\n\t\n    # Calculate the number of utterances per class\n    num_utterances_per_class = np.bincount(num_classes_in_batch)\n\n    # Limit the number of classes to 10 if necessary\n    if len(num_utterances_per_class) > 10:\n        num_utterances_per_class = num_utterances_per_class[:10]\n        embeddings = embeddings[:10]\n\n    # Create a UMAP projection\n    reducer = umap.UMAP(n_components=2).fit(embeddings)\n    projected = reducer.transform(embeddings)\n\n    # Plot the projection\n    fig, ax = plt.subplots()\n    for i in range(len(num_utterances_per_class)):\n        ax.scatter(\n            projected[np.arange(num_utterances_per_class[i]) * len(num_utterances_per_class) + i, 0],\n            projected[np.arange(num_utterances_per_class[i]) * len(num_utterances_per_class) + i, 1],\n            label=i,\n        )\n    ax.legend()\n    ax.set_title(\"UMAP projection\")\n    plt.tight_layout()\n    plt.savefig(\"umap.png\")\n    return fig\n\n", "CodeBleu_score": 37.52426689295854}, {"code": "def plot_embeddings(embeddings, num_classes_in_batch)\n\t\n    # Calculate the number of utterances per class\n    num_utterances_per_class = np.sum(num_classes_in_batch, axis=0)\n\n    # Limit the number of classes to 10 if necessary\n    if num_utterances_per_class.shape[0] > 10:\n        num_utterances_per_class = num_utterances_per_class[:10]\n\n    # Calculate the number of utterances per class\n    num_utterances_per_class = np.sum(num_classes_in_batch, axis=0)\n\n    # Limit the number of classes to 10 if necessary\n    if num_utterances_per_class.shape[0] > 10:\n        num_utterances_per_class = num_utterances_per_class[:10]\n\n    # Calculate the number of utterances per class\n    num_utterances_per_class = np.sum(num_classes_in_batch, axis=0)\n\n    # Limit the number of classes to 10 if necessary\n    if num_utterances_per_class.shape[0] > 10:\n        num_utterances_per_class = num_utterances_per_class[:10]\n\n    # Calculate the number of utterances per class\n    num_utterances_per_class = np.sum(num_classes_in_batch, axis=0)\n\n    # Limit the number of classes to 10 if necessary\n    if num_utterances_per_class.shape[0] > 10:\n        num_utterances_per_class = num_utterances_per_class[:10]\n\n    # Calculate the number of utterances per class\n    num_utterances_per_class = np.sum(num_classes_in_batch, axis=0)\n\n    # Limit the number of classes to 10 if necessary\n    if num_utterances_per_class.shape[0] > 10:\n        num_utterances_per_class = num_utterances_per_class[:10]\n\n    # Calculate the number of utterances per class", "CodeBleu_score": 12.531239078015924}]}
{"instance_id": "19c2b4af256d", "generation_result": [{"code": "def calc_cosine_sim(self, dvecs, centroids)\n\t\n\n    # Initialize cosine similarity matrix\n    cs_matrix = []\n\n    # Iterate over each speaker\n    for spk, utts in self.speaker_utt_dict.items():\n\n        # Initialize a list for cosine similarity values for each utterance\n        cs_row = []\n\n        # Iterate over each utterance\n        for utt in utts:\n\n            # Get d-vector\n            dvec = dvecs[utt]\n\n            # Get new centroids excluding current utterance\n            new_centroids = self.calc_new_centroids(utt, dvecs, centroids)\n\n            # Compute cosine similarity of d-vector with new centroids\n            cs_dvec_new_centroids = torch.mm(\n                dvec.unsqueeze(0), new_centroids.transpose(0, 1)\n            )\n\n            # Clamp similarity values\n            cs_dvec_new_centroids = torch.clamp(cs_dvec_new_centroids, min=0)\n\n            # Concatenate cosine similarity values\n            cs_row.append(cs_dvec_new_centroids)\n\n        # Stack cosine similarity values for each utterance\n        cs_row = torch.stack(cs_row)\n\n        # Append cosine similarity values for each utterance to the cosine similarity matrix\n        cs_matrix.append(cs_row)\n\n    # Stack cosine similarity values for each speaker to the cosine similarity matrix\n    cs_matrix = torch.stack(cs_matrix)\n\n    return cs_matrix\n", "CodeBleu_score": 40.43824133735751}, {"code": "def calc_cosine_sim(self, dvecs, centroids)\n\t\n    # Initialize a list to hold the cosine similarity values for each utterance\n    cs_row = []\n    # Iterate over each speaker\n    for spk in self.speakers:\n        # Iterate over each utterance\n        for utt in self.utterances[spk]:\n            # Calculate new centroids excluding the current utterance\n            new_centroids = self.calc_new_centroids(dvecs, centroids, spk, utt)\n            # Calculate the cosine similarity between the utterance and the new centroids\n            cos_sim_matrix = torch.mm(dvecs[spk][utt].unsqueeze(0), new_centroids.transpose(0, 1))\n            # Clamp the similarity values\n            cos_sim_matrix = torch.clamp(cos_sim_matrix, min=0.0, max=1.0)\n            # Concatenate the similarity values\n            cs_row.append(torch.cat(cos_sim_matrix))\n    # Stack the cosine similarity values to form the cosine similarity matrix\n    return torch.stack(cs_row)\n\n", "CodeBleu_score": 31.000043168824025}, {"code": "def calc_cosine_sim(self, dvecs, centroids)\n\t\n    cos_sim_matrix = []\n    for spk, utts in dvecs.items():\n        new_centroids = self.calc_new_centroids(spk, utts, centroids)\n        for utt in utts:\n            cs_row = []\n            for spk_c, centroid in new_centroids.items():\n                cs = torch.mm(utt.unsqueeze(0), centroid.transpose(0, 1))\n                cs = torch.clamp(cs, min=0.0)\n                cs_row.append(cs)\n            cs_row = torch.stack(cs_row)\n            cos_sim_matrix.append(cs_row)\n    return torch.stack(cos_sim_matrix)\n\n", "CodeBleu_score": 37.36869304429661}]}
{"instance_id": "c525b411a1e7", "generation_result": [{"code": " def embed_loss_softmax(self, dvecs, cos_sim_matrix)\n\t\n    losses = []\n    for i, dvec in enumerate(dvecs):\n        # print(i, dvec.shape)\n        # print(cos_sim_matrix[i].shape)\n        # print(cos_sim_matrix[i].shape)\n        # print(cos_sim_matrix[i].shape)\n        # print(cos_sim_matrix[i].shape)\n        # print(cos_sim_matrix[i].shape)\n        # print(cos_sim_matrix[i].shape)\n        # print(cos_sim_matrix[i].shape)\n        # print(cos_sim_matrix[i].shape)\n        # print(cos_sim_matrix[i].shape)\n        # print(cos_sim_matrix[i].shape)\n        # print(cos_sim_matrix[i].shape)\n        # print(cos_sim_matrix[i].shape)\n        # print(cos_sim_matrix[i].shape)\n        # print(cos_sim_matrix[i].shape)\n        # print(cos_sim_matrix[i].shape)\n        # print(cos_sim_matrix[i].shape)\n        # print(cos_sim_matrix[i].shape)\n        # print(cos_sim_matrix[i].shape)\n        # print(cos_sim_matrix[i].shape)\n        # print(cos_sim_matrix[i].shape)\n        # print(cos_sim_matrix[i].shape)\n        # print(cos_sim_matrix[i].shape)\n        # print(cos_sim_matrix[i].shape)\n        # print(cos_sim_matrix[i].shape)\n        # print(cos_sim_matrix[i].shape)\n        # print(cos_sim_matrix[i].shape)\n        # print(cos_sim_matrix[i].shape)\n        # print(cos_sim_matrix[i].shape)\n        # print(cos_sim_matrix[i].shape)\n        # print(cos_sim_matrix[i].shape)\n        # print(cos_sim_matrix[i].shape)\n        # print(cos_sim_matrix[i].shape)\n", "CodeBleu_score": 8.026970923781512}, {"code": " def embed_loss_softmax(self, dvecs, cos_sim_matrix)\n\t\n    loss_row_list = []\n    for dvec in dvecs:\n        cos_sim_row = cos_sim_matrix[dvec]\n        loss_row = F.log_softmax(cos_sim_row, dim=0)\n        loss_row_list.append(loss_row)\n    loss_stack = torch.stack(loss_row_list)\n    return loss_stack\n\n", "CodeBleu_score": 33.02580526761667}, {"code": " def embed_loss_softmax(self, dvecs, cos_sim_matrix)\n\t\n    losses = []\n    for i in range(dvecs.shape[0]):\n        cos_sim_matrix_i = cos_sim_matrix[i, :, :]\n        cos_sim_matrix_i_diag = cos_sim_matrix_i.diag()\n        cos_sim_matrix_i_diag_repeated = cos_sim_matrix_i_diag.repeat(cos_sim_matrix_i.shape[0], 1)\n        cos_sim_matrix_i_diag_repeated_t = cos_sim_matrix_i_diag_repeated.t()\n        cos_sim_matrix_i_log_softmax = F.log_softmax(cos_sim_matrix_i, dim=1)\n        loss_i = torch.sum(torch.mul(cos_sim_matrix_i_log_softmax, cos_sim_matrix_i_diag_repeated_t))\n        losses.append(loss_i)\n    return torch.stack(losses)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 29.79314194312444}]}
{"instance_id": "25ac3945da2e", "generation_result": [{"code": "def embed_loss_contrast(self, dvecs, cos_sim_matrix)\n\t\n    L_row = []\n    for i in range(dvecs.shape[0]):\n        cos_sim_matrix_i = cos_sim_matrix[i]\n        cos_sim_matrix_i[i] = 0\n        L_row.append(torch.sigmoid(cos_sim_matrix_i).max())\n    L = torch.stack(L_row)\n    return L\n\n", "CodeBleu_score": 22.995123242740913}, {"code": "def embed_loss_contrast(self, dvecs, cos_sim_matrix)\n\t\n    L_row = []\n    for i in range(dvecs.shape[0]):\n        cos_sim_matrix_i = cos_sim_matrix[i, :]\n        cos_sim_matrix_i[i] = -1\n        cos_sim_matrix_i = torch.sigmoid(cos_sim_matrix_i)\n        L_row.append(torch.max(cos_sim_matrix_i))\n    L_row = torch.stack(L_row)\n    return L_row\n\n", "CodeBleu_score": 27.316234276918806}, {"code": "def embed_loss_contrast(self, dvecs, cos_sim_matrix)\n\t\n    L = []\n    for i, dvec in enumerate(dvecs):\n        L_row = []\n        for j, dvec2 in enumerate(dvecs):\n            if i != j:\n                cos_sim_matrix_row = cos_sim_matrix[i, :]\n                cos_sim_matrix_row[j] = -1\n                L_row.append(torch.sigmoid(cos_sim_matrix_row))\n        L.append(torch.stack(L_row))\n    return torch.stack(L)\n\n", "CodeBleu_score": 31.38864716890361}]}
{"instance_id": "878d14ffabd2", "generation_result": [{"code": "def forward(self, x, _label=None)\n\t\n    if x.size(0) < 2:\n        raise ValueError(\"GE2E loss requires at least two embeddings per speaker.\")\n    num_speakers = x.size(0)\n    num_utts_per_speaker = x.size(1)\n    dvec_feats = x.size(2)\n    x = x.view(-1, dvec_feats)\n    centroids = torch.mean(x, dim=0, keepdim=True)\n    cosine_sim_matrix = self.calc_cosine_sim(x, centroids)\n    cosine_sim_matrix = cosine_sim_matrix.view(num_speakers, num_utts_per_speaker)\n    cosine_sim_matrix = torch.clamp(cosine_sim_matrix, min=1e-6)\n    exp_sim_matrix = torch.exp(cosine_sim_matrix)\n    mask = torch.eye(num_utts_per_speaker, dtype=exp_sim_matrix.dtype, device=exp_sim_matrix.device)\n    mask = mask.unsqueeze(0).repeat(num_speakers, 1, 1)\n    exp_sim_sum = exp_sim_matrix.sum(dim=1, keepdim=True)\n    exp_sim_sum = torch.clamp(exp_sim_sum, min=1e-6)\n    exp_sim_avg = exp_sim_matrix / exp_sim_sum\n    exp_sim_avg = exp_sim_avg * (1 - mask)\n    exp_sim_avg = exp_sim_avg.sum(dim=1)\n    exp_sim_avg = torch.clamp(exp_sim_avg, min=1e-6)\n    log_exp_sim_avg = torch.log(exp_sim_avg)\n    loss = -log_exp_sim_avg\n    loss = loss.mean()\n    return loss\n\n", "CodeBleu_score": 34.350588542455576}, {"code": "def forward(self, x, _label=None)\n\t\n    # x: (num_speakers, num_utts_per_speaker, dvec_feats)\n    # label: (num_speakers, num_utts_per_speaker)\n    # cos_sim: (num_speakers, num_utts_per_speaker, num_speakers)\n    # centroids: (num_speakers, dvec_feats)\n    # cos_sim: (num_speakers, num_utts_per_speaker, num_speakers)\n    # cos_sim: (num_speakers, num_utts_per_speaker, num_speakers)\n    # cos_sim: (num_speakers, num_utts_per_speaker, num_speakers)\n    # cos_sim: (num_speakers, num_utts_per_speaker, num_speakers)\n    # cos_sim: (num_speakers, num_utts_per_speaker, num_speakers)\n    # cos_sim: (num_speakers, num_utts_per_speaker, num_speakers)\n    # cos_sim: (num_speakers, num_utts_per_speaker, num_speakers)\n    # cos_sim: (num_speakers, num_utts_per_speaker, num_speakers)\n    # cos_sim: (num_speakers, num_utts_per_speaker, num_speakers)\n    # cos_sim: (num_speakers, num_utts_per_speaker, num_speakers)\n    # cos_sim: (num_speakers, num_utts_per_speaker, num_speakers)\n    # cos_sim: (num_speakers, num_utts_per_speaker, num_speakers)\n    # cos_sim: (num_speakers, num_utts_per_speaker, num_speakers)\n    # cos_sim: (num_speakers, num_utts_per_speaker, num_speakers)\n    # cos_sim: (num_speakers, num_utts_per_speaker, num_", "CodeBleu_score": 3.6585365853658542}, {"code": "def forward(self, x, _label=None)\n\t\n    assert x.ndim == 3, \"Input data must have shape (num_speakers, num_utts_per_speaker, dvec_feats)\"\n    assert x.shape[1] >= 2, \"Must have at least two utterances per speaker\"\n    # x: (num_speakers, num_utts_per_speaker, dvec_feats)\n    # x: (num_speakers, dvec_feats)\n    x = x.mean(dim=1)\n    # x: (num_speakers, dvec_feats)\n    # x: (num_speakers, num_speakers)\n    sim_mat = self.calc_cosine_sim(x.unsqueeze(1), x.unsqueeze(0))\n    # sim_mat: (num_speakers, num_speakers)\n    # sim_mat: (num_speakers, num_speakers)\n    sim_mat_scaled = self.scale_sim_mat(sim_mat)\n    # sim_mat_scaled: (num_speakers, num_speakers)\n    # sim_mat_scaled: (num_speakers, num_speakers)\n    loss = self.embed_loss(sim_mat_scaled, _label)\n    # loss: (num_speakers, num_speakers)\n    # loss: (num_speakers, num_speakers)\n    loss = loss.mean()\n    # loss: ()\n    return loss\n\n", "CodeBleu_score": 38.56163741056007}]}
{"instance_id": "86a364c72caa", "generation_result": [{"code": "def get_voices(extra_voice_dirs: List[str] = [])\n\t\n    voice_dirs = [os.path.join(os.path.dirname(os.path.abspath(__file__)), 'voices')] + extra_voice_dirs\n    voices = {}\n    for voice_dir in voice_dirs:\n        for root, dirs, files in os.walk(voice_dir):\n            if not dirs:\n                continue\n            for subdir in dirs:\n                voice_files = []\n                for ext in ['npz', 'wav', 'mp3']:\n                    voice_files += glob(os.path.join(root, subdir, '*.{}'.format(ext)))\n                if not voice_files:\n                    continue\n                voices[subdir] = voice_files\n    return voices\n\n", "CodeBleu_score": 29.474813934389633}, {"code": "def get_voices(extra_voice_dirs: List[str] = [])\n\t\n    voices = {}\n    for voice_dir in extra_voice_dirs:\n        for root, dirs, files in os.walk(voice_dir):\n            for file in files:\n                if file.endswith(\".npz\"):\n                    if root not in voices:\n                        voices[root] = [file]\n                    else:\n                        voices[root].append(file)\n    return voices\n\n", "CodeBleu_score": 23.563559281856122}, {"code": "def get_voices(extra_voice_dirs: List[str] = [])\n\t\n    voice_files = {}\n    for voice_dir in extra_voice_dirs:\n        for dirpath, dirnames, filenames in os.walk(voice_dir):\n            for filename in filenames:\n                if filename.endswith(\".npz\"):\n                    subdir = dirpath.replace(voice_dir, \"\").lstrip(\"/\").lstrip(\"\\\\\")\n                    if subdir not in voice_files:\n                        voice_files[subdir] = []\n                    voice_files[subdir].append(os.path.join(dirpath, filename))\n                elif filename.endswith(\".wav\") or filename.endswith(\".mp3\"):\n                    subdir = dirpath.replace(voice_dir, \"\").lstrip(\"/\").lstrip(\"\\\\\")\n                    if subdir not in voice_files:\n                        voice_files[subdir] = []\n                    voice_files[subdir].append(os.path.join(dirpath, filename))\n    return voice_files\n\n", "CodeBleu_score": 28.97841625363915}]}
{"instance_id": "02f20c8917e6", "generation_result": [{"code": "def load_voice(model, voice: str, extra_voice_dirs: List[str] = [])\n\t\n    if voice == \"random\":\n        return None, None, None, None\n    voice_paths = get_voices(model, voice, extra_voice_dirs)\n    if len(voice_paths) > 1:\n        raise ValueError(f\"Multiple voices found for {voice}: {voice_paths}\")\n    voice_path = voice_paths[0]\n    if not os.path.exists(voice_path):\n        raise FileNotFoundError(f\"Voice {voice} not found at {voice_path}\")\n    if voice_path.endswith(\".npz\"):\n        return load_npz(voice_path)\n    else:\n        return generate_voice(voice_path)\n\n", "CodeBleu_score": 40.38243707105684}, {"code": "def load_voice(model, voice: str, extra_voice_dirs: List[str] = [])\n\t\n    if voice == \"random\":\n        return None, None, None\n    voice_paths = get_voices(voice, model, extra_voice_dirs)\n    if len(voice_paths) > 1:\n        raise RuntimeError(\n            f\"Multiple voices found for {voice} ({model}).", "CodeBleu_score": 21.59938071827988}, {"code": "def load_voice(model, voice: str, extra_voice_dirs: List[str] = [])\n\t\n    voice_paths = get_voices(model, voice, extra_voice_dirs)\n    if len(voice_paths) == 0:\n        raise ValueError(f\"No voice found for model {model} and voice {voice}\")\n    elif len(voice_paths) > 1:\n        raise ValueError(f\"Multiple voices found for model {model} and voice {voice}\")\n    voice_path = voice_paths[0]\n    if voice_path.endswith(\".npz\"):\n        return load_npz(voice_path)\n    else:\n        return generate_voice(model, voice, extra_voice_dirs)\n\n", "CodeBleu_score": 30.626185565312202}]}
{"instance_id": "9458e981389f", "generation_result": [{"code": "def _relative_shift(self, pos_score: torch.Tensor) -> torch.Tensor\n\t\n    batch_size, num_heads, seq_len, _ = pos_score.size()\n    zero_pad = torch.zeros((batch_size, num_heads, seq_len, 1), device=pos_score.device, dtype=pos_score.dtype)\n    padded_pos_score = torch.cat([zero_pad, pos_score], dim=-1)\n    padded_pos_score = padded_pos_score.view(batch_size, num_heads, 2 * seq_len, -1)\n    padded_pos_score = padded_pos_score[:, :, seq_len - 1:, :]\n    return padded_pos_score\n\n", "CodeBleu_score": 41.34919573716924}, {"code": "def _relative_shift(self, pos_score: torch.Tensor) -> torch.Tensor\n\t\n    pos_score_dims = pos_score.size()\n    zeros_tensor = torch.zeros(\n        pos_score_dims[:-2] + (1, pos_score_dims[-1]),\n        dtype=pos_score.dtype,\n        device=pos_score.device,\n    )\n    padded_pos_score = torch.cat([zeros_tensor, pos_score], dim=-2)\n    padded_pos_score = padded_pos_score.view(\n        pos_score_dims[:-2] + (pos_score_dims[-2] + 1, pos_score_dims[-1])\n    )\n    padded_pos_score = padded_pos_score[..., 1:].view_as(pos_score)\n    return padded_pos_score\n", "CodeBleu_score": 28.975709281929262}, {"code": "def _relative_shift(self, pos_score: torch.Tensor) -> torch.Tensor\n\t\n    batch_size, num_heads, seq_length, _ = pos_score.size()\n    zero_tensor = torch.zeros(\n        (batch_size, num_heads, seq_length, 1), device=pos_score.device, dtype=pos_score.dtype\n    )\n    pos_score_padded = torch.cat([zero_tensor, pos_score], dim=-1)\n    pos_score_padded = pos_score_padded.view(\n        batch_size, num_heads, 2 * seq_length, -1\n    )\n    pos_score = pos_score_padded[:, :, seq_length - 1 :, :].view_as(pos_score)\n    return pos_score\n\n", "CodeBleu_score": 36.094932670258814}]}
{"instance_id": "c046d4f00e80", "generation_result": [{"code": "def run_padded_sequence(self, context, lens)\n\t\n    # Extract the context up to the specified length\n    context_embedded = context[:,:self.size]\n\n    # Pad the context to the maximum length\n    context_embedded = torch.nn.utils.rnn.pad_sequence(context_embedded, batch_first=True)\n\n    # Add a channel dimension\n    context_embedded = context_embedded.unsqueeze(1)\n\n    # Apply the convolutional layers\n    for i in range(self.n_layers):\n        context_embedded = self.conv[i](context_embedded)\n        context_embedded = F.relu(context_embedded)\n        context_embedded = self.dropout[i](context_embedded)\n\n    # Remove the channel dimension\n    context_embedded = context_embedded.squeeze(1)\n\n    # Pad the context to the maximum length\n    context_embedded = torch.nn.utils.rnn.pad_sequence(context_embedded, batch_first=True)\n\n    return context_embedded\n\n\n", "CodeBleu_score": 30.403879218607976}, {"code": "def run_padded_sequence(self, context, lens)\n\t\n    # Extract the context for each batch element\n    context_embedded = []\n    for i in range(context.size(0)):\n        context_embedded.append(context[i, :lens[i], :])\n\n    # Pad the contexts to create a consistent batch size\n    context_embedded = pad_sequence(context_embedded)\n\n    # Apply the convolutional layers to the contexts\n    for conv in self.convs:\n        context_embedded = F.relu(conv(context_embedded))\n        context_embedded = self.dropout(context_embedded)\n\n    # Return the padded sequence\n    return context_embedded\n\n", "CodeBleu_score": 30.989571605717547}, {"code": "def run_padded_sequence(self, context, lens)\n\t\n    # Extract the context up to the specified length\n    context_embedded = self.embedding(context)\n    context_embedded = self.dropout(context_embedded)\n    context_embedded = self.transpose(context_embedded)\n    context_embedded = self.relu(self.conv(context_embedded))\n    context_embedded = self.transpose(context_embedded)\n    context_embedded = self.dropout(context_embedded)\n    context_embedded = self.relu(self.conv(context_embedded))\n    context_embedded = self.transpose(context_embedded)\n    context_embedded = self.dropout(context_embedded)\n    context_embedded = self.relu(self.conv(context_embedded))\n    context_embedded = self.transpose(context_embedded)\n    context_embedded = self.dropout(context_embedded)\n    context_embedded = self.relu(self.conv(context_embedded))\n    context_embedded = self.transpose(context_embedded)\n    context_embedded = self.dropout(context_embedded)\n    context_embedded = self.relu(self.conv(context_embedded))\n    context_embedded = self.transpose(context_embedded)\n    context_embedded = self.dropout(context_embedded)\n    context_embedded = self.relu(self.conv(context_embedded))\n    context_embedded = self.transpose(context_embedded)\n    context_embedded = self.dropout(context_embedded)\n    context_embedded = self.relu(self.conv(context_embedded))\n    context_embedded = self.transpose(context_embedded)\n    context_embedded = self.dropout(context_embedded)\n    context_embedded = self.relu(self.conv(context_embedded))\n    context_embedded = self.transpose(context_embedded)\n    context_embedded = self.dropout(context_embedded)\n    context_embedded = self.relu(", "CodeBleu_score": 8.085465668918221}]}
{"instance_id": "db2644f3c681", "generation_result": [{"code": "def run_unsorted_inputs(self, fn, context, lens)\n\t\n    # Sort lengths and indices\n    lens_sorted, indices = lens.sort(dim=0, descending=True)\n    context_sorted = context.index_select(0, indices)\n    # Pack the context\n    packed_context = pack_padded_sequence(context_sorted, lens_sorted.tolist())\n    # Apply the function\n    packed_output = fn(packed_context)\n    # Unpack the output\n    output, _ = pad_packed_sequence(packed_output, batch_first=True)\n    # Map the output back to the original indices\n    output = output.index_select(0, indices)\n    return output\n", "CodeBleu_score": 12.546789090525118}, {"code": "def run_unsorted_inputs(self, fn, context, lens)\n\t\n    lens_sorted, indices = lens.sort(descending=True)\n    context_sorted = context[indices]\n    context_packed = pack_padded_sequence(context_sorted, lens_sorted, batch_first=True)\n    context_packed = fn(context_packed)\n    context_sorted = pad_packed_sequence(context_packed, batch_first=True)[0]\n    context_unsorted = context_sorted[torch.sort(indices)[1]]\n    return context_unsorted\n", "CodeBleu_score": 18.406425140483307}, {"code": "def run_unsorted_inputs(self, fn, context, lens)\n\t\n    lens_sorted, indices_sorted = lens.sort(descending=True)\n    indices_unsorted = indices_sorted.sort()[1]\n    context_sorted = fn(context[indices_sorted])\n    context_unsorted = context_sorted[indices_unsorted]\n    return context_unsorted\n\n", "CodeBleu_score": 7.360786479701388}]}
{"instance_id": "ced082bffc63", "generation_result": [{"code": "def remove_weight_norm(self)\n\t\n    self.kernel_predictor.remove_weight_norm()\n    self.convt_pre.remove_parameterizations()\n    for block in self.conv_blocks:\n        block.remove_parameterizations()\n\n", "CodeBleu_score": 36.153669731358164}, {"code": "def remove_weight_norm(self)\n\t\n    self.kernel_predictor.remove_weight_norm()\n    self.convt_pre.remove_parameterization()\n    for block in self.conv_blocks:\n        block.remove_weight_norm()\n\n", "CodeBleu_score": 34.60682868599729}, {"code": "def remove_weight_norm(self)\n\t\n    self.kernel_predictor.remove_weight_norm()\n    self.convt_pre.remove_parameterizations()\n    for i, block in enumerate(self.conv_blocks):\n        block.remove_parameterizations(i)\n\n", "CodeBleu_score": 40.45907890407094}]}
{"instance_id": "b5a39fd97061", "generation_result": [{"code": "def map_mx_to_torch_model(mx_name)\n\t\n    mx_name = mx_name.lower()\n    if mx_name.startswith('se_'):\n        mx_name = mx_name.replace('se_', 'se-')\n    elif mx_name.startswith('senet_'):\n        mx_name = mx_name.replace('senet_', 'se-')\n    elif mx_name.startswith('inceptionv3'):\n        mx_name = 'inception-' + mx_name[11:]\n    return 'gluon_' + mx_name\n\n", "CodeBleu_score": 46.11724324945434}, {"code": "def map_mx_to_torch_model(mx_name)\n\t\n    torch_name = mx_name.lower()\n    torch_name = torch_name.replace('se_', 'se_')\n    torch_name = torch_name.replace('senet_', 'se_')\n    torch_name = torch_name.replace('inceptionv3', 'inception_v3')\n    torch_name = 'gluon_' + torch_name\n    return torch_name\n\n", "CodeBleu_score": 49.42188852815397}, {"code": "def map_mx_to_torch_model(mx_name)\n\t\n    mx_name = mx_name.lower()\n    if mx_name.startswith('se_'):\n        mx_name = mx_name.replace('se_', 'se-')\n    elif mx_name.startswith('senet_'):\n        mx_name = mx_name.replace('senet_', 'se-')\n    elif mx_name.startswith('inceptionv3'):\n        mx_name = mx_name.replace('inceptionv3', 'inception-v3')\n    return 'gluon_' + mx_name\n\n", "CodeBleu_score": 46.74503741580066}]}
{"instance_id": "7eee5efdbf67", "generation_result": [{"code": "def _num_samples_per_worker(self)\n\t\n    num_samples_per_worker = self.repeats * self.num_samples // self.num_workers\n    if self.training:\n        num_samples_per_worker = math.ceil(num_samples_per_worker)\n    if self.batch_size:\n        num_samples_per_worker = (\n            num_samples_per_worker // self.batch_size * self.batch_size\n        )\n    return int(num_samples_per_worker)\n\n", "CodeBleu_score": 33.688748936212264}, {"code": "def _num_samples_per_worker(self)\n\t\n    num_samples_per_worker = math.ceil(self.repeats * self.total_samples / self.num_workers)\n    if self.batch_size is not None:\n        num_samples_per_worker = math.ceil(num_samples_per_worker / self.batch_size) * self.batch_size\n    return int(num_samples_per_worker)\n\n", "CodeBleu_score": 29.838498301198445}, {"code": "def _num_samples_per_worker(self)\n\t\n    num_workers = self.num_workers\n    if self.repeats > 1:\n        num_workers = self.repeats\n    num_samples = self.num_samples\n    if num_samples == 0:\n        num_samples = self.dataset.num_samples\n    num_samples_per_worker = math.ceil(num_samples / num_workers)\n    if self.batch_size > 0:\n        num_samples_per_worker = (\n            num_samples_per_worker // self.batch_size\n        ) * self.batch_size\n    return int(num_samples_per_worker)\n", "CodeBleu_score": 35.99567751148235}]}
{"instance_id": "a698d65ae190", "generation_result": [{"code": "def find_images_and_targets(\n        folder: str,\n        types: Optional[Union[List, Tuple, Set]] = None,\n        class_to_idx: Optional[Dict] = None,\n        leaf_name_only: bool = True,\n        sort: bool = True\n)\n\t\n    if types is None:\n        types = get_img_extensions()\n\n    labels = []\n    filenames = []\n    for root, subdirs, files in os.walk(folder):\n        for fname in files:\n            base, ext = splitext(fname)\n            if ext.lower() in types:\n                path = os.path.join(root, fname) if not leaf_name_only else fname\n                filenames.append(path)\n                labels.append(os.path.basename(root))\n\n    if class_to_idx is None:\n        if sort:\n            labels = sorted(set(labels))\n        class_to_idx = {label: idx for idx, label in enumerate(labels)}\n    else:\n        if sort:\n            labels = sorted(labels, key=natural_key)\n        class_to_idx = {label: class_to_idx[label] for label in labels}\n\n    if sort:\n        filenames = [x for _, x in sorted(zip(labels, filenames), key=lambda pair: natural_key(pair[0]))]\n        labels = sorted(labels, key=natural_key)\n\n    image_paths_and_class_indexes = list(zip(filenames, [class_to_idx[x] for x in labels]))\n    return image_paths_and_class_indexes, class_to_idx\n\n", "CodeBleu_score": 50.16625891419875}, {"code": "def find_images_and_targets(\n        folder: str,\n        types: Optional[Union[List, Tuple, Set]] = None,\n        class_to_idx: Optional[Dict] = None,\n        leaf_name_only: bool = True,\n        sort: bool = True\n)\n\t\n    if types is None:\n        types = get_img_extensions()\n    labels = []\n    filenames = []\n    for root, subdirs, files in os.walk(folder):\n        rel_path = os.path.relpath(root, folder) if (root != folder) else ''\n        label = basename(rel_path) if leaf_name_only else relpath(root, folder)\n        if class_to_idx is None:\n            labels.append(label)\n        else:\n            label = class_to_idx[label]\n        for f in files:\n            base, ext = splitext(f)\n            if ext.lower() in types:\n                filenames.append(os.path.join(root, f))\n                if class_to_idx is None:\n                    labels.append(label)\n                else:\n                    labels.append(label)\n    if class_to_idx is None:\n        labels.sort()\n        class_to_idx = {v: i for i, v in enumerate(labels)}\n        labels = [class_to_idx[l] for l in labels]\n    if sort:\n        filenames, labels = zip(*sorted(zip(filenames, labels), key=natural_key))\n    return filenames, labels, class_to_idx\n\n", "CodeBleu_score": 51.47775998005415}, {"code": "def find_images_and_targets(\n        folder: str,\n        types: Optional[Union[List, Tuple, Set]] = None,\n        class_to_idx: Optional[Dict] = None,\n        leaf_name_only: bool = True,\n        sort: bool = True\n)\n\t\n    if types is None:\n        types = get_img_extensions()\n    labels = []\n    filenames = []\n    for root, subdirs, files in os.walk(folder):\n        for fname in files:\n            base, ext = splitext(fname)\n            if ext.lower() in types:\n                path = join(root, fname)\n                if leaf_name_only:\n                    path = basename(path)\n                filenames.append(path)\n                if class_to_idx is None:\n                    labels.append(relpath(root, folder))\n                else:\n                    labels.append(class_to_idx[relpath(root, folder)])\n    if class_to_idx is None:\n        if sort:\n            labels, filenames = zip(*sorted(zip(labels, filenames), key=natural_key))\n        class_to_idx = {label: idx for idx, label in enumerate(labels)}\n    else:\n        if sort:\n            filenames, labels = zip(*sorted(zip(filenames, labels), key=lambda x: natural_key(x[1])))\n    return filenames, labels, class_to_idx\n\n", "CodeBleu_score": 44.81961897431035}]}
{"instance_id": "e7fe43bdecdc", "generation_result": [{"code": "def pytorch_worker_seed()\n\t\n    worker_info = get_worker_info()\n    if worker_info is None:\n        return None\n    seed = worker_info.seed\n    if seed is None:\n        return None\n    return seed\n", "CodeBleu_score": 48.60194870100847}, {"code": "def pytorch_worker_seed()\n\t\n    worker_info = get_worker_info()\n    if worker_info is not None:\n        return worker_info.seed\n    else:\n        return pytorch_worker_seed_fallback()\n\n", "CodeBleu_score": 56.566179676644964}, {"code": "def pytorch_worker_seed()\n\t\n    worker_info = get_worker_info()\n    if worker_info is not None:\n        return worker_info.seed % 2 ** 32\n    else:\n        return (get_rank() + 1) % 2 ** 32\n\n", "CodeBleu_score": 50.331052950818034}]}
{"instance_id": "0ffc7e7ded25", "generation_result": [{"code": "def solarize_add(img, add, thresh=128, **__)\n\t\n    lut = [i if i < thresh else i + add for i in range(256)]\n    if img.mode in (\"L\", \"RGB\"):\n        return img.point(lut)\n    else:\n        return img\n\n", "CodeBleu_score": 25.146084802622443}, {"code": "def solarize_add(img, add, thresh=128, **__)\n\t\n    lut = []\n    for i in range(256):\n        if i < thresh:\n            lut.append(min(255, i + add))\n        else:\n            lut.append(i)\n    if img.mode in (\"L\", \"RGB\"):\n        if img.mode == \"RGB\" and len(lut) == 256:\n            lut = lut + lut + lut\n        return img.point(lut)\n    else:\n        return img\n\n", "CodeBleu_score": 85.72627560455048}, {"code": "def solarize_add(img, add, thresh=128, **__)\n\t\n    lut = []\n    for i in range(256):\n        if i < thresh:\n            lut.append(min(255, i + add))\n        else:\n            lut.append(i)\n    if img.mode in (\"L\", \"RGB\"):\n        if img.mode == \"RGB\" and len(lut) == 256:\n            lut = lut + lut + lut\n        return img.point(lut)\n    else:\n        return img\n\n", "CodeBleu_score": 85.72627560455048}]}
{"instance_id": "5a0d29577a21", "generation_result": [{"code": "def _get_weighted_transforms(transforms: Dict)\n\t\n    transform_names = list(transforms.keys())\n    transform_probs = list(transforms.values())\n    transform_probs = [prob / sum(transform_probs) for prob in transform_probs]\n    return transform_names, transform_probs\n\n", "CodeBleu_score": 20.865611314023923}, {"code": "def _get_weighted_transforms(transforms: Dict)\n\t\n    transforms_names = list(transforms.keys())\n    transforms_probs = list(transforms.values())\n    transforms_probs = np.array(transforms_probs) / np.sum(transforms_probs)\n    return transforms_names, transforms_probs\n\n", "CodeBleu_score": 21.139889103628697}, {"code": "def _get_weighted_transforms(transforms: Dict)\n\t\n    transforms_names = []\n    transforms_probs = []\n    for transform, prob in transforms.items():\n        transforms_names.append(transform)\n        transforms_probs.append(prob)\n    transforms_probs = np.array(transforms_probs) / np.sum(transforms_probs)\n    return transforms_names, transforms_probs\n\n", "CodeBleu_score": 32.92652846234663}]}
{"instance_id": "84bc5658c135", "generation_result": [{"code": "def _apply_basic(self, img, mixing_weights, m)\n\t\n    mixed = np.zeros(img.shape, img.dtype)\n    for weight in mixing_weights:\n        mixed += weight * self._apply(img, m)\n    mixed = np.clip(mixed, 0, 255).astype(np.uint8)\n    mixed = Image.fromarray(mixed)\n    return Image.blend(img, mixed, m.rand())\n\n", "CodeBleu_score": 21.787673700910442}, {"code": "def _apply_basic(self, img, mixing_weights, m)\n\t\n    mixed = np.zeros(img.shape, dtype=np.float32)\n    for w in mixing_weights:\n        mixed += w * self.apply_sequence(img, m)\n    mixed = np.clip(mixed, 0, 255).astype(np.uint8)\n    return Image.blend(img, Image.fromarray(mixed), m)\n\n", "CodeBleu_score": 19.24426281100164}, {"code": "def _apply_basic(self, img, mixing_weights, m)\n\t\n    mixed = np.zeros(img.shape, dtype=np.float32)\n    for w in mixing_weights:\n        mixed += w * self.augment_and_mix(img, m)\n    mixed = np.clip(mixed, 0, 255).astype(np.uint8)\n    return Image.blend(img, Image.fromarray(mixed), m)\n\n", "CodeBleu_score": 19.30405001218445}]}
{"instance_id": "84b5b752ff04", "generation_result": [{"code": "def adapt_to_chs(x, n)\n\t\n    if not isinstance(x, (tuple, list)):\n        _logger.warning(f\"The input x is not a tuple or list. Repeating x {n} times to form a tuple.\")\n        return tuple(repeat(x, n))\n    elif len(x) != n:\n        _logger.warning(f\"The input x is a tuple or list, but its length ({len(x)}) does not match the required number of channels ({n}). Calculating the mean of x and repeating it {n} times.\")\n        return tuple(repeat(np.mean(x), n))\n    else:\n        assert len(x) == n, f\"The input x is a tuple or list, but its length ({len(x)}) does not match the required number of channels ({n}).\"\n        return x\n\n", "CodeBleu_score": 37.89958016893616}, {"code": "def adapt_to_chs(x, n)\n\t\n    if not isinstance(x, (tuple, list)):\n        _logger.warning(\"x is not a tuple or list, repeating it {} times\".format(n))\n        x = tuple(repeat(x, n))\n    elif len(x) != n:\n        _logger.warning(\"x is a tuple or list of length {}, but {} is required, taking the mean\".format(len(x), n))\n        x = tuple(repeat(np.mean(x), n))\n    else:\n        assert len(x) == n, \"x is a tuple or list of length {}, but {} is required\".format(len(x), n)\n    return x\n\n", "CodeBleu_score": 48.63997594013691}, {"code": "def adapt_to_chs(x, n)\n\t\n    if not isinstance(x, (tuple, list)):\n        _logger.warning(f\"x is not a tuple or list, repeating x {n} times.\")\n        return tuple(repeat(x, n))\n    if len(x) != n:\n        _logger.warning(f\"x is a tuple or list, but its length {len(x)} does not match the required number of channels {n}.\")\n        return tuple(repeat(np.mean(x), n))\n    return x\n\n", "CodeBleu_score": 36.72685247705492}]}
{"instance_id": "54754b6ba394", "generation_result": [{"code": "def _worker_init(worker_id, worker_seeding='all')\n\t\n    worker_info = get_worker_info()\n    if worker_info is None or worker_info.id != worker_id:\n        raise RuntimeError(\"Couldn't retrieve worker information.\")\n    if callable(worker_seeding):\n        worker_seeding(worker_id, worker_info)\n    elif isinstance(worker_seeding, str):\n        if worker_seeding == 'all':\n            seed = worker_info.seed % (2 ** 32)\n            random.seed(seed)\n            torch.manual_seed(seed)\n            np.random.seed(seed)\n        else:\n            raise ValueError(\"worker_seeding should be either a callable or 'all'\")\n\n", "CodeBleu_score": 40.55181276093832}, {"code": "def _worker_init(worker_id, worker_seeding='all')\n\t\n    worker_info = torch.utils.data.get_worker_info()\n    if worker_info is None:\n        raise RuntimeError(\"This function can only be called inside a worker process\")\n    if worker_info.id != worker_id:\n        raise RuntimeError(\"The worker id doesn't match the current worker\")\n    if isinstance(worker_seeding, str):\n        if worker_seeding == 'all':\n            seed = worker_info.seed % (2 ** 32)\n            random.seed(seed)\n            torch.manual_seed(seed)\n            np.random.seed(seed)\n        elif worker_seeding == 'numpy':\n            seed = worker_info.seed % (2 ** 32)\n            np.random.seed(seed)\n        else:\n            raise ValueError(f\"worker_seeding must be 'all' or 'numpy', but got {worker_seeding}\")\n    elif callable(worker_seeding):\n        worker_seeding(worker_info.seed)\n    else:\n        raise ValueError(f\"worker_seeding must be a callable or 'all' or 'numpy', but got {worker_seeding}\")\n", "CodeBleu_score": 40.014122492294646}, {"code": "def _worker_init(worker_id, worker_seeding='all')\n\t\n    worker_info = torch.utils.data.get_worker_info()\n    if worker_info is None:\n        return\n    if worker_info.id != worker_id:\n        raise RuntimeError('The workerId is not consistent with the workerInfo')\n    if worker_seeding == 'all':\n        random.seed(worker_info.seed)\n        torch.manual_seed(worker_info.seed)\n        np.random.seed(worker_info.seed)\n    elif isinstance(worker_seeding, int):\n        np.random.seed(worker_seeding)\n    elif isinstance(worker_seeding, callable):\n        worker_seeding(worker_info.seed)\n    else:\n        raise TypeError('worker_seeding must be \"all\", an integer, or a callable')\n\n", "CodeBleu_score": 39.02204250428332}]}
{"instance_id": "81dfdce940db", "generation_result": [{"code": "def _decode_and_center_crop(image_bytes, image_size, resize_method)\n\t\n    image_shape = extract_jpeg_shape(image_bytes)\n    image_height, image_width, _ = image_shape\n    padded_center_crop_size = _padded_center_crop(image_height, image_width, image_size, 32)\n    offset_height = ((image_height - padded_center_crop_size[0]) + 1) // 2\n    offset_width = ((image_width - padded_center_crop_size[1]) + 1) // 2\n    crop_window = tf.stack([offset_height, offset_width,\n                            padded_center_crop_size[0], padded_center_crop_size[1]])\n    image = tf.image.decode_and_crop_jpeg(image_bytes, crop_window, channels=3)\n    return _resize(image, image_size, resize_method)\n", "CodeBleu_score": 31.87699768927948}, {"code": "def _decode_and_center_crop(image_bytes, image_size, resize_method)\n\t\n\n    image_shape = extract_jpeg_shape(image_bytes)\n    image_height, image_width = image_shape\n\n    padded_center_crop_size = _padded_center_crop(image_height, image_width, image_size, _CROP_PADDING)\n\n    offset_height = ((image_height - padded_center_crop_size[0]) + 1) // 2\n    offset_width = ((image_width - padded_center_crop_size[1]) + 1) // 2\n    crop_window = tf.stack([offset_height, offset_width,\n                            padded_center_crop_size[0], padded_center_crop_size[1]])\n    crop_window = tf.cast(crop_window, tf.int32)\n\n    image = tf.image.decode_and_crop_jpeg(image_bytes, crop_window, channels=3)\n    image = tf.image.resize_images(image, [image_size, image_size],\n                                   method=resize_method)\n    return image\n\n", "CodeBleu_score": 40.74757544404487}, {"code": "def _decode_and_center_crop(image_bytes, image_size, resize_method)\n\t\n    image_shape = extract_jpeg_shape(image_bytes)\n    image_height = image_shape[0]\n    image_width = image_shape[1]\n    padded_center_crop_size = _padded_center_crop_size(image_height, image_width, image_size)\n    offset_height = ((image_height - padded_center_crop_size[0]) + 1) // 2\n    offset_width = ((image_width - padded_center_crop_size[1]) + 1) // 2\n    crop_window = tf.stack([offset_height, offset_width,\n                            padded_center_crop_size[0], padded_center_crop_size[1]])\n    image = tf.image.decode_and_crop_jpeg(image_bytes, crop_window, channels=3)\n    image = tf.image.resize_images(image, [image_size, image_size],\n                                   method=resize_method)\n    return image\n\n", "CodeBleu_score": 36.87827037922951}]}
{"instance_id": "cf16a6d37111", "generation_result": [{"code": "def preprocess_for_train(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    image = _decode_and_random_crop(image_bytes, image_size, interpolation)\n    image = _flip(image)\n    image = tf.reshape(image, [image_size, image_size, 3])\n    image = convert_image_dtype(image, use_bfloat16)\n    return image\n", "CodeBleu_score": 29.56513940037484}, {"code": "def preprocess_for_train(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    image = _decode_and_random_crop(image_bytes, image_size=image_size, interpolation=interpolation)\n    image = tf.image.random_flip_left_right(image)\n    image = tf.reshape(image, [image_size, image_size, 3])\n    image = convert_image_dtype(image, use_bfloat16)\n    return image\n\n", "CodeBleu_score": 29.866130310855805}, {"code": "def preprocess_for_train(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    image = decode_and_random_crop(image_bytes, image_size, interpolation)\n    image = tf.image.random_flip_left_right(image)\n    image = tf.reshape(image, [image_size, image_size, 3])\n    image = convert_image_dtype(image, use_bfloat16)\n    return image\n\n", "CodeBleu_score": 28.194885223488708}]}
{"instance_id": "78b2a3bd2142", "generation_result": [{"code": "def preprocess_for_eval(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    # Select the resize method based on the interpolation parameter.\n    if interpolation == 'bicubic':\n        resize_method = tf.image.ResizeMethod.BICUBIC\n    elif interpolation == 'bilinear':\n        resize_method = tf.image.ResizeMethod.BILINEAR\n    elif interpolation == 'nearest':\n        resize_method = tf.image.ResizeMethod.NEAREST_NEIGHBOR\n    else:\n        raise ValueError('Interpolation method not supported.')\n\n    # Decode and center the crop.\n    image = _decode_and_center_crop(image_bytes)\n\n    # Resize the image to the specified size.\n    image = tf.image.resize(image, [image_size, image_size], resize_method)\n\n    # Convert the image data type to either bfloat16 or float32 based on the use_bfloat16 flag.\n    image = convert_image_dtype(image, use_bfloat16)\n\n    return image\n\n", "CodeBleu_score": 40.72459883468337}, {"code": "def preprocess_for_eval(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    image_shape = tf.constant([image_size, image_size, 3], dtype=tf.int32)\n    if interpolation == 'bicubic':\n        interpolation_method = tf.image.ResizeMethod.BICUBIC\n    elif interpolation == 'bilinear':\n        interpolation_method = tf.image.ResizeMethod.BILINEAR\n    else:\n        raise ValueError('Invalid interpolation method: {}'.format(interpolation))\n    image = _decode_and_center_crop(image_bytes, image_size, image_size)\n    image = tf.image.resize(image, image_shape, method=interpolation_method)\n    image = tf.reshape(image, [image_size, image_size, 3])\n    return convert_image_dtype(image, use_bfloat16)\n\n", "CodeBleu_score": 54.462014915895615}, {"code": "def preprocess_for_eval(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    image = tf.image.decode_jpeg(image_bytes, channels=3, fancy_upscaling=False, dct_method='INTEGER_FAST')\n    image = _decode_and_center_crop(image, image_size)\n    image = tf.image.resize([image], [image_size, image_size], method=interpolation)[0]\n    image = convert_image_dtype(image, use_bfloat16)\n    return image\n\n", "CodeBleu_score": 26.249387518051115}]}
{"instance_id": "8bd4364a9a0e", "generation_result": [{"code": "def evaluate(self, model, eval_dataloader=None, output_file=\"merged\")\n\t\n    if eval_dataloader is None:\n        eval_dataloader = self.get_eval_dataloader()\n    output_file = os.path.join(self.args.output_dir, output_file)\n    self.predict(model, eval_dataloader, output_file)\n    eval_metrics = self.compute_metrics(output_file)\n    return eval_metrics\n\n", "CodeBleu_score": 54.47433648865394}, {"code": "def evaluate(self, model, eval_dataloader=None, output_file=\"merged\")\n\t\n    if eval_dataloader is None:\n        eval_dataloader = self.eval_dataloader\n\n    output_file = os.path.join(self.args.output_dir, output_file)\n    print(\"***** Running evaluation *****\")\n    print(\"  Num examples = %d\", len(eval_dataloader.dataset))\n    print(\"  Batch size = %d\", eval_dataloader.batch_size)\n\n    model.eval()\n    preds = None\n    out_label_ids = None\n    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n        batch = tuple(t.to(self.args.device) for t in batch)\n        with torch.no_grad():\n            inputs = {\"input_ids\": batch[0], \"attention_mask\": batch[1]}\n            if self.args.model_type != \"distilbert\":\n                inputs[\"token_type_ids\"] = (\n                    batch[2] if self.args.model_type in [\"bert\", \"xlnet\"] else None\n                )\n            outputs = model(**inputs)\n            tmp_eval_loss, logits = outputs[:2]\n            eval_loss += tmp_eval_loss.mean().item()\n        nb_eval_steps += 1\n        if preds is None:\n            preds = logits.detach().cpu().numpy()\n            out_label_ids = inputs[\"labels\"].detach().cpu().numpy()\n        else:\n            preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n            out_label_ids = np.append(\n                out_label_ids, inputs[\"labels\"].detach().cpu().numpy(), axis=0\n            )\n\n    eval_loss = eval_loss / nb_eval_steps\n    preds = np.argmax(preds, axis=1)\n    result = compute_metrics(preds, out_label_ids)\n    result[\"eval_loss\"] = eval_loss\n\n    output_eval_file = os.path.join(self.args.output_dir", "CodeBleu_score": 46.96427959572272}, {"code": "def evaluate(self, model, eval_dataloader=None, output_file=\"merged\")\n\t\n    if eval_dataloader is None:\n        eval_dataloader = self.eval_dataloader\n    if output_file is None:\n        output_file = \"merged\"\n    if not isinstance(eval_dataloader, list):\n        eval_dataloader = [eval_dataloader]\n    if not isinstance(output_file, list):\n        output_file = [output_file]\n    if len(eval_dataloader) != len(output_file):\n        raise ValueError(\"The number of eval_dataloaders must match the number of output_files\")\n\n    for dataloader, file in zip(eval_dataloader, output_file):\n        self.predict(model, dataloader, output_file=file)\n\n    metrics = self.compute_metrics(self.predictions)\n    return metrics\n", "CodeBleu_score": 46.60686684723098}]}
{"instance_id": "22d682c958c0", "generation_result": [{"code": " def print_computed_metrics(self, metrics)\n\t\n    print(f\"Rank1: {metrics['rank1']:.4f}\")\n    print(f\"Rank5: {metrics['rank5']:.4f}\")\n    print(f\"mIoU: {metrics['miou']:.4f}\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 18.579754787867735}, {"code": " def print_computed_metrics(self, metrics)\n\t\n    print(f'Rank1: {metrics[\"rank1\"]:.4f}')\n    print(f'Rank5: {metrics[\"rank5\"]:.4f}')\n    print(f'mIoU: {metrics[\"miou\"]:.4f}')\n\n", "CodeBleu_score": 19.72431960873751}, {"code": " def print_computed_metrics(self, metrics)\n\t\n    print(\"Rank1: {}\".format(metrics[\"rank1\"]))\n    print(\"Rank5: {}\".format(metrics[\"rank5\"]))\n    print(\"mIoU: {}\".format(metrics[\"miou\"]))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 23.55966511210283}]}
{"instance_id": "39c1a50b7e7d", "generation_result": [{"code": "def _eval_predictions(self, segments, data)\n\t\n    # Get the ground truth segments.\n    gt_segments = data['segments']\n    # Get the number of segments in the data.\n    num_segments = len(gt_segments)\n    # Initialize the rank and IoU lists.\n    self._ranks = []\n    self._iou = []\n    # Iterate over the ground truth segments.\n    for gt_segment in gt_segments:\n        # Get the ground truth segment's label.\n        gt_label = gt_segment['label']\n        # Get the ground truth segment's start and end times.\n        gt_start = gt_segment['start']\n        gt_end = gt_segment['end']\n        # Initialize the best IoU and rank to 0.\n        best_iou = 0.0\n        best_rank = 0\n        # Iterate over the predicted segments.\n        for pred_segment in segments:\n            # Get the predicted segment's label.\n            pred_label = pred_segment['label']\n            # Get the predicted segment's start and end times.\n            pred_start = pred_segment['start']\n            pred_end = pred_segment['end']\n            # Calculate the IoU between the ground truth and predicted segments.\n            iou = self._iou_between(gt_start, gt_end, pred_start, pred_end)\n            # Check if the IoU is better than the best IoU so far.\n            if iou > best_iou:\n                # Update the best IoU and rank.\n                best_iou = iou\n                best_rank = 1\n            # Check if the predicted segment's label matches the ground truth segment's label.\n            if pred_label == gt_label:\n                # Update the best rank.\n                best_rank = 1\n            # Update the rank list.\n            self._ranks.append(best_rank)\n            # Update the IoU list.\n            self._iou.append(best_iou)\n    # Calculate the mean IoU.\n    mean_iou = np.mean(self._iou)\n    # Calculate the rank1 and rank", "CodeBleu_score": 19.1048812618189}, {"code": "def _eval_predictions(self, segments, data)\n\t\n    rank1, rank5, mean_iou = 0, 0, 0\n    average_ranks, average_iou = [], []\n    for segment, segment_data in zip(segments, data):\n        iou = self._iou(segment, segment_data)\n        rank = self._rank(iou, segment_data)\n        if rank == 1:\n            rank1 += 1\n        if rank <= 5:\n            rank5 += 1\n        mean_iou += iou\n        average_ranks.append(rank)\n        average_iou.append(iou)\n    rank1 /= len(segments)\n    rank5 /= len(segments)\n    mean_iou /= len(segments)\n    return rank1, rank5, mean_iou, np.mean(average_ranks), np.mean(average_iou)\n\n", "CodeBleu_score": 31.097452982896073}, {"code": "def _eval_predictions(self, segments, data)\n\t\n    average_iou = []\n    average_rank = []\n    for segment, gt_data in zip(segments, data):\n        iou, rank = self._eval_segment(segment, gt_data)\n        average_iou.append(iou)\n        average_rank.append(rank)\n    return np.mean(average_rank[0]), np.mean(average_rank[1]), np.mean(average_iou)\n\n", "CodeBleu_score": 19.385394961986968}]}
{"instance_id": "8ccf84f8d034", "generation_result": [{"code": "def predict_loop(self, model, eval_dataloader, output_file=None)\n\t\n    scores = []\n    model.eval()\n    with torch.no_grad():\n        for batch in eval_dataloader:\n            batch = self.to_ctx(batch)\n            outputs = model(**batch)\n            outputs = self.to_ctx(outputs)\n            scores.append(self.update(outputs, batch))\n    scores = self.finalize(scores)\n    if output_file:\n        self.save_results(scores, output_file)\n    return scores\n", "CodeBleu_score": 39.91768137617345}, {"code": "def predict_loop(self, model, eval_dataloader, output_file=None)\n\t\n    scores = []\n    model.eval()\n    with torch.no_grad():\n        for batch in tqdm(eval_dataloader, desc=\"Predicting\"):\n            batch = self.to_ctx(batch)\n            outputs = model(**batch)\n            outputs = self.post_process_outputs(outputs)\n            outputs = self.update_outputs(outputs, batch)\n            scores.extend(outputs[\"scores\"].tolist())\n    if output_file:\n        with open(output_file, \"w\") as f:\n            json.dump(scores, f)\n    return scores\n\n", "CodeBleu_score": 37.08143488647529}, {"code": "def predict_loop(self, model, eval_dataloader, output_file=None)\n\t\n\n    # Initialize an empty list for scores\n    scores = []\n\n    # Set the model to evaluation mode\n    model.eval()\n\n    # Transfer the model to GPU 0\n    model.to(self.ctx[0])\n\n    # Iterate over the dataloader\n    for data in eval_dataloader:\n\n        # Transfer data to GPU 0\n        data = {k: v.to(self.ctx[0]) for k, v in data.items()}\n\n        # Make predictions without computing gradients\n        with torch.no_grad():\n            outputs = model(**data)\n\n        # Update the scores\n        scores.append(outputs)\n\n    # Finalize the scores\n    scores = self.finalize(scores)\n\n    # Optionally save the scores to an output file\n    if output_file is not None:\n        self.to_ctx(scores, self.ctx[0]).to_file(output_file)\n\n    # Return the scores\n    return scores\n\n", "CodeBleu_score": 49.9846179854568}]}
{"instance_id": "e68529ad727d", "generation_result": [{"code": "def _aggregate_scores(self, scores)\n\t\n    assert len(scores) == 2\n    self.full_scores = None\n    return np.concatenate(scores, axis=0)\n\n", "CodeBleu_score": 20.41325271403593}, {"code": "def _aggregate_scores(self, scores)\n\t\n    assert len(scores) == 2\n    self.full_scores = np.concatenate(scores, axis=0)\n    return np.matmul(self.full_scores[:, 0, :], self.full_scores[:, 1, :].T)\n", "CodeBleu_score": 33.14603884144518}, {"code": "def _aggregate_scores(self, scores)\n\t\n    assert len(scores) == 2, \"There should be exactly two sets of scores\"\n    self.full_scores = np.concatenate(scores)\n    return np.matmul(self.full_scores[:, 0, :], self.full_scores[:, 1, :].T)\n\n", "CodeBleu_score": 27.10197347381197}]}
{"instance_id": "e4e1861fc357", "generation_result": [{"code": "def finalize(self, Y_pred, Y_true, output_file=None)\n\t\n    Y_pred = torch.cat(Y_pred).detach().cpu().numpy()\n    Y_true = torch.cat(Y_true).detach().cpu().numpy()\n    if output_file:\n        pickle.dump([Y_pred, Y_true], open(output_file, \"wb\"))\n    if (Y_pred.shape[0] != Y_true.shape[0]):\n        raise ValueError(\"Prediction and true label vectors are of unequal lengths\")\n    if (Y_pred.shape[1] != Y_true.shape[1]):\n        raise ValueError(\"Prediction and true label vectors are of unequal dimensions\")\n    if (Y_pred.shape[1] != self.n_classes):\n        raise ValueError(\"Prediction and true label vectors are of unequal dimensions\")\n    print(\"Prediction shape: \", Y_pred.shape)\n    print(\"True label shape: \", Y_true.shape)\n    print(\"Number of samples: \", Y_pred.shape[0])\n    print(\"Number of classes: \", self.n_classes)\n    print(\"Number of correct predictions: \", np.sum(Y_pred == Y_true))\n    print(\"Number of incorrect predictions: \", np.sum(Y_pred != Y_true))\n    print(\"Accuracy: \", np.sum(Y_pred == Y_true) / Y_pred.shape[0])\n    print(\"Error: \", np.sum(Y_pred != Y_true) / Y_pred.shape[0])\n    if (np.sum(Y_pred != Y_true) > 0):\n        print(\"First 10 samples with prediction errors: \")\n        print(Y_pred[Y_pred != Y_true][:10])\n        print(Y_true[Y_pred != Y_true][:10])\n    return {\"Y_pred\": Y_pred, \"Y_true\": Y_true}\n\n", "CodeBleu_score": 30.52711153649276}, {"code": "def finalize(self, Y_pred, Y_true, output_file=None)\n\t\n    self.Y_pred = np.concatenate(Y_pred, axis=0)\n    self.Y_true = np.concatenate(Y_true, axis=0)\n    self.print_errors()\n    if output_file is not None:\n        with open(output_file, 'wb') as f:\n            pickle.dump({'Y_pred': self.Y_pred, 'Y_true': self.Y_true}, f)\n    return {'Y_pred': self.Y_pred, 'Y_true': self.Y_true}\n\n", "CodeBleu_score": 22.141564294643914}, {"code": "def finalize(self, Y_pred, Y_true, output_file=None)\n\t\n\n    # convert predictions and true labels to numpy arrays\n    Y_pred = np.array(Y_pred)\n    Y_true = np.array(Y_true)\n\n    # check for prediction errors\n    if not (Y_pred.shape[0] == Y_true.shape[0]):\n        raise ValueError(\n            \"Predicted and true labels have different lengths. Please check your model.\"\n        )\n    if not (Y_pred.shape[1] == Y_true.shape[1]):\n        raise ValueError(\n            \"Predicted and true labels have different numbers of classes. Please check your model.\"\n        )\n\n    # print samples of prediction errors\n    print(\"Prediction errors:\")\n    for i in range(Y_pred.shape[0]):\n        if not np.array_equal(Y_pred[i], Y_true[i]):\n            print(Y_pred[i], Y_true[i])\n\n    # save predictions and true labels as pickle file\n    if output_file is not None:\n        with open(output_file, \"wb\") as f:\n            pickle.dump([Y_pred, Y_true], f)\n\n    # return dictionary containing predicted and true labels\n    return {\"Y_pred\": Y_pred, \"Y_true\": Y_true}\n", "CodeBleu_score": 37.311047911156905}]}
{"instance_id": "7df9ab9a4920", "generation_result": [{"code": "def reduce_metrics(logging_outputs) -> None\n\t\n    \"\"\"\n    Args:\n        logging_outputs (List[Dict[str, Any]]): A list of dictionaries with the following keys:\n            * `loss` (float): The loss value.\n            * `ntokens` (int): The number of tokens in the batch.\n            * `nsentences` (int): The number of sentences in the batch.\n            * `sample_size` (int): The sample size.\n    \"\"\"\n    \"\"\"\n    \"\"\"\n    \"\"\"\n    Args:\n        logging_outputs (List[Dict[str, Tensor]]): A list of dictionaries with the following keys:\n            * `loss` (Tensor): The loss value.\n            * `ntokens` (int): The number of tokens in the batch.\n            * `nsentences` (int): The number of sentences in the batch.\n            * `sample_size` (int): The sample size.\n    \"\"\"\n    \"\"\"\n    \"\"\"\n    \"\"\"\n    Args:\n        logging_outputs (List[Dict[str, Tensor]]): A list of dictionaries with the following keys:\n            * `loss` (Tensor): The loss value.\n            * `ntokens` (int): The number of tokens in the batch.\n            * `nsentences` (int): The number of sentences in the batch.\n            * `sample_size` (int): The sample size.\n    \"\"\"\n    \"\"\"\n    \"\"\"\n    \"\"\"\n    Args:\n        logging_outputs (List[Dict[str, Tensor]]): A list of dictionaries with the following keys:\n            * `loss` (Tensor): The loss value.\n            * `ntokens` (int): The number of tokens in the batch.\n            * `nsentences` (int): The number of sentences in the batch.\n            * `sample_size` (int): The sample size.\n    \"\"\"\n    \"\"\"\n    \"\"\"\n    \"\"\"\n    Args:\n        logging_outputs (List[Dict[str, Tensor]]): A list of dictionaries with the following keys:\n            * `loss` (Tensor): The loss value.\n            * `ntokens` (int): The number of tokens in the batch.\n           ", "CodeBleu_score": 5.5468115053304174}, {"code": "def reduce_metrics(logging_outputs) -> None\n\t\n    \"\"\"\n    Args:\n        logging_outputs: List[Dict[str, Any]]: The list of logging outputs from the log method.\n    \"\"\"\n    \"\"\"\n    \"\"\"\n    \"\"\"\n    Args:\n        logging_outputs: List[Dict[str, Any]]: The list of logging outputs from the log method.\n    \"\"\"\n    \"\"\"\n    \"\"\"\n    \"\"\"\n    Args:\n        logging_outputs: List[Dict[str, Any]]: The list of logging outputs from the log method.\n    \"\"\"\n    \"\"\"\n    \"\"\"\n    \"\"\"\n    Args:\n        logging_outputs: List[Dict[str, Any]]: The list of logging outputs from the log method.\n    \"\"\"\n    \"\"\"\n    \"\"\"\n    \"\"\"\n    Args:\n        logging_outputs: List[Dict[str, Any]]: The list of logging outputs from the log method.\n    \"\"\"\n    \"\"\"\n    \"\"\"\n    \"\"\"\n    Args:\n        logging_outputs: List[Dict[str, Any]]: The list of logging outputs from the log method.\n    \"\"\"\n    \"\"\"\n    \"\"\"\n    \"\"\"\n    Args:\n        logging_outputs: List[Dict[str, Any]]: The list of logging outputs from the log method.\n    \"\"\"\n    \"\"\"\n    \"\"\"\n    \"\"\"\n    Args:\n        logging_outputs: List[Dict[str, Any]]: The list of logging outputs from the log method.\n    \"\"\"\n    \"\"\"\n    \"\"\"\n    \"\"\"\n    Args:\n        logging_outputs: List[Dict[str, Any]]: The list of logging outputs from the log method.\n    \"\"\"\n    \"\"\"\n    \"\"\"\n    \"\"\"\n    Args:\n        logging_outputs: List[Dict[str, Any]]: The list of logging outputs from the log method.\n    \"\"\"\n    \"\"\"\n    \"\"\"\n    \"\"\"\n    Args:\n        logging_outputs: List[Dict[str, Any]]: The list of logging outputs from the log method.\n    \"\"\"\n    \"\"\"\n    \"\"\"\n    \"\"\"\n    Args:\n        logging_outputs: List[Dict[str, Any]]: The list of logging outputs from the log method.\n    \"\"\"\n   ", "CodeBleu_score": 4.70596787321457}, {"code": "def reduce_metrics(logging_outputs) -> None\n\t\n    \"\"\"Aggregate logging outputs from data parallel training.\"\"\"\n    loss_sum = sum(log.get(\"loss\", 0) for log in logging_outputs)\n    ntokens = sum(log.get(\"ntokens\", 0) for log in logging_outputs)\n    sample_size = sum(log.get(\"sample_size\", 0) for log in logging_outputs)\n    metrics.log_scalar(\n        \"loss\", loss_sum / sample_size / math.log(2), sample_size, round=3\n    )\n    metrics.log_scalar(\"ntokens\", ntokens)\n    metrics.log_derived(\n        \"ppl\", lambda meters: utils.get_perplexity(meters[\"loss\"].avg)\n    )\n\n", "CodeBleu_score": 46.75996124146944}]}
{"instance_id": "7f9c326a96f7", "generation_result": [{"code": "def _mm_on_the_fly(\n        self,\n        cmasks,\n        vmasks,\n        attention_mask\n    )\n\t\n    if attention_mask is None:\n        attention_mask = torch.zeros(\n            cmasks.size(0),\n            cmasks.size(1) + vmasks.size(1),\n            cmasks.size(1) + vmasks.size(1),\n            dtype=torch.long,\n            device=cmasks.device\n        )\n        attention_mask[:, :cmasks.size(1), :cmasks.size(1)] = cmasks\n        attention_mask[:, cmasks.size(1):, cmasks.size(1):] = vmasks\n    token_type_ids = torch.cat(\n        [\n            torch.zeros(\n                cmasks.size(0),\n                cmasks.size(1),\n                dtype=torch.long,\n                device=cmasks.device\n            ),\n            torch.ones(\n                cmasks.size(0),\n                vmasks.size(1),\n                dtype=torch.long,\n                device=cmasks.device\n            )\n        ],\n        dim=1\n    )\n    return attention_mask, token_type_ids\n\n", "CodeBleu_score": 49.74611712197986}, {"code": "def _mm_on_the_fly(\n        self,\n        cmasks,\n        vmasks,\n        attention_mask\n    )\n\t\n    if attention_mask is None:\n        attention_mask = torch.zeros(\n            cmasks.size(0),\n            cmasks.size(1)+vmasks.size(1),\n            cmasks.size(1)+vmasks.size(1),\n            dtype=torch.long,\n            device=cmasks.device\n        )\n        attention_mask[:, :cmasks.size(1), :cmasks.size(1)] = cmasks\n        attention_mask[:, :cmasks.size(1), cmasks.size(1):] = vmasks\n        attention_mask[:, cmasks.size(1):, :cmasks.size(1)] = vmasks\n        attention_mask[:, cmasks.size(1):, cmasks.size(1):] = cmasks\n    token_type_ids = torch.cat(\n        (torch.zeros(cmasks.size(0), cmasks.size(1), dtype=torch.long, device=cmasks.device),\n        torch.ones(cmasks.size(0), vmasks.size(1), dtype=torch.long, device=cmasks.device)),\n        dim=1\n    )\n    return attention_mask, token_type_ids\n\n", "CodeBleu_score": 47.8503586049678}, {"code": "def _mm_on_the_fly(\n        self,\n        cmasks,\n        vmasks,\n        attention_mask\n    )\n\t\n    if attention_mask is None:\n        attention_mask = torch.zeros(\n            cmasks.size(0),\n            cmasks.size(1) + vmasks.size(1),\n            cmasks.size(1) + vmasks.size(1),\n            dtype=torch.long,\n            device=cmasks.device,\n        )\n        attention_mask[\n            torch.arange(0, cmasks.size(0), dtype=torch.long, device=cmasks.device),\n            torch.arange(0, cmasks.size(1), dtype=torch.long, device=cmasks.device),\n            torch.arange(0, cmasks.size(1), dtype=torch.long, device=cmasks.device)\n        ] = cmasks\n        attention_mask[\n            torch.arange(0, cmasks.size(0), dtype=torch.long, device=cmasks.device),\n            torch.arange(cmasks.size(1), cmasks.size(1) + vmasks.size(1), dtype=torch.long, device=cmasks.device),\n            torch.arange(cmasks.size(1), cmasks.size(1) + vmasks.size(1), dtype=torch.long, device=cmasks.device)\n        ] = vmasks\n    token_type_ids = torch.cat(\n        [\n            torch.zeros(cmasks.size(0), cmasks.size(1), dtype=torch.long, device=cmasks.device),\n            torch.ones(cmasks.size(0), vmasks.size(1), dtype=torch.long, device=cmasks.device)\n        ],\n        dim=1\n    )\n    return attention_mask, token_type_ids\n\n", "CodeBleu_score": 44.07869042372665}]}
{"instance_id": "fa99d2a148a5", "generation_result": [{"code": "def prepare_inputs_for_generation(\n        self,\n        input_ids,\n        input_video_embeds,\n        attention_mask=None,\n        token_type_ids=None,\n        **model_kwargs\n    )\n\t\n        # print(\"input_ids\", input_ids)\n        # print(\"input_video_embeds\", input_video_embeds)\n        # print(\"attention_mask\", attention_mask)\n        # print(\"token_type_ids\", token_type_ids)\n        # print(\"model_kwargs\", model_kwargs)\n        # print(\"input_ids.size\", input_ids.size)\n        # print(\"input_video_embeds.size\", input_video_embeds.size)\n        # print(\"attention_mask.size\", attention_mask.size)\n        # print(\"token_type_ids.size\", token_type_ids.size)\n        # print(\"input_ids\", input_ids)\n        # print(\"input_video_embeds\", input_video_embeds)\n        # print(\"attention_mask\", attention_mask)\n        # print(\"token_type_ids\", token_type_ids)\n        # print(\"model_kwargs\", model_kwargs)\n        # print(\"input_ids.size\", input_ids.size)\n        # print(\"input_video_embeds.size\", input_video_embeds.size)\n        # print(\"attention_mask.size\", attention_mask.size)\n        # print(\"token_type_ids.size\", token_type_ids.size)\n        # print(\"input_ids\", input_ids)\n        # print(\"input_video_embeds\", input_video_embeds)\n        # print(\"attention_mask\", attention_mask)\n        # print(\"token_type_ids\", token_type_ids)\n        # print(\"model_kwargs\", model_kwargs)\n        # print(\"input_ids.size\", input_ids.size)\n        # print(\"input_video_embeds.size\", input_video_embeds.size)\n        # print(\"attention_mask.size\", attention_mask.size)\n        # print(\"token_type_ids.size\", token_type_ids.size)\n        # print(\"input_ids\", input_ids)\n        # print(\"input_video_embeds\", input_video_embeds)\n        # print(\"attention_mask\", attention_mask)\n", "CodeBleu_score": 5.36986301369863}, {"code": "def prepare_inputs_for_generation(\n        self,\n        input_ids,\n        input_video_embeds,\n        attention_mask=None,\n        token_type_ids=None,\n        **model_kwargs\n    )\n\t\n\n        if input_ids is not None and input_video_embeds is not None:\n            input_ids = torch.cat([input_ids, input_video_embeds], dim=1)\n            attention_mask = torch.cat([attention_mask, torch.ones_like(attention_mask[:, -input_video_embeds.size(1):])], dim=1)\n            token_type_ids = torch.cat([token_type_ids, torch.ones_like(token_type_ids[:, -input_video_embeds.size(1):])], dim=1)\n        elif input_ids is not None:\n            input_ids = input_ids\n            attention_mask = attention_mask\n            token_type_ids = token_type_ids\n        elif input_video_embeds is not None:\n            input_ids = input_video_embeds\n            attention_mask = torch.ones_like(attention_mask[:, -input_video_embeds.size(1):])\n            token_type_ids = torch.ones_like(token_type_ids[:, -input_video_embeds.size(1):])\n        else:\n            raise ValueError(\"You have to specify either input_ids or input_video_embeds\")\n\n        return {\n            \"input_ids\": input_ids,\n            \"attention_mask\": attention_mask,\n            \"token_type_ids\": token_type_ids,\n        }\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 41.54523504461049}, {"code": "def prepare_inputs_for_generation(\n        self,\n        input_ids,\n        input_video_embeds,\n        attention_mask=None,\n        token_type_ids=None,\n        **model_kwargs\n    )\n\t\n    if input_video_embeds is not None:\n        # Modify the sequence length to match the combined length of input_ids and input_video_embeds\n        input_ids = self._modify_sequence_length_for_generation(input_ids, input_video_embeds)\n        # Modify the attention_mask to fit the new sequence length\n        attention_mask = self._modify_attention_mask_for_generation(input_ids, input_video_embeds, attention_mask)\n        # Modify the token_type_ids to fit the new sequence length\n        token_type_ids = self._modify_token_type_ids_for_generation(input_ids, input_video_embeds, token_type_ids)\n\n        # Modify the sequence length to match the combined length of input_ids and input_video_embeds\n        input_video_embeds = self._modify_sequence_length_for_generation(input_video_embeds, input_ids)\n        # Modify the attention_mask to fit the new sequence length\n        attention_mask = self._modify_attention_mask_for_generation(input_video_embeds, input_ids, attention_mask)\n        # Modify the token_type_ids to fit the new sequence length\n        token_type_ids = self._modify_token_type_ids_for_generation(input_video_embeds, input_ids, token_type_ids)\n\n    return {\n        \"input_ids\": input_ids,\n        \"input_video_embeds\": input_video_embeds,\n        \"attention_mask\": attention_mask,\n        \"token_type_ids\": token_type_ids,\n    }\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 38.04681549407476}]}
{"instance_id": "b1b0d25fe620", "generation_result": [{"code": "def _init_pipeline(self, cfg: ConfigType) -> Compose\n\t\n    if 'meta_keys' in cfg:\n        cfg['meta_keys'] = [key for key in cfg['meta_keys'] if key != 'img_id']\n    transforms = cfg['transforms']\n    transforms_names = [transform['type'] for transform in transforms]\n    try:\n        transforms_idx = transforms_names.index('LoadImageFromFile')\n    except ValueError:\n        raise ValueError(\n            'Could not find the LoadImageFromFile transform in the pipeline')\n    transforms[transforms_idx]['type'] = 'mmdet.InferencerLoader'\n    return Compose(transforms)\n\n", "CodeBleu_score": 35.36455338289252}, {"code": "def _init_pipeline(self, cfg: ConfigType) -> Compose\n\t\n    pipeline = cfg['pipeline']\n    # remove 'img_id' from meta_keys if present\n    if 'meta_keys' in pipeline[0] and 'img_id' in pipeline[0]['meta_keys']:\n        pipeline[0]['meta_keys'].remove('img_id')\n    # find the index of the 'LoadImageFromFile' transform\n    transform_idx = self._get_transform_idx(pipeline)\n    # update the transform type\n    pipeline[transform_idx]['type'] = 'mmdet.InferencerLoader'\n    # return the modified pipeline\n    return Compose(pipeline)\n\n", "CodeBleu_score": 23.876259462265036}, {"code": "def _init_pipeline(self, cfg: ConfigType) -> Compose\n\t\n    # remove the 'img_id' key from meta_keys if present\n    if 'meta_keys' in cfg and 'img_id' in cfg['meta_keys']:\n        cfg['meta_keys'].remove('img_id')\n    # find the index of the 'LoadImageFromFile' transform\n    transform_idx = self._get_transform_idx(cfg)\n    # update the transform type to 'mmdet.InferencerLoader'\n    cfg['transforms'][transform_idx]['type'] = 'mmdet.InferencerLoader'\n    # return the modified pipeline configuration wrapped in a Compose object\n    return Compose(cfg['transforms'])\n\n", "CodeBleu_score": 19.212194031709256}]}
{"instance_id": "7967962856eb", "generation_result": [{"code": "def _inputs_to_list(self, inputs: InputsType) -> list\n\t\n    if isinstance(inputs, str):\n        if isdir(inputs):\n            inputs = list_dir_or_file(inputs)\n        else:\n            inputs = [inputs]\n    elif not isinstance(inputs, (list, tuple)):\n        inputs = [inputs]\n    return inputs\n\n", "CodeBleu_score": 37.27664845802001}, {"code": "def _inputs_to_list(self, inputs: InputsType) -> list\n\t\n    if isinstance(inputs, str):\n        if isdir(inputs):\n            inputs = list_dir_or_file(inputs)\n        else:\n            inputs = [inputs]\n    elif not isinstance(inputs, (list, tuple)):\n        inputs = [inputs]\n    return inputs\n\n", "CodeBleu_score": 37.27664845802001}, {"code": "def _inputs_to_list(self, inputs: InputsType) -> list\n\t\n    if isinstance(inputs, str):\n        if isdir(inputs):\n            inputs = list_dir_or_file(inputs)\n        else:\n            inputs = [inputs]\n    elif isinstance(inputs, (list, tuple)):\n        pass\n    else:\n        inputs = [inputs]\n    return inputs\n\n", "CodeBleu_score": 35.422667682536044}]}
{"instance_id": "74f727f06fa0", "generation_result": [{"code": "def _get_chunk_data(self, inputs: Iterable, chunk_size: int)\n\t\n    chunk = []\n    for input in inputs:\n        if isinstance(input, dict) and (\"img\" in input or \"img_path\" in input):\n            input = self.pipeline(copy.deepcopy(input))\n        chunk.append(input)\n        if len(chunk) == chunk_size:\n            yield chunk\n            chunk = []\n    if chunk:\n        yield chunk\n\n", "CodeBleu_score": 26.0379181541554}, {"code": "def _get_chunk_data(self, inputs: Iterable, chunk_size: int)\n\t\n    chunk_data = []\n    for input in inputs:\n        if isinstance(input, dict):\n            if \"img\" in input:\n                chunk_data.append(self.pipeline(input))\n            elif \"img_path\" in input:\n                chunk_data.append(self.pipeline(input))\n            else:\n                chunk_data.append(input)\n        else:\n            chunk_data.append(input)\n        if len(chunk_data) == chunk_size:\n            yield chunk_data\n            chunk_data = []\n    if len(chunk_data) > 0:\n        yield chunk_data\n\n", "CodeBleu_score": 33.097156058547924}, {"code": "def _get_chunk_data(self, inputs: Iterable, chunk_size: int)\n\t\n    chunk_data = []\n    for input in inputs:\n        if isinstance(input, dict) and 'img' in input:\n            chunk_data.append(self.pipeline(input))\n        elif isinstance(input, dict) and 'img_path' in input:\n            chunk_data.append(self.pipeline(input))\n        else:\n            chunk_data.append(input)\n        if len(chunk_data) == chunk_size:\n            yield chunk_data\n            chunk_data = []\n    if len(chunk_data) > 0:\n        yield chunk_data\n\n", "CodeBleu_score": 30.908992933528907}]}
{"instance_id": "a43ab51afa59", "generation_result": [{"code": "def postprocess(\n        self,\n        preds: PredType,\n        visualization: Optional[List[np.ndarray]] = None,\n        return_datasamples: bool = False,\n        print_result: bool = False,\n        no_save_pred: bool = False,\n        pred_out_dir: str = '',\n        **kwargs,\n    ) -> Dict\n\t\n\n    if not no_save_pred:\n        if pred_out_dir:\n            self.init_output_dir(pred_out_dir)\n        else:\n            self.init_output_dir()\n\n    results = []\n    if return_datasamples:\n        warnings.warn(\"Saving datasamples is not supported. Returning predictions.\")\n\n    if isinstance(preds, list):\n        for pred in preds:\n            results.append(self.pred2dict(pred))\n    else:\n        results.append(self.pred2dict(preds))\n\n    result = {\n        'preds': results,\n        'visualization': visualization,\n    }\n\n    if print_result:\n        print(result)\n\n    return result\n", "CodeBleu_score": 48.83048510855547}, {"code": "def postprocess(\n        self,\n        preds: PredType,\n        visualization: Optional[List[np.ndarray]] = None,\n        return_datasamples: bool = False,\n        print_result: bool = False,\n        no_save_pred: bool = False,\n        pred_out_dir: str = '',\n        **kwargs,\n    ) -> Dict\n\t\n\n    if not no_save_pred:\n        if not os.path.exists(pred_out_dir):\n            os.makedirs(pred_out_dir)\n\n    if return_datasamples and pred_out_dir:\n        warnings.warn(\"Saving datasamples is not supported.\")\n\n    results = []\n    for pred in preds:\n        if return_datasamples:\n            results.append(self.pred2dict(pred))\n        else:\n            results.append(self.pred2dict(pred, no_save_pred))\n\n    result_dict = {\n        \"results\": results,\n        \"visualization\": visualization,\n    }\n\n    if print_result:\n        print(result_dict)\n\n    return result_dict\n", "CodeBleu_score": 49.296260423244846}, {"code": "def postprocess(\n        self,\n        preds: PredType,\n        visualization: Optional[List[np.ndarray]] = None,\n        return_datasamples: bool = False,\n        print_result: bool = False,\n        no_save_pred: bool = False,\n        pred_out_dir: str = '',\n        **kwargs,\n    ) -> Dict\n\t\n        ...\n\n", "CodeBleu_score": 17.083333333333332}]}
{"instance_id": "a3c2561f199c", "generation_result": [{"code": "def build_test_pipeline(cfg: ConfigType) -> ConfigType\n\t\n    dataset_pipeline = cfg.pipeline\n    resize_transform = dataset_pipeline[0].copy()\n    resize_transform.transforms = [resize_transform.transforms[0]]\n    test_pipeline = Compose([resize_transform, dataset_pipeline[-1]])\n    return test_pipeline\n\n\n", "CodeBleu_score": 17.833428150961048}, {"code": "def build_test_pipeline(cfg: ConfigType) -> ConfigType\n\t\n    dataset_pipeline = cfg.dataset.pipeline\n    test_pipeline = []\n    test_pipeline.append(dataset_pipeline[0].copy())\n    test_pipeline[0].pop('transforms')\n    test_pipeline[0]['transforms'] = [\n        dict(type='Resize', img_scale=(1333, 800), keep_ratio=True)\n    ]\n    test_pipeline.append(dataset_pipeline[-1])\n    return Compose(test_pipeline)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 23.315919840401495}, {"code": "def build_test_pipeline(cfg: ConfigType) -> ConfigType\n\t\n    # Copy the first transform in the dataset pipeline\n    first_transform = cfg.dataset.pipeline[0].copy()\n    # Modify the first transform to include only the 'Resize' transform\n    first_transform.transforms = [first_transform.transforms[0]]\n    # Copy the last transform in the dataset pipeline\n    last_transform = cfg.dataset.pipeline[-1].copy()\n    # Combine the modified first and copied last transforms into a new test pipeline using Compose\n    test_pipeline = Compose([first_transform, last_transform])\n    return test_pipeline\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 24.200617846699014}]}
{"instance_id": "48a358ab5a38", "generation_result": [{"code": "def inference_mot(model: nn.Module, img: np.ndarray, frame_id: int,\n                  video_len: int) -> SampleList\n\t\n    if isinstance(img, np.ndarray):\n        img = torch.from_numpy(img.astype(np.float32))\n    else:\n        img = img.float()\n\n    if img.dim() == 3:\n        img = img.unsqueeze(0)\n\n    data = SampleList()\n    data.add(\"image\", img)\n    data.add(\"frame_id\", torch.tensor([frame_id]))\n    data.add(\"original_shape\", torch.tensor([img.shape[-2], img.shape[-1]]))\n    data.add(\"image_id\", torch.tensor([frame_id]))\n    data.add(\"video_len\", torch.tensor([video_len]))\n\n    if next(model.parameters()).is_cuda:\n        data = data.to(\"cuda\")\n\n    with torch.no_grad():\n        model_output = model.test_step(data, None)\n\n    return model_output", "CodeBleu_score": 38.71948532717835}, {"code": "def inference_mot(model: nn.Module, img: np.ndarray, frame_id: int,\n                  video_len: int) -> SampleList\n\t\n    data = SampleList([{\"image\": img, \"frame_id\": frame_id,\n                         \"original_shape\": img.shape, \"image_id\": frame_id,\n                         \"video_len\": video_len}])\n    if next(model.parameters()).is_cuda:\n        data = data.to(\"cuda\")\n    else:\n        data = data.to(\"cpu\")\n        model = model.cpu()\n        if isinstance(model, nn.DataParallel):\n            model = model.module\n        for module in model.modules():\n            if isinstance(module, RoIPool):\n                raise ValueError(\"RoIPool is not supported on CPU.\")\n    data = model.test_pipeline(data)\n    with torch.no_grad():\n        model_output = model.test_step(data, 0)\n    return model_output\n\n", "CodeBleu_score": 39.229978590602435}, {"code": "def inference_mot(model: nn.Module, img: np.ndarray, frame_id: int,\n                  video_len: int) -> SampleList\n\t\n    sample = Sample()\n    sample.data = img\n    sample.id = frame_id\n    sample.dataset_name = \"coco_2017\"\n    sample.dataset_type = \"val\"\n    sample.video_name = \"crowd_PETS09-S2L1-040204\"\n    sample.filepath = \"datasets/coco/val2017/000000000402.jpg\"\n    sample.original_shape = img.shape\n    sample.image_id = 40204\n    sample.video_id = 402\n    sample.dataset_dir = \"datasets/coco\"\n    sample.height = img.shape[0]\n    sample.width = img.shape[1]\n    sample.frame_id = frame_id\n    sample.video_len = video_len\n\n    sample = model.test_pipeline(sample)\n    sample = model.prepare_sample(sample)\n\n    if next(model.parameters()).is_cuda:\n        model.eval()\n        with torch.no_grad():\n            model_output = model.forward(sample.data)\n    else:\n        model.eval()\n        with torch.no_grad():\n            model_output = model.forward(sample.data)\n\n    return model_output\n\n", "CodeBleu_score": 29.026548463444634}]}
{"instance_id": "6c4ea2ce1787", "generation_result": [{"code": "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\n\t\n    while True:\n        total_length = len(tokens_a) + len(tokens_b)\n        if total_length <= max_num_tokens:\n            break\n        trunc_tokens = tokens_a if len(tokens_a) > len(tokens_b) else tokens_b\n        assert len(trunc_tokens) >= 1\n        # We want to sometimes truncate from the front and sometimes from the\n        # back to add more randomness and avoid biases.\n        if rng.random() < 0.5:\n            del trunc_tokens[0]\n        else:\n            trunc_tokens.pop()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 79.2280525202322}, {"code": "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\n\t\n    while True:\n        total_length = len(tokens_a) + len(tokens_b)\n        if total_length <= max_num_tokens:\n            break\n        trunc_tokens = tokens_a if len(tokens_a) > len(tokens_b) else tokens_b\n        assert len(trunc_tokens) >= 1\n        # We want to sometimes truncate from the front and sometimes from the\n        # back to add more randomness and avoid biases.\n        if rng.random() < 0.5:\n            del trunc_tokens[0]\n        else:\n            trunc_tokens.pop()\n\n", "CodeBleu_score": 79.2280525202322}, {"code": "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\n\t\n    while True:\n        total_length = len(tokens_a) + len(tokens_b)\n        if total_length <= max_num_tokens:\n            break\n        trunc_tokens = tokens_a if len(tokens_a) > len(tokens_b) else tokens_b\n        assert len(trunc_tokens) >= 1\n        # We want to sometimes truncate from the front and sometimes from the\n        # back to add more randomness and avoid biases.\n        if rng.random() < 0.5:\n            del trunc_tokens[0]\n        else:\n            trunc_tokens.pop()\n\n", "CodeBleu_score": 79.2280525202322}]}
{"instance_id": "007181b6729d", "generation_result": [{"code": "def get_activation(activation_string)\n\t\n    if not isinstance(activation_string, str):\n        return activation_string\n\n    activation_string = activation_string.lower()\n    if not activation_string:\n        return None\n\n    # Threshold\n    elif activation_string == \"threshold\":\n        return tf.keras.activations.threshold\n    # Linear\n    elif activation_string == \"linear\":\n        return tf.keras.activations.linear\n    # Relu\n    elif activation_string == \"relu\":\n        return tf.keras.activations.relu\n    # Softmax\n    elif activation_string == \"softmax\":\n        return tf.keras.activations.softmax\n    # Sigmoid\n    elif activation_string == \"sigmoid\":\n        return tf.keras.activations.sigmoid\n    # Tanh\n    elif activation_string == \"tanh\":\n        return tf.keras.activations.tanh\n    # Selu\n    elif activation_string == \"selu\":\n        return tf.keras.activations.selu\n    # Elu\n    elif activation_string == \"elu\":\n        return tf.keras.activations.elu\n    # Softplus\n    elif activation_string == \"softplus\":\n        return tf.keras.activations.softplus\n    # Softsign\n    elif activation_string == \"softsign\":\n        return tf.keras.activations.softsign\n    # Hard_sigmoid\n    elif activation_string == \"hard_sigmoid\":\n        return tf.keras.activations.hard_sigmoid\n    # Exponential\n    elif activation_string == \"exponential\":\n        return tf.keras.activations.exponential\n    # Swish\n    elif activation_string == \"swish\":\n        return tf.keras.activations.swish\n    # Log_sigmoid\n    elif activation_string == \"log_sigmoid\":\n        return tf.keras.activations.log_sigmoid\n    # Log_softmax\n    elif activation_string == \"log_softmax\":\n        return tf.keras.activations.log_softmax\n", "CodeBleu_score": 35.841843509290136}, {"code": "def get_activation(activation_string)\n\t\n    if not isinstance(activation_string, str):\n        return activation_string\n\n    if not activation_string:\n        return None\n\n    activation_string = activation_string.lower()\n    if activation_string == \"relu\":\n        return tf.nn.relu\n    elif activation_string == \"gelu\":\n        return gelu\n    elif activation_string == \"tanh\":\n        return tf.tanh\n    raise ValueError(\"Unsupported activation: %s\" % (activation_string))\n\n", "CodeBleu_score": 62.47375902586837}, {"code": "def get_activation(activation_string)\n\t\n    if not isinstance(activation_string, str):\n        return activation_string\n\n    if not activation_string:\n        return None\n\n    activation_string = activation_string.lower()\n    if activation_string == \"relu\":\n        return tf.nn.relu\n    elif activation_string == \"gelu\":\n        return gelu\n    elif activation_string == \"tanh\":\n        return tf.tanh\n    else:\n        raise ValueError(\"Unsupported activation: %s\" % activation_string)\n\n", "CodeBleu_score": 76.06042942059969}]}
{"instance_id": "d5d6e0315899", "generation_result": [{"code": "def dropout(input_tensor, dropout_prob)\n\t\n    if dropout_prob is None or dropout_prob == 0.0:\n        return input_tensor\n    else:\n        return tf.nn.dropout(input_tensor, dropout_prob)\n", "CodeBleu_score": 53.31243274886709}, {"code": "def dropout(input_tensor, dropout_prob)\n\t\n    if dropout_prob is None or dropout_prob == 0.0:\n        return input_tensor\n    else:\n        return tf.nn.dropout(input_tensor, dropout_prob)\n", "CodeBleu_score": 53.31243274886709}, {"code": "def dropout(input_tensor, dropout_prob)\n\t\n    if dropout_prob is None or dropout_prob == 0.0:\n        return input_tensor\n    else:\n        return tf.nn.dropout(input_tensor, rate=dropout_prob)\n", "CodeBleu_score": 53.03103381915149}]}
{"instance_id": "828917b9ac53", "generation_result": [{"code": "def create_attention_mask_from_input_mask(from_tensor, to_mask)\n\t\n    from_shape = get_shape_list(from_tensor, expected_rank=[2, 3])\n    batch_size = from_shape[0]\n    from_seq_length = from_shape[1]\n\n    to_shape = get_shape_list(to_mask, expected_rank=2)\n    to_seq_length = to_shape[1]\n\n    to_mask = tf.cast(\n        tf.reshape(to_mask, [batch_size, 1, to_seq_length]), tf.float32)\n\n    # We don't assume that `from_tensor` is a mask (although it could be). We\n    # don't actually care if we attend *from* padding tokens (only *to* padding)\n    # tokens so we create a tensor of all ones.\n    #\n    # `broadcast_ones` = [batch_size, from_seq_length, 1]\n    broadcast_ones = tf.ones(\n        shape=[batch_size, from_seq_length, 1], dtype=tf.float32)\n\n    # Here we broadcast along two dimensions to create the mask.\n    mask = broadcast_ones * to_mask\n\n    return mask\n\n", "CodeBleu_score": 64.6737227741467}, {"code": "def create_attention_mask_from_input_mask(from_tensor, to_mask)\n\t\n    from_shape = get_shape_list(from_tensor, expected_rank=[2, 3])\n    batch_size = from_shape[0]\n    from_seq_length = from_shape[1]\n\n    to_shape = get_shape_list(to_mask, expected_rank=2)\n    to_seq_length = to_shape[1]\n\n    to_mask = tf.cast(tf.reshape(to_mask, [batch_size, 1, to_seq_length]), tf.float32)\n\n    # We don't assume that `from_tensor` is a mask (although it could be). We\n    # don't actually care if we attend *from* padding tokens (only *to* padding)\n    # tokens so we create a tensor of all ones.\n    #\n    # `broadcast_ones` = [batch_size, from_seq_length, 1]\n    broadcast_ones = tf.ones(shape=[batch_size, from_seq_length, 1], dtype=tf.float32)\n\n    # Here we broadcast along two dimensions to create the mask.\n    mask = broadcast_ones * to_mask\n\n    return mask\n", "CodeBleu_score": 60.45344108153367}, {"code": "def create_attention_mask_from_input_mask(from_tensor, to_mask)\n\t\n    from_shape = get_shape_list(from_tensor, expected_rank=[2, 3])\n    batch_size = from_shape[0]\n    from_seq_length = from_shape[1]\n\n    to_shape = get_shape_list(to_mask, expected_rank=2)\n    to_seq_length = to_shape[1]\n\n    to_mask = tf.cast(tf.reshape(to_mask, [batch_size, 1, to_seq_length]), tf.float32)\n\n    # We don't assume that `from_tensor` is a mask (although it could be). We\n    # don't actually care if we attend *from* padding tokens (only *to* padding)\n    # tokens so we create a tensor of all ones.\n    #\n    # `broadcast_ones` = [batch_size, from_seq_length, 1]\n    broadcast_ones = tf.ones(shape=[batch_size, from_seq_length, 1], dtype=tf.float32)\n\n    # Here we broadcast along two dimensions to create the mask.\n    mask = broadcast_ones * to_mask\n\n    return mask\n\n", "CodeBleu_score": 60.45344108153367}]}
{"instance_id": "d7798e41a723", "generation_result": [{"code": "def transpose_for_scores(input_tensor, batch_size, num_attention_heads,\n                           seq_length, width)\n\t\n    output_tensor = tf.reshape(\n        input_tensor, [batch_size, seq_length, num_attention_heads, width])\n\n    output_tensor = tf.transpose(output_tensor, [0, 2, 1, 3])\n    return output_tensor\n\n", "CodeBleu_score": 59.99845781533536}, {"code": "def transpose_for_scores(input_tensor, batch_size, num_attention_heads,\n                           seq_length, width)\n\t\n    if seq_length != input_tensor.shape[1]:\n        raise ValueError(\n            \"The seq_length (%d) != input_tensor.shape[1] (%d)\" %\n            (seq_length, input_tensor.shape[1]))\n\n    input_tensor = tf.reshape(\n        input_tensor, [batch_size, seq_length, num_attention_heads, width])\n\n    return tf.transpose(input_tensor, [0, 2, 1, 3])\n\n", "CodeBleu_score": 56.37945240885126}, {"code": "def transpose_for_scores(input_tensor, batch_size, num_attention_heads,\n                           seq_length, width)\n\t\n    input_tensor = tf.reshape(\n        input_tensor, [batch_size, seq_length, num_attention_heads, width])\n\n    return tf.transpose(input_tensor, [0, 2, 1, 3])\n\n", "CodeBleu_score": 49.719542738107044}]}
{"instance_id": "711d466ace09", "generation_result": [{"code": "def reshape_to_matrix(input_tensor)\n\t\n    if len(input_tensor.shape) < 2:\n        raise ValueError(\"The input tensor must be of rank 2 or higher. Got a tensor of rank {}.\".format(len(input_tensor.shape)))\n    if len(input_tensor.shape) == 2:\n        return input_tensor\n    return tf.reshape(input_tensor, [-1, input_tensor.shape[-1]])\n\n", "CodeBleu_score": 32.226331027290655}, {"code": "def reshape_to_matrix(input_tensor)\n\t\n    if len(input_tensor.shape) < 2:\n        raise ValueError(\"The input tensor must have a rank of at least 2. Received a tensor of rank {}\".format(len(input_tensor.shape)))\n    if len(input_tensor.shape) == 2:\n        return input_tensor\n    input_shape = input_tensor.shape.as_list()\n    input_width = input_shape[-1]\n    output_tensor = tf.reshape(input_tensor, [-1, input_width])\n    return output_tensor\n\n", "CodeBleu_score": 51.625511226674384}, {"code": "def reshape_to_matrix(input_tensor)\n\t\n    if input_tensor.shape.ndims < 2:\n        raise ValueError(\"The input tensor must have at least rank 2. Shape = %s\" %\n                         (input_tensor.shape))\n    if input_tensor.shape.ndims == 2:\n        return input_tensor\n\n    width = input_tensor.shape[-1]\n    output_tensor = tf.reshape(input_tensor, [-1, width])\n    return output_tensor\n\n", "CodeBleu_score": 74.21950355082531}]}
{"instance_id": "7adcd08f531e", "generation_result": [{"code": "def get_train_examples(self, data_dir)\n\t\n    lines = self._read_tsv(os.path.join(data_dir, \"train.tsv\"))\n    examples = []\n    for (i, line) in enumerate(lines):\n        if i == 0:\n            continue\n        guid = \"%s-%s\" % (self.name, i)\n        text_a = tokenization.convert_to_unicode(line[3])\n        text_b = tokenization.convert_to_unicode(line[4])\n        label = tokenization.convert_to_unicode(line[0])\n        if label == \"contradictory\":\n            label = \"contradiction\"\n        examples.append(\n            InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n    return examples\n\n", "CodeBleu_score": 44.997312708999836}, {"code": "def get_train_examples(self, data_dir)\n\t\n    lines = self._read_tsv(os.path.join(data_dir, \"train.tsv\"))\n    examples = []\n    for (i, line) in enumerate(lines):\n        if i == 0:\n            continue\n        guid = \"%s-%s\" % (self.name, i)\n        text_a = line[0]\n        text_b = line[1]\n        label = line[2]\n        if label == \"contradictory\":\n            label = \"contradiction\"\n        examples.append(\n            InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n    return examples\n\n", "CodeBleu_score": 37.620909405572625}, {"code": "def get_train_examples(self, data_dir)\n\t\n    lines = self._read_tsv(os.path.join(data_dir, \"train.tsv\"))\n    examples = []\n    for (i, line) in enumerate(lines):\n        if i == 0:\n            continue\n        guid = \"%s-%s\" % (self.name, i)\n        text_a = line[3]\n        text_b = line[4]\n        label = line[0]\n        if label == \"contradictory\":\n            label = \"contradiction\"\n        examples.append(InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n    return examples\n\n", "CodeBleu_score": 36.13738000521698}]}
{"instance_id": "26a0bb96dab9", "generation_result": [{"code": "def metric_fn(per_example_loss, label_ids, logits, is_real_example)\n\t\n    predictions = tf.argmax(input=logits, axis=-1, output_type=tf.int32)\n    accuracy = tf.metrics.accuracy(\n        labels=label_ids, predictions=predictions, weights=is_real_example)\n    loss = tf.metrics.mean(values=per_example_loss, weights=is_real_example)\n    return {\n        \"eval_accuracy\": accuracy,\n        \"eval_loss\": loss,\n    }\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 62.27890851264386}, {"code": "def metric_fn(per_example_loss, label_ids, logits, is_real_example)\n\t\n    predictions = tf.argmax(input=logits, axis=-1, output_type=tf.int32)\n    accuracy = tf.metrics.accuracy(\n        labels=label_ids, predictions=predictions, weights=is_real_example)\n    loss = tf.metrics.mean(values=per_example_loss, weights=is_real_example)\n    return {\n        \"eval_accuracy\": accuracy,\n        \"eval_loss\": loss,\n    }\n\n", "CodeBleu_score": 62.27890851264386}, {"code": "def metric_fn(per_example_loss, label_ids, logits, is_real_example)\n\t\n    predictions = tf.argmax(input=logits, axis=-1, output_type=tf.int32)\n    accuracy = tf.metrics.accuracy(\n        labels=label_ids, predictions=predictions, weights=is_real_example)\n    loss = tf.metrics.mean(values=per_example_loss, weights=is_real_example)\n    return {\n        'eval_accuracy': accuracy,\n        'eval_loss': loss,\n    }\n\n", "CodeBleu_score": 56.673335173435206}]}
{"instance_id": "6154e4d28509", "generation_result": [{"code": "def convert_examples_to_features(examples, label_list, max_seq_length,\n                                 tokenizer)\n\t\n    features = []\n    for (ex_index, example) in enumerate(examples):\n        if ex_index % 10000 == 0:\n            logger.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n\n        feature = convert_single_example(ex_index, example, label_list,\n                                         max_seq_length, tokenizer)\n\n        features.append(feature)\n\n    return features\n\n", "CodeBleu_score": 73.75375987448578}, {"code": "def convert_examples_to_features(examples, label_list, max_seq_length,\n                                 tokenizer)\n\t\n\n    features = []\n    for (ex_index, example) in enumerate(examples):\n        if ex_index % 10000 == 0:\n            logger.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n\n        feature = convert_single_example(ex_index, example, label_list,\n                                         max_seq_length, tokenizer)\n\n        features.append(feature)\n\n    return features\n\n", "CodeBleu_score": 73.75375987448578}, {"code": "def convert_examples_to_features(examples, label_list, max_seq_length,\n                                 tokenizer)\n\t\n    features = []\n    for (ex_index, example) in enumerate(examples):\n        if ex_index % 10000 == 0:\n            logger.info(\"Writing example %d of %d\", ex_index, len(examples))\n\n        feature = convert_single_example(ex_index, example, label_list,\n                                         max_seq_length, tokenizer)\n\n        features.append(feature)\n    return features\n\n", "CodeBleu_score": 67.0727573198984}]}
{"instance_id": "e09c0db71ebb", "generation_result": [{"code": "def get_next_sentence_output(bert_config, input_tensor, labels)\n\t\n    with tf.variable_scope(\"cls/seq_relationship\"):\n        output_weights = tf.get_variable(\n            \"output_weights\",\n            shape=[2, bert_config.hidden_size],\n            initializer=modeling.create_initializer(bert_config.initializer_range))\n        output_bias = tf.get_variable(\n            \"output_bias\", shape=[2], initializer=tf.zeros_initializer())\n\n        logits = tf.matmul(input_tensor, output_weights, transpose_b=True)\n        logits = tf.nn.bias_add(logits, output_bias)\n        log_probs = tf.nn.log_softmax(logits, axis=-1)\n        labels = tf.reshape(labels, [-1])\n        one_hot_labels = tf.one_hot(labels, depth=2, dtype=tf.float32)\n        per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n        loss = tf.reduce_mean(per_example_loss)\n        return (loss, per_example_loss, log_probs)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 86.94900271179796}, {"code": "def get_next_sentence_output(bert_config, input_tensor, labels)\n\t\n    with tf.variable_scope(\"cls/seq_relationship\"):\n        output_weights = tf.get_variable(\n            \"output_weights\",\n            shape=[2, bert_config.hidden_size],\n            initializer=modeling.create_initializer(bert_config.initializer_range))\n        output_bias = tf.get_variable(\n            \"output_bias\", shape=[2], initializer=tf.zeros_initializer())\n\n        logits = tf.matmul(input_tensor, output_weights, transpose_b=True)\n        logits = tf.nn.bias_add(logits, output_bias)\n        log_probs = tf.nn.log_softmax(logits, axis=-1)\n\n        labels = tf.reshape(labels, [-1])\n        one_hot_labels = tf.one_hot(labels, depth=2, dtype=tf.float32)\n\n        per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n        loss = tf.reduce_mean(per_example_loss)\n        return (loss, per_example_loss, log_probs)\n\n", "CodeBleu_score": 86.94900271179796}, {"code": "def get_next_sentence_output(bert_config, input_tensor, labels)\n\t\n    # Input tensor is of shape [batch_size, hidden_size]\n    # Labels tensor is of shape [batch_size, 1]\n    # Output tensor is of shape [batch_size, 2]\n    with tf.variable_scope(\"cls/seq_relationship\"):\n        output_weights = tf.get_variable(\n            \"output_weights\",\n            shape=[2, bert_config.hidden_size],\n            initializer=modeling.create_initializer(bert_config.initializer_range))\n        output_bias = tf.get_variable(\n            \"output_bias\", shape=[2], initializer=tf.zeros_initializer())\n\n        logits = tf.matmul(input_tensor, output_weights, transpose_b=True)\n        logits = tf.nn.bias_add(logits, output_bias)\n        log_probs = tf.nn.log_softmax(logits, axis=-1)\n        labels = tf.reshape(labels, [-1])\n        one_hot_labels = tf.one_hot(labels, depth=2, dtype=tf.float32)\n        per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n        loss = tf.reduce_mean(per_example_loss)\n        return (loss, per_example_loss, log_probs)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 86.94900271179796}]}
{"instance_id": "1f66b020f485", "generation_result": [{"code": "def gather_indexes(sequence_tensor, positions)\n\t\n    sequence_shape = modeling.get_shape_list(sequence_tensor, expected_rank=3)\n    batch_size = sequence_shape[0]\n    seq_length = sequence_shape[1]\n    width = sequence_shape[2]\n\n    flat_offsets = tf.reshape(\n        tf.range(0, batch_size, dtype=tf.int32) * seq_length, [-1, 1])\n    flat_positions = tf.reshape(positions + flat_offsets, [-1])\n    flat_sequence_tensor = tf.reshape(sequence_tensor,\n                                      [batch_size * seq_length, width])\n    output_tensor = tf.gather(flat_sequence_tensor, flat_positions)\n    return output_tensor\n\n", "CodeBleu_score": 57.853066637478335}, {"code": "def gather_indexes(sequence_tensor, positions)\n\t\n    sequence_shape = get_shape_list(sequence_tensor, expected_rank=3)\n    batch_size = sequence_shape[0]\n    seq_length = sequence_shape[1]\n    width = sequence_shape[2]\n\n    flat_offsets = tf.reshape(\n        tf.range(0, batch_size, dtype=tf.int32) * seq_length, [-1, 1])\n    flat_positions = tf.reshape(positions + flat_offsets, [-1])\n    flat_sequence_tensor = tf.reshape(sequence_tensor,\n                                      [batch_size * seq_length, width])\n    output_tensor = tf.gather(flat_sequence_tensor, flat_positions)\n    return output_tensor\n\n", "CodeBleu_score": 56.404511250960866}, {"code": "def gather_indexes(sequence_tensor, positions)\n\t\n    sequence_shape = get_shape_list(sequence_tensor, expected_rank=3)\n    batch_size = sequence_shape[0]\n    seq_length = sequence_shape[1]\n    width = sequence_shape[2]\n\n    flat_offsets = tf.reshape(\n        tf.range(0, batch_size, dtype=tf.int32) * seq_length, [-1, 1])\n    flat_positions = tf.reshape(positions + flat_offsets, [-1])\n    output_tensor = tf.gather(tf.reshape(sequence_tensor,\n                                         [batch_size * seq_length, width]),\n                              flat_positions)\n    return output_tensor\n\n", "CodeBleu_score": 49.444941488798236}]}
{"instance_id": "956bffee9a33", "generation_result": [{"code": "def _decode_record(record, name_to_features)\n\t\n    example = tf.parse_single_example(record, name_to_features)\n\n    # tf.Example only supports tf.int64, but the TPU only supports tf.int32.\n    # So cast all int64 to int32.\n    for name in list(example.keys()):\n        t = example[name]\n        if t.dtype == tf.int64:\n            t = tf.to_int32(t)\n        example[name] = t\n\n    return example\n\n", "CodeBleu_score": 71.91618952996937}, {"code": "def _decode_record(record, name_to_features)\n\t\n    example = tf.parse_single_example(record, name_to_features)\n\n    # tf.Example only supports tf.int64, but the TPU only supports tf.int32.\n    # So cast all int64 to int32.\n    for name in list(example.keys()):\n        t = example[name]\n        if t.dtype == tf.int64:\n            t = tf.to_int32(t)\n        example[name] = t\n\n    return example\n\n", "CodeBleu_score": 71.91618952996937}, {"code": "def _decode_record(record, name_to_features)\n\t\n    example = tf.parse_single_example(record, name_to_features)\n\n    # tf.Example only supports tf.int64, but the TPU only supports tf.int32.\n    # So cast all int64 to int32.\n    for name in list(example.keys()):\n        t = example[name]\n        if t.dtype == tf.int64:\n            t = tf.to_int32(t)\n        example[name] = t\n\n    return example\n\n", "CodeBleu_score": 71.91618952996937}]}
{"instance_id": "6b97f354e641", "generation_result": [{"code": "def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer,\n                         orig_answer_text)\n\t\n    tok_answer_text = \" \".join(tokenizer.tokenize(orig_answer_text))\n\n    for new_start in range(input_start, input_end + 1):\n        for new_end in range(input_end, new_start - 1, -1):\n            text_span = \" \".join(doc_tokens[new_start:(new_end + 1)])\n            if text_span == tok_answer_text:\n                return new_start, new_end\n\n    return input_start, input_end\n\n", "CodeBleu_score": 69.51763050033676}, {"code": "def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer,\n                         orig_answer_text)\n\t\n    tok_answer_text = \" \".join(tokenizer.tokenize(orig_answer_text))\n\n    for new_start in range(input_start, input_end + 1):\n        for new_end in range(input_end, new_start - 1, -1):\n            text_span = \" \".join(doc_tokens[new_start:(new_end + 1)])\n            if text_span == tok_answer_text:\n                return new_start, new_end\n\n    return input_start, input_end\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 69.51763050033676}, {"code": "def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer,\n                         orig_answer_text)\n\t\n\n    tok_answer_text = \" \".join(tokenizer.tokenize(orig_answer_text))\n\n    for new_start in range(input_start, input_end + 1):\n        for new_end in range(input_end, new_start - 1, -1):\n            text_span = \" \".join(doc_tokens[new_start:(new_end + 1)])\n            if text_span == tok_answer_text:\n                return new_start, new_end\n\n    return input_start, input_end\n\n", "CodeBleu_score": 69.51763050033676}]}
{"instance_id": "aba334b92b34", "generation_result": [{"code": "def _check_is_max_context(doc_spans, cur_span_index, position)\n\t\n    best_score = None\n    best_span_index = None\n    for (span_index, doc_span) in enumerate(doc_spans):\n        end = doc_span.start + doc_span.length - 1\n        if position < doc_span.start:\n            continue\n        if position > end:\n            continue\n        num_left_context = position - doc_span.start\n        num_right_context = end - position\n        score = min(num_left_context, num_right_context) + 0.01 * doc_span.length\n        if best_score is None or score > best_score:\n            best_score = score\n            best_span_index = span_index\n\n    return cur_span_index == best_span_index\n\n", "CodeBleu_score": 78.23889503357583}, {"code": "def _check_is_max_context(doc_spans, cur_span_index, position)\n\t\n    best_score = None\n    best_span_index = None\n    for (span_index, doc_span) in enumerate(doc_spans):\n        end = doc_span.start + doc_span.length - 1\n        if position < doc_span.start:\n            continue\n        if position > end:\n            continue\n        num_left_context = position - doc_span.start\n        num_right_context = end - position\n        score = min(num_left_context, num_right_context) + 0.01 * doc_span.length\n        if best_score is None or score > best_score:\n            best_score = score\n            best_span_index = span_index\n\n    return cur_span_index == best_span_index\n\n", "CodeBleu_score": 78.23889503357583}, {"code": "def _check_is_max_context(doc_spans, cur_span_index, position)\n\t\n    best_score = None\n    best_span_index = None\n    for (span_index, doc_span) in enumerate(doc_spans):\n        end = doc_span.start + doc_span.length - 1\n        if position < doc_span.start:\n            continue\n        if position > end:\n            continue\n        num_left_context = position - doc_span.start\n        num_right_context = end - position\n        score = min(num_left_context, num_right_context) + 0.01 * doc_span.length\n        if best_score is None or score > best_score:\n            best_score = score\n            best_span_index = span_index\n\n    return cur_span_index == best_span_index\n\n", "CodeBleu_score": 78.23889503357583}]}
{"instance_id": "cf92ad509f6a", "generation_result": [{"code": "def load_vocab(vocab_file)\n\t\n    vocab = collections.OrderedDict()\n    index = 0\n    with GFile(vocab_file, \"r\") as reader:\n        while True:\n            token = convert_to_unicode(reader.readline())\n            if not token:\n                break\n            token = token.strip()\n            vocab[token] = index\n            index += 1\n    return vocab\n\n", "CodeBleu_score": 69.38232169171202}, {"code": "def load_vocab(vocab_file)\n\t\n    vocab = OrderedDict()\n    index = 0\n    with open(vocab_file, \"r\", encoding=\"utf-8\") as reader:\n        while True:\n            token = convert_to_unicode(reader.readline())\n            if not token:\n                break\n            token = token.strip()\n            vocab[token] = index\n            index += 1\n    return vocab\n\n", "CodeBleu_score": 64.49369445962223}, {"code": "def load_vocab(vocab_file)\n\t\n    vocab = collections.OrderedDict()\n    index = 0\n    with GFile(vocab_file, \"r\") as reader:\n        while True:\n            token = convert_to_unicode(reader.readline())\n            if not token:\n                break\n            token = token.strip()\n            vocab[token] = index\n            index += 1\n    return vocab\n\n", "CodeBleu_score": 69.38232169171202}]}
{"instance_id": "6c865803ac12", "generation_result": [{"code": "def convert_by_vocab(vocab, items)\n\t\n    output = []\n    for item in items:\n        output.append(vocab[item])\n    return output\n\n", "CodeBleu_score": 79.63749907165396}, {"code": "def convert_by_vocab(vocab, items)\n\t\n    output = []\n    for item in items:\n        output.append(vocab[item])\n    return output\n\n", "CodeBleu_score": 79.63749907165396}, {"code": "def convert_by_vocab(vocab, items)\n\t\n    output = []\n    for item in items:\n        output.append(vocab[item])\n    return output\n\n", "CodeBleu_score": 79.63749907165396}]}
{"instance_id": "e93b9200dc4a", "generation_result": [{"code": "def tokenize(self, text)\n\t\n    tokens = self.basic_tokenizer.tokenize(text)\n    split_tokens = []\n    for token in tokens:\n        for sub_token in self.wordpiece_tokenizer.tokenize(token):\n            split_tokens.append(sub_token)\n    return split_tokens\n\n", "CodeBleu_score": 56.71761519883849}, {"code": "def tokenize(self, text)\n\t\n    split_tokens = []\n    for token in self.basic_tokenizer.tokenize(text):\n        for sub_token in self.wordpiece_tokenizer.tokenize(token):\n            split_tokens.append(sub_token)\n    return split_tokens\n\n", "CodeBleu_score": 88.10762355418231}, {"code": "def tokenize(self, text)\n\t\n    tokens = self.basic_tokenizer.tokenize(text)\n    split_tokens = []\n    for token in tokens:\n        for sub_token in self.wordpiece_tokenizer.tokenize(token):\n            split_tokens.append(sub_token)\n    return split_tokens\n\n", "CodeBleu_score": 56.71761519883849}]}
{"instance_id": "2ff6f6db53c9", "generation_result": [{"code": "def safe_load(path, offset, duration, sample_rate, dtype)\n\t\n    try:\n        logger.info(f\"Loading audio data from {path} with offset {offset}, duration {duration}, sample rate {sample_rate} and dtype {dtype}\")\n        data = self.load(path, offset=offset, duration=duration, sample_rate=sample_rate, dtype=dtype)\n        logger.info(f\"Successfully loaded audio data from {path} with offset {offset}, duration {duration}, sample rate {sample_rate} and dtype {dtype}\")\n        return data, False\n    except Exception as e:\n        logger.exception(f\"Error loading audio data from {path} with offset {offset}, duration {duration}, sample rate {sample_rate} and dtype {dtype}\")\n        return -1.0, True\n\n", "CodeBleu_score": 36.51639231816482}, {"code": "def safe_load(path, offset, duration, sample_rate, dtype)\n\t\n    try:\n        logger.info(\"Loading audio data from {} with offset {} and duration {}\".format(path, offset, duration))\n        audio = self.load(path, offset, duration, sample_rate, dtype)\n        logger.info(\"Successfully loaded audio data from {} with offset {} and duration {}\".format(path, offset, duration))\n        return audio, False\n    except Exception as e:\n        logger.exception(\"Error loading audio data from {} with offset {} and duration {}\".format(path, offset, duration))\n        return -1.0, True\n\n", "CodeBleu_score": 33.414154110531236}, {"code": "def safe_load(path, offset, duration, sample_rate, dtype)\n\t\n    try:\n        logger.info(f\"Loading {path} from {offset} to {duration} with sample rate {sample_rate} and dtype {dtype}\")\n        data = self.load(path, offset=offset, duration=duration, sample_rate=sample_rate, dtype=dtype)\n        logger.info(f\"Successfully loaded {path} from {offset} to {duration} with sample rate {sample_rate} and dtype {dtype}\")\n        return data, False\n    except Exception as e:\n        logger.exception(f\"Failed to load {path} from {offset} to {duration} with sample rate {sample_rate} and dtype {dtype}\")\n        return -1.0, True\n\n", "CodeBleu_score": 38.729848217619455}]}
{"instance_id": "12fbe1f03d8f", "generation_result": [{"code": "def to_n_channels(waveform: tf.Tensor, n_channels: int) -> tf.Tensor\n\t\n    n_waveform_channels = tf.shape(waveform)[-1]\n    if n_waveform_channels == n_channels:\n        return waveform\n    elif n_waveform_channels > n_channels:\n        return waveform[..., :n_channels]\n    else:\n        return tf.tile(waveform, [1, n_channels // n_waveform_channels])[..., :n_channels]\n\n", "CodeBleu_score": 46.2452197927866}, {"code": "def to_n_channels(waveform: tf.Tensor, n_channels: int) -> tf.Tensor\n\t\n    if waveform.shape[0] == n_channels:\n        return waveform\n    elif waveform.shape[0] < n_channels:\n        waveform = tf.tile(waveform, [n_channels // waveform.shape[0] + 1, 1])\n        return waveform[:n_channels]\n    else:\n        return waveform[:n_channels]\n\n", "CodeBleu_score": 43.14787281647931}, {"code": "def to_n_channels(waveform: tf.Tensor, n_channels: int) -> tf.Tensor\n\t\n    if waveform.shape[1] > n_channels:\n        return waveform[:, :n_channels]\n    elif waveform.shape[1] == n_channels:\n        return waveform\n    else:\n        return tf.tile(waveform, [1, n_channels // waveform.shape[1]])[:, :n_channels]\n\n", "CodeBleu_score": 48.55841306680638}]}
{"instance_id": "5e215f029b2e", "generation_result": [{"code": "def spectrogram_to_db_uint(\n    spectrogram: tf.Tensor, db_range: float = 100.0, **kwargs\n) -> tf.Tensor\n\t\n    spectrogram = gain_to_db(spectrogram, ref_value=1.0, amin=1e-20, **kwargs)\n    spectrogram = tf.clip_by_value(spectrogram, -db_range, 0.0)\n    spectrogram = tf.maximum(spectrogram, -db_range)\n    spectrogram = from_float32_to_uint8(spectrogram)\n    return spectrogram\n\n", "CodeBleu_score": 41.061901226271424}, {"code": "def spectrogram_to_db_uint(\n    spectrogram: tf.Tensor, db_range: float = 100.0, **kwargs\n) -> tf.Tensor\n\t\n    # Convert to dB\n    spectrogram_db = gain_to_db(spectrogram, **kwargs)\n    # Clip to range\n    spectrogram_db = tf.clip_by_value(\n        spectrogram_db, clip_value_min=-db_range, clip_value_max=0.0\n    )\n    # Convert to uint8\n    spectrogram_db = tf.maximum(spectrogram_db, 0.0)\n    spectrogram_db = tf.reduce_max(spectrogram_db)\n    spectrogram_db = from_float32_to_uint8(spectrogram_db)\n    return spectrogram_db\n\n", "CodeBleu_score": 41.54720815660755}, {"code": "def spectrogram_to_db_uint(\n    spectrogram: tf.Tensor, db_range: float = 100.0, **kwargs\n) -> tf.Tensor\n\t\n    spectrogram = tf.math.maximum(spectrogram, 1e-10)\n    spectrogram = gain_to_db(spectrogram, db_range)\n    max_db = tf.reduce_max(spectrogram)\n    spectrogram = tf.math.maximum(spectrogram, max_db - db_range)\n    spectrogram = tf.math.minimum(spectrogram, max_db)\n    spectrogram = from_float32_to_uint8(spectrogram)\n    return spectrogram\n\n", "CodeBleu_score": 44.840684740892364}]}
{"instance_id": "b65c0968dae7", "generation_result": [{"code": "def compute_spectrogram_tf(\n    waveform: tf.Tensor,\n    frame_length: int = 2048,\n    frame_step: int = 512,\n    spec_exponent: float = 1.0,\n    window_exponent: float = 1.0,\n) -> tf.Tensor\n\t\n    window = tf.signal.hann_window(frame_length, periodic=True, dtype=tf.float32)\n    window = tf.pow(window, window_exponent)\n    stft = tf.signal.stft(\n        waveform,\n        frame_length=frame_length,\n        frame_step=frame_step,\n        fft_length=frame_length,\n        window_fn=lambda frame_length, dtype: window,\n    )\n    spectrogram = tf.math.pow(tf.abs(stft), spec_exponent)\n    spectrogram = tf.transpose(spectrogram, perm=[1, 0, 2])\n    return spectrogram\n\n", "CodeBleu_score": 43.30505611192503}, {"code": "def compute_spectrogram_tf(\n    waveform: tf.Tensor,\n    frame_length: int = 2048,\n    frame_step: int = 512,\n    spec_exponent: float = 1.0,\n    window_exponent: float = 1.0,\n) -> tf.Tensor\n\t\n\n    # Apply Hann window function\n    window = tf.signal.hann_window(frame_length, periodic=True)\n    window = tf.pow(window, window_exponent)\n\n    # Apply STFT\n    stft = tf.signal.stft(\n        waveform,\n        frame_length=frame_length,\n        frame_step=frame_step,\n        fft_length=frame_length,\n        window_fn=lambda frame_length, dtype: tf.cast(window, dtype),\n    )\n\n    # Compute spectrogram\n    spectrogram = tf.pow(tf.abs(stft), spec_exponent)\n\n    # Transpose spectrogram\n    spectrogram = tf.transpose(spectrogram)\n\n    return spectrogram\n", "CodeBleu_score": 41.2310659979111}, {"code": "def compute_spectrogram_tf(\n    waveform: tf.Tensor,\n    frame_length: int = 2048,\n    frame_step: int = 512,\n    spec_exponent: float = 1.0,\n    window_exponent: float = 1.0,\n) -> tf.Tensor\n\t\n    window_length = frame_length\n    window = tf.signal.hann_window(window_length, periodic=True)\n    window = tf.pow(window, window_exponent)\n    waveform_stft = tf.signal.stft(\n        waveform,\n        frame_length=frame_length,\n        frame_step=frame_step,\n        fft_length=frame_length,\n        window_fn=tf.signal.hann_window,\n        pad_end=False,\n    )\n    waveform_stft = tf.pow(tf.abs(waveform_stft), spec_exponent)\n    waveform_stft = tf.transpose(waveform_stft)\n    return waveform_stft\n\n", "CodeBleu_score": 37.878461805832515}]}
{"instance_id": "a4686140a634", "generation_result": [{"code": "def time_stretch(\n    spectrogram: tf.Tensor,\n    factor: float = 1.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n    new_time_dim = tf.cast(tf.shape(spectrogram)[1] * factor, tf.int32)\n    stretched_spectrogram = tf.image.resize(\n        spectrogram, [tf.shape(spectrogram)[0], new_time_dim], method=method\n    )\n    return resize_image_with_crop_or_pad(stretched_spectrogram, tf.shape(spectrogram)[1])\n\n", "CodeBleu_score": 38.728282631989075}, {"code": "def time_stretch(\n    spectrogram: tf.Tensor,\n    factor: float = 1.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n\n    # Calculate the new time dimension.\n    new_time_dim = tf.cast(tf.shape(spectrogram)[1] * factor, tf.int32)\n\n    # Resize the spectrogram.\n    stretched = tf.image.resize(spectrogram, [tf.shape(spectrogram)[0], new_time_dim], method=method)\n\n    # Crop or pad the spectrogram to the original dimensions.\n    stretched = resize_image_with_crop_or_pad(stretched, tf.shape(spectrogram)[0], tf.shape(spectrogram)[1])\n\n    return stretched\n\n", "CodeBleu_score": 38.38022330661972}, {"code": "def time_stretch(\n    spectrogram: tf.Tensor,\n    factor: float = 1.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n    if factor == 1.0:\n        return spectrogram\n    else:\n        new_time_dim = tf.cast(tf.shape(spectrogram)[1] * factor, tf.int32)\n        resized_spectrogram = tf.image.resize(\n            spectrogram, [tf.shape(spectrogram)[0], new_time_dim], method=method\n        )\n        return resize_image_with_crop_or_pad(resized_spectrogram, tf.shape(spectrogram)[0])\n\n", "CodeBleu_score": 44.017628625426205}]}
{"instance_id": "6dd01101d4ae", "generation_result": [{"code": "def pitch_shift(\n    spectrogram: tf.Tensor,\n    semitone_shift: float = 0.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n    if semitone_shift == 0.0:\n        return spectrogram\n    else:\n        # Calculate the resizing factor\n        resize_factor = tf.cast(\n            2 ** (semitone_shift / 12), dtype=tf.float32\n        )\n        # Resize the spectrogram\n        resized_spectrogram = tf.image.resize(\n            spectrogram,\n            [spectrogram.shape[0], spectrogram.shape[1] * resize_factor],\n            method=method,\n        )\n        # Pad the resized spectrogram\n        pitch_shifted_spectrogram = tf.pad(\n            resized_spectrogram,\n            [\n                [0, 0],\n                [0, tf.maximum(0, spectrogram.shape[1] - resized_spectrogram.shape[1])],\n            ],\n            constant_values=0,\n        )\n        return pitch_shifted_spectrogram\n\n", "CodeBleu_score": 35.49701090728373}, {"code": "def pitch_shift(\n    spectrogram: tf.Tensor,\n    semitone_shift: float = 0.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n    if semitone_shift == 0:\n        return spectrogram\n    factor = 2 ** (semitone_shift / 12)\n    new_size = tf.cast(tf.shape(spectrogram)[0] * factor, tf.int32)\n    spectrogram = tf.image.resize(spectrogram, [new_size, tf.shape(spectrogram)[1]], method=method)\n    spectrogram = tf.pad(spectrogram, [[0, tf.shape(spectrogram)[0] - new_size], [0, 0]])\n    return spectrogram\n\n", "CodeBleu_score": 44.87477749075095}, {"code": "def pitch_shift(\n    spectrogram: tf.Tensor,\n    semitone_shift: float = 0.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n\n    # Calculate the resizing factor\n    factor = 2.0 ** (semitone_shift / 12.0)\n\n    # Resize the spectrogram\n    resized_spectrogram = tf.image.resize(\n        spectrogram,\n        [spectrogram.shape[0], int(spectrogram.shape[1] * factor)],\n        method=method,\n    )\n\n    # Pad the resized spectrogram\n    padded_spectrogram = tf.pad(\n        resized_spectrogram,\n        [\n            [0, 0],\n            [0, tf.maximum(0, spectrogram.shape[1] - resized_spectrogram.shape[1])],\n        ],\n        mode=\"REFLECT\",\n    )\n\n    return padded_spectrogram\n\n", "CodeBleu_score": 29.919686600731843}]}
{"instance_id": "612f0826e56a", "generation_result": [{"code": "def _get_conv_activation_layer(params: Dict) -> Any\n\t\n    if params.get(\"conv_activation\") is None:\n        return nn.ReLU()\n    elif params[\"conv_activation\"] == \"ReLU\":\n        return nn.ReLU()\n    elif params[\"conv_activation\"] == \"ELU\":\n        return nn.ELU()\n    elif params[\"conv_activation\"] == \"LeakyReLU\":\n        return nn.LeakyReLU(negative_slope=0.2)\n    else:\n        raise ValueError(f\"Activation function {params['conv_activation']} is not supported.\")\n\n", "CodeBleu_score": 33.80788225412354}, {"code": "def _get_conv_activation_layer(params: Dict) -> Any\n\t\n    if params.get(\"conv_activation\", None) == \"ReLU\":\n        return ReLU\n    elif params.get(\"conv_activation\", None) == \"ELU\":\n        return ELU\n    elif params.get(\"conv_activation\", None) == \"LeakyReLU\":\n        return LeakyReLU\n    else:\n        return partial(LeakyReLU, negative_slope=0.2)\n\n", "CodeBleu_score": 31.9073576248619}, {"code": "def _get_conv_activation_layer(params: Dict) -> Any\n\t\n    if params.get(\"conv_activation\") == \"LeakyReLU\":\n        return nn.LeakyReLU(negative_slope=0.2)\n    elif params.get(\"conv_activation\") == \"ELU\":\n        return nn.ELU()\n    else:\n        return nn.ReLU()\n\n", "CodeBleu_score": 40.02338733323833}]}
{"instance_id": "2f8b5a2fc0f2", "generation_result": [{"code": "def softmax_unet(\n    input_tensor: tf.Tensor, instruments: Iterable[str], params: Dict = {}\n) -> Dict\n\t\n    # Set default parameters\n    params_default = {\n        \"axis\": 2,\n        \"model\": None,\n        \"input_shape\": None,\n        \"output_shape\": None,\n        \"input_key\": \"mix\",\n        \"output_key\": \"vocals\",\n        \"logit_key\": \"logit\",\n        \"mask_key\": \"mask\",\n    }\n    # Assign default parameters if not provided\n    if params is None:\n        params = params_default\n    else:\n        for key in params_default.keys():\n            params_default[key] = params.get(key, params_default[key])\n        params = params_default\n    # Apply the U-Net model to the input tensor for each instrument\n    logit_mask_list = []\n    for instrument in instruments:\n        # Create a dictionary for the instrument\n        instrument_dict = {\n            params[\"input_key\"]: input_tensor,\n        }\n        # Apply the U-Net model to the instrument dictionary\n        instrument_dict = apply_unet(instrument_dict, params[\"model\"], params[\"input_shape\"], params[\"output_shape\"])\n        # Collect the logit masks\n        logit_mask_list.append(instrument_dict[params[\"logit_key\"]])\n    # Combine the logit masks using a softmax function along a specified axis\n    logit_mask_stack = tf.stack(logit_mask_list, axis=params[\"axis\"])\n    mask_stack = tf.nn.softmax(logit_mask_stack, axis=params[\"axis\"])\n    # Create an output dictionary where each entry is a spectrogram for an instrument\n    output_dict = {}\n    for instrument, mask in zip(instruments, mask_stack):\n        output_dict[instrument] = input_tensor * mask\n    # Return the dictionary of instrument spectrograms\n    return output_dict\n", "CodeBleu_score": 37.592721302342255}, {"code": "def softmax_unet(\n    input_tensor: tf.Tensor, instruments: Iterable[str], params: Dict = {}\n) -> Dict\n\t\n    # get the parameters\n    axis = params.get(\"axis\", 0)\n    # create the output dictionary\n    output_dict = {}\n    # iterate over the instruments\n    for i, instrument in enumerate(instruments):\n        # apply the U-Net model to the input tensor for the instrument\n        logit_mask = apply_unet(input_tensor, instrument, params)\n        # collect the logit masks\n        logit_mask_list.append(logit_mask)\n    # combine the logit masks using a softmax function along the specified axis\n    mask_stack = tf.stack(logit_mask_list, axis=axis)\n    mask = tf.nn.softmax(mask_stack, axis=axis)\n    # iterate over the instruments\n    for i, instrument in enumerate(instruments):\n        # create the spectrogram for the instrument\n        output_dict[instrument] = input_tensor * mask[i]\n    # return the output dictionary\n    return output_dict\n\n", "CodeBleu_score": 38.00147388449364}, {"code": "def softmax_unet(\n    input_tensor: tf.Tensor, instruments: Iterable[str], params: Dict = {}\n) -> Dict\n\t\n\n    # Create an empty dictionary for the output\n    output_dict = {}\n\n    # Iterate over the instruments\n    for instrument in instruments:\n\n        # Apply the U-Net model to the input tensor for the current instrument\n        logit_mask = apply_unet(input_tensor, instrument, params)\n\n        # Append the logit mask to the list\n        logit_mask_list.append(logit_mask)\n\n    # Combine the logit masks using a softmax function along a specified axis\n    logit_mask_combined = tf.stack(logit_mask_list, axis=axis)\n    mask_combined = tf.nn.softmax(logit_mask_combined)\n\n    # Iterate over the instruments and create an output dictionary\n    for i, instrument in enumerate(instruments):\n\n        # Multiply the input tensor with the mask for the current instrument\n        output_dict[instrument] = input_tensor * mask_combined[i]\n\n    # Return the output dictionary\n    return output_dict\n", "CodeBleu_score": 35.969367357203254}]}
{"instance_id": "400b19ce8520", "generation_result": [{"code": "def compute_file_checksum(path)\n\t\n    sha256 = hashlib.sha256()\n    with open(path, 'rb') as file:\n        while True:\n            data = file.read(65536)\n            if not data:\n                break\n            sha256.update(data)\n    return sha256.hexdigest()\n\n", "CodeBleu_score": 38.611110051873965}, {"code": "def compute_file_checksum(path)\n\t\n    sha256 = hashlib.sha256()\n    with open(path, 'rb') as f:\n        for chunk in iter(lambda: f.read(4096), b''):\n            sha256.update(chunk)\n    return sha256.hexdigest()\n\n", "CodeBleu_score": 56.88014708665297}, {"code": "def compute_file_checksum(path)\n\t\n    sha256 = hashlib.sha256()\n    with open(path, 'rb') as f:\n        while True:\n            data = f.read(65536)\n            if not data:\n                break\n            sha256.update(data)\n    return sha256.hexdigest()\n\n", "CodeBleu_score": 38.0396105771608}]}
{"instance_id": "5f2e9c6e15f1", "generation_result": [{"code": "def checksum(self, name: str) -> str\n\t\n    url = f\"{self.host}/{self.repository}/{self.release}/{self.checksum_index}\"\n    response = httpx.get(url)\n    response.raise_for_status()\n    checksum_index = response.json()\n    if name in checksum_index:\n        return checksum_index[name]\n    else:\n        raise ValueError(f\"Model name not found: {name}\")\n", "CodeBleu_score": 35.12512043711573}, {"code": "def checksum(self, name: str) -> str\n\t\n\n    url = f\"{self.host}/{self.repository}/{self.release}/{self.checksum_index}\"\n    with httpx.Client() as client:\n        response = client.get(url)\n        response.raise_for_status()\n        checksums = response.json()\n        if name not in checksums:\n            raise ValueError(f\"Model {name} not found in checksum index.\")\n        return checksums[name]\n\n", "CodeBleu_score": 42.049720451557256}, {"code": "def checksum(self, name: str) -> str\n\t\n    url = f\"{self.host}/{self.repository}/{self.release}/{self.checksum_index}\"\n    response = httpx.get(url)\n    response.raise_for_status()\n    checksum_index = response.json()\n    if name not in checksum_index:\n        raise ValueError(f\"Model {name} not found in checksum index.\")\n    return checksum_index[name]\n\n", "CodeBleu_score": 35.12523967609851}]}
{"instance_id": "9312c8b6032e", "generation_result": [{"code": "def download(self, name: str, path: str) -> None:\n\t\n    # construct the download URL\n    url = f\"{self.url}/{name}\"\n    # log the download initiation\n    logger.info(f\"Downloading model archive from {url}\")\n    # stream the file using HTTP/2\n    with httpx.Client(http2=True) as client:\n        with client.stream(\"GET\", url) as response:\n            # validate the response\n            response.raise_for_status()\n            # create a temporary file\n            with NamedTemporaryFile() as file:\n                # log the temporary file creation\n                logger.info(f\"Created temporary file {file.name}\")\n                # write the file in chunks\n                for chunk in response.iter_raw():\n                    file.write(chunk)\n                # log the download completion\n                logger.info(f\"Downloaded model archive to {file.name}\")\n                # compute the file's checksum\n                checksum = compute_file_checksum(file.name)\n                # log the checksum computation\n                logger.info(f\"Computed checksum {checksum}\")\n                # validate the file's checksum\n                if checksum != self.checksum:\n                    # log the checksum mismatch\n                    logger.error(f\"Checksum mismatch: {checksum} != {self.checksum}\")\n                    # raise an exception\n                    raise IOError(\"Checksum mismatch\")\n                # extract the archive to the specified path\n                with tarfile.open(file.name) as tar:\n                    tar.extractall(path)\n                # log the extraction completion\n                logger.info(f\"Extracted model archive to {path}\")\n                # delete the temporary file\n                os.unlink(file.name)\n                # log the temporary file deletion\n                logger.info(f\"Deleted temporary file {file.name}\")\n", "CodeBleu_score": 48.33803556132541}, {"code": "def download(self, name: str, path: str) -> None:\n\t\n\n    url = f\"{self.base_url}/{name}/{name}.tar.gz\"\n    logger.info(f\"Downloading model archive from {url}\")\n    with NamedTemporaryFile() as f:\n        with httpx.Client() as client:\n            with client.stream(\"GET\", url) as response:\n                response.raise_for_status()\n                for chunk in response.iter_raw():\n                    f.write(chunk)\n        logger.info(f\"Model archive downloaded to {f.name}\")\n        f.seek(0)\n        with tarfile.open(f.name, \"r:gz\") as tar:\n            tar.extractall(path)\n        logger.info(f\"Model archive extracted to {path}\")\n        try:\n            checksum = self.checksum(f.name)\n            if checksum != self.checksum(path):\n                raise IOError(\"Checksums do not match\")\n        except IOError as e:\n            logger.error(f\"Error while validating model archive: {e}\")\n            os.unlink(path)\n            raise e\n        finally:\n            os.unlink(f.name)\n\n", "CodeBleu_score": 47.93659513954961}, {"code": "def download(self, name: str, path: str) -> None:\n\t\n\n    url = f\"{self.base_url}/{name}\"\n    logger.info(f\"Downloading {name} from {url}\")\n    with NamedTemporaryFile() as f:\n        with httpx.Client() as client:\n            with client.stream(\"GET\", url) as response:\n                response.raise_for_status()\n                for chunk in response.iter_raw():\n                    f.write(chunk)\n        f.seek(0)\n        self.checksum(f, name)\n        with tarfile.open(f.name) as tar:\n            tar.extractall(path)\n        os.unlink(f.name)\n        logger.info(f\"Extracted {name} to {path}\")\n\n", "CodeBleu_score": 31.329703958207734}]}
{"instance_id": "ee9493f37d77", "generation_result": [{"code": "def load_configuration(descriptor: str) -> Dict\n\t\n    if descriptor.startswith(\"embedded://\"):\n        name = descriptor[len(\"embedded://\") :]\n        if loader.is_resource(name):\n            with loader.open_text(name) as f:\n                return json.load(f)\n        else:\n            raise SpleeterError(\n                f\"Embedded configuration '{name}' not found. Please check the name.\"\n            )\n    else:\n        with open(descriptor) as f:\n            return json.load(f)\n", "CodeBleu_score": 41.45325663610766}, {"code": "def load_configuration(descriptor: str) -> Dict\n\t\n    if descriptor.startswith(\"embedded://\"):\n        name = descriptor[len(\"embedded://\") :]\n        if loader.is_resource(name):\n            with loader.open_text(name) as f:\n                return json.load(f)\n        else:\n            raise SpleeterError(f\"Embedded configuration '{name}' not found.\")\n    else:\n        if os.path.isfile(descriptor):\n            with open(descriptor) as f:\n                return json.load(f)\n        else:\n            raise SpleeterError(f\"Configuration file '{descriptor}' not found.\")\n", "CodeBleu_score": 44.02196143810475}, {"code": "def load_configuration(descriptor: str) -> Dict\n\t\n    if descriptor.startswith('embedded://'):\n        config_name = descriptor[12:]\n        if not loader.is_resource(config_name):\n            raise SpleeterError(f'Embedded configuration \"{config_name}\" not found.')\n        with loader.open_text(config_name) as f:\n            return json.load(f)\n    else:\n        if not os.path.exists(descriptor):\n            raise SpleeterError(f'Configuration file \"{descriptor}\" not found.')\n        with open(descriptor) as f:\n            return json.load(f)\n", "CodeBleu_score": 44.94943155331879}]}
{"instance_id": "546134b67fbc", "generation_result": [{"code": "def sync_apply(\n    tensor_dict: Dict[str, tf.Tensor], func: Callable, concat_axis: int = 1\n) -> Dict[str, tf.Tensor]\n\t\n    # check if the concatenation axis is either 0 or 1\n    if concat_axis not in [0, 1]:\n        raise NotImplementedError(\n            f\"The concatenation axis must be either 0 or 1, but got {concat_axis}.\"\n        )\n    # concatenate the tensors along the specified axis\n    concat_tensor = tf.concat(list(tensor_dict.values()), axis=concat_axis)\n    # apply the function to the concatenated tensor\n    processed_concat_tensor = func(concat_tensor)\n    # split the processed tensor back into the original dictionary format based on the concatenation axis\n    processed_tensor_dict = {}\n    for key, tensor in tensor_dict.items():\n        if concat_axis == 0:\n            processed_tensor_dict[key] = tf.expand_dims(\n                processed_concat_tensor[\n                    : tf.shape(tensor)[0], : tf.shape(tensor)[1]\n                ],\n                axis=0,\n            )\n        elif concat_axis == 1:\n            processed_tensor_dict[key] = tf.expand_dims(\n                processed_concat_tensor[:, : tf.shape(tensor)[1]], axis=0\n            )\n    return processed_tensor_dict\n", "CodeBleu_score": 45.631104527995994}, {"code": "def sync_apply(\n    tensor_dict: Dict[str, tf.Tensor], func: Callable, concat_axis: int = 1\n) -> Dict[str, tf.Tensor]\n\t\n    if concat_axis not in [0, 1]:\n        raise NotImplementedError(\n            f\"The function only supports concatenation along axis 0 or 1. The given axis is {concat_axis}.\"\n        )\n    if concat_axis == 0:\n        return {\n            key: func(\n                tf.concat(\n                    [tensor_dict[key] for key in tensor_dict], axis=concat_axis\n                )\n            )\n            for key in tensor_dict\n        }\n    else:\n        return {\n            key: tf.split(\n                func(tf.concat([tensor_dict[key] for key in tensor_dict], axis=1)),\n                [tf.shape(tensor_dict[key])[0] for key in tensor_dict],\n                axis=0,\n            )\n            for key in tensor_dict\n        }\n\n", "CodeBleu_score": 32.076279844466285}, {"code": "def sync_apply(\n    tensor_dict: Dict[str, tf.Tensor], func: Callable, concat_axis: int = 1\n) -> Dict[str, tf.Tensor]\n\t\n    if concat_axis not in [0, 1]:\n        raise NotImplementedError(\n            f\"Concatenation axis must be either 0 or 1, but got {concat_axis}.\"\n        )\n    if not tensor_dict:\n        return {}\n    concat_tensor = tf.concat(list(tensor_dict.values()), axis=concat_axis)\n    processed_tensor = func(concat_tensor)\n    split_tensors = tf.split(processed_tensor, num_or_size_splits=len(tensor_dict))\n    processed_tensors = dict(zip(tensor_dict.keys(), split_tensors))\n    return processed_tensors\n\n", "CodeBleu_score": 40.56679053752547}]}
{"instance_id": "df48e78d46d2", "generation_result": [{"code": "def from_float32_to_uint8(\n    tensor: tf.Tensor,\n    tensor_key: str = \"tensor\",\n    min_key: str = \"min\",\n    max_key: str = \"max\",\n) -> tf.Tensor\n\t\n    tensor_min = tf.reduce_min(tensor)\n    tensor_max = tf.reduce_max(tensor)\n    tensor_scaled = tf.cast(\n        tf.cast(tensor - tensor_min, tf.float32)\n        / (tensor_max - tensor_min)\n        * 255,\n        tf.uint8,\n    )\n    return {\n        tensor_key: tensor_scaled,\n        min_key: tensor_min,\n        max_key: tensor_max,\n    }\n\n", "CodeBleu_score": 57.94226317732535}, {"code": "def from_float32_to_uint8(\n    tensor: tf.Tensor,\n    tensor_key: str = \"tensor\",\n    min_key: str = \"min\",\n    max_key: str = \"max\",\n) -> tf.Tensor\n\t\n\n    tensor_min = tf.reduce_min(tensor)\n    tensor_max = tf.reduce_max(tensor)\n\n    tensor = (tensor - tensor_min) / (tensor_max - tensor_min)\n    tensor = tf.cast(tensor * 255, tf.uint8)\n\n    return {\n        tensor_key: tensor,\n        min_key: tensor_min,\n        max_key: tensor_max,\n    }\n\n", "CodeBleu_score": 65.86121634362033}, {"code": "def from_float32_to_uint8(\n    tensor: tf.Tensor,\n    tensor_key: str = \"tensor\",\n    min_key: str = \"min\",\n    max_key: str = \"max\",\n) -> tf.Tensor\n\t\n\n    tensor = tf.convert_to_tensor(tensor)\n    tensor_min = tf.reduce_min(tensor)\n    tensor_max = tf.reduce_max(tensor)\n    tensor_scaled = tf.cast(\n        (tensor - tensor_min) * 255 / (tensor_max - tensor_min), tf.uint8\n    )\n    return {\n        tensor_key: tensor_scaled,\n        min_key: tensor_min,\n        max_key: tensor_max,\n    }\n\n", "CodeBleu_score": 56.84299148602497}]}
{"instance_id": "7f479e5de492", "generation_result": [{"code": "def pad_and_partition(tensor: tf.Tensor, segment_len: int) -> tf.Tensor\n\t\n    tensor_len = tf.shape(tensor)[0]\n    pad_len = segment_len - floormod(tensor_len, segment_len)\n    padded_tensor = tf.pad(tensor, [[0, pad_len], [0, 0]])\n    partitioned_tensor = tf.reshape(padded_tensor, [-1, segment_len])\n    return partitioned_tensor\n", "CodeBleu_score": 23.27930104712058}, {"code": "def pad_and_partition(tensor: tf.Tensor, segment_len: int) -> tf.Tensor\n\t\n    tensor_len = tf.shape(tensor)[0]\n    padding_len = tf.floormod(segment_len - tf.floormod(tensor_len, segment_len), segment_len)\n    padded_tensor = tf.pad(tensor, [[0, padding_len]])\n    return tf.reshape(padded_tensor, [tf.math.floordiv(tf.shape(padded_tensor)[0], segment_len), segment_len] +\n                                     tensor.shape[1:])\n\n", "CodeBleu_score": 33.54918685056767}, {"code": "def pad_and_partition(tensor: tf.Tensor, segment_len: int) -> tf.Tensor\n\t\n    padding_len = segment_len - len(tensor) % segment_len\n    padded_tensor = tf.pad(tensor, [[0, padding_len]])\n    return tf.reshape(padded_tensor, [-1, segment_len])\n\n", "CodeBleu_score": 16.631218152883232}]}
{"instance_id": "1789fae7253f", "generation_result": [{"code": "def pad_and_reshape(instr_spec, frame_length, F) -> Any\n\t\n    num_rows = tf.shape(instr_spec)[0]\n    num_cols = tf.shape(instr_spec)[1]\n    num_rows_to_pad = frame_length - num_rows % frame_length\n    num_rows_to_pad = tf.cond(\n        tf.equal(num_rows_to_pad, frame_length),\n        lambda: tf.constant(0),\n        lambda: num_rows_to_pad,\n    )\n    paddings = tf.constant([[0, num_rows_to_pad], [0, 0]])\n    instr_spec_extended = tf.pad(instr_spec, paddings)\n    instr_spec_extended = tf.reshape(\n        instr_spec_extended, [num_rows + num_rows_to_pad, num_cols]\n    )\n    instr_spec_extended = tf.concat(\n        [instr_spec_extended, tf.zeros([num_rows_to_pad, num_cols])], axis=0\n    )\n    instr_spec_extended = tf.reshape(\n        instr_spec_extended, [num_rows + num_rows_to_pad, F, frame_length]\n    )\n    instr_spec_extended = tf.transpose(instr_spec_extended, [0, 2, 1])\n    return instr_spec_extended\n\n", "CodeBleu_score": 19.629427368382903}, {"code": "def pad_and_reshape(instr_spec, frame_length, F) -> Any\n\t\n    # Calculate the number of rows needed for padding\n    num_rows = int(frame_length - tf.shape(instr_spec)[0] % frame_length)\n\n    # Create an extension of zeros\n    extension = tf.zeros([num_rows, tf.shape(instr_spec)[1]], dtype=tf.float32)\n\n    # Concatenate the extension to the original spectrogram along the frequency axis\n    padded_spec = tf.concat([instr_spec, extension], 0)\n\n    # Reshape the extended spectrogram by merging the first two dimensions\n    padded_spec = tf.reshape(padded_spec, [-1, F])\n\n    return padded_spec\n\n", "CodeBleu_score": 17.269855417008298}, {"code": "def pad_and_reshape(instr_spec, frame_length, F) -> Any\n\t\n    num_rows = tf.shape(instr_spec)[0]\n    num_cols = tf.shape(instr_spec)[1]\n    num_frames = tf.cast(tf.math.ceil(num_cols / frame_length), tf.int32)\n    num_extra_rows = (num_frames * frame_length) - num_cols\n    extra_rows = tf.zeros(shape=(num_extra_rows, num_cols), dtype=tf.float32)\n    padded_instr_spec = tf.concat([instr_spec, extra_rows], axis=0)\n    padded_instr_spec = tf.reshape(padded_instr_spec, (num_frames, frame_length, F))\n    return padded_instr_spec\n\n", "CodeBleu_score": 21.848599031967115}]}
{"instance_id": "8a4f4658d97c", "generation_result": [{"code": "def dataset_from_csv(csv_path: str, **kwargs) -> Any\n\t\n    df = pd.read_csv(csv_path, **kwargs)\n    return tf.data.Dataset.from_tensor_slices(dict(df))\n\n", "CodeBleu_score": 27.127655353502654}, {"code": "def dataset_from_csv(csv_path: str, **kwargs) -> Any\n\t\n    df = pd.read_csv(csv_path, **kwargs)\n    return tf.data.Dataset.from_tensor_slices(dict(df))\n\n", "CodeBleu_score": 27.127655353502654}, {"code": "def dataset_from_csv(csv_path: str, **kwargs) -> Any\n\t\n    df = pd.read_csv(csv_path, **kwargs)\n    return tf.data.Dataset.from_tensor_slices(dict(df))\n", "CodeBleu_score": 27.127655353502654}]}
{"instance_id": "9a16e81b182f", "generation_result": [{"code": "def check_tensor_shape(tensor_tf: tf.Tensor, target_shape: Any) -> bool\n\t\n    result = True\n    for i, target_dim in enumerate(target_shape):\n        if target_dim is not None:\n            result = result and (tf.shape(tensor_tf)[i] == target_dim)\n    return result\n\n", "CodeBleu_score": 36.71137604849451}, {"code": "def check_tensor_shape(tensor_tf: tf.Tensor, target_shape: Any) -> bool\n\t\n    result = True\n    for i, target_length in enumerate(target_shape):\n        result = result and (tf.shape(tensor_tf)[i] == target_length)\n    return result\n\n", "CodeBleu_score": 38.77944358320977}, {"code": "def check_tensor_shape(tensor_tf: tf.Tensor, target_shape: Any) -> bool\n\t\n    result = True\n    for i, target_dim in enumerate(target_shape):\n        if target_dim is not None:\n            result = result and (tf.shape(tensor_tf)[i] == target_dim)\n    return result\n\n", "CodeBleu_score": 36.71137604849451}]}
{"instance_id": "49d5d3084b3a", "generation_result": [{"code": "def get_training_dataset(\n    audio_params: Dict, audio_adapter: AudioAdapter, audio_path: str\n) -> Any\n\t\n    builder = DatasetBuilder(\n        audio_params=audio_params,\n        audio_adapter=audio_adapter,\n        audio_path=audio_path,\n        chunk_duration=30,\n        seed=1234,\n    )\n    return builder.build(\n        train_csv=\"data/train_data.csv\",\n        cache_dir=\"data/cache\",\n        batch_size=2,\n        num_workers=0,\n        num_chunks_per_song=5,\n        augment_audio_parameters={\n            \"time_stretch_ratio\": 0.2,\n            \"noise_factor\": 0.005,\n        },\n        should_augment=True,\n        should_cache=True,\n    )\n\n", "CodeBleu_score": 31.67482274638374}, {"code": "def get_training_dataset(\n    audio_params: Dict, audio_adapter: AudioAdapter, audio_path: str\n) -> Any\n\t\n    builder = DatasetBuilder(\n        audio_params=audio_params,\n        audio_adapter=audio_adapter,\n        audio_path=audio_path,\n        chunk_duration=60,\n        seed=42,\n    )\n    dataset = builder.build(\n        train_csv=\"data/train.csv\",\n        cache_dir=\"data/cache\",\n        batch_size=32,\n        n_chunks_per_song=2,\n        augment=True,\n        cache_files=True,\n    )\n    return dataset\n\n", "CodeBleu_score": 29.561689917599516}, {"code": "def get_training_dataset(\n    audio_params: Dict, audio_adapter: AudioAdapter, audio_path: str\n) -> Any\n\t\n    builder = DatasetBuilder(\n        audio_params=audio_params,\n        audio_adapter=audio_adapter,\n        audio_path=audio_path,\n        chunk_duration=CHUNK_DURATION,\n        seed=SEED,\n    )\n    dataset = builder.build(\n        train_csv=TRAIN_CSV,\n        cache_dir=CACHE_DIR,\n        batch_size=BATCH_SIZE,\n        num_chunks_per_song=NUM_CHUNKS_PER_SONG,\n        augment_audio_parameters=AUGMENT_AUDIO_PARAMETERS,\n        cache_audio=CACHE_AUDIO,\n        cache_labels=CACHE_LABELS,\n        num_workers=NUM_WORKERS,\n    )\n    return dataset\n\n", "CodeBleu_score": 27.600895233948442}]}
{"instance_id": "fe8bbbba06e3", "generation_result": [{"code": "def instruments(self) -> Any\n\t\n    if self.instruments is None:\n        self.instruments = []\n        for instrument in self.metadata.instruments:\n            self.instruments.append(\n                InstrumentDatasetBuilder(\n                    self.metadata,\n                    instrument,\n                    self.dataset_id,\n                    self.client,\n                    self.bucket_name,\n                    self.s3_prefix,\n                )\n            )\n    for instrument in self.instruments:\n        yield instrument\n\n", "CodeBleu_score": 44.22663882520335}, {"code": "def instruments(self) -> Any\n\t\n    if self._instruments is None:\n        self._instruments = []\n        for instrument in self.data.instruments:\n            self._instruments.append(\n                InstrumentDatasetBuilder(self, instrument)\n            )\n    for instrument in self._instruments:\n        yield instrument\n", "CodeBleu_score": 61.455578279834846}, {"code": "def instruments(self) -> Any\n\t\n    if not self._instrument_builders:\n        for instrument in self._instruments:\n            self._instrument_builders.append(\n                InstrumentDatasetBuilder(instrument, self._start_date, self._end_date)\n            )\n    for builder in self._instrument_builders:\n        yield builder\n\n", "CodeBleu_score": 52.80251466171405}]}
{"instance_id": "438b57fd3413", "generation_result": [{"code": "def cache(self, dataset: Any, cache: Optional[str], wait: bool) -> Any\n\t\n    if cache is None:\n        return dataset\n    if wait:\n        while not os.path.exists(cache):\n            logger.info(f\"Waiting for cache to be created at {cache}\")\n            time.sleep(10)\n    return dataset.cache(cache)\n\n", "CodeBleu_score": 43.05207512842675}, {"code": "def cache(self, dataset: Any, cache: Optional[str], wait: bool) -> Any\n\t\n    if cache is None:\n        return dataset\n    if wait:\n        while not os.path.exists(f\"{cache}/index.html\"):\n            logger.info(f\"Waiting for cache to be created at {cache}\")\n            time.sleep(10)\n    os.makedirs(cache, exist_ok=True)\n    return dataset.cache(cache)\n", "CodeBleu_score": 49.149396114874534}, {"code": "def cache(self, dataset: Any, cache: Optional[str], wait: bool) -> Any\n\t\n    if cache:\n        if wait:\n            while not os.path.exists(os.path.join(cache, \"index.json\")):\n                logger.info(f\"Waiting for cache at {cache}\")\n                time.sleep(10)\n        dataset = dataset.cache(cache)\n    return dataset\n\n", "CodeBleu_score": 47.43874465021592}]}
{"instance_id": "8838a3a09a66", "generation_result": [{"code": "def create_estimator(params: Dict, MWF: bool) -> tf.Tensor\n\t\n    model_provider = ModelProvider.default()\n    model_dir = model_provider.get(params[\"model_dir\"], MWF)\n    params[\"model_dir\"] = model_dir\n    config = ConfigProto()\n    config.gpu_options.per_process_gpu_memory_fraction = 0.9\n    config.gpu_options.allow_growth = True\n    run_config = RunConfig(session_config=config)\n    estimator = tf.estimator.Estimator(model_fn=model_fn, model_dir=model_dir, params=params, config=run_config)\n    return estimator\n\n", "CodeBleu_score": 37.535874314538454}, {"code": "def create_estimator(params: Dict, MWF: bool) -> tf.Tensor\n\t\n    model_dir = ModelProvider.default(params[\"model_name\"])\n    if MWF:\n        params[\"model_dir\"] = model_dir\n    else:\n        params[\"model_dir\"] = os.path.join(model_dir, \"model\")\n    session_config = ConfigProto()\n    session_config.gpu_options.allow_growth = True\n    run_config = RunConfig(session_config=session_config)\n    estimator = tf.estimator.Estimator(\n        model_fn=ModelProvider.default(params[\"model_name\"]),\n        params=params,\n        model_dir=params[\"model_dir\"],\n        config=run_config,\n    )\n    return estimator\n\n", "CodeBleu_score": 50.02892638396569}, {"code": "def create_estimator(params: Dict, MWF: bool) -> tf.Tensor\n\t\n    model_dir = ModelProvider.default(params[\"model_dir\"], MWF)\n    params.update(model_dir=model_dir)\n    config = tf.ConfigProto()\n    config.gpu_options.per_process_gpu_memory_fraction = 0.9\n    run_config = tf.estimator.RunConfig(session_config=config)\n    estimator = tf.estimator.Estimator(model_fn=model_fn, params=params, config=run_config)\n    return estimator\n\n", "CodeBleu_score": 33.481822419241745}]}
{"instance_id": "779c812bf2cb", "generation_result": [{"code": "def _separate_tensorflow(\n        self, waveform: np.ndarray, audio_descriptor: AudioDescriptor\n    ) -> Dict\n\t\n\n    # Ensure the waveform is stereo.\n    waveform = to_stereo(waveform)\n\n    # Obtain a prediction generator.\n    prediction_generator = self._get_prediction_generator()\n\n    # Update the data generator with the waveform and audio descriptor.\n    update_data(prediction_generator, waveform, audio_descriptor)\n\n    # Generate a prediction.\n    prediction = next(prediction_generator)\n\n    # Remove the \"audio_id\" key from the prediction.\n    prediction.pop(\"audio_id\")\n\n    return prediction\n\n", "CodeBleu_score": 48.217860011730664}, {"code": "def _separate_tensorflow(\n        self, waveform: np.ndarray, audio_descriptor: AudioDescriptor\n    ) -> Dict\n\t\n    waveform = to_stereo(waveform)\n    prediction_generator = self._get_prediction_generator(audio_descriptor)\n    self.update_data(prediction_generator, waveform)\n    prediction = next(prediction_generator)\n    prediction.pop(\"audio_id\")\n    return prediction\n\n", "CodeBleu_score": 46.6433656126235}, {"code": "def _separate_tensorflow(\n        self, waveform: np.ndarray, audio_descriptor: AudioDescriptor\n    ) -> Dict\n\t\n    waveform = self._to_stereo(waveform)\n    prediction_generator = self._get_prediction_generator()\n    self._update_data(prediction_generator, waveform, audio_descriptor)\n    prediction = next(prediction_generator)\n    prediction.pop(\"audio_id\")\n    return prediction\n\n", "CodeBleu_score": 46.36384098976002}]}
{"instance_id": "cf580cd2ed93", "generation_result": [{"code": "def compute_backbone_shapes(config, image_shape)\n\t\n    if callable(config.BACKBONE):\n        return config.COMPUTE_BACKBONE_SHAPE(image_shape)\n    else:\n        assert config.BACKBONE in [\"resnet50\", \"resnet101\"]\n        return np.array(\n            [[int(math.ceil(image_shape[0] / stride)),\n              int(math.ceil(image_shape[1] / stride))]\n             for stride in config.BACKBONE_STRIDES])\n\n", "CodeBleu_score": 89.40433498562015}, {"code": "def compute_backbone_shapes(config, image_shape)\n\t\n    if callable(config.BACKBONE):\n        return config.COMPUTE_BACKBONE_SHAPE(image_shape)\n    else:\n        assert config.BACKBONE in [\"resnet50\", \"resnet101\"]\n        return np.array(\n            [[int(math.ceil(image_shape[0] / stride)),\n              int(math.ceil(image_shape[1] / stride))]\n             for stride in config.BACKBONE_STRIDES])\n", "CodeBleu_score": 89.40433498562015}, {"code": "def compute_backbone_shapes(config, image_shape)\n\t\n    if callable(config.BACKBONE):\n        return config.COMPUTE_BACKBONE_SHAPE(image_shape)\n    else:\n        assert config.BACKBONE in [\"resnet50\", \"resnet101\"]\n        return np.array(\n            [[int(math.ceil(image_shape[0] / stride)),\n              int(math.ceil(image_shape[1] / stride))]\n             for stride in config.BACKBONE_STRIDES])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 89.40433498562015}]}
{"instance_id": "ebd61a2e59ca", "generation_result": [{"code": "def clip_boxes_graph(boxes, window)\n\t\n    boxes = tf.to_float(boxes)\n    # Split boxes into individual coordinates\n    y1, x1, y2, x2 = tf.unstack(boxes, axis=1)\n    # Split window into individual coordinates\n    wy1, wx1, wy2, wx2 = tf.unstack(window, axis=1)\n    # Clip y1 and x1 to the window boundaries\n    y1 = tf.maximum(tf.minimum(y1, wy2), wy1)\n    x1 = tf.maximum(tf.minimum(x1, wx2), wx1)\n    # Clip y2 and x2 to the window boundaries\n    y2 = tf.maximum(tf.minimum(y2, wy2), wy1)\n    x2 = tf.maximum(tf.minimum(x2, wx2), wx1)\n    # Re-stack the clipped coordinates into a new tensor\n    clipped = tf.stack([y1, x1, y2, x2], axis=1, name=\"clipped_boxes\")\n    # Give it a fixed shape\n    clipped.set_shape(boxes.get_shape())\n    return clipped\n\n", "CodeBleu_score": 49.03699495800996}, {"code": "def clip_boxes_graph(boxes, window)\n\t\n    # split the window and box coordinates into individual components\n    wy1, wy2 = tf.split(window, 2, axis=1)\n    wx1, wx2 = tf.split(wy1, 2, axis=1)\n    bz1, bz2 = tf.split(boxes, 2, axis=1)\n    by1, by2 = tf.split(bz1, 2, axis=1)\n    bx1, bx2 = tf.split(by1, 2, axis=1)\n    # clip the coordinates\n    cy1 = tf.maximum(tf.minimum(by1, wy2), wy1)\n    cy2 = tf.maximum(tf.minimum(by2, wy2), wy1)\n    cx1 = tf.maximum(tf.minimum(bx1, wx2), wx1)\n    cx2 = tf.maximum(tf.minimum(bx2, wx2), wx1)\n    # concatenate the coordinates back together\n    clip_boxes = tf.concat([cx1, cy1, cx2, cy2], axis=1)\n    # clip_boxes.set_shape([boxes.shape[0], 4])\n    return clip_boxes\n\n", "CodeBleu_score": 22.16359441486558}, {"code": "def clip_boxes_graph(boxes, window)\n\t\n    \"\"\"\n    Args:\n      boxes: A tensor of shape [num_boxes, 4] representing bounding boxes.\n        Each bounding box is of the format [y_min, x_min, y_max, x_max]\n      window: A tensor of shape [4] representing the [y_min, x_min, y_max, x_max] window to clip to.\n    Returns:\n      clipped_boxes: A tensor of shape [num_boxes, 4] representing the clipped bounding boxes.\n    \"\"\"\n    # Split the boxes and the window into individual components\n    y_min, x_min, y_max, x_max = tf.unstack(boxes, axis=1)\n    win_y_min, win_x_min, win_y_max, win_x_max = tf.unstack(window, axis=0)\n\n    # Clip the boxes to fit within the window\n    y_min = tf.maximum(tf.minimum(y_min, win_y_max), win_y_min)\n    x_min = tf.maximum(tf.minimum(x_min, win_x_max), win_x_min)\n    y_max = tf.maximum(tf.minimum(y_max, win_y_max), win_y_min)\n    x_max = tf.maximum(tf.minimum(x_max, win_x_max), win_x_min)\n\n    # Concatenate the clipped coordinates back into a tensor of boxes\n    clipped_boxes = tf.stack([y_min, x_min, y_max, x_max], axis=1, name=\"clipped_boxes\")\n    clipped_boxes.set_shape(boxes.get_shape())\n    return clipped_boxes\n\n", "CodeBleu_score": 24.228857483179077}]}
{"instance_id": "8e0411587a70", "generation_result": [{"code": "def rpn_graph(feature_map, anchors_per_location, anchor_stride)\n\t\n    # Shared convolutional layer\n    shared = KL.Conv2D(512, (3, 3), padding='same', activation='relu', kernel_initializer='normal', name='rpn_conv_shared')(feature_map)\n\n    # Class scores\n    x = KL.Conv2D(2 * anchors_per_location, (1, 1), padding='valid', activation='linear', kernel_initializer='zero', name='rpn_out_class')(shared)\n\n    # Reshape class scores to one dimension\n    m = KL.Lambda(lambda t: tf.reshape(t, [tf.shape(t)[0], -1, 2]), output_shape=lambda s: [s[0], -1, 2], name='rpn_class_logits')(x)\n\n    # Softmax activation\n    n = KL.Activation(lambda x: tf.nn.softmax(x, axis=-1), name='rpn_out_class_softmax')(m)\n\n    # Bounding box predictions\n    r = KL.Conv2D(4 * anchors_per_location, (1, 1), padding='valid', activation='linear', kernel_initializer='zero', name='rpn_out_regress')(shared)\n\n    # Reshape bounding box predictions to one dimension\n    s = KL.Lambda(lambda t: tf.reshape(t, [tf.shape(t)[0], -1, 4]), output_shape=lambda s: [s[0], -1, 4], name='rpn_bbox_pred')(r)\n\n    return m, n, s\n\n", "CodeBleu_score": 59.55092486041658}, {"code": "def rpn_graph(feature_map, anchors_per_location, anchor_stride)\n\t\n\n    shared = KL.Conv2D(512, (3, 3), padding=\"same\", activation=\"relu\",\n                       strides=anchor_stride,\n                       name=\"rpn_conv_shared\")(feature_map)\n\n    x = KL.Conv2D(2 * anchors_per_location, (1, 1), padding=\"valid\",\n                  activation=\"linear\", name=\"rpn_class_raw\")(shared)\n    x = KL.Lambda(lambda t: tf.reshape(t, (-1, 2)))(x)\n    x = KL.Activation(activation=\"softmax\", name=\"rpn_class_xxx\")(x)\n    rpn_class_logits = x\n\n    x = KL.Conv2D(anchors_per_location * 4, (1, 1), padding=\"valid\",\n                  activation=\"linear\", name=\"rpn_regr_raw\")(shared)\n    x = KL.Lambda(lambda t: tf.reshape(t, (-1, 4)))(x)\n    rpn_bbox = x\n\n    return [rpn_class_logits, rpn_bbox]\n\n", "CodeBleu_score": 55.3904064875157}, {"code": "def rpn_graph(feature_map, anchors_per_location, anchor_stride)\n\t\n    shared = KL.Conv2D(512, (3, 3), padding='same', activation='relu', strides=(1, 1), name='rpn_conv_shared')(feature_map)\n\n    x = KL.Conv2D(2 * anchors_per_location, (1, 1), padding='valid', activation='linear', name='rpn_out_class')(shared)\n    x = KL.Lambda(lambda t: tf.reshape(t, (-1, 2)))(x)\n    x = KL.Activation('softmax', name='rpn_out_class_logits')(x)\n\n    x = KL.Conv2D(4 * anchors_per_location, (1, 1), padding='valid', activation='linear', name='rpn_out_regress')(shared)\n    x = KL.Lambda(lambda t: tf.reshape(t, (-1, 4)))(x)\n\n    return x, x\n\n", "CodeBleu_score": 48.66623883715964}]}
{"instance_id": "70777c9261d8", "generation_result": [{"code": "def smooth_l1_loss(y_true, y_pred)\n\t\n    diff = K.abs(y_true - y_pred)\n    less_than_one = K.cast(K.less(diff, 1.0), \"float32\")\n    loss = (less_than_one * 0.5 * diff ** 2) + (1 - less_than_one) * (diff - 0.5)\n    return loss\n", "CodeBleu_score": 58.60419868806381}, {"code": "def smooth_l1_loss(y_true, y_pred)\n\t\n    diff = K.abs(y_true - y_pred)\n    less_than_one = K.cast(K.less(diff, 1.0), \"float32\")\n    loss = (less_than_one * 0.5 * diff ** 2) + (1 - less_than_one) * (diff - 0.5)\n    return loss\n\n", "CodeBleu_score": 58.60419868806381}, {"code": "def smooth_l1_loss(y_true, y_pred)\n\t\n    diff = K.abs(y_true - y_pred)\n    less_than_one = K.cast(K.less(diff, 1.0), \"float32\")\n    loss = (less_than_one * 0.5 * K.square(diff)) + (1 - less_than_one) * (diff - 0.5)\n    return loss\n\n", "CodeBleu_score": 57.7344668751119}]}
{"instance_id": "0e747b604087", "generation_result": [{"code": "def build_model(x_train, num_classes)\n\t\n    # reset the TensorFlow graph\n    tf.reset_default_graph()\n\n    # define the input layer\n    x = KL.Input(shape=x_train.shape[1:])\n\n    # define the convolutional layers\n    x = KL.Conv2D(32, (3, 3), activation='relu')(x)\n    x = KL.Conv2D(64, (3, 3), activation='relu')(x)\n\n    # define the max-pooling layer\n    x = KL.MaxPooling2D(pool_size=(2, 2))(x)\n\n    # flatten the output of the convolutional layers\n    x = KL.Flatten()(x)\n\n    # define the dense layers\n    x = KL.Dense(128, activation='relu')(x)\n    x = KL.Dense(num_classes, activation='softmax')(x)\n\n    # create the model\n    model = KM.Model(inputs=x, outputs=x)\n\n    return model\n\n", "CodeBleu_score": 33.80031426760964}, {"code": "def build_model(x_train, num_classes)\n\t\n    # Reset the TensorFlow graph\n    tf.reset_default_graph()\n\n    # Define the input layer\n    input_layer = KL.Input(shape=x_train.shape[1:])\n\n    # Define the first convolutional layer\n    conv1 = KL.Conv2D(32, (3, 3), activation='relu', padding='same')(input_layer)\n\n    # Define the second convolutional layer\n    conv2 = KL.Conv2D(32, (3, 3), activation='relu', padding='same')(conv1)\n\n    # Define the max-pooling layer\n    pool1 = KL.MaxPooling2D(pool_size=(2, 2))(conv2)\n\n    # Flatten the output of the max-pooling layer\n    flat1 = KL.Flatten()(pool1)\n\n    # Define the first dense layer\n    dense1 = KL.Dense(128, activation='relu')(flat1)\n\n    # Define the second dense layer\n    dense2 = KL.Dense(num_classes, activation='softmax')(dense1)\n\n    # Create the model\n    model = KM.Model(inputs=input_layer, outputs=dense2)\n\n    # Compile the model\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n    # Return the compiled model\n    return model\n\n", "CodeBleu_score": 33.46117152036152}, {"code": "def build_model(x_train, num_classes)\n\t\n    tf.reset_default_graph()\n    model = KM.Model()\n    model.add(KL.Input(shape=(x_train.shape[1], x_train.shape[2], x_train.shape[3])))\n    model.add(KL.Conv2D(32, (3, 3), activation='relu'))\n    model.add(KL.Conv2D(64, (3, 3), activation='relu'))\n    model.add(KL.MaxPooling2D(pool_size=(2, 2)))\n    model.add(KL.Flatten())\n    model.add(KL.Dense(128, activation='relu'))\n    model.add(KL.Dense(num_classes, activation='softmax'))\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model\n\n", "CodeBleu_score": 28.432867907861958}]}
{"instance_id": "813e3e2844f4", "generation_result": [{"code": "def extract_bboxes(mask)\n\t\n    # initialize array to store the bboxes\n    bboxes = np.zeros((mask.shape[-1], 4), dtype=np.int32)\n    # loop through the mask layers\n    for i in range(mask.shape[-1]):\n        # get the mask layer\n        m = mask[:, :, i]\n        # get the horizontal and vertical indices where the mask is present\n        horizontal_indices = np.where(np.any(m, axis=0))[0]\n        vertical_indices = np.where(np.any(m, axis=1))[0]\n        # set the bounding box if the mask is present\n        if horizontal_indices.shape[0]:\n            x1, x2 = horizontal_indices[[0, -1]]\n            y1, y2 = vertical_indices[[0, -1]]\n            # store the bounding box\n            bboxes[i] = x1, y1, x2, y2\n    return bboxes.astype(np.int32)\n", "CodeBleu_score": 36.40865094742233}, {"code": "def extract_bboxes(mask)\n\t\n    # Initialize array to store the bounding boxes\n    bboxes = np.zeros((mask.shape[0], 4), dtype=np.int32)\n    # Iterate through the mask layers\n    for i in range(mask.shape[0]):\n        # Find the horizontal and vertical indices where the mask is present\n        horizontal_indices = np.where(np.any(mask[i, :, :], axis=0))[0]\n        vertical_indices = np.where(np.any(mask[i, :, :], axis=1))[0]\n        # If there is no mask, set the bounding box to zeros\n        if horizontal_indices.shape[0] == 0 or vertical_indices.shape[0] == 0:\n            bboxes[i, :] = 0\n        else:\n            # Determine the coordinates of the bounding box\n            x1 = horizontal_indices[0]\n            x2 = horizontal_indices[-1]\n            y1 = vertical_indices[0]\n            y2 = vertical_indices[-1]\n            # Store the bounding box\n            bboxes[i, :] = x1, y1, x2, y2\n    return bboxes.astype(np.int32)", "CodeBleu_score": 30.26251868580969}, {"code": "def extract_bboxes(mask)\n\t\n    # Initialise array to store bboxes\n    bboxes = np.zeros((mask.shape[0], 4), dtype=np.int32)\n\n    # Loop through mask layers\n    for i in range(mask.shape[0]):\n        # Mask for the current layer\n        m = mask[i, :, :]\n\n        # If the mask is not empty\n        if m.max() > 0:\n            # Calculate indices where the mask is present\n            horizontal_indices = np.where(np.any(m, axis=0))[0]\n            vertical_indices = np.where(np.any(m, axis=1))[0]\n\n            # Calculate bounding box coordinates from the indices\n            x1, x2 = horizontal_indices[[0, -1]]\n            y1, y2 = vertical_indices[[0, -1]]\n\n            # Store the bounding box coordinates in the array\n            bboxes[i] = np.array([x1, y1, x2, y2])\n\n    return bboxes.astype(np.int32)\n\n", "CodeBleu_score": 45.677835060179675}]}
{"instance_id": "7ad5d430d6ed", "generation_result": [{"code": "def compute_iou(box, boxes, box_area, boxes_area)\n\t\n    # Calculate the intersection areas\n    y1 = np.maximum(box[0], boxes[:, 0])\n    y2 = np.minimum(box[2], boxes[:, 2])\n    x1 = np.maximum(box[1], boxes[:, 1])\n    x2 = np.minimum(box[3], boxes[:, 3])\n    intersection = np.maximum(y2 - y1, 0) * np.maximum(x2 - x1, 0)\n    # Calculate the union areas\n    union = box_area + boxes_area[:] - intersection[:]\n    # Calculate the IoU\n    iou = intersection / union\n    return iou\n\n", "CodeBleu_score": 56.12768883353916}, {"code": "def compute_iou(box, boxes, box_area, boxes_area)\n\t\n    # Calculate intersection areas\n    y1 = np.maximum(box[0], boxes[:, 0])\n    y2 = np.minimum(box[2], boxes[:, 2])\n    x1 = np.maximum(box[1], boxes[:, 1])\n    x2 = np.minimum(box[3], boxes[:, 3])\n    intersection = np.maximum(0, y2 - y1) * np.maximum(0, x2 - x1)\n    # Calculate union areas\n    union = box_area + boxes_area[:] - intersection[:]\n    # Compute IoU\n    iou = intersection / union\n    return iou\n\n", "CodeBleu_score": 52.87331730308667}, {"code": "def compute_iou(box, boxes, box_area, boxes_area)\n\t\n    # Calculate the intersection areas\n    y1 = np.maximum(box[0], boxes[:, 0])\n    y2 = np.minimum(box[2], boxes[:, 2])\n    x1 = np.maximum(box[1], boxes[:, 1])\n    x2 = np.minimum(box[3], boxes[:, 3])\n    intersection = np.maximum(y2 - y1, 0) * np.maximum(x2 - x1, 0)\n\n    # Calculate the union areas\n    union = box_area + boxes_area[:] - intersection[:]\n\n    # Calculate the IoU\n    iou = intersection / union\n\n    return iou\n\n", "CodeBleu_score": 56.12768883353916}]}
{"instance_id": "15941b852859", "generation_result": [{"code": "def compute_overlaps(boxes1, boxes2)\n\t\n    # Initialize the overlaps matrix\n    n_boxes1 = boxes1.shape[0]\n    n_boxes2 = boxes2.shape[0]\n    overlaps = np.zeros((n_boxes1, n_boxes2))\n    \n    # Calculate the areas of each box in both sets\n    areas1 = boxes1[:,2] * boxes1[:,3]\n    areas2 = boxes2[:,2] * boxes2[:,3]\n    \n    # Iterate through the boxes in the second set\n    for i in range(n_boxes2):\n        # Iterate through the boxes in the first set\n        for j in range(n_boxes1):\n            # Find the coordinates of the intersection\n            x1 = np.maximum(boxes1[j,0], boxes2[i,0])\n            y1 = np.maximum(boxes1[j,1], boxes2[i,1])\n            x2 = np.minimum(boxes1[j,0]+boxes1[j,2], boxes2[i,0]+boxes2[i,2])\n            y2 = np.minimum(boxes1[j,1]+boxes1[j,3], boxes2[i,1]+boxes2[i,3])\n            # Find the area of the intersection\n            w = np.maximum(0, x2 - x1)\n            h = np.maximum(0, y2 - y1)\n            intersection = w * h\n            # Find the area of the union\n            union = areas1[j] + areas2[i] - intersection\n            # Compute the overlap\n            overlaps[j,i] = intersection / union\n    return overlaps\n\n", "CodeBleu_score": 21.739473952656997}, {"code": "def compute_overlaps(boxes1, boxes2)\n\t\n    # initialize the overlaps matrix\n    # The overlaps matrix is a 2D array of shape (boxes1_num, boxes2_num).\n    # Each element in the matrix is the IoU of the corresponding boxes in boxes1 and boxes2.\n    overlaps = np.zeros((boxes1.shape[0], boxes2.shape[0]))\n\n    # for each box in boxes1, compute its overlap with each box in boxes2\n    for i in range(boxes1.shape[0]):\n        # get the coordinates of the ith box in boxes1\n        x1_i, y1_i, x2_i, y2_i = boxes1[i, :]\n\n        # compute the area of the ith box in boxes1\n        area_i = (x2_i - x1_i) * (y2_i - y1_i)\n\n        # for each box in boxes2, compute the intersection with the ith box in boxes1\n        for j in range(boxes2.shape[0]):\n            # get the coordinates of the jth box in boxes2\n            x1_j, y1_j, x2_j, y2_j = boxes2[j, :]\n\n            # compute the area of the jth box in boxes2\n            area_j = (x2_j - x1_j) * (y2_j - y1_j)\n\n            # compute the intersection of the ith box in boxes1 with the jth box in boxes2\n            x1 = np.maximum(x1_i, x1_j)\n            y1 = np.maximum(y1_i, y1_j)\n            x2 = np.minimum(x2_i, x2_j)\n            y2 = np.minimum(y2_i, y2_j)\n\n            # compute the width and height of the intersection\n            w = np.maximum(0, x2 - x1)\n            h = np.maximum(0, y2 - y1)\n\n            # compute the area of the intersection\n            inter_area = w * h\n\n            # compute the overlap between the ith box in", "CodeBleu_score": 21.074838300941543}, {"code": "def compute_overlaps(boxes1, boxes2)\n\t\n    # if there are no boxes, return an empty list\n    if len(boxes1) == 0 or len(boxes2) == 0:\n        return np.zeros((len(boxes1), len(boxes2)))\n\n    # if the boxes are the same, the overlaps are 1\n    if boxes1 == boxes2:\n        return np.ones((len(boxes1), len(boxes2)))\n\n    # initialize the overlaps matrix\n    overlaps = np.zeros((len(boxes1), len(boxes2)))\n\n    # loop through each box in the first set of boxes\n    for i in range(len(boxes1)):\n        # get the coordinates of box1\n        x1_1 = boxes1[i, 0]\n        y1_1 = boxes1[i, 1]\n        x2_1 = boxes1[i, 2]\n        y2_1 = boxes1[i, 3]\n\n        # compute the area of box1\n        area1 = (x2_1 - x1_1 + 1) * (y2_1 - y1_1 + 1)\n\n        # loop through each box in the second set of boxes\n        for j in range(len(boxes2)):\n            # get the coordinates of box2\n            x1_2 = boxes2[j, 0]\n            y1_2 = boxes2[j, 1]\n            x2_2 = boxes2[j, 2]\n            y2_2 = boxes2[j, 3]\n\n            # compute the area of box2\n            area2 = (x2_2 - x1_2 + 1) * (y2_2 - y1_2 + 1)\n\n            # compute the intersection coordinates\n            x1 = max(x1_1, x1_2)\n            y1 = max(y1_1, y1_2)\n            x2 = min(x2_1, x2_2)\n            y2 = min(y2_1, y2_2)\n\n            # compute the intersection area\n            if x1 >= x2 or y1 >= y2:\n", "CodeBleu_score": 18.262547438401416}]}
{"instance_id": "597533722ce3", "generation_result": [{"code": "def load_image(self, image_id):\n\t\n    image_path = self.images[image_id]['path']\n    image = imread(image_path)\n    if image.ndim == 2:\n        image = gray2rgb(image)\n    elif image.shape[2] == 4:\n        image = image[:, :, :3]\n    return image\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 50.40608637004139}, {"code": "def load_image(self, image_id):\n\t\n    image_path = self.image_info[image_id][\"path\"]\n    image = skimage.io.imread(image_path)\n    if image.ndim != 3:\n        image = gray2rgb(image)\n    if image.shape[-1] == 4:\n        image = image[..., :3]\n    return image\n\n", "CodeBleu_score": 77.20823629861854}, {"code": "def load_image(self, image_id):\n\t\n    info = self.image_info[image_id]\n    image = skimage.io.imread(info['path'])\n    image = gray2rgb(image) if(len(image.shape) < 3) else image\n    image = image[:, :, :3]\n    return image\n\n", "CodeBleu_score": 47.846411829878186}]}
{"instance_id": "9b852997ca05", "generation_result": [{"code": "def display_images(images, titles=None, cols=4, cmap=None, norm=None,\n                   interpolation=None)\n\t\n    titles = titles if titles is not None else [\"Image (%d)\" % i\n                                                for i in range(1, len(images) + 1)]\n    rows = len(images) // cols + 1\n    plt.figure(figsize=(14, 14 * rows // cols))\n    i = 1\n    for image, title in zip(images, titles):\n        plt.subplot(rows, cols, i)\n        plt.title(title, fontsize=9)\n        plt.axis('off')\n        plt.imshow(image.astype(np.uint8), cmap=cmap,\n                   norm=norm, interpolation=interpolation)\n        i += 1\n    plt.show()\n\n", "CodeBleu_score": 71.8647826847557}, {"code": "def display_images(images, titles=None, cols=4, cmap=None, norm=None,\n                   interpolation=None)\n\t\n    titles = titles if titles is not None else [\"Image (%d)\" % i\n                                                 for i in range(1, len(images) + 1)]\n    rows = len(images) // cols + 1\n    plt.figure(figsize=(14, 14 * rows // cols))\n    i = 1\n    for image, title in zip(images, titles):\n        plt.subplot(rows, cols, i)\n        plt.title(title, fontsize=9)\n        plt.axis('off')\n        plt.imshow(image.astype(np.uint8), cmap=cmap,\n                   norm=norm, interpolation=interpolation)\n        i += 1\n    plt.show()\n\n", "CodeBleu_score": 71.82530596779856}, {"code": "def display_images(images, titles=None, cols=4, cmap=None, norm=None,\n                   interpolation=None)\n\t\n    titles = titles if titles is not None else [\"Image (%d)\" % i for i in range(1, len(images) + 1)]\n    rows = len(images) // cols + 1\n    plt.figure(figsize=(14, 14 * rows // cols))\n    i = 1\n    for image, title in zip(images, titles):\n        plt.subplot(rows, cols, i)\n        plt.title(title, fontsize=9)\n        plt.axis('off')\n        plt.imshow(image.astype(np.uint8), cmap=cmap, norm=norm, interpolation=interpolation)\n        i += 1\n    plt.show()\n\n", "CodeBleu_score": 73.88824563786821}]}
