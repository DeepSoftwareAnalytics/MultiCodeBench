{"instance_id": "1f9f4d85e825", "generation_result": [{"code": "def load_model_from_config(self, half_attention)\n\t\n    global cached_ldsr_model\n    if cached_ldsr_model is not None:\n        return cached_ldsr_model\n    if self.model_path.endswith(\".safetensors\"):\n        model = safetensors.torch.load(self.model_path)\n    else:\n        model = torch.load(self.model_path, map_location=\"cpu\")\n    if half_attention:\n        model.half()\n    model.eval()\n    model.requires_grad_(False)\n    if self.cache_model:\n        cached_ldsr_model = model\n    return model\n\n", "CodeBleu_score": 23.963523971534414}, {"code": "def load_model_from_config(self, half_attention)\n\t\n    # Load model from cache if available\n    if cached_ldsr_model is not None and self.config.model.model_path == cached_ldsr_model.config.model.model_path and self.config.model.model_type == cached_ldsr_model.config.model.model_type and self.config.model.half_attention == cached_ldsr_model.config.model.half_attention:\n        print(\"Using cached model from\", self.config.model.model_path)\n        model = cached_ldsr_model\n    else:\n        # Load model from path\n        print(\"Loading model from\", self.config.model.model_path)\n        if self.config.model.model_path.endswith(\".safetensors\"):\n            model = torch.load(self.config.model.model_path)\n        else:\n            model = instantiate_from_config(self.config.model)\n        # Cache model for future use\n        if self.config.model.cache:\n            cached_ldsr_model = model\n    # Configure model\n    model.eval()\n    if half_attention:\n        model.half_precision_attention()\n    if self.config.model.memory_format == \"channels_last\":\n        model.use_channels_last()\n    # Move model to device\n    model.to(self.device)\n    # Return model\n    return model\n\n", "CodeBleu_score": 31.37987490640365}, {"code": "def load_model_from_config(self, half_attention)\n\t\n\n    # Check if a cached model is available\n    if self.config.model.cache:\n        if self.config.model.cache_path:\n            cache_path = self.config.model.cache_path\n        else:\n            cache_path = self.config.model.path\n        if os.path.exists(cache_path):\n            if self.config.model.cache_refresh:\n                print(f\"Refreshing cache at {cache_path}\")\n                os.remove(cache_path)\n            else:\n                print(f\"Loading cached model from {cache_path}\")\n                return torch.load(cache_path)\n\n    # Load model from path\n    print(f\"Loading model from {self.config.model.path}\")\n    model = instantiate_from_config(self.config.model)\n\n    # Move model to device\n    model.to(self.device)\n\n    # Configure model\n    model.eval()\n    if half_attention:\n        model.half()\n    if self.config.model.memory_format == \"channels_last\":\n        model = model.to(memory_format=torch.channels_last)\n\n    # Cache model\n    if self.config.model.cache:\n        print(f\"Caching model at {cache_path}\")\n        torch.save(model, cache_path)\n\n    return model\n\n", "CodeBleu_score": 32.77011880069251}]}
{"instance_id": "8dc955007aa2", "generation_result": [{"code": "def convsample_ddim(model, cond, steps, shape, eta=1.0, callback=None, normals_sequence=None,\n                    mask=None, x0=None, quantize_x0=False, temperature=1., score_corrector=None,\n                    corrector_kwargs=None, x_t=None\n                    )\n\t\n    batch_size, height, width, channels = shape\n    print(f'Batch Size: {batch_size}, Height: {height}, Width: {width}, Channels: {channels}')\n    print(f'Eta: {eta}, Steps: {steps}')\n\n    sampler = DDIMSampler(model, eta=eta, score_corrector=score_corrector, corrector_kwargs=corrector_kwargs)\n    samples = sampler.sample(steps, batch_size, cond, shape, mask=mask, x0=x0, quantize_x0=quantize_x0,\n                             temperature=temperature, callback=callback, normals_sequence=normals_sequence, x_t=x_t)\n\n    return samples\n\n", "CodeBleu_score": 42.89361867909474}, {"code": "def convsample_ddim(model, cond, steps, shape, eta=1.0, callback=None, normals_sequence=None,\n                    mask=None, x0=None, quantize_x0=False, temperature=1., score_corrector=None,\n                    corrector_kwargs=None, x_t=None\n                    )\n\t\n    print(f'eta: {eta}')\n    print(f'steps: {steps}')\n    b, c, h, w = shape\n    model.eval()\n    model.to(shared.device)\n    model.to(memory_format=torch.channels_last)\n    if not isinstance(cond, torch.Tensor):\n        cond = cond.to(shared.device)\n    if not isinstance(x0, torch.Tensor):\n        x0 = x0.to(shared.device)\n    if not isinstance(mask, torch.Tensor):\n        mask = mask.to(shared.device)\n\n    sampler = DDIMSampler(model, cond, steps, eta=eta, callback=callback,\n                          normals_sequence=normals_sequence, mask=mask, x0=x0, quantize_x0=quantize_x0,\n                          temperature=temperature, score_corrector=score_corrector,\n                          corrector_kwargs=corrector_kwargs, x_t=x_t)\n    samples = sampler.sample(b)\n    return samples\n\n", "CodeBleu_score": 44.63904166321715}, {"code": "def convsample_ddim(model, cond, steps, shape, eta=1.0, callback=None, normals_sequence=None,\n                    mask=None, x0=None, quantize_x0=False, temperature=1., score_corrector=None,\n                    corrector_kwargs=None, x_t=None\n                    )\n\t\n    batch_size, channels, height, width = shape\n\n    # Print values\n    print(f'Batch size: {batch_size}')\n    print(f'Eta: {eta}')\n    print(f'Steps: {steps}')\n\n    # Create sampler\n    sampler = DDIMSampler(model,\n                          batch_size=batch_size,\n                          use_ddim=False,\n                          noise_schedule=LinearNoiseSchedule(steps),\n                          denoise_fn=denoise_fn_kwargs(eta=eta, score_corrector=score_corrector,\n                                                       corrector_kwargs=corrector_kwargs),\n                          callback=callback,\n                          )\n\n    # Sample\n    samples = sampler.sample(cond,\n                             steps=steps,\n                             normals_sequence=normals_sequence,\n                             mask=mask,\n                             x0=x0,\n                             quantize_x0=quantize_x0,\n                             temperature=temperature,\n                             x_t=x_t,\n                             )\n\n    return samples\n\n", "CodeBleu_score": 44.24788978961677}]}
{"instance_id": "9bdcd9081480", "generation_result": [{"code": "def ema_scope(self, context=None)\n\t\n    if self.use_ema and self.model_ema is not None:\n        if context is not None:\n            print(f\\\"Switching to EMA weights for {context}.\\\")\n        with self.model_ema.average_parameters():\n            yield\n        if context is not None:\n            print(f\\\"Restoring original weights for {context}.\\\")\n    else:\n        yield\n", "CodeBleu_score": 46.09219975091057}, {"code": "def ema_scope(self, context=None)\n\t\n    if not self.use_ema:\n        yield\n        return\n    if context is None:\n        context = self.__class__.__name__\n    ema_model = self.model_ema\n    assert ema_model is not None, f\\\"{context}: EMA model is None. Did you forget to call .to() on the model?\\\"\n    try:\n        # Temporarily swap out the model weights with the EMA weights.\n        for ema_b, model_b in zip(ema_model.buffers(), self.buffers()):\n            model_b.data.copy_(ema_b.data)\n        for ema_p, model_p in zip(ema_model.parameters(), self.parameters()):\n            model_p.data.copy_(ema_p.data)\n        yield\n    finally:\n        # Restore the original model weights.\n        for ema_b, model_b in zip(ema_model.buffers(), self.buffers()):\n            ema_b.data.copy_(model_b.data)\n        for ema_p, model_p in zip(ema_model.parameters(), self.parameters()):\n            ema_p.data.copy_(model_p.data)\n", "CodeBleu_score": 34.411813603941596}, {"code": "def ema_scope(self, context=None)\n\t\n    if not self.use_ema:\n        yield\n        return\n    if context is None:\n        context = self.__class__.__name__\n    with self.model_ema.average_parameters():\n        yield\n        if self.use_ema:\n            print(f\\\"Restoring EMA weights for {context}.\\\")\n            self.model_ema.restore()\n\n", "CodeBleu_score": 38.67034757629758}]}
{"instance_id": "14ab009e1c5c", "generation_result": [{"code": "def init_from_ckpt(self, path, ignore_keys=None)\n\t\n\n    ckpt = torch.load(path, map_location=\"cpu\")\n    if \"state_dict\" in ckpt:\n        ckpt = ckpt[\"state_dict\"]\n    self.load_state_dict(ckpt, strict=False)\n    missing_keys, unexpected_keys = self.load_state_dict(ckpt, strict=False)\n    if ignore_keys is not None:\n        for k in ignore_keys:\n            self.state_dict()[k]\n            missing_keys.remove(k)\n            unexpected_keys.remove(k)\n    if len(missing_keys) > 0:\n        print(f\\\"Missing Keys: {missing_keys}\\\")\n    if len(unexpected_keys) > 0:\n        print(f\\\"Unexpected Keys: {unexpected_keys}\\\")\n\n\n\n", "CodeBleu_score": 36.373323914326626}, {"code": "def init_from_ckpt(self, path, ignore_keys=None)\n\t\n    sd = torch.load(path, map_location=\"cpu\")[\"state_dict\"]\n    if ignore_keys is not None:\n        for k in ignore_keys:\n            sd.pop(k, None)\n    self.load_state_dict(sd, strict=False)\n    missing_keys, unexpected_keys = self.load_state_dict(sd, strict=False)\n    if len(missing_keys) > 0:\n        print(f\"Missing Keys: {missing_keys}\")\n    if len(unexpected_keys) > 0:\n        print(f\"Unexpected Keys: {unexpected_keys}\")\n\n", "CodeBleu_score": 35.94385376864289}, {"code": "def init_from_ckpt(self, path, ignore_keys=None)\n\t\n    if ignore_keys is None:\n        ignore_keys = []\n    # Load weights to CPU (with upgrading for backward compatibility)\n    ckpt = torch.load(path, map_location=\"cpu\")\n    state_dict = ckpt[\"state_dict\"]\n    if \"state_dict_ema\" in ckpt and self.use_ema:\n        state_dict_ema = ckpt[\"state_dict_ema\"]\n    # Delete ignored keys\n    for k in state_dict.copy().keys():\n        for ik in ignore_keys:\n            if k.startswith(ik):\n                print(f\"Deleting key {k} from state_dict.\")\n                del state_dict[k]\n    # Load state dict\n    self.load_state_dict(state_dict, strict=False)\n    if self.use_ema:\n        self.model_ema.set(state_dict_ema)\n    # Print out ignored keys\n    if version.parse(pl.__version__) >= version.parse(\"1.4.0\"):\n        unexpected_keys = self.load_state_dict(state_dict, strict=False)\n    else:\n        unexpected_keys = self.load_state_dict(state_dict)\n    if len(unexpected_keys.unexpected_keys) > 0:\n        print(f\"Unexpected key(s) in state_dict: {', '.join(unexpected_keys.unexpected_keys)}.\")\n    if len(unexpected_keys.missing_keys) > 0:\n        print(f\"Missing key(s) in state_dict: {', '.join(unexpected_keys.missing_keys)}.\")\n", "CodeBleu_score": 38.21060929223762}]}
{"instance_id": "754176e27b81", "generation_result": [{"code": "def forward(self, input, return_pred_indices=False)\n\t\n    quant, diff, info = self.encode(input)\n    dec = self.decode(quant)\n    if return_pred_indices:\n        pred_indices = self.quantize.get_pred_indices(quant)\n        return dec, diff, info, pred_indices\n    return dec, diff, info\n", "CodeBleu_score": 43.88191574146636}, {"code": "def forward(self, input, return_pred_indices=False)\n\t\n    quant, diff, _, _, _ = self.encode(input)\n    dec = self.decode(quant)\n    if return_pred_indices:\n        _, pred_indices = self.quantize.get_codebook_indices(quant)\n        return dec, diff, pred_indices\n    return dec, diff\n\n", "CodeBleu_score": 38.294083248833026}, {"code": "def forward(self, input, return_pred_indices=False)\n\t\n\n    quant, diff, info = self.encode(input)\n    dec = self.decode(quant)\n    if return_pred_indices:\n        return dec, diff, info, self.quantize.get_pred_indices(quant)\n    return dec, diff, info\n", "CodeBleu_score": 50.55380200709002}]}
{"instance_id": "443df6fdaf0d", "generation_result": [{"code": "def get_input(self, batch, k)\n\t\n    x = batch[k]\n    if len(x.shape) == 3:\n        x = x[..., None]\n    if len(x.shape) == 2:\n        x = x[..., None, None]\n    assert len(x.shape) == 4,\n    x = x.permute(0, 3, 1, 2).contiguous()\n    if self.batch_resize_range is not None:\n        x2 = F.interpolate(x, size=self.batch_resize_range[1], mode=\\\"bicubic\\\", align_corners=False)\n        t = self.global_step / self.total_steps\n        s = self.batch_resize_range[0]\n        e = self.batch_resize_range[1]\n        size = int(s + (e - s) * t)\n        x = F.interpolate(x2, size=size, mode=\\\"bicubic\\\", align_corners=False)\n    x = x.float().detach().to(self.device)\n    return x\n\n", "CodeBleu_score": 55.416627076292855}, {"code": "def get_input(self, batch, k)\n\t\n    x = batch[k]\n    if len(x.shape) == 3:\n        x = x[..., None]\n    if len(x.shape) == 2:\n        x = x[..., None, None]\n    if self.batch_resize_range is not None:\n        x = self.resize_batch(x, self.global_step, self.batch_resize_range)\n    x = x.permute(0, 3, 1, 2).contiguous()\n    x = x.to(memory_format=torch.contiguous_format).float()\n    return x.detach()\n\n", "CodeBleu_score": 41.45935960391642}, {"code": "def get_input(self, batch, k)\n\t\n    x = batch[k]\n    x = x.type(self.dtype)\n    if len(x.shape) == 3:\n        x = x[..., None]\n    if len(x.shape) == 2:\n        x = x[..., None, None]\n    if self.batch_resize_range is not None:\n        x = self.resize_batch(x, self.global_step, self.batch_resize_range)\n    x = x.permute(0, 3, 1, 2)\n    x = x.contiguous()\n    return x.detach()\n\n\n", "CodeBleu_score": 36.579546605226774}]}
{"instance_id": "953a2632fc15", "generation_result": [{"code": "def training_step(self, batch, batch_idx, optimizer_idx)\n\t\n    if self.global_step < 1000:\n        self.global_step += 1\n        return\n    if optimizer_idx == 0:\n        # autoencoder training\n        x = self.get_input(batch, self.image_key)\n        xrec, qloss, _ = self(x, return_pred_indices=True)\n        aeloss, log_dict_ae = self.loss(qloss, x, xrec, optimizer_idx=optimizer_idx)\n        aeloss = aeloss.mean()\n        self.log_dict(log_dict_ae, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n        return aeloss\n    else:\n        # discriminator training\n        assert optimizer_idx == 1\n        with self.ema_scope(\\\"discriminator\\\"): # use EMA weights to train the discriminator\n            x = self.get_input(batch, self.image_key)\n            xrec, qloss, pred_indices = self(x, return_pred_indices=True)\n            discloss, log_dict_disc = self.loss(qloss, x, xrec, pred_indices=pred_indices, optimizer_idx=optimizer_idx)\n            discloss = discloss.mean()\n            self.log_dict(log_dict_disc, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n            return discloss\n\n\n\n", "CodeBleu_score": 52.76509506981362}, {"code": "def training_step(self, batch, batch_idx, optimizer_idx)\n\t\n    if optimizer_idx == 0:\n        # autoencoder\n        input = self.get_input(batch, self.image_key)\n        if self.global_step <= 4:\n            # do the first few batches with max size to avoid later oom\n            new_resize = self.batch_resize_range[1]\n        else:\n            new_resize = np.random.choice(np.arange(self.batch_resize_range[0], self.batch_resize_range[1]+16, 16))\n        if new_resize != input.shape[2]:\n            input = F.interpolate(input, size=new_resize, mode=\\\"bicubic\\\")\n        input = input.detach()\n        pred, diff = self(input)\n        ae_loss, log_dict_ae = self.loss(pred, input, return_log_dict=True)\n        ae_loss = ae_loss.mean()\n        self.log(\\\"ae_loss\\\", ae_loss, on_epoch=True, sync_dist=True, prog_bar=True, logger=True)\n        for k, v in log_dict_ae.items():\n            self.log(k, v, on_epoch=True, sync_dist=True, prog_bar=True, logger=True)\n        return ae_loss\n    elif optimizer_idx == 1:\n        # discriminator\n        input = self.get_input(batch, self.image_key)\n        if self.global_step <= 4:\n            # do the first few batches with max size to avoid later oom\n            new_resize = self.batch_resize_range[1]\n        else:\n            new_resize = np.random.choice(np.arange(self.batch_resize_range[0], self.batch_resize_range[1]+16, 16))\n        if new_resize != input.shape[2]:\n            input = F.interpolate(input, size=new_resize, mode=\\\"bicubic\\\")\n        input = input.detach()\n        with torch.no_grad():\n            pred, diff = self(input)\n        disc_loss, log_dict_disc = self.", "CodeBleu_score": 43.13996993220016}, {"code": "def training_step(self, batch, batch_idx, optimizer_idx)\n\t\n    if self.global_step == 0:\n        self.log(\\\"learning_rate\\\", self.optimizers()[0].param_groups[0]['lr'], on_step=True, on_epoch=False, prog_bar=False, logger=True)\n    x = self.get_input(batch, self.image_key)\n    xrec, qloss = self(x)\n    if optimizer_idx == 0:\n        loss = self.loss(qloss, x, xrec)\n        self.log(\\\"train_loss\\\", loss, on_step=True, on_epoch=True, prog_bar=False, logger=True)\n        self.log(\\\"train_recon_loss\\\", qloss, on_step=True, on_epoch=True, prog_bar=False, logger=True)\n        return loss\n    else:\n        raise NotImplementedError\n\n\n\n", "CodeBleu_score": 36.85687962171486}]}
{"instance_id": "b449b5e8b77f", "generation_result": [{"code": "def log_images(self, batch, only_inputs=False, plot_ema=False, **kwargs)\n\t\n    if not self.hparams.log_images:\n        return\n    if self.trainer.sanity_checking:\n        return\n    if self.trainer.fast_dev_run:\n        return\n    if self.trainer.num_sanity_val_steps > 0:\n        return\n    if self.trainer.is_slurm_managing_tasks:\n        return\n    if self.trainer.global_step < self.hparams.log_images_start_step:\n        return\n    if self.trainer.global_step > self.hparams.log_images_end_step:\n        return\n    if self.trainer.global_step % self.hparams.log_images_every_n_steps != 0:\n        return\n    if self.trainer.global_step == self.last_logged_step:\n        return\n    self.last_logged_step = self.trainer.global_step\n    x = self.get_input(batch, self.image_key)\n    xrec, _, _ = self(x, return_pred_indices=True)\n    if plot_ema:\n        with self.ema_scope():\n            xrec_ema, _, _ = self(x, return_pred_indices=True)\n    if self.hparams.colorize_n_channels > 1:\n        x = colorize_img(x)\n        xrec = colorize_img(xrec)\n        if plot_ema:\n            xrec_ema = colorize_img(xrec_ema)\n    if only_inputs:\n        grid = torchvision.utils.make_grid(x, nrow=self.hparams.n_image_tiles)\n        self.logger.experiment.add_image(f'inputs{\"\" if not plot_ema else \"_ema\"}', grid, self.trainer.global_step)\n        return\n    grid = torchvision.utils.make_grid(torch.cat([x, xrec]), nrow=self.hparams.n_image_tiles)\n    self.logger.experiment.add_image(f'inputs_and_reconstructions{\"\" if not plot_ema else \"_ema\"}', grid, self.trainer.global_step)\n    if plot_ema:\n        grid = torchvision.utils.make_grid(torch.cat([x, xrec_ema]), nrow=self.hparams.n_image", "CodeBleu_score": 31.496681230888452}, {"code": "def log_images(self, batch, only_inputs=False, plot_ema=False, **kwargs)\n\t\n    if not self.trainer.global_step % self.log_images_every_n_steps:\n        if self.log_images_to_wandb:\n            self.log_images_to_wandb_step(batch, only_inputs=only_inputs, plot_ema=plot_ema, **kwargs)\n        if self.log_images_to_tensorboard:\n            self.log_images_to_tensorboard_step(batch, only_inputs=only_inputs, plot_ema=plot_ema, **kwargs)\n        if self.log_images_to_comet_ml:\n            self.log_images_to_comet_ml_step(batch, only_inputs=only_inputs, plot_ema=plot_ema, **kwargs)\n        if self.log_images_to_neptune:\n            self.log_images_to_neptune_step(batch, only_inputs=only_inputs, plot_ema=plot_ema, **kwargs)\n\n    return {}\n\n", "CodeBleu_score": 18.12402587190232}, {"code": "def log_images(self, batch, only_inputs=False, plot_ema=False, **kwargs)\n\t\n    if plot_ema:\n        self.model_ema(self)\n    if not hasattr(self, \\\"logger\\\") or self.logger is None:\n        return\n    if hasattr(self, \\\"image_key\\\") and self.image_key is not None:\n        x = self.get_input(batch, self.image_key)\n        x = self.to_rgb(x)\n        if only_inputs:\n            self.logger.experiment.add_images(f\\\"{self.log_prefix}/inputs\\\", x, self.global_step, dataformats=\\\"CHW\\\")\n        else:\n            x_recon = self.last_batch.get(f\\\"last_x_recon{'' if not plot_ema else '_ema'}\\\")\n            x_recon = self.to_rgb(x_recon)\n            self.logger.experiment.add_images(f\\\"{self.log_prefix}/inputs\\\", x, self.global_step, dataformats=\\\"CHW\\\")\n            self.logger.experiment.add_images(f\\\"{self.log_prefix}/reconstructions\\\", x_recon, self.global_step, dataformats=\\\"CHW\\\")\n    if hasattr(self, \\\"other_image_keys\\\") and self.other_image_keys is not None:\n        for k in self.other_image_keys:\n            x = self.get_input(batch, k)\n            x = self.to_rgb(x)\n            if only_inputs:\n                self.logger.experiment.add_images(f\\\"{self.log_prefix}/{k}/inputs\\\", x, self.global_step, dataformats=\\\"CHW\\\")\n            else:\n                x_recon = self.last_batch.get(f\\\"last_x_recon{'' if not plot_ema else '_ema'}\\\")\n                x_recon = self.to_rgb(x_recon)\n                self.logger.experiment.add_images(f\\\"{self.log_prefix}/{k}/inputs\\\", x, self.global_step, dataformats=\\\"CHW\\\")\n                self.logger.experiment.add_images(f\\\"{self.log_prefix}/{k}/reconstructions\\\", x_recon, self.global_step, dataformats=\\\"CHW\\\")\n    if hasattr(self, \\\"last_z\\\") and self.last_z is not None:\n        z = self.", "CodeBleu_score": 32.29385444767651}]}
{"instance_id": "540d089e3481", "generation_result": [{"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss_type == 'l1':\n        loss = (pred - target).abs()\n        if mean:\n            loss = loss.mean()\n    elif self.loss_type == 'l2':\n        loss = (pred - target).pow(2)\n        if mean:\n            loss = loss.mean() if self.mean_loss else loss.sum()\n    else:\n        raise NotImplementedError(f'Unknown loss_type: {self.loss_type}')\n    return loss\n\n", "CodeBleu_score": 55.05663476600648}, {"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss_type == 'l1':\n        loss = (pred - target).abs()\n        if mean:\n            loss = loss.mean()\n    elif self.loss_type == 'l2':\n        loss = (pred - target).pow(2)\n        if mean:\n            loss = loss.mean() if self.mean_loss else loss.sum()\n    else:\n        raise NotImplementedError(f'Unknown loss_type: {self.loss_type}')\n    return loss\n\n", "CodeBleu_score": 55.05663476600648}, {"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss_type == 'l1':\n        loss = F.l1_loss(pred, target, reduction='none')\n        if mean:\n            loss = loss.mean()\n    elif self.loss_type == 'l2':\n        loss = F.mse_loss(pred, target, reduction='none')\n        if mean:\n            loss = loss.mean() if self.mean_loss else loss.sum()\n    else:\n        raise NotImplementedError(f'Unknown loss type: {self.loss_type}')\n    return loss\n\n", "CodeBleu_score": 57.83298241809553}]}
{"instance_id": "63a08283666f", "generation_result": [{"code": "def forward(self, x, c, *args, **kwargs)\n\t\n    # if self.training:\n    #     x, c = self.get_input(batch, self.first_stage_key)\n    # else:\n    #     x, c = self.get_input(batch, self.first_stage_key)\n\n    # if self.training:\n    #     x, c = self.get_input(batch, self.first_stage_key)\n    # else:\n    #     x, c = self.get_input(batch, self.first_stage_key)\n\n    # if self.training:\n    #     x, c = self.get_input(batch, self.first_stage_key)\n    # else:\n    #     x, c = self.get_input(batch, self.first_stage_key)\n\n    # if self.training:\n    #     x, c = self.get_input(batch, self.first_stage_key)\n    # else:\n    #     x, c = self.get_input(batch, self.first_stage_key)\n\n    # if self.training:\n    #     x, c = self.get_input(batch, self.first_stage_key)\n    # else:\n    #     x, c = self.get_input(batch, self.first_stage_key)\n\n    # if self.training:\n    #     x, c = self.get_input(batch, self.first_stage_key)\n    # else:\n    #     x, c = self.get_input(batch, self.first_stage_key)\n\n    # if self.training:\n    #     x, c = self.get_input(batch, self.first_stage_key)\n    # else:\n    #     x, c = self.get_input(batch, self.first_stage_key)\n\n    # if self.training:\n    #     x, c = self.get_input(batch, self.first_stage_key)\n    # else:\n    #     x, c = self.get_input(batch, self.first_stage_key)\n\n    # if self.training:\n    #     x, c = self.get_input(batch, self.first_stage_key)\n    # else:\n    #     x, c = self.get_input(batch, self.first_stage_key", "CodeBleu_score": 2.188995215311005}, {"code": "def forward(self, x, c, *args, **kwargs)\n\t\n    # get time step indices\n    t = self.get_t(c)\n    # get conditioning\n    c = self.get_c(c)\n    # get conditioning if applicable\n    if self.conditioning_key is not None:\n        c = self.get_input(c, self.conditioning_key)\n    # get conditioning if applicable\n    if self.first_stage_key is not None:\n        x = self.get_input(x, self.first_stage_key)\n    # get conditioning if applicable\n    if self.second_stage_key is not None:\n        x = self.get_input(x, self.second_stage_key)\n    # get conditioning if applicable\n    if self.third_stage_key is not None:\n        x = self.get_input(x, self.third_stage_key)\n    # get conditioning if applicable\n    if self.fourth_stage_key is not None:\n        x = self.get_input(x, self.fourth_stage_key)\n    # get conditioning if applicable\n    if self.fifth_stage_key is not None:\n        x = self.get_input(x, self.fifth_stage_key)\n    # get conditioning if applicable\n    if self.sixth_stage_key is not None:\n        x = self.get_input(x, self.sixth_stage_key)\n    # get conditioning if applicable\n    if self.seventh_stage_key is not None:\n        x = self.get_input(x, self.seventh_stage_key)\n    # get conditioning if applicable\n    if self.eighth_stage_key is not None:\n        x = self.get_input(x, self.eighth_stage_key)\n    # get conditioning if applicable\n    if self.ninth_stage_key is not None:\n        x = self.get_input(x, self.ninth_stage_key)\n    # get conditioning if applicable\n    if self.tenth_stage_key is not None:\n        x = self.get_input(x, self.tenth_stage_key)\n    # get conditioning if applicable\n    if self.eleventh_stage_key is not None:\n        x = self.get_input(x, self.ele", "CodeBleu_score": 26.454973751766463}, {"code": "def forward(self, x, c, *args, **kwargs)\n\t\n    if self.training and self.first_stage_key is not None and self.first_stage_model is not None and self.first_stage_model.training:\n        raise RuntimeError(\\\"You cannot train a model that has a first stage model and first stage key set to train mode.\\\")\n    if self.first_stage_key is not None and self.first_stage_model is not None:\n        if self.training:\n            if self.first_stage_model.training:\n                x = self.differentiable_encode_first_stage(x)\n            else:\n                x = self.encode_first_stage(x)\n        else:\n            if self.first_stage_model.training:\n                x = self.decode_first_stage(x)\n            else:\n                x = self.first_stage_model(x)\n    if self.conditioning_key is not None and self.conditioning_model is not None:\n        if self.training:\n            if self.conditioning_model.training:\n                c = self.differentiable_encode_conditioning(c)\n            else:\n                c = self.encode_conditioning(c)\n        else:\n            if self.conditioning_model.training:\n                c = self.decode_conditioning(c)\n            else:\n                c = self.conditioning_model(c)\n    if self.training:\n        return self.shared_step(x, c, *args, **kwargs)\n    else:\n        return self.shared_step(x, *args, **kwargs)\n\n", "CodeBleu_score": 30.536218468701314}]}
{"instance_id": "4627e1e8ed62", "generation_result": [{"code": "def get_codebook_entry(self, indices, shape)\n\t\n    if self.remap is not None:\n        indices = self.remap_to_used(indices)\n    latents = self.embedding(indices.flatten(0, 1))\n    latents = rearrange(latents, 'b n d -> b d n').contiguous()\n    return latents.reshape(shape)\n", "CodeBleu_score": 38.22224726217518}, {"code": "def get_codebook_entry(self, indices, shape)\n\t\n    if self.remap is not None:\n        indices = self.remap_to_used(indices)\n    quantized = self.embedding(indices)\n    if self.sane_index_shape:\n        quantized = quantized.reshape(shape[0], shape[2], shape[3])\n    return quantized\n\n", "CodeBleu_score": 39.805528497171196}, {"code": "def get_codebook_entry(self, indices, shape)\n\t\n    # handle optional remapping of indices\n    if self.remap is not None:\n        indices = self.remap_to_used(indices)\n\n    # get quantized latent vectors\n    z_q = self.embedding(indices).view(shape)\n\n    # reshape back to match original input shape\n    z_q = rearrange(z_q, 'b h w c -> b c h w').contiguous()\n\n    # permute to match the original input tensor\n    # if self.permute:\n    #     z_q = z_q.permute(0, 3, 1, 2).contiguous()\n\n    return z_q\n\n", "CodeBleu_score": 37.07823341988203}]}
{"instance_id": "c55d014827ef", "generation_result": [{"code": "def factorization(dimension: int, factor:int=-1) -> tuple[int, int]\n\t\n    if factor > 0:\n        if factor > dimension:\n            raise ValueError(\"The factor must be less than or equal to the dimension.\")\n        return factor, dimension // factor\n    if factor == 0:\n        raise ValueError(\"The factor cannot be zero.\")\n    if factor < 0:\n        if factor < -dimension:\n            raise ValueError(\"The factor must be greater than or equal to the negative of the dimension.\")\n        factor = 1\n        while factor <= dimension:\n            if dimension % factor == 0:\n                break\n            factor += 1\n        return factor, dimension // factor\n\n", "CodeBleu_score": 31.23522473311112}, {"code": "def factorization(dimension: int, factor:int=-1) -> tuple[int, int]\n\t\n    if factor > 0:\n        if factor > dimension:\n            raise ValueError(f\"factor {factor} must be less than or equal to dimension {dimension}\")\n        return factor, dimension // factor\n    elif factor < 0:\n        if abs(factor) > dimension:\n            raise ValueError(f\"abs(factor) {abs(factor)} must be less than or equal to dimension {dimension}\")\n        factor = dimension\n        while factor > 0:\n            if factor > abs(factor):\n                return factor, dimension // factor\n            factor -= 1\n        raise ValueError(f\"could not find a factorization for dimension {dimension} with factor {factor}\")\n    else:\n        raise ValueError(f\"factor {factor} must be nonzero\")\n\n", "CodeBleu_score": 29.76559294919715}, {"code": "def factorization(dimension: int, factor:int=-1) -> tuple[int, int]\n\t\n    if factor > 0 and dimension % factor == 0:\n        return factor, dimension // factor\n    if factor < 0:\n        for i in range(1, dimension):\n            if dimension % i == 0 and i <= abs(factor):\n                return i, dimension // i\n    raise ValueError(f\"Factorization of {dimension} into two factors with a specified factor of {factor} is not possible.\")\n\n", "CodeBleu_score": 24.636612558452462}]}
{"instance_id": "5d613ec26036", "generation_result": [{"code": "def apply_weight_decompose(self, updown, orig_weight)\n\t\n    if orig_weight is None:\n        return updown\n\n    if updown is None:\n        return orig_weight\n\n    if self.dora_scale is None:\n        self.dora_scale = 1.0\n\n    if self.dora_norm_dims is None:\n        self.dora_norm_dims = 1\n\n    if self.dora_norm_dims == 0:\n        return updown\n\n    if self.dora_norm_dims == 1:\n        return (updown - orig_weight) * self.dora_scale\n\n    if self.dora_norm_dims == 2:\n        return (updown - orig_weight) * self.dora_scale\n\n    if self.dora_norm_dims == 3:\n        return (updown - orig_weight) * self.dora_scale\n\n    return updown\n\n", "CodeBleu_score": 21.726663320211472}, {"code": "def apply_weight_decompose(self, updown, orig_weight)\n\t\n    weight = orig_weight\n    if updown == 'up':\n        weight = weight.transpose(0, 1).contiguous()\n    weight = weight.view(weight.shape[0], -1)\n    weight = weight.to(self.device)\n    weight = weight.type(self.dtype)\n    return weight\n\n", "CodeBleu_score": 22.229064888495742}, {"code": "def apply_weight_decompose(self, updown, orig_weight)\n\t\n    assert updown in ['up', 'down']\n    assert orig_weight is not None\n    assert isinstance(orig_weight, dict)\n\n    if updown == 'up':\n        weight = orig_weight\n    elif updown == 'down':\n        weight = {k: -v for k, v in orig_weight.items()}\n\n    # Make sure all tensors are on the same device and have the same data type\n    device = None\n    dtype = None\n    for k, v in weight.items():\n        if device is None:\n            device = v.device\n        if dtype is None:\n            dtype = v.dtype\n        assert v.device == device\n        assert v.dtype == dtype\n\n    # Combine the original weight and the input weight\n    for k, v in orig_weight.items():\n        if k in weight:\n            weight[k] = weight[k] + v\n        else:\n            weight[k] = v\n\n    # Normalize the combined weight\n    norm = torch.norm(torch.stack([v.flatten() for v in weight.values()]), p=2, dim=1, keepdim=True)\n    for k, v in weight.items():\n        weight[k] = weight[k] / norm\n\n    # Scale the combined weight\n    for k, v in weight.items():\n        weight[k] = weight[k] * self.scale\n\n    # Adjust the combined weight\n    for k, v in orig_weight.items():\n        weight[k] = weight[k] - v\n\n    return weight\n\n", "CodeBleu_score": 26.751468972103165}]}
{"instance_id": "c73d0d1cd3e5", "generation_result": [{"code": "def finalize_updown(self, updown, orig_weight, output_shape, ex_bias=None)\n\t\n    # Match the device/dtype\n    orig_weight = orig_weight.to(updown.dtype)\n    if ex_bias is not None:\n        ex_bias = ex_bias.to(updown.dtype)\n\n    if self.bias is not None:\n        if ex_bias is not None:\n            ex_bias = ex_bias + self.bias\n        else:\n            ex_bias = self.bias\n\n    if len(updown.shape) != len(output_shape):\n        # Adjust updown shape to match output shape\n        updown = updown.reshape(output_shape)\n\n    if len(orig_weight.shape) == len(output_shape) and np.prod(orig_weight.shape) == np.prod(updown.shape):\n        # Adjust updown shape to match original weight shape\n        updown = updown.reshape(orig_weight.shape)\n\n    if self.scale is not None:\n        updown = updown * self.scale\n\n    if self.dora_scale is not None:\n        updown = self.apply_weight_decompose(updown, orig_weight)\n\n    return updown, ex_bias\n\n", "CodeBleu_score": 56.73387922246115}, {"code": "def finalize_updown(self, updown, orig_weight, output_shape, ex_bias=None)\n\t\n    if self.bias is not None and ex_bias is not None:\n        ex_bias = ex_bias + self.bias\n    elif self.bias is not None:\n        ex_bias = self.bias\n\n    if self.alpha is not None and self.scale is not None:\n        raise ValueError(\\\"Module has both alpha and scale defined.\\\")\n\n    if self.alpha is not None:\n        updown = updown * self.alpha\n    elif self.scale is not None:\n        updown = updown * self.scale\n\n    if len(output_shape) != len(updown.shape):\n        updown = updown.reshape(output_shape)\n\n    if self.ops is not None and self.ops == F.linear:\n        updown = updown.transpose(0, 1)\n\n    if self.ops is not None and self.ops == F.conv2d:\n        updown = updown.transpose(0, 1)\n        updown = updown.reshape(output_shape[1], output_shape[0], *output_shape[2:])\n\n    if self.dora_scale is not None:\n        updown = self.apply_weight_decompose(updown, orig_weight)\n\n    return updown, ex_bias\n\n", "CodeBleu_score": 53.43714024518685}, {"code": "def finalize_updown(self, updown, orig_weight, output_shape, ex_bias=None)\n\t\n    if ex_bias is not None:\n        ex_bias = ex_bias.to(device=updown.device, dtype=updown.dtype)\n    if updown.shape[0] == orig_weight.shape[0]:  # upscaling\n        updown = updown.reshape(orig_weight.shape)\n    else:  # downscaling\n        updown = updown.reshape(output_shape)\n    if ex_bias is not None:\n        ex_bias = ex_bias * self.multiplier()\n    if self.bias is not None:\n        ex_bias = ex_bias + self.bias\n    if self.scale is not None:\n        updown = updown * self.calc_scale()\n    if self.dora_scale is not None:\n        updown = self.apply_weight_decompose(updown, orig_weight)\n    return updown, ex_bias\n\n\n\n", "CodeBleu_score": 64.47916227113348}]}
{"instance_id": "c6a41665c62c", "generation_result": [{"code": "def calc_updown(self, orig_weight)\n\t\n    if orig_weight.device != self.weight.device:\n        self.weight = self.weight.to(orig_weight.device)\n        if self.bias is not None:\n            self.bias = self.bias.to(orig_weight.device)\n\n    return self.calc_updown_core(orig_weight)\n\n", "CodeBleu_score": 42.14752299692057}, {"code": "def calc_updown(self, orig_weight)\n\t\n    weight = orig_weight.to(self.weight.device)\n    if self.weight.shape == weight.shape:\n        return self.calc_updown_same(weight)\n\n    if self.weight.shape[1] == weight.shape[1]:\n        return self.calc_updown_in(weight)\n\n    if self.weight.shape[0] == weight.shape[0]:\n        return self.calc_updown_out(weight)\n\n    return self.calc_updown_full(weight)\n\n", "CodeBleu_score": 24.62808667332169}, {"code": "def calc_updown(self, orig_weight)\n\t\n    self.weight.data = orig_weight.data.to(self.weight.device)\n    self.weight.grad = orig_weight.grad.to(self.weight.device)\n    self.ex_bias.data = orig_weight.data.to(self.ex_bias.device)\n    self.ex_bias.grad = orig_weight.grad.to(self.ex_bias.device)\n    return self.calc_updown_weights(orig_weight)\n\n", "CodeBleu_score": 25.48810292470668}]}
{"instance_id": "1bb9e5d94af8", "generation_result": [{"code": "def purge_networks_from_memory()\n\t\n    if len(networks) > 300:\n        logging.info(f'Purging {len(networks) - 300} networks from memory.')\n        for _ in range(len(networks) - 300):\n            networks.popitem()\n        gc.collect()\n\n", "CodeBleu_score": 26.25794302923481}, {"code": "def purge_networks_from_memory()\n\t\n    if len(shared.networks) > 1000:\n        for network_key in list(shared.networks.keys()):\n            if network_key not in shared.networks:\n                continue\n            if shared.networks[network_key].in_use:\n                continue\n            del shared.networks[network_key]\n            gc.collect()\n\n", "CodeBleu_score": 25.265855600273245}, {"code": "def purge_networks_from_memory()\n\t\n    global networks\n    if len(networks) > shared.config.max_networks_in_memory:\n        logging.debug(f\\\"Purging {len(networks) - shared.config.max_networks_in_memory} networks from memory\\\")\n        networks = networks[shared.config.max_networks_in_memory:]\n        gc.collect()\n\n", "CodeBleu_score": 37.04977317679809}]}
{"instance_id": "42b48ebf4d07", "generation_result": [{"code": "def network_restore_weights_from_backup(self: Union[torch.nn.Conv2d, torch.nn.Linear, torch.nn.GroupNorm, torch.nn.LayerNorm, torch.nn.MultiheadAttention]):\n\t\n    if self.weight.grad is not None:\n        return\n    if hasattr(self, \\\"backup_weight\\\") and self.backup_weight is not None:\n        self.weight.data = self.backup_weight\n    if hasattr(self, \\\"backup_bias\\\") and self.backup_bias is not None:\n        self.bias.data = self.backup_bias\n    if isinstance(self, torch.nn.MultiheadAttention):\n        if hasattr(self, \\\"in_proj_weight_backup\\\") and self.in_proj_weight_backup is not None:\n            self.in_proj_weight.data = self.in_proj_weight_backup\n        if hasattr(self, \\\"in_proj_bias_backup\\\") and self.in_proj_bias_backup is not None:\n            self.in_proj_bias.data = self.in_proj_bias_backup\n\n\n\n", "CodeBleu_score": 35.21237515371311}, {"code": "def network_restore_weights_from_backup(self: Union[torch.nn.Conv2d, torch.nn.Linear, torch.nn.GroupNorm, torch.nn.LayerNorm, torch.nn.MultiheadAttention]):\n\t\n    if self.weight.grad is not None:\n        self.weight.grad.data.zero_()\n    if self.bias is not None and self.bias.grad is not None:\n        self.bias.grad.data.zero_()\n\n    if isinstance(self, torch.nn.Linear) or isinstance(self, torch.nn.Conv2d):\n        if self.weight.grad is not None:\n            self.weight.grad.data.copy_(self.weight_backup)\n        if self.bias is not None and self.bias.grad is not None:\n            self.bias.grad.data.copy_(self.bias_backup)\n    elif isinstance(self, torch.nn.LayerNorm) or isinstance(self, torch.nn.GroupNorm):\n        if self.weight.grad is not None:\n            self.weight.grad.data.copy_(self.weight_backup)\n        if self.bias is not None and self.bias.grad is not None:\n            self.bias.grad.data.copy_(self.bias_backup)\n    elif isinstance(self, torch.nn.MultiheadAttention):\n        if self.in_proj_weight.grad is not None:\n            self.in_proj_weight.grad.data.copy_(self.in_proj_weight_backup)\n        if self.in_proj_bias is not None and self.in_proj_bias.grad is not None:\n            self.in_proj_bias.grad.data.copy_(self.in_proj_bias_backup)\n        if self.out_proj.weight.grad is not None:\n            self.out_proj.weight.grad.data.copy_(self.out_proj_weight_backup)\n        if self.out_proj.bias is not None and self.out_proj.bias.grad is not None:\n            self.out_proj.bias.grad.data.copy_(self.out_proj_bias_backup)\n\n\n\n", "CodeBleu_score": 39.35683410914446}, {"code": "def network_restore_weights_from_backup(self: Union[torch.nn.Conv2d, torch.nn.Linear, torch.nn.GroupNorm, torch.nn.LayerNorm, torch.nn.MultiheadAttention]):\n\t\n    if not hasattr(self, \\\"__weights_backup__\\\") or not hasattr(self, \\\"__biases_backup__\\\") or self.__weights_backup__ is None or self.__biases_backup__ is None:\n        return\n\n    if isinstance(self, torch.nn.MultiheadAttention):\n        self.in_proj_weight.copy_(self.__weights_backup__[0].view(self.embed_dim, -1))\n        self.in_proj_bias.copy_(self.__biases_backup__[0].view(-1))\n\n        self.out_proj.weight.copy_(self.__weights_backup__[1].view(self.embed_dim, -1))\n        self.out_proj.bias.copy_(self.__biases_backup__[1].view(-1))\n\n    else:\n        self.weight.copy_(self.__weights_backup__)\n        self.bias.copy_(self.__biases_backup__)\n\n    del self.__weights_backup__\n    del self.__biases_backup__\n\n\n\n\n", "CodeBleu_score": 37.667716969444534}]}
{"instance_id": "c8fcd357131a", "generation_result": [{"code": "def network_forward(org_module, input, original_forward)\n\t\n\n    if loaded_networks is None:\n        return original_forward(org_module, input)\n\n    if not isinstance(input, tuple):\n        input = (input,)\n\n    if not isinstance(org_module, torch.nn.Module):\n        raise RuntimeError(f\\\"network_forward: {org_module} is not a torch.nn.Module\\\")\n\n    if not isinstance(org_module, (torch.nn.Conv2d, torch.nn.Linear, torch.nn.GroupNorm, torch.nn.LayerNorm, torch.nn.MultiheadAttention)):\n        raise RuntimeError(f\\\"network_forward: {org_module} is not a supported torch.nn layer\\\")\n\n    if not isinstance(input, tuple):\n        input = (input,)\n\n    if org_module.training:\n        network_apply_weights(org_module)\n\n    if not org_module.training:\n        network_restore_weights_from_backup(org_module)\n\n    if org_module.training:\n        return original_forward(org_module, *input)\n\n    if not isinstance(org_module, torch.nn.MultiheadAttention):\n        return original_forward(org_module, *input)\n\n    # MultiheadAttention is a special case\n    # First, we need to split the input into qkv\n    qkv = input[0]\n    if not isinstance(qkv, torch.Tensor):\n        raise RuntimeError(f\\\"network_forward: input[0] is not a torch.Tensor\\\")\n\n    if len(qkv.shape) != 3:\n        raise RuntimeError(f\\\"network_forward: input[0] is not a 3D tensor\\\")\n\n    if qkv.shape[0] != 1:\n        raise RuntimeError(f\\\"network_forward: input[0] batch size is not 1\\\")\n\n    qkv = qkv.squeeze(0)\n    qkv_split = qkv.split(qkv.shape[1] // 3, 1)\n    if len(qkv_split) != 3:\n        raise RuntimeError(f\\\"network_forward: input[0] is not a 3D tensor\\\")\n\n    q, k, v = qkv_split\n    q = q.contiguous()\n    k = k.contiguous()\n    v = v.contiguous()\n\n    # Next, we need to split the module's weight into qkv\n    weight = org_module.in_", "CodeBleu_score": 35.084546203016664}, {"code": "def network_forward(org_module, input, original_forward)\n\t\n\n    if loaded_networks is None or len(loaded_networks) == 0:\n        return original_forward(org_module, input)\n\n    # TODO: this doesn't work with multiple networks\n    # TODO: this doesn't work with multiple networks\n    # TODO: this doesn't work with multiple networks\n    # TODO: this doesn't work with multiple networks\n    # TODO: this doesn't work with multiple networks\n    # TODO: this doesn't work with multiple networks\n    # TODO: this doesn't work with multiple networks\n    # TODO: this doesn't work with multiple networks\n    # TODO: this doesn't work with multiple networks\n    # TODO: this doesn't work with multiple networks\n    # TODO: this doesn't work with multiple networks\n    # TODO: this doesn't work with multiple networks\n    # TODO: this doesn't work with multiple networks\n    # TODO: this doesn't work with multiple networks\n    # TODO: this doesn't work with multiple networks\n    # TODO: this doesn't work with multiple networks\n    # TODO: this doesn't work with multiple networks\n    # TODO: this doesn't work with multiple networks\n    # TODO: this doesn't work with multiple networks\n    # TODO: this doesn't work with multiple networks\n    # TODO: this doesn't work with multiple networks\n    # TODO: this doesn't work with multiple networks\n    # TODO: this doesn't work with multiple networks\n    # TODO: this doesn't work with multiple networks\n    # TODO: this doesn't work with multiple networks\n    # TODO: this doesn't work with multiple networks\n    # TODO: this doesn't work with multiple networks\n    # TODO: this doesn't work with multiple networks\n    # TODO: this doesn't work with multiple networks\n    # TODO: this doesn't work with multiple networks\n    # TODO: this doesn't work with multiple networks\n    # TODO: this doesn't work with multiple networks\n    # TODO: this doesn't work with multiple networks\n    # TODO: this doesn't work with multiple networks\n    # TODO: this doesn't work with multiple networks\n    # TODO: this doesn't work with multiple networks\n    # TODO: this doesn't work with multiple networks\n    # TODO: this doesn't work with multiple networks\n    # TODO: this doesn't work with multiple networks\n    # TODO: this doesn't work with multiple networks\n    # TODO: this doesn't work with multiple networks\n    # TODO: this doesn't work with multiple networks\n    # TODO: this doesn't work with multiple networks\n    # TODO: this doesn't work with", "CodeBleu_score": 14.831883440404637}, {"code": "def network_forward(org_module, input, original_forward)\n\t\n\n    def network_forward_hook(module, input, output):\n        if loaded_networks is None:\n            return original_forward(module, input, output)\n\n        network_layer_name = getattr(module, 'network_layer_name', None)\n        if network_layer_name is None:\n            return original_forward(module, input, output)\n\n        current_names = getattr(module, \"network_current_names\", ())\n        wanted_names = tuple((x.name, x.te_multiplier, x.unet_multiplier, x.dyn_dim) for x in loaded_networks)\n\n        if current_names != wanted_names:\n            network_restore_weights_from_backup(module)\n\n            for net in loaded_networks:\n                module = net.modules.get(network_layer_name, None)\n                if module is not None and hasattr(module, 'weight'):\n                    try:\n                        with torch.no_grad():\n                            if getattr(module, 'fp16_weight', None) is None:\n                                weight = module.weight\n                                bias = module.bias\n                            else:\n                                weight = module.fp16_weight.clone().to(module.weight.device)\n                                bias = getattr(module, 'fp16_bias', None)\n                                if bias is not None:\n                                    bias = bias.clone().to(module.bias.device)\n                            updown, ex_bias = module.calc_updown(weight)\n\n                            if len(weight.shape) == 4 and weight.shape[1] == 9:\n                                # inpainting model. zero pad updown to make channel[1]  4 to 9\n                                updown = torch.nn.functional.pad(updown, (0, 0, 0, 0, 0, 5))\n\n                            module.weight.copy_((weight.to(dtype=updown.dtype) + updown).to(dtype=module.weight.dtype))\n                            if ex_bias is not None and hasattr(module, 'bias'):\n                                if module.bias is None:\n                                    module.bias = torch.nn.Parameter(ex_bias).to(module.weight.dtype)\n                                else:\n                                    module.bias.copy_((bias + ex_bias).to(dtype=module.bias.dtype))\n                    except RuntimeError as e:\n                        logging.debug(f\"Network {net", "CodeBleu_score": 34.92574974578193}]}
{"instance_id": "1070211f5c11", "generation_result": [{"code": "def trainables(self)\n\t\n    return [p for p in self.parameters() if p.requires_grad]\n\n", "CodeBleu_score": 6.452447410697191}, {"code": "def trainables(self)\n\t\n    trainables = []\n    for layer in self.modules():\n        if isinstance(layer, torch.nn.Linear) or isinstance(layer, torch.nn.LayerNorm):\n            trainables.append(layer.weight)\n            if layer.bias is not None:\n                trainables.append(layer.bias)\n    return trainables\n\n", "CodeBleu_score": 38.05576294337368}, {"code": "def trainables(self)\n\t\n    trainables = []\n    for layer in self.children():\n        if type(layer) == torch.nn.Linear or type(layer) == torch.nn.LayerNorm:\n            trainables.append(layer.weight)\n            trainables.append(layer.bias)\n    return trainables\n\n", "CodeBleu_score": 55.83630322832156}]}
{"instance_id": "0c3ba62c96a7", "generation_result": [{"code": "def load_hypernetworks(names, multipliers=None)\n\t\n    if multipliers is None:\n        multipliers = [1] * len(names)\n    else:\n        assert len(multipliers) == len(names)\n\n    hypernetworks = []\n    for name, multiplier in zip(names, multipliers):\n        hypernetwork = load_hypernetwork(name)\n\n        if hypernetwork is not None:\n            hypernetwork.set_multiplier(multiplier)\n            hypernetworks.append(hypernetwork)\n\n    shared.hypernetworks = hypernetworks\n\n", "CodeBleu_score": 28.385635087165227}, {"code": "def load_hypernetworks(names, multipliers=None)\n\t\n    if multipliers is None:\n        multipliers = [1.0] * len(names)\n\n    hypernetworks = []\n\n    for i, name in enumerate(names):\n        multiplier = multipliers[i]\n\n        if name in shared.loaded_hypernetworks:\n            hypernetwork = shared.loaded_hypernetworks[name]\n            hypernetwork.set_multiplier(multiplier)\n        else:\n            hypernetwork = load_hypernetwork(name)\n            if hypernetwork is None:\n                return None\n            hypernetwork.set_multiplier(multiplier)\n\n        hypernetworks.append(hypernetwork)\n\n    shared.loaded_hypernetworks = []\n    for hypernetwork in hypernetworks:\n        shared.loaded_hypernetworks.append(hypernetwork)\n\n    return hypernetworks\n\n", "CodeBleu_score": 45.38760522159407}, {"code": "def load_hypernetworks(names, multipliers=None)\n\t\n    global hypernetworks\n    hypernetworks = []\n\n    for name in names:\n        hypernetwork = load_hypernetwork(name)\n        if hypernetwork is None:\n            continue\n\n        if multipliers is not None:\n            hypernetwork.set_multiplier(multipliers[name])\n\n        hypernetworks.append(hypernetwork)\n\n    return hypernetworks\n\n", "CodeBleu_score": 29.715776043254383}]}
{"instance_id": "bea3772ed532", "generation_result": [{"code": "\ndef apply_hypernetworks(hypernetworks, context, layer=None)\n\t\n    context_k, context_v = context, context\n    for hypernetwork in hypernetworks:\n        context_k, context_v = apply_single_hypernetwork(hypernetwork, context_k, context_v, layer=layer)\n    return context_k, context_v\n\n", "CodeBleu_score": 57.435688315169486}, {"code": "\ndef apply_hypernetworks(hypernetworks, context, layer=None)\n\t\n    context_k, context_v = context, context\n    for hypernetwork in hypernetworks:\n        context_k, context_v = apply_single_hypernetwork(hypernetwork, context_k, context_v, layer)\n    return context_k, context_v\n\n", "CodeBleu_score": 68.4402416707376}, {"code": "\ndef apply_hypernetworks(hypernetworks, context, layer=None)\n\t\n    context_k = context\n    context_v = context\n\n    for hypernetwork in hypernetworks:\n        context_k, context_v = apply_single_hypernetwork(hypernetwork, context_k, context_v, layer=layer)\n\n    return context_k, context_v\n\n", "CodeBleu_score": 66.82264135585403}]}
{"instance_id": "922efa3e092f", "generation_result": [{"code": "def attention_CrossAttention_forward(self, x, context=None, mask=None, **kwargs)\n\t\n    # pylint: disable=arguments-differ\n    if context is None:\n        context = x\n\n    # Project input and context.\n    q = self.query_net(x)\n    k = self.key_net(context)\n    v = self.value_net(context)\n\n    # Apply hypernetworks to context.\n    if self.hypernet is not None:\n        k, v = apply_hypernetworks(self.hypernet, k, v, layer=self)\n\n    # Reshape for multi-head attention.\n    q = self.reshape_qkv(q, self.num_heads)\n    k = self.reshape_qkv(k, self.num_heads)\n    v = self.reshape_qkv(v, self.num_heads)\n\n    # Apply attention.\n    attn_output, attn_output_weights = self.attention(q, k, v, mask=mask, dropout=self.dropout)\n\n    # Reshape back from multi-head attention.\n    attn_output = self.reshape_output(attn_output)\n\n    # Apply final linear layer.\n    attn_output = self.output_layer(attn_output)\n\n    return attn_output, attn_output_weights\n\n\n\n\n", "CodeBleu_score": 28.254003182897836}, {"code": "def attention_CrossAttention_forward(self, x, context=None, mask=None, **kwargs)\n\t\n    # x: (batch_size, seq_len, d_model)\n    # context: (batch_size, seq_len, d_model)\n    # mask: (batch_size, seq_len)\n    # output: (batch_size, seq_len, d_model)\n    if context is None:\n        context = x\n    if mask is None:\n        mask = torch.ones(x.shape[0], x.shape[1], dtype=x.dtype, device=x.device)\n\n    # Project queries, keys, and values.\n    q = self.query_net(x)\n    k = self.key_net(context)\n    v = self.value_net(context)\n\n    # Apply hypernetworks to keys and values.\n    k, v = apply_hypernetworks(self.hypernetworks, k, v)\n\n    # Reshape for multi-head attention.\n    q, k, v = self.reshape_for_multihead(q, k, v)\n\n    # Apply multi-head attention.\n    a = self.multihead_attention(q, k, v, mask=mask)\n\n    # Apply dropout.\n    a = self.dropout(a)\n\n    # Apply residual.\n    a = self.residual_net(x, a)\n\n    # Apply final linear layer.\n    a = self.output_linear(a)\n\n    return a\n\n", "CodeBleu_score": 32.04808504912083}, {"code": "def attention_CrossAttention_forward(self, x, context=None, mask=None, **kwargs)\n\t\n    if context is None:\n        context = x\n    if mask is None:\n        mask = torch.ones(x.shape[0], x.shape[1], x.shape[1], device=x.device)\n    if self.key_projection is not None:\n        q = self.query_projection(x)\n        k = self.key_projection(context)\n        v = self.value_projection(context)\n    else:\n        q = self.query_projection(x)\n        k = self.key_projection(x)\n        v = self.value_projection(x)\n    q = q.view(q.shape[0], q.shape[1], self.heads, self.head_dim)\n    k = k.view(k.shape[0], k.shape[1], self.heads, self.head_dim)\n    v = v.view(v.shape[0], v.shape[1], self.heads, self.head_dim)\n    q = q.permute(0, 2, 1, 3)\n    k = k.permute(0, 2, 3, 1)\n    v = v.permute(0, 2, 1, 3)\n    if self.hyper_k is not None:\n        context_k = context.view(context.shape[0], context.shape[1], self.heads, self.head_dim)\n        context_k = context_k.permute(0, 2, 1, 3)\n        context_k = self.hyper_k(context_k)\n        context_k = context_k.permute(0, 2, 1, 3)\n        context_k = context_k.view(context_k.shape[0], context_k.shape[1], self.context_dim)\n        k = k + context_k\n    if self.hyper_v is not None:\n        context_v = context.view(context.shape[0], context.shape[1], self.heads, self.head_dim)\n        context_v = context_v.permute(0, 2, 1, 3)\n        context", "CodeBleu_score": 30.611444288275536}]}
{"instance_id": "8b7455a03a0e", "generation_result": [{"code": "def save_hypernetwork(hypernetwork, checkpoint, hypernetwork_name, filename)\n\t\n    old_checkpoint = hypernetwork.checkpoint\n    old_name = hypernetwork.name\n    old_step = hypernetwork.step\n    hypernetwork.checkpoint = checkpoint\n    hypernetwork.name = hypernetwork_name\n    hypernetwork.step = checkpoint.step\n    try:\n        hypernetwork.save(filename)\n    except Exception:\n        hypernetwork.checkpoint = old_checkpoint\n        hypernetwork.name = old_name\n        hypernetwork.step = old_step\n        hypernetwork.save(filename)\n        raise\n    finally:\n        hypernetwork.checkpoint = old_checkpoint\n        hypernetwork.name = old_name\n        hypernetwork.step = old_step\n\n\n\n", "CodeBleu_score": 39.62463770540799}, {"code": "def save_hypernetwork(hypernetwork, checkpoint, hypernetwork_name, filename)\n\t\n    old_checkpoint = hypernetwork.checkpoint\n    old_name = hypernetwork.name\n    old_optimizer_name = hypernetwork.optimizer_name\n    old_optimizer_state_dict = hypernetwork.optimizer_state_dict\n    try:\n        hypernetwork.checkpoint = checkpoint\n        hypernetwork.name = hypernetwork_name\n        hypernetwork.optimizer_name = None\n        hypernetwork.optimizer_state_dict = None\n        hypernetwork.save(filename)\n    except Exception:\n        hypernetwork.checkpoint = old_checkpoint\n        hypernetwork.name = old_name\n        hypernetwork.optimizer_name = old_optimizer_name\n        hypernetwork.optimizer_state_dict = old_optimizer_state_dict\n        raise\n    finally:\n        hypernetwork.checkpoint = old_checkpoint\n        hypernetwork.name = old_name\n        hypernetwork.optimizer_name = old_optimizer_name\n        hypernetwork.optimizer_state_dict = old_optimizer_state_dict\n\n\n\n", "CodeBleu_score": 36.04210994463258}, {"code": "def save_hypernetwork(hypernetwork, checkpoint, hypernetwork_name, filename)\n\t\n    try:\n        old_checkpoint = hypernetwork.checkpoint\n        old_name = hypernetwork.name\n        hypernetwork.checkpoint = checkpoint\n        hypernetwork.name = hypernetwork_name\n        torch.save(hypernetwork, filename)\n    except Exception:\n        hypernetwork.checkpoint = old_checkpoint\n        hypernetwork.name = old_name\n        raise\n    finally:\n        hypernetwork.checkpoint = old_checkpoint\n        hypernetwork.name = old_name\n\n\n", "CodeBleu_score": 32.857888146425864}]}
{"instance_id": "3175f835140f", "generation_result": [{"code": "def ema_scope(self, context=None)\n\t\n    if not self.use_ema:\n        yield\n        return\n    # If EMA is enabled, copy EMA weights to the model and log this action.\n    if context is not None:\n        context.save_for_ema()\n    self.model_ema.store(self.model)\n    self.model_ema.copy_to(self.model)\n    yield\n    # After the code block execution, restore the original weights and log the restoration.\n    self.model_ema.restore(self.model)\n    if context is not None:\n        context.restore_after_ema()\n\n", "CodeBleu_score": 48.569141025739754}, {"code": "def ema_scope(self, context=None)\n\t\n    if self.use_ema:\n        context = context or {}\n        if self.training:\n            if self.use_ema:\n                # store model weights\n                context['model_weights'] = copy.deepcopy(self.model.state_dict())\n                # copy ema weights to model\n                self.model.load_state_dict(self.model_ema.state_dict())\n            if self.monitor is not None:\n                context['monitor_values'] = copy.deepcopy(self.monitor.get_values())\n        yield\n        if self.training:\n            if self.use_ema:\n                # restore model weights\n                self.model.load_state_dict(context['model_weights'])\n                del context['model_weights']\n                # copy model weights to ema\n                self.model_ema.load_state_dict(self.model.state_dict())\n            if self.monitor is not None:\n                self.monitor.set_values(context['monitor_values'])\n                del context['monitor_values']\n    else:\n        yield\n\n", "CodeBleu_score": 40.27458339920918}, {"code": "def ema_scope(self, context=None)\n\t\n    if not self.use_ema:\n        yield\n        return\n    if context is None:\n        context = get_context()\n    if not context.is_training:\n        yield\n        return\n    if context.is_ema_enabled:\n        yield\n        return\n    context.is_ema_enabled = True\n    if self.use_ema:\n        if context.is_ema_enabled:\n            if context.is_ema_enabled:\n                if context.is_ema_enabled:\n                    if context.is_ema_enabled:\n                        if context.is_ema_enabled:\n                            if context.is_ema_enabled:\n                                if context.is_ema_enabled:\n                                    if context.is_ema_enabled:\n                                        if context.is_ema_enabled:\n                                            if context.is_ema_enabled:\n                                                if context.is_ema_enabled:\n                                                    if context.is_ema_enabled:\n                                                        if context.is_ema_enabled:\n                                                            if context.is_ema_enabled:\n                                                                if context.is_ema_enabled:\n                                                                    if context.is_ema_enabled:\n                                                                        if context.is_ema_enabled:\n                                                                            if context.is_ema_enabled:\n                                                                                if context.is_ema_enabled:\n                                                                                    if context.is_ema_enabled:\n                                                                                        if context.is_ema_enabled:\n                                                                                            if context.is_ema_enabled:\n                                                                                                if context.is_ema_enabled:\n                                                                                                    if context.is_ema_enabled:\n                                                                                                        if context.is_ema_enabled:\n                                                                                                            if context.is_ema_enabled:\n                                                                                                                if context.is_ema_enabled:\n                                                                                                                    if context.is_ema_enabled:\n                                                                                                                        if context.is_ema_enabled:\n                                                                                                                            if context.is_ema_enabled:\n                                                                                                                                if context.is_ema_enabled:\n                                                                                                                                    if context.is_ema_enabled:\n                                                                                                                                        if context.is_ema_enabled:\n                                                                                                                                            if context.is_ema_enabled:\n                                                                                                                                                if context.is_ema_enabled:\n                                                                                                                                                    if context.is_ema_enabled:\n                                                                                                                                                        if context.is_ema_enabled:\n                                                                                                                                                            if context.is_ema_enabled:\n                                                                                                                                                                if context.is_ema_enabled:\n                                                                                                                                                                    if context.is_ema_enabled:\n                                                                                                                                                                        if context.", "CodeBleu_score": 22.792188746927785}]}
{"instance_id": "a7b5d5cd519a", "generation_result": [{"code": "def p_mean_variance(self, x, t, clip_denoised: bool)\n\t\n    assert x.shape == self.x_shape, f\\\"Input has wrong shape. Expected {self.x_shape}, got {x.shape}.\\\"\n    x_recon = self.predict_start_from_noise(x, t)\n    if clip_denoised:\n        x_recon.clamp_(-1., 1.)\n    model_mean, posterior_variance, posterior_log_variance = self.q_posterior(x_recon, x, t)\n    return model_mean, posterior_variance, posterior_log_variance\n\n\n\n", "CodeBleu_score": 42.77394219881094}, {"code": "def p_mean_variance(self, x, t, clip_denoised: bool)\n\t\n    assert self.training is False, \\\"p_mean_variance only defined during inference.\\\"\n    x_recon = self.predict_start_from_noise(x, t=t, noise=None)\n    if clip_denoised:\n        x_recon.clamp_(-1., 1.)\n    model_mean, posterior_variance, posterior_log_variance = self.q_posterior(x_start=x_recon, x_t=x, t=t)\n    return model_mean, posterior_variance, posterior_log_variance\n\n", "CodeBleu_score": 57.43205440177469}, {"code": "def p_mean_variance(self, x, t, clip_denoised: bool)\n\t\n    assert x.shape == self.x_shape, f'x has wrong shape. Found {x.shape}, expected {self.x_shape}'\n    assert t.shape == self.t_shape, f't has wrong shape. Found {t.shape}, expected {self.t_shape}'\n    x = x.to(self.device)\n    t = t.to(self.device)\n    model_output = self.model(x, t)\n    if self.parameterization == 'eps':\n        assert model_output.shape == self.eps_shape, f'Model output has wrong shape. Found {model_output.shape}, expected {self.eps_shape}'\n        eps, rest = model_output.split([self.num_channels, self.num_channels], dim=1)\n        x_recon = self.predict_start_from_noise(x, t, eps)\n    elif self.parameterization == 'x0':\n        assert model_output.shape == self.x_shape, f'Model output has wrong shape. Found {model_output.shape}, expected {self.x_shape}'\n        x_recon = model_output\n    else:\n        raise NotImplementedError('Parameterization not supported')\n    if clip_denoised:\n        x_recon.clamp_(-1., 1.)\n    model_mean, posterior_variance, posterior_log_variance = self.q_posterior(x_start=x, x_t=x_recon, t=t)\n    return model_mean, posterior_variance, posterior_log_variance\n\n", "CodeBleu_score": 56.658493718093794}]}
{"instance_id": "a8652fa78473", "generation_result": [{"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss_type == \\\"l1\\\":\n        loss = torch.abs(pred - target)\n    elif self.loss_type == \\\"l2\\\":\n        loss = F.mse_loss(pred, target, reduction=\\\"none\\\")\n    else:\n        raise NotImplementedError(f\\\"Unknown loss type: {self.loss_type}\\\")\n    if mean:\n        return loss.mean()\n    return loss\n\n\n\n", "CodeBleu_score": 30.115469569861197}, {"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss_type == \\\"l1\\\":\n        loss = torch.abs(pred - target)\n    elif self.loss_type == \\\"l2\\\":\n        loss = (pred - target).square()\n    else:\n        raise NotImplementedError(f\\\"Unknown loss_type '{self.loss_type}'\\\")\n    if mean:\n        loss = loss.mean()\n    return loss\n\n    def forward(self, x, t, noise=None, return_pred_xstart=False):\n        noise = default(noise, lambda: torch.randn_like(x))\n        x_recon = self.q_sample(x_start=x, t=t, noise=noise)\n        if return_pred_xstart:\n            return self.p_mean_variance(x_start=x_recon, t=t, clip_denoised=self.clip_denoised), x_recon\n        return self.p_mean_variance(x_start=x_recon, t=t, clip_denoised=self.clip_denoised)\n\n    @torch.no_grad()\n    def p_losses(self, x_start, t, noise=None):\n        noise = default(noise, lambda: torch.randn_like(x_start))\n        x_noisy = self.q_sample(x_start=x_start, t=t, noise=noise)\n        x_recon = self.p_sample(x_noisy, t=t, clip_denoised=self.clip_denoised)\n        return self.get_loss(x_recon, x_start), x_recon\n\n    def forward_with_losses(self, x, t, noise=None):\n        noise = default(noise, lambda: torch.randn_like(x))\n        x_recon = self.q_sample(x_start=x, t=t, noise=noise)\n        loss = self.get_loss(x_recon, x)\n        return loss, x_recon\n\n    def q_losses(self, x_start, t, noise=None):\n        noise = default(noise, lambda: torch.randn_like(x_start))\n       ", "CodeBleu_score": 33.51025918952602}, {"code": "def get_loss(self, pred, target, mean=True)\n\t\n    if self.loss_type == \\\"l1\\\":\n        loss = torch.abs(pred - target)\n    elif self.loss_type == \\\"l2\\\":\n        loss = (pred - target).pow(2)\n        if not mean:\n            loss = loss.sum(dim=(1, 2, 3))\n    else:\n        raise NotImplementedError(f\\\"Unknown loss type: {self.loss_type}\\\")\n\n    if mean:\n        loss = loss.mean()\n    return loss\n\n    @torch.no_grad()\n    def forward(self, input, t, noise=None, return_pred_xstart=False):\n        assert noise is not None, \\\"Noise must be provided. If you want to sample from the prior instead, use model.sample()\\\"\n        x_recon = self.q_sample(input, t=t, noise=noise)\n        loss = self.get_loss(x_recon, input)\n        if return_pred_xstart:\n            return loss, x_recon\n        return loss\n\n    def forward_with_parameter_count(self):\n        return self.forward(torch.randn(2, 3, self.image_size, self.image_size, device=self.betas.device))\n\n    def sample_from_noise(self, noise=None):\n        noise = default(noise, lambda: torch.randn(1, 1, self.image_size, self.image_size, device=self.betas.device))\n        return self.p_sample(noise, torch.full((1,), self.num_timesteps - 1, device=self.betas.device, dtype=torch.long))\n\n    @torch.no_grad()\n    def interpolate(self, x1, x2, t=None, lam=0.5, clip_denoised=True, return_pred_xstart=False):\n        b, *_, device = *x1.shape, x1.device\n        t = default(t, self.num_timesteps - 1)\n        assert x1.shape == x2.shape\n        t_batched = torch.stack([torch.tensor(t, device=device", "CodeBleu_score": 36.72003714053487}]}
{"instance_id": "d68e4f061349", "generation_result": [{"code": "def _prior_bpd(self, x_start)\n\t\n    batch_size = x_start.shape[0]\n    t = torch.full((batch_size,), self.num_timesteps - 1, device=self.device, dtype=torch.long)\n    return self._prior_bpd_t(x_start, t)\n\n", "CodeBleu_score": 19.68621984969244}, {"code": "def _prior_bpd(self, x_start)\n\t\n    # determine batch size and create tensor t representing the final timestep for each batch element\n    batch_size = x_start.shape[0]\n    t = torch.full((batch_size,), self.num_timesteps - 1, device=self.device, dtype=torch.long)\n    return self._prior_bpd_t(x_start=x_start, t=t)\n\n", "CodeBleu_score": 20.820523669614328}, {"code": "def _prior_bpd(self, x_start)\n\t\n    batch_size = x_start.shape[0]\n    t = torch.full((batch_size,), self.num_timesteps - 1, device=x_start.device, dtype=torch.long)\n    return self.kl_divergence(x_start, t).div(math.log(2.)).mean().item()\n\n", "CodeBleu_score": 21.68535670156872}]}
{"instance_id": "c695d93fdf2d", "generation_result": [{"code": "def make_cache(subsection: str) -> diskcache.Cache\n\t\n    cache_filename = os.environ.get('SD_WEBUI_CACHE_FILE', os.path.join(data_path, f\"cache.{subsection}.json\"))\n    cache_dir = os.environ.get('SD_WEBUI_CACHE_DIR', os.path.join(data_path, f\"cache.{subsection}\"))\n    caches[subsection] = diskcache.Cache(cache_dir, size_limit=4 * 1024 ** 3, disk_min_file_size=256)\n    return caches[subsection]\n\n", "CodeBleu_score": 44.678076327698605}, {"code": "def make_cache(subsection: str) -> diskcache.Cache\n\t\n    cache_dir = os.environ.get(\"SD_WEBUI_CACHE_DIR\", os.path.join(data_path, \"cache\"))\n    os.makedirs(cache_dir, exist_ok=True)\n    cache = diskcache.Cache(\n        os.path.join(cache_dir, f\"{subsection}.db\"),\n        size_limit=4 * 1024 ** 3,\n        disk_min_file_size=256,\n    )\n    return cache\n\n", "CodeBleu_score": 49.15804348283269}, {"code": "def make_cache(subsection: str) -> diskcache.Cache\n\t\n    cache = diskcache.Cache(\n        os.path.join(cache_dir, subsection),\n        size_limit=4 * 1024 ** 3,\n        disk=diskcache.JSONDisk,\n        disk_min_file_size=256,\n    )\n    return cache\n\n", "CodeBleu_score": 59.84110767504388}]}
{"instance_id": "ed4c543f0d38", "generation_result": [{"code": "def cache(subsection)\n\t\n    pass\n", "CodeBleu_score": 0.5319148936170213}, {"code": "def cache(subsection)\n\t\n    pass\n", "CodeBleu_score": 0.5319148936170213}, {"code": "def cache(subsection)\n\t\n    pass\n\n", "CodeBleu_score": 0.5319148936170213}]}
{"instance_id": "ccf5ea2d9fc1", "generation_result": [{"code": "def cached_data_for_file(subsection, title, filename, func)\n\t\n    cache_obj = cache(subsection)\n    mtime = os.path.getmtime(filename)\n    cache_entry = cache_obj.get(title)\n    if cache_entry and cache_entry[0] >= mtime:\n        return cache_entry[1]\n    else:\n        new_data = func()\n        cache_obj[title] = (mtime, new_data)\n        dump_cache()\n        return new_data\n\n", "CodeBleu_score": 14.319810563325147}, {"code": "def cached_data_for_file(subsection, title, filename, func)\n\t\n\n    cache_obj = cache(subsection)\n    mtime = os.path.getmtime(filename)\n    cached_mtime = cache_obj.get(title, {}).get(\\\"mtime\\\", None)\n\n    if cached_mtime is not None and mtime <= cached_mtime:\n        return cache_obj.get(title, {}).get(\\\"data\\\", None)\n\n    data = func()\n    cache_obj[title] = {\\\"data\\\": data, \\\"mtime\\\": mtime}\n    dump_cache()\n\n    return data\n\n", "CodeBleu_score": 25.868239963507545}, {"code": "def cached_data_for_file(subsection, title, filename, func)\n\t\n\n    cache_obj = cache(subsection)\n    mtime = os.path.getmtime(filename)\n\n    if title in cache_obj:\n        cached_mtime = cache_obj[title][\"mtime\"]\n        if cached_mtime >= mtime:\n            return cache_obj[title][\"data\"]\n\n    data = func()\n    cache_obj[title] = {\"mtime\": mtime, \"data\": data}\n    dump_cache()\n    return data\n\n", "CodeBleu_score": 26.302962127215203}]}
{"instance_id": "e01a2d4f0ac2", "generation_result": [{"code": "def report(message: str, *, exc_info: bool = False) -> None\n\t\n    record_exception()\n    for line in message.splitlines():\n        print(f\"*** {line}\", file=sys.stderr)\n    if exc_info:\n        for filename, lineno, name, line in format_traceback(sys.exc_info()[2]):\n            print(f\"    {filename}, line {lineno}, {name}\", file=sys.stderr)\n        print(\"---\", file=sys.stderr)\n\n", "CodeBleu_score": 51.73966172558108}, {"code": "def report(message: str, *, exc_info: bool = False) -> None\n\t\n    record_exception()\n    for line in message.splitlines():\n        print(f\"*** {line}\", file=sys.stderr)\n    if exc_info:\n        print(textwrap.indent(format_traceback(sys.exc_info()[2]), \"    \"), file=sys.stderr)\n        print(\"---\", file=sys.stderr)\n\n", "CodeBleu_score": 60.04033904851287}, {"code": "def report(message: str, *, exc_info: bool = False) -> None\n\t\n    record_exception()\n    message = textwrap.dedent(message)\n    for line in message.splitlines():\n        print(f\"*** {line}\", file=sys.stderr)\n    if exc_info:\n        for line in format_traceback(sys.exc_info()[2]):\n            print(f\"    {line[0]}: {line[1]}\", file=sys.stderr)\n        print(\"---\", file=sys.stderr)\n", "CodeBleu_score": 50.06619169745409}]}
{"instance_id": "125fb9d2320b", "generation_result": [{"code": "def extract_original_prompts(style: PromptStyle, prompt, negative_prompt)\n\t\n\n    if style.prompt is None and style.negative_prompt is None:\n        return False, prompt, negative_prompt\n\n    if style.prompt is not None:\n        prompt_success, prompt = extract_style_text_from_prompt(style.prompt, prompt)\n        if not prompt_success:\n            return False, prompt, negative_prompt\n\n    if style.negative_prompt is not None:\n        negative_prompt_success, negative_prompt = extract_style_text_from_prompt(style.negative_prompt, negative_prompt)\n        if not negative_prompt_success:\n            return False, prompt, negative_prompt\n\n    return True, prompt, negative_prompt\n\n", "CodeBleu_score": 67.03764676325663}, {"code": "def extract_original_prompts(style: PromptStyle, prompt, negative_prompt)\n\t\n\n    if style.prompt is None and style.negative_prompt is None:\n        return False, prompt, negative_prompt\n\n    if style.prompt is None:\n        prompt_extracted = True\n        prompt = \"\"\n    else:\n        prompt_extracted, prompt = extract_style_text_from_prompt(style.prompt, prompt)\n\n    if style.negative_prompt is None:\n        negative_prompt_extracted = True\n        negative_prompt = \"\"\n    else:\n        negative_prompt_extracted, negative_prompt = extract_style_text_from_prompt(style.negative_prompt, negative_prompt)\n\n    return prompt_extracted and negative_prompt_extracted, prompt, negative_prompt\n\n", "CodeBleu_score": 51.1208760730196}, {"code": "def extract_original_prompts(style: PromptStyle, prompt, negative_prompt)\n\t\n    if style.prompt == None and style.negative_prompt == None:\n        return False, prompt, negative_prompt\n\n    if style.prompt != None:\n        success, prompt = extract_style_text_from_prompt(style.prompt, prompt)\n        if not success:\n            return False, prompt, negative_prompt\n\n    if style.negative_prompt != None:\n        success, negative_prompt = extract_style_text_from_prompt(style.negative_prompt, negative_prompt)\n        if not success:\n            return False, prompt, negative_prompt\n\n    return True, prompt, negative_prompt\n\n", "CodeBleu_score": 66.49091781398386}]}
{"instance_id": "0c5f729fbce2", "generation_result": [{"code": "def random_partial(self, count, n_frames)\n\t\n    if self.utterances is None:\n        self._load_utterances()\n    return [(u, u.random_partial(n_frames)) for u in self.utterance_cycler.sample(count)]\n\n", "CodeBleu_score": 50.439362861649606}, {"code": "def random_partial(self, count, n_frames)\n\t\n    if self.utterances is None:\n        self._load_utterances()\n    return [(u, u.random_partial(n_frames)) for u in self.utterance_cycler.sample(count)]\n\n", "CodeBleu_score": 50.439362861649606}, {"code": "def random_partial(self, count, n_frames)\n\t\n    if self.utterances is None:\n        self._load_utterances()\n    return [(u, u.random_partial(n_frames)) for u in self.utterance_cycler.sample(count)]\n\n", "CodeBleu_score": 50.439362861649606}]}
{"instance_id": "31e8b27b8f94", "generation_result": [{"code": "def normalize_volume(wav, target_dBFS, increase_only=False, decrease_only=False)\n\t\n    if increase_only and decrease_only:\n        raise ValueError(\"Both increase only and decrease only are set\")\n    dBFS_change = target_dBFS - 10 * np.log10(np.mean(wav ** 2))\n    if (dBFS_change < 0 and increase_only) or (dBFS_change > 0 and decrease_only):\n        return wav\n    return wav * (10 ** (dBFS_change / 20))\n\n", "CodeBleu_score": 96.28790819755086}, {"code": "def normalize_volume(wav, target_dBFS, increase_only=False, decrease_only=False)\n\t\n    if increase_only and decrease_only:\n        raise ValueError(\"Both increase only and decrease only are set\")\n    dBFS_change = target_dBFS - 10 * np.log10(np.mean(wav ** 2))\n    if (dBFS_change < 0 and increase_only) or (dBFS_change > 0 and decrease_only):\n        return wav\n    return wav * (10 ** (dBFS_change / 20))\n\n", "CodeBleu_score": 96.28790819755086}, {"code": "def normalize_volume(wav, target_dBFS, increase_only=False, decrease_only=False)\n\t\n    if increase_only and decrease_only:\n        raise ValueError(\"Both increase only and decrease only are set\")\n    dBFS_change = target_dBFS - 10 * np.log10(np.mean(wav ** 2))\n    if (dBFS_change < 0 and increase_only) or (dBFS_change > 0 and decrease_only):\n        return wav\n    return wav * (10 ** (dBFS_change / 20))\n", "CodeBleu_score": 96.28790819755086}]}
{"instance_id": "2353831e9033", "generation_result": [{"code": "def load_model(weights_fpath: Path, device=None)\n\t\n    global _model, _device\n    _device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    _model = SpeakerEncoder(device=_device)\n    _model.load_state_dict(torch.load(weights_fpath))\n    _model.eval()\n    print(f\"Loaded model: {_model}", "CodeBleu_score": 25.851826684689698}, {"code": "def load_model(weights_fpath: Path, device=None)\n\t\n\n    global _model\n    global _device\n\n    if device is None:\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # Instantiate model.\n    _model = SpeakerEncoder()\n    _model.load_state_dict(torch.load(weights_fpath))\n    _model.eval()\n    _model.to(device)\n    print(\"Initialized encoder model with {} trainable parameters.\".format(\n        sum([p.numel() for p in _model.parameters() if p.requires_grad])))\n\n    _device = device\n\n", "CodeBleu_score": 45.059702827502115}, {"code": "def load_model(weights_fpath: Path, device=None)\n\t\n    global _model, _device\n    _device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    _model = SpeakerEncoder(_device)\n    _model.load_state_dict(torch.load(weights_fpath, map_location=\"cpu\")[\"model\"])\n    _model.eval()\n    print(f\"Loaded model: {_model}\")\n\n", "CodeBleu_score": 26.61284231966742}]}
{"instance_id": "3231ad480199", "generation_result": [{"code": "def embed_frames_batch(frames_batch)\n\t\n    if _model is None:\n        raise Exception(\"Model not yet loaded! Did you forget to call load_model()?\")\n    frames_batch = torch.from_numpy(frames_batch).to(_device)\n    embeds = _model.forward(frames_batch).detach().cpu().numpy()\n    return embeds\n\n", "CodeBleu_score": 70.38113732417428}, {"code": "def embed_frames_batch(frames_batch)\n\t\n    if not is_loaded():\n        raise Exception(\"Model must be loaded before running inference.\")\n    frames_batch = torch.from_numpy(frames_batch).to(_device)\n    embeds = _model.forward(frames_batch)\n    embeds = embeds.detach().cpu().numpy()\n    return embeds\n\n", "CodeBleu_score": 58.23911151883722}, {"code": "def embed_frames_batch(frames_batch)\n\t\n    # Check that the model is loaded\n    if not is_loaded():\n        raise Exception(\"You must load the model before running inference.\")\n\n    # Convert the NumPy array to a PyTorch tensor\n    frames_batch = torch.from_numpy(frames_batch).to(_device)\n\n    # Perform the forward pass\n    with torch.no_grad():\n        embeds = _model.forward(frames_batch)\n\n    # Detach the result from the computation graph\n    embeds = embeds.detach().cpu().numpy()\n\n    return embeds\n\n", "CodeBleu_score": 55.98497007602759}]}
{"instance_id": "117a41716c8d", "generation_result": [{"code": "def compute_partial_slices(n_samples, partial_utterance_n_frames=partials_n_frames,\n                           min_pad_coverage=0.75, overlap=0.5)\n\t\n    assert 0.0 <= min_pad_coverage <= 1.0, \\\"min_pad_coverage must be between 0.0 and 1.0, got %f\\\" % min_pad_coverage\n    assert 0.0 < overlap < 1.0, \\\"overlap must be between 0.0 and 1.0, got %f\\\" % overlap\n    assert n_samples > 0, \\\"n_samples must be greater than 0, got %d\\\" % n_samples\n    assert partial_utterance_n_frames > 0, \\\"partial_utterance_n_frames must be greater than 0, got %d\\\" % partial_utterance_n_frames\n    # Compute the number of frames and frame step, based on the sampling rate and overlap.\n    frames_per_second = sampling_rate / frame_shift_ms\n    frames_per_partial = int(round(partial_utterance_n_frames * frames_per_second))\n    frame_step = int(round(frames_per_partial * (1.0 - overlap)))\n    assert frame_step > 0, \\\"frame_step must be greater than 0, got %d\\\" % frame_step\n    # Compute the slices, based on frame_step.\n    wav_slices = []\n    mel_slices = []\n    steps_per_second = 1.0 / frame_shift_ms\n    for i in range(0, n_samples - partial_utterance_n_frames + 1, frame_step):\n        mel_slices.append((i, i + partial_utterance_n_frames))\n        wav_slices.append((int(i * steps_per_second), int((i + partial_utterance_n_frames) * steps_per_second)))\n    # Evaluate if extra padding is required.\n    last_wav_slice = wav_slices[-1]\n    last_mel_slice = mel_slices[-1]\n    coverage = (last_wav_slice[1] - last_wav_slice[0]) / float(n_samples)\n    if coverage < min_pad_coverage and len(wav_slices) > 1:\n        # Remove the last slice.\n        wav_slices = wav_slices[:-1]\n        mel_slices = mel_slices[:-1]\n    # Return the slices.\n    return wav_", "CodeBleu_score": 35.72372218234351}, {"code": "def compute_partial_slices(n_samples, partial_utterance_n_frames=partials_n_frames,\n                           min_pad_coverage=0.75, overlap=0.5)\n\t\n    assert overlap >= 0 and overlap < 1, \\\"Overlap must be >=0 and <1. Got %f\\\" % overlap\n    assert min_pad_coverage >= 0 and min_pad_coverage <= 1, \\\"Min pad coverage must be >=0 and <=1. Got %f\\\" % min_pad_coverage\n    assert partial_utterance_n_frames > 0, \\\"Partial utterance n frames must be >0. Got %d\\\" % partial_utterance_n_frames\n    assert n_samples > 0, \\\"n samples must be >0. Got %d\\\" % n_samples\n    frame_step = int(np.ceil(n_samples * (1 - overlap)))\n    n_frames = int(np.ceil(1 + (n_samples - partial_utterance_n_frames) / frame_step))\n    assert n_frames > 0, \\\"Computed n frames must be >0. Got %d\\\" % n_frames\n    assert n_frames * frame_step + partial_utterance_n_frames >= n_samples, \\\"Slice length (%d) + frame step (%d) > total samples (%d). Got %d + %d > %d\\\" % (partial_utterance_n_frames, frame_step, n_samples, n_frames, frame_step, n_samples)\n    wav_slices = []\n    mel_slices = []\n    for i in range(n_frames):\n        mel_range = np.array([i * frame_step, i * frame_step + partial_utterance_n_frames])\n        mel_slices.append(slice(*mel_range))\n        wav_range = mel_range * audio_sample_rate / partials_n_frames\n        wav_slices.append(slice(*wav_range))\n    coverage = (wav_slices[-1].stop - wav_slices[-1].start) / n_samples\n    if coverage < min_pad_coverage and len(mel_slices) > 1:\n        mel_slices = mel_slices[:-1]\n        wav_slices = wav_slices[:-1]\n    return wav_slices, mel_slices\n\n", "CodeBleu_score": 43.31138696653552}, {"code": "def compute_partial_slices(n_samples, partial_utterance_n_frames=partials_n_frames,\n                           min_pad_coverage=0.75, overlap=0.5)\n\t\n    assert 0.0 < overlap < 1.0, \"Overlap must be between 0.0 and 1.0\"\n    assert 0.0 < min_pad_coverage < 1.0, \"Minimum coverage must be between 0.0 and 1.0\"\n    assert partial_utterance_n_frames > 0, \"Number of frames per partial utterance must be greater than 0\"\n    assert n_samples > 0, \"Number of samples must be greater than 0\"\n    # Compute number of frames and frame step\n    frames_per_second = hparams.sample_rate / hparams.hop_size\n    partial_utterance_n_samples = partial_utterance_n_frames * hparams.hop_size\n    frame_step = int(partial_utterance_n_samples * (1.0 - overlap))\n    assert frame_step > 0, \"Frame step must be greater than 0\"\n    # Compute number of steps, ensure last partial utterance has full padding coverage\n    n_steps = 1 + int((n_samples - partial_utterance_n_samples) / frame_step)\n    assert n_steps > 0, \"Number of steps must be greater than 0\"\n    last_partial_utterance_n_samples = n_samples - (n_steps - 1) * frame_step\n    last_partial_utterance_n_frames = int(np.ceil(last_partial_utterance_n_samples / hparams.hop_size))\n    last_partial_utterance_coverage = last_partial_utterance_n_frames / partial_utterance_n_frames\n    assert last_partial_utterance_coverage >= min_pad_coverage, \"Last partial utterance coverage must be at least %f\" % min_pad_coverage\n    # Compute wav and mel slices\n    wav_slices = []\n    mel_slices = []\n    for i in range(n_steps):\n        mel_range = np.array([i * frame_step, i * frame_step + partial_utterance_n_samples])\n        mel_slices.append(slice(*mel_range))\n        wav_range = mel_range * hparams.hop_size / hparams.sample_rate\n        wav_slices.append(slice(*wav_range))\n    # If last", "CodeBleu_score": 30.917396707083107}]}
{"instance_id": "34fe5c864f1c", "generation_result": [{"code": "def plot_embedding_as_heatmap(embed, ax=None, title=\"\", shape=None, color_range=(0, 0.30))\n\t\n    if ax is None:\n        ax = plt.gca()\n    if shape is None:\n        height = int(np.ceil(np.sqrt(len(embed))))\n        shape = (height, -1)\n    img = embed.reshape(shape)\n    ax.imshow(img, cmap=\"inferno\", vmin=color_range[0], vmax=color_range[1])\n    ax.set_title(title)\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.set_xlabel(\"Embedding Dimension\")\n    ax.set_ylabel(\"Embedding Index\")\n    plt.colorbar(label=\"Normalized L2 Norm\")\n\n", "CodeBleu_score": 49.28786901396649}, {"code": "def plot_embedding_as_heatmap(embed, ax=None, title=\"\", shape=None, color_range=(0, 0.30))\n\t\n    if ax is None:\n        ax = plt.gca()\n    if shape is None:\n        height = int(np.sqrt(len(embed)))\n        width = int(np.ceil(len(embed) / height))\n        shape = (height, width)\n    ax.imshow(embed.reshape(shape), cmap=\"inferno\", vmin=color_range[0], vmax=color_range[1])\n    ax.set_title(title)\n    ax.set_xticks([])\n    ax.set_yticks([])\n    plt.colorbar(label=\"Normalized L2-norm\")\n\n", "CodeBleu_score": 45.89431357497903}, {"code": "def plot_embedding_as_heatmap(embed, ax=None, title=\"\", shape=None, color_range=(0, 0.30))\n\t\n    if ax is None:\n        ax = plt.gca()\n    if shape is None:\n        height = int(np.sqrt(len(embed)))\n        shape = (height, -1)\n    ax.imshow(embed.reshape(shape), cmap=\"inferno\", vmin=color_range[0], vmax=color_range[1])\n    ax.set_title(title)\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.invert_yaxis()\n    fig = ax.get_figure()\n    fig.colorbar(ax.images[0], ax=ax)\n\n", "CodeBleu_score": 49.76436643971245}]}
{"instance_id": "92e93d50b62b", "generation_result": [{"code": "def similarity_matrix(self, embeds)\n\t\n    # Determine the number of speakers and utterances per speaker\n    batch_size = embeds.shape[0]\n    n_utterances = embeds.shape[1]\n\n    # Compute the centroids\n    centroids_incl = torch.sum(embeds, dim=1) / n_utterances\n    centroids_excl = torch.sum(embeds, dim=1) / (n_utterances - 1)\n\n    # Initialize the similarity matrix\n    similarity_matrix = torch.zeros((batch_size, batch_size), device=self.loss_device)\n    mask_matrix = torch.ones((batch_size, batch_size), device=self.loss_device) - torch.eye(batch_size, device=self.loss_device)\n\n    # Compute the similarity scores\n    for i in range(batch_size):\n        similarity_matrix[i] = torch.sum(embeds[i] * centroids_incl, dim=1) / (torch.norm(centroids_incl, dim=1) + 1e-5)\n        similarity_matrix[i] += torch.sum(embeds[i] * centroids_excl, dim=1) / (torch.norm(centroids_excl, dim=1) + 1e-5)\n\n    # Scale and bias the similarity matrix\n    similarity_matrix = similarity_matrix * self.similarity_weight + self.similarity_bias\n    similarity_matrix = similarity_matrix * mask_matrix\n\n    return similarity_matrix\n", "CodeBleu_score": 28.202356866311785}, {"code": "def similarity_matrix(self, embeds)\n\t\n    # Determine the number of speakers per batch and the number of utterances per speaker\n    num_speakers = embeds.shape[0]\n    num_utterances = embeds.shape[1]\n\n    # Calculate the centroids for the batch\n    centroids_incl = embeds.mean(dim=1, keepdim=True)\n    centroids_incl = centroids_incl / (torch.norm(centroids_incl, dim=2, keepdim=True) + 1e-5)\n    centroids_excl = torch.sum(embeds, dim=1, keepdim=True)\n    centroids_excl = centroids_excl / (num_utterances - 1)\n    centroids_excl = centroids_excl / (torch.norm(centroids_excl, dim=2, keepdim=True) + 1e-5)\n\n    # Initialize the similarity matrix and the mask\n    sim_matrix = torch.zeros((num_speakers, num_speakers), device=self.loss_device)\n    mask = torch.ones((num_speakers, num_speakers), device=self.loss_device)\n\n    # Calculate the similarity matrix for each speaker\n    for i in range(num_speakers):\n        # Calculate the similarity scores for utterances of the ith speaker\n        sim_scores_incl = torch.matmul(embeds[i], centroids_incl[i].transpose(1, 2))\n        sim_scores_excl = torch.matmul(embeds[i], centroids_excl[i].transpose(1, 2))\n\n        # Update the similarity matrix\n        sim_matrix[i] = sim_scores_incl - self.similarity_bias\n        sim_matrix[:, i] = sim_scores_excl - self.similarity_bias\n        mask[i, i] = 0\n\n    # Scale the similarity matrix\n    sim_matrix = (sim_matrix * self.similarity_weight) + 1\n\n    # Apply the mask\n    sim_matrix = sim_matrix * mask\n\n    return sim_matrix\n\n", "CodeBleu_score": 36.93958714094201}, {"code": "def similarity_matrix(self, embeds)\n\t\n    # Determine the number of speakers per batch and the number of utterances per speaker\n    batch_size, num_uttrs = embeds.shape[:2]\n    num_spks = batch_size // num_uttrs\n\n    # Compute the centroids\n    centroids_incl = torch.sum(embeds, dim=1) / (num_uttrs + 1e-10)\n    centroids_incl = centroids_incl / (torch.norm(centroids_incl, dim=1, keepdim=True) + 1e-10)\n    centroids_excl = torch.sum(embeds.reshape(num_spks, num_uttrs, -1), dim=1) / (num_uttrs - 1 + 1e-10)\n    centroids_excl = centroids_excl / (torch.norm(centroids_excl, dim=1, keepdim=True) + 1e-10)\n\n    # Initialize the similarity matrix\n    sim_mat = torch.zeros((batch_size, batch_size), device=self.loss_device)\n    mask_mat = torch.ones((batch_size, batch_size), device=self.loss_device)\n    mask_mat = torch.tril(mask_mat)\n\n    # Compute the similarity matrix\n    for spk in range(num_spks):\n        # Compute the similarity scores\n        sim_scores_incl = torch.matmul(embeds[spk * num_uttrs:(spk + 1) * num_uttrs], centroids_incl.T)\n        sim_scores_excl = torch.matmul(embeds[spk * num_uttrs:(spk + 1) * num_uttrs], centroids_excl.T)\n\n        # Update the similarity matrix\n        sim_mat[spk * num_uttrs:(spk + 1) * num_uttrs, spk * num_uttrs:(spk + 1) * num_uttrs] = sim_scores_excl\n        sim_mat[spk * num_uttrs:(spk + 1) * num_uttrs, (spk + 1) * num_uttrs:(spk + 2) * num_uttrs] = sim_scores_incl\n        sim_mat[(", "CodeBleu_score": 34.06744026878934}]}
{"instance_id": "a6eba54108a1", "generation_result": [{"code": "def loss(self, embeds)\n\t\n    speakers_per_batch, utterances_per_speaker = embeds.shape[:2]\n\n    # Compute the similarity matrix\n    sim_matrix = self.similarity_matrix(embeds)\n\n    # Reshape it\n    sim_matrix = sim_matrix.view(speakers_per_batch * utterances_per_speaker, speakers_per_batch)\n\n    # Compute the ground truth labels for the batch\n    ground_truth = torch.arange(speakers_per_batch, dtype=torch.long).repeat(utterances_per_speaker)\\\n        .to(self.loss_device)\n\n    # Compute the loss\n    loss = self.loss_fn(sim_matrix, ground_truth)\n\n    # Compute the EER with no backprop\n    with torch.no_grad():\n        # Get the predicted labels by assigning the argmax of the similarity matrix\n        pred_labels = torch.argmax(sim_matrix, dim=1)\n\n        # Compute the false positive rate, true positive rate and threshold\n        fpr, tpr, thresholds = roc_curve(ground_truth.cpu(), pred_labels.cpu(), pos_label=None)\n        eer = brentq(lambda x: 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n\n    return loss, eer\n", "CodeBleu_score": 51.995059766352426}, {"code": "def loss(self, embeds)\n\t\n    speakers_per_batch, utterances_per_speaker = embeds.shape[:2]\n\n    # Calculate the similarity matrix\n    sim_matrix = self.similarity_matrix(embeds)\n    sim_matrix = sim_matrix.reshape(speakers_per_batch * utterances_per_speaker, -1)\n\n    # Calculate the loss\n    target = torch.from_numpy(np.repeat(range(speakers_per_batch), utterances_per_speaker)).to(self.loss_device)\n    loss = self.loss_fn(sim_matrix, target)\n\n    # Calculate the EER with no gradient calculations\n    with torch.no_grad():\n        # Reshape the similarity matrix and get the predicted labels\n        sim_matrix = sim_matrix.reshape(speakers_per_batch, utterances_per_speaker, -1)\n        pred_labels = torch.argmax(sim_matrix, dim=2)\n\n        # Get the true labels\n        true_labels = torch.repeat_interleave(torch.arange(speakers_per_batch), utterances_per_speaker)\n\n        # Compute the false positive rate and true positive rate\n        fpr, tpr, thresholds = roc_curve(true_labels.cpu(), pred_labels.cpu(), pos_label=1)\n\n        # Find the equal error rate\n        eer = brentq(lambda x: 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n\n    return loss, eer\n", "CodeBleu_score": 57.14731138855994}, {"code": "def loss(self, embeds)\n\t\n    speakers_per_batch, utterances_per_speaker = embeds.shape[:2]\n    sim_matrix = self.similarity_matrix(embeds)\n    sim_matrix = sim_matrix.reshape(speakers_per_batch * utterances_per_speaker, -1)\n\n    # Create the labels for the similarity matrix. These are numbered from 0 to\n    # num_speakers - 1. The similarity matrix is flattened and then these labels are\n    # used to select the rows for the diagonal of the matrix.\n    labels = np.repeat(np.arange(speakers_per_batch), utterances_per_speaker)\n\n    # Compute the loss for all the positive pairs and average over the number of positive pairs\n    pos_pair_similarity = torch.exp(sim_matrix).to(self.loss_device)\n    loss = -torch.log(pos_pair_similarity[np.arange(len(labels)), labels]).mean()\n\n    # Compute the loss for all the negative pairs and average over the number of negative pairs\n    # The number of negative pairs is (speakers_per_batch * (utterances_per_speaker - 1))\n    # because all pairs except the diagonal are negative pairs\n    neg_pair_similarity = torch.exp(-sim_matrix)\n    loss += -torch.log(1 - neg_pair_similarity[np.arange(len(labels)), labels]).mean()\n\n    # Compute EER\n    with torch.no_grad():\n        # Find the EER threshold by computing the false positive rate and the false negative rate\n        # for all the possible thresholds\n        eer_thres = self.find_eer_thres(sim_matrix)\n        # Compute the false positive rate and the false negative rate for all the possible thresholds\n        fpr, fnr = self.compute_fpr_fnr(sim_matrix, eer_thres)\n        # Find the index of the threshold that gives the EER\n        eer_index = np.argmin(np.abs(fnr - fpr))\n        eer = fpr[eer_index]\n\n    return loss, eer\n\n", "CodeBleu_score": 45.536277095406504}]}
{"instance_id": "ac7b74b9d1b1", "generation_result": [{"code": "def log_params(self)\n\t\n    if self.disabled:\n        return\n    params = []\n    # Model parameters\n    for key, value in self.model.named_parameters():\n        params.append(\\\"%s: %s\\\" % (key, value.size()))\n    # Data parameters\n    for key, value in self.data.named_parameters():\n        params.append(\\\"%s: %s\\\" % (key, value.size()))\n    # Log to visdom\n    self.vis.text(\\\"<pre>%s</pre>\\\" % \\\"\\", "CodeBleu_score": 11.665563812562677}, {"code": "def log_params(self)\n\t\n    if self.disabled:\n        return\n    params_string = \\\"\\\"\\\"\\\"\n    <style type=\"text/css\">\n    table {\n        border-collapse: collapse;\n        border: 2px solid rgb(200, 200, 200);\n        letter-spacing: 1px;\n        font-size: 0.9rem;\n        font-family: sans-serif;\n        width: 100%;\n        overflow-x: auto;\n    }\n    th, td {\n        border: 1px solid rgb(190, 190, 190);\n        padding: 10px 20px;\n    }\n    tr:nth-child(even) {\n        background: rgb(250, 250, 250);\n    }\n    </style>\n    <table>\n    <tr>\n        <th>Model Parameter</th>\n        <th>Value</th>\n    </tr>\n    <tr>\n        <td>Embedding size</td>\n        <td>%d</td>\n    </tr>\n    <tr>\n        <td>Encoder hidden size</td>\n        <td>%d</td>\n    </tr>\n    <tr>\n        <td>Decoder hidden size</td>\n        <td>%d</td>\n    </tr>\n    <tr>\n        <td>Encoder num layers</td>\n        <td>%d</td>\n    </tr>\n    <tr>\n        <td>Decoder num layers</td>\n        <td>%d</td>\n    </tr>\n    <tr>\n        <td>Encoder dropout</td>\n        <td>%f</td>\n    </tr>\n    <tr>\n        <td>Decoder dropout</td>\n        <td>%f</td>\n    </tr>\n    <tr>\n        <td>Encoder bidirectional</td>\n        <td>%s</td>\n    </tr>\n    <tr>\n        <td>Decoder bidirectional</td>\n        <td>%s</td>\n    </tr>\n    <tr>\n        <td>Encoder", "CodeBleu_score": 14.589746004663711}, {"code": "def log_params(self)\n\t\n    if self.disabled:\n        return\n\n    # Get the parameters\n    model_parameters = self.model.named_parameters()\n    data_parameters = self.data.named_parameters()\n\n    # Create the string to display\n    string = \\\"\\\"\\\"\\\"\n    <h2>Model parameters</h2>\n    <table class=\"table table-striped table-bordered\">\n    <thead>\n    <tr><th>Name</th><th>Value</th></tr>\n    </thead>\n    <tbody>\n    \\\"\\\"\\\"\n    for name, param in model_parameters:\n        string += \\\"<tr><td>%s</td><td>%s</td></tr>\\\" % (name, param)\n    string += \\\"\\\"\\\"\n    </tbody>\n    </table>\n    <h2>Data parameters</h2>\n    <table class=\"table table-striped table-bordered\">\n    <thead>\n    <tr><th>Name</th><th>Value</th></tr>\n    </thead>\n    <tbody>\n    \\\"\\\"\\\"\n    for name, param in data_parameters:\n        string += \\\"<tr><td>%s</td><td>%s</td></tr>\\\" % (name, param)\n    string += \\\"\\\"\\\"\n    </tbody>\n    </table>\n    \\\"\\\"\\\"\n\n    # Display the string\n    if self.implementation_win is None:\n        self.implementation_win = self.vis.text(string, opts=dict(title=\\\"Parameters\\\"))\n    else:\n        self.vis.text(string, win=self.implementation_win, opts=dict(title=\\\"Parameters\\\"))\n\n", "CodeBleu_score": 24.842530586259503}]}
{"instance_id": "4726b7864386", "generation_result": [{"code": "def draw_projections(self, embeds, utterances_per_speaker, step, out_fpath=None, max_speakers=10)\n\t\n    if self.disabled:\n        return\n    # Limit the number of speakers\n    if len(embeds) > max_speakers:\n        embeds = embeds[:max_speakers]\n        utterances_per_speaker = utterances_per_speaker[:max_speakers]\n    # Flatten the embeddings\n    embeds = np.reshape(embeds, (len(embeds), -1))\n    # Project the embeddings\n    projection = umap.UMAP(n_neighbors=5, min_dist=0.01, metric=\\\"cosine\\\").fit_transform(embeds)\n    # Assign colors based on speaker identity\n    colors = np.array([colormap[i % len(colormap)] for i in range(len(embeds))])\n    # Plot the embeddings\n    fig, ax = plt.subplots(figsize=(10, 10))\n    ax.scatter(projection[:, 0], projection[:, 1], c=colors, s=20, alpha=0.5, edgecolors=\\\"none\\\")\n    # Add the utterance count labels\n    for i, utterance_count in enumerate(utterances_per_speaker):\n        ax.text(projection[i, 0], projection[i, 1], str(utterance_count), fontsize=8)\n    # Add the title\n    plt.title(\\\"Step %d\\\" % step)\n    # Update the plot\n    if self.vis is not None:\n        self.vis.matplot(fig)\n    # Save the plot to a file\n    if out_fpath is not None:\n        plt.savefig(out_fpath)\n    # Close the figure\n    plt.close(fig)\n\n", "CodeBleu_score": 45.96689133118752}, {"code": "def draw_projections(self, embeds, utterances_per_speaker, step, out_fpath=None, max_speakers=10)\n\t\n    if self.disabled:\n        return\n    # Limit the number of speakers\n    if len(embeds) > max_speakers:\n        embeds = embeds[:max_speakers]\n        utterances_per_speaker = utterances_per_speaker[:max_speakers]\n    # Flatten the utterance embeddings\n    embeds = np.concatenate(embeds)\n    # Project the utterance embeddings\n    projected = self.projector.transform(embeds)\n    # Assign colors based on speaker identity\n    colors = [\n        colormap[i % len(colormap)]\n        for i in range(len(embeds) // utterances_per_speaker)\n    ]\n    # Plot the projected embeddings\n    self.vis.scatter(\n        projected,\n        opts={\n            \"markersize\": 2,\n            \"markercolor\": colors,\n            \"title\": \"Step %d\" % step,\n        },\n    )\n    # Save the plot to a file if specified\n    if out_fpath is not None:\n        self.vis.save([self.env_name], out_fpath)\n\n", "CodeBleu_score": 37.777763610861115}, {"code": "def draw_projections(self, embeds, utterances_per_speaker, step, out_fpath=None, max_speakers=10)\n\t\n    if self.disabled:\n        return\n    embeds = embeds.reshape(embeds.shape[0], -1)\n    # Limit the number of speakers to visualize\n    if embeds.shape[0] > max_speakers:\n        print(\\\"Warning: The number of speakers is limited to %d. The visualization may not be accurate.\\\" % max_speakers)\n        embeds = embeds[:max_speakers]\n        utterances_per_speaker = utterances_per_speaker[:max_speakers]\n    # Project the embeddings\n    embeds = umap.UMAP(n_components=2, random_state=42).fit_transform(embeds)\n    # Assign colors based on speaker identity\n    colors = [colormap[i % len(colormap)] for i in range(len(embeds))]\n    # Draw the plot\n    self.projection_win = self.vis.scatter(\n        embeds,\n        opts=dict(\n            markersize=10,\n            markercolor=colors,\n            title=\\\"Step %d: %d speakers, %d utterances\\\" % (step, len(embeds), np.sum(utterances_per_speaker)),\n        ),\n    )\n    # Save the plot to a file if specified\n    if out_fpath is not None:\n        self.vis.save([self.projection_win], out_fpath)\n", "CodeBleu_score": 42.82842595169982}]}
{"instance_id": "274fa50985c0", "generation_result": [{"code": "def add_speaker_embedding(self, x, speaker_embedding)\n\t\n    batch_size, time_steps, channels = x.size()\n    speaker_embedding = speaker_embedding.unsqueeze(1).repeat(1, time_steps, 1)\n    speaker_embedding = speaker_embedding.view(batch_size, time_steps, channels)\n    speaker_embedding.transpose_(1, 2)\n    x = torch.cat([x, speaker_embedding], dim=2)\n    return x\n\n", "CodeBleu_score": 18.91627157208352}, {"code": "def add_speaker_embedding(self, x, speaker_embedding)\n\t\n    x_dims = x.size()\n    batch_size = x_dims[0]\n    seq_len = x_dims[1]\n    speaker_embedding_dim = speaker_embedding.size()[1]\n    speaker_embedding_index = 2\n    if self.use_cuda:\n        speaker_embedding = speaker_embedding.cuda()\n    # Repeat the speaker embedding to match the length of the input text\n    speaker_embedding = speaker_embedding.repeat(1, seq_len, 1)\n    # Reshape to match the input tensor\n    speaker_embedding = speaker_embedding.reshape(batch_size, seq_len, speaker_embedding_dim)\n    # Transpose the speaker embedding to match the input tensor\n    speaker_embedding = speaker_embedding.transpose(1, 2)\n    # Concatenate the input tensor and the speaker embedding along the last dimension\n    x = torch.cat((x, speaker_embedding), dim=speaker_embedding_index)\n    return x\n\n", "CodeBleu_score": 40.35431041945235}, {"code": "def add_speaker_embedding(self, x, speaker_embedding)\n\t\n    batch_size, _, _ = x.shape\n    speaker_embedding = speaker_embedding.unsqueeze(1).repeat(1, batch_size, 1)\n    speaker_embedding = speaker_embedding.transpose(1, 2)\n    x = torch.cat([x, speaker_embedding], dim=-1)\n    return x\n\n", "CodeBleu_score": 25.03883120610693}]}
{"instance_id": "e6742ea0fe5e", "generation_result": [{"code": "def init_attention(self, encoder_seq_proj)\n\t\n    device = next(self.parameters()).device\n    batch_size, seq_len, attn_dim = encoder_seq_proj.shape\n    self.cumulative = torch.zeros((batch_size, seq_len), device=device)\n    self.attention = torch.zeros((batch_size, seq_len), device=device)\n\n", "CodeBleu_score": 25.142466566464016}, {"code": "def init_attention(self, encoder_seq_proj)\n\t\n    # Determine the device and shape of the input sequence projection\n    device = next(self.parameters()).device\n    batch_size, seq_len, attn_dim = encoder_seq_proj.shape\n\n    # Create the cumulative and attention tensors\n    self.cumulative = torch.zeros((batch_size, seq_len), device=device)\n    self.attention = torch.zeros((batch_size, seq_len), device=device)\n\n", "CodeBleu_score": 25.142466566464016}, {"code": "def init_attention(self, encoder_seq_proj)\n\t\n    device = next(self.parameters()).device\n    batch_size, seq_len, attn_dim = encoder_seq_proj.shape\n    self.cumulative = torch.zeros(batch_size, seq_len, 1).to(device)\n    self.attention = torch.zeros(batch_size, seq_len, seq_len).to(device)\n", "CodeBleu_score": 23.111860041895326}]}
{"instance_id": "4e1fc1ed0ed7", "generation_result": [{"code": "def load(self, path, optimizer=None)\n\t\n    # Load the model state dict\n    checkpoint = torch.load(path, map_location=lambda storage, loc: storage)\n    self.load_state_dict(checkpoint['model'])\n\n    # Load the optimizer state dict\n    if optimizer is not None and 'optimizer' in checkpoint.keys():\n        optimizer.load_state_dict(checkpoint['optimizer'])\n\n    # Load the step\n    self.step = checkpoint['step']\n\n    # Load the stop threshold\n    self.stop_threshold = checkpoint['stop_threshold']\n\n    # Set the model to eval mode\n    self.eval()\n\n    # Return the checkpoint\n    return checkpoint\n\n", "CodeBleu_score": 39.504944536083485}, {"code": "def load(self, path, optimizer=None)\n\t\n    if not os.path.isfile(path):\n        raise RuntimeError(\\\"=> no checkpoint found at '{}'\\\"\".format(path))\n    checkpoint = torch.load(path, map_location=lambda storage, loc: storage)\n    self.load_state_dict(checkpoint[\\\"state_dict\\\"])\n    if optimizer:\n        optimizer.load_state_dict(checkpoint[\\\"optimizer\\\"])\n    self.reset_step()\n    return checkpoint\n\n", "CodeBleu_score": 32.11018468225317}, {"code": "def load(self, path, optimizer=None)\n\t\n    if isinstance(path, str):\n        state = torch.load(path, map_location=lambda storage, loc: storage)\n    else:\n        state = path\n    self.load_state_dict(state['state_dict'])\n    if optimizer is not None:\n        optimizer.load_state_dict(state['optimizer'])\n    return state\n", "CodeBleu_score": 43.010054923346075}]}
{"instance_id": "178e6aa16c0f", "generation_result": [{"code": "def save(self, path, optimizer=None)\n\t\n    # Use device of model params as location for saved state\n    device = next(self.parameters()).device\n    # Package the model state\n    state = {\n        \"model_state\": self.state_dict(),\n        \"optimizer_state\": optimizer.state_dict() if optimizer is not None else None,\n    }\n    # Save to the path\n    torch.save(state, str(path))\n", "CodeBleu_score": 61.3152851169982}, {"code": "def save(self, path, optimizer=None)\n\t\n\n    # Use device of model params as location for saved state\n    device = next(self.parameters()).device\n\n    # Package both model state and optimizer state\n    model_state = self.state_dict()\n    if optimizer is not None:\n        optimizer_state = optimizer.state_dict()\n    else:\n        optimizer_state = None\n\n    # Package into state\n    state = {\n        \"model_state\": model_state,\n        \"optimizer_state\": optimizer_state,\n    }\n\n    # Save to path\n    torch.save(state, str(path))\n\n", "CodeBleu_score": 55.77387954763883}, {"code": "def save(self, path, optimizer=None)\n\t\n    # Use device of model params as location for saved state\n    device = next(self.parameters()).device\n    state = {\n        \"model_state\": self.state_dict(),\n    }\n    if optimizer is not None:\n        state[\"optimizer_state\"] = optimizer.state_dict()\n    torch.save(state, str(path))\n\n", "CodeBleu_score": 63.01230186732438}]}
{"instance_id": "6528fd80e87b", "generation_result": [{"code": "def english_cleaners(text)\n\t\n", "CodeBleu_score": 1.0}, {"code": "def english_cleaners(text)\n\t\".", "CodeBleu_score": 1.0}, {"code": "def english_cleaners(text)\n\t %s\"\"\" % (\"\n  1. \\\"english_cleaners\\\" for English text\n  2. \\\"transliteration_cleaners\\\" for non-English text that can be transliterated to ASCII using\n     the Unidecode library (https://pypi.python.org/pypi/Unidecode)\n  3. \\\"basic_cleaners\\\" if you do not want to transliterate (in this case, you should also update\n     the symbols in symbols.py to match your data).", "CodeBleu_score": 4.576705590670322}]}
{"instance_id": "8a4fe08e0985", "generation_result": [{"code": "def text_to_sequence(text, cleaner_names)\n\t\n    sequence = []\n    # Check for curly braces and treat their contents as ARPAbet:\n    while len(text):\n        m = _curly_re.match(text)\n        if not m:\n            sequence += _symbols_to_sequence(_clean_text(text, cleaner_names))\n            break\n        sequence += _symbols_to_sequence(_clean_text(m.group(1), cleaner_names))\n        sequence += _arpabet_to_sequence(m.group(2))\n        text = m.group(3)\n\n    # Append an end-of-sequence token:\n    sequence.append(_symbol_to_id[\"~\"])\n    return sequence\n\n", "CodeBleu_score": 81.95842258588908}, {"code": "def text_to_sequence(text, cleaner_names)\n\t\n    sequence = []\n    # Check for curly braces and treat their contents as ARPAbet:\n    while len(text):\n        m = _curly_re.match(text)\n        if not m:\n            sequence += _symbols_to_sequence(_clean_text(text, cleaner_names))\n            break\n        sequence += _symbols_to_sequence(_clean_text(m.group(1), cleaner_names))\n        sequence += _arpabet_to_sequence(m.group(2))\n        text = m.group(3)\n\n    # Append EOS token and return sequence\n    sequence.append(_symbol_to_id[\"~\"])\n    return sequence\n\n", "CodeBleu_score": 81.95842258588908}, {"code": "def text_to_sequence(text, cleaner_names)\n\t\n    sequence = []\n\n    # Check for curly braces and treat their contents as ARPAbet:\n    while len(text):\n        m = _curly_re.match(text)\n        if not m:\n            sequence += _symbols_to_sequence(_clean_text(text, cleaner_names))\n            break\n        sequence += _symbols_to_sequence(_clean_text(m.group(1), cleaner_names))\n        sequence += _arpabet_to_sequence(m.group(2))\n        text = m.group(3)\n\n    # Append EOS token and return sequence\n    sequence.append(_symbol_to_id[\"~\"])\n    return sequence\n\n", "CodeBleu_score": 81.95842258588908}]}
{"instance_id": "e1c25a37af1f", "generation_result": [{"code": "def sequence_to_text(sequence)\n\t\n    result = \"\"\n    for symbol_id in sequence:\n        if symbol_id in _id_to_symbol:\n            s = _id_to_symbol[symbol_id]\n            # Enclose ARPAbet back in curly braces:\n            if len(s) > 1 and s[0] == \"@\":\n                s = \"{%s}\" % s[1:]\n            # Concatenate the new string:\n            result += s\n    return result.replace(\"}{\", \" \")\n\n", "CodeBleu_score": 79.77933491640482}, {"code": "def sequence_to_text(sequence)\n\t\n    result = \"\"\n    for symbol_id in sequence:\n        if symbol_id in _id_to_symbol:\n            s = _id_to_symbol[symbol_id]\n            # Enclose ARPAbet back in curly braces:\n            if len(s) > 1 and s[0] == \"@\":\n                s = \"{%s}\" % s[1:]\n            result += s\n    return result.replace(\"}{\", \" \")\n\n", "CodeBleu_score": 79.77933491640482}, {"code": "def sequence_to_text(sequence)\n\t\n    result = \"\"\n    for symbol_id in sequence:\n        if symbol_id in _id_to_symbol:\n            s = _id_to_symbol[symbol_id]\n            # Enclose ARPAbet back in curly braces:\n            if len(s) > 1 and s[0] == \"@\":\n                s = \"{%s}\" % s[1:]\n            # Concatenate with previous text:\n            result += s\n    return result.replace(\"}{\", \" \")\n\n", "CodeBleu_score": 79.77933491640482}]}
{"instance_id": "7807cad4b05d", "generation_result": [{"code": "def embed_utterance(fpaths, encoder_model_fpath)\n\t\n    # Load the encoder\n    encoder.load_model(encoder_model_fpath)\n\n    # Compute the speaker embedding\n    wav = encoder.preprocess_wav(fpaths[0])\n    embed = encoder.embed_utterance(wav)\n\n    # Save the embedding\n    np.save(fpaths[1], embed, allow_pickle=False)\n\n", "CodeBleu_score": 20.360525896216902}, {"code": "def embed_utterance(fpaths, encoder_model_fpath)\n\t\n    # Load the model if necessary\n    if encoder_model is None:\n        encoder_model = get_pretrained_vae(encoder_model_fpath)\n\n    # Compute the speaker embedding\n    wav = encoder.preprocess_wav(fpaths[0])\n    embed = encoder_model.embed_utterance(wav)\n\n    # Save the embedding\n    np.save(fpaths[2], embed, allow_pickle=False)\n\n", "CodeBleu_score": 45.36625449387897}, {"code": "def embed_utterance(fpaths, encoder_model_fpath)\n\t\n    # Load the encoder\n    encoder.load_model(encoder_model_fpath)\n\n    # Compute the speaker embedding of the utterance\n    wav_fpath, embed_fpath = fpaths\n    wav = encoder.preprocess_wav(wav_fpath)\n    embed = encoder.embed_utterance(wav)\n\n    # Save the embedding\n    np.save(embed_fpath, embed, allow_pickle=False)\n\n", "CodeBleu_score": 35.86511802601938}]}
{"instance_id": "9f860273a325", "generation_result": [{"code": "def create_embeddings(synthesizer_root: Path, encoder_model_fpath: Path, n_processes: int)\n\t\n    # Paths\n    encoder_model_dir = encoder_model_fpath.parent\n    embed_dir = encoder_model_dir.joinpath(\\\"embeds\\\")\n    wav_dir = synthesizer_root.joinpath(\\\"audio\\\")\n    embed_dir.mkdir(exist_ok=True)\n\n    # Load metadata\n    metadata_fpath = synthesizer_root.joinpath(\\\"train.txt\\\")\n    with open(metadata_fpath, encoding=\\\"utf-8\\\") as metadata_file:\n        metadata = [line.strip().split(\\\"|\\\", 1) for line in metadata_file]\n        assert all(len(m) == 2 for m in metadata)\n        metadata = [(m[0], m[1]) for m in metadata]\n\n    # Create a list of audio paths\n    wav_fpaths = [wav_dir.joinpath(m[0] + \\\".wav\\\") for m in metadata]\n    embed_fpaths = [embed_dir.joinpath(m[0] + \\\".npy\\\") for m in metadata]\n\n    # Function for processing a single audio file\n    def process_wav(i):\n        # Read audio\n        wav_fpath = wav_fpaths[i]\n        wav = np.load(wav_fpath)\n\n        # Compute the speaker embedding\n        embed_fpath = embed_fpaths[i]\n        embed = encoder.embed_utterance(wav)\n\n        # Save to disk\n        np.save(embed_fpath, embed, allow_pickle=False)\n\n    # Multiprocessing setup\n    if n_processes > 1:\n        pool = Pool(processes=n_processes)\n        list(tqdm(pool.imap(process_wav, range(len(metadata))), total=len(metadata)))\n        pool.close()\n    else:\n        list(tqdm(map(process_wav, range(len(metadata))), total=len(metadata)))\n\n\n\n\n", "CodeBleu_score": 33.51403423924823}, {"code": "def create_embeddings(synthesizer_root: Path, encoder_model_fpath: Path, n_processes: int)\n\t\n    # Create output directories\n    out_dir = synthesizer_root.joinpath(\\\"training_data\\\")\n    out_dir.joinpath(\\\"mels\\\").mkdir(exist_ok=True, parents=True)\n    out_dir.joinpath(\\\"audio\\\").mkdir(exist_ok=True, parents=True)\n    out_dir.joinpath(\\\"embeds\\\").mkdir(exist_ok=True, parents=True)\n\n    # Load the metadata\n    metadata_fpath = synthesizer_root.joinpath(\\\"train.txt\\\")\n    metadata = load_metadata(metadata_fpath)\n    # print(metadata)\n    # print(len(metadata))\n\n    # Create the file paths\n    wav_fpaths = [synthesizer_root.joinpath(\\\"audio\\\", fname) for fname in metadata.index]\n    embed_fpaths = [out_dir.joinpath(\\\"embeds\\\", fname) for fname in metadata.index]\n    assert len(wav_fpaths) == len(metadata)\n\n    # Function for processing the arguments\n    def func(args):\n        return embed_utterance(*args)\n\n    # Process the files\n    print(\\\"Creating embeddings...\\\")\n    with multiprocessing.Pool(processes=n_processes) as pool:\n        list(tqdm.tqdm(pool.imap(func, zip(wav_fpaths, embed_fpaths)), total=len(metadata)))\n\n    print(\\\"Embeddings created.\\\")\n\n", "CodeBleu_score": 30.088401436374014}, {"code": "def create_embeddings(synthesizer_root: Path, encoder_model_fpath: Path, n_processes: int)\n\t\n\n    # Create paths\n    in_dir = synthesizer_root.joinpath(\\\"training_data\\\", \\\"audio\\\")\n    out_dir = synthesizer_root.joinpath(\\\"generated\\\")\n    metadata_fpath = synthesizer_root.joinpath(\\\"training_data\\\", \\\"train.txt\\\")\n    assert in_dir.exists() and metadata_fpath.exists(),\n        \\\"Please make sure that the \\\", metadata_fpath, \\\" and \\\" in_dir, \\\" directories exist.\\\"\"\n\n    # Read metadata\n    metadata = read_metadata(metadata_fpath)\n\n    # Create output directories\n    out_dir.joinpath(\\\"mels\\\").mkdir(exist_ok=True)\n    out_dir.joinpath(\\\"audio\\\").mkdir(exist_ok=True)\n    out_dir.joinpath(\\\"embeds\\\").mkdir(exist_ok=True)\n\n    # Create a list of audio paths, and a list of tuples with paths needed to create embeddings\n    wav_fpaths = []\n    embed_fpaths = []\n    for m in metadata:\n        wav_fpath = in_dir.joinpath(m[0])\n        embed_fpath = out_dir.joinpath(\\\"embeds\\\", \\\"embed-%s.npy\\\" % m[0])\n        wav_fpaths.append(wav_fpath)\n        embed_fpaths.append(embed_fpath)\n\n    # Create a pool of processes. On Linux, the default is one process per CPU core.\n    # On Windows, the default is a process per CPU core, but you can call\n    # set_start_method(\\\"spawn\\\") to force all child processes to use the spawn\n    # start method. This is useful if you are using Python from a compiled\n    # program (e.g., in a C++ application)\n    pool = mp.Pool(processes=n_processes)\n    tqdm.write(\\\"Creating embeddings...\\\")\n\n    # Do the processing\n    r = list(\n        tqdm(\n            pool.imap(embed_utterance, zip(wav_fpaths, embed_fpaths)),\n            total=len(wav_fpaths),\n            desc=\\\"Embeddings\\\",\n        )\n    )\n    # Close the pool\n    pool.close()\n    # Terminate the pool\n    pool.join()\n\n    # Check for exceptions\n    for wav_fpath, embed", "CodeBleu_score": 36.05053942601556}]}
{"instance_id": "9eb0abda14a9", "generation_result": [{"code": "def eval_model(attention, mel_prediction, target_spectrogram, input_seq, step,\n               plot_dir, mel_output_dir, wav_dir, sample_num, loss, hparams)\n\t\n    # Save attention plot\n    fig, ax = plt.subplots()\n    im = ax.imshow(attention, aspect='auto', origin='lower', interpolation='none')\n    fig.colorbar(im, ax=ax)\n    xlabel = 'Decoder timestep'\n    plt.xlabel(xlabel)\n    plt.ylabel('Encoder timestep')\n    plt.tight_layout()\n    plt.savefig(plot_dir.joinpath(f'attention_{step}_{sample_num}.png'))\n    plt.close()\n\n    # Save predicted mel spectrogram to disk\n    mel_output_fpath = mel_output_dir.joinpath(f'mel-prediction-{step}-{sample_num}.npy')\n    np.save(str(mel_output_fpath), mel_prediction, allow_pickle=False)\n\n    # Save griffin lim inverted wav for debug (reconstruction)\n    wav = audio.inv_mel_spectrogram(mel_prediction.T, hparams)\n    wav_fpath = wav_dir.joinpath(f'wav-{step}-{sample_num}.wav')\n    audio.save_wav(wav, str(wav_fpath), sr=hparams.sample_rate)\n\n    # Save real and predicted mel-spectrogram plot to disk\n    fig, axes = plt.subplots(2, 1, figsize=(10, 8))\n    axes[0].imshow(target_spectrogram, aspect='auto', origin='lower', interpolation='none')\n    axes[0].set_title('Target Mel-Spectrogram')\n    axes[1].imshow(mel_prediction, aspect='auto', origin='lower', interpolation='none')\n    axes[1].set_title(f'Predicted Mel-Spectrogram", "CodeBleu_score": 43.26524834747493}, {"code": "def eval_model(attention, mel_prediction, target_spectrogram, input_seq, step,\n               plot_dir, mel_output_dir, wav_dir, sample_num, loss, hparams)\n\t\n\n    # Save attention plot\n    attention_path = plot_dir.joinpath(f\\\"attention_{step}_{sample_num}.png\\\")\n    save_attention(attention, attention_path, hparams)\n\n    # Save predicted mel spectrogram to disk (debug)\n    mel_output_fpath = mel_output_dir.joinpath(f\\\"mel-prediction-{step}_{sample_num}.npy\\\")\n    np.save(str(mel_output_fpath), mel_prediction, allow_pickle=False)\n\n    # Save griffin lim inverted wav for debug (mel -> wav)\n    wav = audio.inv_mel_spectrogram(mel_prediction.T, hparams)\n    wav_fpath = wav_dir.joinpath(f\\\"step-{step}_{sample_num}.wav\\\")\n    audio.save_wav(wav, str(wav_fpath), sr=hparams.sample_rate)\n\n    # Save real and predicted mel-spectrogram plot to disk (control purposes)\n    spec_fpath = plot_dir.joinpath(f\\\"step-{step}_{sample_num}-spec.png\\\")\n    title_str = f\\\"{hparams.builder.name}", "CodeBleu_score": 29.57831888325768}, {"code": "def eval_model(attention, mel_prediction, target_spectrogram, input_seq, step,\n               plot_dir, mel_output_dir, wav_dir, sample_num, loss, hparams)\n\t\n\n    # Save attention plot\n    attention_path = plot_dir.joinpath(f\\\"attention_{step}_{sample_num}.png\\\")\n    save_attention(attention, input_seq, attention_path)\n\n    # Save predicted mel spectrogram to disk (debug)\n    mel_output_fpath = mel_output_dir.joinpath(f\\\"mel-prediction-{step}-{sample_num}.npy\\\")\n    np.save(str(mel_output_fpath), mel_prediction, allow_pickle=False)\n\n    # Save griffin lim inverted waveform\n    wav = audio.inv_mel_spectrogram(mel_prediction.T, hparams)\n    wav_fpath = wav_dir.joinpath(f\\\"step-{step}-{sample_num}.wav\\\")\n    audio.save_wav(wav, str(wav_fpath), sr=hparams.sample_rate)\n\n    # Print some inputs\n    print(f\\\"| Generating {sample_num}/{hparams.tts_eval_num_samples} | Step {step} | Loss {loss:#.4} |\\\" + \\\" \\\" * 20)\n    print(f\\\"| {input_seq}\\\" + \\\" \\\" * 20)\n\n    # Save plot of mel predictions and target spectrogram (debug)\n    spec_fpath = plot_dir.joinpath(f\\\"mel-spec_{step}_{sample_num}.png\\\")\n    target_spectrogram_fpath = plot_dir.joinpath(f\\\"mel-target-spec_{step}_{sample_num}.png\\\")\n    save_mel_spectrogram(mel_prediction, str(spec_fpath), title=f\\\"Predicted Mel Spectrogram, Step {step}\\\")\n    save_mel_spectrogram(target_spectrogram, str(target_spectrogram_fpath), title=f\\\"Target Mel Spectrogram, Step {step}\\\")\n\n    # Save plot of mel predictions and target spectrogram (debug)\n    spec_fpath = plot_dir.joinpath(f\\\"mel-spec_{step}_{sample_num}.png\\\")\n    target_spectrogram_fpath = plot_dir.joinpath(f\\\"mel-target-spec_{step}_{sample_num}.png\\\")\n    save_mel_spectrogram(mel_prediction, str(spec_fpath), title=f\\\"Predicted Mel Spectrogram, Step {step}\\", "CodeBleu_score": 39.80491555623299}]}
{"instance_id": "6bad4f500286", "generation_result": [{"code": "def num_params(self)\n\t\n    num_trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n    print('Number of trainable parameters: %.3f million' % (num_trainable_params / 1e6))\n\n", "CodeBleu_score": 17.618973087564086}, {"code": "def num_params(self)\n\t\n    params = filter(lambda p: p.requires_grad, self.parameters())\n    num_p = sum([np.prod(p.size()) for p in params])\n    num_p = num_p / 1000000\n    print('Number of params: %.3fM' % num_p)\n", "CodeBleu_score": 38.22896194281255}, {"code": "def num_params(self)\n\t\n\n    # Get the trainable parameters\n    params = filter(lambda p: p.requires_grad, self.parameters())\n\n    # Get the number of parameters\n    num_params = sum([np.prod(p.size()) for p in params])\n\n    # Convert to millions and print\n    num_params = num_params / 1000000\n    print('The model has {:3.3}M trainable parameters.'.format(num_params))\n\n", "CodeBleu_score": 35.985489485559384}]}
{"instance_id": "78b876054bd4", "generation_result": [{"code": "def pad_tensor(self, x, pad, side='both')\n\t\n    if side not in ['both', 'before', 'after']:\n        raise ValueError(\"Side argument must be one of 'both', 'before', or 'after'.\")\n    if side == 'both':\n        x = F.pad(x, (0, 0, pad, pad))\n        x = F.pad(x, (pad, pad, 0, 0))\n    elif side == 'before':\n        x = F.pad(x, (pad, pad, 0, 0))\n    else:\n        x = F.pad(x, (0, 0, pad, pad))\n    return x\n\n", "CodeBleu_score": 25.208279359408326}, {"code": "def pad_tensor(self, x, pad, side='both')\n\t\n    if side == 'both':\n        new_length = x.size(2) + 2 * pad\n    elif side == 'before':\n        new_length = x.size(2) + pad\n    elif side == 'after':\n        new_length = x.size(2) + pad\n    else:\n        raise RuntimeError('The side argument must be \\'before\\', \\'after\\', or \\'both\\'.')\n\n    if torch.cuda.is_available():\n        out = torch.cuda.FloatTensor(x.size(0), x.size(1), new_length).fill_(0)\n    else:\n        out = torch.zeros(x.size(0), x.size(1), new_length)\n\n    if side == 'both':\n        out[:, :, pad:-pad] = x\n    elif side == 'before':\n        out[:, :, pad:] = x\n    elif side == 'after':\n        out[:, :, :-pad] = x\n\n    return out\n\n", "CodeBleu_score": 38.1465817228202}, {"code": "def pad_tensor(self, x, pad, side='both')\n\t\n    x = x.transpose(1, 2)\n    b, d, t = x.size()\n    if side == 'both':\n        x_padded = torch.zeros(b, d, t + 2 * pad).type_as(x)\n        x_padded[:, :, pad:pad + t] = x\n    elif side == 'before':\n        x_padded = torch.zeros(b, d, t + pad).type_as(x)\n        x_padded[:, :, pad:] = x\n    elif side == 'after':\n        x_padded = torch.zeros(b, d, t + pad).type_as(x)\n        x_padded[:, :, :t] = x\n    else:\n        raise RuntimeError(\"The side argument must be one of 'before', 'after', or 'both'.\")\n    return x_padded.transpose(1, 2)\n\n", "CodeBleu_score": 34.01437555497701}]}
{"instance_id": "a7ed1715a3df", "generation_result": [{"code": "def save_attention(attn, path)\n\t\n    import matplotlib.pyplot as plt\n    fig, ax = plt.subplots()\n    im = ax.imshow(attn, cmap='gray')\n    ax.set_xticks([])\n    ax.set_yticks([])\n    fig.tight_layout()\n    fig.savefig(path, dpi=300)\n    plt.close()\n\n", "CodeBleu_score": 36.2673607342093}, {"code": "def save_attention(attn, path)\n\t\n    import matplotlib.pyplot as plt\n    fig = plt.figure(figsize=(10, 10))\n    ax = fig.add_subplot(1, 1, 1)\n    ax.matshow(attn, cmap='viridis')\n    fontdict = {'fontsize': 14}\n    ax.set_xticklabels([''] + ['<sos>'] + ['<eos>'] + (['<pad>'] * 10), fontdict=fontdict, rotation=90)\n    ax.set_yticklabels([''] + ['<sos>'] + ['<eos>'] + (['<pad>'] * 10), fontdict=fontdict)\n    plt.savefig(path)\n    plt.close()\n\n", "CodeBleu_score": 45.06464228952029}, {"code": "def save_attention(attn, path)\n\t\n    import matplotlib.pyplot as plt\n    fig = plt.figure(figsize=(10, 10))\n    ax = fig.add_subplot(1, 1, 1)\n    ax.matshow(attn, cmap='viridis')\n\n    fontdict = {'fontsize': 14}\n\n    ax.set_xticklabels([''] + ['<sos>'] + [t.lower() for t in attn.src_vocab.itos] + ['<eos>'], fontdict=fontdict, rotation=90)\n    ax.set_yticklabels([''] + attn.trg_vocab.itos, fontdict=fontdict)\n\n    plt.savefig(path)\n    plt.close()\n\n", "CodeBleu_score": 45.14736128973453}]}
{"instance_id": "19375c9f44a2", "generation_result": [{"code": "def plot(array)\n\t\n    import matplotlib.pyplot as plt\n    fig = plt.figure(figsize=(12, 6))\n    plt.subplot(1, 1, 1)\n    plt.imshow(array.T, interpolation='nearest', aspect='auto')\n    plt.xlabel('Encoder timestep', fontsize=16, color='black')\n    plt.ylabel('Decoder timestep', fontsize=16, color='black')\n    plt.tick_params(axis='x', colors='black', labelsize=14)\n    plt.tick_params(axis='y', colors='black', labelsize=14)\n    plt.show()\n\n", "CodeBleu_score": 37.70497824720764}, {"code": "def plot(array)\n\t\n    import matplotlib.pyplot as plt\n    fig = plt.figure(figsize=(12, 6))\n    ax = fig.add_subplot(1, 1, 1)\n    ax.spines['bottom'].set_color('blue')\n    ax.spines['top'].set_color('blue')\n    ax.spines['left'].set_color('blue')\n    ax.spines['right'].set_color('blue')\n    ax.tick_params(axis='x', colors='blue', labelsize=14)\n    ax.tick_params(axis='y', colors='blue', labelsize=14)\n    plt.xlabel('Time', fontsize=14, color='blue')\n    plt.ylabel('Amplitude', fontsize=14, color='blue')\n    plt.plot(array)\n    plt.show()\n", "CodeBleu_score": 42.246914874211065}, {"code": "def plot(array)\n\t\n    import matplotlib.pyplot as plt\n    import numpy as np\n    fig = plt.figure(figsize=(12, 6))\n    ax = fig.add_subplot(1, 1, 1)\n    ax.imshow(array, interpolation='nearest', aspect='auto')\n    ax.tick_params(axis='both', which='major', labelsize=14, colors='blue')\n    ax.tick_params(axis='both', which='minor', labelsize=14, colors='blue')\n    ax.set_xlabel('Time', fontsize=16, color='blue')\n    ax.set_ylabel('Frequency', fontsize=16, color='blue')\n    plt.show()\n    plt.close(fig)\n\n", "CodeBleu_score": 49.036875895413104}]}
{"instance_id": "ff94c425920c", "generation_result": [{"code": "def write_to_csv(image_name, prediction, confidence)\n\t\n    with open('predictions.csv', 'a', newline='') as csv_file:\n        fieldnames = ['image', 'prediction', 'confidence']\n        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n\n        if csv_file.tell() == 0:\n            writer.writeheader()\n\n        writer.writerow({'image': image_name, 'prediction': prediction, 'confidence': confidence})\n", "CodeBleu_score": 45.662965885495346}, {"code": "def write_to_csv(image_name, prediction, confidence)\n\t\n    # Create a dictionary with the image name, prediction, and confidence\n    image_data = {\n        \"image_name\": image_name,\n        \"prediction\": prediction,\n        \"confidence\": confidence,\n    }\n\n    # Open the CSV file in append mode\n    with open(\"predictions.csv\", \"a\", newline=\"\") as csv_file:\n        # Create a CSV writer\n        csv_writer = csv.writer(csv_file, delimiter=\",\")\n\n        # Write the header if the file is new\n        if csv_file.tell() == 0:\n            csv_writer.writerow(image_data.keys())\n\n        # Write the data as a new row\n        csv_writer.writerow(image_data.values())\n\n", "CodeBleu_score": 42.84591326944118}, {"code": "def write_to_csv(image_name, prediction, confidence)\n\t\n    # Create a dictionary with the image name, prediction, and confidence\n    prediction_dict = {\n        \"Image Name\": image_name,\n        \"Prediction\": prediction,\n        \"Confidence\": confidence\n    }\n\n    # Open the CSV file in append mode\n    with open('predictions.csv', 'a') as csv_file:\n        # Create a header if the file is new\n        if csv_file.tell() == 0:\n            fieldnames = prediction_dict.keys()\n            writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n            writer.writeheader()\n        # Write the data as a new row\n        writer.writerow(prediction_dict)\n\n", "CodeBleu_score": 48.02856379018859}]}
{"instance_id": "436161602009", "generation_result": [{"code": "def export_torchscript(model, im, file, optimize, prefix=colorstr(\"TorchScript:\"))\n\t\n    f = file.with_suffix(file.suffix + \".torchscript\")\n    LOGGER.info(f\"{prefix} starting export with torch {torch.__version__}...\")\n    ts = torch.jit.trace(model, im, strict=False)\n    if optimize:\n        LOGGER.info(f\"{prefix} optimizing for mobile device...\")\n        ts = optimize_for_mobile(ts)\n    ts.save(f)\n    LOGGER.info(f\"{prefix} export success, saved as {f} ({file_size(f):.1f} MB)\")\n    return f, None\n\n", "CodeBleu_score": 41.690324019745496}, {"code": "def export_torchscript(model, im, file, optimize, prefix=colorstr(\"TorchScript:\"))\n\t\n    f = file.with_suffix(file.suffix + \".torchscript\")\n    LOGGER.info(f\"{prefix} starting export with torch {torch.__version__}...\")\n    model.model.model[-1].export = True  # set Detect() layer export=True\n    model.model.model[-1].jit = True  # set Detect() layer jit=True\n    model.model.model[-1].onnx = False  # set Detect() layer onnx=False\n    model.model.model[-1].tflite = False  # set Detect() layer tflite=False\n    model.model.model[-1].forward = model.forward  # add forward method to Detect() layer\n    ts = torch.jit.trace(model.model.model[-1], im, strict=False)  # trace model\n    if optimize:\n        LOGGER.info(f\"{prefix} running optimize_for_mobile...\")\n        ts = optimize_for_mobile(ts)  # optimize for mobile\n    ts.save(f)  # save model\n    return f, None\n\n", "CodeBleu_score": 40.04018616940709}, {"code": "def export_torchscript(model, im, file, optimize, prefix=colorstr(\"TorchScript:\"))\n\t\n    LOGGER.info(f\"{prefix} starting export with torch {torch.__version__}...\")\n    f = file.with_suffix(file.suffix + \".torchscript\")\n    f = f.relative_to(Path.cwd()) if f.is_file() else f  # always use forward slashes\n    im = im.half() if im.dtype == torch.float16 else im.float()  # to FP16/FP32\n    im = im[None]  # expand for batch dim\n    if optimize:\n        model = optimize_for_mobile(model)\n    torch.jit.trace(model, im).save(f)\n    LOGGER.info(f\"{prefix} export success, saved as {f} ({file_size(f):.1f} MB)\")\n    return f, None\n\n", "CodeBleu_score": 50.820474584129684}]}
{"instance_id": "78335487ac1c", "generation_result": [{"code": "def export_coreml(model, im, file, int8, half, nms, prefix=colorstr(\"CoreML:\")):\n\t\n    check_requirements(\"coremltools>=4.0b3\")\n    from coremltools.models.neural_network import quantization_utils\n    from coremltools.models.utils import save_spec\n    from coremltools.models.neural_network.quantization_utils import \\\n        _quantize_weights, _quantize_weights_with_scales, _quantize_weights_with_zeropoint_and_scale\n    from coremltools.models.neural_network.quantization_utils import _get_quantization_params\n    from coremltools.models.neural_network.quantization_utils import _get_quantization_params_dict\n    from coremltools.models.neural_network.quantization_utils import _get_quantized_model\n    from coremltools.models.neural_network.quantization_utils import _get_quantized_model_v2\n    from coremltools.models.neural_network.quantization_utils import _get_quantized_model_v3\n    from coremltools.models.neural_network.quantization_utils import _get_quantized_model_v4\n    from coremltools.models.neural_network.quantization_utils import _get_quantized_model_v5\n    from coremltools.models.neural_network.quantization_utils import _get_quantized_model_v6\n    from coremltools.models.neural_network.quantization_utils import _get_quantized_model_v7\n    from coremltools.models.neural_network.quantization_utils import _get_quantized_model_v8\n    from coremltools.models.neural_network.quantization_utils import _get_quantized_model_v9\n    from coremltools.models.neural_network.quantization_utils import _get_quantized_model_v10\n    from coremltools.models.neural_network.quantization_utils import _get_quantized_model_v11\n    from coremltools.models.neural_network.quantization_utils import _get_quantized_model_v12\n    from coremltools.models.neural_network.quantization_utils import _get_quantized_model_v13\n    from coremltools.models.neural_network.quantization_utils", "CodeBleu_score": 12.313659654553478}, {"code": "def export_coreml(model, im, file, int8, half, nms, prefix=colorstr(\"CoreML:\")):\n\t\n    check_requirements(\"coremltools>=4.0\")\n    import coremltools as ct\n\n    LOGGER.info(f\"", "CodeBleu_score": 10.18934792561635}, {"code": "def export_coreml(model, im, file, int8, half, nms, prefix=colorstr(\"CoreML:\")):\n\t\n    check_requirements(\"coremltools\")\n    from coremltools.models.neural_network import quantization_utils\n    from coremltools.models.neural_network.quantization_utils import \\\n        QuantizationMode, QuantizationType, RangeEstimationMethod\n    from coremltools.models.neural_network.quantization_utils import \\\n        _quantize_weights, _quantize_weights_to_8bits, _quantize_weights_to_fp16\n    from coremltools.models.utils import _macos_version, _is_macos\n    from coremltools.models.neural_network.quantization_utils import \\\n        _is_quantization_supported_for_layer\n\n    LOGGER.info(f\"", "CodeBleu_score": 17.12000751303456}]}
{"instance_id": "80233cf71580", "generation_result": [{"code": "def generate_individual(input_ranges, individual_length)\n\t\n    individual = []\n    for _ in range(individual_length):\n        individual.append(random.uniform(input_ranges[_][0], input_ranges[_][1]))\n    return individual\n\n", "CodeBleu_score": 41.79375459387889}, {"code": "def generate_individual(input_ranges, individual_length)\n\t\n    individual = []\n    for _ in range(individual_length):\n        individual.append(random.uniform(input_ranges[0], input_ranges[1]))\n    return individual\n\n", "CodeBleu_score": 39.41234669340793}, {"code": "def generate_individual(input_ranges, individual_length)\n\t\n    individual = []\n    for i in range(individual_length):\n        individual.append(random.uniform(input_ranges[i][0], input_ranges[i][1]))\n    return individual\n\n", "CodeBleu_score": 47.630972577458174}]}
{"instance_id": "5ea979cb4a8e", "generation_result": [{"code": "def marginal_log_mean_coeff(self, t)\n\t\n    if self.schedule == 'discrete':\n        t_array = self.t_array\n        log_alpha_array = self.log_alpha_array\n        total_N = self.total_N\n        t_array = t_array.to(t.device)\n        log_alpha_array = log_alpha_array.to(t.device)\n        t = t.to(t.device)\n        alpha_t = torch.exp(log_alpha_array)\n        alpha_t = alpha_t.view(1, -1)\n        t_array = t_array.view(1, -1)\n        t = t.view(-1, 1)\n        alpha_t_minus_1 = F.grid_sample(alpha_t, t_array - 1e-5)\n        alpha_t_minus_1 = alpha_t_minus_1.view(-1)\n        alpha_t_plus_1 = F.grid_sample(alpha_t, t_array + 1e-5)\n        alpha_t_plus_1 = alpha_t_plus_1.view(-1)\n        alpha_t = alpha_t.view(-1)\n        t_array = t_array.view(-1)\n        alpha_t_minus_1 = torch.where(t_array == 0, 1., alpha_t_minus_1)\n        alpha_t_plus_1 = torch.where(t_array == 1., 0., alpha_t_plus_1)\n        alpha_t = (alpha_t_minus_1 * (1. - t) + alpha_t_plus_1 * t) * (1. - t_array) + alpha_t * t_array\n        alpha_t = alpha_t.view(1, -1)\n        return torch.log(alpha_t)\n    elif self.schedule == 'linear':\n        beta_min = self.beta_0\n        beta_max = self.beta_1\n        T = self.T\n        t = t.to(t.device)\n        beta_min = beta_min.to(t.device)\n        beta_max = beta_max.to(t.device)\n        T = T.to(t.device)\n        beta_min = beta_min.view(1, 1)", "CodeBleu_score": 35.07385637289101}, {"code": "def marginal_log_mean_coeff(self, t)\n\t\n    if self.schedule == 'discrete':\n        # The log_alpha_t is computed by a quadratic expression.\n        # log_alpha_t = (1 - t) * (self.log_alpha_array[:, :-1] + t * self.log_alpha_array[:, 1:])\n        # The log_alpha_t is computed by a linear interpolation.\n        log_alpha_t = (1 - t) * self.log_alpha_array[:, :-1] + t * self.log_alpha_array[:, 1:]\n        return log_alpha_t\n    else:\n        if self.schedule == 'linear':\n            beta_min = self.beta_0\n            beta_max = self.beta_1\n            return math.log(beta_min) + t * (math.log(beta_max) - math.log(beta_min)) / self.T\n        else:\n            # The cosine schedule\n            s = self.cosine_s\n            beta_max = self.cosine_beta_max\n            t_max = self.cosine_t_max\n            log_alpha_0 = self.cosine_log_alpha_0\n            if t <= s:\n                return log_alpha_0 - math.log(1. + math.exp(s - t))\n            else:\n                return log_alpha_0 + math.log(1. + math.exp(t - s)) - math.log(1. + math.exp(t_max - s))\n\n", "CodeBleu_score": 38.822255911216885}, {"code": "def marginal_log_mean_coeff(self, t)\n\t\n    if self.schedule == 'discrete':\n        if t < 0 or t > 1:\n            raise ValueError(\"The continuous-time label t should be in the range [0, 1].\")\n        if t == 0:\n            return self.log_alpha_array[0, 0]\n        if t == 1:\n            return self.log_alpha_array[0, -1]\n        else:\n            return self.log_alpha_array[0, (self.t_array[0, :] - t).abs().argmin()]\n    elif self.schedule == 'linear':\n        if t < 0 or t > 1:\n            raise ValueError(\"The continuous-time label t should be in the range [0, 1].\")\n        return torch.log(self.beta_0 + (self.beta_1 - self.beta_0) * t)\n    elif self.schedule == 'cosine':\n        if t < 0 or t > self.T:\n            raise ValueError(\"The continuous-time label t should be in the range [0, {}].\".format(self.T))\n        if t == 0:\n            return self.cosine_log_alpha_0\n        else:\n            return math.log(math.cos(self.cosine_s / (1. + self.cosine_s) * math.pi / 2.)) + (1. + self.cosine_s) / (1. - self.cosine_s) * (math.sin(self.cosine_s / (1. + self.cosine_s) * math.pi / 2.) - math.sin(self.cosine_t_max / (1. + self.cosine_s) * math.pi / 2.)) / (self.cosine_t_max * (1. - math.cos(self.cosine_s / (1. + self.cosine_s) * math.pi / 2.))) * (t - self.cosine_s) / (self.cosine_t_max - self.cosine_s)\n    else:\n        raise ValueError(\"Unsupported noise schedule {}. The schedule needs to be 'discrete' or 'linear' or 'cosine'\".format(self.schedule))\n\n", "CodeBleu_score": 38.95548882129136}]}
{"instance_id": "7da4fae5a278", "generation_result": [{"code": "def inverse_lambda(self, lamb)\n\t\n    if self.schedule == 'discrete':\n        return torch.exp(interpolate_fn(lamb.reshape((-1, 1)), self.log_alpha_array.to(lamb.device), self.t_array.to(lamb.device))).reshape((-1))\n    elif self.schedule == 'linear':\n        beta_t = (self.beta_1 - self.beta_0) * lamb + self.beta_0\n        return torch.sqrt(1. / (2. * beta_t))\n    elif self.schedule == 'cosine':\n        # For the cosine schedule, T = 1 will have numerical issues. So we manually set the ending time T.\n        # Note that T = 0.9946 may be not the optimal setting. However, we find it works well.\n        cosine_t = torch.acos((self.cosine_s + lamb) / (1. + self.cosine_s)) / math.pi * 2. * (1. + self.cosine_s) / self.cosine_s - self.cosine_s\n        return cosine_t\n\n", "CodeBleu_score": 30.430861618007597}, {"code": "def inverse_lambda(self, lamb)\n\t\n    if self.schedule == 'discrete':\n        return interpolate_fn(lamb.reshape((-1, 1)), self.log_alpha_array.to(lamb.device), self.t_array.to(lamb.device)).reshape((-1))\n    elif self.schedule == 'linear':\n        beta_0 = self.beta_0\n        beta_1 = self.beta_1\n        t = (-lamb * 4. + 1.) / (beta_1 - beta_0)\n        return t\n    elif self.schedule == 'cosine':\n        t = torch.acos((lamb + self.cosine_s) / (1. + self.cosine_s) * math.pi / 2.) / math.pi * (1. + self.cosine_s) - self.cosine_s\n        return t\n    else:\n        raise ValueError(\"Unsupported noise schedule {}. The schedule needs to be 'discrete' or 'linear' or 'cosine'\".format(self.schedule))\n", "CodeBleu_score": 29.46866741598859}, {"code": "def inverse_lambda(self, lamb)\n\t\n    if self.schedule == 'discrete':\n        t = interpolate_fn(lamb.reshape((-1, 1)), self.log_alpha_array.to(lamb.device), self.t_array.to(lamb.device)).reshape((-1))\n    elif self.schedule == 'linear':\n        t = (lamb - 0.5 * self.beta_0 * self.T) / (self.beta_1 - self.beta_0)\n    elif self.schedule == 'cosine':\n        log_alpha_fn = lambda s: torch.log(torch.cos((s + self.cosine_s) / (1. + self.cosine_s) * math.pi / 2.))\n        t = torch.acos((torch.exp(lamb) + self.cosine_log_alpha_0) / torch.exp(self.cosine_log_alpha_0)) / (math.pi / 2. * (1. + self.cosine_s)) - self.cosine_s\n    return t\n", "CodeBleu_score": 41.41831521809499}]}
{"instance_id": "a519c853a2b2", "generation_result": [{"code": "def noise_pred_fn(x, t_continuous, cond=None)\n\t\n    t_input = get_model_input_time(t_continuous)\n    if cond is None:\n        if model_type == \"noise\":\n            noise_pred = model(x, t_input, **model_kwargs)\n        elif model_type == \"x_start\":\n            noise_pred = model(x, t_input, **model_kwargs)\n        elif model_type == \"v\":\n            noise_pred = model(x, t_input, **model_kwargs)\n        elif model_type == \"score\":\n            noise_pred = model(x, t_input, **model_kwargs)\n        else:\n            raise ValueError(\"Unknown model type: {}\".format(model_type))\n    else:\n        if guidance_type == \"uncond\":\n            if model_type == \"noise\":\n                noise_pred = model(x, t_input, **model_kwargs)\n            elif model_type == \"x_start\":\n                noise_pred = model(x, t_input, **model_kwargs)\n            elif model_type == \"v\":\n                noise_pred = model(x, t_input, **model_kwargs)\n            elif model_type == \"score\":\n                noise_pred = model(x, t_input, **model_kwargs)\n            else:\n                raise ValueError(\"Unknown model type: {}\".format(model_type))\n        elif guidance_type == \"classifier\":\n            if model_type == \"noise\":\n                noise_pred = model(x, t_input, **model_kwargs)\n            elif model_type == \"x_start\":\n                noise_pred = model(x, t_input, **model_kwargs)\n            elif model_type == \"v\":\n                noise_pred = model(x, t_input, **model_kwargs)\n            elif model_type == \"score\":\n                noise_pred = model(x, t_input, **model_kwargs)\n            else:\n                raise ValueError(\"Unknown model type: {}\".format(model_type))\n        elif guidance_type == \"classifier-free\":\n            if model_type == \"noise\":\n                noise_pred = model(x, t_input, cond, **model_kwargs)\n            elif model_type == \"x_start\":\n                noise_pred = model(x, t_input, cond, **model_kwargs)\n            elif model_type == \"v\":\n                noise_", "CodeBleu_score": 19.927546386846018}, {"code": "def noise_pred_fn(x, t_continuous, cond=None)\n\t\n    if model_type == \"noise\":\n        t_input = get_model_input_time(t_continuous)\n        noise = model(x, t_input, **model_kwargs)\n        noise = noise_schedule.get_noise_pred(noise, t_continuous)\n        return noise\n    elif model_type == \"x_start\":\n        t_input = get_model_input_time(t_continuous)\n        x_start = model(x, t_input, **model_kwargs)\n        x_start = noise_schedule.get_x_start_pred(x_start, t_continuous)\n        return x_start\n    elif model_type == \"v\":\n        t_input = get_model_input_time(t_continuous)\n        v = model(x, t_input, **model_kwargs)\n        v = noise_schedule.get_v_pred(v, t_continuous)\n        return v\n    elif model_type == \"score\":\n        t_input = get_model_input_time(t_continuous)\n        score = model(x, t_input, **model_kwargs)\n        score = noise_schedule.get_score_pred(score, t_continuous)\n        return score\n    else:\n        raise ValueError(f\"Unknown model_type: {model_type}\")\n\n", "CodeBleu_score": 38.4945382032724}, {"code": "def noise_pred_fn(x, t_continuous, cond=None)\n\t\n    t_input = get_model_input_time(t_continuous)\n    if model_type == \"noise\":\n        noise_pred = noise_schedule.get_noise(t_input, cond)\n        return model(x, t_input, cond, **model_kwargs) + noise_pred\n    elif model_type == \"x_start\":\n        return model(x, t_input, cond, **model_kwargs)\n    elif model_type == \"v\":\n        noise_pred = noise_schedule.get_noise(t_input, cond)\n        return model(x, t_input, cond, **model_kwargs) + noise_pred * v_scale\n    elif model_type == \"score\":\n        noise_pred = noise_schedule.get_noise(t_input, cond)\n        return model(x, t_input, cond, **model_kwargs) - noise_pred * score_scale\n    else:\n        raise NotImplementedError\n\n", "CodeBleu_score": 25.845701122815957}]}
{"instance_id": "96a9b7d54823", "generation_result": [{"code": "def dynamic_thresholding_fn(self, x0, t=None)\n\t\n    if t is None:\n        t = self.t_predict\n    x0_abs = x0.abs()\n    if self.variant == 'bh1':\n        q = 0.75\n    elif self.variant == 'bh2':\n        q = 0.5\n    else:\n        raise ValueError(f'Unknown variant {self.variant}')\n    threshold = x0_abs.reshape(x0_abs.shape[0], -1).kthvalue(int(x0_abs.shape[1] * q), dim=1)[0].reshape(x0.shape[0], 1, 1, 1)\n    scale = threshold / self.max_val\n    if self.thresholding:\n        x0 = torch.clamp(x0, -self.max_val, self.max_val)\n    return x0 * scale\n\n\n\n", "CodeBleu_score": 37.093753293478116}, {"code": "def dynamic_thresholding_fn(self, x0, t=None)\n\t\n    if t is None:\n        t = self.t_vals\n    x0 = x0.reshape((-1, self.dim))\n    quantile = np.quantile(np.abs(x0), self.quantile_level)\n    if quantile == 0:\n        return x0\n    scaling_factor = self.max_val / quantile\n    x0_scaled = torch.clamp(x0, -self.max_val, self.max_val)\n    x0_scaled = x0_scaled * scaling_factor\n    return x0_scaled\n\n", "CodeBleu_score": 32.58469796167074}, {"code": "def dynamic_thresholding_fn(self, x0, t=None)\n\t\n    if t is None:\n        t = self.noise_schedule.get_timesteps(x0.shape[0])\n    x0_abs = torch.abs(x0)\n    if self.variant == 'bh1':\n        q = x0_abs.reshape(x0_abs.shape[0], -1).quantile(0.99, dim=1, keepdim=True)\n    elif self.variant == 'bh2':\n        q = x0_abs.reshape(x0_abs.shape[0], -1).quantile(0.999, dim=1, keepdim=True)\n    elif self.variant == 'bh3':\n        q = x0_abs.reshape(x0_abs.shape[0], -1).quantile(0.9999, dim=1, keepdim=True)\n    elif self.variant == 'bh4':\n        q = x0_abs.reshape(x0_abs.shape[0], -1).quantile(0.99999, dim=1, keepdim=True)\n    elif self.variant == 'bh5':\n        q = x0_abs.reshape(x0_abs.shape[0], -1).quantile(0.999999, dim=1, keepdim=True)\n    elif self.variant == 'bh6':\n        q = x0_abs.reshape(x0_abs.shape[0], -1).quantile(0.9999999, dim=1, keepdim=True)\n    else:\n        raise NotImplementedError\n    scaling_factor = self.max_val / q\n    x0_abs_scaled = x0_abs * scaling_factor\n    x0_clamped = torch.clamp(x0_abs_scaled, 0, self.max_val)\n    return x0_clamped\n\n", "CodeBleu_score": 32.64080601287189}]}
{"instance_id": "59c6ec6d7a6a", "generation_result": [{"code": "def data_prediction_fn(self, x, t)\n\t\n    dims = x.dim()\n    p = self.dynamic_thresholding_ratio\n    s = torch.quantile(torch.abs(x).reshape((x.shape[0], -1)), p, dim=1)\n    s = expand_dims(torch.maximum(s, self.thresholding_max_val * torch.ones_like(s).to(s.device)), dims)\n    x = torch.clamp(x, -s, s) / s\n    return x\n\n", "CodeBleu_score": 25.350051633687098}, {"code": "def data_prediction_fn(self, x, t)\n\t\n    dims = x.dim()\n    p = self.dynamic_thresholding_ratio\n    s = torch.quantile(torch.abs(x).reshape((x.shape[0], -1)), p, dim=1)\n    s = expand_dims(torch.maximum(s, self.thresholding_max_val * torch.ones_like(s).to(s.device)), dims)\n    x = torch.clamp(x, -s, s) / s\n    return x\n\n", "CodeBleu_score": 25.350051633687098}, {"code": "def data_prediction_fn(self, x, t)\n\t\n    dims = x.dim()\n    s = torch.quantile(torch.abs(x).reshape((x.shape[0], -1)), self.dynamic_thresholding_ratio, dim=1)\n    s = expand_dims(torch.maximum(s, self.thresholding_max_val * torch.ones_like(s).to(s.device)), dims)\n    x = torch.clamp(x, -s, s) / s\n    return x\n\n", "CodeBleu_score": 23.56163574085947}]}
{"instance_id": "65e3618468a2", "generation_result": [{"code": "def predict_eps_sigma(model, input, sigma_in, **kwargs)\n\t\n    sigma_in = sigma_in.reshape(input.shape)\n    input_scaled = input / sigma_in\n    output_scaled = model(input_scaled, **kwargs)\n    output = output_scaled * sigma_in\n    return output\n\n", "CodeBleu_score": 20.886380738955832}, {"code": "def predict_eps_sigma(model, input, sigma_in, **kwargs)\n\t\n    sigma_in = sigma_in.view(input.shape[0], -1)\n    input_scaled = input * sigma_in\n    output = model(input_scaled, **kwargs)\n    return output / sigma_in\n\n", "CodeBleu_score": 25.65748325229449}, {"code": "def predict_eps_sigma(model, input, sigma_in, **kwargs)\n\t\n    sigma_in = sigma_in.reshape(input.shape)\n    input = input * sigma_in\n    eps_out = model(input, **kwargs)\n    eps_out = eps_out / sigma_in\n    return eps_out\n\n", "CodeBleu_score": 26.991095988607427}]}
{"instance_id": "b35846dfbc66", "generation_result": [{"code": "def cal_intergrand(beta_0, beta_1, taus)\n\t\n    with torch.enable_grad():\n        taus = taus.clone().requires_grad_(True)\n        alpha = t2alpha_fn(beta_0, beta_1, taus)\n        log_alpha = torch.log(alpha)\n        log_alpha.backward(torch.ones_like(log_alpha))\n        intergrand = -taus * log_alpha.grad\n    return intergrand\n", "CodeBleu_score": 39.69720849369713}, {"code": "def cal_intergrand(beta_0, beta_1, taus)\n\t\n    with torch.no_grad():\n        alpha = t2alpha_fn(beta_0, beta_1, taus)\n        log_alpha = torch.log(alpha)\n    log_alpha.backward(torch.ones_like(log_alpha))\n    grad_log_alpha = taus.grad\n    taus.grad.zero_()\n    return grad_log_alpha * alpha\n\n", "CodeBleu_score": 29.998349766348326}, {"code": "def cal_intergrand(beta_0, beta_1, taus)\n\t\n    # Cloning and setting the required gradients for taus.\n    taus.requires_grad_(True)\n    taus_clone = taus.clone().detach().requires_grad_(True)\n    # Calculating alpha and its logarithm.\n    alpha = t2alpha_fn(beta_0, beta_1, taus_clone)\n    log_alpha = torch.log(alpha)\n    # Performing backpropagation to obtain the gradient of log_alpha with respect to taus.\n    log_alpha.backward(torch.ones_like(log_alpha))\n    # Calculating the integrand using the gradient and alpha values.\n    intergrand = alpha * taus_clone.grad\n    return intergrand\n", "CodeBleu_score": 20.2996112163288}]}
{"instance_id": "5f3667566a10", "generation_result": [{"code": "def sample_euler_ancestral(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None)\n\t\n    extra_args = {} if extra_args is None else extra_args\n    if noise_sampler is None:\n        noise_sampler = default_noise_sampler(x)\n    s_in = x.new_ones([x.shape[0]])\n    for i in trange(len(sigmas) - 1, disable=disable):\n        sigma_from, sigma_to = sigmas[i], sigmas[i + 1]\n        sigma_down, sigma_up = get_ancestral_step(sigma_from, sigma_to, eta)\n        denoised = model(x, sigma_from * s_in, **extra_args)\n        d = to_d(x, sigma_from, denoised)\n        if callback is not None:\n            callback({'x': x, 'i': i, 'sigma': sigma_from, 'denoised': denoised})\n        if sigma_up > 0:\n            eps = noise_sampler(sigma_from, sigma_to) * s_noise\n            x = x + eps * (sigma_up ** 2 - sigma_from ** 2) ** 0.5\n        dt = sigma_to - sigma_from\n        # Euler method\n        x = x + d * dt\n    if sigma_up > 0:\n        eps = noise_sampler(sigma_to, sigma_to) * s_noise\n        x = x + eps * (sigma_to ** 2 - sigma_up ** 2) ** 0.5\n    return x\n\n", "CodeBleu_score": 54.39197490243871}, {"code": "def sample_euler_ancestral(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None)\n\t\n    extra_args = {} if extra_args is None else extra_args\n    if noise_sampler is None:\n        noise_sampler = default_noise_sampler(x)\n    s_in = x.new_ones([x.shape[0]])\n    for i in trange(len(sigmas) - 1, disable=disable):\n        sigma_from, sigma_to = sigmas[i], sigmas[i + 1]\n        sigma_down, sigma_up = get_ancestral_step(sigma_from, sigma_to, eta)\n        denoised = model(x, sigma_from * s_in, **extra_args)\n        d = to_d(x, sigma_from, denoised)\n        if callback is not None:\n            callback({'x': x, 'i': i, 'sigma': sigma_from, 'sigma_down': sigma_down, 'sigma_up': sigma_up, 'denoised': denoised})\n        dt = sigma_to - sigma_from\n        # Euler method\n        x = x + d * dt\n        if sigma_up > 0:\n            eps = noise_sampler(sigma_from, sigma_to) * s_noise\n            x = x + eps * (sigma_up ** 2 - sigma_from ** 2) ** 0.5\n    return x\n", "CodeBleu_score": 56.47671909573635}, {"code": "def sample_euler_ancestral(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None)\n\t\n    extra_args = {} if extra_args is None else extra_args\n    s_in = x.new_ones([x.shape[0]])\n    if noise_sampler is None:\n        noise_sampler = default_noise_sampler(x)\n    for i in trange(len(sigmas) - 1, disable=disable):\n        sigma_down, sigma_up = get_ancestral_step(sigmas[i], sigmas[i + 1], eta=eta)\n        denoised = model(x, sigma_down * s_in, **extra_args)\n        d = to_d(x, sigma_down, denoised)\n        if callback is not None:\n            callback({'x': x, 'i': i, 'sigma': sigmas[i], 'sigma_down': sigma_down, 'denoised': denoised})\n        dt = sigmas[i + 1] - sigma_down\n        # Euler method\n        x = x + d * dt\n        if sigma_up > 0:\n            eps = noise_sampler(sigma_down, sigma_up) * s_noise\n            x = x + eps * (sigma_up ** 2 - sigma_down ** 2) ** 0.5\n    return x\n", "CodeBleu_score": 62.551973634948396}]}
{"instance_id": "6261ded88df2", "generation_result": [{"code": "def linear_multistep_coeff(order, t, i, j)\n\t\n    if order == 0:\n        return lambda x: x\n    elif order == 1:\n        if i == j:\n            return lambda x: x\n        else:\n            return lambda x: 0\n    else:\n        if i == j:\n            return lambda x: x\n        elif i == j - 1:\n            return lambda x: (x + 1) / 2\n        elif i == j - 2:\n            return lambda x: (x + 2) / 6\n        elif i == j - 3:\n            return lambda x: (x + 3) / 12\n        elif i == j - 4:\n            return lambda x: (x + 4) / 20\n        else:\n            raise ValueError(f'order {order} not supported for step {i}')\n\n", "CodeBleu_score": 21.508646896479476}, {"code": "def linear_multistep_coeff(order, t, i, j)\n\t\n    if order == 1:\n        return (t[i + 1] - t[i]) / 2\n    elif order == 2:\n        return (3 * (t[i + 1] ** 2 - t[i] ** 2) + (t[i + 1] - t[i]) ** 3) / 8\n    elif order == 3:\n        def term(k):\n            if k == 0:\n                return 0\n            elif k == 1:\n                return (t[i + 1] - t[i]) / 2\n            elif k == 2:\n                return (2 * (t[i + 1] ** 2 - t[i] ** 2) + (t[i + 1] - t[i]) ** 3) / 8\n            elif k == 3:\n                return (3 * (t[i + 1] ** 3 - t[i] ** 3) + (3 * (t[i + 1] ** 2 - t[i] ** 2) + (t[i + 1] - t[i]) ** 3) ** 2) / 16\n            else:\n                raise ValueError(f'The order {order} is too high for the step index {i}.')\n        return integrate.quad(term, 0, j)[0]\n    elif order == 4:\n        def term(k):\n            if k == 0:\n                return 0\n            elif k == 1:\n                return (t[i + 1] - t[i]) / 2\n            elif k == 2:\n                return (2 * (t[i + 1] ** 2 - t[i] ** 2) + (t[i + 1] - t[i]) ** 3) / 8\n            elif k == 3:\n                return (3 * (t[i + 1] ** 3 - t[i] ** 3) + (3 * (t[i + 1] ** 2 - t[i] ** 2) + (t[i + 1] - t[i]) ** 3) ** 2) / 16\n            elif k == 4:\n                return (4 * (t[i + 1] ** 4 - t[", "CodeBleu_score": 18.269432429708164}, {"code": "def linear_multistep_coeff(order, t, i, j)\n\t\n    if order == 0:\n        return 1.\n    elif order == 1:\n        return (t[j] - t[i]) / 2.\n    elif order == 2:\n        return (3. * (t[j] - t[i]) ** 2 - 2.) / 2.\n    elif order == 3:\n        return (15. * (t[j] - t[i]) ** 3 - 10. * (t[j] - t[i]) ** 2 + 3.) / 2.\n    elif order == 4:\n        return (105. * (t[j] - t[i]) ** 4 - 105. * (t[j] - t[i]) ** 3 + 15. * (t[j] - t[i]) ** 2 - 3.) / 2.\n    elif order == 5:\n        return (945. * (t[j] - t[i]) ** 5 - 1350. * (t[j] - t[i]) ** 4 + 600. * (t[j] - t[i]) ** 3 - 150. * (t[j] - t[i]) ** 2 + 15. * (t[j] - t[i]) - 1.) / 2.\n    else:\n        raise ValueError(f'Order {order} is not supported.')\n\n", "CodeBleu_score": 17.132682752967295}]}
{"instance_id": "4ee025e33ab1", "generation_result": [{"code": "def sample_lcm(model, x, sigmas, extra_args=None, callback=None, disable=None, noise_sampler=None)\n\t\n    extra_args = {} if extra_args is None else extra_args\n    noise_sampler = default_noise_sampler(x) if noise_sampler is None else noise_sampler\n    s_in = x.new_ones([x.shape[0]])\n\n    for i in trange(len(sigmas) - 1, disable=disable):\n        denoised = model(x, sigmas[i] * s_in, **extra_args)\n        if callback is not None:\n            callback({'x': x, 'i': i, 'sigma': sigmas[i], 'sigma_hat': sigmas[i], 'denoised': denoised})\n        x = denoised\n        if sigmas[i + 1] != 0:\n            x += noise_sampler(sigmas[i], sigmas[i + 1]) * sigmas[i + 1]\n    return x\n\n", "CodeBleu_score": 75.739795653633}, {"code": "def sample_lcm(model, x, sigmas, extra_args=None, callback=None, disable=None, noise_sampler=None)\n\t\n    extra_args = {} if extra_args is None else extra_args\n    noise_sampler = default_noise_sampler(x) if noise_sampler is None else noise_sampler\n    s_in = x.new_ones([x.shape[0]])\n\n    for i in trange(len(sigmas) - 1, disable=disable):\n        denoised = model(x, sigmas[i] * s_in, **extra_args)\n        if callback is not None:\n            callback({'x': x, 'i': i, 'sigma': sigmas[i], 'sigma_hat': sigmas[i], 'denoised': denoised})\n        x = x - denoised\n        if sigmas[i + 1] != 0:\n            x = x * sigmas[i + 1]\n            x = x + noise_sampler(sigmas[i], sigmas[i + 1]) * sigmas[i + 1]\n    return x\n\n", "CodeBleu_score": 74.61257868332721}, {"code": "def sample_lcm(model, x, sigmas, extra_args=None, callback=None, disable=None, noise_sampler=None)\n\t\n    extra_args = {} if extra_args is None else extra_args\n    noise_sampler = default_noise_sampler(x) if noise_sampler is None else noise_sampler\n    s_in = x.new_ones([x.shape[0]])\n\n    for i in trange(len(sigmas) - 1, disable=disable):\n        denoised = model(x, sigmas[i] * s_in, **extra_args)\n        if callback is not None:\n            callback({'x': x, 'i': i, 'sigma': sigmas[i], 'sigma_hat': sigmas[i], 'denoised': denoised})\n        x = (x - denoised) / sigmas[i]\n        if sigmas[i + 1] != 0:\n            x *= sigmas[i + 1]\n        if sigmas[i + 1] > 0:\n            x += noise_sampler(sigmas[i], sigmas[i + 1]) * sigmas[i + 1]\n    return x\n\n", "CodeBleu_score": 75.27879217972603}]}
{"instance_id": "cb51f91da606", "generation_result": [{"code": "def append_dims(x, target_dims)\n\t\n    dims_to_append = target_dims - x.dim()\n    if dims_to_append < 0:\n        raise ValueError(\n            f\"The tensor has too many dimensions ({x.dim()}). The target dimensions are {target_dims}.\"\n        )\n    if dims_to_append > 0:\n        if x.device.type == \"mps\":\n            x = x.detach().clone()\n        return x.unsqueeze(-1).expand(*x.shape, *[1] * dims_to_append)\n    return x\n\n", "CodeBleu_score": 40.51464684146154}, {"code": "def append_dims(x, target_dims)\n\t\n    dims_to_add = target_dims - len(x.shape)\n    if dims_to_add < 0:\n        raise ValueError(f\"The target dimensions ({target_dims}) are fewer than the current dimensions ({len(x.shape)})\")\n    if dims_to_add > 0:\n        if x.device.type == 'mps':\n            return x.detach().clone().expand(*((1,) * dims_to_add), *x.shape)\n        else:\n            return x.expand(*((1,) * dims_to_add), *x.shape)\n    else:\n        return x\n\n", "CodeBleu_score": 26.76882017606005}, {"code": "def append_dims(x, target_dims)\n\t\n    dims_to_append = target_dims - len(x.shape)\n    if dims_to_append < 0:\n        raise ValueError(f\"The tensor has {len(x.shape)} dimensions, which is more than the target dimensions ({target_dims}).\")\n    if x.is_mps:\n        return x.clone().detach().expand(*x.shape, *([1] * dims_to_append))\n    return x.expand(*x.shape, *([1] * dims_to_append))\n\n", "CodeBleu_score": 34.317614741173955}]}
{"instance_id": "9fac874a3bc7", "generation_result": [{"code": "def download_file(path, url, digest=None)\n\t\n    if not os.path.exists(path):\n        os.makedirs(os.path.dirname(path), exist_ok=True)\n        urllib.request.urlretrieve(url, path)\n    if digest:\n        with open(path, 'rb') as f:\n            file_hash = hashlib.sha256(f.read()).hexdigest()\n        if file_hash != digest:\n            raise RuntimeError(f'Digest mismatch for {path}: expected {digest} but got {file_hash}')\n    return path\n\n", "CodeBleu_score": 44.05870977432254}, {"code": "def download_file(path, url, digest=None)\n\t\n    if not os.path.exists(path):\n        os.makedirs(path)\n    path = os.path.join(path, os.path.basename(url))\n    if not os.path.exists(path):\n        print(f'Downloading {url} to {path}')\n        urllib.request.urlretrieve(url, path)\n    else:\n        print(f'File already exists at {path}')\n    if digest is not None:\n        with open(path, 'rb') as f:\n            file_hash = hashlib.sha256(f.read()).hexdigest()\n            if file_hash != digest:\n                raise RuntimeError(f'Hash mismatch for downloaded file {path}. Expected {digest}, got {file_hash}.')\n    return path\n\n", "CodeBleu_score": 48.22546099627941}, {"code": "def download_file(path, url, digest=None)\n\t\n    path = Path(path)\n    path.parent.mkdir(parents=True, exist_ok=True)\n    if not path.exists():\n        print(f'Downloading {url} to {path}')\n        urllib.request.urlretrieve(url, path)\n    else:\n        print(f'Using existing file {path}')\n    if digest is not None:\n        with open(path, 'rb') as f:\n            file_hash = hashlib.sha256(f.read()).hexdigest()\n        if file_hash != digest:\n            raise ValueError(f'Hash of downloaded file {path} does not match expected digest {digest}')\n    return path\n\n", "CodeBleu_score": 52.72778835758759}]}
{"instance_id": "f23165e897ce", "generation_result": [{"code": "def train_mode(model, mode=True)\n\t\n    if not isinstance(model, nn.Module):\n        raise TypeError('model must be an instance of nn.Module')\n    if not isinstance(mode, bool):\n        raise TypeError('mode must be a boolean')\n    modules = list(model.modules())\n    prev_modes = [m.training for m in modules]\n    try:\n        for m in modules:\n            m.train(mode)\n        yield\n    finally:\n        for m, p in zip(modules, prev_modes):\n            m.train(p)\n\n", "CodeBleu_score": 30.991536572125433}, {"code": "def train_mode(model, mode=True)\n\t\n    prev_states = {}\n    for name, module in model.named_modules():\n        prev_states[name] = module.training\n    for name, module in model.named_modules():\n        module.train(mode)\n    try:\n        yield\n    finally:\n        for name, module in model.named_modules():\n            module.train(prev_states[name])\n\n", "CodeBleu_score": 30.549707967381323}, {"code": "def train_mode(model, mode=True)\n\t\n    if not isinstance(model, nn.Module):\n        raise TypeError('model must be an instance of nn.Module')\n    if not isinstance(mode, bool):\n        raise TypeError('mode must be a boolean')\n    original_mode = []\n    for module in model.modules():\n        original_mode.append(module.training)\n        module.train(mode)\n    yield\n    for module, module_mode in zip(model.modules(), original_mode):\n        module.train(module_mode)\n\n", "CodeBleu_score": 28.57525875399773}]}
{"instance_id": "c6d59c6cb8e1", "generation_result": [{"code": "def ema_update(model, averaged_model, decay)\n\t\n    for current_params, averaged_params in zip(model.parameters(), averaged_model.parameters()):\n        # The update rule is:\n        # averaged_params := decay * averaged_params + (1 - decay) * current_params\n        averaged_params.data.mul_(decay).add_(current_params.data, alpha=1 - decay)\n\n    for current_buffer, averaged_buffer in zip(model.buffers(), averaged_model.buffers()):\n        averaged_buffer.data.copy_(current_buffer.data)\n\n", "CodeBleu_score": 30.210788334820627}, {"code": "def ema_update(model, averaged_model, decay)\n\t\n    for current_params, averaged_params in zip(model.parameters(), averaged_model.parameters()):\n        # Use the true average until the exponential average is more correct\n        averaged_params.data.mul_(decay).add_(current_params.data, alpha=1 - decay)\n    for current_buffer, averaged_buffer in zip(model.buffers(), averaged_model.buffers()):\n        averaged_buffer.data.copy_(current_buffer.data)\n\n", "CodeBleu_score": 30.210788334820627}, {"code": "def ema_update(model, averaged_model, decay)\n\t\n    # Use the true average until the exponential average is more correct\n    decay = min(1 - 1 / (model.global_step + 1), decay)\n    for model_param, avg_param in zip(model.parameters(), averaged_model.parameters()):\n        avg_param.data.mul_(decay).add_(model_param.data, alpha=1 - decay)\n    for model_buffer, avg_buffer in zip(model.buffers(), averaged_model.buffers()):\n        avg_buffer.data.copy_(model_buffer.data)\n\n", "CodeBleu_score": 31.331702398677052}]}
{"instance_id": "600c39a30798", "generation_result": [{"code": "def rand_log_logistic(shape, loc=0., scale=1., min_value=0., max_value=float('inf'), device='cpu', dtype=torch.float32)\n\t\n    min_value = torch.as_tensor(min_value, device=device, dtype=dtype)\n    max_value = torch.as_tensor(max_value, device=device, dtype=dtype)\n    cdf_min = torch.log(torch.exp(min_value) - 1)\n    cdf_max = torch.log(torch.exp(max_value) - 1)\n    uniform_samples = torch.rand(shape, device=device, dtype=dtype)\n    samples = torch.log(torch.exp(cdf_min) + (torch.exp(cdf_max) - torch.exp(cdf_min)) * uniform_samples)\n    return samples\n\n", "CodeBleu_score": 36.56156041528656}, {"code": "def rand_log_logistic(shape, loc=0., scale=1., min_value=0., max_value=float('inf'), device='cpu', dtype=torch.float32)\n\t\n    min_value = torch.as_tensor(min_value, device=device, dtype=dtype)\n    max_value = torch.as_tensor(max_value, device=device, dtype=dtype)\n    cdf_min = torch.log(torch.exp(min_value) - 1)\n    cdf_max = torch.log(torch.exp(max_value) - 1)\n    return torch.log(torch.log(1 + torch.exp(torch.rand(shape, device=device, dtype=dtype) * (cdf_max - cdf_min) + cdf_min)) + 1)\n", "CodeBleu_score": 34.88336207328556}, {"code": "def rand_log_logistic(shape, loc=0., scale=1., min_value=0., max_value=float('inf'), device='cpu', dtype=torch.float32)\n\t\n    min_value = torch.as_tensor(min_value, device=device, dtype=dtype)\n    max_value = torch.as_tensor(max_value, device=device, dtype=dtype)\n    min_cdf = logit(min_value)\n    max_cdf = logit(max_value)\n    cdf_range = max_cdf - min_cdf\n    return torch.exp(torch.log(cdf_range) * torch.rand(shape, device=device, dtype=dtype) + min_cdf)\n\n", "CodeBleu_score": 38.11151815495654}]}
{"instance_id": "75e2def2467f", "generation_result": [{"code": "def get_activation(activation: Literal[\"elu\", \"snake\", \"none\"], antialias=False, channels=None) -> nn.Module\n\t\n    if activation == \"elu\":\n        return nn.ELU()\n    elif activation == \"snake\":\n        return SnakeBeta(channels)\n    elif activation == \"none\":\n        return nn.Identity()\n    else:\n        raise ValueError(f\"Invalid activation type: {activation}\")\n\n", "CodeBleu_score": 42.608873649955676}, {"code": "def get_activation(activation: Literal[\"elu\", \"snake\", \"none\"], antialias=False, channels=None) -> nn.Module\n\t\n    if activation == \"elu\":\n        activation = nn.ELU(alpha=1.0, inplace=True)\n    elif activation == \"snake\":\n        activation = SnakeBeta(channels)\n    elif activation == \"none\":\n        activation = nn.Identity()\n    else:\n        raise ValueError(f\"Invalid activation type: {activation}\")\n\n    if antialias:\n        activation = comfy.ops.Activation1d(activation)\n\n    return activation\n\n", "CodeBleu_score": 61.26924343089254}, {"code": "def get_activation(activation: Literal[\"elu\", \"snake\", \"none\"], antialias=False, channels=None) -> nn.Module\n\t\n    if activation == \"elu\":\n        activation = nn.ELU()\n    elif activation == \"snake\":\n        if channels is None:\n            raise ValueError(\"channels must be specified if activation is snake\")\n        activation = SnakeBeta(channels)\n    elif activation == \"none\":\n        activation = nn.Identity()\n    else:\n        raise ValueError(\"activation must be one of elu, snake, none\")\n\n    if antialias:\n        activation = comfy.ops.Activation1d(activation)\n\n    return activation\n\n", "CodeBleu_score": 55.17386320735033}]}
{"instance_id": "ef1714c79e51", "generation_result": [{"code": "def apply_rotary_pos_emb(t, freqs, scale = 1)\n\t\n    device = t.device\n    dtype = t.dtype\n    if dtype != torch.float32:\n        t = t.to(torch.float32)\n\n    if len(t.shape) == 2:\n        t = rearrange(t, 'n d -> n () d')\n\n    if len(freqs.shape) == 2:\n        freqs = rearrange(freqs, 'n d -> n () d')\n\n    t = t * scale\n    freqs = freqs.to(dtype=t.dtype, device=device)\n    return torch.cat((t, t), dim = -2) + torch.einsum('... i d, ... j d -> ... i j', freqs, t)\n\n", "CodeBleu_score": 27.49067842244789}, {"code": "def apply_rotary_pos_emb(t, freqs, scale = 1)\n\t\n    device = t.device\n    dtype = t.dtype\n    t = t.to(torch.float32)\n\n    # t = rearrange(t, '... (j d) -> ... j d', j = 2)\n    t = t.reshape(-1, 2, t.shape[-1])\n    t1, t2 = t.unbind(dim = -2)\n    t1, t2 = map(lambda t: rearrange(t, 'n -> n ()'), (t1, t2))\n    t1, t2 = map(lambda t: t * scale, (t1, t2))\n    # freqs = rearrange(freqs, '... (j d) -> ... j d', j = 2)\n    freqs = freqs.reshape(-1, 2, freqs.shape[-1])\n    freq1, freq2 = freqs.unbind(dim = -2)\n    # t1, t2 = map(lambda t: t * freq1, (t1, t2))\n    t1 = t1 * freq1\n    t2 = t2 * freq2\n    t = torch.stack((t1, t2), dim = -2)\n    t = torch.cat((t, t), dim = -2)\n    t = t.reshape(t.shape[0], -1, t.shape[-1])\n    t = t.to(dtype=dtype, device=device)\n    return t\n\n", "CodeBleu_score": 37.454126076475035}, {"code": "def apply_rotary_pos_emb(t, freqs, scale = 1)\n\t\n    device = t.device\n    dtype = t.dtype\n\n    if dtype != torch.float32:\n        t = t.to(torch.float32)\n\n    t = rearrange(t, '... (j d) -> ... j d', j = 2)\n    t1, t2 = t.unbind(dim = -2)\n    t2 = rotate_half(t2)\n    t = torch.stack((t1, t2), dim = -2)\n    t = rearrange(t, '... j d -> ... (j d)')\n    t = t * scale.to(dtype=dtype, device=device)\n    return torch.cat((t, freqs), dim = -1)\n\n", "CodeBleu_score": 31.075526094140958}]}
{"instance_id": "a7a5f178c6f6", "generation_result": [{"code": "def idx2vq(self, idx, dim=-1)\n\t\n    return torch.index_select(self.codebook.weight, dim, idx)\n", "CodeBleu_score": 19.80834683526523}, {"code": "def idx2vq(self, idx, dim=-1)\n\t\n    vq = torch.index_select(self.codebook.weight, 0, idx)\n    if dim != -1 and dim != vq.dim():\n        vq = vq.transpose(dim, -1)\n    return vq\n\n", "CodeBleu_score": 38.13402124740265}, {"code": "def idx2vq(self, idx, dim=-1)\n\t\n    return torch.index_select(self.codebook.weight, dim, idx)\n\n", "CodeBleu_score": 19.80834683526523}]}
{"instance_id": "4104c041fb20", "generation_result": [{"code": "def encode(self, x, quantize=False)\n\t\n    x = self.in_block(x)\n    skips = []\n    for i, block in enumerate(self.down_blocks):\n        if i > 0:\n            x = F.avg_pool2d(x, 2)\n        x = block(x)\n        if i in self.skip_indices:\n            skips.append(x)\n    x = self.vquantizer(x, quantize=quantize)\n    return x if quantize else x, skips\n\n", "CodeBleu_score": 35.557943980463946}, {"code": "def encode(self, x, quantize=False)\n\t\n    # Encode\n    z_e_x = self.in_block(x)\n    for block in self.down_blocks:\n        z_e_x = block(z_e_x)\n    z_e_x = z_e_x.permute(0, 2, 3, 1).contiguous().view(-1, z_e_x.size(-1))\n\n    # Quantize\n    if quantize:\n        z_q_x, indices, vq_loss = self.vquantizer(z_e_x)\n        return z_q_x, z_e_x, indices, vq_loss\n    else:\n        return z_e_x\n\n", "CodeBleu_score": 32.508125811658076}, {"code": "def encode(self, x, quantize=False)\n\t\n    x = self.in_block(x)\n    x_down = [x]\n    for block in self.down_blocks:\n        x_down.append(block(x_down[-1]))\n    x_e = x_down[-1]\n\n    if quantize:\n        z_e_x, (vq_loss, commit_loss), indices = self.vquantizer(x_e)\n        return z_e_x, x_e, indices, vq_loss, commit_loss\n    else:\n        return x_e\n\n", "CodeBleu_score": 29.093869480709916}]}
{"instance_id": "403c52ba2d3b", "generation_result": [{"code": "def ema_scope(self, context=None)\n\t\n    if self.use_ema:\n        if context is not None:\n            context.log(f\"Switching to EMA weights.\")\n        self.model_ema(self)\n        yield\n        if context is not None:\n            context.log(f\"Restoring original weights.\")\n        self.model_ema(self, reverse=True)\n    else:\n        yield\n\n", "CodeBleu_score": 39.62764687760042}, {"code": "def ema_scope(self, context=None)\n\t\n    if self.use_ema:\n        if context is not None:\n            context.log.info(f\"Switching to EMA weights.\")\n        with self.model_ema.average_parameters():\n            yield\n    else:\n        yield\n\n", "CodeBleu_score": 23.621083233260155}, {"code": "def ema_scope(self, context=None)\n\t\n    if self.use_ema:\n        if context is not None:\n            context.log(f\"Using EMA weights for {self.__class__.__name__}.\")\n        with self.model_ema.average_parameters():\n            yield\n    else:\n        yield\n", "CodeBleu_score": 23.93290562900808}]}
{"instance_id": "94b7467f98e4", "generation_result": [{"code": "def post_attention(self, attn, x, gate_msa, shift_mlp, scale_mlp, gate_mlp)\n\t\n    x = x + attn * gate_msa.sigmoid()\n    x = self.norm2(x)\n    x = modulate(x, shift_mlp, scale_mlp)\n    x = self.mlp(x) * gate_mlp.sigmoid()\n    return x\n\n", "CodeBleu_score": 40.81355277851249}, {"code": "def post_attention(self, attn, x, gate_msa, shift_mlp, scale_mlp, gate_mlp)\n\t\n    x = x + gate_msa * attn\n    x = self.norm2(x)\n    x = modulate(x, shift_mlp, scale_mlp)\n    x = self.mlp(x)\n    return x * gate_mlp\n\n", "CodeBleu_score": 36.51173718422086}, {"code": "def post_attention(self, attn, x, gate_msa, shift_mlp, scale_mlp, gate_mlp)\n\t\n    x = x + gate_msa * attn\n    x = self.norm2(x)\n    x = modulate(x, shift_mlp, scale_mlp)\n    x = self.mlp(x)\n    return x * gate_mlp\n\n", "CodeBleu_score": 36.51173718422086}]}
{"instance_id": "d9fdc05542a2", "generation_result": [{"code": "def _block_mixing(context, x, context_block, x_block, c)\n\t\n    # context\n    qkv_context, intermediates_context = context_block.pre_attention(x, c)\n    attn_context = optimized_attention(\n        qkv_context,\n        num_heads=context_block.attn.num_heads,\n    )\n    x = context_block.post_attention(attn_context, *intermediates_context)\n    # x\n    qkv_x, intermediates_x = x_block.pre_attention(context, c)\n    attn_x = optimized_attention(\n        qkv_x,\n        num_heads=x_block.attn.num_heads,\n    )\n    context = x_block.post_attention(attn_x, *intermediates_x)\n    return context, x\n\n", "CodeBleu_score": 23.963850114951956}, {"code": "def _block_mixing(context, x, context_block, x_block, c)\n\t\n    qkv_context, intermediates_context = context_block.pre_attention(x, c)\n    qkv_x, intermediates_x = x_block.pre_attention(context, c)\n    qkv = torch.cat((qkv_context, qkv_x), dim=1)\n    attn = optimized_attention(qkv, num_heads=context_block.attn.num_heads)\n    attn_context, attn_x = attn.split(context.size(0), dim=1)\n    context = x_block.post_attention(\n        attn_context,\n        context,\n        *intermediates_context,\n    )\n    x = context_block.post_attention(\n        attn_x,\n        x,\n        *intermediates_x,\n    )\n    return context, x\n\n", "CodeBleu_score": 26.2728705354905}, {"code": "def _block_mixing(context, x, context_block, x_block, c)\n\t\n    assert not context_block.pre_only\n    assert not x_block.pre_only\n    assert context_block.attn.attn_mode == x_block.attn.attn_mode\n    assert context_block.attn.qk_norm == x_block.attn.qk_norm\n    assert context_block.attn.rmsnorm == x_block.attn.rmsnorm\n    assert context_block.attn.pre_only == x_block.attn.pre_only\n    assert context_block.attn.qkv_bias == x_block.attn.qkv_bias\n    qkv_context, intermediates_context = context_block.pre_attention(context, c)\n    qkv_x, intermediates_x = x_block.pre_attention(x, c)\n    qkv = torch.cat((qkv_context, qkv_x), dim=1)\n    attn = optimized_attention(qkv, num_heads=context_block.attn.num_heads)\n    attn_context = attn[:, : context.shape[1], :]\n    attn_x = attn[:, context.shape[1] :, :]\n    context = context_block.post_attention(\n        attn_context, context, *intermediates_context\n    )\n    x = x_block.post_attention(attn_x, x, *intermediates_x)\n    return context, x\n", "CodeBleu_score": 39.7271583707581}]}
{"instance_id": "b219bce30160", "generation_result": [{"code": "def get_timestep_embedding(timesteps, embedding_dim)\n\t\n    assert len(timesteps.shape) == 1  # and timesteps.dtype == tf.int32\n    half_dim = embedding_dim // 2\n    emb = math.log(10000) / (half_dim - 1)\n    emb = torch.exp(torch.arange(half_dim, dtype=torch.float32) * -emb)\n    emb = emb.to(device)\n    emb = timesteps.float()[:, None] * emb[None, :]\n    emb = torch.cat([torch.sin(emb), torch.cos(emb)], axis=1)\n    if embedding_dim % 2 == 1:\n        emb = torch.nn.functional.pad(emb, (0, 1, 0, 0))\n    assert emb.shape == (timesteps.shape[0], embedding_dim)\n    return emb\n\n", "CodeBleu_score": 82.50323718343945}, {"code": "def get_timestep_embedding(timesteps, embedding_dim)\n\t\n    assert len(timesteps.shape) == 1  # and timesteps.dtype == tf.int32\n    half_dim = embedding_dim // 2\n    emb = math.log(10000) / (half_dim - 1)\n    emb = torch.exp(torch.arange(half_dim, dtype=torch.float32) * -emb)\n    emb = torch.arange(timesteps.shape[1], dtype=torch.float32).unsqueeze(1) * emb.unsqueeze(0)\n    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n    if embedding_dim % 2 == 1:\n        emb = torch.nn.functional.pad(emb, (0,1,0,0))\n    return emb\n\n", "CodeBleu_score": 78.36095277029935}, {"code": "def get_timestep_embedding(timesteps, embedding_dim)\n\t\n    assert len(timesteps.shape) == 1  # and timesteps.dtype == tf.int32\n    half_dim = embedding_dim // 2\n    emb = math.log(10000) / (half_dim - 1)\n    emb = torch.exp(torch.arange(half_dim, dtype=torch.float32) * -emb)\n    emb = emb.to(timesteps.device)\n    emb = timesteps.float()[:, None] * emb[None, :]\n    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n    if embedding_dim % 2 == 1:\n        emb = torch.nn.functional.pad(emb, (0,1,0,0))\n    assert emb.shape == (timesteps.shape[0], embedding_dim)\n    return emb\n", "CodeBleu_score": 85.66361672143236}]}
{"instance_id": "6ae8453c86be", "generation_result": [{"code": "def pytorch_attention(q, k, v)\n\t\n    # compute attention\n    b,c,h,w = q.shape\n\n    q = q.reshape(b,c,h*w)\n    q = q.permute(0,2,1)   # b,hw,c\n    k = k.reshape(b,c,h*w) # b,c,hw\n    v = v.reshape(b,c,h*w)\n\n    try:\n        r1 = torch.bmm(q, k)\n        scale = (int(q.shape[-1])**(-0.5))\n        r1 = r1 * scale\n        r1 = torch.nn.functional.softmax(r1, dim=2)\n        r1 = torch.bmm(r1, v)\n    except RuntimeError as e:\n        if \"out of memory\" in str(e):\n            logging.warning(\"out of memory error, switching to slice attention\")\n            r1 = slice_attention(q, k, v)\n        else:\n            raise e\n\n    h_ = r1.permute(0,2,1).reshape(b,c,h,w)\n    return h_\n\n", "CodeBleu_score": 32.548940640135264}, {"code": "def pytorch_attention(q, k, v)\n\t\n    # compute attention\n    b,c,h,w = q.shape\n\n    q = q.reshape(b,c,h*w)\n    q = q.permute(0,2,1)   # b,hw,c\n    k = k.reshape(b,c,h*w) # b,c,hw\n    v = v.reshape(b,c,h*w)\n\n    try:\n        r1 = torch.bmm(q, k)\n    except: #operation not implemented for bf16\n        r1 = xformers_attention(q, k, v)\n\n    scale = (int(q.shape[-1])**(-0.5))\n    r1 = r1 * scale\n\n    r2 = torch.nn.functional.softmax(r1, dim=2).permute(0,2,1)\n    del r1\n\n    r3 = torch.bmm(v, r2)\n    del r2\n\n    h_ = r3.reshape(b,c,h,w)\n    del r3\n    return h_\n\n", "CodeBleu_score": 27.831593200758537}, {"code": "def pytorch_attention(q, k, v)\n\t\n    # compute attention\n    b,c,h,w = q.shape\n\n    q = q.reshape(b,c,h*w)\n    q = q.permute(0,2,1)   # b,hw,c\n    k = k.reshape(b,c,h*w) # b,c,hw\n    v = v.reshape(b,c,h*w)\n\n    try:\n        out = torch.nn.functional.softmax(torch.bmm(q, k), dim=2).permute(0,2,1) # b,hw,hw\n        out = torch.bmm(v, out) # b,c,hw\n        out = out.reshape(b,c,h,w)\n    except RuntimeError as e:\n        if \"out of memory\" in str(e):\n            logging.warning(\"out of memory error, switching to slice attention\")\n            out = slice_attention(q, k, v)\n        else:\n            raise e\n    return out\n\n", "CodeBleu_score": 31.242774784673838}]}
{"instance_id": "fe4433c987ad", "generation_result": [{"code": "def apply_control(h, control, name)\n\t\n    if name in control:\n        c = control.pop(name)\n        try:\n            h = h + c\n        except Exception as e:\n            print(f\"Failed to add control {name} to {h}.\")\n            print(e)\n    return h\n\n", "CodeBleu_score": 32.298840636542835}, {"code": "def apply_control(h, control, name)\n\t\n    if name in control and control[name]:\n        try:\n            h += control[name]\n        except:\n            logging.warning(f\"Unable to apply control {name} to {h}\")\n    return h\n\n", "CodeBleu_score": 28.970731345858436}, {"code": "def apply_control(h, control, name)\n\t\n    if name in control and control[name]:\n        try:\n            h = h + control[name]\n        except Exception as e:\n            warnings.warn(f\"Control {name} could not be applied to h: {e}\")\n    return h\n", "CodeBleu_score": 34.31232247939046}]}
{"instance_id": "56da4547c101", "generation_result": [{"code": "def get_codebook_entry(self, indices, shape)\n\t\n    indices = indices.unsqueeze(1)\n    shape = shape if shape is not None else [-1]\n    quantized = F.embedding(indices, self.embedding.weight)\n    quantized = quantized.view(shape + [-1]).permute(0, 3, 1, 2)\n    return quantized\n\n", "CodeBleu_score": 23.384655733347845}, {"code": "def get_codebook_entry(self, indices, shape)\n\t\n    # get quantized latent vectors\n    z_q = torch.matmul(indices.float(), self.embedding.weight).view(shape)\n    # reshape back to match original input shape\n    z_q = z_q.permute(0, 3, 1, 2).contiguous()\n    return z_q\n\n", "CodeBleu_score": 17.265442814614886}, {"code": "def get_codebook_entry(self, indices, shape)\n\t\n    indices = indices.unsqueeze(1)\n    shape = shape if shape is not None else [-1] + list(indices.shape[1:])\n    quantized = F.embedding(indices, self.embedding.weight)\n    return quantized.view(shape)\n\n", "CodeBleu_score": 21.479447263435556}]}
{"instance_id": "548f8a1fb37d", "generation_result": [{"code": "def make_noise(self)\n\t\n    device = self.constant_input.weight.device\n    noises = [torch.randn(1, 1, 4, 4, device=device)]\n    for i in range(3, self.log_size + 1):\n        for _ in range(2):\n            noises.append(torch.randn(1, 1, 2**i, 2**i, device=device))\n    return noises\n\n", "CodeBleu_score": 69.5232193949351}, {"code": "def make_noise(self)\n\t\n    device = self.constant_input.weight.device\n    noises = [torch.randn(1, 1, 4, 4, device=device)]\n    for size in [2**i for i in range(3, self.log_size + 1)]:\n        for _ in range(2):\n            noises.append(torch.randn(1, 1, size, size, device=device))\n    return noises\n\n", "CodeBleu_score": 56.345787414401435}, {"code": "def make_noise(self)\n\t\n    device = self.constant_input.input.device\n    noises = [torch.randn(1, 1, 4, 4, device=device)]\n    for size in [2**i for i in range(3, self.log_size + 1)]:\n        for _ in range(2):\n            noises.append(torch.randn(1, 1, size, size, device=device))\n    return noises\n\n", "CodeBleu_score": 53.62261372414338}]}
{"instance_id": "42fd805563c6", "generation_result": [{"code": "def mean_latent(self, num_latent)\n\t\n    latent_in = torch.randn(num_latent, self.num_style_feat, device=self.style_mlp[0].weight.device)\n    latent_out = self.style_mlp(latent_in).mean(0, keepdim=True)\n    return latent_out\n\n", "CodeBleu_score": 47.74171329398577}, {"code": "def mean_latent(self, num_latent)\n\t\n    latent_in = torch.randn(num_latent, self.num_style_feat, device=self.device)\n    latent_out = self.style_mlp(latent_in).mean(0, keepdim=True)\n    return latent_out\n", "CodeBleu_score": 43.21509971004608}, {"code": "def mean_latent(self, num_latent)\n\t\n    latent_in = torch.randn(num_latent, self.num_style_feat, device=self.style_mlp[0].weight.device)\n    latent_out = self.style_mlp(latent_in).mean(0, keepdim=True)\n    return latent_out\n\n", "CodeBleu_score": 47.74171329398577}]}
{"instance_id": "2f485fd924af", "generation_result": [{"code": "def color_jitter(img, shift)\n\t\n    img = img.permute(1, 2, 0)\n    img = adjust_hue(img, np.random.uniform(-shift, shift))\n    img = adjust_saturation(img, np.random.uniform(1, 1.5))\n    img = adjust_contrast(img, np.random.uniform(1, 1.5))\n    img = adjust_brightness(img, np.random.uniform(1, 1.5))\n    img = img.permute(2, 0, 1)\n    return img\n\n", "CodeBleu_score": 38.9554579632708}, {"code": "def color_jitter(img, shift)\n\t\n    jitter = torch.rand(3) - 0.5\n    jitter = jitter * shift\n    img = img + jitter\n    img = torch.clamp(img, 0, 1)\n    return img\n\n", "CodeBleu_score": 43.8292946647512}, {"code": "def color_jitter(img, shift)\n\t\n    # Randomly shift the image's RGB values.\n    img = img.astype(np.float32)\n    r_shift = np.random.uniform(-shift, shift)\n    g_shift = np.random.uniform(-shift, shift)\n    b_shift = np.random.uniform(-shift, shift)\n    img[:, :, 0] = np.clip(img[:, :, 0] + r_shift, 0, 1)\n    img[:, :, 1] = np.clip(img[:, :, 1] + g_shift, 0, 1)\n    img[:, :, 2] = np.clip(img[:, :, 2] + b_shift, 0, 1)\n    return img\n\n", "CodeBleu_score": 24.10833594598491}]}
{"instance_id": "c87ccf1fbf48", "generation_result": [{"code": "def feed_data(self, data)\n\t\n    self.lq = data['lq'].to(self.device)\n    if 'gt' in data:\n        self.gt = data['gt'].to(self.device)\n    if 'facial_component_locations' in data:\n        self.facial_component_locations = data['facial_component_locations']\n    if 'facial_component_locations_gt' in data:\n        self.facial_component_locations_gt = data['facial_component_locations_gt']\n\n", "CodeBleu_score": 53.71683130712028}, {"code": "def feed_data(self, data)\n\t\n    self.lq = data['lq'].to(self.device)\n    if 'gt' in data:\n        self.gt = data['gt'].to(self.device)\n    if 'left_eye' in data:\n        self.left_eye = data['left_eye'].to(self.device)\n    if 'right_eye' in data:\n        self.right_eye = data['right_eye'].to(self.device)\n    if 'mouth' in data:\n        self.mouth = data['mouth'].to(self.device)\n\n\n", "CodeBleu_score": 56.77114126112309}, {"code": "def feed_data(self, data)\n\t\n    self.lq = data['lq'].to(self.device)\n    if 'gt' in data:\n        self.gt = data['gt'].to(self.device)\n    if 'left_eye' in data:\n        self.left_eye = data['left_eye'].to(self.device)\n    if 'right_eye' in data:\n        self.right_eye = data['right_eye'].to(self.device)\n    if 'mouth' in data:\n        self.mouth = data['mouth'].to(self.device)\n", "CodeBleu_score": 56.77114126112309}]}
{"instance_id": "8f8c7cb13037", "generation_result": [{"code": "def construct_img_pyramid(self)\n\t\n    # get image pyramid\n    self.img_pyramid = [self.gt]\n    for i in range(self.num_scales - 1):\n        self.img_pyramid.insert(0, F.interpolate(self.img_pyramid[0], scale_factor=0.5, mode='bilinear', align_corners=False))\n\n", "CodeBleu_score": 24.719548087616204}, {"code": "def construct_img_pyramid(self)\n\t\n    # construct image pyramid\n    self.img_pyramid = [self.gt]\n    for i in range(self.net_g.num_scales - 1):\n        self.img_pyramid.insert(0, F.interpolate(self.img_pyramid[0], scale_factor=0.5, mode='bilinear', align_corners=False))\n\n    # uncomment to check data\n    # import torchvision\n    # if self.opt['rank'] == 0:\n        for i in range(self.net_g.num_scales):\n            torchvision.utils.save_image(\n                self.img_pyramid[i], f'tmp/pyramid/pyramid_{i}_{self.idx}.png', nrow=4, padding=2, normalize=True, range=(-1, 1))\n\n\n\n", "CodeBleu_score": 24.636689654515916}, {"code": "def construct_img_pyramid(self)\n\t\n    img_pyramid = [self.gt]\n    for i in range(self.num_pyramid_levels):\n        downscaled_img = F.interpolate(img_pyramid[-1], scale_factor=0.5, mode='bilinear')\n        img_pyramid.insert(0, downscaled_img)\n    return img_pyramid\n\n", "CodeBleu_score": 35.58237736903557}]}
{"instance_id": "51247274d40c", "generation_result": [{"code": "def save(self, epoch, current_iter)\n\t\n    logger = get_root_logger()\n    if self.ema_decay > 0:\n        # save the EMA network\n        model_name = f'net_g_ema_{self.ema_decay:.3f}.pth'\n        model_path = osp.join(self.opt['path']['models'], model_name)\n        self.save_network(self.net_g_ema, model_path, current_iter)\n        logger.info(f'Saving EMA model {model_name} for iteration {current_iter} and epoch {epoch}.')\n    else:\n        logger.info('No EMA model is used for training.')\n\n    # save the current network\n    model_name = f'net_g_{self.opt[\"name\"]}.pth'\n    model_path = osp.join(self.opt['path']['models'], model_name)\n    self.save_network(self.net_g, model_path, current_iter)\n    logger.info(f'Saving model {model_name} for iteration {current_iter} and epoch {epoch}.')\n\n    # save the discriminator networks\n    if self.use_facial_disc:\n        model_name = f'net_d_left_eye_{self.opt[\"name\"]}.pth'\n        model_path = osp.join(self.opt['path']['models'], model_name)\n        self.save_network(self.net_d_left_eye, model_path, current_iter)\n        logger.info(f'Saving model {model_name} for iteration {current_iter} and epoch {epoch}.')\n        model_name = f'net_d_right_eye_{self.opt[\"name\"]}.pth'\n        model_path = osp.join(self.opt['path']['models'], model_name)\n        self.save_network(self.net_d_right_eye, model_path, current_iter)\n        logger.info(f'Saving model {model_name} for iteration {current_iter} and epoch {epoch}.')\n        model_name = f'net_d_mouth_{self.opt[\"name\"]}.pth'\n        model_path = osp.join(self.opt['path']['models'], model_name)\n        self.save_network(self.net_d_mouth,", "CodeBleu_score": 41.356878865752435}, {"code": "def save(self, epoch, current_iter)\n\t\n    if not osp.exists(self.opt['path']['models']):\n        os.makedirs(self.opt['path']['models'])\n    if not osp.exists(self.opt['path']['training_state']):\n        os.makedirs(self.opt['path']['training_state'])\n    # save models\n    if hasattr(self, 'net_g_ema'):\n        torch.save(self.net_g_ema.state_dict(), osp.join(self.opt['path']['models'], f'net_g_ema_{epoch}_{current_iter}.pth'))\n    torch.save(self.net_g.state_dict(), osp.join(self.opt['path']['models'], f'net_g_{epoch}_{current_iter}.pth'))\n    torch.save(self.net_d.state_dict(), osp.join(self.opt['path']['models'], f'net_d_{epoch}_{current_iter}.pth'))\n    if self.use_facial_disc:\n        torch.save(self.net_d_left_eye.state_dict(), osp.join(self.opt['path']['models'], f'net_d_left_eye_{epoch}_{current_iter}.pth'))\n        torch.save(self.net_d_right_eye.state_dict(), osp.join(self.opt['path']['models'], f'net_d_right_eye_{epoch}_{current_iter}.pth'))\n        torch.save(self.net_d_mouth.state_dict(), osp.join(self.opt['path']['models'], f'net_d_mouth_{epoch}_{current_iter}.pth'))\n    # save training states\n    state = {\n        'iter': current_iter,\n        'epoch': epoch,\n        'optimizer_g': self.optimizer_g.state_dict(),\n        'optimizer_d': self.optimizer_d.state_dict(),\n        'optimizer_d_left_eye': self.optimizer_d_left_eye.state_dict(),\n        'optimizer_d_right_eye': self.optimizer_d_right_eye.state_dict(),\n        'optimizer_d_mouth': self.optimizer_d_mouth.state_dict(),\n        'best_metric_results': self.best_metric_results,\n        'current", "CodeBleu_score": 32.24355241682508}, {"code": "def save(self, epoch, current_iter)\n\t\n    if hasattr(self, 'net_g_ema'):\n        torch.save(\n            self.net_g_ema.state_dict(),\n            osp.join(self.opt['path']['models'], f'net_g_ema_{str(epoch).zfill(3)}_{str(current_iter).zfill(8)}.pth')\n        )\n    torch.save(\n        self.net_g.state_dict(),\n        osp.join(self.opt['path']['models'], f'net_g_{str(epoch).zfill(3)}_{str(current_iter).zfill(8)}.pth')\n    )\n    if self.use_facial_disc:\n        torch.save(\n            self.net_d_left_eye.state_dict(),\n            osp.join(self.opt['path']['models'], f'net_d_left_eye_{str(epoch).zfill(3)}_{str(current_iter).zfill(8)}.pth')\n        )\n        torch.save(\n            self.net_d_right_eye.state_dict(),\n            osp.join(self.opt['path']['models'], f'net_d_right_eye_{str(epoch).zfill(3)}_{str(current_iter).zfill(8)}.pth')\n        )\n        torch.save(\n            self.net_d_mouth.state_dict(),\n            osp.join(self.opt['path']['models'], f'net_d_mouth_{str(epoch).zfill(3)}_{str(current_iter).zfill(8)}.pth')\n        )\n    torch.save(\n        self.net_d.state_dict(),\n        osp.join(self.opt['path']['models'], f'net_d_{str(epoch).zfill(3)}_{str(current_iter).zfill(8)}.pth')\n    )\n    if self.opt['rank'] == 0:\n        self.save_training_state(epoch, current_iter)\n    logger = get_root_logger()\n    logger.info(f'save model in {self.opt[\"path\"][\"models\"]}')\n\n", "CodeBleu_score": 34.05333460667205}]}
{"instance_id": "afa0dc8128c1", "generation_result": [{"code": "def clean_folder(folder)\n\t\n    for filename in os.listdir(folder):\n        file_path = os.path.join(folder, filename)\n        try:\n            if os.path.isfile(file_path) or os.path.islink(file_path):\n                os.unlink(file_path)\n            elif os.path.isdir(file_path):\n                shutil.rmtree(file_path)\n        except Exception as e:\n            print('Failed to delete %s. Reason: %s' % (file_path, e))\n", "CodeBleu_score": 69.71577776852638}, {"code": "def clean_folder(folder)\n\t\n    for filename in os.listdir(folder):\n        file_path = os.path.join(folder, filename)\n        try:\n            if os.path.isfile(file_path) or os.path.islink(file_path):\n                os.unlink(file_path)\n            elif os.path.isdir(file_path):\n                shutil.rmtree(file_path)\n        except Exception as e:\n            print('Failed to delete %s. Reason: %s' % (file_path, e))\n", "CodeBleu_score": 69.71577776852638}, {"code": "def clean_folder(folder)\n\t\n    for filename in os.listdir(folder):\n        file_path = os.path.join(folder, filename)\n        try:\n            if os.path.isfile(file_path) or os.path.islink(file_path):\n                os.unlink(file_path)\n            elif os.path.isdir(file_path):\n                shutil.rmtree(file_path)\n        except Exception as e:\n            print('Failed to delete %s. Reason: %s' % (file_path, e))\n", "CodeBleu_score": 69.71577776852638}]}
{"instance_id": "53588b481bdd", "generation_result": [{"code": "def next_batch(self, sample_size=1)\n\t\n    if sample_size > len(self.exps):\n        sample_size = len(self.exps)\n    if sample_size <= 0:\n        return None\n    indices = random.sample(range(len(self.exps)), sample_size)\n    batch = [self.exps[i] for i in indices]\n    for i in sorted(indices, reverse=True):\n        del self.exps[i]\n    return batch\n", "CodeBleu_score": 47.30955691687083}, {"code": "def next_batch(self, sample_size=1)\n\t\n    if sample_size > len(self.exps):\n        sample_size = len(self.exps)\n\n    # Select random samples\n    samples = random.sample(self.exps, sample_size)\n    # Remove samples from the list\n    self.exps = [exp for exp in self.exps if exp not in samples]\n    return samples\n\n", "CodeBleu_score": 66.31566625865123}, {"code": "def next_batch(self, sample_size=1)\n\t\n    if sample_size > len(self.exps):\n        sample_size = len(self.exps)\n    batch = random.sample(self.exps, sample_size)\n    self.exps = list(set(self.exps) - set(batch))\n    return batch\n", "CodeBleu_score": 52.993334797361236}]}
{"instance_id": "6bfa6a4e621c", "generation_result": [{"code": "def find_estimated_top_configs(self)\n\t\n    all_features = []\n    for config in self.all_configs:\n        all_features.append(flatten_dict(config))\n    all_features = np.array(all_features)\n\n    if self.metric == AUTOTUNING_METRIC_LATENCY:\n        all_features = all_features[:, 0:2]\n    elif self.metric == AUTOTUNING_METRIC_PATH:\n        all_features = all_features[:, 2:]\n\n    all_features = all_features.astype(np.float32)\n    all_features = np.nan_to_num(all_features)\n\n    # TODO: add a cost model\n    if len(self.evaluated_perf) > 0:\n        self.cost_model.fit(self.evaluated_configs, self.evaluated_perf)\n        pred_perfs = self.cost_model.predict(all_features)\n        pred_perfs = np.nan_to_num(pred_perfs)\n    else:\n        pred_perfs = np.random.uniform(0, 1, all_features.shape[0])\n\n    if self.metric == AUTOTUNING_METRIC_LATENCY:\n        pred_perfs = pred_perfs[:, 0]\n    elif self.metric == AUTOTUNING_METRIC_PATH:\n        pred_perfs = pred_perfs[:, 1]\n\n    pred_perfs = np.array(pred_perfs)\n    indices = np.argsort(pred_perfs)[::-1]\n    return indices\n\n", "CodeBleu_score": 36.8319185819182}, {"code": "def find_estimated_top_configs(self)\n\t\n    logger.info(\"Start finding estimated top configs\")\n    all_configs = self.all_configs\n    all_configs_flattened = []\n    for config in all_configs:\n        config_flattened = flatten_config(config)\n        all_configs_flattened.append(config_flattened)\n    all_configs_flattened = np.array(all_configs_flattened)\n    all_configs_flattened = all_configs_flattened.astype(np.float32)\n    logger.info(f\"all_configs_flattened: {all_configs_flattened}\")\n    estimated_perf = self.cost_model.predict(all_configs_flattened)\n    estimated_perf = estimated_perf.astype(np.float32)\n    logger.info(f\"estimated_perf: {estimated_perf}\")\n    sorted_idx = np.argsort(estimated_perf)\n    logger.info(f\"sorted_idx: {sorted_idx}\")\n    estimated_top_configs = sorted_idx[-self.num_top_trials :]\n    logger.info(f\"estimated_top_configs: {estimated_top_configs}\")\n    return estimated_top_configs\n\n", "CodeBleu_score": 34.0159372138332}, {"code": "def find_estimated_top_configs(self)\n\t\n    flattened_configs = []\n    for config in self.all_configs:\n        flattened_configs.append(flatten_config(config))\n    flattened_configs = np.array(flattened_configs)\n    flattened_configs = flattened_configs.astype(np.float32)\n    flattened_configs = flattened_configs.reshape(flattened_configs.shape[0], -1)\n    estimated_perf = self.cost_model.predict(flattened_configs)\n    estimated_perf = estimated_perf.reshape(estimated_perf.shape[0])\n    if self.metric == AUTOTUNING_METRIC_LATENCY:\n        estimated_perf = 1 / estimated_perf\n    top_indices = np.argsort(estimated_perf)[::-1]\n    return top_indices\n\n", "CodeBleu_score": 38.94543068957036}]}
{"instance_id": "d4cf48413a57", "generation_result": [{"code": "def flatten(d, parent_key='', sep='_')\n\t\n    items = []\n    for k, v in d.items():\n        new_key = parent_key + sep + k if parent_key else k\n        if isinstance(v, collections.abc.MutableMapping):\n            items.extend(flatten(v, new_key, sep=sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)\n\n", "CodeBleu_score": 82.56863782179195}, {"code": "def flatten(d, parent_key='', sep='_')\n\t\n    items = []\n    for k, v in d.items():\n        new_key = parent_key + sep + k if parent_key else k\n        if isinstance(v, collections.MutableMapping):\n            items.extend(flatten(v, new_key, sep=sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)\n", "CodeBleu_score": 76.93644067302131}, {"code": "def flatten(d, parent_key='', sep='_')\n\t\n    items = []\n    for k, v in d.items():\n        new_key = parent_key + sep + k if parent_key else k\n        if isinstance(v, collections.MutableMapping):\n            items.extend(flatten(v, new_key, sep=sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)\n\n", "CodeBleu_score": 76.93644067302131}]}
{"instance_id": "c2a514249c25", "generation_result": [{"code": "def run_after_tuning(self)\n\t\n    if self.optimal_cmd:\n        logger.info(\n            f\\\"Found an optimal DeepSpeed configuration. Running the command specified in {self.args.ds_config_path}\\\"\n        )\n        p = subprocess.Popen(self.optimal_cmd)\n        p.wait()\n        if p.returncode != 0:\n            logger.info(\n                f\\\"The command specified in {self.args.ds_config_path} failed with return code {p.returncode}.\\\"\n            )\n        else:\n            logger.info(\n                f\\\"The command specified in {self.args.ds_config_path} completed successfully.\\\"\n            )\n    else:\n        logger.info(\n            f\\\"No optimal DeepSpeed configuration found. Run {self.args.ds_config_path} to find the optimal configuration.\\\"\n        )\n\n\n\n", "CodeBleu_score": 44.56928627942428}, {"code": "def run_after_tuning(self)\n\t\n    if self.optimal_cmd is not None:\n        logger.info(\\\"Running the optimal command\\\")\n        with open(os.path.join(self.results_dir, \\\"run_after_tuning.log\\\"), \\\"w\\\") as fd:\n            p = subprocess.Popen(self.optimal_cmd, stdout=fd, stderr=fd)\n            p.wait()\n            if p.returncode == 0:\n                logger.info(\\\"The optimal command completed successfully\\\")\n            else:\n                logger.info(\\\"The optimal command did not complete successfully\\\")\n    else:\n        logger.info(\\\"No optimal command found by autotuning. Please check the logs to see if any optimal command was found.\\\")\n\n\n\n", "CodeBleu_score": 40.7956484036964}, {"code": "def run_after_tuning(self)\n\t\n    if self.optimal_cmd:\n        logger.info(f\\\"Running the optimal DeepSpeed configuration found by autotuning.\\\")\n        logger.info(f\\\"Command: {self.optimal_cmd}\\\")\n        logger.info(f\\\"DeepSpeed config: {self.optimal_ds_config}\\\")\n        p = subprocess.Popen(self.optimal_cmd)\n        p.wait()\n        if p.returncode == 0:\n            logger.info(\\\"Success!\\\")\n        else:\n            logger.info(\\\"Failure!\\\")\n    else:\n        logger.info(\\\"No optimal DeepSpeed configuration found by autotuning. Please check the logs for the tuning results.\\\")\n\n\n\n", "CodeBleu_score": 53.43350253414908}]}
{"instance_id": "61802de52df0", "generation_result": [{"code": "def run_job(self, exp: dict, reservations)\n\t\n    exp_id = exp[\"exp_id\"]\n    result_dir = exp[\"result_dir\"]\n    exp_name = exp[\"name\"]\n    script = exp[\"script\"]\n    args = exp[\"args\"]\n    ds_config = exp[\"ds_config\"]\n    host = exp[\"host\"]\n    port = exp[\"port\"]\n    num_gpus = exp[\"num_gpus\"]\n    master_port = exp[\"master_port\"]\n    master_addr = exp[\"master_addr\"]\n    node_id = exp[\"node_id\"]\n    num_nodes = exp[\"num_nodes\"]\n    num_nodes_requested = exp[\"num_nodes_requested\"]\n    num_nodes_available = exp[\"num_nodes_available\"]\n    num_nodes_requested_available = exp[\"num_nodes_requested_available\"]\n    num_nodes_requested_unavailable = exp[\"num_nodes_requested_unavailable\"]\n    num_nodes_requested_unavailable_gpu = exp[\"num_nodes_requested_unavailable_gpu\"]\n    num_nodes_requested_unavailable_cpu = exp[\"num_nodes_requested_unavailable_cpu\"]\n    num_nodes_requested_unavailable_mem = exp[\"num_nodes_requested_unavailable_mem\"]\n    num_nodes_requested_unavailable_disk = exp[\"num_nodes_requested_unavailable_disk\"]\n    num_nodes_requested_unavailable_gpu_mem = exp[\"num_nodes_requested_unavailable_gpu_mem\"]\n    num_nodes_requested_unavailable_gpu_disk = exp[\"num_nodes_requested_unavailable_gpu_disk\"]\n    num_nodes_requested_unavailable_gpu_cpu = exp[\"num_nodes_requested_unavailable_gpu_cpu\"]\n    num_nodes_requested_unavailable_cpu_mem = exp[\"num_nodes_requested_unavailable_cpu_mem\"]\n    num_nodes_requested_unavailable_cpu_disk = exp[\"num_nodes_requested_unavailable_cpu_disk\"]\n    num_nodes_requested_unavailable_cpu_gpu = exp[\"num_nodes_requested_unavailable_cpu_gpu\"]\n    num_nodes_requested_unavailable_mem_disk = exp[\"num_nodes_requested_unavailable_mem_disk\"]\n    num_nodes_requested_unavailable_mem_", "CodeBleu_score": 7.912064608302689}, {"code": "def run_job(self, exp: dict, reservations)\n\t\n    exp_id = exp[\"exp_id\"]\n    exp_path = exp[\"exp_path\"]\n    exp_name = exp[\"name\"]\n    exp_args = exp[\"args\"]\n    exp_ds_config = exp[\"ds_config\"]\n    exp_script = exp[\"script\"]\n    exp_result_dir = exp[\"result_dir\"]\n    exp_master_port = exp[\"master_port\"] = dist.get_free_port()\n    exp_master_addr = exp[\"master_addr\"] = self.nodes[0].host\n    exp_master_addr_port = f\\\"{exp_master_addr}:{exp_master_port}\\\"\n\n    # set user arguments\n    for k, v in self.arg_mappings.items():\n        k = k.strip()\n        v = v.strip()\n        if k in exp_args:\n            exp_args[k] = v\n\n    # set deepspeed config\n    exp_ds_config[\\\"train_batch_size\\\"] = get_val_by_key(exp_ds_config, \\\"train_batch_size\\\") or get_val_by_key(exp_ds_config, \\\"train_micro_batch_size_per_gpu\\\") * self.num_gpus_per_node\n    exp_ds_config[\\\"gradient_accumulation_steps\\\"] = get_val_by_key(exp_ds_config, \\\"gradient_accumulation_steps\\\") or 1\n    exp_ds_config[\\\"train_micro_batch_size_per_gpu\\\"] = get_val_by_key(exp_ds_config, \\\"train_micro_batch_size_per_gpu\\\") or get_val_by_key(exp_ds_config, \\\"train_batch_size\\\") // self.num_gpus_per_node // exp_ds_config[\\\"gradient_accumulation_steps\\\"]\n    exp_ds_config[\\\"steps_per_print\\\"] = get_val_by_key(exp_ds_config, \\\"steps_per_print\\\") or 1\n    exp_ds_config[\\\"optimizer.params.lr\\\"] = get_val_by_key(exp_ds_config, \\\"optimizer.params.lr\\\") or get_val_by_key(exp_ds_config, \\\"lr\\\")\n    exp_ds_config[\\\"optimizer.", "CodeBleu_score": 21.20100360536674}, {"code": "def run_job(self, exp: dict, reservations)\n\t\n    exp_id = exp[\"exp_id\"]\n    result_dir = exp[\"result_dir\"]\n    os.makedirs(result_dir, exist_ok=True)\n\n    exp[\"master_port\"] = self.get_free_port()\n    exp[\"result_dir\"] = result_dir\n    exp[\"reservations\"] = reservations\n    exp[\"ds_config\"][\"train_batch_size\"] = exp[\"train_batch_size\"]\n    exp[\"ds_config\"][\"train_micro_batch_size_per_gpu\"] = exp[\"train_micro_batch_size_per_gpu\"]\n    exp[\"ds_config\"][\"gradient_accumulation_steps\"] = exp[\"gradient_accumulation_steps\"]\n\n    # update user arguments based on argument mappings\n    for k, v in self.arg_mappings.items():\n        if k in exp[\"ds_config\"]:\n            exp[\"ds_config\"][v] = exp[\"ds_config\"][k]\n            del exp[\"ds_config\"][k]\n\n    # write experiment details to file\n    with open(os.path.join(result_dir, \"exp.hjson\"), \"w\") as fd:\n        hjson.dump(exp, fd, default=lambda x: str(x))\n\n    # write experiment details to file\n    with open(os.path.join(result_dir, \"exp.json\"), \"w\") as fd:\n        json.dump(exp, fd, default=lambda x: str(x))\n\n    # write experiment details to file\n    with open(os.path.join(result_dir, \"exp.base64\"), \"w\") as fd:\n        fd.write(base64.b64encode(json.dumps(exp).encode(\"utf-8\")).decode(\"utf-8\"))\n\n    # write experiment details to file\n    with open(os.path.join(result_dir, \"exp.txt\"), \"w\") as fd:\n        fd.write(json.dumps(exp))\n\n    # write experiment details to file\n    with open(os.path.join(result_dir, \"exp.base64.txt\"), \"w\") as fd:\n        fd.write(base64.b64encode(json.dumps(exp).encode(\"utf-8\")).decode(\"utf-8\"))\n\n    # write experiment details to file\n    with open(os.path.join(result_", "CodeBleu_score": 24.64879930664558}]}
{"instance_id": "4884e2e9665c", "generation_result": [{"code": "def save_exp_results_to_database(self, message, ranks=None, path=None):\n\t\n    if self.dist_init and ranks is not None and self.dist_rank in ranks:\n        message[\"rank\"] = self.dist_rank\n    if self.dist_init and path is not None:\n        with open(path, \"a\") as f:\n            f.write(json.dumps(message) + \"", "CodeBleu_score": 18.428612951188555}, {"code": "def save_exp_results_to_database(self, message, ranks=None, path=None):\n\t\n    if ranks is None:\n        ranks = [0]\n    if self.distributed_available and dist.is_initialized() and dist.get_rank() in ranks:\n        message[\\\"rank\\\"] = dist.get_rank()\n        if path is None:\n            path = os.path.join(self.results_dir, \\\"metrics.json\\\")\n        with open(path, \\\"a\\\") as fd:\n            fd.write(json.dumps(message) + \\\"\\", "CodeBleu_score": 27.70318797757636}, {"code": "def save_exp_results_to_database(self, message, ranks=None, path=None):\n\t\n    if self.initialized:\n        if ranks is None:\n            ranks = self.ranks\n        if self.rank in ranks:\n            if self.rank == 0 or path is None:\n                message[\"rank\"] = self.rank\n                with open(path, \"a\") as fd:\n                    fd.write(json.dumps(message) + \"", "CodeBleu_score": 20.941916143734144}]}
{"instance_id": "0429344c3c4d", "generation_result": [{"code": "def _build_tp_other_layer_map(self, layer_index: int)\n\t\n    assert layer_index < len(self.layer_keys)\n    if self.layer_keys[layer_index] not in self.mp_rank_files:\n        return {}\n\n    files = self.mp_rank_files[self.layer_keys[layer_index]]\n    assert len(files) == self.tp_degree\n    return {i: files[i::self.tp_degree] for i in range(self.tp_degree)}\n\n\n\n", "CodeBleu_score": 27.063720684080806}, {"code": "def _build_tp_other_layer_map(self, layer_index: int)\n\t\n    if not self.has_tp_files():\n        return {}\n\n    assert layer_index < len(self.layer_keys)\n\n    tp_to_layer_map = {}\n    for rank_file in self.mp_rank_files:\n        sd = torch.load(rank_file, map_location=torch.device('cpu'))\n        layer_sd = sd[self.layer_keys[layer_index]]\n        for tp_index, tp_sd in enumerate(layer_sd):\n            if not tp_index in tp_to_layer_map.keys():\n                tp_to_layer_map[tp_index] = []\n            tp_to_layer_map[tp_index].append(rank_file)\n\n    return tp_to_layer_map\n", "CodeBleu_score": 36.368956865327775}, {"code": "def _build_tp_other_layer_map(self, layer_index: int)\n\t\n    layer_files = self.zero_checkpoint.get_files_for_layer(layer_index)\n    if not layer_files:\n        return {}\n\n    layer_files = [fname for fname in layer_files if fname.startswith(f'{PREFIX_CHECKPOINT_FILES}{PREFIX_LAYER}{layer_index}{PREFIX_FILE_NAME}')]\n    assert len(layer_files) == self.tp_degree, f'The number of layer files {len(layer_files)} is not equal to tp_degree {self.tp_degree}'\n\n    layer_files_by_tp = [layer_files[i::self.tp_degree] for i in range(self.tp_degree)]\n    return {i: layer_files_by_tp[i] for i in range(self.tp_degree)}\n\n", "CodeBleu_score": 33.87812190858778}]}
{"instance_id": "1b65ecb9be93", "generation_result": [{"code": "def system_info()\n\t\n    import platform\n    import sys\n    import numpy\n    import torch\n    import TTS\n    import json\n\n    sys.path += [os.path.abspath(\"..\"), os.path.abspath(\".\")]\n\n    system_info = {\n        \"Operating System\": platform.system(),\n        \"Architecture\": platform.machine(),\n        \"Version\": platform.version(),\n        \"Processor\": platform.processor(),\n        \"Python Version\": platform.python_version(),\n        \"PyTorch Version\": torch.__version__,\n        \"Numpy Version\": numpy.__version__,\n        \"TTS Version\": TTS.__version__,\n    }\n    return json.dumps(system_info, indent=4)\n\n", "CodeBleu_score": 35.38588001078935}, {"code": "def system_info()\n\t\n    info = {\n        \"OS\": platform.system(),\n        \"Architecture\": platform.machine(),\n        \"Version\": platform.version(),\n        \"Processor\": platform.processor(),\n        \"Python\": platform.python_version(),\n        \"PyTorch\": torch.__version__,\n        \"NumPy\": numpy.__version__,\n        \"TTS\": TTS.__version__,\n    }\n    return info\n\n", "CodeBleu_score": 48.483711493828096}, {"code": "def system_info()\n\t\n    info = {'Operating System': platform.system(),\n            'Architecture': platform.architecture(),\n            'Version': platform.version(),\n            'Processor': platform.processor(),\n            'Python Version': platform.python_version()}\n    return info\n\n", "CodeBleu_score": 50.351440029362095}]}
{"instance_id": "969bcc5cf3b5", "generation_result": [{"code": "def cuda_info()\n\t\n    try:\n        import torch\n        return {\n            \"cuda\": torch.cuda.is_available(),\n            \"cuda_version\": torch.version.cuda,\n            \"gpu_count\": torch.cuda.device_count(),\n            \"gpu_names\": [torch.cuda.get_device_name(i) for i in range(torch.cuda.device_count())],\n        }\n    except:\n        return {\n            \"cuda\": False,\n            \"cuda_version\": None,\n            \"gpu_count\": 0,\n            \"gpu_names\": [],\n        }\n\n", "CodeBleu_score": 59.37871735925091}, {"code": "def cuda_info()\n\t\n    import torch\n    import subprocess\n    import json\n    import os\n\n    # get the CUDA version\n    cuda_version = torch.version.cuda\n    if cuda_version is None:\n        cuda_version = \"Not Available\"\n\n    # get the available GPU devices\n    gpu_devices = torch.cuda.device_count()\n\n    # get the CUDA driver version\n    try:\n        nvcc_version = subprocess.check_output([\"nvcc\", \"--version\"]).decode(\"utf-8\")\n        nvcc_version = nvcc_version.split(\",\")[2].split(\" \")[-1]\n    except FileNotFoundError:\n        nvcc_version = \"Not Available\"\n\n    # get the CUDA compute capability\n    try:\n        compute_capability = subprocess.check_output([\"nvcc\", \"--version\"]).decode(\"utf-8\")\n        compute_capability = compute_capability.split(\",\")[2].split(\" \")[-2]\n    except FileNotFoundError:\n        compute_capability = \"Not Available\"\n\n    # get the CUDA available devices\n    try:\n        cuda_devices = subprocess.check_output([\"nvidia-smi\", \"--list-gpus\"]).decode(\"utf-8\")\n        cuda_devices = [\n            line.split(\":\")[1].strip()\n            for line in cuda_devices.split(\"", "CodeBleu_score": 35.65114303670115}, {"code": "def cuda_info()\n\t\n    return \"\"\"\\\"\\\"\\\"Get detailed info about the CUDA setup.\\\"\\\"\\\"", "CodeBleu_score": 2.3161978425290095}]}
{"instance_id": "a2ba908a4c70", "generation_result": [{"code": "def compute_encoder_accuracy(dataset_items, encoder_manager)\n\t\n    class_name_key = encoder_manager.config.get('class_name_key', None)\n    class_id_to_name = encoder_manager.config.get('class_id_to_name', None)\n    class_name_to_id = encoder_manager.config.get('class_name_to_id', None)\n    if class_name_key is None:\n        raise ValueError('class_name_key must be specified in the encoder manager configuration.')\n    if class_id_to_name is None and class_name_to_id is None:\n        raise ValueError('class_id_to_name or class_name_to_id must be specified in the encoder manager configuration.')\n    if class_id_to_name is not None and class_name_to_id is not None:\n        raise ValueError('class_id_to_name and class_name_to_id cannot both be specified in the encoder manager configuration.')\n    if class_id_to_name is not None and not isinstance(class_id_to_name, dict):\n        raise ValueError('class_id_to_name must be a dictionary.')\n    if class_name_to_id is not None and not isinstance(class_name_to_id, dict):\n        raise ValueError('class_name_to_id must be a dictionary.')\n    if class_id_to_name is not None and len(class_id_to_name) != len(set(class_id_to_name.values())):\n        raise ValueError('class_id_to_name must have unique values.')\n    if class_name_to_id is not None and len(class_name_to_id) != len(set(class_name_to_id.values())):\n        raise ValueError('class_name_to_id must have unique values.')\n    if class_id_to_name is not None and len(class_id_to_name) != len(class_name_to_id):\n        raise ValueError('class_id_to_name and class_name_to_id must have the same number of items.')\n    if class_name_to_id is not None and len(class_name_to_id) != len(class_id_to_name):\n        raise ValueError('class_id_to_name and class_name", "CodeBleu_score": 23.99122479498476}, {"code": "def compute_encoder_accuracy(dataset_items, encoder_manager)\n\t\n    class_names = encoder_manager.get_class_names()\n    class_name_to_id = encoder_manager.get_class_name_to_id()\n    class_id_to_name = encoder_manager.get_class_id_to_name()\n    if class_names is not None and class_name_to_id is not None and class_id_to_name is not None:\n        class_names = [name for name in class_names if name in class_name_to_id]\n        class_name_to_id = {name: class_name_to_id[name] for name in class_names}\n        class_id_to_name = {class_name_to_id[name]: name for name in class_names}\n    else:\n        class_names = []\n        class_name_to_id = {}\n        class_id_to_name = {}\n    if len(class_names) == 0:\n        print(\"WARNING: No classes found in the dataset. Skipping accuracy calculation.\")\n        return\n    print(\"Computing accuracy for classes: %s\" % (\", \".join(class_names)))\n    encoder_manager.eval()\n    class_accuracies = {name: 0 for name in class_names}\n    num_items = len(dataset_items)\n    for i, item in enumerate(tqdm(dataset_items, total=num_items)):\n        audio_file = item[0]\n        audio_features = encoder_manager.get_audio_features(audio_file)\n        if audio_features is None:\n            continue\n        class_name = item[1]\n        if class_name not in class_name_to_id:\n            continue\n        class_id = class_name_to_id[class_name]\n        predicted_class_id = encoder_manager.predict(audio_features)\n        if predicted_class_id == class_id:\n            class_accuracies[class_name] += 1\n    total_accuracy = 0\n    for class_name in class_names:\n        accuracy = class_accuracies[class_name] / num_items\n        total_accuracy += accuracy\n        print(\"Accuracy for class '%s': %.2f%% (%d/%d)\" % (class_name, accuracy * 100, class_accurac", "CodeBleu_score": 35.659864908609336}, {"code": "def compute_encoder_accuracy(dataset_items, encoder_manager)\n\t\n    class_names = encoder_manager.config.class_names\n    class_id_to_name = encoder_manager.config.class_id_to_name\n    if class_names is None:\n        raise ValueError(\"No class names were provided in the encoder configuration.\")\n    if class_id_to_name is None:\n        raise ValueError(\"No class ID to class name mapping was provided in the encoder configuration.\")\n    num_classes = len(class_names)\n    num_correct = [0] * num_classes\n    num_total = [0] * num_classes\n    for item in tqdm(dataset_items, desc=\"Computing encoder accuracy\"):\n        audio_file = item[\"audio_filepath\"]\n        true_class_name = item[\"speaker_name\"]\n        true_class_id = item[\"speaker_id\"]\n        if true_class_id is None:\n            true_class_id = -1\n        true_class_id = int(true_class_id)\n        if true_class_id < 0 or true_class_id >= num_classes:\n            raise ValueError(\n                f\"The true class ID {true_class_id} is out of range. The class ID must be between 0 and {num_classes - 1}.\"\n            )\n        true_class_name = class_id_to_name[true_class_id]\n        embedding = encoder_manager.extract_embedding(audio_file)\n        predicted_class_id = encoder_manager.predict(embedding)\n        predicted_class_name = class_id_to_name[predicted_class_id]\n        if predicted_class_id == true_class_id:\n            num_correct[true_class_id] += 1\n        num_total[true_class_id] += 1\n    for i in range(num_classes):\n        print(\n            f\"Accuracy for class {class_names[i]}: {num_correct[i] / num_total[i] * 100:.2f}%\"\n        )\n    print(f\"Average accuracy: {sum(num_correct) / sum(num_total) * 100:.2f}%\")\n\n", "CodeBleu_score": 31.316214403589072}]}
{"instance_id": "e01ec69ad58a", "generation_result": [{"code": "def adjust_path_and_remove_silence(audio_path)\n\t\n\n    # get input and output paths\n    input_path = audio_path\n    output_path = input_path.replace(args.input_dir, args.output_dir)\n    output_dir = os.path.dirname(output_path)\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    output_path = output_path.replace(\".wav\", \"_no_silence.wav\")\n    if os.path.exists(output_path) and not args.force:\n        print(f\"File {output_path} already exists. Skipping.\")\n        return output_path, False\n    # load audio\n    audio, rate = librosa.load(input_path, sr=args.sampling_rate)\n    # remove silence\n    audio_no_silence = remove_silence(\n        audio,\n        rate,\n        args.vad_aggressiveness,\n        args.vad_padding,\n        args.vad_window_length,\n        args.vad_hop_length,\n        args.vad_mel_bins,\n        args.vad_fmin,\n        args.vad_fmax,\n        args.vad_sample_rate,\n        args.vad_frame_length_ms,\n        args.vad_frame_shift_ms,\n        args.vad_min_level_db,\n        args.vad_ref_level_db,\n        args.vad_power,\n        args.vad_preemphasis,\n        args.vad_signal_norm,\n        args.vad_symmetric_norm,\n        args.vad_max_norm,\n        args.vad_clip_norm,\n        args.vad_mel_norm,\n        args.vad_do_trim_silence,\n    )\n    # save audio\n    sf.write(output_path, audio_no_silence, rate)\n    return output_path, True\n\n", "CodeBleu_score": 40.51862562039765}, {"code": "def adjust_path_and_remove_silence(audio_path)\n\t\n    output_path = audio_path.replace(args.input_dir, args.output_dir)\n    output_path = pathlib.Path(output_path)\n    if output_path.exists() and not args.force:\n        return output_path, False\n    output_path.parent.mkdir(parents=True, exist_ok=True)\n    vad_model, utils = get_vad_model_and_utils(args.vad_model)\n    vad_model.eval()\n    waveform, sample_rate = utils.load_wav_to_torch(audio_path)\n    with torch.no_grad():\n        activity = vad_model(waveform)\n    activity = activity.cpu().numpy().flatten()\n    start_idx, end_idx = remove_silence(waveform.cpu().numpy(), activity, sample_rate, args.vad_moving_average_width,\n                                        args.vad_max_silence_length)\n    if start_idx == -1 and end_idx == -1:\n        return output_path, False\n    waveform = waveform[start_idx:end_idx]\n    utils.save_wav(waveform, sample_rate, output_path)\n    return output_path, True\n\n", "CodeBleu_score": 42.317522556882345}, {"code": "def adjust_path_and_remove_silence(audio_path)\n\t\n    audio_path = pathlib.Path(audio_path)\n    output_path = audio_path.parent.parent / \"no_silence\" / audio_path.parent.stem / audio_path.stem\n    output_path = output_path.with_suffix(\".wav\")\n    if output_path.exists() and not args.force:\n        return output_path, False\n    output_path.parent.mkdir(parents=True, exist_ok=True)\n    vad_model, utils = get_vad_model_and_utils()\n    audio, sample_rate = utils.load_wav_to_torch(audio_path)\n    audio = audio.unsqueeze(0)\n    audio = utils.vad(audio, vad_model, sample_rate)\n    audio = remove_silence(audio, sample_rate, args.silence_threshold, args.min_silence_duration)\n    utils.save_wav(audio[0], str(output_path), sample_rate)\n    return output_path, True\n\n", "CodeBleu_score": 43.34657697250418}]}
{"instance_id": "7e0bdb0feff8", "generation_result": [{"code": "def resample_files(input_dir, output_sr, output_dir=None, file_ext=\"wav\", n_jobs=10)\n\t\n    if output_dir is None:\n        output_dir = input_dir\n    copytree(input_dir, output_dir)\n    audio_files = glob.glob(os.path.join(output_dir, \"**\", \"*.\" + file_ext), recursive=True)\n    with Pool(n_jobs) as pool:\n        list(\n            tqdm(\n                pool.imap_unordered(resample_file, [(x, output_sr) for x in audio_files]),\n                total=len(audio_files),\n            )\n        )\n    print(\"Resampling complete.\")\n\n", "CodeBleu_score": 37.3864965570912}, {"code": "def resample_files(input_dir, output_sr, output_dir=None, file_ext=\"wav\", n_jobs=10)\n\t\n    if output_dir is None:\n        output_dir = input_dir\n    else:\n        copytree(input_dir, output_dir)\n    audio_files = glob.glob(f\"{output_dir}/**/*.{file_ext}\", recursive=True)\n    func_args = [(af, output_sr) for af in audio_files]\n    with Pool(n_jobs) as p:\n        list(tqdm(p.imap(resample_file, func_args), total=len(func_args)))\n    print(f\"Resampled {len(audio_files)} files to {output_sr} Hz.\")\n\n", "CodeBleu_score": 27.16030081386238}, {"code": "def resample_files(input_dir, output_sr, output_dir=None, file_ext=\"wav\", n_jobs=10)\n\t\n    if output_dir is not None:\n        copytree(input_dir, output_dir)\n    audio_files = glob.glob(f\"{input_dir}/**/*.{file_ext}\", recursive=True)\n    func_args = [(af, output_sr) for af in audio_files]\n    with Pool(n_jobs) as pool:\n        list(tqdm(pool.imap_unordered(resample_file, func_args), total=len(func_args)))\n    print(f\"Resampled {len(audio_files)} audio files to {output_sr} Hz.\")\n\n", "CodeBleu_score": 29.032495681072845}]}
{"instance_id": "a1351834ca57", "generation_result": [{"code": "def evaluation(model, criterion, data_loader, global_step)\n\t\n    model.eval()\n    total_loss = 0.0\n    total_num_samples = 0\n    with torch.no_grad():\n        for i, (x, y, input_lengths, label_lengths) in enumerate(data_loader):\n            # reshape input to group samples of each class\n            x = x.view(\n                c.num_classes_in_batch,\n                c.num_utter_per_class,\n                c.num_mels,\n                -1,\n            )\n            y = y.view(\n                c.num_classes_in_batch,\n                c.num_utter_per_class,\n                -1,\n            )\n            input_lengths = input_lengths.view(\n                c.num_classes_in_batch,\n                c.num_utter_per_class,\n            )\n            label_lengths = label_lengths.view(\n                c.num_classes_in_batch,\n                c.num_utter_per_class,\n            )\n            if use_cuda:\n                x = x.cuda(non_blocking=True)\n                y = y.cuda(non_blocking=True)\n                input_lengths = input_lengths.cuda(non_blocking=True)\n                label_lengths = label_lengths.cuda(non_blocking=True)\n            # forward pass\n            y_pred = model(x, input_lengths)\n            y_pred = y_pred.transpose(1, 2).contiguous()\n            y_pred = y_pred.view(\n                -1,\n                y_pred.size(2),\n            )\n            y = y.view(\n                -1,\n                y.size(2),\n            )\n            # compute loss\n            loss = criterion(y_pred, y, input_lengths, label_lengths)\n            # update total loss\n            total_loss += loss.item() * y.size(0)\n            total_num_samples += y.size(0)\n            # log statistics\n            if i % c.log_interval == 0 and i > 0:\n                avg_loss = total_loss / total_num_samples\n                print(\n                    f\" > Validation: [{i}/{len(data_loader)}] | Loss: {avg_loss:.4f}\"\n                )\n    # generate UMAP plot\n    if c.model_params.get(\"use_torch_spec\", False):\n        embeddings =", "CodeBleu_score": 32.31593007992517}, {"code": "def evaluation(model, criterion, data_loader, global_step)\n\t\n    model.eval()\n    total_loss = 0\n    total_num_samples = 0\n    total_num_correct = 0\n    total_num_samples_per_class = [0] * len(train_classes)\n    total_num_correct_per_class = [0] * len(train_classes)\n    total_num_correct_per_class_top5 = [0] * len(train_classes)\n    total_num_samples_per_class_top5 = [0] * len(train_classes)\n    total_num_correct_per_class_top10 = [0] * len(train_classes)\n    total_num_samples_per_class_top10 = [0] * len(train_classes)\n    total_num_correct_per_class_top20 = [0] * len(train_classes)\n    total_num_samples_per_class_top20 = [0] * len(train_classes)\n    total_num_correct_per_class_top50 = [0] * len(train_classes)\n    total_num_samples_per_class_top50 = [0] * len(train_classes)\n    total_num_correct_per_class_top100 = [0] * len(train_classes)\n    total_num_samples_per_class_top100 = [0] * len(train_classes)\n    total_num_correct_per_class_top200 = [0] * len(train_classes)\n    total_num_samples_per_class_top200 = [0] * len(train_classes)\n    total_num_correct_per_class_top500 = [0] * len(train_classes)\n    total_num_samples_per_class_top500 = [0] * len(train_classes)\n    total_num_correct_per_class_top1000 = [0] * len(train_classes)\n    total_num_samples_per_class_top1000 = [0] * len(train_classes)\n    total_num_correct_per_class_top2000 = [0] * len(train_classes)\n    total_num_", "CodeBleu_score": 9.690804033156544}, {"code": "def evaluation(model, criterion, data_loader, global_step)\n\t\n    model.eval()\n    total_loss = 0.0\n    total_num_samples = 0\n    total_num_utterances = 0\n    with torch.no_grad():\n        for i, batch in enumerate(data_loader):\n            # dispatch data to GPU\n            if use_cuda:\n                batch = [tensor.cuda() for tensor in batch]\n            # prepare data\n            inputs, input_lengths, labels, label_lengths, _ = batch\n            # forward\n            outputs = model(inputs, input_lengths)\n            # reshape to group samples of each class\n            outputs = outputs.view(outputs.shape[0] // c.num_utter_per_class, -1, outputs.shape[-1])\n            labels = labels.view(labels.shape[0] // c.num_utter_per_class, -1)\n            # compute loss\n            loss = criterion(outputs, labels)\n            # compute average loss over all utterances\n            total_loss += loss.item() * outputs.shape[0]\n            total_num_samples += outputs.shape[0]\n            total_num_utterances += outputs.shape[1]\n            # generate and save embeddings plot\n            if i == len(data_loader) - 1 and c.model_params.get(\"use_embs\", False):\n                embeddings = outputs.cpu().numpy()\n                plot_embeddings(embeddings, global_step, c.vis_params, c.model_dir)\n    # compute average loss\n    avg_loss = total_loss / total_num_samples\n    # log evaluation statistics\n    print(\n        f\"| Evaluation Loss: {avg_loss:.4f} | Evaluation Samples: {total_num_samples} | Evaluation Utterances: {total_num_utterances} |\"\n    )\n    return avg_loss\n\n", "CodeBleu_score": 34.89158735612261}]}
{"instance_id": "565f8ed09916", "generation_result": [{"code": "def register_config(model_name: str) -> Coqpit\n\t\n    config_class_name = model_name.capitalize() + \"Config\"\n    try:\n        config_class = find_module(config_class_name, [\"TTS.config.model_configs\"])\n    except ModuleNotFoundError:\n        raise ModuleNotFoundError(f\"No configuration class found for model {model_name}.\")\n    return config_class\n\n", "CodeBleu_score": 23.30004138134538}, {"code": "def register_config(model_name: str) -> Coqpit\n\t\n    config_class_name = model_name.upper() + \"_CONFIG\"\n    if model_name == \"tts\":\n        from TTS.tts.utils.generic_utils import find_module\n\n        config_class = find_module(config_class_name, [os.path.join(\"TTS\", \"tts\", \"config\")])\n    else:\n        from TTS.utils.generic_utils import find_module\n\n        config_class = find_module(config_class_name, [os.path.join(\"TTS\", \"config\")])\n    if config_class is None:\n        raise ModuleNotFoundError(\n            f\"Configuration class {config_class_name} not found. Please make sure that the file is in the right place.\"\n        )\n    return config_class\n\n", "CodeBleu_score": 42.98801060451297}, {"code": "def register_config(model_name: str) -> Coqpit\n\t\n    # construct the class name\n    config_class_name = f\"{model_name.upper()}Config\"\n    # check if the model is \"xtts\" to import its specific configuration\n    if model_name.lower() == \"xtts\":\n        from TTS.tts.utils.generic_utils import find_module\n\n        # search through predefined paths to find and assign the configuration class\n        for path in [\n            os.path.join(os.path.dirname(__file__), \"config\"),\n            os.path.join(os.path.dirname(os.path.dirname(__file__)), \"config\"),\n        ]:\n            try:\n                config_class = find_module(config_class_name, path)\n                break\n            except ModuleNotFoundError:\n                continue\n        else:\n            raise ModuleNotFoundError(\n                f\"The configuration class {config_class_name} is not found.\"\n            )\n    else:\n        # search through predefined paths to find and assign the configuration class\n        for path in [\n            os.path.join(os.path.dirname(__file__), \"config\"),\n            os.path.join(os.path.dirname(os.path.dirname(__file__)), \"config\"),\n        ]:\n            try:\n                config_class = find_module(config_class_name, path)\n                break\n            except ModuleNotFoundError:\n                continue\n        else:\n            raise ModuleNotFoundError(\n                f\"The configuration class {config_class_name} is not found.\"\n            )\n    # return the found configuration class\n    return config_class\n\n", "CodeBleu_score": 31.244302385121316}]}
{"instance_id": "0ea7862506a7", "generation_result": [{"code": "def load_config(config_path: str) -> Coqpit\n\t\n    # load config\n    config_dict = None\n    if config_path.endswith(\\\".yaml\\\") or config_path.endswith(\\\".yml\\\") or config_path.endswith(\\\".json\\\") or config_path.endswith(\\\".py\\\") or config_path.endswith(\\\".ipynb\\\") or config_path.endswith(\\\".md\\\") or config_path.endswith(\\\".txt\\\") or config_path.endswith(\\\".cfg\\\") or config_path.endswith(\\\".ini\\\") or config_path.endswith(\\\".cfg\\\") or config_path.endswith(\\\".cfg\\\") or config_path.endswith(\\\".cfg\\\") or config_path.endswith(\\\".cfg\\\") or config_path.endswith(\\\".cfg\\\") or config_path.endswith(\\\".cfg\\\") or config_path.endswith(\\\".cfg\\\") or config_path.endswith(\\\".cfg\\\") or config_path.endswith(\\\".cfg\\\") or config_path.endswith(\\\".cfg\\\") or config_path.endswith(\\\".cfg\\\") or config_path.endswith(\\\".cfg\\\") or config_path.endswith(\\\".cfg\\\") or config_path.endswith(\\\".cfg\\\") or config_path.endswith(\\\".cfg\\\") or config_path.endswith(\\\".cfg\\\") or config_path.endswith(\\\".cfg\\\") or config_path.endswith(\\\".cfg\\\") or config_path.endswith(\\\".cfg\\\") or config_path.endswith(\\\".cfg\\\") or config_path.endswith(\\\".cfg\\\") or config_path.endswith(\\\".cfg\\\") or config_path.endswith(\\\".cfg\\\") or config_path.endswith(\\\".cfg\\\") or config_path.endswith(\\\".cfg\\\") or config_path.endswith(\\\".cfg\\\") or config_path.endswith(\\\".cfg\\\") or config_path.endswith(\\\".cfg\\\") or config_path.endswith(\\\".cfg\\\") or config_path.endswith(\\\".cfg\\\") or config_path.endswith(\\\".cfg\\\") or config_path.endswith(\\\".cfg\\\") or config_path.endswith(\\\".cfg\\\") or config_path.endswith(\\\".cfg\\\") or config_path.endswith(\\\".cfg\\\") or config_path.endswith(\\\".cfg\\\") or config_path.endswith(\\\".cfg\\\") or config_path.endswith(\\\".cfg\\\") or config_path.endswith(\\\".cfg\\\") or config_path.endswith(\\\".cfg\\\") or config_path.endswith(\\\".cfg\\\") or", "CodeBleu_score": 8.162248357433786}, {"code": "def load_config(config_path: str) -> Coqpit\n\t\n    # determine file extension\n    file_extension = os.path.splitext(config_path)[1]\n    # load the configuration file\n    if file_extension in [\".yaml\", \".yml\"]:\n        with open(config_path, \"r\") as f:\n            config_dict = yaml.safe_load(f)\n    elif file_extension == \".json\":\n        config_dict = read_json_with_comments(config_path)\n    else:\n        raise ValueError(f\" [!] Unknown file extension {file_extension}.\")\n    # process the model name\n    model_name = _process_model_name(config_dict)\n    # register the config class\n    config_class = register_config(model_name)\n    # create an instance of the config class\n    config = config_class()\n    # populate the config with the data\n    config.load_from_dict(config_dict)\n    return config\n", "CodeBleu_score": 41.434798401722276}, {"code": "def load_config(config_path: str) -> Coqpit\n\t\n    config_dict = None\n    if config_path.endswith(\\\".yaml\\\") or config_path.endswith(\\\".yml\\\") or config_path.endswith(\\\".json\\\") or config_path.endswith(\\\".py\\\") or config_path.endswith(\\\".cfg\\\"):\n        if config_path.endswith(\\\".yaml\\\") or config_path.endswith(\\\".yml\\\"):\n            with fsspec.open(config_path, \\\"r\\\", encoding=\\\"utf-8\\\") as f:\n                config_dict = yaml.load(f, Loader=yaml.FullLoader)\n        elif config_path.endswith(\\\".json\\\") or config_path.endswith(\\\".py\\\") or config_path.endswith(\\\".cfg\\\"):\n            config_dict = read_json_with_comments(config_path)\n        else:\n            raise NotImplementedError\n    else:\n        raise NotImplementedError\n    model_name = _process_model_name(config_dict)\n    config_class = register_config(model_name)\n    config = config_class()\n    config.parse_from_config_dict(config_dict)\n    return config\n\n\n\n\n", "CodeBleu_score": 39.37644592663772}]}
{"instance_id": "e0db68b63143", "generation_result": [{"code": "def list_files(basePath, validExts=None, contains=None)\n\t\n    # loop over the directory structure\n    for (rootDir, dirNames, filenames) in os.walk(basePath):\n        # loop over the filenames in the current directory\n        for filename in filenames:\n            # if the contains string is not none and the filename does not contain\n            # the supplied string, then ignore the file\n            if contains is not None and filename.find(contains) == -1:\n                continue\n\n            # determine the file extension of the current file\n            ext = filename[filename.rfind(\".\"):].lower()\n\n            # check to see if the file is an image and should be processed\n            if validExts is None or ext.endswith(validExts):\n                # construct the path to the image and yield it\n                imagePath = os.path.join(rootDir, filename)\n                yield imagePath\n\n", "CodeBleu_score": 77.93987249579064}, {"code": "def list_files(basePath, validExts=None, contains=None)\n\t\n    # loop over the directory structure\n    for (rootDir, dirNames, filenames) in os.walk(basePath):\n        # loop over the filenames in the current directory\n        for filename in filenames:\n            # if the contains string is not none and the filename does not contain\n            # the supplied string, then ignore the file\n            if contains is not None and filename.find(contains) == -1:\n                continue\n\n            # determine the file extension of the current file\n            ext = filename[filename.rfind(\".\"):].lower()\n\n            # check to see if the file is an audio file\n            if validExts is None or ext.endswith(validExts):\n                # construct the file path\n                filePath = os.path.join(rootDir, filename)\n\n                # yield the file and the extension\n                yield filePath\n\n", "CodeBleu_score": 77.90912481144504}, {"code": "def list_files(basePath, validExts=None, contains=None)\n\t\n    fileList = list()\n    # loop through the directory structure\n    for (rootDir, dirNames, filenames) in os.walk(basePath):\n        # loop through the filenames in the current directory\n        for filename in filenames:\n            # if the contains string is not none and the filename does not contain\n            # the supplied string, then ignore the file\n            if contains is not None and filename.find(contains) == -1:\n                continue\n\n            # determine the file extension of the current file\n            ext = filename[filename.rfind(\".\"):].lower()\n\n            # check to see if the file is an audio file\n            if validExts is None or ext.endswith(validExts):\n                # construct the file path\n                filePath = os.path.join(rootDir, filename)\n                fileList.append(filePath)\n\n    # return the list of files\n    return fileList\n\n", "CodeBleu_score": 71.09227954302642}]}
{"instance_id": "b836fcaf0a27", "generation_result": [{"code": "def load_model(xtts_checkpoint, xtts_config, xtts_vocab)\n\t\n    clear_gpu_cache()\n    if not xtts_checkpoint or not xtts_config or not xtts_vocab:\n        return \"Please provide the path to the XTTS checkpoint, config, and vocab.\"\n    try:\n        config = XttsConfig.from_json_file(xtts_config)\n        model = Xtts(config)\n        ckpt = torch.load(xtts_checkpoint, map_location=\"cpu\")\n        model.load_state_dict(ckpt[\"model\"])\n        model.eval()\n        if torch.cuda.is_available():\n            model.cuda()\n        vocab = []\n        with open(xtts_vocab, \"r\") as f:\n            for line in f:\n                vocab.append(line.strip())\n        return \"Model loaded successfully.\"\n    except Exception as e:\n        return \"Error loading model: \" + str(e) + \" \" + str(traceback.format_exc())\n\n", "CodeBleu_score": 48.74171160364737}, {"code": "def load_model(xtts_checkpoint, xtts_config, xtts_vocab)\n\t\n\n    clear_gpu_cache()\n\n    if not xtts_checkpoint or not xtts_config or not xtts_vocab:\n        return \"Error: Please specify a checkpoint, config, and vocab path.\"\n\n    try:\n        # load model config\n        config = XttsConfig.from_json_file(xtts_config)\n        # init model\n        model = Xtts(config)\n        # load model state\n        checkpoint = torch.load(xtts_checkpoint, map_location=torch.device(\"cpu\"))\n        model.load_state_dict(checkpoint[\"model\"])\n        # load vocab\n        model.text_encoder.load_state_dict(torch.load(xtts_vocab))\n        # move to GPU\n        if torch.cuda.is_available():\n            model.cuda()\n        # set model to eval mode\n        model.eval()\n        # set global model variable\n        global XTTS_MODEL\n        XTTS_MODEL = model\n        return \"Success: Model loaded.\"\n    except Exception as e:\n        return \"Error: \" + str(e) + \"", "CodeBleu_score": 51.823064987570355}, {"code": "def load_model(xtts_checkpoint, xtts_config, xtts_vocab)\n\t\n    clear_gpu_cache()\n    if not xtts_checkpoint:\n        return \"Error: No XTTS checkpoint provided.\"\n    if not xtts_config:\n        return \"Error: No XTTS config provided.\"\n    if not xtts_vocab:\n        return \"Error: No XTTS vocab provided.\"\n    if not os.path.exists(xtts_checkpoint):\n        return \"Error: The XTTS checkpoint path does not exist.\"\n    if not os.path.exists(xtts_config):\n        return \"Error: The XTTS config path does not exist.\"\n    if not os.path.exists(xtts_vocab):\n        return \"Error: The XTTS vocab path does not exist.\"\n    # load model config\n    print(\"Loading XTTS model config...\")\n    config = XttsConfig.from_yaml(xtts_config)\n    # initialize model\n    print(\"Initializing XTTS model...\")\n    model = Xtts(config)\n    # load model checkpoint\n    print(\"Loading XTTS model checkpoint...\")\n    checkpoint = torch.load(xtts_checkpoint, map_location=\"cpu\")\n    model.load_state_dict(checkpoint[\"model\"])\n    # load vocab\n    print(\"Loading XTTS vocab...\")\n    vocab = {}\n    with open(xtts_vocab, \"r\") as f:\n        for line in f:\n            items = line.strip().split()\n            vocab[items[0]] = int(items[1])\n    # move to GPU\n    if torch.cuda.is_available():\n        model = model.cuda()\n    # set global variable\n    global XTTS_MODEL\n    XTTS_MODEL = {\"model\": model, \"config\": config, \"vocab\": vocab}\n    return \"Success: XTTS model loaded.\"\n\n", "CodeBleu_score": 46.401046578488966}]}
{"instance_id": "c5d63aa0df26", "generation_result": [{"code": "def compute_embedding(self, x, num_frames=250, num_eval=10, return_mean=True, l2_norm=True)\n\t\n    # pylint: disable=W0201\n    # pylint: disable=W0613\n    # pylint: disable=W0612\n    # pylint: disable=W0611\n    # pylint: disable=W0610\n    # pylint: disable=W0603\n    # pylint: disable=W0602\n    # pylint: disable=W0601\n    # pylint: disable=W0511\n    # pylint: disable=W0510\n    # pylint: disable=W0501\n    # pylint: disable=W0401\n    # pylint: disable=W0301\n    # pylint: disable=W0212\n    # pylint: disable=W0211\n    # pylint: disable=W0201\n    # pylint: disable=W0122\n    # pylint: disable=W0121\n    # pylint: disable=W0102\n    # pylint: disable=W0101\n    # pylint: disable=W0012\n    # pylint: disable=W0011\n    # pylint: disable=R0913\n    # pylint: disable=R0912\n    # pylint: disable=R0911\n    # pylint: disable=R0910\n    # pylint: disable=R0902\n    # pylint: disable=R0901\n    # pylint: disable=R0801\n    # pylint: disable=R0701\n    # pylint: disable=R0601\n    # pylint: disable=R0501\n    # pylint: disable=R0401\n    # pylint: disable=R0301\n    # pylint: disable=R0201\n    # pylint: disable=R0123\n    # pylint: disable=R0122\n    # pylint: disable=R0121\n    # pylint: disable=R0101\n    # pylint: disable=E1101\n    # pylint: disable=E1003\n    # pylint: disable=E0602\n    # pylint: disable=E0601\n    # pylint: disable=E0501\n    # pylint: disable=E0401\n    # pylint: disable=E0301\n    # pylint: disable=", "CodeBleu_score": 3.814953483095076}, {"code": "def compute_embedding(self, x, num_frames=250, num_eval=10, return_mean=True, l2_norm=True)\n\t\n    if self.audio_config[\\\"use_torch_stft\\\"]:\n        num_frames = num_frames // self.audio_config[\\\"hop_length\\\"]\n        num_frames = min(num_frames, x.shape[1] // self.audio_config[\\\"hop_length\\\"])\n\n    if x.shape[1] < num_frames * self.audio_config[\\\"hop_length\\\"]:\n        x = torch.nn.functional.pad(x, (0, num_frames * self.audio_config[\\\"hop_length\\\"] - x.shape[1]))\n\n    if num_eval > 1:\n        offset_range = np.linspace(0, x.shape[1] - num_frames * self.audio_config[\\\"hop_length\\\"], num_eval).astype(np.int32)\n    else:\n        offset_range = np.array([0])\n\n    embeddings = []\n    for offset in offset_range:\n        frame = x[:, offset : offset + num_frames * self.audio_config[\\\"hop_length\\\"]]\n        frame = self.torch_mel_spectrogram(frame)\n        frame = frame.transpose(1, 2).unsqueeze(1)\n        embedding = self.forward(frame, l2_norm=l2_norm)\n        embeddings.append(embedding)\n\n    embeddings = torch.cat(embeddings, dim=0)\n\n    if return_mean:\n        embeddings = embeddings.mean(dim=0)\n\n    return embeddings\n\n", "CodeBleu_score": 42.964259257408635}, {"code": "def compute_embedding(self, x, num_frames=250, num_eval=10, return_mean=True, l2_norm=True)\n\t\n    if self.use_torch_stft:\n        num_frames = int(num_frames / self.audio_config[\"hop_length\"])\n    num_frames = min(num_frames, len(x))\n    eval_offset = np.linspace(0, len(x) - num_frames, num_eval)\n    x_eval = [x[int(offset) : int(offset) + num_frames] for offset in eval_offset]\n    x_eval = np.stack(x_eval, axis=0)\n    x_eval = torch.from_numpy(x_eval).to(self.device)\n    if self.use_torch_stft:\n        x_eval = x_eval.unsqueeze(1)\n    embeddings = self.inference(x_eval, l2_norm)\n    if return_mean:\n        embeddings = torch.mean(embeddings, dim=0)\n        if l2_norm:\n            embeddings = embeddings / torch.norm(embeddings, p=2)\n    return embeddings\n\n", "CodeBleu_score": 43.82391188173663}]}
{"instance_id": "06f5b0335b26", "generation_result": [{"code": "def create_layer(self, block, planes, blocks, stride=1)\n\t\n    downsample = None\n    if stride != 1 or self.inplanes != planes * block.expansion:\n        downsample = nn.Sequential(\n            nn.Conv2d(self.inplanes, planes * block.expansion, kernel_size=1, stride=stride, bias=False),\n            nn.BatchNorm2d(planes * block.expansion),\n        )\n\n    layers = []\n    layers.append(block(self.inplanes, planes, stride, downsample))\n    self.inplanes = planes * block.expansion\n    for _ in range(1, blocks):\n        layers.append(block(self.inplanes, planes))\n\n    return nn.Sequential(*layers)\n\n", "CodeBleu_score": 87.88668147341942}, {"code": "def create_layer(self, block, planes, blocks, stride=1)\n\t\n    downsample = None\n    if stride != 1 or self.inplanes != planes * block.expansion:\n        downsample = nn.Sequential(\n            nn.Conv2d(\n                self.inplanes,\n                planes * block.expansion,\n                kernel_size=1,\n                stride=stride,\n                bias=False,\n            ),\n            nn.BatchNorm2d(planes * block.expansion),\n        )\n\n    layers = []\n    layers.append(block(self.inplanes, planes, stride, downsample))\n    self.inplanes = planes * block.expansion\n    for _ in range(1, blocks):\n        layers.append(block(self.inplanes, planes))\n\n    return nn.Sequential(*layers)\n\n", "CodeBleu_score": 82.33360230242197}, {"code": "def create_layer(self, block, planes, blocks, stride=1)\n\t\n    downsample = None\n    if stride != 1 or self.inplanes != planes * block.expansion:\n        downsample = nn.Sequential(\n            nn.Conv2d(\n                self.inplanes,\n                planes * block.expansion,\n                kernel_size=1,\n                stride=stride,\n                bias=False,\n            ),\n            nn.BatchNorm2d(planes * block.expansion),\n        )\n\n    layers = []\n    layers.append(block(self.inplanes, planes, stride, downsample))\n    self.inplanes = planes * block.expansion\n    for _ in range(1, blocks):\n        layers.append(block(self.inplanes, planes))\n\n    return nn.Sequential(*layers)\n\n", "CodeBleu_score": 82.33360230242197}]}
{"instance_id": "9ac60597b633", "generation_result": [{"code": "def plot_embeddings(embeddings, num_classes_in_batch)\n\t\n    num_classes = min(num_classes_in_batch, 10)\n    num_utterances = [0] * num_classes\n    for i in range(num_classes_in_batch):\n        num_utterances[embeddings[i, 0]] += 1\n    num_utterances = np.array(num_utterances)\n    indices = np.argsort(num_utterances)[::-1]\n    num_utterances = num_utterances[indices]\n    num_classes = min(num_classes, len(num_utterances))\n    indices = indices[:num_classes]\n    embeddings = embeddings[indices]\n    embeddings = umap.UMAP(n_neighbors=15, min_dist=0.1).fit_transform(embeddings)\n    fig, ax = plt.subplots(figsize=(10, 10))\n    ax.scatter(embeddings[:, 0], embeddings[:, 1], s=0.1, c=colormap[indices])\n    ax.set_title(\"UMAP projection\")\n    ax.axis(\"equal\")\n    plt.savefig(\"umap\")\n    plt.close()\n    return fig\n\n", "CodeBleu_score": 45.43361868073613}, {"code": "def plot_embeddings(embeddings, num_classes_in_batch)\n\t\n    # Number of utterances per class\n    num_utterances_per_class = [\n        len(np.where(embeddings.labels == i)[0]) for i in range(num_classes_in_batch)\n    ]\n    # Limit number of classes to 10\n    if len(num_utterances_per_class) > 10:\n        num_utterances_per_class = num_utterances_per_class[:10]\n        embeddings.labels = embeddings.labels[:10]\n    # Calculate UMAP\n    reducer = umap.UMAP()\n    embeddings = reducer.fit_transform(embeddings.embeddings)\n    # Create scatter plot\n    fig = plt.figure(figsize=(10, 10))\n    plt.scatter(\n        embeddings[:, 0],\n        embeddings[:, 1],\n        c=embeddings.labels,\n        cmap=colormap[: len(num_utterances_per_class)],\n    )\n    plt.title(\"UMAP projection\")\n    plt.gca().set_aspect(\"equal\", \"datalim\")\n    plt.colorbar(boundaries=np.arange(11) - 0.5).set_ticks(np.arange(10))\n    plt.savefig(\"umap\")\n    return fig\n\n", "CodeBleu_score": 39.62757414553269}, {"code": "def plot_embeddings(embeddings, num_classes_in_batch)\n\t\n    if num_classes_in_batch > 10:\n        num_classes_in_batch = 10\n    fig, ax = plt.subplots(figsize=(10, 10))\n    reducer = umap.UMAP()\n    embeddings_2d = reducer.fit_transform(embeddings)\n    for i in range(num_classes_in_batch):\n        ax.scatter(\n            embeddings_2d[i * 100 : (i + 1) * 100, 0],\n            embeddings_2d[i * 100 : (i + 1) * 100, 1],\n            c=colormap[i],\n            label=f\"Class {i}\",\n        )\n    ax.set_title(\"UMAP projection\")\n    ax.axis(\"equal\")\n    plt.legend(loc=\"upper right\")\n    plt.tight_layout()\n    plt.savefig(\"umap\")\n    return fig\n\n", "CodeBleu_score": 40.96691757185673}]}
{"instance_id": "19c2b4af256d", "generation_result": [{"code": "def calc_cosine_sim(self, dvecs, centroids)\n\t\n    cos_sims = []\n    for spkr in range(len(dvecs)):\n        new_centroids = self.calc_new_centroids(dvecs, centroids, spkr)\n        utterances = dvecs[spkr]\n        sims = []\n        for utt in range(len(utterances)):\n            sim = torch.clamp(torch.mm(utterances[utt].unsqueeze(0), new_centroids.transpose(0, 1)) / (torch.norm(utterances[utt]) * torch.norm(new_centroids, dim=1)), min=1e-8)\n            sims.append(sim)\n        cos_sims.append(torch.cat(sims))\n    return torch.stack(cos_sims)\n\n", "CodeBleu_score": 38.323694154255804}, {"code": "def calc_cosine_sim(self, dvecs, centroids)\n\t\n    cos_sim = []\n    for spkr in range(dvecs.shape[0]):\n        new_centroids = self.calc_new_centroids(dvecs, centroids, spkr)\n        for utt in range(dvecs.shape[1]):\n            if utt == 0:\n                sim = F.cosine_similarity(dvecs[spkr, utt], new_centroids)\n            else:\n                sim = torch.cat((sim, F.cosine_similarity(dvecs[spkr, utt], new_centroids)))\n        sim = torch.clamp(sim, min=1e-8)\n        cos_sim.append(sim)\n    return torch.stack(cos_sim)\n\n", "CodeBleu_score": 30.89654917869449}, {"code": "def calc_cosine_sim(self, dvecs, centroids)\n\t\n    cos_sim_matrix = []\n    for spkr in range(dvecs.size(0)):\n        new_centroids = self.calc_new_centroids(dvecs, centroids, spkr)\n        for utt in range(dvecs.size(1)):\n            # calculate cosine similarity between the utterance and each centroid\n            centroid_similarity = F.cosine_similarity(\n                dvecs[spkr, utt].unsqueeze(0), new_centroids, dim=1\n            )\n            # clamp the values to avoid very small negatives (floating point errors)\n            centroid_similarity.clamp_(min=0.00001)\n            cos_sim_matrix.append(centroid_similarity)\n    return torch.stack(cos_sim_matrix)\n\n", "CodeBleu_score": 31.0534948385473}]}
{"instance_id": "c525b411a1e7", "generation_result": [{"code": " def embed_loss_softmax(self, dvecs, cos_sim_matrix)\n\t\n    losses = []\n    for spkr_idx, speaker in enumerate(dvecs):\n        loss_row = []\n        for utt_idx, utterance in enumerate(speaker):\n            loss_row.append(\n                F.cross_entropy(\n                    torch.transpose(cos_sim_matrix[spkr_idx][utt_idx].unsqueeze(0), 0, 1),\n                    torch.tensor([spkr_idx]).to(utterance.device),\n                    reduction=\\\"none\\\",\n                )\n            )\n        losses.append(torch.stack(loss_row))\n    return torch.stack(losses)\n\n def embed_loss_contrast(self, dvecs, cos_sim_matrix):\n    \"\"\"The function calculates the embedding loss using contrastive loss.", "CodeBleu_score": 33.4472425273972}, {"code": " def embed_loss_softmax(self, dvecs, cos_sim_matrix)\n\t\n    embed_losses = []\n    for spkr_idx, speaker in enumerate(cos_sim_matrix):\n        embed_losses.append(\n            -F.log_softmax(self.w * cos_sim_matrix[spkr_idx] + self.b, dim=0).mean(0)\n        )\n    return torch.stack(embed_losses)\n\n def embed_loss_contrast(self, dvecs, cos_sim_matrix):\n    \"\"\"The function calculates the embedding loss using contrastive loss.", "CodeBleu_score": 26.6761255403241}, {"code": " def embed_loss_softmax(self, dvecs, cos_sim_matrix)\n\t\n    losses = []\n    for i, row in enumerate(cos_sim_matrix):\n        losses.append(F.nll_loss(F.log_softmax(row, dim=1), torch.tensor(i).to(dvecs.device)))\n    return torch.stack(losses)\n\n def embed_loss_contrast(self, dvecs, cos_sim_matrix):\n    \"\"\"The function calculates the embedding loss using contrastive loss.", "CodeBleu_score": 27.522755484549855}]}
{"instance_id": "25ac3945da2e", "generation_result": [{"code": "def embed_loss_contrast(self, dvecs, cos_sim_matrix)\n\t\n    N, M, _ = dvecs.shape\n    L = []\n    for j in range(N):\n        L_row = []\n        for i in range(M):\n            # exclude the current centroid\n            excl = torch.cat((dvecs[j, :i], dvecs[j, i + 1 :]))\n            # compute the sigmoid of the cosine similarity\n            excl = torch.mean(excl, 0)\n            excl = torch.mm(excl.unsqueeze(1).transpose(0, 1), dvecs[j, i].unsqueeze(0))\n            excl = torch.sigmoid(excl)\n            # compute the contrast loss\n            L_row.append(excl)\n        L_row = torch.stack(L_row)\n        L.append(L_row)\n    return torch.stack(L)\n", "CodeBleu_score": 52.985842847307055}, {"code": "def embed_loss_contrast(self, dvecs, cos_sim_matrix)\n\t\n    N, M, _ = dvecs.shape\n    L = []\n    for j in range(N):\n        L_row = []\n        for i in range(M):\n            cos_sim_row = cos_sim_matrix[j, i]\n            cos_sim_row[j] = -1e6\n            L_row.append(torch.mean(F.sigmoid(cos_sim_row)))\n        L_row = torch.stack(L_row)\n        L.append(L_row)\n    return torch.stack(L)\n\n", "CodeBleu_score": 44.81554971461323}, {"code": "def embed_loss_contrast(self, dvecs, cos_sim_matrix)\n\t\n    N, M, _ = dvecs.shape\n    L = []\n    for j in range(N):\n        L_row = []\n        for i in range(M):\n            excl = torch.cat((dvecs[j, :i], dvecs[j, i + 1 :]))\n            excl = torch.mean(excl, 0)\n            L_row.append(\n                torch.mean(\n                    torch.clamp(\n                        torch.sigmoid(\n                            torch.mm(\n                                dvecs[j, i].unsqueeze(1).transpose(0, 1),\n                                excl.unsqueeze(0),\n                            )\n                            - self.w * torch.mm(cos_sim_matrix[j, i], self.b)\n                        ),\n                        1e-6,\n                    )\n                )\n            )\n        L_row = torch.stack(L_row)\n        L.append(L_row)\n    return torch.stack(L)\n\n", "CodeBleu_score": 47.30240341068048}]}
{"instance_id": "878d14ffabd2", "generation_result": [{"code": "def forward(self, x, _label=None)\n\t\n    # pylint: disable=E1102\n    assert x.dim() == 3, \"Input must have 3 dimensions (num_speakers, num_utts_per_speaker, dvec_feats)\"\n    assert x.shape[1] > 1, \"Input must have at least two utterances per speaker\"\n    # pylint: disable=E1102\n    cos_sim_matrix = self.calc_cosine_sim(x, self.centroids)\n    # pylint: disable=E1102\n    cos_sim_matrix = self.w * cos_sim_matrix + self.b\n    # pylint: disable=E1102\n    embed_loss = self.embed_loss(x, cos_sim_matrix)\n    # pylint: disable=E1102\n    loss = torch.mean(torch.sum(embed_loss, dim=2))\n    return loss\n\n", "CodeBleu_score": 49.011366403546816}, {"code": "def forward(self, x, _label=None)\n\t\n    # pylint: disable=E1102\n    assert x.ndim == 3, \\\"Input must have dimensions (num_speakers, num_utts_per_speaker, dvec_feats)\\\"\n    # pylint: disable=E1102\n    assert x.shape[1] > 1, \\\"Input must have at least two utterances per speaker\\\"\n    N, M, _ = x.shape\n    centroids = torch.mean(x, 1)\n    cos_sim_matrix = self.calc_cosine_sim(x, centroids)\n    cos_sim_matrix = self.w * (cos_sim_matrix - self.b)\n    L = self.embed_loss(x, cos_sim_matrix)\n    return torch.mean(torch.sum(L, 2))\n\n\n\n", "CodeBleu_score": 52.490105443159216}, {"code": "def forward(self, x, _label=None)\n\t\n    # pylint: disable=E1102\n    N, M, D = x.shape\n    assert M >= 2, \\\"The GE2E loss requires at least two utterances per speaker.\\\"\n\n    # Calculate the centroids by averaging the embeddings\n    centroids = torch.mean(x, dim=1)\n\n    # Calculate the cosine similarity matrix\n    cos_sim_matrix = self.calc_cosine_sim(x, centroids)\n\n    # Scale and shift the cosine similarity matrix\n    cos_sim_matrix = self.w * (cos_sim_matrix - self.b)\n\n    # Calculate the embedding loss\n    L = self.embed_loss(x, cos_sim_matrix)\n\n    # Average the loss over speakers\n    return torch.mean(torch.sum(L, dim=1))\n", "CodeBleu_score": 54.13316527334303}]}
{"instance_id": "86a364c72caa", "generation_result": [{"code": "def get_voices(extra_voice_dirs: List[str] = [])\n\t\n    voice_files = {}\n    for voice_dir in [os.path.join(os.path.dirname(__file__), \"voices\")] + extra_voice_dirs:\n        for voice_subdir in os.listdir(voice_dir):\n            voice_subdir_path = os.path.join(voice_dir, voice_subdir)\n            if os.path.isdir(voice_subdir_path):\n                voice_files[voice_subdir] = []\n                for file in os.listdir(voice_subdir_path):\n                    file_path = os.path.join(voice_subdir_path, file)\n                    if os.path.isfile(file_path):\n                        if file.endswith(\".npz\"):\n                            voice_files[voice_subdir].append(file_path)\n                        elif file.endswith(\".wav\") or file.endswith(\".mp3\"):\n                            voice_files[voice_subdir].append(file_path)\n    return voice_files\n\n", "CodeBleu_score": 26.02760637495272}, {"code": "def get_voices(extra_voice_dirs: List[str] = [])\n\t\n    voices = {}\n    for voice_dir in [\n        os.path.join(os.path.dirname(__file__), \"voices\", \"en\"),\n        os.path.join(os.path.dirname(__file__), \"voices\", \"en\", \"fem\"),\n        os.path.join(os.path.dirname(__file__), \"voices\", \"en\", \"male\"),\n        *extra_voice_dirs,\n    ]:\n        if os.path.exists(voice_dir):\n            voices[os.path.basename(voice_dir)] = []\n            for ext in [\"npz\", \"wav\", \"mp3\"]:\n                voices[os.path.basename(voice_dir)] += glob(\n                    os.path.join(voice_dir, \"**\", f\"*.{ext}\"), recursive=True\n                )\n    return voices\n\n", "CodeBleu_score": 26.01510977381415}, {"code": "def get_voices(extra_voice_dirs: List[str] = [])\n\t\n    voices = {}\n    for voice_dir in [os.path.join(\"data\", \"tts\", \"bark\", \"voices\"), *extra_voice_dirs]:\n        for subdir, _, files in os.walk(voice_dir):\n            if subdir == voice_dir:\n                continue\n            if subdir.split(\"/\")[-1] == \"__pycache__\":\n                continue\n            if len(glob(os.path.join(subdir, \"*.npz\"))) > 0:\n                voices[subdir.split(\"/\")[-1]] = glob(os.path.join(subdir, \"*.npz\"))\n            else:\n                voices[subdir.split(\"/\")[-1]] = glob(os.path.join(subdir, \"*.wav\"))\n                voices[subdir.split(\"/\")[-1]] += glob(os.path.join(subdir, \"*.mp3\"))\n    return voices\n\n", "CodeBleu_score": 26.58950150836902}]}
{"instance_id": "02f20c8917e6", "generation_result": [{"code": "def load_voice(model, voice: str, extra_voice_dirs: List[str] = [])\n\t\n    if voice == \\\"random\\\":\n        return None, None, None\n    # load voices\n    voices = get_voices(extra_voice_dirs)\n    if voice not in voices:\n        raise ValueError(f\\\"Voice {voice} not found in {voices.keys()}.\\\")\n    voice_files = voices[voice]\n    if len(voice_files) > 1:\n        raise ValueError(f\\\"Multiple files found for voice {voice}.\\\")\n    voice_file = voice_files[0]\n    if voice_file.endswith(\\\".npz\\\"):\n        logger.info(f\\\"Loading voice {voice} from {voice_file}.\\\")\n        return load_npz(voice_file)\n    # generate .npz file\n    if not os.path.exists(voice_file):\n        raise ValueError(f\\\"Voice file {voice_file} does not exist.\\\")\n    logger.info(f\\\"Generating .npz file for voice {voice}.\\\")\n    return _generate_voice_npz(model, voice, voice_file)\n\n", "CodeBleu_score": 47.15128744220709}, {"code": "def load_voice(model, voice: str, extra_voice_dirs: List[str] = [])\n\t\n    if voice == \\\"random\\\":\n        return None\n    voices = get_voices(extra_voice_dirs)\n    if voice not in voices:\n        raise ValueError(f\\\"Voice {voice} not found.\\\")\n    voice_paths = voices[voice]\n    if len(voice_paths) > 1:\n        raise ValueError(f\\\"Multiple paths found for voice {voice}.\\\")\n    voice_path = voice_paths[0]\n    if voice_path.endswith(\\\".npz\\\"):\n        return load_npz(voice_path)\n    # generate npz file\n    if not os.path.exists(voice_path):\n        raise ValueError(f\\\"Voice path {voice_path} does not exist.\\\")\n    if voice_path.endswith(\\\".wav\\\") or voice_path.endswith(\\\".mp3\\\"):\n        x = convert_audio(voice_path)\n        if x.shape[1] == 1:\n            x = np.repeat(x, 2, axis=1)\n        np.savez(voice_path.replace(\\\".wav\\\", \\\".npz\\\"), x=x)\n        return load_npz(voice_path.replace(\\\".wav\\\", \\\".npz\\\"))\n    raise ValueError(f\\\"Unsupported voice format {voice_path}.\\\")\n\n\n\n\n", "CodeBleu_score": 43.71353937015211}, {"code": "def load_voice(model, voice: str, extra_voice_dirs: List[str] = [])\n\t\n    if voice == \"random\":\n        return None, None, None\n    voice_paths = get_voices(extra_voice_dirs)[voice]\n    if len(voice_paths) > 1:\n        raise ValueError(f\"Multiple voices found for {voice}.\")\n    voice_path = voice_paths[0]\n    if voice_path.endswith(\".npz\"):\n        return load_npz(voice_path)\n    if not os.path.exists(voice_path):\n        raise ValueError(f\"Voice {voice} not found.\")\n    if voice_path.endswith(\".wav\"):\n        voice_data = convert_audio(voice_path)\n    elif voice_path.endswith(\".mp3\"):\n        voice_data = convert_audio(voice_path, \"mp3\")\n    else:\n        raise ValueError(f\"Unsupported audio format: {voice_path}\")\n    # generate npz file\n    npz_file = voice_path.replace(\".wav\", \".npz\").replace(\".mp3\", \".npz\")\n    if not os.path.exists(npz_file):\n        with inference_mode(model):\n            model.generate_npz(voice_data, npz_file)\n    return load_npz(npz_file)\n\n", "CodeBleu_score": 45.22059833375373}]}
{"instance_id": "9458e981389f", "generation_result": [{"code": "def _relative_shift(self, pos_score: torch.Tensor) -> torch.Tensor\n\t\n    batch_size, num_heads, seq_length1, seq_length2 = pos_score.size()  # pylint: disable=unused-variable\n    zero_pad = torch.zeros((batch_size, num_heads, seq_length1, 1), device=pos_score.device, dtype=pos_score.dtype)\n    padded_pos_score = torch.cat([zero_pad, pos_score], dim=-1)\n    padded_pos_score = padded_pos_score.view(batch_size, num_heads, seq_length2 + 1, seq_length1)\n    pos_score = padded_pos_score[:, :, 1:].view_as(pos_score)\n\n    return pos_score\n\n", "CodeBleu_score": 58.61672843294839}, {"code": "def _relative_shift(self, pos_score: torch.Tensor) -> torch.Tensor\n\t\n    batch_size, num_heads, seq_length1, seq_length2 = pos_score.size()  # pylint: disable=unused-variable\n    zero_pad = torch.zeros((batch_size, num_heads, seq_length1, 1), device=pos_score.device, dtype=pos_score.dtype)\n    padded_pos_score = torch.cat([zero_pad, pos_score], dim=-1)\n\n    padded_pos_score = padded_pos_score.view(\n        batch_size, num_heads, seq_length2 + 1, seq_length1\n    )\n    pos_score = padded_pos_score[:, :, 1:].view_as(pos_score)\n    return pos_score\n\n", "CodeBleu_score": 52.77626987972512}, {"code": "def _relative_shift(self, pos_score: torch.Tensor) -> torch.Tensor\n\t\n    batch_size, num_heads, length1, length2 = pos_score.size()\n    zeros = pos_score.new_zeros((batch_size, num_heads, length1, 1))\n    padded_pos_score = torch.cat([zeros, pos_score], dim=-1)\n\n    padded_pos_score = padded_pos_score.view(batch_size, num_heads, length2 + 1, 2 * length1 - 1)\n    pos_score = padded_pos_score[:, :, 1:].view_as(pos_score)\n\n    return pos_score\n\n", "CodeBleu_score": 47.76076161021855}]}
{"instance_id": "c046d4f00e80", "generation_result": [{"code": "def run_padded_sequence(self, context, lens)\n\t\n    # pylint: disable=arguments-differ\n    # get the batch size and sequence length\n    batch_size, seq_len, _ = context.size()\n    # create a list to store the processed contexts\n    processed_context = []\n    # iterate over the batch\n    for i in range(batch_size):\n        # get the length of the current sequence\n        seq_len_i = lens[i]\n        # extract the context for the current batch element\n        context_i = context[i, :seq_len_i, :]\n        # iterate over the convolutional layers\n        for conv in self.convolutions:\n            # apply the convolutional layer\n            context_i = F.relu(conv(context_i))\n            # apply dropout\n            context_i = self.dropout(context_i)\n        # store the processed context\n        processed_context.append(context_i)\n    # convert the list to a tensor\n    processed_context = torch.stack(processed_context)\n    # pad the sequence to create a consistent batch size\n    processed_context = nn.utils.rnn.pad_sequence(processed_context, batch_first=True)\n    # return the padded sequence\n    return processed_context\n\n", "CodeBleu_score": 28.591686132203996}, {"code": "def run_padded_sequence(self, context, lens)\n\t\n    # pylint: disable=arguments-differ\n    batch_size = context.size(0)\n    max_len = context.size(2)\n    out_dim = self.out_dim\n    context = context.transpose(1, 2)\n    # context: (batch, time, dim)\n    # lens: (batch,)\n    # -> context_lens: (batch,)\n    context_lens = lens.int().cpu()\n    # -> context_lens_max: (batch,)\n    context_lens_max = context_lens.max()\n    # -> context_lens_pad: (batch, max_len)\n    context_lens_pad = torch.arange(0, context_lens_max.item()).expand(batch_size, -1).to(context_lens.device)\n    context_lens_pad = context_lens_pad < context_lens.unsqueeze(1)\n    # -> context_lens_pad: (batch, max_len, 1)\n    context_lens_pad = context_lens_pad.unsqueeze(2)\n    # -> context_lens_pad: (batch, max_len, dim)\n    context_lens_pad = context_lens_pad.expand(-1, -1, out_dim)\n    # -> context_lens_pad: (batch, max_len, dim)\n    context_lens_pad = context_lens_pad.float()\n    # -> context_lens_pad: (batch, max_len, dim)\n    context_lens_pad = context_lens_pad.to(context.dtype)\n    # -> context_lens_pad: (batch, max_len, dim)\n    context_lens_pad = context_lens_pad * context.max()\n    # -> context_lens_pad: (batch, max_len, dim)\n    context_lens_pad = context_lens_pad.to(context.dtype)\n    # -> context_lens_pad: (batch, dim, max_len)\n    context_lens_pad = context_lens_pad.transpose(1, 2)\n    # -> context_lens_pad: (batch, dim, max_len)\n    context_lens_pad = context_lens_pad.transpose(1, 2)\n    # -> context_lens_pad: (batch, dim, max_len)\n    context_lens_pad", "CodeBleu_score": 12.14486360148841}, {"code": "def run_padded_sequence(self, context, lens)\n\t\n    if context.size(0) != lens.size(0):\n        raise RuntimeError(\"Context and lens must have the same batch size\")\n    if not context.is_cuda:\n        raise RuntimeError(\"Padded sequence can only be run on GPU\")\n    if self.use_weight_norm:\n        raise RuntimeError(\"Weight norm is not supported for padded sequence\")\n    if self.lstm_type != \"\":\n        raise RuntimeError(\"LSTM is not supported for padded sequence\")\n    if self.use_linear:\n        raise RuntimeError(\"Linear layers are not supported for padded sequence\")\n\n    # extract the context up to the specified length\n    max_len = context.size(1)\n    mask = torch.arange(max_len, device=context.device)[None, :] < lens[:, None]\n    context = context.masked_select(mask.unsqueeze(2)).view(context.size(0), -1, context.size(2))\n\n    # apply the convolutional layers\n    for conv_layer in self.convolutions:\n        context = F.relu(conv_layer(context))\n        context = self.dropout(context)\n\n    # pad the context to create a consistent batch size\n    max_len = lens.max().item()\n    context = context.masked_select(mask.unsqueeze(2)).view(context.size(0), -1, context.size(2))\n    context = F.pad(context, (0, 0, 0, max_len - context.size(1)))\n\n    return context\n\n", "CodeBleu_score": 27.925005501080996}]}
{"instance_id": "db2644f3c681", "generation_result": [{"code": "def run_unsorted_inputs(self, fn, context, lens)\n\t\n    if lens is None:\n        return fn(context)\n    sorted_lens, sorted_indices = torch.sort(lens, descending=True)\n    sorted_context = context[sorted_indices]\n    packed_context = nn.utils.rnn.pack_padded_sequence(sorted_context, sorted_lens, batch_first=True)\n    packed_context = fn(packed_context)\n    context, _ = nn.utils.rnn.pad_packed_sequence(packed_context, batch_first=True)\n    context = context[sorted_indices]\n    return context\n\n", "CodeBleu_score": 35.74186624290089}, {"code": "def run_unsorted_inputs(self, fn, context, lens)\n\t\n    sorted_lens, sorted_indices = torch.sort(lens, descending=True)\n    sorted_context = context[sorted_indices]\n    sorted_context = self.run_padded_sequence(sorted_context, sorted_lens)\n    sorted_context = nn.utils.rnn.pad_packed_sequence(sorted_context, batch_first=True)[0]\n    unsorted_context = sorted_context[torch.argsort(sorted_indices)]\n    return fn(unsorted_context)\n\n", "CodeBleu_score": 17.29316715285867}, {"code": "def run_unsorted_inputs(self, fn, context, lens)\n\t\n    sorted_lens, sorted_indices = torch.sort(lens, descending=True)\n    sorted_context = context[sorted_indices]\n    sorted_context = nn.utils.rnn.pack_padded_sequence(sorted_context, sorted_lens, batch_first=True)\n    sorted_context = fn(sorted_context)\n    sorted_context, _ = nn.utils.rnn.pad_packed_sequence(sorted_context, batch_first=True)\n    sorted_context = sorted_context[sorted_indices]\n    return sorted_context\n\n", "CodeBleu_score": 17.443173144613}]}
{"instance_id": "ced082bffc63", "generation_result": [{"code": "def remove_weight_norm(self)\n\t\n    try:\n        nn.utils.remove_weight_norm(self.kernel_predictor)\n    except ValueError:\n        pass\n    try:\n        nn.utils.remove_weight_norm(self.convt_pre[1])\n    except ValueError:\n        pass\n    for i, conv in enumerate(self.conv_blocks):\n        try:\n            nn.utils.remove_weight_norm(conv[1])\n        except ValueError:\n            pass\n        try:\n            nn.utils.remove_weight_norm(conv[3])\n        except ValueError:\n            pass\n\n", "CodeBleu_score": 39.76568599931253}, {"code": "def remove_weight_norm(self)\n\t\n    self.kernel_predictor.remove_weight_norm()\n    self.convt_pre[1].remove_weight_norm()\n    for block in self.conv_blocks:\n        block[1].remove_weight_norm()\n\n", "CodeBleu_score": 34.287881114546934}, {"code": "def remove_weight_norm(self)\n\t\n    self.kernel_predictor.remove_weight_norm()\n    nn.utils.remove_weight_norm(self.convt_pre[1])\n    for block in self.conv_blocks:\n        nn.utils.remove_weight_norm(block[1])\n\n", "CodeBleu_score": 42.55392119287791}]}
{"instance_id": "b5a39fd97061", "generation_result": [{"code": "def map_mx_to_torch_model(mx_name)\n\t\n    mx_name = mx_name.lower()\n    mx_name = mx_name.replace('se_', 'senet_')\n    mx_name = mx_name.replace('inceptionv3', 'inception_v3')\n    return 'gluon_' + mx_name\n\n", "CodeBleu_score": 25.44052296016685}, {"code": "def map_mx_to_torch_model(mx_name)\n\t\n    # convert to lowercase\n    name = mx_name.lower()\n\n    # replace 'se_' and 'senet_' prefixes\n    name = name.replace('se_', 'senet_')\n    name = name.replace('senet_', 'senet_')\n\n    # replace 'inceptionv3' prefix\n    name = name.replace('inceptionv3', 'inception_v3')\n\n    # prefix with 'gluon_'\n    name = 'gluon_' + name\n\n    return name\n\n", "CodeBleu_score": 34.26894545676282}, {"code": "def map_mx_to_torch_model(mx_name)\n\t\n    if mx_name.startswith('se_'):\n        return 'gluon_senet_' + mx_name[3:]\n    elif mx_name.startswith('senet_'):\n        return 'gluon_' + mx_name\n    elif mx_name.startswith('inceptionv3'):\n        return 'gluon_' + mx_name\n    else:\n        return 'gluon_' + mx_name.lower()\n\n", "CodeBleu_score": 26.823141273868668}]}
{"instance_id": "7eee5efdbf67", "generation_result": [{"code": "def _num_samples_per_worker(self)\n\t\n    if self.is_training:\n        num_samples_per_worker = math.ceil(self.num_samples / self.global_num_workers)\n        if self.batch_size > 1:\n            num_samples_per_worker = math.ceil(num_samples_per_worker / self.batch_size) * self.batch_size\n    else:\n        num_samples_per_worker = math.ceil(self.num_samples / self.global_num_workers)\n    return num_samples_per_worker\n\n", "CodeBleu_score": 40.577578896681096}, {"code": "def _num_samples_per_worker(self)\n\t\n    num_samples_per_worker = self.num_samples // self.global_num_workers\n    num_samples_per_worker = num_samples_per_worker // self.repeats\n    if self.batch_size:\n        num_samples_per_worker = (num_samples_per_worker + self.batch_size - 1) // self.batch_size\n    return num_samples_per_worker\n\n", "CodeBleu_score": 24.35984582682085}, {"code": "def _num_samples_per_worker(self)\n\t\n    if self.repeats > 0 and self.num_workers > 1:\n        # For training with repeats and multiple workers, we want to make sure that each worker processes the same number of samples\n        # so that we don't have to pad the last batch.\n        # We compute the number of samples per worker as follows:\n        # 1. Compute the initial number of samples per worker\n        # 2. Round up to the nearest multiple of batch_size\n        # 3. If the number of samples per worker is less than the number of workers, we need to add an extra worker.\n        #    This is to ensure that each worker has at least one batch.\n        num_samples_per_worker = math.ceil(self.num_samples / self.num_workers)\n        num_samples_per_worker = num_samples_per_worker - num_samples_per_worker % self.batch_size\n        if num_samples_per_worker < self.num_workers:\n            num_samples_per_worker += self.batch_size\n        return num_samples_per_worker\n    else:\n        # For inference, we don't need to make sure that each worker processes the same number of samples.\n        # So we just compute the number of samples per worker as follows:\n        # 1. Compute the initial number of samples per worker\n        # 2. Round up to the nearest multiple of batch_size\n        # 3. If the number of samples per worker is less than the number of workers, we need to add an extra worker.\n        #    This is to ensure that each worker has at least one batch.\n        num_samples_per_worker = math.ceil(self.num_samples / self.num_workers)\n        num_samples_per_worker = num_samples_per_worker - num_samples_per_worker % self.batch_size\n        if num_samples_per_worker < self.num_workers:\n            num_samples_per_worker += self.batch_size\n        return num_samples_per_worker\n\n", "CodeBleu_score": 26.975352052412134}]}
{"instance_id": "a698d65ae190", "generation_result": [{"code": "def find_images_and_targets(\n        folder: str,\n        types: Optional[Union[List, Tuple, Set]] = None,\n        class_to_idx: Optional[Dict] = None,\n        leaf_name_only: bool = True,\n        sort: bool = True\n)\n\t\n    if types is None:\n        types = get_img_extensions()\n    else:\n        if not isinstance(types, (list, tuple, set)):\n            types = [types]\n    types = set(types)\n    if class_to_idx is None:\n        classes = [d.name for d in os.scandir(folder) if d.is_dir()]\n        classes.sort(key=natural_key)\n        class_to_idx = {cls_name: i for i, cls_name in enumerate(classes)}\n    else:\n        classes = [None] * len(class_to_idx)\n        for cls_name, cls_idx in class_to_idx.items():\n            classes[cls_idx] = cls_name\n    images = []\n    dir = os.fsencode(folder)\n    for file in sorted(os.listdir(dir), key=natural_key):\n        filename = os.fsdecode(file)\n        if filename.endswith(types):\n            path = os.path.join(folder, filename)\n            item = path, class_to_idx[os.path.basename(os.path.dirname(path))]\n            images.append(item)\n    if leaf_name_only:\n        images = [(path, class_to_idx[os.path.basename(os.path.dirname(path))]) for path, _ in images]\n    if sort:\n        images.sort(key=lambda x: natural_key(x[0]))\n    return images, classes, class_to_idx\n\n", "CodeBleu_score": 41.83028895475081}, {"code": "def find_images_and_targets(\n        folder: str,\n        types: Optional[Union[List, Tuple, Set]] = None,\n        class_to_idx: Optional[Dict] = None,\n        leaf_name_only: bool = True,\n        sort: bool = True\n)\n\t\n    if types is None:\n        types = get_img_extensions()\n    else:\n        if not isinstance(types, (list, tuple, set)):\n            types = [types]\n    if class_to_idx is None:\n        class_to_idx = {}\n    images = []\n    dir = os.path.expanduser(folder)\n    for root, subdirs, files in sorted(os.walk(dir, followlinks=True)):\n        for f in sorted(files):\n            if f.lower().endswith(types):\n                path = os.path.join(root, f)\n                item = path\n                if leaf_name_only:\n                    item = os.path.splitext(f)[0]\n                class_idx = class_to_idx.get(item, None)\n                if class_idx is None:\n                    class_idx = len(class_to_idx)\n                    class_to_idx[item] = class_idx\n                images.append((path, class_idx))\n    if sort:\n        images = sorted(images, key=lambda x: natural_key(x[0]))\n    return images, class_to_idx\n\n", "CodeBleu_score": 39.95607403840785}, {"code": "def find_images_and_targets(\n        folder: str,\n        types: Optional[Union[List, Tuple, Set]] = None,\n        class_to_idx: Optional[Dict] = None,\n        leaf_name_only: bool = True,\n        sort: bool = True\n)\n\t\n    if types is None:\n        types = get_img_extensions()\n    else:\n        if not isinstance(types, (set, list, tuple)):\n            raise ValueError('types must be a list, tuple, or set')\n    if class_to_idx is None:\n        class_to_idx = {}\n    images = []\n    dir = os.path.expanduser(folder)\n    for root, _, fnames in sorted(os.walk(dir, followlinks=True)):\n        for fname in sorted(fnames):\n            if not fname.lower().endswith(types):\n                continue\n            path = os.path.join(root, fname)\n            label = path.split(os.path.sep)[-1] if leaf_name_only else path\n            if label in class_to_idx:\n                label_idx = class_to_idx[label]\n            else:\n                label_idx = len(class_to_idx)\n                class_to_idx[label] = label_idx\n            images.append((path, label_idx))\n    if sort:\n        images = sorted(images, key=lambda x: natural_key(x[0]))\n    return images, class_to_idx\n\n", "CodeBleu_score": 42.04992652580272}]}
{"instance_id": "e7fe43bdecdc", "generation_result": [{"code": "def pytorch_worker_seed()\n\t\n    worker_info = get_worker_info()\n    if worker_info is not None:\n        seed = worker_info.seed % sys.maxsize\n    else:\n        seed = random.randint(0, sys.maxsize)\n    return seed\n\n", "CodeBleu_score": 48.908284821828}, {"code": "def pytorch_worker_seed()\n\t\n    worker_info = get_worker_info()\n    if worker_info is not None:\n        seed = worker_info.seed % (2**32 - 1)\n    else:\n        seed = (int(os.environ.get('WDS_RANK', 0)) + 1) * 10000\n    return seed\n\n", "CodeBleu_score": 48.971390392562164}, {"code": "def pytorch_worker_seed()\n\t\n    worker_info = get_worker_info()\n    if worker_info is not None:\n        worker_seed = worker_info.seed\n        if worker_seed is not None:\n            return worker_seed\n    # fallback to seed based on rank\n    seed = 0\n    if dist.is_available() and dist.is_initialized():\n        seed = int(torch.initial_seed() % 2**32)\n    return seed\n\n", "CodeBleu_score": 51.451846087299536}]}
{"instance_id": "0ffc7e7ded25", "generation_result": [{"code": "def solarize_add(img, add, thresh=128, **__)\n\t\n    if img.mode not in (\"L\", \"RGB\"):\n        return img\n    lut = []\n    for i in range(256):\n        if i < thresh:\n            lut.append(min(255, i + add))\n        else:\n            lut.append(i)\n    if img.mode == \"RGB\" and len(lut) == 256:\n        lut = lut + lut + lut\n    return img.point(lut)\n\n", "CodeBleu_score": 82.07783343103254}, {"code": "def solarize_add(img, add, thresh=128, **__)\n\t\n    if img.mode not in (\"L\", \"RGB\"):\n        return img\n    lut = []\n    for i in range(256):\n        if i < thresh:\n            lut.append(min(255, i + add))\n        else:\n            lut.append(i)\n    if img.mode == \"RGB\" and len(lut) == 256:\n        lut = lut + lut + lut\n    return img.point(lut)\n\n", "CodeBleu_score": 82.07783343103254}, {"code": "def solarize_add(img, add, thresh=128, **__)\n\t\n    if img.mode not in (\"L\", \"RGB\"):\n        return img\n\n    lut = []\n    for i in range(256):\n        if i < thresh:\n            lut.append(min(255, i + add))\n        else:\n            lut.append(i)\n\n    if img.mode == \"RGB\" or img.mode == \"RGBA\":\n        if img.mode == \"RGBA\":\n            lut = lut + [255] * 256 * 3\n        return img.point(lut)\n    return img.point(lut)\n\n", "CodeBleu_score": 67.76776901250365}]}
{"instance_id": "5a0d29577a21", "generation_result": [{"code": "def _get_weighted_transforms(transforms: Dict)\n\t\n    names = []\n    probs = []\n    for k, v in transforms.items():\n        names.append(k)\n        probs.append(v)\n    probs = np.array(probs)\n    probs = probs / np.sum(probs)\n    return names, probs\n\n", "CodeBleu_score": 56.524921474358536}, {"code": "def _get_weighted_transforms(transforms: Dict)\n\t\n    transforms = list(transforms.items())\n    transform_names = [t[0] for t in transforms]\n    transform_probs = [t[1] for t in transforms]\n    transform_probs = [p / sum(transform_probs) for p in transform_probs]\n    return transform_names, transform_probs\n\n", "CodeBleu_score": 20.623346913752837}, {"code": "def _get_weighted_transforms(transforms: Dict)\n\t\n    total = sum(transforms.values())\n    probs = [(k, v / total) for k, v in transforms.items()]\n    return probs\n\n", "CodeBleu_score": 23.21205865686411}]}
{"instance_id": "84bc5658c135", "generation_result": [{"code": "def _apply_basic(self, img, mixing_weights, m)\n\t\n    img_orig = img.copy()\n    img_mixed = Image.new(img.mode, img.size, 0)\n    for w in mixing_weights:\n        depth = self.depth if self.depth > 0 else np.random.randint(1, 4)\n        ops = np.random.choice(self.ops, depth, replace=True)\n        img_aug = img_orig  # no ops are in-place, deep copy not necessary\n        for op in ops:\n            img_aug = op(img_aug)\n        img_mixed = Image.blend(img_mixed, img_aug, w)\n    img_mixed = img_mixed.convert(\"RGB\")\n    img_mixed = img_mixed.filter(ImageFilter.GaussianBlur(m))\n    img_mixed = ImageOps.autocontrast(img_mixed)\n    img_mixed = ImageOps.equalize(img_mixed)\n    return img_mixed\n\n", "CodeBleu_score": 52.67839352341507}, {"code": "def _apply_basic(self, img, mixing_weights, m)\n\t\n    img_orig = img.copy()\n    img_mixed = Image.new(img.mode, img.size)\n    for w in mixing_weights:\n        depth = self.depth if self.depth > 0 else np.random.randint(1, 4)\n        ops = np.random.choice(self.ops, depth, replace=True)\n        img_aug = img_orig  # no ops are in-place, deep copy not necessary\n        for op in ops:\n            img_aug = op(img_aug)\n        img_mixed = Image.blend(img_mixed, img_aug, w)\n    img_mixed = img_mixed.convert('RGB')\n    return Image.blend(img, img_mixed, m)\n\n", "CodeBleu_score": 52.17164625116057}, {"code": "def _apply_basic(self, img, mixing_weights, m)\n\t\n    img_orig = img.copy()\n    img_mixed = np.zeros_like(img_orig)\n    for w in mixing_weights:\n        depth = self.depth if self.depth > 0 else np.random.randint(1, 4)\n        ops = np.random.choice(self.ops, depth, replace=True)\n        img_aug = img_orig  # no ops are in-place, deep copy not necessary\n        for op in ops:\n            img_aug = op(img_aug)\n        img_mixed += w * img_aug\n    img_mixed = np.clip(img_mixed, 0, 255)\n    img_mixed = Image.fromarray(img_mixed.astype(np.uint8))\n    img = Image.blend(img, img_mixed, m)\n    return img\n\n", "CodeBleu_score": 57.896548151492}]}
{"instance_id": "84b5b752ff04", "generation_result": [{"code": "def adapt_to_chs(x, n)\n\t\n    if isinstance(x, (tuple, list)):\n        if len(x) == n:\n            assert len(x) == n\n            return x\n        elif len(x) == 1:\n            return tuple(repeat(x[0], n))\n        else:\n            _logger.warning(\n                f'Adapting input of length {len(x)} to match the required number of channels {n}.')\n            return tuple(repeat(sum(x) / len(x), n))\n    else:\n        return tuple(repeat(x, n))\n\n", "CodeBleu_score": 27.549522480265022}, {"code": "def adapt_to_chs(x, n)\n\t\n    if isinstance(x, (tuple, list)):\n        if len(x) == n:\n            return x\n        elif len(x) == 1:\n            return tuple(x * n)\n        else:\n            _logger.warning('Input list has length %d, but should have length %d. Will use mean of list.', len(x), n)\n            return tuple([np.mean(x, axis=0, keepdims=True)] * n)\n    else:\n        return tuple(repeat(x, n))\n\n", "CodeBleu_score": 27.149563286399363}, {"code": "def adapt_to_chs(x, n)\n\t\n    if not isinstance(x, (tuple, list)):\n        x = (x,) * n\n    elif len(x) != n:\n        _logger.warning(f'Input channels {len(x)} != {n}, adapting channels')\n        x = (torch.mean(torch.stack(x, dim=0), dim=0),) * n\n    else:\n        assert len(x) == n\n    return x\n\n", "CodeBleu_score": 47.98428328094481}]}
{"instance_id": "54754b6ba394", "generation_result": [{"code": "def _worker_init(worker_id, worker_seeding='all')\n\t\n    worker_info = torch.utils.data.get_worker_info()\n    assert worker_info is not None, 'Requires PyTorch >= 1.7.0'\n    wid = worker_info.id\n    assert wid == worker_id, f'Worker ID {wid} does not match worker seed ID {worker_id}'\n    seed = worker_info.seed\n    if callable(worker_seeding):\n        worker_seeding(seed)\n    elif worker_seeding == 'all':\n        np.random.seed(seed)\n        random.seed(seed)\n        torch.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n    else:\n        np.random.seed(seed + wid)\n\n", "CodeBleu_score": 45.06997856248434}, {"code": "def _worker_init(worker_id, worker_seeding='all')\n\t\n    worker_info = torch.utils.data.get_worker_info()\n    if worker_info is None:\n        raise RuntimeError('PrefetchLoader worker_info is None. Check if it is called from a dataloader with num_workers > 0.')\n    if worker_info.id != worker_id:\n        raise RuntimeError(f'PrefetchLoader worker_info.id {worker_info.id} does not match worker_id {worker_id}. Check if it is called from a dataloader with num_workers > 0.')\n    if callable(worker_seeding):\n        worker_seeding(worker_info.seed)\n    elif worker_seeding == 'all':\n        np.random.seed(worker_info.seed)\n        random.seed(worker_info.seed)\n        torch.manual_seed(worker_info.seed)\n        torch.cuda.manual_seed_all(worker_info.seed)\n    else:\n        np.random.seed(worker_seeding)\n\n", "CodeBleu_score": 33.54484848147815}, {"code": "def _worker_init(worker_id, worker_seeding='all')\n\t\n    worker_info = torch.utils.data.get_worker_info()\n    assert worker_info is not None, 'PrefetchLoader worker_info is None, check pytorch version'\n    wid = worker_info.id\n    assert wid == worker_id, f'PrefetchLoader worker_id {wid} != expected worker_id {worker_id}'\n    if callable(worker_seeding):\n        worker_seeding(wid)\n    elif worker_seeding == 'all':\n        np.random.seed(wid)\n        random.seed(wid)\n        torch.manual_seed(wid)\n        torch.cuda.manual_seed_all(wid)\n    else:\n        raise ValueError(f'Unknown worker_seeding {worker_seeding}')\n\n", "CodeBleu_score": 39.422165162060416}]}
{"instance_id": "81dfdce940db", "generation_result": [{"code": "def _decode_and_center_crop(image_bytes, image_size, resize_method)\n\t\n    shape = tf.image.extract_jpeg_shape(image_bytes)\n    image_height = shape[0]\n    image_width = shape[1]\n\n    padded_center_crop_size = tf.cast(\n        ((image_size / (image_size + CROP_PADDING)) *\\\n         tf.cast(tf.minimum(image_height, image_width), tf.float32)),\n        tf.int32)\n\n    offset_height = ((image_height - padded_center_crop_size) + 1) // 2\n    offset_width = ((image_width - padded_center_crop_size) + 1) // 2\n    crop_window = tf.stack([offset_height, offset_width, padded_center_crop_size, padded_center_crop_size])\n    image = tf.image.decode_and_crop_jpeg(image_bytes, crop_window, channels=3)\n    image = tf.image.resize([image], [image_size, image_size], resize_method)[0]\n\n    return image\n\n", "CodeBleu_score": 62.88302564233583}, {"code": "def _decode_and_center_crop(image_bytes, image_size, resize_method)\n\t\n    shape = tf.image.extract_jpeg_shape(image_bytes)\n    image_height = shape[0]\n    image_width = shape[1]\n    padded_center_crop_size = tf.cast(\n        ((image_size / (image_size + CROP_PADDING)) *\n         tf.cast(tf.minimum(image_height, image_width), tf.float32)),\n        tf.int32)\n    offset_height = ((image_height - padded_center_crop_size) + 1) // 2\n    offset_width = ((image_width - padded_center_crop_size) + 1) // 2\n    crop_window = tf.stack([offset_height, offset_width,\n                            padded_center_crop_size, padded_center_crop_size])\n    image = tf.image.decode_and_crop_jpeg(\n        image_bytes, crop_window, channels=3)\n    image = tf.image.resize([image], [image_size, image_size], resize_method)[0]\n    return image\n\n\n\n\n", "CodeBleu_score": 62.86255551851067}, {"code": "def _decode_and_center_crop(image_bytes, image_size, resize_method)\n\t\n    shape = tf.image.extract_jpeg_shape(image_bytes)\n    image_height = shape[0]\n    image_width = shape[1]\n\n    padded_center_crop_size = tf.cast(\n        ((image_size / (image_size + CROP_PADDING)) *\\\n         tf.cast(tf.minimum(image_height, image_width), tf.float32)), tf.int32)\n\n    offset_height = ((image_height - padded_center_crop_size) + 1) // 2\n    offset_width = ((image_width - padded_center_crop_size) + 1) // 2\n    crop_window = tf.stack([offset_height, offset_width, padded_center_crop_size, padded_center_crop_size])\n    image = tf.image.decode_and_crop_jpeg(image_bytes, crop_window, channels=3)\n    image = tf.image.resize([image], [image_size, image_size], resize_method)[0]\n\n    return image\n", "CodeBleu_score": 62.55299762025057}]}
{"instance_id": "cf16a6d37111", "generation_result": [{"code": "def preprocess_for_train(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    image = _decode_and_random_crop(image_bytes, image_size, interpolation)\n    image = _flip(image)\n    image = tf.reshape(image, [image_size, image_size, 3])\n    image = tf.image.convert_image_dtype(image, dtype=tf.bfloat16 if use_bfloat16 else tf.float32)\n    return image\n\n", "CodeBleu_score": 43.24091205508273}, {"code": "def preprocess_for_train(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    image = _decode_and_random_crop(image_bytes, image_size, interpolation)\n    image = _flip(image)\n    image = tf.reshape(image, [image_size, image_size, 3])\n    image = tf.image.convert_image_dtype(image, dtype=tf.bfloat16 if use_bfloat16 else tf.float32)\n    return image\n\n", "CodeBleu_score": 43.24091205508273}, {"code": "def preprocess_for_train(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    image = _decode_and_random_crop(image_bytes, image_size, interpolation)\n    image = _flip(image)\n    image = tf.reshape(image, [image_size, image_size, 3])\n    image = tf.image.convert_image_dtype(image, dtype=tf.bfloat16 if use_bfloat16 else tf.float32)\n    return image\n\n", "CodeBleu_score": 43.24091205508273}]}
{"instance_id": "78b2a3bd2142", "generation_result": [{"code": "def preprocess_for_eval(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    resize_method = tf.image.ResizeMethod.BICUBIC if interpolation == 'bicubic' else tf.image.ResizeMethod.BILINEAR\n    image = _decode_and_center_crop(image_bytes, image_size, resize_method)\n    image = tf.reshape(image, [image_size, image_size, 3])\n    image = tf.image.convert_image_dtype(image, dtype=tf.bfloat16 if use_bfloat16 else tf.float32)\n    return image\n", "CodeBleu_score": 70.0699363620749}, {"code": "def preprocess_for_eval(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    resize_method = tf.image.ResizeMethod.BICUBIC if interpolation == 'bicubic' else tf.image.ResizeMethod.BILINEAR\n    image = _decode_and_center_crop(image_bytes, image_size, resize_method)\n    image = tf.reshape(image, [image_size, image_size, 3])\n    image = tf.image.convert_image_dtype(image, dtype=tf.bfloat16 if use_bfloat16 else tf.float32)\n    return image\n\n", "CodeBleu_score": 70.0699363620749}, {"code": "def preprocess_for_eval(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n\t\n    resize_method = tf.image.ResizeMethod.BICUBIC if interpolation == 'bicubic' else tf.image.ResizeMethod.BILINEAR\n    image = _decode_and_center_crop(image_bytes, image_size, resize_method)\n    image = tf.reshape(image, [image_size, image_size, 3])\n    image = tf.image.convert_image_dtype(image, dtype=tf.bfloat16 if use_bfloat16 else tf.float32)\n    return image\n", "CodeBleu_score": 70.0699363620749}]}
{"instance_id": "8bd4364a9a0e", "generation_result": [{"code": "def evaluate(self, model, eval_dataloader=None, output_file=\"merged\")\n\t\n    if eval_dataloader is None:\n        eval_dataloader = self.eval_dataloader\n    if eval_dataloader is None:\n        raise ValueError(\"eval_dataloader is None\")\n\n    self.predictor.predict(model, eval_dataloader, output_file)\n    outputs = np.load(os.path.join(self.predictor.pred_dir, output_file + \".npy\"))\n    results = self.metric.compute_metrics(outputs)\n    return {\"results\": results, \"metric\": self.metric}\n\n", "CodeBleu_score": 58.60520745243274}, {"code": "def evaluate(self, model, eval_dataloader=None, output_file=\"merged\")\n\t\n    if eval_dataloader is None:\n        eval_dataloader = self.eval_dataloader\n    self.predictor.predict(model, eval_dataloader, output_file)\n    outputs = np.load(os.path.join(self.predictor.pred_dir, output_file + \".npy\"))\n    results = self.metric.compute_metrics(outputs)\n    return results\n\n", "CodeBleu_score": 64.25130738378562}, {"code": "def evaluate(self, model, eval_dataloader=None, output_file=\"merged\")\n\t\n    if eval_dataloader is None:\n        eval_dataloader = self.eval_dataloader\n    if eval_dataloader is None:\n        raise ValueError(\"eval_dataloader is None\")\n    if not os.path.exists(self.pred_dir):\n        os.makedirs(self.pred_dir)\n    self.predictor.predict(model, eval_dataloader, output_file)\n    outputs = np.load(os.path.join(self.pred_dir, output_file + \".npy\"))\n    results = self.metric.compute_metrics(outputs)\n    self.metric.print_computed_metrics(results)\n    return {\"results\": results, \"metric\": self.metric}\n\n", "CodeBleu_score": 56.375472101227984}]}
{"instance_id": "22d682c958c0", "generation_result": [{"code": " def print_computed_metrics(self, metrics)\n\t\n    rank1 = metrics[\"rank1\"]\n    rank5 = metrics[\"rank5\"]\n    miou = metrics[\"miou\"]\n    print(\n        \"Rank@1: {:.4f} - Rank@5: {:.4f} - mIOU: {:.4f}\".format(rank1, rank5, miou)\n    )\n", "CodeBleu_score": 42.76273135295803}, {"code": " def print_computed_metrics(self, metrics)\n\t\n    rank1 = metrics[\"rank1\"]\n    rank5 = metrics[\"rank5\"]\n    miou = metrics[\"miou\"]\n    print(\"rank1: {:.4f} - rank5: {:.4f} - miou: {:.4f}\".format(rank1, rank5, miou))\n\n\n def _eval_predictions(self, predictions, targets):\n    \"\"\"The function computes the rank1, rank5, and miou metrics for the DiDeMo dataset.", "CodeBleu_score": 39.79561116389158}, {"code": " def print_computed_metrics(self, metrics)\n\t\n    rank1 = metrics[\"rank1\"]\n    rank5 = metrics[\"rank5\"]\n    miou = metrics[\"miou\"]\n    print(\n        \"Rank@1: {:.4f} - Rank@5: {:.4f} - mIoU: {:.4f}\".format(rank1, rank5, miou)\n    )\n\n", "CodeBleu_score": 42.78822427311806}]}
{"instance_id": "39c1a50b7e7d", "generation_result": [{"code": "def _eval_predictions(self, segments, data)\n\t\n    ranks = []\n    ious = []\n    for segment, data_item in zip(segments, data):\n        gt_segments = data_item[\\\"segments\\\"]\n        for gt_segment in gt_segments:\n            iou = self._iou(segment, gt_segment)\n            ious.append(iou)\n            rank = self._rank(segment, gt_segment)\n            ranks.append(rank)\n    rank1 = 100.0 * len([r for r in ranks if r <= 1]) / len(ranks)\n    rank5 = 100.0 * len([r for r in ranks if r <= 5]) / len(ranks)\n    miou = np.mean(ious)\n    return rank1, rank5, miou\n", "CodeBleu_score": 34.84141034507932}, {"code": "def _eval_predictions(self, segments, data)\n\t\n\n    # Get the segments and data.\n    segments = np.array(segments)\n    data = np.array(data)\n\n    # Compute the IoU and rank for each segment-data pair.\n    ious = np.zeros(segments.shape[0])\n    ranks = np.zeros(segments.shape[0])\n    for i in range(segments.shape[0]):\n        ious[i] = self._iou(segments[i], data[i])\n        ranks[i] = self._rank(segments[i], data[i])\n\n    # Compute the mean IoU and rank.\n    rank1 = np.mean(np.where(ranks <= 1, 1, 0))\n    rank5 = np.mean(np.where(ranks <= 5, 1, 0))\n    mean_iou = np.mean(ious)\n\n    return rank1, rank5, mean_iou\n\n", "CodeBleu_score": 25.542239682689328}, {"code": "def _eval_predictions(self, segments, data)\n\t\n    ious = []\n    ranks = []\n    for segment, data in zip(segments, data):\n        # segment: [start, end, label]\n        # data: [start, end, label]\n        # segment and data are sorted by start time.\n        # if segment and data are not sorted by start time,\n        # the result will be wrong.\n        iou = 0.0\n        rank = 0.0\n        for s, e, l in data:\n            # s: start\n            # e: end\n            # l: label\n            # find the index of the segment with the largest overlap with data.\n            # i: index of the segment with the largest overlap with data.\n            i = bisect.bisect_left(segment, (s, e, l))\n            # find the index of the segment with the largest overlap with data.\n            # i: index of the segment with the largest overlap with data.\n            # iou: overlap of the segment with the largest overlap with data.\n            iou = max(iou, self._iou(segment[i], (s, e, l)))\n            # rank: rank of the segment with the largest overlap with data.\n            rank = max(rank, self._rank(segment[i], (s, e, l)))\n        ious.append(iou)\n        ranks.append(rank)\n    ious = np.array(ious)\n    ranks = np.array(ranks)\n    rank1 = float(np.sum(ranks == 1)) / len(ranks)\n    rank5 = float(np.sum(ranks <= 5)) / len(ranks)\n    miou = np.mean(ious)\n    return rank1, rank5, miou\n\n\n", "CodeBleu_score": 37.294380056178184}]}
{"instance_id": "8ccf84f8d034", "generation_result": [{"code": "def predict_loop(self, model, eval_dataloader, output_file=None)\n\t\n    model.eval()\n    model.cuda()\n\n    # initialize the results\n    results = []\n    with torch.no_grad():\n        for batch in tqdm(eval_dataloader, desc=\"Eval\"):\n            batch = self.process_batch(batch)\n            batch = self.transfer_batch_to_gpu(batch, 0)\n            outputs = model(**batch)\n            results.extend(self.finalize_batch(outputs, batch))\n\n    # save the results\n    if output_file is not None:\n        with open(output_file, \"w\") as f:\n            json.dump(results, f)\n\n    return results\n\n", "CodeBleu_score": 33.32468752474813}, {"code": "def predict_loop(self, model, eval_dataloader, output_file=None)\n\t\n    model.eval()\n    model.cuda()\n    model.to(self.device)\n    outputs = []\n    for batch in tqdm(eval_dataloader):\n        batch = tuple(t.cuda() for t in batch)\n        with torch.no_grad():\n            outputs.append(model(*batch))\n    return outputs\n\n", "CodeBleu_score": 31.269699837039173}, {"code": "def predict_loop(self, model, eval_dataloader, output_file=None)\n\t\n    model.eval()\n    model.to(self.device)\n    scores = []\n    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n        batch = self._prepare_batch(batch)\n        with torch.no_grad():\n            batch = self._forward(model, batch)\n            scores.append(batch)\n    scores = self._finalize(scores)\n    if output_file is not None:\n        self._save_predictions(scores, output_file)\n    return scores\n\n", "CodeBleu_score": 30.420946930501007}]}
{"instance_id": "e68529ad727d", "generation_result": [{"code": "def _aggregate_scores(self, scores)\n\t\n    assert len(scores) == 2\n    video_scores = np.concatenate(scores[0], axis=0)\n    text_scores = np.concatenate(scores[1], axis=0)\n    self.full_scores = []\n    return np.dot(video_scores, text_scores.T)\n\n", "CodeBleu_score": 77.64767266794317}, {"code": "def _aggregate_scores(self, scores)\n\t\n    assert len(scores) == 2\n    full_scores = np.concatenate(scores[0], axis=0)\n    full_scores = np.concatenate(scores[1], axis=0)\n    self.full_scores = []\n    return full_scores\n", "CodeBleu_score": 67.41250523133908}, {"code": "def _aggregate_scores(self, scores)\n\t\n    assert len(scores) == 2\n    video_scores = np.concatenate(scores[0], axis=0)\n    text_scores = np.concatenate(scores[1], axis=0)\n    self.full_scores = []\n    return np.dot(video_scores, text_scores.T)\n\n", "CodeBleu_score": 77.64767266794317}]}
{"instance_id": "e4e1861fc357", "generation_result": [{"code": "def finalize(self, Y_pred, Y_true, output_file=None)\n\t\n    Y_pred = np.concatenate(Y_pred, axis=0)\n    Y_true = np.concatenate(Y_true, axis=0)\n    assert Y_pred.shape == Y_true.shape,\n        \\\"Prediction and target shapes do not match: {}, {}\\\".format(Y_pred.shape, Y_true.shape)\n    if output_file is not None:\n        with open(\n                os.path.join(self.pred_dir, output_file + \\\".pkl\\\"),\n                \\\"wb\\\") as fw:\n            pickle.dump(\n                {\\\"Y_pred\\\": Y_pred, \\\"Y_true\\\": Y_true}, fw,\n                protocol=pickle.HIGHEST_PROTOCOL)\n    return {\\\"outputs\\\": Y_pred, \\\"targets\\\": Y_true}\n\n    def _read_assignment(self, T, K, path):\n        \\\"\\\"\\\"\n        refactored from https://github.com/DmZhukov/CrossTask/blob/master/data.py\n        Howto interpret contraints on loss that is going to be minimized:\n        lambd is a big number;\n        self.lambd * C is a big number for all valid position (csv stores invalids)\n\n        def forward(self, O, Y, C):\n            return (Y*(self.lambd * C - self.lsm(O))).mean(dim=0).sum()\n\n        This will load the csv file and fill-in the step col from start to end rows.\n        \\\"\\\"\\\"\n\n        Y = np.zeros([T, K], dtype=np.uint8)\n        with open(path, 'r') as f:\n            for line in f:\n                step, start, end = line.strip().split(',')\n                start = int(math.floor(float(start)))\n                end = int(math.ceil(float(end)))\n                step = int(step) - 1\n                Y[start:end, step] = 1\n        return Y\n\n", "CodeBleu_score": 40.57549466377396}, {"code": "def finalize(self, Y_pred, Y_true, output_file=None)\n\t\n    Y_pred = np.concatenate(Y_pred, axis=0)\n    Y_true = np.concatenate(Y_true, axis=0)\n    assert Y_pred.shape == Y_true.shape,\\\n        \"Shape mismatch: {} != {}\".format(Y_pred.shape, Y_true.shape)\n    # check for errors\n    errors = []\n    for i in range(Y_true.shape[0]):\n        if not np.array_equal(Y_pred[i], Y_true[i]):\n            errors.append(i)\n    if len(errors) > 0:\n        print(\"Found {} errors.\".format(len(errors)))\n        for i in errors:\n            print(\"Y_pred: {}, Y_true: {}\".format(Y_pred[i], Y_true[i]))\n    if output_file is not None:\n        with open(os.path.join(self.pred_dir, output_file + \".pkl\"), \"wb\") as fw:\n            pickle.dump({\"Y_pred\": Y_pred, \"Y_true\": Y_true}, fw,\n                        protocol=pickle.HIGHEST_PROTOCOL)\n    return {\"outputs\": Y_pred, \"targets\": Y_true}\n\n", "CodeBleu_score": 55.65607426967423}, {"code": "def finalize(self, Y_pred, Y_true, output_file=None)\n\t\n    Y_pred = np.concatenate(Y_pred, axis=0)\n    Y_true = np.concatenate(Y_true, axis=0)\n    assert Y_pred.shape == Y_true.shape,\n        \\\"Y_pred.shape: {}, Y_true.shape: {}\\\".format(Y_pred.shape, Y_true.shape)\n    if output_file is not None:\n        with open(\n            os.path.join(self.pred_dir, output_file + \\\".pkl\\\"),\n            \\\"wb\\\") as fw:\n            pickle.dump(\n                {\\\"Y_pred\\\": Y_pred, \\\"Y_true\\\": Y_true}, fw,\n                protocol=pickle.HIGHEST_PROTOCOL)\n    return {\\\"outputs\\\": Y_pred, \\\"targets\\\": Y_true}\n\n", "CodeBleu_score": 36.22212266790257}]}
{"instance_id": "7df9ab9a4920", "generation_result": [{"code": "def reduce_metrics(logging_outputs) -> None\n\t\n    loss_sum = sum(log.get(\"loss\", 0) for log in logging_outputs)\n    sample_size = sum(log.get(\"sample_size\", 0) for log in logging_outputs)\n    metrics.log_scalar(\"loss\", loss_sum / sample_size, sample_size, round=3)\n\n", "CodeBleu_score": 62.429646714521894}, {"code": "def reduce_metrics(logging_outputs) -> None\n\t\n    loss_sum = sum(log.get(\"loss\", 0) for log in logging_outputs)\n    sample_size = sum(log.get(\"sample_size\", 0) for log in logging_outputs)\n\n    metrics.log_scalar(\"loss\", loss_sum / sample_size, sample_size, round=3)\n\n", "CodeBleu_score": 62.429646714521894}, {"code": "def reduce_metrics(logging_outputs) -> None\n\t\n\n    loss_sum = sum(log.get(\"loss\", 0) for log in logging_outputs)\n    sample_size = sum(log.get(\"sample_size\", 0) for log in logging_outputs)\n\n    metrics.log_scalar(\"loss\", loss_sum / sample_size, sample_size, round=3)\n\n", "CodeBleu_score": 62.429646714521894}]}
{"instance_id": "7f9c326a96f7", "generation_result": [{"code": "def _mm_on_the_fly(\n        self,\n        cmasks,\n        vmasks,\n        attention_mask\n    )\n\t\n    if attention_mask is None:\n        attention_mask = torch.cat([\n            cmasks, vmasks.new_zeros((cmasks.size(0), vmasks.size(1)))\n        ],\n        dim=1\n    )\n    token_type_ids = torch.cat([\n        torch.zeros_like(cmasks),\n        torch.ones_like(vmasks)\n    ],\n    dim=1)\n    return attention_mask, token_type_ids\n\n", "CodeBleu_score": 39.73250522583463}, {"code": "def _mm_on_the_fly(\n        self,\n        cmasks,\n        vmasks,\n        attention_mask\n    )\n\t\n    if attention_mask is None:\n        attention_mask = torch.cat([cmasks, vmasks], dim=1)\n    token_type_ids = torch.cat([\n        torch.zeros((cmasks.size(0), cmasks.size(1)), dtype=torch.long),\n        torch.ones((vmasks.size(0), vmasks.size(1)), dtype=torch.long)\n        ],\n        dim=1\n    )\n    return attention_mask, token_type_ids\n\n", "CodeBleu_score": 45.77924516383497}, {"code": "def _mm_on_the_fly(\n        self,\n        cmasks,\n        vmasks,\n        attention_mask\n    )\n\t\n    if attention_mask is None:\n        attention_mask = torch.cat([cmasks, vmasks], dim=-1)\n    token_type_ids = torch.cat([\n        torch.zeros_like(cmasks),\n        torch.ones_like(vmasks)\n    ],\n    dim=-1", "CodeBleu_score": 19.170887596974936}]}
{"instance_id": "fa99d2a148a5", "generation_result": [{"code": "def prepare_inputs_for_generation(\n        self,\n        input_ids,\n        input_video_embeds,\n        attention_mask=None,\n        token_type_ids=None,\n        **model_kwargs\n    )\n\t\n    # input_ids, input_video_embeds, attention_mask, token_type_ids = inputs\n    # check input_ids\n    if input_ids is not None and input_video_embeds is not None:\n        if input_ids.size(1) + input_video_embeds.size(1) > self.config.max_position_embeddings:\n            raise ValueError(\n                \"The sum of input_ids and input_video_embeds cannot exceed {}. \"\n                \"The input_ids has a length of {}, and the input_video_embeds has a length of {}.\".format(\n                    self.config.max_position_embeddings,\n                    input_ids.size(1),\n                    input_video_embeds.size(1),\n                )\n            )\n\n    # prepare attention_mask and token_type_ids\n    if input_ids is not None and attention_mask is not None:\n        # if model is used as a decoder in encoder-decoder model, the attention mask is created on the fly\n        if attention_mask.dim() == 3:\n            extended_attention_mask = attention_mask[:, None, :, :]\n        elif attention_mask.dim() == 2:\n            # Provided a padding mask of dimensions [batch_size, seq_length]\n            # - if the model is a decoder, apply a causal mask in addition to the padding mask\n            # - if the model is an encoder, make the mask broadcastable to [batch_size, num_heads, seq_length, seq_length]\n            if self.config.is_decoder:\n                batch_size, seq_length = input_ids.shape\n                seq_ids = torch.arange(seq_length, device=input_ids.device)\n                causal_mask = seq_ids[None, None, :].repeat(batch_size, seq_length, 1) <= seq_ids[None, :, None]\n                causal_mask = causal_mask.to(\n                    attention_mask.dtype\n                )  # causal and attention masks must have same type with pytorch version < 1.3\n                extended_attention_mask = causal_mask[:, None, :, :] * attention_mask[:, None, None, :]\n            else:\n                extended_attention_mask = attention_mask[:, None, None, :]\n        else:\n            raise ValueError(\n                \"Wrong shape for", "CodeBleu_score": 35.7173623403136}, {"code": "def prepare_inputs_for_generation(\n        self,\n        input_ids,\n        input_video_embeds,\n        attention_mask=None,\n        token_type_ids=None,\n        **model_kwargs\n    )\n\t\n    # check input_ids\n    if input_ids is not None and input_video_embeds is not None:\n        input_shape = input_ids.size()\n        video_shape = input_video_embeds.size()\n        batch_size, seq_len = input_shape[:2]\n        video_batch_size, video_seq_len = video_shape[:2]\n        if batch_size != video_batch_size:\n            raise ValueError(\n                f\"batch_size of input_ids and input_video_embeds must be equal, got {batch_size} and {video_batch_size}\"\n            )\n        if seq_len + video_seq_len > self.config.max_position_embeddings:\n            raise ValueError(\n                f\"The sum of input_ids and input_video_embeds sequence length \"\n                f\"({seq_len + video_seq_len}) is longer than the maximum sequence length \"\n                f\"{self.config.max_position_embeddings}.\"\n            )\n\n    # prepare attention mask\n    if attention_mask is not None:\n        # [batch_size, seq_len] -> [batch_size, 1, 1, seq_len]\n        attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n        # [batch_size, 1, 1, seq_len] -> [batch_size, 1, seq_len + video_seq_len, seq_len + video_seq_len]\n        attention_mask = attention_mask.repeat(1, 1, seq_len + video_seq_len, 1)\n        # [batch_size, 1, seq_len + video_seq_len, seq_len + video_seq_len] -> [batch_size, seq_len + video_seq_len, seq_len + video_seq_len]\n        attention_mask = attention_mask[:, 0, :, :]\n\n    # prepare token type ids\n    if token_type_ids is not None:\n        # [batch_size, seq_len] -> [batch_size, 1, seq_len + video_seq_len]\n        token_type_ids = token_type_ids.unsqueeze(1)\n        # [batch_size, 1, seq_len + video_seq_len] -> [batch_size", "CodeBleu_score": 37.79601365224657}, {"code": "def prepare_inputs_for_generation(\n        self,\n        input_ids,\n        input_video_embeds,\n        attention_mask=None,\n        token_type_ids=None,\n        **model_kwargs\n    )\n\t\n    # get the combined sequence length\n    combined_length = input_ids.size(1) + input_video_embeds.size(1)\n    if input_ids.size(1) > input_video_embeds.size(1):\n        # the video sequence is longer than the text sequence\n        # so we need to add some padding to the text sequence\n        padding_length = input_ids.size(1) - input_video_embeds.size(1)\n        input_ids = F.pad(input_ids, (0, padding_length), value=self.config.pad_token_id)\n        if attention_mask is not None:\n            attention_mask = F.pad(attention_mask, (0, padding_length), value=0)\n        if token_type_ids is not None:\n            token_type_ids = F.pad(token_type_ids, (0, padding_length), value=0)\n    else:\n        # the text sequence is longer than the video sequence\n        # so we need to add some padding to the video sequence\n        padding_length = input_video_embeds.size(1) - input_ids.size(1)\n        input_video_embeds = F.pad(input_video_embeds, (0, padding_length), value=0)\n        if attention_mask is not None:\n            attention_mask = F.pad(attention_mask, (0, padding_length), value=0)\n        if token_type_ids is not None:\n            token_type_ids = F.pad(token_type_ids, (0, padding_length), value=1)\n\n    # adjust the sequence length to the combined length\n    input_ids = input_ids[:, :combined_length]\n    input_video_embeds = input_video_embeds[:, :combined_length]\n    if attention_mask is not None:\n        attention_mask = attention_mask[:, :combined_length]\n    if token_type_ids is not None:\n        token_type_ids = token_type_ids[:, :combined_length]\n\n    return {\n        \"input_ids\": input_ids,\n        \"input_video_embeds\": input_video_embeds,\n        \"attention_mask\": attention_mask,\n        \"token_type_ids\": token_type_ids,\n    }", "CodeBleu_score": 54.957851268299315}]}
{"instance_id": "b1b0d25fe620", "generation_result": [{"code": "def _init_pipeline(self, cfg: ConfigType) -> Compose\n\t\n    # adjust the pipeline configuration\n    if 'LoadImageFromFile' in cfg.data.test.pipeline:\n        load_img_idx = cfg.data.test.pipeline.index('LoadImageFromFile')\n        cfg.data.test.pipeline[load_img_idx] = 'mmdet.InferencerLoader'\n    if 'meta_keys' in cfg.data.test:\n        meta_keys = cfg.data.test.meta_keys\n        if 'img_id' in meta_keys:\n            meta_keys.remove('img_id')\n    # find the index of the 'LoadImageFromFile' transform\n    load_img_idx = None\n    for idx, transform in enumerate(cfg.data.test.pipeline):\n        if transform['type'] == 'LoadImageFromFile':\n            load_img_idx = idx\n            break\n    if load_img_idx is None:\n        raise ValueError('Cannot find the index of the `LoadImageFromFile` '\\\n                         'transform in the test pipeline.')\n    # update the transform type to 'mmdet.InferencerLoader'\n    cfg.data.test.pipeline[load_img_idx]['type'] = 'mmdet.InferencerLoader'\n    # return the modified pipeline configuration wrapped in a Compose object\n    return Compose(cfg.data.test.pipeline)\n", "CodeBleu_score": 31.057990639592237}, {"code": "def _init_pipeline(self, cfg: ConfigType) -> Compose\n\t\n    # adjust the pipeline configuration by removing the 'img_id' key from\n    # meta_keys if present\n    if 'meta_keys' in cfg.data:\n        meta_keys = cfg.data.meta_keys\n        if 'img_id' in meta_keys:\n            meta_keys.remove('img_id')\n            cfg.data.meta_keys = meta_keys\n    # find the index of the 'LoadImageFromFile' transform, raising an error if\n    # not found\n    load_image_from_file_index = None\n    for i, transform in enumerate(cfg.data.test.pipeline):\n        if transform['type'] == 'LoadImageFromFile':\n            load_image_from_file_index = i\n            break\n    if load_image_from_file_index is None:\n        raise ValueError(\n            'LoadImageFromFile is not found in the test pipeline. '\n            'Please make sure that the test pipeline contains it.')\n    # update the transform type to 'mmdet.InferencerLoader'\n    cfg.data.test.pipeline[load_image_from_file_index]['type'] = 'mmdet.InferencerLoader'\n    # return the modified pipeline configuration wrapped in a Compose object\n    return Compose(cfg.data.test.pipeline)\n\n", "CodeBleu_score": 35.61663308675285}, {"code": "def _init_pipeline(self, cfg: ConfigType) -> Compose\n\t\n    pipeline = cfg.data.test.pipeline\n    # remove the img_id key from meta_keys if present\n    if 'LoadImageFromFile' in pipeline:\n        load_image_from_file_cfg = pipeline[pipeline.index('LoadImageFromFile')]\n        if 'meta_keys' in load_image_from_file_cfg:\n            meta_keys = load_image_from_file_cfg['meta_keys']\n            if 'img_id' in meta_keys:\n                meta_keys.remove('img_id')\n    # find the index of the 'LoadImageFromFile' transform, raising an error if not found\n    load_image_from_file_idx = None\n    for idx, transform in enumerate(pipeline):\n        if transform['type'] == 'LoadImageFromFile':\n            load_image_from_file_idx = idx\n            break\n    if load_image_from_file_idx is None:\n        raise ValueError('LoadImageFromFile is not in the pipeline')\n    # update the transform type to 'mmdet.InferencerLoader'\n    pipeline[load_image_from_file_idx]['type'] = 'mmdet.InferencerLoader'\n    return Compose(pipeline)\n\n", "CodeBleu_score": 35.79029622985727}]}
{"instance_id": "7967962856eb", "generation_result": [{"code": "def _inputs_to_list(self, inputs: InputsType) -> list\n\t\n    if isinstance(inputs, (list, tuple)):\n        return inputs\n    elif isinstance(inputs, str):\n        if os.path.isdir(inputs):\n            inputs = [\n                os.path.join(inputs, f) for f in os.listdir(inputs)\n                if f.lower().endswith(IMG_EXTENSIONS)\n            ]\n        else:\n            inputs = [inputs]\n    else:\n        inputs = [inputs]\n    return inputs\n\n", "CodeBleu_score": 44.5378526632034}, {"code": "def _inputs_to_list(self, inputs: InputsType) -> list\n\t\n    if isinstance(inputs, (list, tuple)):\n        return inputs\n    elif isinstance(inputs, str):\n        if os.path.isdir(inputs):\n            inputs = [\n                os.path.join(inputs, f) for f in os.listdir(inputs)\n                if f.lower().endswith(IMG_EXTENSIONS)\n            ]\n        else:\n            inputs = [inputs]\n    else:\n        inputs = [inputs]\n    return inputs\n\n", "CodeBleu_score": 44.5378526632034}, {"code": "def _inputs_to_list(self, inputs: InputsType) -> list\n\t\n    if isinstance(inputs, (list, tuple)):\n        return inputs\n    elif isinstance(inputs, str):\n        if os.path.isdir(inputs):\n            inputs = os.listdir(inputs)\n            inputs = [os.path.join(inputs, f) for f in inputs if f.endswith(IMG_EXTENSIONS)]\n            return inputs\n        else:\n            raise ValueError('The input is a string but it is not a directory.')\n    else:\n        return [inputs]\n\n", "CodeBleu_score": 41.93626474026008}]}
{"instance_id": "74f727f06fa0", "generation_result": [{"code": "def _get_chunk_data(self, inputs: Iterable, chunk_size: int)\n\t\n    chunk_data = []\n    for input in inputs:\n        if isinstance(input, dict):\n            if 'img' in input:\n                img = input['img']\n            elif 'img_path' in input:\n                img = mmcv.imread(input['img_path'])\n            else:\n                raise ValueError(\n                    'The input dict should contain an \\'img\\' or \\'img_path\\' key.')\n            if self.pipeline is not None:\n                img = self.pipeline(img)\n            chunk_data.append(img)\n            if len(chunk_data) == chunk_size:\n                yield chunk_data\n                chunk_data = []\n        else:\n            raise TypeError(\n                'The input should be a dict containing an \\'img\\' or \\'img_path\\' key.')\n    if len(chunk_data) > 0:\n        yield chunk_data\n\n", "CodeBleu_score": 47.411913118599365}, {"code": "def _get_chunk_data(self, inputs: Iterable, chunk_size: int)\n\t\n    chunk_data = []\n    for data in inputs:\n        if isinstance(data, dict):\n            if 'img' in data:\n                data = data['img']\n            elif 'img_path' in data:\n                data = data['img_path']\n        if isinstance(data, str):\n            data = [data]\n        chunk_data.append(data)\n        if len(chunk_data) == chunk_size:\n            yield self._pipeline(chunk_data)\n            chunk_data = []\n    if len(chunk_data) > 0:\n        yield self._pipeline(chunk_data)\n\n", "CodeBleu_score": 35.039884226712886}, {"code": "def _get_chunk_data(self, inputs: Iterable, chunk_size: int)\n\t\n    chunk = []\n    for data in inputs:\n        if isinstance(data, dict) and 'img' in data:\n            chunk.append(data)\n        else:\n            chunk.append({'img': data})\n        if len(chunk) == chunk_size:\n            yield self.pipeline(chunk)\n            chunk = []\n    if len(chunk) > 0:\n        yield self.pipeline(chunk)\n\n", "CodeBleu_score": 21.965794307066915}]}
{"instance_id": "a43ab51afa59", "generation_result": [{"code": "def postprocess(\n        self,\n        preds: PredType,\n        visualization: Optional[List[np.ndarray]] = None,\n        return_datasamples: bool = False,\n        print_result: bool = False,\n        no_save_pred: bool = False,\n        pred_out_dir: str = '',\n        **kwargs,\n    ) -> Dict\n\t\n    # TODO: Add support for saving datasamples.\n    if pred_out_dir != '' and return_datasamples:\n        warnings.warn('Saving datasamples is not supported.')\n\n    if not no_save_pred and pred_out_dir == '':\n        pred_out_dir = osp.join(self.out_dir, 'pred')\n\n    if not osp.exists(pred_out_dir):\n        os.makedirs(pred_out_dir)\n\n    results = {'predictions': [], 'visualization': []}\n    if return_datasamples:\n        results['datasamples'] = []\n\n    if visualization is not None:\n        results['visualization'] = visualization\n\n    if preds is not None:\n        if return_datasamples:\n            for pred in preds:\n                results['datasamples'].append(pred)\n        else:\n            for pred in preds:\n                results['predictions'].append(pred.to_dict())\n\n    if print_result:\n        print_predictions(results['predictions'])\n\n    if not no_save_pred:\n        for pred in results['predictions']:\n            pred_name = pred['filename'].replace('/', '_') + '.json'\n            pred_path = osp.join(pred_out_dir, pred_name)\n            mmcv.dump(pred, pred_path)\n\n    return results\n", "CodeBleu_score": 44.79168450352356}, {"code": "def postprocess(\n        self,\n        preds: PredType,\n        visualization: Optional[List[np.ndarray]] = None,\n        return_datasamples: bool = False,\n        print_result: bool = False,\n        no_save_pred: bool = False,\n        pred_out_dir: str = '',\n        **kwargs,\n    ) -> Dict\n\t\n    if no_save_pred is True:\n        pred_out_dir = ''\n    if not return_datasamples and pred_out_dir != '':\n        warnings.warn('Saving DetDataSample objects is not supported.')\n\n    results = {'predictions': [], 'visualization': []}\n\n    if return_datasamples:\n        results['predictions'] = preds\n    else:\n        for pred in preds:\n            results['predictions'].append(pred.to_dict())\n\n    if visualization is not None:\n        results['visualization'] = visualization\n\n    if print_result:\n        self._print_results(results)\n\n    return results\n\n", "CodeBleu_score": 50.07926985105111}, {"code": "def postprocess(\n        self,\n        preds: PredType,\n        visualization: Optional[List[np.ndarray]] = None,\n        return_datasamples: bool = False,\n        print_result: bool = False,\n        no_save_pred: bool = False,\n        pred_out_dir: str = '',\n        **kwargs,\n    ) -> Dict\n\t\n    if no_save_pred is True:\n        pred_out_dir = ''\n\n    if pred_out_dir != '' and return_datasamples is True:\n        warnings.warn('Saving datasamples is not supported for now.')\n\n    if self.output_processor is None:\n        raise ValueError('Post-processing needs the \\\"output_processor\\\" term'\\\n                         'defined in the config, but got None.')\n\n    if return_datasamples:\n        results = {'predictions': preds}\n    else:\n        results = {'predictions': []}\n\n    if visualization is not None:\n        results['visualization'] = visualization\n\n    if print_result:\n        self.print_result(preds)\n\n    if pred_out_dir != '':\n        self.save_result(preds, pred_out_dir)\n\n    return results\n\n", "CodeBleu_score": 47.16086499670447}]}
{"instance_id": "a3c2561f199c", "generation_result": [{"code": "def build_test_pipeline(cfg: ConfigType) -> ConfigType\n\t\n    # TODO: Awaiting refactoring\n    cfg = cfg.copy()\n    cfg.data.test.pipeline = replace_ImageToTensor(cfg.data.test.pipeline)\n    test_pipeline = Compose(cfg.data.test.pipeline)\n    return test_pipeline\n\n", "CodeBleu_score": 16.91052646453502}, {"code": "def build_test_pipeline(cfg: ConfigType) -> ConfigType\n\t\n    cfg = cfg.copy()\n    # We only need 'Resize' for test pipeline\n    cfg.data.test.pipeline[0].type = 'Resize'\n    # We copy the last transform in the pipeline and combine it with the modified first transform\n    cfg.data.test.pipeline[-1] = cfg.data.test.pipeline[0].copy()\n    # We combine the first and last transforms into a new test pipeline\n    cfg.data.test.pipeline = Compose(cfg.data.test.pipeline)\n    return cfg\n\n", "CodeBleu_score": 23.57583455131509}, {"code": "def build_test_pipeline(cfg: ConfigType) -> ConfigType\n\t\n    # TODO: Awaiting refactoring\n    # if cfg.data.test.pipeline is not None:\n    #     return cfg.data.test.pipeline\n    # else:\n    #     cfg = cfg.copy()\n    #     cfg.data.test.pipeline = replace_ImageToTensor(cfg.data.test.pipeline)\n    #     return cfg.data.test.pipeline\n    return cfg.data.test.pipeline\n\n", "CodeBleu_score": 3.3118530705614906}]}
{"instance_id": "48a358ab5a38", "generation_result": [{"code": "def inference_mot(model: nn.Module, img: np.ndarray, frame_id: int,\n                  video_len: int) -> SampleList\n\t\n    cfg = model.cfg\n    if cfg.data.test.type == 'ConcatDataset':\n        cfg = cfg.data.test.datasets[0]\n\n    if cfg.data.test.pipeline[0].type == 'LoadImageFromFile':\n        cfg.data.test.pipeline[0].type = 'LoadImageFromNDArray'\n\n    test_pipeline = get_test_pipeline_cfg(cfg)\n    test_pipeline = Compose(test_pipeline)\n\n    data = dict(img=img, frame_id=frame_id, video_len=video_len)\n    data = test_pipeline(data)\n    data['img_metas'] = [data['img_metas']]\n    data['img'] = [data['img']]\n    data['data_samples'] = [data['data_samples']]\n\n    if model.data_preprocessor.device.type == 'cpu':\n        for m in model.modules():\n            assert not isinstance(\n                m, RoIPool\n            ), 'CPU inference with RoIPool is not supported currently.'\n\n    with torch.no_grad():\n        results = model.test_step(data)[0]\n\n    return results\n\n", "CodeBleu_score": 56.84421961732369}, {"code": "def inference_mot(model: nn.Module, img: np.ndarray, frame_id: int,\n                  video_len: int) -> SampleList\n\t\n    # prepare data\n    data = dict(img=img, img_id=frame_id, video_meta=dict(original_shape=img.shape,\n                                                        video_len=video_len))\n    # build the data pipeline\n    test_pipeline = get_test_pipeline_cfg(model.cfg)\n    test_pipeline = Compose(test_pipeline)\n    data = test_pipeline(data)\n    data['inputs'] = [data['inputs']]\n    data['data_samples'] = [data['data_samples']]\n\n    # forward the model\n    with torch.no_grad():\n        results = model.test_step(data)[0]\n    return results\n\n", "CodeBleu_score": 30.073879730659925}, {"code": "def inference_mot(model: nn.Module, img: np.ndarray, frame_id: int,\n                  video_len: int) -> SampleList\n\t\n    cfg = model.cfg\n    if cfg.data.test.pipeline is None:\n        cfg = cfg.copy()\n        cfg.data.test.pipeline = get_test_pipeline_cfg(cfg)\n        if isinstance(img, np.ndarray):\n            # Calling this method across libraries will result\n            # in module unregistered error if not prefixed with mmdet.\n            cfg.data.test.pipeline[0].type = 'mmdet.LoadImageFromNDArray'\n\n        cfg.data.test.pipeline = Compose(cfg.data.test.pipeline)\n\n    if model.data_preprocessor.device.type == 'cpu':\n        for m in model.modules():\n            assert not isinstance(\n                m, RoIPool\n            ), 'CPU inference with RoIPool is not supported currently.'\n\n    data = dict(img=img, img_id=frame_id, orig_shape=img.shape, img_info=dict(\n        id=frame_id, frame_id=frame_id, video_id=0, video_len=video_len))\n\n    data = cfg.data.test.pipeline(data)\n    data['inputs'] = [data['inputs']]\n    data['data_samples'] = [data['data_samples']]\n\n    with torch.no_grad():\n        results = model.test_step(data)[0]\n    return results\n\n", "CodeBleu_score": 53.99130461050593}]}
{"instance_id": "6c4ea2ce1787", "generation_result": [{"code": "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\n\t\n    while True:\n        total_length = len(tokens_a) + len(tokens_b)\n        if total_length <= max_num_tokens:\n            break\n        if len(tokens_a) > len(tokens_b):\n            trunc_tokens = tokens_a\n        else:\n            trunc_tokens = tokens_b\n        assert len(trunc_tokens) >= 1\n        # We want to sometimes truncate from the front and sometimes from the\n        # back to add more randomness and avoid biases.\n        if rng.random() < 0.5:\n            del trunc_tokens[0]\n        else:\n            trunc_tokens.pop()\n\n\n", "CodeBleu_score": 65.58756911386239}, {"code": "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\n\t\n    while True:\n        total_length = len(tokens_a) + len(tokens_b)\n        if total_length <= max_num_tokens:\n            break\n\n        trunc_tokens = tokens_a if len(tokens_a) > len(tokens_b) else tokens_b\n        assert len(trunc_tokens) >= 1\n\n        # We want to sometimes truncate from the front and sometimes from the\n        # back to add more randomness and avoid biases.\n        if rng.random() < 0.5:\n            del trunc_tokens[0]\n        else:\n            trunc_tokens.pop()\n\n", "CodeBleu_score": 77.02217016729102}, {"code": "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\n\t\n    while True:\n        total_length = len(tokens_a) + len(tokens_b)\n        if total_length <= max_num_tokens:\n            break\n\n        trunc_tokens = tokens_a if len(tokens_a) > len(tokens_b) else tokens_b\n        assert len(trunc_tokens) >= 1\n\n        # We want to sometimes truncate from the front and sometimes from the\n        # back to add more randomness and avoid biases.\n        if rng.random() < 0.5:\n\n          del trunc_tokens[0]\n\n        else:\n\n          trunc_tokens.pop()\n\n\n\n", "CodeBleu_score": 77.27328758770102}]}
{"instance_id": "007181b6729d", "generation_result": [{"code": "def get_activation(activation_string)\n\t\n    if not isinstance(activation_string, six.string_types):\n        return activation_string\n\n    if not activation_string:\n        return None\n\n    act = activation_string.lower()\n    if act == \"linear\":\n        return None\n    elif act == \"relu\":\n        return tf.nn.relu\n    elif act == \"gelu\":\n        return gelu\n    elif act == \"tanh\":\n        return tf.tanh\n    else:\n        raise ValueError(\"Unsupported activation: %s\" % act)\n", "CodeBleu_score": 95.00892113723604}, {"code": "def get_activation(activation_string)\n\t\n    if not isinstance(activation_string, six.string_types):\n        return activation_string\n\n    if not activation_string:\n        return None\n\n    act = activation_string.lower()\n    if act == \"linear\":\n        return None\n    elif act == \"relu\":\n        return tf.nn.relu\n    elif act == \"gelu\":\n        return gelu\n    elif act == \"tanh\":\n        return tf.tanh\n    else:\n        raise ValueError(\"Unsupported activation: %s\" % act)\n\n", "CodeBleu_score": 95.00892113723604}, {"code": "def get_activation(activation_string)\n\t\n    if not isinstance(activation_string, six.string_types):\n        return activation_string\n\n    if not activation_string:\n        return None\n\n    act = activation_string.lower()\n    if act == \"linear\":\n        return None\n    elif act == \"relu\":\n        return tf.nn.relu\n    elif act == \"gelu\":\n        return gelu\n    elif act == \"tanh\":\n        return tf.tanh\n    else:\n        raise ValueError(\"Unsupported activation: %s\" % act)\n\n", "CodeBleu_score": 95.00892113723604}]}
{"instance_id": "d5d6e0315899", "generation_result": [{"code": "def dropout(input_tensor, dropout_prob)\n\t\n    if dropout_prob is not None and dropout_prob > 0.0:\n        output = tf.nn.dropout(input_tensor, 1.0 - dropout_prob)\n    else:\n        output = input_tensor\n    return output\n\n", "CodeBleu_score": 68.55049391942053}, {"code": "def dropout(input_tensor, dropout_prob)\n\t\n    if dropout_prob is None or dropout_prob == 0.0:\n        return input_tensor\n    else:\n        return tf.nn.dropout(input_tensor, 1.0 - dropout_prob)\n\n", "CodeBleu_score": 60.84090845772291}, {"code": "def dropout(input_tensor, dropout_prob)\n\t\n    if dropout_prob is None or dropout_prob == 0.0:\n        return input_tensor\n    output = tf.nn.dropout(input_tensor, rate=dropout_prob)\n    return output\n", "CodeBleu_score": 70.84479337256124}]}
{"instance_id": "828917b9ac53", "generation_result": [{"code": "def create_attention_mask_from_input_mask(from_tensor, to_mask)\n\t\n    from_shape = get_shape_list(from_tensor, expected_rank=[2, 3])\n    batch_size = from_shape[0]\n    from_seq_length = from_shape[1]\n\n    to_shape = get_shape_list(to_mask, expected_rank=2)\n    to_seq_length = to_shape[1]\n\n    to_mask = tf.cast(\n        tf.reshape(to_mask, [batch_size, 1, to_seq_length]), tf.float32)\n\n    # We don't assume that `from_tensor` is a mask (although it could be). We\n    # don't actually care if we attend *from* padding tokens (only *to* padding)\n    # tokens so we create a tensor of all ones.\n    #\n    # `broadcast_ones` = [batch_size, from_seq_length, 1]\n    broadcast_ones = tf.ones(\n        shape=[batch_size, from_seq_length, 1], dtype=tf.float32)\n\n    # Here we broadcast along two dimensions to create the mask.\n    mask = broadcast_ones * to_mask\n\n    return mask\n\n", "CodeBleu_score": 64.09901013046853}, {"code": "def create_attention_mask_from_input_mask(from_tensor, to_mask)\n\t\n    from_shape = get_shape_list(from_tensor, expected_rank=[2, 3])\n    batch_size = from_shape[0]\n    from_seq_length = from_shape[1]\n\n    to_shape = get_shape_list(to_mask, expected_rank=2)\n    to_seq_length = to_shape[1]\n\n    to_mask = tf.cast(\n        tf.reshape(to_mask, [batch_size, 1, to_seq_length]), tf.float32)\n\n    # We don't assume that `from_tensor` is a mask (although it could be). We\n    # don't actually care if we attend *from* padding tokens (only *to* padding)\n    # tokens so we create a tensor of all ones.\n    #\n    # `broadcast_ones` = [batch_size, from_seq_length, 1]\n    broadcast_ones = tf.ones(\n        shape=[batch_size, from_seq_length, 1], dtype=tf.float32)\n\n    # Here we broadcast along two dimensions to create the mask.\n    mask = broadcast_ones * to_mask\n\n    return mask\n\n", "CodeBleu_score": 64.09901013046853}, {"code": "def create_attention_mask_from_input_mask(from_tensor, to_mask)\n\t\n    from_shape = get_shape_list(from_tensor, expected_rank=[2, 3])\n    batch_size = from_shape[0]\n    from_seq_length = from_shape[1]\n\n    to_shape = get_shape_list(to_mask, expected_rank=2)\n    to_seq_length = to_shape[1]\n\n    to_mask = tf.cast(\n        tf.reshape(to_mask, [batch_size, 1, to_seq_length]), tf.float32)\n\n    # We don't assume that `from_tensor` is a mask (although it could be). We\n    # don't actually care if we attend *from* padding tokens (only *to* padding)\n    # tokens so we create a tensor of all ones.\n    #\n    # `broadcast_ones` = [batch_size, from_seq_length, 1]\n    broadcast_ones = tf.ones(\n        shape=[batch_size, from_seq_length, 1], dtype=tf.float32)\n\n    # Here we broadcast along two dimensions to create the mask.\n    mask = broadcast_ones * to_mask\n\n    return mask\n\n", "CodeBleu_score": 64.09901013046853}]}
{"instance_id": "d7798e41a723", "generation_result": [{"code": "def transpose_for_scores(input_tensor, batch_size, num_attention_heads,\n                           seq_length, width)\n\t\n    output_tensor = tf.reshape(input_tensor, [batch_size, seq_length, num_attention_heads, width])\n\n    output_tensor = tf.transpose(output_tensor, [0, 2, 1, 3])\n    return output_tensor\n\n", "CodeBleu_score": 54.22065594588168}, {"code": "def transpose_for_scores(input_tensor, batch_size, num_attention_heads,\n                           seq_length, width)\n\t\n    output_tensor = tf.reshape(input_tensor, [batch_size, seq_length, num_attention_heads, width])\n\n    output_tensor = tf.transpose(output_tensor, [0, 2, 1, 3])\n    return output_tensor\n\n", "CodeBleu_score": 54.22065594588168}, {"code": "def transpose_for_scores(input_tensor, batch_size, num_attention_heads,\n                           seq_length, width)\n\t\n    output_tensor = tf.reshape(input_tensor, [batch_size, seq_length, num_attention_heads, width])\n\n    output_tensor = tf.transpose(output_tensor, [0, 2, 1, 3])\n    return output_tensor\n\n", "CodeBleu_score": 54.22065594588168}]}
{"instance_id": "711d466ace09", "generation_result": [{"code": "def reshape_to_matrix(input_tensor)\n\t\n    ndims = input_tensor.shape.ndims\n    if ndims < 2:\n    raise ValueError(\\\"Input tensor must have at least rank 2. Shape = %s\\\" % (input_tensor.shape))\n  if ndims == 2:\n    return input_tensor\n  width = input_tensor.shape[-1]\n  output_tensor = tf.reshape(input_tensor, [-1, width])\n  return output_tensor\n\n\n\n\n", "CodeBleu_score": 75.05574768960128}, {"code": "def reshape_to_matrix(input_tensor)\n\t\n    ndims = input_tensor.shape.ndims\n    if ndims < 2:\n    raise ValueError(\\\"Input tensor must have at least rank 2. Shape = %s\\\" % (input_tensor.shape))\n  if ndims == 2:\n    return input_tensor\n  width = input_tensor.shape[-1]\n  output_tensor = tf.reshape(input_tensor, [-1, width])\n  return output_tensor\n\n\n\n\n", "CodeBleu_score": 75.05574768960128}, {"code": "def reshape_to_matrix(input_tensor)\n\t\n    ndims = input_tensor.shape.ndims\n    if ndims < 2:\n    raise ValueError(\\\"Input tensor must have at least rank 2. Shape = %s\\\" % (input_tensor.shape))\n  if ndims == 2:\n    return input_tensor\n\n  width = input_tensor.shape[-1]\n  output_tensor = tf.reshape(input_tensor, [-1, width])\n\n  return output_tensor\n\n\n\n\n", "CodeBleu_score": 75.05574768960128}]}
{"instance_id": "7adcd08f531e", "generation_result": [{"code": "def get_train_examples(self, data_dir)\n\t\n    # train_data_file = os.path.join(data_dir, \\\"xnli.train.tsv\\\")\n    # train_data = self._read_tsv(train_data_file)\n    # examples = []\n    # for (i, line) in enumerate(train_data):\n    if i == 0:\n        continue\n    #     guid = \\\"train-%d\\\" % (i)\n    #     text_a = tokenization.convert_to_unicode(line[0])\n    #     text_b = tokenization.convert_to_unicode(line[1])\n    #     label = tokenization.convert_to_unicode(line[2])\n    #     if label == \\\"contradictory\\\":\n    #         label = \\\"contradiction\\\"\n    #     examples.append(\n    #         InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n    # return examples\n    return None\n\n", "CodeBleu_score": 3.6675608131548456}, {"code": "def get_train_examples(self, data_dir)\n\t\n    lines = self._read_tsv(os.path.join(data_dir, \\\"train.tsv\\\"))\n    examples = []\n    for (i, line) in enumerate(lines):\n      if i == 0:\n        continue\n      guid = \\\"train-%d\\\" % (i)\n      text_a = tokenization.convert_to_unicode(line[0])\n      text_b = tokenization.convert_to_unicode(line[1])\n      label = tokenization.convert_to_unicode(line[2])\n      if label == \\\"contradictory\\\":\n        label = \\\"contradiction\\\"\n      examples.append(InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n    return examples\n\n", "CodeBleu_score": 47.28474828360129}, {"code": "def get_train_examples(self, data_dir)\n\t\n    return self._create_examples(self._read_tsv(os.path.join(data_dir, \\\"xnli.train.tsv\\\"), \\\"\\\"), \\\"train\\\")\n", "CodeBleu_score": 4.428767562809101}]}
{"instance_id": "26a0bb96dab9", "generation_result": [{"code": "def metric_fn(per_example_loss, label_ids, logits, is_real_example)\n\t\n    predictions = tf.argmax(logits, axis=-1, output_type=tf.int32)\n    accuracy = tf.metrics.accuracy(\n        labels=label_ids, predictions=predictions, weights=is_real_example)\n    loss = tf.metrics.mean(values=per_example_loss, weights=is_real_example)\n    return {\n        \"eval_accuracy\": accuracy,\n        \"eval_loss\": loss,\n    }\n\n", "CodeBleu_score": 65.47238279114057}, {"code": "def metric_fn(per_example_loss, label_ids, logits, is_real_example)\n\t\n    predictions = tf.argmax(logits, axis=-1, output_type=tf.int32)\n    accuracy = tf.metrics.accuracy(\n        labels=label_ids, predictions=predictions, weights=is_real_example)\n    loss = tf.metrics.mean(\n        values=per_example_loss, weights=is_real_example)\n    return {\n        \"eval_accuracy\": accuracy,\n        \"eval_loss\": loss,\n    }\n\n", "CodeBleu_score": 61.73755888184837}, {"code": "def metric_fn(per_example_loss, label_ids, logits, is_real_example)\n\t\n    predictions = tf.argmax(logits, axis=-1, output_type=tf.int32)\n    accuracy = tf.metrics.accuracy(\n        labels=label_ids, predictions=predictions, weights=is_real_example)\n    loss = tf.metrics.mean(values=per_example_loss, weights=is_real_example)\n    return {\n        \"eval_accuracy\": accuracy,\n        \"eval_loss\": loss,\n    }\n\n", "CodeBleu_score": 65.47238279114057}]}
{"instance_id": "6154e4d28509", "generation_result": [{"code": "def convert_examples_to_features(examples, label_list, max_seq_length,\n                                 tokenizer)\n\t\n\n    features = []\n    for (ex_index, example) in enumerate(examples):\n        if ex_index % 10000 == 0:\n            tf.logging.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n\n        feature = convert_single_example(ex_index, example, label_list,\n                                         max_seq_length, tokenizer)\n\n        features.append(feature)\n    return features\n\n", "CodeBleu_score": 81.67315584602424}, {"code": "def convert_examples_to_features(examples, label_list, max_seq_length,\n                                 tokenizer)\n\t\n    features = []\n    for (ex_index, example) in enumerate(examples):\n        if ex_index % 10000 == 0:\n            tf.logging.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n\n        feature = convert_single_example(ex_index, example, label_list,\n                                         max_seq_length, tokenizer)\n\n        features.append(feature)\n    return features\n\n", "CodeBleu_score": 81.67315584602424}, {"code": "def convert_examples_to_features(examples, label_list, max_seq_length,\n                                 tokenizer)\n\t\n    features = []\n    for (ex_index, example) in enumerate(examples):\n        if ex_index % 10000 == 0:\n            tf.logging.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n\n        feature = convert_single_example(ex_index, example, label_list,\n                                         max_seq_length, tokenizer)\n\n        features.append(feature)\n    return features\n\n", "CodeBleu_score": 81.67315584602424}]}
{"instance_id": "e09c0db71ebb", "generation_result": [{"code": "def get_next_sentence_output(bert_config, input_tensor, labels)\n\t\n    # Simple binary classification. Note that 0 is \"next sentence\" and 1 is\n    # \"random sentence\". This weight matrix is not used after pre-training.\n    with tf.variable_scope(\"cls/seq_relationship\"):\n        output_weights = tf.get_variable(\n            \"output_weights\",\n            shape=[2, bert_config.hidden_size],\n            initializer=modeling.create_initializer(bert_config.initializer_range))\n        output_bias = tf.get_variable(\n            \"output_bias\", shape=[2], initializer=tf.zeros_initializer())\n\n        logits = tf.matmul(input_tensor, output_weights, transpose_b=True)\n        logits = tf.nn.bias_add(logits, output_bias)\n        log_probs = tf.nn.log_softmax(logits, axis=-1)\n        labels = tf.reshape(labels, [-1])\n        one_hot_labels = tf.one_hot(labels, depth=2, dtype=tf.float32)\n        per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n        loss = tf.reduce_mean(per_example_loss)\n        return (loss, per_example_loss, log_probs)\n\n", "CodeBleu_score": 86.94900271179796}, {"code": "def get_next_sentence_output(bert_config, input_tensor, labels)\n\t\n    # Simple binary classification. Note that 0 is \"next sentence\" and 1 is\n    # \"random sentence\". This weight matrix is not used after pre-training.\n    with tf.variable_scope(\"cls/seq_relationship\"):\n        output_weights = tf.get_variable(\n            \"output_weights\",\n            shape=[2, bert_config.hidden_size],\n            initializer=modeling.create_initializer(bert_config.initializer_range))\n        output_bias = tf.get_variable(\n            \"output_bias\", shape=[2], initializer=tf.zeros_initializer())\n\n        logits = tf.matmul(input_tensor, output_weights, transpose_b=True)\n        logits = tf.nn.bias_add(logits, output_bias)\n        log_probs = tf.nn.log_softmax(logits, axis=-1)\n        labels = tf.reshape(labels, [-1])\n        one_hot_labels = tf.one_hot(labels, depth=2, dtype=tf.float32)\n        per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n        loss = tf.reduce_mean(per_example_loss)\n        return (loss, per_example_loss, log_probs)\n\n", "CodeBleu_score": 86.94900271179796}, {"code": "def get_next_sentence_output(bert_config, input_tensor, labels)\n\t\n    # Simple binary classification. Note that 0 is \"next sentence\" and 1 is\n    # \"random sentence\". This weight matrix is not used after pre-training.\n    with tf.variable_scope(\\\"cls/seq_relationship\\\"):\n      output_weights = tf.get_variable(\n          \\\"output_weights\\\",\n          shape=[2, bert_config.hidden_size],\n          initializer=modeling.create_initializer(bert_config.initializer_range))\n      output_bias = tf.get_variable(\n          \\\"output_bias\\\",\n          shape=[2],\n          initializer=tf.zeros_initializer())\n\n      logits = tf.matmul(input_tensor, output_weights, transpose_b=True)\n      logits = tf.nn.bias_add(logits, output_bias)\n      log_probs = tf.nn.log_softmax(logits, axis=-1)\n\n      labels = tf.reshape(labels, [-1])\n      one_hot_labels = tf.one_hot(labels, depth=2, dtype=tf.float32)\n      per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n      loss = tf.reduce_mean(per_example_loss)\n\n  return (loss, per_example_loss, log_probs)\n\n\n\n\n", "CodeBleu_score": 80.57047219618023}]}
{"instance_id": "1f66b020f485", "generation_result": [{"code": "def gather_indexes(sequence_tensor, positions)\n\t\n    sequence_shape = modeling.get_shape_list(sequence_tensor, expected_rank=3)\n    batch_size = sequence_shape[0]\n    seq_length = sequence_shape[1]\n    width = sequence_shape[2]\n\n    flat_offsets = tf.reshape(tf.range(0, batch_size, dtype=tf.int32) * seq_length, [-1, 1])\n    flat_positions = tf.reshape(positions + flat_offsets, [-1])\n    flat_sequence_tensor = tf.reshape(sequence_tensor, [batch_size * seq_length, width])\n    output_tensor = tf.gather(flat_sequence_tensor, flat_positions)\n    return output_tensor\n\n", "CodeBleu_score": 55.18468812590758}, {"code": "def gather_indexes(sequence_tensor, positions)\n\t\n    sequence_shape = modeling.get_shape_list(sequence_tensor, expected_rank=3)\n    batch_size = sequence_shape[0]\n    seq_length = sequence_shape[1]\n    width = sequence_shape[2]\n\n    flat_offsets = tf.reshape(\n        tf.range(0, batch_size, dtype=tf.int32) * seq_length, [-1, 1])\n    flat_positions = tf.reshape(positions + flat_offsets, [-1])\n    flat_sequence_tensor = tf.reshape(sequence_tensor,\n                                      [batch_size * seq_length, width])\n    output_tensor = tf.gather(flat_sequence_tensor, flat_positions)\n    return output_tensor\n\n", "CodeBleu_score": 57.853066637478335}, {"code": "def gather_indexes(sequence_tensor, positions)\n\t\n    sequence_shape = modeling.get_shape_list(sequence_tensor, expected_rank=3)\n    batch_size = sequence_shape[0]\n    seq_length = sequence_shape[1]\n    width = sequence_shape[2]\n\n    flat_offsets = tf.reshape(tf.range(0, batch_size, dtype=tf.int32) * seq_length, [-1, 1])\n    flat_positions = tf.reshape(positions + flat_offsets, [-1])\n    flat_sequence_tensor = tf.reshape(sequence_tensor, [batch_size * seq_length, width])\n    output_tensor = tf.gather(flat_sequence_tensor, flat_positions)\n    return output_tensor\n\n", "CodeBleu_score": 55.18468812590758}]}
{"instance_id": "956bffee9a33", "generation_result": [{"code": "def _decode_record(record, name_to_features)\n\t\n    example = tf.parse_single_example(record, name_to_features)\n\n    # tf.Example only supports tf.int64, but the TPU only supports tf.int32.\n    # So cast all int64 to int32.\n    for name in list(example.keys()):\n        t = example[name]\n        if t.dtype == tf.int64:\n            t = tf.to_int32(t)\n        example[name] = t\n\n    return example\n\n", "CodeBleu_score": 70.7257133394932}, {"code": "def _decode_record(record, name_to_features)\n\t\n    example = tf.parse_single_example(record, name_to_features)\n\n    # tf.Example only supports tf.int64, but the TPU only supports tf.int32.\n    # So cast all int64 to int32.\n    for name in list(example.keys()):\n        t = example[name]\n        if t.dtype == tf.int64:\n            t = tf.to_int32(t)\n        example[name] = t\n\n    return example\n\n\n\n\n", "CodeBleu_score": 70.7257133394932}, {"code": "def _decode_record(record, name_to_features)\n\t\n    example = tf.parse_single_example(record, name_to_features)\n\n    # tf.Example only supports tf.int64, but the TPU only supports tf.int32.\n    # So cast all int64 to int32.\n    for name in list(example.keys()):\n        t = example[name]\n        if t.dtype == tf.int64:\n            t = tf.to_int32(t)\n        example[name] = t\n\n    return example\n\n\n\n\n", "CodeBleu_score": 70.7257133394932}]}
{"instance_id": "6b97f354e641", "generation_result": [{"code": "def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer,\n                         orig_answer_text)\n\t\n    tok_answer_text = \" \".join(tokenizer.tokenize(orig_answer_text))\n\n    for new_start in range(input_start, input_end + 1):\n      for new_end in range(input_end, new_start - 1, -1):\n        text_span = \\\" \".join(doc_tokens[new_start:(new_end + 1)])\n        if text_span == tok_answer_text:\n          return (new_start, new_end)\n\n    return (input_start, input_end)\n\n\n\n", "CodeBleu_score": 74.91812713327882}, {"code": "def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer,\n                         orig_answer_text)\n\t\n    tok_answer_text = \" \".join(tokenizer.tokenize(orig_answer_text))\n\n    for new_start in range(input_start, input_end + 1):\n        for new_end in range(input_end, new_start - 1, -1):\n            text_span = \" \".join(doc_tokens[new_start:(new_end + 1)])\n            if text_span == tok_answer_text:\n                return (new_start, new_end)\n\n    return (input_start, input_end)\n\n", "CodeBleu_score": 79.51593464740901}, {"code": "def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer,\n                         orig_answer_text)\n\t\n\n    # The SQuAD annotations are character based. We first project them to\n    # whitespace-tokenized words. But then after WordPiece tokenization, we can\n    # often find a \"better match\". For example:\n    #\n    #   Question: What year was <NAME> born?\n    #   Context: The leader was <NAME> (1895-1943).\n    #   Answer: 1895\n    #\n    # The original whitespace-tokenized answer will be \"(1895-1943).\". However\n    # after tokenization, our tokens will be \"( 1895 - 1943 ) .\". So we can match\n    # the exact answer, 1895.\n    #\n    # However, this is not always possible. Consider the following:\n    #\n    #   Question: What country is the top exporter of electornics?\n    #   Context: The Japanese electronics industry is the lagest in the world.\n    #   Answer: Japan\n    #\n    # In this case, the annotator chose \"Japan\" as a character sub-span of\n    # the word \"Japanese\". Since our WordPiece tokenizer does not split\n    # \"Japanese\", we just use \"Japanese\" as the annotation. This is fairly rare\n    # in SQuAD, but does happen.\n    tok_answer_text = \\\"\\\"\\\"\\\"\\\"\n    for (i, token) in enumerate(tokenizer.tokenize(orig_answer_text)):\n        tok_answer_text += token\n        if i < len(orig_answer_text) - 1:\n            tok_answer_text += \\\" \\\"\n    \\\"\\\"\\\"\n    for new_start in range(input_start, input_end + 1):\n      for new_end in range(input_end, new_start - 1, -1):\n        text_span = \\\"\\\".join(doc_tokens[new_start:(new_end + 1)])\n        if text_span == tok_answer_text:\n          return (new_start, new_end)\n\n    return (input_start, input_end)\n\n\n\n\n", "CodeBleu_score": 61.39402163086659}]}
{"instance_id": "aba334b92b34", "generation_result": [{"code": "def _check_is_max_context(doc_spans, cur_span_index, position)\n\t\n\n    best_score = None\n    best_span_index = None\n    for (span_index, doc_span) in enumerate(doc_spans):\n      end = doc_span.start + doc_span.length - 1\n      if position < doc_span.start:\n        continue\n      if position > end:\n        continue\n      num_left_context = position - doc_span.start\n      num_right_context = end - position\n      score = min(num_left_context, num_right_context) + 0.01 * doc_span.length\n      if best_score is None or score > best_score:\n        best_score = score\n        best_span_index = span_index\n\n    return cur_span_index == best_span_index\n\n\n\n\n", "CodeBleu_score": 79.40623652664495}, {"code": "def _check_is_max_context(doc_spans, cur_span_index, position)\n\t\n    best_score = None\n    best_span_index = None\n    for (span_index, doc_span) in enumerate(doc_spans):\n        end = doc_span.start + doc_span.length - 1\n        if position < doc_span.start:\n            continue\n        if position > end:\n            continue\n        num_left_context = position - doc_span.start\n        num_right_context = end - position\n        score = min(num_left_context, num_right_context) +\n               0.01 * doc_span.length\n        if best_score is None or score > best_score:\n            best_score = score\n            best_span_index = span_index\n\n    return cur_span_index == best_span_index\n\n\n\n\n", "CodeBleu_score": 77.6370141958573}, {"code": "def _check_is_max_context(doc_spans, cur_span_index, position)\n\t\n    best_score = None\n    best_span_index = None\n\n  for (span_index, doc_span) in enumerate(doc_spans):\n    end = doc_span.start + doc_span.length - 1\n    if position < doc_span.start:\n      continue\n    if position > end:\n      continue\n    num_left_context = position - doc_span.start\n    num_right_context = end - position\n    score = min(num_left_context, num_right_context) + 0.01 * doc_span.length\n    if best_score is None or score > best_score:\n      best_score = score\n      best_span_index = span_index\n\n  return cur_span_index == best_span_index\n\n\n\n\n", "CodeBleu_score": 80.52029076144164}]}
{"instance_id": "cf92ad509f6a", "generation_result": [{"code": "def load_vocab(vocab_file)\n\t\n    vocab = collections.OrderedDict()\n    index = 0\n    with tf.gfile.GFile(vocab_file, \\\"r\\\") as reader:\n      while True:\n        token = convert_to_unicode(reader.readline())\n        if not token:\n          break\n        token = token.strip()\n        vocab[token] = index\n        index += 1\n    return vocab\n\n\n\n\n", "CodeBleu_score": 70.61639164751475}, {"code": "def load_vocab(vocab_file)\n\t\n    vocab = collections.OrderedDict()\n    index = 0\n    with tf.gfile.GFile(vocab_file, \\\"r\\\") as reader:\n      while True:\n        token = convert_to_unicode(reader.readline())\n        if not token:\n          break\n        token = token.strip()\n        vocab[token] = index\n        index += 1\n    return vocab\n\n\n\n\n", "CodeBleu_score": 70.61639164751475}, {"code": "def load_vocab(vocab_file)\n\t\n    vocab = collections.OrderedDict()\n    index = 0\n    with tf.gfile.GFile(vocab_file, \\\"r\\\") as reader:\n      while True:\n        token = convert_to_unicode(reader.readline())\n        if not token:\n          break\n        token = token.strip()\n        vocab[token] = index\n        index += 1\n    return vocab\n\n\n\n\n", "CodeBleu_score": 70.61639164751475}]}
{"instance_id": "6c865803ac12", "generation_result": [{"code": "def convert_by_vocab(vocab, items)\n\t\n    output = []\n    for item in items:\n        output.append(vocab[item])\n    return output\n\n\n\n\n", "CodeBleu_score": 79.63749907165396}, {"code": "def convert_by_vocab(vocab, items)\n\t\n    output = []\n    for item in items:\n        output.append(vocab[item])\n    return output\n\n", "CodeBleu_score": 79.63749907165396}, {"code": "def convert_by_vocab(vocab, items)\n\t\n    output = []\n    for item in items:\n        output.append(vocab[item])\n    return output\n\n", "CodeBleu_score": 79.63749907165396}]}
{"instance_id": "e93b9200dc4a", "generation_result": [{"code": "def tokenize(self, text)\n\t\n    text = convert_to_unicode(text)\n    text = self.basic_tokenizer.tokenize(text)\n    text = self.wordpiece_tokenizer.tokenize(text)\n    return text\n\n", "CodeBleu_score": 18.778391728140804}, {"code": "def tokenize(self, text)\n\t\n    split_tokens = []\n    for token in self.basic_tokenizer.tokenize(text):\n        for sub_token in self.wordpiece_tokenizer.tokenize(token):\n            split_tokens.append(sub_token)\n\n    return split_tokens\n\n", "CodeBleu_score": 88.10762355418231}, {"code": "def tokenize(self, text)\n\t\n    text = convert_to_unicode(text)\n\n    # This was added on November 1st, 2018 for the multilingual and Chinese\n    # models. This is also applied to the English models now, but it doesn't\n    # matter since the English models were not trained on any Chinese data\n    # and generally don't have any Chinese data in them (there are Chinese\n    # characters in the vocabulary because Wikipedia does have some Chinese\n    # words in the English Wikipedia.).\n    text = self.basic_tokenizer.tokenize(text)\n\n    text = self.wordpiece_tokenizer.tokenize(text)\n\n    return text\n\n", "CodeBleu_score": 18.778391728140804}]}
{"instance_id": "2ff6f6db53c9", "generation_result": [{"code": "def safe_load(path, offset, duration, sample_rate, dtype)\n\t\n    try:\n        logger.debug(\n            \"Loading audio from `%s` from %s to %s with sample rate %s.\",\n            path,\n            offset,\n            offset + duration,\n            sample_rate,\n        )\n        audio_data, _ = tf.audio.decode_wav(\n            tf.io.read_file(path),\n            desired_channels=1,\n            desired_samples=int(duration * sample_rate),\n            desired_samples_per_channel=int(duration * sample_rate),\n            dtype=dtype,\n        )\n        audio_data = audio_data.numpy().flatten()\n        logger.debug(\"Audio data shape: %s.\", audio_data.shape)\n        return audio_data, False\n    except Exception as e:\n        logger.error(\n            \"An error occurs while loading audio from `%s`: %s.\", path, str(e)\n        )\n        return -1.0, True\n\n", "CodeBleu_score": 40.26133304796028}, {"code": "def safe_load(path, offset, duration, sample_rate, dtype)\n\t\n    # pylint: disable=broad-except\n    try:\n        logger.debug(\n            \"Loading audio from `%s` with offset=%s, duration=%s, sample_rate=%s, dtype=%s\",\n            path,\n            offset,\n            duration,\n            sample_rate,\n            dtype,\n        )\n        audio_data, _ = AudioAdapter.default().load(\n            path, offset, duration, sample_rate, dtype\n        )\n        return audio_data, False\n    except Exception as e:\n        logger.error(\n            \"An error occurs while loading audio from `%s` with offset=%s, duration=%s, sample_rate=%s, dtype=%s\",\n            path,\n            offset,\n            duration,\n            sample_rate,\n            dtype,\n        )\n        logger.debug(\"Error details: %s\", e)\n        return -1.0, True\n    # pylint: enable=broad-except\n\n", "CodeBleu_score": 37.42990946427287}, {"code": "def safe_load(path, offset, duration, sample_rate, dtype)\n\t\n    # pylint: disable=broad-except\n    try:\n        # Load audio data from file.\n        logger.debug(\n            \"Loading audio from file '%s' with offset=%s, duration=%s, \"\n            \"sample_rate=%s and dtype=%s\",\n            path,\n            offset,\n            duration,\n            sample_rate,\n            dtype,\n        )\n        data, _ = tf.audio.decode_wav(\n            tf.io.read_file(path),\n            desired_channels=1,\n            desired_samples=int(duration * sample_rate),\n            desired_rate=sample_rate,\n        )\n        # Cast data to target type.\n        data = tf.cast(data, dtype)\n        # Slice data to expected range.\n        data = data[offset * sample_rate : (offset + duration) * sample_rate]\n        # Return data and no error flag.\n        return data, False\n    except Exception as e:\n        # Log error and return default value.\n        logger.debug(\n            \"Unable to load audio from file '%s' with offset=%s, duration=%s, \"\n            \"sample_rate=%s and dtype=%s: %s\",\n            path,\n            offset,\n            duration,\n            sample_rate,\n            dtype,\n            str(e),\n        )\n        return tf.constant(-1.0, dtype), True\n    # pylint: enable=broad-except\n\n", "CodeBleu_score": 31.96785467427654}]}
{"instance_id": "12fbe1f03d8f", "generation_result": [{"code": "def to_n_channels(waveform: tf.Tensor, n_channels: int) -> tf.Tensor\n\tusr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:1097: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:1100: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:1103: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:1106: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:1109: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type", "CodeBleu_score": 27.480602234750673}, {"code": "def to_n_channels(waveform: tf.Tensor, n_channels: int) -> tf.Tensor\n\tusr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:1023:0\n\n    Args:\n        waveform (Tensor): Waveform tensor to adjust.\n        n_channels (int): Number of channels to adjust to.\n\n    Returns:\n        Tensor: Waveform tensor with adjusted channels.\n    \"\"\"\n    if n_channels == 1:\n        return tf.squeeze(waveform, axis=-1)\n    elif n_channels == 2:\n        return tf.tile(waveform, [1, 1, 2])\n    elif n_channels == 4:\n        return tf.tile(waveform, [1, 1, 4])\n    else:\n        return tf.tile(waveform, [1, 1, tf.math.maximum(n_channels, 1)])\n\n", "CodeBleu_score": 40.49761638177279}, {"code": "def to_n_channels(waveform: tf.Tensor, n_channels: int) -> tf.Tensor\n\tusr/spleeter/utils/audio.py\n\n    Args:\n        waveform (tf.Tensor): Waveform tensor to convert.\n        n_channels (int): Number of channels to convert to.\n\n    Returns:\n        tf.Tensor: Converted waveform tensor.\n    \"\"\"\n    if n_channels == waveform.shape[-1]:\n        return waveform\n    elif n_channels > waveform.shape[-1]:\n        return tf.tile(waveform, [1, n_channels // waveform.shape[-1]])[..., :n_channels]\n    else:\n        return waveform[..., :n_channels]\n\n", "CodeBleu_score": 41.73633125677517}]}
{"instance_id": "5e215f029b2e", "generation_result": [{"code": "def spectrogram_to_db_uint(\n    spectrogram: tf.Tensor, db_range: float = 100.0, **kwargs\n) -> tf.Tensor\n\t\n    return from_float32_to_uint8(\n        tf.clip_by_value(gain_to_db(spectrogram, **kwargs), -db_range, 0.0)\n    )\n\n", "CodeBleu_score": 27.770838165819534}, {"code": "def spectrogram_to_db_uint(\n    spectrogram: tf.Tensor, db_range: float = 100.0, **kwargs\n) -> tf.Tensor\n\t\n    db_spectrogram = gain_to_db(spectrogram)\n    db_spectrogram = tf.clip_by_value(db_spectrogram, -db_range, 0.0)\n    db_spectrogram = from_float32_to_uint8(db_spectrogram)\n    return db_spectrogram\n\n", "CodeBleu_score": 35.74507829171516}, {"code": "def spectrogram_to_db_uint(\n    spectrogram: tf.Tensor, db_range: float = 100.0, **kwargs\n) -> tf.Tensor\n\t\n    db_spectrogram = gain_to_db(spectrogram, **kwargs)\n    db_max = tf.reduce_max(db_spectrogram)\n    db_spectrogram = tf.clip_by_value(db_spectrogram, -db_range, db_max)\n    db_spectrogram = from_float32_to_uint8(db_spectrogram)\n    return db_spectrogram\n\n", "CodeBleu_score": 45.09048928357178}]}
{"instance_id": "b65c0968dae7", "generation_result": [{"code": "def compute_spectrogram_tf(\n    waveform: tf.Tensor,\n    frame_length: int = 2048,\n    frame_step: int = 512,\n    spec_exponent: float = 1.0,\n    window_exponent: float = 1.0,\n) -> tf.Tensor\n\t\n    # pylint: disable=too-many-arguments\n    # pylint: disable=too-many-locals\n    # pylint: disable=too-many-statements\n    # pylint: disable=too-many-branches\n    # pylint: disable=too-many-nested-blocks\n    # pylint: disable=too-many-return-statements\n    # pylint: disable=too-many-instance-attributes\n    # pylint: disable=too-many-arguments\n    # pylint: disable=too-many-locals\n    # pylint: disable=too-many-statements\n    # pylint: disable=too-many-branches\n    # pylint: disable=too-many-nested-blocks\n    # pylint: disable=too-many-return-statements\n    # pylint: disable=too-many-instance-attributes\n    # pylint: disable=too-many-arguments\n    # pylint: disable=too-many-locals\n    # pylint: disable=too-many-statements\n    # pylint: disable=too-many-branches\n    # pylint: disable=too-many-nested-blocks\n    # pylint: disable=too-many-return-statements\n    # pylint: disable=too-many-instance-attributes\n    # pylint: disable=too-many-arguments\n    # pylint: disable=too-many-locals\n    # pylint: disable=too-many-statements\n    # pylint: disable=too-many-branches\n    # pylint: disable=too-many-nested-blocks\n    # pylint: disable=too-many-return-statements\n    # pylint: disable=too-many-instance-attributes\n    # pylint: disable=too-many-arguments\n    # pylint: disable=too-many-locals\n    # pylint: disable=too-many-statements\n    # pylint: disable=too-many-branches\n    # pylint: disable=too-many-nested-blocks\n    # pylint: disable=too-many-return-statements\n    # pylint: disable=too-many-instance-attributes\n    # pylint: disable=too-many-arguments\n    # pylint: disable=too-many-locals\n    # pylint: disable=too-many-statements\n    # pylint: disable=too-many-branches\n    # pylint: disable=too-many-nested-blocks\n    # pylint: disable=too-many-return-statements\n    # pylint: disable=too-many-instance-attributes\n    # pylint: disable=too-many-arguments\n    # pylint", "CodeBleu_score": 17.449139280125195}, {"code": "def compute_spectrogram_tf(\n    waveform: tf.Tensor,\n    frame_length: int = 2048,\n    frame_step: int = 512,\n    spec_exponent: float = 1.0,\n    window_exponent: float = 1.0,\n) -> tf.Tensor\n\t\n    window_length = int(frame_length * 2 ** window_exponent)\n    window_step = int(frame_step * 2 ** window_exponent)\n    window = hann_window(window_length, periodic=True)\n    stft_matrix = stft(\n        waveform,\n        frame_length=frame_length,\n        frame_step=frame_step,\n        fft_length=frame_length,\n        window_fn=lambda _: window,\n    )\n    magnitude = tf.abs(stft_matrix)\n    magnitude = tf.pow(magnitude, spec_exponent)\n    return tf.transpose(magnitude)\n\n", "CodeBleu_score": 37.60244333279131}, {"code": "def compute_spectrogram_tf(\n    waveform: tf.Tensor,\n    frame_length: int = 2048,\n    frame_step: int = 512,\n    spec_exponent: float = 1.0,\n    window_exponent: float = 1.0,\n) -> tf.Tensor\n\t\n    # pylint: disable=too-many-locals\n    window = hann_window(frame_length, periodic=True) ** window_exponent\n    stft_matrix = stft(\n        waveform,\n        frame_length=frame_length,\n        frame_step=frame_step,\n        fft_length=frame_length,\n        window_fn=lambda _, __: window,\n        pad_end=True,\n    )\n    magnitude = tf.abs(stft_matrix)\n    magnitude = tf.pow(magnitude, spec_exponent)\n    magnitude = tf.transpose(magnitude)\n    return magnitude\n\n", "CodeBleu_score": 38.99700298383232}]}
{"instance_id": "a4686140a634", "generation_result": [{"code": "def time_stretch(\n    spectrogram: tf.Tensor,\n    factor: float = 1.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n    shape = tf.shape(spectrogram)\n    new_time = tf.cast(shape[0], tf.float32) * factor\n    new_time = tf.cast(tf.math.round(new_time), tf.int32)\n    return tf.image.resize(\n        spectrogram, [new_time, shape[1]], method=method\n    )[:shape[0], :]\n\n", "CodeBleu_score": 34.327308061599076}, {"code": "def time_stretch(\n    spectrogram: tf.Tensor,\n    factor: float = 1.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n    # calculate new time dimension\n    new_time_dim = tf.cast(\n        tf.cast(tf.shape(spectrogram)[0], tf.float32) * factor, tf.int32\n    )\n    # resize spectrogram\n    resized_spec = tf.image.resize(\n        spectrogram, [new_time_dim, tf.shape(spectrogram)[1]], method=method\n    )\n    # crop or pad spectrogram\n    resized_spec = tf.image.resize_with_crop_or_pad(\n        resized_spec, tf.shape(spectrogram)[0], tf.shape(spectrogram)[1]\n    )\n    return resized_spec\n\n", "CodeBleu_score": 40.680500729024445}, {"code": "def time_stretch(\n    spectrogram: tf.Tensor,\n    factor: float = 1.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n    # Calculate the new time dimension.\n    new_time_dim = tf.cast(\n        tf.math.round(tf.shape(spectrogram)[0] * factor), tf.int32\n    )\n    # Resize spectrogram.\n    resized_spec = tf.image.resize(\n        spectrogram,\n        size=[new_time_dim, tf.shape(spectrogram)[1]],\n        method=method,\n    )\n    # Adjust dimensions to original spectrogram.\n    return tf.image.resize_with_crop_or_pad(\n        resized_spec,\n        target_height=tf.shape(spectrogram)[0],\n        target_width=tf.shape(spectrogram)[1],\n    )\n\n", "CodeBleu_score": 36.7428948884733}]}
{"instance_id": "6dd01101d4ae", "generation_result": [{"code": "def pitch_shift(\n    spectrogram: tf.Tensor,\n    semitone_shift: float = 0.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n    F = tf.shape(spectrogram)[1]\n    F_ps = tf.cast(tf.cast(F, tf.float32) * 2 ** semitone_shift, tf.int32)[0]\n    ps_spec = tf.image.resize_images(\n        spectrogram, [tf.shape(spectrogram)[0], F_ps], method=method, align_corners=True\n    )\n    return tf.image.resize_image_with_crop_or_pad(ps_spec, tf.shape(spectrogram)[0], F)\n\n", "CodeBleu_score": 40.97274644930999}, {"code": "def pitch_shift(\n    spectrogram: tf.Tensor,\n    semitone_shift: float = 0.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n    F = tf.shape(spectrogram)[1]\n    F_ps = tf.cast(\n        tf.cast(F, tf.float32) * tf.exp(semitone_shift * 1 / 12), tf.int32\n    )[0]\n    ps_spec = tf.image.resize_images(\n        spectrogram, [tf.shape(spectrogram)[0], F_ps], method=method, align_corners=True\n    )\n    return tf.image.resize_image_with_crop_or_pad(ps_spec, tf.shape(spectrogram)[0], F)\n\n", "CodeBleu_score": 39.640622976155505}, {"code": "def pitch_shift(\n    spectrogram: tf.Tensor,\n    semitone_shift: float = 0.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\t\n    F = tf.shape(spectrogram)[1]\n    F_ps = tf.cast(tf.cast(F, tf.float32) * (2 ** semitone_shift), tf.int32)[0]\n    ps_spec = tf.image.resize_images(\n        spectrogram, [tf.shape(spectrogram)[0], F_ps], method=method, align_corners=True\n    )\n    return tf.image.resize_image_with_crop_or_pad(ps_spec, tf.shape(spectrogram)[0], F)\n\n", "CodeBleu_score": 40.80671091752476}]}
{"instance_id": "612f0826e56a", "generation_result": [{"code": "def _get_conv_activation_layer(params: Dict) -> Any\n\t\n    if params[\"conv_activation\"] == \"relu\":\n        return ReLU()\n    if params[\"conv_activation\"] == \"elu\":\n        return ELU()\n    return LeakyReLU(alpha=0.2)\n\n", "CodeBleu_score": 36.41752111104844}, {"code": "def _get_conv_activation_layer(params: Dict) -> Any\n\t\n    conv_activation = params.get(\"conv_activation\", \"relu\")\n    if conv_activation == \"relu\":\n        return ReLU()\n    if conv_activation == \"elu\":\n        return ELU()\n    if conv_activation == \"leaky_relu\":\n        return LeakyReLU(alpha=0.2)\n    raise ValueError(\n        f\"conv_activation should be one of relu, elu, leaky_relu, not {conv_activation}\"\n    )\n\n", "CodeBleu_score": 36.904929244189546}, {"code": "def _get_conv_activation_layer(params: Dict) -> Any\n\t\n    conv_activation = params.get(\"conv_activation\", \"relu\")\n    if conv_activation == \"relu\":\n        return ReLU()\n    if conv_activation == \"elu\":\n        return ELU()\n    if conv_activation == \"leaky_relu\":\n        return LeakyReLU(alpha=0.2)\n    raise ValueError(\n        f\"Unsupported convolutional activation layer: {conv_activation}\"\n    )\n\n", "CodeBleu_score": 37.4584296887481}]}
{"instance_id": "2f8b5a2fc0f2", "generation_result": [{"code": "def softmax_unet(\n    input_tensor: tf.Tensor, instruments: Iterable[str], params: Dict = {}\n) -> Dict\n\t\n    return apply(apply_softmax_unet, input_tensor, instruments, params)\n\n", "CodeBleu_score": 9.447847813973981}, {"code": "def softmax_unet(\n    input_tensor: tf.Tensor, instruments: Iterable[str], params: Dict = {}\n) -> Dict\n\t\n    output_masks = apply(apply_unet, input_tensor, instruments, params, True)\n    output_masks = tf.stack(output_masks, axis=1)\n    output_masks = tf.nn.softmax(output_masks, axis=1)\n    output_masks = tf.unstack(output_masks, axis=1)\n    output_masks = {\n        instrument: mask for instrument, mask in zip(instruments, output_masks)\n    }\n    output_spectrograms = {\n        instrument: Multiply(name=instrument)([mask, input_tensor])\n        for instrument, mask in output_masks.items()\n    }\n    return output_spectrograms\n\n", "CodeBleu_score": 34.79090921699469}, {"code": "def softmax_unet(\n    input_tensor: tf.Tensor, instruments: Iterable[str], params: Dict = {}\n) -> Dict\n\t\n    logging.info(\"Applying softmax U-Net model.\")\n    output_dict = {}\n    for instrument in instruments:\n        output_dict[instrument] = apply_unet(\n            input_tensor, instrument, params, output_mask_logit=True\n        )\n    # Combine logit masks using a softmax function along the instrument axis.\n    output_tensor = tf.reduce_sum(\n        tf.stack([output_dict[instrument] for instrument in instruments], axis=-1),\n        axis=-1,\n    )\n    output_tensor = tf.nn.softmax(output_tensor, axis=-1)\n    # Create an output dictionary where each entry is a spectrogram for an instrument,\n    # obtained by multiplying the corresponding mask with the input tensor.\n    for instrument in instruments:\n        output_dict[instrument] = Multiply(\n            name=instrument, trainable=False, trainable_inverse=False\n        )([output_dict[instrument], input_tensor])\n    # Multiply the output tensor with the input tensor to get the final output.\n    output_dict[\"output\"] = Multiply(\n        name=\"output\", trainable=False, trainable_inverse=False\n    )([output_tensor, input_tensor])\n    return output_dict\n\n", "CodeBleu_score": 39.97957505797884}]}
{"instance_id": "400b19ce8520", "generation_result": [{"code": "def compute_file_checksum(path)\n\t\n    # pylint: disable=invalid-name\n    BLOCKSIZE = 65536\n    hasher = hashlib.sha256()\n    with open(path, \"rb\") as file:\n        buf = file.read(BLOCKSIZE)\n        while len(buf) > 0:\n            hasher.update(buf)\n            buf = file.read(BLOCKSIZE)\n    return hasher.hexdigest()\n\n", "CodeBleu_score": 47.22825867067571}, {"code": "def compute_file_checksum(path)\n\t\n    sha256 = hashlib.sha256()\n    with open(path, \"rb\") as file:\n        while True:\n            chunk = file.read(8192)\n            if not chunk:\n                break\n            sha256.update(chunk)\n    return sha256.hexdigest()\n\n", "CodeBleu_score": 44.03998652189755}, {"code": "def compute_file_checksum(path)\n\t\n    hash_ = hashlib.sha256()\n    with open(path, \"rb\") as stream:\n        for chunk in iter(lambda: stream.read(4096), b\"\"):\n            hash_.update(chunk)\n    return hash_.hexdigest()\n\n", "CodeBleu_score": 65.05882380907127}]}
{"instance_id": "5f2e9c6e15f1", "generation_result": [{"code": "def checksum(self, name: str) -> str\n\t\n    url = f\"{self._host}/{self._repository}/{self.RELEASE_PATH}/{self._release}/{self.CHECKSUM_INDEX}\"\n    response = httpx.get(url)\n    if response.status_code != 200:\n        raise IOError(f\"Unable to retrieve checksum index from {url}.\")\n    checksums = response.json()\n    if name not in checksums:\n        raise ValueError(f\"No checksum found for model \\\"{name}\\\".\")\n    return checksums[name]\n\n", "CodeBleu_score": 41.006652980775456}, {"code": "def checksum(self, name: str) -> str\n\t\n    url = f\"{self._host}/{self._repository}/{self.RELEASE_PATH}/{self._release}/{self.CHECKSUM_INDEX}\"\n    response = httpx.get(url)\n    if response.status_code != httpx.codes.OK:\n        raise IOError(f\"Cannot retrieve checksum for model {name}: {response.text}\")\n    checksums = response.json()\n    if name not in checksums:\n        raise ValueError(f\"Unknown model name: {name}\")\n    return checksums[name]\n\n", "CodeBleu_score": 44.0746352687333}, {"code": "def checksum(self, name: str) -> str\n\t\n    url = f\"{self._host}/{self._repository}/{self.RELEASE_PATH}/{self._release}/{self.CHECKSUM_INDEX}\"\n    response = httpx.get(url)\n    if response.status_code != 200:\n        raise RuntimeError(\n            f\"Unable to retrieve checksum for {name} from {url} (status code: {response.status_code}).\"\n        )\n    checksums = response.json()\n    if name not in checksums:\n        raise ValueError(\n            f\"Unable to find checksum for {name} in {url} (available names: {list(checksums.keys())}).\"\n        )\n    return checksums[name]\n\n", "CodeBleu_score": 38.782915061253256}]}
{"instance_id": "9312c8b6032e", "generation_result": [{"code": "def download(self, name: str, path: str) -> None:\n\t\n    # pylint: disable=import-outside-toplevel\n    from ...utils.logging import get_logger\n    from ...utils.configuration import get_configuration\n    from ...utils.archive import checksum, extract\n    from ...utils.tensorflow import configure_tensorflow\n    from ...utils.exceptions import SpleeterError\n    from . import ModelProvider\n    from urllib.parse import urlparse\n    import hashlib\n    import httpx\n    import os\n    import tarfile\n    import tempfile\n    import zipfile\n    import shutil\n    import json\n    import re\n    import sys\n    import io\n    import contextlib\n    import importlib\n    import importlib.util\n    import pkg_resources\n    import requests\n    import urllib.request\n    import urllib.error\n    import urllib.parse\n    import pathlib\n    import numpy as np\n    import tensorflow as tf\n    from typing import Dict, Any, List, Tuple, Union, Optional\n    from pathlib import Path\n    from urllib.parse import urlparse\n    from urllib.request import urlopen\n    from urllib.error import HTTPError\n    from urllib.request import urlretrieve\n    from urllib.parse import urljoin\n    from tqdm import tqdm\n    from tqdm.contrib.concurrent import process_map\n    from tqdm.contrib.concurrent import thread_map\n    from tqdm.contrib.concurrent import thread_map\n    from tqdm.contrib.concurrent import process_map\n    from tqdm.contrib.concurrent import thread_map\n    from tqdm.contrib.concurrent import process_map\n    from tqdm.contrib.concurrent import thread_map\n    from tqdm.contrib.concurrent import process_map\n    from tqdm.contrib.concurrent import thread_map\n    from tqdm.contrib.concurrent import process_map\n    from tqdm.contrib.concurrent import thread_map\n    from tqdm.contrib.concurrent import process_map\n    from tqdm.contrib.concurrent import thread_map\n    from tqdm.contrib.concurrent import process_map\n    from tqdm.contrib.concurrent import thread_map\n    from tqdm.contrib.concurrent import process_map\n    from tqdm.contrib.concurrent import thread_map\n    from tqdm.contrib.concurrent import process_map\n    from tqdm.contrib.concurrent import process_map\n    from tqdm.contrib.concurrent import thread_map\n    from tqdm.contrib.concurrent import process_map\n    from tqdm.contrib.concurrent import process_map\n    from tqdm.contrib.concurrent import thread_map\n    from tqdm.contrib.concurrent import process_map\n    from tqdm.contrib.concurrent import process_map\n    from tqdm.contrib.concurrent import thread_map\n   ", "CodeBleu_score": 13.737372147372277}, {"code": "def download(self, name: str, path: str) -> None:\n\t\n    url: str = \\\"/\\\".join(\n        (\n            self._host,\n            self._repository,\n            self.RELEASE_PATH,\n            self._release,\n            name,\n        )\n    )\n    logger.info(\\\"Downloading model from %s\\\", url)\n    with NamedTemporaryFile(\\\"wb\\\", delete=False) as stream:\n        with httpx.stream(\\\"GET\\\", url) as response:\n            response.raise_for_status()\n            for data in response.iter_bytes():\n                stream.write(data)\n        logger.info(\\\"Validating model checksum\\\")\n        checksum: str = compute_file_checksum(stream.name)\n        if checksum != self.checksum(name):\n            raise RuntimeError(\\\"Downloaded model checksum does not match\\\")\n        logger.info(\\\"Extracting model to %s\\\", path)\n        with tarfile.open(stream.name) as archive:\n            archive.extractall(path)\n        os.remove(stream.name)\n    logger.info(\\\"Model downloaded and verified\\\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 45.58216889673048}, {"code": "def download(self, name: str, path: str) -> None:\n\t\n    # pylint: disable=import-outside-toplevel\n    import tarfile\n    import tempfile\n    from os import makedirs\n    from os.path import exists, join\n    from urllib.parse import urlparse\n    from zipfile import ZipFile\n\n    # pylint: enable=import-outside-toplevel\n    url = self._get_download_url(name)\n    logger.info(f\"Downloading {name} model from {url}\")\n    with tempfile.NamedTemporaryFile() as file:\n        with httpx.stream(\"GET\", url) as response:\n            response.raise_for_status()\n            for chunk in response.iter_bytes():\n                file.write(chunk)\n        file.flush()\n        checksum = compute_file_checksum(file.name)\n        if checksum != self.checksum(name):\n            raise RuntimeError(\n                f\"Checksum mismatch for downloaded model {name}. \"\n                f\"Expected: {self.checksum(name)}, got: {checksum}\"\n            )\n        logger.info(f\"Validating {name} model checksum...\")\n        if name.endswith(\".tar.gz\"):\n            with tarfile.open(file.name, \"r:gz\") as tar:\n                tar.extractall(path)\n        elif name.endswith(\".zip\"):\n            with ZipFile(file.name, \"r\") as zip:\n                zip.extractall(path)\n        else:\n            raise RuntimeError(f\"Unsupported archive format for model {name}\")\n    logger.info(f\"Extraction completed for {name} model.\")\n\n", "CodeBleu_score": 45.01538345284114}]}
{"instance_id": "ee9493f37d77", "generation_result": [{"code": "def load_configuration(descriptor: str) -> Dict\n\t\n    if descriptor.startswith(_EMBEDDED_CONFIGURATION_PREFIX):\n        name = descriptor[len(_EMBEDDED_CONFIGURATION_PREFIX):]\n        with loader.open_text(resources, name + \".json\") as file:\n            return json.load(file)\n    elif exists(descriptor):\n        with open(descriptor, \"r\") as file:\n            return json.load(file)\n    else:\n        raise SpleeterError(f\"Configuration file {descriptor} not found.\")\n", "CodeBleu_score": 55.98561277894388}, {"code": "def load_configuration(descriptor: str) -> Dict\n\t\n    if descriptor.startswith(_EMBEDDED_CONFIGURATION_PREFIX):\n        configuration_name = descriptor[len(_EMBEDDED_CONFIGURATION_PREFIX) :]\n        try:\n            with loader.open_text(resources, configuration_name + \".json\") as stream:\n                return json.load(stream)\n        except FileNotFoundError as error:\n            raise SpleeterError(\n                f\"Configuration {descriptor} not found.\"\n            ) from error\n    else:\n        if exists(descriptor):\n            with open(descriptor, \"r\") as stream:\n                return json.load(stream)\n        else:\n            raise SpleeterError(\n                f\"Configuration {descriptor} not found.\"\n            )\n\n", "CodeBleu_score": 51.804224524077384}, {"code": "def load_configuration(descriptor: str) -> Dict\n\t\n    if descriptor.startswith(_EMBEDDED_CONFIGURATION_PREFIX):\n        name = descriptor[len(_EMBEDDED_CONFIGURATION_PREFIX) :]\n        try:\n            with loader.open_text(resources, name + \".json\") as stream:\n                return json.load(stream)\n        except FileNotFoundError:\n            raise SpleeterError(\n                f\"Embedded configuration {descriptor} not found.\"\n            ) from None\n    else:\n        if exists(descriptor):\n            with open(descriptor) as stream:\n                return json.load(stream)\n        else:\n            raise SpleeterError(\n                f\"Configuration file {descriptor} not found.\"\n            ) from None\n\n", "CodeBleu_score": 51.5877494790968}]}
{"instance_id": "546134b67fbc", "generation_result": [{"code": "def sync_apply(\n    tensor_dict: Dict[str, tf.Tensor], func: Callable, concat_axis: int = 1\n) -> Dict[str, tf.Tensor]\n\t\n    if concat_axis not in (0, 1):\n        raise ValueError(\n            f\"Invalid concatenation axis: {concat_axis}.", "CodeBleu_score": 12.241488029308318}, {"code": "def sync_apply(\n    tensor_dict: Dict[str, tf.Tensor], func: Callable, concat_axis: int = 1\n) -> Dict[str, tf.Tensor]\n\t\n    if concat_axis not in [0, 1]:\n        raise ValueError(\"concat_axis must be either 0 or 1\")\n    # get tensors\n    tensors = list(tensor_dict.values())\n    # concatenate\n    tensor = tf.concat(tensors, axis=concat_axis)\n    # apply function\n    tensor = func(tensor)\n    # split\n    split_tensors = tf.split(tensor, len(tensors), axis=concat_axis)\n    # zip\n    zipped_tensors = zip(tensor_dict.keys(), split_tensors)\n    # return\n    return {key: tensor for key, tensor in zipped_tensors}\n\n", "CodeBleu_score": 37.11310056221339}, {"code": "def sync_apply(\n    tensor_dict: Dict[str, tf.Tensor], func: Callable, concat_axis: int = 1\n) -> Dict[str, tf.Tensor]\n\t\n    if concat_axis not in (0, 1):\n        raise ValueError(\"`concat_axis` must be either 0 or 1.\")\n    concat_tensors = tf.concat(\n        [tensor_dict[key] for key in sorted(tensor_dict.keys())], axis=concat_axis\n    )\n    processed_tensor = func(concat_tensors)\n    if concat_axis == 0:\n        processed_tensor = tf.split(processed_tensor, len(tensor_dict), axis=0)\n    else:\n        processed_tensor = tf.split(processed_tensor, len(tensor_dict), axis=1)\n    return {\n        key: processed_tensor[i] for i, key in enumerate(sorted(tensor_dict.keys()))\n    }\n\n", "CodeBleu_score": 43.602464313265585}]}
{"instance_id": "df48e78d46d2", "generation_result": [{"code": "def from_float32_to_uint8(\n    tensor: tf.Tensor,\n    tensor_key: str = \"tensor\",\n    min_key: str = \"min\",\n    max_key: str = \"max\",\n) -> tf.Tensor\n\t\n    tensor_min = tf.reduce_min(tensor)\n    tensor_max = tf.reduce_max(tensor)\n    tensor_range = tensor_max - tensor_min\n    return {\n        tensor_key: tf.cast(\n            tf.math.divide(\n                tf.math.multiply(\n                    tf.cast(tensor - tensor_min, tf.float32),\n                    255.0 / tf.cast(tensor_range, tf.float32),\n                ),\n                1.0,\n            ),\n            tf.uint8,\n        ),\n        min_key: tensor_min,\n        max_key: tensor_max,\n    }\n\n", "CodeBleu_score": 57.75612135937258}, {"code": "def from_float32_to_uint8(\n    tensor: tf.Tensor,\n    tensor_key: str = \"tensor\",\n    min_key: str = \"min\",\n    max_key: str = \"max\",\n) -> tf.Tensor\n\t\n    tensor_shape = tf.shape(tensor)\n    tensor_min = tf.reduce_min(tensor)\n    tensor_max = tf.reduce_max(tensor)\n    tensor = tf.cast(\n        tf.round(\n            tf.cast(tensor - tensor_min, tf.float32)\n            * tf.constant(255.0, dtype=tf.float32)\n            / (tensor_max - tensor_min)\n        ),\n        tf.uint8,\n    )\n    return {\n        tensor_key: tensor,\n        min_key: tensor_min,\n        max_key: tensor_max,\n    }\n\n", "CodeBleu_score": 55.17106004406781}, {"code": "def from_float32_to_uint8(\n    tensor: tf.Tensor,\n    tensor_key: str = \"tensor\",\n    min_key: str = \"min\",\n    max_key: str = \"max\",\n) -> tf.Tensor\n\t\n    min_value = tf.reduce_min(tensor)\n    max_value = tf.reduce_max(tensor)\n    return {\n        tensor_key: tf.cast(\n            tf.math.divide(tf.math.multiply(tensor - min_value, 255.0), max_value - min_value),\n            tf.uint8,\n        ),\n        min_key: min_value,\n        max_key: max_value,\n    }\n\n", "CodeBleu_score": 49.976265226131}]}
{"instance_id": "7f479e5de492", "generation_result": [{"code": "def pad_and_partition(tensor: tf.Tensor, segment_len: int) -> tf.Tensor\n\t\n    pad_len = segment_len - tf.math.mod(tf.shape(tensor)[0], segment_len)\n    padded_tensor = tf.pad(tensor, [[0, pad_len], [0, 0]])\n    return tf.reshape(padded_tensor, [-1, segment_len, tensor.shape[1]])\n\n", "CodeBleu_score": 24.03444329055844}, {"code": "def pad_and_partition(tensor: tf.Tensor, segment_len: int) -> tf.Tensor\n\t\n    pad_len = tf.math.floormod(tf.shape(tensor)[0], segment_len)\n    padded_tensor = tf.pad(tensor, [[0, pad_len], [0, 0]])\n    return tf.reshape(padded_tensor, [-1, segment_len, tensor.shape[1]])\n\n", "CodeBleu_score": 22.49986091461671}, {"code": "def pad_and_partition(tensor: tf.Tensor, segment_len: int) -> tf.Tensor\n\t\n    # Calculate the number of segments\n    num_segments = tf.cast(tf.math.ceil(tf.shape(tensor)[0] / segment_len), tf.int32)\n    # Calculate the padding length\n    padding_len = tf.cast(num_segments * segment_len - tf.shape(tensor)[0], tf.int32)\n    # Pad the tensor\n    padded_tensor = tf.pad(tensor, [[0, padding_len], [0, 0]])\n    # Reshape the tensor\n    return tf.reshape(padded_tensor, [num_segments, segment_len, tf.shape(tensor)[1]])\n\n", "CodeBleu_score": 32.27846390083473}]}
{"instance_id": "1789fae7253f", "generation_result": [{"code": "def pad_and_reshape(instr_spec, frame_length, F) -> Any\n\t\n    # Calculate the number of rows to pad\n    num_rows_to_pad = frame_length - tf.math.floormod(tf.shape(instr_spec)[0], frame_length)\n    # Pad the input spectrogram\n    padded_spec = tf.pad(instr_spec, [[0, num_rows_to_pad], [0, 0]])\n    # Calculate the number of frequency bins after padding\n    num_freq_bins_padded = tf.shape(padded_spec)[1]\n    # Calculate the number of frequency bins after reshaping\n    num_freq_bins_reshaped = tf.math.floordiv(num_freq_bins_padded, frame_length) * F\n    # Reshape the input spectrogram\n    reshaped_spec = tf.reshape(padded_spec, [-1, num_freq_bins_reshaped, frame_length])\n    return reshaped_spec\n\n", "CodeBleu_score": 19.195571567569925}, {"code": "def pad_and_reshape(instr_spec, frame_length, F) -> Any\n\t\n    # Pad the spectrogram to the specified frame length\n    pad_size = tf.math.floormod(frame_length - instr_spec.shape[0], frame_length)\n    padded = tf.pad(instr_spec, [[0, pad_size]] + [[0, 0]] * (len(instr_spec.shape) - 1))\n    # Calculate the number of rows that will be merged during reshape\n    split = (padded.shape[0] + frame_length - 1) // frame_length\n    # Reshape the spectrogram\n    return tf.reshape(\n        padded, tf.concat([[split, frame_length], padded.shape[1:]], axis=0)\n    )\n\n", "CodeBleu_score": 16.61597867571093}, {"code": "def pad_and_reshape(instr_spec, frame_length, F) -> Any\n\t\n    pad_size = tf.math.floormod(frame_length - tf.shape(instr_spec)[0], frame_length)\n    padded = tf.pad(instr_spec, [[0, pad_size]] + [[0, 0]] * (len(instr_spec.shape) - 1))\n    return tf.reshape(\n        padded, tf.concat([[tf.shape(padded)[0] // frame_length, frame_length, F], [1]], axis=0)\n    )\n\n", "CodeBleu_score": 16.387409666656698}]}
{"instance_id": "8a4f4658d97c", "generation_result": [{"code": "def dataset_from_csv(csv_path: str, **kwargs) -> Any\n\t\n    df = pd.read_csv(csv_path, **kwargs)\n    return tf.data.Dataset.from_tensor_slices(df.values)\n\n", "CodeBleu_score": 29.79569440659192}, {"code": "def dataset_from_csv(csv_path: str, **kwargs) -> Any\n\t\n    df = pd.read_csv(csv_path, **kwargs)\n    return tf.data.Dataset.from_tensor_slices(\n        (\n            tf.convert_to_tensor(df[column].values, dtype=tf.float32)\n            for column in df.columns\n        )\n    )\n\n", "CodeBleu_score": 33.87222295092572}, {"code": "def dataset_from_csv(csv_path: str, **kwargs) -> Any\n\t\n    return tf.data.Dataset.from_tensor_slices(pd.read_csv(csv_path).to_dict(\"list\"))\n\n", "CodeBleu_score": 17.698805546157065}]}
{"instance_id": "9a16e81b182f", "generation_result": [{"code": "def check_tensor_shape(tensor_tf: tf.Tensor, target_shape: Any) -> bool\n\t\n    result = True\n    for dim_target, dim_tensor in zip(target_shape, tensor_tf.shape):\n        if dim_target is not None:\n            result = result and (dim_tensor == dim_target)\n    return result\n\n", "CodeBleu_score": 26.64839266964327}, {"code": "def check_tensor_shape(tensor_tf: tf.Tensor, target_shape: Any) -> bool\n\t\n    result = True\n    for index, dim in enumerate(target_shape):\n        if dim is not None:\n            result = result & (tensor_tf.shape[index] == dim)\n    return result\n\n", "CodeBleu_score": 30.609308727836048}, {"code": "def check_tensor_shape(tensor_tf: tf.Tensor, target_shape: Any) -> bool\n\t\n    result = True\n    for dim_tf, dim_target in zip(tensor_tf.shape, target_shape):\n        if dim_target is not None:\n            result = result and (dim_tf == dim_target)\n    return result\n\n", "CodeBleu_score": 26.80328971521676}]}
{"instance_id": "49d5d3084b3a", "generation_result": [{"code": "def get_training_dataset(\n    audio_params: Dict, audio_adapter: AudioAdapter, audio_path: str\n) -> Any\n\t\n    logger.info(\"Building training dataset.\")\n    builder = DatasetBuilder(\n        audio_params, audio_adapter, audio_path, is_validation=False\n    )\n    return builder.build(\n        train_csv=audio_params[\"train_csv\"],\n        cache_dir=audio_params[\"cache_dir\"],\n        batch_size=audio_params[\"batch_size\"],\n        num_chunks=audio_params[\"num_chunks_per_song\"],\n        augmentation_params=audio_params[\"augmentation_params\"],\n        cache=audio_params[\"cache\"],\n    )\n\n", "CodeBleu_score": 45.183990415400274}, {"code": "def get_training_dataset(\n    audio_params: Dict, audio_adapter: AudioAdapter, audio_path: str\n) -> Any\n\t\n    builder = DatasetBuilder(\n        audio_params,\n        audio_adapter,\n        audio_path,\n        chunk_duration=3.0,\n        random_seed=0,\n    )\n    return builder.build(\n        train_csv_path=f\"{audio_path}{SEPARATOR}train.csv\",\n        cache_dir=f\"{audio_path}{SEPARATOR}cache\",\n        batch_size=16,\n        num_chunks_per_song=10,\n        cache=True,\n        shuffle=True,\n        shuffle_buffer_size=2048,\n        prefetch_buffer_size=tf.data.experimental.AUTOTUNE,\n        parallel=True,\n        repeat=True,\n        augment=True,\n    )\n\n", "CodeBleu_score": 39.19269473314137}, {"code": "def get_training_dataset(\n    audio_params: Dict, audio_adapter: AudioAdapter, audio_path: str\n) -> Any\n\t\n    # pylint: disable=too-many-locals\n    logger.debug(\"Building training dataset.\")\n    builder = DatasetBuilder(\n        audio_params,\n        audio_adapter,\n        audio_path,\n        chunk_duration=1.0,\n        random_seed=42,\n    )\n    return builder.build(\n        train_csv=audio_params[\"train_csv\"],\n        cache_dir=audio_params[\"cache_dir\"],\n        batch_size=audio_params[\"batch_size\"],\n        num_chunks_per_song=audio_params[\"num_chunks_per_song\"],\n        shuffle=True,\n        shuffle_buffer_size=audio_params[\"shuffle_buffer_size\"],\n        prefetch_buffer_size=audio_params[\"prefetch_buffer_size\"],\n        cache=audio_params[\"cache\"],\n        augment=audio_params[\"augment\"],\n    )\n\n", "CodeBleu_score": 45.80750808971527}]}
{"instance_id": "fe8bbbba06e3", "generation_result": [{"code": "def instruments(self) -> Any\n\t\n    if self._instrument_builders is None:\n        self._instrument_builders = [\n            InstrumentDatasetBuilder(\n                instrument=instrument,\n                audio_params=self._audio_params,\n                audio_adapter=self._audio_adapter,\n                audio_path=self._audio_path,\n                random_seed=self._random_seed,\n                chunk_duration=self._chunk_duration,\n            )\n            for instrument in self._instruments\n        ]\n    for instrument_builder in self._instrument_builders:\n        yield instrument_builder\n\n", "CodeBleu_score": 37.821324297463626}, {"code": "def instruments(self) -> Any\n\t\n    if self._instrument_builders is None:\n        self._instrument_builders = []\n        for instrument in self._instruments:\n            self._instrument_builders.append(\n                InstrumentDatasetBuilder(\n                    self._audio_adapter,\n                    self._audio_params,\n                    self._audio_path,\n                    instrument,\n                    self._random_seed,\n                    self._chunk_duration,\n                )\n            )\n    for instrument_builder in self._instrument_builders:\n        yield instrument_builder\n\n    @property\n\n\n", "CodeBleu_score": 53.478314916875306}, {"code": "def instruments(self) -> Any\n\t\n    if self._instrument_builders is None:\n        self._instrument_builders = []\n        for instrument in self._instruments:\n            self._instrument_builders.append(\n                InstrumentDatasetBuilder(\n                    audio_params=self._audio_params,\n                    audio_adapter=self._audio_adapter,\n                    audio_path=self._audio_path,\n                    random_seed=self._random_seed,\n                    chunk_duration=self._chunk_duration,\n                    instrument=instrument,\n                )\n            )\n    for instrument_builder in self._instrument_builders:\n        yield instrument_builder\n\n", "CodeBleu_score": 52.21839738224754}]}
{"instance_id": "438b57fd3413", "generation_result": [{"code": "def cache(self, dataset: Any, cache: Optional[str], wait: bool) -> Any\n\t\n    if cache is None:\n        return dataset\n    else:\n        if wait:\n            while not tf.io.gfile.exists(cache + INDEX_FILE_SUFFIX):\n                logging.info(\n                    f\\\"Cache file not found, waiting for {self.cache_wait_time} seconds...\\\"\n                )\n                time.sleep(self.cache_wait_time)\n        logging.info(f\\\"Caching dataset to {cache}...\\\")\n        tf.io.gfile.makedirs(os.path.dirname(cache))\n        dataset.cache(cache)\n        return dataset\n\n", "CodeBleu_score": 40.56937367213448}, {"code": "def cache(self, dataset: Any, cache: Optional[str], wait: bool) -> Any\n\t\n    if cache is None:\n        return dataset\n    if wait:\n        while not tf.io.gfile.exists(cache):\n            logging.info(\n                f\\\"Waiting for cache index file at {cache} to be created before caching.\\\"\n            f\\\"Sleeping for {self.WAIT_TIME} seconds.\\\"\n        )\n            time.sleep(self.WAIT_TIME)\n    dataset = dataset.cache(cache)\n    return dataset\n\n", "CodeBleu_score": 50.03985072254573}, {"code": "def cache(self, dataset: Any, cache: Optional[str], wait: bool) -> Any\n\t\n    if cache is None:\n        return dataset\n    if not os.path.exists(cache):\n        os.makedirs(cache)\n    if wait:\n        while not os.path.exists(os.path.join(cache, \"index\")):\n            logging.info(\n                f\"Cache not found at {cache}. Waiting for {self.WAIT_TIME} seconds.\"\n            )\n            time.sleep(self.WAIT_TIME)\n    return dataset.cache(os.path.join(cache, \"cache\"))\n\n", "CodeBleu_score": 51.626220871077244}]}
{"instance_id": "8838a3a09a66", "generation_result": [{"code": "def create_estimator(params: Dict, MWF: bool) -> tf.Tensor\n\t\n    model_dir = params[\"model_dir\"]\n    params = load_configuration(params)\n    params[\"MWF\"] = MWF\n    model_provider = ModelProvider(model_dir)\n    params[\"model_parameters\"] = model_provider.params\n    params[\"model_dir\"] = model_provider.directory\n    session_config = tf.compat.v1.ConfigProto()\n    session_config.gpu_options.allow_growth = True\n    run_config = tf.estimator.RunConfig(session_config=session_config)\n    estimator = tf.estimator.Estimator(\n        model_fn=model_fn, model_dir=model_dir, params=params, config=run_config\n    )\n    return estimator\n\n", "CodeBleu_score": 43.16222111535469}, {"code": "def create_estimator(params: Dict, MWF: bool) -> tf.Tensor\n\t\n    # Setup tensorflow session configuration to avoid using all GPU RAM\n    config = tf.compat.v1.ConfigProto()\n    config.gpu_options.allow_growth = True\n    config.gpu_options.per_process_gpu_memory_fraction = 0.5\n    config.log_device_placement = False\n    # Setup estimator\n    estimator = tf.estimator.Estimator(\n        model_fn=model_fn,\n        model_dir=params[\"model_dir\"],\n        params=params,\n        config=tf.estimator.RunConfig(session_config=config),\n    )\n    return estimator\n\n", "CodeBleu_score": 38.5999158162891}, {"code": "def create_estimator(params: Dict, MWF: bool) -> tf.Tensor\n\t\n    model_provider = ModelProvider(params[\"model_dir\"])\n    params.update(model_provider.params)\n    session_config = tf.compat.v1.ConfigProto()\n    session_config.gpu_options.allow_growth = True\n    run_config = tf.estimator.RunConfig(session_config=session_config)\n    estimator = tf.estimator.Estimator(\n        model_fn=model_fn, model_dir=params[\"model_dir\"], params=params, config=run_config\n    )\n    return estimator\n\n", "CodeBleu_score": 41.69096165422454}]}
{"instance_id": "779c812bf2cb", "generation_result": [{"code": "def _separate_tensorflow(\n        self, waveform: np.ndarray, audio_descriptor: AudioDescriptor\n    ) -> Dict\n\tusr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])", "CodeBleu_score": 22.219058194363768}, {"code": "def _separate_tensorflow(\n        self, waveform: np.ndarray, audio_descriptor: AudioDescriptor\n    ) -> Dict\n\tusr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])", "CodeBleu_score": 22.21267737737265}, {"code": "def _separate_tensorflow(\n        self, waveform: np.ndarray, audio_descriptor: AudioDescriptor\n    ) -> Dict\n\tusr/bin/env python", "CodeBleu_score": 12.37244910640442}]}
{"instance_id": "cf580cd2ed93", "generation_result": [{"code": "def compute_backbone_shapes(config, image_shape)\n\t\n    if callable(config.BACKBONE):\n        return config.COMPUTE_BACKBONE_SHAPE(image_shape)\n    # Currently supports ResNet only\n    assert config.BACKBONE in [\"resnet50\", \"resnet101\"], \"backbone is not resnet50 or resnet101\"\n    return np.array(\n        [[int(math.ceil(image_shape[0] / stride)),\n        int(math.ceil(image_shape[1] / stride))]\n        for stride in config.BACKBONE_STRIDES])\n\n", "CodeBleu_score": 88.71785567997097}, {"code": "def compute_backbone_shapes(config, image_shape)\n\t\n    if callable(config.BACKBONE):\n        return config.COMPUTE_BACKBONE_SHAPE(image_shape)\n    # Currently supports ResNet only\n    assert config.BACKBONE in [\"resnet50\", \"resnet101\"]\n    return np.array(\n        [[int(math.ceil(image_shape[0] / stride)),\n            int(math.ceil(image_shape[1] / stride))] \n            for stride in config.BACKBONE_STRIDES])\n\n", "CodeBleu_score": 95.08210777674412}, {"code": "def compute_backbone_shapes(config, image_shape)\n\t\n    if callable(config.BACKBONE):\n        return config.COMPUTE_BACKBONE_SHAPE(image_shape)\n    # Currently supports ResNet only\n    assert config.BACKBONE in [\"resnet50\", \"resnet101\"], \"backbone is not resnet50 or resnet101\"\n    return np.array(\n        [[int(math.ceil(image_shape[0] / stride)),\n            int(math.ceil(image_shape[1] / stride))]\n            for stride in config.BACKBONE_STRIDES])\n\n", "CodeBleu_score": 88.67982951914676}]}
{"instance_id": "ebd61a2e59ca", "generation_result": [{"code": "def clip_boxes_graph(boxes, window)\n\t\n    # Split\n    wy1, wx1, wy2, wx2 = tf.split(window, 4)\n    y1, x1, y2, x2 = tf.split(boxes, 4, axis=1)\n    # Clip\n    y1 = tf.maximum(tf.minimum(y1, wy2), wy1)\n    x1 = tf.maximum(tf.minimum(x1, wx2), wx1)\n    y2 = tf.maximum(tf.minimum(y2, wy2), wy1)\n    x2 = tf.maximum(tf.minimum(x2, wx2), wx1)\n    clipped = tf.concat([y1, x1, y2, x2], axis=1, name=\"clipped_boxes\")\n    clipped.set_shape((clipped.shape[0], 4))\n    return clipped\n\n", "CodeBleu_score": 61.634432626045545}, {"code": "def clip_boxes_graph(boxes, window)\n\t\n    # Split\n    wy1, wx1, wy2, wx2 = tf.split(window, 4)\n    y1, x1, y2, x2 = tf.split(boxes, 4, axis=1)\n    # Clip\n    y1 = tf.maximum(tf.minimum(y1, wy2), wy1)\n    x1 = tf.maximum(tf.minimum(x1, wx2), wx1)\n    y2 = tf.maximum(tf.minimum(y2, wy2), wy1)\n    x2 = tf.maximum(tf.minimum(x2, wx2), wx1)\n    clipped = tf.concat([y1, x1, y2, x2], axis=1, name=\"clipped_boxes\")\n    clipped.set_shape((clipped.shape[0], 4))\n    return clipped\n\n", "CodeBleu_score": 61.634432626045545}, {"code": "def clip_boxes_graph(boxes, window)\n\t\n    # Split\n    wy1, wx1, wy2, wx2 = tf.split(window, 4)\n    y1, x1, y2, x2 = tf.split(boxes, 4, axis=1)\n    # Clip\n    y1 = tf.maximum(tf.minimum(y1, wy2), wy1)\n    x1 = tf.maximum(tf.minimum(x1, wx2), wx1)\n    y2 = tf.maximum(tf.minimum(y2, wy2), wy1)\n    x2 = tf.maximum(tf.minimum(x2, wx2), wx1)\n    clipped = tf.concat([y1, x1, y2, x2], axis=1, name=\"clipped_boxes\")\n    clipped.set_shape((clipped.shape[0], 4))\n    return clipped\n\n", "CodeBleu_score": 61.634432626045545}]}
{"instance_id": "8e0411587a70", "generation_result": [{"code": "def rpn_graph(feature_map, anchors_per_location, anchor_stride)\n\t\n    # Shared convolutional base of the RPN\n    shared = KL.Conv2D(512, (3, 3), padding='same', activation='relu',\n                       strides=anchor_stride,\n                       name='rpn_conv_shared')(feature_map)\n\n    shared = KL.Conv2D(512, (3, 3), padding='same', activation='relu',\n                       strides=anchor_stride,\n                       name='rpn_conv_shared')(feature_map)\n\n    # Anchor Score. [batch, height, width, anchors per location * 2].\n    x = KL.Conv2D(2 * anchors_per_location, (1, 1), padding='valid',\n                  activation='linear', name='rpn_class_raw')(shared)\n\n    x = KL.Conv2D(2 * anchors_per_location, (1, 1), padding='valid',\n                  activation='linear', name='rpn_class_raw')(shared)\n\n    # Reshape to [batch, anchors, 2] (where 2 is for bg/fg.)\n    rpn_class_logits = KL.Lambda(\n        lambda t: tf.reshape(t, [tf.shape(t)[0], -1, 2]))(x)\n\n    rpn_class_logits = KL.Lambda(\n        lambda t: tf.reshape(t, [tf.shape(t)[0], -1, 2]))(x)\n\n    # Softmax on last dimension of BG/FG.\n    rpn_probs = KL.Activation(\n        \\\"softmax\\\", name=\\\"rpn_class_xxx\\\")(rpn_class_logits)\n\n    rpn_probs = KL.Activation(\n        \"softmax\", name=\"rpn_class_xxx\")(rpn_class_logits)\n\n    # Bounding box refinement. [batch, H, W, anchors per location, depth]\n    # where depth is [x, y, log(w), log(h)]\n    x = KL.Conv2D(anchors_per_location * 4,\n    (1, 1), padding='valid', activation='linear',\n    name='rpn_bbox_pred')(shared)\n\n    x = KL.Conv", "CodeBleu_score": 75.09616071114459}, {"code": "def rpn_graph(feature_map, anchors_per_location, anchor_stride)\n\t\n    # Shared convolutional base of the RPN\n    shared = KL.Conv2D(512, (3, 3), padding='same', activation='relu',\n                       strides=anchor_stride,\n                       name='rpn_conv_shared')(feature_map)\n    # Anchor Score. [batch, height, width, anchors per location * 2].\n    x = KL.Conv2D(2 * anchors_per_location, (1, 1), padding='valid',\n                  activation='linear', name='rpn_class_raw')(shared)\n    # Reshape to [batch, anchors, 2]\n    rpn_class_logits = KL.Lambda(\n        lambda t: tf.reshape(t, [tf.shape(t)[0], -1, 2]))(x)\n    # Softmax on last dimension of BG/FG.\n    rpn_probs = KL.Activation(\n        \\\"softmax\\\", name=\\\"rpn_class_xxx\\\")(rpn_class_logits)\n    # Bounding box refinement. [batch, H, W, anchors per location, depth] \n    # where depth is [x, y, log(w), log(h)]\n    x = KL.Conv2D(anchors_per_location * 4, (1, 1), padding='valid',\n                  activation='linear', name='rpn_bbox_pred')(shared)\n    # Reshape to [batch, anchors, 4]\n    rpn_bbox = KL.Lambda(lambda t: tf.reshape(t,\n                                               [tf.shape(t)[0], -1, 4]))(x)\n    return [rpn_class_logits, rpn_probs, rpn_bbox]\n\n", "CodeBleu_score": 79.95480427274508}, {"code": "def rpn_graph(feature_map, anchors_per_location, anchor_stride)\n\t\n    # Shared convolutional base of the RPN\n    shared = KL.Conv2D(512, (3, 3), padding='same', activation='relu',\n                       strides=anchor_stride, name='rpn_conv_shared')(feature_map)\n    # Anchor Score.\n    x = KL.Conv2D(2 * anchors_per_location, (1, 1), padding='valid',\n                  activation='linear', name='rpn_class_raw')(shared)\n    # Reshape to [batch, anchors, 2]\n    rpn_class_logits = KL.Lambda(\n        lambda t: tf.reshape(t, [tf.shape(t)[0], -1, 2]))(x)\n    # Softmax on last dimension of BG/FG.\n    rpn_probs = KL.Activation(\n        \\\"softmax\\\", name=\\\"rpn_class_xxx\\\")(rpn_class_logits)\n    # Bounding box refinement.\n    x = KL.Conv2D(anchors_per_location * 4, (1, 1), padding='valid',\n                  activation='linear', name='rpn_bbox_pred')(shared)\n    # Reshape to [batch, anchors, 4]\n    rpn_bbox = KL.Lambda(lambda t: tf.reshape(\n        t, [tf.shape(t)[0], -1, 4]))(x)\n    return [rpn_class_logits, rpn_probs, rpn_bbox]\n\n", "CodeBleu_score": 79.37520037074644}]}
{"instance_id": "70777c9261d8", "generation_result": [{"code": "def smooth_l1_loss(y_true, y_pred)\n\t\n    diff = K.abs(y_true - y_pred)\n    less_than_one = K.cast(K.less(diff, 1.0), \\\"float32\\\")\n    loss = (less_than_one * 0.5 * diff**2) + (1 - less_than_one) * (diff - 0.5)\n    return loss\n\n", "CodeBleu_score": 64.6712900539723}, {"code": "def smooth_l1_loss(y_true, y_pred)\n\t\n    diff = K.abs(y_true - y_pred)\n    less_than_one = K.cast(K.less(diff, 1.0), \\\"float32\\\")\n    loss = (less_than_one * 0.5 * diff ** 2) + (1 - less_than_one) * (diff - 0.5)\n    return loss\n\n", "CodeBleu_score": 60.94925722088058}, {"code": "def smooth_l1_loss(y_true, y_pred)\n\t\n    diff = K.abs(y_true - y_pred)\n    less_than_one = K.cast(K.less(diff, 1.0), \\\"float32\\\")\n    loss = (less_than_one * 0.5 * diff**2) + (1 - less_than_one) * (diff - 0.5)\n    return loss\n\n", "CodeBleu_score": 64.6712900539723}]}
{"instance_id": "0e747b604087", "generation_result": [{"code": "def build_model(x_train, num_classes)\n\t\n    # Reset the TensorFlow graph\n    K.clear_session()\n\n    # Define the input layer\n    input_layer = KL.Input(shape=x_train.shape[1:])\n\n    # Add a convolutional layer with 32 filters, a kernel size of 3x3, and a ReLU activation function\n    conv_layer = KL.Conv2D(32, (3, 3), activation='relu')(input_layer)\n\n    # Add a max pooling layer with a pool size of 2x2\n    pool_layer = KL.MaxPooling2D(pool_size=(2, 2))(conv_layer)\n\n    # Add a convolutional layer with 64 filters, a kernel size of 3x3, and a ReLU activation function\n    conv_layer = KL.Conv2D(64, (3, 3), activation='relu')(pool_layer)\n\n    # Add a max pooling layer with a pool size of 2x2\n    pool_layer = KL.MaxPooling2D(pool_size=(2, 2))(conv_layer)\n\n    # Flatten the output of the previous layer\n    flatten_layer = KL.Flatten()(pool_layer)\n\n    # Add a dense layer with 128 units and a ReLU activation function\n    dense_layer = KL.Dense(128, activation='relu')(flatten_layer)\n\n    # Add a dense layer with the number of classes and a softmax activation function\n    output_layer = KL.Dense(num_classes, activation='softmax')(dense_layer)\n\n    # Create the model\n    model = KM.Model(inputs=input_layer, outputs=output_layer)\n\n    # Compile the model\n    model.compile(loss='categorical_crossentropy',\n                  optimizer=keras.optimizers.Adam(),\n                  metrics=['accuracy'])\n\n    return model\n\n", "CodeBleu_score": 31.786462192495023}, {"code": "def build_model(x_train, num_classes)\n\t\n    # Reset the TensorFlow graph\n    tf.reset_default_graph()\n\n    # Start defining the model\n    model_input = KL.Input(shape=x_train.shape[1:])\n\n    # First convolutional layer\n    x = KL.Conv2D(filters=32, kernel_size=(3, 3), strides=(1, 1), padding='same', activation='relu')(model_input)\n\n    # Second convolutional layer\n    x = KL.Conv2D(filters=32, kernel_size=(3, 3), strides=(1, 1), padding='same', activation='relu')(x)\n\n    # Max-pooling layer\n    x = KL.MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(x)\n\n    # Flatten the output of the second convolutional layer\n    x = KL.Flatten()(x)\n\n    # First dense layer\n    x = KL.Dense(units=128, activation='relu')(x)\n\n    # Second dense layer for classification\n    model_output = KL.Dense(units=num_classes, activation='softmax')(x)\n\n    # Create the model\n    model = KM.Model(inputs=model_input, outputs=model_output)\n\n    # Compile the model\n    model.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.Adam(), metrics=['accuracy'])\n\n    return model\n\n", "CodeBleu_score": 35.510672601495884}, {"code": "def build_model(x_train, num_classes)\n\t\n    # Clear the TensorFlow graph.\n    K.clear_session()\n\n    # Define the input layer.\n    input_shape = x_train.shape[1:]\n    inputs = KL.Input(shape=input_shape)\n\n    # Define the first convolutional layer.\n    x = KL.Conv2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same')(inputs)\n\n    # Define the second convolutional layer.\n    x = KL.Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same')(x)\n\n    # Define the max-pooling layer.\n    x = KL.MaxPooling2D(pool_size=(2, 2))(x)\n\n    # Flatten the output of the previous layer.\n    x = KL.Flatten()(x)\n\n    # Define the first dense layer.\n    x = KL.Dense(units=128, activation='relu')(x)\n\n    # Define the output layer.\n    outputs = KL.Dense(units=num_classes, activation='softmax')(x)\n\n    # Define the model.\n    model = KM.Model(inputs=inputs, outputs=outputs)\n\n    # Compile the model.\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n    return model\n", "CodeBleu_score": 32.90095530809843}]}
{"instance_id": "813e3e2844f4", "generation_result": [{"code": "def extract_bboxes(mask)\n\t\n    boxes = np.zeros([mask.shape[-1], 4], dtype=np.int32)\n    for i in range(mask.shape[-1]):\n        m = mask[:, :, i]\n        # Bounding box.\n        horizontal_indicies = np.where(np.any(m, axis=0))[0]\n        vertical_indicies = np.where(np.any(m, axis=1))[0]\n        if horizontal_indicies.shape[0]:\n            x1, x2 = horizontal_indicies[[0, -1]]\n            y1, y2 = vertical_indicies[[0, -1]]\n            # x2 and y2 should not be part of the box. Increment by 1.\n            x2 += 1\n            y2 += 1\n        else:\n            # No mask for this layer, therefore set the box to zeros\n            x1, x2, y1, y2 = 0, 0, 0, 0\n        boxes[i] = np.array([y1, x1, y2, x2])\n    return boxes.astype(np.int32)\n\n", "CodeBleu_score": 78.72991536104082}, {"code": "def extract_bboxes(mask)\n\t\n    boxes = np.zeros([mask.shape[-1], 4], dtype=np.int32)\n    for i in range(mask.shape[-1]):\n        m = mask[:, :, i]\n        # Bounding box.\n        horizontal_indicies = np.where(np.any(m, axis=0))[0]\n        vertical_indicies = np.where(np.any(m, axis=1))[0]\n        if horizontal_indicies.shape[0]:\n            x1, x2 = horizontal_indicies[[0, -1]]\n            y1, y2 = vertical_indicies[[0, -1]]\n            # x2 and y2 should not be part of the box. Increment by 1.\n            x2 += 1\n            y2 += 1\n        else:\n            # No mask for this layer, zero bbox.\n            x1, x2, y1, y2 = 0, 0, 0, 0\n        boxes[i] = np.array([y1, x1, y2, x2])\n    return boxes.astype(np.int32)\n\n", "CodeBleu_score": 78.72991536104082}, {"code": "def extract_bboxes(mask)\n\t\n    boxes = np.zeros([mask.shape[-1], 4], dtype=np.int32)\n    for i in range(mask.shape[-1]):\n        m = mask[:, :, i]\n        # Bounding box.\n        horizontal_indicies = np.where(np.any(m, axis=0))[0]\n        vertical_indicies = np.where(np.any(m, axis=1))[0]\n        if horizontal_indicies.shape[0]:\n            x1, x2 = horizontal_indicies[[0, -1]]\n            y1, y2 = vertical_indicies[[0, -1]]\n            # x2 and y2 should not be part of the box. Increment by 1.\n            x2 += 1\n            y2 += 1\n        else:\n            # No mask for this layer, zero out the box.\n            x1, x2, y1, y2 = 0, 0, 0, 0\n        boxes[i] = np.array([y1, x1, y2, x2])\n    return boxes.astype(np.int32)\n\n", "CodeBleu_score": 78.72991536104082}]}
{"instance_id": "7ad5d430d6ed", "generation_result": [{"code": "def compute_iou(box, boxes, box_area, boxes_area)\n\t\n    # Calculate intersection areas\n    y1 = np.maximum(box[0], boxes[:, 0])\n    y2 = np.minimum(box[2], boxes[:, 2])\n    x1 = np.maximum(box[1], boxes[:, 1])\n    x2 = np.minimum(box[3], boxes[:, 3])\n    intersection = np.maximum(x2 - x1, 0) * np.maximum(y2 - y1, 0)\n    union = box_area + boxes_area[:] - intersection[:]\n    iou = intersection / union\n    return iou\n\n", "CodeBleu_score": 58.663259083089194}, {"code": "def compute_iou(box, boxes, box_area, boxes_area)\n\t\n    # Calculate intersection areas\n    y1 = np.maximum(box[0], boxes[:, 0])\n    y2 = np.minimum(box[2], boxes[:, 2])\n    x1 = np.maximum(box[1], boxes[:, 1])\n    x2 = np.minimum(box[3], boxes[:, 3])\n    intersection = np.maximum(x2 - x1, 0) * np.maximum(y2 - y1, 0)\n    union = box_area + boxes_area[:] - intersection[:]\n    y1 = np.maximum(box[0], boxes[:, 0])\n    y2 = np.minimum(box[2], boxes[:, 2])\n    x1 = np.maximum(box[1], boxes[:, 1])\n    x2 = np.minimum(box[3], boxes[:, 3])\n    intersection = np.maximum(x2 - x1, 0) * np.maximum(y2 - y1, 0)\n    union = box_area + boxes_area[:] - intersection[:]\n    iou = intersection / union\n    return iou\n\n", "CodeBleu_score": 59.88682136584911}, {"code": "def compute_iou(box, boxes, box_area, boxes_area)\n\t\n    # Calculate intersection areas\n    y1 = np.maximum(box[0], boxes[:, 0])\n    y2 = np.minimum(box[2], boxes[:, 2])\n    x1 = np.maximum(box[1], boxes[:, 1])\n    x2 = np.minimum(box[3], boxes[:, 3])\n    intersection = np.maximum(x2 - x1, 0) * np.maximum(y2 - y1, 0)\n    union = box_area + boxes_area[:] - intersection[:]\n    # Calculate IoU\n    iou = intersection / union\n    # return the intersection over union value\n    return iou\n    y1 = np.maximum(box[0], boxes[:, 0])\n    y2 = np.minimum(box[2], boxes[:, 2])\n    x1 = np.maximum(box[1], boxes[:, 1])\n    x2 = np.minimum(box[3], boxes[:, 3])\n    intersection = np.maximum(x2 - x1, 0) * np.maximum(y2 - y1, 0)\n    union = box_area + boxes_area[:] - intersection[:]\n    iou = intersection / union\n    return iou\n\n", "CodeBleu_score": 59.193644936759384}]}
{"instance_id": "15941b852859", "generation_result": [{"code": "def compute_overlaps(boxes1, boxes2)\n\t\n    # Areas of anchors and GT boxes\n    area1 = (boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1])\n    area2 = (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1])\n\n    # Compute overlaps to generate matrix [boxes1 count, boxes2 count]\n    # Each cell contains the IoU value.\n    overlaps = np.zeros((boxes1.shape[0], boxes2.shape[0]))\n    for i in range(overlaps.shape[1]):\n        box2 = boxes2[i]\n        overlaps[:, i] = compute_iou(box2, boxes1, area2[i], area1)\n    return overlaps\n", "CodeBleu_score": 62.2853282032944}, {"code": "def compute_overlaps(boxes1, boxes2)\n\t\n    # Areas of anchors and GT boxes\n    area1 = (boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1])\n    area2 = (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1])\n\n    # Compute overlaps to generate matrix [boxes1 count, boxes2 count]\n    # Each cell contains the IoU value.\n    overlaps = np.zeros((boxes1.shape[0], boxes2.shape[0]))\n    for i in range(overlaps.shape[1]):\n        box2 = boxes2[i]\n        overlaps[:, i] = compute_iou(box2, boxes1, area2[i], area1)\n    return overlaps\n\n", "CodeBleu_score": 62.2853282032944}, {"code": "def compute_overlaps(boxes1, boxes2)\n\t\n    # Areas of anchors and GT boxes\n    area1 = (boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1])\n    area2 = (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1])\n\n    # Compute overlaps to generate matrix [boxes1 count, boxes2 count]\n    # Each cell contains the IoU value.\n    overlaps = np.zeros((boxes1.shape[0], boxes2.shape[0]))\n    for i in range(overlaps.shape[1]):\n        box2 = boxes2[i]\n        overlaps[:, i] = compute_iou(box2, boxes1, area2[i], area1)\n    return overlaps\n\n", "CodeBleu_score": 62.2853282032944}]}
{"instance_id": "597533722ce3", "generation_result": [{"code": "def load_image(self, image_id):\n\t\n    # Load image\n    image = skimage.io.imread(self.image_info[image_id]['path'])\n    # If grayscale. Convert to RGB for consistency.\n    if image.ndim != 3:\n        image = skimage.color.gray2rgb(image)\n    # If has an alpha channel, remove it for consistency\n    if image.shape[-1] == 4:\n        image = image[..., :3]\n    return image\n\n", "CodeBleu_score": 100.0}, {"code": "def load_image(self, image_id):\n\t\n    # Load image\n    image = skimage.io.imread(self.image_info[image_id]['path'])\n    # If grayscale. Convert to RGB for consistency.\n    if image.ndim != 3:\n        image = skimage.color.gray2rgb(image)\n    # If has an alpha channel, remove it for consistency\n    if image.shape[-1] == 4:\n        image = image[..., :3]\n    return image\n\n", "CodeBleu_score": 100.0}, {"code": "def load_image(self, image_id):\n\t\n    # Load image\n    image = skimage.io.imread(self.image_info[image_id]['path'])\n    # If grayscale. Convert to RGB for consistency.\n    if image.ndim != 3:\n        image = skimage.color.gray2rgb(image)\n    # If has an alpha channel, remove it for consistency\n    if image.shape[-1] == 4:\n        image = image[..., :3]\n    return image\n\n", "CodeBleu_score": 100.0}]}
{"instance_id": "9b852997ca05", "generation_result": [{"code": "def display_images(images, titles=None, cols=4, cmap=None, norm=None,\n                   interpolation=None)\n\t\n    # Number of images\n    n_images = len(images)\n\n    # If no titles are provided, use numbers as titles\n    if titles is None:\n        titles = ['(%d)' % i for i in range(1, n_images + 1)]\n\n    # Number of rows\n    rows = int(np.ceil(n_images / float(cols)))\n\n    # Create figure and set title\n    fig = plt.figure()\n    fig.canvas.set_window_title(titles[0])\n\n    # Create subplots\n    for i in range(len(images)):\n        ax = fig.add_subplot(rows, cols, i + 1)\n        # Set title\n        ax.set_title(titles[i])\n        # Turn off axis\n        ax.set_axis_off()\n        # Display image\n        plt.imshow(images[i], cmap=cmap, norm=norm, interpolation=interpolation)\n    # Show the figure\n    plt.show()\n\n", "CodeBleu_score": 33.66923525746667}, {"code": "def display_images(images, titles=None, cols=4, cmap=None, norm=None,\n                   interpolation=None)\n\t\n    # Number of images\n    n_images = len(images)\n    # Number of subplots to display\n    n_cols = min(n_images, cols)\n    n_rows = np.math.ceil(n_images / n_cols)\n    # Create figure and set title\n    fig = plt.figure()\n    fig.canvas.set_window_title('Images')\n    # Adjust subplot parameters to give specified padding\n    if titles is None:\n        titles = ['Image (%d)' % i for i in range(1, n_images + 1)]\n    plt.subplots_adjust(top=0.85, bottom=0, left=0, right=1, hspace=0.05,\n                        wspace=0.05)\n    # Iterate over images and titles\n    for n, (image, title) in enumerate(zip(images, titles)):\n        a = fig.add_subplot(n_rows, n_cols, n + 1)\n        plt.imshow(image, cmap=cmap, norm=norm, interpolation=interpolation)\n        a.set_title(title)\n    # Show the figure\n    plt.show()\n\n", "CodeBleu_score": 38.113291808692296}, {"code": "def display_images(images, titles=None, cols=4, cmap=None, norm=None,\n                   interpolation=None)\n\t\n    # Number of images\n    n_images = len(images)\n\n    # Number of titles\n    if titles is None:\n        titles = ['(%d)' % i for i in range(1, n_images + 1)]\n    # Number of rows\n    rows = len(titles) // cols + 1\n\n    # Create figure\n    fig = plt.figure()\n\n    for i in range(len(images)):\n        # Create subplot and append to figure\n        ax = fig.add_subplot(rows, cols, i + 1)\n\n        # Display image\n        if isinstance(cmap, str):\n            ax.imshow(images[i], cmap=cmap)\n        else:\n            ax.imshow(images[i])\n\n        # Set title\n        ax.set_title(titles[i])\n\n        # Remove ticks from the plot\n        ax.set_xticks([])\n        ax.set_yticks([])\n\n        # Set grid\n        ax.grid('off')\n\n        # Set interpolation\n        if interpolation is not None:\n            ax.imshow(images[i], interpolation=interpolation)\n\n    # Show the figure\n    plt.show()\n\n", "CodeBleu_score": 36.46334508832165}]}
